question_id,title,body,tags
2081769,Use Mean value theorem to prove the following inequality,"A) Use the Mean value theorem to prove that
\begin{equation}
\sqrt{1+x} < 1 + \frac{1}{2}x \text{ if } x>0
\end{equation} B) Use result in A) to prove that 
\begin{equation}
\sqrt{1+x}>1+\frac{1}{2}x-\frac{1}{8}x^2 \text{ if } x>0
\end{equation} Can someone give an answer for part B) ?",['calculus']
2081794,Is this set of vectors linearly (in)dependent?,"I have the following problem: Are the following vectors linearly independent in $\mathbb{R}^2$?
\begin{bmatrix} -1 \\ 2 \end{bmatrix}\begin{bmatrix} 1 \\ -2 \end{bmatrix}\begin{bmatrix} 2 \\ -4 \end{bmatrix} when I solve this using $c_1 v_1+c_2 v_2+ c_3 v_3=0$ I get an underdetermined system, can anyone help me to understand what this means for the linear independence? Thanks in advance :)","['linear-algebra', 'vector-spaces']"
2081817,"Limits: if $ \lim_{x\to\infty}f(x) = 0 $, does $\lim_{x \to 0}f(1/x)$ exist?","Let $f$ be a function:
$$ f:\mathbb{R} \to \mathbb{R} $$
It is known that:
$$ \lim_{x\to\infty}f(x) = 0 $$
I need to prove / disprove that the following limit exist:
$$ \lim_{x \to 0}f\left(\frac{1}{x}\right) $$",['limits']
2081826,Derivate by $x^2$,"$$\frac{d}{dx^2}x=\frac{1}{2x}$$ I can't wrap my head around that. It appeared in a physics context, i.e. it may not be mathematically rigorous. Thanks for some explanation. Wolfram Alpha tells me I cannot differentiate by $x^2$: Invalid value","['derivatives', 'calculus']"
2081870,Solution to an ordinary differential equation,The general solution to the equation $y''+by'+cy=0$ approaches to $0$ as $x$ approaches infinity if $b$ is negative $c$ is positive $b$ is positive $c$ is negative $b$ is positive $c$ is positive $b$ is negative $c$ is negative,['ordinary-differential-equations']
2081875,"Given that $\cos(x/2)\cos(x/4)\cos(x/8)\ldots=(\sin x)/x,$ prove that $(1/2^2)\sec^2(x/2)+(1/2^4)\sec^2(x/4)\ldots=\csc^2(x) - (1/x^2).$",How to solve this one I know this is related to  differentiation but how to proceed with this??? Please give all steps so that it is easily understood.,['derivatives']
2081895,Real projective space $\mathbb{R}P^1$ is diffeomorphic to $S^1$? [duplicate],"This question already has answers here : Showing diffeomorphism between $S^1 \subset \mathbb{R}^2$ and $\mathbb{RP}^1$ (3 answers) The space obtained by identifying the antipodal points of a circle (4 answers) Closed 7 years ago . I am reading Lee, Manifolds and Differential Geometry, and I am a bit confused. It is seen that the real projective space $\mathbb{R}P^n$ is homeomorphic to $S^n/ \sim$, where $p \sim q$ iff $p = \pm q$. However, in one of the exercises (1.48) one is asked to prove that $\mathbb{R}P^1$ is diffeomorphic to $S^1$. Is this really true (is there a simple proof of this) and does this imply something for the relation between $S^1$ and $S^1/\sim$? Does this also mean that $\mathbb{R}P^1$ is homeomorphic to $S^1$ (since diffeomorphism is a stronger condition)?","['general-topology', 'differential-geometry']"
2081965,Expected value (mean) of $e^Z$ where $Z$ poisson distributed,"I have $Z \sim \operatorname{po}(1)$, and must find $E(e^{Z})$. My method was as follows. I know that for a function $p$ with support $\{x_i \mid i \in I\}$: $$E(g(X)) = \sum_{i\in I} g(x_i)\cdot p(x_i) $$ So I plop my transformed variable in, and I get the following: $$E(e^Z) = \sum_{i=0}^\infty e^i \cdot p(z_i)$$ And the poisson probability mass function: $$E(e^Z) = \sum_{i=0}^\infty e^i \cdot e^\lambda \cdot \frac{\lambda^i}{i!}$$ And I'm not sure how to proceed from here... I took a sneak peak on wolfram, and saw that this sum converges on $e^{(e-1)\cdot \lambda}$, and since our lambda parameter is $1$, $e^{(e-1)}$. But I'm not sure how can, in hand, reach this result.","['statistics', 'sequences-and-series', 'poisson-distribution']"
2081971,From Hermitian matrix operator to differential equations,"I understand that if my operator $A$ is a Hermitian, then suppose it has the following eigen vectors
$$A|v_1\rangle=\lambda|v_1\rangle:(A).(V_1)=\lambda(V_1)\\
A|v_2\rangle=\beta|v_2\rangle:(A).(V_2)=\beta(V_2)\\
\langle v_2|A|v_1\rangle=\lambda\langle v_2|v_1\rangle:(V_2)^{\dagger}.(A).(V_1)=\lambda(V_2)^{\dagger}.(V_1)\\
\text{since}(A).(V_2)=\beta(V_2)\quad\text{hence}\quad (V2)^{\dagger}.(A)^{\dagger}=\beta(V_2)^{\dagger}$$
Now, if $A$ is a hermitian operator, then $A=A^{\dagger}$, hence
$$(V_2)^{\dagger}.(A)^{\dagger}=(V_2)^{\dagger}.(A)=\beta(V_2)^{\dagger}\\
\text{Hence}\quad (V_2)^{\dagger}.(A).(V_1)=\beta(V_2)^{\dagger}(V_1)\\
\text{implies} \quad\langle v_2|A|v_1\rangle =\lambda\langle v_2|v_1\rangle = \beta\langle v_2|v_1\rangle:\lambda\neq\beta\\
\text{Hence}\quad \langle v_2|v_1\rangle=0$$
So, the vectors must be orthogonal to each other and their inner product must be zero, but how do I know in a differential equation if my operator is a hermitian or not and whether I will have quantized solutions whose inner products will be zero? For example the schrodinger wave equation leads to quantized orthogonal solutions. Or rather, how do I extend a matrix based Hermitian operator to a differential equation?","['partial-differential-equations', 'mathematical-physics', 'functional-analysis', 'linear-algebra', 'vector-spaces']"
2081979,Solve a matrix equation,"I need to find $X$ from $$\begin{pmatrix}
1 & 2\\ 
-3 &-6 
\end{pmatrix} X \begin{pmatrix}
1 &2 \\ 
-1 &-2 
\end{pmatrix}=\begin{pmatrix}
2 &4 \\ 
-6 & -12
\end{pmatrix}$$ I wrote $X$ as $$X=\begin{pmatrix}
a & b\\ 
c &d 
\end{pmatrix}$$ and I got $a+2c-b-2d=2$ but I do not know what to do next. Please help.","['matrices', 'matrix-equations']"
2082015,Are monotonic and bijective functions the same?,"The question is simple :  consider the family of monotonic functions ; $m(x) : \mathbb{R} \rightarrow \mathbb{R}$, and the familly of bijective functions ; $b(x) : \mathbb{R} \rightarrow \mathbb{R}$.  Are they actually the same?  If not, I would like to see some simple counter-examples.","['continuity', 'real-analysis', 'analysis', 'functions']"
2082037,Terminology: what is a spun manifold,"I have been reading this paper and encountered the words ""spun manifold"". Now, I know what a spin structure is. I just want to check that ""spun manifold"" is indeed just a grammatically correct form of ""spin manifold"" and not ""a spin manifold with extra structure"".","['terminology', 'differential-geometry']"
2082045,Prove that : $\sum_{n=1}^{\infty}\left({1\over n}-{2\zeta(2n)\over 2n+1}\right)=\ln{\pi}-1$,Prove that $$\sum_{n=1}^{\infty}\left({1\over n}-{2\zeta(2n)\over 2n+1}\right)=\ln{\pi}-1\tag0$$ My try: $$\sum_{n=1}^{\infty}\left({1\over n}-{1\over n+1}\right)=1\tag1$$ From wolfram:(126) $$\sum_{n=1}^{\infty}{\zeta(2n)\over n(2n+1)}=\ln{2\pi}-1\tag2$$ Rewrite: $$\sum_{n=1}^{\infty}\left({\zeta(2n)\over 2n}-{\zeta(2n)\over 2n+1}\right)={1\over 2}\left(\ln{2\pi}-1\right)\tag3$$ $2\times(3)-(0)$: $$\sum_{n=1}^{\infty}\left({\zeta(2n)-1\over n}\right)=\ln{2}\tag4$$ $(4)$ it is a well known proven identity from wikipedia(infinite series) If I got to $(4)$ and it is know on that (4) is correct then it is imply that my original question must be correct?,"['zeta-functions', 'sequences-and-series']"
2082055,Exercise on a twice differentiable function,"I have the following exercise: Let $f$ be a twice differentiable function on $\mathbb R$ such that: $f(-1)<0$ $f(1)>0$ $\lim_{x\to \pm \infty} f(x) =0$ Prove that $f''(x)= 0$ for at least three values of $x.$ I managed to find one such point reasoning about local maximum/minimum, and I know that the function has at least one zero. But I don't know how to proceed further.","['derivatives', 'real-analysis', 'calculus']"
2082076,Closed form for $\sum\limits_{r=-\infty}^\infty \sum\limits_{s=-\infty}^\infty \frac{1}{(k^2\tau^2+(x+2\pi r)^2+(y+2\pi s)^2)^{3/2}}$,"I have a problem while trying to find a closed form for the following double sum. 
$$\sum_{r=-\infty}^\infty \sum_{s=-\infty}^\infty \frac{1}{(k^2\tau^2+(x+2\pi r)^2+(y+2\pi s)^2)^{3/2}}$$ I used Mathematica for the evaluation but it just returns the double sum. I am wondering how this kind of infinite sum can be handled. Any hints or suggestions are welcomed, perhaps by proposing some ideas to simplify it or making some approximations. @Dr. Wolfgang Hintze Thank you very much for the detailed explanation. I have a question, when I try to plot the double sum I get an error ''NSum :Summand (or its derivative)... - is not numerical at point r = 0 ''? To plot it I just wrote this : What is the problem in this formula?",['sequences-and-series']
2082077,Can anyone explain the differential equation for the sunflower seed pattern>,"It seems that there is an algorithm to generate the sunflower seed pattern based on the golden ratio. From this article in the Irish Times : A simple mathematical description of the geometry of sunflower seed
  patterns was devised by Helmut Vogel (1979). He defined the positions
  of the seeds, using polar coordinates (r, θ), by r(n) = √n         and         θ(n) = n φ where φ ≈ 137.5º is the golden angle. Thus, as n increases by one, the
  position rotates through the golden angle and the radius increases as
  the square root of n. All points are on a curve called the generative
  spiral (r = √θ), a form of Fermat spiral which winds ever-tighter as
  it curls outwards. But that doesn't explain how the sunflower actually does it. Now someone has come up with a differential equation that claims to generate the pattern. From the same article: The equation comes from a paper by Pennybacker and Newell which I don't have access to (and probably wouldn't understand if I did). Is this for real? Does this differential equation really work? And if so, can anyone give a physical explanation of what the individual terms are doing? I don't mean ""explain to me what the del operator does"", I mean like ""why do you subtract the cube of the hormone concentration..."" etc. Any insights?",['ordinary-differential-equations']
2082103,Trigonometric/polynomial equations and the algebraic nature of trig functions,"Prove or disprove that an equation involving one trig function (either $\sin,\cos,\tan$, etc) with an argument of the form $ax+b$ for non-zero rational $a,b$  and a polynomial with non-zero rational coefficients and a constant term not equal to $\pm1$ or zero is not solvable in closed form.  For example, $$\sin(x)=2-x-x^2$$ It has solutions near $x=0.752$ and $x=-2.242$ My reasoning for why there is no closed form solution is because If $x$ is a rational multiple of $\pi$, the LHS is algebraic, while the RHS is transcendental. If $x$ is of the form $x=\arcsin(u)\ne\frac ab\pi$ and $u$ is algebraic, then  the LHS is algebraic but the RHS... is not algebraic?  See When is ArcTan a rational multiple of pi? for some information. If $x$ is none of the above, I don't think there exists a closed form solution since $\sin(x)$ cannot be calculated in closed form and neither can $2-x-x^2$. Can someone prove this general idea? If it is solvable, then under what conditions? Attempting to reduce the amount of questions that ask for closed form solutions in these scenarios, like Nonlinear algebraic equation with trigonometric function How to solve $x+\sin(x)=b$ Solving an equation etc. To clarify, closed form in this context is a solution in terms of well-known constants and a finite combination of well-known functions .","['algebra-precalculus', 'trigonometry', 'transcendental-equations']"
2082131,Prove $ \frac{1}{2} (\arccos(x) - \arccos(-x)) = -\arcsin(x)$,"The identity I need to prove is this, and I am very close but I am missing a negative sign which I cannot find.
$$ \frac{1}{2} (\arccos(x) - \arccos(-x)) = -\arcsin(x)$$ I started off by using the $\cos(A-B)$ double angle formula, using $\arccos(x)$ and $\arccos(-x)$ and $A$ and $B$ respectively. Substituting this into the double angle formula, I get that $$\cos(A-B)=1-2x^2$$ Then, from this I say let $x=\sin{y}$, so that I end up with the large expression $$\cos(\arccos(\sin{y})-\arccos(-\sin{y}))=1-2\sin^2(y)$$ Now we can use the cosine double angle formula on the RHS so this becomes $$\cos(\arccos(\sin{y})-\arccos(-\sin{y}))=\cos(2y)$$ Taking the inverse cosine on both sides leads to $$\arccos(\sin{y})-\arccos(-\sin{y})=2y$$ Now reversing the substitution, saying that $y=\arcsin{x}$, we get that $$\arccos(x) - \arccos(-x) = 2\arcsin(x)$$ which is nearly the identity I need to prove except that I have made an error in some step, causing the expression to be wrong. I cannot find this mistake.","['trigonometry', 'inverse']"
2082144,Find all injections $f(n + m) + f(n - m) = f(n) - f(m) + f(f(m) + n) $,"Find all injections  $f: \mathbb N \to \mathbb N$ such that: $f(n + m) + f(n - m) = f(n) - f(m) + f(f(m) + n) $ I have an idea to substitute $n=m$, since if $f(n)=f(m)$, then $n=m$ (the rule for injective functions). If I do that, I get
$f(2m)+f(0)=f(f(m)+m)$ and after that I have no idea what to do. Any help?","['functions', 'functional-equations']"
2082146,Primes as sums of powers,"We know that infinitely many primes are the sums of two squares, but what about other powers than two? For what other powers, $k$, do we know the minimum number, $n$, such that infinitely many primes are the sum of $n$ $k$-th powers?",['number-theory']
2082164,Prove formula for expected value for exponential families,"Let $\{P_\theta: \Theta \subset \mathbb{R^k}\}$ be a regular statistical family. Let $T=(T_1,\ldots,T_k)$ has density of form: $\displaystyle f=c(\theta)\cdot h(t) \cdot \exp \left\{\sum_{j=1}^{k}\theta_jt_j\right\} $. Let $\ell_1\ge0, \ldots , \ell_k \ge 0$ and $\ell_1+\cdots+\ell_k=\ell$. Prove that: $$E_\theta \left[ \prod_{i=1}^k T_i^{l_i} \right] = c(\theta)\frac{\partial^\ell}{\partial \theta_1^{\ell_1} \cdots \partial \theta_k^{\ell_k}}\frac{1}{c(\theta)}.$$ My attempt: $f=h(t)\cdot \exp \left\{\sum_{j=1}^ k\theta_jt_j  - \ln\frac 1 {c(\theta)} \right\}$ is in canonical form and we have $(T_1,\ldots,T_n)$ is sufficient statistic hence $$(ET_1,\ldots,ET_n)= \left( \frac{\partial}{\partial \theta_1}\ln\frac 1 {c(\theta)}, \ldots, \frac{\partial}{\partial \theta_k}\ln\frac 1 {c(\theta)}\right) = \left(c(\theta) \cdot\frac{\partial}{\partial \theta_1}\frac{1}{c(\theta)}, \ldots , c(\theta) \cdot\frac{\partial}{\partial \theta_n}\frac{1}{c(\theta)} \right)$$ and here I stuck.",['statistics']
2082191,Is the Weierstrass function curve homeomorphic to the real line?,"It's intuitive to me to assume that for any continuous function $f:\mathbb{R}\rightarrow\mathbb{R}$ that is defined for all real numbers, it's curve plotted in $\mathbb{R}^2$ is homeomorphic to $\mathbb{R}$. But it then occured to me that the ""arclength"" between any two points on the curve of the Weierstrass function is probably infinite the same way it is on the Koch curve-and it doesn't make intuitive sense for a metric space (if it can even be classified as one) like this to be homeomorphic to the real line.","['general-topology', 'fractals']"
2082200,Upper Bound of Eigenvalues of Symmetric Real Matrix $A$,"Let $A_n=\begin{pmatrix}
1&\frac{1}{2}&\frac{1}{3}&\cdots&\frac{1}{n}\\
\frac{1}{2}&\frac{1}{2}&\frac{1}{3}&\cdots&\frac{1}{n}\\
\frac{1}{3}&\frac{1}{3}&\frac{1}{3}&\cdots&\frac{1}{n}\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
\frac{1}{n}&\frac{1}{n}&\frac{1}{n}&\cdots&\frac{1}{n}
\end{pmatrix}$.
For each $n\in\mathbb{N}$, let $\lambda_0$ be one eigenvalue of $A_n$ ($\lambda_0$ is arbitrarily chosen) Prove that $0<\lambda_0<3+2\sqrt{2}$. A problem in a previous final exam. The number $3+2\sqrt{2}$ seems strange enough... If $\alpha=(x_1, x_2,\cdots,x_n)^\mathrm{T}$ is an eigenvector of $A_n$ belonging to $\lambda_0$, we get $n$ equations:
$$x_1+\frac{x_2}{2}+\cdots+\frac{x_n}{n}=\lambda_0x_1$$
$$\frac{x_1}{2}+\frac{x_2}{2}+\cdots+\frac{x_n}{n}=\lambda_0x_2$$
$$\vdots$$
$$\frac{x_1}{n}+\frac{x_2}{n}+\cdots+\frac{x_n}{n}=\lambda_0x_n$$
I tried to use inequalities to get a proof, but it doesn't work...There should be better approaches. Any advice or help appreciated.","['matrices', 'linear-algebra']"
2082210,How to solve this series: $\sum_{k=0}^{2n+1} (-1)^kk $,"$$\sum_{k=0}^{2n+1} (-1)^kk $$ The answer given is $-n-1$. I have searched for how to do it, but I have problems simplifying the sum and solving it.  How do you go about solving this?","['summation', 'sequences-and-series', 'calculus']"
2082223,How to calc action of operator on functions without diagonalizing the operator?,"I have an hermitian operator
$$
h = -\frac{1}{2} \frac{\partial^2}{\partial x^2} + v(x)
$$
acting on square integrable functions that live in some subspace $V$ of $R$. If I want a computer to calculate the action of this operator on a known function $f$ I would do the following: discretize $V$ into the points $x_i$ such that $\Delta x = x_{i+1} -x_i$ is small. calculate and store $f(x_i)$ for all discrete points $x_i$. calculate the action of $v(x)$ via $$v(x)f(x) \quad\longrightarrow\quad v(x_i)f(x_i) $$ calculate the action of $-\frac{1}{2} \frac{\partial^2}{\partial x^2}$ via $$ -\frac{1}{2} \left(\frac{\partial^2}{\partial x^2}f\right)(x) \quad\longrightarrow\quad -\frac 1 2 \frac{f(x_{i+2}) - 2f(x_{i+1}) + f(x_{i})}{\Delta x^2}  $$ However, how can I calculate the action of the operator
$$
\text {exp}({\text i h t}) = \text{exp}\left(\text i \left[ -\frac{1}{2} \frac{\partial^2}{\partial x^2} + v(x)  \right] t \right)\;,\quad t\in R
$$
on a known function $f$ on a computer? I know that the standard procedure is to diagonalize $h$ (i.e. find the eigenvalues $\varepsilon_k$ and eigenfunctions $\varphi_k$ of $h$). Then I could write
$$
h = \sum_k \varepsilon_k \, \varphi_k \, \varphi^*_k
$$
and
$$
f(x) = \sum_k c_k \, \varphi_k(x)
$$
such that
$$
\text{exp}(\text i h t)f(x) = \sum_k c_k \, \text{exp}(\varepsilon_k) \, \varphi_k(x)
$$ But I want to avoid the diagonalization since it is expensive. I also didn't need to diagonalize $\frac{\partial^2}{\partial x^2}$ to let it act on a function! So is there a clever way to avoid the diagonalization of $h$ in order to let  $\text{exp}(\text i h t)$ act on $f$?","['mathematical-physics', 'operator-theory', 'diagonalization', 'discrete-calculus', 'discrete-mathematics']"
2082233,Impact of Riemannian Geometry on Group Theory,"In the Wikipedia page for Riemannian Geometry , it mentions that the field had made a ""profound impact on group theory"". What are some examples of this? Looking around a bit (including on that page), it seems more like it is the other way around (i.e. group theory informs Riemannian manifold theory). E.g. analyzing a manifold with its fundamental group, or with Lie theory.
(But perhaps these can be viewed inversely.)","['riemannian-geometry', 'abstract-algebra', 'group-theory', 'manifolds', 'differential-geometry']"
2082234,the weights of the induced $\mathbb C^*$-action on the tangent space to $\mathbb P^m$ at fixed points,"Let $\mathbb C^*$ act on $\mathbb C^{m+1}$ with generic weights $\lambda_0,\cdots, \lambda_r$, and this action can be naturally regarded as an action on $\mathbb P^m$. Let $p_i\in \mathbb P^m$ ($i=0,1,\cdots m$) be the points determined by the basis vectors, i.e. the fixed points of the $\mathbb C^*$-action on $\mathbb P^m$. Now at $p_i$ the action on $\mathbb P^m$ will also induce an action on the tangent space $T_{p_i}\mathbb P^m$. Question 1 Show that this induced action on the tangent space at $p_i$ has weights $\lambda_i-\lambda_j$ for $j\neq i$. I am confused about some basic points. For example, I don't know how to interpret and understand the weights of the action, and what is the precise definition of the weights in this case? Also, what is the induced action on the tangent spaces? The following is my attempt. Assume $p_0=[1,0,\cdots, 0]\in \mathbb P^m$. Consider $U_0=\{[x_0,\cdots,x_m]\in \mathbb P^m | x_0\neq 0\}$, and the corresponding local chart $\psi_0 : U_0 \to \mathbb C^m, [x_0,\cdots,x_m]\mapsto (\frac{x_1}{x_0}, \cdots, \frac{x_m}{x_0})$. Let $\rho:\mathbb C^*\to \text{Aut}(\mathbb P^m), t \mapsto \rho_t$ be the action. For example, if we assume $\rho_t[x_0,\cdots, x_m]=[t^{a_0}x_0, \cdots, t^{a_m}x_m]$, then in the above local chart, $\rho_t(y_1,\cdots, y_m)=(t^{a_1-a_0}y_1, \cdots t^{a_m-a_0}y_m)$. Question 2 What if we replace the $\mathbb C^*$ action by a $\mathbb T:=\mathbb C^*\times \cdots \times \mathbb C^*= (\mathbb C^{*})^{m+1}$ action such as $\rho_{(t_0, \cdots, t_m)}[x_0,\cdots, x_m]=[t_0x_0, \cdots, t_mx_m]$? In this case, how to understand the weight of this action? PS : There is a relevant question.","['group-actions', 'projective-space', 'representation-theory', 'algebraic-geometry']"
2082251,Differential equation combined with functional equation which looks simple,"The equation is $$f'(x)=cf(x/2).$$ The problem emerges when I'm trying to deal with a partial differntial equation $$u_{t}(x,t)=u_{xx}(x,t/2).$$ Either using separation of variables: $$u(x,t)=X(x)T(t)\rightarrow X(x)T'(t)=X''(x)T(t/2)$$ so that $$X''(x)-cX(x)=0, T'(t)-cT(t/2)=0$$ or doing Fourier transform: $$\hat{u}_t(s,t)+s^2\hat{u}(s,t/2)=0$$ eventually reduces the problem to solving $$f'(x)=cf(x/2).$$ But the equation does not seem easy to solve. I can only prove the existence of solutions using the Euler method. The numerical simulation looks like The other function plotted for comparison is the exponential function. Does a solution of closed form exist to the equation? If not, how do we solve the original PDE?","['ordinary-differential-equations', 'functional-equations']"
2082303,Let $A$ be a $2 \times 2$ matrix such that $A^2=I$. Find trace and determinant,"Suppose $A$ is a $2 \times 2$ matrix, $A^2=I$ . If $A \ne I,-I$ , find $\mbox{tr}(A)$ and $\det(A)$ . My solution: There are $3$ cases for the eigenvalues of A. case(i) $1,1$ . case(ii) $-1,-1$ . case(iii) $1,-1$ . I guess only the case(iii) holds. But I have no idea whether case(i) and (ii) hold. I know that $tr(A)$ is the sum of the sum of eigenvalues and $det(A)$ is the product of eigenvalues. So the answer is $tr(A)=0$ , $det(A)=-1$ ? Thanks.","['eigenvalues-eigenvectors', 'matrices', 'determinant', 'trace', 'linear-algebra']"
2082327,What is the name of this 'property' in statistics?,"Today we have learned about something which roughly translates to 'Markov-property' at school. I have looked it up and found this page: https://en.wikipedia.org/wiki/Markov_property , however, this is way too complicated for me and I am afraid this is not what I am looking for. We learned that for statistical populations of units 
$x_1, x_2, x_3, \dots, x_n$ where all units are positive numbers (I am sorry if I use the terminology wrong, these words are completely new to me and English is not my native language), the following stands: Let $A > \bar{x}$ (Where $\bar{x}$ is the arithmetic average of the population).
Then there are $\frac{n\bar{x}}{A}$ numbers which are greater or equal to $A$ (Where $n$ is the number of units in the population). I hope I have been clear enough... 
Could anybody tell me what this is and where can I find more information about it?","['terminology', 'statistics']"
2082338,Question on $\mu$-measurable function,"I hope you can help me with this question: Let ($\Omega,\Sigma,\mu$) be a measurable space. Let $f:\Omega\rightarrow \mathbb{R}$ be a real-function. I would like to know: If $\vert f \vert$ is $\mu$-measurable, is $f$ $\mu$-measurable too? Thanks in advance!","['measure-theory', 'analysis']"
2082345,Ideal class group of $\mathbb{Q}(\sqrt{-65})$,"I am trying to show that the ideal class group, $Cl(A)$ of $K:=\mathbb{Q}(\sqrt{-65})$, where $A$ is the ring of integers of $K$, is isomorphic to the product two cyclic groups of order 2 and 4, respectively. I am going to share the process I have followed and enumerate the questions I have come up with. By now, since $-65\equiv 3\mod(4)$ the discriminant is $\Delta=-4\cdot 65$, $A=\mathbb{Z}[\sqrt{-65}]$, the group of units is $U(K)=\{\pm 1\}$, and since we are working with a complex quadratic extension $r_1=0$, $r_2=1$ , and the Minkowski bound is $\mathcal{M}<11$. Therefore, every ideal class must have a prime ideal of norm less or equal than $ 10$. Since every ideal class must have a prime ideal of norm less or equal than 10, the only primes we have to study are those generated by $2,3,4,5,7,$ and $9$ ($\textbf{1.}$ Why is it that all prime ideals must have norm $p$ or $p^2$?). Now, since $2$ and $5$ are the only primes that ramify (because $2,5|\Delta$), then there doesn't exist any prime ideal of norm $4$ ($\textbf{2.}$ Why?). Also, $\left(\frac{-65}{3}\right)=1$, hence $3A=\mathfrak{p}_3\mathfrak{p}_3'$ thus there doesn't exist any prime ideal of norm $9$ ($\textbf{3.}$ Why?). Finally, $7A$ is inert, and therefore principal so $[7A]=1$ All things considered, $Cl(A)=\langle[\mathfrak{p}_2],[\mathfrak{p}_3], [\mathfrak{p}_5]\rangle$. In addition to this, since
$$N(4+\sqrt{-65})=3^4\Rightarrow(4+\sqrt{-65})A=\mathfrak{p}_3^4$$
$$N(5+\sqrt{-65})=2\cdot 3^2\cdot 5\Rightarrow(4+\sqrt{-65})A=\mathfrak{p}_2\mathfrak{p}_3^2\mathfrak{p}_5$$
and there doesn't exist any element of order $10$, $\mathfrak{p}_2\mathfrak{p}_5$ is not a principal ideal, $\mathfrak{p}_3^2$ is not principal and $\mathfrak{p}_3$ has order 4. ($\textbf{5.}$ I am not sure at all of what is going on in this paragraph, I do not follow the logic behind this). And this is were I get stuck, when I try to find the order of each of the elements of $Cl(A)$.","['number-theory', 'dedekind-domain', 'algebraic-number-theory']"
2082348,Calculation of the principal divisor,"I have quite a problem in solving this excercise: Given $P=(0,0)$ and the curve $C: x+y+x^4+y^3=0.$ I want to calculate the principal divisor of $f=y(x^2-y)\in k(C).$ So I want to calculate $$(f)=\sum_{Q\in C}v_Q(f)\cdot Q.$$
Here we have defined $$v_Q(g)=\max_{k\in\mathbb{Z}}(g\in m_Q^k)$$ for any function $g$ defined on $C$. $m_Q$ is the ideal of every function $g$ defined on $C$ with $g(Q)=0$. I don't know where to start. Here I will write steps of the solution found by your help. We have $div(f)=div(y)+div(x^2-y).$ Let us start with $div(y)$. We need to find the points $P\in C$ with $v_P(y)\ne 0$. The points with $v_P(y)\ge 1$ satisfy $y=0$ and $x+y+x^4+y^3=0$, so $x(1+x^3)=0$. That are $(a,0)$ with $a\in\{0,-1,(-1)^{1/3},-(-1)^{2/3}\}$. But what we can say about the points with $v_P(y)\le -1$? There are no such points because $y$ has no poles. For example: Calculation of $v_{(-1,0)}(y)$: Clearly $y\in m_{(-1,0)}$. But we can't find two functions $g,h\in m_{(0,-1)}$ with $y=gh$ and $g(0,-1)=h(0,-1)=0$. So $y\notin m_{(0,-1)}^2$. So $v_{(0,-1)}(y)=1$. The same argument shows in general $v_{(a,0)}(y)=1$ for all $a$ like above. That means that we have $$div(y)=(0,0)+(-1,0)+((-1)^{1/3},0)+(-(-1)^{2/3},0)+z\cdot\infty.$$ Since a principle divisor has degree $0$ we can conclude $z=-4$. So we have found the divisor $div(y)$. Now: Calculate $div(x^2-y)$. There are no poles so we are only searching points with $v_P(x^2-y)\ge 1$. That means $x^2-y=0=x+y+x^4+y^3$. So $x^2=y$ and $x+x^2+x^4+x^6=0.$ And now I am in trouble.",['algebraic-geometry']
2082353,Saturated damped harmonic oscillator,"Consider a critically damped harmonic oscillator: $$x' = v$$
$$v' = -2v - x$$ Such a system has the property that if $x(0) < 0$ and $x(0)+v(0)\leq 0$, then $x(t) < 0$ for all $t$. Now suppose the system is modified so that the acceleration is subject to some minimum value, i.e. $v' \geq a_{min}$. Is there some way of maintaining the above property, i.e. $x(t) < 0$ for all $t$? More precisely, is it possible to do one or both of the following: Add additional constraints on the initial state ($x(0)$, $v(0))$ so that $v'(t) = -2v(t) - x(t) \geq a_{min}$ for all $t$. Add a term to the differential equations, i.e. $v' = -2v-x + g(x,v)$ so that $v'(t) = -2v-x + g(x,v) \geq a_{min}$, perhaps also under additional constraints on the initial state. In other words, I want to make a small tweak to the critically damped harmonic oscillator to take into account saturation of acceleration.","['control-theory', 'ordinary-differential-equations', 'linear-control']"
2082354,How to prove with Lagrange theorem,"How can I prove that: if $f: \Bbb R \rightarrow \Bbb R$ is convex and differentiable, such that $y = 0$ is an asymptote for $x \rightarrow \infty$, then $f(x) \ge 0, \forall x \in \Bbb R$ How can I prove that $f(x) \ge 0$ with the Lagrange theorem?","['derivatives', 'calculus']"
2082380,$\lim_{x\to 1^-} \sum_{k=0}^\infty \left( x^{k^2}-x^{(k+\alpha)^2}\right)$,"Has anybody seen (or can anybody come up with) a proof that 
 $$\lim_{x\to 1^-} \sum_{k=0}^\infty \left( x^{k^2}-x^{(k+\alpha)^2}\right) = \alpha$$ 
for all $\alpha > 0$? And also that 
 $$\lim_{\alpha\to 0^+} \left[ \lim_{x\to 1^-}\frac{\alpha -  \sum_{k=0}^\infty \left( x^{k^2}-x^{(k+\alpha)^2}\right)}{1-x}
\right]=\frac16$$ I thought I had a proof of the first limit but it was flawed. EDIT I had written the wrong expression for the second of the limits.  It is now corrected.","['lacunary-series', 'limits']"
2082385,Why are the integral curves of this vector field easy to find?,"I'm reading a book of differential geometry that states that it's easy to get the integral curves of this vector field: $$X(x,y)=(x^2-y^2,2xy)$$ But proceeding the way the book says, first I have to take that equation as $$\frac{dx}{dt}=x^2-y^2$$
$$\frac{dy}{dt}=2xy$$ But from that I don't know what more to do to proceed. I know that the integral curves must be circles of center $(0,t)$, and the vector field seems as this","['ordinary-differential-equations', 'differential-geometry']"
2082386,Fact about polynomials,"Suppose $Q_1,..., Q_n$ are polynomials in $x_1,x_2,...,x_{2n-1}, x_{2n}$ such that 
$$\tag{1}Q_1 x_1+Q_2 x_3+\cdots_+Q_n x_{2n-1}=0,\\
Q_1 x_2+Q_2 x_4+\cdots_+Q_n x_{2n}=0.$$
I wonder if it is always true that all $Q_1, \cdots, Q_{n}$ vanishes. It is easy to prove the case when $n=1$. So I want to prove it by using induction, but I am stuck. Edit: I can solve the case for $n=1,2$. For $n=1$, $(1)$ implies that $Q_1x_1=0$ and $Q_1x_2=0$, which implies that $Q_1=0$. For $n=2$, $(1)$ implies that 
$$Q_1x_1+Q_2x_3=0\mbox{ and }Q_1x_2+Q_2x_4=0.\tag{2}$$
As XTL pointed out, multiply the first equation by $x_2$ and multiply the second equation by $x_1$ and take the difference, we obtain 
$Q_2(x_2x_3-x_1x_4)=0$, which implies that $Q_2=0$. Put it back to $(2)$, we get $Q_1=0$.","['polynomials', 'calculus', 'functions', 'algebra-precalculus', 'linear-algebra']"
2082389,"Prove $\int_0^1 {3-\sqrt{5}x\over (1+\sqrt{5}x)^3} \, dx={1\over 2}$ using an alternative method","Prove that $$\int_0^1 {3-\sqrt{5}x\over (1+\sqrt{5}x)^3} \,dx={1\over 2}\tag1$$ My try: $u=1+\sqrt{5}x$ then $du=\sqrt{5} \, dx$ $${1\over \sqrt 5}\int_1^{1+\sqrt{5}}(4u^{-3}-u^{-2}) \, du$$ $$\left. {1\over \sqrt{5}}(-2u^{-2}+u^{-1}) \right|_1^{1+\sqrt{5}}={1\over 2}$$ Prove $(1)$ using an alternative method other than substitution method.","['integration', 'definite-integrals']"
2082432,Understanding Euler's Sums of Cubes Substitution:,"Distant relationship with this post . In Euler's proof for $A^3+B^3+C^3=D^3$ , he started with $$p(p^2+3q^2)=s(s^2+3r^2)\tag1$$ And made the substitution $\color{red}{p=ax+3by,q=bx-ay,s=3cy-dx,r=dy+cx}$ to get it into $$(a^2+3b^2)(x^2+3y^2)(ax+3by)=(d^2+3c^2)(x^2+3y^2)(3cy-dx)\tag2$$ $$\beta(ax+3by)=\gamma(3cy-dx)\tag3$$ With $\beta=a^2+3b^2,\gamma=3c^2+d^2$ . Euler even made another substitution . Starting with $(3)$ , substitute $\color{blue}{x=-3nb\beta+3nc\gamma,y=na\beta+nd\gamma}$ . Question: How did Euler know to substitute $p=ax+3by,q=bx-ay,s=3cy-dx,r=dy+cx$ and $x=-3nb\beta+3nc\gamma,y=na\beta+nd\gamma$ ? The book does say that Euler used the identity $$A^3+B^3+C^3-D^3=n^3(\gamma^3-\beta^3)(\lambda+\mu)(\lambda^2-\lambda\mu+\mu^2-3\beta\gamma)\tag4$$ But I'm not sure how this fits into the problem.","['algebra-precalculus', 'algebraic-geometry']"
2082433,"Find the biggest number $M$ such that the inequality $a^2+b^{1389} \ge Mab$ holds for every $a,b \in [0,1]$.","Find the biggest number $M$ such that the following inequality holds for every $a,b \in [0,1]$, $$a^2+b^{1389} \ge Mab$$ My attempt :We should find the minimum of $\frac{a}{b}+\frac{b^{1388}}{a}$.By putting $a=b=0^{+}$ we will get to $1 \ge M$ and clearly $M \ge 0$ but how much is $M$?","['inequality', 'a.m.-g.m.-inequality', 'optimization', 'multivariable-calculus', 'maxima-minima']"
2082441,How do I find all all the subgroups of a group?,"I'm failing to see the logic behind what makes up a subgroup. I understand the requirements of a subgroup (associativity, identity etc....) but I don't actually know how to find the subgroups. I think it has something to do with ""getting back to the identity"", but I may be wrong? I know that the identity is a subgroup and the whole group is a subgroup. That's all. Any help would be appreciated!","['finite-groups', 'group-theory', 'cyclic-groups']"
2082472,Solve the inequality $\sin(x)\cdot|\tan{x}|\le\frac{3}{2}$,"As in the title, solve the inequality $\sin(x)\cdot|\tan{x}|\le\frac{3}{2}$ for $x\in[0;2\pi]$. My concern is that I don't knwo how to get rid of the absolute value sign - should I consider separately cases of the $\tan{x}$ being positive and negative? Any hints greatly appreciated.","['inequality', 'trigonometry']"
2082488,Basis for the Quotient Topology,"I was studying some basic topology and was wondering about the basis for a quotient topology. I came up with the following, but I can't seem to find something this elementary stated explicitly anywhere, and this set of notes (link is 404, only available in Google cache) seems to suggest that a the (fix: Brian) basis is in general unclear. Have I made a mistake somewhere? Lemma : Let $q: X\rightarrow X/{\sim}$ be a quotient map, and let $\mathcal{B}$ be a basis for the original topology $\tau$ on $X$. Then $q(\mathcal{B})=\{q(B)\mid B\in\mathcal{B}\}$ is a basis for the quotient topology $\tau_q$ iff $q$ is an open map. Proof : Suppose $q(\mathcal{B})$ is a basis for $(X/{\sim}, \tau_q)$. Therefore, the image of each $B\in\mathcal{B}$ is open, and since every open $O\subseteq X$ is a union of the basis elements (and $q(\emptyset)=\emptyset$), $q(O)$ is open. Thus $q$ is an open map. Suppose $q$ is open. $q$ is surjective, so $q(\mathcal{B})$ covers $X/{\sim}$ since $\mathcal{B}$ covers $X$. Now, consider the intersection $O':=q(B)\cap q(B')$. It is open, since $q$ is open. Thus $O:=q^{-1}(q(B)\cap q(B'))$ is open as $q$ is continuous. Since $q$ is surjective, $O'=q(q^{-1}(O'))=q(O)=\bigcup_{\delta}q(B_{\delta})$ as $O$ is a union of some basis elements $B_{\delta}$. So for all $x\in O', x\in q(B_{\delta})\subseteq O'$ for some $\delta$. Thus $q(\mathcal{B})$ is a basis. By the same construction, any open set in $\tau_q$ is expressible as a union of basis elements, and conversely any union of basis elements is in $\tau_q$. Thus $q(\mathcal{B})$ is indeed a basis for $\tau_q$. $\blacksquare$ Edit: I just realized this certainly does not solve for the general case, derp. It still seems quite useful to not appear though. For instance, the typical question of the quotient of $\mathbb{R}^2$ by $(x,y)\sim(a,b)\Leftrightarrow x^2+y^2=a^2+b^2$ can be seen to be an open map, since every 'open box' in $\mathbb{R}^2$ maps to open intervals in $[0,\infty)$, so the intervals $[0, x)$ and $(a, b)$ with $a$ positive form a basis.","['general-topology', 'quotient-spaces']"
2082493,Using an integration technique:$ \int^b_a \left[f(x)+f^{-1}(x)\right]dx=b^2-a^2$,"I read about this integration technique on quora: If $a,b$ are fixed points of $f$, then $$ \int^b_a \left[f(x)+f^{-1}(x)\right]dx=b^2-a^2$$ Apparently it was used in the final of the 2013 MIT Integration bee but I can't find that question anywhere.. REPHRASING QUESTION : Could someone show me some integrals that can be cleverly solved with this technique?",['integration']
2082503,Different ways of solving $\sin x \cos y = -1/2$ and $\cos x \sin y = 1/2$,"I was solving this question which said $$ \sin x \cos y = -1/2$$
$$\cos x \sin y = 1/2$$ and we had to solve for $x$ and $y$. One thing that I deduced was that if I simply add the two equation then I would get $$\sin x\cos y  + \cos x\sin y=0$$
$$\Rightarrow \sin(x+y)=0$$
$$\Rightarrow x+y= n\pi$$ Now I can substitute either $x$ or $y$ back in the equation and can easily find the answer. This is one single way of solving the equation. Can there be some more intuitive ways of reaching the answer? This is not a kind of doubt, but I just want to enhance my horizons regarding different approaches to a single problem.",['trigonometry']
2082515,Finding the subspace and basis of a set that is made out of matrices,"I've tried to solve the next problem, but I'm not quite sure about the 'a' part and don't know how to start with 'b' Any improvements/clarifications/help is appreciated The question: Let $J$ be a $3*3$ matrix given by: \begin{bmatrix}0&1&0\\0&0&1\\0 &0&0\end{bmatrix} (a) Is the following set a subspace? $S=[A∈ℝ^{3*3}|AJ=JA]$ My attempt S is nonempty since it contains the $0$ matrix, $0*J=0=J*0$ For $A∈S$ and $\alpha$ a real  number scalar: $(\alpha A)J=\alpha (AJ)=\alpha(JA)=J(\alpha A)$ For $A∈S$, $B∈S$: $(A+B)J=AJ+BJ=JA+BA=J(A+B)$ So both closure properties are satisfied meaning S is a subspace of $ℝ^{3*3}$ (b) Find a basis for S and determine its dimension Well I don't really get the question, should I find the basis of the matrix J? or something different? I know that if the basis consists of $n$ vectors than S has dimension $n$
But how to get to this basis/these vectors is my main struggle Thanks in advance :)","['matrices', 'linear-algebra']"
2082595,Sufficient condition for inflection point,"I am a bit confused about the following statement. Let $f$ be a real function of a real variable and $x_0 \in \mathbb{R}$. If $f$ is differentiable in a neighborhood $I$ of $x_0$, $f'(x_0) = 0$ and $f'(x_0) > 0$ for all $x \in I \setminus \{x_0\}$, then $x_0$ is an inflection point. Is this statement true? Reading this question: Prove that $f$ has an inflection point at zero if $f$ is a function that satisfies a given set of hypotheses ,
I think my statement is false, but I cannot give a counterexample. Can anyone help me out? EDIT. (Thanks to @Zestylemonzi's comment) My definition of inflection point is as follows: $x_0$ is an inflection point if $f$ is differentiable at $x_0$ and there exists a neighborhood $J$ of $x_0$ such that the function $d(x) = f(x) − f(x_0) − f'(x_0) (x − x_0)$ has the same sign as $x−x_0$ for all $x \in J \setminus \{ x_0 \}$, or $d(x) = f(x) − f(x_0) − f'(x_0) (x − x_0)$ and $x−x_0$ have opposite signs for all $x \in J \setminus \{ x_0 \}$ I think that according to this definition my statement is true. I am aware of another possible definition of inflection point : $x_0$ is an inflection point if $f$ is differentiable at $x_0$ and there exists $\delta > 0$ such that $f$ is convex (respectively, concave) for $x \in (x_0 - \delta, x_0)$ and concave (respectively, convex) for $x \in (x_0, x_0 + \delta)$. Is my statement true according to the second definition?","['derivatives', 'real-analysis', 'examples-counterexamples', 'calculus']"
2082602,Limit problem involving bijective function,"Let $f: \Bbb N^{\star}\to \Bbb N^{\star}$ a bijective functions such that exists $$\lim _{n \to \infty} {\frac {f(n)} {n}}.$$ Find the value of this limit. I noticed that, if $f(n)=n$, then the limit is $1$. I couldn't make more progress. Can you help me?","['functional-analysis', 'limits']"
2082636,"What does a number look like that's in $\mathbb{Z}\left[\sqrt{14}, \frac{1}{2}\right]$ that's not in $\mathbb{Z}[\sqrt{14}]$?","I know the very basic basics of $\mathbb{Z}[\sqrt{14}]$. Numbers in it are of the form $a + b \sqrt{14}$, with $a, b \in \mathbb{Z}$. Numbers like $-3 + \sqrt{14}$ and $7 - 8 \sqrt{14}$. The norm function is $N(a + b \sqrt{14}) = a^2 - 14b^2$, which I'm told is not an Euclidean function even after the absolute value adjustment. Proposition $4.11$ in this paper by Franz Lemmermeyer http://www.rzuser.uni-heidelberg.de/~hb3/publ/survey.pdf mentions $\mathbb{Z}\left[\sqrt{14}, \frac{1}{2}\right]$. I don't think I've ever read about a domain like that before, except perhaps in a very general way that the specifics eluded me. I'm guessing $\mathbb{Z}\left[\sqrt{14}, \frac{1}{2}\right]$ contains all the same numbers of $\mathbb{Z}[\sqrt{14}]$ as well as some other numbers. What's the form of those other numbers? What are some concrete examples of those other numbers? P.S. Proposition $4.11$ is on page $14$ of $56$ of the PDF. Looks like a brilliant survey. I need to print it out and sit down to read it beginning to end.","['abstract-algebra', 'ring-theory']"
2082660,"Ratio of Legs in 15, 75, 90 triangles","$\text{What is the ratio of legs in a right triangle with angles of 15, 75, and 90?}$ I know the ratio of legs in a $30, 60, 90$ triangle, which is the lengths $1$, $\sqrt{3}$, and $2$ respectively. This is what I have got so far: Using the 30-60-90 Ratio How would I be able to take this a step further and be able to find the answer? Thanks in advance.","['triangles', 'geometry']"
2082670,"If $|f|+|g|$ is constant on $D,$ prove that holomorphic functions $f,~g$ are constant on $D$.","Let $D\subseteq \mathbb{C}$ be open and connected and 
  $f,~g:D \rightarrow \mathbb{C}$ holomorphic functions such that 
  $|f|+|g|$ is constant on $D.$ Prove that $f,~g$ are constant on $D.$ Attempt. I noticed first that this problem is already set before, and is considered  to be duplicate ( If $|f|+|g|$ is constant then each of $f, g$ is constant ) and we are sent directly to problem sum of holomorphic functions . However, the last reference deals with the sum $|f|^2+|g|^2$, which I do not see how is connected to the sum $|f|+|g|$, as stated in our title (for example, the equality $|f|^2+|g|^2=(|f|+|g|)^2-2|f||g|$ is not helpful here). Thanks in advance!","['complex-analysis', 'holomorphic-functions', 'complex-numbers']"
2082717,Divisibility involving the product (ad+bc)(ac+bd),"Prove that if $abcd|(ad+bc)(ac+bd)$, then $\frac{ac}{bd}$ is a perfect square of a rational number. $a,b,c,d$ are positive integers. I am extremely lost on how to do this problem; any help?","['square-numbers', 'number-theory', 'rational-numbers', 'integers', 'elementary-number-theory']"
2082746,Calculus: Sketching a Graph that satisfies the following conditions (I),"I have to sketch a graph that satisfies the conditions: A. $f(2)=f(4)=0$ B. $f'(x)\lt0$ if $x\lt3$ C. $f'(3)$ does not exist D. $f'(x) \gt 0$ if $x \gt 3$ E. $f''(x) \lt 0$,$x\ne 3$ I am a bit stuck on how to tell if the second derivative is always negative from a graph?  I know that there is a sharp turn at $x=3$, and there is also a minimum there but the second derivative part trips me up.","['derivatives', 'calculus']"
2082787,Integration by parts with limits,"$$ \int{u\frac{dv}{dx}} \equiv uv -\int{v\frac{du}{dx}}$$
I prefer to write as: $ \int{uv} \equiv u\int{v} -\int{(\int{v})\frac{du}{dx}}$ I get how this works, $\int{x\ln{x} = \ln{x}\cdot\frac{x^2}{2} -\frac{x^2}{4}}$ However, when limits are applied, I don't seem to be able to get the right answer, doing: $$\int_1^2{x\ln{x}} = \ln{x}\bigg[\frac{x^2}{2}\bigg]^2_1-\int^2_1{\bigg(\bigg[\frac{x^2}{2}\bigg]^2_1\frac{1}{x})} =
\\ \int_1^2{x\ln{x}} = \bigg[\frac{\ln{x}\cdot x^2}{2}\bigg]^2_1-\int^2_1{\bigg(\bigg[\frac{x}{2}\bigg]^2_1\bigg)} = 
\\ \ln{4} - \cdots$$ How is the second part resolved? $- \int^2_1{\frac12}$?? $= - [\frac x2]^2_1 =  -\frac12; \ln{4} - \frac12$ isn't the right answer.","['integration', 'definite-integrals']"
2082817,The sum of digits in a 2-digit number,"The sum of digits in a two digit number formed by the two digits from $1$ to $9$ is $8$. If $9$ is added to the number then both the digits become equal. Find the number. My attempt: Let the two digit number be $10x+y$ where, $x$ is a digit at tens place and $y$ is the digit at unit's place. According to question: $$x+y=8$$ I could not figure out the other condition. Please help. Thanks in advance.",['algebra-precalculus']
2082836,Prove that $(2+ \sqrt5)^{\frac13} + (2- \sqrt5)^{\frac13}$ is an integer [duplicate],"This question already has answers here : Calculate simple expression: $\sqrt[3]{2 + \sqrt{5}} + \sqrt[3]{2 - \sqrt{5}}$ [duplicate] (4 answers) Closed 7 years ago . When checked in calculator it is 1. But how to prove it?
Also it is not a normal addition like $x+ \frac1x$ which needs direct rationalization. So I just need something to proceed.",['number-theory']
2082845,Show a scheme is integral if and only if it is irreducible and reduced.,"I'm trying to solve Exercise 5.2.F from Vakil's notes: Show a scheme $X$ is integral if and only if it is irreducible and reduced. Where we say $X$ is reduced (integral) if $\mathscr O_X(U)$ is reduced (an integral domain) for all open subsets $U$ of $X$. Clearly, if $X$ is integral, then each $\mathscr{O}_X(U)$ is a domain, hence reduced, so $X$ is reduced. I'm not sure how to see that $X$ is irreducible. It's obvious for an affine scheme, since if $\mathscr{O}_X(X)=:A$ is a domain then $\text{Spec } A$ is irreducible. Am I able to use this to tackle the general case? Like if $X=\cup_i U_i$ with each $U_i$ affine open, does each $U_i$ being irreducible imply that $X$ is irreducible? This doesn't seem like the right way to approach it. I'm not sure, I just feel stuck. Any hints would be greatly appreciated (I'd prefer that over someone just giving me the answer).","['schemes', 'algebraic-geometry']"
2082856,Combinations problem involving a standard pack of $52$ playing cards and a $4$ sided die: Part 1,"Consider a standard pack of $52$ playing cards. The cards are distributed into $4$ piles completely randomly by tossing a four-sided die for each card. How many possible arrangements are there for distributing the
  cards among the four piles? My attempt goes as follows: $$\frac{52!}{(52-13)!\cdot13!\cdot2!}\approx 3.18\times 10^{11}$$
My reasoning for calculating it this way is because we are choosing $13$ cards from a selection of $52$ cards but there are $2$ different colors so I divide by these repeats. The correct answer is $2.03\times 10^{31}$
or in exact form, it is $20282409603651670423947251286016$. I am $20$ orders of magnitude away from the correct answer which shows that combinatorics is not my strong point. Could anyone please give me any hints or tips on how to reach the correct answer? Edit: Answers below indicate there is ambiguity in the word ""arrangements"" shown in the quote (the ordering of each pile may or may not matter). In which case I can only apologize for this ambiguity but I am simply quoting the professors question word for word .","['algebra-precalculus', 'combinatorics', 'intuition', 'combinations']"
2082882,"Maximal ideal of $\mathbb{Q}[x_1,...,x_n]$ is contained in only finitely maximal ideals of $\mathbb{C}[x_1,...,x_n]$","Suppose $M\subset \mathbb{Q}[x_1,...,x_n]$ is maximal. I'd like to show that as a subset of $R= \mathbb{C}[x_1,...,x_n]$, $M$ is contained in only finitely many maximal ideals. By Hilbert-Nullstellensatz, I know max ideals of $R$ are of the form $(x_1-a_1,...,x_n-a_n)$, so perhaps if $M$ were contained infinitely many of them, one could obtain a contradiction? I'm not quite sure where to go with this.","['abstract-algebra', 'ring-theory', 'commutative-algebra']"
2082924,Proof that the binomial transform is involution,"Let $\{x_n\}$, $n=0,1,2,\ldots$, be a sequence, and let $\{y_n\}$, $n=0,1,2,\ldots$, be its binomial transform, that is,
$$
y_n=\sum_{k=0}^{n} (-1)^k {n\choose k} x_k.
$$
I need to prove that the binomial transform is an involution, that is,
$$
x_n=\sum_{k=0}^{n} (-1)^k {n\choose k} y_k.
$$ I tried to use the combinatorial Vandermonde's identity but I failed. Please help me to prove.",['combinatorics']
2082942,Algebraic dimension of the projective space,"This is the exercise 2.1.3 from Huybrechts's ""Complex Geometry - An Introduction"". Determine the algebraic dimension of the following manifolds: $\mathbb{P}^1$, $\mathbb{P}^n$, and the complex torus $\mathbb{C}/(\mathbb{Z}+i\mathbb{Z})$. For the latter, you might need to recall some basic facts on the Weierstrass $\wp$-function. How big is the function field of $\mathbb{C}$? The algebraic dimension of $X$ is defined to be the transcendence degree of the function field $K(X)$, the field of meromorphic functions on $X$. And there is a propostion 2.1.9 (Siegel) states that the algebraic dimension of a compact connected manifold is not greater than the geometric (complex) dimension. My attempt: For $\mathbb{P}^1$ this is trivial: the algebraic dimension is $1$ since there are non-trivial meromorphic functions (e.g. the identity map is holomorphic) on the Riemann sphere, so the dimension is greater than $0$. The function field of $\mathbb{C}$ should have an infinite algebraic dimension since the functions $f(z)=z^k, k=1, 2, \cdots$ are holomorphic. Question: how about $\mathbb{P}^n$ (the projective space) and the complex torus? Note that the RR theorem are not studied yet so I'm seeking a solution without it.","['complex-geometry', 'algebraic-geometry']"
2082950,Is there any double series that cannot exchange the sum?,"I want to find a sequence $(u_{n,p})_{(n,p)\in\mathbb{N}^2}$ that satisfied:
$$
\sum_{n=0}^{\infty}\sum_{p=0}^{\infty}u_{n,p} ~\text{is convergent}
$$
$$
\sum_{p=0}^{\infty}\sum_{n=0}^{\infty}u_{n,p} ~\text{is convergent too}
$$
but
$$
\sum_{p=0}^{\infty}\sum_{n=0}^{\infty}u_{n,p} \neq \sum_{n=0}^{\infty}\sum_{p=0}^{\infty}u_{n,p}
$$",['sequences-and-series']
2082952,"What does $\frac{x^3}{9}\bigg|_0^1$ mean, and how should it be spoken?","$$\frac{x^3}{9}\Bigg|_0^1$$
The vertical line above: what does it mean, and how would I state this whole structure in spoken words, so that a screen reader would be able to read it aloud correctly?","['algebra-precalculus', 'integration', 'notation', 'calculus']"
2082966,Can a circle be divided into congruent shapes with one not sharing a point with circumferences?,"Just wondering out of curiosity. For instance, this does not qualify: Having been trying for a while, I would appreciate if someone can give a proof that it is impossible (in case it is). Thanks in advance :).","['combinatorics', 'geometry']"
2082968,Necessary and sufficient condition for a periodic system to have a non-trivial periodic solution.,"Consider the linear periodic system in $\mathbb{R}^n$
\begin{equation} \begin{cases}
\dot{x}(t) & = A(t)x(t),\\
x(0) & = x_0,
\end{cases} \label{eq:Floquet} \tag{1}
\end{equation}
where $A(t)$ is a real $n\times n$ matrix function which is smooth in $t$ and periodic of period $T>0$. Floquet theory states that there exists at least 1 non-trivial solution $\chi(t)$ satisfying
\begin{equation}
\chi(t+T) = \mu\chi(t), \ \ t\in(-\infty, \infty), \label{eq:Floquet2} \tag{2}
\end{equation}
where $\mu$ is an eigenvalue of the Floquet matrix. $\mu$ is more well-known as a Floquet multiplier of the system. What is the necessary and sufficient conditions so that \eqref{eq:Floquet} has a non-trivial $T$-periodic solution? By non-trivial I meant a periodic solution with minimal period $T$. It seems like according to \eqref{eq:Floquet2}, one would want to impose condition on the Floquet matrix such that it has eigenvalue $\mu=1$, but I know nothing about this, not to mention that this sounds like a very strong condition. Spefically, I am looking for conditions that stem from Floquet theory.","['ordinary-differential-equations', 'dynamical-systems']"
2082969,Probability event with $70$% success rate occurs three consecutive times for sample size $n$,"It has been a long time since I've done probability so I am not sure which to do (if either are correct). Thank you for taking the time to look at my work. Probability an event occurs is $70$%.
I'm looking for the probability our event occurs three times in a row for sample size $n$. $(.7)^3=.343$ is the probability to occur three consecutive times $1-.343=.657$ would be the chance to fail. First idea: For $n=3$ our success rate is $.343$ $n=4$ we have two opportunities for success, thus $1-(.657)^2=.568351$ $n=5$, three opportunities for success, thus $1-(.657)^3=.71640$... Generalization: Probability for success: $$1 - (.657)^{n-2}$$ Second idea: Probability when $n=3$ would be $(.7)^3$ At $n=4$ we'd have $(.7)^3+(.3)(.7)^3$ For $n=5$ we'd have $(.7)^3+(.3)(.7)^3+(.3)^2(.7)^3+(.3)(.7)^4$ I'm leaning towards the second idea...but I'm failing to see a generalization for it. Please excuse my LaTeX it has been a long time since I've asked/answered any questions. Thank you.","['combinatorics', 'probability']"
2082980,Relation between inverse tangent and inverse secant,"I've been working on the following integral
$$\int\frac{\sqrt{x^2-9}}{x^3}\,dx,$$
where the assumption is that $x\ge3$. I used the trigonometric substitution $x=3\sec\theta$,which means that $0\le\theta<\pi/2$. Then, $dx=3\sec\theta\tan\theta\,dx$, and after a large number of steps I achieved the correct answer:
$$\int\frac{\sqrt{x^2-9}}{x^3}\,dx=\frac16\sec^{-1}\frac{x}{3}-\frac{\sqrt{x^2-9}}{2x^2}+C$$ I was able to check my answer using Mathematica. expr = D[1/6 ArcSec[x/3] - Sqrt[x^2 - 9]/(2 x^2), x];
Assuming[x >= 3, FullSimplify[expr]] Which returned the correct response: Sqrt[-9 + x^2]/x^3 Mathematica returns the following answer: Integrate[Sqrt[x^2 - 9]/x^3, x, Assumptions -> x >= 3] -(Sqrt[-9 + x^2]/(2 x^2)) - 1/6 ArcTan[3/Sqrt[-9 + x^2]] Which I can write to make more clear. $$-\frac16\tan^{-1}\frac{3}{\sqrt{x^2-9}}-\frac{\sqrt{x^2-9}}{2x^2}+D$$ Now, you can see that part of my answer is there, but here is my question. How can I show that
$$\frac16\sec^{-1}\frac{x}{3}\qquad\text{is equal to}\qquad -\frac16\tan^{-1}\frac{3}{\sqrt{x^2-9}}$$
plus some arbitrary constant? What identities can I use? Also, can anyone share the best web page for inverse trig identities? Update: I'd like to thank everyone for their help. The Trivial Solution's suggestion gave me:
$$\theta=\sec^{-1}\frac{x}{3}=\tan^{-1}\frac{\sqrt{x^2-9}}{3}$$
Then the following identity came to mind:
$$\tan^{-1}x+\tan^{-1}\frac1x=\frac{\pi}{2}$$
So I could write:
\begin{align*}
\frac16\sec^{-1}\frac{x}{3}-\frac{\sqrt{x^2-9}}{2x^2}
&=\frac16\tan^{-1}\frac{\sqrt{x^2-9}}{3}-\frac{\sqrt{x^2-9}}{2x^2}\\
&=\frac16\left(\frac{\pi}{2}-\tan^{-1}\frac{3}{\sqrt{x^2-9}}\right)-\frac{\sqrt{x^2-9}}{2x^2}\\
&=\frac{\pi}{12}-\frac16\tan^{-1}\frac{3}{\sqrt{x^2-9}}-\frac{\sqrt{x^2-9}}{2x^2}
\end{align*}
Using Olivier's and Miko's thoughts, I produced this plot in Mathematica. Plot[{1/6 ArcSec[x/3] - Sqrt[x^2 - 9]/(
   2 x^2), -(1/6) ArcTan[3/Sqrt[x^2 - 9]] - Sqrt[x^2 - 9]/(
   2 x^2)}, {x, -6, 6},
 Ticks -> {Automatic, {-\[Pi]/12, \[Pi]/12}}] Which shows that the two answers differ by $\pi/12$, but only for $x>3$.","['trigonometry', 'calculus']"
2082986,How to show $\sum\limits_{r=0}^n \frac{1}{r!} \lt\left (1 + \frac{1}{n}\right)^{n+1}$ for all $n \ge 1$?,"Using the binomial expansion, it is quite is easy to show that $$\left(1+\frac{1}{n}\right)^n \le \sum_{r=0}^{n} \frac{1}{r!} $$ for all $n\in\mathbb{Z^+}$, with equality holds when $n=1.$ (Can it be proved by the mathematical induction?) But it seemes to me really difficult to prove that $$\sum_{r=0}^{n} \frac{1}{r!}\lt \left(1+\frac{1}{n}\right)^{n+1}$$ for all $n\in\mathbb{Z^+}$. Can anyone prove it? As it can be proved that the sequences $\{(1+\frac{1}{n})^n\}$ and $\{(1+\frac{1}{n})^{n+1}\}$ converge to the same limit, the above inequalities will help establish the equivalence between the following definitions of e :
$$ \begin{align} e &= \lim_{n\to \infty}\left(1+\frac{1}{n}\right)^n\\ e &= \sum_{r=0}^{\infty} \frac{1}{r!} \end{align} $$","['inequality', 'binomial-theorem', 'algebra-precalculus', 'factorial', 'summation']"
2082992,"How to prove ${}_{2}F_{1}\left(\frac{1}{3},\frac{2}{3};\frac{3}{2}; \frac{27}{4}z^2(1-z)\right) = \frac{1}{z}$","With reference to the following post On $\int_0^1\arctan\,_6F_5\left(\frac17,\frac27,\frac37,\frac47,\frac57,\frac67;\,\frac26,\frac36,\frac46,\frac56,\frac76;\frac{n}{6^6}\,x\right)\,dx$ I used $${}_{k}F_{k-1}\left(\frac{1}{k+1} ,\cdots ,\frac{k}{k+1};\frac{2}{k}
 \cdots ,\frac{k-1}{k},\frac{k+1}{k};\left(
 \frac{z(1-z^k)}{f_k}\right)^k \right) = \frac{1}{1-z^k}$$ Where $$f_k \equiv \frac{k}{(1+k)^{1+1/k}}$$ This formula seems to be too complicated for the general case. Let us look at the easiest case $k=2$ then we have $${}_{2}F_{1}\left(\frac{1}{3},\frac{2}{3};\frac{3}{2}; \frac{27}{4}z^2(1-z^2)^2\right) = \frac{1}{1-z^2} $$ This is equivalent to proving
$${}_{2}F_{1}\left(\frac{1}{3},\frac{2}{3};\frac{3}{2}; \frac{27}{4}z^2(1-z)\right) = \frac{1}{z}\tag{1}$$ For some ranges of $z$. Question: Any idea how to prove $(1)$ ?","['number-theory', 'hypergeometric-function', 'integration', 'summation', 'sequences-and-series']"
2083016,weak convergence of discrete uniform to lebesgue measure using characteristic functions,"Here is the question. Suppose we have a sequence of probability measures $\{\mu_n\}$ defined
  \begin{align*}
    \mu_n(\{x\}) = \frac{1}{n}
\end{align*}
  for $x = 0,\frac{1}{n}, \frac{2}{n},...,\frac{n-1}{n}$. Show that $\mu_n \Rightarrow \lambda$ where $\lambda$ is the Lebesgue measure on $[0,1]$. By the continuity theorem we have the equivalence between weak convergence and pointwise convergence of characteristic functions. So it suffices to show that the characteristic functions $\{\phi_n\}$ of the sequence of measures $\{\mu_n\}$ converges pointwise to the characteristic function of $\lambda$. That is, I want to show that for any $t \in [0,1]$,
\begin{align*}
    \phi_n(t) = \int e^{itx} \mu_n(dx) \to \int e^{itx} \lambda(dx) = \phi(t)
\end{align*}
First I computed the characteristic function of $\mu_n$, 
\begin{align*}
    \phi_n(t) = \int e^{itx} \mu_n (dx) = \sum_{k=0}^{n-1} \frac{1}{n} e^{it\frac{k}{n}}
\end{align*}
And the characteristic function of $\lambda$,
\begin{align*}
    \phi(t) = \int e^{itx} \lambda(dx) = \int_0^1 e^{itx} dx = \frac{e^{it} - 1}{it}
\end{align*}
I am stuck here. I can't show that
\begin{align*}
    \lim_n \sum_{k=0}^{n-1} \frac{1}{n} e^{it\frac{k}{n}} = \frac{e^{it} - 1}{it}
\end{align*}
but I computed some values in Matlab and I'm fairly certain its true. How can I evaluate this limit / show it is equal to the expression on the right?","['characteristic-functions', 'probability-theory', 'measure-theory']"
2083134,Simple applications of Lie algebra in group theory,"In his book Lie Algebra, Jacobson gives a motivation for Lie algebra as a tool used in a difficult problem in group theory - Burnside's problem. I was wondering if there is any simple/elementary application of Lie algebra in group theory which can be illustrated in class in the beginnning/introduction while teaching the course on Lie algebra.","['applications', 'group-theory', 'lie-algebras']"
2083143,"Rudin theorem 1.17, understanding the monotonicity of the function sequences defined.","I'm going through the proof of theorem 1.17 of Rudin's Real and Complex Analysis . Theorem 1.17. Let $f: X \to [0,\infty]$ be measurable. There exist simple measurable functions $s_n$ on $X$ such that $0 \leq s_1 \leq s_2 \leq \dots \leq f$, $s_n(x) \to f(x)$ as $n \to \infty$, for every $x \in X$. The proof starts by defining $\delta_n = 2^{-n}$, for each $n$ and each real number $t$ there's an integer $k_n(t)$ such that $$
k_n(t)\delta_n \leq t < (k_n(t)+1)\delta_n,
$$ this point is easily proved considering that the equation $$t = \mu \delta_n$$ has solution and taking $k_n(t) = \left\lfloor \mu \right\rfloor$ proves the statement above. Later the sequence $$\varphi_n(t) = \left\{ \begin{array}{lr} k_n(t)\delta_n & 0 \leq t < n \\ n & n \leq t \leq \infty \end{array} \right.$$ is defined. For each $n \;\;\varphi_n$ is a Borel function on $[0,\infty]$, but why? Is that because for each $n$ we have $\varphi_n$ is a simple function? Then it is stated that
$$
t - \delta_n < \varphi_n(t) \leq t\;\; 0 \leq t \leq n
$$
And this bit it is easy to prove by using the definition of $k_n(t)$. Almost finally it is stated that $$
0 \leq \varphi_1 \leq \varphi_2 \leq \ldots \leq t
$$ this bit puzzles me since $$
\begin{multline}
\left\{
\begin{array}{l}
t - \delta_n < \varphi_n(t) \leq t \\
t - \delta_{n-1} < \varphi_{n-1}(t) \leq t
\end{array} \Rightarrow
\right.
\left\{
\begin{array}{l}
t - \delta_n < \varphi_n(t) \leq t \\
- t \leq - \varphi_{n-1}(t) < - t + \delta_{n-1}
\end{array} 
\right. \Rightarrow \\
-\delta_n \leq \varphi_n(t) - \varphi_{n-1}(t) \leq \delta_{n-1}
\end{multline}
$$
and it doesn't tell me anything... And finally it is just stated that defining $s_n = \varphi_n \circ f$ has the required properties. My questions: Why is the sequence $\varphi_n$ measurable (borel function in this case)? Why is the sequence $\varphi_n$ monotonic? Why is the sequence $s_n$ monotonic? Update: Maybe I figured out 1 and 2, Since $\varphi_n$ is monotonic the counter image of any open set should be a Borelian set (union of open sets). Taking the difference
$$
(\varphi_{n+1} - \varphi_n)(t) = \left\{ \begin{array}{lr}
k_{n+1}(t) \delta_{n+1} - k_n(t) \delta_n & 0 \leq t < n \\
k_{n+1}(t) \delta_{n+1} - n & n \leq t < n + 1 \\
1 & n + 1 \leq t < \infty
\end{array}
\right.
$$ For $0 \leq t < n$ we have $$
\begin{multline}
k_{n+1}(t) \delta_{n+1} - k_n(t) \delta_n = k_{n+1}(t) \delta_{n+1} - 2 k_n(t) \delta_{n + 1} = (k_{n+1}(t) - 2k_n(t))\delta_{n+1} = 0
\end{multline}
$$ The equality to $0$ follows from the fact that it must be $k_{n+1}(t) = 2k_n(t)$
Given the uniqueness of the integer the multiplied by $\delta_j$ bound $t$. For $n \leq t < n + 1$ we have $$
(n\delta^{-1}_n) \delta_n = n \leq t < n + 1 = ((n+1)\delta^{-1}_n) \delta_n \Rightarrow k_{n}(t) = n2^n = n \delta^{-1}_n \Rightarrow k_{n}(t)\delta_n = n \Rightarrow k_{n+1}(t) \delta_{n+1} - n = n + 1 - n = 1
$$ I can then rewrite $$
(\varphi_{n+1} - \varphi_n)(t) = \left\{ \begin{array}{lr}
0 & 0 \leq t < n \\
1 & n \leq t < \infty
\end{array}
\right. \Rightarrow 0 \leq (\varphi_{n+1} - \varphi_n)(t) \Rightarrow \varphi_n \leq \varphi_{n+1}
$$ I keep trying to figure out why the sequence $s_n$ is monotonic.","['real-analysis', 'measure-theory', 'proof-explanation']"
2083169,Topology on ring of endomorphisms,"Let $R$ be a topological ring, let $M$ be a topological commutative group, and let $R\rightarrow \operatorname{End}(M)$ be a ring homomorphism giving $M$ the structure of a left $R$-module (throughout, $\operatorname{End}$ will refer to endomorphisms in the category of topological groups). The objective is to find a topology on $\operatorname{End}(M)$ that has the property that $R\rightarrow \operatorname{End}(M)$ is continuous iff the map $R\times M\rightarrow M$ is continuous. My first attempt was: Declare a net $\lambda \mapsto T_{\lambda}\in \operatorname{End}(M)$ to converge to $T\in \operatorname{End}(M)$ iff for every net $\mu \mapsto v_{\mu}\in V$ converging to $v\in V$, it follows that the net $(\lambda ,\mu )\mapsto T_{\lambda}(v_{\mu})$ converges to $T(v)$. If this defines a topology, then I believe the topology will have the desired property and even make $\operatorname{End}(M)$ into a topological ring itself.  The question is: Does this definition of convergence define a topology?  If not, what is a counter-example?  If not this topology, is there any such topology?  If so, what is it?","['modules', 'topological-groups', 'ring-theory', 'general-topology', 'topological-vector-spaces']"
2083209,Adjoint differential equations,"Consider the vector differential equations
\begin{equation}
\mathbf{x}^{\prime}=\mathbf{A}(t)\cdot\mathbf{x}\tag{1}
\end{equation}
and
\begin{equation}
\mathbf{y}^{\prime}=-\mathbf{A}^{\ast}(t)\cdot\mathbf{y},\tag{2}
\end{equation}
where $\mathbf{A}^{\ast}$ is the complex conjugate transpose of $\mathbf{A}$ and $\mathbf{x},\mathbf{y}$ are column vectors. It is well-known that (1) and (2) are said to be adjoint to one another.
Further, we know that if $\mathbf{x}$ and $\mathbf{y}$ are solutions of (1) and (2), respectively, then
\begin{equation}
\mathbf{y}^{\ast}\cdot\mathbf{x}=\text{constant}.\notag
\end{equation} Now, consider the higher-order (scalar) differential equations
\begin{equation}
\sum_{i=1}^{n}p_{i}(t)x^{(i)}(t)=0,\tag{3}
\end{equation}
where $p_{n}(t)\neq0$, and
\begin{equation}
\sum_{i=1}^{n}(-1)^{(i)}[p_{i}y]^{(i)}(t)=0.\tag{4}
\end{equation}
Also, (3) and (4) are said to be adjoint to one another.
Further, if $x$ and $y$ are solutions of (3) and (4), respectively, then (see [1, (8.17) on pp. 67])
\begin{equation}
\sum_{i=0}^{n}\sum_{j=0}^{i-1}(-1)^{j}x^{(i-j-1)}(t)[p_{i}z]^{(j)}(t)=\text{constant}.\label{hmfeq}\tag{*}
\end{equation} The inner sum in \eqref{hmfeq} resembles the matrix multiplication formula.
So, recognizing the similarities between systems and scalar equations, is it possible to obtain the result for higher-order equations by transforming them into vector equations? I could not establish any bridge here. I am experiencing problems in transforming (4) into a useful matrix representation. References [1]. P. Hartman, Ordinary Differential Equations , SIAM, 2002.","['ordinary-differential-equations', 'adjoint-operators']"
2083216,Finite subgroups and $H^2 = H$,"Let $G$ be a group, and $H$ a subset of $G$. If $H$ is a subgroup of $G$, then $H^2 = H$. I'm interested in what holds if $H^2 = H$.
It's easy to verify that $H$ is a semigroup. If $H$ is finite, $H$ is also a monoid: Assume $e \notin H$. Fix any $h \in H$. Then $h \notin hH$. Since $hH \subseteq H^2 = H$, we have $|hH| < |H|$, a contradiction. Thus $e \in H$. If $H$ is infinite, then $H$ needn't be a monoid: $(\mathbb{Q}, +)$ is a group, and $(\mathbb{Q}^+, +)$ isn't a monoid, even though $\mathbb{Q}^+ + \mathbb{Q}^+ = \mathbb{Q}^+$. But what if $H$ is finite; is $H$ perhaps a group? Thank you!","['finite-groups', 'abstract-algebra', 'group-theory']"
2083217,The sum of series with natural logarithm: $\sum_{n=1}^\infty \ln\left(\frac{n(n+2)}{(n+1)^2}\right)$ [duplicate],"This question already has answers here : Prove that $\sum_{n=1}^\infty \ln\left(\frac{n(n+2)}{(n+1)^2}\right)$ converges and find its sum (5 answers) Closed 7 years ago . Calculate the sum of series: $$\sum_{n=1}^\infty \ln\left(\frac{n(n+2)}{(n+1)^2}\right)$$ I tried to spread this logarithm, but I'm not seeing any method for this exercise.","['logarithms', 'telescopic-series', 'sequences-and-series']"
2083238,A querry regarding inverse trig. functions,"Find the value of $\cos^{-1}\left({{-\sqrt 3}\over 2}\right)$ Let $\cos^{-1}\left({{-\sqrt 3}\over 2}\right) = y$ $\Rightarrow$ $\cos y = {{-\sqrt3}\over 2}$ I) $\cos y=\cos\left({-\pi\over6}\right)$ $\cos y=\cos\left(\pi - {\pi\over6}\right)$ $\Rightarrow$ $\cos y=\cos\left({5\pi\over6}\right)$ $\Rightarrow$ $y={5\pi\over6}$ $\epsilon$ $\left[0,\pi\right]$ II) $\cos y = {{-\sqrt3}\over 2}$ $\Rightarrow$ $\cos y = \cos\left({-\pi\over6}\right)$ Since, $\cos(-x) = \cos(x)$ $\Rightarrow$ $\cos y = \cos\left({\pi\over6}\right)$ $\Rightarrow$ $y={\pi\over6}$ $\epsilon$ $\left[0,\pi\right]$ The I) is my textbok solution, and II) is what i came up with. Is my answer (II) correct?","['trigonometry', 'inverse']"
2083255,A tricky integral inequality,"A friend has submitted this problem to me: Let $0<a<b<1$ and $f:[0,1]\to \mathbb R$ be a differentiable function such that $$\displaystyle \frac{\int_0^a f(x) dx}{a(1-a)}+\frac{\int_b^1 f(x) dx}{b(1-b)}=0$$ Prove that $\displaystyle \left| \int_0^1 f(x)dx\right|\leq \left(\frac{b-a}2\right)\sup_{x\in [0,1]}\left|f'(x)+2\normalsize{\frac{\int_0^a f(t) dt}{a(1-a)}}\right|$ I haven't solved this yet, but I've made some progress: Since $\frac{1}{1-a}\left(\frac{1}{a}\int_0^a f(x) dx\right)+ \frac{1}{b}\left(\frac{1}{1-b}\int_b^1 f(x) dx\right)=0$, the Mean Value Theorem yields the existence of $\xi_a\in[0,a]$ and $\xi_b\in[b,1]$ such that $$bf(\xi_a)+(1-a)f(\xi_b)=0$$
This implies $f(\xi_a)$ and $f(\xi_b)$ have opposite signs, thus there is some $c\in [\xi_a,\xi_b]$ such that $f(c)=0$. We may suppose WLOG that $\sup_{x\in [0,1]} |f'(x)|<\infty$ (there's nothing to prove otherwise). Let $M=\sup_{x\in [0,1]} |f'(x)|$. One can write $$\displaystyle  \begin{align}\int_0^1 f(x)dx &= \int_0^a f(x)dx + \int_a^b f(x)dx + \int_b^1 f(x)dx \\
&= \frac{\int_0^a f(x) dx}{a(1-a)} \left( a(1-a) -b(1-b)\right) +  \color{red}{\int_a^b f(x)dx} \end{align} $$ Note that $\displaystyle \color{red}{\int_a^b f(x)dx} = \color{green}{\int_a^b (f(x)-f(a))dx} + \color{blue}{(b-a)f(a)}$. Since $f(a) = f(a)-f(c) = f'(\xi_c) (a-c)$, we have $|f(a)|\leq M |a-c|\leq M$. The Mean Value Theorem also gives $\displaystyle \left| \int_a^b (f(x)-f(a))dx \right| \leq M \int_a^b (x-a) dx = M \frac{(b-a)^2}2$.
Putting everything together we have the estimate
$$ \left| \int_0^1 f(x)dx\right|\leq \left(\frac{b-a}2\right)  \left[2 \left|\frac{\int_0^a f(x) dx}{a(1-a)}\right| \underbrace{\frac{ \left( a(1-a) -b(1-b)\right)}{b-a}}_{\leq 1} + \color{green}{M (b-a)} +\color{blue}{2M }   \right]$$ and we get the bound $$ \left| \int_0^1 f(x)dx\right|\leq \left(\frac{b-a}2\right)\left(2\normalsize{\frac{\int_0^a f(t) dt}{a(1-a)}} + 3M\right) $$ which is not as sharp as what's required... Note: this problem is similar to this Prove an integral inequality $|\int\limits_0^1f(x)dx|\leq\frac{1-a+b}{4}M$ I've tried to apply similar techniques, to no avail.","['real-analysis', 'integration', 'integral-inequality', 'inequality']"
2083271,"Find $x^2+y^2+z^2$ if $x+y+z=0$ and $\sin{x}+\sin{y}+\sin{z}=0$ and $\cos{x}+\cos{y}+\cos{z}=0$ for any $x,y,z \in [-\pi,\pi]$","Find $x^2+y^2+z^2$ if $x+y+z=0$ and $\sin{x}+\sin{y}+\sin{z}=0$ and $\cos{x}+\cos{y}+\cos{z}=0$ for any $x,y,z \in [-\pi,\pi]$. My attempt :I found one answer $x=0,y=\frac{2\pi}{3},z=-\frac{2\pi}{3}$ which gives the answer $x^2+y^2+z^2=\frac{8{\pi}^2}{9}$.But I want a way to find the answer using equations.ANy hints?",['algebra-precalculus']
2083296,Convergence of $\sum_{n\geq 0}\frac{(n!)^d}{(d\cdot n)!}$ [duplicate],"This question already has answers here : How to prove the convergence of $\sum_{n\geq 0}\frac{n!^d}{(dn)!}$ for $d\geq 2$? (3 answers) Closed 7 years ago . In an exam I have been asked to discuss the convergence of a series regarding a parameter $d$. Here's the following : $\sum_{n=0}^\infty \frac{(n!)^d}{(d\cdot n)!}$ The answer is that this series converges for $d \geq 2$. I totally understand that if $d \leq 1$, the series will not converge but I am blocked while trying to use the d'Alembert or Cauchy's rules. Can somebody give me a tip ?","['factorial', 'sequences-and-series', 'convergence-divergence']"
2083311,Is $\sum_{k=1}^{n} \sin(k^2)$ bounded by a constant $M$?,I know $\sum_{k=1}^{n} \sin(k)$ is bounded by a constant . How about $\sum_{k=1}^{n} \sin(k^2)$?,"['sequences-and-series', 'calculus']"
2083315,Is the limit of $f(x)-\ln(x)$ equal to the limit of $f(x+n)-\ln(x)$?,"Given that $\displaystyle \lim_{x \to +\infty} (f(x) - \ln x) = 0$, can I say that $\displaystyle \lim_{x \to +\infty} (f(x+n)- \ln x) = 0$?",['limits']
2083318,Orthogonal projection of a right prism to a horizontal plane.,"Given that BC=BK=FI=5 units, CF=KI=3 units, CD=FG=IH=KJ=BE=6 units and that angle KBC = angle BCF = 90 °. MNOP is vertical plane parallel to surfaces BCFIK and EDGHJ and perpendicular to surface BCDE. MPQR is a horizontal plane parallel to surface BCDE. Draw the orthogonal projection of the prism onto the horizontal plane PQRM. I had a test at school and I was unable to solve this question. I failed at figuring out what should the horizontal distance from point K to point I would be. Show me how would you do it please. And also help me choose a better tag for this if you can.",['geometry']
2083338,How to inverse a function that has a range,"I'm given 
$$ k(x)=\begin{cases} -5x+6,& x<4\\-4x+2,&x\ge 4\:\end{cases} $$
And I have to find $k^{-1}(x)$. I found inverse of the both functions
$$
-\frac{x-2}{4}
$$
$$
-\frac{x-6}{5}
$$ But what will their range be?","['inverse-function', 'calculus', 'functions']"
2083361,"A non-Riemann-integrable function $f\colon [0, 1]^2 \to \mathbb{R}$ such that $f(\cdot, y), f(x, \cdot)$ are Riemann integrable.","I need to find a bounded $f\colon [0, 1]^2 \to \mathbb{R}$ which is not Riemann integrable, but such that $f(\cdot, y), f(x,\cdot)$ are Riemann integrable as functions from $[0, 1]$ to $\mathbb{R}$ for every fixed $x,y$ . I've tried several tricks that didn't work out. I began suspecting that there is no such example but I also couldn't prove it. Any ideas?","['multivariable-calculus', 'riemann-integration']"
2083381,Logic proof help?,"I've spent over 3 days trying to learn logical equivalences/proof but no matter how much I try I can't seem to do the exercises I've been given. At all . For example: 
"" Given that P $\land$ ¬P is an inconsistency, show that ¬(¬P $\lor$ Q) $\land$ ¬(¬Q $\lor$ P) is an inconsistency without using a truth table. "" My proof is as follows: Given: ¬(¬P $\lor$ Q) $\land$ ¬(¬Q $\lor$ P) Double negation: ¬(¬P $\lor$ Q) $\land$ (Q $\lor$ P) Commutativity: ¬(Q $\lor$ ¬P) $\land$ (Q $\lor$ P) Distributivity: Q $\lor$ (¬P $\land$ P) Commutativity: Q $\lor$ (P $\land$ ¬P) I'm not sure what to do after this point, although I suspect it's because it's utterly wrong. Please could someone share some guidance?","['logic', 'discrete-mathematics']"
2083389,Application of trigonometry : boat distance [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question The height of a light house is meter.The angle of depression of 2 ships on opposite sides of the light house are observed to be $30$ and $45$ degree .Find the distance between 2 ships",['trigonometry']
2083400,Troubles understanding the independence of events,"I have troubles to understand when an event is independent and when not. So I don't understand one step in the solution to this exercise: We have $X,X_1,X_2,\dots$ be i.i.i. real random variable with density $f(x)$ and distribution function $F(x)$. Further $k\in \mathbb{N}$ and $t\in\mathbb{R}$ and 
  $$N:=\inf\{k\in\mathbb{N}:X_k>X\}$$
  Show $$P(N=k,X\leq t)=\int_{-\infty}^t F(x)^{k-1}(1-F(x))f(x) \, \mathrm{d}x$$ Then the solution says this: By the definition of $N$ and independence of $(X,X_1,\dots,X_k)$ we have
  $$P(N=k,X\leq t)=P(X_1,\dots,X_{k-1}\leq X,X_k>X,X\leq t) \\ \underbrace{=}_{(*)} \int_{-\infty}^t P(X_1,\dots,X_{k-1}\leq X,X_k>X)f(x) \, \mathrm{d}x = \int_{-\infty}^t F(x)^{k-1}(1-F(x))f(x) \, \mathrm{d}x$$ Does this imply that $\{X_1,\dots,X_{k-1}\leq X,X_k>X\}$ is independent of $\{X<t\}$ or why can I write the integral after $(*)$ like that? Both events have the random variable $X$, so both events should be dependent? Why can I write the event $\{X<t\}$ as an integral and the other event stays in $P(X_1\dots)$ form? My second question is about $N$. $N$ is clearly dependent on $X_1,\dots,X_k$ but is it also dependent on $X$? Thank you for explanation.","['independence', 'probability-theory']"
2083409,"$E \subset \mathbb{R}$ such that $\lambda(E \cap [a,b])=\frac{1}{2}(b-a)$ for all $a,b\in\mathbb{R}$","This is Problem 6.45 from Jones' Lebesgue Integration on Euclidean Space . Prove that there does not exist a set $E \subset \mathbb{R}$ such that  $\lambda(E \cap [a,b])=\frac{1}{2}(b-a)$ for all $-\infty<a<b<\infty$. It is one of 'some facts about integration', according to the book, but I don't see how this problem can be solved using integration. The statement is similar to $\int_{a}^{b} \chi_E \, d\lambda=\frac{1}{2}(b-a)$, but I'm not even sure if $E$ is measurable or not, so the integral doesn't make sense yet. Can someone give me a hint for this problem?","['real-analysis', 'lebesgue-integral', 'measure-theory', 'lebesgue-measure']"
2083410,The relation between trace and determinant of a matrix,Let $M$ be a symmetric $n \times n$ matrix. Is there any equality or inequality that relates the trace and determinant of $M$?,"['matrices', 'trace', 'linear-algebra', 'determinant']"
2083415,"in triangle ABC there satisfies equation: $\cos A \cos B+\sin A\sin B\sin C=1$, determine possible values of $C$ [duplicate]","This question already has answers here : If $\sin(a)\sin(b)\sin(c)+\cos(a)\cos(b)=1$ then find the value of $\sin(c)$ (3 answers) Closed 7 years ago . Question in triangle ABC there satisfies equation: $\cos A\cos B+\sin A\sin B\sin C=1$, determine possible values of C What I have so far I've noticed that the given equation looked similar to $\cos(A-B)$ which was $\cos A\cos B+\sin A\sin B$. This I can extrapolate that $\cos(A-B)-\sin A\sin B+\sin A\sin B\sin C=1$ Thus $\cos(A-B)=1+\sin A\sin B+\sin A\sin B\sin C$ and when i factor out the common terms: $\cos(A-B)=1+\sin A\sin B(1-\sin C)$ Right here I am not sure how to proceed and I got stuck","['algebra-precalculus', 'trigonometry', 'geometry']"
2083520,Is a convex combination of conditional probabilities the conditional probability of a convex combination of unconditional probabilities?,"Let $P_1,...,P_k$ be probability measures on $(\Omega, \mathcal{F})$, and let $E \in \mathcal{F}$ with $P_i(E) > 0$, $i=1...k$. Suppose $\beta_i \geq 0$, $i=1,...,k$, and $\sum_{i=1}^k \beta_i = 1$. Then, $\sum_{i=1}^k \beta_i P_i(\cdot \mid E)$ defines a probability measure on $(\Omega, \mathcal{F})$, because each conditional probability $P_i(\cdot \mid E)$ is a probability measure and a convex combination of probabilities is a probability. My question is whether $\sum_{i=1}^k \beta_i P_i(\cdot \mid E)$ can be expressed as the conditional probability, given $E$, of a convex combination of $P_1,...,P_k$. Question. Does there exist $\alpha_i \geq 0$, $i=1,...,k$, with $\sum_{i=1}^k \alpha_i = 1$ such that $(\sum_{i=1}^k \alpha_i P_i)(\cdot \mid E) = \sum_{i=1}^k \beta_i P_i(\cdot \mid E)$? I believe the answer is yes because I think I can verify the case $i=2$ by a brute force calculation that I won't reproduce here (it's quite messy and, I think, not very instructive). The trouble is, I get the feeling that I'm missing some basic fact or observation that would answer my question in a more elegant and illuminating way.","['convex-hulls', 'probability-theory', 'probability']"
2083522,Double integral; changing order of integration,I'm attempting some exercises on double integration in Schaum's Outline of Calculus. The integral is $$I = \int_{y=1}^{2}\int_{x=0}^{y^{3/2}} \frac{x}{y^2}\ dx\ dy.$$ I can do it in this direction and it turns out to be $I = \frac{3}{4}$ but for some reason I'm struggling to swap the order of integration and I think I may be confused. I presume it'll be in the form of $$I = \int_{x = \text{const}}^\text{const}\int_{y = \text{const}}^{y(x)} \frac{x}{y^2}\ dy \ dx$$,"['multivariable-calculus', 'integration']"
2083548,lower bound for a function of $\Gamma$,"I would like to show that $$\frac{\Gamma(1-2x)\Gamma(1+x)}{\Gamma(1-x)}\geq 1$$ for real $x$ such that $|x|\leq \frac12$, where $\Gamma$ is the usual gamma function. I looked at the derivatives and went a long road to prove this. I though somehow believe there should be a simpler way. I appreciate any hints or comments. Many thanks!","['real-analysis', 'calculus']"
2083561,Isomorphism of affine varieties,"Let $X\subseteq\mathbb{C}^n$ and $Y\subseteq\mathbb{C}^m$ be two complex affine varieties (zero sets of systems of polynomials). Suppose that
$$f:X\to Y$$
is a polynomial map, or more precisely, it is given explicitly by
$$f(x)=(f_1(x),\ldots,f_m(x)),$$
where $f_i:\mathbb{C}^n\to\mathbb{C}$ are polynomials in $n$-variables. Question: If $f$ is a bijection, is it necessarily an isomorphism of complex affine varieties?","['abstract-algebra', 'polynomials', 'complex-geometry', 'algebraic-geometry']"
2083574,Proof for Maximum Likelihood Estimation,"I can follow the steps but I don't really understand what it wants to say. The whole equation is done to prove the inequality, what does inequality say?","['maximum-likelihood', 'statistics']"
2083608,From the given figure [duplicate],"This question already has an answer here : Geometry with Trigonometry (1 answer) Closed 7 years ago . From the given figure, prove that $$\cot\ \theta=\cot\ A+\cot\ B+\cot\ C$$ My work. I have got this solution from one of my friends. But I didn't understand all the process. I understood the application of ceva theorem in the first step but the steps after that I can’t understand. Also how has be $\prod $ used here, why? How it works here?  Please make me understand","['trigonometry', 'geometry']"
2083642,Limit in n-dimensions with dot product,"Let there be three vectors $\mathbf a$, $\mathbf b$, and $\mathbf h$ in $\mathbb R^n$. Define the real number
$$p(\mathbf h) \doteq (\mathbf a \cdot \mathbf h) (\mathbf b \cdot \mathbf h)$$
Does the following limit exist? In particular, does it equal $0$?
$$\lim_{\mathbf h \to \mathbf 0} \frac {p(\mathbf h)} {|\mathbf h|} $$
I was thinking of expressing $p$ as a product of sums, like
$$p(\mathbf h)=\left(\sum_{j=1}^n a_jh_j\right)\left(\sum_{k=1}^n b_kh_k\right) = \sum_{i=1}^na_ib_ih_i^2 + \sum_{j\neq k} a_jb_kh_jh_k $$
Are there any dot product properties I may be forgetting at this point?","['multivariable-calculus', 'linear-algebra', 'limits']"
2083657,Linear algebra proof with linear operator,"Let $V$ be a finite dimensional vector space, $T \in L(V)$ a linear map whose matrix $M(T)$ is the same for every basis of $V$. Show that $T$ is a scalar multiple of the identity map $I$. I know it has to do with something about the vectors being linearly independent in a basis but I don't know where to go with that when trying to find a contradiction",['linear-algebra']
