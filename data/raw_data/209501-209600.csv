question_id,title,body,tags
4205951,Solve $\sqrt{\frac{(2 - x)(2 - y)}{(2 + x)(2 + y)} + \frac{(2 - y)(2 - z)}{(2 + y)(2 + z)} + \frac{(2 - z)(2 - x)}{(2 + z)(2 + x)}}$,"Find the value of the expression $S$ , knowing that $x^2 + y^2 + z^2 + xyz = 4$ and $x, y, z \in (0, \infty)$ : $$
S = \sqrt{\frac{(2 - x)(2 - y)}{(2 + x)(2 + y)} + \frac{(2 - y)(2 - z)}{(2 + y)(2 + z)} + \frac{(2 - z)(2 - x)}{(2 + z)(2 + x)}}
$$ My attempt: As $x^2 + y^2 + z^2 + xyz = 4$ , we can choose $A, B, C \in \left[0, \frac{\pi}{2}\right)$ for which $x = 2 \cos A$ , $y = 2 \cos B$ și $z = 2 \cos C$ , a well-known substitution identity. Now, let's prove that: $$\tan^2 A = \frac{2 - x}{2 + x}$$ It is true that: $$\cos A = \frac{1 - \tan^2 \frac{A}{2}}{1 + \tan^2 \frac{A}{2}}$$ Therefore: $$\tan^2 A = \frac{1 - \cos A}{1 + \cos A}$$ Equivalent to the identity we wanted to prove true. Let's substitute in the expression: $$S = \sqrt{\tan^2 \frac{A}{2} \tan^2 \frac{B}{2} + \tan^2 \frac{B}{2} \tan^2 \frac{C}{2} + \tan^2 \frac{C}{2} \tan^2 \frac{A}{2}}$$ Which doesn't seem to be constant. I suspect the problem is actually wrong, because if, instead of one square root, we'd have three square roots, the experession would have evaluated to: $$S'= \tan \frac{A}{2} \tan \frac{B}{2} + \tan \frac{B}{2} \tan \frac{C}{2} + \tan \frac{C}{2} \tan \frac{A}{2} = 1$$ Which is calculable by a very well known identity. Could the problem be wrong? Or it's something I'm missing, a further step to solve the initial problem? Thanks in advance!","['radicals', 'algebra-precalculus', 'trigonometry', 'substitution']"
4205998,How to construct such a scheme?,"It's an exercise in the Wedhorn's book: Let $k$ be an algebraically closed field. Give a $k$ -scheme $X$ such that: There is a morphism $f:\mathbb A_k^1\rightarrow X$ which is homeomorphic on the topological space. $\dim T_xX=1$ at all except one $x\in X$ . $X$ is not reduced. I have tried $k[X,Y,Z]/(X^2+Y^3,Z^2)$ . In this case, we can calculate its jacobbian, and see that the tangent space has dimension $3$ at $0$ but $2$ elsewhere. But in the case $k[X,Y]/(X^2-Y^3)$ it satisfies all but condition 3. My idea is: Since we need to get a ""special"" point, we may construct a curve which is singular at only one point. (In above cases $X^2-Y^3$ is the curve I need). But I don't know how to modify it to make it satisfy all conditions. Could you help me achieve it? Or could you give an example directly? Thanks!","['algebraic-geometry', 'schemes']"
4205999,How do we solve for $x$ in $x^5-x-1=0$?,"What is the procedure to finding the simplest exact (or atleast a verifiable approximation to desired level of precision) correct answer to this quintic equation, and more generally to other polynomial equations of degree- 4 and higher yielding non-rational zeroes? And in this particular case, what are all the (exact or approximate) solutions? On desmos there appears to be one real x-intercept at ~1.167; I would imagine that there are up to four other non-real solutions (or fewer, should some have multiplicity greater than 1---is this possible for non-real complex roots?)","['galois-theory', 'algebra-precalculus', 'roots']"
4206022,How we solve $0=y\sin\frac{3t}{2}+x\cos\frac{3t}{2}-y\sin(t)-x\cos(t)$ for real $t$ ($\frac{4\pi}{3}<t<2\pi$) or at least get the curve's degree?,"I have a system of equation: $x^2+y^2=y\sin\frac{3t}{2}+x\cos\frac{3t}{2}$ $x^2+y^2=y\sin(t)+x\cos(t)$ I want to get an algebraic curve depending only from $x$ and $y$ . If it is too complex, it would be already sufficient to determine, which degree the resulting curve has. Can we at least determine the degree (the largest power $n$ that appears in $x^n$ ) of the curve equation in $x$ and $y$ ? I am interested only in a (real) range $\frac{4\pi}{3}<t<2\pi$ , which hopefully gives a condition that reduces the effort in solving the equation.","['algebraic-curves', 'curves', 'real-analysis', 'algebraic-geometry', 'trigonometry']"
4206023,Is the image of a smooth homeomorphism diffeomorphic to the domain?,"This was motivated by the definition of a smooth embedding as an injective smooth immersion that is a topological embedding; I see no reason a-priori for the image of an embedding as defined to be diffeomorphic to the original manifold unless this was true (and if they aren't diffeomorphic then calling it a smooth embedding doesn't seem to make sense to me). For example, it is well-known that the $x \mapsto  x^3$ is a smooth homeomorphism that is not a diffeomorphism. However, its image is clearly diffeomorPHIC to its domain (both being $\mathbb{R}$ with the standard smooth structure). My question is therefore this: is it true in general that if $f: M \rightarrow N$ is a smooth homeomorphism then $M$ and $N$ are diffeomorphic? EDIT: the key thing I somehow missed was the ""immersion"" part, yes a smooth homeomorphic IMMERSION is a diffeomorphism by the Inverse Function Theorem. But my question still stands.","['diffeomorphism', 'smooth-manifolds', 'general-topology', 'differential-topology', 'differential-geometry']"
4206127,Projective plane does not minimally immerse into $S^3$,"Can someone please give me a reference (or a proof) of the fact that the projective plane cannot be minimally immersed into the 3-sphere? My reference is: Lawson, “Complete Minimal Surfaces in $\mathbb S^3$ ”, see Corollary 1.6. In this paper, among other things, Lawson explains the relation between the zeros of the Hopf holomorphic differential form on a minimal surface in $\mathbb S^3$ and the topology of the surface, which seems to bee the idea that leads to Corollary 1.6, my desired claim. However, I miss the logical step to prove the claim, maybe somebody can explain it to me. It is anyway clear to me that the projective plane cannot be embedded in $\mathbb S^3$ and that it can be immersed in $\mathbb S^3$ .","['minimal-surfaces', 'differential-geometry']"
4206159,"Apply the Girsanov theorem, determine the stochastic dynamics of $S^{(1)}$ and determine the risk-neutral price of $X$","I'm not sure if I'm applying Change of Numeraire and Girsanov correctly in part c) and d). Also with the information I got, I don't know how to get a result for e). Consider a financial market with 2 stocks and a bank account $S_t^{(0)}=1$ for $0\le t\le T$ , i.e. the riskless interest rate is $r=0$ . Under a risk-neutral martingale measure $Q$ , assume the following dynamics of 2 stock prices $\{S_t^{(1)}\}_{0 \le t \le T}$ and $\{S_t^{(2)}\}_{0 \le t \le T}$ : $$dS_t^{(1)}=\sigma_{11}S_t^{(1)}d\widetilde{W}_t^{(1)}$$ with $S_0^{(1)}>0$ , and $$dS_t^{(2)}=\sigma_{21}S_t^{(2)}d\widetilde{W}_t^{(1)}+\sigma_{22}S_t^{(2)}d\widetilde{W}_t^{(2)}$$ with $S_0^{(2)}>0$ ,
with constants $\sigma_{11}$ , $\sigma_{21}$ , $\sigma_{22}$ . Note that $\{\widetilde{W}_t^{(1)}\}_{0 \le t \le T}$ and $\{\widetilde{W}_t^{(2)}\}_{0 \le t \le T}$ are 2 independent standard Brownian motions under the risk-neutral martingale measure $Q$ . It is further assumed that the matrix \begin{bmatrix}\sigma_{11}&0\\\sigma_{21}&\sigma_{22}\end{bmatrix} is irreversible. a) Using Itô's formula, show that the solution of the SDE, $dS_t^{(2)}=\sigma_{21}S_t^{(2)}d\widetilde{W}_t^{(1)}+\sigma_{22}S_t^{(2)}d\widetilde{W}_t^{(2)}$ , with $S_0^{(2)}>0$ is given by $S_t^{(2)}=S_0^{(2)} \exp(\frac{-1}{2}(\sigma_{21}^2+\sigma_{22}^2)t+\sigma_{21}\widetilde{W}_t^{(1)}+\sigma_{22}\widetilde{W}_t^{(2)} )$ . My solution for a) One can see this by applying, $df=f_tdt+f_xdW_t+\frac{1}{2}f_{xx}d\langle W\rangle_t$ On get then $dS_t^{(2)}=-\frac{1}{2}(\sigma^2_{21}+ \sigma^2_{22})S_t^{(2)}dt+\sigma^2_{21}S_t^{(2)}d\widetilde{W}_t^{(1)}+\sigma^2_{22}S_t^{(2)}d\widetilde{W}_t^{(2)}+\frac{1}{2}(\sigma_{21}^2dt+\sigma_{22}^2dt)$ where here is important to see that since the $\widetilde{W}_t^{(i)}$ are independent, $d\widetilde{W}_t^{(1)}d\widetilde{W}_t^{(2)}=0$ .
If we simplify we get the desired solution. b) Explain why we can use the risk-neutral valuation formula to price any contingent claim $X$ in this financial market. My solution for b) Since the market is complete, due to the fact that the matrix with sigma is invertible and that we are under a risk-neutral martingale measure, we can use risk-neutral valuation to price any contingent claim $X$ c) Consider the following contingent claim or derivative $$X(T)=S_T^{(2)}1_{\{S_T^{(1)}>K\}}$$ . Using ""Change of Numeraire"" and $S^{(2)}$ as numeraire, show that the risk-neutral price $\Pi_0^X$ of the contingent claim $X$ at time $t=0$ is given by $$\Pi_0^X=S_0^{(2)} E_{Q^1}[1_{\{S_T^{(1)}>K\}}]$$ wherethe measure $Q^1$ has to be determined, i.e. determine its Radon-Nikodym derivative with respect to $Q$ $$\frac{dQ^1}{dQ}|F_t$$ my solution to c) One has $$\frac{dQ^1}{dQ}|F_t=\frac{N_t}{S_t^{(0)}}\frac{S_0^{(0)}}{N_0}$$ since new numeraire is supposed to be $S^{(2)}$ and $S_t^{(0)}=1$ for all $t$ ( $r=0$ ) we get $$\frac{dQ^1}{dQ}|F_t=\frac{S_t^{(2)}}{1}\frac{1}{S_0^{(2)}}$$ and one has $\Pi_0^X=1*E_Q[S_T^{(2)}1_{\{S_T^{(1)}>K\}}]=S_0^{(2)}E_{Q^1}[1_{\{S_T^{(1)}>K\}}]$ d) Consider the probability measure $Q^1$ from (c). Applying Girsanov theorem, determine the stochastic dynamics of $S^{(1)}$ under $Q^1$ .
Knowing that a general SDE of this form $$dSt=\hat{\mu} S_tdt+\hat{\sigma}S_td\hat{W}_t$$ with, $S_0>0$ has the solution $$St=S_0\exp((\hat{\mu}-\frac{1}{2}\hat{\sigma})t+\hat{\sigma}\hat{W}_t)$$ write down the solution of the derived SDE . my solution to d) (Updated) Using Girsanov we know that for any equivalent measure we have $$\frac{dQ^1}{dQ}=\frac{S_T^{(2)}}{1}\frac{1}{S_0^{(2)}}=L_T$$ but at the same time $$L_T=\exp\{-\int_0^T \gamma_s^{(1)}d\widetilde{W}_s^{(1)}
-\int_0^T \gamma_s^{(2)}d\widetilde{W}_s^{(2)}-\frac{1}{2}\int_0^T(\gamma_s^{(1)})^2+(\gamma_s^{(2)})^2ds\}$$ since we know that $S_T^{(2)}/S_0^{(2)}=\exp(-\frac{1}{2}(\sigma_{21}^2+\sigma_{22}^2)T+\sigma_{21}\widetilde{W}_T^{(1)}+\sigma_{22}\widetilde{W}_T^{(2)})$ , it follows that $\gamma_t^{(1)}=-\sigma_{21}$ and $\gamma_t^{(1)}=-\sigma_{22}$ . Now using the fact that $d\hat{W}_t^{(i)}=d\widetilde{W}_t^{(i)}+\gamma_t^{(i)}dt$ are Brownian motions under $Q^1$ Applying this to $dS_t^{(1)}=\sigma_{11}S_t^{(1)}(d\hat{W}_t^{(1)}+\sigma_{21}dt)$ gives $\hat{\mu}=\sigma_{21}\sigma_{11}$ and $\hat{\sigma}=\sigma_{11}$ e) Combining results from c) and d), determine the risk-neutral price of this derivative $X(T)$ at time $t=0$ Solution to e (according to suggestion) Ok. Since my calculation is correct, I just need to calculate the following now: (I'm taking $S_0^{(2)}$ out of the calculations since it's a constant) $E_{Q^1}[1_{\{S_T^{(1)}>K\}}]=Q^1(S_T^{1}>K)=Q^1(S_0^{(1)}\exp((\sigma_{11}\sigma_{21}-\frac{1}{2}\sigma_{11}^2)T+\sigma_{11}\hat{W}_T^{(1)})$ and after moving some terms, using the fact that $-W_t=W_t$ and that $W_t\sim N(0,t)$ , $=Q^1(\frac{\ln(\frac{S_0^{(1)}}{K})+(\sigma_{11}\sigma_{12}-\frac{1}{2}\sigma_{11}^2)T}{\sigma_{11}\sqrt T}> Z)$ where Z is standard normal distribution.","['stochastic-analysis', 'finance', 'stochastic-differential-equations', 'brownian-motion', 'probability-theory']"
4206170,Convergence of $\sum_{n=1}^{\infty} \frac{1}{\lambda(n)^s}$,"Does there exist $s>1$ such that the infinite sum: $$\large \sum_{n=1}^{\infty} \frac{1}{\lambda(n)^s}$$ converges? $\lambda(n)$ denotes the Carmichael lambda function. There exists a lower bound for the Carmichael lambda function. $$ \large \frac{\ln(n)^{\ln\ln\ln(n)}}{\ln(2)} \ < \ \lambda(n) \ \le \ n-1 $$ Maybe there are better lower bounds, but if this inequality holds true for all $n$ , then we have a valid lower bound.
This speaks for the existence of an $s$ such that the above infinite sum converges. Unlike the Euler totient function the Carmichael lambda function has lots and lots of solutions. For example $\lambda(n) = 12$ has 84 solutions. $\lambda(n) = 36$ has 480 solutions.
Peter recently showed that $\lambda(n) = 2 \ 570 \ 400$ has more than $4 \cdot 10^{32}$ solutions. Which makes it difficult for determing $s$ . Or doesn't $s$ exist at all?","['number-theory', 'elementary-number-theory', 'sequences-and-series']"
4206175,$SO(3)$ contains no maximal tori,"In Linear Algebraic Groups I (Stanford, Winter 2010) ( http://virtualmath1.stanford.edu/~conrad/252Page/handouts/alggroups.pdf ) course notes, there appears the theorem that if $G$ is a linear algebraic smooth connected group over $k$ , then all $k$ split torus are conjugate. In remark 4.2, They write about this not being true if we considered nonsplit toruses, claiming that this term may not be even well defined! They claim that in $SO(3)/\operatorname{Spec}(\mathbb{R})$ , there is no maximal tori (while there is a maximal split one). I don't understand how there is no maximal one, don't dimension considerations contradict this? Exact quotes: Theorem 4.2.1 . If $G$ is any smooth connected linear algebraic $k$ -group, all maximal $k$ -split tori $T \subset G$ are $G(k)$ -conjugate. Remark 4.2.2. ... The theorem is false if “maximal $k$ -tori” were to replace “maximal $k$ -split tori”. And note that it is trivial (by dimension reasons) that maximal $k$ -split tori always exist; in contrast, there may be no $k$ -split maximal $k$ -tori ... I assumed the latter maximal was a typo and they meant there may be no maximal $k$ -tori.","['algebraic-geometry', 'algebraic-groups']"
4206194,what is the meaning of this set $\{x : f(x) \neq g(x)\}?$,"Note :My motivation comes from this question I have some  confusion regarding Rudin RCA book, page number $55$ Lusin's  theorem  : Suppose $f$ is a complex   measurable  function on $X$ . $\mu(A) < \infty$ , $f(x)= 0$ if $x \notin A $ and $\epsilon >0$ . Then  there  exists a $g \in C_c(X)$ such that $ \mu(\{x : f(x) \neq g(x)\}) <\epsilon$ My confusion : what is the meaning of this set $\{x : f(x) \neq g(x)\}?$ My thinking : $\{x : f(x) \neq g(x)\}=\{x : f(x) - g(x) \neq 0 \}=\{x : f(x) - g(x)> 0 \ \text{or} \ f(x) - g(x) <0  \}$ If $f-g >0$ then $\{x : f(x) - g(x) > 0 \}= (f-g)^{-1}(0,\infty)$ If $f-g <0$ then $\{x : f(x) - g(x) < 0 \}= (f-g)^{-1}(-\infty,0)$ But here $\mu(-\infty,0)=\mu(0,\infty)=\infty \implies   \mu(\{x : f(x) \neq g(x)\})=\infty$ This  lead to the contradiction that $ \mu(\{x : f(x) \neq g(x)\} <\epsilon$","['elementary-set-theory', 'measure-theory', 'lebesgue-measure', 'analysis']"
4206201,Why is it necessary for density functions to be absolutely continuous with respect to a measure in order for the cross entropy to be defined?,"In the Wikipedia page describing cross entropy, the following expression is written down to denote the cross entropy $H$ between two densities $p(x)$ and $q(x)$ : $H(p,q) = - \int_\mathcal{X}p(x)\log q(x)dr(x)$ The page mentions that "" $p$ and $q$ must be absolutely continuous with respect to some measure $r$ (usually $r$ is a Lebesgue measure on a Borel $\sigma$ -algebra)"". The definition of absolute continuity is this: Let $I$ be an interval on the real line; let $(x_k, y_k)$ be a finite sequence of pairwise disjoint sub-intervals of $I$ where $x_k < y_k \in I$ ;  a function $f$ is absolutely continuous with respect to $I$ if $\forall \epsilon > 0$ there exists $\delta >0$ such that    we have $\sum_k (y_k - x_k) < \delta \implies \sum_k|f(y_k) - f(x_k)|<\epsilon$ I have some intuition for what absolute continuity with respect to $I$ might mean - if I want the output to change by a tiny amount $\epsilon$ from $f(x_k)$ to $f(y_k)$ , an absolutely continuous function on $I$ guarantees that I only need to push $x_k$ by a tiny amount that is upper bounded by $\delta$ . But why is this an important assumption to make in order for the cross entropy to be defined? I just don't see the link at all, and I am looking for some intuition.","['absolute-continuity', 'probability-theory', 'probability']"
4206228,The field of quotients of the ring of germs of functions.,"Let $Y\subseteq \mathbb{A}^n$ be an affine variety with affine co-ordinate ring $A(Y)=k[x_1\dots, x_n]/I(Y)$ , where $k$ is an algebraically closed field. Denote with $\mathcal{O}(Y)$ the ring of all regular functions $f\colon Y\to k$ . Fix an affine variety $Y$ and a point $P\in Y$ . Let $U,V\subseteq Y$ be open neighbourhoods of $P$ . For $f\in\mathcal{O}(U)$ and $g\in\mathcal{O}(V)$ define $f \sim g$ if there exists a open neighbourhood $W\subseteq U\cap V$ of $P$ such that $f|_W=g|_W$ . Then $\sim$ s an equivalence relation between these the set of all such pairs $(U,f)$ . The set $\mathcal{O}_{P,Y}$ (or $\mathcal{O}_P)$ of all equivalence classes is a local ring, called the ring of germs of functions. For a pair of functions $(U,f)$ , $(V,g)$ (as above except we ignore a specific point $P$ ), define $f\sim'g$ if there exists a open set $W\subseteq U\cap V$ such that $f|_W=g|_W$ . Then $\sim'$ is an equivalence relation and the set $K(Y)$ of all equivalence classes is a field. Denote now with $K(R)$ the quotient field of an integral domain $R$ . I have to prove that in general the following is true: $$\boxed{K(Y)=K(\mathcal{O}_P)}$$ We consider the restriction map $$\mathcal{O}_{P,Y} \to K(Y)\quad\text{defined as}\quad [f]\mapsto [f]'.$$ I have shown that this map is injective, then results that $$K(\mathcal{O}_{P,Y})\subseteq K(Y).$$ For the opposite inclusion I was suggested to use the following: Using the above facts, irreducibility of $Y$ and $$\color{red}{K(Y)=\bigcup_{Q\in Y}K(\mathcal{O}_Q)}$$ Question 1. Why is the expression in red true? show that $$K(\mathcal{O}_Q)=K(Y)\quad\text{for each}\quad Q\in Y.$$ Could someone suggest me how to proceed? I have no idea how to deal with this inclusion. Thanks!","['proof-explanation', 'proof-writing', 'algebraic-geometry', 'solution-verification']"
4206232,Constructing the category $\mathbf{Grp}$ from the sets of group structures,"I was wondering if we can construct the category $\mathbf{Grp}$ of groups from the function $G$ which associates to every set $X$ the set $G(X)$ of group structures on $X$ , or what we have to add to this function to make it work. Namely, a group is just a pair $(X,a)$ where $X$ is a set and $a \in G(X)$ . The homomorphisms make some trouble, though. A homomorphism $(X,a) \to (Y,b)$ should be a map $f : X \to Y$ such that some property is satisfied, but I don't see how to express it in terms of $G(X)$ and $G(Y)$ alone. If $f$ is bijective , then we have an induced map $f_* : G(X) \to G(Y)$ , and the condition is just $f_*(a)=b$ . For general $f$ , it seems that we need some extra structure on $G(-)$ to make it work. So my question is: What is this extra structure? Of course, $G(X)$ is just the fiber of the forgetful functor $U : \mathbf{Grp} \to \mathbf{Set}$ at $X$ , but I wonder if we can define this structure without ""assuming"" $\mathbf{Grp}$ is already there. The problem is that $U$ is an iso-fibration, but not a fibration or an opfibration. Let me know if I should clarify the question further. The analogous problem for $\mathbf{Top}$ can be solved as follows: Consider the functor $T : \mathbf{Set}^{\mathrm{op}} \to \mathbf{Pos}$ which maps a set $X$ to the partially ordered set $T(X)$ of topologies on $X$ and a map $f : X \to Y$ to the map $f^* : T(Y) \to T(X)$ , $f^*(\tau) = \{f^{-1}(U) : U \in \tau\}$ . Then, a topological space is a pair $(X,\tau)$ with $\tau \in T(X)$ , and a continuous map $(X,\tau) \to (Y,\sigma)$ is a map $f : X \to Y$ such that $f^*(\sigma) \subseteq \tau$ . In other words, $\mathbf{Top}$ is the Grothendieck construction of the functor $\mathbf{Set}^{\mathrm{op}} \to \mathbf{Pos} \hookrightarrow \mathbf{Cat}$ .","['universal-algebra', 'group-theory', 'grothendieck-construction', 'category-theory']"
4206242,A convex polyhedron floats in the sea. Can $90\%$ of its volume be below water level and more than half of its surface be above water level?,"An object in the shape of a convex polyhedron floats in the sea. Could it happen that $90\%$ of its volume is below the water level and more than half of its surface is above the water level? As I was told, this is a PURE mathematical problem. There are shapes with infinite area and finite volume, but they are not convex. Roughly speaking, if you have a face, then the area of the other faces is always greater than the area of this one. The idea behind the idea is this: project all the other faces onto this plane. The projection will cover our face, since the polyhedron is convex. And the projected area is not more than the area of the projected faces. However, I can't prove it mathematically, I thought to consider a pyramid or a cone, for example: consider a regular pyramid floating downward with its top. Let the area of its lateral surface be $S$ . Then the area of the base is equal to $S\cos α$ , where $α$ is the angle of inclination of the lateral faces to the base.","['trigonometry', 'geometry']"
4206266,How can I make the following expression to be a martingale?,"Let $B$ denotes a Brownian motion in a filtration $\mathcal{F}$ and we define $X$ as the following process: $$X_{t}=\int_{0}^{t}h\left(B_{s}\right)ds,$$ where $h:\mathbb{R}\mapsto\left[1,\infty\right)$ is a smooth function. $M$ is definied as the following: $$M_{t}=\int_{0}^{t}e^{-X_{s}}ds+v\left(B_{t}\right)e^{-X_{t}},$$ where $v:\mathbb{R}\mapsto\mathbb{R}$ . What does $v$ have to satisfy if we want $M$ to be a martingale in $\mathcal{F}$ ? Here is my solution so far which may contains a lot of mistakes, misunderstandings and/or inaccuracy. If we want $M$ to be a martingale, then $M$ shouldn't contain any $\int_{0}^{t}\ldots ds$ member in its decomposition. Unfortunatelly there is $\int_{0}^{t}e^{-X_{s}}ds,$ which is a nonnegative process with finite total variation. We know it has finite total variation because we Lebesgue integrate a positive expression. Even $X$ has finite total variation because of the same reason. $X$ is nonnegative, because $h:\mathbb{R}\mapsto\left[1,\infty\right)$ , so $X$ is an integration of positive values.
We can make $\int_{0}^{t}e^{-X_{s}}ds$ disappear in $M$ , if $M$ is the constant $0$ process, therefore $$\int_{0}^{t}e^{-X_{s}}ds=-v\left(B_{t}\right)e^{-X_{t}}.$$ I do hope this is the only way to make $M$ to be a martingale, but at this point I am not so sure. We want to find the Itô decomposition of $-v\left(B_{t}\right)e^{-X_{t}}$ in the form of $g\left(X_{t},B_{t}\right)$ . I got the following result for this: $$v\left(B_{t}\right)e^{-X_{t}}=\int_{0}^{t}v\left(B_{s}\right)e^{-X_{s}}h\left(B_{s}\right)-\frac{1}{2}\frac{\partial^{2}v}{\partial y^{2}}\left(B_{s}\right)e^{-X_{s}}ds+\int_{0}^{t}-\frac{\partial v}{\partial y}\left(B_{s}\right)e^{-X_{s}}dB_{s},
$$ which should be equal with $\int_{0}^{t}e^{-X_{s}}ds$ for all $t$ . As far as I know there is a statement, (I think we called it Doob-lemma, but I think I am mistaken here, because I don't really remember this crystal clean...) that the semimartingale decomposition is unique, so $$\begin{cases}
e^{-x}=v\left(y\right)e^{-x}h\left(y\right)-\frac{1}{2}\frac{\partial^{2}v}{\partial y^{2}}\left(y\right)e^{-x}\\
0=-\frac{\partial v}{\partial y}\left(y\right)e^{-x}
\end{cases}\Longrightarrow\begin{cases}
1=v\left(y\right)h\left(y\right)-\frac{1}{2}\frac{\partial^{2}v}{\partial y^{2}}\left(y\right)\\
0=\frac{\partial v}{\partial y}\left(y\right)
\end{cases}.$$ We have to solve this system of differential equations. From the last equation we know $v$ is a constant function. Furthermore, $h$ has to be the reciproke of $v$ , because if $v$ is constant, than $\frac{\partial^{2}v}{\partial y^{2}}\left(y\right)=0$ , and $1=const\cdot h\left(y\right)$ , so $h$ is a constant as well. $h$ can't be the constant $0$ , because $h:\mathbb{R}\mapsto\left[1,\infty\right)$ . First I thought we made this clause, because we didn't want $X$ to cross the $0$ , therefore in the case of division we can use Itô's formula appropriately. This is my solution, but I don't think it is right. I feel like a made a lot of mistakes but I want to get a clear solution...I don't want this exercise to be unsolved. Some corrections: You have written in the comments quite well, if $v\left(B_{t}\right)e^{-X_{t}}$ has the form $-\int_{0}^{t}e^{-X_{s}}ds+M_{t}$ , then I get back my own $M$ martingale which means $M$ doesn't necessary have to be the constant zero process. As I did before, the dynamic of $v\left(B_{t}\right)e^{-X_{t}}$ can be written as $$\int_{0}^{t}-v\left(B_{s}\right)e^{-X_{s}}h\left(B_{s}\right)+\frac{1}{2}\frac{d^{2}v}{dy^{2}}\left(B_{s}\right)e^{-X_{s}}ds+\int_{0}^{t}\frac{dv}{dy}\left(B_{s}\right)e^{-X_{s}}dB_{s},$$ which leads back to the differential equation: $$v\left(y\right)h\left(y\right)-\frac{1}{2}\frac{d^{2}v}{dy^{2}}\left(y\right)=1,$$ thanks to the uniqueness of Doob-Meyer decomposition and the fact, that the expressions above are measurable functions of a normal distributed random variable, which is absolute continuous to the Lebesgue measure (moreover they are equivalent). We made the $\int_{0}^{t}\ldots ds$ member disappear, but it is not enough for $M$ to be a martingale. However, I know a condition, which makes $M$ to be a martingale. We know $M_{t}=\int_{0}^{t}\frac{dv}{dy}\left(B_{s}\right)e^{-X_{s}}dB_{s}$ from the dynamic mentioned above. If $\frac{dv}{dy}\left(B_{s}\right)e^{-X_{s}}$ is progressive measurable, and $$\mathbb{E}\left[\int_{0}^{t}\left(\frac{dv}{dy}\left(B_{s}\right)e^{-X_{s}}\right)^{2}ds\right]<\infty,$$ then $M$ is a martingale. If a process is adapted and continuous, then the process is progressive measurable, so it is enough to $\frac{dv}{dy}\left(y\right)$ be continuous, because we would get compositions of continuous processes which is also continuous. Finally, if I did everything alright, and $v$ satisfies the conditions below, than $M$ is martingale. 1. $v$ is a solution of the differential equation: $$v\left(y\right)\cdot h\left(y\right)-\frac{1}{2}\frac{d^{2}v}{dy^{2}}\left(y\right)=1.$$ 2. $\frac{dv}{dy}\left(y\right)$ is continuous. 3. $\mathbb{E}\left[\left(\frac{dv}{dy}\left(B_{s}\right)\right)^{2}e^{-2\int_{0}^{s}h\left(B_{u}\right)du}\right]<\infty$ for all $s$ , thanks to the Fubini theorem. The conditions above perhaps can be generalized, is it a better solution?","['local-martingales', 'stochastic-processes', 'martingales', 'ordinary-differential-equations']"
4206284,Clarifications on the proof of the Jordan-Schoenflies Theorem,"I am currently going through the proof of the Jordan-Schoenflies Theorem by S.S. Cairns ( http://eretrandre.org/rb/files/Cairns1951_193.pdf ). I find that because it is a somewhat old article there are some conventions that are unclear to me. I wanted to ask about them and hopefully get unstuck. 1) The article claims this can be quickly established by familiar methods. I tried doing it and I found that using the corollary (A) of the Jordan-Schoenflies: I was able to deduce that either the interior of the $b_1$ is in the interior of $b_2$ or else their interiors are disjoint. I assume by hypothesis that $b_1$ and $b_2$ both have the Jordan-Schoenflies property. This claim holds because if part of $b_1$ lied in the interior of $b_2$ I can only exit through the simple arc $b$ , which forces all of $b_1$ to be inside the interior of $b_2$ or in $b$ , else we have a self-intersecting curve. As a consequence the interior of $b_1$ is also contained in the interior of $b_2$ .  The argument if part of $b_1$ lied in the exterior of $b_2$ is similar. I can then map all of $E$ to itself bringing $b_1$ to a circle, which makes things look nicer. But even in this scenario I don't see how deleting $b$ I can prove Jordan-Schoenflies for $b_1+b_2-b'$ . Would someone have a suggestion? 2) I think crossing a polygon is not well-defined in this paper. If I pass the straight line $y=1$ through the vertex of a triangle $(1,0), (0,1), (0,0)$ it doesn't have an endpoint interior to it. I suppose I can say look for each side of the polygon a small tubular neighborhood of it. Then I could define ""a polygonal path crosses a polygon if it intersects one side of the polygon, and that in a neighborhood of such intersection the path has points on both sides of the tubular neighborhood of the side it intersects on."" I was hoping there is an easier definition to work with and that makes this Lemma true. 3) In the middle of proving Lemma 3.1 he establishes (A) and (B), which is fine: but then he proceeds to prove (C): My question is about part (2). Here $\alpha$ is defined earlier by: He claims proving (C) presents no difficulty, but I was not able to do it yet. It's not clear to me how $\alpha$ as defined have to come from either of $\alpha_1$ or $\alpha_2$ . I would appreciate any nudge towards showing this point (2) of (C), Lemma 3.1.","['euclidean-geometry', 'general-topology', 'low-dimensional-topology']"
4206286,A double integration via Fubini theorem,"$$\int_0^1\int_0^\infty ye^{-xy}\sin x\,dx\,dy$$ How can I calculate out the value of this integral? P.S. One easy way is to calculate this integral over $dy$ first, to get an integral form $\frac{1-e^{-x}(x+1)}{x^2}\sin x$ , if I calculated correctly, but I don't know any way to calculate out this value other than a hard work with contour integral.  - so I wonder if there be a way other than this (=integrate over $dy$ and do contour integral) (I tried some integration by parts and substitutions but it seems it does not work well, probably the $\infty$ at the second integral is crucial. I think this requires capturing a term in the integrand and converting to a further integral, and reverting the order of integral by Fubini's theorem.. but I'm not sure)","['calculus', 'analysis', 'real-analysis']"
4206341,Why does the factor of 1/2 appear in implicit summation of 2-form?,"I've been watching some videos on differential forms, and I found this brilliant chap. What confuses me is at about 6:12, he calls the other component redundant, and I'm confused what allows him to do so. Consider two 1-forms, $\omega,\tau\in\Omega^1(\mathbb{R}^2)$ , and then consider their wedge \begin{align}
\epsilon & = \omega \wedge \tau\\
&= \omega_\mu \tau_\nu dx^\mu \wedge dx^\nu \\
& = \epsilon_{\mu\nu}dx^\mu \wedge dx^\nu
\end{align} using implicit summation. Expanding this sum, $$
\epsilon = \epsilon_{11}dx^1 \wedge dx^1 + \epsilon_{12}dx^1 \wedge dx^2 + \epsilon_{21}dx^2 \wedge dx^1 + \epsilon_{22}dx^2 \wedge dx^2,
$$ but since these ( $dx^\mu$ ) are $1$ -forms, $dx^\mu \wedge dx^\mu = 0$ , and $dx^\mu \wedge dx^\nu = - dx^\nu \wedge dx^\mu $ , so we may cut out two terms and reduce the others to \begin{align}
\epsilon & = \epsilon_{12} dx^1 \wedge dx^2 + \epsilon_{21} dx^2 \wedge dx^1 \\
& = (\epsilon_{12} -\epsilon_{21})dx^1 \wedge dx^2.
\end{align} Now, I know that this method (implicit summation) double counts, and hence the full expression reads $\epsilon = \frac{1}{2}\epsilon_{\mu\nu}dx^\mu \wedge dx^\nu$ in the end, but why can I say that $\epsilon_{12}=-\epsilon_{21}$ ? I know that for an antisymmetric matrix this must be true, but what allows me to say this just from the mathematics of wedge products, and the properties of the coefficients? Sorry if this is quite a rudimentary question, I'm just trying to get my head around it.","['exterior-algebra', 'differential-forms', 'differential-geometry']"
4206428,Is $x < y \implies f(x) < f(y)$ equivalent to $x < y \iff f(x) < f(y)$?,"Given a function $f: \mathbb{R} \to \mathbb{R}$ , is $x < y \implies f(x) < f(y)$ equivalent to $x < y \iff f(x) < f(y)$ . If $f(x) < f(y)$ , then the contradiction occurs only $_\text{(or is it not the ""only"" case)}$ when $x > y$ . If that happens, then $f(x) > f(y)$ and so the two statements are always equivalent. Am I missing something? Is it always the case or it changes when we change the injectivity, surjectivity, domain or anything else?",['functions']
4206463,"In a tournament of $2^n$ people, what is the probability of player $i$ and player $j$ meet at $k$-th round","In a knock-out tournament of $2^n$ people, where if $i < j$ , player $i$ is better than player $j$ and will beat her in any parts of the tournament. What is the probability of player $i$ and player $j$ will meet at $k$ -th round for any $1 \leq i \leq j \leq 2^n$ , $1 \leq k \leq n$ (assuming the positions of player at the start of tournament are all random)? This problem is inspired by the case $i=1,j=2,k=n$ , where the probability is $\frac{2^{n-1}}{2^n-1}$ , determined by the probability that player $2$ is at the different half of the player $1$ during the start. Similarly, player 3 will meet with player $1$ at the final tournament by probability $\frac{2^{n-1}-1}{2^n-1}\frac{2^{n-1}}{2^n-2}$ (player $2$ meet player $1$ first). And similarly for all other players by applying same logic. I'm curious about the general case that player $i$ and player $j$ will meet at $k$ -th round and am wondering if my rationale below is correct. We know that the situation happens only when player $i$ is the best player among the $2^k$ nearest player and player $j$ is the best in her $2^{k-1}$ nearest player. By combinatorics, it's equivalent to that fixed $i, j$ , among the $2^k-2$ numbers that are nearest to player $i$ during the contest, the random $2^{k-1}-1$ numbers are all greater than $j$ and the remaining $2^{k-1}-1$ are both greater than $i$ . So it's $$P(i,j,k) = \frac{\binom{2^n-j}{2^{k-1}-1}\binom{2^n-i - 2^{k-1}-1}{2^{k-1}-1}}{\binom{2^n}{2^{k}-2}} \frac{1}{2^{n-k+1}}$$ Where the $\frac{1}{2^{n-k+1}}$ is the additional probability that $i, j$ are in the two nearest $2^{k-1}$ groups. Is this correct?","['puzzle', 'combinatorics', 'probability']"
4206517,"Is $\sum_{n=1}^{\infty} \frac{\sin(n^2x)}{n}$ uniformly convergent on $(0,\pi)$?","I am trying to prove that the function $\sum_{n=1}^{\infty} \frac{\sin(n^2x)^2}{n^3}$ is not a fractal by showing that it has a well defined derivative (as fractals do not). In order to do that, I have to find out whether the function $\sum_{n=1}^{\infty} \frac{\sin(n^2x)}{n}$ is uniformly convergent on the interval $(0,\pi)$ . If it is, the original function is not a fractal! It is clear that using the Weierstrass M-test it can be shown that: $\sum_{n=1}^{\infty} \frac{\sin(n^2x)}{n^\alpha}$ where $\alpha > 1$ is uniformly convergent since $\sum_{n=1}^{\infty} \frac{1}{n^\alpha}$ converges and $|\frac{\sin(n^2x)}{n^\alpha}| \leq \frac{1}{n^\alpha}$ . Now the case where $\alpha = 1$ the function $\sum_{n=1}^{\infty} \frac{\sin(nx)}{n}$ (no $n^2$ in the sine) is the fourier trasnform of a sawtooth wave - so it converges uniformly everywhere except for when $x$ is a multiple of $\pi$ . I'm not sure if the function I'm investigating (with $n^2$ in the sine) would share a similar property. I have done quite a bit of research and it seems nobody has analysed this specific function yet and I'm a bit unsure as to how I can continue here. I believe that somehow the following substitution might help: $$\sum_{n=1}^{\infty} \frac{\sin(n^2x)}{n} = \sum_{n=1}^{\infty} \frac{1}{2in} (e^{i n^2 x} - e^{-i n^2 x})$$ But I can't get to any results from here either. It would be amazing if you could give me some pointers as I'm making no progress (I'm a non-math PhD student who is stuck figuring this out) and am wasting ungodly amounts of time on this without a solution in sight. Thanks so much for your help in advance! EDIT: It can be proven that $\sum_{n=1}^{\infty} \frac{\sin(n^2x)}{n}$ is pointwise convergent using Dirichlet's test fairly easily. <--- This is incorrect - there was a mistake in my derivation","['convergence-divergence', 'uniform-convergence', 'sequences-and-series']"
4206543,"Let $G$ be a group of order $pq$, where $p$, $q$ are distinct primes and $p<q$. Assume that $q$ $\not\equiv$ 1 mod$p$. Prove that $G$ is cyclic.","This is an exercise in Serge Lang’s Algebra in the first chapter. I am wondering why q $\not\equiv$ 1 mod $p$ is assumed considering it is unnecessary. Indeed, if that is excluded from the requirements, then let $H_q$ and $H_p$ be the Sylow subgroups of orders $q$ and $p$ , respectively. Then they are cyclic and thus have trivial intersection. Since they have trivial intersection, the product of groups $H_qH_p$ (which is a group since $H_q$ is normal) is isomorphic to $H_q$$\times$$H_p$ which has order $pq$ and so it is equal to $G$ . Considering $p$ and $q$ are coprime, $G$ is cyclic. Is this solution correct/an acceptable answer to this problem? If so, why is the aforementioned requirement provided? Note that all of the information used in my proof is either in the exercises preceding this one or in the chapter on Sylow subgroups.","['group-theory', 'finite-groups']"
4206553,Is this matrix positive semidefinite? $M_{ij} = \sqrt{|x_i+x_j|} - \sqrt{|x_i-x_j|}$ where $x_i$'s are reals,"Let finite number of $x_i$ 's be reals. Define matrix $M_{ij} = \sqrt{|x_i+x_j|} - \sqrt{|x_i-x_j|}$ . Is this matrix positive semidefinite? I am reading this year's IMO problem number 2, which would be trivially true if we prove that $M_{ij}$ is positive semidefinite. Problem $\boldsymbol2$ . Show that the inequality $$\sum_{i=1}^{n}\sum_{j=1}^{n}\sqrt{\left|x_i - x_j\right|} \leqslant \sum_{i=1}^{n}\sum_{j=1}^n\sqrt{\left|x_i + x_j\right|}$$ holds for all real numbers $x_1,...,x_n$ .","['matrices', 'inequality', 'linear-algebra', 'positive-semidefinite']"
4206576,General Formula for an arbitrary Rotation of Sphere,"I am currently reading F. Klein's book ""Lectures on the Icosahedron and the Solution of Equations of the Fifth Degree"" and in Part 1, Chapter 2, we desire to deduce a general formula for an arbitrary rotation of sphere. The idea is as follows: first we use stereographic projection to identify $S^2 = \{(\xi,\eta,\zeta)\in \mathbb{R}^3| \quad  \xi^2+\eta^2+\zeta^2=1\}$ with Riemann sphere $\mathbb{C} \cup \{\infty\}$ via the following formula: $z = x + i y = \frac{\xi+i\eta}{1-\zeta}$ . If we further identify $\mathbb{C}\cup \{\infty\}$ with $\mathbb{CP}^1$ , we can conclude every rotation will be represented by a fractional linear substitution. Now, given a rotation which fixes antipodal points $(\xi,\eta,\zeta), (-\xi,-\eta,-\zeta)$ , which correspondences to $\frac{\xi+i\eta}{1-\zeta}, -\frac{\xi+i\eta}{1+\zeta}$ in $\mathbb{C}\cup \{\infty\}$ and rotate through an angle $\alpha$ counterclockwise. Above process can be decomposed into two steps: first we move $-\frac{\xi+i\eta}{1+\zeta}$ to $0$ and move $\frac{\xi+i\eta}{1-\zeta}$ to $\infty$ , which corresponds to the following fractional linear substitution up to some constant: $$
C\cdot \frac{z+\frac{\xi+i\eta}{1+\zeta}}{z -\frac{\xi+i\eta}{1-\zeta} }
$$ Then we get our new $0,\infty$ axis and then we rotate our $(\xi',\eta')$ -plane (equatorial plane) by an angle $\alpha$ counterclockwise, which correspond to multipyling factor $e^{i\alpha}$ .
Suppose $z'$ is the coordinate after the rotation, we must have: $$
\frac{z'+\frac{\xi+i\eta}{1+\zeta}}{z' -\frac{\xi+i\eta}{1-\zeta} } = e^{i\alpha}\frac{z+\frac{\xi+i\eta}{1+\zeta}}{z -\frac{\xi+i\eta}{1-\zeta} } \quad(*)
$$ The author claimed that if we do the following change of notation: $$
\xi \sin(\frac{\alpha}{2})=a, \quad \eta \sin(\frac{\alpha}{2})=b, \quad \zeta\sin(\frac{\alpha}{2})=c, \quad \cos(\frac{\alpha}{2})=d,
$$ Then $(*)$ can be rewritten as $$
z' = \frac{(d+ic)z-(b-ia)}{(b+ia)z+(d-ic)} \quad (**)
$$ That is where I stuck. I think I understand the process of deduction of general rotation formula but I am not sure how to get the simple form $(**)$ . Now the problem is purely elementary and I try to verify $(**)$ by brute force but it doesn't seem to be a correct way . I guess it will involve with some trigonometric formulas to simplify the computation. Could you please offer me some suggestions on how to start from $(*)$ to derive $(**)$ ? Thank you in advance.","['analytic-geometry', 'geometry', 'trigonometry', 'complex-numbers', 'rotations']"
4206602,A uniform lower bound for $\left| |x-y|^{\alpha-2}(x-y)-|x-z|^{\alpha-2}(x-z) \right|$ when $|y-z|\geq 1$,"It is shown in the post Is this function on $\mathbb{R}^{2}$ always positive? that the modulus $|f|$ of the function $f:\mathbb{R}^{2} \rightarrow \mathbb{R}^{2} $ given by : $$f(x):=\alpha\left( |x-y|^{\alpha-2}(x-y)-|x-z|^{\alpha-2}(x-z) \right)$$ where $\alpha >0,\alpha \neq 1$ is strictly positive when $y\neq z$ . I am trying to get a uniform lower bound of $|f|$ given that $|y-z|> 1$ . By uniform I mean in uniform in $x$ . Any hints ? Some thoughts:
Let $t=x-y$ . Denote $\eta=y-z$ so that $x-z=t+\eta$ . Then it suffices to find a (uniform in $t$ ) lower bound for $$g(t):=\left| |t|^{\alpha-2}t-|t+\eta|^{\alpha-2}(t+\eta) \right|$$ I had earlier attempted the bound $$\left| |t|^{\alpha-2}t-|t+\eta|^{\alpha-2}(t+\eta) \right|\geq
\left| |t|^{\alpha-1}-|t+\eta|^{\alpha-1} \right|$$ but the latter is not bounded away from zero because (as @Calvin Khor pointed out) it attains zero at $t=-\eta/2$ . An update :
When $x$ lies on the perpendicular bisector $L$ of $\overline{yz}$ we have $|f(x)|\geq \frac{1}{2^{\alpha-2}}|y-z|^{\alpha-1}$ .
Why?
For every $x$ on the bisector we have $|x-y|=|x-z|$ . Therefore, for any $x\in L$ , $|f(x)|=|x-y|^{\alpha-2}|x-y-(x-z)|=|x-y|^{\alpha-2}|y-z|$ .
If $x$ is the midpoint of $\overline{yz}$ then $|x-y|=\frac{1}{2}|y-z|$ .
If $x \in L$ then $|x-y|>\frac{1}{2}|y-z|$ ,","['harmonic-analysis', 'inequality', 'analysis', 'real-analysis']"
4206676,Prove that $(\mathbb{R}^{n+1} \backslash \{0\}) / \sim_1$ and $\mathbb{S}^n/ \sim_2$ are homeomorphic.,"(a) Take the unit sphere $S^n$ in $\mathbb{R}^{n + 1}$ and partition it into subsets which contain exactly two points, the points being antipodal (at opposite ends of a diameter). $P^n$ is the resulting identification space. We could abbreviate our description by saying that $P^n$ is formed from $S^n$ by identifying antipodal points. (b) Begin with $\mathbb{R}^{n + 1} \backslash \{0\}$ and identify two points if and only if they lie on the same straight line through the origin. (Note that antipodal points of $S^n$ have this property.) Prove that $[b]$ and ( $\mathbb{S}^n/ \sim$ )(...[a]) are homeomorphic where $\sim$ denotes the identification of the antipodal points. $\require{AMScd}$ \begin{CD}
    \mathbb{R}^{n+1}\backslash\{0\} @>g(x) = x /||x||>> S^{n} \\
     \ @VV\pi(x) = cl(x)V \\
     \ @. \mathbb{S}^{n} / \sim  \\
\end{CD} We see that $g(x)$ is a continuous surjective map.Also we know that $\pi(x)$ is a surjective map from the compact space( $\mathbb{S}^n$ ) to the hausdroff space ( $\mathbb{S}^n/ \sim)$ so it is an identification map. I know of the theorem that , $\require{AMScd}$ \begin{CD}
    \mathbb{X} @>g(x)>> Y \\
     @VV p V  \\
     \  \mathbb{X} / \sim  \\
\end{CD} If $g(x)$ is an identification map and $p$ is the projection map then we know that $Y$ is homeomorphic to $\mathbb{X} / \sim$ . This is what I could come close to. Can someone help me out from here instead of suggesting some other answer? I did go through the various answers on stackexchange and nothing seems to help me as I dont really get the intuition. edit1:After PaulFrost's answer I proceeded in the following way : Let $\require{AMScd}$ \begin{CD}
    \mathbb{S^{n}} @>h(x)=x>> \mathbb{R}^{n+1} \backslash \{0\} @>\pi(x)=cl(\{x\})>> (\mathbb{R}^{n+1} \backslash \{0\})/\sim_1 \\
     @. @. @.   \\
\end{CD} where $\pi(x)$ is a surjective map so $\pi \circ h(x) $ is surjective and since $\mathbb{S}^n$ is compact and $(\mathbb{R}^{n+1} \backslash \{0\})/ \sim_1$ is Hausdorff so we can conclude that $\pi \circ h(x)$ is the identification map. Now, $\require{AMScd}$ \begin{CD}
    \mathbb{S^{n}} @>h(x)=x>> \mathbb{R}^{n+1} \backslash \{0\} @>\pi(x)=cl(\{x\})>> (\mathbb{R}^{n+1} \backslash \{0\})/\sim_1 \\
     @VV \pi_1(y) = cl(\{y\}) V  @. @.   \\
     \mathbb{S}^n / \sim_2 \\
\end{CD} So we can conclude that $(\mathbb{R}^{n+1} \backslash \{0\} ) / \sim_1 $ is isomorphic to $\mathbb{S}^n / \sim_2$","['general-topology', 'analysis']"
4206768,Prove that $\int_{1}^{+\infty} \arctan\left(\frac{1}{t}\right)\tanh(t)\operatorname{sin}(t)dt$ exists and is a finite value.,"I'm trying to prove that this limit $$\int_{1}^{+\infty} \arctan\left(\frac{1}{t}\right)\tanh(t)\operatorname{sin}(t)dt$$ exists and it's finite. I have proved that $$\int_{1}^{+\infty} \left|\arctan\left(\frac{1}{t}\right)\tanh(t)\operatorname{sin}(t) \right|dt = +\infty$$ so I can't use summability. My other attempt to solve the given integral is the fact that $f(t)=\arctan\left(\frac{1}{t}\right)\tanh(t)\sin(t)$ have the same behaviour of the function $$g(t)=\frac{\sin(t)}{t}$$ as $t\to +\infty$ and I succeded in proving that $\int_{1}^{+\infty} g(t)dt$ converges. But I know from theory that the asymptotic method can only be used if the integrands have a constant sign. So how can I use what I achieved to prove that the given integral converges?
And, more in general, when the integrand has a variable sign what criterion can I use? Can I use something similar to the asymptotic method to deduce that two improper integrals have the same behavior? PS: another attempt was to use the integral criterion for the series with the general term $f(n)$ and the Dirichlet's test, but the problem is that I can't use the integral criterion since the integrand is not decreasing. So I have another question: is there a way to extend the integral criterion? (for example for functions that are not decreasing)","['integration', 'improper-integrals', 'real-analysis']"
4206773,Finding the maximum value for $\left\lceil\frac{b-x}{2}\right\rceil + \left\lceil\frac{x-a}{2}\right\rceil$,"I have given two numbers $a$ and $b$ , we have to choose an optimal $x$ (with $a\le x\le b$ ) such that $$\left\lceil\frac{b-x}{2}\right\rceil + \left\lceil\frac{x-a}{2}\right\rceil = \text{maximum}$$ I thought the optimal way to choose $x$ we are such that it got canceled from both sides. Therefore, maximum sum will be $$\left\lceil \frac{b}{2}\right\rceil + \left\lceil-\frac{a}{2}\right\rceil$$ It works for some cases, but what am I missing?","['optimization', 'ceiling-and-floor-functions', 'functions']"
4206786,When is $\operatorname{Spec} A^G \cong (\operatorname{Spec} A)/G$ true?,"Let $G$ be a group acting on a ring $A$ . I would like to know in which generality we know that $\operatorname{Spec} A^G \cong (\operatorname{Spec} A)/G$ . Moreover, when this is true, it also holds for the underlying topological spaces? I know that both facts hold when $G$ is finite (but I don't have a proof and I would be grateful if someone sketched one or commented where I can find it). But I'm also interested in more general groups. In particular, I wonder if it holds for profinite groups. Edit: let me be more precise. If $G$ is a finite group we can define a scheme $(\operatorname{Spec} A)/G$ whose underlying topological space is the usual quotient and whose structure sheaf is simply $(\pi_*\mathcal{O}_{\operatorname{Spec}A})^G$ , where $\pi$ is the canonical projection. This satisfies a universal property in the category of schemes. In this case, I don't know how to prove that this scheme is isomorphic to $\operatorname{Spec}A^G$ . For infinite groups, the quocient as defined above need not be a scheme. But it exists nevertheless as a topological space. In this case, it is still true that the topological space $(\operatorname{Spec} A)/G$ is homeomorphic to $\operatorname{Spec}A^G$ ?","['algebraic-geometry', 'abstract-algebra', 'moduli-space', 'algebraic-groups']"
4206788,Limit of $\exp(f(x))$ implies existence of limit of $f(x)$,"My question comes from this observation: If $f(x)$ is a complex-valued, continuous function on interval $(0,1)$ , and we know that limit of $f(x)$ at $x=1$ exists and finite, then we know that limit of $\exp(f(x))$ at $x=1$ exists, since $exp(x)$ function is continuous. I guess that the converse still holds: if we know that limit of $\exp(f(x))$ at $x=1$ exists and it is not 0, then limit of $f(x)$ at $x=1$ exists. But I am stuck here, since the logarithmic function is not single-valued. Thus, even if we know the limit of $\exp(f(x))$ at $x=1$ , we don't even know what the limit of $f(x)$ at $x=1$ could be. How do I approach this problem? Any help is appreciated.","['complex-analysis', 'analytic-functions', 'real-analysis']"
4206791,Any trick for evaluating $\left(\frac{\sqrt{3}}{2}\cos(\theta) + \frac{i}{2}\sin(\theta)\right)^7$?,"Expressions of the form $(a\cos(\theta) + bi\sin(\theta))^n$ come up from time to time in applications of complex analysis, but to my knowledge the De Moivre's formula can only be applied with $a = b$ . Is there some trick to deal with the case of $a \neq b$ , for example when the expression is $\left(\frac{\sqrt{3}}{2}\cos(\theta) + \frac{i}{2}\sin(\theta)\right)^7$ ?","['complex-analysis', 'trigonometry', 'complex-numbers']"
4206825,"Suppose $G$ is a cyclic group and $a,b\in G$. There doesn't exist any $x\in G$ such that $x^2=a$ and does not exist any $y\in G$ such that $y^2=b$","Suppose G is a cyclic group and $a,b \in G$ . There doesn't exist any $x \in G$ such that $x^2=a$ . Also, there does not exist any $y \in G$ such that $y^2=b$ then (a) there exists an element $g \in G$ such that $g^2=ab$ (b) there exists an element $g \in G$ such that $g^3=ab$ (c) the smallest exponent $k>1$ such that $g^k=ab$ for some $g \in G$ is $4$ (d) none of the above My intuition says $a$ and $b$ must be odd powers of generator say $p$ . So, we can write $a=p^{2n+1}$ and $b=p^{2m+1}$ so $ab=p^{2n+2m+2}$ which gives us option (a). Please verify if the approach is right.","['group-theory', 'cyclic-groups']"
4206840,$p$-adic structure of Pell-type equations,"As an example, consider an integer solution of $ x ^ 2-3y ^ 2 = 13 $ . $ y $ that satisfies this equation is $y_k = \frac {(4+ \sqrt {3}) (2+ \sqrt {3}) ^ k-(4- \sqrt {3}) (2- \sqrt {3}) ^ k} { 2 \sqrt {3}} $ for any integer $ k $ . By  calculation $ 3 | y_k \Leftrightarrow k \equiv1 \mod 3 $ $ 3 ^ 2 | y_k \Leftrightarrow k \equiv7 \mod 3 ^ 2 $ $ 3 ^ 3 | y_k \Leftrightarrow k \equiv7 \mod 3 ^ 3 $ $ 3 ^ 4 | y_k \Leftrightarrow k \equiv7 \mod 3 ^ 4 $ $ 3 ^ 5 | y_k \Leftrightarrow k \equiv7+2\cdot 3^4 \mod 3 ^ 5 $ $ 3 ^ 6 | y_k \Leftrightarrow k \equiv7+2\cdot 3^4+2\cdot 3^5 \mod 3 ^ 6 $ You can see that it has a regular structure.
In other words, a solution divisible by $3^d$ appear at intervals of $3$ powers and do not appear anywhere else.
I tried similar calculations for other Pell-type equations, and I was convinced that they had a similar structure. Can you prove this structure in general? We consider the Pell-type equation $ x ^ 2-py ^ 2 = N $ with a solution. For any $ d $ $ p ^ d | y_k \Leftrightarrow k \equiv r \mod p^{d+e}. $ $ r $ should be uniquely determined as $ p $ -adic number.","['algebraic-number-theory', 'number-theory', 'p-adic-number-theory', 'elementary-number-theory', 'pell-type-equations']"
4206880,Prove that $\int_0^{\infty} \frac{1-\cos(at)}{t^{1+\alpha}} dt = \frac{\pi}{2 \Gamma(\alpha+1) \sin (\alpha \pi /2 )} |a|^{\alpha}$,$$\int_0^{\infty} \frac{1-\cos(at)}{t^{1+\alpha}} dt = \frac{\pi}{2 \Gamma(\alpha+1) \sin (\alpha \pi /2 )} |a|^{\alpha}$$ Is there a name for this integral? I saw someone refers to this integral as a well-known property. Is it from some probability distribution? We might be integrated by parts like $$\int \frac{\cos(at)}{t^{1+\alpha}} dt = \int \frac{d\sin(at)}{at^{1+\alpha}} $$ but it seems rather painful and I am going down a recursive rabbit hole.,"['integration', 'indefinite-integrals', 'definite-integrals']"
4206902,Motivation and references for 'the topology of uniform convergence',"I'm a graduate student working on $\textbf{algebraic number theory}$ . While reading papers, I have seen authors mentioning that the group of characters is equipped with the topology of uniform convergence. I tried to understand the definition of the topology of uniform convergence but it seemed to be out of scope of the introductory course on general topology. I mean it seem to be using the concept of $\textbf{uniform space}$ . And I don't have enough time to read books like Bourbaki or Stephen Willard. So could you please recommend me short exposition on the topology of uniform convergence? or explain to me what is the point of this concept? Thank you very much.","['uniform-spaces', 'general-topology', 'reference-request']"
4206907,Solve: $2^{\cos^{2014}x} - 2^{\sin^{2014} x} = \cos^{2013} (2x)$,"I have encountered this in a Romanian Mathematical magazine at the 10th grade section (so using more advanced things like Calculus shouldn't be necessary). Solve: $$2^{\cos^{2014} x} - 2^{\sin^{2014} x} = \cos^{2013}(2x)$$ My first approach was to solve it in the interval $[-2\pi, 2\pi)$ and I tried dividing it into different subintervals to try and work with increasing/decreasing functions. This however fails as there are some intervals in which both side have the same property. Then, I tried working with inequalities, particulary the AM-GM Inequality, but I could not solve it either. Have you got any clues?","['contest-math', 'trigonometry', 'exponential-function']"
4206933,"If $f \circ f =f$, then what is $f$?","This is an exercise from Mathematical Analysis II by Zorich:( the surface here is the same as manifold, not necessarily 2-dimensional.) Let $f:\mathbb R^n
 \to \mathbb R^n$ be a smooth mapping satisfying the condition $f \circ f =f$ .
a) Show that the set $f(\mathbb R^n)$ is a smooth surface in $\mathbb R^n$ .
b) By what property of the mapping $f$ is the dimension of this surface determined? My solution: $$f'(f(x))f'(x)=f'(x)\Rightarrow \forall x\in f(\mathbb R^n),f(x)=x,(f'(x)-I)f'(x)=O.\tag1$$ We can conclude that eigenvalue of $f'(x)$ is $0$ or $1$ . Let $P^{-1}(x)f'(x)P(x)=J(0)+\begin{pmatrix}I_r&O\\O&O\end{pmatrix}$ where $|P(x)|\neq0$ , and according to (1) we can know that $J(0)=O$ . Since $tr(f'(x))=rank(f'(x))$ is continous on $f(\mathbb R^n)$ , and $f(\mathbb R^n)$ is connected, we can know that $\forall x\in f(\mathbb{R}^n), rank(f(x))\equiv r$ . $\forall x \in \mathbb{R}^n, (P^{-1}(x)\begin{pmatrix}I_r&O\\O&O\end{pmatrix}P(x)-I)f'(x)=O\Rightarrow rank(f'(x))\leq r$ $\forall f(x_0)\in f(\mathbb{R}^n),\ rank (f'(f(x_0)))=r, \forall x\in O(f(x_0)),rank(f'(x))\leq r\Rightarrow \exists \bar O(f(x_0)),\forall x\in \bar O(f(x_0)), rank(f'(x))=r$ . According to The rank theroem, we can find $\tilde O(x_0), \tilde O(f(x_0))$ and diffeomorphisms $\phi ,\varphi,f=\phi\circ\pi\circ\varphi^{-1}$ where $ \pi:(u^1,\cdots,u^r,\cdots,u^m)\mapsto(u^1,\cdots,u^r)$ , so that the set $f(\mathbb R^n)$ is a smooth surface in $\mathbb R^n$ and the dimension depends on the rank of $f$ on $f(\mathbb R^n)$ . My doubts: (1) Is my solution above correct? Feel free to point out any mistakes or offer any concise solutions. (2)Does $f$ have to be a linear mapping with the form $f(x)=P^{-1}\begin{pmatrix}I_r&O\\O&O\end{pmatrix}P$$\cdot$$x$ ? (3)If we only assume $f\in C^0(R^m,R^m)$ , what properties of $f$ can we get? Is $f(\mathbb R^n)$ still a surface?","['functional-equations', 'multivariable-calculus', 'smooth-manifolds', 'real-analysis']"
4206965,I don't know how to exactly compute this determinant,"I've tried to compute this determinant by row transformations and column transformations, but it gives me a formula that doesn't work. The determinant is: \begin{vmatrix}
x & a & b & c & d\\
a & x & b & c & d\\
a & b & x & c & d\\
a & b & c & x & d\\
a & b & c & d & x
\end{vmatrix} I thought I could start doing row 5 - row 4, row 4 - row 3, row 3 - row 2 and row 2 - row 1 and then you get this determinant: \begin{vmatrix}
x & a & b & c & d\\
a-x & x-a & 0 & 0 & 0\\
0 & b-x & x-b & 0 & 0\\
0 & 0 & c-x & x-c & 0\\
0 & 0 & 0 & d-x & x-d
\end{vmatrix} Then I made column 1 - column 2, column 2 - column 3, column 3 - column 4, column 4 - column 5 and you get: \begin{vmatrix}
x-a & a-b & b-c & c-d & d\\
0 & x-a & 0 & 0 & 0\\
0 & 0 & x-b & 0 & 0\\
0 & 0 & 0 & x-c & 0\\
0 & 0 & 0 & 0 & x-d
\end{vmatrix} And, as it is triangular, you can multiply the diagonal elements, so you get that the determinant is: $(x-a)^2(x-b)(x-c)(x-d)$ But this isn't correct and I don't know what to do, could someone please help me? I'd really appreciate.","['determinant', 'solution-verification', 'linear-algebra']"
4206980,How to prove that a rational function is a polynomial,"$\newcommand\Q{\mathbb Q}$ I have a rational function $f(\vec x)$ over $\Q$ in many variables (7 variables). I do not know what $f$ is, but I can evaluate it for random choices of $x_1,\dots,x_7\in \Q$ without $x_i$ for any $i=1,\dots, 7$ . I always get a polynomial over $x_i$ . This indicates to me that $f$ may indeed be a polynomial. If I can evaluate $f$ in this way, is there a smart way to prove that $f$ is a polynomial? Edit : We may assume that we know what the upper bound of the total degrees of the numerator and the denominator are.","['functions', 'rational-functions', 'polynomials']"
4206994,Is an irreducible algebraic curve over $\mathbb{C}$ uniquely determined by a local parametrization?,"Let $F(X, Y)$ be an irreducible polynomial of $\mathbb{C}[X, Y]$ with $F(0, 0) = 0$ and non-singular there, and let $(X(t), Y(t))$ be a local analytical parametrization of the corresponding curve, that is, $F(X(t), Y(t)) = 0$ for $t$ in a neighborhood of $0$ , with $X(0) = Y(0) = 0$ and $X'(0) \neq 0$ . If, for some polynomial $G(X, Y)$ , we have $G(X(t), Y(t)) = 0$ for $t$ in the neighborhood in question, does then $G(X, Y)$ necessarily belong to the ideal generated by $F(X, Y)$ ? In other words, can the whole curve $F(X, Y) = 0$ be reconstructed from its local snippet? And for an arbitrary algebraically closed field, is there a well-known characterization for a subset of an irreducible algebraic curve to define the curve already ? Is it for instance enough for such a subset to be infinite ?","['algebraic-curves', 'analytic-continuation', 'algebraic-geometry', 'parametrization', 'differential-geometry']"
4206996,Prove that a simplex is a manifold with corners,"First of all  we remember some elementary definitions and results about manifolds and simplexes. Definition A function $f$ defined in a subset $S$ of $\Bbb R^k$ is said of class $C^r$ if it can be extended to a function $\phi$ (said $C^r$ -extension) that is of class $C^r$ in a open neighborhood of $S$ . Lemma If $f$ is a function defined in a subset $S$ of $\Bbb R^n$ such that for any $x\in S$ there exists a function $f_x$ defined in a neighborhood of $x$ that is of class $C^r$ and compatible with $f$ on $U_x\cap S$ then $f$ is of class $C^r$ . Lemma If $U$ is an open set of $H_n:=[0,+\infty)^n$ then the derivatives of two different extensions $\phi$ and $\varphi$ of a $C^r$ -function $f$ agree in $U$ . Definition A $k$ -manifold with corners in $\Bbb R^n$ of class $C^r$ is a subspace $M$ of $\Bbb R^n$ whose points have a neighborhood $V$ in $M$ that is the immage of a homeomorphism $\phi$ of calss $C^r$ defined an open set $U$ of $\Bbb R^k$ or of $H_k$ and whose derivative has rank $k$ . Definition If $x_0,...,x_k$ are $(k+1)$ affinely indipendent points of $\Bbb R^n$ (which means that the vectors $(x_1-x_0),...,(x_k-x_0)$ are linearly independent) then simplex determined by them is the set $$
\mathcal S:=\Biggl\{x\in\Bbb R^n: x=x_0+\alpha^i\vec v_i\,\,\,\text{and}\,\,\, \sum_{i=1}^k\alpha^i\le1\,\,\,\text{and}\,\,\,\alpha^i\ge0\,\,\,\text{for all}\,i=1,\dots,k\Biggl\}
$$ where $\vec v_i:=(x_i-x_0)$ for each $i=1,\dots,k$ . With the previous definition we let try to prove that any simplex is a $k$ -manifold with corners. First of all we define the function $f$ from $\Bbb R^k$ to $\Bbb R^n$ by putting $$
f(x):=x_0+x^1\vec v_1+\dots+x^k\vec v_k
$$ for each $x\in\Bbb R^k$ and thus we observe that it is a smooth homeomorphism because it is the composition of a translation with a linear and injective map betweeen finite dimensional topological vector spaces -for details about this statement we refer to to the text Functional Analysis of Walter Rudin. So if we show that $f^{-1}[S]$ is a manifold with corners then the statement follows immediately. Then we observe that $$
f^{-1}[\mathcal S]=\Big\{x\in\Bbb R^k:x^1+\dots+x^k\le 1\,\,\,\text{and}\,\,\,x^i\ge0\,\,\,\text{for all}\,\,\,i=1,\dots,k\Big\}
$$ and thus we conclude that the set $f^{-1}[\mathcal S]$ is the standard $k$ -simplex $\mathcal E_k$ generated by the point $O,O+\hat e_1,\dots,O+\hat e_k$ which (if this can interest) is equal to the intersection of a $k$ parallelotopes $\mathcal P_1,\dots,\mathcal P_k$ with the not negative space $H^k_k$ due to analogous arguments here advanced. Now we find a coordinate patch about any point $x$ of $\mathcal E_k$ and we are doing this assuming that it is $$
x^1+\dots+x^k\lneq 1\,\,\,\,\,\,\text{or either}\,\,\,x^1+\dots+x^k=1
$$ Now the function scalar $g$ defined in $\Bbb R^k$ through the equation $$
g(x):=x^1+\dots+x^k
$$ for any $x\in\Bbb R^k$ is trivially continuous (indeed it is a liner map between finite dimensional topological vector spaces or alternatively is finite sum of continuous functions) so that the set $g^{-1}\big[(0,1)\big]$ is open and thus observing that $$
g^{-1}\big[(0,1)\big]\cap\mathcal E_k=\big\{x\in\Bbb R^k:0\lneq x^1+\dots+x^k\lneq 1\big\}\cap\big\{x\in\Bbb R^k:x^1+\dots+x^k\le 1\,\,\,\text{and}\,\,\,x^i\ge0\,\,\,\text{for all}\,\,\,i=1,\dots,k\big\}=\\
\big\{x\in\Bbb R^k:0\lneq x^1+\dots+x^k\lneq 1\big\}\cap\big\{x\in\Bbb R^k:x^i\ge0\,\,\,\text{for all}\,\,\,i=1,\dots,k\big\}=g^{-1}\big[(0,1)\big]\cap H^k_k
$$ we conclude that the restriction of the identity map to the set $g^{-1}\big[(0,1)\big]\cap H^k_k$ is a coordinate patch about $x$ in an open set of the not negative space $H^k_k$ when in the first case it is $$
x^1+\dots+x^k\neq 0
$$ too. Otherwise $\mathcal E_k$ is contained into the not negative space $H^k_k$ so that if $x\in\mathcal E_k$ is such that $$
x^1+\dots+x^k=0
$$ then $x$ is just the origin $O$ and thus if we find an open cube centered at $O$ whose intersection with $H^k_k$ is contained in $\mathcal E_k$ then the restriction of the identity map to the intersection between $H^k_k$ and this open cube is a coordinate patch about $O$ defined in an open set of $H^k_k$ . So observing that if $x$ is an element lying in the intersection between $H^k_k$ and the open cube $C\Big(O,\frac1k\Big)$ centered at $O$ and of radius $\frac1k$ then $$
x^1+\dots+x^k\le k\cdot\max\big\{|x^i|:i=1,\dots,k\big\}=k\cdot\| x-O\|_\infty<k\cdot\frac1 k=1
$$ so that the statement follows immediately. Now we have to find a coordinate patch supposing that $x$ is an element of $\mathcal E_k$ such that $$
x^1+\dots+x^k=1
$$ but unfortuantely I have some difficulties and thus I ask to do it. Could someone help me, please? $$
\underline{\text{**ACHTUNG**}}
$$ Courteously I ask not to prove the statement by showing that the simplex $\mathcal E_k$ is a trasversal intersection between manifolds with corners because I studied trasversality ONLY for manifold WITHOUT boundary and unfortunately I did NOT REALLY understand the proof of trasversality theorems for manifold with corners: moreover I have to prove the result for pratical purposes (I am studying Continuum Mechanics) and thus I really need to find a coordinate patch about the points of the obliqual face of $\mathcal E_k$ , that's all. Thanks for your attention if you have read what is written above.","['general-topology', 'solution-verification', 'differential-geometry']"
4207028,What is the name for the unreachable part of a function's codomain?,"A function associates each element from a ""domain"" set/space with an element from a ""codomain"" set/space. It does not have to associate all the elements in the codomain. If it does use them all, it is ""surjective"". I would like to know, what is the name for the set/space of elements in the codomain that are not used by a non-surjective function? I am particularly interested in the context of linear mappings. The space I would like the name of is somewhat like the opposite of the nullspace.","['notation', 'functions', 'linear-algebra', 'linear-transformations', 'terminology']"
4207043,"How to rewrite $\int\limits_A^B \frac{x^n \exp(-\alpha x)}{\small(x + \beta\small)^m} \, dx$?","Currently, I am a post-graduate researcher in Telecommunications. During the process of evaluating the transmission error probability, I need to evaluate the following integral $I = \int\limits_A^B \frac{x^n \exp(-\alpha x)}{\small(x + \beta\small)^m} \, dx$ ? How to rewrite this improper integral in terms of special function (for example $Ei(x)$ , Bessel,...)? Notice that $A, B, \alpha,\beta > 0$ (positive real number) and $m,n$ are two positive integers. I have tried to compute this integral with different values of $A, B, \alpha,\beta > 0$ and $m,n$ by using Wolfram Mathematica. It seems that the results of this integral have a form of the exponential integral function $Ei\left( x \right) = \int\limits_{t =  - x}^\infty  {\frac{{{e^{ - t}}}}{t}dt}  = \int\limits_{t =  - \infty }^{t = x} {\frac{{{e^t}}}{t}dt}$ as: $I = C_1\bigg[C_2 + C_3\big[ {\rm Ei}\big(- \alpha(\beta+ A)\big) - {\rm Ei}\big(- \alpha(\beta+ B)\big) \big] \bigg]$ . Are there any way to find out the correct values of $C_1$ , $C_2$ , and $C_3$ . Thank you for your enthusiasm!","['integration', 'improper-integrals', 'special-functions']"
4207060,Is my understanding of divisibility correct? (Divisibility with negative numbers),"From what I am getting from my textbook: $a\rvert b$ if there exists at least one $c∈Z^+$ such that $ac=b$ So let's say I need to show that 2 divides 10. Then from the definition of divisibility, there exists some $c∈Z^+$ such that $2c=10$ . We can then say $c=5$ and since $5∈Z^+$ then ultimately we can say that $2|10$ . So now let's say I need to show 1 divides $-5$ . Then from the definition of divisibility, there exists some $c∈Z^+$ such that $(1)(c)=(-5)$ . Then we can say that $c=(-5)/(1)=(-5)$ but $(-5)∉Z^+$ . So then I can say that 1 does not divide $(-5)$ ? Is my understanding of this correct?","['divisibility', 'discrete-mathematics']"
4207063,Probability of creating two same groups,"There are 10 different types of stickers. We have a collection of 20 stickers, where each sticker appears twice (each and every sticker has a duplicate). Now we are asked to put 10 stickers in two bags (each bag contains exactly 10 stickers), when there are no same stickers in every bag, so they are distinct. What is the probability of this arrangement? My attempt was to think about ways to choose an $x$ which will suit the general formula $\cfrac{ x}{{20 \choose 10}}$ but with no success. I also thought about calculating the probability using the intuitive method of choosing the first bag: $\cfrac{2}{{20}}⋅\cfrac{2}{{19}}⋅\cfrac{2}{{18}}⋅\cfrac{2}{{17}}⋅\cfrac{2}{{16}}⋅\cfrac{2}{{15}}⋅\cfrac{2}{{14}}⋅\cfrac{2}{{13}}⋅\cfrac{2}{{12}}⋅\cfrac{2}{{11}}$ When we have 10 iterations of probabilities of choosing one of 2 different stickers out of what we have at the moment of choosing. But I was wrong. The book says that the probability is just: $$\cfrac{ 2^{10}}{{20 \choose 10}}$$ Which looks like a combination of my two attempts. I still don't get the point. It seems like a relatively easy question, but I find it confusing.","['combinatorics', 'probability']"
4207093,Is there any difference in There exist and For some?,When the backwards E notation $\exists$ is shown I've been told that it can mean There exist but I've also been told that it could mean For some. Is there a difference between one or the other? Do I have to use one for a certain circumstance?,"['elementary-set-theory', 'notation', 'logic', 'quantifiers']"
4207094,Generalized second derivative of a concave and piecewise $C^2$ function,"It is mentioned in page 20 of this paper that if $f: \mathbb{R}_+ \to \mathbb R$ is a concave and piecewise $C^2$ function, then the generalized second derivative of $f$ is a signed measure $\mu_f$ such that $\mu_f(\mathrm{d}r) \leq f''(r)\mathrm{d}r$ . I am wondering if anyone can help me understand the relation $\mu_f(\mathrm{d}r) \leq f''(r)\mathrm{d}r$ . As far as know, a concave function $f$ has the left hand derivative $f'_{-}(x)$ for every $x$ , and $f'_{-}$ is left-continuous and non-increasing, so we can define a Borel measure on $\mathbb{R}_+$ by the prescription $$\mu_f([x,y)) = f'_{-}(x) - f'_{-}(y),$$ and we can also denote this measure by $f''(\mathrm{d}x)$ . May I know how the authors of the aforementioned paper can claim the sentence in bold letters?","['measure-theory', 'real-analysis', 'functional-analysis', 'convex-analysis', 'weak-derivatives']"
4207099,Sum of $\dfrac{1}{n^2+1}$ using Parseval's theorem,"I know this question has been widely answered here, but without using Fourier analysis. Also there is a video referring to this trick but I want to use a different Fourier series. First of Parseval's Theorem states: $\displaystyle{\dfrac{1}{\pi}\int_{-\pi}^{\pi}[f(x)]^2 = [a_0]^2+\sum_{n=1}^{\infty}[a_n]^2+[b_n]^2}$ . I calculate $a_0$ like $\frac{1}{2\,\pi}\,\int_{-\pi}^{\pi}f(x)\,\mathrm{dx}$ instead of $\frac{1}{\pi}\,\int_{-\pi}^{\pi}f(x)\,\mathrm{dx}$ , so no need for $\frac{1}{2}\,a_0$ Here I'd like to involve the series of $\cosh(x)$ whose partition I have been asked for in a prior question. It should be: $\displaystyle{\cosh(x) = \underbrace{\dfrac{\sinh(\pi)}{\pi}}_{a_0}+\sum_{n = 1}^{\infty}\underbrace{\color{red}{2}\,\dfrac{1}{\pi}\,\dfrac{\sinh(\pi)}{1+n^2}\,(-1)^n\,\cos(n\,x)}_{a_n}}$ Plugging those terms into the original theorem: $$\begin{align}
&\dfrac{1}{\pi}\int_{-\pi}^{\pi}\cosh^2(\pi) = \left(\dfrac{\sinh(\pi)}{\pi}\right)^2+\sum_{n=1}^{\infty}\left(\color{red}{2}\,\dfrac{1}{\pi}\,\dfrac{\sinh(\pi)}{1+n^2}\,(-1)^n\right)^2 \\\\
& 1+\dfrac{\sinh(2\,\pi)}{2\,\pi}= \dfrac{\sinh^2(\pi)}{\pi^2}\,\left[1+\sum_{n=1}^{\infty}\color{red}{4}\,\left(\dfrac{1}{1+n^2}\right)^2\right]\\\\
&\sqrt{\left(\dfrac{\pi^2}{\sinh^2(\pi)}+\dfrac{\sinh(2\,\pi)\,\pi}{2\,\sinh^2(\pi)}-1\right)\,\color{red}{\dfrac{1}{4}}} = \sum_{n=1}^{\infty}\dfrac{1}{1+n^2} \quad ?
\end{align}$$ Actually the value is coming close to the approximated sum, but it's not exactly the same result... Edit I fixed the Fourier Series highlighting the missing term in red. On the other hand I eradicated some factors, not sure about making it worse. It's still differing from the approximation. approximation $\approx 1,0767$ Therefore it works exactly like suggested by Stefan Lafon in the remarks: setting $x=\pi$ $\begin{align}
&\cosh(\pi) = \dfrac{\sinh(\pi)}{\pi}+\displaystyle{\sum_{n=1}^{\infty}2\,\dfrac{1}{\pi}\,\dfrac{\sinh(\pi)}{1+n^2}}\\\\
&\Rightarrow \quad \dfrac{\cosh(\pi)\,\pi}{2\,\sinh(\pi)}-\dfrac{1}{2} =\displaystyle{\sum_{n=1}^{\infty}\dfrac{1}{1+n^2}}
\end{align}$ It just remains a mystery why the method on top fails","['fourier-analysis', 'parsevals-identity', 'sequences-and-series']"
4207106,"How to find functions which are ""good (lower) bounds"" of trigonometric sequences such as $\vert\sin(n)\vert$ and $1-\sin(n)$","I know that $\{ \sin(n): n\in\mathbb{N} \}\ $ is dense in $[0,1].$ Furthermore, $\ \forall n\in\mathbb{N},\ \sin(n)\neq 0.\ $ So I am wondering: For each of the following, does there exist some minimal $p>0$ and some $\ N\in\mathbb{N}\ $ such that $\quad\frac{1}{n^p} < \vert\sin(n)\vert\quad \forall\ n\geq N$ $\quad\frac{1}{n^p} < 1-\sin(n)\quad \forall\ n\geq N$ ? Also, instead of $\ \quad\frac{1}{n^p},\ $ it might be better to not limit ourselves, and instead ask if there is a minimal $\ p\ $ so that $\ \quad\frac{a}{n^p} < \vert\sin(n)\vert\quad \forall\ n\geq N\ $ for some $a>0$ . Furthermore, if $\ n^p\ $ doesn't work here, maybe something stronger like $\ 2^n\ $ does? I don't know... Or maybe a better way to ask the question is: What functions are a ""good lower bound"" for each of the above functions $\ (\ \vert\sin(n)\vert\ $ and $\ 1-\sin(n)\ )$ ? What methods are there to tackle such problems?","['trigonometry', 'sequences-and-series', 'real-analysis']"
4207129,A basic question about ideals on natural numbers.,"Here is a basic question about the power set of the natural numbers. It is related to the goal of understanding convergent sequences in a general topological space. Suppose $S$ is a collection of subsets of $\{1,2,3,...\}$ so that all of the following hold: If $A \in S$ and $B \subset A$ , then $B \in S$ (i.e. $S$ is closed under taking subsets) If $A \in S$ and $B \in S$ then $A \cup B \in S$ (i.e. $S$ is closed under taking finite unions) If $F \subset \{1,2,3,...\}$ is finite, then $F \in S$ ( i.e. $S$ contains all finite sets of numbers). The question: Suppose $A \subset \{1,2,3,...\}$ such that for each infinite subset $B \subset A$ , there exists an infinite subset $C \subset B$ such that $C \in S$ . Must $A \in S$ ? Motivational details: The connection to topology arises from the fact that permuting the terms of a sequence in a space $X$ as no effect on whether the sequence converges or not to $x \in X$ . Suppose $X$ is a countably infinite set with $x \in X$ , and $S$ is an arbitrary collection of functions from $\{1,2,3,...\}$ to $X$ . We next impose on $X$ the finest topology so that each $f \in S$ continuously extends to a continuous function defined over the one point compactification of $\{1,2,3,...\}$ ,with $f(\infty)=x$ . The above conditions 1,2,3 translate loosely to the facts that, subsequences of a convergent sequences converge, we can interleave two sequences converging to $x$ , and that appending finitely many terms to a convergent sequence has no effect on its convergence. The above question translates to the fact that, in a space $X$ , if every subsequence of a sequence has a subsequence converging to $x$ , then the sequence itself converges to $x$ . If we call the latter condition 4), then the sequential analogue of the original set theoretic question is whether the sequential counterparts to conditions 1) 2) and 3) are adequate to ensure condition 4).","['elementary-set-theory', 'general-topology']"
4207136,Continuous map of Hausdorff space,"Let $f: X\rightarrow Y$ be a continuous map of Hausdorff space and $K\subseteq X$ be a compact subset. Suppose that $(a)$ $f|_K: K\rightarrow f(K)$ be a homeomorphism, and $(b)$ for every $x\in K$ there exists open neighborhood $U_x$ of $x$ and $V_x$ of $f(x)$ such that $f$ stricts to a homeomorphism $U_x\rightarrow V_x$ given by $x\mapsto f(x)$ i.e. $V_x=f(U_x)$ . Then I want to prove that there exists an open subset $U\subseteq X$ containing $K$ and an open subset $V\subseteq Y$ , such that $f$ restricts to a homeomorphism $U\rightarrow V$ given by $x\mapsto f(x)$ . I have no idea how to construct this open set, maybe it should be the intersection of the open neighborhood of $x\in K$ ? But I think it is not necessary to keep it open. Can someone help me? Then I tried like below:
Since $\{U_x|x\in K\}$ is an open cover of $K$ , then by compactness of $K$ , it should have a finite subcover say $\{U_{x_i}|i=1,...,n\}$ Then define $U=\bigcup_{i=1}^n U_{x_i}$ . I can see that the restriction is continuous and surjective, but I can not show that it is also injective, and therefore the continuity of its inverse map.","['general-topology', 'compactness']"
4207143,Horocircle and pseudosphere,"I found in several books by Aminov, Gromov and other current authors that deal with isometric dives, that the horocircle can be immersed isometrically in the pseudosphere (or failing that, on the Dini' surface), although visually I found several references on this platform or in books, but I have not been able to find what would be the explicit immersion, any suggestions? We know from Hilbert's theorem that the whole plane cannot be isometrically immersed in $\mathbb{R}^3$ but with this at least part of it is, hence my interest in getting explicitly which immersion could be. Knowing that there is at least a part of the hyperbolic plane that can be isometrically immersed in $\mathbb{R}^3$ , the next question I will work on will be: What is the largest region of the hyperbolic plane with this characteristic?","['surfaces', 'riemannian-geometry', 'geometry', 'hyperbolic-geometry', 'differential-geometry']"
4207173,Folland 2.22 absolute value of Lebesgue integral less than integral of absolute value,"I have been learning real analysis but I am a bit shaky with complex numbers. In the following proposition, Folland writes $|\int f|=\overline{\operatorname{sgn}(\int f)}\int f$ ? Why is this true and what is the motivation for doing this? I understand that in the real numbers, $x=\operatorname{sgn}(x)|x|$ ? I could understand then getting $|x|=\frac{1}{\operatorname{sgn}(x)}x$ . However why does the above hold in the complex numbers using modulus instead of absolute value? EDIT: One more question, why is it that $\int \alpha f$ is real?","['complex-analysis', 'measure-theory', 'lebesgue-integral', 'real-analysis']"
4207221,Show whether it is a stopping time or not.,"Let $X = \{ X_t\}$ be a stochastic process on $(\Omega, \mathbb{F}, P)$ . Explain briefly why each of the following is or is not a stopping time: (c) Let $\{A_j \}_{j=1}^n$ be a sequence of measurable sets, and consider the first time $X$ reaches $A_n$ , after first reaching $A_{n-1}$ , after first reaching $A_{n-2}$ , ... , after first reaching $A_1$ . Let $T_i = \inf \{ t \in \mathbb{T}: X_t \in A_i\}$ such that $T_i \le T_{i+1}$ for all $i$ , where $\mathbb{T}$ is a set of time. I have to show that $\{T_n \le t\} \in \mathcal{F}_t$ for all $t \in \mathbb{T}$ , where $\mathcal{F}_t$ is a filtration of $\mathcal{F}$ . My idea is that: $\{T_n \le t\} = \cup_{j 
 = T_{n-1} }^t X^{-1}_j(A_n)$ . Then, $X^{-1}_j(A_n) \in \mathcal{F}_t$ for each $j \le t$ .  But I don't know how to deal with $T_{n-1}$ on the subscript here.  Can someone help me?","['stochastic-processes', 'stopping-times', 'probability-theory']"
4207225,probability of a single student not being accepted into any college,"There are $n$ students applying to n colleges. Each college has a ranking over all students (i.e. a permutation) which, for all we know, is completely random and independent of other colleges. College number $i$ will admit the first $k_{i}$ students in its ranking. If a student is not admitted to any college, he or she might file a complaint against the board of colleges, and colleges want to avoid that as much as possible. (a) If for all $i, k_i = 1$ (i.e. if every college only admits the top student on its list), what is the probability that all students will be admitted to at least one college? (b) What is the probability that a particular student, Alice, does not get admitted to any college? Prove that if the average of all $k_i$ 's is at least $2*\log(n)$ , then this probability is at most $\frac{1}{n^{2}}$ . (Hint: use the inequality $1 —x \leq e^{-x})$ (Just to clarify, it says that the average of all $k_i$ is at least $2\ln n,$ NOT $21nn$ .) I'm having some problem trying to figure out part b. First I tried picking an arbitrary college $i$ , which accepts $k_i$ students. The college has $n!$ ways of ranking all the students. For Alice to note get admitted, we can select the $k_i$ students $(n-1) P k_i$ ways. Then the probability simplifies down to $\frac{1}{(n-1-k_i)!}$ Now I want to extrapolate this, and the problem also gives that $\frac{k1 + \cdots + kn}{n} = 2 \ln n,$ but I'm not sure how to use that to give the result the problem wants. Any help would really be appreciated!","['permutations', 'combinatorics', 'probability']"
4207226,Generalizing solutions to the differential equation $y^{(n)} = y$.,"Question: Can you generalize to find solutions of the equation $y^{(n)} = y$ ? What I have done: I know the derivative all the way to the fifth power, and what I have surmised is the further down the derivative ""rabbit hole"" so to speak of this differential equation, the first 2 terms are always $c_1e^x$ and $c_2e^{-x}$ with cosines and since splitting the remaining terms. But I feel that isn't the true solution, any help is much appreciated.","['calculus', 'ordinary-differential-equations']"
4207250,"Prove$\lim \limits_{x \to \infty} e^{-Px}\int Q'(x)\frac{e^{Px}}{P} \ dx$ exists and find it, for constant $P>0$ and $Q'(x) \to 0$ as $x \to \infty$","I'm looking to prove $\lim \limits_{x \to \infty} e^{-Px}\int Q'(x)\frac{e^{Px}}{P} \ dx$ exists and determine what it is, for constant $P>0$ and a continuous $Q'(x)$ s.t. $Q'(x) \to 0$ as $x \to \infty$ I can explain what would happen but I'm not sure how to put it rigorously. Consider the successive applications of integration by parts to $e^{-Px}\int Q'(x)\frac{e^{Px}}{P} \ dx$ : $$\Rightarrow Q'(x)\cdot\frac{1}{P^2}-e^{-Px}\int \left[ Q''(x)\int \frac{e^{Px}}{P} \right]$$ $$\Rightarrow Q'(x)\cdot\frac{1}{P^2}-Q''(x)\cdot\frac{1}{P^3}+ e^{-Px}\int \left[ Q'''(x)\int\int \frac{e^{Px}}{P} \right]$$ And so on. Since the $e^{-Px}$ term will keep on cancelling the effect of $e^{Px}$ for successive integration by parts, it seems therefore the convergence of the function will mainly be determined by our supposition that $Q'(x) \to 0$ as $x \to \infty$ , so it seems to me that the function converges to $0$ , but I don't know how to state this rigorously. How exactly would I begin to show this, since it seems to me we can't estimate how $Q''(x), Q'''(x), ...$ behave other than the fact they approach $0$ as $x \to \infty$ ? This got me stuck on estimating the value of the integral at the end of each application of integration by parts, so any help is appreciated!","['limits', 'calculus', 'convergence-divergence', 'real-analysis']"
4207256,Question about my proof of: $ \lim_{h \to 0}f(ch)=\lim_{ch \to 0}f(ch)$ for $c\neq 0$,"This post will be broken up into two sections: the first section will contain the proof, and the second section will contain the question. The proof will be written formally, as the question is more easily understood referencing the formal description. (Note, the context of this post is in Spivak's Calculus , which treats all functions, unless otherwise stated, as having a domain of $\mathbb R$ ). Prove: $\displaystyle \lim_{h \to 0}f(ch)=\displaystyle \lim_{ch \to 0}f(ch)$ for $c\neq 0$ , which is equivalent to: If $c\neq 0$ and $\displaystyle \lim_{h \to 0}f(ch)=L$ , then $\displaystyle \lim_{ch \to 0}f(ch)=L$ $\quad$ and $\quad$ If $c\neq 0$ and $\displaystyle \lim_{ch \to 0}f(ch)=L$ , then $\displaystyle \lim_{h \to 0}f(ch)=L$ We will only prove the first implication (the converse is completed similarly): By assumption: $\displaystyle \lim_{h \to 0}f(ch)=L \iff \forall \varepsilon \gt 0 \ \exists \delta \gt 0 \  \forall h \in \mathbb R \left [ 0 \lt |h| \lt \delta \rightarrow |f(ch)-L| \lt \varepsilon \right ]$ We want to show that for an arbitrary $\varepsilon$ , we can construct a $\delta$ such that $\color{red}{\forall ch} \in \mathbb R \left [ 0 \lt |ch| \lt \delta \rightarrow |f(ch)-L| \lt \varepsilon \right]$ For $\varepsilon$ , we know by assumption that there is a $\delta_{\varepsilon}$ such that: $\forall h \in \mathbb R \left [ 0 \lt |h| \lt \delta_{\varepsilon} \rightarrow |f(ch)-L| \lt \varepsilon \right ]$ Now, consider a $\delta ^* = \min\left(\delta_{\varepsilon},\frac{\delta_{\varepsilon}}{|c|}\right)$ . If $0 \lt |h| \lt \delta^* \leq \delta_{\varepsilon}$ , by assumption we have: $|f(ch)-L| \lt \varepsilon$ . Further, if $0 \lt |h| \lt \delta^* \leq \frac{\delta_{\varepsilon}}{|c|}$ , then $0 \lt |ch| \lt |c|\delta^*$ . Therefore, let our desired $\delta$ be defined as $\delta = |c|\delta^*$ . As long as $0\lt|ch| \lt \delta$ , all of our criteria is met. In the above proof, I made use of the following statement: $\color{red}{\forall ch} \in \mathbb R \left [ 0 \lt |ch| \lt \delta \rightarrow |f(ch)-L| \lt \varepsilon \right]$ Through my brief experience in maths, the universally quantified object $ch$ is atypical. I suspect the proper way to denote this is by establishing a function of the form: $g(h)=ch$ and then defining a single symbol as representing its output. i.e. something like $s_h :=g(h)$ . More specifically, we should write $g$ formally as: $g: \mathbb R \to \mathbb R$ where $h \mapsto ch$ . We would then rewrite the statement as: $\color{red}{\forall s_h} \in \mathbb R \left [ 0 \lt |s_h| \lt \delta \rightarrow |f(s_h)-L| \lt \varepsilon \right]$ This seems to emulate the more familiar notation of a universal quantifier, where only one symbol follows the quantifier. I do not really know the deep theory behind first-order logic, but I suspect the reason this proof ""works out"" is because my new symbol $s_h$ has the capacity of sweeping through all objects within $\mathbb R$ . Said differently, the previously defined function $g$ can be shown to be surjective with respect to $\mathbb R$ . If the above is true, are there times where the change of variable function is not surjective, and this causes the equality between two limits to fail ?","['first-order-logic', 'logic', 'real-analysis', 'calculus', 'limits']"
4207290,How to do Taylor expansion of a differential form intrinsically?,"Let $(M,g)$ be a closed Riemannian manifold with Levi-Civita connection $\nabla$ , let $\alpha$ be a n-form on $M$ , let $P$ be a point in $M$ . My questions are (1) how could we do Taylor expansion of $\alpha$ at P in an intrinsic way? (2) Suppose $\alpha$ is parallel with $\nabla \alpha=0$ , what will happen to that Taylor expansion? Here is some thoughts of this question: (1) We could choose some local coordinates at a neighborhood of P and choose a trivialization and a basis of the n-form bundle, then the Taylor expansion will be the expansion for the coefficients using these basis, but it doesn't look like a ""good"" definition as it depends on the trivialization. (2) My guess is a Taylor expansion of $\alpha$ at $P$ would be the following: for $Q$ lies in a neighborhood of $P$ , we pick a curve $\gamma(t)$ such that $\gamma(0)=P$ and $\gamma(1)=Q$ , then the Taylor expansion of $\alpha$ at $Q$ would be $$\alpha(Q)=\alpha|_P+\nabla_{\gamma'(0)}\alpha|_P+\nabla_{\gamma'(0)}\nabla_{\gamma'(0)}\alpha|_P+\cdots.$$ Here $\alpha(Q)$ is understood as using parallel transport of $\alpha|_Q$ to the fiber of n-forms at $P$ using the Levi-Civita connection. But I don't know how to prove it. But one thing confuses me, suppose $\nabla\alpha=0$ , then the Taylor series seems to be vanishes immediately? But the above Taylor series doesn't reflects any information of $\alpha$ in a neighborhood of $P$ even we know what $\alpha|_P$ is.","['riemannian-geometry', 'differential-geometry']"
4207291,Evaluating $\frac{1}{2\pi i}\int^{a+i\infty}_{a-i\infty}\frac{x^s}{s-\beta}ds$ using Feynman integration,"I am trying to prove that $$
\frac{1}{2\pi i}\int^{a+i\infty}_{a-i\infty}\frac{x^s}{s-\beta}ds
=\begin{cases} x^{\beta}, & x > 1 \\ 0, & 0 < x < 1 \\ \end{cases}
$$ for $0<{\rm Re}(\beta)<a$ by using Feynman integration and solving a homogeneous second order differential equation respectively. Here is my attempt: \begin{align*}
\frac{1}{2\pi i}\int^{a+i\infty}_{a-i\infty}\frac{x^s}{s}ds& =\frac{x^a}{2\pi}\int^{\infty}_{0}\frac{x^{it}(a-it)+x^{-it}(a+it)}{a^2+t^2}dt\\ 
& =\frac{x^a}{\pi}\left(\int^{\infty}_{0}\frac{\cos(\ln(x)ta)}{1+t^2}dt+\int^{\infty}_{0}\frac{t\sin(\ln(x)ta)}{1+t^2}dt\right). 
\end{align*} \begin{align*}
{\rm Re}(\oint_C\frac{x^{iza}}{1+z^2}dz) 
& ={\rm Re}\left(\lim_{R \to \infty}\int^{R}_{-R}\frac{x^{iza}}{1+z^2}dz+\int_{\Gamma}\right) \\
& =\frac{\pi}{x^a}.\lim_{R \to \infty} \left| \int_{\Gamma} \right| \\
& \leqslant\lim_{R \to \infty} \frac{2}{R}\int^{\frac{\pi}{2}}_{0}\frac{x^{-2R\theta/\pi}}{1+(Re^{i\theta})^{-2}}d\theta \\
& =0.
\end{align*} Therefore $$
\int^{\infty}_{0}\frac{\cos(\ln(x)ta)}{1+t^2}dt = \frac{\pi}{2x^a}.
$$ Set $$
I(a):=\int_{R}\frac{\cos(\ln(x)ta)}{1+t^2}dt
$$ so $$
I'(a)=\ln(x)(-{\rm sgn}(\ln(x))\pi+\int_{R}\frac{\sin(\ln(x)ta)}{t(1+t^2)}dt)
$$ and $$
I''(a)-\ln^2(x)I(a) = 0.
$$ Therefore $I(a)=c_1x^a+c_2x^{-a}$ for $$
c_1=\begin{cases} 0, & x > 1 \\ \pi, & 0 < x < 1 \\ \end{cases} \text{ and } 
c_2= \begin{cases} \pi, & x > 1 \\ 0, & 0 < x < 1.\end{cases}
$$ Assuming my attempt is correct thus far, can I evaluate $$
\int_{R}\frac{t\sin(\ln(x)at)}{1+t^2}dt=-\frac{I'(a)}{\ln(x)}:=D(a)={\rm sgn}(\ln(x))\pi-\int_{R}\frac{\sin(\ln(x)at)}{t(1+t^2)}dt.
$$ Then $D'(a)=-\ln(x)I(a)$ , so $$D(a)=c_2x^{-a}-c_1x^{a}.$$ $$
\frac{1}{2\pi i}\int^{a+i\infty}_{a-i\infty}\frac{x^s}{s}ds=\frac{x^{a}}{2\pi}((c_1x^{a}+c_2x^{-a})+c_2x^{-a}-c_1x^{a})=\begin{cases} 1, & x > 1 \\ 0, & 0 < x < 1. \end{cases}
$$ (as defined herein) using Feynman integration? It is used extensively in Riemann's paper in finding an analytic representation of the jump function (equivalently, an asymptotic estimate of the prime counting function by applying the Moebius inversion formula [following from the associativity of the Dirichlet convolution] to the analytic representation of the jump function) and in a proof of Perron's formula.  Pardon my illegible uglyography and quotidian rogitation, albeit I consider the transient apanthropinization as a pars pro toto for expunged pedagogical anomalies.","['ordinary-differential-equations', 'complex-analysis', 'contour-integration', 'solution-verification', 'leibniz-integral-rule']"
4207300,Weak convergence in $L^p$ implies strong convergence in $H^{-1}$,"Let $B(0,1) \subset \mathbb{R}^d$ and $p > d$ . Suppose $f_n$ weakly converges to $f$ in $L^p(B(0,1))$ . Prove that $f_n$ strongly converges to $f$ in $H^{-1}(B(0,1))$ . Here $H^{-1}(B(0,1))$ is the dual space of $H^1_0(B(0,1))$ with respect to $L^2-$ topology. I checked many books such as Evans' PDE, Brezis' FA or Mazya's Sobolev Spaces but didn't found this result. Any ideas to prove it? (References are also welcome). Thank you.","['partial-differential-equations', 'sobolev-spaces', 'functional-analysis', 'real-analysis']"
4207309,Doubt regarding pushforward followed by pullback of sheaves,"The answer might be well-known, but I couldn't find it antwhere. Sorry for my lack of knowledge. Let $\require{AMScd}
\begin{CD}
Y' @>i'>> X\\
@Vf'VV @VVfV \\
Y @>i>>Z 
\end{CD}
$ be a fiber diagram of finite-type schemes over $\mathbb{C}$ , where $i'$ and $i$ are closed immersions. Let $\mathcal{F}$ be a coherent sheaf on $Y$ . Is it true that $i'_*f'^*(\mathcal{F})= f^*i_*(\mathcal{F})$ ? My argument is as follows: To prove the statement, we can assume $Z$ is affine, and then choosing an affine open $U\subset X$ , the fiber diagram above becomes a fiber diagram of affine schemes (since $i,i'$ are closed immersions). Then the question becomes a question of modules over rings, in which case it is straightforward to check. Could someone kindly let me know if the statement is true or not?","['algebraic-geometry', 'sheaf-theory']"
4207312,Proof that there there is no method to divide students into 3 groups,"There are 10 students in a class: Michael Michelle Jack Daniel James Jane Tom Thompson Chris Tracy These are the pairs that like to talk together in class: Michael - Michelle Michael - Jack Michael - Daniel Michelle - James Michelle - Jane Jane - Tom Jane - Thompson Thompson - Chris Jack - Thompson Jack - Tracy Daniel - Tom Daniel - Chris James - Tracy James - Chris Tom - Tracy Proof that there is no method to divide 10 students into 3 groups so that each group does not have a pair that likes to talk together. I tried to use contradiction:
Suppose that there are a way to do this. 10 students would be divided into 3 groups as such 3 - 3 - 4 I also notice that each student likes to talk to three other students But i'm stuck here and don't know what to do next",['discrete-mathematics']
4207338,Suggestions for textbooks for measures and integration theory on Banach space,"Traditional book deals with real valued function and now I want to study the case in which the range of a function is in Banach space, can anyone recommend some books about it, thank you in advance","['banach-spaces', 'functional-analysis', 'book-recommendation', 'real-analysis']"
4207341,If all the sides of an n sided polygon are equal. Is it always a regular polygon?,"For an $n$ -sided polygon, if all the sides are equal, is it a regular polygon? If yes then why is it defined to have equal angles? If not so, how to prove that all angles are equal? edit: I meant it for $n>4$ edit 2: I tried to play with some figures","['euclidean-geometry', 'geometry', 'polygons']"
4207374,Prove:$\int_{0}^{\infty} x^9K_0(x)^4\text{d}x =\frac{42777\zeta(3)-51110}{2048}$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Wolfram Alpha says: $$
\int_{0}^{\infty} xK_0(x)^4\text{d}x
=\frac{7\zeta(3)}{8}
$$ Where $$K_0(x)
=\int_{0}^{\infty} e^{-x\cosh z}\text{d}z
$$ And I proved it by using Mellin transform. But I also found(guess): $$
\begin{aligned}
&\int_{0}^{\infty}x^3K_0(x)^4\text{d}x
=\frac{7\zeta(3)-6}{32} \\
&\int_{0}^{\infty}x^5K_0(x)^4\text{d}x
=\frac{49\zeta(3)-54}{128}\\
&\int_{0}^{\infty}x^7K_0(x)^4\text{d}x
=\frac{1008\zeta(3)-1184}{512}\\
&\int_{0}^{\infty} x^9K_0(x)^4\text{d}x
=\frac{42777\zeta(3)-51110}{2048}
\end{aligned}
$$ How to prove them?The Mellin transform doesn't work on $x^3,x^5,x^7...$ .Any help be appreciated. Please see:Hypergeometric Forms for Ising-Class Integrals","['integration', 'improper-integrals', 'riemann-zeta', 'indefinite-integrals', 'bessel-functions']"
4207421,Combinatorial Proof for $\sum_{k=0}^n \binom{2n}{2k}^2 = \frac{1}{2}\left( \binom{4n}{2n}+(-1)^n \binom{2n}{n} \right)$,"For context I am trying to prove the four identities: $$\sum_{k\,\mathrm{even}} \binom{n}{k}^2 = \begin{cases} \frac{1}{2}\left( \binom{2n}{n}+(-1)^{n/2} \binom{n}{n/2} \right)\quad\mathrm{for}\,n\,\mathrm{even} \\ \frac{1}{2}\binom{2n}{n}\quad\mathrm{for}\,n\,\mathrm{odd}\end{cases}$$ $$\sum_{k\,\mathrm{odd}} \binom{n}{k}^2 = \begin{cases} \frac{1}{2}\left( \binom{2n}{n}-(-1)^{n/2} \binom{n}{n/2} \right)\quad\mathrm{for}\,n\,\mathrm{even} \\ \frac{1}{2}\binom{2n}{n}\quad\mathrm{for}\,n\,\mathrm{odd}\end{cases}$$ The cases for $n$ odd are easy, since it can be interpreted as choosing a subset of $n$ coins from a bag of $n$ gold and $n$ silver coins, and demanding a fixed parity of the gold (and hence) silver coins, and observing that each possible choice of $n$ coins maps onto the choice of the complementary set of $n$ coins when the parity is fixed to be the other choice. No such simple bijection exists for the case where $n$ is even, so I am not sure how to modify the scenario to yield a double counting solution to the problem. I am open to a solution that involves counting paths on a square lattice of size $n$ .","['summation', 'binomial-coefficients', 'combinatorics', 'combinatorial-proofs']"
4207424,Vanishing differential forms in cohomology,"Let $X$ be a smooth differentiable manifold. Consider on $X$ a closed $p$ -form $\eta$ and a closed $q$ -form $\omega$ , which have associated cohomology classes $[\eta] \in H^p(X)$ and $[\omega] \in H^q(X)$ . Now assume their wedge product is zero in cohomology $[ \eta \wedge \omega ] = 0 \in H^{p+q}(X)$ . My question is: Is it always possible to find cohomologically equivalent elements $\eta' \in [\eta]$ and $\omega' \in [\omega]$ such that $\eta' \wedge \omega' = 0$ (i.e. such that the wedge product is genuinely zero, not only in cohomology)? Naively one needs to determine whether the exact form $\mathrm{d}\xi$ making $\eta \wedge \omega + \mathrm{d}\xi = 0$ can always be written in the form $(\eta + \mathrm{d}\alpha) \wedge (\omega + \mathrm{d}\beta) - \eta \wedge \omega$ for some $\alpha$ and $\beta$ . But this seems a difficult question, so I am wondering if there is a better argument.","['homology-cohomology', 'de-rham-cohomology', 'differential-geometry']"
4207472,"Analytic K-Homology (Higson, Roe) - Exercise 8.8.8","I am trying to understand the proof of Proposition 8.3.16 (***) in Higson and Roe's Analytic K-Homology book. They rely on Exercise 8.8.8 (which is the only argument I don't fully understand). It goes as follows: Let $A$ be a $C^*$ -algebra, $\rho:A\rightarrow \mathbb{B}(H)$ a representation of $A$ on a (separable, multigraded) Hilbert space $H$ . Define $$ \mathcal{D}_\rho(A)=\{T\in\mathbb{B}(H)\;:\;[T,\rho(a)]\sim0,\;\;\;\forall a\in A\},$$ and $$\mathcal{D}_{\rho}(A//A)=\{T\in \mathcal{D}_{\rho}(A)\;:\;T\rho(a)\sim0\sim\rho(a)T,\;\;\;\forall a\in A\}, $$ where $T\sim S$ if and only if $T-S\in\mathbb{K}(H)$ . Exercise 8.8.8: Suposse $P\in\mathcal{D}_{\rho}(A)$ is self-adjoint and that $\rho(a)P\rho(a^*)$ is positive modulo compact operators for every $a\in A$ . Prove that $P$ is positive modulo $\mathcal{D}_{\rho}(A//A)$ . The book provides a hint: since $P$ is self-adjoint, we can write $P=P_+-P_-$ , where $P_{\pm}$ are positive and $P_+P_-=P_-P+=0$ . Following this line of reasoning, the idea would be to show that $P_-\in\mathcal{D}_{\rho}(A//A)$ . I have tried computing $$ \langle \rho(a)P_-x,\rho(a)P_-x\rangle,$$ for some $x\in H$ , with the hope that I could prove this vanishes modulo compacts, but to no avail. (***)- Disclaimer: the reason why I also explicted this result is because in that setting one might use that the representation $\rho$ is non-degenerate (Lemma 8.3.8), meaning that $\overline{\rho(A)}H$ is dense in $H$ , although Exercise 8.8.8 appears to be true in the more general picture.","['c-star-algebras', 'k-theory', 'analysis', 'hilbert-spaces', 'functional-analysis']"
4207522,How many words of length $k$ are there such that no symbol in the alphabet $\Sigma$ occur exactly once?,"Introduction Given an alphabet $\Sigma$ of size $s$ , I want to find a way of counting words $w$ of length $k$ which obey the rule: No symbol occurs exactly once in $w$ .
We'll call this number $Q^s_k$ . I am particularly interested in closed-form expressions, for $Q_k^s$ or at least expressions that are fairly easy to calculate when the number of symbols is moderately large (say $s \sim 50$ ). I'm not particularly up to speed in this area of maths, but I've tried a couple of different things. I'll list them below, and end with my questions on how to move on. Deterministic Finite Automata The language I've described above is regular, so it's possible to construct a discrete finite automaton describing it. Here's what that looks like for an alphabet of two symbols The blue and green arrows correspond to inputs of the two different types of symbols. The accepting states of the DFA are 02, 20 and 22. The number of accepted words of length $k$ is then the number of paths of length $k$ from the initial state to an accepting state. From this cs stackexchange question I've found that once you have the transition matrix of the DFA, the problem boils down to calculating powers of the transition matrix, and then looking at particular rows and columns. Unfortunately, there are $3^s$ states in the DFA (for each symbol, we can have encountered it 0, 1 or more than 1 times), and $s\times 3^s$ nonzero transitions between them. Combinatorial, approach My second approach to this problem was to try and find a recursive expression for $Q^s_k$ . If we restrict the alphabet to one symbol, i.e. $s = 1$ , then if $k \neq 1$ , there's exactly one valid word, otherwise there are none. Note that by definition the empty word is valid. If we extend the alphabet to two words, we can write the new expression in terms of $Q^1$ , giving $Q^2_k = \sum_{m=0; m\neq1}^k Q^1_{k-m} C(k, m)$ . The idea is that we can form valid words with two symbols by taking $m$ instances of the new symbol and inserting into the valid words of length $k - m$ with one symbol. We don't use $m = 1$ in the sum since that would not give a valid word, and the case $k -m$ = 1 is taken care of by the fact that $Q^1_1 = 0$ . The combinatorial factor accounts for the fact that we have to select $m$ slots in the final string for the occurrences of the new symbol, and all permutations of those slots are equivalent. In fact, this approach generalises neatly to the recursive expression \begin{equation} \tag{*}\label{eq:combinatorics}
Q^{s+1}_k = \sum_{m=0; m\neq1}^k Q^{s}_{k-m} C(k, m),  
\end{equation} since the logic for what happens when we add a new symbol to an existing alphabet is exactly the same. Unfortunately, that's about where I got stuck. Inclusion exclusion As an alternative to trying to count all the valid words, we could take the perspective that the number of valid words is the total number of words minus the number of invalid words. For example, the number of valid words of length $k$ with an alphabet of two words is $Q^2_k = 2^k - 2k + 2\delta_{k,2}$ . The first term on the RHS is the total number of words of length $k$ . The next term accounts for the fact that all words with one occurrence of one symbol and $k-1$ of the other are invalid; there are two symbols and $k$ slots to fit them into, giving $2k$ . The last term accounts for the double counting that occurs when we exclude words where both the first and the second symbol occur exactly once. In general we start with the $s^k$ possible words, and subtract all words where a given symbol only occurs once. For each symbol there are $k (s-1)^{k-1}$ such words, since there are $k$ slots for the symbol of interest, and the remaining slots must be filled from an alphabet consisting of all the other symbols. This gives $s\cdot k\cdot (s-1)^{k-1}$ words where a symbol occurs only once. However, this double counts all the cases where two symbols occur only once. There are $C(s, 2)$ pairs of symbols in the alphabet, and there are $P(k, 2)$ possible permutations of these symbols in a word of length $k$ . The remaining $(k - 2)$ symbols are chosen from an alphabet of length $s - 2$ , giving $(s-2)^{k-2}C(s, 2) P(s, 2)$ words of this form. Continuing, we arrive at the general expression \begin{equation} \tag{**}\label{eq:inclusion}
Q^{s}_k = \sum_{i=0}^{\mathrm{min}(s, k)} (-1)^{i}(s - i)^{k-i} C(s, i) P(k, i)
\end{equation} Note that for this expression to be true, we must consider $0^0 = 1$ . Again, I was unable to simplify this much further. Questions Constructing the giant DFA feels like an unsatisfactory approach, because it completely ignores the symmetries in the problem. If the DFA is in the initial state "" $000\ldots$ "", then any symbol it encounters is in some sense equivalent. Is there some clever way of using the symmetries of the problem to reduce the size of the DFA? Can either (or both) of the expressions $\eqref{eq:combinatorics}$ and $\eqref{eq:inclusion}$ be simplified further? Is there a nice algebraic argument for why $\eqref{eq:combinatorics}$ and $\eqref{eq:inclusion}$ are equal? I know they must be, since they count the same thing, but I don't see any simple way of showing it","['combinatorics-on-words', 'automata', 'combinatorics', 'inclusion-exclusion']"
4207556,Expected number of cards turned over before seeing first Ace?,"A standard 52-car deck is shuffled, and cards are turned over one-at-a-time starting with the top card. What is the expected number of cards that will be turned over before we see the first Ace? (Recall that there are 4 Aces in the deck.) There's a very clever way to do this. The $4$ aces partition the deck into $5$ components, with size on average ${{52 - 4}\over5} = 9.6$ . We then have to draw the first Ace, so the expected number of cards that'll be turned over before we see it is $9.6 + 1 = 10.6$ . However, for those out there who are stupid like myself (or more generously put, want to practice our computational fortitude), let's do it by brute force. We want to calculate $$1\left({4\over{52}}\right) + 2\left({{48}\over{52}}\right)\left({4\over{51}}\right) + 3 \left({{48}\over{52}}\right)\left({{47}\over{51}}\right)\left({{4}\over{50}}\right) + 4 \left({{48}\over{52}}\right)\left({{47}\over{51}}\right)\left({{46}\over{50}}\right)\left({{4}\over{49}}\right) + \ldots + 48\left({{48}\over{52}}\right)\left({{47}\over{51}}\right)\ldots\left({{2}\over{6}}\right)\left({{4}\over{5}}\right) + 49\left({{48}\over{52}}\right)\left({{47}\over{51}}\right)\ldots\left({{2}\over{6}}\right)\left({{1}\over{5}}\right)\left({{4}\over{4}}\right) = {4\over{52}} + \sum_{n = 2}^{49}\left(n \left(\prod_{i=1}^{n-1} {{49 - i}\over{53 - i}}\right) \left({4\over{53 - n}} \right)\right)$$ However, I'm not sure how to proceed with evaluating that expression. How on Earth can I get it to evaluate to $10.6$ ?","['combinatorics', 'card-games', 'probability']"
4207613,Invariance of divergence under arbitrary coordinate transformations,"I've found the following proof which seems to have the conclusion, that the divergence is invariant under a general coordinate transformation when defined with the derivatives of the respective (transformed) components: Let $f:\mathbb{R}^n\longrightarrow \mathbb{R}^n$ and $J_f$ the jacobi matrix associate to $f$ . $\phi:\mathbb{R}^n \longrightarrow \mathbb{R}^n :(y_1,...,y_n)\longmapsto (x_1...,x_n)$ is a change of coordinates.
So $x_i=\phi_i(y_1,...,y_n)$ , and $J_\phi$ is Jocobian matrix associate to $\phi$ . Let $g:\phi^{-1}\circ f\circ \phi:\mathbb{R}^n \longrightarrow \mathbb{R}^n: (y_1,...,y_n)\longmapsto (y_1...,y_n)$ ; the jacobian matrix associate to $g$ is $J_g$ . Chain rule implies: $J_g=J_\phi^{-1} J_f J_\phi$ . Now $div g=tr(J_g)=tr(J_\phi^{-1} J_f J_\phi)=tr(J_f)=div f$ So, the divergence is invariant under a coordinate transformation. But e.g. in spherical coordiantes the divergence clearly is NOT simply: $divf = tr(J_g) =\frac{\partial {g}_r}{\partial r} + \frac{\partial {g}_\theta}{\partial \theta}+\frac{\partial {g}_\phi}{\partial \phi}$ So I do not understand where the above proof breaks down or where the mistake is.","['divergence-operator', 'multivariable-calculus', 'differential-geometry', 'real-analysis']"
4207651,"For *any* real sequence, can we make it so that, if we sum parts of it's series in order the terms came in, the resulting sequence is monotone?","I was inspired by my own question here , to ask a far more general one: If $\ (a_n)_{n\in\mathbb{N}}\ $ is a sequence of real numbers, then
does there exist a strictly increasing sequence of natural numbers $\
k_1,\ k_2,\ k_3,\ \ldots\ \quad $ i.e. $\ k_1<k_2<k_3<\ldots\ $ such
that the following sequence, $$\ \left(\ \displaystyle\sum_{n=1}^{k_1} a_n\ ,\ \sum_{n=k_1+1}^{k_2}
a_n\ ,\ \sum_{n=k_2+1}^{k_3} a_n\ ,\ \ldots\right)\ $$ is a monotone sequence of real numbers. By monotone, I mean the sequence is either non-increasing or non-decreasing. I can't think of a counter-example off the top of my head. However, I have no idea how I would go about proving such a result. I think that this would be quite an amazing result if true. I am thinking that such a counter-example similar to $\ (a_n) = (\ 1,\ 0,\ -1,\ 0,\ 1,\ 0,\ -1,\ 0,\ \ldots)\ $ should exist (i.e. there's no way to split this up in the above requirement in order to make this monotone). Clearly this isn't true for this sequence: $(1+0+-1,\ 0+1+0+-1,\ 0+1+0+-1,\ 0+1+0+-1,\ \ldots) = (0,\ 0,\ 0,\ \ldots)\ $ is monotone. But like I say, I feel like there ought to be an oscillating counter-example related to this one. If the answer is yes, then I guess we would have to do some sort of proof by contradiction [direct proof seems harder]. ""Suppose such a counter-example exists. But such a counter-example cannot exist, because if it did, we get some sort of contradiction.""","['sequences-and-series', 'monotone-functions', 'real-analysis']"
4207674,Expected value of the smaller of two random variables.,"I am studying machine learning by Kevin Murphy and a very simple problem got me stuck. Note for the jaintors - I know a solution is probably available, but I want to know why my solution is bad. I will rewrite the problem here so I can explain what is wrong with my solution. Suppose X, Y are two points sampled independently at random from the interval $[0,1]$ . What is the expected location of the leftmost point? Here is a picture of my approach: I have split a $[0,1]\times [0,1]$ plane into two triangles, one of which satisfies the condition $Y<X$ . This happens with probability of $0.5$ , but I can, without loss of generality, just calculate the result for this triangle, since for the other one the result will be the same, just with flipped variables. This way, I already have $Y<X$ . Now, I need to derive a pdf for $Y$ , given $X=x$ . If $X=x$ , than $Y$ must be smaller than $x$ . This means, that $y\in(0,x)$ . Going further, if $y \in (0,x)$ , and $y$ is uniformly distributed, than the pdf of $Y$ given $X$ must be: $P(Y|X=x)={1\over x}$ - since the pdf must integrate to one. Now what I need is the pdf of $Y$ itself, so I can calculate it's expected value. For this reason, I sum over the whole available $X$ space: $p(y)=\int_{y}^{1}{1 \over x}dx$ - i integrate from $y$ , because $x > y$ . This way, I get $p(y) = -\log(y)$ . Then, the expected value is: $\mathbb{E}[y] = -\int_{0}^{1}y\log(y) = {1 \over 4}$ . However, using another approach, I can do: $p(\min(X,Y) > x) = p(X > x, Y > x) = (1 - x)(1 - x) = (1-x)^2$ Let $Z = \min(X, Y)$ , then: $p(Z > x) = (1-x)^2$ , and: $p(Z < x) = 1 - (1-x)^2$ , giving: $P_Z(z) = 1-(1-z)^2$ , which is the cdf of $Z$ - so I can obtain the $pdf$ by differentiation. $p(z) = {d \over {dz}}P_Z(z) = 2(1-z)$ Now, I can go straight into the expected value: $\mathbb{E}[z] = \int_{0}^{1}2(z-z^2) = {1 \over 3}$ After checking on the internet, the second result i got (1/3), is the valid one. But I'm not really that concerned with why this is the right result, but rather why my first approach was wrong.","['statistics', 'order-statistics', 'probability', 'density-function']"
4207684,When can we write one function as a non-negative integral of another?,"Let $f(x)$ and $g(x,y)$ be positive real-valued functions. It is often quite useful to write $$f(x) = \int g(x,y)\, {\rm d}\mu(y)$$ for some non-negative measure $\mu(y)$ . For example, this can be very useful for converting bounds on $g$ into bounds on $f$ . I'm interested in necessary and/or sufficient conditions for when this is possible. As a simple example, if $f$ , $g$ , and $\mu$ are nice enough, then if $g(x,y)$ has non-negative Fourier transform in $x$ for all $y$ , $f$ must also have non-negative Fourier transform as well. More generally, let $T_x$ be any linear functional satisfying $\int T_x g(x,y) \,{\rm d} \mu(y) = T_x f(x)$ . Then, if $T_x g(x,y)$ is non-negative for all $y$ , then $T_x f(x)$ must also be non-negative. Is this condition sufficient? Are there simpler necessary and/or sufficient conditions?","['functional-analysis', 'real-analysis']"
4207704,What happens if I do a fractional number of Bernoulli trials?,"I want to simulate $n$ independent Bernoulli trials with probability of success $p$ , and calculate the probability that at least one of those trials succeeds. If multiple trials succeed, I don't particularly care how many. So I use this formula to calculate the overall probability of at least one success: $$
1 - (1 - p)^n
$$ I then do a single Bernoulli trial with the above probability. That works quite well, as it allows me to aggregate many Bernoulli trials into a single meta-trial. If this meta-trial fails, I can later conduct another (once a meta-trial succeeds, I stop running trials altogether). (I am aware that I could instead draw from a geometric distribution in advance, add up the values of $n$ , and wait until I exceed the value drawn. But keeping track of the total value of $n$ is inconvenient, so I don't do this.) Now, I'm beginning to think that I want to have ""fractional trials"" to deal with cases where an event is smeared out over time instead of happening in a single discrete lump. There are two ""obvious"" ways to do that: Just let $n$ be a positive real number (technically, a floating point number) and use the same formula as above. Use a Poisson distribution with $\lambda = np$ , so that the resulting probability is $1 - e^{-np}$ . I understand, in general terms, what (2) is simulating: It's a Poisson process with rate $p$ and length or ""duration"" $n$ . On average, a success happens every $\frac{1}{p}$ units of length, and we're asking whether any successes happen within the next $n$ units of length. What I don't understand is (1). I can visualize it as a binomial distribution for integral $n$ , but if $n$ is a real number, it's unclear to me what the probability actually represents, in a simple and intuitive way. I can see that: (1) is equivalent to (2) for large $n$ and small $p$ (by the Poisson limit theorem). For both (1) and (2), the probability of at least one success is not affected by combining or splitting meta-trials, when $p$ is held constant. So neither of them is ""obviously wrong."" For non-small $p$ , (1) has a higher probability of a success than (2) (by playing around in Desmos). For small $p$ , (1) is still bigger, but the difference is negligible. I believe that this corresponds to the fact that in (1), $p$ is a probability, and we approach a degenerate case as $p$ approaches 1, while in (2), $p$ is ""just"" a density factor and is not bounded above by $p = 1$ . My understanding of the geometric distribution suggests that (1) should have a success on average every $\frac{1}{p}$ units of length. But that seems unlikely since (2) already has that property, and (1) is always greater than (2). Ultimately, I need to choose between these two ways of modeling my problem, and I want to understand what (1) represents in order to make that decision. What does the binomial distribution intuitively represent, if $n$ is not an integer?","['poisson-distribution', 'probability-distributions', 'soft-question', 'probability']"
4207706,Approximation of Lebesgue measurable set,"Let $E \subseteq \mathbb{R}$ be a set with finite Lebesgue measure. I want to show that for each $\varepsilon > 0$ , there exists a disjoint collection of closed intervals $\{I_k\}$ such that $m(E \setminus \bigcup I_k) = 0 $ and $\sum_{k}m(I_k) \leqslant m(E) + \varepsilon$ . I know that $E$ can be approximated by a $F_\sigma$ set, but it seems like we are proving a slightly stronger statement. I considered \begin{equation*}
m^{**}(E) = \inf \{ \sum_{k}m(I_k) \mid E \subseteq \bigcup_{k}I_k\}
\end{equation*} where the $I_k$ 's are disjoint closed intervals. I thought $m^{**}(E) = m^{*}(E)$ (one direction is easy; for the other direction, I use the fact that each closed interval can be realized as the closure of an open interval, and that $m^{*}(E) = \inf \{m(O): E \subset O, O \text{ open}\}$ ). Is this correct? If so, how can we proceed from there and argue that the infimum can be achieved?","['measure-theory', 'real-analysis']"
4207740,"Which group actions are ""set-transitive""?","Given a faithful group action $G$ on a finite set $X$ of cardinality $n$ and an integer $1\le k\le n$ , say that $G$ is $k$ -set-transitive if we can map any unordered $k$ -tuple in $X$ to any other via an element $g\in G$ . (Contrast with being $k$ -transitive, in which case this holds for ordered tuples.) Say that $G$ is set-transitive if it is $k$ -set-transitive for all $k$ . Obviously, $S_n$ and $A_n$ are set-transitive with their natural action on an $n$ -element set (except $n=2$ in the alternating case), and in general $k$ -transitivity implies both $k$ -set-transitivity and $(n-k)$ -set-transitivity. The Mathieu group $M_{12}$ is $k$ -set-transitive for all $k\neq 6$ . Some questions: Are there any set-transitive actions besides the two categories mentioned above? Besides $A_4$ and $A_5$ , are there any groups which are $k$ -set-transitive for all $k\le 4$ which are not also $4$ -transitive? More generally, are there any group actions which are $k$ -set-transitive for some $k\le n/2$ but not $k$ -transitive? The condition seems much weaker, but I'm struggling to generate examples. Any pointers to discussion of this property in the literature would also be welcome.","['group-theory', 'group-actions', 'reference-request']"
4207745,"How to think about a quotient sets modulo an equivalence relation, and well-defined functions on the quotient set.","Perhaps there is not a correct way to think about it but I would want to know how others think about it. Here are my problems/questions, after my definitions: Definition 1. Let $X$ be a set and $\sim$ be an equivalence relation on $X$ . Then $[x]:=\{y \in X \mid y \sim x\}$ and $X/{\sim} := \{[x] \mid x \in X\}$ . My question could be summarized to ""How should I think about $X/{\sim}$ ?"". Consider $\mathbf{Z}/{\sim}$ with $z_1 \sim z_2$ $\iff$ $z_1-z_2$ is even. One then obtains $\mathbf{Z}/{\sim} = \{[0],[1]\}=\{\{...,-4,-2,0,2,4,...\},\{...,-5,-3,-1,1,3,5,...\}\}.$ The way I think about the set of all equivalence classes is that one collects all equivalent elements into one set for all elements and obtains the set on the very right in the example. Then one picks a ""name"" for each of those sets, calling it by one of its members. In the example one has the canonical choices of $[0],[1]$ . If I now pick an arbitrary element $a \in \mathbf{Z}/{\sim}$ , then there exists a $z \in \mathbf{Z}$ such that $a=[z]$ . This is because I can simply call the set $a$ by one of its representatives, in this case $z$ or in the example above $[0]$ or $[1]$ . When defining a function it then suffices to define it on all the ""names"" $[z]$ because I can give each object in $\mathbf{Z}/{\sim}$ one. The function being well defined then comes down to showing that it is independent of the name each object has been given.
Is this a valid way to think about this concept or are there other, perhaps better ways to do so? I am not sure if I am satisfied with the way I would explain it to myself since the ""giving it a name"" does not really sound that rigorous. I guess one could also view this as a sort of assignment which assigns to every set of equivalent elements a member of it (which is not well defined) and then assigns to it a value such that this process is well defined. Edit: The following is still not entirely clear to me. When defining a function from a quotient set to another set, one usually defines this in the following way: $$f: X/{\sim} \to A, \ [x] \mapsto a(x).$$ How should I think about this? Do I first choose a (arbitrary) complete system of representatives, define this function for them and then show that it is not dependent of the choice of the complete system, or do I map all $[x]$ , $ x \in X$ and then realize that the images of equivalent elements are the same, meaning that the function is well defined?","['equivalence-relations', 'relations', 'abstract-algebra', 'discrete-mathematics', 'elementary-set-theory']"
4207823,When does a group have a geometric realization?,"I came across the following question in a set of lecture notes: If $(G, *)$ is a group, when, in general, is it true that there exists $X \subset \mathbb{R}^n$ such that $G \cong \text{Sym}(X)$ ? Or, less formally, when does an abstract group have a geometric realization? In this context, $\text{Sym}(X)$ is the group of isometries of $\mathbb{R}^n$ that permute $X$ . I'm quite confused about this. As far as I understand, the elements of the group act on the vertices by some group action. I suppose in order for $G$ to be identified with the group of symmetries, the action should be faithful, and transitive. But I know far too little about groups in order to be able to say anything more general or detailed about what we can deduce about the group structure. I also don't understand the importance of the $X$ being elements of $\mathbb{R}^n$ , although this is a key part of the question. So, summing up: what conditions does such a group need to satisfy, and how would the answer differ if we drop the condition that $X \subset \mathbb{R}^n$ ?","['permutations', 'euclidean-geometry', 'geometry', 'symmetric-groups', 'group-theory']"
4207878,Nice proof that an expectation vanishes?,"Let $X$ and $Y$ be independent standard normal random variables. I have an ugly(ish) proof that for any $\epsilon>0$ , $
\mathbb E\log\|e_1+\epsilon(X,Y)\|=0
$ ,
where $e_1$ is the unit vector in the first coordinate direction (or more generally $\mathbb E\log\|v+\epsilon N\|=\log\|v\|$ for any non-zero vector (where $N=(X,Y)$ )). My proof is based on using polar coordinates to describe the distribution of $(X,Y)$ and then complex analysis for a fixed $r$ , taking care to deal with the branch cut of the logarithm in the case when $\epsilon r>1$ . It feels that the result should be well known and/or there should be a cleaner approach. Does anyone have either a reference for this fact or a nice proof? EDIT : In the light of the answer by angryavian below, it seems that my ""proof"" was over-optimistic. In the case when $\epsilon r>1$ , there is cancellation of one of the real and imaginary coordinates along the branch cut, but not the other one. I now think the expectation is not zero, but rather $\int_{1/\epsilon}^\infty 2\pi\log(r\epsilon)re^{-r^2/2}$ . This agrees nicely with angryavian's simulations in the case $\epsilon=1$ .","['complex-analysis', 'normal-distribution']"
4207888,generator of semigroup of multiplication operators on $L^p$,"Suppose $1 \leq p <\infty$ . Let $(T_t)_{t \geq 0}$ be a strongly continuous semigroup of multiplication operators on $L^p(0,1)$ defined by $T_t(f)=m_t \times f$ where the function $m_t \colon [0,1] \to \mathbb{R}$ is measurable. How show that there exists a measurable function $g$ on $[0,1]$ such that $m_t=e^{tg}$ almost everywhere for all $t \geq 0 $ ? I tried the following. For any $t,t' \geq 0$ , the relation $T_tT_{t'}(1_{[0,1]})=T_{t+t'}(1_{[0,1]})$ gives $m_t(x)m_{t'}(x)=m_{t+t'}(x)$ for almost all $x$ and $m_0=1_{[0,1]}$ . By the characterization of the exponential function (problem of the measurability of $t \mapsto m_t(x)$ ?), we obtain $m_t(x)=e^{tg(x)}$ for some $g(x)$ . Now, why $g$ is measurable and why we can define $g$ on all $[0,1]$ ?","['semigroup-of-operators', 'lp-spaces', 'functional-analysis', 'real-analysis']"
4207900,$\epsilon$-$\delta$ proof of $\lim_{x \to 3} x^2 = 9$,"I've been learning about $\epsilon$ - $\delta$ proofs and attempted to come up with my own proof that $$
\lim_{x \to 3} x^2 = 9
$$ exists (I did use some help from some textbooks). Is my proof valid and free of redundancies? My proof: Scratch work: Written formally, this is: $$
\forall \epsilon > 0, \exists \delta > 0 \text{ s.t. } 0 < |x - 3| < \delta \implies |x^2 - 9| < \epsilon
$$ We start off by simplifying the conclusion of the implication: $$
|x^2 - 9| = |x + 3|\cdot  |x - 3|
$$ Now we have $|x + 3| \cdot |x - 3| < \epsilon$ . Because $|x - 3| < \delta$ , we have $|x - 3| \cdot |x + 3|  < \delta |x + 3|$ . In an attempt to find a suitable $\delta$ , we let $\delta|x + 3| < \epsilon$ . Solving for $\delta$ , we get $\delta < \frac{\epsilon}{|x + 3|}$ . There is a problem though: $\delta$ is defined in terms of $\epsilon$ and the randomly chose point $x$ . The definition of $\delta$ can only depend on $\epsilon$ . To get around this problem, we will have to estimate the size of $|x + 3|$ . We start of by assuming that $\delta < 1$ , which implies that $|x - 3| < 1$ . Solving for $x + 3$ , we get $5 < x + 3 < 7$ . We now know that $|x + 3| < 7$ when $\delta < 1$ .  Since $|x^2 - 9| < \delta|x + 3|$ and $|x + 3| < 7$ (if $\delta < 1$ ), we have $|x^2 - 9| < 7 \delta$ . In an attempt to find a suitable $\delta$ , we let $7 \delta < \epsilon$ . Solving for $\delta$ , we get $\delta < \frac{\epsilon}{7}$ when $\delta < 1$ . So we have now deduced two restrictions: $\delta < 1$ and $\delta < \frac{\epsilon}{7}$ . To satisfy both restrictions, we let $\delta < \min \left\{ 1, \frac{\epsilon}{7} \right\}$ . When $\epsilon > 7$ , then $\delta < 1$ and when $\epsilon < 7$ , we have $\delta < \frac{\epsilon}{7}$ . We can now write up the proof. Proof: For every $\epsilon > 0$ there exists a $0 < \delta < \min \left\{ 1, \frac{\epsilon}{7} \right\}$ such that $0 < |x - 3| < \delta \implies |x^2 - 9| < \epsilon$ . There are two things that can happen: $\delta < 1$ and $\delta < \frac{\epsilon}{7}$ . Case 1 - $\delta < 1$ : The implication's hypothesis is $0 < |x - 3| < \delta$ . Multiply both sides by $|x + 3|$ to get $0 < |x^2 - 9| < |x + 3| \delta$ . Because $\delta < 1$ , we know (from our scratchwork) that $|x + 3| < 7$ and $\delta$ is also less that $\frac{\epsilon}{7}$ . This means that $0 < |x^2 - 9| < |x + 3| \delta < 7 \delta < [7 \frac{\epsilon}{7} = \epsilon]$ . Case 2 - $\delta < \frac{\epsilon}{7}$ : For $\delta$ to be less that $\frac{\epsilon}{7}$ , we must have that $\frac{\epsilon}{7} < 1$ (as per the definition of $\min$ ). Because $\delta < \frac{\epsilon}{7}$ and $\frac{\epsilon}{7} < 1$ we have $\delta < 1$ . Look to Case 1 for what follows after. $\square{}$","['epsilon-delta', 'calculus', 'solution-verification', 'limits', 'quadratics']"
4207926,When Must the Rate of Convergence be Worse on the Boundary,"WLOG, let $$f(z)=\sum_{n=0}^\infty a_nz^n$$ be a holomorphic function on the unit disc with radius of convergence $1$ around $z=0$ . My question is about a sense in which we can say that the rate of convergence of the taylor expansion of $f$ around $0$ must be worse everywhere on the boundary of the disc of convergence than in the interior. If we define $$E_n(z) = \left|f(z) - \sum_{k=0}^n a_k z^k\right|$$ to be the error resulting from the $n$ -term truncation of the Taylor series. Then more specifically, the question can be stated as follows: Given some $z_1$ in the interior of the unit disc, and some $z_2$ on the boundary, and perhaps some conditions on $f$ , do we know that for all sufficiently large $n$ , $E_n(z_1) \leq E_n(z_2)$ ? I believe that this holds automatically if the Taylor series of $f$ diverges at $z_2$ , but either way the interesting case seems to me to be the one where the Taylor series converges at $z_2$ , so that can be assumed if you like. By Cauchy-Hadamard, we can know that for all $z$ in the interior of the unit disc (with $ 0<|z|=r < 1$ ), we have that $$\limsup_{n \to \infty} |a_nz^n|^{1/n} \leq r < 1 $$ and that we can not obviously say that about the boundary. But it seems a lot less trivial to show lower bounds on the error.","['complex-analysis', 'taylor-expansion']"
4207959,Why do we always need the Schwarz lemma when bounding the trace of a Kähler metric?,"My undergraduate thesis topic is Kähler geometry. The general direction is something like the Calabi-Yau theorem or more adventurously some singular Calabi-Yau theorem, but this is not certain yet. One thing that I am noticing a lot of in my reading of Kähler geometry is that if we have two Kähler metrics $\omega$ , $\eta$ , then to get a bound of the form $$\text{tr}_{\omega}(\eta) \leq C$$ we need to use the Schwarz lemma -- Essentially, we apply the maximum principle to some term like $$\log \text{tr}_{\omega}(\eta) - A \varphi,$$ where $\omega = \eta + dd^c \varphi$ and $A>0$ is large. This requires an assumption on the (Ricci/bisectional/holomorphic sectional) curvatures of $\omega$ , $\eta$ (depending on which Laplacian one computes with). I feel that I understand how to use the Schwarz lemma to get these estimates, but I want to ask why we have to use it (if we have to?). This is prompted by studying singular metrics, for examples cone and cusp metrics: To formulate my question, let $D$ be a divisor in a compact Kähler manifold $M$ , and for simplicity, assume that $D$ has simple normal crossings. A cone Kähler metric is a Kähler metric which is smooth on $M - D$ and is quasi-isometric to $$\frac{i}{2} \sum_{j=1}^k | z_j |^{2(1-\beta_j)} dz_j \wedge d\overline{z}_j + \frac{i}{2} \sum_{j \geq k+1} dz_j \wedge d\overline{z}_j.$$ A cusp Kähler metric is a smooth Kähler metric on $M-D$ which is quasi-isometric to $$\frac{i}{2} \sum_{j=1}^k | z_j |^{-2}| \log | z_i |^2 |^2 dz_j \wedge d\overline{z}_j + \frac{i}{2} \sum_{j \geq k+1} dz_j \wedge d\overline{z}_j.$$ From these descriptions, can one not see immediately that if $\omega$ is cusp and $\eta$ is cone, then $$\text{tr}_{\omega}(\eta) \leq C | z_i|^2 | \log | z_i |^2|^2,$$ which would give $$\text{tr}_{\omega}(\eta) \leq C \prod_j | \sigma_j |^2 | \log | \sigma_j |^2 |^2,$$ if $\sigma_j$ are the defining sections for the divisor $D$ ? What initially came to my mind is a coordinate dependence problem, but this seems to contradict the fact that many calculations of this type involve normal coordinate calculations. Sorry if this question is silly.","['kahler-manifolds', 'riemannian-geometry', 'complex-geometry', 'algebraic-geometry', 'differential-geometry']"
4207982,On equi-integrability (also called uniform integrability),"Let $(X, \Sigma, \mu)$ be a probability space and let $\mathscr{F}$ be a norm-bounded subset of $L^{1}(\mu)$ . We say that $\mathscr{F}$ is equi-integrable if for every $\epsilon>0$ there is some $\delta>0$ such that for any $A \in \Sigma$ with $\mu(A) \leq \delta$ and for all $f \in \mathscr{F}$ , $$ \int_{A}|f| d \mu \leq \epsilon $$ Alternatively, being equi-integrable is equivalent to $$ \lim _{C \rightarrow \infty} \sup _{f \in \mathscr{F}}\int_{\{|f|>C\}}|f| d \mu=0 \quad \quad (1)$$ According to Theorem 4.5.6 in Measure Theory by Bogachev we have If $f_{n}$ be a sequence in $L^{1}(\mu)$ and   for each $A \in \Sigma$ the sequence $\int_{A} f_{n} d \mu$ has a finite limit, then $\left\{f_{n}\right\}$ is bounded in $L^{1}(\mu)$ and is
equi-integrable. I'm confused about the last statement. Consider the sequence $f_n=n I_{[0,\frac{1}{n}]}$ . It is norm-bounded and it is not equi-integrable as it does not satisfy (1). However, $\lVert{f_n}\rVert_{L^1}=1$ . This implies that $\{f_n\}$ seen as a subset of $(L^{\infty})^*$ , is bounded in the norm of $(L^{\infty})^*$ . By Banach-Alaoglu it has a convergent subsequence, that converges to a (potentially only finitely additive) measure $\nu\in (L^{\infty})^*$ . Therefore, passing to the subsequence, for every $A\in \Sigma$ , $$\lim_n\int_A f_n d\mu=\lim_n\int f_n I_A d\mu=\int I_A d\nu=\nu(A).$$ Therefore, the subsequence seems to satisfy the hypothesis in Theorem 4.5.6 in Bogachev. What am I missing? Thanks!","['measure-theory', 'probability-theory']"
4208001,Math needed to understand hodge star operator,"I'm a physics student trying to understand the hodge star operator and how to use it to derive the Laplacian. I looked into it, but the math language used is complete jibberish (notation seems insanely prohibitive). What exact field does the hodge star pertain to? Can you list the things I need to know in order to learn about this/books to read?","['differential-topology', 'differential-geometry']"
4208024,Understanding Layer Cake Representation,"It is a common exercise to show that if $f\in L^1 $ then $$\int_R |f(x)|d\mu(x) =\int_0^\infty \mu(\{|f|\geq t\})dt $$ In showing this, it is common to make the observation that $$ \mathbb{1}_{[0,|f(x)|]}(t) = \mathbb{1}_{\{|f|\geq t\}}(x) $$ I can't seem to figure out how these are equal and what the notation means. I am familiar with the indicator function, but not of an indicator function being a function of $ t$ or $x$ here, so perhaps this is part of my misunderstanding. Why are these two indicator functions equal?","['lebesgue-measure', 'lebesgue-integral', 'real-analysis']"
4208037,Simplest proof of Taylor's theorem,"I have for some time been trawling through the Internet looking for an aesthetic proof of Taylor's theorem. By which I mean this: there are plenty of proofs that introduce some arbitrary construct: no mention is given of from whence this beast came.  and you can logically hack away line by line until the thing is solved. but this kind of proof is ugly.  a beautiful proof should rise naturally from the ground. I've seen one proof claiming to do it from the fundamental theorem of calculus. It looked messy. I've seen several attempts to use integration by parts repeatedly. But surely it would be tidier to do this without bringing in  all of that extra machinery. The nicest two approaches seem to involve using the mean value theorem and Rolle's theorem.  but I can't find a lucid presentation of either approach. Maybe my brain is unusually stupid, and the approaches on Wikipedia etc are perfectly good enough for everyone else. Does anyone have a crystal clear understanding of this phenomenon? Or a web-link to such an understanding? *EDIT*: Eventually a Cambridge mathematician explained it to me in a way that I could understand, and I have written up the proof here . To my mind it is the most instructional proof I have encountered, yet putting it as an answer received mostly downvotes. It seems strange to me that no one else seems to concur.  But it should be up to the keenest mathematical minds to choose which answer should be accepted. It shouldn't be up to me. Therefore I will bow to the wisdom of the community, and accept the currently most-upvoted answer. I have learned from Machine Learning that a ""Committee of Experts"" outperforms any one expert, and I am certainly no expert.","['soft-question', 'taylor-expansion', 'real-analysis']"
4208054,motivation of the need to use higher order infinitesimal when defining derivative,"I am trying to derive the concept of derivative and differential from limit and linear approximation for reviewing the subject. And I cant figure the motivation of using higher order infinitesimal as a requirement to define derivative during the middle of it. Here is what I did [Step 1] : I start by supposing the only thing I know is the concept of limit. And I should proceed to develop the notion of derivative from the idea of linear approximation [Step 2] : Assume a single variable real function $f(x)$ as example, suppose $f(x)$ is defined on an interval $[(a-r), (a+r)]$ where $r$ is positive. What I want to do is to estimate the value of any $f(x)$ within this interval by using linear approximation: $$f(x)=A(x-a)+f(a)+E$$ where $E$ is the error of approximation $$E=[f(x)-f(a)]-A(x-a)$$ [Step 3]: As the constant $A$ can be chosen randomly, if I want to bring in the notion of derivative , I have to find some sort of motivation that requires me to find a specific $A$ which satisfies $$\lim\limits_{x\to a}\frac{E}{(x-a)}=0$$ that is to say the requirement for $A$ is that it has to make $E$ a higher order infinitesimal with respect to $(x-a)$ . Question : [Q] : So what is actually the motivation behind such requirement? [Q1.1] : I understand from the geometrical perspective it signifies the tangent line, but then what makes the tangent line so special that brings me the motivation to use it as my linear approximation constant $A$ ? [Q1.2] : Moreover, what is the algebraical motivation behind such requirement without considering the geometrical interpretation ?","['multivariable-calculus', 'calculus', 'derivatives']"
4208056,Combinatorics of tickets (same number as many as three times.),"Here's the question I'm trying to solve: In a lottery game, players choose 6 numbers between 1 and 24. Suppose players may choose the same number as many as three times. such as $\{2,2,5,20,22,23\}$ and $\{7,7,7,10,10,19\}$ , How many tickets are possible? I'm trying to consider the following cases: 6 distinct numbers: $\binom {24}6$ 1 pair and 4 distinct numbers: $\binom {24}1\times \binom {23}4$ 2 pairs and 2 distinct numbers: $\binom {24}2\times \binom {22}2$ 3 pairs: $\binom {24}3$ 1 triplet, 3 distinct: $\binom {24}1\times \binom {23}3$ 1 triplet, 1 pair, 1 distinct: $\binom {24}1\times \binom {23}1\times \binom{22}1$ 2 triplets: $\binom {24}2$ I'm not pretty sure if my answer looks correct, and I'm especially confused at case 6: can I represent that as $\binom {24}2\times\binom {22}1$ ? Is there a better way to think about this question? Thanks for the help!",['combinatorics']
4208059,"Find $\cos\theta$ where $‖\mathbf{a}‖=6, ‖\mathbf{b}‖=8, ‖\mathbf{a}+\mathbf{b}‖=11$, and $\theta$ is the angle between $\mathbf{a}$ and $\mathbf{b}$. [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question This is a question from AOPS that I don't really understand. I would love it if someone can show me how to do this question from the very beginning. Given vectors $\mathbf{a}$ and $\mathbf{b}$ such that $\|\mathbf{a}\| = 6,$ $\|\mathbf{b}\| = 8,$ and $\|\mathbf{a} + \mathbf{b}\| = 11.$ Find $\cos \theta,$ where $\theta$ is the angle between $\mathbf{a}$ and $\mathbf{b}.$ I don't know (not I don't understand but I don't know) how to find $\mathbf{a}$ or $\mathbf{b}$ since it's not a number but a vector.
This is supposed to be a lesson next year and I'm trying to preview but I really don't understand this.","['algebra-precalculus', 'vectors']"
4208064,"For $\sum_{n \geq 0}a_n z^n=f(z)=e^z /\cos{\frac{\pi}{2}z}$, what is $a_{100}$ with error $0.0001$ at most?","For $\displaystyle\sum\limits_{n \geq 0}a_n  z^n$ the Maclaurin series of $f(z)=\frac{e^z}{\cos{\frac{\pi}{2}z}}$ , what is an approximation of $a_{100}$ with error $0.0001$ at most? This is a past qualifying exam question, so no computer. I am looking for a solution that could be given in a timed exam setting, by a student with one semester of graduate Complex Analysis, familiar with first half of Stein's book, with Churchill's book, and somewhat with first half of Ahlfor's book. I write $c(z)\,=\,\cos{\frac{\pi}{2}z}\,=\,\displaystyle\sum\limits_{\;n\geq0\\\text{(even)}}c_n  z^n,$ and also $\theta=\frac{2}{\pi}<1.$ We have $c_n\,=\,\frac{(-1)^{n/2}}{n!\theta^n}.$ $$f(z)\,=\,\frac{1\,+\,z\,+\,\frac{z^2}{2!}\,+\,\frac{z^3}{3!}\,+\,\frac{z^4}{4!}\,+\,\cdots}{1\;\quad-\quad\frac{z^2}{2!\theta^2}\quad+\quad\frac{z^4}{4!\theta^4}\,-+\,\cdots}\qquad\qquad(z\in\mathbb{D}).$$ Using the usual method, I found $a_0,a_1,a_2,a_3,a_4$ by hand, so we have $\,f(z)\,=\,$ $$\displaystyle\sum\limits_{n \geq 0}a_n  z^n\,=\,1+z+\left(\frac{1}{2}+\frac{1}{2\theta^2}\right)z^2+\left(\frac{1}{6}+\frac{1}{2\theta^2}\right)z^3+\left(\frac{1}{24}+\frac{1}{4\theta^2}+\frac{5}{24\theta^4}\right)z^4+\cdots.$$ I tried to discern a pattern by keeping the expressions more abstract, i.e. keeping the $a_n$ and $c_n$ . Doing so we can write e.g., $$a_2\,=\,\frac{1}{2!}\,-\,c_2\,=\,\frac{1}{2!}\,-\,\displaystyle\sum\limits^{2}_{\,\ell\,=\,2\\\text{(even)}}c_\ell  a_{n-\ell},$$ $$a_7\,=\,\frac{1}{7!}\,-\,(c_2 a_5+c_4 a_3+c_6)\,=\,\frac{1}{7!}\,-\,\displaystyle\sum\limits^{7}_{\,\ell\,=\,2\\\text{(even)}}c_\ell  a_{n-\ell},$$ $$a_{100}\,=\,\frac{1}{100!}\,-\,(c_2 a_{98}+c_4 a_{96}+\cdots+c_{98}a_2+c_{100}).$$ I am not sure what to do next. I did consider Cauchy's Integral Inequalities, but could not see how to get a useful estimate from them. If $0<r<1,$ $$a_{100} = \frac{f^{(100)}(0)}{100!\,} = \frac{1}{2\pi i } \oint_{|z| = r} \frac{e^z}{c(z)\,z^{101}} dz,$$ $$|a_{100}| \leq \frac{1}{r^{100}}\max_{|z| = r}\left|\frac{e^z}{\cos(\frac{\pi}{2}z)} \right|,$$ but for either $r$ very near zero, or very near one, or say for $r=$ one-half, the estimate seems to not be useful.","['complex-analysis', 'approximation', 'taylor-expansion']"
4208077,Finishing up a proof on a counting lemma by Sperner,"Background While working with set families related with Sperner's theorem I found this lemma which I am to prove: If $\Delta\mathcal F=\{ B\in \binom{[n]}{k-1}:\ B\subseteq A,\ \text{for a set } A\in \mathcal F\}$ denotes the shadow of a set $A\in\mathcal F\subseteq \binom{[n]}{k}$ , then the following inequality holds $$\frac{|\Delta\mathcal F|}{\binom{n}{k-1}}\geq \frac{|\mathcal F|}{\binom{n}{k}}.$$ Equality holds if and only if $\mathcal F$ is empty or $\mathcal F$ is precisely $\binom{[n]}{k}$ . Here $\binom{[n]}{k}$ denotes the set of $k$ -subsets of $[n]$ , the set of the first $n$ positive integers. The reference given for the proof was Sperner's 1928 paper . My Approach I have been able to prove the inequality as follows: For every $B\in\Delta\mathcal F$ , there are $n-(k-1)$ ways to add an element to the set in such a way that we obtain a $k$ -subset of $[n]$ . This means that there can be no more than $n-k+1$ sets inside of $\mathcal F$ in which we can find $B$ as a subset . On the other hand, by virtue of the definition of the shadow of $\mathcal F$ , every set $A\in\mathcal F$ contains exactly $\binom{k}{k-1}=k$ sets of $\Delta\mathcal F$ . Now the number of pairs of sets $(A,B)\in \mathcal F\times \Delta\mathcal F$ such that $B\subseteq A$ is exactly $|\mathcal F|·k$ . But such quantity cannot exceed the amount of sets inside of $\Delta\mathcal F$ times the number of ways in which we can find it as a subset of a set in $\mathcal F$ . This amount is precisely $|\Delta F|·(n-k+1)$ . In conclusion we obtain $$|\mathcal F|·k\leq|\Delta F|·(n-k+1), $$ which is equivalent to the initial inequality $\frac{|\Delta\mathcal F|}{\binom{n}{k-1}}\geq \frac{|\mathcal F|}{\binom{n}{k}}$ . My Problem The final part of the lemma asks us to prove when does equality occur. It can be seen that if $\mathcal F$ is empty or it's the set of $k$ -sets, then the inequality becomes an equality. To prove the other direction is the part giving me some troubles. Proving the other direction is equivalent to proving If $\mathcal F$ is a non-empty set family of $k$ -subsets of $[n]$ , and $\frac{|\Delta\mathcal F|}{\binom{n}{k-1}}= \frac{|\mathcal F|}{\binom{n}{k}}$ , then $\mathcal F$ is precisely the set of $k$ -subsets of $[n]$ . My initial observation was that if the equality holds, then two things occur: The number of pairs $(A,B)$ such that $B\subseteq A$ is exactly $|\Delta F|·(n-k+1)$ . For every $B\in\Delta \mathcal F$ , the $n-k+1$ $k$ -sets that can contain $B$ will appear in $\mathcal F$ . However with this information I am not able to see how can I deduce that $\mathcal F$ is $\binom{[n]}{k}$ , I don't know what am I missing but I feel that it's right under my nose. There's also this hunch that tells me that I could also go along the lines of proving $\Delta \mathcal F$ is $\binom{[n]}{k-1}$ . In such a case deducing that $ \mathcal F$ is $\binom{[n]}{k}$ follows from the hypothesis. I also tried reading Sperner's paper but I don't know german and I can't find similarities between my question and the paper itself. (I mean, it's almost a 100 years old, some notation ought to change) My Question exactly How can I prove the last part of the lemma? Also, is my approach for the inequality correct? Did I count correctly? Any type of lead, suggestion or comment will be kindly received.","['solution-verification', 'combinatorics', 'extremal-combinatorics']"
4208106,A question about sizes of antichains,"Background While working with antichains, set families and Sperner's theorem I came across an exercise which asked the following: If $A_1,\dots,A_k$ are disjoint antichains of the set $\mathcal P([n])$ , then $$|A_1|+\dots+|A_k|\leq \sum_{i=-\lfloor \frac{k}{2}\rfloor}^{\lfloor \frac{k-1}{2}\rfloor}\binom{n}{\lfloor\frac{n+i}{2}\rfloor}.$$ Here $[n]$ denotes the set of the first $n$ positive integers. My problem I am completely stumped by this problem because of the floor functions in particular. I can't see what is the right hand side of the inequality counting. I can see however a couple of things: The amount on the left: $|A_1|+\dots+|A_k|$ is precisely the size of $\displaystyle \bigcup_{j=1}^k A_j$ because of the fact that the sets are disjoint . I know that by Sperner's theorem , the size of an antichain is bounded by $\binom{n}{\lfloor\frac{n}{2}\rfloor}$ . So the sum of the sizes is at most $k$ times such quantity. But I cannot compare $k\binom{n}{\lfloor\frac{n}{2}\rfloor}$ with the sum on the RHS. I also know about a couple of results, Dilworth and Mirsky's theorems . But Mirksky's claim is that we need at least an amount of antichains equal to the number of elements in the longest chain to cover $\mathcal P([n])$ . And we don't even know if our sets form a covering. In Essence: How can I interpret the sum on the right hand side of the inequality? Can it be written more easily? What is it counting? Any hint, or indication on how to approach this problem will be kindly received.","['elementary-set-theory', 'ceiling-and-floor-functions', 'combinatorics', 'extremal-combinatorics']"
4208124,How to differentiate this matrix multiplication?,"(Sorry for poor question I don't know how to write a math equation) $U_i$ for $i=1,2,...,d$ and $M, S$ are $C^{(N \times N)}$ matrices.
And $\alpha_i$ for $i=1,2,...,d$ and $t$ are complex scalars. $H=\sum_{i=1}^d\alpha_iU_i$ and $Q=\exp(jtH)$ where $j$ is the imaginary unit - $j^2=-1$ . Let $f(\alpha_1,\alpha_2,...,\alpha_d)=\operatorname{tr}[MQSQ^H]$ Where $^H$ is the Hermitian transpose. Then how can we describe the partial derivative of $f$ with respect to $\alpha_i$ ?","['differential', 'matrices', 'calculus', 'matrix-calculus', 'derivatives']"
4208140,Intersection of n circles,"I am trying to devise a general method (ultimately an R script) to calculate the area of the union of $n$ circles, similarly to this post , except that I am not constraining the radius to be equal for all circles. Same as the above post, I am opting for the inclusion-exclusion approach with the formula in this answer , which means I need to calculate the areas of the intersections of all possible sets of 2, 3, ..., n circles that do overlap . So the first problem I had to tackle was: when do $m \le n$ circles have a non-empty intersection? I started by looking at the case with $m = 2$ . Apparently, if $d$ is the distance between the centres of the two circles, and $R_1, R_2$ their radii, their perimeters have two distinct intersection points (i.e. there is a non-empty overlapping region that does not degenerate to a whole circle) only if $|R_1 - R_2| < d < R_1+R_2$ (this would be case 3 below): Then I set out to calculate the area of the two circular segments that form the intersection. They are already available in some websites, but I wanted to delve into this myself, to understand the machinery of it. And indeed, in doing that I found a complication that does not seem to be addressed by most websites. The most common case is 1 above, where the sum of the areas of the circular segments (which by convention is always calculated as the 'smaller' cut-off of the circle) is indeed equal to the intersection of the two circles. However, as $d$ decreases, at some point the chord becomes equal to the diameter of the smaller circle (case 2). The general formula still holds, and it simply gives half the area of the whole small circle. For $d$ below that, however, the formula based on the chord length fails: one would need to take the 'larger' part of the smaller circle that is cut off by the chord (so basically its total area minus the 'conventional' circular segment). This is case 3. I found that this happens when $d < \sqrt {|R_1^2-R_2^2|}$ . My own formulae, based on $d$ rather than on the chord length, still work, because the area of the triangle that must be subtracted from the area of the circular sector to obtain the one of the circular segment becomes negative, so it is in fact summed. Still I thought it was important to find this out. To tackle cases with $m > 2$ , my main questions/doubts at the moment are: how to find a condition (based on $d$ 's and radii) that determines when there is a non-empty intersection between $m$ circles whether the above issue with a smaller circle being 'more inside' another circle than 'outside' would affect those cases, too, or not So I am trying to find what condition differentiates case 1 from 2 below: And of course the same for $m = 4, 5, ..., n$ . Intuitively I am guessing that the circles' centres will need to be 'closer together' the larger $m$ gets, for a non-empty intersection between all $m$ circles to exist. But at the moment I would not know how to approach it. Any ideas/advice? Thanks! EDIT clarification I can calculate the positions of points $A, B, C$ in the above picture. What I don't know is how to test if these points enclose an area that belongs to the intersection of the 3 circles (case 1) or not (case 2). It might be obvious or very easy, but I just have not figured it out, yet, so I would appreciate any advice on the matter. E.g. I can see that in case 1 each point $P$ in $A, B, C$ , is 'inside' the 3rd circle (i.e. $P$ is at a distance $d < R$ from the centre of the circle that did not generate $P$ ). But I don't know if this is a generally valid test, and I don't immediately see how it would extend to $m > 3$ and to cases where the radii are all different.","['circles', 'geometry']"
4208142,Proof of the eyeball theorem.,"$\underline{\text{Introduction}:-}$ I think the eyeball theorem is not really that popular and it also doesn't have a Wikipedia page. I have first found it on Quora, however I don't currently remember what the question was, so I cannot link it. I have also found a question in MO Searching the name on google also gives some proofs, but those proofs didn't really satisfied me. So I tried proving it on my own. $\underline{\text{Eyeball Theorem}:-}$ $■\text{Statement}:$ $|GH|=|EF|$ $■\text{My Proof}:-$ (I couldn't find a similar picture, so I just drew it quickly with a sketch pen. If you have problem understanding this picture, then please ask me) From my picture, $\angle AI_2P=\angle BI_1R=\angle ACB=\angle ADB=90°$ So, $\begin{cases}\sin{\alpha}=\frac{RI_1}{BR}=\frac{AC}{AB}\\\sin{\theta}=\frac{PI_2}{AP}=\frac{BD}{AB}\end{cases}$ $\implies\begin{cases}\frac{RI_1}{r_1}=\frac{r_2}{AB}\\\frac{PI_2}{r_2}=\frac{r_1}{AB}\end{cases}$ $\implies\begin{cases}AB\cdot RI_1=r_1r_2\\AB\cdot PI_2=r_1r_2\end{cases}$ $\implies AB\cdot RI_1=AB\cdot PI_2$ $\implies 2RI_2=2PI_2$ $\implies PQ=RS$ . $\blacksquare$ $《\bigstar》\text{Generalization}:-$ I think that the $2D$ case implies the $N-D$ case.( $N\geq 2$ ) Since, the $N$ -cones from the centres of the two $N$ -balls will create an $N-1$ -ball in the places of intersection and since their radii are the same then their $N-1$ volume would be same too which is our concern here. Correct me if I'm wrong. $\underline{\text{My Question}:-}$ I would like to have my proof reviewed and I would also like to have more elegant proofs of this theorem.","['proof-writing', 'circles', 'geometry', 'alternative-proof', 'solution-verification']"
4208168,Prove that $e^{2x-\frac{x^2}{2}}+e^{x-\frac{x^2}{2}}-2e^x-x \ge0$，where $x\leq0$,"Prove that $$e^{2x-\frac{x^2}{2}}+e^{x-\frac{x^2}{2}}-2e^x-x \ge0$$ where $x\leq0$ . First I tried to find the derivative , but it is hard to find the sign of it.
I also found that when $x$ is roughly smaller than $-2$ , $-2e^x-x \ge 0$ is easy to prove.
So the question is how to prove it when $x$ is near 0, I tried $e^x \ge x+1$ , failed totally. Wish someone could help and thanks in advance.","['calculus', 'derivatives', 'inequality']"
4208176,Where does De Moivre's formula for complex roots come from?,"I have noticed that in some places on the web (such as here and on the Encyclopedia of Math ), De Moivre's familiar theorem, $$z^n=\big(\rho(\cos\phi+i \sin\phi)\big)^n=\rho^n(\cos n\phi + i \sin n\phi)$$ is expressed in terms that make it easier to compute the roots of a complex number, $$z^\frac{1}{n}=\big(\rho(\cos\phi+i \sin\phi)\big)^\frac{1}{n}=\rho^\frac{1}{n}\left(\cos\frac{\phi+2\pi k}{n} + i \sin \frac{\phi+2\pi k}{n}\right), \\  
 k=0,1,\ldots n-1$$ I was wondering, what is a simple way to derive the second version from the first? I know that adding a multiple of $2\pi$ to angle does not change its $\sin$ or $\cos$ , but I am not sure how to use that knowledge to derive the second version. EDIT: I don't mean to be a snob, but it would be better for me (and any poor sap who stumbles upon this question in the years to come) if the amount of extra mathemetical information in answers was kept to a minimum. I will upvote any answers that are technically correct, but I won't accept the answer if it seems to require more math knowledge than the average high-schooler would be comfortable with.","['algebra-precalculus', 'trigonometry', 'complex-numbers', 'polar-coordinates']"
