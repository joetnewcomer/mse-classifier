question_id,title,body,tags
2235566,Limit proof by definition,"I have the following limit:
$$\lim_{x \to 6} \frac{x+1}{x-5} = 7$$
How do I prove this equation by the famous definition of the limit? (the one includes delta and epsilon).
I know how to prove simple limits like this one: $$\\\lim_{x \to 6} x-5 = 1 
\\ |x - x_0| < \delta \implies |x - 6| < \delta 
\\ |f(x) -L| < \epsilon \implies |x-5 -1| < \epsilon  \implies |x - 6| < \epsilon  
\\ \implies \delta = \epsilon$$ But in the first example I asked about I have:
$$|x-6| < \delta$$ $$|\frac{x+1}{x-5} -7| < \epsilon \implies |\frac{36-6x}{x-5}| < \epsilon$$
don't really know how to proceed, thanks in advance :)","['proof-verification', 'calculus', 'limits']"
2235615,(Fundamental) Solution of the Helmholtz equation,"The fundamental solution of the Helmholtz equation in $\mathbb{R}^3$
$$(\Delta+k^2)u=-\delta \tag{1}$$
is well known: 
$$u(x)=\frac{e^{\pm ik|x|}}{4\pi |x|}$$
solves the Helmholtz equation in distributional sense. The usual ansatz to obtain fundamental solutions is to Fourier transform both sides. Then $(1)$ becomes
$$(-|x|^2+k^2)\hat{u}(x)=-1 \implies \hat{u}(x)=\frac{1}{k^2-|x|^2}.$$
The problem now arises is that we want to inverse Fourier transform $\hat{u}$ to obtain the solution. But $\hat{u}$ has singularities on the sphere of radius $k$. How do we proceed in a rigorous distributional way to obtain the above fundamental solution? What do people usually do and is there any book about such problems?","['real-analysis', 'partial-differential-equations', 'distribution-theory', 'functional-analysis', 'analysis']"
2235633,"There are $3{,}684{,}030{,}417$ different semigroups of order $8$.","Early on in ""Nine Chapters on the Semigroup Art,"" by Alan J. Cain it is claimed that there are $3{,}684{,}030{,}417$ different (non-isomorphic) semigroups with $8$ elements. How was this number reached?","['combinatorics', 'semigroups']"
2235638,"ABC is Isosceles right triangle with $AB=BC$ $P$ , $Q$ are points on $AC$ such that $AP ^2 + CQ^2 = PQ^2$ What is the value of angle $PBQ (x) $?","ABC is Isosceles right  triangle with $AB=BC$ $P$  , $Q$ are points on  $AC$ such that 
  $$AP ^2  + CQ^2  =  PQ^2$$ What is the value  of angle $PBQ (x) $? Thank you for help","['euclidean-geometry', 'geometry']"
2235668,Finding the original function from a Hessian.,"I'm trying to find the original function from the Hessian defined as: $Hf:= \begin{bmatrix}x-2y & x+2y\\x+2y & 2x+2y\end{bmatrix}$ Since the Hessian is symmetric, and the mixed order partials are equivalent, then there exists some $C^2$ function that can be differentiated twice to give this matrix. I'm not sure how I should go about solving this, though, and I should note it should not require integration. Regardless, I did try to approach via integration by integrating $x-2y$ and $2x+2y$ to return the partials of x and y respectively, but these do not result in giving the mixed order derivatives. Is there an alternative approach I should be taking, that does not involve integration? I guess you could logically deduce it...? EDIT: Could I perhaps use a Taylor series expansion somehow? Though I don't think I have enough information...",['multivariable-calculus']
2235730,Diagonalization of an infinite matrix,"Let A be an infinite matrix with all its first column elements equal to 1 and the rest of them equal to 0. A =\begin{pmatrix} 1  & 0 & 0 & 0 & \cdots\\
1  & 0 & 0 & 0 &\cdots\\
1  & 0 & 0 & 0 & \cdots\\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{pmatrix} Can A be diagonalized?","['matrices', 'diagonalization', 'operator-theory']"
2235760,Algebraic Topology before Differential Topology?,I'm currently up to connectedness and Compactness in Topology and I was wondering if I should start algebraic topology first or begin with differential topology. I'm looking at Hatcher's algebraic topology and using lee's topological manifolds as a supplement alongside it. Then there's Guillemins differential topology. Which subject would be ideal to study first? Any advice would be appreciated tremendously.,"['algebraic-topology', 'general-topology', 'differential-topology']"
2235791,Error in approximation by convolution with a mollifier or a Gaussian,"Let $f : \mathbb{R} \to \mathbb{R}$ be bounded and supported on some finite non-empty open interval $(a,b)$. For $\delta > 0$, let $\phi_\delta(x) = \delta^{-1}\phi(x/\delta)$ be the standard mollifier. If $f$ is smooth, what is an upper bound for $\|(f \ast \phi_\delta) - f\|_{L_p}$ in terms of $\delta$? I'm especially interested in the case when $p=1$. What in the case when $f$ is only continuous? It would be interesting to know what happens when $\phi(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$ is the Gaussian as well. I think these things should be well known, but I was not able to find a reference online. Thanks!","['approximation-theory', 'convolution', 'analysis']"
2235802,Equality of integrals versus equality of integrands,"This is a puzzling point for me, and I see it often used in some textbooks when authors switch between differential and integral forms of equations (e.g. in electrodynamics). I wish to know if there is any proven mathematical theorem or lemma that relates to this. Given the equality between two integrals: $$\int_{V} f(x,y,z)\, dV =\int_{V} g(x,y,z)\, dV.$$ It is often said that, if this equality holds for an arbitrary domain $V$ (e.g. a volume), then this  equality implies equality of the involved integrands too, that is $f=g$. Now, although I understand that this works because the arbitrariness of $V$ implies that one could choose an infinitesimally small $V$ where the integrands could be considered constants and taken outside the integrals, allowing for the above conclusion to be obtained, I am not sure whether it is a general rule or one that doesn't have other necessary conditions. For example, would it work if the functions $f$ and $g$ were dependent on $V$ to start with? We could have cases or phenomena where arbitrary $V$ gives different $f$ and $g$ and the equality of integrals still holds, but would it then still imply equality of integrands? Is there any known theorem or lemma that talk about this more rigorously?","['real-analysis', 'mathematical-physics', 'calculus', 'functional-analysis', 'integration']"
2235807,image of a set under a polynomial in $Z_p$ intersecting every interval of length $p/k^{2/3}$,"I am trying to solve the following question: Prove that there exists a constant $c > 0$ so that:   For every $p$
  prime, and any $A \subseteq \mathbb Z_p, |A| = k$ there exists a
  polynomial $f$ of degree at most 3, so that $\{f(a)| a\in A\}$
  intersects every interval of length at least $\frac{cp}{k^{2/3}}$. Now, I have already proved that we can have a linear polynomial so that the image intersects every interval of length $\frac{cp}{\sqrt k}$. I tried to apply the same method here, trying to utilize the fact that I have more freedom to choose the coefficients. This is my attempt at a solution: Choose randomly $x, y, z, w \in \mathbb Z_p$ and consider the polynomial $f(a) = xa^3 +ya^2+za-w$. Denote $A = \{a_1, a_2, \ldots , a_k\}$ and we can assume that $0\notin A$. For every $1 \le i \ne j \le k$ let $X_{ij}$ be the indicator random variable for the event $f(a_i) \in [0,u] \land f(a_j) \in [u+1, 2u+1]$ where $u \sim \frac{p}{k^{2/3}}$. Let $X = \sum_{i \ne j} X_{ij}$. Now, $E[X] = \sum_{i \ne j} E[X_{ij}] = k(k-1)(u+1)^2/p^2 \sim k^{2/3}$. Using Chebyshev's inequality we can bound: 
$$\mathrm {Pr}(X=0) \le \frac{Var[X]}{E[X]^2} \le \frac{1}{E[X]} + \frac{1}{E[X]^2}\sum_{(i,j) \ne (i', j')}Cov(X_{ij}, X_{i'j'})$$ If the covariance term was zero then we'd get $\mathrm {Pr}(X=0) \le \frac{1}{E[X]} \sim \frac{1}{k^{2/3}}$. Then the expected value of the intervals missed by the image of $f$ would be $\sim\frac{p}{k^{2/3}} \sim u$ but this is a contradiction since if an interval of length $2u$ is missed then $u+1$ of its subintervals are also missed (I disregard the constants here to save time). However when calculating the covariance term I get 
$$Cov(X_{ij}, X_{il}) = \frac{(u+1)^3}{p^3} - \frac{(u+1)^4}{p^4} \sim \frac{(u+1)^3}{p^3} \sim \frac{1}{k^2}$$ The summation over all $i,j,l$ gives the order of magnitude of $k$ so when divided by $E[X]^2$ I get a term $\sim \frac{1}{k^{1/3}}$ which is much bigger than the $1/E[X]$ term so the proof fails. Now I'm stuck. Any help will be appreciated :)","['polynomials', 'additive-combinatorics', 'probability', 'combinatorics', 'probabilistic-method']"
2235823,Bounds on volume of a Riemannian manifold given its diameter,"Suppose that we are given an $n$-dimensional Riemmanian manifold $M$. Then it is naturally endowed with a metric function $d$ compatible with Riemannian structure (the distance of two points is the infimum of lengths of all curves connecting the two points). Suppose that the diameter of the manifold is finite, i.e.
$$ L = \mathrm{sup}_{x,y \in M} d(x,y) < \infty. $$
The question is what can be said about the volume of the manifold. In particular, is it true that there exist constants $c,C >0$ such that
$$ c L^d \leq vol(M) \leq C L^d. $$
If this helps I am happy with results with additional assumptions, e.g. that $M$ is compact.","['riemannian-geometry', 'volume', 'geometry', 'metric-spaces', 'differential-geometry']"
2235844,Conditions for integrals to be equal,"Suppose that $f, g,h:[0,1]\rightarrow \mathbb{R}$ are functions that satisfy $f,g\geq 0$ and  $$\int_0^1 f(t)h(t)dt = \int_{0}^1 g(t)h(t)dt.$$ What are necessary and sufficient conditions on $h$ to ensure that $$\int_0^1f(t)dt = \int_0^1g(t)dt?$$ EDIT: ( Some thoughts: )
I thought maybe if $h>0$, this would be enough. However, I was shown that, for example, if $ f = 1/h $ and $g = 1/\left(\int h\right)$, then $$\int fh = \int gh.$$ In this case, for $\int f = \int g$ to be satisfied,  $h$ must satisfy $$\int \frac{1}{h} = \frac{1}{\int h}$$ and of course many $h>0$ do not satisfy that condition (e.g. $h(t) = 1+t$). Since, as user251257 points out, $h(t) \equiv1$ is sufficient, I was curious what the answer to the above might be. ( Where this question came from ) I originally came across this question when looking at what can be said when you have solutions $y_i$ and $y_2$ to the Ricatti equations $y_i' + y^2_i + r_i =0$ on the interval $[a,b]$ that satisfy $y_1(a) = y_2(a)$ and $y_1(b) = y_2(b)$. In this case, you can write $$0 = g(y_2(t)-y_1(t))|_a^b = \int_a^b g(r_1(t)-r_2(t))dt$$ where $g$ is a function that satisfies $g' = (y_1 + y_2)g$. So, I wanted to claim that this means $\int_a^b r_1(t)dt = \int_a^b r_2(t)dt$, and in the process, I realized I didn't know the answer to the question above.","['real-analysis', 'calculus', 'integration', 'lebesgue-integral', 'measure-theory']"
2235857,Proof that a DVR is a Euclidean domain,"I am independently studying valuation rings and I am currently trying to prove that a DVR is a Euclidean domain using the definition here http://abstract.pugetsound.edu/aata/section-factorization-domains.html . Here is what I have: Let $V = \{a \in F| \nu(a)\geq 0\}$ where $\nu: F\rightarrow \mathbb{Z}\cup \{\infty\}$ is a discrete valuation. Then 1.) For all nonzero $a,b \in V$ $\nu(a)\leq \nu(ab)$ since $ab \in \left <a \right >$ (this is a theorem). 2.) Let $a,b \in V$ with $b\neq 0$. Then if $ab^{-1} \in V$, we can write $a =b(ab^{-1}) +0$. If $ab^{-1} \not\in V$, then $\nu(b) > \nu(a)$. Now we can write $\nu(a) = \nu(b +(a-b)) \geq \min\{\nu(b), \nu(a-b)\}.$ I would like to be able to conclude that $\nu(a) \geq \nu(a-b)$ so that I can write $a = 1(b) + (a-b)$. It seems that this follows from $\nu(b) > \nu(a)$, but I am not completely confident that this is the case. Thus, my question is: does $\nu(b) > \nu(a) \Rightarrow \nu(a) \geq \nu(a-b)$? If so, why? Or is my approach flawed?","['valuation-theory', 'abstract-algebra', 'ring-theory']"
2235935,Normal operator is invertible if and only if it is bounded below,"Let $H$ be a Hilbert space, and let $T$ be a bounded linear operator on $H$.  We say that $T$ is bounded below if there exists an $M > 0$ such that $\|Tx\| \geq M \|x\|$ for all $x \in H$. It is a consequence of the open mapping theorem that $T$ is invertible if and only if it is bounded below and has dense image. If $T$ is normal ($TT^{\ast} = T^{\ast}T$), I have heard that $T$ is invertible if and only if it is bounded below.  Is this true?","['functional-analysis', 'reference-request']"
2235941,"Characterisation of Banach space $B$, given the RKHS of a Gaussian random variable in $B$.","The setting: Given a Banach space valued Gaussian random variable $X$ (throughout I will assume everything to be centered and separable) we can define its reproducing kernel Hilbert space $$H:=\overline{\{\mathbb E(hX)| h=f(X),f\in B^*\}}\subseteq B$$ (here $\mathbb E$ denotes the Bochner integral, i.e. $\mathbb E(hX)\in B$) with inner product $\langle \mathbb E(hX),\mathbb E(gX)\rangle:=\mathbb Ehg$ (the usual $L^2$ inner product). The problem: Given the (separable) RKHS $H$, what can we say about $B$? It does not seem to be uniquely determined -- think of Brownian motion on $[0,1]$ with RKHS $H^1([0,1])$ which can be seen as random element in the space of Hölder continuous or just continuous functions or even distributions. My attempts so far: I think one should be able to name a few properties given the following theorems (citing from my lecture): The RKHS $H$ is a separable Hilbert space. The injection map $\text{id}:H\rightarrow B$ is continuous and even compact . [Karhunen-Loève expansion] We can write $X=\sum_j h_jg_j$ a.s. (convergence in $B$) for $g_i$ iid normals and $h_j$ some orthonormal basis of $H$. The map $\phi:B^*\rightarrow H, f\mapsto \mathbb E(f(X)X)$ is weak-* sequentially continuous, i.e. $f_n(x)\rightarrow f(x)$ for all $x\in B\Rightarrow\phi(f_n)\rightarrow\phi(f)$ in $H$. [Cameron-Martin] Let $\mu$ be the law of $X$ (centered!), $\tau_h\mu$ be the pushforward measure under translation by $h$, then $\tau_h\mu$ and $\mu$ are mutually absolutely continuous iff $h\in H$. My interpretations: As a starting point, we only need to look at separable Hilbert spaces $H$. This one looks like it would be good for some kind of characterisation, but I don't quite see what to do with it. Intuitively it tells us that $\|\cdot\|_B$ should be ""coarser"" than $\|\cdot\|_H$, in the sense that it ""doesn't distinguish quite as much between different elements"". The Karhunen-Loève expansion is my biggest hope - it basically says that we only need to consider those spaces $B$ in which this sum converges almost surely. Given some concrete $H$ (like some Sobolev space) I am still not sure how to use this in practice though - in special cases it has been worked out explicitly (e.g. Gaussian Free Fields), but is there a more general theory behind it? This one should restrict the possible options for $B$ a little bit more, but again: No clue how to make this precise. Informally this might tell us that $B$ must not be too small (otherwise we get more $B$ weak-* convergent sequences $f_n$ which would imply convergence of $\phi(f_n)\rightarrow\phi(f)$ for too many $f_n$). Maybe one could turn this in some kind of ""lower bound"" on $B$? Not sure if this is useful at all here. Most of the points above look, in some sense like a ""lower bound"" on $B$ (it has to be at least this big, etc.). Naturally there is no ""upper bound"", but the second point looks promising to derive a non-trivial property $B$ has to fulfil (aside from being at least thaaat big). A wrong conjecture: Note that the second point - which is necessary, but apparently not sufficient - motivates the following construction for $B$: Choose $\|\cdot\|_B$ so that the $H$-unit ball is compact in $(H,\|\cdot\|_B)$ and then take $B$ to be the closure of $H$ with respect to $\|\cdot\|_B$. While this works in the 1D Brownian motion setting with $\|\cdot\|_B = \|\cdot\|_\infty$ and taking $B=C([0,1])$, the closure of $H^1([0,1])$ w.r.t. $\|\cdot\|_\infty$, it gives a $B$ which is too small in dimensions $d\geq 2$. To see this, notice that (by a corollary of Rellich-Kondrachov) we have that the identity from $H^1(D)$ (say $D$ is the unit disk in $\mathbb R^2$) to $L^2(D)$ is compact. However, as seen in https://arxiv.org/pdf/math/0312099.pdf (proposition 2.7), the Gaussian random variable having $H^1(D)$ as RKHS is barely not in $L^2$ in two dimensions and even less so in higher dimensions.","['banach-spaces', 'probability-theory', 'random-variables', 'sobolev-spaces', 'reproducing-kernel-hilbert-spaces']"
2235943,Struggling with Equivalence relations,"I am currently studying for my exams this summer. I have a hard time getting to grips with the following question on discrete mathematics: Consider the relationship $T$ between ordered pairs of natural numbers such that $(a, b)$ is related to $(c, d):$ $$[(a, b) T (c, d)] \iff ad = bc$$ Is T an equivalence relation?",['discrete-mathematics']
2235950,Using Lagrange Multipliers to find the largest possible area of a rectangular box with diagonal length L.,"I am trying to find the largest possible area of a rectangular box having diagonal length L, by method of Lagrange Multipliers. Here is my approach: Let $f(x,y) = xy$ define the area of a rectangular box with length $x$, width $y$. By Pythagorean Theorem, $L^2 = x^2 + y^2$, so that $L = \sqrt{x^2+y^2}$. Define the constraint function $G(x,y) = \sqrt{x^2 + y^2} - L$ Computing the gradients, $\nabla f(x,y) = (y,x)$ $\nabla G(x,y) = (x(x^2+y^2)^{-1/2}, y(x^2+y^2)^{-1/2})$ Then by method of Lagrange Multipliers, $$ y = \lambda x(x^2 + y^2)^{-1/2} $$ $$ x = \lambda y (x^2+y^2)^{-1/2}$$ It is clear to me that one candidate for a critical point is when $x = y = 0$ (from the gradient of f), and $f(0,0) = 0$. With some algebra you can also determine that $x^2 = y^2$ so that $x = y$. But that's as far as I can go. As I don't have any numerical values, I assume I'm finding a general expression that determines the largest possible area of the box. Any help appreciated.",['multivariable-calculus']
2235997,Reflecting ray on triangle in 3D space,"I am working in a 3D space. I have a ray, or more specifically the start and transmission vector of a ray in 3D space. I also have a triangle, which I know for a fact is hit by the ray. By triangle I of course mean I have three 3D points. I can also calculate a normal vector from this triangle easily. Now I need to get a new ray, which would be the reflection after that specific hit. I also don't know exacltly where on the triangle the point of contact is, but it obviously has to be taken into account to get the reflected ray. That point would be the start of that new ray. This is trivial to do in 2D, but I can't figure out how to do it in 3D.","['3d', 'triangles', 'geometry']"
2236002,How do I apply the product (capital pi) symbol here?,"I know $\prod$ is the product symbol but I'm not sure how to apply it in the result shown below.  The $a$, $b$, and $d$ are constants. $$\frac{\prod\left(\frac{(b - a)(b + a)}{d^2 + b^2}\,;x\,\middle|\,\frac{(b - a)(b + a)}{d^2 + b^2}\right)}{(d^2 + b^2)^\frac{3}{2}}$$ Any help appreciated.  By the way, this result above came from the integration of this: $1/(d^2 + b^2\cos^2x+a^2\sin^2x)^\frac{3}{2}$ EDIT:  How do I evaluate that result to a numerical result? (e.g. With integration limits $0$ to $2\pi$)","['special-functions', 'integration', 'notation']"
2236038,Null space of linear differential operator,"Let $L\colon C^n(a,b) \to C(a,b)$ a linear differential operator in the form
$$L= \frac{d^n}{dx^n}+a_1(x)\frac{d^{n-1}}{dx^{n-1}}+ \dotsb +a_{n-1}(x) \frac{d}{dx} +a_n(x),$$ where $a_j (x) $ are continuous functions in $(a,b)$ for $j=1,2,\dotsc,n$. My text book says that the null space of $L$, i,e, those functions of class $C^n$ for which $L(f)=0$ ($0$ is the zero function), has dimension $n$, it seems logical, but I don't know how to prove it, maybe this space consist only of polynomials? Thank you.","['functional-analysis', 'real-analysis', 'ordinary-differential-equations', 'linear-algebra']"
2236119,Is every PID a UFD?,"I was reading PID—""In a PID every non-zero, non-unit element can be written as product of irreducibles."" So here Uniqueness of the product is not mentioned so I think the product may or may not be unique. But when the product becomes unique up to associates then such a PID is called a UFD but this does not mean that every PID is a UFD, rather it means every UFD is a PID, am I wrong somewhere logically? This came to me since when I was referring the Proof of PID implies UFD, there starting from PID it proved that the product is unique up to associates and hence proved UFD?","['abstract-algebra', 'unique-factorization-domains', 'principal-ideal-domains']"
2236143,(Revised) Prove that $\phi$ is a group homomorphism and find the kernel.,"Quoting "" Let $\phi : \Bbb Z \rightarrow \Bbb Z$ be given by $\phi(n) = 7n$. Prove that $\phi$ is a group homomorphism. Find the kernel, and the image of $\phi$."" My understanding: Part 1: Given $n,m \in \Bbb Z$, let's check that $\phi(n+m) =\phi(n)+\phi(m)$ $$\phi(n+m) = 7(n+m) = 7n + 7m = \phi(n)+\phi(m)$$ Therefore as the group operation in $\phi$ is preserved, $\phi$ is a group homomorphism. Part 2 (revised): $\phi$ is one-to-one as:  $\space \space \phi (n) = \phi(m) \Rightarrow 7n =7m \Rightarrow n=m  $. We also know that $\phi(e)=e$ by the property of homomorphism. Therefore, no other object than $0$ in the domain can map to $0$ in the codomain. It follows that:
$$\ker(\phi)=\{ x \in \Bbb Z : \phi(x)=e\}=\{e\}$$ Part 3 (revised): I claim that the only possible image-elements of $\phi$ are the multiples of $7$ in $\mathbb Z$ denoted $7\mathbb Z$. To prove this I have to show that Im$(\phi) \subset 7 \Bbb Z$ and that $ 7 \Bbb Z \subset $ Im$(\phi)$. Proving Im$(\phi) \subset 7 \Bbb Z$
: $\forall y \in $ Im$(\phi)$ such that $  y =\phi(x)$ where $x \in 7 \Bbb Z$. It follows that $y \in 7 \Bbb Z$, therefore Im$(\phi) \subset 7 \Bbb Z$ Proving $ 7 \Bbb Z \subset $ Im$(\phi)$: $\forall y \in  7\Bbb Z$ such that $  y =7x$ where $x \in \phi(x)$. It follows that $y \in \phi(x)$, therefore $ 7 \Bbb Z \subset $ Im$(\phi)$. Any input to my understanding is much appreciated.","['abstract-algebra', 'group-theory', 'proof-verification']"
2236159,"Find function whose gradient is Ax, with A antisymmetric","I have two variables $x_1,x_2\in\mathbb{R}$, and the vector function $g(x)=[x_2,-x_1]^T=Ax$, where A=[0,1;-1,0], and $x=[x_1,x_2]^T$. Is there any way to find a function $f:\mathbb{R^2}\rightarrow \mathbb{R}$, such that $\nabla f(x)=g(x)$?","['matrices', 'symmetric-matrices', 'integration', 'vector-analysis']"
2236160,Integral of $\cos(\sqrt{x^3})$,"As the title explains, I can't seem to get anywhere on this one, so could someone show me how I would proceed with solving this? $$\int\cos\left(\sqrt{x^3}\right)\,dx$$ I can't shrug off the feeling that I'll end up somewhere in the complex realm.",['integration']
2236194,Derivation for hypergeometric distribution formula and comparsion with Bernoulli formula,"To understand the binomial probability distribution function, I was lucky to see the connection between the Bernoulli probability: $P(X=k) = {n\choose k}p^k(1-p)^{1-k}$, which is based on the formula for one try: $P(x=k) = p^k(1-p)^{1-k}$ which is $p$ for $k=1$ and $1-p$ for $k=0$. The Bernoulli probability distribution function is a function that represents the probability of having k successes on $n$ trials of something. If we do the experiment more than one time, let's say, $k$ times and the probability stays the same on every trial, then the probability function $P(s)$ is just the number of $s$ successes in these $k$ trials. This is exactly $kCs$ or $k\choose s$. Why? See the text at $(1)$in the end. Now, for the hypergeometric distribution, we're doing kinda like in the Bernoulli case, but the probability changes after very trial. We can think of the success and failure as being the removal of a ball of color $A$ from a sack of balls of color $A$ or $B$. In the Bernoulli case, it's like we putting the ball back in the sack for the next trial, so the probability of getting a ball of color $A$ is the same on every trial. In the hypergeometric case is like we were not putting it back again, so the probability of getting a ball of color $A$ changes after each trial. For some reason, its formula is:
$$P(X=k) = \frac{{K\choose k} {{N-K}\choose {n-k}}}{N\choose n}$$ Main question: How to  find the formula for the hypergeometric distribution? ** (1) Explanation for the Bernoulli formula (the combinatoric part):** Our experiment for let's say, $k=5$ trials when $S$ means success and $F$ means failure looks like this: $$SSSSS\\ SSSSF \\ SSSFS \\ SSFSS \\ \cdots \\ FFFFF$$ Our function $P(s=3)$ should count how many objects exists in the set above with exactly 3 $S$ letters, for example: $SSSFF, SFSFS, SSFFS, \cdots$. To count this, we're gonna think about how many permutations of $A,B,C$ exist in $A,B,C,D,E$, that is, how many ways can we do things like: $$ABCDE\\ABDCE\\ADEBC\\ \cdots \\ EDBCA$$ We know that there are $5!$ ways to permute every symbol, but we just care about the permutations on $A,B,C$, so $ABCDE$ and $ABCED$ are the same for this case. In general, for every permutation in the set above, there will be an equivalent one with $D$ and $E$ switched. Other example can be: $ABDEC$ being the same as $ABEDC$, so we're gonna divide by how many ways there are to permute $E$ and $D$, which is $2!$. So the number of ways to permute $A,B,C$ in $A,B,C,D,E$ is $\frac{5!}{2!} = \frac{5!}{(5-3)!}$. This is the known formula $\frac{p!}{(p-k)!}$. Getting back to our case, I see the problem of counting how many $3$ $S$ucesses in $5$ trials as counting how many permutations of letters $A,B,C$ in the set $A,B,C,D,E$, with the restriction that we don't want to count repetitions (you'll understand this part soon). For example, the $3$ sucesses would look like this: $$SSSFF\\SSFSF\\SFSFS\\\cdots\\FFSSS$$
I don't know exactly how to explain this part, but I see this as being the same as counting the permutations $A,B,C$ inside $A,B,C,D,E$, except that now, $ABCDE = ACBDE = BACDE = BCADE = CABDE = CBADE$, that is, $S_1S_2S_3FF = S_2S_1S_3FF = S_3S_1S_2FF = S_1S_3S_2FF = S_2S_3S_1FF = S_3S_2S_1FF$. I could differentiate the $F$ letters by $F_1, F_2$ but since we're not gonna count repetitions on $F$ as in the formula $\frac{p!}{(p-k)!}$, I'm considering that there exists only one case. Now we must just se that in general there will be $3!$ equal cases for each object, because $ABCDE = ACBDE = \cdots$. The reason is obvious: there is $3!$ ways to permute $A,B,C$ or $S_1,S_2,S_3$, so we just divide the formula by $3!$ or $k!$, that's why the formula for combination is ${p\choose k} = \frac{p!}{k!(p-k)!}$ UPDATE : While writing this, I realized that it's a lot simpler if we just consider the permutations of the following letters: $S_1,S_2,S_3,F_1,F_2$, eliminate the permutations for $F_1,F_2$ by dividing by $2!$ and eliminate the permutations of $S_1S_2S_3$ because $S_1S_2S_3F_1F_2 = S_1S_3S_2F_1F_2 = \cdots $, which is $3!$ in total. I made a mess but I think it's good to see my thinking process. Now that we counted how many successes there are, it's just a matter of multiplying by $p^k(1-p)^{1-k}$ to get the formula.","['combinations', 'combinatorics', 'statistics', 'probability']"
2236208,What is the geometry of the intersection of hyperelliptic cones and similar objects?,"Given a symmetric matrix $C$ with zeroes on its diagonal I am interested in the solutions of $x^TCx=0$. As $C$ is symmetric with zero trace I can let $C=A^TDA$ for some orthonormal matrix $A$ and diagonal matrix $D$ with at least one positive and negative element (provided $C\neq 0$). If I then make the rotation $y=Ax$ ($A$ is a rotation if you choose the right sign for the eigenvectors) then $x^TCx=0\Leftrightarrow \sum_{i=1}^n D_{ii}y_i^2=0$, which is a much easier shape to conceptualise. For example, when $n=3$ it will be an elliptic cone so long as $D_{ii}\neq 0$ for all $i$. What I am interested in is the intersection of the solutions of $x^TC^kx$ for a collection of matrices $C^1\ldots, C^k$. I can convince myself that the intersection of two unequal elliptic cones is the union of up to four one dimensional subspaces (just look at their mutual intersection on the unit sphere, which are unequal ellipses), but after that I'm lost. In particular I have two questions. First, what is the name of the shape given by $\sum_{i=1}^na_iy_i^2=0$ where $a_i>0$ for some $i$ and $a_i<0$ for some $i$? Second, if for now we call this shape a hyper-X in n dimensions, is the intersection of two of these shapes a hyper-X in $n-1$-dimensions (outside of some pathological counter-examples)?","['abstract-algebra', 'polynomials', 'geometry']"
2236282,Confidence Intervals: Sampling Distribution of the Sample Mean or the Distribution itself?,"When I see a confidence interval, such as the Z-Interval, is it approximating the sampling distribution of sample means or approximating a normal distribution from the sample or is it something else entirely? In particular, I'm wondering about Garwood’s CI for the Poisson Distribution.","['statistics', 'confidence-interval']"
2236283,Can every irreducible cubic be proved irreducible using the Eisenstein criterion?,"Let $k$ be an algebraically closed field, and let $f(X,Y) \in k[X,Y]$.  One common way to show that $f$ is irreducible is to regard $f$ an element of $k[X][Y]$ or $k[Y][X]$ and use the Eisenstein criterion.  Since $k$ is algebraically closed, the nonzero prime ideals of $k[X]$ are of the form $(X - x)$ for $x \in k$, and so to apply Eisenstein in this way, $f$ would have to look something like $$X^3 + g_2(Y-a)X^2 + g_1(Y-a)X + h(Y)(Y-a)$$ for $g_1, g_2 \in Tk[T], h \in k[T], h(Y-a) \neq 0$. Now, $f$ is irreducible if and only if $f(\phi(X,Y))$ is irreducible, where $\phi$ is an affine transformation of the form $X \mapsto cX + dY + e, Y \mapsto fX + gY + h$, and $\textrm{det} \begin{pmatrix} c& d \\ f &g \end{pmatrix} \neq 0$.  So it is possible that one may prove $f$ is irreducible by applying a change of variables, and then apply the Eisenstein criterion. My question is, are there examples of irreducible cubic polynomials for which no change of variables will allow you to conclude irreducibility by the Eisenstein criterion?  If such examples exist, I would particularly be interested in an example with $k = \overline{\mathbb{F}_p(t)}$.","['irreducible-polynomials', 'factoring', 'algebraic-geometry', 'commutative-algebra']"
2236285,Differentiating $\sin(x+y)+ \cos(x+y) = \log(x+y)$ with respect to $x$.,"It was given that $$\sin\left(x+y\right)+ \cos\left(x+y\right) = \log\left(x+y\right)$$ and I was asked to find $\frac{dy}{dx}$ for this.
I attempted it as: $1-$ $$\sin\left(x+y\right)+ \cos\left(x+y\right) = \log\left(x+y\right)$$
$2-$ $$\frac{d}{dx} \left(\sin\left(x+y\right)+ \cos\left(x+y\right)\right) =\frac {d}{dx} \log\left(x+y\right)$$
$3-$ $$\cos\left(x+y\right)\left(1+\frac{dy}{dx}\right) - \sin\left(x+y\right)\left(1+\frac{dy}{dx}\right) = \frac{ \left(1+\frac{dy}{dx}\right) }{x+y}$$ And now in step $4$, term $\left(1+\frac{dy}{dx}\right)$ gets cancelled from both sides and we are left with $$\cos\left(x+y\right) - \sin\left(x+y\right)= \frac{1}{x+y}.$$ So we don't get any term containing $\frac{dy}{dx}$. So how can I find out $\frac{dy}{dx}$ if I am not getting it, same problem arises if I differentiate it again.","['implicit-differentiation', 'ordinary-differential-equations', 'calculus']"
2236295,Is the complement of the closed unit ball connected?,"In $\mathbb{R}^2$, how can I show directly that the complement of the closed unit ball is connected?",['general-topology']
2236330,Column sum of character table,"Consider character table of a finite group, whose rows are indexed by irreducible $\mathbb{C}$-characters and columns by conjugacy classes.
Then Huppert says in his book on character theory: (1) For any row, the sum of entries is always non-negative integer. (2) The sum of column entries is an integer, and may be negative . (3) The Mathieu group $M_{11}$ has a conjugacy class such that its column sum is negative. Q. My question is related to (3); isn't there any smaller order group for which column sum is negative?","['finite-groups', 'representation-theory', 'group-theory', 'characters']"
2236356,Computing $x^{2017}+\frac1{x^{2017}}$ given the value of $x+\frac1x$.,"If $x+\dfrac{1}{x}=2017$, then the value of $x^{2017}+\dfrac{1}{x^{2017}}$ is, A) $2017^{2016(\sqrt{5}-\sqrt{3})}$ B) $2017^{2016(\sqrt{3}-\sqrt{2})}$ C) $2017^{2\sqrt{3}}$ D)  $2017^{3\sqrt{2}}$ E) None of the above Progress: We could make use of the identity $$x^{m+n}+\dfrac{1}{x^{m+n}}=\left( x^m+\dfrac{1}{x^{m}}\right)\left(x^{n}+\dfrac{1}{x^{n}}\right)-\left(x^{m-n}+\dfrac{1}{x^{m-n}}\right), $$ however this is quite tedious since partitioning $2017$ and going on further takes a lot of time. Any solutions or hints are welcome.",['algebra-precalculus']
2236359,Galois theory and cryptography,I'm trying to find applications of Galois theory in different areas. Currently the most interesting for me is the cryptography. As I understood the theory of finite fields (a part of Galois theory) has a strong connection with cryptography. My question is the following: Is there an important cryptographic problem that cannot be solved without the Galois theory application?,"['finite-fields', 'abstract-algebra', 'galois-theory', 'cryptography']"
2236379,Is there any extension theorem about $n$-sphere?,"Let $A$ be a closed subset of $n$-sphere, $f$ a continuous surjective map from $A$ to $n$-sphere, could it be extended to the whole $n$-sphere? If not, what conditions should be added to make it true? Since there exists surjective map from $[0,\infty)$ to $\mathbb{R}^n$, thus to $S^n$, I want to make use of it and find a proof similar with Tietze extension theorem, but I haven't thought of anything concrete.","['general-topology', 'real-analysis']"
2236413,"What does it mean that the three-body problem has been proved to be ""unsolvable"" and how was it done?","Wikipedia says: In 1887, mathematicians Heinrich Bruns and Henri Poincaré showed that there is no general analytical solution for the three-body problem given by algebraic expressions and integrals. The motion of three bodies is generally non-repeating, except in special cases. I have two questions: What does an ""analytical solution"" mean here -- analytic in the sense of an analytic function ? Or in the sense of being built out of elementary functions? At a high level, how does the proof go? How does one rule out the existence of a solution like this? [As a sidebar: is there a mathematical connection between Wikipedia's second sentence (""generally non-repeating"") and its first? I can certainly imagine non-repeating solutions which are analytical. Are repeating solutions necessarily analytical?]","['physics', 'ordinary-differential-equations']"
2236416,"How to show for $x\in \mathbb{R}$, $|x|\leqslant 1+x^2$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question How to show for $x\in \mathbb{R}$, $$|x|\leqslant 1+x^2$$ I can show $2|x|\leq 1+x^2$, but how to show the above?","['algebra-precalculus', 'inequality', 'quadratics']"
2236461,Solve the differential equation $y''+4y=g(t)$ using Laplace transformation,"How can i solve the following differential equation using Laplace Transformation? $$y'' + 4y = g(t)$$ 
$$Y(0)=0; Y'(0)=0$$
where: 
$$
 g(t) =
\begin{cases} 
0; &  0 \leq t <5  \\ 
(t-5)/5; &  5 \leq t <10 \\
1; &  t \geq 10 \\
\end{cases},
$$",['ordinary-differential-equations']
2236470,Find limit of sequence using integral,"Given sequence $$a_m = \frac{1}{m^2} \sum \limits_{k = 1}^m \sqrt[3]{(mx + k + 1)\cdot (mx + k)^2}$$ Find its limit using integral. I thought it may be solved using either Euler-Mclaurin formula or Riemann sum. Unfortunately, the function under the sum sign is pretty uncomfy to operate with.","['limits', 'integration', 'definite-integrals', 'riemann-sum', 'summation']"
2236490,Closed form for definite integral involving Erf and Gaussian?,"Question Is there a closed form for integrals such as $\int_{-\infty }^{\infty } e^{-y^2} \text{erf}(1-y) \, dy$ The integrant seems simple enough?","['integration', 'analysis']"
2236525,Doubling measure implies doubling metric space,"I have the following as an exercise. A measure $\mu$ is doubling if there exists a constant $c_d$ such that $\mu(B(x,2R)) \leq c_d \mu(B(x,R))$ for all balls in the metric space. The claim is that this implies the metric space is doubling, that is there exists $N\in \mathbb{N}$ such that a ball of radius R can be covered with at most $N$ balls of radius $R/2$. I tried proving this indirectly: Suppose you need countably many balls of radius $R/2$ to cover a ball of radius $R$: $$\bigcup_{i=1}^N B(x_i,R/2) \subset B(x,R) \Rightarrow \mu\left(\bigcup_{i=1}^N B(x_i,R/2)\right) \leq \mu(B(x,R)) $$ for all $N$. Then I would use doubling condition to see that $\mu(B(x,R)) \geq \infty$ which is a contradiction. However, I don't know how to get sum of measures between the inequality. I was also thinking that maybe I can arrange balls $B(x_i,R/2)$ such a way that every ball covers alone some set of positive measure, so that the measure of $B(x,R)$ would again be infinite, but I couldn't figure out that rigorously. (How I ensure that there wouldn't be alot of of non-open sets to cover after some point?) I also thought that maybe it should be proven directly using
$$ c_1\left(\dfrac{r}{R}\right)^{Q_1} \leq \dfrac{\mu(B(x,r))}{\mu(B(y,R))} \leq c_2\left(\dfrac{r}{R}\right)^{Q_2},  $$
but again I didn't figure out how to control overlapping in the covering. Some hint to get to the right direction would be appreciated.","['metric-spaces', 'measure-theory']"
2236532,Proof of Implicit Function Theorem: special case,"I was reading this pdf online on the Implicit Function Theorem (pg 18-19). I did not understand the last line : Why "" continuity of $f$ and uniqueness of $y$ implies continuity of $g$ ""?","['derivatives', 'implicit-function-theorem', 'multivariable-calculus', 'proof-explanation', 'manifolds']"
2236605,Rolling a die 1000 times. Estimate the probability $ℙ( \prod^{1000}_{n=1} X_n ≤ z^{1000})$ for real $1 < z < 6$,"Rolling a die 1000 times, denoting the outcome of roll by $X_n$. Estimate the probability  $ℙ( \displaystyle\prod^{1000}_{n=1} X_n ≤ z^{1000})$ for real $1 < z < 6$ I've taken the natural log of both sides to change the product into a sum and I've subtracted $1000*𝔼[ln(X_n)]$ and divided by $\sqrt{1000*Var(ln(X_n))}$ on both sides so that it has standard normal distribution. I was going to use the Central Limit Theorem but as $z$ is an unknown real I think the Law of Large Numbers would be more useful but I am unsure how to go about it. I know that with a low $z$ the value will be closer to $0$ and a high $z$ it will be closer to $1$. Any help would be greatly appreciated!","['statistics', 'probability']"
2236656,$f(A-B)=f(A)-f(B)$ when f is an injection,"I have $f:X\to Y$, an injection, and sets $A$ and $B$ subsets of $X$ and $Y$ respectively. Now I have to prove that $$f(A-B)=f(A)-f(B).$$ I do know that they are not equal all the time; I came up with counter examples for that. However, I have an intuition that it may be a different case when f is injection, but I cannot even start my proof. Can you help me with this?","['real-analysis', 'functions']"
2236675,Compute $\lim\limits_{x\to 0_+}e^{\frac{1}{\sqrt{x}}}\cdot(1-\sqrt{x})^\frac{1}{x}$,"Evaluate $$\lim_{x\to 0_+}e^{\frac{1}{\sqrt{x}}}\cdot(1-\sqrt{x})^\frac{1}{x}$$
I tried using $$\lim_{x\to 0}(1+x)^\frac{1}{x} = e$$ like so:
$$l = \lim_{x\to 0_+}e^\frac{1}{\sqrt{x}}\cdot\bigg[\big(1+(-\sqrt{x})\big)^{-\frac{1}{\sqrt{x}}}\bigg]^{\frac{-1}{\sqrt{x}}} = \lim_{x\to 0_+}e^{\frac{1}{\sqrt{x}}-\frac{1}{\sqrt{x}}} = e^0 = 1$$
However, the right answer is $\frac{1}{\sqrt e}$. Why is it that the whole expression in square brackets can't be taken as $e$ in this case?","['real-analysis', 'limits']"
2236696,Wave Equation by separation of variables,"I have a simplified version of the wave equation which I need to solve using variable separation . This formulation is destined to represent the propagation of a wave in a thin column subjected to a constant load on the free end and fixed at the bottom end (""standing wave""): GOVERNING PDE $$u_{xx} = u_{tt}$$ Since this is not a ""book problem"" the boundary/initial conditions are a bit fuzzy to me. I am however quite certain of the following: BOUNDARY CONDITIONS $$u(0,t) = 0 $$
$$u_x(L,t) = 1$$
$$u_t(0,t) = 0$$ First arises from fixed end, second from constant load and 3 again from fixed end. INITIAL CONDITIONS $$u(x,0) = 0$$
$$u_t(x,0) = 0$$ Represents the at-rest state of the column before loading is applied. ATTEMPTED SOLUTION I tried to solve this equation by variable separation but reached a dead end: The separation of variables assumption states that $u(x,t) = X(x)T(t)$ which breaks the problem down into a set of two ODE's as follows: $$X''+\omega^2X=0$$
and,
$$T''+\omega^2T = 0$$ For which the solutions are $$X(x) = A\sin\omega x+B\cos \omega x$$
and $$T(t) = C \sin\omega t+D\cos \omega t$$ Applying BC (1) yields $B=0$. IC (1) yields $D=0$ then we are left with: $$AC\omega \cos (\omega L) \sin (\omega t)=1$$ and $$AC \sin (\omega x)=0$$ Which is sort of a dead end... since we cannot solve for constant $AC$. Does anyone know how to approach this specific problem using separation of variables? EDIT After following though with @DisintegratingByParts solution, I got the following results: $$B_n = \frac {-2L}{(n+\frac{1}{2})^2\pi^2} \sin \left(\left(n+\frac{1}{2}\right)\pi \right)$$ Therefore $$u(x,t) = v(x,t)+x$$ and $$v(x,t) = \sum_{n=0}^{\infty}B_n\sin((n+1/2)\pi x/L)\cos((n+1/2)\pi t/L)$$. Because $$u(x,t) = v(x,t)+x$$ and $$v(x,t) = \sum_{n=0}^{\infty}\frac {-2L}{(n+\frac{1}{2})^2\pi^2} \sin ((n+1/2)\pi)\sin((n+1/2)\pi x/L)\cos((n+1/2)\pi t/L)$$, $$u(x,t) = x+\sum_{n=0}^{\infty}\frac {-2L}{(n+\frac{1}{2})^2\pi^2} \sin ((n+1/2)\pi)\sin((n+1/2)\pi x/L)\cos((n+1/2)\pi t/L)$$ Plotting this for $x=L/2$ and $t$ from $0$ to $5$ against a reference solution yields the following plot Where the red is the reference solution and the blue is the solution proposed by @DisintegrationByParts. EDIT 2 The reference solution I am using is the following: where $\lambda = \frac {(2n-1)\pi}{2L}$ and $p_0=K=L=c=1$","['algebra-precalculus', 'wave-equation', 'linear-algebra', 'partial-differential-equations']"
2236718,Big -O time complexity of an algorithm.,"I need to analyze this pseudo code, do_something() is O(1) and you may assume N = 2^k for some positive integer k. I will write on each line the answer I think is correct. Set n ← N   **1**
For i ← 1 to n  **n**
  set counter ← 1  **1**
  While counter ≤ n   **logn**
    For j ← counter to 1 **Im not sure about it, maybe 2^n -1**
      do_something()  **1**
    end
    counter ← 2 * counter **1**
  end
end Is my analysis correct? And how I translate it to big O notation? Thank you.","['algorithms', 'asymptotics', 'analysis', 'computer-science']"
2236742,Rewriting Sample Variance Formula,Why can $s^2 = \frac{\sum_{i=1}^{n}{(X_i - \bar{X})^2}}{n-1}$ be written as $ s^2 = \frac{\sum_{i=1}^{n}{X_i^2 - n(\bar{X})^2}}{n-1} $?,"['statistics', 'variance']"
2236778,Infinite countable Union of $\sigma-\text{Algebras}$ exercise,"Show that if $\{\mathscr{A}_n\}_{n\in\mathbb{N}}$ is an increasing sequence of $\text{Algebras}$ of subsets of a set $X$. Show by example that even if $\{\mathscr{A}_n\}$ is a $\sigma-\text{Algebras}$ for every ${n\in\mathbb{N}}$ the union still may not be a $\sigma-\text{Algebra}$.
Solution:Let $X=\mathbb{N}$, and $\mathscr{A}_n=$ the family of subsets of $\{1,2,3...n\}$ and their complements. Clearly, $\mathscr{A}_n$ is a $\sigma-\text{Algebras}$ and $\mathscr{A}_1\subset\mathscr{A}_2\subset\mathscr{A}_3...$ However $\bigcup_\limits{n\in\mathbb{N}}^{}\mathscr{A}_n$ is the family of all finite and co-finite subsets of $\mathbb{N}$ which is not a $\sigma-\text{Algebra}$$\:\:\blacksquare$ How can $\bigcup_\limits{n\in\mathbb{N}}^{}\mathscr{A}_n$ not be a $\sigma-\text{Algebra}$ if the sequence is increasing? It seems a contradiction to me.
Thanks in advance.",['measure-theory']
2236779,Nowhere differentiable continuous functions and local extrema,"Consider any continuous function which is nowhere differentiable (such as Weierstarss function). If this function would be monotone on some interval $(a,b)$ then there is a result which states that in this interval this function would be almost everywhere differentiable which is contradiction. Therefore any such function cannot be differentiable in any interval. However this not settles the issue of having local extrema. Is it possible to construct everywhere continuous, nowhere differentiable function $f:[-1,1] \to \mathbb{R}$ with the following property: $f$ has local minimum at $x_0=0$, $f$ has local maximum at $x_1=\frac12$, $f(0)>f(\frac12)$ and $x_0,x_1$ are only extremas of $f$","['derivatives', 'real-analysis', 'continuity', 'analysis']"
2236785,How to determine the optimal control law?,"Given the differential equation $$\dot x = -2x + u$$ determine the optimal control law $u = - kx$ that minimizes the performance index $$J = \int_0^{\infty} x^2 \, \mathrm d t$$ My approach was to find the state feedback $k$ . But since the value of $R$ (positive semidefinite Hermitian) is not given, that means $R=0$ . How do I determine the optimal control for this system where $R=0$ ?","['optimization', 'optimal-control', 'control-theory', 'linear-control', 'ordinary-differential-equations']"
2236828,Proof of $f(A∩B)⊆f(A)∩f(B)$ [duplicate],"This question already has answers here : Prove $f(S \cap T) \subseteq f(S) \cap f(T)$ (3 answers) $f(A \cap B)\subset f(A)\cap f(B)$, and otherwise? (4 answers) Closed 7 years ago . Let there be a function $f:R→R$. Let $A$ & $B$  be two subsets of $R$. From my experience, I know that $f(A∩B)⊆f(A)∩f(B)$. I don't know how to prove it. Also, are there certain conditions that the function $f$ or the sets $A$ & $B$ should follow for the above condition to be true?","['elementary-set-theory', 'functions']"
2236859,Possible trivial counterexample to exam question [duplicate],"This question already has answers here : A ""reverse"" diagonal argument? (2 answers) Closed 7 years ago . So doing an exam question for set theory this was the question: Let X be a set. Prove there is no injection $f : P(X) → X.$ Would X being an empty set be a counter example because in this case the powerset is a set of a single element $\{\}$ hence it is mapped to a null element. However would this be the case that $f$ would not be a function, since there isn't $(a,b)\in f$ hence $f$ isn't defined for the whole of the domain hence $f$ is not a function. So yes I understand that this isn't a proof for this problem but while thinking about the problem I was trying some examples and non-examples and couldn't work out why this failed until I actually put it into words. Sorry for a convoluted question, I don't really have the correct notation to explain myself.",['elementary-set-theory']
2236897,Series solution for a differential equation.,"I have the differential equation $(1-x^2)y'' -2xy' +\lambda y = 0$. If I solve it with a power series of the form $\displaystyle \sum_{n=0}^{\infty} a_n (x+1)^n $ centered at $x=-1$ I got a recurrence relation. Should I get the same recurence if I solve my differential equation with a power series of the form $\displaystyle \sum_{n=0}^{\infty} a_n x^n$ centered at $x=0$? Should it give the same coefficients $a_j$? Thanks in advance, Best regards.","['recurrence-relations', 'ordinary-differential-equations', 'power-series']"
2236905,Find all the solutions of a trigonometric equation,"Find all the solutions of the equation
$\sin^5(x) + \cos^3(x) =1.$
I tried it and get 
$(1-\cos(x))(\sin^3(x)-\cos^3(x) -\cos^2(x) -1)=0$
but I have no idea how to proceed from here.
please help me to solve it.
Thanks in advance.",['trigonometry']
2236919,Find the rational canonical form of a matrix from its minimal and characteristic polynomials,"What is the rational canonical form of $A$?
  $$A=\begin{bmatrix}
    1       & 1 & 0 & 0 \\
    0       & 1 & 0 & 0 \\
    2      & 3 & -1 & 4\\
   1     & 1 & -1 & 3\\
\end{bmatrix}$$ I found that the minimal polynomial $m_A(x)=(x-1)^2$ and the characteristic polynomial $c_A(x)=(x-1)^4$. Therefore the invariant factors can be 
$$x-1,x-1,(x-1)^2$$
or
$$(x-1)^2,(x-1)^2$$
Therefore the rational canonical form may be
$$\begin{bmatrix}
    1      & 0 & 0 & 0 \\
    0       & 1 & 0 & 0 \\
    0     & 0 & 0 & -1\\
  0    & 0 & 1 & 2\\
\end{bmatrix}$$
or
$$\begin{bmatrix}
    0      & -1 & 0 & 0 \\
    1       & 2 & 0 & 0 \\
    0     & 0 & 0 & -1\\
  0    & 0 & 1 & 2\\
\end{bmatrix}$$ How do I quickly figure out which one is the correct one?","['abstract-algebra', 'linear-algebra']"
2236922,"Show $\mathbb{E}(\theta| Y_{1}, \ldots, Y_{n}) \to \theta$ a.s.","The following is excercise 5.5.2 from Durrett's Probability Theory and Examples . Let $Z_{1}, Z_{2}, \ldots, Z_{n}$ be i.i.d. with
  $\mathbb{E}|Z_{i}|<\infty$, let $\theta$ be be an independent random
  variable with finite mean, and let $Y_{i}=Z_{i} + \theta$. Show that
  $\mathbb{E}(\theta| Y_{1}, Y_{2}, \ldots, Y_{n}) \to \theta$ a.s. I have started by using theorem 5.5.7 in Durrett to show that, if $\mathcal{F}_{n} = \sigma(Y_{1}, \ldots, Y_{n})$, then we have
$$\mathbb{E}(\theta | Y_{1}, Y_{2}, \ldots, Y_{n}) = \mathbb{E}(\theta | \mathcal{F}_{n}) \to \mathbb{E}(\theta | \mathcal{F}_{\infty})$$
a.s. and in $L^{1}$, where $\mathcal{F}_{\infty} = \sigma\left( \bigcup_{i\geq 1} Y_{i}\right)$. Now if I knew that $\theta$ was measurable w.r.t. $\mathcal{F}_{\infty}$ then I would have that $\mathbb{E}(\theta | \mathcal{F}_{\infty}) = \theta$, but sadly I do not know that this is true... I am pretty much stuck here. I do not know how to incorporate that the $Z_{i}$'s are i.i.d. with finite mean, or that $\theta$ is independent with finite mean. I am honestly so bad at this stuff. Any help is appreciated.","['martingales', 'probability-theory', 'measure-theory', 'convergence-divergence']"
2236935,Density of Fredholm operators,Let $X$ be a Banach space. It is well known that an operator $T\in B(X)$ is Fredholm if and only if $\pi(T)$ is invertible in the Calkin algebra $B(X)/K(X)$. Now suppose that the invertible elements in $B(X)/K(X)$ are dense. Can we conclude that Fredholm operators are dense in $B(X)$? Note that there exist spaces for which every operator $T\in B(X)$ is Fredholm with index 0.,"['functional-analysis', 'banach-spaces', 'operator-theory', 'banach-algebras']"
2236943,How many roots does $f=x^4+x^2+2$ have in $\mathbb Q[x]/(f)$?,"Given an irreducible $f\in \mathbb Q[x]$, how to determine the number of roots of $f=0$ in $\mathbb Q[x]/(f)$? For some simple examples, I know how to do. If $f(x)=x^2+x-1$, the we use $x\in\mathbb Q[x]/(f)$ to represent $(-1+\sqrt 5)/2$, and $-x-1$ will be another root. If $f=x^3-2$, $x\in\mathbb Q[x]/(f)$ will represent $2^{1/3}$, and it is the only root of $f=0$ in $\mathbb Q[x]/(f)$, since the other roots are complex. But for more complicated $f$, e.g. $f=x^4+x^2+2$, I have no idea how to find the number of roots in the extended field. Thank you.","['abstract-algebra', 'extension-field', 'field-theory']"
2236949,Is uniform ($L^\infty$) convergence for random variables useful?,"The convergence modes for random variables are mostly analogous to those in real analysis: convergence in distribution $\leftrightarrow$ weak convergence convergence in probability $\leftrightarrow$ (unnamed? I've seen it called convergence in $L^0$) convergence a.s. $\leftrightarrow$ pointwise a.e. convergence convergence in $L^p$ $\leftrightarrow$ convergence in $L^p$ (assume $1 \leq p < \infty$) In real analysis we emphasize pointwise convergence vs. uniform convergence, so I was wondering if the concept of uniform convergence also carries over into probability theory. The definition of convergence in $L^\infty$ seems to be the right one as it allows one to exclude sets of measure zero: $$||f||_\infty := \inf\{C \geq 0 : |f(x)| \leq C \text{ for a.e. }x\}$$
$$X_t \overset{L^\infty}\to X \quad\text{if}\quad\lim_{t\to\infty} ||X_t-X||_\infty = 0.$$ Is the concept of $L^\infty$ convergence used in probability theory?","['probability-theory', 'convergence-divergence', 'uniform-convergence', 'random-variables']"
2237003,How do I show that the complex conjugate of an integral is equal the integral of the conjugate?,"$f:[a,b]\mapsto \mathbb{C}$ is Riemann integrable. It is given also that $a,b \in\ $ and that $a<b $.
How do I show that: $\int_{a}^{b} \! \overline{f(x)} \, dx=\overline{\int_{a}^{b} \! f(x) \, dx } $ $\int f(x) dx = \int \operatorname{Re} f(x) dx + i\int \operatorname{Im} f(x) dx$ $\int_{a}^{b} \! \overline{ \operatorname{Re} f(x)  + i\int \operatorname{Im} } \, dx=\overline{\int_{a}^{b} \!  \operatorname{Re} f(x) + i\int \operatorname{Im} \, dx } $ $\int_{a}^{b} \!{\operatorname{Re} f(x) dx - i\int \operatorname{Im} f(x) } \, dx={\int_{a}^{b} \! \operatorname{Re} f(x) dx - i\int \operatorname{Im} f(x) dx } $ Is that all that I need for proof?",['complex-analysis']
2237015,Calculus determine whether a function is differentiable (hard),"I know that i need to prove 
$$\lim_{ h\to0^+}\dfrac{ g(x+h) - g(x) }{h}$$
exists but i cannot find a way to simplify the expression ...","['derivatives', 'calculus', 'limits']"
2237026,Non-linear first order differential equations,"What technique should I use to solve the following first-order nonlinear ordinary differential equation:
$$y'(x)+\log (y(x))=-x-1$$ Thanks in advance",['ordinary-differential-equations']
2237044,Find the determinant of the linear transformation $L(A)=A^T$ from $\mathbb{R}^{n\times n}$ to $\mathbb{R}^{n\times n}$,"Find the determinant of the linear transformation $L(A)=A^T$ from $\mathbb{R}^{n\times n}$ to $\mathbb{R}^{n\times n}$. The solution is $(-1)^{\frac{n(n-1)}{2}}$. I've found some resources to do this when for the case from $\mathbb{R}^{2\times 2}$ to $\mathbb{R}^{2\times 2}$ (in fact from stackexchange ), but in the $n$ case, I'm really confused about the formula. Using the same logic as the 2x2 case, I get that in an equivalent mapping of $L:\mathbb{R}^{n^2} \rightarrow \mathbb{R}^{n^2}$ is: $L(e_{11},e_{12},e_{nn})=\begin{bmatrix}
    1 & 0 & 0 & \dots  & 0& 0 \\
    0 & 0 & 0 & \dots  & 1& 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots& \vdots \\
0 & 1 & 0 & \dots  & 0& 0 \\
    0 & 0 & 0 & \dots  & 0& 1
\end{bmatrix} \begin{bmatrix}
    e_{11} \\
    e_{21} \\
    \vdots\\
\vdots \\
    e_{nn}
\end{bmatrix}$ Where we can find the determinant of the large $0$ and $1$ matrix above (let's call it matrix B) to find the determinant of the linear transformation L. I know that $det(B)=(-1)^s$ where $s=$the number of swaps made to get B into rref form. However, I just can't see how we need $\frac{n(n-1)}{2}$ swaps and not $n$ swaps?","['linear-algebra', 'linear-transformations', 'determinant']"
2237070,minimum distance for 3 points to stand on a line,"We have the coordinates of 3 points like $A$, $B$ and $C$ how to calculate the minimum distance $x$ such that if we move $A$, $B$, and $C$ at most $x$ meters they stand in a line","['algebraic-geometry', 'geometry']"
2237097,"Given a $2\times 2$ matrix $A$, compute $A^7$.","Let $A =
        \begin{pmatrix}
        e^{2x} & -1  \\
        0 & e^{2x}-1  \\
        \end{pmatrix}
$. Compute $A^7$. I've tried the obvious way of multiplying $A$ with $A$, then $A^2$ with $A^2$, but I arrived at a messy result in the top right member of the matrix. Is there a general form to be noticed here?","['matrices', 'linear-algebra']"
2237106,Solution of ordinary linear differential equation with variable coefficients,"Is it possible to solve a second order ordinary linear differential equation with variable coefficients (Polynomials) by using Laplace transform method? If possible please guide.
$$y''+2xy'+8y=0,     \\y(0)=3,  y'(0)=0. $$",['ordinary-differential-equations']
2237116,Ring of regular functions of isomorphic quasiprojective varieties,I know that if two quasi-projective varieties are isomorphic then their coordinate rings may not be isomorphic. But what can we say about their ring of global regular functions? Are they isomorphic? Let $X\subset\mathbb P^n$ be a quasi-projective variety. Denote $$K[X]=\{f:X\rightarrow k\mid f\text{ is regular at every point of }X\}.$$ So my question is if $X\cong Y$ then is it true that $K[X]\cong K[Y]$? My guess is they are isomorphic. Please help me. Thank you,"['projective-space', 'algebraic-geometry', 'commutative-algebra']"
2237174,Splitting the birthday paradox into months,"One of the biggest studies in my universities' statistics class is the Birthday Paradox. It main goal is to determine that, given n students in a class, what's the probability that at least two of these students share a birthday. Now, suppose there are k students in a class and every student is equally likely to be born in any of the twelve months of the year. One of the questions we were asked is: What is the minimum value for k such that the probability there are two (or more) students born in the same month is at least 1/2? I modified the original equation to try and account for the fact that this takes count of months and not the entire year. P(A) = Event that two people have the same birthday in the month. Thus, $$P(A') = \frac{k! \cdot365 {365 \choose 30}}{365^k}$$ Where the ${365 \choose 30}$ is taking into account a specific given month. (This is where I believe I'm making an error) Then, the answer can be achieved by $$P(A) = 1-P(A')$$
Finally, I can use some bounding and solve for $$P\left(A \geq \frac{1}{2}\right) $$ However, I'm missing lots of calculations in between the lines which I am not sure how to achieve. How can I go about solving this problem appropriately?","['statistics', 'probability', 'paradoxes']"
2237184,Question about definition of independent family of events,"A family $\mathcal A=(A_i:i\in I)$ of events is called independent, if for all finite subsets $J$ of $I$,
  $$
\mathbb P\left(\bigcap_{i\in J}A_i\right)=\prod_{i\in J}\mathbb P(A_i).
$$ I'm wondering why we need $J$ to be a finite subset? Infinite products are defined after all, and we also have the following:
$$
\mathbb P\left(\bigcup_{i=1}^\infty A_i\right)=\sum_{i=1}^\infty \mathbb P(A_i),
$$
where $A_1,A_2,\dots$ are disjoint events in $\mathcal F$. So I don't see why the definition requires $J$ to be finite; where would it go wrong otherwise?","['independence', 'probability-theory']"
2237185,deriving the formula of the torsion of a curve,"in our class we defined the torsion $τ(s)$ of a curve $γ$ parameterized by arc length this way 
$τ(s) = B'(s) \cdot N(s) $ where $B(s)$ is the binormal vector and $N(s)$ the normal vector in many other pdf's and books it's defined this way ($τ(s) =  -B'(s) \cdot N(s)$) but let's stick to the first definiton. we were given in our class other formulas to compute the torsion : $$τ(s) = -\frac{\det(γ'(s),γ''(s),γ'''(s))}{||γ''(s)||^2} $$ $$τ(t) = -\frac{\det(γ'(t),γ''(t),γ'''(t))}{||γ'(t)\timesγ''(s)||^2}$$ ok the first one is used when the curve is parameterized by arc length and the second one can be used to compute the torsion of any regular curve $γ$ whether $||γ'|| = 1$ or not I tried proving them both and i think I've been able to prove the first one : $$\begin{align} τ(s) = B'(s) \cdot N(s) = (T(s)\times N(s))' \times N(s) =(T'(s) \times N(s) + T(s) \times N'(s)) \cdot N(s)\end{align}$$ since the curve is parameterized by arc length $T'(s) = N(s)$ so $T'(s) \times N(s) =0$ $$\begin{align} τ(s) =( T(s) \times N'(s)) \cdot N(s)=\det( T(s) , N'(s),N(s)) \end{align}$$ $$\begin{align} =-\det( T(s) , N(s),N'(s)) =-\det(γ'(s),\frac{γ''(s)}{||γ''(s)||},\frac{γ'''(s)}{||γ''(s)||}) = -\frac{\det(γ'(s),γ''(s),γ'''(s))}{||γ''(s)||^2}\end{align}$$ check this proof and tell me If I proved it right for the second one I tried replacing $γ'(s)$ by $γ(s^{-1}(t))'$ where $s(t) = \int_0^t ||γ'(u)||du$ did the same thing for $γ'(s)$ and $γ''(s)$ applied the chain rule but got stuck any help or hints concerning the second formula would be appreciated. Thank you !","['curves', 'calculus', 'geometry']"
2237192,Ho to determine with following statements using logic :,"Been trying really hard to get to a conclusion using these statments Consider the following two statements:
1.  The book is either at Anna’s house or at Mary’s house. Assume A means “the book is at Anna’s house” and M means “the book is at Mary’s house”. We can represent this as A V M.
2.  We also know that the book can’t be at both places at the same time, i.e.
(A & ¬M) V (¬A & M).
3.  If the book is not at Anna’s house, then it must be at Mary’s house, i.e.
¬A  M Given these three statements, can we determine where the book is? Explain you answer in detail step by step. You must prove your “yes/no” answer.","['logic', 'discrete-mathematics']"
2237193,To show a function is bilinear symmetric non degenerate form,"Let $V$ be vector space of set of $n×n$ matrices over $R$. Define $\langle A,B \rangle = \mathrm{trace}(AB)$, $A$, $B$ in $V$. show that $\langle \ \ ,\  \rangle$ is a non degenerate symmetric bilinear form. Now succeeded in showing that the function  $\langle \ \ ,\  \rangle$ is a symmetric bilinear form by checking the properties of bilinear form but now to show that it is non degenerate I need help.
I know a function is nondegenerate if left radical or right radical of  $\langle \ \ ,\  \rangle$ is zero.
That means for a bilinear form $B$ on $V$ , if 
I get $S=\{y\in V \ |\  B(x,y)=0, \mbox{for all } x\in V\}=\{0\}$ then $B$ is nondegenerate. So if I start with  $\langle A, B \rangle =0$
Then $\mathrm{trace}(AB)=0$ for all $A\in V$ My claim is to show $B=0$?
Am I going right?",['linear-algebra']
2237207,Evaluate $\int_0^1 \sin\left(\sqrt{\frac{1-x}{x}}-\sqrt{\frac{x}{1-x}}\right)\frac{dx}{x\left(3x^2 - 3x +1\right)}$,"I have it on good authority that the following monstrosity $$I=\int_0^1 \sin\left(\sqrt{\frac{1-x}{x}}-\sqrt{\frac{x}{1-x}}\right)\frac{dx}{x\left(3x^2 - 3x +1\right)}$$ is not only convergent but has an analytic closed form. After spending a long time struggling with it I was not able to convert it into a familiar form, mainly because of the nested function within the $\sin(\cdot)$ term which I am unable to get rid of. The essential singularities of the integrand about $x=0$ and $x=1$ make it difficult even to approximate the integral to any meaningful precision, so I am unable even to formulate a conjecture as to the closed form. I am hoping that some kind soul will be able to rid me of this suspense and solve the devil-incarnate that is this integral.",['integration']
2237250,Volume of a body formed by rolling a sphere inside an ellipsoid,"Is there any analytical solution or approximation to the volume of a body that is formed by the center of a sphere of radius $r$ that is rolling inside an ellipsoid? The answer to the extreme limiting cases are obvious: when the ratio of the smallest semi-axis of the ellipsoid to the sphere's radius is less than 1, $\frac{a_{min}}{r}\leq 1$, then $V=0$. when $\frac{a_{min}}{r}>> 1$, then $V\rightarrow V_{ellipsoid}$. Is there an analytical expression that quantifies $V$ as a function of the ellipsoid eigenvalues (i.e., semi-axes)? Note that, by definition, all points of this body are at least a distance of $r$ from the interior surface of the ellipsoid.","['volume', 'convex-optimization', 'differential-geometry', 'geometry']"
2237271,Transformation of $|z-1|=1$ and $\mathrm{Re}(z)^2 = \mathrm{Im }(z)^2-1$ under $f(z) = \sqrt{z}$,"I have two regions in the complex plane, defined by
$|z-1|=1$ and $(\,\mathrm{Im}(z))^2 = (\,\mathrm{Re}(z))^2-1$, $\mathrm{Re}(z)>0$. I am being asked to find and sketch the image of those regions  under the mapping $f(z) = \sqrt{z}$ . My attempt We have $f(z) = \sqrt{r}e^{i\frac{\theta}{2}}$ and $|z-1|=1 \iff r = 2\cos(\theta)$ If $|z-1|=1 $, then:
 $$f(z) =  \sqrt{2|\cos(\theta)|}\cos\left(\frac{\theta}{2}\right) + 
i\sqrt{2|\cos(\theta)|}\sin\left(\frac{\theta}{2}\right)$$ It means that $f(z)$ lies on the trace of the curve $\left(\sqrt{2|\cos(\theta)|}\cos\left(\frac{\theta}{2}\right); 
\sqrt{2|\cos(\theta)|}\sin\left(\frac{\theta}{2}\right) \right)$ if $|z-1| = 1.$ Am I right so far? I couldn't recognize this plane curve. And I don't know how to proceed in the region  $(\,\mathrm{Im}(z))^2 = (\,\mathrm{Re}(z))^2-1$.
How can I do it?",['complex-analysis']
2237309,Calculate $\lim _{n\to \infty }a_n\int _0^1 x^{2n}\sin \frac{\pi x}{2}dx$,I have to calculate $$\lim _{n\to \infty }a_n\int _0^1 x^{2n}\sin \frac{\pi x}{2}dx$$ Where $$a_n = \sum _{k=1}^n\sin \frac{k\pi }{2n}$$ I have found that $$\lim _{n\to \infty} \frac{a_n}{n} = \frac{2}{\pi} $$ if that helps in any way.,['integration']
2237334,Local minimum and maximum of two variable implicit function,"What is local minimum and maximum of implicit function $F(x,y)=x^4+y^4−x^2 −y^2=0$? I calculated first and second derivative: $$\frac{\partial F(x, y(x))}{\partial x}=4x^3+4y^3y'-2x-2yy'$$
$$\frac{\partial F^2(x, y(x))}{\partial x^2}=12x^2+12y^2(y')^2+12y^3y''-2(y')^2-2yy''-2$$
$$y'=\frac{2x-4x^3}{4y^3-2y}$$
$$y''=\frac{(y')^2-6x^2-6y^2(y')^2+1}{6y^3-y}$$ Then I don't know how to continue. How to find stationary points? How to decide if stationary point is local minimum or maximum?","['multivariable-calculus', 'calculus']"
2237338,No need for chain rule with $f'(g(x))?$,"Let's say I take a simple example of a function, $f(x) = \sin(x)$. I want to calculate $f'(x^2)$. The answer is $\cos(x^2)$. However, if $f'(x) = \frac{d}{dx}f(x)$, then shouldn't $f'(x^2) = \frac{d}{dx}f(x^2)$ which is $\frac{d}{dx}\sin(x^2)$ which means $f'(x)$ is $2x\cos(x^2)$? I'm trying to understand where I went wrong in my understanding, as it seems like you don't actually need to apply chain rule.","['derivatives', 'calculus']"
2237428,How do I calculate $\int_{\gamma }f(z)dz$?,"Given is $z_0\in \mathbb{C}$ and $f(z)=(z-z_0)^n$ for $n\in \mathbb{Z}$. How do I calculate for $\gamma :\left [ 0,2\pi \right ] \mapsto \mathbb{C}$, $\gamma (t)=z_0+re^{it}$ with $r>0$ integral $\int_{\gamma }f(z)dz$? My idea:
I'm not sure if  $\gamma(t)=z$ because if it is like that than $z-z_0=re^{it}$.
$\int_{\gamma }f(z)dz=\int_{0}^{2\pi}(re^{it})^ndt$. But still this looks wierd.",['complex-analysis']
2237445,What does the third derivative imply here?,"Let $f:I \subseteq \mathbb{R} \rightarrow \mathbb{R}$ be a function that is (at least) three times differentiable on the interval $I$. Suppose that $f'''$ is continuous and that $f'(a)=f''(a)=0$ for an $a \in I$ but that $f'''(a) \neq 0$. Is it possible for $f$ to reach a local extremum in $a$? I reckon that it is not possible, seen as $f$ has an inflection point in $a$. But I'm not sure how to put this in a proof.","['derivatives', 'real-analysis']"
2237462,Complex Numbers and the Norm,"I am given that for a complex number $w=a+bi$, define $\overline{w}=a-bi$ and $N(w)=w \overline{w}$. I have provided my answers for parts a and b, but I am not sure they are correct. I need help with figuring out part c. (a) Compute $N(w)=w \overline{w}$ explicitly. Here is what I have gotten:$N(w)=w \overline{w}= (a+bi)(a-bi)= a^2+b^2$. (b) Show that $N(rs)=N(r)N(s)$ for any two complex numbers $r,s$. Here is my work: Let $r=a+bi$, $s=c+di
$ Then $$rs=(a+bi)(c+di)=ac+adi+cbi-bd=(ac-bd)+(ad+cb)i.
$$ Now, from a, have that $N(r)= a^2+b^2$ and $N(s)= c^2+d^2$. So, $$N(r)N(s)=(a^2+b^2)(c^2+d^2).
$$ Also, $N(rs)=N((ac-bd)+(ad+cb)i) $ and from part a then, $N(rs)=(ac-bd)^2+(ad+cb)^2$ When expanded, $$a^2c^2-abcd-abcd+b^2d^2+a^2d^2+abcd+abcd+b^2c^2$$ \begin{align*}
&=a^2c^2+a^2d^2+b^2d^2+b^2c^2\\
&=a^2(c^2+d^2)+b^2(c^2+d^2)\\
&=(a^2+b^2)(c^2+d^2).
\end{align*} Hence, $N(rs)=N(r)N(s)$. (c) Prove that $N(\overline{v})=N(v) $ and $N(v^n)=N(v)^n$ for any complex number. This is the part I am confused as to how to prove. I let $v=a+bi$ and $\overline{v}=a-bi$. Then I am not sure how to use what I have shown before for this part.","['complex-numbers', 'abstract-algebra', 'proof-verification', 'proof-writing', 'discrete-mathematics']"
2237467,Solving a large $n \times n$ Lights Out Board,"My motivation is solving arbitrary Lights Out puzzles with $n^2$ cells with a method given in MathWorld : basically imagine the game as a square grid graph and construct an $n^2 \times n^2$ ""adjacency matrix"" $A$ . Then for initial configuration $b$ , solving $Ax = b$ gives you button presses to solve the game. For a matrix of size $n^2 \times n^2$ , normally Gaussian elimination takes $O((n^2)^3) = O(n^6)$ time, but maybe having a sparse binary matrix (by binary I mean all operations are mod 2, over field $\mathbb Z / 2 \mathbb Z$ ) can make the operation faster. I know the matrix is very sparse because every cell only has at most 4 neighbors, so almost all the matrix entries are zero. Any references to algorithms or software implementations that can solve binary sparse matrices efficiently are appreciated.","['gaussian-elimination', 'matrices', 'reference-request', 'binary', 'puzzle']"
2237479,"Prove: If $(g \circ f)$ is bijective, is $f$ bijective?","I need to prove or disprove for a discrete mathematics assignment the following statement: $(g \circ f)$ is bijective $\rightarrow$ $f$ is bijective, $f: X \rightarrow Y$ $\hspace{.5cm} g:Y\rightarrow Z$ All of the domains and codomains here are supposed to be the real numbers. I'm having a hard time understanding how to prove things about functions, which we just got into. I assume that I need to break this into two cases, those cases being to prove or disprove: $(g \circ f)$ is injective $\rightarrow$ $f$ is injective $(g \circ f)$ is surjective $\rightarrow$ $f$ is surjective I've been playing around with drawing different circle diagrams for $X$ , $Y$ and $Z$ . For $f(x)$ , I drew diagrams where $|X|$ < $|Y|$ , where $|X|$ = $|Y|$ , and where $|X|$ > $|Y|$ , and for $g(f(x))$ , I did the same for $Y$ and $Z$ . It appears to me that when $(g \circ f)$ is surjective, $f$ is not necessarily surjective, because you could draw the functions out like this: This makes sense to me as a counterexample, but I'm not sure if that's right. Next, as for $f$ being injective when $(g \circ f)$ is injective, I found a proof (see problem 3.3.7 part a at the top of the first page) that explained that this is true, but I didn't really understand the explanation. Also, when I drew it out myself trying to find an example for which it would be false, I found this: And it seems to me to show that when $(g \circ f)$ is injective, $f$ is not necessarily injective, but that contradicts the proof that I found online. The only two possibilities are that the proof I found was wrong, or that I'm drawing these functions and sets incorrectly and breaking some rule(s) that I'm unaware of. Any insight into what I'm doing wrong with my drawings, my logic, or into how to go about correctly solving this problem would be hugely appreciated. Thank you all.","['elementary-set-theory', 'proof-writing', 'functions']"
2237525,Does the Banach-Alaoglu theorem imply finite dimensionalness?,"Given a normed separable vector space $X$, the Banach-Alaoglu theorem states that the closed unit ball in the dual space $X^*$ is compact in the weak-* topology. Since $X$ is separable, that topology is metrizable. Now, the Alaoglu theorem states that for any normed vector space, the closed unit ball is compact iff the space is finite dimensional. Applying this to $X^*$ allegedly yields that $X^*$ is finite dimensional (obviously false). Where is my mistake?","['functional-analysis', 'general-topology', 'measure-theory']"
2237527,Find the Second coefficient of the series $\frac{1}{(1+x)^\frac{1}{4}} = \sum_{n=0}^\infty c_nx^n$,"If $$\frac{1}{(1+x)^\frac{1}{4}} = \sum_{n=0}^\infty c_nx^n$$ then $c_2$ is.... I think I know how to attempt the problem but I'm not sure if I'm on the right track I started out with the known series $$\frac{1}{1-x}=\sum_{n=0}^\infty x^n$$ I then attempted to manipulate the left side so it would be the same as the equation above. Then depending on how the x term looks I could plug in a value for n to find the second coefficient of the series; however, I am having some trouble manipulating the problem due to the quarter power. So far I got up to here... $$\frac{1}{1+x}=\sum_{n=0}^\infty x^n(-1)^n$$ I know that changing the $x$ term on the left side will change the $x$ term on the right side, but I'm unclear if I can change the whole denominator on the left side and how that would effect the right side since the quarter power is over the whole denominator. It is an old final so I do know the answer is $\frac{5}{32}$ but I don't know how to get that answer.","['power-series', 'sequences-and-series', 'calculus']"
2237558,Is the Haar measure on Abelian groups regular?,"I have just started studying Fourier analysis on topological groups from Folland's A course in abstract harmonic analysis and Rudin's Fourier analysis on groups . It seems that Rudin defines the Haar measure as a regular measure on the Borel sets, while Folland defines it as (what he calls) a Radon measure. A regular measure is a (non-negative) measure $\mu$ that satisfies $$\mu(B) = \sup\{\mu(K)\colon K\subseteq B,K~\text{compact}\} = \inf\{\mu(G)\colon G\supseteq B,G~\text{open}\},\forall B~\;\text{Borel},$$ and a Radon measure is a measure $\mu$ which satisfies each of the following: $$\mu(K)<\infty,\forall K~\text{compact},$$ $$\mu(B) = \inf\{\mu(G)\colon G\supseteq B,G~\text{open}\},\forall B~\text{Borel},$$ $$\mu(G) = \sup\{\mu(K)\colon K\subseteq G,K~\text{compact}\},\forall G~\text{open}.$$ Obviously, the two are not the same. Furthermore, Wikipedia agrees with Folland, and even gives an example where the (left-invariant) Haar measure is not regular. All three sources seem to work on locally compact Hausdorff groups, the only major difference being that Rudin restricts to Abelian groups. I understand that the Abelian assumption makes the left-invariance and right-invariance distinction redundant, but does it also make regularity and ""Radon-ity"" equivalent?","['fourier-analysis', 'locally-compact-groups', 'measure-theory', 'haar-measure', 'algebraic-topology']"
2237585,How many rats are needed to find 3 poisoned bottles out of n bottles of wine?,"You organized a party with 1000 bottles of wine but you know that 1 bottle was poisoned before the party and you don’t want anybody to die.Luckily you are in the lab and you have 10 lab rats so you decide to use them to test which bottle is poisoned.The poison takes 1 hour to take effect also the party occurs in 1 hour. That was the original statement of the drunk rats problem.
I was wondering how many rats you would need to need to detect for certain which 3 bottles of wine that are poisoned out of n bottles?",['number-theory']
2237599,What is the meaning of $dz$ in Complex Integrals?,"I'm sorry if I sound too ignorant, I don't have a high level of knowledge in math. Interested in learning to integrate through other paths besides the real line in the complex plane, I searched how to do so in the internet. The most understandable ""proof"" of the ""Line Integration Formula"" was the following: $C$ is a contour defined by $P(t)=R(t)+iI(t)$, $a<t<b$ $∫f(z)dz$ (through $C$) $=∫f(z(t))z'(t)dt$ (from $a$ to $b$) since $z$ is in function of $t$, and $z'(t)=dz/dt$, so $dz$=$z'(t)dt$ The latter confuses me in the first step, when we state that the Integration of $C$ in $f$ $=∫f(z)dz$ (through $C$). With the $∫f(z)$, I can imagine the sum of many (infinite) inputs of $C$ in $f$. But what is the meaning of $dz$? In ""real"" integrals, $dx$ can be seen as $(b-a)/m$, where $m$ is the number of inputs we are adding, but I struggle when trying to see $dz$ in the same way since I can´t find the parallel of that $(b-a)$ in complex integrals. My first intuition was to think of $(b-a)$ as the length of the curve, but the ""Line Integration Formula"" seems to pay no attention on the length of the contour it is integrating. Is there a way to approximate a complex integral in a similar way to the ""real"" integrals? I believe such approximation, if it exists, might give me an insight on the meaning of $dz$. I think that my question is hard to explain, so if there is something that is confusing I would be happy to clarify. I would really appreciate any explanation or insight!","['complex-analysis', 'integration', 'complex-numbers', 'complex-integration']"
2237655,How to show Rademacher functions are independent.,"Rademacher functions are defined on [0,1] in the following way. Let $x= 0.d_1 d_2 d_3...$ , the binary expansion of x in [0,1], where the expansion is non terminating (so 1= 0.111111111.... and not 1). Then the kth radamacher functions is defined as $R_k(x)= -1$ if $d_k=0$ and $R_k(x)=1$ if $d_k=1$ . The measure used is borel measure on [0,1]. We say $f$ and $g$ are independent if $m(f^{-1}(A) \cap g^{-1}(B))= m(f^{-1}(A)) m(g^{-1}(B))$ for all borel sets A,B. I have tried to show any two Rademacher functions are independent but have not succeeded. I thought of splitting the cases where each function equals 1 and -1 and so on but it gets more and more complicated. Can anyone help me out? Thanks.","['borel-sets', 'lebesgue-integral', 'measure-theory', 'analysis']"
2237684,"MLE of uniform distribution on the interval $[\theta − \frac{1}{2}, \theta + \frac{1}{2}]$","$X_1, X_2, . . . , X_n$ are i.i.d. observations from a uniform distribution on
the interval $[\theta − \frac{1}{2}, \theta + \frac{1}{2}]$. Show that any $\theta$ between $X_{max} − \frac{1}{2}$ and $X_{min} + \frac{1}{2}$  maximizes the likelihood, and therefore, can be taken as the MLE. I am confused...because $f(x|\theta) = \frac{1}{(\theta+\frac{1}{2}) -(\theta -\frac{1}{2})} = 1$ for $\theta-\frac{1}{2} \le x \le\theta+\frac{1}{2}$ and 0 otherwise... Isn't the likelihood function $L(x|\theta) = 1$? The how can it be maximized between $X_{max} − \frac{1}{2}$ and $X_{min} + \frac{1}{2}$ ?","['maximum-likelihood', 'statistics']"
2237720,Calculating $f(0)$ and $f'(0)$ given that $f(2^{-n})>0$ and $f(3^{-n})<0$,Let $f\colon \mathbb{R}\to\mathbb{R}$ be a differentiable function so that $f(2^{-n})>0$ and $f(3^{-n})<0$ for every $n\geq{2}$. Calculate $f(0)$ and $f'(0)$. I have tried solving it using the Intermediate Value Theorem without any results. How can I solve it?,"['real-analysis', 'calculus', 'functions']"
2237745,Binomial coefficient sum involving floor function,"Can the following summation be evaluated exactly (closed form) $$\sum\limits_{j=-\left[ \frac{p}{n} \right]}^{\left[ \frac{p}{n} \right]}{{{\left( -1 \right)}^{j}}\left( \begin{matrix}
   2p  \\
   p-nj  \\
\end{matrix} \right)}$$ Here [x] is the floor function.  If so, how? I tried egorychev but got tangled up with the residues and the floor function.","['combinatorics', 'summation', 'binomial-coefficients', 'sequences-and-series']"
2237777,What are different ways to prove that $\sum_{n=1}^{\infty}\frac 1n$ is divergent? [duplicate],"This question already has answers here : Why does the series $\sum_{n=1}^\infty\frac1n$ not converge? (27 answers) Closed 7 years ago . I have been studying Cauchy criterion for sequences, and have come across a rather simple proof for the harmonic series, and why it diverges. More so, we have the following: $$\sum_{n=1}^{\infty}\frac 1n=\infty\Rightarrow divergent$$ Here is my simple proof: Consider the sequence $\left\{a_n\right\}_{n=1}^{\infty}$ such that $a_n=\frac 1n$, then $\forall \epsilon>0,\exists \ N \in\mathbb{R}$ for $m,n\in \mathbb{N}$, such that, $$m,n>N\Rightarrow|a_m-a_n|<\epsilon$$ Pick $m=2n$, then we have the following: $$|a_{2n}-a_n|=\sum_{k=n+1}^{2n}\frac {1}{k}\geq \sum_{k=n+1}^{2n} \frac {1}{2n}=\frac 12$$ So pick $\epsilon=\frac 12\Rightarrow  \left\{a_n\right\}_{n=1}^{\infty}$ is not Cauchy and thus divergent. My question is, are there any other slick and easy proofs for the above claim, and if so, what are they? This series at first surprised me, as it initially doesn't seem divergent. Thanks in advance!","['real-analysis', 'harmonic-numbers', 'sequences-and-series']"
2237781,Area of a surface using integration. Confusion with aspect of formal definition.,"My textbook explains that, when finding the area of a surface using integration, we approximate each surface element by $$\left| \Delta u \dfrac{\partial \overrightarrow{r}}{\partial u} \times \Delta v \dfrac{\partial \overrightarrow{r}}{\partial v}\right|.$$ $\partial \overrightarrow{r}$ is defined to be the tangent vector to $C_1: v = v_0$ and $C_2: u = u_0$ at $P_0$. I don't understand why we are multiplying $\dfrac{\partial \overrightarrow{r}}{\partial u}$ and $\dfrac{\partial \overrightarrow{r}}{\partial v}$ by $\Delta u$ and $\Delta v$. We know that $\partial \overrightarrow{r}$ is tangent to $C_1$ and $C_2$, and we know that $\Delta v$ and $\Delta u$ are scalars. Multiplying a vector (in this case, $\partial \overrightarrow{r}$) by a scalar scales that vector by a factor of the scalar/constant. But what purpose does this serve with $\Delta u$ and $\Delta v$? I don't understand how this is supposed to change the tangent vectors $\dfrac{\partial \overrightarrow{r}}{\partial u}$ and $\dfrac{\partial \overrightarrow{r}}{\partial v}$? How is this multiplication/scaling changing these tangent vectors from what they originally were (from $\dfrac{\partial \overrightarrow{r}}{\partial u}$ and $\dfrac{\partial \overrightarrow{r}}{\partial v}$)? Why is it necessary to define it in this way? I would greatly appreciate it if people could please take the time to explain this.","['surface-integrals', 'surfaces', 'integration', 'area']"
2237799,Cayley–Menger determinant.,"Consider $n$-simplex. I read that Cayley–Menger determinant is a squared-volume of simplex. But I don't know how to get it. Actually after some attempts I get this : $\displaystyle V_s^2 = \frac 1 {(m!)^2}G(v_1\ldots v_n)$, where $G(v_1\dots v_n)$ is Gramian . But how can we transform Gramian to Cayley–Menger matrix? Any ideas?","['matrices', 'simplex', 'determinant']"
2237816,What is the limiting distribution of this continuous time Markov chain?,"Consider the following inventory model. You are managing an inventory of bikes. Demand for bikes arrives
as a rate 1 Poisson process. Whenever a demand arrives, it is always a demand for exactly one bike.
At any given time t, you have two types of inventory - inventory on hand $H_t$, and inventory ordered but not
yet arrived $O_t$. When a demand arrives, if you have at least one bike on hand (i.e. $H_t \ge 1$), you meet the
demand with the inventory on hand, i.e. you give the bike to the customer and $H_t$ is reduced by 1. If you
have no inventory on hand and a demand comes, AND you have either 0 or 1 bikes in inventory ordered
but not yet arrived (i.e. $O_t \le 1$), you lose that customer’s demand, and place an order for a new bike. A
newly ordered bike takes an i.i.d. Expo(2) amount of time to arrive. If you have no inventory on hand
and a demand comes, and you have at least 2 bikes in inventory ordered but not yet arrived, you lose that
customer’s demand, and do not place any new orders. Suppose that a demand arrives at some time $t\gg 0$.
What is the probability that this customer receives a bike? I tried to model this problem by CTMC with states of double-index. That is, state $(i,j)$ for $i\ge 0$ and $0\le j\le 2,$ where $i$ indicates for number of $H_t$ and $j$ indicates for number of $O_t$. And let $A$ be generating matrix of this Markov chain and find $\eta A=0$ with all entries of $\eta$ are positive and $\sum_{(i,j)}\eta_{(i,j)}=1$. Then the answer should be $1-\sum_{j=0}^2\eta_{(0,j)}$. But the system of $\eta A=0$ is too complicated to find a solution. I wonder if there is some other way to look at it.","['stochastic-processes', 'probability-theory', 'markov-chains', 'probability', 'stochastic-calculus']"
2237826,On the multiplication with constant in topological vector spaces,"Exercise A(2) on p41 of Kelley's and Namioka's book ""Linear Topological Spaces"" asks to prove that if $A$ is a closed set of scalars not containing $0$ and $B$ is a closed subset of a linear topological space not containing $0$, then $AB$ is also closed. Q1: How to prove this? (It is enough to consider only compact $A$) Q2: Is it still true if $A$ is compact (but possibly containing $0$) and $B$ is bounded? Q3: Any reference that I could quote regarding these facts? Thank you.","['functional-analysis', 'reference-request', 'general-topology', 'topological-vector-spaces']"
2237845,Find the variance of sum of indicator variables,"Compute $\operatorname{Var} (X)$ where $X = \sum_{i=1}^n x_i$ I am aware of the formula $$\operatorname{Var} (X) = \sum_{i=1}^9 \operatorname{Var} (X_i) + \sum_{i \ne j} \operatorname{Cov } (X_i, X_j)$$ But I cannot seem to apply it here Edit: We know that $\text{Var } (X_i) = 0.234$ Thus $\sum_{i=1}^{9} \text{Var} (X_i) = 9 \times 0.234 = 2.106$ $\displaystyle \sum_{i \ne j} \text{Cov} (X_i, X_j)$ , we know that if $j > i + 1$ then $\text{Cov} (X_i, X_j) = 0$ , thus $\sum = 9 \times 0.046875 = 0.4219$ Thus $\text{Var} (X) = 9 \times 0.2344 + 0.421875 = 2.53$","['statistics', 'probability']"
2237872,What is the absolute minimum needed on a space to do differential calculus?,"A long time ago, I came across Lang's Differential Manifolds . Besides his definition of manifold, one thing that made me pretty nuts was this So after studing some real analysis and more linear algebra and topology, I came back to this page and began questioning this definition. He assumes all topological vector spaces in the text are going to be what he calls banachable (can be given a norm that induces the given topology and is complete in such norm). So I thought this definition was only a reestatement of the definition using the norm, but without the norm. But I really didn't understand this definition, this concept of tangency. After some more research I found Sadayuki Yamamuro's Differential Calculus in Topological Vector Spaces . Not only I can understand this concept (somehow) but I could translate this definition to topological vector spaces over any topological field. This concept of $M$-differentiation generalizes the Fréchet and (linear) Gâteaux derivatives and much more. So my question is: how far can this concepts of differentiability be generalized? Is Lang's definition with tangency to $0$ valid not only for banachable ones, but for arbitrary topological vector spaces? Is the $M$-differentiation the farthest we can go? Can something along this lines be done with modules?","['derivatives', 'functional-analysis', 'definition', 'topological-vector-spaces', 'frechet-derivative']"
2237893,Find an example of degree-100 extension of $\Bbb Q(\zeta_5)$ and $\Bbb Q(\sqrt[3]{2})$.,"I am trying to find an example of degree-100 extension of $\Bbb Q(\zeta_5)$ and an example of degree-100 extension of $\Bbb Q(\sqrt[3]{2})$. For the example of degree-100 extension of $\Bbb Q(\sqrt[3]{2}),$ I suspect that $\Bbb Q(\sqrt[3]{2},\sqrt[100]{-3})$ is an example. Since $3$ and $100$ are co-prime, that means I can prove $[\Bbb Q(\sqrt[3]{2},\sqrt[100]{-3}):\Bbb Q]$ is at least $300$ and at most $300.$  Therefore $[\Bbb Q(\sqrt[3]{2},\sqrt[100]{-3}):\Bbb Q]=300.$ Then by the tower law, problem solved. For the example of degree-100 extension of $\Bbb Q(\zeta_5)$, I really don't know that how to find (and prove) such an example. Note: I haven't learnt Galois Theory. So please don't use that.Thanks so much.","['irreducible-polynomials', 'abstract-algebra', 'extension-field', 'field-theory']"
