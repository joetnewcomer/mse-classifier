question_id,title,body,tags
4440912,"The ""average"" in average rate of change comes from sum of instantaneous rate of change divided by number of rates of change?","To show what I mean here is a graph of $y = x^2$ . The red line represents AROC from $a$ to $b$ . The blue lines represent the IROC at some points $x$ , where $a<x<b$ If I were to calculate the gradients of those blue lines, add them up and divide by the number of lines I used, would I get the average rate of change? If this is true; is this why the ""average"" exists in average rate of change?","['calculus', 'functions', 'algebra-precalculus']"
4440971,Relation between supremum of quadratic variation expectation with expectation of supremum of martingale,"Let $X$ be a local martingale. Then the quadratic variation $[X]$ is such that $X^2-[X]$ is a local martingale. I am tasked with showing that $$\sup_{t\geq0}\mathbb{E}([X]_t)<\infty\iff\mathbb{E}\left(\sup_{t\geq0}X_t^2\right)<\infty$$ and I'm not sure how to do this using the properties of local martingales. Any help would be greatly appreciated, thanks!","['stochastic-processes', 'local-martingales', 'martingales', 'quadratic-variation', 'probability-theory']"
4440982,Shifting functions to minimise the maximum variance accross a domain.,"Assume we have a family of $N$ continuous functions $f_i(x)\in C_\infty$ along domain $D$ . Define a set of constants $c_i \in  \mathbb{R}$ and define the functions $g_i(x)$ so each $g_i(x)$ . \begin{equation}
	g_i(x) = f_i(x) + c_i
\end{equation} The mean of the functions $g_i$ at $x$ is denoted $\mu_g(x)$ \begin{equation}
	\mu_g(x) = \frac{1}{N} \sum_{i=1}^{N} (g_i(x))
\end{equation} We define the variance of $g_i(x)$ using the following: \begin{equation}
	Var_g(x) = \frac{1}{N} \sum_{i=1}^{N} (g_i(x) - \mu_g(x))^2
\end{equation} Our goal is to find the set of constants $c_i$ to minimise the quantity. \begin{equation}
\max_{x \in D} \{ (Var_g(x)) \} 
\end{equation} What is a reasonable strategy for finding the set of $c_i$ 's?",['statistics']
4441123,"References for doing geometry using ""higher degree curves as fundamental objects.""","In a typical introduction to coordinate geometry, we fix a couple of intersecting lines in an affine plane and then parameterise points via coordinates. Treating ""points as fundamental objects"" means that all other geometric objects are given by equations whose solutions are points. On the other hand, we can treat ""lines as fundamental objects"": We can define points via pencil of lines (although two lines in the pencil uniquely determine the pencil). After fixing three non-concurrent lines (i.e. a special triangle), we can obtain (""abridged"") homogenous coordinates for all the lines in the plane and conics circumscribing the special triangle. This approach can be found in old books (say, A treatise on conic sections by Salmon). In this approach, certain theorems of geometry become simple. One can prove Pascal's theorem on conics, using this approach (it can be found in Felix Klein's ""Geometry from an advanced viewpoint""). It looks like one can similarly consider ""conics as fundamental objects"". For instance, a point can then be represented by the pencil of conics passing through that point. In other words, a point will be specified by 5 homogenous coordinates. Conics passing through a line will be given by 3 homogenous coordinates that represent the line and so on. My question is: What are some references, say in modern algebraic geometry, that treat ""higher degree curves as fundamental objects"" and parametrises (coordinatizes?) other geometric objects using higher degree curves?","['algebraic-geometry', 'reference-request']"
4441131,Question about conditional expectation fact,"This fact comes from *Concentration of Measure for the Analysis of Randomised Algorithms * by Dubhashi and Panconesi (page 76, equation 5.2). Let $X$ and $Y$ be two discrete random variables and two arbitrary functions $f$ and $g$ . Then $$E[E[f(X)g(X,Y) | X]] = E[f(X)E[g(X,Y)|X]]$$ This is how far I got in the proof: By law of iterated expectation $E[X] = E[E[X|Y]]$ we have that: $$E[E[f(X)g(X,Y) | X]] = E[f(X) \cdot g(X,Y)]$$ Now I would like to apply something the $E[XY] = E[X] \cdot  E[Y]$ which only holds for two independent variables $X$ and $Y$ . But I don't see $f(X)$ and $g(X,Y)$ as independent. Am I missing something here? If they were independent I'd continue: $E[f(X) \cdot g(X,Y)] = E[f(X)] \cdot E[g(X,Y)]$ Re-apply law of iterated expectation: $$E[f(X)] \cdot E[g(X,Y)] = E[f(X)] \cdot E[E[g(X,Y)|X]] = E[f(X) \cdot E[g(X,Y)|X]] $$ $\square$ Could someone assist me in my thinking here?","['expected-value', 'conditional-expectation', 'probability-theory', 'probability']"
4441169,"Compressible Navier-Stokes vs Incompressible, which is 'Easier' or usually has shorter computation times for finding numerical solutions?","This is a broad question which might not have a clear answer. I am aware that the incompressible NS equations can be used to approximate the compressible when the Mach number is low, and that this approximation is validated by results in functional analysis. However, I am currently wondering what the 'use' is for this approximation. I remember during my bachelor's degree in physics lessons that we were taught that this approximation was useful as the incompressible version was 'easier' for computing (by hand) exact solutions in specific cases like pipe flow. This just seemed intuitive to me at the time, as there were of course fewer terms. Does this hold true when computing numerical solutions though? Is the incompressible version easier in most or even any cases? I ask as I am aware that the incompressible version can have its own set of problems (for example one can't use the barotropic assumption that the pressure is just the density to the power of some constant in the incompressible case). So perhaps no broad comparisons between the two make sense?","['functional-analysis', 'partial-differential-equations', 'physics', 'numerical-methods', 'fluid-dynamics']"
4441205,dense subspace of $L^2$ that is disjoint to $L^p$,"I wonder if it is possible to have a dense subspace $U \subseteq L^2$ that is disjoint to $L^p$ for some $p\neq 2$ . I would expect that such $U$ exists, but I'm stuck finding an example.","['banach-spaces', 'lebesgue-integral', 'functional-analysis']"
4441309,Statistical estimator of expected value of the gradient of an unknown function,"Fix a probability space $(\Omega, \mathcal{A}, \Bbb P),$ a continuously differentiable function $f:\Bbb R^n \rightarrow \Bbb R,$ and a random vector $X: \Omega \rightarrow \Bbb R^n.$ Furthermore, we define the random variable $Y: \Omega \rightarrow \Bbb R$ as $Y:= f \circ X.$ Note that, if $\Bbb P_X$ denotes the distribution of $X$ on $(\Bbb R^n, \mathcal{B}),$ then $f$ can be seen as a random variable defined in the probability space $(\Bbb R^n, \mathcal{B}, \Bbb P_X).$ Similarly, the function $\nabla f: \Bbb R^n \rightarrow \Bbb R^n$ is random vector defined on that space. I am interested in the expectation of this random vector, i.e., $E_{X \sim \Bbb P_X} [\nabla f].$ Suppose now that an i.i.d random sample $X_1, \ldots, X_n$ of $\mathbb{P}_X$ is observed, together with the values $Y_i := f \circ X_i.$ Thus, in particular, the function $f$ remains unknown. Is there a way to construct a good (unbiased, consistent) estimator for $E_{X \sim \Bbb P_X} [\nabla f]$ ?","['statistical-inference', 'parameter-estimation', 'sampling', 'probability-theory', 'random-variables']"
4441310,Polar decomposition of a linear combination of unitary matrices,"Consider a complex-valued square matrix $M$ of the form $$M = \frac{1}{2}\left(U_1 + e^{-i\phi}U_2\right),$$ where $U_1$ and $U_2$ are unitary matrices and $\phi$ is a real number. Moreover, consider the polar decomposition of $M$ : $$M = U P,$$ where $U$ is a unitary matrix and where $P$ is a positive semi-definite Hermitian matrix. I want to know if the matrices $U$ and $P$ can be directly expressed in closed form as a function of $U_1$ , $U_2$ , and $\phi$ . Is that possible? I somehow doubt it, but I thought I'd ask. We can assume that $M$ is nonsingular for simplicity. EDIT: Shortly after posting this question, I discovered that there are at least expressions for the polar decomposition of 2 by 2 real matrices and of 2 by 2 complex matrices . Though I'm still wondering if something more can be said for the structure above in any dimension.","['unitary-matrices', 'svd', 'positive-semidefinite', 'matrices', 'linear-algebra']"
4441338,Hitting time for 2-dimensional Brownian motion,"Fix $a>0$ and let $(A_t^-)_{t\geq0}$ and $(A_t^+)_{t\geq 0}$ be two independent one-dimensional Brownian motions, starting from $-a$ and $a$ , respectively. Set $$T=\inf\{t\geq0:A_t^-=A_t^+\}.$$ I am tasked to show that $T<\infty$ almost surely and to find a density function $T$ . I really don't know how to do this - the hint given is to consider an orthogonal transformation of $(A_t^-,A_t^+)_{t\geq0}$ which would also be a Brownian motion, but I'm unsure how to use this. Any help anyone could give would be great, thanks!","['stochastic-processes', 'stopping-times', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4441353,Time evolution and matrix ODE,"Let $M$ be a linear operator on a finite-dimensional complex Hilbert space (thus, $M$ is just a matrix). Assume that $\text{Tr}M = 1$ , $M$ is self-adjoint and $M \ge 0$ (positive semi-definite). In quantum mechanics, one postulates that every such $M$ might depend on $t$ , in which case it satisfies the following ODE: $$\frac{dM(t)}{dt} = -i[H, M] \tag{1} \label{1}$$ where $H$ is a fixed self-adjoint operator (the Hamiltonian operator) and $[\cdot, \cdot]$ is the commutator $[A,B] := AB-BA$ . I have two questions about this equation. First Question: I am considering $M$ as a bounded linear operator on an underlying Hilbert space with norm operator. A natural way to define $dM(t)/dt$ is as follows. Given $\varepsilon > 0$ , there exists $\delta > 0$ and an operator $L = L(t)$ such that if $0 < |h| < \delta$ then $$\bigg{\|} \frac{M(t+h)-M(t)}{h}-L(t)\bigg{\|} = \bigg{\|} \frac{M(h)}{h}-L(t)\bigg{\|} < \varepsilon$$ in which case $L(t) := dM(t)/dt$ . On the other hand, we could interpret (\ref{1}) as a collection of ODEs given by the entries of $M$ , that is, if $M_{ij}$ is the $ij$ -th entry of $M$ then (\ref{1}) means: $$\frac{dM_{ij}(t)}{dt} = -i(H_{ij}M_{ij}-M_{ij}H_{ij}) \tag{2}\label{2}$$ Are these two interpretations equivalent? Second Question: If $M$ is fixed and we define $M(t) := e^{-iHt}Me^{iHt}$ , we see that $M(t)$ satisfies (\ref{1}) with initial condition $M(0) = M$ . Is it a unique solution? If we interpret (\ref{1}) as being (\ref{2}), then the answer seems to be a yes. However, I don't know any uniqueness results if (\ref{1}) is treated as an ODE for operators (or elements of a Banach space).","['ordinary-differential-equations', 'linear-algebra', 'functional-analysis', 'quantum-mechanics', 'mathematical-physics']"
4441365,Find $A^m B^n$ for noncommutative variables,"Let $$
\begin{cases}
AC=CA+\alpha A,\\
BC=CB+\beta B,\\
AB=BA+\gamma C.
\end{cases}
$$ It is no so hard to find that \begin{gather*}
A^n C^m=(C+\alpha n)^m A^n,\\
B^n C^m=(C+\beta n)^m B^n.
\end{gather*} Unfortunately I cant find a good formula for $A^m B^n.$ For $m=1$ I have got $$
A B^n=B^n A+n \gamma B^{n-1}(C-\frac{n-1}{2} \beta).
$$ Similar, but some complicated formulas I have for $m=2,3$ but I cant see what will  be for arbitrary $m$ and $n.$ Is there any way to present $A^m B^n$ in not ugly form?","['combinatorics', 'noncommutative-algebra']"
4441380,How to determine the distribution of the limiting random variable,"Given a sequence of random variables $\{X_n\}_{n \geq 0}$ defined as follows. $X_0 = p, X_{n+1} = qX_n + (1-q)1_{Y_{n+1} \leq X_n}, \forall n \geq 0, $ where $p, q \in (0, 1)$ are constants and $Y_i \sim U(0, 1)$ is a sequence of i.i.d random variables with standard uniform distribution for all $i \geq 0$ . Define $X_{\infty} = \lim_{n \to \infty}X_n$ . I am confused about how to determine the distribution of $X_{\infty}$ .","['probability-limit-theorems', 'probability-theory', 'probability', 'random-variables']"
4441402,Is there a sense in which a function converges to its total derivative?,"In short: Is there a way to rigorously define the total derivative (of $F$ at $x$ ) as a function $dF_x$ which is (1) linear and (2) a usual topological limit of some function $\mu:X\to Y$ , where $X$ and $Y$ are topological spaces? Motivation: Let $F:\mathbb R^n\to\mathbb R^m$ . The total derivative of $F$ at $x\in\mathbb R^n$ is defined as the linear map $dF_x:\mathbb R^n\to\mathbb R^m$ such that $$\lim_{v\to0}\frac{\|F(x+v)-F(x)-(dF_x)v\|}{\|v\|}=0,$$ if such a linear map exists. This definition makes sense, but it is a bit indirect. The idea is that $F$ looks more and more like $dF_x$ near $x$ , so it seems that the more natural definition should be something like $\lim_{v\to0} F(x+v)-F(x)=(dF_x)v$ . The following points are suggestive: Normally when $\lim_{x\to c}\|f(x)-L\|=0$ , this can be formulated as $\lim_{x\to c}f(x)=L$ . The division by $\|v\|$ in the above definition is highly reminiscent of the operator norm . I don't think there is any trouble extending the notion of operator norm to general continuous functions (although it may be infinite), via $\|F\|_\text{op}=\sup_{0\neq v\in\operatorname{dom}(F)}\frac{\|F(v)\|}{\|v\|}.$ Then, defining $\hat F_x(v)=F(x+v)-F(x)$ , we might be able to manipulate the first definition into something like $$\lim_{v\to0}\hat F_x(v)=dF_x,$$ where it is understood that the limit is with respect to the operator norm. Or rather, as we look at $F$ restricted to smaller neighborhoods of $x$ , the operator norm of $\hat F_x-dF_x$ restricted to this neighborhood approaches $0$ . $F$ would approach many functions in this sense (including itself, for example), but the derivative would be the only linear function it approaches in this sense. However, when I try making this rigorous, I run into some challenges. Topologically, the limit of a function $F$ near $x$ must be a value in the range of $F$ , not a function with the same domain and range of $F$ ; so this formulation will require speaking of the limit of some function besides $F$ , probably mapping into function spaces or spaces of germs of functions near $x$ . This is where my knowledge gets more limited and I hit a wall. Update: Here's my current progress. Let $C^0_x=\{f\in C^0(U,\mathbb R^m)\,|\,U\text{ is a neighborhood of }x\}$ . Define function addition/subtraction on $C^0_x$ as usual function addition/subtraction restricted to the intersection of both functions' domains. Topologize this space using the metric $d(f,g)=\sup_{t\in\operatorname{dom}(f-g)\setminus\{x\}}\frac{\|f(t)-g(t)\|}{\|t-x\|}$ . This metric can take infinite values, but this seems ok according to this . Finally, let $\mu_{x,F}:(0,\infty)\to C^0_x$ be defined by $\mu_{x,F}(\delta)=F\big|_{B_\delta(x)}$ . Then, perhaps, we can define $dF_x$ (if it exists) as the unique linear map defined on all of $\mathbb R^n$ such that $$\lim_{\delta\to0}\mu_{x,F}(\delta)=F(x)+dF_x.$$ Does this work? Is there a more elegant or standard way to do this?","['topological-vector-spaces', 'frechet-derivative', 'multivariable-calculus', 'functional-analysis', 'derivatives']"
4441463,Baire category and relative topology,"The present problem is realted to the following posting in MSE. Recall that for a given a topological space $(X,\tau)$ , a set $E\subset X$ is nowhere dense in $X$ if $\operatorname{Int}(\overline{E})=\emptyset$ . A subset of $X$ that can be written as a countable union of nowhere dense sets in $X$ is said to be a set of first category. The space $(X,\tau)$ is said to be of first category if $X$ , as a subset of itself, is set of first category. A set $A$ may be of first category in some space $(X,\tau)$ but not a subspace of first category. For example, $\mathbb{Z}\subset\mathbb{R}$ is a set of first category in $\mathbb{R}$ (the latter endowed with the usual Euclidean topology) but, $\mathbb{Z}$ as a subspace of $\mathbb{R}$ is not of first category, for the topology on $\mathbb{Z}$ inherited by that on $\mathbb{R}$ is the discrete topology in which every subset of $\mathbb{Z}$ is both open and closed. The question that I have is: Suppose $A$ is considered as a subspace of $X$ , that is $A$ is endowed with the topology $\tau_A$ described above. If $(A,\tau_A)$ is a space of first category, is $A$ a set of first category in $(X,\tau)$ ? At first this may appear as a problem of semantics, but I think it is more nuanced than that. I  think that in general, the answer to my question is no, but I have not found any counter-example yet. Edit: following the reasoning of @Ruy, it turns out my guess was wrong, and that in fact $A$ is a set of first category in $X$ . Thanks for the interest and the answer!",['general-topology']
4441470,For any set $A$ and definable class function $f$ there is a set $A'$ containing $A$ that is closed under $f$,"The problem is to prove in ZF that for any set $A$ and any definable class function $f$ , there is a set $A'$ containing $A$ with $im(f\restriction_{A'})\subset A'$ . The way it's stated (with definable class functions) suggests that the axiom schema of replacement is involved. Just taking $A'$ to be $im(f)$ doesn't work because it doesn't necessarily contain $A$ . Another option I was thinking about is to consider, for each $a$ , the collection of $a, f(a), ff(a), fff(a), \dots$ and take the union over all $a\in A$ . Then the union will contain $A$ , and the image of the union under $f$ will be a subset of the union because the image of the union is the union of the image. But how is replacement used in this proof? Maybe it should be used to prove that the collection of $a, f(a), ff(a),\dots $ is a set? The image of $f$ is a set, and this collection, being a subset of the image of $f$ (together with the element $a$ ), is itself a set. Is this right?","['elementary-set-theory', 'solution-verification', 'set-theory']"
4441518,The probability of throwing $n$ dice with each result being contained in a set,"I recently asked this question on MSE: The probability of throwing two dice with each result being contained in a set and although I am happy with the answer, I cannot figure out how this generalizes to $n$ or even 3 dice. To state the case for $n$ dice: If we have the sets $S_1, S_2, \dots, S_n$ of possible outcomes and we throw $n$ indistinguishable dice (with the same number of sides) simultaneously, what is the probability that each die is contained in different sets, that is, what is the probability that the first is contained in set $S_{i_1}$ , the second is contained in $S_{i_2}$ , the third in $S_{i_3}$ , $\dots$ and the $n$ -th in $S_{i_n}$ with $i_1 \neq i_2 \neq i_3 \neq \dots \neq i_n$ . An example for $n=2$ : When have 2 dice and two sets $A$ and $B$ being $\{ 1, 2, 4\}$ and $\{ 1, 2, 5\}$ , respectively.
We will end up with the possibilities of $\{ 1,1 \}, \{ 2,2 \}$ which account for a $\frac{1}{36}$ probability each and $\{ 1,2 \}$ , $\{ 1,4 \}$ , $\{ 1,5 \}$ , $\{ 2,4 \}$ , $\{ 2,5 \}$ , $\{ 4,5 \}$ accounting for a $\frac{2}{36}$ probability each, for a total chance of $\frac{14}{36}=\frac{7}{18}$ . As @user2661923 correctly pointed out in the other question, you could also determine this by calculating $\frac{(2\times 3^2) - 2^2}{36} = \frac{14}{36} = \frac{7}{18}$ . Formula for $n=2$ : In general the chance for $n=2$ dice with sets $A$ and $B$ is: $$\frac{2! \# A \# B - \#(A\cap B)^2}{6^2}$$ . Formula for $n=3$ : This is where I got stuck, I could not obtain the formula for three dice with the sets $A$ , $B$ and $C$ . However, I came this far: $$\frac{3! \#A \#B \#C - 3\#A\#(B\cap C)^2 - 3\#B\#(A\cap C)^2 - 3\#C\#(A\cap B)^2 \pm \dots}{6^3}$$ I do not know what should be at the dots. My question: What is the correct formula for $n=3$ (and for larger $n$ )? I also wrote some python code to check some cases and provide support , mostly so that double work is avoided; everthing can be controlled by changing the variable sets (add anoter list to add a die). from itertools import product
from sympy.utilities.iterables import multiset_permutations
from collections import Counter


# sets = [[1, 2, 3], [3, 4, 5], [4, 5, 6]]
sets = [[1, 2, 4], [1, 2, 5]]

n_dice = len(correct)
counter = Counter()

for roll in product(range(1, max(max(i) for i in sets)+1), repeat=n_dice):
    times = 0
    for perm in multiset_permutations(roll):
        if [perm[i] in sets[i] for i in range(n_dice)] == [True]*n_dice:
            times = 1
    if times > 0:
        counter[tuple(sorted(roll))] += times

count = 0
for k, v in sorted(counter.items()):
    count += v
    print(f'total: {count}, dice roll: {k}, multiplicity: {v}')","['dice', 'combinatorics', 'probability']"
4441519,How many five-digit positive integers are there that are divisible by three?,"How many five-digit positive integers are there that  are divisible by three? Given answer is $90000/3=30000$ I know that a number is divisible by $3$ if the sum of its digits are divisible by $3$ . I think that for any number $ABCD$ , it will be member of either $(0\mod(3))$ or $(1\mod(3))$ or $(2\mod(3))$ . I thought that when it belongs to $(0\mod(3))$ , we have $4$ choices for the last digits such as $0,3,6,9$ . When it belongs to $(1\mod(3))$ , we have $3$ choices for the last digits such as $2,5,8$ . When it belongs to $(2\mod(3))$ , we have $3$ choices for the last digits such as $1,4,7$ . Hence, for any $5$ digit numbers, $4/10$ of them will be divisible by $3$ . So, the answer is $90000*(4/10)=36000$ . Am I right? If not, can you explain the given answer clearly?","['elementary-number-theory', 'combinatorics', 'discrete-mathematics']"
4441640,Determining if a die is fair or not by rolling six times and observing only the sum,"Suppose you have two bags and each has 6 dice. In one bag all the die are fair. In the other each die has a bias - die “1” has a probability of returning 1 of $\frac{1}{6} +\epsilon$ and $\frac{1}{6} -\frac{\epsilon}{5}$ for any of the other numbers. Die 2 has a probability of returning 2 of $\frac{1}{6} +\epsilon$ and $\frac{1}{6} - \frac{\epsilon}{5}$ for any of the other numbers and so on. Someone chooses a bag at random and one of the die out of that bag at random. They roll the same die six times and give you the sum. What is the probability that the die came from the biased bag. You can denote the number of ways to get sum S from r rolls as $N^{S}_{r}$ . This is what I got so far: From Bayes,
P(bag|sum) = P(sum|bag)P(bag) / P(sum) $\propto$ P(sum|bag) For the fair bag P(sum|bag is fair) = $\frac{N^{S}_{r}}{6^{6}}$ For the unfair bag P(sum|bag is not fair) = $\frac{1}{6}$ P(sum|die biased to return 1) + $\frac{1}{6}$ P(sum|die biased to return 2) + ... + $\frac{1}{6}$ P(sum|die biased to return 6) Thanks!","['statistics', 'combinations', 'conditional-probability', 'combinatorics', 'probability']"
4441661,Find a mobius transfromation that maps $|z|=2$ into $|z+1|=1$,"Find a mobius transfromation that maps $|z|=2$ into $|z+1|=1$ . mapping $-2$ and $0$ to $0$ , $i$ respectively. I started by substituting in the general form $T(z)= \frac{az-b}{cz-d}$ . I get $b = 2a $ and $ d =-ib$ . Now, since $T(0) \neq \infty $ we set $d=1$ which gives $b=i$ and $a=\frac{1}{2}i$ . Next thing I know is that $| \frac{\frac{1}{2}iz+i}{-2c+1}-1|=1$ when $|z|=2$ I want to use it to find $c$ but I don't know how to proceed and I'm not sure if my previous steps are valid.","['complex-analysis', 'mobius-transformation']"
4441726,Does the pattern of identities for $(x+y+z)^{2n-1}$ continue past $2 n - 1 = 5$?,"We have the following identities. \begin{align}
(x+y+z)^3&=x^3+y^3+z^3+3(x+y)(y+z)(z+x)\\
(x+y+z)^5&=x^5+y^5+z^5+5(x+y)(y+z)(z+x)(x^2+y^2+z^2+xy+yz+zx)
\end{align} I'm wondering is there any pattern like above in the expansion of $(x+y+z)^{2n-1}$ where $n\in\mathbb{Z}^+$ ? The pattern I see so far is appearance of the coefficient $2n-1$ and $(x+y)(y+z)(z+x)$ . The above expansions can be found by factoring $(x+y+z)^{t}-x^{t}-y^{t}-z^{t}$ by using Cyclic Polynomials. For example for $t=5$ the factorization is provided on Brilliant . I tried factoring $(x+y+z)^{7}-x^{7}-y^{7}-z^{7}$ using the method I got: $$(x+y)(y+z)(z+x) [A(x^4+y^4+z^4)+B(x^3y+y^3z+z^3x)+C(x^2y^2+y^2z^2+z^2x^2)]$$ Here I'm not sure whether I should include $D(xy^3+yz^3+zx^3)$ too or not.","['algebra-precalculus', 'factoring', 'polynomials']"
4441757,Conjecture: $\lim_{x\to 0^+}f(2x)\log f(x)=0$.,"I encountered the following problem: Suppose $f$ is a strictly increasing $C^1$ function defined on $[0,a]$ for some $a>0$ , such that $f(0)=f'(0)=0$ . Is it true that $\lim_{x\to 0^+}f(2x)\log f(x)=0$ ? I tried to use the fact that $z^{\alpha}\log z\to 0$ as $z\to 0^+$ for any $\alpha>0$ , and write $f(2x)\log f(x) = \frac{f(2x)}{f(x)^{\alpha}}f(x)^{\alpha}\log f(x)$ . It would then be sufficient to show that there always exists an $\alpha>0$ such that $\frac{f(2x)}{f(x)^{\alpha}}$ is bounded at zero. However, it turns out this is not the case. Here's a counterexample: $f(x) = e^{-e^{1/x}}$ (augmented with $f(0)=0$ ). But even for this counterexample the initial conjecture works. Any help would be appreciated.",['real-analysis']
4441774,Solve $x+\sin(x) = \frac\pi2$ [duplicate],"This question already has answers here : What is the solution of $\cos(x)=x$? (13 answers) Closed 2 years ago . I was recently doing a high school geometry problem. I believe that the intermediate steps are less than relevant, but if needed, I can post them too. I ended up with the equation $$x+\sin(x)=\frac\pi2$$ I have no idea how to solve it. Any help would be appreciated.","['trigonometry', 'geometry']"
4441790,Importance Sampling: What is the role of the target distribution $p(x)$ in Monte Carlo integration?,"I am currently implementing a few different approaches to Monte Carlo Integration and there is a conceptual hurdle that I don’t fully understand yet. The notation varies from one source to another, but importance sampling is commonly conveyed in a form below (or something similar), where $p(x)$ is the target distribution and $q(x)$ is the importance/proposal distribution. $$ \int f(x) \frac{ p(x) }{ q(x) } q(x) dx$$ The role of $q(x)$ seems pretty clear - it defines the PDF that we draw $x$ from, and it should (ideally) be higher in regions where additional samples would be more useful. I’m still not completely clear on the role of $p(x)$ , however. The fact that it’s referred to as the target , and that we want the variance in $\frac{p(x)}{q(x)}$ to be low, makes me wonder if $p(x)$ should resemble the output distribution of $f(x)$ . What role does $p(x)$ serve in this equation? And are there reliable ways to select a good distribution? (I apologize if this has been answered already, I searched twice and wasn't able to find a specific answer to this)","['integration', 'monte-carlo', 'probability-distributions', 'numerical-methods', 'probability-theory']"
4441860,Need help understanding how these assumptions imply?,"I am trying to understand the assumption proof of Theorem 3 in the paper ""A Universal Law of Robustness via isoperimetry"" by Bubeck and Sellke. Theorem 3. Let $\mathcal{F}$ be a class of functions from $\mathbb{R}^{d} \rightarrow \mathbb{R}$ and let $\left(x_{i}, y_{i}\right)_{i=1}^{n}$ be i.i.d. input-output pairs in $\mathbb{R}^{d} \times[-1,1]$ . Fix $\epsilon, \delta \in(0,1)$ . Assume that: The function class can be written as $\mathcal{F}=\left\{f_{\boldsymbol{w}}, \boldsymbol{w} \in \mathcal{W}\right\}$ with $\mathcal{W} \subset \mathbb{R}^{p}$ , $\operatorname{diam}(\mathcal{W}) \leq W$ and for any $\boldsymbol{w}_{1}, \boldsymbol{w}_{2} \in \mathcal{W}$ , $$
\left\|f_{\boldsymbol{w}_{1}}-f_{\boldsymbol{w}_{2}}\right\|_{\infty} \leq J\left\|\boldsymbol{w}_{1}-\boldsymbol{w}_{2}\right\|
$$ The distribution $\mu$ of the covariates $x_{i}$ can be written as $\mu=\sum_{\ell=1}^{k} \alpha_{\ell} \mu_{\ell}$ , where each $\mu_{\ell}$ satisfies c-isoperimetry, $\alpha_{\ell} \geq 0, \sum_{\ell=1}^{k} \alpha_{\ell}=1$ , and $k$ is such that $9^{4} k \log (8 k / \delta) \leq n \epsilon^{2}$ . The expected conditional variance of the output is strictly positive, denoted $\sigma^{2}:=\mathbb{E}^{\mu}[\operatorname{Var}[y \mid x]]>0$ . Then, with probability at least $1-\delta$ with respect to the sampling of the data, one has simultaneously for all $f \in \mathcal{F}$ : $$
\frac{1}{n} \sum_{i=1}^{n}\left(f\left(x_{i}\right)-y_{i}\right)^{2} \leq \sigma^{2}-\epsilon \Rightarrow \operatorname{Lip}(f) \geq \frac{\epsilon}{2^{9} \sqrt{c}} \sqrt{\frac{n d}{p \log \left(60 W J \epsilon^{-1}\right)+\log (4 / \delta)}} .
$$ I want to clearly understand these 3 assumptions, typically they want to mean the max component of the absolute value of the subtraction is atleast J times $l2$ norm of the subtraction of those two $w$ vectors. Have not understood the intuition at all noise should be positive , but what does ""expected conditional variance of the output"" mean , I am unable to figure it out. Can anyone explain it naively in a lucid manner why these assumptions they have taken and what is the significance of it?","['statistics', 'proof-explanation', 'inequality', 'probability-theory', 'random-variables']"
4441904,Induced representations: question about proof in Folland's book,"Consider the following fragment from Folland's book ""A course in abstract harmonic analysis"": 6.1 The Inducing Construction Let $G$ be a locally compact group, $H$ a closed subgroup, $q : G \to G / H$ the canonical quotient map, and $\sigma$ a unitary representation of $H$ on $\mathcal{H}_\sigma$ . We denote the norm and inner product on $\mathcal{H}_\sigma$ by $\lVert u \rVert_\sigma$ and $\langle u,v \rangle_\sigma$ , and we denote by $C \left ( G, \mathcal{H}_\sigma \right )$ the space of continuous functiosn from $G$ to $\mathcal{H}_\sigma$ .
If $f \in C \left ( G, \mathcal{H}_\sigma \right )$ , we shall frequently wish to apply the operators $\sigma(\xi)$ to the values $f(x)$ , and to avoid clutter we shall usually write $\sigma(\xi)f(x)$ instead of the more precise $\sigma(\xi)[f(x)]$ . The main ingredient in the inducing construction is the following space of vector-valued functions: $$\mathcal{F}_0 = \left \{ f \in C \left ( G, \mathcal{H}_\sigma \right ) \ : \ \text{$q \left ( \text{supp } f \right )$ is compact and $f(x \xi) = \sigma(\xi^{-1})f(x)$ for $x \in G$, $\xi \in H$} \right \}.$$ Here is how to produce functiosn in $\mathcal{F}_0$ : 6.1 Proposition. If $\alpha : G \to \mathcal{H}_\sigma$ is continuous with compact support, then the function $$f_\alpha(x) = \int_H \sigma(\eta) \alpha(x \eta) \ d \eta$$ belongs to $\mathcal{F}_0$ and is left uniformly continuous on $G$ . Moreover, every element of $\mathcal{F}_0$ is of the form $f_\alpha$ for some $\alpha \in C_c \left ( G, \mathcal{H}_\sigma \right )$ . Proof: Clearly $\boxed{q(\text{supp } f_\alpha) \subset q(\text{supp } \alpha)}$ , and ... How can we conclude that $q(\operatorname{supp}(f_\alpha))$ is compact from the boxed inclusion? I know that $q(\operatorname{supp}(\alpha))$ is compact, so it suffices to show that the former set is closed. But I don't see why this is true. Any help/comments/insight is highly appreciated!","['measure-theory', 'harmonic-analysis', 'representation-theory', 'functional-analysis', 'general-topology']"
4441975,Is this mapping continuous or not?,"Let $(X,d)$ be a connected metric space(e.g. metrizable topological vector space, or $R^n$ (with $n\ge 2$ )) with metric $d$ , $A$ is a closed subset of $X$ with the property that for each $x∈X$ , there is a unique $y∈A$ with $d(x,y)=\min_{z∈A}d(x,z)$ . This defines a function $f:X→A$ by mapping all $x∈X$ to $y∈A$ as above. Is $f:X\to A$ continuous? Some Remarks: (1)Of course if $f$ is continuous, then $A$ is a retract of $X$ . A very general example is that every closed convex complete subset in any inner product space has this property by Hilbert projection theorem(and in this case $f$ is a projection(Lipschitz continuous with Lipschitz constant $1$ )).But I tried but failed to find a non-convex closed $A$ with this property in even $R^2$ , so in $R^2$ , $A$ with such property must be convex? (2)And from this post Show that $a\to f(a)$ from $A$ to $S$ is continuous. , we can see this is true for $X=\mathbb{R}$ . (3)But this $f$ may be not continuous for $X$ being only a metric space in general by a previous post Mapping to unique nearest point . (After thinking for a while, I think this counter-example is somewhat related to the un-connectedness of $X$ , so I am curious about what if $X$ is connected? ).","['continuity', 'functions']"
4442027,Question about Ito formula and BSDE,"When I was reading the paper from Peng, I saw an equation which I had no idea about how to get it. The details are shown below: For a BSDE : $$y_t = \xi + \int_{t}^{T}g_0(s)ds - \int_{t}^{T}z_sdB_s$$ Which has a fixed $\xi \in L^2(\mathscr{F}_T)$ and $g_0(\cdot)$ satisfying $E(\int_{0}^{T}|g_0(t)|dt)^2 < \infty$ .
There exists a unique pair of process $(y., z.)\in L_{\mathscr{F}}^2(0,T;R^{1+d})$ satisfies the BSDE shown before. Now, we consider the case where $\xi$ And $g_0(\cdot)$ are both bounded. Since we have $$y_t = E^{\mathscr{F}_t}[\xi + \int_t^Tg_0(s)ds]$$ Thus the process is also bounded. We can apply Itô’s formula to $|y_s|^2e^{\beta s}$ for $s \in [t,T]$ : $$|y_t|^2e^{\beta t} + \int_t^T[\beta|y_s|^2+|z_s|^2]e^{\beta s} ds
= |\xi|^2e^{\beta T} + \int_t^T2y_sg_0(s)e^{\beta s}ds - \int_t^T e^{\beta s}2y_sz_sdB_s$$ Here comes the problem. How did we get this equation? In other words, how did the Itô’s formula deal with $|y_s|^2e^{\beta s}$ ?","['stochastic-analysis', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
4442121,Can you row reduce the Wronskian with functions?,"UPDATED BELOW Recently, I came across a problem asking me to exploit the Wronskian to determine the independence of a few functions. The functions were $\sin(x), \cos(x), x\sin(x)$ , and $x\cos(x)$ . I immediately noticed it would be a $4\times 4$ matrix with (probably?) some cyclical functions. I absolutely refused to ""brute force"" the matrix -- but I still wanted to solve it by hand. I noticed in this case if I could use row reduction I could get some zero entries, expand the determinate across that row, or down that column and lessen the work. I don't immediately see a problem with this, if you do row reduction aren't you stating that the two systems are equivalent? I asked my professor if this is a valid approach and he stated he has never thought about it. He then further challenged me to use his solutions which were done ""long and painfully"" and compare what I would do if row reduction is valid. It seems we got the same answer -- but I'm not sure if that will always happen. TL;DR: Can you do row reduction after setting up the Wronskian, but before you take the determinate? Is this always valid? How can one prove either way? Furthermore, can you do column reduction? Problem Statement My work: Professor's Solution: $W=4$ Update: Guidance from a previous professor. Credit Glenn Lahodony, UTSA mathematics. ""This is a very interesting question! The short answer to your question is yes, you can perform row reduction on a matrix of functions.  However, we may have to be careful when performing row reduction on such a matrix depending on the problem of interest.  The matrix you are considering in your problem has entries involving sin(x), cos(x), xsin(x), and xcos(x).  The domain of these functions (and their derivatives) is all real numbers.  However, if we perform certain row reduction operations such as multiplying by 1/sin(x), we have restrictions on the domain (i.e. x is not an integer multiple of pi).  If we then find the Wronskian is nonzero, the set of functions is still linearly independent on any interval not containing integer multiples of pi.  However, if we need to prove linear independence on every interval, we would have to consider a separate case where x is an integer multiple of pi.  Depending on the row operations performed, we may also end up multiplying a row by zero (i.e. multiplying by sin(x) amounts to multiplying a row by zero if x is an integer multiple of pi) which could result in a zero Wronskian for certain values. We can perform row reduction operations to columns of a matrix as well, but the applications are very limited as the system of algebraic equations is not equivalent if column operations are performed.  The only case I can think that column operations would be needed is in studying properties of the transpose of a matrix.""","['linear-algebra', 'wronskian', 'ordinary-differential-equations']"
4442129,Investigating continuity for function at given point,"Given the function $$
f(x,y)=\begin{cases}\big|1+xy^2\big|^\dfrac{1}{x^2+y^2} & \quad\hfill (x,y)\neq(0,0)\\\\ 1 &\quad\hfill (x,y)=(0,0)
\end{cases}
$$ investigate whether the function is continuous at $(0,0)$ . Usually, I claim $p\in\mathbb{R}$ such that $y=px$ , $x\rightarrow 0$ , placing them in $\displaystyle \lim_{x\rightarrow0}f(x,y)$ and seeing how that works out. If $\displaystyle \lim_{x\rightarrow0}f(x,y)=f(0,0) $ then (according to how I was taught) the function is continuous at $(0,0)$ . In this specific exercise, I can't seem to solve it using methods I know, i.e the one explained above, or just choosing $y$ to be any variation of $x$ (e.g. $y=\sqrt x $ ). So now I'm at a standstill in my thoughts. Any hints/tips would be really helpful! Thanks!","['continuity', 'multivariable-calculus', 'absolute-value']"
4442195,What does a.s.P. mean in this context?,"I am reading a statistics book, ""Sufficient Dimension Reduction"" by Bing Li. In Chapter 2, it says: We say that $\mathcal{G}_1$ and $\mathcal{G}_2$ are conditional independent given $\mathcal{G}_3$ , ...,if for every $A \in \mathcal{G}_1$ and $B \in \mathcal{G}_2$ , we have $P(A \cap B | \mathcal{G}_3)=P(A|\mathcal{G}_3) P(B | \mathcal{G}_3), a.s.P.$ What does a.s.P. mean in this context? I didn't find clues in previous texts, so I guess this should be a pretty common symbol.","['measure-theory', 'statistics', 'independence', 'probability-theory', 'terminology']"
4442199,Integral of the deconvolution kernel density estimator,"let $Y_i = m(X_i) + \eta_i$ , $W_j = X_j + U_j$ , $E[\eta_i | X_i] = 0$ with $X_i \sim f_X$ , $U_i \sim f_U$ be an errors-in-variable problem and $K_{U}(x) = \dfrac{1}{2 \pi} \int \mathrm{e}^{-itx} \dfrac{\phi_K(t)}{\phi_{U}(t/h)} \, dt$ with $\phi_K$ characteristic function of a Kernel and $U$ the characteristic function of the error-variable $U$ .
I already solved the integral \begin{align}
E \left[ \dfrac{1}{h} K_{U} \left(\dfrac{w - W_i}{h} \right)\right] &= \int E \left[ \mathrm{e}^{-it(w - W)/h}\right] \dfrac{\phi_K(t)}{\phi_{U}(t/h)} \,dt \\
&=\dfrac{1}{2 \pi} \int E \left[\mathrm{e}^{-iz(w - W)} \right] \dfrac{\phi_K(hz)}{\phi_{U}(z)} \, dz \\
&= \dfrac{1}{2 \pi} \int \phi_X(z) \phi_U(z) \mathrm{e}^{-izw} \dfrac{\phi_K(hz)}{\phi_{U}(z)} \, dz \\
&= \dfrac{1}{h} \int K\left( \dfrac{u - x}{h} \right) f_X(u) \, du \\
&= \int K(v) f_X(x + hv) \, dv
\end{align} I substituted in the second line with $t = hz$ , $dt = h \, dz$ and used Plancherels theorem in the fourth line. Now I want to show that $$ E \left[ \dfrac{1}{h} K_{U} \left( \dfrac{x - W_i}{h} \right) \, (Y_i - m(x))  \right] = \int (m(x + hv) - m(x)) \, K(v) f_X(x + hv) \, dv,$$ which is nearly the same as above. Unfortunately I don't know how to handle the $Y_i$ .","['integration', 'statistics', 'characteristic-functions', 'regression', 'probability']"
4442206,Is the following function rational?,"The rational function is defined as the quotient of two polynomials. Can functions that are equal to rational functions also be called rational, because they have the same properties? One concrete example I could come up with is: $$
f(x) = \frac{x+0^{|x|}}{x^2-1} \\
g(x) = \frac{x^2}{x^3-x}
$$ To my understanding these two funtions are identical.
So, is $f$ rational? Edit: I assume that $0^0$ is undefined as well as $g(0)$ hence $x/x$ is undefined for $x=0$ . Also found the error and fixed it.","['functions', 'rational-functions']"
4442265,Find volume between $x^2+y^2+z^2 \le 2(y+z)$ and $ x^2+y^2 \le z^2$,"Recently I've been proposed the next problem at my calculus course. Find the volume of the following corps: $$\left\{(x,y,z) \in \mathbb{R}^3 : \; x^2+y^2+z^2 \le 2(y+z), \; x^2+y^2 \le z^2\right\}$$ On first place, I attempted to plot the functions via Mathematica.
This is a screenshot of the plot I've thought to divide the figure in 4 shards like this and this , so I can find the volume of each one separately. (sorry for only having the images, I don't know how to explain it) For the first part, I tried to find the intersection of the sphere ( $x^2+y^2+z^2≤2(y+z)$ ) and the cone ( $x^2+y^2 \le z^2$ ) and try to find a parabolic cylinder that goes trough that intersection and is normal to the XY plane, so I can have $y$ in function of $x$ so I'm able to do the integral, but I'm struggling here, so I hope you can help me.","['integration', 'multivariable-calculus', 'calculus', 'volume']"
4442270,Showing that the best approximating linear map for a Lipschitz function is also Lipschitz,"For $d,n \in \mathbb{N}$ with $1 \leq d<n$ , let $f: \mathbb{R}^d \to \mathbb{R}^n$ be a Lipschitz map with some constant $L \geq 1$ . Let $B=B(0,r)$ for some $r>0$ and define the quantity $$ \Omega(B) = \inf_{A} \left( \frac{1}{|B|} \int_{B} \left(  \frac{|f(y)-A(y)|}{r} \right)^2 dy \right)^{\frac{1}{2}}, $$ where the infimum is over all linear maps $A: \mathbb{R}^{d} \to \mathbb{R}^{n}$ and $|B|$ is just the $d$ -Lebesgue measure of $B$ . Suppose that $\Omega(B) < \epsilon$ where $\epsilon$ is as small as we wish. I am trying to show that the infimizing map $A$ for $\Omega(B)$ is also Lipschitz with constant, say, $2L$ . This intuitively seems to be true as $\Omega(B)$ being small means that $A$ must approximate an $L$ -Lipschitz function $f$ very well inside $B$ and so should not deviate much from $f$ . I suspect even without $\Omega(B) < \epsilon$ assumption, the best approximating map should still be $2L$ -Lipschitz. For example if we had defined $``L^{\infty}""$ version of this quantity by $$\Omega_{\infty}(B) = \inf_{A} \frac{|| f-A ||_{L^{\infty}(B)}}{r}$$ then $ \Omega_{\infty}(B) \leq \frac{|| f-f(0) ||_{L^{\infty}(B)}}{r}  \leq L$ so that for the map $A$ that realizes infimum in $\Omega_{\infty}(B)$ , $|f(x)-A(x)| \leq L r$ for any $x \in B$ and from here it is not difficult to show that $A$ is $2L$ -Lipschitz. I am having troubles with the map $A$ when defined for integral version $\Omega(B)$ though. Some help would be appreciated. References: These quantities originate from Dorronsoro's paper .","['harmonic-analysis', 'functional-analysis', 'lipschitz-functions', 'real-analysis']"
4442299,Arrange friends,"We have for friends ( A , B , C , and D ) and we have 10 single rooms ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ). The rooms are ordered in a row. A and B always want to be in rooms that are neighbors C and D always want to be in rooms that are neighbors A and C always DO NOT want to be in rooms that are neighbors How many ways there are to order the friends in the rooms? I started to list all possible combinations in my opinion and I manage to list 196, so I am not sure if 196 is the right answer However, mathematically I cannot get 196. I am trying to calculate all the combinations to order 4 people in 10 rooms by this formula: $n = 10$ , $k = 4$ $\binom{n}k$ = $\dfrac{n!}{k!(n - k)!}$ Therefore $\binom{10}4 = \dfrac{10!}{4!(10 - 4)!} = \dfrac{10 * 9 * 8 * 7 * 6!}{4! * 6!} = \dfrac{5040}{24} = 210$ However, these are all combinations without considering the requirements 1 from 3. I tried to calculate 1 to 3 and after that extract this number from 210 to get smaller numbers, but I do not know how to consider these orderings. Any help is really appreciated.","['combinations', 'combinatorics']"
4442323,Is there a simpler proof for this basic fact about concave functions?,"Consider a function $f \colon[0, 1] \rightarrow [0, 1]$ . Suppose that $f$ is continuous, strictly increasing, strictly concave on $[0, c]$ where $c \in (0, 1)$ , strictly convex on $[c, 1]$ , and has three fixed points: $x = 0$ , $x = \hat{x} \in (0, 1)$ and $x = 1$ . I want to prove that $f(x) > x$ for $x \in (0, \hat{x})$ (i.e. $f$ is always above the 45 degree line between zero and the interior fixed point). However, my proof seems a bit convoluted. Is there any easier way to see this? Here is my proof: First case: suppose that $c \geq \hat{x}$ . Then $f$ is concave over the full interval $[0, \hat{x}]$ . One may then confirm that $f(x) > x$ for all $[0, \hat{x}]$ as claimed (I will skip details). Second case: suppose that $c < \hat{x}$ . Then it remains true that $f(x) > x$ for all $x \in [0, c]$ . In particular, $f(c) > c$ . Furthermore, if there existed some $x' \in (c, \hat{x})$ such that $f(x') \leq x'$ , then either $f(x') = x'$ (impossible since $\hat{x}$ is the only interior fixed point) or else $f(x') < x'$ . But if $f(x') < x'$ , there would need to exist some $x'' \in (z, x')$ such that $f(x'') = x''$ (by the intermediate value theorem); which is again impossible since $\hat{x}$ is the only interior fixed point. Either way, then, we conclude that $f(x) > x$ for all $x \in (0, \hat{x})$ . As I say, it's a bit convoluted! Is there an easier way? The intuitive idea is that $f$ starts out above the 45 degree line. Also, it is a continuous function and has exactly one fixed point. So it must be above the fixed point to the left of it.",['functions']
4442326,How can I find the probability of getting a Yahtzee using probability generating functions for each roll?,"I was recently taught the concept of Probability Generating Functions (PGFs) and while revising said concept, I came across a question about the game Yahtzee. The question was as follows: Find the probability of achieving a Yahtzee made entirely of the number $k$ on five 6-sided dice after 3 rolls. You may assume the player achieving the Yahtzee removes all of the dice which rolled $k$ after each of the 3 rolls. Most of the answers I googled after attempting to solve this consisted of combinatorics and/or simply doing it manually or by using other ways I don't understand; however, while attempting to solve this using PGFs, I came across a curious problem in my method which I was unsure could be worked with, leading me to ask this question, this problem is explained below: Let $X$ be the number of 6-sided dice in a game of Yahtzee that land on the number $k$ in one roll. $X$ can be modelled binomially as $X \sim B(n,\frac{1}{6})$ for each throw where $n$ is the number of dice in each throw. The probability generating function for $X$ is $G_X(t) = (\frac{5}{6}+\frac{1}{6}t)^n$ Thus, for the first roll, the PGF is as follows: $G_{X_1}(t)=(\frac{5}{6}+\frac{1}{6}t)^5$ However, given the second roll is dependent on the amount of successes of the first roll, I would have to implement the first roll's outcome into the second roll's PGF: $G_{X_2}(t)=(\frac{5}{6}+\frac{1}{6}t)^{5-X_1}$ And the third roll would implement the same concept: $G_{X_3}(t)=(\frac{5}{6}+\frac{1}{6}t)^{5-X_1-X_2}$ This is not the answer I'm looking for since I would like $G_{X_3}(t)$ and $G_{X_2}(t)$ to be solely in terms of $t$ to then solve the question by finding the coefficient of $t^5$ . I would like to have the skill to manipulate embedded random variables in PGFs, thus my question is: If possible, how can I implement the PGF of a random variable into the PGF of another random variable? (A method which I can use to solve the above question for example)","['statistics', 'probability-distributions', 'probability']"
4442327,"Alternative to showing that $J(m,n)=\frac{1}{2}[(m+n)^2+3m+n], \ J:\mathbb{N}^2\to \mathbb{N}$ is bijective","I came up with the following proof, but it seems too complicated so I was wondering if anyone else has a simpler idea. Proof. The idea is to notice that $J(m,n)$ can also be expressed equivalently as $$ J(m,n)=\left(\sum_{i=1}^{m+n}i\right)+m \tag{1} $$ To prove injectivity, suppose $J(m_1,n_1)=J(m_2,n_2)$ . We'll prove $m_1=m_2$ (the fact that $n_1=n_2$ should follow immediately after). For this, suppose that $m_1+n_1<m_2+n_2$ , then using the (1) formulation of $J$ , we have $$\sum_{i=1}^{m_1+n_1}i+ \ m_1=\sum_{i=1}^{m_2+n_2}i+ \ m_2 \quad \Longrightarrow \quad m_1=\sum_{i=m_1+n_1+1}^{m_2+n_2}i+ \ m_2$$ which is a contradiction (to see why, subtract $m_1$ from both sides). With similar reasoning we can prove that $m_1+n_1>m_2+n_2$ is impossible as well. Thus we conclude that $m_1+n_1=m_2+n_2$ . Using the original formulation of $J$ we get: $$ [(m_1+n_1)^2+3m_1+n_1]=[(m_2+n_2)^2+3m_2+n_2]. $$ With the fact that $m_1+n_1=m_2+n_2$ , this reduces to $2m_1=2m_2$ , thus $m_1=m_2$ . This is injectivity proved. For surjectivity, let us choose some $z\in \mathbb{N}$ . We need to show that there exists $m,n\in \mathbb{N}$ such that $J(m,n)=z$ . Let $k$ be the largest element in $\mathbb{N}$ such that $\sum_{i=1}^ki\leq z$ . Then we can define $$ m=z-\sum_{i=1}^ki \quad \text{and} \quad n=k-m. $$ It is obvious that $m\in \mathbb{N}$ . To check that $n\in \mathbb{N}$ , notice that $$ \sum_{i=1}^ki\leq z<\sum_{i=1}^{k+1}i \quad \Longrightarrow \quad 0\leq m<k+1 \quad \Longrightarrow \quad m\leq k.$$ Defining $n,m$ this way gives us $$J(m,n)=\sum_{i=1}^{m+n}i+m=\sum_{i=1}^ki+m=z$$ as needed. $\Box$","['functions', 'multivalued-functions']"
4442421,Notation for element in set builder notation,If I want to describe some elements of a real space (say $\mathbb{R}^3$ ) is it proper to write: $$\{x \in \mathbb{R}^3 \mid P(x) \}$$ or better $$\{\mathbf{x} \in \mathbb{R}^3 \mid P(\mathbf{x}) \}$$ where with bold we denote an ordered tuple?,"['elementary-set-theory', 'notation']"
4442448,Show that $\mathbb{E}(X|\mathcal{F}_t)$ is a square-integrable martingale,"For a BSDE : $$y_t = \xi + \int_{t}^{T}g_0(s)ds - \int_{t}^{T}z_sdB_s$$ Which has a fixed $\xi \in L^2(\mathscr{F}_T)$ and $g_0(\cdot)$ satisfying $E(\int_{0}^{T}|g_0(t)|dt)^2 < \infty$ .
There exists a unique pair of process $(y., z.)\in L_{\mathscr{F}}^2(0,T;R^{1+d})$ satisfies the BSDE shown before. We define $$M_t = E^{\mathscr{F}_t}[\xi + \int _0 ^T g_0(s)ds]$$ The paper says that $M$ is a square integrable $(\mathscr{F}_t)$ -martingale. So my problem is that how to prove that? I think maybe we need to show that for every $t\ge0, E(M_t^2) < +\infty$ . But I have no idea about the detail, or maybe we need to find another way to prove $M$ is a square integrable $(\mathscr{F}_t)$ -martingale?","['stochastic-integrals', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
4442468,How to evaluate the integral $\int_{|z|=1}ze^{1/z^2}dz$ using the residue theorem?,"Question : Using the Cauchy Residue Theorem, evaluate the integral of $$\int_{|z|=1}ze^{1/z^2}dz$$ Naturally, we let $f(z)=ze^{1/z^2}$ and expand $e^x$ to get $$f(z)=z\big(1+\frac{1}{z^2}+\frac{1}{2z^4}+\frac{1}{6z^6}+\cdots\big)=z+\frac{1}{z}+\frac{1}{2z^3}+\frac{1}{6z^5}+\cdots$$ First note the order of the pole is $2$ , so the order of the derivative is $1$ . $$\operatorname{Res}{(f(z),0)}=\frac{1}{2i\pi}\lim_{z\to 0}\frac{d}{dz}(z\big(z+\frac{1}{z}+\frac{1}{2z^3}+\frac{1}{6z^5}+\cdots\big))$$ At this point, I'm confused on what terms I need to kill off, or to what extent do I keep the expansion? Would I just restrict this to become $$\lim_{z\to0}{2z+1+\frac{1}{2z}}=1$$ or must I consider higher order terms?","['complex-analysis', 'contour-integration', 'complex-numbers']"
4442487,Classification of essential singularity with Maximum,"I am trying to proof the following statements: Let $f \colon \{ z \in \mathbb{C} : 0 < \vert z \vert < r_0 \}$ be a holomorphic function. For $0<r<r_0$ we define $M_r(f):= \operatorname{max} \{ \vert f(z) \vert : \vert z \vert = r\}$ . The following statements hold true: i) $0$ is removable iff $M_r(f)$ is bounded for $r \rightarrow 0$ . ii) $0$ is a Pole iff $M_r(f) \rightarrow \infty$ and $\exists l \in \mathbb{N}: M_r(z^lf)$ is bounded for $r \rightarrow 0$ . iii) $0$ is an essential singularity iff $\forall l \in \mathbb{N}:  M_r(z^lf)\rightarrow \infty $ I have managed to prove i) and ii). Also, because of that the direction ""<="" is clear to me for iii), but I am struggling with the other one. I would be very thankful for help.",['complex-analysis']
4442509,Determining the sign of the directional derivative and the partial derivatives on a surface,"This is the question: The solution says: a) The surface is given by $z=f(x,y)$ If we see in graph as we move towards $\vec{u}=<5,0>$ , $z$ increases, thus $D_{\vec{u}}f(2,-4)$ is positive. I'm not sure what it means to 'move toward <5,0>'. Does that just mean move from the point (2,-4) towards the point $(5,0)$ in the xy plane? And how do we know that z increases? In a contour plot we can normally see numbers but in this case I don't see anything. Similarly, for c) the solution says: z increases rapidly, thus $D_{\vec{u}}f(2,-4)$ is positive. Again, how do I know it's increasing 'rapidly'? Also the solution for d) is: At $f(2,-4)$ z is neither minima nor maxima as we can see in the graph thus the answer is NEI. How do I know that it's not a minima or maxima? And what does that mean in terms if $f_{xx}$ ? I.e. how do I solve this versus finding the answer for $f_{yy}$ . The only thing the solution says for e) is: Same reason as d) Lastly, Are these the correct answers: a) $D_{\vec{u}}f(2,-4)$ is positive b) $D_{\vec{u}}f(2,-4)$ is positive c) $D_{\vec{u}}f(2,-4)$ is positive d) NEI e) NEI","['partial-derivative', 'multivariable-calculus']"
4442514,"Why is the image of $H_1(X,\mathbb{Z})\rightarrow (\Omega^1)^*$ a lattice in $(\Omega^1)^*$? (for Albanese varieties)","Albanese varieties are described here , and provide motivation for this question, although are not central to it. Let $X$ be an algebraic variety and $\Omega^1$ the space of everywhere regular differential $1$ -forms on $X$ . In the construction of Albanese varieties, one defines the map $\phi: H_1(X,\mathbb{Z})\rightarrow (\Omega^1)^*$ given by $\phi: \gamma\mapsto (\omega\mapsto \int_\gamma \omega)$ for each $\gamma\in H_1(X,\mathbb{Z})$ and $\omega\in \Omega^1$ . It is stated that $\phi(H_1(X,\mathbb{Z}))$ is a lattice in $(\Omega^1)^*$ . Why is this the case? Recall that being a lattice implies $\phi(H_1(X,\mathbb{Z}))\otimes_\mathbb{Z} \mathbb{R}\simeq (\Omega^1)^*$ . Note: some sources such as chapter 5 of Beauville note that this is due to Hodge theory, but I don't see how","['hodge-theory', 'algebraic-geometry', 'sheaf-theory']"
4442564,Double integral and change of variable [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question $$
\iint_D \left(x^2-y^2\right)\ dxdy
$$ over $D$ which is bounded by region enclosed by the four curves $y = x, y = x + 1, xy = 1$ and $xy = 2$ in the first quadrant. What will be a suitable change of variable for this question?","['integration', 'multivariable-calculus', 'change-of-variable', 'multiple-integral']"
4442565,Expected Value of Slot Machine,"I have a probability question that has to do with slot machines. Here is how the game works: There are two reels, one on the left and one on the right.  There are several symbols on each reel, one of which is an 8. The 8 symbol lands on the left reel 1 in 9 plays of the game (or spins). The 8 symbol lands on the right reel 1 in 10 plays. The reels spin independently of each other, and each play is independent from previous plays. Each play of the game spins both reels. Each reel has a large 8 above it, split into 8 segments. As the 8 symbol lands on the corresponding reel below it, one additional segment of the large 8 lights up. Once both large 8s are fully lit (i.e. at least 8-8s have landed on each reel) a 50 credit bonus is paid and the game ends. If one reel’s large 8 is already fully lit, the game isn’t over yet (i.e. do NOT pay the 10 credit bonus on a spin that completes the game), and a new 8 lands on its corresponding reel, a 10 credit bonus is paid. Each play of the game costs 1 credit. I need to compute the expected number of credits won or lost while playing this game. Well I have an idea on how to approach this. On the first reel, an eight appears 1 every 9 spins. On the second reel, it appears 1 every 10 spins. Therefore, the odds favoring an 8 is 1 in 9 and 1 in 10. What I think the expected value of this scenario is this: If $n$ is the number of reels, then $$EV = -1(\frac{n-1}{n})(\frac{n-1}{n}) + 9(\frac{1}{10})(\frac{n-1}{n}) + 9(\frac{1}{11})(\frac{n-1}{n}) + 49 (\frac{1}{10})(\frac{1}{11}). $$ Do you think I am approaching in the right direction? Thank you for your help!","['expected-value', 'statistics', 'monte-carlo']"
4442566,Relationship Between Bayesian Optimization and Gaussian Process,"In Bayesian Optimization , the function (i.e. objective function) that we are trying to optimize is modelled using some surrogate function - this surrogate function usually turns out to be a Gaussian Process. I am trying to better understand - in Bayesian Optimization, why exactly do we decide to model the objective function using a Gaussian Process? Could we have based the surrogate on some other class of functions instead? My guess is that perhaps Gaussian Processes have a certain type of ""mathematical flexibility"" which makes them an ideal choice for a surrogate models in the context of optimization. But in the end - is there some specific reason as to why Gaussian Processes are used in Bayesian Optimization (compared to some other class of surrogate function) ? Thanks!","['machine-learning', 'optimization', 'statistics', 'normal-distribution']"
4442572,Does interchanging the derivative and integral apply for $G’(x)=\frac{d}{dx} \Big{(}\int_{1}^{e^x}\frac{1}{1+log(t)}dt \Big{)}?$,"Question I am looking to differentiate the following integral with respect to $x$ (for $x>0$ ): $$G(x)=\int_{1}^{e^x}\frac{1}{1+log(t)}dt$$ Attempt I know that I can bring the derivative inside of the integral if the inside function and its partial derivative are continuous within the limits. My only hesitation in this case, is if this proposition still applies to the case where the limits are functions of variables. If so, does that make the derivative of $G(x)$ simply: $$G'(x)=\int_{1}^{e^x}0dt=0$$ Since the derivative of the inside with respect to $x$ is $0$ (as it is constant with respect to $x$ ). Or am I misunderstanding something? It doesn't feel like this can be the correct answer to the problem, so I would be grateful for any clarification.","['integration', 'definite-integrals', 'analysis', 'real-analysis', 'derivatives']"
4442577,Surjective Closed Map from Affine Plane to Affine Line,"I was wondering if there is a way to prove that there does not exist a surjective, closed map $f \colon \mathbb{A}^2 \to \mathbb{A}^1$ where closed is in terms of the Zariski topology. Intuitively, it seems that collapsing a variety in the plane to a line should result in a singularity resulting in some open set somewhere. For example, if we have the natural projection $\pi \colon \mathbb{A}^2 \to \mathbb{A}^1$ then it cannot be universally closed as $Z(xy-1)$ maps to $D(0)$ , which is what motivated my intuition.","['zariski-topology', 'algebraic-geometry']"
4442586,Does the limit superior of every subsequence equals to the same measurable function imply convergence of the original sequence?,"In an abstract measure space $(X, A, \mu)$ , we consider a sequence of measurable functions $(f_n)_n$ with $f_n : X \to \mathbb{R}$ such that there exists some measurable function $f : X \to \mathbb{R}$ , we have for every subsequence $\{n_k\}_k$ of $\{n\}$ , $$
     \limsup_{k \to \infty} f_{n_k} = f
     ~\mbox{a.e.}
$$ holds. Then we ask if the following claim holds true $$
      \lim_{n \to \infty} f_n = f
      ~\mbox{a.e.}
$$ Idea: If the claim is false, I think maybe this example can help. $(X,A,\mu) = ([0,1], \mathcal{B}[0,1],m)$ , $f = 1$ , $f_n = 1_{A_n}$ with $A_n \in \mathcal{B}[0,1]$ and $m(A_n) = \frac{1}{2}$ . The key is to construct $A_n$ . Since now the counterexample is found, I want to ask does the condition leads to convergence in measure induced by $f_n$ , i.e. $\mu_n(A) := \mu(f_n \in A)$ for $A \in \mathcal{B}(\mathbb{R})$ satisfies $\lim_n \int_{\mathbb{R}} g(x) \mu_n(dx) = \int_{\mathbb{R}} g(x) \mu(dx)$ for any bounded continuous $g$ on $\mathbb{R}$ ? if $\mu(X) = 1$ , does the condition leads to convergence in probability of $f_n$ , i.e. for any $\varepsilon$ , does $\lim_n \mu(|f_n - f| \geq \varepsilon) = 0$ ? The answer is no for two questions, if we take i.i.d. random variable with Bernoulli distribution.","['weak-convergence', 'convergence-divergence', 'probability', 'real-analysis']"
4442587,Find the smallest value of $\alpha\in \mathbb{R}$ such that for all $x>0$ you have $\left(1+\frac{1}{x}\right)^{x+\alpha}>e$,"Find the smallest value of $\alpha\in \mathbb{R}$ such that for all $x>0$ you have $$\left(1+\frac{1}{x}\right)^{x+\alpha}>e$$ By now i have tried the usual, taking logarithm and trying to solve for alpha, there i get the next condition: $$\alpha>\frac{1-\ln\left(1+\frac{1}{x}\right)^x}{\ln\left(1+\frac{1}{x}\right)}$$ so the problem now is to maximize this function, but i didn't get it through standar ways. I know that $\lim_{x\to \infty}f(x)=\frac12$ (where $f$ is the last function), but to say that $\frac12$ is the supremum i need to prove that it's increasing or bounded by $\frac{1}{2}$ and i didn't got any of those two things.","['optimization', 'calculus']"
4442620,What does it mean to show that an integral exists?,"Question I am slightly unclear on some terminology that I have come across in the following question: Let $f(x, t) = xe^{−xt}$ Show that the integral $I(x) = \int_0^{\infty} f(x, t)\ dt $ exists for all $x ≥ 0$ . What does it mean to show that an integral exists? Does this simply mean to show that it converges? I looked this up for some clarification but could only find questions rather than a precise description of what this means. I would be grateful for any explanation.","['integration', 'analysis', 'real-analysis', 'convergence-divergence', 'terminology']"
4442659,For which natural numbers 𝑛 ≥ 3 is it possible to cut a regular 𝑛-gon into smaller pieces with regular polygonal shape?,I have been working on this question and I found that any regular polygon with n sides works.My claim is that we can cut any regular polygon of n sides into smaller regular polygons with n sides.And we will have smaller polygons with n sides and rhombuses.But the thing is that I haven't found a way to prove this.Is there anything wrong with my claim or is their a way to prove my claim? Edit -This is for n=5 (not perfect),"['contest-math', 'combinatorics', 'discrete-mathematics', 'polygons']"
4442668,Does the Nullity Theorem hold in fields of characteristic 2?,"I'm playing around with involutory ( $M^2 = I$ ) matrices over finite fields with characteristic 2 ( $\mathbb{F}_{2^m}$ ).
I came across the nullity theorem , which seems very useful to check if submatrices are (non)singular. It basically states (as far as I understand it), that the nullity of a submatrix in $M$ is equal to the complementary submatrix of the inverse ( $M^{-1}$ ). Out of caution I routinely check my assumptions and try to find counterexamples.
I checked the 4x4 matrices for $m=6$ and found one which seems to contradict the theorem: (sagemath code) G=GF(2**6, repr='int')
a=G(58.bits())
b=G(59.bits())
c=G(62.bits())
d=G(63.bits())
M=matrix(4,4,[b,a,d,c,d,c,a,d,d,d,c,a,d,d,d,b])
print(M)
# [59 58 63 62]
# [63 62 58 63]
# [63 63 62 58]
# [63 63 63 59]

print(M*M)
# [1 0 0 0]
# [0 1 0 0]
# [0 0 1 0]
# [0 0 0 1]

import itertools
for d in [2,3]:
    for p1 in itertools.combinations(range(4), d):
        for p2 in itertools.combinations(range(4), d):
            S=M.matrix_from_rows_and_columns(p1, p2)
            if S.determinant() == 0:
                print('det', p1, p2)
            if S.nullity() != 0:
                print('nul', p1, p2)
# det (2, 3) (0, 1)
# nul (2, 3) (0, 1)

print(M.matrix_from_rows_and_columns((2,3), (0,1)))
# [63 63]
# [63 63] As you can see, the matrix $M$ is clearly involutory, so $M = M^{-1}$ , but it also has one obviously singular submatrix. In addition, it ONLY contains this one singular and nullity non-zero submatrix! According to the nullity theorem, I would expect a second singular submatrix, as there should be a complementary submatrix in the inverse of $M$ . I checked various sources, incl. the original papers, but found no obvious reason, the theorem should not work over special fields. So my questions are: Is there a problem/error in the nullity theorem? If not, what is the reason it does not work over $\mathbb{F}_{2^m}$ ? Is there something (e.g. additional constraints) to make it work over $\mathbb{F}_{2^m}$ ?","['matrices', 'involutions', 'determinant', 'linear-algebra']"
4442682,A tower stands vertically in the interior of a field which has the shape of an equilateral triangle of side $a$. If the angles of elevation...,"A tower stands vertically in the interior of a field which has the shape of an equilateral triangle of side $a$ . If the angles of elevation of the top of the tower are $\alpha$ , $\beta$ , $\gamma$ from the corners of the field find the height of the tower. Answer: $$a\sqrt{\frac{p^2+q^2+r^2-\sqrt{2p^2q^2+2p^2r^2+2q^2r^2 -p^4 -q^4 - r^4}}{2(p^4 + q^4 + r^4 -p^2q^2 -p^2r^2 - q^2r^2)}}$$ where $p= \cot \alpha$ , $q = \cot \beta$ and $r = \cot \gamma$ My Attempt $[.]$ denotes area $$[APB]+[BPC]+[CPA] = [ABC]$$ $$[ABC] = \frac{\sqrt{3}}{4}a^2$$ I tried finding the area of $\triangle APB$ , $\triangle BPC$ , $\triangle CPA$ by Heron's formula but the result was too complicated because I had to add square roots : something of the form $\sqrt{x} + \sqrt{y} + \sqrt{z} = w$ . Moreover, the $x,y,z$ in this expression are also complicated. I admit that the answer is complicated too but I am searching for an elegant approach towards the solution (something like substitution or anything else). Is there any elegant approach?","['trigonometry', 'geometry']"
4442715,"Problem on continuously differentiable function on (0, ∞)","Q. Let $f$ be continuously differentiable on $(0, \infty)$ and let $f(0)=1$ . Show that if $|f(x)| \leq e^{-x}$ for $x \geq 0$ , then there is $x_{0}>0$ such that $f^{\prime}\left(x_{0}\right)=-e^{-x_{0}}$ . I think Setting $g(x)=f(x)-e^{-x}, x \geq 0$ . Then $g(0)=0, g(x) \leq 0$ and $\lim _{x \rightarrow \infty} g(x)=0$ . If $g(x) \equiv 0$ , then $f^{\prime}(x)=-e^{-x}$ for $x \in(0, \infty)$ . So, suppose that there is $a>0$ such that $g(a)<0$ . Then for sufficiently large $x$ , say $x>M$ , we have $g(x)>\frac{1}{2} g(a)$ . Consequently, $g$ attains its minimum value at some $x_{0}$ in $(0, M )$ . Thus $g^{\prime}\left(x_{0}\right)=0$ . Is this okay? Also give me another approach.","['continuity', 'solution-verification', 'derivatives', 'real-analysis']"
4442738,Show example that if $\phi$ is not injective then the change of variables might yield different result,"I need to find non-injective parameterization to some domain such that : $$\int_D f(x,y) dxdy \ne \int_E (f\circ \phi)(u,v)J(u,v) dudv$$ The example I found turns out to be wrong. (The integral is the same when integrating using change of variables and direct computation). My try : The domain is $D=\{ 1\le xy \le 2 , 1 \le \frac {x}{y} \le 2\}$ and I want to calculate $\int_D 1dxdy$ The try is wrong, Thanks to @user1027216 comment. I searched for a counter-example for hours in the internet and couldn't find one. Any help would be appreciated.
Thank you in advance.","['multivariable-calculus', 'change-of-variable', 'multiple-integral']"
4442777,"Evaluating $\prod_{r=1}^{2016}(\alpha^{r}+\alpha^{-r})$, for $\alpha=e^{i\pi/2017}$","Given $\alpha=e^{i\pi/2017}$ . We need to find $$\prod_{r=1}^{2016}(\alpha^{r}+\alpha^{-r})$$ With De moivre's, the problem can be modified to $$\prod_{r=1}^{2016}2\cos\left(\frac{r\pi}{2017}\right)$$ at which I paired up the front and back terms to make it $$\prod_{r=1}^{1008}4\cos^2\left(\frac{r\pi}{2017}\right)$$ where I couldn't proceed further.
Please help with this. Also (as this problem was present with a few of the kind) is there any geometrical interpretation on the argand plane for this expression?","['trigonometry', 'complex-numbers']"
4442791,"What is the equation of the circle having a radius of square root of 85, through (5,9) and (1,-7)?","In solving this problem, I first solved the slope of the perpendicular bisector and I solved the midpoint of the two points. I don't know what to do next. I hope you can help me.",['geometry']
4442871,Permuted Hamming distance,"Suppose Alice wants to send a message to Bob, they agree on a $n$ letters alphabet $\Omega = \{a_1, \cdots, a_n\}$ and they both agree on a shared secret $\omega=\omega_1 \cdots \omega_m$ $\omega_i \in \Omega \,\forall i$ that must be sent before the message. Unfortunately, Bob forgets the encoding of $\Omega$ and is only able to decode a message over a permuted alphabet $\sigma(\Omega) = \{\sigma(a_1), \cdots, \sigma(a_n)\}$ where $\sigma$ is a permutation of $\Omega$ . If e.g. $\Omega$ are bits, it means that 0 and 1 could be flipped or not. Suppose Alice wants to prove her identity to Bob, so she sends their shared secret $\omega$ to Bob, so Bob will decode the message $m := \sigma(\omega) = \sigma(\omega_1) \cdots \sigma (\omega_n)$ . Bob will accept the message iff there exists a permutation $\tau$ s.t. $\tau (m) = \omega$ . Now suppose that the channel over which Bob receives messages is noisy, Bob wants to know how close a message is close to a secret, so he computes: $$
d_H^S (m,\omega) := \min_{\tau} \left(d_H (\tau(m), \omega)= \sum_i d_H(\tau(m_i), \omega_i)\right) 
$$ and accept the message iff $d_H^S (m,\omega)$ is smaller than some threshold $C$ . As an example if $\Omega=\{0,1\}$ , $d_H^S(00,11) = 0$ (bits are flipped) while $d_H^S(00,10) = 1$ (whether or not bits are flipped, distance is one). Now suppose Charlie wants to fake the identity of Alice, he then sends random messages to Bob. To compute the threshold $C$ , the natural question emerges: Question: What could be the average value of $d_H^S (a,b)$ between two random words $a$ and $b$ of $n$ letters?","['cryptography', 'discrete-mathematics', 'probability', 'information-theory']"
4442872,In the proof of Lemma 6.18 associated to the Hopf-Rinow theorem.,"I'm reading the John M.Lee, Introduction to Riemannian manifolds, second edition, p.167, Lemma 6.18 and I stuck at some statement : My question is, Question 1. Why can we write a unit-speed minimizing geodesic from $p$ to $q_i$ (whose existence is gauranteed by the lemma) as form of $\operatorname{exp}_p(tv_i)$ for some unit vector $v_i$ ? Question 2. How can we prove the "" $q_i=\operatorname{exp}_p(d_iv_i)$ ""? I feel that I somewhat didn't understand about the exponential map. Can anyone helps?","['proof-explanation', 'riemannian-geometry', 'differential-geometry']"
4442930,Derivative of a map $f:\mathbb R \to \ell^2 $ to a separable Hilbert space vs derivative of each component of the Hilbert basis.,"${\newcommand{\R}{\mathbb{R}}}$ Let $\ell^2 $ be the Hilbert space of square summable sequences of real numbers. Consider a map $f: \R \to \ell^2$ ,  that has components $f_n:\R\to \R$ , i.e. $f(t)=\{f_n(t)\}_{n\in \mathbb N}$ so that $\sum_n(f_n(t))^2<+\infty$ . Motivation If the derivative $\frac d {dt}f(t)$ exists, then it is given by $\{\partial_tf_n(t)\}_n\in \ell^2$ componentwise.
However the existence of the derivatives $\{\partial_tf_n(t)\}_n\in \ell^2$ does not imply the existence of $\frac d {dt}f(t)$ , because we need the remainder $$ R_n(t,h) = f_n(t+h) - f_n(t) - \partial_t f_n(t)h $$ to be $o(|h|)$ for $h\to 0$ uniformely in $n$ . I would like to know if adding a Sobolev-type assumption we obtain existence of the derivative, precisely: Question: Let $f$ as above, and suppose that the componentwise derivativee $\{\partial^{(k)}_t f_n\}_n \in \ell^2$ and the $\partial^{(k)}_t f_n$ are continuous for $k\leq 2$ . Moreover assume that $$\int_\R \sum_n n^2|\partial_t^{(k)}f_n(t)|^2 dt <\infty$$ Does this  imply that the derivative $\frac {d}{dt}f(t) \in \ell^2$ exists for all $t$ ? (and thus coincide with $\{\partial_t f_n\}_n$ ).","['measure-theory', 'lebesgue-measure', 'hilbert-spaces', 'sobolev-spaces', 'functional-analysis']"
4442948,Find $100$th number $k$ such that there is no $n$ for which $n$! ends in $k$ zeroes.,"$24! = 620,448,401,733,239,439,360,000$ ends in four zeroes, and $25! =
15,511,210,043,330,985,984,000,000$ ends in six zeroes. Thus, there is no integer $n$ such that $n!$ ends in exactly five zeroes. Let $S$ be the set of all $k$ such that for
no integer $n$ does $n!$ end in exactly $k$ zeroes. If the numbers in $S$ are listed in
increasing order, $5$ will be the first number. Find the $100$ th number in that list. I used the approach of finding the number of $5$ s in $(n+1)(n+2)...(2n)$ and $(n)(n-1)...1$ and their difference will be the number of zeroes in $\binom{2n}{n}$ . But I'm still not sure how to find a pattern for the number of zeroes not possible for any $n$ .","['binomial-coefficients', 'divisibility', 'discrete-mathematics']"
4443027,Find $b_{32}$ if $\prod_{n=1}^{32}(1-z^n)^{b_n}\equiv 1-2z \pmod{z^{33}}$ and $b_n\in\mathbb{Z}^{+}$,"A polynomial product of the form $$(1 - z)^{b_1} (1 - z^2)^{b_2} (1 - z^3)^{b_3} (1 - z^4)^{b_4} (1 - z^5)^{b_5} \dotsm (1 - z^{32})^{b_{32}},$$ where the $b_k$ are positive integers, has the surprising property that if we multiply it out and discard all terms involving $z$ to a power larger than 32, what is left is just $1 - 2z.$ Determine $b_{32}.$ Your answer can be in exponential notation. I tried letting this polynomial equal to $z^{32}q(z) + 1-2z$ , but I don't see how this can help. I also tried to solve simpler versions of the problem, such as finding $b_2$ if the product was $(1-z)^{b_1}(1-z^2)^{b_2}$ . I found that the product $(1-z)^2(1-z^2)^1$ works. I then tried to solve the problem when the product was $(1-z)^{b_1}(1-z^2)^{b_2}(1-z^3)^{b_3}$ . However, this got quite confusing, and I'm not sure what to do other than randomly test numbers. Now I'm not sure what I should do next. Thanks in advance!!!","['algebra-precalculus', 'polynomials']"
4443061,Finding all square-free n such that $n\mid \sigma(n)$,"Let $n$ be a positive squarefree integer, and $\sigma(n)$ be the sum of the all the factors of n, then find all the numbers $n$ satisfying: $$ n\mid \sigma(n)$$ First of all, it seems like the only solutions is $n=6$ ,I tried to use p-adic valuation and prove n can't have primes $p \ge 5$ , but it didn't work out pretty well, here is my work: Note that $n \le \sigma(n)$ , note that $\sigma(n)$ is always even, hence $n$ is even. Let: $$n=2^{2\alpha_1+1} \cdot p_2^{2\alpha_2+1} \cdot ... \cdot p_k^{2\alpha_k+1}$$ $$ \sigma(n)= (1+2^1+...+2^{2\alpha_1+1})(1+p_1^1+...+p_1^{2\alpha_2+1})...(1+p_k^1+...+p_ k^{2\alpha_k+1})=$$ $$(2^{2\alpha+2}-1)(\frac{p_1^{2\alpha_2+2}-1}{p_1-1})...(\frac{p_k^{2\alpha_k+2}-1}{p_k-1})=$$ $$(4^{\alpha+1}-1)(\frac{p_1^{2\alpha_2+2}-1}{p_1-1})...(\frac{p_k^{2\alpha_k+2}-1}{p_k-1})$$ Now we will prove that $n$ can't have primes $p_i \ge 5$ and $\alpha_1=0$ Assume n has primes $p_i \ge 5$ , then $p_i^{2i+1} | n$ which implies: $$p_i^{2i+1}| (4^{\alpha+1}-1)(\frac{p_1^{2\alpha_2+2}-1}{p_1-1})...(\frac{p_k^{2\alpha_k+2}-1}{p_k-1})$$ Note thet $v_{p_i} (p_i^{2i+1})=2i+1$ , then: $$ v_{p_i}((4^{\alpha+1}-1)(\frac{p_1^{2\alpha_2+2}-1}{p_1-1})...(\frac{p_k^{2\alpha_k+2}-1}{p_k-1})) \ge 2i+1$$ $$v_{p_i}((4^{\alpha+1}-1)+v_{p_i}(p_1^{2\alpha_2+2}-1)+...+v_{p_i}(p_k^{2\alpha_2+2}-1) -v_{p_i}(p_1-1)-...-v_{p_i}(p_k-1) \ge 2i+1$$ ...
At this point, I get stuck, and this approach didn't work, I don't have any other ideas other than looking to the number of the primes, could I get a hint please? ( I want to work this question by myself)","['contest-math', 'number-theory', 'discrete-mathematics', 'elementary-number-theory']"
4443062,Solutions of a Linear Differential Equation in a Banach Algebra,"Assume $A$ is a real Banach algebra (which need not necessarily be commutative or finite-dimensional) with unit and the function $f: \mathbb{R}\to A$ satisfies the differential equation $$\frac{df\left( t \right)}{dt}=f\left( t \right)\cdot s\left( t \right)$$ for all $t\in \mathbb{R}$ with a given continuous function $s: \mathbb{R}\to A$ . How can you prove that $f\left( t \right)$ is invertible for all $t\in \mathbb{R}$ if there is (at least) one $t_{0}\in \mathbb{R}$ for which $f\left( t_{0} \right)$ is invertible? In case the Banach algebra is just $\mathbb{R}$ everybody knows the answer: the only function $f:\mathbb{R}\to \mathbb{R}$ which then satisfies the differential equation and $f\left( t_{0} \right)=r_{0}$ with a given $r_{0}\in \mathbb{R}$ is defined by $$f\left( t \right)=r_{0}\cdot \exp\left( \int_{t_{0}}^{t}s\left( u \right)du \right)$$ for all $t\in \mathbb{R}$ and thus the assertion ensues immediately. Virtually the same reasoning is valid if the Banach algebra is commutative (no matter whether it is finite-dimensional), only the real exponential function has to be replaced by the exponential on the Banach algebra which is defined by the power series $$\exp\left( a \right)=\sum_{i=0}^{\infty}\frac{1}{i!}\cdot a^{i}$$ which is convergent for all $a\in A$ , and once again, the uniquely determined function $f:A\to A$ which satisfies the differential equation and $f\left( t_{0} \right)=r_{0}$ with a given $r_{0}\in A$ is defined by $f\left( t \right)=r_{0}\cdot \exp\left( \int_{t_{0}}^{t}s\left( u \right)du \right)$ for all $t\in \mathbb{R}$ , where it is well known how to define this integral over the continuous function $s$ with values in $A$ . So, if $f\left( t_{0} \right)=r_{0}$ is invertible then $f\left( t \right)$ is invertible for all $t\in \mathbb{R}$ as this is true for $\exp\left( \int_{t_{0}}^{t}s\left( u \right)du \right)$ . Unfortunately this does not work if the Banach algebra A is not commutative because then a solution of the differential equation cannot be given explicitly using the exponential function on A.","['banach-algebras', 'ordinary-differential-equations']"
4443063,"""summand"" is to ""sum"" $\left(\sum\right)$ as ___ is to ""product"" $\left(\prod\right)$","(Not a duplicate of If ""multiples"" is to ""product"", ""_____"" is to ""sum"" ) When working with sums or series, I often refer to $a_n$ in the summation $\sum\limits_n a_n$ as the ""summand"". Is there an analogous word for $b_n$ in the product $\prod\limits_n b_n$ ? Wikipedia's description of ""summation"" makes several mentions of $a_n$ as the ""summand"" or ""addend"", but no equivalent is provided in the article on ""product"" . My guess would be ""multiplicand"", but there appears to be a consistent definition ( 1 , 2 , 3 ) for it as referring to a specific number in a product. To paraphrase, In the product $a\times b$ , we call $b$ the ""multiplicand"" and $a$ the ""multiplier"". I've also found that in the sum $a+b$ , ""summand"" could refer to either $a$ or $b$ ; ""addend"" can be used interchangeably with ""summand"" in the difference $a-b$ , $a$ is the ""minuend"" and $b$ is the ""subtrahend"" in the quotient $\frac ab$ , $a$ is the ""dividend"" and $b$ is the ""divisor"" if $\star$ is an arbitrary binary operator, ""operand"" works like ""summand"" and can refer to either $a$ or $b$ in the expression $a\star b$ as summarized in this table . ""Summand"" and ""addend"" being treated as synonymous almost mirrors the commutativity of $+$ , but I suspect this is not the reason for their interchangeability. Optional : Is there a historical/etymological reason for making the distinction between the ""multiplicand"" $a$ and ""multiplier"" $b$ in $a\times b$ , as well as the other operations listed above? (The latter question may be more appropriate for the math history or English/Latin language SE's)","['algebra-precalculus', 'math-history', 'terminology']"
4443080,Can someone help verify the first part of my solution for this DDE?,"I'm trying to solve this delayed differential equation that I saw in a paper I'm studying $$y′(t)=ay(t)−ay(t−\tau)$$ with $a$ as a positive constant, $y(t−\tau)=0$ for $t<\tau$ and the initial condition as $y(0)=y_0$ . (I posted about this a couple of days ago and was making a terrible mistake so I deleted that one. I'm doing it again now, but I compare my solution to the one that someone else got and it's slightly different, even though I think my working is correct mostly? I can't see their working so I'm not sure where/if I'm going wrong, so I just wanted to confirm if I'm making a mistake or not.) I'll outline my working below: On $[0,\tau]$ we have $$y′(t)−ay(t)=0$$ which gives us $$y(t)=y_0e^{at}$$ if we use an integrating factor of $e^{−at}$ . For $[\tau,2\tau]$ : $$y′(t)−ay(t)=−ay_0e^{a(t-\tau)}$$ which gives me $$y(t) = y_0e^{at}[1-ae^{-a\tau}(t-\tau)]$$ Then, for $t \in [2\tau, 3\tau]$ , I can just continue using the method of steps, but I get a super long expression that does not,,, particularly seem like it would be very generalizable, so I'm not sure if I can get a closed form solution or not (also why I think I might be making a mistake). (Edit: Adding it here anyway) For $t \in [2\tau, 3\tau]$ : $$y(t) = y_0 e^{at}\bigg[1-at+2a\tau +
ae^{-a\tau}\bigg(a\bigg(\frac{t^2}{2} - 2\tau\bigg) + \tau - 2\tau^2\bigg) \bigg]$$ Could someone please confirm if I got the first bits that I've written up there correct though? I'd appreciate it a lot, as I do tend to make many many basic errors sometimes. EDIT 2: Got the answer by continuing to use the method of steps and changing how I wrote my solution for $t \in [2\tau, 3\tau]$ to $$y(t) = y_0\bigg[e^{at}-a(t-\tau)e^{a(t-\tau)} + \frac{a^2}{2}(t-2\tau)^2e^{a(t-2\tau)}\bigg]$$ instead of what it was before, which gave me a much nicer way to look at it. Then I generalized this to pretty much the exact thing user DinosaurEgg wrote below except I realized it after looking at the answers fully once I was done lol. But yeah, so I get the sum $$y(t) = y_0 \sum_{k=0}^{n} \frac{(-a)^k}{k!} (t-k\tau)^ke^{a(t-k\tau)}$$ which is again, the exact same as the answer below. Thanks for all the help though! I do appreciate it! :)","['delay-differential-equations', 'ordinary-differential-equations']"
4443081,Application of uniform bounded principle,"Let $(a_n)_{n∈N}$ be a sequence of real numbers. Show that the series $∑_{n=1}
^{\infty} |a_n−
a_{n+1}|$ converges if and only if the series $∑_{n=1}^{\infty} a_nb_n$ converges for every convergent
series $∑_{n=1}^{\infty}b_n$ in $\mathbb{R}$ . It is an application of uniform bounded principle. Can anyone give me some hint how to approach this problem.","['sequences-and-series', 'functional-analysis', 'real-analysis']"
4443152,Which objects can be Minkowski halved?,"The Minkowski Sum of two subsets $A,B \subset \mathbb{R}^n$ is $$A \oplus B = \{a + b | a \in A, b \in B\}$$ For a given $A$ , is there some condition that tells me when I can find a $B$ such that $A = B \oplus B$ ? For a specific example/context I was thinking about, let $K = $ filled in Koch Snowflake in $\mathbb{R}^2$ . Then, $K \oplus K = $ filled in hexagon. This is not too surprising given that there is theorem saying $\frac{1}{n} (A + \cdots n \text{ times} \cdots + A) \to \text{Convex Hull}(A)$ , so $K$ is like half a hexagon (and perhaps, it is the 'smallest' half a hexagon, if you drop the filled-in-ness of it, up to a measure 0 set?). Is there something that is half of $K$ ?","['euclidean-geometry', 'geometry', 'polygons', 'recreational-mathematics', 'fractals']"
4443224,How could I prove / disprove that every non-zero integer can be written in the form $p-x^2$ where $p$ is a prime and $x$ is a positive integer?,Question : Can every non-zero integer be written in the following form? $$p-x^2$$ I was thinking about if every non-zero integer could be written in the form $p-x^2$ where $p$ is a prime and $x$ is a positive integer so I ran a bit of code in python to see if there was a chance it may be true and it seemed promising. I haven't seen the problem asked anywhere else so i thought I'd ask for help in proving (or maybe disproving) the result.,"['number-theory', 'elementary-number-theory', 'integers', 'square-numbers', 'prime-numbers']"
4443225,Differentiable but not continuously-differentiable function: not the usual one,"It is well-known that the function $f:\mathbb R^2\to \mathbb R $ defined by $$f(x,y)=\begin{cases}(x^2+y^2)\sin\left(\frac1 {\sqrt{x^2+y^2}}\right),&(x,y)\neq 0\\0,&(x,y)=0\end{cases}$$ is differentiable everywhere but $\dfrac{\partial f}{\partial x}(x,y)$ and $\dfrac{\partial f}{\partial y}(x,y)$ are not continuous at $(0,0)$ , this is the standard example to prove that there exist differentiable but not continuously-differentiable functions (e.g., see https://math.stackexchange.com/q/3338764 ). My question: is there any other (reasonable) example (from $\mathbb R^2$ to $\mathbb R$ ) that differs significantly from this one? I mean: no radial simmetry and not obtained by continuous transformation from the above (and possibly avoiding the $\sin$ function) and such that the calculation can be performed by undergraduate students.","['multivariable-calculus', 'derivatives', 'real-analysis']"
4443272,How many combinations of groups are there where no member of a group has been with another member before?,"I found this hard to word in the title, so let me give an example. I have 16 students, and I want to split them up into 4 groups of 4. However, I want to make sure that every time I have a new combination of groups, that a student has never been with any of their new groupmates before. I found info on how to separate objects into groups, but not in only unique ways as I've described above. I think I figured this out the rote way, by assigning each student a number and then finding the unique combinations, and got 3. But I'd like to know the math behind it, and how it could be applied to other configurations (20 students w/ 4 groups of 5?). Thanks for any help you can give!","['permutations', 'combinatorial-designs', 'combinatorics']"
4443326,How to find the password space given several restrictions?,"I am trying to determine all valid passwords (the password space) that fulfill this list of requirements. Password is exactly 15 characters long. Must contain at least 2 lowercase letters (26 in total). Must contain at least 2 uppercase letters (26 in total). Must contain at least 2 numbers 0-9 (10 in total). Must contain at least 2 special characters (like !, @, # ...) (35 in total). I know that the unrestricted password space would be (26+26+10+35)^15 = 6.33*10^29 so the final answer must be less than that. My first approach was to find all of the invalid passwords and subtract that from the unrestricted password space. However, even this step proved to be too complicated to for me. I broke down all invalid passwords into a matrix of how they fulfill the 4 ""contain"" requirements. requirements fulfillment table I thought that by finding the number of passwords that fit in each row excluding the first and last row, I'll have all invalid passwords and thus I can subtract that from the unrestricted password space. The first row is all valid passwords which what we are trying to find. The last row is impossible because it is impossible to fill 15 characters without using any lowercase, uppercase, numbers, and/or special characters at least twice. Order does matter, so permutations are used somewhere but I'm not sure how. Any ideas on how to approach this is a simpler way? Thanks.","['permutations', 'combinations', 'combinatorics']"
4443385,Why does $\frac{z+2}{z-1}$ not have a series expansion around $|z-1|>1$?,"Determine the Laurent series of $$z \mapsto \frac{z+2}{z-1}$$ over the region $$C := \left\{ z \in \mathbb{C} \mid |z-1| > 1 \right\} $$ With the region being simplified to $$ \frac{1}{|z-1|} < 1$$ the solution would be $$ 1+\frac{3}{z-1} $$ However, I am confused as to how this is the conclusion. Why is there no expansion of further terms of the Laurent series? Because if you change the inequality you get a actual expansion like so: In the annulus: $$C:=\{z \in \mathbb{C} \mid 0<|z|<1 \} \\$$ $$|z| < 1$$ Gives: $$f(z)=\frac{z+2}{z-1}=1-3\sum_{n=0}^{\infty}z^n$$ So why does that give a series solution but the first way does not?","['complex-analysis', 'laurent-series']"
4443400,Maclaurin series of $1- \cos^{2/3} x$ has all coefficients positive,"Experimenting with WA I noticed that the function $1- \cos^{\frac{2}{3}}x$ has the Maclaurin expansion with all coefficients positive ( works for any exponent in $[0, \frac{2}{3}]$ ). A trivial conclusion from this is $|\cos x|\le 1$ , but it implies more than that, for instance see this .  Maybe   some ''natural'' proofs are available.  Thank you for your interest! Note: an attempt  used a differential equation satisfied by the function. But the answer by @metamorphy just solved it the right way. $\bf{Added:}$ Some comments about series with positive coefficients. By $P$ we denote a series with positive coefficients ( no free term), If $a>0$ then $\frac{1}{(1-P)^a} = 1+P$ (moreover, the positive expression on RHS is a polynomial in $a$ with positive coefficients If $0<a < 1$ then $(1-P)^a = 1-P$ . Similarly the expressions for $a = \frac{t}{t+1}$ are positive in $t$ . 2'. If $1-f= P$ then $1- f^{a} =P$ for any $0 < a < 1$ , and similar with above. $\bf{Added:}$ It turns out that the function $\cos^{2/3} x$ has a continued fraction (an $S$ -fraction, from Stieltjes) that is ""positive"" ( similar to the continued fraction for $\tan x$ ). This is a stronger statement than the one before. Maybe there is some approach  using hypergeometric functions.","['calculus', 'taylor-expansion']"
4443480,"Let $X,Y,Z$ be three random variables such that the correlation coefficients $\rho_{XY}=0.2, \rho_{YZ}=0.2$, what values can $\rho_{XZ}$ take?","Let $(X,Y,Z)^T$ be jointly normal variable with zero mean such that the correlation coefficients $\rho_{XY}=0.2, \rho_{YZ}=0.2$ , what values can $\rho_{XZ}$ take? Prove that there exists a decomposition $Y=aN_2,$ $X=bN_1+cN_2,$ $Z=dN_1+eN_2+fN_3,$ where $N_1,N_2,N_3$ are independent stand normal variables. I know that the covariance matrix $\Sigma$ is a semi positive definite matrix and hence admits a Cholesky decomposition $\Sigma = LL^T$ . But I am not sure how to proceed","['cholesky-decomposition', 'normal-distribution', 'correlation', 'linear-algebra', 'probability-theory']"
4443565,Reference for Sobolev-Hölder embedding for unbounded domains,"I would like to know whether the following is true and references: If $\Omega\subset\mathbb R^n$ is open (not necessarily bounded) and $k = n/p + r+\alpha$ , and $\alpha \in (0,1),  r\in \mathbb N, k\geq 0,p\geq 1$ , then there is a continuous embedding of the Sobolev space $$ W^{k,p}(\Omega)\to C^{r,\alpha}(\Omega)$$ into the space of Hölder functions. I have found in several places the case the case $k=1$ , and in other places, the case when $\Omega$ is bounded. In this generality it is stated in Wikipedia for $\Omega=\mathbb R^n$ . I am looking for a more reliable source.
It is possible that $\Omega$ open is not enough, in that case I would be interested to the case when $\Omega = \mathbb R^n$ or to know what are the most general hypothesis (e.g. $\Omega$ star domain).","['holder-spaces', 'sobolev-spaces', 'functional-analysis', 'real-analysis']"
4443566,What is the relationship between the vector fields of conjugate flows?,"Let $F(x,t)$ and $G(x,t)$ be two flows on $\mathbb{R}^d$ associated to the vector fields $f(x)$ and $g(x)$ , respectively. Assume $F$ and $G$ are conjugate in the sense that there exists a homeomorphism, $h$ , such that $$\tag{*} G(x,t) = h^{-1}(F(h(x),t))$$ for all $x,t$ . The relation $(*)$ is nice but it's often easier to work directly with vector fields than it is with their flows. So, my question is: Is there a simple relationship between $f$ and $g$ given (*)? Edit: We can also assume $h$ is a diffeomorphism if necessary.","['vector-fields', 'dynamical-systems', 'ordinary-differential-equations', 'differential-geometry']"
4443621,Can you always invert the sign of entries of a doubly stochastic matrix so that it becomes invertible?,"A square matrix is called a doubly stochastic matrix if all entries are non-negative reals, and each column and each row sums up to $1$ . Birkhoff–von Neumann theorem states that a matrix $A$ is a doubly stochastic matrix if and only if it is a convex combination of permutation matrices. In particular, we can always find a permutation $\sigma \in S_n$ such that $A_{i \sigma(i)} \neq 0$ for $i=1,\ldots,n$ . This follows easily from Hall's marriage theorem . I wondered if a stronger property is true: can we always change the sign of a subset of entries of $A$ such that the determinant becomes non-zero? In other words, is there a matrix $B$ such that $|B_{ij}| = A_{ij}$ for $i,j = 1,\ldots,n$ and $\mathrm{det}(B) \neq 0$ ? I don't really know what tools could be used to prove this.","['linear-algebra', 'combinatorics']"
4443630,When is the leaf space $M/\mathcal{F}$ of a foliation smooth?,"Let $(M,\mathcal{F})$ be a regular smooth foliation on a manifold $M$ . In general, the leaf space $M/\mathcal{F}$ is quite pathological. However, when $M$ is a Poisson manifold with compact, 1-connected leaves, the leaf space $M/\mathcal{F}$ is known to be a smooth manifold (cf. Poisson Manifolds of Compact Type 2 by Crainic, Fernandes and Martinez Torres). I am wondering what conditions on the leaves should be put to make this work. Is it the compactness, the 1-connectedness or the combination of the two?","['foliations', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
4443722,Mysterious distributed optimization problem,"Problem. Let $x = (x_1,\ldots,x_N) \in K^{N}$ , i.e., each element $x_i$ can take at most $K$ discrete values. Let $x_{(i)}$ , for $i \in \{1,\ldots,I\},$ possible overlapping subsets of $x$ . For example, for $K = 2$ , $N= 3$ , and $I = 2$ we may have $x_{(1)} = (x_1,x_2)$ and $x_{(2)} = (x_2,x_3)$ . Consider the following problem: $$
\min_{x\in K^{N}} \frac{\sum_{i\in [I]} f_i(x_{(i)})}{(\sum_{i\in [I]} g_i(x_{(i)}))^2},
$$ where each of the $f_i(x_{(i)})$ is positive, and $\sum_{i\in [I]} g_i(x_{(i)}) >0$ , however some of the $g_i(x_{(i)})$ can be negative. I could not find any resource that treated this integer problem in a distributed manner or even in a centralized manner. Question. Has anyone seen a similar problem or has any idea on how to solve this problem distributedly (similarly, for example, to the max-sum algorithm)? Attempts. I have tried to simplify or approximate the problem to put it in a standard distributed optimization form as $$
\min_{x\in K^{N}}\sum_{i \in [I]} h_i(x_{(i)}),
$$ which can be solved in many ways by distributed algorithms. For example, if the $g(x_{(i)})$ were to be strictly positive, we could have an upper bound on the value of this problem $$
\min_{x\in K^{N}} \frac{\sum_{i\in [I]} f_i(x_{(i)})}{(\sum_{i\in [I]} g_i(x_{(i)}))^2} \leq \min_{x\in K^{N}} \frac{\sum_{i\in [I]} f_i(x_{(i)})}{\sum_{i\in [I]} g_i(x_{(i)})^2} \leq \min_{x\in K^{N}} \sum_{i\in [I]}\frac{f_i(x_{(i)})}{ g_i(x_{(i)})^2},
$$ however, this is not the case unfortunately. Is there any other trick that I could use to at least approximate the problem? Resources. The closest related work I could find is this paper in which the objective is a function of sums, although it's only for continuous variables.","['convex-optimization', 'combinatorics', 'integer-programming', 'discrete-mathematics', 'optimization']"
4443817,How can I prove that $f(x)=\det (A+xB)= \alpha x+\beta $?,"I have two matrices $A$ and $B$ , such that : $$A=A(a,b,c)=\begin{pmatrix} a & c & c & \dots & c \\ b & a & c & \dots & c\\ b & b & a & \dots & c\\ \vdots &\vdots &\vdots & \ddots &\vdots\\ b & b & b &\dots& a \end{pmatrix} \hspace{1cm} \text{and} \hspace{1cm} B=A(1, 1, 1)$$ And we've : $$f(x)=\det(A+xB)$$ I have to prove the existence of two real numbers $\alpha$ and $\beta$ , such that : $$f(x)=\alpha x +\beta$$ Just prove their existence not their values, because later in the same exercise we have to calculate $f(-c)$ and $f(-b)$ , then deduce their values, then deduce $\det(A)$ . So That's why I think calculating the determinant won't be a good idea I guess. Any Ideas to do so ?","['matrices', 'determinant']"
4443820,How many perfect matchings does a complete k-partite graph have?,"Suppose I have a complete k-partite graph $G = K_{n_1, n_2, ..., n_k}$ such that $\sum_{i=1}^k n_i = n$ is even. How many perfect matchings does $G$ have? E.g. $K_{1,1,2}$ has 2 perfect matchings; $K_{1,1,4}$ has no perfect matchings; $K_{2,2,2}$ has 8 perfect matchings. I have attempted tackling this problem by the inclusion exclusion principle - considering the number of matchings in a complete $K_n$ graph and then subtracting the number of matchings with a pair in one part of the partition multiplied by $K_{n-2}$ , adding the number of matchings with a pair in two parts of the partition multiplied by $K_{n-4}$ , etc. etc. However, I struggled to count the number of matchings of $K_{n}$ which contain a pair in one part of the partition. E.g. if we consider $K_{2,2,4}$ , then there are $7*5*3$ matchings for $K_8$ . Then there are $5*3$ matchings that pair two values from the first partition, $5*3$ that pair from the second partition. To count how many pair two values from the third, we first choose a pair, $4C2 = 6$ , and multiply by $5*3$ . However, we then count pairs that include two pairs in the third partition twice, so we need to subtract $4C2/2 * 5 * 3$ . We then end up with $5*5*3$ matchings with a repeat in at least one partition. We then need to add matchings with pairs in two partitions, and subtract matchings with pairs in 3 partitions, etc. This seems like a really really long sum, which is fine for small $n$ , but not so much as it increases. I am hoping there is a more elegant solution to this problem?","['graph-theory', 'matching-theory', 'combinatorics']"
4443854,How to solve this limit with factorial? $\lim_{n\to \infty}\frac{n!}{n^n}(\sum_{k=0}^n\frac{n^k}{k!}-\sum_{k=n+1}^\infty \frac{n^k}{k!})$,"I want to solve the limit $$\lim_{n\to \infty}\frac{n!}{n^n}\left(\sum_{k=0}^n\frac{n^k}{k!}-\sum_{k=n+1}^\infty \frac{n^k}{k!}\right)$$ This problem may be about Stirling's Approximation and Taylor series of exponent functions. As $n$ is big enough, according to Stirling's approximation, $$ n!\approx\sqrt{2\pi n}\frac{n^n}{e^n} $$ and according to Taylor series, $$ \sum_{k=0}^\infty\frac{n^k}{k!}=e^n $$ The difference is the minus instead of the plus of the first $n$ term and the remainder. I have tried to use the Taylor expansion with an integral remainder, $$ e^n=\sum_{k=0}^n\frac{n^k}{k!}+\int_0^n\frac{(n-t)^n}{n!}e^t\mathrm{d}t $$ so we have the remainder as $$ \sum_{k=n+1}^n\frac{n^k}{k!}=\int_0^n\frac{(n-t)^n}{n!}e^t\mathrm{d}t $$ and the first $n$ term as $$ \sum_{k=0}^n\frac{n^k}{k!}=e^n-\int_0^n\frac{(n-t)^n}{n!}e^t\mathrm{d}t $$ Therefore, we have \begin{aligned}
\frac{n!}{n^n}(\sum_{k=0}^n\frac{n^k}{k!}-\sum_{k=n+1}^\infty \frac{n^k}{k!})
&=\frac{n!}{n^n}\left(e^n-2\int_0^n\frac{(n-t)^n}{n!}e^t\mathrm{d}t\right)\\
&=\frac{n!e^n}{n^n}\left(1-2\int_0^n\frac{(n-t)^n}{n!}e^{-(n-t)} \mathrm{d}t\right)\\
&=\frac{n!e^n}{n^n}\left(1-\frac{2}{n!}\int_{0}^n s^ne^{-s}\mathrm{d}s\right)\\
&=\frac{n!e^n}{n^n}\left(1-\frac{2}{n!}\gamma(n+1,n)\right)\\
&\approx \sqrt{2\pi n}\left(1-\frac{2}{n!}\gamma(n+1,n)\right)\\
\end{aligned} where the lower incomplete gamma function $$ \gamma(n+1,n)=\int_{0}^n t^n e^{-t}\mathrm{d}t  $$ But how to continue?
And I also plot the scattering points of the series , we can see the limit is near 1.333 maybe. How to continue the calculation and what is the limit?","['gamma-function', 'limits', 'factorial', 'taylor-expansion']"
4443877,Is it possible to have n matrices that generate a linearly independent vectors for the same input?,"The question I'm trying to solve is finding when there are $n$ $n \times n$ matrices $A_1, \dots, A_n$ such that for all $v \neq 0$ , $A_1v, \dots, A_nv$ are linearly independent. I think it's possible for $n = 1$ and all even $n$ , but I'm not sure how to find an explicit construction. So far, I've only shown it's impossible for odd $n > 3$ : WLOG we can assume $A_1 = I$ and then some other matrix has an eigenvector because $n$ is odd, which means plugging that eigenvector into all the matrices will not generate linearly independent vector. I think this also proves that no matrix can have an eigenvector, but I'm not 100% sure. Context: My friend asked me this question a couple of days ago. They're only in a linear algebra class, but I or any of my friends haven't been able to solve this with what we know from analysis/algebra so I thought to ask here. Thank you in advance!","['matrices', 'linear-algebra']"
4443966,Limit of an integral as $x\to 0$,"Given an integral function $\displaystyle F(x) = \int_{0}^{x}f(t)dt$ with $f(0)=3$ and $f$ continuous, determine the limit: $$\lim_{x\to 0}\frac{\displaystyle \int_{0}^{x}f(t)dt}{x}.$$ I know I could apply L'Hôpital's rule or the mean value theorem, but can I state simply that
since $F'(x)=f(x)$ and then $F'(0)=3$ , so $$\displaystyle F(x) \;=\; \int_{0}^{x}f(t)dt \;=\; 0+3x+o(x),$$ as $x\to 0$ , and so $$\lim_{x\to 0}\frac{\displaystyle \int_{0}^{x}f(t)dt}{x^3} \;=\; \lim_{x\to 0}\frac{2x}{x} \;=\; 2\;?$$","['integration', 'real-analysis', 'calculus', 'functions', 'limits']"
4443990,Hypervolume of hypersurface over unit hypersphere,"Given $f:\mathbb{R}^n \to \mathbb{R}^1$ such that $$f(x_1, x_2, x_3,...,x_n)=\frac{1}{(x_1^2+x_2^2+x_3^2+...+x_n^2)^k},$$ what values of $k$ would make the hypervolume of $f(x_1,x_2,x_3,...x_n)$ within a unit $n$ -sphere finite? What values would make it infinite? My attempt thus far has been to integrate it over all $n$ variables, redefining the bounds in hyperspherical coordinates in order to simplify the demonimator into $\text{radius}^{2k}$ .","['multivariable-calculus', 'general-topology', 'linear-algebra']"
4444009,"If a polyhedron's faces and vertex figures are convex, is the polyhedron convex?","Suppose a polyhedron's faces are convex polygons, and its vertex figures are convex spherical polygons (or convex cones, depending on definitions). Must the polyhedron be convex? As a counter-example for the analogous question in 2D, the pentagram $\{5/2\}$ has convex ""faces"" (line segments) and convex vertex figures ( $36^\circ$ arcs; less than $180^\circ$ ) but is a non-convex, self-intersecting polygon. The polyhedron may be in 3D Euclidean, spheric, or hyperbolic space. -- Actually it doesn't matter, because these all have standard embeddings in 4D: $$\mathbb E^3=\{(x,y,z,w)\in\mathbb R^4\mid w=1\}$$ $$\mathbb S^3=\{(x,y,z,w)\in\mathbb R^4\mid x^2+y^2+z^2+w^2=1\}$$ $$\mathbb H^3=\{(x,y,z,w)\in\mathbb R^4\mid x^2+y^2+z^2-w^2=-1,\;w>0\}$$ The polyhedron is represented as a cone (or its intersection with the hypersurface). A point in 3D is represented as a ray, all the positive scalar multiples of some vector in $\mathbb R^4$ . So we might as well work in $\mathbb S^3$ , and use the standard dot product, though the following could be formulated in terms of general linear algebra (e.g. using quotient spaces instead of orthogonal complements). Let $A$ be an abstract 3-polytope , and $A_k$ be the set of rank $k$ elements of $A$ , for $k=-1,0,1,2,3$ . Let $\sigma$ be a function on $A$ with the following properties: If $x\in A_k$ , then $\sigma x$ is a $(k+1)$ -dimensional linear subspace of $\mathbb R^4$ . (If you prefer, consider its intersection with $\mathbb S^3$ , which is then a $k$ -dimensional geodesic subspace.) If $x\leq y$ in $A$ , then $\sigma x\subseteq\sigma y$ . This function is a realization of $A$ . (It's nondegenerate if the last part is strengthened: $x\leq y$ if and only if $\sigma x\subseteq\sigma y$ .) Let $v$ be a function on incident pairs in $A$ (differing in rank by $1$ ) with the following properties: If $A_k\ni x<y\in A_{k+1}$ , then $v(x,y)\in\mathbb S^3$ is a unit vector orthogonal to $\sigma x$ and contained in $\sigma y$ . Since the dimensions differ by $1$ , there are only $2$ such unit vectors, differing in sign. (This vector $v(x,y)$ is supposed to point from $\sigma x$ to the ""inside"" of $\sigma y$ , and $-v(x,y)$ to the ""outside"". These notions make sense for convex polytopes, and less sense for self-intersecting polytopes.) If $A_{k-1}\ni w<x<z$ and $x\neq y$ and $w<y<z\in A_{k+1}$ , then the exterior product $v(w,x)\wedge v(x,z)=-v(w,y)\wedge v(y,z)$ . (Note that $v(w,x)\cdot v(x,z)=0$ by the previous property.) Both of these unit bivectors are contained in the $1$ -dimensional space $\bigwedge^2\big(\sigma w^\perp\cap\sigma z\big)$ , so they must be equal up to sign. We require opposite signs. (If going from vertex $w$ to edge $x$ to face $z$ is turning clockwise, then going from vertex $w$ to the other edge $y$ to face $z$ should be turning anticlockwise.) This function gives a kind of orientation of the polyhedron. It also gives the angles in faces, and the dihedral angles. Suppose $w<x<z$ and $w<y<z$ are two edges meeting at a vertex in a face; then the angle $\theta$ between the edges is completely determined (modulo $2\pi$ ) by $\cos\theta=v(w,x)\cdot v(w,y)$ and $\sin\theta=v(w,x)\cdot v(y,z)$ . In fact, given $A$ , and the angles and edge lengths, we can reconstruct $\sigma$ and $v$ , up to isometry of $\mathbb S^3$ . Likewise, given $A$ and $v$ , we can reconstruct $\sigma$ . All of that is just my idea of an (oriented) not-necessarily-convex polyhedron in spheric space. (Of course it could be modified for Euclidean or hyperbolic space. But those modifications only involve the metrical properties, not the incidence relations or convexity.) Now to formalize the question. I'll use the notation $[x,z]_k=\{y\in A_k\mid x\leq y\leq z\}$ . Edges are convex: $$\forall w\in A_{-1},\;\forall z\in A_1,\;\forall x\in[w,z]_0,\;\forall y\in[w,z]_0, \\ (y=x)\lor\big(v(x,z)\cdot v(w,y)>0\big).$$ Angles in faces are convex: $$\forall w\in A_0,\;\forall z\in A_2,\;\forall x\in[w,z]_1,\;\forall y\in[w,z]_1, \\ (y=x)\lor\big(v(x,z)\cdot v(w,y)>0\big).$$ Dihedral angles are convex: $$\forall w\in A_1,\;\forall z\in A_3,\;\forall x\in[w,z]_2,\;\forall y\in[w,z]_2, \\ (y=x)\lor\big(v(x,z)\cdot v(w,y)>0\big).$$ Faces are convex: $$\forall w\in A_{-1},\;\forall z\in A_2,\;\forall x\in[w,z]_1,\;\forall y\in[w,z]_0, \\ (y\leq x)\lor\big(v(x,z)\cdot v(w,y)>0\big)$$ (in addition to edge and angle convexity for the face $z$ ; but we've already specified edge and angle convexity for everything). Vertex figures are convex: $$\forall w\in A_0,\;\forall z\in A_3,\;\forall x\in[w,z]_2,\;\forall y\in[w,z]_1, \\ (y\leq x)\lor\big(v(x,z)\cdot v(w,y)>0\big).$$ Polyhedron is convex? $$\forall w\in A_{-1},\;\forall z\in A_3,\;\forall x\in[w,z]_2,\;\forall y\in[w,z]_0, \\ (y\leq x)\lor\big(v(x,z)\cdot v(w,y)>0\big)$$ In words, a polyhedron is convex , if each vertex is inside the half-space for each face (unless the vertex is part of the face), and the faces and vertex figures are convex polygons. I haven't shown that this definition of a convex polyhedron is equivalent to the usual definitions (intersection of a finite set of half-spaces, convex hull of a finite set of points in 3D, or conical hull of a finite set of vectors in 4D). At least, I have shown it to myself. If someone wants to ask a new Question about the equivalence, I suppose I could answer. For now, take it for granted. Here's an idea for a proof, in the case of Euclidean space. An element $x\in A_k$ is realized as a $k$ -dimensional affine subspace $\sigma x\subset\mathbb E^3\cong\mathbb R^3$ . Let's ignore the distinction between a $0$ -dimensional subspace and the unique point it contains, so that a vertex $x\in A_0$ has $\sigma x\in\mathbb E^3$ rather than $\sigma x\subset\mathbb E^3$ . Let $l\in A_{-1}$ and $g\in A_3$ be the least and greatest elements. Take an arbitrary face $f$ of the polyhedron, with inward normal vector $v(f,g)$ . Let $c\in\mathbb R$ be the constant such that $v(f,g)\cdot\sigma x=c$ for all vertices $x\leq f$ . We want to show that $v(f,g)\cdot\sigma x>c$ for all other vertices $x\not\leq f$ . Certainly $v(f,g)\cdot\sigma y>c$ for any vertex $y$ adjacent to a vertex $x\leq f$ . Since the vertex figures are convex, we have $v(f,g)\cdot v(x,e)>0$ for any edge $x\leq e\not\leq f$ . The adjacent vertex is $\sigma y=\sigma x+t\,v(x,e)$ , where $t>0$ is the length of the edge; thus $v(f,g)\cdot\sigma y=v(f,g)\cdot\sigma x+t\,v(f,g)\cdot v(x,e)>c$ . By similar reasoning, given $x\leq f$ , any vertex $y$ in a face containing $x$ also has $v(f,g)\cdot\sigma y>c$ , because $\sigma y$ is in the (2D) convex cone generated by two edges $e,e'\geq x$ . So we have, at least, a subset $B\subseteq A$ of the polyhedron, where all vertices are known to be either in $f$ or on the positive side of $f$ . Consider $B$ as a surface, with a curve as its boundary. Take the vertex $x$ on the boundary of $B$ with the smallest value of $v(f,g)\cdot\sigma x$ . It follows that $v(f,g)\cdot v(x,e)\geq0$ for any edge $e\geq x$ on the boundary, as that edge connects $x$ to another vertex on the boundary which must not have a smaller dot product with $v(f,g)$ . Add to $B$ the faces incident with $x$ , along with their edges and vertices, thus changing the boundary of $B$ so that it no longer contains $x$ . It should be the case that any such new vertex $y$ has $v(f,g)\cdot\sigma y\geq v(f,g)\cdot\sigma x$ , as a result of the convexity of the vertex figure at $x$ (but I'm not sure about this). Repeat until $B=A$ . (Since the polyhedron is connected, every vertex will be reached.) The dot product with $v(f,g)$ is always increasing, thus always greater than $c$ . Can we fill in the gaps in this proof? Can it be modified for spheric space? The argument in the previous section depends on finiteness, to find a minimum, and to reach all vertices eventually. I haven't yet specified that $A$ must be a finite set. Here's an infinite counter-example: a pentagrammic cylinder, or a stack of pentagrammic prisms, in hyperbolic space. Using hyperbolic cylindrical coordinates, the vertices are located at $$x_{(m,n)}=(\sinh(a)\cos(2m\cdot72^\circ),\sinh(a)\sin(2m\cdot72^\circ),\cosh(a)\sinh(nb),\cosh(a)\cosh(nb)),$$ $$m\in\mathbb Z\bmod5,\quad n\in\mathbb Z.$$ ( $a$ is the cylinder's radius. $b$ is a translation distance along the cylinder's axis.) The polyhedron's faces are ""rectangles"", with vertex sets of the form $\{x_{(m,n)},x_{(m,n+1)},x_{(m+1,n)},x_{(m+1,n+1)}\}$ . These are convex. (The angles in a face are less than $90^\circ$ , because of the hyperbolic geometry. We need a word to replace ""rectangle"" which refers to symmetry rather than angles.) The polyhedron's vertex figures are also convex, being rhombic. (In Euclidean space, one type of edge would have a dihedral angle of $180^\circ$ , violating strict convexity, but in hyperbolic space it's less than $180^\circ$ .) The polyhedron as a whole is not convex, because the plane containing $\{x_{(0,0)},x_{(0,1)},x_{(1,0)},x_{(1,1)}\}$ has some vertices on both sides; e.g. $x_{(2,0)}$ and $x_{(3,0)}$ are separated by that plane. Of course this could be projected to Euclidean space (Beltrami-Klein model; divide the first three coordinates by the fourth), but then it would have less symmetry, and two accumulation points $(0,0,\pm1)$ .","['polyhedra', 'discrete-geometry', '3d', 'convex-geometry', 'geometry']"
4444014,"Show that if $A$ and $B$ commute, then $B$ commutes with $e^A$","Show that if $A$ and $B$ commute, then $B$ commutes with $e^A$ For the first one I have $$e^A = I + A + \frac{A^2}{2!} + \frac{A^3}{3!} +\dots$$ I believe I can pull out $$I + A + \frac{A^2}{2!} + \frac{A^3}{3!} +\dots$$ and multiply by $B$ on either side but not sure the validity of this proving this","['exponentiation', 'examples-counterexamples', 'matrices', 'solution-verification', 'linear-algebra']"
4444036,Does this setup imply that $E[(M_t-M_s)^4 \mid \mathcal{F}_s]$ is bounded?,"Suppose $(M_t)_{t \geq0}$ is a martingale w.r.t. a filtration $(\mathcal{F}_t)_{t \geq0}$ . Suppose that $$
E[(M_t-M_s)^2 \mid \mathcal{F}_s]
$$ is uniformly bounded by some constant. I want to prove that $$
E[(M_t-M_s)^4 \mid \mathcal{F}_s] \quad \quad  (*)
$$ is uniformly bounded as well. It would certainly suffice to show that $$
E[(M_t-M_s)^4 \mid \mathcal{F}_s] \leq  E[(M_t-M_s)^2 \mid \mathcal{F}_s]^2
$$ but all inequality-results I can conjure up (Jensens, Cauchy Schwartz, Hölder, etc.) all bound in the ""wrong direction"". I also suspect this bound is too good to be true. But is $(*)$ bounded regardless? We probably need to use the martingale property somewhere but I can't find any useful results regarding the fourth moment of a martingale. Any help is appreciated!","['conditional-expectation', 'martingales', 'probability-theory']"
4444048,Suppose $T$ is an operator on $V$ and $T^2 = I$ and $-1$ is not an eigenvalue of $T$. Prove that $T = I$.,"My attempt: Since $ -1$ is not an eigenvalue of $T\implies T+I$ is invertible, that is $(T+I)^{-1}$ exists. Now $(T+I)(T-I) = T^2 - I \implies (T+I)(T-I) = 0$ . After applying $(T+I)^{-1 }$ on left and right hand side of the previous equation we get: $ (T+I)^{-1} (T+I) (T-I) = (T+I)^{-1}(0) \implies T-I = 0 \implies T = I$ .",['linear-algebra']
4444057,"Can an ""improper Lebesgue integral"" differ from the ordinary Lebesgue integral?","A comment by Aloizio Macedo in this post claims claims Every ""improperly Riemann integrable function"" is also ""improperly Lebesgue integrable"". That Lebesgue doesn't know how to handle this as much as Riemann (not counting H-K) is a commonly wide-spread myth: for instance, you just take $\lim_{x \to \infty} \int_{[c,x]} f$ in the same way as Riemann. The difference is that Lebesgue has a meaning on itself for the value $\int_{[c,\infty]} f$ , whereas Riemann doesn't, and this can differ from the ""improper"". I'm skeptical of the claim ""and this can differ from the 'improper'."" Perhaps he means ""differ"" in that the Lebesgue integral $\int_{[c,\infty)}f$ might not exist, whereas the improper Riemann or Lebesgue integral defined by $\lim_{x \to \infty} \int_{[c,x]} f$ might exist. This I agree with, as there are classic examples like $f(x)=\sin(x)/x.$ But perhaps he is making a stronger claim, that there exists a function for which the ordinary Lebesgue integral and improper Lebesgue integral exist, but have different values. This I find hard to imagine. Hence: Is there a Lebesgue-integrable function $f:[c,\infty)\to\mathbb R$ such that $\lim_{x\to\infty}\int_{[c,x]}f\neq\int_{[c,\infty)}f$ ? Both integrals above are Lebesgue integrals, and the limit is assumed to exist.","['integration', 'measure-theory', 'improper-integrals', 'lebesgue-integral', 'real-analysis']"
4444162,"Issues with the Fourier Transform of $f(t)=(1-t^2)^4$ on $[-1,\,1]$, should be analytical but looks like having a singularity with noise-like rippling","Issues with the Fourier Transform of $f(t)=(1-t^2)^4$ on $[-1,\,1]$ , should be analytical but looks like having a singularity with noise-like rippling Intro I was trying to made a compact-supported approximation of a Gaussian Envelope $$g(t)= e^{-4t^2} \tag{Eq. 1}\label{Eq. 1}$$ So I was trying to use instead: $$f(t)= (1-t^2)^4\cdot\theta(1-t^2) \cong \left(\frac{1-t^2+|1-t^2|}{2}\right)^4 \tag{Eq. 2}\label{Eq. 2}$$ where $\theta(t)$ is the Heaviside step function . Both which can be seen here : As is shown in the image, at ""my taste"" the match ""looks good"", but accurately speaking, it fulfill that: The function $f(t) = 0,\,|t|\geq 1$ , so it is of compact support. At the edges of the domain $\partial t =\{-1;\,1\}$ the function $s(t)=(1-t^2)^4$ is already zero: $s(-1)=s(1)=0$ , so shouldn't be much issues related with the term $\theta(1-t^2)$ , since it is going to be at most an avoidable discontinuity: $\lim\limits_{t \to \partial t^{\pm}} f(t) = \lim\limits_{t \to \partial t} f(t) = 0$ , so the function is continuous on $\mathbb{R}$ . Using Wolfram-Alpha, the following norms shows to be bounded: $\|f\|_\infty = 1 < \infty$ , $\|f\|_1 = \frac{256}{315} \approx 0.813 < \infty$ , $\|f\|_2^2 = \frac{65536}{109395} \approx 0.599 < \infty$ so is bounded, absolute integrable, and energy limited. Also its derivative is: $$\begin{array}{r c l}
\frac{d}{dt}\left((1-t^2)^4\cdot\theta(1-t^2)\right) & = & \frac{d}{dt}\left((1-t^2)^4\right)\cdot\theta(1-t^2)+(1-t^2)^4\cdot\frac{d}{dt}\left(\theta(1-t^2)\right)\\
& = & -8\,t\,(1-t^2)^3\cdot\theta(1-t^2)+(1-t^2)^4\cdot\frac{d}{dt}\left(\frac{1+\text{sgn}(1-t^2)}{2}\right)\\
& = & -8\,t\,(1-t^2)^3\cdot\theta(1-t^2)+2\,t\,(1-t^2)^4\cdot\delta(1-t^2)\\
& = & -8\,t\,(1-t^2)^3\cdot\theta(1-t^2)+2\,t\,(1-t^2)^3\cdot\underbrace{(1-t^2)\cdot\delta(1-t^2)}_{=\,0,\,\text{since}\,(1-x)\delta(1-x)=0} \\
\end{array}$$ $$\Rightarrow \frac{df(t)}{dt}  = -8\,t\,(1-t^2)^3\cdot\theta(1-t^2) \tag{Eq. 3}\label{Eq. 3} $$ so its derivative is also continuous by the same reasons of point (ii), and $\|f'\|_\infty = \frac{1798}{343\sqrt{7}} \approx 1.9 < \infty$ , $\|f'\|_1 = 2 < \infty$ , and $\|f'\|_2^2 = \frac{131072}{45045} \approx 2.9 < \infty$ . So with all these point the approximation $f(t)$ look quite ""well-behaved"": note that is not a ""smooth function"", but same analysis of point $(4)$ could be done successfully for $f''$ also. From now on I will continue labeling the observations so you can accurately point where I am having a misconception. Since the function $f(t)$ is of compact-support and also is squared-integrable (since $\|f\|_2^2 < \infty$ as show in point $(3)$ ), I where expecting that the Paley–Wiener theorem to be fulfilled, so the Fourier Transform of $f(t)$ was going to be an analytical function . Using Wolfram Alpha I calculate the Fourier Transform (under the ""electricians"" definition), in 3 different ways showing each of them the same result so I believe is right calculated - way 1 , way 2 , and way 3 : $$\hat{f}(w) = \int\limits_{-\infty}^{\infty} f(t)\,e^{-iwt}\,dt = \int\limits_{-1}^{1} (1-t^2)^4\,e^{-iwt}\,dt = \frac{768 \Big(5w\,(2 w^2-21)\cos(w)+(w^4-45w^2+105) \sin(w)\Big)}{w^9}  \tag{Eq. 4}\label{Eq. 4}$$ So far so good, since \eqref{Eq. 4} looks like the classical Fourier Transform made by a polynomial mixed with some trigonometric functions, but when trying to calculate $\|\hat{f}\|_1$ and $\|iw\hat{f}\|_1$ the integrals get stuck on Wolfram-Alpha, so I decide to plot it to see what is going on. I am going to graph it against the Fourier Transform of $g(t)$ , which under the same definition of the transform is going to be: $$\hat{g}(w) = \frac{\sqrt{\pi}}{2}e^{-\frac{w^2}{16}}\tag{Eq. 5}\label{Eq. 5}$$ They look similar, but something ""weird"" is happening near the DC component $w \approx 0$ for the solution $\hat{f}(w)$ of \eqref{Eq. 4}. To have a better insight I plot $\hat{f}(w)$ also in Desmos and there exist an ""horrible noise-like rippling"" that also ""looks"" to be diverging as a singularity! I tried to find the value of $\|\hat{f}\|_\infty$ with Wolfram-Aplha but I believe are numerical errors since the value change when changing the domain limits, but are numbers of the order of $10^{93}$ !!!. I believe that a Analytical function is smooth (as it should be the Fourier Transform $\hat{f}$ following point $(5)$ ), so it should be bounded on every closed interval of its domain: maybe here I am mistaken, but since analytic functions are smooth, they should be continuous on every closed interval of the domain (since are already differentiable), and since is continuous on a closed interval it should achieve a bounded maximum and minimum because of the Extreme value theorem . But even if I am wrong, I wasn't expecting an analytical function to be so ""ill-behaved"" as is being $\hat{f}$ on $w \in [-0.2,\,0.2]$ . Questions So the main questions are: Q1: Is $\hat{f}(w)$ of \eqref{Eq. 4} properly obtained? Also related quantities as $\|\hat{f}\|_1$ and $\|\hat{f}\|_{\infty}$ Q2: If right, Why is this ""rippling"" happening? It is right? or is a miscalculation of the graphic software? or a numerical issue? Actually its looks like ""Brownian"" or ""White Noise"". Q3: Is truly having a singularity at some angular frequency $w$ ? (doing limits in Wolfram Alpha doesn't work, at least it is saying that $\lim_{w\to 0} \hat{f}(w)=\frac{256}{315}\approx 0.813$ which is near the main tendency, but surely below the rippling peaks - maybe the singularity is near but not in $w=0$ ). Q4: Is this rippling a known result like the "" Gibbs' Phenomenon ? How is called? If it is a known phenomenon I would like to search for any references - maybe the is a way to avoid it. Q5: Where I am misleading my analysis? (since this singularity shouldn't exists under my assumptions and my actual knowledge - which is quite basic by the way) Motivation After seeing this video about the absorption of light, temporal dispersion, and Kramers-Kronig relations, required for the complex-valued modeling of the refraction index to preserve causality (is quite a good video, simple and short), I see why dispersion must happen or other way the absorbed frequency will lead to a ""time-wide"" signal which will become non-causal in the analysis (it will be entering the absorptive medium before the frequency-component subtraction had happen on the first place). But since a Gaussian Envelope is used, the ""causality-issue"" where already present since this function only vanishes at infinity, but since spectra decays faster than $1/|w|$ , the Kramers-Kronig relation still holds ""hidding"" this issue. This is why I was trying to find another envelope but with an accurate compact-support, since it is multiplied by a trigonometric function it Fourier Transform is only a displaced version of the Transform of the Envelope function, and here is where I found the ""problem"" since the approximation I take has this ""issue"" in the spectrum - don't knowing now if its a ""real-life issue"" or just a numeric problem of the calculation algorithms. Hope you find it has interesting I am, beforehand thanks you very much. added later I have noted now another interesting thing: Integrating $\hat{f}$ as is shown here , and integrating $\hat{g}$ as is shown here gives the same result: $$\int\limits_{-\infty}^{\infty}\hat{f}(w)\,dw = \int\limits_{-\infty}^{\infty}\hat{g}(w)\,dw = 2\pi$$ I have tried another slightly different approximation: $$q(t) = \left(\frac{1-t^2+|1-t^2|}{2}\right)^{\pi} \tag{Eq. 6}\label{Eq. 6}$$ And following Wolfram-Alpha its Fourier Transform is: $$\hat{q}(w) = \int\limits_{-\infty}^{\infty} q(t)\,e^{-iwt}\,dt = \sqrt{\pi}\,\Gamma(1+\pi)_0\tilde{F}_1\left(;\,\frac{3}{2}+\pi;\,-\frac{w^2}{4}\right)  \tag{Eq. 7}\label{Eq. 7}$$ which plot doesn't show the noise-like-rippling-singularity, so with this, as it was noted on the comments and answers, the situation was a numerical issue. Even so, if you directly replace all the numbers $\pi$ by the number $4$ in \eqref{Eq. 7} the rippling appears again. Later I noted that the result of \eqref{Eq. 4} is equivalent to: $$\hat{f}(w) = \sqrt{\pi}\,\Gamma(1+4)_0\tilde{F}_1\left(;\,\frac{3}{2}+4;\,-\frac{w^2}{4}\right)  \tag{Eq. 8}\label{Eq. 8}$$ So obviously again the numerical issues arises, but for a ""good"" approximation I am using on Wolfram-Alpha : $$\hat{f}^*(w) = \sqrt{\pi}\,\Gamma(1+4)_0\tilde{F}_1\left(;\,\frac{3}{2}+4.0001;\,-\frac{w^2}{4}\right)  \tag{Eq. 9}\label{Eq. 9}$$ and suddenly the problem is gone... it is kind of ridiculous that ""just"" for the exact numbers the problem happen. Since the regularized confluent hypergeometric function is not quite extended on every software library I find useful the following property shown by Wolfram Alpha to make approximated plots of these kind of functions, by taking an arbitrary parameter $a$ : $$\sqrt{\pi}\,\Gamma(1+a)_0\tilde{F}_1\left(;\,\frac{3}{2}+a;\,-\frac{w^2}{4}\right) = \text{sgn}(w)\sqrt{\pi}\,\left(\frac{2}{w}\right)^{a+\frac{1}{2}}\Gamma(1+a)J_{a+\frac{1}{2}}(w) \tag{Eq. 10}\label{Eq. 10}$$ with $J_n(x)$ the Bessel function of the first kind. Unfortunately, I wasn't able to find $\|\hat{f}\|_1$ under any of these approximations (both shown on the last image), ""as it where diverging"" (I don't know if this is the case), even when $\|\hat{g}\|_1=2\pi << \infty$ , which is counter-intuitive for me giving its similarity (the lobes should be adding even less area than a Gaussian in principle), but comparing their graphs shows that the compact-support function has lobes much more spread than the Gaussian, as is shown here . But nevertheless, the tails are decreasing faster than $|\frac{1}{w^2}|$ so I don´t know why the result is not finite (see here ). As example, if I used the other aporoximarion: $$\hat{f}^*(w) \approx \frac{962.612\,J_{4.5001}(w)}{w^{4.5001}}\tag{Eq. 11}\label{Eq. 11}$$ Wolfram-Alpha is unable to find $\|\hat{f}^*(w)\|_1$ , but it is possible to bound it as: $$\|\hat{f}^*(w)\|_1 < \underbrace{\int\limits_{-10}^{10}\left|\frac{962.612\,J_{4.5001}(w)}{w^{4.5001}}\right|\,dw}_{6.34416}+\underbrace{\int\limits_{10}^{\infty}\left|\frac{2}{|w^{2}|}\right|\,dw}_{0.2} <\infty$$ as can be seen here and here . And since the difference with the approximation and the main function with numerical issues is in the bounded domain $[-1,\,1]$ , it should also be bounded, so the norm $\|\hat{f}(w)\|_1<\infty$ but somehow Wolfram-Alpha can't find it.","['fourier-analysis', 'real-analysis', 'lp-spaces', 'upper-lower-bounds', 'finite-duration']"
4444177,Proof verification related to the Intermediate value theorem.,"Suppose that f is a continuous function on [0, 2] such that f(0) = f(2). Show
that there is a real number ξ ∈ [1, 2] with f(ξ) = f(ξ − 1). ξ ∈ [1, 2], ξ-1 ∈ [0, 1]
Case 1: if f(0)=f(1) then ξ = 2, f(ξ) = f(0)= f(1) = f(2) therefore f(ξ)= f(ξ-1)=f(2)=f(1) Case 2: if f(0)>f(1) then f(2) > f(1). By continuity there must exist some m where f(2) ≥ m ≥ f(1) and by IMV theorem there exists some ξ ∈ [1, 2] where f(ξ)=m. Now, ξ-1 ∈ [0, 1] and by continuity there must also exist the same m, f(0)≥ m ≥ f(1). Therefore by IMV theorem again there must exist at least 1 value where f(ξ − 1) = m and since m is common there must exist at least 1 value where f(ξ − 1) = f(ξ). Case 3: f(1)>f(0). This is pretty much the same thing as case 2. -This is a very difficult calculus question and I've never written a proof and I'm in Algebra II. Pls don't flame me if this is very stupid.","['calculus', 'solution-verification', 'learning']"
4444179,"In how many ways can $\mathbb{Z_5}$ act on $\{1,2,3,4,5\}$","In how many ways can $\mathbb{Z_5}$ act on $\{1,2,3,4,5\}$ ? I could figure out if $\mathbb{Z_5}$ acts on a set $\{1,2,3,4,5\}$ the orbit $\theta_i$ is either $1$ (the tribial action) or $5$ . Now we see that if the action is transitive then the shouldnt it correspond to $5$ cycles of the form $(12345)^i$ where $1 \le i \le 4$ ? So since the homomorphism $\mathbb{Z_5} \to S_5$ is completely determined by $1_5$ , it can send the first symbol to any one of $4$ choices the second symbol to any one of the $3$ choices and so on. So there are $4!$ choices. Theres an answer to the question in the website whoch I was not able to understand. Is this ok? Also I think we will get the same answer if we replace $\mathbb{Z}_m$ by any other cyclic group, say $\mathbb{Z}$ .","['group-actions', 'group-theory', 'abstract-algebra']"
4444192,Artin's Algebra 4.4.8 [duplicate],"This question already has answers here : Eigenvectors of a Homothety operator (2 answers) Closed last year . Q.
Let $T$ be a linear operator on a finite-dimensional vector space for which every nonzero vector is an eigenvector. Prove that $T$ is multiplication by a scalar. I did not find this question in old posts. Approach:- First suppose $\operatorname{dim}(V)=1$ . Then there is a non-zero vector $v \in V$ such that $V=\{c v: c \in F\}$ . By hypothesis we have $\lambda \in F$ such that $T(v)=\lambda v$ , so that $T(c v)=c T(v)=c(\lambda v)=\lambda(c v)$ i.e. $T=\lambda I$ where $I: V \rightarrow V$ is the identity operator. So in this case we are done. Next suppose, $\operatorname{dim}(V) \geq 2$ . Then let $u, w$ be two linearly independent vectors of $V$ . Now we have $\alpha, \beta \in F$ such that $T(u)=\alpha u$ and $T(w)=\beta w$ . Now note that $u+w \neq 0$ as $\{u, w\}$ is a linearly independent set. Hence there is $\gamma \in F$ such that $T(u+w)=\gamma(u+w)$ . So that $\alpha u+\beta w=T(u)+T(w)=T(u+$ $w)=\gamma(u+w)$ . Hence $\alpha u+\beta w=\gamma u+\gamma w$ i.e. $(\alpha-\gamma) u=(\gamma-\beta) w$ . Since $\{u, w\}$ is a linearly independent set we have $\alpha-\gamma=0=\gamma-\beta$ i.e. $\alpha=\beta$ . What we observe is that for every vector $w$ which is linearly independent with $u$ we have $T(w)=\alpha u$ where $\alpha \in F$ is such that $T(u)=\alpha u$ . Now every linearly independent subset can be extended to a basis of $V$ . So let $\left\{v_{1}, \ldots, v_{n}\right\}$ be a basis of $V$ with $u=v_{1}$ , then for any $x \in V$ with representation $x=c_{1} v_{1}+c_{2} v_{2}+\ldots+c_{n} v_{n}$ with $c_{1}, \ldots, c_{n} \in F$ we have $T(x)=c_{1} T\left(v_{1}\right)+$ $c_{2} T\left(v_{2}\right)+\ldots+c_{n} T\left(v_{n}\right)=c_{1}\left(\alpha v_{1}\right)+c_{2}\left(\alpha v_{2}\right)+\ldots+\left(c_{n} \alpha v_{n}\right)=\alpha\left(c_{1} v_{1}+\ldots+c_{n} v_{n}\right)=\alpha x$ i.e. $T=\alpha I$ .
The case when $\operatorname{dim}(V)=0$ is trivial as in this case $V=\{0\}$ so that $T=0=0 I$ , where $I: V \rightarrow V$ is the identity operator. Please cheak this.. Also you can give your approach. Thank you...",['linear-algebra']
4444213,"""Tipping point"" between asymptotic behavior of gamma function along the line $z=x+mxi$","Disclaimer: I am an undergraduate student about a semester into introductory complex analysis. I am entirely out of my depth here, just curious about something I noticed. The gamma function $\Gamma(z) := \int_0^\infty\limits z^{z-1}e^{-x} dx$ is a generalization of the standard factorial function $n! = n \times (n-1) \times (n-2) \dots \times 1$ defined over the complex plane (minus nonpositive integers), and with the caveat that for nonnegative integer $n$ , $\Gamma(n) = (n-1)!$ As such, on the (positive) reals, $\Gamma$ grows superexponentially. Indeed more generally, for any complex $z$ , $\Gamma(z+x) \to \infty$ (in magnitude) as $x \in \mathbb{R}$ tends to infinity. Interestingly though, the opposite is true of the imaginary axis: $\Gamma(z+iy) \to 0$ as $y \in \mathbb{R}$ gets large. Informally then, the real part of $z$ makes $\Gamma(z)$ large, and the imaginary part makes it small. I was curious: how much stronger is one of these effects than the other? To make this precise, consider the function $f(\theta) = \lim_{x \to \infty}\limits \left|\Gamma(x e^{i\theta})\right|$ ; pick an angle, and track the magnitude of $\Gamma$ for inputs of increasing magnitude with that argument in the complex plane. $f(\theta)=\infty, f(\pi/2) = 0$ , are there any values for which it's finite and nonzero? Where does it change? Among those values where it tends to infinity, can we say anything about its growth? I toyed around in Mathematica, specifically with Manipulate[Plot[Abs[Gamma[x*Exp[I*t]]], {x, 0, 50}], {t, 0, Pi/2}]} and found that for a value of $t$ around 1.23 the end behavior seems to change. However when I increase the upper limit for $x$ , say Manipulate[Plot[Abs[Gamma[x*Exp[I*t]]], {x, 0, 100}], {t, 0, Pi/2}]} , I need to bring $t$ to about 1.36. In light of this, I wouldn't be surprised if $f(\theta) = \infty$ for all $\theta < \pi/2$ , and looking at the graphs, neither would I be surprised if the growth is always superexponential, even factorial in nature. It's certainly not what I would have expected, I figured there would be some point in $(0, \pi/2)$ where the behavior changes, but it's not entirely out of left field. Thank you in advance for any possible insight.","['complex-analysis', 'asymptotics', 'gamma-function']"
4444245,Independent events or conditional probability?,"In my daily morning walk there is a 20% chance I drop my pouch.
Every day I follow the exact same straight path, to the district square and then back.
Assuming that my walk is on a straight segment from A to B and then back from B to A, what is the probability I dropped it from A to B? I know this seems to be a very easy problem, but I am a little confused:
Initially, there is equal probability for me to drop my pouch in any of the two directions.
So for AB, this probability is 50% of 20%? But for the second part of my walk back home, the probability is conditional.
If I have already dropped it in AB, the probability is zero. If I haven’t, the probability is 100% of the total 20%, that is, 20%. Any clue? Edit : There are two answer which have been updated, but they sort of conflict each other. Any help clearing it out would be greatly appreciated.",['probability']
