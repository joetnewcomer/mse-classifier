question_id,title,body,tags
3550289,How to prove that $\tan \left( \frac{\pi}{2} - \theta \right) = \cot \theta$ [duplicate],"This question already has answers here : Prove $\tan\big(\frac{\pi}{2} -\theta \big) = \cot \theta$ (5 answers) Closed 4 years ago . I am asked to simplify the following expression: $$\tan \left( \frac{\pi}{2} - \theta \right)$$ The book gives me the answer $\cot \theta$ but, when I try to derive that formula using $$\tan \left( A \pm B \right) = \frac{\tan A \pm \tan B}{1 \mp \tan A \cdot \tan B}$$ I get $\tan \frac{\pi}{2}$ as one of the terms (which is undefined). $$\tan \left( A \pm B \right) = \frac{\tan A \pm \tan B}{1 \mp \tan A \cdot \tan B} = \frac{\tan \frac{\pi}{2} - \tan \theta}{1 + 0 \cdot \tan \theta}$$ How do I proceed? Thank you.",['trigonometry']
3550323,Existence of ample divisor with smooth image,"Let $f:X \rightarrow Y$ be a proper, generically finite morphism between smooth, projective varieties. Is there an ample divisor $D$ on $X$ such that $f(D)$ is smooth?",['algebraic-geometry']
3550332,Proving Division Rings of $p^2$ Elements are Fields,"Exercise III.2.11 (Aluffi, Algebra Ch 0): Let $R$ be a division ring consisting of $p^2$ elements, where $p$ is a prime. Prove that $R$ is commutative (and thus $R$ is a field). Note: I do so without invoking Wedderburn's Theorem, that every finite division ring is a field. Also, within the context of this text, Rings must include unity. Proof Assume towards a contradiction that $R$ is not commutative. Then it's center, $C$ , is a proper subring of $R$ . Now, since $|R| = p^2$ , $|C| = p$ , as subrings are also subgroups and thus by Lagrange's Theorem $|C|$ must divide $|R|$ . Notice that since $0_R, 1_R \in C$ , $|C| > 1$ . Now, let $r \in R, r \not \in C$ be arbitrary. Notice that the centralizer of $r$ contains both $r$ and $C$ , as r commutes with itself and the center is the intersection of all centralizers. Thus the centralizer must contains more than $p$ elements. Since the centralizer is also a subring, this forces the centralizer to be all of $R$ . Lastly, since $r$ was arbitrary it follows the centralizer for every element is all of $R$ . Therefore, the center of $R$ is $R$ and thus we have a contradiction and it follows that $R$ is commutative. Personal Notes: I feel that this proof is a little wordy, but beyond that I'm curious for any advice on if there is a necessity for more clarity or if there are any elements that should be cut from the proof.","['division-ring', 'ring-theory', 'abstract-algebra', 'solution-verification']"
3550334,Unable to think about 10-combinations of a multiset,"I am trying assignments of an Institute  in which I don't study and i could not think about this problem. Problem is -> Determine the number of 10-combinations of multisets S= { 3.a, 4.b, 5.c} I think the answer should be number of solutions of equation $  x_1+ x_2+x_3$ =10 such that $0\leq x_1\leq 3 $ , $0 \leq x_2 \leq 4 $ , $ 0\leq x_3 \leq 5 $ . But i am unable to think how to find solution of this equation under such constraints. Can somebody please help.","['combinations', 'multisets', 'combinatorics', 'discrete-mathematics']"
3550436,Action of $SO(4)$ on $\Lambda^2 \mathbb{R}^4$,"I'm beginning to study $4$ -dimensional Riemannian manifolds, in particular the decomposition induced by the Hodge $\star$ -operator on the space of the differential two-forms. I'm considering $\mathbb{R}^4$ as an example: I understood that $\Lambda^2 \mathbb{R}^4=\Lambda_{+}\oplus \Lambda_{-}$ , where $\Lambda_{\pm}$ is the eigenspace of $\star$ relative to the eigenvalue $\pm 1$ . The group $SO(4)$ acts on $\Lambda^2 \mathbb{R}^4$ in the standard way induced by the action of $SO(4)$ on $\mathbb{R}^4$ , i.e. if $X\in SO(4)$ and $a\wedge b$ is a two-form, then $X(a\wedge b)=Xa\wedge Xb$ . I've got three questions: $1)$ Since $\Lambda^2 \mathbb{R}^4\cong so(4)$ (the Lie algebra of $SO(4)$ ), are there some relations between the action of $SO(4)$ on $\Lambda^2 \mathbb{R}^4$ and the adjoint action of $SO(4)$ on $so(4)$ ? $2)$ What is the action of $SO(4)$ on $\Lambda^2 \mathbb{R}^4$ when restricted to $\Lambda_{\pm}$ ? I know that $so(4)\cong so(3)\oplus so(3)$ , so I thought it could be useful to compute the action, but I don't know how to do it. $3)$ I read that the restriction of the action to $\Lambda_{\pm}$ induces a surjective homomorphism $\phi:SO(4)\rightarrow SO(3)\times SO(3)$ which is two to one. How can I prove this and compute the homomorphism? I understood that there is a surjective homomorphism from $SU(2)$ to $SO(3)$ and that $SU(2)\times SU(2)$ is the universal cover of $SO(4)$ (a double cover, in particular): could it be useful?","['lie-algebras', 'riemannian-geometry', 'group-theory', 'lie-groups', 'differential-geometry']"
3550471,"Helical ""packing"" of deltahedra?","I'm looking for help with terminology: I'm a glass artist / PhD researcher aiming to use a modular, geometric approach to making.  I've been looking to use repeated polyhedra (with regular polygonal faces) as a modular substructure to hold more complex 'decorative' units. With so many options, I've initially limited myself to the convex deltahedra - and have been exploring how these can be put together. Rather than close packing, as is possible with the octahedron/tetrahedron, I'm interested in more loosely packed structures and formations. I found that I can create a wide variety of helical formations from pairs of the convex deltahedra. (Screen grabs of a two example CAD models below) With not having the correct words/terminology to describe these, I'm having trouble finding relevant work to read - or any areas of applied maths which might be relevant. I think that this is part of Topology - but is there a more specific area I should be looking at? And are these helices a form of packing - as the structure is only repeated in one rather than three dimensions? The closest thing I've found, is the Boerdijk-Coxeter helix - which does seem to relate to spherical packing. Also are the radius and vertical offset the only characteristics which I need to specify in order to designate the underlying helix, or do I need to consider other characteristics? Any pointers greatly appreciated.","['polyhedra', 'geometry', 'applications', 'packing-problem']"
3550500,A sequence $(a_n)_{n\ge 1}$ such that $a_1>0$ and $a_{n+1}=a_n-\ln(1+a_n)$,"Let $(a_n)_{n\ge 1}$ be a sequence  such that $a_1>0$ and $$a_{n+1}=a_n-\ln(1+a_n)$$ a) Prove that $a_{n+1}<\frac{a_n^2}{2}, \forall n\in \mathbb{N}$ and $\lim\limits_{n\to \infty} (n^{2019}a_n)=0$ . b) If $a_1<2$ , prove that $a_{12} \in (0,10^{-300})$ . It is easy to see that $a_n >0$ , $\forall n\in \mathbb{N}$ . I considered the function $f:(0,\infty)\to \mathbb{R}$ , $f(x)=x-\ln(1+x)-\frac{x^2}{2}$ and I could easily prove that this function is strictly decreasing, so it follows that $$x-\ln(1+x)<\frac{x^2}{2}, \forall x>0 \tag{*}$$ Now from $(*)$ we get that $a_{n+1}<\frac{a_n^2}{2}, \forall n\in \mathbb{N}$ . I couldn't make much further progress. I could show that $\lim\limits_{n\to \infty}a_n=0$ , but then I got stuck.","['limits', 'sequences-and-series', 'real-analysis']"
3550553,"Find the slope of the graph of $xy-4y^2=2$ at $(9,2)$","Find the slope of the graph of $xy-4y^2=2$ at $(9,2)$ I will take the derivative with respect to $x$ , being careful to use implicit differentiation when differentiating a $y$ term, solving for $\frac{dy}{dx}$ ( $\frac{dy}{dx}$ is the slope!!) and then plugging in the point $(9,2)$ Solution: $\frac{d}{dx}(xy-4y^2)=\frac{d}{dx}2=0$ $(\frac{d}{dx}(xy)-4\frac{d}{dx}y^2)=0$ $(\frac{d}{dx}(xy)-4(2y\frac{dy}{dx})=0$ I will use the product rule to evaluate $\frac{d}{dx}(xy)$ : $((\frac{d}{dx}(x)y+x(\frac{d}{dx}y))-8y\frac{dy}{dx}=0$ $((1)y+x((1)\frac{dy}{dx}))-8y\frac{dy}{dx}=0$ $(y+x\frac{dy}{dx})-8y\frac{dy}{dx}=0$ $y+(x-8y)\frac{dy}{dx}=0$ $(x-8y)\frac{dy}{dx}=-y$ $\frac{dy}{dx}=\frac{-y}{(x-8y)}$ Cool, so now I've taken the derivative, was careful to use implicit differentiation when necessary, and solved for $\frac{dy}{dx}$ , which tells us the slope at any $(x,y)$ point that we plug in. The problem statements wants us to find the slope at $(9,2)$ , so let's plug that in!! $\frac{dy}{dx}=\frac{-2}{(9-8(2))}$ $\frac{dy}{dx}=\frac{-2}{9-16}$ $\frac{dy}{dx}=\frac{-2}{-7}$ $\frac{dy}{dx}=\frac{2}{7}$","['calculus', 'solution-verification', 'derivatives']"
3550584,Find the minimal polynomial of $a$,"Let $a=\sqrt[4]2+\sqrt2$ a) Is it true that $\mathbb Q(a)=\mathbb Q(\sqrt2, \sqrt[4]2)$ ? b) Find the minimal polynomial of $a$ over $\mathbb Q$ c) Find the minimal polynomial of $a$ over $\mathbb Q(\sqrt2)$ d) Is it true that $\sqrt[3]2\in \mathbb Q(a)$ ? In b) I have: $$a=\sqrt[4]2+\sqrt2$$ $$a^2-2\sqrt2a+2=\sqrt2$$ $$a^4-4a^2-8a+2=0$$ So we get the polynomial $f(a)=a^4-4a^2-8a+2$ and from Eisenstein's criterion we know that it is irreductible polynomial over $\mathbb Q$ so it is minimal polynomial over $\mathbb Q$ . In c) I have: $$g(a)=a^2-3\sqrt2a+2$$ But I don't know how to do a) and d)","['minimal-polynomials', 'abstract-algebra']"
3550586,Is there a group theoretic proof that $(\mathbf Z/(p))^\times$ is cyclic?,"Theorem: The group $(\mathbf Z/(p))^\times$ is cyclic for any prime $p$ . Most proofs make use of the fact that for $r\geq 1$ , there are at most $r$ solutions to the equation $x^r=1$ in $\mathbf Z/(p)$ , a result which doesn't seem â€” understandably â€” to have any group theoretic proofs. K. Conrad gives ten different proofs â€” and hints at some others â€” in his paper here . The first six make use of the previously mentioned fact, while the seventh proof makes extensive use of cyclotomic polynomials and is thus still not group-theoretic. I was also able to find a linear algebra based proof in the second chapter of TeorÃ­a Elemental de Grupos by Emilio Bujalance GarcÃ­a, but still, no group theoretic proof to be found.","['proof-writing', 'cyclic-groups', 'abstract-algebra', 'group-theory', 'soft-question']"
3550588,"When tackling trig identities, which side is the best to start from?","I'm currently in Y13. I wanted to know which side of a trig identity is typically best to work from (since I know you have to work from one side to the other). Here's an example of an identity I just proved: $$\tan(\frac{\pi}{4}-\frac{x}{2}) = \sec(x)-\tan(x)$$ I figured out the proof after a false start by working from both ends, then putting the steps together. But working from the RHS is a lot more straightforward than the LHS; the latter requires you to deduce that: $$\frac{1-\tan(\frac{x}{2})}{1+\tan(\frac{x}{2})} = \frac{(1-\tan(\frac{x}{2}))^2}{1-\tan^2(\frac{x}{2})}$$ which is fine, but not immediately obvious. Even if you continue working in this direction, the next steps are not very reassuring either. I was trying to multiply by $\frac{1+\tan(\frac{x}{2})}{1+\tan(\frac{x}{2})}$ , which wasn't very helpful. So I was wondering: are there any giveaway signs that one side of the identity will be easier to work from than the other? In hindsight, for this one, the double angles on the RHS were much easier to manipulate than the angle addition on the LHS. Is there a sort of hierarchy anyone has learnt from their experience?","['trigonometry', 'proof-writing']"
3550591,"(Proof Updated, Verification in Need) Show that $(s,t)\mapsto t\wedge s$ for $s,t\geq 0$ and $(s,t)\mapsto e^{-|t-s|}$ are positive semi-definite.","We say a positive symmetric $n\times n$ matrix $M$ over $\mathbb{R}^n$ is semi-definite if $v^{\intercal}Mv\geq 0$ for all nonzero $v\in\mathbb{R}^n$ . We say a function $f:\mathbb{T}^2\longrightarrow\mathbb{R}$ to be positive semi-definite if $\Big(f(t_k, t_j)\Big)_{k,j=1}^n$ is a positive semi-definite matrix for all $(t_k)_{k=1}^n\in\mathbb{T}^n$ With this definition, I am working on an exercise asking me to show $(1)$ the function $f:(s,t)\mapsto t\wedge s$ defined for $s,t\geq 0$ is positive semi-definite; $(2)$ the function $c:(s,t)\mapsto e^{-|t-s|}$ is positive semi-definite. For the first one, I tried to use the fact that $$\int_{\mathbb{R}^{+}}\mathbb{1}_{[0,t]}\mathbb{1}_{[0,s]} \,d\mu = t\wedge s,$$ so that each term in the matrix is of the form $b_{i,j}:=f(t_i,t_j)=\int_{\mathbb{R}^{+}}\mathbb{1}_{[0,t_{j}]} \mathbb{1}_{[0,t_{}} \, d\mu$ , for $t,j=1,\cdots,n$ . Let $v=(v_1,\ldots, v_n)\in\mathbb{R}^n$ , then $$v^{\intercal} Mv=a_1 \sum_{i=1}^n a_i b_{i,1}+a_2\sum_{i=1}^n a_i b_{i,2} + a_3 \sum_{i=1}^n a_i b_{i,3}+\cdots+a_n \sum_{i=1}^n a_i b_{i,n}.$$ But then I don't know what to do next.. For the second one, the exercise gives an hint: using an auxiliary Hilbert space $H$ and a $h_t\in H$ such that $\langle h_t, h_s\rangle=c(s,t)\ldots$ I don't really know how to use this hint... I really need an answer with some details, since this is an exercise in Stochastic Process, instead of functional analysis and so forth, so I don't have enough background of this... Thank you so much! Edit 1: (Proof of the first one) Following MaoWao 's suggestion, I think I proved the first one. Firstly let me claim that if $H$ is a Hilbert space, then its corresponding inner product $\langle\cdot, \cdot\rangle_H:H\times H\longrightarrow\mathbb{R}$ is positive semi-definite. Indeed, we have for any $n\in\mathbb{N}$ , $x_1,\ldots, x_n \in H$ and $c_1,\ldots, c_n \in\mathbb{R}$ that $$\sum_{i,j=1}^n c_i c_j \langle x_i,x_j\rangle_H = \left<\sum_{i=1}^n c_i x_i,\sum_{j=1}^n c_j x_j \right>_H =\Big\|\sum_{i=1}^n c_i x_i\Big\|_H^2\geq 0.$$ In fact, the above result also holds for pre-Hilbert space, since the notion of completeness was not involved in the above argument. Thus, we only need to find a specific (pre-)Hilbert Space $H$ and a $h_{t}\in H$ such that $\langle h_t, h_s\rangle_H = t\wedge s$ . But this is easy, let's consider $H:=L^2(\mathbb{R}_{+})$ , and $h_t:=\mathbb{1}_{[0,t]}$ . It is clear that $h_t\in H$ , and for any $t,s\geq 0$ , we have $$\langle h_t, h_s\rangle_{L^2(\mathbb{R}_{+})} = \int_{\mathbb{R}_{+}}\mathbb{1}_{[0,t]} \mathbb{1}_{[0,s]} \, d\mu=t\wedge s,$$ and thus we are done. Edit 2: (Proof of the second one) I've searched all over the places. The function in $(2)$ is Abel kernel, but it is rarely discussed since Abel kernel is closely related to Poisson kernel, and most of the discussions are on the latter. Later, I found a really close one: the Gaussian kernel and here is a link about the proof of Gaussian kernel is really a kernel. That is, it is positive semidefinite. https://stats.stackexchange.com/questions/35634/how-to-prove-that-the-radial-basis-function-is-a-kernel In this link, one answer used the characteristic function. This greatly inspired me. I also found a characteristic function, which is the one for Cauchy distribution. Below is the proof: Recall the Cauchy Distribution $(x_{0},\gamma)$ with $\gamma>0$ has the characteristic function $$\varphi(t)=e^{ix_{0}t-\gamma|t|}.$$ Using this, we can write $c(s,t)=h(s-t)$ where $h(t):=e^{-|t|}=\mathbb{E}e^{itZ}$ is the characteristic function of a random variable $Z$ with Cauchy $(0,1)$ distribution. Then for real numbers $x_{1},\cdots, x_{n}$ and $a_{1},\cdots, a_{n}$ , we have \begin{align*}
\sum_{j,k=1}^{n}a_{j}a_{k}h(x_{j}-x_{k})&=\sum_{j,k=1}^{n}a_{j}a_{k}\mathbb{E}e^{i(x_{j}-x_{k})Z}\\
&=\mathbb{E}\Big(\sum_{j,k=1}^{n}a_{j}e^{ix_{j}Z}a_{k}e^{-ix_{k}Z}\Big)\\
&=\mathbb{E}\Big(\Big|\sum_{j=1}^{n}a_{j}e^{ix_{j}Z}\Big|^{2}\Big)\geq 0.
\end{align*} Thus, $c$ is positive semi-definite. It seems that I did not use the hint at all for part $(2)$ , so I believe there must be another way. I do need someone to check if my proof in the edit 1 and edit 2 is correct. I am gonna open a bounty in 19 hours later, for proof checking and possible new proof. Thank you!","['positive-semidefinite', 'operator-theory', 'stochastic-processes', 'linear-algebra', 'functional-analysis']"
3550604,Find $n$ such that $J=n\mathbb Z$,"Let the ideal $I=(4+3i)\mathbb Z[i]$ and $J=I\cap\mathbb Z$ . Prove that $J$ is an ideal in $\mathbb Z,$ and find $n$ such that $J=n\mathbb Z$ . The first part of the task is to use the definition of an ideal by checking if it meets the appropriate properties. However I don't know what the ideal $J$ looks like because I must find part of the common $\mathbb Z$ and $\mathbb Z[i]$ . 
Also, I don't know exactly how to find $n$ ?","['abstract-algebra', 'ideals']"
3550614,Part of proof of Yoneda's Lemma from Vakil,"I am trying to understand the second half of the proof of Yoneda's Lemma, which is given as a problem in Vakil's notes. So suppose we have two objects $A$ and $A'$ in a Category $D$ , and morphisms $i_C:Mor(C, A) â†’ Mor(C,Aâ€²)$ that commute with the maps $Mor(C,A) â†’ Mor(B,A)$ , which are induced by some morphism $f:Bâ†’C$ . Then I have shown that the $i_C$ (as $C$ ranges over the objects of $D$ ) are induced from a unique morphism $g: A â†’ Aâ€²$ . Specifically, there is a unique morphism $g: A â†’ Aâ€²$ such that for all $C \in D$ , $i_C$ is given by $u\mapsto g\circ u$ . Now, the next part of the problem, labelled (b), asks to show that if all the $i_C$ are bijections, then $g$ is actually an isomorphism. Now, the diagram that was useful for the first part is the following: $$\require{AMScd}
\begin{CD}
\operatorname{Mor}(A,A) @>{i_A}>> \operatorname{Mor}(A,A')\\
@V{}VV @V{}VV \\
\operatorname{Mor}(C,A) @>{i_C}>> \operatorname{Mor}(C,A')
\end{CD}$$ However, I am having trouble seeing how to apply this to (b). I know that $i_A(Id_A)=g\circ Id_A=g$ . Further, since the sets $\operatorname{Mor}(A,A)$ and $\operatorname{Mor}(A,A')$ are in bijection with each other, $g$ maps back to $Id_A$ via postcompositon with some morphism $h:A'\rightarrow A$ , giving $g$ a left inverse. Is this correct? If the right inverse for $g$ can be constructed in a similar manner, does this give the result?","['algebraic-geometry', 'abstract-algebra', 'yoneda-lemma', 'category-theory']"
3550700,Is there a generalized solution to the birthday problem? [duplicate],"This question already has answers here : Probability of 3 people in a room of 30 having the same birthday (9 answers) Closed 4 years ago . The problem of the calculating the probability that there is a birthday shared by at least 2 people in a group of size n is well known. I am wondering if there is a way of finding the probability of there being a birthday shared by m people in a group of size n. I couldn't find any info about this online and was unable to solve it myself. The particular question asked by a friend of mine was on the chances of there being some day which is at least 4 peoples birthday from a group of 50. I was able to get the answer through a monte-carlo simulation, but am still interested in an analytic solution. Edit: I have since solved the problem, using Bernoulli trials, a technique I just learned in my discrete math class. The general formula is $1-\left(\sum_{i=0}^{m-1}\operatorname{nCr}\left(n,i\right)\cdot\frac{1}{365}^{i}\cdot\frac{364}{365}^{\left(50-i\right)}\right)^{365}$ . The part inside the sum is the chance of, for a fixed day, that there are exactly i shared birthdays. The sum finds the chance that there are less than m shared birthdays. Raising this to the power of 365 finds the chance that there are less than m shared birthdays every day. Subtracting this from 1 gets the chance that there are m or more shared birthdays. The answer I got for the specific case agreed with my simulation within +-.000001.","['discrete-mathematics', 'combinatorics', 'birthday', 'probability']"
3550704,Proof of irregularity of an octagon determined by lines from vertices to midpoints of sides of a square,"I stumbled upon this old question ""Area of octagon constructed in a square"" that involves finding the area of an octagon within a square as shown: I found that the octagon is not regular. So, my question is this: Prove that the octagon shown above is not regular, assuming the only length we are initially given is the side length of the square. Note: The edges of the octagon are determined by the line joining the vertices of the square to midpoints of the square's sides. So far, the easiest way I could think of was to use the cosine rule to get the largest angle in $\triangle ABC$ (below) and show that the total of the angles would not be equal to the expected sum of a regular octagon. Are there any other ways of proving that this octagon is not regular?",['geometry']
3550711,Golden Angle Golden Spiral,"Note. The answer must produce something self-similar that progresses in such a manner regardless of whether positive or negative values are graphed. Note. The ""distance between 'arcs'"" I'm talking about is defined in my image (the red dotted line represents such a distance between 'arcs'). The parametric equations, $x,\ y=\sin(t)Â·Ï†^{t/Ï€},\ \cos(t)Â·Ï†^{t/Ï€}$ , yield a spiral where the distance between the 'arcs' grows (/shrinks) gradually reaching powers of $Ï†$ at every multiple of $Ï€$ (every $180$ degrees). ( See My Graph. ) I want to find parametric equations for a spiral where this distance grows (/shrinks) by powers of Ï† in exactly the same way relative to multiples of the golden angle, that is, $\frac{2Ï€}{Ï†}$ ( Instead of doing so relative to Ï€ ). ( $Ï†$ is defined as $Ï†=\left(\frac{1+5^{1/2}}{2}\right)$ herein!) Such a thing seems totally possible; however I have had little luck thus far. So, I'd be thankful for some help! Thanks. Graph: https://www.desmos.com/calculator/0iotz6gmdu","['golden-ratio', 'curves', 'calculus', 'functions', 'trigonometry']"
3550716,Proving recurrence relations using induction,"I'm trying to solve recurrence relations and then prove them via induction.  I'm a bit stuck on this question.  I'm finding it a bit hard to get it around my head for some reason.  The recurrence relation is $T(n) = 2 T(n/2) + 1$ And the formula that I got for it is $T(2^k) = 2^{k+1} -1$ So for my induction, I did this: Claim: $T(2^k) = 2^{k+1} -1$ Base case: $k = 1$ $T(2^1) - 1 = 2 - 1 = 1$ Assuming that it's true for some $k$ so proving for $k+1$ Inductive case: $T(2^{k+1})$ $= 2 T(2^{k+1} /2) + 1$ $= 2 T(2^{k+1-1} ) + 1$ $= 2 T (2^k) + 1$ $= 2 T (2^{k+1} -1) + 1$ (From the claim, -1 & +1 cancel out) $= 2 (2^{k+1} )$ Could anyone tell me whether this would be the correct solution or if I've gone wrong somewhere? I'm a bit unsure on the last step, as I've got $2x (2^k+1)$ which is adding indices so maybe this would  give $2^k+2$ which would be incorrect, but I'm unsure. Thank you.","['induction', 'solution-verification', 'recurrence-relations', 'discrete-mathematics']"
3550741,Algebraic Geometric Analogue of Brown's Representability,"Brown's representability theorem is very usefull to show that the functor $$X \rightarrow H^i(X,A)$$ is representable. I would be interested to see if there exists an analogue of this statement in the context of algebraic geometry, in particular to show that algebraic geometric analogues of Eilenberg-Maclain spaces exist. For example, does there exists an ``l-adic Eilenberg-Maclain space"", i.e. a scheme or stack which represents the functor $$X\rightarrow H_{et}^i(X,\mathbb{Q}_\ell).$$","['algebraic-geometry', 'grothendieck-topologies', 'category-theory', 'algebraic-topology']"
3550832,Why does the curl vector point along the axis of rotation?,"Thanks for reading. Say we have a vector-field $F=P(x,y,z)\hat{i}+Q(x,y,z)\hat{j}+R(x,y,z)\hat{z}$ , and we compute the curl of $F$ at some point $(x,y,z)$ by calculating $\nabla \times F$ at that point. Geometrically, I intuitively understand the components of the resulting vector. The $\hat{i}$ component will be a vector representing the curl of the vector field on the $(y,z)$ plane. The $\hat{j}$ component will be a vector representing the curl of the vector field on the $(x,z)$ plane. The $\hat{k}$ component will be a vector representing the curl of the vector field on the $(x,y)$ plane. Additionally, I geometrically understand each of the resulting ""curls"" independently, and their equations independently (for example, I understand why $(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y})\hat{k}$ gives us the curl on the $xy$ plane at that point). However, sources online (Khan Academy) say that the overall resulting vector points ""orthogonal to the overall direction of rotation at $(x,y,z)$ "". And that...I don't intuitively understand. That is, why is it that the fact that each of the resulting components of $\nabla \times F$ represents the curl on one of the planes at a point $(x,y,z)$ imply that the overall resulting vector points perpendicular to the overall rotational direction at that point (following the right-hand rule)? Thanks! Edit: Take a look at this post from Wikipedia: Suppose the vector field describes the velocity field of a fluid flow
  (such as a large tank of liquid or gas) and a small ball is located
  within the fluid or gas (the centre of the ball being fixed at a
  certain point). If the ball has a rough surface, the fluid flowing
  past it will make it rotate. The rotation axis (oriented according to
  the right hand rule) points in the direction of the curl of the field
  at the centre of the ball, and the angular speed of the rotation is
  half the magnitude of the curl at this point. And then.... The curl of the vector at any point is given by the rotation of an
  infinitesimal area in the $xy$ -plane (for $z$ -axis component of the curl), $zx$ -plane (for $y$ -axis component of the curl) and $yz$ -plane (for $x$ -axis
  component of the curl vector). This can be clearly seen in the
  examples below. In a nutshell, I'm trying to connect the two interpretations. I understand the second one. And I understand the equations for each of the components separately. What I don't yet understand is how it gives us an overall vector pointing along an axis of rotation... Thank you!","['curl', 'geometry', 'multivariable-calculus', 'calculus', 'intuition']"
3550840,Can the absolute Picard functor be trivial?,"Consider $\pi : X\rightarrow S$ a separated scheme of finite type over some base scheme $S$ . The absolute Picard functor $\operatorname{Pic}_X$ from the category of $S$ -schemes to that of abelian groups, is defined by $$\operatorname{Pic}_X(T):=\operatorname{Pic}(X_T)=\{\text{isomorphism classes of invertibles sheaves on } X_T=X\times_ST\}$$ It is claimed in every source I met (eg. on ncatlab , last paragraph of $2$ ) that this functor is never a separated presheaf in the Zariski topology, since given a non trivial invertible sheaf on $X_T$ , it can be made trivial on some Zariski covering $\{T_i\rightarrow T\}$ so that the natural map $\operatorname{Pic}(X_T)\rightarrow \prod_i\operatorname{Pic}(X_{T_i})$ is not injective. But can we be sure that there always exists a non trivial invertible sheaf on $X_T$ for some suitable $S$ -scheme $T$ ? That is, could the Picard functor of some scheme $X$ actually send every $T$ to the trivial group ?","['algebraic-geometry', 'schemes', 'sheaf-theory']"
3550850,"Is $Y_n := \prod_1^n \xi_i$ for $\xi_i$ i.i.d. $\text{Unif}(0,2)$ a sequence of uniformly integrable random variables?","Is $Y_n := \prod_1^n \xi_i$ for $\xi_i$ i.i.d. $\text{Unif}(0,2)$ a sequence of uniformly integrable random variables? I know that $\mathbb E[Y_n]=1$ for all $n$ , and looking at the martingale $\{Y_n, \cal A_n\}$ (for $\mathcal A_n := \sigma[Y_0, \ldots, Y_n], Y_0 := 1$ ), I can use the martingale convergence theorem to get that $Y_n\to_\text{a.s.} Y_\infty$ for some $Y_\infty \in \mathcal L_1$ . Furthermore, from more work with martingales I got that $\mathbb E[Y_n^2]\to \infty$ , and so the variance is going off to $\infty$ . The $Y_n$ are not bounded because if the $\xi_i$ happen to be above $1$ then we're going to shoot up, but at the same time such events are ""rare"", and so my intuition is useless. I'm not sure where to go from here -- this example has truly baffled me.","['martingales', 'uniform-integrability', 'probability-theory', 'random-variables']"
3550903,What is $e^{\scriptscriptstyle i^{e^{i^{.^{.^{.}}}}}}$?,My friend asked me this question as a joke but I'm genuinely curious as to what the answer is. What is $$e^{\scriptscriptstyle i^{e^{i^{.^{.^{.}}}}}}$$ ? My attempt: I first set $z$ equal to $e^{\scriptscriptstyle i^{e^{i^{.^{.^{.}}}}}}$ . $z=e^{\scriptscriptstyle i^{e^{i^{.^{.^{.}}}}}}$ $z=e^{\scriptscriptstyle i^z}$ $z=e^{iz}$ I then used the Euler's Identity $e^{i\pi}=-1$ to find that $e^i=(-1)^\frac{1}{\pi}$ $z=(-1)^{\frac{z}{\pi}}$ $z=i^{\frac{2z}{\pi}}$ I really don't know what to do at this point. Is this even solvable? Thanks in advance.,"['algebra-precalculus', 'complex-numbers', 'recreational-mathematics']"
3551002,Normalize a vector (an array) - divide by total sum or absolute value?,"When I think of normalizing a vector I mean divide each element with the absolute value of the whole vector, i.e. \begin{align}
a &= (2, 4, 3, 1) \\
\hat{a} &= \frac{(2, 4, 3, 1)}{\sqrt{30}} \approx (0.37, 0.73, 0.55, 0.18)
\end{align} However, I'm reading a programming book (Java) and it says: ""You have a sequence of real numbers and want to return a new normalize sequence whose sum is equal to one. This can be done by dividing each number in the sequence with the total sum of the sequence. For example; $(2, 4, 3, 1)$ should return $(0.2, 0.4, 0.3, 0.1)$ since the sum is $2 + 4+ 3 +1 =10$ ."" But why divide with $10$ and not the absolute value? What have I missed? Is there a different terminology for ""normalization"" in mathematics and computer science, respectively? Of is there actually a fundamental difference between a vector, an array and a sequence?","['computational-mathematics', 'statistics', 'absolute-value', 'numerical-methods']"
3551013,A step in proving the graph of a Borel map is Borel,"In the appendix Takesaki's book on theory of operator algebras, he showed that if $f$ is a Borel map from a metric space $X$ to a metric space $Y$ , then the graph of $f$ is a Borel set of $X \times Y$ . An important step in this proof is the following: If $(Z, d)$ is a metric space and $g: Z \to Z\,$ is a Borel map, then $z \mapsto d(z, g(z))$ is a Borel map from $Z$ to $\mathbb{R}$ . I can understand the above statement if $Z$ is separable. However, I fail to see how to prove it in the general case when the metric space $Z$ is not separable. Any help is appreciated.","['measure-theory', 'descriptive-set-theory']"
3551106,Projected Euclidean vs. Geodesic distances of embedded manifolds,"Let $(M,g)$ be a Riemannian manifold embedded into $R^n$ with $g$ induced by the Euclidean distance of $R^n$ (i.e., this is an isometric embedding). Suppose $x, y \in M$ , and consider the Euclidean line segment $c(t) = (1-t)x + t y$ where $t$ is a scalar: $t \in [0..1]$ . Now, define a curve $p(t) \in M$ by projecting $c(t)$ onto $M$ . Thus: $p(t) = Proj_{M} (c(t)) = \arg\min_{p \in M} \|p - c(t)\|_2$ . I'm wondering if there is a relation between the geodesic on $M$ between $x$ and $y$ and the length of the curve $p(t)$ (which must lie on $M$ by definition). Any simplifying assumptions such as $M$ being compact, without boundary and with a positive reach are OK in my context. For example, if $M$ is a sphere, then it seems that $p(t)$ will trace the geodesic between $x$ and $y$ , unless they are on the poles. Is there a characterization of this situation? Is this true locally (e.g., for $y$ in the neighborhood of $x$ )? To me, it seems that $p(t)$ must provide a better estimate of the geodesic, but I cannot find relevant results. Thank you.","['manifolds', 'smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
3551152,Find the Conditional Expectation in this case,"Suppose that the joint density for two random variables is given by $$ f(x,y) =  \frac{e^{-\frac{x}{y}} e^{-y}}{y} $$ where $x,y \in (0, \infty)^2$ . Find the $E(X|Y=y),~$ $\forall y > 0$ . My solution: I would like to find it using the conditional density so I started as $E(X|Y=y) = \int_0^{\infty} y~ f_{X|Y=y}(x,y)dy$ Then, I found the conditional density $f_{X|Y}(x | y)$ for $X$ given that $Y = y,~  \forall y > 0$ . Using the definition, $f_{X|Y}(x | y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}$ . First to find the marginal of $Y$ from the joint as $$f_Y(y)= \int_0^{\infty} \frac{e^{-\frac{x}{y}} e^{-y}}{y} dx = e^{-y}. $$ Following, $$f_{X|Y}(x | y) = \frac{f_{X,Y}(x,y)}{f_Y(y)} = \frac{\frac{e^{-\frac{x}{y}} e^{-y}}{y}}{e^{-y}} = \frac{e^{-\frac{x}{y}} }{y}$$ Finally, $E(X|Y=y) = \int_0^{\infty} y~ f_{X|Y=y}(x,y)dy = \int_0^{\infty} y \frac{e^{-\frac{x}{y}} }{y} dy $ but the integral doesn't converge. What is wrong in my solution? EDIT: I think I did a mistake here $E(X|Y=y) = \int_0^{\infty} y~ f_{X|Y=y}(x,y)dy$ and the it should be as $E(X|Y=y) = \int_0^{\infty} x~ f_{X|Y=y}(x,y)dx = \int_0^{\infty} x \frac{e^{-\frac{x}{y}} }{y} dx = y$ Is this true? Also what is the difference between $E(X/Y)$ and $E(X/Y=y)$ or $f_{X,Y}(x,y)$ and $f_{X,Y=y}(x,y)$ ? thanks for help","['expected-value', 'probability-distributions', 'conditional-expectation', 'probability']"
3551202,Proof that derivative is the best linear approximation?,"I found this answer that stated the following theorem - Theorem: Let ð‘“ be a real valued function defined in a neighbourhood of point ð‘Ž and continuous at ð‘Ž and lets assume that it is approximated by a linear function ð‘” given by ð‘”(ð‘¥)=ð´ð‘¥+ðµ in the neighbourhood of ð‘Ž. Then we say that ð‘” is best linear approximation of ð‘“ in the neighbourhood of ð‘Ž if the following equation holds : $$ \lim_{x\to a}  \frac{f(x)-g(x)}{x-a}=0$$ Such a linear approximation exists if and only if ð‘“â€²(ð‘Ž) exists and moreover in that case we have ð‘”(ð‘¥)=ð‘“(ð‘Ž)+ð‘“â€²(ð‘Ž)(ð‘¥âˆ’ð‘Ž). This answer also uses this theorem to prove that the derivative is truly the best linear approximation. More like this is the 'sense' in which it is the best approximation. After researching online I found that the idea seems to be that the derivative is the only linear approximation for which the approximation error tends to $0$ faster than $ð‘¥-ð‘Ž$ as $ð‘¥â†’ð‘Ž$ , and based on this we call it the best approximation. My question is, how does this actually prove that the derivative will beat any other linear approximation? How does it formally (if possible intuitively also) prove that the derivative is better than all the other approximations.","['multivariable-calculus', 'calculus', 'linear-approximation', 'partial-derivative', 'derivatives']"
3551219,"A nonlinear 4th order ODE, not sure how to solve","Here is a problem I am not sure how the solution is arrived. It would be great if you could show how to obtain the given solution as in the question. Given the ODE, and the boundaries conditions, obtain P(r) in terms of W(r). For those who are interested, the full questions is as such,","['ordinary-differential-equations', 'fluid-dynamics']"
3551232,Figuring out the formula of a recurrence relation,"I've got a recurrence relation which I'm stuck on figuring out the formula for: The relation is: $T(n) = 2 T(n/4) + 1$ where $T(1) = 1$ So for my tests, I did: $T(4) = 2 T(4/4) + 1 = 2 T(1) + 1 = 2\times1+1 = 3$ $T(16) = 2 T(16/4) + 1 = 2 T(4) + 1 = 2\times (2\times1+1) +1 = 7$ $T(64) = 2 T (64/4) + 1 = 2 T(16) + 1 = 2\times ( 2\times (2\times1+1) +1 )+1 = 15$ So I've noticed that by writing out all the multiplications, the number of $2$ 's and number of $1$ 's seems to be equal to the power of $4$ , i.e. at $T(64)$ , equivalent to $4^3$ , there are $3$ lots of $2$ 's and $1$ 's in the multiplication (not including the $*1$ which doesn't change the answer anyway) so I thought that the formula may then be: $T(4^n ) = 2n + 1n$ But when I tried it out it didn't seem to work, I've also noticed the actual answers go up by the answer to the previous relation $ + 4* $ the power of the previous $n$ , e.g. at: $T(16)$ , the answer is $7$ , answer to the previous is $3$ , power of the previous is $1$ $(4^1 ) 3 + 4x1 = 7$ $T(64)$ , the answer is $15$ , answer to the previous is $7$ , power of the previous is $2$ $(4^2 ) 7 + 4*2 = 15$","['recurrence-relations', 'discrete-mathematics']"
3551258,"Is this Function Differentiable? $f(x) = \begin{cases} x^2-4x+5, & \text{if } x\neq 0\\ 3x+5, & \text{if } x=0 \end{cases}$","$$f(x) = \begin{cases} x^2-4x+5,  & \text{if $x\neq0$ } \\ 3x+5, & \text{if $x=0$ } \end{cases}$$ Can we Present this function as a Differentiable Function whose Derivate Function is Not Continues?
I Suppose the Derivate Function would be: $$f'(x) = \begin{cases} 2x-4,  & \text{if $x\neq0$ } \\ 3, & \text{if $x=0$ } \end{cases}$$ Am I Right?
And is this function basically differentiable?
Thanks in Advance.",['derivatives']
3551261,Is there any Bochner Technique type for two forms?,"One of the beautiful classical results in DG is the Bochner Technique. Theorem (Bochner, 1948). If $(M, g)$ is compact and has $\rm Ric\geq 0$ , then every harmonic $1$ -form is parallel. I want to know is there any similar results for two forms together non-negative Ricci curvature and then some estimate for second Betti number (If am right)?","['differential-topology', 'riemannian-geometry', 'differential-geometry']"
3551284,Poker combinatorics,"I have a problem solving the following: Out of a $52$ -card deck, I'm missing all my kings and one card of number $2$ . Now if I pull five cards randomly, what is the chance to get $4$ of a kind? Because I am missing $4$ kings and one card of number $2$ , it remains only $11$ out of $13$ values to get $4$ of a kind from. When we have chosen our value, we can choose the last ( $5^\text{th}$ ) card in $11 \times 43 = 473$ different ways (according to the multiplication principle). $11$ is the values that remain and $43$ is the cards that remain after I have pulled $4$ cards. $47$ cards from the start (because I am missing $5$ cards). 
The total of different poker hands is $\displaystyle \binom{47}{5}$ and the probability to get $4$ of a kind is therefore $$(11 \times 43) \div \dfrac{47 \times 46 \times 45 \times 44 \times 43}{1 \times 2 \times 3 \times 4 \times 5} = \frac{1 \times 2 \times 3 \times 4 \times 5 \times 11}{47 \times 46 \times 45 \times 44} = \frac{1 \times 2 \times 3 \times 4 \times 5 \times 11}{47 \times (2 \times 23) \times (5 \times 3 \times 3) \times (4 \times 11)} = \frac{1}{47 \times 23 \times 3} = 0,0003 = 0,03 \%$$ Is this a correct calculation? Also if I would drop one more card, which card should I drop to maximize my chances so that $5$ randomly cards give $4$ of a kind? Directly, I would say that I could drop one more of the number $2$ because from the begging I only had $3$ cards of the number $2$ and therefore I could not get any $4$ of a kind from number $2$ anyways. Is it correct or should I think otherwise? I am thankful for any tips and advice.","['discrete-mathematics', 'poker', 'combinatorics', 'probability']"
3551304,Counter intuitive Prior/Posterior relationship in Bayesian inference for estimated probability fusion,"I am trying to infer the probability distribution of a binary variable $X$ (True or False) using observations $O = \langle O_1,O_2,\ldots,O_n\rangle$ , mutually independent given X. I also have a ML algorithm that takes an observation $o_i$ and learns to predict a score $s_i$ which, I imagine, is an estimation of the probability of X being True, $\forall i, s_i \approx P(X=T\mid O_i=o_i)$ . Now, I want to compute the probability of $P(X=T\mid O=o)$ since I have multiple observations that should improve the final score by fusing the estimated probabilities given by the ML algorithm.
Using Bayes formula I get: $$P(X=T\mid O=o) = \frac{P(O=o_1,o_2,\ldots,o_n\mid X=T)P(X=T)}{P(O=o_1,o_2,\ldots,o_n\mid X=T)P(X=T)+P(O=o_1,o_2,\ldots,o_t\mid X=F)P(X=F)}$$ And since I can now separate each observation probabilities since they are independent given $X$ , I get: $$P(X=T\mid O=o) = \frac{P(X=T)\displaystyle\prod_{i}^{n}P(O_i=o_i\mid X=T)}{P(X=T)\displaystyle\prod_{i}^{n}P(O_i=o_i\mid X=T)+P(X=F)\displaystyle\prod_{i}^{n}P(O_i=o_i\mid X=F)}$$ But now here comes a problem, I can't put a value on $P(o_i\mid X)$ , I only have $s_i \approx P(X=T\mid O_i=o_i)$ and the priors $P(X)$ , so I use Bayes formula once again on $P(o_i\mid X)$ . It seems to go well, the $P(O_i=o_i)$ are also simplified, but at the end I get this: $$P(X=T\mid O=o) \approx \frac{P(X=T)^{-n+1}{\displaystyle\prod_{i}^{n}s_i}}{P(X=T)^{-n+1}\displaystyle\prod_{i}^{n}{s_i} + P(X=F)^{-n+1}{\displaystyle\prod_{i}^{n}(1-s_i)}}$$ It seems like a nice formula, and has nice properties (for instance, predicted scores of 0.5 are neutral to the final posterior probability, given $n$ is constant or given uninformative priors). Unfortunately, it starts to look very wrong once you play with the prior. If you look closely, the priors $P(X)$ are driving the final probability toward the opposite probability. For instance, with a prior $P(X=T)=0.9$ , the final probability goes towards $0$ which is very weird since it seems to me that priors shouldn't work like this at all. However, the formula seems to work quite well when I set $P(X=T)=P(X=F)=0.5$ and doesn't have the usual downsides of computing the average of the score probabilities. So, my questions are: Is there something wrong with the proof, the assumptions or with the interpretation, or is this a correct formula with prior behaving counter-intuitively? Is there already a proof of a formula for doing probability fusion like this? And also, are there other cases of prior driving the posterior to the opposite probability?","['statistical-inference', 'statistics', 'bayesian', 'solution-verification', 'probability']"
3551326,"generalized derivative of $\log |x|$ (sobolev derivative), where $x\in (-1,1)$","Let $u\in L_{loc}(a,b)$ and $\phi \in C_0^{\infty}$ . Function $v$ is generalized derivative of $u$ , if $$1)v\in L_{loc}(a,b)$$ $$2)\int_{a}^bu(x)\phi'(x)dx=-\int_{a}^bv(x)\phi(x)dx $$ for $\forall \phi \in C_0^{\infty}$ I am trying to find the generalized derivative of $ln|x|$ when $x\in (-1,1)$ . There is one problem is point $0$ . I tried to cut special point by using limit. By definition: \begin{align*}
\int_{-1}^1 \log|x|\phi'(x)dx
    &=\int_{-1}^0 \log(-x)\phi'(x)dx+\int_0^1 \log(x)\phi'(x)dx \\
    &=\lim_{\epsilon\to0}\int_{-1}^{-\epsilon}\log(-x)\phi'(x)dx+\lim_{\delta\to0}\int_{\delta}^1\log(x)\phi'(x)dx \\
    &=\lim_{\epsilon\to0}\left[\log(-x)\phi(x)|_{-1}^{-\epsilon}-\int_{-1}^{-\epsilon}\frac{\phi(x)}{x}dx\right]+\lim_{\delta\to0}\left[\log(x)\phi(x)|_{\delta}^{1}-\int_{\delta}^1\frac{\phi(x)}{x}dx\right] \\
    &=\lim_{\epsilon\to 0}\left[\log(\epsilon)\phi(-\epsilon)-\int_{-1}^{-\epsilon}\frac{\phi(x)}{x}dx\right]+\lim_{\delta\to0}\left[-\log(\delta)\phi(\delta)-\int_{\delta}^1\frac{\phi(x)}{x}dx\right] \\
   &=\lim_{\epsilon\to 0, \delta\to 0}[\log(\epsilon)\phi(-\epsilon)-\log(\delta)\phi(\delta)]-\lim_{\epsilon\to 0, \delta\to 0}\left[\int_{-1}^{-\epsilon}\frac{\phi(x)}{x}dx+\int_{\delta}^1\frac{\phi(x)}{x}dx\right]
\end{align*} For existence the generalized derivative must be $\log(\epsilon)\phi(-\epsilon)-\log(\delta)\phi(\delta) = 0$ and integrals must converge. But $\frac{1}{x}\notin L_{loc}(-1,1)$ and the equality with logarithms is not right for all $\phi$ . Then I conclude that the derivative does not exist. Is it right?","['sobolev-spaces', 'derivatives']"
3551328,Efficient algorithms for detecting finite topologies,"Given a finite collection $X$ of finite sets, what are some efficient methods for deciding whether or not $X$ is a topology? Obviously it is possible to brute-force the problem by checking the conditions $\emptyset\in X$ , and $\bigcup Y\in X$ , $\bigcap Y\in X$ for every $Y\in \mathcal{P}(X)$ ; but this is extremely inefficient - $O\left(2^{|X|}\right)$ , I think - for large $X$ . Is there a faster way to check if $X$ is a topology?","['elementary-set-theory', 'general-topology', 'computational-mathematics', 'discrete-mathematics']"
3551353,Multivariate Application of Slutsky's lemma,"Let $(A_n)_{n \geq 1}$ be a sequence of random matrices in $\mathbb{R}^{d \times d}$ and $(T_n)_{n \geq 1}$ and $(Z_n)_{n \geq 1}$ be  two sequences of random vectors in $\mathbb{R}^{d}$ . We assume that: $\forall n \geq 1$ , $Z_n = A_n T_n$ $A_n$ converges in probability towards a deterministic non singular random matrice $A$ $Z_n$ converges in distribution towards a random vector $Z$ . Show that $T_n$ converges in distribution towards $A^{-1}Z$ . We do not assume that the $A_n$ are non-singular. If the $A_n$ were non-singular it would be easy to say $T_n = A_n^{-1}A_nT_n$ and apply Slutsky's lemma to obtain the result. Without assuming non-singularity but if we could assume $A_n$ nonnegative I would use $T_n = (A_n+ \frac{1}{n})^{-1}(A_n+ \frac{1}{n})T_n$ to show the result. In the general case, I do not know how to prove it. Any hint ?","['matrices', 'linear-algebra', 'probability-theory', 'weak-convergence']"
3551358,Which linear maps $\mathbb{R}^{n^2} \to \mathbb{R}^{n^2} $ map $\text{GL}_n$ into $\text{GL}_n$?,"Can we characterize all linear maps $\mathbb{R}^{n^2} \to \mathbb{R}^{n^2} $ which map $\text{GL}_n$ into $\text{GL}_n$ ? In particular, is it true that every such map is given by $X \to AXB$ or $X \to AX^TB$ where $A,B \in \text{GL}_n$ and $X^T$ denotes the usual transpose operation on matrices. (Note that changing the metric which respect to we are transposing amounts to changing $A,B$ ).","['general-linear-group', 'matrices', 'matrix-calculus', 'linear-algebra', 'lie-groups']"
3551410,Umbral calculus - eigenfunctions of operator,"I'm very new to umbral caluclus and I have come across a paper that makes use of some results in this area, which I do not quite understand. The problem I have is the following. Consider the following operator \begin{equation}
\mathcal{S} = a^{-1}(bI\Delta_{-1} + c\Delta_{1}), \quad I \in \mathbb{Z^+}
\end{equation} where $\Delta_h[f(I)] =f(I+h)-f(I), h \in \mathbb{Z}$ It is claimed by the paper that given that $\Delta_{1}(I)_m=m(I)_{m-1}$ and $I\Delta_{-1}(I)_m = -m(I)_m$ [where with $(I)_m$ we denote the falling factorial] the eigenfunctions $\psi(I)$ of the operator are computable as: \begin{equation}
\psi_n(I) = \sum_{m=0}^{n} {n \choose m} \left(-\frac{c}{b}\right)^m(I)_{n-m}
\end{equation} and the eigenvalues \begin{equation}
\lambda_n=-n\frac{b}{a}
\end{equation} Note that $\mathcal{S}(I)_m = -m\frac{b}{a}[(I)_m - \frac{c}{b}(I)_{m-1}]$ How is this caluculation performed? I have looked at Rota's book but I see no reference to the computation of the eigenfunctions. Can anybody point me out in the right direction? Thank you all in advance","['calculus', 'operator-theory', 'umbral-calculus', 'eigenfunctions']"
3551445,The volume as a probability measure: Stuck in the proof,"I have troubles proving the fact that if we define $Vol(A)=\idotsint 1_{A}(x_1,\dots,x_n) \,dx_1 \dots dx_n$ and $\mathbb{P}(B)=\frac{Vol(A\bigcap B)}{Vol(A)}$ then $\mathbb{P}(\bigcup\limits_{n=1}^{\infty} B_{n})= \sum_{n=1}^{\infty} P(B_{k})$ if $B_{1}, B_{2}\dots$ are disjoint. I'm starting with multivariable calculus, so I don't know how to formalize this. I know that this happens because the function $1_{A}=1$ iff what we want to integrate is part of the set, and it makes sense that is the sum because we are not ""counting twice"" the same points, because the fact that our $B_{k}$ sets are disjoints, but as I already said, I don't know how to work with integrals in $\mathbb{R}^{n}$ so it is hard to me to write this proof in a formal way. I will appreciate any help","['measure-theory', 'real-analysis', 'multivariable-calculus', 'probability-theory', 'probability']"
3551452,Understanding the $L^{2}-$Limit of Gaussian Random Vectors,"I am working on an exercise stating as follows: Suppose the process $X_{t}$ is a Gaussian process, and let $H$ be the Hilbert space generated by $(X_{t})_{t\in\mathbb{R}}$ , that is, the space consisting of $L^{2}-$ limits of linear combinations of values of $X_{t}$ .  Prove that every element in $H$ is a Gaussian random variable. The solution of the exercise said that: It suffices to prove the $L^{2}-$ limit of Gaussian random vectors is still a Gaussian random vector. That is, if $\vec{X}_{t}$ , $t=1,2,\cdots$ is a sequence of Gaussian random vectors with mean $\vec{\mu}_{t}$ and covariance matrix $\Sigma_{t}$ such that $\vec{X}_{t}\longrightarrow \vec{X}_{\infty}$ in $L^{2}$ , then $\vec{X}_{\infty}$ is a Gaussian random vector. Then it proved as follows: Recall that the convergence in $L^{2}$ of $\vec{X}_{t}$ to $\vec{X}_{\infty}$ implies $\vec{\mu}_{t}=\mathbb{E}\vec{X}_{t}$ converges to $\vec{\mu}_{\infty}=\mathbb{E}\vec{X}_{\infty}$ , and the
  element-wise convergence of covariance matrices $\Sigma_{t}$ to the
  corresponding covariance matrix $\Sigma_{\infty}$ . Further, the $L^{2}-$ convergence implies the corresponding convergence in
  probability and hence by the bounded convergence we have $$\varphi_{\vec{X}_{t}}(\vec{\theta})\longrightarrow
> \varphi_{\vec{X}_{\infty}}(\vec{\theta})\ \text{for each}\
 \vec{\theta}\in\mathbb{R}^{d}.$$ Since $\varphi_{\vec{X}_{t}}(\theta)=e^{i<\vec{\theta},\vec{\mu}_{t}>}e^{-\frac{1}{2}<\vec{\theta},
> \Sigma_{t}\vec{\theta}>}$ for any $t<\infty$ , it follows that the same
  applies for $t=\infty$ , it follows that the same applies for $t=\infty$ . It is a well-known fact of linear algebra that the element-wise limit $\Sigma_{\infty}$ of positive semi-definite matrices $\Sigma_{n}$ is
  necessarily also positive semi-definite. Thus, in view of the definition of Gaussian random vector using
  characteristic function, we see that the limit $\vec{X}_{\infty}$ is a
  Gaussian random vector, whose parameters are the limits of the
  corresponding parameters of $\vec{X}_{t}$ . I have three questions about this solution: Firstly, why is showing the $L^{2}-$ limit of Gaussian random vectors still being a Gaussian random vector sufficient? Is that because then we know that $<b,\vec{X}_{\infty}>$ is a Guassian random variable for all $b$ ? but where is $b$ living in? $\mathbb{R}^{\infty}$ ? Secondly, in the proof, why $\varphi_{X_{t}}(\vec{\theta})\longrightarrow \varphi_{X_{\infty}}(\vec{\theta})$ for all $\vec{\theta}\in\mathbb{R}^{d}$ ? if we take the limit, should it be $\mathbb{R}^{\infty}$ ? I believe the first and the second questions are both around one point: how to understand the limit of Gaussian vectors: so this proof basically shows that the ""entires"" of the Guassian vector $\vec{X}_{t}$ is not increasing. It is always a $\mathbb{R}^{d}-$ valued random vector. But why and how? If we increase the limit, why does not the entries change? so the limit is not from $\vec{X}_{t}:=(Y_{1},\cdots, X_{t})\longrightarrow(Y_{1},\cdots, Y_{\infty})=\vec{X}_{\infty}$ ? How to understand the Gaussian random vectors and Gaussian process? Thank you!","['stochastic-analysis', 'proof-explanation', 'stochastic-processes', 'probability-theory', 'stochastic-calculus']"
3551475,The distribution for the sum of sequences of independent random variable,"$(X_n)_{n \geq 1}$ and $(Y_n )_{n \geq 1}$ are two sequences of independent random variables with a value in $\{Â 0,1\}$ . Suppose that the random variables are mutually independent and $\forall n \geq 1, ~ p(X_n=1) = p$ and $ P (Y_n = 1) = q~$ where $p, q  \in (0, 1).$ Define $S_n = \sum_{k=1}^n X_k$ , $T_n = \sum_{k=1}^n X_k Y_k$ and $N = \inf\{n \geq 0, T_{n+1} = 1\}.$ Find the distribution of $S_n,~ T_n$ and $N$ . I don't have an idea how to find the distribution of $T_n$ or $N$ . For $S_n$ : Thanks to the comment of NCh , I modified the answer for $S_n$ as $S_n = X_1+ \dots + X_n$ where each $X_i$ takes the value $0$ with probability $1-p$ and $1$ with probability $p$ . Define the value $1$ as success while $0$ failure, then $ S_n = \sum_{i=1}^n X_i$ represents the number of successes which follows the binomial distribution $\text{Binomial}(n,p)$ . Could you please help and tell me what to do with $T_n$ and $N$","['probability-distributions', 'probability-theory', 'probability']"
3551485,"Behavior of $I(n) = \int_{-\pi}^{\pi}\int_{-\pi}^{\pi}e^{ i n (x+y)}\,\frac{2\sin^2(x)\sin^2(y)}{2k -\cos(x)-\cos( y)}\,\mathrm{d}x\,\mathrm{d}y$","Consider the following integral: $$
I(n) = \int_{-\pi}^{\pi}\int_{-\pi}^{\pi}e^{ i n (x+y)}\,\frac{2\sin^2(x)\sin^2(y)}{2k -\cos(x)-\cos( y)}\,\mathrm{d}x\,\mathrm{d}y,
$$ where $k>1$ is a real number and $n>0$ is an integer number. My question. Is it possible to characterize the asymptotic behavior of $I(n)$ as a function of $n$ ? Numerics suggests that $I(n)\sim \frac{1}{\sqrt n}\left(k-\sqrt{k^2-1}\right)^{2n}$ , however I have not yet been able to formally prove this fact. This is not an homework question. Any help is welcome! Thanks in advance! Progress. I briefly sketch my attempt to solve the problem, hoping that someone may find this useful. Note that $g(x,y)$ is analytic in a neighborhood of $\mathbb{R}^2/\mathbb{Z}$ . Thus, we can shift the integral in the complex direction $it$ as long as $g$ remains analytic. This is indeed the case for all $$
t < t_0 :=  \cosh^{-1} k
    =  \ln  \left(k + \sqrt{k^2-1}\right)= -\frac1{2\pi} \ln  \left(k - \sqrt{k^2-1}\right).
$$ Thus, by letting $t=t_0$ , we get \begin{align*}
I(n) &= e^{-2 n t_0}\int_{-\pi}^{\pi}\int_{-\pi}^{\pi}e^{ in(x+y)}\,g(x+it_0,y+it_0)\,\mathrm{d}x\,\mathrm{d}y\\
&= \left(k-\sqrt{k^2-1}\right)^{2n} I'(n),
\end{align*} where \begin{align*}
I'(n) &= \int_{-\pi}^{\pi}\int_{-\pi}^{\pi}e^{ in(x+y)}\,g(x+it_0,y+it_0)\,\mathrm{d}x\,\mathrm{d}y\\ &=\int_{-\pi}^{\pi}\int_{-\pi}^{\pi}\frac{e^{i n (x+y)}}{K}\frac{(K^2(2-K^2e^{-2i x})-e^{-2ix})(K^2(2-K^2e^{-2i y})-e^{-2 iy}) }{K(4k-K(e^{- i x}+e^{- i y}))-e^{ i x}-e^{ i y}}\,\mathrm{d}x\,\mathrm{d}y\\
\end{align*} where $K:=k+\sqrt{k^2-1}$ . Although the integrand of $I'(n)$ blows up at $x=0$ , $y=0$ , $I'(n)$ still converges absolutely, and I think it should be possible to prove that $I'(n)$ exhibits subexponential decay. As a further observation, note that by using the change of variables $z=e^{i x}$ , $t=e^{iy}$ , $I'(n)$ can be rewritten as $$
I'(n):=\oint_{|z|=1} \oint_{|t|=1} \frac{z^{n-2}t^{n-2}\left(2K^2z^2-K^4-z^4\right)\left(2K^2t^2-K^4-t^4\right)}{K\left(4kzt-K(z+t)\right)-(z+t)zt}\,\mathrm{d}z\,\mathrm{d}t.
$$ Perhaps, this integral could be handled more easily using contour integration techniques; however it looks like pretty messy. I'm currently stuck here.","['integration', 'fourier-analysis', 'definite-integrals', 'analysis']"
3551521,"For $n\in\mathbb{N}^+$, $\Re(s)>0$, evaluate $\int_{0}^{\infty}\sin\left(2\pi ne^{x}\right)\left[\frac{s}{e^{sx}-1}-\frac{1}{x}\right]dx$","Let $n$ be a positive integer, and $s\in \mathbb{C}\;,\Re(s)>0$ . I want to compute the integral : $$\int_{0}^{\infty}\sin\left(2\pi ne^{x}\right)\left[\frac{s}{e^{sx}-1}-\frac{1}{x}\right]dx$$ I tried using the integral presentation found here : $$\sin(2\pi ne^{x})=\frac{\sqrt{\pi}}{2\pi i }\int_{\gamma-i\infty}^{\gamma+i\infty}\frac{\Gamma(z)}{\Gamma\left(\frac{3}{2}-z \right )}\left(\pi n \right )^{-2z+1}e^{-(2z-1)x}dz\;\;\;\;\;0<\gamma<1$$ in conjunction with : $$r+\log(r)+\psi\left(\frac{1}{r}\right)=-\int_{0}^{\infty}e^{-y}\left(\frac{r}{e^{ry}-1}-\frac{1}{y}\right)dy$$ Where $\psi\left(\cdot\right)$ is the digamma function. But that made the problem even more difficult. Any help is highly appreciated. EDIT : Using the Fourier reciprocity : $$\frac{s}{e^{sx}-1}-\frac{1}{x}=2\int_{0}^{\infty}\left(\frac{1}{e^{2\pi \theta/s}-1}-\frac{s}{2\pi \theta} \right )\sin(x\theta)d\theta$$ Our integral reads : $$2\int_{0}^{\infty} g(\theta)\left(\frac{1}{e^{2\pi \theta/s}-1}-\frac{s}{2\pi \theta} \right )d\theta$$ Where : $$g(\theta)=\int_{0}^{\infty}\sin(2\pi ne^{x})\sin(x\theta)dx$$ $$=\frac{\sqrt{\pi}}{2\pi i }\int_{\gamma-i\infty}^{\gamma+i\infty}\frac{\Gamma(z)}{\Gamma\left(\frac{3}{2}-z \right )}\left(\pi n \right )^{-2z+1}\frac{\theta}{\theta^{2}+(2z-1)^{2}}dz$$","['complex-analysis', 'residue-calculus', 'definite-integrals', 'real-analysis']"
3551568,Maximal possible value of $|f(z)|$ given an upper bound of its value on the unit circle $|f(e^{i\theta})|\leq g(\theta)$,"Let $f(z)$ be a function that is complex analytic in an open region containing the unit disk. Suppose we are given an upper bound of $|f(z)|$ on the unit circle, i.e. $|f(e^{i\theta})|\leq g(\theta), 0\leq\theta\leq 2\pi$ . Then what is the maximal possible value of $|f(0)|$ ? I feel this should be related to Cauchy's integral technique, since we have $$f(0)=\frac{1}{2\pi i}\oint_{|z|=1}f(z)\frac{h(z)}{z}dz, $$ where $h(z)$ is analytic in the closed unit disk and $h(0)=1$ . Therefore, $$|f(0)|\leq \max_{0\leq\theta\leq2\pi}g(\theta)|h(e^{i\theta})|.$$ Hopefully by choosing an optimal $h(z)$ we could find a tight upper bound of $|f(0)|$ . Is this a well-researched problem? More generally, if we are given an upper bound of the absolute value of an analytic function $f(z)$ in a region or a closed curve, how could we get an optimal upper bound on $|f(z)|$ in the interior of this region or curve? Any useful hints or references are highly welcomed. Thanks in advance!","['complex-analysis', 'optimization', 'contour-integration']"
3551594,Does the Frobenius method work for all second order linear differential equations with only regular singular points?,"For $$x^2y''+x(x^2+1)y'+(x-4)y=0,\tag1$$ there is a regular singular point at $0$ , but when I tried to use the Frobenius method and substituted $$y=\sum_{n=0}^\infty a_n x^\left(n+r\right)\tag2$$ into the differential equation, I obtained \begin{multline}
(r^2-4)a_0x^r+[(r^2+2r+5)a_1+a_0]x^\left(r+1\right)
\\+\sum_{n=0}^\infty\left\{[(n+r+2)^2+2]a_\left(n+2\right)+a_\left(n+1\right)+(n+2)a_n\right\}=0\tag3
\end{multline} I understand that if all $x$ in an interval satisfy the differential equation, then all the coefficients must be zero. But then this means $r=\pm2$ (from the first term), and certainly the second term will not be satisfied by arbitrary $a_0$ and $a_1$ (the initial conditions). Therefore, my question is does the Frobenius method only work for certain second order linear differential equations with only regular singular points, like where $p(x)$ and $q(x)$ in $~x^2y''+p(x)y'+q(x)y=0~$ are first or second degree polynomials?","['frobenius-method', 'ordinary-differential-equations']"
3551630,"Solving a differential-initial value equation: $y'(x) = \frac{y}{x+y^3}$, $ y(2)=2$","The equation and given values are $$
\begin{split}
\frac{dy}{dx} &= \frac{y}{x+y^3}\\
y(2) &= 2
\end{split}
$$ At first I thought it was a separation of variables problem, but I then was told later on by a tutor that it could be solved with substitution. 
I then subbed-in with the following variables: $$u+x\frac{du}{dx}=\frac{ux}{x+(ux)^3}$$ Then subtracted the $u$ from both sides and multiplied by the denominator: $$x\frac{du}{dx}=\frac{ux-u(x+(ux)^3)}{x+(ux)^3}$$ $$x\frac{du}{dx}=\frac{ux-ux-u^4x^3}{x+u^3x^3}$$ $$x\frac{du}{dx}=\frac{-u^4x^3}{x+u^3x^3}$$ Now If I factor out an x from the quotient $$x\frac{du}{dx}=-\frac{x(u^4x^2)}{x(1+u^3x^3)}$$ $$x\frac{du}{dx}=-\frac{u^4x^2}{1+u^3x^3}$$ Now can I further simplify the quotient till I can integrate or should I go back and do something different to make it easier.","['initial-value-problems', 'calculus', 'ordinary-differential-equations']"
3551654,Does $a^b=a^c$ imply $b^a=c^a$?,"Does $a^b=a^c$ imply $b^a=c^a$ ? I can't prove it mathematically. Can anyone show me how to prove this mathematically (if it's true), or disprove it? (Note: I will delete if duplicate but I couldn't find any) My attempt: $a^b=a^c$ $a^{c-b}=1$ That means that either $a=1$ or $c-b=0$ $c-b=0 \implies b=c$ If $a=1$ then $c-b=0 \implies b=c$ is not necessarily true so $a=1$ is a counterexample for $b \ne c$ . Is this right? Is there another way to do it? Thanks in advance.","['algebra-precalculus', 'recreational-mathematics']"
3551731,Direct Product vs. Cartesian Product,"I'm currently reading through an introduction to topology book in which the first chapter is an overview of set theory. In this chapter, the Cartesian Product of two sets: $$A \times B $$ is discussed, which seems relatively straightforward. The author then goes on to say that one can generalize the Cartesian Product to any number of sets with the Direct Product: $$\prod_{i=1}^n A_i = A_1 \times A_2 \hspace{1mm} \times \hspace{1mm} ... \hspace{1mm} \times \hspace{1mm} A_n $$ Which, again, at face value seems relatively straightforward. However, I feel as though there's much more to the direct product than it just being an extension of the Cartesian Product. This being said, what is the difference between the two? Is it true that the Cartesian Product is really the special case of the Direct Product when $n=2$ ? I would really appreciate it if anyone can help me out! Thanks!","['elementary-set-theory', 'direct-product', 'terminology']"
3551745,Proving $(\sin^2 \alpha+\sin\alpha \cos \alpha)^{\sin \alpha}(\cos^2 \alpha+\sin \alpha \cos \alpha)^{\cos \alpha}\leq 1$,"If $\alpha \in \left(0, \frac{\pi}{2}\right)$ , prove that: $$(\sin^2 \alpha+\sin\alpha \cos \alpha)^{\sin \alpha}(\cos^2 \alpha+\sin \alpha \cos \alpha)^{\cos \alpha}\leq 1$$ I know $\sin \alpha$ and $\cos \alpha$ are positive over $\alpha \in \left(0, \frac{\pi}{2}\right)$ , so with $a=\sin \alpha,\ b=\cos \alpha$ , $a>0,\ b>0$ and $a^2+b^2=1$ , the inequality is $$(a^2+ab)^a(b^2+ab)^b \leq 1$$ $$\Leftarrow a^ab^b(a+b)^{a+b} \leq 1$$ and here I don't know how to prove this inequality.","['trigonometry', 'inequality']"
3551773,Why is $\int_{-\infty}^{+\infty} (e^{itx}-1-\frac{itx}{1+x^2})\frac{1}{\pi x^2} dx = -|t|?$,I'm reading a paper and it is claimed that for every $t\in \mathbb{R}$ $$\int_{-\infty}^{+\infty} (e^{itx}-1-\frac{itx}{1+x^2})\frac{1}{\pi x^2} dx = -|t|$$ Note that there is no problem at $0$ since we can give the integrand a value at $0$ such that this expression becomes continuous. How can I see that this equality is true? Do I need complex analysis to see this?,"['integration', 'complex-analysis']"
3551774,the partial sum of a series of functions?,"For the function $f_n(x) = x e^{-nx^2}\bigl((n+1) e^{-x^2}-n\bigr),~~ \forall n \geq 0$ and $\forall x \in [0,1]$ . We define the partial sum as $N_n(x) = \sum_{i=0}^n f_i(x)$ I'm trying to find the partial sum in order to compare $$ \lim_{n \rightarrow \infty } \int_0^1 N_n(x) dx~~ \text{and}  \int_0^1 \lim_{n \rightarrow \infty } N_n(x) dx $$ The problem is that I can't simplify and find the partial sum. I tried to simplify it as \begin{eqnarray}
N_n(x) = \sum_{i=0}^n f_i(x) & = & \sum_{i=0}^n x e^{-ix^2}\bigl((i+1) e^{-x^2}-i\bigr) \nonumber  \\
& = & \sum_{i=0}^n \bigl[ x(i+1) e^{-x^2(1+i)} - ixe^{-ix^2}\bigr] \nonumber \\
& = & x e^{-x^2}\sum_{i=0}^n  (i+1) e^{-ix^2} - x \sum_{i=0}^n ie^{-ix^2} \nonumber \\
\end{eqnarray} How would I continue to find the partial sum? Also, what is the idea of the question? i.e what is the idea of comparing th limit of integral to the integral of the limit ? Update: using the telescoping from the hint in the answer below, the sum equals $(n+1) x e^{-(n+1)x^2}$ . For the other part of my question regarding the comparison , I found $$ \lim_{n \rightarrow \infty } \int_0^1 N_n(x) dx =  \frac{1}{2}\lim_{n \rightarrow \infty } \bigl(1- e^{-(n+1)} \bigr) = \frac{1}{2}$$ and the other one is $$ \int_0^1 \bigl( \lim_{n \rightarrow \infty }  N_n(x) \bigr) dx =  \int_0^1 \bigl( \lim_{n \rightarrow \infty }  (n+1) x e^{-(n+1)x^2} \bigr)dx  = \int_0^1 0dx = 0 $$ SO WHAT? What is the idea of a similar comparison generally ?","['calculus', 'sequences-and-series', 'real-analysis']"
3551807,The set $5^{-\infty}\mathbb{Z}$ is a colimit,"I am trying to understand why the set of rational numbers whose denominators are powers of $5$ , $5^{-\infty}\mathbb{Z}$ , is a colimit. Specifically, why is $5^{-\infty}\mathbb{Z}$ the colimit of the diagram $$\mathbb{Z}\longrightarrow5^{-1}\mathbb{Z}\longrightarrow5^{-2}\mathbb{Z}\longrightarrow\cdots$$ I'm not sure what the maps to $5^{-\infty}\mathbb{Z}$ are. They can't be inclusion, or else we won't get commutativity. What am I missing? I imagine the arrows above are given by division by $5$ , and maybe the maps to the limit are multiplication by $5$ ?","['abstract-algebra', 'limits-colimits', 'category-theory']"
3551823,Let $\lim_{n\to\infty}(a_n)=\infty$ and $(b_n)_{n\in\mathbb N}$ be bounded. Prove that $\lim_{n\to\infty}(a_n+b_n)=\infty$.,"Question: Let $(a_n)_{n\in\mathbb N}$ and $(b_n)_{n\in\mathbb N}$ be sequences with real values. Let $\lim_{n\to\infty}(a_n)=\infty$ and $(b_n)_{n\in\mathbb N}$ be bounded. Prove that $\lim_{n\to\infty}(a_n+b_n)=\infty$ . Proof: Since we know: $\lim_{n\to\infty}(a_n)=\infty\space$ $\Leftrightarrow\space$ $\forall K\in\mathbb R^+ \space \space\exists N_1\in\mathbb N $ : $n\gt N_1 \space$ $\Rightarrow a_n\gt 2K.$ and $(b_n)_{n\in\mathbb N}$ is bounded $\Leftrightarrow\space$ $\exists K\in\mathbb R^+ \space $ such that $\space \forall n\in\mathbb N \space$ $ |b_n|\le K.$ Now taking $N=N_1$ : $\Rightarrow$ $a_n\gt 2K$ and $|b_n|\le K$ $\Leftrightarrow$ $2K\lt a_n$ and $-K\le b_n \le K$ $\Rightarrow$ $a_n+b_n \gt 2K-K=K$ $\space \forall n\gt N$ Hence $\lim_{n\to\infty}(a_n+b_n)=\infty$ This is an optional question from my Analysis I course, thought it would be a fun little proof. Be great if anyone could check what I have done -struggling to get to grips with these types of proofs.","['analysis', 'real-analysis', 'alternative-proof', 'solution-verification', 'limits']"
3551825,Using convolution formula to find PMF and then to show negative binomial distribution,"Let $X$ and $Y$ be independent random variables taking only integer values. Let $Z=X+Y$ , which also takes only integer values. Its PMF can be computed by the convolution formula: for any integer $z$ , \begin{align}
\ P_Z(z) & = P(Z=z) =P(X+Y=z) \\
 & = \sum_{x=-\infty}^{\infty} P(X=x,X+Y=z)\\ 
 & = \sum_{x=-\infty}^{\infty} P(X=x,Y=z-x) \\
 & = \sum_{x=-\infty}^{\infty} P(X=x)P(Y=z-x) \\ 
 & = \sum_{x=-\infty}^{\infty} P_X(x) P_Y(z-x)
\end{align} Question: Let $X$ and $Y$ be independent and have geometric distribution with parameter $p$ . By computing the PMF, show $X+Y$ has a negative binomial distribution with parameters $r=2$ and $p$ . Geometric distribution is given by $f(x)=(1-p)^{x-1} p$ and negative binomial distribution is $P(X=x|r,p)={x-1 \choose r-1}p^r (1-p)^y$ . I would like to know how to calculate the convolution formula for PMF. Any help is appreciated.","['statistics', 'probability-distributions', 'probability']"
3551861,Multinomial Distribution -- How to calculate percentiles?,"I've read the rules and searched but I do not even know what I'm looking for.
Here is my problem: Suppose I have a bag containing three different marbles: red, green, and blue. I am drawing a single marble from the bag each time with replacement. I would like to know how many times, on average, do I need to draw marbles from the bag until I have drawn $25$ of each type of marble. I would like to plot this distribution and be able to calculate percentiles on this distribution. After some research I think this is a multinomial distribution. I can calculate the probability that I have exactly $25$ of each marble after $75$ draws by the following: $$\frac{75!}{25!\times25!\times25!}\times\left(\frac13\right)^{25}\times \left(\frac13\right)^{25} \times\left(\frac13\right)^{25}$$ which works out to about $1.06\%$ . However, I don't know how to proceed with turning this into a distribution so I can calculate percentiles. Please advise on how to proceed.","['probability-distributions', 'coupon-collector', 'multinomial-coefficients', 'multinomial-distribution', 'probability']"
3551881,Do the elements of a set have to be unique?,"Does the mathematical definition of a set specify/imply that its elements be unique? For context, this question has arisen in my mind from my experience using the Python programming language where one of the properties of the 'Set' data structure is that its elements are necessarily unique. Prior to this I had done a maths degree, but don't recall this being highlighted when learning about [mathematical] sets; certainly not with the emphasis it is given in Python. Moreover, I feel like I came across questions or problems where sets would sometimes have repeated elements (perhaps problems in probability or combinatorics), but I may be mis-remembering this or it may have simply been an abuse of the notation. But an example might be: What is the probability that the sum of two numbers, one each drawn randomly from the sets $A = \{1, 2, 2, 3, 3, 3\}$ and $B = \{1, 2, 3, 4\}$ is at least 6? (where the desired answer is 8/24, rather than 3/12) The Set Theory Wikipedia page does not use the term ""unique"" or ""distinct"" in reference to set elements. I came across this Stack Overflow question , but it's obviously geared heavily towards programming, so it's hard to know if that answers are really about the mathematical concept rather than programming data structures. It does mention that a set where repeated elements are allowed is called a Multiset, and in making this distinction, the Wikipedia page for Multiset does assert that a set is only allowed a single instance of an element. But it goes on to say that the term Multiset was only coined in the 1970s, so I'm left wondering what Mathematicians did before then if they wanted collections of objects with duplicates?",['elementary-set-theory']
3552057,"Dividing 12 people into any number of groups, such that person A and B are not in the same group?","In how many ways can you divide 12 people into any number of groups, such that person A and B are not in the same group? I am trying to solve this question and so far I am thinking of this in terms of Stirling numbers of the second kind. The way I am thinking of this problem is as follows: Calculate the ways to partition 12 in to any number of blocks (greater than 2 since we can't have 1 group as that group must contain A & B together) $$ \sum_{i = 2}^{12} S(12, i) = 4,213,596$$ and then subtract that from the possible ways of partitioning 11 people in to any number of blocks (here I am making A & B the same person AB so that they are always in the same group) giving the following: $$\sum_{i = 1}^{11} S(11,i) = 678,570$$ subtracting from each other I get: $$3,535,026$$ Which seems quite high in my opinion. Is this the correct approach or am I missing something?","['bell-numbers', 'combinatorics', 'stirling-numbers', 'set-partition']"
3552102,"if $a+b>c+d$ and $c<a<b<d$, then $ab > cd$","show that if $a+b>c+d$ and $c<a<b<d$ , then $ab > cd$ .
I was wondering if there is a simple way to prove this statement or a counter example.",['algebra-precalculus']
3552117,Prove $\sum_{n\ge1}\frac{1}{q^n+q^{-n}}=\tfrac14(\vartheta_3^2(q)-1)$,"Prove that $$\sum_{n\ge1}\frac{1}{q^n+q^{-n}}=\tfrac14(\vartheta_3^2(q)-1),$$ provided by Wolfam . Note that here, we use the notational conventions $$\vartheta_3(z,q)=\sum_{n\in\Bbb Z}q^{n^2}e^{2niz},$$ $$\vartheta_3(q)\equiv \vartheta_3(0,q),$$ and of course $\vartheta_3^2(q)=\vartheta_3(q)\cdot\vartheta_3(q)$ . I have gotten a significant portion of the way. We have that $$f(q)=\sum_{n\ge1}\frac{1}{q^n+q^{-n}}=L(q,-1;q^2),$$ where $$L(a,b;q)=\sum_{n\ge1}\frac{a^n}{1-bq^n}\qquad |q|>1.$$ It can be shown, for sufficient $a$ and $b$ , that $$L(a,b;q)=L(b^{-1},a^{-1};q).$$ This is the case here, so we have $$f(q)=L(-1,q^{-1};q^2),$$ which is $$f(q)=L(1,q^{-2};q^4)-L(1,q^{-3};q^4),$$ by splitting the sum up into parts of even and odd index. This may be evaluated in terms of the $q$ -digamma function $\psi_q(s)$ as $$f(q)=\tfrac1{4\ln q}\left(\psi_{q^{-4}}(\tfrac34)-\psi_{q^{-4}}(\tfrac14)\right).$$ This is $$4f(q)\ln q=\frac{\partial}{\partial s}\ln\left[\Gamma_{q^{-4}}(\tfrac12+s)\Gamma_{q^{-4}}(\tfrac12-s)\right]\bigg|_{s=1/4}.$$ Then from here we can show that $$\left(\Gamma_{q^{-4}}(\tfrac12+s)\Gamma_{q^{-4}}(\tfrac12-s)\right)^{-1}=\frac{q^{4s^2+3}}{(q^4;q^4)_\infty^3 (1-q^4)}\vartheta_4(-2is\ln q, q^2),$$ but I have no idea about how the logarithmic derivative (w.r.t $s$ ) of this has anything to do with $\vartheta_3^2$ . Could I have some help? Thanks.","['analytic-number-theory', 'theta-functions', 'special-functions', 'sequences-and-series']"
3552129,Do finitely generated nilpotent groups contain torsion free subgroups of finite index?,"I have a question about the proof of proposition 6.9 of the paper ""Rational Subgroups of Biautomatic Groups"" by Gersten and Short (available here ). The proposition states that a finitely presented nilpotent subgroup $H$ of a biautomatic group contains an abelian subgroups of finite index. In the second line of the proof the authors claim that they can pass to a torsion free subgroup of $H$ of finite index and then proceed to prove the result for finitely generated, torsion free nilpotent subgroups. They do not justify why they can always find such a finite index subgroup. I know that in finitely generated nilpotent groups the torsion subgroup $T$ is always finite and that $H/T$ is torsion free but this is not what the authors are claiming. So my question is: if $H$ is a finitely generated nilpotent group, does there exist a subgroup $H'$ such that $H'$ is torsion free and $[H:H']<\infty$ ?","['geometric-group-theory', 'group-theory', 'nilpotent-groups']"
3552194,On the alternating quadratic Euler sum $\sum_{n = 1}^\infty \frac{(-1)^n H_n H_{2n}}{n^2}$,"My question is: Can a closed-form expression for the following alternating quadratic Euler sum be found? Here $H_n$ denotes the $n$ th harmonic number $\sum_{k = 1}^n 1/k$ . $$S = \sum_{n = 1}^\infty \frac{(-1)^n H_n H_{2n}}{n^2}$$ What I have managed to do so far is to convert $S$ to two rather difficult integrals as follows. Starting with the result $$\frac{H_{2n}}{2n} = -\int_0^1 x^{2n - 1} \ln (1 - x) \, dx \tag1$$ Multiplying (1) by $(-1)^n H_n/n$ then summing the result from $n = 1$ to $\infty$ gives $$S = -2 \int_0^1 \frac{\ln (1 - x)}{x} \sum_{n = 1}^\infty \frac{(-1)^n H_n}{n} x^{2n}. \tag2$$ From the following generating function for the harmonic numbers $$\sum_{n = 1}^\infty \frac{H_n x^n}{n} = \frac{1}{2} \ln^2 (1 - x) + \operatorname{Li}_2 (x),$$ replacing $x$ with $-x^2$ leads to $$\sum_{n = 1}^\infty \frac{(-1)^n H_n}{n} x^{2n} = \frac{1}{2} \ln^2 (1 + x^2) + \operatorname{Li}_2 (-x^2).$$ Substituting this result into (2) yields $$S = -2 \int_0^1 \frac{\ln (1 - x) \operatorname{Li}_2 (-x^2)}{x} \, dx - \int_0^1 \frac{\ln (1 - x) \ln^2 (1 + x^2)}{x} \, dx,$$ or, after integrating the first of the integrals by parts twice $$S = -\frac{5}{2} \zeta (4) + 4 \zeta (3) \ln 2 - 8 \int_0^1 \frac{x \operatorname{Li}_3 (x)}{1 + x^2} \, dx - \int_0^1 \frac{\ln (1 - x) \ln^2 (1 + x^2)}{x} \, dx. \tag3$$ I have a slim hope the first of these integrals can be found (I cannot find it). As for the second of the integrals, it is proving to be a little difficult. Can someone find each of the integrals appearing in (3)? Or perhaps an alternative approach to the sum will deliver the closed-form I seek, I am fine either way. Update Thanks to Ali Shather, the first of the integrals can be found. Here \begin{align}
\int_0^1 \frac{\ln (1 - x) \operatorname{Li}_2 (-x^2)}{x} \ dx &=\sum_{n=1}^\infty\frac{(-1)^n}{n^2}\int_0^1 x^{2n-1}\ln(1-x)\ dx\\
&= -\sum_{n=1}^\infty\frac{(-1)^nH_{2n}}{2n^3}\\
&=-4\sum_{n=1}^\infty\frac{(-1)^nH_{2n}}{(2n)^3}\\
&=-4 \operatorname{Re} \sum_{n=1}^\infty i^n\frac{H_n}{n^3}.
\end{align} And using the result I calculated here , namely $$\operatorname{Re} \sum_{n=1}^\infty i^n\frac{H_n}{n^3} = \frac{5}{8} \operatorname{Li}_4 \left (\frac{1}{2} \right ) - \frac{195}{256} \zeta (4) + \frac{5}{192} \ln^4 2 - \frac{5}{32} \zeta (2) \ln^2 2 + \frac{35}{64} \zeta (3) \ln 2,$$ gives \begin{align}
\int_0^1 \frac{\ln (1 - x) \operatorname{Li}_2 (-x^2)}{x} \, dx &= -\frac{5}{2} \operatorname{Li}_4 \left (\frac{1}{2} \right ) + \frac{195}{64} \zeta (4) - \frac{5}{48} \ln^4 2\\
& \qquad + \frac{5}{8} \zeta (2) \ln^2 2 - \frac{35}{16} \zeta (3) \ln 2.
\end{align}","['integration', 'definite-integrals', 'euler-sums', 'harmonic-numbers', 'closed-form']"
3552263,Prove $v$ is not surjective,"Let $ v$ : $\mathbb R \rightarrow \mathbb R^2$ be a differentiable function such that the velocity vector $\cfrac{dv}{
dt}\neq 0$ at all $t\in \mathbb R$ . Prove
that $v$ is not surjective. I tried to work with definition and tried to prove contrapositive statement but It get me nowhere. Could anyone give me a hint to start with (not solution).","['multivariable-calculus', 'differential-topology']"
3552270,What exactly is an identity morphism?,"I come from a Physics/CS background and I just picked up a book on Category Theory for programmers. The first chapters talks about considering the $\leq$ as a morphism and seeing if we have a category. I get the first requirement of associativity, but I'm having trouble with the identity morphism requirement. As I understand it, the identity morphism must satisfy $f\circ id_{A} = f$ and $id_{B}\circ f = f$ for some $f::A\rightarrow B$ . I'm confused here because it seems that $id$ simply returns its input so wouldn't $id$ always exist? My textbook says that because ""every object is less than or equal to itself"" then we have an identity morphism. I don't follow the reasoning here; why does the fact that $a\leq a$ being always true imply an identity morphism?","['functions', 'category-theory']"
3552410,Balls of minimal variance,"Consider a metric space $(X, \rho)$ and let's say that $\mu$ is a Borel $\sigma$ -finite measure on this space. For every measurable set $A$ its variance is defined as $$
V_{\mu, \rho}(A) := \inf_{a'\in A}\int\limits_{A}\rho(a,a')\mu(\mathrm da').
$$ Here if minimum is attained at some $a'$ we can think of the latter as a mass center of $A$ with respect to $\mu$ and $\rho$ . Let's call a finite measure set $A$ good if there's no set with the at least that measure and lower variance. That is, $A$ is good iff $$
V_{\mu,\rho}(A) = \inf_{A':\mu(A') \geq \mu(A)}V_{\mu, \rho}(A').
$$ Intuitively it seems that if $\mu$ and $\rho$ are consistent in some way, then $A$ is good iff it is a $\rho$ -ball a.e. that is there exists $x\in X, r\geq 0$ such that $\mu(A\triangle B_\rho(x,r)) = 0$ where $\triangle$ denotes the symmetric difference between two sets. Are there any results of the latter kind known? In particular, let's say $X = \Bbb R^2$ endowed with the Euclidean metric $\rho$ and Lebesgue measure $\mu$ . Some particular questions would be Is that true that every ball is good? Are there any good sets that are not balls a.e.? What if $\rho(a,a')$ is a $q$ -norm for $q\neq 2$ ? What if $\rho$ is a Euclidean metric, but $\mu$ has a density of a standard normal random variable?","['measure-theory', 'metric-spaces']"
3552415,For what values of $a\in \mathbb{R}$ is the integral $\int\int_{\mathbb{R}^2} \frac{1}{(1+x^4+y^4)^a}dxdy$ convergent?,"Well, first notice that the above integral equals to $2\int_{0}^\infty \int_0^\infty \frac{1}{(1+x^4+y^4)^a}dxdy$ since the integrand is an even function of both $x$ and $y$ . There are two approaches which I thought to do here. Change of variables: $x^2 = r\cos \theta , y^2 = r\sin \theta$ ; which I am not sure it's valid since there are value of theta which give the rhs to be negative, and we are in real calculus. The legitimate approach is to break the integral into: $$\int_0^1 \int_0^1+\int_0^1\int_1^\infty + \int_1^\infty\int_0^1+\int_1^\infty \int_1^\infty$$ I thought to compare the integrand with $x^2+y^2$ , i.e when $x,y \in [0,1]$ we know that $x^2 \ge x^4$ and when $x>1$ then the opposite follows. Seems a bit long calculation. Can anyone help me with this? Thanks!","['integration', 'multivariable-calculus', 'improper-integrals']"
3552435,Projective $n$ - space is not affine over any ring $R$,"A similar question has been asked here already, but there was no final answer to the problem in the most general case. I wish to show that: For $n>1$ and a ring $R$ , the projective $n$ - space $\mathbb{P}_R^n$ is not affine unless $R=0$ . What I have so far: Assume $\mathbb{P}_R^n$ was affine, then $\mathbb{P}_R^n\simeq \operatorname{Spec}(R)$ . Now by construction $\mathbb{P}_R^n$ contains the affine subspace $\mathbb{A}_R^n\simeq \operatorname{Spec}(R[\frac{t_1}{t_0},...,\frac{t_1}{t_0}])$ as an open subscheme. Hence the inclusion $\mathbb{A}_R^n\hookrightarrow\mathbb{P}_R^n$ induces some ring homomorphism $R\rightarrow R[\frac{t_1}{t_0},...,\frac{t_1}{t_0}]$ . And that is about it... The book I read (Bosch, Algebraic Geometry and Commutative Algebra) uses for the case that $R=K$ is a field an argument based on $K'$ - valued points $\mathbb{P}_K^n = \operatorname{Hom}_K(\operatorname{Spec}(K'),\mathbb{P}_K^n)$ , where $K'$ is field extension of $K$ . He shows that if $\mathbb{P}_K^n = \operatorname{Spec}(K)$ was affine, it would be a one - point space and then constructs a bijection $\mathbb{P}_K^n(K')\leftrightarrow \mathbb{P}^n(K')$ , where the RHS is the ordinary projective $n$ - space over $K'$ . I was hoping to argue similarly, but I am lost at this point.","['algebraic-geometry', 'projective-schemes', 'schemes', 'projective-space']"
3552438,Likelihood Ratio Test for the Normal Distribution with unknown mean,"Let $(X_{1},...,X_{n})$ a $n$ sample of the law $N( \mu, \sigma^{2})$ .
We assumed we don't know $\mu$ and $\sigma^{2}$ . Let $\mu_{0} \in \mathbb{R}$ . Show that the Likelihood-ratio test for $\mu = \mu_{0}$ against $\mu
 \ne \mu_{0}$ is function of $$ 1 + \frac{(\bar{X_{n}} - \mu_{0}
 )^{2}}{\sigma_{n}^{2}} $$ with $\sigma_{n}^{2}  = \sum_{i=1}^{n}
 \frac{(X_{i} - \bar{X_{n}})^{2}}{n}$ EDIT : I showed that the Likelihood-ratio test is $$
\exp\left( \frac{n}{2 s_{n}^{2}(X)} (\bar{X_{n}} - \mu_{0} ) \right)
$$ with $s_{n}^{2}(X) = \sum_{i=1}^n\frac{(X_i-\mu_0)^2}{n}$ But I don't know how to conclude. Thanks and regards.","['statistical-inference', 'statistics', 'hypothesis-testing']"
3552451,Minimum amount of different tests to be made (combination problem),"A history professor who teaches three sections of the same course every semester decides to make several tests and use them for the next 10 years (20 semesters) as final exams. The professor has two policies: not to give the same test to more than one class in a semester, and not to repeat the same combination of three tests for any two semesters. Determine the minimum number of different tests that the professor should prepare. The author says the answer is ${x\choose 3}=
20$ , which is the solution of the equation $x(x âˆ’ 1)(x âˆ’ 2) = 120$ and its solution is $x = 6$ . My thinking was this: Given that there are 3 sections per semester, there are 3 final exams per semester and $3\cdot 20=60$ finals exams throughout $20$ semesters. Therefore we should be solving ${x\choose 3}=60$ not ${x\choose 3}=20$ . Am I missing something? Any help would be greatly appreaciated :)","['combinations', 'combinatorics', 'discrete-mathematics']"
3552533,Show that no non-constant polynomial can generate only prime numbers,"This problem is taken from ""Mathematics for Computer Science"" (Lehman, Leighton, Meyer, 2018). Problem For $n = 40$ , the value of the polynomial $p(n) := n^2 + n + 41$ is not prime, as noted in Section 1.1. But we could have predicted based on general principles that no non-constant polynomial can generate only prime numbers. In particular, let $q(n)$ be a polynomial with integer coefficients, and let $c:=q(0)$ be the constant term of $q$ . (a) Verify that $q(cm)$ is a multiple of $c$ for all $m \in \mathbb{Z}$ . (b) Show that if $q$ is nonconstant and $c > 1$ , then as $n$ ranges over the nonnegative integers $\mathbb{N}$ there are infinitely many $q(n) \in \mathbb{Z}$ that are not primes. Hint : You may assume the familiar fact that the magnitude of any nonconstant polynomial $q(n)$ grows unboundedly as $n$ grows. (c) Conclude that for every nonconstant polynomial $q$ there must be an $n \in \mathbb{N}$ such that $q(n)$ is not prime. Hint : Only one easy case remains. Solution attempt (a) The polynomial can be expressed as $q(n) = c + a_1n + a_2n^2 + \cdots + a_kn^k$ . So, $q(cm) = c + a_1cm + a_2c^2m^2 + ... + a_kc^km^k$ . Since all terms of $q(cm)$ are divisible by $c$ , $q(cm)$ is a multiple of $c$ for all $m \in \mathbb{Z}$ . (b) As $n$ ranges over the nonnegative integers, it will range over infinitely many values of the form $n=cm$ ( $m \in \mathbb{Z}$ ). As shown in (a), for each $n=cm$ , $q(cm)$ is a multiple of $c$ . Therefore, assuming that the magnitude of $q(n)$ grows unboundedly as $n$ grows, this means that $q(n)$ will take infinitely many non-prime values. (c) Item (b) covered the cases where $c > 1$ . For nonconstant $q$ , two cases remain: $c < -1$ and $-1 \leq c \leq 1$ . For $c < -1$ , a similar argument to (b) applies: as $n$ ranges over the negative integers, it will range over infinitely many values of the form $n=cm$ (where $m$ is a negative integer). For each of these values, $q(n)$ is a multiple of $c$ . Therefore, assuming that the magnitude of $q(n)$ grows unboundedly as $n$ grows, this means that $q(n)$ will take infinitely many non-prime values. For $c -1 \leq c \leq 1$ , $q(0) = c$ is an example of root that is not prime. Therefore, for every nonconstant polynomial $q$ , there must be an $n \in \mathbb{N}$ such that $q(n)$ is not prime. Is this proof correct? Thank you in advance.","['proof-writing', 'solution-verification', 'polynomials', 'discrete-mathematics', 'prime-numbers']"
3552599,Finite equational basis for trigonometric identities,"Consider the structure $(\mathbb{R}, +,-,*,\sin,\cos,0,1)$ , where $+$ is addition, $-$ is additive inverse, $*$ is multiplication, $\sin$ is the sine function, and $\cos$ is the cosine function. Is there a finite basis for the equational identities of that structure? In fact, I conjecture that, in addition to the axioms of a commutative ring, all you need are that $\sin(0)=0$ , $\cos(0)=1$ , $\sin(-x)=-\sin(x)$ , $\cos(-x)=\cos(x)$ , the sine of sum formula, the cosine of sum formula, and $\sin^2(x)+\cos^2(x)=1$ .","['universal-algebra', 'trigonometry', 'model-theory']"
3552915,"Determine the point on the plane $4x-2y+z=1$ that is closest to the point $(-2, -1, 5)$","Determine the point on the plane $4x-2y+z=1$ that is closest to the point $(-2, -1, 5)$ . This question is from Pauls's Online Math Notes. He starts by defining a distance function: $z = 1 - 4x + 2y$ $d(x, y) = \sqrt{(x + 2)^2 + (y + 1)^2 + (-4 -4x + 2y)^2}$ However, at this point, to make the calculus simpler he finds the partial derivatives of $d^2$ instead of $d$ . Why does this give you the same answer?","['partial-derivative', 'multivariable-calculus']"
3552928,Find $\lim_{x\to0} \frac{\log(1+3x)}{f(x)}$ given $\lim_{x\to0} \frac{f(x)}{\sin(x)} = 2$,It's assumed that $\lim_{x\to0} \frac{f(x)}{\sin(x)} = 2$ . Find $$\lim_{x\to0} \frac{\log(1+3x)}{f(x)}$$ I don't think that it would work out by a random plugging. Let $f(x) = 2\sin(x).$ $$\lim_{x\to0}\frac{\log(1+3x)}{f(x)} = \lim_{x\to0}\frac{\log(1+3x)}{2\sin(x)} = [\log(1+3x) = 3x + O(x^2)] = \lim_{x\to0}\frac{3x+O(x^2)}{2\sin(x)} = \frac{3}{2}.$$ But what did I miss? I cannot find a way to prove whether these two limits are connected (or that the result is unique).,"['limits', 'calculus']"
3552976,Exterior covariant derivative of connection 1-forms,"I defined the connection 1-form on a principal bundle $\mathcal{P}=(P,M,\pi;G)$ as the $\mathfrak{g}$ -valued 1-form $ \bar{\omega_{p}}=\left( \theta^{A}_{(L)}(p)+\mathrm{Ad}^{A}_{B}(g^{-1})\omega^{B}_{\mu}\mathrm{d}x^{\mu} \right) \otimes T_{A} $ where $\mathfrak{g}$ is the Lie algebra of the group G with basis $\{ T_{A} \}$ , $\theta^{A}_{(L)}=L^{A}_{a}(g)\mathrm{d}g^{a}$ is the local basis of left-invariant 1-forms on the bundle and $p=[x,g]_{(\alpha)}$ is a point on the bundle with respect to a local trivialization.
Then, I defined the exterior covariant derivative (ECD) of $\mathfrak{g}$ -valued $k$ -forms with respect of the connection $\omega$ as the $\mathfrak{g}$ -valued $(k+1)$ -form given by: $ \mathrm{D}_{\omega}\theta \left( \Xi^{(1)},...,\Xi^{(k+1)}\right) = \mathrm{d}\theta\left(\Xi^{(1)}_{(H)},...,\Xi^{(k+1)}_{(H)}\right)$ where $\Xi^{(1)}_{(H)},...,\Xi^{(k+1)}_{(H)}$ are the horizontal parts of the vector fields $\Xi^{(1)},...,\Xi^{(k+1)}$ with respect of said connection. This means that in order to find the ECD of a form, one should first compute the usual differential of said form, and then contract it with the horizontal parts of $k+1$ vector fields via the connection. My goal is to calculate the exterior covariant derivative of the connection 1-form $\bar{\omega}$ , which according to the book I'm using as reference (""Natural and Gauge Natural Formalism for Classical Field Theories"" by Lorenzo Fatibene and Mauro Francaviglia), it should have this form: $
\mathrm{D}_{\omega}\bar{\omega}=\frac{1}{2}\mathrm{Ad}^{A}_{D}(g^{-1})\left( \partial_{\mu}\omega^{D}_{\nu} - \partial_{\nu}\omega^{D}_{\mu}+c^{D}_{\cdot BC}\omega^{B}_{\mu}\omega^{C}_{\nu}\right)\mathrm{d}x^{\mu}\wedge\mathrm{d}x^{\nu}\otimes T_{A}
$ and it is referred to as the curvature 2-form, where $c^{D}_{\cdot BC}$ is the structure costant of the Lie algebra $\mathfrak{g}$ given by $-c^{D}_{\cdot BC} \rho_{D} =[\rho_{B},\rho_{C}]$ , with $\rho_{A}$ being a right-invariant field on the principal bundle. The book omits many details and I have tried countless times to compute the ECD, but I eventually got stuck everytime, and I don't know what to do. I know that you can use known formulas to relate the differential of the adjoint matrix Ad to structure constants, but they still don't get me anywhere.
I very much hope somebody else has gone through this already and can give me a hand, or at least some hints on how to structure my computation.","['lie-algebras', 'principal-bundles', 'connections', 'differential-forms', 'differential-geometry']"
3552985,The conditional probability when some random variables are independent,"For $n \geq 1$ ,  let $X_1, X_2, \dots, X_n$ be a sequence of independent random variables each with a value equals $1$ or $0$ with probability $p$ and $q$ , respectively. If $N = \inf \{n \geq 0, Z_{n+1} = 1 \}$ such that $Z_n = \sum_{i=1}^n X_i Y_i$ and $Y_1, \dots, Y_n$ is a sequence of random variables defined exactly as the first sequence above ""sequence of $X_i$ "" and independent of it. Show that: $1.~~ p(\cap_{i=1}^n  (X_i=x_i) /N=n) = \prod_{i=1}^n p(X_i=x_i /N=n).$ $2.~~ \forall i \in[1,n],~~p(X_i = 1/ N=n) = p(X_i=1 /X_iY_i=0) = \frac{p(1-q)}{1-pq}.$ $\Longrightarrow$ I tried with the first part as \begin{eqnarray}
p(\cap_{i=1}^n  (X_i=x_i) /N=n) &=& p((X_1=x_1 \cap X_2=x_2 \dots \cap X_n=x_n)/N=n)\\
&=& \frac{p((X_1=x_1 \cap X_2=x_2 \dots \cap X_n=x_n) \cap N=n)}{p(N=n)}\\
\text{after that I should say }\\
&=& p((X_1=x_1) /N=n) p((X_2=x_2) /N=n) \dots p((X_n=x_n) /N=n)\\
&=&  \prod_{i=1}^n p(X_i=x_i /N=n)
\end{eqnarray} But I don't know how to get this. I know that the sequence is independent but I don't see how I can use this property to arrive to the last two lines. $\Longrightarrow$ For the second part, we know $N \sim Geometric(pq)$ , $Z_n \sim \text{Binomial}(n,pq)$ and $p(X_i =1)=p$ then \begin{eqnarray}
p(X_i = 1/ N=n) &=& \frac{p(X_i=1 \cap N=n)}{p(N=n)}\\
&=& \vdots
\end{eqnarray} and I don't know also how to continue from here. I really appreciate any help or hint","['probability-distributions', 'probability']"
3553079,Does every topological manifold admit a good or sufficient cover?,"A topological manifold (without boundary) is a locally euclidean Hausdorff space, which is second countable (or maybe only paracompact, that is to say make the definition as strict as you like). A good open cover of a space $X$ is an open cover $(U_i)_i$ , where every finite intersection $U_{i_1} \cap ... \cap U_{i_k}$ is either empty or contractible. I define a sufficient open cover to be a cover, where every finite intersection is either empty or a disjoint union of contractible spaces. Does every topological $n$ -manifold admit a good open cover or a sufficient open cover? All the information I could find about good covers of manifolds involved smooth or differentiable manifolds. On ncatlab it is only stated that the methods involved for proving the existence of good covers in the smooth case don't work in the topological case. No reference to a counterexample or other disprove is given. Note that I am not really (just sort of) interested in the cover being finite. As explained in this answer every $n$ -manifold admits a cover by $n+1$ charts. However there is no reason, why this cover should possibly be a good cover. The closest reference for finite good covers is (again according to ncatlab ) this paper by Osborn Stern. However the connectivity conditions confuse me and I don't quite understand how they replace the contractibility condition. My goal is to show in an axiomatic way (see my question here ) that for every closed $n$ -manifold the homology groups of order $>n$ vanish. I think the existence of a (possibly infinite) sufficient cover would open a way for showing this by induction on the (by compactness) finite number of sufficient opens involved. Of cause, the general existence of a finite sufficient cover would (if my inductive argument is correct) imply that for arbitrary topological $n$ -manifolds the higher homology groups vanish... I would not be surprised, if there is a good reason, why there is no good or sufficient cover in general. However, as I am fairly new to the subject of topological manifolds, I remain hopeful that there is one. Thank you for your time and efforts.","['manifolds', 'general-topology', 'algebraic-topology']"
3553094,All straight curves are geodesics imply Euclidean metric,"Let $g_{ij}$ be a Riemannian Metric in $\Bbb R^n$ with the following properties: $g_{ij}(0)=\delta_{ij}$ , where $ \delta_{ij} =
\begin{cases}
1,  & \text{ $i=j$} \\
0, & \text{ $i \neq j$}
\end{cases}$ For every $q\in \Bbb R^n$ and $1\leq i \leq n,$ the curve $\gamma_i(t)= q + te_i$ is a geodesic of $g$ , where $e_i = (\delta_{i1}, ... , \delta_{in}).$ Prove that $g$ is the Euclidean metric. What I tried: since the above curves are geodesics, I know that $|\gamma_i'(t)|_g^2=\langle e_i,e_i \rangle _g=g_{ii}\;$ is constant, and thus $g_{ii} = 1$ for every $1\leq i \leq n$ . I'm having trouble showing that $g_{ij}=0$ for $j \neq i$ . Any ideas?","['isometry', 'geodesic', 'differential-geometry']"
3553138,Name of group of order $p-\bigl(\frac ap\bigr)$ constructed from field $\Bbb Z_p$?,"Let $p$ be an odd prime, and $a$ be an element of field $\Bbb Z_p$ . Define $l$ as the Legendre symbol $\displaystyle\biggl(\frac ap\biggr)$ . When $l=+1$ , define $b$ as a particular solution of $b^2=a$ in $\Bbb Z_p$ , e.g. the odd one in range $[1,p)$ . Define the set $\mathcal S$ as: $$\mathcal S=\begin{cases}
\{\infty\}\cup\Bbb Z_p&\text{when }l=-1\\
\{\infty\}\cup\Bbb Z_p-\{0\}&\text{when }l=0\\
\{\infty\}\cup\Bbb Z_p-\{b,\,p-b\}&\text{when }l=+1
\end{cases}$$ Define internal law $\boxplus$ in $\mathcal S$ as: $$x\boxplus y=\begin{cases}
y&\text{when }x=\infty\\
x&\text{when }x\ne\infty\text{ and }y=\infty\\
\infty&\text{when }x\ne\infty\text{ and }y=-x\\
(x+y)^{-1}(x\,y+a)&\text{(computed in $\Bbb Z_p$) otherwise}
\end{cases}$$ $(S,\boxplus)$ is a finite Abelian groupÂ¹ of order $p-l$ . What's its name in standard literature? Â¹ The Abelian group axioms hold: Closure of $\boxplus$ follows from definitions, trivially for the case $l=-1$ , and with some level of care in the other ones. The neutral is $\infty$ . This follows from the first three cases of the definition of $\boxplus$ . The opposite $-x$ is computed as in $\Bbb Z_p$ when $x\ne\infty$ , and we define $-\infty$ as $\infty$ . That $x\boxplus(-x)=\infty=(-x)\boxplus x$ follows from the first and third cases of the definition. Associativity $(x\boxplus y)\boxplus z=x\boxplus(y\boxplus z)$ needs to be proved in multiple cases: When any of $x$ , $y$ or $z$ is $\infty$ , we use the first two cases of the definition. Otherwise, when $y$ is $-x$ or $-z$ , we use the first three cases of the definition. In the remaining general case, it boils down to basic algebra. The exclusions in the definition of $\mathcal S$ prevent degenerate cases. Commutativity is easy.","['legendre-symbol', 'finite-groups', 'definition', 'group-theory', 'terminology']"
3553143,Prove that a set is a subset using element method of proof,"$(A\cup B)-(B\cap C) \subseteq(A-B)\cup(B-C)$ Suppose $x \in(A\cup B)-(B\cap C)$ , then $(x\in A \lor x\in B) \land (x\notin B \lor x\notin C) \equiv$ $\equiv\bigl((x\in A \lor x\in B)\land x\notin B\bigr)\lor\bigl((x\in A \lor x\in B)\land x\notin C\bigr)\equiv$ $\equiv\bigl((x\notin B \land x\in A) \lor (x\notin B \land x\in B) \bigr)\lor\bigl((x\notin C \land x\in A) \lor (x\notin C \land x\in B) \bigr)$ Consider the following cases: $(x\notin B \land x\in A) \iff x\in A-B \therefore x \in (A-B)\cup(B-C)$ $(x\notin B \land x\in B)$ , that's not possible $(x\notin C \land x\in A)$ , not sure how to deduce that $x \in (A-B)\cup(B-C)$ $(x\notin C \land x\in B) \iff x \in B - C \therefore x \in (A-B)\cup(B-C)$ Have I divided the proof into cases correctly? If so, how to finish the case #3?","['elementary-set-theory', 'discrete-mathematics']"
3553182,Prove a combinatorial identity with a combinatorial argument [duplicate],"This question already has answers here : Give a combinatorial proof: $ n(n+1)2^{n-2} = \sum_{k=1}^{n}k^2\binom{n}{k} $ (3 answers) Closed 4 years ago . $$\sum_{k=1}^{n}{k^2}\cdot\dbinom{n}{k} = n(n+1)2^{n-2}$$ Attempt: I have re-written in it as follows: $$\sum_{k=1}^{n}\dbinom{k}{1}\dbinom{k}{1}\dbinom{n}{k} = \dbinom{n}{1}\dbinom{n+1}{1}  2^{n-2}$$ the LHS seems like a case where a committee of k people is to be formed from n people. We choose a president in k ways. Then, we choose a secretary in k ways again. Which means that the president could be chosen as a secretary as well? (probably not the best example). 
How can I use the RHS to show the same thing?","['combinations', 'combinatorics', 'combinatorial-proofs', 'discrete-mathematics']"
3553188,Solving $15^{\log_5(3)}\cdot x^{\log_5(9x)+1}=1$,I need help solving this logarithm exercise: $$15^{\log_5(3)}\cdot x^{\log_5(9x)+1}=1$$ What I've done is re-writing the equation $$\Rightarrow \qquad 5^{\log_5(3)}\cdot 3^{\log_5(3)}\cdot x^{\log_5(9x)+1}=1 \tag{1}$$ Then applying logarithms on both sides $$\Rightarrow \qquad \log_5(3^{\log_5(3)}\cdot5^{\log_5(3)}\cdot x^{log_5(9x)+1})=\log_5(1) \tag{2}$$ re-writing the equation a little bit $$\Rightarrow \qquad \log_53+\log_53^{\log_5(3)}+\log_5x^{\log_5(9x)+1}=\log_55 \tag{3}$$ But then I'm not entirely sure how to proceed,"['algebra-precalculus', 'logarithms']"
3553204,Morphisms between moduli spaces,"Assume that I have two moduli spaces $M_1,M_2$ solving two problems $F_1,F_2:C\to Sets$ , think for example about $C=Sch/S$ . 
Furthermore I have a morphism $f:M_1\to M_2$ or equivalently a natural transformation $\alpha_f:F_1\to F_2$ . I would like to know how to relate geometric properties of $f$ (e.g. being etale,smooth,open,closed...) with the abstract properties of $\alpha_f$ . For example take the case where $\alpha_f$ ""forgets"" some part of the moduli problem; even more concretely assume $F_1(X)=\{A/X$ an abelian variety with some extra structure 1 and extra structure 2 $\}$ and $F_2(X)=\{A/X$ an abelian variety with extra structure 1 $\}$ , then I have the natural transformation $\alpha_f:F_1\to F_2$ corresponding to forgetting the second extra structure. In this example what can I conclude about the corresponding morphism $f:M_1\to M_2$ (and why)? My intuition tells me that it should be open, but I can't find a rigorous argument for that.","['algebraic-geometry', 'moduli-space', 'reference-request']"
3553209,Have I incorrectly used conditional expectation here?,"Let $N$ be a Poisson random variable with parameter $\lambda$ . If the parameter $\lambda$ is not fixed, but an exponential random variable with parameter $1$ , find $E[N]$ . Here is a correct solution given to this problem : $E[N] = E[E[N | \Lambda]] = \int_{0}^{\infty} E[N|\Lambda = \lambda] \cdot f_{\Lambda}(\lambda)d\lambda = \int_{0}^{\infty} \lambda \cdot e^{-\lambda} = \Gamma(2) = 1$ , where $\Lambda$ represents an exponential random variable with parameter $1$ . Here is my proposed solution : $E[N] = E[E[N | \Lambda = \lambda]] = E[\lambda] = 1$ . Basically, I don't see why the integration in the correct solution is necessary - we already have the necessary knowledge about the desired expectations here, since a Poisson random variable with parameter $\lambda$ has expectation $\lambda$ , and an exponential random variable with parameter $\theta$ has expectation $\frac{1}{\theta}$ . Have I violated the theory of conditional expectation here ? Thanks!","['conditional-expectation', 'expected-value', 'probability-theory', 'probability', 'random-variables']"
3553218,Open sets as countable unions of open balls in a nonseparable space,"Let $(M, d)$ be a non-separable metric space. Denote by $B = B(x, \epsilon)$ a generic open ball of $M$ . Is there any condition on $M$ such that the following claim is true: Claim : Given any open set $A$ of $M$ , there is a countable family of open balls $B_i^A = B(x_i^A, \epsilon_i^A)$ , $i \in \mathbb{N}$ , such that $$
A = \bigcup_{i\in \mathbb{N}} B_i^A \qquad ?
$$ Of course when the space is separable this is true, for separability is equivalent to second countability for metric spaces. That is why I am concerned with non-separable spaces. Also, notice that I am not asking for a countable basis for the topology, for that would be asking for separability. Instead, I am asking for a ""locally countable basis"", that is, conditions under which any open set can be covered by a countable family of open balls, the family changing according to the open set in question.","['general-topology', 'metric-spaces', 'measure-theory']"
3553247,"How do we make the ""intuitive calculation"" of conditional probability, rigorous?","The definition of Conditional Probability for events $A$ and $B$ in sample space $S$ is $$\mathbb{P}(A|B)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}.$$ Sometimes, we use a rearranged version of this formula to calculate the probability of the intersection of events - called the multiplicative law of probability: $$\mathbb{P}(A\cap B)=\mathbb{P}(A|B)\times \mathbb{P}(B)$$ When using this formula, how does one calculate $\mathbb{P}(A|B)?$ Since by definition the intersection is required to find the conditional probability? Is there an alternative definition/way to compute the conditional probability when you don't know the intersection? I have calculated the conditional probability through intuition many times (E.g. picking Red/Blue marbles out of a bag, without replacement), but I was wondering if there was some sort of standard convention on how calculate the conditional probability when you don't know the intersection? Example. Say we have three people (Alex, Bob, Carol) with their three hats. Say I take all their hats, mix them up, and then return one to each person. What is the probability that person A and B get exactly their own hat back? ""Solution"" : The way I would think of it is: Let $E_A$ and $E_B$ be the events that Alex and Bob get their hats back respectively. Then, $$\mathbb{P}(E_A\cap E_B)= \mathbb{P}(E_B)\times \mathbb{P}(E_A|E_B)$$ The probability of $E_B$ would be $\frac{1}{3}$ . Now, the way I would calculate $\mathbb{P}(E_A|E_B) $ intuitively , even though I don't know what the intersection is (because that's what I'm trying to find), is ""Since Bob has his hat, I have two hats left, which gives a probability of $\frac{1}{2}$ for Alex to get his hat back."" This intuitive logic of getting to the conditional probability directly, when I didn't use/bypassed the definition, is what I would like to clarify/formalise.","['conditional-probability', 'definition', 'probability']"
3553273,"What is exactly spectral expansion, and how is it related to Fourier?","In the theory of automorphic forms, we often refer to the decompositions we write as to ""spectral expansions"". I would like to understand better how this is related to spectral theory of ""relevant"" operators. Let's begin with Fourier analysis on $\mathbb{R}/\mathbb{Z}$ . In that case, the usual Fourier theory states that all (nice enough, say continuous and $C^1$ by parts) function $f$ can be written in the form (I write as usual $e(x) = \exp(2i\pi x)$ . \begin{equation}
f(x) = \sum_{n \in \mathbb{Z}} f_n e(nx).
\end{equation} However I do not see that particularly through the glasses of spectral theory. Since $\mathbb{R}/\mathbb{Z}$ is a compact spaces, spectral theory essentially ensures that it has discrete spectrum. But here is part of my lack of understanding: of what kind of spectrum are we talking about? I can see that $e(n \cdot)$ is an eigenfunction for certain operators, typically the differentiation or the one-dimensional laplacian. But why do we consider these operators more than any other? In other words: is Fourier analysis inherently of this form (instead of taking other ""good operator"" to use and providing other spectral expansions)? Now, what about Fourier analysis on $\mathbb{R}$ . In that case, we have the Fourier transformation theory, and every good enough (say Schwartz, even if it is a far too strong condition) function $f$ can be written in the form \begin{equation}
f(x) = \int_{\mathbb{R}} \hat{f}(y) e(xy) \mathrm{d}y. 
\end{equation} This is a certain kind of ""continuous"" spectral expansion. Is it exactly a (continuous) spectral expansion (the $e(y\cdot)$ are also eigenfunctions of differential operators) or is that merely an analogy? Is there a formalism to make them both really appear as such? And, as above, is that an intrinsic form of spectral expansion or is it depending on a certain kind of operator we chose to look through? Finally, we arrive at automorphic forms. So we have a certain group $G$ , say $GL_2(\mathbb{R})$ , and we look at functions on it. It is quite a large group, so that we quotient by a maximal compact subgroup $K = O_2(\mathbb{R})$ . Any function $f$ on $G$ can therefore be expanded as \begin{equation}
f(g) = \sum_{m \in \mathbb{Z}} f_m(g),
\end{equation} where $f_m(g)$ satisfies the transformation rule $f_m(g r_\theta ) = e(m\theta) f_m(g)$ for the rotation $r_\theta \in K$ . I think this has a relation with the fact that $K$ is compact and should have discrete spectrum, but I do not see the relation with $f$ : it is not eve,n $K$ -invariant. Is that a kind of generalized spectral expansion, where $f$ can be summed over all the possible "" $K$ -periodic"" functions, i.e. satisfying the above transformation rule? So we have a function on $G$ and we split the study between a function on $G_K$ and a spectral decomposition over $K$ ? Finally, for more general automorphic forms and Maass forms. In that case, we decompose every function that is eigenfunction of the (hyperbolic) laplacian as a sum over the laplacian spectrum. Maybe a slightly different question than what is above but the one that motivated all the others is : Why do we chose the laplacian more than any other differential operator? Is it intrinsic/unique in some sense? Or could we get another theory of automorphic forms using another operator?","['number-theory', 'automorphic-forms', 'spectral-theory']"
3553311,Proving $\det(AB) = \det(A) \det(B)$ with elementary matrices,"I am trying to follow Artin's proof that $\det(AB) = \det(A) \det(B)$ , but many of the details are omitted, so I am having difficulty.
I have established the following two lemmas. Lemma 1: $\det(EA) = \det(E) \det(A)$ for any elementary matrix $E$ and matrix $A$ . Lemma 2: For an invertible matrix $A = E_1 E_2 \cdots E_n$ , we have $$\det (A) = \det(E_1) \det(E_2) \cdots \det(E_n).$$ Artin uses these facts to conclude that $$\det(AB) = \det(E_1 \cdots E_k B) = \det(E_1) \cdots \det(E_n) \det(B). $$ I do not understand why this result follows from the lemmas. Help with this would be appreciated.","['matrices', 'proof-explanation', 'determinant', 'linear-algebra']"
3553387,showing $(\arctan(z))' = \frac{1}{1+z^2}$ is true for $z\in C$,"I wish to show that $(\arctan(z))' = \frac{1}{1+z^2}$ is true for $z\in C$ . I've found, after some algebra, $$ \arctan(z) = \frac{i(e^{iz} + e^{-iz})}{e^{iz} - e^{-iz}} \Rightarrow \arctan(z)' = \frac{4}{e^{2iz}-2+e^{-2iz}}$$ then using $z=x+iy$ , $$\frac{4}{e^{2ix-2y} +e^{-2ix+2y}-2} = \frac{4}{(e^{-2y}+e^{2y})cos(2x) +(e^{-2y} - e^{2y})i\sin{2x} -2})$$ then using $\sin z = \frac{e^{iz}-e^{-iz}}{2i}$ and $\cos z = \frac{e^{iz} + e^{-iz}}{2}$ , $$\frac{4}{e^{-2y}e^{iz}+e^{2y}e^{-iz}-2} = \frac{4}{e^{-3y}e^{ix} + e^{3y}e^{-ix}-2}   $$ I cannot get past this point, nor do I know if I'm even on the right path. This problem is a metric ton of algebra, but if you have a hint or solution or spot a misstep, please let me know. Thank you.","['complex-analysis', 'multivariable-calculus', 'derivatives']"
3553485,Prove that $T$ is invertible if and only if $0$ is not an eigenvalue of $T$,"Prove that a linear operator $T$ on a finite-dimensional vector space is invertible if and only if zero is not an eigenvalue of $T$ . Definition: Let $T$ be a linear operator on a vector space $V$ . A nonzero vector $v \in V$ in an eigenvector of $T$ if there exists a scalar $\lambda$ such that $T(v)= \lambda v$ . The scalar $\lambda$ is called an eigenvalue . Let $A$ be in $M_{n,n}(F)$ . A nonzero vector $v\in F^n$ is an eigenvector of $A$ if $v$ is an eigenvector of $L_{A}$ . The scalar $\lambda$ is called the eigenvalue of $A$ . Proof: $\Rightarrow$ Let $T$ be a finite linear operator, and $T(v)=Av=\lambda v$ for $A$ to be a $M_{n,n}(F)$ matrix. If $T(v)$ is invertible, then $T(T^{-1})=(Av)(Av)^{-1}=(\lambda v)(\lambda v)^{-1}=I_n$ . That means $\lambda v$ is nonzero. $\Leftarrow$ If zero is not an eigenvalue of $T$ , that means $\lambda v=Av \neq0$ , then $det(Av)$ $\neq$ $0$ . Hence $T$ is invertible. I know this is a crappy work, but this is all I can think about.","['determinant', 'linear-algebra', 'eigenvalues-eigenvectors']"
3553560,Are the Complex Numbers a subset of the Split Quaternions?,"I saw that if $a+bi\in \mathbb C$ , it's also an element of the split quaternions ( $\mathbb P$ ), since $a+bi=a+bi+0j+0k$ . Does this mean $\mathbb C\subset\mathbb P$ ? If so, does it follow that all Cayley-Dickenson Constructions are a subset of the split version of twice the dimensions? I remember from set theory that if $(x \in A)\rightarrow(x\in B)$ then $A\subset B$ , but I don't know if it applies here, since $A$ and $B$ might have different properties.","['abstract-algebra', 'hypercomplex-numbers']"
3553579,Converting an infinite series to a definite integral,"The expression that I have is, $$\lim_{n \to \infty}\sum_{j=0}^n\left(\frac{1}{\sqrt{n^2 + j}} + \frac{1}{\sqrt{n^2 - j}}\right).$$ Original expression: $$\lim_{n \to \infty}\sum_{j=-n}^n \left(\frac{1}{\sqrt{n^2 - j}}\right).$$ I have prove that this summation is equal to '2' What I have tried, Taylor expanding the root and then summing the terms trying to convert the summation into an integral but this doesn't seem to work because of the $\frac{j}{n^2}$ term which comes when one tries to factor out the $n^2 $ from the denominator. I had posted this problem Converting an infinite summation to an integral which looks similar but doesnt not have the root expression. THis was by accident but I decided not to delete the question because people had answered already.","['analysis', 'sequences-and-series']"
3553599,"$ \mathcal{F}_n = \sigma(\{0\},\{1\},\{2\}, \dots , \{n\})$ showing $\cup_{n\geq0} \mathcal{F}_n $ is not a sigma algebra","Problem : We consider on $\mathbb{N}$ , for all $n\geq 0$ the sigma algebra $ \mathcal{F}_n = \sigma(\{0\},\{1\},\{2\}, \dots , \{n\})$ show that the sequence of sigmas $(\mathcal{F}_n, n\geq 0)$ is increasing but $\cup_{n\geq0} \mathcal{F}_n $ is not a sigma algebra. Solution : Let $ \mathcal{F} = \cup_{n\geq0} \mathcal{F}_n $ suppose $\mathcal{F}$ is a sigma algebra, we have : $\{2n\} \in \mathcal{F}_{2n} \subset \mathcal{F} $ and $2\mathbb{N}=\cup_{n\geq0} \{2n\}$ This way, there exists $n_0$ such that $2\mathbb{N} \in \mathcal{F}_{2n}$ . But the only elements of infinite cardinal of $\mathcal{F}_{n_0}$ are of the form $\mathbb{N} \setminus A$ , where $A$ is a subset of $\{0,1,\dots,n_0\}$ we obtain a contradiction. End of solution What I thought of doing first was trying to verify the three axioms of a sigma algebra, using a result of the lecture that a union sigma algebras is not a sigma algebra, But I don't understand the way this solution is constructed, why are we supposing $\mathcal{F}_{2n}$ a subsequence of a sigma algebra is not necessarily a sigma algebra, Is there some result I have missed? there exists $n_0$ such that $2\mathbb{N} \in \mathcal{F}_{2n}$ , why there exist? if $\mathcal{F}_{2n}$ is a sigma algebra, $\Omega = 2\mathbb{N}$ is in $\mathcal{F}_{2n}$ ""But the only elements of infinite cardinal of $\mathcal{F}_{n_0}$ are of the form $\mathbb{N} \setminus A$ "" , I spent an hour looking for what this means I don't get it, did I miss another result here? How did we use this fact : "" $(\mathcal{F}_n, n\geq 0)$ is increasing but ..."" I mean how does it contradict anything? I am guessing it is either trying to use the monotone class theorem ""the Dynkin system"" which I don't see how it relates to this problem, or I have missed some result ...","['measure-theory', 'proof-explanation', 'solution-verification', 'algebras', 'borel-sets']"
3553613,Is there any example of a simple Abelian ring which is not domain?,A ring $R$ is called: simple if it has no two-sided ideal; a domain if it has no zero divisor; abelian if each idempotent of $R$ is central. Is there any example of a simple abelian ring which is not a domain?,"['ring-theory', 'abstract-algebra', 'noncommutative-algebra', 'examples-counterexamples']"
3553628,Limit $\lim_{n\to\infty}\sum_{k=0}^n\frac{n^{2k}}{(k!)^2}\big/\sum_{k=0}^\infty\frac{n^{2k}}{(k!)^2}$,"Question: How to prove that $$\lim_{n\to\infty}\sum_{k=0}^n\frac{n^{2k}}{(k!)^2}\Bigg/\sum_{k=0}^\infty\frac{n^{2k}}{(k!)^2}=\frac12?$$ ( $0^0$ is defined to be one).
  In addition, can we calculate $$\lim_{n\to\infty}\sqrt n\left(\sum_{k=0}^n\frac{n^{2k}}{(k!)^2}\Bigg/\sum_{k=0}^\infty\frac{n^{2k}}{(k!)^2}-\frac12\right)?$$ Relating to this question , there seems to be two possible ways, one is using central limit theorem, the other one is to turn this sum into an integral and estimate it. Unfortunately, the first possible method cannot be applied because the random variable $X_n$ with $$P(X_n=x)=\frac{n^{2x}}{(x!)^2}\Bigg/\sum_{k=0}^\infty\frac{n^{2k}}{(k!)^2}$$ does not have good properties like Poisson distribution. I'm able to calculate $\mathrm E(X)$ and $\mathrm{Var}(X)$ , which are $\frac{I_1(2n)}{I_0(2n)}$ and $n^2\left(1-\frac{I_1(2n)}{I_0(2n)}\right)$ respectively. CLT cannot be applied here. I'm not familiar with generalized CLT, so I'm hoping for an analytical method. Analytical Attempt Denote $\sum_{k=0}^n\frac{n^{2k}}{(k!)^2}\big/\sum_{k=0}^\infty\frac{n^{2k}}{(k!)^2}$ by $L_n$ . $$L_n=1-\frac{n^{2n+2}{}_1F_2(1;n+2,n+2;n^2)}{((n+1)!)^2I_0(2n)}\\
=1-\left(\frac1{\sqrt{\pi n}}+O(n^{-3/2})\right){}_1F_2(1;n+2,n+2;n^2)$$ But we have $$_1F_2(\cdots)=(n+1)\int_0^1(1-t)^n{}_0F_1(2+n;n^2t)dt\\
=e^{-n}\sqrt{2\pi n}(n+O(1))\int_0^1t^{-(n+1)/2}(1-t)^nI_{n+1}(2n\sqrt t)dt\\
=e^{-n}\sqrt{8\pi n}(n+O(1))\int_0^1t^{-n}(1-t^2)^nI_{n+1}(2nt)dt$$ Where all $I$ 's above denote Bessel I function. I think the asymptotic behavior of $I_n(z)$ when $n\approx kz\gg 0$ is needed, but I don't have reference of it.","['limits', 'summation', 'sequences-and-series']"
3553684,What is the stationary distribution of a bishop's legal moves in chess?,"I have been asked to consider the following questions and I'm not sure what to do. Can anyone help out? Consider the random movement of a knight on a chessboard. At each time step, we pick one of the bishopâ€™s legal moves at random. (1) What is the stationary distribution? (2) What is the expected number of moves to return to the corner (1,1) when we start there? Any help would be much appreciated!","['probability-distributions', 'markov-chains', 'stochastic-processes', 'probability-theory', 'probability']"
3553739,Roots of the equation $(x^2+3x+4)^2+3(x^2+3x+4)+4=x$,"The equation $(x^2+3x+4)^2+3(x^2+3x+4)+4=x$ has (A) all its solution real but not all positive (B) only two of its solution real (C) two of its solution positive and two negative (D) none of its solution real My approach is as follow $(x^2+3x+4)^2+3(x^2+3x+4)+4-x=y$ $f(x)=y=x^4+6x^3+20x^2+32x+32$ $f(-x)=y=x^4-6x^3+20x^2-32x+32$ Using Descartes rule no positive roots but the possible ways we can have either 4,2,0 negative roots. $f'(x)=4x^3+18x^2+40x+32$ $f''(x)=12x^2+36x+40$ which is imaginary From here I am not able to approach.","['algebra-precalculus', 'functions']"
3553896,The definition of the expected value,"I wondered why the expected value defined by the lebesgue integral is a reasonable way to define the weighted average of a random variable, which the expected value represents. What would be a good way to clarify that relation/justify the definition? The discrete cases are very clear, I just wondered about the continuous cases. Thank you in advance! Edit: The definition I know is the following. Let $X:\Omega \to \mathbb{R}$ be a regular random variable on the probability space $(\Omega,A,P)$ . Then the expected value is defined as $E(X):= \int X dP$ .",['probability-theory']
3554086,Is there a connection between the construction of $\mathbb{Q}$ and exotic $\mathbb{R}^4$'s,"I learned that there exist so-called ""exotic"" $\mathbb{R}^4$ 's. That is, there exist topological spaces which are homeomorphic but not diffeomorphic to $\mathbb{R}^4$ . Quite remarkably, it has been proven that $4$ is the only value of $n$ for which there exists an exotic $\mathbb{R}^n$ . Moreover, it has also been shown that there are uncountably many exotic $\mathbb{R}^4$ 's. https://en.wikipedia.org/wiki/Exotic_R4 I admit that I do not at all understand the existence proof of an exotic $\mathbb{R}^4$ , but it seems to be quite non-constructive. It rests upon the existence of a non-trivial smooth 5 dimensional $h$ -cobordism which must exist by some other theorem. https://projecteuclid.org/download/pdf_1/euclid.jdg/1214437666 However I had an idea of where these things could be coming from (or at least something interesting if not related to exotic $\mathbb{R}^4$ 's). Recall the standard construction of $\mathbb{Q}$ from $\mathbb{Z}$ : on the set $\mathbb{Z}\times(\mathbb{Z}-\{0\})$ we define a relation by $$(a,b)\sim(c,d)\Longleftrightarrow ad=bc$$ Then you show it's an equivalence relation and on the set of equivalence classes, you define addtion, multiplication, show that they're well-defined, etc. However, recall from elementary set theory the definition of a relation: A relation on $X$ is nothing more than a subset of $X\times X$ . So from this perspective, $\mathbb{Q}$ can be identified with a particular subset of $\big(\mathbb{Z}\times(\mathbb{Z}-\{0\})\big)^2\subseteq \mathbb{Z}^4$ . But $\mathbb{Z}^4\subseteq \mathbb{R}^4$ , so that means that $\mathbb{Q}$ is sitting inside of $\mathbb{R}^4$ . Clearly, we already know that $\mathbb{Q}\subseteq \mathbb{R}^4$ by inclusion into any one of the coordinates. However, these are quite different than the $\mathbb{Q}$ coming from the set contruction, which I will call $\tilde{\mathbb{Q}}$ to distingtuish from the ""ordinary"" $\mathbb{Q}$ 's sitting inside $\mathbb{R}^4$ . Of course, $\tilde{\mathbb{Q}}$ has it's natural metric. However, it doesn't coincide with the standard metric as a subset of $\mathbb{R}^4$ . So perhaps the completion of $\tilde{\mathbb{Q}}$ with respect to the $\mathbb{R}^4$ metric is what leads to these exotic $\mathbb{R}^4$ 's. Obviously, this is all conjecture but I would appreciate any comments or insight.",['differential-geometry']
3554097,Are Minkowski functionals (a.k.a. gauges) *strictly* convex?,"Take a strictly convex subset $C$ of say, $\mathbb{R}^d$ . 
For the sake of simplicity, assume that $C$ is compact and $0$ belongs to its interior. Define the Minkowski functional \begin{align*}
f \colon \mathbb{R}^d & \to [0,+\infty)\\
x &\mapsto \min\{\tau\ge 0 : x \in \tau C\}
\end{align*} It is known that $f$ is convex. Is its square $f^2$ strictly convex? At least in two dimensions, it feels like it should be. 
It looks like a paraboloid whose sections are scaled $\partial C$ 's (rather than ellipses). 
However, I have no idea how to prove it I and have no geometric intuition whatsoever for $d\ge 3$ .","['normed-spaces', 'analysis', 'real-analysis', 'functional-analysis', 'convex-analysis']"
3554138,Range of the map $(a_n)\to \prod_{n=1}^\infty(1+a_n).$,Let $A$ be the set of nonnegative sequences $(a_n)$ such that $\sum_{n=1}^{\infty}a_n=1.$ Find the range of the map $P:A\to \mathbb R$ defined by $$P((a_n))= \prod_{n=1}^\infty(1+a_n).$$,"['infinite-product', 'real-analysis']"
