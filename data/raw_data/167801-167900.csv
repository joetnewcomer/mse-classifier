question_id,title,body,tags
2933753,Ordering finite groups by sum of order of elements,"Given two finite groups $G, H$ , we are going to say that $G<_oH$ if either a. $|G|<|H|$ or b. $|G|=|H|$ and $\displaystyle\sum_{g\in G} o(g)<\sum_{h\in H} o(h)$ , where $o(g)$ denotes the order of the element $g$ (has this ordering a name?). What is the smallest example (in this ordering) of a pair of nonisomorphic groups such that $G$ and $H$ are incomparable, i.e., such that they have same cardinal and same sum of orders of elements?","['group-theory', 'finite-groups']"
2933783,Evaluate $\lim\big(\frac{1}{n+1}+\frac{1}{n+2}+\ldots+\frac{1}{2n}\big)$ using sequential methods,"Evaluate $$\lim\Big(\frac{1}{n+1}+\frac{1}{n+2}+\ldots+\frac{1}{2n}\Big)$$ using sequential methods. Of course: $$\frac{1}{n}\bigg(\frac{1}{1+1/n}+\frac{1}{1+2/n}+\ldots+\frac{1}{1+n/n}\bigg) \rightarrow \int_{0}^{1}\frac{dx}{1+x}=\ln 2$$ but by using only sequences, I don't know where to start from. I thought of the squeeze theorem, but i am not sure how to get $\ln 2$ on the way. Thanks in advance.","['limits', 'calculus', 'analysis', 'sequences-and-series']"
2933829,why do we define $\int{e^{-t^2}dt}=\frac{\sqrt{\pi}}{2}erf(t)+c$?,"I am relearning differential equation on my own, and came across a problem that gives the integral as erf function. Why is it defined in this way $\int{e^{-t^2}dt}=\frac{\sqrt{\pi}}{2}erf(t)+c$ ? I couldn't integrate it, and I am guessing it's not possible to integrate so that's why it's defined?","['error-function', 'ordinary-differential-equations']"
2933843,Generating function for number of words with no k consecutive 0's?,"I am trying to determine the generating function for the sequence $a_n$ , where $a_n $ is the number of words of length n over the alphabet {0, 1, 2, ..., q-1} that do not contain the subword $0^k$ . I came up with the recurrence $a_{n+k} = sum_{i=n}^{i+k-1} (q-1)a_i$ for n $\geqslant$ k. And, I know that $a_i = q^i$ for 0 $\leqslant$ i $<$ k. I actually got an answer for the generating function, but it is so ugly that I don't believe it could be the correct answer to the problem. I did it over again and got the same answer, so either I am making a mistake somewhere along the way or it is correct. I am curious if someone else could give it a go and see if it turns out to be a very complicated answer for them as well. This is the answer I got, I'm not sure how to format it nicely so it will be hard to follow: $(1/(1-k(q-1)x^k))[((1 - (q x)^k)/(1 - q x)) (1 - (k - 1) (q - 1) x^k) - (x^{k + 1} (q - 1) (k q^k x^{k - 1} (1 - q x) + q (1 - (q x)^k)))/(1 - q x)^2]$ Attempt to reformat: $${1\over1-k(q-1)x^k}B$$ where $$B={(1-(qx)^k)(1-(k-1)(q-1)x^k)\over(1-qx)}-{x^{k+1}(q-1)(kq^kx^{k-1}(1-qx)+q(1-(qx)^k))\over(1-qx)^2}$$ Edit: I found a textbook online that claims the generating function is this: $(1-x^k)/(1-qx+(q-1)x^{k+1})$ But seeing as I didn't get anything close to that and they provide little proof in the text, I am not sure it is correct either.","['combinatorics', 'discrete-mathematics', 'generating-functions']"
2933916,$ \lim_{n \to \infty} \frac{(-1)^n(3-n)}{(3n-5)}$ and ...,"To find limits: $(a) \lim_{n \to \infty} \frac{(-1)^n(3-n)}{(3n-5)}$ $(b) \lim_{n \to \infty} \frac{n^3}{n!}$ For the first one the sequence is oscillating so it does not converge. For the 2nd one I used ratio test: Let $a_n = \frac{n^3}{n!} $ , then $\frac{a_{n+1}}{a_n} = \frac{n! \times (n+1)^3}{n^3 \times (n+1)!} = (1+ \frac1n)^3 \times \frac{1}{1+n}$ . Thus, $|\frac{a_{n+1}}{a_n}| < 1$ , by ratio test limit is $0$ . Is the solutions correct?","['limits', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
2933961,Littlewood-Paley decomposition with arbitrary dilation factor,"In all harmonic analysis literature I've seen, the dilation factor people always use in the Littlewood-Paley decomposition is $2$ , i.e. decompose the function dyadically. To be specific here's what happens: Let $\mathcal{S}(\mathbb{R}^d)$ be the Schwartz space and $\mathcal{S}'(\mathbb{R}^d)$ denotes the space of tempered distributions. Choose $\varphi_0,\varphi\in\mathcal{S}(\mathbb{R}^d)$ be such that $\text{supp}(\widehat{\varphi_0})\subseteq\{\xi:|\xi|<1\}$ and $\text{supp}(\hat{\varphi})\subseteq\{\xi:\frac{1}{2}<|\xi|<2\}$ , where $\text{supp}(\cdot)$ denotes the support of a function and $\hat{\varphi}$ denotes the Fourier transform of $\varphi$ . $\sum_{j=0}^\infty\widehat{\varphi_j}(\xi)=1$ for all $\xi\in\mathbb{R}^d$ , where $$\widehat{\varphi_j}(\xi)=\hat{\varphi}(2^{-j}\xi),\quad\forall\xi\in\mathbb{R}^d, j\in\mathbb{N}.$$ Then it can be shown that every $f\in\mathcal{S}'(\mathbb{R}^d)$ can be decomposed in the following way: $$f=\sum_{j=0}^\infty f*\varphi_j,\quad(*)$$ with the series converging in $\mathcal{S}'(\mathbb{R}^d)$ . This is a classical way to dyadically decompose a function. My question is, what happens if we use other dilation factors? Say let $M$ be an invertible $d\times d$ such that all its eigenvalues are greater than $1$ in magnitudes. Choose $\varphi_0,\varphi\in\mathcal{S}(\mathbb{R}^d)$ be such that $$\sum_{j=0}^\infty\widehat{\varphi_{j;M}}(\xi)=1$$ for all $\xi\in\mathbb{R}^d$ with $$\widehat{\varphi_{0;M}}(\xi)=\widehat{\varphi_0}(\xi),\quad \widehat{\varphi_{j;M}}(\xi)=\hat{\varphi}((M^T)^{-j}\xi),\forall j\in\mathbb{N}.$$ I believe in this case we still have the decomposition $(*)$ with $\varphi_j$ being replaced by $\varphi_{j;M}$ . However if we use the dilation factor $M$ instead of $2$ to define function spaces, say for example we define $$\|f\|_{B_{p,q}^s;M}:=\left(\sum_{j=0}^\infty (|\text{det}(M)|^{js}\|f*\varphi_{j;M}\|_{L^p(\mathbb{R}^d)})^q\right)^{\frac{1}{q}},$$ and for $s\in\mathbb{R}, 0<p,q<\infty$ we define $$B_{p,q}^s(\mathbb{R}^d;M):=\{f\in\mathcal{S}'(\mathbb{R}^d):\|f\|_{B_{p,q}^s;M}<\infty\}.$$ Does the space $B_{p,q}^s(\mathbb{R}^d;M)$ coincide with the Besov space $B_{p,q}^s(\mathbb{R}^d)$ ? Moreover, are the semi-norms on these spaces equivalent? Side remark: the Besov space $B_{p,q}^s(\mathbb{R}^d)$ is defined via $$B_{p,q}^s(\mathbb{R}^d):=\{f\in\mathcal{S}'(\mathbb{R}^d):\|f\|_{B_{p,q}^s}<\infty\},$$ where $$\|f\|_{B_{p,q}^s}:=\left(\sum_{j=0}^\infty (2^{jsd}\|f*\varphi_{j}\|_{L^p(\mathbb{R}^d)})^q\right)^{\frac{1}{q}}.$$","['fourier-analysis', 'harmonic-analysis', 'reference-request', 'littlewood-paley-theory', 'functional-analysis']"
2933982,"$\{µ(E) : E ∈ S\} = [0, 1] ∪ [3, c]$. Prove that $c ≥ 4$. Can you give an example of $(X, S, µ)$ if $c = 4$?","Suppose that there exists a measure space $(X, S, µ)$ with $\{µ(E) : E ∈ S\} = [0, 1] ∪ [3, c]$ .
Prove that $c ≥ 4$ . Can you give an example of $(X, S, µ)$ if $c = 4$ ? My Try: Clearly $\mu(X)=c$ Now suppose we have $A,B\in S$ s.t $\mu(A)=3$ and if $\mu(B)=1$ . If $\mu(A \cap B)=0$ . We are done. Otherwise we have $\mu(A \cap B)\neq0$ then $0<\mu(A \cap B) \leq \mu(B)=1$ then $2\leq \mu(A\setminus (A\cap B))=\mu(A)-\mu(A \cap B)<3$ . What about the example?","['measure-theory', 'lebesgue-measure', 'outer-measure', 'borel-measures', 'real-analysis']"
2933994,What Field(s) Cover Calculus with Matrices and High Dimensional Geometry?,"I am taking a course in machine learning and have found that the linear algebra and multivariable calculus from my engineering degree only take me part way in understanding some derivations. One specific example is differentiating things related to matrices (like differentiating wrt a matrix whose determinant appears in the function...) But this is by no means the only fuzzy bit. I have done just enough geometry, linear algebra and low dimensional calculus to have a notion that these things somehow extend into things involving matrices and things involving high dimensional spaces. I've tried looking on places like wikipedia's topic listing in mathematics, the page ""Categories within Mathematics"" at arxiv.org, and undergraduate mathematics curricula however I don't think I even know enough to know whether I'm looking at what I'm looking for... if that makes sense.... Also, I've found some compilations of matrix derivatives but a) it's disconnected from any context and b) seems like a cookbook solution and so these haven't been satisfying. So... how does what I'm saying here map to topics in mathematics? If I can cheat and ask a ""sub question""... whatever these topics end up being, what are some typical paths people take to get from fairly applied linear algebra and calculus to these ""advanced"" topics? Items on such a path could be books, names of courses, whatever... I just need some guidance towards context and prerequisites.","['geometry', 'learning', 'matrices', 'calculus', 'soft-question']"
2934011,Curvature of a homogenous manifold.,"I was a reading a paper and it seemed to me that in one of the equations the authors used the fact if $M$ is a homogenous Riemannian manifold (i.e., the group of isometries of $M$ act transitively on $M$ ) then the scalar curvature of $M$ is constant. Is this fact true? I was unable to prove this. Is there a simple proof (or not so simple proof) of this fact? P.S. : The manifold was also a gradient Ricci soliton. Can that make the scalar curvature constant. Thanks","['riemannian-geometry', 'ricci-flow', 'homogeneous-spaces', 'global-analysis', 'differential-geometry']"
2934028,Velocity of a particle on a Parabola,"A particle moves along the top of the
  parabola $y^2 = 2x$ from left to right at a constant speed of 5 units
  per second. Find the velocity of the particle as it moves through
  the point $(2, 2)$ . So I isolate $y$ , giving me $y=\sqrt{2x}$ . I then find the derivative of $y$ , which is $1/\sqrt{2x}$ . And $\sqrt{2x}=y$ so the derivative of also equal to $1/y$ . At $(2,2)$ the derivative is 0.5. Not sure where to go from there though. Any help is appreciated.","['calculus', 'derivatives']"
2934038,Different axioms of probability,I wonder if there are some other attitudes to the probability than those related to Kolmogorov? Did somebody try to do those things differently?,"['axioms', 'probability-theory', 'probability']"
2934091,Factorization of a $116$-digit number,"What is the prime factorization of this number : $$2510840694154672305534315769283066566440942177785613805158$$ $$3255420347077336152767157884641533283220471088892806902579$$ ? If we concatenate the Mersenne-numbers $\ \ M(193)=2^{193}-1\ \ $ and $\ \ M(192)=2^{192}-1\ \ $ and divide by $5$ , we get this $116$ -digit composite number. I tried to factor it with the $1.34$ -version from yafu, but ecm did not give a result. The quadratic sieve is very time-consuming. The smallest prime factor of this number probably has more than $40$ digits. How can I factor this number ?","['number-theory', 'prime-factorization', 'elementary-number-theory', 'prime-numbers']"
2934102,"If $f$ is injective on $B_r(z_0)$ and holomorphic on $\overline{B_r(z_0)} $, then $f(B_r(z_0))\cap f(\partial B_r(z_0) )=\emptyset$ [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I need to show that if $f$ is holomorphic on an open set $U$ , $\overline{B_r(z_0)} \subseteq U$ and $f|_{B_r(z_0)}$ is injective, then $$ f^{-1}(w)= \frac{1}{2\pi i} \int_{\partial B_r(z_0) } \frac{zf'(z)}{f(z)-w}dz$$ for $w \in f(B_r(z_0))$ . But I don't know how can I show that $w \notin f(\partial B_r(z_0))$ . Can you help me, please?","['complex-analysis', 'holomorphic-functions']"
2934112,"Relation between two different definitions of ""regular surface""","I am currently being confused by two different definitions given in the books ""manifolds and differential geometry (by jeffrey lee)"" and ""differential geometry of curves and surfaces (by do carmo)"". First definition, given by Lee:
""A subset $S$ of a smooth n-manifold $M$ is called a ""regular submanifold"" of dimension $k$ if every point $p \in S $ is in the domain of a chart (U,x) that has the following ""regular submanifold property"" with respect to $S$ : $x(U \cap S ) =x (U) \cap (\mathbb{R}^k \times \{c\})$ for some $c \in \mathbb{R}^{n-k}$ . Note: In Lee's book, a smooth manifold is defined to be a Hausdorff paracompact topological space attached with smoothly compatible charts covering the whole space, i.e. a smooth atlas. Another (somewhat related) definition given by do Carmo is: A subset $S \subset \mathbb{R}^3 $ (although I think this can be easily replaced by $\mathbb{R}^n$ ) is a ""regular surface"" if, for each $p \in S$ , there exists an open $V \subset \mathbb{R}^3$ and a map $x:U \rightarrow V \cap S$ of an open set $U \subset \mathbb{R}^2$ onto $V \cap S \subset \mathbb{R}^3 $ such that x is $C^\infty$ x is a homeomorphism onto its image The differential $ dx(q):\mathbb{R}^2 \rightarrow \mathbb{R}^3$ is injective for all $q \in U$ . My question is: How are these definitions related?
More precisely, Given a regular surface in sense of do Carmo, is it always a regular submanifold of dimension 2 of $\mathbb{R}^3$ in sense of Lee? (i.e. is there a 'canonical' way to construct charts having the regular submanifold property?) Do every regular sub 2-manifold of $\mathbb{R}^3$ in sense of Lee becomes a regular surface in sense of do Carmo? Thanks.","['smooth-manifolds', 'differential-geometry']"
2934117,Analogy between the fundamental theorems of arithmetic and algebra,"For the learned mathematician it may be obvious and not worth mentioning: that the fundamental theorems of arithmetic and algebra look very similar and have to do with each other, in abbreviated form: $$ n = p_1\cdot p_2 \cdots p_k$$ $$ P(z) = z_0\cdot(z_1 -z)\cdot (z_2 -z) \cdots (z_k -z)$$ which makes obvious that the irreducible polynoms of first degree play the same role in $\mathbb{C}[X]$ as do the prime numbers in $\mathbb{Z}$ (which both are unitary rings). It also gives — in this special case — the wording ""fundamental theorem"" a specific meaning: It is stated that and how some irreducible elements build the fundaments of a structure. Is this analogy helpful, or is it superficial and maybe misleading? If the former, can it be formalised? If the latter, what are the differences that make it merely superficial?","['arithmetic', 'abstract-algebra']"
2934134,"5 pairs of shoes, from which 4 shoes are chosen at random. What is the probability that at least one pair is chosen?","This question has been answered here: There are 5 pairs of shoes out of which 4 shoes are taken one by one. What is the probability that at least one pair is present in the shoes taken? As have similar questions for example here: Probability of having at least one pair by drawing 4 shoes from 12 pairs. I understand the solutions above, what I don't understand is the solution given in the book: The method they use is the same as above calculate one minus the probability of there being no pairs present. $$1-\frac{\binom{5}{0}\binom{5}{4} + \binom{5}{1}\binom{4}{3} +\binom{5}{2}\binom{3}{2} +\binom{5}{3}\binom{2}{1}+ \binom{5}{4}\binom{1}{0}}{\binom{10}{4}}$$ My initial interpretation is that in the numerator: 1.) The first binomial term is the number of pairs present. 2.) The second binomial is the number of ways to chose the ""other shoes"" so that the total chosen is four. However, it would appear that my interpretation of these two binomial terms must be incorrect? If I were correct about the first binomial term what would $\binom{5}{0}$ represent? All the shoes must come from one of the pairs so it's not possible to have none of the pairs represented? Also what these ""other shoes"" be in the second binomial term? Again every shoe is chosen from the 5 pairs so the 5 pairs are present every time? Can someone please give me the correct interpretation of the solution in the book? Thanks","['discrete-mathematics', 'combinatorics', 'probability', 'permutations']"
2934203,Understanding proof of Ptolemy's theorem - how does it connect to a circle?,"Proving: ""a quadrilateral can be inscribed in a circle iff the product of its diagonals equals the sum of the products of its opposite sides"" The book I'm following uses this diagram and goes on to show: (1) We can express: $$AB \cdot CD + AD \cdot BC = AC \cdot BD$$ (2) Using: $$\sin\alpha \sin\beta + \sin\gamma\sin\delta = \sin(\beta+\gamma)\sin(\alpha+\gamma)$$ Further, the book goes on derive ""Ptolemy's Identity"" from this: If $\alpha + \beta + \gamma + \delta = \pi$ , then $\sin\alpha \sin\beta + \sin\gamma\sin\delta = \sin(\beta+\gamma)\sin(\alpha+\gamma)$ This statement is equivalent to the part of Ptolemy's theorem that says if a quadrilateral is inscribed in a circle, then the product of the diagonals equals the sum of the products of the opposite sides. I somehow can't follow the proof completely, because: I don't understand what rewriting the equation from (1) to (2) actually shows. It just shows that I can express the equality using sine of the angles, but how does it make the theorem true? In the identity described, how is the sum of the angles equal to $\pi$ connected to the theorem and why are these statements equivalent, as claimed above?","['euclidean-geometry', 'trigonometry', 'proof-explanation']"
2934258,Is there geometric intuition for the $\frac{3}{2}$ exponent in radius of curvature formula?,I can follow a derivation of radius of curvature $$\frac{\left[1+\left(\frac{dy}{dx}\right)^2\right]^{3/2}}{\frac{d^2y}{dx^2}}$$ but I can't see intuitively how there should be a $\frac{3}{2}$ exponent in there. Is there a diagram or explanation that makes this intuitive?,"['calculus', 'geometry', 'intuition', 'differential-geometry']"
2934265,Find all cosets of $H$ and $K$ in $G$,"Let $G = \mathbb{Z}_{10} \times \mathbb{Z}_{4}$ and $H = \langle(3, 2) \rangle$ and $K = \langle(4, 2)\rangle$ . Find all cosets to H and K. We have \begin{equation*}
\begin{split}
G & = \mathbb{Z}_{10} \times \mathbb{Z}_{4} \\
& = \{0, 1, 2, \ldots, 9 \} \times \{0, 1, 2, 3 \} \\
& = \{(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), \ldots, (1, 3), (2, 0), \ldots, (2, 3), \ldots, (9, 0), \ldots, (9, 3) \} ,
\end{split}
\end{equation*} and \begin{equation*}
\begin{split}
H & = \langle(3, 2)\rangle \\
& = \{k \cdot (3, 2) : k \in \mathbb{Z} \} \\
& = \{ (k \cdot 3, k \cdot 2) : k \in \mathbb{Z} \} \\
& = \{(0, 0), (3, 2), (6, 0), (9, 2), (2, 0), (5, 2), (8, 0), (1, 2), (4, 0), (7, 2) \} ,
\end{split}
\end{equation*} since \begin{equation*}
\begin{split}
0 \cdot (3, 2) = (0, 0) \quad &\land \quad 1 \cdot (3, 2) = (3, 2) \quad \land \quad 2 \cdot (3, 2) = (6, 0) \\
3 \cdot (3, 2) = (9, 2) \quad & \land \quad 4 \cdot (3, 2) = (2, 0) \quad \land \quad 5 \cdot (3, 2) = (5, 2) \\
6 \cdot (3, 2) = (8, 0) \quad & \land \quad 7 \cdot (3, 2) = (1, 2) \quad \land \quad 8 \cdot (3, 2) = (4, 0) \\
9 \cdot (3, 2) = (7, 2) \quad & \land \quad \ldots 
\end{split}
\end{equation*} . Now, in order to find all cosets of H, do I have to check for every $(a, b) \in G$ what sets I receive when I compute $(a, b) + H = \{(a, b) + h : h \in H \}$ ? I.e. do I have to compute 40 different sets? Is there a less brute force/tedious method?","['group-theory', 'abstract-algebra']"
2934275,Area of a triangle inside a triangle,"In $ABC$ , let $D$ , $E$ , and $F$ be points on the sides $BC$ , $AC$ and $AB$ , respectively, such that $BC = 4CD$ , $AC=5AE$ , and $AB= 6BF$ . If the area of $ABC$ is $120$ , what is the area of $DEF$ . I tried connecting the vertices of the inner triangle to the opposite vertices of the outer triangle, but I think that they wont be collinear because of Ceva's Theorem, I cant think of anything else except for assuming the triangle is right and using coordinate geometry. I am asking for a nice solution",['geometry']
2934312,Find $\lim_{x \to \infty} \frac{\sin{x}+\cos{x}}{x}$,"Find $$\lim_{x \to \infty} \Bigg(\frac{\sin{x}+\cos{x}}{x}\Bigg)$$ My thinking is $$-1 < \sin{x} < 1 $$ $$-1 < \cos{x} < 1.$$ Therefore $$-2 < \sin{x} + \cos{x} < 2$$ $$\frac{-2}{x} < \frac{\sin{x} + \cos{x}}{x} < \frac{2}{x}.$$ But is this even allowed? If so, why? Thanks","['limits', 'calculus', 'proof-verification', 'limits-without-lhopital']"
2934325,Solving the one particle partition function,"We have $N$ oscillators and each of them is described by the Hamiltonian: $$H = \frac{p^2}{2m} + \frac{Kq^4}{4} $$ I have to compute the average total energy $\langle E\rangle$ of the $N$ oscillators. But to do so, first I have to compute the one particle partition function and to do so I have to solve the following integral: $$Z_1 (V,T) = \iint_{\mathbb{R}^2}  e ^{-\beta \,H_1(p,q)}\,dp\,dq.$$ So in this case: $$ Z_1 = \int_{-\infty}^{\infty}  e^{-\beta\frac{p^2}{2m}}\,dp \int_{-\infty}^{\infty}  e^{-\beta \frac{Kq^4}{4}}\,dq. $$ I know this integral can be solved by the Gauss method, knowing that: $$\int_{-\infty}^{\infty} e^{-x^2} dx = \sqrt{\pi}.$$ For the first integral I got: $$\int_{-\infty}^{\infty}  e^{-\beta\frac{p^2}{2m}}\,dp = \sqrt{\frac{2m\pi}{\beta}}$$ I am having difficulties solving the second one: $$\int_{-\infty}^{\infty}  e^{-\beta \frac{Kq^4}{4}}\,dq.$$ I have tried to make a the change of variables: $q^4 = a^2,$ but this does not simplify the calculation. What method should I use? Once you get $Z_1$ : $$Z_N = (Z_1)^N.$$ Then you just have to apply: $$\langle E \rangle = -\frac{ \partial \log(Z_N)}{\partial \beta}.$$ ANSWER $$\langle E \rangle = \frac{3N}{4\beta}.$$","['integration', 'calculus', 'statistical-mechanics', 'derivatives', 'mathematical-physics']"
2934438,"Variational Problem, Role of boundary conditions in derivation of Euler-Lagrange?","This is something of a sanity check as I don't have that much formal background in the calculus of variations. My intuition to the block-quoted question below is that the answer is affirmative but I'm afraid I'm making a big blunder. I am trying to understand the role of boundary conditions in deriving the Euler-Lagrange equations, and in particular whether they are necessary at all. To make things precise, assume $$
R : X \to \mathbb{R}
$$ Is a convex Gateaux-differtiable functional. $X$ is a Hilbert space of functions, like say a closed subspace of the Sobolev space $H^p$ . $R$ is furthermore of the form: $$ R(u)= \int_\Omega L(x,u) dx $$ for some region $\Omega$ of $\mathbb{R}^n$ . $L$ is nice (satisfying all the usual requirements on coercivity, smoothness and so on) and is allowed to depend on $u\in X$ . The case where $L$ depends on the derivative and higher order derivatives up to order $p$ of $u$ is also of interest. However, I should stress that the ""base case"" without derivatives is of independent interest to me. My question is the following: Does there exist a (nonlinear) partial differential operator $A$ (Euler-Lagrange?) such that the unique minimizer $u$ of $R$ is the unique solution to $A(u)=0$ . What this boils down to is that I am trying to ascertain the role of boundary conditions in the derivation of the Euler-Lagrange equations as presented in for instance Evans and whether it is possible to charactarize optimization problem by a certain partial differential operator (and in this application assuming boundary conditions makes no sense). As a remark, my confusion about whether the Euler-Lagrange equations depend on boundary conditions or not probably comes from me only studying evans, where the boundary conditions are naturally part of the problem, as the end goal is to study PDE. Here on the other hand, the objectives are switched and the end goal is to study optimization using PDE.","['optimization', 'functional-analysis', 'calculus-of-variations']"
2934464,Lebesgue integral of function zero,"Maybe a silly question but can I show that the Lebesgue integral of a function which is zero everywhere is also zero. I have approached this with a ""standard machine"" type proof. I can easily show it (1) assuming that the function is a characteristic function, (2) with a simple function, and (3) extending by monotonicity with non-negative function , but I get stuck in (4) any function. Let's asuume that $h$ is some function. I know by definition that $h=h^+-h^-$ and that the $\mu(h)=\mu(h^+) - \mu(h^-)$ , where $\mu(h)=\int h d\mu$ for some measure $\mu$ . Now I have that $h=0$ , which means that $h^+=h^-$ , which implies that the two parts are zero, since $max(f(x),0)=-min(f(x),0)$ only for $f(x)=0$ . Now, how do I translate that to the integrals?","['measure-theory', 'lebesgue-integral']"
2934471,"Check simple proof that $\lim\limits_{s\to0^+}\sum\limits_{n=1}^\infty(-1)^nf(n)^{-s}=-\frac12$ if $f>0$, $f''\le0$ and $f(+\infty)=+\infty$","$f:[1,+\infty)\to \mathbb{R}_+$ satisfies  $\ f''\leq0,\ f(+\infty)=+\infty $. Prove 
  $$\lim_{s\to0^+}\sum_{n=1}^\infty (-1)^n[f(n)]^{-s}=-\frac{1}{2}$$ Some of my friends showed me this question and declared it's difficult. However I've found a quite simple proof. Could you help me examine if it's correct? Proof: Obviously, $\ f$ is strictly increasing. For $s\in(0,1)$, due to Leibniz's Test, $\sum_{n=1}^\infty (-1)^n[f(n)]^{-s}$ converges and we denote it by $(-1)S(s)$.
 $$f''\leq0\ \&\ x^{-s}\ \text{is convex}\Rightarrow\ x^{-s}\circ f=[f]^{-s}\ \text{is convex}$$
$$\Rightarrow\ f^{-s}(n)-f^{-s}(n+1)\geq f^{-s}(n+1)-f^{-s}(n+2),\quad n\in\mathbb{N}_+$$ Let $a_n^s$ represents $f^{-s}(n)$, now we have, $$S(s)=a_1^s-a_2^s+a_3^s-a_4^s\cdots\ .$$
Define $S\tilde(s)$ as
$$S\tilde(s)=a_2^s-a_3^s+a_4^s-a_5^s\cdots\ .$$ Notice that 
$$S(s)\geq S\tilde(s),\ S(s)\leq S\tilde(s)+a_1^s-a_2^s, \ S(s)+S\tilde(s)=a_1^s,$$
which implies
$$a_1^s/2\leq S(s)\leq a_1^s-a_2^s/2\ .$$ Let $s\to 0^+$, now we proved $\lim_{s\to 0^+}S(s)=1/2$, that is the conclusion.","['proof-verification', 'sequences-and-series', 'real-analysis']"
2934472,Sum of identically distributed but not independent Bernoulli's is non-uniform,"Let $X_1,X_2,\dots$ denote a sequence of identically distributed, exchangeable, but not independent, Bernoulli $(p)$ random variables. If there exists $n>0$ such that $$
\sum_{i=1}^{n}X_i
$$ is not uniformly distributed on $\{0,1,\dots,n\}$ , how can we show that this implies $$
\sum_{i=1}^{n}X_i + X_{n+1}
$$ is not uniformly distributed on $\{0,1,\dots,n+1\}$ ? If $p \ne 1/2$ then the claim follows by expectations, so we can consider $p=1/2$ .","['random-variables', 'probability-theory', 'probability', 'sequences-and-series']"
2934479,Exact Differential: Integrating Factor in Higher Dimensions,"In two dimensions, we can turn every inexact differential $f(x,y)dx+g(x,y)dy$ into an exact version by multiplying both functions $f(x,y)$ and $g(x,y)$ by an additional function $I(z)$ , i.e. $I(z) f(x,y) dx + I(z) g(x,y) dy = 0$ , where $z = xy$ . Then, the condition that the cross-derivatives should be the same, i.e. $$f_y(x,y) I(z) + x I'(z) f(x,y) = g_x(x,y) I(z) + y I'(z) g(x,y),$$ lead to a differential equation for $I$ and therewith to the solution. Is this trick of multiplying the inexact differential by an additional function also possible in higher dimension, i.e. for $n$ variables? In other words, is there a possibility to make $\sum_{i=1}^n a_i(x) dx_i$ exact, where $a_i(x)$ are arbitrary $C^1$ -functions? An idea would be $$\sum_{i=1}^n a_i(x) \sum_{k=1}^n I_{ik}(x_i x_k) dx_i$$ to account for the cross-derivatives. Is this solveable for $I_{ik}$ or are there other approaches? Does anybody has experiences therefore? Thanks already.","['differential', 'multivariable-calculus', 'integrating-factor', 'ordinary-differential-equations']"
2934485,"Bishop - Pattern Recognition & Machine Learning, Exercise 1.4","I'm working on exercise 1.4 in Bishop's Pattern Recognition & Machine Learning book. This exercise is about probability densities. I've two questions about this exercise. First, I don't understand equation 1.27. He writes:
""Under a nonlinear change of variable , a probability density transforms differently from a simple function, due to the Jacobian Factor."" I never ever heard about the Jacobian factor. What is that factor? ""For instance, if we consider a change of variables $x = g(y)$ , then a function $f(x)$ becomes $\tilde f(g(y))$ . Now consider a probabilty density $p_x(x)$ that corresponds to a density $p_y(y)$ with respect to the new variable $y$ , where the suffices denote the fact that $p_x(x)$ and $p_y(y)$ are different densities. Observations falling in range $(x, x + \delta x)$ will, for small values of $\delta x$ , be transformed into the range $(y, \delta y)$ where $p_x(x)\delta x \simeq p_y(y)\delta y$ , [...]"" What does the relation $\simeq$ mean in this context? ""[...] and hence $$
\begin{align}
p_y(y) &= p_x(x) \left| \frac{\text{d}x}{\text{d}y}\right|\\
&= p_x(g(y))\left|g'(y)\right|.""
\end{align}
$$ This is equation 1.27. I don't understand where this equation comes from. Why is there this absolute value? ""One consquence of this property is that the concept of the maximum of a probabilty density is dependent on the choice of variable."" And at this point the book refers to exercise 1.4: ""Consider a probability density $p_x(x)$ defined over a continous variable $x$ , and suppose that we make a nonlinear change of variable using $x = g(y)$ , so that the density transforms according (1.27). By differentiating (1.27), show that the location $\hat y$ of the maximum of the density in $y$ is not in general related to the location $\hat x$ of the maximum of the density over $x$ by the simple functional relation $\hat x = g(\hat y)$ as a consequence of the Jacobian factor. This shows that the maximum of a probability density (in contrast to a simple function) is dependent on the choice of variable. Verify that, in the case of a linear transformation the location of the maximum transforms in the same way as the variable itself."" I don't understand, what this exercise asks me to do... :/ Would be great, if someone could help me...","['machine-learning', 'pattern-recognition', 'probability', 'density-function']"
2934491,Divisor proof of the Fundamental Theorem of Algebra,"I was asked by my professor to prove the Fundamental Theorem of Algebra using the fact that for any function defined on the projective plane $deg(div(f))=0$ . Now if we let $p(x,z)=\frac{\sum_{i=0}^na_ix^iz^{n-i}}{z^n}, a_i\in\mathbb{K}$ then every polynomial has a pole at the point of order $n$ at infinity $(x,z)=(1,0)$ and thus it must have $n$ roots. The thing is this proof does not require $\mathbb{K}$ to be algebraically closed. So this would imply that for example in $\mathbb{P}^1(\mathbb{R})$ every polynomial of degree $n$ has $n$ roots. Could someone please explain to me where is the mistake?","['algebraic-geometry', 'proof-verification']"
2934506,Linear functions on Riemannian manifolds,"Let $(M,g)$ be  a  Riemannian manifold  with the  corresponding $LC$ connection $\nabla$ .  For  a smooth function $f:M \to \mathbb{R}$ , the  Hessian of $f$ is  defined  as  a  2-linear map on the tangent space as follows $$Hess(f)(x)(V,W)=g(\nabla_V \nabla f,W)$$ where $V,W \in T_x M$ . A  linear  function is  a  function whose  Hessian is  identically  zero. What  is  an example  of  a  non constant  linear  function on each  of the  following  manifolds 1)On $S^2$ with  standard metric 2)On $\mathbb{C}P^2$ with Fubini Study  metric Furthermore,  what  can be said  about the  dimension of the  space  of  linear functions?","['partial-differential-equations', 'riemannian-geometry', 'differential-geometry']"
2934512,Explicit formula of exponential of companion matrix,"Let $$A=\begin{bmatrix} a_k & a_{k-1} & a_{k-2} & \cdots & a_2 & a_1 \\ 1 & 0 & 0 & \cdots & 0 & 0 \\ 0 & 1 & 0 & \cdots & 0 & 0 \\ 0 & 0 & 1 & \cdots & 0 & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots &\\0 & 0 & 0 & \cdots & 1 & 0\end{bmatrix}$$ be a $k \times k$ matrix of { $a_k$ } on a commutative ring. Find the explicit expression of the last row of $A^n$ in terms of { $a_k$ }, where $k\le n$ .","['matrices', 'matrix-exponential', 'linear-algebra', 'companion-matrices']"
2934516,How to solve $y=\tan(θ)$ for $θ$?,"If $\arctan(\tan(\theta))$ is not necessarily equal to $\theta$ , how come if we are given $y=\tan(θ)$ the solution in terms of $\theta$ is $\theta=\arctan(y)$ ? I'm trying to intergrate $1/(1+y^2)$ using trig substitution and I am trying to get my solution, $\theta$ , in terms of $y$ , $y=\tan(\theta)$","['integration', 'trigonometry']"
2934522,Show that this matrix is not diagonalizable,"Say I have a matrix: $$A = \begin{bmatrix} 2 & 0 \\ -1 & 2 \end{bmatrix} $$ Is this matrix diagonalizable? Does a 2x2 matrix always have 2 eigenvalues (multipicity counts). Why is this? I know this matrix (because it's lower triangular) has the eigenvalue of 2 with multiplicity 2... but does a matrix of this size always have 2 eigenvalues. Why is this? Is there any way to know if the eigenvalue of 2 has two eigenvectors or not quickly? Here's the way I know to find the eigenvector: $$\begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} - \begin{bmatrix} 2 & 0 \\ -1 & 2 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} $$ $$ eigenvector = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ t \end{bmatrix} = t * \begin{bmatrix} 0 \\ 1 \end{bmatrix}$$ By this theorem, it is not diagonalizable because it only has 1 eigenvector right and the matrix has 2 rows and 2 columns:","['diagonalization', 'linear-algebra', 'eigenvalues-eigenvectors']"
2934536,Find all positive integral values of $x$ if $\prod_{m=0}^{1008} (x-{2m+1 \over 2})^{2m+1} \lt 0$,"Lately, I have been taking multiple classes on such math problems. So while I was solving some math problems, I came over this question. The question originally says: How many positive integer solutions has the following inequality: $$\left(x-{1\over 2}\right)^1\left(x-{3\over 2}\right)^3\cdots\left(x-{2017\over 2}\right)^{2017} \lt 0.$$ I managed to change to: $$\prod_{m=0}^{1008} \left(x-{2m+1 \over 2}\right)^{2m+1} \lt 0.$$ I have been trying so hard to simplify more to get a solution but failed, and I was hoping if I could receive some help on this one. Thank you anyways.",['algebra-precalculus']
2934540,Matrix group isomorphic to $\mathbb{Z}$,"Define the set $$
G := \left\{
\begin{pmatrix}
1 - 2n & n  \\
-4n & 1 + 2n 
\end{pmatrix} : n \in \mathbb{Z}\right\}.
$$ Show that $(G, \cdot)$ is a group using the usual matrix multiplication. Furthermore, decide whether $G \cong \mathbb{Z}$ . To show that $G$ is a group, we note that matrix multiplication is associative. The identity matrix $I_{2 \times 2}$ takes the role of the identity element in G. Since $\det(A) = 1 \neq 0$ we know that each element $A \in G$ has an inverse $A^{-1} \in G$ which can be computed by the formula $A^{-1} = \dfrac{1}{\det(A)} \cdot \begin{pmatrix}
d & -b  \\
-c & a 
\end{pmatrix}$ for any $2 \times 2$ matrix $A = \begin{pmatrix}
a & b  \\
c & d 
\end{pmatrix}$ . Thus, $(G, \cdot)$ is a group. Now, for the homomorphism part. Define the map $\phi : \mathbb{Z} \to G$ by $$
\phi(n) = \begin{pmatrix}
1 - 2n  & n  \\
-4n & 1  + 2n
\end{pmatrix}.
$$ First, we show that $\phi$ is a homomorphism. I.e. that $\forall m, n \in \mathbb{Z}$ we have \begin{equation}
\label{eq:homomorphism_condition}
\phi(n + m) = \phi(n) \cdot \phi(m) .
\end{equation} We have \begin{equation*}
\begin{split}
\phi(n) \cdot \phi(m) & = \begin{pmatrix}
1 - 2n  & n  \\
-4n & 1  + 2n
\end{pmatrix} \cdot \begin{pmatrix}
1 - 2m  & m  \\
-4m & 1  + 2m
\end{pmatrix} \\
& = \begin{pmatrix}
(1 - 2n)(1 - 2m) + n(-4m)  & (1 - 2n)m + n(1 + 2m)  \\
-4n(1 - 2m) + (1 + 2n)(-4m) & -4nm + (1 + 2n)(1 + 2m) 
\end{pmatrix} \\
& = \begin{pmatrix}
1 - 2m - 2n + 4mn - 4mn  & m - 2nm + n + 2mn  \\
-4n + 8nm -4m - 8mn & -4nm + 1 + 2m + 2n + 4mn 
\end{pmatrix} \\
& = \begin{pmatrix}
1 - 2m - 2n  & m + n  \\
-4n  -4m &  1 + 2m + 2n 
\end{pmatrix} \\
& =\begin{pmatrix}
1 - 2(n + m)  & m + n  \\
-4(n + m) &  1 + 2(n + m) 
\end{pmatrix} \\
& = \phi(n + m),
\end{split} 
\end{equation*} which proves that $\phi$ is a homomorphism. Next, we show that $\phi$ is bijective. To see that $\phi$ is injective, let $\phi(n) = \phi(n')$ . Hence, \begin{equation*}
\begin{pmatrix}
1 - 2n  & n  \\
-4n & 1  + 2n
\end{pmatrix} = \begin{pmatrix}
1 - 2n'  & n'  \\
-4n' & 1  + 2n'
\end{pmatrix},
\end{equation*} if and only if \begin{equation*}
\begin{split}
1 - 2n = 1 - 2n' \quad & \iff \quad n = n' \\
n = n' \quad & \iff \quad n = n' \\
-4n = -4n' \quad & \iff \quad n = n' \\
1 + 2n = 1 + 2n' \quad & \iff \quad n = n'.
\end{split}
\end{equation*} Thus, $\phi$ is injective. I am stuck on showing that $\phi$ is surjective.","['group-theory', 'abstract-algebra', 'group-isomorphism']"
2934553,Eigenvalues of an operator in a real separable Hilbert space as smooth functions of a parameter,"Suppose $V$ is a infinite-dimensional separable real Hilbert space and $$
T\,\colon\,I \to B(V),
$$ where $I = [0,a) \subset \mathbb{R}$ and $B(V)$ denotes the space of bounded linear operators on $V$ . The short-hand notation is $T(\epsilon) \equiv T_{\epsilon}$ for any $\epsilon \in I$ . Suppose further that $T$ is continuous (possibly Frechet differentiable?) and that $T_0$ is simply given by $$T_0v = cv,\quad(1)$$ where $c > 0$ . Final assumptions are that for any $\epsilon \in I$ , $T_{\epsilon}$ is self-adjoint and is an isomorphism  and for $\epsilon > 0$ it is not of the form given in (1). (Side note: I am looking at a specific example but I wanted to look at the problem more abstractly, perhaps some of the 'assumptions' either follow from the others or are not needed). I would like to be able to characterise the set of eigenvalues and eigenvectors of $T_{\epsilon}$ and how they depend on the parameter $\epsilon$ . Trivially, the form of $T_0$ implies that $c$ is the sole eigenvalue of $T_0$ and its corresponding eigenspace is the whole of $V$ . It seems fairly natural that the eigenvalues will be continuous functions of $\epsilon$ since $T$ is a continuous function and so if $\lambda_\epsilon$ is an eigenvalue of $T_\epsilon$ with the corresponding eigenvector $v_{\epsilon}$ then $$
(\lambda - c)v_{\epsilon} = (T_{\epsilon}-T_0)v_{\epsilon} \implies |\lambda - c|\|v_{\epsilon}\| \leq \|(T_{\epsilon}-T_0)\|\|v_{\epsilon}\| \implies |\lambda - c| \leq C\epsilon,
$$ with the first equality following from the fact that $v_{\epsilon}$ is also an eigenvector for $T_0$ . But what gives us the certainty that $T_{\epsilon}$ has any eigenvalues at all? How many eigenvalues can it have? Since it is self-adjoint and $V$ is separable, there can be at most countably many eigenvalues, so it is tempting to conjecture that the set of eigenvalues is $$(\lambda_1(\epsilon),\lambda_2(\epsilon),\dots)$$ with each entry being a continous function of $\epsilon$ with $\lambda_i(0) = c$ and possibly $\lambda_i(\epsilon) = \lambda_j(\epsilon)$ for some $i\neq j$ . A further conjecture would be that each $\lambda_i(\epsilon)$ can be associated with one eigenvector $v_i$ such that $(v_i,v_j) = 0$ and the closure of ${\rm span}(v_1,v_2,\dots)$ is the whole of $V$ . For that it feels like we should employ Spectral Theorem, so in particular perhaps establish that $ (T_{\epsilon} - T_0)$ is a compact operator? Is it? I do not quite see it. I will humbly appreciate any potential comments, I am quite confused here, in particular the notion of orthonormal basis for infinite-dimensional Hilbert space seems a bit elusive. As I mentioned in the side-note, I am keen to consider this more broadly, so perhaps if one of the assumptions makes the problem trivial (perhaps the fact that $T_{\epsilon}$ is an isomorphism?), please feel free to comment more broadly.","['hilbert-spaces', 'inner-products', 'spectral-theory', 'functional-analysis']"
2934580,Algebraic relation between polynomials,"The problem statement is ""Let $F \in \mathbb{C}[t]$ have degree at most $D \geq 1$ , and let $G \in \mathbb{C}[t]$ have degree $E \geq 1$ . Show that there is a $P \not = 0$ in $\mathbb{C}[X,Y]$ with degree at most $E$ in $X$ and $D$ in $Y$ such that $P(F,G) = 0$ ."" I've tried to work this out by supposing $$P = \sum_{i=0}^{E} \sum_{j=0}^{D}c_{ij}X^iY^j$$ , and noticing that I have control over the choice of $(1+E)(1+D)$ coefficients. I was hoping to be able to use this information to create a homogeneous system of linear equations to give a non-trivial solution for the $c_{ij}$ that would force $P(F,G) = 0$ . Is this a viable approach? If so, what would my next step be? I asked my professor, and the hint he gave me was to think about the resultant of $F$ and $G$ . I know how to construct the matrix whose determinant is the resultant of $F$ and $G$ , and I know the resultant is $0$ if $F$ and $G$ share a common factor, but I don't know how that helps us with this problem Thanks in advance for any comments, hints, or solutions!","['abstract-algebra', 'resultant', 'polynomials']"
2934589,Definite Integral of an Infinite Sum,"It is given that $$ f(x) = \sum_{n=1}^\infty \frac{\sin(nx)}{4^n} $$ How would one go about calculating $$ \int_0^\pi f(x)\ dx $$ EDIT: I was able to calculate the integral of the $ \sin(nx) $ part using $u$ substitution. However, I lack the required knowledge to combine an integral and an infinite sum as this is the first time  I am doing this kind of a question. I am currently studying in 11th in India under the CBSE curriculum. This question appeared in one of my internal tests and I am trying to get an explanation for it. I'd like to mention that I only posess basic knowledge of integration and differentiation as taught in my coaching center and knowledge of 11th grade NCERT math.","['integration', 'calculus', 'arithmetic-progressions', 'sequences-and-series']"
2934621,Proving property of two trees.,"Consider a graph $G$ . Let $A, B$ are two trees in a graph and $T_a, T_b$ represents their corresponding edge sets. Also an edge $e \in E$ is an extension of tree $A$ . If $T_b \cup \{e\}$ forms a cycle then exactly one of the following holds: There exists an edge $e_b \in T_b$ which is also an extension of tree A For all $e_b \in T_b$ , $T_a \cup \{e_b\}$ forms a cycle. An edge $e \notin T_a$ is an extension of tree $A$ if $T_a \cup \{ e \}$ also forms a tree. Actually, I am trying to prove that collection of edge sets which induce trees is a connected sub matroid of a graphic matroid. I have proved remaining 3 properties of a connected sub matroid but struck at this one.","['graph-theory', 'matroids', 'trees', 'discrete-mathematics']"
2934625,Fourier transform of a product in $L^1$,"There is a little thing that I do not understand, about the Fourier transform of a product of functions in $L^1$ (and only in this space), with the relation ${\mathcal F}(f g)(\lambda) = \mathcal{F}(f)\star \mathcal{F}(g) (\lambda)$ (and not the easier relation ${\mathcal F}(f \star g)(\lambda) = \mathcal{F}(f)(\lambda)\mathcal{F}(g) (\lambda)$ ). Let us note $\mathcal{F}(f)$ the Fourier transform of a function $f$ , if this transformation exists. I can demonstrate that for any $f\in L^1(\mathbb{R})$ , its Fourier transform exists. I will now consider 2 cases: For any $f,g\in L^1$ with $\mathcal{F}(g)\in L^1$ , therefore I can show $fg\in L^1$ . Thus, $\mathcal{F}(fg)$ exists.
Plus, the inverse Fourier transform of $\mathcal{F}(g)$ exists. And in this case we can write: \begin{eqnarray*}
{\mathcal F}(f g)(\lambda) &= & \int_{-\infty}^{+\infty} e^{-2 i \pi \lambda t} f(t) \int_{-\infty}^{+\infty} e^{2 i \pi u t} \mathcal{F}(g)(u) du dt  \tag{1}\\
&= & \int_{-\infty}^{+\infty} \mathcal{F}(g)(u) \int_{-\infty}^{+\infty} e^{-2 i \pi (\lambda - u) t} f(t) dt du \\
& = & \int_{-\infty}^{+\infty} \mathcal{F}(g)(u) \mathcal{F}(f)(\lambda - u) du \\
&= & {\mathcal F}(f) \star {\mathcal F}(g) (\lambda) .
\end{eqnarray*} From this relation we can deduce that $\mathcal{F}(f)\star \mathcal{F}(g)$ exists, so everyting is okay. In the second case, I want to know if with $f,g\in L^1$ and $fg \in L^1(\mathbb{R})$ , it is possible to prove the same relation: ${\mathcal F}(f g)(\lambda) = \mathcal{F}(f)\star \mathcal{F}(g) (\lambda)$ ? Because the first step in (1) uses the inverse Fourier transform, which is not necessary existing in my second case. I assume that the answer is yes, since I saw some people using this relation with some functions that are in $L^1$ such that there Fourier transforms are not in $L^1$ (if there is not mistakes). But I would like to prove it.","['fourier-analysis', 'lebesgue-integral', 'convolution', 'fourier-transform', 'functional-analysis']"
2934638,Hunt for exact solutions of second order ordinary differential equations with varying coefficients.,"Let $a,a_1,a_2,b \in {\mathbb R}$ . Being inspired by the answer to Solve $y''(x)=[a(x^2-1)^2+b]y(x)$ we found solutions of the following second order ODE : \begin{equation}
\frac{d^2 y(x)}{d x^2} + \left(  a x^4 + a_1 x^2 + a_2 x + b\right) y(x)=0
\end{equation} Indeed if we write: \begin{equation}
y(x) = \exp\left( -\imath \frac{\sqrt{a}}{3} x^3 - \imath \frac{a_1}{2 \sqrt{a}} x\right) \cdot v(x)
\end{equation} the function $v(x)$ satisfies the triconfluent Heun equation https://dlmf.nist.gov/31.12 . We have: \begin{equation}
\frac{d^2 v(u)}{d u^2} + u(u+\gamma) \frac{d v(u)}{d u} + (\alpha u - q) v(u)=0
\end{equation} where \begin{eqnarray}
\gamma &=&  \sqrt[3]{-1} 2^{5/6} \sqrt[6]{a} \sqrt{\frac{a_1}{a}}\\
\alpha &=& 1+\frac{\imath a_2}{2\sqrt{a}} \\
q &=& -\left( \frac{\sqrt[3]{-1} \left(4 \sqrt{2} a^{3/2} \sqrt{\frac{a_1}{a}}+2 i \sqrt{2} a a_2 \sqrt{\frac{a_1}{a}}+4 a b-a_1^2\right)}{4\ 2^{2/3} a^{4/3}}\right)
\end{eqnarray} and \begin{equation}
u:=\frac{(-1)^{1/6}}{2^{1/3} a^{1/6}}\left(x - \imath \sqrt{\frac{a_1}{(2 a)})}\right)
\end{equation} Here is a code snippet that verifies our claim: a =.; a0 =.; a1 =.; a2 =.; b =.; m =.; n = -I Sqrt[a]/
   3; Clear[y]; Clear[u]; Clear[v];
y[x_] = Exp[n x^3] u[x];
myeqn = Collect[(D[
       y[x], {x, 2}] + (a x^4 + a1 x^2 + a2 x + b) y[
        x]) Exp[-n x^3], {u[x], u'[x], u''[x]}, Simplify];
u[x_] = Exp[m x] v[x]; m = -I a1/(2 Sqrt[a]);
myeqn1 = Collect[Simplify[myeqn Exp[-m x]], {v[x], v'[x], v''[x]}, 
   Simplify];
myeqn2 = Collect[
   myeqn1 /. x :> u + I Sqrt[a1/(2 a)] /. v[u + A_] :> v[u] /. 
     Derivative[1][v][u + A_] :> Derivative[1][v][u] /. 
    Derivative[2][v][u + A_] :> Derivative[2][v][u], {u[x], u'[x], 
    u''[x]}, Simplify];
Ab = (-1)^(1/6)/(2^(1/3) a^(1/6));
subst = {u :> Ab u, Derivative[1][v][u] :> 1/Ab Derivative[1][v][u], 
   Derivative[2][v][u] :> 1/(Ab)^2 Derivative[2][v][u]};
Collect[Expand[(Ab^2 myeqn2)] /. subst /. v[Ab u] :> v[u], {v[u], 
  v'[u], v''[u], u^_}, Simplify] Update:
Now let $a$ , $a_0$ , $a_1$ , $a_2$ and $b$ be real numbers. Likewise consider another second order ODE. We have: \begin{equation}
\frac{d^2 y(x)}{d x^2} + \left( \frac{a}{x^4} + \frac{a_0}{x^3} + \frac{a_1}{x^2} + \frac{a_2}{x} +b\right) y(x)=0
\end{equation} Then by writing : \begin{equation}
y(x)= x^{1+\frac{a_0}{2 \imath \sqrt{a}}} \exp\left[\imath \left(\frac{\sqrt{a}}{x} + \sqrt{b} x  \right)\right] \cdot v(x)
\end{equation} The function $v$ satisfies the doubly-confluent Heun equation. We have: \begin{equation}
\frac{d^2 v(u)}{d u^2} + \left( \frac{\delta}{u^2} + \frac{\gamma}{u} + 1\right) \frac{d v(u)}{d u} + \frac{\alpha u-q}{u^2} v(u) = 0
\end{equation} where: \begin{eqnarray}
\delta &=& 4 \sqrt{a b}\\
\gamma &=&2 - \frac{\imath a_0}{\sqrt{a}}\\
\alpha &=& 1-\frac{\imath a_0}{2 \sqrt{a}} - \frac{\imath a_2}{2 \sqrt{b}}\\
q &=& \frac{\imath a_0}{2 \sqrt{a}} + \frac{a_0^2}{4 a}-a_1-2 \sqrt{a b}
\end{eqnarray} and $u:=x/(2 \imath \sqrt{b})$ . The following Mathematica code snippet provides the ""proof"". We have: a =.; a1 =.; a2 =.; b =.; a0 =.; m =.; n =.; p =.; Clear[y]; \
Clear[v]; Clear[m]; x =.;
m[x_] = x^(1 + a0/(2 I Sqrt[a])) Exp[I (Sqrt[a]/x + Sqrt[b] x)] ;
y[x_] = m[x] v[x];
myeqn = Collect[
   Simplify[(D[
        y[x], {x, 2}] + (a /x^4 + a0 /x^3 + a1 /x^2 + a2 /x + b) y[
         x])/m[x]], {v[x], v'[x], v''[x]}, Simplify];
myeqn = Collect[Simplify[myeqn ], {v[x], v'[x], v''[x], x^_}, 
   Simplify];
Ab = 1/(2 I Sqrt[b]);
subst = {x :> Ab x, Derivative[1][v][x] :> 1/Ab Derivative[1][v][x], 
   Derivative[2][v][x] :> 1/(Ab)^2 Derivative[2][v][x]};
Collect[Expand[(Ab^2 myeqn)] /. subst /. v[Ab x] :> v[x], {v[x], 
  v'[x], v''[x], x^_}, Simplify] Finally let $a$ , $a_0$ , $a_1$ , $a_2$ and $b$ be real numbers. Consider the following ODE. We have: \begin{equation}
\frac{d^2 y(x)}{d x^2} + \left( a x^2 + a_0 x + a_1 + \frac{a_2}{x} +\frac{b}{x^2}\right) y(x)=0
\end{equation} Then by writing: \begin{equation}
y(x)=\exp\left( -\frac{\imath}{2\sqrt{a}} x(a_0+a x)\right) \cdot x^{\frac{1}{2}(1+\sqrt{1-4 b})} \cdot v(x)
\end{equation} the function $v$ satisfies the biconfluent Heun equation. We have: \begin{equation}
\frac{d^2 v(u)}{d u^2} -\left( \frac{\gamma}{u} + \delta + u\right)\frac{d v(u)}{d u} + \frac{\alpha u - q}{u} v(u) = 0
\end{equation} where \begin{eqnarray}
\delta &=& -\frac{1}{2}\left( 1-\imath \right) \frac{a_0}{a^{3/4}}\\
\gamma &=& - 1-\sqrt{1-4 b}\\
\alpha &=& \frac{4 a^{3/2} \left(\sqrt{1-4 b}+2\right)+4 \imath a a_1-\imath a_0^2}{8 a^{3/2}}\\
q &=& -\frac{(2+2 \imath) \sqrt{a} a_2+(1-i) a_0 \left(\sqrt{1-4 b}+1\right)}{4 a^{3/4}}
\end{eqnarray} and $u:=(-1)^{1/4} x/(\sqrt{2} a^{1/4})$ . Again we used Mathematica to verify the result: Clear[v]; Clear[y]; a =.; a0 =.; a1 =.; a2 =.; b =.; A =.; d =.; \
Clear[m]; Clear[y]; Clear[v];

m[x_] = E^(-((I x (a0 + a x))/(2 Sqrt[a]))) x^(
   1/2 (1 + Sqrt[1 - 4 b]));
y[x_] = m[x] v[x];
ll = Collect[
   Simplify[(D[
        y[x], {x, 2}] + (a x^2 + a0 x + a1 + a2/x + b/x^2) y[x])/
     m[x]], {v[x], v'[x], v''[x]}, Simplify];
ll = Collect[
   Simplify[ll/Coefficient[ll, v''[x]]], {v[x], v'[x], v''[x], x^_}, 
   Simplify];
Ab = (-1)^(1/4)/(Sqrt[2] a^(1/4));
subst = {x :> Ab x, Derivative[1][v][x] :> 1/Ab Derivative[1][v][x], 
   Derivative[2][v][x] :> 1/(Ab)^2 Derivative[2][v][x]};
ll1 = Collect[
  Ab^2 (ll /. subst /. v[Ab x] :> v[x]), {v[x], v'[x], v''[x], x^_}, 
  Simplify] Now my question would be twofold. Firstly, is there any mathematical software which can handle confluent Heun functions (just as Mathematica handles hypergeometric functions for example).
Secondly, can we actually find similar solutions (i.e. map our ODE onto hte Heun equation) in the case when the coefficient at the function $y(x)$ in the ODE is an arbitrary polynomial of order strictly bigger than two ?","['special-functions', 'ordinary-differential-equations']"
2934650,Competition Diophantine Equation,"I am looking for the number of integer solutions to this system of equations $$a^2+b^2+c^2+d^2=2500$$ and $$(a+50)(b+50)=cd$$ I tried moving terms around in the first equation and using the difference of two squares to try and gain some information, but came up empty. I was wondering what the best way to attack this problem is. Should I take the equations mod something, or should I try and place some bounds on the variables and try to derive the number of solutions from that? I ask that y'all only give me hints . Thanks","['elementary-number-theory', 'algebra-precalculus', 'polynomials', 'diophantine-equations']"
2934675,When is a positive power of $f(x)$ integrable?,"I want to ask the complement to this question ( Lebesgue-integrability of roots and powers of a function ).
Suppose $f$ is a Lebesgue measurable function and in particular that $f(x) \geq 0 \: \forall x \in \mathcal{X}$ and $\int_{\mathcal{X}} f(t) dt = 1$ , i.e., $f$ is a probability density function (pdf). I want to know what are the conditions I need on $f$ such that $\int_{\mathcal{X}} f(t)^p dt < \infty$ for $ 0 < p < \infty$ . Edit Following feedback from @Dog_69 (!) and @Clement C. and some digging on my own, here are a few more observations I hope will be useful.
Let $\{g_n\}$ be ""a sequence of nonnegative simple functions"". According to this ( http://mathworld.wolfram.com/LebesgueIntegrable.html ) we would need the following two conditions on $f(x)^p$ : $\sum_{n=1}^\infty\int_{\mathcal{X}} g_n(t) dt < \infty$ , and; $f(x)^p = \sum_{n=1}^\infty g_n(x)$ a.e. Now, if one could frame the counter-examples to my foolish claim (edited out) that the function is integrable for $ 0 <p \leq 1$ by showing that they do not conform to these conditions, it would be a start, I reckon. More edits According to this link Product of measurable and integrable functions , if $f$ is integrable and $g$ is bounded and measurable, then $fg$ will be integrable. Since here $f(x)$ is assumed  to be integrable, a sufficient condition is that $g(x) = f(x)^{p-1}$ is bounded and measurable. I had erroneously claimed that for $ 0 < p < 1$ any pdf would integrable. Let us work on the counter-examples kindly provided by @Dog_69 and @Clement C. to find out what went south: $f_0(x) = \frac{2}{x^2}$ with $\mathcal{X} = [2, +\infty)$ . Take $p = 1/2$ and thus $f_0(x)^{1/2} = \frac{2}{x^2} \cdot \frac{x}{\sqrt{2}}$ . Since $g_0(x) = \frac{x}{\sqrt{2}}$ is not bounded on $\mathcal{X}$ we have that it doesn't conform to the condition above. $f_1(x) = \frac{1}{\pi(1+x^2)}$ with $\mathcal{X} = (-\infty, +\infty)$ . For simplicity, again take $p = 1/2$ and hence we have $f_1(x)^{1/2} = \frac{1}{\pi(1+x^2)} \cdot \sqrt{\pi (1+x^2)}$ . Again, $g_1(x) = \sqrt{\pi (1+x^2)}$ is not bounded on $\mathcal{X}$ . If one could show that the result in the link is an iff statement, we'd be done, because we'd have a necessary and sufficient condition on $f$ . But unfortunately this is not the case. Consider $\int_{0}^{+\infty} t^2 h(t) dt$ where $h$ is, say, a Gamma pdf. Clearly this expectation exists, but $g(t) = t^2$ is measurable but unbounded on $\mathcal{X}$ .","['calculus', 'lebesgue-integral', 'measure-theory', 'probability']"
2934676,Miscalculating the determinant,"I am learning linear algebra and am getting stuck when trying to calculate the determinant using elementary row operations. Consider the matrix A. \begin{vmatrix}
0 & 1 & 2 & 3 \\
1 & 1 & 1 & 1 \\
-2 & -2 & 3 & 3 \\
1 & 2 & -2 & -3 \\
\end{vmatrix} According to the solution in my textbook and Matlab the determinant should be 10. I however find -20. Here is what I did.
I first interchanged row 1 and row 3. \begin{vmatrix}
1 & 2 & -2 & -3 \\
1 & 1 & 1 & 1 \\
-2 & -2 & 3 & 3 \\
0 & 1 & 2 & 3
\end{vmatrix} I then substracted row 1 from row two. I also added the first row twice to the third row. \begin{vmatrix}
1 & 2 & -2 & -3 \\
0 & -1 & 3 & -2 \\
0 & 2 & -1 & 9 \\
0 & 1 & 2 & 3
\end{vmatrix} Then, I added the second row twice to the third row and once to the fourth row. \begin{vmatrix}
1 & 2 & -2 & -3 \\
0 & -1 & 3 & -2 \\
0 & 0 & 5 & 5 \\
0 & 0 & 5 & 1
\end{vmatrix} My final operation was to substract the third row from the fourth row, which gave: \begin{vmatrix}
1 & 2 & -2 & -3 \\
0 & -1 & 3 & -2 \\
0 & 0 & 5 & 5 \\
0 & 0 & 0 & -4
\end{vmatrix} Finally, I calculated the determinant: $(-1)^1 \cdot 1 \cdot -1 \cdot 5 \cdot -4 = -20$ The $(-1)^1$ is there since I did one operation in which I interchanged two rows.
I would really appreciate if you could tell me what I did wrong. Martijn","['determinant', 'linear-algebra']"
2934694,Proof of limit of exponential function,"I was doing a proof that for $a\in(0,1)$ $$\lim_{x\to\infty}a^x=0$$ Proof For arbitrary $\epsilon>0$ we wish to find $x_0\in \mathbb{R}$ such that $\forall x\in \mathbb{R}$ if $x>x_0$ then $|a^x|<\epsilon$ . By some analysis we set $x_0= \frac{\ln{\epsilon}}{\ln{a}}$ . Now, if we take $x>x_0$ we have \begin{align*}
x>\frac{\ln{\epsilon}}{\ln{a}}\\
x\ln{a}<\ln{\epsilon}\\
\ln{a^x}<\ln{\epsilon}\\
a^x=|a^x|<\epsilon
\end{align*} Here i saw that the fact that $a\in (0,1)$ helps to convert the $<$ to $>$ . Now, obviously for $a>1$ this shouldn't hold, so in this case i wanted to show that $\lim_{x\to\infty}a^x\neq0$ that is there exists $\epsilon >0$ such that for any $x_0\in \mathbb{R}$ there is $x\in \mathbb{R}$ we have both $x>x_0$ and $a^x\geq \epsilon$ . Now, i have no idea what should i pick, it seems reasonable to me that $\epsilon$ should somehow depend on $x_0$ but I can't do that according to the order of the quantifiers. So, maybe take $\epsilon = 1/2$ . Now consider arbitrary $x_0$ , we wish to find some $x$ such that if $x>x_0$ we get that $a^x\geq \epsilon$ . We know that $a^x$ is always positive. Take $x=x_0+1$ to obtain $$a^x=a^{x_0+1}=a^{x_0}+a\geq 0+a>1>1/2>\epsilon$$ I am wondering, if this is the correct approach and possible if there can be something more elegant done? Thanks","['alternative-proof', 'limits', 'proof-verification', 'real-analysis']"
2934707,"Understanding why f(z) can be differentiable, but not analytic","I'm working through a practice exam and can only do part of this problem. Let $f(z) = x^3 + iy^3$ . Find all points where the Cauchy-Riemann equations are satisfied, and give a brief explanation of why $f(z)$ is differentiable but not analytic at these points. I can do the first part: $u(x,y)=x^3$ and $v(x,y)=v^3$ $u_x=3x^2$ , $v_y=3y^2$ $u_y=0, v_x=0$ So $u_y=-v_x$ , but the Cauchy-Riemann equations are only satisfied, that is $u_x=v_y$ or $3x^2=3y^2$ when $x=y$ . How do I address being differentiable and not analytic?","['complex-analysis', 'analyticity']"
2934708,"$(X,d)$ is compact if each $(X,d_k)$ is compact","For each $k\in \mathbb N$ , let $d_k$ be a metric on $X$ such that $d_k(x,y)\leq 1$ for all $x,y\in X$ . Define a new metric, $$d(x,y)=\sum_{k\geq 0}2^{-k}d_k(x,y)$$ Now show that if $(X,d_k)$ is compact for every $k$ then so is $(X,d)$ . Here is my attempt - let $(x_n)$ be an infinite sequence, I have extracted, through a diagonal argument, a subsequence $(x_{nn})$ which converges to, say, $y_k$ in the metric $d_k$ , for every $k\in \mathbb{N}$ . Now I do not understand how to proceed from here. This question may have been already asked before, but I could not find it.","['metric-spaces', 'compactness', 'sequences-and-series']"
2934724,How to transform the PDE?,"I am doing the following problem: Transform the equation $$y\left(\frac{\partial z}{\partial x}\right) - x \left(\frac{\partial z}{\partial y} \right) = (y-x)z$$ by introuducing new independent variables: $$u = x^2 + y^2, v = \frac{1}{x} + \frac{1}{y}$$ and a new function: $$w = \ln(z) - (x + y)$$ I did everything as it is in the example I have (system of first order diferentials) and I got to this equation as the new one: $$\left(\frac{xz}{y^2}- \frac{yz}{x^2}\right) \frac{\partial w}{\partial v} = 0$$ The solutions say this is the result: $\frac{\partial w}{\partial v} = 0$ I checked multiple times what I've done and I cannot find a mistake. Am I allowed to divide by the term in the braces and treat it as a constant that's different from 0? If I can, why? It doesn't make sense to me. If we try to write it in terms of v and w, we cannot divide it at all. How to do this properly and is there another way at approaching it?","['partial-derivative', 'multivariable-calculus', 'partial-differential-equations']"
2934741,"How to solve these $3$ equations for three unknowns $x$,$y$,$z$? [duplicate]","This question already has answers here : Solve system of simultaneous equations in $3$ variables: $x+y+xy=19$, $y+z+yz=11$, $z+x+zx=14$ (2 answers) Closed 5 years ago . Question: Solve: $xy+x+y=23\tag{1}$ $yz+y+z=31\tag{2}$ $zx+z+x=47\tag{3}$ My attempt: By adding all we get $$\sum xy +2\sum x =101$$ Multiplying $(1)$ by $z$ , $(2)$ by $x$ , and $(3)$ by $y$ and adding altogether gives $$3xyz+ 2\sum xy =31x+47y+23z$$ Then, from above two equations after eliminating $\sum xy$ term we get $$35x+51y+27z=202+3xyz$$ After that subtracting $(1)\times 3z$ from equation just above (to eliminate $3xyz$ term) gives $$35x +51y-3z(14+x+y)=202\implies (x+y)[35-3z]+16y-42z=202$$ I tried pairwise subtraction of $(1),(2)$ and $(3)$ but it also seems to be not working. Please give me some hint so that I can proceed or provide with the answer.","['contest-math', 'algebra-precalculus', 'systems-of-equations']"
2934777,Trouble understanding coordinate systems as charts on differentiable manifolds.,"I had a question involving the notion of coordinates. Here, I will use polar coordinates on $\mathbb{R}^2$ as an example. From my admittedly lacking understanding of differential geometry, polar coordinates form a chart on the differential manifold formed by the punctured plane $\mathbb{R}^2 \setminus \{(0,0)\}$ . Now, my understanding of a chart is that it is a homeomorphism from an open subset $U$ of the manifold $M$ to an open subset of a Euclidean vector space that effectively assigns local coordinates to $U$ . This is where my confusion arises. The usual assignment of polar coordinates to $\mathbb{R}^2 \setminus \{(0,0)\}$ is something like $(x,y) = (r\cos(\theta),r\sin(\theta))$ , but the definition of a chart from above suggests that it should instead be something like $(r,\theta) = \big(\sqrt{x^2+y^2},\arctan\big({\frac{y}{x}}\big)\big)$ . And yet, we are all familiar with the fact that the former assignment is used to calculate objects like the Jacobian matrix in polar coordinates and, consequently, the Euclidean metric in polar coordinates. My question is, why the discrepancy? I apologize if I am not explaining this well enough, I will do my best to clarify further if need be.","['coordinate-systems', 'polar-coordinates', 'differential-geometry']"
2934821,How many ways can a number $n>2$ be expressed as the difference of two squares?,"I provide my answer to the title question, which I pursue purely out of my own curiosity. My question to the community is: Have I thought this through correctly, or have I made one or more mistakes? It is identically true algebraically, $$\Bigl(\frac{\frac{n}{t}+t}{2}\Bigr)^2-\Bigl(\frac{\frac{n}{t}-t}{2}\Bigr)^2=n$$ when $t\mid n$ and the following conditions hold: $\frac{n}{t}>t$ and $\frac{n}{t}\equiv t\mod{2}$ . The first condition is necessary to ensure that both LHS terms inside the brackets will be positive, and the second condition is necessary to ensure that each LHS numerator is even, so that each LHS fraction is an integer. My first conclusion is: The number of ways to express $n$ as the difference of two squares will simply be equal to the number of examples of $t$ that meet the stated conditions. Let $n=p_1^{a_1}p_2^{a_2}\cdots p_m^{a_m}$ . Then for any factor $t$ of $n$ , $t=p_1^{b_1}p_2^{b_2}\cdots p_m^{b_m}$ where $0\le b_i\le a_1$ (with the stipulation that $a_i>0$ ). Each prime factor $p_i$ may appear from $0$ to $(a_i)$ times in various $t$ , giving $(a_i+1)$ choices, so the total number of factors $\tau(n)=(a_1+1)(a_2+1)\cdots(a_m+1)$ . But not all factors will meet the required conditions. I note that a question has been posed and answered here about a very limited instance of my question, where every $p_i$ is odd and every $a_i=1$ . The first condition, $\frac{n}{t}>t$ , requires that $t$ be strictly smaller than $\sqrt{n}$ . This is in general easy to accommodate because for every factor $t<\sqrt{n}$ there is a corresponding factor $\frac{n}{t}>\sqrt{n}$ . So the number of factors smaller than $\sqrt{n}$ is just $\frac{\tau(n)}{2}$ with one caveat. If $n$ happens to be a perfect square, then there will be one factor $t=\frac{n}{t}=\sqrt{n}$ , in which case the number of factors would be $\frac{\tau(n)-1}{2}$ . This ambiguity can be addressed using the floor function: the number of factors that comply with the first requirement can be expressed as $\lfloor \frac{\tau(n)}{2}\rfloor$ . Next, accommodation must be made for the second requirement $\frac{n}{t}\equiv t\mod{2}$ . In effect, this means that if $n$ contains factors of $2$ within it, they may not all be included in, or omitted from, $t$ . If $n=2^xq$ where $q$ is odd, then with respect to the exponent of $2$ we must have $1\le b\le (x-1)$ . There are two fewer options (i.e. $0$ and $x$ ) to choose from. So if $n$ is even, we must multiply our previous result (i.e. $\lfloor \frac{\tau(n)}{2}\rfloor$ ) by $\frac{x-1}{x+1}$ to replace the incorrect multiplier $(x+1)$ with the correct value $(x-1)$ . Note that if $2$ is present in $n$ only once, the correction factor $\frac{x-1}{x+1}=0$ . This correctly shows that any number which is twice an odd number (including the number $2$ itself) cannot be expressed as the difference of two squares. This fact is independently known because $2(2j+1)=4j+2\equiv 2 \mod{4}$ which cannot be the difference of two squares. I conclude that the number of ways that a number $n>2$ can be expressed as the difference of two squares is $\lfloor \frac{\tau(n)}{2}\rfloor$ if $n$ is odd, and $\lfloor \frac{x-1}{x+1} \cdot \frac{\tau(n)}{2}\rfloor$ in $n$ is even, where $x$ is the exponent of $2$ in the prime factorization of $n$ .","['number-theory', 'elementary-number-theory']"
2934836,Solving $xf^{\prime\prime}= 4f^{\prime}- 25x^9 f$,"Can someone explain to me how to solve the following differential equation, $$xf^{\prime\prime}= 4f^{\prime}- 25x^9 f \qquad  \text{with initial conditions}   \ f(0)=0, \ \ f^{'}(1)=1 $$ There is a hint which asks me to make the substitution, $t=x^5$ . I really am a total novice in differential equations and my only attempt has been to write the function as a power series and try to guess the coefficients. It doesn't seem very inspiring though.",['ordinary-differential-equations']
2934838,Can a field be canonically reconstructed from its multiplicative group?,"I understand that there are a lot of restrictions on which abelian groups arise as the multiplicative groups of fields. My question is kind of the adjoint: Suppose I pick a field $k$ , but I do not tell you what it is. I give you an abelian group $G$ and tell you that $G\cong k^\times$ . But you only receive $G$ as an abstract group; that is, $G$ does not come with the natural action on $(k, +)$ or any other information inherited 
  from $k$ . Given $G$ , can you canonically define a field $\hat k$ such that $\hat k \cong k$ ? I would not be surprised at all if the answer were ""no."" But as I'm not convinced either way, I optimistically attempted a natural construction. I first observed that even though we don't know anything about $k$ as a set, we have a 1-1 correspondence between $G$ and the nonzero elements of $k$ . We need to invent a additive neutral element and also somehow produce an addition operation compatible with multiplication. Let $(\Sigma, +)$ be the free abelian group generated by the elements of $G$ ; $\sigma: G\to \Sigma$ is the identification of $g \in G$ as $\sigma(g) \in \Sigma$ . At this point $\Sigma$ contains the desired $(k, +)$ as a subgroup, although it is way too big. Continuing in the freest possible way, define: $R = G \otimes_\mathbb{Z} \Sigma$ . $R$ is naturally distributive, but it is far from having the full additive structure of $k$ . Let $P$ be the ideal generated by all elements $\left\{g\otimes\sigma(g')-gg'\otimes\sigma(1)\mid g, g \in G\right\}$ . Set $S=R/P$ . By allowing natural distributivity and 0 compatible with $\otimes$ , $S$ is a ring with multiplication defined by $(g\otimes 1)(g'\otimes 1) = gg'\otimes 1$ . And obviously $S$ contains $k$ , but it also has so much other contents and structure that I think it's not even a field. Can I get any pointers on what to do next (quotient? localize?)? Or if my objective is impossible, I'd like to see where this attempt breaks down.","['vector-spaces', 'field-theory', 'abstract-algebra', 'adjoint-functors', 'abelian-groups']"
2934844,$A^TA=B^TB$. Is $A=QB$ for some orthogonal $Q$?,"Suppose that $A$ and $B$ are two real square matrices and $A^TA=B^TB$ . Can we say that $A=QB$ for some orthogonal matrix $Q$ ? If they are vectors we have $\|a\|^2=a^Ta=b^Tb=\|b\|^2$ , so intuitively clear, since we just have to rotate. But it is hard to picture the matrix case but I have not been able to show.","['orthogonal-matrices', 'linear-algebra']"
2934857,Help understanding proof of the following statement $E(Y) = \sum_{i = 1}^{\infty} P(Y \geq k)$,"If Y is a discrete random variable that assigns positive probabilities to only the positive integers, show that $E(Y) = \sum_{i = 1}^{\infty} P(Y \geq k)$ Where $E(Y)$ is the expected value (or mean) of Y. This is a text book question and I'm having trouble understanding the solution as they don't give any worded explanations. Here is the solution: 1 $\hspace{1.4cm}\sum_{i = 1}^{\infty} P(Y \geq k)$ 2 $\hspace{1.4cm} = \sum_{k = 1}^{\infty}\sum_{j = k}^{\infty} P(Y=k)$ 3 $\hspace{1.4cm} = \sum_{k = 1}^{\infty}\sum_{j = k}^{\infty} P(j)$ 4 $\hspace{1.4cm} = \sum_{j = 1}^{\infty}j\cdot P(j)$ 5 $\hspace{1.4cm} = \sum_{y = 1}^{\infty}y\cdot P(y) = E(Y)$ I'm not sure how to interpret the inequality $Y \geq k$ inside the probability function, does it read ""values k assigned to the random variable $Y$ such that $k$ is less than or equal to $Y$ ? But how can a value $k$ be less than a random variable $Y$ that doesn't take on any values? I'm also very confused about steps 1 to 2, and 3 to 4. (1 to 2): I don't understand how they broke down the inequality inside the probability function. I think the nested summations are making this complicated for me to understand. Sorry for the broad question and wordy post. Any quick short explanation of anything will be very much appreciated.","['statistics', 'summation', 'expected-value', 'inequality', 'probability']"
2934862,Taylor approximation of cos function,"I have the following problem: Knowing the linear approximation of the Taylor approximation of the form(1): $$
f(x_{0} + \Delta x) \approx f(x_{0}) + f'(x_{0}) \Delta x
$$ I have to determine the linear approximation(2) $$\cos (32) = \cos(30^{\circ} + 2^{\circ}) = cos(\frac{\pi}{6}+\frac{2\pi}{180})$$ with the help of (3) $$\cos(30^{\circ}) = \cos( \frac {\pi}{6}) = \frac{\sqrt{3}}{2}$$ The answer is: (4) $$\cos(32^{\circ}) = cos(\frac{\pi}{6}+\frac{2\pi}{180})$$ (5) $$\approx \cos(\frac{\pi}{6}) - \sin(\frac{\pi}{6})\frac{\pi}{90}$$ (6) $$= \frac{\sqrt{3}}{2}-\frac{2\pi}{180}$$ Which (trigonometric?) rule allows us to pass from (4) to (5)?","['approximation', 'trigonometry', 'taylor-expansion']"
2934887,1 Dimensional ODE with solution in $L^2$,"For part of a bigger question, I need either a solution or a counterexample to the following 1 dimensional ODE. Let $h\in L^2(0,\infty)$ (complex valued functions) and $r \in (0,\infty)$ . Under boundary conditions $w(0) = c_1a + c_2$ and $w'(0) = c_3a + c_4$ where the $c_i$ are complex constants and $a$ is a complex parameter, is there an $a \in \mathbb{C}$ for which there is a solution in $H^2(0,\infty)$ for: $$w''(t) = irw(t) - h(t) \ \ \ (t\in (0,\infty))?$$ The strange boundary conditions are just my way of saying that there is one parameter I want to solve for if a solution exists in order to couple this ODE with another one on the negative real line. If I use Laplace transforms, this is equivalent to asking if there exists $w \in H^2(0,\infty)$ such that $$\hat{w} = \frac{\hat{h}}{ir-s^2} - \frac{w(0)+sw'(0)}{ir - s^2}.$$ There is a solution to this ODE by taking the inverse Laplace transform of the above equation, namely $$ w(t) = -\frac{1}{\sqrt{ir}}\int_0^t h(u)\sinh(\sqrt{ir}(t-u))du + \frac{w(0)}{\sqrt{ir}}\sinh(\sqrt{ir}t) + w'(0)\cosh(\sqrt{ir}t).$$ However, this is not obvious to me whether or not there is an $a$ for which it's in $L^2(0,\infty)$ . Is there another solution to this ODE that is in $L^2(0,\infty)$ or if not, is there an explicit counterexample? In other words, given $h\in L^2(0,\infty)$ , are there nonhomogeneous boundary conditions of the form aforementioned that give me a solutions $w\in H^2(0,\infty)$ .","['analysis', 'functional-analysis', 'ordinary-differential-equations']"
2934902,Problem 30 from Shakarchi Stein's book,"If $E$ and $F$ are measurable sets with $m(E)>0$ and $m(F)>0$ . Prove
that $E+F$ contains some interval. I know that this problem is very popular in MSE and I found many topics but most of the solutions use Fourier transform, Lebesgue density theorem which I am not familiar yet. I know the following two facts are true: If $\alpha\in(0,1)$ and $m_*(E)>0$ then exists an open interval $I$ such that $m_*(E\cap I)\geq \alpha m_*(I)$ . 2)If $E\subset  \mathbb{R}$ with $m(E)>0$ then $E-E$ contains some neighborhood of zero. My professor said that above problem could be solved via these problems. I was thinking on this problem about a week but no results. However, I have ideas: Let's $\alpha=\frac{9}{10}$ then exists open intervals $I$ and $J$ such that $m(E\cap I)\geq \frac{9}{10}m(I)$ and $m(F\cap J)\geq \frac{9}{10}m(J)$ . I had idea to shift one of the intervals, say $I+a\subset J$ WLOG. But this did not give any good results. Anyway, the idea if shifting may not work if $I=(-\infty,a)$ and $J=(b,+\infty)$ . I would be very grateful if somebody will show how to solve this problems using my ideas. And please do not close this topic because other topic on this problem have quite advanced solutions and I would like to see more simpler using the ideas which I provide.","['measure-theory', 'real-analysis']"
2934903,Integral $\int_0^1 \ln\left(\frac{1-x}{1+x}\right)\ln\left(\frac{1-x^2}{1+x^2}\right)\frac{dx}{x}$,"Greetings I saw here (among the last integrals) that: $$\int_0^1 \ln\left(\frac{1-x}{1+x}\right)\ln\left(\frac{1-x^2}{1+x^2}\right)\frac{dx}{x}=\pi C$$ Where $C$ is Catalan's constant. Did this integral appear here before? (my quick search did not found anything). I gave it a try and got stuck. Denoting the integral as $I$ and using that $\ln \left(\frac ab\right)=\ln a- \ln b\ $ we have: $$I=K(1,1)-K(1,-1)-K(-1,1)+K(-1,-1)$$ Where $$K(a,b)=\int_0^1\frac{\ln(1+ax)\ln(1+bx^2)}{x}dx$$ Differentiating under the integral sign: $$\frac{\partial^2}{\partial a \partial b}K(a,b)=\int_0^1 \frac{x^2}{(1+ax)(1+bx^2)}\,dx$$ By partial fractions we get: $$\frac{1}{a^2+b}\left(\int_0^1 \frac{ax}{bx^2+1}\,dx -\int_0^1 \frac{1}{bx^2+1}\,dx +\int_0^1 \frac{1}{ax+1} \,dx\right)$$ $$=\frac{1}{a^2+b}\left(\frac{a\ln(1+b)}{2b}-\frac{\arctan \left(\sqrt b\right)}{\sqrt{b}} +\frac{\ln(1+a)}{a}\right) $$ And now since $K(0,b)=K(a,0)=0$ $$K(a,b)=\frac12\int_0^a \int_0^b \frac{x\ln(1+y)}{y(x^2+y)}\,dy\,dx-\int_0^a \int_0^b \frac{\arctan \left(\sqrt y\right)}{\sqrt{y}(x^2+y)}\,dy\,dx +\int_0^a \int_0^b \frac{\ln(1+x)}{x(x^2+y)}\,dy\,dx$$ Is there a clever way to solve this?
Another way is to start by using: $$-\frac12\ln\left(\frac{1-x}{1+x}\right)=\sum_{n=1}^\infty \frac{x^{2n+1}}{2n+1}$$ $$I=4\sum_{n,k=1}^\infty \frac{1}{(2n+1)(2k+1)}\int_0^1 x^{4n+2k+2}\,dx=4\sum_{n,k=1}^\infty \frac{1}{(2n+1)(2k+1)(4n+2k+3)}$$ But I dont know how to deal with this series. I would appreciate some help with this integral!","['integration', 'definite-integrals', 'closed-form']"
2934906,Explanation of spectral theorem for reals to high schoolers who have only done computational single-variable calculus.,"This is a shot in the dark and a pretty tall order, but I am wondering if anybody could give a good explanation of the spectral theorem for the reals to high schoolers who have only seen computational calculus of one variable and have not taken a linear algebra course before? An overview of the proof, why does one care about the result, what is the result, how to visualize it, etc. I've tried myself to explain to high school students this result but failed, so maybe there is someone out there who is a better teacher who can do this.","['proof-explanation', 'vector-spaces', 'calculus', 'linear-algebra', 'intuition']"
2934915,Can I simplify $n\cdot r\cdot \sin(90^\circ-\frac{180^\circ}{n})\sqrt{r^2-r^2\sin^2(90^\circ-\frac{180^\circ}{n})}$ further?,"I need to write a simplified formula for this: $$A_i = n\cdot r\cdot \sin\left(90^\circ-\frac{180^\circ}{n}\right)\sqrt{r^2-r^2\sin^2\left(90^\circ-\frac{180^\circ}{n}\right)}$$ I am not very confident that I know enough trigonometry identities to simplify this completely.  Other than converting $\sin(90^\circ-x)$ to $\cos(x)$ , I am not sure there isn’t anything I’m missing, with the radical sign in there and everything. $n$ and $r$ are variables and natural numbers. $A_i$ is a value based on $n$ and $r$ . This is in degrees, if that wasn’t clear.","['algebra-precalculus', 'trigonometry']"
2934919,Convergence in measure and boundness implies weak convergence in $L^p$,"Let $(f_n)_{n=1}^\infty$ be a sequence in $L^p ([0,1]), 1\leq p<\infty$ . Suppose that $f_n\rightarrow f$ in measure and that $\sup\limits_{n\in\mathbb{N}} \|f_n\|<\infty$ . Show that $f_n\rightarrow f$ in the weak topology of $L^p([0,1])$ . As a reminder: a net $\{g_\alpha\}_{\alpha\in I}$ converges in the weak topology of $X$ if and only if $\phi(g_\alpha)\rightarrow \phi(g)$ for all $\phi\in X^*$ . We know convergence in measure and boundness in the $L^p$ norm gives us convergence in $L^p$ . I think I could use the Dominated Convergence Theorem but I'm not sure how to proceed. I also found a proof of this Theorem but with the hypothesis $f_n\rightarrow f$ a.e., and they use Egorov's theorem, but I rather not use it. I thank any suggestion you have.","['measure-theory', 'weak-convergence', 'lp-spaces', 'functional-analysis', 'convergence-divergence']"
2934940,A multiple choice question concerning first order linear differential equations,"Consider the initial value problem $$y'(t)=f(t)y(t)$$ with $y(0)=1$ where $f:\mathbb{R}\rightarrow \mathbb{R}$ is a  continuous function. Then this initial value problem has (1) Infinitely many solutions for some $f$ (2)a unique solution in $\mathbb{R}$ (3) no solution in $\mathbb{R}$ for some $f$ (4) a solution in an interval containing $0$ , but not on $\mathbb{R}$ for some $f$ My efforts $$\frac{dy(t)}{y(t)} = f(t)dt$$ $$\log y(t) = \int_0^tf(t)dt +C$$ Now at $t=0$ $y=1$ so we get $$0=0+C$$ So $$y(t)=\exp(\int_0^tf(t)dt)$$ So either $2$ is true or $4$ is true? I am not able to go further from here. What logic should I use now Edit: Since $\int_{0}^{t} f$ is continuous and $e^x$ is continuous and composition of continuous function is continuous so $2$ is true. Is this a correct way of thinking?","['initial-value-problems', 'ordinary-differential-equations']"
2934970,Rational numbers are not locally compact,"I'm trying to show that $\mathbb Q$ is not locally compact using this definition: So I need to show that there is some point $x\in \mathbb Q$ such that no matter what neighborhood of $x$ in $\mathbb Q$ I take, no compact subset of $\mathbb Q$ can contain it. Any neighborhood of $x$ in $\mathbb Q$ is of the form $(a,b)\cap \mathbb Q$ where $x\in (a,b)$ . But I think my problem is that I don't understand/feel how compact sets in $\mathbb Q$ look like (except finite sets). If there is a compact subset of $\mathbb Q$ containing $(a,b)\cap \mathbb Q$ , what does it contradict to?",['general-topology']
2934973,Pre-calculus Complex coordinate law of cosine help,"Let $\theta = \angle BAC$ . Then we can write $\cos \theta = \dfrac{x}{\sqrt{2}}$ Find x. Currently, that is all I have. I think I should use Law of cosine. Where should I progress?","['trigonometry', 'algebra-precalculus', 'geometry', 'complex-numbers']"
2934979,Count how many different final position are there?,"A frog started from the origin of the coordinate plane and made three jumps. Each time the frog jumped a distance of 5 units and landed at a point with integer coordinate. How many different possibilities of the final position of the frog are there? I've tried to write some programs to calculate it, on khanAcademy: https://www.khanacademy.org/computer-programming/speed-optimization-of-happy-frog-jumping-everywhere/5425374405623808 and the result of each jumps is given like this: 12 73 264 553 916, also I notice that the final position of the frog forms a decagon on the coordinate plane. Thus, I know that the answer is 264, but I want to know what is the method to count it by hand?","['coordinate-systems', 'combinatorics']"
2935026,Is there a simple explanation to why the line of best fit passes through $\bar x$ and $\bar y$?,"Is there a clear explanation someone can give an undergrad as to why a line of best fit in a linear model must always pass through a point/coordinate indicating the mean of the $x$ and $y$ values ( $\bar{x}$ , $\bar{y}$ ) and why the sums of the least squares must equal $0$ ? It is a hard concept I have not been able to grasp… Thank you.","['linear-regression', 'statistics']"
2935047,Alternate Way of Computing Complex Polynomial?,I'm computing the value of this polynomial: $$\left(\frac{2}{z}+\frac{z}{2}\right)^2+2$$ Where $z = -1 + \sqrt{3}i$ I converted to polar $z = 4e^{i5\pi/6}$ to grind out the first term then converted back to cartesian so I could add. I was wondering if there was a faster way of simplifying this expression. I ask because its a question on an old timed exam.,['complex-analysis']
2935053,Number of distinct total orders on a set,"This is from a class question, not looking for someone to do homework for me, just seeking clarification. Let the set X=(a,b,c) How many distinct total orders can be defined on this set? Why? I did search around a bit and from here , someone said that for a set of N elements the number of total orders is N! My question is: is this correct, and why? My class notes define a ""total order"" as a partial order which is comparable, with a partial order being a relation that is Reflexive, Transititve and Antisymmetric. I get all that, but I'm confused as to how this relates to the question given.","['elementary-set-theory', 'order-theory']"
2935059,Bifurcation of time dependent parameters,"Consider the following differential equation, $$\frac{du}{dt}=w+u-u^3.$$ Suppose that the parameter changes slowly in time depending on the value of $u$ . That is, consider the system of equations $$\frac{du}{dt}=w+u-u^3.$$ $$\frac{dw}{dt}=-\epsilon u,$$ where $\epsilon>0$ is very small. Using your bifurcation diagram from part a, sketch what a solution looks like for small $\epsilon$ . So I have a graph of my bifurcation diagram of a time-independent parameter right here and now I am tasked with considering this time-dependent parameter. I'm very new to this. Previously I've learned how to work with bifurcations through the MATCONT program in MATLAB, but I don't think there's a way I can set my parameter as a function of $t$ . I need help on how to work with this problem.","['bifurcation', 'ordinary-differential-equations']"
2935073,The binary expansion with coefficients equal to Bernoulli random variables,"Let $X_1, X_2, . . .$ be i.i.d. Bernoulli(p) random variables with $p \ne 1/2$ . Let $Y =\sum_{i=1}^{\infty}X_i/2^i$ . Show that there is a set A of Lebesgue measure $0$ so that $\mathbb P(Y\in A)=1$ . I am thinking about using the second Borel-Cantelli Lemma to approach it, by finding a sequence $A_n$ with $\mathbb P(A_n)$ with the sum $\sum_{i=1}^{\infty} \mathbb P(A_n)=\infty$ and $Y^{-1}(A)=\limsup(A_n)$ . Unfortunately I couldn't use the condition $p\ne 1/2$ . I also saw a hint somewhere: Try to find a set like that using the Strong Law of Large Numbers. It might be helpful to consider the sequence of the coefficients of the binary expansion. But I can't figure out the connection between the LLN (about the convergence of the average of i.i.d of r.v.'s) and this problem. Thanks in advance!",['probability-theory']
2935115,Let $\mathcal F$ be the set of mappings $f:\Bbb N \to \Bbb N$ for which $f(m) \ge f(n)$ for $m \le n$. Show that $\mathcal F$ is countable,"Let $\mathcal F$ be the set of mappings $f:\Bbb N \to \Bbb N$ for which $f(m) \ge f(n)$ for $m \le n$ . Show that $\mathcal F$ is countable. My attempt: For all $f\in \mathcal F$ , $f$ will be eventually constant, i.e. there exists $N\in \Bbb N$ such that $f(n)=f(N)$ for all $n>N$ . Let $N_f$ be the least element of such $N$ for each $f\in \mathcal F$ . Let $\operatorname{Seq}(\Bbb N)$ be the set of all finite sequences from $\Bbb N$ . We define a mapping $G:\mathcal F \to \operatorname{Seq}(\Bbb N)$ by $G(f)=f_{\restriction \{0,\cdots,N_f\}}$ . Then $G$ is clearly injective. Hence $|\mathcal F| \le |\operatorname{Seq}(\Bbb N)|$ . We already know that $\operatorname{Seq}(\Bbb N)$ is countable. It follows that $\mathcal F$ is countable. My questions: Does my proof look fine or contain gaps? I feel that this proof depends on the theorem If $A$ is countable, then the set of finite sequences from $A$ is countable , which in turn requires several heavy lemmas. I would like to ask  for a simpler proof.","['elementary-set-theory', 'proof-verification', 'alternative-proof']"
2935184,How to obtain elements of dihedral group in GAP,"I'd like to obtain a list of all permutations contained in D_n using GAP.
E.g. for D_8 I've tried the following: gap> G := DihedralGroup(8);
<pc group of size 8 with 3 generators>
gap> List(G);
[ <identity> of ..., f3, f2, f2*f3, f1, f1*f3, f1*f2, f1*f2*f3 ] How can I obtain a list of actual permutations contained in D_8 instead?","['gap', 'group-theory']"
2935205,Does the elliptic curve $y^2 = 4 x^3 -6075$ have any integer points?,"Let $E$ be the elliptic curve $y^2 = 4 x^3 -6075$ . I ran the following Mathematica code, which searches naively for integer solutions to $E$ but it did not find any solutions $(x, y) \in E(\mathbb{Z})$ satisfying $0 \leq x \leq 10^6$ . T = Table[ z = 4 x^3 - 6075; 
      If[ IntegerQ[Sqrt[z]], 
         {x, Sqrt[z]}
      , 0]
  , {x, 0, 1000000}];
DeleteCases[T, 0] Is $E(\mathbb{Z})$ empty? Also, what is the easiest database on Mordell curves to access? I have had some trouble installing the Pari database in the past.","['number-theory', 'elliptic-curves']"
2935212,Showing that $\sqrt[3]{9+9\sqrt[3]{9+9\sqrt[3]{9+\cdots}}} - \sqrt{8-\sqrt{8-\sqrt{8+\sqrt{8-\sqrt{8-\sqrt{8+\cdots}}}}}} = 1$?,"$$\sqrt[3]{9+9\sqrt[3]{9+9\sqrt[3]{9+\cdots}}} - \sqrt{8-\sqrt{8-\sqrt{8+\sqrt{8-\sqrt{8-\sqrt{8+\cdots}}}}}} = 1$$
In the second nested radical, the repeating pattern is $(-,-,+)$. I approached this problem in a rather boring way. That is, the first expression satisfies the equation,
$$y=\sqrt[3]{9+9y} \implies y^3=9+9y$$
and the second one satisfies,
$$x=+\sqrt{8-\sqrt{8-\sqrt{8+x}}} \implies \left[(x^2-8)^2-8\right]^2=8+x$$
Now this magically factors into 
$$(x^2 - x - 8) (x^3 - 2 x^2 - 11 x + 23) (x^3 + 3 x^2 - 6 x - 17) = 0$$
It turns out that the $x$ we are looking is the solution of the third factor. And so,
$$x^3 + 3 x^2 - 6 x - 17=0 \implies (x+1)^3-9(x+1)-9=0$$ 
And magically $x+1=y$. The problem here is that this does not give me much understanding of nested radicals, it is just plain bash. Q. All in all, my question asks, if there's a way to prove the difference
  is 1 without (or a little less) bashing and a nice general way to deal
  with such expressions (like the examples below)? Next I tried the following, Assume that we want to solve
$$t=\sqrt{7-\sqrt{7+\sqrt{7-\sqrt{7+\sqrt{7-\sqrt{7+\cdots}}}}}}$$
What we can try is the following,
$$t=\oplus\sqrt{7\ominus\sqrt{7+t}}$$
When we repeatedly square, we loose the information of the signs highlighted. That is, the equation $(t^2-7)^2=t+7$ has some nice roots corresponding to,
$$t=+\sqrt{7+\sqrt{7+t}}\qquad \text{and}\qquad t=-\sqrt{7-\sqrt{7+t}}$$
But these can be reduced by their symmetry,
$$t=+\sqrt{7+t}\qquad \text{and}\qquad t=-\sqrt{7+t}$$
Solution to both of these satisfy $t^2=t+7$ (which are the roots we don't want). And so we get the intuition why that huge equation has a quadratic factor. Now in this case, after long division we will end up with a quadratic equation whose solution (that we want) is $2$. If we use this in our original question, we get a 6th degree equation with no further intuition. I also saw this on Wikipedia with no explanation. There is a typing error in the above image.","['real-numbers', 'algebra-precalculus', 'radicals', 'nested-radicals']"
2935222,Can a semialgebraic function be continuously extended to the closure of its domain?,"Let $C$ be a bounded semialgebraic set and $F: C \to \mathbb{R}$ be a bounded and continuous semialgebraic function. Is it true that $f$ can be continuously extended to the closure of $C$ ?  I was not able to find a counter-example, but I believe I am missing something and unable to prove it.","['general-topology', 'algebraic-geometry', 'algebraic-topology', 'real-analysis']"
2935235,"Finding a bijection from $\{1,2,...,nm\}$ to $X \times Y$","I'm trying to prove that for two finite sets $X,Y$ , where $|X|=n$ , $|Y|=m$ , $|X||Y|=|X \times Y|$ . I know that there exists bijections $f:\{1,2,...,n\} \rightarrow X$ and $g:\{1,2,...,m\} \rightarrow Y$ and I'm trying to find a bijection $h:\{1,2,...,nm\} \rightarrow X \times Y$ . I know that this is equivalent to showing there exists a bijection $k: X \times Y \rightarrow \{1,2,...,nm\}$ . One such bijection that seemingly works is $k(x_i,y_j) = (i-1)m + j$ , where $1 \leq i \leq n$ and $1 \leq j \leq m$ . I've tried to prove the injectivity and surjectivity of this function but to no avail: Injecivity: Suppose $k(x_a,y_b) = k(x_c,y_d)$ for $x_a, x_c \in X$ and $y_b, y_d \in Y$ . This gives $(a-1)m + b = (c-1)m + d$ but I've not been able to show that $a = c, b = d$ from here. Surjectivity: Suppose $z \in \{1,2,...,nm\}$ then $\exists i,j$ s.t. $ z = (i-1)m + j$ for some $i,j$ satisfying $1 \leq i \leq n$ and $1 \leq j \leq m$ . Now, since $X, Y$ can be written as $X = \{x_1,x_2,...,x_n\}$ , $Y = \{y_1,y_2,...,y_m\}$ and $X \times Y  = \{(x,y) | x \in X \wedge y \in Y\}$ then we can say $\exists(x_i, y_j) \in X \times Y$ and by the definition of $k$ we have $k((x_i, y_j)) = (i-1)m + j$ . But I'm not sure if this all watertight since I've assumed that $z$ can be written in the desired form.",['elementary-set-theory']
2935291,The uniqueness (up to isomorphism) of the complete linear ordering that has a countable dense subset,"In my textbook, the proof of this theorem is about 5 lines and omits many important details. I'm worry that I'm not truly understand it, so I present it here. I hope that someone can verify it for me. The proof is a little bit long. I'm deeply grateful for your help! Suppose that $(C,<)$ and $(C',<')$ are complete linearly ordered sets without endpoints. $P\subseteq C$ and $P'\subseteq C'$ are countable and dense in $C,C'$ respectively. Then $(C,<)$ and $(C',<')$ are isomorphic. Lemma: Let $(P, <)$ and $(P',<')$ be countable dense linearly ordered sets without endpoints. Then $(P, <)$ and $(P',<')$ are isomorphic. (I presented a proof here ) For $p\in P$ , there exists $c\in C$ such that $c<p$ by the fact that $C$ has no endpoints. Since $c,p\in C$ and $c<p$ , there exists $p'\in P$ such that $c<p'<p$ by the fact that $P$ is dense in $C$ . As a result, for all $p\in P$ , there is $p'\in P$ such that $p'<p$ . Thus $P$ has no endpoints. Since $P\subseteq C$ and $C$ is linearly ordered, $P$ is linearly ordered. Similarly, $P'$ has no endpoints and is linearly ordered. As a result, $(P, <)$ and $(P',<')$ are countable dense linearly ordered sets without endpoints. Thus $(P, <)$ and $(P',<')$ are isomorphic by Lemma . We denote $\sup X$ to be the supremum calculated in $(C,<)$ for $X\subseteq C$ , and $\sup' X'$ to be the supremum calculated in $(C',<')$ for $X'\subseteq C'$ . Since $C$ and $C'$ are complete, $\sup X$ and $\sup' X'$ do exist  for all $X\subseteq C$ and $X'\subseteq C'$ . Let $f:P \to P'$ be an isomorphism between $(P, <)$ and $(P',<')$ .  We define a mapping $g:C\to C'$ by $$g(x)=\sup' \{f(p) \mid p\in P \text{ and } p<x\}$$ Next we prove that $g$ is an an isomorphism between $(C, <)$ and $(C',<')$ such that $g(x)=f(x)$ for all $x\in P$ . 1. $g$ is well-defined For $x\in C$ , there exists $c\in C$ such that $c<x$ by the fact that $C$ has no endpoints. Since $c,x\in C$ and $c<x$ , there exists $p\in P$ such that $c<p<x$ by the fact that $P$ is dense in $C$ . As a result, for all $x\in C$ , there is $p\in P$ such that $p<x$ . Hence $\{f(p) \mid p\in P \text{ and } p<x\}\neq \emptyset$ and thus $g(x)=\sup' \{f(p) \mid p\in P \text{ and } p<x\}$ exists by the fact that $C'$ is complete. 2. $g$ is surjective For $c'\in C'$ , $x'=\sup' X'$ for some $X'\subseteq P'$ since $P'$ is dense in $C'$ .  Let $X=f^{-1}[X']$ , then $X\subseteq P$ and $f[X]=X'$ . Let $x=\sup X$ , then $x\in C$ . Next we prove $g(x)=x'$ . \begin{align} g(x)&=\sup' \{f(p) \mid p\in P \text{ and } p<x\} \\
&= \sup' \{f(p) \mid p\in P \text{ and } p<\sup X\}\\
&= \sup' \{f(p) \mid p\in P \text{ and } \exists p'\in X:p<p'\}\\
&= \sup' \{f(p) \mid f(p)\in f[P] \text{ and } \exists f(p')\in f[X]:f(p)<'f(p')\}, f\text{ is an isomorphism}\\
&= \sup' \{f(p) \mid f(p)\in P' \text{ and } \exists f(p')\in X':f(p)<'f(p')\}\\
&= \sup' \{c \mid c\in P' \text{ and } \exists c'\in X':c<'c'\}, \text{ let } c:=f(p) \text{ and }c'=f(p')\\
&= \sup' \{c \mid c\in P' \text{ and } c<'\sup' X'\}\\
&= \sup' \{c \mid c\in P' \text{ and } c<' x'\},\space \sup' X'=x'\\
&= x',C' \text{ is complete}\end{align} To sum up, $g(x)=x'$ and thus $g$ is surjective. 3. For all $x\in P:g(x)=f(x)$ For $x\in P$ : \begin{align}g(x)&= \sup' \{f(p) \mid p\in P \text{ and } p<x\}\\
&= \sup' \{f(p) \mid f(p)\in f[P] \text{ and } f(p)<'f(x)\},f \text{ is an isomorphim and } p,x\in P\\
&= \sup' \{c \mid c\in P' \text{ and } c<'c'\},\text{let } c:=f(p) \text{ and } c':=f(x)\\
&= c',C' \text{ is complete}\\
&=f(x)\end{align} Hence $g(x)=f(x)$ for all $x\in P$ . 4. For all $x,y \in C: x<y \iff g(x)<' g(y)$ It suffices to prove that for all $x,y \in C: x<y \implies g(x)<' g(y)$ . $x,y \in C$ , $x<y$ , and $P$ is dense in $C \implies$ there exist $p_1,p_2\in P$ such that $x<p_1<p_2<y$ . $p_1<p_2$ and $p_1,p_2\in P \implies f(p_1)<'f(p_2)$ by the fact that $f$ is an isomorphism between $(P, <)$ and $(P',<') \implies g(p_1)<'g(p_2)$ by definition of $g$ . $x<p_1 \implies \{f(p) \mid p\in P \text{ and } p<x\}\subsetneq\{f(p) \mid p\in P \text{ and } p<p_1\} \implies \sup'\{f(p) \mid p\in P \text{ and } p<x\} \le' \sup'\{f(p) \mid p\in P \text{ and } p<p_1\} \implies g(x)\le' g(p_1).$ Similarly, $g(p_2)\le' g(y)$ . To sum up, we have $g(x)\le' g(p_1)<'g(p_2)\le' g(y)$ and thus $g(x)<' g(y)$ . This completes the proof. Update: Thanks to @CopyPastIt's answer! I correct my proof here. 1. We denote $\sup X$ to be the supremum calculated in $(C,<)$ for $X\subseteq C$ , and $\sup' X'$ to be the supremum calculated in $(C',<')$ for $X'\subseteq C'$ . Since $C$ and $C'$ are complete, $\sup X$ and $\sup' X'$ do exist  for all $X\subseteq C$ and $X'\subseteq C'$ . For this assertion to be correct, I must add the condition that $X,X'$ are bounded from above in $C$ and $C'$ respectively. 2. For $x\in C$ , there exists $c\in C$ such that $c<x$ by the fact that $C$ has no endpoints. Since $c,x\in C$ and $c<x$ , there exists $p\in P$ such that $c<p<x$ by the fact that $P$ is dense in $C$ . As a result, for all $x\in C$ , there is $p\in P$ such that $p<x$ . Hence $\{f(p) \mid p\in P \text{ and } p<x\}\neq \emptyset$ and thus $g(x)=\sup' \{f(p) \mid p\in P \text{ and } p<x\}$ exists by the fact that $C'$ is complete. For $g$ to be well-defined, I must also prove that $\{f(p) \mid p\in P \text{ and } p<x\}$ is bounded from above in $C'$ for all $x\in C$ . For $x\in C$ , there exists $y\in C$ such that $x<y$ by the fact that $C$ has no endpoints. Since $P$ is dense in $C$ , there exists $p'\in P$ such that $x<p'<y$ . Then $\forall p<x:p<p'$ . It follows that $\forall (p<x\text{ and }p\in P):f(p)<'f(p')$ by the fact that $f$ is an isomorphism between $(P, <)$ and $(P',<')$ . Hence $\{f(p) \mid p\in P \text{ and } p<x\}$ is bounded from above by $f(p')$ . 3. For $c'\in C'$ , $x'=\sup' X'$ for some $X'\subseteq P'$ since $P'$ is dense in $C'$ .  Let $X=f^{-1}[X']$ , then $X\subseteq P$ and $f[X]=X'$ . Let $x=\sup X$ , then $x\in C$ . Next we prove $g(x)=x'$ . \begin{align} g(x)&=\sup' \{f(p) \mid p\in P \text{ and } p<x\} \\
&= \sup' \{f(p) \mid p\in P \text{ and } p<\sup X\}\\
&= \sup' \{f(p) \mid p\in P \text{ and } \exists p'\in X:p<p'\}\\
&= \sup' \{f(p) \mid f(p)\in f[P] \text{ and } \exists f(p')\in f[X]:f(p)<'f(p')\}, f\text{ is an isomorphism}\\
&= \sup' \{f(p) \mid f(p)\in P' \text{ and } \exists f(p')\in X':f(p)<'f(p')\}\\
&= \sup' \{c \mid c\in P' \text{ and } \exists c'\in X':c<'c'\}, \text{ let } c:=f(p) \text{ and }c'=f(p')\\
&= \sup' \{c \mid c\in P' \text{ and } c<'\sup' X'\}\\
&= \sup' \{c \mid c\in P' \text{ and } c<' x'\},\space \sup' X'=x'\\
&= x',C' \text{ is complete}\end{align} To sum up, $g(x)=x'$ and thus $g$ is surjective. This assertion contains typo and is wrong. I fixed it as follows. For $x'\in C$ , let $X'=\{p'\in P' \mid p'<x'\} \subseteq P'$ . It is easy to verify that $X'$ is bounded from above in $P'$ , that $X'\neq\emptyset$ , and that $\sup' X'=x'$ . Let $X=f^{-1}[X']$ , then $X\subseteq P$ and $f[X]=X'$ . Futhermore, $X'\neq\emptyset$ , $X'$ is bounded from above in $P'$ , and $f$ is an isomorphism between $(P, <)$ and $(P',<')$ imply that $X$ is bounded from above in $P$ and that $X\neq\emptyset$ . Hence $x:=\sup X \in C$ does exists. Next we prove $g(x)=x'$ . \begin{align} g(x)&=\sup' \{f(p) \mid p\in P \text{ and } p<x\} \\
&= \sup' \{f(p) \mid p\in P \text{ and } p<\sup X\}\\
&= \sup' \{f(p) \mid p\in P \text{ and } \exists p'\in X:p<p'\}\\
&= \sup' \{f(p) \mid f(p)\in f[P] \text{ and } \exists f(p')\in f[X]:f(p)<'f(p')\}, f\text{ is an isomorphism}\\
&= \sup' \{f(p) \mid f(p)\in P' \text{ and } \exists f(p')\in X':f(p)<'f(p')\}\\
&= \sup' \{y' \mid y'\in P' \text{ and } \exists z'\in X':y'<'z'\}, \text{ let } y':=f(p) \text{ and }z':=f(p')\\
&= \sup' \{y' \mid y'\in P' \text{ and } y'<'\sup' X'\}\\
&= \sup' \{y' \mid y'\in P' \text{ and } y'<' x'\},\space \sup' X'=x'\\
&= x',C' \text{ is complete}\end{align} To sum up, $g(x)=x'$ and thus $g$ is surjective.","['elementary-set-theory', 'order-theory', 'proof-verification']"
2935303,Proving a convexity inequality,"Given $f: \mathbb{R} \to \mathbb{R}$ convex, show that: $$ \frac{2}{3}\left(f\left(\frac{x+y}{2}\right) + f\left(\frac{z+y}{2}\right) + f\left(\frac{x+z}{2}\right)\right) \leq f\left(\frac{x+y+z}{3}\right) + \frac{f(x) + f(y) + f(z)}{3}.$$ I have tried some ideas, such as transforming it into $$ f\left(\frac{x+y}{2}\right) + f\left(\frac{z+y}{2}\right) + f\left(\frac{x+z}{2}\right) - 3f\left(\frac{x+y+z}{3}\right)\\
\leq  f(x) + f(y) + f(z) - f\left(\frac{x+y}{2}\right) - f\left(\frac{z+y}{2}\right) - f\left(\frac{x+z}{2}\right) $$ (which graphically seemed intuitive) and using that such an $f$ lies above its tangents, but did not succeed… Ideas are welcome :)","['jensen-inequality', 'real-analysis', 'inequality', 'convex-analysis', 'karamata-inequality']"
2935365,"Suppose $ H\leqslant G $, prove that if $ (H, G')=\langle e \rangle $, then $ (H', G)=\langle e \rangle $.","I am working on this Exercise from Algebra by Hungerford (Exercise II.7.3(b)). It states If $ H $ and $ K $ are subgroups of a group $ G $ , let $ (H, K) $ be the subgroup of $ G $ generated by the elements $ \{ hkh^{-1}k^{-1}|h\in H, k\in K \} $ . Show that If $ (H, G')=\langle e \rangle $ , then $ (H', G)=\langle e \rangle $ . $ G' $ is the commutator subgroup of $ G $ . My attempt: $ (H', G)= \langle e \rangle $ is the same thing as $ H' $ is in the center of $ G $ . Then I am stuck... I couldn't find any useful tool to simplify the problem. Can someone give me a hint? Thank you.","['group-theory', 'abstract-algebra']"
2935406,Solving the diffusion equation with an absorbing boundary,"There is a one-dimensional diffusion process in which particles start running at $t = 0$ and from $x_o > 0$ . When particles reach x = 0 they are removed from the system, thus the total concentration is not conserved anymore. I have to solve the diffusion equation, which is the following partial differential equation: $$\frac{\partial P (R, t)}{\partial t} = D\triangledown^2P(R,t) $$ Where $P(R, t)$ is the probability that the particles  arrive at R at time t. And I am given the initial conditions: $$c(x,0) = N\delta(x - x_o)$$ $$c(0, t) = 0$$ I have been doing some research in how to do so and I came across with a method which is based on a particular Gaussian function: $$G (R, t) = (\frac{1}{4\pi Dt})^{\frac{d}{2}} e^{\frac{(R-R_0)^2}{4Dt}}$$ Where d is the dimensionality of the system. But the issue here is that we are working with an 'absorbing boundary' that makes the condition $c(0, t) = 0$ useless because we work from $x_o > 0$ . Then how could I solve the probability (i.e this differential equation)? I have been suggested the method of images, but not sure about it.","['integration', 'ordinary-differential-equations', 'calculus', 'statistical-mechanics', 'mathematical-physics']"
2935414,Span of vectors with more entries in each vector than the amount of vectors,"I can easily visualize why $\begin{bmatrix}
    1\\ 0
\end{bmatrix}$ and $\begin{bmatrix}
    0\\ 1
\end{bmatrix}$ spans $R^2$ . I have learned that we need at least two linearly independent vectors to span $R^2$ , $3$ to span $R^3$ and so on... This is easy to visualize when the entries in the vectors are the same as the amount of total vectors. But what happens if we for instance have the two vectors $\begin{bmatrix}
    1\\ 0 \\ 1 \\ 0
\end{bmatrix}$ and $\begin{bmatrix}
    0\\1\\0\\1 
\end{bmatrix}$ . Clearly,  the two vectors are linearly independent, but do they still only span $R^2$ ? If we add the two vectors we produce $\begin{bmatrix}
    1\\ 1\\1\\1
\end{bmatrix}$ which is a vector in $R^4$ , right? So my question boils down to what is the span of $\begin{bmatrix}
    1\\ 0 \\ 1 \\ 0
\end{bmatrix}$ and $\begin{bmatrix}
    0\\1\\0\\1 
\end{bmatrix}$ , and in general what is the span of a set of vectors with more entries in each vector than vectors in total?","['linear-algebra', 'vector-spaces']"
2935465,How make summation for a series which contains arbitrary elements,"I am studding a research paper in winch author presented a analytical model for set traversal and different cases of time complexity.
I am not understanding the one point in the model that is related to summation formulation equation n# 18. $\frac{m_1 + 1}{2}t + \frac{l_2 + 1}{2}t + .... + \frac{l_n + 1}{2}t$ $ = \frac{1}{2} \bigr(m_1 + n + \sum_{a=l_2}^{l_n} a\bigr) t$ Why $l_2 + l_3 + l_4 + ..... + l_n = \sum_{a=l_2}^{l_n} a$ why its not $\sum_{a=2}^{n} l_a$","['algebra-precalculus', 'summation', 'discrete-mathematics']"
2935470,Minimum of a sum proof.,"The problem I am working on is: Let $ Y = \{{y_1,y_2,y_3,...y_n\}}$ and $c=median(Y)$ . Prove that: $$
\text{min}\left[\sum_{i=1}^n \lvert y_{i}-c\rvert\right]=c $$ My question is: Is the $\text{min}\left[\right]$ function here, asking for the which $y_i$ the distance between $y_i$ and $c$ , $\lvert y_{i}-c\rvert$ is the smallest? Or is it asking, for the literal minimum of the arugments its given, like $\text{min}\left[99,1,4,7,121\right]=1$ ? What I have tried so far: 1. First I tried an example to get a feel for things: Let $ Y = \{{1,2,3,4,5\}}$ and $c=3$ . Then we have: $$\begin{align}
\sum_{i=1}^5 \lvert y_{i}-3\rvert & = \lvert 1-3\rvert = 2 \\
 & = \lvert 1-3\rvert = 2 \\ 
 & = \lvert 2-3\rvert = 1 \\
 & = \lvert 3-3\rvert = 0 \\
 & = \lvert 4-3\rvert = 1 \\
 & = \lvert 5-3\rvert = 2 \\ 
 & = 2+1+0+1+2=6
\end{align}$$ So, $$ \text{min}\left[\sum_{i=1}^5 \lvert y_{i}-3\rvert\right]=\text{min}\left[6\right]=6
$$ Since this would now be acounterexample to the statement I am supposed to prove, I must be having a misunderstanding? 2. If the $\text{min}\left[\right]$ function works the other way, I tried the following proof: Proof: Since the distance between a point and itself is $0$ , $\left[c-c\right]=0$ and the fact that there can only be one median $c$ for a given set of $y$ , clearly $\left[c-c\right] \lt \left[y_i-c\right]$ for all $y_i.$ Is this correct if thats the case?","['maxima-minima', 'statistics', 'proof-verification', 'median']"
2935471,Show that $\int_0^{\infty}\frac{\operatorname{Li}_s(-x)}{x^{\alpha+1}}\mathrm dx=-\frac1{\alpha^s}\frac{\pi}{\sin(\pi \alpha)}$,"I have come across the following integral while going over this list (Problem $35$ ) $$\int_0^{\infty}\frac{\operatorname{Li}_s(-x)}{x^{\alpha+1}}\mathrm dx=-\frac1{\alpha^s}\frac{\pi}{\sin(\pi \alpha)}~~~~s>0, \alpha\in(0,1)$$ where $\operatorname{Li}_s(x)$ denotes the Polylogarithm Function. Using the series expansion of $\operatorname{Li}_s(-x)$ yields $$\begin{align}
\int_0^{\infty}\frac{\operatorname{Li}_s(-x)}{x^{\alpha+1}}\mathrm dx&=\int_0^{\infty}\frac1{x^{\alpha+1}}\left[\sum_{n=1}^{\infty}\frac{(-x)^n}{n^s}\right]\mathrm dx\\
&=\sum_{n=1}^{\infty}\frac{(-1)^n}{n^s}\int_0^{\infty}\frac{x^n}{x^{\alpha+1}}\mathrm dx\\
&=\sum_{n=1}^{\infty}\frac{(-1)^n}{n^s}\int_0^{\infty}x^{n-\alpha-1}\mathrm dx
\end{align}$$ One can easily see the problems concerning the convergence of the last integral. Also, I am not even sure whether it is possible to change the order of summation and integration in this case or not. Another approach is based on an integral representation of $\operatorname{Li}_s(-x)$ so that the given integral becomes $$\begin{align}
\int_0^{\infty}\frac{\operatorname{Li}_s(-x)}{x^{\alpha+1}}\mathrm dx&=\int_0^{\infty}\frac1{x^{\alpha+1}}\left[\frac1{\Gamma(s)}\int_0^{\infty}\frac{t^{s-1}}{e^t/(-x)-1}\mathrm dt\right]\mathrm dx\\
&=-\frac1{\Gamma(s)}\int_0^{\infty}\int_0^{\infty}\frac{t^{s-1}}{x^{\alpha}(e^t+x)}\mathrm dx\mathrm dt\\
\end{align}$$ From hereon, I do not know how to proceed. Since the solution reminds me of Euler's Reflection Formula it might be possible to reshape the integral in terms of the Gamma Function somehow. I am asking for a whole evaluation of the given integral. I did not found anything closely related to this question but correct me if I am wrong. Thanks in advance!","['integration', 'polylogarithm', 'definite-integrals', 'closed-form']"
2935527,Is a linear operator continuous if its kernel is closed?,Let $T$ be a linear operator between two infinite dimensional normed spaces $X$ and $Y$ whose kernel is a closed subset of its domain. Does it imply that $T$ is bounded or not necessary ? If yes I would be very grateful if one could mention the proof. If no it would be better if a counter-example were provided.,"['operator-theory', 'normed-spaces', 'functional-analysis']"
2935545,what is the distinction between differential equations and differential forms?,"Say I have these two things: $$
A(x,y)+B(x,y)\frac{dy}{dx}=0
$$ $$A(x,y)dx+B(x,y)dy=0$$ what is the difference? I always thought they were just different ways of expressing the same things, but recently I was told it wasn't quite like that. How come?",['ordinary-differential-equations']
2935555,Number of real roots of an iterated quadratic: $x^2-3/2$,"I was messing around with polynomials and their real roots when I, as recreational mathematicians do, asked myself the following random question: Suppose I am given a polynomial $P(x)$ . How can I find the number of real roots of the polynomial $P^{\circ n}(x)$ , representing the n-fold composition of $P$ with itself (counting multiplicity)? I started with the simple polynomial $P_1(x)=x^2-1$ . This was an easy example, as it turned out that $P_1^{\circ n}(x)$ has $n+1$ real roots, which was simple to prove. My next example was the polynomial $P_2(x)=x^2-2$ . This one was more difficult, but I eventually determined that $P_2^{\circ n}(x)$ has $2F_{n+1}-2$ real roots, where $F_n$ represents the sequence of Fibonacci numbers with $F_0=F_1=1$ . In general, I am considering polynomials of the form $P_c(x)=x^2-c$ . For $c$ less than $-1$ , the iterates of this polynomial have no real roots, and for $c$ greater than $3$ , the $n$ th iterate of this polynomial seems to have $2^n$ real roots. The polynomial with which I am stumped is $$P_{3/2}(x)=x^2-\frac{3}{2}$$ While I have been unable to derive a formula for the number of real roots of $P_{3/2}^{\circ n}(x)$ , by observing the number of real roots for the first couple of iterations, I have come up with the remarkable conjecture that the number of real roots of the $n$ th iterate is given by $2p(n)$ , where $p(n)$ represents the number of partitions of $n$ . This conjecture, if true, would be truly amazing. How can I prove it? NOTE: To prove the formulae I obtained for $P_1$ and $P_2$ , I divided the real line up into intervals that the polynomials in question sent to one another, and from this I obtained a recursive formula for each. However, I cannot figure out how to nearly divide $\mathbb R$ into intervals in the same way.","['combinatorics', 'polynomials', 'function-and-relation-composition', 'roots']"
2935579,Proof of Zorn's Lemma,"I am looking at the Zorn's Lemma. The axiom of choice is the following: 
Let $I$ be an arbitrary set of indices and let $A_i$ be a family of non-empty sets ( $A_i\neq \emptyset$ ), then there exists a map \begin{equation*}f: \ I\rightarrow \bigcup_{i\in I}A_i \ \text{ with } \ f(I)\in A_i\end{equation*} The Zorn's Lemma is the following: 
Each nonempty partially ordered set in which each totally ordered subset has an upper bound contains at least one maximum element. $$$$ I am looking at the proof: A sketch of the proof of Zorn's lemma follows, assuming the axiom of choice. Suppose the lemma is false. Then there exists a partially ordered set $P$ such that every totally ordered subset has an upper bound, and every element has a bigger one. For every totally ordered subset $T$ we may then define a bigger element b(T), because T has an upper bound, and that upper bound has a bigger element. To actually define the function b, we need to employ the axiom of choice. Using the function $b$ , we are going to define elements $a_0 < a_1 < a_2 < a_3 < \ldots$ in $P$ . This sequence is really long: the indices are not just the natural numbers, but all ordinals. In fact, the sequence is too long for the set $P$ ; there are too many ordinals (a proper class), more than there are elements in any set, and the set $P$ will be exhausted before long and then we will run into the desired contradiction. The $a_i$ are defined by transfinite recursion: we pick $a_0$ in $P$ arbitrary (this is possible, since $P$ contains an upper bound for the empty set and is thus not empty) and for any other ordinal $w$ we set $a_w = b(\{a_v: v < w\})$ . Because the $a_v$ are totally ordered, this is a well-founded definition. This proof shows that actually a slightly stronger version of Zorn's lemma is true: If $P$ is a poset in which every well-ordered subset has an upper bound, and if $x$ is any element of $P$ , then $P$ has a maximal element greater than or equal to $x$ . That is, there is a maximal element which is comparable to $x$ . $$$$ I have some questions: Then there exists a partially ordered set $P$ such that every totally ordered subset has an upper bound, and every element has a bigger one. This is the negation of Zorn's lemma, isn't it? $$$$ Using the function $b$ , we are going to define elements $a_0 < a_1 < a_2 < a_3 < \ldots$ in $P$ . This sequence is really long: the indices are not just the natural numbers, but all ordinals. In fact, the sequence is too long for the set $P$ ; there are too many ordinals (a proper class), more than there are elements in any set, and the set P will be exhausted before long and then we will run into the desired contradiction. Could you explain me this part? 
 What exactly is an ordinal? And how do we get the contradiction? $$$$ The $a_i$ are defined by transfinite recursion: we pick $a_0$ in $P$ arbitrary (this is possible, since $P$ contains an upper bound for the empty set and is thus not empty) and for any other ordinal $w$ we set $a_w = b(\{a_v: v < w\})$ . Because the $a_v$ are totally ordered, this is a well-founded definition. Do we define these $a_i$ by taking a subset of $P$ and then we get a bigger element, then we take a bigger subset and so we get a bigger element, and so on?","['axiom-of-choice', 'proof-explanation', 'set-theory', 'analysis']"
2935617,Calculate $\sum_{n=0}^\infty \frac1{(4n^2 - 1)^2}$,"How do I find the value of the following infinite series? $$\sum_{n=0}^\infty \frac{1}{(4n^2-1)^2} $$ My attempt at a solution: $$\sum_{n=0}^\infty \frac{1}{(4n^2-1)^2} = \sum_{n=0}^\infty \frac{1}{((2n-1)(2n+1))^2} = \sum_{n=0}^\infty \left(\frac{1}{2}\left(\frac{1}{(2n-1)}-\frac{1}{(2n+1)}\right)\right)^2  = \frac{1}{4}\sum_{n=0}^\infty \left(\frac{1}{2n-1}-\frac{1}{2n+1}\right)^2 $$ I then tried to compute the partial sums of this series, but with no luck. Does anyone else know how to do it?","['sequences-and-series', 'real-analysis']"
2935681,Results proved using perfectoid spaces understandable by an undergraduate,"Many advanced areas of research in mathematics, the Riemann hypothesis, the Taniyama–Shimura conjecture, Green–Tao theorem etc. all have interesting consequences that could be stated using only undergraduate mathematics. Since perfectoid spaces are a hot topic but are out of reach for most of us, I am curious if they have been used to prove anything that an advanced undergraduate could understand. If not, are there any such consequences of the Langlands program? Though I understand this latter question may be too broad.","['algebraic-geometry', 'langlands-program']"
2935682,How to calculate $\lim_{x\to 1} \frac{\sqrt x - x^2}{1-\sqrt x}$ without L'Hopital rule?,"$$\lim_{x\to 1} \frac{\sqrt x - x^2}{1-\sqrt x}$$ I solved it by using L'hopital and got $3$ . I've also attempted to solve it without L'hopital, but unfortunately I couldn't.","['limits', 'calculus', 'limits-without-lhopital']"
2935703,Minimize quadratic function with gradient descent method,"Anyone could help me wih this question, please? I made an exam and I didn't know how I can solve this. $A \in \mathbb{R}^{n \times n}$ is a matrix symmetric positive definite, $b \in \mathbb{R}^{n\times 1}$ and we have $f(x) = \dfrac{1}{2}x^TAx -b^Tx$ . Give a point $x^0 \in \mathbb{R}^{n \times 1}$ , show that the gradient method descent given by $$ x^{k+1}=x^k -\left(||A||_{2}\right)^{-1} \nabla f(x^k)$$ converges to the global minimum. So, this $|| \cdot||_2$ is an Euclidean norm. I made this: $|| A||_2 = || A|| = \sup\{||Ax||:||x|| =1 \}
$ , and I thought A has positive real eigenvalues $0 < \lambda_1 \leq \lambda_2 \leq \cdots\leq \lambda_n$ and that I would obtain an orthonormal basis of the matrix $A$ formed by the eigenvectors $v_1, v_2, ..., v_n$ associated with the respective eigenvalues $\lambda_i$ .
So, how $||Av|| = ||\lambda v|| = |\lambda| \;||v|| = \lambda$ I suppose that $||A|| = \lambda_n = \lambda$ and so $||A||^{-1}=\dfrac{1}{\lambda}$ .
Now, I have the following sequence $$
x^{k+1}=x^k -\dfrac{1}{\lambda} \nabla f(x^k).
$$ I have argued that $f$ has only a local minimum and that it is global.
Also said that the minimum is given by $x^{*} =A^{-1}b$ . But I could not show that the sequence above converges to $x^{*}$ . Then I thought I did not need to show that I was converging to $x^{*}$ . It is enough to show that it converged, for it guarantees that it would be for this element due to its uniqueness (only having a global minimum). But I could not solve it.
It reminded me of contraction. I have already tried a recurrence by doing $$
x^{k+1} - x^0 = -\dfrac{1}{\lambda}\left( \nabla f(x^0) + \nabla f(x^1) + \cdots +\nabla f(x^k) \right)
$$ but I could not move forward. I also tried to think in $||x^{k+1}-x^{k} || = ||-\frac{1}{\lambda} \nabla f(x^k) || = \frac{1}{\lambda} || \nabla f(x^k) ||$ ,
which was when it reminded me of a contraction. But I could not finish anything either.
Incidentally, this holds if $1 / \lambda$ is less than $1$ and I could not guarantee that. Could you help me? I already did the test, but I wanted to see the solution. PS: I found this, but I can't understand. https://en.wikipedia.org/wiki/Modified_Richardson_iteration","['multivariable-calculus', 'calculus', 'optimization']"
2935712,Not every ideal of a subring $S$ of $R$ is of the form $I \cap S$ for $I \lhd R$,I just got done showing that if $I \lhd R$ and $S \leq R$ then $I \cap S \lhd S$ . I'm now looking for an example to show that not every ideal of a subring $S$ of $R$ is of the form $I \cap S$ for $I \lhd R$ . I tried finding examples in matrix rings to no avail.. If somebody could offer me a new perspective it would be much appreciated!,"['ring-theory', 'abstract-algebra', 'ideals']"
2935743,Reference for expectation and variance of the product of independent random variables,"Given two independent random variables X, Y, the expectation of their product XY is: $\mathrm{E}[XY] = \mathrm{E}[X]\cdot\mathrm{E}[Y]$ Similarly, the variance of the product of these variables is: $\mathrm{Var}[XY] = \mathrm{Var}[X]\cdot \mathrm{Var}[Y] + \mathrm{Var}[Y]( \mathrm{E}[X])^2 + \mathrm{Var}[X] (\mathrm{E}[Y])^2$ While proofs or sketches of proofs can be found online (even within this forum), I have been struggling to find a citable reference of the above formulas (i.e., a textbook or a paper). Can you provide a suitable reference?","['expected-value', 'variance', 'probability-theory', 'reference-request']"
2935772,Applying L'Hopital's rule for $\lim_{x\to\infty} x((1+1/x)^x-e)$ [duplicate],"This question already has answers here : Find this limit using L`Hopitals rule [duplicate] (2 answers) Closed 4 years ago . I know the limit is $-e/2$ but I can't get there. I know I should be using L'Hopitals Rule here, I tried both $0/0$ and $\infty/\infty$ , either way it's a big mess. Please help. Maybe you can use the $e^{log(...)}$ trick but I haven't found it to be useful. Edit: I'm not familiar with the Big-O notation","['limits', 'calculus']"
2935783,Solve differential equation: $f'''(x)=f(x)f'(x)f''(x)$,I came across $f'''(x)=f(x)f'(x)f''(x)$ but I don't know how to solve it. I tried $\frac{f'''(x)}{f''(x)}=f(x)f'(x)$ $\ln|f''(x)|=\frac{1}{2}f(x)^2+c_{1}$ But from there I have no idea how to proceed. Please help me solve this if possible.,"['calculus', 'ordinary-differential-equations']"
2935784,Proving that $\sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n^{2}}=\frac{\pi^{2}}{12}$,"I want to prove that $$\sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n^{2}}=\frac{\pi^{2}}{12}, $$ using the Fourier Series for the $2\pi$ -periodic function $f(\theta)=\theta^{2},\quad (-\pi<0<\pi)$ , that is, $$\theta^{2}=\frac{\pi^{2}}{3}+4\sum_{n=1}^{\infty}\frac{(-1)^{n}\cos(n\theta)}{n^{2}} $$ So, $$\sum_{n=1}^{\infty}\frac{(-1)^{n}\cos(n\theta)}{n^{2}}=\frac{\theta^{2}}{4}-\frac{\pi^{2}}{12}.$$ I'm looking for a $\theta$ such that $(-1)^{n}\cos(n\theta)=(-1)^{n+1}\Rightarrow \cos(n\theta)=-1$ for all $n\in\mathbb{N}$ . How can I choose that $\theta$ ?","['fourier-series', 'sequences-and-series']"
2935855,"""Bad"" Fourier Series derivation","Let $f(\theta)$ $2\pi$ -periodic such that $f(\theta)=e^{\theta}$ for $-\pi<0<\pi$ , and $$e^{\theta}=\sum_{n=-\infty}^{\infty}c_{n}e^{in\theta}\,\,\, \mathrm{for}\,\, |\theta|<\pi $$ it's  Fourier series. If we formally differentiate this equation, we obtain $$e^{\theta}=\sum_{n=-\infty}^{\infty}inc_{n}e^{in\theta}. $$ But this implies $c_{n}=inc_{n}$ or $(1-in)c_{n}=0$ , so, $c_{n}=0\,\forall n\in\mathbb{Z}$ . This is obviously wrong. Where is the mistake? The only thing that I could contest is the derivative of $f$ . I know a theorem saying that a sufficient condition is $f$ continuous and piecewise smooth, and $f$ is not continous. But it don't implies that I can't derivate term by term.","['fourier-series', 'fourier-analysis', 'sequences-and-series']"
2935893,Does swapping columns of a matrix cause the rows of the inverse matrix to be swapped?,"This question came up while I was performing some computation on a few matrices on an unrelated problem in computer science. Let $A$ be an invertible matrix with columns $A_1, \dots A_n$ . Let $B$ be its inverse, with rows $B_1, \dots, B_n$ . Now construct a new matrix $\hat{A}$ by swapping two columns $A_i$ and $A_j$ . Let $\hat{B}$ be the inverse of $\hat{A}$ . In my specific computations, I noticed that $\hat{B}$ was nothing more than $B$ with the rows $B_i$ and $B_j$ swapped. Is this always true? I doubt it was a coincidence because the numbers were quite random. Note: I was working with real numbers but I'd be interested to know if the field makes any difference.","['linear-algebra', 'inverse']"
