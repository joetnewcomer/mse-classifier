question_id,title,body,tags
4677188,Approximation of $\sqrt{2}$.,"Let $(x_n)$ be the sequence define by \begin{cases}
x_1 = 1 \\
x_{n+1} = \frac{1}{2}\left( x_n + \frac{2}{x_n} \right)
\end{cases} .
Show that $1\leqslant x_n \leqslant\frac{3}{2}\ \forall n\in\mathbb{N}$ . I've tried by induction. My induction hypothesis was the vadility of $1\leqslant x_k \leqslant\frac{3}{2}$ for some $k\in\mathbb{N}$ . From that it is easy to show that $x_{k+1}\geqslant 1$ . In fact, $1\leqslant x_k$ implies $3\leqslant x_k^2+2$ and $x_k\leqslant\frac{3}{2}$ implies $1\leqslant\frac{3}{2x_k}$ . Then, \begin{align}
x_{k+1} &= \frac{x_k^2+2}{2x_k}\\
 &\geqslant \frac{3}{2x_k}\\
&\geqslant 1.
\end{align} But I'm stuck with $x_{k+1}\leqslant\frac{3}{2}$ . I only get $x_{k+1}\leqslant\frac{17}{8}$ , way over $\frac{3}{2}$ .
I think there must be some preliminay result that makes this last part easier, but I do not know. Any hint would be enough. Thanks in advance.","['induction', 'approximation', 'discrete-mathematics', 'sequences-and-series']"
4677198,"Apostol: Find $\lim_{(x,y)\to (0,0)} \sqrt{\frac{x^2+y^2}{(x+3x^3+3xy^2)^2+(y+3y^3+3yx^2)^2+x^2+y^2}}$?","The following problem is from Chapter 8 of Apostol's Calculus , Vol II (a) Find a vector $V(x,y,z)$ normal to the surface $$z(x,y)=\sqrt{x^2+y^2}+(x^2+y^2)^{3/2}$$ at a general point $(x,y,z)$ of the surface, $(x,y,z)\neq (0,0,0)$ . (b) Find the cosine of the angle $\theta$ between $V(x,y,z)$ and the
z-axis and determine the limit of $\cos{\theta}$ as $(x,y,z)\to
> (0,0,0)$ . I used the fundamental product of the vector equation $$\vec{r}(x,y)=\langle x, y, z(x,y)\rangle$$ which is $$\frac{\partial\vec{r}}{\partial x}\times\frac{\partial\vec{r}}{\partial y}=\left \langle \frac{x}{\sqrt{x^2+y^2}}-\frac{3x^3+3xy^2}{\sqrt{x^2+y^2}}, -\frac{y}{\sqrt{x^2+y^2}}-\frac{3x^3+3xy^2}{\sqrt{x^2+y^2}}, 1 \right\rangle$$ Then $$\frac{\partial\vec{r}}{\partial x}\times\frac{\partial\vec{r}}{\partial y}\cdot \hat{k}=\left\lVert \frac{\partial\vec{r}}{\partial x}\times\frac{\partial\vec{r}}{\partial y} \right\rVert \cos{\theta}$$ $$\cos{\theta}=\sqrt{\frac{x^2+y^2}{(x+3x^3+3xy^2)^2+(y+3y^3+3yx^2)^2+x^2+y^2}}$$ My question is how to find the limit of this expression as $(x,y)\to (0,0)$ ?","['limits', 'multivariable-calculus']"
4677224,Convergence of $\int_1^{+\infty}\sin\left( \frac{\sin(x)}{\sqrt{x}} \right) \frac{dx}{\sqrt{x}}$,"There is an integral $\int_1^{+\infty}\sin\left( \frac{\sin(x)}{\sqrt{x}} \right) \frac{dx}{\sqrt{x}}$ . Prove that it converges only conditionally. It is absolutely divergent because when $x \to +\infty$ one would have $\frac{\sin(x)}{\sqrt{x}} \to 0$ , so $\left|\sin\left( \frac{\sin(x)}{\sqrt{x}} \right)\right| \geq \frac{1}{2} \left| \frac{\sin(x)}{\sqrt{x}} \right| $ , at least when $x > x_0$ for some $x_0 > 1$ . So for $x > x_0$ one would have $$
\left| \sin\left( \frac{\sin(x)}{\sqrt{x}} \right) \frac{1}{\sqrt{x}}\right| \geq \frac{1}{2}\frac{|\sin(x)|}{x}
$$ It is known that $\int_1^{+\infty} \frac{|\sin(x)|}{x} dx$ diverges. But how to prove that $\int_1^{+\infty}\sin\left( \frac{\sin(x)}{\sqrt{x}} \right) \frac{dx}{\sqrt{x}}$ converges? Any help would be appreciated. UPD. After reading the comments I came to a solution. Please check it out. We have $$
\sin\left(\frac{\sin(x)}{\sqrt{x}}\right) = \frac{\sin(x)}{\sqrt{x}} + g(x)
$$ where $g(x) = o\left(\frac{\sin^2(x)}{x}\right)$ when $x \to +\infty$ . So $$
\sin\left(\frac{\sin(x)}{\sqrt{x}}\right)\frac{1}{\sqrt{x}} = \frac{\sin(x)}{x} + h(x)
$$ where $h(x) = o\left(\frac{\sin^2(x)}{x^{\frac{3}{2}}}\right)$ when $x \to +\infty$ . The integral $\int_1^{+\infty } \frac{\sin^2(x)}{x^{\frac{3}{2}}} dx$ converges absolutely, so $\int_1^{+\infty } h(x) dx$ converges absolutely and the convergence of $\int_1^{+\infty}\sin\left( \frac{\sin(x)}{\sqrt{x}} \right) \frac{dx}{\sqrt{x}}$ depends only on the convergence of $\int_1^{+\infty}\frac{\sin(x)dx}{x}$ . This integral is convergent, so the integral under consideration converges.","['integration', 'solution-verification', 'improper-integrals', 'real-analysis']"
4677231,Primary Decomposition: $G=G(p_1)\oplus...\oplus G(p_k)$,"Rotman's book proves that if $\text{exp}(G)=n=p_1^{e_1}...p_t^{e_t}$ , we have $G=\sum_{j=1}^t\oplus_j G(p_j)$ where $G(p_j)=\{g| \: \text{ord}(g)=p_j^k \:\text{for some k}\}.$ In class, this theorem was slightly modified so that we sum over every prime such that there is a certain $x_i$ with $\text{ord}(x_i)=p_i$ . Can I modify Rootman's result such that this other version holds? I basically need to show I am summing over the same set of primes. I proved that the set of primes $q_i$ with $\text{ord}(x_i)=q_i$ for some $x_i$ is contained in Rotman's set of primes. I am having trouble with the other direction. For the sake of completennes here is one inclusion. If $\text{ord}(x)=q$ , we have by Rotman's result that $x=x_1+...+x_t\in \sum_j \oplus_j G(p_j)$ with some $x_j\not=0$ . Considering the fact that $qx=0$ , we have $0=\sum_i q x_i$ .Suppouse by way  of contradiction that $q\not =p_i$ for every $i$ .Because the sum is direct and $qx_i\in G(p_i)$ (indeed, $G(p_i)$ is a subgroup), we have $qx_i=0$ for every $i$ , in particular for $i=j$ . Therefore $qx_j=0$ and $p_j|q$ . Therefore $p_j=q$ . I still need to prove that for every $p_j$ dividing $\text{exp}(G)$ there is a certain $x$ with $\text{ord}(x)=p_j$","['abelian-groups', 'group-theory', 'finite-groups']"
4677272,(Proof verification) Cohn's Measure Theory Ch 6.2 ex 3: $(\overline{D}\mu)(a)=(\underline{D}\mu)(a)=f(a)$ if $f$ is cont. at $a$,"This problem is Sect. 6.2 no. 3 of Measure Theory of Cohn, I want to know is my proof correct and is there a more concise proof? Let $f$ be a nonnegative function in $\mathscr{L}^1(\mathbb{R},\mathscr{B}(\mathbb{R}^d),\lambda,\mathbb{R})$ , and $\mu$ be the finite Borel measure on $\mathbb{R}^d$ given by $\mu(A)=\int_A f d\lambda$ . Show that $(D\mu)(x)=f(x)$ at each $x$ at which $f$ is continuous ( $\lambda$ is the Lebesgue measure). Proof. Let $a \in \mathbb{R}^d$ such that $f$ is continuous at $a$ . For each $\epsilon>0$ , there is a closed cube $C^{\epsilon}$ containing $a$ s.t. $f(x)\in [f(a)-\epsilon,f(a)+\epsilon]$ whenever $x\in C^\epsilon$ , then $$(f(a)-\epsilon)\lambda(C^{\epsilon})\leq\mu(C^\epsilon)=\int_{C^\epsilon} fd\lambda\leq(f(a)+\epsilon)\lambda(C^\epsilon)$$ This means $$\frac{(f(a)-\epsilon)\lambda(C^\epsilon)}{\lambda(C^\epsilon)}\leq \frac{\mu(C^\epsilon)}{\lambda(C^\epsilon)}\leq \frac{(f(a)+\epsilon)\lambda(C^\epsilon)}{\lambda(C^\epsilon)}$$ and therefore, $$(\overline{D}\mu)(a)=limsup_{\epsilon\rightarrow0}\{\frac{\mu(C)}{\lambda(C)}:C\in\mathscr{C},a\in C, e(C)<\epsilon\}=f(a)$$ ( $\mathscr{C}$ is the collection of all sets of the form $[a_1,b_1]\times...\times[a_d,b_d]$ and $e(C)$ the volume of $C$ ) and Similarily, $(\underline{D}\mu)(a)=f(a).$ Q.E.D.","['lebesgue-measure', 'measure-theory', 'solution-verification', 'lebesgue-integral']"
4677297,Assumption of Normality in Central Limit Theorem,"Regarding the distributions of random variables, I understand the following points: The distribution of the expected value (i.e. ""mean"") of any random variable is (asymptotically) normally distributed - this is stated within the Central Limit Theorem The sums and differences of normally distributed random variables are also normally distributed This being said, now consider the popular ""T-Test"" . The T-Test can be used to determine if the difference between the ""mean value"" of some random variable from two samples are equal or not - in this case, we can consider this ""value"" as a random variable. And since we know that the difference in the ""mean values"" of any two random variables follows a normal distribution, the T-Test exploits this fact and thereby uses the normal distribution to determine the mean differences between two samples is statistically significant or not. Something I have never quite been able to understand: In a T-Test, we are told that we  require the underlying distribution of both samples to be normally distributed - yet, the difference between the mean values of ANY two random variables is normally distributed . My Question: Thus, why is the assumption of normality required in a T-Test when we know that the difference between the mean values of any two random variables is always normally distributed (provided there are enough observations)? I can understand why this might not be the case when we have very few observations in each sample - but when we have many observations in both samples, why is the assumption of bormality still required for the T-Test? Thanks!","['statistical-inference', 'statistics', 'central-limit-theorem', 'hypothesis-testing', 'probability']"
4677298,On different types of convergence of Fourier Series.,"I‘m currently going through my lecture notes and am a bit confused by the different convergence results and would be happy if someone could clarify:) In the following we always have $f \in L^2((-\pi, \pi), \mathbb{C})$ and $c_k(f):=\frac{1}{2\pi}\int_{-\pi}^{\pi} f(x) e^{-ikx} \ dx$ . We define the $N^{th}$ Fourier partial sum as: $S_N(f)(x):=\sum_{k=-N}^N c_k(f)e^{ikx}$ . Now my first result is that with $\mathcal{F}:=\{\frac{e^{ikx}}{\sqrt{s\pi}} | k \in \mathbb{Z}\}$ the span $\operatorname{Span}(\mathcal{F})$ , i.e. the Fourier partial sums, are dense in the space of continuous, $2\pi-$ periodic functions with respect to the $L^{\infty}$ norm, i.e. the sup norm (up to set of measure zero). Then it follows (since continuous and compactly supported functions are dense in $L^2((-\pi, \pi), \mathbb{C})$ ), that $\operatorname{Span}(\mathcal{F})$ is also dense in all of $L^2((-\pi, \pi), \mathbb{C})$ w.r.t. the $L^2$ norm. Thus $\mathcal{F}$ is a basis for the Hilbert space $L^2((-\pi, \pi), \mathbb{C})$ . Now my professor remarks that the preceding does not imply that the Fourier sequence of Fourier partial sums converges uniformly to any continuous, compactly supported (on $(-\pi, \pi)$ ) $f$ . I‘m not sure I get that, we have just stated that: $$
\forall \epsilon>0 \ \exists S_N(f) \in \operatorname{Span}(\mathcal{F}): ||f-S_N(f)||_{L^{\infty}}=\operatorname{ess} \sup |f-S_n|=\inf \{M \in \mathbb{R} | \mu(\{x:|f(x)-S_N(f)(x)|>M\})=0\} < \epsilon
$$ Haven’t we? This is equivalent to uniform convergence almost everywhere, isn’t it? What’s the problem? Edit: The only problem I see here is that all coefficients in $S_N(f)$ are allowed to change as $\epsilon$ gets smaller, whereas if we consider uniform convergence „the coefficients with lower indices“ have to stay fixed as $\epsilon$ gets smaller (I hope this makes sense), could that be the issue? What about pointwise convergence almost everywhere, I know that if a sequence of functions converges in $L^p$ norm then there exists a subsequence that converges pointwise almost everywhere. Wouldn’t that suggest that for all $f \in L^2(-\pi, \pi)$ there is a sequence $(N_j)_{j \in \mathbb{N}}$ s.t.: $$
f(x)=\lim_{j \rightarrow \infty} \sum_{k=N_j}^{N_j}c_ke^{ikx}
$$ for almost every $x \in (-\pi, \pi)$ . I fail to see why this doesn’t imply: $$
f(x)=\lim_{N \rightarrow \infty} S_N(f)(x)
$$ Shouldn’t these limits be identical? Regarding this my professor remarks that we do indeed have pointwise convergence almost everywhere but that this isn’t a direct consequence from Hilbert space theory (only proved in 1966 by L. Carleson). So I‘d be happy if someone could clarify my confusion regarding uniform and pointwise convergence:)","['measure-theory', 'fourier-analysis', 'harmonic-analysis', 'functional-analysis', 'fourier-series']"
4677299,Convexity/concavity of a strictly increasing and continuous function,"Consider a continuous, strictly increasing function $f:\mathbb{R}_{+}\to \mathbb{R}_{+}$ with $f(0)=0$, and $x>f(x)$ for all $x>0$. Is this enough to conclude anything about convexity/concavity in the neighborhood of 0? In other words, Is there an $\varepsilon>0$ such that $f$ is convex or concave on $[0,\varepsilon]$? If not, what would a counterexample look like?","['convex-analysis', 'real-analysis']"
4677318,Apostol: Why exactly is $\frac{1}{2}\sqrt{\frac{y}{x}}$ not defined at origin if the partial derivative of $\sqrt{|xy|}$ exists at origin?,"Consider the scalar field $f(x,y)=\sqrt{|xy|}$ . Here is a $3D$ plot In Apostol's Calculus we are asked to verify that the partial derivatives $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ are zero at the origin. I know how to show this by using the definition of partial derivatives. For example $$\frac{\partial f(0,0)}{\partial x}=\lim\limits_{h\to 0} \frac{f(\langle 0,0\rangle+h\langle 1,0\rangle)-f(0,0)}{h}\tag{1}$$ $$\lim\limits_{h\to 0} \frac{f(h,0)-f(0,0)}{h}\tag{2}$$ $$=0$$ But my question regards the expression $$\frac{\partial f(x,y)}{\partial x}=\frac{1}{2}\sqrt{\frac{y}{x}}\tag{3}$$ which is the partial derivative when $x>0$ and $y>0$ (alternatively, when both are negative). What are the precise reasons that this expression isn't true at the origin, and yet the partial derivative is defined at the origin? Note that this expression isn't defined at the origin. But why? It is defined for all points except the origin on the line of points of form $(x,0)$ . To obtain (3), we took the limit of $$\frac{f(x+h,y)-f(x,y)}{h}\tag{4}$$ and the resulting expression isn't defined at the origin. On the other hand to show that the partial derivative does exist at the origin we took the limit of $$\frac{f(0+h,0)-f(0,0)}{h}\tag{5}$$ It is not clear to me why the limit of this expression is defined, but the limit of (4) evaluated at the origin isn't defined.","['partial-derivative', 'limits', 'multivariable-calculus']"
4677334,Why are these two projective schemes isomorphic?,"From Ravi Vakil's Foundations of Algebraic Geometry, exercise 7.4.C: Let $R = k[x, y, z]/(xz, yz, z^2)$ . Show that $\operatorname{Proj} R \cong \mathbb{P}_k^1$ (as schemes). My thought was to induce a map of schemes via the ring morphism $\phi: R \rightarrow k[s, t]$ , with $x \mapsto s, \; y \mapsto t, \; z \mapsto 0$ . But I don't know how to show that this induces an isomorphism of schemes. In particular, in $\operatorname{Proj} R$ the two ideals $(x + y)$ and $(x + y + z)$ (for example) are different, which doesn't seem to have a counterpart in $\mathbb{P}_k^1$ . Edit: It's been suggested that we can check this locally. To check locally, define $\alpha:k[s,t]→R$ by $s,t↦x,y$ . This induces $\pi:\operatorname{Proj}R→\mathbb{P}_1^k$ such that $\pi^{-1}(D(s))=D(x)$ and $\pi^{-1}(D(t))=D(y)$ . I can see that $\pi$ gives a bijection on each of these open sets, but I can't see that it gives isomorphisms of schemes: for example, taking $\varphi:\mathcal{O}_{\operatorname{Proj}R}→\mathcal{O}_{\mathbb{P}_1^k}$ induced by $\alpha$ , we get $\varphi(D(s)):k[\frac{t}{s}]→k[\frac{y}{x},\frac{z}{x}]$ , which is not an isomorphism. Am I doing this wrong?","['algebraic-geometry', 'projective-schemes']"
4677340,Is probability theory just a theory of finite measure in abstract sense? Or is there anything more to it?,"I have had some exposure to measure theoretic probability theory/Brownian motion, and a bit of analysis. There are a lot of theories in probability whose intuition come from real world example but essentially probability theory is a study of random variable whose domain is a sample space, whose measure is 1 (or finite). So in a sense, there's nothing special about probability theory in an abstract sense. If I view it as something in the category of pure mathematics, is it legitimate to say that probability theory is just a subset of analysis dealing with finite measure? Or, is there any subtlety that I'm missing? I'd appreciate any insights.",['probability-theory']
4677347,How to solve the ODE $(y(x))^{((y'(x))^{((y''(x))^{((y'''(x))^{\cdot^{\cdot^{\cdot}}})})})}=f(x)$?,"Introduction Why This Question? I have often asked myself what a solution to the Ordinary Differential Equation ( ODE ) would be and when I recently saw this ODE again, tried it again and failed again, I wanted to ask how this ODE was solved could become? The latest reason I'm asking this question has to do with Fractional Calculus , for which I'm asking myself a similar but a lot more complex question. Clarification Of The Question To make the equation clearer: Let's say that $\operatorname{D_{x}^{v}}\left[ f \right]$ is the $v$ th derivative with respect to the variable $x$ of the function $f$ and that the "" Power Tower "" $a^{b^{c^{d^{\cdot^{\cdot^{\cdot}}}}}}$ is $a^{\left( b^{\left( c^{\left( d^{\cdot^{\cdot^{\cdot}}} \right)} \right)} \right)}$ . This gives us the following ODE : $$
\begin{align*}
\left( \operatorname{D_{x}^{0}}\left[ y\left( x \right) \right] \right)^{\left( \operatorname{D_{x}^{1}}\left[ y\left( x \right) \right] \right)^{\left( \operatorname{D_{x}^{2}}\left[ y\left( x \right) \right] \right)^{\cdot^{\cdot^{\cdot}}}}} &= f\left( x \right)\\
\end{align*}
$$ The whole thing goes up to the $n$ th derivative of $y$ where $n \in \mathbb{N}$ : $$
\begin{align*}
\left( \operatorname{D_{x}^{0}}\left[ y\left( x \right) \right] \right)^{\left( \operatorname{D_{x}^{1}}\left[ y\left( x \right) \right] \right)^{\left( \operatorname{D_{x}^{2}}\left[ y\left( x \right) \right] \right)^{\cdot^{\cdot^{\cdot^{\left( \operatorname{D_{x}^{n - 1}}\left[ y\left( x \right) \right] \right)^{\operatorname{D_{x}^{n - 1}}\left[ y\left( x \right) \right]}}}}}}} &= f\left( x \right)\\
\end{align*}
$$ Now let us rewrite left-hand side to $P_{n}\left( y\left( x \right) \right) \equiv \left( \operatorname{D_{x}^{0}}\left[ y\left( x \right) \right] \right)^{\left( \operatorname{D_{x}^{1}}\left[ y\left( x \right) \right] \right)^{\left( \operatorname{D_{x}^{2}}\left[ y\left( x \right) \right] \right)^{\cdot^{\cdot^{\cdot^{\left( \operatorname{D_{x}^{n - 1}}\left[ y\left( x \right) \right] \right)^{\operatorname{D_{x}^{n - 1}}\left[ y\left( x \right) \right]}}}}}}}$ were $P_{n}$ is an operator defined by this equation. With that, we can write the ODE from the question title as follows: $$
\begin{align*}
\lim\limits_{n \to \infty}\left[ P_{n}\left( y\left( x \right) \right) \right] = f\left( x \right) \tag{1}\\
\end{align*}
$$ Now the question again: How to solve the ODE $\lim\limits_{n \to \infty}\left[ P_{n}\left( y\left( x \right) \right) \right] = f\left( x \right)$ ? My Best Attempts At Finding A Solution Try And Error / ""Guessing"" Trying $y\left( x \right) = c \cdot \exp\left( x \right)$ $y\left( x \right) = c \cdot \exp\left( x \right)$ where $c$ is some constant gives us: $$
\begin{align*}
\left( \operatorname{D_{x}^{0}}\left[ c \cdot \exp\left( x \right) \right] \right)^{\left( \operatorname{D_{x}^{1}}\left[ c \cdot \exp\left( x \right) \right] \right)^{\left( \operatorname{D_{x}^{2}}\left[ c \cdot \exp\left( x \right) \right] \right)^{\cdot^{\cdot^{\cdot}}}}} &= f\left( x \right)\\
\left( \operatorname{D_{x}^{0}}\left[ y\left( x \right) \right] \right)^{\left( \operatorname{D_{x}^{1}}\left[ y\left( x \right) \right] \right)^{\left( \operatorname{D_{x}^{2}}\left[ y\left( x \right) \right] \right)^{\cdot^{\cdot^{\cdot}}}}} &= f\left( x \right)\\
\left( c \cdot \exp\left( x \right) \right)^{\left( c \cdot \exp\left( x \right) \right)^{\left( c \cdot \exp\left( x \right) \right)^{\cdot^{\cdot^{\cdot}}}}} &= f\left( x \right)\\
\left( c \cdot \exp\left( x \right) \right) \uparrow\uparrow \infty &= f\left( x \right) \tag{2.1}\\
\end{align*}
$$ where $\uparrow \uparrow$ comes from the Knuth's Up-Arrow Notation . A plot of $f$ for $c = 1$ : That would give us a ( $y\left( x \right) = c \cdot \exp\left( x \right)$ ) solution to the special case $\lim\limits_{n \to \infty}\left[ P_{n}\left( y\left( x \right) \right) \right] = \left( c \cdot \exp\left( x \right) \right) \uparrow\uparrow \infty$ . Trying $y\left( x \right) = c \cdot \sin\left( x + n \cdot \frac{\pi}{2} \right)$ $y\left( x \right) = c \cdot \sin\left( x + n \cdot \frac{\pi}{2} \right)$ where $c$ and $n$ with $n \in \mathbb{Z}$ are some constants gives us some simple observtions: $$
\begin{align*}
c \cdot \sin\left( x + 0 \cdot \frac{\pi}{2} \right) &= c \cdot \sin\left( x \right)\\
c \cdot \sin\left( x + 1 \cdot \frac{\pi}{2} \right) &= c \cdot \cos\left( x \right)\\
c \cdot \sin\left( x + 2 \cdot \frac{\pi}{2} \right) &= -c \cdot \sin\left( x \right)\\
c \cdot \sin\left( x + 3 \cdot \frac{\pi}{2} \right) &= -c \cdot \cos\left( x \right)\\
c \cdot \sin\left( x + 4 \cdot k \cdot n \cdot \frac{\pi}{2} \right) &= c \cdot \sin\left( x + n \cdot \frac{\pi}{2} \right)\\
\end{align*}
$$ where $k$ is some intiger ( $k \in \mathbb{Z}$ ) constant. This allows us to: $$
\begin{align*}
\left( \operatorname{D_{x}^{0}}\left[ c \cdot \sin\left( x + n \cdot \frac{\pi}{2} \right) \right] \right)^{\left( \operatorname{D_{x}^{1}}\left[ c \cdot \sin\left( x + n \cdot \frac{\pi}{2} \right) \right] \right)^{\left( \operatorname{D_{x}^{2}}\left[ c \cdot \sin\left( x + n \cdot \frac{\pi}{2} \right) \right] \right)^{\cdot^{\cdot^{\cdot}}}}} &= f\left( x \right)\\
\left( c \cdot \sin\left( x + \left( n + 0 \right) \cdot \frac{\pi}{2} \right) \right)^{\left( c \cdot \sin\left( x + \left( n + 1 \right) \cdot \frac{\pi}{2} \right) \right)^{\left( c \cdot \sin\left( x + \left( n + 2 \right) \cdot \frac{\pi}{2} \right) \right)^{\cdot^{\cdot^{\cdot}}}}} &= f\left( x \right) \tag{2.2}\\
\end{align*}
$$ A plot of $f$ for $c = 1$ and $n = 0$ : Not displayed because the plot is probably too complex. Someone is welcome to add a plot of this. Trying $y\left( x \right) = \frac{1}{x}$ This gives us: $$
\begin{align*}
\left( \operatorname{D_{x}^{0}}\left[ \frac{1}{x} \right] \right)^{\left( \operatorname{D_{x}^{1}}\left[ \frac{1}{x} \right] \right)^{\left( \operatorname{D_{x}^{2}}\left[ \frac{1}{x} \right] \right)^{\cdot^{\cdot^{\cdot}}}}} &= f\left( x \right)\\
\left( \frac{1}{x} \right)^{\left( -\frac{1}{x^{2}} \right)^{\left( \frac{1}{x^{3}} \right)^{\cdot^{\cdot^{\cdot}}}}} &= f\left( x \right)\\
\sqrt[\ddots]{\frac{1}{x}} &= f\left( x \right)\\
\end{align*}
$$ A Plot of $f$ : Observating Since the ODE contains a sort of infinite "" Power Tower "" and $0^{0}$ is undefined, $\operatorname{D_{x}^{n}}\left[ y\left( x \right) \right] \ne 0,\, \text{for}\, n \in \mathbb{N}$ must hold 'cause some $\operatorname{D_{x}^{z}}\left[ y\left( x \right) \right] = 0$ where $z$ is some Number in $\mathbb{N}$ implies $\cdot^{\operatorname{D_{x}^{z}}\left[ y\left( x \right) \right]^{\operatorname{D_{x}^{z + 1}}\left[ y\left( x \right) \right]^{\cdot}}} = \cdot^{0^{0^{0^{\cdot}}}}$ . This means that $y$ must be derivable infinitely often in at least one interval without ever yielding $0$ in this interval, which already rules out a large number of functions. As I write this I realize that it is difficult to find a legible but correct and accurate notation for this. I would also be open to anyone suggesting a better notation. The simplest case of functions for which this is the case are functions which have a derivative of order $n$ , which gives the function or another derivative of the function itself or in formuls: $\operatorname{D_{x}^{v}}\left[ y\left( x \right) \right] = \operatorname{D_{x}^{w}}\left[ y\left( x \right) \right],\, \text{for}\, \mathbb{N} \ni w \ne v \in \mathbb{N}$ I would solve this ODE like this (for this I'll assume $v > w$ ): $^{\left[ 1 \right]}$ $$
\begin{align*}
\operatorname{D_{x}^{v}}\left[ y\left( x \right) \right] &= \operatorname{D_{x}^{w}}\left[ y\left( x \right) \right]\\
\operatorname{D_{x}^{v - w}}\left[ y\left( x \right) \right] &= y\left( x \right)\\
\operatorname{D_{x}^{v - w}}\left[ e^{\lambda \cdot x} \right] &= e^{\lambda \cdot x}\\
\lambda^{v - w} \cdot e^{\lambda \cdot x} &= e^{\lambda \cdot x}\\
\lambda^{v - w} &= 1\\
\lambda &= 1^{\frac{1}{v - w}}\\
\lambda_{k} &= \left( \exp\left( 2 \cdot k \cdot \pi \cdot i \right) \right)^{\frac{1}{v - w}}\\
\lambda_{k} &= \exp\left( \frac{2 \cdot k \cdot \pi}{v - w} \cdot i \right)\\
\\
\lambda_{k} &= \operatorname{cis}\left( \frac{2 \cdot k \cdot \pi}{v - w} \right)\\
\lambda_{k} &= \cos\left( \frac{2 \cdot k \cdot \pi}{v - w} \right) + \sin\left( \frac{2 \cdot k \cdot \pi}{v - w} \right) \cdot i\\
\end{align*}
$$ where $\operatorname{cis}$ is the [CiS-Function][8]. That means $\lambda^{v - w} + 1 = \prod\limits_{k = 1}^{\left| \mathbb{S} \right|}\left( \lambda - \operatorname{cis}\left( \frac{2 \cdot k \cdot \pi}{v - w} \right) \right)$ where $\mathbb{S}$ is the set of all roots of the equation and $\left| \mathbb{S} \right|$ is the [Cardinality][9] of $\mathbb{S}$ . This gives the multiplicity of the all roots $= 1$ . With $y_{h}\left( x \right) = Q_{0}\left( x \right) \cdot \exp\left( \Re\left( \lambda_{k} \right) \cdot x \right) \cdot \cos\left( \Im\left( \lambda_{k} \right) \cdot x \right) + P_{0}\left( x \right) \cdot \exp\left( \Re\left( \lambda_{k} \right) \cdot x \right) \cdot \sin\left( \Im\left( \lambda_{k} \right) \cdot x \right)$ we'll get: $$
\begin{align*}
y_{h}\left( x \right) &= Q_{0}\left( x \right) \cdot \exp\left( \Re\left( \lambda_{k} \right) \cdot x \right) \cdot \cos\left( \Im\left( \lambda_{k} \right) \cdot x \right) + P_{0}\left( x \right) \cdot \exp\left( \Re\left( \lambda_{k} \right) \cdot x \right) \cdot \sin\left( \Im\left( \lambda_{k} \right) \cdot x \right)\\
y_{h}\left( x \right) &= \sum\limits_{k = 1}^{\left| \mathbb{S} \right|}\left[ c_{2 \cdot k} \cdot \exp\left( \cos\left( \frac{2 \cdot k \cdot \pi}{v - w} \right) \cdot x \right) \cdot \cos\left( \sin\left( \frac{2 \cdot k \cdot \pi}{v - w} \right) \cdot x \right) + c_{2 \cdot k + 1} \cdot \exp\left( \cos\left( \frac{2 \cdot k \cdot \pi}{v - w} \right) \cdot x \right) \cdot \sin\left( \sin\left( \frac{2 \cdot k \cdot \pi}{v - w} \right) \cdot x \right) \right] \tag{3.1.1}\\
\end{align*}
$$ where $\mathbb{S} = v - w$ . But there are also the solutions given by: $$
\begin{align*}
\operatorname{D_{x}^{v}}\left[ y\left( x \right) \right] &= \operatorname{D_{x}^{w}}\left[ y\left( x \right) \right]\\
\operatorname{D_{x}^{v - 1}}\left[ y\left( x \right) \right] &= \operatorname{D_{x}^{w - 1}}\left[ y\left( x \right) \right] + c_{-1}\\
\operatorname{D_{x}^{v - 2}}\left[ y\left( x \right) \right] &= \operatorname{D_{x}^{w - 1}}\left[ y\left( x \right) \right] + c_{-2} + c_{-1} \cdot x\\
&\cdots\\
\operatorname{D_{x}^{v - w}}\left[ y\left( x \right) \right] &= y\left( x \right) + \sum\limits_{k = 1}^{w}\left[ c_{-k} \cdot x^{k} \right] \tag{3.1.2}\\
\end{align*}
$$ So $y\left( x \right) = \sum\limits_{k = 1}^{w}\left[ c_{-k} \cdot x^{k} \right] + \sum\limits_{k = 1}^{\left| v \right|}\left[ c_{2 \cdot k} \cdot \exp\left( \cos\left( \frac{2 \cdot k \cdot \pi}{v - w} \right) \cdot x \right) \cdot \cos\left( \sin\left( \frac{2 \cdot k \cdot \pi}{v - w} \right) \cdot x \right) + c_{2 \cdot k + 1} \cdot \exp\left( \cos\left( \frac{2 \cdot k \cdot \pi}{v - w} \right) \cdot x \right) \cdot \sin\left( \sin\left( \frac{2 \cdot k \cdot \pi}{v - w} \right) \cdot x \right) \right]$ aka $y\left( x \right) = \sum\limits_{k = 1}^{w}\left[ c_{-k} \cdot x^{k} \right] + \sum\limits_{k = 1}^{v - w}\left[ c_{2 \cdot k} \cdot \exp\left( \cos\left( \frac{2 \cdot k \cdot \pi}{v - w} \right) \cdot x \right) \cdot \cos\left( \sin\left( \frac{2 \cdot k \cdot \pi}{v - w} \right) \cdot x \right) + c_{2 \cdot k + 1} \cdot \exp\left( \cos\left( \frac{2 \cdot k \cdot \pi}{v - w} \right) \cdot x \right) \cdot \sin\left( \sin\left( \frac{2 \cdot k \cdot \pi}{v - w} \right) \cdot x \right) \right] \tag{3.1.3}$ . Which would be one possible form that $y$ could take to be defined and thus be defined for a given $f$ . Another way to satisfy the condition is that we solve the inequality $\operatorname{D_{x}^{z}}\left[ y\left( x \right) \right] \ne 0$ . But we have already solved that for $z = w$ (a similar equation), see Formula $\left( 3.1.2 \right)$ , with: $$y\left( x \right) \ne \sum\limits_{k = 1}^{z}\left[ c_{-k} \cdot x^{k} \right] \tag{3.1.4}$$ But that's obvious, 'since every finite polynomial is no solution.","['power-towers', 'ordinary-differential-equations']"
4677380,"Probability of drawing a red ball before a blue ball, after already drawing the first blue ball","Given we have $20$ balls in a bucket: $4$ blue, $4$ red, rest are all white. We draw randomly from the bucket without replacement until we see the first blue ball. Now we continue drawing. Which is more likely to happen first - seeing a red ball, or seeing another blue ball? Now, I did some simulations and the answer seems to be that both are equally likely to happen. But I'm having trouble seeing this mathematically. My thought process is as follows: After the first blue ball is drawn, the probability of drawing a red or another blue ball first is determined by how many red and blue balls are left in the bucket. Now, obviously there are only $3$ blue balls left. But the expected number of red balls left are $3\frac{1}{5}$ . This is because, as shown below, , we can think of the order of us drawing balls as a permutation of the balls, and so each of the red ball have equal likelihood of being placed in one of the red slots, and so expected number of red ball in each red slot is $4/5$ , and thus the expected number of red balls after the first ""B"" is $4/5*4 = 3\frac{1}{5}$ . So we are expecting more red balls than blue balls remaining in the bucket. Then shouldn't that mean we have higher probability of draw red ball first before anothehr blue ball?","['conditional-probability', 'probability-distributions', 'probability']"
4677442,Find the global minimum of $ab+a+b$ subject to $a^2+b^2=25$,"Let $a,b$ be real numbers. İf $a^2+b^2=25$ find minimum value of $ab+a+b$ My solution: $b=\sqrt{25-a^2}$ $f(a)=a\sqrt{25-a^2}+a+\sqrt{25-a^2}$ and I use $f'(a)=0$ But this way long and difficult to me. Is there any easy solution? Thanks.","['maxima-minima', 'calculus', 'algebra-precalculus']"
4677515,Differential-difference equation solution,"I have the following set of differential-difference equations $$
\begin{aligned}
& (s F(s))^{\prime}=f(s-1) \text { for } s>1 \\
& (s f(s))^{\prime}=F(s-1) \text { for } s>2
\end{aligned}$$ with initial conditions $F(s)=\frac{2 e^\gamma}{s}$ for $1 \leq s \leq 3, f(s)=0$ for $1 \leq s \leq 2$ I want to show that the equations above are satisfied by $$  \begin{gathered}
F(s):=2 e^\gamma\left(\frac{\mathbb{1}_{s>1}}{s}+\sum_{j \geq 3, \text { odd }} \frac{1}{j !} \int_{[1,+\infty)^{j-1}} \mathbb{1}_{t_1+\ldots t_{j-1} \leq s-1} \frac{d t_1 \ldots d t_{j-1}}{t_1 \ldots t_j}\right) \\
f(s):=2 e^\gamma \sum_{j \geq 2, \text { even }} \frac{1}{j !} \int_{[1,+\infty)^{j-1}} \mathbb{1}_{t_1+\ldots +t_{j-1} \leq s-1} \frac{d t_1 \ldots d t_{j-1}}{t_1 \ldots t_j}
\end{gathered}
$$ with the convention that $t_j:=s-t_1-\cdots-t_{j-1}$ . I am struggling to see how I can differentiate when the indicator is under the integral. I have tried a Dirac delta approach, as well as a recursive argument but I didn't get anywhere. Any pointers would appreciated.","['integration', 'number-theory', 'analytic-number-theory', 'recurrence-relations']"
4677534,What is the probability of choosing a white ball again,"I am struggling with the math conditional probability task: There are two boxes. One has 14 white and 32 black balls, another has 23 white and 43 black balls. Choosing a box randomly, we choose a white ball. What is a probability, that from the same box we will choose a white ball again? My solution: In general, there are 46 balls in the first box, and 66 box in the second one. There is a condition, that we already chose a white ball and we gonna choose from the same box white ball. So, to choose a ball from the first box again, we have a probability: $$\frac{14-1}{46-1} \cdot \frac{1}{2}$$ and from the second box: $$\frac{23-1}{66-1} \cdot \frac{1}{2}$$ I know, that If I sum these two probabilities the result will be wrong because I did not specify a condition. I believe I need to use Bayes' theorem. Thus: $$P(A \mid B) = \frac{P(A~\text{and}~B)}{P(B)}$$ A is event of choosing a white ball again from the same box knowing that B is true (B = firstly we chose a white ball from the random box) $$P(A~\text{and}~B) = P(A) \cdot P(B)$$ Then we have: $$\frac{1}{2} \cdot \frac{P(\text{Probability of choosing a white ball})}{P(\text{Probability of choosing a white ball in general})}$$ I am getting a wrong result and I am struggling with the correctness of my solution","['conditional-probability', 'probability']"
4677539,Sigma-algebra generated by conditional expectation,"I am dealing with the following question: given two dependent random variables $X_1,X_2$ , I am wondering whether the following equivalence for the generated sigma-algebras holds: $$\sigma(X_1)=\sigma(X_1 + c) $$ $$\sigma(X_1,X_2)=\sigma(X_1 + c,X_2+\mathbb{E}[X_2|X_1])$$ where $c$ is a known constant. For the first one. I am pretty sure that it holds, as the heuristic is telling us: on LHS ""I know"" $X_1$ and on the RHS ""I know"" $X_1$ up to a known constant. For the second one: the conditional expectation $\mathbb{E}[X_2|X_1]$ is $\sigma(X_1)$ -measurable, i.e. $$ \sigma(\mathbb{E}[X_2|X_1]) \subset \sigma(X_1)$$ and it can be written as $f(X_1)$ a.s., where $f$ is Borel measurable.
So what we need to show is that $$\sigma(X_1,X_2)=\sigma(X_1 + c,X_2+f(X_1))$$ But as $f$ is deterministic, and given 1., this means that 2. holds, if the following holds $$\sigma(X_1,X_2)=\sigma(X_1,X_2+X_1)$$ which looks correct to me. Am I right? Is it enough to say that 2. holds? I have tried to write it formally with sets but I did not manage to find evidence of 2. At the same time I have not been able to find a counterexample. Any hint or idea would be helpful. Thank you in advance.","['measure-theory', 'conditional-expectation', 'measurable-functions', 'probability-theory', 'random-variables']"
4677548,Proof for the general formula for $a^n+b^n$.,"Based on the following observations. That is $$a+b = (a+b)^1  \\  a^2+b^2 = (a+b)^2-2ab \\ a^3+b^3 = (a+b)^3-3ab(a+b) \\ a^4+b^4= (a+b)^4-4ab(a+b)^2+2(ab)^2\\ a^5+b^5 
= (a+b)^5 -5ab(a+b)^3+5(ab)^2(a+b)\\\vdots$$ I came to  make the following conjecture as  general formula. $$ a^n +b^n =\sum_{k=0}^{n-1}(-1)^k \frac{n\Gamma(n-k)}{\Gamma(k+1)\Gamma(n-2k+1)}(a+b)^{n-2k}(ab)^k $$ where $\Gamma(.) $ is gamma function. I tried up proving the result using binomial theorem $\displaystyle (a+b)^n=\sum_{r=0}^n a^{n-r}b^r$ for positive integers $a,b$ however, I didn't find  any elegance in the work.  So in the expect of some beautiful proofs,  I wish to share  general formula here. Thank you","['summation', 'binomial-coefficients', 'combinatorics']"
4677572,"Show that if $K\triangleleft A\times B$ is non-abelian, then at least one of $K\cap A,K\cap B$ is non-trivial.","Let $A,B$ be groups, $K\triangleleft A\times B$ be a non-abelian normal subgroup. Show that at least one of the intersection $K\cap A$ , $K\cap B$ is non-trivial. My attempts: We assume by contradiction that $K\cap A$ , $K\cap B$ are both trivial. We can show that the natural homomorphism: $$ \phi: A\times B \rightarrow (A/K\cap A)\times (B/K\cap B) $$ has kernel $\ker\phi=K$ , therefore by assumption $A\times B$ is isomorphic to $(A\times B)/K$ , I believe this implies that $K$ is Abelian, otherwise there will be some non-trivial commutator lies in K, therefore trivial in $(A\times B)/K$ . I feel a bit unsafe about this proof, I am not sure that ' $A\times B$ is isomorphic to $(A\times B)/K$ ' can happen. Are there any corrections or comments? THANKS! Edit: This proof seems problematic, maybe $\ker\phi\neq K$ ? Are there any other ideas?","['normal-subgroups', 'group-theory', 'abstract-algebra', 'direct-product']"
4677579,Combinatorics: How many paths?,"Original Question: Students sit at their desks in three rows of eight. Felix, the class pet, must be passed to each student exactly once, starting with Alex in one corner and finishing with Bryn in the opposite corner. Each student can pass only to the immediate neighbour left, right, in front or behind. One possible path is shown. How many different paths can Felix take from Alex to Bryn? So what confuses me about this question is typically this style of ""how many paths"" gives a more limited direction, e.g. only right and up towards the target. However, in this question, the students can pass left, right, in front, and behind. So the traditional method I usually use for these type of questions of counts on points wouldn't work (finding sum of number of ways to each point)? Or at least I think it doesn't work.",['combinatorics']
4677593,Characterization of function with $n$-th derivative bounded by $n!$,"Given a $f \in C^\infty_0(\mathbb{R}^n)$ (the set of smooth functions that vanishes at infinity) is in general false that its derivative are bounded, also if the function is bounded (see e.g. $f(x)=\frac{\sin(e^x-1)}{x}$ ). For polynomial we know that $P_n^{(n)}(x)=Cn!$ where $C$ is a constant. There is some similar estimation for some subclass of $C^\infty_0$ functions i.e. is it possible to characterize in some way the functions such that for any $n > N_0$ $$
\sup_{x \in \mathbb{R}^n}f^{(n)}(x) \le n! M^n
$$ where $N_0$ and $M$ are constants? EDIT: the function $
f(x):= e^{-|x|^2} 
$ satisfies all the conditions so not only the polynomial satisfies this inequality. Is it true also for all the compactly supported functions on $\mathbb{R}^n$ ?","['estimation', 'real-analysis', 'complex-analysis', 'functions', 'exponential-function']"
4677626,Did I use Stirling formula incorrectly?,I have to prove that $\frac{\sqrt{2\pi n} * e^{-n}*n^k}{k!}  \to e^{-x^2/2}$ when $\frac{k-n}{\sqrt{n}} \to x$ and $n \to \infty$ I used Stirling formula and get that $\frac{\sqrt{2\pi n} * e^{-n}*n^k}{k!} = e^{k-n} * (n/k)^k \frac{\sqrt{2\pi n}}{\sqrt{2\pi k}}$ From $\frac{k-n}{\sqrt{n}} \to x$ we get that $n/k \to 1$ than $\frac{\sqrt{2\pi n}}{\sqrt{2\pi k}} \to 1$ So we get $e^{k-n} * (n/k)^k$ But $(n/k)^k = (1 + (n-k)/k)^{k/(n-k) * (n-k)} = e^{n-k}$ because $(n-k) / k \to 0$ So everything tends to $e^{k-n} * e^{n-k} = 1$ What do I do wrong?(,['analysis']
4677649,"Does the functor $\operatorname{Hom}(\operatorname{Spec}(-),X)$ preserve filtered colimits?","I want to apply this Lemma to the functor $X(-)$ for a scheme $X$ which is locally finite presentation over $A$ , which by definition is just the functor $\text{Hom}(\text{Spec}\,(-),X)$ . But this requires the functor to preserve filtered colimits.","['algebraic-geometry', 'category-theory']"
4677654,Borel subsets of $\mathbb{R}$ and membership algorithms,"Edit : As the comments by many people show, the answer is no, and my argument for open sets is wrong. Still, I believe that my mistakes are interesting, so I'm not going to delete the question. Fix a Borel subset $B$ of $\mathbb{R}$ . Is there an algorithm that takes $x \in \mathbb{R}$ as input and decides whether $x$ belongs to $B$ ?
The algorithm is allowed to make queries of the form ""Compare $x$ to $y$ "" for $y\in\mathbb{R}$ . The result of such a query is one of $<$ , $=$ and $>$ . Note that $B$ is arbitrary but fixed. It is not part of the input of the algorithm. If $B$ is open, such an algorithm exists: First we can find an integer $T$ such that $|x|\leq T$ . Then it remains to decide whether or not $x$ belongs to one of finitely many open intervals. Note that my notion of an algorithm is allowed to have real numbers as variables and perform arithmetic operations on them. My notion of an algorithm is also allowed to have a pre-computed infinite sequence of real values in memory (for my example where $B$ is open, I'm assuming that the endpoints of the intervals whose union is $B$ are stored in memory before the algorithm starts running, in a convenient order). My general motivation here is my intuition that in some sense Borel sets are ""computable"". I haven't decided exactly what this means yet, but the question above depicts my first attempt. For this reason, if you think there's a similar more interesting question, please let me know.","['borel-sets', 'measure-theory', 'computability']"
4677759,How to factor a polynomial quickly in $\mathbb{F}_5[x]$,"I was doing an exercise in Brzezinski's Galois Theory Through Exercises and needed to factor the polynomial $x^6+5x^2+x+1=x^6+x+1$ in $\mathbb{F}_5[x]$ . Is there a quick way to do this? I can see it has no roots and therefore no linear factors, so it must either factor as the product of a quadratic and a quartic or two cubics, but listing the irreducible quadratics and cubics mod $5$ is laborious. I found all of the irreducible quadratics, which are \begin{multline} 
x^2\pm 2, \, x^2+x+1, \, x^2+x+2, \, x^2+2x-2, \, x^2+2x-1, \\ 
x^2-2x+2, \, x^2-2x-2, \, x^2-x+1, x^2-x+2
\end{multline} and checked that none of these are factors, so it must factor as the product of two cubics, which I know I could find either by finding all irreducible cubics or by writing $$
x^6+x+1 = (x^3+ax^2+bx+c)(x^3+dx^2+ex+f) 
$$ and then equating coefficients. I used a computer to perform the Berlekamp algorithm which yielded the factorisation $$
x^6+5x^2+x+1 = (x^3-2x^2-1)(x^3+2x^2-x-1)
$$ but it would take me a very long time to do this by hand. Can anyone suggest any tricks that could be applied here?","['irreducible-polynomials', 'finite-fields', 'abstract-algebra', 'factoring', 'polynomials']"
4677806,Solve for $x$ correct to $3$ decimal places: $27=\frac{1}{6^x}$,"$\color{white}{\require{cancel}{placeholder}}$ So recently, I decided to do some fairly basic trigonometry for fun to see what I was able to remember. I decided to give myself a challenge and attempted to solve $27=\frac{1}{6^x}$ for $x$ to $3$ decimal places. While I was able to solve the equation, I'm not sure that I was right with the answer that I achieved. Here's how I got my answer: Set up the equation: \begin{align}
27 = \frac{1}{6^x}
\end{align} Multiply both sides by $6^x$ : \begin{align}
27(6^x) = \frac{1}{\cancel{6^x}}(\cancel{6^x})\iff 27(6^x) = 1
\end{align} Divide both sides by $27$ : \begin{align}
\frac{\cancel{27}(6^x)}{\cancel{27}}=\frac{1}{27} & \iff 6^x = \frac{1}{27}
\end{align} Rewrite the equation in logarithmic form: $a^b=x \implies \log_ax=b$ \begin{align}
\log_6\frac{1}{27}=x \iff \log_627^{-1}=x \iff -\log_627=x
\end{align} Simplifying the logarithm, we get $x \approx -1.839$ . This is my question: Was I correct with the solution that I achieved, and if not, how would I achieve the correct answer?","['algebra-precalculus', 'solution-verification', 'logarithms']"
4677844,Prove that $\int_{0}^{+\infty} e^{-tx}\frac{\sin x}{x} dx$ is differentiable,"I'm trying to prove that $$F(t) = \int_{0}^{+\infty} e^{-tx}\frac{\sin x}{x} dx$$ is differentiable in $]0,+\infty [$ . First, $F(t)$ is well defined since $F(t) \leq \int_{0}^{+\infty} e^{-tx} dx$ whose integration is finite. Second, I've tried first to prove that $F(t)$ is continuous using dominated convergence theorem, by that I have to find a domination as an integrable function $g$ so that $$
\lvert e^{-tx}\frac{\sin x}{x} \rvert \leq g(x)
$$ I've thought of $g(x) = \lvert \frac{\sin x}{x} \rvert$ but it's not a good candidate since it is not Lebesgue integrable. So I've used directly the definition of continuation, it seems good since: $$
\lvert F(t) - F(t') \rvert \leq \int_{0}^{+\infty} \lvert e^{-tx} - e^{-t'x} \rvert dx \overset{t \to t'}{\to} 0
$$ Third, unfortunately, this direction doesn't help to prove that $F(t)$ is derivable. So, I've came back to the differentiability under domination, it doesn't seem working neither. I've tried to look for a domination of $$
\lvert (e^{-tx}\frac{\sin x}{x})' \rvert = \lvert t e^{-tx}\frac{\sin x}{x}\rvert
$$ but cannot found any. Many thanks for any suggestion.","['integration', 'lebesgue-integral', 'real-analysis', 'continuity', 'derivatives']"
4677900,Set of generators of the alternating group $A_9$ . The group $A_9$ is generated by $(14)(29)(37)(56)$ and $(123)(456)(789)$.,"I was reading K. Conrad's paper on the group $SL_2(\mathbb{Z})$ . There he says the group $A_9$ turns out to be generated by $(14)(29)(37)(56)$ and $(123)(456)(789)$ . I want to show this. My attempt: We know that alternating group $A_n$ is generated by all $3$ cycles. In fact I can xeduce that all $3$ cycles of the form $(12j)$ where $j$ is an integer such that $2<j\le n$ generate $A_n$ . Now I want to show any $3$ -cycle in $A_9$ of the form $(1 2 j)$ where $j$ in $\{ 3,4,5,6,7,8,9\}$ can be expressed as a product of $(123)(456)(789)$ and $(14)(29)(37)(56)$ . I'm stuck on how to show this.","['permutations', 'groebner-generators', 'abstract-algebra', 'symmetric-groups', 'group-theory']"
4677908,Is the set of all linear orders on $\mathbb{N}$ linearly orderable?,"In studying the issue of linear orders and well ordering in the context of ZF Set Theory (without the Axiom of Choice), I have recently been thinking about the following question: Is the set of all linear orders of $\mathbb{N}$ linearly orderable? It feels as though there may be some way to construct a linear order on this set, perhaps by considering the number generated by the ordering (e.g $1 - 2 - 3 . . .$ would be smaller than $2 - 1 - 3 . . . $ ) but I am struggling to formalise this idea. Also, this feels like an idea that may rely upon the Axiom of Choice (although, again, I am unsure about this). My other idea, is that this is a result that depends upon well ordering (in which case, due to the absence of the Axiom of Choice, would make the answer no). I would be grateful for any clarity here.","['well-orders', 'natural-numbers', 'elementary-set-theory', 'axiom-of-choice', 'set-theory']"
4677963,Embedding of fiber products of schemes,"Let $X\to Z$ and $Y\to Z$ be scheme morphisms (everything noetherian, of finite type over an algebraically closed field $k$ of char $0$ ), and consider a morphism to another scheme $X\times_ZY\to T$ that is injective. Assuming that for all $x\in X$ , and $y\in Y$ , the restrictions $\{\mathrm{Spec}(k(x))\}\times_ZY\to T$ and $X\times_Z\mathrm{Spec}(k(y))\to T$ are embeddings, can we conclude that $X\times_Z Y\to T$ is an embedding? I have a hunch this is false, but maybe under good circumstances? EDIT: I can assume that $X\times_Z Y$ is the proj of (the symmetric product of) a coherent sheaf over $X$ .","['algebraic-geometry', 'schemes']"
4677993,How to compute fundamental quantities in differential geometry for surfaces given in implicit form?,"In my (undergraduate) readings in differential geometry, it seems that they assume that only parametrized surfaces are relevant. But what do we do when we get a surface in implicit form such as: $$f(x,y,z)=0 \tag{?}$$ In very simple cases such as $x+y+z=0$ we can solve for one of the variables, say $z$ and then we obtain a parametrization: $$\phi(x,y) = (x,y,-x-y)$$ But we can have a very complicated function where this is not possible. In this cases, how do we compute the plethora of fundamental quantities we see in differential geometry? Such as: First fundamental form. Second fundamental form. Normal curvature. Mean curvature. The ways to compute these quantities I've seen in books mostly depend on we having a parametrized surface. By having the first and second items, I guess I'd be able to compute the other items. Is there a way to do it or this is actually case is ignored because it's impossible in general?",['differential-geometry']
4678006,Understanding free product groups,"I am trying to understand free product groups and I considered the group $G=\langle a,b|a^2b^{-3}\rangle$ . As in this group $a^2=b^3$ , it seems like every element of G can be written as $b^{n_1}ab^{n_2}a\ldots$ , as every $a$ to a power greater than 2 ""disappears"" into a power of b. Hence i feel like $G$ is the free product $\mathbb{Z} \star C_2$ where $C_2$ is the cyclic group of order 2. It appears that my intuition is probably false as a similar reasoning would lead to say that it is also the free group $\mathbb{Z}\star C_3$ . Can someone explain me why my intuition is false ?","['combinatorial-group-theory', 'group-theory', 'abstract-algebra', 'free-product']"
4678102,"If $x^2+px-q=0$ and $x^2+px+q=0$ have integer roots for real $p$ and $q$, then $p^2=a^2+b^2$ for some integers $a$ and $b$","Two equations $x^2+px-q=0$ and $x^2+px+q=0$ have integer roots, where $p$ and $q$ are real numbers. Prove that for some integers $a$ and $b$ , $p^2 = a^2 + b^2$ . In other words, have $\{a,b,p\}$ be a Pythagorean triple. I used the quadratic equation and found that $p^2-4q$ and $p^2+4q$ are both perfect squares, and thus $2p^2 = m^2+k^2$ , but I'm stuck and not sure how to continue.","['algebra-precalculus', 'pythagorean-triples', 'quadratics']"
4678106,Proving $1 <\int_0^{2 \pi} \frac{\sin x}{x} d x < \frac{\pi}{2}$,"I would be grateful if you could help me prove the following inequality: $$
1 < \int_0^{2 \pi} \frac{\sin x}{x} d x < \frac{\pi}{2}
$$ Note that we can split the integral into two parts: $$
\begin{aligned}
\int_0^{2 \pi} \frac{\sin x}{x} d x & =\int_0^\pi \frac{\sin x}{x} d x+\int_\pi^{2 \pi} \frac{\sin x}{x} d x \\
& =\int_0^\pi \frac{\sin x}{x} d x+\int_0^\pi \frac{\sin (x+\pi)}{x+\pi} d x \\
& =\int_0^\pi\left(\frac{1}{x}-\frac{1}{x+\pi}\right) \sin x d x \\
& =\pi \int_0^\pi \frac{\sin x}{x(x+\pi)} d x 
\end{aligned}
$$ Unfortunately, I am currently stuck and I am not sure how to proceed with the proof. Any help or comments on this matter would be greatly appreciated.","['integration', 'calculus']"
4678111,Why is this true of matrices? Linearly dependent rows make linearly dependent columns,"Here’s a cool math thing about matrices that I don’t understand why it’s true: If one row of the matrix is a linear combination of other rows, then one of the columns will be a linear combination of other columns. For example: $$ \begin{bmatrix}
3 & 7 & 2 \\
1 & 2 & 3 \\
4 & 9 & 5
\end{bmatrix} $$ The third row is just the first row plus the second row. The third column is 17 times the first column -7 times the second column. (I had to use Wolfram Alpha to solve that.) This is just one example, but it’s always true. Should I just accept that this is the case, or can anyone provide insight into why this is the case?","['matrices', 'linear-algebra']"
4678178,"Solving $\sqrt{3} \sin x = \cos x$ for $0<x<2\pi$. There should be only two solutions, but I get four.","So I have problem for this question, where I get a two different solutions to the same question, and is any of them wrong? Because according to my math book, I should be getting only 2 solutions, where I'm getting 4. When $0<x<2\pi$ , solve $$\sqrt{3} \sin x = \cos x$$ Here, I've squared both sides so it becomes $$3\sin^2x = \cos^2x$$ and since $\cos^2x = 1-\sin^2x$ $$3\sin^2x = 1-\sin^2x$$ $$\sin^2x = \frac 1 4$$ which becomes $$\sin x = \pm \frac 12$$ and $$ x = \frac \pi 6, \frac {5\pi}{6}, \frac{7\pi}{6}, \frac{11\pi}{6}.$$ Is there anything wrong with this solution? and if what please indicate.","['algebra-precalculus', 'trigonometry']"
4678210,Minimizing Dirichlet Energy with Constraints,"I am interested in minimizing the Dirichlet energy $$E[f]=\iint_{D_1(\mathbf{0})} \|\nabla f\|^2 \mathrm{d}A ,$$ over the unit disk $D_1(\mathbf{0})=\{(x_1,x_2) \in \mathbf{R}^2:x_1^2+x_2^2 \leq 1\}$ given equality constraints of the form $\{f(\mathbf{x}_n)=y_n:1 \leq n \leq N \}$ with $\|\mathbf{x}_n\|=1$ and $y_n \in \mathbf{R}$ . I know that specifying $f$ on the boundary in full, in terms of a continuous function, determines the solution entirely as the harmonic function given by Poisson's formula $$f(r \cos \theta, r \sin \theta)=\frac{1}{2\pi} \int_{0}^{2\pi} \frac{1-r^2}{1-2r\cos(\theta-t)+r^2}f(\cos t,\sin t) \mathrm{d}t. $$ However, I'm not sure what happens when only a select few boundary values are provided. In summary, I want to solve the following optimization problem: $$\operatorname{argmin} _f\iint_{x_1^2+x_2^2\leq 1} \|\nabla f(x_1,x_2)\|^2 \mathrm{d}x_1 \mathrm{d}x_2\\ s.t. f(\cos(t_n),\sin(t_n))=y_n, \quad 1 \leq n \leq N. $$ My attempt: Obviously if there is only one constraint $f(\mathbf{x}_1)=y_1$ the solution is the constant function $f(x_1,x_2) \equiv y_1$ . I've also managed to express the Dirichlet energy in smaller disks as a quadratic form of the boundary values: $$E_R[f]=\iint_{D_R(\mathbf{0})} \|\nabla f\|^2 \mathrm{d}A = \frac{1}{4\pi^2} \int_0^{2\pi} \int_0^{2\pi} \frac{4 \pi R^2 \left( (1+R^4) \cos(t_1-t_2)-2R^2 \right)}{\left(1-2R^2 \cos(t_1-t_2)+R^4\right)^2} f(\cos t_1, \sin t_1) f(\cos t_2, \sin t_2)\mathrm{d}t_1 \mathrm{d}t_2. $$ Here $0 \leq R < 1$ , and unfortunately a straightforward limit $R \to 1$ appears to make the integral divergent. Letting $$K(t_1-t_2):=\frac{1}{4 \pi^2} \frac{4 \pi R^2 \left( (1+R^4) \cos(t_1-t_2)-2R^2 \right)}{\left(1-2R^2 \cos(t_1-t_2)+R^4\right)^2},$$ I computed the variational derivative with respect to $f$ and found the condition $$\int_0^{2\pi} K(\tau-t) f(\cos t, \sin t) \equiv 0, \quad \text{ for all } \tau.$$ I am stuck here as I don't know how to use the constraints in this variational approach. Any help on solving the problem with more than 1 constraint would be greatly appreciated. I would love to have the solution of the most general problem, but if that's too hard, special cases such as boundary values given on the vertices of a regular polygon would also be informative. Thanks.","['calculus-of-variations', 'boundary-value-problem', 'harmonic-functions', 'real-analysis']"
4678243,Find the minimum of $x^2+y^2+z^2$ under several constraints,"Let $x,y,z \in (-1;1)$ such that $$13\left( {xy + yz + zx + 1} \right) + 14\left( {x + y + z + xyz} \right) = 0.$$ Find the minimum value of the following expression $$P=x^2+y^2+z^2.$$ I guess the minimum value is $\frac{3}{4}$ and I will try to prove that prediction. From the conditions of the variables, I thought of trigonometry. However, it cannot be used due to the conditions $$13\left( {xy + yz + zx + 1} \right) + 14\left( {x + y + z + xyz} \right) = 0.$$ I also thought of the $pqr$ method, but the variables are real numbers (which can be negative), so I couldn't implement the idea. Please provide some suggestions. Thank you.","['maxima-minima', 'algebra-precalculus', 'inequality']"
4678259,Computing an integral on the unit sphere,"I am having trouble, trying to compute the integral \begin{align*}
I(a)= \int_{\mathbb{S}^{d-1}} (1-\cos(a w_1)) \, d \sigma_{d-1}(w)\qquad a>1 
\end{align*} where $w= (w_1,\cdots, w_d)$ and $\mathbb{S}^{d-1}=\{w\in \mathbb{R}^{d}\,:\, w_1^2+\cdots+w_d^2=1\}$ . 1- Can the explicit value of $I(a)$ be computed? 2- How can prove that there is $c>0$ such that $I(a)\geq c$ for all $a>1$ ? PS: The second question is a conjecture.","['integration', 'analysis', 'real-analysis']"
4678283,Deriving the volume of an elliptic torus,"Following this question , I was looking forward to derive the volume of the elliptic torus using: $$V = \int_\Omega rdrd\theta dz$$ in the cylindrical coordinates. where the origin of the system will lie at the tangent to the inner circles as shown in the figure, then, and if rotational symmetry, the volume is: $$
V = \int\limits_{0}^{2\pi}d\theta \int\limits_{R_0-b}^{R_0+b}\int\limits_{0}^{2a} rdrdz
$$ . I need a help in choosing the right boundaries and if there is need to use the implicit equation to pass through the derivation. This looks weird for me, and I guess the answer should look like: $$V=2\pi R_0 \times \pi ab$$","['integration', 'analytic-geometry', 'geometry', 'volume']"
4678349,"Linear span of $\{e^{-n x}\}_{n\in\mathbb N}$ is dense in $L^2((0,\infty))$. Explicit expansion?","I proved that the linear span of $\{e^{-n x}\}_{n\in\mathbb N}$ is dense in $L^2((0,\infty))$ (see below). Question : Given $f\in L^2((0,\infty))$ , can I find ""expansion coefficients"" $f_n$ such that $$f(x) = \sum_{n\in\mathbb N} f_ne^{-n x} ?$$ Proof of density : Using the Stone-Weierstrass theorem for locally compact Hausdorff space, the linear span of $\{e^{-n x}\}_{n\in\mathbb N}$ is dense in $C_0((0,\infty))$ , the space of continuous functions vanishing at infinity. Since $C_0((0,\infty))$ is dense in $L^2((0,\infty))$ , we prove the result. Note : After very nice comments, the question is simplified.","['dense-subspaces', 'analysis', 'real-analysis']"
4678354,Expected value exponential inequality non-negative random variable,"Let $X$ be a non-negative random variable and $p \geq e$ , $q > 0$ be two constant values such that $$
P [X \geq x] \leq p e^{-x^2/q^2} \quad \forall x \geq 0.
$$ Prove that $$
\mathbb{E}[X] \leq q(1+\sqrt{\log p}).
$$ There's my first attempt. Using the identity $$
\mathbb{E}[X] = \int_0^\infty (1-F(x)) d x - \int_{-\infty}^0 F(x) d x
$$ and the fact that $X \geq 0$ I obtain: $$
\mathbb{E}[X] = \int_0^\infty (1-F(x)) d x
$$ and using the hypothesis I get $$
\mathbb{E}[X] \leq \int_0^{+\infty} p e^{-x^2/q^2} d x = pq \int_0^{+\infty} e^{-z^2} dz = \frac{pq \sqrt{\pi}}{2}
$$ where the last equality is given by the gaussian integral . But then I'm stuck. The second attempt I tried was to partition the events set in some smart way, that is using $( X \geq q )$ or $(X \geq \sqrt{\log p} )$ but I couldn't go anywhere.","['expected-value', 'probability-theory', 'probability']"
4678367,"Division of binary numbers, confusing","I am trying my best to divide the following: Perform the following computations in binary arithmetic (Show how you perform
the computations): My attempt: I watched: https://www.youtube.com/watch?v=PF_sLkAnI1U https://www.youtube.com/watch?v=VKemv9u40gc Question: What is a more easy way, or approach to perform this, without really getting lost and confused? EDIT: Due to confusion of the question above, I am simplifying this: N.B. It's not about straightly calculating it, but, documenting the exact steps how you arrive to that solution. It states in the task, Perform the following computations in binary arithmetic (Show how you perform
the computations): According to the solution paper, this is what I see.","['binary-operations', 'arithmetic', 'binary', 'discrete-mathematics']"
4678387,When is the following integral zero?,"We assume we have two functions $f,g:\mathbb{R} \rightarrow \mathbb{R}$ , $d$ some integer. I have the following integral expression $$\forall b_{1},b_{2}  \leq d : \int_{-\delta}^{\delta}[ \int_0^t f(s)^{b_{1}}g(s)^{b_{2}} ds \cdot ( \int_0^t \sum_{i_{1},i_{2}=1}^{d}a_{i_{1},i_{2}}{f(s')}^{i_{1}}{g(s')}^{i_{2}}ds')]dt=0$$ My question is the following, can I conclude that the only way this integral can become zero is by $$ \sum_{i_{1},i_{2}=1}^{d}a_{i_{1},i_{2}}{f(s')}^{i_{1}}{g(s')}^{i_{2}}=0$$ being zero already? My attempts: What I already know is, that we can choose $g(s)^2f(s)^2$ which is a positive function. Using the mean-value theorem (these $f,g$ have nothing to do with the ones above) $$\int_{a}^{b} f(x) g(x) d x=f(c) \int_{a}^{b} g(x) d x$$ we can conclude that the integral only can become zero if $$\int_{-\delta}^{\delta} \int_0^t \sum_{i_{1},i_{2}=1}^{d}a_{i_{1},i_{2}}{f(s')}^{i_{1}}{g(s')}^{i_{2}}ds'dt=0$$ is already zero, But here I am stuck.","['integration', 'real-analysis', 'complex-analysis', 'functions', 'sequences-and-series']"
4678413,Prove that $\mathcal{P}(\mathbb{Z}^+)$ is uncountable,"Prove: $\mathcal{P}(\mathbb{Z}^+)$ is uncountable Here is an excerpt from a proof of this result that I am struggling to follow: As a part of the proof that $\mathcal{P}(\mathbb{Z}^+)$ is uncountable, my textbook defines the set $\bar{{Z}}$ as can be seen in the picture above. What I am confused about, is why we can presume that $\bar{{Z}}$ is not equal to the empty set?","['elementary-set-theory', 'proof-explanation', 'cardinals']"
4678422,How to solve an improper triple integral,"Let $D=[(x,y,z)\in R^3: x^2+y^2+z^2\le 1, z\ge 0, z^2-x^2-y^2\le0]$ and let $f(x,y,z)=\frac {z} {\sqrt{x^2+y^2}}$ . The exercise is about calculating the integral: $\iiint_D f(x,y,z) dxdydz$ . The text tells to be careful because it is an improper integral. Now, I know how to use integration ""by wires"", that is used when we can write the domain $D$ as $D=[(x,y,z)\in R^3: g_1(x,y)\le z\le g_2(x,y); (x,y)\in E]$ , and ""by layers"", that is used when we can write the domain $D$ as $D=[(x,y,z)\in R^3:h_1\le z \le h_2; (x,y)\in E(z)]$ . I also know the Jacobians of spherical coordinates [ $J=r^2cos(\phi)$ ] and of cylindrical coordinates [ $J=r$ ] and how to use the change of variables in a triple integral. I know, at last, that $z^2-x^2-y^2\le 0$ is an infinite cone with vertex in the origin. Still, I don't know where to start the exercise.",['multivariable-calculus']
4678427,Bijective extension of Lipschitz map defined over a dense set,"Let $Y$ be a complete metric space (I'm interested in particular in the case $Y=c_0$ , if relevant) and $X\subset Y$ dense. Given a 1-Lipschitz bijection $f$ from $X$ onto $X$ , does it extend to a 1-Lipschitz map $f':Y\rightarrow Y$ which is injective and/or surjective? Lipschitz maps preserves Cauchy sequences, so the extension exists, but is it possible to recover some injectivity/surjectivity of the map? I believe the answer might be negative, because for general metric spaces it is negative, but in this case the initial map is bijective from the dense to itself and I'm not sure if this extra information can help somehow.","['metric-spaces', 'lipschitz-functions', 'dense-subspaces', 'functions', 'functional-analysis']"
4678440,Function with same symmetry as planar hexagonal lattice,"I'm currently trying to define a function which has the same symmetry as a planar hexagonal lattice. For a quadratic lattice the solution $f(x,y) = \cos(x) + \cos(y)$ is quite simple but I wasn't able to modify it to really fulfill the desired properties. My current ""best"" solution is $f(x,y) = \cos(x-\frac{1}{\sqrt{3}}y) + \cos(x+\frac{1}{\sqrt{3}}y)$ . As can be seen in the attached image, this results in function maxima at every lattice point but the function does not have the desired rotation and reflection properties. Does someone know the solution to this problem or if this is even possible? Right now I have the feeling that the approach of combining two cosine functions does not work as the directions necessary are not orthogonal and the function maxima therefore not circular. PS: The example in the image uses a Radius of $2\pi$ My best solution so far","['reflection', 'symmetry', 'geometry', 'rotations']"
4678513,"Alternative book for ""infinite limits and limits at infinity"" for a real function ? (Rudin PMA 4.33)","Do you know any book that covers the ""infinite limits and limits at infinity"" notion for a real-valued function ? I ask because I find the theorem $4.33$ from Rudin Principles of Mathematical Analysis on that topic quite unprecise. Here are three points that make think the theorem as presented by Rudin is unprecise: He doesn't mention the case where $x$ is a limit point of $E$ , not belonging to $E$ . He wants $V\cap E$ to be nonempty. For me, it would be $V\cap E \backslash \{x\}$ instead. He doesn't say which set $x$ belongs to. For all these reasons I am looking for an alternative text, which covers the topic. PS: Here is the theorem from Rudin: Let $f$ be a real function defined on $E \subset R$ . We say that $f(t) \rightarrow A$ as $t \rightarrow x$ where $A$ and $x$ are in the extended real number system, if for every neighborhood $U$ of $A$ there is a neighborhood $V$ of $x$ such that $V \cap E$ is not empty, and such that $f(t) \in U$ for all $t \in V \cap E$ , $t \neq x$ .","['limits', 'general-topology', 'functions', 'reference-request']"
4678586,Name for logical algebra in which $A \lor B = A + B - AB$?,"An answer from 5 years ago provides simple rules for converting logical propositions to ordinary elementary algebraic expressions: $$\lnot{A} = 1 - A$$ $$A \land B = AB$$ $$A \lor B = A + B - AB$$ Where $A$ and $B$ can only take on values 1 for ""TRUE"" and 0 for ""FALSE"", so $A^2 = A$ , $A^2B = AB$ . Is there a commonly used name for this set of rules?  It is not Boolean Algebra because in Boolean Algebra the rules of elementary algebra do not apply. $A \lor B \equiv A + B; 1 + 1 = 1$ .  It is not exactly GF(2) (i.e., integer arithmetic modulo two) because I am using the ""minus"" sign. I ask because I am writing a basic introduction to probability from a Jaynesian/Keynesian point of view and want to go straight from logic to probability without going through set theory a la Kolmogorov.  I gather that the above rules are widely used in programming languages where GF(2) is not implemented. See the Wikipedia article . If there is no accepted name for this, I will just call it ""converting logical propositions to ordinary algebra limited to 0/1 variables"".","['boolean', 'propositional-calculus', 'probability-theory']"
4678700,Does there exist an infinite set admitting precisely four linear orders?,"I am studying a course on ZF Set Theory where I encountered the following question: Does there exist a set admitting precisely four linear orders? Since any set with $n$ elements exhibits $n!$ linear orders, it feels obvious to conclude that there does not exist such a set since $n! \neq 4$ for all $n \in \mathbb{N}$ (certainly not a finite set). However, I was wondering if there is a proof (or counterexample) for the claim that is more rigorous than this observation. The question seemed to be posed as more than just a trivial consequence of this fact and so I am assuming that I have oversimplified the problem and that if I broaden my consideration to infinite sets, then it becomes potentially a lot more tricky to conclude that the statement is incorrect in such a simple manner. I would be grateful for any clarity here. Further Thoughts Having now entertained the possibility of constructing infinite sets that exhibit precisely four linear orders, I have tentatively come up with an example that I think might work ( edit: this does not work). Perhaps if we define the set $$X := \mathbb{Z} \times \{ 1,2,3,4 \}$$ Then we can define the following four linear orders: $$ \text{Order 1: $(a, i) <_1 (b, j)$ if and only if $a < b$ or ($a = b$ and $i < j$)}$$ $$ \text{Order 2: $(a, i) <_2 (b, j)$ if and only if $a > b$ or ($a = b$ and $i < j$)}$$ $$ \text{Order 3: $(a, i) <_3 (b, j)$ if and only if $i < j$ or ($i = j$ and $a < b$)}$$ $$ \text{Order 4: $(a, i) <_4 (b, j)$ if and only if $i < j$ or ($i = j$ and $a > b$)}$$ However, I am hesitant to accept this as an answer without first showing that these are indeed the only four possible linear orders here (since the question requires that there are precisely four). It's certainly the case that four is a lower bound here, but how do I show that there are no others (if indeed this is the case)? Edit The above example is not correct. Please refer to the answer I have produced below for the reason why that is the case. Thanks again to @spaceisdarkgreen and @EdwardH for their help in the comments.","['well-orders', 'examples-counterexamples', 'solution-verification', 'combinatorics', 'upper-lower-bounds']"
4678704,A curious limit for $-\frac{1}{2}$,How to prove this ? $$-\frac{1}{2} = \lim_{x\to+\infty}\sum_{n=1}^{\infty}(-1)^n \frac{x^{2n-1}}{(2n)! \sqrt{\ln 2n}}$$ It reminded me of the fact that $$-\frac\pi2 = \lim_{x\to+\infty}\sum_{n=1}^{\infty}(-1)^n \frac{x^{2n-1}}{(2n)! \ln 2n}$$ that has been proven here : A curious limit for $-\frac{\pi}{2}$,"['limits', 'calculus', 'alternating-expression']"
4678735,A right angle relates to orthocenter and circumcenter,"I'm struggling to solve this. It would be very nice if you use no knowledge on circle to solve this (this problem was introduced before the context of circle). Let $ABC$ be an acute triangle, with orthocenter $H$ , circumcenter $O$ . The line parallel to BC passing through $O$ intersects AC at K. Let $I$ be the midpoint of $AH$ . Prove that $\widehat{BIK}=90^o$ . My attempts: I drew three altitudes, and called them $AD$ , $BE$ , $CF$ respectively. Let $J$ be the intersection of $EF$ and $AH$ . Then I can see that $CJ$ is perpendicular to $BI$ and I'm trying to prove that $CJ$ and $IK$ are parallel. Please help me. Thanks.","['contest-math', 'euclidean-geometry', 'geometry', 'triangles', 'problem-solving']"
4678800,"Find a function $f$ with some constraints that maximize $E[(f(X)-f(X+Y))X]$, where $X$ and $Y$ are independent and normal","I am faced with the following problem: Problem. $X$ and $Y$ are independent normal random variables both with zero mean, $\operatorname{Var}(X)=1,\operatorname{Var}(Y)<1$ (seems not crucial).
Find $f$ maximizing $$E[(f(X)-f(X+Y))X], $$ where $f$ is nondecreasing, odd and smooth, satisfying $|f|\leq 1$ . The restriction ""smooth"" is added by me to make the problem seem simpler. It can also be replaced by other restrictions. If possible, can we find the target $f$ when it is only subject to nondecreasing, odd and bounded? I would be grateful if there is any hint because I am totally not familliar with finding function to optimize some target.","['statistics', 'probability-limit-theorems', 'probability-distributions', 'functional-analysis', 'probability-theory']"
4678817,What are pairwise independent random variables that have the maximum degree of dependence possible,"I am interested to know how to construct a set of pairwise independent random variables that has the maximum dependence possible, and why this specific way of construction is the best we can do.","['probability-theory', 'probability']"
4678827,Conjecture: Any two matrices of size $n×n$ with same characteristic and minimal polynomial are similar implies $n\le 3$.,"Notations: $\mathcal{M}_n(\Bbb{R}) $ : the set of all $n×n$ matrices over $\Bbb{R}$ $\chi_A(x)$ : Characteristic polynomial of $A$ $m_A(x)$ : Minimal polynomial of $A$ $A\sim B$ : $\exists P\in Gl_n(\Bbb{R})$ such that $B=P^{-1}AP$ Conjecture : $(\forall A, B\in \mathcal{M}_n(\Bbb{R})$ with $\chi_A=\chi_B$ and $m_A=m_B$ implies $A\sim B) $ implies $n\le 3$ Attempt: Let's work with converse. For $n\ge 4,\exists A, B\in\mathcal{M}_n(\Bbb{R}) $ with $\chi_A=\chi_B$ and $m_A=m_B$ such that $A\not\sim B\tag{1}$ Special cases : $\boxed{n=4}:$ $A=\left(\begin{array}{cccc}
0 & 1 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 0 & 0
\end{array}\right)$ $B=
\left(\begin{array}{cccc}
0 & 1 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{array}\right)$ $$\chi_A(x) =x^4=\chi_B(x) $$ and $$m_A(x) =x^2=m_B(x) $$ $\begin{align}\textrm{rank}(A)=2\neq 1=\textrm{rank}(B) &\implies A\not\sim B\\\end{align}$ $\boxed{n=5}$ : $A=\left(\begin{array}{ccccc}
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1\\
0 & 0 & 0 & 0 & 0
\end{array}\right)$ $B=
\left(\begin{array}{ccccc}
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0
\end{array}\right)$ $$\chi_A(x) =x^5=\chi_B(x) $$ and $$m_A(x) =x^2=m_B(x) $$ $\begin{align}\textrm{rank}(A)=2\neq 1=\textrm{rank}(B) &\implies A\not\sim B\\\end{align}$ $\boxed{\text{General cases}(n\ge 4)}$ : Strategy: Find two nilpotent matrices $A,B \in \mathcal{M}_n(\Bbb{R})$ with degree of nilpotency $2$ and $\textrm{rank}(A) \neq \textrm{rank}(B)$ $A=(a_{ij}) $ and $B=(b_{ij}) $ where $a_{ij}=\begin{cases} 1 &(i,j)\in\{(2,1),(n-1,n)\}\\0& \text{otherwise}\end{cases}$ $b_{ij}=\begin{cases} 1 &(i,j) =(2,1)\\0& \text{otherwise}\end{cases}$ Then $$\chi_A(x) =x^n=\chi_B(x) $$ and $$m_A(x) =x^2=m_B(x) $$ $\begin{align}\textrm{rank}(A)=2\neq 1=\textrm{rank}(B) &\implies A\not\sim B\\\end{align}$ For $n\le 3$ and $\forall A,B\in\mathcal{M}_n(\Bbb{R})$ with $\chi_A=\chi_B $ and $m_A =m_B $ implies $A\sim B $ ( here ) $\tag{2}$ Conclusion: $(1) $ and $(2) $ together implies that the conjecture is true. Question: Is my attempt correct? What about the direct proof $[$ starting with any two matrices having the said properties and then showing $\textrm{deg}(\chi_A) =\textrm{deg}(\chi_B)\le 3]$ ? Note: Two matrices are similar iff both have the same Jordan normal form upto the permutation of Jordan blocks.","['jordan-normal-form', 'abstract-algebra', 'linear-algebra', 'characteristic-polynomial', 'similar-matrices']"
4678846,How to calculate the volume of this particular solid using triple integrals,"Evaluate the volume of the solid $$A=\{(x,y,z)\in \mathbb{R}^3: x^2+y^2-2y\le0; 0\le z\le 10-3\sqrt {x^2+y^2}\}.$$ Knowing that integration ""by wires"" can be used when the domain of integration $\Omega$ can be written as $$\Omega=\{(x,y,z)\in  \mathbb{R}^3: g_1(x,y)\le z\le g_2(x,y); (x,y)\in D\}$$ we can setup the integral $$\begin{align}\iiint_{A}dxdydz&=\iint_{x^2+y^2-2y\le 0}\Big(\int_{0}^{10-3\sqrt{x^2+y^2}}dz\Big)dxdy
\\&=\iint_{x^2+y^2-2y\le 0}\Big(10-3\sqrt {x^2+y^2}\Big)dxdy.\end{align}$$ Now, the domain $D=\{(x,y)\in  \mathbb{R}^2: x^2+y^2-2y\le 0\}$ is a circle with centre in $(0,1)$ and radius $1$ . I used the following polar coordinates: $x=r\cos(\theta)$ , $y=1+r\sin(\theta)$ because of the translation of the circle and the integral became: $$\int_{0}^{2\pi}\int_{0}^{1}(10-3\sqrt{r^2\cos^2(\theta)+1+2r\sin(\theta)+r^2\sin^2(\theta)})rdrd\theta\\
=\int_{0}^{2\pi}\int_{0}^{1}\Big(10-3\sqrt{r^2+1+2r\sin(\theta)}\Big)rdrd\theta.$$ As the final calculations are not so straightforward I'm perplexed about my method and, in the first place, I'm asking if this is correct, in the second place if there exists another method which is simpler.",['multivariable-calculus']
4678868,countable generated sigma-algebra is separable?,"Let $X$ be a metric space and $\mathcal{B}(X)$ the Borel- $\sigma$ -algebra. Assume that $\mathcal{B}(X)$ is countably generated, i.e. there exists $\mathcal{C} \subset \mathcal{B}(X)$ countable such that $\sigma(\mathcal{C})=\mathcal{B}(X)$ . Does this imply that $X$ is second countable and therfore separable?",['measure-theory']
4678881,How to calculate $\int_{0}^{\infty}\frac{x^{2}\ln x}{(1+x^{3})^4}dx$,"I have to calculate this definite integral: $$ \int_{0}^{\infty}\frac{x^{2}\ln x}{(1+x^{3})^4}dx$$ I calculatated as a indefinite integral and the answer is $$-\dfrac{\ln\left(x^3+1\right)}{27}-\dfrac{\ln\left(x\right)}{9\left(x^3+1\right)^3}+\dfrac{\ln\left(x\right)}{9}+\dfrac{1}{27\left(x^3+1\right)}+\dfrac{1}{54\left(x^3+1\right)^2}$$ But what I have to do if I want to calculate the definite integral?
How can I apply here the fundamental theorem of calculus","['calculus', 'definite-integrals', 'upper-lower-bounds']"
4678929,"Find $P(X+Y>s+t \mid X>s)$ where $X$,$Y$ are independent exponential distribution","Let $X$ , $Y$ be independent exponential distribution with parameter $\lambda$ I want to find $P(X+Y>s+t \mid X>s)$ where $X$ , $Y$ are independent exponential distribution. My attempt: \begin{aligned}
P(X+Y>s+t, X>s) &= \int \int \mathbb{1}_{x>s} \mathbb{1}_{x+y>s+t} f_{X}(x)f_{Y}(y) \,dx\,dy \\&= \int_{s}^{\infty} \int_{s+t-x}^{\infty} f_{X}(x)f_{Y}(y) \,dx\,dy
\end{aligned} But I stuck since \begin{equation}
\int_{s+t-x}^{\infty} f_{Y}(y) \,dy = \int_{s+t-x}^{\infty} \lambda e^{-\lambda y} \,dy = e^{-\lambda (s+t-x)}
\end{equation} gives \begin{equation}
\int_{s}^{\infty} \lambda e^{-\lambda x} e^{-\lambda (s+t-x)} \,dx = \int_{s}^{\infty} \lambda e^{-\lambda (s+t)} \,dx = \lambda e^{-\lambda (s+t)}x |_{s}^{\infty} = \infty
\end{equation} What did I miss?","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
4679010,Compute the largest natural number $n$ such that $2^n$ divides $S=\binom{2}{1}+\binom{4}{2}+\binom{8}{4}+\cdots+\binom{2^{100}}{2^{99}}$,"Honestly, I'm pretty stuck on this. I've made some basic observations, but I'm not sure they'll help. For any positive integer $k$ , $$\binom{2^k}{2^{k-1}}=\frac{(2^k)!}{((2^{k-1})!)^2}$$ I was thinking that the factorial being squared in the denominator might allow for some nice symmetry, but I can't seem to see any. This binomial coefficient also counts the number of ways to choose $2^{k-1}$ objects from a set of $2^k$ objects, but I couldn't find a combinatorial representation for the sum as a whole. Using Legendre's formula, $v_2((2^k)!)=2^k-1$ and $v_2((2^{k-1})!)=2^{k-1}-1$ . So, $v_2 \binom{2^k}{2^{k-1}}=(2^k-1)-2(2^{k-1}-1)=1$ .","['elementary-number-theory', 'combinatorics']"
4679026,Approximating a random variable by random variables with continuous densities,"Let $X$ be a nonnegative random variable, and suppose there is an $s > 0$ for which $\mathbb E[X^s] < \infty$ . I want to construct a family $(Y_\sigma)_{\sigma > 0}$ of nonnegative random variables with the following properties: $X$ and $Y_\sigma$ are independent for all $\sigma > 0$ ; $XY_\sigma$ has a continuous density for all $\sigma > 0$ ; $Y_\sigma$ converges in distribution to $1$ as $\sigma \searrow 0$ . My reference text (Achim Klenke's Probability Theory: A Comprehensive Course , 3rd ed., Exercise 15.1.5ii) recommends taking $Y_\sigma$ to be independent of $X$ and uniformly distributed on $[1-\sigma, \sigma]$ . This doesn't satisfy 2, though: if $X = 1$ almost surely, or if $X$ is supported on a Cantor set, then $XY_\sigma$ doesn't have a continuous density. Is a construction like this possible? EDIT: I tried doing this where $Y_\sigma$ has a continuous density as well, e.g., $Y_\sigma$ has a ""tent density"": $$
f_\sigma(y) = \begin{cases}
\sigma^{-2}(y+\sigma-1) & \textrm{if } 1-\sigma \leq y \leq 1\\
-\sigma^{-2}(y-\sigma-1) & \textrm{if } 1 < y \leq 1+\sigma \\
0 & \textrm{otherwise}
\end{cases}
$$ But in that case, we get: $$
\mathbb P\left[ XY_\sigma \leq x \right] = \int \mathbb P\left[XY_\sigma \leq x \: | \: Y_\sigma = y\right] \, \mathbb P_{Y_\sigma}[dy] = \int \mathbb P\left[ X \leq \frac x y \right] f_{\sigma}(y) \, dy
$$ and without knowing anything about the distribution of $X$ , I'm not sure how we can conclude that $x \mapsto \mathbb P\left[ XY_\sigma \leq x \right]$ is differentiable. Alternatively, we could write $$
\mathbb P\left[XY_\sigma \leq y\right] = \int_0^\infty \mathbb P\left[Y_\sigma \leq \frac{y}{x}\right] \, \mathbb P_X[dx] 
$$ but even if $y \mapsto \mathbb P[Y_\sigma \leq y/x]$ is differentiable, that derivative may not be integrable with respect to $x$ and the measure $\mathbb P_X$ , so differentiation under the integral may not be allowed.","['independence', 'probability-distributions', 'probability-theory', 'random-variables']"
4679048,"Prove $(a^3+b^3+c^3)^2 \ge 3(a^4 + b^4 + c^4)$ for $abc = 1$ ($a, b, c > 0$)","Let $a,b,c$ be positive real numbers such that $abc=1$ . Does it then hold that $$(a^3+b^3+c^3)^2 \ge 3(a^4 + b^4 + c^4)$$ A lazy and non-rigorous computation suggests that, after substituting $c = \frac {1}{ab}$ , the resulting function in $a,b$ is convex, and hence the local minimum at $a = b = 1$ is the global minimum and the inequality is true. This problem came out of an amusing game I have been playing lately, in which I ask ChatGPT to make up Putnam problems which I then attempt (they are often hilariously false). The above question was one such problem which my basic knowledge of olympiad inequalities was unable to make much headway in either direction on (my ideas to use mean inequalities or symmetric polynomial inequalities failed, though I did not try Lagrange multipliers or similar methods). If the AI stole it from somewhere I would like to know.","['algebra-precalculus', 'symmetric-polynomials', 'inequality']"
4679050,Where am I going wrong with using the chain rule to compute the derivative of this composite function?,"Let $g$ be a function of $y_1$ and $y_2$ ; $g = f(y_1, y_2) = y_1 \times y_2$ . And further let $y_1 = x^2$ and $y_2 = 3\sin(x)$ Therefore $g = 3x^2 \sin(x)$ . The derivative with respect to $x$ then becomes $\frac{dg}{dx} = 6x\sin(x) + 3x^2 \cos(x)$ . Now I would like to compute the same derivative using the chain rule. So $\frac{dg}{dx} = \frac{1}{2}\frac{dg}{dx} + \frac{1}{2}\frac{dg}{dx} = \frac{1}{2}\frac{dg}{dx}\frac{dy_1}{dy_1} + \frac{1}{2}\frac{dg}{dx}\frac{dy_2}{dy_2} = 
\frac{1}{2}\frac{dg}{dy_1}\frac{dy_1}{dx} + \frac{1}{2}\frac{dg}{dy_2}\frac{dy_2}{dx}
= \frac{1}{2}y_2 \times 2x + \frac{1}{2} y_1 \times 3\cos(x) = 3x\sin(x) + \frac{3}{2}x^2\cos(x)
$ Which is off by the factor $\frac{1}{2}$ . The algebra breaks down at some point here. Can anyone see where am going wrong?","['derivatives', 'chain-rule']"
4679079,"Proving Surjectivity when $f: \mathbb{N} \times \mathbb{N}, f(a,b) = 2a-b$","Suppose we have a function defined as $f: \mathbb{N} \times \mathbb{N}, f(a,b) = 2a-b$ This was my solution:
Suppose $z\in\mathbb{Z}$ . Then for some $x,y\in\mathbb{N} \times \mathbb{N}, f(x,y) = z.$ $2x-y = z$ $\Rightarrow -y=z-2x$ $\Rightarrow y = -z + 2x$ $2x = z + y$ $\Rightarrow{x = \frac{z+y}{2}}$ This was supposed to show values for $x$ and $y$ exist in the domain for all outputs. But I was told the following: Choice of $(a,b)$ may not be in $\mathbb{N} \times \mathbb{N}$ Correct set up but did not choose $a$ , $b$ appropriately, based on each case for $y(y>0, y<0, y=0)$ Could someone help me understand what this means?",['functions']
4679098,Is the orthogonal group of $\mathbb R^\infty$ contractible?,"Let $\mathbb R^\infty$ be the direct sum of countably many copies of $\mathbb R$ topologized as the colimit over its finite dimensional subspaces. This topology should make every linear self-map of $\mathbb R^\infty$ continuous. Let $O(\mathbb R^\infty)$ be the group of linear isomorphisms $A\colon \mathbb R^\infty\to \mathbb R^\infty$ that preserve the inner product $\langle v,w\rangle=\sum_i v_iw_i$ . Question: Is $O(\mathbb R^\infty)$ contractible with the compact-open topology? I believe that the answer should be yes, but I'm short of a proof. I'm aware of the following facts: The unit sphere $S(\mathbb R^\infty)$ is contractible (e.g. Hatcher's book, Example 1B.3 on page 88). The space of linear isometric embeddings $\mathbb R^\infty \to \mathbb R^\infty$ is contractible (Lewis-May-Steinberger, Equivariant Stable Homotopy Theory , Lemma 1.5 on page 60). Bott's orthogonal group $O(\infty)=\mathrm{colim}_n O(n)$ is not contractible (Bott perdiodicity). If $\mathcal H$ is the Hilbert space completion of $\mathbb R^\infty$ , then the orthogonal group $O(\mathcal H)$ is contractible with the norm topology (Kuiper's theorem). I'm pretty sure that there are continuous embeddings $O(\infty)\hookrightarrow O(\mathbb R^\infty) \hookrightarrow O(\mathcal H)$ non of which is surjective. I'm struggling to adapt either Hatcher's argument or the one used by Lewis-May-Steinberger to $O(\mathbb R^\infty)$ . Any ideas?","['general-topology', 'algebraic-topology']"
4679111,Symmetric formulation for the heat equation,"Consider the heat equation: $$\partial_t u-div(A\nabla u)=f$$ with $u(0)=0, u=0$ on the boundary of the domain of definition, call it $U$ . Consider a test function $v=v(x,t)$ , and perform the following operations: integrate the strong formulation from $0$ to $t$ : $$u(t)-\int_0^t div(A\nabla u)(\tau)d\tau=...$$ integrate in space and take a convolution product with $v$ : $$\int_0^T\int_U u(t,x)dx v(T-t,x)dt-\int_0^T\int_U\int_0^t div(A\nabla u)(\tau,x)d\tau v(T-t,x)dx  dt=...$$ apply, in space, the divergence theorem: $$\int_U (u(\cdot,x)*v(\cdot,x))(T)dx +\int_U\left(\left (\int_0^\cdot  (A\nabla u)(\tau,x)d\tau\right ) * \nabla v(\cdot,x) \right)(T)dx = ...$$ Now, if $A$ is constant in time, we get a symmetric formulation. In fact, doing some simple change of variables yields: $$\left(\left (\int_0^\cdot  \nabla u(\tau,x)d\tau\right ) * \nabla v(\cdot,x) \right)(T) = \left(\left (\int_0^\cdot  \nabla v(\tau,x)d\tau\right ) * \nabla u(\cdot,x) \right)(T)$$ This also shows that as soon as $A$ is not constant in time, symmetry cannot be expected. Yet, the authors of this very short paper seem to claim this is the case, in the first page. Can anyone confirm that the this convolution-based formulation in non-symmetric in the general case, even if $u,v,A$ are taken as continuous piecewise linear functions in time? It seems strange that a peer reviewed paper contains such an error, so that probably I am making a mistake somewhere. Notes I have decided to use a linear heat equation, which one would obtain after linearization of the formulation in the linked paper my point I think could be made with ODEs directly, the space variable does not play a role here. I decided to keep it to mantain a certain similarity with the above paper Context Solving the heat equation with a space-time method, naturally yields non-symmetric linear systems to be solved, because of the presence of the time derivative. However, applying the convolution operator, one could hope to make those systems symmetric (albeit dense). This might yield increased computational efficiency.","['calculus', 'finite-element-method', 'numerical-methods', 'partial-differential-equations']"
4679118,Does a completely metrizable space admit a compatible metric where all intersections of nested closed balls are non-empty?,"It is well-known that given a metric space $(X,d)$ , the metric is complete if and only if every intersection of nested (i.e. decreasing with respect to inclusion) closed balls of vanishing diameters have non-empty intersection. This is usually referred to as Cantor's Intersection Theorem. My question basically is: Is it true that every completely metrizable topological space admits one compatible metric such that every intersection of nested closed balls has a non-empty intersection? I will give some more details: Let's call spherically complete a metric as above where every intersection of nested closed balls has non-empty intersection. I'm very aware that not every complete metric is spherically complete. Only on this forum, there are several discussions on similar topics (see e.g. this question and the list of its related questions). However, my question is not about a fixed metric, but about the existence of one single compatible metric for a topological space.
And a priori, it seems possible to me that even a metric space whose metric is not spherically complete may still admit another compatible spherically complete metric. We know that there exist spaces that have spherically complete metrics. For example: All (complete) metrics on a compact space are spherically complete. The standard euclidean metric on $\mathbb{R}$ is spherically complete. All completely ultrametrizable spaces admit a compatible spherically complete metric. I suspect that every locally compact completely metrizable space admits a spherically complete metric, and probably the same is true for Polish spaces (using that they are homeomorphic to $G_\delta$ subsets of the Hilbert cube).
But I could not find any reference for this. So to be more precise my question is: Do all completely metrizable topological spaces admit a compatible spherically complete metric? If not, is it known what is the largest class of (completely metrizable) topological spaces that admit one? Even an answer/reference only for locally compact or Polish spaces would be nice. Thank you! Edit (April 24th): If in a few more days there won't be an answer, I'll try cross-posting this on MathOverflow as well.","['general-topology', 'complete-spaces', 'metric-spaces']"
4679152,Understanding and verifying the formula for $\binom{T}{B}$,"I am studying a formula that I came across in my research, which is given by: $$\binom{T}{B}= \frac{1}{T-B}\sum_{\ell=1}^{\infty} \binom{T + \ell - 1}{B - 1} \left(1 - \frac{B}{T} \right)^\ell \cdot \ell$$ I have checked this equation numerically and found it to be accurate, and I want to clarify that the right-hand side of this formula is not an expectation with respect to $\ell$ , but I am having difficulty understanding its meaning and how it was derived. I have tried searching for possible generating functions or discrete probability distributions that might be related to this formula, but so far, I have not been successful. Any help or clarification would be greatly appreciated. Thank you.","['probability-distributions', 'combinatorics', 'discrete-mathematics', 'generating-functions', 'sequences-and-series']"
4679156,"Does there exist an everywhere weakly continuous but nowhere strongly continuous function $f : [0,1] \to X$?","Let $(X, \|\cdot\|)$ be a Banach space and $\mathbb{K}$ be its field of scalars ( $\mathbb{R}$ or $\mathbb{C}$ ). A function $f : [0,1] \to X$ is said to be strongly continuous in $x \in [0,1]$ if it is continuous in $x$ as a function $f : ([0,1], |\cdot|) \to (X, \|\cdot\|)$ , while it is said to be weakly continuous in $x$ if, for all $\Lambda \in X^* := \mathcal{L}(X,\mathbb{K})$ , $\Lambda \circ f : [0,1] \to \mathbb{K}$ is continuous in $x$ . Of course, strong continuity implies weak continuity, and in finite dimension weak continuity implies strong continuity too (it suffices to write $f =: (f_1, f_2, \dots, f_d)$ and to remember that $f$ is continuous iff the $f_k$ s are), but there are functions that are weakly continuous everywhere but not strongly continuous in one point: this post for example features such a function (they chose to make the function go from a domain of $\mathbb{C}$ there but it's very much adaptable to an interval of $\mathbb{R}$ ). and I feel like it should not be hard to have countably many discontinuities by summing countably many functions that are each weakly continuous everywhere and strongly continuous everywhere except in one point each. On the other hand, the existence of nowhere (strongly) continuous functions is guaranteed, for example by picking any non-linear solution of the Cauchy functional equation $g(x+y) = g(x) + g(y)$ and a vector $x$ in $X$ , and then taking $f : t \in [0,1] \mapsto g(t)x \in X$ , and you can probably take Hamel bases of $\mathbb{R}$ and $X$ and make an example with an infinite-dimensional range $f : \sum_i t_i e_i\mapsto \sum_i g(t_i)e'_i$ , but it's hard to see whether those highly irregular functions can make place for a weakly continuous function... As such this got me wondering how ""badly"" can weak continuous functions behave in regards to strong continuity, and as such here is my question: Question : Does there exist $X$ an infinite-dimensional Banach space such that there exists an everywhere weakly continuous yet nowhere strongly continuous function $f : [0,1] \to X$ ? Feel free to edit or re-tag if needed.","['continuity', 'banach-spaces', 'functional-analysis', 'examples-counterexamples']"
4679171,Does $f(f(x))=x$ imply surjectivity? [duplicate],"This question already has answers here : $f:X\rightarrow X$ such that $f(f(x))=x$ (4 answers) Closed last year . If for $f: \mathbb{R} \rightarrow \mathbb{R}$ $f(f(x))=x \quad \forall x \in \mathbb{R}$ , can we say that the function is surjective? I came across this problem while solving a different problem, and I think the answer should be yes, because all such real $x$ is getting mapped to by the function. So it should be surjective, however I'm not sure as I'm quite new in learning functions, so I want to verify my answer. Thank you.","['functions', 'solution-verification']"
4679206,"Find the limit (if it exists) $\lim_{(x,y) \to (0,0)} \dfrac{xy}{x^2 + |y|}.$","I want to find the limit $$\lim_{(x,y) \to (0,0)} \dfrac{x \sin(y)}{x^2 + |y|},$$ if it exists. My idea was to use the fundamental trig limit to obtain $$\lim_{(x,y) \to (0,0)} \dfrac{x \sin(y)}{x^2 + |y|} = \lim_{(x,y) \to (0,0)} \dfrac{xy}{x^2 + |y|} \dfrac{\sin{y}}{y},$$ but now, I need to calculate the limit of the first factor, which I can't. If you solve the limit computationally, you'll know it exists and it equals zero, so you could use polar coordinates to prove it. But the exercise do not say the limit exists, so the use of polar coordinates is not allowed. Do you know how to find this limit $$\lim_{(x,y) \to (0,0)} \dfrac{xy}{x^2 + |y|}?$$",['multivariable-calculus']
4679287,Find the area of the shaded region in the triangle GDT.,"In the graph, $T$ is the point of tangency, the area of the equilateral region $ABT$ is $4\sqrt3$ and $GA=VB$ . Calculate the area of the shaded region.(S: $4\sqrt3$ ) $S_{\triangle ABT}=\frac{l^2\sqrt3}{4}\implies 4\sqrt3=\frac{l^2\sqrt3}{4}\\
\therefore l = 4 =AB=BT=AT$ $AT^2=AD \cdot AV=AD \cdot (4+VB)\implies 16=AD \cdot (4+VB)$ I can't finish.","['euclidean-geometry', 'geometry', 'plane-geometry']"
4679329,Upper bounding third side of a triangle.,"Given triangle with vertices $A,B,C$ (sides $|AB|=c,|BC|=a,|AC|=b$ ) and also given that the angle at $A$ , denoted by $\alpha$ is at most $\pi/3$ . Moreover, we know that there is some positive $d$ such that the sides $b,c$ satisfy $b\leq d, c\leq d$ . I want to show that the side $a$ must also satisfy $a\leq d$ . I tried approaching this by the law of cosines, writing $$a^2=b^2+c^2-2bc\cos \alpha \leq b^2+c^2-bc=(b-c)^2+bc \leq (b-c)^2+d^2$$ but then got stuck. It seems like i need to get rid of the term $(b-c)^2$ somehow. The first $\leq$ comes from the fact that $\alpha \leq \frac{\pi}{3}$ thus $\cos \alpha \geq \frac{1}{2}$ i.e. $-\cos \alpha \leq -\frac{1}{2}$ .","['triangles', 'trigonometry', 'geometry', 'upper-lower-bounds']"
4679347,A Borel measure on $\mathbb{R}$ whose nullsets are exactly the countable sets,"When learning measure theory for the first time, students often think that all (Lebesgue) nullsets in $\mathbb{R}$ are countable - but then have those hopes dashed by counterexamples like the Cantor set. What I'm wondering is the following: is there a ""non-boring"" measure $\mu$ , defined on all Borel subsets of $\mathbb{R}$ (under its standard topology), such that, if $A \subseteq \mathbb{R}$ is Borel, then $$\mu(A) = 0 \iff A \text{ is countable}?$$ If not, why? By ""non-boring"", I mean that there is some set $B \subseteq \mathbb{R}$ measurable with $0 < \mu(B) < \infty$ . Otherwise, you could let $\mu$ be the measure which sends all countable sets to 0 and all uncountable sets to $\infty$ , which would be a pretty boring measure.",['measure-theory']
4679422,"What is known about this structure with idempotence, commutativity, cancellation, and another unnamed property?","I'm interested in an algebraic structure $(S,\cdot)$ satisfying the following axioms: Idempotence: $a \cdot a = a$ for all $a \in S$ Commutativity: $a \cdot b = b \cdot a$ for all $a, b \in S$ Cancellation: if $a \cdot b = a \cdot c$ then $b = c$ for all $a, b, c \in S$ Unknown: $(a \cdot b) \cdot (c \cdot d) = (a \cdot c) \cdot (b \cdot d)$ for all $a, b, c, d \in S$ I know of essentially just two examples that aren't also associative: Fix a set of variables $\{x_1, \dots, x_n\}$ . Define $S$ to be the set of convex combinations of $x_i$ with dyadic rational coefficients, and define $a \cdot b = \frac{a + b}{2}$ . This is easy to visualize by letting the variables represent $n$ points that span $(n-1)$ -dimensional space with a binary operation that returns the midpoint. We can also relax the coefficients from dyadic rationals to rationals or even reals. $S = \{a, b, c\}$ with binary operation $a \cdot b = c$ , $a \cdot c = b$ , and $b \cdot c = a$ Here's a couple very basic properties I do know: From idempotence and (4) we can easily derive $a \cdot (b \cdot c) = (a \cdot b) \cdot (a \cdot c)$ , meaning that $\cdot$ distributes over itself. From idempotence and cancellation, we can see that $a \cdot b = a$ implies that $b = a$ . I don't know much else about it, and I'm curious if it has a name, other non-associative examples, or any interesting properties. I'm also happy to hear about similar algebraic structures, especially non-associative ones with property (4). There's no application for this exactly, just something I came up with while drawing diagrams related to the first example above.",['abstract-algebra']
4679441,Understanding proof that the gradient of a scalar field transforms as a vector under rotations.,"Chapter 1 of Griffiths' Electrodynamics is called ""Vector Analysis"". There is a problem in that chapter that I would like to understand in detail.
A question about this particular problem has been asked before , but it doesn't go into the types of details I will go through and ask about in the current question. Problem 1.14 Suppose that $f$ is a function of two variables $y$ and $z$ only. Show that the gradient $\nabla f=(\partial f/\partial
 y)\hat{y}+(\partial f/\partial z)\hat{z}$ transforms as a vector under
rotations, Eq. 1.29. Hint: $(\partial f/\partial\bar{y}) = (\partial f/\partial y)(\partial
 y/\partial \bar{y})+(\partial f/\partial z)(\partial z/\partial
 \bar{y})$ , and the analogous formula for $\partial f/\partial
 \bar{z}$ . We know that $\bar{y}=y\cos{\phi}+z\sin{\phi}$ and $\bar{z}=-y\sin{\phi}+z\cos{\phi}$ ; ""solve"" these equations for $y$ and $z$ (as functions of $\bar{y}$ and $\bar{z}$ ), and compute the
needed derivatives $\partial y/\partial\bar{y}$ , $\partial
 z/\partial\bar{y}$ , etc. The cited equation 1.29 is a matrix equation for transforming coordinates in one set of axes to coordinates in another set of axes that is rotated by $\phi$ radians relative to the first coordinates. $$\begin{pmatrix}
\overline{y}\\ \overline{z}
\end{pmatrix}=\begin{pmatrix}
\cos{\phi} & \sin{\phi} \\ -\sin{\phi} & \cos{\phi}
\end{pmatrix}\begin{pmatrix}
y \\ z
\end{pmatrix}\tag{1}$$ From the hint, it seems we are asked to figure out what $\partial f/\partial\bar{y}$ and $\partial f/\partial\bar{z}$ are, and to verify that they satisfy the relationship $$\begin{pmatrix}
\partial f/\partial\bar{y}\\ \partial f/\partial\bar{z}
\end{pmatrix}=\begin{pmatrix}
\cos{\phi} & \sin{\phi} \\ -\sin{\phi} & \cos{\phi}
\end{pmatrix}\begin{pmatrix}
\partial f/\partial y\\ \partial f/\partial z
\end{pmatrix}\tag{2}$$ That is, $$\overline{\nabla f}=\begin{pmatrix}
\cos{\phi} & \sin{\phi} \\ -\sin{\phi} & \cos{\phi}
\end{pmatrix}\nabla f\tag{3}$$ If these two vectors do satisfy this relationship, then it means that they behave as expected under the rotation transformation. But what is happening at the linear algebra level here? My first question is: what do $\frac{\partial f}{\partial\overline{y}}$ and $\frac{\partial f}{\partial\overline{z}}$ mean exactly? The square matrix, let's call it $R$ , in (1) is the transformation. The columns are the coordinates of the transformed standard basis vectors $\hat{i}$ and $\hat{j}$ , namely $R\hat{i}$ and $R\hat{j}$ , and these form a basis for the range of the transformation. If we stick the gradient vector on the right hand side of (1) we are transforming the vector and obtaining coordinates in the new basis. This happens by taking the same linear combination used with the old basis, but now with the new basis. As far as I can tell $\frac{\partial f}{\partial\overline{y}}$ and $\frac{\partial f}{\partial\overline{z}}$ are technically the partial derivatives of $f$ under the new basis (that is, if we were to figure out what $f(\overline{y},\overline{z})$ is). Thus, what (3) says is that this new gradient turns out to have the same coordinates under the new basis as under the old basis, which is how all vectors behave under a linear transformation (is this correct?). That is, by showing (3) we are showing that gradients are just regular ol' vectors like any others in its vector space. My second question is: why, ex ante, would there be a possibility that these two vectors would not satisfy this relationship? Here are the calculations I did to accomplish this task Let $$g(\overline{y}, \overline{z})=f(y(\overline{y}, \overline{z}),z(\overline{y}, \overline{z}))$$ That is, $f$ as a function of coordinates in the new basis. $$\frac{\partial g}{\partial \overline{y}}=\frac{\partial f}{\partial \overline{y}}\tag{4}$$ $$\frac{\partial g}{\partial \overline{z}}=\frac{\partial f}{\partial \overline{z}}\tag{5}$$ Therefore, if we can find these partial derivatives of $g$ we will have found $\overline{\nabla f}$ , and we hope that this is an expression in terms of $\nabla f$ . Using the chain rule, we have $$\frac{\partial g}{\partial \overline{y}}=\nabla f(y(\overline{y},\overline{z}),z(\overline{y},\overline{z}))\cdot \left ( \frac{\partial y}{\partial\overline{y}}\hat{i} +\frac{\partial z}{\partial\overline{y}}\hat{j} \right )\tag{6}$$ $$=\frac{\partial f}{\partial y}\frac{\partial y}{\partial \overline{y}}+\frac{\partial f}{\partial z}\frac{\partial z}{\partial \overline{y}}\tag{7}$$ Similarly, $$\frac{\partial g}{\partial \overline{z}}=\nabla f(y(\overline{y},\overline{z}),z(\overline{y},\overline{z}))\cdot \left ( \frac{\partial y}{\partial\overline{z}}\hat{i} +\frac{\partial z}{\partial\overline{z}}\hat{j} \right )\tag{8}$$ $$=\frac{\partial f}{\partial y}\frac{\partial y}{\partial \overline{z}}+\frac{\partial f}{\partial z}\frac{\partial z}{\partial \overline{z}}\tag{9}$$ Note that we know $\overline{y}(y,z)$ and $\overline{z}(y,z)$ because this is given by (1), but we don't know $y(\overline{y},\overline{z})$ or $z(\overline{y},\overline{z})$ . This is why we need to solve for $y$ and $z$ in (1), and when we do this we obtain $$y=-\overline{z}\sin{\phi}+\overline{y}\cos{\phi}$$ $$z=\overline{y}\sin{\phi}+\overline{z}\cos{\phi}$$ From which we can compute $$\frac{\partial y}{\partial\overline{y}}=\cos{\phi}$$ $$\frac{\partial y}{\partial\overline{z}}=-\sin{\phi}$$ $$\frac{\partial z}{\partial\overline{y}}=\sin{\phi}$$ $$\frac{\partial z}{\partial\overline{z}}=\cos{\phi}$$ and plugging these into (7) and (9) we obtain $$\frac{\partial f}{\partial\overline{y}}=\frac{\partial f}{\partial y}\cos{\phi}+\frac{\partial f}{\partial z}\sin{\phi}\tag{7}$$ $$\frac{\partial f}{\partial\overline{z}}=\frac{\partial f}{\partial y}(-\sin{\phi})+\frac{\partial f}{\partial z}\cos{\phi}\tag{7}$$ Thus, we have shown by direct computation of the partial derivatives that (2) and (3) are true.","['vector-fields', 'multivariable-calculus', 'linear-algebra', 'vector-analysis']"
4679448,Where are the other Pythagorean Theorems?,"One way to prove the Pythagorean Theorem is by noticing that its altitude divides it into two pieces similar to itself. The theorem immediately follows from the fact that areas scale with the square of the ratio and the ratios are determined by the sides of the big triangle. The thing about this proof is that it doesn't seem to have anything to do with triangles. Wouldn't any family of shapes that can be - non- trivially - divided into similar copies of itself yield a similar result? The condition for a non-trivial division is inspired by the fact that any triangle or parallelogram can be divided into four congruent copies of itself, but this division doesn't lead to any interesting equation. I'm not sure what formal condition makes a division trivial, though. One could argue that the $(\sqrt 2:1)$ parallelogram can be non trivially divided into two similar copies of itself. However, we can't really obtain any equation from it, since there is no degree of freedom on its sides. Could it be that the right triangles consist of the only interesting family? Is the Pythagorean Theorem really that special? Update 1: I just came across this article , which surprisingly proves that any polygon that can be divided into two similar copies of itself must be similar to one of the following: As the parallelogram, the Golden Bee do not give us any equation. This already implies that the Pythagorean Theorem is, if not the only one, the simpler of its kind. But that is not taking fractals into account: Update 2: As told in a comment, a rep-tile is a polygon that decomposes into congruent copies of itself. Also, an irreptile (irregular rep-tile) is a polygon that decomposes into similar copies of itself (those are the interesting ones). Searching for these terms led to some interesting places: This site gives some examples of irreptiles and discuss some results around them. In this presentation you can see lots of irreptiles to fire the imagination. This article gets into methods to build some irreptiles families. Looking through those I had the impression that we won't get any interesting equation from a irreptile if it is divided into many pieces more then its number of sides. That being said, the following shape called my attention: This one leads to the equation $a^2+2b^2 = c^2$ . However, this seems to be, again, a rigid shape, so we can't really choose $a$ and $b$ .","['euclidean-geometry', 'combinatorics', 'geometry']"
4679452,Evaluate $\lim\limits_{x \rightarrow 0} \bigg ({\frac{(1+x)^{\frac{2}{x}}}{e^2}}\bigg)^\frac{4}{\sin x}$,"$$\lim_{x \to 0} \bigg ({\frac{(1+x)^{\frac{2}{x}}}{e^2}}\bigg)^\frac{4}{\sin x}$$ I was solving this but I got the wrong answer. I want to figure out why it is wrong. I did: $$L=\lim_{x \rightarrow 0} \bigg ({\frac{(1+x)^{\frac{2}{x}}}{e^2}}\bigg)^\frac{4}{\sin x}$$ $$\ln L =\lim_{x \rightarrow 0} \bigg(\frac{4}{\sin x} \frac{2}{x} \  \ln(1+x) -\frac{4}{\sin x}\ln e^2\bigg)$$ $$\ln L =\lim_{x \rightarrow 0} \bigg(\frac{4}{\sin x} \ \frac{2}{x} \  \ln(1+x) -\frac{8}{\sin x}\bigg)$$ Since $\lim\limits_{x\to0} \frac{\ln(1+x)}{x}=1$ , $$\ln L =\lim_{x \rightarrow 0}\bigg(\frac{8}{\sin x}-\frac{8}{\sin x}\bigg)$$ $$\ln L=0$$ $$L=e^0 =1$$ However, the given answer is $e^{-4}$ . Can someone please tell me the mistake I made here. I would be grateful if someone shared the correct solution. Thanks.","['limits', 'limits-without-lhopital']"
4679559,Why quasi coherent sheaves?,So i was wondering why one considers quasi-coherent sheaves in algebraic geometry. I have read a lot that they are closely linked to the geometric properties of the underlying space. This means that you can infer something about the geometry of a ringed space by looking at at the sheaf defined on it. I would love to have some examples in mind where this occurs and maybe there are other motivations on why to study them. Thanks!,"['algebraic-geometry', 'quasicoherent-sheaves']"
4679576,$\limsup_n |X_n|^{1/n}=1$ a.s iff $E[\log(1+|X_1|)$ is finite,"Problem: Suppose $(X_n:n\in\mathbb{N})$ is an i.i.d sequence such that $P[X_1=0]<1$ . Show that $Z:=\limsup_n|X_n|^{1/n}=1\ a.s $ iff $E[\log(1+|X_1|)]<\infty$ In the direction from $\limsup_n |X_n|^{1/n}=1$ a.s to $E[\log(1+|X_1|)$ is finite i manage to get
that if I show $$\lim_n\prod^n_{j=1}E[|X_j|^{1/n}]<\infty$$ i finished. But I haven't managed to show it. In the other direction i tried defining $E_n=\{|X_n|^{1/n}=1\}$ and using Borel-Cantelli but haven't managed to get much progress. Can somebody help me?","['measure-theory', 'probability-theory']"
4679606,Discrepancy in definite integral $\int_{0}^{2\pi}\frac{1}{10+3\cos x}dx$ using Desmos,"I was trying to calculate the following definite integral of a function $f(x)$ : $$\int_{0}^{2\pi}\frac{1}{10+3\cos x}dx$$ After some effort, I managed to find the following antiderivative: $$F\left(x\right)\ =\ \frac{2\tan^{-1}\left(\sqrt{\frac{7}{13}}\tan\left(\frac{x}{2}\right)\right)}{\sqrt{91}}$$ I double checked on Wolfram Alpha, and the back answers of my textbook. I confirmed that my integration is correct. Now I had to apply the limits to get the final answer, so I calculated $F(2\pi) - F(0)$ , and got the answer $0$ . However, when I tried to calculate the definite integral using Desmos, I obtained two different answers.
First, I directly entered the equation in the form $\int_{0}^{2\pi}f\left(x\right)dx$ , and I got a non-zero answer $(0.658656788383)$ . Then, I entered $F(2\pi) - F(0)$ , and this time, I got the answer $0$ . Finally, I plotted $y = f(x)$ , and saw that for all points, $y > 0$ , which means the definite integral couldn't be zero. I'm confused about why Desmos is giving me different answers for the same integral, and I'm not sure which one is correct. Is this a problem with Desmos, or is there an error in my calculations? I've double-checked my integration and the antiderivative, and I believe they are correct. Could there be some issue in my way of applying the limits? Any insights or suggestions on how to resolve this discrepancy would be greatly appreciated. Thank you.","['integration', 'desmos', 'definite-integrals', 'calculus', 'trigonometry']"
4679666,Coloring game on a chessboard,"On an $8×8$ board, you and your friend play the following game. Initially, all the unit squares are white. First, you color any $n$ squares blue. Second, your friend chooses $4$ rows and $4$ columns, then colors all squares in those rows and columns black. What is the minimal $n$ such that, regardless of your friend's move, the board has always at least one blue square remaining? Alright, I believe the minimal $n$ is $13$ . I have proved that for $n = 12$ , your friend always wins. There are always $4$ rows which have at least $8$ blue squares together. He can color those and $4$ columns for each remaining blue square (in the worst case). If you choose $8$ squares on the diagonal your friend can color first $4$ columns and last $4$ rows, if such diagonal goes from top left to bottom right. I have tried to find a construction for $n = 13$ but, so far I have failed to do so.","['chessboard', 'combinatorics', 'coloring']"
4679692,Why all supersingular elliptic curves over $\bar{\mathbb{F}_p}$ are isogenous?,"Lemma 3.2.1 in Baker, González-Jiménez, González, Poonen, ""Finiteness theorems for modular curves of genus at least 2"", Amer. J. Math. 127 (2005), 1325–1387. enter image description here I don't understand why ""all supersingular elliptic curves over $\bar{F_p}$ are isogenous"".
Could anyone help me? This problem arises from Supersingular elliptic curves and their ""functorial"" structure over F_p^2 . Moreover, I found some similar question: Isogenies between elliptic curves and their endomorphism rings and Isogenies between supersingular elliptic curves .","['finite-fields', 'algebraic-geometry', 'isogeny', 'elliptic-curves']"
4679777,Fractional part of $(3+\sqrt{2})^n$,"For $r\in\mathbb{R}$ , define $\Vert{r}\Vert:=\inf_{n\in\mathbb{Z}}\vert r-n\vert$ .
By Question 4208947 and Question 1536761 , we know that $\lim_{n\rightarrow \infty}\Vert(3+2\sqrt{2})^n\Vert=0$ and $\lim_{n\rightarrow \infty}\Vert(2+\sqrt{3})^n\Vert=0$ . The method is interesting but can only be applied to restricted numbers. I am wondering whether $\lim_{n\rightarrow \infty}\Vert(3+\sqrt{2})^n\Vert$ exists and is there $s\in \mathbb{R}\setminus\{0\}$ such that $\lim_{n\rightarrow \infty}\Vert(3+\sqrt{2})^n\cdot s\Vert=0$ . Any advice would be helpful. Thanks!","['limits', 'fractional-part', 'number-theory']"
4679786,Is the intersection of a power set always empty?,"I make this question because of a book exercise, in which I have to use the intersection of a power set. That is, given a non-empty set $A$ , I need to use $\bigcap_{X \in \mathcal{P}(A)}X$ , but knowing that the definition of intersection of sets is $\bigcap_{X \in \mathcal{A}}X = \{x | x \in X \text{ for all } X \in \mathcal{A} \}$ and that a power set always contains the empty set, the set in the definition will always be empty because there is no $x$ which belongs to the empty set. Is that deduction correct?",['elementary-set-theory']
4679859,Differentiating under the integral sign in $n$ variables to show $\partial^{\alpha}(f*g)=f*\partial^{\alpha}g$,"As an easy corollary of Dominated Convergence theorem, Folland gives a criterion for differentiating under the integral sign and uses it to prove Proposition $8.10$ : Proposition $8.10:$ If $f \in L^1$ , $g \in C^k$ and $\partial^{\alpha}g$ is bounded for all multi-indices $\alpha$ with $|\alpha| \leq k$ , then the convolution $f*g \in C^k$ and $\partial^{\alpha}(f*g)=f*\partial^{\alpha}g$ . As proof, Folland simply states this is clear from Theorem $2.27$ as stated above. Attempt: We have $f*g(x)=\int f(y)g(x-y)dy$ . Letting $G:\mathbb{R^n} \times [a,b] \rightarrow \mathbb{C}$ , where $G(y,x_1)=f(y)g(x-y)$ where $x_i$ is fixed for all $i>1$ . Then we see hypotheses in Theorem $2.27.$ b are satisfied and we get $\frac{\partial}{\partial x_1} f*g(x)=\int f(y)\frac{\partial}{\partial x_1}g(x-y) dy=f*\frac{\partial}{\partial x_1}g $ . Now we simply induct on $|\alpha|$ to get the desired result. Is this how we justify differentiating under the integral sign in $n$ variables?","['measure-theory', 'lebesgue-integral', 'convolution']"
4679937,On the rigour of a proof.,"I've been studying the following function $f$ defined on $\mathbb{R}^+$ as follows: $$f(x)=\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x+...}}}}$$ Which I have found a closed form for on the same domain of definition in the following manner:
Because of the recursive nature of the series, one can write the following: $$f(x)=\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x+...}}}}\Longleftrightarrow f(x)=\sqrt{x+f(x)}$$ Hence, treating $f(x)$ as the unknown element of the equation yields: $$f^2(x)-f(x)-x=0$$ And consequently solving for $f(x)$ : $$\Delta=1^2+4x\Longrightarrow f_1(x)=\frac{1+\sqrt{1+4x}}{2}\wedge f_2(x)=\frac{1-\sqrt{1+4x}}{2}$$ Only one of these definitions is in fact correct, considering all outputs of the function have to be positive and remarking that, on $\mathbb{R}^+$ : $$1\leq1+4x\Longleftrightarrow f_2(x)\leq0\ \ \forall x\in\mathbb{R}^+$$ Leaving us with the only possible conclusion that: $$f(x)=\frac{1+\sqrt{1+4x}}{2}$$ My question is as to how rigorous this proof is. Does it truly hold for all values of $x\in\mathbb{R}^+$ ? The two functions certainly seem to match up on graphing calculators like Desmos. Is this rigorous enough, or still too hand-wavy?","['functional-equations', 'recreational-mathematics', 'functions']"
4679979,Pushforward of the Segre embedding in K-theory,"Fix $n$ , $m\ge 1$ , and let $d=\binom{m+n}{m}$ and $N=mn+m+n$ . Consider the Segre embedding $\sigma:\mathbb{P}^m\times \mathbb{P}^n \hookrightarrow \mathbb{P}^{N}$ , which has degree $d$ . I'm trying to understand the pushforward of the external tensor product of sheaves. Namely: What is $\sigma_*({\mathcal{O}}_{\mathbb{P}^m}(a)\boxtimes{\mathcal{O}}_{\mathbb{P}^n}(b))$ in terms of (various) ${\mathcal{O}}_{\mathbb{P}^N}(l)$ ? It is well-known that the K-theory of $\mathbb{P}^N$ is generated by $[{\mathcal{O}}_{\mathbb{P}^N}(l)]$ for $l=0,...,N$ . So my follow up would be: What is $[\sigma_*({\mathcal{O}}_{\mathbb{P}^m}(a)\boxtimes{\mathcal{O}}_{\mathbb{P}^n}(b))]$ in K-theory, in terms of $[{\mathcal{O}}_{\mathbb{P}^N}(l)]$ for $l=0,...,N$ ?","['algebraic-k-theory', 'projective-schemes', 'coherent-sheaves', 'algebraic-geometry', 'projective-space']"
4680040,Is this a stronger form of the triangle inequality?,"Let $a,b,c$ be the length of the sides of a triangle, $s$ be its semi-perimeter and $A$ be its area. I wanted to see if the well know Euclidean triangle inequality $a+b > c$ can be improved. I inscribed billions of triangles of all shapes in a unit circle centered on the origin (code given below) and observed $$
a + b - c \ge  \frac{8A^2}{s^3}\bigg(1 + \frac{4A^2}{c^4}\bigg) \tag 1
$$ Since both terms on the RHS are positive, the standard triangle inequality follows. The first term is a constant for a fixed triangle while the second term is dependent on $c$ . Intuitively, this makes sense as it shows that $a+b-c$ not only depends on the triangle but also on which side is selected from the other two. I could not find this inequality my searches so I am not sure if this is known or not. Question : I am looking for a proof or any reference in existing literature. import random as rn

n = 1
min = 10**9

while True:
    x1 = ((-1)**rn.randint(0,1))*(rn.random())
    y1 = ((-1)**rn.randint(0,1))*(1 - x1**2)**0.5
    x2 = ((-1)**rn.randint(0,1))*(rn.random())
    y2 = ((-1)**rn.randint(0,1))*(1 - x2**2)**0.5
    x3 = ((-1)**rn.randint(0,1))*(rn.random())
    y3 = ((-1)**rn.randint(0,1))*(1 - x3**2)**0.5
    
    a = round(((x1 - x2)**2 + (y1 - y2)**2)**0.5,14)
    b = round(((x2 - x3)**2 + (y2 - y3)**2)**0.5,14)
    c = round(((x3 - x1)**2 + (y3 - y1)**2)**0.5,14)
    
    if (a+b < c) or (b+c < a) or (c+a < b):
        print(x1,y1,x2,y2,x3,y3,a,b,c)
        break
    
    s = round((a+b+c)/2,14)
    area = round((s*(s-a)*(s-b)*(s-c))**0.5,14)
    
    # Test inequality
    test = round((((a+b-c)*(s**3)/(area**2) - 8)*(c**4)/(area**2)),14)
    
    if test < min:
        min = test
        print('Inequality',a, b, c, s, area, min)
    
    if n%1000000 == 0:
        print(n)
    
    n = n + 1","['euclidean-geometry', 'inequality', 'geometry', 'triangles', 'algebra-precalculus']"
4680043,"Given a rotated rectangle with known center coordinates, width, height, and angle of rotation, how can the coordinates of the bounding box be found? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Say a rectangle is rotated x degrees clockwise around it's center point. Known: The width/height of the inner rectangle, the center point, and the angle. Desired: The 4 coordinates of each corner of the bounding box. How can this be calculated?","['trigonometry', 'geometry']"
4680159,Understaing the proof of genus of smooth projective curve.,"I'am reading Miranda's book that proves that the genus $g(X)$ of a smooth projective complex plane curve $X=\left\{[x: y: z] \in \mathbb{P}^2 \mid F(x, y, z)=0\right\}$ of degree $d$ is equal to $$
g(X)=\frac{(d-1)(d-2)}{2}
$$ He takes the standard projection $$
\begin{gathered}
\pi: \mathbb{P}^2 \rightarrow \mathbb{P}^1 \\
{[x: y: z] \rightarrow[x: z]}
\end{gathered}
$$ in his prove. My quesion is that this projection is not well-defined usually, does he mean that under some linear trasformation we can make this projection well-defined ? Let us consider a elliptic curve $Y^{2}Z=X(X-Z)(X-2Z)$ , then [0:1:0] is on the curve hence $\pi$ is not well-defined. If we replace x by x+cy and z by z+dy then we can avoid that!","['complex-geometry', 'algebraic-geometry']"
4680203,"If $x$ and $y$ are two real quantities connected by the equation $9x^2+2xy+y^2-92x-20y+244=0$ then will $x \in [3,6]$, and $y \in [1,10]$?","If $x$ and $y$ are two real quantities connected by the equation $$9x^2+2xy+y^2-92x-20y+244=0$$ then will $x \in [3,6]$ , and $y \in [1,10]$ ? What could be some of the intuitive / standard ways of finding that ? What I've tried: My attempt using completing squares : The given $eq^n$ can be expressed as $8(x-3)(x-6) + (x+y-10)^2 = 0$ From here , it's clear that $(x+y-10)^2 ≥ 0$ and for that $(x-3)(x-6) ≤ 0$ which implies $x \in [3,6]$ and $|x+y-10| ≥ 0$ Now since, $|x+y-10| ≥ 0$ and $x \in [3,6]$ therefore, $y \in [4,7]$ .
Where am I being wrong ?","['algebra-precalculus', 'quadratics']"
4680242,How to prove that this function $(x^2 + y^2) \exp{(\frac{xy}{x^2+y^2})}$ has limit?,"I have this multivariable function, and I need to prove that limit does exist and it's equal to zero. $$(x^2 + y^2) \exp{\left(\frac{xy}{x^2+y^2}\right)}$$ for $(x, y) \rightarrow (0, 0)$ I've tried using polar coordinates method, and  I've got: $$\rho^2\exp{\frac{1}{\rho}}$$ then I write this inequality showing that: $$\left|(x^2 + y^2) \exp{\left(\frac{xy}{x^2+y^2}\right)}\right| \leq \left|\frac{1}{\rho}\right|$$ but the right hand side diverges as $\rho \to 0$ , and therefore i can't tell nothing about the left hand side. I've tried using the restriction method, I've chosen: $(mx^n, 0)$ and I've obtained, $$m^2x^2x^n * \exp{\frac{0}{m^2x^2x^n}} = 0$$ but I can't say if the limit does exist, because restriction method tells me if the limit doesn't exist. how can I do it? EDIT (solved): there was an error in the polar coordinates substitution, there isn't $\frac{1}{\rho}$ , instead I should've written $\rho^2*\exp{\left(\frac{\rho^2*(\cos * \sin)}{\rho^2}\right)}$ and the rest is straightforward, I've obtained $\rho^2*e$ which is the right hand side of the inequality, and it converges as expected.","['limits', 'multivariable-calculus']"
4680258,Convergence about an infinite double random array,"Suppose that we have an infinite double random array $\{X_{ij}:i\geq 1,j\geq 1\}$ , and every elements in this array are independent identically distributed with mean zero and variance unit. We define $$Y_{i,n}=\frac{1}{n}\sum_{j=1}^nX_{ij}$$ and by the strong law of large number, for any $i\geq 1$ , we have $$Y_{i,n}\to 0,\quad\text{a.s.}\qquad Y_{i,n}^2\to 0,\quad\text{a.s.}\qquad(n\to\infty)$$ My question is that do we still have $$\frac{1}{n}\sum_{i=1}^nY_{i,n}^2\to 0\quad\text{a.s.}\quad?$$ First, I know that it is obviously wrong in arbitrary double array $\{a_{ij}:i\geq 1,j\geq 1\}$ , for example: $$a_{ij}=\frac{2^i}{j},\quad\text{for any $i\geq 1$, we have}\quad b_{i,n}=\frac{1}{n}\sum_{j=1}^na_{ij}\to 0$$ and unfortunately, $$\frac{1}{n}\sum_{i=1}^nb_{i,n}^2\to\infty$$ However, in this problem, we have independent identically distributed structure, and I think it can be proved.","['measure-theory', 'probability-limit-theorems', 'means', 'law-of-large-numbers', 'probability-theory']"
4680282,Find the sum of the shaded areas in the figure below,"Calculate : $A1 + A2 + A3$ , if $PQ=2$ . (S: $\frac{\pi}{2}-1)$ I confess that I was not able to develop this exercise... I found it very difficult.. Apparently $ PAIQ $ would be a parallelogram and this could be useful in the resolution but would need to be demonstrated","['euclidean-geometry', 'geometry', 'plane-geometry']"
4680307,Continuous functions in Sobolev spaces,"Let $W^{k,p} (\Omega)$ be a Sobolev space, $\Omega \subset \mathbb{R}^N$ . Formally, $W^{k,p}(\Omega)$ consists of equivalence classes of functions with finite Sobolev norm. Two functions, $f$ and $g$ , are said to be equivalent ('equal') if $\Vert f-g \Vert_{W^{k,p}} =0$ . In many proofs concerning Sobolev functions, say, Poincaré inequality, one first shows the claim for smooth functions and then generalises by using the fact that smooth functions $C_0^{\infty} (\Omega)$ are dense in $W_0^{1,2}(\Omega)$ . However, it is unclear to me what is meant by writing $u \in C_0^{\infty}(\Omega)$ . Does it mean that $u$ is equivalent to a smooth function (in the $W^{1,2}$ sense) or that $u$ is itself a smooth function? One proof for Poincaré inequality on $\Omega:=(a, b)\subset \mathbb{R}$ starts by writing the value of $u \in C_0^{\infty}(a, b)$ as $$ u(x)=\int_a^x u dx+u(a), $$ however, if $u$ is considered to be a Sobolev function, one cannot talk about its point wise values, $u(a)$ in particular. Or is it some kind of an agreement that $u(a)$ is the value of the smooth function to which $u$ is equivalent (in $W^{1,2}$ )? Another confusion arises when talking about the trace operator. In the Wiki page , it is said that the trace $T: W^{1,p}(\Omega) \rightarrow L^p (\partial \Omega)$ is such that $Tu = u\mid_{\partial \Omega}$ for any $u \in W^{1,p}(\Omega) \cap C(\overline{\Omega})$ . Again, $W^{1,p}$ consist of equivalence classes whereas $C(\overline{\Omega})$ consists of functions. So does the intersection mean functions with finite Sobolev norm which are equivalent to some continuous function? And is $u\mid_{\partial \Omega}$ the boundary function of $u$ , or the set of functions equivalent to it?","['analysis', 'real-analysis', 'sobolev-spaces', 'trace-map', 'functional-analysis']"
4680314,Dealing with variables in multivariable limit questions,"I am trying to solve the question below. My initial approach was to look at the limit along the x and y axis which both yield 0. Now, I moved on to look at the limit along the path of y = mx.
When I did this, i noticed that I had to fix my x and substitute y for mx instead of saying x = y/m and y = mx. When I did the latter, I ended up going in circles and it produced the original equation. My question is, why do I need to fix the x value? What is the mathematical reasoning?","['limits', 'multivariable-calculus', 'problem-solving']"
4680318,When does every automorphism of a ring of continuous functions come from an automorphism of the space?,"In recent days I am studying rings of continuous functions and I was wondering whether the following is true: Let $X$ be a topological space that is Tychonoff (that is, $X$ is completely regular and $T_{1}$ ) and $C(X)$ be the ring of all real valued continuous functions on $X$ . Now, if $F: X \rightarrow X$ be a homeomorphism (with respect to the same topology on $X$ ), then there is a ring isomorphism $G: C(X) \rightarrow C(X)$ given by $G(f) = f \circ F$ for all $f \in C(X)$ . Now, my question is the following: if $G': C(X) \rightarrow C(X)$ be any ring isomorphism, then does there exist a homeomorphism $F': X \rightarrow X$ (with respect to the same topology on $X$ ) such that $G'(f) = f \circ F'$ for all $f \in C(X)$ ? If not, then for which topological spaces $X$ the statement is true? If $X$ is completely regular, then all real-valued continuous functions on $X$ determine the topology of $X$ . My question is somewhat different and that can be seen from the question.","['continuity', 'general-topology']"
4680336,de Rham cohomology of dual abelian variety,Suppose $X$ is an abelian variety over a field $k$ . Consider the de Rham cohomology of $X^{\vee}$ (denoted by $H_{dR}^i(X^{\vee})$ ). I want to ask is $H_{dR}^i(X^{\vee})$ naturally dual to $H_{dR}^i(X)$ ?,"['algebraic-geometry', 'de-rham-cohomology', 'abelian-varieties']"
