question_id,title,body,tags
4271831,Difference between two methods,"$f(x)=\begin{cases} 
      x^2 \sin (\frac{1}{x}) & x\neq 0 \\
      \ 0 & x=0  
   \end{cases}
\ $ Check Differentiability at $x$ = $0$ My Approach: $f'(0)=\lim_{x\to 0}\frac{f(x)-f(0)}{x-0}=\lim_{x\to 0}\frac{x^2\sin(\frac{1}{x})}{x}=\lim_{x\to0} x\sin\frac{1}{x}$ . this limit is defined, so indeed the function is differentiable at zero. Second Approach If i derivate functions directly and calculate $f'(0^+)$ and $f'(0^-)$ . $f'(x)=x^2 \cdot \cos(\frac{1}{x}) \cdot (\frac{-1}{x^2}))+2x\; \cdot \sin (\frac{1}{x})$ So $f'(x)=\cos(\frac{1}{x})$ which lies in  [-1,1] for $x$ approaching to $0$ . Hence Limit does not exist. So function must be non-differentiable. Why second method give false result when i directly differentiate. For all other question except in question when function is oscillating i get same result by both method","['limits', 'derivatives']"
4271858,Continuous Bijection which is Not a Homeomorphism,"The problem is: given $A = (0,1) \subset \mathbb{R}$ , find a set $B \subset \mathbb{R}^2$ and a function $g: A \longrightarrow B$ which is a continuous bijection but not a homeomorphism. So I know that in order for $g$ not to be a homeomorphism, its inverse must not be continuous. I have seen a similar example of a function $f: (0,1] \longrightarrow S^1$ which not a homeomorphism since $f^{-1}$ is not continuous at $(1,0).$ However, if the domain of $f$ were instead $(0,1)$ it would be a homeomorphism, so I'm not sure how (or if) $S^1$ would work in the problem for the codomain of $g$ . I've looked at the answers to the other questions on Math.SE pertaining to examples of non-homeomorphic continuous bijections but none have seemed to help, e.g. the domain is usually more like $(0,1]$ than $(0,0)$ .","['general-topology', 'functions']"
4271946,Loewy Length of Tjurina algebra,"Suppose $k$ is an algebraically closed field with characteristic $0$ . Let $k[x_0,\ldots,x_n]$ be the graded polynomial ring and $f\in k[x_0,\ldots,x_n]$ be a homogeneous polynomial of degree $d$ and $A=k[x_0,\ldots,x_n]/(f)$ . Let $J=(\partial f/\partial x_0,\ldots,\partial f/\partial x_n) $ be the Jacobian ideal of $A$ . Assume $A$ has an isolated singularity. It turns out that the Tjurina algebra $A/J$ is Artinian. In the paper Orlov spectra: bounds and gaps , the authors mentioned in the proof of Lemma 5.18 that
the Loewy length of $A/J$ is $d(n+1)-2n-1$ by Macaulay's theorem, where the Loewy length of $A/J$ is the minimal number $m$ such that $((x_1,\ldots,x_n)/(f,J))^m=0$ . My question: what is Macaulay's theorem here? It doesn't have a reference in their paper. I can't find this theorem in Google. I also don't know how to show the Loewy length is that number. Is Macaulay's theorem well-known?","['homological-algebra', 'algebraic-geometry', 'commutative-algebra']"
4271962,What is the meaning of the residue field of a point in scheme?,"If I consider the analogy of local ring at a point to the space of function germs at the point, then the residue field can be seen as the values that functions can take at the point. But when I consider the residue field of generic point or the residue field of a point in a scheme over a non-algebraically closed field, the above analogy becomes unreasonable to me. On Wikipedia entry ""Residie field"", it says ""One can say a little loosely that the residue field of a point of an abstract algebraic variety is the 'natural domain' for the coordinates of the point."" Can you elaborate also a bit on this?","['affine-varieties', 'algebraic-geometry', 'schemes', 'projective-varieties']"
4271999,"Question in my work in the problem ""if $f:[a,b]\to\mathbb{R}$ has finite limit in every point of $[a,b]$ then $f$ is bounded""","Let $f:[a,b]\to\mathbb{R}$ a function such that $f$ has finite limit in every point $x \in [a,b]$ , prove that $f$ is bounded. I was thinking if this problem could be solved in the following way: since by hypothesis $f$ has finite limit for every $x \in [a,b]$ , I can define $\tilde{f}:[a,b]\to \mathbb{R}$ such that $\tilde{f}$ has the same value of the limits of $f$ in every point of $[a,b]$ . By doing this, $\tilde{f}$ is continuous in all $[a,b]$ and since $[a,b]$ is compact it follows that $\tilde{f}$ has absolute maximum and minimum in $[a,b]$ and these maximum and minimum are bounds for $f$ in $[a,b]$ , hence $f$ is bounded. Could this work? If this is wrong, can someone explain me why it doesn't work? Moreover, is there a way to write formally the function $\tilde{f}$ ?","['limits', 'solution-verification', 'real-analysis']"
4272058,Why does the trick to remember the trigonometric table work?,"When I was first introduced to trigonometric ratios, I learned the following trick to remember the trigonometric table of standard angles ( $0^{\circ}$ , $30^{\circ}$ , $45^{\circ}$ , $60^{\circ}$ , $90^{\circ}$ ): Sine: Take the numbers $0$ , $1$ , $2$ , $3$ , $4$ for each angle $0^\circ$ , $30^\circ$ , $45^\circ$ , $60^\circ$ , $90^\circ$ . Divide each number by $4$ and take the square root of the result. This will give the $\sin$ ratio of each corresponding angle. Tangent: Take the numbers $0$ , $1$ , $3$ , $9$ for each angle $0^\circ$ , $30^\circ$ , $45^\circ$ , $60^\circ$ . Divide each number by $3$ and take the square root of the quotient. This will give the $\tan$ ratio of each corresponding angle. (Note that $\tan90^\circ$ is undefined) For example if we want to find the value of $\sin60^\circ$ , we divide $3$ by $4$ and take the square root of $\frac34$ . Then we get the result $\frac{\sqrt3}{2}$ which is indeed the value of $\sin60^\circ$ . This two tricks allow to remember the whole trigonometric table using the relation of $\sin$ and $\tan$ with other trigonometric ratios. I think the trick is well-known. My question is that why do these tricks work ? Are they kind of coincidence (which is highly unlikely)? I can prove the trigonometric ratios individually. For example, I can prove geometrically that $\sin60^\circ$ is equal to $\frac{\sqrt3}{2}$ .","['trigonometry', 'angle', 'soft-question']"
4272088,"How to solve $x=\lim\limits_{t\to0} Q^{-1}(t,t)\implies \text{Ei}(-x)=-1\implies Γ(0,x)=1$?","Based on: Conjecture: $$\lim\limits_{x\to\infty}\operatorname{Re}\text W_x(x)\mathop=\limits^?-\ln(2\pi)$$ and On completing the solution for $$\int_0^1 Q^{-1}(x,x) dx$$ and other constants. Here is the goal limit again using the central Inverse Regularized Gamma function $Q^{-1}(x,x)$ : $$α\mathop=^\text{def} \lim_{x\to0} Q^{-1}(x,x)=Q^{-1}(0,0)\ne 0$$ Note that Wolfram Alpha says that $Q^{-1}(0,0)=0$ , but this is just due to the definition of the function. If you take a look at the graph, the “true” limit clearly is not $0$ : However, we can use the main Taylor series definition centered at $x=1$ with the new $C_{n,x}$ coefficient notation. Note that there are other series in the bolded link. $$Q^{-1}(x,x)=((1-x)x!)^\frac1x+\frac {((1-x)x!)^\frac2x}{x+1}+ \frac {(3x+5)((1-x)x!)^\frac3x}{2(x+1)^2(x+2)}+O\left((x-1)^\frac4x \right)\mathop =^\text{def}\sum_{n=0}^\infty C_{n,x}\cdot ((1-x)x!)^\frac nx$$ Now let’s apply the limit noting that we can apply the limit term by term. The limit is a long process, but ends up being in terms of $e$ and the Euler-Mascheroni constant γ : $$α= \lim_{x\to0} Q^{-1}(x,x)=Q^{-1}(0,0)= \lim_{x\to 0}\sum_{n=0}^\infty C_{n,x}\cdot ((1-x)x!)^\frac nx=\sum_{n=1}^\infty \frac{C_{n,0}}{e^{(γ+1)n}} = e^{-(γ+1)}+ e^{-2(γ+1)}+ \frac 54 e^{-3(γ+1)}+\frac{31}{18} e^{-4(γ+1)}+ \frac{361}{144}e^{-5(γ+1)}+\frac{4537}{1200} e^{-6(γ+1)} +…=.26473… $$ A few observations on the constants: The nth numerator and denominator of $C_{n,0}$ is on the order of $10^n$ for most of the first few terms. Here is a graph of the first few coefficients in the expansion. An equivalent problem is by solving the following differential equation for the function : $$y(x)y’’(x)-y’(x)^2(y(x)+1-a)=0,y(x)=Q^{-1}(a,x)\implies y(x)y’’(x)-y’(x)^2(y(x)+1)=0,y(x)=\lim_{a\to 0} Q^{-1}(a,x),y(x)=y(a,x),y(0,0)=α$$ It turns out that the special case can be solved in terms of an Exponential Integral function . Let me “make up” an inverse Exponential Integral function: $$y(x)y’’(x)-y’(x)^2(y(x)+1)=0\implies c+x=\frac{\text{Ei}(-y(x))}{c}\implies c^2+cx=\text{Ei}(-y(x))\implies y(x)=-\text{Ei}^{-1}(c^2+cx)$$ So the constant is maybe related to Soldner’s Constant Another idea is letting $x=a$ for: $$y(x)y’’(x)-y’(x)^2(y(x)+1-a)=0, y(0)$$ but I am not sure if this would produce $y=Q^{-1}(x,x)$ . Amazingly, our differential equation works: $\text{Ei}(-α)=-1$ Therefore: $$\text{Ei}(-Q^{-1}(0,0))=\text{Ei}\left(-\sum_\Bbb N C_{n,0} e^{-n(γ+1)}\right)=-1$$ So a “closed form” using a made up Inverse Exponential Integral function would be: $$α=\lim_{x\to0}Q^{-1}(x,x)=-\text{Ei}^{-1}(-1)$$ You can also find the following assuming an inverse function: $$y’=ye^y\implies c_1+x=\text{Ei}(-y(x))\implies y(x)=-\text{Ei}^{-1}(x+c_1), x+c_1=-1$$ A better way would be to solve $$\text{Ei}(-x)=-1,x=α$$ for $x$ . Let’s use this method to find many more digits of the constant $$α=0.264737010451543159461927…$$ while the other real root, for fun, of $\text{Ei}(-x)+1$ is: $$x= -0.1724867417161…$$ Here are $3$ other identities of this constant using this Incomplete Gamma function identity and this Generalized Exponential Integral function . The third one uses the Hyperbolic Cosine and Hyperbolic Sine Integral functions $$Γ(0,α)=\text E_1(α)=1\implies \int_α^\infty \frac{dx}{xe^x}=1$$ $$\text{Shi}(α)-\text{Chi}(α)=1$$ Note that other constants may also fit the formulas for the constant. Many other identities can be similarly derived, so please go ahead; I have written enough for now. How can I find an alternate form of the constant? Please correct me and give me feedback!","['special-functions', 'ordinary-differential-equations', 'sequences-and-series', 'limits', 'constants']"
4272096,A detailed proof of Fatou’s lemma,"This is an important lemma, so I would like to give it a detailed attempt of how such hypothesis is necessary for the lemma to hold. Could you confirm if my understanding is fine? Let $(X_{n})$ be a sequence of a.s. nonnegative random variables. Then $\mathbb{E}\left(\liminf X_{n}\right) \leq \liminf \mathbb{E}\left(X_{n}\right)$ . If there exists a $Y$ such that $X_{n} \leq Y$ a.s. for all $n$ and $\mathbb{E}(Y)<\infty$ , then $\mathbb{E}\left(\limsup X_{n}\right) \geq \limsup \mathbb{E}\left(X_{n}\right)$ . Proof: Lemma 1: $\mathbb E [\inf_{k \ge n} X_k] \le \inf_{k \ge n} \mathbb E [X_k] \le \sup_{k \ge n} \mathbb E [X_k] \le \mathbb E [\sup_{k \ge n} X_k]$ for all $n$ . Proof: We have $\inf_{k \ge n} X_k \le X_t$ and thus $\mathbb E [\inf_{k \ge n} X_k] \le \mathbb E [X_t]$ for all $t\ge n$ . Similarly, we obtain $\sup_{k \ge n} X_k \ge X_t$ and thus $\mathbb E [\sup_{k \ge n} X_k] \ge \mathbb E [X_t]$ for all $t\ge n$ . The result then follows by taking the $\inf$ and the $\sup$ . Notice that $$\mathbb{E}\left(\liminf X_{n}\right) \leq \liminf \mathbb{E}\left(X_{n}\right) \iff \mathbb{E}\left(\lim_n \inf_{k\ge n} X_{k}\right) \leq \lim_n \inf_{k \ge n} \mathbb{E}\left(X_{k}\right).$$ Let $Y_n = \inf_{k\ge n} X_{k}$ . Then $(Y_n)$ is a non-decreasing sequence of non-negative random variable such that $Y_n \nearrow \lim_n \inf_{k\ge n} X_{k}$ . By monotone convergence theorem, $\mathbb{E} (Y_n) \nearrow \mathbb{E}\left(\lim_n \inf_{k\ge n} X_{k}\right)$ . So it suffices to show that $\mathbb E (Y_n)  \le \inf_{k \ge n} \mathbb{E}\left(X_{k}\right)$ for all $n$ . However, this follows directly from our Lemma 1 . Lemma 2: Let $( X_{n} )$ be a non-increasing sequence of non-negative random variables such that $X_n \searrow X$ a.s., and $\mathbb E (X_0) <\infty$ . Then $\mathbb{E}\left(X_{n}\right) \searrow \mathbb{E}\left( X \right)$ . Proof: We have $(X_0-X_n)$ is a non-decreasing sequence of non-negative random variables such that $(X_0-X_n) \nearrow (X_0-X)$ a.s. By monotone convergence theorem, $\mathbb{E}\left(X_0-X_{n}\right) \nearrow \mathbb{E}\left(X_0- X \right)$ . With $\mathbb E (X) \le \cdots \le \mathbb E (X_1) \le \mathbb E (X_0) <\infty$ , we have $\mathbb{E} (X_0) - \mathbb{E} (X_{n}) \nearrow \mathbb{E} (X_0) - \mathbb E(X)$ . The result then follows. Notice that $$\mathbb{E}\left(\limsup X_{n}\right) \geq \limsup \mathbb{E}\left(X_{n}\right) \iff \mathbb{E}\left(\lim_n \sup_{k\ge n} X_{k}\right) \geq \lim_n \sup_{k \ge n} \mathbb{E}\left(X_{k}\right).$$ Let $Z_n = \sup_{k\ge n} X_{k}$ . Then $(Z_n)$ is a non-increasing sequence of non-negative random variable such that $Z_n \searrow \lim_n \sup_{k\ge n} X_{k}$ . Moreover, $\mathbb E (Z_{0}) \leq \mathbb E (Y) < \infty$ . By Lemma 2 , $\mathbb{E} (Z_n) \searrow \mathbb{E}\left(\lim_n \sup_{k\ge n} X_{k}\right)$ . So it suffices to show that $\mathbb E (Z_n)  \ge \sup_{k \ge n} \mathbb{E}\left(X_{k}\right)$ for all $n$ . However, this follows directly from our Lemma 1 .","['convergence-divergence', 'solution-verification', 'probability-theory']"
4272097,Fibonacci Addition Identity for Fibonacci Numbers Separated by 3 Terms,"The Fibonacci Addition Identity states that: $F_{n}=F_{m}F_{n-m+1} + F_{m-1}F_{n-m}$ .
This was useful in showing that: $F_{i+k}=F_{k-2}F_{i+1} + F_{k-1}F_{i+2}$ .
However, I would like to use this result to express the same for $F_{i}$ and $F_{i+3}$ , where we can express $F_{i+k}$ in some linear combination of $F_{i}$ and $F_{i+3}$ Is there any way to do this? I haven't been able to make use of the typical Fibonacci substitutions to make any progress.","['number-theory', 'fibonacci-numbers', 'discrete-mathematics']"
4272258,Convergence in probability implies convergence in quantile of inverse quantile.,"I'm having some problems proving the following result. Let $F_{n}, n=0,1,2, \ldots$ , be c.d.f.'s such that $F_{n}
 \rightarrow{ }_{w} F_{0} .$ Let $G_{n}(U)=$ $\sup \left\{x: F_{n}(x)
 \leq U\right\}, n=0,1,2, \ldots$ , where $U$ is a random variable
having the uniform $U(0,1)$ distribution. Show that $G_{n}(U)
 \rightarrow{ }_{p} G_{0}(U)$ . Here I think G represents a sort of inverse quantile functions. If we assume that the r.v. are continuous, then G is just the random variables themselves. However, I have no idea how to prove this general case when it's not assumed that the inverse quantile exists. Any help is appreciated. Thanks.","['measure-theory', 'probability-distributions', 'convergence-divergence', 'probability']"
4272290,What functions do we need to solve linear second order differential equations with polynomial coeficients?,"Disclaimer: I have posted this question on mathoverflow.net following the instructions of this topic . I'm now trying to understand how can a ordinary differential equation be tested to decide if it's integrable or not. Recently I become aware of the Painlevé property and start to read the following paper by Robert Conte: The Painlevé Approach to Nonlinear Ordinary Differential Equations In section 2.1, he states: ""A very deep result of L. Fuchs, Poincaré and Painlevé is that the class of first order ODEs...
...defines one and only one function... ...the elliptic function introduced earlier by Weierstrass..."" My initial question is: Does this means that the solutions of any integrable first order ODE can by expressed by elementary functions and the Weierstrass elliptic function? I know that many functions that are solutions to second order ODE (Exponentials, Bessel functions , hypergeometric functions, Airy functions ...) can be expressed by generalized hypergeometric series. My main question is: Are there some set of functions such that all solutions to linear second order ODE, with polynomial coeficients, can be expressed with? (may be: rational functions, exponentials and generalized hypergeometric series) If yes, where can I find a comprehensive list?","['integrable-systems', 'special-functions', 'ordinary-differential-equations']"
4272295,Are $\pi$ and $\tan^{-1}\left(2\right)$ rational multiples of each other? [duplicate],"This question already has answers here : Is ArcTan(2) a rational multiple of $\pi$? (4 answers) Closed 2 years ago . For a proof of quantum universality, I need to show that $\tan^{-1}\left(2\right)$ is not a rational multiple of $\pi$ . How do I show this? I feel like showing algebraic independence over the rationals is hard in general, but is it possible for $\pi$ and an awkward trigonometric value?","['irrational-numbers', 'algebraic-independence', 'pi', 'trigonometry', 'quantum-computation']"
4272313,Solving set equation $X \cap A = X \cup A$ for $X$,"How to solve the following set equation: $X \cap A = X \cup A$ for $X$ ? Intuitively it's obvious that it has to be $X = A$ but in class we solved it rigorously using de Morgan's rules. We started from identity $(X \cup A)\cap(X \cup A)^c = \emptyset$ , then we used the given equation to get $(X \cup A)\cap(X \cap A)^c = \emptyset$ and then we used de Morgan's rule: $(X \cup A)\cap(X^c \cup A^c) = \emptyset$ . And now I can't figure it out have we went from here to $(X\cap X^c)\cup(X \cap A^c)\cup(A \cap X^c)\cup(A \cap A^c) = \emptyset.$ Please help. Thanks.",['elementary-set-theory']
4272378,Is there a geometry behind the singularity formation in solutions to nonlinear ODE's?,"Disclaimer: I have posted this question on mathoverflow.net following the instructions of this topic . If we take two apparently simple first order ODE's like $y'=y$ and $y'=y^2$ we find that: For the first one the general solution is $y=C\exp(t)$ and is defined for all $t$ . For the second one, the general solution is $y=\frac{1}{C-t}$ and it's defined only for $t<C$ . There is a way to 'see' geometrically the solutions of these equations as leaves of the ""Characteristic Foliation"" of the standard contact structure of $R^3$ . Look at this site and this article for some nice pictured examples. Looking for the characteristic foliation of the two equations, the shape of the surface seens to play a big role. Apparently is the curvature of the surface that, somehow, 'controls' the growth of solutions. My question is: Are there some notion of curvature that explains why nonlinear EDO's can have solutions with movable singularities? I know that Contact geometry doesn't have local invariants as Riemannian geometry, so, I believe, the answer to this question lies beyond the domain of Contact/Symplectic geometry. ps: taking other linear and nonlinear equations, with or without time varying coeficients, I had the same impression. Here are some pictures that I made: Characteristic foliation for $y'=y$ Characteristic foliation for $y'=y^2$","['symplectic-geometry', 'ordinary-differential-equations', 'curvature', 'contact-geometry', 'differential-geometry']"
4272419,"best strategies for 'Squid Game' episode 6 game 4 ""marble game"" players","Two players each get $n=10$ marbles. Every alternating turn, one player (say, the player whose turn it is) hides an amount of own marbles in fist, and, the other player must guess if hidden amount is odd or even, and, that other player (i.e., the player whose turn it is not) also must bet an amount of own marbles. If guess is right, the player whose turn it is gives (as much as possible) the amount of bet marbles to opponent. If guess is wrong, the player whose turn it is takes the amount of bet marbles from opponent. Next, the turn now alternates to the other player. The game stops when one player has all $2n=20$ marbles. The losing player gets killed (in the series, that is). Which strategies for both players (perhaps one strategy for the one who gets first turn, and one strategy for the other player) give maximal probabilities to win (and to not get killed). We must assume both players know the starting conditions and are perfect mathematicians and logicians. On a side note: is this an old or new game? If it is a known old game, can anyone tell where its 'official' rules (and, maybe, solution(s)) are documented? remark (series details coming so warning : spoiler ahead) There is a YT video where rules are explained to be: if guesser guesses wrong, guesser must give amount 'h' that was hidden by hider, not amount 'b' bet by guesser. https://www.youtube.com/watch?v=GX4AkD_vdhw The video also gives the simple solution for that variant. And presenter also mentions rules are not clear, but, most people, he says, believe 'h'. Some comments claim otherwise. Fair enough. From examples in series it is not entirely clear to me either what the rule is. There is only one example where guesser guesses wrong. In that example, guesser bets b=2, and hider hides h=3, and guesser guesses odd. One can see the guesser give 2 (amount b bet), and, next it is shown guesser has only one more left. The guesser stops playing. If the rule were to give 3 (amount h hidden), guesser would immediately loose, and if the guards were paying attention, guesser would have been killed. If the rule were to give 2 (amount b bet) then guesser, upon becoming hider, would also loose. But, current guesser is a cheater so, in both cases (having to give b=2 or h=3), it fits him to hold that last marble. Note that the cheater is portrayed to be clever and his opponent to be dumb. And the opponent did not know the game. However, if rule were to have to give h(=3), then guesser would cheat hard in not holding on to the rules, and, be lucky to still be alive. After all, the outcome of each games in the series is said to be fatal, but fair. But, if rule were to have to give b(=2), then guesser would still cheat hard by stopping the game, and, even manipulating opponents marbles. A moderately clever guard paying attention would notice game was not played fair, and, a somewhat more clever guard paying attention would even know guesser lost in any case.","['nash-equilibrium', 'linear-algebra', 'game-theory', 'probability-theory', 'probability']"
4272429,prime counting function $\pi(x)$ is $o(x)$,"What's the simplest and/or shortest proof that $\lim_{x \to \infty} \frac{\pi(x)}{x} = 0$ , where $\pi(x)$ is the prime counting function?  I'm curious to see if there's a slick proof that is simpler and shorter than Chebyshev's proof that $\pi(x) \asymp \frac{x}{\log x} \ (x \to \infty)$ .  Of course, using the prime number theorem to prove it is cheating.  In particular, can it be proved using a simple idea like the sieve of Eratosthenes?","['number-theory', 'prime-numbers']"
4272479,A simple maximization problem in a convex set and the projection operator,"Let $H$ be a Hilbert space and let $g \in H$ be a fixed nonzero element. Consider the maximization problem $$\max \left\{ (f,g): f \in H \quad \text{and} \quad \|f\|\leq 1 \right\}$$ This maximum is equal to $\|g\|$ and a maximizer function is $f_0 = g/\|g\|$ . Suppose now $K \subset H$ is convex and closed. Consider the above maximization problem, but restricted to $K$ $$\max \left\{ (f,g): f \in K \quad \text{and} \quad \|f\|\leq 1 \right\} \tag{1}$$ My question is, what is the above maximum value and can we describe a maximizer using the projection operator $P:H \to K$ ? If not, under what general assumptions (on $K$ and $g$ ) can we do this? In special cases this is possible. For example suppose $K$ is a cone (or a subspace), then $g$ satisfies the condition $$(g-Pg,Pg) = 0 \tag{2}$$ Now by the characterization of the projection $(g-Pg,k-Pg) \leq 0$ for any $k \in K$ . Applying this to $k=f$ and using (2) gives $(g-Pg,f) \leq 0$ , hence $$(f,g) = (f,Pg) + (f, g-Pg) \leq (f,Pg)$$ So assuming in addition that $K$ is a cone, the maximum in (1) is bounded by $\|Pg\|$ and this value is in fact attained and the maximizer is $f_1 = Pg/\|Pg\|$ . More generally, what can be said when $K$ is just a closed and convex set? Any insight or references will be much appreciated!","['quadratic-programming', 'convex-optimization', 'hilbert-spaces', 'functional-analysis', 'convex-analysis']"
4272483,How to know if I have changed the graph of a function or not?,Case 1: $$\frac{2x^2}{4x^3}$$ $$\frac{1}{2x}$$ The graph hasn't changed. Case 2: $$\frac{6t^4+4t^2}{t}$$ $$6t^3+4t$$ The graph hasn't changed. Case 3: $$\frac{x^3-8}{x^2-4}$$ $$\frac{(x-2)(x^2+2x+4)}{(x+2)(x-2)}$$ $$\frac{(x^2+2x+4)}{(x+2)}$$ The graph hasn't changed. Question(s): How will I know that the graph will be the same after my operations? What things should I always avoid if I want to keep the function/graph the same?,"['functions', 'graphing-functions']"
4272634,Is the unitary group $U(H)$ with the strong operator topology locally compact?,"Suppose that $H$ is a complex Hilbert space. Endow the unitary group $U(H)$ with the strong operator topology (SOT) - that is, $u_{i}\to u$ in $U(H)$ if and only if $u_{i}x\to ux$ in $H$ for all $x\in H$ . One can show that $U(H)$ is a topological group. Is $U(H)$ locally compact? My intuition says that this is not true for general $H$ . However, this is true for $H=\mathbb{C}^{n}$ . In fact, if $H$ is finite dimensional, then $U(H)$ is even compact.","['unitary-matrices', 'operator-theory', 'topological-groups', 'functional-analysis', 'locally-compact-groups']"
4272654,An Intuition for Paracompactness,"I do have an intuitive understanding of compactness based on Euclidean space but not so much for paracompactness. Based on the Heine-Borel theorem, for a subset $S$ of Euclidean space $\mathbb{R}^n$ , the following two statements are equivalent: $S$ is closed and bounded. $S$ is compact. Which gives an intuitive idea of compatness. I would highly appreciate if someone gave a pictorial and an intuitive idea of paracompactness.","['paracompactness', 'intuition', 'general-topology', 'differential-topology', 'compactness']"
4272660,An ordering of $\mathbb{Q}(X)$ which makes it an Archimedean field.,"The following question is from one of my homework: Show that the field $\mathbb{Q}(X)$ , the field of rational functions over $\mathbb{Q}$ in one indeterminate $X$ , admits the structure of an ordered field having the Archimedean property. What can you say about $\mathbb{Q}(X, Y) ?$ My interpretation of the question is that it asks to define an valid ordering on $\mathbb{Q}(X)$ which has the Archimedean property. I am unable to find such an ordering. I could find some orderings on $\mathbb{Q}(X)$ (for example, to define $f>g$ if $f-g$ has both the numerator and denominator having positive leading coefficients) but all of those failed to satisfy the Archimedean property. Any help is appreciated. Thanks in advance.","['analysis', 'real-analysis', 'field-theory', 'polynomials', 'rational-functions']"
4272714,$\dim(W + U) = \dim(W) + \dim(U) - \dim(W \cap U)$ have a correlation with $|A\cup B|=|A|+|B|-|A\cap B|$ with the sets?,"In mathematics, the Grassmann formula is a relation concerning the dimension of the vector subspaces of a vector space or of the projective subspace of a projective space. We know that the enunciation Grassmann's formula is: Let $V$ a  vector space  on a  field $\Bbb K$ that have finite dimension. If $W$ and $U$ be two subspaces of $V$ with $$W + U := \{\mathbf{w}+\mathbf{u}, \mathbf{w} \in W, \mathbf{u} \in U\}$$ then $$\dim(W + U) = \dim(W) + \dim(U) - \dim(W \cap U) \tag 1$$ Obviously, if the sum is direct (I use the $\oplus$ symbol), then the intersection between the two subspaces consists only of the null vector ( $W \cap U=\mathbf{0}$ ), hence $$\dim(W \oplus U) = \dim(W) + \dim(U)$$ Now my question is indirectly for my 14-year old students but it is useful for me if there is a relationship with the Grassmann formula . If I have any two sets $A, B$ , it is very easy to verify with the examples that: $$\bbox[yellow,5px,border:2px solid red]{|A\cup B|=|A|+|B|-|A\cap B|} \tag 2$$ But the $(2)$ has a correlation with $(1)$ and how you can adapt it to get a suitable answer-explanation very simple with an example?","['linear-algebra', 'education', 'intuition', 'elementary-set-theory', 'soft-question']"
4272758,"Prove $1-\frac{1}{2} x^{2} \leq \cos x,\: \forall x \in \mathbb{R}$","Prove that $$1-\frac{1}{2} x^{2} \leq \cos x,\: \forall x \in \mathbb{R}$$ I am trying to prove this using taylor's theorem: We have: $$f(x)=f(0)+f'(0)x+\frac{x^2}{2!}f''(0)+...$$ Now approximating the function as a polynomial of degree $3$ we get: $$\cos x=1-\frac{x^2}{2!}+\frac{\sin c}{6}x^3$$ where $c \in (0, x)$ Case $1.$ if $0 < x \le \pi$ , then we have $\sin c \geq 0$ and $x^3 >0$ , so we get $$\cos x \geq 1-\frac{x^2}{2}$$ Case $2.$ If $\pi < x \leq 2\pi$ , i am confused, because $\sin c<0$ Any help?","['power-series', 'trigonometry', 'taylor-expansion', 'inequality']"
4272808,A matrix with $a_{ij}\equiv \delta_{ij}\pmod p$,"Let $p$ be an odd prime number. Let $A\in M_{n\times n}(\mathbb{Z})$ be a matrix satisfiying $a_{ij}\equiv \delta_{ij}\pmod{p}$ . Prove that: if $|\det(A)|=1$ and $A^m=I$ for some $m\in\mathbb{N}^+$ , then we have $A=I$ . I tried to write $A$ as $pX+I$ where $X\in M_{n\times n}(\mathbb{Z})$ . Then there is an annihilation polynomial of $X$ by $(pX+I)^m=I$ , and the problem turns to be proving $X=O$ . But I don't know how to complete this proof.","['matrices', 'linear-algebra']"
4272897,Why do the antiderivatives of $y=1$ follow $e^{x}$?,"If you integrate $y=1$ , you get $x$ . If you integrate that , you get $\frac{x^{2}}{2}$ , following the power rule. If you continue this, integrating over and over, the antiderivatives are: $1$ , $x$ , $\frac{x^{2}}{2}$ , $\frac{x^{3}}{6}$ , $\frac{x^{4}}{24}$ , and so on. It follows the pattern of $\frac{x^{n}}{n!}$ , which happens to be the infinite polynomial of $e^{x}$ . Just by integrating $y=1$ , you follow the $exp()$ function. Is there any intuitive way why this is?","['integration', 'indefinite-integrals', 'calculus', 'exponential-function']"
4272913,What do conjugate points mean in lagrangian mechanics?,"Good evening, my mathematical physics professor explained conjugate points to us in a way that just didn't make much sense to me, so I looked for a deeper more geometric definition online. I found some, but I'm having trouble understanding the connection between the ""geometric"" definition of conjugate points and the ""lagrangian"" one. I'll start by sharing my understanding of it in the geometric sense (which may well be wrong): Take a point $p$ on a riemannian manifold and a smooth family $\gamma_s(t)$ of geodesics passing through it, say at $t=0$ . Let $\gamma = \gamma_0.$ We define the Jacobi field associated to $\gamma$ as $$ J(t) = \left.\frac{d}{ds}\gamma_s(t)\right|_{s=0} $$ which intuitively describes the separation between the points on $\gamma$ and an infinitely close other geodesic in the family at time $t$ . From what I've read, an equivalent definition is to say that a Jacobi field is any solution of the equation $$ \frac{D^2}{dt^2}J + R(J, \dot \gamma)\dot \gamma = 0 \ \ \ (\star_1)$$ where $\frac{D^2}{dt^2}$ denotes the derivative along $\gamma$ and $R$ is the Riemann curvature tensor. A point $q = \gamma(\tau)$ is said to be a conjugate of $p$ if $J(\tau) = 0$ , meaning that our family ""merges"" into this point (at least in the limit). For example, any two opposite points of $S^2$ are conjugate. Now for the lagrangian definition: the setting is 1-d lagrangian mechanics, so we have some functions $\gamma, \eta: \Bbb{R} \to \Bbb{R}$ , an action functional $$ \mathcal{A}_\mathcal{L}(\gamma) \triangleq \int_{t_0}^{t_1} \mathcal{L}(\gamma(t), \dot\gamma(t), t)dt, $$ its first variation (let's call $\phi(\lambda) = \mathcal{A}_\mathcal{L}(\gamma+\lambda\eta) $ ) $$ \delta\mathcal{A}_\mathcal{L}(\gamma; \eta) \triangleq \phi'(0)$$ and its second variation $$ \delta^2\mathcal{A}_\mathcal{L}(\gamma; \eta) \triangleq \phi''(0).$$ It turns out that if we freeze $\gamma$ we can write $\delta^2\mathcal{A}_\mathcal{L}(\gamma; \eta)$ as an action functional $\mathcal{A}_\mathcal{Q}(\eta)$ ( $\mathcal{Q}$ is called ""auxiliary lagrangian""), so we can consider the Euler-Lagrange equation associated with it. Any solution to this equation is called a Jacobi field, and if we couple it with the initial conditions $$ \eta(\tau) = 0, \dot\eta(\tau) = 1$$ for some time $\tau$ , we have a unique solution $\tilde\eta(t)$ . Finally, for every solution $\tau_\star$ of $\tilde\eta(\tau_\star) = 0$ we say that $ (\tau, \gamma(\tau)) $ and $(\tau_\star, \gamma(\tau_\star))$ are conjugate points. For completeness, the mentioned Euler-Lagrange equation turns out to be $$ -\left(a \dot \eta\right)' + (c-\dot b)\eta=0 \ \ \ (\star_2)$$ where $$ a(t) = \mathcal{L}_{\dot\gamma\dot\gamma}(\gamma(t), \dot\gamma(t), t), b(t) = \mathcal{L}_{\gamma\dot\gamma}(\gamma(t), \dot\gamma(t), t), c(t) = \mathcal{L}_{\gamma\gamma}(\gamma(t), \dot\gamma(t), t). $$ Now, several questions: Do $\star_1$ and $\star_2$ correspond in some way? They do look sort of similar but I can't really figure out what $J$ and $R$ would be in the lagrangian formulation. It seems like $J$ would correspond to $\eta$ , but the substitution doesn't quite turn out right. What manifold am I considering in the lagrangian setting? What is the ""hidden geometry"" of this context? A while ago we defined the Jacobi metric in the mechanical context (where $L = T-V$ and $E = T+V$ is conserved) as $$ ds^2 = (E - V(q))\sum_{h, k}A_{hk}(q)dq_hdq_k $$ where $q_i$ are the lagrangian coordinates and $A$ is the kinetic matrix, but I can't really see the connection with what I described until now. I guess the manifold we are considering now, since we are in the 1-dimensional case, would be $(t, \gamma(t)) \in \Bbb{R} \times \Bbb{R}$ with the euclidean metric on the time coordinate and some metric dependent on the lagrangian on the space coordinate, but I can't really pull everything together. What do conjugate points represent in the mechanical context? Really, I can't figure this out. This auxiliary lagrangian doesn't seem to have some neat physical or geometric meaning to me, so I'm just a bit confused about this. The physics lesson looked like a nonsensical gibberish of derivatives to me, so any help is appreciated.","['riemannian-geometry', 'euler-lagrange-equation', 'functional-analysis', 'mathematical-physics', 'differential-geometry']"
4272928,Proving a Function $X \to \mathbb{R}$ Measurable,"Let $(X,\mathcal{R},\mu)$ be a measure space and let $f : X \to \mathbb{R}$ be a function. If $f^{-1}([a,\infty))$ is measurable for all $a \in \mathbb{R}$ , then $f$ is measurable. But say all I can show is that $f^{-1}([a,\infty))$ is measurable for almost all $a \in \mathbb{R}$ . Is that enough to prove that $f$ is measurable?","['measure-theory', 'measurable-functions', 'real-analysis']"
4272937,"Find a homomorphism $\phi$ from $U(30)$ to $U(30)$ such that $\ker(\phi) = \{1,11\}$ and $\phi(7) = 7$","Find a homomorphism $\phi$ from $U(30)$ to $U(30)$ such that $\ker(\phi) = \{1,11\}$ and $\phi(7) = 7$ . Note: I have seen this Homomorphism from U(30) to U(30) with a given kernel but I have more general questions about the First Isomorphism Theorem that I would like to address. This answer also feels case specific in that it uses the generators, and I am more interested in the general process. Note 2: We have not covered rings, only groups. I am having some confusion with the First Isomorphism Theorem, and I want to confirm/deny how I am supposed to interpret this. The statement given in the book is: Let $\phi$ be a homomorphism from $G$ to $\bar{G}$ . Then the mapping from $\frac{G}{\ker(\phi)} \to \phi(G)$ given by $g\ker(\phi) \to \phi(g)$ is an isomorphism. In symbols, $\frac{G}{\ker(\phi)} \approx \phi(G)$ Now, so far in the problems I've done, and the examples we did in class, $\phi(G) = \bar{G}$ has been used. As soon as I got to this problem though, this claim no longer seems to make sense. The kernel is non-trivial, so having $\frac{U(30)}{\{1,11\}} \approx U(30)$ makes no sense because the orders would not match up. What I am hoping to confirm is how to find $\phi(G)$ because no examples were done in class nor in the book. I would also like to know when it is appropriate to take $\phi(G) = \bar{G}$ so that I avoid this confusion again. It says the map is given by $g \ker(\phi) \to \phi(g)$ . I suppose what I would do is find this map $\forall g \in G$ . So, in $U(30) = \{1,7,11,13,17,19,23,29\}$ , I would have $\phi(1) = 1 * \{1,11\} = \{1,11\}$ $\phi(7) = 7 * \{1,11\} = \{7,77\} = \{7,17\}$ $\phi(11) = 11 * \{1,11\} = \{11,121\} = \{11,1\} = \phi(1)$ $\phi(13) = 13 * \{1,11\} = \{13,143\} = \{13,23\}$ $\phi(17) = 17 * \{1,11\} = \{17,187\} = \{17,7\} = \phi(7)$ $\phi(19) = 19 * \{1,11\} = \{19,209\} = \{19,29\}$ $\phi(23) = 23 * \{1,11\} = \{23,253\} = \{23,13\} = \phi(13)$ $\phi(29) = 29 * \{1,11\} = \{29,319\} = \{29,19\} = \phi(19)$ Then, $\phi(U(30)) = \{1,7,13,19\}$ Is this the correct process? If so, is there not an issue with the fact that the result is isomorphic to $\mathbb{Z}_4$ (i.e. isomorphic to a cyclic group)? From here though, I could claim that $\phi$ is the following: $\{1,11\} \to 1$ $\{7,17\} \to 7$ $\{13,23\} \to 13$ $\{19,29\} \to 19$ I am having a hard time believing this is indeed a homomorphism though. I feel like this maps to $\phi(U(30))$ , not $U(30)$ itself. Can someone convince me that this is an homomorphism? Thanks again for your help!","['group-homomorphism', 'group-isomorphism', 'finite-groups', 'abstract-algebra', 'group-theory']"
4272944,"L is a complete lattice, prove that there exists $x \in L$, which is a fixed point of $f$ and is the smallest $y$ that $f(y) \le y$.","I have such task: Let $(L, \le)$ be a complete lattice and $f: L \to L$ a monotonic function. Proof that there exists $x \in L$ which is a fixed point of $f$ and is the smallest $y$ that $f(y) \le y$ . I believe it has much to do with the Knaster–Tarski theorem . I think that by proving the Knaster–Tarski theorem I show that there in fact exists such element. But I have problems understanding the theorem's proof. Or perhaps there's a better and easier way to prove that. If anyone could explain that in simple words I would be very thankful.","['elementary-set-theory', 'logic', 'lattice-orders']"
4272945,Evaluating partial derivatives while using chain rule,"On p. 184 of ""Methods of Mathematical Physics, Vol. 1"" by Courant & Hilbert, they take the following integral (variation of a functional) $$
\Phi(\epsilon) = \int_{x_0}^{x_1} F(x, y + \epsilon \eta, y^{\prime} + \epsilon \eta^{\prime} ) \,dx
$$ and differentiate it with respect to $\epsilon$ under the integral. They then claim that $$
\Phi^{\prime} (0) = \int_{x_0}^{x_1} \left (\frac{\partial F}{\partial y} \eta + \frac{\partial F}{\partial y^{\prime}} \eta^{\prime} \right)\,dx
$$ If I understand correctly, they used the chain rule for total derivatives. In other words, if we label the arguments of the multi-variable function $F$ as $F=F(x,f,g)$ , where $f\left(y,\varepsilon,\eta\right) = y+\varepsilon\eta$ and $g\left(y^{\prime},\varepsilon,\eta^{\prime}\right) = y^{\prime}+\varepsilon\eta^{\prime}$ , then, using the chain rule, one can write $$
\frac{\partial}{\partial\varepsilon}F\left(x,f,g\right)=\frac{\partial F}{\partial f}\frac{\partial f}{\partial\varepsilon}+\frac{\partial F}{\partial g}\frac{\partial g}{\partial\varepsilon}=\frac{\partial F}{\partial f}\eta+\frac{\partial F}{\partial g}\eta^{\prime}
$$ Notice that $F$ is differentiated with respect to $f$ and $g$ , whereas in their case it's $y$ and $y^{\prime}$ . It seems that the claim is $$
\left.\frac{\partial F}{\partial f}\right|_{\varepsilon=0}=\frac{\partial F}{\partial f\left(y,\varepsilon=0,\eta\right)}=\frac{\partial F}{\partial y}
$$ (And similarly for $g$ ). But is there a rigorous proof that such evaluation is allowed? Because if we take the simple example of $F(x,f) = f$ and $f\left(y,\varepsilon,\eta\right) = \varepsilon$ , we get, on the one hand, $$\left.\frac{\partial F}{\partial f}\right|_{\varepsilon=0}=\frac{\partial F}{\partial f}=1$$ but on the other hand, notationally, $$\frac{\partial F}{\partial f\left(y,\varepsilon=0,\eta\right)}=\frac{\partial F}{\partial0}$$ which is a complete nonsense.","['multivariable-calculus', 'calculus']"
4272964,"Solve $z^2 + \bar{z} = \frac 1 2$, for $z \in \mathbb C$.",I want to solve the equation following in a set of complex numbers: $$z^2 + \bar z = \frac 1 2$$ My work so far Apparently I have a problem with transforming equation above into form that will be easy to solve. I tried to multiply sides by $z$ and use fact that: $z\bar z = |z|^2$ but it doesn't seem great idea. After that I tried the following: $$\bar z = \frac 1 2 - z^2 \Leftrightarrow |z| = | \frac 1 2 - z^2|$$ and then rewrite as $z = Re(z) +Im(z)$ but also result was not satisfying. Could you please give me a hand with solving this equation?,"['complex-analysis', 'complex-numbers']"
4272999,What are the visual properties that make a function linear?,"I am currently studying linear algebra and we came across the definition of a scalar-valued linear function which is a mapping from a vector to a scalar (real in our case). I also watched 3B1B's video on linear transformations (mappings) in which he introduced two key properties for explaining why a mapping was linear:  all lines in the grid space must remain lines and the origin must remain fixed in place. Is there a similar intuition about what properties scalar-valued functions need to have to be linear. I understand that they must exhibit superposition: $$f(\alpha x+\beta y)=\alpha f(x)+\beta f(y)$$ for all numbers $\alpha, \beta$ and all $n$ -vectors $x,y$ But what does this mean visually? Thanks!","['functions', 'linear-algebra', 'linear-transformations']"
4273056,A specific case of Le Cam's Theorem,"I was wondering if any of you folks could help me with this statistics problem. I need to show the two following inequalities: \begin{align}
R_n (H) &: = \underset{\hat{f}}{\inf}\underset{f\in H}{\sup} E_f [(\hat{f}(y_0) - f(y_0))^2] 
\\&\geq \frac{1}{4} \underset{f_0 , f_1 \in H}{\sup} (f_0(y_0) - f_1(y_0))^2 . \int_{R^n} \min(p_{f_0}^n(y) ,p_{f_1}^n(y))\,dy
\end{align} and $$\int_{R^n} \min(p_{f_0}^n(y) ,p_{f_1}^n(y))dy \geq \exp\left\{\frac{-1}{4\sigma^2}\sum_{i=1}^{n} (f_0(x_i) - f_1(x_i))^2 \right\}$$ where we observe $Y_1 , ..., Y_n$ avec $Y_i \sim N(f(x_i) , \sigma ^2)$ with $x_i = \frac{i-1/2}{n}$ . We note $\mathbf{Y}_n := (Y_1 , ..., Y_n)$ the observation and $p_{f}^n$ its density.
Here on assume that $\sigma ^2$ is known and $f:[0,1] \to R$ is unknown.
But for simplification we want just to estimate $f(y_0)$ avec $y_0 \in(0,1)$ .
Thank you for your help","['expected-value', 'statistics', 'probability-theory']"
4273135,Solving $\lim_{n→∞} \left( \frac{1^k+2^k+\cdot \cdot \cdot +n^k}{n^k}-\frac{n}{k+1} \right)$ where $k∈\mathbb{N}$ [duplicate],"This question already has answers here : Evaluating $\lim\limits_{n\to\infty} \left(\frac{1^p+2^p+3^p + \cdots + n^p}{n^p} - \frac{n}{p+1}\right)$ (7 answers) Closed 2 years ago . I had to find the limit of the sequence $$ a_n =\frac{1^k+2^k+\cdot \cdot \cdot +n^k}{n^k}-\frac{n}{k+1}$$ , where $ k $ is a natural number. After applying the Stolz theorem, I was able to get here. $$\lim \:_{n\to \:\infty \:}\left(\frac{\left(k+1\right)\left(n+1\right)^k-\left(\left(n+1\right)^{k+1}-n^{k+1}\right)}{\left(k+1\right)\left(\left(n+1\right)^k-n^k\right)}\right)$$ I tried different ways on continuing from here but unfortunately I am unable to get anywhere. I appreciate any kind of help. Edit: For users who may want a solution without the Big O notation, I was able to solve this limit by using the binomial theorem to calculate largest coefficient, since the numerator and denominator are polynomials in $n$ , as a user below suggested.","['limits', 'calculus', 'sequences-and-series']"
4273163,Simple random walk on cycle graph (Ending on specific vertex after cover time),"I'm considering a simple random walk on a cycle graph comprising a number of vertices, labelled $1$ to $5$ consecutively. Suppose I start at vertex 1 and can traverse to either side ( $2$ or $5$ ). I continue this random walk until I have covered all vertices. What is the probability that I finish on node $3$ , and the expected number of steps to get there? How do I calculate the same quantities if I finish on the other vertices? How do I generalise to say $n$ vertices? I have seen multiple resources discussing the cover time on a cycle graph, but in this context, I guess I have to include a discussion on either stopping time after visiting all vertices (which I don't really know how to include here, perhaps via a conditional probability) or transforming the end state as an absorbing state (which I guess won't make it a Markov chain anymore). I also suspect that for the first part (ending on node 3) we can exploit symmetry, but I'm not sure how. What should I do here? Thanks! Edit: after simulating this, I've noticed that the probabilities of ending at any node is $1/4$ . The expected number of steps terminating at $3$ (or $4$ ) should be $11$ , and the expected number of steps terminating at $2$ (or $5$ ) should be $9$ .","['conditional-probability', 'conditional-expectation', 'markov-chains', 'expected-value', 'probability']"
4273183,"Complete reducibility, in linear algebra versus topology. The example of the translation representation of $\mathbb R$ in $L^2({\mathbb{R}})$","On page 2 of these notes , the author provides an example of how a topologically reducible representation of a group can be lacking topological irreducibles. Before asking questions, I offer a simple introduction. INTRODUCTION Consider the representation of $\mathbb R$ on $L^2({\mathbb R})$ by translations $
 f(x)\;\longmapsto\;f(x\,+\,a)
 $ written as $$
 {\mathbb{R}} \ni a\;:\quad f\;\longmapsto\;f\circ\hat{\cal{R}}_a\;\;.
 $$ An ${L^2({\mathbb R})}$ function can be Fourier-expanded as $$
 f(x)\,=\int e^{\, 2\,\pi\, i\, t\, x}\, \hat{f}(t)\,dt\;\;.\qquad\qquad\qquad (*)
 $$ Since $\,f \in {L^2({\mathbb R})}\,\Longrightarrow\,\hat{\cal{R}}_a\, f \in {L^2({\mathbb R})}\,$ , we can expand also the function $f\circ\hat{\cal{R}}_a\, (x) =f(x+a)\,$ : $$
 f(x\,+\, a)\,=\,\int e^{\, 2\,\pi\, i\, t\, x}\, \hat{f}_a(t)\,dt\;\;.
 $$ On the other hand, the insertion of $\, x+a\,$ instead of $\, x\,$ in (*) renders us $$
 f(x+a)\,=\int e^{\, 2\,\pi\, i\, t\, x}\, e^{\, 2\,\pi\, i\, t\, a}\, \hat{f}(t)\,dt\;\;.
 $$ The former and the latter formulae, together, are indicating that the Fourier transform establishes an isomorphism between the translation representation of $\mathbb R$ on $L^2({\mathbb{R}})$ and the representation realised by multiplication by exponentials: $$
 f(x)\,\longmapsto\,f(x\,+\, a)\quad\Longleftrightarrow\quad \hat{f}(t)\,\longmapsto\,e^{\, 2\,\pi\, i\, t\, a}\, \hat{f}(t)\;\;,\qquad\qquad\qquad (**)
 $$ so the Fourier transforms of $f$ and $f\circ\hat{\cal{R}}_a$ have the same support. By Schur's lemma, an irreducible, if it exists, must be one-dimensional. This means that there is such a $t$ that (*) becomes $$
 f(x)\,= e^{\, 2\,\pi\, i\, t\, x}\, \hat{f}(t)\;\;.
 $$ while (**) becomes $$
 f(x+a)\,= e^{\, 2\,\pi\, i\, t\, x}\, e^{\, 2\,\pi\, i\, t\, a}\, \hat{f}(t)\;\;.
 $$ Combined, these two expressions yield: $$
 f(x+a)\,= \, e^{\, 2\,\pi\, i\, t\, a}\,f(x)\;\;.\qquad\qquad\qquad (***)
 $$ Such a function, however, will not be residing in $L^2({\mathbb{R }})$ .
Indeed, equality (***) enables us to write the squared norm $\,||\,f\,||^2\,=\,\int dx\,|f(x)|^2$ as $$
 ||\,f\,||^2\,=\,\int da\,|\,e^{\, 2\,\pi\, i\, t\, a}\,f(0)\,|^2\,=\,|\,f(0)\,|^2\,\int da\,|\,e^{\, 2\,\pi\, i\, t\, a}\,|^2\,=\;\infty\;\;.
 $$ So the representation of $\mathbb R$ on $L^2({\mathbb{R}})$ lacks topological irreducibles. At the same time, this representation is topologically reducible because it is said in these notes that, for any measurable set $S\subset\mathbb R$ , the vector space of functions $f$ for which $\hat{f}$ has support in $S$ gives an invariant closed subspace. QUESTION 1. Is there an elementary proof of the said fact that, for any measurable $S\subset\mathbb R$ , the space of functions $f$ for which $\hat{f}$ has support in $S$ is an invariant closed subspace? Since $f$ and $f\circ{\cal{R}}_a$ have the same support, the invariance holds, and the issue is closedness. To prove it, can we somehow use the fact that the Fourier transform is an isometry? QUESTION 2. Would it be right to say that the representation is completely reducible algebraically, but not topologically? To put it simply, can we say that the sole exponentials, not being topological irreducibles, still can be regarded as algebraic irreducibles? QUESTION 3. Below come two definitions of complete reducibility.  Equivalent in linear algebra and in finite-dimensional topological vector spaces, they are not necessarily equivalent in infinite dimensions. Consider a representation A(G) of a group G is a TVS (topological vector space). $\quad$ Def 1 $\quad$ A(G) is topologically completely reducible if it splits into a (finite or infinite) direct sum of topological irreducibles. $\quad$ Def 2 $\quad$ A(G) is topologically completely reducible if any of its topological subrepresentations has a complementary topological subrepresentation. The representation of $\mathbb{R}$ on $L^2({\mathbb{R}})$ evidently does not satisfy Def 1.
(This representation simply lacks topological irreducibles.) Since for any measurable $S\subset\mathbb R$ the space of functions $f$ for which $\hat{f}$ has support in $S$ is an invariant closed subspace, may we claim that the representation of $\mathbb{R}$ on $L^2({\mathbb{R}})$ satisfies Def 2  (and is completely reducible in the sense of this Def)?  What makes me doubt this is the presence of the word ` any ' in that definition. By considering all measurable $S\subset\mathbb R$ , will we count all topological subrepresentations?","['representation-theory', 'general-topology', 'fourier-analysis', 'fast-fourier-transform']"
4273241,How to obtain $ \frac{\mathrm{d}}{\mathrm{d} \mathbf{T}} (\mathbf{Z} \circ (\mathbf{T}\mathbf{X})) = \mathbf{X}^T \otimes Diag(\mathbf{Z}) $?,"$$
\frac{\mathrm{d}}{\mathrm{d} \mathbf{T}} (\mathbf{Z} \circ (\mathbf{T}\mathbf{X})) = \mathbf{X}^T \otimes Diag(\mathbf{Z})
$$ For $T \in \mathbb{R}^{K \times K}, Z,X \in \mathbb{R}^{K \times 1}$ I want to solve this derivative, however no matter what I found online, I just couldn't find a way to construct an equation with Kronecker product as shown above here $\circ$ means Hadamard product. Any helps is much appreciated!","['derivatives', 'linear-algebra', 'optimization']"
4273290,Glass Bridge Game in Squid Game (Episode 7/ Game 5),"Spoilers for Squid Game . In the show, there is a game where people try to cross a bridge, made of $n=18$ rows of 2 side by side glass panes, which they must cross one row at a time. One glass pane can support a person while the other  will break, causing the person to fall and get eliminated. Each person must select which glass pane to jump onto, from one row to the next, and try to reach the other side without falling through. In the show, some people cross the same bridge later than others, so they can tell which of the steps already crossed are sturdy or not. Assume that the selection of the sturdy and weak glass panes are random, that later players take the same steps that previous players took up to the point that they fall through (i.e. no forgetting which pane is sturdy or guessing again on an already solved pane). Ignore all human elements, like people trying to force others to fail to figure out future panes, or being able to tell the difference between tempered sturdy panes and weaker panes (i.e. guessing is random). Given $n$ rows of glass panes, how many players would it take until there is a player with a $>50\% $ chance of crossing the bridge? In the show there are 16 players and $n=18$ rows of glass, so what is the most likely outcome, in terms of number of people being able to cross the bridge?",['probability']
4273292,What is $\sqrt{-36}?$,"I am working on a math problem $(x^{2} - 6x + 45)$ where I must utilize the ""completing the square"" method, and was able to break down the problem to $(x-3)^{2} = -36.$ I know I have to take the square roots of both sides getting $x-3$ on the left, but for the right, I know 6 is one of the roots for the square root of -36 but what would be the other?  Initially, I thought it would be 6i but that is wrong. What is the other root?","['algebra-precalculus', 'radicals']"
4273331,Computing $\int_0^\infty \frac{dx}{(x^2 + 1)^2} $ with the residue theorem,"I am solving this integral and am stuck on proving an inequality, I believe I have the rest worked out. My work:
First observe that the integrand is even, hence $$\int_0^\infty \frac{dx}{(x^2+1)^2} = \frac{1}{2}\int_{-\infty}^\infty \frac{dx}{(x^2+1)^2}$$ We now consider the integral on the r.h.s. above and integrate over the upper half circle with radius $R$ and the segment $[-R, R]$ on the real axis. We let $R$ tend towards infinity. Denoting this region by $D$ , and the upper half circle by $C$ , by Cauchy's Residue Theorem we have, $$\int_{D} \frac{dz}{(z^2+1)^2} = 2\pi i \sum \text{Res} = \int_{-\infty}^\infty \frac{dx}{(x^2+1)^2} + \int_C \frac{dz}{(z^2+1)^2}$$ I first calculate the residues of the integrand in $D$ . Notice there is a pole of order 2 at $z = i$ . Hence, $$\text{Res}_{z = i} = \lim_{z \rightarrow i} \Bigl( \frac{(z-i)^2}{(z^2+1)^2} \Bigr)' = \frac{1}{4i}$$ Thus, $$\frac{\pi}{2} = \int_{-\infty}^\infty \frac{dx}{(x^2+1)^2} + \int_C \frac{dz}{(z^2+1)^2}$$ Now I would like to show that the complex integral on the r.h.s. tends to 0 as $R$ tends to infinity: $$\int_C \frac{dz}{(z^2+1)^2} \leq \Biggl \vert \int_C \frac{dz}{(z^2+1)^2} \Biggr \vert \leq \sup f(X) \cdot L = \sup f(X) \cdot \pi R$$ This is as far as I have gotten. I know I have to play with the triangle inequality somehow to bound the integrand from above and determine $\sup f(x)$ , but I cannot figure out how to do this. One thought I had was to use the reverse triangle inequality since: $$|(z^2+1)^2| = |z^4 +2z^2 + 1| \geq \bigl| |z^4| - |2z|^2 - 1\ \bigr|$$ so that, $$\frac{1}{(z^2+1)^2} \leq \frac{1}{\bigl| |z^4| - |2z|^2 - 1\ \bigr|} = \frac{1}{\bigl| R^4 - 2R^2 - 1\ \bigr|}$$ This will wrap things up as $$\sup f(x) \cdot R = \frac{\pi R}{\bigl| R^4 - 2R^2 - 1\ \bigr|}$$ which will tend to 0 as $R$ tends to infinity. Putting it all together and diving by two, $$\int_0^\infty \frac{dx}{(x^2 + 1)^2} = \frac{\pi}{4}$$ This is the correct numerical answer, but I am not sure if my use of the reverse triangle inequality is correct, can anyone confirm? Thanks!","['integration', 'improper-integrals', 'complex-analysis', 'calculus', 'contour-integration']"
4273341,"Does every Cauchy sequence converge to *something*, just possibly in a different space?","Question. If I attempt to prove that space $X$ is complete by pursuing the strategy, “Assume $x_n \rightarrow x$ ; the space $X$ is complete if $x \in X$ ,” then why is that wrong? Context. I know the definition of Cauchy sequences and convergent sequences, and that the definition of completeness is that Cauchy sequences in the space converge. And so I know that if one is attempting to prove that a space is complete, then the usual proof should start, “Assume that $x_n$ is a Cauchy sequence; we will show that $x_n$ converges in $X$ .” The misconception I seem to be battling is this: It seems like a Cauchy sequence must converge to something , just that the something might not be in the space. So it seems to me like the question really is, “Is the limit in the space or is it not in the space?” The classic example is the sequence of rationals that converges to $\sqrt{2}$ . The sequence is Cauchy within the space of the rationals, and also the sequence does converge , just to a limit that is outside the space in consideration. So recently, I began a proof of completeness with the line, “Assume $f_n \rightarrow f$ . We want to show that $f \in X$ .” And the feedback was, “Unclear what is being proved. Nothing related to completeness. 0/4 points.” It seems to me that showing that the limit of a convergent sequence resides in the space is equivalent to saying that Cauchy sequences converge. Why is that wrong? Thank you!","['cauchy-sequences', 'functional-analysis', 'analysis', 'real-analysis']"
4273374,Prove that $M=\{ x\in S^{n-1} \ | \ f(x)=0\}$ is an embedded submanifold of $S^{n-1}$.,"Let $n>1$ . Define $f:\mathbb{R}^n \to \mathbb{R}$ by $$f(x_1,…,x_n)=x_1^3+…+x_n^3$$ Prove that $$M=\{ x\in S^{n-1} \ | \ f(x)=0\}$$ is an embedded submanifold of $S^{n-1}$ . What is its dimension? $S^{n-1}$ is the unit sphere in $\mathbb{R^n}$ . My attempt: We have $M=f^{-1}(0)$ . The Jacobian matrix $[df_*]=\begin{pmatrix} 
3x_1^2 & … & 3x_n^2 
\end{pmatrix}$ Since $x\in S^{n-1}$ , we have $x_1^2+…+x_n^2=1$ , hence $x_1,…, x_n$ cannot be all zeros and then $[df_*]$ has a full rank. As a result, $M=f^{-1}(0)$ is an embedded submanifold of dim $1$ . Is it correct? Thank you!","['submanifold', 'smooth-manifolds', 'differential-geometry']"
4273402,What is a negative inversion?,"In a solution to the problem, IMO 2015 #3 Evan Chen and Telv Cohl
used the notation of negative inversion which looks like Homothety: Here , $\angle$$AML$ = $\angle ABL=\angle AKL$ . Also, $H, J$ are the midpoints of $BC, AC$ . $D, E, F$ are the feet of the perpendiculars drawn from $C, A, B$ to their opposite sides. $I$ is the orthocenter of this triangle. Now, if I perform a negative inversion , then what will happen to the circles passing through the center? also, what will happen to the lines not passing through the center? Can this inversion take the $\odot(ABC)$ to its nine-point circle? As the plain inversion around the circle with center $O$ sends point $P$ to $P'$ so that $OP$$\times$$OP'=r^2$ where r is the radius of the circle we are inverting around. How do actually negative inversion map the points?","['contest-math', 'recreational-mathematics', 'problem-solving', 'geometry']"
4273426,Closed-form of an integral,"I came across the following integral: $$
\int_0^{2\pi} \frac{\sin \theta \ d \theta}{(x-a \cos \theta)^2+(y-b \sin \theta)^2}$$ where $x,y$ are real variables independent of $\theta$ and $0<b<a$ . Now I was wondering if it could be written in a closed-form. I have been trying a number of different things but nothing seems to be working. Is there anyone how knows if this is even possible at all? And if so, would you be so kind to help me in the right direction? Any hint that gets me in the right direction is much appreciated.","['integration', 'trigonometric-integrals']"
4273468,Exponential decaying covariance of powers of random variables,"Let $X_1,X_2...$ be uniformly bounded random variables and $C_1,C_2>0$ such that $$\left\lvert \operatorname{Cov}(X_i,X_j)\right\rvert < C_1\cdot e^{-C_2|i-j|}$$ Does this imply exponential decay of $\operatorname{Cov}(X_i^2,X_j^2) $ as well?
(i.e., are there $C_3,C_4$ such that $$\left\lvert \operatorname{Cov}(X_i^2,X_j^2)\right\rvert < C_3\cdot e^{-C_4|i-j|})$$ In general this doesn't have to be true for $$\operatorname{Cov}(f(X_i),f(X_j)) $$ for any $f$ , but intuitively I would expect it for a polynomial (and specifically $f(x)=x^2$ ).","['probability-distributions', 'covariance', 'probability-theory', 'probability']"
4273606,What is the motivation behind the weak limit?,"We defined the weak limit as: Let $X$ be a Banach space. $x_n \in X$ converges
weakly to $x_0 \in X$ , if $\: \:\forall _{\phi \in X^{*}}\:\phi
 \left(x_n\right)\rightarrow \phi \left(x_0\right)$ But why are we even bothering to introduce the weak limit in the first place? What's the motivation and use behind that? I am in an Operator Theory course, and we only got a short introduction into functional analysis (we never had functional analysis before), and all I know is that it's important for the Riesz theorem (I assume) and Banch-Alaoglu theorem (due to weak topology) (which I also don't understand the use of it, especially regarding Operator Theory)","['operator-theory', 'weakly-cauchy-sequences', 'functional-analysis', 'weak-convergence']"
4273609,How to check singular cochains are not sheaves?,"Let $\mathcal{S}^{q}$ be a presheaf of singular cochain on a topological space $X$ , that is the functor $\mathcal{S}^{q} \colon \mathcal{O}(X)^{op} \to \mathbb{Z}$ - $\mathsf{mod}$ is given by $\mathcal{S}^{q}(U)= \operatorname{Hom}(S_{q}(U), \mathbb{Z})$ , where $S_q(U)$ is a singular chain on X.
This is a well known fact that these cochains are presheaves but not sheaves.
However, I cannot check this fact by myself. I am reading Warner's Foundations of Differential Manifolds and Lie Groups . Let $\{ U_{\alpha} \}_{\alpha \in A}$ be a covering of an open set $U \subset X$ .
I learned that a presheaf $\mathcal{F}\colon \mathcal{O}(X)^{op} \to \mathbb{Z}$ - $\mathsf{mod}$ is called a sheaf if following conditions 1 and 2 are satisfied. If there are $s, t \in \mathcal{F}(U)$ satisfing $\ s|_{U_{\alpha}} = t|_{U_{\alpha}}$ for arbitrary $\alpha \in A$ , then $s=t$ . If there is a family $\{f_{\alpha} \in \mathcal{F}(U_{\alpha}) \}_{\alpha \in A}$ satisfing $s_{\alpha}|_{U_{\alpha} \cap U_{\beta}} = s_{\beta}|_{U_{\alpha} \cap U_{\beta}} \ $ for $\ \alpha, \beta \in A, \ $ there exists a global section $f \in \mathcal{F}(U) $ such that $f|_{U_{\alpha}} = f_{\alpha}$ for $\alpha \in A$ . In this book of page 192 insist that the presheaf $\mathcal{S}^{q} \colon \mathcal{O}(X)^{op} \to \mathbb{Z}$ - $\mathsf{mod}$ ( for $q 
\ge 1$ )  satisfies later condition 2 only. I can not check this. How should I do to confirm these fact?(It is satisfied with 2 and not 1.)","['sheaf-theory', 'algebraic-topology', 'differential-geometry']"
4273647,Proof of strong law of large numbers for i.i.d Bernoulli,"I was able to come up with this 'proof' of the strong law of large numbers for Bernoulli random variables, but it seems far too simple and short compared standard proofs and haven't been able to find any sources which follow this sort of reasoning, so I'm very doubtful. However, I can't seem to find any mistakes in my reasoning, so I'd appreciate if someone could point out any. Let $\{ X_n \}_{n = 1}^\infty$ be a sequence of i.i.d. Bernoulli random variables with parameter $p$ . Let $Y$ be the random variable $$Y = \lim_{n \to \infty} \frac{1}{n} \sum_{i = 1}^n X_i$$ Since $\left| \frac{1}{n} \sum_{i = 1}^n X_i \right| \le 1$ , we can apply the dominated convergence theorem to get $$\textbf{E}[Y^2] = \textbf{E}\left[ \left( \lim_{n \to \infty} \frac{1}{n} \sum_{i = 1}^n X_i \right) \left( \lim_{n \to \infty} \frac{1}{n} \sum_{i = 1}^n X_i \right) \right] = \textbf{E}\left[ \lim_{n \to \infty} \frac{1}{n^2} \sum_{i = 1}^n \sum_{j = 1}^n X_i X_j \right] = \lim_{n \to \infty} \textbf{E}\left[ \frac{1}{n^2} \sum_{i = 1}^n \sum_{j = 1}^n X_i X_j \right] $$ Moving expectation into the finite sums, $$\textbf{E}[Y^2] = \lim_{n \to \infty} \frac{1}{n^2} \sum_{i = 1}^n \sum_{j = 1}^n \textbf{E}[X_i X_j] = \lim_{n \to \infty} \frac{1}{n^2} \sum_{i = 1}^n \sum_{j = 1, j \ne i}^n p^2 = \lim_{n \to \infty} \frac{(n^2 - n)p^2}{n^2} = p^2$$ The terms in the double sum where $i = j$ which would become $p$ rather than $p^2$ can be ignored since there are only $n$ of them and so are dominated by the $1/n^2$ . After showing that $\textbf{E}[Y] = p$ , $$\textbf{Var}[Y] = \textbf{E}[Y^2] - \textbf{E}[Y]^2 = p^2 - p^2 = 0$$ which implies that $Y$ is constant almost surely. Since $\textbf{E}[Y] = p$ , we have $Y = p$ almost surely so that $$\frac{1}{n} \sum_{i = 1}^n X_i \overset{a.s.}{\longrightarrow} p \text{ as $n \to \infty$}$$ Which is the strong law of large numbers. The only step I can think of which is possibly faulty is defining $Y$ as the limit of random variables, without knowing if this actually exists. If this is the case, I feel we could still use $\liminf$ or $\limsup$ and apply Fatou's lemma to get a two sided inequality with $p$ .","['convergence-divergence', 'law-of-large-numbers', 'probability-theory']"
4273657,Solving $\cos(2z) + (2-2i)\cos(z) = 2i-1$,"I'm trying to solve $$\cos(2z) + (2-2i)\cos(z) = 2i-1$$ when $z \in \mathbb{C}$ . My work so far I want to use formula $\cos(2z) = 2\cos^2(z) - 1$ to expand $\cos(2z)$ : $$2\cos^2(z) - 1 + (2-2i)\cos(z) = 2i-1$$ Let $t := \cos(z)$ $$2t^2 - 1 +2t - 2it -2i + 1 =0$$ $$2t^2 + 2t(1-i) - 2i =0$$ $$t^2 + t(1-i) - i = 0$$ $$\Delta = (1-i)^2 - 4 \cdot (-i) = (1-i)^2 + 4i = 1-2i-1+4i = 2i$$ $$\sqrt{\Delta} = \sqrt{2i}$$ So it turns out that $t = \frac{1 - i \pm \sqrt{2i}}{2}$ Expanding $t$ as $cos(z)$ and using fact that $cos(z) = \frac{e^{iz} + e^{-iz}}{2}$ we end up with: $$e^{iz} + e^{-iz} = 1-i\pm\sqrt{2i}$$ and here I got stuck, I'm not sure what should I do next. I tried to multiply by $e^{ix}$ but it doesn't seem to be a good idea. I tried to use fact that $\sqrt{i} = \cos(\pi / 4) + i \sin(\pi /4)$ but it also brought me nothing. Could you please help me solving this equation?","['complex-analysis', 'complex-numbers']"
4273746,"Continuous real function $f$ such that $f(a)<0,f(b)>0$ but with no ""switching point"" $c\in(a,b)$","Let $f:[a,b]\rightarrow\mathbb{R}$ be a continuous real function and assume $f(a)<0<f(b)$ . Does $f$ necessarily have a point $c\in (a,b)$ such that $f\leq 0$ on a left neighborhood of $c$ and $f\geq 0$ on a right neighborhood of $c$ (a ""switching point"", for that matter)? This is a seemingly simple question, but it took me longer than I expected to answer it. The answer turns out to be negative; however the counterexample I found it a bit ""exotic"" and I wondered if simpler examples can be found. My counterexample is based on the Cantor set , and it actually produces a smooth $f$ . First, for a closed interval $I$ , let $g_I$ be a smooth bump function whose support is $I$ . Specifically, $g_I(x)>0$ for $x\in I$ and $g_I(x)=0$ otherwise. Let $\mathcal{I}_n$ be the set of intervals removed at the $n$ -th stage of the construction of the Cantor set (of which there are $2^{n-1}$ ). Define $f:[0,1]\rightarrow\mathbb R$ by $$f(x)=\sum_{n=1}^\infty  \sum_{I\in\cal{I}_n}(-1)^ng_{I}(x).$$ Note that all the removed intervals are pairwise disjoint, so $f$ is well-defined. Now, it's not to hard to prove that: $f$ is smooth. The set $\left\{x:f(x)=0\right\}$ is the Cantor set $C$ . Therefore any $x\not\in C$ can't be a switching point of $f$ . For every $c\in C$ , there's a negative point and a positive point in every right and every left neighborhood of $c$ . Therefore $f$ has no switching points. Now it only remains to choose $0<a<b<1$ such that $f(a)<0<f(b)$ . This turned out to be more involved than I expected; are any simpler counterexamples, preferably ones that would appear more ""familiar"" to a standard Calculus course major?","['continuity', 'calculus', 'examples-counterexamples', 'real-analysis']"
4273783,Solve $\lim_{x\to \pi/4} \frac{\sin x - \cos x}{x-\pi/4}$ [duplicate],"This question already has answers here : Evaluate: $\lim_{\theta \to \frac {\pi}{4}}\frac {\cos \theta - \sin \theta}{\theta - \frac {\pi}{4}}$ (5 answers) Closed 2 years ago . As the title suggests, we have to solve the limit: $\lim_{x\to \frac\pi4} \frac{\sin x - \cos x}{x-\frac \pi4}$ I'm able to solve it by using L'Hospital's rule and got an answer $\sqrt2$ but the problem is that this rule is not allowed at school level. So I tried another method: $$\lim_{x\to \frac\pi4} \frac{\sin x - \cos x}{x-\frac \pi4}$$ $$\lim_{h\to 0} \frac{\sin(π/4+h) - \cos(π/4+h)}{h}$$ By using the identity of $\sin(a+b)$ and $\cos(a+b)$ , we get: $$\lim_{h\to 0} \frac{[\sin π/4+ \cos π/4][\cos h + \sin h]}{h}$$ If we here substitute $h=0$ , we get $√2/0$ . Can we solve it further?
Please help! BTW sorry for the bad formatting.","['limits', 'trigonometry', 'limits-without-lhopital']"
4273784,"Find area bounded by given curves. $y=\arcsin x,\;y=\arccos x,\,y=0 $","Find area bounded by given curves. $$y=\arcsin x,y=\arccos x,y=0 $$ So we need to find that shaded area.Intersection point is $\frac{\pi}{4}$ So we can write $\int_{0}^{\frac{\pi}{4}}\arcsin x + \int_{\frac{\pi}{4}}^{1} \arccos x$ and after calculation get $\frac{\pi}{4}(\arcsin(\frac{\pi}{4})-\arccos(\frac{\pi}{4}))$ but answer is $\sqrt2 - 1$","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'derivatives']"
4273792,Formal definition for conditional expectation of $\mathbb{E}[X\mid X>a]$,"I would like to know why is it true that for any random variable $X$ discrete, continues, or any other case with cumulative distribution function $F_X(x)$ it happens that, for example that the expectation of $X|X>a$ can be written as: $$
\mathbb{E}[X\mid X>a] =\frac{\int_{a}^\infty x\,dF_X(x)}{\mathbb{P}[X>a]}.
$$ And the general case, $X\in C\subset \mathbb{R}$ then $$
\mathbb{E}[X\mid X\in C] =\frac{\int_{C} x\,dF_X(x)}{\mathbb{P}[X\in C]}.
$$ I'd like to know a formal argument... but all I found is formal definitions such using random variables such as $\mathbb{E}[X|\mathcal{F}]$ , but not any explanation that why is this happening.","['measure-theory', 'probability-theory', 'probability']"
4273817,Taylor expansion of length of image of circle by Riemannian exponential map,"Let $(M, g)$ be a Riemannian manifold (without boundary) and let $p \in M$ . Consider a linear $2$ -plane $P$ in $T_p M$ and let $C_r$ be a circle in $P$ , of radius $r$ and centered at $0$ (for $r$ small enough). How can we prove that $$\mathrm{Length}(\mathrm{exp}_p(C_r)) = 2 \pi \left(r - \frac{\mathrm{sec}(P)}{6} r^3 + O(r^4) \right), $$ where $\mathrm{exp}_p$ is the Riemannian exponential map at $p$ and $\mathrm{sec}(P)$ is the sectional curvature of the plane $P$ ? I tried using the definition of length: the curve $\mathrm{exp}_p(C_r)$ can be parametrized as $$\gamma : [0,2\pi] \to M, \quad \gamma(t) = \mathrm{exp}_p (r\cos(t) v + r\sin(t) w), $$ where $\{v, w\}$ is an orthonormal basis for $P$ . Then, we know that $$\mathrm{sec}(P) = R_p(v, w, w, v), $$ where $R$ is the $(0, 4)$ version of the Riemann curvature tensor, and we have that, \begin{align}
\mathrm{Length} (\gamma) &= \int_0^{2\pi} |\dot{\gamma}(t)|dt \\
&= \int_0^{2\pi} \left| \left( d \left(\mathrm{exp}_p \right) \right)_{r\cos(t) v + r\sin(t) w} \left( -r\sin(t)v + r\cos(t)w \right)  \right| dt,
\end{align} but I am not sure how to continue further, as I do not exactly know how to compute the differential of the exponential map at those particular points. I thought about using the fact that, since $r$ is small enough, the image of $\gamma$ is contained in a chart of Riemannian normal coordinates, call them $(x^1, \cdots, x^n)$ . Then, in these coordinates, the components of the Riemannian metric satisfy $$g_{ij} = \delta_{ij} - \frac{1}{3} \sum_{k, l} R_{iklj}(p)x^k x^l + O(|x|^3), $$ where $R_{ikjl}$ are the components of the $(0,4)$ version of the Riemann curvature tensor (this is Exercise 10.1 in Lee's Introduction to Riemannian manifolds, 2nd edition). However, I do not know how to use this.","['curvature', 'riemannian-geometry', 'differential-geometry']"
4273845,Closed-form for $B=\lim_{n\to\infty}\sum_{a_1=1}^{\infty}\frac{1}{a_1^2}\sum_{a_2=1}^{a_1}\frac{1}{a_2^2}\cdots\sum_{a_n=1}^{a_{n-1}}\frac{1}{a_n^2}$?,"Introduction This problem came to my mind few years ago when I first learned about limits and infinite sums. I saw sums, double sums, triple sums etc, but never an infinite sum basically an infinite chain of summation symbols which will converge to a real value. So I constructed the expression below, My Question Is there a closed form of the following number- $\textstyle\displaystyle{B=\lim_{n\rightarrow\infty}\sum_{a_1=1}^{\infty}\frac{1}{a_1^2}\sum_{a_2=1}^{a_1}\frac{1}{a_2^2}\cdots\sum_{a_n=1}^{a_{n-1}}\frac{1}{a_n^2}}$ Or some more easier representations not involving this infinite chain of sigma symbols with only finite amount of indices? Does it even converge? Details And Observation Let's first define a double sequence for which will converge to $B$ . $\textstyle\displaystyle{\Sigma_{m,n}=\sum_{a_1=1}^{n}\frac{1}{a_1^2}\sum_{a_2=1}^{a_1}\frac{1}{a_2^2}\cdots\sum_{a_m=1}^{a_{m-1}}\frac{1}{a_m^2}}$ So, $B=\Sigma_{\infty,\infty}$ A list of a few terms would be $\Sigma_{1,\infty}$ $\textstyle\displaystyle{=\frac{\pi^2}{6}}$ $\Sigma_{2,\infty}$ $\textstyle\displaystyle{=\frac{7\pi^4}{360}}$ $\Sigma_{3,100}$ $\textstyle\displaystyle{=1.95233691...}$ $\Sigma_{4,36}$ $\textstyle\displaystyle{=1.93916539...}$ $\Sigma_{5,28}$ $\textstyle\displaystyle{=1.92936096...}$ $\vdots$ I can't get more values for this, Wolfram Alpha doesn't work. Notice that $\Sigma_{m,n}$ satisfies a recurrence relation- $\textstyle\displaystyle{\Sigma_{m,n}=\sum_{k=1}^{n}\frac{\Sigma_{m-1,k}}{k^2}}$ Where, $\Sigma_{0,n}=1$ This recurrence relation is very similar to Hyperharmonic numbers , it just has a factor of $k^{-2}$ within the sum and the initial value is different. The hyperharmonic numbers satisfies $\textstyle\displaystyle{H_n^{(r)}=\sum_{k=1}^{n}H_k^{(r-1)}}$ Beyond this observation I am clueless.","['limits', 'summation', 'recreational-mathematics', 'sequences-and-series']"
4273858,Formal definition of a conditional distribution function by Markov kernels,"Suppose $X,Y$ are two real valued random variables on $(\Omega,F,P)$ , i.e. measurable maps $X:\Omega\to\mathbb R$ and $Y:\Omega\to\mathbb R$ . I am interested in defining a conditional distribution function $F_{Y\mid X=x}(y)$ Since $\mathbb R$ is polish, we can define the regular conditional distribution of $Y$ given $X=x$ by a Markov kernel $k_{Y,X}(x,A)$ where $P(Y\in A\mid X=x)=k_{Y,X}(x,A)$ for almost all $x\in\mathbb R$ and  for all $A\in \mathbb R$ .  Now I would like to define $F_{Y\mid X=x}(y):=k_{Y,X}(x,(-\infty,y])$ and call this a conditional distribution function. But I am curious if this makes any sense since $P(Y\in A\mid X=x)=k_{Y,X}(x,A)$ is just defined almost everywhere. So my question is, if it is possible to define a conditional distribution function in this setting? Does my definition makes sense? If so could someone elaborate on this?","['conditional-probability', 'conditional-expectation', 'probability-distributions', 'probability-theory']"
4273869,Show that $\frac{dP}{d\nu} = \sum_{n=1}^{\infty} a_n \frac{d P_n}{d \nu}$?,"Let $P=\sum_{n=1}^{\infty} a_n P_n$ be a probability measure where $a_n >0$ , $\{P_n\}$ be a sequence of probability measures and $\sum_{n=1}^{\infty} a_n=1$ . If $P \ll \nu$ where $\nu$ is a sigma-finite measure, is it possible to show that $\frac{dP}{d\nu} = \sum_{n=1}^{\infty} a_n \frac{d P_n}{d \nu}$ ? I know that by the Radon–Nikodym theorem, the last statement is true for a finite sum but I am not really sure how to prove this infinite sum.","['measure-theory', 'lebesgue-measure', 'probability-theory', 'probability', 'radon-nikodym']"
4273954,Dandelin's proof of Pascal's theorem,"In the Wikipedia page of Pascal's theorem it is said with no citation that Dandelin, the geometer who discovered the celebrated Dandelin spheres, came up with a beautiful proof using ""3D lifting"" technique that is analogous to the 3D proof of Desargues' theorem. The proof makes use of the property that for every conic section we can find a one-sheet hyperboloid which passes through the conic. I managed find the source of this, that is, Dandelin's paper ""Hyperboloids of revolution
and the hexagons of Pascal and Brianchon"", translation available for example here . However, it seems that the paper in question and its structure and language are rather old and partly due to that I find it very hard to follow. I am familiar with the basics of hyperboloids but I am not quite sure what Dandelin's proof of Pascal's theorem in section 16 is trying to convey. Taking that into account, could anyone help me understand the proof better? Maybe rephrase it or guide me in the right path? Thank you in advance.","['proof-explanation', 'conic-sections', 'geometry']"
4274024,Why is $2\sin{(akx)}\sin{(bkx)}=\cos{((a-b)kx)}-\cos{((a+b)kx)}$?,"I am learning introductory quantum mechanics from the Introduction to Quantum Mechanics by David J. Griffiths. I stumbled upon this proof for orthogonality of two different solutions regarding the infinite well problem on the page 33: I have some problems understanding the passage from the first line to the second line. Why is $$\frac{2}{a}\sin{\left(\frac{m\pi}{a}x\right)}\sin{\left(\frac{n\pi}{a}x\right)}=\frac 1 a \cos{\left(\frac{m-n}{a}\pi x\right)}-\cos{\left(\frac{m+n}{a}\pi x\right)}$$ true? I haven't yet found any way to solve it using trigonometric identities. There are lots of physics variables here, but this is Mathematics Stack Exchange. That's why I can ask it in the generalised way: In general, why is $$2\sin{(akx)}\sin{(bkx)}=\cos{((a-b)kx)}-\cos{((a+b)kx)}$$ true?","['quantum-mechanics', 'trigonometry']"
4274029,Sum of normal random variables being not normal,"If X and Y are identically distributed as $N(0,1)$ (but not independent) with covariance $cov(X, Y)=\frac{1}{2}$ (and these are the only constraints). Is it possible to construct X and Y such that X+Y is not normally distributed?","['statistics', 'normal-distribution']"
4274037,Do distance-preserving maps from $\mathbb R^2 \rightarrow \mathbb R$ exist?,"So, I know that it's 'impossible' to have a perfectly bijective map $F:\mathbb R^2 \rightarrow \mathbb R$ , but I was wondering nevertheless: what would the 'best' possible map be, that is closest to being bijective? Additionally, can you make $F$ 'preserve distance' in some sense - and if so, what would the best form of $F$ be to do that? To clarify, if you had $n$ points in 2-D space, $F$ would ensure that points clustered close together stay relatively close. Apologies if this is too vague a question! Edit: Maybe I should make it clearer what I mean by 'close' since I didn't define it very well. I was thinking about collision detection, which is how I initially stumbled on this question. In collision detection, if you have two (circular) objects $A$ and $B$ , that have co-ords $(x_1,y_1), (x_2,y_2)$ , they collide if the sum of the distances between them is less than the sum of their radii. Practically though, it's computationally really expensive to do this check if you have a very large number of objects. In that case, you'd want to have a rough idea of the neighbourhood of each object, and only check for collisions within that neighbourhood -  some defined circular region of radius $r$ . If you had a map from $\mathbb R^2 \rightarrow \mathbb R$ (albeit a slightly imperfect, discontinuous map), then it'd be really easy to figure out what the neighbourhood is. Hopefully that makes things clearer!","['elementary-number-theory', 'general-topology', 'geometry', 'real-analysis']"
4274056,Counterexample to the uniform convergence of a differentiable function sequence,"I'm struggling trying to find a real function sequence $\{f_n\}_n$ such that $\forall n: f_n$ is defined on an open and limited interval $(a,b)$ ; $\forall n: f_n$ is everywhere differentiable (wrt $x$ ) on $(a,b)$ ; $\exists x_0\in (a,b)$ such that $\{f_n(x_0)\}_n$ converges; $\{f'_n\}_n$ is uniformly convergent on $(a,b)$ ; $\{f_n\}_n$ does not converges uniformly on $(a,b)$ . Now, conditions 2), 3), 4) are sufficient to guarantee pointwise convergence of $\{f_n\}_n$ on $(a,b)$ , let's say to a function $f:(a,b)\to\mathbb{R}$ , and the chance to interchange limit with differentiation in the sense that $$\forall x\in (a,b): f'(x)=\lim_n f'_n(x).$$ Moreover, under those same supposititions, we can have uniform convergence on every compact subinterval of $(a,b)$ . My goal is to find a function sequence that satisfies together all those five conditions: I suppose that the problem must come from openess of the functions' domain that messes up with the uniform convergence of the $\{f_n\}_n$ : unfortunately, all my attempts failed so I'm here to ask you some help to find a sequence like that. Any ideas?","['derivatives', 'metric-spaces', 'uniform-convergence', 'real-analysis']"
4274118,Solve $A=\text{adj}(A)$ in $\mathbf{M}_n(\mathbb{R})$.,"Solve the equation $A=\text{adj}(A)$ in $\mathbf{M}_n(\mathbb{R})$ . Where $\text{adj}(A)$ is the adjugate of $A$ (tranpose of cofactor matrix). Here is what I know: $A^2=\det(A)I_n$ $\text{rank(A)} \lt n-1 \implies \text{rank}(\text{adj}(A)) = 0$ Here is what I have done so far: We have 3 cases: If $det(A) = 0$ Then $A^2=0$ , thus $\text{rank}(A)\leq2-1=1$ . This implies $\text{rank(A)} = \text{rank}(\text{adj}(A)) \leq 1$ So $\text{rank}(A) = 0$ , that is, $A=0$ If $det(A) > 0$ Then $\left(X-\sqrt{\det A}\right)\left(X+\sqrt{\det A}\right)$ has $A$ as a root. Thus $A$ is diagonalizable (since its minimal polynomial has simple roots) and its eigenvalues are a subset of $\{\sqrt{\det A}, -\sqrt{\det A} \}$ . Thus $A=P^{-1}
\begin{bmatrix}
\pm\sqrt{\det A} & &\\
& \ddots &\\
& & \pm\sqrt{\det A}
\end{bmatrix}
P$ Which we can verify as a solution to our equation. If $det(A) < 0$ Then $A^2=\det(A)I_n$ . Thus $B^2=-I_n$ with $B=\frac{1}{\sqrt{-\det A}}A$ How do I solve for $B$ ?","['matrices', 'determinant', 'linear-algebra']"
4274129,Finding the sum of $\cos{x}+2\cos{2x}+...+n\cos{nx}$ [duplicate],"This question already has answers here : Summing $ \sum _{k=1}^{n} k\cos(k\theta) $ and $ \sum _{k=1}^{n} k\sin(k\theta) $ (3 answers) Closed 2 years ago . I'm struggling to find the sum of $S_n=\cos{x}+2\cos{2x}+3\cos{3x}...+n\cos{nx}$ I know that for $z=e^{ix}$ , $2\cos{nx}=z^n+\frac{1}{z^n}$ . So I've tried $2S_n=(z+2z^2+3z^3+...+nz^n)+(\frac{1}{z}+\frac{2}{z^2}+\frac{3}{z^3}+...+\frac{n}{z^n})$ but I don't know how to find the sum of $\sum_{r=1}^nrz^r$ or $\sum_{r=1}^nrz^{-r}$ Is there a way to find that sum? Is the way I'm doing it going in the right direction? Edit for the answer using Thomas Andrews' comment $S_n=\frac{d}{dx}\frac{\sin{\frac{n+1}{2}x}\sin{\frac{n}{2}x}}{\sin{\frac{x}{2}}}$ $=\frac{\sin{\frac{x}{2}}(a\sin{bx}\cos{ax}+b\sin{ax}\cos{bx})-\sin{ax}\sin{bx}\cos{\frac{x}{2}}}{\sin^{2}{\frac{x}{2}}}$ where $a=\frac{n+1}{2}$ and $b=\frac{n}{2}$","['trigonometry', 'summation', 'complex-numbers']"
4274182,Prove $3^n > 2^n + n^2$ by Induction,"Prove that $3^n > 2^n + n^2; \forall n \ge 2$ using induction. Base Case: $n=2$ $$3^2 > 2^2 + 2^2 \implies 9 > 8$$ Inductive Step: $$\text{If } 3^k > 2^k + k^2; \forall n \ge 2 \text{ Then } 3^{k+1} > 2^{k+1} + (k+1)^2$$ So, $$3^{k+1} = 3*3^k > 3(2^k + k^2) \text{ // By our inductive hypothesis}$$ $$= 3*2^k + 3k^2  > 2*2^k + 3k^2$$ $$=2^{k+1} + 3k^2 > 2^{k+1} + (k+1)^2 \text{ // How to show?}$$ My Question: I feel like I know intuitively that $3k^2 > (k+1)^2$ ; $\forall n \ge 2$ but is there a better way to show this? Or a better way to conclude my proof in general? Thanks for any help the community can give. :)","['induction', 'proof-writing', 'solution-verification', 'discrete-mathematics']"
4274266,Isn't my book wrongly equating $\frac{\frac{\sin^2x-\cos^2x}{\sin x\cos x}}{\frac{\sin^2x+\cos^2x}{\sin x\cos x}}$ and $-\cos2x$?,"Problem: Differentiate with respect to x: $\frac{\tan x-\cot x}{\tan x+\cot x}$ My book's solution: $$\begin{align}
\frac{\tan x-\cot x}{\tan x+\cot x} &=\frac{\dfrac{\sin x}{\cos x}-\dfrac{\cos x}{\sin x}}{\dfrac{\sin x}{\cos x}+\dfrac{\cos x}{\sin x}} \\[0.5em]
&=\frac{\dfrac{\sin^2x-\cos^2x}{\sin x\cos x}}{\dfrac{\sin^2x+\cos^2x}{\sin x\cos x}}\tag{1}\\[0.5em]
&=\frac{\sin^2x-\cos^2x}{\sin^2x+\cos^2x}\tag{2}\\[0.5em]
&=-\cos 2x
\end{align}$$ Now, $$\frac{d}{dx}(-\cos 2x)=2\sin 2x$$ Question: $(1)$ and $(2)$ are different because their domains are different. So, isn't saying $(1)=(2)$ wrong?","['calculus', 'functions', 'derivatives', 'trigonometry']"
4274284,Finding all $ f: \mathbb{R} \to \mathbb{R}$ such that $ f(x+f(y))=f(x)+2xy^2+y^2f(y) $,"$$
f: \mathbb{R} \to \mathbb{R}, f(x+f(y))=f(x) + 2xy^2 + y^2f(y)
$$ How can we solve this problem? This is my try, but can't go more. $$
P(x, y): f(x+f(y))=f(x) + 2xy^2 + y^2f(y) \\
P(0, 0): f(f(0))=f(0) \\
let \ f(0) = a \Rightarrow f(a)=a. \\
P(a, a): f(2a)=a+2a^3+a^3 \\
P(a, 0): f(2a)=a \\
\Rightarrow 3a^3=0 \Rightarrow a = 0, f(0) = 0. \\
\ \\
Assume) \ \exists \ t \ s.t. \ t \ \neq 0, \ f(t)=0. \\
P(t, t): 0 = 2t^3 \Rightarrow t = 0, \text{Contradiction.} \\
\therefore f(t)=0 \Leftrightarrow t = 0.
\ \\
P(0, y): f(f(y))=y^2f(y). \\
P(f(x), x): f(2f(x))=f(f(x))+2f(x) \cdot x^2 + x^2f(x) = 4f(f(x))=4x^2\cdot f(x) \\
P(2f(x), x): f(3f(x))=f(2f(x))+4f(x)\cdot x^2 + x^2f(x) = 9f(f(x)) = 9x^2 \cdot f(x) \\
\cdot \\
\cdot \\
\cdot \\
f(nf(x))=n^2f(f(x))=n^2x^2f(x) \ for \ \forall n \in \mathbb{N}. \\
\ \\
P(-f(x), x): f(0)=f(-f(x))-2f(x) \cdot x^2 + x^2f(x) \Rightarrow f(-f(x)) = x^2f(x) = f(f(x)) \\
P(-2f(x), x): f(-f(x))=f(-2f(x))-4f(x)\cdot x^2 + x^2f(x) \Rightarrow f(-2f(x))=4x^2f(x)=4f(f(x)) \\
\cdot \\
\cdot \\
\cdot \\
f(mf(x))=m^2f(f(x))=m^2x^2f(x) \ for \ \forall m \in \mathbb{Z}. \\
$$","['functional-equations', 'functions']"
4274311,Density of a random vector [duplicate],"This question already has an answer here : Joint density of uniform distribution and maximum of two uniform distributions. (1 answer) Closed 2 years ago . Let $X,Y \sim U[0,1]$ be two independent uniformly distributed on $[0,1]$ random variables. Let $Z := \max(X,Y)$ . I'm interested in the probability density function of the vector $(X,Z)^T$ . The CDF of $(X,Z)^T$ is \begin{align*}
F_{X,Z}(x,z) &= \mathbb{P}(X \leq x, \max(X,Y) \leq z) = \mathbb{P}(X \leq x, X \leq z, Y \leq z) = \\
&= xz I\{0 \leq x \leq z \leq 1\} + z^2 I\{0 \leq z \leq 1, z \leq x\} + x I\{0\leq x \leq 1, 1 < z\} + I\{1 \leq x, 1 \leq z\}
\end{align*} Now $p_{X,Z}(x,z) = \frac{\partial^2 F_{X,Z}(x,z)}{\partial x \partial z} = I\{0 \leq x \leq z \leq 1\}$ . But $\int_{\mathbb{R}^2}I\{0 \leq x \leq z \leq 1\} dxdz = \frac{1}{2} \neq 1$ The setting of the problem is very simple but I just don't understand where is the mistake. Thank you in advance.","['probability-distributions', 'probability-theory', 'probability']"
4274312,Conditioned Normal Distributions and joint PDF's,"Let X, Y, Z be r.v.s such that X ⇠ N (0, 1) and conditional on X = x, Y and Z are i.i.d. N (x, 1). (a) Find the joint PDF of X, Y, Z. (b) Find the joint PDF of Y and Z. You can leave your answer as an integral, though
the integral can be done with some algebra (such as completing the square) and facts
about the Normal distribution. Is my reasoning correct? I did the following: a) $f(X=x,Y=y,Z=z)=f(X=x)f(Y=y|X=x)f(Z=z|X=x,Y=y)=f(X=x)f(Y=y|X=x)f(Z=z|X=x)=$ $$\frac{1}{2\sqrt \pi} e^{-x^2/2} \,  \frac{1}{2\sqrt \pi} e^{-(y-x)^2/2} \,   \frac{1}{2\sqrt \pi} e^{-(z-x)^2/2} \,$$ b) $$f(Y,Z)=\int_{-\infty}^{\infty}( \frac{1}{2\sqrt \pi} e^{-x^2/2} \,   \frac{1}{2\sqrt \pi} e^{-(y-x)^2/2} \,  \frac{1}{2\sqrt \pi} e^{-(z-x)^2/2} \, dx)$$","['conditional-probability', 'statistics', 'probability', 'density-function']"
4274318,The cluster point set of $\{\frac{1}{n}\sum_{k=0}^{n-1} \delta_{T^kx}\}$ is connected,"Question: Let $X$ be a compact metric space and $T:X\to X$ is a continuous map. Arbitrarily fixed $x\in X$ , let $\delta_{T^kx}$ denote the Dirac measure mass at $T^kx$ . Prove The cluster  set of the sequence $\{\frac{1}{n}\sum_{K=0}^{n-1} \delta_{T^kx} \in C(X)^*:n\in \Bbb{N}_{+}\}$ is connected, for the weak-star topology. My observation: Denote its cluster set by $E$ . By Banach-Alaogu theorem, $E$ is a nonempty compact set, and every measure in $E$ is $T$ -invariant. But it seems not necessarily being convex. To prove the connection. Suppose $H: E\to \{0,1\}$ be a continuous map, it enough to prove $H$ is constant. But I don’t know how to make $H$ be specific. Another observation is that every nonempty open set of $w^*$ -topology is quit big, by definition which contains an finite codimensional subspace. I also consider some simple examples. Let $X=\mathbb{T}$ , with rotation transform, in this case $E$ contains one element-the Lebesgue measure. If $X=[0,1]$ , $Tx=x^2$ , $E=\{\delta_0\}$ when $x\neq 1$ , and $E=\{\delta_1\}$ otherwise. Unfortunately such exampleS don’t give me some useful information because $\#E=1$ .","['measure-theory', 'weak-topology', 'functional-analysis', 'dynamical-systems']"
4274321,Unbiased Estimator of $\sigma^2$ using Least square estimates,"Suppose I have $Y_i \sim N(\beta_i ,\sigma^2)$ for $i=1,2,3$ . It is given that $\beta_1 + \beta_2= \beta_3$ and $\hat{\beta}_1 = \frac{2Y_1 - Y_2 +Y_3}{3} , \hat{\beta}_2 = \frac{2Y_2-Y_1+Y_3}{3},\hat{\beta}_3= \frac{Y_1+Y_2+2Y_3}{3}$ where $\hat{\beta}_i$ 's are the least square estimates of $\beta_i$ 's. I am trying to find an unbiased estimator for $\sigma^2$ using the Least squares estimates, but I can't guess how should I combine the $\hat{\beta}_i $ 's?","['statistical-inference', 'statistics', 'estimation', 'regression', 'least-squares']"
4274355,Proof for the given identity of $\frac{1}{1-az-bz^2}$,"I know that for $a,b\in \mathbb{R}$ we have the formula $\frac{1}{1-az-bz^2}=\sum_{n=0}^{\infty}\sum_{k=0}^{\lfloor n/2\rfloor}\binom{n-k}{k}a^{n-2k}b^{k}z^n$ However, I am unsure how to derive it. I'm sure that there exists some identity with regards to deriving the given formula, I just can't seem to find it. I know that using the Taylor series we could write the expression $\frac{1}{1-az-bz^2}$ for example as $\frac{1}{1-az-bz^2}=\sum_{n=0}^{\infty}\frac{a^n\Big(\frac{z}{1-bz^2}\Big)^{n+1}}{z}$ when $|b-1|<\Big| \frac{-1+az+z^2}{z^2}\Big|$ but I am unable to proceed from this point and this is all I could think of.","['number-theory', 'calculus', 'binomial-theorem']"
4274384,Probability of a random distribution being in a set of distributions,"Let $S=\{s_1,\dots,s_k\}$ be some set of size $k$ and let $\mathit{Dist}(S)$ be the set of all probability distributions over $S$ where a probability distribution is simply a function $p \colon S \rightarrow [0,1]$ with $\sum_{s\in S}p(s) = 1$ . We construct a random probability distribution $t$ over $S=\{s_1,\dots,s_k\}$ in the following way: generate a multiset $R$ of $k-1$ random variables each sampled uniformly from $[0,1]$ sort them ascending and label them $n_1$ to $n_k$ , i.e., $\hat{\{}n_i\mid i\in \{1,\dots,k-1\}\hat{\}}=R$ and $n_1\leq n_2 \leq \dots \leq n_{k-1}$ define $n_0=0$ and $n_k=1$ for each $i$ we define $t(s_i)=n_i-n_{i-1}$ This can be visualised as taking the number line from 0 to 1, split it into $k$ parts by dropping $k-1$ separators randomly and asserting each probability a value that corresponds to the distance between two adjacent separators. Note that $t$ is indeed a probability distribution since $n_i\geq n_{i-1}$ and $\sum_{s\in S}t(s) = 1$ . Given a set of distributions $T \subseteq Dist(S)$ , what is the probability that $t \in T$ ? If it helps, we can assume that $T$ gives a dense range of possible values for the probability of each element of $S$ , i.e., for each $i \in \{1,\dots,k\}$ there are some $\underline{p_i},\overline{p_i}\in [0,1]$ with $\underline{p_i} < \overline{p_i}$ and $T = \{ p \mid p \in \mathit{Dist}(S) \text{ and } \forall i.p(s_i)\in [\underline{p_i},\overline{p_i}] \}$ . For $k=2$ this is straight forward since $p(s_1) = 1-p(s_2)$ , i.e., it is essentially just one random variable. However even extending this to a set of three elements already gives me troubles. The main issue I keep running into is that I know the $p(s_i)$ are not independent (since they have to add to 1). I'm not looking for a closed form necessarily. A way to compute this algorithmically would be perfectly fine.","['probability-distributions', 'probability-theory', 'probability']"
4274404,Solve real ode $\ddot x + x = 0$ by introducing $i$?,"It is said that we can solve differential equ $\ddot x + x = 0$ by writing it as $(d/dt + i) (d/dt - i)x=0$ . Why can we do this?
Certainly we cannot simply say $\frac{d^2}{dt^2} = \left(\frac{d}{dt}\right)^2$ , since the linear operators, such as $d/dt$ , are not the same as numbers. For example, the former is usually not commutative (though in this special case it is), and even possibly non-associative. $\\$ Perhaps it is because that (1) $\frac{d^2}{dt^2} = \frac{d}{dt} \frac{d}{dt}$ according to the def of 2nd-order differentiation (2) the operators $d/dt$ , though non-commutative, obey distribution law, similar to the matrix? $\\$ Anyway despite the confusion about laws that linear operators follow, if I try to expand the expression $(d/dt + i) (d/dt - i)x$ from right to left then it makes sense: $(d/dt + i) (d/dt - i)x = (d/dt + i) (dx/dt - ix)= d/dt (dx/dt - ix)  + i (dx/dt - ix) \\
= \ddot x + x$ And so we can solve the ode from left to right: From $(d/dt + i)f(t) = 0$ we get $f(t)$ , then from $(d/dt - i)x(t) = f(t)$ we get $x(t)$ . $\\$ More generally speaking, are there some rules that the addition and multiplication of (partial) differentiation operators ( $d/dt$ and $\partial/\partial x$ , etc.) and numbers would follow? it seems that (partial) differentiation operators and matrices, both of which can be regarded as linear operators, share similarities. But they are still seemingly different, e.g. $d/dt$ and $\partial/\partial x$ may not obey associative laws while matrices do. Is there an article or book discussing the similarities and dissimilarities between the two? is there any chapter or essay that discusses in general the way of solving a higher order ode by factorizing the operators?","['linear-transformations', 'ordinary-differential-equations']"
4274417,"X and Y are two independent standard gaussian variables, what is the Probability that X>100*Y?","We want to find $P(X> 100*Y)$ . My strategy was to first express $X>100*Y$ as $X-100*Y>0$ and express $Z=X-100*Y$ . $Z$ should be another normal random variable, yet not standard. My approach further was to standardise it so I can use 68–95–99.7 rule. However this approach is leading nowhere. Can anyone give any hints how I should approach it?","['statistics', 'probability', 'random-variables']"
4274431,Combining Covariances of Two Sets,"According to Wikipedia , the formula for combining the covariances of two sets is: $$C_X=C_A+C_B+(\overline{x}_A-\overline{x}_B)(\overline{y}_A-\overline{y}_B) \cdot\frac{n_An_B}{n_X} $$ where: $A$ and $B$ are the first and second sets. $C$ is the Covariance. $n$ is the number of samples. $n_X = n_A + n_B$ . $x$ and $y$ are the features. I implemented this formula by splitting one dataset into two equal sets, for testing purposes, yet the result is quite different from the original dataset covariance. Now, let $M_{AB}$ be this part of the above formula: $$(\overline{x}_A-\overline{x}_B)(\overline{y}_A-\overline{y}_B) \cdot\frac{n_An_B}{n_X}$$ Looking at this implementation , the author basically applied the following formula: $$
C_X = \frac{(C_A \color{red}{\cdot n_A})  + (C_B \color{red}{\cdot n_B}) + M_{AB}}{\color{red}{n_X}}
$$ which gives the correct combined covariance !. I could not understand how the latter is derived or achieved algebraically ? Or if it's even similar to the former formula? because there are extra $\color{red}{n_A}$ and $\color{red}{n_B}$ that are added to the second formula! Your help is appreciated.","['statistics', 'covariance', 'algorithms']"
4274461,Underlying probability space for a Markov chain,"I took a rigorous course on probability theory last semester, and am taking a much less theoretical course on stochastic processes now and was thinking about how some of the concepts translate over. In particular, I wanted to know how to specify an underlying probability space for a discrete, time-homogenous Markov chain with state space $\cal{S} = \{ 1, 2, \cdots, n \}$ , an initial probability vector $\pi_0 \in \mathbb{R}^n$ and transition matrix $P$ . I'll describe a construction I came up with. First, define a sequence of probability spaces $\{ \left( \Omega_n, \cal{F}_n , \textbf{P}_n \right) \}_{n = 0}^\infty$ , where $\Omega_n = \cal{S}$ and $\cal{F}_n = \mathcal{P} \left( \cal{S} \right)$ for all $n \ge 0$ . The probability distirbution $\textbf{P}_n : \cal{F}_n \to [0, 1]$ is defined by the probability vector $\pi_n := \pi P^n$ in the obvious way where the entries of $\pi_n$ determine the probabilities on each singleton of $\cal{F}_n$ . I thought a natural choice for a sample space $\Omega$ for the Markov chain would be the set of all functions $f : \mathbb{N}_{\ge 0} \to \cal{S}$ . We can define for each $n \ge 0$ the evaluation map $X_n : \Omega \to \Omega_n$ , defined on each $\omega : \mathbb{N}_{\ge 0} \to \mathcal{S}$ in $\Omega$ by $X_n(\omega) = \omega(n)$ . Then we generate a $\sigma$ -algebra on $\Omega$ from the preimages of all the $X_n$ $$\mathcal{F} = \sigma \{ X_n^{-1} \left( \{ k \} \right) \subseteq \Omega \mid n \in \mathbb{N}_{\ge 0}, k \in \cal{S} \}$$ Now to define $\textbf{P} : \cal{F} \to [0, 1]$ . For each event $\{ X_n = k\} := X_n^{-1} \left( \{ k \} \right) \in \mathcal{F}$ , set $$\textbf{P} \left( \{ X_n = k\} \right) := \textbf{P}_n \left( \{ k \} \right)$$ If everything so far is sound, my question is on how can to extent $\textbf{P}$ to a probability on the whole of $\mathcal{F}$ (if such an extension even exists)? I feel that this is most likely done using the Carathéodery extension theroem by first somehow extending to the algebra generated by the events $\{  X_n = k \}_{k \in \mathcal{S}, n \ge 0}$ . I'm guessing we can maybe define for finite intersections using the transition matrix $P$ by $$\textbf{P} \left( \bigcap_{i = 1}^N \{ X_{n_i} = k_i \} \right) := \textbf{P} \left( \{ X_{n_1} = k_1 \} \right) \prod_{i = 1}^{N - 1} P_{k_i, k_{i + 1}}^{n_{i + 1} - n_i}$$ for $n_1 < n_2 < \cdots < n_N$ , and then extend to finite unions by inclusion-exclusion, but beyond that I'm completely lost. If my construction is hopelessly flawed and doomed to fail, I'd appreciate if someone could explain the proper way to specify the underlying probability space of a Markov chain anyways.","['stochastic-processes', 'measure-theory', 'probability-theory', 'markov-chains']"
4274462,"""On the Probability of Sequences in the Genoese Lottery"" by Euler. How did he do it?","This a problem solved by Leonard Euler. Translated English version is available in Euler archives.[E338] On the Probability of Sequences in the Genoese Lottery Euler .
I have difficulty in understanding Corollary 2 on page 29 he derived out of the blue. Can any one explain how did he arrive at  the formula for the number of cases of all species which belong to a particular  value of k. Here's an attached screenshot. I only want explanation of the first factor in those formulas (m-1),(m-1)(m-2)/2....","['integer-partitions', 'combinatorics', 'probability']"
4274503,Is the geometric representation of $\mathbb{R}^n$ in any sense canonical?,"This is somewhat of a soft question, but it's been bothering me for a while so I really need to ask it. When we study vector spaces in an abstract setting, we're introduced to a coordinate-free way to think about the objects involved, and learn to dissociate them from the geometric visualization of arrows or points living in a system of perpendicular cartesian grid. Metric notions are also generalized, so that we speak about norms instead of lengths, bilinear forms instead of dot products. Then we learn that no computational power is lost in this generalization, since all finite-dimensional inner product spaces are isomorphic to $\mathbb{R}^n$ equipped with the dot product, allowing us to 'get a grip' on an arbitrary abstract space. So far so good. But now comes my problem: if all of this is true, then why do we still represent $\mathbb{R}^n$ geometrically? No textbook or lecture I've ever come across ever distinguishes between $\mathbb{R}^n$ and geometric Euclidean space; in fact, $\mathbb{R}^n$ is often $\it{called}$ Euclidean space. Why is this the case? Is the geometric representation somehow privileged? Why not identify $\mathbb{R}^n$ with the space of non-constant polynomials in n powers of $x$ with real coefficients? Is the space of geometric vectors equipped with the physical angle-measuring dot product 'special'?","['hilbert-spaces', 'linear-algebra', 'geometry']"
4274506,Isn't my book doing this math about differentiation wrongly?,"Problem: Differentiate with respect to $x$ : $\ln\left(e^x\left(\frac{x-1}{x+1}\right)^{\frac{3}{2}}\right)$ My attempt: Let, $$y=\ln\left(e^x\left(\frac{x-1}{x+1}\right)^{\frac{3}{2}}\right)$$ Both $e^x$ and $\frac{x-1}{x+1}$ are positive: $e^x$ can never be negative, and you can take the square root of only positive numbers, so $\frac{x-1}{x+1}$ is positive as well. So, we can apply logarithm properties: $$=\ln e^x+\ln\left(\frac{x-1}{x+1}\right)^{\frac{3}{2}}$$ $$=x+\frac{3}{2}\ln\left(\frac{x-1}{x+1}\right)$$ Now, $$\frac{dy}{dx}=1+\frac{3}{2}.\frac{x+1}{x-1}.\frac{d}{dx}\left(\frac{x-1}{x+1}\right)$$ $$=1+\frac{3}{2}.\frac{x+1}{x-1}.\frac{x+1-x+1}{(x+1)^2}$$ $$=1+\frac{3}{2}.\frac{x+1}{x-1}.\frac{2}{(x+1)^2}$$ We can cancel $(x+1)$ and $(x+1)$ in the numerator and denominator because the graph doesn't change. $$=1+\frac{3}{x^2-1}$$ $$=\frac{x^2+2}{x^2-1}$$ My book's attempt: Let, $$y=\ln\left(e^x\left(\frac{x-1}{x+1}\right)^{\frac{3}{2}}\right)$$ $$=\ln e^x+\ln\left(\frac{x-1}{x+1}\right)^{\frac{3}{2}}$$ $$=x+\frac{3}{2}\ln\left(\frac{x-1}{x+1}\right)$$ $$=x+\frac{3}{2}\left(\ln(x-1)-\ln(x+1)\right)\tag{1}$$ $$...$$ $$\frac{dy}{dx}=\frac{x^2+2}{x^2-1}$$ Question: In my book's attempt, is line $(1)$ valid? I think my book's assumption in $(1)$ that $(x-1)$ and $(x+1)$ must be positive is unfounded. I deduced that $\frac{x-1}{x+1}$ is positive; $\frac{x-1}{x+1}$ can be positive even when both $(x-1)$ and $(x+1)$ is negative. So, is line $(1)$ of my book valid?","['calculus', 'functions', 'derivatives']"
4274515,Wasserman IID Samples Convergence in Quadratic Mean,"Here is the question: Exercise 6.8.3. Let $X_1, X_2, \dots, X_n$ be iid and let $\mu = \mathbb{E}(X_i)$ . Suppose that variance is finite. Show that $\overline{X}_n \xrightarrow{\text{qm}} \mu$ . Here is the given answer: Let $Y_i = X_i - \mu$ . It has variance $\sigma_Y = \sigma$ and mean $\mu_Y = 0$ . We have: $$
\begin{align}
& \mathbb{E}[(\overline{X}_n - \mu)^2] = \\
& = \mathbb{E}\left[\left(\frac{1}{n} \sum_{i=1}^n (X_i - \mu) \right)^2\right] \\
& = \frac{1}{n^2} \mathbb{E} \left[ \left(\sum_{i=1}^n Y_i \right)^2 \right] \\
& = \frac{1}{n^2} \left( \sum_{i=1}^n \mathbb{E}[Y_i^2] - \sum_{i=1}^n \sum_{j=1, j \neq i}^n \mathbb{E}[Y_i Y_j] \right) \\
& = \frac{1}{n} \left( (\sigma_Y^2 + \mu_Y^2) - (n-1) \mu_Y^2 \right) \\
& = \frac{\sigma}{n}
\end{align}
$$ Therefore, $\lim _{n \rightarrow \infty} \mathbb{E}[(\overline{X}_n - \mu)^2] = \lim _{n \rightarrow \infty} \sigma / n = 0$ , and so $\overline{X}_n \xrightarrow{\text{qm}} \mu$ . Unfortunately I follow until about step 3. For one thing, I am not sure how the $ \frac{1}{n}\ $ turned into $ \frac{1}{n^2}\ $ . Furthermore, I do not see how the double summations from i=1 to n and j=1, j $\neq$ i come in. If anyone could explain the concept used or even just point a resource to learn it, I would appreciate that. The book is Wasserman's All of Statistics.","['expected-value', 'statistical-inference', 'statistics']"
4274527,Significant differences between Riemannian and pseudo-Riemannian submanifolds,"In Chapter 8 of his Introduction to Riemannian Manifolds , Jack Lee discusses (Riemannian) submanifolds. In particular, in the first section, which is titled The Second Fundamental Form and presents the fundamental equations of submanifold geometry (Weingarten, Gauss, Codazzi), he writes: The results in the first section of this chapter apply virtually without modification
to Riemannian submanifolds of pseudo-Riemannian manifolds (ones on which the
induced metric is positive definite), so we state most of our theorems in that case. $\ldots$ Some of the results can also be extended to pseudo-Riemannian submanifolds of mixed signature, but there are various pitfalls to watch out for in that case; so for simplicity we restrict to the case of Riemannian submanifolds. This makes me wonder where exactly these ""pitfalls"" are. In fact, by comparing with Chapter 4 in O'Neill's Semi-Riemannian Geometry With Applications to Relativity , I fail to see immediate differences between the two theories. So my question is, what significant differences are there (both in terms of results and proofs thereof) between Riemannian and pseudo-Riemannian submanifolds? EDIT: This is not a duplicate of What significant differences are there between a Riemannian manifold and a pseudo-Riemannian manifold? . I am asking about differences between a submanifold of a pseudo-Riemannian manifold whose induced metric is positive definite and one whose induced metric is merely nondegenerate. I am not interested in comparing them at the level of manifolds, but specifically as submanifolds. To reiterate, I want to understand why the material presented in (the first section of) Chapter 8 of Lee's book cannot be extended more or less trivially to pseudo-Riemannian submanifolds.","['submanifold', 'semi-riemannian-geometry', 'riemannian-geometry', 'differential-geometry']"
4274666,Finding all positive integer solutions!,"I would like to find all positive integer solutions for the below equation: $1+n^3=m^4$ I suspect that it has no solution. How to prove that no solution exists? I'm interested in knowing how to begin with solving such questions more than the particular solution of that question. Is there a standard method to use (such like solving a quadratic equation) for these types of questions? Hint on how to start solving the problem would also help. I tried many algebraic manipulations like writing it as a difference of two squares twice, but didn't get to any where. I only figured that $n$ can't be a prime.","['number-theory', 'elementary-number-theory', 'diophantine-equations']"
4274673,"The number of permutations $p$ of {1,2,3,4,5,6,7,8,9} such that $p_1<p_2, p_2>p_3, p_3<p_4<p_5, p_5>p_6, p_6<p_7<p_8<p_9$","How could I count the number of permutations $ p=p_1p_2p_3p_4p_5p_6p_7p_8p_9 $ of {1,2,3,4,5,6,7,8,9} such that $p_1<p_2, p_2>p_3, p_3<p_4<p_5, p_5>p_6, p_6<p_7<p_8<p_9$ ? I need some hints. I tried counting them all by hand by counting the cases for $p_1 = 1,2,3, ... 8 $ (It can't be 9, obviously). But it takes a long time and it's hard to check for mistakes, so I was curious whether there is a shorter/simpler method. I also thought there could be a method of drawing a graph (ascending six times and descending twice at the appropriate positions), but couldn't finalize it. Thanks for your help!",['combinatorics']
4274721,Linearization of the action of PGL(n) on the projective space of dimension n-1,"Since I do not have enough reputations I could not comment directly at a similar question asked at https://mathoverflow.net/questions/219064/choosing-a-group-action-to-do-git-of-hypersurfaces?newreg=e451eed220d945dcb8711cd31ef335be . According to Mumford's GIT, there is a linearization of the line bundle $\mathscr{O}(n)$ for the standard action of $\text{PGL}(n)$ on $\mathbf{P}_{k}^{n-1}$ . I would like to see an explicit description of this action on the global section of $\mathscr{O}(n)$ , which is a representation of the projective linear group $\text{PGL}(n)$ . I tried the case $n=2$ and the representation of $\text{GL}(2)$ on $\text{Sym}^{2}(k^{2})$ coming from the standard representation $\text{id}:\text{GL}(2)\rightarrow\text{GL}(2)$ . However, an explicit computation suggests that a matrix $\begin{pmatrix}
a& b \\
c& d
\end{pmatrix}$ is sent to $\begin{pmatrix}
a^2 & ab & b^2\\
2ac& ad+bc & 2bd\\
c^2 & cd & d^2
\end{pmatrix}$ under the homomorphism $\text{GL}(2)\rightarrow \text{GL}(\text{Sym}^2 k^2)\simeq\text{GL}(3)$ . It does not appear that the kernel of this homomorphism is the center. Since Jason Starr mentioned that the action of $\text{PGL}(n)$ lifts to an action on the space of degree $d$ homogeneous polynomials as long as $n$ divides $d$ , I was expecting the representation of $\text{PG}L(n)$ on the global sections of $\mathscr{O}(n)$ comes from the symmetrized representation of the general linear group. Nevertheless, the above calculation suggests otherwise. Maybe I did something wrong or maybe the linearization is abstract?","['geometric-invariant-theory', 'algebraic-geometry']"
4274802,Diverge? $\lim_{n \to \infty} \sin(n)+ \sin(n^2)=?$,"I encounter with a problem that is to find: $$\lim_{n \to \infty, n \in \mathbb{N}} \sin(n)+ \sin(n^2)=?$$ So I do know the set $\{\sin n: n \in \mathbb{N}\}$ is dense in $[-1,1]$ and also knowing that $\lim_{n \to \infty} \sin(n^2) $ does not converge and it even has infinite limit points. However, I'm running in a trouble that is proving that if $\sin(n)+ \sin(n^2)$ converges, and exists such a subsequence $\{n_k\}_{k\geq 1} \subset \mathbb{N}$ , $\lim_{k \to \infty} \sin(n_k^2) = a$ then: $$\lim_{k \to \infty} \sin(n_k) \neq L - a$$ .","['limits', 'real-analysis']"
4274838,Elegant/streamlined proof of lattice isomorphism/correspondence theorem for groups?,"The lattice isomorphism/correspondence theorem for groups is an elegant statement about (normal) subgroups of $G$ containing $N$ corresponding with (normal) subgroups of $G/N$ . However, I am not aware of any clean/streamlined proof (in Dummit & Foote and Aluffi, it is brushed over as an exercise). Does anyone have such a streamlined proof? For reference, here is my ""ugly"" proof, which is very long and perhaps even wrong: $\newcommand{\quot}[2]{#1/#2} \newcommand{\normaleq}{\trianglelefteq}$ For a normal subgroup $N \normaleq G$ , we prove that the map $\varphi$ defined by $H \mapsto \quot H N$ establishes a bijection between subgroups of $G$ containing $N$ and subgroups of $\quot GN$ . Well-defined : we check that for a subgroup $H \leq G$ containing $N$ , $\quot HN$ is actually a subgroup of $\quot GN$ . Well, $e\in H \implies eN = e_{G/N} \in \quot HN$ ; associativity is inherited from $\quot GN$ ; $g,h\in H \implies (gN)(hN) =: (gh)N  \in \quot HN$ ; and $g\in H \implies (gN)^{-1} = g^{-1} N \in \quot HN$ . Inverse : we construct an explicit inverse using the canonical projection $\pi : G \to \quot GN$ ( $g\mapsto gN$ ), namely we show that $\pi^{-1}$ is the desired inverse. First, we want to show for a subgroup $H \leq G$ containing $N$ , $\pi^{-1}(\quot HN) := \{g\in G: gN \in \quot HN\} = H$ . The ( $\supseteq$ ) containment is true because $h\in H \implies hN \in \quot HN$ ; and the ( $\subseteq$ ) containment is true because $\quot HN = \{hN : h \in H\}$ implying that $gN \in \quot HN \implies gN = hN$ for some $h\in H$ , i.e. $g = hn \in H$ since $n$ is some element of $N \subseteq H$ . Second, we show that for a subgroup $S$ of $\quot GN$ , $\pi^{-1}(S)$ is a subgroup of $G$ containing $N$ , and $\varphi(\pi^{-1}(S)) = S$ . We know that inverse images via homomorphisms are subgroups, and since $S$ in particular contains $e_{G/N}$ , the inverse image $\pi^{-1}(S) \supseteq \pi^{-1}(\{e_{G/N}\}) = \ker(\pi) = N$ . Next suppose that $g\in \pi^{-1}(S) \implies \pi(g) = gN \in S$ . This tells us that $\varphi(\pi^{-1}(S)) \subseteq S$ . Now suppose $gN \in S$ .  (i.e. is an arbitary element of $S \leq \quot GN$ since its elements are all of the form $gN$ for some $g\in G$ ). Well, $\pi(g) = gN \in S \implies g\in \pi^{-1}(S) \implies gN \in \varphi(\pi^{-1}(S))$ , and so $S \subseteq \varphi(\pi^{-1}(S))$ , completing the proof. a bijection between normal subgroups of $G$ containing $N$ and normal subgroups of $\quot GN$ . Well-defined : we check that for a normal subgroup $H \leq G$ , $\quot HN$ is actually a normal subgroup of $\quot GN$ . In part (a) above, we already verified all the ""subgroup"" related claims, so we just need to verify normality. Let $h\in H$ be arbitrary. Then for any $g\in G$ , we have $(gN)(hN)(gN)^{-1} = ghg^{-1} N$ , but since $ghg^{-1} \in H$ , $ghg^{-1} N \in \quot H N$ , so indeed $\quot HN$ is normal in $\quot GN$ . Inverse : we construct an explicit inverse using the canonical projection $\pi : G \to \quot GN$ ( $g\mapsto gN$ ), namely we show that $\pi^{-1}$ is the desired inverse. In part (a) above, we already verified all the ""subgroup"" related claims, so we just need to verify normality, namely that the inverse image of a normal subgroup is normal. Well if $x\in \pi^{-1}(S)$ for a normal subgroup $S \normaleq GN$ , then for any $g \in G$ , $gxg^{-1}$ will satisfy $\pi(gxg^{-1}) = \pi(g) \pi(x) \pi(g^{-1}) \in S$ (since $\pi(x) \in S$ and $S$ is normal), so indeed $gxg^{-1} \in \pi^{-1}(S)$ . But $x\in \pi^{-1}(S)$ was arbitrary, so $g \pi^{-1}(S) g^{-1} \subseteq \pi^{-1}(S)$ for all $g\in G$ , which suffices to prove that $\pi^{-1}(S) \normaleq G$ (since $gNg^{-1} \subseteq N \implies Ng^{-1} \subseteq g^{-1} N$ and also $gN \subseteq Ng$ where again $g$ is taken over all of $G$ ).","['alternative-proof', 'normal-subgroups', 'group-theory', 'abstract-algebra']"
4274844,Solve a contour integral: $\int_{\gamma}\frac{z^2+1}{z(16z^2+1)}$,"I am trying to solve a contour integral: $$\int_{\gamma}\frac{z^2+1}{z(16z^2+1)}$$ Where $\gamma$ is the postivley oriented circular contour of radius 1/4 about the point 1. My attempt at the solution: $\frac{z^2+1}{z(16z^2+1)}$ has singularitites at $z=0$ and $z= +i1/4$ and $z=-i1/4$ I am trying to see if the poles are insider countour, however i don't know how to releate complex poles to real adius.","['integration', 'complex-analysis']"
4274886,Expected number of rolls until lcm is greater than $2000$?,"You continually roll a fair $10$ sided dice. What is the expected number of rolls until the lowest common multiple of all numbers that have appeared is greater than $2000$ ? The primes in the numbers $1$ to $10$ are $2,3,5,7$ . The lowest common multiple of these numbers  is $210$ . However, different numbers could come in different frequencies (one $9$ is worth two $3$ s). How can you deal with this? I am happy for solutions with approximate answers. The true value by simluation is around $18.8$ .","['number-theory', 'gcd-and-lcm', 'expected-value', 'probability-theory', 'probability']"
4274899,Expected number of rolls until all consecutive differences have been seen,"Call a consecutive difference"" the absolute value of the difference between two consecutive rolls of a $6$ sided die. For example, the sequence of rolls $143511$ has the corresponding sequence of consecutive differences $31240$ . What is the expected number of rolls until all $6$ consecutive differences have appeared? I can see that there are $6$ possible consecutive differences: $0, 1,...,5$ and they appear among any pair of dice with probability $6/36, 10/36, 8/36, 6/36, 4/36, 2/36$ respectively. However, they could come in all sorts of orders. I am happy with answers that are close approximations. By simulation the answer is around 25.8.","['expected-value', 'dice', 'probability-theory', 'probability']"
4274919,Expected value of $100$th item in series,"A sequence is defined in the following way: $x_0 = 0$ , and: $
  x_{n+1} =
  \begin{cases}
                                   x_n + A_n & \text{if $A_n | x_n$} \\
                                   x_n - A_n & \text{otherwise}
  \end{cases}
$ where $A_n$ is an integer chosen at random with uniform probability from $\{1,2,...,10\}$ . What is the expected value of $x_{100}$ ? I cannot see how I can calculate this if I dont know the probability that $A_n$ divides $x_n$ ? I can see that $\mathbb{E}[x_1] = 5.5$ but I cannot see how to find the expected value of the sum.","['expected-value', 'probability-theory', 'probability', 'sequences-and-series']"
4275161,Isn't my book using logarithm wrongly to differentiate?,"Problem: Differentiate with respect to $x$ : $\frac{e^{-3x}(3x+5)}{7x-1}$ My book's attempt: Let, $$y=\frac{e^{-3x}(3x+5)}{7x-1}$$ $$\ln(y)=\ln(\frac{e^{-3x}(3x+5)}{7x-1})\tag{1}$$ $$\text{rest of the math...}$$ Question: Isn't taking $\ln()$ unfounded in $(1)$ : $\frac{e^{-3x}(3x+5)}{7x-1}$ could a negative number? Don't we have to just use the product and quotient rule to find the derivative here?","['calculus', 'derivatives', 'logarithms']"
4275223,Extension of the solution,"Let $H \in C[\mathbb{R}^{n}, \mathbb{R}]$ and $H(x) \to \infty$ as $|x | \to \infty$ . Suppose $f \in C[\mathbb{R}_{+} \times \mathbb{R}^{n}, \mathbb{R}^{n}]$ and for some $M >0$ : $$
\frac{\partial H}{\partial x} \cdot f(t,x) \leq 0, \quad |x| \geq M, t\in \mathbb{R}_{+} =[0, \infty)
$$ Show that the solutions of ODE: $x' = f(t,x), x(t_{0}) = x_{0}$ exists on $[t_{0}, \infty)$ . My attempt is that I am trying to prove that $f(t,x)$ is bounded on the right maximal interval of existence. Using the integration-formula $$
x(t) = x_0 + \int_{t_0}^{t} f(w,x(w)) dw
$$ to derive that $x(t)$ is bounded on the maximal interval. However, I was stuck using the assumption on $H(x)$ . I appreciate any help how to prove this.","['ordinary-differential-equations', 'real-analysis']"
4275331,Why is the radius chosen for radians?,"If I understand this correctly, the approach to measure angles with radians originated in relation to questions about speed. If the length of a circular sector of a circle is equal to the radius, we call it a radian. If the radius is e.g. $1$ then the circumference is equal to $6.28$ . This means that if we have a wheel and it does a complete turn it covered $6.28$ meters
assuming the radius is $1$ meter. What I am not clear is the original choice of radian. I can understand that if we wrapped a string around a wheel and turned the wheel for a complete circle while the string unwraps at the same time, then the length of the string that was unrolled would be $6.28$ meters (i.e. equal to the circumference). But I am not clear why the radian was chosen to be equal with the radius? Was it a mere convenience so as to get $6.28$ radians for a complete turn? The core of my question is that I am having trouble  envisioning a wheel moving and the meaning of a movement of $1$ radius in relation to the movement so as to make sense to use it a measurement.","['angle', 'circles', 'trigonometry', 'intuition', 'algebra-precalculus']"
4275344,How to toss a coin in your head. [duplicate],"This question already has answers here : How to mentally flip a coin? (10 answers) Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved This is quite a soft question but hopefully, this community has some nice answers. What is a good way to ""toss a coin"" in your head? That is an ""algorithm"" to generate heads $50/50$ of the time roughly that is hard for you to influence without thinking too deeply. I am looking for little mathematical tricks to ""randomise"" and scramble ones ability to select a handful or manageable numbers. In a way that is hard to forsee, predict or influence like the listed below. I am not looking for ways to manipulate real-world data like freckles, hair, second hands, books or digits of pi. My methods so far Mod 3: I think of the first two digit numbers that come into my head, multiply them together and if the result is $1$ mod $3$ I say heads, if $2$ mod $3$ I say tails and if $0$ mod $3$ I go again. This seems to work quite well but now I know how the game works it is quite hard to not overinfluence my choices without picking very large numbers. Collatz down I think of a number quickly and then perform the collatz operations on it, halfing if even and tripling then adding one if odd. If it takes an odd number of steps to reach $1$ I say heads and if even I say tails. I worry this doesn't yield heads $50\%$ of the time however. Penultimate digit I think of two $2$ digit numbers and multiply them together. If the tens digit is even I say heads, if it is odd I say tails. Can someone think of other quick methods that are hard to influence?","['soft-question', 'probability']"
4275362,uniformly continuous function $f$ such that $\sum 1/f(n)$ is convergent?,"Does there exist a uniformly continuous function $f:[1,\infty)\to \mathbb R$ such that $\sum_{n=1}^\infty 1/f(n)$ is convergent ? I know that $\exists M>0$ such that $|f(x)|< Mx, \forall x\in [1,\infty)$ , so $|1/f(n)|>1/(Mn) ,\forall n \ge 1$ , thus $\sum_{n=1}^\infty |1/f(n)|$ is divergent. But I don't know what happens with $\sum_{n=1}^\infty 1/f(n)$ . Please help","['uniform-continuity', 'sequences-and-series', 'real-analysis']"
4275390,Markov Kernels Corresponding to Conditional Probabilities.,"Let $(\Omega,\mathcal A,P)$ be a probability space and $\mathcal F$ a sub- $\sigma$ -algebra of $\mathcal A$ . Then the map $A\mapsto P[A|\mathcal F]$ is not a probability measure on $(\Omega,\mathcal A)$ , even almost surely. For example, although $$P[\cup_{i=1}^\infty A_i|\mathcal F]=\sum_{i=1}^\infty P[ A_i|\mathcal F] \quad \quad P\text{-almost surely}$$ for any sequence $(A_i)$ of disjoints sets in $\mathcal A$ (by the conditional DCT), the null set depends on the chosen sequence $(A_i)$ , and there are possibly uncountably many such sequences. On the other hand, a markov kernel $\kappa:\mathcal A \times \Omega \to [0,1]$ with source $(\Omega,\mathcal F)$ and target $(\Omega,\mathcal A)$ will satisfy the properties For every $A\in\mathcal A$ , the map $\omega\mapsto \kappa(A,\omega)$ is $\mathcal F$ -measurable. For every $\omega\in\Omega $ , the map $A \mapsto \kappa(A,\omega)$ is a probability measure on $(\Omega,\mathcal A)$ . Question: Can I find  a markov kernel as above such that $\kappa(A,\cdot)$ is a version of $P[A|\mathcal F]$ for all $A\in \mathcal A$ ? If I understand correctly Wikipedia , we need the target space $(\Omega,\mathcal A)$ to be Polish with its Borel $\sigma$ -algebra. Can we dispense with this assumption? An argument for the case of a countable generator partitioning $\Omega$ : Suppose there exists a countable partition $\mathcal C= \{ C_n: n \in \mathbb{N}\}$ of $\Omega$ such that $\mathcal A=\sigma(\mathcal C)$ . WLOG we may assume that each $C_n$ is nonempty. Then $ \sigma(\mathcal C) = \{\cup_{i \in I} C_i: I \subseteq \mathbb{N}\}$ . Consider the real random variable $X=\sum_{n=1}^{\infty} n1_{C_n}$ on $(\Omega,\mathcal A,P)$ . Then $X$ has a regular conditional distribution given $\mathcal F$ , i.e. there exists a markov kernel $\kappa_{X,\mathcal F}:\mathcal B(\mathbb R) \times \Omega \to [0,1]$ with source $(\Omega,\mathcal F)$ and target $(\mathbb R,\mathcal B(\mathbb R))$ such that $\kappa_{X,\mathcal F}(B,\cdot)$ is a version of $P[X\in B|\mathcal F]$ for all $B\in\mathcal B(\mathbb R)$ . Now, given $\cup_{i \in I} C_i\in\mathcal \sigma(\mathcal C)$ and $\omega\in\Omega$ put $$\kappa(\cup_{i \in I} C_i,\omega):=\kappa_{X,\mathcal F}(\cup_{i \in I}\{i\},\omega)$$ Since $\mathcal C$ is a partition of $\Omega$ into nonempty sets the representation $\cup_{i \in I} C_i$ is unique and $\kappa$ is well-defined. We also check that $\kappa$ is a markov kernel with source $(\Omega,\mathcal F)$ and target $(\Omega,\mathcal A)$ . Moreover, $$\kappa(\cup_{i \in I} C_i,\cdot)=\kappa_{X,\mathcal F}(\cup_{i \in I}\{i\},\cdot)=P[X\in \cup_{i \in I}\{i\}|\mathcal F]=P[\cup_{i \in I} C_i|\mathcal F]$$ $P$ -almost surely for every $\cup_{i \in I} C_i\in\mathcal \sigma(\mathcal C)$ . Therefore $\kappa$ is a markov kernel with the desired property. Is this correct? Thanks a lot for your help.","['measure-theory', 'conditional-probability', 'conditional-expectation', 'polish-spaces', 'probability-theory']"
4275401,Independent increments $\iff$ increments independent of natural filtration?,"Let $X=(X_t)_{t\in T}$ be a stochastic process with $X_0=0$ , and $(\mathcal{F}^X_t)_{t\in T}$ the natural filtration, that is, $\mathcal{F}^X_t=\sigma(X_r : r\le t)$ . Is it true that the following are equivalent? $X$ has independent increments, i.e. for $t_1<\ldots<t_n$ in $T$ we have $(X_{t_n}-X_{t_{n-1}}),\ldots (X_{t_2}-X_{t_1})$ are independent for every $s<t$ in $T$ we have $X_t-X_s$ is independent of $\mathcal{F}^X_s$ . For $1)\implies 2)$ I first thought I could just say that if $1)$ is true then $X_t-X_s$ independent of $X_r-0$ for every $r\le s$ and then conclude that $\sigma(X_t-X_s)$ is independent of $\sigma(X_r : r\le s)$ but it seems I cannot conclude this right away as I saw in this example . For $2)\implies 1)$ I was thinking that if $t_1<\ldots<t_n$ then $2)$ implies that $X_{t_n}-X_{t_{n-1}}$ is independent of $\sigma(X_r : r\le t_{n-1})$ . But then in particular $X_{t_n}-X_{t_{n-1}}$ is independent of $\sigma(X_{t_1},\ldots,X_{t_{n-1}})$ . But considering the measurable map $f(x_1,\ldots,x_{n-1})=(x_{n-1}-x_{n-2},\ldots,x_2-x_1)$ we get that $X_{t_n}-X_{t_{n-1}}$ is independent of $(X_{t_{n-1}}-X_{t_{n-2}},\ldots,X_{t_2}-X_{t_1})$ but I don't think think this allows me to conclude that all the increments are independent.","['stochastic-processes', 'independence', 'probability-theory', 'probability']"
4275429,Expected number of rolls in a game of snakes and ladders,"You are playing a game of snakes and ladders. You start at square $1$ , and each turn you roll a $6$ sided dice and move the corresponding number of squares. When you reach at least square $25$ , you stop. There is also a snake which connects squares $5$ and $20$ (i.e. if you land on $20$ , you immediately move down to $5$ ), and there is a ladder from $10$ to $15$ (so if you land on 10, you immediately move up to $15$ ). What is the expected number of rolls until you finish the game? [ Source ] If we think about the game without any snakes or ladders, you need to move at least $24$ squares. The expected number you roll on each throw is $3.5$ . However, how can you calculate your expected stopping value, when it is likely to be past $25$ ? I cannot see how to factor in the probability of hitting the snake or ladder.","['expected-value', 'dice', 'probability-theory', 'probability']"
4275437,Relation between the two spin structures of $S^1$,"I know that $S^1$ has two spin structures $s$ and $t$ , corresponding to its two double covers. I am trying to understand which diffeomorphism $f \colon S^1 \rightarrow S^1$ sends $s$ to $t$ , i.e. $f^*(s)=t$ . I am thinking about spin structures as cohomology classes in $H^1(E(M);\mathbb{Z}/2\mathbb{Z})$ , where $E(M)$ is the tangent frame bundle on $M$ . Any help would be appreciated: I guess that my problem depends on my poor understanding of spin structures.","['principal-bundles', 'spin-geometry', 'lie-groups', 'differential-geometry']"
