question_id,title,body,tags
2364084,When to multiply the assumed particular solution by $t$ (Method of undetermined coefficients),"I'm given the DE : $$y''-y = 8te^{t}$$ First I solve the corresponding homogenous equation : $$r^2-1 = 0 \implies r_1= 1 \ r_2= -1$$ So The general solution of the corresponding homogenous equation looks like : $$y_c = c_1e^t + c_2e^{-t}$$ Now I want to find a particular solution so I assume $Y_1(t)$ solves the DE $$Y_1(t) = (At + B) e^t$$ Calculating we end up with : $$Y_1''(t) = (2A+B)e^t + Ate^t$$ However then the DE takes a form : $$(2A+B)e^t = 8te^t$$ Which does not make sense. So I believe I should have multiplied the assumed particular solution by $t$ beforehand. However I know that I do this multiplication when the particular solution is in the homogenous equation. However it is not. So I am confused. In General, I have a problem about when to multiply the assumed particular equation by t. For instance consider : $$f(x,y,y',y'') = xe^xsinx$$ Where $f$ is linear in all its input variables. Now let us further assume the corresponding homogenous equation is in some form of this : $$c_1e^x+c_2e^{-x}+ xc_3$$ Now How should I contruct the particular solution? Should I multiply the particular solution by $x?$ I don't think so since $sinx$ is not in the homogenous equation. However, If someone could give me an explanation on these two questions and make my mind clear about this issue I would be really glad. Thanks,",['ordinary-differential-equations']
2364101,A ring as a finite union of fields,"Let a ring $R$ be a finite union of fields all having the same unit. I want to prove that $R$ is itself a field. I wrote $R=\bigcup _{i=0}^{n}F_i$, with $F_0=\{0,1\}$ and $F_i$'s are fields. Since we deal with a finite union, there must exist $j\geq 1$ such that $F_k\subseteq \bigcup _{i=k+1}^nF_i$ for $k<j$, but $F_j\nsubseteq \bigcup _{i=j+1}^nF_i$ . Then I put $B=\bigcup _{i=j+1}^nF_i$. Certainly, we have $R=\bigcup _{i=j}^nF_i$. I tried to show that $R=F_i$, for one of the latter $F_i$'s. Suppose not, so we could choose $b\in B-F_j$ (because $F_j\neq R$) and $a\in F_j-B$. The element $ab\in R$ is either in $F_j$ or in $B$. In the first case, $b=a^{-1}ab\in F_j$, which is a contradiction. In the other case, could we deduce that $a=abb^{-1}\in B$ to reach a contradiction? In fact, $b$ and $b^{-1}$ are both in $B$. But, we are not sure whether $B$ is multiplicatively closed to reach the desired contradiction. Any help is appreciated!","['abstract-algebra', 'noncommutative-algebra', 'ring-theory', 'commutative-algebra', 'field-theory']"
2364118,"100 people with 100 dollars each, give 1 dollar to a random other person. What's the distribution?","Source: http://www.decisionsciencenews.com/2017/06/19/counterintuitive-problem-everyone-room-keeps-giving-dollars-random-others-youll-never-guess-happens-next/ ""Imagine a room full of 100 people with 100 dollars each. With every tick of the clock, every person with money gives a dollar to one randomly chosen other person. After some time progresses, how will the money be distributed?"" I've seen various blog posts on this that use code to simulate the distribution. My question is: what is the reason that the distribution becomes skewed rather than stay uniform, what math do I need to understand this problem? I initially tried reasoning with expected value, that each person has at each moment an expected value of $1. So I thought it would be uniform. Clearly this can change as soon as one person goes broke, then the expected value drops. Thanks",['statistics']
2364186,Find Christoffel Coefficients,"Evaluate the Christoffel coefficients of the following surface of revolution: $$X(\theta,s)=(r(s)\cos\theta,r(s)\sin\theta,z(s))$$ So we first start with finding the mertic $X_{\theta}=(-r\sin\theta,r\cos\theta,0)$ $X_{s}=(r'\cos\theta,r'\sin\theta,z')$ $g_{11}=<X_{\theta},X_{\theta}>=r^2\sin^2\theta+r^2\cos^\theta=r^2(\sin^2\theta+\cos^\theta)=r^2$ $g_{12}=g_{21}=<X_{\theta},X_{s}>-rr'\sin\theta \cos\theta+rr'\sin\theta \cos\theta=0$ $g_{22}=<X_{s},X_{s}>=(r')^2\cos^2\theta+(r')^2\sin^2\theta+(z')^2=(r')^2(\cos^2\theta+\sin^2\theta)+(z')^2=(r')^2+(z')^2$ So we get $$g_{ij}=\begin{pmatrix}r^2 & 0\\0 & (r')^2+(z')^2\end{pmatrix}$$ Now in the book I saw that $(r')^2+(z')^2=1$ why is that? can we say that all surface of revolution have the same metric? $$g_{ij}=\begin{pmatrix}r^2 & 0\\0 & 1\end{pmatrix}?$$ Now we have to find $$g_{ij,\theta}=\begin{pmatrix}0 & 0\\0 & 0\end{pmatrix}$$ and $$g_{ij,s}= \begin{pmatrix}2r^2r' & 0\\0 & 0\end{pmatrix}$$ and $$g^{ij}=\frac{1}{r^2}\begin{pmatrix}1 & 0\\0 & r^2\end{pmatrix}$$ Now how do I calculate Christoffel coefficients?","['riemannian-geometry', 'differential-geometry', 'surfaces']"
2364233,"Prove : If $\sum_na_nb_n$ converges whenever $\sum b_n^2 \lt \infty,$ then $\sum a_n^2<\infty$","Suppose that $a_n$ is a sequence of real numbers  such that $\sum_na_nb_n$ converges whenever $\sum_n b_n^2 \lt \infty$. Show that $\sum_{n=1}^{\infty}a_n^2 \lt \infty$. My try: I defined $T: l^2 \to \mathbb{R}$ by sending $(b_1,b_2,\ldots,b_n,\ldots,) \to \sum_{n=1}^{\infty}a_nb_n$. Then $T$ is linear. I want to show that $T$ is bounded and then that will give us the result for $b^n=(a_1,a_2,\ldots,a_n,0,\ldots,0)$, $$T\left(\frac{b^n}{\sqrt{\sum_{j=1}^n a_j^2}}\right)=\frac{a_1^2+a_2^2+\ldots a_n^2}{\sqrt{\sum_{j=1}^n a_j^2}}=\sqrt{\sum_{j=1}^n a_j^2} \le \|T\|, \forall n \in \mathbb{N}$$
which implies that $$\sum_{j=1}^{\infty} a_j^2 =\lim_{n \to \infty} \sum_{j=1}^n a_j^2 \le \|T\|^2 \lt \infty$$ The only thing which remains to be shown now is that $T$ is bounded for which I tried to evoke the Closed Graph Theorem. Suppose that $b^n=(b^n(1),b^n(2),\ldots,b^n(j),\ldots) \in l^2 $ converge to $0$ and $T(b^n) \to y$. Let $\epsilon \gt 0$. Then there exists $n_0 \in \mathbb{N}$ such that for all $n \ge n_0$, we have $\|b^n\|_2 \lt \frac{\epsilon}{2}$ which in particular implies that $|b^n(j)| \lt \frac{\epsilon}{2}$ for all $j$ and all $n \ge n_0$. Since $T(b^n) \to y$, there exists $n_1 \in \mathbb{N}$ such that for all $n \ge n_1$, $|T(b^n)-y| \lt \frac{\epsilon}{2}$. Then for all $n \ge \max{(n_0,n_1)}$$$|y| \le |y-T(b^n)|+|T(b^n)| \lt \epsilon+\frac{\epsilon}{2}(|a_1|+|a_2|+\ldots+|a_k|)$$
(Note: Since $T(b^n) \lt \infty$, the tail of the series goes to $0$ which means that $|\sum_{j \ge k} a_jb^n(j)| \lt \frac{\epsilon}{2}$ ) This is not what I intended to show. Can I conclude from here that $y=0$ since $\epsilon \gt 0$ is arbitrary? For me the problem is that apriori, I don't have a way to get away with the $|a_j|$'s, since they depend on the choice of $b^n$. Note: There is an answer to this question here: If $\sum a_n b_n <\infty$ for all $(b_n)\in \ell^2$ then $(a_n) \in \ell^2$ . But I wanted to know if I can go via this route and get to the answer and if not, why so. Thanks for the help!!","['functional-analysis', 'real-analysis', 'banach-spaces', 'hilbert-spaces']"
2364253,How to handle purely imaginary Hamiltonians,"Suppose I have a system of complex ODE's of the form $$ i\dot{\mathbf{c}}(t)=\mathbf{f}(\mathbf{c}(t))$$ and I can write down a Hamiltonian such that each ODE can be written as $$\dot{c}_j=\frac{\partial\mathcal{H}}{\partial c_j^*}$$ for each $j$ where * denotes complex conjugate. As a very simple example, the Hamiltonian $$ \mathcal{H}=-i\left(|c_0|^2+|c_1|^2\right)
$$ leads to the equations $$i\dot{c}_0=c_0, \qquad i\dot{c}_1=c_1. $$ Questions: What are the conjugate momenta for this system (Are they just the complex conjugates)? Also, is there any way to transform this problem (via action-angle coordinates/madelung transform) to one where the Hamiltonian is purely real or where the system evolves under real dynamics? Is an imaginary Hamiltonian even an issue if I want to analyze a much more complicated non-linear system of this type using canonical perturbation or bifurcation theory?","['classical-mechanics', 'hamilton-equations', 'ordinary-differential-equations', 'dynamical-systems']"
2364264,Prove that $\gamma = 99^{\circ}$ in this quadrilateral.,"$ABCD$ is a quadrilateral. $\measuredangle{BAD}=86^{\circ}$ and $\measuredangle{CDA}=68^{\circ}$, $|AB|=|CD|$, $E$ and $F$ are midpoints of their segments. $\measuredangle{DEF}=\gamma$. Prove that $\gamma=99^{\circ}$. My most productive try is defining midpoints of $AB$ and $DC$ as $G$, $H$ respectively and observing the parallelogram $GEFH$. This uses the information of lengths but not the angles. And i couldn't find a way to use angles.","['quadrilateral', 'euclidean-geometry', 'trigonometry', 'geometry']"
2364295,Amateur proof verification: Between two consecutive roots of $f'$ there is at most one root of $f$,"i need someone to verify if i'm doing anything wrong on proving the following theorem (i'm new to real analysis and formal proofs). Also, suggestions on how to write it better would be appreciated. $f: I\rightarrow R$ differentiable. Beteween two consecutives roots of $f'$ there is at most one root of $f$ I think i can see why this is true. Informal Attempt: Let $g$ be a constraint of $f$ to the interval $[a,b]$, $a<b$, $g: [a,b]\rightarrow R$. if $f'(a)=g'(a)$ and $f'(b)=g'(b)$ are consecutive zeros of $f'$, they are the sole zeros of $g'$. By the Weierstrass extreme value theorem, since $g$ is continuous and $[a,b]$ is a compact set, we know $g$ obtains its extreme values. Since the only two zeroes of $g'$ are, by its very definition, $a$ and $b$, they must be these extreme values. From this point on, i know that, by ""looking"" at the graph of $g$, it must intersect the $x$ axis at most once, otherwise there would be other  $g'(x)=0$. How can i write this down formally? Have i missed anything? After proving that result for $g$, I intended to apply it to $f$ and get to the final result. Thanks for your attention.","['real-analysis', 'proof-verification', 'calculus', 'functions', 'proof-writing']"
2364309,"If $f$ is differentiable, then $f$ is continuous","Let  $f: \mathbb{R}^2 \to \mathbb{R} $ be a function. Prove or disprove: 1) If $f$ is differentiable at $(a,b)$, then $f$ is continuous at $(a,b)$ 2) If $f$ is continuous at $(a,b)$, then $f$ is differentiable at $(a,b)$ What I already have: If I want to show that $f$ is differentiable at $a$ (and with that also continuous  at $a$), I do it like this: $\lim_{h\to 0}  f(a+h)-f(a)= \lim_{h\to 0} {\frac {f(a+h)-f(a)}{h}\cdot h}$ $=\lim_{h\to 0} {\frac {f(a+h)-f(a)}{h}\cdot \lim_{h\to 0}h}=f'(a)\cdot0=0 $ However here I need to show it for a point $(a,b)$. My idea was to show it like this: If $f$  is differentiable at $(a,b) \to \forall h \in \mathbb{R}^2$ $f((a, b) + h) - f(a, b) = \nabla f(a,b)*h + r(h)$ and $\frac{r(h)}{|h|  }  \to 0 $  for     $h \to 0$ 2) I know that this is not true. So would a counterexample be $f(x,y)= |x| + y| ? $ Thanks in advance!","['real-analysis', 'calculus', 'multivariable-calculus', 'continuity', 'analysis']"
2364320,prove $\sum_{i=1}^{n}(x_i-\bar{x})^2\lt\sum_{i=1}^{n}(x_i-a)^2$,"How do we prove that
$$
\sum_{i=1}^{n}(x_i-\bar{x})^2\lt\sum_{i=1}^{n}(x_i-a)^2
$$ where $a$ is any value other than $\bar{x}$, the arithmetic mean. My Attempt: $$
\sum_{i=1}^{n}(x_i^2-2x_i\bar{x}+\bar{x}^2)\lt\sum_{i=1}^{n}(x_i^2-2x_ia+a^2)\\\sum_{i=1}^{n}x_i^2-2\bar{x}\sum_{i=1}^{n}x_i+\sum_{i=1}^{n}\bar{x}^2\lt\sum_{i=1}^{n}x_i^2-2a\sum_{i=1}^{n}x_i+\sum_{i=1}^{n}a^2\\-2\bar{x}.n\bar{x}+n.\bar{x}^2\lt-2a.n\bar{x}+n.a^2\\-2\bar{x}^2+\bar{x}^2\lt-2a\bar{x}+a^2\\-\bar{x}^2\lt-2a\bar{x}+a^2
$$
But how do I proceed further or is there any better way ?","['means', 'statistics', 'standard-deviation']"
2364332,"Find angles in a triangle, with two similar triangles with scale factor $\sqrt{3}$","Triangle ABC has point D on BC which creates triangle ABD and ACD. They differ with the scale factor $\sqrt{3}$. What are the angles? I know ADB and ADC cannot be right, as it shares the side AD and cannot have a scale factor of $\sqrt{3}$. I have tried to approach it with $\frac{AD}{DC}=\frac{BD}{AD}=\sqrt{3}$ (or $\frac{AD}{AC}=\frac{BA}{AD}=\sqrt{3}$). Meaning that $DC=x$ $AD=\sqrt{3}x$ $BD=3x$. This is where I get stuck. Because I dont know if the triangle is right angled, and cannot use Pythagoras. I dont have any angles, hence cannot use $\frac{sin(a)}{angleA}$. How do I solve this?","['angle', 'euclidean-geometry', 'triangles', 'geometry']"
2364350,"Alternative definition of $\|f\|_{\infty}$ as the smallest of all numbers of the form $\sup\{|g(x)| : x \in X \}$, where $f = g$ almost everywhere [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Recall that $\|f\|_\infty = \inf\{M\ |\ |f(x)|\leq M \text{ for almost every } x \in X\}$, for every function $f$ defined on $X$. Show that, if $\|f\|_\infty$ is finite, then $\|f\|_\infty$ is also the smallest of all numbers of the form $\sup\{|g(x)| : x \in X \}$, where $f = g$ almost everywhere with respect to the Lesbegue measure. I am trying to understand how $L^{\infty}$ is defined.
Could someone help me see how to prove the above fact?","['lp-spaces', 'measure-theory']"
2364384,"Movement time of object with constant jerk, limited acceleration and velocity","A product is initially at rest on a conveyor belt: The initial conditions of the product can be described as follows:$$x_i=0$$ $$v_i=0$$ $$a_i=0$$$$j_i = j⋆ $$. The product will be moved forward distance of exactly Δx (known) to position it under the conveyor belt: When the movement is complete, the product will be sprayed for a few moments, so it must come to a complete rest at this point: The conveyor, which must move the product into position, will complete the movement in the minimum time possible. The conveyor's control system will apply a constant jerk of j⋆ (known), -j⋆ (known) or zero. However, the conveyor must obey these limits: It has a max velocity of v⋆ (known), a max acceleration of a⋆ (known), a max deceleration of d⋆ (known). Assume the product does not slide or slip on the conveyor. How long will it take the product to complete the movement of Δx ? Side Note: At first glance, it seems as though the problem can be solved by a simple application of the kinematic equation for one-dimensional motion with constant jerk (and its derivatives):
$$x = x_0 + v_0 t + \frac{1}{2} a_0 t^2 + \frac{1}{6}jt^3$$
However, the limits v⋆ , a⋆ and d⋆ pose a major problem. For some values of Δx , the conveyor may not even reach max velocity, max acceleration, or max deceleration.  Therefore, there will be 8 possible scenarios in which different combinations of these limits are reached.  To assist with this part of the problem, I have created a decision chart that illustrates the different possible scenarios: Conveyor Limit Decision Chart As Δx gets smaller and smaller, the possibilities range from all three limits reached... ... to no limits reached... ... making it hard to describe the time passed during the movement with a single function.  I am stumped at this point- how do I predict which limits will be met and which will not?","['physics', 'kinematics', 'limits']"
2364401,How can I compute simple examples of the associated vector bundle to a coherent sheaf?,"Given a coherent sheaf $\mathcal{E} \in \text{Coh}(X)$ over a scheme there is a way to associate a relatively affine scheme over $X$. This is done by constructing the $\mathcal{O}_X$-algebra
$$
\text{Sym}^\bullet(\mathcal{E}) = \mathcal{O}_X \oplus \mathcal{E} \oplus \text{Sym}^2(\mathcal{E})\oplus \text{Sym}^3(\mathcal{E})\oplus \cdots
$$
and then taking relative spec
$$
\mathbb{V}(\mathcal{E}) = \underline{\text{Spec}}_{\mathcal{O}_X}(\text{Sym}^\bullet(\mathcal{E}))
$$
I am having trouble figuring how to compute basic examples of this, so how can I compute $\mathbb{V}(\mathcal{E})$ in simple cases? such as $\mathcal{O}(k)$ over $\mathbb{P}^n$ $\mathcal{O}(k)\oplus \mathcal{O}(l)$ over the same space I know that I can use these computations to find the same associated vector bundles to some projective variety using pullbacks. As per Ben's suggestion, I'll look at $\mathbb{P}^1$. Since we have the embedding
$$
\mathcal{O}(-1) \xrightarrow{\begin{bmatrix} x \\ y \end{bmatrix}} \mathcal{O}\oplus\mathcal{O}
$$
I expect this to be the sub-variety of $\mathbb{A}^2_{\mathbb{P}^1}$ defined by the equation
$$
\frac{\mathcal{O}_{\mathbb{P}^1}[a,b]}{(ya - xb)}
$$
where $\mathbb{P}^1 = \text{Proj}(\mathbb{C}[x,y])$. In general, this should be given by all of the linear relations cutting out a line over each point of $\mathbb{P}^n$.","['coherent-sheaves', 'vector-bundles', 'algebraic-geometry', 'commutative-algebra']"
2364426,Show that $\int_0^\pi\frac{2\cos(2\theta)+\cos(3\theta)}{5+4\cos(\theta)}d\theta=\frac{\pi}{8}$,"I want to prove that $\displaystyle\int_0^\pi\frac{2\cos(2\theta)+\cos(3\theta)}{5+4\cos(\theta)}d\theta=\frac{\pi}{8}$ My ideas, I don't know if they lead anywhere: Let's substitute $\cos(\theta)=\frac{e^{i\theta}+e^{-i\theta}}{2}$ and $z=e^{i\theta}$ right after: $\displaystyle\int_0^\pi\frac{2\cos(2\theta)+\cos(3\theta)}{5+4\cos(\theta)}d\theta=-i\cdot\int_1^{-1}\frac{z^2+z^{-2}+\frac{1}{2}z^3+\frac{1}{2}z^{-3}}{5z+2z^2+2}dz$ This now gives me 4 new integrals, for example $\displaystyle-i\int_1^{-1}\frac{z^2}{2z^2+5z+2}dz$, $\displaystyle-i\int_1^{-1}\frac{1}{2z^4+5z^3+2z^2}dz$ and so on. But since I haven't been able to solve any of the new integrals, I'm a little lost. Edit: Can't I do a partial fractions decomposition of all the 4 integrals and solve them seperately?","['complex-analysis', 'integration', 'definite-integrals', 'contour-integration']"
2364428,Find all numbers that are their own multiplicative inverse in mod p where p is prime. [duplicate],"This question already has answers here : Proving that an integral domain has at most two elements that satisfy the equation $x^2 = 1$. (5 answers) Closed 6 years ago . Find all numbers that are their own multiplicative inverse in $mod$ $p$ where $p$ is prime. I recall that when $p$ is prime, all integers from 1 to the modulus minus 1, so all numbers from $1$ to $p-1$, have multiplicative inverses in mod $p$. So, the numbers that are their own multiplicative inverse would be $1$ and $1-p$. Can someone please explain why?","['prime-numbers', 'modular-arithmetic', 'discrete-mathematics']"
2364447,Properties of the Floquet Generator,"Consider the differential equation
$$ \vec x'(t) = A(t) \vec x(t) , $$
where $A(t)$ is periodic: $A(t+T) = A(t)$.
Let $\Phi(t)$ be its fundamental solution, i.e. $\Phi(0) = I$ and $\Phi'(t) = A(t) \cdot \Phi(t)$. The Floquet Theorem tells us that
$$ \Phi(t) = Q(t) \cdot \mathrm e^{B t} , $$
where $Q(t+T) = Q(t)$ and $B$ is the time-independent Floquet generator.
(To prove the theorem, choose $B$ such that $\mathrm e^{BT} = \Phi(T)$ and define $Q(t) = \Phi(t) \cdot \mathrm e^{-B t}$.) My question is:
What is known about the properties of $B$ depending on the properties of $A$? For example, If every $A(t)$ is skew-adjoint, I know that $\Phi(T)$ is unitary and I can choose $B$ to be skew-adjoint as well. If every $A(t)$ is a right (doubly) stochastic matrix, can $B$ always be chosen to be a right (doubly) stochastic matrix? If the spectrum of $A(t)$ is in the left-half plane $\{ z \in \mathbb C: \Re(z) < 0 \}$ for every $t$, does the same hold for $B$?
Or similarly, if every $A(t)$ is the generator of a contractive semigroup, is $B$ as well? Are there any general results or techniques?","['matrices', 'periodic-functions', 'ordinary-differential-equations', 'analysis']"
2364494,Prove $D_{24}$ is not isomorphic to $S_4$.,"The question from ""Abstract Algebra,"" by Dummit & Foote is: Prove that the dihedral group $D_{24}$ is not isomorphic to $S_4$ . We can express the presentation of $$D_{24}=\{r,s | r^{12}=s^2=1, rs=sr^{-1}\}.$$ Obviously for some permutation $\sigma \in S_4, \sigma ^4=1$ and $(1 2)(1 2)=1$ . So $S_4$ can also be expressed as $$S_4=\{ \sigma, k | \sigma^4=k^2=1\},$$ where $k=(1 2)$ so $D_{24}$ is homomorphic to $S_4$ . Also, $|D_{24}|=24$ and $S_4=4!$ so $D_{24} \cong S_4$ . This is contradicting the question. Why am I wrong? Thanks in advance!","['finite-groups', 'group-homomorphism', 'group-theory', 'group-isomorphism']"
2364534,Variance of Chi Square Distribution as the Sum of Unit Normal Random Variables,"Okay, so I am interested if there is a way to derive the variance for a Chi-Square distribution using the property that it is the sum of independent unit normal distributions squared. For example, if $X$ is a Chi-Square random variable with $n$ degrees of freedom, it has the distribution: $\displaystyle\operatorname{}\left(\sum_{i=1}^n Z_i^2\right)$ where $Z$ is Normal$(0,1)$ I know that $Var(X)= E(X^{2}) -[E(X)]^{2}$ To start finding $E(X)$ I begin with the fact that each $Z$ has $E(Z)=0$ and $Var(Z)=1$. This implies that $E(Z^2)=1$ since $Var(Z)=E(Z^{2})-[E(Z)]^2$ Since $X = \displaystyle\operatorname{}\left(\sum_{i=1}^n Z_i^2\right)$ then $E(X)=\displaystyle\operatorname{}\left(\sum_{i=1}^n 1\right)=n$ I am lost on what the next step would be. I have this start, but don't where to go next. Any thoughts on how to find $E(X^{2})$? $\begin{align}
X^2 =& (\sum_{i=1}^nZ_i^2)^2\\
=& \sum_{i=1}^nZ_i^4+\sum_{i \neq j}^nZ_i^2Z_j^2
\end{align}$ I can't see any way this sum not getting nasty. Note: I can solve this using integration of the PDF for the Chi-Square distribution, but I was wondering if there is any way to do it using the property that Chi-Squared is sum of Squared Normal.","['covariance', 'probability-distributions', 'statistics', 'probability', 'random-variables']"
2364549,Given $1 \le |z| \le 7$ Find least and Greatest values of $\left|\frac{z}{4}+\frac{6}{z}\right|$,Given $1 \le |z| \le 7$ Find least and Greatest values of $\left|\frac{z}{4}+\frac{6}{z}\right|$ I have taken $z=r e^{i \theta}$ $\implies$ $1 \le r \le 7$ Now $$\left|\frac{z}{4}+\frac{6}{z}\right|=\left|\frac{r \cos \theta}{4}+\frac{ir \sin \theta}{4}+\frac{6 \cos \theta}{r}-\frac{6 i \sin \theta}{r} \right|$$ So $$\left|\frac{z}{4}+\frac{6}{z}\right|=\sqrt{\frac{r^2}{16}+\frac{36}{r^2}+3 \cos (2\theta)}$$ any clue from here?,"['complex-analysis', 'trigonometry', 'complex-numbers']"
2364572,Find all possible values of $x+y+z$.,"Let $x, y, z$ be non-zero integers such that ${x\over y}+{y\over z}+{z\over x}$ is an integer. Find all possible values of $x+y+z$. Please provide a proof with all solutions.","['number-theory', 'integers', 'fractions']"
2364588,Prove series $\sum \frac{a_i}{1+|a_i|}$ converges,"If $\sum_{i=1}^n a_i$ converges, does $\sum_{i=1}^n \frac{a_i}{1+|a_i|}$ always converge? If not, please give a counter-example. Thanks.","['sequences-and-series', 'convergence-divergence']"
2364614,How to obtain a countable dense subset disjoint from the closure of a free sequence,"I've encountered some dificulties while reading the article, Hereditary normality versus countable tightness in countably compact spaces, from Nyikos (1992). In particular in a passage of the Reduction Theorem where the author makes the assertion that, given a separable, countably compact, $T_5$ topological space $X$ with an uncountable free sequence, we can take $W = \{x_{\alpha} : \alpha < \omega_{1}\}$ free sequence in $X$ and $D \subset X$ countable dense such that $D \cap \overline{W} = \emptyset$. If i could verify that every $d \in D$ is such that there is an ordinal $\beta < \omega_{1}$ such that $d \in \overline{\{x_{\alpha} : \alpha < \beta\}}$ then we could construct a new the free sequence by shifting the starting point of the old one. The point is that the case above is not necessarily true. Other than that I can't see how the countable dense interacts with the initial free sequence in a way that helps the problem.","['general-topology', 'set-theory', 'separable-spaces']"
2364630,Chicken population growth,"A chicken starts laying eggs at age 1 year old.
Given enough food, a chicken lays 1 egg per day until 8 years old. How often can a population of chickens double in population? Just Curious, Boston.",['ordinary-differential-equations']
2364645,Is the entropy function over the simplex differentiable?,"Consider the entropy function over the simplex, $f(x) = \sum\limits_{i = 1}^J x_i \log(x_i), x_i \geq 0, \sum\limits_{i = 1}^J x_i =1$ and $f(x) = +\infty$ everywhere else This function is given in Boyd and Vanderberg's book Pg. 93. My question is whether this function is differentiable, i.e. does $\nabla f(x)$ exist? I was guessing it does exist, because taking a derivative with respect to each $x_i$ gives me $(\nabla f(x))_i = (1 + \log(x_i))$. But I saw some discussion online (can't find it now, maybe here: https://johncarlosbaez.wordpress.com/ ) that this function is not differentiable. Which also makes sense, because the sharp corner of this function. So if it is not differentiable, then what is the $(1 + \log(x_i))$ term after I take its gradient as usual?","['derivatives', 'real-analysis', 'convex-analysis', 'functional-analysis', 'convex-optimization']"
2364655,If the adjoint of an operator is bounded is the operator as well?,"Let $X$ and $Y$ be normed vector spaces. Let $T: X \to Y$ be a linear operator. Let $T^* : Y^* \to X^*$ be the adjoint of $T$ defined by $T^*(f) =f \cdot T$. Show that if $T^*$ is bounded then $T$ is bounded. I know that the converse of this statement is true but I'm not sure about this direction. If it is not true, I am wondering if there is some extra hypothesis which makes it true.","['real-analysis', 'normed-spaces', 'operator-theory', 'functional-analysis', 'vector-spaces']"
2364668,Flow of constant vector field on $\mathbb{S}^n$ (Jost: Exercise 1.16),"This is a problem that comes from Jost's Riemannian Geometry and Geometric Analysis (Chapter 1, Exercise 16). We consider the constant vector field $X(x)=a$ for all $x\in\mathbb{R}^{n+1}$. We obtain a vector field $\tilde{X}(x)$ on $\mathbb{S}^n$ by projecting $X(x)$ onto $T_x\mathbb{S}^n$ for $x\in\mathbb{S}^n$. Determine the corresponding flow on $\mathbb{S}^n$. I've been stuck on this for a while, and haven't been sure exactly what to try. This is the partial work that I have so far: Let $\mathbf{1}_{n+1}$ be the $(n+1)\times(n+1)$ identity matrix. Note that the projection onto the tangent space $T_x\mathbb{S}^n$ is simply given by
$$P_x:=\mathbf{1}_{n+1}-xx^\top:\mathbb{R}^{n+1}\to T_x\mathbb{S}^n.$$
Now we see that 
$$\tilde{X}(x)=P_x(X(x))=P_x(a)=(\mathbf{1}_{n+1} - xx^\top)a = a - xx^\top a = a - \langle{a,x}\rangle x\in T_x\mathbb{S}^n.$$
So for any $p\in\mathbb{S}^n$ we are looking for a solution to the following ODE
$$\begin{cases}\dot\gamma(t)=\tilde{X}(\gamma(t))=a-\langle{a,\gamma(t)}\rangle\gamma(t),\\\gamma(0)=p.\end{cases}$$
We have two fixed point solutions given by $\gamma_{\pm}(t)=\pm\frac{a}{\|a\|}.$ I'm pretty sure (although I haven't shown this yet) that if we start from any point that isn't a fixed point that the flow will tend to $\frac{a}{\|a\|}$. At first I thought that the flow might be related to the projection of the flow of $X$ in $\mathbb{R}^{n+1}$, i.e. take $\Gamma:\mathbb{R}\to\mathbb{R}^{n+1}$ given by $\Gamma(t)=p + ta$, and then see if $\gamma(t):=\frac{\Gamma(t)}{\|\Gamma(t)\|}$ solves the ODE. Unfortunately, it doesn't solve it on $\mathbb{S}^1\subseteq\mathbb{R}^2$ (although a reparameterization would probably do the trick in this case since $\mathbb{S}^1$ is $1$-dimensional and the direction of the flow seems to be correct), and in any higher dimensions it doesn't even flow in the right direction. So I'm pretty sure this idea is incorrect, and I wasn't able to get it to work at all. The second idea I had was to consider the geodesic starting at $p$ and in the direction $\dot\gamma(0)=a-\langle{a,p}\rangle p\in T_p\mathbb{S}^n$, and maybe reparameterize it if need be. Unfortunately, this also didn't work and I wasn't able to figure out whether the reparameterization idea would work here. Other than that, I've been pretty stuck and I'm not sure if I'm supposed to be able to just be able to solve this ODE directly or if there is some geometric insight that I can use to determine this flow. Any hints and explanations would be extremely appreciated! I've been working on this for a long time, and this is the only question in this chapter I haven't been able to figure out.","['riemannian-geometry', 'ordinary-differential-equations', 'differential-geometry']"
2364669,"The 52 cards from a standard deck are distributed to four distinct people, 13 to each.","Find the probability that each person gets at least three cards from each suit. My professor gave us two different answers, both of which I don't understand. I would like to figure out if which one is correct and why OR if there is another answer. First attempt:
$$\frac{\left[4\binom{13}{4,3,3,3}\right]^{4}}{\binom{52}{13,13,13,13}}$$ Second attempt:
$$\frac{4!\binom{13}{4,3,3,3}^{4}}{\binom{52}{13,13,13,13}}$$ What in the world is going on with this problem. Please help and thank you!","['combinations', 'multinomial-coefficients', 'probability', 'combinatorics', 'discrete-mathematics']"
2364687,"If $M^\perp$ consists only of the zero vector, then is $M$ total in $X$?","Let $X$ be an inner product space, and let $M$ be a non-empty subset of $X$. Then $M$ is said to be total in $X$ if the span of $M$ is dense in $X$. We have the following result: If  $M$ is total in $X$, then $M^\perp$ consists only of the zero vector in $X$. Does the converse hold too? I know that the anwer is in the affirmative if $X$ is also a Hilbert space. What is the situation if $X$ is an inner product space but not a Hilbert space? By definition, 
$$ M^\perp \colon= \{ \ x \in X \ \colon \ \langle x, v \rangle = 0 \ \mbox{ for all } \ v \in M \ \}.$$
And, $M^\perp$ is a (vector) subspace of $X$ and is a closed set in the metric space induced by the inner product.","['real-analysis', 'hilbert-spaces', 'functional-analysis', 'inner-products', 'analysis']"
2364688,"If $f(x)$ is continuously decreasing and $\lim_{x \to \infty} f(x) = 0$, is $xf(x)$ uniformly continuous?","Suppose $f:[0, \infty) \to [0, \infty)$ is decreasing and continuous with $\lim_{x \to \infty} f(x) = 0$. Let $g(x) = xf(x)$. Is $g(x)$ uniformly continuous on $[0, \infty)$? My work: I've been able to come up with a counterexample where $g(x)$ is not Lipschitz . Since Lipschitz is stronger than uniform continuity, this isn't a full solution, but nevertheless the ideas may be useful. That said, there might be a very simple counterexample that's eluding me . First, recall that there are continuous functions $h:[0, \infty) \to [0, \infty)$ such that $$\limsup_{x \to +\infty} \ h(x) = +\infty$$ but $\int_0^{+\infty} h(x) \ dx < +\infty$ Here is a brief construction: for integer $n \geq 1$, in each interval $[n, n+\frac{2^{1-n}}{n}]$ the graph of $h$ looks like an isosceles triangle of area $2^{-n}$ and height $n$. Elsewhere $h(x) = 0$. It's easy to see the integral of $h$ over $\mathbb{R}_+$ is $1$, but its $\lim \sup$ is $+\infty$. Now consider that $$f(x) = 1 - \int_0^{x} h(t) \ dt$$ is continuous (actually differentiable), decreasing and vanishing at $+\infty$. If we consider $g(x) = xf(x)$, note that $g'(x) = xf'(x) + f(x) = f(x) - xh(x)$. $f(x)$ is bounded and $|xh(x)|$ can be made arbitrarily large, and hence $|g'|$ can be made arbitrarily large, so there is no uniform bound on $\left|\frac{g(x) - g(y)}{x-y}\right|$. This implies $g$ is not Lipschitz. This example may or may not be uniformly continuous, but I can't prove it.","['continuity', 'real-analysis', 'uniform-continuity', 'limits']"
2364690,mn is is even iff m is even or n is even,"The problem states: Let $mn$ be integers. Show that $mn$ is even if and only if $m$ is even or $n$ is even. They are asking to prove an iff statement. So it can be said that $P→Q$ and $Q→P$. I can prove that $¬P→¬Q$ and $¬Q→¬P$ as far is I understood. I tried it like this: $¬P→¬Q$: $mn$ is odd if $m$ and $n$ are odd. An odd integer can be represented as $2x + 1$, so we can rewrite $mn$ as $(2x + 1)(2y + 1) =  2(2xy + x + y) + 1$ which is odd. $¬Q→¬P$: if $m$ and $n$ are odd, then $mn$ is odd. How to write this then? Isn't that essentially the same as the previous? Or did I misunderstand how to prove iff in this case.","['logic', 'proof-verification', 'discrete-mathematics']"
2364705,Frobenius’ Method to Solve an ODE (Hydrogen Atom - Radial Equation),"My goal is to find two linearly independent solutions to the ODE $$
r^2\frac{d^2R}{dr^2}+2r\frac{dR}{dr}+[r^2+\lambda r-l(l+1)]R=0
$$ in the interval $[0,\infty)\ni r$, where $R=R(r)$, and $\lambda\in\mathbb{R}_+$ and $l\in\mathbb{Z}_0$ are fixed constants. Just to put in context my problem, this ODE determined the radial component of the wavefunction of an electron in an atom. The standard approach (e.g. here ) is to investigate the asymptotic behaviour of the equation for large and small $r$, factor out this behaviour and then use a series to try to find the solution of what's left. Nonetheless, I'm looking for an answer of the form: ''The general solution to this ODE is given by $R(r)=c_1y_1(r)+c_2y_2$, but since $R(0)$ must be well-defined and $y_2(r)\to-\infty$ as $r\to0$, then $c_2=0$ for the answer to be physically acceptable''. I know that the ''physically acceptable'' solution is given by $$
y_1\propto\left(\frac{2r}{n}\right)^le^{-r/n}L^{2l+1}_{n+l}\left(\frac{2r}{n}\right)
$$
for all $n\in\mathbb{Z}_+$, and where $L^{2l+1}_{n+l}$ are the associated Laguerre polynomials. My attempt to find $y_1$ and $y_2$ is to use Frobenius’ method, but i keep stuck at some point: since $r=0$ is a regular singular point of the differential equation, there exists at least one solution of the form $$
R(r)=\sum_{k=0}^\infty c_kr^{k+s}
$$ where $s$ are indicial roots to be found. Then $$
\frac{dR}{dr}(r)=\sum_{k=0}^\infty (k+s)c_kr^{k+s-1}\quad\text{and}\quad\frac{d^2R}{dr^2}(r)=\sum_{k=0}^\infty (k+s)(k+s-1)c_kr^{k+s-2}
$$ and so, substituting in the equation, it yields \begin{align} 
0&=r^s\Biggl[c_0[s(s+1)-l(l+1)]+(c_1[(s+1)(s+2)-l(l+1)]+\lambda c_0)r \\
 &\quad+\sum_{k=2}^\infty\Bigl(c_k[(k+s)(k+s-1)+2(k+s)-l(l+1)]+c_{k-2}+\lambda c_{k-1}\Bigr)r^k\Biggr] 
\end{align} The first term gives me the indicial roots $$
c_0[s(s+1)-l(l+1)]=0\quad\Rightarrow\quad s_1=l\quad\text{and}\quad s_2=-l-1
$$ and because nothing is gained by taking $c_0=0$. Now, since $s_1$ and $s_2$ are distinct and the difference $s_1-s_2=2l+1$ is a positive integer, then there exist two linearly independent solutions of the form $$
R^1(r)=\sum_{k=0}^\infty c_kr^{k+l},\quad c_0\neq0
$$ and $$
R^2(r)=CR^1(r)\ln(r)+\sum_{k=0}^\infty b_kr^{k-l-1}
$$ where $C$ is a constant that could be zero (I'm following the book ''Differential Equations with Boundary-Value Problems; DENNIS G. ZILL and MICHAEL R. CULLEN''.) For the second term $$
c_1[(s+1)(s+2)-l(l+1)]+\lambda c_0=0
$$ from where $$
c_1=\frac{l+l^2-\lambda c_0}{2+3l+l^2}\quad\text{for $s_1=l$},\quad\text{and}\quad c_1=\frac{l+l^2-\lambda c_0}{l(l-1)}\quad\text{for $s_2=-l-1$}
$$ For $k\geq2$, I have $$
c_k[(k+s)(k+s-1)+2(k+s)-l(l+1)]+c_{k-2}+\lambda c_{k-1}=0
$$ How can I continue from here to find the solution/series $R^1(r)$ and $R^2(r)$ ? How are they ? Is there a better way to proceed in order to find $R^1(r)$ and $R^2(r)$ ? Thanks in advance.","['special-functions', 'ordinary-differential-equations', 'quantum-groups', 'partial-differential-equations']"
2364721,Prove that $\frac{1}{15}<\frac{1}{2}*\frac{3}{4}* \dots *\frac{99}{100}<\frac{1}{10}$ [duplicate],"This question already has answers here : show $\frac{1}{15}< \frac{1}{2}\times\frac{3}{4}\times\cdots\times\frac{99}{100}<\frac{1}{10}$ is true (2 answers) Closed 6 years ago . Prove that $\frac{1}{15}<\frac{1}{2}*\frac{3}{4}* \dots *\frac{99}{100}<\frac{1}{10}$ My attempt :If we name the value $A$ we have: $A^2<\left(\frac{1}{2}*\frac{3}{4}* \dots *\frac{99}{100}\right)\left(\frac{2}{3}*\frac{4}{5}* \dots \frac{100}{101}\right)=\frac{1}{101} \Rightarrow A<\frac{1}{10}$ But I don't know, how to prove the other side?","['inequality', 'number-comparison', 'algebra-precalculus', 'products', 'fractions']"
2364728,3 variable multiplication with 1 constraint lagrange multiplier,"Using Lagrange multipliers, I need to calculate all points $(x,y,z)$ such that $$x^4y^6z^2$$ has a maximum or a minimum subject to the constraint that $$x^2 + y^2 + z^2 = 1$$ So, $f(x,y,z) = x^4y^6z^2 $ and $g(x,y,z) = x^2 + y^2 + z^2 - 1$ then i've done the partial derivatives $$\frac{\partial f}{\partial x}(x,y,z)=\lambda\frac{\partial g}{\partial x}$$ which gives $$4x^3y^6z^2 = 2xλ$$ $$6x^4y^5z^2 = 2yλ$$ $$2x^4y^6z = 2zλ$$ which i subsequently go on to find that $3x^2 = 2y^2 = 6z^2 $ This is where i've hit a dead end. Where do i go from here? or am i doing it all wrong? Thanks.","['a.m.-g.m.-inequality', 'optimization', 'multivariable-calculus', 'maxima-minima', 'lagrange-multiplier']"
2364787,Basics on Douady spaces,"I am interested in some basic references for Douady spaces (which are analytic analogies of Hilbert schemes). The point is that I would be happy to work with Douady spaces of non-Kähler (even not Moishezon or Fujiki-$\mathcal{C}$) spaces. I hope the whole intuition coming from Hilbert schemes(their existence, compactness, smooth points, dimension of tangent bundle etc.)will stay relevant, but wanted to get sure that I didn't miss any hidden rock.","['complex-geometry', 'reference-request', 'differential-geometry']"
2364823,Problem related to integrals and probability densities.,"Suppose $X_1, X_2$ are independent random variables, with the same support $[0,1]$, on the same probability space with densities $f_1,f_2$ respectively. By support I mean, $f_i$'s are $0$ outside $[0,1]$. We have that $$\int_{x} f_1(x)f_2(z-x) \,dx = \int_{x} g_1(x)g_2(z-x) \,dx$$ for all $z \in [0,2]$ and $g_i(y) = f_i(1-y)$ for both $i=\{1,2\}$. We also have that $$ f_i(x) \leq f_i(1-x), \forall x \in [0.5,1]$$ for both $i=\{1,2\}$. I want to conclude that $$ f_1(x)f_2(z-x) = f_1(1-x)f_2(1-(z-x)) $$ for some $z$. My try: Rewrite (1) as $$\int_{x} (f_1(1-x)f_2(1-(z-x)) - f_1(x)f_2(z-x)) \,dx = 0. \quad (*)$$ For $z=1.5$, since we can restrict our interest to $0\leq z-x \leq 1$, we'll have both $x$ and $(z-x)$ exceeding $0.5$. Now from (2) we'll have $$ f_1(x) \leq f_1(1-x) ~\text{and} \\ f_2(z-x) \leq f_2(1-(z-x)).$$ So $$f_1(1-x)f_2(1-(z-x)) - f_1(x)f_2(z-x) \geq 0.$$ With $(*)$ we can in fact conclude $$ f_1(x)f_2(z-x) = f_1(1-x)f_2(1-(z-x)).$$ Now if the above conclusion holds can we say more? That is, from (2) I want to further conclude that $$ f_1(x) = f_1(1-x) $$ and $$f_2(z-x) =f_2(1-(z-x))$$ for some $z$. Please comment on both the above conclusions I made. Thanks in advance for any help! Please feel free to make any further conclusions from these facts too, it'd be interesting to know them.","['density-function', 'probability-theory', 'calculus', 'probability', 'measure-theory']"
2364824,Sheaf of flat modules,"Let $X$ be a scheme. Can someone give me an (non-trivial) example of a sheaf $F$ of $\mathcal O_X$-module such that $F(U)$ is a flat $\mathcal{O}_X(U)$-module for every $U$? Is there a notion for such sheaves in literature? A trivial example can be: Let $X$ be an integral scheme and $K$ be the function field of $X$. Then the locally constant sheaf defined by $F(U) = K$ for any affine $U$ will be such an example. This is because for any affine $U = Spec(A)$, the functional field $K$ is indeed the field of fraction of $A$ and $A \hookrightarrow K$ is certainly flat. P.S: I am aware of this definition: Let $f:X \rightarrow Y$ be a morphism of schemes and $F$ be a sheaf on $X$. Then $F$ is flat over $Y$ if for every $x\in X$, the stalk $F_x$ is a flat $\mathcal O_{Y,f(x)}$-module. But I don't think this notion is equivalent to my question . Thanks in advance!",['algebraic-geometry']
2364905,"Do local isometries form a group? If so, what is the group for Euclidean space?","Burago, Burago, and Ivanov use the following definition on p.78, for metric spaces $(X,d)$ , $(Y,\delta)$ : A map $f: X \to Y$ is called a local isometry at $x \in X$ if $x$ has a neighborhood $U_x$ such that (the restriction of) $f$ maps $U_x$ isometrically onto an open set $U_y$ in $Y$ . Based on this, I make the following definition: A map $f: X \to X$ is called a local self -isometry at $x \in X$ , or perhaps a local isometry anchored at $x \in X$ , if $x$ has a neighborhood $U_x$ such that (the restriction of) $f$ maps $U_x$ isometrically onto a (possibly different) neighborhood $V_x$ of $x$ , and if the map $f$ fixes $x$ , i.e. $f(x) = x$ . EDIT: We could also say "" pointed local isometries (autometries)"", mirroring the terminology used in Theorem 10.10.1 on p.398 of Burago, Burago, Ivanov (""pointed homeomorphism""). /EDIT If we don't require that $f$ fixes $x$ , then $f$ could map onto a neighborhood of $x$ in a way which (""morally"") is not ""centered/focused"" around $x$ (e.g. in Euclidean space a translation of $U_x$ to another neighborhood in which $x$ is closer or further from the boundary than before). (The condition doesn't seem necessary to make the set a group, however, see below. Thus its purpose should be understood solely as a heuristic to avoid and exclude local isometries in which the presence of $x$ in the domain and range/image isn't an ""afterthought"", like for translations.) Question: (a) Given a metric space $(X,d)$ and a point $x \in X$ , does the set of all local self -isometries at $x$ form a group (which one might call the local isometry group at $x$ )? (b) If the answer to (a) is affirmative, what is this group for any point in Euclidean space? (It has to be the same at each point because of homogeneity.) Is it just $O(n)$ ? Attempt: (a) Any isometry $f: X \to X$ is bijective, so has an inverse $f^{-1}: X \to X$ , which is also clearly an isometry, since $f$ is. $d(f^{-1}(x_1),f^{-1}(x_2)) = d(f^{-1}(f(y_1)), f^{-1}(f(y_2))) = d(y_1, y_2)$ , where $y_1$ and $y_2$ are the points such that $f(y_1) = x_1, f(y_2) = x_2$ , and since $f$ is an isometry, $d(x_1, x_2) = d(f(x_1),f(x_2)) = d(y_1, y_2) = d(f^{-1}(x_1), f^{-1}(x_2))$ , thus $f^{-1}$ is an isometry. Now assume that $f$ is a local self-isometry at $x$ . Then $f$ restricts to an isometry of $U_x$ onto $V_x$ , for $U_x$ and $V_x$ neighborhoods of $x$ . But that means that $f^{-1}$ restricts to an isometry of $V_x$ onto $U_x$ , so $f^{-1}$ is a local self-isometry at $x$ , so the set of all local self-isometries at $x$ is closed under inverses. (Since $f(x)=x$ implies that $f^{-1}(x) = f^{-1}(f(x)) = x$ .) The identity is obviously a local self-isometry at $x$ for any point $x \in X$ (because it maps any neighborhood $U_x$ of $x$ isometrically onto itself and fixes every point), and associativity follows from the associativity of function composition. So the set of local self-isometries at $x$ is a group. (b) According to this document, exercise 40 on p. 9 , every local isometry of Euclidean space extends to a global isometry. Therefore, it should be the case that any local self-isometry of Euclidean space extends to a global isometry which fixes $x$ . Perhaps another way to prove this is true is to note that every neighborhood of $x$ is homeomorphic to Euclidean space itself, thus in particular locally homeomorphic. Moreover, since Euclidean space is a length space , the metric coincides with the intrinsic metric , so every isometry is also an arcwise isometry . Then one might be able to  use the result that every local homeomorphism which is an arcwise isometry is a local isometry to show the local isometry group is isomorphic to the group of (global) isometries for Euclidean space which fix $x$ . This question might also be relevant to finding an answer. Anyway, since for any point $x$ in Euclidean space, the group of (global) isometries fixing $x$ is isomorphic to the orthogonal group $O(n)$ , it would follow, if either of the above two arguments are correct, that the local isometry group at $x$ is isomorphic to $O(n)$ . Motivation: The notion of a ""pointed local autometry group"" is an attempt to fix this deficiencies of using ""pointed (global) autometry groups"", as pointed out here , for trying to define a notion of direction in arbitrary metric spaces , as well as of angle measure .","['metric-spaces', 'metric-geometry', 'proof-verification', 'geometry']"
2364927,"If $A\approx A', B\approx B'$, then $B^A\approx B'^{A'}$","My attempt: Suppose $A\approx A', B\approx B'$, then there exists bijections $f_A:A\to A',f_B:B \to B'$. We need to show that $B^A\approx B'^{A'}$. That is, there is a bijection from the set $F$ of all functions from $A$ to $B$ to the set $F'$ of functions from $A'$ to $B'$. Define function $g:F\to F'$ as: for each function from $f\in F$, for $f(a)=b$ where $a\in A$ and $b\in B$, $g(f)=f'\in F'$ send $f_A(a)\in A'$ to $f_B(b)\in B'$. But I have trouble proving that the $g$ we defined is a bijection. I think it would be better if we describe $g$ in a more formal way instead of giving a plain sentence. So how may I prove it, or if there is some better way to do that? Thanks. EDIT： Following the hint from the comment. The following is my attempt to define the inverse of $g$. For each function $f'\in F'$. Define $h(f')$ as: for each function $f'$ in $F'$ such that $f'(a')=b'$, $h(f')$ sends $f^{-1}_A(a')$ to $f^{-1}_B(b')$. We can do this because the function $f$ is invertible. I still think in order to prove formally that $h$ and $g$ are inverses of each other, we need to formalize the definition of both of them... So could someone tell me if we could do this? Or is there other way to verify that they are inverses? EDIT': Given $f\colon A\to B$, then the composition $f\circ f_A^{-1}$ is a function from $A'$ to $B$; and if we then compose $f_B$ on top of this we get a function $f_B\circ f\circ f_A^{-1}$ which is indeed from $A'$ to $B'$. Define $g: F\to F'$ as $g(f)=f_B\circ f\circ f_A^{-1}$. We prove that $g$ is a bijection. Suppose $g(f_1)=g(f_2)$, then for every $a'\in A'$ we have that $g(f_1)(a')=g(f_2)(a')$. Thus $f_B\circ f_1\circ f^{-1}_A(a')=f_B\circ f_2\circ f^{-1}_A(a')$. Use the fact that $f_B$ is bijective, we have $f_1(f_A^{-1}(a'))=f_2(f_A^{-1}(a'))$ for all $a'\in A'$. As $f^{-1}_A$ is surjective, $f_1(a)=f_2(a)$ for each $a\in A$ and therefore $f_1=f_2$. For each $f'\in F'$, we need to prove that there exist $f\in F$ such that $g(f)=f'$. For $f'\in F'$, define $f$ as $f^{-1}_B\circ f'\circ f_A$. Thus $f\in F$ and $g(f)=f'$ as desired. This proves $g$ is surjective.","['cardinals', 'elementary-set-theory']"
2364931,A question about quasi-nilpotent operators,"Let $X$ be an infinite dimensional Banach space, and let $Q\in B(X)$ be a bounded quasi-nilpotent operator ($\sigma(Q)=\{0\}$). I am trying to prove that for every $\epsilon >0$ we can find an infinite dimensional subspace $Y\subset X$ such that the restriction $\left. Q\right\vert _{Y}:Y\rightarrow X$ of $Q$ to $Y$ is such that $\left\Vert \left.Q\right\vert _{Y}\right\Vert <\epsilon $. 
Any help please ?
Thank you.","['banach-spaces', 'operator-theory', 'functional-analysis', 'linear-transformations', 'spectral-theory']"
2364932,Help with an expectation of log beta (or Gamma) function,"Suppose a vector ${\bf x} = (m,1-m) \times s$  where $(m,1-m)$ is distributed according to a beta distribution beta$(a,b)$ with density $f(m,1-m) = \frac{m^{a-1}(1-m)^{b-1}} {B (a,b)}$, $s$ is distributed according to a Gamma distribution gamma$(\alpha, \beta)$ with density $g(s) = {\frac {\beta ^{\alpha }}{\Gamma (\alpha )}}s^{\alpha \,-\,1}e^{-\beta s}$. All parameters $a,b,\alpha,\beta$ are known. Also note $m,s$ are independent. Then the question is: is there anyway to calculate the expectation of log beta function: $\log B ({\bf x})$, which is $\Bbb E [\log \Gamma (sm) + \log \Gamma (s(1-m)) - \log \Gamma (s) ]$ with both $m$ and $s$ as random variables? See also related question . My attempt : I am not sure, but maybe we can use Stirling's approximation That is $\ln \Gamma (x) \approx (x - \frac{1}{2})\ln x - x + \frac{1}{{12(x + 1)}} + \frac{1}{2}\ln 2\pi  + O(x)$ and my application scenario allows dropping the constant addends and the $O(x)$ term. Then, $\ln \Gamma (sm) + \ln \Gamma (s(1 - m)) - \log \Gamma (s) = sm\ln sm - \frac{1}{2}\ln sm + \frac{1}{{12(sm + 1)}} + s(1 - m)\ln s(1 - m) - \frac{1}{2}\ln s(1 - m) + \frac{1}{{12(s(1 - m) + 1)}} - s\ln s - \frac{1}{2}\ln s + \frac{1}{{12(s + 1)}}$ Note $s,m$ are independent random variables, and $\Bbb E[s]$, $\Bbb E[m]$, $\Bbb E[\ln s]$, $\Bbb E[\ln m]$ have known form. The remaining problems are to calculate $\Bbb E[\frac {1}{s+1}]$, $\Bbb E[\frac {1}{sm+1}]$, $\Bbb E[s \ln s]$ and $\Bbb E[m \ln m]$, etc. See the related question .","['statistics', 'probability', 'approximation']"
2364945,Reference request: Inversions and $\sum_{w\in W}q^{\ell(w)}$ for arbitrary Coxeter groups,"For the symmetric group $S_n$, an inversion of a permutation $\pi∈S_n$ is a pair $1\leq i<j\leq n$ such that $\pi(i)>\pi(j)$. It is known that the length $\ell(\pi)$ of a permutation (i.e. the least number of simple transpositions needed to express $\pi$) coincide with its number of inversions. This is useful to compute
$$\sum_{\pi\in S_n}q^{\ell(\pi)} = (n)_q!$$
What happens if I replace $S_n$ by an arbitrary finite Coxeter group? Is there still a notion for an inversion? What is the above sum in this case? Edit Okay, to be able say that I have my homework done, here some thoughts on the problem: A general definition of the inversion number for Weyl groups seems to be the following: The inversion number of a word $w$ is its number of positive roots that are mapped to negative roots. This still equals its length $\ell(w)$. Example: $B_n$ For this group this seems to be easy (since I can draw pictures in my head for what it does…): It is simply the group $S_n$ with one additional generator that flips let's say the first sign in a sequence. Apart from permuting elements, we may flip zero to $n$ signs, where we have $n$ possibilities for the first sign, $n-1$ for the second etc. So for $B_n$, the required polynomial should be $$\sum_{w\in B_n}q^{\ell(w)} = (1+nq + (n-1)q^2 + \cdots + q^n)(n)_q!$$ which coincides with counting the elements explicitly (at least, for $B_2$) Is there a nicer way to express this number? Is there a general systematic, e.g. how to obtain this from the presentation? Is there a listing for other Coxeter groups?","['combinatorics', 'symmetric-groups', 'coxeter-groups']"
2364963,Decomposing a matrix into binary ones,"Let $A$ denote the set of all 3×2 binary matrices (those containing only 0's and 1's) in which the sum of each column adds up to 2. Can I decompose
$$B=\begin{bmatrix}
3/4 & 1/2 \\
3/4 & 3/4 \\
1/2 & 3/4 \end{bmatrix}$$
into a linear combination of matrices in $A$? For example,
$$B=1/2 \begin{bmatrix}1 & 1 \\ 0 & 0 \\ 1 & 1\end{bmatrix} + 1/2 \begin{bmatrix}1 & 0 \\ 0 & 1 \\ 1 & 1\end{bmatrix}$$
or something like that. EDIT: I want to show there is at least a matrix that cannot be decomposed. How about 
$$C=\begin{bmatrix}
3/4 & 2/3 \\
3/4 & 2/3 \\
1/2 & 2/3 \end{bmatrix}$$","['matrices', 'linear-algebra']"
2364992,Showing $\sigma(n!) < \frac{(n+1)!}{2}$,"Prove that $\sigma(n!) < \frac{(n+1)!}{2}$ for all positive integers $n$, where $n \geq 8$. $\sigma(n)$ is sum of positive divisors of $n$. My thought : $n=p_1^{k_1}p_2^{k_2}...p_m^{k_m}$, where $p_1, p_2, ..., p_m$ are primes. $\sigma(n) = \displaystyle\prod_{i=1}^m\left(\frac{p_i^{k_i+1}-1}{p_i-1}\right)$ I think this problem may be solved by using LTE. How can we find $\sigma(n!)$ ?","['number-theory', 'divisor-sum']"
2365003,"Number of connected components of a graph with ""3-partitions"" as vertices and ""doublings"" as edges","Let $n\in\mathbb{N}$ $(=\{0,1,2,3,\dots\})$. Let $G_n$ be a (directed) graph with vertices $$V_n = \{ \{a,b,c\} \subset\mathbb{N} : \space a+b+c = n \}$$ and edges $$E_n = \{  (A, B) \in V^2 : \space B \text{ can be formed from } A \text{ by 'doubling'}\},$$ where 'doubling' means that we double one number of $A$ and take that same amount away from some other number of $A$ (so the sum stays at $n$). Example of doubling (take $2$ from $3$ and add it to $2$): $$ \{ 3, 2, 2 \} \rightarrow \{1, 4, 2\}.$$ Here's an example graph for $n=7$: ( Notes: I have ordered the numbers biggest first in the node sets. Therefore the numbers can change places after forming the doubling. I haven't included loops that are formed when the doubling leads to the same set of number ) Here are some more cases : jsfiddle having pictures of the graphs (use the arrow keys to change $n$). Question: How many (weakly) connected components does $G_n$ have? Denote that number by $c_n$. I have hypothesized that $c_n$ is the number of odd divisors of $n$ ( OEISA001227 ). I have checked this upto $n=200$ and for $n=225$ and $n=315.$ My thoughts: Let's use ordered tuples as the nodes for ease of notation. For odd $n$ the node $(n, 0, 0)$ is by itself since it can't be formed by doubling and doubling it can only lead to itself. So this could correspond to the odd factor $n$. Now I think I got it : A component is formed when each number of each node is divisible by an odd factor $d$ of $n$. The doubling can't lead to other components where some number isn't divisible by $d$ since $ d | (x+2dk) \iff d|x$. In other words the components are formed by considering the greatest common divisor of each node. That divisor must also be a divisor of $n$ since the numbers sum to $n$. On the other hand, we have a (undirected) path between each node that has the same $\gcd$ by ... (solving linear diophantine equation and factors of $2$ don't matter ?) How could one finish this proof?","['discrete-mathematics', 'graph-theory', 'elementary-number-theory']"
2365006,$f\circ f\circ f(x)=x^9$ then $f$ is increasing,"The full statement is: If $f:\Bbb R \to \Bbb R$ is a continuous function and $f\circ f\circ f(x)=x^9$ then $f$ is increasing. $(1)$ I was thinking about suppose that $f$ is decreasing (or constant) and then it is easy to get a contradiction. After that I would like to state that "" If $f:\Bbb R \to \Bbb R$ is continous and not increasing then there is an interval where $f$ is decreasing or constant. "" and then I can use $(1)$ and get the result. But I still can't prove if the statement is true. Any hint or any other solution?","['continuity', 'real-analysis', 'functions']"
2365077,"The reason behind the trick of assuming $p$ and $q$ are independent, differentiating, then applying the relation.","We want to simplify this expression: $$\bar{n}=\sum_{n=0}^N W(n)n=\sum_{n=0}^N\frac{N!}{n!(N-n)!}p^{n}q^{N-n}n$$ where $q=1-p$. The trick used is to write: $$np^{n}=p\frac{\partial}{\partial p}(p^{n})$$ plugging in, $$\bar{n}=\sum_{n=0}^N\frac{N!}{n!(N-n)!}\left[p\frac{\partial}{\partial p}(p^{n})\right]q^{N-n}$$ But the book does something strange here. $$\implies \bar{n}=p\frac{\partial}{\partial p}\left[\sum_{n=0}^N\frac{N!}{n!(N-n)!}p^{n}q^{N-n}\right]$$ the, according to binomial theorem, $$\bar{n}=p\frac{\partial}{\partial p}(p+q)^N$$ $$\bar{n}=pN(p+q)^{N-1}$$ Now, if we use $q=1-p$, $$\bar{n}=pN$$ The question: Why did we consider $p$ and $q$ independent variables and put the partial derivative out of the sum, then differentiate and at last apply the relation between $p$ and $q$? Why are we allowed to do so? (I don't want to know why we decided to do so (then one would simply say ""because otherwise we couldn't solve it!""), but rather what makes us enable to do so.) The book: Fundamentals of Statistical and Thermal Physics, Frederick Reif, First chapter","['multivariable-calculus', 'partial-derivative']"
2365108,Rudin proof: closed subsets of compact sets are compact,"I'm reading (self-study) Rudin's mathematical analysis book and I'm going through the proof of the following theorem (theorem 2.35 p38): Closed subsets of compact sets are compact I will now write out my proof, so can someone verify whether I filled in the details correctly? (so in general, is every step I made valid, are the explanations correct etc. Any questions to test my understanding will be appreciated as well. Proof (edit): Assume $F \subset K \subset X$ with $F$ closed and $K$ compact. Let $\{V_a\}$ be an open cover of $F$. Then $F \subset \bigcup_a V_a$. We prove that there is a finite subcover of $F$, from which compactness will follow. Consider the union $\{V_a\}\cup \{F^c\} := V$. We prove that this is an open cover of $K$. Any set in the collection $\{V_a\}$ is open, by hypothesis that this is an open cover of $F$, and $F^c$ is open, since $F$ is closed. So every set in this collection is open. Since $X = F \cup F^c \subset V \cup F^c$, it follows that $K \subset V \cup F^c $ (as $K \subset X$) and this proves that the union written above is an open cover of $K$. Because $K$ is compact, it follows that there is a finite collection $\Omega \subset V$ which is a subcover of $K$, so $K \subset \bigcup_{A \in \Omega} A$, and because $F \subset K$, it also follows that $F \subset \bigcup_{A \in \Omega} A$. Now, if $F^c \notin \Omega$, then $\Omega = \{V_{a_1}, \dots, V_{a_n}\}$ for open sets $V_{a_1}, \dots, V_{a_n}$, which is a finite subcover of $F$, since $F \subset \bigcup_{A \in \Omega} A$. Otherwise, if $F^c \in \Omega$, then $\Omega = \{F^c, V_{a_1}, \dots, V_{a_m}\}$, but $F \cap F^c = \emptyset$, so $F \subset V_{a_1}\cup \dots \cup V_{a_n}$, such that $\Omega - \{F^c\}$ is a finite subcover of $F$. QED","['general-topology', 'metric-spaces', 'compactness']"
2365114,Introduction to D-modules,"I would like to ask you, how to start learning the theory of D-modules from the point of view of algebraic topology. What are the pre-requisites for that ?, and more important: what is the logical order to study them?. I have a solid background in algebraic geometry and theory of sheaves.","['algebraic-topology', 'abstract-algebra', 'algebraic-geometry']"
2365160,What would be an example of Neumann boundary conditions on a two dimensional domain,"The Wikipedia page says that it would involve the derivative with respect to some normal vector being constant, but I don't quite understand this. Is the value of the normal vector adjusted across the domain, or does it remain the same? Can I say that the standard partial derivative is the derivative with respect to the normal vector if I have a rectangular domain? Thank you for your help.","['multivariable-calculus', 'boundary-value-problem', 'partial-differential-equations']"
2365188,Is there a sequence of real polynomials which converge uniformly on an interval in $\mathbb{R}$ but not on a rectangle in $\mathbb{C}$?,"In particular I wondered about the following: The Weierstrass-function $\mathcal{W}$ is continuous and nowhere differentiable. By the Stone-Weierstrass-Theorem we can approximate $\mathcal{W}$ on $[0,1]$ uniformly by real polynomials. Let $p_n(x)$ be such a sequence of polynomials. Now we consider the $p_n$ as complex polynomials. On $[0,1]$ the $p_n$ of course still converge pointwise to $\mathcal{W}$. On $[0,1]\times i[-\frac{1}{2},\frac{1}{2}]\subseteq\mathbb{C}$ however this convergence can not be uniform anymore, as this would imply holomorphy on $(0,1)\times i(-\frac{1}{2},\frac{1}{2})$ which would imply real differentiability on $(0,1)$. I find this very unintuitive, so i would like to see a concrete example of a sequence of polynomials converging uniformly on some $[a,b]\subseteq \mathbb{R}$ but not converging uniformly on any $[a,b]\times i[-\epsilon,\epsilon]\subseteq\mathbb{C}$, if possible with a direct verification that this is (not) the case.","['complex-analysis', 'polynomials', 'uniform-convergence']"
2365192,"Analytic ""Lagrange"" interpolation for a countably infinite set of points?","Suppose I have a finite set of points on the real plane, and I want to find the univariate polynomial interpolating all of them. Lagrange interpolation gives me the least-degree polynomial going through all of those. Is there an analogous construct for a countably infinite, sparse set of points on the real plane, instead using analytic functions and power series? There is obviously some difficulty in forming a perfect analogy, as Lagrange interpolation yields the ""lowest degree"" polynomial interpolating the points, whereas there is no such thing as a ""lowest degree"" power series. However, perhaps there is some generalized measure of the complexity of a power series that is decently workable, and which restricts to the lowest-degree polynomial in the finite case. If so, how does this work? Is there an easy way to obtain the nth coefficient of the power series from the points?","['real-analysis', 'analytic-functions', 'interpolation-theory', 'lagrange-interpolation']"
2365217,Differential Equation $4(x-2)^2\frac{dy}{dx}=(x+y-1)^2$,"How to proceed with the following differential equation? $$4(x-2)^2\dfrac{dy}{dx}=(x+y-1)^2$$ Can this be solved with separable variable method after some substitution. Trying to substitute $(x+y-1) = v$ does not seem to help. Any ideas? ========== Edit ========== With the help of Jaideep's answer, I think following is how to proceed further : $$u\frac{dk}{du}=\frac{(k+1)^2}{4}-k$$
$$\Rightarrow u\frac{dk}{du}=\frac{(k-1)^2}{4}$$
$$\Rightarrow \frac{dk}{(k-1)^2}=\frac{du}{4u}$$
$$Integrating,\ \frac{-1}{(k-1)}=\frac{\log u}{4} + \frac{\log c}{4}$$
$$\Rightarrow \log c(x-2)=\frac{4(2-x)}{y-x+1}$$ I think this is correct, but I am not able to understand how Wolfram|Alpha got its solution.","['ordinary-differential-equations', 'calculus']"
2365228,Find n so that the following converges $\int_1^{+ \infty} \left( \frac{nx^2}{x^3 + 1} - \frac 1 {13x + 1} \right) dx$,"Question Determine $n$ such that the following improper integral is convergent $$  \int_1^{+ \infty} \left(
    \frac{nx^2}{x^3 + 1} 
    -\frac{1}{13x + 1} 
  \right) dx
$$ I'm not sure how to go about this. Working This is convergent if $$
  \lim_{b \to + \infty} \int_1^b \left(\frac{nx^2}{x^3 + 1} - \frac{1}{13x + 1} \right) dx
$$ exists. The indefinite integral is \begin{equation*}
  \begin{aligned}
    \int \left(\frac{nx^2}{x^3 + 1} - \frac{1}{13x + 1} \right)dx
    &  = \int \left(\frac{nx^2}{x^3 + 1}
    \right) - \int \left(\frac{1}{13x + 1} \right)dx \\
    &= \frac{n}{3} \cdot \ln(x^3 + 1) - \frac{1}{13} \cdot \ln(13x + 1) 
  \end{aligned}
\end{equation*} Which gives \begin{equation*}
  \begin{aligned}
    &\lim_{b \to + \infty}
    \int_1^b \left(\frac{nx^2}{x^3 + 1} - \frac{1}{13x + 1} \right) dx \\
    &= \lim_{b \to + \infty} \left[\frac{n}{3} \cdot \ln(x^3 + 1) - \frac{1}{13} \cdot \ln(13x + 1) \right]_1^b \\
    &= \lim_{b \to + \infty}
    \left(\left[\frac{n}{3} \cdot \ln(b^3 + 1) - \frac{1}{13} \cdot \ln(13b + 1) \right] - \left[\frac{n}{3} \cdot \ln(2) - \frac{1}{13} \cdot \ln(14) \right] \right)
    \\ &= \lim_{b \to + \infty}
    \left(\frac{n}{3} \cdot \ln(b^3 + 1) - \frac{1}{13} \cdot \ln(13b + 1) - \frac{n}{3} \cdot \ln(2) + \frac{1}{13} \cdot \ln(14) \right)
    \\ &= \lim_{b \to + \infty}
    \left(
      \frac{n}{3}
      \left(
        \ln(b^3 + 1) -  \ln(2)
      \right)
      - \frac{1}{13}
      \left(
        \ln(13b + 1)
        -  \ln(14)
      \right)
    \right)
    \\ &= \lim_{b \to + \infty}
    \left(
      \frac{n}{3}
      \left(
        \ln \left(\frac{b^3 + 1}{2}\right) 
      \right)
      - \frac{1}{13}
      \left(
        \ln \left(\frac{13b + 1}{14}\right)
      \right)
    \right)
  \end{aligned}
\end{equation*} I've tried to use L'Hopital's from here as I have the form $(+ \infty ) - ( +
\infty)$. But things went pretty south. So I'm sure there's a better approach.","['integration', 'convergence-divergence', 'calculus', 'limits']"
2365235,Covering a square with N disjoint squares,"I could clearly fit in 4 squares together to make a square. Or, I could fit in 6 squares into a square as well: But this is not possible with 3 squares (or so I think!) How does this generalize? How can we decide for which N such a covering is possible?","['number-theory', 'geometry']"
2365256,What is the cutting point of two distributions?,"I just want to know what is the cutting point between the predictive bayesian distribution and the predictive frequentist distribution (plugin) (the one that uses the MLE). I will only consider distributions with positive support. This is my attempt: Let $x_f^{*}$ be the point such that \begin{align}
& \int_0^\infty f (x_f^{*} \mid \theta) \pi(\theta\mid \textbf{x}) \, d\theta = f(x_f^{*}\mid\hat{\theta}) \\[10pt]
\iff & \int_0^\infty f (x_f^{*} \mid \theta)\pi(\theta\mid\textbf{x}) \, d\theta - f(x_f^{*}\mid\hat{\theta}) = 0 \\[10pt]
\iff & \int_0^\infty f (x_f^{*} \mid \theta)\pi(\theta\mid\textbf{x}) \, d\theta - \int_0^\infty f(x_f^{*}\mid\hat{\theta})\pi(\theta\mid\textbf{x})\,d\theta = 0 \\[10pt]
\iff & \int_0^\infty [f (x_f^{*} \mid \theta) - f(x_f^{*} \mid \hat{\theta})] \pi(\theta\mid\textbf{x})\,d\theta = 0
\end{align} Hence, $f(x^{*}_f\mid\theta) = f(x^{*}_f\mid\hat{\theta})$. Also by the multivariate and unbounded version of the mean value theorem (Apostol analysis book) $$
\int_0^\infty f (x_f \mid \theta)\pi(\theta\mid\textbf{x})\,d\theta = f(x_f \mid c) \int_0^\infty\pi(\theta\mid\textbf{x}) \, d\theta \text{ for some } c \in [\inf f(x_f \mid \theta), \sup f(x_f\mid\theta)]
$$ Since $\pi(\theta\mid\textbf{x})$ is a posterior distribution and integrates one, $$
f(x_f\mid c)\int_0^\infty \pi(\theta\mid\textbf{x}) \, d\theta = f(x_f\mid c) = f(x_f\mid \hat{\theta}) \Rightarrow c = \hat{\theta}
$$ But I make an example (the exponential and they do not cut at the MLE). What is wrong here ?","['bayesian', 'statistics', 'distribution-tails']"
2365293,Compute some expectations involving beta and gamma distributions,"a) Let $X \sim {\text {Beta}}(a,b)$, with density $f(x) = \frac{x^{a-1}(1-x)^{b-1}} {B (a,b)}$. Find the expectation of $X \ln X$ and $\frac{1}{X+1}.$ b) Let $Y \sim {\text {Gamma}}(\alpha,\beta)$, with density $g(y) = {\frac {\beta ^{\alpha }}{\Gamma (\alpha )}}y^{\alpha \,-\,1}e^{-\beta y}$. Find the expectation of $Y \ln Y$ and $\frac{1}{Y+1}.$ c) Assume $X,Y$ are independent. Find $\Bbb E [\frac{1}{XY+1}]$ and $\Bbb E [\frac{1}{Y-XY+1}]$. I reduced my previous question to the above much simpler questions. Any help with this question will help the previous question as well. Any help with any of above expectation calculations will be very much appreciated.","['gamma-distribution', 'probability-theory', 'expectation']"
2365297,Fixed points in mapping from Möbius strip to disk [Explanation or reference needed],"One of the most elegant demonstrations in topology is the proof of the inscribed rectangle problem (a solved variant of the unsolved inscribed square problem ) which states that for any plain, closed continuous closed curve $\Gamma$ in $\mathbb{R}^2$, there exist four points that are the corners of an inscribed rectangle.  (There are variants of this problem involving cyclicality as well.) The proof relies on a clever representation of the locations of unordered pairs of points on $\Gamma$ with a point $p$ on a Möbius strip, and a function $f(p)$ that represents the Euclidean distance between the points on $\Gamma$. The key step in the proof invokes the topological fact that the mapping of the Möbius strip and $f(p)$ to the plane of $\Gamma$, involving the unwrapping of the strip's boundary to coincide with $\Gamma$ guarantees that there exist two points, $p_1$ and $p_2$ that map to the same point on the plane and have the same value, i.e., $f(p_1) = f(p_2)$. Is there a good reference, proof, or even intuitive demonstration of this fact?","['differential-topology', 'orientation', 'general-topology', 'surfaces', 'non-orientable-surfaces']"
2365301,Prove Standard deviation greater than or equal to Mean deviation,"Ho do we prove that the standard deviation is greater than or equal to the mean deviation about the arithmetic mean ? $$
\sqrt\frac{\sum_{i=1}^{n}(x_i-\bar{x})^2}{n}\geq\frac{\sum_{i=1}^{n}|x_i-\bar{x}|}{n}
$$ and under what conditions we get the equality ? I think i understand that it is because of the squaring in standard deviation which tends to give more weightage to the data far from the central tendency.","['statistics', 'standard-deviation']"
2365308,We know $H(f\star g)\leq H(f)+H(g)$. Is there a better bound for $H(f\star f)$?,"Shannon entropy $H(f)\triangleq \sum_x -f(x)\log f(x)$ is sub-additive. That is, if $f,g$ are pmfs: $$H(f\star g)\leq H(f)+H(g).$$ Equality is attained when $f$ and $g$'s distributions are separated after convolution. For example if $x_0\sim\operatorname{B}(.5),\ x_1=1000x_1$ and $x_2\sim \operatorname{B}(.5)$, imagine the distribution of: $y=x_1 + x_2$ over $\{0,1,1000,1001\}$. I cannot think of a case where this sort of preservation happens when $f=g$, which makes me suspicious there is a much better upper bound for $H(f\star f)$ than $2H(f)$. Does anyone know of one?","['information-theory', 'real-analysis', 'probability-theory']"
2365332,Finding the mistake in the limit,"Please, help me with finding the mistake, where did i go wrong: $$L=\lim_{n \to \infty}(\frac{1}{\sqrt{n}\sqrt{n+1}}+\frac{1}{\sqrt{n}\sqrt{n+2}}+...+\frac{1}{\sqrt{n}\sqrt{n+n}}) $$ I tried the squeeze theorem, and I can see that $L \in [\frac{1}{2},1]$. I found the solution, as $$L=\lim_{n\to\infty}\sum_{i=1}^n{\frac{1}{\sqrt{n}\sqrt{n+i}}}$$
$$L=\lim_{n\to\infty}\sum_{i=1}^n{\frac{1}{{n}\sqrt{1+\frac{i}{n}}}}$$ We can look at the limit as a way to calculate the area of a function of reals, so we have $$f(x)=\frac{1}{\sqrt{1+x}}$$
$$L=\int_{0}^1f(x)=...=2(\sqrt2-1)$$ Is there a way to do it without integration? Another way, a better way? Second part of the question: Trying another way of finding solution, I made a mistake that shows lack of fundamental understanding: $$L=\lim_{n \to \infty}(\frac{1}{\sqrt{n}\sqrt{n+1}}+\frac{1}{\sqrt{n}\sqrt{n+2}}+...+\frac{1}{\sqrt{n}\sqrt{n+n}}) $$ Now we take $ln$ of both sides: $$\ln{L}=\ln\lim_{n \to \infty}(\frac{1}{\sqrt{n}\sqrt{n+1}}+\frac{1}{\sqrt{n}\sqrt{n+2}}+...+\frac{1}{\sqrt{n}\sqrt{n+n}}) $$ $$\ln{L}=\lim_{n \to \infty}(\ln\frac{1}{\sqrt{n}\sqrt{n+1}}+\ln\frac{1}{\sqrt{n}\sqrt{n+2}}+...+\ln\frac{1}{\sqrt{n}\sqrt{n+n}}) $$ $$\ln{L}=-\lim_{n \to \infty}(\ln({\sqrt{n}\sqrt{n+1}})+\ln({\sqrt{n}\sqrt{n+2}})+...+\ln({\sqrt{n}\sqrt{n+n}})) $$ $$\ln{L}=-\lim_{n \to \infty}(\ln({\sqrt{n}\sqrt{n+1}})+\ln({\sqrt{n}\sqrt{n+2}})+...+\ln({\sqrt{n}\sqrt{n+n}})) $$ $$\ln{L}=-\lim_{n \to \infty}\ln{\sqrt{{(n^n)\prod_{i=1}^n(n+i)}}}$$ There are indeterminate forms here, but they are clearly diverging to infinity. $$\ln{L}=-\infty$$ $$L=0$$
Where did I go wrong in the other procedure?",['limits']
2365345,Exactly 5 Platonic solids: Where in the proof do we need convexity and regularity?,"The famous statement that Only five convex regular polyhedra exist. is usually proven as follows: Let $P$ be a convex regular polyhedron with V vertices, E edges and F faces. Moreoever, let n be the number of edges of each face and c be the number of edges which meet at a vertex. Then $F=\frac{2E}{n}$ and $V=\frac{2E}{c}$ and $c\geq 3, n\geq 3$. Substituting this into Euler's polyhedron formula
$$
V-E+F=2,
$$ and doing some easy calculations, one gets that only $$
(V,E,F)\in\{(4,6,4),(8,12,6),(20,30,12),(6,12,8),(12,30,20)\}
$$
are possible combinations. So far, so good. But where do we need that $P$ is convex and regular ? (I think the regularity is at least used for $n\geq 3, c\geq 3$, isn't it?)","['polyhedra', 'platonic-solids', 'geometry']"
2365371,Prove $ \frac{1}{x^{2n}}+x^{2n}$ is an integer for all $n$. [duplicate],"This question already has answers here : Prove by induction that a^n+a^-n is an integer. (2 answers) Closed 6 years ago . Question: Suppose $ \frac{1}{x^2}+x^2$ is an integer. Prove that $ \frac{1}{x^{2n}}+x^{2n}$ is an integer for all natural $n$. Hint: Use Strong Induction My attempt: Base Case is trivial. I.H: Assume the result is true for $n = 1,2, ...., k.$ Consider $n = k+1$. $ \frac{1}{x^{2\left(k+1\right)}}+x^{2\left(k+1\right)}\ =\ \frac{1}{x^{\left(2k+2\right)}}+x^{2k+2}$. I am not sure what to do from here and how to use the induction hypothesis.","['algebra-precalculus', 'induction']"
2365380,Name for the set of possible functions $A\to B$,"Everybody knows the Cartesian product $A \times B$, where $|A\times B| = |A| \cdot |B|$. But is there a name for the set of possible functions $A \to B$, where $|A \to B| = |B|^{|A|}$ E.g. $$
A = \{0, 1\} \\
f_1,f_2,f_3,f_4\colon A \to A \\
f_1\colon x \mapsto 0 \\
f_2\colon x \mapsto 1 \\
f_3\colon x \mapsto x \\
f_4\colon 0 \mapsto 1 \\
f_4\colon 1 \mapsto 0 \\
$$ I know of the symmetric group, which is a similiar notion, but it only covers bijections, and not all possible functions.","['elementary-set-theory', 'terminology', 'notation', 'functions']"
2365405,Show the divergence of $\sum_{k=2}^\infty \frac{1}{\sqrt k (\ln k)^2} $,"I'm attempting to understand how the following sum behaves for different values of $p$ and $q$:
$$\sum_{k=2}^\infty \frac{1}{ k^q (\ln k)^p} $$I think I have figured out every case, except when $q \in (0,1)$ and $p>1. $ To determine what to do, I gave concrete values to $p$ and $q$. Namely, $q=\frac{1}{2}$, and $p=2$. But I still can't figure out how to prove the behavior of the series. Using Wolfram Alpha, I know that the series below diverges, but I don't know how to prove it. Does anyone have any insight to showing either the general case above or the more concrete case below? Thanks in advance. $$\sum_{k=2}^\infty \frac{1}{\sqrt k (\ln k)^2} $$","['divergent-series', 'sequences-and-series', 'calculus']"
2365423,Why is the Picard-Fuchs equation for elliptic curves of second order?,"Consider the elliptic curve defined by a polynomial
$$
F(x,y,z) = t(x^3 + y^3 + z^3) - 3xyz
$$
If we consider the period integral
$$
\pi(t) = \int_{\gamma} \omega(t)
$$
where $\gamma$ is a $1$-cycle defined at and around $F$ for a variation of $t$. We can associate a second-order differential equation
$$
\frac{d^2 \pi(t)}{dt^2} + A(t)\frac{d\pi(t)}{dt} + B(t) = 0
$$
called the Picard-Fuchs equation. Why is this differential equation second order? Does this have something to do with the real dimension of the complex variety?","['complex-geometry', 'algebraic-geometry', 'modular-forms', 'algebraic-topology', 'ordinary-differential-equations']"
2365424,Convergence of CDFs to each other,"Suppose $\{X_n\}_n$ and $\{Y_n\}_n$ are sequences of (say, continuous) random variables with corresponding CDFs $\{F_n\}_n$ and $\{G_n\}_n$. Suppose that 
$$|X_n - Y_n| \xrightarrow{a.s} 0~.$$ Is it clear (without additional assumptions) that $$|F_n(t) - G_n(t)| \rightarrow 0~,$$ for all $t$? Working through the ""standard"" proof for the case where $Y_n = Y$ makes it seem that we might require equicontinuity of the families $\{F_n\}_n$ and $\{G_n\}_n$, for each $t$, but I was wondering if such an assumption is necessary for the result to go through.",['probability-theory']
2365431,How does rounding affect Fibonacci-ish sequences?,"I'm curious how one might account for rounding in simple recurrence relations. $\textbf{Explanation}$ For a specific problem, suppose we have a sequence of positive integers $a_1, a_2, a_3,...$ with where each $a_i$ obeys the following rule
$$a_n=a_{n-1}+\text{floor}\Big[\frac{a_{n-4}}{2}\Big]$$
Where ""floor"" here just means round down. For example if $a_1=5$ and $a_4=7$ then
$$a_5=7+\text{floor}\Big[\frac{5}{2}\Big]=9$$ And if we start with $a_1=a_2=a_3=a_4=6$ and continue the sequence we get
$$6\rightarrow6\rightarrow6\rightarrow6\rightarrow9\rightarrow12\rightarrow15\rightarrow18\rightarrow22\rightarrow28\rightarrow35\rightarrow44\rightarrow55\rightarrow...$$
What is $a_{100}$? And more importantly, can $a_n$ in general be exactly calculated without calculating all the previous terms in the sequence? Here's what's been tried so far $\textbf{Linear Algebra Approximation}$ The update rule for the previous sequence can be approximated with a linear transformation
$$
A = 
\begin{bmatrix}
1 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
\frac{1}{2} & 0 & 0 & 0 \\
\end{bmatrix}
\quad \quad \text{with} \quad \quad
A \cdot
\begin{bmatrix}
a_n \\
b_n \\
c_n \\
d_n \\
\end{bmatrix}
=
\begin{bmatrix}
a_{n+1} \\
b_{n+1} \\
c_{n+1} \\
d_{n+1} \\
\end{bmatrix}
$$
where the $a_n$ in each vector corresponds to the same $a_n$ of the previous sequence example. This matrix has only one all-positive eigenvector with an eigenvalue $\lambda=1.254...$ which is the growth rate approached by the sequence. One could predict the value of $a_n$ by calculating
$$A^n \cdot \begin{bmatrix}
a_1 \\
a_1 \\
a_1 \\
a_1 \\
\end{bmatrix}$$ The problem with using a linear transformation is that rounding isn't taken into account. So the predictions one gets inevitably overshoot. And some starting sequences don't even increase when rounding is taken into account. Consider $a_1=a_2=a_3=a_4=1$
$$1\rightarrow1\rightarrow1\rightarrow1\rightarrow1\rightarrow1\rightarrow1\rightarrow1\rightarrow...$$ $\textbf{Error Estimation}$ For initial values of the sequence that highly divisible by $2$, the rounding down won't affect the sequence for a longer time. More specifically if we start with
$$a_1=a_2=a_3=a_4=k2^m \text{ where } k \text{ is odd}$$
Then $a_{4m+5}$ will be the first term affected by the rounding.
One could introduce a rounding error function. Something of the form
$$f(k, m) = \text{% error approached by the initial value } k2^m$$ It was suggested to use linear dynamical discrete systems to derive an exact formula for $a_n$. How exactly that is done is still unknown. Are there any good example problems similar to this one?","['dynamical-systems', 'sequences-and-series', 'linear-transformations', 'approximation']"
2365440,Existence of derivative at the origin,"I'm supposed to find for which values of $p$ and $q$ the following function is differentiable at the origin: $$   
f(x,y,z) = 
     \begin{cases}
       \dfrac{(x^2y^2)^p(1-\cos(z))^q}{x^2+y^2+z^2} & \text{if } (x,y,z) \neq (0,0,0) \\
        0 & \text{if } (x,y,z) = (0,0,0)\\
      \end{cases}
$$ I've made some progress using the Taylor expansion of the cosine about $0$, but couldn't get far even doing so. Could anyone share a solution?","['derivatives', 'real-analysis', 'limits']"
2365463,How do I calculate the chord length I need to produce this Japanese sun pattern?,"Japanese Sun Pattern I want to draw a pattern as above with the circles overlapping. I have the diameter of the outer concentric circle. I want each sun pattern I draw to be equal in size - i.e. equal arc length for the outermost circle (white). Given a distance between each circles center both horizontally and vertically, how would I calculate the chord length to ensure I draw each arc equally (I am drawing on graph paper).","['circles', 'arc-length', 'geometry']"
2365495,To put a point at infinity,"In physics, we say that we can put points at infinity in order to compactify some manifolds, like compactifying the plane via the stereographic projection. But, in order to do this, we also need to add a point at infinity. To map the plane to the sphere we use the aforementioned projection. But, how do we naturally include the extra point at infinity? Do we do it in a forced way by just defining a point as the point at infinity? If so, how exactly? Lastly, since infinity is not a number (so it cannot correspond to a point, but correct me if I am wrong), how can we assign a point to it? How can we intuitively think of adding a point at infinity? Thank you.","['general-topology', 'differential-geometry']"
2365539,Diffeomorphisms from spheres in $\mathbb{R}^n$,"I'm stuck  in the following exercise: Let $f:B_r (x_0) \subset \mathbb{R}^n \to \mathbb{R}^n$ be a diffeomorphism from $B_r (x_0)$ onto $f(B_r (x_0))$. If $\|f'(x)^{-1}\|\leq M$ for all $x\in B_r (x_0)$ and $|f(x_0)|\leq r/M$, then $0\in f(B_r (x_0))$. I've tried using the mean-value inequality to find a sequence converging to zero, but it doesn't seem to be any hypothesis on the bound $M$, so I'm not sure if that's the right way to prove it. Since the only point of the domain I know something about is the center $x_0$, I guess I should try to show the origin belongs to some neighborhood of $f(x_0)$, but I'm not sure how to do it. 
Could someone help me?","['real-analysis', 'diffeomorphism', 'inverse-function']"
2365554,Eigenvalues of differential operator L(second order differential equation),"I'm looking for eigenvalues of operator L such as: $$ L=\frac{d^2}{dt^2} + \Gamma \frac{d}{dt} $$ I know that I need to solve equation: $$ f''(t) + \Gamma f'(t) = \lambda f(t) $$ But I have no idea even where to start. Edit 1: $\Gamma$ is a constant, not a function of $t$ and $\Gamma > 0$. Edit 2: I tried to solve a differential equation which I wrote. I got something like this: $$ f(t) = C_1 e^{-\frac{\Gamma t}{2}} + C_2 t e^{-\frac{\Gamma t}{2}} $$ for
$$ \lambda = - \frac{1}{4} \Gamma^2 $$
Is this lambda my eigenvalue? But this is solution for $\Delta=0$ of a characteristic equation. What about other values of delta? As far as I know $\Gamma > 0$","['eigenvalues-eigenvectors', 'differential-operators', 'eigenfunctions', 'ordinary-differential-equations', 'linear-algebra']"
2365566,What is the rank of $\bigwedge ^kA$ as a function of rank $A$?,"Let $V,W$ be $d$-dimensional real vector spaces of dimension $d$. Let $A \in \text{Hom}(V,W)$. Consider the induced map on the exterior algebras $\bigwedge ^kA:\bigwedge ^k V \to \bigwedge ^k W$. ($1 \le k \le d$). It is easy to see* that $\text{rank} \bigwedge ^k A$ depends only on $\text{rank} \, A$. What is the function $F_k:\{0,\dots,d \} \to \{0,\dots,d \}$ such that $\text{rank} \bigwedge ^k A=F_k(\text{rank} \, A)?$ Partial results: The cases $k=1,d$ are trivial. $F_k(r) =0$ for $r < k$. $F_k(k) =1$. $F_k(s) \le \binom sk$ for $s \ge k$. Indeed, suppose that $v_1,\dots,v_s$ are independent and that $A|_{\text{Span}\{ v_1,\dots,v_s\}}$ is injective. Complete $v_1,\dots,v_s$ to a basis of $V$. Then for any choice   of $k$ basis elements, not all of them in $\{ v_1,\dots,v_s\}$ $\bigwedge ^k A$ gives zero when acting upon the corresponding ""wedge-element"". $F_k(s) = \binom sk$ for $s =k,d$. Note the case $s=d$ just means that if $A$ is invertible then $\bigwedge ^k A$ is invertible. Summary: It remains to compute $F_k(s)$ when $k<s<d$. I am not sure how to handle the possible linear dependencies of the images of $\bigwedge ^k A$ on different basis elements. *Any two maps of the same rank are equivalent, and the exterior power is a functor. (So for invertible maps $(\bigwedge ^k A )^{-1}= \bigwedge ^k A^{-1}$ ).","['exterior-algebra', 'combinatorics', 'matrix-rank', 'differential-geometry', 'linear-algebra']"
2365588,"Why $x^Tx>x^Tz$ with high probability, for independent standard normal high dimensional vectors $x$ and $z$","Let's say we sample two random vectors from the same multivariate Gaussian distribution $N(0, I)$. We are interested in the relationship between $x^Tx$ and $x^Tz$. At first, I thought this question is purely wasting time because, for a sampled vector $x$, we can easily construct different $z$ that can be legally sampled, to make any of the three signs ($>$, $<$ or $=$) holds. However, my simulation surprised me. For millions of random seeds with the vector length equals to 100, $x^Tx>x^Tz$. It looks like this is a guaranteed behavior while we all know it's not. I wonder if there could be any explanation from the statistics point of view, like something stating the unlikeliness of sampling out a $z$ that can make $x^Tx<x^Tz$? Please forgive me if you think this question is silly. But try it with your favorite programming language, you may come back to upvote me.","['probability-theory', 'statistics', 'normal-distribution']"
2365626,"$\lambda$ (Dynkin) system equivalent definitions, proof?","According to wikipedia ( https://en.wikipedia.org/wiki/Dynkin_system#Definitions ), there are two equivalent definitions of a $\lambda$-system. How are these two definitions equivalent? To prove equivalence, do I have to show that one definition implies all of the conditions of the other definition and vice versa? For example, let's start with the second definition: $D$ is a Dynkin system if (S1) $\Omega \in D$, (S2) if $A \in D$, then $A^c \in D$, (S3) $D$ is closed under countable disjoint union: if $A_1, A_2, \cdots   
   \in D$ and $A_i \cap A_j = \emptyset$ for all $i \neq j$, then $\cup_{n \in \mathbb{N}} A_n \in D$. We want to show that the above definition implies the first definition: (F1) $\Omega \in D$, (F2) If $A, B \in D$ and $A \subset B$ then $B \setminus A \in D$. (F3) If $A_1 \subset A_2 \subset \cdots$ with each element being in $D$, then $\cup_{n \in \mathbb{N}}  A_n\in D$. Showing (F1) is true is trivial since it is the same as (S1). Showing (F2) is true can be done as follows: Note that $B \setminus A = B \cap A^c = (B^c \cup A)^c$ is in $D$ because it is the complement of the set $B^c \cup A$ which is in $D$ because it is the union of two disjoint sets $A$ and $B^c$ both of which are in $D$. Question 1) How can I complete the above proof by showing that (F3) is true? Question 2) How can I prove that (F1)-(F3) implies (S1)-(S3)?","['measure-theory', 'elementary-set-theory']"
2365656,How to find limit of this function: $\lim_{n\to\infty} 0.99\ldots99^{10^n}$?,"How to find this limit: $$ \lim_{n\to\infty}  0.99\ldots99^{10^n},$$ $$ \text{where number of 9 is } n. $$ My solution:
$$ \text{if } n\to\infty \text{ then } 0.99\ldots99 \to\ 0.(9) $$
$$ \text{because } 0.(9) = 1 \implies \lim_{n\to\infty}  0.99\ldots99^{10^n}=1^{10^\infty} = 1 $$ But my solution is wrong. Why? How can I correct it?","['exponential-function', 'limits']"
2365683,Commutation between σ-algebra and intersection,"I wonder the following property is true or not:
$$
\sigma(\bigcap_n \mathcal C_n)=\bigcap_n \sigma (\mathcal C_n)
$$
where $\mathcal C_n$ are decreased collections of sets in a measurable space. And ""σ "" denotes the generated σ-algebra. The motivation of this question is that I want to prove or disprove the following question in probability theory: Suppose $\mathcal F_t$ is a right-continuous filtration, and $\mathcal G$ is a σ-algebra. Then is the filtration $\mathcal F_t\vee \mathcal G$ is still right-continuous? 
Actually we need to prove 
$\bigcap_{h>0}\sigma(\mathcal F_{t+h}\cup \mathcal G)=\sigma(\bigcap_{h>0}\mathcal \{\mathcal F_{t+h}\cup \mathcal G\})$. If we  let $\mathcal C_n=\mathcal F_{t+1/n}\cup \mathcal G$ then we will get the result. Below are my attempts: The relation ""$\subset$"" is trivial, and it is also trivial when $\mathcal C_n$ are σ-algebra. The hard part is the relation ""$\supset$"".I try to find a counterexample that $\bigcap_n \mathcal C_n=\emptyset$ with some proper measurable set $A\in \sigma (\mathcal C_n)\forall n$. I've tried some simple examples like $\mathcal C_n=\{n,n+1,...\}\subset\mathbb N$, but the relation turn out to be right. About the filtration question, I proved when $\mathcal G$ is the σ-algebra generated by a countable partition of the whole space: Since every set $A$ in $\mathcal F_t\vee \mathcal G$ can be writen as $A=\bigcup_n(G_n\cap F_n^t)$, where $\{G_n\}\subset \mathcal G$ is the partition and $F_n^t\in\mathcal F_t\forall n$. Then for all $h>0, A=\bigcup_n(G_n\cap F_n^{t+h})$. Notice that $G_n$ are disjoint, thus $A=\bigcap_h\bigcup_n(G_n\cap F_n^{t+h})=\bigcup_n(G_n\cap \bigcap_hF_n^{t+h})\in\mathcal F_{t+}\vee\mathcal G=\mathcal F_t\vee\mathcal G$.","['probability', 'measure-theory', 'filtrations']"
2365698,Laurent expansion for $\sqrt{z(z-1)}$,"Let $f(z) = \sqrt{z(z-1)}$. The branch cut is the real interval $[0,1]$, and $f(z)>0$ for real $z$ that are greater than 1. I need to find the first few terms of the Laurent expansion of $f(z)$ for $\left|z\right| > 1$ (centered at zero). I also need the radius of convergence. I don't really know where to start for this one. I tried rewriting as $f(z) = z\sqrt{1 - \frac{1}{z}}$, as some have suggested, but this doesn't appear that enlightening. This is a study question when reviewing for a qualifying exam.","['complex-numbers', 'laurent-series', 'complex-analysis', 'summation', 'power-series']"
2365722,Does minimal submanifolds minimize area locally?,"Consider $(\tilde{M},g)$ a riemannian manifold and $M \subset \tilde{M}$ riemannian submanifold. Is it true that if $M$ is a minimal submanifold of $\tilde{M}$ then for every $p \in M$ there exists a neighborhood $W$ of $p$ in $\tilde{M}$ such that $V=W\cap M$ has least area among every $\Omega \subset W$ with $\partial \Omega = \partial V$? I've been thinking about it, I think it is true but I don't know how to prove. If it's true, how should I go about proving it?","['minimal-surfaces', 'riemannian-geometry', 'differential-geometry', 'calibrated-geometry']"
2365746,Prove the series $\sum_{n=1}^{\infty}\frac{1}{{5^n}^!}$ converges to an irrational number,This is the sum $$\sum_{n=1}^{\infty}\frac{1}{{5^n}^!}$$ My first attempt was to assume that the series does converge to a rational number $a/b$. But the $n!$ bothered me and I failed in my proof. How would you try to prove this series?,"['recreational-mathematics', 'rationality-testing', 'sequences-and-series', 'calculus']"
2365748,Prove that the $p$-th power map is a homomorphism of $P$ ***INTO*** $Z(P)$,"If $p$ is an odd prime and $P$ is a group of order $p^3$ then the $p$-th power map $\varphi:x\mapsto x^p$ is a homomorphism of $P$ INTO $Z(P)$. (Abstract Algebra: Dummit & Foote, Sec. 5.4, Ex. 9) I've shown that $\varphi$ is indeed a homomorphism. I'm struggling to find why $\varphi(P)\le Z(P)$. Assume $P$ is non-abelian. Then $[P,P]=Z(P)$, which is of order $p$. Don't know if it helps. I've also used that fact that if $x,y\in G$ and both $x$ and $y$ commute with $[x,y]$, then for all positive integers $n$, $(xy)^n=x^ny^n[y,x]^{\frac{n(n-1)}{2}}$ in my proof that $\varphi$ is a homomorphism. I'm sure that the authors mean $\varphi(P)\le Z(P)$ instead of $\varphi(P)=Z(P)$ just by considering $P\cong\mathbb Z_p\times\mathbb Z_p\times\mathbb Z_p$. I've tried to found examples that can help me. For $P\cong\mathbb Z_9\rtimes\mathbb Z_3$, $\varphi(P)=Z(P)$ so the result isn't surprising. Just have no idea how to proceed. Can somebody please help?","['abstract-algebra', 'group-theory']"
2365763,Integrating Logistic Functions,"For logistic functions in the form of $\frac{C}{1+Ae^{-bx}}$ where $C,A,b>0$ and $x$ is the independent variable, how does one integrate this function type? since during integration, the denominator is to the power of $(-1)$ and integrating will resulting in a power of $(0)$. I have tried a few websites such as cymath, wolfram and symbolab but havent been able to understand their working. for example 
$$\int \frac{1}{1+e^{-x}}dx=x+\ln \left|1+e^{-x}\right|+C$$. edit: if the anti-derivative is used for finding an area of the original function through integration, how can an unknown bound for a particular area be solved? is it possible?",['integration']
2365777,Number of successes when the successes are positively correlated,"In $n$ trials, each with success-probability $p$, what is the probability of at least $k$ successes, $P[n,k]$ ? The answer depends on the dependence between the trials: A. If the trials are independent, then the number of successes is a Binomial variable, so:
$$
P^{\text{ind}}[n,k] = \sum_{i=k}^n {n\choose k} p^i (1-p)^{n-1}
$$ B. If the trials can be arbitrarily dependent, then $P[n,k]$ might be 0. E.g, take $p=1/2$, $n=2,k=2$, and assume $X_2 = 1 - X_1$ (trial 2 fails iff 1 succeeds; from Henry's comment). C. Now suppose that the variables are dependent ""in the good direction"":
$$
\forall i_1,\ldots,i_j: \Pr[X_{i_1}=1\cap\cdots\cap X_{i_j}=1] \geq \Pr[X_{i_1}=1]\cdots\Pr[X_{i_j}=1] = p^j
$$
that is, if we already had some successes, the probability of another success weakly increases. Here, $P[n,k]$ cannot be zero. A trivial upper bound is $P[n,k] \geq p^k$ (as commented by muzzlator), since even if we consider only trials $1,\ldots,k$, the assumption implies that the probability  all of them succeed is at least $p^k$ Is there a better lower bound for $P[n,k]$ in this case? Initially I thought that $P[n,k]\geq P^{\text{ind}}[n,k]$, since intuitively, the dependence between the variables can only increase the number of successes. However, this is not true. As a simple example (from muzzlator's comment), take $k=1, n\to\infty$ and assume that all variables are the same: $X_i=X_1$ for all $i$. Then, $P^{\text{ind}}[n,1]\to 1$ while $P[n,1]=p$. So, what is a correct lower bound on $P[n,k]$?","['independence', 'probability', 'correlation']"
2365778,Finding the relation between identity and identical functions,"Consider the function $f(x,a,b,c,d) = (((x \oplus a) + b) \oplus c) + d$ defined on $\mathbb{Z}_ {2^n}$, where $\oplus$ denotes bitwise exclusive-OR (XOR) and $+$ denotes addition modulo $2^n$. Let's start with two definitions: $f(\cdot,a,b,c,d)$ is an identity function on $\mathbb{Z}_{2^n}$ if $\forall x \in \mathbb{Z}_{2^n}, f(x,a,b,c,d) = x$. $f(\cdot,a',b',c',d')$ is identical to $f(\cdot,a,b,c,d)$ if $\forall x \in \mathbb{Z}_{2^n}, f(x,a',b',c',d') = f(x,a,b,c,d)$. In answer to this question , it has been shown that the number of identity functions is equal to $n \times 2^{n+2}$. Furthemore, when looking in practice for several $n$, it seems that $n \times 2^{n+2}$ is also the number of identical functions for a given $(a,b,c,d) \in (\mathbb{Z}_{2^n})^4$. In fact, identity functions are the specific case where $(a,b,c,d) \in (\mathbb{Z}_{2^n})^4$ is such that $f(x,a,b,c,d) = x$. Is there a way to prove that the number of identity functions is equal to the number of identical functions? In other terms, how can we extend the demonstration for all $(a,b,c,d) \in (\mathbb{Z}_{2^n})^4$?","['finite-groups', 'binary', 'proof-writing', 'modular-arithmetic', 'linear-algebra']"
2365803,The limit of $a_1^{a_2^{.^{.^{.^{a_n}}}}}-a_n^{a_{n-1}^{.^{.^{.^{a_1}}}}}$ as $n\to\infty$.,"If this is a duplicate in any way, I'm sorry. The Question: For what $(a_i)_{i\in \Bbb N}\in \Bbb R_+^{\Bbb N}$ does $$L:=\lim_{n\to \infty}a_1^{a_2^{.^{.^{.^{a_n}}}}}-a_n^{a_{n-1}^{.^{.^{.^{a_1}}}}}$$ exist and what values does $L$ take? Thoughts: Of course, $L$ exists and would be $0$ when $a_i=1$ for all $i\in\Bbb N$. Also $L=1$ if
$$a_i=\begin{cases}
2 &: i=1 \\
1 &: i\neq 1.
\end{cases}$$","['sequences-and-series', 'limits']"
2365830,"How many five digit numbers divisible by $3$ can be formed using the digits $0,1,2,3,4,7$ and $8$ if each digit is to be used at most once","How many five digit numbers divisible by $3$ can be formed using the digits $0,1,2,3,4,7$ and $8$ if each digit is to be used at most once? The total number of $5$ digit numbers using the digits $0,1,2,3,4,7$ and $8$ is $6\times6\times5\times4\times3=2160.$ Now I found the numbers not divisible by $3$, i.e. even numbers ending in $2,4,8.$ Even numbers from the digits $0,1,2,3,4,7$ and $8$ are $5\times5\times4\times3\times3=930.$ So the numbers divisible by $3$ are $2160-930=1230$ but the answer is $744.$ Where I am wrong?","['divisibility', 'integers', 'permutations', 'combinatorics', 'discrete-mathematics']"
2365857,Does this the limit exist?,"Assume that there exist constants $c_i>0, i=1,\dots,5$, $\beta>1\ge\gamma>0$, and $
    \theta>0$ such that
  \begin{eqnarray}
   & &c_1 |x|^\beta\le V(x)\le c_2 |x|^\beta,\qquad c_4r^\theta\le \mu(B(x,r))\le c_5r^\theta,\\
   && \frac{|V(x)-V(y)|}{|x-y|^\gamma}\le c_3\big(\max\{ |x|,|y|\}\big)^{\beta-\gamma}
  \end{eqnarray}
  for all $x,y\in \mathbb{R}$ and sufficently large $r>0$. Let $ V^\wedge $ (resp. $V^\vee $) be the piecewise constant function which takes the value $\sup_{x\in X_i} V(x)$ (resp. $\inf_{x\in X_i} V(x)$) on $X_i=[i,i+1]$ for $i\in \mathbb{Z}$. I want to know that whether the limit
$$\lim_{\lambda\to \infty}\frac{\mu(\{x\in \mathbb{R}:V^\vee(x)\le \lambda\})}{\mu(\{x\in\mathbb{R}:V^\wedge(x)\le \lambda\})}$$
 exists or not? It is easy to check that
$$ \lim_{\lambda\to \infty}\frac{\mu(\{x\in \mathbb{R}:V^\vee(x)\le \lambda\})}{\mu(\{x\in\mathbb{R}:V^\wedge(x)\le \lambda\})}\ge 1.$$ Moreover, I have showed the folowing staements: For any $i\in \mathbb{Z}$ and any $x,y\in X_i$,
  \begin{equation}\label{eq:Bohr_01}
  \begin{aligned}
    |V(x)-V(y)|&\le c_3|x-y|^\gamma\big(\max\{ |x|,|y|\}\big)^{\beta-\gamma}\\
    &\le c_3|X_i|^\gamma\big(\max\{ |x|,|y|\}\big)^{\beta-\gamma}
     \\
    &=c_3\big(\max\{ |x|,|y|\}\big)^{\beta-\gamma}.
  \end{aligned}
  \end{equation}
  By the definition of $V^b(x)$, $b\in \{\vee,\wedge\}$, there exist two sequences $\{x_{n,m}\}_{n=1}^\infty\subseteq X_i$, $m=1,2$, such that $V^\wedge(x)=\lim_{n\to \infty}V(x_{n,1})$ and $V^\vee(x)=\lim_{n\to \infty}V(x_{n,2})$ for any $i\in \mathbb{Z}$ and $x\in X_i$. It follows that, for any $x\in \mathbb{R}$ such that $|x|\ge 1$,
  \begin{equation}\label{eq:V_-+}
    \begin{aligned}
      V^\wedge(x)-V^\vee(x)&=\lim_{n\to \infty}(V(x_{n,1})-V(x_{n,2}))\\
      &\le c_4\big(\lim_{n\to \infty}\max\{ |x_{n,1}|,|x_{n,2}|\}\big)^{\beta-\gamma}\\
      &\le c_4\big(  |x|+ 1\big)^{\beta-\gamma}\le c_5|x|^{\beta-\gamma}.
    \end{aligned}
  \end{equation} Thanks very much for you help.","['functional-analysis', 'real-analysis', 'measure-theory']"
2365869,Generalizing linear ODE's to Banach spaces,"The most general form of a linear IVP that was considered in my course is $$\dot x(t) = A(t)x(t)+b(t),\quad t\in J, \quad x(t_0)=x_0,$$ for $J$ an interval, $t_0\in J$, $A\in C(J,\mathbb{R}^{m\times m})$, and $b\in C(J,\mathbb{R}^m)$. The unique solution is derived using fundamental matrices and given as $$x(t) = X(t)\left(X^{-1}(t_0)x_0+\int_{t_0}^t X^{-1}(\tau)b(\tau)\operatorname{d}\tau\right)$$ with $X$ being a fundamental matrix. However, considering the same ODE in general Banach spaces, that is, $A\in C(J, \mathcal{L}(E))$, $b\in C(J, E)$, $E$ being a banach-space, what's the change? Is $$(\star)\quad x(t) = e^{\int_{t_0}^t A(\tau)\operatorname{d}\tau}x_0 +\int_{t_0}^t e^{\int_{\tau}^t A(s)\operatorname{d} s}b(\tau)\operatorname{d}\tau$$ the unique solution to the IVP? I understand the possibility of the space of solutions being of infinite dimension prohibits the use of fundamental matrices. Are fundamental matrices used to give concrete answers to concrete IVP's only? As I have worked through linear ODE's with constant $A\in\mathcal{L}(E)$ in banach-spaces, what's the difference coming from those ODE's? What becomes false after replacing ""$(t-t_0)A$"" by ""$\int_{t_0}^tA(\tau)\operatorname{d}\tau$""?","['real-analysis', 'banach-spaces', 'calculus', 'ordinary-differential-equations', 'initial-value-problems']"
2365871,Does bounded expectation imply almost sure boundedness for the sample mean?,"Let $X_1,X_2,X_3,\dots, X_n$ be i.i.d random variables from a probability distribution. Further there is a positive constant M such that $|E[X]|<M.$ My question is does the strong law imply that the sample mean $\bar{X_n}$ is almost surely bounded (i.e $P(|\bar{X_n}|>C)=0$,$\forall n\geq n_0$ and some constant $C>0.$) for very large n?","['probability-theory', 'measure-theory', 'statistics']"
2365893,Invertibility of one-to-one and onto linear operators on a Hilbert space,"$\mathcal{H}$ is a Hilbert space. $D$ is some dense subspace of $\mathcal{H}$. A linear operator $A:D\rightarrow \mathcal{H}$ is one-to-one (i.e., $Au=0$ implies $u=0$) and onto (i.e., $Range(A)=\mathcal{H}$). Do these conditions suffice to establish that $A$ has a bounded inverse?","['functional-analysis', 'operator-theory', 'hilbert-spaces']"
2365914,Evaluate $\lim_{ x\to \infty} \left( \tan^{-1}\left(\frac{1+x}{4+x}\right)-\frac{\pi}{4}\right)x$,"Evaluate 
  $$\lim_{ x\to \infty} \left(  \tan^{-1}\left(\frac{1+x}{4+x}\right)-\frac{\pi}{4}\right)x$$ I assumed $x=\frac{1}{y}$ we get $$L=\lim_{y \to 0}\frac{\left(  \tan^{-1}\left(\frac{1+y}{1+4y}\right)-\frac{\pi}{4}\right)}{y}$$ using L'Hopital's rule we get $$L=\lim_{y \to 0} \frac{1}{1+\left(\frac{1+y}{1+4y}\right)^2} \times \frac{-3}{(1+4y)^2}$$ $$L=\lim_{y \to 0}\frac{-3}{(1+y)^2+(1+4y)^2}=\frac{-3}{2}$$ is this possible to do without Lhopita's rule","['derivatives', 'limits', 'calculus', 'algebra-precalculus', 'limits-without-lhopital']"
2365921,Elliptic curves as the zeros of a Weierstrass equation.,"Let $\mathcal{L}$ be a very ample invertible sheaf on a $\mathbb{K}$-scheme $X$. In particular $\mathcal{L}$ induces a closed embedding $X\rightarrow \mathbb{P}^n$. It follows that $X\simeq \mathrm{Proj}(\mathbb{K}[X_0,\ldots,X_n]/I)$ for some homogeneous ideal $I$ of $\mathbb{K}[X_0,\ldots,X_n]$ and $\mathcal{L}\simeq \mathcal{O}_X(1)$. Hence,
$$
\mathbb{K}[X_0,\ldots,X_n]/I\simeq \bigoplus_{n\geq 0}H^0(X,\mathcal{O}_X(n))\simeq \bigoplus_{n\geq0}H^0(X,\mathcal{L}^{\otimes n}).
$$
I want to use this to study elliptic curves. If $X$ is an elliptic curve and $P_0\in X$, then $\mathcal{L}=\mathcal{O}_X(3\cdot P_0)$ is a very ample invertible sheaf with $h^0(X,\mathcal{L})=3$. Therefore it induces a closed embedding $X\rightarrow \mathbb{P}^2$ such that the homogeneous coordinate ring of the associated closed subscheme of $\mathbb{P}^2$ is
$$
\bigoplus_{n\geq 0} H^{0}(X,3n\cdot P_0).
$$
Recall that $h^{0}(X,n\cdot P_0)=n$ for every $n>0$.
According to Hartshorne's book,
$$
H^0(X,\mathcal{O}_X(2\cdot P_0))=\langle 1,x\rangle,
$$
$$
H^0(X,\mathcal{O}_X(3\cdot P_0))=\langle 1,x,y\rangle,
$$
$$
H^0(X,\mathcal{O}_X(6\cdot P_0))\supseteq \langle1,x,y,x^2,xy,x^3,y^2\rangle.
$$
Since $h^{0}(X,6\cdot P_0)=6$, it follows that we have a linear relation
$$
F:=a+bx+cy+dx^2+exy+fx^3+gy^2=0.
$$
My question is, how do we know that $1,x,y$ are the only independent elements in $\bigoplus_{n\geq 0} H^{0}(X,3n\cdot P_0)$? How do we know that the previous equation is the only relation between them? Any help, or correction in case I was misunderstanding something, would be appreciated. $\textbf{Edit:}$ I think that we can use the so called Castelnuevo's lemma to obtain that the map
$$
H^0(X,\mathcal{O}_X(3(n-1)\cdot P_0))\otimes H^0(X,\mathcal{O}_X(3\cdot P_0))\rightarrow H^0(X,\mathcal{O}_X(3n\cdot P_0))
$$
is surjective for every $n\geq 2$. Therefore, the elements of $H^0(X,\mathcal{O}_X(3n\cdot P_0))$ are products of elements of $H^0(X,\mathcal{O}_X(3\cdot P_0))$ for every positive integer $n$. So, the only thing I had left to prove is that there are no more relations betwen $1,x,y$. To be more precise, how can we prove that there is no polynomial $G$ in the indeterminates $x,y$ such that it is zero as a rational function and is not in the ideal generated by $F$? In other words, how can we prove that 
$$
\bigoplus_{n\geq 0} H^0(X,\mathcal{O}_X(3n\cdot P_0))\simeq \mathbb{K}[x,y]/\langle F\rangle ?
$$","['elliptic-curves', 'sheaf-cohomology', 'algebraic-geometry']"
2365933,simplify $\sin^{-1}(\tan x)$,I'm aware of how we can simplify functions which have $Arc$ as an argument . For example $\sin(\cos^{-1}(x)) = \sqrt{1-x^2}$ but what about cases which $Arc$ is out of the parentheses ? For instance consider this : $\sin^{-1}(\tan x)$ . Is there any way for simplification ?,"['trigonometry', 'inverse-function', 'functions']"
2365937,Question regarding prime numbers,"Is the following statement true?:
""If $n$ is an integer ($n \geq 2$) then 
$\frac {1}{n^2} \left(\binom{2n}{n}-2\right)$ is an integer if and only if $n$ is prime.""",['number-theory']
2365940,Trace operators on modules,"This question is motivated by this other one. A classical result of linear algebra states what follows Up to scalar, trace is the only linear operator $\text{M}(n,k) \stackrel{t}{\to} k $ such that $t(AB) = t(BA)$ . So there are two natural generalization of the problem. The first one is in the direction of replacing $k$ with a ring $A$ . The other is to ignore the restriction to finite dimensional vector spaces. Consider a finitely generated module $M$ on a commutative unital ring $A$ . End( $M$ ) has a natural module structure. A trace operator is a morphism of modules End( $ M) \stackrel{tr}{\to} A$ such that $tr(fg)=tr(gf)$ . Tr, the set of trace operators is a submodule of $\text{Hom}_A(\text{End(M)}, A)$ . In the special case of vector spaces, its dimension is $1$ . What happens for general rings? Can we recover a partial result? How many trace operators are there? What should be the trace for a morphism on modules? Sum of eigenvalues looks to me naive. Examples In the case of $M = \mathbb{Z}_6, A = \mathbb{Z}$ . There are no trace operators. Attempts In the case of vector spaces the submodule genetated by $fg-gf$ has codimension 1. What's its codimension in the case of free modules? Maybe this question makes much more sense when $A$ is a PID.","['representation-theory', 'abstract-algebra', 'modules', 'linear-algebra']"
2365953,Computing the inverse of some matrix,"Let $A(n) = (a_{i,j})_{1\le i,j \le n}$ be defined through: 
$a_{i,j} = 1, \text{ if } i \equiv 0 \mod (j)$, $0$ otherwise.
What is the inverse of $A$? Is there an ""easy"" formula for the inverse?
It seems that the inverse of the matrix has only entries $0,1,-1$.","['matrices', 'inverse', 'elementary-number-theory']"
2365957,Corollary of Hahn-Banach theorem regarding the norm,"Let $E$ be a normed space. Then, for each $x \in E$ we  can define its norm as 
$\|x\| = \sup \{|l(x)| : l \in E^∗, \|l\| ≤ 1 \} $. Proof: Let $x \in E$ and set $S = \sup\{|l(x)| : l \in E^∗, \|l\| ≤ 1 \} \leq \|x\|.$ On the one-dimensional subspace $F = \mathrm{span}(x)$ we define a functional $f$ by $f(\lambda x)= \lambda \|x\| $ for all $\lambda  \in \mathbb{R}$ 
it follows that $f \in F^*$ with $\|f\| = 1$.  Hence, by the Hahn-Banach Theorem,
there exists some $l \in E^*$ with $l|_F\equiv f$ and $\|l\| = 1$. In particular this implies  $|l(x)| = |f(x)| = \|x\|$. This implies that $S = \|X\|$ and that the supremum is attained. Now, why is it, that there a $\leq$ sign in the set of wich we take the  supremum, since the proof uses a functional that  clearly has norm equal to 1?","['functional-analysis', 'normed-spaces']"
2365967,"Existence of $\lim_{(x,y,z)\to(0,0,0)}\frac{\ln(1+x^2y^4z^6)}{(x^2+y^2+z^2)^{\alpha+\frac{1}{2}}}$ for some $\alpha$.","Hy guys! for which values of $\alpha\in\mathbb R$ does this limit exists? 
$$\lim_{(x,y,z)\to(0,0,0)}\frac{\ln(1+x^2y^4z^6)}{(x^2+y^2+z^2)^{\alpha+\frac{1}{2}}}$$ I know that for all $\alpha\leq-\frac{1}{2}$ the limit exists, but how can I argue to prove (or disprove) that this limit exists for $\alpha>-\frac{1}{2}$?","['multivariable-calculus', 'calculus', 'limits']"
2365972,Necessity of an assumption in the Going-down Theorem,"In the Going-down Theorem (Theorem 5.16 p. 64) in Introduction to Commutative Algebra by Atiyah and MacDonald, we have an integral inclusion $A\subset B$ of domains, $A$ being integrally closed. The assumption that $B$ is a domain is used in the proof, and I suspect that this assumption is necessary, but I haven't been able to prove it. Here is a way of stating the question: Let $A\subset B$ be an integral inclusion of (commutative) rings , $A$ being an integrally closed domain. Let $\mathfrak m$ be a maximal ideal of $B$, let $\mathfrak p$ be the contraction of $\mathfrak m$ in $A$, and let $\mathfrak p'$ be a prime ideal of $A$ contained in $\mathfrak p$:
$$
\begin{matrix}
B&\supset&\mathfrak m\\ 
|&&|\\ 
A&\supset&\mathfrak p&\supset&\mathfrak p'
\end{matrix}
$$ Is $\mathfrak p'$ the contraction of a prime contained in $\mathfrak m$? My suspicion is that the answer is ""not necessarily"". Such an answer would show the necessity of the assumption that $B$ is a domain. [The symbol $\subset$ is used as in Bourbaki, i.e. $X\subset X$ for all set $X$.]","['abstract-algebra', 'maximal-and-prime-ideals', 'commutative-algebra']"
