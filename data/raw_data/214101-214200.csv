question_id,title,body,tags
4337823,A question on irreducible unitary representations of the unitary group $ U(n)$,"Let $n$ be a positive integer and $U(n)$ be the group of $n\times n$ unitary matrices. I have two question regarding the irreducible unitary representations of $U(n)$ . Is there any irreducible unitary representation $\pi$ of $U(n)$ such that $1<\dim \pi < n$ , where $\dim \pi$ is the dimension of the corresponding representation space. Let $\pi_1:U(n)\to GL_n(C)$ and $\pi_2:U(n)\to GL_n(C)$ be defined by $$\pi_1(u)=u,~~~\pi_2(u)=\overline u,~u\in U(n).$$ Here $\overline{u}$ is the conjugate matrix of $u$ .
Clearly, $\pi_1$ and $\pi_2$ are two irreducible unitary representation  of $U(n)$ where the  dimension of the corresponding representation space is $n$ . Are these all irreducible unitary representation  of $U(n)$ (upto unitary equivalence) where the dimension of the corresponding representation space is $n$ ? I know that there is a Weyl dimension formula for  irreducible unitary representation  of $U(n)$ .
But I was not able to solve the above questions using that formula. Also I am very new in this area. Any help or reference will be highly appreciated. Thanks in advance!","['matrices', 'unitary-matrices', 'representation-theory']"
4337876,Proof of Comparability Theorem for well-ordered sets using transfinite recursion,"I am reading through Halmos's ""Naive Set Theory"". In Section 18, Halmos uses transfite recursion to prove the Comparability Thereom for well-ordered sets: The assertion is that if $\langle X, \leqslant_X \rangle$ and $\langle Y, \leqslant_Y \rangle$ are well ordered sets, then either $X$ and $Y$ are similar, or one of them is similar to an initial segment of the other. We assume that $X$ and $Y$ are non-empty well ordered sets such that neither is similar to an initial segment of the other; we proceed to prove that under these circumstances $X$ must be similar to $Y$ . Suppose that $a \in X$ and that $t$ is a sequence of type $a$ in $Y$ ; in other words $t$ is a function from $s(a)$ into $Y$ . Let $f(t)$ be the least of the proper upper bounds of the range of $t$ in $Y$ , if there are any; in the contrary case, let $f(t)$ be the least element of $Y$ . In the terminology of the transfinite recursion theorem, the function $f$ thereby determined is a sequence function of type $X$ in $Y$ . Let $U$ be the function that the transfinite recursion theorem associates with this situation. An easy argument (by transfinite induction) shows that, for each $a \in X$ , the function $U$ maps the initial segment determined by $a$ in $X$ one-to-one onto the initial segment determined by $U(a)$ in $Y$ . This implies $U$ is a similarity, and the proof is complete. I think I understand the proof up until this step: An easy argument (by transfinite induction) shows that, for each $a
  \in X$ , the function $U$ maps the initial segment determined by $a$ in $X$ one-to-
one onto the initial segment determined by $U(a)$ in $Y$ .
This implies $U$ is a similarity, and the proof is complete. I can't seem to get my head around this ""easy transfinite induction"". I think I get what I am meant to do. Let $S$ be the set such that $a \in S$ if and only if there exists a bijection between $s(a)$ and $s(U(a))$ . We need to prove that if $s(b) \subseteq S$ , then $b \in S$ . This would imply that $S = X$ by the transfinite induction principle. I assume I need to construct a bijection from $s(b)$ to $s(U(b))$ using the bijections for each $a < b$ , but I am unsure of how to proceed with this. Note that Halmos has not introduced the concept of ordinal numbers at this point in the book, only well-ordered sets. As a result, I'd appreciate any help which avoids the use of ""ordinal terminology"". There has been a question on the site that has asked about this before but I couldn't really make sense of the answers and comments. EDIT: At the suggestion of the comments I decided to work through the following example: $$
X = a > b > c > d \\
Y = e > f > g > h
$$ Then starting at $a$ we have $$
U(a) = e \\
U(b) = f \\
U(c) = g \\
U(d) = h \\ 
$$ I kind of understand how the proof works in these kind of instances as each element has an immediate predecessor. In this case, we can use the following argument. Any $x \in X$ has an immediate predecessor, say $x^{\prime}$ (apart from base case that is easy). So we must have $s(x) - \{x^{\prime}\} = s(x^{\prime})$ . We construct an isomorphism by taking the isomorphism we know exists from $s(x^{\prime})$ to $s(U(x^{\prime}))$ and simply add the pair $(x^{\prime}, U(x^{\prime}))$ . However, I don't really understand what to do when an element of $X$ does not have an immediate predecessor. For example consider the set of positive integers $X$ , where every odd is greater than every even. Then consider the set $Y$ of positive integers where every even is greater than every odd. In both sets order the odds with respect to each other as normal, and the evens with respect to each other as normal. So we have $$
X = 2 < 4 < 6 < \dots < 1000< 1002 < \dots < 1 < 3 < \dots \\
Y = 1 < 3 < 5 < \dots < 1001 < 1003 < \dots < 2 < 4 < \dots 
$$ How does transfinite recursion deal with the case where $b = 1$ as $1$ has no immediate predecessor? EDIT: I think I understand the proof now. Either: i) $b$ has an immediate predecessor $c$ in $s(b)$ . In which case, all $a \in s(b) - \{c\}$ are mapped one-to-one into $U|_{s(c)}$ and thus $U|_{s(b)}$ . By definition of $U$ , $U(c) < U(b)$ and $U(c) > U(a)$ for all $a \in  s(b) - \{c\}$ , thus $U(c) \in s(U(b)$ and $U(c) \neq U(a)$ for all $a \in s(c)$ . Hence $U|_{s(b)}$ is an injection into $s(U(b))$ . Assume for the sake of contradiction that $U|_{s(b)}$ is not onto. Then there exists some $y$ such that $U(c) < y < U(b)$ . However, then $U(b)$ would not be the least upper bound of $U|_{s(b)}$ , thus we have a contradiction and no such $y$ can exist. So $U|_{s(b)}$ must map onto $s(U(b))$ . Hence $U|_{s(b)}$ is a surjection and an injection and thus a bijection onto $s(U(b))$ as desired. ii) $b$ has no immediate predecessor in $s(b)$ . Then for every $a \in s(b)$ there exists another $c \in s(b)$ such that $a \in s(c)$ . Observe that we can write $U|_{s(b)}$ as $\bigcup_{a \in s(b)}U|_{s(a)}$ , and hence $U|_{s(b)}$ is an injection. Next we prove that $s(U(b))$ is in the range of $U|_{s(b)}$ . For the sake of contradiction assume there is $y \in s(U(b))$ such that $y$ is not in the range of $U|_{s(b)}$ . Then there are three cases that can occur: $y$ is the least element in $s(U(b))$ . Let $x$ be the least element in $X$ . By definition of $U$ we must have $U(x) = y$ . Hence $y$ is in the range of $U|_{s(b)}$ for all $b \neq x$ . Since $U|_{s(x)} = \{\}$ , this implies case (1) cant occur. There are elements $a$ and $c$ in the range of $U|_{s(b)}$ such that $a < y < c$ . However this would imply $y$ is not in the range of $U|_{s(c)}$ , a contradiction. Thus case (2) can't occur. $a < y$ for  all $a \in s(U(b))$ . In this case $U(b)$ is not the least upper bound of $U|_{s(b)}$ , a contradiction by the definition of $U$ . Thus case (3) can't occur. So in all cases, we have a contradiction and thus $s(U(b))$ must lie in the range of $U|_{s(b)}$ . Lastly we observe that the range of $U|_{s(b)}$ lies in $s(U(b))$ . If it didn't then $U(b)$ would not be an upper bound for $U|_{s(b)}$ . Thus we conclude that $U|_{s(b)}$ is a surjection onto $s(U(b))$ . Thus as $U|_{(s(b)}$ is surjective and injective, it is bijection onto $s(U(b))$ as desired. Thus for both cases $(i)$ and $(ii)$ the result holds and we are done. EDIT: This is answer to a question in the comments below posed by jsmith: How does the definition of U imply that U(c) < U(b) By definition $U(a) = f(U^{a})$ so either: $\text{range}(U^{a}) < U(a)$ or $f(U^{a})$ is the least element of $Y$ . In the first case, the result follows trivially. We can show that the second case causes a contradiction. More specifically, if the second case occurs then the range of $U^{a}$ has no upper bound and must contain a maximal element of $Y$ . As a result, $Y$ must be similar to an initial segment of $X$ , which we assumed was not the case. I will leave the details for you to work out but I hope that answers your question!","['elementary-set-theory', 'well-orders', 'set-theory']"
4337878,Does Sigma Algebra Necessarily Induce a Measure?,"I am wondering if we have a space $X$ and an outer measure $\mu^*$ defined on $P(X)$ , is it always true that the restriction $\mu^*|_\mathcal{F}$ is in fact a measure for an arbitrary $\sigma$ -algebra $\mathcal{F}$ on $X$ even without necessarily satisfying the Catheodory’s Criterion? If yes, why? If no, why do we call sets in $\mathcal{F}$ as $\mathcal{F}$ -measurable?","['measure-theory', 'outer-measure', 'real-analysis']"
4337880,What is the probability that two randomly selected derangements of order $n$ are derangements of each other?,"First, let's acknowledge that if $n=1$ then $P(A_1)=0$ , since there are no derangements of a set of order one; and if $n=2$ , $P(A_2)=0$ , since there is only one derangement of a set of order two. For $n=3$ , the derangements of the set { $1,2,3$ } are { $2,3,1$ } and { $3,1,2$ }. Since these are the only examples, and the 1st, 2nd, and 3rd digits are all different, the probability so far is $P(A_3)=1$ . Now take into account $n=4$ . The derangements now are { $2,1,4,3$ } { $2,3,4,1$ } { $2,4,1,3$ } { $3,1,4,2$ } { $3,4,1,2$ } { $3,4,2,1$ } { $4,1,2,3$ } { $4,3,1,2$ } { $4,3,2,1$ } In this example, I think that the chance of picking two derangements that are derangements of each other is $P(A_4)=.5$ , since, for example, if 1 is picked, then there is a $\frac{1}{2}$ chance of picking either 5, 6, 8, or 9 from the eight remaining derangements, and I believe the same is found if any other derangement is picked first. We can define $P(A, n)=\frac{1}{n} \Sigma_{i=1}^n P(A_i)$ . What can we expect $\lim_{n\rightarrow\infty}P(A,n)$ to be? If we cut off $n$ at $n=4$ , then we get $P(A,4) = \frac{1}{4}(0+0+1+.5) = 0.375$ . Can we expect $\lim_{n\rightarrow\infty}P(A,n)$ to converge to zero, or can we expect $\lim_{n\rightarrow\infty}P(A,n)>0$ ? If a non-zero value of the limit exists, what is it, and how can it be found?","['permutations', 'derangements', 'combinations', 'combinatorics']"
4337917,Symmetry groups corresponding to terms in an algebra,"Given an algebra (in the sense of universal algebra ) $\mathcal{A}$ with at least one operation of arity $>1$ , let the symmetry spectrum of $A$ be the class $\mathsf{SySp}(\mathcal{A})$ of finite groups $G$ such that there is some $G\cong H\subseteq S_n$ and some $\mathcal{A}$ -term $t(x_1,...,x_n)$ (in which each variable $x_i$ ( $1\le i\le n$ ) actually appears - no ""dummy variables"") with the property that $$H=\{\sigma\in S_n:\forall a_1,...,a_n\in\mathcal{A}[t(a_1,...,a_n)=t(a_{\sigma(1)},...,a_{\sigma(n)})]\}.$$ For example, although exponentiation is not commutative, the symmetry spectrum of $\mathcal{E}=(\mathbb{N};\mathit{exp})$ does contain the group $S_2$ via the term $(x^y)^z$ . Meanwhile, we have (modulo isomorphism shenanigans) that $\mathsf{SySp}(\mathbb{N};\max)=\{S_n:n\in\mathbb{N}\}$ , and if $\star:\mathbb{N}^2\rightarrow\mathbb{N}$ is injective we have $\mathsf{SySp}(\mathbb{N};\star)$ consists of just the trivial group(s). (Note that we have a choice here between genuine terms and terms with parameters . I'm tentatively more focused in the former, but I'm definitely interested in the latter as well and open to the possibility that the latter is actually more worth considering.) I'm generally interested in what we can say about the function $\mathsf{SySp}$ . One thing I'm thinking about specifically is different ways in which $\mathsf{SySp}(\mathcal{A})$ can be ""large"" for a given algebra $\mathcal{A}$ . Here's one question which has come up in this context: Suppose $\mathsf{SySp}(\mathcal{A})$ contains arbitrarily large finite groups. Must every finite group embed into an element of $\mathsf{SySp}(\mathcal{A})$ ? I strongly suspect that the answer is negative , and indeed there are plenty of natural candidate counterexamples. However, giving sufficiently complete descriptions of symmetry spectra even for ""reasonable"" algebras seems difficult.","['universal-algebra', 'model-theory', 'finite-groups', 'logic', 'group-theory']"
4338032,An iterative logarithmic transformation of a power series,"Consider the following iterative process. We start with the function having all $1$ 's in its Taylor series expansion: $$f_0(x)=\frac1{1-x}=1+x+x^2+x^3+x^4+O\left(x^5\right).\tag1$$ Then, at each step we apply the following transformation: $$f_{n+1}(x)=x^{-1}\log\left(\frac{f_n(x)}{f_n(0)}\right).\tag2$$ A few initial iterations give us: $$
\begin{array}{l}
f_1(x)=1+\frac{x}{2}+\frac{x^2}{3}+\frac{x^3}{4}+\frac{x^4}{5}+O\left(x^5\right), \\
f_2(x)=\frac{1}{2}+\frac{5 x}{24}+\frac{x^2}{8}+\frac{251 x^3}{2880}+\frac{19 x^4}{288}+O\left(x^5\right), \\
f_3(x)=\frac{5}{12}+\frac{47 x}{288}+\frac{2443 x^2}{25920}+\frac{5303 x^3}{82944}+\frac{412589x^4}{8709120}+O\left(x^5\right),
\end{array}\tag3
$$ and their initial terms form the following sequence: $$1,\,\frac{1}{2},\,\frac{5}{12},\,\frac{47}{120},\,\frac{12917}{33840},\,\frac{329458703}{874222560},\,\dots,\tag4$$ whose denominators grow pretty quickly, but which appear to slowly converge to a value $$\lambda\stackrel{\color{#aaaaaa}?}\approx0.3678\dots\tag5$$ If we assume that the process with the iterative step $(2)$ converges to a fixed point, we can see that it must have a form: $$f_\omega(x)=-x^{-1}\,W(-c\,x)=c+c^2 x+\frac{3\, c^3\, x^2}{2}+\frac{8\, c^4\, x^3}{3}+\frac{125\, c^5\, x^4}{24}+O\left(x^5\right),\tag6$$ where $W(\cdot)$ is the Lambert 𝑊-function , and $c$ is a coefficient that is not uniquely determined but depends on the choice of the initial function $f_0(x)$ . In our case, $c=\lambda$ . Questions: Does this process actually converge to a fixed point? If yes, then what is a closed-form expression (or another useful description) for $\lambda$ ? Update: An explicit recurrent formula for the coefficients (the parenthesized superscript $m$ in $a_n^{\small(m)}$ is just the second index of the coefficient; the sum $\sum_{\ell=1}^m$ is assumed to be $0$ when $m=0$ ): $$f_n(x)=\sum_{m=0}^\infty a_{n\vphantom{+0}}^{\small(m)}x^m,$$ where $$a_{0\vphantom{+0}}^{\small(m)}=1,\quad a_{n\vphantom{+0}}^{\small(m)}=\frac1{\,a_{n-1}^{\small(0)}\,}\left(a_{n-1}^{\small(m+1)}-\frac1{m+1} \sum_{\ell=1}^m\ell\;a_{n\vphantom{+0}}^{\small(\ell-1)}\,a_{n-1}^{\small(m-\ell+1)}\right).$$ The sequence of coefficients $(4)$ is $\big\{a_{n\vphantom{+0}}^{\small(0)}\big\}$ .","['logarithms', 'lambert-w', 'closed-form', 'power-series', 'limits']"
4338045,On cardinality of a set of continuous functions,"What is the cardinality of the set of real valued continuous functions $f$ on $[0,1]$ such that $f(x)$ is rational whenever $x$ is rational? I know that it's at least infinitely countably many because for any rational $q$ , the contant function $f(x)=q$ on $[0,1]$ is in the set in question. Also, other functions like $f(x)=x^n$ , for any integer $n$ , are in this set. But, what throws me at this point is my inability to infer whether there are other functions that make the set uncountable. If the case there aren't, how do I prove that it's only countably many?","['continuity', 'real-analysis']"
4338106,Showing a Set is Uncountable (Using Cantor's Diagonalization),"Good day!
Found this tricky exercise in an elementary set theory course: Given the set: $L = \mathcal{P}(\mathbb{N} \times \mathbb{N}) \setminus {}^\mathbb{N}\mathbb{N}$ (where ${}^\mathbb{N}\mathbb{N}$ donotes all the functions from $\mathbb{N}$ to $\mathbb{N}$ )
show it is uncountable. (More precisely show that it's cardinality is greater than $\aleph_0$ ) I started by finding a countable set (which is trivial to do) with in $L$ . That means $|L|\geq \aleph_0$ . Now, suppose for the sake of contradiction that $|L| = \aleph_0$ and let $\varphi: \mathbb{N} \rightarrow L$ be a bijection.
This is where the harder part begins. To reach a contradiction one must construct a relation $R$ over $\mathbb{N}$ which is either not serial or not functional (or both) and satisfies: $\forall n \in \mathbb{N} : R \neq \varphi(n)$ . I am not quite sure how to construct such a relation... For all $n \in \mathbb{N}$ , $\varphi(n)$ is not serial or not functional . So I can see how to construct a relation, for example $\mathbb{N} \times \{0\}$ that will differ from all the non-serial relations $\varphi(n)$ by at least one element (that is because it is serial). However, I am not sure how to proceed from this point and build a relation that will differ from all the other non-functional relations and still be a member of $L$ . Any hints will be greatly appreciated. Thanks to any readers! Have a lovely day!","['elementary-set-theory', 'cardinals']"
4338117,Application of Hahn–Banach theorem to approximation problem.,"Theorem 3.5. of Rudin's functional analysis states Suppose $M$ is a subspace of a locally convex space $X$ , and $x_0 \in X$ . If $x_0$ is not in the closure of $M$ , then there exists $\Lambda \in  X^*$ such that $\Lambda x_0 = 1$ but $\Lambda x = 0$ . And a remark below This theorem is the basis of a standard method of treating certain approximation problems: In order to prove that an $x_0 \in X$ lies in the closure of some subspace $M$ of $X$ it is suffices (if $X$ is locally convex) to show that $\Lambda x_0 = 0$ for every continuous linear functional $\Lambda$ on $X$ that vanishes on $M$ . I'd like to see a couple of applications of this in infinite dimension vector space, but I struggle to find any. Can anyone suggest a reference maybe? Or just show a couple of examples?","['hahn-banach-theorem', 'functional-analysis']"
4338122,Differences in theory of Groebner bases when we work over finite fields.,"I was having a look at the Buchberger algorithm as my work requires solving system of multivariate polynomial equations over finite fields. I am reading from here : Lecture 1 I came across the following passage. I am bit confused whether I should continue with these lecture notes. The reason is that my work is over finite fields, but the author has  explicitly assumed the field to be complex numbers. My question is: Is there much difference in the theory of Groebner bases (Buchberger algorithm) when we study it over finite fields when compared to field of complex numbers? Where do the differences come? I would be grateful for any kind of help. It is traditional in the subject to be coy about which field we are
working over. This is because most authors want to be very general and
hint at the possibility of working over finite field or the field of
real numbers. But, to be honest,I’m really only interested in the case
where the ground field is the complex numbers $\mathbb{C}$ . This is
because the field of complex numbers are closed, and hence the result
we obtain are more extensive.","['finite-fields', 'groebner-basis', 'field-theory', 'algebraic-geometry', 'abstract-algebra']"
4338133,A continuous function such that the inverse image of a bounded set is bounded,"Suppose $f:\mathbb{R}\longrightarrow \mathbb{R}$ be an arbitrary continuous fuction such that the inverse image of a bounded set is bounded. Then show that, $1$ ) The image under $f$ of a closed set is closed. $2$ ) $f$ is not necessarily a surjective function. My attempt ( $1$ ) : Say, $X\subset \mathbb{R}$ is an arbitrary closed set, such that $f(X)=Y\subset \mathbb{R}$ is not closed. Then I am trying to prove by contradiction. Case 1 : $X$ is bounded. Hence $X$ is closed and bounded $\implies$ compact. Since $f$ is continuous, $f(X)=Y$ is also compact $\implies Y$ is closed. (a contradiction) Case 2 : $X$ is not closed and not bounded. So $f(X)=Y$ is not bounded and not closed. If $Y$ is open and not bounded then $Y=\mathbb{R}$ , but $\mathbb{R}$ is clopen (closed and open). (a contradiction). Hence, $f(X)=Y$ where $X$ is closed and not bounded and $Y$ is neither open nor closed and also not bounded. Now I am confused that how to get a contradiction of this final case. My attempt ( $2$ ) : Here, I cannot understand the meaning of the inverse image of a bounded set. I think inverse function will exist only for the bijections. In this context how I can find a continuous map $f$ which is not a surjection, and inverse image of the bounded set is bounded.","['closed-map', 'continuity', 'compactness', 'real-analysis']"
4338189,Is the real solution of $\ln(x)=-e^x$ transcendental?,"The function $$f(x)=\ln(x)+ e^x$$ defined on $\mathbb R_+$ has a unique real root $u$ . It satisfies $$\ln(u)=-e^u$$ and $$\frac{1}{u}=e^{e^u}$$ The numerical value is $$0.269874137573449223877\cdots $$ Can $u$ be proven to be transcendental ? Numerical analysis with PARI/GP with the algdep-command reveals large coefficients upto degree $50$ indicating that $u$ is probably transcendental. The Lindemann-Weierstrass theorem does not help , $e^{e^u}$ need not be transdencental for algebraic $u$ . Perhaps, someone can prove that $u$ is transcendental assuming Schanuel's conjecture. If a transcendentality proof is not possible undonditionally, can at least the irrationality be proven unconditionally ?","['number-theory', 'roots', 'transcendental-numbers']"
4338264,Fattened volume of a curve,"Let $\gamma:[0,1] \rightarrow \mathbb{R}^3$ be a smooth curve with nonvanishing velocity and let $C_\gamma = \gamma([0,1])$ be the image. Denote by $B_r = \{ x : \|x\| < r \}$ the open ball of radius $r$ centered at the origin. Then I expect the following to hold: $$\lim_{r \rightarrow 0} \frac1{r^2} \mathcal{H}^3 (C_\gamma + B_r) = \pi \cdot \mathcal{H}^1(C_\gamma)$$ where $A + B = \{ x+y : x \in A, y \in B \}$ is the Minkowski sum of two sets $A, B$ and $\mathcal{H}^d(A)$ is the $d$ -dimensional Hausdorff measure of $d$ . The above setup can be easily generalised by replacing a curve with a $d$ -dimensional embedded submanifold $M$ with boundary and by replacing $\mathbb R^3$ by $\mathbb R^D$ : $$\lim_{r \rightarrow 0} \frac1{r^{D-d}} \mathcal{H}^D (M + B_r) =  \omega_{D-d} \cdot \mathcal{H}^d(M)$$ with $\omega_k = \pi^{k/2}/\Gamma(\frac k2 + 1)$ being the volume of the $k$ -dimensional unit ball. Does anyone know if this type of result was proven elsewhere before?","['convex-geometry', 'measure-theory', 'differential-geometry']"
4338285,Is my derivation of the summation formula of the first squares correct?,"I have been thinking about the problem of finding the sum of the first squares for a long time and now I have an idea how to do it.
However, the second step of this technique looks suspicious. $$\sum_{i=1}^n i = \frac{n^2+n}{2}$$ $$\int\sum_{i=1}^{n}idi=\int\frac{\left(n^{2}+n\right)}{2}dn$$ $$\sum_{i=1}^{n}\left(\frac{i^{2}}{2}+C_{1}\right)=\left(\frac{n^{3}}{3}+\frac{n^{2}}{2}\right)\cdot\frac{1}{2}+C_{0}$$ $$\sum_{i=1}^{n}i^{2}=\frac{n^{3}}{3}+\frac{n^{2}}{2}-2nC_{1}+2C_{0} $$ Assuming $C_{0}=0$ . Next, we are going to find the constant $C_{1}$ From step 4, we can conclude that: $C_{1}=\frac{n^{2}}{6}+\frac{n}{4}-\sum_{i=1}^{n}\frac{i^{2}}{2n}$ . We can fix $n$ , at any value, it is more convenient to take one( $n=1$ ) then $C_{1}=-\frac{1}{12}$ $$\sum_{i=1}^{n}i^{2}=\frac{n^{3}}{3}+\frac{n^{2}}{2}+\frac{n}{6}$$ Using the induction method, we can prove the correctness of this formula and that the value of the constant $C_{0}$ is really zero. But I created this question because the second step looks very strange, since the left part was multiplied by differential $di$ , and the right by $dn$ . If we assume that the second step is wrong, then why did we get the correct formula of summation of first squares? Note: The technique shown based on the integrated one is really  interesting for me, using the same reasoning we can get the formula of the first cubes and so on EDIT1 According to @DatBoi's comment, we can calculate constants $C_{0}$ and $C_{1}$ by solving a system of linear equations. The desired system must contain two equations, since we have two unknown values( $C_{0}$ and $C_{1}$ ). To achieve this, we need to use the right part of the statement from step 4 twice, for two different n. For simplicity, let's take $n=1$ for first equation and $n=2$ for second equation, then the sum of the squares for these $n$ is 1 and 5, respectively. The main system $$
\left\{ 
\begin{array}{c}
\frac{1}{3}+\frac{1}{2}-2C_{1}+2C_{0}=1 \\ 
\frac{8}{3}+\frac{4}{2}-4C_{1}+2C_{0}=5 \\ 
\end{array}
\right. 
$$ After simplification $$
\left\{ 
\begin{array}{c}
\ C_{0}-C_{1}=\frac{1}{12} \\ 
\ C_{0}-2C_{1}=\frac{1}{6} \\ 
\end{array}
\right. 
$$ Roots: $C_{0}=0$ and $C_{1}=-\frac{1}{12}$ EDIT2 Considering @epi163sqrt's answer, the second step should be changed and it will take this form: $$\sum_{i=1}^{n}\int_{ }^{ }idi=\int_{}^{}\frac{\left(n^{2}+n\right)}{2}dn$$ My hypothesis . If we have: $$\sum_{i=1}^{n}i^{p}=f\left(n,p\right)$$ Where $f$ is a closed form for summation, then this should be true for any natural degree $$\sum_{i=1}^{n}\int_{}^{}i^{p}di=\int_{}^{}f\left(n,p\right)dn\ \to\ \sum_{i=1}^{n}\frac{i^{\left(p+1\right)}}{p+1}=\int_{}^{}f\left(n,p\right)dn-nC_{1}$$ Can you prove or disprove this hypothesis? My questions above are no longer relevant EDIT3. Time for fun. Let's try to get a formula for summing the first cubes $$\sum_{i=1}^{n}i^{2}=\frac{n^{3}}{3}+\frac{n^{2}}{2}+\frac{n}{6}$$ $$\sum_{i=1}^{n}\int_{ }^{ }i^{2}di=\int_{ }^{ }\frac{n^{3}}{3}+\frac{n^{2}}{2}+\frac{n}{6}dn$$ $$\sum_{i=1}^{n}\frac{i^{3}}{3}=\frac{n^{4}}{12}+\frac{n^{3}}{6}+\frac{n^{2}}{12}-nC_{1}+C_{0}$$ $$
\left\{ 
\begin{array}{c}
\frac{1}{4}+\frac{1}{2}+\frac{1}{4}-3C_{1}+3C_{0}=1 \\ 
\frac{16}{4}+\frac{8}{2}+\frac{4}{4}-6C_{1}+3C_{0}=9 \\ 
\end{array}
\right. 
$$ Roots: $C_{0}=0$ and $C_{1}=0$ $$\sum_{i=1}^{n}i^{3}=\frac{n^{4}}{4}+\frac{n^{3}}{2}+\frac{n^{2}}{4}$$ GREAT EDIT4 19.01.2022 So far I have no proof, however, the calculation of constants( $C_{0}$ and $C_{1}$ ) can be significantly simplified by changing the lower index of summation to 0. 1b. Let $M_{p}(n)$ be a closed form to obtain the summation, with degree of $p$ . I. e. $$\sum_{i=0}^{n}i^{p}=M_{p}\left(n\right)$$ 2b. Now let's assume that the statement written below is true $$\sum_{i=0}^{n}\int_{ }^{ }i^{p}di=\int_{ }^{ }M_{p}\left(n\right)dn$$ 3b. For now, we'll just take the integrals. $$\sum_{i=0}^{n}\left(\frac{i^{p+1}}{p+1}+C_{1}\right)=\int_{ }^{ }M_{p}\left(n\right)dn$$ 4b. Now let's express the sum explicitly. Also, we will move the $C_{1}$ without changing its sign, this is a valid action, since multiplying the constant by (-1) leads to another constant $$\sum_{i=0}^{n}i^{p+1}=\left(\int_{ }^{ }M_{p}\left(n\right)dn+nC_{1}\right)\left(p+1\right)$$ 5b. So we got the recurrent formula: $$M_{p}(n) = \left(\int_{ }^{ }M_{p-1}\left(n\right)dn+nC_{p}\right)p$$ $$M_{0}(n) = n+1$$ 6b. Now we have to build and resolve a system for two unknown constants. Therefore, the number of equations is two, we are also going to take n=0 and n=1: $$
\left\{ 
\begin{array}{c}
M_{p}(0)=0 \\ 
M_{p}(1)=1
\end{array}
\right. 
$$ 7b. As I said, we have two constants. In order to see this, we will add a new definition for $W_{p-1}(n)$ that satisfies the following expression: $\int_{ }^{ }M_{p-1}\left(n\right)dn=W_{p-1}\left(n\right)+C_{-p}$ . $$
\left\{ 
\begin{array}{c}
\left(W_{p-1}\left(0\right)+C_{-p}+0C_{p}\right)p=0 \\ 
\left(W_{p-1}\left(1\right)+C_{-p}+1C_{p}\right)p=1
\end{array}
\right. 
$$ 8b. I will skip the formal proof of the fact, but the intuition is that $W_{p}(n)$ is a polynomial that does not have a constant term. Therefore, we can safely know that $W_{p}(0)=0$ . let's rewrite and simplify the system: 8b.1. $$
\left\{ 
\begin{array}{c}
\left(C_{-p}\right)p=0 \\ 
\left(W_{p-1}\left(1\right)+C_{-p}+C_{p}\right)p=1
\end{array}
\right. 
$$ 8b.2. $$
\left\{ 
\begin{array}{c}
C_{-p}=0 \\ 
\left(W_{p-1}\left(1\right)+C_{p}\right)p=1
\end{array}
\right. 
$$ 8b.3 $$
C_{p}=\frac{1}{p}-W_{p-1}\left(1\right)
$$ 9b. We have completed the study of the constant. The last action is to match everything together. $$
M_{p}\left(n\right)=p\left(\left(\int_{ }^{ }M_{p-1}\left(n\right)dn\right)_{n}-n\left(\int_{ }^{ }M_{p-1}\left(n\right)dn\right)_{1}\right)+n
$$ $$M_{0}(n) = n+1$$ 10b. (New step 29.04.2022) The previous step was not recorded correctly. I will also proceed to the calculation of definite integrals: $$  
M_{p}(n) =
\begin{cases}
n+1,  & \text{if $p$ is zero } \\
p\int_{0}^{n}M_{p-1}\left(t\right)dt-np\int_{0}^{1}M_{p-1}\left(t\right)dt+n, & \text{otherwise}
\end{cases}
$$","['integration', 'summation']"
4338296,Find radius that minimizes the surface of the solid,"I have solved the following problem: ""A solid, with a volume of $8cm^3$ , consists of a cylinder and two equilateral cones, external to the cylinder and each with a base in common with the cylinder itself.
Find the base radius so that the surface of the solid is minimal."" but the result doesn't agree with the result of the book, which is $\sqrt[3]{\frac{3+\sqrt{3}}{\pi}}$ . I don't see where my mistake is, so I would appreciate some feedback on my solution, thanks. My solution: Since the cones are equilateral their lateral surface is $\pi r\cdot 2r=2\pi r^2$ ; the lateral surface of the cyilinder is $2\pi rh$ , where $h$ is the height of the cylinder so the total lateral surface is $S=2\pi r^2+2\pi r h$ . From $V_{tot}=8$ we can find $h$ as a function of $r$ by noting that $V_{cone}=\frac{1}{3}\pi r^2\cdot \sqrt{3}r$ , $V_{cyl}=\pi r^2h$ so $V_{tot}=2V_{cone}+V_{cyl}=2\left( \frac{1}{3}\pi r^2\cdot \sqrt{3}r\right)+\pi r^2h=8$ which implies that $h=\frac{8}{\pi r^2}-\frac{2}{\sqrt{3}}r$ . So, $S=2\pi r^2+2\pi r\left(\frac{8}{\pi r^2}-\frac{2}{\sqrt{3}}r \right)=2\pi r^2(1-\frac{2}{\sqrt{3}})+\frac{16}{r}$ hence $S'(r)=4\pi r(1-\frac{2}{\sqrt{3}})-\frac{16}{r^2}=0\Leftrightarrow\fbox{$r=\sqrt[3]{\frac{4\sqrt{3}}{\pi (\sqrt{3}-2)}}$}$ .","['maxima-minima', 'calculus', 'solution-verification', 'geometry']"
4338314,Infinite-dimensional algebraic objects,"Let $H$ be the infinite-dimensional seperable complex Hilbert space, and $P(H)$ denote its projectivization $H / \Bbb{C}^{\times}$ , where $\Bbb{C}^{\times}$ acts on $H$ by multiplication. It can be seen that $P(H)$ has a rather simple topology: It is a model of $K(\Bbb{Z},2)$ . Suppose $A$ is a continuous symmetric $d$ -linear function on $H$ . We can then define the homogeneous polynomial of degree $d$ w.r.t. $A$ as $p_A : H \rightarrow \Bbb{C} : v \mapsto A(\underbrace{v,v,...,v}_{d \text{ times}})$ . The zero locus of $p_A$ is invariant under the $\Bbb{C}^{\times}$ action and has a well-defined projection onto $P(H)$ . This can be seen as an analogue of finite-dimensional projective algebraic sets. We call $p_A$ irreducible if it can not be written as the multiplication of $2$ non-constant homogeneous polynomials, and non-singular if the Fréchet derivative of $p_A$ is non-degenerate everywhere aside from the origin. Question : Are there any nontrivial topological results on the common zero loci of finitely many irreducible/non-singular polynomials, such as their total homotopy groups are finitely generated (e.g. the algebraic sets of degree $1$ have homotopy type $K(\Bbb{Z},2)$ )? In particular, since the unit sphere in $H$ is diffeomorphic to $H$ itself thus admits a pullback complex structure (which differs from most finite-dimensional cases), is it also realizable as an algebraic set? Motivation : Low-degree (compared to the ambient space) hypersurfaces and their intersections have simpler topology (e.g. do not achieve the maximal possible Kodaira dimension) and richer symmetries (e.g. have infinite-order automorphism groups) in contrast to the high-degree generic ones. I'm wondering if every finite number is ""low enough"" compared to the infinity of a Hilbert space.","['hilbert-spaces', 'algebraic-geometry', 'algebraic-topology']"
4338321,Showing that a certain complex matrix is positive definite,"Consider the $n$ -variable complex function $f(z)=f(z_1,\dots,z_n)=\log (1+|z|^2)=\log (1+|z_1|^2+\cdots+|z_n|^2)$ . I am trying to show that the Hessian (the $n\times n$ matrix whose $(i,j)$ -entry is $\frac{\partial^2 f}{\partial z_i\partial \bar{z}_j}$ ) is complex positive definite. By calculation I've shown that the Hessian is given by $\frac{1}{(1+|z|^2)^2} (a_{ij})$ where $a_{ij}=-z_j\bar{z}_i$ if $i\neq j$ and $a_{ii}=1+\sum_{j\neq i}|z_j|^2$ . Clearly it suffices to show that the matrix $(a_{ij})$ is positive definite, but it seems that the calculation showing this directly is quite complicated. Any hints please?","['matrices', 'complex-analysis', 'linear-algebra', 'hessian-matrix', 'positive-definite']"
4338351,Counterexample Immersed Submanifold.,"Let $M$ be a manifold. Let $N$ be a subset of $M$ and suppose that $N$ is endowed with a manifold structure for which the inclusion $i: N \rightarrow M$ is a smooth map. Is $N$ necessarily an immersed submanifold of $M$ ? My definition of an immersed submanifold is the following: an immersed submanifold of $M$ is a subset $H \subset M$ with a topology and a smooth manifold structure such that the inclusion $i: H \rightarrow M$ is an immersion. Looking at the definition of an immersed submanifold, it looks clear to me that the statement is not true since the derivative of the inclusion at every point has to be injective. However, I can't come up with a counterexample. Furthermore, I have trouble to find examples of inclusions whose derivative at a point is not injective. Someone who can given me a clear example? Thanks in advance!","['manifolds', 'submanifold', 'differential-geometry']"
4338356,Continuous bilinear maps on sections of vector bundles,"Let $E \to M$ and $F \to N$ be two vector bundles over smooth manifolds $M, N$ . Denote by $\pi_1, \pi_2$ the projections of $M \times N$ to $M$ and $N$ , respectively. Equip the spaces of sections of a vector bundle with its usual Fréchet space structure to make it into locally convex vector spaces. If we denote by $E \boxtimes F := \pi_1^* E \otimes \pi_2^* F$ the external tensor product vector bundle over $M \times N$ , is there then a natural vector space isomorphism $$ \Gamma(E \boxtimes F)^* \cong \left( \Gamma(E) \otimes_{\mathbb{R}} \Gamma(F) \right)^*?$$ The dual on the left-hand side denotes the continuous dual of a locally convex vector space, the dual on the right-hand side denotes the space of jointly continuous, $\mathbb{R}$ -bilinear maps. I think this is folklore, but I cannot for the life of me find a source proving this precise result. Is there a canonical source to go to? Is this somewhere in Dieudonné's ""Eléments d'analyse""?","['topological-vector-spaces', 'vector-bundles', 'functional-analysis', 'dual-spaces', 'differential-geometry']"
4338414,What is differential probability? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I am trying to understand the following statement (taken from a context related to volumetric image rendering): Density denotes the differential probability that a ray interacts with the volumetric “medium” of the scene at a particular point What does differential probability mean? (especially that density in this context is defined at any point in 3D space)","['differential', 'density-function', 'probability', 'differential-geometry']"
4338429,Let $a_n$ be the position of the nth $1$in the string $t_n$. Prove that: $a_n = [\frac{1+\sqrt5}{2} . n]$,"Consider the transformation $f$ acting on a binary string: turn every $(0, 1)$ in it into $ (1, 01).$ The notation $ s_n$ is the string produced after acting $ f $ on $1$ all n times.
Example : $ 1\Rightarrow 01 \Rightarrow 101 \Rightarrow 01101 \Rightarrow ...$ $1)$ Calculate the number of pairs $ (0,0) $ appearing in the string $s_n$ $2)$ Arrange consecutive strings $s_1, s_2,... s_n$ into a string $t_n$ . Let $a_n$ be the position of the nth $1$ and $b_n$ the position of the nth $0$ in the string $t_n$ . Prove that: $a_n = [\frac{1+\sqrt5}{2} . n]$ , $b_n = [\frac{1+3\sqrt5}{2} . n]$ Note: $[z]$ is the floor function. $+)$ We will prove inductively that no two zeros are next to each other $.(1)$ Let $(1)$ be true for $n = k .$ Consider the following minor cases: Case $1: ....11.... \Rightarrow ....0101....$ Case $2: 01... \Rightarrow 101.....$ Case $3: 101....\Rightarrow 01101....$ So $(1)$ is also true for $n = k+1 .$ We complete the proof $(1).$ Thus the number of pairs $(0,0)$ in the string $s_n = 0$ for all $n.$ I am very stuck with request $2) . $ I hope to get help from everyone. Thanks very much !","['combinations', 'combinatorics', 'combinatorial-proofs', 'combinatorial-game-theory']"
4338467,What is the definition of the vector field which is generated by rotations of the circle,"I'm reading the first lines of page 5 in the article which called ""Cohomologie équivariante et théorème de Stokes"", which says Let $M$ be a smooth manifold on which $S^1$ acts. Let $J$ be the vector field generated by the rotations of a circle. We denote by $\mathcal{L}(J)$ the lie derivative in the direction of $J$ . If $\xi$ is a vector field which vanishes at a point $p$ , then $\mathcal{L}(J)$ induces an invertible transformation on $T_pM$ . What does it mean that $J$ is the vector field generated by rotations of a circle?","['vector-fields', 'group-actions', 'differential-geometry']"
4338508,Book for Blowing up,"I'm looking for a book for blowing up in algebraic geometry. I prefer those for beginners containing enough details and plenty of computable concrete examples. In Hartshorne, there are a lot of examples of blowing up throughout the book. But, in my opinion, most of them are not in detail. He only sketches how to compute blowing up, strict transform, and some intersection properties. And I always feel difficult to fill the details.","['complex-geometry', 'algebraic-geometry', 'schemes']"
4338513,What is the probability that all faces have appeared in some order in some six consecutive rolls?,We roll a 6-sided die n times. What is the probability that all faces have appeared in some order in some six consecutive rolls? Is there a way to do this without resorting to markov chains?,"['dice', 'probability-theory', 'probability']"
4338514,"Prove that in a $4n$-gon, every other diagonal passes through a common point","Suppose two regular $2n$ -gons in the plane, which interesect one another to form a $4n$ -gon. Prove that every other diagonal of this $4n$ -gon, i.e. $P_{1}P_{2n+1},P_{3}P_{2n+3},...,P_{2n-1}P_{4n-1}$ passes through a common point with every other. A $4n$ -gon has an even number of sides, so the diagonals of a regular $4n$ -gon all meet in a single point. We can obtain a regular polygon easily by using e.g. two congruent polygons rotated by $ 45^{\circ}$ and this fulfills the condition. Even if the resulting polygon is not regular, it should hold (for every other diagonal). In all examples I attempted this is the case, but I had trouble proving it.
I'll be glad to hear your ideas.","['euclidean-geometry', 'geometry', 'polygons']"
4338532,Find all values of $a$ for which the inequality $(x-a-2)(x^2-(a^2+5a-3)x+5a^3-2a^2-5a+2) \leq 0$ has at most one positive solution,"Find all values of $a$ for which the inequality $(x-a-2)(x^2-(a^2+5a-3)x+5a^3-2a^2-5a+2) \leq 0$ has at most one
positive solution. Attempt: The equation has three roots $x_{1}=a+2$ , $x_{2}=a^{2}-1$ and $x_{3}=5 a-2$ . In order for the inequality to have at most one root, the condition that one root is non-positive, and the other two coincide and are positive, must be satisfied. Further, we consider three cases: $x_{1} \leq 0, \quad x_{2}=x_{3}>0$ $x_{2} \leq 0, \quad x_{1}=x_{3}>0$ $x_{3} \leq 0, \quad x_{1}=x_{2}>0$ Right? Or is it possible to reason differently?","['algebra-precalculus', 'inequality']"
4338556,Transformation of an exponential distribution,"Suppose that $X\sim Exp(p)$ is exponentially distributed with expectation $1/p$ . Does there exist transformation $f:\mathbb{R}\to\mathbb{R}$ such that $f(X)$ will be some (ideally ""nice"") distribution with expectation $\mathbb{E}f(X)=p$ ? Here, $f$ can not depend on $p$ . Context: In GLM models (generalized linear models) we usually deal with a random variables $Y\sim exp(p)$ where $p=\beta_0+\beta_1X_1$ . So $\mathbb{E}Y=\frac{1}{\beta_0+\beta_1X_1}$ which is kind of ugly. It would be nice if we can change Y in such a way that $f(Y)$ will have nice linear expectation. Is it possible?","['statistics', 'probability-distributions', 'exponential-distribution']"
4338584,Two definitions of ergodicity,"Definition A: Let $(X,\mathcal{A},\mu)$ be a probability space. Let $T: X\rightarrow X$ is $\mu$ -invariant ( $\mu(T^{-1}E)=\mu(E)$ for all $E\in \mathcal A$ ). Then $T$ is ergodic if for every $E\in \mathcal{A}$ with $T^{-1}(E) = E$ , we have $\mu(E)=0$ or $1$ . Definition B: Let $(X,\mathcal{A},\mu)$ be a probability space. Let $T: X\rightarrow X$ is $\mu$ -invariant ( $\mu(T^{-1}E)=\mu(E)$ for all $E\in \mathcal A$ ). Then $T$ is ergodic if for every $E\in \mathcal{A}$ with $T^{-1}(E) \subset E$ , we have $\mu(E)=0$ or $1$ . The difference between two definitions is $T^{-1}(E) = E$ vs. $T^{-1}(E) \subset E$ . Definition A comes from Definition 2.13 of the book by Einsiedler and Ward and Definition B is from the entry ""Ergodicity"" of Wikipedia . There are more things to check for the Definition B. So B is stronger than A. Can we prove they are equivalent?","['measure-theory', 'ergodic-theory']"
4338603,Why is the rank of the determinant map constant?,"Consider the group $GL(n,\mathbb{R})$ . I am interested in showing that the determinant map on this group has a constant rank. Our professor justified this fact in the following way, let $A\in GL(n,\mathbb{R})$ and $a=det(A).$ Then, $$\operatorname{det} = l_a\circ \operatorname{det}\circ L_{A^{-1}}$$ where $l_a(x)=ax$ for $x\neq 0$ and $L_{A^{-1}}(X)=A^{-1}X$ are left translation on $\mathbb{R}^*$ and $GL(n,\mathbb{R})$ respectively. Then using the chain rule one gets that $$\operatorname{rank} d_{X}\operatorname{det} = \operatorname{rank} d_{A^{-1}X}\operatorname{det}$$ and then choosing $A=X$ we can see that the rank is constant. I am not sure how exactly is the chain rule being applied here explicitly to deduce that the rank of the two operators is the same at different points. Could one show me the computation explicitly if possible?","['matrix-rank', 'inverse-function', 'lie-groups', 'differential-geometry']"
4338632,Convergence of estimator defined by supremum over measurable sets,"Let $X \in L_1$ be a positive random variable on the probability space $([0,1], \mathcal B, P)$ , where $\mathcal B$ is the Borel $\sigma$ algebra on $[0,1]$ . Consider $$\phi(A) = E[X\mid A] \cdot I\big( P(A) \geq c \big),$$ for $A \in \mathcal B$ and $1 > c > 0$ , and define $\phi^* = \sup\limits_{A \in \mathcal B}
\phi(A)$ . Here, $I(\cdot)$ is an indicator function. Let $\{X_i\}_{i=1}^n$ be a random sample. Let $P_n:\mathcal B \rightarrow \mathbb R$ be the empirical measure; i.e. $P_n(\cdot)=\sum_{i=1}^n \delta_{X_i}(\cdot)$ , where $\delta_{X_i}$ is the Dirac measure at $X_i$ , and let $E_n[\cdot]=\int_{\Omega} \cdot dP_n $ be the empirical expectation. Consider $$\phi_n(A) = \frac{E_n[X \cdot I(A)]}{P_n(A)} \cdot I\big( P_n(A) \geq c \big).$$ and define the estimator $\phi_n^* = \sup\limits_{A \in \mathcal B}\phi_n(A)$ . I am interested in what can be said about whether $\phi_n^*$ converges to $\phi^*$ almost surely (or in probability). Observations: For each fixed $A \in \mathcal B$ we have that $\phi_n(A) \longrightarrow \phi(A)$ almost surely by the strong law of large numbers. For each $n$ we can find $A_n \in \mathcal B$ such that $\phi_n(A_n) = \phi_n^*$ .
I.e. $A_n$ is a candidate $A \in \mathcal B$ that maximizes $\frac{E_n[X \cdot I(A)]}{P_n(A)} \cdot I\big(P_n(A) \geq c \big)$ . If there is an $A^* \in \mathcal B$ such that $\phi^* = \phi(A^*)$ , then we have that $$\phi_n(A_n) \geq \phi_n(A^*) \longrightarrow \phi(A^*) = \phi^*$$ almost surely, from obervation 1) and 2).","['parameter-estimation', 'asymptotics', 'empirical-processes', 'convergence-divergence', 'probability-theory']"
4338645,Epsilon-Delta proof of an infinite limit,"Whilst searching in some of my older books, I found this particular exercise: If $\displaystyle\lim_{x \to +\infty}{\frac{f(x+1)}{f(x)}=1,}$ prove that $\displaystyle\lim_{x \to +\infty}{\frac{f(x+4)}{f(x)}=1.}$ Now, I managed to solve this problem with a trick by constantly applying the substitution $x\to x+1$ . So we get: $\displaystyle\lim_{x \to +\infty}{\frac{f(x+2)}{f(x+1)}=1,} \displaystyle\lim_{x \to +\infty}{\frac{f(x+3)}{f(x+2)}=1,}$ and $\displaystyle\lim_{x \to +\infty}{\frac{f(x+4)}{f(x+3)}=1.}$ In the end, $\displaystyle\lim_{x \to +\infty}{\frac{f(x+1)}{f(x)}\cdot\frac{f(x+2)}{f(x+1)}\cdot\frac{f(x+3)}{f(x+2)}\cdot\frac{f(x+4)}{f(x+3)}=\displaystyle\lim_{x \to +\infty}{\frac{f(x+4)}{f(x)}}}=1$ . But I am looking for something closer to using the definition of $\epsilon-\delta$ and I was wondering if that truly was possible. I searched but was not able to find a solution. Thanks!","['limits', 'calculus', 'epsilon-delta']"
4338673,Can an incomplete geodesic intersect a point infinitely many times?,"Consider a Lorentzian manifold $(M,g)$ . Must a maximal, affinely parameterized geodesic $\gamma: [0,b) \to M$ which intersects a point $p \in M$ infinitely many times be complete (i.e., have $b = \infty$ )? It's easy to show that the answer is positive in the Riemannian setting by considering a normal ball around $p$ and noticing that the geodesic must radially traverse its diameter infinitely many times, accumulating infinite length. The same picture applies in the Lorentzian case, up until the critical conclusion of ""accumulating infinite length"", since the ""diameter"" of a normal neighborhood is no longer meaningful. Indeed, the radial trajectories may, in principle become closer and closer to null with subsequent passings of $p$ in such a way that the accumulated length/proper time is finite (or zero if the geodesic was null in the first place). While the Riemannian reasoning breaks down, is there some other reason that intersecting a single point infinitely many times enforces completeness in general, or can one perhaps find a counterexample? Feel free to assume any causality condition you like, so that the question is really about spacelike geodesics. It is worthwhile to note that the most obvious way to obtain infinitely repeated intersection of a single point is, of course, to consider a closed loop, which is always complete when maximally extended (at least when non-null, which would be the case under even a very lax causality condition). Also worth noting is that maximal, incomplete geodesics with limit points can exist (e.g. in the Misner spacetime), which demonstrates that something too close to the Riemannian reasoning (which also rules this out in a Riemannian setting) can't work. Edit: Didier has helpfully provided the counterexample of the Clifton-Pohl torus . This example, being compact, admits closed timelike curves, however, so I am still interested in the question of whether this is possible under a more restrictive causality condition. In particular, since the issue arising in the above normal neighborhood picture is that the geodesic may become too close to null in normal coordinates, it seems intuitive that $M$ 's being stably causal (i.e. $M$ would not admit closed causal curves even under small perturbations to the metric) may rule this out. Can this intuition be made precise, or could even a less restrictive causality condition suffice?","['general-relativity', 'semi-riemannian-geometry', 'geodesic', 'differential-geometry']"
4338684,show that $f$ is constant over the level sets of $\phi$ if $G = \nabla \phi $ and $fG$ is conservative,"As the title suggests, i'm having trouble to show this. By premise, $G : \mathbb{R}^3 \rightarrow \mathbb{R}^3$ , $\phi: \mathbb{R}^3 \rightarrow \mathbb{R}$ twice continously differentiable and $f : \mathbb{R}^3 \to \mathbb{R}$ . The problem suggests using the following identity: $\nabla \times (fG) = \nabla f \times G \ + f(\nabla \times G)$ . By replacing $G = \nabla \phi$ , using that $fG$ is conservative, and that since $\phi \in C^2 \implies \nabla \times (\nabla \phi) = 0$ , it can be concluded that $\nabla f \times \nabla \phi = 0$ . Now, how can I use this fact to prove that $f$ is constant over the level sets of $\phi$ ?","['multivariable-calculus', 'vector-analysis']"
4338765,Hartshorne proposition III.9.7,"I cannot follow the end of the proof of proposition III.9.7. The situation is as follows. We have a map $f:X\to Y$ of schemes with $Y$ integral, regular of dimension 1. The proposition says that $f$ is flat if and only if every associated point of $X$ maps to the generic point of $Y$ . It's the if-part I cannot follow. Hartshorne takes $x\in X$ and assumes $y:=f(x)$ is a closed point. Then $\mathcal{O}_{Y,y}$ is a DVR so we can choose a uniformizing parameter $t$ . We want to show $\mathcal{O}_{X,x}$ is torsion free so to get a contradiction we assume $t$ is a zero divisor on $\mathcal{O}_{X,x}$ . This means $f^\#(t)$ lands in some associated prime $P\subset \mathcal{O}_{X,x}$ of $(0)$ and $P$ corresponds to an associated point $x'\in X$ . Hartshorne claims that $f(x')=y$ which would be a contradiction but I cannot figure out why $x'$ could not map to the generic point of $Y$ . Any help is appreciated!","['algebraic-geometry', 'commutative-algebra', 'sheaf-theory']"
4338772,"Is this object a simpler Brunnian ""rubberband"" loop than those studied?","The standard configuration of Brunnian ""rubberband"" loops shows a series of unknots each bent into a U-shape, with their ends looped around the middle of the next unknot, as shown here (drawn by David Epstein) Brunnian rubberband loop object : This has the defining Brunnian link property that all elements are interlocked, but the removal of any element causes all the others to disentangle.  But it requires each element to link to the next with 8 cross-overs, and be bent around so as to form a minimum of four bights along its ""length"". I have been playing around with an object that I call an ""exaltation of larks"", since it is based on lark's-head knots to connect the elements.  This reduces the number of cross-overs to 6 per pair, and allows for each element to have only two bights along its length.  A visual is here exaltation of larks object : This also seems to fulfill the Brunnian definition, and is substantially simpler.  I am wondering if (1) this object is already known and has another name, and/or (2) the object is not Brunnian for some reason I have overlooked.  Thanks!  (First time here...) Added 12/22/21: I've done a pretty good look-back in the papers at this point, and here's what I've come up with.  Dale Rolfsen describes a Brunnian link which is equivalent to a five-element ""exaltation of larks"" (p. 69, exercise 15, Knots and Links, 1976).  This is cited directly in at least one other place (""New Criteria and Construction of Brunnian Links"" by Sheng Bai and Weibiao Wang https://arxiv.org/pdf/2006.10290.pdf ).  None of those authors note that this is a simpler version of the Brunnian chain than the standard, and most compendiums of Brunnian links do not show this pattern at all. So I think I have what I need here, thanks for the help!","['knot-theory', 'general-topology', 'knot-invariants']"
4338788,Counting ways to distribute 7 apples and 5 pears between 4 children with restriction,"I was given the following problem: In how many ways $7$ apples and $5$ pears can be distributed between four kids if each child gets three fruits? My counting is not the strongest, here is how I approached the problem: first I will distribute the pears then fill the missing spots with apples. We can use stars and bars to count the distribution of pears: $\binom{5+4-1}{4-1}=\binom 8 3=56$ . We need to subtract cases when there are more than $3$ pears: $4$ (one basket with $5$ pears) and $\frac{4!}{2!}$ ( $4$ pears in one basket and one in some other basket). Thus, the total number is $56-4-12=40$ . Is this correct? Is there a better way to explain this problem to students?","['combinations', 'combinatorics']"
4338793,"Can you use a random variable as intput to the Dirac delta, i.e. $\delta(X)$?","Question : If $X$ is a random vairable and $\delta$ is the Dirac delta, is $\delta(X)$ meaningful? Useful? Motivation: In this other question , we have $X\sim Exp(p)$ , and it is asked whether there exists a function $g:\Bbb R\to \Bbb R$ , independent of $p$ , such that $\Bbb Eg(X) = p$ . The comments revealed that this is not possible. However, it is possible to approximate such an $g$ by approximating a ""one-sided"" Dirac delta. For example, with $g_\alpha = \alpha I_{[0,\frac1\alpha]}$ for $\alpha>0$ , where $I_{A}$ is an indicator function, we have in fact $$
\lim_{\alpha\to\infty}\Bbb Eg_\alpha(X) 
= \lim_{\alpha\to\infty}\Bbb E\left[\alpha I_{[0,\frac1\alpha]}(X)\right] 
= p.
$$ Knowing this, I am now tempted to simply say that $g=\delta$ somehow answers the problem (since $g_\alpha$ has the properties of $\delta$ in the limit). In other words, $\delta(X)$ should be some sort of ""random variable distribution(?)"" with an expected value of $p$ . Caveat: The limit is not really $\delta$ , because I don't think $\int_0^\infty\delta(x)f(x)\ dx$ is well-defined in general; or maybe it is $\frac{f(0)}{2}$ ? I would like something like $\delta^+$ that satisfies $\int_0^\infty\delta^+(x)f(x)\ dx = f(0)$ , but I'm out of my depth here. Anyway, that's not the point of my question, so please just imagine I had chosen a better example... I somehow suspect $\delta(X)$ makes sense if you ""intend to"" use it in an integral, but naturally $\delta(X)$ will not be a random variable in the usual sense (right?). For example, I might argue that $\Bbb E \delta(X)$ is meaningful, because we can write $$
\Bbb E \delta(X) \overset?= \int_\Bbb R \delta(x)f_X(x)\ dx = f_X(0).
$$ I should mention that my knowledge of the Dirac delta is rudimentary. I know how it can be used in integrals, and that it can be seen as a measure or as a distribution. I understand the measure perspective, but to be honest, I don't know the formalities of distribution theory. I think this question naturally points to a distribution perspective, because of the family of functions, and this is why I am on shaky ground here.","['expected-value', 'probability-distributions', 'probability-theory', 'dirac-delta']"
4338810,MLE of number of colors,"I'm looking at this question and the solution given and I understand it, but I'm unable to see where I'm going wrong. The question states that there are $k$ equally frequent colors and we do not know $k.$ We examine four smarties and notice that they are red, green, red, orange. We wish to find the maximum likelihood estimate $\hat{k}$ . The solution given is that $$\text{lik}(k) = \frac{(k-1)(k-2)}{k^3} $$ since we are looking at the probability that the second and fourth color differs from the first and the third is equal to the first. This can easily seen to be maximized when $\hat{k} = 5$ . My issue is why we are looking at the probability that the second colour and fourth are different from the first. If we instead look at the probability as the probability of seeing R, G, R, O given there are $k$ colours then the likelihood function is just $$\text{lik}(k)=\frac{1}{k^4}$$ since all sequences of colours are equally likely. This is maximized when $k=3$ as there must at least be 3 different colors. I can sort of see that I'm going wrong somewhere as my answer is independent of the sequence we get, but where exactly am I going wrong? And why is the correct interpretation to ignore the actual sequence we get and only look at the differentiation between the colors? EDIT: I'm trying to reimagine a question with $k$ being the maximum positive integer allowed and we see a specific sequence 3, 1, 3, 7. In that case I believe my interpretation would probably be correct. So it must have something to do with the fact that colors aren't ordered, but I'm not able to convince myself exactly what the issue is.","['statistics', 'parameter-estimation', 'maximum-likelihood']"
4338873,The Heat Equation in Brezis' book,"I am reading the heat equation in Functional Analysis, Sobolev Spaces, and Partial Differential Equations by Haim Brezis, and having some concerns about the proof, whose screenshot is as attached below. The notation $u\in L^2 (0, \infty; H^1_0(\Omega))$ I can't find the definition of the notation in the book, but I believe it means $$ \int_0^\infty \left\Vert u(t) \right\Vert ^2 _{H^1_0(\Omega)} 
 dt < \infty$$ On $H^1_0(\Omega)$ , $\left\Vert \nabla u(t) \right\Vert _{L^2(\Omega)} $ and $\left\Vert u(t) \right\Vert _{H^1(\Omega)} $ are equivalent norms by Poincare inequality. The boundary condition has been incorporated in the definition of the domain of A. I think the key is $H_0^1(\Omega)$ since for a function $g\in H_0^1(\Omega)$ that admits a continuous representative, $g$ vanishes on the boundary. From the definition of domain $D(A) = H^2(\Omega) \cap H^1_0(\Omega)$ , does it imply $g\in D(A)$ has a continuous representative? I believe the answer is no. $D(A^l) = \{g\in H^{2l}(\Omega); g = \Delta g = ... = \Delta^{l-1}g = 0 \text{ on the boundary} \}$ I can see $D(A^l) \subset H^{2l}(\Omega)$ . The argument ( $l = 2$ as an example) goes as follows. $$ g \in D(A^2) \Rightarrow g \in D(A), - \Delta g \in D(A) \Rightarrow - \Delta g + g \in H^2(\Omega) \Rightarrow g \in H^4(\Omega)$$ The last arrow follows from a regularity theorem (Theorem 9.25). But again, $g\in D(A^l)$ may not have a continuous representative (at least when $l$ is not sufficiently large) so how to see $g = \Delta g =... = 0$ on the boundary? $u\in C^k((0,\infty); C^k(\bar \Omega))$ for all k My understanding is that $u$ as a function from $(0,\infty)$ to $C^k(\bar \Omega)$ is a $C^k$ function, and for each fixed $t_1$ , $u(t_1)$ is a $C^k$ function on $\bar \Omega$ . How do we conclude from here that $u(x,t): \Omega \times (0, \infty) \to \mathbb{R}$ is $C^k$ ? The differentiability of $u\in C^k((0,\infty); C^k(\bar \Omega))$ uses the Banach space topology while the partial derivative $u_t(x,t)$ in $t$ is formulated in the usual sense. More specifically, let $[U(t)](x)$ be the map viewed as $(0,\infty)$ to $C^k(\bar \Omega)$ and $u(x,t)$ be the one viewed as $\Omega \times (0,\infty) \to \mathbb{R}$ , how to show that $u_t(x,t) = d[U(t)](x)/dt $ ? (Update) I consulted Evan's PDE book, and my own answers (at least how I convinced myself) go as follows. For point 1, yes. More generally, $u\in L^p(0,\infty; X)$ for a Banach space $X$ means $$ \int_0^\infty \left\Vert u(t) \right\Vert ^ p _X dt < \infty $$ For point 2 & 3, we need the fact that $$ u \in H^1_0 (\Omega) \iff u = 0 \text{ on the boundary}$$ To justify the statement (since $u$ can differ from a set of measure zero and the boundary has measure zero), we probably need the theory of trace. Anyways, in Brezis' book, only a special case is proven (when $u$ has a continuous representative, see Theorem 9.17), and the trace operator is briefly mentioned in page 315. I think I came up with a solution for point 4. Let $u(x,t) = [U(t)](x)$ . Fix a point $x_0 \in \Omega$ . $$(\frac{d[U(t)]}{dt}) (x_0) = (\lim_{h\to 0}\frac{U(t+h)-U(t)}{h})(x_0) = \lim_{h\to 0}\frac{[U(t+h)](x_0)-[U(t)](x_0)}{h} = u_t(x_0,t)$$ since the continuity of evaluation at $x_0$ allows us to pass through the limit. If there is any mistake or there is a better way/answer for my questions, please let me know. Anyways I hope this post helps those who only read Brezis' book like me.","['sobolev-spaces', 'functional-analysis', 'heat-equation', 'partial-differential-equations']"
4338875,Finding polynomial without constant term that commutes with $f(x)=x^3+3x$,"Consider the polynomial $f(x)=x^3+3x$ over $\mathbb{Z}$ . I am trying to find a polynomial $g(x)$ $(\neq f^{\circ n})$ of any degree (or series) without constant term which commutes with $f$ (or any iteration $f^{\circ n}, ~n \geq 1)$ under composition. Trivially, any $g(x)=x$ , is another polynomial (or series) commutes with $f(x)$ . By hand it seems to be laborious. Suppose I start with an investigation if there are degree $2$ polynomial $g(x)=ax+bx^2$ such that $f \circ g=g\circ f$ . Then \begin{align}
&f(g(x))=f(ax+bx^2)=3(ax+bx^2)+(ax+bx^2)^3=3ax+3bx^2+a^3x^3+3a^2bx^4+3ab^2x^5+b^3x^6, \\
&g(f(x))=g(x^3+3x)=a(x^3+3x)+b(x^3+3x)^2=3ax+ax^3+bx^6+6bx^4+9bx^2
\end{align} Comparing both equations, we get $b=0$ and $a=a^3 \Rightarrow a=\pm 1$ . In this case $g(x)=\pm 1$ , the trivial one. Suppose I start with an investigation if there are degree $3$ polynomial $g(x)=ax+bx^2+cx^3$ such that $f \circ g=g\circ f$ . Then \begin{align}
&f(g(x))=f(ax+bx^2+cx^3)=3(ax+bx^2+cx^3)+(ax+bx^2+cx^3)^3=3ax+3bx^2+3cx^3+c^3x^9+3bc^2x^8 \hspace{3cm}+(3ac^2+3b^2c)x^7+(6abc + b^3)x^6 + (3a^2c + 3ab^2)x^5 + 3a^2bx^4 + a^3x^3, \\
&g(f(x))=g(x^3+3x)=a(x^3+3x)+b(x^3+3x)^2+c(x^3+3x)^3=3ax+ax^3+2bx^6+6bx^4+9bx^2+cx^9+9cx^7+27cx^5+27cx^3
\end{align} Comparing both sides we get $b=0$ and the following equations: \begin{align}
a^3-a=24c, \\
a^2c=9c, \\
3ac^2=9c,\\
c^3=c.
\end{align} Solving these, we see $c^3=c$ and $3ac^2=a^2c$ . These two gives us $c=0$ or $c=\pm 1$ . If $c \neq 0$ , then $a=\pm 3$ . Thus $g(x)=\pm (3x+ x^3)$ , which is equivalent to $f(x)$ upto signs. Suppose I start with an investigation if there are degree $4$ polynomial $g(x)=ax+bx^2+cx^3+dx^4$ such that $f \circ g=g\circ f$ . Then it becomes laborious. Is there any way to find non-trivial $g$ with the help of PARI/GP or SAGE ? Edit 1: According to the hints given by @achille hui, I have found that $g(x)=-5x-5x^3-x^5$ commutes with $x^3+3x$ . However, I am looking for an polynomial whose first degree coefficient is $3$ or multiple of $3$ . I would appreciate one such example. Edit 2: But I need to find the polynomial with degree one coefficient, a multiple of $3$ and it is certainly possible as $f$ commutes with its iteration and each iteration has the degree one coefficient , a multiple of 3","['elementary-number-theory', 'ring-theory', 'abstract-algebra', 'polynomials']"
4338884,Any Well-Ordering of $\mathbb{R}$ has no Corresponding Metric,"Background I have been doing some reading over winter break so far and found the idea that $\mathbb{R}$ has a well-ordering strange. So I have been thinking about it a little and was wondering if my proof for the following property is sound. If not, is it true? Proof critics are always encouraged! Thanks! Statement First, let me define a term Definition: An ordering $\leq$ has a corresponding metric $d(x,y)$ if: $$x < y < z \Rightarrow d(x,y) < d(x,z)\ \land\ d(y,z) < d(x,z)$$ The property I have tried to prove is Theorem: Any well-ordering of $\mathbb{R}$ has no corresponding metric Proof Because of the well-ordered property, we can define $L_n$ to be the set containing the $n$ least elements in $\mathbb{R}$ . $\forall x \in \mathbb{R}\setminus L_2$ , there exists an $r_1 \in \mathbb{R}$ such that the open ball $B_{r_1}(x)$ has two unique least elements not equal to $x$ . Call the smaller of the two $l$ . Because $B_{r_1}(x)$ is open, there exists some $\epsilon_1 > 0$ such that $B_{\epsilon_1}(l) \subset B_{r_1}(x)$ . Now define $r_2 := d(x, l)$ . By definition of open ball, $l \not\in B_{r_2}(x)$ . Because of well-order, $B_{r_2}(x)$ has some least element $l'$ . There exists some $\epsilon_2 > 0$ such that $B_{\epsilon_2}(l') \subset B_{r_2}(x)$ . So we have $l < l'$ , $l \not\in B_{\epsilon_2}(l')$ and so $\forall y \in B_{\epsilon_2}(l)$ , $y < l' \Rightarrow y \not\in B_{r_2}(x)$ . Thus, $B_{\epsilon_2}(l) \cap B_{r_2}(x) = \emptyset$ . Now, let $\epsilon := \min(\epsilon_1, \epsilon_2)$ . $B_\epsilon(l) = \{l\}$ . It contains no elements less than $l$ , as otherwise $l$ is not the min element of $B_r(x)$ ; it has no elements larger than $l$ , as otherwise they would be in $B_{r_2}(x)$ . Thus, for any open ball $B$ with min element $l$ , there exists $\epsilon > 0$ such that $B_\epsilon(l) = \{l\}$ . Let $G$ be a set of all minimum elements of open balls. There is a clear injection from $G$ to $\mathbb{Q}$ by selecting any rational number in a small enough ball (using the regular metric) centered around elements of $G$ . Now notice that every open bounded interval is an open ball, and for any $x \in \mathbb{R}$ , there exists some open interval $(x, b)$ where $b > x$ . The smallest element in $(x, b)$ is the next element above $x$ . Thus, for every $x \in \mathbb{R}$ , the preceeding element of $x$ is in $G$ (and is unique with respect to $x$ ). This implies that since $\mathbb{R}$ is uncountable, so is $G$ . This is our contradiction, and so a corresponding metric $d$ can not exist. QED .","['well-orders', 'order-theory', 'set-theory', 'real-analysis']"
4338892,I got stuck on a system of three equations,"I have this homework problem about functions. The goal is to find a function of form $$ f(x)=a+bc^{x}, c > 0  $$ $$ and $$ $$ f(0) = 15, f(2)=30, f(4)=90 $$ and then to find the domain of a another function called g(x) where $$ g(x) = ln(x), x = f(x) $$ . For the first part of the problem, to find a function of form, I created a system of thee equations $$ \begin{cases} a+bc^0=15\\ a+bc^2=30\\ a+bc^4=90 \end{cases} $$ And this is where I got stuck. I don't know how to solve this system. Any help will be appreciated.",['functions']
4338931,Probability for a random variable to be greater than its mean,"I'm looking for a general lower bound on $\mathbb{P}(X \geq \mathbb{E}(X))$ (or, equivalently, on $\mathbb{P}(X \leq \mathbb{E}(X))$ ). My informal (and naive) reasonning so far I can think of 2 ways to make this probability arbitrarily small: Stacking all the probability mass on a single point -- consider, e.g., $X_\epsilon = \begin{cases} 0 &\text{ w.p. }& 1-\epsilon, \\ 1 &\text{ w.p. }& \epsilon. \end{cases}$ Taking a small amount of probability mass to infinity -- consider, e.g., for a given random variable $Y$ , the random variable $\tilde{Y}_\epsilon$ obtained from $Y$ according to $\tilde{Y}_\epsilon = \begin{cases} Y &\text{ w.p. }& 1-\epsilon, \\ Y + 1/ \epsilon^2 &\text{ w.p. }& \epsilon. \end{cases}$ These examples suggest that the desired lower bound should depend positively on the variance of $X$ (example 1.) and negatively on the ``diameter'' of $X$ (example 2.), that is, $$ \text{diam}(X) = \text{sup}~ ( \text{supp}(X)) - \text{inf}~ ( \text{supp}(X)).$$ (To put it simply, assume that $X \in [a,b]$ almost surely and make the lower bound a function of $b-a$ .) Are you aware of such result? If not, do you think that my hopes are justified, or did I miss a counter-example? Edit: A natural candidate would be $$ \mathbb{P}(X \geq \mathbb{E}(X)) \geq \frac{\text{Var}(X)}{(b-a)^2}. $$ Do you have any specific counter-example for this inequality?","['expected-value', 'probability-theory', 'probability', 'upper-lower-bounds']"
4338933,"Are there such things as 3-dimensional (and higher) analogues of matrices, and if so, do they have any applications?","A matrix is a group of numbers arranged in a rectangle. I wonder, has anyone studied 3-dimensional and higher analogues of matrices? For example, there could be such a thing as a 2 by 2 by 2 3d matrix, whose entries are all equal to 1. Has anyone else defined these entities, and more importantly, are they used in mathematics?",['matrices']
4338939,A continuous $\overline{\mathbb{F}^W}$ - adapted process X is $\sigma(W)$ - measureable.,"Let $(\Omega,\mathcal{F},\mathbb{F}=(\mathcal{F}_t)_{t \geq 0},\mathbb{P})$ be a filtered probability space satisfying the usual conditions, $W$ an $\mathbb{F}$ -Brownian motion and $X$ a continuous $\overline{\mathbb{F}^W}$ - adapted process with $\overline{\mathbb{F}^W}$ being the completed filtration generated by $W$ . We equip $C([0,\infty),\mathbb{R})$ with the sigma algebra generated by the projections. In the script my professor claims that $X:\Omega \rightarrow C([0,\infty),\mathbb{R})$ is $\sigma(W)$ - measureable, but I don't see why this should be true since $X$ is only $\overline{\mathbb{F}^W}$ - and not $\mathbb{F}^W$ - adapted. Remark: This is used for showing that a strong solution to an SDE is a function of $W$ . I would be grateful for any help.","['stochastic-pde', 'stochastic-analysis', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory']"
4338941,The trace as an integral over the projective space,"Let $(E,h)$ be a Hermitian vector space of dimension $n$ and $u\in End(E)$ . We have an expression of the trace of $u$ as the integral $$Tr(u)=\frac{n}{A}\int_{S}\langle v,uv\rangle d\mu$$ where $S$ is the unit sphere in $E$ and $A$ its volume (see Integral around unit sphere of inner product ). Is there a similar expression but using the projective space instead of the sphere? That is some equality that looks like $$Tr(u)=C\int_{\mathbb{P}E}\frac{\langle v,uv\rangle}{||v||^2} d\mu_{FS}$$ for some constant $C$ and $d\mu_{FS}$ being the Fubini-Study volume form.","['trace', 'linear-algebra', 'projective-space', 'differential-geometry']"
4338950,Prove that $\exists c > 0$ such that $||f||_2 \le c ||T(f)||_{\infty}$,"Problem (Unsolvable, since the statement is false.) Let $X = (C^2([0,1]),||\cdot||_2)$ , where $||f||_2 = ||f''||_{\infty} + ||f'||_{\infty} + ||f||_{\infty}$ . Define $T:X\to C([0,1])$ by: $\ \ \ \ T(f) = f'' + af' + bf$ where $a,b \in C([0,1])$ such that $T$ is surjective. Prove that $\exists c > 0$ such that $||f||_2 \le c ||T(f)||_{\infty}$ What I've gathered I know that $X$ is complete and that $T$ is linear. $T$ is bounded as well, since for $||f||_2 = 1$ : $||T(f)||_\infty = ||f'' + af' + bf||_\infty \le ||f''||_\infty + A ||f'||_\infty + B ||f||_\infty \le (1+A + B)$ . where $A = ||a||_\infty, B=||b||_\infty$ . So far, I know that $T$ is a bounded linear map, so I know that: $\forall f \in X: ||T(f)||_\infty \le ||A||\cdot||f_2||$ So what I need to prove is that $\exists c > 0$ such that $\forall f \in X:$ $\frac{||T(f)||_\infty}{||A||} \le ||f||_2 \le c ||T(f)||_\infty$ i.e. that $||\cdot||_2$ and $||T(\cdot)||_\infty$ are equivalent norms, right? I have no idea how to proceed from here.. Edit Since I got a counterexample showing that above is not true in general, I show below the original question (taken from an exam), which I shortened, but maybe changed the question in doing so: Original Problem Consider the Banach space $C^2([0,1])$ with $||\cdot||_2$ as defined above. Let $a,b \in C([0,1])$ and assume that for every $g\in C([0,1])$ , the differential equation $f''+ a f' +b f = g$ has a solution $f\in C^2([0,1])$ . Prove that there exists $c > 0$ , such that for every $g \in C([0,1])$ , the above equation has a solution $f\in C^2([0,1])$ , with $||f||_2 \le c ||g||$ Problem $\neq$ Original Problem I realize now that the property $ \forall f: ||f||_2 \le ||T(f)||_\infty $ is not explicitly required. My bad.","['continuity', 'normed-spaces', 'functional-analysis']"
4339044,Formal definition of $n$ by $0$ and $0$ by $n$ matrices,"A matrix is usually informally defined as a rectangular array of numbers. To make this definition formal, we can define a matrix as a map from $\{1,...,m\} \times \{1,...,n\}$ to the underlying field of scalars, where $\times$ denotes cartesian product. However, a subtle complication arises when $m=0$ or $n=0$ . In that case, the matrix would be an empty function. The problem, however, is that there is then no way to distinguish between $m \times 0$ matrices from $0 \times n$ matrices. In fact, under the cartesian product definition, for all natural numbers $m$ , $m'$ , $n$ , and $n'$ , the $m \times 0$ , $m' \times 0$ , $0 \times n$ , and $0 \times n'$ matrices are all the same entity, namely the empty function. This is, to me, an undesirable state of affairs. I want to be able to distinguish, for example, $2 \times 0$ , $3 \times 0$ , $0 \times 2$ , and $0 \times 3$ matrices. Is there a better definition of matrix that some mathematician has written about in some paper or book that avoids that problem?","['matrices', 'definition']"
4339068,Calculate Indefinite Integral $\int \frac{x^5(1-x^6)}{(1+x)^{18}}dx$,"The following integration is given by Wolfram Alpha $$\int \frac{x^5(1-x^6)}{(1+x)^{18}}dx=\frac{x^6(28x^4+16x^3+39x^2+16x+28)}{168(1+x)^{16}}.$$ My question is: what is the best (meaning least work), method to achieve this result by hand ? There are two approaches I see, partial fractions, maybe setting $y=1+x$ , but this is still alot of work. Or perform successive integration by parts. Both still involve a lot of calculation. The compact form of the answer makes me hope there is a nice way to achieve it. Guessing the ultimate form and then calculating the derivative is, to me, a less desirable method.","['integration', 'calculus']"
4339097,"Prove the converse of ""The sum of two odd consecutive numbers is a multiple of 4""","The sum of two odd consecutive numbers is a multiple of 4. I've tried rewriting this as:
If $a$ and $b$ are two consecutive odd numbers, then $a+b=4p$ , where $p\in\mathbb{N}$ . I'm trying to prove the converse of the statement, which I think is: If $z$ is a number of the form $4p,\ p\in\mathbb{N}$ , then $z$ can be written as the sum of two odd consecutive numbers $a$ and $b$ . Proof. We write $4p$ as $2p-1+2p+1$ , and we get $z=(2p-1)+(2p+1)$ . If we denote $2p-1$ by $a$ and $2p+1$ by $b$ , then we can notice that $a<b$ and $b-a=2$ , so $a$ and $b$ are two odd consecutive numbers. Is the above proof complete? I've recently started studying a proof writing book, and I want to make sure that I don't assume what need to be proved or other statements that may or may not be true.","['algebra-precalculus', 'proof-writing']"
4339099,Study the injectivity and surjectivity of the function f,"Let $f:\mathbb{R}\to\mathbb{R}$ , where $
 f(x) =
\begin{cases}
2x+1,  & \text{if $x$ is rational} \\
\sqrt2 x+3, & \text{if $x$ is irrational}
\end{cases}
$ The injectivity: I worked out the injectivity of the function by finding a rational number $x_1$ and an irrational number $x_2$ such that $f(x_1)=f(x_2)$ , but $x_1!=x_2$ , which are $x_1=2, f(2)=2\cdot 2+1=5$ and $x_2=\sqrt{2}$ , $f(\sqrt{2})=\sqrt{2\cdot 2}+3=5,f(2)=f(\sqrt{2})$ and $2!=\sqrt{2}$ which means that it's not injective. The surjectivity: If $x$ is rational $\implies y=2x+1\implies x=\frac{y-1}{2}$ , and if $x$ is irrational $x=\frac{y-3}{\sqrt{2}}$ ; I don't really know how to show that the function is surjective, but I know that when $y=2x+1$ (the first equation), it will touch all the rational numbers, but I'm not sure about the second equation if there's an y it doesn't touch and if it doens't then it means that it's not surjective... I was wondering if there is a general way to study the injectivity and surjectivity of these types of functions. Thanks for the help.","['algebra-precalculus', 'functions']"
4339159,Asymptotic analysis with affine relations,I have very little experience with asymptotics so would appreciate some help. I have a function $g$ and I have an asymptotic to a rational function: $g\sim r$ . I guess I can just take the highest powers above and below and so I have $\displaystyle g\sim \frac{1}{N^4}$ but what I actually have is $\displaystyle g\sim \frac{(N-4)!}{N!}$ via: $$1<\dfrac{g(N)}{(N-4)!/N!}<\frac{N-2}{N-3}.$$ Now I have another function $f$ that is related to $g$ by: $$f(N)+(N-3)g(N)=\frac{1}{N(N-1)(N-2)}.$$ Can I figure out the asymptotics for for $f$ from this? Naively I get $f\sim 0$ but that seems a bit iffy. Actually very very iffy because naively putting in the two different asymptotes gives different answers. Any help appreciated.,"['limits', 'functions', 'asymptotics']"
4339167,Constructing non-equivalent atlases,"In a lecture on differential geometry, we had the following definition of equivalent atlases: Two atlases $\mathcal A$ and $\mathcal B$ on $M$ are called equivalent if $\mathcal A \cup \mathcal B$ is an atlas on $\mathcal M$ . The definition of atlas we had is the following: Let $M$ be a second countable Hausdorff topological space. An $n$ - dimensional smooth atlas on $M$ is a collection of maps $$\mathcal A = \left\{ \left(\varphi_i, U_i\right) \mid i\in A\right\}, \quad \varphi_i: U_i\rightarrow \varphi_i(U_i)\subset \mathbb R^n,$$ such that all $U_i \subset M$ are open, all $\varphi_i$ are homeomorphisms, and $\{U_i, i\in I\}$ is an open covering of $\mathcal M$ $\varphi_i\circ \varphi_j^{-1}: \varphi_j\left(U_i\cap U_j\right)\rightarrow \varphi_i\left( U_i\cap U_j\right)$ are smooth for all $i, j\in I$ . Now, this thread gives the following ""recipe"" for constructing non-equivalent atlases: Here is a very easy way to construct inequivalent atlases on the same differentiable manifold $X$ , e.g. $X=\mathbb{R}$ or $X=\mathbb{S}^1$ . Pick any homeomorphism $f : X \to X$ which is not a diffeomorphism (one always exists). For each chart in the given atlas $(U,\phi)$ , define a chart $(f^{-1}(U),\phi \circ f)$ in the new atlas. The overlap condition holds between charts in this new atlas because the $f$ 's cancel out. But an overlap between a chart in the new atlas and one in the old is not smooth, because the $f$ does not cancel out and it would follow that $f$ is smooth which it isn't. Question : Why exactly cannot $f$ be a diffeomorphism? Or to ask it differently: For example, let's assume that $f$ is ""only"" a $C^{1}$ diffeomorphism, wouldn't the recipe still hold, because a $C^{1}$ diffeomorphism is in general not smooth, i.e. $C^{∞}$ ? EDIT : This is the definition of smoothness that we had for a map $f$ between two smooth manifolds: Let $M$ and $N$ be two smooth manifolds. A continuous map $f:M\to N$ is called smooth if for all charts $(\varphi, U)$ of $M$ , $(\psi, V)$ of $N$ , $$\psi\circ f\circ \varphi^{-1}: \varphi(U\cap f^{-1}(V)) \to \psi(V)$$ is smooth. If I apply this to our case, I get: $$\phi_{\alpha}^{-1}\circ h\circ \phi_{\alpha}: \phi_{\alpha}^{-1}(\phi_{\alpha}^{-1}(U_{\alpha})\cap h^{-1}(\phi_{\alpha}^{-1}(U_{\alpha}))) \to \phi_{\alpha}^{-1}(\phi_{\alpha}^{-1}(U_{\alpha})).$$ We've tried to convince ourselves that $f$ is not differentiable at $\phi_{\alpha}^{-1}(0)$ , but is $\phi_{\alpha}^{-1}(0)$ an element of the domain of $\phi_{\alpha}^{-1}\circ h\circ \phi_{\alpha}$ ? I don't think so for the following reason: $$h^{-1}(\phi_{\alpha}^{-1}(U_{\alpha})) = h^{-1}(\{x\in U_{\alpha}\mid \phi_{\alpha}(x)\in U_{\alpha}\}) = \{y\in B_{1}(0) \mid h(y)\in\{x\in U_{\alpha}\mid \phi_{\alpha}(x)\in U_{\alpha}\}\}.$$ And here comes my problem : We know that $h: B_{1}(0)\to B_{1}(0)$ , so how can $h(y)$ be an element of the set $\{x\in U_{\alpha}\mid \dots\}$ , which is a subset of $U_{\alpha}$ ? After all, $B_{1}(0)$ and $U_{\alpha}$ are in no way related to each other.","['manifolds', 'differential-geometry']"
4339171,Find all functions $f:\mathbb N\to \mathbb N$ such that $\frac{4f(x)f(y-3)}{f(x)f(y-2)+f(y)f(x-2)}$ is an integer for all $x>2$ and $y>3$.,"The following problem is from an online contest that ended today: Find all functions $f:\mathbb N\to \mathbb N$ such that $$\frac{4f(x)f(y-3)}{f(x)f(y-2)+f(y)f(x-2)}$$ is an integer for all $x>2$ and $y>3$ . I have made the following progress: Taking $x=y\geq4$ , the constraint becomes $$\frac{2f(x-3)}{f(x-2)}$$ which we want to be an integer for all $x\geq4$ . Now one case is that $\frac{2f(x-3)}{f(x-2)}=c$ for all $x\geq4$ , where $c$ is a positive integer constant. We consider this case. Let $\frac{2f(n)}{f(n+1)}=c$ for all $n\geq1$ . We have $$f(n+1)=\frac{2f(n)}{c}\implies f(n)=\left(\frac 2 c\right)^{n-1}f(1)$$ If $c>2$ , then $f(n)$ can not always be integer. So we have $c=1$ or $c=2$ . If $c=2$ , then we have $f(n)=f(1)=k$ for all $n$ . Plugging it in the given expression, we have $$\frac{4k^2}{2k^2}=2$$ which is an integer. So $f(n)=k$ works. If $c=1$ , then we have $f(n)=2^{n-1}f(1)=2^{n-1}k$ . We can confirm that this function works by plugging in the expression. But there is other case when $\frac{2f(n)}{f(n+1)}$ is not a constant. I can't find a way to solve this case.","['contest-math', 'elementary-number-theory', 'functions', 'functional-equations']"
4339196,Prove that $\lim_{n\to\infty}\|f_n-f\|_p=\lim_{n\to\infty}\|g_n-g\|_p=0$,"I am studying a funtional analysis course and I found the following problem: Let $(f_n)$ and $(g_n)$ be two sequences such that $f_n\to f$ and $g_n\to g~\mu-a.e$ and assume that there exists some $p\in[1,\infty)$ such that: $$\|f_n-g_n\|_p=\|f_n+g_n\|_p=\|f-g\|_p=\|f+g\|_p=1$$ Show that $\lim_{n\to\infty}\|f_n-f\|_p=\lim_{n\to\infty}\|g_n-g\|_p=0$ . It looks pretty easy, however I cannot find the right path. I suppose I have to use Riesz lemma ( $f_n\to f~\mu-a.e$ and $\|f_n\|_p\to\|f\|_p$ implies $\|f_n-f\|_p\to 0 $ ) but I don't know how to prove that $\|f_n\|_p\to\|f\|_p$ . I tried to use triangular inequality, the parallelogram law and to use the definition of norm $p$ , but I can't find nothing useful. Many thanks.","['lp-spaces', 'normed-spaces', 'functional-analysis']"
4339198,Do $u\in H^1(\Omega)$ and $\Delta u\in L^2(\Omega)$ imply $u\in H^2(\Omega)$?,"Suppose $\Omega$ is a bounded open domain in $\mathbb R^n$ with regular boundary. If $u\in H^1(\Omega)$ and $-\Delta u\in L^2(\Omega)$ , do we have $u\in H^2(\Omega)$ for sure? Here we say $\Delta u\in L^2(\Omega)$ by meaning $u$ has weak derivatives $\partial_{x_i}^2 u$ for $i=1,\cdots,n$ and $$
\Delta u:=\sum_{i=1}^n \partial_{x_i}^2u\in L^2(\Omega).
$$ There is a related problem , and some comments refer to Theorem 8.12 in Gilbarg and Trudinger's book . I think their theorems are essentially proving the following result: Suppose $\Omega$ is a bounded open domain in $\mathbb R^n$ with regular boundary. If $u\in H_0^1(\Omega)$ and $-\Delta u\in L^2(\Omega)$ , then $u\in H^2(\Omega)$ . The proof can be accomplished using the Lax-Milgram theorem and the regularity of the weak solution. However, the original problem does not assume $u$ has zero trace, and this result cannot be directly applied. In Helffer's Spectral Theory and Its Applications , Equation (4.4.5) at Page 38 defines the space $$
W(\Omega) = \{u\in H^1(\Omega):-\Delta u\in L^2(\Omega)\}
$$ and claims that there is NO $W(\Omega)\subset H^2(\Omega)$ . It seems that the answer to the original problem is negative, but I am not sure where to find a counterexample. Any helpful comments or answers are appreciated.","['sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
4339234,"Prove that there exists $x,y \in \mathbb{Z}$ such that $x^2 - 11y^2 = p$ if and only if $p = 1,3,4,5,9 \bmod 11$ with $p \equiv 1 \bmod 4$ prime","I know that $\left( \frac{11}{p} \right) = 1$ for $p = 1,3,4,5,9 \bmod 11$ and $\mathbb{Z}[\sqrt {11}]$ is a UFD but i do not know how to proceed. One of the hint is to consider $t \in \mathbb{Z}[\sqrt{11}]$ such that $p \mid t^2 - 11$ and to factorize it in $\mathbb{Z}[\sqrt{11}]$ but i got stuck a long the way. Any suggestions ?","['number-theory', 'diophantine-equations']"
4339263,Riemannian metric making a submanifold totally geodesic and with positive injectivity radius,"Consider a manifold $N$ and a submanifold $M$ . We're assuming that $N$ and $M$ can be non-compact. Is it possible to find a metric $g$ of $N$ such that $M$ is totally geodesic with respect to $(N,g)$ and such that the metric $g$ has positive injectivity radius ? This question came to me because I am trying to parametrize a infinite dimensional banach manifold of curves, hence I need the positive injectivity radius, and I also need that these maps satisfy specific boundary conditions, hence the totally geodesic condition.
I have been thinking about this but I am have no idea on how to proceed. Edit: This problem came to mind because I wanted to see if  I could find a metric in $T^*M$ such that it has positive injectivity radius and with totally geodesic fibers. An idea would be to consider a metric in the conformal class of the Sasaki metric $g_S$ . We know that the Sasaki metric has totally geodesic fibers. Then if we could consider something like $e^{2f}g_{S}$ such that it would have positive injectivity radius and $\text{grad}(f)$ is tangent to the fibers I belive we could consider this metric to get the desired result. However I have not yet been able to find this smooth function $f$ . Any help is appreciated, thanks in advance.","['differential-topology', 'riemannian-geometry', 'differential-geometry']"
4339265,Can an induced subgraph be two vertices with no edges in between?,"I was looking through the definition of an induced subgraph, and it states that if $G'$ is an induced subgraph of $G$ , then $V(G')\subseteq V(G)$ and $E(G')=E(G)\cap$$V(G')\choose2$ . My question is, if you have a big enough graph $G$ , and choose two completely separate vertices such that there is no edge between them, then is the resulting graph $G'$ an induced subgraph (where $G'$ is just two separate nodes)? It contains no edges, so it is just the two nodes, but I saw nothing of this mentioned in the book I'm reading so I had the doubt. Similarly, if you chose one single vertex of $G$ , is it also an induced subgraph?","['graph-theory', 'discrete-mathematics']"
4339288,"Determine whether or not the function f it is bijective 2f(3-2x)+f(3/2-x/2)=x, where x is a real number","So the problem says: Study the bijectivity of the functions f:R->R, which follow the property: a) $2f(x)+f(1-x)=x-8$ ; b) $2f(3-2x)+f(\frac{3-x}{2})=x$ , where x is a real number for both. The first one, a), I solved it by substituing $y=1-x=>x=1-y$ , gets us the following ecuation: $2f(1-x)+f(x)=-x-7=>f(1-x)=\frac{-x-7-f(x)}{2}$ , and if we substitute this in the first ecuation we get: $2f(x)+\frac{-x-7-f(x)}{2}=x-8=>4f(x)-f(x)-x-7=2x-16<=>3f(x)=3x-9=>f(x)=x-3$ And this function is indeed bijective. Now, for the second one, b), I don't know what to substitute x with to find the function f. I tried a lot of systems of ecuation with no luck, like $y=\frac{3-x}{2}=>x=3-2y$ , which will get us the echivalent functional ecuation: $2f(4y-3)+f(y)=3-2y$ , and $y=3-2x=>x=\frac{3-y}{2}$ which will get us to $2f(y)+f(\frac{3-y}{4})=\frac{3-y}{2}$ . I don't know what connection to find between these ecuations...","['functional-equations', 'functions']"
4339295,What is the Variance of $(1-X)/X$ and the Expectation of $(1-X)/X^{2}$?,"Knowing that $X = \begin{cases} 
0.05 \ \ \text{with probability} \ \ 0.7\\
0.2\ \ \text{with probability} \ \ 0.3\end{cases}$ What would be the Variance of $\frac{1-X}{X}$ and the Expectation of $\frac{1-X}{X^2}$ ? I have computed it and would like to know if my development is correct : \begin{aligned}
Var\left[\frac{1-X}{X}\right] &= E\left[\left(\frac{1-X}{X}\right)^2\right] - \left(E\left[\frac{1-X}{X}\right]\right)^2 \\
&= \left(\frac{0.95}{0.05}\right)^2\cdot 0.7 + \left(\frac{0.8}{0.2}\right)^2\cdot 0.3 - \left(\frac{0.95}{0.05}\cdot 0.7 + \frac{0.8}{0.2}\cdot 0.3\right)^2 = 47.25
\end{aligned} and $$E\left[\left(\frac{1-X}{X^2}\right)\right] = \left(\frac{0.95}{0.05^2}\right)\cdot 0.7 + \left(\frac{0.8}{0.2^2}\right)\cdot 0.3 = 272 $$","['expected-value', 'variance', 'probability-theory', 'probability']"
4339299,Is there a Lagrangian that produces these equations? How can I find one if it exists?,"Consider the two differential equations \begin{align*}
\ddot{x}_{A} - \gamma(x_{A} + x_{B}) &= 0, \\
\ddot{x}_{B} + \gamma(x_{A} + x_{B}) &= 0
\end{align*} where $\gamma$ is a constant.
I am looking either for a Lagrangian $L = L(x, \dot{x}, t)$ that can reproduce these equations or a proof that shows this is impossible. Unfortunately, my initial question was framed too narrowly, so I will leave two questions (they're not exactly the same as you will see in the answers). The first one was my original one. The second one is my new one. Question 1. Is there a function $L = L(x, \dot{x}, t)$ such that \begin{align*}
\frac{d}{dt}\frac{\partial L}{\partial \dot{x}_{A}} - \frac{\partial L}{\partial x_{A}} = \ddot{x}_{A} - \gamma(x_{A} + x_{B}), \\
\frac{d}{dt}\frac{\partial L}{\partial \dot{x}_{B}} - \frac{\partial L}{\partial x_{B}} = \ddot{x}_{B} + \gamma(x_{A} + x_{B})?
\end{align*} Question 2. Is there a function $L = L(x, \dot{x}, t)$ such that when setting the Euler-Lagrange expressions to zero we obtain differential equations that yield the same solutions as my original differential equations? After looking around, I found the inverse problem for Lagrangians giving the Helmholtz conditions. Let us define $f^{A}(x^{A}, x^{B}) = \gamma\cdot (x^{A} + x^{B})$ and $f^{B}(x^{A}, x^{B}) = -\gamma\cdot (x^{A} + x^{B})$ as in the wiki page, and let us define $$\Phi_{j}^{i} = \frac{1}{2} \frac{\mathrm{d}}{\mathrm{d} t} \frac{\partial f^{i}}{\partial \dot{x}^{j}} - \frac{\partial f^{i}}{\partial x^{j}} - \frac{1}{4} \frac{\partial f^{i}}{\partial \dot{x}^{k}} \frac{\partial f^{k}}{\partial \dot{x}^{j}}.$$ Here we easily obtain $$ (\Phi_{j}^{i}) = \begin{pmatrix} -\gamma & -\gamma \\ \gamma & \gamma \end{pmatrix}. $$ Now the wiki page says that we need to find a non-singular symmetric matrix $g = (g_{ij})$ such that conditions $(\text{H}1)$ - $(\text{H}3)$ in the wiki page are satisfied (see image at the bottom for conditions).
I found that $$ g = \begin{pmatrix} 1 & \frac{3}{2} \\ \frac{3}{2} & 2 \end{pmatrix} $$ works, so I concluded that there should be a Lagrangian for my differential equations. However, I have not been able to find such any desired function that works. Questions: Was my above reasoning in determining that a Lagrangian exists correct? Did I make a correct application of Douglas's theorem? Can anyone help me find a Lagrangian? I tried various functions, yet they all failed to produce the differential equations. If no one can give me an explicit answer, then can anyone tell me what steps I should take to construct a Lagrangian? Some references: The wikipedia page is here . This is the link to Douglas's paper on the inverse problem for Lagrangians. This post asks about the inverse problem, and the answers provide some references. The thesis referenced in the MSE post provides an interesting so-called explicit condition on page 67, but it is only stated for the 1D one particle case. NOTE: It seems their condition only works for problems similar to Question 1. In Example IV.1 they conclude that a damped oscillator doesn't have a Lagrangian for the general case. This is true in the context of Question 1, but false in the context of Question 2 as this post demonstrates . If you multiply the differential equation by an exponential, then a Lagrangian can reproduce the differential equation. The MSE post referenced Olver's Applications of Lie groups to differential equations . Relevant wikipedia image as of 12/6/2022 in case it gets edited:","['inverse-problems', 'ordinary-differential-equations', 'calculus-of-variations']"
4339317,Area of minimal submanifold in $S^3$,"I am asked to prove that lower bound of area of compact minimal submanifold with no boundary in $S^3$ is $4\pi$ . Only idea i have is Gauss equation using orthonormal frame $e_1,e_2$ . We have $$1=K+|B_{12}|^2-\langle B_{11},B_{22}\rangle,$$ where $K$ is the Gauss curvature of $M$ and $B_{ij}=B(e_i,e_j)$ is the second fundamental form. Since $M$ is minimal, $B_{11}=-B_{22}$ . We get: $1=K+||B||^2/2$ . Thus by Gauss-Bonnet, $$\mathrm{Vol}(M)=2\pi \chi(M) + \frac{1}{2}\int_{M}^{} \|B\|^2 \,d\mathrm{Vol}.$$ But I can not estimate the second fundamental form.","['minimal-surfaces', 'riemannian-geometry', 'differential-geometry']"
4339331,Derivative of trace of tensor,"Given a rank-2 tensor $Q$ , how would one work out $\frac{d Tr(Q^2)}{dQ}$ ? If the tensor $Q$ was traceless and symmetric (ie $Tr(Q)=0$ and $Q=Q^T$ ) , would this change things? Thanks","['trace', 'tensors', 'derivatives']"
4339382,Lang's proof that there exists $x>0$ such that $\cos x=0$,"In Undergraduate Analysis on p. 90, Lang assumes the existence of two functions $f$ (sine) and $g$ (cosine) satisfying the conditions $f(0)=0$ , $g(0)=1$ , $f'=g$ and $g'=-f$ . He then goes on to show that there exists $x>0$ such that $\cos x=0$ . With your help, I would like to make some of the steps in his proof more explicit. Suppose that no such number exists. Since $\cos$ is continuous, we
conclude that $\cos x$ cannot be negative for any value $x>0$ (by
intermediate value theorem). Hence $\sin$ is strictly increasing for
all $x>0$ , and $\cos x$ is strictly decreasing for all $x>0$ ... It's clear why $\sin$ is strictly increasing on the interval $(0,\infty)$ . Why is $\cos$ strictly decreasing on the interval $(0,\infty)$ ? This would require $\sin x>0$ for all $x\in(0,\infty)$ . But how can I show this? ... Let $a>0$ . Then $0<\cos 2a=\cos^2a-\sin^2a<\cos^2a$ . By induction,
we see that $\cos(2^n a)<(\cos a)^{2^n}$ for all positive integers $n$ . Hence $\cos(2^na)$ approaches $0$ as $n$ becomes large, because $0<\cos a<1$ . Since $\cos$ is strictly decreasing for $x>0$ , it
follows that $\cos x$ approaches $0$ as $x$ becomes large, and hence $\sin x$ approaches $1$ . In particular, there exists a number $b>0$ such that $$\cos b<\frac{1}{4}\text{ and }\sin b>\frac{1}{2}.$$ If $\lim_{n\rightarrow\infty}\cos(2^na)=0$ for all $a>0$ , how can I conclude that $\lim_{x\rightarrow\infty}\cos x=0$ ? Let $\epsilon>0$ . Then I would have to show that there exists $s\in\mathbb{R}$ such that $$(\forall x)(x\in(s,\infty)\implies|\cos x|<\epsilon).$$ It's not immediately clear how to find such an $s$ . I would have to use the fact that $\lim_{n\rightarrow\infty}\cos(2^na)=0$ for all $a>0$ , but I don't know how.","['proof-explanation', 'real-analysis', 'continuity', 'trigonometry', 'derivatives']"
4339397,"Proving that $L^1(X,M,\mu)$ is not reflexive","Let $(X,M,\mu)$ be a measure space s.t there exists an infinite sequence of disjoints measurable sets of strictly positive finite measure. Show that $L^1(X,M,\mu)$ is not reflexive. My attempt: this question followed a question that asked to prove that a TVS $V$ is reflexive $\iff$ the closed unit ball in $V$ is weakly compact. I tried showing that $\{f\in L^1:\|f\|_1\leq1\}$ is not weakly compact, but I wasn't able to find an open covering by weak open sets s.t it has no finite subcovering. Any help would be appreciated.","['measure-theory', 'topological-vector-spaces', 'real-analysis', 'lp-spaces', 'weak-topology']"
4339450,"Example of 2 random variables s.t. $(X+Y)$ ~ $U(0,2)$","In my book I found: Can you give an example of 2 random variables $X,Y$ S.T $(X+Y)$ ~ $U(0,2)$ and $X,Y$ are not independent. Any ideas of how I can find such 2 random variabes? I would prefer if those random variables tell a story so I can relate to the real world, for example selecting number in $[0,1]$ has uniform disturbution of $(0,1)$ .","['uniform-distribution', 'independence', 'probability-distributions', 'probability', 'random-variables']"
4339457,What do the fibers of the double tangent bundle look like?,"Consider the tangent bundle $\pi:TM\to M$ for some smooth manifold.
As outlined in the Wikipedia page , we can then consider the double tangent bundle via the projection $\pi_*:TTM \to TM$ , with $\pi_*$ the pushforward of the canonical projection $\pi$ . In the above page, they then proceed to mention that, given $$\xi =\xi^k \frac{\partial}{\partial x^k}\Bigg|_x\in T_x M,
\qquad
X =X^k \frac{\partial}{\partial x^k}\Bigg|_x \in T_x M,\tag A$$ and ""applying the associated coordinate system"" $\xi\mapsto (x^1,...,x^n,\xi^1,...,\xi^n)$ on $TM$ , the fiber on $TTM$ at $X\in T_x M$ takes the form $$(\pi_*)^{-1}(X)=\left\{
X^k\frac{\partial}{\partial x^k}\Bigg|_\xi +
Y^k\frac{\partial}{\partial \xi^k}\Bigg|_\xi :
\,\, \xi\in T_x M,\,\, Y^1,...,Y^n\in\mathbb R
\right\}.\tag B$$ I'm struggling to understand where this expression comes from. I think I understand that $(x^1,...,x^n,\xi^1,...,\xi^n)$ is a (local) parametrisation for $TM$ , and I can see that the fiber we are interested in is $T_X TM$ , that is, the set of elements of $TTM$ above $X$ , but I don't understand what the expression in the last equation represents. If I were to write a fiber $T_x M$ of the tangent bundle, this would be the set of pairs $(x,v)$ with $v$ ranging across all (equivalence classes of) smooth curves $I\to M$ passing through $x$ .
In local coordinates, and focusing on the ""curve component"" of tangent vectors, I suppose we could write this as the set $$\pi^{-1}(x)=\left\{v^k \frac{\partial}{\partial x^k}\Bigg|_x
: \,\, v^k\in\mathbb R\right\}.$$ By way of analogy, I'd guess $T_X TM$ to be the set of pairs $(X,V)$ with $V$ (equivalence classes of) curves $I\to TM$ passing through $X\in T_x M$ .
But even switching to local coordinates, I'm not sure how to go from this description to the expression in (B).","['vector-fields', 'vector-bundles', 'tangent-bundle', 'differential-geometry']"
4339514,Why doesn’t an X percent increase in speed equal and X percent decrease in travel time?,"If I am traveling 60 miles per hour and I increase my speed 10% to 66 miles per hour, why has my travel time only been reduced 9.09% and not 10%? Thanks","['soft-question', 'analysis']"
4339532,The number of cliques of size $4$ where all the edges are of the same colour is at most $\frac{\binom{n}{4}}{3^5}.$,"Let $K_n$ be the complete graph on $n$ vertices. Prove that for all integers $n \ge 4$ , we can assign each edge one of $3$ colours such that the number of cliques of size $4$ where all the edges are of the same colour is at most $$\frac{\binom{n}{4}}{3^5}.$$ Can someone give some hints? I am not able to see the problem. There are $\binom{n}{2}$ edges in $K_n$ .","['graph-theory', 'coloring', 'combinatorics', 'discrete-mathematics']"
4339554,Prove that $P(A \cap B^*) = P(A) - P(A \cap B )$,"Just started studying elementary statistics and I am trying to figure out why $P(A \cap B^*) = P(A) - P(A \cap B )$ (not a problem from a book) but I get stuck: I use the following two 2 equations: $$ \begin{align*} P(A^*) &= 1- P(A) & (1)\\ P(A \cup B ) &= P(A) + P(B) - P(A \cap B) & (2)\end{align*}$$ First I write LHS of the problem using the RHS of the second equation $$ \begin{align*}  P(A \cap B^*) &= P(A) + P(B^*) - P(A \cup B^* ) \\ &= P(A) + 1 - P(B) - P(A \cup B^* )   
\end{align*}$$ This is where I get stuck. Can you help me?","['elementary-set-theory', 'statistics', 'probability']"
4339591,Trigonometric Function variables,"I'm currently reading Precalculus with limits and got into chapter 4 of Trigonometry. I now understood that, an angle $u$ is a real number that correspond to the points $(x,y)$ in the unit circle. So we have now two functions which is $x=\cos u$ and $y=\sin u$ according to the definitions of the right triangle (trigonometric ratios) and unit circle. My dilemma is, since we have now defined cosine as a function of $x$ , so we could call it a function $x=g(u)=\cos u$ , when I make a graph, I choose $x$ as vertical axis and $u$ as horizontal axis, because cosine move from right to left in a unit circle. But in the book they choose cosine as $y=\cos x$ . But $y$ is already taken as a definition of sine function, $y=f(u)=\sin u$ . So how did they interchanged the variables here? This matter is very confusing to me.","['algebra-precalculus', 'functions', 'change-of-variable', 'trigonometry']"
4339626,What if an automorphism fixes every maximal subgroup pointwise. Is it then the identity? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question This question came up in the discussion over here My first thought was that then it fixes the Frattini subgroup.   Any help? For reference we found that the answer is no when each maximal subgroup is merely mapped back to itself.","['automorphism-group', 'group-theory', 'maximal-subgroup', 'frattini-subgroup']"
4339690,Prove that set of limits bound is open.,"Consider the following problem: $\dot{x} = A(t) x$ . Let $X_{A}(t,s)$ be the Cauchy operator of the following system. Let $M$ be the space of continuous functional matrices. Let's denote by: $$
\rho(A, B) = \sup_{t \in \mathbb{R}_{+}} \|A(t) - B(t)\|, \text{ where } A,B \in M 
$$ Let's consider the upper-limit function $\phi(A) = \underset{{t-s \to +\infty}}{\overline{\lim}} \dfrac{1}{t - s}\log\|X_A(t,s)\|$ . We want to show that $\forall a \in \mathbb{R}_{+}$ the set: $\{A\in M: \phi(A) < a\}$ (i.e. Lyapunov's exponent is bounded with $a$ ) is open w.r.t. the metrics defined above. I'was trying to use the simple definition of the openness in metric space (assume $a$ is fixed): $$
B_{\epsilon} (A) \subset A, \text{ for } A \in M
$$ Which is equal to: $\exists B : \phi(A), \phi(B) < r$ and $\rho(A,B) < \epsilon$ . We can try to denote: $\{\phi(A) <a \} \sim \{A: \|X_A\| \sim e^{ r(t-s) + o(t-s)}\}$ , but here I guess we need to use some more fundamental fact about $X_A$ (maybe continuous dependency on $A$ ). UPD. We can consider the following problem as following. There is exists $Q(t) \in M$ : $B(t) = A(t) + Q(t)$ , $\rho(A,B) = \sup_t \|Q(t)\| < \epsilon$ and $\phi(A), \phi(B) < a$ . I thought about diagonal operator with small enough values on the diagonal (since I guess we can only consider the diagonal matrices). Any hints?","['cauchy-problem', 'functional-analysis', 'ordinary-differential-equations']"
4339706,Why does the sign in Newton's method matter?,"Deriving Newtons Method visually as with the help of a right triangle and assuming $x_1$ lies the left of $x_0$ we get $$x_1 = x_0 - \frac{f(x_0)}{f'(x_0)}$$ Using slope over run. but if we assume $x_1$ lies to the right of $x_0$ we get: $$x_1 = x_0 + \frac{f(x_0)}{f'(x_0)}$$ So I thought it doesn't matter, but solving some problems with $$x_1 = x_0 + \frac{f(x_0)}{f'(x_0)}$$ I diverge. Could someone enlighten me where the fault in my thinking lies? Thank you!","['newton-raphson', 'geometry', 'applications', 'calculus', 'derivatives']"
4339718,Find a finite morphism $F:X\to\mathbb{P}^k$.,"I am working in the following problem from my algebraic geometry course: Let $X$ be a projective set of $\mathbb{P}^n$ . Prove that there exists a finite regular morphism $F:X\to\mathbb{P}^k$ , where $k=\dim X$ . (In case of doubt finite means that $k[X]$ is  integral over $F^*(k[\mathbb{P}^k])$ and $F(X)$ is dense in $\mathbb{P}^n$ , which is equivalent with $F^*$ injective) Any help or hint will be appreciated, thanks :) Edit: Here is my attempt: If $X=\mathbb{P}^n$ we are done, since we can take $F$ to be the identity. If $X\neq\mathbb{P}^n$ there exist $x\in\mathbb{P}^n\setminus X$ , so we can consider the map $f_1$ as the maps that projects $X$ from $x$ . If $f_1(X)=\mathbb{P}^{n-1}$ we can take $F=f_1$ . If $f_1(X)\neq\mathbb{P}^{n-1}$ there exist $y\in\mathbb{P}^{n-1}\setminus f_1(X)$ , so we can consider the map $f_2$ as the maps that projects $f_1(X)$ from $y$ .  If $f_2(f_1(X))=\mathbb{P}^{n-2}$ we can take $F=f_2\circ f_1$ ,.... Since this process will end when we arrive $\mathbb{P}^k$ we can say that $F=f_{n-k}\circ\dots\circ f_{1}$ . Of course this $F$ is well defined and it must be the solution to the problem, but I don't know how to prove that it is finite. If I can show that $f_i$ (a projection from a point) is a finite regular morphism we will be done, but I don't have ideas for this. Edit: I know @KReiser said it could be a duplicate, but there the answer just work for characteristic $0$ and it gives us a 'geometric solution' that i don't like so much. I would like to have a pure algebraic solution and, if possible, an explicit formula for $F$ or a constructive method.","['morphism', 'algebraic-geometry']"
4339719,"For local ring $R$, does funcotor $\operatorname{Hom( Spec}R, X)$ characterize scheme $X$?","Let $\bf{Sch, Sets, Ring}$ be a category of schemes, sets, commutative rings. By Yoneda's lemma, scheme $X$ is characterized by contravariant functor $$\operatorname{Hom}(*, X): \bf{Sch}^{op}\to Sets$$ Now thinking  glueing of schemes by affine schemes, $X$ can be characterized by covariant functor $$\operatorname{Hom}({\operatorname{ Spec} }(*), X): \bf{Ring}\to Sets$$ I want to know that whether $X$ is characterized by only local rings? i.e. for schemes $X, Y$ , if for all local ring $R$ , $\operatorname{Hom}({\operatorname{ Spec} }(R), X)\cong \operatorname{Hom}({\operatorname{ Spec} }(R), Y)$ , then $X\cong Y$ ?","['local-rings', 'algebraic-geometry', 'ring-theory', 'abstract-algebra']"
4339827,Comparison of saturated ideals and radical ideals,"Given a graded ring $B = A[x_0,\dots,x_n]$ , $I$ a homogeneous ideal of $B$ not containing $B_+$ . Then what are the relations between the racial ideal of $I$ and saturation of $I$ ? As far as I know, there are the following results, indicating possible deeper relations between them: Saturated ideals are not necessarily radical. Radical ideals are saturated. There is a bijection between the closed subschemes of $\operatorname{Proj}(B)$ and the saturated homogeneous ideal of $B$ not containing $B_+$ . (Projective Nullstellensatz, from wiki):
There is a bijection between homogeneous radical ideals not containing $B_+$ and subsets of $\mathbb{P}^n$ of the form $V(I):= \{x\in \mathbb{P}^n \mid f(x)=0 \text{ for all } f \in I\}.$ Is it true that saturated ideal indicate a scheme structure while radical ideal only indicates its topological property? Thanks in advance! (Typos corrected based on the answer of KReiser.)",['algebraic-geometry']
4339828,Asymptotic solution to $y'=\sin(xy)$,"I need to prove that the following equation $y'=\sin (xy)\tag{1}$ Has a solution $y\not\equiv 0$ such that $\lim\limits_{x\to+\infty}y=0$ . I was able to conclude that any solution of this equation (except for $y\equiv 0$ ) cannot cross the line $y=0$ , because by the theorem of existence and uniqueness for any $x_0\in\mathbb R$ there may only exist one solution satisfying starting condition $y(x_0)=0$ , which is $y\equiv 0$ . This means that, for example, if a solution has a point with a value greater than zero, then the entire solution is greater than zero and it is bounded from below. However, I am not sure where to go next. Even if I could prove that a solution is monotonically decreasing starting at some point, being bounded by zero doesn't guarantee that the limit equals zero. Any help would be appreciated","['asymptotics', 'ordinary-differential-equations']"
4339836,Length of shortest walk always equal to length of shortest path in an undirected graph?,"My discrete mathematics course text defines the distance between two nodes $u$ and $v$ in an undirected graph $\mathcal{G}$ as the length of the shortest walk between $u$ and $v$ . A path was defined as a walk in which all edges are different. When looking up other definitions I found that they all appear to define distance in terms of the shortest path between two nodes. My question , as I cannot think of any counterexample where the length of the shortest walk is different from that of the shortest path: Why might distance be defined here in terms of the shortest walk instead of the shortest path?","['graph-theory', 'discrete-mathematics']"
4339844,Can we compute all digits of the Euler-Mascheroni constant?,"Let $\gamma$ be the Euler-Mascheroni constant from calculus. Is there an algorithm that computes the $n$ -th digit of the decimal expansion of $\gamma$ given $n$ as input? For all we know $\gamma$ could be a rational number, it could even be something like $k/10^n$ and have a terminating decimal expansion. Do we run into difficulties in this case when trying to compute the $n$ -th or later digits? Clarification : As another example assume we have a computable strictly increasing lower bound series $a_n$ and a computable strictly decreasing upperbound series $b_n$ both converging to an unkown real number $x$ : $a_n < x < b_n$ and $b_n-a_n\rightarrow0$ . Now assume that $x$ is in fact $0.5$ but we do not know this. All we see is that no matter how (finitely) many $a_n$ and $b_n$ we compute that always $a_n < 0.5 < b_n$ . How can we then ever know the first digit of the decimal expansion of $x$ ? Ist could be $4$ or $5$ . My questions is: Could we run into a similar problem with $\gamma$ ? Is there a known computable function $f(n)$ that provably computes the $n$ -th digit of the decimal expansion of $\gamma$ for every $n$ ? Theorertically of course there is such an algorithm in any case. But can we write it down not knowing wether $\gamma$ is irrational?","['computational-mathematics', 'calculus', 'computability']"
4339875,Maximum likelihood estimator and asymptotic distribution,"Let $X_1,\dots,X_n$ be a random sample from X whose density is given
by $$f(x,\theta) = c(\theta)(1-\exp(-|x|))I\{|x|\leq\theta\}$$ Find the maximun likelihood estimator of $\theta$ and show that $n(\hat{\theta}-\theta)$ converges in distribution to a gamma
distribution. First we need to find $c(\theta)$ , it's clear that it's a normalization constant, so integrating the density we get that $c(\theta)= 1/2(\theta+e^{-\theta}-1)$ and we may show that \begin{align}
\mathcal{L}(\theta;x) &= \prod_{i=1}^n c(\theta)(1-\exp(-|x_i|))I\{|x_i|\leq\theta\} \\
 &=c^n(\theta)I_{(0,\theta)}\left(\frac{x_{(n)}-x_{(1)}}{2}\right) \prod_{i=1}^n (1-\exp(-|x_i|)) \\
 &=c^n(\theta)I\left(\frac{x_{(n)}-x_{(1)}}{2},+\infty\right)(\theta) \prod_{i=1}^n (1-\exp(-|x_i|))
\end{align} and as $c(\theta)$ is a decreasing function of $\theta$ , we get that $$
\hat{\theta} =\frac{x_{(n)}-x_{(1)}}{2}
$$ however I'm stuck at the convergence part, I tried manipulating the expression and applying the Jacobian method. Jacobian yields a quite difficult convolution.
Other post shows something similar however the $X$ s are exponentially distributed and the results follow from the memoryless property.","['statistical-inference', 'order-statistics', 'probability', 'maximum-likelihood']"
4339885,"Car, gas, crow problem - looking for analytic solution","I made up this problem recently. possible configuration Imagine you are lost in a car on a long circular road in a hot desert.
You have enough gas to go halfway around the circle. There is a gas
station that is your only hope for survival, but you don’t even know
where you are, let alone which direction will get you to the gas. With this much information, your best option is to pick a direction
and go that way until you reach the gas or run out. Your chances of
survival are 50%. Now imagine there is also a dead crow on the side of the road. All
three objects are randomly distributed around the circle with all
locations equally likely. Also, you are told which direction will get
you to the crow before the gas station. Does this help? What should
you do? What are your chances? I have the sketch of a solution, but it leans on a simulation. Is there a purely analytic solution? The gas station and the car divide the road into two segments, and you
 need the smaller segment to survive. The randomly placed crow is more
 likely to be in the larger segment, simply because it’s larger. You
 should go in the other direction. coordinate system To avoid considering cases
 separately, we define a coordinate system going from the car opposite
 the crow-first direction with circle circumference 1, so g < c. We
 know that 0 ≤ g < 1 but the probability of g → 1 is vanishingly small
 since it would also require that c → 1. It is reasonable to assume
 (and simulation confirms) that the probability density function for g
 is linear so it must be probability density function The probability of survival is the sum of
 the areas shown, p(g<1/2) = 3/4.","['puzzle', 'probability']"
4339896,Probability of reaching target after n tries with regression on failure.,"I used to play MMO game with upgrading system such that when failing to upgrade an item it does not get destroyed but insted get weakened. So let's say i have an item that is +3. When i try to upgrade it to +4 (with some chance of success p , and failure 1-p ) and fail, it gets weakened to +2.
So let's say upgrade levels are +0 ... +9 and the odds of success are as follows: (this is an example, they can be arbitrary, +0 to +1 is always 1) P(+0 -> +1) = 1
P(+1 -> +2) = 0.9
P(+2 -> +3) = 0.8
P(+3 -> +4) = 0.7
P(+4 -> +5) = 0.6
P(+5 -> +6) = 0.5
P(+6 -> +7) = 0.4
P(+7 -> +8) = 0.3
P(+8 -> +9) = 0.2 What is the probability that I will fail exactly n times before reaching max upgrade? I have been thinking about it many times. I can only sumulate it, I don't even know if there is a nice general approach to it. And of course simulation is only approximate and I don't get meaningful results for failure counts of ~>300 because they are hit very rarely.
The result for 100,000,000 runs can be found here https://pastebin.com/mXdNHd6z (code: https://pastebin.com/LKRwj4nk ) It plots like this for the first 200 probabilities The plot may suggest that there is a nice formula, possibly exponential.
For practical purposes the values that I get from the simulation are more than sufficient, but my curiosity is not fullfilled. So the question one is if there's a way to compute exact results. 
Or even a nice iterative method. And if I wanted to find out what is the average amount of times I will fail before reaching +9, would it be the smallest n failures such that the sum of all probabilities for exactly k: k < n failures is greater than 0.5?
 If so then it would be around between 53 and 54. And what if we gave each upgrade a cost. Would it be possible to calculate average cost of upgrading from +0 to +9 with a similar approach (if there is one)?",['probability']
4339899,Conditional expectation of $X+Y$ given $Y-X$,"Consider the following joint density function $$f_{X,Y}(x,y)=e^{-y}$$ if $0<x<y$ and 0 in other case. If I want to find the following expectation $$E[X+Y|Y-X]$$ How do I calculate?
My attempt is correct? I know that by definition $$E[X |Y]=\int x f_{X|Y}(x,Y)dx$$ So if I make the following variable change (Is allowed?) $U=X+Y$ and $V=Y-X$ then the expectation only would be $E[U|V]$ And by above definition I need to find the conditional of U given V. To do this, I try to apply the following equation $$f_{U,V}(u,v)=f_{X,Y}(x=\frac{u-v}{2},v=\frac{u+v}{2})|J|$$ Where the Jacobian is \begin{vmatrix}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v}\\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{vmatrix} and that is $\frac{1}{2}$ So the function with new variables is $f_{U,V}(u,v)=\frac{1}{2}e^{\frac{-u-v}{2}}$ So, now we can find the conditional, $f_{U|V}=\frac{f_{U,V}(u,v)}{f_{V}(v)}$ where $f_{V}(v)=\int_{0}^{\infty} f_{U,V}(u,v)du = \int_{0}^{\infty}\frac{1}{2}e^{\frac{-u-v}{2}}du = \frac{1}{2}e^{-\frac{v}{2}}(2e^{\frac{-0}{2}})$ Is from 0 to $\infty$ because $0<U=x+y$ (is there something wrong here? Somebody told me that in this step there is an error but I don't know) Finally $$f_{U|V}=\frac{1}{2}e^{-u/2}$$ So $$E[U|V]=\int_{0}^{\infty} u\frac{1}{2}e^{-u/2}du  =\frac{1}{2}(0+2(0)e^{-\frac{0}{2}}+4e^{\frac{-0}{2}})=2$$ Please, if I do something wrong or all is wrong and there's another path to get the correct answer let me know please. Technically although I am in a course of probability, I am teaching to myself.","['expected-value', 'conditional-expectation', 'probability']"
4339918,Prove that solutions to $\dot x=f(x)$ are monotonic,"I have the following problem: let $f(x)$ be a continuous function with real argument. Consider the following equation: $\dot x=f(x) \tag{1}$ Prove that all of its solutions are monotonic. Here is the solution I came up with: let $U=\{x:f(x)=0\}$ . If $x_0\in U$ , then the function $x\equiv x_0$ satisfies the equation and it is monotonic. Now let $x_1\notin U$ . Let $$ m_1=\sup\{u\in U:u<x_1\}\\
   M_1=\inf\{u\in U:x_1<u\} $$ By the theorem of existence and uniqueness, there exists a unique solution for any starting condition of kind $x(t_0)=x_0$ where $t\in\mathbb R,x_0\in U$ and we found such a solution above: $x\equiv x_0$ . Therefore solutions to problems of kind $x(t_0)=x_1$ where $t_0\in\mathbb R,x_0\in U$ may not cross the line $x=x_0$ for any $x_0\in U$ , therefore $x(t)\in \langle m_1,M_1\rangle\;\;\forall t\in\mathbb R$ where the interval is open on the corresponding end if $m_1\in U$ or $M_1\in U$ and closed otherwise. Since $f$ is continuous, it has constant sign on this interval $\langle m_1,M_1\rangle$ , because it never reaches zero on this interval, therefore $\dot x=f(x)>0\;\;\forall t\in\mathbb R$ or $\dot x=f(x)<0\;\;\forall t\in\mathbb R$ , which means $x$ is strictly monotonic. Obviously, every solution can be expressed as a solution to a problem with starting conditions $x(t_0)=x_2$ where $t_0\in\mathbb R$ and $x_2\in U$ or $x_2\notin U$ , therefore every solution is either constant or strictly monotonic. However someone pointed out to me that the problem didn't state that $f$ is everywhere differentiable, which means that the theorem of existence and uniqueness doesn't quite hold, like for $f(x)=x^{2/3}$ solutions $x(t)=0$ and $x(t)=t^3/27$ intersect, yet they are both monotonic. And I am not sure what to do with that, because the statement of the problem is still supposed to be true. How can it be proved without using differentiability of $f$ ?",['ordinary-differential-equations']
4339939,Understanding a statement about the series $S =\sum_2 ^\infty \frac{1}{n\ln n}$,"In Mathematical Methods for Physicists , the author writes the following with reference to the series $S =\sum_2 ^\infty \frac{1}{n\ln n}$ : We form the integral $\int_2^\infty \frac{1}{x\ln x} dx$ which diverges, indicating that $S$ is divergent. Because $n\ln n \gt n$ , the divergence is slower than that of the harmonic series. So far so good. He further writes: But because $\ln(n)$ increases more slowly than $n^\epsilon$ , where $\epsilon$ can have an arbitrarily small positive value, we have divergence even though the series $\sum_n n^{-{(1+\epsilon)}}$ converges. This is the part that confuses me. Perhaps the author is referring to the comparison test. It is known that $\sum_n n^{-{(1+\epsilon)}}$ , i.e. $\sum_n \frac{1}{n.n^\epsilon}$ converges since $1+\epsilon \gt 1$ . It would suffice to prove that $n\ln(n) \gt n.n^\epsilon$ . For this we compare how fast $\ln(n)$ and $n^\epsilon$ increase with $n$ . Let's take their derivatives, $\frac{1}{n}$ and $\frac{\epsilon}{n^{1-\epsilon}}$ . Now we do not know the nature of $\epsilon$ . How do I proceed with the conclusion?","['calculus', 'analysis', 'real-analysis']"
4339961,Need a clarification of the proof that the prime ideal space of a distributive bounded lattice is compact,"11.19 Theorem, from B. A. Davey, H. A. Priestley,   Introduction to lattices and order, Let $L$ be a bounded distributive lattice, then the prime ideal space $\langle \mathcal{I}_p(L); \tau \rangle$ is compact. $\tau$ is the topology whose basis is $\mathcal{B}= \{X_b \cap(X\setminus X_c) : b,c \in L\}$ , with $X_a = \{I \in \mathcal{I_p}(L) : a \notin I\}$ . We want to prove that the subbasis $\mathcal{S} = \{X_b : b \in L\}\cup \{X \setminus X_c : c \in L\}$ satisfies Alexander's Lemma. Let $\mathcal{U} := \{X_b : b\in A_0\}\cup\{X \setminus X_c : c \in A_1\}$ , a open cover of $\mathcal{I_p}(L)$ . Let $J$ be the ideal generated by $A_0$ and $G$ the filter generated by $A_1$ . It is easy to prove that $J \cap G \neq \emptyset$ . So $J \cap G \neq \emptyset$ and let $a \in J \cap G$ ; if $A_0$ and $A_1$ are both non-empty, there exist $b_1, \dots, b_j \in A_0$ and $c_1, \dots , c_k \in A_1\:$ s.t. $\:c_1 \wedge \dots \wedge c_k \le a \le b_1 \vee \dots \vee b_j$ , whence $X = X_1 = X_{b_1}\cup \dots \cup X_{b_j} \cup (X \setminus X_{c_1}) \cup \dots \cup (X \setminus X_{c_k})$ . What escapes me is why we can write $1$ in that way if $J \cap G \neq \emptyset$ ?","['lattice-orders', 'order-theory', 'combinatorics', 'discrete-mathematics']"
4340015,How to proof that smooth function vanishing on xy-coordinates cross must be of form $xyg$?,"I am reading Jet Nestruev's Smooth Manifolds and Observables book and I am struggling with the exercise 2.12. It states that any smooth function $f:\mathbb{R}^2\to\mathbb{R}$ vanishing on coordinate cross $K=\{x=0\}\cup\{y=0\}$ must be of form $f(x,y)=xyg(x,y)$ for some smooth function $g:\mathbb{R}^2\to\mathbb{R}$ . The exercise is states just after Hadamard's lemma , so I guess I should use it or related ideas somehow. EDIT. I just managed to prove that $$f(x,y)=x\int_0^1\frac{\partial f}{\partial x}(tx, y)dt \\
f(x,y)=y\int_0^1\frac{\partial f}{\partial y}(x, ty)dt$$ Thus $f(x,y)=xg_1(x,y)=yg_2(x,y)$ . However, I now stuck there. EDIT 2. Ok, I guess I know the answer. Since $f$ vanishes on the cross $K$ , $\frac{\partial f}{\partial x}$ vanishes on $x$ -axis. Thus we can apply the same argument as previously and obtain that $$f(x,y)=xy\int_0^1\int_0^1\frac{\partial^2f}{\partial x\partial y}(tx, sy)dsdt$$","['multivariable-calculus', 'smooth-functions', 'taylor-expansion', 'real-analysis']"
4340025,Markov strong property exercise,"Let $(X_n)$ be a Markov chain with Q being its transition matrix. Let $T=\inf\{n \ge 0:X_n \in A\}$ and Let $u(x)=P_x(T<+\infty)$ . Prove that $u$ verifies the system : $$
\begin{cases}{}
u(x)=1 &\text{if } x\in A   \\ 
u(x)=Pu(x) &\text{if } x\notin A
\end{cases}
. 
$$ My attempt and understanding : My understanding is that $T$ represents ""the first time we get to the subset $A$ "" and $u(x)$ represents ""the probability of hitting the subset $A$ starting from a $X=x$ "" ( because $P_x(T=+\infty)$ should represent the probability of never getting in the subset $A$ ). That being said, the first part of the system makes sense because if $(X=x) \in A$ then $T=0$ ( smallest $n$ ) and we are already in $A$ so the probability of getting to $A$ should be equal to $1$ . The problem is the second part, how am I going to use markov strong property to prove that?","['markov-chains', 'stochastic-processes', 'markov-process', 'probability-theory', 'probability']"
4340028,"Given two distinct intersecting circles, length of that chord of larger circle which is bisected by the smaller circle is equal to?","Two circles whose centres lie on the x axis, whose radii are $\sqrt2cm$ and $1cm$ and whose centres are 2 cm apart intersect at a point A.The chord AC of the larger circle cuts the smaller circle at a point B and is bisected by that point. What is the length of chord AC?
.
My attempt :
. .
As shown in the diagram above I started by assuming the centre of smaller circle S 1 (Of radius 1) to be origin and the centre of the larger circle S 2 (Of radius $\sqrt2$ ) to be $(2,0)$ as the centres are separated by 2 units.
I then solved : S 1 : $ x^2 + y^2 = 1 $ ; and S 2 : $ (x-2)^2 + y^2 = 2 $ to obtain $A(\frac{3}4 , \frac{\sqrt7}4)$ I noted the following equations: Since B is given to be mid point of chord AC: X B = $\frac{X_A + X_C}2$ ; $Y_B = \frac{Y_A +Y_C}2$ Since C lies on $S_2$ : $ (X_C-2)^2 + Y_C^2 = 2 $ The distance of B from origin is 1 unit : $(X_B-0)^2 + (Y_B-0)^2 = 1$ $(\frac{X_A + X_C}2 - 0)^2$ + $(\frac{Y_A +Y_C}2 - 0)^2 = 1$ $(\frac{\frac{3}4 + X_C}2 - 0)^2$ + $(\frac{\frac{\sqrt7}4 +Y_C}2 - 0)^2 = 1$ This equation and equation generated in point 2 together are two equations in two variables and i should be able to solve them to get the co-ordinate of C. This however is proving to be cumbersome. .
Is there a better way to avoid this approach.","['euclidean-geometry', 'analytic-geometry', 'coordinate-systems', 'circles', 'geometry']"
4340047,How to solve a complicated ODE,"The equation is $$
f^{\prime}(x) = \gamma \frac{f(x)+f^2(x)}{\log\left(\frac{f(x)}{1+f(x)}\right)}
$$ with the initial condition $f(0)$ , where $x\geq 0$ and $f(0)\geq0$ . The solution is $$f(x)=\frac{1}{-1+\exp\sqrt{2\gamma x + \log\left(\frac{f(0)+1}{f(0)}\right)^2}}$$ I think the ODE can be solved by separating the variables $$
\int_{0}^{f(x)} \frac{\log\left(\frac{f(z)}{1+f(z)}\right)}{f(z)+f^2(z)} df = \int_{0}^{x} \gamma dz
$$ The right-hand size is easy. I do not know how to solve the integration of the left-hand side.","['integration', 'indefinite-integrals', 'ordinary-differential-equations']"
4340056,Nondecreasing and piecewise-continuous => piecewise-continuously-differentiable?,"I'm interested to know if a piecewise-continuous monotone function from $\mathbb{R}$ to $\mathbb{R}$ is also piecewise-continuously differentiable. I mean piecewise in the sense that there is a finite or countably infinite and isolated set $S\subset\mathbb{R}$ , and intervals covering $\mathbb{R}\setminus S$ such that on any of these intervals, continuity and continuous-differentiability are verified. I know that a monotone function is almost everywhere differentiable ( this topic ) there are monotone and everywhere differentiable functions that are not continuously differentiable ( this other topic ) almost everywhere differentiable may not imply almost everywhere continuously differentiable, the derivative can actually be nowhere continuous ( this one ) It seems to me that this third reference is not exactly a counter-example as from what I understand, it features a sort of devil-staircase function and I guess this one is not piecewise-continuous? (possible accumulating sequence of discontinuities)","['piecewise-continuity', 'analysis']"
4340068,Trying to find an example of a real-valued functions that shares two or more values with its derivative.,"I'm trying to find an example of a real-valued functions that shares two or more values with its derivative, but f' doesn't equal f. By sharing two values a and b, I mean f = a whenever f' = a, and f = b whenever f' = b. I know it's impossible for 2 meromorphic complex entire functions (that's what my paper's about) so I need a function that's differentiable once, and not analytic on C.","['complex-analysis', 'derivatives', 'real-analysis']"
4340070,A curious identity involving the Appell hypergeometric series.,"By playing around with exact solutions  to ordinary differential equations from 1 (here I mean with entry 1.4.5.31 in page 180 to be exact) we have discovered a following curious identity involving the Appell hypergeometric function $F_1() $ . Let $\xi_1,\xi_2,\xi_3,\xi_4,\xi_5 $ and $t$ be parameters. Then we define another parameters $\chi_1,\chi_2,\chi_3$ as follows: \begin{array}{lll}
\chi_1 &=& -\frac{2 \left(\xi_1^2-\xi_4 \xi_1-\xi_5 \xi_1+\xi_4 \xi_5\right)}
{(\xi_1-\xi_2) (\xi_1-\xi_3)}\\
\chi_2 &=& \frac{2 \left(\xi_2^2-\xi_4 \xi_2-\xi_5 \xi_2+\xi_4 \xi_5\right)}{(\xi_1-\xi_2)
   (\xi_2-\xi_3)}\\
\chi_3 &=& \frac{2 \left(\xi_3^2-\xi_4 \xi_3-\xi_5 \xi_3+\xi_4 \xi_5\right)}
{(\xi_1-\xi_3) (\xi_3-\xi_2)}
\end{array} Then we have checked numerically that the following identity holds true: \begin{eqnarray}
&&\frac{1}{t-\xi_2} \cdot F_1\left( 1, \chi_1+1,\chi_3+2,2, \frac{\xi_1-\xi_2}{t-\xi_2}, \frac{\xi_3-\xi_2}{t-\xi_2} \right) + \\
&& \frac{(1+\chi_1)(\xi_1-\xi_2)}{2(t-\xi_2)^2} \cdot F_1\left( 2, \chi_1+2,\chi_3+2,3, \frac{\xi_1-\xi_2}{t-\xi_2}, \frac{\xi_3-\xi_2}{t-\xi_2} \right) + \\
&& \frac{(2+\chi_3)(\xi_3-\xi_2)}{2(t-\xi_2)^2} \cdot F_1\left( 2, \chi_1+1,\chi_3+3,3, \frac{\xi_1-\xi_2}{t-\xi_2}, \frac{\xi_3-\xi_2}{t-\xi_2} \right) = \\
%
&&\left(t-\xi_1\right)^{-\chi_1-1}
\left(t-\xi_2\right)^{-\chi_2}
\left(t-\xi_3\right)^{-\chi_3-2}
\end{eqnarray} A snippet of Mathematica code that verifies that identity numerically is included below: t =.; tt =.;
{xi[1], xi[2], xi[3], xi[4], xi[5]} = 
  RandomReal[{-1, 0}, 5, WorkingPrecision -> 50];
{chi[1], chi[2], 
   chi[3]} = {-((
    2 (xi[1]^2 - xi[1] xi[4] - xi[1] xi[5] + xi[4] xi[5]))/((xi[1] - 
       xi[2]) (xi[1] - xi[3]))), (
   2 (xi[2]^2 - xi[2] xi[4] - xi[2] xi[5] + xi[4] xi[5]))/((xi[1] - 
      xi[2]) (xi[2] - xi[3])), (
   2 (xi[3]^2 - xi[3] xi[4] - xi[3] xi[5] + xi[4] xi[5]))/((xi[1] - 
      xi[3]) (-xi[2] + xi[3]))};

remm1 := 2 a ((1/(t - xi[2])
         AppellF1[1, chi[1] + 1, chi[3] + 2, 
         2, (xi[1] - xi[2])/(t - xi[2]), (xi[3] - xi[2])/(t - 
           xi[2])] + ( (1 + chi[1]) (xi[1] - xi[2]))/(2 (t - xi[2])^2)
         AppellF1[2, 2 + chi[1], 2 + chi[3], 3, (xi[1] - xi[2])/(
         t - xi[2]), (-xi[2] + xi[3])/(
         t - xi[2])] + ((2 + chi[3]) (-xi[2] + xi[3]))/(
        2 (t - xi[2])^2)
         AppellF1[2, 1 + chi[1], 3 + chi[3], 3, (xi[1] - xi[2])/(
         t - xi[2]), (-xi[2] + xi[3])/(t - xi[2])] ) - (t - 
        xi[1])^(-chi[1] - 
       1) (t - xi[2])^-chi[2] (t - xi[3])^(-chi[3] - 2));

t = RandomReal[{0, 1}, WorkingPrecision -> 50];
{remm1} 1 Polyanin, A. D.; Zaitsev, Valentin F. , Handbook of exact solutions for ordinary differential equations., Boca Raton, FL: CRC Press. xxvi, 787 p. (2003). ZBL1015.34001 . The question is now can we prove that identity? I tried to use the recurrence relations from the Wikipedia website Appell hypergeometric function but they do not involve a source term that we have in the right hand side above. How do we go about proving that relation?","['ordinary-differential-equations', 'hypergeometric-function']"
4340076,Does homotopy equivalence of pairs distinguish knots?,"I was trying to figurate what is the meaning of homotopy equivalence of a pair. In particular if we consider $X_1\subseteq X$ and $Y_1\subseteq Y$ , and we have $(X,X_1)$ is homotopy equivalent to $(Y,Y_1)$ , is this condition stronger then imposing only $X_1\simeq X$ and $Y_1 \simeq Y$ ? I would like to know if there are some examples in which the pairs carry more structure then the single spaces. For example if we consider homeomorphism in place of homotopy equivalences, then $((\mathbb{R}^3,f(S^1))$ for different embeddings $f$ is not homeomorphic as a pair unless the two knot are equivalent. Does this example work also if we replace homotopy equivalent to homeomorphic? Remark: I just want to add a little motivation to this question.
I was interested in finding a relative version of the Whitehead theorem i.e. something like: if $(X,X_1), (Y,Y_1)$ are CW pairs and if there is a function that induces an isomorphism on every $\pi_n(X) \simeq \pi_n(Y),\;\pi_n(X_1) \simeq \pi_n(Y_1),\; \pi_n(X,X_1) \simeq \pi_n (Y,Y_1)$ then there is a homotopic equivalence between pairs $(X,X_1)\simeq (Y,Y_1)$ . I strongly suspect this to be false because by the five lemma we can deduce the isomorphism between $\pi_n(X,X_1) \simeq \pi_n (Y,Y_1)$ from that of $\pi_n(X) \simeq \pi_n(Y),\;\pi_n(X_1) \simeq \pi_n(Y_1)$ . The previous example would give a counterexample because each single space is homeomorphic, so the long exact sequences in homotopy are isomorphic but the two spaces are not homotopic equivalent as pairs.","['knot-theory', 'general-topology', 'algebraic-topology']"
4340081,A mistake in Grothendieck's Tôhoku paper? Theorem 5.2.1,"I was reading the ``Sur quelques points d'alegbre homologique'' English translation when I came across the spectral sequences for equivariant cohomology shown in Theorem 5.2.1 (in the original French document). I checked the original version to see if it had been a translator's error, but no, it is not, so Theorem 5.2.1 says the following: There exists on the category of Abelian $G$ -sheaves on $X$ two co-homological spectral functors abutting on the graded functor $H^n(X,G;\mathscr F)$ , whose initial
terms are, respectively: $$
I_2^{p,q} = H^p(Y;\mathscr{H}^q(X;\mathscr F))
$$ and $$
II_2^{p,q} = \mathscr{H}^p(G,H^q(X;\mathscr F)).
$$ (Here $Y$ denotes the quotient $X/G$ ). I think there are two mistakes: In $I_2$ , I think $\mathscr H^q(X;\mathscr F) $ actually means $$
\mathscr H^q(X;\mathscr F)
$$ which is the sheaf on $Y$ associated to the presheaf $V\mapsto H^q(f^{-1}(V),G;\mathscr F)$ (here $f:X\rightarrow Y$ is the quotient map). In $II_2$ , $\mathscr H^p(G,H^q(X;\mathscr F))$ is meaningless for me, and I think it should be $$
H^p(G, H^q(X;\mathscr F)).
$$ How could a sheaf converge to an Abelian group? Finally, notice that, with my corrections, Theorem 5.2.1 looks pretty much like Theorem 4.4.1 (always referring to the French edition). Am I right then?","['spectral-sequences', 'algebraic-geometry', 'sheaf-cohomology', 'equivariant-cohomology']"
4340095,The domain of the Haar measure of the $n$-dimensional Torus,"I'm trying to find the Haar measure of the $n$ -dimensional Torus. To find this measure I'm using the notion of pushforward measure of the Lebesgue measure. To understand my question please consider the information below: Let $\mathbb{T}^n:=\mathbb{R}^n/\mathbb{Z}^n$ be the quotient of the group $(\mathbb{R}^n,+)$ by the subgroup $(\mathbb{Z}^n,+)$ . Define $d _{\mathbb{T}^n}:\mathbb{T}^n\times \mathbb{T}^n\to\mathbb{R}$ by $d_{\mathbb{T}^n}(x+\mathbb{Z}^n,y+\mathbb{Z}^n) :=\inf \big\{\Vert x-y+ k\Vert _n:k\in\mathbb{Z}^n\big\}$ . We can show that $(\mathbb{T}^n,d _{\mathbb{T}^n})$ is a metric space and that $\mathbb{T}^n$ is a abelian compact group with respect to that metric. We can also show that $\pi  :\mathbb{R}^n\to \mathbb{T}^n$ given by $\pi (x):=x+\mathbb{Z}^n$ is an open Lipschitz continuous map. Define $\pi_\star \mathfrak{B}_n :=\big\{B\subseteq\mathbb{T}^n:\pi ^{-1}[B]\in \mathfrak{B}_n\big\} $ in which $\mathfrak{B}_n$ is the Borel $\sigma$ -algebra of $\mathbb{R}^n$ . Suppose that $\mathfrak{B}_{\mathbb{T}^n}$ is the Borel $\sigma$ -algebra of $(\mathbb{T}^n,d _{\mathbb{T}^n})$ . My question is: does the equality $\mathfrak{B}_{\mathbb{T}^n}=\pi _\star \mathfrak{B}_n$ hold? It's easy to show that $\mathfrak{B}_{\mathbb{T}^n}\subseteq \pi _\star \mathfrak{B}_n$ since $\pi$ is a measurable map. However I don't know if $\pi_\star \mathfrak{B}_n\subseteq \mathfrak{B}_{\mathbb{T}^n}$ . Thank you for your attention.","['measure-theory', 'borel-measures', 'haar-measure', 'measurable-functions', 'borel-sets']"
4340122,Confused about the meaning of a differantial map in baby do Carmo.,"The images are from section $2-4$ of do Carmo's Differential Geometry of Curves and Surfaces . In the following discussion $T_p(S_1)$ is the tangent plane of $S_1$ (a regular surface) at $p$ (a point of such surface). Similarly with $T_{\varphi(p)}(S_2)$ . I do not understand what it means for $\varphi$ to be differentiable since it is defined as a function $S_1\rightarrow S_2$ , where $S_1$ is not an open set of $\mathbb{R}^3$ . As I understand it, the goal of proposition $2$ (the proposition following the discussion above, and which is pictured below in the post) is to define what it means for a function between two regular surfaces to be differentiable, so it makes no sense to claim that $\varphi$ is differentiable at this point of the text. In case it is needed, proposition $2$ reads as follows: What does it mean for $\varphi$ to be differentiable?","['differential', 'proof-explanation', 'linear-transformations', 'derivatives', 'differential-geometry']"
4340130,How to solve $0=\dot{y}-\dot{y}^2-y-t^2$?,"I was messing around with some differential equations(long story involving a game of snake, a game of minesweeper, chemical bonds, a basketball, and a friend) when I made a mistake and got the equation $0=\dot{y}-\dot{y}^2-y-t^2$ . However, this seemed like a much more interesting equation than what I should've got but I couldn't figure out how to solve this which brought me here. Could someone give me a hint or point me towards some reading or techniques that could help me solve it?  Thanks. I apologize if I violated any customs of MSE or rules, I'm not familiar.",['ordinary-differential-equations']
