question_id,title,body,tags
4505560,"Proof that $n! > n + 1$, $\forall n > 2$","I'm trying to understand the usage of math induction, so I came up with this exercise: Show that $n! > n + 1$ , $\forall n > 2, n\in\mathbb N$ . So I started with the base case of $n = 3$ : \begin{gather*}
3! > 3 + 1 \\
6 > 4.
\end{gather*} Considering $k! > k+1$ and $k > 2$ , now to prove $(k + 1)! > k+1+1$ : \begin{gather*}
(k+1)! = (k+1)k! = kk! + k! \\
kk! + k! > (k+1)k+k+1.
\end{gather*} (Changing $k!$ to $k+1$ , because of the condition above.) Since $k > 2$ can I do the following? $$(k+1)! > (0+1)1 + k + 1.$$ Then $$(k+1)! > k + 1 + 1.$$ Is this the correct way to prove the statement?","['induction', 'solution-verification', 'discrete-mathematics']"
4505569,Show $\sum\limits_{k=0}^\infty (-1)^k \frac{\Gamma\left(k+\frac14\right)}{\Gamma\left(k+\frac74\right)} = -\Gamma\left(-\frac12\right) = 2\sqrt\pi$,"A few days ago, I posted a partial solution to an old question re. showing $$\int_0^1 \frac{\sqrt{1-x^4}}{1+x^4} \, dx = \frac\pi4$$ by converting $\frac1{1+x^4}$ to a Taylor series and I ended up recovering a sum involving a ratio of Gamma functions. The missing piece is to show $$\sum_{k=0}^\infty (-1)^k \frac{\Gamma\left(k+\frac14\right)}{\Gamma\left(k+\frac74\right)} = 2\sqrt\pi$$ Similar resolved questions seem to suggest the duplication or multiplication formulae would be of some help and/or the identities showcased here , but I'm not sure how to align the sum I have to match the forms given in the identity. I'm most interested in a hint - what manipulations should I consider to get a form that more closely resembles the known results above? Any other methods are acceptable, but I'm specifically trying to compute this sum in order to evaluate the aforementioned integral. By converting Gammas to factorials to binomial coefficients, I got $$\begin{align*}
\sum_{k=0}^\infty (-1)^k \frac{\Gamma\left(k+\frac14\right)}{\Gamma\left(k+\frac74\right)} &= \left(-\frac32\right)! \sum_{k=0}^\infty (-1)^k \frac{\left(k-\frac34\right)!}{\left(k+\frac34\right)!\left(-\frac32\right)!} \\[1ex]
&= \Gamma\left(-\frac12\right) \sum_{k=0}^\infty (-1)^k \binom{k-\frac34}{k+\frac34} \\[1ex]
&= 2\sqrt\pi \sum_{k=0}^\infty \binom{\frac{8k-11}4}{\frac{8k-1}4}
\end{align*}$$ where in the last equality, I applied Pascal's rule $\binom nk=\binom{n-1}k+\binom{n-1}{k-1}$ (though I'm not 100% sure it holds for $n,k\in\Bbb Q$ ) to eliminate the positive terms in the series. So the new missing piece is $$\sum_{k=1}^\infty \binom{\frac{8k-11}4}{\frac{8k-1}4} = 1$$","['gamma-function', 'definite-integrals', 'hypergeometric-function', 'sequences-and-series']"
4505584,Is it possible to derive the Jacobian from the chain rule for multivariable functions?,"In the single variable calculus, when doing a u substitution, we have $\qquad\begin{align}\int_a^b{f'(x)}\,\mathrm dx &= \int_a^b{(h \circ g)'(x)\,\mathrm dx}\\& = \int_a^b{(h' \circ g)(x)\,g'(x)\,\mathrm dx} \\&= (h \circ g)(x) \Big|_a^b \\&= h(g(b)) - h(g(a)) \\&= \int_{g(a)}^{g(b)}{h'(u)\,\mathrm du}\end{align}$ Here, there's a nice connection between the chain rule and the Jacobian, where the $g'(x)dx$ term comes out of the wash without any appeals to geometric reasoning, like is often the case when introducing the Jacobian term for the multivariable change of variables.  Is it possible to make an analogous derivation for the multivariable case? What confuses me is that the multivariable chain rule has the form of a sum, but the Jacobian term has the form of a difference, so at first glance it seems not possible.","['jacobian', 'multivariable-calculus', 'chain-rule']"
4505598,Inner product inequality for symmetric matrix,"I could use some guidance regarding this question. Let $A$ be an $n\times n$ symmetric matrix with non-negative elements. Prove that for any nonzero $x \in \mathbb{R}^n$ with non-negative elements the following inequality holds: $$
\Bigg(\frac{\langle x, Ax\rangle}{\langle x, x\rangle}\Bigg)^m \leq \frac{\langle x, A^mx\rangle}{\langle x, x\rangle}, \quad m \in \mathbb{Z}^+
$$ where $\langle \cdot, \cdot \rangle$ denotes the inner (dot) product. Hint: Use induction. All I have so far is this after the inductive step: $$
\Bigg(\frac{\langle x, Ax\rangle}{\langle x, x\rangle}\Bigg)^k \Bigg(\frac{\langle x, Ax\rangle}{\langle x, x\rangle}\Bigg) \leq \Bigg(\frac{\langle x, A^kx\rangle}{\langle x, x\rangle}\Bigg) \Bigg(\frac{\langle x, Ax\rangle}{\langle x, x\rangle}\Bigg)
$$ for some $k \in \mathbb{Z}^+$ , trying to show the inequality for $k+1$ . I messed around with the symmetric matrix fact and transposes on the RHS, but I did not get anywhere. I asked around at the department, but I did not get much help. A few questions right away: What fundamental thing am I missing here? There must be some piece of information or some overall concept that I am not seeing. These look like Rayleigh quotients (although I do not know much about them). Are they key to solving this problem? What does non-negative entries give you? Is it simply to ensure that you don't divide by zero, or is there something else going on? I know that each $x$ in the statement is essentially a unit vector. Does this have something to do with the problem? When you apply $A$ though, I do not think that $x$ remains a unit vector in general. UPDATE(1): I'm pretty sure this has to do with the spectra of real symmetric matrices. I just looked up Rayleigh quotients, and it seems like there are some theorems more generally that involve forms like those in this question. UPDATE(2): I was given a suggestion (not a hint, so I do not know yet if it is fruitful) to try to prove the case for $m=2$ and then prove the inductive step of $m$ implies $m + 2$ . I have not yet had the time to try this, but it seems promising.","['matrices', 'linear-algebra', 'symmetric-matrices']"
4505638,"A question about an improper integral over an open set $A$. Why does $D$ need to be a closed set? (""Analysis on Manifolds"" by James R. Munkres)","I am reading ""Analysis on Manifolds"" by James R. Munkres. The following is the definition of an improper integral in this book. Definition. Let $A$ be an open set in $\mathbb{R}^n$ ; let $f:A\to\mathbb{R}$ be a continuous function. If $f$ is non-negative on $A$ , we define the (extended) integral of $f$ over $A$ , denoted $\int_A f$ , to be the supremum of the numbers $\int_D f$ , as $D$ ranges over all compact rectifiable subsets of $A$ , provided this supremum exists. In this case, we say that $f$ is integrable over $A$ (in the extended sense). More generally, if $f$ is an arbitrary continuous function on $A$ , set $$f_{+}(x)=\max\{f(x),0\}\text{ and }f_{-}(x)=\max\{-f(x),0\}.$$ We say that $f$ is integrable over $A$ (in the extended sense) if both $f_{+}$ and $f_{-}$ are; and in this case we set $$\int_A f=\int_A f_{+}-\int_A f_{-},$$ where $\int_A$ denotes the extended integral throughout. $D$ needs to be a bounded set since we define the (ordinary) integral of $f$ only over a bounded set. The author wrote $D$ is a compact rectifiable subsets of $A$ . Why does $D$ need to be a closed set? If $A=\{(x,y)\mid x^2+y^2<1\}$ , $D:=A$ is a bounded rectifiable subset of $A$ , but there is a continuous function $f$ on $A$ which is not bounded. Can we define an improper integral over $A$ as follows? Definition. Let $A$ be an open set in $\mathbb{R}^n$ ; let $f:A\to\mathbb{R}$ be a continuous function. If $f$ is non-negative on $A$ , we define the (extended) integral of $f$ over $A$ , denoted $\int_A f$ , to be the supremum of the numbers $\int_D f$ , as $D$ ranges over all bounded rectifiable subsets of $A$ $\color{red}{\text{on which $f$ is bounded}}$ , provided this supremum exists. In this case, we say that $f$ is integrable over $A$ (in the extended sense). More generally, if $f$ is an arbitrary continuous function on $A$ , set $$f_{+}(x)=\max\{f(x),0\}\text{ and }f_{-}(x)=\max\{-f(x),0\}.$$ We say that $f$ is integrable over $A$ (in the extended sense) if both $f_{+}$ and $f_{-}$ are; and in this case we set $$\int_A f=\int_A f_{+}-\int_A f_{-},$$ where $\int_A$ denotes the extended integral throughout. Can we modify the definition of an improper integral over $A$ in such a way $D$ doesn't need to be a closed set in a new definition?","['multivariable-calculus', 'definition', 'improper-integrals']"
4505640,"Show that $\frac{\sin^3 \beta}{\sin \alpha} + \frac{\cos^3 \beta}{\cos \alpha} = 1$ with certain given $\alpha, \beta$","Let $$\frac{\sin (\alpha)}{\sin (\beta)} + \frac{\cos (\alpha)}{\cos (\beta)} = -1 \tag{$1$}$$ where $\alpha, \beta$ are not multiples of $\pi / 2$ . Show that $$\frac{\sin^3 (\beta)}{\sin (\alpha)} + \frac{\cos^3 (\beta)}{\cos (\alpha)} = 1\tag{$2$}$$ I've tried to rewrite $(1)$ and insert into $(2)$ to get $$ - 1 -  \frac{\sin^4 \beta \cos^2 \alpha + \cos^4 \beta \sin^2 \alpha}{\sin \alpha \sin \beta \cos \alpha \cos \beta} = 1 \\
\iff \frac{\sin^4 \beta \cos^2 \alpha + \cos^4 \beta \sin^2 \alpha}{\sin \alpha \sin \beta \cos \alpha \cos \beta} = -2$$ But I can't simplify any further, maybe there are some trig. identities I'm missing?","['algebra-precalculus', 'trigonometry']"
4505645,Can any two points in the plane be assigned continuously a path between them that is transversal to two circles?,"More formally, I'm interested in continous functions $$\Gamma : \mathbb{R}^2\times \mathbb{R}^2 \to ([0,1] \to \mathbb{R}^2)$$ such that $\Gamma(p,q)(0)=p$ and $\Gamma(p,q)(1)=q$ , with the additional condition that for every $(p,q)\in \mathbb{R}^2\times \mathbb{R}^2$ , we have that $\Gamma(p,q) \pitchfork X$ , where $X$ is two circles of radius 1 centered in $(-2,0)$ and $(2,0)$ . For the transversality condition to make sense, we require that $\Gamma(p,q)$ is smooth. It can be seen that when $X$ is just one circle, this can be done, by composing the path that takes $p$ to the origin, and then goes back to $q$ . Sadly, the same trick can't be applied to the two circles, at least as far as I can see. I suspect there is no function that satisfies these requirements, but I have not been able to show this. I'm interested in this as it's a special case of topological complexity , and may give a strictly greater upper bound. Edit: I'm assuming the compact open topology on $[0,1] \to \mathbb{R}^2$ .","['differential-topology', 'differential-geometry']"
4505676,Solve the differential equation: $(3x^2-y^2)dy-2xdx=0$,"I have the differential equation: $$(3x^2-y^2) dy - 2x dx=0 $$ I need it to look like the equation: $$y'+p(x)y=q(x)$$ in order to apply the integrating factor. I believe it can be solved using integrating factors because that is the section in the textbook it has come from, but I am totally stumped on how to get $y'$ isolated for the integrating factor. Is this an integrating factor equation, and if so how do I deal with these types of equations?","['integration', 'calculus', 'integrating-factor', 'ordinary-differential-equations']"
4505681,Properness of the blow ups of arithmetic schemes,"In Silverman's textbook Advanced Topics in the Arithmetic of Elliptic Curves, on page 345, he defined the blow up of an arithmetic surface (or a two dimensional scheme) at the point $(\pi,x,y) = (0,0,0)$ (i.e. reducing modulo $\pi$ since we are interested in what happens on the special fiber). I believe we can extend his definition directly to blowing up subschemes e.g., say if we want to blow-up the whole $x$ -axis instead, then we only consider the generators $\pi$ and $y$ for the subscheme? Also, his definition should not be limited to only arithmetic surfaces (we just add more variables and charts)? Now let us fix an arithmetic scheme $C$ over a local ring $\mathcal{O}$ with uniformizer $\pi$ . Following Silverman's definition, I suppose blowing up $C$ at any arithmetic subschemes (which we include the additional generator $\pi$ so that the generic fiber remains unaffected) gives us a new model $C^{\prime}$ , together with a blow up map $C^{\prime} \rightarrow C$ . My main question being, if $C$ is proper then is $C^{\prime}$ also proper? i.e. is the morphism $C^{\prime} \rightarrow \text{Spec}(\mathcal{O})$ a proper morphism?","['algebraic-geometry', 'blowup']"
4505698,Is there any other method to show that $\int_{0}^{\frac{\pi}{2}} x \ln (\sin x) d x =-\frac{\pi^{2}}{8} \ln 2+\frac{7}{16}\zeta(3)?$,"Noting that the evaluation of the integral can be simplified by the Fourier series of $\ln(\sin x)$ , $$\ln (\sin x)+\ln 2=-\sum_{k=1}^{\infty} \frac{\cos (2 k x)}{k}$$ Multiplying the equation by $x$ followed by integration from $0$ to $\infty$ , we have $$
\begin{aligned}
\int_{0}^{\frac{\pi}{2}} x \ln (\sin x) d x+\int_{0}^{\frac{\pi}{2}} x\ln 2 d x&=-\sum_{k=1}^{\infty} \int_{0}^{\frac{\pi}{2}} \frac{x \cos (2 k x)}{k} d x\\
\int_{0}^{\frac{\pi}{2}} x \ln (\sin x) d x+\left[\frac{x^{2}}{2} \ln 2\right]_{0}^{\frac{\pi}{2}}&=-\sum_{k=1}^{\infty} \frac{1}{2 k^{2}} \int_{0}^{\frac{\pi}{2}} x d(\sin 2 k x)\\
\int_{0}^{\frac{\pi}{2}} x \ln (\sin x) d x&=-\frac{\pi^{2}}{8} \ln 2-\frac{1}{2} \sum_{k=1}^{\infty} \frac{1}{k^{2}}\left[\frac{\cos 2(x)}{2 k}\right]_{0}^{\frac{\pi}{2}} \\
&=-\frac{\pi^{2}}{8} \ln 2-\frac{1}{4} \sum_{k=1}^{\infty} \frac{(-1)^{k}-1}{k^{3}}\\
&=-\frac{\pi^{2}}{8} \ln 2+\frac{1}{4}\left(\sum_{k=1}^{\infty} \frac{2}{(2 k+1)^{3}}\right)\\
&=-\frac{\pi^{2}}{8} \ln 2+\frac{1}{2}\left[\sum_{k=1}^{\infty} \frac{1}{k^{3}}-\sum_{k=1}^{\infty} \frac{1}{(2 k)^{3}}\right]\\
&=-\frac{\pi^{2}}{8} \ln 2+\frac{7}{16}\zeta(3) \blacksquare
\end{aligned}
$$ Furthermore, $$
\begin{aligned}
\int_0^{\frac{\pi}{2}} x \ln (\cos x) d x&=\frac{\pi}{2} \int_0^{\frac{\pi}{2}} \ln (\sin x)-\int_0^{\frac{\pi}{2}} x \ln (\sin x) d x \\
&=-\frac{\pi^2}{4} \ln 2-\left(-\frac{\pi^2}{8} \ln 2+\frac{7}{16}\zeta(3)\right) \\
&=-\frac{\pi^2}{8} \ln 2-\frac{7}{16} \zeta(3)
\end{aligned}
$$ and $$
\int_0^{\frac{\pi}{2}} x \ln (\tan x) d x=\int_0^{\frac{\pi}{2}} x \ln (\sin x) d x-\int_0^{\frac{\pi}{2}} x \ln (\cos x) d x=\frac{7}{8}\zeta(3)
$$","['integration', 'improper-integrals', 'definite-integrals', 'calculus', 'zeta-functions']"
4505724,Where does this algorithm for detecting whether a polygon's vertices are given counter clock-wise come from?,"There's an algorithm I found online which I have been using (I verified it works correctly for both convex and concave polygons). Let's say we have a simple polygon, angles between edges are different from $180$ degrees. Vertices are given in such a way that $(x_i, y_i)$ and $(x_{i+1}, y_{i+1})$ form an edge. For each vertex $(x_i, y_i)$ , accumulate the sum of $(x_{i+1} - x_{i}) * (y_{i+1} + y_{i})$ . If the sum is negative, then the polygon's vertices are given in counter clock-wise order. Else it's clock-wise order (I don't know about the case where the sum equals $0$ , I'm not aware if it even exists). Note that for the last vertex, index $i+1$ goes back to the first one (as in a loop). I'd like to know the explanation of why this works. If possible, a proof would be appreciated (an intuitive explanation would be enough though). I thought this was a math/geometry question rather than algorithmic so I posted it here.","['geometry', 'polygons']"
4505736,Differentiability at the limit of non-differentiable points,"Let $f : \mathbb{R} \to \mathbb{R}$ be a continuous function. Suppose there exists a sequence $(x_n)_{n \ge 1} \subseteq \mathbb{R}$ such that: there exists $x_\infty \in \mathbb{R}$ such that $x_n \to x_\infty$ as $n \to \infty$ $f$ is not differentiable at each point $x_n$ Do we know that $f$ cannot then be differentiable at $x_\infty$ ? I suspect it is impossible for $f$ to be differentiable at $x_\infty$ but I have not been able to prove this myself. I have tried approximating the Newton quotients at $x_\infty$ by those at $x_n$ , making use of continuity, but to no avail. I have also not found this result anywhere online. Hints towards the proof of this result or counter-examples would be greatly appreciated. Of course, after posting, I realized that this question may be stated more succinctly. For $f \in C(\mathbb{R})$ , is the set $$A := \{x \in \mathbb{R}\ |\ f'(x) \text{ does not exist}\}$$ closed?","['continuity', 'calculus', 'derivatives', 'real-analysis']"
4505752,Is it possible for Riemann Sum and Standard Integration to have different answers?,"I have a specific question for this equation : $$\frac{1}{x+1}
$$ Using Standard Integration, $$\int_{0}^{1} \frac{1}{x+1}
$$ which is approximately 0.69 Using Riemann Sum (right end point), however, I get this $$ \lim _{n \to \infty }  \Sigma ^n _{i=1} \frac{1}{n+i}
$$ $$ = \lim _{n \to \infty } ( \frac {1} {n+1} + \frac {1} {n+2} + \frac {1} {n+3} + ... +\frac {1} {2n} )
$$ $$ = \lim _{n \to \infty } (\frac{1}{n})( \frac {1} {1+\frac{1}{n}} +  \frac {1} {1+\frac{2}{n}} +  \frac {1} {1+\frac{3}{n}} + ... +\frac {1} {2} )
$$ $$=  (\frac{1}{\infty})( \frac {1} {1+0} + \frac {1} {1+0} + \frac {1} {1+0} + ... +\frac {1} {2} )
$$ $$= 0
$$ Is this right?","['integration', 'limits', 'riemann-sum']"
4505822,"What is another way to prove the least amount of pence, nickels, dimes and quarters that guarantees making any change exactly?","In United States, there are four common coins people carry, \begin{align*}
1 \text{ Penny} &= \$0.01 \\
1 \text{ Nickel} &= \$0.05 \\
1 \text{ Dime} &= \$0.10 \\
1 \text{ Quarter} &= \$0.25 \\
\end{align*} Define making the exact change as not getting any coins back from the cashier, i. e., paying the fractional dollar in exact coins. E. g., one buys two gallons of milk for $\$10.19$ then making the exact change here means paying the fractional dollar as a dime, a nickel and four pence. Some people (like myself) always pay the ceiling of the total bill and end-up with an ever increasing pile of coins under the armrest of their cars. To remedy that, I started thinking if I could assure myself of making the exact change each time, I will be better encouraged to actually take some of those coins with me. Clearly, the more random coins one carries, the greater the probability of making the exact change but, What is the least amount of pence, nickels, dimes and quarters that guarantees making any change exactly? Taking a greedy approach, you can conclude that you need 3 quarters since if you needed four then you are paying in a dollar, 2 dimes since three means you can use a quarter, 1 nickel since two of them mean you can use a dime and 4 pence since five would mean you can use a nickel. Now I want to prove this in a more algebraic way where 4 pence, 1 nickel, 2 dimes and 3 quarters fall out as a solution of something. Let $\lceil x_1\rceil, \lceil x_2\rceil, \lceil x_3\rceil, \lceil x_4\rceil$ be the number of pence, nickels, dimes and quarters then, Assume that we will on average need 2 of each type of coins. The coins will have to sum up to more than $\$0.98$ since otherwise we can't make $\$0.99$ . One way I have done it is using this system, \begin{align*}
x_1+x_2+x_3+x_4 &> 8 && \text{By assumption 1}\\
x_1+5x_2+10x_3+25x_4 &> 98 && \text{By observation 2}\\
x_1+x_2 &> 4 && \text{By assumption 1}\\
x_1+5x_2 &> 6 && \text{By an observation similar to 2}\\
\end{align*} Solving this system does yield, $$
\lceil x_1\rceil, \lceil x_2\rceil, \lceil x_3\rceil, \lceil x_4\rceil
=
4, 1, 2, 3
$$ But I find this proof to be kinda hand-wavy and obvious in its reverse-engineering. Does someone see a better, more elegant algebraic/combinatoric proof here? Maybe something with the binomials or a generating function?","['alternative-proof', 'combinatorics', 'combinatorial-proofs']"
4505909,"Let $E$ be a Banach space, $p \in (1, \infty)$, and $L_p := L_{p}(X, \mu, E)$. Is $(L_p)^* \cong L_{p'}$ where $\frac{1}{p} + \frac{1}{p'} = 1$?","Let $(X, \Sigma, \mu)$ be a $\sigma$ -finite measure space and $(E, |\cdot|)$ a Banach space. Here we use Bochner integral .  The space $L_p := L_{p}(X, \mu, E)$ consists of (equivalence classes of) all Bochner measurable functions $f$ with values in $E$ whose norm $|f|$ lies in the standard $L_{p}(X, \mu, \mathbb R_{\ge 0})$ space. A standard result is that if $E = \mathbb R$ and $p \in (1, \infty)$ then the continuous dual space $(L_p)^*$ of $L_p$ is indeed $L_{p'}$ where $\frac{1}{p} + \frac{1}{p'} = 1$ . One of the proof relies on the continuous linear operator $T:L_{p'} \to (L_p)^*$ defined by $$
\langle T u, f\rangle := \int_X u(x)f(x)\mathrm d \mu(x) \quad \forall u \in L_{p'}, \forall f \in L_{p}. 
$$ Then $T$ is indeed a linear surjective isometry. However, I could not see how to use the same approach for a general Banach space $E$ . Is there such a linear surjective isometry $T$ in case $E$ is a general Banach space?","['banach-spaces', 'lp-spaces', 'functional-analysis', 'dual-spaces', 'bochner-spaces']"
4505924,Probability of two people being on the same team,"There are 12 players that are randomly distributed to 2 teams, so there are 6 players on each team. Question 1 : What is the probability of two specific players being on the same team? Using combinations, the answer is $$2\times\frac{\binom{10}{4}}{\binom{12}{6}}=\frac{5}{11}\approx45\%$$ because you have 12 players to pick from to fill 6 spots for the first team for the denominator, and 10 players to pick from to fill the 4 remaining spots after fixing player 1 and 2 for the numerator. You multiply by 2 because it could happen for either team. I would assume the answer is 50% . Why isn't it? Suppose there are two teams A and B, and two players 1 and 2. The probability of player 1 being on team A is 1/2 , and the probability of player 2 being on team A is also 1/2 . Thus, the odds of player 1 and 2 being on team A is 1/4 and probability of both of them being on team B is also 1/4 . Adding those two up gives 1/2 . Thus, there's a 50% chance of both players being on team A or team B. Question 2 : Why isn't the answer exactly 50% when doing it via combinations? Even intuitively, it doesn't make sense why it's not 50%. The odds of two players being on two different teams is ~ 55%?? Why? I don't understand. Furthermore, as teams get bigger (e.g., 50 players, so 2 teams of 25 players), the odds of 2 players being on the same team becomes 49%. I would assume the odds to stay the same, yet it's approaching 50%. Why?","['probability-theory', 'probability']"
4506007,"Is there any sort of relationship between the terms ""base"" ( number systems) and ""basis"" ( linear algebra) as both have similar definitions.","I was going through the topic of number systems today when I went to see the proper definition of a base. It said ""the number of digits or combination of digits that a system of counting uses to represent numbers."" I remembered seeing the term basis in linear algebra having a similar definition: ""a set B of vectors in a vector space V is called a basis if every element of V may be written in a unique way as a finite linear combination of elements of B."" So, are the two topics related in any way? Or  Are they the same thing? Or are they totally different things?","['soft-question', 'linear-algebra', 'number-systems', 'terminology']"
4506112,How to calculate $\int_0^{2\pi}\frac{\cos(\phi)-R}{1-2R\cos(\phi)+R^2}\cos(n\phi)~d\phi$?,"I wish to calculate $$I(R)=\int_0^{2\pi}\frac{\cos(\phi)-R}{1-2R\cos(\phi)+R^2}~\cos(n\phi)~d\phi,$$ where $n\in\mathbb{N}$ , $R\in[0,1)$ . Based on trial and error from plugging numbers into Wolfram alpha I think the answer is $$I(R)=\begin{cases}
0, & n=0, \\
\pi R^{n-1}, & n\ge1.
\end{cases}$$ However, I don't know how to show this more rigorously. Do you know how to calculate the above integral, and do you know if my formula is correct?","['integration', 'definite-integrals', 'electromagnetism', 'complex-analysis', 'calculus']"
4506118,"Solve $\arcsin x=\arccos x, x\in[-1,1]$","First I solved it like this $\arcsin x=\arccos x \iff \sin(\arcsin x)=\sin(\arccos x)$ implies  for $x\in[-1,1]$ : $x=\pm\sqrt{1-x^2} \implies x^2=1-x^2\iff x=\pm \frac 1{\sqrt 2}$ but $-\frac 1{\sqrt 2}\in[-1,1]$ is not solution, what is the mistake? Did you detect the error by checking? as in irrational equations ? However $\arcsin x=\arccos x \iff \arcsin x=\frac{\pi}2-\arcsin x\iff 2\arcsin x=\frac{\pi}2 \iff \arcsin x=\frac{\pi}4$ implies for $x\in[-1,1]$ : $x=\sin \frac{\pi}4=\frac1{\sqrt 2}$ .","['trigonometry', 'functions', 'inverse-function', 'real-analysis']"
4506151,Finding all functions satisfying $f(x f(x+y))+f(f(y) f(x+y))=(x+y)^{2}$,"Determine all functions $f:\mathbb{R} \to \mathbb{R}$ such that $$f(x f(x+y))+f(f(y) f(x+y))=(x+y)^{2}, \forall x,y \in \mathbb{R} \tag1)$$ My approach:
Let $x=0$ , we get $$f(0)+f\left((f(y))^2\right)=y^2$$ $\Rightarrow$ $$f\left((f(y))^2\right)=y^2-f(0)\tag2 $$ Let us assume $f(0)=k \ne 0$ Put $y=0$ above, we get $$f(k^2)=-k$$ Also put $y=-x$ in $(1)$ , we get $$f(kf(x))+f(kf(-x))=0, \forall x \in \mathbb{R}$$ Put $x=0$ above we get $$f(k^2)=0$$ $\Rightarrow$ $f(k^2)$ has two different images $0,-k$ which contradicts that $f$ is a function. Hence $k=0 \Rightarrow f(0)=0$ .
So from $(2)$ we get: $$f\left((f(y))^2\right)=y^2 \cdots (3)$$ Now put $y=0, x=f(x)$ in $(1)$ , and use the fact $f(0)=0$ ,we get $$f\left((f(x))^2\right)=(f(x))^2$$ Since $x$ is dummy variable, we get $$f\left((f(y))^2\right)=(f(y))^2 \cdots (4)$$ From $(3),(4)$ , we get $$f(x)=\pm x$$ I just want to ask, is my approach fine? If not where is the flaw? Also other approaches are welcomed.","['contest-math', 'functional-equations', 'functions']"
4506209,Is there a general method to find the asymptotic order for this sequence?,"Given $$a_{n+1}=a_n+\frac{n}{a_1+\dots+a_n},\qquad a_1>0$$ The answer is $$\lim_{n\to\infty} a_n\sim\sqrt{3}\cdot\sqrt{n}-\frac{\sqrt{3}}{4}\cdot\frac{1}{\sqrt{n}}$$ It is easy to show this sequence is increasing, and is divergent, because if assume the opposite $A=\lim_{n\to\infty} a_n$ , we will get $A=A+\frac{1}A$ , which gives contradictions. We also have $$a_{n+1}\ge a_n +\frac{1}{a_n}\ge2$$ From here I can show: $$a_1+\frac{n}{a_n}\le a_{n+1}\le a_1+\frac{n}{a_1}$$ Update.(1) This equation can be also written as: $$\begin{align}
a_n&=\sum_{k=1}^n a_k-\sum_{k=1}^{n-1} a_k\\
\\
a_n&=\frac{n}{a_{n+1}-a_n}-\frac{n-1}{a_{n}-a_{n-1}}\\
\\
a_n(a_{n+1}-a_n)(a_{n}-a_{n-1})&=n(a_{n}-a_{n-1})-(n-1)(a_{n+1}-a_n)\tag{1}
\end{align}$$ If assume $a_n=c\cdot n^p$ $$a_{n+1}-a_n\sim cp\cdot n^{p-1},~~~a_{n}-a_{n-1}\sim cp\cdot n^{p-1}$$ Plug into $(1)$ and only keep the leading order: $$c^2p\cdot n^{2p-1}=1$$ So we get $p=\frac{1}2$ and $c=\sqrt{2}$ Why the leading order coefficient is $\sqrt{2}$ , not $\sqrt{3}$ ? The bottom line is, if I take the answer as template, let $$a_n=c\sqrt{n}+t\frac{1}{\sqrt{n}}+o(\frac{1}{\sqrt{n}})$$ Pretend we don't know coefficients $c$ and $t$ . Now we want to compute $c$ and $t$ . To the leading order approximation, We have $$a_{n+1}-a_n= c\cdot \frac{1}{2\sqrt{n}}+O(\frac{1}{n^{3/2}})$$ Next, plug into Eq. $(1)$ and we can solve for $c=\sqrt{2}$ . Why does this give a contradiction? Update.(2) Thank you to @Sangchul Lee , @Somos and @Youem I put the computation part in the answer box below, and it works for asymptotic approximation at arbitrary order.","['recursion', 'recurrence-relations', 'analysis', 'real-analysis', 'sequences-and-series']"
4506254,What's the shortest path to simplify $\tan{85°} \tan{35°} \tan{15°}+\tan{55°} \tan{5°} \tan{75°}+2\cot{50°}$?,"We have this expression: $$\tan{85°} \cdot \tan{35°} \cdot \tan{15°}+\tan{55°} \cdot \tan{5°} \cdot \tan{75°}+2\cot{50°}$$ I want this expression to be in its simplest form.
I fed this into the Symbolab calculator but it gave a very long answer. According to my own information, I wrote it like this: $$\cot{5°} \cdot \cot{55°} \cdot \cot{75°}+\tan{55°} \cdot \tan{5°} \cdot \tan{75°} + 2\cot{50°}=\\ \\ \frac{\cos{5°} \cdot \cos{55°} \cdot \cos{75°}}{\sin{5°} \cdot \sin{55°} \cdot \sin{75°}}+\frac{\sin{5°} \cdot \sin{55°} \cdot \sin{75°}}{\cos{5°} \cdot \cos{55°} \cdot \cos{75°}}+2\frac{\cos{50°}}{\sin{50°}}$$ And now, if: $$a=\cos{5°} \cdot \cos{55°} \cdot \cos{75°}\\b=\sin{5°} \cdot \sin{55°} \cdot \sin{75°}\\c=\cos{50°}\\d=\sin{50°}$$ It will be very long so i show my final way with the variables $a,b,c,d$ : $$\frac{a}{b}+\frac{b}{a}+2\frac{c}{d}=\frac{a^{2}d+b^{2}d+2abd}{abd}$$ but as you see, it is also very long. I am looking for the simplest form for this expression.","['trigonometry', 'algebra-precalculus', 'geometry']"
4506272,"Is there a systematic way of knowing what happens to the set of solutions of a differential equation if you ""differentiate"" the differential equation?","Suppose we know all functions $\phi$ that are solutions of the differential equation $$G(x)=F_1(x,y(x),y'(x),\dots,y^{(n)}(x))=0$$ for some arbitrary function $F_1$ . Clearly those same $\phi$ also satisfy the differential equation $$G'(x)=F_2(x,y(x),y'(x),\dots,y^{(n+1)}(x))=0$$ obtained by ""differentiating the differential equation"". Intuitively, I assume new solutions are generated, but I'm not certain if that is always the case and if there is a systematic way to relate those new solutions to the ones we already have for the original differential equation and to the functional form of $F_1$ and $F_2$ . For example, the solutions to the differential equation $$y'=0$$ are of the form $y(x)=C$ for some constant $C$ . ""Differentiating"" the differential equation, we get $$y''=0$$ which has solutions of the form $y(x)=C_1x+C_2$ for some constants $C_1$ and $C_2$ . The set of solutions of the second differential equation contains the solutions of the first one, as expected, but also contains new solutions, which in this case are antiderivatives of the original solutions. It doesn't seem that things work out always as they do for this simple case, though. Is there a systematic way of knowing what will happen to the solution set of a differential equation if you ""differentiate"" it?","['calculus', 'ordinary-differential-equations']"
4506278,What is the name of the distribution of the maximum number of distinguishable balls in a single distinguishable box?,"Imagine that there are $k$ distinguishable balls and $n$ distinguishable boxes. Each ball is placed into a random box. Next we calculate the maximum number of balls in a single box, which will be a value between $\left \lceil \dfrac{k}{n}\right \rceil$ , when the balls are distributed almost evenly, and $k$ , all balls in one box. Let's call this $X_{k,n}$ What distribution does $X_{k,n}$ follow? I've encountered a situation where $k$ is about $100,000$ and $n$ is about $100$ . How could I estimate the CDF for these particular values? What is $\mathbb{P}(X_{k,n}<2000)$ ?","['statistics', 'balls-in-bins', 'probability']"
4506287,Need help understanding a paper's statement.,"Given $$
\boldsymbol{E}_\ell= \alpha\left(\boldsymbol{I}+\alpha \boldsymbol{Z}_{\ell} \boldsymbol{Z}_{\ell}^{*}\right)^{-1}
$$ where $\boldsymbol{Z}_\ell \in \mathbb{R}^{n\times m}$ and $\boldsymbol{E}_\ell \in \mathbb{R}^{n\times n}$ . A paper claims that this is related to Ridge autoregression residual, and it states: $$
\tag{1}
\boldsymbol{E}_{\ell} \boldsymbol{z}_{\ell}=\alpha\left(\boldsymbol{z}_{\ell}-\boldsymbol{Z}_{\ell}\left[\boldsymbol{q}_{\ell}\right]_{\star}\right)
$$ where $\left[\boldsymbol{q}_{\ell}\right]_{\star} = \underset{\boldsymbol{q}_{\ell}}{\operatorname{argmin}} \alpha\left\|\boldsymbol{z}_{\ell}-\boldsymbol{Z}_{\ell} \boldsymbol{q}_{\boldsymbol{\ell}}\right\|_{2}^{2}+\left\|\boldsymbol{q}_{\ell}\right\|_{2}^{2}$ . I know the formulation of $[\boldsymbol{q}_{\ell}]_{\star}$ ends up with the solution of Ridge regression .
And I know $\boldsymbol{z}_{\ell}-\boldsymbol{Z}_{\ell}\left[\boldsymbol{q}_{\ell}\right]_{\star}$ will then be the regression residual.
However, I cannot see the reason for equation (1) to hold. Any idea?","['regression', 'statistics', 'linear-algebra']"
4506321,Plot of Fourier series involving prime,"So, I was playing with Fourier series just for fun and got a weird idea. I'm sure that someone have think of this series before $f(x) = \displaystyle\sum_{n=1}^{\infty} {{\frac {(-1)^n} {p_n}}\sin(p_nx)}$ Where $p_n$ is the n-th prime number This is the plot involving first 20 prime numbers using desmos. I have 2 questions regarding this series. Is this plot smooth and/or analytics in the limit? Does anyone know how to plot this function? I try using python, but couldn't really figure out how to do it.","['python', 'graphing-functions', 'functional-analysis', 'real-analysis']"
4506330,Different Types of Markov Chains?,"I have been trying to learn more about different types of Markov Chains. So far, here is my basic understanding of them: Discrete Time Markov Chain: Characterized by a constant transition probability matrix ""P"" Continuous Time Markov Chain: Characterized by a time dependent transition probability matrix ""P(t)"" and a constant infinitesimal generator matrix ""Q"". The Continuous Time Markov Chain is based on the Exponential Distribution and thereby must obey the Memoryless Property. Non Homogenous Continuous Time Markov Chain: Characterized by a time dependent transition probability matrix ""P(t)"" and a time dependent infinitesimal generator matrix ""Q(t)"". Non Homogenous Continuous Time Markov Chains are not necessarily based on the Exponential Distribution and thereby do not need to obey the Memoryless Property. This brings me to my question - given the above information, I am having difficulty understanding the difference between these Markov Chains and a Semi-Markov Process. Based on some readings that I have done, it seems like a Semi Markov Process is characterized by its ""Sojourn Time"" - that is, the amount of time that is spent in some state. Since the probability distribution of this ""Sojourn Time"" does not necessarily need to be Exponentially Distributed, a Semi Markov Process does not need to obey the Memoryless Property. If this is the case, then how exactly is the Semi-Markov Process different from a Non Homogenous Continuous Time Markov Chain? Can someone please help me understand this? Perhaps some example could illustrate in which conditions it might be more advantageous to use a Semi-Markov Process compared to a Continuous Markov Process and vice versa? Thanks! Note: I am currently reading this reference : Difference between non-homogeneous Markov and Semi-Markov? Note 1: I understand that as the name suggests, ""Semi-Markov"" does not need to obey the ""Markov Property"" . That is, in Semi-Markov, the future states that the process can go to does not only depend on the current state, and can also depend on the history of the process. This is in contrast to a Continuous Markov Process in which the Markov Property must be obeyed , and the future state of a Continuous Markov Process can only depend on the current state. Note 2: I am told that one of the differences between Semi-Markov and Markov is how the notion of time is recorded. In a Continuous Markov Process, time is recorded in the ""clock forward"" format - this means that all time transitions are recorded from some initial time. In a Semi Markov Process, time is recorded in the ""clock reset"" format - this means that the clock is restarted every time a transition is recorded. Supposedly, ""clock forward"" is not able to take into consideration the history of the Markov Process - whereas ""clock reset"" is able to take into consideration the history of the Markov Process. However, I do not understand this point - I am not sure as to why recording time using different formats ""magically"" allows the Markov Process to consider the history or not to consider the history of the process. Note 3: It seems to me like a Semi Markov Process is closer to a ""Discrete Markov Chain"" : If my understanding is correct, a Semi Markov Process does not have a P(t) matrix or any type of Q matrix - a Semi Markov Process only has a time independent P matrix. Is this correct? Note 4: Again, if my understanding is correct, it seems as though a Semi Markov Process might not be as useful for modelling real world phenomena compared to a Continuous Markov Process (i.e. when modelling a person transitioning between medical condition, it will likely be useful to know the history of how long he has been healthy or sick) - with this being said, in what kinds of situations is it useful to use  Semi Markov Processes?","['markov-process', 'statistics', 'markov-chains', 'probability']"
4506338,DCT in a proof of (vague) portmanteau theorem over locally compact metric spaces,"Let $(X,d)$ be a locally compact metric space and let $\mathscr{B}$ be the Borel sets. Let $\mu_n,\mu:\mathscr{B}\to [0,\infty]$ be regular measures over $(X,\mathscr{B})$ . We say $\mu_n$ converges vaguely  to $\mu$ (or $\mu_n \stackrel{\textrm{v}}{\to}\mu$ ) if $\int_Xud\mu_n\to \int_Xud\mu$ for all compactly supported continuous $u$ (i.e. $u \in C_c(X))$ . As part of a proof of a portmanteau theorem, I read that $$\mu_n(B)\to\mu(B),\,\forall B \in \mathscr{B}\textrm{ relatively compact s.t. }\mu(\partial B)=0\implies\mu_n \stackrel{\textrm{v}}{\to}\mu$$ In the proof of this passage, it is first shown that $\{t>0:\mu(\partial\{u\geq t\})>0\}$ has Lebesgue measure zero. Then, a DCT is applied as following $$\begin{aligned}\lim_{n \to \infty}\int_{(0,\infty)}\mu_n\{u\geq t\}dt&=\lim_{n \to \infty}\int_{(0,\|u\|_\infty)}\mu_n\{u\geq t\}dt\color{red}{\stackrel{(?)}{=}}\\
&=\int_{(0,\|u\|_\infty)}\lim_{n \to \infty}\mu_n\{u\geq t\}dt=\\
&=\int_{(0,\|u\|_\infty)}\mu\{u\geq t\}dt\end{aligned}$$ While the rest seems to me clear, I do not understand the $\color{red}{(?)}$ passage and how DCT has been applied. For example, if $M:=\sup_n\mu_n(X)<\infty$ we would have $$\mu_n\{u\geq t\}\mathbf{1}_{(0,\|u\|_\infty)}(t)\leq M\mathbf{1}_{(0,\|u\|_\infty)}(t)\in L^1(dt)$$ and we could use DCT and take the limit over $\{t>0:\mu(\partial\{u\geq t\})=0\}$ . However, I don't see how DCT is used seemingly without assumptions on $\mu_n$ with the exception of regularity. Is $(X,d)$ being locally compact involved here? Help appreciated.","['measure-theory', 'weak-convergence', 'metric-spaces', 'real-analysis', 'general-topology']"
4506344,Why is the Radon–Nikodym theorem important in probability?,"I was reading this Wikipedia page and came across the following statement: The Radon–Nikodym theorem essentially states that, under certain conditions, any measure ν can be expressed in this way with respect to another measure μ on the same space. The function  f  is then called the Radon–Nikodym derivative and is denoted by {\displaystyle {\tfrac {d\nu }{d\mu }}}{\displaystyle {\tfrac {d\nu }{d\mu }}}. 1 An important application is in probability theory, leading to the probability density function of a random variable. I am trying to understand why this important - i.e., why is the Radon–Nikodym theorem important in defining the probability density function of a random variable? In the courses I have taken in statistics/probability (engineering), we were always shown the definition of a probability distribution (of a random variable) without any mention of the Radon-Nikodym theorem. I would have never even known that such a theorem existed, let alone that such a theorem would be so important in defining the probability density of a random variable. Why is the Radon-Nikodym theorem so important in defining the probability density of a random variable? Is it really that important that it can not be defined without this theorem?","['derivatives', 'probability']"
4506347,Torsion-free affine connection under Geodesic normal coordinates is determined by curvature tensor,"I am reading Chern's Lectures on Differential Geometry. Chern proves the theorem that A torsion-free affine connection is completely determined by the curvature tensor ,i.e. the Theorem 2.3 on page 147 on this page . However, I can not understand (2.20) in that link, I copied what it states below. If we consider a normal coordinate system $\alpha^{i}$ at a fixed point $O$ and also choose a natural frame at $O$ , then we get a frame field $e_{i}$ and its dual $\theta^{i}$ . Let $\theta^{j}_{i}$ be the restriction of the everywhere linearly independent $m^{2}$ differential 1-forms,then we have $$
\left\{
\begin{align}
\theta^{i} &=\overline{\theta}^{i}+\alpha^{i}dt\\
\theta^{j}_{i} &=\overline{\theta}^{j}_{i}
\end{align}\right.
$$ where the $\overline{\theta}^{i}$ and $\overline{\theta}^{j}_{i}$ means they do not contain $dt$ I can not see how this is derived, can anyone help me out?","['geodesic', 'riemannian-geometry', 'coordinate-systems', 'connections', 'differential-geometry']"
4506349,Confused regarding Terence Tao's proof of Tonelli's Theorem,"I am struggling with Tao's proof of Tonelli's Theorem perhaps, admittedly, due to not having properly studied all the previous material regarding product measures in his book on the topic. I'd appreciate any help understanding any of the steps in the proof I am confused by. Theorem 1.7.15 (Tonelli’s theorem, incomplete version). Let $(X, B_X, \mu_X)$ and $(Y, B_Y , \mu_Y )$ be $\sigma$ -finite measure spaces, and let $f : X \times Y \to [0, +\infty]$ be measurable with respect to $B_X \times B_Y$ . Then: (i) The functions $x\mapsto \int_Y f(x,y)d\mu_Y$ and $y\mapsto \int _X f(x,y) d\mu_X$ are measurable with respect to $B_X$ and $B_Y$ respectively. (ii) We have $$\int_{X\times Y} f(x,y) \ d(\mu_X\times\mu_Y) = \int_Y\left(\int_X f(x,y) \ d\mu_X\right)d\mu_Y.$$ Proof: By writing the $\sigma$ -finite space $X$ as an increasing union $X = \cup_{n=1}^{\infty} X_n$ of finite measure sets, we see from several applications of the monotone convergence theorem (Theorem 1.4.44) that it suffices to prove the claims with $X$ replaced by $X_n$ . Thus we may assume without loss of generality that $X$ has finite measure. Similarly we may assume $Y$ has finite measure. Note from (1.36) that this implies that $X × Y$ has finite measure also. Q1: why does it suffice to show the result for $X$ replaced by $X_n$ ? I do not even understand how replacing $X$ with $X_n$ allows us to use the monotone convergence theorem. However, letting $X'_n=\cup_{i=1}^nX_i$ and, $Y'_n=\cup_{i=1}^nY_i$ we get $$f = \lim_{n\to \infty}f\ 1_{X'_n\times Y'n},$$ and the monotone convergence theorem can be applied, yet -if this is what Tao means- how may we use the monotone convergence theorem to conclude the result holds for $f$ given that it holds for any $f\ 1_{X'_n\times Y'n}$ ? Every unsigned measurable function is the increasing limit of unsigned simple functions. By several applications of the monotone convergence theorem (Theorem 1.4.44), we thus see that it suffices to verify the claim when $f$ is a simple function. By linearity, it then suffices to verify the claim when $f$ is an indicator function, thus $f = 1_S$ for some $S \in B_X × B_Y$ . Q2: why is every unsigned measurable function the increasing limit of simple unsigned measurable functions? Let $C$ be the set of all $S \in B_X × B_Y$ for which the claims hold. From the repeated applications of the monotone convergence theorem (Theorem 1.4.44) and the downward monotone convergence theorem (which is available in this finite measure setting) we see that C is a monotone class. Q3: Let $A_1\subseteq A_2 \subseteq \ldots $ all be members of $C$ , let $A'_n = \cup_{i=1}^nA_n$ and $A = \cup_{i=1}^{\infty}A_n$ . Since $$1_A = \lim_{n\to \infty}1_{A'_n}$$ we may apply the monotone convergence theorem to conclude that $C$ is closed under countable monotone unions. However, I do know how to show that $C$ is closed under countable monotone intersections. By direct computation (using (1.36)), we see that $C$ contains as
an element any product $S = E × F$ with $E \in B_X$ and $F \in B_Y$ . By finite additivity, we conclude that $C$ also contains as an element any a disjoint finite union $S = E_1×F_1∪\ldots ∪E_k×F_k$ of such products. This implies that $C$ also contains the Boolean algebra $B_0$ in the proof of Proposition 1.7.11, as such sets can always be expressed as the disjoint finite union of Cartesian products of measurable sets. Applying the monotone class lemma, we conclude that $C$ contains $\langle B_0\rangle = B_X × B_Y$ , and the claim follows. Q4: The algebra $B_0$ is the collection of all finite unions $$E_1\times F_1 \cup \ldots \cup E_n\times F_n$$ of $B_X$ -measurable sets $E_k$ and $B_Y$ -measurable sets $F_k$ . How does one show $B_0$ is closed under complements and finite intersections?","['integration', 'lebesgue-measure', 'lebesgue-integral']"
4506377,Does tangent exist at $x=1$ to $y(x) =\ln\left(\frac{1+\sqrt{1-x^2}}x\right)-\sqrt{1-x^2}$? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question $y(x) =\ln\left(\frac{1+\sqrt{1-x^2}}x\right)-\sqrt{1-x^2}$ is defined on $(0,1]$ . Does there exist it's tangent at the end point $x=1$ ? Left hand derivative of $y(x)$ exists at the point at $x=1$ and it is continuous on $(0,1]$ . But I can not understand if tangent to $y(x)$ at $x=1$ exists.I think any straight line which passes through $(1,0)$ will be a tangent to $x=1$ to $y(x)$ because it just touches at that point. Can anyone please help me ?","['calculus', 'derivatives', 'real-analysis']"
4506443,Limit when both left and right hand sides are undefined?,"I read here that $$\lim_{x \to 0} \sqrt{x} = 0$$ This is because the function $$f(x) = \sqrt{x} =
\begin{cases}
      \sqrt{x} & \text{if}\ x \geq 0 \\
      \text{undefined} & \text{otherwise}
    \end{cases}$$ is undefined for negative values of $x$ , so the limit can't be taken in the first place. But, what if both left and right sides are undefined? Consider $$
g(x) = 
    \begin{cases}
      0 & \text{if}\ x = 0 \\
      \text{undefined} & \text{otherwise}
    \end{cases}
$$ What will be $$\lim_{x \to 0} g(x)$$ now?","['limits', 'calculus', 'epsilon-delta', 'real-analysis']"
4506456,"Fixed a symplectic form, any differential of a regular function is a contraction of the symplectic form","Let $M$ be a smooth manifold over $\mathbb{R}$ and $\omega$ a symplectic form on $M$ , i.e. a regular, non-degenerate, closed 2-form. Given a vector field on $M$ , $X \in \mathcal{V}(M)$ , we define the contraction of $\omega$ with respect to $X$ as $$i_X\omega:=\omega(X,-).$$ The problem I am addressing is the following one For any $f \in \mathcal{O}(M)$ there exists a vector field $X$ such that $i_X\omega = df$ I have two ideas to address this problem. First, fixed $f$ I write explicitly the $1$ -forms $df$ and $i_X\omega$ locally and then I set the equality. This gives me sufficient local conditions to determine $X$ . The second idea (which, if correct, is equivalent to the first one) is the following. Let me write the symplectic form as a skew-symmetric non-degenerate bilinear map $$\omega : \mathcal{V}(M) \times \mathcal{V}(M) \rightarrow \mathcal{O}(M).$$ Then, if we restrict ourselves in a local chart, we have a basis for $\mathcal{V}(M)$ and there exists a matrix $M \in M_{ n \times n}(\mathcal{O}(M))$ such that it expresses $\omega$ with respect to the fixed basis of $\mathcal{V}(M)$ . For the property of $\omega$ I would say that $M$ has an inverse matrix in $M_n(\mathcal{O}(M))$ . This is sufficient to conclude. In fact, in coordinates, by the identity $i_X\omega = df$ , we would have that $$\begin{bmatrix}
X_1, & \dots & X_n \end{bmatrix}=\begin{bmatrix} \partial_{x_1}f, & \dots & ,\partial_{x_n}f \end{bmatrix} \cdot M^{-1}$$ which define $X$ locally. Now remain to prove that these ""local definitions"" can be put together to define a global vector fields. I would prove this last step using just the definition (if is there any shortcut I will be happy to know that). A third way to prove it could be the following one. Since $\omega$ is a symplectic form, it defines a smooth bundle isomorphism $$TM \rightarrow T^*M, \ T_pM \ni (p,\nu) \mapsto \omega_p(\nu,-) \in T_p^*M$$ (This can be seen as a consequence of the Bundle homomorphism characterization Lemma )
So, the thesis is straightforward. I would like to know if this is a correct way to proceed. Thank you","['symplectic-geometry', 'symplectic-linear-algebra', 'differential-forms', 'differential-geometry']"
4506461,"Let $X$ be a metric space. Suppose there exists $r >0$ such that $\overline{B(x,r)}$ is compact for every $x \in X$. Show that $X$ is complete.","Let $X$ be a metric space. Suppose there exists $r >0$ such that $\overline{B(x,r)}$ is compact for every $x \in X$ . Show that $X$ is complete. Let $x_n$ be a Cauchy sequence in $X$ . We want to show that $x_n \to a \in X$ . Since $x_n$ is Cauchy for all $\varepsilon >0$ there exists $k \in \Bbb N$ such that $n,m>k \implies d(x_n,x_m)< \varepsilon$ . Thus for $r$ there exists $n_0$ such that $n,m > n_0 \implies d(x_n,x_m)< r$ . I also have that for all $n$ there exists $r>0$ such that $\overline{B(x_n,r)}$ is compcat, but I cannot draw the conclusion from these facts that $x_n$ would converge?","['general-topology', 'metric-spaces']"
4506463,"Spivak, Ch. 15, Problem 31: Prove that $\sin$ isn't defined implicitly by an algebraic equation. Understanding a step in the proof.","The following is a problem from Ch. 15 of Spivak's Calculus (a) After all the work involved in the definition of $\sin$ , it would be disconcerting to find that $\sin$ is actually a rational
function. Prove that it isn't. (There is a simple property of $\sin$ which a rational function cannot possibly have.). (b) Prove that $\sin$ isn't even defined implicitly by an algebraic
equation. That is, there do not exist rational functions $f_0,...,f_{n-1}$ such that $$(\sin x)^n+f_{n-1}(x)(\sin x)^{n-1}+...+f_0(x)=0,\text{ for all }
> x\tag{1}$$ Hint: Prove that $f_0=0$ , so that $\sin{x}$ can be factored out. The
remaining factor is $0$ except perhaps at multiples of $\pi$ . But this
implies that it is $0$ for all $x$ ( Why? ) You are now set up for a
proof by induction. My question is precisely the one in bold above. It has been asked before here , but the answer there is but a hint that is already contained in the solution manual. Here is how the question arises in the context of a proof of part $(b)$ . $(a)$ A rational function has a finite number of roots (unless it is zero everywhere). $\sin$ has infinite roots $$\sin{k\pi}=0,k\in\mathbb{Z}$$ and $\sin$ is not $0$ everywhere. Thus $\sin$ is not a rational function. $(b)$ $$x=k\pi\implies\sin(k\pi)=0$$ So the entire expression $(1)$ becomes just $f_0(k\pi)=0$ for all $k\in\mathbb{Z}$ . But $f_0$ is a rational function so it must be $0$ , otherwise it'd have infinite roots. Then $(1)$ becomes $$\sin{x}[ (\sin{x})^{n-1}+f_{n-1}(x)(\sin{x})^{n-2}+...+f_1(x) ]=0\tag{2}$$ Now $x\neq k\pi, k\in\mathbb{Z}\implies\sin{x}\neq 0$ , so the expression in brackets must equal zero. So here is the part that my question is about. We have $$(\sin{x})^{n-1}+f_{n-1}(x)(\sin{x})^{n-2}+...+f_1(x)=0\tag{3}$$ Here is what the solution manual says at this point The term in brackets is continuous and $0$ except perhaps at multiples
of $2\pi$ , so it is $0$ everywhere. This doesn't explain why. $(3)$ is true for all $x\neq k\pi$ , but possibly not at $x=k\pi$ . Rational functions can also be undefined at a finite number of points, but I believe this case is ruled out for any points because $(1)$ is true for all $x$ , correct? Ie, even at points $x=k\pi$ , the lefthand expression in $(3)$ must at least be defined for $(2)$ and thus $(1)$ to be true, correct? Therefore, the rational functions in question are continuous everywhere, $\sin$ and powers of $\sin$ are continuous everywhere, and so the expression in $(3)$ is continuous everywhere. But if we assume that the lefthand expression in $(3)$ is anything other than $0$ at $k\pi$ , then it is discontinuous at such points, which is a contradiction. I think the general idea is on the right track, but I don't feel very comfortable with the details of this last part. So, going back to original the problem statement: Why?","['proof-explanation', 'calculus', 'trigonometric-integrals', 'rational-functions', 'derivatives']"
4506544,"Ratio of $\Gamma(n)$ to $\Gamma(n/2)^2, n\to \infty?$","I'm trying to find the limit $$\lim_{n\to \infty}\dfrac{\Gamma(n)}{\Gamma(\frac{n}{2})^2},$$ and also trying to find the precise decay rate for the above, i.e. the function $D(n)$ so that if $G(n):=\frac{\Gamma(n)}{\Gamma(\frac{n}{2})^2}$ $$\lim_{n\to \infty}\frac{G(n)}{D(n)}=1$$ How do we find them? My guess is that the limit will be zero, but not sure.","['improper-integrals', 'asymptotics', 'analysis', 'gamma-function', 'real-analysis']"
4506552,$f(x)=\cos(ax) + \sin(bx)$,"With the restriction $a \ne 0, b \ne 0 $ why $$f(x)=\cos(ax) + \sin(bx)$$ is periodic if and only if $a/b \,  
 \in \mathbb{Q} $ ? I tried to justify, but I don't know how to justify that if $h=f+g$ , being $f$ of periods $p_1=2\pi/a\times m,$ and $g$ of periods $p_2=2\pi/b\times n$ , $m,n\in\mathbb Z\backslash\{0\}$ , then for $h$ to be periodic there must be $m,n$ such that $p_1=p_2$ . I've seen several answers here but none seemed enlightening and direct enough.","['periodic-functions', 'calculus', 'trigonometry']"
4506585,Sublattice of an Atomic Lattice [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Is the sublattice of an atomic lattice atomic? If not, then what additional condition is necessary for a sublattice to be atomic?","['lattice-orders', 'discrete-mathematics']"
4506603,"How many satellites surround a sphere of diameter $x$, where the satellites are $y$ miles apart from each other, and $z$ miles above the sphere?","So I'm not a mathematician but a writer. I'd like to know if there's a formula that could solve the following problem for a sci-fi book I'm writing: How many satellites would it take to surround a sphere of diameter $x$ , where the satellites are $y$ miles apart from each other, and $z$ miles above the sphere? e.g. $x = 8,000$ miles, $y = 1$ mile, $z = 100$ miles.","['geometry', 'packing-problem']"
4506609,How does $a^y = x+\sqrt{x^2+1}$ imply $a^{-y} = -x+ \sqrt{x^2+1}$ and then $x=\sinh(y\ln a)$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I want to understand the steps in an argument given for the following question. Find the inverse of the following function: $$f(x) = \log_a(x+\sqrt{x^2+1})$$ We find: $$\begin{align}
a^y &= \phantom{-}x+\sqrt{x^2+1} \tag1 \\[4pt]
a^{-y} &= -x+ \sqrt{x^2+1} \tag2
\end{align}$$ whence: $$x = \frac{1}{2} (a^y-a^{-y}) = \sinh(y\ln a) \tag3$$ I understand how he converts it to an exponential $(1)$ , but then $(2)$ doesn’t make sense to me, as well as part $(3)$ .","['exponentiation', 'algebra-precalculus']"
4506620,"If $A + tB$ is similar to $A$ for infinitely many values of $t$, where $A$ is diagonalizable, is $B$ necessarily equal to $0$?","I came across a similar question here , which should guarantee that $B$ is nilpotent, but I wonder if knowing that the matrices are similar (not just having the same eigenvalues) is enough to conclude that $B = 0$ . I'm particularly interested in the case where $A$ is diagonalizable, but I would not be surprised if that isn't relevant here. I considered using the Jordan-Chevalley decomposition to show that $B = 0$ since if $A = P(A + tB)P^{-1}$ for $t \neq 0$ , we have $A = A + 0$ as well as $A = PAP^{-1} + tPBP^{-1}$ , where $A$ and $PAP^{-1}$ are diagonalizable and $0$ and $tPBP^{-1}$ are nilpotent. However, I don't know that $A$ and $B$ commute, so this decomposition is not necessarily unique. Any help would be appreciated.","['matrices', 'matrix-equations', 'linear-algebra', 'matrix-decomposition']"
4506706,"Evaluating $\lim_{x \to \infty} \sqrt{x^2 + 2x} - x$. Why doesn't $x^2$ overpower $2x$ when $x\to\infty$, so that the answer is $0$ (instead of $1$)?","Evaluate $$\lim_{x \to \infty} \sqrt{x^2 + 2x} - x$$ Hello! I was solving this problem, and here is my approach: Inside the square root $x^2$ overpowers $2x$ as $x \to \infty$ , so $$\lim_{x \to \infty} \sqrt{x^2 + 2x} - x = \lim_{x \to \infty} \sqrt{x^2} - x = \lim_{x \to \infty} |x| - x = 0$$ which is wrong answer. Actual answer is $1$ . This is the way I have solved so many problems when $x \to \infty$ , especially when the terms are in denominator. For example, consider $$\lim_{x \to \infty} \frac{x^2 + x}{2x^2 + 4x + 10}$$ Now, we ignore the smaller power terms, and answer is $\frac{1}{2}$ . Why can we ignore smaller powers here but not in the original question on top? Sorry if this is a stupid question.","['limits', 'calculus', 'real-analysis']"
4506749,Why is it incorrect to integrate both sides of $\frac{dy}{dx}=y$ to get $y=\frac{y^2}{2}+c$?,"I know that when we have the D.E. $\frac{dy}{dx}=y$ , we divide by $y$ , and after integration we get the answer we all know. Supposedly, integrating both sides of an equation should return a true statement about the function at hand. But, when we integrate directly $$\frac{dy}{dx}=y$$ we get $$y=\frac{y^2}{2}+c$$ which is not true for $y=e^x$ . What is the error in integrating both sides of the equation? When I have $\frac{dy}{y}=dt$ I am also integrating on different variables on each side.","['calculus', 'ordinary-differential-equations']"
4506766,Two (different) infinite-dimensional spheres [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 1 year ago . Improve this question Let $\mathbb{R}^\infty$ be the set of sequences $x = (x_0, x_1, x_2, \ldots)$ with $x_i \in \mathbb{R}$ , let $\{e_n\}$ be the standard basis and let $||x||_2 = \sum\limits_{i=0}^\infty x_i^2$ . We consider a nice subset, known by many as $\ell^2 = \{ x \in \mathbb{R}^\infty : ||x||_2 < \infty \}$ . The norm $||\cdot ||_2$ defines a topology on $\tau_A$ on $\ell^2$ , as well as the unit disk $D_A^\infty = \{ x \in \ell^2 : ||x||_2 \leq 1\}$ and the unit sphere $S_A^\infty = \{ x \in \ell^2 : ||x||_2 = 1\}$ . And so we have obtained the disk and the sphere that make analyst happy. But there is another sect of mathematicians, who define infinite-dimensional spheres in a different way: topologists. Let us consider the ""standard"" sphere $S_B^n \subseteq \mathbb{R}^\infty$ , $S^n_B = \{(x_0, \ldots, x_n, 0, 0, \ldots) \text{ such that }\sum\limits_{i=0}^n x_i^2 = 1 \}$ .
Let $S_B^\infty = \bigcup\limits_{n=0}^\infty S_B^n$ the union of all finite-dimensional spheres.
The disks $D_B^n$ and $D_B^\infty$ are defined in a similar way, substituting $||x||_2 = 1$ with $||x||_2 \leq 1$ . In other words $S_B^\infty$ is the set of all sequences that are eventually null, with norm equal to 1, while $S_A^\infty$ is just the set of all sequences with norm equal to 1. So clearly $S_B^\infty \subset S_A^\infty$ and $D_B^\infty \subset D_A^\infty$ (but they are not equal). But wait, we're talking about sets, what's actually the topology on $S_B^\infty$ ? Following this question , let's use the weak topology, that we will call $\tau_B$ : a set $U\subseteq \mathbb{R}^\infty$ is open iff $U \cap \mathbb{R}^n$ is open in $\mathbb{R}^n$ for every $n$ . Question 1. What is the relation between $\tau_A$ and $\tau_B$ on $S^\infty_A$ ? (or even better, on the larger set $\ell^2$ ?) I'd say that $\tau_A \subset \tau_B$ because if we intersect an open disk $\mathring{D}(0,r) = \{ x \in \ell^2 : ||x||_2 < r\}$ with a finite-dimensional subspace, we get $\mathring{D}(0,r) \cap \mathbb{R}^n = \mathring{D^n}(0,r)$ . However, $\tau_A \neq \tau_B$ , because the convex hull (=finite linear convex combinations) of the points $\{1 e_1, \frac{1}{2} e_2, \frac{1}{3} e_3, \ldots, \frac{1}{n} e_n, \ldots\}$ is an open set in $\tau_B$ but not in $\tau_A$ . But sadly, I can't gain any further insight on how $\tau_A$ and $\tau_B$ behave on $S_A^\infty$ . Question 2. On the vector space $V = \mathbb{R}^\infty$ , is $\tau_B$ (the weak topology in the sense of intersection with finite-dimensional things) the same as the weak topology (in the sense of duals, i.e. $v_n \rightharpoonup v$ iff for every $f\in V'$ , $f(v_n) \to f(v)$ )? Since the 2-norm is continous on $(\ell^2, \tau_A)$ , then $S_A^\infty$ and $D^\infty_A$ are closed in $(\ell^2, \tau_A)$ .  The closure of $D^\infty_B$ in $(\ell^2, \tau_A)$ is precisely $\overline{D}^\infty_B = D^\infty_A$ , because: It is possible to approximate every sequence $x = (x_0, x_1, \ldots), ||x||_2 \leq 1$ with the truncations $(x_0, \ldots, x_n, 0,0, \ldots) \in D^\infty_B$ Every sequence $\{y^{(n)}\} \in D^\infty_B$ that has limit in $\ell^2$ , must have $||y^{(n)}|| \leq 1$ . With a similar argument, we can see that the closure of $S^\infty_B$ in $(\ell, \tau_A)$ is $S^\infty_A$ , although the truncation of $x \in S^\infty_A$ have to be rescaled to have norm 1, so they are in $S^\infty_B$ . Question 3. Is it true that the closures of $D^\infty_B, S^\infty_B$ in $(\ell^2, \tau_B)$ are $D^\infty_A, S^\infty_A$ ? In general, I would like to understand the differences between the spaces $(S^\infty_B, \tau_B)$ and $(S^\infty_A, \tau_A)$ . For example, by the same first question , we know that both $S^\infty_A$ and $S^\infty_B$ are contractible. One way to do it, is to construct explicitly an homotopy between a generic $x = (x_0, x_1, x_2, \ldots)$ and $(1,0,0,\ldots)$ . We do $f_t(x_0, x_1, \ldots) = (1-t)\cdot (x_0, x_1, x_2, \ldots) + t \cdot (0, x_0, x_1, \ldots)$ . Then we use $g_t(0, x_0, x_1, \ldots) = (1-t)(0,x_1,x_2,\ldots)+t(1,0,0,\ldots)$ .
Combining $f / |f| $ and $g/ |g|$ we get the desired homotopy. Observe that if we start with $x \in S^\infty_B$ , the homotopy has always image in $S^\infty(B)$ . For $S^\infty_B$ , another interesting approach uses algebraic topology: We inductively give a cell structure on $S^n_B$ as the union of $S^{n-1}_B$ and two $n$ -cells (half-spheres). The union of all $S^n_B$ gives a cell structure on $S^\infty_B$ . Observe that the weak topology of cell complexes of $S^\infty_B$ is the same as $\tau_B$ : one is the intersection with cells of size $\leq n$ , the other one with $\mathbb{R}^n$ , which are the same. We have that $H_n(S^\infty_B) = H_n (S^{n+1}_B)$ for the theory of cell complexes. But $H_n(S^{n+1}_B) = 0$ (for example, by putting a friendlier cell structure on $S^{n+1}_B$ , as one $0$ -dimensional and one $n+1$ -dimensional cell; the homology does not depend on cell structure). Therefore $H_n(S^\infty_B)=0$ for all $n>0$ . Using that $S^\infty_B$ is simply-connected, by Hurewicz theorem also $\pi_n(S^\infty_B)=0$ for all $n$ . Then by Whitehead theorem , $S^\infty_B$ is contractible. Wow, this is great! So we can deduce contractibleness from algebraic topology. It's cool, but it is also a bit overkill. Indeed, we could have deduced that $H_n(S^\infty_B)$ and $\pi_n(S^\infty_B)$ are all trivial from contractibility. And since also $S_A^\infty$ is contractible, it must have all trivial homology/homotopy groups. However, maybe it is still interesting to study $S^\infty_A$ topologically? For example: Question 4. It is possible to put a cell structure on $(S^\infty_A, \tau_A)$ ? It is possible to do in a way such that $S^\infty_B$ is a subcomplex of $S^\infty_A$ ? (TL;DR)  What are the relations between the spaces $D_A^\infty, D_B^\infty, S_A^\infty, S_B^\infty$ with the topologies $\tau_A, \tau_B$ ? Related questions: 1) Unit sphere in $\mathbb{R}^\infty$ is contractible? , 2) Infinite-Dimensional Unit Ball, Sphere, and Disc Question ,  3) Prove: The weak closure of the unit sphere is the unit ball.","['general-topology', 'functional-analysis', 'algebraic-topology']"
4506780,How to design a periodic function,"I am a colorist in post production and I am working on tools that generally speaking transform colors. For this purpose I am looking to design a function that accommodates the following points. $$(-\pi, 0);\; (-\frac{2\pi}{3}, -1);\; (-\frac{\pi}{3}, -0.5);\; (0, 0);\; (\frac{\pi}{3}; 0.5);\; (\frac{2\pi}{3}, 1); (\pi, 0)$$ The function should be continuous, easy to compute and as precise as possible. From the given points one can see that I am looking for something halfway between a sinus and a sawtooth curve. This is what I found so far: $$f(x)=\sin(x-0.57\sin(x))$$ It is relatively close but not quite as precise as I would want it to be and I don't like the little bumps in the straight part of the curve between $-\frac{2\pi}{3}$ and $\frac{2\pi}{3}$ . Any help is greatly appreciated.","['periodic-functions', 'trigonometry']"
4506829,"show that if $\lim (f(x_1)+\cdots +f(x_n))/n$ exists whenever $\lim (x_1+\cdots +x_n)/n$ exists, then $f$ is continuous","Show that if $f:[0,1]\to \mathbb{R}$ and $$\lim \limits _{n\to \infty}\frac{f(x_1)+\cdots + f(x_n)}{n}$$ exists whenever $$\lim \limits _{n\to \infty}\frac{x_1+\cdots + x_n}{n}$$ exists, where $(x_n)$ is a sequence of real numbers, then $f$ is continuous. It suffices to show that if $x_n\to x$ , then $f(x_n)\to f(x)$ . It might be useful to show this via a contradiction; suppose there is a sequence $x_n$ so that $x_n\to x\in \mathbb{R}$ but $f(x_n)\not \to f(x)$ . We know that $\dfrac{f(x_1)+\cdots +f(x_n)}n$ exists and equals $L$ , say. By definition, there exists $\varepsilon >0$ so that for all $N$ , $\exists n\geq N$ such that $|f(x_n)-f(x)|\geq \varepsilon$ . But I'm not sure how to get a contradiction from here.","['limits', 'calculus', 'sequences-and-series', 'real-analysis']"
4506921,About the definition of group algebra,"In several texts it is standar to define the group algebra $\mathbb{C}[G]$ of a group $G$ as $$
\mathbb{C}[G]:=\left\{ \sum_{x\in G}c_{x}x  \colon c_{x}\in \mathbb{C} \right\}.
$$ Now, these expressions are just notation, and as I understand, formally it is actually a set of functions $f\colon G \to \mathbb{C}$ with finite support. My first question is: why use this finite sum notation instead of thinking of these expressions as what they are (functions)? And for example, in Representation Theory of Finite Groups by Benjamin Steinberg, for a finite group $G$ the group algebra $L(G)$ is defined as the set of functions $f\colon G \to \mathbb{C}$ , this is $\mathbb{C}^{G}$ . Then he talks about the regular representation, and he defines $
\mathbb{C}[G]:=\left\{ \sum_{x\in G}c_{x}x  \colon c_{x}\in \mathbb{C} \right\}
$ , but if $L(G) = \mathbb{C}[G]$ why not use $L(G)$ instead? Or are they different? As far as I know, the text does not make this clarification. I guess what I really want to ask is why denote group algebra functions this way? I have seen that an isomorphism is constructed between $\mathbb{C}[G]$ and $\mathbb{C}^{G}$ ( $G$ finite), but in that sense, these elements would be thought of as different from each other.","['group-rings', 'definition', 'abstract-algebra', 'algebras', 'group-theory']"
4506961,"Showing that $|\cos x|+|\cos 2x|+\cdots+|\cos nx|\geq\lfloor\frac{n}{2}\rfloor$, where $n\neq 2,4,6$","For every natural number $n\neq 2,4,6$ and any real number $x$ , prove the inequality: $$\sum_{k=1}^n|\cos kx|=|\cos x|+\cdots+|\cos nx|\geq\lfloor\frac{n}{2}\rfloor$$ My idea is that since the sum is concave in any interval where no $\cos kx$ changes sign, the minimum can only be obtained on points where some $\cos kx$ changes sign, namely on $x=\frac{p}{2k}\pi$ where $p$ is any integer such that $(p,2k)=1$ and $1\leq k\leq n$ . When $2k\leq n$ , namely the sum can cover $|\cos\frac{\pi}{2k}|+|\cos\frac{2\pi}{2k}|+\cdots+|\cos\frac{2k\pi}{2k}|=\cot\frac{\pi}{4k}$ for $\lfloor\frac{n}{2k}\rfloor$ times, it's easy to verify that the sum is greater than $\lfloor\frac{n}{2}\rfloor$ . But when $2k>n$ , the sum become hard to manage for me. Could anyone help me to complete this, or should I change my original idea? Update: Now I've proved that $|\cos \frac{lp\pi}{2k}|\neq|\cos \frac{mp\pi}{2k}|$ when $ak\leq l<m<(a+1)k$ for some  integer $a$ . So when $2k>n$ the problem can be converted to $$\cos\frac{\pi}{2k}+\cdots+\cos\frac{k\pi}{2k}+\sin\frac{\pi}{2k}+\cdots+\sin\frac{(n-k)\pi}{2k}\\=\frac{1}{2}\cot\frac{\pi}{4k}-\frac{1}{2}+\frac{1}{2}\cot\frac{\pi}{4k}-\frac{\cos(\frac{(n-k)\pi}{2k}+\frac{\pi}{4k})}{2\sin\frac{\pi}{4k}}\geq\lfloor\frac{n}{2}\rfloor$$ which is a pure inequality problem I think. However, I can't solve it; could anyone help me with that?","['trigonometry', 'inequality', 'sequences-and-series']"
4506975,What is the total differential used for?,"I was reading through all of these explanation of what a total differential is and how it looks. But what is it used for? Why would someone want to know a small change dz? The book highlights the similarity with one viariable calculus, where dy=f'(x)dx, this for me is just a way to rearrange dy/dx=f'(x) to integrate and find y=f(x), not to know the small change of dy because it is not used for anything. Am I missing some concept? I know is rather a simple question, but I would appreciate some help","['multivariable-calculus', 'calculus']"
4506983,Lower bound for the distribution of the sum of independent Bernoulli variables ( not i.i.d),"Suppose we have $X_1,\cdots,X_n$ be independent Bernoulli random variables with $n$ odd, $P(X_i=1)=p_i$ and $p_i \in [1/2,1]$ for all $i=1,\cdots,n$ .
I want to prove that: \begin{equation}
P(\sum_{i=1}^n X_i\geq\frac{n+1}{2}) \geq \frac{\sum_{i=1}^np_i}{n}
\end{equation} This translates to the following statement: The probability that the majority of the $X_i$ are $1$ is bigger than or equal to the average of the probabilities. I strongly believe this is true, but I am not sure how a probabilistic proof would go, it seems that a reverse Markov inequality or Jensen's inequality can not give me this result. Any help would be greatly appreciated!","['inequality', 'probability-theory', 'upper-lower-bounds', 'bernoulli-distribution']"
4507061,Does this linear operator tends to zero?,"Let $X$ separable Hilbert space with basis $\{e_n\}$ , $U_N=\{e_1,...e_N \} \subset X$ , $P_N$ the projection operator over $U_N$ . Define $Q_N=I-P_N$ the orthogonal complement ( $I$ identity) and $B \colon X \to X$ compact linear operator. I want to prove that $$\lim_{N \to \infty}||BQ_N||= 0$$ I would proceed in the following way: For every $x \in X$ we have $P_N x \to x$ as $N \to \infty$ . Then $Q_N x \to 0$ so that $BQ_N x \to 0$ . Now since $B$ is compact we have that the set $$\overline{B Q_N \{ x \in X: |x|=1\}}=B Q_N \{ x\in X: |x|=1\} $$ is compact. This should imply that the convergence $BQ_N x \to 0$ is uniform in $\{x \in X, |x|=1 \}$ , i.e. $$\sup_{|x|=1} |B Q_Nx| \to 0$$ which implies my claim. How do I justify the uniform convergence in $\{x \in X, |x|=1 \}$ ?","['hilbert-spaces', 'banach-spaces', 'operator-theory', 'functional-analysis']"
4507066,"Let $a,b \in \mathbb{R}$. If $a$ and $b$ have different signs, and $|a|\ge |b|$, show $|a+b|\le |a|$.","Does anyone know of a way to argue this with more brevity? I have an argument but I think it too verbose. I would be really surprised if there is not a significantly more efficient argument one could make. But I have not been able to construct one for the life me. Context: I was reading page 93 in Foundations of Modern Analysis by Avner Friedman and he mentioned this result in passing. I wanted to verify it for myself, thinking it would be trivial, but apparently no (not for me at least). My attempt: If $|a| \ge |b|$ , then $-|a| \le b \le |a|$ . Consider two cases: (1) $a \geq 0 \geq b $ and (2) $a \leq 0 \leq b$ . Case (1): We can state $2a \geq a \geq a+b$ . We have $|a| = a$ since $a\geq 0$ . So we get $|a| \geq a+b$ . And secondly we can state $a+b\geq b \geq 2b$ . We have $b \geq -|a|$ . We get $a+b\geq -|a|$ . Hence the two together: $-|a| \leq a+b \leq |a| \iff |a+b|\leq |a|$ . Case (2): We can state $2a \leq a \leq a+b$ . And $a \leq 0$ means $|a| = -a$ .
Whence $-|a| \leq a+b$ . Moreover, $a+b\leq b \leq 2b$ , together with $b \leq |a|$ , gives us $a+b \leq |a|$ . Again we get $-|a| \leq a+b \leq |a|$ . $\blacksquare$ This must be able to be shortened, no?
Thanks everyone!","['algebra-precalculus', 'analysis', 'real-analysis']"
4507085,A finite measure space is separable if and only if its induced $L_1$ space is separable,"I'm reading about symmetric difference operator on a $\sigma$ -finite measure space from Wikipedia. Below is my try to prove a statement mentioned in this article. Let $(\Omega,\mathcal{A},\mu)$ be a finite measure space and $(E, |\cdot|)$ a separable Banach space. We define a pseudometric $d_{\mu}(A, B) := \mu(A \triangle B)$ on $\mathcal A$ . Then $d_\mu$ becomes a metric when $\mathcal A$ is considered modulo the equivalence relation $A \sim B \iff \mu(A \triangle B) = 0$ . Let $L_1 := L_{1}(\Omega, \mu, E)$ and $\|\cdot\|_1$ be its canonical norm. Theorem: $d_\mu$ is separable if and only if $\|\cdot\|_1$ is separable. Could you please have a check on my attempt? My attempt: Let $S := S(\Omega, \mu, E)$ be the space of simple functions and $D$ a countable dense subset of $E$ . $\implies$ Assume $d_\mu$ is separable and $\mathcal D$ a countable dense subset of $\mathcal A$ . Because $S$ is dense in $L_1$ . It suffices to show that $S$ is separable. Let $$
S_1 := \left \{ \sum_{i=1}^m a_i 1_{A_i} \,\middle\vert\, m \in\mathbb N^*; a_1, \ldots, a_m \in D; A_1, \ldots,  A_m \in \mathcal D \right \}.
$$ Then $S_1 \subset S$ is countable. Let's prove that $S_1$ is dense $S$ . Fix $\varepsilon >0$ and $f=\sum_{i=1}^m x_i 1_{B_i} \in S$ such that $(x_i)_{i=1}^m \subset E$ is pairwise different and $(B_i)_{i=1}^m \subset \mathcal A$ is pairwise disjoint.
Then $$
\begin{align}
\left \| f - \sum_{i=1}^m a_i 1_{A_i} \right \|_1 &\le \sum_{i=1}^m \left \|  x_i 1_{B_i} -  a_i 1_{A_i} \right \|_1 \\
&\le \sum_{i=1}^m \|(x_i-a_i) 1_{A_i}\|_1 + \|x_i (1_{B_i} - 1_{A_i})\|_1 \\
&= \sum_{i=1}^m |x_i-a_i| \mu(A_i) + |x_i| d_\mu (A_i, B_i) \\
\end{align}
$$ There are $A_i \in \mathcal D$ such that $d_{\mu}(A_i, B_i) \max_j |x_j| \le \frac{\varepsilon}{2m}$ . There are $a_i \in D$ such that $|x_i-a_i| \mu(A_i)< \frac{\varepsilon}{2m}$ . Then $$
\left \| f - \sum_{i=1}^m a_i 1_{A_i} \right \|_1 \le \varepsilon.
$$ $\impliedby$ Assume $\|\cdot\|_1$ is separable. Fix some $a \in E \setminus \{0\}$ . A subset of a separable metric space is separable, so $\{ a1_A \mid A \in \mathcal A\}$ is separable. Let $S_2$ be a countable dense subset of $\{ a1_A \mid A \in \mathcal A\}$ . Let $\mathcal D := \{A \in \mathcal A \mid a1_A \in S_2\}$ . Let's prove that $\mathcal D$ is dense in $\mathcal A$ . Fix $\varepsilon >0$ and $A \in \mathcal A$ . There is $B \in \mathcal D$ such that $$
\|a1_A -a1_B\|_1 = |a| d_\mu(A, B) = |a| \mu(A \triangle B) < \varepsilon |a|.
$$ This means there is $B \in \mathcal D$ such that $\mu(A \triangle B) < \varepsilon$ . This completes the proof.","['banach-spaces', 'measure-theory', 'separable-spaces', 'metric-spaces', 'lp-spaces']"
4507109,Scaled sines equation: $ C \cos(Ax) = \cos(x) $,"$$
C \cos(Ax) = \cos(x)
$$ $C, A, x \in \mathbb{R}$ . Is there a known solution (for $x$ )? If not, how could one approach it? WA struggles . I thought of solving it for $C$ for each $x$ and for some $A_0$ , then for another $A_1$ and seeing if it can generalize. Arrcosining until a full period is tiled seems to be a piecewise nightmare. My goal is its inequality, which can be derived from the equality by solving periodicity, so if the inequality is somehow easier then that's also welcome.",['trigonometry']
4507121,Probability of point being inside triangle formed by 3 other points on boundary of square.,"I was trying this problem which I'll restate here: ""Three points are chosen uniformly at random from the boundary of a square and a fourth point is chosen uniformly at random from the interior. The probability that the 4th point lies in the triangle formed by the other 3 points can be expressed as $\frac{a}{b}$ where $a$ and $b$ are coprime positive integers. What is the value of $a+b$ ?"" I thought I solved this correctly, but my casework probabilities don't agree with the community solutions. Essentially, this problem is asking for the average area of a triangle formed by 3 points on the boundary of a square. To calculate this, I broke it up into cases. Consider the events: $E_1$ : All points on different sides. $E_2$ : 2 points on same side, 1 point on adjacent side $E_3$ : 2 points on same side, 1 point on opposite side $E_4$ : 3 points on same side of square Then, $$\mathbb{E}[\text{Area}]= \sum_i \mathbb{E}[\text{Area}|E_i]P(E_i)$$ After some lengthy calculations I get $\mathbb{E}[\text{Area}|E_i]=\frac{1}{4},\frac{1}{12},\frac{1}{6},0$ for $i=1,2,3,4$ respectively, which agree with the solutions. However, I got $P(E_1)=P(E_3)=P(E_4)=\frac{1}{5}$ and $P(E_2)=\frac{2}{5}$ which doesn't agree with the solutions. I arrived at this by counting the total number of arrangements of points as ${3+(4-1) \choose 3}=20$ by Stars and bars . Then I simply counted that there are 4 ways to get $E_1,E_3,E_4$ , and 8 ways to get $E_2$ . I confirmed this by exhaustively drawing out all 20 possibilities. However, the solutions give $P(E_1)=\frac{3}{8}$ , $P(E_2)=\frac{3}{8}$ , $P(E_3)=\frac{3}{16}$ , and $P(E_4)=\frac{1}{16}$ . As part of this it is stated that there are $4^3$ total arrangements of points. But doesn't this assume that the points are distinguishable? I thought the point of using stars and bars was for arranging indistinguishable items, and points surely seem to count as indistinguishable.","['geometry', 'probability']"
4507137,Minimum distance from the vertices of a quadrilateral [duplicate],"This question already has an answer here : Common meeting point for 3 points to reach 4th point [closed] (1 answer) Closed 1 year ago . Let $A(0, 1)$ , $B(1, 1)$ , $C(1, -1)$ , $D(-1, 0)$ be four points. If $P$ be any other point then $PA+PB+PC+PD\ge d$ find $d$ . I tried solving this question using triangle inequality, but I am not sure about my answer. 1. $$
\left.
\begin{matrix}
PA+PC \geq AC
\\  
PB+PD \geq BD
\end{matrix}
\right \} \implies {d = AC+BD \approx 4.47}
$$ 2. $$
\left.
\begin{matrix}
PA+PB \geq AB
\\  
PB+PC \geq BC
\\
PC+PD \geq CD
\\
PD+PA \geq DA
\end{matrix}
\right \} \implies {d = (1/2) \times (AB+BC+CD+DA) \approx 3.3}
$$ 3. $$
\left.
\begin{matrix}
PA+PB \geq AB
\\  
PB+PC \geq BC
\\
PC+PD \geq CD
\\
PD+PA \geq DA
\\
PA+PC \geq AC
\\
PB+PD \geq BD
\end{matrix}
\right \} \implies {d = (1/3) \times (AB+BC+CD+DA+AC+BD) \approx 3.7}
$$ I am getting different answers using triangle inequality. I think that there must exist some “sure-fire” method for this question. After seeing the proof of Erdős–Mordell Inequality, I feel that the solution to this question might involve reflections. But I am unsure about it. How to reach the final solution to this question?",['geometry']
4507197,Distance covered by a bouncing ball in a rectangle.,"So I was wondering based on a given number of bounces, what is the distance covered by a bouncing ball inside rectangle. The factors I can give are: Angle of starting velocity Starting position (of course inside the rectangle) size of the box Number of bounces I've thought of this in terms of series of lines, in terms of 'unravelling' the square into 2d space so the bounces just become intersections, but with no results. My first question so please tell me if I missed something. Thank you.",['geometry']
4507220,$7$ points and $10$ sides $\overset{?}{\Longrightarrow}$ a quadrilateral,"Show that there exists a quadrilateral in graph $G$ with $7$ vertices and $10$ sides. More generally, show that there exists a quadrilateral in a graph with $n$ vertices and $l$ sides where $$q\ge2,n=q^2+q+1,l\ge\frac12q(q+1)^2+1.$$ The simplest way of tracking quadrilaterals is to count angles. If $$A,B,C\in V;AB,BC\in E$$ then call this structure an angle. Two angles with the same pair of vertices sticking out form a quadrilateral. So we want to show that the number of angles exceeds the number of vertex pairs. Let the degree of vertices of $G$ be $d_i(i\in[1,7])$ . Then the number of angles is $S=\displaystyle\sum_{k=1}^7\mbox C_{d_i}^2$ . If there $\exists i,j$ for which $d_i-d_j\ge2$ , let $$d_i’=d_i-1,d_j’=d_j+1$$ and we have $$\mbox C_{d_i}^2+C_{d_j}^2>C_{d_i’}^2+C_{d_j’}^2\Longrightarrow S>S’.$$ This points out that the minimum of $S$ is reached when $d_1=2,d_2=\cdots=d_7=3$ or any other order. It is $$S_\min=6\times\mbox C_3^2+\mbox C_2^2=19<\mbox C_7^2=21.$$ Very unfortunately we didn’t reach our goal. In conclusion we need a more accurate way of counting quadrilaterals .","['graph-theory', 'combinatorics']"
4507243,Two definitions of Evolute of a curve?,"Recall, when the tangents to a curve $\gamma$ are normal to another curve, the second curve is called an involute of $\gamma.$ In literature, there are two seemingly different dual notions for involutes. Definition $1$ The evolute of a given curve $\gamma$ is another curve to which all the normals of $\gamma$ are tangent. Definition $2$ Given a $\gamma$ , another curve is called an evolute of $\gamma$ if it is an involute of the second. With the second definition, and arc-length parametrization, an internet source shows that its evolute as $$\gamma(s)+\rho(s)N(s)+\rho(s)\cot\left(\displaystyle\int\tau ds+c\right)B(s).$$ But, for a plane curve, the first definition yields the ""locus of all its centers of curvature"" $$\gamma(s)+\rho(s)N(s)$$ as the evolute. This doesn't seems agree with the other for $\tau=0.$ Are these two definitions actually inequivalent? If so, what is the correct terminology? Also, I would like to see a reference discussing these types of constructions in the theory of curves.","['plane-curves', 'curves', 'envelope', 'curvature', 'differential-geometry']"
4507284,Are expressions considered Mathematical objects?,"In linear algebra we learn about the idea of a set of 'polynomials' would this set be equivalent to a normal set such as the set of real numbers? The idea of sets (in my understanding) is that they can contain Mathematical objects (numbers, functions, other sets etc). Do we consider expressions as Mathematical objects? In which case is it more correct to say the following in terms of equality: 'The mathematical object that $x+1$ represents is the same as the mathematical object that $x+2-1$ represents'? Instead of: ' $x+1$ and $x+2-1$ are the same mathematical object' If expressions are objects in their own right? I would generally consider them 'syntactic objects' but the fact we can form sets with them suggests they are Mathematical Objects. Edit: I have learnt that in dealing with Polynomials we are dealing with mappings based on indeterminates, if we can have an expression as part of a set, something like ' $x+1$ is an element of A' is ambiguous, as are we referring to the expression or it's value? Is it possible to put an expression in a set?","['elementary-set-theory', 'algebra-precalculus', 'linear-algebra']"
4507374,"Are a $2\times 2$ matrix in $\text{SL}(2,\mathbb{Z})$ and its transpose conjugate in $\text{GL}(2,\mathbb{Z})$?","I've been studying some math by myself this summer, and have recently been doing some reading about the groups $\text{GL}(2,\mathbb{Z})$ , $\text{SL}(2,\mathbb{Z})$ , etc. I've been trying to get a better grasp of conjugation in these groups, but have been unable to figure out something relatively simple: Are the following two matrices conjugate in $\text{SL}(2,\mathbb{Z})$ ? If not maybe in $\text{GL}(2,\mathbb{Z})$ ? $$\begin{bmatrix}
a & b\\
c & d
\end{bmatrix}, 
\begin{bmatrix}
a & c\\
b & d
\end{bmatrix}\in\text{SL}(\mathbb{Z}[t,t^{-1}])
$$ If so is there a nice conjugating element? Thanks in advance! I don't have the greatest linear algebra background, but am loving group theory, which might be why I am not getting this.","['matrices', 'general-linear-group', 'group-theory', 'linear-algebra']"
4507478,Small doubts in a proof of the Cantor-Bernstein theorem?,"I'm reading Kolmogorov/Fomin's Introductory Real Analysis, here: Questions: I can see that $A \supset A_2 \supset A_4 \dots$ and $A_1 \supset A_3 \supset A_5 \dots$ but it's not clear to me why $A \supset A_1 \supset A_2 \supset A_3 \supset ...$ It's not clear to me why $A-A_1$ is equivalent to $A_2-A_3$ , can you expand a little bit more about this?",['elementary-set-theory']
4507483,Proof of monotonicity of conditional expectation,"I am trying to show monotonicity of conditional expectation. Namely, if $X\leq Y$ almost surely, and $\mathcal{M}$ is some $\sigma$ -algebra then $\mathbb{E}[X|\mathcal{M}]\leq\mathbb{E}[Y|\mathcal{M}]$ . I know that $\forall E \in \mathcal{M}$ , $$\int_E \mathbf{E}[Y|\mathcal{M}]-\mathbf{E}[X|\mathcal{M}]dP \geq 0$$ Thus, consider the set of elements for which $\mathbf{E}[Y|\mathcal{M}]-\mathbf{E}[X|\mathcal{M}] < 0$ denoted $G$ , on this set, $\int_G \mathbf{E}[Y|\mathcal{M}]-\mathbf{E}[X|\mathcal{M}]dP< 0$ which implies that $G\notin \mathcal{M}$ . I would like to say then that $P(G)=0$ , but since $G$ is not in the $\sigma$ -algebra I don't even know if it is measurable. What am I missing here? How do I know that this set has zero measure?","['measure-theory', 'probability-distributions', 'conditional-expectation', 'probability-theory', 'probability']"
4507506,How did Ramanujan came up with this?,"The following is a picture of equation from Ramanujan's lost notebook. In this page, Ramanujan gives a closed form for, $$\sum_{n\geq 1}\sigma_{s}(n)x^{n}$$ In an attempt initially it's claimed that, $$\pi+2\pi\sum_{n\geq 1}e^{-2n\pi x}=\frac{1}{x}+\frac{1}{x+i}+\frac{1}{x-i}+\frac{1}{x+2i}+\frac{1}{x-2i}+
\cdots$$ I am not sure how to prove this. Moreover After EQ. $(9.2.2)$ , he also claims the following fact, $$\sum_{n\geq 1}\sigma_{s-1}(n)e^{-2\pi nx}=\sum_{n\geq 1}\left(1^{s-1}e^{-2\pi nx}+2^{s-1}e^{-4\pi nx}+\cdots\right)$$ I am also unaware how to prove this? All of my attempts were flawed and were not bearing something new, but rather bringing me where I started from. (Hence I am not mentioning them here).","['number-theory', 'divisor-counting-function', 'divisor-sum']"
4507510,"A $\sigma$-algebra is complete w.r.t. the metric $d_{\mu}(A, B) := \mu(A \triangle B)$","I'm trying to prove below statement that appears in this question . Let $(\Omega,\mathcal{A},\mu)$ be a finite measure space. We define a pseudometric $d_\mu:\mathcal A \times \mathcal A \to [0, \infty)$ by $d_{\mu}(A, B) := \mu(A \triangle B)$ . Then $d_\mu$ becomes a metric when $\mathcal A$ is considered modulo the equivalence relation $\sim$ defined by $$
A \sim B \iff \mu(A \triangle B) = 0 \quad \forall A,B \in \mathcal A.
$$ Theorem: $(\mathcal A, d_\mu)$ is a complete metric space. Could you have a check on my attempt? Is there other approach that do not require an excursion through $L_1$ space? My attempt: Let $L_1 :=L_1(\Omega, \mu, \mathbb R)$ . Consider the map $T:\mathcal A \to L_1, A \mapsto 1_A$ . It follows from $1_{A \triangle B} = |1_A-1_B|$ that $T$ is an isometry. Because $L_1$ is complete , it suffices to show that $T(\mathcal A)$ is closed in $L_1$ . Let $(1_{A_n}) \subset T(\mathcal A)$ and $f \in L_1$ such that $\|1_{A_n} -f\|_1 \to 0$ . There is a subsequence $\varphi$ of $\mathbb N$ such that $1_{A_{\varphi (n)}} \to f$ $\mu$ -a.e. Because $\operatorname{im} (1_{A_{\varphi (n)}}) \subset \{0, 1\}$ , we have $f(x) \in \{0, 1\}$ for $\mu$ -a.e. $x \in \Omega$ . Let $A := f^{-1} (1)$ . Then $f = 1_A$ $\mu$ -a.e. It follows that $\| 1_{A_{\varphi (n)}} - 1_A\|_1 \to 0$ . Because convergent sequence is Cauchy, we get $\| 1_{A_{n}} - 1_A\|_1 \to 0$ . This completes the proof.","['measure-theory', 'complete-spaces', 'metric-spaces', 'alternative-proof', 'lp-spaces']"
4507523,Certain Galois extension over $\mathbb{Q}$ not contain $\sqrt[4]{2}$,"I want to show that $\sqrt[4]{2}$ is not contained in any field $K$ that is Galois over $\mathbb{Q}$ with $G(K/\mathbb{Q}) \cong S_n$ , for any positive integer $n$ . The statement is obvisouly true when $n = 1,2,3$ . Indeed, if $n \leqslant 3$ and $\sqrt[4]{2} \in K$ , then $x^4 - 2$ splits completely in $K$ . Hence, $K$ contains $\mathbb{Q}(\sqrt[4]{2},i)$ , which implies $8 \mid n!$ , a contradiction. However, I have no idea how to prove this for $n \geqslant 4$ . I also know that the condition $G(K/\mathbb{Q}) \cong S_n$ implies $K$ is the splitting field of some irreducible polynomial $f$ over $Q$ . But I don't know whether this fact is useful here.","['galois-theory', 'abstract-algebra']"
4507531,How to use induction in this sense?,"For context, I've learned induction before, in a different way than this author mentions, but I believe I understand it's a rephrasing of it. However, I have no idea how to apply it to these two problems in particular, here is the definition of induction, I attempted this problem first, but my technique is not correct Proof : Let $E=\{n\in\mathbb{N}\mid (x_1+x_2+\cdots +x_{n-1})+x_n=x_1+x_2+\cdots+x_n\}$ then we want to show by induction that $E=\mathbb{N}$ . Consider that $E\subset \mathbb{N}$ and that $x_1=x_1$ so $1\in E$ . Now, assume that $k\in E$ then \begin{align*}
        (x_1+\cdots +x_{k-1})+x_k=x_1+\cdots+x_k
    \end{align*} furthermore consider the sum \begin{align*}
        (x_1+\cdots + x_k)+x_{k+1}
    \end{align*} then by associativity of the reals, we can achieve \begin{align*}
        (x_1+\cdots+x_{k-1})+(x_{k}+x_{k+1})
        =x_1+\cdots+x_k+x_{k+1}
    \end{align*} by the inductive hypothesis since $x_k+x_{k+1}$ is real therefore $E=\mathbb{N}$ and the sum $x_1+\cdots +x_n$ is defined independently of the insertion of parentheses. However, this seems circular in some way; any suggestions on using this definition correctly and/or proving this statement by induction? Also, I think I missed a couple of cases regarding the bracket movements.","['induction', 'analysis', 'real-analysis']"
4507532,Proving without using the given condition,"Let $f:\;\mathbb R\longmapsto\;\mathbb R$ , $A,B\subset\mathbb R$ . Suppose we have $f(A)\subseteq B$ , then prove (1) $f^{-1}(\overline{A})=\overline{f^{-1}(A)}$ ; (2) $f^{-1}(A \cup B)=f^{-1}(A) \cup f^{-1}(B)$ ; (3) $f^{-1}(A \cap B)=f^{-1}(A) \cap f^{-1}(B)$ . By 357725 , 291777 and 228711 ,  one can  prove the above 3 formulas as follows (1) $$
\begin{aligned}
f^{-1}(\overline{A}) &:=\{x \in \operatorname{dom}(f): f(x) \in \overline A \} \\
&=\{x \in \operatorname{dom}(f): f(x) \notin A \} \\
&=: \overline{f^{-1}(A)}.
\end{aligned}
$$ (2) $$
\begin{aligned}
f^{-1}(A \cup B) &:=\{x \in \operatorname{dom}(f): f(x) \in A \cup B\} \\
&=\{x \in \operatorname{dom}(f): f(x) \in A \text { or } f(x) \in B\} \\
&=\{x \in \operatorname{dom}(f): f(x) \in A\} \cup\{x \in \operatorname{dom}(f): f(x) \in B\} \\
&=: f^{-1}(A) \cup f^{-1}(B) .
\end{aligned}
$$ (3) $$
\begin{aligned}
f^{-1}(A \cap B) &:=\{x \in \operatorname{dom}(f): f(x) \in A \cap B\} \\
&=\{x \in \operatorname{dom}(f): f(x) \in A \text { and } f(x) \in B\} \\
&=\{x \in \operatorname{dom}(f): f(x) \in A\} \cap\{x \in \operatorname{dom}(f): f(x) \in B\} \\
&=: f^{-1}(A) \cap f^{-1}(B).
\end{aligned}
$$ It seems that I prove the processes without using the condition Suppose we have $f(A)\subseteq B$ . Is this condition unnecessary?  Or am I missing something?  A counterexample?","['elementary-set-theory', 'functions']"
4507565,Velocity with respect to position [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question We are given an expression for an object's position with respect to time. To find its velocity at $t=0,$ can we put $t=0$ into the position then divide by zero? If not, why?","['kinematics', 'derivatives']"
4507583,Find $f^{(n)}(1)$ on $f(x)=(1+\sqrt{x})^{2n+2}$,Find $f^{(n)}(1)$ on $f(x)=(1+\sqrt{x})^{2n+2}$ . Here is a solution by someone: \begin{align*} f(x)&=(1+\sqrt{x})^{2n+2}=\sum_{k=0}^{2n+2}\binom{2n+2}{k}x^{\frac{k}{2}}\\ &=\sum_{k=0}^{2n+2}\binom{2n+2}{k}\sum_{j=0}^{\infty}\binom{\frac{k}{2}}{j}(x-1)^j\\ &=\sum_{j=0}^{\infty}\sum_{k=0}^{2n+2}\binom{2n+2}{k}\binom{\frac{k}{2}}{j}(x-1)^j. \end{align*} Hence \begin{align*} f^{(n)}(1)&=n!\sum_{k=0}^{2n+2}\binom{2n+2}{k}\binom{\frac{k}{2}}{n}=n!\cdot4(n+1)^2. \end{align*} Is it correct? How to compute $$n!\sum_{k=0}^{2n+2}\binom{2n+2}{k}\binom{\frac{k}{2}}{n}=n!\cdot4(n+1)^2?$$,"['calculus', 'solution-verification', 'derivatives', 'real-analysis']"
4507596,"Conjugacy in $\text{GL}(2, \mathbb{Z})$ vs $\text{GL}(2, \mathbb{Q})$","When are two elements $x,y\in\text{GL}(2, \mathbb{Z})$ conjugate in $\text{GL}(2, \mathbb{Q})$ , but not in $\text{GL}(2, \mathbb{Z})$ ? Does this ever happen? I feel that it should sometimes be the case, but cannot come up with any concrete examples. EDIT: so it is possible; but is it possible if we require the conjugating element in $\text{GL}(2, \mathbb{Q})$ to have determinant $\pm 1$ ?","['general-linear-group', 'group-theory', 'linear-algebra']"
4507603,Isomorphism between two quotient ring [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I am self studying Artin's algebra and stuck with this exercise which asks 'Are $\mathbb{Z}[x] /\langle x^2 +7 \rangle$ and $\mathbb{Z}[x] /\langle 2x^2 + 7\rangle$ isomorphic ?' My sense tells answer is no but to
show non-isomorphism , we should execute one property that one ring has and other hasn't ... I couldn't figure out the property... Thanks in advance for any help/ hints ..","['ring-isomorphism', 'ring-theory', 'abstract-algebra', 'polynomial-rings', 'ring-homomorphism']"
4507631,Maximize the distance between a w shaped line construction and two points,"I am trying to hang string lights on my rectangular porch in a W-shaped fashion. There are two ceiling fans interrupting the pattern and I need to maximize the distance between the ceiling fan blades and the string lights. The perpendicular distance from the center of each ceiling fan to each line should be equal. I tried to use cad to solve this and I am close but I'd like to get an exact solution if possible. I believe one way to go about it is to use the following Theorem and solve that set of equations along with the equations of the lines. There might also be a derivative involved since we are maximizing something. I began using the theorem and things got pretty nasty right away. I am hoping someone might have a more elegant solution. The unknown variables are x_1, x_2 and x_3.","['algebra-precalculus', 'calculus', 'geometry']"
4507675,Number of ways to send out $k$ distinct postcards to $n$ friends if we require each friends receives at least $1$ post card.,"This problem comes from the Lovasz Combinatorial Problems and Exercises book. I'm left rather confused for the second half of the second problem. It asks:  We have $k$ distinct postcards and want to send them all to our $n$ friends (a
friend can get any number of postcards, including 0). How many ways can this
be done? What happens if we want to send at least one card to each friend? The first half is rather simple, however, the second half has me puzzled. This is equivalent to counting the number of surjections yes? However, the answer in the back of the book is $\binom{k}{n}*n!$ . My reasoning gives me that the number of ways to do this would be: $$n^k-\sum_{i=1}^{n}\binom{n}{i}(n-i)^{k}$$ (something like that I'm typing in a bit of a rush so forgive a mistake if there is one). But my reasoning is that you count the number of ways to send out the postcards in any manner, and subtract out the ones where you miss some group of $i$ friends. I can't see my error or how my answer is related to the books.","['inclusion-exclusion', 'combinatorics', 'balls-in-bins', 'discrete-mathematics', 'stirling-numbers']"
4507702,"Solving $x+y+z=4$, $x^2+y^2+z^2=14$, $x^3+y^3+z^3=34$","Solve the system $$\begin{equation} \label{equation1}
\begin{split}
x+y+z=4 \\
     x^2+y^2+z^2=14 \\
x^3+y^3+z^3=34
\end{split}
\end{equation}$$ My work: I found out that $$xy+yz+xz=1$$ and $$x^2y+x^2z+y^2x+y^2z+z^2x+z^2y=22$$ After this I'm stuck. Any help is greatly appreciated. EDIT This not a duplicate. I'm looking for a detailed solution and not a solution just by inspection. Also I thought of a new idea. Maybe e should consider a cubic polynomial whose roots are $x,y,z$","['algebra-precalculus', 'systems-of-equations', 'linear-algebra', 'real-analysis']"
4507721,"Bound this integral: $\int_{B_x\cap(\mathbb R^d\backslash B_0)}\frac{1}{|x-y|^{d+a}}dy\leq \frac{C}{\operatorname{dist}(x,\partial B_0)}$","For $x\in\mathbb R^d$ , let $B_x$ be the open ball with radius 1 with center $x$ . For all $x\in B_0$ and all $y\in \mathbb R^d\backslash B_0$ , we have $$\operatorname{dist}(x,\partial B_0)\leq|x-y|.$$ where $\operatorname{dist}(x,\partial B_0)$ denotes the distance from from $x$ to $\partial B_0$ . Show that for all $a<1$ there exists $C\geq 0$ with $$I:=\int\limits_{B_x\cap(\mathbb R^d\backslash B_0)}\frac{1}{|x-y|^{d+a}}dy\leq \frac{C}{\operatorname{dist}(x,\partial B_0)}$$ for all $x\in B_0$ . I do not know how to show this - I started with $$I\leq \int\limits_{B_x\cap(\mathbb R^d\backslash B_0)}\frac{1}{\operatorname{dist}(x,\partial B_0)^{d+a}}dy$$ but I don't know how to continue.","['integration', 'inequality', 'lebesgue-integral']"
4507749,Are there conjectured closed forms for factorial rational zeta series?,"Context At the bottom of this page from Wolfram Mathworld on the Riemann zeta function , the following constants are defined in equations (133), (137), and (141), respectively: \begin{align*} C_{1} &:= \sum_{n=2}^{\infty} \frac{\zeta(n)}{n!} \\ 
& \approx 1.078188729575818482758265436769832381707219, \\\qquad C_{2} &:= \sum_{n=1}^{\infty} \frac{\zeta(2n)}{n!} \\ 
& \approx 2.407446554790328514709486656223022725582266, \text{ and} \\  C_{3} &:= \sum_{n=1}^{\infty} \frac{\zeta(2n)}{(2n)!} \\
&\approx 0.869001991962908998811054805561395688892494.\end{align*} It is also stated that ""These sums have no known closed-form expression"". However, it could be the case that there are conjectured closed forms for these constants that correspond to the actual values with a high degree of accuracy. Question : Are there any of such conjectured closed forms for $C_{1}$ , $C_{2}$ , and $C_{3}$ ?","['riemann-zeta', 'conjectures', 'sequences-and-series']"
4507755,Determine the group structure from its character table: a group of order $24$ as an example,"Let $G$ be a finite group and the following is its character table (of irreducible $\mathbb{C}$ -representations): $$
\begin{matrix} 
&g_1=1&g_2&g_3&g_4&g_5&g_6&g_7\\
\hline
\chi_1 &1&1&1&1&1&1&1\\
\chi_2 &1&1&1&\omega^2&\omega&\omega^2&\omega \\
\chi_3 &1&1&1&\omega&\omega^2&\omega&\omega^2 \\
\chi_4 &2&-2&0&-1&-1&1&1\\
\chi_5 &2&-2&0&-\omega^2&-\omega&\omega^2&\omega\\
\chi_6 &2&-2&0&-\omega&-\omega^2&\omega&\omega^2\\
\chi_7 &3&3&-1&0&0&0&0\\
\end{matrix}$$ My question is how to prove: $G$ is the semi-direct product of its Sylow 2-subgroup and its Sylow 3-subgroup. My knowledge on this group : The order of $G$ : $24$ . [By the square sum of the first column] Number of elements $m_i$ in each conjugacy class $\mathcal{C}_{g_i}$ with representative $g_i$ : $(m_i)=(1,1,6,4,4,4,4)$ . Kernel of each irreducible repn $\pi_i$ with character $\chi_i$ : $$
\ker \pi_1=G, \,  \ker \pi_2=\ker \pi_3 = \mathcal{C}_{g_1} \cup \mathcal{C}_{g_2} \cup \mathcal{C}_{g_3}, \,  \ker \pi_7 = \mathcal{C}_{g_1} \cup \mathcal{C}_{g_2}
$$ and the remaining representations are faithful (i.e. with trivial kernel). Normal subgroups: $\{1\}, \mathcal{C}_{g_1} \cup \mathcal{C}_{g_2}, \mathcal{C}_{g_1} \cup \mathcal{C}_{g_2} \cup \mathcal{C}_{g_3}, G$ . They are of order $1,2,8,24$ respectively. Commutator subgroup: $[G,G] = \mathcal{C}_{g_1} \cup \mathcal{C}_{g_2} \cup \mathcal{C}_{g_3}$ of order $8$ . Center: $Z(G)= \mathcal{C}_{g_1} \cup \mathcal{C}_{g_2}$ of order $2$ . Sylow $2$ -subgroup (of order $8$ ): there is already a normal subgroup $[G,G]$ of order $8$ . So this is the unique Sylow $2$ -subgroup of $G$ . Call it $P$ . Sylow $3$ -subgroups (of order $3$ ): since there is no normal subgroup of order $3$ , there are more than one Sylow $3$ -subgroup. By Sylow theorem, there are four Sylow $3$ -subgroups, which are all isomorphic to $C_3$ , the cyclic group of order $3$ . Call them $Q_1, Q_2, Q_3, Q_4$ . BUT I got stuck here to go any further to the show $G = P \rtimes Q_i$ for some $i$ . Thank you all for your help!","['characters', 'representation-theory', 'semidirect-product', 'sylow-theory', 'group-theory']"
4507788,Evaluate $\lim_{n \to \infty} \frac{a_n}{2 ^ {n - 1}}$ if $a_n = a_{n - 1} + \sqrt{a_{n - 1}^2 + 1}$,"Let $a_i (i \in \mathbb{N}_{0})$ be a sequence of real numbers such that $a_0 = 0$ and $$a_n = a_{n - 1} + \sqrt{a_{n - 1}^2 + 1} \text{ } \forall n \geq 1$$ Evaluate the limit $$\lim_{n \to \infty} \frac{a_n}{2 ^ {n - 1}}$$ Hello, I am trying to solve this problem. I honestly have no idea how to approach this, but I think the answer will be $1$ because as $n \to \infty$ , $a_n$ gets bigger and $a_n \approx 2a_{n-1}$ and since $a_1 = 1 = 2^{0}$ , the limit will approach $1$ . (Of course, this is just a guess). The intended solution is too much magic. Substitute $a_n = \cot \theta$ . Now $$a_{n + 1} = a_n + \sqrt{a_{n}^2 + 1} = \cot \theta + \csc \theta$$ $$=\frac{\cos \theta + 1}{\sin \theta}$$ $$=\frac{2 \cos^2{\frac{\theta}{2}}}{2 \sin \frac{\theta}{2} \cos \frac{\theta}{2}}$$ $$=\cot \frac{\theta}{2}$$ And now, we can solve the limit and answer is $\frac{4}{\pi}$ . But, is there a more normal method to solving this (other than just thinking out of nowhere that substituting $a_n = \cot \theta$ is helpful)? Please note that my question is about finding an alternative solution that's much more ""thinkable"". So, this is not a duplicate. Thanks","['limits', 'calculus', 'sequences-and-series']"
4507799,"If $|ax^2+bx+c|\le100$ for all $|x|\le 1$, What is the maxima for $|a|+|b|+|c|$","Here is a similar problem posted before. I try to use this method to solve this problem. $$f(-1)=a-b+c, f(0)=c, f(1)=a+b+c$$ So we have $|c|=|f(0)|\le 100$ $$|2a|=|f(-1)-2f(0)+f(1)|\le|f(-1)|+2|f(0)|+|f(1)|\le400$$ So we have $|a|\le200$ But how to find an upper bound for $b$ ? Due to the symmetry, I make a guess for the maxima of $|a|+|b|+|c|$ occurring when $b=0$ , then we have $$y=200x^2-100~~~\text{or}~~~y=-200x^2+100$$ But is there a rigorous way to prove it?","['calculus', 'algebra-precalculus']"
4507815,Minimum value of $\sum_{n=0}^\infty\frac{\cos nx}{3^n}$?,"This question appeared on my test: If the range of the expression $$\sum_{n=0}^\infty\frac{\cos nx}{3^n}$$ is $[a,b]$ , then $(b-a)$ equals ... and I am unable to find the lower limit. I found the maximum value with the help of the infinite geometric series: since $$\cos nx\leqslant1$$ then $$\cos nx/3^n\leqslant 1/3^n$$ so the sum of the series is less than or equal $$1+1/3+1/3^2+\cdots$$ which is $$1/(1-1/3) = 3/2$$ How do I find the minimum value?","['maxima-minima', 'calculus', 'trigonometry']"
4507829,Find solution for $\frac{dx}{dt}=|x|$,My solution: $x>0$ $$\begin{align}\frac{\mathrm dx}{\mathrm dt}=x &\Rightarrow \frac{\mathrm dx}{x}=\mathrm dt \\ &\Rightarrow \int \frac{\mathrm dx}{x} = \int\mathrm dt \\ &\Rightarrow \ln(x)= t + C_{0} \\ &\Rightarrow e^{\ln(x)}=e^{t+C_0} \\ &\Rightarrow x=C_1e^t.\end{align}$$ $x<0$ $$\begin{align} \frac{\mathrm dx}{\mathrm dt}=-x &\Rightarrow -\frac{\mathrm dx}{x}=\mathrm dt \\ &\Rightarrow -\int \frac{\mathrm dx}{x} = \int\mathrm dt \\ &\Rightarrow \ln(x)= -t - C_{2} \\ &\Rightarrow e^{\ln x} =e^{-t-C_2} \\ &\Rightarrow x=C_3e^{-t}. \end{align}$$ $x=0$ $$\begin{align}\frac{\mathrm dx}{\mathrm dt}=0 &\Rightarrow \int \frac{\mathrm dx}{\mathrm dt} = \int 0 \\ &\Rightarrow \int\mathrm dx= \int 0\mathrm dt \\ &\Rightarrow x=C_4.\end{align}$$ I think that 1. and 2. are ok but I'm not sure 3. is correct.,['ordinary-differential-equations']
4507840,Finding expectation with given PDF of $f(x)=2xe^{-x^2}$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question A random variable has PDF given by $f(x)=2xe^{-x^2}$ . Derive the expectation of $x$ . Everytime I integrate this for expectation I am getting nowhere. Any help would be greatly appreciated.","['integration', 'statistics', 'expected-value', 'probability']"
4507844,Simple modules of quantum planes,"Let $k$ be an algebraically closed field. Let $R := k\langle x,y \rangle/(yx-qxy) (q \in k^*)$ . We often call $R$ a quantum plane. If $q$ is a primitive $n$ -th root, then for any $(\zeta, \xi) \in k^* \times k^*$ we can construct a simple right $R$ -module $V$ such that it has $k$ -basis $v_i (i \in \mathbb{Z}_n)$ and $$ v_i.x = \zeta q^i v_i, \quad v_i.y = \xi v_{i+1}
$$ as its module structure. In Lemma 3.3 in page 203 of this note , it is shown that the annihilator of $V$ is $(x^n-\zeta^n,y^n-\xi^n)$ and the ideal is maximal. It seems that we have $R/(x^n-\zeta^n,y^n-\xi^n) \simeq V$ if we assume the lemma because $(x^n-\zeta^n,y^n-\xi^n)$ is maximal.
In detail, we have a surjection $R/(x^n-\zeta^n,y^n-\xi^n) \rightarrow V$ . Then, the kernel seems to be the zero module. Of course, this argument contradicts the above. Question 1. Where is the error in the above argument ? Moreover, there is another description of simple modules of quantum planes in Lemma 4.19 of this paper .
There, it is shown that $V \simeq R/(x^n-\zeta^n,\xi x- \zeta y)$ implicitly.
Note that isomorphisms between $V$ and $R/(x^n-\zeta^n,\xi x- \zeta y)$ are not constructed in the paper.
(There, the fat points of quantum planes are classified.) Question 2. How do we construct an explicit isomorphism between them ? Any comments are welcome.
Thank you. Edit : We have the same question on MO . I would appreciate it if you could see it too.","['noncommutative-geometry', 'algebraic-geometry', 'ring-theory', 'abstract-algebra', 'noncommutative-algebra']"
4507855,"For what values ​of $k$, $f(x)=\lvert x^{2}+(k-1) \lvert x \rvert -k \rvert$ is non-differentiable at five points?","I arrived at the following answer by drawing graphs with a calculator: $$k<0 \quad \& \quad k≠-1$$ But I was hoping that maybe it would be possible to find the answer by using an algebraic solution and not by drawing diagrams. A tip that I used: Using the fact that to find points that are not differentiable functions that have absolute value the point where the branches of the absolute value are connected usually do not have a derivative. As I said, I'm looking for an algebraic solution.","['calculus', 'functions', 'derivatives']"
4507859,How do you prove the mean is the best estimator?,"sorry if this wastes your time but I am not a mathematician. I do like numbers though. After reading a problem where you were shown a set of X numbers, and asked how would you predict the next one, I learnt about the average. By plotting I realize that the average is a number that will always be between the max and the min but also that is the number whose differences to all the sample will by smallest possible (at least in the case of picking up a single number). I tried to write some math representing this but I have no real idea how to prove it. I imagine that this could be proved by contradiction: any other number gives rise to a larger difference. But I couldnt get anywhere. Would you give me some hints, or pointers as to what kind of ideas do you need to prove it?","['average', 'statistics', 'parameter-estimation', 'means']"
4507872,Measuring the conformality or equal-areaness of a projection from the sphere to some surface embedded in $R^3$,"Suppose I want to map from the sphere to some other surface that is embedded into $\Bbb R^3$ , which we will treat as an embedding into $\Bbb R^3$ . In this situation, suppose I already have some formula to map from latitudes and longitudes, denoted $(\lambda, \phi)$ to $(x, y, z)$ coordinates in $\Bbb R^3$ . I would like to see if this map is equal area, conformal, etc, and in general just evaluate what the metric looks like at some arbitrary point. I haven't studied this stuff formally, but I would guess that the method would be something like this. Given that I have some function $f: (\lambda, \phi) \mapsto (x, y, z)$ , I can get the Jacobian matrix of partial derivatives. This will give two vectors which span a plane tangent to the surface at that point. Then I would guess that, for instance, we can: See if these vectors are orthogonal and have the same norm. If this is true everywhere, then the map is conformal. Compute the area of the parallelogram formed by the two vectors, and see if it has the same area everywhere. If so, the map is equal area. However I don't think this is exactly correct, because if we differentiate with respect to the coordinate basis of latitudes and longitudes on the sphere, what we're really doing is mapping from the equirectangular projection to $\Bbb R^3$ , and the equirectangular projection is neither a conformal nor equal-area map from the sphere to begin with. So some kind of adjustment term will surely be necessary, but I am hoping I have the correct basic idea with the Jacobian. How does this work?","['geometry', 'jacobian', 'multivariable-calculus', 'cartography', 'differential-geometry']"
4507923,Proving the existence of KKT points,"Let a function $f:\mathbb{R}^d \rightarrow \mathbb{R}$ an index set $I$ , and inequality constraints $g_i(x)\leq 0, ~\forall i \in I $ . How do we prove the existence of KKT points for $f$ subject to $g_i, ~i \in I$ without using e.g the existence of a minimum of $f$ (through Weierstrass for example) in conjunction with a constraint qualification? I have a feeling that it could be a fixed point theorem. PS:
In an optimization problem, we implicitly prove that the set of KKT points is nonempty through evoking some constraint qualification for feasible local optima of the problem. i.e. , Since local optima must be KKT points then the set of KKT points is nonempty. Can we use a straightforward argument for the existence of KKT points irregardless of local optima and constraint qualifications?","['optimization', 'lagrange-multiplier', 'analysis', 'karush-kuhn-tucker']"
4507933,Given the CDF $F(x)=1-e^{-x^2};0≤x<∞$. Derive the moment generating function of $X$,"I know you use $∫e^{tx}f(x)\,dx$ but I cant figure out the calculation. Any help would be appreciated","['moment-generating-functions', 'statistics', 'probability']"
4507963,"Given a random vector has univariate normal marginals, and a positive definite Covariance. Does this mean the vector is multivariate normal?","So the question is basically in the title. We know that if $X \sim N(\mu, \sigma)$ is multivariate normal for $\sigma$ positive definite, then we have that $X_i$ is normal. But is the converse statement true? Say $X = (X_1, ..., X_n)$ with $X_i \sim N(\mu_i, \sigma_{ii})$ , $\sigma_{ij} = Cov(X_i, X_j)$ such that $\sigma$ is positive definite. Does that mean that $X$ is multivariate normal? If we leave out the positive definite - condition, we can easily construct a counterexample, e.g. $X = -Y \sim N(\mu, \sigma)$ but $X+Y \equiv 0$ , so we have found a non trivial linear combination of $(X, Y)$ which is not normal. I have looked quite a bit in the literature and couln't find a proof or counterexample. Thanks","['statistics', 'normal-distribution', 'probability']"
4507969,"Efficiency of general solution to ""primes of the form $x^2+ny^2$""","Cox's book Primes of the form $x^2+ny^2$ presents a general solution to the question ""for a (fixed $n>0$ and) given prime $p$ , does $p=x^2+ny^2$ have an integer solution?"".
The general solution is ""there is an integer polynomial $f_n$ such that $p=x^2+ny^2$ iff (some side conditions hold and) $f_n(x)=0$ has a solution mod $p$ "". I am concerned that the condition on the RHS isn't any easier to verify than the one on the LHS, and hence that this isn't a ""solution"" to the problem at all. Note that in checking whether $p=x^2+ny^2$ has a solution, we know that $\left|x\right|,\left|y\right|<\sqrt p$ , hence, at worst, we need to check about $(2\sqrt{p})^2=4p$ values of $(x,y)$ . However, in checking whether $f_n(x)=0\mod p$ has a solution, we need to check all $p$ values of $x$ , so this is at best a factor of $4$ improvement, which isn't much (and may well be worse than that, since $f_n$ could be a more complicated polynomial to compute that $x^2+ny^2$ ). By contrast, in Fermat's original theorem (where $n=1$ ), the condition is just $p=1\mod 4$ , which is obviously much faster to check. Similarly, in the other cases presented near the beginning of the book, the solution is given in terms of congruence conditions on $p$ . In these simplest cases (when the class number $h(-4n)$ is $1$ ), these congruence conditions come from quadratic reciprocity: one first derives the condition that $D$ is a square mod $p$ , and then quadratic reciprocity lets us reformulate this in terms of congruence conditions on $p$ . Hence, I could imagine that (at least for some $n$ ), there is some ""reciprocity law"" allowing us to check whether $f_n=0\mod p$ has a solution in terms of congruence conditions on $p$ , which would be an improvement. Is this what's going on? Note that toward the end of the book, Cox discusses an algorithm to solve the problem. But as far as I can tell, this is just an algorithm to find $f_n$ , not to solve $f_n(x)=0\mod p$ . (And anyway, there is obviously an algorithm to solve $f_n(x)=0\mod p$ ; the issue at hand is whether there is an efficient algorithm. I apologize if the answer to this question is already contained in Cox's book. I haven't read it in detail.","['number-theory', 'diophantine-equations', 'computational-complexity', 'prime-numbers', 'quadratic-forms']"
4507984,Proving that $\cos{x} + \{x\}$ is not periodic?,"Let's say I have the function $\cos{x} + \{x\}$ , where $\{x\} = x - \lfloor x \rfloor$ . How would I prove that the function is not periodic? It seems intuitively true to me, but I can't seem to be able rigorously prove it. I.e., I can't negate the possibility that there is some positive $p$ such that $\cos{x} + \{x\} = \cos{(x + p)} + \{x + p\}$ for all $x \in \mathbb{R}$ .","['periodic-functions', 'functions']"
4507996,Conditional probability measure theoretic definition,"Let $(\Omega, \mathcal{F}, P)$ be a probability space, $\mathcal{G} \subseteq \mathcal{F}$ a $\sigma$ -field in $\mathcal{F}$ . Given $A \in \mathcal{F}$ , the Radon-Nikodym theorem implies that there is ${ }^{[3]}$ a $\mathcal{G}$ -measurable random variable $P(A \mid \mathcal{G}): \Omega \rightarrow \mathbb{R}$ , called the conditional probability, such that $$
\int_{G} P(A \mid \mathcal{G})(\omega) d P(\omega)=P(A \cap G)
$$ for every $G \in \mathcal{G}$ , and such a random variable is uniquely defined up to sets of probability zero. A conditional probability is called regular if $\mathrm{P}(\cdot \mid \mathcal{B})(\omega)$ is a probability measure on $(\Omega, \mathcal{F})$ for all $\omega \in \Omega$ a.e. I am used to the non-measure theoretic definition of conditional probability defined for events where $P(A|B)=\frac{P(A\cap B)}{P(B)}$ . Why is this function not defined like $$\frac{1}{P(G)}\int_{G} P(A \mid \mathcal{G})(\omega) d P(\omega)=\frac{P(A \cap G)}{P(G)}
$$ which would make it in line with the definition for events? I understand that $P(G)=0$ is an issue, but what is the motivation of defining this representation for $P(A\cap G)$ ?","['conditional-probability', 'probability-distributions', 'probability-theory', 'probability']"
4507999,"Find two dependent random variables $X$ and $Y$ such that $\phi_{X+Y}(t)=\phi_{X}(t) \phi_{Y}(t)$, $\forall t$.","Hello Math Stack Exchange community, I am currently studying the concept of characteristic functions in probability theory and came across an interesting problem. I understand that for independent random variables, the characteristic function of their sum is the product of their individual characteristic functions, i.e., $\phi_{X+Y}(t) = \phi_X(t) \phi_Y(t)$ for all $t$ . However, I am curious about whether this property can hold for dependent random variables as well. Problem: Find two dependent random variables $X$ and $Y$ such that $\phi_{X+Y}(t) = \phi_X(t) \phi_Y(t)$ for all $t$ . Here is what I know that might help: The characteristic function of a random variable $X$ is defined as $\phi_X(t) = E[e^{itX}]$ , where $i$ is the imaginary unit, and $E[\cdot]$ represents the expected value.
The characteristic function of a sum of two random variables is given by $\phi_{X+Y}(t) = E[e^{it(X+Y)}]$ .
With this knowledge, I attempted to answer the problem: Let $X$ and $Y$ be two random variables with known characteristic functions $\phi_X(t)$ and $\phi_Y(t)$ . We want to find a pair of dependent random variables $X$ and $Y$ that satisfy the given condition. At this point, I got stuck. I tried to relate the expected values of $X$ and $Y$ and their characteristic functions, but couldn't make any progress. I am not sure how to approach this problem further or whether there are specific properties of dependent random variables that can help me. Thank you!","['moment-generating-functions', 'characteristic-functions', 'independence', 'probability-theory']"
4508020,"Let $G$ be a finite group, $H$ be a subgroup of $G$ with $[G:H]=2$ and for all $h \in H-{1}$, $C_G(h) \leq H$. Prove $G-H$ forms a conjugacy class.","I'm studying for an exam and I am very much stuck on this problem. I wasn't able to find anything similar enough to this problem to help me yet, so I thought I would make a post. Let $G$ be a finite group and $H$ a subgroup of $G$ with $[G:H]=2$ . In addition, suppose that for all $h \in H-\{1\}$ that $C_G(h) \leq H$ . Prove that the elements of $G-H$ form a conjugacy class of $G$ . So, I am starting to wonder if I am approaching this the wrong way. I've tried to prove this using double containment, starting both ways to see if one is easier, but I get stuck early on in both ways. I'll show what I have below. Proof: Let $x \notin H$ . We want to show $Cl(x) = G-H$ . $(\subseteq)$ Let $a \in Cl(x)$ . Then there exists $g \in G$ such that $gxg^{-1}=a$ . For the sake of contradiction, suppose $a \in H$ . Then, $gx = ag$ . If $g \in H$ then $gx \notin H$ as $[G:H]=2$ implies $G/H = \{H, Hx\}$ . Yet, $ag \in H$ which would create a contradiction. Hence, $g \notin H$ . [This is where I get stuck in this direction.] $(\supseteq)$ Let $a \in G-H.$ Then, $a = hx$ for some $h \in H$ . [This is where I get stuck here. I know we want to find some $g \in G$ such that $gxg^{-1} = hx$ but I am having trouble finding any such $g$ .]","['group-theory', 'abstract-algebra', 'finite-groups']"
4508025,Picturing the proof of every open set of real numbers is a countable union of disjoint intervals.,"Every open set of real numbers is the union of a countable collection of disjoint open intervals. I understand the proof given on Royden's real analysis book page 42, I'm having a hard time picturing what does the intervals look like on the real line. Can anyone give me some idea on that? Suppose $O$ is open, for each $x\in O$ , there exists $y>x$ such that the open interval $(x,y)\subset O$ and define $b=\sup \{y:(x,y)\subset O\}$ . Define $a=\inf\{z: (z,x)\subset O\}$ , and $I_x=(a,b)$ then the proof basically going to show that the collection of all such intervals forms a union of $O$ and the collection has countably many such intervals and they are pairwise disjoint. Take the open set (0,1) and consider $x=1/2$ then $b=1$ and $a=0$ then we have $I_x=(0,1)$ , it seems like no matter what $x$ I choose the collection seems to only have 1 open interval in it which is essentially the open set (0,1) itself, I guess it does satisfies the requirement in the proof, but something just seems off on my understanding.","['elementary-set-theory', 'real-analysis']"
4508074,Integral involving $\sin({\ln{x}})$,"I need help with the following integral $$I=\int_0^\infty \frac{\sin{(\ln x)}}{x^4+x^2+1}dx$$ I know the answer is $$I=-\frac{\pi}{2}\frac{\sinh{\frac{\pi}{6}}}{\cosh{\frac{\pi}{2}}}$$ but I can't find how to get there. I tried integration by parts with $u=\sin({\ln{x}})$ but it went nowhere. I tried substituting $u=\ln{x}$ , but I was clueless after that. Not much else seems very sensible.","['integration', 'improper-integrals']"
4508076,High-order complex derivative in MATLAB,"First derivative can be calculated by the complex-step derivative formula: $f'(x)=\frac{Im(f(x+ih))}{h}$ Generalization of the above for calculating derivatives of any order employs multicomplex numbers, resulting in multicomplex derivatives: $f^{(n)}(x)=\frac{C^{(n)}_{n^2-1}(f(x+i^{(1)}h+i^{(n)}h))}{h^n}$ According to the Wiki Complex-variable methods : In Matlab, the calculation of the first order derivative is very easy to implement: x=0:0.01:10;
h=0.001;
f=sin(x);
df=imag(sin(x+h*i))./h;
plot(x,f)
hold on
plot(x,df) I do not understand how to implement the calculation of the second order derivative, because I do not understand what is $i^{(1)},i^{(2)}...i^{(n)}$ and how the operator $C^{(n)}_{n^2-1}$ is calculated. EDIT: Here is the program for the second order derivative, which is calculated incorrectly. I don't understand how to use $Imag_{12}$ x=0:0.01:15;
h=0.0000001;
imx1=x+(i)*h;
imx2=x+(i+i)*h;
f=(x).^2;
df = imag((imx1).^2)./h;
ddf = imag((imx2).^2)./h^2;","['complex-analysis', 'numerical-methods', 'derivatives', 'matlab']"
4508095,"Can we classify all finitely generated projective modules over $k[x,y,x^{-1},y^{-1}]$?","Let $k$ be a field and we consider the ring $R=k[x,y,x^{-1},y^{-1}]$ . Can we classify all finitely generated projective modules over $R$ ? In particular, are there non-free examples? I considered the similar question for $k[x,x^{-1}]$ . Since $k[x,x^{-1}]$ is a PID, any finitely generated projective module is free. If we think of the analogue of topology, we know that there are definitely non-free vector bundles over $(\mathbb{C}^{*})^{2}$ since it is not contractible. However the same is true for $\mathbb{C}^{*}$ but there is no non-free finitely generated projective modules over $k[x,x^{-1}]$ , which shows that this analogue is not always precise. Again, my question is : Can we classify all finitely generated projective modules over $R$ ? In particular, are there non-free examples?","['projective-module', 'abstract-algebra', 'commutative-algebra', 'modules']"
