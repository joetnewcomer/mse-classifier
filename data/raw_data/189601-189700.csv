question_id,title,body,tags
3557359,Does there exist a volume-preserving diffeomorphism of the disk without conformal points?,"This question is related to this one , though is supposed to be easier. Let $D \subseteq \mathbb{R}^2$ be the closed unit disk. Does there exist a smooth volume-preserving diffeomorphism $f:D \to D$ such that the singular values of $df$ are everywhere distinct? i.e. I want $\sigma_1(df_p) \neq \sigma_2(df_p)$ for every $p \in D$ , and the product $\sigma_1(df)\sigma_2(df)=1$ to be constant. This answer provides the following example for such a diffeomorphism $D\setminus \{0\} \to D \setminus \{0\}$ with the required properties: $f_c: (r,\theta)\to r\big(\cos(\theta+c\log(r)), \sin(\theta+c\log (r))\big),\;\; $ (for every non-zero $c ֿ\in \mathbb R$ we get an example). Edit-a description of a possible topological obstruction: Set $\mathcal{NC}:=\{ A \in M_2(\mathbb{R}) \, |    \det A \ge 0 \, \,\text{ and } \, A \text{ is not conformal} \,\}$ , where by a non-conformal matrix, I refer to a matrix whose singular values are distinct. (i.e. I allow non-zero singular matrices in $\mathcal{NC}$ ). Suppose that such an $f \in \text{Diff}(D)$ exists. Then $df|_{\partial D}:\partial D \to \mathcal{NC}$ is homotopic to a constant. $df|_{\partial D}$ maps $T\partial D$ to itself, and in particular, at a specific point $\theta \in \mathbb{S}^1$ , $(df|_{\partial D})_{\theta}(T_{\theta}\partial D)=T_{f(\theta)}\partial D$ . So, thinking on $e_2$ as an element of $T_{(0,1)}\partial D$ , we have $R_{f(\theta)}^{-1} \circ df_{\theta} \circ R_{\theta}(e_2)=\lambda(\theta) e_2$ , for some positive factor $\lambda(\theta)$ . Setting $A_{\theta}:=R_{f(\theta)}^{-1} \circ df_{\theta} \circ R_{\theta}$ , and $$\mathcal{F}:=\{ A \in \text{SL}_2(\mathbb{R}) \, | \, Ae_2 \in \operatorname{span}(e_2) \, \, \text{ and } \, \, A \, \text{ is not conformal} \,\},$$ we get $$ df_{\theta} =R_{f(\theta)} \circ A_{\theta} \circ R_{-\theta}, \, \, \, A_{\theta}: \partial D \to \mathcal{F}. \tag{1}$$ If $\mathcal{F}$ were contractible in $\mathcal{NC}$ , we could deform $A_{\theta}$ to a constant map $ \partial D \to \mathcal{NC}$ . Thus, by equation $(1)$ , $df|_{\partial D}$ would be homotopic to the map $\theta \to R_{f(\theta)} \circ A \circ R_{-\theta}$ for some constant non-conformal matrix $A \in \mathcal{NC}$ . Writing $A=R_{\alpha} \Sigma R_{\beta}$ where $\Sigma$ is non-negative and diagonal, we would get that $df|_{\partial D}$ is homotopic to $\theta \to R_{f(\theta)+\alpha} \circ \Sigma \circ R_{-\theta+\beta}.$ On the space of non-conformal matrices $\mathcal{NC}$ , there is a continuous map* $H:\mathcal{NC} \to \mathbb{S}^1$ , given by $H(R_{\phi} \Sigma R_{\theta})= R_{2\theta}$ . This leads to a contradiction to the contractibility of $df|_{\partial D}$ : Indeed, if it were homotopic to a constant, then so would the map $\theta \to R_{f(\theta)+\alpha} \circ \Sigma \circ R_{-\theta+\beta}.$ Composing it with $H$ , we obtain the map $\theta \to R_{-2\theta+2\beta}$ , or $\theta \to -2\theta$ , which is not homotopic to a constant. Since $\mathcal{F}$ is not contractible in $\mathcal{NC}$ , this argument fails. However, perhaps a more refined topological argument could obtain more, I don't know. *The map $H$ is well-defined, since $U\Sigma V^T=(-U)\Sigma (-V)^T$ , and this is the only ambiguity in $U,V$ for a matrix in $\mathcal{NC}$ . Thus $\theta$ is well defined up to an addition of $\pi$ .","['real-analysis', 'quasiconformal-maps', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
3557398,undetermined coefficient method question for second-order (particular solution),"Here are two second-order differential equations. $$ y''+9y=\sin(2t) \tag 1 $$ $$ y'' +4y =\sin(2t)  \tag 2 $$ I am told to use undetermine coefficients method to solve. For 1), I use $y_p=A \cos(2t)+B \sin(2t)$ to get $A=0$ and B= $\frac{1}{5}$ and get $y_p=\frac{1}{5} \sin(2t)$ For 2), I realize that that method doesn't work and told to do $y_p=t(A \cos(2t)+B \sin(2t)$ Why does it work then?",['ordinary-differential-equations']
3557550,Find $\lim_{n\to \infty}\int _0^{\frac{\pi}{2}} \sqrt{1+\sin^nx}$,"Define $$I_n=\int _0^{\frac{\pi}{2}} \sqrt{1+\sin^nx}\, dx$$ I have to show this sequence is convergent and find its limit. I proved it is decreasing: $\sin^{n+1} x \le \sin^n x \implies I_{n+1} \le I_n$ . Also, it is bounded because: $$0 \le \sin^n x \le 1 \implies \frac\pi{2} \le \int _0^{\frac{\pi}{2}} \sqrt{1+\sin^n x}\, dx\le \frac{\pi\sqrt{2}}{2}$$ so it is convergent. I'm stuck at finding the limit. I think it should $\frac{\pi}{2}$ but I'm not sure.","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'limits']"
3557583,How to rearrange a second order differential equation?,How would I go about rearranging a second order ODE? For instance if I wanted to rearrange the below for $\theta$ how would I go about it? $$\ddot{\theta}+\frac{g}{l}\sin \theta=0$$,['ordinary-differential-equations']
3557612,Functional Equation Involving Trigonometric Functions: $f(\sin x +\sin y)=f(x+y)$,"Find all functions $f : \mathbb{R} \to \mathbb{R} $ such that $f(\sin x +\sin y)=f(x+y)$ .
My guess is that the function is constant, I've found that: The function is even $f(\sin x + \sin y)=f(\cos x + \cos y)$ The function is periodic with period $\pi$ I don't really know what else to do.","['functional-equations', 'trigonometry', 'functions']"
3557626,Do any significant changes happen in hypercomplex numbers beyond the eight dimensions of the octonions?,"They continue in the fashion of powers of 2: reals (1), complex (2), quaternion (4), octonions (8), and then there is sedonions(16), right?  And, this keeps going, right?
Do any significant changes happen in hypercomplex numbers beyond the eight dimensions of the octonions, the way octonions mark where associativity is lost?","['abstract-algebra', 'hypercomplex-numbers']"
3557693,"Translating ""at most $n$ other people"" / ""exactly $n$ other people"" (besides possibly himself) into predicate formulas","Problem (Adapted from ""Mathematics for Computer Science"", Lehman, Leighton, Meyers, 2018.) Translate the following sentences into predicate formulas: (a) There is a student who has e-mailed at most two other people in the class, besides possibly himself. (b) There is a student who has e-mailed exactly two other people in the class, besides possibly himself. The domain of discourse are the students in the class. Use the predicate $E(x,y)$ to mean that "" $x$ has sent e-mail to $y$ "". Solution attempt (a) The sentence can be rephrased as: ""There is a student who did not e-mail 3 or more people in the class, besides possibly himself"". Or, in other words: ""There is a student $x$ such that, for all students $y_1,y_2,y_3$ , if $x$ has e-mailed all three of $y_1,y_2,y_3$ , then at least two of $y_1,y_2,y_3$ are equal to each other, or at least one of $y_1,y_2,y_3$ is equal to $x$ "". So, this can be expressed as: $\exists x \forall{y_1,y_2,y_3} \left[ E(x,y_1) \land E(x,y_2) \land E(x,y_3)) \implies (y_1=y_2 \lor y_1 = y_3 \lor y_2=y_3 \lor y_1=x \lor y_2=x \lor y_3=x ) \right]$ The value of this predicate formula is false if all students have e-mailed at least three distinct students $y_1,y_2,y_3$ all of which are different from $x$ . (b) Two solution attempts: The first attempt is based on rephrasing the sentence as: ""There are three distinct students $x,y,z$ such that $x$ e-mailed both $y$ and $z$ and, for all students $w$ , if $x$ has e-mailed $w$ , then $w$ is equal to one of $x,y,z$ "": $ \exists{x,y,z} \forall w \left[ E(x,y) \land E(x,z) \land y\neq z \land y\neq x \land z\neq x \land (E(x,w) \implies w=x\lor w=y\lor w=z) \right] $ The second attempt is based on rephrasing the sentence as: ""There are three distinct students $x,y,z$ such that, for all students $w$ : $x$ has e-mailed $w$ exactly when $w$ is equal to one of $x,y,z$ , or $w=x$ ( $x$ has possibly e-mailed himself)"". $ \exists{x,y,z} \forall w \left[ w=x\lor (y\neq z \land y\neq x \land z\neq x 
 \land (E(x,w) \iff w=x\lor w=y\lor w=z) \right] $ The value of the predicate formulas of both attempts above is false if there is no student $x$ who e-mailed exactly two distinct people $y,z$ . Is this solution correct?","['quantifiers', 'predicate-logic', 'logic', 'discrete-mathematics']"
3557749,Showing a sequence is not Schauder basis,"I'm tring to show that in $L^2[0,1]$ , there are at least two distinct choices of coefficients $\left(c_{n}\right)_{n \in \mathbb{Z}}$ such that for $0<b<1$ $$1=\sum_{n \in \mathbb{Z}} c_{n} e^{2 \pi i b n x}.$$ One seems obvious that $c_0=1$ and other $c_n$ are all $0$ . How to determine other choices of $c_n$ ? Also I want to show this series converges in $L^2$ -norm so $
\left\{e^{2 \pi i b n x}\right\}_{n \in \mathbb{Z}}
$ is not a Schauder basis.","['functional-analysis', 'real-analysis']"
3557778,Is finding martingales black magic?,"I've recently been learning about the power of martingales for calculating various stochastic quantities. This is a bit of a meta question: is coming up with the right martingale for the quantity you want just complete black magic? As an example, consider an unbiased random walk, where we start at position $x$ and want the probability we either hit $0$ or $L$ . In this case (after shifting the start to $0$ and left/right limits to $-x$ and $L-x$ ) a good martingale to use for the calculation is $$S_n=\sum_{i=1}^n X_i$$ where $X_i$ is the random variable of the steps ( $X_i=\pm 1$ with equal probability). However, this is not a martingale when the walk is biased, i.e. $X_n=+1$ with probability $p$ and $X_n=-1$ with probability $q=1-p$ . Then it turns out that we should use a different martingale for the calculation, namely $$Y_n=\left(\frac{q}{p}\right)^{\sum_{i=1}^n X_i}.$$ How does one come up with the appropriate martingale to calculate a given quantity in general? Edit: fixed definition of appropriate martingale for unbiased case above, thank you commenters...","['martingales', 'probability']"
3557790,"If a random variable $X$ is such that $P(X=0)<1,$ then there is an $\varepsilon>0$ with $P(\vert X\vert>\varepsilon)>0$","$\textbf{The Problem:}$ Show that if a random variable $X$ is such that $P(X=0)<1,$ then there is an $\varepsilon>0$ with $P(\vert X\vert>\varepsilon)>0.$ $\textbf{My Proof:}$ Since $P(X=0)<1,$ we have that $0<P(\{X=0\}^\complement)=P(\vert X\vert>0)\leq1$ . Now suppose that no such $\varepsilon>0$ exists. Then for all $n\in\mathbb N$ we have $P\left(\vert X\vert>\frac{1}{n}\right)=0$ . Now the sequence $\left(\left\{\vert X\vert>\frac{1}{n}\right\}\right)_{n\in\mathbb N}$ is nested with $\left\{\vert X\vert>\frac{1}{n}\right\}\nearrow\{\vert X\vert>0\}.$ To see this, note that for $n\in\mathbb N$ we have that if $\vert X(\omega)\vert>\frac{1}{n}$ , then $\vert  X(\omega)\vert>\frac{1}{n+1}$ , so $\left\{\vert X\vert>\frac{1}{n}\right\}\subseteq\left\{\vert X\vert>\frac{1}{n+1}\right\}$ . Now let $\omega\in\bigcup_n\left\{\vert X\vert>\frac{1}{n}\right\}.$ Then there is $n\in\mathbb N$ such that $\vert X(\omega)\vert>\frac{1}{n}$ , and hence $\vert X(\omega)\vert>0$ . Thus, $\bigcup_n\left\{\vert X\vert>\frac{1}{n}\right\}\subseteq\{\vert X\vert>0\}.$ For the reverse inclusion let $\omega\in\{\vert X\vert>0\}.$ Then by the Archimedean property there exists $n\in\mathbb N$ such that $\vert X(\omega)\vert>\frac{1}{n}$ , so $\omega\in\bigcup_n\left\{\vert X\vert>\frac{1}{n}\right\}.$ Hence, $\bigcup_n\left\{\vert X\vert>\frac{1}{n}\right\}=\{\vert X\vert>0\}.$ Thus, the continuity of the probability measure implies that $$\lim\limits_{n\to\infty}P\left(\left\{\vert X\vert>\frac{1}{n}\right\}\right)=P(\vert X\vert>0)=0,$$ which is a contradiction. The result follows. Do you agree with the proof above? If not, it would be much appreciated if anyone could please point out where the proof goes wrong. Thank you for your time.","['solution-verification', 'probability-theory', 'probability']"
3557929,"if $g$ is analytic on the open disk, is nonzero, and if $|g(z)| \to 1$ as $|z| \to 1$, then $g$ is constant","I am trying to show that If $g$ is analytic on the open disk, is nonzero, and if $|g(z)| \to 1$ as $|z| \to 1$ , then $g$ is constant. I want to apply the maximum principle and the minimum principle to conclude that the modulus is constant. I think that this would be possible if I could extend the domain to the closed disk as I would conclude that the maximum and minimum values of $g$ both have modulus 1. However, I do not know if I can/how to extend the definition of $g$ to the boundary. Any help would be greatly appreciated.",['complex-analysis']
3557931,Japanese Temple Geometry Problem: Two tangent lines and three tangent circles.,"I am working on my Senior Thesis for my Bachelor's Degree in Mathematics. My project involves Japanese San Gaku problems, and moving said problems from Euclidean Geometry to Spherical and Hyperbolic Geometry. I've been working on a particular problem for weeks now. The problem is stated as follows: Problem 1.2.5: A circle $O(r)$ has its center on a line $m$ , and has a tangent line $\ell$ . The circles $O_1(r_1)$ and $O_2(r_2)$ both touch $O(r)$ externally and also the lines $\ell$ and $m$ . Show that \begin{align*}
4r=r_1+6\sqrt{r_1r_2}+r_2.
\end{align*} I have worked very hard on this and have come up with a lot of stuff. One very useful result is the following: Useful Result. Given tangent circles $O_1(r_1)$ and $O_2(r_2)$ , and a line $AB$ tangent to $O_1$ at $A$ and to $O_2$ at $B$ , it follows that $$|AB| = 2\sqrt{r_1 r_2}$$ See Useful Result . Additionally, I've managed to construct the figure in Geogebra. This is quite a difficult task unless you know what you're doing (which I did not at first!). Here is what the figure looks like after construction . By equating $AC$ and $A'C'$ in this image , I've been able to solve the problem, but not by hand. After using the Useful Result to rewrite $AC$ , and a clever usage of the Pythagorean Theorem to write $A'C'$ in terms of $r$ , $r_1$ , and $r_2$ , I get an algebraic nightmare. Mathematica can solve it for $r$ and provide us with the desired result, but that's a little unsatisfying. This problem comes from the book Japanese Temple Geometry Problems: San Gaku by H. Fukagwa and D. Pede. I can't find the problem anywhere online, and the ""solution"" in the back only says ""Written on a surviving tablet in the Yagamata prefecture in 1823."" If some incredibly smart individual out there could help me come up with a better way of solving this problem, I would be forever in your favor!","['sangaku', 'tangent-line', 'circles', 'geometry']"
3557935,$1-1+1-1+1-1+\cdots$ and $1-2+3-4+5-6+7-\cdots$ and Taylor's theorem,"Some background. I was exploring the series expansion for $\ln(1+\cos x)$ in an attempt to expand it (at least, initially!) up to the third non-zero term, and along the way I unexpectedly stumbled upon the infinite series $1-1+1-1+...$ and $1-2+3-4+5-6+7-...$ . Then I thought to expand it instead via a different method, as I had divergent series as coefficients which obviously wouldn't have helped in determining the terms exactly. The coefficients via this method (as expected) came out as reals, and I was then, out of curiosity, tempted to equate these respectively, arriving at a very unexpected (in this context) result: $1-1+1-1+1-1+1-...=\frac{1}{2}$ and $1-2+3-4+5-6+...=\frac{1}{4}$ . What caught me as particularly surprising is that these are well-known results for the sums via other methodologies. The ""proof"" is below. By Taylor's theorem, substituting the series for $\ln(1+x)$ and $\cos x$ : \begin{align}
\ln(1+\cos x)
&=\sum_{n=1}^\infty \left(\frac{(-1)^{n-1}}{n}\left({\sum_{k=0}^\infty \frac{(-1)^{k}x^{2k}}{(2k)!}}\right)^{n}\right)\\
&=\sum_{n=1}^\infty \left(\frac{(-1)^{n-1}}{n}\left(1-\frac{x^2}2+\frac{x^4}{24}-\cdots\right)^n\right)\\
\end{align} We are only concerned with the first 3 terms overall, and terms whose order is 6 or above cannot contribute towards the coefficients of the lower order terms (0,2,4 in this case). So consider $\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^n$ , ignoring order 6 or higher terms, for $n\in\mathbb{Z_{>0}}$ : \begin{align}
\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^2
&=\ 1-\frac{2x^2}{2}+\frac{8x^4}{24}-\cdots\\
\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^3
&=\ 1-\frac{3x^2}{2}+\frac{21x^4}{24}-\cdots\\
\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^4
&=\ 1-\frac{4x^2}{2}+\frac{40x^4}{24}-\cdots\\
\end{align} and so on. Conjecture that (from the patterns in the coefficients) $$\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^m=1-\frac{mx^2}{2}+\frac{am^2+bm+c}{24}x^4-\cdots$$ for some $a,b,c\in\mathbb{R}$ . We know some $x^4$ coefficients from the manual calculations, so we can solve for a, b, c (using m = 1, 2, 3): $$a+b+c=1$$ $$4a+2b+c=8$$ $$9a+3b+c=21$$ $$\implies a=3,b=-2,c=0$$ $$\therefore\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^m=1-\frac{mx^2}{2}+\frac{3m^2-2m}{24}x^4-\cdots$$ for m = 1, 2, 3. Assuming this as an inductive hypothesis for some $m\in\mathbb{Z_{>0}}$ , we have: \begin{align}
\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^{m+1}
&=\left(1-\frac{x^2}{2}+\frac{x^4}{24})^m(1-\frac{x^2}{2}+\frac{x^4}{24}\right)\\
&=\left(1-\frac{mx^2}{2}+\frac{3m^2-2m}{24}x^4-\cdots\right)\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)\\
&=1-\frac{m+1}{2}x^2+\frac{3m^2-2m+6m+1}{24}x^4-\cdots\\
&=1-\frac{m+1}{2}x^2+\frac{3(m+1)^2-2(m+1)}{24}x^4-\cdots\\
\end{align} $$\therefore\left(1-\frac{x^2}{2}+\frac{x^4}{24}\right)^m=1-\frac{mx^2}{2}+\frac{3m^2-2m}{24}x^4-\cdots\forall{m}\in\mathbb{Z_{>0}}$$ by mathematical induction. Substituting this expansion back into the initial series expression, ignoring any terms in $x^6$ or higher (again because these terms will not contribute towards lower order coefficients): \begin{align}
\sum_{n=1}^\infty \left(\frac{(-1)^{n-1}}{n}(1-\frac{x^2}{2}+\frac{x^4}{24})^n\right)
&=\sum_{n=1}^\infty \left(\frac{(-1)^{n-1}}{n}\left(1-\frac{nx^2}{2}+\frac{3n^2-2n}{24}x^4\right)\right)\\
&=\sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n}-\frac{x^2}{2}\sum_{n=1}^{\infty}(-1)^{n-1}+\frac{x^4}{24}\sum_{n=1}^{\infty}(3n-2)(-1)^{n-1}\\
&=\sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n}-\frac{x^2}{2}\sum_{n=1}^{\infty}(-1)^{n-1}+\frac{x^4}{24}\left(3\sum_{n=1}^{\infty}n(-1)^{n-1}-2\sum_{n=1}^{\infty}(-1)^{n-1}\right)\\
\end{align} Now we've expressed the first 3 terms with the coefficients as series, let's expand it in a different way. \begin{align}
\ln(1+\cos x)
&=\ln(2+(\cos x-1))\\
&=\ln\left(2\left(1+\frac{1}{2}\left(\cos x-1\right)\right)\right)\\
&=\ln2+\ln\left(1+\frac{1}{2}(\cos x-1)\right)\\
&=\ln2+\sum_{n=1}^\infty \left(\frac{(-1)^{n-1}}{n}\left(\frac{1}{2}{\sum_{k=1}^\infty \frac{(-1)^{k}x^{2k}}{(2k)!}}\right)^n\right)\\
&=\ln2+\frac{1}{2}\sum_{n=1}^\infty \left(\frac{\left(\frac{-1}{2}\right)^{n-1}}{n}\left(-\frac{x^2}{2}+\frac{x^4}{24}-\cdots\right)^n\right)\\
&=\ln2+\frac{1}{2}\left(\left(-\frac{x^2}{2}+\frac{x^4}{24}-\frac{x^6}{720}+\cdots\right)-\frac{1}{4}\left(-\frac{x^2}{2}+\frac{x^4}{24}-\frac{x^6}{720}+\cdots\right)^2+\cdots\right)\\
\end{align} Here, we can leave from consideration any of the order 6 or higher terms in the first ""bracket"", any of the order 4 or higher terms in the second ""bracket"" and all terms in any subsequent ""bracket"" for the purpose of our expansion (only looking for the first 3 terms). None of these will contribute towards the coefficients of the first few terms, as the power of the ""bracket"" is incrementing by 1 at each step. We get: \begin{align}
\ln(1+\cos x)
&=\ln2+\frac{1}{2}\left(\left(-\frac{x^2}{2}+\frac{x^4}{24}\right)-\frac{1}{4}\left(-\frac{x^2}{2}\right)^2+\cdots\right)\\
&=\ln2-\frac{x^2}{4}-\frac{x^4}{96}+\cdots\\
\end{align} Now that we have 2 different expressions for $\ln(1+\cos x)$ , we can compare their coefficients: $$\sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n}-\frac{x^2}{2}\sum_{n=1}^{\infty}(-1)^{n-1}+\frac{x^4}{24}(3\sum_{n=1}^{\infty}n(-1)^{n-1}-2\sum_{n=1}^{\infty}(-1)^{n-1})=\ln2-\frac{x^2}{4}-\frac{x^4}{96}$$ From this: $$\sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n}=\ln2;$$ $$-\frac{1}{2}\sum_{n=1}^{\infty}(-1)^{n-1}=-\frac{1}{4}\implies\sum_{n=1}^{\infty}(-1)^{n-1}=\frac{1}{2};$$ $$\frac{1}{24}\left(3\sum_{n=1}^{\infty}n(-1)^{n-1}-2\sum_{n=1}^{\infty}(-1)^{n-1}\right)=-\frac{1}{96}$$ $$\implies3\sum_{n=1}^{\infty}n(-1)^{n-1}-2\left(\frac{1}{2}\right)=-\frac{1}{4}\implies\sum_{n=1}^{\infty}n(-1)^{n-1}=\frac{1}{4}$$ So the reason for the post was, why is it that this arrives at said conclusions, and where do the flaws in the method lie? What's the catch? These are obviously quite unintuitive (but interesting, nonetheless) values for such series, arrived at in a completely unforeseen (to me, at least) way. Would highly appreciate any clarifications/explanations. P.S - I'm an A-level/high-school student, so if there are obvious issues then apologies; I haven't done any analysis yet.","['analysis', 'real-analysis', 'taylor-expansion', 'sequences-and-series', 'power-series']"
3558249,Validating solution to PDE using integral transforms,"I'm trying to obtain the analytical solution of a Fokker-Planck PDE, which the solution is a probability density function, and then use this to find the mean of some quantity in the paper. The paper has a solution which they say can be found via Mehler-Fock transform. Their solution which I am trying to obtain reads $$P_{\varepsilon}(L,u) = \frac{e^{-\varepsilon^2sL/4}}{2\sqrt{2\pi}(\varepsilon^2sL)^{\frac32}}\int_{u}^{\infty}\frac{xe^{-x^2/(\varepsilon^2sL)}}{\sqrt{\cosh{(x)}-\cosh{(u)}}}dx.$$ I obtain a different solution to this one. $\textbf{My attempt:}$ The equation in the paper reads \begin{align}
\frac{\partial P_\varepsilon}{\partial L} = \varepsilon^2s\frac{\partial}{\partial u}\bigg[(u^2-1)\frac{\partial P_\varepsilon}{\partial u}\bigg], \quad P_\varepsilon(L=0,u) = \delta(u-1).
\end{align} This is a $1d$ Fokker-Planck equation, where $P_\varepsilon(L,u)$ is a probability density for some diffusion Markov process. To solve this equation, note that the right hand side is simply a Legendre differential equation. Denote the legendre function of the first kind via \begin{align}
	\frac{d}{du}(u^2-1)\frac{d}{du}P_{-\frac12+i\mu}(u) = -\bigg(\mu^2+\frac14\bigg)P_{-\frac12+i\mu}(u),
\end{align} which has an integral representation \begin{align}
	P_{-\frac12+i\mu}(u) = \frac{\sqrt{2}}{\pi}\cosh{(\pi\mu)}\int_{0}^{\infty}\frac{\cos{(\mu\tau)}}{\sqrt{\cosh{(\tau)}+u}}d\tau.
\end{align} Now use the Mehler-Fock transform. The Mehler-Fock transform of an integrble function $f$ defined on $[1,\infty)$ is the function $\check f$ defined on $[0,\infty)$ where \begin{align}
\hat{f}(\mu) = \int_{1}^{\infty}f(u) P_{-\frac12+i\mu}(u)\,du,
\end{align} with inverse transform \begin{align}
	f(u) = \int_{0}^{\infty}\check{f}(\mu)\mu\tanh{(\mu\pi)}P_{-\frac12+i\mu}(u)\,d\mu.
\end{align} Applying the Mehler-Fock transform to the PDF $P_\varepsilon(L,u)$ gives \begin{align}
	\check p(L,\mu) = \int_{1}^{\infty}p(L,u)P_{-\frac12+i\mu}(u)du.
\end{align} Taking a partial derivative in $L$ gives \begin{align}
	\frac{\partial\check p}{\partial L}(L,\mu) = \varepsilon^2s\int_{1}^{\infty}\frac{\partial}{\partial u}\bigg[(u^2-1)\frac{\partial p}{\partial u}(L,u)\bigg]P_{-\frac12+i\mu}(u)du.
\end{align} Integrating twice more by parts gives \begin{align}
	\frac{\partial\check p}{\partial L}(L,\mu) = \varepsilon^2s\int_{1}^{\infty}p(L,u)\frac{\partial}{\partial u}\bigg[(u^2-1)\frac{\partial P_{-\frac12+i\mu}}{\partial u}(u)\bigg]du.
\end{align} Using the ODE $(2)$ which satisfies the Legendre function, the Mehler-Fock transform satisfies the ODE \begin{align}
	\frac{\partial \check p}{\partial L}(L,\mu) = -\varepsilon^2s\bigg(\mu^2+\frac14\bigg)\check p(L,\mu), \quad \check p(L=0,\mu) = 1.
\end{align} Then \begin{align}
	\check p(L,\mu) = \exp{\bigg(-\bigg(\mu^2+\frac14\bigg)L\varepsilon^2s\bigg)}.
\end{align} Hence, the solution to $(1)$ is \begin{align}\nonumber
	P_\varepsilon(L,u) &= \int_{0}^{\infty}\mu\tanh{(\mu\pi)}P_{-\frac12+i\mu}(u)\exp{\bigg(-\bigg(\mu^2+\frac14\bigg)L\varepsilon^2s\bigg)}d\mu \\
	&= \int_{0}^{\infty}\mu\tanh{(\mu\pi)}P_{-\frac12+i\mu}(u) = \frac{\sqrt{2}}{\pi}\cosh{(\pi\mu)}\int_{0}^{\infty}\frac{\cos{(\mu\tau)}}{\sqrt{\cosh{(\tau)}+u}} \\
&\times\exp{\bigg(-\bigg(\mu^2+\frac14\bigg)L\varepsilon^2s\bigg)}d\tau d\mu.
\end{align} This paper claims to have a solution $$P_{\varepsilon}(L,u) = \frac{e^{-\varepsilon^2sL/4}}{2\sqrt{2\pi}(\varepsilon^2sL)^{\frac32}}\int_{u}^{\infty}\frac{xe^{-x^2/(\varepsilon^2sL)}}{\sqrt{\cosh{(x)}-\cosh{(u)}}}dx.$$ Here I have computed my solution vs theirs, for varying parameter $L\in[1,10]$ and fixed $\varepsilon^2s$ Clearly they do not agree. $\textbf{My second question}$ is how they use this solution to find a mean value via \begin{align}\mathbb{E(R)} &= \int_{1}^{\infty}\bigg(\frac{u-1}{u+1}\bigg)P_{\varepsilon}(L,u)du \\
&= \frac{e^{(-\varepsilon^2sL/4)}}{2\sqrt{2\pi}(\varepsilon^2Ls)^{\frac32}}\int_{1}^{\infty}\bigg(\frac{u-1}{u+1}\bigg)\int_{u}^{\infty}\frac{xe^{-x^2/(4\varepsilon^2sL)}}{\sqrt{\cosh{(x)}-\cosh{(u)}}}dxdu
\end{align} $\textbf{AND then reduce this expression to read}$ $$\mathbb{E(R)}=1-\frac{4}{\sqrt{\pi}}e^{-\varepsilon^2sL}\int_{0}^{\infty}\frac{x^2e^{-x^2}}{\cosh{(\sqrt{\varepsilon^2sL}x)}}dx,$$ since it's easy to see asymptotically that they behave differently. I'm looking for help in either of these questions. Thanks for the help in advance.","['integration', 'improper-integrals', 'contour-integration', 'partial-differential-equations', 'integral-transforms']"
3558306,Possible to have $m$-dimensional $C^k$ embedded submanifold in $\mathbb{R}^p$ with canonical projections onto $m$ dimensions are of dim $<m$ a.e.?,"Notations/terminologies: Let $\lambda_m$ be the Lebesgue measure on $\mathbb{R}^m$ .
Let $\sigma \in \Sigma_p$ be a permutation of the symbols $\{1,2...p\}$ .
Let $k\mathbb{N}$ denote a desired degree of smoothness of the manifold in question. Let $M^m \subset \mathbb{R}^p$ be a $C^k$ embedded submanifold of dimension $m$ . Does this always mean that: at least one of the canonical projections onto a collection of $m$ canonical (Euclidean) co-ordinates give rise to/have an image $N$ , so that $N \subset \mathbb{R}^m$ is also $\lambda_m$ almost everywhere a submanifold (with or without boundary) of $\mathbb{R}^p$ of dimension exactly $m$ ? I.e. is it true that, for any canonical projection $\pi: (x_1,...x_p) \mapsto (x_{\sigma(1)},x_{\sigma(2)},...x_{\sigma(m)})$ , where $\sigma \in \Sigma_p, N:=\pi(M)$ is also a $\lambda_m$ almost everywhere a submanifold? In other words, my question is: is it possible to construct a submanifold $M^m \subset \mathbb{R}^p$ so that all of the ${p \choose m}$ canonical projections onto any of the $m$ co-ordinates have dimension strictly less than $m$ for a subset of positive $\lambda_m$ measure? Attemp I: The way I was thinking of constructing a counter example is by constructing a non-linear map from a low dimension to a high dimension so that there's enough co-ordinates to make the map invertible, but none of the low dimensional projections are invertible. E.g. I was thinking of constructing a map: $F:\mathbb{R}^2\to \mathbb{R}^3$ given by: $F(x,y):=(xy^2, x^2y, xy + x^2y)$ so that we can check that projection onto any of the two co-ordinates have manifold dimension $1$ . But I'm not sure if it's possible to construct such a counterexample? Attemp II: I think for any smooth submanifold in question defined using global charts (like the ones I was trying in Attempt I ) wouldn't give us a counter example. This is because, locally, any smooth submanifold of dimension $m$ can be written as $F(x_1,...x_m)\mapsto (y_1,...y_p)$ where $y_i = y_i(x_1,...x_m)$ is a smooth function so that the Jacobian $[\frac{\partial y_i}{\partial x_j}]_{1 \leq i \le m, 1 \leq j \leq p }$ is of rank $m$ , hence an $m \times m$ submatrix, which WLOG can be assumed to be $[\frac{\partial y_i}{\partial x_j}]_{1 \leq i \le m, 1 \leq j \leq m }$ is of full rank $m$ . But then the projection $(y_1,...y_m, y_{m+1},...y_p) \mapsto (y_1,...y_m)$ gives us locally a manifold of dimension $m$ . But I guess we can't construct a global such projection?","['linear-algebra', 'geometry', 'riemannian-geometry', 'differential-geometry']"
3558346,Transforming sum with alternating signs into something less prone to catastrophic cancellation,"I'm wrestling with this formula, formally a function of the vector $\pmb{F}$ : $$\begin{aligned}
S(\pmb{F}) &:=\sum_{a=0}^A (-1)^{A-a}\,\binom{A}{a}\,
\Biggl[\sum_{b=0}^{A} F_b \,
\binom{a}{b}\Big/\binom{K}{b}\Biggr]^B
\\&\equiv
\sum_{a=0}^A (-1)^{a}\,\binom{A}{a}\,
\Biggl[\sum_{b=0}^{A} F_b \,
\binom{A-a}{b}\Big/\binom{K}{b}\Biggr]^B
\end{aligned}
$$ (the sums in brackets actually end at $a$ and $A-a$ , remaining terms being zero). The coefficients are all integers and can have these orders of magnitude: $A\sim \text{10 to 1000}$ $B\sim \text{10 to 300}$ $K \sim \text{50000 to 100000}$ and the argument satisfies $0\le F_b<1$ and $\sum_b F_b <1$ , which implies that $S(\pmb{F})\ge0$ . Unfortunately the terms of this sum (in $a$ ) easily assume extremely similar but opposite values, leading to catastrophic cancellation in numerical computations. For example one can get meaningless results with negative sign. Taking $b$ -independent terms out of the brackets doesn't help, because it doesn't change the relative precision of the mutually almost-cancelling terms. I've also tried grouping together the terms with $\tbinom{A}{a}$ and $\tbinom{A}{A-a}$ , but catastrophic cancellation occurs (especially if $A$ is even) even across such groups. Since I'm numerically dealing with this formula in R, I've tried using the Rmpf package for arbitrary-precision computation. It manages to get the correct result (which I can compute with Mathematica) in some cases, as opposed to machine-precision computation, but it still fails in some situations. And it makes the computation much slower. Does anyone have some clever ideas of how to transform this formula to avoid the catastrophic cancellation? I'd be really grateful for your help. Context This formula arises in network theory, in the problem of guessing the number of connections from a set of nodes to another, disjoint set of nodes. The first set has $K$ nodes, the second can be assumed to have infinite nodes. We know that exactly $A$ nodes of the first set connect to exactly $B$ nodes of the second, but we don't know which node connects to which. Each of the $A$ nodes must connect to at least one of the $B$ nodes; but not all $B$ nodes need to be receiving a connection. $F_b$ represents the fraction (relative frequency) of nodes in the second set that receive $b$ connections from the first set. The formula, derivable with some combinatorics, gives the probability (except for a factor independent of $\pmb{F}$ ) for a particular frequency distribution $\pmb{F}$ . The alternating sum appears from the probability-sum rule with more than two terms. Roughly speaking, we're calculating $1$ minus the probability that at least one node from the first set has no connections, or at least two nodes from the first set have no connections, and so on. Ultimately I'd be interested in the logarithm of $S(\pmb{F})$ . From this context, I'd be happy to hear suggestions for slight variations or limits that may lead to a more manageable formula. Cheers!","['catastrophic-cancellation', 'combinatorics', 'probability']"
3558428,Finding Common Tangent of two circles,"It seems like easy question, I know all radii are equal in length but I still didn't manage to find BC, any help?","['circles', 'geometry']"
3558449,Proof of conformal property for circle inversion,"I'm reading a College Geometry: A Unified Development [unfortunately not available through google book preview], and I came across the Theorem, that circle inversions preserve angles between two arbitrary intersecting curves The proof goes about proving, that the angle $\theta$ between the curves $C_1$ and $C_2$ (depicted in $\color{red}{\text{red}}$ and $\color{blue}{\text{blue}}$ colors respectively in the figure) is the same as the angle $\theta'$ between the images of those curves under the circle inversion. $t_1$ and $t_2$ (colored lines) are tangents to $C_1$ and $C_2$ curves respectively. $t_1'$ and $t_2'$ circles are images of $t_1$ and $t_2$ under the inversion (straight lines are mapped to circles through $O$ - the center of circle of inversion). Dashed lines near the $O$ are tangents to circles, and the solid lines near the second point of circle intersection ( $P'$ ) - are tangents to mapped curves. The proof goes by claiming that $\varphi$ - the angle between dashed tangent lines to $t_1'$ and $t_2'$ , is equal to $\theta$ (one of the properties, that were previously proved, is that tangent line to the circle at $O$ is parallel to its image under the inversion) [so far so good] ... and the angle $\theta'$ between the tangent lines to the $C_1'$ and $C_2'$ mapped curves equals the angle between the circles ( $t_1'$ and $t_2'$ )
  at point of intersection, which in turn equals to $\varphi$ ... So, once I know that black non-dashed lines are indeed tangent to circles I'm done. But why are those lines tangent to circles?","['circles', 'geometry']"
3558528,What is the expected volume of the simplex formed by $n+1$ points independently uniformly distributed on $\mathbb S^{n-1}$?,"I was surprised that I couldn’t find this question answered on this site (not for lack of trying). I need the result for answering Probability of random sphere lying inside the unit ball , so I’m posting it as a separate question and answer for future reference. What is the expected volume of the simplex formed by $n+1$ points independently uniformly distributed on $\mathbb S^{n-1}$ ? MathWorld has the answers $\frac3{2\pi}$ for $n=2$ under Circle Triangle Picking and $\frac{4\pi}{105}$ for $n=3$ under Sphere Tetrahedron Picking , but the general result is surprisingly hard to find.","['spheres', 'geometric-probability', 'volume', 'geometry', 'probability']"
3558549,Sub Rings of the Real Field $\mathbb{R}$,"Are there proper sub-rings of $\mathbb{R}$ which are not fields , which contain $\mathbb{Q}$ ? Towards answering this, I am aware of the result that proper sub rings of $\mathbb{Q}$ , properly containing $\mathbb{Z}$ are principal ideal domains. My hunch is that proper sub-rings of $\mathbb{R}$ properly containing $\mathbb{Q}$ should be a field. Can anyone help me please with the proof or with a counter example?","['ring-theory', 'abstract-algebra']"
3558767,$e^\pi - \pi^e < 1$? [duplicate],This question already has answers here : Proving that $e^{\pi}-{\pi}^e\lt 1$ without using a calculator (4 answers) Closed 4 years ago . We have Comparing $\pi^e$ and $e^\pi$ without calculating them but it doesn't give an approximation of the actual difference. Is there a way without calcualting an approximation of them to prove $e^\pi - \pi^e < 1$ ?,"['exponentiation', 'inequality', 'pi', 'real-analysis']"
3558787,Is it possible to find the distribution of $\frac{X_1-\bar{X}}{s^2}$?,"Let $X_i \sim N(\mu,\sigma^2)$ where both $\mu,\sigma$ are unknown. Is it possible to find the distribution of $\frac{X_1-\bar{X}}{s^2}$ , where $s^2$ is the sample variance. It seems difficult to guess the nature of the distribution as $X_1-\bar{X} $ has a normal distribution while $s^2$ has chi-squared distribution and both are not independent.
But is there a way to compute the distribution?","['statistics', 'probability-distributions', 'normal-distribution']"
3558810,Prove that $e^\pi+\frac{1}{\pi} < \pi^e+1$,"Prove that: $$e^\pi+\frac{1}{\pi}< \pi^{e}+1$$ Using Wolfram Alpha $\pi e^{\pi}+1 \approx 73.698\ldots$ and $\pi(\pi^{e}+1) \approx 73.699\ldots$ Can this inequality be proven without brute-force estimations (anything of the sort $e\approx 2.7182...$ or $\pi \approx 3.1415...$ )? I've just seen this and I remembered I've seen the question asked here in an older paper, but I don't remember the details. Note that this is sharper because it can be written as: $$e^{\pi}-\pi^e<1-\frac{1}{\pi}<1$$ I've tried, but none of the methods in the linked question (which study the function $x^\frac{1}{x}$ ) can be applied here.","['calculus', 'estimation', 'inequality', 'real-analysis']"
3558841,How many points uniquely determine a skew normal distribution?,"The normal distribution pdf is defined to be: $$
\phi(x| \mu, \sigma)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-\mu)^{2}}{2\sigma^2}}
$$ The cdf is given by: $$
\Phi(x)=\int_{-\infty}^{x} \phi(t) dt
$$ If we are given the value of $\phi(x)$ at three distinct points, this is sufficient to uniquely determine $\mu$ and $\sigma$ . The pdf of the general skew normal distribution is defined to be: $$
f(x)=\frac{2}{\omega} \phi\left(\frac{x-\xi}{\omega}\right) \Phi\left(\alpha\left(\frac{x-\xi}{\omega}\right)\right)
$$ How many points are needed to determine $\xi$ , $\omega$ and $\alpha$ ? Even though we have one more variable than in the non-skewed case, it looks like two skew normal distributions can only intersect in two places. So maybe we need only three again? @Rahul points out in the comments that the pdf of two skew normal distributions can in fact  intersect at three points. This leads to the guess, is four points sufficient to uniquely determine the distribution?","['calculus', 'normal-distribution', 'statistics']"
3558916,"trying to prove dot product definitions (algebraic and geometric) are equal, found inconsistency","Im trying to follow this proof https://www.youtube.com/watch?v=9aomoLESkeI The intention is to prove that the geometric and algebraic definitions of the dot product are equivalent. The author does most of the proof using mostly coordinate notation ( $<a_x,a_y,a_z>$ ), I wanted to simplify things by using vector notation ( $\vec{A}$ ) until the very end and I found I cant do it, not sure why. First I declare the law of cosines using the vectors $\vec{A},\vec{B}$ . I declare vector $\vec{C}$ to be $\vec{C}=\vec{A}-\vec{B}$ So, law of cosines: $$
|A|^2+|B|^2-2|A||B|cos(\theta)=|A-B|^2
$$ I expand the right hand side $$
\require{cancel}
\text{Expand right side: }
|A|^2+|B|^2-2|A||B|cos(\theta)=|A|^2+|B|^2-2|A||B| \\
\text{simplify: }\cancel{|A|^2}+\cancel{|B|^2}\cancel{-2}|A||B|cos(\theta)=\cancel{|A|^2}+\cancel{|B|^2}\cancel{-2}|A||B|\\
\text{then: }|A||B|cos(\theta)=|A||B|
$$ Ok so thats a funny result that could only be true if $\theta$ is $0$ whats the meaning of this? can somehow $|A||B|$ be transformed into $a_xb_x+a_yb_y+c_xc_y$ ? the proof on the video would suggest so, but i dont see how I can do that from $$
|A||B|= \sqrt{a_x^2+a_y^2+a_z^2}\sqrt{b_x^2+b_y^2+b_z^2}
$$ maybe im missing some simple algebraic trick?","['trigonometry', 'algebra-precalculus', 'linear-algebra', 'geometry']"
3558939,Distribution of cumulated intensity for Poisson process,"I've come across a statement in a textbook that is not proved, and I have a hard time coming up with the proof myself. Let $\tau$ be the first jump of a (time inhomogeneous) Poisson process with intensity $\lambda(t)$ . Define the cumulated intensity : $\Lambda(t):=\int_0^t\lambda(u) \, du$ The claim the authors make (which I want to see a proof of) is: One of the important facts about Poisson processes is a property of the jump time $\tau$ according to its own cumulated intensity $\Lambda$ . We have $\Lambda(\tau)=:\xi \sim $ exponential standard random variable. (If anyone is interested, the book is Interest Rate Models - Theory and Practice
2nd ed. by Brigo and Mercurio, where this statement is found on page 698)","['exponential-distribution', 'probability-distributions', 'probability-theory', 'poisson-process']"
3558942,Prove that $\binom{n}{0} + 2\binom{n}{1} + ...(n+1)\binom{n}{n} = (n + 2)2^{n-1}$.,"How can I prove the following identity? $$\binom{n}{0} + 2\binom{n}{1} + ...(n+1)\binom{n}{n} = (n +
 2)2^{n-1}$$ I thought about differentiating this: $$(1 + x) ^ n = \sum_{k = 0} ^ {n} \binom{n}{k}x^k$$ and then evaluating it at $x = 1$ , but I didn't get to my desired result. I just kept finding that $\displaystyle\sum_{k = 0}^n k \binom{n}{k} = n2^{n-1}$ .",['combinatorics']
3558946,Find the relative maximum and relative minimum of the function $f(x) = 49x + \frac{4}{x}$,"Find the relative maximum and relative minimum of the function $f(x) = 49x + \frac{4}{x}=49x+4x^{-1}$ Solution: Step 1: Find the values of $x$ where $f'(x)=0$ and $f'(x)$ DNE. $f'(x)=49-4x^{-2}=49-\frac{4}{x^2}$ We can see that $f'(x)$ DNE when $x=0$ . Now lets find the values of $x$ that make $f'(x)=0$ . $f'(x)=49-4x^{-2}=49-\frac{4}{x^2}=0$ $\rightarrow x^2=\frac{4}{49}$ $\rightarrow x= \pm \sqrt{\frac{4}{49}}$ $\rightarrow x= \pm \frac{2}{7}$ Thus our critical points are $x=0, x= \frac{2}{7}, x= -\frac{2}{7}$ Step 2: Make a number line and plot the sign of $f'(x)$ to find where $f(x)$ is increasing and decreasing. $f'(-3)= 49-\frac{4}{9}>0$ $f'(\frac{-1}{7})=49-\frac{4}{(\frac{-1}{7})^2}=49-4(49)=-3(49)<0$ $f'(\frac{1}{7})=49-\frac{4}{(\frac{1}{7})^2}=49-4(49)=-3(49)<0$ $f'(3)=49-\frac{4}{3^2} >0$ Thus $f(x)$ is increasing as $x$ increases towards $\frac{-2}{7}$ and then $f(x)$ is decreasing as $x$ increases towards $0$ . Therefore $x=\frac{-2}{7}$ corresponds to a relative maximum. Thus $f(x)$ is decreasing $x$ increases towards $\frac{2}{7}$ and then $f(x)$ is increases as $x$ increases towards $\infty$ . Therefore $x=\frac{2}{7}$ corresponds to a relative minimum. Step 3: find the corresponding $y$ values. $f(\frac{2}{7})=28$ $f(\frac{-2}{7})=-28$ Therefore $(\frac{2}{7},28)$ is a relative minimum and $(\frac{-2}{7},-28)$ is a relative maximum. This may be weird that our relative maximum is smaller than our relative minimum, but since these are ""relative"" maximum and minimum, they are only being compared to points on the graph very near to it, so this is okay.","['maxima-minima', 'calculus', 'solution-verification', 'derivatives']"
3559025,Questions about pseudo metric on quotient space,"Let $(X,d)$ be a metric space and $\sim$ be an equivalence relation on $X$ , then we can form the quotient space $X/\sim$ . We can also define a pseudo metric on the set of equivalence classes as follows: given two equivalence classes $[x]$ and $[y]$ , we define $$
     d'([x],[y]) = \inf\{d(p_1,q_1)+d(p_2,q_2)+\dotsb+d(p_{n},q_{n})\} $$ where the infimum is taken over all finite sequences $(p_1, p_2,
 \dots, p_n)$ and $(q_1, q_2, \dots, q_n)$ with $p_1\sim x, q_n\sim y,
 q_i\sim p_{i+1}, i=1,2,\dots, n-1$ . I guess this definition is the same as considering the complete graph whose vertex set is $X/\sim$ , put weight $\inf\{d(z,w)\mid z\sim x, w\sim y\}$ on the edge connecting $[x]$ and $[y]$ , and declare the distance between $[x]$ and $[y]$ to be the infimum of length of paths from $[x]$ to $[y]$ . It can be shown that $d'$ is a pseudo metric and the topology it induces is coarser than $X/\sim$ , i.e., it contains fewer open sets. My questions are: When is $d'$ compatible with $X/\sim$ ? I can see they are compatible when $X/\sim$ is compact and the topology induced by $d'$ is Hausdorff, namely $d'$ is a metric, in which case the identity map from $X/\sim$ to topology induced by $d'$ is a homeomorphism. The answer to this post seems to prove that $X/\sim$ is metrizable if $X$ is compact and $X/\sim$ is Hausdorff, but I do not see whether the metric can be taken to be $d'$ . Consider a collection of circles $S_n$ , each of circumference $1$ with usual metric (the length of great arc connecting two points). The disjoint union $\bigsqcup_n S_n$ is metrizable by declaring points from different circles have distance $2$ . Take the wedge sum $\bigvee_n S_n$ . It seems to me that the pseudo metric $d'$ on $\bigvee_n S_n$ is a metric, so it cannot be compatible with the quotient topology which is not first countable. It does not seem like the Hawaiian earring either. What is it?","['general-topology', 'metric-spaces']"
3559171,Finitely generated group unlikely to be generated by randomly chosen elements.,"A well known interesting fact is that if two integers are picked ""at random"" (in an appropriate asymptotic sense), the chances they generate the integers is $6/\pi^2$ . So, the integers can be generated by two randomly selected elements with non-vanishing probability. I am wondering if there exists a (finitely generated) group that is almost certainly not generated by any finite number of ""randomly chosen"" elements. That is, is there a group $G$ with generators $\{g_1, ... , g_n\}$ such that for every $k$ , our if we pick $k$ elements of $G$ at random, there is a vanishingly small probability that the selected elements generate $G$ . To clarify, select elements randomly from balls of a given radius in the group, using the word metric with given generators, and see if the probability $k$ elements chosen from the ball generate $G$ tends to $0$ as the radius of the balls grows. This definition is compatible with the above result about the integers.","['combinatorial-group-theory', 'finitely-generated', 'abstract-algebra', 'group-theory', 'probability']"
3559193,How to calculate the spiral around a curve?,"I have a curve given by a set of points and want to build a spiral around it (like this). I tried it by STEP 1: calculated the vector of each step by $$v_n = \left<x_n,y_n,z_n\right> - \left<x_{n-1},y_{n-1},z_{n-1}\right>$$ STEP 2: I assumed a vector/plane of the rotating trajectory as $$ w_n = \left<\cos(i),1,\sin(i)\right>$$ where $i$ represents the intervals and increases by the point number. And calculated the perpendicular vector to find the spiral points by $$X = v_y \cdot w_z - v_z \cdot w_y $$ $$Y = v_z \cdot w_x - v_x \cdot w_z $$ $$Z = v_x \cdot w_y - v_y \cdot w_x $$ And, of course, I normalized the scale. The spiral is formed, but I have discontinuity or deformation at sharp angles (probably because the vector direction is changed). It is not a matter of smoothness, as the spiral changes its direction to rotate in the opposite direction. Where did I do wrong?","['calculus', 'vectors', 'vector-spaces', 'geometry']"
3559230,Is P(A and B) = P(B and A)?,I was wondering if the following statement is true and if there are any times in which the following is not true: Is the probability of A and B equal to the probability of B and A? P(A and B) = P(B and A)? Would it be suffice to show that they are equal based on P(A and B) representing the same intersection as P(B and A) on a venn diagram?,['probability']
3559255,Asymptotic expansion of $f(x)=\sum _{n=1}^{\infty } \frac{\sin \left(\sqrt{n}x\right)}{n}$ at the origin,How can we prove that the following function is well-defined (i.e. the series converges) for all $x\in\mathbb{R}$ ? $$f(x)=\sum _{n=1}^{\infty } \frac{\sin \left(\sqrt{n}x\right)}{n}$$ Is it possible to obtain an asymptotic expansion of $f$ near $0+$ ? Here is a relevant problem.,"['fourier-series', 'limits', 'asymptotics', 'sequences-and-series']"
3559296,Function field of integral scheme,"Let $X$ be an integral scheme. For any affine open $U = \operatorname{Spec}(A)$ the field of fractions $K(A)$ of $A$ is the stalk of $X$ at its generic point. This is called the function field of $X$ and denoted by $K(X)$ . Now let $U$ be some open set of $X$ , not necessarily affine. Then the field of fractions $K(\mathcal{O}_X(U))$ of the ring of sections on $U$ is a subfield of $K(X)$ because the restriction maps of the structure sheaf of $X$ are injective. But is it true that $K(\mathcal{O}_X(U)) = K(A)$ ? I have the feeling that this is not true, while if we take the sheaf associated to the presheaf $U \mapsto K(\mathcal{O}_X(U))$ , then we do get the constant sheaf associated to $K(X)$ . On the other hand, the wikipedia article https://en.wikipedia.org/wiki/Function_field_(scheme_theory) states ''the fraction fields of the rings of regular functions on any open set will be the same"".","['algebraic-geometry', 'schemes']"
3559345,What axiom system for the complex numbers is categorical?,"A theory is categorical if it has a unique model up to isomorphism.  First-order Peano arithmetic is not categorical, but second-order Peano arithmetic is categorical, with the natural numbers as its unique model.  The first-order theory of real closed fields is not categorical, but the second-order theory of Dedekind-complete ordered fields is categorical, with the real numbers as its unique model.  ZFC is not categorical, but Morse-Kelley Set Theory with an appropriate axiom about inaccessible cardinals is categorical. My question is, what theory of the complex numbers is categorical?  The first order theory of algebraically closed fields of characteristic zero is not categorical, because both the field of algebraic complex numbers and the field of complex numbers satisfy it.  So is there some second-order axiom we can add to this theory to make it categorical?","['model-theory', 'logic', 'real-analysis', 'complex-numbers', 'second-order-logic']"
3559374,How to know if the solution given by Runge Kutta is good?,"We're trying to solve a nonlinear ODE problem using the Runge-Kutta method (4th order). How to see how accurate the solution is? Since we don't know the exact solution, there is nothing to compare the numerical solution to. We thought about decreasing the step size and comapring solutions for varying step sizes, but I don't think that makes sense since different step sizes give vectors of different lengths as solutions. Basically, we solved a system of nonlinear ODEs using Runge-Kutta, and don't know how good the solution is. Any suggestions?","['runge-kutta-methods', 'numerical-methods', 'ordinary-differential-equations']"
3559383,Gaussian integral over all possible real matrices,"I am trying to compute the following gaussian integral over all possible real matrices $J$ : $$I=\int \exp\left\{-\frac{N}{2}\text{Tr}\left[\mathbf{J}\mathbf{A}\;\mathbf{J}^T+2\mathbf{BJ}-\gamma \mathbf{JJ} \right]\right\}\mathrm{d}\mathbf{J}$$ Where $\mathbf{A}$ and $\mathbf{B}$ are Hermitian matrices. When $\gamma=0$ I can complete the square and integrate this Gaussian integral without any problem (assuming I know the eigenvalues and determinant of $\mathbf{A}$ ): $$\mathbf{J}\mathbf{A}\;\mathbf{J}^T+2\mathbf{BJ}=\left(\mathbf{J}^T-\mathbf{B}\mathbf{A}^{-1}\right)\mathbf{A}\left(\mathbf{J}-\mathbf{A}^{-1}\mathbf{B}\right)-\mathbf{B}\mathbf{A}^{-1}\mathbf{B}$$ However for general $\gamma\in \mathbb{R}$ I cannot seem to know how to evaluate this integral by completing the square: $\mathbf{J}^T\mathbf{A}\;\mathbf{J}+2\mathbf{BJ}-\gamma \mathbf{JJ}$ $\mathbf{J}$ is real but not symmetric. when $\gamma=0$ this integral converges so I do not see any reason why it would not be generalised to general $\gamma$ with an appropriate $\mathbf{A}$ . Any remark or advice is always appreciated. Thank you. Edit : A different way to express the integral $I$ is the following: $$I=\int \left(\prod_{ij}\mathrm{d}J_{ij}\right)\exp\left\{-\frac{N}{2} \sum_{i, j, k} J_{k i} A_{i j} J_{k j}+N\sum_{k, j} B_{k j} J_{k j}+\frac{N\gamma}{2}\sum_{ij}J_{ij}J_{ji}\right\}$$ Assuming I already know the eigenvalues of $\mathbf{A}$ and thus $\det(\mathbf{A})$ , how can I compute the integral $I$ ?","['matrices', 'gaussian-integral']"
3559416,Matrix exponential of sum of matrices,"I'm having trouble understanding why, in general, if $\mathbf A, \mathbf B$ are $n \times n$ matrices, it does not hold that $$\exp(\mathbf A + \mathbf B) = \exp(\mathbf A)\exp(\mathbf B)$$ but holds in general when $\mathbf A\mathbf B = \mathbf B \mathbf A$ . Specifically, I can try to prove this using the definition of matrix exponentials as follows, without using commutativity of the matrices. Notice that \begin{align*}
\exp(\mathbf A + \mathbf B) &\equiv \sum_{i=0}^{\infty} \frac{(\mathbf A + \mathbf B)^i}{i!} \\
&= \sum_{i=0}^{\infty} \sum_{j = 0}^{i} \binom ij \frac{\mathbf A^{i-j}\mathbf B^{j}}{i!} \\
&= \sum_{i=0}^{\infty} \sum_{j = 0}^{i} \frac{\mathbf A^{i-j}}{(i-j)!} \frac{\mathbf B^j}{j!}
\end{align*} which does not rely on the commutativity of multiplying $\mathbf A, \mathbf B$ , but only relies on the commutativity of matrix addition under absolute convergence. Meanwhile, \begin{align*}
\exp(\mathbf A)\exp(\mathbf B) &\equiv\left(\sum_{k = 0}^{\infty} \frac{\mathbf A^k} {k!} \right)\left(\sum_{l=0}^\infty \frac{\mathbf B^l}{l!}\right)\\
&= \sum_{k=0}^\infty\sum_{l=0}^\infty \frac{\mathbf A^k}{k!} \frac{\mathbf B^l}{l!}
\end{align*} which still does not rely on the commutativity of multiplying the two matrices.
However, we can see that both sums converge (because the matrix exponential always converges) and also that we can rewrite the first expression to be equal to the second one, because we can set any values for $k, l$ and see that an equivalent term occurs in the first sum, and vice versa. Why does this fail when $\mathbf A, \mathbf B$ do not commute?","['matrices', 'matrix-exponential', 'linear-algebra', 'taylor-expansion']"
3559507,Origin of the formula $\partial\log\det X=\operatorname{Tr}\left(X^{-1}\partial X\right) $,I found that for any first order differential operator $\partial $ acting on a square matrix $X$ we have $$\partial\log\det X=\operatorname{Tr}\left(X^{-1}\partial X\right) $$ I'm quite sure it origins from $$ \det(\exp (X))=\exp (\operatorname{Tr}(X)) $$ But I get lost with the operator. I don't get how we can insert the differential inside the trace. Can anybody help?,"['analysis', 'matrices', 'matrix-calculus', 'linear-algebra', 'lie-groups']"
3559511,Determine whether $\sqrt[n]{1+\sqrt[n]{2+\sqrt[n]{3+\cdots+\sqrt[n]{n}}}}$ diverges or not,"Determine whether $\{a_n\}$ is convergent or not, where $$a_n:=\sqrt[n]{1+\sqrt[n]{2+\sqrt[n]{3+\cdots+\sqrt[n]{n}}}}.$$ At least, we can obtain $$\sqrt[n]{1+\sqrt[n]{2+\sqrt[n]{3+\cdots+\sqrt[n]{n}}}}\le\sqrt{1+\sqrt{2+\sqrt{3+\cdots+\sqrt[n]{n}}}}<2,$$ which imlies $\{a_n\}$ is bounded. But is it monotonic?","['nested-radicals', 'limits', 'calculus']"
3559520,"Proof that If $f:X\to Y$ is surjective and $X$ is finite, then $Y$ is finite without AC","I try to prove the following exercise without using the axiom of choice. On the second item, the proof that I found in the books, to define the function $g:Y\to X$ , for every $y\in Y$ , due to the surjective of $f$ they choose $x=g(y)\in X$ such that $f(x)=y$ . Let $f:X\to Y$ , prove. If $Y$ is finite and $f$ injective, then $X$ is finite. If $X$ is finite and $f$ surjective, then $Y$ is finite. Proof: Done. Let $X=\{x_1,\ldots,x_n\}$ . Given any $y\in Y$ , because $f$ is surjective, then exists $k\in I_n=\{1,\ldots,n\}$ such that $y=f(x_k)$ . Consider the set $$A_y:=\{i\in I_n:f(x_i)=y\}$$ Then $A_y\neq\emptyset$ because $k\in A$ , so, by the Well-ordering principle, $A_y$ has a minimal element, let this element $i_y$ , then $$f(x_{i_y})=y.$$ Define $g:Y\to X$ by $g(y):=x_{i_y}$ . Im particular we get $$f(g(y))=f(x_{i_y})=y,\quad \forall y\in Y.$$ Prove that $g$ is injective. Let $y,z\in Y$ such that $g(y)=g(z)$ , then $$g(y)=g(z)\Longrightarrow f(g(y))=f(g(z)) \Longrightarrow  y=z.$$ Therefore, $g$ is injective, then by the first item, $Y$ is finite. Is it right?","['functions', 'discrete-mathematics']"
3559547,When is the image of a line bundle again a line bundle,"Hello everybody Motivation of my question Let $X$ be a scheme. Given a morphism $\mathcal{L}\overset{\beta}\to\mathcal{O}_X$ of line bundles over $X$ . I want to understand under what conditions the image $\mathrm{Im}(\beta)$ is a line bundle. I will specify my question below. To avoid trivialities, lets assume the morphism is not surjective. I already know the following results If $X$ is a smooth curve over a field, then every non-zero coherent $\mathcal{O}_X$ -submodule of a line bundle is already a line bundle*. 
Because $\mathrm{Im}(\beta)$ is coherent and non-zero, we see that $\mathrm{Im}(\beta)$ is a line bundle in this case. If $X$ has the property, that every local ring $\mathcal{O}_{X,x}$ is a PID, then 
the submodule $\mathrm{Im}(\beta)_x=\mathrm{Im}(\beta_x)\subseteq \mathcal{O}_{X,x}$ is free of rank $1$ , which implies that $\mathrm{Im}(\beta)$ is free, by finite presentedness of $\mathrm{Im}(\beta)$ . My question What if $X$ is a smooth scheme over a base scheme $S$ , say of relative dimension $1$ , for example? I am mainly interested in the case of a morphism $\mathcal{L}\overset{\beta}{\to} \mathcal{O}_{X_S}$ (non-surjective), where $X_S=S\times_{\mathrm{Spec}{k}}X$ where $S$ is any scheme over a field $k$ and $X$ is a smooth irreducible curve over $k$ . Under what conditions is the image $\mathrm{Im}(\beta)$ a line bundle? Some of my ideas Maybe one can prove that the restriction of $\mathrm{Im}(\beta)$ to the fibres $X_S\to \mathrm{Spec}(\kappa(s))$ are line bundles, by using the first result from above and then conclude, that $\mathrm{Im}(\beta)$ is a line bundle?
Alternatively one could try to understand the properties of the local rings $\mathcal{O}_{X_S,x}$ , but I have doubts that they have any useful properties? PS: *This follows by the fact that the stalks are valuation rings and that torsion-free modules over valuation rings are flat by Tag 0539 , and the fact that over finitely generated flat modules over a local ring are free by [Matsumura, Commutative Algebra, Prop. 3G, page 21], and a standard Argument about finitely presented modules.","['algebraic-geometry', 'coherent-sheaves', 'line-bundles']"
3559550,Automorphisms of the Complex Field and Model Theory,"A friend of mine has recently drawn my attention to a seemingly shocking result by the mathematician Joel David Hamkins The real numbers are not interpretable in the complex field about the possibility of interpret the real numbers in the complex field by using only the field structure of $\mathbb{C}$ . After a little thought the result seems not so strange to me anymore, but the ""elementary proof"" given by Hamkins in the quoted page contains two statements which I do not know. (I) For any $z \in \mathbb{C}$ , any two complex numbers transcendental over $Q(z)$ are automorphic in $\mathbb{C}$ by an automorphism fixing $z$ . (II) Any k-tuples $x \in \mathbb{C}^k$ and $y \in \mathbb{C}^k$ that exhibit the same algebraic equations over $\mathbb{Q}(p_1,\dots,p_n)$ will be automorphic by an automorphism fixing $(p_1,\dots,p_n)$ . My knowledge of abstract algebra is quite elementary, let us say at the level of Michael Artin's Algebra, and I cannot understand exactly the meaning of these two statements. Could someone give me some reference where I could find them and their proof? Thank you very much in advance for your great help.","['field-theory', 'model-theory', 'abstract-algebra', 'complex-numbers']"
3559614,"Evaluate $\int_{(-\infty,\infty)^n}\frac{\prod_{k=1}^n \sin(a_k x_k)}{\prod_{k=1}^n x_k}\frac{\sin(\sum_{k=1}^n a_k x_k)}{\sum_{k=1}^n a_k x_k}$","Suppose $a_1, \cdots, a_n>0$ , how to evaluate $$\int_{(-\infty,\infty)^n}\frac{\prod_{k=1}^n \sin(a_k x_k)}{\prod_{k=1}^n x_k}\frac{\sin(\sum_{k=1}^n a_k x_k)}{\sum_{k=1}^n a_k x_k}dx_1\cdots dx_n$$ Any help will be appreciated.","['integration', 'definite-integrals']"
3559619,Proving inverse image is a normal subgroup,"Let $\Phi: G \to G'$ be a group homomorphism, and $H' \lhd G'$ . We need to prove that $\Phi^{-1} (H') \lhd G$ . Here is my current attempt at this. I have already proved that $\Phi^{-1} (H') \leq G$ , so the only thing left to show is that $\Phi^{-1} (H')$ is closed under conjugation by $G$ . By definition, $$\Phi^{-1}(H') = \{g \in G' \mid \Phi(g) \in H' \}.$$ Let $a \in \Phi^{-1}(H')$ and $b \in G'$ . Consider the element $bab^{-1}$ . We will show $bab^{-1} \in \Phi^{-1} (H')$ , which is true if and only if $\Phi(bab^{-1}) \in H'$ . By the homomorphism property of $\Phi$ , we have $$\Phi(bab^{-1}) = \Phi(b) \Phi(a) \Phi(b^{-1}) = \Phi(b) \Phi(a) \Phi(b)^{-1}.$$ This is where I am stuck. If $\Phi$ were onto, this would be fine. We'd be able to write any element of $G'$ as $\Phi(b)$ for some $b \in G$ . We already have $\Phi(a) \in G'$ . So this product would live in $H'$ since $H'$ is a normal subgroup. Absent $\Phi$ being onto, how can I finish the proof?","['group-homomorphism', 'proof-explanation', 'group-theory', 'normal-subgroups']"
3559685,Does the vanishing of the Lie derivative of the metric at a single point imply that the associated flow is isometric at some point?,"Let $(M,g)$ be a smooth Riemannian manifold, and let $X \in \Gamma(TM)$ be a smooth compactly supported vector field on $M$ . Suppose that $(L_X g)(p)=0$ for some specific point $p \in M$ . Let $\phi_t$ be the flow of $X$ . Is it true that for every $t$ , $(d\phi_t)_{q(t)}$ is an isometry for some suitably chosen point $q(t)$ ? Is it true for $q(t)=p$ ? The point is that if we know that $L_Xg=0$ everywhere, i.e. $X$ is Killing, then $\phi_t$ is a global isometry. However, from inspecting the proof, it does not seem ""localizable"" (i.e. I think that the vanishing of $L_Xg$ at a point should not imply that the flow is an isometry, not even at a single point. But I don't know how to construct an example.)","['riemannian-geometry', 'vector-fields', 'differential-topology', 'symmetry', 'differential-geometry']"
3559707,Probability of listening to a song in a shuffled playlist at time $t$?,"Suppose we have a playlist of $N$ songs, with durations $\{t_1, t_2, \dots, t_N\}$ . The list is set to shuffle indefinitely. This means that, after each song is finished playing, the next song is chosen uniformly at random, so each song, including the one that just played, has an equal probability of playing, regardless of their duration. When the playlist is started, it chooses a song uniformly at random. Let $P_j(t)$ be the probability that the playlist would currently be playing song $j$ after time $t$ has passed since beginning the playlist. Intuitively, I'm sure that in the limit of large $t$ , $P_j(t)$ must converge towards $t_j/\sum_i t_i$ . But is that convergence asymptotic, or is there a hard cutoff time after which that is the distribution? For example, is this the distribution for all $t > \max(\{t_i\})$ ? If so, it's not obvious to me why this might be the case. Also I know that, for all $t < \min(\{t_i\})$ the probability distribution must be uniform. What is $P_j(t)$ in its general form? (Not just the above limits). Is there an analytic form? My attempt: Without loss of generality, order the songs in ascending duration, so $t_1 \le t_2 \le \dots \le t_N$ . But with some loss of generality, let's assume that no two songs have equal duration (a reasonable assumption for continuous time), so $t_1 < \dots < t_N$ . As we know, for $t<t_1$ , the probability distribution is uniform. For $t_1 < t < t_2$ , the only possible song that could have completed is the first song. Assuming $t < 2 t_1$ , $P_1(t) = 1/N^2$ . This is because the only way song $1$ can be playing is if song $1$ was rolled twice in a row. For $j \ne 1$ , $P_j(t) = \frac{N+1}{N^2}.$ That is, either they were rolled initially and are still playing, or song $1$ was rolled, and then song $j$ was rolled afterwards. Now, this doesn't cover the general case for $t_1 < t < t_2$ , only for $t < 2 t_1.$ However, $t_2$ can be any multiple of $t_1$ in principle. Suppose, for example, that $t_2 = 6.5 t_1$ , and $t = 5.5 t_1$ . Then the only way song $1$ can be playing at time $t$ is if it was rolled six times in a row. Thus $P_1(t) = 1/N^6.$ And $P_j(t) = \frac{N^5 + N^4 + N^3 + N^2 + N + 1}{N^6}$ for $j \ne 1$ . This can go on and on, for $t_2$ being any arbitrary multiple of $t_1$ . So it seems like that must be specified: For $(k-1) t_1 < t < k t_1 \le t_2$ , $$P_1(t) = 1/N^k$$ and $$P_j(t) = \frac{\sum_{i=0}^{k-1} N^i}{N^k}; \hspace{1mm} j \ne 1.$$ This approach shows why it's problematic to go anywhere beyond $t = t_2$ . For any interval $t_j < t < t_{j+1}$ , there will be a different discrete expression every time $t$ passes any sum of earlier times. For example, if we're evaluating the probability for $t_8 < t < t_9$ , and the sum $t_2 + 2 t_4 + t_7$ falls within this range, then there will be a different expression before and after that time, because there's a chance that the playlist just finished that sequence. Thus, I believe that a closed-form expression for $P_j(t)$ would take the form of an intractable number of disjoint expressions for different ranges, and for arbitrary $\{t_1, \dots, t_N\}$ it might be impossible to even show an expression, because this involves an arbitrary number of disjoint expressions. If that's true, though, then an answer may still be given for my question to the large- $t$ limit.","['markov-process', 'probability']"
3559753,On the definition of tangent vector on a smooth manifold,"Let $M$ be a smooth manifold and $a \in M$ . A tangent vector to $M$ at $a$ is a linear map $$A: C^\infty(M) \rightarrow \mathbb R$$ such that $$A(fg)=A(f)g(a) + f(a)A(g)$$ I am trying to understand this definition in $\mathbb R^n$ . Let $f\in C^\infty(\mathbb R^n,\mathbb R)$ . The tangent plan to the graph of $f$ at point $a$ is $$ T_a \Gamma_f =\{(x,f(a)+\langle\nabla f(x),(x-a)\rangle) | x \in \mathbb R^n \}$$ So what is $A$ in this context ? A ""tangent vector"" should be an element of the ""tangent plan"" no ? So I thought it would be $A(f)=f(a)+\langle\nabla f(x),(x-a)\rangle$ but it does not respect the Leibniz product rule from the definition.","['differential-geometry', 'geometry', 'real-analysis']"
3559764,"Finding all angles that satisfy $8 \cos ^{3} \theta-6 \cos \theta+1=0 \quad \text { for } \theta \in[-\pi, \pi]$","$\text { Hence, solve the equation } 8 \cos ^{3} \theta-6 \cos \theta+1=0 \quad \text { for } \theta \in[-\pi, \pi]$ The previous part was to prove that $\cos 3 \theta=4 \cos ^{3} \theta-3 \cos \theta \quad \text { by replacing } 3 \theta \text { by }(2 \theta+\theta)$ . So, I used this to simplify the equation to $2 \cos 3 \theta +1 = 0$ $\implies \cos 3 \theta =\frac{-1}{2}$ Since $\cos^{-1} \frac{-1}{2} = \frac{2\pi}{3} + 2 \pi n$ , $\implies \theta =\frac{2 \pi}{9},\frac{8 \pi}{9},\frac{-8 \pi}{9}$ or $\frac{-2 \pi}{9}$ . However, the graph seems to be showing another root which is $\frac{4 \pi}{9}$ . Why did I miss this root? How should I find more angles that satisfy the equation in a given range. In general, how does one find all angles that satisfy an equation even after adding $2 \pi n$","['algebra-precalculus', 'roots', 'trigonometry']"
3559875,Why is the subgroup generated by a subset $U$ defined as the set of *finite* combinations of elements of $U$? Why discard the infinite ones?,"From Wikipedia: a generating set of a group is a subset such that every element of the group can be expressed as a combination (under the group operation) of finitely many elements of the subset and their inverses. I'm confused because there are examples of combinations of infinitely many elements of a subset which converge to an element that cannot be represented as a finite combination, and I feel like that element should be in the generated subgroup. Why shouldn't it? As an example of what I mean, take $(\Bbb R, +)$ and its subset $\{\frac 1{n!} \mid n \in \Bbb N \}$ . Why shouldn't $e$ be in the generated subgroup?","['group-theory', 'abstract-algebra']"
3559942,An indeterminate limit form of infinity/infinity,"I am trying to solve the limit: $$\lim_{x\to\infty}x^\frac{5}{3}\left(\left(x+\sin\left(\frac{1}{x}\right)\right)^\frac{1}{3}-x^\frac{1}{3}\right)$$ I was trying to find a way to bring it into a fraction form to apply L'Hospital's rule, and I tried using $$a-b=\frac{a^3-b^3}{a^2+ab+b^2}$$ But it made it even more complex and after applying L'Hospital's rule I got stuck with all the terms. Is there a smarter way to evaluate it?","['indeterminate-forms', 'limits', 'calculus']"
3560029,Thue's proof of finitely many integer solutions to $x^2 - y^3 = c$,I believe I read somewhere that a mathematician named Thue in 1902 proved that the equation $x^2 - y^3 = c$ has only finitely many integer solutions for nonzero integer $c$ . Does anyone know where I can find the original paper or an exposition of his original proof? I am interested in looking up the proof since it was proved early before many advanced algebraic machinery I was wondering if I could study his original proof and understand it since my background is very modest.,['number-theory']
3560170,"What can using the ""opposite"" combination for integration by parts be used for","For example, $$I=\int xe^{x}dx$$ By taking the derivative of x, and then repeating integration by parts once, the integral can be evaluated trivially. However, when taking the derivative of $e^{x}$ and integrating $x^2$ , the process goes on forever. Another example would be $$J=\int \sin{x}\cos{x}dx$$ By repeating integration by parts, $$J= sin^2x + cos^2x + sin^2x + cos^2x ....$$ Does this have any use/any interesting links? e.g. could be used to evaluate integrals where other ""combination"" of choosing which term is differentiated/integrated is not plausible.",['integration']
3560211,Positive-definiteness of matrix with $|a_{ij}| \leq \frac{1}{ij}$,"Let $A$ be a $N\times N$ -matrix with elements $$
a_{ii}=1 \quad\text{and}\quad
a_{ij} = \frac{1}{ij}  \quad\text{for}~ i\neq j.
$$ Then $A$ is positive-definite, as can be easily seen from $$
x^T A x = \sum_i x_i^2 + \sum_{i \neq j} \frac{x_i x_j}{ij} 
\geq
\sum_i \frac{x_i^2}{i^2} + \sum_{i \neq j} \frac{x_i x_j}{ij} 
=
\left(\sum_i \frac{x_i}{i}\right)^2 \geq 0.
$$ Assume now that $A$ is a real symmetric $N\times N$ -matrix with elements $$
\tag{1}
a_{ii}=1 \quad\text{and}\quad
|a_{ij}| \leq \frac{1}{ij}  \quad\text{for}~ i\neq j.
$$ Is it possible to show that $A$ is also positive-definite (or positive-semidefinite)? Here I posted a refinement of this question by assuming $0 \leq a_{ij} \leq \frac{1}{ij}$ for $i \neq j$ in $(1)$ .","['matrices', 'linear-algebra', 'positive-semidefinite', 'positive-definite']"
3560271,Solving an optimal stopping problem with pulling a card of a certain color,"Given a deck of 52 card, 26 black and 26 red, the player pulls cards one by one, seeing each pulled card's color. At any moment the player can stop and pull the final card. If this card is red, he wins, he loses otherwise. Since everything is finite and discrete, I considered using dynamic programming to brute-force this task, calculating the mean of an indicator variable of winning for each $r$ and $b$ , the amount of red and black cards pulled, respectively. Stopping if the expected value in the current moment is higher than the expected value after pulling the next cards seems to be the optimal solution, but I couldn't prove that rigorously. Is there a more elegant solution than brute force? A proof for my solution would be also appreciated.","['gambling', 'decision-theory', 'card-games', 'discrete-mathematics', 'probability']"
3560326,"Two continuous functions over a closed, bounded and Jordan-Measurable set: Show that its graph Jordan-Measurable","Let f,g: $A\subset\mathbb{R}^{n}\mapsto\mathbb{R}$ continuous functions over the Jordan-measurable, bounded and closed set A, such that $f(x)\leq g(x) \forall x\in A$ Show that $B=\{(x,y)\in\mathbb{R}^{n+1}|f(x)\leq y \leq g(x)\}$ is Jordan-measurable in $\mathbb{R}^{n+1}$ I have problems with the proof. I don't know how to complete this but we know that if the function is continuous, then f is uniformly continuous, then for every $\epsilon>0$ there is a $\delta>0$ such that if $|x-y|<\delta \implies |f(x)-f(y)<\epsilon|$ .Then maybe we can cover the graph with a finite number of rectangles but, to use this argument I must prove that the graph is continuous I guess... I will appreciate any help to end this proof because I'm a little lost.","['multivariable-calculus', 'measure-theory', 'real-analysis']"
3560371,2nd-order ODE Solution for when Discriminant = 0,"My textbook says the following: Knowing $y(t)=ce^{rt}$ is a solution for $ay''+by'+cy=0$ , we find for the case of $b^2-4ac=0$ that $r=\frac{-b}{2a}$ . Therefore one particular solution is $y(t) = \exp\left(\frac{-b}{2a}t \right)$ . However, another linearly independent solution exists and that is $y(t)=t\exp\left(\frac{-b}{2a}t \right)$ . The derivation to find the second equation is that we can say $y(t)=ce^{rt}=v(t)\exp\left(\frac{-b}{2a}t \right)$ with variation of parameters, and then plug that $v(t)\exp\left(\frac{-b}{2a}t \right)$ into the equation (1) $$ay''+by'+c=0 \tag{1}$$ we find that $v''(t) = 0$ and therefore v(t) is a linear function chosen to be $v(t)=t$ . Hence the 2nd solution is $y(t)=te^{rt}$ and solution space is $y(t)=c_1e^{rt}+c_2te^{rt}$ . I have plugged $v(t)\exp\left(\frac{-b}{2a}t \right)$ many times into equation (1) yet can never seem to get the conclusion that $v''(t) = 0$ . What does my textbook mean by this? CITATION: page 213, Differential Equations & Linear Algebra, Second Edition, by Farlow, Hall, McDill, West","['linear-algebra', 'ordinary-differential-equations']"
3560456,Does the limit of the union (intersection) of 2 sets equal the union (intersection) of the limits?,"Suppose we have two sequences of sets $A_n$ and $B_n$ such that $A_n \to A$ and $B_n \to B$ i.) Does $A_n \bigcup B_n \to A \bigcup B$ and ii.) Does $A_n \bigcap B_n \to A \bigcap B$ If not, is there a counterexample? $$$$ I don't think the limit should hold. $$$$ For example, $\liminf_{n\to\infty} A_n \bigcup B_n \neq \liminf_{n\to\infty} A_n \bigcup \liminf_{n\to\infty} B_n$ . This is apparent if we take $A_n = \{{(-1)^n}\}$ and $B_n = \{{(-1)^{n+1}}\}$ . The same $A_n$ and $B_n$ show that $\limsup_{n\to\infty} A_n \bigcap B_n \neq \limsup_{n\to\infty} A_n \bigcap \limsup_{n\to\infty} B_n$ . On the other hand, per Proof that lim sup of union equals union of lim sup , the relation holds for $\limsup$ in the case of union and $\liminf$ in the case of intersection. I've also worked through both of those cases but am stuck here, and I'm not sure what I'm missing.","['limits', 'measure-theory', 'probability-theory']"
3560457,Maximum principle of a differential Equation,"Consider  the boundary value problem $$g(x) - g’’(x) = f(x), \qquad x \in (0,1),$$ with boundary conditions $g(0)=g(1)=0$ and $f$ beeing a function in $C([0,1])$ . My question is, how can I show from the information above that $$||g||_{\infty}\le \frac{1}{7}||f||_{\infty},$$ and that this implies that there is an unique solution for $f\in C([0,1])$ ? ( $||\cdot||_{\infty}$ denotes the $\infty$ -norm) I know there is a proposition that applies for the Poisson’s equation on the form $$-g’’(x) = f(x) \quad x \in (0,1) \quad g(0)=g(1)=0$$ That states that $||g||_{\infty}\le \frac{1}{8}||f||_{\infty}$ . I did not however manage to apply this in order to solve the given equation. Anyone who can help me?","['ordinary-differential-equations', 'real-analysis']"
3560459,"Can we make sense of $\frac{\partial f(x_1, x_2, \dots, x_n)}{\partial g(x_1, x_2, \dots, x_n)}$?","Let $f, g \in \mathbb R^n \rightarrow \mathbb R$ . Is there some way to make sense of the expression: $$\frac{\partial f(x_1, x_2, \dots, x_n)}{\partial g(x_1, x_2, \dots, x_n)}$$ I want some way to measure ""how much $f$ changes along $g$ "" --- I'm not sure what a reasonable definition of this. Here is one that came to mind. Let us define $\frac{\partial f}{\partial g}\big(t \big)$ : At each point $t \in \mathbb R^n$ , we first compute $g'(t) \in \mathbb R^n$ . Now, we compute the directional derivative of $f$ along $g'(t)$ : $$
\frac{\partial f(x_1, x_2, \dots x_n)}{g(x_1, x_2, \dots x_n)} (t) : \mathbb R^n \rightarrow \mathbb R\equiv (\nabla_{g'(t)} f)(t) = \lim_{h \rightarrow 0} \frac{f(t + hg'(t)) - f(t)}{h}$$ Is this a well-known idea? If so, what is it called? My problem with this is that it returns a scalar at each point: what I am actually interested in is to find a new function which tells me ""how to move $f$ infinitesimally so we can make it closer to $g$ . The given definition above clearly generalizes to any manifold: all I need is a directional derivative, which I do possess on a manifold: Can we say something interesting about this in a larger setting?","['partial-derivative', 'calculus', 'derivatives', 'differential-geometry']"
3560463,How to find total derivative of function $f$ from $\mathbb R^2$ to $\mathbb R$,"I'm trying to find the total derivative $Df(x,y)$ of $f(x,y)=y\cos x$ . I'm familiar with this definition mostly: https://en.wikipedia.org/wiki/Total_derivative#The_total_derivative_as_a_linear_map I believe there is a theorem that states that if the partial derivatives exist and are continuous, then $f$ is differentiable. I haven't learned this theorem in class however, and would like to avoid using it if possible. Is there any easy ways to see if this derivative even exists on $\mathbb R^2$ ?","['normed-spaces', 'analysis', 'real-analysis', 'multivariable-calculus', 'derivatives']"
3560519,Is there a global definition of the tangent bundle?,"The tangent bundle of a smooth manifold is usually defined by equipping the disjoint union of the tangent spaces with a smooth structure. Is there a way to define the tangent bundle as a vector bundle first, and then obtain the tangent spaces as the fibres?","['vector-bundles', 'tangent-bundle', 'smooth-manifolds', 'differential-geometry']"
3560554,Showing a sufficient statistic is not complete,"Let $X_1.....X_n$ be independent Normal( $\theta, \theta^2$ ) for $\theta >0$ . Find a minimal sufficient statistic. Is it complete? I have a minimal sufficient statistic, $T=(\sum_{1}^{n}X_i,\sum_{1}^{n}X_i^2$ ), but cannot show it is complete.  I suspect I can find a counterexample to the completeness condition, but I have no intelligent strategy other than guessing random functions with expectation $0$ . A sketch or even starting point would be helpful. Edit:
Duplicate of Not complete but minimal sufficient statistic But I would still like some intuition as to how one would come up with such a function.","['statistical-inference', 'statistics']"
3560596,can the number $x^2 +y^2$ when with x and y positive integers,"can the number $x^2 +y^2$ when with $x$ and $y$ positive integers,
  end in $03$ ? I know that $x^2+y^2$ can never end with unit digit 03 but am not sure how would I show the proof of that.",['discrete-mathematics']
3560625,Proving $\sum _{k=1}^n \frac{(-1)^{k-1} 16^k (k-1)! k! (k+n-1)!}{((2 k)!)^2 (n-k)!}=\frac{4}{n}\sum _{k=1}^n \frac{1}{2 k-1}$,"How can one prove $$
\sum_{k = 1}^{n}\frac{\left(-1\right)^{k - 1}\, 16^{k}\,
\left(k - 1\right)!\, k!\, \left(k + n - 1\right)!}
{\left[\left(2k\right)!\right]^{\, 2}\,\left(n - k\right)!} =
\frac{4}{n}\sum_{k = 1}^{n}\frac{1}{2k - 1}
$$ I was given this without proof, but only a hint (to evaluate $\int_{0}^{\pi/2}\frac{2}{n}\,\frac{1 - \cos\left(2nx\right)}{\sin\left(x\right)} \, dx$ in $2$ ways) instead. By induction the integral is easily seen equivalent to RHS, but I wonder how on earth it's related to LHS. Any help will be appreciated.","['integration', 'summation', 'definite-integrals', 'special-functions']"
3560657,"Cardinality of sets - Even numbers, naturals, real","Pardon me for asking a question that appears to have been asked many times before, but after browsing several questions here what I found were a bunch of questions that were tangential to my inquiries, however they didn’t really strike exactly what I meant. I am quite new to set theory. I am currently studying the concept of cardinality, where so many have an existential crisis. When it comes to infinite sets, we say two sets have equal cardinality when it’s possible to establish a bijective correspondence between them. After having the initial shock that Card( $\mathbb{N}$ ) equals Card(Even), I’m trying to wrap my mind around the intuition as to why this is the case, but the cardinality of the reals is greater than that of natural numbers. I understand, in one case a bijection is possible, in the other it is not, but intuitively that does feel enough. I’m not looking for a proof, to be honest, there are enough of them on the internet, it’s more like an intuition as to why there is a difference between the cases that I wanted. If anyone could help me, I would be deeply grateful.",['elementary-set-theory']
3560671,"$\int\limits_0^1 \frac{\sqrt{x-x^3} \log (x)}{x \left(x^2+1\right)} \, dx+\int\limits_0^{\frac{\pi }{2}} \frac{x \sqrt{\cos (x)}}{\sin (x)} \, dx$","How to prove the (numerically correct) identity $$\int\limits_0^1 \frac{\sqrt{x-x^3} \log (x)}{x \left(x^2+1\right)} \, dx+\int\limits_0^{\frac{\pi }{2}} \frac{x \sqrt{\cos (x)}}{\sin (x)} \, dx
=-\frac{\pi ^2}{4}$$ I tried elementary techniques but none of them works. Maybe a clever contour integration is what we need? Any help will be appreciated.","['integration', 'complex-analysis', 'definite-integrals']"
3560675,Prove that a linear map is an isomorphism iff the image set of the basis of V is a basis of W.,"So, here's the result that I'm trying to prove: Let $V$ and $W$ be vector spaces over $F$ and let $(v_1,v_2,...,v_n)$ be a basis of $V$ . Then, a linear map $f:V \to W$ is an isomorphism if
  and only if the list $(f(v_1),f(v_2),...,f(v_n))$ is a basis of $W$ . Proof Attempt: Let $A = (v_1,v_2,....,v_n)$ and $B = (f(v_1),f(v_2),....,f(v_n))$ . Then, we have to show that the list B is linearly independent and generates $W$ . Consider the following linear combination: $$\sum_{k=1}^{n}\alpha_k f(v_k) = 0\\
\implies \sum_{k=1}^{n} f(\alpha_k v_k) = 0\\
\implies f[\sum_{k=1}^{n} \alpha_k v_k] = 0\\
\implies \sum_{k=1}^{n} \alpha_k v_k = 0\\
\implies \forall k \in \{1,2,...,n\}: \alpha_k = 0$$ In the above computation, we have used the linearity of $f$ and the fact that $f(0) = 0$ , alongside the bijectivity of $f$ . This proves the linear independence of the list of vectors in $B$ . Then, we need to show that it generates $W$ . Clearly, $L(B) \subset W$ . Let $w \in W$ . Then, there exists a unique element $v \in V$ such that $f(v) = w$ . So, we have: $$w = f(v) = f[\sum_{k=1}^{n} \alpha_k v_k]\\
\implies w = \sum_{k=1}^{n} f(\alpha_k v_k)\\
\implies w = \sum_{k=1}^{n} \alpha_k f(v_k)\\
\implies w \in L(B).$$ This proves that $W \subset L(B)$ and, thus, proves that $L(B) = W$ . This shows that the list $B$ is a basis of $W$ . Now, suppose that B is a basis of W. Then, we have to show that $f:V \to W$ is bijective. Let $u_1,u_2 \in V$ such that: $f(u_1) = f(u_2)$ Then, $u_1 = \sum_{k=1}^{n} \alpha_k v_k$ and $u_2 = \sum_{k=1}^{n} \beta_k v_k$ . So, we have: $f(u_1)-f(u_2) = f(u_1-u_2) = 0$ $\implies f[\sum_{k=1}^{n} (\alpha_k - \beta_k)v_k)] = 0$ $\implies \sum_{k=1}^{n} (\alpha_k-\beta_k)f(v_k) =0$ $\implies \forall k \in \{1,2,...,n\}: \alpha_k = \beta_k$ Hence, that proves that $u_1 = u_2$ . This proves the injectivity of $f$ . Then, consider $f(V)$ . Clearly, $f(V) \subset W$ . Let $w \in W$ . Then, we have: $$w = \sum_{k=1}^{n} \alpha_k f(v_k)\\
\implies w = \sum_{k=1}^{n} f(\alpha_k v_k)\\
\implies w = f[\sum_{k=1}^{n} \alpha_k v_k]\\
\implies w \in f(V)$$ . This proves that $W \subset f(V)$ and, therefore, $f(V) = W$ . That proves that f is surjective. Since f is injective and surjective, it follows that it is bijective and that proves that it is an isomorphism. I'd like some serious criticism on my proof above. Is it correct? Is my proof writing okay? How can I improve it?","['solution-verification', 'linear-algebra']"
3560689,On expressing $\frac{\pi^n}{4\cdot 3^{n-1}}$ as a continued fraction.,"It is a celebrated equation that $$\frac{\pi}{4}=\cfrac{1}{1+\cfrac{1^2}{3+\cfrac{2^2}{5+\cfrac{3^2}{7+\ddots}}}}$$ However, there are two other conjectured equations that I found which, if true (they seem to be), might reveal a pattern. $$\frac{\pi^2}{12}=\cfrac{1}{1+\cfrac{1^4}{3+\cfrac{2^4}{5+\cfrac{3^4}{7+\ddots}}}}$$ $$\frac{\pi^3}{36}=\cfrac{1}{1+\cfrac{1^6}{3+\cfrac{2^6}{5+\cfrac{3^6}{7+\ddots}}}}$$ Conjectured General Formula: For natural $n\geqslant 1$ , $$\frac{\pi^n}{4\cdot 3^{n-1}}=\cfrac{1}{1+\cfrac{1^{2n}}{3+\cfrac{2^{2n}}{5+\cfrac{3^{2n}}{7+\ddots}}}}$$ Can these be numerically verified? I have not the skill to by-hand prove/disprove these, and have only been using Wolfram Alpha to arrive at these conjectures. It would also be much appreciated if one could suggest a program I could install in order to evaluate these continued fractions independently, as well as the code required. Will PARI/GP suffice? Thanks.","['conjectures', 'number-theory', 'elementary-number-theory', 'pi', 'continued-fractions']"
3560747,A function satisfying a series equation,"We already solved or argued differential equations, integral equations and even function equations. But I encounter the following, which I call it ""series equation"": Is there any nonzero function $f$ satisfying the series equation $$f(x)=\sum_{n=0}^{\infty}\frac{f(n)}{n!}x^n \tag{*} $$ The source: I interested to solve the equation $$f'(x)=f(x+1)$$ and I found out $$f^{(n)}(x)=f(x+n)$$ thus: $f^{(n)}(0)=f(n)$ and if we consider $f$ having a taylor series, then the expression $(*)$ will be the case. But I have no idea how $f$ would be. Is such a function exists? Thanks.","['ordinary-differential-equations', 'analysis', 'real-analysis', 'taylor-expansion', 'delay-differential-equations']"
3560772,Solving $\frac{5}{8} \cot36^\circ = \cos^3x$ without substituting the trig values for $36^\circ$,"Find the value of $x$ such that $$\frac{5}{8} \cot36^\circ = \cos^3x$$ The answer is $x=18^\circ$ . It's really messy to plug in the standard values of $\cos36^\circ$ , $\sin36^\circ$ and miraculously guess a suitable value of $x$ and prove that our guess is correct. Is there any nice way to find the value of $x$ ?","['trigonometry', 'proof-writing']"
3560860,Vapnik-Chervonenkis inequalities,"I kown of two Vapnik-Chervonenkis inequalities.
The original one, by Vapnik and Chervonenkis, $$P\left(\sup_{A \in \mathcal{A}} \left |\nu_n(A) - \nu(A) \right | >\varepsilon \right) \leq 8\, S(\mathcal{A},n)\, e^{-n\varepsilon^2/32}$$ and the following (I am not sure whom to credit for - I read it in Devroye,Lugosi Combinatorial methods in density estimation ) $$\mathbb{E}\left[\sup_{A \in \mathcal{A}} \left |\nu_n(A) - \nu(A) \right | \right] \leq 2 \sqrt{\dfrac{\log 2\,S(\mathcal{A},n) + \log 2}{n}}$$ How do they compare?","['machine-learning', 'statistics', 'probability-theory', 'probability']"
3560877,100 persons 100 sweets problem,"There are $100$ persons, including men,women and children.
  Then there are $100$ sweets. Each man will get $10$ sweets Each woman will get $5$ sweets Each child will get $.5$ (i.e half) of the sweet At the end of sharing every 100 person should get sweets, and there should be no sweets left. How many men, women and children are present? I have tried to make two equations by the way $M + W + C = 100$ $10M + 5W + .5C = 100$ were $M$ => no. of men $W$ => no. of women
and $C$ => no. of children But in order to solve this equation of three variables, I think I need one more equation But is there anything else that I miss here?","['discrete-mathematics', 'diophantine-equations']"
3560946,Why is this way to solve the differential equation wrong?,I'm looking at the following problem: There is a solution and I understand it but I tried to answer it by writing: $$y'=\frac{(y+1)^2}{y}=y+\frac{1}{y}+2$$ Integrating it to obtain: $$y=y^2+\log(y)+2y+c$$ And solving for $c$ in $y(2)=0$ . The trouble is that it seems this is not a solution to the differential equation in the problem but I can't figure out why. I' ve tried to use Mathematica to check for equality but it failed.,['ordinary-differential-equations']
3560948,Countable topological dynamical system,"Definitions For the purpose of this post, a (topological) dynamical system is a compact metric space $X$ equipped with a homemomorphism $T:X\to X$ . We say that a subset $S$ of $\mathbb Z$ is relatively dense in $\mathbb Z$ if there is a positive integer $N$ such that for all $a\in \mathbb Z$ the set $\{a+1, a+2, \ldots, a+N\}$ has non-empty intersection with $S$ . Let $x$ be a point in a dynamical system $(X, T)$ . $\bullet$ The orbit of $x$ is defined as $O_x=\{T^nx:\ n\in\mathbb Z\}$ . $\bullet$ We say that a point $x\in X$ is almost periodic if for all neighborhoods $U$ of $x$ in $X$ , the set $\{n\in \mathbb Z:\ T^nx\in U\}$ is relatively dense in $\mathbb Z$ . $\bullet$ We say $x$ periodic if the orbit of $x$ is finite. Clearly, any periodic point is almost periodic. Question 1 Assuming $(X, T)$ is a dynamical system with $|X|=|\mathbb N|$ , is it necessary that every almost periodic point is also periodic? I do not know the answer to the above question. In fact, I do not know any ""good"" examples of a countable dynamical system. If you are aware of good examples then please feel free to share. Question 2 Assuming $(X, T)$ is a dynamical system with $|X|=|\mathbb N|$ , is it necessary that $X$ has a periodic point? The answer to this question is in the affirmative.
This is because we know that there is a $T$ -invariant probability measure $\mu$ on $X$ . Since $X$ is countable, there is a point $x$ in $X$ such that $\mu(x)>0$ . Now the orbit of $x$ must be finite, for otherwise, by the $T$ -invariance of $\mu$ , we would have that $\mu(X)=\infty$ . Can we give an argument with does not go via measure theory and is purely topological in nature?","['general-topology', 'ergodic-theory', 'dynamical-systems']"
3560983,How to prove the spectrality of valuation spectrum of a commutative ring?,"Please help me to understand the proof of spectrality of valuation spectrum of a commutative ring. I'm trying to understand the proposition 4.7 in page 25 here .
Sorry I spent a lot of time on this proof but I really can't understand. How do these topologies are defined over $SPV(A)$ and why the clopen subsets of the topology induced by $P(A\times A)$ are of the form $SPV(A)(f/s)$ ? How the topology induced by $P(A\times A)$ is defined? (What are the open sets here?) Why $SPV(A)$ is $T_0$ ? (How is the open sun set containing one of $f$ or $s$ but not the other one defined?)","['general-topology', 'algebraic-geometry', 'valuation-theory']"
3560986,First integral of ODE system,"I am trying to make sense out of the first integral of non-linear ODE systems. $\bullet$ Is the first integral only relevant to a certain type of ODE's e.g. autonomous, first order? $\bullet$ How is the first integral generally defined and/or calculated of say: $$
\begin{pmatrix}
\dot x\\\dot y
\end{pmatrix}
=
\begin{pmatrix}
x-y\\ xy
\end{pmatrix}
$$ $\bullet$ What can I normally do knowing it (very generally speaking)? Thanks for your help. Edit: Concrete problem is given (Open to suggestions)","['integration', 'ordinary-differential-equations']"
3561070,Mathematical model for period of pendulum,"I'm doing an exercise in calculus about functions as math models, and I'm stuck in this one particular exercise about representing a pendulum's period. The problem goes: The period of a pendulum is directly proportional to the square root of the length of the pendulum, and a pendulum of length 8 feet has a period of 2 seconds. So, by definition of ""directly proportional to the square root of the length of the pendulum"", we have: $$f(x) = k \sqrt x$$ This next part is where I'm not sure; ""a pendulum of length 8 feet has a period of 2 seconds."": $$2= 8k$$ hence $$k = 1/4$$ Plugging that in, I arrived with: $$f(x) = \frac {\sqrt {x} }{4} $$ I took a look at the math model answer and the book says that the correct mathematical model is $$f(x) = \sqrt \frac{x}{2} $$ and that if you were to find the period of a pendulum with length 2 feet, it is 1 second.","['calculus', 'functions', 'mathematical-modeling']"
3561123,Applying discrete calculus to prove the results of the classical calculus,"Can the discrete calculus be used as a way of proving theorems of the classical calculus? For example, isn't the following a proof of the fundamental theorem of calculus by means of the discrete calculus? The discrete derivative can be defined as: $\Delta f(x) = {f(x+h)-f(x) \over h}$ Similarly, the discrete integral can be defined as: $\sum\limits_{x:a \to b} f(x) = hf(a) + hf(a+h) + hf(a+2h) + ... + hf(b-h)$ And here's a some kind of the fundamental theorem of calculus in the discrete world: $\sum\limits_{x:a \to b} \Delta f(x) = (f(a+h)-f(a)) + (f(a+2h)-f(a+h)) + ... + (f(b)-f(b-h))=f(b) - f(a)$ Isn't the following true in such case? $\int\limits_a^b f(x) dx = \lim\limits_{\lambda(P) \to 0} \sum\limits_{k=1}^n f(\alpha_k) \Delta x_k = \lim\limits_{h \to 0} \sum\limits_{x:a \to b} f(x)$ Another example. Isn't the following a proof of $(e^x)'=e^x$ ? $\Delta a^x=a^x$ ${a^{x+h}-a^x \over h}=a^x$ $a^x a^h - a^x=ha^x$ $a^h-1=h$ $a=(1+h)^{1 \over h}$ $\lim\limits_{h \to 0} (1+h)^{1 \over h} = e$ $f'(x)=\lim\limits_{h \to 0} \Delta f(x)$ $(e^x)'=e^x$ Edit. As was noted, the Dirichlet function gives a counterexample: $\lim\limits_{h \to 0} \sum\limits_{x:a \to b} \chi_\mathbb{Q}=0$ While the Dirichlet function doesn't have a Riemann integral. However, the Dirichlet function does have a Lebesgue integral equal to zero. Would the above formulas be correct if we'll restrict $f$ to only functions continuous over $[a,b]$ ?","['calculus', 'discrete-mathematics', 'discrete-calculus']"
3561134,Prove the three lines are concurrent.,"Let $O$ be the circumcenter of $\triangle ABC$ with $\angle
A=60^{\circ}$ , $P$ be an arbitary point on the  circumcircle of $\triangle BOC$ , and $D,E,F$ be the circumcenters of $\triangle
 BPC,\triangle CPA, \triangle APB$ respectively. Prove $AD,BE,CF$ are
   concurrent. Some intermediate results: $AD$ bisects $\angle BAC$ ,and $OD \perp BC$ ; $O,P$ are isogonal conjugate points of $\triangle DEF$ .",['geometry']
3561155,"Given $f(x) = \frac1{ax+b}$, for which $a$, $b$ such that $x_1=f(x_3) $, $ x_2=f(x_1) $, $x_3=f(x_2) $ are distinctive","For real numbers $a$ and $b$ define $$f(x) = \frac1{ax+b}$$ For which $a$ and $b$ are there three distinct real numbers $x_1$ , $x_2$ , $x_3$ such that $f(x_1) = x_2$ , $f(x_2) = x_3$ and $f(x_3) = x_1$ ? I tried to isolate a/b, and I found 3 values: 
f(x1) = x2 ---> 1/(a.x1 + b) = x2 ---> a.x1.x2 + b.x2 = 1 ---> I f(x2) = x3 ---> 1/(a.x2 + b) = x3 ---> a.x2.x3 + b.x3 = 1 ---> II f(x3) = x1 ---> 1/(a.x3 + b) = x1 ---> a.x1.x3 + b.x1 = 1 ---> III make I=II then, a/b = (x3 - x2)/x2.(x1 - x3) ---> x1 ≠ x3, for example, but i don't know how to proceed from here.","['algebra-precalculus', 'functions', 'rational-functions', 'fixed-points']"
3561167,Assume $\lambda_\min(A_kA_k^{\rm T})>\varepsilon$ and show that $A_k^{\rm T}(A_kA_k^{\rm T})^{-1}$ is bounded,"For all $k\in\mathbb{N}$ , let $A_k\in\mathbb{R}^{n\times m}$ , where $n\leq m$ , and assume that there exists $\varepsilon>0$ such that for all $k\in\mathbb{N}$ , $\lambda_\min(A_kA_k^{\rm T})>\varepsilon$ , where $\lambda_\min$ denotes the minimum eigenvalue of a symmetric positive semidefinite matrix. Can we show that $A_k^{\rm T}(A_kA_k^{\rm T})^{-1}$ is bounded? For the case where $n=m$ , I can see that $A_k^{\rm T}(A_kA_k^{\rm T})^{-1}$ is bounded. My question is about the case where $n<m$ . Any hint is appreciated.","['matrices', 'matrix-rank', 'linear-algebra', 'inverse']"
3561188,Galois group of residue field extension,"Let $E$ and $K$ be number fields, so that $E/K$ is normal, $G:= \textrm{Gal}(E/K)$ , $\mathcal{O}_E$ and $\mathcal{O}_K$ their respective rings of integers. The theory of Artin $L$ -series relies on the fact that for any prime ideal $\mathfrak{p} \subset \mathcal{O}_K$ , if $\mathfrak{P} \subset \mathcal{O}_E$ is a prime above $\mathfrak{p}$ , and $G_{\mathfrak{P}}$ and $I_{\mathfrak{P}}$ are the corresponding decomposition and inertia subgroups, then we have the following isomorphism: $$ G_{\mathfrak{P}}/I_{\mathfrak{P}} \cong \textrm{Gal}\left( (\mathcal{O}_E/\mathfrak{P})/(\mathcal{O}_K/\mathfrak{p}) \right) $$ But is this Galois group even well-defined for all prime ideals? In Proposition 9.4, ch. I, §9, of $\textit{Algebraic Number Theory}$ , Neukirch proves that $(\mathcal{O}_E/\mathfrak{P})/(\mathcal{O}_K/\mathfrak{p})$ is normal for any prime $\mathfrak{p} \subset \mathcal{O}_K$ and any choice of $\mathfrak{P} \subset \mathcal{O}_E$ above $\mathfrak{p}$ . He then goes on to show that there is a surjective homomorphism $$G_{\mathfrak{P}} \to \textrm{Gal}\left( (\mathcal{O}_E/\mathfrak{P})/(\mathcal{O}_K/\mathfrak{p}) \right)$$ and defines the inertia group to be the kernel of said homomorphism, thereby establishing the aforementioned isomorphism. Now we know that if $\mathfrak{p} \subset \mathcal{O}_K$ is unramified over $\mathcal{O}_E$ , then $(\mathcal{O}_E/\mathfrak{P})/(\mathcal{O}_K/\mathfrak{p})$ must be separable. But if $\mathfrak{p}$ ramifies over $\mathcal{O}_E$ , how can we know that the corresponding residue field extension is separable? Thank you for your attention.","['algebraic-number-theory', 'galois-theory', 'ring-theory', 'abstract-algebra', 'group-theory']"
3561246,Find where $f(x)=\frac{x^2+4}{x^2-4}$ is concave upwards and concave downwards,"Find where $f(x)=\frac{x^2+4}{x^2-4}$ is concave upwards and concave downwards Solution: We are going to take the second derivative, find the critical points, and then test each region. Lets get to the second derivative by doing the quotient rule twice: $f'(x) = \frac{(x^2-4)\frac{d}{dx}(x^2+4)-(x^2+4)\frac{d}{dx}(x^2-4)}{(x^2-4)^2}$ $=\frac{(x^2-4)(2x)-(x^2+4)(2x)}{(x^2-4)^2}$ $=\frac{-16x}{(x^2-4)^2}$ Okay, theres the first derivative. Let's do it again!! $f''(x)=\frac{(x^2-4)^2 \frac{d}{dx}(-16x) - (-16x)\frac{d}{dx}(x^2-4)^2}{(x^2-4)^4}$ Note that we have to the chain rule to evluate $\frac{d}{dx}(x^2-4)^2$ . $\rightarrow f''(x)=\frac{(x^2-4)^2 (-16) - (-16x)(2(x^2-4)\frac{d}{dx}(x^2-4))}{(x^2-4)^4}$ $\rightarrow =\frac{(x^2-4)^2 (-16) - (-16x)(2(x^2-4)(2x)}{(x^2-4)^4}$ $\rightarrow =\frac{(x^2-4)^2 (-16) + 64x^2(x^2-4)}{(x^2-4)^4}$ Canceling off one of the $(x^2-4)$ from each term in the numerator and the denominator... $\rightarrow =\frac{(x^2-4) (-16) + 64x^2}{(x^2-4)^3}$ $\rightarrow =\frac{-16x^2+64+64x^2}{(x^2-4)^3}$ $\rightarrow =\frac{64+48x^2}{(x^2-4)^3}$ Few.. okay the hard part is over. Now we need to find the points to chop our number line up from, so we have to solve $f''(x)=0$ and find where $f''(x)$ DNE. Let's solve where $f''(x)=0$ $f''(x) =\frac{64+48x^2}{(x^2-4)^3}=0$ $\rightarrow 64+48x^2 = 0$ $\rightarrow x= \pm \sqrt{-64/48}$ So $f''(x)=0$ only when $x$ is imaginary... so those values are irrelevant. On the other hand, $f''(x)$ DNE when $x = \pm 2$ , because the denominator will be zero. So these are the values we split our number line up by. Since $f''(-3)=\frac{496}{125}>0$ , we have that $f(x)$ is concave up on $(-\infty,-2)$ . Since $f''(0)=-1<0$ , we have that $f(x)$ is concave down on $(-2,2)$ . Since $f''(3)=\frac{496}{125}>0$ , we have that $f(x)$ is concave up on $(2,\infty)$ .","['calculus', 'solution-verification', 'derivatives']"
3561248,differentiability and continuity of multivariable functions,"I'm trying to understand differentiability of multivariable functions. The textbook says,
""If the partial derivatives ƒx and ƒy of a function ƒ(x, y) are continuous throughout an open region R, then ƒ is differentiable at every point of R."" Hass, Joel R.; Heil, Christopher E.; Weir, Maurice D.. Thomas' Calculus (Page 818). Pearson Education. Kindle Edition. So in two dimensions, if something is continuous, it might not be differentiable, because it could be pointy
(that's an official math term, right?) Couldn't that happen in three dimensions too? Also, I was wondering whether the converse of the above is true - i.e. if a multivariable function is differentiable, that means it's continuous and that the partial derivatives exist. And if not, what's the counterexample? Thank you!","['continuity', 'multivariable-calculus', 'derivatives']"
3561272,"What can I say about this ""black box"" function?","Situation I have a function $f(t)$ with the following known properties: $f(t)$ is defined for $t \ge 0$ (i.e. ignore $t < 0$ ). $t_2 \ge t_1 \implies f(t_2) \ge f(t_1)$ (i.e. the function is monotonically decreasing). There exists some $t_\alpha$ , $0 < t_\alpha < \infty$ , for which $t \ge t_\alpha \implies f(t) = 0$ . Over the interval $0 \le t \le t_\alpha$ , the function satisfies ${K \over 2} f(t) \le t_\alpha - t \le {3 K \over 2} f(t)$ for some known positive constant $K$ . Question Given the above information, can I say anything else about the behavior of $f$ ? In particular, if I know the value of $f(t)$ over some period $0 \le t \le t_\beta$ where $t_\beta < t_\alpha$ , is it possible to predict $t_\alpha$ ? If ""yes"", is the above still consistent if I remove the criteria that the function is monotonically decreasing, and does that make $t_\alpha$ unpredictable? Background (Feel free to skip this) Ultimately, I am attempting to construct a failure model for a fictional process, such that: The time of failure ( $t_\alpha$ ) is predetermined and invariant for any given ""instance"", but variable across ""instances"". At any given point, I can predict the time of failure to within ±50%. I cannot predict the exact time of failure until it actually occurs. The model does not need to follow any existing physical process.",['functions']
3561291,Searching for a second order ODE whose solution is bell shape (Gaussian function),"I'm studying a nonequilibrium dynamics of a stochastic system. I found that in mean-field approximation the numerical solution resembles a bell shaped function (Gaussian function) with is zero at initial time, then reaches its maximum and finally decays to zero. I was wondering if there exist a second order ODE whose solutions are smooth and resemble Gaussian curves.  I know that one can get the ODE satisfies by a Gaussian function just deriving it twice. But this is not the point.  I'm interested in a general ODE which exhibits solutions which have similar behaviour of Gaussians. Thank you in advance.","['gaussian', 'stochastic-calculus', 'ordinary-differential-equations']"
3561329,"Ramified primes, as defined by Neukirch","In ch. I, §8, p. 49 of the English translation of $\textit{Algebraic Number Theory}$ , Neukirch states the following (I paraphrase, but retain his notation): Let $L/K$ be an extension of number fields, and let $\mathcal{O}$ and $\mathcal{o}$ be their respective rings of integers. Then every prime ideal $\mathfrak{p} \subset \mathcal{o}$ possesses a unique factorisation $$\mathfrak{p} = \mathfrak{P}_1^{e_1}...\mathfrak{P}_1^{e_1}$$ where the $\mathfrak{P}_i$ are prime ideals in $\mathcal{O}$ . He then says that the prime ideal $\mathfrak{P}_i$ in the above factorisation is said to be $\textbf{unramified}$ over $K$ if $e_i = 1$ ( $\textit{i.e.}$ if the prime only appears once in the factorisation) $\underline{\textit{and}}$ the residue field extension $(\mathcal{O}/\mathfrak{P}_i)/(\mathcal{o}/\mathfrak{p})$ is separable. But $(\mathcal{O}/\mathfrak{P}_i)/(\mathcal{o}/\mathfrak{p})$ is a finite extension of finite fields, so will it not always be separable? Why is this included in the definition? Below I have included the excerpt from the book I am referring to. Thank you for your attention. $\textbf{Addendum:}$ In response to the comments below, include two more excerpts from ch. I, §9, p. 58 and p. 59 respectively. From p. 58: From p. 59: I should explain that his notation is $\kappa(\mathfrak{P}) := \mathcal{O}/\mathfrak{P}$ and $\kappa(\mathfrak{p}) := \mathcal{o}/\mathfrak{p}$ . Now if the assumption was only included for formal reasons (because he did not want to assume that finite fields are perfect), then why would he repeatedly refer to the separability of the residue field extension as a ""special case""?","['algebraic-number-theory', 'galois-theory', 'ring-theory', 'abstract-algebra', 'group-theory']"
3561340,Confusion about Lang's Conjecture,"I've been reading the following Paper ( https://arxiv.org/abs/1809.06818 ) on arithmetic hyperbolicity. We say that a scheme $X/\overline{\mathbb{Q}}$ is arithmetically hyperbolic over $\overline{\mathbb{Q}}$ is for every $\mathbb{Z}$ -finitely generated subring $A\subset \overline{\mathbb{Q}}$ , the set of $A$ -rational points is finite, i.e. $X(A)$ is finite. In this Paper, the mention a conjecture (Conjecture 1.1), in which claims that $X$ is arithmetically hyperbolic over $\overline{\mathbb{Q}}$ if and only if for every abelian variety $A$ over $\overline{\mathbb{Q}}$ , every morphism $A\rightarrow X$ over $\overline{\mathbb{Q}}$ is constant. However, I am confused what this would mean in the case that $X$ is an abelian variety itself. There exist elliptic curves of rank one over $\overline{\mathbb{Q}}$ , i.e. $X(\mathbb{Z})$ is a finite torsion group. However, the map $X\rightarrow X, x\mapsto 2x$ is not constant. Therefore $X$ should not have only finitely many integral points. Since this seems like an obvious counterexample, I must misunderstand something very basic about this conjecture. What is it?","['algebraic-number-theory', 'algebraic-geometry', 'arithmetic-geometry']"
3561383,Minimum value when $abc+ab+4bc+9ca=144$,"If $a,b,c$ are non-negative real numbers such that $abc+ab+4bc+9ca=144$ , find the minimum value of $a+b+c$ . I tried with Lagrange multipliers. I got the system: $bc+b+9c=ca+a+4c=ab+4b+9a$ Replacing in the condition, I found four solutions, but only one $(4,0,4)$ is non-negative. So the minimum value is $8$ . My question is, can this be done without Lagrange Multipliers?","['maxima-minima', 'multivariable-calculus', 'optimization', 'inequality', 'quadratics']"
3561439,Does the Komlós theorem hold in infinite measure spaces?,"I read an article, and they use a certain theorem, called Komlós theorem, which says: Let $(E,\mathcal {A}, \mu ) $ be a finite measure space and $ (f_n)_{n\geq 1} \subset \mathcal {L}_{\mathbb {R}}^1$ is a sub-sequence  with : $$\sup_n \int_{E}{|f_n| d\mu} < \infty .$$ Then there exist $ h _{\infty} \in  \mathcal {L}_{\mathbb {R}}^1 $ and a sub-sequence $ (g_k)_k $ of $(f_n)_n $ such that for every sub-sequence $ (h_m)_m $ of $(g_k)_k$ : $$ \frac{1}{i}\sum_{j=1}^{i}{h_j}\to   h _{\infty} \text{ a.s. }$$ if the space is of infinite measure, is that this result rest valid ? please an idea?","['measure-theory', 'lp-spaces', 'convergence-divergence', 'cesaro-summable']"
3561468,A problem about the connectivity of vertices that must have the same color for any proper minimal coloring of a graph.,"The question is now also published in MathOverflow (here) . I'm trying to solve a problem about connectivity of entangled vertices in a graph. Two vertices $u, v$ of a finite graph $G(V, E)$ are said to be entangled if for any proper coloring $c:V(G)\rightarrow\mathbb{N}$ with $\chi(G)$ colors we have $c(u) = c(v)$ , that is, they must have the same color. What I'm trying to prove is that, given two entangled vertices $u, v\in V(G)$ , there is $w\in V(G)$ (possibly equal to $v$ ) also entangled with $u$ so that there is a set of size $\chi(G)-1$ of disjoint paths from $u$ to $w$ . EDIT: The proof cited below was incorrect, as shown by the accepted answer. I was able to prove, using the vertex-connectivity version of Menger's theorem and induction, that the previous statement is true if $v$ is the only vertex in $G$ entangled with $u$ , so I've been trying to show that if there is not a set of size $\chi(G)-1$ of disjoint paths from $u$ to $v$ (considering $u$ and $v$ entangled), there is still a vertex in $G-v$ entangled with $u$ , but without success. Another idea I had was showing that the minimal (in the number of edges) subgraph of $G$ for which there is still a vertex entangled with $u$ , has exactly one vertex entangled with $u$ . I would appreciate some help with this subject.","['coloring', 'graph-theory', 'graph-connectivity', 'combinatorics', 'problem-solving']"
3561477,Does $\big(\cos\frac{\pi}4+i\sin\frac{\pi}4\big)^n = \cos\frac{n\pi}4+i\sin\frac{n\pi}4$ for all $n$?,"This was the original question: Prove that $(1+i)^{10}=32i$ for $i=\sqrt{-1}$ using only trigonometry. It turned out to be an interesting problem, which then led to another question instead. Firstly, from Euler's identity, I know that $i=e^{\pi i\div 2}$ $$\therefore 1+i=\sqrt 2\bigg(\cos\frac{\pi}4 + i\sin\frac{\pi}{4}\bigg)$$ I know $\sqrt{2}^{10}=32$ so it suffices to prove that $$i=\bigg(\cos\frac{\pi}4 + i\sin\frac{\pi}{4}\bigg)^{10}$$ And this was where I got stuck. However, working backwards from $32i$ , it led me to a conjecture. $$32i=32(0+i)=32\bigg(\cos\frac{\pi}2+i\sin\frac{\pi}{2}\bigg)$$ and now in order to make the denominators equal to $4$ without changing the value of the brackets, I decided to do it like this. $$\cos\frac{\pi}2+i\sin\frac{\pi}{2}=\cos\bigg(2\pi+\frac{\pi}2\bigg)+i\sin\bigg(2\pi+\frac{\pi}{2}\bigg)=\cos\frac{10\pi}4+i\sin\frac{10\pi}{4}$$ $$\therefore \cos\frac{10\pi}4+i\sin\frac{10\pi}{4}=\bigg(\cos\frac{\pi}4+i\sin\frac{\pi}4\bigg)^{10}$$ Can the last equation be generalised for some $n$ , i.e. $$\bigg(\cos\frac{\pi}4+i\sin\frac{\pi}4\bigg)^n \stackrel{\small ?}{=}\cos\frac{n\pi}4+i\sin\frac{n\pi}4$$ Or is this just a coincidence for $n=10$ ? Can the denominator also be changed arbitrarily and not just be fixed on $4$ ? How does one go about proving this using only trigonometry? I can't apply the binomial theorem because then I need to use the gamma function, which I don't believe falls under trigonometry in the sense of the question (it was a challenge set by a teacher at school actually, but just for fun and not homework-based). So what other methods are there? Also, I asked my teacher where he found this problem and he wouldn't tell me lest I would search up the answer, and... well, it seems I am... ahem . So hints would be much appreciated. Any ideas? Thanks.","['trigonometry', 'complex-numbers']"
3561530,Let $f:A→B$ be an onto function and let $T⊆B$. Prove that $(f◦f^{-1})(T) =T$,"Let $f:A→B$ be an onto function and let $T⊆B$ .  Prove that $(f◦f^{−1})(T) = T$ How would I go about proving this? Let $x$ be an element of $(f◦f^{−1})(T)$ . By the definition of composition/composite functions, $\text{Dom}(f◦f^{−1}) = \text{Dom}(f)$ and the $\text{Ran}(f◦f^{−1}) = \text{Ran}(f^{-1})$ . By the definition of the inverse, the range of $f^{-1}$ is the domain of $f$ . Thus $(f◦f^{−1})(T)$ is a function from $A$ to $A$ . (?) I can't get much past this. My prof hinted to using double containment but I can't get past the first part.","['elementary-set-theory', 'discrete-mathematics', 'real-analysis']"
3561564,Smoothness and the eigenvalues of the Hessian,"A continuously differentiable function $f$ is $\beta$ -smooth if the gradient is $\beta$ -Lipschitz: $$ || \nabla f(x) - \nabla f(y)|| \leq \beta ||x-y||, $$ where $\nabla f(\cdot)$ denotes the gradient vector of $f$ at  a given point, $\beta$ is a scalar, and $||\cdot||$ is an $\ell_2$ -norm. In the book Convex Optimization: Algorithms and Complexity by Sebastien Bubeck it is said that: The above Lipschitz condition implies (if the function is twice
  differentiable) the eigenvalues of the Hessians being smaller than $\beta$ . I cannot see how.","['eigenvalues-eigenvectors', 'smooth-functions', 'lipschitz-functions', 'functional-analysis', 'convex-analysis']"
3561630,Some help and clarification to more-rigorously derive Green's Function solution for ODE $\dot{x}(t)-Ax(t) = f(t)$ with $x(0) = x_0$,"I'm trying to derive this solution given to us in a systems / control-theory course without proof. The first-order vector linear differential equation in question is $$\dot{x}(t)-Ax(t) = f(t)$$ $$x(0) = x_0$$ Where $x$ is a $n$ -dimensional vector in $R^n$ (for instance, a two or three dimensional vector), and $A$ is an $n$ x $n$ constant matrix. The unexplained solution is $$x(t) = e^{At}x_0 + \int_0^t e^{A(t-\tau)}f(\tau)d\tau$$ Where the first term $e^{At}x_0$ is the homogenous solution to $$\dot{x}(t)-Ax(t) = 0$$ $$x(0) = x_0$$ and the second term is the ""particular"" solution, the one I'm interested in deriving. I think it comes from a Green's function approach where $f(t)$ is decomposed into an infinite sum of dirac delta functions $\delta$ : $$
f(t) = \int_0^\infty f(\tau)\delta(t-\tau)d\tau
$$ So by linearity of the differential equation, we first find a solution (response, trajectory, aka Green's function, $G(\tau,t)$ ) for a single delta function ""system input"" at time $\tau$ and add them up invoking the linearity of the differential equation to obtain: $$
x(t) = \int_0^\infty G(t, \tau)f(\tau)d\tau
$$ But I'm a bit confused how to set up the ODE to get the Green's function. Following wiki and other sources, I have: $$
\dot{G}(t,\tau) - A G(t, \tau) = \delta(t-\tau)
$$ but this $\delta$ has to be vector-valued since the left side is. And I suppose this ""dirac vector"" should point parallel to $f(\tau)$ : let $\hat{f}(\tau)$ be this unit vector to get it pointing properly. $$
\dot{G}(t,\tau) - A G(t, \tau) = \hat{f}(\tau) \delta(t-\tau)
$$ Then there is the question of what initial value to use with this ODE. I don't really know how to answer that and why. But I tried $G(t=0,\tau)=0$ for all $\tau$ . I figured if the initial condition wasn't zero it would mess up the homogenous solution obeying a non-zero initial condition(?). Then the ODE says the system does nothing until $t > \tau$ , after which it does its natural (homogenous) response after a sudden jump from the origin to the point $\hat{f}(\tau)$ : $$
G(t,\tau) = 0\space for \space 0<t<\tau
$$ $$
G(t,\tau) = e^{A(t-\tau)} \hat{f}(\tau) \space for \space 0<\tau<t
$$ Which can be captured in one line by using the step function $H(t-\tau)$ ( $1$ if $t>\tau$ , $0$ otherwise): $$
G(t,\tau) = H(t-\tau) e^{A(t-\tau)}
$$ Then adding up all the responses (ODE is linear, more justification desired) gives $$ x(t) = \int_0^\infty H(t-\tau) e^{A(t-\tau)} f(\tau) d\tau $$ The integrand is zero if $\tau$ exceeds t, so it simplifies to $$ x(t) = \int_0^t e^{A(t-\tau)} f(\tau) d\tau $$ All of which tries to explain the particular solution. Does this argument seem ok? Any areas where you could better explain what is going on? Specifically, I'm not sure if my reasoning about the boundary condition / initial value on $G$ is correct. Also my justification or reasoning for using $\hat{f}(\tau)$ to give my dirac delta function a vector value seems a little hand-wavy. Thanks.","['greens-function', 'linear-control', 'ordinary-differential-equations', 'dynamical-systems']"
3561750,Jacobian for polar decomposition,"Let $f:\mathrm{Mat}_n(\mathbb{C})\to\mathbb{C}$ be some function and let us suppose we want to make a change of variables in the integral $$ \int_{A\in \mathrm{Mat}_n(\mathbb{C})}f(A)\mathrm{d}{A} $$ from $A$ to $|A| U$ , i.e., the polar decomposition of $A$ , where $|A|\equiv\sqrt{A^\ast A}$ and $U$ is the unique partial isometry with kernel equal to that of $A$ (there is a theorem saying it exists). What is the Jacobian matrix of the transformation $A \mapsto (|A|, U$ )? I.e., what is $J$ such that the following equation holds: $$ \int_{A\in \mathrm{Mat}_n(\mathbb{C})}f(A)\mathrm{d}{A} = \int_{P\geq0,U^\ast U\,\mathrm{idempotent}}f(P U)|\det(J(P,U))|\mathrm{d}{P}\mathrm{d}{U}$$ I tried to calcualte it but I'm not getting anything simple. In particular, I've written $A = A_R + i A_I$ with $A_R = \frac{1}{2}(A+A^\ast); A_I = \frac{1}{2i}(A-A^\ast)$ so that $A$ is parametrized by two self-adjoint matrices. In turn, we may write $|A| = \exp(H_1) ; U = \exp(i H_2)$ for two self-adjoint matrices $H_1,H_2$ (assuming for a moment that $A$ is invertible so that $U$ is actually unitary). Hence we want to calculate the Jacobian of the transformation $(H_1,H_2)\mapsto(A_R,A_I)$ from $\mathrm{Herm}_n(\mathbb{C})^2\to \mathrm{Herm}_n(\mathbb{C})^2$ . This, however, starts to get ugly, with the differential of the exponential map for example being given by functional calculus of the adjoint super operator ( https://en.wikipedia.org/wiki/Derivative_of_the_exponential_map ) and having to use the determinant of a block matrix formula. Is there an easier way out? Possible solution: In Edelman's PhD thesis there are given Jacobians to get from a matrix A to its LQ decomposition , and from its LQ decomposition to its Cholesky decomposition (Theorem 3.1). This possibly solves the problem as follows: \begin{align} \int_{A\in \mathrm{Mat}_n(\mathbb{C})}f(A)\mathrm{d}{A} &= \int_{L\text{ lower triangular},\,U\text{ unitary}} f(LU)\prod_{i=1}^{n}L_{ii}^{2n-2i+1}\mathrm{d}{L}\mathrm{d}{U} \\ &=2^{-n}\int_{P\geq0,\,U\text{ unitary}} f(\sqrt{P}U)\mathrm{d}{P}\mathrm{d}{U}\\&=2^{-n}\int_{P\geq0,\,U\text{ unitary}} f(PU)|\det(P\otimes I+I\otimes P^\ast)|^2\mathrm{d}{P}\mathrm{d}{U}\\&=2^{-n}\int_{P\geq0,\,U\text{ unitary}} f(PU)\prod_{1\leq i,j\leq n}(\lambda_i(P)+\lambda_j(P))^2\mathrm{d}{P}\mathrm{d}{U}\end{align} with the usual abuse of notation that $\mathrm{d}{L}$ integrates only over the non-zero elements of $L$ , $\mathrm{d}{U}$ is the volume element within the unitary group, and $\mathrm{d}{P}$ the volume element on self-adjoint matrices (so only $n$ real and $\frac{1}{2}n(n-1)$ complex matrix elements). $\lambda_j(P)$ is the $j$ th eigenvalue of the matrix $P$ . Remaining question : Why is the LQ-decomposition change of variables valid for complex matrices? A complex unitary $n\times n$ matrix is $n^2$ real parameters, whereas a lower triangular matrix is $n(n+1)$ real parameters. On the other hand, a complex matrix is $2n^2$ real parameters, so there seem to be $n$ real parameters too many in this decomposition? (This is not a problem if matrices have real entries). Note that for the Cholesky decomposition this is not an issue since then the lower triangular matrix has positive entries on its diagonal. Could it be possible to make an LQ decomposition for complex matrices where the lower triangular has positive entries on the diagonal? Is this what Edelman is referring to? Unfortunately, precisely for the complex LQ decomposition he does not give a reference nor a proof.","['reference-request', 'jacobian', 'change-of-variable', 'linear-algebra', 'functional-analysis']"
3561814,What is the surface obtained by identifying antipodal points of $\mathbb{S}^1 \times \mathbb{S}^1$?,"This is perhaps a soft question. Let $X=\mathbb{S}^1 \times \mathbb{S}^1$ . Let $\mathbb{Z}_2$ act on $X$ by setting $(-1) \cdot (\theta,\psi)=(\theta+\pi,\psi+\pi)$ . Consider the quotient space $X/ \mathbb{Z}_2$ which is obtained after identifying $ (\theta,\psi) \sim(\theta+\pi,\psi+\pi)$ . Is there a succinct description of $X/ \mathbb{Z}_2$ as some product or twisted/fibered product or something like that? Are there other ""simple"" descriptions of this space? Is it related to some projective space? I feel like there should be a ""right"" terminology to describe it, or a way to recognize it as some familiar space, but I fail to see it. I understand that identifying antipodal points on the $2$ -torus embedded in $\mathbb{R}^3$ results in a Klein bottle- but this is not the same identification we are doing here: Here we identify $(\theta,\psi)=(\theta+\pi,\psi+\pi)$ , and in the embedded description we identify $(\theta,\psi)=(\theta+\pi,-\psi)$ .","['surfaces', 'quotient-spaces', 'general-topology', 'differential-topology', 'algebraic-topology']"
3561919,Erdős's proof of Sylvester's theorem,"Sylvester's theorem states that the product of $k$ consecutive integers $>k$ has a prime factor $p>k$. Erdős gave a short proof for it, but I can't find Erdős's proof online. Can anyone give a sketch of the proof? Thanks.","['number-theory', 'elementary-number-theory']"
3562009,Example of function such that its each derivative and itself vanishes at 0 but any polynomial multiple of that function doesnot vanishes at infinity,I wanted to understand defination of schwartz class function . So I am trying to find example of function f(x) such that $\lim_{|x|\to \infty}f^{(n)}(x)=0$ where $n\in \mathbb N\cup\{0\}$ (this should true for all n) But $\lim_{|x|\to \infty}x^af^{(n)}(x)\neq 0$ where $n\in \mathbb N\cup\{0\}$ for some $a>1$ I tried many examples but did not get. Please Help me to find such an example Any help will be appreciated,"['functional-analysis', 'examples-counterexamples', 'real-analysis']"
