question_id,title,body,tags
1872237,The lower bound of the smallest eigenvalue of a symmetric positive definite matrix,"I encounter a symmetric positive definite matrix whose features are all diagonal entries are $1$. all the other entries are in $[0, 1)$, but the matrix is not diagonally dominant. Now I am looking for a positive lower bound of the smallest eigenvalue, expressed by trace and Frobenius norm. I have seen a lot of papers related to this topic. Especially, the result in this paper is very close to my goal. But that expression still involves the maximum eigenvalue and determinant. I have seen the answer of Lower bound on the smallest eigenvalue . I'm happy if the answer posted in that is correct. But I think it's wrong. Does that kind of lower bound exist? Anyone could give me any tips? Thanks so much!","['matrices', 'eigenvalues-eigenvectors', 'positive-definite']"
1872246,"Is there a probability measure on $[0,1]$ with no subsets with measure $\frac{1}{2}$?","I have a decidedly weird question. Does there exist a probability measure $(\mu, \mathcal{F})$ on $[0,1]$ such that 1) $\mu(x) = 0$ for every $x \in [0,1]$ 2) For every $r \in [0,1] \setminus \lbrace \frac{1}{2} \rbrace$, there exists $A \in \mathcal{F}$ with $\mu(A) = r$, and 3) There is no $A \in \mathcal{F}$ with $\mu(A) = \frac{1}{2}$? The question comes about from a prelim problem in which the existence of a measure-$\frac{1}{2}$ set was assumed, and it made me wonder whether the assumption was for convenience or necessary.  Pigeonhole principle?  Ultrafilters?","['real-analysis', 'probability-theory', 'functional-analysis', 'descriptive-set-theory', 'measure-theory']"
1872251,Practicality of the Lebesgue integral,"I am getting pretty frustrated with the Lebesgue integral mainly because it seems highly impractical to calculate anything non-trivial. Whenever I look for a concrete calculation all I see are encomiums about how wonderful it is and then invariably the only concrete calculation is the Dirichlet function where unsurprisingly the measures are easy to calculate. When a run of the mill function is offered to be calculated ,I have seen one of two kinds of  responses: 1) The answer involves a trick that can't be generalized 2) The answer given is  "" The function is Riemann integrable so just use that"" i.e forget about the Lebesgue integral. The only method I have seen that aspires to practicality is to use the monotone convergence theorem i.e get a bunch of simple functions whose limit is the function you want to integrate. Integrate them and take the limit. I have tried this for $x^2$ and I run into hard sums which are summed by guess what...help of the Riemann integral.(could be that I chose an inconvenient set of simple functions but that partly proves my point-very easy to make things complicated) So is the Lebesgue integral mostly used in formal situations and then occasionally some highly pathological function is pulled to justify the work? Are there examples where the Lebesgue integral is of practical importance and there can be no recourse to the Riemann integral?  Highly discontinuous functions are not welcome.","['real-analysis', 'lebesgue-integral']"
1872276,What is the derivative of $x^i$?,"What would the derivative be of $x^i$? Would it simply be $ix^{(1-i)} $? I tried running the Power rule, and I got that is that right?","['derivatives', 'calculus']"
1872306,Expected number of virus cells,"I've found this question in a past programming assignment from a course I'm currently reading. Its statement looks like this : A recent lab accident resulted in the creation of an extremely dangerous virus that replicates so rapidly it's hard to predict exactly how many cells it will contain after a given period of time. However, a lab technician made the following observations about its growth per millisecond: $\bullet$ The probability of the number of virus cells growing by a factor of $a$ is $0.5$ $\bullet$ The probability of the number of virus cells growing by a factor of $b$ is $0.5$ Given a, b, and knowing that initially there is only a single cell of virus, calculate the expected number of virus cells after $t$ milliseconds. As this number can be very large, print your answer modulo $(10^9 + 7)$ . As I have no prior training in probability or combinatorics, this problem doesn't make much sense to me . I've done some searching about expected values in the context of probability, but I can't see how to model the data I'm given. Perhaps there's something very obvious I'm missing, but I'm not able to see it at the moment. How would you solve this?","['algorithms', 'probability']"
1872318,Question regarding the earth's spherical geometry,"Hello I have a question and it is as follows: What is a general formula to derive an angle from true north such that I know how to face a certain object assuming I know the longitude and latitude of my position and the longitude and latitude of the place? To clarify I will provide an example:
If I was in Seattle, Washington, USA $(47.6062^\circ\text{ N}, 122.3321^\circ\text{ W})$ what angle from true north would I have to turn to face Moscow, Russia $(55.7558^\circ\text{ N}, 37.6173^\circ\text{ E})$ such that if I walk along this direction I will reach Moscow without having to turn left nor right.","['circles', 'trigonometry', 'spherical-trigonometry']"
1872322,How to formally use Taylor expansions for $n$th derivatives and generating functions?,"When deriving Catalan numbers , the generating function takes on this form: $$C(x) = \frac{1}{2} (1 - \sqrt{1-4x}) = \frac{1}{2} (1 - f(x))$$ where $f(x) = \sqrt{1-4x}$ How does one formally show what it evaluates to? I can do it somewhat ""informally"" like so: $$f(x) = \sqrt{1-4x} = \sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!} x^n$$ $$f(x) = \frac{f^{(0)}(0)}{0!} x^0 + \frac{f^{(1)}(0)}{1!} x^1 + \frac{f^{(2)}(0)}{2!} x^2 + \frac{f^{(3)}(0)}{3!} x^3 + \frac{f^{(4)}(0)}{4!} x^4 + ...$$ Now at this point I can literally start computing the successive derivatives of $f(x)$ and then plug in $0$ for each $x$ and look for patterns and ""eyeball"" a closed-form for it. But is there a way at this point to formally show what the $n$th derivative is? Or is this more of an art than a science that changes depending on the function? I could eyeball the $n$th derivative formula as well: $f^{0}(x) = (1 - 4x)^{\frac{1}{2}}$ $f^{1}(x) = (\frac{1}{2})(-4)(1 - 4x)^{-\frac{1}{2}}$ $f^{2}(x) = (-\frac{1}{2})(\frac{1}{2})(-4)(-4)(1 - 4x)^{-\frac{3}{2}}$ $f^{3}(x) = (-\frac{3}{2})(-\frac{1}{2})(\frac{1}{2})(-4)(-4)(-4)(1 - 4x)^{-\frac{5}{2}}$ $f^{4}(x) = (-\frac{5}{2})(-\frac{3}{2})(-\frac{1}{2})(\frac{1}{2})(-4)(-4)(-4)(-4)(1 - 4x)^{-\frac{7}{2}}$ At this point I can eyeball the pattern and solve it manually. But again, what if I couldn't do that? Is there a more direct and methodical way to this?","['derivatives', 'number-theory', 'generating-functions', 'catalan-numbers', 'power-series']"
1872323,If I generate a random matrix what is the probability of it to be singular?,"Just a random question which came to my mind while watching a linear algebra lecture online.  The lecturer said that MATLAB always generates non-singular matrices. I wish to know that in the space of random matrices, what percentage are singular? Is there any work related to this?","['matrices', 'random-matrices', 'linear-algebra']"
1872385,Why is the unique readability of wff's important?,"I am reading Classical Mathematical Logic by Epstein. The author defines: $L(\neg, \rightarrow, \vee, \wedge, p_0, p_1, ...)$ i. For each $i=0, 1, 2, ..., (p_i)$ is an atomic wff, to which we assign the number $1$ . ii. If $A$ and $B$ are wffs and the maximum of the numbers assigned to $A$ and to $B$ is $n$ , then each of $(\neg A), (A \rightarrow B), (A \vee B), (A \wedge B)$ is a compound wff to which we assign the number $n+1$ . These are compound wffs. iii. A concatenation of symbols is a wff if and only if it is assigned some number $n \ge 1$ according to (i) or (ii) above. Then the author then proves the ""Unique Readability of wffs"", which he only defines as ""no wff can be read as, for example, both $A \rightarrow B$ and $C \wedge D$ "" I don't really see the benefits of proving this. The only benefits I can think of are: $1)$ Each wff can only be assigned one number according to (ii). I've looked over the next pages, and the book seems to only make use of this fact in the definition of the length of a wff. It doesn't seem important, because a proposition can only be assigned a number if it is using the maximum number of parenthesis (which we almost never do). There are plenty of equivalent propositions which are assigned different numbers. Similarly, there are plenty of equivalent such that $A \rightarrow B \equiv C \wedge D$ and at the end of the day, we mostly just care about equivalence; the number being assigned to a proposition seems like just an artifact we had to use in order to make our language formal. $2)$ If the unique readability theorem was false, then parenthesis don't behave in the intuitive way we want them to (that is, we intuitively believe that with enough parenthesis we can specify one and only one order of operations/connectives) While this seems more important, proving that a proposition is uniquely readable does not prove that our intuition of parenthesis is necessarily right. Am I missing something?","['propositional-calculus', 'logic', 'discrete-mathematics']"
1872400,Covariance Matrix of Uniform Distribution Positive Definite,"Suppose that $B$ is a Lebesgue measureable subset of $\mathbb{R}^d$. Let $U$ be the uniform distribution on $B$. Let $x \sim U$, $\mathbb{E}[x] = 0$. and let $M = \mathbb{E}[xx^T]$, be the covariance matrix. What conditions on $B$ guarantee that $M$ is strictly positive definite? EDIT: Figured it out. Suppose that $M$ is not p.d., then there exists $u$ such that $u'E[xx']u = 0$, which by linearity of expectation means that $E[x'u^2] = 0$. Since $x'u^2$ is a non-negative random variable with zero expectation this means that $x'u = 0$ almost surely. But that means that $x$ lies in $u^{\perp}$ with probability one, which means that up to sets of measure zero $B$ is contained in a lower-dimensional subspace of $\mathbb{R}^{d}$. Hence a sufficient (but clearly not necessary) condition is that $B$ is full-rank, i.e. contains an open set in $\mathbb{R}^d$.","['probability-theory', 'geometry']"
1872410,Israel tst 2011 geometrical inequality,"Inside an equilateral triangle of area $S$ lies a point, whose distances to the vertices are $x,y, z$. 
Prove that $xy + yz + zx \geq \frac{4}{\sqrt{3}} S$ I haven't got any idea yet. But I guess Fermat's point can be helpful....($x+y+z\ge \sqrt{3}a$)","['inequality', 'trigonometry', 'geometric-inequalities', 'geometry', 'contest-math']"
1872456,probability/combinatorics question with marbles,"An urn has 20 green out of 50 marbles. Draw all 50 marbles without replacement. Let
X = # of green marble runs of any length.
Example : GGGG BBB GG BB G BB. . .
In the above example, there are 3 runs in the first 14 trials. To get X, we would have to
examine the entire sequence of the 50 trials. Find P(X=1) For X=1 runs, that means that all 20 green marbles are together. There are 31 spots where that run could be. I came up with the answer
$\frac{20 \choose 20}{50 \choose 20} * 31 $ Is this correct? The biggest trouble that I have in computing my answer is that the green marbles have to be in a row . If the green marbles didn't have to be in a row then would it just be ${50 \choose 20}$?","['combinatorics', 'statistics', 'probability']"
1872480,Partial fractions and using values not in domain,"I'm studying partial fraction decomposition of rational expression. In this video the guy decompose this rational expression: $$ \frac{3x-8}{x^2-4x-5}$$ this becomes: $$\frac{3x-8}{(x-5)(x+1)} = \frac{A}{x-5} + \frac{B}{x+1} $$ $$[(x-5)(x+1)]\times \frac{3x-8}{(x-5)(x+1)} = \frac{A}{x-5} + \frac{B}{x+1}\times[(x-5)(x+1)]$$ $$3x-8 = A(x+1) + B(x-5)$$ then he substitutes $x$ with $-1$ this is when I feel something is wrong. because $x=-1$ is not in the domain of the original function and all the statement below the original function hold true only if they comply within the domain of the original function isn't it? so at the time this guy substitute $x=-1$ the statement become false? I'm newbie in math, please explain to me with easy understanding and step by step.","['algebra-precalculus', 'partial-fractions', 'functions', 'rational-functions']"
1872518,Solving for the inverse of a matrix,"I'm currently working on a problem in special relativity and it requires me to find the inverse of a matrix of a very specific form as follows: Given a 4x4 matrix $A$ of the form $$A = I + a\cdot\vec{v}\cdot\vec{v}^T$$  I need to find a matrix $B$ where $$B = I + b\cdot\vec{v}\cdot\vec{v}^T$$ such that $$ AB = I$$ Where  $\vec{v}\in \mathbb{R}^4$, and $I$ denotes the identity matrix. We are looking to find $b$ in terms of $a$. I tried multipliying the two expressions together but I wasn't really sure how to solve for $b$ in a matrix equation or how the outer product between $\vec{v}$ with itself plays a role in this problem.","['matrices', 'linear-algebra']"
1872534,inhomogeneous heat equation with mixed boundary conditons,"Solve $$U_{t}=U_{xx}+u$$ with mixed boundary conditions $$U_x(0,t)=0, U(l,t)=0$$
and initial condition $$U(x,0)=\varphi(x)$$ I know that I have to use separation of variables and I have an idea of how to do it when its either just Dirichlet or just Neumann but both together and with a source I have no idea any help would be appreciated.","['heat-equation', 'ordinary-differential-equations', 'partial-differential-equations']"
1872541,"If $A$ and $B$ are positive constants, show that $\frac{A}{x-1} + \frac{B}{x-2}$ has a solution on $(1,2)$","I have problem which I couldn't figure out how to solve;
If $A$ and $B$ are positive constants, show that $$0=\frac{A}{x-1} + \frac{B}{x-2}$$ has a solution on the open interval  $(1,2)$. If you support your answers with rigorous proof, I appreciate that. What I thought was taking interval roughly close to the end points from the inside i.e $[1.1,1.9]$, but then it wouldn't be rigorous solution to this problem. After that, I totally stuck since I couldn't determine to closed interval, which prevented me from using any useful theorem. Note: The problem is taken from G.Simmons Calculus with Analytic Geometry 2nd.","['partial-fractions', 'calculus', 'limits']"
1872565,PDF/CDF of max-min type random variable,"For i.i.d. random variables, we may write the CDF of $t=\max(t_1,\cdots,t_N)$ as 
$$F_t(t)=F_{t_i}(x)^n$$
and the CDF of $x=\min(x_1,\cdots,x_N)$ as 
$$F_x(x)=1-(1-F_{x_i}(x))^n$$ When we have $X=\max(\min(\cdots),\cdots,\min(\cdots))$, we may combine above results if every entry in $\min(\cdots)$ are independent to each other in each $\min$ term. However, I have some common terms in $\min(\cdots)$ terms. For example: 
$$X=\max(\min(x_1,x_2,x_3),\min(x_1,x_4,x_5),\min(x_5,x_6,x_7),\min(x_3,x_6,x_8))$$
I understand that there are correlations, but I am not sure how I can apply, specially for the general case. Can someone guide me to derive $F_X(x)$?","['correlation', 'probability-theory', 'probability-distributions', 'probability', 'order-statistics']"
1872569,Algebraic Expressions with Fractions,"can someone review this and see if i've done it correctly please. $$\frac{\frac {3x} {y}}{\frac {2x}{7}}
$$ $$= \frac{3x}{y} . \frac{7}{2x}$$ $$= \frac{21x}{2xy}
$$ $$= \frac {21}{2y}
$$ Thank you for your time.",['algebra-precalculus']
1872573,A proof by René Schilling that a continuous Lévy process is integrable,"In his treatise ""An Introduction to Lévy and Feller Processes"" ( arXiv link ), Prof. Dr. René Schilling gives a short and seemingly straightforward proof for the claim that a continuous Lévy process is integrable (Lemma 8.2 on p. 50). More precisely, the claim goes as follows. Lemma Let $(X_t)_{t\geq0}$ be a Lévy process with càdlàg paths such that $|\Delta X_t(\omega)|:=|X_t-\lim_{s\uparrow t} X_s|\leq c$ for all $t\geq 0$ and some constant $c>0$. Then $E(|X_t|^p)<\infty$ for all $p\geq 0$. The proof begins as follows. Proof Let $\mathcal{F}^X_t:=\sigma(X_s,s\leq t)$ and define the stopping times
  $$
\tau_0:=0,\hspace{1cm} \tau_n:=\inf\left\{t>\tau_{n-1} : |X_t-X_{\tau_{n-1}}|\geq c\right\}.
$$
  Since $X$ has càdlàg paths, $\tau_0<\tau_1<\tau_2<\cdots$. Moreover, by the strong Markov property, $\tau_n-\tau_{n-1}\sim\tau_1$ and $\tau_n-\tau_{n-1}\perp\mathcal{F}^X_{\tau_{n-1}}$, i.e. $(\tau_n-\tau_{n-1})_{n\in\mathbb{N}}$ is an iid sequence. The strong Markov property referred to in the proof is the following theorem (Theorem 4.12 on p. 29): Theorem Let $X$ be a Lévy process on $\mathbb{R}^d$ and set $Y:=(X_{t+\tau}-X_\tau)_{t\geq0}$ for some a.s. finite stopping time $\tau$. Then $Y$ is again a Lévy process satisfying a) $Y\perp(X_r)_{r\leq\tau}$, i.e. $\mathcal{F}^Y_\infty\perp\mathcal{F}^X_{\tau^+}:=\left\{F\in\mathcal{F}^X_\infty : F\cap\{\tau<t\}\in\mathcal{F}^X_t, \forall t\geq 0\right\}$. b) $Y\sim X$, i.e. $X$ and $Y$ have the same finite dimensional distributions. So the strong Markov property can only be applied in the lemma if $\tau_n$ is an a.s. finite stopping time, but why should this be the case? So suppose, to avoid this caveat, we adjust the underlying probability space and consider only those $\omega$'s for which $\bigwedge_{k=1}^\infty \tau_k(\omega)\neq\infty$. Why should the correspondingly adjusted process $X$ remain Lévy, as required in order to apply the strong Markov property?","['stochastic-processes', 'probability-theory', 'markov-process', 'stopping-times', 'levy-processes']"
1872575,Calculation of $\frac{a_{20}}{a_{20}+b_{20}}$?,"The solution of $\frac{a_{20}}{a_{20}+b_{20}}$ is $-39$ (This is wrote by answer sheet) from the recursive system of equations : \begin{cases}
a_{n+1}=-2a_n-4b_n  \\
b_{n+1}=4a_n+6b_n\\
a_0=1,b_0=0
\end{cases} This is taken from $2007$ GATE entrance exam in India . anyone can show me how we can calculate this answer? Update 1: Three answer is added, but my main problem is remains up to yet, non of these three answers didn't include the main aspect of this question. my main problem is via simplification and replacement in last part of solution.","['recurrence-relations', 'real-analysis', 'algebra-precalculus', 'recursion', 'linear-algebra']"
1872578,Commutativity of matrix and its transpose,If a matrix is symmetric or skew-symmetric it commutes in the obvious way with its transpose. (For symmetric: $SS^T=S^2$ and $S^TS =S^2$) The less obvious is the case of commutativity  for orthogonal matrix but such matrix also commutes with its transpose because $RR^T=RR^{-1}= I= R^{-1}R=R^TR$. Questions: Are there  other cases when a matrix commutes with its transpose ? What is the general property of such matrix which allows it to commute with its transpose ?,"['matrices', 'linear-algebra']"
1872620,Understanding the Skorohod-space,"I am having a lack of understanding the Skorohod space considering cadlag processes. A random variable $X$ is measurable  mapping between two measure spaces  say $(\Omega,\mathcal{F})\mapsto (\tilde{\Omega},\tilde{\mathcal{F}})$. I am struggeling about the primary space and the state space in terms of the Skorohod space, when we are considering cadlag-processes. This may be trivial, but i hope you can help me out. Lets say we have a cadlag-process $X=(X_t)_{t\geq 0}$. How can we describe the mapping in terms of the Skorohod space $D(T,\mathbb{R}^{k})$, which is defined by consisting all cadlag paths from $T$ to $\mathbb{R}^{k}$, where $T:=[0,\infty)$
We denote by $\mathcal{B}_T$ the corresponding $\sigma$-algebra , So can we  relate $D([0,\infty)\mathbb{R}^{k}$ to $\Omega$ or $\tilde{\Omega}$? 
An what does terms of the canonical presentation $X(t,\omega)=\omega(t)$ change? My attempt: Generally we have a given probability space $(\Omega,\mathcal{F},P)$ and a cadlag process $X=(X_t)_{t\geq 0}$ defined on it. For every $\omega \in \Omega$ the mapping $X(\omega)$ defined by
$$
X(\omega): T\rightarrow \mathbb{R}^{k}\\
t\rightarrow X_t(\omega)
$$
is an element of $D(T,\mathbb{R}^{k})$. So we can see $X$ as the mapping 
$$
X:\Omega \mapsto D(T,\mathbb{R}^{k})
$$
This mapping is $(\mathcal{F},\mathcal{B}_T)$ measurable. We denote $Q:=P^{X}$ as the by $X$ induced measure on $(D(T,\mathbb{R}^{k}),\mathcal{B}_T)$. So for every $\mathbb{R}^{k}$-valued cadlag-process $X$, we can refer the Skorohod space, as the statespace of $X$. However  $X$ can be seen as the equivalent process of $\pi:=(\pi_t)_{t\geq 0}$ on $(D(T,\mathbb{R}^{k}),\mathcal{B}_T)$; the coordinate process $\pi_t(\omega)=\omega_t$ for $\omega \in  D(T,\mathbb{R}^{k})$
so that
$$
\pi:(D(T,\mathbb{R}^{k}),\mathcal{B}_T)\mapsto (D(T,\mathbb{R}^{k}),\mathcal{B}_T)
$$
Let $Q^{\pi}$ be the induced measure of $\pi$. Then
on $((D(T,\mathbb{R}^{k}),\mathcal{B}_T,Q)$ it holds $Q^{\pi}=Q=P^{X}$ as defined above. So we can always identify a cadlag process as a random variable on the Skorohod space. So we can use both representations","['stochastic-processes', 'probability-theory', 'probability', 'stochastic-calculus']"
1872621,"If you roll two six-sided dice, what is the probability that the dice add to 10 or higher?","When answering these sort of questions people mostly resort to diagrams and I'm wondering if there is a way to calculate the probability without going through each outcome, just solely on the given variables.","['statistics', 'probability']"
1872632,Iterating three tangent circles using Malfatti Circles,"First, construct three tangent circles (blue circles), then construct the triangle joining their centers. Then construct three Malfatti Circles for this triangle (green circles). Go on. What I'm asking is this - it appears that the radii of the green circles are closer together, than the radii of the blue cirlces. I.e., if the radius of a blue circle is $R_k$ and the green circle $r_k$, then we have: $$\frac{\max{R_k}}{\min{R_k}} \geq \frac{\max{r_k}}{\min{r_k}},~~~k=1,2,3$$ Where equality is supposed to be only for three equal cirlces. Is this true for any initial three cirlces? Does it mean, that this iteration will have three equal cirlces as a limit? I know that there are formulas for the radii of the Malfatti Circles, but they are very complicated and I would appreciate it if someone gives me a quick answer. If not, I will have to do a lengthy calculation.","['circles', 'triangles', 'geometry']"
1872652,"How to express the rest of division by three, with very elementary functions?","Is it possible to express 
$\; ""\!n\pmod 3\!""\;$ 
with combinations of the functions plus, minus, multiplication, division and exponentiation in $\mathbb C$ or preferably in $\mathbb Z[i]$? I'ts not allowed with modulo or integer parts, and the inverses are only valid if they can be expressed as above. Also all sums, products etc must be finite. There is a simular question Closed-Form Modular Arithmetic where trigonometric functions and logarithms are allowed. If it isn't possible, how to prove that?","['algebra-precalculus', 'modular-arithmetic', 'provability']"
1872674,Volume in zero dimensional space,"Suppose $A\subset \mathbb{R}^n$ is a compact, convex and centrally symmetric set such that $(x_1,\ldots,x_n)\in A$ if
$$ |x_1|+\ldots+|x_r|+2\left(\sqrt{x_{r+1}^2 + x_{r+2}^2} + \ldots + \sqrt{x_{n-1}^2 + x_{n}^2}\right) \leq n$$ If $n=r+2s$, I want to prove that (Lebesgue measure)
  $$ \mathrm{vol}(A) = \frac{n^n}{n!}2^r \left(\frac{\pi}{2}\right)^s$$ To prove this, I assumed that $V_{r,s}(t)$ denote the volume of the subset $\mathbb{R}^{r+2s}$ defined by
$$|x_1|+\ldots+|x_r|+2\left(\sqrt{x_{r+1}^2 + x_{r+2}^2} + \ldots + \sqrt{x_{r+2s-1}^2 + x_{r+2s}^2}\right) \leq t$$ Therefore,
$$V_{r,s}(t) = t^{r+2s}V_{r,s}(1)$$
I have so far proved that ( following this ): $$V_{r,s}(1) =\frac{1}{n!}2^r \left(\frac{\pi}{2}\right)^sV_{0,0}(1)$$ But to conclude my proof I must show that $$V_{0,0}(1)=1$$ So, the question is : Why $V_{0,0}=1$?","['multivariable-calculus', 'integration', 'lebesgue-measure', 'volume']"
1872713,Understanding a Cosine Derivative,"Let $c$ be a constant. Why is it that $$
D_x \left(- \frac{\cos(cx)}{c} \right) = \sin(cx)?
$$ I understand that $D_x \cos(x) = - \sin(x)$.  So what trigonometric identity is allowing us to infer the above?","['derivatives', 'trigonometry']"
1872716,Induction - Countable Union of Countable Sets,"Stephen Abbott has a an exercise in Chapter 1 (1.2.12) that suggests that one cannot use induction to prove that a countable union of countable sets is countably infinite. One answer is that $n={}$ infinity cannot be demonstrated via induction, as inifinity is not a natural number. This seems sketchy. Rudin  in chapter 2 clearly distinguishes the use of inifinity symbol for a union of sets to indicate a countably infinite union of sets and distinguishes it from the infinity used to extend the reals. All of this also appears to ignore the fact $N$ is countably infinite by definition. Therefore any bijection with $N$ is also proved for countably infinite cases. So why cannot induction be used to argue countable union of countable sets is countable? Here is an example where induction is being used in the context of countably infinite sets. Using induction to prove that the infinite set of polynomials is countably infinite","['induction', 'elementary-set-theory']"
1872717,Is every totally ordered finite dimensional vector space a lexicographic order for some basis?,"Let's say we have a finite-dimensional vector space $V$ over a totally ordered field $\mathbb{K}$. Is every choice of totally ordered vector space structure (i.e compatible with the addition and scalar multiplication) on $V$ a lexicographic order for some ordered basis? By ""compatible"" I mean that the translation and multiplication by non-negative scalars are order homomorphisms. Sorry if this is a dumb question; I know next to nothing about order theory. The reason I'm wondering is that I'm curious if a choice of base (i.e. simple roots) for a root system $\Phi$ in $V$ is equivalent to a choice of totally ordered vector space structure on $V$ by lexicographic ordering.","['linear-algebra', 'order-theory']"
1872749,Are polynomial fractions and their reductions really equal? [duplicate],"This question already has answers here : Is $\frac{x^2+x}{x+1}$ a polynomial? (3 answers) Closed 7 years ago . I'm reading Larson's AP Calculus textbook and in the section on limits (1.3) it suggests finding functions that ""agree at all but one point"" in order to evaluate limits analytically.  For example, given 
$$
f(x) = \frac{x^3-1}{x-1}
$$
we can factor and reduce to get
$$
\frac{x^3-1}{x-1} = \frac{(x-1)(x^2+x+1)}{x-1} = \frac{\require{cancel} \bcancel{(x-1)}(x^2+x+1)}{\require{cancel} \bcancel{(x-1)}}=x^2+x+1
$$
But it then refers to this reduced expression as 
$$
g(x) = x^2+x+1
$$
suggesting that $f(x) \neq g(x)$ (also implied by the figure below). Is it true that $f(x) \neq g(x)$?  I realize that $f(x)$ as originally expressed is indeterminate at $x=1$, but doesn't the cancellation of $(x-1)$ allow us to calculate the ""true"" value of $f(1)$?  If so, I believe the graph of $f(x)$ in the figure is misleading at best by drawing the plot as being undefined at $x=1$.","['graphing-functions', 'polynomials', 'calculus', 'functions']"
1872751,Separable polynomial definition (Confused),"I understand there is a new and old definition of separable polynomial ( https://en.wikipedia.org/wiki/Separable_polynomial ). For the following definition: a) A polynomial over $F$ is said to be separable if it has no multiple roots (given in Dummit & Foote). b) A polynomial $P(X)$ over a given field $K$ is separable if its roots are distinct in an algebraic closure of $K$ (Wikipedia). c) A polynomial $f$ in $F[X]$ is a separable polynomial if and only if every irreducible factor of $f$ in $F[X]$ has distinct roots ( https://en.wikipedia.org/wiki/Separable_extension#cite_note-6 ). Question: Which is the correct definition? I can see that (b) implies (c), for instance, but not sure if they are equivalent. Thanks for help.","['abstract-algebra', 'galois-theory']"
1872776,Combinatorics: 30 people rotating around 6 tables - largest number of different people each person can meet?,"Say there are 30 people at an event and six tables (A-F), each seating five people. There are six sessions, and each person must visit each table exactly once. I /think/ that it's impossible for everyone to meet 24 different people under these conditions (because the number of tables isn't prime?), but is there an algorithm for working out the largest number of people each person can meet? I've tried various trial and improvement methods and am getting nowhere! Thanks in advance for the help.",['combinatorics']
1872834,Getting from the ring of periods to the field of periods (which constant become periods in this case)?,"The Ring of periods is a fascinating concept in number theory. However, it's rather restrictive, since many popular constants (such as $e$, $\gamma$) are not periods. Periods are integrals of algebraic functions over algebraic domains. Periods form a ring, i.e. sum and product of periods is a period. However, they do not form a field, as far as I know, i.e. the quotient of periods is not necessarily a period (only if the denomiator is an algebraic number, as far as I understand). What happens if we also consider any quotient of periods a period? Will some additional important constants join the happy family of periods? One example I have (and the motivation for this question) is the Gauss hypergeometric function , which has an interesting integral definition: $${\displaystyle \mathrm {B} (b,c-b)\,_{2}F_{1}(a,b;c;z)=\int _{0}^{1}x^{b-1}(1-x)^{c-b-1}(1-zx)^{-a}\,dx\qquad \Re (c)>\Re (b)>0,}
$$ provided $|z| < 1$ or $|z| = 1$ and both sides converge Using the integral definition of the Beta function we can write: $$_2F_{1}(a,b;c;z)=\frac{\int _{0}^{1}x^{b-1}(1-x)^{c-b-1}(1-zx)^{-a}\,dx}{\int_{0}^{1} x^{b-1} (1-x)^{c-b-1}dx}$$ Thus for any $a,b,c \in \mathbb{Q}$ (provided the integrals converge) and $z$ algebraic the hypergeometric function will belong to the field of periods. The other example is $1/ \pi$, since it's still not known if it's a period (according to Wikipedia). A more interesting example is from this answer : $$\int_0^{\infty} \dfrac{\tanh(x)\,\tanh(x s)}{x^2}\,dx = \frac{4s}{\pi^2} \int_0^1 \ln\left(\frac{1-x}{1+x}\right) \ln\left(\frac{1-x^s}{1+x^s}\right) \,\frac{dx}{x} $$ This function belongs to the field of periods for any rational $s$, because $\pi^2$ is a period, and logarithms can be represented by integrals of rational functions (or algebraic functions if $s$ is not whole). Any references on this topic (specifically the field of periods, not the ring of periods) will be appreciated as well.","['number-theory', 'special-functions', 'hypergeometric-function', 'constants']"
1872855,Two Divergent series such that their sum is convergent.,"Give an example of two divergent series of real numbers sch that their sum is convergent. I have read that the sum of two divergent series can be divergent or convergent. I have found that, the series $\sum_{n=1}^\infty\frac{1}{n}$ and $\sum_{n=1}^\infty\frac{1}{n+1}$ both are divergent series and their sum $\sum(\frac{1}{n}+\frac{1}{n+1})$ is also a divergent series. Again, If we take $u_n=(-1)^n$ and $v_n=(-1)^{n+1}$ Then both $\sum u_n$ and $\sum v_n$ are divergent (Oscilatory). But their sum, i.e, $\sum (u_n+v_n)$ is convergent and equals to $0$. But I cannot find any other example of two divergent series with their sum convergent. How can I give such an example. Please help...!!",['sequences-and-series']
1872898,How do you find the metric tensor for a given manifold?,"Is there some general way to derive the metric tensor for a given manifold M ? For example, how was the metric for the surface of a sphere $$ds^2=d\theta^2+\sin^2\theta \, d\phi^2$$ first derived?","['tensors', 'manifolds', 'metric-spaces', 'differential-geometry', 'surfaces']"
1872900,Do eigenvalues of a linear transformation over an infinite dimensional vector space appear in conjugate pairs?,"While attempting to answer a question here (namely, the finite dimensional case of the title question: Prove that if $\lambda$ is an eigenvalue of $T$, a linear transformation whose matrix representation has all real entries, then $\overline{\lambda}$ is an eigenvalue of $T$), I noticed the asker did not specify a finite dimensional vector space. Though the person who asked the question was satisfied with a finite-dimensional response, I was wondering if the analogue was true for infinite dimensional vector spaces. I have seen several proofs of this fact relying on $V$ being finite dimensional. One proof utilizes the roots of the characteristic polynomial; if the coefficients are real then the roots come in conjugate pairs. The second notable proof I've seen goes something like: $$(T-\lambda I)v = 0$$
$$\overline{(T-\lambda I)v} = 0$$
$$(\overline{T} - \overline{\lambda I})\overline{v} = 0$$
$$(T- \overline{\lambda}I)\overline{v} = 0$$ where we define $\overline{T}$ as taking the conjugate of each element of the matrix representation of $T$, and we define $\overline{v}$ as conjugating each entry in the n-tuple representation of $v$ with respect to a basis. Going backwards will give you that, given the conditions set earlier, $\lambda$ is an eigenvalue if and only if $\overline{\lambda}$ is an eigenvalue, and also, $v$ is an eigenvector with eigenvalue $\lambda$ if and only if $\overline{v}$ is an eigenvector with eigenvalue $\overline{\lambda}$. The first thing we would have to do is have some notion that is similar to the matrix representation of $T$ having all real entries. What exactly would that be? Would we have to work with infinite matrices, or (assuming the axiom of choice) could we define $T$ such that it takes basis vectors to linear combinations of basis vectors with real coefficients and that would suffice? If we assume the axiom of choice and take a basis of $V$, I am under the impression that the second proof I provided for the finite dimensional case could extend to the infinite dimensional case. Is it necessary to use the axiom of choice for a proof, though? Overall, my question is: 
First, is there an analog of $T$ having all real matrix entries in an infinite dimensional case? Denote this property, if it exists, $P$. Second: Does anyone have a proof or counterexample of the following?:
Let $V$ be an infinite dimensional complex vector space, and let $T$ be a linear transformation with $P$. $\lambda$ is an eigenvalue of $T$ if and only if $\overline{\lambda}$ is an eigenvalue of $T$. Can we also add: $v$ is an eigenvector with eigenvalue $\lambda$ if and only if $\overline{v}$ is an eigenvector with eigenvalue $\overline{\lambda}$? Whatever $\overline{v}$ may happen to mean in this case. If we can do this without infinite matrices, infinite basis, or assuming the axiom of choice, I would much prefer that! But I understand it may be necessary.","['functional-analysis', 'eigenvalues-eigenvectors', 'linear-algebra', 'infinite-matrices']"
1872905,Does a differential manifold implies existence of unique tangent space at every point?,"I like differential geometry and I want to know if a differentiable manifold implies unique tangent space at every point. I have searched but the definition I have found of differential manifold is that differentiable manifold is a topological manifold in which we construct an atlas that covers the entire manifold and the transition functions in the intersection of the coordinate systems of the atlas are differentiable. But this doesn't say anything about tangent spaces. Whatever I have read about differential geometry, tangent spaces are defined always after the definition of differentiable manifold, so it seems tangent spaces are not need to define what a differentiable manifold is and for a topological manifold to be a differentiable manifold is only need that we construct an atlas with the characteristics I said before. But then if you have for example, a 1 dimensional topological manifold that is a real line with one point that is not smooth, because it has the form of a peak, there is no unique tangent space at this point, although there is in the other points. Then we construct an atlas in which the transition functions are differentiable at this point just  because we can define the coordinate systems in a way that the transition functions from one coordinate to another  is differentiable. Although the manifold has no unique tangent space in this point. I always believed that differentiability at manifold level implies unique tangent space and that the manifold doesn't care about coordinate system differentiability. But the definition of differentiable manifold is based on differentiability in coordinate systems. So if we can define an atlas that is smooth but the real manifold is not, Therefore, differentiable manifold doesn't implies unique tangent space. Am I missing something? Sorry for my bad expression but english is not my mother tongue and I tried to express the better I can.",['differential-geometry']
1872909,Partially ordered set and a method that can quickly solve these questions?!,"I prepare for Msc Qualification exam. This is 2015 Exams, with Answer
  is option (2). Set $M={2,3,4,...}$ is given. Suppose $M \times M$ is sorted in which $ (a,b) \leq (c,d)$ if and only if $c$ is Divisible on $a$ and condition $b \leq d$ is  hold. Which of these option is true about minimal and maximal elemetns of partial ordered set $(M \times M, \leq)$? in following option $p$ is arbitrary prime number. 1) each pair $(p,m)$ for $m \in M$ be a minimal element and there is no maximal element. 2) each pair $(p,m)$ for $m \in M$ be a minimal element and there is maximal element. 3) each pair $(p,2)$ for $m \in M$ be a minimal element and there is no maximal element. 4) each pair $(p,m)$ for $m \in M$ be a minimal element and there is maximal element. my challenge is via a method that can solve this problem and like it, in a quick way? is it any description or hint?","['relations', 'elementary-set-theory', 'discrete-mathematics']"
1872925,How to describe a summation of $\frac{1}{2^x3^y}$ and evaluate.,I want too calculate the value of this sum: $$\sum  \frac{1}{2^x3^y}$$ Where we sum up all permutations of terms involving a nonnegative integer $x$ and a nonnegative integer $y$. How can I notationally describe this sum and how can I calculate it (assuming it does converge).,"['algebra-precalculus', 'notation', 'calculus']"
1872929,Solution(s) to dot product of vectors,"I have some questions about the uniqueness of matrices when post- and pre-multiplied with vectors (inner product). Say we have two vectors $\vec{a}$ and $\vec{b}$, whose inner product is a scalar, known to satisfy the following equation involving matrix $\left[C\right]$: $$
\vec{a} \cdot \vec{b} = \vec{a}^T \vec{b} = \vec{a}^T \left[C\right] \vec{b}
$$ In this case, is $\left[C\right]$ guaranteed to be the identity matrix? Can it be anything else? Why? Along the same lines, is it ever possible to ""eliminate"" vectors from an equation? For example, if we also have a matrix $\left[D\right]$ that satisfies the equation: $$
\left[C\right] \vec{b} = \left[D\right] \vec{b}
$$ Could we just post-multiply each side by $\vec{b}^{-1}$ to obtain $\left[C\right]$ = $\left[D\right]$? Is this valid under any set of conditions? Thanks",['linear-algebra']
1872959,Consequtive integers covered by $n$ arithmetic progressions,"Suppose there are $n$ terms of an arithmetic progression of the form $$(2i+1)k + x_i, ~~~~ i = 1,\ldots,n, ~~~~ k \geq 0,$$ for varying initial integer terms $x_i \geq 0$. The problem is to cover $m(n)$—the maximum possible number of consecutive integers starting with $1$ (the number is covered, if it belongs to at least one of these progressions). For example, is it true that
$$m(n) \approx \operatorname*{LCM}\limits_{i=1,\,\ldots\,,\,n}(\{2i+1\}) \text{?}$$ Numerical simulations seem to suggest $m(n) \approx Cn$.","['number-theory', 'dynamical-systems', 'prime-numbers', 'sequences-and-series']"
1872968,How to find the determinant of this $n \times n$ matrix in a clever way?,"Is there any clever and short way to find out the determinant of the following matrix? \begin{bmatrix}
        b_1 & b_2 & b_3 & \cdots & b_{n-1} & 0 \\
        a_1 & 0 & 0 & \cdots & 0 & b_1 \\
        0 & a_2 & 0 & \cdots & 0 & b_2\\
        \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
        0 & 0 & 0 & \cdots & a_{n-1} & b_{n-1} \\
        \end{bmatrix} Any help will be appreciated.","['matrices', 'linear-algebra', 'determinant']"
1872977,How to decompose $\sin(a+b)$ into $f(a)+g(b)$?,"I wonder how to decompose  $\sin(a+b)= f(a)+g(b)$ so that $f, g$ were not infinite series assuming that $a$ and $b$ are real numbers?",['trigonometry']
1872996,Interesting property of Fibonacci numbers,"Let we have an integer number $m$ such that $p \mid m \implies (p^2-1) \mid m$  for any prime divisor $p$ of $m.$ Prove that for such $m$ we have $F_{n+m}=F_{m} \mod m $  for any $n>1.$ Any ideas  how to prove it? I checked  it for $m=24, 48 $ but cant prove in general.","['number-theory', 'combinatorics', 'elementary-number-theory']"
1872998,Prove that matrix $A$ diagonalizable if $A^2=I$ using characteristic polynomial,"Prove that the matrix $A$ is diagonalizable if $A^2=I$ using characteristic
  polynomial I saw an answer that used the minimal polynomial of $A$. Can that be proven without using minimal polynomial? In my university we didn't learn minimal polynomial and therefore we cannot use it in our tasks and exams. We didn't get this particular question for homework but I just wanted to know if I can use this claim in my tasks and exams.","['matrices', 'diagonalization', 'linear-algebra']"
1873008,Ham Sandwich Theorem - intuitive proof,"Ham Sandwich Theorem . Given 3 measurable ""objects"" in $\mathbb{R}^3$, it is possible to divide all of them in half (with respect to their measure, i.e. volume) with a single 2-dimensional plane. Can we turn the following idea into a constructive proof of the Ham sandwich theorem? Let $B^3_1$, $B^3_2$ and $B^3_3$ be balls in $\mathbb{R}^3$. It is clear that we can divide every volume in two with a plane that intersects the centers of those balls. But now we are free to morph upper and lower halves in an ""isovolumetric"" fashion without changing the fact that the plane halves the volume. Also, if there are volume migration between the sides of the plane, then the net change must be zero. If we divide each half ball into a large, but finite, collection of cubes, then it should be clear that we can approximate any shape with equal volume.","['algebraic-topology', 'general-topology', 'differential-geometry', 'soft-question']"
1873009,"Geometric mean, harmonic mean and loss functions",Consider a sequence $(x_i)_{i \in I}$ of real numbers indexed on a set $I$. The mode of the series is the minimizing argument for the $L_0$ loss $$ \text{mode}[ \; (x_i)_{i \in I} \; ] = \arg\min_{u \in \mathbb{R}} \; \; \sum_{i \in I} \; | x_i - u|^0 $$ The median is the minimizing argument for the $L_1$ loss $$ \text{median}[ \; (x_i)_{i \in I} \; ] = \arg\min_{u \in \mathbb{R}} \; \; \sum_{i \in I} \; | x_i - u|^1 $$ The arithmetic mean is the minimizing argument for the $L_2$ loss $$ \text{arithmetic mean}[ \; (x_i)_{i \in I} \; ] = \arg\min_{u \in \mathbb{R}} \; \; \sum_{i \in I} \; | x_i - u|^2 $$ Can we find similar results for the harmonic mean or the geometric mean? Thanks!,"['means', 'statistics', 'average']"
1873042,Name this 2-dim Parametric Curve,"is there a name for the following parametric curve? Thanks. I encountered it when playing around with the tangent lines and normal lines of an ellipse. The parameter $\theta$ is the same parametric angle for the related ellipse that's $\{x = A\cos\theta,\; y = B\sin\theta \}$. $$ x = \frac{A^2-B^2}{A^2\cos^2\theta + B^2\sin^2\theta }\cdot A\cos\theta \\
 y = \frac{A^2-B^2}{A^2\cos^2\theta + B^2\sin^2\theta }\cdot B\sin\theta \\
$$
for any $A>0,\; B>0$ It looks really familiar, doesn't it? The plot looks like a goggle ( click to see image ) and approaches two separate circles when $A \ll B$.  I don't have much clue as to how to make this into polar coordinates. The Mathematica code for the image is as below A = 2; B = 5;
ParametricPlot[{
  {  (* ellipse  *)
   A*Cos[u], 
   B*Sin[u]  },
  (A^2 - B^2)/(A^2*Cos[u]^2  +  B^2*Sin[u]^2) { A*Cos[u], B*Sin[u]
    } (* the parametric curve in question *)
  },
 {u, 0, 2 \[Pi]},
 PlotLabel -> ""A = "" <> ToString[A] <> "", B = "" <> ToString[B]
 ] . . ======== Update ========= In response to Blue's comment, here is how this parametric curve arises: Take the normal line of a given point on the ellipse $\vec{r_0} \equiv \{x_0 = A\cos\theta,\; y_0 = B\sin\theta \}$:
$$ y = y_0 + \frac{ A^2 y_0 }{B^2 x_0} (x - x_0) \qquad \text{or} \qquad B^2 x_0 \cdot y = (B^2 - A^2) x_0 y_0 + A^2 y_0 \cdot x $$ Consider the intercepts of this normal line with the axes:
$$ y \to 0 \implies x_n \equiv \frac{A^2 - B^2}{A^2} x_0 \\
x \to 0 \implies y_n \equiv \frac{B^2 - A^2}{B^2} y_0 
$$
Note that $x_n$ and $x_0$ are of opposite sign when $A < B$, while $y_n$ and $y_0$ are of opposite sign when $A > B$. Now, an invariance can be formed by these intercepts:
$$ \vec{r_1} \equiv \{ x_n, -y_n \}  \implies \vec{r_0} \cdot \vec{r_1} = A^2 - B^2 \\
\vec{r_2} \equiv \{ y_n, x_n \}  \implies \vec{r_0} \times \vec{r_2} = A^2 - B^2 
$$
The inner product and the outer product (cross) can be considered equivalent forms of the same thing, but geometrically the outer product gives the signed triangle area whereas the inner product is more difficult to show in a plot. The parametric curve in question is an attempt to visualize $\vec{r_0} \cdot \vec{r_1}$, for example as seen in this diagram for $A=3,\; B=1$: the red dot marks $\vec{r_1} = \{ x_n, -y_n \}$, the dim red dot marks $\{ x_n, y_n \}$ as a reference, the green bigger ellipse is the locus of $\vec{r_1}$ and also of
the reference point, the blue dot is the point on the ellipse $\vec{r_o}$, the ray coming out of the blue dot is the normal line, where the
intercepts $(x_N, 0)$ and $(0, y_N)$ are also marked, the ray coming out of the red dot shows the projection (for inner
product) of $\vec{r_1}$ onto the dim blue thick ray for $\vec{r_0}$, and finally, the yellow-ish curve is the locus of the parametric curve in
question, being the intersection of the aforementioned gray thin ray
projected onto the thick blue ray. Note that the yellow curve is not really demonstrating the invariance $A^2 - B^2$, which is just a factor in the parametric equation. I'm still working on visually showing the invariance as an inner product, and this goggle-like curve is so far just a delightful by-product along the way. Thank you for your time.","['algebra-precalculus', 'conic-sections', 'plane-curves']"
1873049,Find all solutions of $e^{e^z}=1$ in the complex space.,"Find all solutions of $e^{e^z}=1$ in the complex space. Attempt: $e^{e^z}=1$. Assuming $e^z$ is a complex number, I will start off solving $e^z=e^{x+yi}=1$: $e^x(\cos y+i\sin y)=1\Rightarrow \sin y=0\Rightarrow y=\pi k$. Now, $e^x>0\Rightarrow \cos y =0,\pm 1$. Then $y$ has to be $2\pi k$, and $x$ has to be $0$. Assuming I don't miss solutions (which I really can't tell), I set: $$e^z=2\pi k i$$.
$e^x(\cos y+i\sin y)=2\pi k i\Rightarrow y={\pi\over 2}+\pi m, \sin y=\pm 1$. Now $\pm e^x =2\pi k$ (Am I allow to do this?). Now I get $x=\ln (2\pi k)$
but $\ln$ only work with $k\ge 1$ in the Real Numbers, so taking $\pm e^x=2\pi k (k>0)$ instead seems to solve the issue. I get: $z=ln(2\pi k)+i({\pi \over 2}+\pi m), (k,m)\in \Bbb{Z}_+\times \Bbb{Z}$. Checking it gives the required results. My only insecurity is all the branches idea. Would you guide me?","['complex-analysis', 'proof-verification']"
1873055,"Can we say that $\prod_{i=1}^n (1-x_i)\ge 1-\sum_{i=1}^n x_i,\ \forall n\in \mathbb{N},\ \forall x_i\in [0,1)$?","Can we say that $$\prod_{i=1}^n (1-x_i)\ge 1-\sum_{i=1}^n x_i$$ for all $n\in \mathbb{N}$ and for all $x_i\in [0,1)$ ? The statement is easy to see to be true for $n=2,3$ . However, what to do for general $n\in \mathbb{N}$ ? I am having this feeling that this should be a very trivial/well studied thing, but I am afraid to say I do not know a name for this in this in literature. So, please refer me to appropriate literature if this is pretty well known. I thought of a pretty trivial ""proof"" using induction, but I am not sure about the applicability of induction in this case. I am still providing the ""proof"" for completeness:
The statement is true for $n=1,2$ . Let it be true for $k$ . Then, $\forall,\ x_1,\cdots,\ x_k\in [0,1)$ , $$\prod_{i=1}^k (1-x_i)\ge 1-\sum_{i=1}^k x_i$$ Then note that, for any $\{y_i\}_{i=1}^{k+1}\in [0,1)$ , we have $$
\prod_{i=1}^{k+1}(1-y_i)
    \ge \left(1-\sum_{i=1}^k y_i\right)(1-y_{k+1})
    \ge 1-\sum_{i=1}^{k+1}y_i
$$ hence the claim is established. $\blacksquare$ Any help is appreciated.","['algebra-precalculus', 'induction', 'inequality', 'probabilistic-method']"
1873067,Which answer is correct? Finding the limit of a radical as $x$ approaches infinity.,"When I take $$\lim_{x \to -∞} \sqrt{x^2+7x}+x,$$ I multiply by the conjugate over the conjugate to get $$\lim_{x \to -∞}\frac{7x}{\sqrt{x^2+7x}-x},$$ and multiply by either $\frac{\frac{1}{x}}{\frac{1}{x}}$ or $\frac{\frac{1}{-x}}{\frac{1}{-x}}$ to get an undefined answer or $\frac{-7}{2}.$ My teacher's solution involves multiplying by $\frac{\frac{1}{-x}}{\frac{1}{-x}}:$ $$=\lim_{x \to -∞}\frac{-7}{\sqrt{x^2/x^2+7x/x^2}+1}$$ $$=-\frac{7}{\sqrt{1+0}+1}$$ $$=\frac{-7}{2}$$ However, I multiplied by $\frac{\frac{1}{x}}{\frac{1}{x}}$ and got the following: $$\lim_{x \to -∞}\frac{7}{\sqrt{x^2/x^2+7x/x^2}-1}$$ $$\frac{7}{\sqrt{1+0}-1}$$ $$\frac{7}{0}$$ Which is undefined. Why does multiplying by what is essentially $1$ cause different answers in general, and in particular for evaluating limits?","['radicals', 'limits']"
1873069,Baby Rudin - Theorem 1.35 Cauchy Schwartz,"I'm stumped on the following difficulty while reading baby Rudin (p.15). Let $A=\sum|a_j|^2, B=\sum|b_j|^2, C=\sum a_j \overline{b}_j$: $$\begin{align}
\sum|Ba_j-Cb_j|^2 &= \sum(Ba_j-Cb_j)(B\overline{a}_j-\overline{Cb_j}) \\
&= B^2\sum|a_j|^2{\bf -B\overline{C}\sum a_j\overline{b}_j-BC\sum \overline{a}_j b_j}+|C|^2\sum|b_j|^2 \\
&=B^2A-B|C|^2=B(AB-|C|^2)
\end{align}$$ Note that the second line of the equation, the book indicates that the middle 2 term together to be zero. Why is that?","['real-analysis', 'inequality', 'analysis']"
1873070,Can the limit of a sequence of bounded functions be unbounded?,"It is assumed that the sequence of functions converges and that each function is bounded, then can the limiting function be unbounded?",['real-analysis']
1873084,How to tell if you have specified sufficient initial data for a differential equation?,"I recently learnt that the following 'wave equation' is not well-posed $$
\begin{cases}
\partial_{tt}u=\partial_{xx} u, & (0,1)\times\mathbb R\\
u(t,0)=u(t,1)=0,&t\in [0,1]
\end{cases}
$$ since the solution will not be unique. I was told that in this case it is sufficient that one specifies $\partial_t u(0,0)$ and $\partial_t u(0,1)$ in order to have a unique solution and I can understand the proof. But, could someone please explain to me what's going on here morally? Why is it that specifying only $u(\cdot,0)$ and $u(\cdot,1)$ is not enough, but further specifying the derivatives then is enough? I'd love an explanation that allows me to have a feeling for more general equations as to what would constitute sufficient initial data. If that's too vague, consider the concrete example: is the following heat-type equation well-posed 
$$
\begin{cases}
\partial_{t}u+\triangle^2u=0, & (0,\infty)\times\mathbb R^d\\
u(0,x)=f(x)\in C^\infty_c(\mathbb R^d\to\mathbb R)
\end{cases}
$$ provided we seek only solutions with sub-exponential growth (as with the usual heat equation) or do I need to specify more information about the derivatives of the solution at $t=0$ or something else entirely? And, how could you tell either way?","['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
1873116,General structure of the proof that every compact metric space is the continuous image of the Cantor set,I am reading the book General Topology by Stephen Willard and I have seen the theorem Every compact metric space is a continuous image of the Cantor set. The book also presents its proof. The proof is very lengthy and I am struggling to understand it. Could someone provide an outline of the general structure of the proof?,"['real-analysis', 'compactness', 'general-topology', 'metric-spaces', 'cantor-set']"
1873154,Show that $f$ is identically zero in $\mathbb{C}$,"Let $f$ be holomorphic in $\mathbb{C}$. Prove that if $|f(z)| \leq
 M|z|^{\alpha}$ with $0 <\alpha <1$, then $f$ is identically zero
  in$\mathbb{C}$ I know that $f$ has a Taylor expansion $f(z)=\sum_{n=0}^{\infty}c_nz^n$. And that if there exist positive constants $M$ and $K$ and a positive integer $k$ such that for $|z|\geq K$ $$|f(z)|\leq M|z|^k$$
then $f$ is a polynomial of degre at most $k$. So if I could modify somehow this result and conclude that $f$ is a polynomial of degree at most $\alpha$, then $f$ would be identically zero in $\mathbb{C}$, but I'm not able to do that (it might even be false), so I'd like some hint that points me towards a good path to solve it. Thanks in advance! EDIT: As indicated in the comments, if $|z|\geq 1$, then $|z^{\alpha}| < |z|$. So let's consider $R>0$, $R>r\geq1$ and the circle $\gamma(0,r)$. Then the constants $c_n$ of the Taylor expansion are given by $$c_n=\frac{1}{2\pi i}\int_{\gamma}\frac{f(z)}{z^{n+1}} dz$$
and applying the Estimation Theorem gives $$|c_n|\leq \frac{1}{r^n}M(r)$$
where $M(r)= \sup \{|f(z)|:|z|=r \}$. Then, since $|z|^{\alpha}<|z|=r$ $$M(r) < Mr$$
we have $$|c_n|\leq \frac{M}{r^{n-1}}$$
And since $r>1$ can be made arbitrarly large, we must have $c_n=0$ for all $n>1$,so $f$ is constant. Since $f(0)=0$ from the hypothesis, $f$ is identically zero in each disc $D(0;R)$, so $f$ is identically zero in $\mathbb{C}$. Is this correct?","['complex-analysis', 'taylor-expansion']"
1873195,Prove using induction the following equation is true.,If $$(1-x^2)\frac{dy}{dx} - xy - 1 = 0$$ Using induction prove the following for any positive integer n$$(1-x^2)\frac{d^{n+2}y}{dx^{n+2}} - (2n+3)x\frac{d^{n+1}y}{dx^{n+1}} - (n+1)^2\frac{d^ny}{dx^n} = 0$$ I know Leibtniz can be used to solve it easier but I need the proof to use induction.,"['derivatives', 'calculus', 'induction', 'ordinary-differential-equations', 'implicit-differentiation']"
1873201,Calabi-Yau $3$-fold given as elliptically fibered manifold over $\mathbb{C}P^1 \times \mathbb{C}P^1$,"Consider a Calabi-Yau three-fold given as an elliptically fibered manifold over $\mathbb{C}P^1 \times \mathbb{C}P^1$$$y^2 = x^3 + f(z_1, z_2)x + g(z_1, z_2),$$where $z_1$, $z_2$ represent the two $\mathbb{C}P^1$s and $f$, $g$ are polynomials in $f$ in $(z_1, z_2)$. What is the degree of the polynomials $f$ and $g$? What is the number of independent complex structure deformations of this Calabi-Yau, and what is the Hodge number $h^{2, 1}$? How many Kähler deformations are there, and what does this imply for $h^{1, 1}$?","['manifolds', 'hodge-theory', 'complex-geometry', 'algebraic-geometry']"
1873228,What is the geometrical meaning of the integral of a vector valued function?,"If $f:\mathbb{R}\rightarrow\mathbb{R}$ is an integrable function. then $\int_a^b f(x)dx$ can be considered as the area between the graph and the x-axis. But what if $f:\mathbb{R}^n\rightarrow \mathbb{R}^m$? Let $\gamma:[0,1]\rightarrow \mathbb{R}^n$ be a smooth curve. what is the geometrical meaning of $\int_\gamma f\cdot dl$? (or in case $n=m$, $= \int_0^1 f(\gamma(t))\gamma '(t)\cdot dt$?) Thanks :) (for simplicity, you can assume $m,n$ are small numbers... i.e $f:\mathbb{R}^2\rightarrow \mathbb{R}^2$ )","['calculus', 'integration', 'contour-integration', 'differential-geometry', 'vector-spaces']"
1873241,A question about automorphism groups of finite groups,"I’ve encountered the following question whilst helping a colleague study for comprehensive exams, and I’m stuck on it: Let $G$ be a finite group such that the natural action of the automorphism group $\mathrm{Aut}(G)$ on the set $G\setminus\{e\}$ of nonidentity elements of $G$ is transitive.  Show that $G$ is an elementary Abelian $p$-group for some prime number $p$. My Thoughts to Date:  Transitivity of the action of $\mathrm{Aut}(G)$ on $G\setminus\{e\}$ means that, given any two nonidentity elements $g,h\in G$, there is an automorphism $\phi\in\mathrm{Aut}(G)$ such that $\phi(g)=h$.  Since automorphisms preserve orders of elements, this implies that all nonidentity elements of $G$ have the same order, say $n$.  By Lagrange’s theorem, $n\mid|G|$, and hence every divisor $d$ of $n$ also divides $|G|$.  If $n$ had two distinct prime divisors $p$ and $q$, then $G$ would contain elements of order $p$ and $q$ by Cauchy’s theorem $\ (\Rightarrow\Leftarrow)$.  Therefore $\exists!$ prime $p$ such that $p\mid n$.  If $n=p^\ell$ for some $\ell>1$, then $G$ would contain elements of order $p^m$ for each $1\leq m\leq\ell\quad(\Rightarrow\Leftarrow)$.  Therefore $n=p$, whence $G$ is a $p$-group.  I could use some suggestions on the “elementary Abelian” part.  All would be appreciated.","['finite-groups', 'abstract-algebra', 'group-theory']"
1873259,Partial derivatives confusion in the equation $PV=nrT$,"The question is to exactly: ""If the variables $P,V$ and $T$ are related by the equation $PV=nRT$ where $n$ and $R$ are constant, simplify the expression""
$$
\frac{\partial V}{\partial T}\frac{\partial T}{\partial P}\frac{\partial P}{\partial V}
$$
The first step of the solution confuses me. The solution solves explicitly for $V$ and solves
$$
V=\frac{nrT}{P}\Rightarrow \frac{\partial V}{\partial T}=\frac{nr}{P}
$$
Treating $P$ as constant with respect to $T$, which it isn't, since 
$$
P=\frac{nrT}{V}
$$
Where is my reasoning flawed?","['multivariable-calculus', 'partial-derivative', 'calculus']"
1873286,Prove $[\sin x]' = \cos x$ without using $\lim\limits_{x\to 0}\frac{\sin x}{x} = 1$,"I came across this question: How to prove that $\lim\limits_{x\to0}\frac{\sin x}x=1$? From the comments, Joren said: L'Hopital Rule is easiest: $\displaystyle\lim_{x\to 0}\sin x = 0$ and $\displaystyle\lim_{x\to 0} = 0$, so $\displaystyle\lim_{x\to 0}\frac{\sin x}{x} = \lim_{x\to 0}\frac{\cos x}{1} = 1$. Which Ilya readly answered: I'm extremely curious how will you prove then that $[\sin x]' = \cos x$ My question :  is there a way of proving that $[\sin x]' = \cos x$ without using the limit $\displaystyle\lim_{x\to 0}\frac{\sin x}{x} = 1$. Also, without using anything else $E$ such that, the proof of $E$ uses the limit or $[\sin x]' = \cos x$. All I want is to be able to use L'Hopital in $\displaystyle\lim_{x\to 0}\frac{\sin x}{x}$. And for this, $[\sin x]'$ has to be evaluated first. Alright... the definition that some requested. Def of sine and cosine : Have a unit circumference in the center of cartesian coordinates. Take a dot that belongs to the circumference. Your dot is $(x, y)$. It relates to the angle this way: $(\cos\theta, \sin\theta)$, such that if $\theta = 0$ then your dot is $(1, 0)$. Basically, its a geometrical one. Feel free to use trigonometric identities as you want. They are all provable from geometry.","['derivatives', 'alternative-proof', 'limits']"
1873290,Number of multisets with restrictions on specific element count,"I am looking to find the number of multisets with restrictions on the number of specific elements. This isn't for homework, it is a work related problem. My set of items is {A, a, B, b}. I want to get the number of multisets with at least 3 B/b, where there is at least one of each B/b. For multisets order does not matter, and duplicates are allowed. To get the total number of multisets with cardinality $k$, from my set of cardinality n = 4, I have been using, $$\frac{(n + k - 1)!}{(k!(n-1)!}$$However I want to count the number of multisets with at least 3 B or b, and a minimum of 1 each. For $k = 3$ there are two solutions, $\{B, B, b\}$ and $\{B, b, b\}$. These two sets represent the minimum requirements to pass. Then for any $k$, I was thinking the number of multisets to be $$2 * (\frac{(4 + (k-3) - 1)!}{((k-3)!(n-1)!})$$ My reasoning was that the possible valid sets are $\{B, B, b, x_3, x_4, ... x_k\}$ and $\{B, b, b, x_3, x_4, ... x_k\}$,  where $\{x_3, x_4, ..., x_k\}$ is any multiset from {A, a, B, b} with cardinality $k - 3$. The problem I ran into is that this method counts some sets multiple times. For example, if k = 4, the valid sets are $\{B, B, b, x_3\}$ and $\{B, b, b, y_3\}$, where $x_3$ and $y_3$ are any item in $\{A, a, B, b\}$. For $x_3 = b$ and $y_3 = B$, I am counting the set $\{B, B, b, b\}$ twice. I tried predicting the number of duplicate counts and subtracting that, but could not find an answer. If I assign each element in my set $\{x_0 = A, x_1 = a, x_2 = B, x_3 = b\}$, another way to phrase the problem is to find the number of solutions to, $$ x_0 + x_1 + x_2 + x_3 = k $$
where, 
$$k >= x_0, x_1, x_2, x_3 >= 0$$ $$k >= x_2 + x_3 >= 3$$ $$x_2 >= 1$$ $$x_3 >= 1 $$ EDIT Thanks for the help guys. I have changed the requirement of 3 'B/b' elements to $minB$ 'B/b' elements. So for a multiset of size D with $minB$ required 'B\b' elements, $\{B, b, x_3, x_4, ... x_D\}$ is a valid multiset if $\{x_3, x_4, ... x_D\}$ has minB-2 'B/b' elements. Based off of these comments I came up with, $$ numValid(D, minB) = \sum_{nb = minB - 2}^{D-2} [(nb + 1) * (D - nb - 1)] $$ The first term inside the sum $(nb + 1)$ is the number of multisets of size $minB - 2$ chosen from $\{B, b\}$. The second term is the number of multisets of size $D - 2 - nb$ chosen from $\{A, a\}$. Then summing $nb$ from $ minB - 2$ to $D - 2$, we get the number of multisets of the form $\{B, b, x_3, x_4, ... x_D\}$, where $\{x_3, x_4, ... x_D\}$ has at least $(minB - 2)$ 'B/b' elements. Is this correct?
Thanks again. :)","['combinations', 'multisets', 'elementary-set-theory', 'combinatorics', 'linear-algebra']"
1873292,Intuitive understanding of the matrix of a linear transformation,Is it accurate to say that a matrix $M(T)$ of the linear map $T:V\to W$ encodes the linear map into a series of numbers by showing how the linear map applied to the basis vectors of $V$ can be expressed as basis vectors of $W$? Is this a healthy way to imagine and intuit the matrix of a linear map?,"['matrices', 'linear-algebra', 'linear-transformations']"
1873306,Sum identity using Stirling numbers of the second kind,"Experimenting with series representations of $e^{x e^x}$ I came across the two seemingly different power series
$$e^{x e^x} = \sum_{n=0}^{\infty} x^n \sum_{k=0}^{n} \frac{(n-k)^k}{(n-k)! \cdot k!}$$
and
$$e^{x e^x} = \sum_{n=0}^{\infty} x^n \sum_{k=0}^{n} \frac{1}{(n-k)!} \sum_{i=0}^{k} \frac{1}{(k-i)!} {k-i \brace i}\,.$$
They should be and are in fact identical. By equating the coefficients it is obvious that
$$\sum_{k=0}^{n} \frac{(n-k)^k}{(n-k)! \cdot k!} = \sum_{k=0}^{n} \frac{1}{(n-k)!} \sum_{i=0}^{k} \frac{1}{(k-i)!} {k-i \brace i}\,.$$
Going through some values of $n$ reassures the correctness of the equation, however I have no idea how one could show this algebraically. An equivalent equation would be
$$\sum_{k=0}^{n} {n \choose k} (n-k)^k = \sum_{k=0}^{n} {n \choose k} \sum_{i=0}^{k} {k \choose i} \sum_{j=1}^{i} (-1)^{i-j} {i \choose j} j^{k-i}\,,$$
which is obtained through multiplication of both sides by $n!$ and an explicit formula for the Stirling numbers. No matter what I try, I seem to run into a brick wall. How can I (algebraically) show that the equation is correct for arbitrary $n$?","['stirling-numbers', 'combinatorics', 'summation', 'power-series']"
1873321,Suggestions for Constructing a Random Variables from Correlated Observations,"Let $\mathcal{X} \neq \phi $ be a finite set. Let $P_{XY_1Y_2}$ be a fixed  joint distribution over $\mathcal{X}\times\mathcal{X}\times\mathcal{X}\ $ and that a random sample $(X,Y_1,Y_2 )$ is drawn using $P_{XY_1Y_2}$.
Suppose there are two parties, $A$ and $B$ such that $A$ gets to observes $Y_1$ and $B$ gets to observes $Y_2$. Additionally $A$ knows the conditional distribution $P_{Y_1|X}$ and similarly $B$ knows the distribution $P_{Y_2|X}$. I'm interested in finding a method which makes party $A$ construct $\hat{X}_1$ using the information $P_{Y_1|X}$ and $Y_1$ and similarly $B$ construct $\hat{X}_2$ using the information $P_{Y_2|X}$ and $Y_2$ such that $P(\hat{X}_1 \neq \hat{X}_2)$ is 'small' and $P(\hat{X}_1\neq X)$ is 'small'. In other words I am interested in constructing a random variable based on correlated observations. I do agree that the problem is not well posed. I look for only informal suggestions. Any relevant literature would also be really helpful. Thanks in advance for your help.","['stochastic-processes', 'statistics', 'probability', 'statistical-inference']"
1873347,Show that entire function $f:\mathbb{C}\to \mathbb{C}$ so that $|f(\cos z)|\leq A |z|^n$ is constant [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $f:\mathbb{C}\to \mathbb{C}$ a holomorphic function so that $|f(\cos z)|\leq A |z|^n$ for all $z$ with $|z|>1$ and positive numbers $A$ and $n$. Show that $f$ is constant. I want to apply Liouville's theorem on $f$. So we have to show that $f$ is bounded. But from that inequality how we can say that $f$ is bounded ?","['complex-analysis', 'holomorphic-functions']"
1873364,Geometry Of Unitary Transformations,"Ever since I first took Linear Algebra, I have over time realized how concepts like determinants, eigenvalues, diagonalization, orthogonal transformations and so on have very intuitive geometric interpretations, and so begun to fill in many gaps in my understanding of the subject (the course I took, just like most freshman-year courses I've taken, focused too much on pure computation and not enough on intuition / visualization, as do most course books on linear algebra that I've read). Now, determinants are volumes (technically, multiplicative volume changes), diagonalization is just re-expressing the action of a matrix in terms of scalings of an eigenbase, and orthogonal transformations are norm- (and inner product-) preserving, and therefore fully characterized by compositions of rotations and reflections (and I still don't fully understand why the books start with $A^{-1}=A^T$ as the first definition of an orthogonal matrix…)… …but what about Unitary transformations / matrices ? They are also norm- (and inner product-) preserving, but may take complex-valued entries. How can I interpret this geometrically? For example, I know that the group SU(2) can be visualized as the 3-sphere (they are apparently diffeomorphic), but I cannot see the picture clearly in my head.","['soft-question', 'linear-algebra', 'complex-numbers', 'geometry']"
1873371,Is $2^{16} = 65536$ the only power of $2$ that has no digit which is a power of $2$ in base-$10$?,"I was watching this video on YouTube where it is told (at 6:26) that  $2^{16} = 65536$ has no powers of $2$ in it when represented in base-$10$. Then he - I think as a joke - says ""Go on, find another power of $2$ that doesn't have a power of $2$ digit within it. I dare you!"" So I did. :) I wrote this little Python program to check for this kind of numbers: toThePower = 0
possiblyNoPower = True

while True:
    number = str(2**toThePower)
    for digit in number:
        if int(digit) in [1,2,4,8]:
            possiblyNoPower = False
            print('Not ' + number)
            break
    if possiblyNoPower:
        print(number + ' has no digit that is a power of 2.')
    toThePower += 1
    possiblyNoPower = True Sidenote: I could use the programming language Julia instead of Python, which may be much quicker, but I already checked for really big numbers and such a program (and brute-force in general) will never proof that there are no other powers of $2$ having this property. It might disprove it, but I think the chance is really really small. I checked all the way to $2^{23826}$, which is a 7173 digit number, but no luck. Since the numbers are getting more and more digits with bigger powers of $2$, the chance of a number having no digit that is a power of $2$ becomes smaller and smaller. I made a plot of $\frac{\text{number of digits being a power of 2}}{\text{total number of digits}}$ versus the $n$th power of $2$ on a logarithmic scale. This graph is wrong! See the edit. As I predicted, the graph drops really fast to almost $0$. I think that $\frac{\text{number of digits being a power of 2}}{\text{total number of digits}}\rightarrow 0$ as $n \rightarrow \infty$ and thus $\text{P}(n\text{th power of 2 having no digits of 2 in it}) \rightarrow 0$, but this is just my intuition. So my question: Is $2^{16} = 65536$ the only power of $2$ that has no digit in it that is a power of $2$ (so no digit $\in \{1,2,4,8\}$) when represented in base-$10$? Is there a proof, a counterexample, or is it an open question? I'm also curious about powers of $2$ having no digits that are a power of $2$ in other bases than $10$. Edit: As @Aweygan noted, the graph above is wrong. I accidentally divided by the number itself, instead of the amount of digits the number has. Below a good version, on a linear scale. From this graph it appears that $\frac{\text{number of digits being a power of 2}}{\text{total number of digits}} \rightarrow 0.4 $ as $n \rightarrow \infty$. This seems to make sense, since $\text{P}(\text{digit} \in {1,2,4,8}) = 4/10 = 0.4$ and since the number of digits becomes larger and larger, the law of large numbers becomes ""visible"". About the possible duplicate: that question was posted 4.5 years ago. The status of the problem (proven, disproven, open question) might well be changed in the maintime. :)","['number-theory', 'decimal-expansion', 'recreational-mathematics', 'exponentiation']"
1873385,"How do I use the fact that $\langle \gamma, \lambda \rangle > 0$?","If I post all the details of my question up to the point where I am stuck, no one is going to want to read all that.  So I hope there is someone reading this question who is familiar with Weyl chambers and systems of positive roots will immediately know the answer.  The book I'm using is Humphreys, Linear Algebraic Groups . $G$ is a connected linear algebraic group with maximal torus $T$.  A regular cocharacter of $T$ is a cocharacter whose image is contained in exactly the same Borel subgroups as those which contain $T$.  If $I(T)$ is the identity component of the intersection of all Borel subgroups containing $T$, then $\mathscr L(I(T))$ is $\textrm{Ad } T$ stable, so there is an $\textrm{Ad } T$-stable complement $$\bigoplus\limits_{\alpha} \mathfrak g_{\alpha}'$$ in $\mathfrak g$ for various nontrivial characters $\alpha$.  The set of such characters is denoted $\Psi$. This is page 157 of Humphreys' book.  Why is it the case that $\langle \alpha, \lambda \rangle > 0$ if and only if $B(\lambda) \cap Z_{\alpha} = B_{\alpha}$?  This is true by definition when $\lambda = \lambda_0$, but even with the Weyl group equivariance ($w.B(\lambda) = B(w.\lambda)$) and whatnot I don't understand why his claim suddenly follows.  I understand everything in the argument up to that point.","['algebraic-geometry', 'root-systems', 'algebraic-groups', 'lie-algebras', 'lie-groups']"
1873391,Is there a better way to find the closest point on a line?,"I'm given a question that asks: ""Find the point on $L(x) = 4x-3$ that is closest to the point $(1,3)$."" My best guess was to find the derivative of the distances and set it equal to zero and solve to attempt to find a minimum. I come up with the derivative being$$f'(x)=\frac{1}{2}(17x^2-50x+37)^{-\frac{1}{2}}(34x-50)$$ 
And solving for x I end up with $50/34$ or about 1.4705. Now all I have to do is just plug that into the original linear equation. And when I graphed it out in desmos that appears to solve the problem correctly. My only issue is that my solution doesn't account for if there was a maximum instead of a minimum on the distance equation. Is there a more correct solution to this problem?","['derivatives', 'optimization', 'calculus']"
1873393,Sketch the set of points determined by the following condition,"$|2 \overline{z}+i|=4$ if I let $z=x+yi$, I got $4x^2+(1-2y)^2=16$ I don't know if that's right. Should I just plot that in wolfram alpha?","['algebra-precalculus', 'complex-analysis']"
1873444,"If $T$ and $T^2$ have equal rank then $V=\ker T\oplus {\rm im}\, T$ for $V$ finite dimensional.","I am trying to prove the following: Let $V$ be a finite-dimensional vector space. Consider an operator $T$ on $V$ such that $\text{dim range}(T)=\text{dim range}(T^2)$. Show that $V=\text{null}(T)\oplus \text{range}(T)$. I have been given the following hint: $\textit{Hint:}$ Show that there does not exist a $y\in V$ such that $y\notin \text{null}(T)$ and $y\notin \text{range}(T)$. I decided to try and prove the hint by contradiction... Suppose that there exists a $y\in V$ such that $y\notin \text{null}(T)$ and $y\notin \text{range}(T)$. Then 
$$Ty\neq 0$$
and for every $v\in V$ we have $$Tv\neq y.$$ I have NO idea what to do with this information! I would greatly appreciate some hints/help! Thank you.","['direct-sum', 'linear-algebra', 'linear-transformations']"
1873460,Is there a relationship between the pullback in differential geometry and that in category theory?,"1. Is there a relationship between the pullback in differential geometry and the pullback in category theory? [ 2. Is there a relationship between the pushforward/pushout in differential geometry and the pushforward/pushout in category theory? Although the answer to the above ( 1. ) is equivalent to the answer to this question ( 2. ) by duality.] As far as I can tell, (for the terms in differential geometry) the pullback is a contravariant functor, and the pushforward is a covariant functor. 3. Is there a way to turn every contravariant functor into a category theory pullback or vice versa? (Or every covariant functor into a category theory pushforward or vice versa?) If the answer to 3. is no, then it seems like the fact that the two concepts have the same name is just a historical accident, and does not indicate that one is meant to generalize the other. 4. Is it true that the similar terminology between the two fields is a historical accident? Or are they both meant to evoke the same type of basic example?","['category-theory', 'differential-geometry', 'terminology']"
1873463,How to solve $y'' + y = -2\sin(x)$?,I don't know how to find the particular solution of  $$y'' +  y = -2\sin(x)$$ I started with $$y'' +  y = 0$$ to find the homogeneous form $$A\cos(x)+B\sin(x)$$ But now i am stuck.,['ordinary-differential-equations']
1873479,Proving an expression is an integer,"Prove that $$A^2 \cdot \dfrac{\sum_{m=1}^{n}(a^{3m}-b^{3m})}{\sum_{m=1}^n(a^m-b^m)}-3A^2$$ is an integer if $a=\frac{k+\sqrt{k^2-4}}{2}, b=\frac{k-\sqrt{k^2-4}}{2}$, where $k>2$ is a positive integer, and $A =  \dfrac{1}{\sqrt{k^2-4}}$ for all positive integers $n$. I thought about expanding the numerator and denominator. For the denominator we get $\sum_{m=1}^n(a^m-b^m) = \left(a \cdot \dfrac{a^n-1}{a-1}-b \cdot \dfrac{b^n-1}{b-1}\right)$ and we can similarly do the same thing with the numerator. How do we use all this to solve the question?",['number-theory']
1873482,Are there non-standard counterexamples to the Fermat Last Theorem?,"This is another way to ask if Wiles's proof can be converted into a ""purely number-theoretic"" one. If there is no proof in Peano Arithmetic then there should be non-standard integers that satisfy the Fermat equation. I vaguely remember that most proofs in analytic number theory are known to be convertible into elementary ones, probably because some version of predicative analysis is a conservative extension of arithmetic. But Wiles's proof uses not as much analysis as high end algebraic geometry, so I am not sure. Is it convertible into elementary arithmetic, is that even known?","['nonstandard-analysis', 'model-theory', 'number-theory', 'reverse-math', 'logic']"
1873484,Show that the set of algebraic numbers is countable(Proof verification),"I just want to make sure that my proof is good. There is a bijection between $\mathbb{Q}^n$ into $X^n$ given by $\phi : \mathbb{Q}^n \rightarrow X^n = \{ x^n + a_{n - 1}x^{n - 1} + ... + a_0 : a_i \in \mathbb{Q} \}$, where $(a_{n - 1},...,a_0) \mapsto x^n + a_{n - 1}x^{n - 1} +\ ...\ + a_0$. It is clear that this is a bijection. The set of all polynomials of degree n with coeffient 1 on the highest degree term is countable. Since it is in bijection with $\mathbb{Q}^n$, which is countable. Since each $X^n$ is countable countable unions of it is countable hence there is a bijection $\theta : \bigcup X^n \rightarrow \mathbb{Z}_+$. Let A_n be the algebraic numbers of all polynomials of degree n. We wish to show that $A_n$ is countable. Since $X^n$ is countable there exists a bijection between $\mathbb{Q}^n$ and $X^n$. Therefore there exists a bijection $\eta : X^n \rightarrow \mathbb{Z}_+$, which means $A_n = \bigcup_{i \in \mathbb{Z}_+} A_{\eta(i)\ n}$, where $A_{\eta(i)\ n}$ is the solutions of the ith polynomial. Since this is union of countable sets A_n is countable, which means that $A^{\infty} = \bigcup_{i \in \mathbb{Z}_+} A_i$ is countable as it is countable unions of countable sets. That means that algebraic numbers are countable.","['elementary-set-theory', 'proof-verification']"
1873521,On a constant associated to equilateral triangle and its generalization.,"I guess many of you are familiar with the result described as follows: If ABC is an equilateral triangle, and P is any point on the incircle of ΔABC, then AP² + BP² + CP² is constant. See link below. http://www.cut-the-knot.org/pythagoras/EquiIn3D.shtml I was wondering whether this can be true for any regular polygon and found it to be true too. In the website it is mentioned that the result holds for any circle with center at the centroid of the triangle, but it does not mention whether the result holds for any other polygon. I will propose the following generalization: Consider any regular polygon (n-gon) and a point $P$ on its circumcircle (or on any other circle with center at the centroid of the n-gon). Denote the vertices of the n-gon as $A_i$. Then, it follows $$K_n=\sum_{1}^{n}{A_iP^2}$$ Where $K_n$ is a constant. On the other hand, in the case of a triangle, it admits a 3D-generalization(?) as you can see in this link (although they do not mention it in the link) http://www.cut-the-knot.org/pythagoras/3DExercise.shtml I have noticed that this is also true for the insphere of the tetrahedron, so I suspect that for any sphere centred at the centroid the result holds, just as the 2D case. I checking whether the result holds for a cube, but failed. Although it is true just for some segments of the cube since we can inscribed a tetrahedron in a cube. Does anyone know whether this generalization is indeed new? Thanks in advance. EDITED 1: I have removed the golden ratio part for being irrelevant. EDITED 2: Here is what I think is a more general result: http://geometriadominicana.blogspot.com/2016/07/sum-of-squares-of-distances-to-vertices.html",['geometry']
1873522,Product-like metric on a pseudo-Riemmanian manifold foliated by Lie group orbits,"Suppose we have an $n$-dimensional pseudo-Riemmanian manifold $(M,g)$ on which a connected Lie group $G$ acts isometrically (I am most interested in the Lorentzian case if it matters). Suppose that the orbits of $G$ foliate $M$. Are there general conditions such that we can choose local flat coordinates $(x^a,y^i)$, with $x^a$ ($a,b=1,2,\cdots,k$) parameterizing the leaves of the foliations, and with the leaves of the foliation determined by $y^i = c^i$ for constants $c^i$ ($i,j=k+1,\cdots,n$), such that the metric is locally a warped product of the form
$$ds^2 = g_{ij}(y)dy^i\,dy^j + f(y)\tilde{g}_{ab}(x)dx^a\,dx^b?$$ Some motivation for the question. In general relativity, metrics of such types are often studied. For example, the Schwarzchild metric
$$ds^2 = -\left(1-\frac{2M}{r}\right)dt^2 + \left(1-\frac{2M}{r}\right)^{-1} + r^2\left(d\theta^2 + \sin^2\theta\ d\phi^2\right),$$
among others, is of the above type. In general, one can show that a (spatially) spherically symmetric (Lorentzian) metric takes the form
$$ds^2 = -A(r,t) dt^2 + B(r,t) dr^2 + C(r,t)dr\,dt + D(r,t)\left(d\theta^2 + \sin^2\theta\ d\phi^2\right).$$
I would be very happy to see a rigorous derivation of the above fact. All of the arguments I've seen so far are by physicists and are largely heuristic. Why can we choose coordinates $(t,r,\theta,\phi)$ such that the metric takes the above form? I am also interested in generalizations of the above fact, which is the special case of the metric in a manifold foliated by (space-like) $SO(3)$ orbits. Can it be generalized to other Lie groups? If so, how? And under what conditions?","['riemannian-geometry', 'differential-geometry', 'lie-groups']"
1873552,Part (a) of Exercise 3.4 of Eisenbud's Commutative Algebra,"In the part (a) of Exercise 3.4, it suggests that we may use the relation:
  $${\text{Content}(fg)}\subset{\text{Content}(f)\text{Content}(g)}\subset{\text{rad}\left(\text{Content}(fg)\right)}$$
  to deduce that if $\text{Content}(f)$ contains a nonzerodivisor of $R$, then $f$ is nonzerodivisor of $S=R[x_1,. . . ,x_r]$. But I don't know how to do this, for example, if all the coefficients of g are in the nilpotent radic of R, assuming that fg=0, the relation above gives nothing. Is there some tips please?","['abstract-algebra', 'polynomials', 'commutative-algebra']"
1873559,Which non-negative matrices have negative eigenvalues?,"It's easy to proof by counterexample that non-negative matrices can have negative eigenvalues. For example, the following matrix has -1 as an eigenvalue: $$
A =
 \begin{bmatrix}
  0 & 0 & 0 & 1 \\
  1 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 \\
  0 & 0 & 1 & 0 \\
 \end{bmatrix}
$$ However, which are the properties of those matrices, is there a generalization of them? Thanks!","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
1873568,Conformality of two concentric annuli,"I'm trying to prove that two annuli, $A_1:=\{z:r_1<\left| z \right| < R_1  \}$ and $A_2:=\{z:r_2<\left| z \right| < R_2  \}$ are conformally equivalent if and only if $\frac{R_1}{r_1}=\frac{R_2}{r_2}$. I'd need to prove this using relatively simply concepts, i.e. the proof should probably be quite short. I think that in this case some other theorems, like Riemann's mapping theorem, should be used, but I don't know how Riemann's mapping theorem might apply in this case. One possibility of such a proof could be via the proof that if $A:=\{z:1<\left| z \right| < R_1  \}$ and $A':=\{z:1<\left| z \right| < R_2  \}$ if and only if $R_1=R_2$, by deducing that if we divide the set $A_1$ by $r_1$, and set $A_2$ by $r_2$, then $\frac{R_1}{r_1}=\frac{R_2}{r_2}$ (or is this too lame?). Unfortunately, the proof I could find, of this theorem, for the case of $A$ and $A'$, uses concepts which I don't yet know. I'd appreciate some help with this problem.","['complex-analysis', 'conformal-geometry', 'teichmueller-theory']"
1873571,Quadratic Inequality in terms of variable $x$,"Find the values of $a$ for which the inequality $x^2+ax+a^2+6a<0\;\forall x \in (1,2)$ $\bf{My\; Try::}$ We can Write Equation as $$x^2+ax+\frac{a^2}{4}+\frac{3a^2}{4}+6a<0$$ So $$\left(x+\frac{a}{2}\right)^2+\frac{3a^2+24a}{4}<0$$ Now how can i solve after that, Help required, Thanks",['algebra-precalculus']
1873578,Flipping coins- percentages of heads vs tails,If I flip a coin multiple times and count the number of time it fell on heads and the number of times it fell on tails and keep a track of them. In how many flips on average will the delta between percentage of heads and percentage of tails will be less than 0.1%?,"['statistics', 'probability']"
1873596,Why is the complex domain of cosine naturally a sphere?,"Near the end of this MAA piece about elliptic curves , the author explains why the complex domain of the cosine function is a sphere: since it's periodic, its domain can be taken as a cylinder, wrapping up the real axis. And because cosine of $\theta\pm i\infty$ is $\infty$, the two ends of the cylinder can be identified with a single point $\infty$. Ok, great, but this sounds to me like a pinched torus. Can I have a clearer explanation why this is a sphere?","['riemann-sphere', 'complex-analysis', 'general-topology', 'complex-numbers']"
1873602,"For which $a$, equation $4^x-a2^x-a+3=0$ has at least one solution.","Find all values of $a$ for which the equation $4^x-a2^x-a+3=0$ has at least one solution. $\bf{My\; Try::}$ We can write it as $$2^{x}-a-\frac{a}{2^x}+\frac{3}{2^x}=0$$ So $\displaystyle \left(2^x+\frac{3}{2^x}\right)=a\left(1+\frac{1}{2^x}\right).$ Now for the existance of solution $\displaystyle 2^x+\frac{3}{2^x}\geq 2\sqrt{3}$ Using $\bf{A.M\geq G.M}$ So $\displaystyle a\left(1+\frac{1}{2^x}\right)\geq 2\sqrt{3}\Rightarrow a\geq \sqrt{3}\cdot \frac{2^x}{2^x+1}\geq \sqrt{3}$ But answer given as $a\geq 2,$ please explain me whats wrong with that, Thanks",['algebra-precalculus']
1873625,Check if an ellipse is within another ellipse,"I have an ellipse $E_1$ centered at $(h,k)$, with semi-major axis $r_x$, semi-minor axis $r_y$, both aligned with the Cartesian plane. How do I determine if another ellipse $E_2$ is within this given ellipse $E_1$? $E_2$ can be anywhere in the Cartesian plane. What is given, is the centerpoint at $(i,j)$, the semi-major axis $r_x$ and the semi-minor axis $r_y$ and a rotation angle $\alpha$ (can be $0$, so no rotation). I need this for a computed algorithm. 
Given this computer science background, what i use right now is the formula from here , and choose a point on the ellipse $E_2$, check if its within $E_1$ and choose another point, $1°$ further and check that point again and so on, until i complete $360°$. I was thinking, that there has to be a better solution, a more formal and complete one (i think it should be possible to get a wrong result with the current algorithm in some very special cases). 
Yet, i haven't found a better solution.","['conic-sections', 'euclidean-geometry', 'geometry']"
1873689,Behaviour of a clamped Bspline curve at t=1.0,"A bspline curve of order $k$ is given by
$$C(t) = \sum_{i=0}^n P_i N_{i,k}(t).$$
where $P_i$ are the control points and $N_{i,k}(t)$ a basis function defined on a knot vector
$$T = (t_0,t_1,...t_{n+k}).$$
with
$$N_{i,1}(t) =  \begin{cases}1 & t_i \le t \lt t_{i+1}  \\ 0 & \text{otherwise} \end{cases}$$
$$N_{i,k}(t) = \frac{t-t_i}{t_{i+k-1}-t_i}N_{i,k-1}(t)+\frac{t_{i+k}-t}{t_{i+k}-t_{i+1}}N_{i+1,k-1}(t).$$ A clamped bspline curve has the additional property that the first and last knot in $T$ are of multiplicity $k$, e.g. $T=(0,0,0,0,0.5, 1,1,1,1)$ for a cubic spline. (Formulas are based on Shape Interrogation for Computer Aided Design and Manufacturing ) What I don't understand now is why the curve at $t=1$ will coincide with the last control point. If I run the recursive definition of the basis functions, I will always come to the point where 
$$N_{i,1}(1) = \begin{cases}1 & t_i \le 1 \lt t_{i+1} \\ 0 & \text{otherwise}\end{cases}.$$
In the example above where $T=(0,0,0,0,0.5,1,1,1,1)$, I have the intervals $[0,0),[0,0.5),[0.5,1), [1,1)$. None of these will satisfy the condition $t_i \le 1 \lt t_{i+1}$.  So all ends of my recursion will result in 0, and $C(1)=0$. Where am I wrong? Or is there a special case for intervals of form $[a,a)$ ?
I implemented a simple version of the formulas above that I can provide if necessary. For $P = ([0,0], [0,1], [1,1], [1,0])$ and $T=(0,0,0,0,1,1,1,1)$ I got","['spline', 'curves', 'algebraic-geometry', 'geometry']"
1873699,The second differential as a differential on the double tangent bundle,"I know what the second differential of $f : \Bbb R^n \to \Bbb R$ means. Nevertheless, when working with abstract manifolds and in the absence of a connection, one cannot come up with a 2-covariant tensor that could reasonably be called $\Bbb d ^2 f$. This is why one gets around this by viewing $\Bbb d ^2 f$ as the differential $\Bbb d (\Bbb d f) : T(TM) \to \Bbb R$ of $\Bbb d f : TM \to \Bbb R$. In a bit of spare time I decided to try this on $\Bbb R^n$ and see what it gives. Unfortunately, the result lost me, because I do not know how to interpret it. Here it is. If $(\Bbb d _x f) (v) = \sum \limits _i (\partial _{x_i} f) (x) v_i$, then $$\begin{align}
\Bbb d _{(x,v)} (\Bbb d f) &= \sum _j \partial _{x_j} \left( \sum \limits _i (\partial _{x_i} f) (x) v_i \right) \Bbb d x_j + \sum _j \partial _{v_j} \left( \sum \limits _i (\partial _{x_i} f) (x) v_i \right) \Bbb d v_j \\
&= \sum _{j,i} (\partial _{x_j} \partial _{x_i} f) (x) v_i \ \Bbb d x_j + \sum _{j,i} (\partial _{x_i} f) (x) \delta _{ij} \ \Bbb d v_j \\
&= \sum _{j,i} (\partial _{x_j} \partial _{x_i} f) (x) v_i \ \Bbb d x_j + \sum _i (\partial _{x_i} f) (x) \ \Bbb d v_i .
\end{align}$$ There are two things bothering me here: the presence of those parasitic $v_i$ in the first sum; they should somehow play the role of $\Bbb d x_i$; the second term, containing derivatives of order 1. I guess that the first question is answered by noting that $\Bbb d (\Bbb d f)$ and $\Bbb d ^2 f$ are not really identical, that in fact they just correspond to each other by the isomorpshim that takes the map $(X, V) \to \Bbb d _{(x,v)} (\Bbb d f) (X, V)$ to... to what? I already have 4 arguments $x,v,X,V$, while $\Bbb d^2 f$ should have only 3. Equally important, how to eliminate the second sum?","['derivatives', 'smooth-manifolds', 'vector-space-isomorphism', 'bilinear-form', 'differential-geometry']"
1873702,Roots of polynomial with positive coefficients,"My question is very simple. Suppose we have a polynomial defined as follows:
$$p(x) = a_nx^n + a_{n-1}x^{n-1} + \cdots+a_0 $$
where all of the $a_n$'s are all real and positive. Is there something that we can say about the roots of $p(x)$? Can we say the roots of $p(x)$ all contain negative real parts? Thanks!","['algebra-precalculus', 'roots', 'polynomials']"
1873724,Horizontal Asymptotes,"Find all asymptotes of: $$f(x) = \frac{a + be^x}{ae^x+b}$$ The way I've been taught is that the $+a$ and $+b$ in the numerator and denominator respectively do not contribute when x tends to infinity, therefore are negligible. Left with $f(x) = \frac{be^x}{ae^x} = \frac{b}{a}$, $y = \frac{b}{a}$
 is the only asymptote I was able to identify (through this method). However, plotting the function with $a = 3$ and $b = 2$, there is clearly another horizontal asymptote, where $y = \frac{a}{b}$: Is there any way I could've known about the second asymptote without graphing?","['graphing-functions', 'limits']"
1873734,"Residue Theorem to solve $\int_{x=-\infty}^\infty \frac{sin(x \, l)}{x \, (x^2+a^2) \, (x^2+b^2)} \, e^{\, \mathrm{j} \,x\,c} \, \mathrm{d}x$","I would like to solve the following integral using residue theorem $\int_{x=-\infty}^\infty \frac{sin(x \, l)}{x \, (x^2+a^2) \, (x^2+b^2)} \, e^{\, \mathrm{j} \,x\,c} \, \mathrm{d}x$ with $a^2 \in\mathbb{C}$, $\,b^2 \in\mathbb{C}$, $\,l \in \mathbb{R}, \, l>0$ and $\,c \in \mathbb{R}, \, c>0$. I have troubles with the point $x=0$, since the residue becomes $0$ because $\mathrm{sin}(0)=0$. Can I simply ignore the removable discontinuity and use the Cauchy theorem to solve it? I will have two poles with positive imaginary part. The original problem is a bit more complicated: $\int_{x=-\infty}^\infty \int_{y=-\infty}^\infty \frac{sin(x \, l)}{x \, ((x^2+y^2)^2 - k^4)} \, e^{\, \mathrm{j} \,x\,c} \, e^{\, \mathrm{j} \,y\,d} \, \mathrm{d}x \, \mathrm{d}x$ with $k^4 \in\mathbb{C}$, $\,l \in \mathbb{R}, \, l>0$, $\,c \in \mathbb{R}, \, c>0$ and $\,d \in \mathbb{R}, \, d>0$.
I try to integrate first with respect to $x$ and then with respect to $y$ (or the other way around).","['complex-analysis', 'improper-integrals', 'residue-calculus']"
1873757,How to determine whether a function is odd or even in a positive domain,"We know that a function f is even if $f(-x)=f(x)$ and odd if $f(-x)=-f(x)$. Now my question : if the domain of the function is positive number, then how can I determine whether it odd or even? For example:    determine whether the following function is odd or even. $$f(x)=x(10-x), 0<x<10$$",['functions']
1873787,Show that $3^n+4^n+\dots+(n+2)^n=(n+3)^n$ has no answers for $n\ge 6$.,"Considering
$$3^n+4^n+\dots+(n+2)^n=(n+3)^n$$ Clearly $n=2$ and $n=3$ are solutions of this equation and this equality does not hold for $n=4$ and $n=5$. How can I show this equation has no solutions for $n>5$. Thanks.","['number-theory', 'calculus']"
1873793,"If Mutual Information measures dependence, why is it symmetric?","From Wikipedia we can read: In probability theory and information theory, the mutual information
  (MI) of two random variables is a measure of the mutual dependence
  between the two variables. in fact, $I(X;Y)=I(Y;X)$. But we know that MI can measure also non-linear dependence. To clarify that concept, I made this Venn diagram to describe what I know about dependence, linear correlation and causality in probability and statistics. $Y = X^2$ is an example of dependence between two RVs, that is not contained in the set of correlation, and cannot be detected by Pearson's coefficient. In that case, mutual information will be greater than zero suggesting a mutual dependence. But that's not true! I mean: Y depends on X but not viceversa. If Mutual Information measures dependence, why is it symmetric, while dependence is not?","['independence', 'statistics', 'probability']"
1873795,Compute trigonometric limit without use of de L'Hospital's rule,"$$ \lim_{x\to 0} \frac{(x+c)\sin(x^2)}{1-\cos(x)}, c \in \mathbb{R^+} $$ Using de L'Hospital's rule twice it is possible to show that this limit equals $2c$. However, without the use of de L'Hospital's rule I'm lost with the trigonometric identities. I can begin by showing 
$$
\lim\frac{x\sin (x^2)(1+\cos(x))}{\sin^2x}+\frac{c\sin x^2(1+\cos x)}{\sin^2x}=\lim\frac{\sin (x^2)(1+\cos(x))}{\sin x}+\frac{c\sin x^2(1+\cos x)}{\sin^2x},
$$ and here I'm getting stuck. I will appreciate any help.","['limits-without-lhopital', 'trigonometry', 'calculus', 'limits']"
1873807,What is the algebraic closure of $\mathbb F_q$?,"What is the algebraic closure of $\mathbb F_q$ with $q$ being some power of a prime $p$ ? I wrote, ''the algebraic closure'' because, they're the same up to isomorphism right ? It cannot be finite, otherwise it is not algebraically closed, so how does it look like ?","['finite-fields', 'abstract-algebra', 'field-theory']"
1873828,Showing two integrals are equal,"I would like to show that $$
\frac{1}{\Gamma\left(\, -\alpha\,\right)}
\int_{-\infty}^{x}\frac{\,\mathrm{f}\left(\, t\,\right)}
{\left(\, x - t\,\right)^{\alpha + 1}}\,\mathrm{d}t
= \frac{1}{2\pi} \int_{-\infty}^{\infty}(\mathrm{i}t)^{\alpha}\,\mathrm{e}^{\mathrm{i}kt}
\int_{-\infty}^{\infty}\,\mathrm{e}^{-\mathrm{i}kt}
\,\mathrm{f}\left(\, k\,\right)\,\mathrm{d}k\,\mathrm{d}t$$ for fixed $\alpha$ such that $\Re(\alpha) < 0$ and square integrable $f$. I was under the impression that this is a straight forward computation, but I am having difficulty getting anywhere. I apologize for the weak effort in advance. So far, I've tried considering rewriting the LHS in terms of the Cauchy Integral formula by writing something like $$\frac{1}{\Gamma(-\alpha)} \left(\int_{-\infty}^{\infty} \frac{f(t)}{(x-t)^{\alpha+1}}dt - \int_{x}^{\infty} \frac{f(t)}{(x-t)^{\alpha+1}}dt\right)$$ but I can't get anything that is both useful and makes sense from here. I've also considered trying to solve one of the integrals on the RHS using some contour integration technique by considering something like $$\int_{-\infty}^{\infty} e^{-itx}f(x)dx = \lim_{R\rightarrow \infty}\int_{-R}^{R} e^{-itz} f(z)dz + \int_{\gamma} e^{itz}f(z)dz$$ but it is difficult for me to decide how I can choose a $\gamma$ which makes sense knowing nothing about the poles of $f$. The only other ideas I have might be to try to use integration by parts to rewrite both sides using properties of the Fourier transform or possibly try to rewrite the integrals as infinite sums but I'm not getting anywhere.","['complex-analysis', 'calculus']"
1873837,Position of indices when and after being raised/lowered,"I'm having some trouble with the rule of lowering and raising indices in my textbook:
$$\begin{align}
\tau_{j_1\ldots j_s j}^{i_1\ldots i_{k-1}\hspace{0.5em}i_{k+1}\hspace{0.5em}\ldots i_r} & \equiv d_{ji_k}\tau_{j_1\ldots j_s}^{i_1\ldots i_{k-1}\hspace{0.5em}i_k\hspace{0.2em}i_{k+1}\hspace{0.5em}\ldots i_r}\\
\tau_{j_1\ldots j_s}^{i_1\ldots i_{k-1}\hspace{0.5em}i_k\hspace{0.2em}i_{k+1}\ldots i_r} & \equiv d^{ji_k}\tau_{j_1\ldots j_sj}^{i_1\ldots i_{k-1}\hspace{0.5em}i_{k+1}\hspace{0.5em}\ldots i_r}\\
\end{align}$$
etc. What confuses me is the position of indices when and after being lowered/raised. These formulas seem to imply that one may raise any index to any place he likes. However, I found a contradiction when applying this rule to an antisymmetric second-order covariant tensor, for example:
$$a^{ij}\equiv d^{ii'}d^{jj'}a_{i'j'}$$
While, if the raised indices can be placed anywhere:
$$a^{ji}\equiv d^{jj'}d^{ii'}a_{i'j'}=a^{ij}$$
Contradicting the antisymmetry of $a^{ij}$ (it can be easily proved that $a^{ij}$ is antisymmetric due to the antisymmetry of $a_{ij}$. I searched the Wikipedia, but the definition there seems to allow only raising the last index below to the first index above, which, I think, is not satisfying since one should be able to raise/lower any index. Did I take anything wrong? Or was my textbook making a mistake? More precisely, what's the restrictions for the position of indices being raised/lowered and after that?","['tensors', 'geometry']"
