question_id,title,body,tags
3276360,Quasi-uniform triangulations and discretizations,"Given an interval $\Omega=(0,1)$ I can define a family of discretizations (triangulations) $\{\tau_n\}_{n\in\mathbb{N}}$ where, for a fixed $\bar{n}\in\mathbb{N}$ , we have a certain sequence of $\bar{n}+2$ points $x_0<\dots<x_{\bar{n}+1}$ , so actually $\bar{n}$ intervals. Now my question is, when is my family of discretizations quasi-uniform ? I have seen the definition that it is if $\exists c>0$ such that $\frac{h_{max}^{(n)}}{h_{min}^{(n)}}<c$ where $h_{min}^{(n)},h_{max}^{(n)}$ are respectively the minimum and the maximum lengths of the intervals in the discretization $\tau_n$ , and $c$ shouldn't depend on $n$ . On another book I've seen another definition, which simply compares the minimum diameter of the triangulation $\tau_n$ with the length of an interval in a uniform triangulation of $n+2$ points on $\Omega$ , which is $\frac{1}{n+1}$ . In formulas, this writes: $\frac{h_{min}^{(n)}}{1/(n+1)}>\varepsilon$ for every $n$ , where $\varepsilon$ should not depend on $n$ . So in case such an $\varepsilon$ exists, we say the family of discretizations quasi-uniform. So my questions are: Which one of these definitions is correct? If both are, are they equivalent? Is the notion of quasi-uniform discretization of a 1-dimensional domain related to the one of regularity (or shape-regularity ) of a triangulation of a 2-dimensional polygonal domain?","['discrete-geometry', 'computational-geometry', 'triangulation', 'finite-element-method', 'discrete-mathematics']"
3276373,Let $G$ be a $p$-group: $|G| = p^r$. Prove that $G$ contains a normal subgroup of order $p^k$ for every nonnegative $k \le r$.,"Let $p$ be a prime number, and let $G$ be a $p$ -group: $|G| = p^r$ . Prove that $G$ contains a normal subgroup of order $p^k$ for every nonnegative $k \le r$ . The answers here and here use induction but they assume $G$ , where $|G|=p^r$ , has normal subgroups of order $p^k$ for $k <r$ . Induction should start by assuming for every $p$ -group of order $p^k$ where $0\le k <r$ , there exists normal subgroups of order $p^i$ where $0 \le i \le k$ . We have to show there exists normal subgroups of order $p^i$ where $0 \le i \le r$ .","['proof-verification', 'finite-groups', 'fake-proofs', 'abstract-algebra', 'group-theory']"
3276391,Prime factor inversion,"Define a function $f(n)$ for $n \in \mathbb{N}$ to ""invert"" the prime
factorization of $n$ in the following sense. Let me start with an example. If $n= 3564 = 2^2 \cdot 3^4 \cdot 11^1$ ,
then $f(n) = 2^2 \cdot 4^3 \cdot 1^{11} = 256$ ,
inverting the base primes and their exponents. In general, if the prime
factorization of $n$ is $p_1^{m_1} \cdot \cdots \cdot p_k^{m_k}$ ,
then $f(n) = m_1^{p_1} \cdot \cdots \cdot m_k^{p_k}$ . Continuing the above example: \begin{eqnarray}
f(n) &= f(3564 = 2^2 \cdot 3^4 \cdot 11^1) & = 2^2 \cdot 4^3 \cdot 1^{11} = 256\\
f^2(n)&=f(256=2^8) &= 8^2 =64 \\
f^3(n)&=f(64 = 2^6) &= 6^2 = 36 \\
f^4(n)&=f(36 = 2^2 \cdot 3^2) &= 2^2 \cdot 2^3 = 32\\
f^5(n)&=f(32 = 2^5) &= 5^2 = 25\\
f^6(n)&=f(25 = 5^2) &= 2^5 = 32
\end{eqnarray} and we have fallen into a $2$ -cycle. My questions concern the iterated behavior of $f(n)$ . Q1 . If $n= \Pi p_i$ , $p_i$ primes, then $f(n) = 1$ .
If $n= \Pi p_i^{p_i}$ , $p_i$ primes, then $f(n) = n$ , a fixed point.
Are any other $n$ fixed points or $n$ that eventually lead to $1$ -cycles? Q2 . If $n= \Pi p_i^{q_i} \cdot q_j^{p_j}$ , all of $p_i,p_j,q_i,q_j$ primes, then $f^2(n)=n$ , a $2$ -cycle.
Can you see a characterization of those $n$ that eventually fall into a $2$ -cycle? Q3 . Are there any $n$ that eventually fall into $k$ -cycles
for $k \ge 3$ ?","['number-theory', 'fixed-points', 'prime-factorization', 'prime-numbers']"
3276430,Eigenvalues of a $12 \times 12$ Jacobian matrix,"Consider the set of coordinates $X_{i, j}^{(\ell)}$ and $Y_{i, j}^{(\ell)}$ , where $i \in (1, 2, 3), j \in (1, 2, 3)$ and $i \neq j$ and $\ell = \pm 1$ . The superscipt $(\ell)$ is an index. Consider the change of variables from $\mathbf{X}$ to $\mathbf{Y}$ defined by \begin{equation}
Y_{i, j}^{(\ell)} = c_{i, j}^{(\ell)} + c_{i, k}^{(\ell)} X_{k, j}^{(\ell)} + c_{i, j}^{(-\ell)} X_{j, i}^{(-\ell)} X_{i, j}^{(\ell)} + c_{i, k}^{(-\ell)} X_{k, i}^{(-\ell)} X_{i, j}^{(\ell)} \tag{1}
\end{equation} where $(i, j, k)$ is any permutation of $(1, 2, 3)$ and the $c_{i, j}^{(\ell)}$ are any non-zero numbers constrained, for each $i$ , by $c_{i, j}^{(1)} + c_{i, k}^{(1)} + c_{i, j}^{(-1)} + c_{i, k}^{(-1)} = K$ for some constant $K > 0$ . So, for example, if $i = 1$ then $c_{1, 2}^{(1)} + c_{1, 3}^{(1)} + c_{1, 2}^{(-1)} + c_{1, 3}^{(-1)} = K$ . Let $\mathbf{J}(\mathbf{X})= \partial \mathbf{Y}/ \partial \mathbf{X}$ be the Jacobian matrix of the transformation in Eq. $(1)$ . Let $\mathbf{1}$ be a vector of length $12$ consisting of all $1$ 's. Fact 1 : The constant $K$ is an eigenvalue of $\mathbf{J}(\mathbf{1})$ . Fact 2 : If there is a vector $\mathbf{v}$ satisfying $\mathbf{J}(\mathbf{1}) \mathbf{v} = K \mathbf{v}$ , then it has the property that $v_{i, j}^{(k)} + v_{j, i}^{(k)} = 0$ , where $v_{a, b}^{(c)}$ is the entry of $\mathbf{v}$ corresponding to coordinate $X_{a, b}^{(c)}$ . I know Fact 1 and Fact 2 are facts, because the eigensystem of a $12 \times 12$ matrix is still (barely) within the realm of tractability for computers. My question is therefore: Is there any way to see that either Fact 1 or Fact 2 is true without having to actually find the eigenvector? A relevant, similar question I have asked is here . In that case, it was ""easy"" to guess the eigenvector but I can't see how to do that here. It has to be something about the structure of Eq. $(1)$ . There's this idea that if the sum of the coefficients of a transformation is constrained to be a constant in the way I've described, then this constraint shows up as an eigenvalue of the Jacobian and I wonder how general this property is. Mathematica Code Using the convention $c_{i, j}^{(\ell)} = c[i, j, \ell]$ , the Jacobian is jac = {{c[1, 2, 1] + c[1, 3, 1], 0, 0, 0, 0, c[1, 2, 1], 0, 0, 0, c[1, 3, 1], c[1, 3, -1], 0},
{0, c[1, 2, -1] + c[1, 3, -1], 0, 0, c[1, 2, -1], 0, 0, 0, c[1, 3, -1], 0, 0, c[1, 3, 1]},
{0, 0, c[1, 2, 1] + c[1, 3, 1], 0, 0, c[1, 2, 1], c[1, 2, -1], 0, 0, c[1, 3, 1], 0, 0},
{0, 0, 0, c[1, 2, -1] + c[1, 3, -1], c[1, 2, -1], 0, 0, c[1, 2, 1], c[1, 3, -1], 0, 0, 0},
{0, c[2, 1, 1], 0, 0, c[2, 1, 1] + c[2, 3, 1], 0, 0, 0, c[2, 3, -1], 0, 0, c[2, 3, 1]},
{c[2, 1, -1], 0, 0, 0, 0, c[2, 1, -1] + c[2, 3, -1], 0, 0, 0, c[2, 3, 1], c[2, 3, -1], 0},
{0, c[2, 1, 1], c[2, 1, -1], 0, 0, 0, c[2, 1, 1] + c[2, 3, 1], 0, 0, 0, 0, c[2, 3, 1]},
{c[2, 1, -1], 0, 0, c[2, 1, 1], 0, 0, 0, c[2, 1, -1] + c[2, 3, -1], 0, 0, c[2, 3, -1], 0},
{0, 0, 0, c[3, 1, 1], c[3, 2, -1], 0, 0, c[3, 2, 1], c[3, 1, 1] + c[3, 2, 1], 0, 0, 0},
{0, 0, c[3, 1, -1], 0, 0, c[3, 2, 1], c[3, 2, -1], 0, 0, c[3, 1, -1] + c[3, 2, -1], 0, 0},
{c[3, 1, -1], 0, 0, c[3, 1, 1], 0, 0, 0, c[3, 2, 1], 0, 0, c[3, 1, 1] + c[3, 2, 1], 0},
{0, c[3, 1, 1], c[3, 1, -1], 0, 0, 0, c[3, 2, -1], 0, 0, 0, 0, c[3, 1, -1] + c[3, 2, -1]}} Where the rows are in the order rows = {c[1, 2, -1], c[1, 2, 1], c[1, 3, -1], c[1, 3, 1], c[2, 1, -1], c[2, 1, 1],
      c[2, 3, -1], c[2, 3, 1], c[3, 1, -1], c[3, 1, 1], c[3, 2, -1], c[3, 2, 1]} Here's a sample set of $c_{i,j}^{(\ell)}$ where $K = 1$ : vec = {0.213509, -0.0587312, 0.321703, 0.523519, 0.255562, -0.290697,
     0.211956, 0.823179, 0.0321628, 1.087800, -0.373663, 0.253700} Running the following command: Eigenvalues[jac /. Thread[rows -> vec]] Shows that $1$ is indeed an eigenvalue.","['matrices', 'jacobian', 'nonlinear-system', 'eigenvalues-eigenvectors']"
3276453,Contractible space which is not pointed-contractible. At all. Ever. Under any circumstances.,"A space $X$ is contractible if there is a homotopy $G:X\times I\rightarrow X$ with $G_0=id_X$ and $G_1=x_0$ the constant map at some point $x_0\in X$ . The space $X$ is said to be pointed-contractible (with respect to $x_0$ ) if $G$ can be chosen such that $G_t(x_0)=x_0$ for all $t\in I$ . An interesting example comes from the comb space, which, as parametrised here , is contractible, and is pointed contractible with respect to the point $(0,0)$ , but is not pointed contractible with respect to the point $(0,1)$ . A student of mine posed me the following question. I didn't know an answer, so I'll pitch it to you guys. Is there a space which is contractible, but not pointed-contractible with respect to any given basepoint?","['general-topology', 'algebraic-topology']"
3276462,"$x^a+ y^b + z^c$ is irreducible in $\mathbb C[x,y,z]$","Let $a,b,c$ be positive integers. Then $f = x^a + y^b + z^c$ is irreducible in $\mathbb{C}[x,y,z]$ . By Gauss, $f$ is irreducible in $\mathbb{C}[x,y,z]$ iff is so in $\mathbb{C}(z)[x,y]$ , and so iff in $\mathbb{C}(y, z)[x]$ . So by Eisenstein, it is sufficient to show that $y^b + z^c$ has a single prime factor. If $b=c$ , this factors through $\Pi(y + \zeta ^i z)$ for some primitive root of unity $\zeta$ , so ok. But if $b\neq c$ ? This question is related to this post . Thank you very much!","['irreducible-polynomials', 'multivariate-polynomial', 'abstract-algebra', 'polynomials', 'complex-numbers']"
3276572,"Do we have for all $M \in SL_n(\Bbb K)$, $\lVert M \rVert \geq 1$ when $\lVert \cdot \rVert$ is a matrix norm?","Let be $\lVert \cdot \rVert$ a matrix norm (submultiplicative). Do we have for all matrices of determinant 1, the following lower bound: $$\lVert M \rVert \geq 1$$ I'm very confused and could not find any counterexample and I find this statement very fishy, I tried to experiment with: \begin{bmatrix}
1&  x \\
0&  1
\end{bmatrix} But, its Frobenius norm cannot be small enough.","['determinant', 'normed-spaces', 'matrices', 'linear-algebra', 'matrix-norms']"
3276576,Intuition of Liouville's Theorem (differential algebra) Proof,"At the end of my abstract algebra class this spring, we were given an overview of differential algebra and some differential Galois theory. We went too fast to prove anything nontrivial, but I found Liouville's theorem on elementary antiderivatives really interesting and have been trying to get a handle on the proof recently. The theorem is as follows: Theorem (Liouville) Let $\mathbb{F}$ be a differential field. For $a\in \mathbb{F}$ , there exists an elementary differential extension $\mathbb{E}$ containing an antiderivative of $a$ if and only if there exist $u_1,...,u_n,v\in\mathbb{F}$ , and constants $c_1,...,c_n\in\mathbb{F}$ such that $$ a = v' + \sum_{i=1}^nc_i\frac{u_i'}{u_i}.$$ The only proof I've found online is that due to Maxwell Rosenlicht (see theorem 4.3 here ). Its logic is reasonably easy to follow but it's long enough that I can't piece together an intuition from it. Does anyone have / know of an intuitive account for why Liouville's differential algebra theorem is true?","['proof-explanation', 'abstract-algebra', 'differential-algebra', 'intuition']"
3276577,Looking for density estimator with time complexity $< \mathcal O(n^2)$,"I am doing univariate non-parametric density estimation on a dataset $D$ , and I want to 1) Train a density estimator on $D$ 2) Compute the estimated density at each point in $D$ These two operations should have an aggregate time complexity $< \mathcal O(n^2)$ , and the PDF estimate should be differentiable. An $\mathcal O(n)$ time complexity of the aggregate process is possible with the histogram estimator. However, the PDF estimate is not differentiable. The Gaussian kernel density estimator gives a differentiable PDF estimate. However, the time complexity of the aggregate process is $\mathcal O(n^2)$ . With Gaussian KDE, density estimation of a single point is $\mathcal O(n)$ , so density estimation of all points in the dataset would be $\mathcal O(n^2)$ . Thus this method does not have an aggregate time complexity $< \mathcal O(n^2)$ . Does there exist a density estimator that would satisfy both conditions? EDIT: I found a way to have an aggregate time complexity of $\mathcal O(n)$ with Gaussian KDE. See the following link... http://www-stat.wharton.upenn.edu/~lzhao/papers/MyPublication/Fast_jcgs.2010.pdf","['machine-learning', 'statistics', 'density-function']"
3276595,How exactly is this integral finite?,"From here , Koosis is proving the inequality that $$\int_{- \infty}^{\infty}\frac{\log^+|S(x)|}{1+x^2}dx \qquad (1) $$ is finite by using the fact that $\int_{- \infty}^{\infty}\frac{\log^+|S(x+i)|}{1+x^2}$ is finite. Reading through he applies this theorem and uses a so called ""Hall of Mirrors"" argument by applying this theorem to the half plane $Im(z) < 1$ to get for any real number $\zeta$ that $$\log|S(\zeta)| \leq \frac{1}{\pi}\int_{- \infty}^{\infty}\frac{\log^+|S(x+i)|}{1+(x-\zeta )^2}dx.$$ Now, I'm not sure how to use this to get $(1)$ as the LHS is not exactly what we know to be finite. His mention of Fubini's Theorem makes be think of multiplying both sides by $\frac{1}{1+\zeta ^2}$ and integrating over the reals. Would this work? My thought is that the $\frac{1}{1+(x-\zeta) ^2}$ factor would mess with this argument. I'm not sure if this is the right way to go. Any suggestions or words of encouragement?","['complex-analysis', 'improper-integrals', 'harmonic-functions']"
3276614,"Uniform Random Variable on $[0,1]$ and Bernoulli$(1/2)$","Let $X_1,X_2,...$ be independent, identically distributed (iid) random variables with distribution Bernoulli $(1/2)$ . Define the random variable: $$Y=\sum_{n=1}^\infty\frac{X_n}{2^n}.$$ Then $Y$ is unifromly distributed over the unit interval $[0,1]$ . The proof of this result can be found in our mathstacexchange: Series of independent Bernoulli variables But the inverse proposition: If $Y$ is a uniform random variable on unit interval $[0,1]$ , and $$Y=\sum_{n=1}^\infty\frac{X_n}{2^n}.$$ then $X_1,X_2,...$ are iid sequences of Bernoulli $(1/2)$ . How can we prove this result?","['probability-distributions', 'uniform-distribution', 'probability-theory']"
3276655,What can we say about $Q$ given $Q=P+\frac{1}{2}I$ where $P \in M_n \left(\mathbb{Z}\right)$,"Let $P$ be a $n\times n$ matrix with integral entries and $Q=P+\frac{1}{2}I$ , where $I$ denotes the $n\times n$ identity matrix. Then what can you say about $Q$ . Is it Idempotent? Is it Invertible? Is it Nilpotent? Is it Unipotent? According to me It is not necessary that it is Idempotent, Nilpotent, Unipotent. For a counter example take the diagonal matrix $B=\left( b_{ij}\right)$ with $b_{ii}=i$ . I claim that it is invertible. Consider the matrix $2Q=2P+I$ . Reduce each entry modulo 2. We get $I$ whose determinant is $1$ hence determinant of $2Q$ must be odd. Hence $2Q$ is invertible and hence $Q$ is invertible. Is this method correct?
Do you have any other better method?","['matrices', 'linear-algebra', 'block-matrices']"
3276665,Limit of the quotient of sums of recurrent relations,"We define a sequence $a_n(b,x)$ for any $b,x\in [0,1]$ by letting $a_0(b) = 1$ and further: $$
a_{n+1}(b,x)
=
x \cdot ( b \cdot a_n(b,x)^2+(1-b)\cdot a_n(b,x)\ ),
$$ note that in the extreme values, $b\in \{0,1\}$ we have $a_n(0,x)=x^n$ and $a_n(1,x)=x^{2^n-1}$ . Let us define $A(b,x)=\sum_{n=0}^{\infty} a_n(b,x)$ . It is clear that for any $x \in (0,1)$ and $b \in [0,1]$ we have $A(b,x)$ is a convergent sequence. However for $x=1$ this sequence becomes divergent for all values of $b$ . I am interested in computing the following limit: $$
\lim_{x\rightarrow 1^-} \frac{A(b,x)}{A(1,x)}.
$$ It is not too hard to show that, for $b=0$ , this quotient is equal to infinity. However, for other values $b\in (0,1)$ I have found numerically that this quotient is some finite number. The numerical values I found seem to suggest that the following should hold: $$
\lim_{x\rightarrow 1^-} \frac{A(b,x)}{A(1,x)}
=
1 + \frac{(1-b)\log(2)}{b}
$$ I can however not find how to verify this in an analytical way. I have tried to obtain upper/lower bounds on the values of $a_{n+1}(b,x)$ which would allow for some closed formula but with no success. I am looking for methods which may be used to tackle this problem.","['recurrence-relations', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
3276700,What is the weak closure of $C_c(X)$ in $C_b(X)$?,"Given a locally compact Hausdorff space $X$ , let $C_b(X)$ denote the bounded continuous functions with sup norm and $C_c(X)$ denote the continuous functions with compact support. I was wondering what the closure of $C_c(X)$ looks like in the weak topology i.e. topology induced by $C_b(X)^*$ . I am not sure if this is obvious, but I would appreciate any help on this, thanks!","['banach-spaces', 'general-topology', 'functional-analysis']"
3276709,How to derive the number of spanning tree in this graph,"Given this undirected graph $G_n$ where $n>2$ , Let $h_n$ be number of possible spanning tree ( $h_1=0$ ) and let $g_n$ be the number of sides connecting node $1$ and node $n$ . Then, how can I express $g_n$ using $h_n$ and $h_{n-1}$ ?
Can someone tell me how to derive this?","['graph-theory', 'spectral-graph-theory', 'discrete-mathematics']"
3276710,Is the expectation of a random variable itself a random variable?,"The expectation of a random variable X is a function of X . Now, functions of random variables are random variables themselves. Then is the expectation of a random variable itself a random variable?","['expected-value', 'probability-theory']"
3276716,Evaluating a determinant with multiple variables and multiplicative inverses,"I just asked a question on math stack exchange and doing some progress on it: Proof of Pascal's Theorem (on circles) using complex numbers. . I did some complex number geometry, got the intersection points and now all it remains is to evaluate a determinant which should evaluate to zero. Here's the determinant: $$
   \frac{i}{4} \begin{vmatrix}
    \frac{ab(d+e) - de(a+b)}{ab-de} & \frac{\frac{1}{ab}(\frac 1d+\frac1e) - \frac{1}{de}(\frac1a+\frac1b)}{\frac{1}{ab}-\frac{1}{de}} & 1 \\
    \frac{bc(e+f) - ef(b+c)}{bc-ef} & \frac{\frac{1}{bc}(\frac1e+\frac1f) - \frac{1}{ef}(\frac1b+\frac1c)}{\frac{1}{bc}-\frac{1}{ef}} & 1 \\
    \frac{cd(f+a) - fa(c+d)}{cd-fa} & \frac{\frac{1}{cd}(\frac1f+\frac1a) - \frac{1}{fa}(\frac1c+\frac1d)}{\frac{1}{cd}-\frac{1}{fa}} & 1 \\
    \end{vmatrix}
$$ where $a, b, c, d, e, f$ are points $A, B, C, D, E, F$ on the unit circle. I don't want to evaluate this by straight up multiplication and minors. Please explain how to solve this determinant using its properties. Thanks!","['matrices', 'complex-geometry', 'determinant', 'complex-numbers']"
3276737,Finding $\sin^2\alpha+\sin^2\beta+\sin^2\gamma$ given $\sin \alpha+\sin \beta+\sin\gamma=0=\cos\alpha+\cos\beta+\cos\gamma$,"I am supposed to find the value of $\sin^2\alpha+\sin^2\beta+\sin^2\gamma$ and I have been provided with the information that $\sin \alpha+\sin \beta+\sin\gamma=0=\cos\alpha+\cos\beta+\cos\gamma$ . I tried to approach this using vectors. We can consider three unit vectors that add up to $0$ . Unit vectors because the coefficients of the $\sin$ and $\cos$ terms are $1$ . For the sake of simplicity, let one of the vectors $\overline{a}$ be along the $x$ -axis. Let the angles between $\overline{b}$ and $\overline{c}$ be $\alpha$ , between $\overline{a}$ and $\overline{b}$ be $\gamma$ and between $\overline{a}$ and $\overline{c}$ be $\beta$ . Then we have: $$\begin{aligned}\overline{a}&=\left<1,0\right>\\ \overline{b}&=\left<-\cos\gamma, -\sin\gamma\right>\\ \overline{c}&=\left<-\cos\beta, \sin\beta\right>\end{aligned}$$ $$\begin{aligned}\cos\gamma+\cos\beta &=1\\ \sin\beta&=\sin\gamma\end{aligned}$$ Now, $\cos \gamma$ and $\cos\beta$ must have the same sign. So we get $\sin\alpha=-\sqrt{3}/2$ , $\sin\beta=\sqrt{3}/2$ and $\sin\gamma=\sqrt{3}/2$ . This contradicts with the answer key provided according to which $\sum_{cyc}\sin^2\alpha=3/2$ . What am I doing wrong? This was the picture I had in mind with $\overline{a}$ aligned with the horizontal.","['trigonometry', 'problem-solving', 'vectors']"
3276751,Powers of a commutator are commutators?,"Let $G$ be a finite group and $g$ be a commutator. It can be shown that if $m$ is a positive integer coprime to order of $g$ , then $g^m$ is also a commutator ( link ). Q. Is there any example of a finite group such that $g$ is a commutator but for $m$ a divisor of order of $g$ , the element $g^m$ is not a commutator?","['group-theory', 'finite-groups']"
3276774,Why can't I calculate the $R^2$ in some regression models if I use the method of maximum likelihood estimation?,"I've modeled two regression models the first is a multiple linear regression (OLS) $$Y=\beta_0+\beta_1X_1+...+\beta_nX_n+e$$ and I can get its $R^2$ . The second model is a spatial autoregressive model (SAR) $$Y=\rho W+\beta_0+\beta_1X_1+...+\beta_nX_n+e$$ where W is the contiguity matrix and $\rho$ is an unknown parameter. This model is estimated by the method of maximum likelihood but I cannot calculate its $R^2$ and rather I have to use the $R^2$ Nalgerkerke. I've found this ""There is no direct equivalent to the OLS R-squared, these models are fitted by maximum likelihood."" from http://r-sig-geo.2731867.n2.nabble.com/How-to-calculate-squared-R-of-spatial-autoregressive-models-td5762576.html but I'd like to know why I cannot calculate $R^2$ for this model if the formula is just $$R^2=1-\frac{\sum(y_i-\hat{y_i})^2}{\sum(y_i-\overline{y})^2}$$","['linear-regression', 'statistics', 'maximum-likelihood']"
3276778,Does convergence in probability imply that $\lim_{n \to \infty} P(|X_n-X| \geq \varepsilon_n) = 0$ for any $\varepsilon_n$?,"Assume that $X_n \overset{P}{\to} X$ i.e. for all $\varepsilon > 0$ $$
\lim_{n \to \infty} P(|X_n-X| \geq \varepsilon) = 0 .
$$ Let $(\varepsilon_n)_{n \in \mathbb{N}}$ be any sequence of $\varepsilon$ 's bounded by $1$ . Does $$
\lim_{n \to \infty} P(|X_n-X| \geq \varepsilon_n) = 0 ?
$$ I'm thinking this is false but have a hard time coming up with a counter example, any ideas?",['probability-theory']
3276794,Automorphism in complex analysis,"Just a quick question about naming: in complex analysis, it appears to me that an ""Automorphism"" of $\Omega\subseteq \mathbb C$ means a conformal mapping from a subset $\Omega$ to itself. So, an automorphism doesn't necessarily preserve the group structure of $\mathbb C$ However, in group theory, ""automorphisms"" should be isomorphisms. Why the same word mean such two different things? NOTE Someone appears to say that this is a duplicate question, so let me make it more clear: My question is, does the word ""automorphisms"" mean means completely different things in group theory and complex analysis? Or actually they mean similar things? That question does not mention groups at all.","['complex-analysis', 'group-theory', 'notation']"
3276826,General solution of $\tan (2x)\tan (x)=1$,"For the question, $\tan (2x) \tan x=1$ , I divided it by $\tan x$ , and got the solution as $\frac{(2n+1)\pi}{6}$ . $\tan 2x= \cot x= \tan\left(\frac{\pi}{2}-x\right)$ . So, $2x=n\pi+ \frac{\pi}{2}-x$ . So, $3x= \frac{(2n+1)\pi}{2}$ But the book solved using the formula of $\tan (2x)$ , and got the solution as $\frac{(6n \pm 1)\pi}{6}$ . I can see that my solution has odd multiples of $\pi/2$ , which should be discarded, but I thought of it only after checking the solution. Also, in a way, it suggests that we can't solve these questions in different ways because that way we might get extra solutions. So, how to ensure which method to follow?","['proof-explanation', 'trigonometry', 'proof-writing']"
3276833,How to calculate the range of $f(x) = x^2 - 2 | x |$?,"So if I do this : $$
\left|x\right|^2 - 2 \left|x \right|+1-1 = \left( \left|x\right|-1 \right) ^2-1
$$ I get the range to be $[-1,\infty)$ and I also get this answer from GeoGebra. But I don't think this as the correct process.
Because for $f(x) = x^2 + 2 |x|$ without any calculation I can tell that its range is $[0,\infty)$ . But by the previous process, I get, $f(x)=(|x|+1)^2 - 1$ so the range is also $[-1,\infty)$ which is not true.
So how can I calculate the range of this function?",['functions']
3276848,What Constitutes a Decreasing Interval?,"Problem: For what values of $x$ is $f(x)=x^4-4x^3$ increasing? decreasing? The first half of the answer in the book is: if $x<3$ , $f'(x)\leq 0$ and $f$ is decreasing There is also a note: Note that $f$ is decreasing at $x=0$ even though $f'(0)=0$ If $f'(0)=0$ , then $f$ at $x=0$ is neither increasing nor decreasing. So why is the book saying what it's saying?","['calculus', 'derivatives']"
3276858,A counterexample in measure theory on $\sigma$-infinite spaces,"Usually measure theory books include the following theorem (citing Proposition 5.1.3 in Cohn's measure theory book) Let $(X, \mathcal A , \mu )$ and $(Y, \mathcal B, \nu )$ be $\sigma$ -finite measure spaces. If $E$ belongs to the $\sigma$ -algebra $\mathcal{A}\times\mathcal{B}$ , then the function $x\mapsto\nu(E_x)$ is $\mathcal{A}$ -measurable
  and the function $y\mapsto\mu(E^y)$ is $\mathcal{B}$ -measurable. where $E_x=\{y\in Y\mid (x,y)\in E\}$ and $E^y=\{x\in X\mid (x,y)\in E\}$ are the slices of $E$ . I'm looking for an example of two spaces (with at least one of them necessarily not $\sigma$ -finite) for which this theorem fails. I believe one of the two needs to be not only $\sigma$ -infinite but also s-infinite. If this is not the case I would be very interested in an example involving an s-finite but not $\sigma$ -finite space.","['measure-theory', 'examples-counterexamples', 'analysis']"
3276920,"$\lim_{n\to\infty}\frac{n^{3/2}}{\ln n}\int_0^1\frac{\sqrt x\ln x}{(1+2x-x^2)^n}dx$, an attempt to generalize Watson's lemma","Background and Motivation Given that Watson's lemma , a theorem that can compute the asymptotic expansion of $$\int_0^N p(x)e^{-zq(x)}dx\text{ as } z\to\infty,$$ with the elementary restriction of $p,q$ being $\color{red}{C^\infty}[0,\varepsilon)$ for some $\varepsilon>0$ (and some other restrictions), I want to generalize this theorem, to some extent. Question I've already evaluated the limit $$\lim_{n\to\infty}n^{3/2}\int_0^1\frac{\sqrt x}{(1+2x-x^2)^n}dx=\frac{\sqrt{2\pi}}8,$$ and I can even use Watson's lemma to calculate the asymptotic expansion to $O(n^\alpha)$ with arbitrary big $\alpha$ . i) Given $\mu>0$ , can we evaluate $$\lim_{n\to\infty}\frac{n^{3/2}}{\ln^\mu n}\int_0^1\frac{\sqrt x\ln^\mu (1/x)}{(1+2x-x^2)^n}dx?$$ Furthermore, is there a method to find all terms of the asymptotic expansion of the integral? This integral is merely an example of the generalization of the theorem. ii) Reference request: Given $p(x)\in C^{\color{red}\omega}(0,\varepsilon)\cap C^{\color{red}\omega}(a-\varepsilon,a)\cap R[0,a]$ , is there a method to find the full asymptotic expansion of $$\int_0^a p(x)e^{zx}dx$$ as $z\to\infty$ ? Attempt of (i) Fix $\varepsilon>0$ , noting that $\displaystyle\int_\varepsilon^1\frac{\sqrt x\ln ^\mu(1/x)}{(1+2x-x^2)^n}=O(n^{-N})$ for all $N\in\mathbb R$ , this term can be neglected. There is a very classical method for this type of integrals which is substituting $\sqrt xdx=dt$ , then obtaining $$\int_0^{\varepsilon'}\frac{\ln^\mu (1/x)}{(1+2x^{2/3}-x^{4/3})^n}dx$$ and finally neglect the higher order term. But this method can only be used when $\mu=0$ and can only get the rough order of it.","['definite-integrals', 'asymptotics', 'reference-request', 'calculus', 'limits']"
3276930,"Computational results for the sequence $n!+{p_n}!+1$ are, well, very very unusual","Peter and I were discussing in a chat room and I thought that it would be nice to test the sequence $$n!+{p_n}!+1$$ for primality. Then I wrote Peter that I expect much of primes in this sequence, well, at least the expression is formed in such a way that there could be much of them. But, to my and Peter´s surprise, he computed and tested this sequence for all $1\leq n \leq 100$ and found that for only $n=3$ in that range the expression is prime. He is pushing his computations even further, even at this moment, and, for me almost unbelievable is that he passed $n=800$ and found that only for $n=3$ the expression is prime and we are curious why there is (so far) only one prime although small factors are impossible for large values of $n$ . So, we agreed that a question about this unusuality should be asked, and I ask three of them, very much interrelated and connected and answerable. Do we have any explanation of why this sequence has, for the range computed, so a small number of primes? Is it naive to expect that this sequence has an infinite number of primes? What ""should"" be expected to happen in some very large ranges? Update : No further primes up to $n=1000$ .","['number-theory', 'soft-question', 'prime-numbers', 'factorial']"
3276942,Group homomorphism on $\mathbb{R}$,Let $f :\mathbb{R} \rightarrow \mathbb{R}$ be a group homomorphism W. R. T.  Usual addition such that $f$ is bounded at a neighborhood of $0$ . Can you conclude that $f$ is continuous at $0$ ?,"['abstract-algebra', 'analysis']"
3276950,Minimal polynomial of $\cos(72°)$ over $\Bbb Q$,"I'm asked to find the minimal polynomial over $\Bbb Q$ for $\mathrm{cos}(72°)$ : given the complex number $z=\mathrm{cos}(72°)+i\mathrm{sin}(72°)$ , one can say that $z^5=1$ . Since $$(a+bi)^5=a^5+5a^4bi-10a^3b^2-10a^2b^3i+5ab^4+b^5i$$ I concluded that $\mathrm{cos}(72°)^5-10\mathrm{cos}(72°)^3\mathrm{sin}(72°)^2+5\mathrm{cos}(72°)\mathrm{sin}(72°)^4=1$ , so if I replace $\mathrm{cos}(72°)$ with $x$ and $\mathrm{sin}(72°)$ with $\sqrt {1-x^2}$ , I obtain the polynomial $p(x)=16x^5-20x^3+5x-1$ . Now, it's clear that $1$ is a root, and dividing $p(x)$ by $x-1$ gives as result $p'(x)=16x^4+16x^3-4x^2-4x+1$ . However I don't know how to show that $p'(x)$ is irreducible, since I can't use Eisenstein and the non existence of rational roots doesn't imply the irriducibility. Thank you in advance","['trigonometry', 'abstract-algebra', 'factoring', 'minimal-polynomials']"
3276986,solve the equation in $\Bbb C $,\begin{array}{l}{\text {Solve in } \mathbb{C}}: \\ {x^{2}+\left(\frac{x}{x+1}\right)^{2}=3} \\ {\text { my try: }} \\ {x^{2}(x+1)^{2}+x^{2}=3(x+1)^{2}} \\ {x^{2}\left(x^{2}+2 x+1\right)-3\left(x^{2}+2 x+1\right)+x^{2}=0} \\ ({x^{2}-3 )\left(x^{2}+2 x+1\right)+x^{2}=0} \\ {x^{4}+2 x^{3}-x^{2}-6 x-3=0} \\ {\text { Now what should I do? }}\end{array},['algebra-precalculus']
3276988,Deceptively difficult coin weighing puzzles,"A coin weighing problem is a problem that looks something like this: You have twelve coins. Eleven of them weigh the same; one of them is either heavier or lighter than the other eleven. You want to figure out which one is the counterfeit, and whether it is heavier or lighter. The only way you can measure the difference between coins is by using a balance, putting some number of coins on one side, and some on the other. What is the minimal number of weighings you need, and what strategy achieves this minimal number? For this puzzle, it turns out that only three weighings suffice! I won't reproduce the strategy here. Proving that three weighings is also optimal is then easy: there are 24 possible configurations (each of the 12 coins could be the odd one out, and it could be lighter or heavier), and a single weighing only has three possible outcomes: left is heavier, right is heavier, or the scale is balanced. This means that with two weighings you can only distinguish $3^2 = 9$ configurations. There are many, many variations of coin weighing puzzles all over the internet. Very often, optimality of a strategy is proved by an argument like I gave above: showing that $$
\lceil \log_3(\text{number of configurations}) \rceil = \text{number of moves}.
$$ I am wondering if it is possible to create coin puzzles that are deceptively hard by that measure: where there are relatively few configurations, and yet solving the puzzle requires relatively many moves. For the purposes of giving this question some scope, I want to define a ""coin weighing problem"" as follows, although I am definitely also interested if it turns out a slight generalization gives a more interesting answer. A coin weighing problem consists of and integer $n \in \mathbb N$ , and a set $P \subseteq \mathbb N \times \mathbb N$ such that for all $(l, h) \in P$ we have $l + h < n$ . An $m$ -move solution to a coin weighing problem $(n, P)$ consists of a strategy for identifying the light, normal and heavy coins out of an $n$ -coin configuration, under the additional assumption that if $l, h$ are the number of light and heavy coins in our set, then $(l, h) \in P$ , such that in the worst case it requires at most $m$ weighings. We assume that a heavy and a light coin together weigh as much as two normal coins. In this formulation, our coin weighing problem above corresponds to $n = 12$ , $P = \{(0, 1), (1, 0)\}$ . My question: How big can we make the gap between the $3$ -log of the number of possible configurations of the coin problem and the optimal strategy? Edit: I realize now that, as formulated, there are unsolvable coin puzzles, such as $n = 2, P = \{(0,1), (1, 0)\}$ . Let me add the additional assumption that there are no indistinguishable configurations like the one I just wrote, and I will ponder on what a natural condition is to avoid indistinguishable configurations.","['recreational-mathematics', 'puzzle', 'combinatorics']"
3276990,Is this proof for inverse transform method valid for any pdf?,"This is from a uni course that includes a chapter about simulation, and introduces this method for random number generation. While the theorem is about a general $f(x)$ (probability density function), it seems to me the proof assumes an uniform distribution, and therefore it's valid only for that. Am I wrong? Is this particular proof valid for any probability density function? Theorem and proof:","['random', 'statistics', 'proof-verification', 'simulation']"
3276998,Some property of differentiable function,"Let $f:[0,2]\to\mathbb{R}$ be a continuous function and $f$ is differentiable on $(0,2)$ , and let $f(0)=f(2)=0$ . Now, suppose that there is a point $c\in(0,2)$ such that $f(c)=1$ . Then, there is a point $k\in(0,2)$ such that $\vert f'(k)\vert>1$ . Intuitively, it is pretty trivial. But, I can't find any way to prove it. How to prove it using the M.V.T. or another popular theorem? Give some idea or advice. Thank you!","['rolles-theorem', 'derivatives', 'real-analysis']"
3277024,What is the least common factor in equation $\frac{5}{x+4}=4+\frac{3}{x-2}$,"I am attempting to solve for x $\frac{5}{x+4}=4+\frac{3}{x-2}$ I know that I need to find the least common denominator. In this case, since I cannot see a clear relationship among them all I think it's just the product of all 3 denominators: $\frac{5}{x+4}=4+\frac{3}{x-2}$ = $\frac{5}{x+4}=\frac{4}{1}+\frac{3}{x-2}$ LCD: $(x+4)(1)(x-2)$ = $(x+4)(x-2)$ Is this the LCD? Because I tried to use this in solving my equation but I arrived at a quadratic. I don't think that my textbook wants me to use quadratics in this section but I'm not sure. Here's how I arrived at that: $\frac{5}{x+4}=\frac{4}{1}+\frac{3}{x-2}$ $(x+4)(x-2)\frac{5}{x+4}=(x+4)(x-2)\frac{4}{1}+(x+4)(x-2)\frac{3}{x-2}$ Then cancel out common factors: $(x-2)5=(x+4)(x-2)(4)+(x+4)(3)$ $5x-10=(x+4)(x-2)(4)+3x+12$ And if I multiple out the middle term I'll get a polynomial, which is unexpected so I'm not sure I'm on the right path here... am I? $5x-10=4x^2-4x-8+3x+12$ $5x-10=4x^2-x+4$","['fractions', 'algebra-precalculus']"
3277059,How to prove the formula for the joint PDF of two transformed jointly continuous random variables?,"In my notes, there is a theorem stated (see below), without proof. I have no idea how to prove this, because I've only just started learning multivariable calculus, and therefore lack a strong enough understanding of multivariable calculus at this point in time. Furthermore, I cannot find a proof anywhere online. So I was wondering if anyone knows how to prove this result, and explain each step? Or failing that, what are the prerequisites for proving this result?","['probability-distributions', 'jacobian', 'multivariable-calculus', 'transformation', 'probability']"
3277080,"Number of maximal ideals of $F_q[x_1,...,x_n]$","I am currently studying commutative algebra and came across the following question. Let $F$ be a finite field with $q$ elements, let $A=F[x_1,...,x_n]$ and denote by $m$ a maximal ideal in $A$ . How many maximal ideals are in $A$ such that $A/m = F$ ? How many maximal ideals are in $A$ such that $A/m = L$ , where $|L| = q^k$ ? How many maximal ideals are in $A$ ? I know that maximal ideals of $F[x_1,...,x_n]$ , where $F$ is an algebraically closed field are of the form $(x-a_1,...,x-a_n)$ , but how does a maximal ideal looks like in that kind of a situation?","['finite-fields', 'maximal-and-prime-ideals', 'algebraic-geometry', 'polynomial-rings', 'commutative-algebra']"
3277092,"If $f : \mathbb{R}^n \to \Bbb R$ is monotone over all lines, can it be written as $h \circ l$ where $h$ is monotone and $l$ is a linear form?","Let be $f : \Bbb R^n \to \Bbb R$ monotone over all lines (not affine ones, but if there is an answer over affine lines, I'm interested.) Is it possible to find $h : \mathbb{R} \to \Bbb R$ monotone and $l : \Bbb R^n \to \Bbb R$ linear so that $f = h \circ l$ ? I tried to look by supposing I have such a factorization, and as $\ker l$ is a hyperplane, I have $n - 1$ lines where $f$ is constant.
I tried to use the monotonicity condition by trying to compare $f(0)$ and over lines, but it didn't work.","['linear-algebra', 'monotone-functions', 'real-analysis']"
3277101,Is every norm for real matrices extensible to a norm for complex matrices?,"I was browsing this website haphardly and hit upon this question: Do we have for all $M \in SL_n(\Bbb K)$, $\lVert M \rVert \geq 1$ when $\lVert \cdot \rVert$ is a matrix norm? . It suddenly dawned on me that all submultiplicative matrix norms I have ever seen are actually defined for complex matrices. None of them is defined specifically for real matrices. Is it because every submultiplicative norm for real matrices is extensible to the complex case? More specifically: Suppose $\|\cdot\|_r$ is a submultiplicative matrix norm on $M_n(\mathbb R)$ . (The subscript $r$ is just a notation; it is not a number and it doesn't signify any specific matrix norm.) Does there always exist a submultiplicative norm $\|\cdot\|_c$ on $M_n(\mathbb C)$ such that $\|A\|_r=\|A\|_c$ for every $A\in M_n(\mathbb R)$ ? If homogenity and submultiplicativity are not required, we can pick a vector norm on $\mathbb R^2$ and define $\|X+iY\|_c=\|(\|X\|_r,\|Y\|_r)\|$ . However, I don't see any way to implement the homogenity condition $\|(a+ib)(X+iY)\|_c=|a+ib|\|X+iY\|_c$ , not to mention submultiplicativity. I have also considered the case where there exists a submultiplicative norm $\|\cdot\|_R$ on $M_{2n}(\mathbb R)$ such that $$\left\|\pmatrix{A&0\\ 0&A}\right\|_R=\|A\|_r$$ for every $A\in M_n(\mathbb R)$ . My first thought was to define $$\|X+iY\|_c=\left\|\pmatrix{X&-Y\\ Y&X}\right\|_R.$$ If such a norm $\|\cdot\|_R$ does exist, then $\|\cdot\|_c$ is automatically submultiplicative. However, I don't see any reason why $\|\cdot\|_R$ should exist and why the homogenity condition $$\left\|\pmatrix{aI&-bI\\ bI&aI}\pmatrix{X&-Y\\ Y&X}\right\|_R=|a+ib|\left\|\pmatrix{X&-Y\\ Y&X}\right\|_R.$$ is satisfied.","['matrices', 'normed-spaces', 'linear-algebra', 'analysis']"
3277124,Simplifying $\sqrt\frac{\left(a^2\cos^2t+b^2\sin^2t\right)^3}{\left(b^2\cos^2t+a^2\sin^2t\right)^3}$,"I am looking to simplify these term   [ I forgot the 3 :( ] $$\sqrt\frac{\left(a^2\cos^2t+b^2\sin^2t\right)^3}{\left(b^2\cos^2t+a^2\sin^2t\right)^3}$$ where $a$ and $b$ are two non-negative reals. (This is not homework. I am just trying to make my expression easy, but I didn't find a way.) Thanks for your help.","['algebra-precalculus', 'trigonometry']"
3277140,Image of quasiprojective variety under closed map,"Let $f: X\to Y$ be a regular map of projective varieties that is closed (in the sense that it takes Zariski closed sets to Zariski closed sets).  Let $V\subset X$ be a quasiprojective subvariety (i.e. locally closed and irreducible). Is $f(V)$ a quasiprojective subvariety of $Y$ ? (I'm aware that under arbitrary regular maps, the image of quasiprojective need only be constructible, but I am assuming the map is closed). I am also interested in the more general question in the topological category - see Difference of closed sets under closed map . UPDATE: This has been resolved at https://mathoverflow.net/questions/335512/image-of-quasiprojective-variety-under-closed-map , where a counterexample of a blow-down is given.",['algebraic-geometry']
3277149,Compact hypersurface [duplicate],"This question already has answers here : Prove that Gauss map on M is surjective (4 answers) Closed 5 years ago . I want to show that for some compact hypersurface $M$ in $\mathbb{R}$ it holds that for any $v \in S^n$ there exists some $p \in M$ , s.t. $v$ is the unit normal to $T_pM$ . I also have the hint to look at the hyperplanes $Rv+v^{\perp}$ for some $R>0$ , s.t. this hyperplane and $M$ are disjoint and then to look at the point with minimal distance to $Rv+v^{\perp}$ . Now I can picture this (for $n \leq 3$ obviously) because then $Rv+v^{\perp}$ is some plane $\cong \mathbb{R}^2$ and if I take the point with minimal distance its obvious (graphically) that its tangent space has to be orthogonal to $v$ . But I don't know how to formalize this. I can get the point with minimal distance to $Rv+v^{\perp}$ by looking at the smooth function $f:M \rightarrow \mathbb{R}, q \mapsto dist(q,Rv+v^{\perp})$ which has a minimum since $M$ is compact. Can somebody give me some hint how to formalize this (for all dimensions)?
Thanks!","['tangent-spaces', 'riemannian-geometry', 'differential-geometry']"
3277160,Classification of Subgroups of Multiplicative Group of Complex Numbers,"It is easy to show (using an argument involving $\lvert z\rvert$ ) that the finite subgroups of $\mathbb{C}^\times$ are exactly $\mathbb{Z}/n\mathbb{Z}$ for all $n\in \mathbb{N}$ embedded  as the $n^{th}$ roots of unity. A more interesting question is: can we classify all subgroups of $\mathbb{C}^\times$ ? The finite case has been taken care of. Here is a tentative list: $(1)$ The finite subgroups embedded as $\mathbb{Z}/n\mathbb{Z}$ . $(2)$ ${S}^1\hookrightarrow \mathbb{C}^\times$ as the unit circle. $(3)$ $\mathbb{Q}^\times$ , $\mathbb{R}^\times$ and actually $K^\times$ for any intermediate field extension $\mathbb{Q}\subseteq K\subseteq \mathbb{C}$ . $(4)$ $\mathbb{R}_{>0}$ , $\mathbb{Q}_{>0}$ , $K_{>0}$ for $\mathbb{Q}\subseteq K\subseteq \mathbb{R}$ . $(5)$ $S^1\times K_{>0}$ embedded as $\{Re^{i\theta}: R\in K_{>0}, \theta \in [0,2\pi]\}$ , whenever $K_{>0}$ is as in $(4)$ . This one might deserve some justification: If $x_0=R_0 e^{i\theta_0}$ and $x_1=R_1e^{i\theta_1}$ are given for $R_0,R_1\in K_{>0}$ and $\theta_0,\theta_1\in [0,2\pi]$ then $x_0x_1=R_0R_1e^{i(\theta_0+\theta_1)}$ which is also in $S^1\times K_{>0}$ . Inverses follow similarly, by $R_0\in K_{>0}$ implying $R_0^{-1}\in K_{>0}$ . Of course, $1$ is in the set. (6) $\mathbb{Z}/n\mathbb{Z}\times K_{>0}$ for $K_{>0}$ as in $(4)$ and $\mathbb{Z}/n\mathbb{Z}\hookrightarrow \mathbb{C}^\times$ as the $n^{th}$ roots of unity. Is this list exhaustive? (I doubt  it.) I haven't thought too much about this, because I'm busy with other math things. However, if the answer to this is known I would appreciate a reference (an explanation would be great too, if this is actually easy).","['group-theory', 'abstract-algebra']"
3277190,"Give an example of function continuous for $x \in (a, b)$ and not differentiable in $a < x_{1} <\dots < x_{n} < b $, but differentiable in intervals.","Give an example of a function continuous for x $\in (a, b)$ and not differentiable
in $a < x_{1}< \dots < x_{n} < b $ , but differentiable
in intervals $(a, x_{1}), (x_{n}, b), (x_{i}, x_{i+1})$ for $i = 1,
 \ldots, n-1 $ My idea: $$
f(x)=
\begin{cases}
\frac{x-a}{x_{1}-a} & x \in  [a,x_{1}] \\
-\frac{(x-x_{n-1})}{(x_{n}-x_{n-1})} + 1  &  x \in  [x_{n-1}, x_{n}] \text{ and  n odd} \\
\frac{x-x_{n-1}}{x_{n}-x_{n-1}}  & x \in  [x_{n-1}, x_{n}]\text{ and n even }\\
\frac{x-x_{n}}{x_{n}-b} & x \in [x_{n},b]
\end{cases}
$$","['analysis', 'real-analysis']"
3277216,References related to group actions and ergodic theory,"this is a reference request question. I have recently developed interest in operator algebras and it has come to my attention that there is plenty of ongoing research at the intersection of that field with ergodic theory and group actions. I would like to have a good look at all that, however, I have only elementary knowledge of group theory and zero knowledge of ergodic theory and dynamical systems in general. Could anyone help me develop a ""study path"" towards that area?","['group-theory', 'ergodic-theory', 'reference-request']"
3277309,Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio?,"In the book Thomas's Calculus (11th edition) it is mentioned (Section 3.8 pg 225) that the derivative $\frac{\textrm{d}y}{\textrm{d}x}$ is not a ratio. Couldn't it be interpreted as a ratio, because according to the formula $\textrm{d}y = f'(x)\textrm{d}x$ we are able to plug in values for $\textrm{d}x$ and calculate a $\textrm{d}y$ (differential). Then if we rearrange we get $\frac{\textrm{d}y}{\textrm{d}x}$ which could be seen as a ratio. I wonder if the author say this because $\mbox{d}x$ is an independent
variable, and $\textrm{d}y$ is a dependent variable, for $\frac{\textrm{d}y}{\textrm{d}x}$ to be a ratio both variables need to be independent.. maybe?","['calculus', 'math-history', 'nonstandard-analysis', 'analysis']"
3277327,Does every set have a set that is bigger? [duplicate],"This question already has answers here : Is every set a subset? (3 answers) Closed 5 years ago . Is it true that for every set $A$ , there exist set $B$ , such that $A \subset B$ , where $B$ is bigger (has at least one element more) than $A$ ? What about infinite sets?",['elementary-set-theory']
3277383,Prove $\sum_{n=1}^{\infty}\frac{\Gamma(n+\frac{1}{2})}{(2n+1)(2n+2)(n-1)!}=\frac{(4-π)\sqrt{\pi}}{4}$,"Prove $$S=\sum_{n=1}^{\infty}\frac{\Gamma(n+\frac{1}{2})}{(2n+1)(2n+2)(n-1)!}=\frac{(4-π)\sqrt{\pi}}{4}.$$ I don't know how to evaluate this problem .
At first I used partial fraction but I got divergent series, so I used $$\Gamma(n)=\int_0^{+\infty}t^{n-1}e^{-t}dt.$$ This yields $$S = \sum_{n=1}^{\infty}\frac{\int_0^{+\infty}t^{n-1}e^{-t}dt}{(2n+1)(2n+2)(n-1)!}$$ Now i can exchange integral and sum.
But I don't know how to proceed.","['integration', 'summation', 'closed-form', 'gamma-function']"
3277388,PDF of sum of exponentials conditioned on equality.,"I am trying to solve the following exam practice problem: Let $X_1, X_2$ be independent exponential random variables with
  parameter $1$ . Find the conditional PDF of $X_1+X_2$ given that $\frac{X_1}{X_2}=1.$ Find the conditional PDF of $X_1+X_2$ given that $X_1-X_2=0.$ The events $\frac{X_1}{X_2}=1$ and $X_1-X_2=0$ are the same. Does this mean that conditioning on either of these two events should give
  the same answer? Here is my approach: For part 1, I computed the joint PDF of $Y= X_1+X_2$ and $Z=\frac{X_1}{X_2}$ and my result was $$f_{Y,Z}(y,z) = \frac{ye^{-y}}{(z+1)^2}\mathbf{1}_{\{y\geq 0, z \geq 0\}}.$$ This shows that $Y$ and $Z$ are independent. So, conditioned on $Z=1$ , we get that $$f_{Y|Z}(y|1) = ye^{-y}\mathbf{1}_{\{y\geq 0\}}.$$ For part 2, I computed the joint PDF of $U=X_1+X_2$ and $V=X_1-X_2$ and got $$f_{U,V}(u,v) = \frac{1}{2}e^{-u}\mathbf{1}_{\{u\geq |v|\}}.$$ I then used Bayes' theorem to get that $$f_{U|V}(u|0) = e^{-u}\mathbf{1}_{\{u\geq 0\}}.$$ My question regards part 3. Firt of all, part 3 states that the events $X_1-X_2=0$ and $\frac{X_1}{X_2}=1$ are the same. But, I disagree because the event $X_1-X_2=0$ contains the measure-zero event $X_1=X_2=0$ , which is not contained in the event $\frac{X_1}{X_2}=1$ . Is this not the case? Second of all, if these two events were indeed the same, then why would conditioning on them produce a different result?","['conditional-probability', 'probability']"
3277455,Compute the following sum in closed form : $\sum_{n=1}^{\infty}\frac{n\binom{2n}{n}}{4^{n}(2n+1)(2n-1)(4n+1)}$,$$\text{Find : }\sum_{n=1}^{\infty}\frac{n\binom{2n}{n}}{4^{n}(2n+1)(2n-1)(4n+1)}$$ I know that $\displaystyle\sum_{n=0}^{\infty}\binom{2n}{n}x^{2n}=\frac{1}{\sqrt{1-4x^{2}}}$ so $\displaystyle\sum_{n=1}^{\infty}n\binom{2n}{n}x^{2n}=\frac{2x^2}{\sqrt{1-4x^{2}}}$ . But I don't know he to complete this work because I find hypergeometric function.,"['integration', 'closed-form', 'sequences-and-series']"
3277485,Compute the integral $\int_0^1 \frac{x^4}{\sqrt{x(1-x)}}dx$ using residue,"I'm trying to compute the following integral: $$
\int_{0}^{1}\frac{x^{4}}{\,\sqrt{\,{x\left(\,{1-x}\,\right)}\,}\,}\,\mathrm{d}x
$$ So what I am try to do is using a dog bone contour, let me call D, so in the bottom part I have the negative sign, but the orientation is invert in consideration to the upper part, then because the only singularity that I have is on  x = $\infty$ I have that $$
\!\!\int_{D}f(z)\,dz = \!2\!\int_{0}^{1}\!\!\!f(x)dx
+ \left(\int_{\gamma_1}\!\! +\!\!\int_{\gamma_2}\right)
\!\!f(\xi)d\xi = 2\pi i\, \mbox{Res}(f,\infty)
$$ and then calculating the residue at infinity I have found this $3 \pi/8$ , but the answer is actually $35 \pi/128$ . Can someone help me figure what to do?","['complex-analysis', 'contour-integration']"
3277492,Bayes Estimator under $L_{\eta}$,"I am wondering if the following loss function is well known and if it is, does it have a standard name: $$
L_{\eta} (\theta, a) = (\theta-a) (\eta - \mathbb{I}_{(-\infty, a)} (\theta) ), \quad \eta \in (0,1).
$$ where $\eta$ is fixed. For the following problem:
For a single observation $x \sim \text{Uniform}(0, \theta)$ , with prior on $\theta$ being $\tau(\theta) = \theta e^{-\theta} \mathbb{I}_{(0, \infty)}(\theta)$ , we get the following posterior distribution: \begin{align*}
    h_{\Theta|X}(\theta|x) = e^{x-\theta} \mathbb{I}_{(x, \infty)}(\theta).
\end{align*} I am trying to compute the bayes estimator, $\hat{\theta}$ , under $L_{\eta}$ . I am wondering if my answer is correct, here is my working: We wish to find $\hat{\theta}$ that minimises the expected posterior loss: \begin{align*}
    \mathbb{E}[L_{\theta}(\theta, \hat{\theta}) | X] &= \int_{-\infty}^{\infty} (\theta - \hat{\theta}) (\eta - \mathbb{I}_{(-\infty, \hat{\theta})}(\theta)) h(\theta|x) d \theta \\
    &=(\eta -1)\int_{-\infty}^{\hat{\theta}} (\theta - \hat{\theta})  h(\theta|x) d \theta  + \eta \int_{\hat{\theta}}^{\infty} (\theta - \hat{\theta})  h(\theta|x) d \theta.
\end{align*} Using Leibniz rule to differentiate this wrt $\hat{\theta}$ and setting to zero (and omitting some minor details), we get \begin{align*}
    - (\eta -1) \int_{-\infty}^{\hat{\theta}} h(\theta|x) d \theta  - \eta \int_{\hat{\theta}}^{\infty}  h(\theta|x) d \theta = 0.
\end{align*} Solving for $\hat{\theta}$ gives \begin{align*}
    & -(\eta -1)\int_{-\infty}^{\hat{\theta}}  e^{x-\theta} \mathbb{I}_{(x, \infty)}(\theta) d \theta  - \eta \int_{\hat{\theta}}^{\infty}  e^{x-\theta} \mathbb{I}_{(x, \infty)}(\theta) d \theta = 0\\
    \implies & -(\eta -1) e^{x} \int_{\min \{ \hat{\theta}, x \} }^{\hat{\theta}} e^{-\theta} d \theta  - \eta e^{x} \int_{\max\{\hat{\theta}, x\}}^{\infty}  e^{-\theta}  d \theta = 0\\
    \implies & -(\eta -1) e^{x} \int_{ x }^{\hat{\theta}} e^{-\theta} d \theta  - \eta e^{x} \int_{\hat{\theta}}^{\infty}  e^{-\theta} d \theta = 0\\
    \implies & -(\eta -1) (1-e^{x- \hat{\theta}}) - \eta e^{x - \hat{\theta}} = 0\\
    \implies & \hat{\theta}= x-\log(1-\eta).
\end{align*} Note that we have made the assumption that $\hat{\theta} \ge x$ , so that \begin{align*}
    \min \{\hat{\theta}, x \} = x, \quad \max \{\hat{\theta}, x \} = \hat{\theta},
\end{align*}","['statistical-inference', 'statistics', 'bayesian', 'machine-learning', 'probability']"
3277515,Morphisms of constant rank in algebraic geometry,"There is a somewhat loose dictionary between algebro-geometric notions and differential-topological ones. especially if we restrain to the domain of smooth algebraic varieties. We have notions of a closed embedding of smooth varieties and smooth/etale morphisms of smooth varieties, which might be thought of as counterparts of smooth embeddings and submersions/local diffeomorphisms in the smooth category. Now is there an analogue of the notion of a map of constant rank, that is such a morphism for which the differential has constant rank at every point? A nice consequence in differential topology is that then the fibers of this map are smooth submanifolds. In algebraic geometry I am only aware of theorems which assume submersivity to guarantee smooth fibers. And if there is no such analogues, what is the reason for this difference between smooth and algebraic categories?","['differential-topology', 'algebraic-geometry']"
3277521,Non-parametric confidence interval (Chebyshev inequality),"I want to solve the following exercise: Given is an expected value of random variable X, E(X) = 200 at time
point t and a standard deviation of 50. (a) Find the 95% confidence interval of the random variable
at this time point t. (b) What is the probability that the value of the Random Variable is at least 300? There is no probability distribution and no further data given. My approach: As one of the comments points out, I am using Chebyshev's theorem: $$P(|X-\mu|\ge k\sigma)\le\frac{1}{k^2}$$ I find the following results - are these correct? (a) For $1 - \frac{1}{k^2}$ = 0.95 I find k = 4.4721 and hence with $k\sigma-\mu \leq X \leq k\sigma+\mu$ I find the lower bound of 23.6 and the upper bound of 423.6068 for the 95% confidence interval. (b) Not quite sure here, but my approach would be using $$|X-200|\ge 100$$ with $\sigma = 50$ we get $\frac{1}{2^2}$ and hence
a probability of 25% for a value of X being at least 300.
Is that correct?","['statistics', 'confidence-interval', 'probability']"
3277546,Why use the derivative and not the symmetric derivative?,"The symmetric derivative is always equal to the regular derivative when it exists, and still isn't defined for jump discontinuities. From what I can tell the only differences are that a symmetric derivative will give the 'expected slope' for removable discontinuities, and the average slope at cusps. These seem like extremely reasonable quantities to work with (especially the former), so I'm wondering why the 'typical' derivative isn't taken to be this one. What advantage is there to taking $\lim\limits_{h\to0}\frac{f(x+h)-f(x)} h$ as the main quantity of interest instead? Why would we want to use the one that's defined less often?","['calculus', 'functions', 'derivatives']"
3277554,Partial derivative of integral of multi variable function,"This is the problem I am working on. Here, I find it hard to calculate $\nabla{\times}\mathbf{H}$ . For example, to calculate $(\frac{\partial H_z}{\partial y}-\frac{\partial H_y}{\partial z})\mathbf{i}$ , I am not sure how to continue after $$\frac{\partial H_y}{\partial z}=\frac{\partial} {\partial z}\left({\int_{x_0}^x \! G_z(x’, y, z) \, \mathrm{d}x’}\right)$$ . I think the core problem here is that $x$ is taken as variable in the integral while $z$ was taken as variable in the partial derivative. What should I do to solve it? Thanks.","['partial-derivative', 'multivariable-calculus']"
3277597,Primary school competition problem find the area of a square,"Someone posted online a primary school competition problem. I solved it using the Cayley-Menger determinant (finding the answer is 169/2 after getting the length of AD being 13) but it probably isn't the intended solution. I cannot find a ""primary school"" way to solve it. The problem is to find the area of the square given the following: The angles $\angle ABC=90^o,\angle BCD=90^o$ .","['contest-math', 'euclidean-geometry', 'area', 'geometry']"
3277635,Are Hessian matrices always symmetric? [duplicate],"This question already has answers here : Are there any Hessian matrices that are asymmetric on a large set? (2 answers) Closed 3 years ago . The objective function of interest is: $$
\phi = \text{log}|PWP^T| + \text{tr}((PWP^T)^{-1}PVP^T)
$$ where $P = J + XU^T$ and $V$ , $J$ and $U^T$ are known matrices. I assume that the $V$ and $W$ are positive definite. The first partial derivative of $\phi$ with respect to $X$ is \begin{align*}
Y^{-1}(JWU + JVU) + Y^{-1}X(U^TWU + U^TVU) - Y^{-1}ZY^{-1}(JWU + XU^TWU) 
\end{align*} where $Y = PWP^{T}$ and $Z = PVP^T$ . The first partial derivative with respect to $W$ gives $$
PWP^T = PVP^T.
$$ Hence combining the solutions of partial derivatives $X = -JVU(U^TVU)^{-1}$ . If I compute $\nabla_{xx}\phi$ and evaluate at the solution what I would get is $$
[(U^TWUX^T + U^TWJ^T)Y^{-1} \otimes Y^{-1}](I + K)[(U^TWUX^T + U^TWJ^T)^T \otimes I] + (U^TVU \otimes Y^{-1})
$$ where $K$ is the commutation matrix. However, I can't see that it is symmetric. If this is not symmetric, Hessian would not be symmetric. Is it always the case that the Hessian needs to be symmetric?","['matrices', 'matrix-calculus', 'linear-algebra', 'optimization', 'hessian-matrix']"
3277690,A closed form for: $\int_{0}^{\infty} \frac{1}{(x-\log x)^2}dx$,"Is it possible to find a closed-form expression for this integral? $$\int_{0}^{\infty} \frac{1}{(x-\log x)^2}dx$$ Generalization of the Integral: $$\int_{0}^{\infty} \frac{1}{(x-\log x)^{p}}dx$$ where, $\log x$ is a natural logarithm, $p\in\mathbb{Z^{+}}_{≥2}$ The indefinite integral can not be expressed by elementary mathematical functions according to Wolfram Alpha. I can add a visual plot. So, I dont know, is it possible to find a closed-form or not. But, I have a numerical solution: $$\int_{0}^{\infty} \frac{1}{(x-\log x)^2}dx≈2.51792$$","['integration', 'definite-integrals', 'calculus', 'closed-form', 'problem-solving']"
3277722,Prove $\operatorname{ div} (\phi A) = (\operatorname{ grad} \phi) \cdot A + \phi \operatorname{ div} A$,"Prove $$\operatorname{ div} (\phi A) = (\operatorname{grad} \phi) \cdot A + \phi \operatorname{ div} A$$ where $A$ is differentiable vector function and $\phi $ is differentiable scalar function. Now i write $A =(A_1. A_2. A_3)$ and so i write as $ div (\phi A) = (f_x,f_y,f_z) \cdot (\phi A_1,\phi A_3,\phi A_3)$ which equals to $f_x(\phi A_1) + f_y(\phi A_2) + f_z(\phi A_3)  $ . how do i proceed? thanks","['multivariable-calculus', 'vector-analysis']"
3277723,For which of the following functions $f$ is $f(x)=f(2-x)$ for all $x$?,"The options are: $A)$ $f(x) = x(x+2)$ $B)$ $f(x) = x-2$ $C)$ $f(x) = 2-x$ $D)$ $f(x) = 3x(x-2)$ $E)$ $f(x) = x^2(2-x)^2$ The answer given in my worksheet is $E)$ and I got it. My doubt is why it can't be $D)$ also? Solving thus: $f(x) = 3x(x-2)$ $f(2-x) = 3(2-x)(2-x-2) = 3(2-x)(-x) = 3[-(2-x)](x) = 3(x-2)x = 3x(x-2) = f(x)$ If I substitute some random values for $x$ , then it works as well. Example $1$ : $x = 3 \rightarrow f(x) = f(3) = 3(3)(3-2) = 9$ $f(2-x) = f(2-3) = f(-1) = 3(-1)(-1-2) = 9$ Example $2$ : $x = -1/2 \rightarrow f(x) = f(-1/2) = 3(-1/2)(-1/2-2) = 15/4$ $f(2-x) = f(2-(-1/2)) = f(5/2) = 3(5/2)(5/2-2) = 3(5/2)(1/2) = 15/4$ Thus, I think $D)$ should be the answer as well. Please correct me if I am missing something.",['functions']
3277803,Minimum time to reach B from A: Iran Math Olympiad 2001 [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question In this picture: a runner wants reach B by starting at A. Velocity in White space is $10 m/s$ and in brown space is $5 m/s$ . what is the minimum time that he need? 
I upload original image of question but it wrote in Farsi. Assume that Brown region is a an unbounded band along the $y$ axis. $$a)\sqrt{26}$$ $$b)\sqrt{20}$$ $$c)5$$ $$d)\sqrt{30}$$ $$e)\sqrt{34}$$","['contest-math', 'optimization', 'geometry']"
3277811,How do I find the function that is perfectly between y = x^2 and y = x?,"At first, I thought it was as simple as taking both functions adding them together then dividing by two, but this is not the case for what I am looking for. Here is a plot of the following: y = x
y = x^2
y = 1/2*(x^2+x)
points exactly in between y = x and y = x^2 As you can see the red line representing y = 1/2*(x^2+x) does not land on the green points which are exactly in between the two functions y = x and y = x^2. What I am trying to learn how to do is figure out how to find the function which represents the exact middle between the two equations y = x and y = x^2. I have already tried using an excel sheet to fit a line to all the green points I have calculated and still can't come up with a good line that fits. I have looked into calculating the midpoint between two given points and that only helped calculate the points between the two equations, and didn't help towards obtaining a function that perfectly represents the line between y = x and y = x^2. Thanks for any help or suggestions towards the right domain of math reserved for solving cases like this one. cheers!!",['functions']
3277816,The solution of the Euler-Bernoulli equation via separation of variables,"I'm trying to solve the Euler-Bernoulli equation for a beam for a construction with a solution in the form of $w(\zeta,t) = f(\zeta)g(t)$ (separation of variables). However, it is mentioned that this can still be tricky, but since the beam is undamped, we can assume $ g(t) = e^{i \lambda t}$ with $\lambda$ real. My first question is; why is the above choice logical to assume? (does this have anything to do with $e^{i \lambda t} = cos(\lambda t) + i sin(\lambda t)$ for $\lambda \in \mathbb{R}$ ) I made a beginning with the separation of variable technique: The Euler-Bernoulli equation: $\rho\frac{\partial^{2}w}{\partial t^{2}}(\zeta,t) = -EI\frac{\partial^{4}w}{\partial \zeta^{4}}(\zeta,t),\text{ }\text{ }\text{ }\zeta \in [0,1], t \geq 0$ With boundary conditions: $w(0,t) = w(1,t), \frac{\partial w}{\partial \zeta}(0,t) = \frac{\partial w}{\partial \zeta}(1,t)$ Substituting $w(\zeta,t) = f(\zeta)g(t)$ in the Euler-Bernoulli equation gives: $\rho\frac{\partial^{2}f(\zeta)g(t)}{\partial t^{2}}(\zeta,t) = -EI\frac{\partial^{4}f(\zeta)g(t)}{\partial \zeta^{4}}(\zeta,t)$ $f(\zeta)\rho\frac{\partial^{2}g(t)}{\partial t^{2}}(\zeta,t) = -EIg(t)\frac{\partial^{4}f(\zeta)}{\partial \zeta^{4}}(\zeta,t)$ Since space and time are in this case independent of each other, both sides have to be equal to the same constant - $\lambda$ . This gives us a set of three qualities: $
1. \frac{d^2g(t)}{dt^2} = -g(t)\lambda \\
2. \frac{d^4f(\zeta)}{d\zeta^4} = \rho f(t)\frac{\lambda}{EI} \\
3. w(0,t) = w(1,t) \rightarrow f(0) = f(1)
$ The general solution of g(t) (if positive) is: $
g(t) = C_{1} * e^{-i \sqrt{\lambda}t} + C_{2} * e^{i \sqrt{\lambda}t}
$ Using Euler's formula, g(t) becomes: $
g(t) = C_1 cos(\sqrt{\lambda}t) + C_2 sin(\sqrt{\lambda}t)
$ From here, I am pretty much stuck to find a proposed solution for $f(\zeta)$ and to determine the solution to this problem. The second question now is how to determine a proposed solution for $f(\zeta)$ and how to determine the 3 lowest (strictly positive) $\lambda$ 's such that $w(\zeta,t) = f(\zeta)e^{i\lambda t}$ is a non-zero solution of the Euler-Bernoulli equation? Thank you for all of your help.","['multivariable-calculus', 'calculus', 'partial-differential-equations', 'partial-derivative', 'boundary-value-problem']"
3277822,Relation between covariant derivative and differential,"Let $\frac{\hat{D}}{dt}$ be the covariant derivative via the Levi-Civita connection. For the definition of this, my reference is Lee's ""Riemannian manifolds"", Lemma 4.9. For the present context, this says that: for some vector field $X_t$ along a curve $u(t)$ , the covariant derivative of $X_t$ is $\frac{\hat{D}}{dt} X_t = \nabla_{\dot{u}(t)}X(t)$ , where $\nabla$ in our case is the Levi-Civita connection. Furthermore, let $V$ be a vector space, considered as a manifold. Short version of the question Given some smooth curve $p(t)$ in V , can we make sense of $\frac{\hat{D}}{dt} p(t)$ , and does it hold that $\frac{\hat{D}}{dt} p(t) = \frac{d}{dt} p(t)$ ? I know that the tangent spaces of a vector space can be identified with the space itself. Longer version I'm trying to understand the following argument (from this paper). Let $e_{\epsilon}(t)$ be a curve in a Lie group, considered either as a function of $\epsilon >0$ or $t > 0$ . This curve fulfills $e_{0}(t) = e$ (the exact definition of the curve, from a differential equation, can be found on page 5 in the paper). Let $\frac{\hat{D}}{dt}$ be the covariant derivative via the Levi-Civita connection. In the paper, it is then established that $$\frac{\hat{D}}{dt} \frac{d}{d\epsilon} e_{\epsilon}(t) = T_e L_{e_{\epsilon}(t)}\dot{v}(t) + \epsilon \frac{\hat{D}}{d\epsilon}(T_e L_{e_{\epsilon}(t)}\dot{v}(t)),$$ which implies that $$ \left.\frac{\hat{D}}{dt} \left(\frac{d}{d\epsilon} e_{\epsilon}(t) \right|_{\epsilon=0}\right) = \left(\left.\frac{\hat{D}}{dt} \frac{d}{d\epsilon} e_{\epsilon}(t)\right) \right|_{\epsilon=0} =\dot{v}(t).$$ The thing I don't understand is the following conclusion from the above; $$\left. \frac{d}{d t} \left(\frac{d}{d\epsilon} e_{\epsilon}(t) \right|_{\epsilon=0}\right) = \dot{v}(t).$$ My thoughts: since $\left.\frac{d}{d\epsilon} e_{\epsilon}(t) \right|_{\epsilon=0}$ is a curve in the Lie algebra, they seem to use that the covariant derivative reduces to the differential  if the input is a curve on a vector space (in this case the Lie algebra) - but I haven't been able to establish that fact. Thanks","['lie-algebras', 'lie-groups', 'differential-geometry']"
3277824,how do you differentiate $\ln(x)$ using the difference quotient. [duplicate],"This question already has answers here : Proof of the derivative of $\ln(x)$ (5 answers) Closed 5 years ago . So the limit is as $h$ approaches $0$ of $\displaystyle \frac{\ln(x+h)-\ln(x)}{h}$ , which simplifies to $\displaystyle\frac{\ln\left(\frac{x+h}{x}\right)}{h}$ , which simplifies to $\displaystyle\frac{\ln\left(1+ \frac hx\right)}{h}$ . I got stuck here.  , how should I continue?",['derivatives']
3277867,What is the soln to the differential equation : $x'''+x''+xx' = 0$?,"$$x'''+x''+xx' = 0, \quad x' = \frac{\mathrm{d}x}{\mathrm{d}t}.$$ Is there an analytic soln for this?
I am very new to numerical methods also, how do I solve this in python, if a numerical method is the only possibility?",['ordinary-differential-equations']
3277889,Euler characteristic for topological surfaces and triangulations,"I've been trying to understand the Euler characteristic of surfaces. Let's define the Euler characteristic of a (regular, closed) surface $S$ as $\chi(S)=V-E+F$ , where $V$ , $E$ and $F$ are, respectively, the number of vertices, edges and faces of a given triangulation of $S$ . Then we should prove this does not depend on the given triangulation. This is kind of straightforward, using induction if you will, if we can take common refinements of two triangulations. Of course we need to consider finite triangulations in order to $\chi(S)$ to be computable. I got to this question: Prove Euler characteristic is a homotopy invariant without using homology theory where a comment links to the Wikipedia article Hauptvermutung . This article's name is the conjecture that any two triangulations of a surface have a common refinement, and it states that ""The manifold version is true in dimensions $\displaystyle m\leq 3$ ."" So this is the problem: consider any geodesic triangle $T$ on a sphere $S$ . Then $T$ determines a triangulation of $S$ . Consider a new triangle $T'$ , obtained from $T$ by keeping two of its edges the same, but changing the other edge by a ''wave of increasing period'', i.e., something that resembles the graph of $\sin^2(1/x)e^{-1/x^2}$ on the interval $[0,1/2\pi]$ (which is even smooth). Then the finite triangulations determined by $T$ and $T'$ do not admit a common finite refinement, which seems to contradict the Wikipedia's article. Of course, there are other ways of proving invariance of $\chi$ by the chosen triangulation in different contexts, such as using Gauss-Bonnet in the case of reguar surfaces, or homology theory for CW complexes. But how would one go to prove that the Euler characteristic of a compact topological surface does not depend on the given triangulation (which seems to always exist by Theorem 6.2.8 of this book ? Or how could one define the Euler characteristic of arbitrary topological manifolds in better ways?","['general-topology', 'algebraic-topology']"
3277926,Does there exist an injective homomorphism $\phi : \Bbb G_a \to \Bbb G_m$?,"Let $k$ be an algebraically closed field. $\Bbb G_a$ is the Linear algebraic group with the underlying variety being $\Bbb A = k$ with the additive group operation . $\Bbb G_m$ is the Linear algebraic group with the underlying variety being $\Bbb A^*$ with the multiplicative group operation. The question is : Does there exist an injective homomorphism $\phi : \Bbb G_a \to \Bbb G_m$ ? My attempt : No! Let $\phi \in Hom(\Bbb G_a,\Bbb G_m)$ . Then that will induce a $k$ -algebra  homomorphism from the Coordinate ring of $\Bbb G_m$ to the Coordinate ring of $\Bbb G_a$ i.e. $\phi ^* : \frac{k[X,Y]}{(XY-1)} \to k[T]$ i.e. $\phi ^* :  k[X,X^{-1}] \to k[T]$ . $X$ is a unit in $ k[X,X^{-1}]$ , hence $\phi^*(X)$ must be a unit in $k[T]$ . But ${k[T]}^* = k^*$ thus $\phi^*(X)=c \text{ (say)}\in k^*$ , but that will fail the injectivity of $\phi^*$ since $c \in k[X,X^{-1}]$ and $\phi^*(c)=c$ . Are my arguments valid? Please point out mistakes if any.","['algebraic-geometry', 'proof-verification', 'algebraic-groups']"
3277956,"Proof - Set of all subsets of a finite set $\Omega$, $2^\Omega$, is also finite","As the title implies, I am asked to show that the set of all subsets of some finite set $\Omega$ , $2^\Omega$ , is also finite. However, I am not really sure if the proof that I've come up with is entirely correct. This is what I have: Since $\Omega$ is finite, then $\exists n \in \mathbb{Z}_+$ such that we can construct a bijection from the finite set of integers $\{x \in \mathbb{Z}_+ | x \leq n\}$ to every element of $\Omega$ . Furthermore, since $2^\Omega$ is the set of all subsets of $\Omega$ , then the total number of such sets is $q = 2^n < \infty$ . Therefore, we can construct a bijection from the finite set of integers $\{ x \in \mathbb{Z}_+ | x \leq q \}$ to every element of $2^\Omega$ . $\square$ I would appreciate any corrections or comments.","['elementary-set-theory', 'proof-verification']"
3277974,What is the number of positions of a 15 puzzle (4x4 board),"I wonder if someone can help me get a number for possible positions for this childs game of 15 sliding pieces in a 4x4 board. The normal game has numbers 1-15 that need to be sorted. The board I want to calculate has only three kind of pieces: 6 red, 6 green, 3 black, and of course the empty slot you can move through. I referred to this post and came up with this solution $\binom{16}{6}\binom{10}{6}\binom{4}{3}=6.726.720$ . Does that look correct, or did I miss something?","['combinations', 'combinatorics']"
3277979,Angle Between Two Vectors Facing A Point,"I need a mathematical algorithm for finding the angle, formed by three points, which is open toward a fourth point. For example, in Fig1 below I desire angle $\theta$ because it is ""facing"" point $P$ . However, the formula for the angle between two vectors, $$\theta=\cos^{-1}\Big(\frac{\vec{u}\cdot\vec{v}}{uv}\Big)$$ gives angle $\alpha$ because the above relation always returns the angle which is less than 180 degrees. (Fig1) In contrast, in Fig2 I desire angle $\alpha$ because this time it is ""facing"" point $P$ . (Fig2) As a final example, in Fig3 below I need angle $\theta$ , because it is still technically ""facing"" point $P$ . (Fig3) This problem can occur in any orientation, making it difficult to say, for example, ""If point $P$ is to the left of points $A, B, C$ , use the leftmost angle, otherwise use the right"" or something like that. Any help would be appreciated! My Attempted Solution If anyone's interested, my current solution is to split the angle facing the point $P$ in half and add those two together. In Fig4 below, I use the grey line to split the angle in half, find $s$ and $t$ using dot product, then add $s$ and $t$ to get the angle facing $P$ . (Fig4) This works until I reach a situation like that in Fig5 . What I need is $s+t$ , but what I get is $q+t$ because the dot product gives $q$ (since $s$ is greater than $180$ degrees). (Fig5) It's a conundrum.","['trigonometry', 'angle', 'vectors']"
3278013,if the indefinite integral of $x^x$ was $f(x)$ what would the indefinite integral of $x^{1/x}$ be in terms of $x$ and $f(x)$?,so what this means is if $f(x)$ is the indefinite integral of $x^x dx$ then what would the indefinite integral of $x^{1/x}$ be in terms of $x$ and $f(x)$,['integration']
3278018,Equivalence statement of locally compact,"Let $(X,\mathscr{T})$ be a topological space. Then the following are equivalent: $\forall x\in X,~\exists~\text{compact}~G\subseteq X,~x\in G^o$ $\forall x\in X,~\exists~\text{open set}~U\subseteq X,~x\in U\subseteq \overline{U}$ and $\overline{U}$ is compact If it were LCH, I know how to prove it. However, it is only locally compact. (The problem is came from our teacher's exercise.) I guess the equivalent is a mistake. Isn't it?",['general-topology']
3278055,"If a principal bundle has a global section, then it is the trivial bundle","Let be $(P, M, \pi, G)$ a principal G-bundle. Suppose that there is a section $\sigma: M \to P$ . I want to prove that $P \cong M \times G$ . I built $f: M\times G \to P$ such that $f(p, g) = \sigma(p) \cdot g$ where $\cdot$ is the right action on the bundle. I can prove that this is smooth and bijective, but I cannot prove that this is a diffeomorphism. My idea is to use the global rank theorem, but I can prove only that the map has constant rank o n each fiber , where the action is transitive. How can I prove that the rank is the same everywhere?","['principal-bundles', 'fiber-bundles', 'lie-groups', 'differential-geometry']"
3278072,Solve $ \lim_{x\to 0}\ (\sqrt {2x+1}\ -\ \sqrt[3]{1-3x})^{x}$ without using L'Hospital,"I need to solve $$ \lim_{x\to 0}\ (\sqrt {2x+1}\ -\ \sqrt[3]{1-3x})^{x}$$ Please note that I'm first year student and that this can be solved much simpler than in the answers. I tried doing $$\lim_{x\to 0} \ e^{x \cdot \ln\Bigl(\sqrt{2x+1}-1-\left(\sqrt[3]{1-3x}-1\right)\Bigr)}$$ then going with the limit inside the function like this $$\exp\left\{\lim_{x\to0}x \cdot
\ln\left[\lim_{x \to 0}\Bigl(\sqrt{2x+1}-1\Bigr) \cdot
\lim_{x \to 0} \left(1-
\frac{ \sqrt[3]{1-3x}-1\over x }{ \sqrt{2x+1}-1 \over x }\right)\right] \right\}$$ But problem is that although I can solve third limit this way, I get that second limit is 0, which makes that 0 is inside of $\ln$ and thus is incorrect attempt.  Please help, I'm new here, I wan't to contribute back and this is from my university math exam.","['limits', 'limits-without-lhopital']"
3278076,Find the area of the polygon in the figure,"In the triangle in the figure the area of the triangle $MBP$ is equal to $9$ , the area of the triangle $NPC$ is equal to $8$ and the area of the triangle $BCP$ is equal to $24$ . I need to figure out the area of the polygon $AMPN$ . Notice that $M$ and $N$ are NOT the midpoints of $AB$ and $AC$ . It seems to me we don't have enough elements to figure this out. Moreover, I've been trying to find similarities between the shown triangles but I couldn't find any.",['geometry']
3278149,"""well-known"" inequality for numerical radius of an operator [duplicate]","This question already has answers here : Upper bound for norm of Hilbert space operator (2 answers) Closed 5 years ago . Let $H$ be a Hilbert space. For $T\in\mathcal{B}(H)$ , define the numerical radius of $T$ as $\omega(T):=\displaystyle{\sup_{\|x\|=1}|\langle Tx,x\rangle|}$ . I am trying to prove that $\|T\|\leq2\omega(T)$ but I just don't see it. My text simply says that this is ""easy"" and all the literature online simply refer to this estimate as ""well-known"". I know that, for self-adjoint operators, it is $\|T\|=\omega(T)$ and it can be proven elementary. I thought that maybe writing $T=X+iY$ with $X,Y$ self-adjoint could help, but then I couldn't estimate the numerical radii of $X,Y$ related to $\omega(T)$ .","['operator-theory', 'functional-analysis']"
3278159,Global Sections of Blow Up,"Let $Y$ be a regular surface (therefore a proper, $2$ -dimensional scheme over base field $k$ ) and $y \in Y$ a rational closed point (so $k(y)=k$ ). Denote by $I$ the ideal sheaf corresponding to $y$ interpreted as closed subscheme. Performing the blowup at $y$ we obtain $b: X=Bl_y(Y):= Proj(\oplus_n I^n) \to Y$ . My question is how are the global sections $H^0(Y, O_Y), H^0(X, O_X)$ related to each other? Indeed higher cohomologies are depending only on derived image sheaf $R^1f_* O_X$ via five term Leray Serre sequence $$0 \to H^1(Y, f_*O_Y) \to H^1(X, O_X) \to H^0(Y, R^1f_* O_X) \to H^2(Y, f_*O_X) \to H^2(X, O_X) $$ The cruical point is what happens with global sections $H^0(-)$ . My motivatating example was the blowup $X:=Bl_{(0,0)}(\mathbb{A}^2_k)$ of affine surface $\mathbb{A}^2_k$ over basefield $k$ at $(0,0)$ (corresponds to max ideal $(x,y)$ ). Denote $R:=k[x,y), \mathfrak{a}=(x,y)$ . In order to avoid ambiguity we identify $\oplus_n I^n := R[\mathfrak{a} \cdot T]$ with bookkeeping undeterminant $T$ . Concrete calculations using Cech cohomology provide $H^0(Y, O_Y)=H^0(X, O_X)$ . So my question is under which conditions the blowing up of points preserve the property $H^0(Y, O_Y)=H^0(X, O_X)$ ? What are sufficient conditions and why? Is it neccessary that the point $y$ has to be regular or/ and rational? In what dimension are the blowups with this property performed? Are there recomendable sources which treat this question?","['algebraic-geometry', 'blowup']"
3278160,Moment Matching Distributions,"I have to estimate the coefficients $c_1,...,c_n$ of the relationship $$
Y = c_1X_1+c_2X_2+\cdots + c_nX_n
$$ So the distribution of $Y$ is given through a linear combination of other random variables. Further there is the fact that for any sample of $X_1,...,X_n$ it holds that $$
0\leq\sum_{i=1}^nX_i<1
$$ I have ways to generate samples for $Y,X_1,...,X_n$ , giving me a linear system of equations. One way to estimate the coefficients $c_1,...,c_n$ in that case would be through least squares. But I'd like to do better than that. Another alternative would be to use moment matching (with centralized moments), e.g., $$
E[(Y-E[Y])^m] = E[(c_1X_1+c_2X_2+\cdots + c_nX_n-E[c_1X_1+c_2X_2+\cdots + c_nX_n])^m]
$$ which would add further equations for higher moments with $m>1$ . This however leads to a system of equations which is non-linear in the parameters $c_1,...,c_n$ . Are there any principled ways to solve such non-linear systems? The best idea I had so far (for $m=1,...,N$ ) is to find a numerical solution through optimizing the squared loss of the LHS and the RHS of the equation through gradient descent. Do you have any better ideas or principled approaches to suggest?","['statistics', 'maximum-likelihood', 'numerical-methods']"
3278170,Is there a metric on $\mathbb{R}^2$ such that the unit circle is a geodesic?,"It is well known that all 1D spaces are flat, and furthermore that all paths in 1D spaces with unit tangent vectors are geodesics.  In particular the space $S^1$ is flat, and the closed loop traversing the space once is a geodesic. I've been attempting to embed $S^1$ as the unit circle in $\mathbb{R}^2$ so that the unit tangent vector field $\vec V = [1]$ on $S^1$ , which is parallel transported about $S^1$ , remains parallel transported about the unit circle in $\mathbb{R}^2$ , and furthermore that the path $l(t) = [t]$ which is a geodesic in $S^1$ has its $\mathbb{R}^2$ embedding $l(t) = [\cos (t), \sin (t)]$ also a geodesic. However, I haven't been able to make the terms work out.  I've been attempting to transform the metric tensor and the vector field from $S^1$ to the higher-dimensional space $\mathbb{R}^2$ using the partial derivatives of the coordinate transformations, and from the derived $\mathbb{R}^2$ metric tensor finding the Christoffel symbols, then checking that $V$ and $l$ are parallel transported and a geodesic respectively using the parallel transport condition $<\vec U, \nabla \vec V > = 0$ and the geodesic condition $<\vec U, \nabla \vec U> = 0$ . So, is there a metric on $\mathbb{R}^2$ so that the path $l(t) = [\cos(t), \sin(t)]$ is a geodesic, and if so, what is it?  (I'm ready for the answer, since I've burned through all the options that seem evidently available to me).","['tensors', 'geodesic', 'differential-geometry']"
3278200,Iteratively replacing $3$ chocolates in a box of $10$,"In the fridge there is a box containing 10 expensive high quality Belgian chocolates, which my mum keeps for visitors. Every day, when mum leaves home for work, I secretly pick 3 chocolates at random, I eat them and replace them with ordinary cheap ones, that have exactly the same wrapping. On the next day I do the same, obviously risking to eat also some of the cheap ones. How many days on average will it take for the full replacement of the expensive chocolates with cheap ones? I would say $10/3$ but this is very simplistic. 
Also, the total number of ways to pick 3 chocolates out of 10 is $\binom {10} 3=\frac {10!}{3!7!} = 120$ which means that after 120 days I will have replaced all chocolates but I don't think it is correct. Any help?",['combinatorics']
3278216,The order of a $ 2 \times 2 $ matrix mod $ p $,"This is a question I found in an old textbook in group theory: We are asked to prove the order of any matrix $ A $ in the group of $ 2 \times 2 $ invertible matrices over $ F_p $ where $ p $ is a prime, $ A \in \text{GL}(2,F_p) $ , divides either $ p^2-1 $ or $ p^2-p $ . I know that the order of this group is $ (p^2-p) (p^2-1) $ thus the order of the matrix must divide it by Lagrange, but the question asks for something stronger. This is obviously true for diagonal matrices and matrices that have one zero. What about a general invertible $ 2 \times 2 $ matrix? How to show its order must divide one of the two factors given? I thought about using the Sylow theorems but nothing comes to mind. Perhaps group actions? The solution eludes me and I would appreciate help on this.","['matrices', 'group-theory', 'finite-fields', 'finite-groups']"
3278259,Why is $g = \sum_{k=1}^n $ $\sum_{l=1}^n cos(g_k-g_l)$ always non-negative?,"Why is $g = \sum_{k=1}^n $ $\sum_{l=1}^n cos(g_k-g_l)$ always non-negative? g is the magnitude square of the complex-valued function, f, defined below, so it is , for sure, non-negative, but I have such a hard time accepting its non-negativity by looking at the current form of g. Is there a way to re-write g in such a way that it would be easy to see its non-negativity property? $f= \sum_{k=1}^n e^{ig_k}$ is a complex-valued function where g_k is a real-valued function of k. Then the magnitude square of the function $\ |f|^2 = \sum_{k=1}^n e^{ig_k} \sum_{l=1}^n e^{-ig_l}$ = $\sum_{k=1}^n $ $\sum_{l=1}^n e^{ig_k} e^{-ig_l}$ = $\sum_{k=1}^n $ $\sum_{l=1}^n cos(g_k-g_l)$ Therefore $\sum_{k=1}^n $ $\sum_{l=1}^n cos(g_k-g_l)$ >= 0 as it is the magnitude of a complex number.",['trigonometry']
3278276,"Let ${y(x)}$ be the general solution of $y'(x) = A_{n \times n}(x) y(x)+f (x),$ I want to show the next inequality.","Let $y(x)$ be the general solution of $y'(x) = A_{n x n}(x) y(x)+f (x)$ , $x \in I$ where $A(x)$ and $f (x)$ have continuous entries
over I. Let be $\phi_{x_o}$ the fundamental matrix in $x_o \in I$ . Show that for every $x_o \in I$ , we have $$|y(x)|_n \leq n \|\phi_{x_o}\|\left(|y(x_o)|_n + \int_{x_0}^x\|\phi^{-1}_{x_o}\|\cdot|f(t)|_n\,dt \right).$$ We denote $|v|_n = \sqrt{\sum_n |v_k|^2}$ , where $v=(v_1,v_2,...,v_n)^t$ and the matrix norm $\displaystyle\|A\|=\max_{1\leq i \leq n}\left\{\sum_{j=1}^{n} |a_{ij}|\right\}.$ My attempt: I know that that the general solution of the ODE is given by $$y(x)=W(x)\left(c+ \int_{x_0}^x W^{-1}(s) f(s)\,ds\right)$$ $$|y|_n \leq |W(x)c|_n + \left|W(x)\int_{x_0}^x W^{-1}(s) f(s) \,ds\right|_n$$ then $$|y|_n \leq n\|W(x)\|\,|c|_n + \int_{x_0}^x\|\phi^{-1}_{x_o}\|\cdot|f(t)|_n\, dt $$ But after that step I do not know what to do, someone could help me please? I know the next results: $$|Av|_n \leq n\cdot \|A\|\, |v|_n$$ if $\phi(t):[a,b] \rightarrow M_n(\mathbb{R}) $ and $f:[a,b] \rightarrow \mathbb{R}^n $ have continuous entries then $$\left|\int_a^b \phi(t) f(t) \,dt\right|_n \leq \int_a^b \|\phi(t)\|\, |f(t)|_n\, dt. $$","['matrix-exponential', 'matrix-norms', 'ordinary-differential-equations']"
3278311,Is there a way to avoid uniform convergence in Urysohn's extension theorem?,"In the remarkable book Rings of Continuous Functions , by Gillman and Jerison, I came across with the following theorem: Urysohn's Extension Theorem (UT). A subspace $S$ of $X$ is $C^*$ -embedded in $X$ if and only if any two completely separated sets in $S$ are completely separated in $X$ . The proof goes as usual, with the construction of a sequence of continuous functions that converges uniformly to the desired function. Then, after proving Urysohn's Lemma (for normal spaces), UT yields Tietze's Theorem (TT). On the other hand, there are some ways to prove TT without mentioning uniform convergence at all. For instance, as done by Scott here . Since UT is a generalization of TT, I wonder if there is a way to prove UT without using uniform convergence. I tried to adapt Scott's argument to the general setting of UT, but I could not avoid the need for normality.","['alternative-proof', 'continuity', 'general-topology', 'soft-question']"
3278312,Prove that $x^2-5y=3$ has no integer solutions.,"Here is my attempt: $x^2-5y=3$ is the same as $2x^2-10y=6$ , which is equivalent to $2x^2=10y+6$ . 
This means that 2 divides $x^2$ . Since 2 is prime, then 2 divides x, or $x=2k$ for some integer $k$ . Then we have that $(2k)^2-5y=3$ , or $4k^2=5y+3$ . This implies that 2 divides $5y+3$ , but this is a contradiction. This looks valid to me but any feedback is greatly appreciated.","['elementary-number-theory', 'proof-verification', 'discrete-mathematics']"
3278355,A Binomial Identity simplify,"I want to symplify $$ \sum_{\ell=1}^{k} \frac{1}{\ell}\sum_{m=1}^{\min\{\ell,k-\ell\}}\binom{\ell}{m}\binom{k-\ell-1}{m-1}.
$$","['complex-analysis', 'binomial-coefficients', 'binomial-theorem', 'generating-functions']"
3278359,Figuring out the underlying construction of a finite set,"Say you have a set A = { 1, 5, 10 } which when put through a function, produces the set: B = { 1, 5, 10, 6, 11, 15, 16 } That is, f(A) = B . I’m not sure the proper way to write the math for this, but the gist is that the output is made from all combinations of the input set, without duplicates. So, for example, here's how the elements are computed in the given example: B = { 1, 5, 10, 5+1, 10+1, 10+5, 10+5+1 } Each individual item in A is combined with the others so that the output is a list of unique values. That is, since 1+5 is the same as 5+1 , we only use that value once. A more concise way to describe the function is that it is the set of unique values from all nonempty combinations. The second (and most important) part to this is to go the other direction: given B , how do you figure out A ? Since I’m so new to this whole field, my question can be broken down into a few parts: What is the proper way to write the mathematics described above? What branch of mathematics would generally cover this problem? What would a solution to this specific problem look like? I'm generally coming from more of a self-taught programming background, so I'm sure there are plenty of gaps in my knowledge. Sorry for my ignorance, and thanks in advance! I've explored a bit of of matrix algebra for this, and I've had some success with combining A and its transpose, taking the lower triangular matrix from the result, and combining this with the original values of A. I was hoping there would be a straight-forward way to reverse this process? (Possibly wishful thinking?) I think there may also be some way to puzzle through this by looking at which combinations of values in B produce the others, but I imagine there's a much more elegant way to go about it. I've asked a corresponding question regarding the CS aspects of this problem here: https://stackoverflow.com/questions/56825784/is-there-a-standard-approach-to-solving-non-injective-functions","['elementary-set-theory', 'functions', 'notation']"
3278361,"Limit of $\frac{x^2}{x^2 + y^2}$ as $(x,y) \to (0,0)$","I am very new to multivariate calculus and I came up for an incorrect ""proof"" that $$\lim_{(x,y) \to (0,0)} \frac{x^2}{x^2 + y^2} = 0.$$ I understand that the limit does not exist because approaching with $x=0$ and with $y=0$ yield different answers, however after several proofreads I do not understand where I have gone wrong with the ""proof"" given below. We wish to show that for every $\epsilon$ there exists a $\delta$ such that $$\sqrt{x^2+y^2} < \delta \implies | \frac{x^2}{x^2 + y^2} | < \epsilon.$$ Choose $\delta = \min({\sqrt{\epsilon}, 1}).$ Now, because $\delta$ is at most 1, $$\sqrt{x^2+y^2} < \delta \leq 1$$ $$1 < \frac{1}{x^2+y^2}$$ $$\sqrt{x^2+y^2} < \delta \implies \frac{x^2}{1} < \epsilon.$$ Substituting in $\delta = \sqrt{\epsilon}$ , we obtain $$x^2+y^2 < \epsilon \implies x^2 < \epsilon$$ which is true.","['limits', 'multivariable-calculus', 'epsilon-delta']"
3278362,Fibration preserved by Base Change to Generic Fiber,"Let $f: S \to B$ be a fibration from an integral surface $S$ to integral curve $B$ . Here I use following definitions: A surface (resp. curve) is a $2$ -dim (resp. $1$ -dim) proper $k$ scheme over fixed field $k$ . Fibration has two properties: $O_B = f_*O_S$ all fibers of $f$ are geometrically connected Now I wan't to see why the property 1. is ""stable"" with respect to the generic fiber; namely if we benote by $\eta \in B$ the unique generic point of $B$ and $S_{\eta}:=f^{-1}(\eta)$ it's fiber and by $g: S_{\eta} \to \eta$ the corresponding induced map then I wan't to know why $$O_{k(\eta)}= g_* O_{S_{\eta}}$$ holds? Some remarks: Note that as scheme $\eta= Spec(k(\eta))$ . Futhermore if we denote the canonical immersions $i_B: \eta \to B$ and $i_S: S_{\eta} \to S$ . My attempts: Firstly, how is the structure sheaf of $S_{\eta}$ is concretely defined. My intuitive choice would be $O_{S_{\eta}}= O_S \otimes_k k(\eta)$ . Is this correct? Now the problem (in order to verify $O_{k(\eta)}= g_* O_{S_{\eta}}$ ): Let $U$ be open in $\eta$ so wlog $U= \eta$ . I have to show that $g_* O_{S_{\eta}}(\eta)=k(\eta)$ (= $O_{k(\eta)}(\eta)$ ) Then I proceed using definitions and that $i_B \circ g = f \circ i_S$ by construction: $$g_* O_{S_{\eta}}(\eta)= g_* O_{S_{\eta}}(i_b^{-1}(\eta))= {i_B} _* g_*O_{S_{\eta}}(\eta)=f_* {i_S} _* O_{S_{\eta}}(\eta)= {i_S} _* O_{S_{\eta}}(S_{\eta})= {i_S} _* O_S \otimes_k k(\eta)(S_{\eta})= O_S(S) \otimes_k k(\eta)$$ And exactly this is the problem: I don't see why $O_S(S) \otimes_k k(\eta)= k(\eta)$ . Does anybody see where is the error in my reasonings?","['algebraic-geometry', 'surfaces', 'schemes']"
3278391,$\int_0^\pi e^{\cos x}\cos(x-\sin x) dx$,"I’m not quite sure how to approach this problem using complex analysis. Mainly I’m confused int the contour to use after changing the integral by changing $\cos(x-\sin x)$ to become $Re(e^{i(x-\sin x})$ , so that the integral became $$Re\int_0^\pi e^{\cos x}e^{i(x-\sin x)} dx$$ $$Re\int_0^\pi e^{(\cos x-i*\sin x)} e^{ix} dx$$ And then I subbed in $z=e^{ix}$ So then $\frac{dz}{iz}=dx$ $$Re\int_C \frac{1}{iz} dz$$ But I’m not sure what contour to use to solve this","['integration', 'complex-analysis']"
3278394,Domain of $\log_2\log_3\log_2\log_3\log_2x$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I'm working on some math questions for a scholarship. I'm having a tough time as it seems to be tough for me. Please help me out The function $\log_2\log_3\log_2\log_3\log_2x$ has the interval $x>?$ as its maximum domain on real numbers. The key answer is $x>512$ and I have no idea how to go about with the answer.","['algebra-precalculus', 'logarithms']"
3278409,Is my proof correct?: Let $K$ be a normal subgroup of a finite group $G$ and assume $|K|$ and $|G/K|$ are coprime. Prove $K$ is characteristic in $G$.,"Let $f \in \mathrm{Aut}(G)$ . Let $f(K)=H$ . Consider $\pi: G \to G/K$ . Since $\pi(H)=HK/K \le G/K$ , then $|\pi(H)| \mid |G/K|$ . Also, since $HK/K \cong H/(H\cap K)$ , then $|\pi(H)|=|H/(H\cap K)|$ . So, $|\pi(H)||H\cap K|=|H|=|f(K)|=|K|$ . So, $|\pi(H)|\mid |K|$ . Therefore, $|\pi(H)|\mid (|K|, |G/K|)=1$ . Thus, $|\pi(H)|=1$ , and thus $\pi(H)=\{K\}$ . So, $H \subset \ker \pi =K$ . Since $|H|=|K|$ , then $H=K$ . Hence, $f(K)=K$ . So, $K$ is characteristic. Is this a correct proof? Is there an easier way to do this?","['automorphism-group', 'characteristic-subgroups', 'proof-verification', 'abstract-algebra', 'group-theory']"
3278414,Solve the following equation: $\sqrt {\sin x - \sqrt {\cos x + \sin x} } = \cos x$,"Solve the following equation: \begin{array}{l}{\sqrt{\sin x-\sqrt{\cos x+\sin x}}=\cos x} \\ \text{my try as follows:}\\{\sin x-\sqrt{\cos x+\sin x}=\cos ^{2} x} \\ {\sin x-\cos ^{2} x=\sqrt{\cos x+\sin x}} \\ {\sin ^{2} x+\cos ^{4} x-2 \sin x \cos ^{2} x=\cos x+\sin x} \\ {\sin ^{2} x+\cos ^{4} x-2 \sin x \cos ^{2} x-\cos x-\sin x=0} \\ {\sin ^{2} x+\cos ^{4} x-2 \sin x\left(1-\sin ^{2} x\right)-\cos x-\sin x=0} \\{\sin ^2}x + {\left( {1 - {{\sin }^2}x} \right)^2} - 2\sin x\left( {1 - {{\sin }^2}x} \right) - \cos x - \sin x = 0\\ {\sin ^{2} x+\sin ^{4} x-2 \sin ^{2} x+1-2 \sin x+2 \sin ^{3} x-\cos x-\sin x=0} \\ {\sin ^{4} x+2 \sin ^{3} x-\sin ^{2} x-3\sin x-\cos x+1=0}\end{array} Now i think it gets more complicated , any help would be appreciated","['nested-radicals', 'algebra-precalculus', 'trigonometry']"
3278448,"On closed forms for the binomial sum $\sum_{n=1}^\infty \frac{z^n}{n^p\,\binom {2n}n}$ for general $p$?","Define the function, $$A_p(z)=\sum_{n=1}^\infty \frac{z^n}{n^p\,\binom {2n}n}$$ I've asked about the special case $z=1$ of this function before. At the end of this post , we find for $p\geq 2$ a closed-form in terms of a log sine integral. A variant is, $$A_p(1)=\sum_{n=1}^\infty \frac{1}{n^p\,\binom {2n}n} = \frac{(-2)^{p}}{(p-2)!}\int_0^{\color{red}{\pi/6}} x\,\ln^{p-2}\big(\sqrt4\sin x\big)dx\tag1$$ and some experimentation shows, $$A_p(2)=\sum_{n=1}^\infty \frac{2^n}{n^p\,\binom {2n}n} = \frac{(-2)^{p}}{(p-2)!}\int_0^{\color{red}{\pi/4}} x\,\ln^{p-2}\big(\sqrt2\sin x\big)dx\tag2$$ However, another post is about the case $z=4$ and we have the similar, $$A_p(4)=\sum_{n=1}^\infty \frac{4^n}{n^p\binom{2n}{n}}
=\frac{(-2)^p}{(p-2)!}\int_0^{\color{red}{\pi/2}}  x\ln^{p-2}(\sin x)\,dx\tag3$$ Q: What is the formula for $A_p(3)$ ? And what other $A_p(z)$ are formulas (whether as log sine integrals or other) known for general $p$ ? Edit : As I suspected, there is a formula for $z=3$ . Courtesy of nospoon's answer below, we have, $$A_p(3)=\sum_{n=1}^\infty \frac{3^n}{n^p\binom{2n}{n}}
=\frac{(-2)^p}{(p-2)!}\int_0^{\color{red}{\pi/3}}  x\ln^{p-2}\big(\tfrac2{\sqrt3}\sin x\big)\,dx\tag4$$","['integration', 'definite-integrals', 'binomial-coefficients', 'closed-form', 'sequences-and-series']"
3278449,Evaluate $\lim_{n\rightarrow \infty} \left[ \frac{1}{(n+1)(n+2)} + \frac{2}{(n+2)(n+4)} + \cdots + \frac{n}{6n^2} \right]$,"Evaluate: $$\lim_{n\rightarrow \infty} \left[ \dfrac{1}{(n+1)(n+2)} + \dfrac{2}{(n+2)(n+4)} + \cdots + \dfrac{n}{6n^2} \right]$$ $\text{My Attempt:}$ breaking down the summation series into: $$\sum_{r=1}^{n} \dfrac{r}{(n+r)(n+2r)}$$ . 
Further breaking down into two separate series: $$\sum_{r=1}^{n} \dfrac{r}{(n+r)(n+2r)}=\sum_{r=1}^{n} \dfrac{(n+2r)-(n+r)}{(n+r)(n+2r)}$$ This will reduce to give: $$\sum_{r=1}^{n} \dfrac{1}{n+r} - \sum_{r=1}^{n}\dfrac{1}{n+2r}$$ Now, applying limits to the sum: $$\lim_{n\rightarrow\infty}\left[\sum_{r=1}^{n} \dfrac{1}{n+r} -  \sum_{r=1}^{n} \dfrac{1}{n+2r}\right]$$ Taking $n$ common in denominator and converting to Definite integral taking $\dfrac{r}{n}=x$ this reduces to: $$\int_{0}^{1}\dfrac{\text{dx}}{1+x}-\int_{0}^{1} \dfrac{\text{dx}}{1+2x}$$ Edit: Solving this we will get the answer as $\ln\left(\dfrac{2}{\sqrt{3}}\right)$ . I have had committed an error in the evaluation of the 2nd integral as Mr. Robert Z has pointed out below.","['limits', 'summation', 'definite-integrals', 'riemann-sum']"
3278479,Find the chromatic polynomials to this graph,The numbers $1$ to $5$ are name of the each vertex. What is the chromatic polynomial for this graph? I tried have tried using the deletion contraction theorem but when I do that I get a different result each time.,"['graph-theory', 'coloring', 'discrete-mathematics']"
3278538,Weak convergence and weak* convergence in $l^\infty$,"I'm looking for a sequence that converges weakly* in $l^\infty$ but not weakly. In $l^1$ I can take $(e_n)_n$ with $e_n(k)=\delta_{k,n}$ , but I don't know if there is a simple example in $l^\infty$ . Thank you!",['functional-analysis']
3278541,Is it possible to start with a knight at some corner of a chess board and reach the opposite corner passing once through all the squares?,"Is it possible to start with a knight at some corner of a chessboard and reach the opposite corner passing once through all the squares? The knight can reach the other corner or any square for that matter. But if it were to pass through all the squares just for once, that means there should be 63 moves; the 63rd move being the one that it would make to reach the other corner. If the knight starts on a black square(1A), the other corner would also be black(8H). A knight reaches a square of the initial color after every even number of moves. 63 is an odd number, hence we've reached a contradiction. So, it is not possible. Is my reasoning correct? Also, is it possible for the knight to reach the vertically opposite end (say, 1A to 8A) passing through every square once? Edit: I would appreciate a mathematical approach to prove that this is possible. Edit 2: As pointed out in the answer, it is indeed possible to reach the vertically opposite end passing through every square once. I've been thinking along the same lines and I came up with this: Since the 63rd move should land the knight (say, starting from white-1A) on the desired corner which is of the opposite color (black-8A in this case) and an odd number of moves results in a square of the alternate color, so this is possible. Does this suffice to prove it?","['chessboard', 'knight-tours', 'combinatorics']"
