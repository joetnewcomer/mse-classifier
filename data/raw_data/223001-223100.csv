question_id,title,body,tags
4579312,If an analytic $f$ is injective on $\partial D$ then $f$ is injective on $D$ [duplicate],"This question already has an answer here : One-one analytic functions on unit disc (1 answer) Closed 1 year ago . Let $f(z)$ be analytic in a simply connected domain $D$ and on its boundary, the simple closed contour $C$ . If $f(z)$ is injective on $C$ , then $f(z)$ is injective on $D$ . If $z_0\in D$ is a point such that $f(z_0)\notin f(C)$ then we can use an argument principle to conclude $f(z_0) = f(z)$ then $z =z_0$ for $z\in D$ . This is because by assumption, $f(C)$ is also a simple closed curve. But if there is a point $z\in C$ such that $f(z_0) =f(z)$ then the integrand in the argument principle is not well-defined so I can't use this method. I think the open mapping theorem and maximum modulus principle imply this cannot happen but I can't see the explicit contradiction. Please help. Edit: There was a comment given by Aphelli which is incomplete but I wonder if it can be resolved: So the idea is using the argument principle with a slight modification on $C$ . Since $f$ is holomorphic on $\overline{D}$ , there is an open set $U\supset\overline{D}$ such that $f\in\mathcal{H}(U)$ . Since $C$ is compact, we can find a simple close curve $C'\subset U$ enclosing $C$ as close as we want. Once we can show there is such $C'$ that $f$ is injective on $C'$ then we can apply the argument principle as before to prove the statement. The problem is the existence of such $C'$ . Showing the existence of $C'$ such that $f'\neq 0$ on $C'$ is easy but this only ensures local injectivity on $C'$ .",['complex-analysis']
4579336,Is there a systematic way to generate such matrices?,"Given a positive integer $n > 3$ , is there a way (if any) to generate a matrix $M \in \mathbb{F}_2^{n \times n}$ that satisfies the following three conditions: $M$ is invertible, more precisely, $M \in \text{SL}(n,\mathbb{F}_2)$ $M$ has an almost equal number of $0$ and $1$ entries. That is, $$\| N_M(1) - N_M(0) \| \leq 1$$ where $N_M(1)$ denotes the number of $1$ entries in $M$ . For every $3 \times 3$ -sized blocks (thanks for pointing out this in comments) $M_s$ of $M$ , $$\|N_{M_s}(1) - N_{M_s}(0)\| \leq 1$$ For example, I think, when $n = 4$ , a matrix $$\begin{pmatrix}
1 & 0 & 1 & 0\\
0 & 1 & 0 & 0\\
1 & 1 & 0 & 1\\
0 & 1 & 1 & 0
\end{pmatrix}$$ satisfies all the $3$ requirements listed above. However, I am not sure if such matrices exist for some larger $n > 4$ and I am curious. Motivation I want to use such matrices to encode binary messages. Invertibility ensures one can decode such messages. Up to size $8 \times 8$ is enough for practical uses, I think.","['matrices', 'finite-fields']"
4579346,Some questions about the longest run,"The upper bound follows from Borel-Cantelli lemma,I only know that $$\displaystyle \limsup_{ n\to \infty}\ell_{n}/\log_{2}n\leq 1\quad \text{a.s.}$$ Intuitively, $$\displaystyle \limsup_{ n\to \infty}\ell_{n}/\log_{2}n\leq 1\quad \text{a.s.} \quad\Leftrightarrow\quad \displaystyle \limsup_{ n\to \infty}L_{n}/\log_{2}n\leq 1\quad \text{a.s.}$$ but how can I get this equivalence rigorously? For the lower bound, I don't understand that $\text{“we break the first}$ $n$ trials into disjoint blocks of length $$\left [ \left (1-\epsilon \right )\log_{2}n \right ]+1,$$ on which the with the variables are all 1 with probability $$2^{-\left [ \left (1-\epsilon \right )\log_{2}n \right ]-1}\quad .""$$ And why $$P(L_{n}\leq(1-\epsilon)\log_{2}n)\le (1-n^{-(1-\epsilon)}/2)^{n/(\log_{2}n)}\quad ?$$","['probability-theory', 'probability']"
4579484,Find the directional derivative of function,"$$f(x,y) = 
    \begin{cases}
    \displaystyle
    \frac{x^{3}-3xy^{2}}{x^{2}+y^{2}}, 
    & (x,y) \neq (0,0) \\
    0 , & (x,y) = (0,0)
    \end{cases}$$ Find the directional derivative in $(0,0)$ such that makes an angle $135°$ with $x$ positive direction. My attempt: $\lim_{h\to 0}f_x(0+h,0)=\frac{\frac{(0+h)^3-3(0+h)0^2}{(0+h)^2+0^2}-f(0,0)}{h}=1$ $\lim_{h\to 0}f_y(0,0+h)=\frac{\frac{(0)^3-3(0)(0+h)^2}{(0)^2+(0+h)^2}-f(0,0)}{h}=0$ The directional derivative is $(-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$ , i have to normalize the vector , so $(-1,1)$ . The directional derivative is $(1,0)(-1,1)=-1$ My solution isn't correct , I can't get why , hope for some help , thanks !","['partial-derivative', 'multivariable-calculus', 'calculus', 'derivatives']"
4579514,Verify proof: $a^{n}-b^{n} = (a-b) \sum\limits_{k=0}^{n-1} a^{k}b^{n-1-k}$,"A short disclaimer: I do know this question has been asked multiple times here and several answers (including combinatorics) have been given already. However, among all these posts, I did not find anywhere the answer by induction (as I have it understood) and hence, I thought to ask here for verification. One should prove the following: $a^{n}-b^{n} = (a-b) \sum\limits_{k=0}^{n-1} a^{k}b^{n-1-k} \quad (\star)$ Proof: $1^{\text{st}}$ Step: For $n=1$ it holds. Induction Step: We assume that $(\star)$ is true for $n$ . In order to show, that it is also valid for $n+1$ , we calculate $a^{n+1}-b^{n+1}=a^n\cdot a-b^n\cdot b=a^n\cdot a-b^n\cdot a+b^n\cdot a-b^n\cdot b$ $=(a^n-b^n)a+b^n(a-b)\qquad \text{at this point we use } (\star)$ $=a(a-b)(a^{n-1}+ba^{n-2}+b^2a^{n-3}+\dots+b^{n-2}a+b^{n-1})+b^n(a-b)$ $=(a-b)(a^{n}+ba^{n-1}+b^2a^{n-2}+\dots+b^{n-2}a^2+b^{n-1}a+b^n)$ Does this suffice as a proof? Or, is there any mistake in the induction step that I don't see right now? Many thanks in advance!","['algebra-precalculus', 'binomial-theorem', 'induction']"
4579530,How do I find the inverse of $y=2x^2+2x+2$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I'm supposed to find the inverse of $$y=2x^2+2x+2 $$ I know that the inverse isn't a function, as it isn't injective, but we're supposed to find it in domain $[0, 1]$ .
So far, I've switched x for y, and tried to solve for y, and gotten so far: $$ x=2y^2+2y+2$$ $$ 2y^2 + 2y = x-2$$ $$ y(y+1)=1/2 (x-2)$$ WolframAlpha tells me the inverse is supposed to be $$ y= 1/2 (\pm\sqrt{2x-3}-1 ) $$ But I have no idea how to get so far","['calculus', 'functions', 'inverse-function']"
4579579,How to find the extreme values of the following function?,"Function $$ f(x,y) = 3x^2 + 3xy + y^2 + y^3 $$ My Solution $$f_x = 6x + 5y + 3y^2 = 0$$ $$f_y = 3x + 2y +3y^2 = 0$$ Solve system of equations to find y=-x, substitute back into $f_x$ and $f_y$ to find (x,y) = (0,0) or/and (-1/3 , 1/3). $$D = f_{xx} f_{yy} - {f_{xy}}^2$$ This gives that (0,0) is local minimum and (-1/3 , 1/3) is a saddle point. BUT, the actual solution to this question says that (0,0) is the only extreme point, so what happened to (-1/3 , 1/3) ?? Thank you for the help! :)","['analysis', 'real-analysis', 'maxima-minima', 'multivariable-calculus', 'calculus']"
4579586,Find the sum of $\sum_{i=1}^{5}x^5_i+\sum_{i=1}^{5}\frac{1}{x^5_i}$,"Suppose $x^5+5x^3+1=0$ and $x_i$ denotes all the complex roots. Find the sum of $$\sum_{i=1}^{5}x^5_i+\sum_{i=1}^{5}\frac{1}{x^5_i}$$ This polynomial is irreducible over $\Bbb Q[x]$ . I used Vieta's general formulas, however I am not sure if my calculations correct. Here are my calculations. $$x_1x_2+x_1x_3+x_1x_4+x_1x_5+x_2x_3+x_2x_4+x_2x_5+x_3x_4+x_3x_5+x_4x_5=5$$ $$x_1x_2x_3x_4x_5=-1$$ $$x_1+x_2+x_3+x_4+x_5=0$$ $$x_1x_2x_3+x_1x_2x_4+x_1x_2x_5+x_1x_3x_4+x_1x_3x_5+x_1x_4x_5+x_2x_3x_4+x_2x_3x_5+x_2x_4x_5+x_3x_4x_5=0$$ $$x_1x_2x_3x_4+x_1x_2x_3x_5+x_1x_2x_4x_5+x_1x_3x_4x_5+x_2x_3x_4x_5=0$$ This seems so difficult using these formulas. I couldn't make any solution. How can I simplify this summation? $$x_1^5+x_2^5+x_3^5+x_4^5+x_5^5$$ I've tried bring the fractions to common denominator and I got $$\frac {1}{x_1^5}+\frac {1}{x_2^5}+\frac {1}{x_3^5}+\frac {1}{x_4^5}+\frac {1}{x_5^5}=\frac {(x_1x_2x_3x_4)^5+(x_1x_2x_3x_5)^5+(x_1x_2x_4x_5)^5+(x_1x_3x_4x_5)^5+(x_2x_3x_4x_5)^5}{(x_1x_2x_3x_4x_5)^5}=-(x_1x_2x_3x_4)^5-(x_1x_2x_3x_5)^5-(x_1x_2x_4x_5)^5-(x_1x_3x_4x_5)^5-(x_2x_3x_4x_5)^5$$ But, I couldn't make any further progress from here.","['contest-math', 'algebra-precalculus', 'irreducible-polynomials', 'polynomials']"
4579620,Do we explicitly need to assume that a cumulative distribution function satisfies $F(x)\xrightarrow{x\to\infty}1$?,"Remember that $F:\mathbb R\to[0,1]$ is called distribution function if $F$ is nondecreasing; $F$ is right-continuous; $F(x)\xrightarrow{x\to-\infty}0$ ; $F(x)\xrightarrow{x\to\infty}1$ . Now, drop condition (4.) for a moment. Let $$F^{-1}(u):=\inf\{x\in\mathbb R:F(x)\ge u\}\;\;\;\text{for }u\in(0,1).$$ We can show that $$F^{-1}(u)\le x\Leftrightarrow u\le F(x)\;\;\;\text{for all }x\in\mathbb R\text{ and }u\in(0,1)\tag1$$ and hence $$\{u\in(0,1):F^{-1}(u)\le x\}=(0,F(x)]\cap(0,1)\;\;\;\text{for all }x\in\mathbb R.\tag2.$$ Using this we should immediately be able to conclude that $$X:=F^{-1}$$ is a random variable on $((0,1),\mathcal B((0,1)),\mathcal U_{(0,\:1)})$ , where $\mathcal U_{(0,\:1)}$ denotes the uniform distribution on $(0,1)$ , with $$\operatorname P\left[X\le x\right]=F(x)\;\;\;\text{for all }x\in\mathbb R\tag3,$$ which is to say that $F$ is the distribution function of $X$ . However, since any measure is continuous from below and $$\mathbb R=\bigcup_{x\in\mathbb R}(-\infty,x]\tag4,$$ the left-hand side of $(3)$ needs to approach $1$ as $x\to\infty$ and hence condition (4.) is satisfied. So, am I missing something or is (4.) an immediate consequence of (1.)-(3.)?","['cumulative-distribution-functions', 'measure-theory', 'probability-theory']"
4579621,"A non piece-wise bijective function between $\mathbb{R}$ and $[0,\infty)$.","I found the piece-wise bijection: $f(x)=\begin{cases}
0 & x=1 \\ 
e^{x-1} & x \in \{2,3,4,...\} \\
e^{x} & x \in \mathbb{R}- \mathbb{N}
\end{cases}$ But can we find a non piece-wise bijection? I tried the composition idea: $f_{1}:(0,1] \rightarrow [0,\infty)$ and $f_{2}:\mathbb{R} \rightarrow (0,1]$ I know $f_{1}=\text{sech}^{-1}(x)$ but I couldn't find a non piece-wise bijection $f_{2}$ . Is there a non piece-wise bijection between $\mathbb{R}$ to $[0,\infty)$ ?","['elementary-set-theory', 'cardinals']"
4579706,"$\mathcal P((-1,1))$ and the set of all functions $f:\Bbb R\to\Bbb R$ which attain every value in $\Bbb Z$ uncountably many times are equinumerous?","Prove that $\mathcal  P((-1,1))$ and the set of all functions $f:\Bbb R\to\Bbb R$ which attain every value in $\Bbb Z$ uncountably many times are equinumerous. My attempt: My first claim: Let $S=\{A\subseteq\Bbb R\mid A\text{ is uncountable}\}.\operatorname{card}(S)=2^{\mathfrak c}.$ $\boxed{\leq}:$ $$S\subseteq\mathcal P(\Bbb R)\implies\operatorname{card}(S)\le\operatorname{card}(\mathcal P(\Bbb R))=2^{\mathfrak c}.$$ $\boxed{\geq}:$ Let $\Phi:\mathcal P((-1,1))\to S,$ $$\Phi: A\mapsto A\cup B, B=\Bbb R\setminus((-1,1)\cup\Bbb Q).$$ We see $A\cap B=\emptyset,\forall A\in\mathcal P((-1,1)),$ therefore $$\Phi(A_1)=\Phi(A_2)\implies A_1\cup B=A_2\cup B\implies A_1=A_2,$$ so $\Phi$ is injective. $\implies 2^{\mathfrak c}=\operatorname{card}(\mathcal P((-1,1))\le\operatorname{card}(S).$ Therefore, $\operatorname{card}(S)=2^{\mathfrak c}.$ Let $T:=\{f:\Bbb R\to\Bbb R\mid f^{-1}(n)\text{ is uncountable },\forall n\in\Bbb Z\}.$ $\boxed{\leq }:$ $$T\subseteq \{f:\Bbb R\to\Bbb R\}\implies \operatorname{card}(T)\le 2^{\mathfrak c}.$$ $\boxed{\geq }:$ Let $S_{\Bbb Z}=\{A\in S\mid\ A\cap (n,n+1)\text{ is uncountable },\forall n\in\Bbb Z\}.$ Let $\Psi: S_{\Bbb Z}\to T, \Psi:A\mapsto \Psi_A,$ $$\Psi_A(x)=\begin{cases}n, &x\in A\cap (n,n+1)\\ 0,& \text{otherwise}\end{cases}$$ I think $\Psi$ is well-defined because $\operatorname{card}(\Psi^{-1}_A(n))=\operatorname{card}(A\cap (n,n+1))=\mathfrak c,\forall n\in\Bbb Z,\forall A\in S_{\Bbb Z}.$ We can write each $A\in S_{\Bbb Z}$ in a unique way as $$A=\bigcup_{n\in\Bbb Z}\underbrace{A\cap (n,n+1)}_{:=A^{(n)}}.$$ For $A_1,A_2\in S_{\Bbb Z}, A_1\ne A_2,\Psi_{A_1}$ and $\Psi_{A_2}$ must differ for some $x$ because $\exists n\in\Bbb Z, A_1^{(n)}\ne A_2^{(n)}.$ So, I believe, $\Psi$ is injective and hence $$\operatorname{card}\left(S_{\Bbb Z}\right)\le\operatorname{card}(T).$$ I think $\operatorname{card}\left(S_{\Bbb Z}\right)=2^{\mathfrak c}$ because we might similiarly prove that the set $S_n$ of all uncountable subsets of $(n,n+1)$ is also of the cardinality $2^{\mathfrak c},$ each $A\in  S_{\Bbb Z}$ is of the form $$A=\bigcup_{n\in\Bbb Z}A^{(n)}, A^{(n)}\in S_n,n\in\Bbb Z$$ and $$\operatorname{card}\left(S_{\Bbb Z}\right)=\operatorname{card}\left(\displaystyle\prod_{n\in\Bbb Z} S_n\right)=\left(2^{\mathfrak c}\right)^{\aleph_0}=2^{\mathfrak c\cdot\aleph_0}=2^{\mathfrak c}$$ because $$\mathfrak c\cdot 1\le\mathfrak c\cdot\aleph_0\le\mathfrak c\cdot\mathfrak c=\mathfrak c.$$ So, $2^{\mathfrak c}\le\operatorname{card}(T).$ And finally, $\operatorname{card}(T)=2^{\mathfrak c}.$ On the other hand, $$\operatorname{card}(\mathcal P((-1,1)))=2^{\operatorname{card}((-1,1))}=2^{\mathfrak c}.$$ Are my conclusion and final answers valid? Also, is there any other way of proving this?","['elementary-set-theory', 'proof-writing', 'solution-verification']"
4579726,"Finding a counterexample of approximating the solution to $x'=f(x,t)$ by $x_{n+1}'=f(x_n,t)$","Suppose we have a Cauchy problem $$x'=f(x(t),t),\quad x(t_0)=C,$$ where $f$ is Lipschitz continuous in its first argument and continuous in its second argument.
By Picard theorem, there must exist a unique solution within some interval $t\in[t_0-\varepsilon,t_0+\varepsilon]$ .
Now, on this interval, construct a sequence of functions $(x_n)$ by recursive formula $$x_0(t):=C,\quad x_{n+1}(t):=\int_{t_0}^tf(x_n(s),s)\,\mathrm ds+C.$$ It is clear that $x_{n+1}'(t)=f(x_n(t),t)$ ,
and if we take the limit $n\to\infty$ ,
we can see $\lim_{n\to\infty}x_n$ is the solution to the original ODE
if the sequence $(x_n')$ converges uniformly on the interval we consider. Because of the uniform convergence condition, we can potentially get a wrong solution by this procedure.
Is it guaranteed that $\lim x_n$ exists?
Can we construct $f$ and $C$ such that $(x_n')$ fails to converge uniformly, and $\lim x_n$ exists but is a wrong solution?
If we cannot, then is it guaranteed that $\lim x_n$ is a solution as long as it exists?
Can we find the sufficient and necessary condition for $\lim x_n$ to exist and be the solution?","['perturbation-theory', 'ordinary-differential-equations', 'real-analysis']"
4579756,Most efficient way to solve $z^4 + 1 = 0$? [duplicate],"This question already has answers here : How to find the roots of $x^4 +1$ (8 answers) Closed 1 year ago . I would like to know  the most efficient way to solve (in $\mathbb{C}$ of course): $$z^4+1 = 0$$ I already tried: $$\Longleftrightarrow z^2 = \pm \sqrt {-1} = \pm i$$ $$\Longleftrightarrow z = \pm \sqrt{\pm i}$$ which is quite confusing. I also tried: $$\rho \exp{(4i\theta)} = -1$$ $$\Longleftrightarrow 4i\theta = \ln \left(\frac{-1}{\rho}\right)$$ which is also confusing, as $\ln(z\le 0)$ does not exist...","['complex-analysis', 'complex-numbers']"
4579824,Can we write an indicator function as an integral?,"Let $f$ be a function from some set $A$ into $\mathbb{R}$ , and consider the indicator function $\large\mathbb{1}_{f^{-1}(-\infty, x]}$ for $x\in \mathbb{R}$ . Intuitively I feel I can write $$
\large\mathbb{1}_{f^{-1}(-\infty, x]}=\int_{-\infty}^x \large\mathbb{1}_{f^{-1}\{z\}} \text{dz}
$$ but I have absolutely no idea how this could be proved, given $f$ is arbitrary. Is it even correct? I wish I had more to say about this but it is quite confusing to me. Perhaps this would be proved via some probability/measure-theory argument? Any thoughts on this would be great! [This is not a homework question, I am just very curious.]","['improper-integrals', 'measure-theory', 'functions', 'definite-integrals']"
4579835,Independent random variables and product spaces,"Let's say we have a measurable space $(\Omega,\mathcal{A},P)$ and some independent random Variables $X$ and $Y$ . Then we know that $P(X+Y \le s)$ is equal to $P(\{a \in \Omega : X(a)+Y(a) \le s\})$ . Now $P_{X}$ together with the Borel sets form a measurable space, analog also with $Y$ , so we construct the product space with the mass $P_{X}\otimes P_{Y}$ . The question is, how can we show that $$P(\{a \in \Omega : X(a)+Y(a) \le s\}) = P(X+Y \le s) = (P_{X}\otimes P_{Y})(\{(a,b) \in \mathbb{R^2} : a + b \le s\})$$ The first equality is just definition, but how do we prove the second equality?","['stochastic-analysis', 'statistics', 'probability-theory']"
4579844,monotone convergence theorem and calculaton of Hausdorff dimension,"I'm reading these notes , and I'm stuck on an application of the monotone convergence theorem. In computing the Hausdorff dimension of the Koch curve (page 13), the authors show that $H^p(S^{i+1}(A)) > H^p(S^i(A))$ for all $i$ $H^p(K)$ is a finite limit of the sequence $H^p(S^{i}(A))$ where the $S^i(A)$ are the $i^{th}$ iteration of the recursive construction of the Koch curve, $K = \lim_{i \to \infty} S^i(A)$ is the Koch curve, and $p$ is the Hausdorff dimension of $K$ . $H^p(X)$ is the Hausdorff measure of a set $X$ . The authors then use the monotone convergence theorem to conclude $$H^p(\lim_{i \to \infty} S^i(A)) = \lim_{i \to \infty} H^p(S^i(A)).$$ I'm not much of an analyst and really struggling to see how this is an application of the monotone convergence theorem. I'm used to this theorem being the statement that limits commute with integrals in the case of positive integrable functions in an increasing sequence. I don't see where the functions, or the integrals are in the above statement - could someone explain what the relevant measures, integrals, and functions are? Edit : I've added a bounty because I still haven't gotten a satisfying answer here or found one any other way. I'm aware that the argument presented in the linked text might be incorrect/insufficiently rigorous, so I would also accept a rigorous computation using other methods, as long as they remain ""at the level"" of the paper (i.e. I've seen some methods that rely on Radon measures and some other measure theory background I'm not familiar with. I would accept an answer that is rigorous but doesn't use these methods).","['measure-theory', 'hausdorff-measure', 'examples-counterexamples', 'solution-verification', 'fractals']"
4579852,Solve the equation $\sqrt{x^2-9x+24}-\sqrt{6x^2-59x+149}=|5-x|$,"Solve the equation $$\sqrt{x^2-9x+24}-\sqrt{6x^2-59x+149}=|5-x|$$ $x=5$ is the only real solution We can note that $x^2-9x+24>0$ and $6x^2-59x+149>0$ for all $x$ . The first thing I decided to try: as $$|5-x|=\begin{cases}5-x,x\le5\\x-5,x>5\end{cases},$$ we can look at two different cases based on the sign of $5-x$ : let's $x>5$ . Then the equation becomes $$\sqrt{x^2-9x+24}-\sqrt{6x^2-59x+149}=x-5\\\sqrt{x^2-9x+24}=x-5+\sqrt{6x^2-59x+149}\\x^2-9x+24=(x-5)^2+6x^2-59x+149+2(x-5)\sqrt{6x^2-59x+149}\\30x-3x^2-7=(x-5)\sqrt{6x^2-59x+149}$$ We have to raise both sides to the power of 2 again, so I decided to stop here. Something else we can try is to raise both sides of the initial equation to the power of 2, as $|5-x|^2=(5-x)^2,$ so we have $$7x^2-68x+173-2\sqrt{(x^2-9x+24)(6x^2-59x+149)}=x^2-10x+25\\3x^2-29x+72=\sqrt{(x^2-9x+24)(6x^2-59x+149)}$$ I think it's obvious I am missing something.","['real-numbers', 'algebra-precalculus', 'real-analysis']"
4579891,Existence of smooth solutions to the heat equation with variable coefficients,"Consider $$\partial_{t} u - a  \partial_{x}^{2}u = f, $$ in $[0,1]\times [0,T]$ ,  where $a, f : \mathbb{R} \times \mathbb{R}^+ \to \mathbb{R}$ are smooth. Additionally, the initial data $$u(x,0) = u_{0} $$ is smooth and we further endow $u$ with boundary conditions $$u(0,t) = u(1,t).  $$ Question: How can you prove the existence of a smooth solution to this problem? I have read somewhere that this follows from the theory of linear parabolic equations, but I don't know which theory exactly. I did see a proof in Evans for the homogeneous heat equation in $\mathbb{R}$ but that's about it.","['heat-equation', 'analysis', 'sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
4579922,Time derivative of a composite function,"Let $x(t)\in R^n$ be a time-dependent variable and consider two vector-valued functions $g : R^n \mapsto R^m$ and $f : R^m \mapsto R^p$ . What is the time derivative $\frac{d}{dt}f(g(x(t)))$ ? As I understand, applying the chain rule we have that \begin{equation}
\frac{d}{dt}f(g(x(t))) = \frac{\partial }{\partial x} f(g(x)) \frac{dx}{dt}.
\end{equation} The partial derivative (a Jacobian matrix, actually) is given by \begin{equation}
\frac{\partial }{\partial x} f(g(x)) = D_f(g(x)) D_g(x),
\end{equation} where $D_f(a)$ is the Jacobian matrix of a function $f$ evaluated at $a$ . Is this correct? Or should it be $(D_f(g(x)))^T$ in the equation above?","['multivariable-calculus', 'chain-rule']"
4579937,Surprising determinant/trace relation,"For any $N$ , we can take $N$ diagonal $N\times N$ matrices $C_i$ which form an orthonormal set:
Tr $(C_i,C_j)=\delta_{ij}$ . Now take any diagonal $N\times N$ matrix $D$ , and form the matrix $M_{ij} = Tr(D C_i C_j)$ . To my surprise, I find empirically that the determinant of $M$ is the determinant of $D$ ; in fact, the eigenvalues of $M$ are the eigenvalues of $D$ . For instance: if $N=2$ , then $C_1 = diag\{1,1\}/\sqrt{2}$ and $C_2 = diag\{1,-1\}/\sqrt{2}$ ; then if $D=diag\{a,b\}$ , $$M = \frac{1}{2}\left[\matrix{a+b & a-b \\ a-b & a+b}\right]$$ whose determinant is $ab$ and whose trace is $(a+b)$ , showing it has eigenvalues $a,b$ . I suppose this must follow from some elementary facts about matrices, but so far I can't seem to prove it myself.  Is it obvious? What argument or theorem does it follow from? [Note added: since someone answered as though this were an obvious triviality, let me point out how it works for $N=3$ .  The $C_i$ are proportional to diag $\{1,1,1\}$ , diag $\{1,-1,0\}$ , diag $\{1,1,-2\}$ , properly normalized for orthnormality; in this case if $D$ = diag $\{a,b,c\}$ , the matrix $M$ is $$\left[
\begin{array}{ccc}
 \frac{a}{2}+\frac{b}{2} & \frac{a}{2 \sqrt{3}}-\frac{b}{2 \sqrt{3}} & \frac{a}{\sqrt{6}}-\frac{b}{\sqrt{6}} \\
 \frac{a}{2 \sqrt{3}}-\frac{b}{2 \sqrt{3}} & \frac{a}{6}+\frac{b}{6}+\frac{2 c}{3} & \frac{a}{3 \sqrt{2}}+\frac{b}{3 \sqrt{2}}-\frac{\sqrt{2} c}{3} \\
 \frac{a}{\sqrt{6}}-\frac{b}{\sqrt{6}} & \frac{a}{3 \sqrt{2}}+\frac{b}{3 \sqrt{2}}-\frac{\sqrt{2} c}{3} & \frac{a}{3}+\frac{b}{3}+\frac{c}{3} \\
\end{array}
\right]
$$ and it is not instantly obvious that the eigenvalues of this matrix are $a,b,c$ .]","['matrices', 'trace', 'determinant', 'linear-algebra']"
4579985,How far can we go with the integral $I_n=\int_0^1 \frac{\ln \left(1-x^n\right)}{1+x^2} d x$,"Inspired by my post , I decided to investigate the integral in general $$
I_n=\int_0^1 \frac{\ln \left(1-x^n\right)}{1+x^2} d x$$ by the powerful substitution $x=\frac{1-t}{1+t} .$ where $n$ is a natural number greater $1$ . Let’s start with easy one \begin{aligned}
I_1 &=\int_0^1 \frac{\ln \left(\frac{2 t}{1+t}\right)}{1+t^2} d t \\ &=\int_0^1 \frac{\ln 2+\ln t-\ln (1+t)}{1+t^2} d t \\&=\frac{\pi}{4} \ln 2+\int_0^1 \frac{\ln t}{1+t^2}-\int_0^1 \frac{\ln (1+t)}{1+t^2} d t \\&=\frac{\pi}{4} \ln 2-G-\frac{\pi}{8} \ln 2 \\
&=\frac{\pi}{8} \ln 2-G\end{aligned} By my post $$I_2= \frac{\pi}{4} \ln 2-G $$ and $$\begin{aligned}I_4 &=\frac{3 \pi}{4} \ln 2-2 G
\end{aligned}
$$ $$
\begin{aligned}
I_3=& \int_0^1 \frac{\ln (1-x)}{1+x^2} d x +\int_0^1 \frac{\ln \left(1+x+x^2\right)}{1+x^2} d x \\=& \frac{\pi}{8} \ln 2-G+\frac{1}{2} \int_0^{\infty} \frac{\ln \left(1+x+x^2\right)}{1+x^2} d x-G
\\ =& \frac{\pi}{8} \ln 2-\frac{4G}{3} +\frac{\pi}{6} \ln (2+\sqrt{3})
\end{aligned}
$$ where the last integral refers to my post . Let’s skip $I_5$ now. $$
I_6=\int_0^1 \frac{\ln \left(1-x^6\right)}{1+x^2} d x=\int_0^1 \frac{\ln \left(1+x^3\right)}{1+x^2} d x+I_3\\
$$ $$\int_0^1 \frac{\ln \left(1+x^3\right)}{1+x^2} d x = \int_0^1 \frac{\ln (1+x)}{1+x^2} d x +\int_0^1 \frac{\ln \left(1-x+x^2\right)}{1+x^2} d x\\=\frac{\pi}{8}\ln 2+ \frac{1}{2} \int_0^{\infty} \frac{\ln \left(1-x+x^2\right)}{1+x^2} d x-G \\= \frac{\pi}{8}\ln 2+ \frac{1}{2}\left( \frac{2 \pi}{3} \ln (2+\sqrt{3})-\frac{4}{3} G \right)- G \\= \frac{\pi}{8} \ln 2+\frac{\pi}{3} \ln (2+\sqrt{3})-\frac{5}{3} G $$ Hence $$I_6= \frac{\pi}{4} \ln 2+\frac{\pi}{2} \ln (2+\sqrt{3})-3 G$$ How far can we go with  the integral $I_n=\int_0^1 \frac{\ln \left(1-x^n\right)}{1+x^2} d x?$","['integration', 'improper-integrals', 'calculus', 'definite-integrals']"
4580054,Question about the exercise: Prove that $\prod A_{i}$ may be identified with the set of all choice functions for $h$,"For the following question: $(1):$ A cartesian product of the family $(A_i|i\in I)$ of sets is the set $$\prod_{i\in I}A_i=\{f|f:I\rightarrow\cup_{i\in I}A_i \text{with } f(i)\in A_i \text{ for each } i\in I\}$$ together with the projections $\pi_{j}:\prod_{i\in I}A_i\rightarrow A_j,$ $f\mapsto f(j)$ In the context of $(1)$ , let $h:\coprod A_{i}\rightarrow I$ be defined by $h(a,i)=i$ . Prove that $\prod A_{i}$ may be identified with the set of all choice functions for $h$ in the sense of the following exercise: Exercises: Prove that for every function $f:A\rightarrow B$ with $A$ non-empty, there exists a 'choice function' $g:B\rightarrow A$ such that $f\circ g \circ f=f$ Note: $\prod_{i\in I}A_i$ denotes the cartesian product of sets $A_i$ and $\coprod_{i\in I}A_i$ denotes the coproduct of sets $A_i$ . Can someone explain what the question is asking.  Specifically, I don't understand the phrase ""...that $\prod A_{i}$ may be identified with the set of all choice functions for $h$ ..."".
Thank you in advance.",['elementary-set-theory']
4580058,Chinese remainder theorem for groups of matrices over rings,"Let $N=p_1^{k_1}p_2^{k_2}$ and consider the group $GL_2(\mathbb{Z}/N\mathbb{Z})$ .
I would like to use Chinese remainder map to show that the group is isomorphic to $GL_2(\mathbb{Z}/p_1^{k_1}\mathbb{Z}) \times GL_2(\mathbb{Z}/p_2^{k_2}\mathbb{Z})$ . Does the following map gives an isomorphism: $$A \mapsto (A \bmod{p_1^{k_1}}, A \bmod{p_2^{k_2}})$$ where $A \bmod{p_i^{k_i}}$ just means all entries of $A$ taking modulo $p_i^{k_i}$ . This is clearly a well defined map since if the determinant of $A$ is coprime to $N$ , then each modulo $p_i^{k_i}$ is also a unit in each $\mathbb{Z}/p_i^{k_i}\mathbb{Z}$ . Moreover, this is one-to-one correspondence by CRT. However, I am a bit lost of showing the subjectivity (both of their sizes are equal). Any thought?","['matrices', 'group-theory', 'chinese-remainder-theorem']"
4580060,Showing that $M_n(\mathbb{C})$ is a unital C*-algebra,"I am trying to show that the set of $n \times n$ complex matrices $M_n(\mathbb{C})$ is a unital C*-algebra. Showing that it is a unital *-algebra is no problem, but I am struggling to show that $$ \| A^*A \| = \| A \|^2. $$ I am using the norm on $M_n(\mathbb{C})$ defined by the inner product $$ \langle A, B \rangle = \operatorname{Tr}(A^*B) $$ for all $A, B \in M_n(\mathbb{C})$ . That is, $$ \|A \| = \sqrt{\langle A,A \rangle} = \sqrt{\operatorname{Tr}(A^*A)}. $$ Then $$ \|A^*A\| = \sqrt{\operatorname{Tr}([A^*A]^*A^*A)} = \sqrt{\operatorname{Tr}(A^*AA^*A)}. $$ But $$ \|A\|^2 = \operatorname{Tr}(A^*A) = \sqrt{\operatorname{Tr}(A^*A)\operatorname{Tr}(A^*A)}.$$ So I need to show that $$\operatorname{Tr}(A^*A A^*A) = \operatorname{Tr}(A^*A)\operatorname{Tr}(A^*A).$$ I am aware of the trace property $$\operatorname{Tr}(A \otimes B) = \operatorname{Tr}(A)\operatorname{Tr}(B).$$ But is $\operatorname{Tr}(A^*A A^*A) = \operatorname{Tr}(A^*A \otimes A^*A)$ ? The Kronecker product gives $A^*A \otimes A^*A$ as an $n^2 \times n^2$ matrix, whereas $A^*A A^*A$ is an $n \times n$ matrix.","['c-star-algebras', 'functional-analysis', 'operator-algebras']"
4580102,"Give the exact solution of the equation $e^x - \dfrac{1}{x} = m$, where $m$ is a real number","Problem: Give the exact solution of the equation $$e^x - \dfrac{1}{x} = m.$$ By taking the derivative of the function $f(x) = e^x-\dfrac{1}{x}$ , we have $f$ is an increasing function and also takes the value over the real numbers. But, I get difficulty when solving exactly this equation. I am very much appreciated it if anyone can give me some hints.","['calculus', 'algebra-precalculus', 'real-analysis']"
4580103,Finding tangent vectors to unit circle,"I am working on the following problem from Tu's Introduction to Manifolds . I have been able to do (a), but (b) is causing me some troubles. My approach so far is as follows. Let $C$ be some smooth curve in $\mathbb{R}^2$ with $C(0) = p$ and $C'(0) = \frac{\partial}{\partial \overline{x}}\Big|_p.$ Then $$i_*\Big(\frac{\partial}{\partial \overline{x}}\Big|_p\Big) = \frac{d}{dt}\Big|_0(i \circ C)(t) = \frac{\partial i}{\partial C}\Big|_{C(0)} \frac{\partial C}{\partial t}\Big|_0 = \frac{\partial i}{\partial C}\frac{\partial}{\partial \overline{x}}\Big|_p.$$ I am unable to take this any further and it seems I am on the wrong track. Can anyone provide some hints?","['manifolds', 'smooth-manifolds', 'differential-geometry']"
4580113,"if a function is only differentiable at one point , how is the differential calculated","I recently came across this question on a function differentiable at only one point It looks like the author of the answer has used the epsillion-delta definition of the limit, however, I'm familiar only with this definition $$ \frac{dy}{dx}  = \lim \limits_{h \to 0}\frac{f(x+h)-f(x)}{h}$$ and I don't see how I can use this if a function is only continous at one point. Could someone please help","['limits', 'calculus', 'derivatives']"
4580130,this unsolvable equation can be graphed?,"I found something funny. Look at this particular equation $$y = \sqrt{x^2 - x + 8}$$ Now suppose I want to find the domain for this graph. I know that the radical must be greater than or equal to zero (you can't take real solutions of negative radicands). Then all I do is set up an inequality to solve for the x values which are not possible by this graph $$x^2 - x + 8 \ge 0$$ I solve by completing the square to get: $$\left(x-\frac 1 2 \right) ^2  + \frac{31}{4} \ge 0$$ So far so good. But wait. If you continue solving further, you end up having to square root a negative number...which in the real math world is not possible. YET...if I put the initial graph function at the very top of this post on Desmos.com, I am able to get a nice graph. WHAT IN THE WORLD?","['algebra-precalculus', 'graphing-functions', 'quadratics']"
4580146,"Finding the total number of ways of rolling exactly one 1, exactly one 2, or exactly one 6 when rolling a die three times","Given the question that $6^3$ =216 results of rolling a six-sided die three times, how many of them included getting exactly one 1, exactly one 2, or exactly one 6? Here's my approach, Let 'A' be the number of ways of getting exactly one '1' in rolling the die three times, A = 3 x $5^2$ = 75 ways Let 'B' be the number of ways of getting exactly one '2' in rolling the die three times, B = 3 x $5^2$ = 75 ways Let 'C' be the number of ways of getting exactly one '6' in rolling the die three times, C = 3 x $5^2$ = 75 ways The number of ways of getting exactly one '1' and exactly one '2' and exactly one '6' is = 6 (Because the possible ways are {1,2,6}, {1,6,2}, {2,1,6}, {2,6,1}, {6,1,2}, {6,2,1}) A, B, and C are not disjoint. Therefore,
by the principle of inclusion-exclusion,
the number of ways of getting exactly one '1' or exactly one '2' or exactly one '6' should be = (A + B + C) - (the number of ways of getting exactly one '1' and exactly one '2' and exactly one '6') = 75 + 75 + 75 - 6 = 219 But the textbook answer is 159 which means that my approach is incorrect. The textbook doesn't include the explanation for the answer and hence, I'm requesting a help to find the mistakes/error in my solution. Thanks in advanced.","['inclusion-exclusion', 'combinatorics', 'discrete-mathematics']"
4580148,Why is the mean value theorem used here?,"I am really struggling to understand the mean value theorem. I have taken calculus up through advanced calculus, and a bunch of analysis courses. I don't ""get"" it though. Sure, I know the statement, but I don't know how to use it.  Here is an example. 2.47 Theorem. Suppose that $\Omega$ is an open set in $\mathbb{R}^n$ and $G: \Omega \rightarrow \mathbb{R}^n$ is a $C^1$ diffeomorphism. a. If $f$ is a Lebesgue measurable function on $G(\Omega)$ , then $f \circ G$ is Lebesgue measurable on $\Omega$ . If $f \geq 0$ or $f \in L^1(G(\Omega), m)$ , then $$
\int_{G(\Omega)} f(x) d x=\int_{\Omega} f \circ G(x)\left|\operatorname{det} D_x G\right| d x .
$$ b. If $E \subset \Omega$ and $E \in \mathcal{L}^n$ , then $G(E) \in \mathcal{L}^n$ and $m(G(E))=\int_E\left|\operatorname{det} D_x G\right| d x$ . Proof. It suffices to consider Borel measurable functions and sets. Since $G$ and $G^{-1}$ are both continuous, there are no measurability problems in this case, and the general case follows as in the proof of Theorem $2.42$ .
A bit of notation: For $x \in \mathbb{R}^n$ and $T=\left(T_{i j}\right) \in G L(n, \mathbb{R})$ , we set $$
\|x\|=\max _{1 \leq j \leq n}\left|x_j\right|, \quad\|T\|=\max _{1 \leq i \leq n} \sum_{j=1}^n\left|T_{i j}\right|
$$ We then have $\|T x\| \leq\|T\|\|x\|$ , and $\{x:\|x-a\| \leq h\}$ is the cube of side length $2 h$ centered at $a$ . Let $Q$ be a cube in $\Omega$ , say $Q=\{x:\|x-a\| \leq h\}$ . By the mean value theorem , $g_j(x)-g_j(a)=\sum_j\left(x_j-a_j\right)\left(\partial g / \partial x_j\right)(y)$ for some $y$ on the line segment joning $x$ and $a$ , so that for $x \in Q,\|G(x)-G(a)\| \leq h\left(\sup _{y \in Q}\left\|D_y G\right\|\right)$ . In other words, $G(Q)$ is contained in a cube of side length $\sup _{y \in Q}\left\|D_y G\right\|$ times that of $Q$ , so that by Theorem $2.44, m(G(Q)) \leq\left(\sup _{y \in Q}\left\|D_y G\right\|\right)^n m(Q)$ . If $T \in G L(n, \mathbb{R})$ , we can apply this formula with $G$ replaced by $T^{-1} \circ G$ together with Theorem $2.44$ to obtain $$
\begin{aligned}
m(G(Q)) &=|\operatorname{det} T| m\left(T^{-1}(G(Q))\right) \\
& \leq|\operatorname{det} T|\left(\sup _{y \in Q}\left\|T^{-1} D_y G\right\|\right)^n m(Q)
\end{aligned}
$$ Could someone help intuitively understand why the mean value theorem is used here? I want to understand the application of the theorem deeply and chew on it until it becomes an obvious clear application. It must be a very fundamental and powerful result if it is the reason Taylor's theorem works. Could someone please share with me their intuition for it?","['geometry', 'analysis', 'real-analysis', 'multivariable-calculus', 'calculus']"
4580151,$\int \frac{\cos 4x}{4 \sin 2x} dx$,"$\int \frac{\cos 4x}{4 \sin 2x} dx$ Let $u=2x$ , $dx = 1/2 du$ $\int \frac{\cos 2u}{4 \sin u} \frac{1}{2} du = \frac{1}{8} \int \frac{1-2\sin^2 u}{\sin u}du \frac{1}{8} \int \frac{1}{\sin u} du - \frac{1}{8} \int 2 \sin u$ How do I integrate $\int \frac{1}{\sin u} du$ to get $\ln (\tan x)$ ? The online calculator told me to use Weierstrass Substitution which I have not learnt before. Is there any other way to solve this ?","['integration', 'indefinite-integrals', 'calculus', 'trigonometry']"
4580215,"Do we have $\sum_{n=1}^{\infty}\frac{\gcd\left(1+n!,1+n^{2}\right)}{n!}\stackrel{?}{=}e$?","I try to find a balanced exercise on it with Desmos. It seems we have: $$\sum_{n=1}^{\infty}\frac{\gcd\left(1+n!,1+n^{2}\right)}{n!}\overset?=e$$ It seems non trivial as you can remark in replacing the square by a cube. At first glance we can think it's problem with hidden rearrrangement theorem. Keeping in mind that I try the Riemann theorem on sequence and rearrangemnt without success. I keep in mind also the Wilson's theorem but cannot progress. Notice I check the result with Wolfram Alpha. How to (dis)prove it? Thanks in advance for the help. Side notes : It's a strange idea but why calculus couldn't be here I mean ,and I think it's stupid but let's try that : $$\gcd\left(\operatorname{floor}\left(\sqrt{2\pi n}\left(\frac{n}{e}\right)^{n}\right)+1,n^{2}+1\right)=^?1,n\in N^+$$ I think it's not very useful but why not ? I think that because it's really not the same problem but as Leibniz said knowledge is like path between some islands... Ps: It's the floor function . As second notes see also Bounds on the difference between the polylogarithm with negative base and the gamma function Edit 04/02/2023 There is also where $x$ is a prime : $$\gcd(x!-x^x,1+x^2)=1$$ Then I was thinking to Green-Tao theorem but nothing consistent at all. Edit 01/11/2023 : We can represent (I suggest to show it) that : $\exists a,r,n,m,u>0$ such that : $$n!+1=\lfloor(1-a^r)^{-m}\rfloor=P$$ And : $$n^2+1=\lfloor(1-a^r)^{-u}\rfloor=Q$$ Then it seems we have if $a$ is irrational and $P,Q$ have the prime factorization : $$P=\prod_{i=1}^{v}p^{b_i}_{a_i},Q=\prod_{i=1}^{N}p^{c_i}_{d_i}$$ If so $\exists a_k,b_k,c_k,d_k$ such that $p^{b_k}_{a_k}$ is not in the prime factorization of $Q$ and vice versa . A simple progress : If $p$ is a prime having as last digit a $9$ and let $(p-1)^2+1=M$ then if $M$ have $9$ representation as sum of two square and $M^2$ have $4$ pythagorean triples then the conclusion follows and the conjecture is true .To understand that try an  example like $p=269$ it seems also $p=883$ works too $25<882,37<882,29×29<882$ .Build it is rather easily . Remark see https://publications.ias.edu/deligne/paper/357 and Can we invert these analogous ""Dirichlet"" series for GCD / LCM convolution? Some works : If $n,n+2$ is a twin pairs then : $$4(n-1)!+4+n=0\operatorname{mod}(n(n+2))$$ Or $\exists k$ such that : $$4(n-1)!+4+n=kn(n+2)$$ Or : $$4(n-1)!+4=kn^2+(2k-1)n$$ It's  divisible by four next we compare with : $$kn^2+(2k-1)n=4q((n-1)^2+1)$$ Or : $$(k-4q)n^2+(2k-1+8q)n-8q=0$$ If the discriminant is not a perfect square then the roots are irrational and so there is no solution in $q$ : $$2(10k^2-6k+1)$$ Is never a perfect square using $k$ is odd and (optional) the Hasse-Minkowski theorem so it's true that $\gcd((n-1)!+1,(n-1)^2+1)=1$ when $n,n+2$ are prime numbers if $10k^2-6k+1,(n-1)^2+1$ have no common prime factor . We can enlarge with $$f\left(x,y\right)=\gcd\left(\left(10\cdot\left(2\operatorname{floor}\left(x\right)+1\right)^{2}-6\left(2\operatorname{floor}\left(x\right)+1\right)+1\right),\left(\operatorname{floor}\left(y\right)\right)^{2}+1\right)$$ Let introduce : $$f\left(x,y\right)=\gcd\left(10\left(2x+1\right)^{2}-6\left(2x+1\right)+1,y^2+1\right)$$ Conjecture : Let $n>1$ be a natural number then : $$f(n^3,n)=1$$ Or : $$f(n^3,n)=\prod_{i=1}^{m}p_{a_i}$$ Where $p_{a_j}$ is a prime number here distinct As a factorial cannot be a perfect power greater than one we delete some cases I know that if $n-1$ finishes with a $8$ then $(n-1)^2+1$ divisible by $5$ and  primes then I cannot go further One example : It's known that it can be write as a sum of square and squared as a sum of square too. The prime factorization of the number in the sum of square is not in $(n-1)^2+1$ but could be in $(n-1)!+1$ as the example $n=29$ To be continued. In formalism it gives (all the unknow are positive integers and here $p,p+2$ are twin primes): Theorem 1 : Using Wilson's theorem for a prime $p$ Then $p$ is the smallest prime  divisor of $(p-1)!+1$ if $p-1$ ends with a $8$ . Theorem 2 : If $p-1$ have as last digit $8$ then $(p-1)^2+1=P$ is divisible by $5$ .Then if $P=a^2+b^2,P^2=c^2+d^2$ then the integer factors in the prime decomposition of $a,b,c,d$ are not in the prime decomposition of $P$ .
Finally one of $a,b,c,d$ share a prime with $(p-1)!+1=K$ .This prime is the smallest divisor excepting $1$ of $K$ . Theorem 3 with condition of theorem 2 : Recalling theorem 1 , $p$ is the smallest divisor excepting $1$ of $K$ .
The smallest prime divisor of $K$ greater than $p$ is greater than the greater divisor of $P$ which is a prime number as divisor, if $P^2$ have $4$ representation as sum of two square . Some materials for a proof : Let says with the conditions of theorem 1,2,3 on $K$ and let $K$ have $C_p$ prime factor distinct then it seems we have : $$C_p<\ln\left(\left(p-\frac{3}{2}\right)\ln\left(p\right)-p-1+\frac{1}{2}\ln\left(2\pi\right)\right)+\frac{1}{\sqrt{p}}$$ Using Ramanujan-Hardy theorem and Binet's formula . For $P$ which have $T_p$ prime factor distinct it seems we have for $p$ sufficiently large : $$T_p<\ln\left(\ln\left(\left(p-1\right)^{2}+1\right)\right)+\frac{1}{2}\sqrt{\ln\left(\ln\left(\left(p-1\right)^{2}+1\right)\right)}$$ Reference : https://www.jstor.org/stable/2305816?origin=crossref","['number-theory', 'gcd-and-lcm', 'factorial', 'sequences-and-series']"
4580242,Differential of a smooth map in terms of sheaves of deriviations,"$\newcommand{\C}{\mathscr{C}^\infty}$$\newcommand{\blank}{{-}}$$\newcommand{\from}{\colon}$$\newcommand{\after}{\circ}$$\newcommand{\Der}{\mathrm{Der}}$ Recall that for a smooth manifold $M$ , there is an equivalence of categories between vector bundles on $M$ and locally free sheaves of $\C_M$ -modules, given by mapping a vector bundle $E \to M$ to the sheaf of sections $\Gamma(\blank, E)$ . If $p \from E \to M$ and $p' \from E' \to N$ are two vector bundles and $F \from M \to N$ is a smooth map, then a map of vector bundles $E \to E'$ covering $F$ is by definition a smooth map $\Phi \from E \to E'$ such that $p' \after \Phi = F \after p$ which is linear on the fibers. In terms of sheaves, this corresponds to a morphism $\Gamma(\blank, E) \to F^*\Gamma(\blank,E') := \C_M \otimes_{F^{-1}\C_N} F^{-1} \Gamma(\blank, E')$ . In particular, the tangent bundle $T_M$ corresponds to the sheaf of derivations of the sheaf of algebras $\C_M$ .
If $F \from M \to N$ is a smooth map of manifolds, then it is well-known that the differential of $M$ defines a map of vector bundles $DF \from T_M \to T_N$ covering $F$ .
Can this morphism be described in terms of the morphism of sheaves $\Der(\C_M) \to F^*\Der(\C_N)$ ?","['smooth-manifolds', 'vector-bundles', 'algebraic-geometry', 'sheaf-theory', 'tangent-bundle']"
4580243,invariant closed subset under group scheme action (Mumford GIT),"I have a question about a condition in Mumford's GIT book. One page 8, remark (6) iii) they talk about a closed subset $W$ of $X$ which is invariant under the action of $G$ . The context is that $G$ is a group in $Sch/S$ which acts on an $S$ -scheme $X$ . I do not understand what it means for a sub set of $X$ to be invariant. The group action of $G$ in the category of schemes does not induce a group action on the underlying topological spaces and sets, so how does $G$ move the points of $|X|$ around? In case that all schemes are of finite type over an algebraically closed field $k$ , I would just look at the action of $G(k)$ on $X(k)$ . But what does invariant under the action of $G$ mean in the general case?","['group-actions', 'geometric-invariant-theory', 'algebraic-geometry', 'schemes']"
4580251,Can we reduce a 3rd order linear ODE to a 2nd order linear ODE? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question This ODE came out in my exam $$y''' -4y'' -12y'=x^2 $$ However, I was only taught till 2nd order linear ODEs. Can we reduce this to a 2nd order or even a 1st order ODE and then use substitution to find the general solutions at the end? How does it work?",['ordinary-differential-equations']
4580266,"if $ A^2 = 2I $, then $ A + I, A - I $ are invertible","Let $ A $ be a matrix of order $ n \times n $ and assume $ A^2 = 2I $ . I want to prove than $ A + I $ and $ A - I $ are invertible. Is my proof valid? $ A^2 = 2I \Rightarrow A^2 - I = I => (A - I)(A + I) = I $ Therefore, $ A - I, A + I  $ are invertible","['matrices', 'solution-verification', 'linear-algebra']"
4580285,Is there any better way of finding the required value,If $$z=\cos\theta+i\sin\theta$$ find the value of $$\frac{1+z}{1-z}$$ The solution that I have is this $$z=\cos\theta+i\sin\theta \implies$$ $$\frac{1+z}{1-z}=\frac{1+(\cos\theta+i\sin\theta)}{1-(\cos\theta+i\sin\theta)}=\frac{(1+\cos\theta)+i\sin\theta}{(1-\cos\theta)+i\sin\theta}$$ $$=\frac{2\cos^2\frac{\theta}{2}+i\:\:2\sin\frac{\theta}{2}\cos\frac{\theta}{2}}{2\cos^2\frac{\theta}{2}-i\:\:2\sin\frac{\theta}{2}\cos\frac{\theta}{2}}=\frac{\cos\frac{\theta}{2}}{\sin\frac{\theta}{2}}\cdot \frac{\cos\frac{\theta}{2}+i\sin\frac{\theta}{2}}{\sin\frac{\theta}{2}-i\cos\frac{\theta}{2}}$$ $$=\cot\frac{\theta}{2}\cdot\frac{\color{red}{i}}{\color{red}{i}}×\frac{\cos\frac{\theta}{2}+i\sin\frac{\theta}{2}}{\sin\frac{\theta}{2}-i\cos\frac{\theta}{2}}$$ $$=\color{red}{i}\cot\frac{\theta}{2}\cdot\frac{\cos\frac{\theta}{2}+i\sin\frac{\theta}{2}}{\color{red}{i}\sin\frac{\theta}{2}-\color{red}{i}i\cos\frac{\theta}{2}}$$ $$=\color{red}{i}\cot\frac{\theta}{2}\cdot\frac{\cos\frac{\theta}{2}+i\sin\frac{\theta}{2}}{\cos\frac{\theta}{2}+\color{red}{i}\sin\frac{\theta}{2}}$$ $$=\color{red}{i}\cot\frac{\theta}{2}$$ Now how on earth one will imagine the steps written in red. I am looking for an easy and a logical answer to this question. The solution that I have is impractical as you all can see. I tried it doing by $e^{i\theta}$ but no good happen. Any help is greatly appreciated.,"['alternative-proof', 'trigonometry', 'complex-numbers']"
4580306,I am confused with simply how to solve local optimization problems of multiple variables on a curve.,"For $f(x,y) = x^3 + x^2y - y^2 - 4y$ ,
find and classify all critical points of $f$ on the line $y = x$ . My Attempt: $$\frac{\partial}{\partial x}f(x,y) = 3x^2 + 2xy = 0 \implies x(3x + 2y) = 0 \implies x = 0,-4,1$$ $$\frac{\partial}{\partial y}f(x,y) = x^2 - 2y - 4 = 0 \implies x^2 - 2y = 4 \implies y = -2,6,-3/2$$ So, the critical points are: $(0,-2), (-4,6), (1,-3/2)$ The first thing I thought was that $y = x$ was the domain of allowed critical points, but then that would mean that no critical points would work. Next I thought we needed to plug in $x$ for $y$ in the partial derivatives to find the true critical points, but that didn't work either. What is confusing to me is when I look at the answer key, it says the critical points are $(1,1)$ and $(-2/3,-2/3)$ , but when I plug these points into the partial derivatives set equal to zero, neither of them satisfy the equations. I know I am missing something here, but I just don't know what it is.","['optimization', 'multivariable-calculus']"
4580323,$\lim_{n \to \infty} \frac{1}{\sqrt{n^k}} (1 + \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} + \cdots + \frac{1}{\sqrt{n}})^k$ by Stolz–Cesàro,"$$\lim_{n \to \infty} \frac{1}{\sqrt{n^k}} (1 + \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} + \cdots + \frac{1}{\sqrt{n}})^k, k \in \mathbb{N}$$ Putting $\sqrt{n^k}$ in denominator, we get: $$\lim_{n \to \infty} \frac{(1 + \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} + \cdots + \frac{1}{\sqrt{n}})^k}{\sqrt{n^k}}$$ Apllying Stolz–Cesàro theorem: $$\lim_{n \to \infty} \frac{(1 + \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} + \cdots + \frac{1}{\sqrt{n}})^k - (1 + \frac{1}{\sqrt{2}} + \frac{1}{\sqrt{3}} + \cdots + \frac{1}{\sqrt{n-1}})^k }{\sqrt{n^k} - \sqrt{(n-1)^k}}$$ I don't know how to use properly multinomial formula, but the biggest number in numerator among denominators in first brackets with $k$ degree is $\sqrt{n^k}$ , in second brackets $\sqrt{(n-1)^k}$ , their coefficients are $1$ $$\lim_{n \to \infty} \frac{ \frac{1}{\sqrt{n^k}} - \frac{1}{\sqrt{(n-1)^k}} + \frac{1}{\sqrt{(n-1)^k}} - \cdots + 1 - 1 + \cdots}{\sqrt{n^k} - \sqrt{(n-1)^k}}$$ Multiplying numbers in first brackets by $\frac{1}{\sqrt{n}}$ gives us $\frac{1}{\sqrt{2^{k-1} \times n}} + \frac{1}{\sqrt{3^{k-1} \times n}} + \cdots + \frac{1}{\sqrt{(n-1)^{k-1} \times n}}$ etc., but I suspect we can't get same numbers in second brackets. My guess, after evaluating sum (or difference) in numerator, we need to divide each number of our expression by $\sqrt{n^k}$ .
In denominator we get 0, because $$\sqrt{\frac{n^k}{n^k}} - \sqrt{\frac{(n-1)^k}{n^k}} = 1 - \sqrt{(\frac{n-1}{n})^k} = 1 - 1 = 0, n \to \infty$$ Multiplication and division of denominator by conjugate ( $\sqrt{n^k} + \sqrt{(n-1)^k}$ ) gives ""slightly"" different picture (unless I've done several big mistakes in a row): $$\frac{ n^k - (n-1)^k }{ \sqrt{n^k} + \sqrt{(n-1)^k} } = \frac{ n^k - n^k + kn^{k-1} - \cdots }{ \sqrt{n^k} + \sqrt{(n-1)^k} } = \frac{kn^{k-1} - {k\choose 2}n^{k-2} +\cdots }{ \sqrt{n^k} + \sqrt{(n-1)^k} }, n \to \infty$$ Dividing by $n^{k-1}$ : $$\frac{k - 0 + 0}{ \sqrt{\frac{n^k}{n^{2(k-1)}  } } + \sqrt{ \frac{(n-1)^k}{n^{2(k-1)}}  }} = \frac{k - 0 + 0 - \cdots}{0 + 0}$$ So my guess was a bad guess. Unfortunately, by this point I'm pretty much lost. Hence, I need some help... Thank you!","['limits', 'calculus', 'real-analysis']"
4580349,A random walk on the clock,"The setting is simple but I just don't know how to calculate a desired quantity, which will be introduced later. Suppose we have a clock with $M$ points on it, i.e. $\{0, 1, 2, ..., M-1\}$ , where $M\in\mathbb N$ . Suppose we starts at some point on the clock, say $a$ , where $0\leq a\leq M-1$ . We have a probability of $1/2$ to stay at point $a$ , and a probability of $1/2$ to move. If we move, then we will have a probability $p$ to move clockwise and a probability $1-p$ to move anti-clockwise, where $0<p<1$ . Let $X_n$ denote our location after $n$ steps. We define $d(x)=\min\{x,M-x\}$ for all $0\leq x \leq M-1$ , i.e. the distance between $x$ and the point $0$ . Question: Calculate $\lim_{n\to \infty} \mathbb E[d(X_n)]$ , where $\mathbb E$ means expectation. I have been thinking this problem for quite a while but don't know how to approach it. We cannot use the same argument as in this problem (cf. @Did 's post in the answer) because $\mathbb E[d(X_n)|X_0=y]$ may be different for different $y$ , where $0\leq y \leq M-1$ . That's, our problem doesn't have ""symmetry"". Also, I don't know if we need to consider $\lim_{n\to \infty} \mathbb E[d(X_n)]$ as a whole or consider the limit and the expectation separately, which means first calculating the expectation and then taking limit. Thanks for help!","['random-walk', 'markov-chains', 'probability']"
4580358,Does the accessibility lemma from convex analysis also hold in infinite dimensions?,"Accessibility lemma. Let $C \subset \mathbb R^n$ be convex and nonempty. For all $\lambda \in [0, 1)$ , $x \in \text{ri}(C)$ and $y \in \overline{C}$ we have $(1 - \lambda) x + \lambda y \in \text{ri}(C)$ , where $$
\text{ri}(C)
:= \{ x \in C: \exists \varepsilon > 0: B(x, \varepsilon) \cap \overline{\text{aff}}(C) \subset C \}
$$ is the relative interior of $C$ with respect to $\overline{\text{aff}}(C)$ and $\text{aff}(C)$ is the affine hull of $C$ , and $B(x, \varepsilon) := \{ y \in \mathbb R^n: | x - y |_2 < \varepsilon \}$ is the Euclidean norm ball.
(The name is taken from Exercise 4 in Chapter 3.1 of Niculescu, Persson - Convex Functions and Their Applications .) Note that since $\mathbb R^n$ is finite-dimensional, $\overline{\text{aff}}(C) := \overline{\text{aff}(C)} = \text{aff}(C)$ . I propose the following proof for this statement when $\mathbb R^n$ is replaced by any normed space $(E, \| \cdot \|)$ , which uses the proof in Rockafellar's ""Convex Analysis"" (1970, Princeton Press) (Thm. 6.1, p. 45)  but I don't know if it is correct. Proof. Let $x \in \text{ri}(C)$ , $y \in \overline{C}$ and $\lambda \in [0, 1)$ .
We want to show that $(1 - \lambda) x + \lambda y \in \text{ri}(C)$ . As $x \in \text{ri}(C)$ , there exists a $\varepsilon > 0$ such that \begin{equation*}
    B_{\varepsilon}(x) \cap \overline{\text{aff}}(C) \subset C.
\end{equation*} Since $y \in \overline{C}$ , we have $y \in C + B_{\rho}(0)$ for every $\rho > 0$ .
Hence for all $\rho > 0$ with $\rho < \varepsilon \frac{1 - \lambda}{1 + \lambda}$ we have (see also this answer ) \begin{align*}
    B_{\varepsilon}\big((1 - \lambda) x + \lambda y\big)
    & = (1 - \lambda) x + \lambda y + B_{\varepsilon}(0) \\
    & \subset (1 - \lambda) x + \lambda \big( C + B_{\rho}(0) \big) + B_{\varepsilon}(0) \\
    & = (1 - \lambda) \left[x + \frac{1 + \lambda}{1 - \lambda} B_{\rho}(0) \right] + \lambda C \\
    & = (1 - \lambda) B_{\rho \cdot \frac{1 + \lambda}{1 - \lambda}}(x) + \lambda C \\
& \subset (1 - \lambda) B_{\varepsilon}(x) + \lambda C
\end{align*} and thus \begin{align*}
B_{\varepsilon}\big((1 - \lambda) x + \lambda y\big) \cap \overline{\text{aff}}(C)
    & \subset (1 - \lambda) B_{\varepsilon}(x) \cap \overline{\text{aff}}(C)
    + (\lambda C) \cap \overline{\text{aff}}(C) \\
    & 
\overset{(\star)}{=} (1 - \lambda) \left[B_{\varepsilon}(x) \cap \overline{\text{aff}}(C)\right]
    + (\lambda C) \cap \overline{\text{aff}}(C) \\
& \subset (1 - \lambda) C
    + (\lambda C) \cap \overline{\text{aff}}(C)
    = (1 - \lambda) C + \lambda C
    = C,
\end{align*} where in the last step we use that $C$ is convex and in the third last step that $\lambda C \subset C \subset \overline{\text{aff}}(C)$ . (We also use that $(A + B) \cap C = (A \cap C) + (B \cap C)$ and that $A \subset B$ implies $(A \cap C) \subset (B \cap C)$ as well as $\lambda (A \cap B) = (\lambda A) \cap (\lambda B)$ for sets $A, B, C \subset E$ and $\lambda \ne 0$ as well as the fact that (closed) affine hull commutes with linear transformations such as scalings.) Any comments on the correctness of this proof are highly appreciated.
I am not sure about the equality $(\star)$ : I think that since $T(z) :=  (1 - \lambda) z$ is a linear map, $T(\text{aff}(A)) = \text{aff}(T(A))$ for any set $A$ , so that $(1 - \lambda) \text{aff}(C) = \text{aff}((1 - \lambda) C)$ . Since $\lambda \in (0, 1)$ , we have $(1 - \lambda) C \subset C$ and thus $\text{aff}\big((1 - \lambda) C\big) \subset \text{aff}(C)$ . Applying the closure should yield $(1 - \lambda) \overline{\text{aff}}(C) \subset \overline{\text{aff}}(C)$ . Since $\lambda (A \cap B) = (\lambda A) \cap (\lambda B)$ for any non-zero scalars $\lambda$ and sets $A, B \subset E$ , we still need the other inclusion which I am not sure how to prove (or if it even holds).
In my finite-dimensional intuition, this certainly makes sense, since it doesn't matter if we take the scaled ball and intersect it with an affine subspace to yield a scaled disk, or first intersecting the ball with the affine subspace and then scale it.","['normed-spaces', 'solution-verification', 'general-topology', 'convex-analysis', 'affine-geometry']"
4580403,How to prove the binomial identity $\binom{n + 1}{a + b + 1} = \sum_{k = 0}^n \binom{k}{a}\binom{n - k}{b}$,"Prove the identity: $$\binom{n + 1}{a + b + 1} = \sum_{k = 0}^n \binom{k}{a}\binom{n - k}{b}$$ So far I understand the left side represents how many ways there are picking a+b+1 elements from a set (lets say X) with cardinality n+1. The right side as far as I understand means how many ways there are choosing a elements from a set with cardinality k and then number of ways choosing b elements from a set with n-k cardinality. However I do not see how adding all these sums as k goes from 0 to n adds up to the left hand side.
Thanks for the help. (Side note: I wish to prove this without generating functions or Vandermondes identity, but with a counting argument).","['binomial-theorem', 'combinatorial-proofs', 'discrete-mathematics']"
4580444,Showing the circular part of the Bromwich contour vanishes,"I've been using this example $$\mathcal{L}^{-1} \{\frac{3}{(s^2+9)^2}\}(t)$$ Actually calculating the inverse is fine, but I got stuck trying to prove that the circular part of the Bromwich contour goes to $0$ . If the angle to the top of the circular path from the x-axis is $\alpha$ , then the whole path rotates $2(\pi-\alpha)$ : $$\int_{0}^{2(\pi-\alpha)}{\frac{3e^{st}}{(s^2+9)^2}\,ds}$$ $s=Re^{i\theta},ds=iRe^{i\theta}d\theta$ $$3iR\int_{0}^{2(\pi-\alpha)}{\frac{e^{Rte^{i\theta}}}{(R^2e^{i2\theta}+9)^2}e^{i\theta}\, d\theta}$$ Using $|\int_{a}^{b}{f(x)\, dx}|\leq\int_{a}^{b}{|f(x)|\, dx}$ and the reverse triangle inequality on the denominator : $$\begin{align}\left|3iR\int_{0}^{2(\pi-\alpha)}{\frac{e^{Rte^{i\theta}}}{(R^2e^{i2\theta}+9)^2}e^{i\theta}\,d\theta}\right|&\leq 3iR\int_{0}^{2(\pi-\alpha)}{\left|e^{Rte^{i\theta}}\right|\frac{1}{\left|R^2e^{i2\theta}+9\right|^2}\left|e^{i\theta}\right|\, d\theta}\\&\leq3iR\int_{0}^{2(\pi-\alpha)}{\left|e^{Rt\cos(\theta)}\right|\left|e^{iRt\sin(\theta)}\right|\frac{1}{|R^2-9|^2}\, d\theta}\\&\leq\frac{3iR}{|R^2-9|}\int_{0}^{2(\pi-\alpha)}{\left|e^{Rt\cos(\theta)}\right|\, d\theta}
\end{align}$$ How can I continue this to show that the expression goes to $0$ as $R\to\infty$ ? I'm worried that the use of the reverse triangle inequality and the bounds of integration are incorrect. Edit : The contour looks like the image on below, which is from here","['integration', 'complex-analysis', 'contour-integration', 'complex-integration']"
4580452,matrix derivation,"Given $$
\mathbf{Z}=\mathbf{A}\mathbf{H}\mathbf{W}
$$ where $\mathbf{Z}\in\mathbb{R}^{V\times d_n}$ , $\mathbf{A}\in\mathbb{R}^{V\times V}$ , $\mathbf{H}\in\mathbb{R}^{V\times d_{n-1}}$ , and $\mathbf{W}\in\mathbb{R}^{d_{n-1}\times d_n}$ , what is the matrix representation of $$\frac{\partial \mathbf{Z}}{\partial \mathbf{H}}$$ ? My understanding is that it should be $\mathbf{A}\mathbf{W}$ , but their dimensions don't match.","['matrices', 'matrix-calculus']"
4580463,Is this limit solvable using Stolz’s theorem ? $\lim_\limits{n\to+\infty}\left(\frac12+\frac3{2^2}+\dots+\frac{2n−1}{2^n}\right)$,"I would like to ask if this example is solvable using Stolz’s theorems, because I have a $2^n$ expression at the bottom and a quadratic expression at the top, so it doesn't work for me, but maybe I'm calculating wrong.","['power-series', 'limits', 'calculus', 'sequences-and-series']"
4580478,Definition of a signed measure in Folland and its meaning when the signed measure is not finite,"So I'm currently reading about signed measures in ""Real analysis"" by Folland. In it, he defines a signed measure as follows. Let $(X,\mathcal{M})$ be a measurable space. Then a signed measure $\nu$ is a function from $\mathcal{M}$ to $[-\infty,\infty]$ satisfying (i) $\nu(\emptyset) = 0$ (ii) At most one of the values $\pm\infty$ are assumed (iii) If $A_1, A_2, \dots$ are (pairwise) disjoint elements of $\mathcal{M}$ , then $\nu(\cup_iA_i) = \sum_i\nu(A_i)$ where the sum on the RHS converges absolutely if the LHS is finite. Now suppose we have a sequence $(A_i)$ of sets in $\mathcal{M}$ . If $\nu(\cup_iA_i) = \infty$ , it is not obvious to me that, under these hypotheses, for any permutation $\sigma$ , one automatically has $\sum_i\nu(A_{\sigma(i)}) = \infty$ . Is this in fact automatically true or is it an additional assumption that we bake into the definition?","['measure-theory', 'signed-measures']"
4580484,Integral of symplectic form?,"It is often said that ""differential forms are used for integration"". Typically people like to talk about the integral $\int_M \omega$ of a differential form $\omega$ , and exterior derivative, one of the most important operation of differential form, is defined such that Stokes' theorem $\int_{\partial M} \omega = \int_M \mathrm{d} \omega$ can hold. Symplectic manifold is a manifold equipped with a symplectic 2-form $\omega$ . Therefore, according to the former consideration, it should be interesting to talk about integral of the symplectic form on a two-dimensional submanifold of a symplectic manifold. However, it seems that people in the field of symplectic geometry is quite indifferent on this. Why?","['submanifold', 'symplectic-geometry', 'smooth-manifolds', 'derivatives', 'differential-geometry']"
4580489,Are there homomorphisms $SU(2) \xrightarrow{} SU(2)$ with higher homotopy?,"I am quite new to the subject, so I apologize in advance for anything sloppy which I may write. The homotopy group $\pi_3(SU(2))= \mathbb{Z}$ classifies maps of the form $S^3 \simeq SU(2) \xrightarrow{} SU(2)$ , where $\mathbb{Z}$ , roughly speaking, measures how many times $SU(2)$ wraps onto itself. These maps, and their homotopy classification, appear in the standard construction of instantons via bundles. For example, we have that the map $g_0: x \in S^3 \mapsto e \in SU(2)$ , with $e$ being the identity, belongs to the class $0$ of $\pi_3(SU(2))$ . The following map belongs to the class $1$ : $g_1: (x_1,x_2,x_3,x_4)\in S^3 \mapsto \frac{1}{r} (x^4 \mathbb{1} + \sum_i x_i \sigma_i) \in SU(2)$ , where $r^2=x_1^2 + x_2^2 + x_3^2 +x_4^2$ , $\sigma_i$ are the Pauli matrices and $\mathbb{1}$ is the identity. For $n>1$ , the map $g_n: x \mapsto r^{-n}(x^4 \mathbb{1} + \sum_i x_i \sigma_i)^n$ belongs to the class $n$ of $\pi_3(SU(2))$ . Clearly, all these maps can be thought to be maps from $SU(2)$ to $SU(2)$ . $g_0$ and $g_1$ are homomorphisms, i.e. $g(x y)=g(x)g(y)$ . However, since $SU(2)$ is not abelian, it is evident that $g_n$ with $n>1$ is $not$ a homomorphism. Now, finally to my question: are there homomorphisms $SU(2) \xrightarrow{} SU(2)$ that belong to the class $n$ of $\pi_3(SU(2))$ for $n>1$ ? If so, could you give me an example? Thanks in advance!","['group-homomorphism', 'unitary-matrices', 'homotopy-theory', 'lie-groups', 'differential-geometry']"
4580515,Why $\lim\limits_{n \to \infty} \frac{\frac1{\sqrt{n+1}}}{\sqrt{n+1}-\sqrt{n}} = \lim\limits_{n \to \infty}\frac{\sqrt{n+1} + \sqrt{n}}{\sqrt{n+1}}$?,"I just need clarification about answer to this question: $$\lim_{n \to \infty} \frac{\sum\limits_{i = 1}^{n} \frac{1}{\sqrt{i}}}{\sqrt{n}} = \lim_{n \to \infty} \frac{\sum\limits_{i = 1}^{n+1} \frac{1}{\sqrt{i}} - \sum\limits_{i = 1}^{n} \frac{1}{\sqrt{i}}}{\sqrt{n+1} - \sqrt{n}} = \lim_{n \to \infty} \frac{\frac{1}{\sqrt{n+1}}}{\sqrt{n+1}-\sqrt{n}} = \lim_{n \to \infty}\frac{\sqrt{n+1} + \sqrt{n}}{\sqrt{n+1}} = 2$$ $$\lim_{n \to \infty} \left(\frac{1}{\sqrt{n}} \sum\limits_{i = 1}^{n} \frac{1}{\sqrt{i}}\right)^k = 2^k$$ In this case I don't understand why $$\lim_{n \to \infty} \frac{\frac{1}{\sqrt{n+1}}}{\sqrt{n+1}-\sqrt{n}} = \lim_{n \to \infty}\frac{\sqrt{n+1} + \sqrt{n}}{\sqrt{n+1}}$$ My reasoning. $$\frac{ \frac{1}{a} }{b - c} = \frac{1}{a(b-c)}$$ Hence, we get: $$\lim_{n \to \infty} \frac{\frac{1}{\sqrt{n+1}}}{\sqrt{n+1}-\sqrt{n}}=\lim_{n \to \infty} \frac{1}{n+1 - \sqrt{n}\sqrt{n+1}} = \lim_{n \to \infty} \frac{ \frac{1}{n} }{ \frac{n}{n} + \frac{1}{n} + \sqrt{\frac{n^2}{n^2}+\frac{n}{n^2} } } = \lim_{n \to \infty}\frac{0}{1+0+\sqrt{1+0}}=0$$ While $\lim_{n \to \infty}\frac{\sqrt{n+1} + \sqrt{n}}{\sqrt{n+1}} = \lim_{n \to \infty}\frac{ \sqrt{\frac{n}{n} + \frac{1}{n}} + \sqrt{\frac{n}{n}} }{ \sqrt{\frac{n}{n} + \frac{1}{n} } } = \lim_{n \to \infty}\frac{\sqrt{1+0} + \sqrt{1} }{\sqrt{1+0}} = 2$ Any explanation, what's going on? Thank you","['limits', 'calculus', 'real-analysis']"
4580521,Find a large subset of any $n$ integers where $a_1+2a_2=2a_3+2a_4$ is impossible.,"Let $B=\left\{ b_{1},...,b_{n}\right\} $ a set of distinct positive integers. Show that there exists a subset $A\subseteq B$ , such that $|A|>\frac{1}{10} \cdot n$ , and such that there does not exist $a_1,a_2,a_3,a_4 \in A$ such that $a_1+2a_2=2a_3+2a_4$ . ( $n 
\geq 1$ ) I have no clue where to start, I do have a few hints:
consider $p=4k+3$ and consider $(1/2,2/3]$ , (specifically that $3\cdot (1/2,2/3]\cap 4\cdot(1/2,2/3] = \emptyset$ ).
It should involve random variables and expectations, but I guess it is not the only way to solve, so any way would be highly appreciated. Thanks in advance.","['discrete-mathematics', 'combinatorics', 'probability']"
4580576,Is the b coefficient in the general cosine function $f(x)=a\cos(b(x-c))+d$; the number of times the function repeats in 2 $\pi$ radians?,"I have a series of problems in the class I am in.  The problems ask me to find the equation for a mystery graph. The b Coefficient; can be found by counting the number of maximums.  This method works good, until the graph doesn't fit nicely into $2\pi radians$ .  I was wondering if there was an equation that could replace this process? The function to the graph is $ f(x)=cos(4.2(x))$ (Red graph) and $x=2\pi$ is the (blue graph). It is easy to find the 4.  I can do it by counting the number of maximums from points $(0,0)$ to $(2\pi,0)$ .  But, I was looking for some help in find the .2.  Right now I have to draw another graph and try to match my graph with the unknown graph.  I think I can divide.  Something like $Period/2 \pi$ .  I would have to know the period however, and to do this I would have to know what the coefficient b is to find the period.",['trigonometry']
4580577,Is the integral $\int_0^T |f(x)| dx$ always strictly greater than $0$?,"Let $T>0$ be fixed and let $f$ be a real valued integrable function such that $f\not\equiv 0$ . Is it always true that $$\int_0^T |f(x)| \ dx >0 \space ?$$ If yes, could someone give an hint for the proof? If not, could you please exhibit a counterexample? Thank you in advance.","['integration', 'examples-counterexamples', 'real-analysis', 'complex-analysis', 'calculus']"
4580588,Invertability of derivative of a bijection,"I am working on the following problem: Let $f(x)$ a differentiable bijection $\mathbb{R}^n \rightarrow \mathbb{R}^n$ , and its inverse $f^{-1}$ also differentiable. Then $f'$ is invertible for $ \forall x \in \mathbb{R}^n$ . I want to get the solution myself (so I am not asking for it), but I am confused on this example: If $f(x)=x$ , then it is differentiable, and it is its own inverse, so again, differentiable, and it is a bijection from $\mathbb{R} \rightarrow \mathbb{R}$ , but the derivative is $f'(x)=1$ is not invertible because it is not injective. Maybe this problem only makes sense for $n\geq2$ , but I don't see why because multivariable theory is built on top of single-variable. Am I missing something obvious?","['multivariable-calculus', 'functions', 'derivatives', 'inverse']"
4580618,"Holomorphic extensions of harmonic functions in $\mathbb R^n$, $n\geq2$","Let $\Omega$ be a (smooth) domain in $\mathbb R^n$ , $n\geq2$ and $u$ be a harmonic function in $\Omega$ continuous up to the boundary. I read somewhere (but unfortunately I don't remember where) that if $B_{\mathbb R^n}(p,r)$ is a ball contained in $\Omega$ , then there exists a function $\tilde u$ in $B_{\mathbb C^n}(p, r/\sqrt{2})$ which is holomorphic with respect to each variable and coincides with $u$ in $B_{\mathbb R^n}(p,r/\sqrt{2})$ . That is, $\tilde u$ is a function of $z_1, ..., z_n$ such that $\frac {\partial}{\partial \bar z_i} \tilde u = 0$ , $1 \leq i \leq n$ . I have two ideas on how to show this (hopefully): use that $u(x) = \int_{\partial B(p,r)} u(\xi) P(x,\xi)\,d\xi$ where $P$ is the Poisson kernel, and somehow change the Poisson kernel for an ""adequate complex extension"". use that $u(x) = \sum_{\alpha} a_\alpha P_\alpha(x-p)$ where $P_\alpha$ is a harmonic homogeneous polynomial of degree $\alpha$ (Taylor expansion of $u$ around $p$ ). Then, somehow ""complexify"" each harm. homog. pol. to obtain a power series. Do you know of any reference on how to complete these arguments? Or, ideally, a good reference where I can read about complexification of harmonic functions and applications? Also, I am interested in reference texts on holomorphic functions in multiple variables since I know very little about the topic.","['harmonic-functions', 'harmonic-analysis', 'complex-geometry', 'reference-request', 'complex-analysis']"
4580623,"What is the solution to $n_{i}=(e^{-x})\frac{dn_{i-1}}{dx}$ for i=0,1,2,...?","Given the recursion relationship $n_{i+1}=(e^{-x})\frac{dn_{i}}{dx}$ for $i=0,1,2,\ldots,$ where $n_{0}$ is a known function of $x$ , what is the solution to $n_{k}$ in terms of $x$ and the derivatives of $n_{0}$ , where $k$ is any positive integer? Re what I've tried, things simplify a bit if I express $n_0$ as $n_0=e^{y}$ and look for the solution in terms of the derivatives of $y$ (which may be expressed in terms of $x$ , $n_0$ , and the derivatives of $n_0$ as required by the question). We have then, $n_1=e^{y-x}\frac{dy}{dx}$ $n_{2}=e^{y-2x}\left(  \frac{d^{2}y}{dx^{2}}+\left(  \frac{dy}{dx}\right)^{2}-\frac{dy}{dx}\right)$ $n_{3}=e^{y-3x}\left(  \frac{d^{3}y}{dx^{3}}+3\frac{dy}{dx}\frac{d^{2}%
y}{dx^{2}}+\left(  \frac{dy}{dx}\right)  ^{3}-3\left(  \frac{dy}{dx}\right)
^{2}-3\frac{d^{2}y}{dx^{2}}+2\frac{dy}{dx}\right)  $ This gives a feel for where things are headed. The solution comes down to figuring out the integer coefficients of all the resultant derivatives and products of derivatives.","['calculus', 'derivatives', 'ordinary-differential-equations', 'sequences-and-series']"
4580662,Give the density of the product of independent Uniform random variables,"I had a lot of trouble doing this problem on my exam. Can anybody please tell me why my attempt fails? Let $X$ ~ $Uniform(0,1)$ and $Y$ ~ $Uniform(0,1)$ be independent random variables. Find the density of $Z = XY.$ Solution attempt:
For $a\in(0,1):$ $F_Z(a) = P(Z<a) = P(XY < a) = P(Y < \frac{a}X) = \int_0^1\int_0^{\frac{a}x}f_X(x)f_Y(y)dydx = \int_0^1\int_0^{\frac{a}x}1dydx = \int_0^1(\frac{a}x)dx = (aln|x|)\rvert_0^1 = aln(1) - aln(0) = -aln(0)$ which is undefined..","['statistics', 'probability-distributions', 'solution-verification', 'probability', 'density-function']"
4580707,I stuck on this step on the chapter about Lagrange multipliers.,"As I understood, to find critical solutions for some system of equations, we assume if giving a function f() restricted to some level set of other functions g i () with the same domain and range, then its gradient vector will be perpendicular to the tangent vector of the level set at a critical points of f() and can be expressed as a linear combination of gradient vectors of functions g i () at that point. Also to get particular results, we should have enough information for our linear equation. The book suggests to just put our linear combinations to restriction functions and solve for lambdas. That is the part that I'm struggling. I can't describe the meaning of this step and why we can acquire the answer without even using information from the distance function.",['multivariable-calculus']
4580727,"given that $h$ is differentiable and $h(3x)+h(2x)+h(x) = 0$ for all $x\in\mathbb{R},$ is $h$ necessarily identically zero?","Let $h:\mathbb{R}\to\mathbb{R}$ be a differentiable function such that $h(3x)+h(2x)+h(x) = 0$ for all $x\in\mathbb{R}.$ Determine with proof whether $h\equiv 0.$ I think $h$ is identically zero. Let $(1)$ be the original equation. Suppose for a contradiction that there exists some $x$ for which $h(x)\neq 0.$ Then differentiating (1) gives $3 h'(3x) + 2h'(2x)+h'(x)=0$ for all $x\in \mathbb{R}.$ Clearly $h(0) = 0,$ which can be seen by substituting $x=0$ into $(1)$ . Claim 1: For any $\epsilon > 0,$ and any $t\neq 0, |\dfrac{h(t)}{t}| < \epsilon.$ I'm not sure how to prove claim 1, but if it's true, then it immediately follows that h is identically zero.","['functional-equations', 'calculus', 'derivatives', 'real-analysis']"
4580771,Question on how to show that a set $A\times B$ is isomorphic to a coproduct of $B$ copies of $A$,"For the following question: Prove that the set $A\times B$ is isomorphic to a coproduct of $B$ copies of $A$ . [Hint:  Think of the plane as a union of horizontal lines.] Am i correct to interpret the hint as suggesting that I consider the map from $f:A\times B \rightarrow A\times \{\{b_i:i\in B, \wedge i\in I\}\times \{i\}\}$ , defined by the function $f(a,b_i)=(a, (b_i,i))$ and $a\in A$ , $b_i\in B$ for $i\in I$ .  Here $(b_i,i)$ denotes I am decomposing $B$ into singleton sets $\{(b_i,i)\}$ . Thank you in advance.",['elementary-set-theory']
4580792,Relationship between two triangle side lengths where one other side is shared,"This question was asked on an Australian year 10 (15 to 16 year olds) practice exam.
Diagram of two triangles with sides a and b indicated: ""Determine the relationship between the values of $a$ and $b$ by writing $a$ in terms of $b$ "". The solution given was simply the following. $a=\dfrac{b}{b \sqrt 3 - 1} \tag{1}\label{1}$ My attempt to solve this used the cosine rule on each of the two smaller triangles to get the side length opposite the $30°$ angle, then on the larger triangle for the side length opposite the $60°$ angle, giving the following relationship between a and b. $\left(\sqrt{a^2 + 1 - a \sqrt 3} + \sqrt{b^2 + 1 - b \sqrt 3}\right)^2 = a^2 + b^2 - ab \tag{2}\label{2}$ However I was not able to simplify (\ref{2}) to get equation (\ref{1}). My question is: How can equation (\ref{2}) be simplified to give equation (\ref{1}) using algebra that is accessible to a high school student? Is there another way, perhaps using other trigonometric identities, that does not use the form of (\ref{2})?","['trigonometry', 'algebra-precalculus', 'geometry']"
4580831,Why are the tangent spaces $T_pM$ and $T_qM$ disjoint for $p \neq q$?,"Let $p, q \in M$ , where $M$ is some smooth manifold. Then, according to Tu's book Introduction to Manifolds the tangent spaces $T_pM$ and $T_qM$ are disjoint. Of course we define the tangent bundle $TM$ as a disjoint union, but Tu claims that even if we had not done this the tangent spaces $T_pM$ and $T_qM$ in $TM$ would still be disjoint. Why is this? Is it because if we view $T_pM$ as the set of all derivations mapping $C_p^\infty(M)$ (the algebra of germs of $C^\infty$ functions at $p$ ) into $\mathbb{R}$ then $T_pM$ and $T_qM$ have different domains, and so the functions are by definition different? I am hoping that there is a more geometrical reason for this.","['tangent-spaces', 'tangent-bundle', 'smooth-manifolds', 'differential-geometry']"
4580934,Limit of $\sqrt[3]{(n+1)(n+2)(n+3)}-n$,"I came up with a Solution for $\displaystyle\lim_{n\to\infty}\sqrt[3]{(n+1)(n+2)(n+3)}-n$ That's my Solution: $\sqrt[3]{(n+1)(n+2)(n+3)}-n$ \begin{aligned}
		&=\frac{\left(\sqrt[3]{n^3+6 n^2+11 n+6}-n\right)\left(n^2+n \sqrt[3]{n^3+6 n^2+11 n+6}+\left(n^3+6 n^2+11 n+6\right)^{2 / 3}\right)}{\left(n^2+n \sqrt[3]{n^3+6 n^2+11 n+6}+\left(n^3+6 n^2+11 n+6\right)^{2 / 3}\right)}\\
		&=\frac{6 n^2+11 n+6}{n^2+n \sqrt[3]{n^3+6 n^2+11 n+6}+\left(n^3+6 n^2+11 n+6\right)^{2 / 3}}\\
		&\lim_{n\to \infty}\frac{n^2(6+\frac{11}{n}+\frac{6}{n^2})}{n^2\left(1+\frac{n \sqrt[3]{n^3+6 n^2+11 n+6}+\left(n^3+6 n^2+11 n+6\right)^{2 / 3}}{n^2}\right)}\\
		&=\lim_{n\to \infty}\frac{n^2(6+\frac{11}{n}+\frac{6}{n^2})}{n^2}\lim_{n\to \infty}\frac{1}{1+\frac{n \sqrt[3]{n^3+6 n^2+11 n+6}+\left(n^3+6 n^2+11 n+6\right)^{2 / 3}}{n^2}}\\
		&=6\frac{1}{\displaystyle{\lim_{n\to \infty}}1+\frac{n \sqrt[3]{n^3+6 n^2+11 n+6}+\left(n^3+6 n^2+11 n+6\right)^{2 / 3}}{n^2}}\\
		&=6\frac{1}{1+\displaystyle{\lim_{n\to \infty}}\frac{n \sqrt[3]{n^3+6 n^2+11 n+6}+\left(n^3+6 n^2+11 n+6\right)^{2 / 3}}{n^2}}\\
		&=\frac{6}{1+\displaystyle{\lim_{n\to \infty}}\frac{\sqrt[3]{n^3+6 n^2+11 n+6}}{n}+\frac{\left(n^3+6 n^2+11 n+6\right)^{2 / 3}}{n^2}}\\
		&=\frac{6}{1+\left(\displaystyle{\lim_{n\to \infty}}\frac{\sqrt[3]{n^3+6 n^2+11 n+6}}{n}\right)+\left(\displaystyle{\lim_{n\to \infty}}\frac{\left(n^3+6 n^2+11 n+6\right)^{2 / 3}}{n^2}\right)}\\
		&=\frac{6}{1+\left(\displaystyle{\lim_{n\to \infty}}\sqrt[3]{\frac{n^3+6 n^2+11 n+6}{n^3}}\right)+\left(\displaystyle{\lim_{n\to \infty}}\frac{\left(n^3+6 n^2+11 n+6\right)^{2 / 3}}{n^2}\right)}\\
		&=\frac{6}{1+\left(\displaystyle{\lim_{n\to \infty}}\sqrt[3]{\frac{1+\frac{6}{n}+\frac{11}{n^2}+\frac{6}{n^3}}{1}}\right)+\left(\displaystyle{\lim_{n\to \infty}}\frac{\left(n^3+6 n^2+11 n+6\right)^{2 / 3}}{n^2}\right)}\\
		&=\frac{6}{1+1+\left(\displaystyle{\lim_{n\to \infty}}\frac{\left(n^3+6 n^2+11 n+6\right)^{2 / 3}}{n^2}\right)}\\
		&=\frac{6}{1+1+\left(\displaystyle{\lim_{n\to \infty}}\left(\frac{n^3+6 n^2+11 n+6}{n^3}\right)^{2 / 3}\right)}\\
		&=\frac{6}{1+1+\left(\displaystyle{\lim_{n\to \infty}}\frac{1+\frac{6}{n}+\frac{11}{n^2}+\frac{6}{n^3}}{1}\right)^{2 / 3}}\\
		&=\frac{6}{1+1+1}=2\\
	\end{aligned} The question I wanna ask is there a more elegant way to solve this problem?",['limits']
4580935,Laurent series for $\frac{1}{(z-1)^2(z+2)}$,"Find Laurent expansions for $$\frac{1}{(z-1)^2(z+2)}$$ in $A_1 = D(0,1)$ , $A_2 = {z : 1 < |z| < 2}$ , $A_3 = {{z : \sqrt{2} < |z-i| < \sqrt{5}}}$ I've split it into partial fractions and am trying to apply Taylor's expansion to the square term and convergent sum of geometric to the other two. I'm confused because I'm not sure how you're meant to know the expansion point (like $z$ or $z-1$ etc). Please can someone tell me how to do the $A_2$ scenario? Thanks in advance.","['complex-analysis', 'laurent-series']"
4580990,Which assumptions on $f$ and $T$ make $\int_0^T f(x) dx \le \int_0^T |f(x)|^2 dx\implies 1\le \int_0^T |f(x)| dx$ true?,"Let $T>0$ be fixed and let $f$ be a real valued function such that $$\|f\|_{\infty}\le T,$$ where $\|\cdot\|_{\infty}$ denotes the sup-norm. My question is the following: if $$\int_0^T f(x) dx \le \int_0^T |f(x)|^2 dx,$$ is that true $$\int_0^T f(x) dx \le \int_0^T |f(x)|^2 dx\iff 1\le \int_0^T |f(x)| dx \, ?$$ On the spot I would say that it is not true. Could someone please help me to understand under which assumptions on $f$ and/or $T$ does the previous equivalence hold true? Thank you in advance! $\bf{EDIT:}$ Also partial answer will be accepted (in particular, I am mostly interested in the condition "" $\implies$ ""). Thank you.","['analysis', 'real-analysis', 'calculus', 'functions', 'inequality']"
4581028,How many partitions of the set $\Bbb Q$ are there?,"Let $\mathcal S:=\{P\mid P\text{ is a partition of }\Bbb Q\}.$ Find $\operatorname{card}(\mathcal S).$ My attempt: $\boxed{\leq}:$ I believe each $P\in\mathcal S$ may be determined by a surjection $f_P:\Bbb Q\to\Bbb N,$ that is, each $B\in P$ is of the form $f_P^{-1}(n)$ for some $n\in\Bbb N$ and, because $f_P$ is surjective, there is no empty set in the partition $P.$ Let $\mathcal T:=\{f:\Bbb Q\to\Bbb N\mid f\text{ is surjective }\}.$ Then $\mathcal T\subseteq\{f:\Bbb Q\to\Bbb N\}\implies\operatorname{card}(\mathcal T)\le\aleph_0^{\aleph_0}=\mathfrak c$ because $$\mathfrak c=2^{\aleph_0}\le\aleph_0^{\aleph_0}\le\mathfrak c^{\aleph_0}=\left(2^{\aleph_0}\right)^{\aleph_0}=2^{\aleph_0\cdot\aleph_0}=2^{\aleph_0}=\mathfrak c.$$ Therefore, $\operatorname{card}(\mathcal S)\le\mathfrak c.$ $\boxed{\geq }:$ Let's define $\Phi:\mathcal P(\color{blue}{[0,1]\cap\Bbb Q})\to\mathcal S,\Phi: A\mapsto\Phi(A)$ $$\Phi(A):=\begin{cases}\boldsymbol{\{}\Bbb Q\setminus(\color{blue}{\Bbb Q\cap [0,1]}\cup\{2\}),A,\{2\}\cup \color{blue}{\Bbb Q\cap [0,1]}\setminus A\boldsymbol{\}}, &A\ne\emptyset,\\\boldsymbol{\{}\Bbb Q\boldsymbol{\}},& A=\emptyset.\end{cases}$$ I think $\Phi(A)$ is well defined since all its elements are disjoint, non-empty and their union is $\Bbb Q.$ I took out $\{2\}$ in order for $\Phi(A)$ and $\Phi([0,1]\cap\Bbb Q\setminus A)$ to be different according to the axiom of extensionality and for $\Phi$ to be injective.
Hence $\mathfrak c=2^{\aleph_0}\operatorname{card}\mathcal P(\Bbb Q\cap[0,1])\le\operatorname{card}(\mathcal S).$ Finally, $\operatorname{card}(\mathcal S)=\mathfrak c.$ I would like to verify my work. Is my answer valid?","['elementary-set-theory', 'solution-verification']"
4581061,Probability of Transitive Tournament,"n people participate in a tournament, every 2 people compete each other.
A tournament is called transitive if it fulfills the following property: For every combination of three players, if player A beat player B and
player B beat player C, then player A also beat player C. Show that a tournament is transitive if and only if there is an arrangement of the participants such that a certain participant defeated another participant if and only if he appears before him in the arrangement, and this arrangement is unique. Calculate the probability that random tournament is transitive, as the quotient of the number of transitive tournaments and the number of tournaments in general. Thoughts I can prove the first part by finding an arrangement according to the number of wins of each player and doing induction, but not sure how to apply the induction step?. In the second part, according to the definition of a tournament graph I will get $\left|\Omega\right|=2^{{n \choose 2}}$ but how do I find the event?","['graph-theory', 'probability-theory']"
4581069,Does the set have to be pretty explicitly given as an element of the topology?,"So I've been trying to grasp the concept of what is and isn't a topological space recently, and it has brought me upon some questions that I'm on the fence as to whether they are topological spaces or not. Suppose for example we have: $$
\tau = \{ U \subseteq \mathbb{R}^2: \text{U is an open disc or } \emptyset \} 
$$ This is extremely vague to me. If we chose $(x,y)$ tending towards infinity, we would get very close to the set $X$ being included in $\tau$ , but the fact that it's never completely within makes me want to say that it's not a topological space. I think it will also fail under the union/intersection of a number of open discs, but the main issue is in understanding whether it violates the rule that the set itself and the empty set must be elements of the topological space.","['elementary-set-theory', 'general-topology']"
4581087,The Characteristic Property of Disjoint Union Spaces (Understanding theorem statement),"I am reading Introduction to Topological Manifolds by Lee, and I have a question about the statement of Theorem 3.41 (Characteristic Property of Disjoint Union Spaces) on page 64: Theorem 3.41 (Characteristic Property of Disjoint Union Spaces) Suppose that $(X_{\alpha})_{\alpha \in A}$ is an indexed family of topological spaces, and $Y$ is any topological space. A map $f: \bigsqcup_{\alpha \in A} X_{\alpha} \to Y$ is continuous if and only if its restriction to each $X_{\alpha}$ is continuous. The disjoint union topology is the unique topology on $\bigsqcup_{\alpha \in A} X_{\alpha}$ with this property. My questions: Since the $X_{\alpha}$ 's are not subsets of $\bigsqcup_{\alpha \in A} X_{\alpha}$ , the statement the "" $f: \bigsqcup_{\alpha \in A} X_{\alpha} \to Y$ is continuous if and only if its restriction to each $X_{\alpha}$ is continuous "" cannot be literally true. In a preceding paragraph, the author says, For each $\alpha \in A$ , there is a canonical injection $\iota_{\alpha}: X_{\alpha} \to \bigsqcup_{\alpha \in A} X_{\alpha}$ given by $\iota_{\alpha}(x) = (x,\alpha)$ , and we usually identify each set $X_{\alpha}$ with its image $X_{\alpha}^{*} = \iota_{\alpha}(X_{\alpha})$ . Based on the above line about ""identifying"" $X_{\alpha}$ with $X_{\alpha}^{*}$ , my best guess is that the author means the following: $$ f: \bigsqcup_{\alpha \in A} X_{\alpha} \to Y \text{ is continuous iff }\; f|_{X_{\alpha}^{*}}: X_{\alpha}^{*} \to f(X_{\alpha}^{*}) \; \text{is continuous for all } \alpha \in A. \hspace{1cm} (\dagger) $$ Is this the correct interpretation? If $(\dagger)$ is correct, then what are the topologies on $X_{\alpha}^{*}$ and $f(X_{\alpha}^{*})$ assumed to be? Without knowing the topologies on the domain and range of $f|_{X_{\alpha}^{*}}$ , the statement "" $f|_{X_{\alpha}^{*}}$ is continuous"" is ambiguous to me. In looking at this very similar question (where they consider the case where $f$ is an identity map), according to Brian M. Scott, ""the subspaces are assumed to have their given topologies."" How then is the topology on $X_{\alpha}^{*}$ , call it $\tau_{\alpha}^{*}$ , related to the ""original"" topology, call it $\tau_{\alpha}$ , on $X_{\alpha}$ ?  The discussion in that post omitted the $*$ notation, which left me a little confused. My (probably wildly wrong) guess here is that $\tau_{\alpha}^{*} = \{B \times \{\alpha\}: B \in \tau_{\alpha} \}$ ; in other words, $\tau_{\alpha}^{*}$ is obtained by simply ""tagging"" all sets in $\tau_{\alpha}$ with $\alpha$ . It would help greatly to have an answer that uses the most precise notation possible (even if it's  clunky!); I find the whole ""identify each $X_{\alpha}$ with its image $X_{\alpha}^{*}$ "" to lead to ambiguity and confusion... Update 11/21/22: Thanks to drhab and  Paul Frost for confirming that $\tau_{\alpha}^{*} = \{B \times \{\alpha\}: B \in \tau_{\alpha} \}$ . Here is a (very pedantic) proof of this fact. Proposition. Let $(X_{\alpha},\tau_{\alpha})_{\alpha \in A}$ be an indexed collection of topological spaces, let $X := \bigsqcup_{\alpha \in A} X_{\alpha}$ , and let $\tau := \left\{B \subseteq X: \iota_{\alpha}^{-1}(B) \in \tau_{\alpha} \text{ for all } \alpha \in A  \right\}$ be the disjoint union topology on $X$ , where $\iota_{\alpha}: X_{\alpha} \to X$ is the canonical injection $\iota_{\alpha}(x) = (x,\alpha)$ . Let $\tau_{\alpha}^{*}$ denote the subspace topology on $X_{\alpha}^{*} := \iota_{\alpha}(X_{\alpha})$ ; that is, $\tau_{\alpha}^{*} := \{ X_{\alpha}^{*} \cap B: B \in \tau \}$ . Then \begin{align*}
    \tau_{\alpha}^{*} = \{U \times \{\alpha\}: U \in \tau_{\alpha} \}.
\end{align*} Proof . Fix $\alpha \in A$ and let $V_0 \in \tau_{\alpha}^{*}$ . Then $V_0 = X_{\alpha}^{*} \cap U_0$ for some $U_0 \in \tau$ . Then \begin{align*}
    V_0 &= \iota_{\alpha}(X_{\alpha}) \cap \iota_{\alpha}(\iota_{\alpha}^{-1}(U_0))  \\[4pt]
      &= \iota_{\alpha}(X_{\alpha} \cap \iota_{\alpha}^{-1}(U_0)) && (\text{since } \iota_{\alpha} \text{ is injective}) \\[4pt]
      &= (X_{\alpha} \cap \iota_{\alpha}^{-1}(U_0)) \times \{\alpha\}.
\end{align*} Now $X_{\alpha} \in \tau_{\alpha}$ by the fact that $\tau_{\alpha}$ is a topology on $X_{\alpha}$ , and $\iota_{\alpha}^{-1}(U_0) \in \tau_{\alpha}$ by the fact that $U_0 \in \tau$ . Therefore, $X_{\alpha} \cap \iota_{\alpha}^{-1}(U_0) \in \tau_{\alpha}$ , and so $V_0 \in \{U \times \{\alpha\}: U \in \tau_{\alpha}\}$ . This proves that $\tau_{\alpha}^{*} \subseteq \{U \times \{\alpha\}: U \in \tau_{\alpha}\}$ . Now suppose $V_0 \in \{U \times \{\alpha\}: U \in \tau_{\alpha} \}$ . Then $V_0 = U_0 \times \{\alpha\}$ for some $U_0 \in \tau_{\alpha}$ . Now note that \begin{align*}
        \iota_{\beta}^{-1}(V_0) = \iota_{\beta}^{-1}(U_0 \times \{\alpha\}) = 
        \begin{cases}
            U_0 & \text{if } \beta = \alpha, \\[2pt]
            \emptyset & \text{if } \beta \neq \alpha.
        \end{cases}
\end{align*} Since $U_0 \in \tau_{\beta}$ when $\beta = \alpha$ , and since $\emptyset \in \tau_{\beta}$ for all $\beta$ , we have $\iota_{\beta}^{-1}(V_0) \in \tau_{\beta}$ for all $\beta \in A$ . Therefore, $V_0 \in \tau$ . It follows that $V_0 \in \tau_{\alpha}^{*}$ . [This is by the following fact: If $U$ is a subspace of $S$ , and $S$ is a subspace of $X$ , then a subset of $S$ that is open in $X$ is also open in $S$ .] This shows that $\{U \times \{\alpha\}: U \in \tau_{\alpha} \} \subseteq \tau_{\alpha}^{*}$ , which completes the proof. $\qquad \square$",['general-topology']
4581104,Eigen values of the ODE $(1+x^2)y’’+2xy’+\lambda x^2y=0$.,"How to find eigen values of the Sturm Liouville Boundary value problem $$(1+x^2)y’’+2xy’+\lambda x^2y=0, y’(1)=0, y’(10)=0?$$ I know how to solve Sturm Liouville problem of the form $y’’+\lambda =0$ by making three cases as $\lambda=0,\lambda>0$ and $\lambda <0$ . In this Problem I only know that $\lambda=0$ is an eigen value because for $\lambda=0$ there are non constant solutions. I don’t know to discuss all other eigenvalues . Please help . Thank you.","['boundary-value-problem', 'sturm-liouville', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
4581115,Difference between subspace and subset in topology,"In Murkres's book Topology, I read that Y(a subset of some set X）is called the subspace of X if we consider the subspace topology. But what is the difference here between being a subspace and a subset? the definition in the book",['general-topology']
4581151,Is a Polish subgroup of a Lie group Lie?,"Let $G$ be a Lie group. Suppose that $H\le G$ is a subgroup such that endowed with the subspace topology it is a Polish space. Does this imply that $H$ is a Lie group? I know that in case $H$ is closed the answer is yes by Cartan's theorem, but I do not know the answer in this relaxed case.","['group-theory', 'topological-groups', 'lie-groups']"
4581219,"$f,g$ are nonnegative decreasing and $\int_0^x f \le \int_0^x g$ for each $x \in [0,1]$ then $\int_0^1 f^2 \le \int_0^1 g^2$","Here's what I am trying to prove: Let $f,g :[0,1] \to \mathbb R$ be nonnegative decreasing functions and suppose that $\int_0^x f \le \int_0^x g$ holds for each $x \in [0,1]$ then show that $\int_0^1 f^2 \le \int_0^1 g^2$ . I tried to prove it but I think there's possibly some errors. Here's my attempt, nonetheless: Let $\alpha =\sup \{ x\in [0,1] : g(x)<f(x) \}$ . Now, if $y>\alpha$ then $g(y)\ge f(y)$ . Since $g$ and $f$ are decreasing, we have $(g-f)(g+f)\ge (g(1)+f(1) )(g-f)$ on $(\alpha,1]$ and hence $\int_\alpha ^1 (g^2-f^2) \ge0$ since $g-f \ge 0$ on $(\alpha,1].$ Since $g$ and $f$ are decreasing, we have $(g-f)(g+f)\ge (g(1)+f(1) )(g-f)$ on $[0,\alpha)$ and hence $\int_0 ^\alpha (g^2-f^2) \ge0$ since $\int_0^\alpha (g-f) \ge 0$ by assumption. And thus, the result follows. Is my attempt correct? Also alternative proofs are welcome!","['measure-theory', 'lebesgue-integral']"
4581220,Cauchy principal value: methods,"I don't understand yet the logic in the Cauchy Principale Value (P.V.) calculations. Let the resideu theorem: $$\color{red}{\oint_Cf(z) \ dz = 2\pi i \sum_{k=1}^n \underset{z=z_k}{Res}\{f(z)\}}$$ (we supposed that $C$ was a ""nice"" closed curb and $f(z)$ analytical $\forall z \in \mathrm{int} \ C$ except in $z_k$ ( $1 \le k \le n$ )) I've got 2 poorly justified examples in my exercice book (forgive me for non-mathjaxing all of that follows) : Apparently, the corrector found a nice closed curb, but don't want to share how he did it. Usually, when $f(z)$ is pair, we can define such a $C$ curb: but it doesn't explain why the 2 factor is missing in the answer . Why $\pi i$ instead of $2\pi i$ ? Second example: $$P.V. \int_{-\infty}^{+\infty} \frac{\exp(-4ix)}{(x+i)^2} \ dx$$ Here, we have a reversed cup (why reversed ?) $\gamma (R)$ with the segment $C(R)$ : Again, I don't understand what has been done. Is there a general methode I can understand to solve those 2 exercices ? EDIT : corrected one error pointed out in the comments ( $(x+i)^2$ instead of $(x+1)^2$ )","['integration', 'complex-analysis', 'cauchy-principal-value', 'residue-calculus', 'complex-integration']"
4581236,Surface area of portion of $n$ sphere such that $\sum_{i=1}^nx_i\leq 0$ and $x_n\leq 0$.,"I want to find the proportion of the surface area of those points on the surface of the unit $n$ sphere whose coordinates sum to a nonpositive number and for which the last coordinate is nonpositive. My attempt at doing this is to try and calculate $$\int_{Q} 1 \ dS$$ where $Q$ is the region in $\mathbb{R}^n$ given by the conditions $\sum_{i=1}^n x_i\leq 0$ , $x_n\leq 0$ and $\sum_{i=1}^n x_i^2=1$ and then divide by the surface area of the $n$ sphere. For example, in $\mathbb{R}^2$ , this is the arc length of the unit circle from $\pi$ to $7\pi/4$ . The proportion is therefore $3\pi/4/ (2\ \pi)=3/8$ .  I am not sure if computing a surface integral is the best way to proceed but for now I cannot think of any other method of doing this. I know that the $n$ sphere can be parameterized using spherical coordinates but my issue is that I cannot find the appropriate integral bounds to carry out this computation. In spherical coordinates this reduces to solving for $(\theta_1,\ldots,\theta_{n-1})\in [0,\pi]^{n-2}\times [0,2\pi)$ such that $$\cos(\theta_1)+\sin(\theta_1)\cos(\theta_2)+\ldots+\cos(\theta_{n-1})\Pi_{i=1}^{n-2}\sin(\theta_i)+\Pi_{i=1}^{n-1}\sin(\theta_i)\leq 0$$ and $$\Pi_{i=1}^{n-1}\sin(\theta_i)\leq 0.$$ Due to $\sin(x)\geq0$ for $x\in [0,\pi]$ , the last condition gives $\theta_{n-1}\in [\pi,2\pi)$ . I am trying to figure out how to work with the first condition. I want to possibly compute ""similar"" surface areas on the unit $n$ sphere so I want to find a way of doing it that can be generalized.","['spheres', 'surface-integrals', 'geometry', 'multivariable-calculus', 'spherical-coordinates']"
4581250,Defined function $f:\mathbb{R} \to \mathbb{R}$ which satisfies $f(2xy) + f(f(x + y)) + f(x + y) = xf(y) + yf(x)$,"Defined function $f:\mathbb{R} \to \mathbb{R}$ which satisfies $$f(2xy) + f(f(x + y)) + f(x + y) = xf(y) + yf(x)$$ for any real numbers $x$ and $y$ . Determine the sum of all values $|f(48)|$ which possible. My working: $$f(2xy) + f(f(x + y)) + f(x + y) = xf(y) + yf(x)$$ Assume: $y=-x$ $$f(2x^2) + f(f(0)) + f(0) = xf(-x) -xf(x)$$ $$2f(x^2) + f(f(0)) + f(0) = -xf(x) -xf(x)$$ $$2f(x^2) + f(f(0)) + f(0) = -2xf(x)$$ $$2f(x^2) + f(f(0)) + f(0) = -2f(x^2)$$ $$f(f(0)) + f(0) = -2f(x^2)-2f(x^2)$$ $$f(f(0)) + f(0) = -4f(x^2)$$ so, I still have doubts about my work with the process. how?","['contest-math', 'functional-equations', 'functions']"
4581259,Question about Szemeredi Graph Regularity Lemma,"The below argument is from Zhao's book ""Graph Theory and Additive Combinatorics"". Definition 2.1.2 ( $\varepsilon$ -regular pair) Let $G$ be a graph and $U,W\subseteq V(G)$ . We call $(U,W)$ an $\varepsilon$ - regular pair in $G$ if for all $A\subseteq U$ and $B\subseteq W$ with $|A|\geq \varepsilon |U|$ and $|B|\geq \varepsilon
 |W|$ , one has $$|d(A,B)-d(U,W)|\leq \varepsilon.$$ If $(U,W)$ is not $\varepsilon$ -regular, then we say that their irregularity is witnessed by some $A\subseteq U$ and $B\subseteq W$ satisfying $|A|\geq \varepsilon |U|$ , $|B|\geq \varepsilon |W|$ , and $|d(A,B)-d(U,W)|> \varepsilon.$ Definition 2.1.7 ( $\varepsilon$ -regular partition) Given a graph $G$ , a partition $\mathcal{P}=\{V_1,\dots,V_k\}$ of its
vertex set is an $\varepsilon$ - regular partition if $$\sum_{\substack{(i,j)\in [k]^2\\\text{$(V_i,V_j)$ not
 $\varepsilon$-regular}}}|V_i||V_j|\leq \varepsilon |V(G)|^2.$$ Now we can formulate Regularity Lemma: Theorem 2.1.9 (Szemeredi's graph regularity lemma) For every $\varepsilon>0$ , there exists a constant $M$ such that every graph
has an $\varepsilon$ -regular partition into at most $M$ parts. In order to prove this fundamental result we need to introduce the following definition: Definition 2.1.10 (Energy) Let $G$ be an $n$ -vertex graph. Let $U,W\subseteq V(G)$ . Define $$q(U,W):=\frac{|U||W|}{n^2}d(U,W)^2,$$ where $d(U,W)$ is an edge density between $U$ and $W$ . For partitions $\mathcal{P}_{U}=\{U_1,\dots,U_k\}$ of $U$ and $\mathcal{P_W}=\{W_1,\dots,W_l\}$ of $W$ , define $$q(\mathcal{P}_U,\mathcal{P}_W):=\sum
 \limits_{i=1}^{k}\sum_{j=1}^{l}q(U_i,W_j).$$ Finally, for a partition $\mathcal{P}=\{V_1,\dots,V_k\}$ of $V(G)$ , define its energy to be $$q(\mathcal{P}):=q(\mathcal{P},\mathcal{P})=\sum
 \limits_{i,j=1}^{k}q(V_i,V_j)=\sum
 \limits_{i,j=1}^{k}\frac{|V_i||V_j|}{n^2}d(V_i,V_j)^2.$$ Since the edge density is always between $0$ and $1$ , we have $0\leq q(\mathcal{P})\leq 1$ for all partitions $\mathcal{P}$ . The following lemma plays an important role in the proof of Graph Regularity Lemma. Lemma 2.1.14 (Energy boost for an irregular partition) If a partition $\mathcal{P}=\{V_1,\dots,V_k\}$ of $V(G)$ is not $\varepsilon$ -regular, then there exists a refinement $\mathcal{Q}$ of $\mathcal{P}$ where every $V_i$ is partitioned into at most $2^{k+1}$ parts, and such that $$q(\mathcal{Q})>q(\mathcal{P})+\varepsilon^5.$$ Proof of the graph regularity lemma (Theorem 2.1.9). Start with a trivial partition of the
vertex set of the graph. Repeatedly apply Lemma 2.1.14 whenever the current partition is
not $\varepsilon$ -regular. By Lemma 2.1.14, the energy of the partition increases by more than $\varepsilon^5$ at each iteration. Since the energy of the partition is $\leq 1$ , we must stop after $<\varepsilon^{-5}$ iterations, terminating in an $\varepsilon$ -regular partition. If a partition has $k$ parts, then Lemma 2.1.14 produces a refinement with $\leq k2^{k+1}$ parts. We start with a trivial partition with one part, and then refine $<\varepsilon^{-5}$ times. Observe the crude bound $k2^{k+1}\leq2^{2^k}$ . So the total number of parts at the end is $\leq \text{tower}(\lceil 2\varepsilon^{-5}\rceil)$ (In the book author writes ceiling function, but I believe one can take floor function as well, i.e. I mean that the total number of parts at the end is $\leq \text{tower}(\lfloor 2\varepsilon^{-5}\rfloor)$ ), where $$\text{tower}(k):=2^{2^{.^{.^{.^{2}}}}} \Bigg\} \text{height $k$}$$ Question: The regularity lemma is quite flexible. For example, we can start with
an arbitrary partition of $V(G)$ instead of the trivial partition in
the proof, in order to obtain a partition that is a refinement of a
given partition. The exact same proof with this modification yields
the following. Theorem 2.1.19 (Regularity starting with an arbitrary intiial partition) For every $\varepsilon>0$ , there exists a constant $M$ such that for
every graph $G$ and a partition $\mathcal{P}_0$ of $V(G)$ , there
exists an $\varepsilon$ -regular partition $\mathcal{P}$ of $V(G)$ that
is a refinement of $\mathcal{P}_0$ , and such that each part of $\mathcal{P}_0$ is refined into at most $M$ parts. I don't think that it follows easily and directly as we did in original formulation of regularity lemma. This is what I've done so far. Let $\varepsilon>0$ be given and take any graph $G$ and fix some partition $\mathcal{P}_0=\{V_1,\dots,V_k\}$ of $V(G)$ . If $\mathcal{P}_0$ is $\varepsilon$ -regular, then we are done. Otherwise, we apply Lemma 2.1.14  and obtain a partition $\mathcal{P}_1$ of $V(G)$ which refines $\mathcal{P}_0$ such that $|\mathcal{P}_1|\leq k2^{k+1}$ and $q(\mathcal{P}_1)>q(\mathcal{P}_0)+\varepsilon^5$ . By the same reasoning at the $N$ th step we have an $\varepsilon$ -partition $\mathcal{P}_N$ , where $N<\varepsilon^{-5}$ and $$|\mathcal{P}_N|\leq 2^{2^{\dots^{2^k}}} \Bigg \} \text{height $(2N+1)$}.$$ But we see that the size of $\mathcal{P}_N$ depends depends also on $k=|\mathcal{P}_0|$ . We know that $2N+1\leq \lfloor 2\varepsilon^{-5}\rfloor+1$ . But I am wondering is it possible to give an upper bound for $k$ in terms of $\varepsilon$ ? Can anyone help me please? P.S. Unfortunately, my edit was wrong so I've decided to give bounty on this question. I worked hard on this question for 3-4 days but I don't know the answer. EDIT: I think that I understood the claim. It was much simpler than I thought before. Suppose we fix some partition $\mathcal{P}_0$ of $V(G)$ . We already know that if we start from trivial partition, then we end up in $\varepsilon$ -regular partition (say $\mathcal{P}'$ ) in fewer than $\text{tower}(\lfloor 2\varepsilon^{-5} \rfloor)$ steps. However, if we intersect $\mathcal{P}_0$ with partitions (by intersection I mean a common refinement of two partitions) at each step and since energy does not decrease under refinement, then we still obtain an $\varepsilon$ -regular partition which refines $\mathcal{P_0}$ and the number of parts in this partition is $\leq |\mathcal{P}_0|\text{tower}(\lfloor 2\varepsilon^{-5} \rfloor)$ . Does it make sense?","['graph-theory', 'extremal-combinatorics', 'combinatorics', 'discrete-mathematics', 'extremal-graph-theory']"
4581304,Finding primitive roots including negative sign,"I commonly run into the following question such that if $p$ and $q=4p+1$ are both odd primes prove that $2$ is primitve root modulo q . However , i could not prove it for other number that are given at the end , so can you help me for them , how should we approach them when they are negative , i could not deduce the solution What i obtained: The part for $2$ , i have the following : Note that $q\not=2$ since $4\cdot2+1=9$ is not prime. $\mathrm{ord}_p(2)\vert p-1=4q$ , so $\mathrm{ord}_p(2)=1,\;2,\;4,\;q,\;2q,\;\mathrm{or}\;4q$ . Clearly $\mathrm{ord}_p(2) \not= 1$ , and $\mathrm{ord}_p(2)\not=2$ since $4\equiv1(\text{mod }p) \implies p=3$ but $3\not=4q+1$ for any positive integer $q$ . Also $\mathrm{ord}_p(2)\not=2$ because $2^4=16\equiv1(\text{mod }p)\implies p=3 \text{ or } 5$ . It has been shown that $p\not=3$ and $p=5\implies q=1$ which is not prime. Suppose $\mathrm{ord}_p(2)=q$ . Then $2^q\equiv1(\text{mod }p)$ . Let $g$ be a primitive root modulo $p$ , so that $2\equiv g^i(\text{mod }p)$ for some $i\in\mathbb{Z}$ . Then $$g^{iq}\equiv1(\text{mod }p)\implies p-1\vert iq\implies iq=k(p-1)=4kq\implies i=4k$$ for some $k\in\mathbb{Z}$ . So $2\equiv g^{4k}(\text{mod }p)$ and $2$ is a square modulo $p$ , which means $p\equiv\pm1(\text{mod }8)$ . Hence, either $8\vert p-1$ or $8\vert p+1$ . If $8\vert p-1$ , then $p-1=4q=8l\implies q=2l$ for some $l\in\mathbb{Z}$ , so $q$ is even, which is impossible since $q\not=2$ . If instead $8\vert p+1$ , then $p+1=4q+2=8l\implies2q+1=2l$ for some $l\in\mathbb{Z}$ , which is impossible. Thus $\mathrm{ord}_p(2)\not=q$ . Suppose $\mathrm{ord}_p(2)=2q$ . Then $2^{2q}\equiv1(\text{mod }p)$ . Let $g$ be a primitive root modulo $p$ , so that $2\equiv g^i(\text{mod }p)$ for some $i\in\mathbb{Z}$ . Thus $$g^{2iq}\equiv1(\text{mod }p)\implies p-1\vert 2iq\implies2iq=4kq\implies i=2k$$ for some $k\in\mathbb{Z}$ , so $2$ is a square modulo $p$ , which has been shown to be false. Therefore $\mathrm{ord}_p(2)\not=2q$ . Hence $\mathrm{ord}_p(2)=4q=p-1$ and 2 is a primitive root modulo $p$ . However , i stuck in showing that $-2$ is also primitive roots modulo $q$ . Moreover , how can we show that $3$ and $-3$ are also primitve roots if $p >3$","['cryptography', 'elementary-number-theory', 'discrete-mathematics', 'primitive-roots']"
4581306,"Convergence, continuity and differentiability of $f(x)=\frac{1}{x}-\frac{1}{x+1}+\frac{1}{x+2}-\frac{1}{x+3}+\ldots$","I am self-learning Real Analysis from the text, Understanding Analysis by Stephen Abbott. I'd like someone to verify, if my below proof and deductions are rigorous and technically correct. [Abbott 6.4.6] Let \begin{equation*}
f( x) =\frac{1}{x} -\frac{1}{x+1} +\frac{1}{x+2} -\frac{1}{x+3} +\frac{1}{x+4} -\dotsc 
\end{equation*} Show that $\displaystyle f$ is defined for all $\displaystyle x >0$ . Is $\displaystyle f$ continuous on $\displaystyle ( 0,\infty )$ ? How about differentiable? Proof . Well-definedness of $\displaystyle f$ . Define \begin{equation*}
f_{n}( x) =\frac{( -1)^{n}}{x+n}
\end{equation*} Then, \begin{equation*}
f( x) =\sum _{n=0}^{\infty } f_{n}( x)
\end{equation*} Let $\displaystyle x_{0}$ be an arbitrary point, such that $\displaystyle x_{0}  >0$ . Fix $\displaystyle x=x_{0}$ . We have: \begin{equation*}
\frac{1}{x_{0}} \geq \frac{1}{x_{0} +1} \geq \frac{1}{x_{0} +2} \geq \dotsc \geq 0
\end{equation*} Moreover, \begin{equation*}
\lim \frac{1}{x_{0} +n} =0
\end{equation*} By the Alternating Series Test for convergence, $\displaystyle \sum _{n=0}^{\infty } f_{n}( x)$ converges pointwise on $\displaystyle x >0$ . Continuity of $\displaystyle f$ . Let $\displaystyle [ a,\infty )$ be any interval such that $\displaystyle a >0$ . Let us group each of pair of terms of $\displaystyle f$ and write: \begin{equation*}
\begin{array}{ c l }
f( x) & =\left(\frac{1}{x} -\frac{1}{x+1}\right) +\left(\frac{1}{x+2} -\frac{1}{x+3}\right) +\left(\frac{1}{x+4} -\frac{1}{x+5}\right) +\dotsc 
\end{array}
\end{equation*} Define: \begin{equation*}
g_{n}( x) =\frac{1}{( x+2n)} -\frac{1}{( x+2n+1)} =\frac{1}{( x+2n)( x+2n+1)}
\end{equation*} Then, \begin{equation*}
f( x) =\sum _{n=0}^{\infty } g_{n}( x)
\end{equation*} Now, $\displaystyle g_{0}( x) =\frac{1}{a( a+1)} =M_{0}$ . Moreover, \begin{equation*}
0\leq g_{n}( x) \leq \frac{1}{( 2n)( 2n+1)} \leq \frac{1}{4n^{2}} =M_{n}
\end{equation*} for all $\displaystyle n\geq 1$ . Since $\displaystyle \sum _{n=0}^{\infty } M_{n}$ converges, by the Weierstrass $\displaystyle M$ -Test, $\displaystyle \sum_{n=0}^{\infty } g_{n}( x)$ converges uniformly on $\displaystyle [ a,\infty )$ for any $\displaystyle a >0$ . Since each $\displaystyle g_{n}( x)$ is continuous for $\displaystyle x >0$ , by the Term-by-term continuity theorem, $\displaystyle f( x)$ is continuous on $\displaystyle [ a,\infty )$ , where $\displaystyle a >0$ . Thus, $\displaystyle f$ is continuous on $\displaystyle ( 0,\infty )$ . Differentiability of $\displaystyle f$ . Since each $\displaystyle g_{n}( x)$ is differentiable for $\displaystyle x >0$ , by the Term-by-Term differentiability theorem, $\displaystyle f$ is differentiable on $\displaystyle [ a,\infty )$ where $\displaystyle a >0$ . Thus, $\displaystyle f$ is differentiable on $\displaystyle ( 0,\infty )$ .","['sequence-of-function', 'real-analysis', 'solution-verification', 'uniform-convergence', 'sequences-and-series']"
4581411,Proving that the $n \times n$ Hilbert matrix is positive definite,"Prove that the following matrix is positive definite. $$ A = \begin{bmatrix} 1 & \frac12 & \dots & \frac1n \\ \frac12 & \frac13 & \dots & \frac1{n+1} \\ \vdots  & \vdots & \ddots & \vdots \\ \frac1n & \frac1{n+1} & \dots & \frac1{2n-1} \end{bmatrix} $$ I tried to do a couple of thing without succes (it's immediate that $A=A^t$ ): First: I tried to show this by definition: In this case, the matrix is real so, let $v^t=(x_1,...,x_n)^t$ be a real vector, I tried to show that $v^tAv>0$ . I then arrive to $$x_1^2+x_1x_2+\frac{2x_1x_3}{3}+\dots+\frac{x_2x_n}{n+1}+\frac{x_n^2}{2n-1}$$ but I don't know how to show that this last equation is $>0$ , is there some identity to show this? Second , I tried to define the matrix as a inner product, that is $a_{ij} = \langle v_j, v_i \rangle$ where $\{v_1,\dots,v_n\}$ is a ortonormal base for $\mathbb{R}^n$ . How can I define those $v_j$ 's? Third , I tried to show that the minors of the matrix are all positives, I tried to do it by induction over the matrix size, for $n=1$ is trivial because $A$ is $1>0$ in this case, for $n=2$ , we have $$A=\begin{bmatrix}
1 & 1/2 \\
1/2 & 1/3 \\
\end{bmatrix}$$ and then $$\begin{vmatrix}
1 & 1/2 \\
1/2 & 1/3 \\
\end{vmatrix}=1/3-1/4>0$$ If we suposse that for size $n-1$ we have determinant positive then I tried to calculate $\det(A)$ using determinants of size $n-1$ all positives. (I did it expanding by the first column): $$\det(A)=1\det\begin{bmatrix}
1/3 & \dots & 1/n+1 \\
\vdots & \vdots & \vdots \\
1/n+1 & \dots & 1/2n-1 \\
\end{bmatrix}-\frac{1}{2}\det\begin{bmatrix}
1/2 & \dots & 1/n \\
\vdots & \vdots & \vdots \\
1/n+1 & \dots & 1/2n-1 \\
\end{bmatrix}+\dots+(-1)^n\frac{1}{n}\det\begin{bmatrix}
1/2 & \dots & 1/n \\
\vdots & \ddots & \vdots \\
1/n & \dots & 1/2n-2 \\
\end{bmatrix}$$ Then, how can I do to show $\det(A) > 0$ keeping in mind that every determinant is positive?","['hankel-matrices', 'matrices', 'linear-algebra', 'hilbert-matrices', 'positive-definite']"
4581418,What is the $L^2$ space of sections of a vector bundle?,"The vector bundle in question can be a holomorphic line bundle with a hermitian metric.
Given $\mathcal{L}$ a holomorphic line bundle with transition functions $g_{ij}$ a section is defined as a collection ${h_i}$ such that $h_i=g_{ij}h_j$ . Or is another definition of sections better for this purpose?
Oh i should probably mention that this is on a compact Riemann surface.","['complex-analysis', 'functional-analysis', 'differential-geometry']"
4581419,Proving by contradiction odd values,"I need to prove the following by contradiction: "" $𝑥$ and $𝑦$ are odd integers, then $𝑥𝑦$ is odd"" I'm sure this question isn't very hard to solve, however, my understanding of contraposition is very weak. I have only learned it recently and I do not feel like I am totally grasping the concept. From my understanding, I am almost trying to prove this by saying the opposite statement. But I feel like this is oversimplifying it. I know that based on previous proofs an even number is in a form like $𝑧=2𝑎$ an odd is the same as the even except plus 1: $𝑥=2𝑎+1$ . Based on knowing this I assume I am able to use contraposition. But where do I start to use this?",['discrete-mathematics']
4581479,Solve matrix equations of the form: $X^T A X = B_1$ and $X A X^T = B_2$,"I have trouble solving the following two matrix equations for unknown $X \in \mathbb{R}^{n \times n}$ : $$X^T A X = B_1$$ $$X A X^T = B_2$$ where, $A$ , $B_1$ and $B_2$ are all $n \times n$ symmetric matrices such that $A^n = I_n$ , and $B_1 \neq B_2$ There is a solution for $X$ if $A$ and the $B$ 's are positive-definite, but is there an analytical solution for any matrices? Edit:
Is there a way to solve this when $A^n \neq I_n$ ? I'm interested in the case where $A$ , $B_1$ , $B_2$ and $X$ are matrices of one-to-one mappings, i.e., $A_{ij}=A_{ji}=1$ if $i$ maps to $j$ , and $0$ otherwise.
For example: $$A = \begin{bmatrix}
    0 & 1 & 0 & 0 & 0 \\
    1 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 1  & 0\\
    0 & 0 & 1 & 0  & 0\\
    0 & 0 & 0 & 0  & 1
\end{bmatrix}$$","['matrices', 'matrix-equations', 'quadratics', 'linear-algebra']"
4581505,"Given $A\cup B = A$, prove $B$ is the empty set","Given the above statement, i've to prove that $B = \emptyset$ I started by using the theorem for set equality that says. Given two sets $A$ and $B$ we have that: $A = B \iff A \subset B \quad\land\quad B \subset A$ therefore i just plug things in and i got that $A \cup B \implies x \in A \quad\lor\quad x \in B$ and $A \cup B = A \iff A \subset A \cup B \quad\land\quad A \cup B \subset A$ After that i got a stuck, its my first time doing this kind of proofs. Thanks in advance",['elementary-set-theory']
4581508,"What are the linear transformations that preserves the cross product, i.e. $ R(v\times w) = (Rv) \times (Rw), \forall v,w \in \mathbb{R}^3 $","Let us just focus on $\mathbb{R}^3$ currently. We study the set of all $3\times 3$ matrices $R$ satisfying $$
R(v\times w) = (Rv) \times (Rw), \forall v,w \in \mathbb{R}^3
$$ where $\times$ is the cross product. It is known that this condition is satisfied when $R \in \text{SO}(3)$ . However, is there any matrix other than the ones in $\text{SO}(3)$ that satisfies this condition?","['tensors', 'geometry', 'linear-algebra', 'vector-analysis', 'lie-groups']"
4581518,Is this general nested radical for $\pi$ true?,"We have, I. Liu Hui (c. 300 AD) $$\pi \approx 3\cdot2^{\color{red}8}\times \underbrace{\sqrt{2 - \sqrt{2 + \sqrt{2 + \sqrt{2 + \sqrt{2 + \sqrt{2 + \sqrt{2 + \sqrt{2 + \sqrt{2+\sqrt{\color{blue}1}}}}}}}}}}}_{\color{red}{10}\text{ square roots}}$$ II. Viete (c. 1590 AD) $$\pi = \lim_{k\to\infty} 4\cdot2^{\color{red}{k}}\times \underbrace{\sqrt{2-\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{2+\cdots+\sqrt{\color{blue}2}}}}}}}}}_{\color{red}{k+2}\text{ square roots}}$$ III. (yours truly) $$\pi \approx 6\cdot2^{\color{red}8}\times \underbrace{\sqrt{2 - \sqrt{2 + \sqrt{2 + \sqrt{2 + \sqrt{2 + \sqrt{2 + \sqrt{2 + \sqrt{2 + \sqrt{2+\sqrt{\color{blue}3}}}}}}}}}}}_{\color{red}{10}\text{ square roots}}$$ For the blue numbers, I tried $\sqrt{5}$ , $\sqrt{6}$ , and other $\sqrt{n}$ , nothing worked, until I remembered $\sqrt{2}$ and $\sqrt{3}$ has a trigonometric context. Question: Given, $$\color{blue}{\beta}=4\cos^2\left(\frac{\pi}{\color{brown}{\alpha}}\right)$$ then is it true that, $$\pi = \lim_{k\to\infty} \color{brown}{\alpha}\cdot2^{\color{red}{k}}\times \underbrace{\sqrt{2-\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{2+\cdots+\sqrt{\color{blue}\beta}}}}}}}}}_{\color{red}{k+2}\text{ square roots}}$$ So the first three are simply the cases $\alpha = 3,4,6$ , though if the general form is true, then one can use other positive integers. P.S. For example, note that if $\alpha = 5$ , then we relate $\pi$ to our old friend the golden ratio $\phi$ , $$\pi = \lim_{k\to\infty} \color{brown}{5}\cdot2^{\color{red}{k}}\times \underbrace{\sqrt{2-\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{2+\cdots+\sqrt{\color{blue}{\phi^2}}}}}}}}}}_{\color{red}{k+2}\text{ square roots}}$$","['nested-radicals', 'golden-ratio', 'radicals', 'pi', 'trigonometry']"
4581543,Prove $\sum_{n=0}^{\infty}\frac{(2n)!}{4^n(n+1)!(n)!} = 2$,"Hi my homework question is to prove $$
\sum_{n = 0}^{\infty}\frac{\left(2n\right)!}{4^{n}\left(n+1\right)!\left(n\right)!} = 2.
$$ I know the sequence of partial sums $S_n$ converges from trying the ratio test. I tried to prove that $S_n\rightarrow 2$ as $n\rightarrow \infty$ by expressing writing $\frac{1}{1-x}$ with taylor expansion so that I can represent 2 with an infinite summation and subtract that. That didn't work. Next I tried to just consider each term in the summation and write $4^n=2^n2^n$ and then write $2^n(n)!=(2n)(2n-2)(2n-4)...(4)(2)$ and try to cancel terms and then write the leftover $2^n(n+1)!=(2n+2)(2n)...(4)(2))$ . I was left with alternating products on the nominator and denominator, I belive it looked something like this $\frac{(2n-1)(2n-3)...(3)(1)}{(2n+2)(2n)(2n-2)...(4)(2)}$ . I don't know where to go from here and don't have other ideas to try. Please help.","['number-theory', 'summation', 'sequences-and-series']"
4581591,"Is $\ \{x(t) \in L^1[0,1] \mid 0 \le x(t) \le t^2\} $ a compact in $ L^1[0,1]$?","In my functional analysis class I was given a problem: Is $\ \{x(t) \in L^1[0,1] \mid  0 \le x(t) \le t^2\} $ a compact in $ L^1[0,1]?$ $L^1[0,1]$ is Banach space and I know that subset of a Banach space is compact if and only if it is closed and totally bounded. And also I know that for banach spaces totaly bounded is equivalent to relative compactness. So I need to check if my subset is closed and relatively compact or not. But I don’t understand how to do this by definition. In our lectures we proved criterions of relative compactness for some banach spaces (for example: $l_2$ , $C[a,b]$ ), but haven’t proved and even discussed criterion for $L^p[0,1]$ , where $p \geq 1$ . Nevertheless, in the functional analysis exercise book of professors of my deparment I found a criterion for relative compactness for $L^p[0,1]$ which was proved by Kolmogorov (1931) and M. Riesz (1933): Set $M$ in $L^p[0,1], \ 1 \leq p < \infty$ is relatively compact if and only if and only if $\ \forall\varepsilon > 0 \ \exists \delta>0 \ \forall x \in [0,1] \ \forall h \in [0,\delta] $ $$ \displaystyle \left (\int \limits_0^{1-h} |x(t+h)-x(t)|^p dt \right)^{\frac{1}{p}} \leq \varepsilon $$ But we haven’t discussed this theorem, so it is better to solve this problem by definition. But I will be grateful if you give me a source where the proof of this theorem is written. So I appreciate any hint about how to solve this problem using only definitions and criterions which I mentioned in the second paragraph.","['functional-analysis', 'analysis', 'real-analysis']"
4581628,Commutation of differentiation with any linear map,"Consider $A$ and $B$ are two linear operators in finite-dimensional vector space. So to commute, they should share something special. For matrix $A$ to commute with all other is quite restrictive: it would only true for multiply of identity. However differential is also linear operator and it commutes with any matrix $A$ : $$
f(x) = A\phi(x), [D_{(x_0)}A\circ\phi(x)](h) = A[D_{(x_0)}\phi(x)](h).
$$ This is quite a puzzle for me from this perspective.","['derivatives', 'linear-transformations', 'differential-geometry']"
4581632,"Prove that the number of ""good"" subsets in $A$ and the number of ""good"" subsets in $B$ have the same parity.","Let $G=(A,B)$ be a bipartite graph. We call a subset $X$ of $A$ ""good"" if every vertex of $B$ is adjacent to at least one vertex of $X$ . We call a subset $Y$ of $B$ ""good"" if every vertex of $A$ is adjacent to at least $1$ vertex in $Y$ . Prove that the number of ""good"" subsets in $A$ and the number of ""good"" subsets in $B$ have the same parity. My idea is to ""simplify"" the $G$ graph but keep the parity of $|X|$ and $|Y|$ . But I am stuck with how to transform my graph G . I hope to get help from everyone. Thanks very much!","['graph-theory', 'combinatorics']"
4581655,Distribution of t-statistic with two small sample sizes,"Let $X_1,...,X_n\stackrel{i.i.d}\sim N(μ_0,σ_0^2)$ $Y_1,...,Y_m\stackrel{i.i.d}\sim N(μ_1,σ_1^2)$ the $X_i$ 's are independent of the $Y_i$ 's where $μ_0,μ_1,σ_0^2,σ_1^2$ are unknown parameters. Suppose the data collected are summarized as follows: Sample sizes: $n=m=5$ ; Sample means: $\bar{X}_{n}=1.2$ $\bar{Y}_{m}=1.0$ Sample variances: $\hat{\sigma}_1^2=\hat{\sigma}_2^2=0.5$ . Choosing from the following distributions that best approximates the distribution of the $t$ -statistic $T_{n=5,m=5}$ under the null hypothesis $H_0 ∶μ_0=μ_1$ : Normal distribution with mean $μ≠0$ Standard normal distribution $t$ -distribution $F$ -distribution The most ""plausible"" distribution would be the $t$ -distribution. Seems about right? My question is related to the degrees of freedom (assuming my choice of distribution is correct). For the $t$ -distribution, the $t$ -statistic is given by $$ T_n =\frac{\bar{X_{n}}-\bar{Y_{m}}}{\sqrt{\hat{\sigma}_1^2/m+\hat{\sigma}_0^2/n}}$$ The general formula is $df = n_1 + n_2 - 2$ , where $n_1,n_2$ are $n,m$ respectively so $T_n \sim t_{n+m-2, \alpha/2}$ therefore $df=8$ . On the other hand, according to Welch-Satterthwaite's (WS) formula (which calculates an approximation to the effective degrees of freedom), under $H_0$ , $T_n$ is approximately distributed as $t_{40}$ where $df=40$ . Which $df$ value is the correct one? The problem says "" best approximates "" so WS makes sense but I'm not completely sure.","['statistics', 'independence', 'probability-distributions', 'normal-distribution', 'sampling']"
4581804,Please help with this basic combinatorics problem,"Statement 20 Students in a class have to vote to elect a president. There are 3 candidates :
a) In how many ways can the votes be distributed? b)In how many of these ways will Peter have the most votes? I tried: a) We can see that the votes are undistinguishable objects, this can be seen as a problem of combinations with repetition, where we have two delimiters and 20 balls to arrange. There are $22 \choose  20$ =231 ways to do this. I know this is right. b) My reasoning: Peter needs at least 11 votes to win a majority. We set those aside and distribute the other 9 in $11 \choose  9$ =55 ways. But the right answer should be 73, according to my textbook. What is my problem? Thank you so much for the help!","['combinations', 'combinatorics', 'discrete-mathematics']"
4581831,Upper estimates for $\int_0^M |f(x)|^2 dx$ and $ \int_0^M f(x) dx$ when $\|f\|_{\infty}\le M$.,"Let $M>0$ be fixed and let $f$ be a continuous real valued function such that $$\|f\|_{\infty}\le M,$$ where $\|\cdot\|_{\infty}$ denotes the sup-norm. Under these assumption, I need an upper estimate for $$\int_0^M |f(x)|^2 dx\quad\text{ and }\quad \int_0^M f(x) dx.$$ About me the correct ones are the following: $$\int_0^M |f(x)|^2 dx\le M^3$$ and $$\int_0^M f(x) dx\le M^2.$$ Could someone please tell me if am I right or not? Thank you in advance.","['integration', 'calculus', 'analysis', 'real-analysis']"
4581835,Probability - two balls in the box: one we don't know its color and the other is red. What's the probability it's white?,"Bob has a black box (you can't see what's inside the box). A long time
ago Bob put one ball into the box but he doesn't remember what color
the ball was. With equal probability it can be a white ball or a red
ball. A. Bob takes a red ball and puts it in the same box.  Now
there are two balls in the box: one ball Red that Bob just put in and
a ball that was in the box earlier (Bob doesn't remember its color).
Now Bob draws Randomly one ball out of the box and it turned out to
be a red ball. Calculate the probability that the ball that has been
in the box for a long time is a white ball given the action taken by
Bob. My attempt: There are two options, since we already know of them is red, $A_1= \{\text{White, Red}\}$ or $A_2= \{\text{Red, Red}\}$ , so $\Pr[A_1 \cup A_2] = 1/2$ ?",['probability']
4581848,Non-standard complex structures on $\Bbb H\times \Bbb H$ so that multiplication is holomorphic,"Let $$\mu:\Bbb H\times \Bbb H\to \Bbb H, \qquad (x,y)\mapsto x\cdot_{\Bbb H} y$$ denote the product of two quaternions. With the standard identification $$\Bbb H\cong\Bbb C^2\cong \Bbb R^4, \qquad x_0+ix_1+jx_3+kx_4\equiv (x_0+ix_1, x_3+ix_4)\equiv(x_0,x_1,x_3,x_4)$$ one sees: $$\mu: \Bbb C^4 \to \Bbb C^2,\qquad (z_1,z_2,w_1,w_2)\mapsto (z_1\cdot w_1-z_2\overline{w_2}, z_1w_2+z_2\overline{w_1})$$ and $\mu$ is not holomorphic. My question is whether or one can choose a different complex structure on the domain so that this map does become holomorphic. More precisely: Does there exist a real orthogonal transformation $A\in O(8)$ so that $$(\mu\circ A): \Bbb C^2\times \Bbb C^2\cong\Bbb R^4\times\Bbb R^4\to \Bbb R^4 \cong \Bbb C^2,\qquad x\mapsto \mu(Ax)$$ is holomorphic?","['complex-analysis', 'complex-geometry', 'kahler-manifolds', 'quaternions']"
4581852,Question about $L^1$ space and Uniform integrability.,"Let $(\Omega,\Sigma,\mu)$ be a finite measure space and let $\mathscr{F}=\{f_{\alpha}:\Omega\to \mathbb{R}:\alpha\in I\}$ be an arbitrary family of measurable real functions on $\Omega$ . $\mathscr{F}$ is uniformly integrable if the following two conditions hold: \begin{equation}
    (i) \sup_{\alpha}\int_{\Omega}|f_{\alpha}|d\mu=C<\infty \hspace{1cm}
     (ii) \lim_{\mu_{(A)}\to 0}\int_{A}|f_{\alpha}|d\mu=0
  \end{equation} Remark: (i) follows form (ii). I want to know the difference when $\mathscr{F}\subset L^1$ and when $\mathscr{F}$ is uniformly integrable since this two looks same to me.Please help. what I understood is that here condition (ii) means that $\forall \epsilon>0  ,\hspace{0.5 cm} \exists \delta>0$ such that $\int_{A}|f_\alpha|d\mu<\epsilon$ whenever $f\in \mathscr{F}$ and for all $\mu(A)<\delta.$","['measure-theory', 'lebesgue-measure', 'uniform-integrability', 'lebesgue-integral', 'real-analysis']"
4581870,A(n easy ?) partial differential equation,"This might be a classic partial differential equation, but I couldn't find anything on the Internet: Find all the functions $f: \mathbb{R}^n \to \mathbb{R}$ verifying $ \displaystyle\sum\limits_{k=1}^n x_k \frac{\partial f}{\partial x_k}(x) = 0 $ . I do understand this means that $\forall x \in \mathbb{R}^n, \langle \nabla f(x), x \rangle = 0$ , but I still do not see how to proceed. Thank you for your help.","['multivariable-calculus', 'partial-differential-equations']"
4581889,"If $Y$ is a sub-Gaussian random variable, must $Y-E(Y\mid X)$ be also sub-Gaussian?","Question. If $Y$ is a sub-Gaussian random variable, must the regression residual $\varepsilon=Y-E(Y\mid X)$ be also sub-Gaussian? In terms of the variance, it is well known that $\mathrm{var}(\varepsilon)\leq \mathrm{var}(Y)$ . However, I can't find out a way to prove (or disprove) the sub-Gaussianity of $\varepsilon$ when $Y$ is sub-Gaussian.","['statistics', 'probability']"
4581926,"""Dual"" notion of sheafification","Let $\mathcal{F}$ be a presheaf on a topological space $X$ . A sheafification of $\mathcal{F}$ is a sheaf $\mathcal{F}^\mathrm{sh}$ on $X$ together with a presheaf morphism $\mathrm{sh_\mathcal{F}}: \mathcal{F} \to  \mathcal{F}^\mathrm{sh}$ fulfilling the universal property:
For every sheaf $\mathcal{G}$ on $X$ and presheaf morphism $\phi: \mathcal{F} \to \mathcal{G}$ there is a unique sheaf morphism $\widetilde{\phi}: \mathcal{F}^\mathrm{sh} \to \mathcal{G}$ making the following diagram commute: $$
\require{AMScd}
\begin{CD}
\mathcal{F} @>\mathrm{sh_\mathcal{F}}>> \mathcal{F}^\mathrm{sh}\\
@VV\phi V @VV\widetilde{\phi} V\\
\mathcal{G} @>\mathrm{id}>> \mathcal{G}
\end{CD}
$$ Now this seams to be a ""colimit type"" construction and I want to know if there is a ""limit type"" construction. Precisely: Let's say a sheafification of type B of $\mathcal{F}$ is a sheaf $\mathcal{F}^\mathrm{shB}$ together with a presheaf morphism $\mathrm{shB}_\mathcal{F}: \mathcal{F}^\mathrm{shB} \to \mathcal{F}$ fulfilling the universal property: For every sheaf $\mathcal{G}$ and presheaf morphism $\phi : \mathcal{G} \to \mathcal{F}^\mathrm{shB}$ there is a unique sheaf morphism $\widetilde{\phi} : \mathcal{G} \to \mathcal{F}^\mathrm{shB}$ making the following diagram commute: $$
\require{AMScd}
\begin{CD}
\mathcal{F} @<\mathrm{shB_\mathcal{F}}<< \mathcal{F}^\mathrm{shB}\\
@AA\phi A @AA\widetilde{\phi} A\\
\mathcal{G} @<\mathrm{id}<< \mathcal{G}
\end{CD}
$$ (I have basically changed the direction of all arrows from the previous definition.) Now can anybody tell me... ... whether there is an established name for that? ... whether this is actually useful in (algebraic) geometry? Also note that the left side of the diagram is always in the category of presheaves, and the right side ""in the category of sheaves"". Sheafification gives a left-adjoint functor to the forgetful functor $\mathrm{Shv_X} \to \mathrm{PreShv_X}$ . So I guess what I am asking for is a right-adjoint functor to this forgetful functor. Thank you so much!","['algebraic-geometry', 'category-theory', 'sheaf-theory']"
4581939,Problem 3.28 from Folland’ Real Analysis.,"The problem is If $F\in NBV$ , let $G(x)=\lvert\mu_{F}\rvert((-\infty,x])$ . Prove that $\lvert\mu_{F}\rvert=\mu_{T_{F}}$ by showing that $G=T_{F}$ via the following steps. a. From the definition of $T_{F},\ T_{F}\leq G$ . b. $\lvert\mu_{F}(E)\rvert\leq\mu_{T_{F}}(E)$ when $E$ is an interval, and hence when $E$ is a Borel set. c. $\lvert\mu_{F}\rvert\leq\mu_{T_{F}}$ , and hence $G\leq T_{F}$ . Below are my attempts. For a, fix $x\in\mathbb{R}$ . Then $$\begin{aligned}
T_{F}(x)&=\sup\left\lbrace \sum_{j=1}^{n}\lvert F(x_{j})-F(x_{j-1})\rvert:n\in\mathbb{N},-\infty<x_{0}<...<x_{n}=x\right\rbrace \\
&=\sup\left\lbrace \sum_{j=1}^{n}\lvert\mu_{F}((x_{j-1},x_{j}]):n\in\mathbb{N},-\infty<x_{0}<...<x_{n}=x\right\rbrace \\
&\leq\sup\left\lbrace \sum_{j=1}^{n}\lvert\mu_{F}\rvert((x_{j-1},x_{j}]):n\in\mathbb{N},-\infty<x_{0}<...<x_{n}=x\right\rbrace \\
&=\lvert\mu_{F}\rvert((x_{0},x])\leq\lvert\mu_{F}\rvert((-\infty,x])=G(x).\\
\end{aligned}$$ Hence $T_{F}\leq G$ . For b, assume $E=(a,b]$ , where $-\infty<a<b<\infty$ . Choose $a=x_{0}<...<x_{n}=b$ , then $$\begin{aligned}
\lvert\mu_{F}(E)\rvert&=\lvert F(b)-F(a)\rvert\\
&=\lvert F(b)-F(x_{n-1})+...+F(x_{1})-F(a)\rvert\\
&\leq\sum_{j=1}^{n}\lvert F(x_{j})-F(x_{j-1})\rvert\\
&\leq\mu_{T_{F}}((a,b]).\\
\end{aligned}$$ For c, if $E$ is a Borel set, use Exercise 3.21 we have $$\lvert\mu_{F}\rvert(E)=\sup\left\lbrace \sum_{j=1}^{n}\lvert\mu_{F}(E_{j})\rvert:n\in\mathbb{N},\ E_{1},...,E_{n}\ \text{disjoint},\ E=\bigcup_{j=1}^{n}E_{j}\right\rbrace .$$ Then from b, $$\lvert\mu_{F}\rvert(E)\leq\sup\left\lbrace \sum_{j=1}^{n}\mu_{T_{F}}(E_{j}):n\in\mathbb{N},\ E_{1},...,E_{n}\ \text{disjoint},\ E=\bigcup_{j=1}^{n}E_{j}\right\rbrace=\mu_{T_{F}}(E).$$ Does these work? Or are there a better proof for this problem? Any help or advice would be appreciated.","['measure-theory', 'solution-verification', 'real-analysis']"
4582054,Product of Permutation Representation Characters,"Consider the action of $S_n$ on $x_i$ , where $x_i$ is a set of $i$ -element subsets of $X =$ { $ 1, 2, ..., n$ } (so $|x_i| = {n \choose i}$ ). Now, let $\pi_i$ be the character of the permutation representation of $S_n$ acting on $x_i$ (ie, $\pi_i(g)$ is the trace of an ${n \choose i} \times {n \choose i}$ -dimensional permutation matrix acting on the complex vector space $\mathbb{C} x_i$ ). A paper I am currently reading states (in Appendix C) that for $0 \leq l \leq k \leq n/2$ , the inner product of two such characters $\langle \pi_l, \pi_k \rangle = \langle \pi_l \pi_k, \mathbb{1}_G \rangle = l + 1$ where $\mathbb{1}_G$ is the trivial representation. I am struggling to understand why this is so. After reading some notes on representation theory, I know that the character $\pi_i(g)$ is the number of elements fixed by a permutation $g$ , and that $\langle \pi_l, \pi_k \rangle = \langle \pi_l \pi_k, \mathbb{1}_G \rangle$ is the number of orbits of $S_n$ on $x_l \times x_k$ . I'm also convinced that for $i \neq 0, n$ the permutation representation on $x_i$ is faithful (though I'm not sure if this is relevant). I don't know how to prove this result - any help would be much appreciated!","['permutations', 'characters', 'representation-theory', 'symmetric-groups', 'group-theory']"
