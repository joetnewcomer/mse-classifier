question_id,title,body,tags
901924,Stuck simplifying a fractional expression,"$$ \frac { \frac { 1 }{ 1+x+h } -\frac { 1 }{ 1+x }  }{ h }  $$
$$ \frac { 1(1+x) }{ 1+x+h(1+x) } -\frac { 1(1+x+h) }{ 1+x(1+x+h) } $$
$$ \frac { -h }{ (1+x+h)(1+x) } \quad *\quad \frac { 1 }{ h } $$ This is what I have so far. I have no idea what my next step is. I get to: $$\frac { -h }{ (1+x+h)(1+x)(h) } $$ and don't know where to go from here..",['algebra-precalculus']
901945,If $G/H$ and $G$ are connected linear algebraic groups must $H$ also be connected?,"Let $k$ be a perfect field (e.g of characteristic zero) and let $G$ and $H$ be linear algebraic groups over $k$, with $H$ a normal subgroup of $G$. If both $G$ and $G/H$ are connected, must $H$ also be?","['general-topology', 'algebraic-geometry', 'number-theory']"
901967,Jacobian for a matrix transformation: Example of Cholesky decomposition,"I would like to generally understand how the Jacobian of a matrix transformation can be computed. 
As a concrete example, consider the Transformation from a (correlation) matrix to its Cholesky factor: $\Omega \rightarrow L$, where $\Omega = LL^T$ The absolute determinant of the Jacobian is allegedly $\prod_{i=2}^K L_{ii}^{i-K}$, which tells me that the Jacobian must be triangular/diagonal. I tried to vectorize $\Omega$ and $L$ in row-major order and compute the square matrix of partial derivatives with $\binom{K}{2}+ K$ columns and rows, but I ended up with something far from triangular. Any help is appreciated!","['statistics', 'linear-algebra', 'probability-theory']"
901969,Must a normed vector space be over $\mathbb{R}$ or $\mathbb{C}$?,"If it must be, why is this so? In the maths courses I have taken normed vector spaces always have been over $\mathbb{R}$ or $\mathbb{C}$, but I don't see that this has to be so. I am asking because I was proving that if a normed vector space X has a Schauder basis then it is separable, and I had a proof along the lines of this question How to prove that if a normed space has Schauder basis, then it is separable? What about the converse? The problem I had is this proof requires that we have a sequence of rationals converging to the scalars of the vector space. But if the vector space is not $\mathbb{R}$ or $\mathbb{C}$ then why should this work? Someone has asked this in a comment to the second answer of the question above, but I do not quite understand the response given: ""The fiel has to be restricted, notably because of the definition of a normed space: an absolute value is needed""
I can see that we do use absolute values on the scalars of the vector space but I'm not then sure why this restricts them to being in $\mathbb{R}$ or $\mathbb{C}$, other than I don't know how to define an absolute value on something other than a real or complex number (but wikipedia seems to suggest it can be done).","['vector-spaces', 'normed-spaces', 'linear-algebra']"
901991,How to prove that $\tan^2(\frac\theta2)= \tan^2(\frac\alpha2)\tan^2(\frac\beta2)$?,"I'm unable to solve this question: $\cos(\theta)=\dfrac{\cos(\alpha)+\cos(\beta)}{1+\cos(\alpha) \cos(\beta)}$ Prove: $\tan^2\left(\frac\theta2\right)= \tan^2\left(\frac\alpha2\right)\tan^2\left(\frac\beta2\right)$ I have tried  the following: Using the indentity: $\cos(\theta)=\dfrac{1-\tan^2\left(\frac\theta2\right)}{1+\tan^2\left(\frac\theta2\right)}$ Diving by $\cos (\alpha)  \cos (\beta)$ Creating a triangle, to find $\tan(\theta)= \sin \alpha  \sin \beta$ Every time I got a huge complex equation with roots. Any help would be appreciated.",['trigonometry']
902007,Why is continuous differentiability required?,"I have two questions. My book proves that if $f:\mathbb{C}\rightarrow \mathbb{C}$ is a holomorphic function, then it satisfies the Cauchy-Riemann equations, and if we look at the function as $F: \mathbb{R}^2\rightarrow \mathbb{R}^2$, then this function is differentiable. The point is that we are looking at conditions when we can go from complex differentiability(holomorphic), to differentiability when we look at the function in terms of real variables only ,that is differentiability in multivariate calculus. question 1: The book also has the converse of the above[picture], and in the converse result they write
$ f = u(x,y)+i*v(x,y)$, where $z=x+i*y$, now in the converse result they have that the u and v must be continuously differentiable, why not only differentiable? Do you guys see in the proof where they need to use that it is continuously differentiable and it would fail if they were only differentible? question 2: A related question is: If we have a function $h: \mathbb{R}^2\rightarrow \mathbb{R}^2 $. And we know that the partial derivatives exist, but they are not continous, will it may be that h is not differentiable?","['multivariable-calculus', 'complex-analysis', 'calculus', 'real-analysis']"
902012,"Find all roots of $\,x^2\!\equiv x\pmod{\!900}$, i.e all idempotents in $\Bbb Z_{900}$","I need the idempotent elements of $Z_{900}$ $2^2\cdot 3^2\cdot 5^2=900$ Of course there's $$0 \pmod 4 \\
0 \pmod 9 \\
0 \pmod {25} \\
$$ and $$
1 \pmod 4 \\
1 \pmod 9 \\
1 \pmod {25} \\
$$ I found the answers by making a C++ program to test all numbers, but I don't know how to quick solve on paper. This is the output of the program $(0, 0, 0) \rightarrow 0 \\
(1, 1, 1) \rightarrow 1 \\
(0, 0, 1) \rightarrow 576 \\
(0, 1, 0) \rightarrow 100 \\
(1, 0, 0) \rightarrow 225 \\
(1, 1, 0) \rightarrow 325 \\
(0, 1, 1) \rightarrow 676 \\
(1, 0, 1) \rightarrow 801 \\$","['ring-theory', 'elementary-number-theory', 'abstract-algebra', 'idempotents']"
902017,How far can one see over the ocean?,"Since Earth is a sphere, one has only a limited visibility radius. How far is that, actually? This Q&A was inspired by this question, about whether or not Legolas can see the 24km distant Riders of Rohan.",['geometry']
902025,"The map $t\mapsto (\cos t,\sin t)$ is injective from $[0,2\pi)$ onto the circle, but its inverse is not continuous","Question: given $\phi:[0,2\pi[\mapsto\mathbb{R}^2$ a map defined by $\phi(t)=(\cos t,\sin t)$ then Shown that $\phi$ is injective into unitary circle $S^1=\{(x,y)\in\mathbb{R}^2\mid x^2+y^2=1\}$ Find the inverse map $\phi^{-1}$ from $\phi$ Shown that $\phi^{-1}$ can't be continuous try for 1. i think, we have that for all $t$ that $\sin(t+2\pi)=\sin t$ and $\cos(t+2\pi)=\cos t$, then for $t\in[0,2\pi)$ we have that not exists $t_1\ne t_2$ that $(\cos t_1,\sin t_1)=(\cos t_2,\sin t_2)$ because: $\cos t_1=\cos t_2\Rightarrow t_1=t_2\vee t_1=2\pi-t_2\\
\sin t_1=\sin t_2\Rightarrow t_1=t_2\vee t_1=\pi-t_2$ and $2\pi-t_2\ne\pi-t_2$, since $2\pi\not\in [0|2\pi[$ then we dont have twice times the same point since $\phi(2\pi)=\phi(0)$ for 2. i think it is on this way $\phi\circ\phi^{-1}=I\\
(\cos t,\sin t)=(x,y)\\
\cos t=x,\sin t=y\iff \tan t=\frac{\sin t}{\cos t}=\frac{y}{x}\iff t=\arctan\frac{y}{x}$ then $\phi^{-1}(x,y)=\arctan\left(\frac{y}{x}\right)$","['general-topology', 'calculus', 'real-analysis']"
902028,What is a transformation?,"I am not a native English speaker and I have been pointed out that the word ""transformation"" as a synonym of ""function"" is grammatically incorrect. However, I even found a wikipedia and a mathworld entries where they use ""transformation"" as a synonym of function: http://en.wikipedia.org/wiki/Transformation_(function) http://mathworld.wolfram.com/Transformation.html Which one is correct?","['transformation', 'functions', 'definition']"
902037,Is this Differential equation a linear DE?,"$\Large{y\frac{dy}{dx}-xy=0}$ Could you please explain to me why it is or isn't? Much obliged, thank you.",['ordinary-differential-equations']
902042,How does research in math differ from research in statistics?,"I'm at a crossroads where I'm considering switching my major from electrical engineering to math, because quite frankly, I'm just not getting enough math to satisfy my passion from engineering. While my school doesn't offer a statistics bachelor's degree, it does offer statistics courses from the math department. Now, I would certainly like to combine a level of employability with a math degree. I'm very curious, though, about what I could do with statistics (I had never given it much thought before). So my questions are: How closely related are math and statistics? Can someone go from a math bachelor's degree (having taken some statistics courses) to a statistics graduate degree? Could someone who loves math also love statistics? Having asked all of the above, what are the differences between how university statistics professors and math professors approach research problems? How does the quantity and range of unsolved statistics problems compare to the ones in math? Note: I have had no exposure to statistics at all so far.","['statistics', 'soft-question']"
902088,How find this integral $I=\int_{0}^{1}\int_{0}^{1}\frac{\ln{(1+xy)}}{1-xy}dxdy$,"Find this integral $$I=\int_{0}^{1}\int_{0}^{1}\dfrac{\ln{(1+xy)}}{1-xy}dxdy$$ My try: since $$\dfrac{1}{1-xy}=\sum_{n=0}^{\infty}(xy)^n$$ so $$I=\sum_{n=0}^{\infty}\int_{0}^{1}y^n\int_{0}^{1}x^n\ln{(1+xy)}dx$$ because $$\int_{0}^{1}x^n\ln{(1+xy)}dx=\dfrac{x^{n+1}\ln{(1+xy)}}{n+1}|_{0}^{1}-\dfrac{1}{n+1}\int_{0}^{1}\dfrac{x^{n+1}y}{1+xy}dx=\dfrac{\ln{(1+y)}}{n+1}-I_{1}$$ where $$I_{1}=y\int_{0}^{1}\dfrac{x^{n+1}}{1+xy}dx$$ even if $I_{1}$ can use the beta function,
But  then  follow I can't it.Thank you","['multivariable-calculus', 'improper-integrals', 'integration']"
902102,The nature of isomorphism between fundamental groups with different base points,"New to algebraic topology. Munkres (Topology, 2 ed.) in the last paragraph on page 332 says that ""If $X$ is path-connected, all the groups $\pi_1(X,x)$ are isomorphic, so it is tempting to try to ""identify"" all these groups with one another and to speak of the fundamental group of the space X, without reference to base-point"". He goes on to say that there is no ""natural way of identifying"" these groups and ""different paths...may give rise to different isomorphisms between these groups."" I just don't get this at all. If groups are isomorphic, they are the same algebraically. What does ""different isomorphisms between groups"" mean? Again on the following page (334) he says that even in a path-connected space, the induced homomorphism map is an isomorphism, and ""these groups are isomorphic, [but] they are still not the same group"". This is driving me nuts. The Klein-$V_4$ group is isomorphic to $\mathbb{Z_2} \times \mathbb{Z_2}$ and $D_4$; OK they do not ""arise"" naturally the same way, but they are the same algebraically and up to isomorphism. My question is this: Agreed in a space not path-connected, you cannot dispense with the base-point; why is that a problem in a path-connected space? Thank you in advance.","['general-topology', 'algebraic-topology', 'fundamental-groups']"
902107,"Is there an easier way to find the ""natural"" integration constant?","Suppose we take consequtive derivatives of a function at a point and then interpolate them with Newton series (Newton interpolation formula) so to obtain a smooth curve. $$f^{(s)}(x)=\sum_{m=0}^{\infty} \binom {s}m \sum_{k=0}^m\binom mk(-1)^{m-k}f^{(k)}(x)$$ If the series converges at $s=-1$ we take this value to be the ""natural"" value of antiderivative of $f$ at the point $x$ (assuming that integral is the -1-th derivative). For instance, for function $f(x)=a^x$ the expansion converges (if converges, which is not the case for all $a$) to $a^x (\ln a)^s$, or $(\ln a)^s$ at $x=0$. Thus antiderivative of $a^x$ should naturally have value of $\frac{1}{\ln a}$ at $x=0$. Is there an easier way to obtain this value, and possibly, more universal (working where the series diverges)?","['partial-derivative', 'sequences-and-series', 'integration', 'derivatives']"
902118,Solve $y''-3y'+2y=x^2$,"Solve $$y''-3y'+2y=x^2$$ My approach: Homogen solution:
$$y = Ae^x +Be^{2x}$$ Particular solution:
$$ y_p = x(Ax^2+Bx+C) = Ax^3+Bx^2+Cx $$
$$ y_p' = 3Ax^2 + 2Bx + C$$
$$y_p'' = 6Ax + 2B$$ Put his into the initial equartion to get A, B and C gives me:
$A=0, B=1/2, C=3/2$ This leads me to the answer: $$y = Ae^x +Be^{2x}  + x^2/2 + 3x/2$$ However the correct answer is $$y = Ae^x +Be^{2x}  + x^2/2 + 3x/2+ 7/4$$ Where's my miss? Where comes the last term from?",['ordinary-differential-equations']
902133,Intuitive and convincing argument that functions are vectors,"Back to school time again. As I'm discussing all the mathy stuff and insights gained over the summer, I cannot help but notice that many of my peers in second or third year undergrad cannot bridge the gap between vectors and functions. When I ask them why, the answer I most often get are as follows: Vectors are something learned in linear algebra, and they are
basically pointy arrows in $R^2$ and written with brackets $\{\}$ or sometimes with
arrow on the top Functions are completely different, they are the object under study in calculus, never drawn as an arrow, doesn't
satisfy some axioms of vector space, can be drawn in a lot of ways
(piecewise or continuous, with crazy oscillations). You can do a lot with functions, such as taking the derivative of it,
or the integral of it. You can find the inverse of a function. No authority has ever said functions are vectors, if they did I would
believe them Yes, functions are sometimes contained in brackets, but that is just a
vector of functions, not a function Can someone please provide a good example may bridge this gap? Also, is there any books that explicitly bridge the two concepts? Thank you for your inputs!","['linear-algebra', 'calculus', 'vectors', 'functions']"
902171,How is it called when you apply min / max seperatly to each dimension?,"I want to do the following: $$\begin{pmatrix}3\\1\\4\\1\end{pmatrix} = \min(
\begin{pmatrix}4\\4\\4\\4\end{pmatrix}, 
\begin{pmatrix}3\\1\\4\\10000\end{pmatrix},
\begin{pmatrix}10\\10\\10\\10\end{pmatrix},
\begin{pmatrix}10\\10\\10\\1\end{pmatrix})$$ So I have many vectors in $\mathbb{R}^n$ and I want to describe that I apply the min-function seperately to each dimension to get a new vector in $\mathbb{R}^n$. Is there a common function / a better way to describe it thant ""apply min separately to each dimension""?","['terminology', 'functions']"
902175,How should I prove that: $\sum_{i=1} ^{n}(\sin(\frac{i\pi}{n}))^2=\frac{n}{2}$,"$$\sum_{i=1} ^{n}\Big(\sin\big(\frac{i\pi}{n}\big)\Big)^2=\frac{n}{2}$$ An interesting conclusion and checked for validity...holds for $n\geq 2$, but yet do not know how to prove it. Are there any suggestions? I am trying too....cheers!","['trigonometry', 'summation', 'sequences-and-series']"
902190,Found an odd relationship! Could someone help me to prove or debunk it?,"I finished up in hospital which typically means that one has A LOT of spare time to kill and after using electronic devices so much that it makes you sorry I flinched into doodling and and light-headedly playing with a calculator. It happend than that I bounced upon one odd thing: For every right-angled triangle I sketched it seemed to be true that
$$\frac{\sin^2{\alpha}+\sin^2{\beta}+\sin^2{\gamma}}{\cos^2{\alpha}+\cos^2{\beta}+\cos^2{\gamma}}=2$$ Because my medicament dispenser is kinda slacking my brain I can't really concentrate on finding a way to prove this relation and a counterexample is also not in sight. So can someone please help me out? This open question is always on my mind and starts to become annonying :) yours, Levix",['trigonometry']
902230,Linear operators on the functions $f:\mathbb{R}\rightarrow\mathbb{R}$ that distribute over multiplication,"Let $V$ denote the vector space of all functions $f:\mathbb{R}\rightarrow\mathbb{R}$. What are the linear operators $L:V\rightarrow V$ such that $L[fg]=L[f]L[g]$ for all $f,g\in V$? I made a bit of progress by considering the functions
$$\chi_t(x) = \begin{cases}
1 & x=t \\
0 & x\neq t
\end{cases}.$$
For fixed $x\in\mathbb{R}$, the value of $L[\chi_t](x)$ is either $0$ or $1$ for each $t\in\mathbb{R}$. If there exists some $t$ such that $L[\chi_t](x)=1$, then $L[f](x)=f(t)$. I was unable to do the case in which $L[\chi_t](x)=0$ for all $t$.",['linear-algebra']
902233,How do we know the classification of finite simple groups is finished?,"I don't have much experience in group theory beyond knowing what a finite simple group is, as well as the other basic parts of the topic, so far as I can tell, so if the actual answer is too complex to be summarised clearly then just say and I'll understand that. So far as I can tell the classification of finite simple groups states that if you were to find a finite simple group of order n, it would either belong to one of the four families or is one of the 26 sporadic simple groups. Is it proven then that there are no more possible families of finite simple groups, so incredibly large that we couldn't find them with computation? Is it proven that all of the sporadic simple groups smaller than the monster have been found, and that there cannot be any greater ones? If the answer is yes for either one or both of those questions, then is there any way to summarise why in fairly simple terms? If the answer is no for either then what is the mathematical community's opinion on that?","['finite-groups', 'group-theory', 'abstract-algebra', 'simple-groups']"
902242,"Let $J$ be a $k \times k$ jordan block, prove that any matrix which commutes with $J$ is a polynomial in $J$","Let $J$ be a $k \times k$ jordan block, prove that any matrix which commutes with $J$ is a polynomial in $J$. I appreciate your hints, Thanks","['jordan-normal-form', 'matrices', 'linear-algebra']"
902247,Monotonic curvature and self intersections.,"I'm trying to prove that if $\alpha: I \to \Bbb R^2$ is a differentiable curve, where $I$ is an interval, has strictly monotonic curvature, then $\alpha$ has no self-intersections. My attempt : We suppose WLOG that $\alpha$ is parametrized by arc-length. If the curvature $\kappa$ is strictly monotonic, and is defined on an interval, then $\kappa$ is injective. We consider $t_0, t_1 \in I$ such that $\alpha(t_0) = \alpha(t_1)$, and I want to prove that $t_0 = t_1$. If I manage to get $\kappa(t_0) = \kappa(t_1)$, then I'm done. For me, it is rather clear that this will be indeed the case, but I'm failing to justify that. My first thought was that: $$\alpha(t_0) = \alpha(t_1) \implies \alpha'(t_0) = \alpha'(t_1) \implies \alpha''(t_0) = \alpha''(t_1) \implies \kappa(t_0) = \kappa(t_1)$$
and so we would use $\kappa$'s injectivity to get $t_0 = t_1$. But the first two implications are clearly false, in general. Is there some way to use the hypothesis we have to conclude these implications, or is there another approach to the problem whatsoever? Thanks for your time. On the other hand, I think that this might be false, since we could ""glue"" some curves together, like this: What to do? Edit: As pointed by Semiclassical in the comments, the image on the pic is not a counter-example, since the curvature first increases, then decreases. But the fault in my first attempt still remains.","['curvature', 'curves', 'differential-geometry']"
902256,$f(z)$ and $g(z)$ are Meromorphic functions such $|f(z)|\le|g(z)|$ for all $z\in\mathbb{C} $ then $ f=ag$,"We know that if $f(z)$ and $g(z)$ are entire functions such that $g(z)\ne0$ and $|f(z)|\le|g(z)|$ for all $z\in\mathbb{C} $ then by Liouville's theorem $$ f=ag$$ for some constant $a\in \mathbb{C} $ . Now my question is this that similar to above argument, if $f(z)$ and $g(z)$ are Meromorphic functions such $|f(z)|\le|g(z)|$ for all $z\in\mathbb{C} $ then I want to show $$ f=ag$$ for some constant $a\in \mathbb{C} $ . I am thinking in this way that because because poles and zeros of Meromorphic functions are isolated , by the Riemann's theorem on removable singularities and By using analytic continuation to eliminate removable singularities also we can have $$ f=ag$$ for some constant $a\in \mathbb{C} $ . Is this true way?",['complex-analysis']
902274,Zariski cohomology of $\mathbb{A}^1$ over a local ring with values in $\mathbb{G}_m$,"Let $X$ be a the spectrum of a regular local ring. What is known about the vanishing of the Zariski cohomology group
$$
H^n(\mathbb{A}^k_X,\mathbb{G}_m)
$$
for $n,k\geq 0$? If $X$ has dimension $d$ and if $n>k+d$ so that $\mathbb{A}^k_X$ has dimension smaller than $n$, then the group is zero but can one say more?",['algebraic-geometry']
902280,Why doesn't this converge?,"Why doesn't $$\int_{-1}^1 \frac{1}{x}~\mathrm{d}x$$ converge? I mean you would think that because of symmetry the area from the negative side and positive side cancel out, resulting in the integral equaling zero. By the way, I'm familiar with the way to test if an integral converges by splitting the integral into $0$ to $1$ and $0$ to $-1$ and adding up the integral sums. I don't really understand why this is valid though in the case of symmetry.","['definite-integrals', 'calculus', 'integration', 'functions']"
902300,How to solve this finite-difference equation?,"How to solve the following finite-differences equation:
$$f(x) = f(x-1) + f(x-\sqrt{2}), \quad x\in [\sqrt{2}, +\infty) \,?$$
Let's say $f(x) = f_0(x)$ for $x \in [0, \sqrt{2})$ is a given function. My attempt: Let's write characteristic function $L(\lambda) = 1 - e^{-\lambda} - e^{-\lambda\sqrt{2}}$. We need to solve the equation $L(\lambda) = 0$. There should be a root $\lambda_0 > 0$ (because $L$ is monotonously increasing, $L(0) = -1$ and $\lim_{\lambda \to +\infty} L(\lambda) = 1$), but my first question there is: how many other roots it has? Is it true that it has infinite number of (complex) roots? why?..","['recurrence-relations', 'finite-differences', 'functional-analysis', 'functional-equations']"
902304,A probability of a monochromatic cycle on a randomly colored lattice graph.,"Let $G$ be an undirected $6 \times 6$ lattice graph.  The $36$ vertices of $G$ are each randomly colored with one of $5$ colors with equal probability.  Such a coloring is called ""successful"" if and only if there exists at least one nontrivial monochromatic cycle.  What is the probability that such a random coloring on $G$ is successful? For a simpler version of this question, suppose we are interested in the probability that $G$ contains a monochromatic $4$-cycle--i.e., a $2 \times 2$ square.  This should be more tractable, I think.  It is very easy to compute explicit probabilities in closed form for $2 \times 2$ and $3 \times 3$ lattices.  Alternatively, we could consider fewer colors, say just $2$. As a side question, has this sort of problem been investigated in the mathematical literature?  Generalizations that provide bounds on the number of such graphs? (Bonus points if you know where the inspiration for this question comes from.)","['discrete-mathematics', 'graph-theory', 'combinations', 'probability', 'combinatorics']"
902306,On the definition of groups of multiplicative type,"Let $k$ be a field of characteristic 0. The definition of a linear algebraic $k$-group of multiplicative type (m.t.) I've seen the most in the literature is that $G$ is of m.t. if it is a $\bar{k}/k$-twist of a closed subgroup of a torus. Some authors however define $G$ as a commutative group which is an extension of a finite group by a torus. Why are these two definitions equivalent? Also, from the first definition it's clear that a closed subgroup of a group of m.t. is again a group of m.t. From the second definition this is not that clear. Let me expand this a bit. Suppose $G$ is of m.t. Then $G$ is commutative and fits a s.e.s. $1 \to H \to G \to F \to 1$ with $H$ a torus and $F$ finite. Take a closed (commutative) subgroup $G'$ of $G$. How do we construct a s.e.s. $ 1 \to H' \to G' \to F' \to 1$ with $H'$ a torus and $F'$ finite such that $G'$ fits it? What I'm having trouble is seeing why $H'$ should be a torus again. My idea was to take the connected component of the identity of $G'$ and try to build a sequence with that, but I'm not too sure.","['homological-algebra', 'algebraic-geometry', 'group-theory']"
902311,The Day Camp Stacking Game,"My friend works at a day camp as a counselor and he told me about an interesting game he plays with his group of kids. You have a perfectly shuffled, regular $52$-card deck and a group of $2 \leq n < 52$ kids under your supervision. You ask the kids to sit in circle and randomly distribute a card to each one of them. Thus, each individual gets either a red or a black card (depending on the suit) that they stick on their forehead for everyone to see. The counselor then sits in the middle of the circle and passes through the remaining $52-n$ cards from top to bottom of the deck and show each card to the group. The rules are as follows: Each kid has an assigned chair to start the game. This chair can either be vacant or occupied during the game and cannot be moved. If a red (resp. black) card is shown, each kid with a red (resp. black) card sits on the chair to his/her left. If someone is already on the chair, the kid sits on him/her. If a kid has someone on top of him/her, he/she cannot move when his/her color is drawn, i.e. only kids on top of a stack can move. If two kids have the same color and this color is drawn, they rotate (shift) clockwise by $1$, $e.g.$ if $n=4$ and the colors are $RRBR$ and red is drawn, then the new configuration is $R \sharp (BR) R$, where $(BR)$ means black kid under red kid and $\sharp$ denotes an empty seat. It is possible that no kid moves on a card. The game ends when there are no more cards in the deck. Everybody wins in this game, like any day camp game. I found this game extremely fascinating due to its underlying mathematical structure. It gives rise to some deep mathematical problems which I am currently trying to understand. Let's assume the game does not stop when the deck of cards is empty, but instead after some multiple (called turn ) of the remaining cards are shown, i.e. after $(52-n)m$ cards are shown for some $m$. I wish to analyse two cases, namely cycling though the deck after each turn or shuffling the deck after each turn. By cycling , I mean that the first card that was shown in the first turn is the first card shown in the second turn and so on. In case of cycling, what is the probability that, at some point in the game, all $n$ children are stacked on top of each other? What about shuffling? In both cases, what happens when we let $m \to \infty$? I know very little about probabilities, let alone combinatorics, but I feel like these questions might be difficult to answer. This problem might even be undecidable. The game somewhat reminds me of an ""ordered"" version of the Tower of Hanoi with additional rules, which I know how to solve but I can't see how it could be useful in the current situation. One thing I noticed is that in either cases, the full stack $-$ if it ever happens $-$ will alternate in color as a direct corollary of rule 2. I had some ideas, for instance considering a graph with charges on each vertex and then realizing a stacking as an edge contraction and charge transfer, but this seems overly complicated. How would you go about this problem? Any help would be greatly appreciated.","['card-games', 'probability', 'combinatorics']"
902313,Why are clopen sets a union of connected components?,"The wikipedia page on clopen sets says ""Any clopen set is a union of (possibly infinitely many) connected components."" I thought any topological space is the union of its connected components? Why is this singled out here for clopen sets? Does it have something to do with it $x\in C$ a clopen subset $C$ of a space $X$, then $C$ actually contains the entire component of $x$ in $X$?",['general-topology']
902316,"Proof of ""continuity from above"" and ""continuity from below"" from the axioms of probability","One of the consequences of the axioms of probability ($\sigma$ field and probability axiom) is the ""infinite subset"" and ""infinite union"" property, I can't figure out how it follows from them. if $A_1 \subseteq A_2 \subseteq \cdots$ then $$P\left(\bigcup_{i=1}^{\infty}A_i \right) = \lim_{k \to \infty} P(A_k)  $$ If $A_1 \supseteq A_2 \supseteq \cdots$ then $$P\left(\bigcap_{i=1}^{\infty}A_i \right) = \lim_{k \to \infty} P(A_k)  $$","['probability-theory', 'probability']"
902320,Is there a theory of integration in elementary terms for definite integrals?,"Let's call a real number explicit if it can be expressed starting from integers by using arithmetic operations, radicals, exponents, logarithms, trigonometric and inverse trigonometric functions. For complex numbers $i$ is allowed in addition to integers, and we get what Chow calls EL numbers . Rational numbers, $e=\exp(1)$ and $\pi=\cos^{-1}(-1)$ are explicit, some algebraic numbers and Euler's $\gamma$ probably are not. If a function has an elementary anti-derivative then a definite integral of it with explicit limits is also explicit. The converse is not true, $\int x^{-2}e^{-\frac1{x^2}}dx$ is not elementary, but $\int_0^1 x^{-2}e^{-\frac1{x^2}}dx=\frac{\sqrt{\pi}}{2}$ is explicit (this is the Gaussian integral up to substitution). There is a theory of Liouville that allows proving that some anti-derivatives are not elementary, is there something similar for definite integrals? There was a long standing unanswered question on MSE about computing $\int_0^{\frac{\pi}{2}}\frac1{(1+x^2)(1+\tan x)}dx$, and the consensus was that it's not expressible in elementary terms. How does one prove something like that? EDIT: Since there doesn't appear to be a general theory any non-trivial example of a proof for a particular case would be interesting. Also, explicit or EL numbers above are only an example, proving non-expressibility in some other reasonable terms would also be interesting.","['closed-form', 'calculus', 'integration', 'definite-integrals', 'elementary-functions']"
902350,what is the meaning behind this combinatorial identity,In the following comment: Solution of $\large\binom{x}{n}+\binom{y}{n}=\binom{z}{n}$ with $n\geq 3$ $$ \binom{2n-1}{n} + \binom{2n-1}{n} = \binom{2n}{n} $$ I'm wondering about the meaning of this equation - i.e why it is true.(of course I can see mathematically why this is true). Also I'm thinking if there is any general equation that this is just private case (i.e equation that will right also for number which are not 2 or something like this),['combinatorics']
902352,Finding the closest matrix of a given form,"Let's say I have a vector $(a_1\dots a_n)$ , where each component is between $-1$ and $1$ .
Now from this vector I define a $n\times n$ matrix $M$ such that $$M_{ij} = \begin{cases} 1&\,& i = j\\ a_i\cdot a_j&\,& i\neq j\end{cases}$$ I now call $E$ the space of all matrices $M$ I can build with that process. Now, I have a given matrix $A$ that can be assumed to be symetrical, having $1$ on the diagonal and that is symetric definitive positive. Is there a way to find a matrix from the space $E$ that is the closest to $A$ ? By ""closest"" I mean with any norm that seems reasonable. I guess this can be difficult to solve, and I'm just trying to find direction for this problem. Alternatively, any iterative solution that can converge to the result could be fine as well. Any idea? Thanks!","['optimization', 'matrices', 'linear-algebra']"
902373,Image of a entire function.,"Let $f:\mathbb{C} \rightarrow \mathbb{C}$ be a non-constant entire function. by Liouville's Theorem, $f(\mathbb{C})$ is dence in $\mathbb{C}$. by the Open Mapping Theorem $f(\mathbb{C})$ is open in $\mathbb{C}$. by a simple argument from general topology $f(\mathbb{C})$ is connected. What more can we say, about $f(\mathbb{C})$?",['complex-analysis']
902397,Showing that $\sum_{i=1}^n \frac{1}{|x-p_i|} \leq 8n \left( 1 + \frac{1}{3} + \frac{1}{5} + \cdots + \frac{1}{2n-1} \right)$,"I'm taking a summer analysis course and preparing for our final exam later this week. Our professor gave us the following problem on our mock exam, and I can't seem to get anywhere on it. Does anyone have an idea of how one might proceed? I've tried thinking about it in terms of a geometric series, but that got me nowhere. I've also studied the behavior of the first several partial sums, but again to no avail. Let $0 \leq p_i \leq 1$ for $i = 1, 2, \dots, n$. Show that $$ \sum_{i=1}^n \frac{1}{|x-p_i|} \leq 8n \left( 1 + \frac{1}{3} + \frac{1}{5} + \cdots + \frac{1}{2n-1} \right)$$ for some $x$ satisfying $0 \leq x \leq 1$.","['summation', 'sequences-and-series', 'real-analysis']"
902401,Families of Idempotent $3\times 3$ Matrices,"I did the following analysis for $2\times2$ real idempotent (i.e. $A^2=A$) matrices:
$$
\begin{bmatrix}a&b\\c&d\end{bmatrix}^2=\begin{bmatrix}a^2+bc&(a+d)b\\(a+d)c&bc+d^2\end{bmatrix}=\begin{bmatrix}a&b\\c&d\end{bmatrix}
$$
So in particular we have $(a+d)c=c$ and $(a+d)b=b$ so if either $b$ or $c$ is nonzero we have $a+d=1$. We also see that $a$ and $d$ both satisfy the equation $x^2+bc=x\iff x^2-x+bc=0$ which is a quadratic equation having solutions
$$
x=\frac{1\pm\sqrt{1-4bc}}{2}=0.5\pm\sqrt{0.25-bc}
$$
But this is only possible if $bc\leq 0.25$ for otherwise the above expression is not real. This gives us the following cases: CASE 1: If $b,c=0$ we have $x\in\{0,1\}$ and since $a+d=1$ is unnecessary we have four possibilities: $(a,d)\in\{(0,0),(1,0),(0,1),(1,1)\}$. CASE 2: If $bc=0.25$ we have $x=0.5$ so $a=d=0.5$. CASE 3: If $bc<0.25$ yet $(b,c)\neq(0,0)$ we have $x\in L=\{0.5-\sqrt{0.25-bc},0.5+\sqrt{0.25-bc}\}$ and to have $a+d=1$ we must have $\{a,d\}=L$ so that if $a$ is one solution, then $d$ is forced to be the other solution. Or the other way around. The cases can be illustrated via the following diagram graphing the hyperbola $xy=0.25$ corresponding to CASE 2 , the area $xy<0.25$ corresponding to CASE 3 , and the point $(0.0)$ corresponding to CASE 1 : The blue bands show the graphs of $xy=k$ for $k=0.05$ to $0.20$ and the cyan bands show $xy=k$ for $k=-0.05,-0.10,...$ For instance one could choose $(b,c)=(3.75,-1)$ so that $\sqrt{0.25-bc}=2$ thus rendering $x=0.5\pm 2=-1.5$ and $2.5$ and form the matrix
$$
A=\begin{bmatrix}-1.5&3.75\\-1&2.5\end{bmatrix}
$$
which will then be idempotent, as an example of CASE 3 . QUESTIONs: Can similar descriptions be derived for $3\times 3$ matrices? Is this a well known description of idempotent $2\times 2$ matrices?","['idempotents', 'linear-algebra']"
902413,Intersection of Images of a function,"I'm trying to understand intuitively why the image ( under some function ) of the intersection of subsets of the domain of that function  is only contained ( and not equal ) to the intersection of the images of the corresponding sets. What's the intuition on to why $f\left(\bigcap_{i\in I}A_i\right)\subseteq\bigcap_{i\in I}f(A_i)$ and not $f\left(\bigcap_{i\in I}A_i\right)=\bigcap_{i\in I}f(A_i)$. Whats the intuition on to why the intersection of images might contain more elements than the image of intersections ? I even have one counter-example, suppose $f :\mathbb R \to\mathbb R$ defined as $f(x) = x^2$. Then indeed, if A1 = [-1,0] and A2 = [0,1], then the image of the intersections must be contained ( and not be equal ) to the intersection of images. But i'm trying to abstract and see what's the general requirement for the function and for the family of sets  so that the image of intersection is contained and not equal to the intersection of images.","['elementary-set-theory', 'functions']"
902422,$\int_{|z| = 2} \frac{1}{f(z)(1+f(z))^2} dz$ where $f(z) = z^{1/2}$ with branch such that $\Re f(z) \geq 0$,"As the title states, the definite integral in question is $$\int_{|z| = 2} \frac{1}{f(z)(1+f(z))^2} dz,$$ where $f(z) = z^{1/2}$ with branch cut such that $\Re f(z) \geq 0$, i.e., the cut is the negative real axis. Edit : As pointed out in a comment, this isn't an integral that should be done with a keyhole contour (edited out of my original question). My question thus is, how does one evaluate this contour integral given that the contour is not a closed loop due to the presence of a branch cut? I am studying out of Ahlfors and integrating on a Riemann surface hasn't been introduced yet.","['branch-cuts', 'complex-analysis', 'contour-integration']"
902445,Is it absolutely certain that repeated random selection of integers from 0 to 100 will eventually select every integer?,"I would like to know if one can be absolutely certain, after a number of trials, to circle every integer from 1 to 100 using the following method: Let's say you write down integers 0 to 100 from left to right. Then take one 6-sided die and roll it. The result gives the number of integers from 0 to move to the right along the number line you wrote down. Circle that number. Do it again, moving along the number line and circling numbers. So if on the first roll the die resulted in a 3, you move 3 spaces from zero to the number 3. If the second roll results in a 5, you move 5 spaces from the number 3, to the number 8. Then circle number 8. So now you have 3 and 8 circled. Do this all the way up through the numbers until the last die roll results in a number that would put you past 100. Call this set of results ""layer 1"". Now repeat this again on the exact same set of numbers, with some already circled. This will be called ""layer 2"". Then do ""layer 3"" and so on until all numbers from 1 to 100 are circled (if possible). What I am puzzled by is that common sense tells me that eventually all numbers from 1 to 100 will be circled. This appears to me to be absolutely certain. But the results depend on die rolls, which are based on probability, which says there's a small chance that 1 billion ""layers"" can still leave one number uncircled. I would guess that a resolution to this would be that there's a limit to how many layers of this there could be before all numbers are circled. Or is there always going to be some probability that billions of layers of this can be done with still one or more numbers uncircled? I'm interested in seeing the mathematics behind this problem.","['dice', 'probability']"
902454,Eigen values of AB and BA,"let A be a linear transformation from $R^n$ to $R^m$, and B be a linear transformation from $R^m$ to $R^n$, it's easy to show that AB and BA has same eigen-value(except $0$). But my question is  how to show that the multiplicity of eigen-values are the same? can anyone give a proof just from the theory of linear transformation? I mean, without matrix computation.","['linear-algebra', 'eigenvalues-eigenvectors']"
902463,Projective curve $x^d+y^d+z^d=0$ is nonsingular using Jacobian matrix,"According to this question: Nonsingular projective variety of degree $d$ ,
  the curve $x^d+y^d+z^d=0$ in $\mathbb{P}^2$ is nonsingular. I'm trying to prove this. Hartshorne defines nonsingular points using the Jacobian matrix in $\mathbb{A}^n$ and proves proves that this is the same as the local ring being regular. Then the book proceeds to define nonsingular points for arbitrary varieties as having regular local rings. Since checking the Jacobian matrix is easier, I'm trying to show that we can do this in $\mathbb{P}^2$ too. Let $P=(1:0:0)$. Let $U_0$ be the open subset of $\mathbb{P}^2$ with $x\neq 0$. Since the local ring of $P$ in $U_0$ is the same as that in $\mathbb{P}^2$, it is sufficient to consider the image of the curve under the isomorphism in $U_{0}\to\mathbb{A}^2$. According to theorem 2.2 in the book, the image of $(1:0:0)$ is $(0, 0)$ and the image of $C\cap U_{0}$ (C is the curve) is $Z(1+y^d+z^d)$. The Jacobian matrix is $0$ at this point so my reasoning must be flawed. Where is the flaw? How can I apply the Jacobian matrix criteria in projective space and why?",['algebraic-geometry']
902472,"Perfect Map $p:\ X\to Y$, $Y$ compact implies $X$ compact","I was assigned the following homework problem for an introductory course in topology: Let $p:\ X\to Y$ be a closed continuous surjective map such that $p^{-1}(\{y\})$ is compact for each $y\in Y$.  Show that if $Y$ is compact, then $X$ is compact. My question is, are the assumptions that $p$ be continuous and surjective even necessary for the result to hold?  I've done the following: Let $U$ be an open set containing $p^{-1}(\{y\})$.  Since $X-U$ is closed, $p(X-U)$ is closed in $Y$.  Then $W=Y-p(X-U)$ is an open set that contains $y$.  Since $(X-U)\cap p^{-1}(W)=\varnothing$, $p^{-1}(\{y\})\subset p^{-1}(W) \subset U$. Let $\{U_\alpha\}$ be an open cover of $X$.  Then $p^{-1}(\{y\})\subset \bigcup_{k=1}^{N}U_k$.  For each $y\in Y$, define $W_{y}$ as above; then $\{W_y\}$ is an open cover of $Y$, so can be covered by $W_{y_1},...,W_{y_m}$, and $p^{-1}(\{y_j\})\subset p^{-1}(W_{y_j})\subset \bigcup_{k=1}^{N}U_k$.  Since $X=p^{-1}(Y)$, $X=\bigcup_{j=1}^{n} p^{-1}(W_{y_j})$.  Since each $p^{-1}(W_{y_j})$ is covered by finitely many $U_k$, so is $X$, so it is compact. If we dispense with surjectivity of $p$, the only thing that happens is that $p^{-1}(\{y\})$ may be empty, but then $p^{-1}(\{y\})\subset p^{-1}(W)$ holds vacuously, and the empty set is still compact. If $p$ is continuous, then $p^{-1}(W)$ is open, but I don't use the fact that it is open at all in my proof. I know that all 3 assumptions make a map ""perfect"" so that may be why they were mentioned, but can we dispense with continuity and surjectivity? Thanks.","['general-topology', 'proof-verification']"
902514,Exponential of a polynomial of the differential operator,"Given that $$\exp(aD)f(x)=f(x+a)$$ where $\exp(D)$ is the exponential of the differential operator $D$, is there a similar closed-form, general  expression for $\exp(g(D))f(x)$, where $g(D)$ is a polynomial function of $D$?","['lie-algebras', 'differential-geometry', 'exponential-function', 'lie-groups', 'taylor-expansion']"
902547,Normal Groups and Quotient Groups,"These concepts are currently confusing me. My reading first defined a normal subgroup as one that is the kernel of a group homomorphism. Then it introduced the terms ""left coset"" and ""right coset,"" which seem straightforward enough. The next definition was that of the index of a subgroup $[G:H]$. Then, it gave another definition of a normal subgroup (?), namely, one invariant under conjugation ($gNg^{-1} = N \,\,\forall \, g \in G$). Finally, it gave the following tortuous definition of a quotient group: The set of all left cosets of a normal subgroup $N$ with the law of composition $(gN)(hN) = (gh)N$ and having order $[G:N]$. Why are those definitions equivalent? And what does that last horrendous definition mean? I'd really appreciate it if someone could provide an intuitive/simple explanation of these terms.","['group-theory', 'abstract-algebra']"
902568,Mean and Variance of the Weibull Distribution,"The density of the Weibull Distribution is given by: $$f(x) = \alpha x^{\alpha-1}e^{-x^{\alpha}}$$ The Gamma function is defined as:
$$\Gamma(\alpha)=\int_{0}^{\infty}x^{\alpha-1}e^{-x} \,dx$$ Show that $E(X)=\Gamma(\frac{1}{\alpha}+1)$ and $Var(X)=\Gamma(\frac{2}{\alpha}+1)-\Gamma^2(\frac{1}{\alpha} + 1)$",['probability']
902571,Sum of the $11^\mathrm{th}$ power of the roots of the equation $x^5+5x+1=0$,"Find the sum of the $11^\mathrm{th}$ power of the all roots of the equation $$
x^5+5x+1=0
$$ My Attempt: Let $R=\{\alpha,\beta,\gamma,\delta,\mu\}$ be the set of all roots of the equation ${x^5+5x+1=0}$, and let $x\in R$. Then we have $$
\begin{align}
x^5 &= -\left(5x+1\right)\\
x^{10}&=25x^2+1+10x\\
\tag1 x^{11}&=25x^3+10x^2+x
\end{align}
$$ Taking the sum of $(1)$ on all elements $x\in R$ gives us $$
\sum_{x\in R}x^{11}=25\cdot\sum_{x\in R}x^3+10\cdot\sum_{x\in R}x^2+\sum_{x\in R}x
$$ How can I solve the problem from this point?",['algebra-precalculus']
902573,What trig. identity would help solve $2 + \cos(2x) = 3\cos(x)$?,I need help with a homework question that has me puzzled. I need to solve the following equation: $$2 + \cos(2x) = 3\cos(x)$$ I don't see a good trig identity to apply. I tried $\cos(2x) = 2\cos^2(x) - 1$ but that did not seem to help. What would be a good identity to try instead?,"['trigonometry', 'algebra-precalculus', 'quadratics']"
902580,"Example of a function $u\in L^\infty(0,T,H^1)$ such that $u_t\notin L^\infty(0,T,H^1)$","Could someone give me an example of a function $u\in L^\infty(0,T,H^1)$ such that $u_t$ exists (in the distributional sense), $u_t\in L^\infty(0,T,L^2)$ and $u_t\notin L^\infty(0,T,H^1)$? Thanks. EDIT (to add context). Let $f_0\in H^1$. If $u(t)=f_0$ (constant function with respect to $t$) then $u\in L^\infty(0,T,H^1)$ and $u_t\in L^\infty(0,T,H^1)$. If $u(t)=\int_0^t f_0\;dt$, the same occurs. These are simple examples in which I've thought. Essentially, my question is: How to derive $u$ with respect to $t$ affects the regularity of $u(t)$ with respect to $x$? I think that examples can help me to understand it.","['sobolev-spaces', 'measure-theory', 'functional-analysis', 'distribution-theory']"
902581,"Integrate $\int \sqrt{(\sec{x} +\tan{x})}\ \cdot \sec^2x\,dx$","Integrate: $$\int \sqrt{(\sec{x} +\tan{x})}\ \cdot \sec^2x\,dx$$ My attempt : I substituted $\sec{x} + \tan{x} $ as $t^2$ Then, $$ (\sec{x} \cdot \tan{x} + \sec^2x) dx =2tdt$$ $$\sec{x}(  \tan{x} + \sec{x}) dx =2tdt$$ $$\sec{x}\cdot t^2 dx =2tdt$$ But there is  $\sec{x}$ left outside.",['integration']
902592,Confused over the solution of partial differential equation $xu_x+u_t=0$,"Consider, $$ \displaystyle x\frac{\partial u}{\partial x}+\frac{\partial
 u}{\partial t} = 0 $$ with initial values $ t = 0 : \ u(x, 0) = f(x) $ and calculate the
 solution $ u(x,t) $ of the above Cauchy problem using the method of
 characteristics. And here is the solution, I will point out where i am stuck: We parametrise the initial conditions by $\mathbb n:x_0(\mathbb n)=\mathbb n$, $t_0(\mathbb n)=0, u_0(\mathbb n)=f(\mathbb n)$ Solve the characteristic equations $$
    \matrix{
     \frac{\partial x(\sigma,\mathbb n)}{\partial \sigma} = x, && x(0,\mathbb n)=n \\ 
     \frac{\partial t(\sigma,\mathbb n)}{\partial \sigma} = 1, && x(0,\mathbb n)=n \\
     t(0,\mathbb n)=0: && x(\sigma,\mathbb n)=e^{\sigma}\mathbb n \\
     t(\sigma,\mathbb n)=\sigma
    }
    $$ This is where i am stuck and confused How did they get $x(\sigma,\mathbb n)=e^{\sigma}\mathbb n$? I just cannot see where the $e$ came from, please forgive my stupidity but can someone please tell me how they got this solution? When i integrate i do not get this! I will put the rest of the solution so the reader can follow, I am only stuck with the part mentioned though Calculate $\sigma$ and $\mathbb n$ in terms of $x$ and $t$ (coordinate change) $$ \sigma = t, \mathbb n = xe^-{t} $$ Solve the compatibility condition $ \frac{\partial u}{\partial \sigma} = 0, u(0,\mathbb n) = f(\mathbb n) $. Hence, $u(\sigma)=f(\mathbb n)$ Hence we have $u(x,t)=f(xe^{-t})$","['ordinary-differential-equations', 'partial-differential-equations']"
902603,Why does $(a+b)^2= a^2+b^2 + 2ab$? Why is the $2ab$ there?,"When I was doing research on finding the derivative I came across something strange. If $f(x) = x^2$ you find the derivative by going $$\frac{f(x+h)^2-f(x)^2}{h}
=\frac{x^2+2xh+h^2-x^2}{h}.$$ Why is there the $2xh$? Can someone explain the logic behind this? I'm assuming you plus $x$ and $h$ together then square it but why?",['algebra-precalculus']
902622,What does $\frac12(D_{2p}\times D_{2p})$ mean in group theory?,"Reading a thesis, I have come across the (unexplained) notation
$$\frac{1}{2}(D_{2p}\times D_{2p})\cong (p\times p):2,$$
where $D_{2p}$ is a dihedral group.   What does this ""$\frac12$"" notation mean?  For what groups $G$ does $\frac12 G$ make sense?","['notation', 'group-theory']"
902626,Showing that the Binary Icosahedral Group (given by a presentation) has order $120$.,"Say we have a group generated by $a, b$, with the relations $(ab)^2=a^3=b^5$ (note that these are not necessarily equal to the identity). How do I show that the group has $120$ elements? Without having orders given to any of the generators, why can't there be infinitely many elements? This problem is from Stillwell's ""Mathematics and its History,"" after Section 22.8. I'm first asked to show that adding the relation that all $3$ equal the identity gives the icosahedral group, isomorphic to $A_5$, and then to show that this implies the group without that relation has at least $60$ elements. Then, a parenthetical after that problem says ""Harder: Show that it in fact has $120$ elements"".",['group-theory']
902649,Variational formulations in group theory?,"I apologise if this is a naÃ¯ve question. Are there any known / widely applicable / important variational formulations in (finite) group theory? That is, a relationship of the form
$$\alpha(G) = \sup\{\phi(g) : g \in G\}, $$
where $\alpha$ is a number (or set) determined by some property of the group $G$ and $\phi$ is a real-valued (or set-valued) function on $G$. For a silly example, take $\phi(g)$ to be the order of an element and $\alpha$ the order of the group; the group is cyclic iff $\alpha(G) = \#G = \sup_{g \in G}\phi(g).$ I would also be interested in results on the probability distributions induced on groups or reflected in their structure in some way. For instance, suppose a finite group $G$ acts on a finite set $A$, and let $\phi : G \rightarrow \mathbb{N}_{\geq 1}$ be a function such that
$\phi(g) = \#\{a \in A : g\cdot a = a\}$. In other words, $\phi$ assigns to each $g \in G$ the number of fixed points it has in $A$. Now let $X$ be a random variable uniformly distributed on the finite set $G$. Then Burnside's lemma states that the expectation $\mathbb{E}[\phi(X)]$ gives us the number of orbits of the action:
$$|A/G| = \mathbb{E}[\phi(X)] \stackrel{\cdot}{=} \sum_{g \in G} \frac{\phi(g)}{|G|}.$$
I'd appreciate any key words / things to Google. I assume many things come from number theory.","['random-variables', 'probability', 'finite-groups', 'big-list', 'group-theory']"
902655,Application of Bessel Function,"I have read number of books and online literature on Bessel function. Theoretically, I have known about Bessel function. What is practical significance of Bessel function? How can Bessel function practically interpreted?","['geometry', 'ordinary-differential-equations', 'special-functions', 'calculus', 'bessel-functions']"
902656,How does a differential act when we identify $T_p(M\times N)\cong T_{p_1}M\times T_{p_2}N$?,"It's fairly common to identify the tangent space of a product manifold as
$$
T_p(M\times N)\cong T_{p_1}M\times T_{p_2}N
$$
where $p=(p_1,p_2)$, and the actual isomorphism is given by $v\in T_p(M\times N)$ is sent to $(d(\pi_M)_p(v),d(\pi_N)_p(v))$, where the $\pi_N$ and $\pi_M$ are the projections. If we make this identification, how do differentials act on the identified elements? I mean, suppose you have a smooth map $F:M\times N\to S$, and a corresponding differential $dF_p:T_p(M\times N)\to T_{F(p)}S$. If $v\in T_p(M\times N)$, using the isomorphism I could identify $v$ with $(v_1,v_2)$ for $v_1\in T_{p_1}M$, $v_2\in T_{p_2}N$. I'm confused on how one could evaluate $dF_p$ on $(v_1,v_2)$, since it's in a new tangent space, albeit an isomorphic one. What would be the correct way to evaluate $dF_p(v_1,v_2)$ after this identification?","['differential-topology', 'smooth-manifolds', 'differential-geometry']"
902668,Tractrix exercise,"Exercise: Let 
$$\begin{align*}\gamma:(0,\pi) &\to \mathbb R^2\\
t &\mapsto \gamma(t)=(\sin t,\ \cos t+\log \left(\tan(\frac{t}{2}) \right), \end{align*}$$
be the parametrized curve of the tractrix. Let $P$ be a point on the tractrix, $L$ the tangent line that passes through $P$, and $Q$ the intersection of $L$ with the $y$ axis. Prove that the distance between $P$ and $Q$ is $1$. My attempt and questions: If $P=\gamma(t_0)$, with $t_0 \in (0,\pi)$, then the tangent line $L$ is $$L: \gamma(t_0)+\lambda\gamma'(t_0).$$ The intersection of the line and the $y$ axists is the point $Q=(0,\gamma_2(t_0)-\dfrac{\gamma_2'(t_0)}{\gamma_1'(t_0)}\gamma_1(t_0)$. I've tried to find the point of intersection between the tangent line and the $y$ axis and then calculate $\|P-Q \|^2$ but I got stuck. $$\|P-Q\|^2=\|(\gamma_1(t_0),-\frac{\gamma_2'(t_0)}{\gamma_1'(t_0)}\gamma_1(t_0))\|^2$$ I've calculated 
$$\gamma'(t_0)=\left(\cos(t),-\sin(t)+\frac{\sec^2(\frac{t_0}{2})}{2\tan(\frac{t_0}{2})} \right),$$ 
to know $\gamma_1'(t_0),\gamma_2'(t_0)$ but I couldn't arrive to anything. Also, what if $\gamma_1'(t_0)=0$?, This can perfectly be the case, for example take $t=\frac{\pi}{2}$. How can I analyze that case separately from the rest? Any suggestions, ideas, answers would be appreciated.","['multivariable-calculus', 'plane-curves']"
902714,"Prove that $f$ is differentiable in $(0,0)$ if and only if $\lim_{t\to0+} g(t)$ exists","Let $g:[0,\infty)\to\mathbb{R}$ be a mapping and $f(x,y)=xg(\sqrt{x^2+y^2})$ for all $(x,y)\in\mathbb{R^2}$. Prove that $f$ is differentiable in $(0,0)$ $\iff$ $\lim_{t\to0+} g(t)$ exists. My attempt: $\implies:$ We suppose that $f$ is differentiable in $(0,0)$, that is, there is a linear mapping $A=(a_1 \,\, a_2)$ such that $$f(x,y)-f(0,0)-A(x,y) = R(x,y)$$ where R is a remainder term satisfying $\lim_{(x,y)\to(0,0)}\frac{R(x,y)}{|(x,y)|} = 0$. Given the definition of $f$, we see that $f(0,0)=0$, so that $$f(x,y)-A(x,y) = R(x,y) \\ \iff xg\big(\sqrt{x^2+y^2}\big)-A(x,y) = R(x,y) \\ \iff g\big(\sqrt{x^2+y^2}\big) = \frac{R(x,y)}{x}+\frac{1}{x}A(x,y) = \frac{R(x,y)}{x}+\frac{1}{x}(a_1x + a_2y).$$ Now we use polar coordinates: $x=r\cos\theta$ and $y=r\sin\theta$, which gives $$g(r) = \frac{R(r\cos\theta, r\sin\theta)}{r\cos\theta}+\frac{1}{r\cos\theta}(a_1r\cos\theta+a_2r\sin\theta) \\= \frac{R(r\cos\theta, r\sin\theta)}{r\cos\theta}+a_1+a_2\frac{\sin\theta}{\cos\theta}$$ Now taking the limit as $r\to0$ the first term vanishes and we see that the limit is $$a_1+a_2\frac{\sin\theta}{\cos\theta}.$$ $\impliedby:$ Now we suppose that $\lim_{t\to0+} g(t)$ exists. I will calculate the two partial derivatives and then show that the matrix $$A = \big(\partial_1 f(0,0) \,\,\, \partial_2 f(0,0)\big)$$ is indeed the derivative of $f$ in $(0,0)$. We have that $$\partial_1 f(0,0) = \lim_{t\to0} \frac{f(t,0)}{t} = \lim_{t\to0}\frac{tg(t)}{t}=\lim_{t\to0}g(t) = M$$ for some finite $M$; and $$\partial_2 f(0,0)=\lim_{t\to0}\frac{f(0,t)}{t} = 0.$$ We shall now investigate $$\lim_{(x,y)\to(0,0)} \frac{f(x,y) -f(0,0) - A(x,y)}{\sqrt{x^2+y^2}}.$$ Using polar coordinates this becomes $$\lim_{r\to0} \frac{r\cos\theta \,g(r) - Mr\cos\theta}{r} = \lim_{r\to0} \cos\theta \,g(r) - M\cos\theta = 0,$$ as desired. This post turned out much longer than I had anticipated -- I'm sure I made it far more complicated than necessary :)","['multivariable-calculus', 'derivatives']"
902721,Find the 1005th digit after the decimal point expansion of the square root of N.,"Let $N$ be the positive integer with $2008$ decimal digits, all of them $1$. That is, $N=1111...1111$, with $2008$ occurrences of the digit $1$. Find the $1005th$ digit after the decimal point expansion of $\sqrt{N}$. The proof given simply states two values and shows that their squares are greater than and less than $N$, and uses this to show that they are less than and greater than $\sqrt{N}$ and so the $1005th$ digit is $1$. This was from a calculator free exam so I don't see how this could have possibly been done, does anyone have any ideas?","['decimal-expansion', 'contest-math', 'number-theory']"
902722,Evaluate $\int{\sin^3(x)\cos^2(x)}dx$ [duplicate],"This question already has answers here : Evaluating $\int P(\sin x, \cos x) \text{d}x$ (3 answers) Closed 8 years ago . I'm trying to solve $\int{\sin^3(x)\cos^2(x)}dx$. I got $-\frac{1}{2}\cos(x)+C$, but the memo says $\frac{1}{5}\cos^5(x)-\frac{1}{3}\cos^3(x)+C$ This is my working: Your help is appreciated!","['trigonometry', 'calculus', 'integration', 'indefinite-integrals']"
902765,Ring of rational power series,Let $A$ be any commutative ring with 1. A power series $f\in A[[t]]$ is called rational if we can find a $g\in A[t]$ such that $fg\in A[t]$. It is clear that the set of rational power series forms a ring which I call $R_A$. I am looking for references giving properties of $R_A$ in terms of properties of $A$. For example is it true that $R_A$ is noetherian if $A$ is noetherian? If $A$ is a field this should be ok because $R_A=A(t)$ which is a field. Other properties I would like to know are flatness and integrality of $R_A$.,"['commutative-algebra', 'algebraic-geometry', 'abstract-algebra']"
902791,Finding a solution basis,"Find a real solution basis of $$y'=\left( \begin{matrix}-1&-2&0\\0&2&0\\-1&-3&2\\ \end{matrix} \right)y.$$ The characteristic equation of this matrix is $$P(t) = (1-t)(2-t)^2.$$
So next I calculated eigenvectors for the eigenvalues $1$ and $2$, which are $$u\overset{def}=(1,0,1) \text{ and }v\overset{def}=(0,0,1) \text{ respectively}.$$
The eigenvalue $2$ has algebraic multiplicity $2$ but it only has one eigenvector. So if I'm correct we need a principal vector. I computed this and it is $$v_p\overset{def}=(2,-1,0).$$ Now the solution basis is $$B=\Big\{t\mapsto u e^t, t\mapsto ve^{2t}, ?? \Big\}.$$ My question is, what is the third function? What solution does the principal vector I have computed correspond to? Thank you.","['ordinary-differential-equations', 'calculus']"
902792,Why are all convergent sequences necessarily Cauchy?,"I can understand the proof, which I could do myself: $|s_n - s_m| = |s_n - s + s - s_m|$ $\Rightarrow |s_n - s_m|  \leq |s_n - s| + |s_m - s|  $ For some $\epsilon > 0, \exists\ \ N(\epsilon) \in \mathbb{N} s.t.$ $ |s_n - s_m|  \leq \epsilon + \epsilon \ \  \forall \ \  m,n > N(\epsilon)$ $\therefore |s_n - s_m| \leq 2\epsilon$ where $2\epsilon \in \mathbb{R}$ The fact that the Cauchy sequence gradually closes up, i.e. the elements get closer together in obvious from the last point. $N(\epsilon)$ necessarily is a non-increasing  function of $\epsilon$. But I can't see this physically. I can think of instances in which the sequence may close up, then diverge, then close up again at infinity. What stops the sequence from coming as close as $\epsilon_1$ to the limit, then diverging, then coming back after some elements. Of course, the Cauchy sequence does not allow this, but I don't see how this instance violates the basic definition of an existence of a limit. The basic definition simply requires that $\forall \epsilon >0\exists N \in \mathbb{N}\  s.t. |s_n -s| < \epsilon$. There is no rule on the $\epsilon$ increasing with $N$.","['cauchy-sequences', 'limits']"
902797,How $\sqrt{2}=1+\frac{1}{\sqrt{2}+1}$?,I have found it in the chapter about chain fractionals. I am unable to transform it to such state. $$\sqrt{2}=1+\sqrt{2}-1=?=1+\frac{1}{\sqrt{2}+1}$$,"['radicals', 'arithmetic', 'algebra-precalculus']"
902798,"In a group of exponent $2^n$, $[x^{2^{n-1}},y^{2^{n-1}},\ldots,y^{2^{n-1}}]=1$?","In a group of exponent $2^n$, is the following equality true? $[x^{2^{n-1}},\underbrace{y^{2^{n-1}},\ldots,y^{2^{n-1}}}_n]=1$. Here, $[a, b, c]=[[a, b], c]$. Call the above question ""Question 2"". In this related question (which we shall call ""Question 1""), the OP asked whether $[x, y, y, y]$ is trivial in a group of exponent four. However, the post containing Question 1 has been edited heavily to also ask Question 2. Hence, I have posted this thread, which is meant as an outlet for Question 2. This means that Question 2 will no longer detracts from the original, tough, and rather interesting question which is Question 1. I hope this make sense to you...",['group-theory']
902822,About automorphisms of commutative semigroups,"Suppose that $M$ is a commutative monoid and that the product $P$ of $M$ and the nonnegative integers $\mathbb{N}$ with addition has no nontrivial automorphisms. The set $S$ of pairs $(m,n)$ in $P$ with $n>0$ is closed under addition. Can $S$ have a 
nontrivial automorphism?","['semigroups', 'abstract-algebra']"
902827,Proof that the limit of a sequence is $e=2.71828\ldots$,"Consider the sequence $\{a_n\}$ defined as follows $$\{a_n\}_{n\ge{1}}=\left(1+\frac{1}{n}\right)^n\;,\; n\in\mathbb{N}$$ The question is to prove that $\{a_n\}$ has a limit as $n\to\infty$ and to find that limit. This is a well known sequence and everbody in our class knows that the limit is Euler's number (i.e $e=2.718\ldots$ ). Indeed in high-school 'e' was defined as the limit of this particular sequence. However none of us could come up with a rigrous proof that this is indeed true. Do note that since we have only touched upon limits so far in our calculus class, our teacher probably doesn't want a proof involving integrals.","['sequences-and-series', 'calculus', 'limits']"
902835,Find the closure of $\mathbb{R}^{\infty}$ in $\mathbb{R}^{\omega}$ under the box topology,"Find the closure of $\mathbb{R}^{\infty}$ in $\mathbb{R}^{\omega}$ under the box topology. Note: $\mathbb{R^{\infty}}$ is the set of all sequences $(t_1,t_2,\dots)$ such that $t_i\neq0$ for only finitely many values of $i$, and $\mathbb{R}^{\omega}=\mathbb{R} \times \mathbb{R}\times \mathbb{R} \times \dots$ In Munkres' Topology , this is part of an exercise. My answer is that the set $\mathbb{R}^{\infty}$ is closed in the box topology hence, its closure is itself. Here is my justification of my answer: Suppose $x=(x_1,x_2,\dots)$ is not in $\mathbb{R}^{\infty}$.
  So for some $i_1,i_2,i_3,\dots$ we get, $x_{i_j} \neq 0$ for $j=1,2,3,\dots$ It's clear that for every $x_{i_j}$, there is a neighborhood $U_{i_j}$ of $x_{i_j}$ in $\mathbb{R}$ such that $0 \not\in U_j$. Now, let $U=\prod_{i=1}^{\infty} A_i$ where $A_{i_j}=U_{i_j}$ and $A_i$ is any open set of $x_i$ for $i \neq i_j$ for $j=1,2,3,\dots$ Now, $U$ is a neighborhood of $x$ but $U\cap \mathbb{R}^{\infty}=\emptyset$ so $x$ is not a limit point of $\mathbb{R}^{\infty}$ hence is not in the closure of $\mathbb{R}^{\infty}$. Why is $U\cap \mathbb{R}^{\infty}=\emptyset$? Because for every element $y=(y_1,y_2,\dots)$ of $U$, there are an infinite number of $y_i$'s such that $y_i\not=0$ (by definition) so $y\not\in \mathbb{R^{\infty}}$. My question is, is this answer true? If not, where does my argument go wrong? In this case, could you give me any hints to figure out the right answer myself? (hints not solution)",['general-topology']
902838,How prove this limit $\left(\frac{1}{2}+\sum_{k=1}^{n-1}(-1)^{\lfloor\frac{mk}{n}\rfloor}\{\frac{mk}{n}\} \right)^n=\frac{1}{\sqrt{e}}$,"let $m$ is even number,and $n$ is  odd number,and such $(m,n)=1$, show this limit: $$\lim_{n\to\infty}\left(\dfrac{1}{2}+\sum_{k=1}^{n-1}(-1)^{\left\lfloor\dfrac{mk}{n}\right\rfloor}\left\{\dfrac{mk}{n}\right\} \right)^n=\dfrac{1}{\sqrt{e}}$$ where $\{x\}=x-\lfloor x\rfloor$ I think we can find this sum $$\sum_{k=1}^{n-1}(-1)^{\left\lfloor\dfrac{mk}{n}\right\rfloor}\left\{\dfrac{mk}{n}\right\}$$
But I can't","['calculus', 'limits']"
902850,Uniform sampling with replacement item frequency,"Suppose we are sampling from $N$ distinct items uniformly with replacement $M$ times. What can be said about the distribution of frequencies of items drawn? For example, if I sort all the frequencies from greatest to smallest and plot them, what will be the shape of the curve fitting this data? For one particular item everything is more or less clear to me: expected number of times being drawn is $\frac{M}{N}$, variance is $\frac{M(N-1)}{N^2}$. But I don't understand how to get any general results. EDIT: at the time I was writing the question I was just too confused about the whole experiment, so I couldn't formulate a precise question. After @Euxpraxis1981's answer and running some simulations myself here is what I am interested in. Suppose $N \ll M$, we perform the described experiment and count how many items were drawn $0$ times, $1$ time, $2$ times, etc. If we plot these ""item counts"" for each ""frequency"", what will be the shape of the graph? I tried with $N=100$ and different $M$ from $1000$ to $100000$. The histograms I got look a lot like binomial distribution pmf, with a mean (the greatest number of items being drawn) of $\frac{M}{N}$.","['statistics', 'probability-theory', 'sampling', 'probability-distributions', 'probability']"
902851,"Show that $X$ is Hausdorff if and only if the diagonal $\Delta = \{(x, x):x \in X\}$ is closed in $X \times X$ [duplicate]","This question already has answers here : $X$ is Hausdorff if and only if the diagonal of $X\times X$ is closed (4 answers) Closed 9 years ago . I am aware that there is a similar question elsewhere, but I need help with my proof in particular. Can someone please verify it or offer suggestions for improvement? Show that $X$ is Hausdorff if and only if the diagonal $\Delta = \{(x, x):x \in X\}$ is closed in $X \times X$ The proof is trivial if $|X|=1$. So, assume that $|X|>1$. Suppose $X$ is Hausdorff. Let $(a, b) \in X \times X - \Delta$. Note that such an element exists, since $|X|>1$. Then, $a \neq b$. Since $X$ is Hausdorff, we can pick open sets $U_a$ and $U_b$ such that $a \in U_a$, $b \in U_b$, and $U_a \cap U_b = \varnothing$. Now, note that $(U_a \times U_b) \cap \Delta = \varnothing$ (Assume $(p, q) \in (U_a \times U_b) \cap \Delta.$ Then, $(p,q) \in \Delta$ implies that $p = q$. But then, $p \in U_a$ and $p \in U_b$, contradicting the fact that $U_a \cap U_b = \varnothing$). This implies that the set $X \times X - \Delta$ is open in $X \times X$. So, $\Delta$ is closed in $X \times X$. Now, suppose $\Delta$ is closed in $X \times X$. Then, $X \times X - \Delta$ is open in $X \times X$. So, there exists a basis element $U \times V$ of $X \times X$ such that $(a, b) \in U \times V$ and $U \times V \subseteq X \times X - \Delta$. This implies that $(U \times V) \cap \Delta = \varnothing$. Then, $a \in U$ and $b \in V$. Note that $U \cap V = \varnothing$. If such were not the case, then there exists a $y \in U \cap V$. This implies that $(y, y) \in U \times V$, contradicting the fact that $(U \times V) \cap \Delta = \varnothing$. This shows that $X$ is Hausdorff.","['general-topology', 'separation-axioms', 'proof-verification']"
902852,Smooth map on differential manifolds,"given two differential manifolds $M_1$ and $M_2$. I have to show that the projection $\pi: M_1 \times M_2 \to M_1$ is smooth. By definition, I then need to show that for a point $(a,b)\in M_1\times M_2$ with charts $(\psi,\phi): U\times \tilde U\ni (a,b) \to V\times \tilde V$ and open sets $U\subset M_1, \tilde U \subset M_2, V\subset \mathbb{R}^n, \tilde{V}\subset \mathbb{R}^k$, we have that 
\begin{align}
\phi^\prime \circ \pi \circ (\phi^{-1},\psi^{-1})
\end{align}
is smooth. I know that $\phi^\prime, \phi\text{ and }\psi$ are smooth but I only know that $\pi$ is linear. Is it sufficent to conclude that this map is smooth?","['smooth-manifolds', 'differential-geometry']"
902853,How can I bring $\sin(x)$ to the following form?,What steps do we take for the following? $$\sin x = \frac{{2\tan\frac{x}{2}}}{1+\tan^2\frac{x}{2}}$$,['trigonometry']
902905,Improper Integral $\int_0^1\left(\left\{\frac1x\right\}-\frac12\right)\frac{\log(x)}xdx$,"My initial question was to find if this integral 
$$ \int_0^1 \left(\left\{\frac 1x\right\}-\frac12\right)\frac{\log(x)}{x}dx$$ is convergent or divergent. ($\left\{\frac 1x\right\}$ is the fractional part of $\frac 1x$ ). My try :: \begin{align}\int_0^1\left(\left\{\frac 1x\right\}-\frac 12\right)\frac{\log(x)}{x} dx & =-\int_1^\infty (\left\{y\right\}-1/2)\frac{\log(y)}{y} dy \\ & = \sum_{m=1}^{\infty} \int_{m}^{m+1} (\left\{y\right\}-1/2)\frac{\log(y)}{y} dx \\ & = \frac14\sum_{m=1}^{\infty} \left(\log^2 (m+1)+\log^2(m)-2\int_0^1\log^2(x+m) 
dx \right) \\ &= ... \end{align}
Finally the integral is convergent since the series obtained is convergent. The curious thing is that Mathematica returns $0.\times 10^{-2}$ by numerical integration. Then my question is: Is this integral equal to zero? Thank you for your help.","['closed-form', 'sequences-and-series', 'calculus', 'integration', 'definite-integrals']"
902907,Alternate ways to find the limit of a given sequence,"I need to find the $\lim_{n\to\infty}\{x_n\}$ where $\{x_n\}$ is defined as $$\{x_n\}_{n\ge1}=n^{\frac{1}{n}}\;,\;n\in \mathbb{N}$$ Now if I had a function $f:\mathbb{R}-\{0\}\to\mathbb{R},\quad f(x)=x^{\frac{1}{x}}$, then I could easily find it's limit as follows:
$$f(x)=x^{\frac{1}{x}}\\taking\,natural\,logarithm\,on\,both\,sides\\\ln{f(x)}=\frac{1}{x}\ln(x)\\Let\,\lim_{x\to\infty}\ln{f(x)}=L\;then\\L=\lim_{x\to\infty}\frac{1}{x}\ln(x)\\Applying\;L'Hospital's\;Rule\\L=\lim_{x\to\infty}\frac{1}{x}=0\\\Rightarrow\lim_{x\to\infty}{f(x)}=1$$ Since my sequence is clearly a subset of this function, it's limit is also 1. However, I am worried that this method may not be 'rigrous' enough. Is there any other way to find the limit of this sequence? P.S : I tried finding the limit using the $\epsilon-N$ definition of a limit, but could not do so.",['calculus']
902916,Prove $f$ isn't continuous at $\frac{1}{\pi}$,"Let $f(x)=\left\lfloor {\sin {1 \over x}} \right\rfloor$ (meaning floor of $\sin x$). I need to prove that $f(x)$ isn't continuous at $x=\frac{1}{\pi}$. Proof: For a nehiborhood of $\frac{1}{\pi}$: $x < \frac{1}{\pi} \implies \frac{1}{x} > \pi \implies -1 < \sin \frac{1}{x} < 0
\implies \left\lfloor \sin \frac{1}{x} \right\rfloor = -1$
$x > \frac{1}{\pi} \implies \frac{1}{x} < \pi \implies 0<\sin \frac{1}{x} < 1 \implies \left\lfloor \sin \frac{1}{x} \right\rfloor = 0$ Now, the author claims: $$\lim\limits_{x\to \frac{1}{\pi}^+} f(x) = 0 \ne \lim\limits_{x\to \frac{1}{\pi}^-} f(x) = 1$$ Didn't he mean: $$\lim\limits_{x\to \frac{1}{\pi}^+} f(x) = -1 \ne \lim\limits_{x\to \frac{1}{\pi}^-} f(x) = 0$$ So is it a mistake of the author or am I missing something?","['calculus', 'trigonometry', 'real-analysis', 'ceiling-and-floor-functions', 'limits']"
902934,Solution of $y''(x) -k = \delta(x-x_0)y(x)$,"I need to solve following differential equation
$y''(x) -k = y\delta(x-x_0)$ subject to boundary conditions \begin{eqnarray}
y(x=-a) = 0 \\
y(x=b) = p
\end{eqnarray} I am not sure if it is possible to solve at all. I got solution for 
a similar problem . Can anyone suggest any way for moving ahead? EDIT Physical background: Non dimensional differential equation for Poiseuille flow, is $$y''(x) = k $$ Where $y(x)$ is transverse velocity and $k$ contains information of viscosity, gravity. to account for additional friction between wall and fluid velocity a term proportional to velocity and in the vicinity of wall is added(in the limit a $\delta$ function) so that differential equation is 
$$y''(x) -k = \delta(x-x_0)y(x)$$","['boundary-value-problem', 'ordinary-differential-equations']"
902957,Is division allowed in rings and fields?,"Is division allowed in ring and field? The definition of ring I am using here does not require the presence of multiplicative inverse. I think in general, division is not a well-defined operation in rings because division can only occur if multiplicative inverse exists (not sure about this though). For field, since multiplicative inverse exists for all elements in the field, if we want to perform the operation $x/y$ whereby $x$ and $y$ belongs to the field, we multiply $x$ by $1/y$. Is my understanding correct? This is my first abstract algebra class, so I am still quite confused.","['ring-theory', 'abstract-algebra', 'field-theory']"
902977,"Is entire function constant when $ |f(z)|\le \log|z|,\ |z|>1$.","Let $ f : \mathbb{C} \to \mathbb{C} ,$ entire and $|f(z)|\le \log|z|,\ |z|>1. $ Show that $f$ is constant. What first comes to mind is Louville's theorem, but log 's problems with analyticity confuse me.",['complex-analysis']
903004,Subsets $S$ such that $7 \notin S $ or $2 \notin S $,"How many subsets $S \subseteq\{1,2...10\}$ are there such that $7 \notin S $ or $2
 \notin S $? I can't find the right way to write a formal response. I think that we should consider at least $18$ subsets, deleting once $2$ and once $7$ from the entire set. Then we should consider all the pairs and triples excluding the same two elements and so on. I'm really confused, any tips?","['elementary-set-theory', 'combinatorics']"
903017,Placing a circle in a square lattice,"Two part question. Consider the square lattice $\mathbb{Z}^2$ : Imagine you are going to place a circle of radius $r$ somewhere in $\mathbb{R}^2$ . Question 1: What is the radius of the largest circle that cannot be placed anywhere in $\mathbb{R}^2$ without overlapping or containing any of the points in $\mathbb{Z}^2$ ? I'm fairly certain the answer is $r < \frac{\sqrt{2}}{2}$ . If you place a circle with center, for example $\left(\frac{1}{2}, \frac{1}{2}\right)$ , computing the radius that would touch the four corners is quite easy, but I don't know how to prove that this is the best place to do it, as intuitively obvious as it might be. Question 2: A circle with radius $r$ , such that $0 < r \leq \frac{\sqrt{2}}{2}$ is randomly placed somewhere in $\mathbb{R}^2$ . What is the probability, as a function of $r$ , that the circle contains or overlaps at least one point in the lattice? My suspicion is to try to only consider a limited subset of the 2D plane, e.g. $-1 < x < 1$ and $-1 < y < 1$ , and then do something with ratios of the area of the circle and the area of the limited region, but I'm not exactly sure what I would do with that. Obviously the probability is 1 for all $r \geq \frac{\sqrt{2}}{2}$ , because of question 1.","['integer-lattices', 'probability', 'circles']"
903028,Determinant-like expression for non-square matrices,"I'm interested in whether for any real matrix of size $m \times n$ there is a real number with the following properties: It is a polynomial expression with real coefficients in the entries of the matrix. The expression depends on $m,n$ only. It is zero precisely when the matrix is not of full rank ($\min\left\{m,n\right\}$). For square matrices, the determinant has these properties. If this is a known thing, what is it called and where can I read about it?","['matrices', 'linear-algebra', 'determinant']"
903038,Morphism of schemes $f\colon X\to Y$ associated to a continuous map of the underlying spaces $|X|\to |Y|$,"I am sorry for asking two questions in one but they are strongly related. What is an example of (affine?) schemes $X=(|X|,\mathcal{O}_X)$ and $Y=(|Y|,\mathcal{O}_Y)$ and a map of topological spaces $|f|\colon|X|\to |Y|$ that cannot be promoted into a map $f\colon X\to Y$ of schemes? I guess something like $exp:\mathbb{R}\to\mathbb{R}$ is an example but I cannot prove that it is an example. What is an example of (affine?) schemes $X=(|X|,\mathcal{O}_X)$ and $Y=(|Y|,\mathcal{O}_Y)$ and a map of topological spaces $|f|\colon|X|\to |Y|$ that can be promoted into a map $f_1\colon X\to Y$ of schemes and into a map $f_2\colon X\to Y$ a map of schemes with $f_1\neq f_2$?",['algebraic-geometry']
903086,Characterizing C* algebra generated by elements.,"Let $A$ be a C*-algebra and $A_0\subset  A$. Then it is known that the $\mathbb{C}$-algebra generated by $A_0$ (i.e. the intersection of all sub-$\mathbb{C}$-algebras containing it ) is just the vector space of finite, linear elements on the form $T_1\cdots T_n$ where $n\in \mathbb{N}$ and $T_j \in A_0$. And the Banach algebra generated by $A_0$ is the norm closure of the above. Is there a corresponding neat result for C*-algebras?","['banach-algebras', 'functional-analysis', 'abstract-algebra']"
903107,Composite Relations,"I'm new to functions and relations, and I've only just figured out that there are 16 relations on a set with 2 elements. I can't figure out what is meant by R ; R â R other than the fact it is a composite relation! Any help / guidance would be appreciated!","['relations', 'functions']"
903117,Integral of Sinc Function Squared Over The Real Line [duplicate],"This question already has answers here : Proof of $\int_0^\infty \left(\frac{\sin x}{x}\right)^2 \mathrm dx=\frac{\pi}{2}.$ (15 answers) Closed 6 years ago . I am trying to evaluate
$$\int_{-\infty}^{\infty} \frac{\sin(x)^2}{x^2} dx $$
Would a contour work? I have tried using a contour but had no success.
Thanks. Edit: About 5 minutes after posting this question I suddenly realised how to solve it. Therefore, sorry about that. But thanks for all the answers anyways.","['trigonometry', 'integration']"
903118,Solving the differential equation $9x(1-x)y''-12y'+4y=0$,Solve in series the following ODE: $$9x(1-x)y''-12y'+4y=0$$ expanding $y(x)$ about $x_0=0$. My guess: I think it is by Frobenius series since it is not an ordinary point.,['ordinary-differential-equations']
903142,"For an irrational number $a$ the fractional part of $na$ for $n\in\mathbb N$ is dense in $[0,1]$ [duplicate]","This question already has answers here : For $x\in\mathbb R\setminus\mathbb Q$, the set $\{nx-\lfloor nx\rfloor: n\in \mathbb{N}\}$ is dense on $[0,1)$ (5 answers) Closed 9 years ago . How to prove that the $\{$ fractional part of $n\alpha\mid n \in \mathbb{N}$ $\}$ is dense in $[0,1]$ for an irrational number $\alpha$. NOTICE that $n$ is in $\mathbb{N}$ Also notice that this is not a duplicate of the mentioned question as it does not carry a correct answer and the partially correct answer in the said question is given for integer multiple case, NOT for $n \in \mathbb{N}$","['general-topology', 'sequences-and-series', 'irrational-numbers', 'real-analysis']"
903143,Show $\sin(x+h) \cdot \cos x - \cos(x+h) \cdot \sin x = \sin h$ (without limits please - straight trigonometry only).,"I've tried an algebraic approach using the identity $\sin(x) = \sin(x+h-h) = \sin(x+h)\cos(h) - \cos(x+h)\sin(h)$, leading to a complicated expression I'm having trouble simplifying: $[\sin(x)\cos(h) + \cos(x)\sin(h)]\cos(x) - [\cos(x)\cos(h) - \sin(x)\sin(h)][\sin(x+h)\cos(h)-\cos(x+h)\sin(h)]$ (further expansion of $\sin(x+h)$ and $\cos(x+h)$ in this expression did not help me). Use of this identity was a hint in the book I came across this in, What is Mathematics, by Courant and Robbins, p. 422. Note: Since posting I realized how to show symbolically in the updated diagram below, which has been updated since posting (including also the correct algebraic answer given by @Andre Nicholas).","['trigonometry', 'calculus']"
903184,Calculate $\lim_{n\to\infty} n^\alpha \Big(\frac{\sqrt[n+1]{(n+1)!}}{n+1} - \frac{\sqrt[n]{n!}}{n}\Big)$,"Let $\alpha$ be a positive number. Find $$\lim_{n\to\infty} n^\alpha \Big(\frac{\sqrt[n+1]{(n+1)!}}{n+1} - \frac{\sqrt[n]{n!}}{n}\Big).$$ I'd love to post a useful solution attempt, but all of my efforts seem far off. :) Please help me, thank you!","['calculus', 'limits']"
903188,Homology commutes with direct sum and product?,"I'm looking at exercise 1.2.1 from Weibel's An Introduction to Homological Algebra . (I need to show that homology commutes with direct sum and direct product.) Is it possible to show that cokernels commute with direct sum and product? Since $H_{n} (C)=\ker d_{n}/ \operatorname{im} d_{n+1}$, $H_{n} (C)=\operatorname{coker} d'_{n+1},$ where $d'_{n+1}:C_{n+1}\rightarrow \ker d_{n}\subset C_{n}$. Or, if I can show that kernels, images, and quotients commute with direct sums and products, would that also prove the statement? I can start with $\oplus(\ker d_{n})=\ker(\oplus d_{n})$, $\oplus(\operatorname{im} d_{n+1})= \operatorname{im}(\oplus d_{n+1})$, then $\oplus (\ker d_n /\operatorname{im} d_{n+1})=\ker \oplus d_n / \operatorname{im}\oplus d_{n+1}$ or $\oplus \ H_n (C)=H_n (\oplus \ C)$ where $ \oplus \ d$ is the differential for the chain complex $\oplus \ C. $ Similar statements for direct product. So my questions are, is it true that quotients, image, and kernel commute with direct sum and product? I think its easy to see that image and kernel commute with direct sum and product but I don't know how to prove it. I'm not so sure about quotients.","['homological-algebra', 'abstract-algebra']"
903225,Generators of the Symmetric group $S_3$,"I am trying to find the generator(s) of the Symmetric Group $S_3$ and I have attempted this via brute force by listing the permutations of $S_3$ and composing and repeating them but I have not found any generators.
- thanks","['permutations', 'finite-groups', 'group-theory', 'symmetric-groups']"
903272,Is it possible to accurately calculate an irregularly shaped frustum's volume?,"I have the following water basin Now imagine this basin is filled with water to the top, is there anyway to accurately calculate the volume of water stored in it using only top and bottom areas A1 and A2? The basin is basically an irregular polygon with circular rounded edges. Furthermore the edges taper down at given slope as can be seen in the section drawing (Please disregard the red line) Ideally the volume would be a function of y or depth. Thanks for your help.","['geometry', '3d', 'algebra-precalculus']"
903277,About composition of Holder functions.,"Let $f,g$ be Holder continuous functions with respective exponents $\alpha, \beta \in (0,1)$. More precisely $f \in C^{\alpha}(\mathbb{R}^n;\mathbb{R}^n)$, $g\in C^{\beta}(\mathbb{R}^n,\mathbb{R})$. I am wondering whether the composition $g \circ f:\mathbb{R}^n \to \mathbb{R}$ is Holder of exponent $\min \{\alpha,\beta \}$, i.e, if $g \circ f \in C^{\min\{\alpha,\beta \}}(\mathbb{R}^n;\mathbb{R})$. This is claimed on a paper I am reading but I don't know how to prove it. The naive approach would be: $$
|g(f(x+h))-g(f(x))| \le [g]_{\beta}|f(x+h)-f(x)|^{\beta} \le [g]_{\beta} |h|^{\beta \alpha} [f]^{\beta}_{\alpha}
$$ so we have $g \circ f \in C^{\alpha \beta }(\mathbb{R}^n)$. But $\alpha \beta < \min \{\alpha,\beta\}$, so this is not very helpful. Any help would be welcome. Thak you.","['fourier-analysis', 'partial-differential-equations', 'holder-spaces', 'real-analysis', 'banach-spaces']"
903280,Finding the expected value of a function of random variables,"I'm having troubles with finding marginal density functions and expected values in my probability theory class. I was hoping someone would be able to walk me through the solution to this question (I have the answer, I just don't understand how to get to it). The question:
$$f(y_1, y_2)=\left\{\begin{array}{ccc}\frac{1}{y_1}&& 0\leq y_2 \leq y_1 \leq 1\\0&& \mathrm{elsewhere}\end{array}\right.$$
Find $\mathbf{E}(Y_1-Y_2)$. The answer is $1/4$ and I understand that I have to first find the marginal density functions and use them to find $\mathbf{E}(Y_1)$ and $\mathbf{E}(Y_2)$ but I can't even do that. Any help would be greatly appreciated.","['probability-theory', 'probability']"
903283,When is $k(X)$ algebraic over $k(Y)$ for a dominant morphism $f:X\rightarrow Y$ between varieties.,"Let $f:X\rightarrow Y$ be a dominant morphism between irreducible varieties over an algebraically closed field $k$. When is $k(X)$ algebraic over $k(Y)$? Is there an if and only if criterion?
What if $f$ is even bijective?
From what I've found it should be enough that $f$ is finite. Is there a weaker criterion?","['commutative-algebra', 'algebraic-geometry']"
903303,Evaluate $\int_2^4\frac{\sqrt{x^2-4}}{x^2}\mathrm dx$,Evaluate $$\int\limits_2^4\frac{\sqrt{x^2-4}}{x^2}\mathrm dx$$ My working: $x=2\sec\theta\quad\Rightarrow\quad\theta=\arccos\left(\frac{2}{x}\right)$ $dx=2\sec\theta\tan\theta d\theta$ $I=\int\frac{\sqrt{4\sec^2\theta-4}}{4\sec^2\theta}2\sec\theta\tan\theta d\theta=\int\frac{\tan^2\theta}{\sec\theta}d\theta=\int\frac{\sin^2\theta}{\cos\theta}d\theta=\int\sec\theta d\theta-\int\cos\theta d\theta\\=\ln|\sec\theta+\tan\theta|-\sin\theta+C\\=\left.\ln\left|\frac{x}{2}+\frac{\sqrt{1-(2/x)^2}}{2/x}\right|-\sqrt{1-\left(\frac{2}{x}\right)^2}\right]_2^4$ EDIT $=\ln\left|\frac{4+\sqrt{12}}{2}\right|-\sqrt{1-\frac{1}{4}}-\ln\left|\frac{2+\sqrt{0}}{2}\right|-\sqrt{1-1}\\=\ln|2+\sqrt{3}|-\frac{\sqrt{3}}{2}\qquad\blacksquare$,['integration']
