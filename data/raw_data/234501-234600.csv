question_id,title,body,tags
4900145,How do I check if set of solutions to differential equation form vector subspace,"Let $\mathbb{R}^\mathbb{R}$ be the vector space of all functions $f:\mathbb{R}\to\mathbb{R}$ , with addition and scalar multiplication defned pointwise. Which of the following sets of functions form a vector subspace of $\mathbb{R}^\mathbb{R}?$ The set of solutions of the differential equation $\ddot{x}(t)+(t^2-3)\dot{x}(t)+t^4x(t)=0.$ The set of solutions of $\ddot{x}(t)+(t^2-3)\dot{x}(t)+t^4x(t)=\sin
t.$ The set of solutions of $(\dot{x}(t))^2-x(t)=0.$ The set of solutions of $( \ddot{x} ( t) ) ^4+ ( x( t) ) ^2= 0.$ I already know that there's a theorem homogenous linear diff. equations being vector spaces, but without it how exactly do I verify each case manually? I suppose I can solve each one and write down the general solution, but I read that there are infinite number of solutions. So how do I verify if axioms hold if there's possibly infinite solutions to check?","['linear-algebra', 'vector-spaces', 'ordinary-differential-equations']"
4900149,"No 3 vectors independent over $\mathbb{Z}$ in $\mathbb{Z}^2$, without AoC","Q: Are there three $\mathbb{Z}^2$ vectors independent over $\mathbb{Z}$ ? Context: This problem arise naturally when I'm characterizing possible sub-""latice"" in $\mathbb{Z}^2$ . Formally let $S$ be a set of points (""lattice points"") with integer coordinates, and a set of vectors $V = \{ v_1, v_2, ...\} \subseteq \mathbb{Z}^2$ so that for all $p \in S$ and $v \in V$ , $$p \pm v \in S.$$ i.e., is also a lattice point. We can make a hexagon-like lattice by choosing $(v_1, v_2, v_3) = ((2, 1), (2, -1), (0, 2))$ . Notice, then, $v_1 - v_2 = v_3$ , so $v_3$ is redundant in $V$ - so we can remove $v_3$ while keeping the set $S$ . So we ask: is the third vector always redundant, in the sense that $$ cv_3 = av_1+bv_2, a,b,c \in \mathbb{Z} \implies a=b=c=0?$$ A quick search of ""L.I. over $\mathbb{Z}$ "" reveals that in $\mathbb{Z}^n$ , linear independence over $\mathbb{Z}$ is equivalent to $\mathbb{R}$ . This overkills the question: if there were 3 independent vectors over $\mathbb{Z}$ , then they are too in $\mathbb{R}$ , a contradiction. However, the prove involves using the basis of $\mathbb{R}$ over $\mathbb{Q}$ to deal with the real coefficients, which is a implication of Axiom of Choice. Since my question is simple enough, I would like to see if there is a proof without AoC. Q: Is the negation of ""there exists three $\mathbb{Z}^2$ vectors independent over $\mathbb{Z}$ "" provable without AoC?","['integer-lattices', 'axiom-of-choice', 'linear-algebra', 'set-theory']"
4900158,The vertices of a pentagram are five random points on a circle. Conjecture: The probability that the pentagram contains the circle's centre is $3/8$.,"The vertices of a pentagram are five uniformly random points on a circle. Is the following conjecture true: The probability that the pentagram contains the circle's centre is $\frac38$ . (The pentagram is said to contain the circle's centre if the central pentagon, or any of the five triangles adjecent to the central pentagon, contains the circle's centre.) A simulation with $10^7$ such random pentagrams yielded a proportion of $0.3750079\approx1.00002\times\frac38$ containing the circle's centre. Thus, my conjecture. How to set up a simulation Let the circle be $x^2+y^2=1$ with centre $O\space(0,0)$ . Assume the first random point is $(1,0)$ and let the other four random points be, going anticlockwise from $(1,0)$ : $(\cos\theta_1,\sin\theta_1)$ , $(\cos\theta_2,\sin\theta_2)$ , $(\cos\theta_3,\sin\theta_3)$ , $(\cos\theta_4,\sin\theta_4)$ . Let $A,B,C,D,E$ be the five regions in the circle and outside the pentagram, going anticlockwise starting with the region between $(1,0)$ and $(\cos\theta_1,\sin\theta_1)$ . $O$ lies in $A$ if and only if $\theta_2>\pi,$ and $\theta_4-\theta_1<\pi$ . $O$ lies in $B$ if and only if $\theta_2>\pi,$ and $\theta_3-\theta_1>\pi$ . $O$ lies in $C$ if and only if $\theta_3-\theta_1>\pi$ , and $\theta_4-\theta_2>\pi$ . $O$ lies in $D$ if and only if $\theta_3<\pi,$ and $\theta_4-\theta_2>\pi$ . $O$ lies in $E$ if and only if $\theta_3<\pi,$ and $\theta_4-\theta_1<\pi$ . The pentagram contains $O$ just if $O$ lies in none of $A,B,C,D,E$ . If my conjecture is true, then, given the simplicity of the probability, there might be a proof based on some kind of symmetry.","['conjectures', 'geometric-probability', 'circles', 'geometry', 'probability']"
4900165,What is Sørensen–Dice coefficient formula for 3 sets?,"What is actually Sørensen–Dice coefficient formula for 3 sets? for 2 sets, it is: $$\frac{2 \cdot |A \cap B|}{|A| + |B|}$$ But for 3 sets? is this right? : $$\frac{3 \cdot |A \cap B \cap C|}{|A| + |B| + |C|}$$ I searched but I cannot find an exact answer","['statistics', 'reference-request']"
4900232,$\sigma$-algebra on $\mathbb{R}$ generated by the collection of all one-point sets.,"I have seen that the $\sigma$ -algebra on $\mathbb{R}$ which is generated by the collection of all one-point sets is the collection of all sets with a countable number of elements and their complements, and have more or less seen a proof of why. But I would like to know how one would come up with this set and then decide to prove it's the one we are looking for. How does someone reason to decide and test if that set is the expected $\sigma$ -algebra?.","['elementary-set-theory', 'measure-theory']"
4900248,Norm of triangular matrix with constant rows $\approx \sqrt{d \log 2}$?,"Suppose I have a triangular $d\times d$ matrix $A$ with constant rows normalized to have Euclidean norm 1. Empirically it appears that operator norm (largest singular value) of such matrix is $\sqrt{d \log 2}$ , for large $d$ , why? For instance, for $d=5$ $$A=\left(
\begin{array}{ccccc}
 1 & 0 & 0 & 0 & 0 \\
 \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} & 0 & 0 & 0 \\
 \frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}} & 0 & 0 \\
 \frac{1}{2} & \frac{1}{2} & \frac{1}{2} & \frac{1}{2} & 0 \\
 \frac{1}{\sqrt{5}} & \frac{1}{\sqrt{5}} & \frac{1}{\sqrt{5}} & \frac{1}{\sqrt{5}} & \frac{1}{\sqrt{5}} \\
\end{array}
\right)$$ with norm $1.94787$ . For larger values of $d$ , $\sqrt{d \log 2}$ appears to make a nice fit: Related earlier question about singular spectrum of such matrix -- Why do singular values decay as O(1/k) for this triangular matrix? Notebook: forum-triangular-singularvalues.nb",['linear-algebra']
4900270,How to factor $(x-y)^5 + (y-z)^5 + (z-x)^5$,"We see that polynomial is cyclic.
For $x=y$ , $P(y)=0$ so $x-y$ factors polynomial. Because it is cyclic we instantly know other two factor $(y-z)$ and $(z-x)$ . $P(x,y,z)=(x-y)(y-z)(z-x)*N(x)$ Because we know that P(x,y,z) has the highest 4th degree of $x,y$ and $z$ it must me that $N(x)$ is the second-degree polynomial.
I just don't know how the $N(x)$ would look like in terms of its coefficients.
If the coefficient for all members would be the same as it looks like in the solution it would be easy to solve this, however I can't logically comprehend why we can claim that given that P(x,y,z) has members with coefficients 10,5,1.","['linear-algebra', 'polynomials']"
4900287,$\lim\limits_{x\to 0} \dfrac{e^x-e^{x\cos x}}{x+\sin x}$ [duplicate],"This question already has answers here : Limit of the exponential functions: $\lim_{x\to 0} \frac{e^x-e^{x \cos x}} {x +\sin x}$ (6 answers) Closed 2 months ago . $\operatorname{lim}_{x\to 0} \dfrac{e^x-e^{x\cos x}}{x+\sin x}$ , L'hopital is not allowed. Divide all limit by $x$ then $\lim\limits_{x\to 0} \dfrac{\dfrac{e^x}{x}-\dfrac{e^{x\cos x}}{x}}{1+\dfrac {\sin x}{x}}$ Denumerator goes to $2$ . What about numerator? The limit $\lim\limits_{x\to 0} \dfrac{e^x}{x}$ does not exist. Neither the limit of $x\to 0$ $\dfrac{e^{x\cos x}}{x}$ exists","['limits', 'calculus', 'limits-without-lhopital']"
4900290,A question about Markov chain's definition,"Background: One version of the definition of a discrete-time Markov chain is as follows: Let $S$ be a countable set. A stochastic process $(Z_n)_{n\in\mathbb{N}_0}$ is called a discrete-time Markov chain if and only if for all $n\in\mathbb{N}$ and $x,y,x_0,...,x_{n-1}\in S$ with $P(Z_0=x_0,...,Z_{n-1}=x_{n-1},Z_n=x)>0$ , one has $$P(Z_{n+1}=y|Z_0=x_0,...Z_{n=1}=x_{n-1},Z_n=x)=P(Z_{n+1}=y|Z_n=x)=P(Z_1=y|Z_0=x)$$ My question is: Is $$
P(Z_{n+1}=y|Z_0=x_0,...Z_{n=1}=x_{n-1},Z_n=x)=P(Z_{n+1}=y|Z_n=x)
$$ a necessary part of the definition? Can I remove it? If not, consider that just use $$P(Z_{n+1}=y|Z_0=x_0,...Z_{n=1}=x_{n-1},Z_n=x)=P(Z_1=y|Z_0=x)$$ as the formula in the definition of discrete time of Markov chain, how to show that $$P(Z_{n+1}=y|Z_0=x_0,...Z_{n=1}=x_{n-1}, Z_n=x)=P(Z_{n+1}=y|Z_n=x)$$ by the new definition?","['stochastic-processes', 'markov-process', 'probability-theory', 'markov-chains']"
4900345,Show that $f=0$ $\mu$ a.e.,"Let $([0,1], \mathcal{B}([0,1]), \mu)$ be a measure space with Lebesgue measure $\mu$ . Let $f\in L_1([0,1])$ and a fixed constant $b\in (0,1)$ so that $$
\int_B fd\mu=0
$$ for all $B\in \mathcal{B}([0,1])$ with $\mu(B)=b$ . Show that $f=0$ $\mu$ a.e. My proof is as follows: (that is different from Want to show that $f=0$ a.e. ). W.L.O.G, assume that for any open set $G$ with $\mu(G)=b$ we have $
\int_G fd\mu=0$ . Then for every $a\in \mathbb{R}$ , we have $$\int_a^{a+b}f(x)dx=0$$ Then for every $c>0$ we have $$
\frac{1}{c}\int_a^{a+c}f(x)dx=\frac{1}{c}\int_{a+b}^{a+b+c}f(x)dx
$$ As $c\to 0$ , we get $f(a)=f(a+b)$ a.e. for $a\in \mathbb{R}$ . Define $$E_n:=\{a\in\mathbb{R}: f(a)=f(a+b)=\dots=f(a+nb)\neq f(a+(n+1)b)\}$$ with $\mu(E_n)=0$ for $n\in\mathbb{N}$ . Let $S:=\mathbb{R}\setminus \cup_{n=1}^\infty E_n$ . we have $$
\lim_{n\to\infty}\frac{1}{b/n}\int_a^{a+b/n}f(x)dx=f(a)
$$ for all $a\in S\setminus\mathbb{Z}$ . Let $G=(a,a+b/n)\cup(a+b/n, a+2b/n)\cup\dots\cup (a+n-1, a+n-1+b/n)$ . $G$ is open and $m(G)=1$ . So $$
\int_G fd\mu=0=\frac{1}{b/n}\int_a^{a+b/n}f(x)dx\to f(a)
$$ So $f(a)=0$ a.e. for every $a\in\mathbb{R}$ .",['real-analysis']
4900353,Proof for relation between functions,"For start lets define $\sigma(d)$ : the sum of divisors of d, with $d \in \mathbb{Z}$ $\tau(d)$ : the number of divisors of d, with $d \in \mathbb{Z}$ I want to prove that, $\sum_{d|n} \frac{n}{d}\sigma(d) =  \sum_{d|n} d\tau(d) $ My idea is based on, $\sigma(d) = \sum_{t|d} t$ and $\tau(d) = \sum_{t|d} 1$ , so I wrote: $ \sum_{d|n} \frac{n}{d}\sigma(d) = \sum_{d|n} \frac{n}{d} \sum_{t|d} t = 
\sum_{d|n} \sum_{t|d} \frac{n}{d} t$ Changing the summation order (I will use the notation '':'' to say ''such that'') $\sum_{t|n} \sum_{d|n \,:\, t|d} \frac{n}{d} t$ since t divides d then $d = kt$ for some $k \in \mathbb{Z}$ . Also since $kt | n$ and $t | n$ then $k | \frac{n}{t}$ $\sum_{t|n} \sum_{k | \frac{n}{t}} \frac{n}{kt} t = \sum_{t|n} n \sum_{k | \frac{n}{t}} \frac{1}{k} $ From here I dont know how proceed, i wanted to end up on someting like $ \sum_{t|n} \frac{n}{t} \sum_{k | \frac{n}{t}} 1 = \sum_{t|n} \frac{n}{t} \tau(\frac{n}{t}) = \sum_{d|n} d \tau(d)$ , but I have not found a way. Any ideas of how to proceed or any other way I can prove that $\sum_{d|n} \frac{n}{d}\sigma(d) =  \sum_{d|n} d\tau(d)$ ? (If it is possible avoid the use of Dirichlet product)","['discrete-mathematics', 'prime-numbers']"
4900361,Why is $\iiint_B(12x^2+2z)dxdydz=\iiint_B4(x^2+y^2+z^2)dxdydz$?,"According to my teacher $$\iiint_B (12x^2+2z) \, dx \, dy \, dz = \iiint_B 4(x^2+y^2+z^2) \, dx\, dy\, dz$$ where $B=\{(x,y,z)\mid x^2+y^2+z^2\le 1\}$ . I have absolutely no clue why the triple integrals should be equal.","['integration', 'multivariable-calculus']"
4900406,"Show that $\int_{0}^{1} \frac{\tan^{-1}(x^2)}{\sqrt{1 - x^2}} \, dx = \frac{1}{2}\pi \tan^{-1}\left(\sqrt{\frac{1}{\sqrt{2}} - \frac{1}{2}}\right)$","Problem: Show that $$\int_{0}^{1} \frac{\tan^{-1}(x^2)}{\sqrt{1 - x^2}} \, dx = \frac{1}{2}\pi \tan^{-1}\left(\sqrt{\frac{1}{\sqrt{2}} - \frac{1}{2}}\right)$$ Some thinking before trying At least we can show that the identity holds, since the taylor series of the integrand is pretty easy to derive and then you can just integrate it term by term. Also the RHS is just $\arctan$ so there's nothing really difficult. Still I have no idea how to approach it if the answer wasn't given.. I'm thinking of integration by parts, differentiating $\arctan$ and integrating $\frac{1}{\sqrt{1-x^{2}}}$ into $\arcsin x$ . I  realized that the taylor series of the integrand isn't that easy to derive... almost impossible to express in terms of $n$ . My attempt here's how far I got. Inserting the Taylor series for $\arctan(x^2)$ we find $$\sum_0^\infty {{(-1)^n}\over {2n+1}} \{\int_0^1 {{x^{4n+2}}\over {\sqrt{1-x^2}}}\}$$ Now define $$a_k:=\int_0^1 {{x^k dx}\over {\sqrt{1-x^2}}}=\int_0^{\pi/2}(\sin t)^k dt$$ then $a_{k+2}=({{k+1}\over {k+2}})a_k$ with PI.
Hence $${{a_{2k}\over {a_0}}={{1\cdots (2k-1)}\over {2\cdots (2k)}}}$$ and we find $$\pi/2 \sum_0^\infty {{(-1)^n}\over {2n+1}} \cdot {{1\cdot 3\cdots(4n +1)}\over {2\cdot 4\cdots (4n+2)}}$$ Now it's probably something with $\arcsin x$ or ${{\arcsin x}\over {\sqrt{1-x^2}}}$ . Also the angle $\pi/16$ seems to appear.","['integration', 'calculus', 'definite-integrals', 'closed-form']"
4900408,Help with stars and bars,"I have 2 questions, which I need to check: How many ways can you add up 7 numbers (each >=0), so that they total to 37, but the first three numbers add up to 6. My answer: $C^8_6*C^{34}_{31} = 335104$ I used theorem two from the following , to find ways which 4 numbers add up to 31 and 3 numbers add up to 6 and multiplied them. A message consists of 12 packets and 45 blank spaces (blank spaces must be between packets), each packet must have a minimum of 3 blank spaces between them, how many ways can a message be sent this way? My answer: $12!*C^{22}_{12} = \frac{22!}{10!}$ There are 12! factorial ways to shuffle the packets and for each combination of packets there are 12 (45-33 bcs 3 min between each packet) spaces that need to be kept in between 11 spaces(between 12 packets), so same theorem two of stars and bars ( $s_1+s_2+...+s_{11}=12$ ).","['combinatorics', 'discrete-mathematics']"
4900413,What is $\sup\{a\}$ such that for continuous non differentiable function $f$ $\lim\limits_{h\to0}\frac{f(x+h)-f(x)}{h^a}$ exist?,"Now asked on MO here The definition of the derivative of a function $f$ at a point $x$ is : $\lim\limits_{h \to 0 }\frac{f(x+h)  -f(x)}{h}$ but what if we change that $h$ to be any continuous function $g(h)$ that goes to $0$ as $h$ goes to $0$ ? Lets focus our attention at powers of $h$ so $g(h)=h^a$ for some $a$ . If $f$ is a differentiable function at $x$ any function other than $h$ the absolute value of the derivative will go to either $0$ or $\infty$ . I will refer to $\lim\limits_{h \to 0 }\frac{f(x+h) -f(x)}{g(h)}$ as $a-$ derivative. A question that came to my mind Given any continuous function nowhere differentiable on $\mathbb{R}$ is there a $g$ such that the absolute value of the $a-$ derivative"" exist almost everywhere and how to determine such functions ? The case where $g(x)$ is big enough to make this $a-$ derivative $0$ is annoying because one can say a very big number like $a=1/TREE(3) $ and say, for example, at this number the $a-$ derivative exist and it is $0$ for the Weierstrass function, so let me clear that : Given any continuous function nowhere differentiable what is the $\sup g(x)$ such that $\lim\limits_{h \to 0 }\frac{f(x+h)  -f(x)}{g(h)}= 0$ for all $x$ where the $\sup $ is taken of all real $a$ does the absolute value of the $a-$ derivative exist at that value ? Lets take L. van der Waerden's function for example Define $g(x)= |x|$ for $|x|\in [-1,1]$ , $g(x+2)=g(x)$ $$f(x)= \sum_{n \ge 1} \frac{3^n g\left(4^n x\right) }{4^n}$$ Is an example of continuous function nowhere differentiable, There is a proof that the absolute value secant line's slope goes to infinity as it approaches the point so this made me wonder will the   absolute value of the $a-$ derivative exist if we decrease the power of $h$ ?","['conjectures', 'real-analysis', 'continuity', 'limits', 'derivatives']"
4900557,Geometry in Complex Numbers involving incentre,"Q: Complex numbers $z_1$ , $z_2$ , $z_3$ are represented by the points of contact $D$ , $E$ , $F$ of the in-circle of a triangle $ABC$ , with centre $O$ of the incircle taken as the origin. If $BO$ meets $DE$ at $G$ , find the complex number represented by $G$ ? Options: I $\qquad\displaystyle\frac{(z_1-z_2)z_3}{z_2-z_3}$ II $\qquad\displaystyle\frac{(z_1-z_2)z_3}{z_2+z_3}$ III $\qquad\displaystyle\frac{(\overline z_1- \overline z_2)z_3}{z_2+z_3}$ IV None Of These My Try: I know that incircle touches the sides, and hence the sides are tangents to the same, with touching points $z_1, z_2, z_3$ So, as $B$ is point of intersection of the $2$ tangents with point of intersection with circle at $z_1$ and $z_2$ , $\displaystyle B = \frac{2z_1z_2}{z_1+z_2}$ , and likewise for other vertices. I am stuck after this.","['circles', 'geometry', 'complex-numbers']"
4900563,Convergent sequences under different probability measures,"What is the „intuitive” reason behind the following statements... Let $\left(X_{n}\right)_{n}$ be a sequence of random variables. Let us assume $\mathbf{Q}\ll\mathbf{P}$ , i.e. the $\mathbf{Q}$ probability measure is absolutely continuous with respect to the $\mathbf{P}$ probability measure. If $\left(X_{n}\right)_{n}$ is almost surely convergent under $\mathbf{P}$ , then ${(X_n)}_n$ is almost surely convergent under $\mathbf{Q}$ as well. If $\left(X_{n}\right)_{n}$ is convergent in the stochastic convergence under $\mathbf{P}$ , then ${(X_n)}_n$ is also convergent in the stochastic convergence under $\mathbf{Q}$ as well. I think the first statement makes sense to me: If I randomly choose an $\omega$ according to $\mathbf{P}$ , then it is 100% sure (i.e. it has $1$ probability), that ${(X_n)}_n$ will be convergent. Since the $\mathbf{Q}$ -zero probabilities are the same as the $\mathbf{P}$ -zero probabilities, then the $\mathbf{Q}$ -one probabilities are the same as the $\mathbf{P}$ -one probabilities. Therefore, if I randomly choose an $\omega$ according to $\mathbf{Q}$ , then ${(X_n)}_n$ will also be convergent under $\mathbf{Q}$ almost surely. However, it is not as obvious to me in case of stochastic convegence as in the case of almost sure convergence. Can you give me some „intuitive” explanation why it is indeed true? Also, is the $X$ limit the same under different measures? (I think it is, but I need confirmation...)","['measure-theory', 'probability', 'almost-everywhere', 'stochastic-calculus', 'radon-nikodym']"
4900590,A simple question about the Hodge star,"The usual definition of the Hodge star says that it maps $\Lambda^k(V)$ to $\Lambda^{n-k}(V)$ in such a way that for each pair $\omega, \eta \in \Lambda^k(V)$ holds $\omega \wedge *\eta = \langle \omega, \eta \rangle \operatorname{vol}$ . I was curious whether this definition is equivalent to saying that $\omega \wedge *\omega = \operatorname{vol}$ for each $\omega$ of unit norm. Initially I thought that it should follow immediately from the polarization identity as follows: $$
\begin{aligned}
2 \, \langle \omega, \eta \rangle \operatorname{vol}
& =
\langle \omega, \omega \rangle \operatorname{vol}
+ \langle \eta, \eta \rangle \operatorname{vol}
-\langle \omega-\eta, \omega-\eta\rangle \operatorname{vol}
\\[3pt]
& = \omega \wedge *\omega + \eta \wedge *\eta - (\omega-\eta) \wedge *(\omega-\eta)
\\[3pt]
& = \omega \wedge *\eta + \eta \wedge *\omega.
\end{aligned}
$$ This would give a proof if $\omega \wedge * \eta$ were equal to $\eta \wedge *\omega$ , but this doesn't seem to follow from the definition of $*\omega$ by $\omega \wedge *\omega = \operatorname{vol}$ . So my question is what am I missing? Or does the definition with one $\omega$ instead of two $\omega, \eta$ allow for a different choice of $*\omega$ ?","['multivariable-calculus', 'exterior-algebra', 'differential-geometry']"
4900604,Complete Boolean Algebra and Distributivity [duplicate],"This question already has an answer here : $\kappa$-distributivity and Stone's Representation Theorem (1 answer) Closed 2 months ago . Thear are two facts about Boolean algebra: Every Boolean algebra is isomorphic to an algebra of sets. (Stone's Representation Thm) Every complete algebra of sets is completely distributive. (said in Jech's book) For completely distributivity , I mean $$
\bigwedge_{\alpha<\kappa}\bigvee_{i\in I_\alpha}u_{\alpha,i}=\bigvee_{f\in\prod_{\alpha<\kappa}I_\alpha}\bigwedge_{\alpha<\kappa}u_{\alpha,f(\alpha)}.
$$ Then combine these two facts it seems that every complete Boolean algebra is completely distributive (?). However, I find a result saying that a complete Boolean algebra is completely distributive iff it's atomic, i.e., it's a CABA (complete atomic Boolean algebra). In case it's isomorphic to a power set. There must be something wrong. From this post , an algebra of sets
may not use $\cap$ as its infinitive meet. But then how does fact 2 hold?","['elementary-set-theory', 'boolean-algebra', 'general-topology', 'set-theory']"
4900637,How to find the equation for $x_{\theta}$ in this problem?,"I have the below problem from a research paper that I would like to solve $$\frac{1}{2}(y^2 - \bar{y}^2) = -\gamma\sin\theta-\gamma_{\theta}\cos\theta + a_1 \tag1$$ where, $$a_1 = \gamma\sin\theta +\gamma_{\theta}\cos\theta$$ at $\theta=\bar{\theta}$ . This gives me the following $$y=[2(-\gamma\sin\theta-\gamma_{\theta}\cos\theta + \gamma\sin\bar\theta +\gamma_{\bar\theta}\cos\bar\theta) + \bar{y}^2]^{1/2}$$ Here, $\bar{y}$ is the value of $y$ at some angle $\theta = \bar{\theta} \tag2$ . Now, there are further couple of equations, $$yy_{\theta} = - (\gamma + \gamma_{\theta\theta})\cos\theta \tag3$$ and $$yx_{\theta} = (\gamma + \gamma_{\theta\theta})\sin\theta \tag4$$ Where, $$\gamma(\theta) = 1+\gamma_{d}\cos[4(\theta+\theta_{0})]$$ Here, $\gamma_{d}$ is a constant, and $\theta_{0}$ is some phase angle. I have to substitute the above equation and it gives me: $$yx_{\theta} = \gamma\sin\theta-16\gamma_{d}\cos[4(\theta + \theta_0)]$$ Now, after substituting $y$ from equation(2) into equation(4) I need to find an equation for $x_{\theta}$ such that can be integrated numerically. Here, subscripts denote differentiation, so $\gamma_{\theta\theta}$ is double derivate of $\gamma(\theta)$ w.r.t $\theta$ , and similar for $x_{\theta}$ . Can anyone suggest how to proceed forward and then obtain the equation for $x_{\theta}$ ? I'm stuck at what to do with $\bar{y}$ , how should I eliminate it. The paper doesn't state the final equation for $x_\theta$ , and hence I'm here for a solution. Any suggestions or help appreciated. Source for the paper: here , the problem stated is from equations 2.7, 2.6 and 1.5 in the article.","['partial-derivative', 'derivatives', 'mathematical-modeling']"
4900694,How can I find a positively oriented parametrization?,"I need to find a parametrization of the surface defined by $x^{2}+y^{2}+z^{2}=9$ , $x+y+z\geq 1$ . The orientation must be compatible with the orientation of its boundary, that is, if the loop lies in the $xy$ -plane then we have to choose the circulation counterclockwise and the flux upward. How can I do that? Any hint?","['multivariable-calculus', 'parametrization']"
4900730,If $\sum_{k=1}^{n}\dfrac{2^{k}\left({k^2+2k+2}\right)}{(k+1)(k+2)}=2^{15}-2^{11}-1$. Then value of $n$ is,If $\sum_{k=1}^{n}\dfrac{2^{k}\left({k^2+2k+2}\right)}{(k+1)(k+2)}=2^{15}-2^{11}-1$ . Then value of $n$ is Attempt: I tried to form telescopic sequence. First I wrote $k^{2}+2k+2=(k+1)^2+1$ but this is not helping me. Then I tried to write $k^2+2k+2=(k+1)(k+2)-(k)$ but this is not helping me either. So anyone here give me some hints?,"['algebra-precalculus', 'sequences-and-series']"
4900749,Sufficient condition for compact implies closed.,"This is from a discussion me and my friend was having regarding compact implies closed. If $X$ is hausdorff then every compact subset of $X$ is closed.
But Hausdorffness is more than sufficient since there are examples which are not hausdorff but still the conclusion holds (e.g. $\mathbb{R}$ with cocountable topology. Now we want to know does the conclusion hold in weakly Hausdorff spaces. That is if a space is weakly Hausdorff then is this true that every compact subset is closed? Note: A space $X$ is weakly Hausdorff if for any compact Hausdorff space $Y$ and a continuous map $f : Y \to X$ we have $f(Y)$ is closed in $X$","['general-topology', 'compactness']"
4900755,Complex Integral ML Lemma,"I must solve $$\int_{-\infty}^{\infty}\frac{x \text{sin}x}{x^2+4} dx$$ I simplified this to $$\int_{-\infty}^{\infty}\frac{x \text{sin}x}{x^2+4} dx = \frac{1}{i}\int_{-\infty}^{\infty}\frac{x e^{ix}}{x^2+4} dx$$ Now consider only $$\int_{-\infty}^{\infty}\frac{x e^{ix}}{x^2+4} dx $$ I considered $C_R$ to be the simple closed contour traversing anticlockwise along the real axis from $-R$ to $R$ being $\Gamma_R$ and then along the half-circle $|z|=R$ in the upper half of the plane which is $\Delta_R$ This gives $$ \text{Res}(f,2i) = \int_{\Gamma_R} f(z) dz + \int_{\Delta_R} f(z) dz $$ The first integral along $\Gamma_R$ is $\int_{-\infty}^{\infty}\frac{x e^{ix}}{x^2+4} dx$ as $R$ tends to positive infinity. The residue of $f$ at $2i$ equals $\frac{\pi i}{e^2}$ So $$ \int_{-\infty}^{\infty}\frac{x e^{ix}}{x^2+4} dx = \frac{\pi i}{e^2} - \int_{\Delta_R} f(z) dz $$ where the second integral is taken as $R$ tends to positive infinity So I compute the second integral by the ML Lemma where $L$ is $\pi R$ and $$|\frac{ze^{iz}}{z^2+4}| <= \frac{|ze^{iz}|}{R^2-4} = \frac{R|e^{iz}|}{R^2-4} = \frac{R}{R^2-4} = M$$ So $ML = \frac{\pi R^2}{R^2-4}$ which tends to $\pi$ as $R$ tends to positive infinity. This means the final answer is $\frac{\pi i}{e^2} - \pi$ but the solution says the answer is $\frac{\pi }{e^2}$ because the second integral is equal to $0$ as $R$ tends to positive infinity and only considering the imaginary part. Can I have some help?","['integration', 'complex-analysis', 'calculus', 'trigonometric-integrals', 'derivatives']"
4900767,Equivalent characterizations of finite sets,"How can we show that the following notions of finiteness for a nonempty set $X$ are equivalent? There exists $n \in \mathbb{N}$ such that there is an injection $X \hookrightarrow \{1, \ldots, n\}$ There exists $n \in \mathbb{N}$ such that there is no injection $\{1, \ldots, n\} \hookrightarrow X$ There exists $n \in \mathbb{N}$ such that there is a surjection $\{1, \ldots, n\} \twoheadrightarrow X$ There exists $n \in \mathbb{N}$ such that there is no surjection $X \twoheadrightarrow \{1, \ldots, n\}$ There exists $n \in \mathbb{N}$ such that there is a bijection $X \leftrightarrow \{1, \ldots, n\}$ (5.) $\Rightarrow$ (1.) and (5.) $\Rightarrow$ (3.) are clear. I think I can show the following: (1.) $\Rightarrow$ (2.): By contradiction. Let $f: X \hookrightarrow \{1, \ldots, n\}$ be an injection. Assume that for all $m \in \mathbb{N}$ , there is an injection $\{1, \ldots, m\} \hookrightarrow X$ . In particular, take $m = n + 1$ and let $g: \{1, \ldots, n + 1\} \hookrightarrow X$ be an injection. Then $f \circ g: \{1, \ldots, n + 1\} \hookrightarrow \{1, \ldots, n\}$ is an injection, which is a contradiction. (3.) $\Rightarrow$ (4.): By contradiction. Let $f: \{1, \ldots, n\} \twoheadrightarrow X$ be a surjection. Assume that for all $m \in \mathbb{N}$ , there is a surjection $X \twoheadrightarrow \{1, \ldots, m\}$ . In particular, let $m = n + 1$ and let $g: X \twoheadrightarrow \{1, \ldots, n + 1\}$ be a surjection. Then $g \circ f: \{1, \ldots, n\} \twoheadrightarrow \{1, \ldots, n + 1\}$ is a surjection, which is a contradiction.","['elementary-set-theory', 'functions', 'natural-numbers']"
4900833,comparing two measures on the square,"Let $\lambda _d$ denote the Lebesgue measure on $R^d$ . Let $\mu$ be a measure on Borel subsets of $[0,1]^2$ such that $\mu <<\lambda _2$ and $\mu (B\times [0,1])=\mu ([0,1]\times B)=\lambda _1 (B)$ for every Borel $B\subseteq [0,1]$ . Does it necessarily hold that $\mu (D)=\lambda _2 (D)$ for every Borel $D\subseteq [0,1]^2$ ?",['measure-theory']
4900867,"What is minimum of the integral function $I(x)= \int_0^\infty \frac{1}{(1+t^x)^x} \,dt$","A while ago I stumbled on a YT movie with regards the integral $$\int_0^\infty \frac1{(1+x^\phi)^\phi} dx = 1 .$$ Here $\phi=1.6180\ldots$ is the Golden Ratio. But then I thought, what will happen if you take $\phi$ as the variable and increase or decrease its value? You'll get the following function: $$I(x) =\int_0^\infty\frac{1}{(1+t^x)^x}dt .$$ with $I(\phi) = 1$ . So making $x$ smaller I noticed at $1$ there is a pole which goes to $+\infty$ . But much nicer is letting $x$ get much larger. What I did not expect is that if $x$ gets very large the value of $I(x)$ seems to go ""back"" to 1 again. I can not check that further than $708$ because after (at $709$ ) Wolfram times out and gives an error. But this means that there's a minimum between phi and infinity. I found that minimum (using public wolfram) to be around $x = 3.542$ .
BUT, all values between $3.539$ and $3.546$ give the same lowest answer and that is $0,66568$ And then at $3.538$ or $3.547$ Wolfram gives $0,665681$ . So the question is: What is the exact minimum value and at what $x$ is that? And how can that be derived?","['integration', 'definite-integrals', 'improper-integrals', 'calculus', 'optimization']"
4900885,Inverse direction of Hodge index theorem,"The Hodge index theorem states that the intersection matrix associated to curves on a smooth algebraic surface has a specified signature---namely, if the intersection matrix has size $n \times n$ then it has signature $(1, n-1)$ , meaning it has 1 positive eigenvalue and $(n-1)$ negative eigenvalues.
This gives a fairly strong restriction on matrices that come from the intersection pairing on some surface. What is known about the ""inverse"" direction of this problem?
If we write down some symmetric, integer-valued matrix $M$ whose signature is $(1, n-1)$ , does there exist some smooth algebraic surface $X$ whose intersection matrix is $M$ ?
Any information or references would be appreciated. For a more concrete sub-question, is there a smooth algebraic surface whose intersection matrix is the following one? $$
\begin{pmatrix}
0 & 1 & 1 & 1 \\
1 & 0 & 2 & 2 \\
1 & 2 & 0 & 2 \\
1 & 2 & 2 & 0
\end{pmatrix}
$$ Cross-posted on Mathoverflow.","['algebraic-geometry', 'linear-algebra', 'intersection-theory']"
4900906,"Interior and closure of sets on metric spaces combined $ A, A^{\circ}, \overline{A^{\circ}}, \ldots $","For $ A \subseteq X $ where $ X $ is a metric space, the following sets can be formed: $ A, A^{\circ}, \overline{A^{\circ}}, ( \overline{A^{\circ}} )^{\circ}, \ldots $ and $ \overline{A}, ( \overline{A} )^{\circ}, \overline{( \overline{A} )^{\circ}}, \ldots $ a.) Show: This system of sets consists of at most 7 different sets. b.) Find an example $ A \subseteq X $ , where indeed 7 different sets occur. I've been struggling with this for a while now. I firstly tried to construct the example first to understand it better, so that $ ( \overline{A^{\circ}} )$ and $( \overline{A} )^{\circ}$ are different but this gets messy really fast. I just couldnt find a good way to start this problem so im very glad for any help.","['metric-spaces', 'vector-spaces', 'function-and-relation-composition', 'analysis', 'real-analysis']"
4900931,Flat Bundle vs Trivial bundle,"In R.W. Sharpe's Differential Geometry, a flat fibre bundle is defined as a bundle whose transition functions are constant. I don't understand the difference between this and a trivial bundle. Because, surely if the transition functions are constant you can just perform a rescaling so that they all become the identity, making the bundle trivial?","['vector-bundles', 'fiber-bundles', 'differential-geometry']"
4900988,Find the principal part at poles,"I am asked to find the isolated singularities of the function and determine their typing, order, and finding the principal part at each pole. So for the first function, which is $\frac{e^z-e}{z^2-1}$ , I know that it has poles of order $1$ at $\pm1$ . However, when trying to find the principal part, I'm running into some issues. My attempt was to rewrite as a partial, $$\frac{e^z-e}{z^2-1} = -\frac{e^z-e}{2(z+1)} + \frac{e^z-e}{2(z-1)}$$ But then I'm not really sure where to go from there. I suppose you're supposed to try and convert it to a Laurent series, but I'm still kind of confused on how to go about that. Any hints or help is appreciated. Edit: Tried to keep pushing on. Here's what I got: $$-\frac{e^z-e}{2(z+1)} + \frac{e^z-e}{2(z-1)} = -\frac{e^z-e}{2}(\frac{1}{1-(-z)}) - \frac{e^z-e}{2}(\frac{1}{1-z})$$ $$\Rightarrow -\frac{e^z-e}{2} ( \sum_{k=0}^{\infty}(-z)^k + \sum_{k=0}^{\infty} z^k )$$ But I have a feeling I've done something wrong, as it appears that no principal part pops out (all exponential values will be positive). Did I do something wrong?","['complex-analysis', 'taylor-expansion', 'laurent-series']"
4901014,"If $P(x)$ is a fifth degree polynomial such that $P(x)+1$ is dividable by $(x-1)^3$ and $P(x)-1$ is dividable by $(x+1)^3$, How to find $P(x)$","I saw this question: If $P(x)$ is a fifth degree polynomial such that $P(x)+1$ is dividable by $(x-1)^3$ and $P(x)-1$ is dividable by $(x+1)^3$ ,find $P(x)$ . I tried my best to find such function but I couldn't,somehow I got that this function doesn't exist which I refuse to believe because these type of questions always have an answer it is likely that I make a mistake somewhere. So assume $P(x)+1 =(x-1)^3(x-a)(x-b)$ and $P(x)-1=(x+1)^3(x-c)(x-d)$ $2= (x-1)^3(x-a)(x-b)- (x+1)^3(x-c)(x-d)$ for all $x$ If we substitute $x=0$ we have $-ab-cd=2$ Using the fact that the result of the sum of roots is the coefficient of $x^4$ which should be zero $$-3-a-b-3+c+d=0$$ $$c+d-a-b=6 \tag{*} $$ If we substitute $x= 1$ we get $1-c-d+cd = -\frac 1 4$ If we substitute $x=-1$ we get $1+a+b+ab=-\frac 1 4$ Adding  the last two equalities  we get $$a+b-c-d+2 +(ab+cd)= -\frac 1 2$$ $$a+b-c-d+2 +(-2)= -\frac 1 2$$ $a+b-c-d=-\frac 1 2$ which contradicts (*)","['algebra-precalculus', 'polynomials']"
4901028,Is every continuum-sized dense subset of the irrationals order isomorphic to the irrationals?,"This is a strengthening of a question another user asked, here: Are irrational numbers order-isomorphic to real transcendental numbers? . In the answer to that question, it was stated that the irrationals are order-isomorphic to the transcendental reals. My question is this. Suppose $S$ is a continuum-sized subset of the set of irrational numbers, which has the property that it is everywhere dense, meaning, between any two distinct reals, there exists a real number belonging to $S$ . Must $S$ be order-isomorphic to the irrationals?","['order-theory', 'real-analysis']"
4901030,How many homomorphism are from Z to the ring of matrices?,"Let be $(\mathbb{Z},+,\cdot)$ and $(\mathcal{M}_{2x2}(\mathbb{Z}) , + ,\cdot)$ rings, and $\phi: \mathbb{Z} \longrightarrow \mathcal{M}_{2x2}(\mathbb{Z})$ a function. How many ring homomorphisms $\phi$ are? I was thinking that $\phi \equiv 0 : x \mapsto 0 $ is the only, because in $\mathbb{Z}$ we don't have nilpotent elements and in $\mathcal{M}_{2x2}(\mathbb{Z})$ exist nilpotent elements (and cero divisors)","['matrices', 'ring-theory', 'functions', 'abstract-algebra', 'ring-homomorphism']"
4901061,Longest Increasing Subsequence Upper Bound,"Assume that the sequence $\{X_n\}$ are i.i.d with uniform distribution on $[0,1]$ . Now, define the length longest increasing subsequent as $$L_n=\max\{k:X_{i_1}<X_{i_2}<...<X_{i_k}, 1\leq i_1<i_2<...<i_k\leq n\}.$$ (1) Show that $\mathbb P[L_n\geq 2e\sqrt{n}]<\exp\{-2e\sqrt{n}\}$ (2) Show that for every $a>\frac{1}{3}$ there exists a constant b such that for all large $n$ we have: $$\mathbb P[|L_n-\mathbb E[L_n]|\geq n^a]\leq\exp\{-n^b\}.$$ My attempt : (1) It is not difficult to prove that the upper bound is $\mathbb P[L_n\geq k]\leq \begin{pmatrix}n\\ k\end{pmatrix}\cdot\frac{1}{k!}$ . However, I cannot connect this bound with the bound given in the question. (2) Using Azuma's inequality, I can prove that $$\mathbb P[|L_n-\mathbb E[L_n]|\geq t]\leq2\exp\bigg\{-\frac{t^2}{2n}\bigg\}.$$ Again, I don't know how to get rid of the coefficient 2 outside the exponential and also the coefficient 1/2 inside the exponential. If you have any suggestions, it would be greatly appreciated. Thank you!","['stochastic-processes', 'sequences-and-series', 'inequality', 'probability-theory', 'probability']"
4901127,"Asymptotics of $\int_{0}^{1} \frac{\tan^{-1}(x^n)}{\sqrt{1 - x^n}} \, dx $","Working around this question , I tried to compute $$I_n=\int_{0}^{1} \frac{\tan^{-1}(x^n)}{\sqrt{1 - x^n}} \, dx $$ which I have been unable to express using Mathematica for general $n$ . Some tedious work led me to $$I_n=\frac{\sqrt{\pi }}{n}\,\,\frac{\Gamma \left(\frac{n+1}{n}\right)}{\Gamma \left(\frac{3 n+2}{2 n}\right)}\,\,\,\, _4F_3\left(\frac{1}{2},1,\frac{n+1}{2 n},\frac{2 n+1}{2 n};\frac{3}{2},\frac{3 n+2}{4 n},\frac{5 n+2}{4 n};-1\right)$$ If $n$ is a real positive number (not necessarily an integer) or a complex, it works fine. Where the problem comes is when I try to build the asymptotics : the front factor does not make any problem using Stirling approximation but I am stuck with the hypergeometric function. At the time, thanks to  @Mariusz Iwaniuk, the only thing I know is that $$L=\underset{n\to \infty }{\text{limit}}\, _4F_3\left(\frac{1}{2},1,\frac{n+1}{2 n},\frac{2 n+1}{2 n};\frac{3}{2},\frac{3 n+2}{4 n},\frac{5 n+2}{4 n};-1\right)$$ $$L=\sec ^{-1}\left(\sqrt[4]{2}\right) \,\cosh^{-1}\left(1+\sqrt{2}\right)$$ Numerically, for infinitely large values of $n$ , making the coefficients rational, $$\, _4F_3\left(\frac{1}{2},1,\frac{n+1}{2 n},\frac{2 n+1}{2 n};\frac{3}{2},\frac{3 n+2}{4 n},\frac{5 n+2}{4 n};-1\right) \sim L \left(1-\frac{57}{971 n}+\frac{26}{775 n^2}-\frac{34}{1741
   n^3}+O\left(\frac{1}{n^4}\right)\right)$$ This makes that, for the time being, using rationalized coefficients $$I_n=\frac{4257}{2435 n}-\frac{817}{695 n^2}+\frac{1287}{1201
   n^3}+O\left(\frac{1}{n^4}\right)$$ My question is : Can we obtain the exact first coefficients of the asymptotics of the hypergeometric function ?","['integration', 'transcendental-functions', 'special-functions', 'asymptotics', 'trigonometry']"
4901155,Generalization of $\max$ and $\min$ of $Ax^2+Bxy+Cy^2$ with given $x^2+y^2=k$.,"Generalization of $\max / \min (Ax^2+Bxy+Cy^2)$ with given $x^2+y^2=k$ . ( $A, C > 0$ ) My first thought was drawing an ellipse and circle of each of the 2 given equations and think about the intersections the two shapes make. \begin{align}
& \text{let the ellipse be defined as: } \\
& Ax^2+Bxy+Cy^2+Dx+Ey+F=0. \\
\Rightarrow \; & 
\begin{cases}
A = a^2sin^2\theta+b^2\cos^2\theta & B = 2(b^2-a^2)\sin\theta\cos\theta \\
C = a^2\cos^2\theta+b^2\sin^2\theta & D = -2Ax_0-By_0 \\
E = -Bx_0-2Cy_0 & F = A{x_0}^2+Bx_0y_0+C{y_0}^2-a^2
\end{cases}, \\
& \text{where $a$ is semi-major axis, $b$ is the semi-minor axis,} \\
& \text{$(x_0, y_0)$ is the center coordinates, and $\theta$ is the rotation angle.} \\
& \cdots
\end{align} …It seemed too hard and involved to me. Then my second thought was to make it with trigonometry… \begin{align}
& x = \sqrt{x^2+y^2}\cos\theta, y=\sqrt{x^2+y^2}\sin\theta. \\
\Rightarrow \; & Ax^2+Bxy+Cy^2=(x^2+y^2)(A\cos^2\theta+B\cos\theta\sin\theta+C\sin^2\theta) \\
=\;&k(A\cos^2\theta+B\cos\theta\sin\theta+C\sin^2\theta) \\
=\;&kA\dfrac{\cos(2\theta)+1}{2}+kB\dfrac{\sin(2\theta)}{2}+kC\dfrac{-\cos(2\theta)+1}{2} \\
=\;& \dfrac{k(A+C)}{2}+\dfrac{k(A-C)}{2}\cos(2\theta)+\dfrac{kB}{2}\sin(2\theta) \\
\ \\
&\text{let }
\begin{cases} \dfrac{A-C}{\sqrt{(A-C)^2+B^2}}=\sin\gamma \\ \dfrac{B}{\sqrt{(A-C)^2+B^2}}=\cos\gamma
\end{cases}. \\
\ \\
\Rightarrow \; & Ax^2+Bxy+Cy^2 \\
=\;& \dfrac{k(A+C)}{2}+\dfrac{k}{2}\sqrt{(A-C)^2+B^2} (\cos(2\theta)\sin\gamma+\sin(2\theta)\cos\gamma) \\
=\;& \dfrac{k(A+C)}{2}+\dfrac{k}{2}\sqrt{(A-C)^2+B^2}\sin(2\theta+\gamma) \\
\in \;& \left[\dfrac{k(A+C)}{2}-\dfrac{k}{2}\sqrt{(A-C)^2+B^2}, \dfrac{k(A+C)}{2}+\dfrac{k}{2}\sqrt{(A-C)^2+B^2}\right]
\end{align} Yeah, so did I solved this… but this also seems too complicated. Are there other simpler solutions to this, and is the answer I’ve got right?","['elliptic-equations', 'trigonometry', 'solution-verification']"
4901197,Periodic perturbation of ODE,"Recently, I have read Khasminskii's book (Stochastic Stability of Differential Equations). In Section 3.5, the author mentioned that the following is well-known in the theory of ODEs. If $x_0$ is an asymptotically stable equilibrium point of an autonomous system $\mathrm{d}x/\mathrm{d}t=F(x)$ and $f(t)$ is $\theta$ -periodic, then for sufficiently small $\epsilon$ the system $\mathrm{d}x/\mathrm{d}t=F(x)+\epsilon f(t)$ has a $\theta$ -periodic solution in a neighborhood of the equilibrium point. But I didn't find references about the fact. Could you give me some references or hints? Thanks!","['perturbation-theory', 'periodic-functions', 'ordinary-differential-equations', 'dynamical-systems']"
4901216,A probability involving areas in a random pentagram inscribed in a circle: Is it really just $\frac12$?,"The vertices of a pentagram are five uniformly random points on a circle. The areas of three consecutive triangular ""petals"" are $a,b,c$ . The petals are randomly chosen, but they must be consecutive, either clockwise or anticlockwise. A simulation of $10^7$ such random pentagrams yielded a proportion of $0.5000179$ satisfying $a^2<bc$ . Is the following conjecture true: $P(a^2<bc)=\frac{1}{2}$ Remarks Note that the three petals must be consecutive. Calling the areas of consecutive petals $a,b,c,d,e$ , simulations suggest that: $P(a^2<bd)\approx0.468$ $P(a^2<be)\approx0.505$ $P(a^2<cd)\approx0.460$ Curiously, the probability that seems to equal $\frac{1}{2}$ , i.e. $P(a^2<bc)$ , does not involve a symmetrical arrangement of three petals. As for other random star polygons $\{\frac{n}{2}\}$ inscribed in a circle, simulations suggest that Star polygon $\left\{\frac{6}{2}\right\}$ : $P(a^2<bc)\approx0.505$ . Star polygon $\{\frac{7}{2}\}$ : $P(a^2<bc)\approx0.504$ . I used the shoelace formula to calculate the areas of the triangular petals. One might expect that a probability of $\frac12$ should have an intuitive explanation, but sometimes probabilities of $\frac12$ are hard to explain . Underlying reason? Simulations suggest that the random pentagram/pentagon shape is teeming with probabilities of powers of $\frac12$ . In the following diagram, the letters represent areas of the regions. $P(g+b+h<a+f+c)\overset{?}{=}\color{red}{\frac{1}{2}}$ $P(g+h<f)\overset{?}{=}\color{red}{\frac{1}{4}}$ $P((g+a+k)(h+c+i)>(b+f+e+d+j)^2)\overset{?}{=}\color{red}{\frac{1}{8}}$ $P(\text{each region with area $g,b,f,l$ contains the centre of the circle})=\color{red}{\frac{1}{16}}$ ( proved ) $P(\text{areas of petals increase going around once, clockwise or anticlockwise})\overset{?}{=}\color{red}{\frac{1}{32}}$ I am not asking to prove these other probability claims. I am presenting them to suggest that there may be an underlying reason why the pentagram has probabilities of powers of $\frac12$ , which may be relevant to my conjecture. These simulations make me more inclined to believe that my conjecture is true, but I could be wrong .","['integration', 'conjectures', 'geometric-probability', 'circles', 'probability']"
4901305,A discrete multiple sum over strictly increasing integer sequences.,"Let $l \ge 1$ and $m \ge 0$ and $k \ge l$ be integers.
Then let ${\bf A}= \left( A_\eta \right)_{\eta=1}^l$ and ${\bf B}= \left( B_\eta \right)_{\eta=1}^l$ be parameters.
We define the following multiple sum: \begin{equation}
{\mathfrak S}_{l,m}^{({\bf A},{\bf B})} (k):=
\sum\limits_{1 \le j_1 < j_2 < \cdots < j_l \le k}
\prod\limits_{\eta=0}^{l-1} (j_{\eta+1} + A_{\eta+1})^{(m+1)} (j_{\eta+1} + B_{\eta+1})^{(m)} \tag{1}
\end{equation} This sum appears in a natural way when finding the Frobenius power series solution to a certain class of ordinary differential equations with polynomial coefficients, see this post . We define auxiliary coefficients as follows: \begin{eqnarray}
&&{\mathfrak C}_l=
-\sum\limits_{\eta=1}^l \sum\limits_{\xi=0}^{(2(l-\eta)+1)m+l-\eta}
\Lambda^{(l,\eta)}_\xi \cdot (A_\eta-l+\eta)^{(m+l-\eta+2+\xi)}\\
&&\Lambda^{(l+1,\eta)}_\xi =
\sum\limits_{\xi_2=0 \vee \xi-( (2(l-\eta)+1)m+l-\eta )}^{(2m+1) \wedge \xi}
\sum\limits_{\xi_1=0 \vee (\xi_2-m-1)}^m \\
&&
\left( \right. \\
&&
\left.
\Lambda^{(l,\eta)}_{\xi-\xi_2} \cdot
(B_{l+1} - A_{l+1}-m-1)^{(m-\xi_1)}
(A_{l+1} - A_\eta-(m+\xi-\xi_2)-1)^{(m+1+\xi_1-\xi_2)} \cdot \binom{m+1+\xi_1}{\xi_2} \binom{m}{\xi_1} \cdot 1_{\eta \le l} + \right.\\
&&
\left.
{\mathfrak C}_l (B_{l+1} - A_{l+1}-m-1)^{(m-\xi)} \cdot \binom{m}{\xi} \cdot 1_{\eta=l+1} \right. \\
&& \left.
\right) \cdot \frac{1}{m+l-\eta+3+\xi} \tag{2a}
\end{eqnarray} for $l=1,2,\cdots$ , $\eta=1,\cdots,l+1$ and $\xi=0,\cdots, (2(l+1-\eta)+1)m+l+1-\eta$ subject to $\Lambda^{(1,1)}_\xi = (B_1-A_1-m-1)^{(m-\xi)} \binom{m}{\xi}/(m+2+\xi) $ . We have shown that this multiple sum admits the following neat closed form expression below. \begin{equation}
{\mathfrak S}_{l,m}^{({\bf A},{\bf B})} (k) = 
\sum\limits_{\eta=1}^l \sum\limits_{\xi=0}^{(2(l-\eta)+1) m+l-\eta}
\Lambda^{(l,\eta)}_\xi \cdot
\left(
(k+A_\eta-(l-\eta))^{(m+(l-\eta+2)+\xi)}
-
(0+A_\eta-(l-\eta))^{(m+(l-\eta+2)+\xi)}
\right) \tag{2}
\end{equation} We have derived the result by decomposing the product of Pochhammer symbols into linear combinations of single Pochhammer symbols  and then performing the sums bottom -up by using the Pascal triangle identity $\binom{n}{k} = \binom{n+1}{k+1} - \binom{n}{k+1}$ . Note: We coefficient at the highest  order term in $(2)$ are given below: \begin{eqnarray}
&&\Lambda^{(l,1)}_{\xi_m(l)} = \frac{1}{l! (2m+2)^l} \\
\end{eqnarray} where $\xi_m(l):= (2(l-1)+1)m+l-1$ . Proof by induction: Fix $l$ and take the new factor in the term in the multiple sum and decompose that factor into a linear combination of upper Pochhammer symbols as follows: \begin{equation}
(j_{l+1} + A_{l+1})^{(m_1)} (j_{l+1} + B_{l+1})^{(m_2)} 
=
\sum\limits_{\xi_1=0}^{m_2} (B_{l+1}-A_{l+1}-(m_1))^{(m_2-\xi_1)} \cdot \binom{m_2}{\xi_1}
\cdot
(j_{l+1} + A_{l+1})^{(m_1+\xi_1)} \tag{3a}
\end{equation} for integer $m_1,m_2 \ge 0$ . Now we in $(1)$ we replace $k \rightarrow j_{l+1}-1$ and multiply both sides of it by $(3)$ . Then we have: \begin{eqnarray}
&&\sum\limits_{1 \le j_1 < j_2 <  \cdots < j_l < j_{l+1} }
\prod\limits_{\eta=0}^l
(j_{\eta+1} + A_{\eta+1})^{(m+1)}
(j_{\eta+1} + B_{\eta+1})^{(m)}
= \\
&&
\left.
\sum\limits_{\eta=1}^l
\sum\limits_{\xi=0}^{(2(l-\eta)+1)m+l-\eta}
\sum\limits_{\xi_1=0}^m
\Lambda^{(l,\eta)}_\xi \cdot
(B_{l+1}-A_{l+1}-(m+1))^{(\eta-\xi_1)} \cdot \binom{m}{\xi_1}
\cdot
\underline{\underline{
(j_{l+1} + A_\eta -(l-\eta+1))^{(m+(l-\eta+2)+\xi)}
(j_{l+1}+A_{l+1})^{(m+1+\xi_1)}}} + \right. \\
&&
{\mathfrak C}_l 
\sum\limits_{\xi_1=0}^m
(B_{l+1}-A_{l+1}-(m+1))^{(\eta-\xi_1)} \cdot \binom{m}{\xi_1}
\cdot
\underline{(j_{l+1}+A_{l+1})^{(m+1+\xi_1)}} \tag{3b}
\end{eqnarray} Now we decompose the doubly underlined term by using $(3b)$ repeatedly. We have: \begin{eqnarray}
&&\sum\limits_{1 \le j_1 < j_2 <  \cdots < j_l < j_{l+1} }
\prod\limits_{\eta=0}^l
(j_{\eta+1} + A_{\eta+1})^{(m+1)}
(j_{\eta+1} + B_{\eta+1})^{(m)}
= \\
&&
\left.
\sum\limits_{\eta=1}^l
\sum\limits_{\xi=0}^{(2(l-\eta)+1)m+l-\eta}
\sum\limits_{\xi_1=0}^m
\sum\limits_{\xi_2=0}^{m+1+\xi_1}
\Lambda^{(l,\eta)}_\xi \cdot
(B_{l+1}-A_{l+1}-(m+1))^{(\eta-\xi_1)} \cdot \binom{m}{\xi_1}
\cdot
(A_{l+1} - (A_\eta-(l-\eta+1)) - (m+l-\eta+2+\xi))^{(m+1+\xi_1-\xi_2)} \cdot \binom{m+1+\xi_1}{\xi_2}
\cdot
\underline{(j_{l+1} + A_\eta-(l-\eta+1))^{((m+l-\eta+2+\xi+\xi_2)}}
+ \right. \\
&&
{\mathfrak C}_l 
\sum\limits_{\xi_1=0}^m
(B_{l+1}-A_{l+1}-(m+1))^{(\eta-\xi_1)} \cdot \binom{m}{\xi_1}
\cdot
\underline{(j_{l+1}+A_{l+1})^{(m+1+\xi_1)}} \tag{3c}
\end{eqnarray} Now we do the sum over $j_{l+1} = 1,\cdots, l$ by using the identity $ \sum\limits_{j=1}^k (j + A)^{(m)} = ((k+A)^{(m+1)} - A^{(m+1)})/(m+1)$ . We have: \begin{eqnarray}
&&\sum\limits_{1 \le j_1 < j_2 <  \cdots < j_l < j_{l+1} \le k}
\prod\limits_{\eta=0}^l
(j_{\eta+1} + A_{\eta+1})^{(m+1)}
(j_{\eta+1} + B_{\eta+1})^{(m)}
= \\
&&
\left.
\sum\limits_{\eta=1}^l
\sum\limits_{\xi=0}^{(2(l-\eta)+1)m+l-\eta}
\sum\limits_{\xi_1=0}^m
\sum\limits_{\xi_2=0}^{m+1+\xi_1}
\Lambda^{(l,\eta)}_\xi \cdot
(B_{l+1}-A_{l+1}-(m+1))^{(\eta-\xi_1)} \cdot \binom{m}{\xi_1}
\cdot
(A_{l+1} - (A_\eta-(l-\eta+1)) - (m+l-\eta+2+\xi))^{(m+1+\xi_1-\xi_2)} \cdot \binom{m+1+\xi_1}{\xi_2}
\cdot
%
%\underline{(j_{l+1} + A_\eta-(l-\eta+1))^{((m+l-\eta+2+\xi+\xi_2)}}
\frac{(k+A_\eta-(l-\eta+1))^{(m+l-\eta+3+\xi+\xi_2)} - (A_\eta-(l-\eta+1))^{(m+l-\eta+3+\xi+\xi_2)}}{m+l-\eta+3+\xi+\xi_2}
%
+ \right. \\
&&
{\mathfrak C}_l 
\sum\limits_{\xi_1=0}^m
(B_{l+1}-A_{l+1}-(m+1))^{(\eta-\xi_1)} \cdot \binom{m}{\xi_1}
\cdot
%
%\underline{(j_{l+1}+A_{l+1})^{(m+1+\xi_1)}} 
%
\frac{(k+A_{l+1})^{(m+2+\xi_1)} - (A_{l+1})^{(m+2+\xi_1)}}{m+2+\xi_1} \tag{3d}
\end{eqnarray} Now we simplify. We firstly move the sum over $\xi_1$ to the very right (to the very bottom of the nested multiple sum) and then we substitute $\xi+\xi_2 \rightarrow \xi$ . This yields: \begin{eqnarray}
&&\sum\limits_{1 \le j_1 < j_2 <  \cdots < j_l < j_{l+1} \le k}
\prod\limits_{\eta=0}^l
(j_{\eta+1} + A_{\eta+1})^{(m+1)}
(j_{\eta+1} + B_{\eta+1})^{(m)}
= \\
&& \left.
\sum\limits_{\eta=1}^l
\sum\limits_{\xi=0}^{(2(l+1-\eta)+1)m+l+1-\eta} \right. \\
&& \left.
\left(
%
\left(
\sum\limits_{\xi_2=0 \vee((2(l-\eta)+1)m+l-\eta)}^{(2m+1) \wedge \xi}
\sum\limits_{\xi_1=0 \vee (\xi_2-m-1)}^m
\Lambda^{(l,\eta)}_{\xi-\xi_2}
\cdot
%
(B_{l+1}-A_{l+1} -(m+1))^{(m-\xi_1)} \binom{m}{\xi_1}
\cdot
(A_{l+1}-(A_\eta-(l-\eta+1)) - (m+l-\eta+2+\xi-\xi_2))^{(m+1+\xi_1-\xi_2)} \binom{m+1+\xi_1}{\xi_2}
\right)
\cdot 
\frac{
(k+A_\eta-(l-\eta+1))^{(m+(l-\eta+3)+\xi)}
-
(A_\eta-(l-\eta+1))^{(m+(l-\eta+3)+\xi)}
}{m+l-\eta+3+\xi}
%
%
\right) +\right. \\
&&
{\mathfrak C}_l 
\sum\limits_{\xi_1=0}^m
(B_{l+1}-A_{l+1}-(m+1))^{(\eta-\xi_1)} \cdot \binom{m}{\xi_1}
\cdot
\frac{(k+A_{l+1})^{(m+2+\xi_1)} - (A_{l+1})^{(m+2+\xi_1)}}{m+2+\xi_1} \tag{3e}
\end{eqnarray} Now we notice that the term in the last, single, sum is equal to the term in the double sum above it at $\eta = l+1$ . As such we absorb the last single sum  into the double sum above it at $\eta = l+1$ .
Then we see that the right hand side of the equality above has the same functional form as the conjecture $(2)$ being evaluated at $l \rightarrow l+1$ with the new coefficients $\Lambda^{(l+1,\eta)}_\xi$ being given by $(2a)$ . This completes the proof. Below we verify the result numerically: (*Checking the final result.*)
lmax = 4; kmax = 10; m = 2;(*m=1,2,3*)
Clear[Lambda, k];

Lambda = Table[
   0, {l, 1, lmax}, {\[Eta], 1, l}, {\[Xi], 
    0, (2 (l - \[Eta]) + 1) m + l - \[Eta]}];
Lambda[[1, 1]] = 
  Table[(up[Subscript[B, 1] - Subscript[A, 1] - (m + 1), 
     m - \[Xi]] Binomial[m, \[Xi]])/(m + 2 + \[Xi]), {\[Xi], 0, m}];

Do[
  Cl = \!\(
\*UnderoverscriptBox[\(\[Sum]\), \(\[Eta] = 1\), \(l\)]\(
\*UnderoverscriptBox[\(\[Sum]\), \(\[Xi] = 
       0\), \(\((2 \((l - \[Eta])\) + 1)\) m + 
       l - \[Eta]\)]\((\(-1\))\) Lambda[\([l, \[Eta], \ 
        1 + \[Xi]]\)] up[\((
\*SubscriptBox[\(A\), \(\[Eta]\)] - \((l - \[Eta])\))\), \((m + \((l \
- \[Eta] + 2)\) + \[Xi])\)]\)\);
  Lambda[[l + 1, \[Eta], 1 + \[Xi]]] = If[\[Eta] <= l, (\!\(
\*UnderoverscriptBox[\(\[Sum]\), \(\[Xi]2 = 
         Max[0, \[Xi] - \((\((2 \((l - \[Eta])\) + 1)\) m + 
             l - \[Eta])\)]\), \(Min[\((2  m + 1)\), \[Xi]]\)]\(
\*UnderoverscriptBox[\(\[Sum]\), \(\[Xi]1 = 
          
          Max[0, \[Xi]2 - m - 1]\), \(m\)]Lambda[\([l, \[Eta], 
           1 + \[Xi] - \[Xi]2]\)] \((up[
\*SubscriptBox[\(B\), \(l + 1\)] - 
\*SubscriptBox[\(A\), \(l + 1\)] - \((m + 1)\), m - \[Xi]1])\) \((up[
\*SubscriptBox[\(A\), \(l + 1\)] - \((
\*SubscriptBox[\(A\), \(\[Eta]\)])\) - \((m + \[Xi] - \[Xi]2)\) - 
             1, \((m + 1 + \[Xi]1)\) - \[Xi]2] Binomial[\((m + 
              1 + \[Xi]1)\), \[Xi]2] Binomial[m, \[Xi]1])\)\)\)), 
     Cl up[Subscript[B, l + 1] - Subscript[A, l + 1] - (m + 1), 
       m - \[Xi]] Binomial[m, \[Xi]]] 1/(m + (l - \[Eta] + 3) + \[Xi]);
  , {l, 1, lmax - 1}, {\[Eta], 1, l + 1}, {\[Xi], 
   0, (2 (l + 1 - \[Eta]) + 1) m + l + 1 - \[Eta]}];
l = lmax - 1;
Print[""l,m="", {l, m}];
(*Closed form expression--fast*)
ex1 = Table[Expand[(\!\(
\*UnderoverscriptBox[\(\[Sum]\), \(\[Eta] = 1\), \(l\)]\(
\*UnderoverscriptBox[\(\[Sum]\), \(\[Xi] = 
          0\), \(\((2 \((l - \[Eta])\) + 1)\) m + 
          l - \[Eta]\)]Lambda[\([l, \[Eta], 
           1 + \[Xi]]\)] \((up[\((k + 1 + 
\*SubscriptBox[\(A\), \(\[Eta]\)] - \((l - \[Eta] + 
               1)\))\), \((m + \((l - \[Eta] + 
               2)\) + \[Xi])\)])\)\)\) + Cl)], {k, 1, kmax}];
(*From definition--slow*)
ex2 = Table[Sum[\!\(
\*UnderoverscriptBox[\(\[Product]\), \(\[Eta] = 0\), \(l - 1\)]\(up[\((
\*SubscriptBox[\(j\), \(\[Eta] + 1\)] + 
\*SubscriptBox[\(A\), \(\[Eta] + 1\)])\), \((m + 1)\)] up[\((
\*SubscriptBox[\(j\), \(\[Eta] + 1\)] + 
\*SubscriptBox[\(B\), \(\[Eta] + 1\)])\), m]\)\), 
    Evaluate[
     Sequence @@ 
      Table[{Subscript[j, \[Eta]], 
        If[\[Eta] == 1, 0, Subscript[j, \[Eta] - 1]] + 1, k}, {\[Eta],
         1, l}]]], {k, 1, kmax}];
(ex1 - ex2) // Expand

During evaluation of In[328]:= l,m={3,2}

Out[336]= {0, 0, 0, 0, 0, 0, 0, 0, 0, 0} Having said all this my question would be whether it is possible to derive the result $(2)$ using a combinatorial approach?","['pochhammer-symbol', 'summation', 'discrete-mathematics']"
4901342,$\mathbb{N}$ is the smallest infinite set,"We work in $\mathsf{ZF}$ . Given sets $A, B \neq \emptyset$ , we define the following relations: $A <_{\text{inj}} B$ $\Leftrightarrow$ $\exists f: A \hookrightarrow B$ : $f$ is injective and $\not\exists g: B \hookrightarrow A$ : $g$ is injective $A <_{\text{surj}} B$ $\Leftrightarrow$ $\exists f: B \twoheadrightarrow A$ : $f$ is surjective and $\not\exists g: A \twoheadrightarrow B$ : $g$ is surjective We say that a set $C \neq \emptyset$ is finite if there exists $n \in \mathbb{N}$ and a bijection $C \leftrightarrow \{1, \ldots, n\}$ . Say that a set $X \neq \emptyset$ is injectively small if we have $X <_{\text{inj}} \mathbb{N}$ . Say that a set $X \neq \emptyset$ is surjectively small if we have $X <_{\text{surj}} \mathbb{N}$ . Can we show that there exists no infinite injectively small set $X \neq \emptyset$ ? Can we show that there exists no infinite surjectively small set $X \neq \emptyset$ ? My attempt at a proof that there exists no infinite injectively small set $X \neq \emptyset$ : Assume that $X \neq \emptyset$ is infinite and injectively small, hence there is an injection $f: X \hookrightarrow \mathbb{N}$ . Since $X$ is nonempty, let $n_1 = \operatorname{min} f(X)$ and $x_1 \in X$ with $f(x_1) = n_1$ . Since $X$ is not in bijection with $\{1\}$ , we have $f(X) \setminus \{n_1\} \neq \emptyset$ . Let $n_2 = \operatorname{min} f(X) \setminus \{n_1\}$ and $x_2 \in X$ with $f(x_2) = n_2$ . Since $X$ is not in bijection with $\{1, 2\}$ , we have $f(X) \setminus \{n_1, n_2\} \neq \emptyset$ . Let $n_3 = \operatorname{min} f(X) \setminus \{n_1, n_2\}$ and $x_3 \in X$ with $f(x_3) = n_3$ . Continue inductively, obtaining a sequence $(x_k)_{k \in \mathbb{N}}$ with distinct terms. Now, define $g: \mathbb{N} \hookrightarrow X, g(k) = x_k$ . Since the terms of $(x_k)_{k \in \mathbb{N}}$ are distinct, $g$ is injective (in fact bijective), a contradiction. My attempt at a proof that there exists no infinite surjectively small set $X \neq \emptyset$ : Assume that $X \neq \emptyset$ is infinite and surjectively small, hence there is a surjection $f: \mathbb{N} \twoheadrightarrow X$ . For each $x \in X$ , set $n_x = \operatorname{min} f^{-1}(\{x\})$ . Now, the mapping $x \mapsto n_x$ is an injection $X \hookrightarrow \mathbb{N}$ , hence by an analogous argument there is a bijection $X \leftrightarrow \mathbb{N}$ , contradicting our assumption. Are these proofs correct? In particular, am I correct in that they do not depend on the axiom of choice in any way? Can we show that there is no infinite surjectively small set by directly exhibiting a surjection in the other direction and arriving at a contradiction?","['elementary-set-theory', 'solution-verification', 'set-theory']"
4901357,Curve of self-intersection $+1$ on complex algebraic surface,"Given a compact complex (algebraic) surface, assume it contains a holomorphic curve whose self-intersection equals $+1$ . Is there anything we can say about this complex surface? More precisely, will the existence of the $(+1)$ -curve give any topological or geometric obstruction to the complex surface? Any comments or discussions are appreciated.","['complex-geometry', 'algebraic-geometry', '4-manifolds']"
4901375,"Convergence of series $\sum_{n=1}^{\infty}x_n^{\alpha}$ subjected to $x_1\in(0,1),x_{n+1}=x_n-\frac{x_n^2}{\sqrt{n}}$.","Suppose $x_1\in(0,1),x_{n+1}=x_n-\frac{x_n^2}{\sqrt{n}}$ , find the values of $\alpha\in\mathbb{R}$ for which the series $\sum_{n=1}^{\infty}x_n^{\alpha}$ is convergent. The following is my solution, please help me to check if there is something which is not right. Any help and comments will welcome.
Other methods also welcome!
(PS: I also post this in AoPs $x_{n+1}=x_n-\frac{x_n^2}{\sqrt{n}}$ . Solution: It is not difficult to prove that: $\{x_n\}$ is strictly decreasing and $$0<x_n<1,\quad \lim_{n\to\infty }x_n=0.$$ By $x_{n+1}=x_n-\frac{x_n^2}{\sqrt{n}}$ , we get $$\frac{1}{x_{n+1}}=\frac1{x_n}+\frac{1}{\sqrt{n}-x_n},$$ then $$\frac{1}{x_{n+1}}-\frac{1}{x_1}=\sum_{k=1}^{n}\frac{1}{\sqrt{k}-x_k}.$$ So $$\frac{1}{x_1}+\sum_{k=1}^{n}\frac{1}{\sqrt{k}}<\frac{1}{x_{n+1}}=\frac{1}{x_1}+\sum_{k=1}^{n}\frac{1}{\sqrt{k}-x_k}
<\frac{1}{x_1}+\frac{1}{1-x_1}+\sum_{k=2}^{n}\frac{1}{\sqrt{k}-1}.
$$ As we know $$\sum_{k=1}^n\,\frac{1}{\sqrt{k}}\sim2\sqrt{n},\quad n\to\infty,$$ we know that $$x_n\sim\frac{1}{2\sqrt{n}},\quad n\to\infty.$$ Note the limits: by Stolz's theorem, $$\lim_{n\to\infty}\frac{1}{\sqrt{n}}\sum_{k=1}^{n}\frac{1}{\sqrt{k}}=2
=\lim_{n\to\infty}\frac{1}{\sqrt{n}}\sum_{k=2}^{n}\frac{1}{\sqrt{k}-1},$$ this means that $$\lim_{n\to\infty}2\sqrt nx_{n+1}=1,$$ Hence $$x_n\sim\frac1{2\sqrt n},\quad n\to\infty.$$ So $\sum_{n=1}^{\infty}x_n^{\alpha}$ is convergent if and only if $\alpha>2$ . Here also another method for $$x_n\sim\frac1{2\sqrt n},\quad n\to\infty,$$ but  we should know $\lim_{n\to\infty}\sqrt nx_n$ exists first and the limit is positive, if someone can give the proof of the existence and $>0$ . \begin{align*}
\lim_{n\to\infty}\sqrt nx_n
&=\lim_{n\to\infty}\frac{x_n}{\frac1{\sqrt{n}}}\\
&=\lim_{n\to\infty}\frac{x_n-x_{n+1}}{\frac1{\sqrt{n}}-\frac1{\sqrt{n+1}}}\\
&=\lim_{n\to\infty}\frac{\frac{x_n^2}{\sqrt{n}}}{\frac{\sqrt{n+1}-\sqrt{n}}{\sqrt{n(n+1)}}}\\
&=\lim_{n\to\infty}\sqrt{n+1}(\sqrt{n+1}+\sqrt{n})x_n^2\\
&=\lim_{n\to\infty}\frac{\sqrt{n+1}(\sqrt{n+1}+\sqrt{n})}{n}(\sqrt nx_n)^2\\
&=2\lim_{n\to\infty}(\sqrt nx_n)^2,
\end{align*} this implies $$\lim_{n\to\infty}\sqrt nx_n=\frac{1}{2}.$$","['limits', 'solution-verification', 'sequences-and-series', 'real-analysis']"
4901395,How do I check if this number is transcendental?,"Two days ago, I tried to create an infinite series that might be able to generate a transcendental number, and when I checked the proper definition, it was mentioned that, it is a number that cannot be expressed as a root of any non zero polynomial with integral coefficients and that it is irrational. (Please tell me if I am right) And I came up with this series: $$\gamma = \sum_{r=0}^\infty\frac{1}{(5^r)(10^{f(r)})},f(r)=4^r$$ The basic idea that I had was to generate a number, such that its decimals will be randomised by the digits of the numbers of geometric progression of 2. $$\frac{1}{5^1}=0.2$$ $$\frac{1}{5^2}=0.04$$ $$\frac{1}{5^3}=0.008$$ $$\frac{1}{5^4}=0.0016$$ $$\frac{1}{5^5}=0.00032$$ By means of trial and error, I found $10^{4^r}$ to be a perfect factor to addup zeroes in the decimal
And I believe that the function would produce something like this: $0.10002.........$ My questions are: Is the definition of transcendental number I mentioned legit? Does the series produce a transcendental number? If it does not, can I by chance define a $f(r)$ such that it gives a trascendental number? I am a complete novice to this, so feel free to leave your advice and opinions here!
Thank you!","['number-theory', 'solution-verification', 'transcendental-numbers', 'sequences-and-series', 'question-verification']"
4901423,Lower bound of $\frac{\|(\mathbf X \otimes \mathbf X^\top)\theta\|_2^2}{np}$,"According to Theorem 7.16 of High-Dimensional Statistics: A Non-Asymptotic Viewpoint (M. Wainwright, 2019), we know that for $\mathbf X\in\mathbb R^{n\times p}, X_{ij}\overset{iid}{\sim}N(0,1),$ there are universal positive constants $c_1 < 1 < c_2$ such that $$
\frac{\|\mathbf X\theta\|_2^2}{n}\geq c_1 \|\theta\|_2^2 - c_2 \frac{\log p}{n} \|\theta\|_1^2,\theta\in\mathbb R^p
$$ with high probability.
I want to derive the lower bound of the following, $$
\frac{\|(\mathbf X \otimes \mathbf X^\top)\theta\|_2^2}{np}, \theta\in\mathbb R^{np}
$$ The proof of Theorem 7.16 makes use of the Gordon-Slepian inequality and the variable $\dfrac{\langle u, \mathbf X v\rangle}{\sqrt n}$ is zero-mean Gaussian with variance $n^{-1}$ , where $u,v$ are unit vertors. While for $Z_{u,v} = \dfrac{\langle u, (\mathbf X \otimes \mathbf X^\top) v\rangle}{\sqrt n}$ , $Z_{u,v}$ is not a Gaussian variable and its mean is not zero, the techniques of Gaussian process cannot be used. If we consider $\theta = \theta _1 \otimes \theta_2, \theta _1 \in\mathbb R^{p}, \theta _2 \in\mathbb R^{n}$ , we can obtain that $$
\frac{\|(\mathbf X \otimes \mathbf X^\top)\theta\|_2^2}{np}\geq c_1 c_1^\prime \|\theta\|_2^2 - c_2c_1^\prime \frac{\log p}{n} \|\theta_1\|_1^2 \|\theta_2\|_2^2 - c_2^\prime c_1 \frac{\log n}{p} \|\theta_2\|_1^2 \|\theta_1\|_2^2
$$ Since $\|\theta\|_1\geq\|\theta\|_2,$ we can get $$
\frac{\|(\mathbf X \otimes \mathbf X^\top)\theta\|_2^2}{np}\geq \tilde c_1 \|\theta\|_2^2 - \tilde c_2 (\frac{\log p}{n} + \frac{\log n}{p}) \|\theta\|_1^2
$$ So I guess that the right-hand side of the inequality is the lower bound. But I don't know how to cope with $\theta$ without the Kronecker structure.
Thanks for your help!!!","['statistics', 'normal-distribution', 'upper-lower-bounds', 'random-matrices', 'probability-theory']"
4901498,What is the largest possible diameter of a shape made of centers of bounding boxes?,"Given a shape in the plane (fixed in space) and an angle $\theta$ , the shape's bounding box with orientation $\theta$ is the smallest rectangle containing the shape such that one of the sides is an angle of $\theta$ counterclockwise from horizontal. We may call the center of the bounding box with orientation $\theta$ , the bounding center with orientation θ , and then we may define the bounding center figure (bcf) of a shape to be the set of all bounding centers over all angles $\theta\in[0,\frac\pi2)$ . (This terminology is original to me. If someone already gave this concept a name, I do not know it.) The GIF below shows the bcf of the equilateral triangle; the result is a Reuleaux triangle, a shape of constant width. (If you're curious, the bcf of the Reuleaux triangle is a shape made of three limaçons.*) On the other hand, the bcf of any shape with twofold symmetry (such as a circle or ellipse) is a point. There are several natural questions to ask about this concept. One natural one is: If a figure has diameter $\bf1$ , what is the largest possible diameter of its bcf? (Recall that the diameter of a figure is the largest distance between two of its points.) The equilateral triangle (shown below) gives a lower bound of $\dfrac14$ . I conjecture that this is optimal. However, I am not certain how to approach proving this. *I mentioned that $\operatorname{bcf}^2($ equilateral triangle $)=\operatorname{bcf(}$ Reuleaux $)$ is made of three limaçons. Up to scaling, these limaçons look like $r=\sqrt{\frac32}-\sqrt3\cos\theta$ , but rotated, and shifted by $\frac{\sqrt3-1}2$ . I haven't verified this algebraically, only empirically. Most of the work towards making the GIF was done by the Discord user timpa.","['optimization', 'triangles', 'geometry']"
4901499,Calculate the value of $\sec^4\frac{\pi}{9}+\sec^4\frac{2\pi}{9}+\sec^4\frac{3\pi}{9}+\sec^4\frac{4\pi}{9}$.,"Calculate the value of $\sec^4\frac{\pi}{9}+\sec^4\frac{2\pi}{9}+\sec^4\frac{3\pi}{9}+\sec^4\frac{4\pi}{9}$ . I tried a lot of ways to calculate the sum, like converting to $\cos$ , applying the triple angle formula, or taking $2$ , $3$ , or even all $4$ at a time and simplifying. But I couldn't get anywhere.","['algebra-precalculus', 'trigonometry']"
4901542,Can we conclude that $X$ is $\mathcal{G}$-mensurable if $X=\mathbb{E}[X|\mathcal{G}]$ a.e.?,"Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space, $\mathcal{G\subseteq F}$ be a sub- $\sigma$ -algebra and $X:\Omega\to \mathbb{R}$ be an integrable random variable. Can we conclude that $X$ is $\mathcal{G}$ -mensurable if $X=\mathbb{E}[X|\mathcal{G}]$ a.e.? I don't know if it's true or false, however, according to some arguments in statistics (as you can see in this link , for instance), that proposition should be true. EDIT: The following is the definition of conditional expectation I'm using. Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space, $\mathcal{G\subseteq F}$ be a sub- $\sigma$ -algebra and $X:\Omega\to
\mathbb{R}$ be an integrable random variable. We say that $f:\Omega\to
 \mathbb{R}$ is a conditional expectation of $X$ given $\mathcal{G}$ if
the following propositions are true: $f$ is $\mathcal{G}$ -measurable and $\mathbb{P}$ -integrable; $\int _Gfd\mathbb{P}=\int _GXd\mathbb{P}$ for all $G\in\mathcal{G}$ .","['conditional-expectation', 'statistics', 'probability-theory']"
4901561,Prove that two integrating factors define a solution.,"I've been toiling away at this proof problem from Chapter 2.4 ending exercises of Differential Equations 3rd ed by Shepley L. Ross, but to no avail. Show that if $\mu (x, y)$ and $v(x, y)$ are integrating factors of $$ M(x, y)\ dx + N(x, y)\ dy = 0 $$ such that $\mu (x, y) / v(x, y)$ is not constant, then $$ \mu (x, y) = cv(x, y) $$ is a solution of the differential equation for every constant $c$ . My approach was that, iff $ \mu (x, y) / v(x, y) = c $ defines a solution for the differential equation then we must have $$ \frac{ \partial }{ \partial x } \left( \frac {u}{v} \right) = M(x, y) $$ $$ \frac{ \partial }{ \partial y } \left( \frac {u}{v} \right) = N(x, y) $$ and therefore must set out to prove it. On the outset, we have $$ \frac{ \partial }{ \partial y }(\mu M) = \frac{ \partial }{ \partial x }(\mu N) $$ $$ \frac{ \partial }{ \partial y }(v M) = \frac{ \partial }{ \partial y }(v M) $$ A bit of simplification later $$ \mu \left( \frac{ \partial M }{ \partial y } - \frac{ \partial N }{ \partial x } \right) = N(x, y) \frac{ \partial \mu }{ \partial x } - M(x, y) \frac{ \partial \mu }{ \partial y } $$ $$ v \left( \frac{ \partial M }{ \partial y } - \frac{ \partial N }{ \partial x } \right) = N(x, y) \frac{ \partial v }{ \partial x } - M(x, y) \frac{ \partial v }{ \partial y } $$ Doing the obvious and dividing, we have $$ \frac{u}{v} = \frac{ N(x, y) \displaystyle { \frac{ \partial \mu }{ \partial x } - M(x, y) \frac{ \partial \mu }{ \partial y } } }{ N(x, y) \displaystyle{ \frac{ \partial v }{ \partial x } - M(x, y) \frac{ \partial v }{ \partial y } } } $$ At this point, as I did not have any other leads, I decided to try and calculate the x and y partial derivatives. Starting with a deep breath, I set out onto a journey that lasted a good while but did not bring fruitful results. Enough terms did not cancel out and I am out of ideas, please HELP.","['integrating-factor', 'ordinary-differential-equations']"
4901610,Show that adding cosines with different frequencies implies the frequencies are the same,"How can I show that the following is true? For all $n \in \mathbb{N},$ and $a_i, b_i \in \mathbb{R}$ , if $a_i \neq 0$ for all $0<i\leq n$ , and if $$a_1 \cos{(b_1 x)} + a_2\cos{(b_2 x)} + a_3 \cos{(b_3 x)} + ... + a_n \cos{(b_n x)} = 0$$ for all $x \in \mathbb{R}$ , then for every $b_i$ there exists some $b_k$ such that $|b_i| = |b_k|$ . Ideas: In general, we can proceed by contradiction and assume that at least one pair is distinct. Furthermore, the non-distinct ones can just be added to form a new single term: $a_j \cos(b_j x) + a_k \cos(b_k x) = (a_j + a_k) \cos(b_j x)$ if $b_j=b_k.$ Therefore, we can assume WLOG that every $b$ is distinct. You can show easily show that $$\begin{pmatrix} 1 & 1 & ... & 1 \\ b_1 & b_2 & ... & b_n \\ b_1^2 & b_2^2 & ... & b_n^2 \\ \vdots & \vdots & \vdots & \vdots \\ b_n^{n-1} & b_2^{n-1} & ... & b_n^{n-1} \end{pmatrix} \begin{pmatrix}a_1 \\ a_2 \\ a_3 \\ \vdots \\ a_n \end{pmatrix}=0$$ Now, because $\vec{a}$ is not zero, we know that the determinant of that matrix must be zero. Another idea would be to proceed by induction; the cases $n=1, 2$ are easy but I am not sure if this is feasible. Finally, I think the easiest approach would probably be to view this as a Fourier series, and if each cosine is viewed as a vector, we just need to use the usual inner product to show that each of those are linearly independent vectors for distinct $b_j.$ The problem, though, is that because the $b$ s are reals, there is no convenient integral limits to choose. One strategy might be to approximate the $b$ s by rational numbers, and then choose the appropriate integral limits, and show that any error can be made arbitrarily small. I do not know for sure if this would work. I cannot make any of the above ideas work, and it also feels like there should be a simple and elegant way to do this.",['trigonometry']
4901618,"Constant C in Proposition 2.6.1 of Vershynin's ""High-Dimensional Probability"" : Sums of independent sub-gaussians","I'm searching the specific value of constant C in Proposition 2.6.1 regarding the sums of independent sub-Gaussian random variables, as presented in Vershynin's ""High-Dimensional Probability"" : https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf . Proposition 2.6.1 : Let $X_1, \ldots , X_n$ be independent
mean-zero sub-gaussian random variables, then, $$\left\| \sum_{i=1}^n X_i \right\|_{\psi_2}^2 \leq C \times \sum_{i=1}^n \|X_i\|_{\psi_2}^2$$ Based on the definition of a sub-Gaussian random variable (proposition 2.5.2) from the same book (i wrote only proposition interesting for this discussion) : (i) The tails of ( X ) satisfy $\mathbb{P}(|X| \geq t) \leq 2 \exp\left(-\frac{t^2}{K_1^2}\right) \quad \text{for all} \ t > 0. $ (iv) The MGF of $ X^2 $ is bounded at some point, namely, $\mathbb{E}\exp\left(\frac{X^2}{K_4^2}\right) \leq 2. $ (v) The MGF of $ X $ satisfies $ \mathbb{E}\exp(\lambda X) \leq \exp(K_5^2 \lambda^2) \quad \text{for all} \ \lambda \in \mathbb{R}. $ So if we read the proof we get the relation $K_1=K_4$ and $K_1 = 2K_5$ so $K_4 = 2K_5 (1)$ A discussion about this proposition : Making the absolute constant in sub-gaussian characterization explicit And this is the definition of the sub-gaussian norm : $$||X||_{\psi_2} = \inf\{t > 0, \mathbb{E}(e^{-\frac{X^2}{t^2}}) \leq 2\}$$ That's to say, the smallest $K_4$ I try to prove this and find C : For all $\lambda \in \mathbb{R}$ , we have $$\begin{align*}
\mathbb{E} \exp \left( \lambda \sum_{i=1}^n X_i \right)
&= \prod_{i=1}^n \mathbb{E}( e^{\lambda X_i} ) \quad \text{(by independence)} \\
&\leq \prod_{i=1}^n e^{\lambda^2 \left(\frac{1}{2}\right)^2 \|X_i\|_{\psi_2}^2} \quad \text{(since $X_i$ is sub-Gaussian and we use relation (1), we determine $\frac{\|X_i\|_{\psi_2}}{2}$)} \\
&= e^{\lambda^2 K_5^2} \quad \text{where } K_5^2 = \frac{1}{4} \sum_{i=1}^n \|X_i\|_{\psi_2}^2.
\end{align*}
$$ So we get this, where the square of this sum yields the anticipated result. $$
\left\|\sum_{i=1}^n X_i\right\|_{\psi_2} \leq 2 K_5
$$ I would appreciate if anyone could provide insight on determining $C$ or correct my understanding if I am misinterpreting the proposition. Because I find $C=1$ and it doesn't work... Indeed, if we take $n$ identical sub gaussian $X$ we get $$ \left\| \sum_{i=1}^n X \right\|_{\psi_2}^2 \leq 1 * \times \sum_{i=1}^n \|X\|_{\psi_2}^2 $$ That's to say, $$n^2 ||X||_{\psi_2}^2 \leq n ||X||_{\psi_2}^2 $$ $$ n^2 \leq n $$ This counterexample doesn't work, I missed independance ! It seems the constant works.","['moment-generating-functions', 'inequality', 'probability-theory', 'probability', 'random-variables']"
4901654,"If $T$ is sufficient, then there's a probability $\mathbb{P}_0$ such [...] $\mathbb{E}_{\mathbb{P}_0}(X|T)=E_P(X|T)$ a.e. for all $P\in\mathfrak{M}$","Let $(\Omega,\Sigma)$ be a measurable space and $\mathfrak{M}$ a family of probability measures. Suppose that exists a $\sigma$ -finite measure $\mu:\Sigma \to\overline{\mathbb{R}}$ such $P\ll \mu$ for all $P\in\mathfrak{M}$ . If $T$ is a sufficient statistic w.r.t. to $\mathfrak{M}$ , is at least one of the following propositions true? There's a probability measure $\mathbb{P}_0:\Sigma\to \mathbb{R}$ such that for all $X\in\cap _{P\in\mathfrak{M}}\mathcal{L}^1(P)$ we have that $X\in\mathcal{L}^1(\mathbb{P}_0)$ and $\mathbb{E}_{\mathbb{P}_0}(X|T)=E_P(X|T)$ $P$ -a.e. for all $P\in\mathfrak{M}$ . There's a probability measure $\mathbb{P}_0:\Sigma\to \mathbb{R}$ such that for all $X\in\mathcal{L}^1(\mathbb{P}_0)$ we have $X\in\mathcal{L}^1(P)$ and $\mathbb{E}_{\mathbb{P}_0}(X|T)=E_P(X|T)$ $P$ -a.e. for all $P\in\mathfrak{M}$ . Using the Theorem 2.4 which can be found at the page 38 of the book ""A Graduate Course on Statistical Inference"" written by Li and Babu (see this link ), I was able to prove that there's a probability measure $\mathbb{P}_0:\Sigma\to \mathbb{R}$ such for all $X\in(\cap _{P\in\mathfrak{M}}\mathcal{L}^1(P))\cap\mathcal{L}^1(\mathbb{P}_0)$ we have $\mathbb{E}_{\mathbb{P}_0}(X|T)=E_P(X|T)$ $P$ -a.e. for all $P\in\mathfrak{M}$ I also know that $\mathbb{P}_0 $ can be defined as $\mathbb{P}_0 :=\sum_{P\in\mathfrak{N}}c_PP$ in which $\mathfrak{N}$ is an enumerable subfamily of $\mathfrak{M}$ and $\{c_P\}_{P\in\mathfrak{N}}\subseteq (0,\infty )$ satisfies $\sum_{P\in\mathfrak{N}}c_P=1$ . Besides, $\mathfrak{N}$ has the following property: if $P(E)=0$ for all $P\in\mathfrak{N}$ , then $P(E)=0$ for all $P\in\mathfrak{M}$ . At least one of the proposition appears in some arguments in statistics (as you can see in this link , for instance) and in the page 41 of the book I mentioned (see the proof of the Theorem 2.5).","['conditional-expectation', 'measure-theory', 'probability-theory', 'statistics']"
4901672,Surfaces with irregularity zero,"I was wondering if there is a classification of algebraic surfaces with irregularity zero (i.e $H^1(\mathcal{O}_X)=0$ ). If not, can you help me come up with examples other than (weak) del Pezzos, hypersurfaces in $\mathbb{P}^3$ , branched covers of $\mathbb{P}^2$ ? I shall list them below: I) (Ariyan Javanpeykar) Any simply connected surface, e.g., fake projective planes or smooth complete intersections in $\mathbb{P}^n$ . Any comments are appreciated. Thanks.",['algebraic-geometry']
4901673,Integral $ \int \frac{1}{\sqrt[3]{(x-1)^7 (x + 1)^2}} \mathrm{d} x $,"I'm having trouble with the following integral: $$ \int \frac{1}{\sqrt[3]{(x-1)^7 (x + 1)^2}} \mathrm{d} x $$ I rewrite it in the form $$ \int \frac{1}{(x - 1)\sqrt[3]{(x - 1)^2(x^2 - 1)^2}}\mathrm{d} x $$ and tried substituting $ x = \cosh \theta$ . But, it doesn't seem to simplify to anything nice. I also tried $x = \cosh(2 \theta)$ in the first one and $x = \sec\theta$ , but they don't seem to work, either.","['integration', 'indefinite-integrals', 'calculus', 'real-analysis']"
4901677,What is canonical spectral theorem?,"My teacher has given me the following definition of the canonical spectrum theorem: Let a matrix the $A \in M_{n\times n}(\mathbb{R})$ , and set of eigenvalues, $\sigma(A)$ ={ $\lambda_1$ , $\lambda_2$ ........, $\lambda_k$ }. $A$ is diagonalisable $\Leftrightarrow A = \sum{\lambda_ip_i}$ such that $p_i:$ orthogonal projection onto $\mathcal{E}_{\lambda_i(A)},$ the eigen space $p_ip_j  = 0 \space \text{if} \space i\neq j$ $\sum{p_i}=I,$ the identity matrix. My question is: I don't understand above 3 points, what does mean of orthogonal projection here, and how it is related with eigen space? Also point 2 and 3 don't understand. Anybody help me to understand by giving example of above 3 points.","['matrices', 'linear-algebra', 'spectral-graph-theory', 'eigenvalues-eigenvectors']"
4901680,Problem of Fourier transform and compact operator,"Problem: Is the Fourier transform $F(f(x))=\int_{-\infty }^{\infty }f(y)e^{-ixy}dy$ a compact operator in the case of $F:L_1\left ( \mathbb {R} \right )\to \mathrm{BC}\left ( \mathbb{R}\right )$ . $\mathrm{BC}$ - Bounded and continuous My attempt to solve: Let's take a sequence of functions $e^{-n|x|}$ . They all lie in $L_1$ . The Fourier transform of them is (up to a universal coefficient) the Cauchy distribution $n/(n^2 + y^2)$ , the $C$ -norm of this is simply the maximum value, it is achieved at the point $y = 0 $ and equals $1/n$ . But this does not give anything, there the norm was $2/n$ , in the image it became proportional to $1/n$ How, in this case, can one reason and prove whether an operator is compact or non-compact?","['operator-theory', 'compact-operators', 'functional-analysis']"
4901795,Expected value of number of specific cards in starting hands in a card game,"I don't understand what I found when calculating the expected value of a card game. A deck contains 40 cards. 8 of them are red cards and 32 of them are blue cards. At the start of the game, 5 cards are drawn to be the starting hand. The question is to find the expected value of drawing red cards in the starting hand. d = number of cards in deck r = number of red cards in deck b = number of blue cards in deck s = number of starting hands n = number of red cards drawn in starting hand I believe the probability drawing n red cards in starting hand is something like this, $P_n = \frac{C_n^r×C_{s-n}^{b}}{C_s^{d}}$ I believe the expected value of number of red cards drawn in starting hand should be like (and given that r>s), $$E(n) = \sum_{n=0}^s n×P_n$$ So r=8 in this case, E(n) should be 1 if calculated correctly After that, I modify the deck, so there will be 16 red cards and 24 blue cards. And the deck size remains unchanged. So r=16 and the returned value of E(n) is 2 if I calculated correctly. At this moment, I noticed that to calculate E(n), I can just simply calculate this, $\frac{r} {d}×s$ I don't understand why this is possible. I tried to simplify the expression of the equation from my original calculation but I failed. Could anyone kindly explain to me what happened? Edit: Thank you very much for the first 2 responses, but I would like to ask if there is a way to simplify this, $\sum_{n=0}^s n × \frac{C_n^r×C_{s-n}^{b}}{C_s^{d}}$ (where r>s) Into this, $\frac{r} {d}×s$","['expected-value', 'card-games', 'probability']"
4901800,Automorphic elements,"Given two elements $x,y$ in a group, how can you quickly check whether there exists an automorphism $\phi$ such that $\phi(x) = y$ ? This seems well known but I can't find it online anywhere.
Obviously the order of $x$ and $y$ are equal, but this is not sufficient (consider the elements in $S_4 (12)34$ and $(12)(34)$ which both have order 2 but are not automorphic). If it is not known for all groups, is it possible to solve this just for abelian groups? It might be helpful to use the Finitely Generated by Cyclic group representation.","['automorphism-group', 'group-theory']"
4901839,How to find the legs of a right triangle if its hypotenuse is numerically equal to its area?,"I have an amazing ancient problem collection book by Chistyakov. I have managed to solve 165 problems of 248 (as of now, i. e. of time of current question posting); the remaining ones are really hard. If you permit, I would like to ask help for problem #100. One must find the legs (= catheti) of a right triangle, if it is known that its hypotenuse is numerically equal to its area. So hypotenuse equals area (ignoring the units of measurement). Find the catheti! [I suppose if there are multiple such triangles, then we must find all of them or find a general formula or something like that.] How to solve such kind of geometrical problem?","['triangles', 'geometry']"
4901900,Is every ring a homomorphic image of some ideal or subring of a free algebra?,"Since every group is the homomorphic image of a free group, and every module is the homomorphic image of a free module, do we have an analogous result for the rings? To take into account of nonunital rings, I wonder if we can always start from an ideal, or at least a subring, of a free algebra, and get to a given ring by quotiening out some ideal of the ideal/subring? Thanks. Here ""free algebra"" is meant to refer to the noncommutative ring of multivariable polynomials over $\mathbb{Z}$ , with the concatenation as the multiplication and empty set as the multiplicative identity. (But perhaps can also allow to be over a field or even a PID?) Also, here, by ""homomorphic image"", I mean a ""ring homomorphic image"", not just a module homomorphic image.","['ring-theory', 'abstract-algebra', 'ideals']"
4901914,"Given 99 bags of red and blue sweets, is there a selection of 50 bags containing at least half of each type of sweet?","Assume you have 99 bags containing sweets of two kinds, say blue and red. Is it always possible to pick out 50 bags such that you have at least half the total of red sweets and half the total of blue sweets. You can peek inside each bag to know what they contain. I knows it is always possible, and I have a solution, but I'm not particularly happy with it. I remember solving it three years ago and I thought my solution was more elegant then, but I've forgotten it. I am interested to know if there are other neat solutions. Here is my solution: Sort them in decreasing order of red sweets, then from bags numbered 2 to 99, arrange them in pairs (2,3), ..., (98,99). Then pick the ones with most blue sweets in each pair. This ensures that you end up with more than half of the total blue sweets minus the ones in bag 1, which we take anyway.
But we can also match bag 1 with the one not taken in (2,3), then the ""winner"" of (2,3) with the loser of (4,5) etc. And in this pairing, all the ones we selected have more red sweets than the one we don't take. Since all the bags except the winner among (98,99) are part of such a pairing and that each time we selected the one with most red sweets, we are sure to have at least half (of the total minus the number of red sweets minus the number of red sweets in the winner of (98,99), which we take anyways). Crucially I feel like the takeaway is ""If you have any (even size) collection and any matching in that collection, picking the greater value for each match ensures you pick at least half the total value"".","['puzzle', 'combinatorics', 'discrete-mathematics']"
4901924,Error with Cauchy Integral Formula,I have been told to calculate $$ \int_0^{2\pi}\frac{1}{2+2\text{sin}(\theta)} d\theta $$ I set $z = e^{i \theta}$ so parameterising by the unit circle and ended up with $$ \int_C \frac{1}{(z+i)^2} dz $$ My issue is that $-i$ is not in the interior of the unit circle so I can't apply the cauchy integral formula. This means I also can't apply the residue theorem.,"['integration', 'definite-integrals', 'analysis', 'complex-analysis', 'calculus']"
4901954,Minimum of $\frac{x^3}{x-6}$ for $x>6$ without using derivative?,"Find the minimum of $y=f(x)=\dfrac{x^3}{x-6}$ for $x>6$ . I can solve the question using derivatives but I have no any idea how to do it without them. Using derivatives, we find $x=9$ and $y_{min}=243$ . When looking for the minimum of a function, calculus is the default choice (with good reason), but it is a relatively new idea in Mathematics. Questions such as the one posed here are an interesting intellectual challenge because the obvious approach (calculus) is not the only approach. Without calculus there is no obvious approach though...","['optimization', 'algebra-precalculus', 'inequality', 'real-analysis']"
4901962,Integral $\int_0^{\frac{\pi}{2}}\frac{\log\left(\sin x\right)}{\cos^2x+y^2\sin^2x}{d}x=-\frac{\pi}{2}\frac{\log\left(1+y\right)}{y}$,"Prove that $$\int_0^{\frac{\pi}{2}}\frac{\log\left(\sin x\right)}{\cos^2\left(x\right)+y^2\sin^2\left(x\right)}{\rm d}x=-\frac{\pi}{2}\frac{\log\left(1+y\right)}{y}$$ where $y\ge0$ . I came across this problem on a math forum, but after attempting to solve it, I didn't get any results. I attempted to solve the problem using the residue theorem but found it challenging to prove the expression. I believe the issue lies in how to handle $\log(\sin x)$ , which might require some clever substitution. But I couldn't think of any. I hope someone can provide the correct solution. Thanks!","['integration', 'trigonometric-integrals', 'improper-integrals']"
4901997,Will solutions of SDE with different initializations intersect at some point?,"Inspired by the result for ODE here , which shows that solutions to the same ODE with different initializations do not intersect, I am wondering if similar results also hold for SDE? Consider the SDE $$dX_t=f(X_t,t)dt+g(X_t,t)dB_t,$$ if both $f,g$ are Lipschitz with linear growth, then there must exists a unique strong solution for any independent initialization. If we consider the equation with $X_0=Z$ and $X_0=Y$ , will there exist some $t$ such that $X_t^Y=X_t^Z$ a.s., (or some other way to define 'intersection of the solutions' more properly)? As mentioned, there does not exist such $t$ in ODE. I tried to extend the ODE results here, but it seems one essential part is that for ODE, the uniqueness of solution is ensured on the interval $[-a,a]$ , meaning that you can can extend the solution uniquely to the left-hand direction, so that the result follows. But for SDE, seems the unique results are only for the right-hand direction, and I am not sure whether a similar proof can be obtained. Any existing results can help?","['stochastic-processes', 'stochastic-differential-equations', 'stochastic-calculus', 'ordinary-differential-equations']"
4902019,Inequality involving a symmetric matrix and minors of an orthogonal matrix,"Fix $n \geq 3$ and take any orthonormal vectors $x,y,z \in \mathbb{R}^n$ . Let also $A \in M_n(\mathbb{R})$ be a symmetric matrix with positive entries ( $A_{ij} = A_{ji} > 0$ ). Is the following inequality true? $$\sum_{i<j}A_{ij}\begin{vmatrix}x_i & x_j \newline y_i & y_j \end{vmatrix}^2 \leq \sum_{i<j<k}\max(A_{ij},A_{ik},A_{jk})\begin{vmatrix}x_i & x_j & x_k \newline y_i & y_j & y_k \newline z_i & z_j & z_k \end{vmatrix}^2$$ For $n=3$ the proof is easy: The sum on the LHS has just three terms and one can simply write $$A_{12}\begin{vmatrix}x_1 & x_2 \newline y_1 & y_2 \end{vmatrix}^2 + A_{13}\begin{vmatrix}x_1 & x_3 \newline y_1 & y_3 \end{vmatrix}^2 + A_{23}\begin{vmatrix}x_2 & x_3 \newline y_2 & y_3 \end{vmatrix}^2 \leq \max(A_{12},A_{13},A_{23})\Bigg(\begin{vmatrix}x_1 & x_2 \newline y_1 & y_2 \end{vmatrix}^2 + \begin{vmatrix}x_1 & x_3 \newline y_1 & y_3 \end{vmatrix}^2 + \begin{vmatrix}x_2 & x_3 \newline y_2 & y_3 \end{vmatrix}^2\Bigg) = \max(A_{12},A_{13},A_{23}) = RHS$$ Numerical experiments suggest that the inequality holds for $n>3$ as well, but how to prove it? Any hint would be much appreciated!","['determinant', 'orthonormal', 'linear-algebra', 'symmetric-matrices', 'inequality']"
4902036,How to approximate $\frac{(2n+1)}{2^n} \int_0^1 \left( 1-x^2+\sqrt{1+2x^2-3x^4}\right)^ndx$,"Let $$
f(x) = \frac{1-x^2+\sqrt{1+2x^2-3x^4}}{2}
$$ How to approximate the integral $$
I_n = (2n+1)\int_0^1 f(x)^n dx?
$$ Experiments seem to indicate that it is something like $cn^{0.75}+1$ where $c$ is somewhere around $1.8128$ . Context This formula appears in the expected value of crosses placed on a $2 \times n$ board before a square block of $4$ appears (i.e there are crosses on four squares that share a corner) when placing them randomly. Using transfer matrix we get that the generating function for boards without square blocks enumerated by number of crosses is $$\left(
\begin{array}{r}1 & 1 & 1
\end{array} \right)
\left(\begin{array}{rrr}
1 & 1 & 1 \\
2 x & 2 x & 2 x \\
x^{2} & x^{2} & 0
\end{array}\right)^{n-1}
\left(
\begin{array}{rrr}
1 \\
2x \\
x^2
\end{array} \right)
$$ Then using the fact that $(2n+1)\int_0^1 x^k(1-x)^{2n-k}dx = {2n \choose k}^{-1}$ and diagonalizing the matrix we get that the expected value is $$
(2n+1) \int_{0}^{1}b\left(x\right)^{n-1}\left(\frac{1}{2}+\frac{1+x^{2}-2x^{4}}{2\sqrt{1+2x^2-3x^4}}\right)-a\left(x\right)^{n-1}\left(\frac{1+x^{2}-2x^{4}}{2\sqrt{1+2x^2-3x^4}}-\frac{1}{2}\right)dx
$$ where $b(x)=f(x)$ and $a(x) = \frac{1-x^2-\sqrt{1+2x^2-3x^4}}{2}$ . The $a$ -part tends to zero and the coefficient of $b$ is approximately $1$ where $b^{n-1}$ isn't small, so therefore I'm only asking about the $b$ . See this Desmos for experiments. UPDATE My try: Leaving out and changing irrelevant parts, let's try to calculate $\lim_{n\to \infty} n^{1 \over 4}\int_0^1 \left(\frac{1-x^2+\sqrt{1+2x^2-3x^4}}{2}\right)^ndx$ . Changing variable $t=n^{1 \over 4}x$ , we have $$
\int_0^{n^{1 \over 4}} \left(\frac{\sqrt{n}-t^2+\sqrt{n+2\sqrt{n}t^2-3t^4}}{2\sqrt{n}}\right)^n dt
$$ Now the integrand seems to converge as $n \to \infty$ , so this seems promising...","['integration', 'definite-integrals', 'combinatorics', 'generating-functions', 'diagonalization']"
4902094,Divergence of $\big(\frac{5k^2}{k+1}\big)^k\cdot\frac{1}{5(k+1)}$ to $\infty$,"I want to show that the sequence $x_k=\left(\frac{5k^2}{k+1}\right)^k\cdot\frac{1}{5(k+1)}$ diverges to $\infty$ .
I struggle to show this formally, I know that $x^k$ for $x>1$ will ""win"" against $\frac{1}{5(k+1)}$ but I don't know how to show this formally. Thanks in advance!","['convergence-divergence', 'sequences-and-series', 'analysis', 'real-analysis']"
4902204,Find the length of EF in the following trapezoid,"While I am trying to solve the following problem: In the trapezoid $ABCD$ , $AD\parallel BC$ , $\angle B = 30^{\circ}$ , $\angle C =60^{\circ}$ . $E$ , $M$ , $F$ , $N$ are the midpoints of $AB$ , $BC$ , $CD$ , $DA$ respectively. Given that $BC =7$ , $MN = 3$ . Find $EF$ . First, I made 3 perpendiculars from A, N and D on the base BC, then I tried to make use of the two angles $\angle B = 30^{\circ}$ , $\angle C =60^{\circ}$ to get the length of these perpendiculars but in vain because I don't have the length of any side from the resulting triangles! Any hint to find the length of $EF$ is appreciated.",['geometry']
4902205,Showing a basic market admits no arbitrage,"Setting We work in $\left(\Omega, \mathcal{F},\left(\mathcal{F}_t\right)_{t=0}^1, \mathbb{P}\right)$ . Let $d=1, T=1$ and assume the discounted price equals the non-discounted price. Take $S_0^1 \in \mathbb{R}_{+}$ , and $S_1^1 \in\left\{\alpha S_0^1, \beta S_0^1\right\}$ each with positive probability s.t. $0<\alpha<\beta$ . Task I want to show that $\alpha<1<\beta $ iff there is no arbitrage. Additionally I'd like to find an example which shows that if $\mathcal{F}_0$ is not the trivial $\sigma$ -algebra, then there exists an arbitrage. Attempt I know that for there to be an arbitrage I'd need to find $H_1$ s.t. $$
\mathbb{P}\left(H_1 \cdot \left(S_1^1-S_0^1\right) \geq 0\right)=1 \text { and } \mathbb{P}\left(H_1 \cdot \left(S_1^1-S_0^1\right)>0\right)>0 .
$$ but I don't know what to base the proof on besides that. I would be grateful for any help!","['measure-theory', 'finance', 'stochastic-processes', 'martingales', 'probability-theory']"
4902210,$X_n \rightarrow_d X$ and UI implies $\mathbb{E}X_n\rightarrow\mathbb{E} X$,"Claim: Let $(X_i),X$ be real valued r.v.s. Then $X_n \rightarrow_d X$ and uniformly integrable implies $\mathbb{E}X_n\rightarrow\mathbb{E} X$ How can I prove this claim directly? Here's how I proved it using machinery: If $X_n\rightarrow_d X$ then there exists a sequence $(\overline{X_n})$ and r.v. $\overline{X}$ with the same distributions as $(X_n)$ and $X$ resp. on the same probability space such that $\overline{X}_n\rightarrow_{p}\overline{X}$ . The new sequence is also UI, so we have $\overline{X}_n \rightarrow_{L^1} \overline{X}$ . Thus $\mathbb{E}\overline{X}_n\rightarrow \mathbb{E}\overline{X}$ and we can remove the overlines as the distributions are the same.","['uniform-integrability', 'measure-theory', 'probability-theory', 'weak-convergence']"
4902238,Asymptotic expansion of $\int_0^{2\pi} \frac{\cos(nt)}{1+t^2} dt$,"Question: Show that the integral $$I(n) = \int_0^{2\pi} \frac{\cos(nt)}{1+t^2} dt $$ has an asymptotic expansion of the form as $n\to\infty$ $$ f(n) = \sum_{k=0}^\infty a_k \frac{1}{n^{2k}} $$ Evaluate the coefficients $a_0, a_1, a_2$ . What I've done: I've used two ways to evaluate the integral: (1) I tried to relate to Riemann Lebesgue lemma. $I(n)$ is even in $n$ , so I changed variables from $t$ to $u = t - \pi$ and then got: $$I(n) = Re[ \int_{-\pi}^{\pi} \frac{\exp(-inu)\exp(-in\pi)}{1+(u+\pi)^2} dt] $$ then the function $\frac{\exp(-in\pi)}{1+(u+\pi)^2}$ is infinitely differentiable, so the integral decays like $O(n^{-k})$ for any $k\in N$ , so all coefficients, after expanding the integral in the inverse powers form, must vanish. My question: if my argument of vanishing coefficients holds, how do I show the integral do have expansion w.r.t. inverse powers of $n$ ? This seems arbitrary to me, especially that I don't know the coefficients $a_n$ . (2) I write $I(n) = Re[\int_0^{2\pi} (1+t^2)^{-1} e^{int} dt] =^{it = -s} = Re[\int_0^{-2\pi i} (1-s^2)^{-1} e^{-ns} i ds]$ , and then show integrals of the same integrand from ${-2\pi i}$ to ${-2\pi i +2\pi}$ (call it $J_1$ ) and $2\pi - 2\pi i$ to $2\pi$ (call it $J_2$ ) vanish (the first vanish at rate $O(\frac{1}{n})$ and the second $O(e^{-2n\pi})$ ). Then I write: $$I(n) = Re[\int_0^{2\pi} (1-s^2)^{-1} e^{-ns} ids]$$ Using Watson's lemma, I can easily show $I(n) = Re[i\sum_{m=0}^\infty \Gamma(2m+1)/n^{2m+1}] = 0 (*)$ . My question: I'm a bit unsure about my argument about showing the integral from 0 to $-2\pi i$ equal to the integral from 0 to $2\pi$ , since what I've shown is that $J_{1,2} = 0$ but not, say, $J_{1,2} = o(I(n))$ . Further, my result in $(*)$ is in odd powers of n, but the required form is in even powers, which made me suspicious about my answer. Could anyone verify this for me please? Thank you!","['integration', 'complex-analysis', 'physics', 'asymptotics']"
4902239,Which differential equations are invariant under change of camera projection?,"For background, I am working in the plane $\mathbb{R}^2$ . I know that the derivatives $\frac{\partial}{\partial x}$ and $\frac{\partial}{\partial y}$ are invariant under translation. I know that the Laplacian $\frac{\partial^2}{\partial x^2} + \frac{\partial ^2}{\partial y^2}$ is invariant under rotations. And now I want to consider homographies, which are invertible transformations $\tau:\mathbb{R}^2 \rightarrow \mathbb{R}^2$ of the form $$\tau(x,y) = \left\langle \frac{ax + by +c}{gx + hy + i},\;\frac{dx + ey +f}{gx + hy + i}\right\rangle.$$ I have constructed one differential operator that I believe is invariant under all homographies: $$\mathscr{L}(F) =  F_{xx} F_y^2 - 2 F_{xy} F_x F_y + F_{yy} F_x^2$$ But I'm sufficiently unfamiliar with this area of mathematics that I don't know if there is a systematic way to find others, or all such operators. I also don't know if the answer to my question is a well-known result. I'm looking for insights such as: A characterization of these operators, their closure properties, a second example of a differential operator that's invariant under homography, suggestions for how to efficiently establish invariance, hints about how to generate such operators, and/or keywords for what to look at next. Most of what I've been able to find involves Lie groups on
manifolds, which is outside my area of expertise and feels
potentially heavy-handed and abstract for a situation in $\mathbb{R}^2$ . Edit: Incidentally, I found this theorem , which states: A differential operator $L=\sum_{\lvert \alpha\rvert\le m} a_\alpha(x)\partial^\alpha$ is invariant under all translations and rotations iff $L$ can be written as a polynomial $\mathsf{poly}(-\Delta)$ in the Laplacian $-\Delta$ . Because invariance under homography implies invariance under translation and rotation, I initially asssumed the theorem would apply here. After some initial confusion, I concluded that the operator in the theorem is assumed to be linear in a way that mine isn't, because mine depends on $F_y^2$ and so on. But I still hope the theorem represents a step in the right direction.","['differential-operators', 'group-actions', 'projective-geometry', 'differential-geometry']"
4902256,Property of vector-valued measure,"Let $B$ be a Banach space, let $(X,\mathcal{A})$ be a measurable space, and let $\mu:\mathcal{A}\to B$ be a vector-valued measure of bounded variation. In general, if $B$ doesn't have the Radon-Nikodym property, it is not true that we can express $\mu$ as $$
\mu(A) = \int_A f(x)\,m(dx)
$$ for some (real-valued) measure $m$ on $X$ and some Bochner-measurable function $f:X\to B$ . Instead, could we find a (real-valued) measure $m$ on a product $X\times Y$ and a Bochner-measurable function $f:Y\to B$ , such that $\mu$ can be written as the following Bochner integral, $$
\mu(A) = \int_{A\times Y} f(y) \,m(dx\,dy) \qquad ?
$$ (This seems easy when Radon-Nikodym holds, but what about in the general case?)","['integration', 'banach-spaces', 'measure-theory', 'bochner-spaces']"
4902296,Question About Function Integrability - Proposition 2.3.10 from Measury Theory by Donald Cohn,"I am self-studying measure theory using Measure Theory by Donald Cohn. The book makes the following definition: Definition $\quad$ Suppose that $f:X\to[-\infty,+\infty]$ is $\mathscr{A}$ -measurable and that $A\in\mathscr{A}$ . Then $f$ is integrable over $A$ if the function $f\chi_A$ is integrable, and in this case $\int_Afd\mu$ , the integral of $f$ over $A$ , is defined to be $\int f\chi_Ad\mu$ . I am confused by the proof the following proposition: Proposition $\quad$ 2.3.10 $\quad$ Let $(X,\mathscr{A},\mu)$ be a measure space, and let $f$ be a $[0,+\infty]$ -valued $\mathscr{A}$ -measurable function on $X$ . If $t$ is a positive real number and if $A_t$ is defined by $A_t=\{x\in X:f(x)\geq t\}$ , then \begin{align*}
    \mu(A_t)\leq\frac{1}{t}\int_{A_t}fd\mu\leq\frac{1}{t}\int fd\mu.
\end{align*} Proof $\quad$ The relation $0\leq t\chi_{A_t}\leq f\chi_{A_t}\leq f$ and part (c) of Proposition 2.3.4 imply that \begin{align*}
    \int t\chi_{A_t}d\mu \leq \int_{A_t}fd\mu\leq\int fd\mu.
\end{align*} Since $\int t\chi_{A_t} = t\mu(A_t)$ , the proposition follows. My question with this proof is what if $f\chi_{A_t}$ is not integrable? According to the definition, $\int f\chi_{A_t}d\mu = \int_{A_t}fd\mu$ is only defined when $f\chi_{A_t}$ integrable. However, in this proposition, $f$ is a nonnegative extended real-valued function, which means it can have value $+\infty$ at some point in $X$ . Then \begin{align*}
\begin{split}
&\int(f\chi_{A_t})^+d\mu\\
=\ &\int(f\chi_{A_t})d\mu\\
=\ &\sup\left\{\int gd\mu:g\ \text{is a nonnegative simple real-valued $\mathscr{A}$-measurable function on $X$}\\
\text{and $g\leq f\chi_{A_t}$}\right\}\\
=\ &\sup\left\{\sum_{i=1}^mb_i\mu(B_i):\text{$g$ is $\mathscr{A}$-measurable; $g=\sum_{i=1}^mb_i\chi_{B_i}\leq f\chi_{A_t}$;}\\
\text{and for all $i$ $b_i\in[0,+\infty)$, $B_i$'s are disjoint subsets of $X$ that belong to $\mathscr{A}$}\right\}
\end{split}
\end{align*} may have value $+\infty$ , and so that $f\chi_{A_t}$ is not integrable, and so $\int_{A_t}fd\mu$ is not defined. Am I missing anything? Or is the book wrong? I really appreciate any help! Background Information: Construction of the Integral Stage 1 $\quad$ We begin with the simple function. Let $(X,\mathscr{A})$ be a measurable space. We will denote by $\mathscr{S}$ the collection of all simple real-valued $\mathscr{A}$ -measurable functions on $X$ and by $\mathscr{S}_+$ the collection of nonnegative functions in $\mathscr{S}$ . Let $\mu$ be a measure on $(X,\mathscr{A})$ . If $f$ belongs to $\mathscr{S}_+$ and is given by $f = \sum_{i=1}^ma_i\chi_{A_i}$ where $a_1,\dots,a_m$ are nonnegative real numbers and $A_1,\dots,A_m$ are disjoint subsets of $X$ that belong to $\mathscr{A}$ , then $\int f d\mu$ , the integral of $f$ with respect to $\mu$ , is defined to be $\sum_{i=1}^ma_i\mu(A_i)$ (note that this sum is either a nonnegative real number or $+\infty$ ). Stage 2 $\quad$ As our next step, we define the integral of an arbitrary $[0,+\infty]$ -valued $\mathscr{A}$ -measurable function on $X$ . For such a function $f$ , let \begin{align*}
\int fd\mu = \sup\left\{\int gd\mu:g\in\mathscr{S}_+\ \text{and}\ g\leq f\right\}.
\end{align*} Stage 3 $\quad$ Finally, let $f$ be an arbitrary $[-\infty,+\infty]$ -valued $\mathscr{A}$ -measurable function on $X$ . If $\int f^+d\mu$ and $\int f^-d\mu$ are both finite, then $f$ is called integrable (or $\mu$ -integrable or summable ), and its integral $\int fd\mu$ is defined by \begin{align*}
\int fd\mu = \int f^+d\mu - \int f^-d\mu.
\end{align*} The integral of $f$ is said to exist if at least one of $\int f^+d\mu$ and $\int f^-d\mu$ is finite, and again in this case, $\int fd\mu$ is defined to be $\int f^+d\mu - \int f^-d\mu$ . In either case one sometimes writes $\int f(x)\mu(dx)$ or $\int f(x)d\mu(x)$ in place of $\int fd\mu$ .","['integration', 'measure-theory', 'analysis', 'real-analysis', 'measurable-functions']"
4902306,Understanding inductive proof of $\sum_{{m=k}}^{N} {m\choose k} = {N+1\choose K+1}$,"I am an engineering student and am trying to prove the following combinatorics identity in math: $$\sum_{{m=k}}^{N} C(m,k) = C(N+1, K+1)$$ It was suggested to me to use Proof By Induction so I tried to do this problem. Step 1: Show this identity is true for a specific choice of $N=K$ LHS: $$\sum_{{m=K}}^{N} C(m,k) = \sum_{{m=k}}^{k} C(m,k) = C(k,k) = 1$$ RHS: $$C(N+1, k+1)$$ $$\text{Substitute } N=k $$ $$C(k+1, k+1) = C(K+1, K+1) = 1$$ Step 2: Assume this identity is true for any $N$ Step 3: Prove this identity is true for $N=N+1$ : LHS: $$\sum_{{m=k}}^{N} C(m,k)$$ $$\text{Substitute } N=N+1 $$ $$ \sum_{{m=k}}^{N+1} C(m,k)$$ RHS: $$C(N+1, K+1)$$ $$\text{Substitute } N=N+1 $$ $$C(N+1+1, k+1) = C(N+2, K+1)$$ Now, prove that LHS=RHS, i.e. $$\sum_{{m=k}}^{N+1} C(m,k) = C(N+1+1, k+1) = C(N+2, K+1)$$ We start by showing: $$\sum_{{m=k}}^{N+1} C(m,K) = \color{red}{\sum_{{m=k}}^{N} C(m,k)} + \sum_{{m=N+1}}^{N+1} C(m,k) = \color{red}{C(N+1, k+1)} + C(N+1, k)$$ Note that $\color{red}{\sum_{{m=k}}^{N} C(m,k)} = \color{red}{C(N+1, k+1)}$ is assumed to be true because of Step 2. Next, we continue the proof to show: $${C(N+1, k+1)} + C(N+1, k)$$ $$ = \frac{(N+1)!}{(K+1)!(N-K)!} + \frac{(N+1)!}{(K)!(N+1-K)!}$$ Using the idea that in general, $n! = n \cdot (n-1)!$ , we write: $$\frac{(N+1)!}{(K+1)!K!(N-k)!} + \frac{(N+1)!}{K!(N-k)!(N+1-k)}$$ Looking for common terms, we write: $$\frac{(N+1)!}{K!(N-k)!} \left[ \frac{1}{(K+1)} + \frac{1}{(N+1-k)} \right]$$ $$= \frac{(N+1)!}{K!(N-K)!} \left[ \frac{N+1-K + K+1}{(K+1)(N+1-k)} \right]$$ $$=\frac{(N+1)!}{K!(N-K)!} \left[ \frac{N+2}{(K+1)(N+1-k)} \right]$$ Now, using the following identities based on $n! = n \cdot (n-1)!$ : $$(N+1)! (N+2) = (N+2)!$$ $$K!(K+1) = (K+1)!$$ $$(N-k)!(N+1-K) = (N+1-k)!$$ We can now write: $$\frac{(N+1)!}{K!(N-K)!} \left[ \frac{N+2}{(K+1)(N+1-k)} \right] = \frac{(N+2)!}{(K+1)!(N+1-k)!} = C(N+2, K+1)$$ And this is exactly what we were trying to show in Step 3: $$\text{Original Identity: }  $$ $$\sum_{{m=k}}^{N} C(m,k) = C(N+1, k+1)$$ $$\text{Substitute } N=N+1 $$ $$\sum_{{m=k}}^{N+1} C(m,k) = C(N+1+1, k+1) = C(N+2, K+1)$$ Thus, this concludes the proof. Note that Step 3 was not possible to prove without the assumption in Step 2 being a valid assumption (i.e. Step 3 serves to validate the assumption taken in Step 2). Now, here is where my question begins: My prof told me that my work looks correct (the prof didn't do a full review, but looked at it quickly), but I personally don't agree with this proof. In general, I am confused how mathematical induction proofs work in general. Here is my attempt to justify this proof: In Step 1, we proved that this identity true for a specific choice of $N=K$ , e.g. $N=1$ In Step 2, we assumed that this identity is true for any $N$ . The way I see it as that suppose we took the case of $N=5$ , we use $N=1$ as stepping stones to reach $N=5$ But I am confused about Step 3. It seems the main argument is that Step 3 can only be true if our assumption about Step 2 is true. But isn't the $N+1$ case contained within the $N$ case? If we assume this identity is true for the general $N$ case (as we did in Step 2), are we not forced to accept it must also be true for the $N+1$ case? When we say any $N$ , isn't $N+1$ also ""any $N$ ""? Is $N+1$ a general case? Can someone please help me understand how the argument of mathematical induction serves to prove this identity?","['induction', 'solution-verification', 'combinatorics']"
4902336,How can five sticks with coinciding ends be arranged in space such that they are at their maximum angles to each other?,"So the origin of my question is from a model known as VSEPR which helps you predict the shapes of molecules. According to this model, the bonds (or electron groups) arrange themselves in space in such a way that they are at maximum angles to their adjacents and therefore electrons experiencing minimum repulsion. few shapes of molecules So my query arises from the fact that when arranging 5 bonds in space around an atom , due to some asymmetry they aren't actually arranged with maximum angles from each other and hence two bonds are at shorter angles and 3 are at larger. But if I were to arrange them at equal angles, what would the spatial arrangement be and what possibly that angle could be?","['geometry', '3d']"
4902389,A metrizable space is realcompact iff it has non-measurable cardinality?,"A space is realcompact if its a closed subspace of an arbitrary product of real lines, with product topology. A cardinal $\kappa$ is called measurable if there exists a (countably additive) $\{0, 1\}$ -valued measure $\mu:\kappa\to \{0, 1\}$ with $\mu(\kappa) = 1$ and $\mu(\{x\}) = 0$ for $x\in \kappa$ . This is not the standard meaning of what set theorists call a measurable cardinal. Apparently the proper terminology is "" $\sigma$ -measurable cardinal"". Its known that a discrete space is realcompact if and only if it has non-measurable cardinality. The proof of this result basically follows trivially from definitions. Its also known that a metrizable space of non-measurable cardinality must be realcompact. For discussion on above see Rings of continuous functions by Gillman and Jerison. In the same book, they never question if realcompactness is equivalent to non-measurable cardinality also for metric spaces. Is that true? Also see this post for some previous discussion of whetever every metrizable space is realcompact.","['metrizability', 'general-topology', 'realcompact-spaces', 'large-cardinals']"
4902401,$\sigma$-algebra generated by $\mathcal{A} \cup N_{\mu}$.,"Let $(X,\mathcal{A},\mu)$ be a measure space, Let $N_{\mu} = \{B \subset X :
\text{ there is }  C \in \mathcal{A} \text{ such that } B \subset C \text{ and } \mu[C] = 0\}$ and $\mathcal{\bar{A}}$ be the $\sigma$ -algebra generated by $\mathcal{A} \cup N_{\mu}$ . Let $\mathcal{A}_{1} = \{A \cup B : A \in \mathcal{A} \text{  and }  B \in N_{\mu} \}$ $\mathcal{A}_{2} = \{A∆B : A \in \mathcal{A} \text{ and } B \in N_{\mu}\}$ $\mathcal{A}_{3} = \{A - B : A \in \mathcal{A} \text{ and } B \in N_{\mu} \}$ $\mathcal{A}_{4} = \{(A \cup B_{1})-B_{2} : A \in \mathcal{A} \text{ and } B_{1}, B_{2} \in N_{\mu} \}$ Show that $\mathcal{A}_{i} = \mathcal{\bar{A}}$ for $1 \leq i \leq 4$ and $\mu$ has a natural extension $\bar{\mu }$ to $\mathcal{\bar{A}}.$ At first, I showed that $\mathcal{A}_{1}$ is a $\sigma$ -algebra and $\mathcal{A}_{1}=\mathcal{\bar{A}}$ , after that $\mathcal{A}_{1} = \mathcal{A}_{2}$ . I am stuck at proving the remaining equality, $\mathcal{A}_{2}=\mathcal{A}_{3}=\mathcal{A}_{4}$ . For $\mathcal{A}_{2}=\mathcal{A}_{3}$ , I tried to express $A \cup B$ in terms of $E-F$ for some $E \in \mathcal{A}$ and $F \in N_{\mu}$ so that $\mathcal{A}_{1} \subset \mathcal{A}_{3}$ and the other inclusion follows from $\mathcal{A}_{1}$ being a $\sigma$ -algebra. How can I do that? Thank you.",['measure-theory']
4902413,Urn problem with intermediate step,"Suppose there is an urn containing 120 black and 30 white balls. Now, you randomly draw 100 balls (without replacement). Then, you draw 2 balls again (without replacement) from the selected 100 balls. What is the probability that both balls drawn are white? Intuitively, I would say that the intermediate step of selecting the 100 balls should not matter. Thus, I would calculate it as $\frac{30}{150} \times \frac{29}{149}$ . My question would be: How can one formally model the intermediate step with the 100 balls and show that it does not matter?",['probability']
4902425,"If $(a,b,c)$ are the sides of a triangle, what is the probability that $ax+by \ge c$?","Posted in MO since it is unanswered in MSE. Let $(a,b,c)$ be the side of a triangle. In its most general linear form, the triangle inequality can be expressed as: Does $ax + by \ge c$ for fixed $x,y \ge 0$ hold? Trivially the inequality holds if both $x$ and $y$ are $\ge 1$ ; however if one of or both of $x$ and $y$ is non-negative and $\le 1$ then $ax+y \ge c$ is not necessarily true. Assuming that vertices of a triangle are uniformly random on a circle we can ask the probability $P(ax+by \ge c)$ . In this question we found a closed form for the probability $P(a+b \ge cx)$ , $x \ge 1$ . This question is an attempt at generalization of this result. Experimental data show that if $0 \le x,y \le 1$ then, $$
P(ax + by \ge c) = \frac{4}{\pi^2}\chi_2(x) + \frac{4}{\pi^2}\chi_2(y) \tag 1
$$ and if $0 \le x \le 1 \le y$ then, $$
P(ax + by \ge c) = 1 + \frac{4}{\pi^2}\chi_2\left(\frac{x}{y}\right) 
- \frac{4}{\pi^2}\chi_2\left(\frac{1}{y}\right)  \tag 2
$$ where $\displaystyle \chi_2(x) = \sum_{k=0}^{\infty} \frac{x^{2k+1}}{(2k+1)^2}, |x| \le 1$ is the Legendre Chi function . Question 1 : Is $(1)$ true? Question 2 : Is $(2)$ true?","['integration', 'summation', 'geometry', 'inequality', 'probability']"
4902427,General formula for reversing double integral bounds,"The double integral over the region: $$
R = \left\{ \left( x,\: y \right) : a \leqslant x \leqslant b,\: g\left( x \right) \leqslant y \leqslant h\left( x \right) \right\}
$$ is expressed as $$
\iint_R f\left( x,\: y \right) \mathrm{d}A = \int_a^b \left[ \int_{g\left( x \right)}^{h\left( x \right)} f\left( x, \: y \right) \mathrm{d}y \right] \mathrm{d}x.
$$ By Fubini's theorem, any permutation of the order of integration is equivalent if the function $f$ is integrable. Assuming $f$ is integrable, if $g$ and $h$ are invertible on the interval $a \leqslant x \leqslant b$ , is it correct to say that in general, the reversed double integral has the form: $$
\iint_R f\left( x, \: y \right) \mathrm{d}A = \int_{g\left( a \right)}^{h\left( b \right)} \left[ \int_{h^{-1}\left( y \right)}^{g^{-1}\left( y \right)} f\left( x, \: y \right) \mathrm{d}x \right] \mathrm{d}y.
$$ Would there be any other restrictions on the functions $g$ and $h$ ? Thank you for your help.","['integration', 'inverse-function', 'multivariable-calculus', 'multiple-integral', 'fubini-tonelli-theorems']"
4902452,How does one do this translation?,"Good day! I am having a bit of trouble understanding the part where the paper said ""Then a simple translation in $x$ can absorb the linear term $\langle b,x \rangle$ into the quadratic form."" Am I correct in under standing that $f$ , after translation, would become something like $f(x) = (x-x_0)^TA(x-x_0)$ ? I tried doing this translation for $x \in \mathbb{R}^2$ where $x = [x_1 \hspace{4pt} x_2]^T$ , $b = [b_1 \hspace{4pt} b_2]^T$ , $A = \begin{bmatrix} a_1 & a_2 \\ a_3 & a_4 \end{bmatrix}$ . I expanded this and it turned out pretty gnarly in the middle and I cannot do completion of squares anymore. Is there an easier way to do this? Thank you!","['linear-algebra', 'ordinary-differential-equations']"
4902457,Show that $\int_{\mathbb R^d}fd\mu=\sum_{x\in C}f(x)$ for countable $C\subset R^d$,"Problem. Let $C$ be a countable subset of $\mathbb R^d$ and $\mu$ be the counting measure on $\mathbb R^d$ i.e., $$\mu(A)=\#(A\cap C),\qquad A\in \mathcal B(\mathbb R^d).$$ Show that for a measurable $f:\mathbb R^d\rightarrow \mathbb R$ , $$\int_{\mathbb R^d}fd\mu=\sum_{x\in C}f(x),$$ whenever either the integral on the LHS is defined or $$\min\left(\sum_{x\in C}f^+(x),\sum_{x\in C}f^-(x)\right)<\infty.$$ Attempt. For the first 2 cases, I have proved that the result holds. I'm unable to proceed with case 3. Case 4 follows simply from case 3 by writing $f=f^+-f^-$ excluding the case when $\infty-\infty$ is reached. Case 1. $f$ is a simple indicator function i.e., for some set $X\in\mathcal B(\mathbb R^d),~f=\mathbb I_X$ . Case 2. $f$ is a simple, non-negative function. Then, for $X_i\in\mathcal B(\mathbb R^d)$ , $f$ can be written as $\sum_{i=1}^n\alpha_i\mathbb I_{X_i}$ for some $\alpha_i>0$ and $X_i\in\mathcal B(\mathbb R^d)$ . (proof using step 1) Case 3. $f$ is non-negative. Then, we can find non-negative simple functions $f_n$ such that $0\le f_n\uparrow f$ . It follows that $0\le \int f_nd\mu\uparrow\int fd\mu$ . Using step 2, $\int_{\mathbb R^d}f_n d\mu=\sum_{x\in C}f_n(x)$ . We know that the LHS increases to $\int fd\mu$ . Enough to show that RHS increases to $\sum_{x\in C}f(x)$ . How do I proceed for this step? Is my method correct? Case 4. $f$ is non necessarily non-negative.","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'borel-measures']"
4902497,Every TVS is $T_{3.5}$ (Tychonoff) even if it is not $T_0$,"I'm studying the first properties of Topological Vector space, and I'm confused about the separation properties. Is every TVS $T_{3.5}$ even if it is not $T_0$ ? This is confirmed by this wikipedia link, in which it's stated: ""A vector space is an abelian group with respect to the operation of addition, and in a topological vector space the inverse operation is always continuous (since it is the same as multiplication by ${\displaystyle -1}$ ). Hence, every topological vector space is an abelian topological group. Every TVS is completely regular"" https://en.wikipedia.org/wiki/Topological_vector_space#Topological_structure The confusion arises because in every other book it's stated the the $T_0$ property is necessary.","['topological-vector-spaces', 'topological-groups', 'analysis', 'functional-analysis', 'general-topology']"
4902516,Proof of uniqueness of power series representation of function,"My question relates to the theorem and proof given in Erwin Kreyszig's Advanced engineering mathematics: Theorem 1 is as follows: My question is as follows: In the proof of Theorem 2, what is the justification for concluding $a_{m+1}$ = $b_{m+1}$ , given that $a_{m+1}$ = $b_{m+1}$ only holds when z = 0 and the second equation has not been shown to be equivalent to the first at z = 0?","['power-series', 'analysis']"
4902605,When and Where will 2 Random Walks Meet for the First Time?,"This is a question I thought of recently: (Based on some set of initial conditions, i.e. initial positions and movement probabilities , and the current time and positions) When and Where will 2 Random Walks Meet for the First Time? Is there a joint probability function? I found the following questions that discuss related topics, but not something that directly relates to this question : Probability that random walk will reach state $k$ for the first time on step $n$ https://youtu.be/F_kt51Qj1RI?si=zymYfp4EoNSjMoJ7 Probability distribution of time at which 2 random walks meet Intersection of two simple random walks https://mathoverflow.net/questions/54590/intersection-probabilities-for-random-walk-in-d2 Alternate proof for two random walks in $d$ dimension meeting As an example, consider the following two Random Walks (Here, $X(t)$ represents the position of the first Random Walk at time $t$ and $Y(t)$ represents the position of the second Random Walk at time $t$ . $S_x(t)$ and $S_y(t)$ represent the random movement, i.e. outcomes at each point $t$ ): $$
X(t) = 
\begin{cases} 
x_0 & \text{if } t = 0 \\
X(t-1) + S_x(t) & \text{if } t > 0 
\end{cases}
$$ $$
Y(t) = 
\begin{cases} 
y_0 & \text{if } t = 0 \\
Y(t-1) + S_y(t) & \text{if } t > 0 
\end{cases}
$$ $$ x_0, y_0, X(t), Y(t) \in \mathbb{Z} \quad \forall t $$ $$
S_x(t) = 
\begin{cases} 
+1 & \text{with probability } p= p_1 \\
-1 & \text{with probability } p= 1- p_1
\end{cases}
$$ $$
S_y(t) = 
\begin{cases} 
+1 & \text{with probability } p= p_2 \\
-1 & \text{with probability } p= 1- p_2
\end{cases}
$$ Thus, is it possible to derive a probability distribution capable of describing the distribution for the ""number of steps"" (time) and ""position"" where these two Random Walks will intersect based on their initial conditions and current conditions? Can a joint probability distribution function be written? I have watched videos (e.g. https://www.youtube.com/watch?v=iH2kATv49rc ) where they explain that a Random Walk in 1 Dimension (like my question) and 2 Dimensions is guaranteed to visit each possible point (however, this is not true for Random Walks in more than 2 Dimensions). Thus, I would informally conclude that since both Random Walks can occupy any point in the same domain (i.e. set of all integers), it is reasonable to believe that their intersection is theoretically possible. However, just because they have the ability to occupy and point in the same domain, I am not sure that this theoretically guarantees their intersection. For example, suppose both Random Walks start very far from each other, and one of them has higher probability to move left and the other has higher probability move right. In this situation, it seems logical to believe that it will take more time and more steps for these two Random Walks to intersect ... compared to a situation where these two Random Walks started at positions closer to each other and had higher probabilities of moving in the same direction. I also understand that conditional on the current positions of both Random Walks, some intersection times are not possible. For example, if both Random Walks are currently situated very far from each other, an intersection will not be possible in the next time point (a certain minimum amount of time is logically required). I think this will affect the ""Support"" of these intersection distributions. I tried to run multiple simulations in R corresponding to this situation (note: if a given simulation takes more than 100,000 turns, I terminate the simulation. I record the time taken as 100,000 on the time graph, but I don't record the position on the position graph). library(data.table)
library(ggplot2)
library(gridExtra)

x0 <- -2
y0 <- 2

meeting_times <- integer()
meeting_positions <- integer()

for (i in 1:1000) {
    X <- x0
    Y <- y0
    
    t <- 0
    
    while (X != Y && t < 100000) {
        step_X <- sample(c(-1, 1), 1)
        step_Y <- sample(c(-1, 1), 1)
        
    
        X <- X + step_X
        Y <- Y + step_Y
        
        
        t <- t + 1
    }
    

    if (t >= 100000) {
        t <- 100000
    } else {
     
        meeting_positions <- c(meeting_positions, X)
    }
    
 
    meeting_times <- c(meeting_times, t)
}

df_times <- data.table(time = meeting_times)

p1 <- ggplot(df_times, aes(x = time)) +
    geom_density(fill = ""black"", color = ""black"") +
    ggtitle(paste(""Distribution of Time Needed for Random Walks to Meet\nAverage Time ="", round(mean(meeting_times), 2),
                  ""\nAverage Time ="", mean(meeting_times),
                  ""\nNumber of Simulations = 1000"",
                  ""\nStarting Points: X0 ="", x0, "", Y0 ="", y0,
                  ""\nProbabilities: P(X step) = 0.5, P(Y step) = 0.5"")) +
    theme_bw()


df_positions <- data.table(position = meeting_positions)

p2 <- ggplot(df_positions, aes(x = position)) +
    geom_density(fill = ""black"", color = ""black"") +
    ggtitle(paste(""Distribution of Meeting Positions for Random Walks\nAverage Position ="", round(mean(meeting_positions), 2),
                  ""\nNumber of Simulations = 1000"",
                  ""\nStarting Points: X0 ="", x0, "", Y0 ="", y0,
                  ""\nProbabilities: P(X step) = 0.5, P(Y step) = 0.5"")) +
    theme_bw() The outputs of the simulation look like this: And just as an example, I showed an example of the trajectory taken by a given simulation until the intersection occurs: x0 <- -2
y0 <- 2

path_X <- integer()
path_Y <- integer()

X <- x0
Y <- y0

t <- 0

while (X != Y && t < 100000) {

    step_X <- sample(c(-1, 1), 1)
    step_Y <- sample(c(-1, 1), 1)
    
    X <- X + step_X
    Y <- Y + step_Y
    

    path_X <- c(path_X, X)
    path_Y <- c(path_Y, Y)
    

    t <- t + 1
}


df_paths <- data.frame(time = 1:length(path_X), X = path_X, Y = path_Y)

ggplot(df_paths, aes(x = time)) +
    geom_line(aes(y = X, color = ""red"")) +
    geom_line(aes(y = Y, color = ""blue"")) +
    scale_color_manual(values = c(""red"" = ""red"", ""blue"" = ""blue"")) +
    labs(color = ""Random Walk"", x = ""Time"", y = ""Position"") +
    ggtitle(""Paths of Two Random Walks"") +
    theme_bw() And here is the same plot for 1000 simulations: x0 <- -2
y0 <- 2


df_paths <- data.frame()


for (i in 1:1000) {
    path_X <- integer()
    path_Y <- integer()
    print(i)
   
    X <- x0
    Y <- y0
    
    
    t <- 0
    

    while (X != Y && t < 100000) {
       
        step_X <- sample(c(-1, 1), 1)
        step_Y <- sample(c(-1, 1), 1)
        
        
        X <- X + step_X
        Y <- Y + step_Y
        
       
        path_X <- c(path_X, X)
        path_Y <- c(path_Y, Y)
        
        t <- t + 1
    }
    

    df_paths_i <- data.frame(time = 1:length(path_X), X = path_X, Y = path_Y, simulation = rep(i, length(path_X)))
    
  
    df_paths <- rbind(df_paths, df_paths_i)
}

ggplot(df_paths, aes(x = time)) +
    geom_line(aes(y = X, color = ""red""), alpha = 0.1) +
    geom_line(aes(y = Y, color = ""blue""), alpha = 0.1) +
    scale_color_manual(values = c(""red"" = ""red"", ""blue"" = ""blue"")) +
    labs(color = ""Random Walk"", x = ""Time"", y = ""Position"") +
    ggtitle(""Paths of Two Random Walks for 100 Simulations"") +
    theme_bw() But going back to my question: Based on some set of initial conditions $x_0$ , $y_0$ $p_1$ and $p_2$ and their current positions, is it possible to derive probability distributions which describe the number of steps and the time required for intersection? Can a Joint Probability Distribution Function be derived? $$ G : P(T=t | x_0, y_0, p_1, p_2, T-1 = t-1) \sim  ??? $$ $$ H: P(X(t) = Y(t) | x_0, y_0, p_1, p_2, X(t-1) = x(t-1), Y(t-1) = y(t-1) ) \sim  ??? $$ $$ \text{Joint Probability Distribution Function of Intersection Time and Intersection Position: } P(G,H) \sim  ??? $$ Thanks! Note: In the above distributions $T=t$ is a random variable representing the intersection time $X(t-1) = x(t-1)$ and $Y(t-1) = y(t-1)$ are random variables representing the position of both Random Walks at the most recent time","['random-walk', 'probability']"
4902607,The Variational form of a biharmonic PDE,"Suppose $\Omega \subset \mathbb{R}^d$ is a $C^{1,1}$ domain. Consider the biharmonic boundary value problem (BVP): $$
\begin{cases}
\Delta^2 u = f \\
\nabla u \cdot \nu = g \\
u = u_D
\end{cases}
$$ wherein $\Delta^2 u = \Delta\Delta u$ is the application of the Laplace operator twice. (a) Determine appropriate Sobolev spaces within which the functions $u, f, g,$ and $u_D$ should lie, and formulate an appropriate variational problem for the BVP. Show that the two problems are equivalent. (b) Show that there is a unique solution to the variational problem. [Hint: use the Elliptic Regularity Theorem to prove coercivity of the bilinear form.] (c) What would be the natural boundary conditions (BCs) for this partial differential equation? (d) For simplicity, let $u_D$ and $g$ vanish and define the energy functional $
\int \left| \Delta v(x) \right|^2 - 2f(x)v(x) \, dx.
$ It is clear for me that $f \in L^2(\Omega)$ , $u \in H^2(\Omega)$ , $u_D \in H^{2/3}(\Omega) $ and $g \in H^{1/2}(\partial \Omega)$ , I am having a hard time understanding how to construct the variational form for this problem. So we start by integrating against $v \in H^2_0(\Omega)$ , we have $$\int_\Omega \Delta^2 u \, v \,dx = \int_\Omega f\, v \,dx.$$ Now we consider the left hand side and try to get a term that contains $g$ which then we move to the right hand side; \begin{align}
\int_\Omega \Delta^2 u \, v \, dx &= \int_\Omega \nabla \cdot \nabla \Delta u \, v dx\\
&=\int_{\partial \Omega} v \, \nabla \Delta u \cdot \vec{n}\, dS^{d-1}(x) -\int_\Omega \nabla v \cdot \nabla \Delta u \, dx \\
&=\int_{\partial \Omega} v \, \nabla \Delta u \cdot \vec{n}\, dS^{d-1}(x) -\int_{\partial \Omega}(\nabla v) (\Delta u)\cdot \, dS^{d-1}(x)+ \int_\Omega \nabla \nabla \cdot \Delta u \, dx 
\end{align} I can not see how to implement $g$ here? Could you please help!","['integration', 'calculus-of-variations', 'partial-differential-equations', 'regularity-theory-of-pdes', 'boundary-value-problem']"
4902676,Is this identity I found playing around with generating function with coefficients $I(n) = \int_{0}^\pi \sin^n(x) dx$ useful and or reducible?,"Let $I(n) = \int_{0}^\pi \sin^n(x) dx$ , using $\sin^2(x) = 1-\cos^2(x)$ and integrating by parts we get. $$ \begin{align} I(n) = \dfrac{n-1}{n} I(n-2) \end{align}  $$ With $I(0) = \pi$ and $I(1) = 2$ now let $y: \mathbb{R} \to \mathbb{R}$ be the generating function with coefficients $I(n)$ e.g. $$y(x) = I(0) +I(1)x + I(2) x^2 +\cdots$$ Since $nI(n) -(n-1) I(n-2) = 0$ we have, $$ y'(1-x^2) - xy = 2$$ subject to $y(0) = \pi$ and $y'(0) = 2$ This ODE has the solution: $$ y(x) = 2\dfrac{\arcsin(x)}{\sqrt{1-x^2}} + \dfrac{\pi}{\sqrt{1-x^2}} $$ This implies $$ \dfrac{d}{dx} \int y(x) dx = \dfrac{d}{dx} \left[ \arcsin^2(x) +\pi \arcsin(x) \right]$$ since the Taylor series for $\arcsin(x)$ is $$\sum_{n=0}^\infty \dfrac{x^{2n+1}}{2^{2n} (2n+1)} {2n \choose n} $$ so combining the above we get $$ y(x) = 2\sum_{n=0}^\infty\sum_{m=0}^\infty \dfrac{x^{2n + 2m +1}}{2^{2n+2m} (2n+1)} {2n \choose n}{2m \choose m} + \pi \sum_{n=0}^\infty \dfrac{x^{2n}}{2^{2n}} {2n \choose n}  $$ We can rewrite the expression as, $$y(x) = 2\sum_{n=0}^\infty \dfrac{x^{2n+1}}{2^{2n}} \sum_{m=0}^n \dfrac{{2m \choose m} {2n-2m \choose n-m}}{2m+1} + \pi \sum_{n=0}^\infty \dfrac{x^{2n}}{2^{2n}} {2n \choose n}$$ finally by our assumption that the coefficient of $x^n$ is $I(n)$ and using the fact that: $$I(2n+1) = 2\prod_{m =1}^n \dfrac{2m}{2m+1}$$ and $$I(2n) = \pi \prod_{m=1}^n \dfrac{2m-1}{2m}$$ we get $$\prod_{m =1}^n \dfrac{2m}{2m+1} = \dfrac{1}{2^{2n}} \sum_{m=0}^n \dfrac{{2m \choose m} {2n-2m \choose n-m}}{2m+1} $$ This expression just looks funky. I plugged in some values and it does seem to hold. I can't seem to find anything about this expression but maybe it just isn't that useful?","['ordinary-differential-equations', 'reduction-formula', 'binomial-coefficients', 'combinatorics', 'generating-functions']"
4902689,Noether normalization for Laurent polynomial algebras,"Let $k$ be a field and $n\geq 1$ .
Let $A=k[X_1,\dotsc,X_n,X_1^{-1},\dotsc,X_n^{-1}]$ be the algebra of Laurent polynomials in the variables $X_1,\dotsc,X_n$ over $k$ .
Let $B=A/I$ where $I$ is an ideal of $A$ . Is there necessarily a subring $B_0$ of $B$ such that $B$ is integral over $B_0$ , and $B_0$ is isomorphic to $k[Y_1,\dotsc,Y_t,Y_1^{-1},\dotsc,Y_t^{-1}]$ for some $t\geq 0$ ? In other words, I am looking for something like the Noether normalization theorem, but for Laurent polynomial algebras rather than polynomial algeras. Edit (possible strategies): Since $A/I$ is a finitely generated $k$ -algebra, Noether normalization says that $A/I$ is integral over a subring isomorphic to a polynomial algebra over $k$ in $d$ variables. I can prove that $A$ has Krull dimension $n$ , and so $d \leq n$ . Not sure if that helps because I don't see how to extend the polynomial algebra to a Laurent polynomia algebra, and I see no reason for this to be possible in general. Maybe we can consider the subring $A'=k[X_1,\dotsc,X_n]$ of $A$ , and the ideal $I\cap A'$ of $A'$ . Then $B'=A'/I\cap A'$ is integral over a polynomial subalgebra $B'_0$ . We can view $B'$ as a subalgebra of $B$ , and now my idea is to localize by inverting $X_1,\dotsc,X_n$ , but that doesn't seem to bring us to the desired conclusion. Maybe we can imitate the proof of Noether normalization? Say assuming that $k$ is infinite because then the proof is simpler and a linear change of variables works to prove Noether normalization. The proof is inductive, adding on generator of the polynomial algebra at a time. But this is unlikely to work, because a ""generic"" Laurent polynomial is not invertible, while the usual proof of Noether normalization works by showing that a generic element works as the next generator of the polynomia subalgebra. Maybe a geometric intuition would work? Say $k=\mathbb{C}$ (I know that algebraic geometry is more general, but I don't understand it well yet). Well, in that case, $A$ corresponds to the subset $V$ of $\mathbb{A}^n$ of points where no coordinate is $0$ . Now, $A/I$ corresponds to a subset of $V$ . A subring of $A/I$ isomorphic to an algebra of Laurent polynomials will correspond to a map onto an algebraic set similar to $V$ (but possibly of a lower dimension) with finite fibers (finite fibers is not enough for integrality, but I'm just gaining intuition). So I can start with ordinary noether normalization to obtain an integral map to $\mathbb{A}^d$ for some $d$ , and then hopefully tweak things such that no point with a zero coordinate is in the image.","['krull-dimension', 'algebraic-geometry', 'commutative-algebra', 'ideals']"
4902711,Directly computing dimension of $M_P/M_P^2$,"Consider the variety $V_1\colon Y^2=X^3$ in $\mathbb{A}^2$ . By the Jacobian matrix it is easy to see that this variety has a singularity at $P=(0,0)$ . This means that $M_P/M_P^2=\frac{\langle\bar X,\bar Y\rangle}{\langle\bar X^2,\bar Y^2,\bar X\bar Y\rangle}$ considered as a $\bar K$ -vector space must have dimension more than one. $\{\bar X, \bar Y\}$ evidently spans $M_P/M_P^2$ . So I need to show that its elements are linearly independent, i.e. if $a\bar X + b\bar Y=0$ then $a=b=0$ . Given that there are two ideals in the denominator which every term is computer modulo them I'm not sure how I should proceed. The precise thing that I need some help with is how to argue about dimension given both the ideal that determines the variety and $M_P^2$ being identified with zero. Also, I know there are workarounds with doing this computations but I want to compute the dimension directly.","['algebraic-geometry', 'solution-verification']"
4902725,Is there a kind of L'hopital's Rule for multivariable calculus?,"Question: Is there a kind of L'hopital's Rule for multivariable calculus? It is widely known that L'hopital's can be applied for single-variable functions also multivariable functions when one reduces the variable. For example, $$\lim\limits_{(x,y) \to (0,0)} \frac{x^2y + \sin(y)}{y} = \lim\limits_{(x) \to (0)}x^2 + \lim\limits_{(y) \to (0)} \frac{\sin(y)}{y}=1 $$ So, converting a multivariable limit to a single-variable limit provides us to use the rules we have for single-variable limit i.e., L'Hopital's Rule is allowed. Do we have such a theorem or rule to take limit easily for multivariable functions?","['limits', 'calculus']"
4902744,Checking KKT Constraint Qualifications,"When checking whether the CQ are satisfied in KKT, i.e. checking for Linear Independence amongst all combinations of the constraints. Is it true to say we only need to check combinations that could be simultaneously effective (binding)? So if two constraints can't bind at the same time i.e. they would create the empty set, i don't need to check their gradient vectors for Liner Dependence? This seems to be the case but i recently came across the following in a mark scheme, i'm struggling to interpret it, and wondered if it was stating the above but only for mixed constraint problems. For mixed constraint problems, we follow the same approach as for inequality constraint optimisation problems, but we can restrict our attention to just those combinations of effective constraints that include the equality constraints. I would also appreciate some intuition as to why the importance of Linear Dependence in the context of KKT constraints is I feel like the Lagrange/KKT solutions wouldn't know how to allocate tangent points if multiple constraints were LD - somewhat analogous to how regression fails with perfect collinearity of regressors? Thanks! Example Taking the example in the image below: • I would state that the CQ holds for $h_1, h_3$ because if $h_1$ is binding $x = 0$ which implies $y=5$ , which creates vectors $(0,1)$ and $(6,1)$ which are LI. • Their approach is always the other way round. They say for $h_1$ and $h_3$ to be LD, then $x = -1$ this violates the constraint $h_1$ Are both approaches valid? Does my approach of starting with the implications of the constraints being binding run into any problems, compared with starting with the implications of the constraints being LI.","['optimization', 'multivariable-calculus', 'karush-kuhn-tucker', 'constraints']"
4902747,"If $B^3=B$, is $B$ diagonalizable?","Let $B\in M_n(\mathbb{F})$ such that $B^3=B$ . Is $B$ diagonalizable? If $B^3=B$ , then $B^3-B=0$ . Consider the polynomial $p(x)=x^3-x$ . If $p(B)= B^3-B=0$ . Since we know that the minimal polynomial of $B$ must divide any polynomial $g(x)\in \mathbb{F}[x]$ such that $g(B)=0$ , then $m_B(x)|p(x)$ and $p(x)=x(x-1)(x+1)$ . This means that $m_B(x)$ splits and each root of $m_B$ has multiplicity 1. So, $B$ is diagonalizable. Is this correct?","['matrices', 'minimal-polynomials', 'solution-verification', 'linear-algebra', 'diagonalization']"
4902760,A probability involving side lengths of a random triangle on a disk: Is it really $\frac37$?,"Choose three uniformly random points on a disk, and let them be the vertices of a triangle. Call the side lengths, in random order, $a,b,c$ . What is $P(a^2<bc)$ ? A simulation with $10^7$ such random triangles yielded a proportion of $0.4285833\approx1.00003\times\frac{3}{7}$ satisfying $a^2<bc$ , leading me to believe that the probability is $\frac37$ . I find this alleged probability interesting because it seems that the number $7$ rarely appears in the answers to natural geometry or probability questions. Context Recently I have learned that some probabilities related to circles, of the form $P(x^2<yz)$ , have simple rational values (for example, a probability about a random triangle inscribed in a circle, and a probability about a random pentagram inscribed in a circle). So I wondered if there is a probability like this related to a disk. I may have found one. My attempt Let the boundary of the disk be $x^2+y^2=1$ . Using disk point picking , let the three points be $C\left(\sqrt{r_1}\cos\theta_1,\sqrt{r_1}\sin\theta_1\right)$ $B\left(\sqrt{r_2}\cos\theta_2,\sqrt{r_2}\sin\theta_2\right)$ $A\left(\sqrt{r_3},0\right)$ where each $r$ is a uniformly random real number from $0$ to $1$ , and each $\theta$ is a uniformly random real number from $0$ to $2\pi$ . Let $a=BC$ , $b=AC$ , $c=AB$ . This gives: $$P(a^2<bc)=P\left(r_1+r_2-2\sqrt{r_1r_2}\cos(\theta_1-\theta_2)<\sqrt{\left(r_1-2\sqrt{r_1r_3}\cos\theta_1+r_3\right)\left(r_2-2\sqrt{r_2r_3}\cos\theta_2+r_3\right)}\right)$$ I do not know how to set up an integral, nor any other way to calculate the probability. Apparently, after the dust settles, we should be left with $\frac37$ .","['integration', 'conjectures', 'geometric-probability', 'geometry', 'probability']"
4902817,"If $\int_Afd\mu\geq0$ for all $A\in\mathscr{A}$, then $\int f\chi_Ad\mu=0$ for $A=\{x\in X:f(x)<0\}$","I am self-studying measure theory using Measure Theory by Donald Cohn. I am confused by his proof of the following result: Corollary 2.3.13 $\quad$ Let $(X,\mathscr{A},\mu)$ be a measure space, and let $f$ be a $[-\infty,+\infty]$ -valued integrable function on $X$ such that $\int_Afd\mu\geq0$ holds for all $A$ in $\mathscr{A}$ . Then $f\geq0$ holds $\mu$ -almost everywhere. Proof $\quad$ Let $A = \{x \in X:f(x) < 0\}$ . Then $\int f\chi_Ad\mu = \int_Afd\mu = 0$ (since $f<0$ on $A$ , yet we are assuming that $\int_Afd\mu\geq0$ ). $\dots$ I couldn't understand why $\int f\chi_Ad\mu = \int_Afd\mu = 0$ . Basically, this is how I got stuck: Let $A=\{x\in X:f(x)<0\}$ . Then $A\in\mathscr{A}$ because $f$ is integrable which means it must be $\mathscr{A}$ -measurable. We also have that $f\chi_A\leq0$ . Then $\int f\chi_Ad\mu = \sup\left\{\int gd\mu:g\in\mathscr{S}_+\ \text{and}\ g\leq f\chi_A\right\}$ , where $\mathscr{S}_+$ is the set of all nonnegative simple real-valued $\mathscr{A}$ -measurable functions on $X$ . However, shouldn't such $g$ not exist? Or, perhaps more appropriately, if $f(x)\geq0$ for all $x$ in $X$ then $A=\emptyset$ so that $\int f\chi_Ad\mu = \int0d\mu = 0$ ; if $f(x)<0$ for some $x$ in $X$ then there would be no $g$ in $\mathscr{S}_+$ such that $g\leq f\chi_A$ , so that $\int f\chi_Ad\mu = \sup\left\{\int gd\mu:g\in\mathscr{S}_+\ \text{and}\ g\leq f\chi_A\right\} = \sup\{\emptyset\} = -\infty$ . $***************\ *$ So, it seems to me that the assumption $\int_Afd\mu\geq0$ for all $A\in\mathscr{A}$ implies that $f(x)\geq0$ for all $x$ in $X$ . But why? Here are some of my thoughts: I think we need to consider two cases: Suppose $\int_Afd\mu=+\infty$ , which means either $\int(f\chi_A)^+d\mu$ or $\int(f\chi_A)^-d\mu$ or both is not finite. Suppose $\int_Afd\mu\geq0$ and it is finite. However, I don't know how to proceed next. I don't know if any of the above makes sense. I really appreciate it if someone could help me out here! Update $\quad$ I just realized that everything below the "" $***\ *$ "" doesn't really make sense, because the question want us to prove $f\geq0$ holds $\mu$ -almost everywhere. For a construction of the integral in this book, please see this post .","['integration', 'measure-theory', 'analysis', 'real-analysis', 'measurable-functions']"
4902829,Is there a way to show that this large equation approximately simplifies to $d\sin\theta$ under certain conditions?,"I'd like to know what the steps would be to show that $$
\sqrt{\left(\frac{d}{2}\right)^2+\left(\frac{L}{\cos{\theta}}\right)^2+dL\tan{\theta}}-\sqrt{\left(\frac{d}{2}\right)^2+\left(\frac{L}{\cos{\theta}}\right)^2-dL\tan{\theta}}
$$ is approximately equal to $$
d\sin{\theta}
$$ or if that's not possible without certain conditions, then under what conditions, e.g. $d << L$ . In case context helps, the question came up while studying the setup for Young's double slit experiment: It seems standard in textbooks to assert, as above, that the angles of two rays (one from each slit) that meet some distance away can be considered equal, in order to arrive at a simple equation for the difference in length between the two rays. But I was curious what the exact formula was. So I sketched it out and found the length of the two rays depend not only on the distance between the slits $d$ but the distance between the slits and the screen, $L$ : $$
L_1,L_2=\sqrt{\left(\frac{d}{2}\right)^2+\left(\frac{L}{\cos{\theta}}\right)^2\mp dL\tan{\theta}}
$$ I don't imagine this can be simplified any further, so next I wanted to verify that the difference between the two lengths approximately equals $d\sin\theta$ , but I'm not sure how to go about doing that. Ignore unless needed In case I made a mistake, here's the sketch I used. I'm saying ignore this unless necessary because I used different variables, which unfortunately confuses things. The $a$ in the sketch is really $d$ above, and the $d$ in the sketch is what I've called $L$ above. And the $\bar\theta$ in the sketch is the angle of an imaginary ray that begins at the center of the two rays.","['trigonometry', 'approximation']"
4902848,Closed form for $\int_0^{\pi/2}\arctan\left(\frac12\sin x\right)\mathrm dx$?,"Is there a closed form for $I=\int_0^{\pi/2}\arctan\left(\frac12\sin x\right)\mathrm dx$ ? Context Earlier I asked ""Find the area of the region enclosed by $\frac{\sin x}{\sin y}=\frac{\sin x+\sin y}{\sin(x+y)}$ and the $x$ -axis"". The answer turned out to be $\frac{\pi^2}{8}$ , but the proof is non-trivial . A natural follow-up question is, what happens if we change all the sines to tangents? So what is the area of the region enclosed by $\frac{\tan x}{\tan y}=\frac{\tan x+\tan y}{\tan (x+y)}$ and the $y$ -axis, from $y=0$ to $y=\frac{\pi}{2}$ ? To make the algebra easier, we swap $x$ and $y$ , and seek the area enclosed by the new graph and the $x$ -axis. Letting $X=\tan x$ and $Y=\tan y$ , we have $X\left(\frac{X+Y}{1-XY}\right)=XY+Y^2$ . Solving the quadratic in $Y$ gives $y=\arctan\left(\frac12\sin 2x\right)$ . So the area is $\int_0^{\pi/2}\arctan\left(\frac12\sin 2x\right)\mathrm dx$ , which equals $\int_0^{\pi/2}\arctan\left(\frac12\sin x\right)\mathrm dx$ due to symmetry. My attempt I found that $\int_{0}^{\pi/2}\arctan(\sin(x))dx=\frac{\pi^2}{8}-\frac{\ln^2(\sqrt{2}-1)}{2}$ . I tried to use this method , but it doesn't seem to work, because in my integral there is a $\frac12$ , which is not equal to its reciprocal.","['integration', 'calculus', 'definite-integrals', 'closed-form']"
4902883,"When taking the derivative of $8 \log \frac{x}{10}$, do we ignore the factor of $\frac{1}{10}$? Why?","In the midterm of probability in my university, they demand for us to calculate the derivative of $ h(x)=8\times\log_e{(\frac{x}{10})}$ and i find $ h(x)^`=\frac{8}{10x}$ but in the solution they write $ h(x)^`=\frac{8}{x}$ , i repeat the operation a lot of time but i can't find the same result, i know that this derivative was simple but i was really confused! how to find the same result and why we ignore 10 ?
the second question, in the exponential low, we find $\lambda = \frac{1}{8}$ and in the correction of the teacher he write that the $E(x)=\frac{1}{8}$ not 8, i really want to know how!","['derivatives', 'probability']"
4902931,Finding $\left[\left(\frac{A^3-B^3}{B^3}\right)+\left(\frac{A^4+B^4}{B^4}\right)\right]$,"Question: If $A=\displaystyle\int_0^1\left(1-x^{2023}\right)^{\frac 1{2022}}\, dx$ $\quad$ and $\quad$$B=\displaystyle\int_0^1\left(1-x^{2022}\right)^{\frac 1{2023}}\, dx \quad$ then find the value of $\left[\left(\dfrac{A^3-B^3}{B^3}\right)+\left(\dfrac{A^4+B^4}{B^4}\right)\right]$ My attempt Requierd to find $\dfrac{A^3}{B^3}+\dfrac{A^4}{B^4}$ so I am thinking about to use Intgration By Parts to generate this type of reation $A=kB\; ; k\in R$ Let $I(m,n)=\displaystyle\int_0^1\left(1-x^m\right)^{n}$ by IBP I get the reduction formula, $$(1+mn)I(m,n)= \require{cancel}\cancelto{0}{x\big(1-x^m\big)^n\bigg\vert_0^1}+mn\,I(m,n-1)$$ $$
\left(1+\dfrac{2023}{2022}\right)A=\dfrac{2023}{2022}\int_0^1\big(1-x^{2023}\big)^{-\frac{2021}{2022}}\tag{1}$$ $$\left(1+\dfrac{2022}{2023}\right)B=\dfrac{2022}{2023}\int_0^1\left(1-x^{2022}\right)^{-\frac {2022}{2023}}\tag{2}$$ but in both $(1)$ and $(2)$ I could not transform $RHS$ in in terms of $B$ , $A$ and got stuck, Please help.","['integration', 'calculus', 'definite-integrals']"
