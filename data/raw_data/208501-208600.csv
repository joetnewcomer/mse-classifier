question_id,title,body,tags
4180707,"Solve : $(2x - 4y + 5) \frac{dy}{dx} + x - 2y + 3 = 0 , y(2) = 2.5$","I have to find the solution of following differential equation: $$(2x - 4y +  5) \frac{dy}{dx} + x - 2y + 3 = 0 , y(2) = 2.5$$ It can be re-written as $$(2(x - 2y) +  5) \frac{dy}{dx} + (x - 2y) + 3 = 0$$ let $u = (x-2y)$ so, on differentiating both sides with respect to $x$ we get $$\frac{du}{dx} =  1 - 2\cdot\frac{dy}{dx}$$ $$\therefore \frac{dy}{dx} = \frac{1 - \frac{du}{dx}}{2}$$ using this value in initial equation we get, $$(2u + 5) (\frac{1-\frac{du}{dx}}{2}) + u + 3 = 0$$ On further solving we get $$\frac{2u + 5}{4u + 11}.\frac{du}{dx} = 1$$ Integrating both sides with respect to $x$ , we get $$\int \frac{2u + 5}{4u + 11}.du = \int dx$$ $$\implies \int (\frac{1}{2} - \frac{1}{8u + 22})\cdot du = x + C$$ $$\implies \frac{1}{2}\cdot u - \frac{1}{8} \ln(8u + 22) = x + C$$ putting the value of $u$ in the above equation we get, $$\implies \frac{1}{2} (x-2y) - \frac{1}{8} ln(8(x - 2y)+ 22) = 2 + C$$ we have, when $x = 2 , y = 2.5.$ When I put these value of $x$ and $y$ in  the above equation to obtain the value of $C$ $ln(-negative Value)$ is obtained, what is wrong here? The answer given in the book is : $$4x + 8y + ln(4x - 8y + 11) = 28$$",['ordinary-differential-equations']
4180721,The Integral of a Differential 1-form Along a Generic Curve,"Good day! Got stuck again on a Calc-III question, on the subject of differential 1-forms.
I'll present the question and my thought process in trying to solve it up to now. Let $f: [a,b] \rightarrow \mathbb{R}$ be a continuous function of bounded variation such that $f(a) = f(b) = 0$ .
Let $\varphi:= (t, f(t))$ for all $t \in [a,b]$ Prove that: $\int_\varphi (2xy + y)dx + x^2dy = \int_a^b f(x) dx$ First let us change notations a little by introducing - \begin{gather*}        
        F: \mathbb{R}^2 \rightarrow \mathbb{R}^2 \\
        (x, y) \rightarrow (2xy + y, x^2)
\end{gather*} Then, $F \cdot d(x, y)$ is a differential 1-form. It is easily deducible that the we are to prove that $\int_\varphi F \cdot d(x, y) = \int_a^b f(x) dx$ . Now, $f$ is integrable over $[a,b]$ as it is continuous there.
In addition, $\varphi$ is continuous as a collection of continuous functions, and it is also a collection of functions of bounded variation. Thus, $\varphi$ is rectifiable and therefore $F \cdot dx$ is integrable over $\varphi$ as $F$ is also continuous in $\mathbb{R}^2$ let alone in the image of $\varphi$ . As for the actual question at hand, the first thing that came to mind is to show that $F$ is a conservative field (specifically using the 1-form case of Poincare's lemma ). But as it turns out, (or immediately seen if you're used to these kind of processes unlike myself)
the matrix $JF_a$ is asymmetrical for pretty much every $a$ : $\frac{\partial [F]_1}{\partial y}(x) = 2x + 1$ , $\frac{\partial [F]_2}{\partial x}(x) = 2x$ .
As you can see it is almost symmetric but not quite. So in fact $F$ is not a conservative field. Even if we try to restrict its domain, I don't think this will yield anything useful. (I don't think any possibly helpful restriction is star-shaped anyhow) The next thing one might try is to show somehow that $\varphi$ is continuously differentiable but $f$ might very well not be differentiable in it's domain. At a last-ditch effort I tried computing the Rieman's sum of the left side and somehow get the right side, but I got no where with that except maybe a waste of a page. Thanks for reading! Would be grateful if anyone could lend a hint towards the solution. Have a great time for the rest of your day!","['multivariable-calculus', 'line-integrals', 'differential-forms']"
4180736,Let $f$ be a right continuous function then show that it is continuous from $\mathbb{R}_l \to \mathbb{R}$,"The most simplest example of a right continuous function that could come to my mind is $$f(x)= \begin{cases}x &x \le 2 \\  x+1 & x>2\\
\end{cases}$$ This function is discontinuous at the point $x=2$ but it is right continuous .The function is not continuous in $\mathbb{R}$ as if we consider the open set $(1,3)$ then the pre-image of this set is $(1,2] \cup (3,4) $ which is not open in $\mathbb{R}$ . Is the set $(1,2] \cup (3,4)$ open in $\mathbb{R}_l$ ?I am not sure though . This is a question from Munkres and I need to prove this, so may be I am making some mistakes. I think it will be of the form $$(1,2] \cup (3,4) = \cup _{n \ge 2}[1- \frac{1}{n},2) \cup_{n \ge 2} [2,2-\frac{1}{n}) \cup_{n \ge 2} [3-\frac{1}{n},4-\frac{1}{n})$$ Here $\mathbb{R}_l$ denotes the lower limit topology. Is this way of writing it correct? Edit 1:I was thinking of considering two cases : Case 1: when $a \ne f(x_1-)$ and $ b \ne f(x_2-)$ then we consider the open interval $(a,b)$ .So the inverse $f^{-1}(a,b)=(f^{-1}(a),f^{-1}(b))$ Case 2: when $a = f(x_1-)$ then $f^{-1}(a,b)=[x_1,f^{-1}b)$ Case 3: when $b=f(x_2-)$ can also be dealt similarly . Case4: when $a=f(x_1-)$ and $b=f(x_2-)$ (where $x_1 < x_2 $ then $f^{-1}(a,b)=[x_1,x_2)$ . This is how I am thinking about the general proof . Am I in the right path?I would encourage some hints rather than the complete answer.","['continuity', 'general-topology', 'analysis']"
4180760,Doubt in application of Cauchy's Residue Theorem in the proof of Prime Number Theorem,"I have been studying the proof of Prime Number Theorem as outlined in the book Introduction to Analytic Number Theory by Apostol and I came across the following lemma : In the proof of this lemma, the author takes two different contours for $u>1$ and $0<u\leq 1$ respectively and tries to show the required result.
Notice that the function has poles at integers $n = 0,-1,\cdots,-k$ . The case for $u>1$ is a straightforward application of Cauchy's Integral Theorem but I am having trouble understanding the case for $0<u\leq 1$ . Here is the proof, using Cauchy's Residue Theorem, as mentioned in the text: The first equality is pretty clear to me but I just can't understand the second equality. How does the author jump from the first line to the second ? Please help!","['complex-analysis', 'analytic-number-theory']"
4180808,Example of an ODE with an asymptotically stable equilibrium which is unstable in the corresponding linearization,"I am looking for an example of an ODE $x'=f(x)$ with an asymptotically stable equilibrium (WLOG $x=0$ ) such that $0$ is an unstable equilibrium of the linearized equation $x'=Ax$ where $A=Df(0)$ . The strategy is to choose $f$ so that zero is an eigenvalue of $A$ whose algebraic multiplicity exceeds its geometric multiplicity (leading to linearly growing solutions), but whose nonlinear parts promote stability. The example I came up with is $$f:\mathbb R^2\to\mathbb R^2, \quad f(x_1,x_2)=(x_2-x_1^3,-x_2^3),\quad A=Df(0)=\begin{pmatrix} 0 & 1\\ 0 & 0\end{pmatrix}.$$ In the above example, solutions to the linearized problem $x'=Ax$ have the form $x(t)=(c_2 t+c_1,c_2)$ for constants $c_1$ and $c_2$ , so $0$ is an unstable equilibrium. The nonlinear problem, however, has the following phase diagram: This phase diagram seems to strongly suggest that the origin is asymptotically stable in the nonlinear system, but I cannot prove it with, e.g., a Lyapunov function. How can I show the origin is asymptotically stable in this or a similar example?","['stability-in-odes', 'ordinary-differential-equations']"
4180831,What is the correct format for the formula of linear regression?,"Lets say you have $i$ independent variables, ( $y_1$ , $y_2$ , ... $y_i$ ), and each of them have the SAME two predictors, $x_1$ and $x_2$ . I thought that the formula for the linear regression model for each $y$ would be: $$
y_i = \beta_0 + \beta_{i,1}x_1 + \beta_{i,2}x_2 + \epsilon_i
$$ But based on Wikipedia, the formula looks like it would be: $$ y_i = \beta_0 + \beta_1x_{i,1} + \beta_{2}x_{i,2} + \epsilon_i$$ Here is a picture from another site: This formula suggests there are $n*p$ predictor variables with $p$ unique ones for each response variable. Why is my formula incorrect? Shouldn't the coefficients change depending on which response variable I am trying to model?","['regression', 'statistics', 'linear-regression']"
4180887,"$C([0,1])$ with norm $\|f\| := \sum_{n=1}^\infty|\frac{f(q_n)}{2^n}|$ complete?","Let $(q_n)$ be a sequence of all rational numbers from $[0, 1]$ . On $C([0,1])$ let's consider norm: $$\|f\| := \sum_{n=1}^\infty|\frac{f(q_n)}{2^n}|$$ I want to check whether norm $\|f\|$ is complete. My work so far Let's take $(f_k) \subset C([0,1 ])$ Cauchy sequence. We want to prove or disaprove that $$\exists{f \in C([0, 1])}: \|f_k - f\|\rightarrow 0 $$ I wanted first to focus on more general problem: $$\exists f: \|f_k - f\| \rightarrow 0$$ and after proving that, we can focus on showing that such $f$ has to be in $C([0, 1])$ . So - because $(f_k)$ is a Cauchy Sequence then: $$\exists_l : \|f_k  - f_l\| = \sum_{n=1}^\infty \frac{|f_k(q_n) - f_l(q_n)|}{2^n} \le \epsilon$$ for any $\epsilon > 0$ $$\|f_k  - f\| = \sum_{n=1}^\infty \frac{|f_k(q_n) - f(q_n)|}{2^n}= \sum_{n=1}^\infty \frac{|f_k(q_n) - f_l(q_n) + f_l(q_n) - f(q_n)|}{2^n} \le$$ $$\le \sum_{n=1}^\infty \frac{|f_k(q_n) - f_l(q_n)|}{2^n} + \frac{|f_l(q_n) - f(q_n)|}{2^n} \le$$ $$\le \sum_{n=1}^\infty \frac{\epsilon}{2^n} + \frac{\sup\{|f_l(q_n) - f(q_n)| : n \in \mathbb{N} \}}{2^n} = \epsilon + \sup\{|f_l(q_n) - f(q_n)| : n \in \mathbb{N} \}$$ . And here I got stuck, because $\sup\{|f_l(q_n) - f(q_n)| : n \in \mathbb{N} \}$ is not arbitrary small. Do you know how to fix this/could you please give me a hint what is correct approach to this problem","['convergence-divergence', 'normed-spaces', 'functional-analysis', 'real-analysis']"
4180937,Entire analytic automorphisms of C,"I was looking at a proof in Serge Lang's Introduction To Complex Analysis at a graduate-level regarding the form of analytic automorphisms which are entire functions I have a question about one of the steps in the proof. I have trouble understanding how the assertion "" $f$ is analytic automorphism of $\mathbf{C}$ implies that there exist $\delta, c> 0$ such that if $|w|>1\ \delta$ then $|f(w)>c|$ "" is made.
If anybody could help me understand that, I would really appreciate it.",['complex-analysis']
4180968,Example of a dynamical system,"Consider a dynamical system defined by a non-autonomous ordinary differential equation $$ \dot z = X(z,t)$$ where $X$ is periodic with respect to time of period $T > 0$ . Consider the flow evaluated at $T$ , i.e., the map $\Phi^X_{T,0} : M \to M$ , where $M$ is a manifold (but let us take an open subset of $\mathbb R^n$ ) and $\Phi^X_{T,0}$ is the map that assigns to each $z \in M$ the value at time $T$ of the solution that starts from $z$ at time $0$ . This map can be interpreted as a Poincaré map in the extended quotient phase space $M \times S^1_T$ , $S^1_T$ being the one dimensional circle of length $T$ . Let us suppose that $\Phi^X_{T,0}$ has a fixed point $\bar z$ . I would like to find an explicit example of $X$ such that $\bar z$ is an equilibrium of the system. I have found the sufficient condition $X(\bar z,t)=0$ for every $t$ , but I am still missing the explicit example. Can someone help me?","['examples-counterexamples', 'ordinary-differential-equations', 'dynamical-systems']"
4180970,Show derivative function is bounded to prove uniform continuity,"I've been trying to solve this question: Let $f(x) = \frac {x}{x+sin x}$ , prove that $f$ is uniformly continuous in $(0,\infty)$ I've been trying to prove that $f'$ is bounded in $(0,\infty)$ , but reached: $f'(x)= \frac {sin x - xcosx}{(x+sin x)^2}$ I'm not sure how to go on from here to show that $f'$ is truly bounded. Would appreciate any help! Thanks!","['uniform-continuity', 'continuity', 'calculus', 'limits', 'derivatives']"
4180993,Regarding radius of convergence of power series.,"This is a question from a past exam. Let $f(z) = \frac{1}{1+z^2}$ . Find the power series representation of $f$ centered at $1$ and its radius of convergence. My solution: Notice that (it was given as a hint) $$
f(z)=\frac{i}{2}\left[\frac{1}{1+i} \cdot \frac{1}{\left(1+\frac{z-1}{1+i}\right)}-\frac{1}{1-i}\cdot \frac{1}{\left(1+\frac{z-1}{1-i}\right)}\right]
$$ Then I expanded the fractions using the geometric series. To find the radius of convergence in this particular case is easy. First, notice that for $|z-1| < |1+i|=\sqrt{2}$ the series converges to $f(z)$ so the radius $R$ must be greater than or equal to $\sqrt{2}$ . Next notice that the distance of $1$ from the singularities at $\pm i$ should be greater than or equal to $R$ so $R \leq \sqrt{2}$ . Putting it all together we get $R = \sqrt{2}$ . First of all, is my solution correct? If yes then I wonder how to solve such problems if we can't get both inequalities. Calculating the $\limsup$ of the coefficients can be quite difficult so what is a good way to approach them? Is it to try and write $f$ as the sum of terms of the form $$\frac{1}{1 \pm \frac{z-a}{a-z_k}}$$ where $z_k$ are the singularities of $f$ (where $a$ denotes the center)? Thanks in advance.","['complex-analysis', 'power-series']"
4181000,"Prove a subset of $C[-1,1]$ is a closed subspace.","Consider $C[-1,1]$ under the sup-norm. Let $$X= \{f\in C[-1,1]:\int_{-1}^{0}f=\int_{0}^{1}f=0\},$$ Prove $X$ is a closed subspace of $C[-1,1]$ My attempt: It is trivial to prove $X$ is a subspace. To prove $X$ is closed: consider a sequence of function $\{f_k\}$ in $X$ that converges to $f$ in $C[-1,1]$ . Then $\forall \epsilon>0,\exists N\in \mathbb{N}$ such that $$\lVert f_n-f_m\rVert_{\infty}\leq\epsilon$$ for $n,m\geq N$ , and $$\lim_{n\rightarrow\infty}\lVert f_n-f_m\rVert_{\infty}=\lVert\lim_{n\rightarrow\infty}f_n-f_m\rVert_{\infty}=\lVert f-f_m\rVert_{\infty}\leq\epsilon$$ by continuity of the sup-norm. By the property of $X$ : $$\int_{-1}^{0}f-f_m\leq\int_{-1}^{0}\vert f-f_m \vert\leq \int_{-1}^{0}\lVert f-f_m\rVert_{\infty}\leq\epsilon$$ Then $-\epsilon\leq\int_{-1}^{0}f\leq\epsilon$ since $\int_{-1}^{0}f_m=0$ . We conclude $\int_{-1}^{0}f=0 $ and similarly $\int_{0}^{1}f=0$ . As $\{f_k\}$ are continuous functions uniformly converges to $f$ , $f$ is continuous. Therefore, $f\in X$ and $X$ is a closed subspace in $C[-1,1]$ . Thanks in advance.","['continuity', 'solution-verification', 'functional-analysis']"
4181022,"Total differentiation, is this true: $D(Df(a))(a) = f$?","Let $f: \mathbb{R}^p \rightarrow \mathbb{R}^q$ be linear. We have proven that for $T$ linear it is: $$
D T(a) = T.
$$ So it should imply that: $$
D(Df(a))(a) = f.
$$ Right? This seems to be trivial, but I had a long discussion with a fellow student who keeps saying that it is $0$ and also, that considering such examples is absolutely irrelevant in mathematical praxis. So who is wrong...?","['derivatives', 'real-analysis']"
4181023,An inequality using column sums of inverse matrices,"I want to prove a matrix analogue to inequality $\left(\frac{1-x(1-\alpha)}{\alpha}\right)^{\alpha} x^{1-\alpha}$ for $\alpha \in [0,1)$ and $x \in [0,1]$ , which has a nice proof using GM-AM, as shown in https://math.stackexchange.com/a/4176762/165163 . Let $A=\{a_{ij}\}$ be an $N \times N$ strictly substochastic matrix, $\alpha_i \equiv 1 - \sum_j a_{ij} $ , $x$ be a scalar, and $f(x):\mathbb{R} \rightarrow \mathbb{R}$ be a function defined by $$f(x) = \prod_{i}\left(\frac{\sum_{l}w_{li}(1)}{\sum_{l}w_{li}(x)}\right)^{\frac{\sum_{j}w_{ji}\left(1\right)\alpha_{i}}{\bar{w}}}x^{\frac{\bar{w}-N}{\bar{w}}},$$ where $w_{ij}(x)$ are the elements of matrix $W(x) \equiv \left(I-xA\right)^{-1}$ and $\bar{w}\equiv\sum_{i,j}w_{ij}$ . I want to show that $f(x) \leq 1$ for all $x\in[0,1]$ . It is clear that $f(1) = 1$ and that $f(0) = 0.$ Also, note that $\sum_i w_{ji}(1) \alpha_i = 1$ for all $j$ , which follows from $$ \{ \sum_i w_{ji}(1) \alpha_i \} = W(1)\left(I-\mathcal{D}\left\{ A\iota\right\} \right)\iota=W(1)\left(I-A\right)\iota=\left(I-A\right)^{-1}\left(I-A\right)\iota=\iota.$$ This implies that the sum of the powers in the terms above is 1, $$\sum_i \frac{\sum_j w_{ji}(1)\alpha_i}{\bar{w}} + \frac{\bar{w}-N}{\bar{w}} = 1.$$ Thus, we can think of $f(x)$ as a geometric mean. But the arithmetic mean corresponding to this geometric mean, namely $$ \sum_{i} \left(\frac{\sum_{j}w_{ji}\left(1\right)\alpha_{i}}{\bar{w}}\right) \left(\frac{\sum_{l}w_{li}(1)}{\sum_{l}w_{li}(x)}\right) + \left(\frac{\bar{w}-N}{\bar{w}} \right) x $$ does not equal one, so the GM-AM approach didn't work immediately in this case, except in the case in which $A$ is diagonal, where the expression just below collapses to $$ \sum_{i} \left(\frac{1}{\bar{w}}\right) \left(\frac{1-x (1-\alpha_i)}{\alpha_i}\right) + \left(\frac{\bar{w}-N}{\bar{w}} \right) x =1, $$ and so the desired inequality follows from the GM-AM. Simulations suggest that the inequality holds -- in fact, they suggest that $f(x)$ is always increasing. I could try to show $f'(x) \geq 0$ but this would limit how much I can then generalize, for example to having $x$ be a vector, in which case $f(x):\mathbb{R}^N \rightarrow \mathbb{R}$ is given by $$f(x)= \prod_{i}\left(\frac{\sum_{l}w_{li}(1)}{\sum_{l}w_{li}(x)}\right)^{\frac{\sum_{j}w_{ji}\left(1\right)\alpha_{i}}{\bar{w}}} \prod_i x_i^{\frac{\sum_j w_{ij}-N}{\bar{w}}},$$ where $w_{ij}(x)$ are now the elements of matrix $\left(I-A \mathrm{diag}(x)\right)^{-1}$ Any suggestions would be much appreciated.","['matrices', 'algebra-precalculus', 'a.m.-g.m.-inequality']"
4181025,PCA for retrieval from absolute values,"I have given $(x_i)_{i=1}^n$ $d$ -dimensional iid. random variables with $x_i\sim\mathcal N(0,I_d)$ and $y_i=|\langle x_i,\theta\rangle|$ with $\theta\in\mathbb R^d$ . First I have to assume that $\|\theta\|_2=1$ and define $\hat\theta$ as the maximal eigenvector of the matrix $\frac1n\sum_{i=1}^n y_i^2 x_i x_i^T$ . Now I have to prove that $\hat\theta$ is a consistent estimator of $\theta$ . My ideas: That means I have to show that $\lim\limits_{n\to\infty} P(\|\hat\theta-\theta\|_2\geq\varepsilon)=0\quad \forall\theta\in\mathbb R^d$ . I guess the first thing to do is to find a explicit representation of $\hat\theta$ depending on $n$ . Maybe the min-max theorem could be helpfull. In the second part I have to provide an estimator for arbitrary $\theta\in\mathbb R^d$ based on the $\hat\theta$ from above and show its consistency. For this case there was also a hint to construct a random matrix $Z$ for a pair $(x,y)$ such that $E[Z] =\sqrt\frac2\pi(\theta^*\otimes\theta^*+I_d)$ . The Exercise can similarly be found on page 257 in Wainwright , Exercise 8.7. I hope someone can help me. Greetings","['statistics', 'parameter-estimation', 'normal-distribution', 'random-matrices', 'random-variables']"
4181065,Question about the definition of feature map arising in machine learning,"I'm working through the following paper of learning a non-negative function in a reproducing kernel hilbert space setting (RKHS). In particular, section 2.2 on page 3 is a bit confusing to me in terms of the feature map. We are given a kernel $K:X\times X\to\mathbb{R}$ and we denote with $\mathcal{H}$ the corresponding RKHS of $K$ . Usually, one denotes the feature map by $$k_x:=K(\cdot, x):X\to \mathcal{H}$$ with the property then that $K(x,y) = \langle k_y, k_x\rangle$ . A function $\phi$ satisfying the property $$ K(x,y) = \langle \phi(y), \phi(x)\rangle $$ is often called the feature map and the function $k_x$ the canonical feature map. It is well known that such a feature map is not unique. The idea of the paper is to look at sum of square functions. This is strongly motivated from polynomials. In the polynomial case one says the polynomial $p(x)$ has a sum of square (SoS) representation if there are other polynomials $q_i$ such that $$ p(x) = \sum_{i=1}^d q_i(x)^2$$ If we denote the vector of monomials up to degree $d$ by $\phi(x)=(1,x, \dots,x^d)^T$ one can write $q(x)=(q_1(x),\dots,q_d(x))^T$ as $$q(x)=V\phi(x)$$ where the rows of $V$ contain the coefficient of the polynomials $q_i$ . For univariate polynimoials its well known that being positive and having such a SoS representation is equivalent. In the above context one can then write $$p(x)=\sum_i^dq_i(x)^2=q(x)^Tq(x) = \phi(x)^TV^TV\phi(x)$$ with $Q:=V^TV$ being pos semidefinite. Even for generic feature map $\phi$ (not necessary monomials) the condition on the pos. semidefiniteness of $Q$ leads to a non-negative function $p(x)$ . In the paper they try to generalize the above idea. For a kenerl $K:X\times X\to\mathbb{R}$ they define an associated feature map via $\phi:X\to\mathcal{H}$ , where $\phi(x):=(\phi_i(x))_{i\in I}$ for $\phi_i:X\to\mathbb{R}$ and a index set $I$ , e.g $\{1, 2, 3,\dots\}$ .
They define the space of sum of squares as $$\mathcal{S}:=\{x\mapsto \phi^T(x)Q\phi(x): Q\ge 0\}\tag{1}$$ where $Q\ge 0$ means a positive semidefinite matrix $Q$ . Couple of question to these definitions Why is such a $\phi$ a feature map? It seems to me that $\phi$ is a tuple of real valued functions while $K$ is scalar valued. That doesn't make much sense to me. In the polynomial motivation $\phi_i$ where basically a basis to represent $q$ . But I don't see how this translate to the general one. They allow for countably infinite index set. How does this make sense in $(1)$ ? They claim that $\mathcal{S}$ is a subspace of the Hilbert Space $\mathcal{H}\otimes\mathcal{H}$ . Why can $\mathcal{H}\otimes\mathcal{H}$ be represented as $$\{x\mapsto \sum_{i,j} Q_{i,j}\phi_i(x)\phi_j(x):Q\in\mathbb{R}^{d\times d}\} $$","['tensor-products', 'machine-learning', 'functional-analysis', 'reproducing-kernel-hilbert-spaces']"
4181124,Derivative of trace of a matrix function [$\operatorname{Tr}(X\log(Y))$] w.r.t. a scalar,"$\DeclareMathOperator{\Tr}{Tr}$ I'm trying to find a closed form for $\frac{\partial}{\partial \theta}\Tr(X\log(Y))$ where $X(\theta)$ and $Y(\theta)$ are Hermitian positive definite matrices with trace 1 (i.e. full rank density matrices), parametrized by scalar $\theta$ , that in general don't commute. If it was $\frac{\partial}{\partial \theta}\Tr(X\log(X))$ , I could write down the Taylor expansion of the log and using the cyclic property of the trace rearrange all the terms coming from differentiating $X^n$ and pretend like I was doing single variable calculus; which would give me $\Tr(X'\log(X) - X')$ , where $X'\equiv\frac{\partial X}{\partial \theta}$ . However, if I were to apply the same approach to $\frac{\partial}{\partial \theta}\Tr(X\log(Y))$ , the Taylor expansion gives me a sum of terms like $\Tr(X(Y-1)^n)$ (here $1$ is the identity matrix of the same dimension as $X$ & $Y$ ): $$
\Tr(X\log(Y)) = \sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n}\Tr(X(Y-1)^n)
$$ When differentiated, the trace in the $n$ -th term produces $$
\Tr(X'(Y-1)^n)+\Tr(XY'(Y-1)^{n-1})+\dots+\Tr(X(Y-1)^k\,Y'\,(Y-1)^{n-k-1})+\dots+\Tr(X(Y-1)^{n-1}Y')
$$ The first term can be separated to give $\Tr(X'\log(Y))$ . However, the rest of the terms can't be rearranged in a nice way since $X$ breaks the sort of cyclic symmetry we had in the previous case. This is the point I'm stuck at; is there a different approach with which I can manipulate this expression to obtain a closed form derivative? Thanks in advance. Context: I'm trying to differentiate von Neumann relative entropy in quantum mechanics/information.","['entropy', 'logarithms', 'matrices', 'matrix-calculus', 'derivatives']"
4181131,"Is map of ""deltas"" to points on closed interval continuous?","I am having trouble properly coming up with an answer to the following: Let $f: [0, 1] \to \mathbb{R}$ be a continuous function, using the usual (Cauchy) characterization of continuity. Define the set $D_{\epsilon} (x) = \{d \in \mathbb{R}: \forall x' \in [0, 1]:|x'-x| < d \implies |f(x')-f(x)| < \epsilon\}$ for all $x \in [0,1]$ , and let $\delta(x) = \sup{D_{\epsilon}}$ . Does the fact that $f(x)$ is continuous on $[0,1]$ imply that the map $\delta(x)$ is continuous too? I have tried to derive a contradiction, by showing that if some points in the interval are close enough, but their $\delta$ differs by a finite amount, one of the $\delta$ is not actually the supremum of the set D for one of them, but I didn't manage to take my argument too far. Any help is much appreciated!","['epsilon-delta', 'analysis', 'real-analysis', 'continuity', 'functions']"
4181132,"The integral in ""The Number 23"" movie; What is it?","I was watching a movie called The Number 23 and saw this mathematical expression, I was wondering if it has any meaning and if it is, What does it mean? Here you can -Or Can't- see the expression It's not very clear, but I Think it's written like this: $$\triangledown x G=\int_{0}^{1} t\triangledown x(px\frac{\mathrm{d} r}{\mathrm{d} t})dt $$ Any ideas?","['calculus', 'ordinary-differential-equations', 'probability']"
4181146,Elementary tensors of pure states separate the points of the spatial tensor product,"Let $A,B$ be two $C^*$ -algebras. Given two states $\phi\in S(A),\psi\in S(B)$ one defines the elementary tensor state $\phi\odot\psi:A\odot B\to\mathbb{C}$ as the unique linear map satisfying $\phi\odot\psi(a\otimes b)=\phi(a)\psi(b)$ on elementary tensors. One proves with elementary arguments that all such functionals are algebraically positive and continuous with respect to the spatial norm (and by Takesaki's theorem w.r.t any $C^*$ -norm on $A\odot B$ ). Denote the extension of $\phi\odot\psi$ on a state on $A\otimes B$ (the spatial TP) by $\phi\otimes\psi\in S(A\otimes B)$ . Let $0\neq x\in A\otimes B$ . It is easily proven that there exist states $\phi\in S(A)$ and $\psi\in S(B)$ so that $\phi\otimes\psi(x^*x)>0$ . Here is a simple argument: As seen in Murphy, we have that $$\|x^*x\|_{\min}=\sup_{\tau\in S(A),\rho\in S(B)}\|\pi_\tau\otimes\pi_\rho(x^*x)\|_{B(H_\tau\otimes H_\rho)}$$ where $(H_\tau,\pi_\tau,\xi_\tau)$ is the GNS triplet. So we find a pair of states $\tau,\rho$ such that $\|\pi_\tau\otimes\pi_\rho((x^*x)^{1/2})\|\neq0$ , i.e. $\pi_\tau\otimes\pi_\rho((x^*x)^{1/2})\neq0$ . This thing here is an operator on $H_\tau\otimes H_\rho$ , i.e. $\pi_\tau\otimes\pi_\rho((x^*x)^{1/2})\in B(H_\tau\otimes H_\rho)$ , and as it is non-zero, we can find unit vectors $\xi_1\in H_\tau,\xi_2\in H_\rho$ such that $\pi_\tau\otimes\pi_\rho((x^*x)^{1/2})(\xi_1\otimes\xi_2)\neq0$ . Simply set $\phi(a)=\langle\pi_\tau(a)\xi_1,\xi_1\rangle$ and $\psi(b)=\langle\pi_\rho(b)\xi_2,\xi_2\rangle$ . These are obviously states on $A$ and $B$ , and $\phi\otimes\psi(x^*x)\neq0$ . Here is my question : apparently it is also true that the elementary tensors of pure states separate the positive elements, i.e. given $0\neq x\in (A\otimes B)_+$ , we can find pure states $\phi\in\text{PS}(A),\psi\in\text{PS}(B)$ such that $\phi\otimes\psi(x)\neq0$ . How can this be proved, or where can I find a proof? Here are the things I tried With elementary arguments as those presented in Murphy's book, I was able to show that we also have $$\|x\|_{\min}=\sup_{\tau\in\text{PS}(A),\rho\in\text{PS}(B)}\|\pi_\tau\otimes\pi_\rho(x)\|_{B(H_\tau\otimes H_\rho)}$$ the proof is the same but one has to observe first that the direct sum of the GNS representations for pure states $\bigoplus_{\tau\in\text{PS}(A)}\pi_\tau:A\to\mathbb{B}(\bigoplus_{\tau\in\text{PS}(A)}H_\tau)$ are faithful. The thing is that, if I try to mimic the above proof, the rest of the proof gets messed up: If I knew that the vectors $\xi_1,\xi_2$ found earlier are the canonical cyclic vectors, then everything would work out like a charm since the resulting states $\phi,\psi$ would actually be $\tau$ and $\rho$ which would be pure this time. But of course this is too much to expect I guess. Also, one can show (similarly to the other equations) that $$\|x^*x\|_{\min}=\sup\bigg\{\frac{\tau\otimes\rho(y^*x^*xy)}{\tau\otimes\rho(y^*y)}:\tau\in\text{PS}(A),\rho\in\text{PS}(B),y\in A\odot B\text{ s.t. }\tau\otimes\rho(y^*y)\neq0\bigg\}.$$ So we could start like this: $x^*x\neq0$ , so we get pure states $\tau,\rho$ and some $y\in A\odot B$ such that $\tau\otimes\rho(y^*x^*xy)\neq0$ . And everything would be perfect if there wasn't this $y$ appearing here. Another approach would be this product states on the tensor product *-algebra using the so-called slice map, but I want to consider elements of the entire $C^*$ -tensor product and not just the algebraic TP. That means that (among many other things) I'd have to extend the slice map to the spatial TP and I don't see this happening at the moment. edit: of course slice maps extend to the spatial tp, this is a simple corollary of the fact that the TP of two completely positive maps is again c.p.","['c-star-algebras', 'functional-analysis', 'operator-algebras']"
4181149,Equivalence of Axiom of Regularity,"So Axiom of regularity states:  every non-empty set A contains an element that is disjoint from A I'm wondering if this is equivalent as any set is not a member of itself? If so, how do we prove it? If not, I am wondering is knowing any set is not a member of itself sufficient to derive that a∈b and b∈a cannot be both true?","['axioms', 'set-theory']"
4181188,Equivalent condition for the existence of left hand limit.,"Let $f : [a,b] \longrightarrow \mathbb R$ be a function. Let $c \in (a,b].$ Then I know that $\lim\limits_{x \to c^-} f(x) = l$ iff for every sequence $\{x_n\}_{n \geq 1}$ converging to $c$ from below we have $f(x_n) \to l.$ Now my question is $:$ Instead of taking any sequence $\{x_n\}_{n \geq 1}$ converging to $c$ from below if we take any sequence $\{x_n \}_{n \geq 1}$ increases to $c$ do we have the same conclusion as above i.e. can it still be said that $\lim\limits_{x \to c^-} f(x) = l\ $ ? In order to show that it is true we need to show that if for any sequence $\{x_n \}_{n \geq 1}$ increases to $c$ the sequence $\{f(x_n)\}_{n \geq 1}$ converges to $l$ then for any sequence $\{x_n\}_{n \geq 1}$ converging to $c$ from below the sequence $\{f(x_n)\}_{n \geq 1}$ also converges to $l.$ But I don't know how to show that. Any suggestion in this regard will be appreciated. Thanks!","['limits', 'sequences-and-series', 'real-analysis']"
4181201,How to evaluate $\frac{\partial}{\partial x}\Gamma(f(x))$?,"If my math is correct: $$\Gamma(f(x))=\displaystyle \int_0^\infty e^{-t}t^{f(x)-1}dt$$ $$\frac{\partial}{\partial x}\Gamma(f(x))=\displaystyle \int_0^\infty e^{-t}f^\prime (x)\textrm{ln}(t)t^{f(x)-1}dt$$ $$=f^\prime (x)\displaystyle \int_0^\infty e^{-t}\textrm{ln}(t)t^{f(x)-1}dt$$ But here I get stuck. The integral is tantalizingly close to the definition of $\Gamma(f(x))$ , except for the $\textrm{ln}(t)$ term. I've tried integration by parts to resolve the integral but no luck so far. I'm hoping to find a formula for $\frac{\partial}{\partial x}\Gamma(f(x))$ that can be expressed from the terms: $\Gamma(f(x))$ , $\Gamma^\prime(x)$ , or $\Gamma(x)$ and therefore can be calculated in the R program I'm running.","['integration', 'derivatives', 'gamma-function']"
4181212,Proving a specific matrix is positive definite.,"I am trying to prove the following matrix is positive definite: Let $A$ be a $n\times n$ real positive definite matrix, so it holds that $A = UDU^T$ where $$D=diag(d_{11},d_{22},\ldots,d_{nn}) > 0$$ For any $n\times1$ vector $w$ , let $q = U^Tw$ , $q$ is also $n\times1$ vector, so define the matrix $$B = Aww^TA + (w^TAw)A - 2UD^*U^T$$ where $D^*$ is a diagnoal matrix and $\{D^*\}_{ii} = (d_{ii}q_{i})^2$ . I want to prove $B$ is positive definite. I used that $A = UDU^T$ to rewrite $$B = UDqq^TDU - UD^*U^T + U(D\cdot(q^TDq))U^T - UD^*U^T$$ The eigenvector $U$ doesn't affect positive definite so I omitted it. The first part $Dqq^TD-D^*$ : provides non-diagonal elements. The second part $D\cdot(q^TDq) - D^*$ provides diagonal elements. Than I need to prove all eigenvalues of $Dqq^TD-D^* + D\cdot(q^TDq) - D^*$ are positive and I'm stuck at this point. Am I wrong? Does $B$ really positive definite ? (I think it's because I tryed a lot of simulations with no conflicts) Thank you so much for any suggestions.","['matrices', 'linear-algebra', 'positive-definite']"
4181235,$\mathcal D$ is the space of compactly supported smooth functions on $\mathbb R^n$. Show $\mathcal D'$ is complete with respect to weak$^*$-topology.,"The topology on $\mathcal D$ is the locally convex topology induced by seminorms like $\|\partial_\alpha f\|_\infty$ , where $\alpha=(d_1,\ldots,d_n) $ and $\partial_\alpha=\partial_{x_1}^{d_1}\ldots\partial_{x_n}^{d_n}$ . $\mathcal D'$ is its continuous dual space. The weak^ $^*$ topology on $\mathcal D'$ is induced by seminorms like $\|\phi\|_f=|\phi(f)|$ , where $f\in \mathcal D$ . It is easy to define the limit of a Cauchy net on $\mathcal D'$ : $\Phi(f)=\lim_{\lambda}\phi(f)$ . However, I don't know how to show $\Phi$ is continuous, that is, $\Phi(f_\eta)\to 0$ whenever $\|\partial_\alpha f_\eta\|\to 0$ for all $\alpha\in \mathbb N^n$ . Could someone please give me some hints?","['fourier-analysis', 'functional-analysis']"
4181244,A nonsingular plane curve of degree 5 has no linear system of dimension 1 and degree 3,"Let $C$ be a nonsingular plane curve of degree 5 over $\mathbb{C}$ , as in the question, I want to show that $C$ has no linear system of dimension 1 and degree 3, that is has no $g^{1}_{3}$ . First, note $g(C)=6$ . Suppose $C$ has $g_{3}^{1}$ , then there exists an effective divsior $D$ , such that $\mathrm{deg}(D)=3$ and $\mathrm{dim} |D|\geq1$ . By Riemann-Roch, we have $\mathrm{dim}|D|-\mathrm{dim}|K-D|=-2$ . Thus $\mathrm{dim}|K-D|=\mathrm{dim}|D|+2\geq 3$ . But then how to deduce this is impossible? Note here actually I did not use that $C$ is a plane curve. This condition should be necassary. Could you help me?","['algebraic-geometry', 'projective-varieties']"
4181269,How fast would I need to climb to keep the setting sun at the horizon?,"(I originally asked this question over on Aviation Stack Exchange, only for it to be closed as off-topic. I was told this was a better place to ask.) The title is basically the whole question. If I wanted to see the same sunset twice, how fast would I need to climb? I'm currently at about 32.5 degrees North latitude, and would be starting my climb from about 2,000 feet above MSL , but what I'm really interested in is some equation or function that I could use no matter where I am. (Note: Although I'm American, I tend to prefer the metric system. However, aviation in general is stuck using feet to measure altitude around the world (with very few exceptions), hence the altitude in feet above. So I guess what I'm saying is, use whichever system you prefer, and I can convert to the other when necessary.) Edit: To simplify the question, let's just assume I start climbing the moment the exact center of the sun reaches the horizon. What vertical speed would I need to keep the sun in that position relative to the horizon? And then I know that I just need to exceed that speed to ""see the sunset again"". I would be flying West at about 73 KIAS* (which, assuming standard temperature, would be about 76 knots true, or 39 m/s), so I don't know that my horizontal speed is going to affect the answer all that much. *That's the maximum rate-of-climb airspeed for the typical plane I rent (Cessna 172 Skyhawk, if anyone's curious).",['geometry']
4181312,Can it be said that $E(A_1)E(A_2) = E(A_2) E(A_1) = 0$ for $A_1 \cap A_2 = \varnothing\ $?,"Let $(X,\mathcal A)$ be a measurable space. Let $\mathcal H$ be a Hilbert space and $E : \mathcal A \longrightarrow \mathscr P (\mathcal H)$ be a projection valued map. For all $x \in \mathcal H$ with $\|x\| = 1$ define $\mu_x : \mathcal A \longrightarrow [0,\infty]$ by $\mu_x (A) = \left \langle x, E(A) x \right \rangle,$ $A \in \mathcal A.$ Suppose that $\mu_x$ is a probability measure for all $x \in \mathcal H$ with $\|x\| = 1.$ Then can we conclude the following $:$ If $A_1, A_2 \in \mathcal A$ with $A_1 \cap A_2 = \varnothing$ then $$E(A_1) E(A_2) = E(A_2) E(A_1) = 0.$$ Our instructor implicitly assuming this result to conclude that $E$ is countably additive but I can't figure out as to why it should be true. Do anybody have any idea about it? Thanks for your time.","['hilbert-spaces', 'measure-theory', 'projection', 'functional-analysis']"
4181373,"One of the Sylow $p$-subgroups of $G$ must be normal, for some prime $p $ dividing $|G|=870$.","Let $G$ be a group of order $870 = 2 \cdot 3 \cdot 5 \cdot 29$ . Show that at least one of the Sylow $p$ -subgroups of $G$ must be normal, for some prime $p $ dividing $|G|$ . The number of Sylow $29$ -subgroups is of the form $1 +29k$ , with $k \in \mathbb{Z}$ , and divides $30$ . Thus the number of Sylow $29$ -subgroups is  either $1$ or $30$ . If there is only one Sylow $29$ -subgroup, then we are done. Thus, let us assume that there are $30$ of them. Now, these Sylow $29$ -subgroups have prime order, so they intersect trivially, and provide us with $30\cdot 28=840$ elements of order $29$ . Symilarly, there are at least $6$ Sylow $5$ -subgroups (If we assume that it is not unique) and thus there are $6\cdot 4=24$ elements of order $5$ in $G$ . Repeating the same procedure, there are at least $10$ Sylow $3$ -subgroups (If we assume that it is not unique) and thus there are $10\cdot 2=20$ elements of order $3$ in $G$ . Finally,  there are at least $3$ Sylow $2$ -subgroups (If we assume that it is not unique) and thus there are $3\cdot 1=3$ elements of order $2$ in $G$ . So We have counted (adding the identity element) $840+ 24+20+3+1>870$ elements, wich is impossible. Thus at least one of the Sylow $p$ -subgroups of $G$ must be unique and hence normal, for some prime $p $ dividing $|G|$ . Is what I have done correct?","['normal-subgroups', 'abstract-algebra', 'solution-verification', 'sylow-theory', 'group-theory']"
4181398,Space diagonals of a dodecahedron,"I have been studying on platonic solids for a while and figuring out properties of dodecahedron. A dodecahedron with sidelength $a$ has $60$ surface diagonals and $100$ space diagonals, $10$ being long, $30$ being medium, and $60$ being short. I have figured out length of long diagonal is $a\sqrt{1+\phi^{4}}$ and medium diagonal is $a\phi^{2}$ but I couldn't find the short diagonal.","['solid-geometry', 'geometry', 'platonic-solids']"
4181415,Integro-Differential equation from my Complex analysis exam,"My recent complex analysis exam had the following problem as the last question, which I had a hard time solving. The problem Use the Laplace transform to solve the following differential equation for $u(t)$ . $$\dfrac{du(t)}{dt}+\dfrac{1}{2}\int_{0}^{t} e^{-t'}u(t-t')\,dt'=0$$ with the initial condition $u(0)=1.$ My attempt When applying the Laplace transform, the first term becomes $s\hat{u}(s)-u(0)=s\hat{u}(s)-1$ For the second term I used the formula for a Laplace transform of a convolution integral $$ L\bigg\{ \int_{0}^{t} g(\tau)f(t-\tau)\,d\tau\bigg\} = \hat{f}(s)\hat{g}(s) $$ This approach meant that the second term would be $\dfrac{1}{2}e^{-s}\hat{u}(s)$ After isolationg for $\hat{u}(s)$ I had $$ \hat{u}(s)= \dfrac{1}{s+\dfrac{1}{2}e^{-s}} $$ I then tried to apply the inverse Laplace transform $$u(s)=\dfrac{1}{2\pi i}\int_{\lambda - i\infty}^{\lambda + i\infty} \dfrac{e^{st}}{s+\dfrac{1}{2}e^{-s}}\,ds$$ When trying to find a singular point in the integrand, I found the solution $s=\mathrm{LambertW}\left(-\dfrac{1}{2}\right)$ I am not entirely familiar with the LambertW-function, and my attempt ended here. My question Did I make any mistakes leading up to the inverse Laplace? Is my approach even correct? How would you go about solving? Is this considered an Integro-differential-equation? Thanks for your time. :)","['complex-analysis', 'laplace-transform', 'integro-differential-equations']"
4181423,Proving a manifold with certain homogeneous property is Einstein,"Let $M$ be a Riemannian manifold such that for all pairs of points $(p, q), (r, s)$ satisfying $d(p, q) = d(r, s)$ , there exists an isometry $f: M \to M$ which takes $p$ to $r$ and $q$ to $s$ . Prove that $M$ is an Einstein manifold. My proof: let $p, q$ be arbitrary points in $M$ and $v, w$ unit vectors in $T_p M$ and $T_q M$ , respectively. Take totally normal balls of radius $\varepsilon$ around $p$ and $q$ and choose points $r, s$ on the geodesics $\gamma_v, \gamma_w$ (i.e, the only geodesics starting at $p$ and $q$ with initial tangent vectors $v$ and $w$ , respectively) such that $d(p,r)=d(q,s) = \frac{\varepsilon}{2}$ . Then there exists an isometry $f$ which takes $p$ to $q$ and $r$ to $s$ . Since isometries take geodesics to geodesics (and preserve lengths) and $\gamma_v, \gamma_w$ are the only geodesics connecting $p$ to $r$ and $q$ to $s$ with length $\frac{\varepsilon}{2}$ , it follows that $f \circ \gamma_v = \gamma_w$ , and hence that $\mathrm{d}f_p(v) = w$ . Since the Ricci tensor is invariant under isometries, this proves that the Ricci tensor is constant on the unit bundle of $M$ and it follows easily (from polarization, for instance) that $M$ is Einstein. Is this alright, do you see any mistakes in my argument or anything that could be improved? I`d appreciate any help. Thanks in advance! EDIT: Following Kajelad's comment, I thought it would be good to add a minor change to the argument. Now I think it's entirely correct.","['manifolds', 'geometry', 'riemannian-geometry', 'differential-geometry']"
4181436,If $\lim_k \mathcal{F}_k=\mathcal{W}$ then $E[X\mid\mathcal{F}_k]$ converges to $E[X\mid\mathcal{W}]$,"Let $X \in L^1,(\mathcal{F_k})_k$ a sequence of sub- $\sigma$ -algebra and converging to $\mathcal{W},$ i.e. $$\mathcal{W}=\bigcap_{k \in \mathbb{N}}\sigma\left(\bigcup_{q \geq k} \mathcal{F}_q\right)=\sigma\left(\bigcup_{k \in \mathbb{N}}\bigcap_{q \geq k}\mathcal{F}_q\right).$$ Prove that $E[X\mid\mathcal{F}_k]$ converges in $L^1$ to $E[X\mid\mathcal{W}].$ Does the almost sure convergence hold? Justify. If we let $\mathcal{Q}^1_k=\sigma(\bigcup_{q \geq k}\mathcal{F}_q),\mathcal{Q}_k^2=\bigcap_{q \geq k}\mathcal{F}_q,$ then $E[X\mid\mathcal{Q}_k^1]$ and $E[X\mid\mathcal{Q}_k^2]$ converges a.s and in $L^1$ to $E[X\mid\mathcal{W}].$ How to prove that $E[X\mid\mathcal{F}_k]$ converges in $L^1$ ? What about the a.s convergence ? Remark: the problem can be solved using the inequality: $$E[|E[X|\mathcal{F}_k]-E[X|\mathcal{W}]|] \leq E[|E[X|\mathcal{F}_k]-E[X|\mathcal{Q}_k^2]|]+E[|E[X|\mathcal{Q}_k^2]-E[X|\mathcal{W}]|] \leq E[|E[X|\mathcal{Q}_k^1]-E[X|\mathcal{Q}_k^2]|]+E[|E[X|\mathcal{Q}_k^2]-E[X|\mathcal{W}]|].$$","['conditional-expectation', 'martingales', 'stochastic-analysis', 'probability-theory']"
4181458,$U(1)$ principal bundle over $\mathbb{S}^1$,"In this question , the accepted answer uses the fact that the only $U(1)$ principal bundle over $\mathbb{S}^1$ is the trivial bundle. I'm not quite familiar with the classification of bundles, so I don't quite understand why this is true. Any pointers or sketch of the proof would be appreciated.","['principal-bundles', 'differential-geometry']"
4181468,Sum of Squares of Binomial Distributions?,"The $\chi^2$ distribution describes the sum of squares of independent normal random variables. Is there an analogous distribution for the discrete case of sum of squares of independent (identical) binomial random variables? I'm particularly interested in concentration bounds on the resulting sum. I know how to compute the expectation (and I suppose Chernoff bound from there), but is there anything stronger or more explicit to be said? I know that the binomial distribution approximates a normal distribution---does this imply that the sum of squares of binomials also approximates a $\chi^2$ distribution? Thanks in advance.","['statistics', 'probability-distributions', 'binomial-distribution', 'concentration-of-measure']"
4181488,How do I calculate the Hausdorff $2$-measure of a square directly?,"It is well-known that the Hausdorff $n$ -measure (with suitable normalization) agrees with the Lebesgue measure on Euclidean $n$ -space. Thus, it's clear that the $H^2$ measure of the unit square is $1$ . However, I cannot find or create a direct proof of this fact from the definition of Hausdorff measure as the limit of the Hausdorff content: $$ H^{d}_{\delta} (S) := \inf \left\{ \sum_{i=1}^{\infty} \frac{\omega_d}{2^d}(\operatorname{diam} U_i )^d : \bigcup_{i=1}^{\infty} U_i \supseteq S, \operatorname{diam} U_i < \delta \right\}.$$ I have tried covering with squares and with circles so far, but so far the lowest Hausdorff content I can find is $\pi/2$ , which is the Hausdorff content of a covering made of $2^n$ smaller squares. To clarify the question: find a sequence of covers $\{U_i\}$ of $S$ with so that the diameter of the sets in each cover approaches zero and the sum $\sum_{i=1}^{\infty} \frac{\omega_d}{2^d}(\operatorname{diam} U_i )^d$ approaches 1. Note: $\omega_d$ is the volume of the $d$ -dimensional unit ball. A source proving agreement of this normalization with Lebesgue measure can be found in these lecture notes from CUHK professor Kai-Seng Chou: https://www.google.com/url?sa=t&source=web&rct=j&url=https://www.math.cuhk.edu.hk/course_builder/1415/math5011/MATH5011_Chapter_3.2014.pdf&ved=2ahUKEwj975GE_7rxAhWMMd8KHSETDycQFnoECBIQAQ&usg=AOvVaw2ak7RZJQW-6f6Gth2OTBoc . See pages 14 and 18.","['measure-theory', 'hausdorff-measure', 'geometry']"
4181518,Spivak's Calculus: Chapter 13 Question 21,"I'm having a bit of trouble with the part b of the following question from Spivak's Calculus: In particular, I'm not certain about what assumptions regarding the function $f^{-1}$ and $f$ , I'm allowed to make, based on the information of the question. Can I assume that $f^{-1}$ is defined everywhere on [a, b], and bounded, and that f is integrable? And if not, what can I assume about $f^{-1}$ and $f$ ? Thanks in advance! Could you also avoid giving hints on how to actually solve the question, as I would still like to attempt it myself.","['calculus', 'definite-integrals', 'analysis']"
4181565,A halving neighborhood theorem for compact Hausdorff spaces.,"$\newcommand{\set}[1]{\{#1\}}$ $\newcommand{\mc}{\mathcal}$ Question Let $X$ be a compact Hausdorff space and $\Delta_2(X)$ denote the set $\set{(x, x):\ x\in X}$ in $X\times X=:X^2$ .
I want to confirm that the following is true (a proof if supplied below). Theorem 1. Let $X$ be a compact Hausdorff space and $O$ be a neighborhood of $\Delta_2(X)$ in $X^2$ .
Then there is a neighborhood $Q$ of $\Delta_2(X)$ such that whenever $(x, y)$ and $(y, z)$ are in $Q$ for some $x, y, z\in X$ , we have $(x, z)\in O$ . The motivation for this is to generalize, to compact Hausdorff spaces, the following fact about metric spaces that if "" $d(x, y), d(y, z)< \varepsilon/2$ then $d(x, z)< \varepsilon$ .""
My larger gaol was to have a device which allows mimicking proofs in topological dynamics for compact metric spaces to arbitrary compact Hausdorff spaces. The purpose of this post is two-fold.
One is to verify my proof below, and the other is to get a shorter proof of the theorem above.
(If you do not want to read my proof and supply your own proof then please go ahead and share!)
I am somewhat apprehensive about my proof since it is longer that what seems necessary and also that it took me many iterations to get the details right, for I had incorrectly proven it multiple times in the process. Purported Proof Lemma 2. Let $X$ be a compact Hausdorff space and $A$ be a closed set in $X$ .
Let $U$ be neighborhood of $A$ .
Then there is a neighborhood $O$ of $A$ in $X$ such that $\bar O\subseteq U$ . Proof. Restatement of the fact that compact Hausdorff spaces are normal. Lemma 3. Let $X$ be a compact Hausdorff space and $O$ be a neighborhood of $\Delta_2(X)$ in $X^2$ .
Then there is an open cover $\mc V$ of $X$ such that $$
(V\cup V')\times (V\cup V') \subseteq O
$$ whenever $V, V'\in \mc V$ are such that $V\cap V'\neq \emptyset$ . Proof. We say that an open cover $\mc U$ of $X$ if good if $\overline{\bigcup_{U\in \mc U} U\times U} $ is contained in $O$ .
It is clear from Lemma 2 and from compactness of $X$ that finite good open covers of $X$ exist.
Also, given an open cover $\mc U$ of $X$ , we say that $G$ in $\mc U$ is well-behaved if whenever $G\cap U\neq \emptyset$ for some $U$ in $\mc U$ , we have $(G\cup U)\times (G\cup U)$ is contained in $O$ . Let $\mc U=\set{U_1, \ldots, U_m, G_1, \ldots, G_n}$ be a be an arbitrary finite good open cover of $X$ , where each $G_i$ is well-behaved and each $U_i$ is not well-behaved.
If $m=0$ then we are done.
So assume that $m\geq 1$ .
It automatically follows that then $m\geq 2$ .
We will construct a finite good open cover of $X$ which has fewer ill-behaved elements.
By Lemma 2 we know that there is a neighborhood $Q$ of $\Delta_2(X)$ which contained $\overline{\bigcup_{U\in \mc U} U\times U}$ such that $\bar Q\subseteq O$ . Let $K$ be the boundary of $U_1\cup \cdots \cup U_{m-1}$ .
For each $p$ in $K$ , let $W_p$ be a neighborhood of $p$ in $X$ such that $W_p$ is contained in $G_i$ whenever $W_p\cap G_i\neq \emptyset$ , and $(W_p\cup U_i)\times (W_p\cup U_i)\subseteq Q$ whenever $W_p\cap U_i$ is not empty.
The existence of $W_p$ can be established by a compactness argument.
Since $K$ is compact, there is a finite set $F$ of $K$ such that $\set{W_p:\ p\in F}$ covers $K$ .
Define $U_m'=U_m\setminus \overline{U_1\cup \cdots \cup U_{m-1}}$ and $$
\mc U'
=
\set{U_1, \ldots, U_{m-1}, U_m', G_1, \ldots, G_n} \cup \set{W_p:\ p\in F}
$$ It is easy to check that $U_m'$ as well as each $G_i$ is well-behaved in $\mc U'$ .
Also, $\overline{\bigcup_{U'\in \mc U'} U'\times U'}$ is contained in $Q$ , and hence $\mc U'$ is a good open cover.
This finishes the proof. $\blacksquare$ Theorem 4. Let $X$ be a compact Hausdorff space and $O$ be a neighborhood of $\Delta_2(X)$ in $X^2$ .
Then there is a neighborhood $Q$ of $\Delta_2(X)$ such that whenever $(x, y)$ and $(y, z)$ are in $Q$ for some $x, y, z\in X$ , we have $(x, z)\in O$ . Proof. Let $\mc U$ be an open cover of $X$ such that whenever $U$ and $U'$ in $\mc U$ are such that $U\cap U' \neq \emptyset$ , we have $(U\cup U')\times (U\cup U')$ is contained in $O$ .
Such an open cover is furnished by Lemma 3.
Define $Q=\bigcup_{U\in \mc U} U\times U$ .
Now let $(x, y)$ and $(y, z)$ be in $Q$ for some $x, y, z\in X$ .
Then there are $U$ and $U'$ in $\mc U$ such that $(x, y)\in U\times U$ and $(y, z)\in U'\times U'$ .
Thus $U\cap U'$ is non-empty, and thus $(U\times U')\times (U\times U')$ is contained in $O$ .
But since $(x, z)$ is in $(U\cup U')\times (U\cup U')$ , we see that $(x, z)\in O$ , and we are done. $\blacksquare$","['general-topology', 'metric-spaces', 'compactness']"
4181571,Sectional curvature of Hadamard manifolds vanishes along certain planes if exponential map preserves norm,"I've been trying to solve the following exercise: Let $M$ be a Hadamard manifold (simply connected, complete and with sectional curvature $K \leq 0$ ). Show that: i) Let $p \in M, v, w \in T_pM$ linearly independent, $\gamma_v$ the geodesic with initial condition $v$ , and $E_w$ the parallel vector field along $\gamma_v$ with $E_w(0) = w$ . If $\|\mathrm{d}(\exp_p)_v(w)\| = \|w\|$ , then $K(\gamma_v'(t), E_w(t)) = 0$ for all $0 \leq t \leq 1$ . ii) Every metric ball $B_r(p)$ in $M$ , $r \geq 0$ , is strictly convex, i.e every geodesic segment connecting two points of $B_r(p)$ is contained in $B_r(p)$ . But I haven't had any good ideas so far. I know the exponential map is a global diffeomorphism in this case and I think maybe some solution could come from using Jacobi fields/variations but I couldn't think of anything concrete along those lines. I'd appreciate any help. Thanks in advance! EDIT : Since Hadamard manifolds don't have any conjugate points, I see now how i) is a straightforward consequence of Rauch's comparison theorem. I'm still stuck on ii) though and I realize now I should've split it into two posts, so I'm going to ask it in another post.","['manifolds', 'smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
4181573,Extending a linear operator satisfying an order condition,"Let $\ell^\infty$ be the usual space of bounded sequences, and consider the subspace $V_1 ⊂ \ell^\infty$ consisting of vectors with finite $1$ -norm. (That is, $V_1$ contains those $x ∈ \ell^\infty$ such that $\sum_i |x_i|$ is finite. I'm not calling this space $\ell^1$ because it inherits the $\infty$ -norm instead. I don't know what terminology would be standard.) Let $\ell^\infty$ be ordered such that $x \ge y$ iff $x_i \ge y_i$ for each $i$ . Let $$
P = \{ x ∈ \ell^\infty ∣ x ≥ y \text{ for some } y ∈ V_1 \}
$$ Does there exist a linear operator $f : \ell^\infty \to V_1$ that extends the identity map on $V_1$ with the property that $x \ge f(x)$ for all $x ∈ P$ ? This seems like it should follow from some version of the Hahn-Banach theorem, maybe?","['banach-lattices', 'banach-spaces', 'functional-analysis']"
4181622,Circle of Best Approximation and Curvature,"Recently I have been studying the curvature formula. I understood the change in direction with respect to distance definition $||\frac{dT}{ds}||$ , but was bothered when introduced to the radius of curvature definition. Since most explanations use the differential definition to explain the derivation of the formula, they only gloss over the radius form. On Wikipedia it this is what it states about Radius of Curvature: ""For a curve, it equals the radius of the circular arc which best approximates the curve at that point."" https://en.wikipedia.org/wiki/Curvature What my question is, is how can the circle of best approximation be defined? in Taylor Series, we learn about the best possible polynomial approximation of a certain degree, so how do we define the best circle approximation? thank you for reading :)","['multivariable-calculus', 'differential-geometry']"
4181640,Alternate proof that principal minors of a singular correlation matrix are non-negative?,"What are some elegant proofs to demonstrate that the principal minors of a valid, but singular correlation matrix are all non-negative? A valid correlation matrix is a square symmetric matrix, has ones on its main diagonal and is positive semi-definite. A singular correlation matrix has a matrix determinant of zero, i.e. at least one eigenvalue equal to zero. The proof that I have: It can be shown that any $n\times n$ singular correlation matrix can be written as the product of $n$ idempotent matrices. It can also be shown that the eigenvalues of an idempotent matrix are always either $0$ or $1$ . Therefore, it is not possible that singular correlation matrices have negative eigenvalues. As a consequence, all principal minors of singular correlation matrices have to be non-negative.","['matrices', 'linear-algebra']"
4181641,Root of the given polynomial $4x^3-3x-p$,"Given the equation $$ 4x^3 - 3x-p=0 $$ In the question we were required to find the root of this equation in the interval $[1/2,1]$ and $-1\le p\le 1$ . The answer is arrived at by substituting x as $\cos(\theta)$ and using the multiple angle formula. However assuming I am not able to think of this substitution in the exam(it does not appear to be a very intuitive one considering the range of $x$ is not $[-1,1]$ ), is there another method to arrive at this answer?","['trigonometry', 'roots']"
4181651,Dirichlet integral integrates to $\pi$,"I think the question has been asked several times on MSE since seems a standard fourier analysis fact, but I don't find any reference for it. Let's consider the Dirichlet Kernel as $$D_n(x) := \sum\limits_{\lvert k \rvert \leq n}e^{ikx}$$ In a further proposition I think it's used something like $$\int_0^{\pi} D_n(x)dx = \pi$$ Is this true? I tried to computed this explicitly or using $$D_n(x) = \frac{\text{sin}\left(\left(n+\frac{1}{2}\right)x\right)}{\text{sin}(\frac{x}{2})} \hspace{0.2cm} \forall x\ne 0 \hspace{0.2cm} \text{mod} 2\pi$$ But I got stuck since the integral of the part relative to $\text{cos}$ is $0$ , since integrate to $\text{sin}$ , and the part relative to $\text{cos}$ depends on $k$ but the $i$ term remains. Where is my mistake? And how to prove $$\int_0^{\pi} D_n(x)dx = \pi$$ if true
?","['integration', 'fourier-analysis', 'definite-integrals', 'analysis', 'fourier-series']"
4181676,Meaning of the p-value,"Suppos that we have a null-hypothesis $H_0: \ \theta=\theta_0$ . Our alternative hypothesis could be for example $H_1: \ \theta\ne \theta_0$ . We want to test the null-hypothesis so we construct a test-statistic $T$ which has some probability distribution. Based on the data we compute the numerical value of the test statistic to be $t$ . For some reason, we define $p:=2 \mathrm{min}\left\{\mathbb{P}\left(T\ge t\mid H_0  \right),\mathbb{P}(T \le t\mid H_0)\right\}$ and if the p-value is very small, we abandon the null-hypothesis. Could someone please clarify, how does the p-value imply that the null-hypothesis is incorrect? And also in cases when we have $H_1: \ \theta>\theta_0$ or $H_1: \ \theta < \theta_0 $ . I think it would be more convenient to check the probability of having $T\in \left(t-\varepsilon, t+\varepsilon \right)$ assuming that the null-hypothesis holds.","['statistical-inference', 'statistics']"
4181680,Do the seminorms $|\int_{\frac{1}{n-1}}^\frac{1}{n} f(x) dx|$ separate points?,"Let's define family of seminorms: $$p_n(f)  = |\int_{\frac{1}{n+1}}^\frac{1}{n} f(x) dx|$$ on $C([0, 1])$ I want to check whether this norm family of seminorms separates points. i.e. I have to check: $$(p_n(f) = 0 )\Rightarrow f = 0$$ My work so far $$p_k(f) = |\int_{\frac{1}{k+1}}^{\frac{1}{k}} f(x) dx| = 0 \Rightarrow \int_{\frac{1}{k+1}}^{\frac{1}{k}} f(x) dx = 0$$ Now I wanted to take function defined in the following way: This function completely fits our example - area under the curve on the interval $[\frac{1}{k+1}, \frac{1}{k}]$ is $0$ . But I realized that it's different function for different $k$ (definition of this function depends on k). I tried to find another function that is $k -$ independent however I wasn't able to. I was also trying to prove that it has to be separable (that $f \equiv 0$ ) but also I end up with nothing. Could you please give me hint what's the direction I should follow?","['integration', 'normed-spaces', 'functional-analysis', 'real-analysis']"
4181750,Upper bound of $\left(1+\dfrac{1}{2}\right)\left(1+\dfrac{1}{4}\right)...\left(1+\dfrac{1}{2^n}\right)$ [duplicate],"This question already has answers here : Proof that a sequence is convergent (4 answers) Closed 3 years ago . In a calculus book that I'm reading, there is a problem as follows: Prove that this sequence $x_n=\left(1+\dfrac{1}{2}\right)\left(1+\dfrac{1}{4}\right)...\left(1+\dfrac{1}{2^n}\right)$ is monotonic, bounded and then converges. I can prove that this sequence is increasing, but I found no way to find the upper bound for $x_n$ . Please give me some hint to find the upper bound, and may be the limit of this sequence also. Thanks","['real-analysis', 'calculus', 'upper-lower-bounds', 'sequences-and-series', 'limits']"
4181786,Can a real sequence have every natural number as its limit point?,Does there exist a real sequence such that every $n\in\mathbb N$ is its limit point? I can not get anywhere with this.,['sequences-and-series']
4181798,Positive semidefinite matrix ordering and nuclear norm of products,"Let $A, A', B, B'$ be finite-dimensional, complex-valued, Hermitian, positive-semidefinite matrices. Moreover, let $(A-A')$ and $(B-B')$ also be positive-semidefinite. The 1-norm is defined as $\|X\|_1 = Tr(\sqrt{X^*X})$ where $X^*$ is the transpose conjugate of $X$ . Can one claim anything about the relationship between $\|AB\|_1$ and $\|A'B'\|_1$ ? In particular is $$\|AB\|_1\geq \|A'B'\|_1?$$ If not, can someone show a counterexample?","['matrices', 'linear-algebra', 'positive-semidefinite', 'nuclear-norm']"
4181829,"Classifying all numbers $n$ with the property that if $p$ is a prime, then $p \mid n \iff p-1 \mid n$","If a number $n$ has the property that if $p$ is a prime, then $p \mid n \iff p-1 \mid n$ , we call $n$ a nice number for brevity. A recent question on MSE (edit: now deleted) asks to prove that $1806$ is the only squarefree nice number. (That question gives no context and looks like a contest problem, and was therefore ill-received, but the actual question is a nice puzzle.) After solving it, I relayed the question to some friends, but initially I accidentally forgot the squarefree condition. This led me to wondering about nice numbers that are not necessarily squarefree. A fairly simple argument proves that all nice numbers must be multiples of $1806$ , and this is the first step towards solving the puzzle as well. (Edit: namely, for any nice $n$ , we have $2-1 \mid n$ , so $2 \mid n$ ; but then also $3 \mid n$ . It follows that $6 \mid n$ , and therefore $7 \mid n$ . Then $2 \cdot 3 \cdot 6 = 42 \mid n$ , so $43 \mid n$ , and we obtain that $2 \cdot 3 \cdot 6 \cdot 43 = 1806 \mid n$ -- but $1807$ is not prime, so the argument ends here.) However, there are more examples that are not squarefree: $1806 = 2 \times 3 \times 7 \times 43$ . $12642 = 2 \times 3 \times 7^2 \times 43$ . $88494 = 2 \times 3 \times 7^3 \times 43$ . $6030842622 = 2 \times 3 \times 7 \times 43^2 \times 77659$ . These are all examples up to $2 \times 10^{10}$ (via computer search). The sequence is not currently listed on OEIS. The sequence has now been listed, at A345765 . My question is: can we classify all nice numbers? Are there only four? Are there more? Infinitely many?","['number-theory', 'recreational-mathematics', 'prime-factorization']"
4181874,Calculating triple integral between paraboloid and plane,"I have the following integral: $$ \iiint zdxdydz$$ on the area bound by the following surfaces: $$z=\frac{a^2}{b^2} (x^2+y^2)$$ $$z=a$$ $$a,b>0$$ The first surface is a paraboloid and the second is a plane. Now, this is how I proceeded: Introducing cylindrical coordinates: $x=r\cos\phi$ $y=r\sin\phi$ $|J| = r$ now, what I did next was substitute $z=a$ in the equation of the paraboloid. I got that $x^2+y^2 = \dfrac{b^2}{a}$ , or that $r=\dfrac{b}{\sqrt{a}}$ which led me to believe that $$r\in \left[0, \frac{b}{\sqrt{a}}\right]$$ $$\phi \in \left[0, 2\pi\right]$$ Now regarding $z$ , I know that it is bound from below by the paraboloid and from above by the plane, so I deduced that the boundaries are: $$z\in \left[\frac{a^2}{b^2}r^2, a\right]$$ . My integral gets the form: $$\int_0^{\frac{b}{\sqrt{a}}} \int_0^{2\pi} \int_{r\frac{a^2}{b^2}}^a zr dz d\phi dr$$ Which evaluates to $\dfrac{\pi ab^2}{3}$ , but my workbook solution is $\dfrac{a^2 b^2 \pi}{4}$ . Could anyone tell me where I went wrong?","['integration', 'multivariable-calculus', 'cylindrical-coordinates', 'multiple-integral']"
4181884,Does the independence of X and Y imply that E[XY|G]=E[X|G]E[Y|G]?,"let $\mathcal{G}$ be a sigma algebra and $X$ , $Y$ be $r.vs$ satisfies $E[X|\mathcal{G}]$ ， $E[Y|\mathcal{G}]$ exists.Is there hold $E[XY|\mathcal{G}]=E[X|\mathcal{G}]E[Y|\mathcal{G}]$ in general? the reason why i think above property is right is as follows. When i prove $E[X_t|\mathcal{F_s}]=X_s$ in the proof that Itô integral of step process $f(t, \omega)$ in $L_{ad}^2([a,b] \times \Omega)$ is a martingale.( $X_t=\int_a^tf(t,\omega)dB(t)$ ) As $E[X_t|\mathcal{F_s}]=E[\int_a^sf(r,\omega)dB(r)|\mathcal{F_s}] + E[\int_s^tf(r,\omega)dB(r)|\mathcal{F_s}]=X_s+E[\int_s^tf(r,\omega)dB(r)|\mathcal{F_s}]$ we only need to prove $E[\int_s^tf(r,\omega)dB(r)|\mathcal{F_s}] = 0$ without losing generality.Assume $\int_s^tf(r,\omega)dB(r) = \sum_{j=1}^l \eta_{j-1}(B_{t_j} - B_{t_{j-1}})$ . As $\eta_{j-1}\in \mathcal{F_{t_{j-1}}}$ and $B_{t_j} - B_{t_{j-1}}$ independent of $\mathcal{F_{t_{j-1}}}$ . It is obvious that $E[\int_s^tf(r,\omega)dB(r)|\mathcal{F_s}] = 0$ is true if the property claimed in title is right. As we known. $E[\int_s^tf(r,\omega)dB(r)|\mathcal{F_s}] = 0$ .so i guess the property is right boldly. Can u tell me whether it holds in above condition? Thanks in advance!","['stochastic-analysis', 'measure-theory']"
4181890,If $2^{2x-1}$ = $(\frac{1}{5})^x$ and $\log 2 = a$ prove that: $x=\frac{a}{a+1}$,"This is my working out: $2^{2x} \times 2^{-1} = (5^{-1})^x$ $\log 2^{2x-1} = \log 5^{-x}$ $2x-1 \log 2 = -x\log5$ $2x-1\times a = -x\log5$ At this point, I got stuck.","['functions', 'logarithms']"
4181923,Is there an elementary proof that $\int_0^{\infty}|\sin(x)|^{x}\ dx$ converges or diverges?,"Is there an elementary proof that $\displaystyle\int_0^{\infty}|\sin(x)|^{x}\ dx$ converges or diverges? I tried the following: $|\sin(x)|\leq 1-\frac{1}{3}\left( x-\frac{\pi}{2} \right)^2$ on $[0,\pi]$ . Therefore, $$\int_0^{\infty}|\sin(x)|\ dx\leq\sum_{n=0}^{\infty}\left(\int_0^{\pi} \left(1-\frac{1}{3}\left( x-\frac{\pi}{2} \right)^2\right)^{\pi n}\ dx \right).$$ But now WolframAlpha says that those integrals require the hypergeometric function which I know nothing about. As usual, I've got a feeling that I'm missing an easier method...","['integration', 'convergence-divergence', 'improper-integrals']"
4181931,Proving that group $G$ of order $|G|=35$ is Abelian,"This is my outline of proof: By Sylow's theorems, $G$ has two unique subgroups $H$ and $K$ respectively of order $5$ and order $7$ and both are Abelian; as groups of prime order are Abelian Next I use the counting argument to say: Taking any $x\in{H}$ and $y\in{K}$ such that $x,y\notin{e}$ , I will have more number of ordered pairs than the remaining elements in $G$ (i.e $35-5-7+1=24$ )  ( $e$ is common to both so $+1$ ) Thus there are combinations which are same; implying $x$ and $y$ commute; but how do I proceed to prove that even the other elements commute. Kindly help in showing the way further.","['group-theory', 'abstract-algebra', 'abelian-groups', 'sylow-theory']"
4181940,Derivative of differential of time-dependent flow of a vector field,"I am reading the book ""Morse Theory and Floer Homology"" by Audin and Damian. And I am kind of stuck in understanding the last part of a proof in such book. Currently we are trying to prove Proposition 5.4.5. If a critical point of $H$ is nondegenerate as a periodic solution of the Hamiltonian system, then it is nondegenerate as a critical point of the function $H$ . (Screenshot here) So, part of the proof is here Computed at the critical point $x$ of $H$ , the first term is zero and the second one is exactly $$
\left(d^{2} H\right)_{x}\left(X_{f}(x), Z(x)\right)=\left(d^{2} H\right)_{x}(Y, Z)
$$ We now suppose that $x$ is nondegenerate as a trajectory of $X_{H}$ . This means that for every $Z \neq 0$ , we have $$
T_{x} \psi^{1}(Z)-Z \neq 0.
$$ (Screenshot here) My main question is in page 138. So it says that $$\frac{d}{dt} T_x \psi^{t}(Z) = T_x \psi^{t} ([X_H, Z])$$ I do not understand where this equality comes from. I have been trying to see if this is a known property of time-dependent flows, but no success until now. Here $X_H$ denotes the time-dependent Hamiltonian vector field. Here is the things I have tried until now: In the appendix of such book it is known that for time-dependent flows we have that $\frac{dY}{dt} = (dX_t)_{x(t)}Y$ iff $Y(t) = T_{x(t)}\psi^{t}(Y(0))$ , where $X_t$ is any time-dependent vector field. So, we can apply this to our case where $X_t = X_H$ , but after that I am not sure how to go from $(dX_H)_{x(t)}Y$ to $T_x \psi^{t}([X_H, Z])$ . The other thing that I tried to imitate was the fact that for time-dependent vector fields we get a formula for $\frac{d}{dt} \psi^{*} Y_{t}$ , where it is very similar with what I want, but at the end this formula is for the pullback of my flow $\psi$ and I want a formula for the push-forward. Again, maybe this is a basic fact of time-dependent flows and it is easy to prove but if I could get any kind of hint or reference I could look to understand this I would be grateful. Thanks in advance","['vector-fields', 'symplectic-geometry', 'morse-theory', 'differential-geometry']"
4181945,The classical statement of class field theory,"The classical statement of class field theory is that for finite Galois extension $L/Q$ the following are equivalent: a. $L$ is a class field. b. $L/Q$ is abelian. c. $L \subset Q[\zeta _n]$ for some $n$ . Here class field means for any unramified prime $p \in Q$ how the prime splits in $L$ depends only on the congruence class of $p$ modulo some modulus $N$ -which is a natural number. I want to see this statement proven using the modern version either the idelic or the ideal theoretic, as something like a corollary. $b \iff c$ is Kronecker Weber so that is fine (and the opposite direction is very easy). It seems that ( https://ayoucis.wordpress.com/2015/01/26/a-class-field-theoretic-phenomenon/ ) proves that $a\iff b$ . But I don't quite get the proof. Can someone explain how this works? maybe explain the central idea of the proof?","['class-field-theory', 'number-theory', 'algebraic-number-theory']"
4181964,What are significant differences between polynomial rings and the ring of formal power series?,"I would like to know the difference between the polynomial ring and the ring of formal power series. We didn't mention it in our lectures so I have to understand it on my own. I saw some posts here and on the internet and I understand that: Formal power series are a generalisation of polynomials, in that the coefficients can be infinite while in the polynomial they have to end at one point. Then I guess we can evaluate the polynomial and therefore have a function while this isn't so simple in power series since they do not always converge. I feel like this is a difference between polynomials and power series, but what about them as rings? Does it make a difference? Are there other important differences? Edit: I also saw on Wikipedia that there are differences when we consider topologies, but I haven't done any topological course until now. This is my very first algebra course.","['formal-power-series', 'abstract-algebra', 'polynomials']"
4181982,Is it possible that pair of isomorphic subgroups of finite group is conjugate in some larger group,"Let $G$ be a finite group. Let $H$ and $I$ be a pair of isomorphic but non-conjugate subgroups of $G$ . Does there always exists a finite group $L$ containing $G$ such that $H$ and $I$ is conjugate in $L$ ? This is how far I've got : by Cayley's theorem, it suffices to assume that $G$ is a finite symmetric group. I have a simple example but fail to prove the general case. The example is: let $$G=S_4=\langle a,b,c \mid a^2 = b^3 = c^4 = abc = 1\rangle$$ $$H = \langle a \rangle; \quad I = \langle c^2 \rangle.$$ It is easy that $a$ is not conjugate to $c^2$ in $S_4$ . Now we construct group embedding $$\begin{align}
\varphi: S_4 &\to S_6,\\
 a &\mapsto (23)(56),\\
 b &\mapsto(134),\\
c&\mapsto(56)(1432)
\end{align}$$ It is easy that $\varphi(a)$ conjugates to $\varphi(c^2)$ in $S_6$ .","['group-presentation', 'group-theory', 'abstract-algebra', 'group-isomorphism']"
4181994,When is a field an algebraic extension of a fixed subfield?,"Let $K$ be a field, $G\leq\mbox{Aut}(K)$ a group of field automorphisms of $K$ . When is the extension $K/K^G$ algebraic? Recall that $K^G$ is the subfield of $K$ consisting of the elements of $K$ that are fixed by every element of $G$ . If the group is finite, then the extension is finite and Galois, but in the infinite case this is not always true. For example, consider the field $K=\mathbb C(t)$ of rational functions with complex coefficients, and consider the subgroup $G$ generated by the translation $t\mapsto t+1$ , which is clearly an automorphism. It is not hard to see that $K^G =\mathbb C$ (the constant rational functions) and that therefore $K/K^G$ is transcendental.","['field-theory', 'galois-theory', 'abstract-algebra', 'extension-field']"
4182003,Equivalence classes of solutions to a matrix equation,"I am wondering whether the following problem can be mathematically formalized an solved. Namely, I want to quantify how many solutions there are to the equation $$U_1 \cdot U_2 \cdot U_3 \cdot U_4 = U_4 \cdot U_3 \cdot U_2 \cdot U_1$$ where $\{U_a\}_{a=1}^4$ are unitary matrices of arbitrary (but the same) dimensions. For example, using the Pauli matrices, $U_1 = U_2 =\sigma_x$ with $U_3 = U_4 = \sigma_z $ would be a simple solution with $2\times 2$ matrices. Clearly, there are infinitely many solutions, but one can organize them into enumerable equivalence classes. For example, replacing $U_a\mapsto U_a e^{\mathrm{i} \phi_a}$ maps one solution to another equivalent solution. Similarly, if $V$ is a unitary matrix of the same dimension, then $U_a \mapsto V\cdot U_a \cdot V^{-1}$ (with the same $V$ for all $a\in\{1,2,3,4\}$ ) gives another equivalent solution. However, not all solutions of the same dimension are equivalent; for example, the solution $U_1 = U_2 = \sigma_x$ and $U_3 = U_4 = \sigma_z$ cannot be related to the solution $U_1 = U_3 = \sigma_x$ and $U_2 = U_4 = \sigma_z$ using the listed equivalence transformations. Finally, one can introduce the notion of ""reducible"" solutions, i.e. ones that are equivalent to $U_a$ 's with a block-diagonal structure; and similarly, there are presumably ""irreducible"" higher-dimensional solutions. Clearly, the knowledge of irreducible solutions is sufficient for the identification of all possible solutions. The two example solutions listed above are both irreducible in this sense. Thus, trying to be more precise now, I am wondering how to identify all equivalence classes of irreducible solutions to the equation at the beginning. Remark: The fact that I chose to describe this problem using group-theoretical terminology is not an accident. The formulated problem should indeed be equivalent to looking for irreducible representations of the following free group with a constraint, $$\Gamma = \left<a,b,c,d\,|\, a.b.c.d.a^{-1}.b^{-1}.c^{-1}.d^{-1}=1\right>.$$ This group corresponds to the maximal torsion-free subgroup of the Fuchsian symmetry group of the $\{8,8\}$ -tessellation of the hyperbolic plane. Perhaps this offers some of you helpful geometric insights that I overlooked.","['equivalence-relations', 'representation-theory', 'matrices', 'hyperbolic-geometry', 'group-theory']"
4182014,Mean of the sum of two independent geometric distributions with different probabilities of success,"Let $X \sim Geom(p_1), Y \sim Geom(p_2)$ be two independent geometrically distributed random variables with probabilities of success $p_1, p_2$ respectively. I want to find the mean of the sum of these distributions; $X+Y$ . I know that the mean of a geometric distribution is $\frac{1}{p}$ , where $p$ is the probability of success. In this answer: Sum of two Geometric Random Variables with different success probability , I see that: $$\mathbf{P}(X+Y = n) = \frac{p_1p_2}{p_1 - p_2}
\big( (1-p_2)^{n-1} - (1-p_1)^{n-1} \big), \qquad n = 2,3,\ldots 
$$ How do I then find the mean of this distribution? How does the mean of this distribution relate to the sum of the means of the independent distributions? ( $\frac{1}{p_1} + \frac{1}{p_2}$ ) Intuitively, I'm not sure whether we'd expect the mean of the sum of the independent distributions to be roughly the same as the sum of the means of the independent distributions?","['statistics', 'negative-binomial', 'probability-distributions', 'binomial-distribution', 'probability']"
4182027,Proof verification: Is my induction proof correct?,"Can anyone proof read my induction proof for this particular task? For $$ n \in \mathbb{N_0} $$ $a_n$ is defined as $$ a_n= (-1)^{(n+1)}+ 1 + (-3n) + 2n^2 $$ and $b_n$ as: $$ b_n =\begin{cases}
          a_n \quad &\text{if} \, 0 ≤ n ≤ 2, \\
          b_{n-1}+b_{n-2}-b_{n-3}+8 \quad &\text{if} \, 3 ≤ n \\
     \end{cases}
$$ Prove or disprove by induction that $ a_n = b_n $ $$Induction Base: n=0,n=1,n=2,n=3$$ $$b_0 =a_0 =(−1)0+1 +1−3·0+2·02 =−1+1=0$$ $$b1 =a1 =(−1)1+1 +1−3·1+2·12 =1+1−3+2=1$$ $$b_2 =a_2 =(−1)2+1 +1−3·2+2·22 =−1+1−6+8=2 
$$ $$b_3 =b_2 +b_1 −b_0 +8=2+1−0+8=11 
$$ Induction Hypothesis: For all $k \in \mathbb {N_0}$ and k<n and n≥3 for some random but fixed $n \in \mathbb{N_0} $ a_k = b_k Induction Step: $bn =b_{n−1} +b_{n−2} −b_{n−3} +8=(−1)^n +1−3(n−1)+2(n−1)$ $+(−1)^n−1 +1−3(n−2)+2(n−2)^2−(−1)^{n−2} +1−3(n−3)+2(n−3)^2+8$ $=(−1)(−1)(−1)^{n−2} +1−3(n−1)+2(n−1)^2 +(−1)^{n−1} +1−3(n−2)+2(n−2)^2$ $−(−1)^{n−2} −1+3(n−3)−2(n−3)^2 +8 = 1(−1)^{n−2} +1−3n+3+2n^2 −2^{n+1}+1(−1)^{n−1} −3n+6+2n^2 −4n+4$ $−(−1)^{n−2} +3n−9−2n^2 −6n+9+8= (11) =(−1)(−1)(−1)^{n−1} +9−3n+2n^2 −4n+2+2n^2 −8n+8−2n^2 +12n−18$ $=(−1)^{n+1} +1−3n+2n^2 =a_n $","['induction', 'solution-verification', 'discrete-mathematics']"
4182160,How does the Hopf fibration generalize to maps $S^{2n+1}\to \mathbb{CP}^n$?,"I've been reading about Hopf fibrations .
In the Wikipedia page, they state that the Hopf construction generalises to higher-dimensional projective spaces. More specifically, they write that The Hopf construction gives circle bundles $p:S^{2n+1}\to\mathbb{CP}^n$ over complex projective space [sic]. This is actually the restriction of the tautological line bundle over $\mathbb{CP}^n$ to the unit sphere in $\mathbb{C}^{n+1}$ ."" I understand the ""standard"" $n=1$ case as follows: define $p:S^3\to S^2$ as: $$p(x_1,y_1,x_2,y_2) = (2(x_1 x_2+y_1 y_2), 2(x_1 y_2-x_2 y_1),x_1^2+y_1^2-x_2^2-y_2^2),$$ so that $p(x_1,y_1,x_2,y_2)\in S^2$ whenever $(x_1,y_1,x_2,y_2)\in S^3$ . We then use the standard diffeomorphism $S^2\simeq\mathbb{CP}^1$ to conclude that there is a projection $\tilde p:S^3\to\mathbb{CP}^1$ which, for all $(x_1,y_1,x_2,y_2)\in S^3$ and $\theta\in\mathbb R$ , satisfies $$\tilde p(\cos\theta\, x_1,\sin\theta\, y_1,\cos\theta\, x_2,\sin\theta\, y_2)
= \tilde p(x_1,y_1,x_2,y_2),$$ meaning that $\tilde p^{-1}(x)\simeq S^1$ for all $x\in\mathbb{CP}^1$ . While that is fine, it hinges on the Hopf map $S^3\to S^2$ , which seems rather ad hoc . How does this mapping generalise to $\mathbb{CP}^n$ ?","['projective-geometry', 'complex-geometry', 'hopf-fibration', 'algebraic-topology', 'differential-geometry']"
4182173,Show that the Sylow $17$-subgroup of $G$ is central,"Let $G$ be a group of order $935 = 5 \cdot 11 \cdot 17$ . Show that the Sylow $17$ -subgroup
of $G$ is central. What I have: The number of Sylow $17$ -subgroups is $1+17k$ , for some $k\in \mathbb{Z}$ and divides $55$ . Thus, the number of Sylow $17$ -subgroups is $1$ . That is, there is just one Sylow $17$ -subgroup and therefore is normal in $G$ . My problem is in saying that it is central, how could I do it? If we consider the number of Sylow $11$ -subgroups, we have that there are just one Sylow $11$ -subgroup as well. In the case of the number of Sylow $5$ -subgroups this is either $1$ or $11$ . It occurs to me that one way to show that the Sylow $17$ -subgroup is central is to show that $G$ is abelian, but I don't know how to follow. I would appreciate any help.","['finite-groups', 'normal-subgroups', 'abstract-algebra', 'sylow-theory', 'group-theory']"
4182181,Prove that $\mathcal C \subseteq \mathcal P(Y)$ implies $\sigma(f^{-1} [\![ \mathcal C ]\!]) = f^{-1} [\![ \sigma(\mathcal C) ]\!]$,"I've come up with this proposition in a lecture note. Consider $f: X \to Y$ . For $A \subseteq  X$ , we define $f[A] := \{f(x) \in X \mid x \in A\}$ . For $A \subseteq  Y$ , we define $f^{-1}[A] := \{x \in X \mid f(x) \in A\}$ . For $\mathcal C \subseteq \mathcal P(X)$ , we define $f [\![ \mathcal C ]\!] := \{f[A] \mid A\in \mathcal C\}$ . For $\mathcal C \subseteq \mathcal P(Y)$ , we define $f^{-1} [\![ \mathcal C ]\!] := \{f^{-1}[A] \mid A\in \mathcal C\}$ . The smallest $\sigma$ -algebra containing $\mathcal C$ is denoted by $\sigma(\mathcal C)$ . Prove that $\mathcal C \subseteq \mathcal P(Y)$ implies $\sigma(f^{-1} [\![ \mathcal C ]\!]) = f^{-1} [\![ \sigma(\mathcal C) ]\!]$ . A proof can be found here . It's so interesting that I tried to give it a shot. Could you please confirm if my attempt is fine? My attempt: We notice that if $\mathcal A$ is a $\sigma$ -algebra over $X$ , then $\overrightarrow{\mathcal A} := \{B \subseteq Y \mid f^{-1}[B] \in \mathcal A\}$ is a $\sigma$ -algebra over $Y$ . Similarly, if $\mathcal B$ is a $\sigma$ -algebra over $Y$ , then $\overleftarrow{\mathcal B} := \{f^{-1}[B] \mid B \in \mathcal B\}$ is a $\sigma$ -algebra over $X$ . It maybe the case that $f[f^{-1}[B]] \subsetneq B$ , so it's not necessarily true that $\overrightarrow{\mathcal A} \subseteq f [\![ \mathcal A ]\!]$ . For $A \in f^{-1} [\![ \overrightarrow{\mathcal A} ]\!]$ , $A = f^{-1}[B]$ for some $B \in \overrightarrow{\mathcal A}$ . By construction of $\overrightarrow{\mathcal A}$ , $A \in \mathcal A$ and thus $f^{-1} [\![ \overrightarrow{\mathcal A} ]\!] \subseteq \mathcal A$ . Let $$\begin{aligned} \mathfrak S_1 &= \{\mathcal A \text{ is a } \sigma \text{-algebra} \mid f^{-1} [\![ \mathcal C ]\!] \subseteq \mathcal A\} \\ \mathfrak S_2 &= \{ f^{-1} [\![ \mathcal B ]\!] \mid  \mathcal B \text{ is a } \sigma \text{-algebra and } \mathcal C \subseteq \mathcal B\} \end{aligned}.$$ Then $\sigma( f^{-1} [\![ \mathcal C ]\!] ) = \bigcap \mathfrak S_1$ and $f^{-1} [\![ \sigma(\mathcal C) ]\!] =\bigcap \mathfrak S_2$ . Let $\mathcal A \in \mathfrak S_2$ . Then $\mathcal A = f^{-1} [\![ \mathcal B ]\!]$ for some $\sigma$ -algebra $\mathcal B$ such that $\mathcal C \subseteq \mathcal B$ . Then $\mathcal A$ is also a $\sigma$ -algebra such that $f^{-1} [\![ \mathcal C ]\!] \subseteq f^{-1} [\![ \mathcal B ]\!] = \mathcal A$ . This means $\mathcal A \in \mathfrak S_1$ and thus $\mathfrak S_2 \subseteq \mathfrak S_1$ . We then obtain $\bigcap \mathfrak S_1 \subseteq \bigcap \mathfrak S_2$ . For $B \in \mathcal C$ , $f^{-1} [B] \in f^{-1} [\![ \mathcal C ]\!] \subseteq \mathcal A$ . Then $B \in \overrightarrow{\mathcal A}$ by construction of $\overrightarrow{\mathcal A}$ . Hence $\mathcal C \subseteq \overrightarrow{\mathcal A}$ and thus $f^{-1} [\![ \overrightarrow{\mathcal A} ]\!] \in \mathfrak S_2$ . This means for every $\mathcal A \in \mathfrak S_1$ , we can find a $\sigma$ -algebra $f^{-1} [\![ \overrightarrow{\mathcal A} ]\!] \in \mathfrak S_2$ such that $f^{-1} [\![ \overrightarrow{\mathcal A} ]\!] \subseteq \mathcal A$ (this inclusion is proved above). Hence $\bigcap \mathfrak S_2 \subseteq \bigcap \mathfrak S_1$ . Finally, $\bigcap \mathfrak S_1 = \bigcap \mathfrak S_2$ . This completes the proof. Update: From this comment by Nate Eldredge, I would like to apply his enlightening idea. We want to prove $f^{-1} [\![ \sigma(\mathcal C) ]\!] \subseteq \sigma(f^{-1} [\![ \mathcal C ]\!])$ which is the hard part of the theorem. Let $$\mathcal D = \{A \subseteq Y \mid f^{-1} [A] \in \sigma(f^{-1} [\![ \mathcal C ]\!]\}.$$ Then it's easy to show that $\mathcal D$ is indeed a $\sigma$ -algebra over $Y$ and $\mathcal C \subseteq \mathcal D$ . Hence $\sigma(\mathcal C) \subseteq \mathcal D$ . The inclusion then follows.","['measure-theory', 'solution-verification']"
4182201,$\lim_{x\to\infty}$ when the function is defined only on a bounded subset of $\mathbb{R}$,"I know that, for example, if $$f(x)=\frac{e^{-\sin x}}{x}$$ for all real $x\gt 0$ , then $\lim_{x\to\infty}f(x)=0$ . However, how do we compute $\lim_{x\to\infty}$ when the function is defined only on a bounded subset of $\mathbb{R}$ ? If the domain of $g$ is the interval $[0,\pi]$ and $g=\sin$ on $[0,\pi]$ , is it true that $$\lim_{x\to\infty}g(x)=0?$$ It's true if and only if for every $\varepsilon\gt 0$ there exists $M\gt 0$ such that $|g(x)|\lt \varepsilon$ whenever $x\gt M$ . And it seems that for $x\in [0,\pi]$ we can make $|g(x)|$ arbitrarily close to $0$ and at the same time, we don't get to choose from any $x\gt\pi$ . Or is this a misinterpretation? Edit to make this clear:
The definition of limit I am using is: If the domain of $f$ is $D$ , then $$\lim_{x\to\infty}f(x)=L\iff \forall \varepsilon \gt 0\, \exists M\gt 0 \,\forall x\in D:\, x\gt M\implies |f(x)-L|\lt \varepsilon .$$","['limits', 'calculus', 'real-analysis']"
4182207,8 Indistinguishable objects randomly sorted into six buckets - What is the probability of at least three buckets receiving the objects?,"Eight indistinguishable objects are to be randomly put into six buckets. What is the probability that at least three buckets will receive the objects? My approach was to let X= the number of buckets that receive objects, and then the required probability is $P(X≥3) = 1- P(X=1) - P(X=2)$ $= 1 - [(6C1)(1/6)^8 + (6C2)(2/6)^8]$ $= 0.99771$ My reasoning was that for all of the objects falling into one bucket, there are 6 possible buckets $(6C1)$ , and the probability for eight objects to fall into the same bucket in a row is $(1/6)^8 $ By the same logic, for all of the objects to be divided into two of the six buckets, there are $(6C2)$ possible ways to choose those two buckets...And the probability is $(2/6)^8 $ . Then, the probability that at least three buckets will receive objects is 1- (the probability of only one bucket receiving all the objects + two buckets receiving all the objects) However, the answer given to this question was 0.8904.  Where am I going wrong, and is there a better way to approach questions like this? Does it fall into any specific discrete distribution? Note : I also tried $1- [(6C1)(1/6)^8] - (6C2)[(1/6)^7(5/6) + (1/6)^6(5/6)^2 + (1/6)^5(5/6)^3 + (1/6)^4(5/6)^4 + (1/6)^3(5/6)^5 + (1/6)^2(5/6)^6 + (1/6)(5/6)^7]$ And got $0.8721$ . I am not sure if this approach is better.","['statistics', 'combinatorics', 'balls-in-bins', 'probability']"
4182266,Create a function such that...,"I'm reading Kevin Houston's book ""How to Think Like a Mathematician"" and I came across this stumper: Find an example of a non-polynomial function $f:    \mathbb{R}    \rightarrow \mathbb{R}$ such that $f'(x)$ is negative for $x <0$ and positive for $x \ge 0.$ Some functions that immediately came to mind were $f(x)=|x|$ and $f(x)=-\frac{1}{x^2}$ , but in both of these cases $f'$ is undefined at $x=0$ . It seems to me that a graph of $f'$ will have to include a point at the origin extending into the first (NE) quadrant and contain an piece in the third (SW) quadrant that is asymptotic to the y-axis. We $\textit could$ make a piecewise definition and split $f$ into two parts, but this seems to go against the spirit of the question. Ideas?","['functions', 'derivatives']"
4182274,Proving an upper bound on trigonometric polynomial,"Let $k$ be a natural number. Apparently it is possible to show that there is a choice of $0<n_1\leq n_2\leq \cdots \leq n_k$ with $n_i$ integers such that as $k\rightarrow \infty,$ $$
\max_{\theta \in (0,2\pi]} \left|\sum_{i=1}^k \sin n_i \theta\right|\leq ck^{2/3}$$ for some constant $c>0,$ but I cannot find a reference. Edit: Regarding the comment, I am happy with even a sequence $(n_i)$ which changes with $k$ , to start with. For background see this MO question here","['trigonometric-series', 'inequality', 'analysis']"
4182295,problem with implicit derivative using ln,"I have the following expression: $(xy)^{x^{2}}=(\tan y)^{xy^{3}}$ With $y$ being an implicit and differentiable function of $x$ . I want to find an expression for $y'$ . My first attempt is to use Ln function: $x^{2}\ln(xy)=xy^{3}\ln(\tan y)$ . But now I have two options:
a) I use implicit differentiation (and other rules of differentiation) on the above equation.
b) I rewrite the above expression as $x\ln(xy)=y^{3}\ln(\tan y)$ , and then use implicit differentiation . In my opinion, the two options should lead to the same final result. But for my surprise, this is not the case. For a: $ y'=\dfrac{y^{3}\ln(\tan y)-x-2x\ln(xy)}{\dfrac{x^{2}}{y}-3xy^{2}\ln(\tan y)-xy^{3}\dfrac{\sec^{2}y}{\tan y}}$ For b: $ y'=\dfrac{-1-2\ln(xy)}{\dfrac{x}{y}-3y^{2}\ln(\tan y)- y^{3}\dfrac{\sec^{2}y}{\tan y}}$ What am I doing wrong? Which option is correct?","['implicit-differentiation', 'derivatives']"
4182311,"The good, the bad and the ugly with conditional probability/expectation","I thought that I understand conditional probability and expectation until I saw this question: The problem for conditional expectation. Basically, it is given that: $$(X,Y)\sim f(x,y)=\begin{cases}
2xy &\text{ if $0<x<2y<2$} \\
0 &\text{ otherwise }
\end{cases}$$ And it is asked to find $E[Y|X=aY]$ Background: The good I understand Method 2, where one would write: $E[Y|X=aY]=E\left[Y|\frac{X}{Y}=a\right]=E[Y]=\frac{4}{5}$ $\frac{X}{Y}=a$ is dropped from expectation after proving that $Y$ and $\frac{X}{Y}$ are independent using the transformation $(X,Y)\to(X/Y,Y)$ I also (kind of) understand Method 1 in the answer, with conditioning on $Y$ (although my intuition tells me the result is correct only because $Y$ and $X/Y$ are independent): $$E[Y|X=aY]=\int_0^1E[Y|X=aY,Y=y]f_Y(y)\,dy=\int_0^1yf_Y(y)\,dy=E[Y]=\frac{4}{5}$$ But when I try my own intuitive approaches, I'm getting stuck. Question 1: The Bad I interpret the conditional pdf $f_{Y|X}(y|t)=Cf_{Y,X}(y,t)$ as sectioning the joint pdf surface with the plane $y=t$ and scaling the resulting curve to a pdf. My intuition tells me that the conditional pdf of $Y$ given $X=aY$ should similarly be found by sectioning the joint pdf with the plane $x=ay$ and scaling to a pdf. The pdf would be $Cf(ay,y)=Cay^2=3y^2, 0<y<1$ , and the conditional expectation: $E[Y|X=aY]=\int_0^13y^3\,dy=\frac{3}{4}$ What am I missing here? Question 2: The Ugly I'm trying now to do the same thing as in Method 1, but condition on $X$ rather than on $Y$ : $$E[Y|X=aY]=E[X/a|X=aY]=\frac{1}{a}E[X|X=aY]\\=\frac{1}{a}\int_0^aE[X|X=aY,X=x]f_X(x)\,dx=\frac{1}{a}\int_0^a xf_X(x)\,dx$$ Which is something very ugly depending on $a$ , instead of $4/5$ . Again, many thanks for anybody who could point out the mistakes in my thinking.","['conditional-probability', 'probability']"
4182313,"Prove that in quadrilateral $ABCD$, $|\angle DAB|=|\angle ABC|=|\angle BCD|$, $|DA|=|DC|$, if $|BE|=|DA|$, $CE$ is the angle bisector of $\angle BCD$","When trying to solve this problem in the usual way - playing around with angles in the quadrilateral - I always seemed to be lacking one piece of information. I am certain there is something crucial to be extracted out of the fact, that three of the angles and two of the sides are equal. I have never however come across such an object (nor do I know if it even has a name or whether it bears some crucial reseamblance to the parallelogram in terms of properties) so I wondered which useful or peculiar properties might be of interest here. Thank you for your effort.","['quadrilateral', 'geometry']"
4182317,Joint to marginal distribution does not make sense,"Let the joint distribution of $X$ and $Y$ be $$
f_{X,Y}(x,y) = 
\begin{cases}
cxe^{-2y} &0\leq x\leq 1, y> 0 \\
c(2-x)e^{-2y} &1\leq x\leq 2, y> 0 \\
0 &\mathrm{else}
\end{cases}
$$ Find the value of $c$ . $$\underbrace{\int _1^2\int_0^\infty c(2-x) e^{-2y} \, dy \, dx}_{c/4} +  \underbrace{\int _0^1\int_0^\infty cxe^{-2y} \, dy \, dx}_{c/4} = 1 \implies c=2 $$ Are $X$ and $Y$ independent, are $X$ and $Y$ uncorrelated? Hence, I calculated $f_X(x)=c=2$ , but this does not make any sense because now $1 = \int_{-\infty}^{+\infty} f_X(x) \, dx= \int_0^2 f_X(x) \, dx= \Big[2x\Big]^2_0 = 4$ .",['probability']
4182326,Proving $\det A = 1$,"Given a real invertible $2 \times 2$ matrix $A$ with $A + A^{-1} = I$ , I need to prove that $\det A = 1$ . I know how to prove that $\det A = \frac{1}{\det A^{-1}}$ , but don't have access to the fact that the determinant of a sum is the sum of the determinants (but only multiplicativity). Is there another way to prove this?","['matrices', 'proof-explanation', 'determinant', 'linear-algebra']"
4182338,Heat Kernel on a compact manifold without boundary [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I was wondering if the conservation of mass which is obvious for the heat equation in $\mathbb{R}^n$ holds also for the heat kernel in a general compact manifold without boundary. I mean I want to know if the heat kernel generally satisfies $$\int dx K(x,y,t)=1$$ If so, may I have some references to look at? Thanks in advance","['heat-equation', 'manifolds', 'regularization', 'probability']"
4182339,Proposition 4.23 in Royden & Fitzpatrick,"I'm studying Proposition 4.23 (given as follows) in Royden & Fitzpatrick's Real Analysis. $\textbf{Proposition}$ Let $f$ be a measurable function on $E$ . If $f$ is integrable over $E$ ,  then for each $\varepsilon > 0$ , there is a $\delta > 0$ for which \begin{equation} \text{if $A \subset E$ is
measurable and $m(A) < \delta$, then $\int_{A} \lvert f \rvert <\varepsilon$} \tag{26} 
\end{equation} Conversely, if $m(E) < \infty$ ,
if for each $\varepsilon > 0$ , there is a $\delta > 0$ for which (26)
holds, then $f$ is integrable over $E$ . For the ""converse"" part, the authors say that Let $\delta_0 > 0$ respond to the $\varepsilon = 1$ challenge.
Since $m(E) < \infty$ , according to the preceding lemma, $E$ can be expressed as the disjoint union of a finite collection of
measurable subsets $\{E_k\}_{k=1}^{N}$ , each of which has measure
less then $\delta_0$ . Therefore, \begin{equation}
\sum_{k=1}^{N} \int_{E_k}  f  < N 
\end{equation} By the additivity over domains of integration it follows that if $h$ is a nonnegative measurable function of finite support and $0 \leqslant h \leqslant f$ on $E$ , then $\int_{E}h < N$ . Therefore $f$ is integrable. This argument seems a little bit weird to me because I think it is unnecessary to introduce $h$ . Since \begin{equation*}
\int_{E} \lvert f \rvert = \sum_{k=1}^{N} \int_{E_k}  \lvert f \rvert < N 
\end{equation*} we can conclude directly that $f$ is integrable. Do I miss something?","['measure-theory', 'lebesgue-integral', 'real-analysis']"
4182344,"What is the ""fairly easy"" proof that the automorphism group of a Steiner system $S(t,k,n)$ is highly transitive?","I was reading this writeup on the Mathieu groups, and got stuck on a statement in page 2: The Mathieu groups $M_{11}, M_{12}, M_{22}, M_{23},$ and $M_{24}$ are defined as follows: $M_{11}=\{\sigma\in S_{11}: \sigma(S) \in S(4,5,11) \text{ for all } S\in S(4,5,11)\}.$ $M_{12}=\{\sigma\in S_{12}: \sigma(S) \in S(5,6,12) \text{ for all } S\in S(5,6,12)\}.$ $M_{22}=\{\sigma\in S_{22}: \sigma(S) \in S(3,6,22) \text{ for all } S\in S(3,6,22)\}.$ $M_{23}=\{\sigma\in S_{23}: \sigma(S) \in S(4,7,23) \text{ for all } S\in S(4,7,23)\}.$ $M_{24}=\{\sigma\in S_{24}: \sigma(S) \in S(5,8,24) \text{ for all } S\in S(5,8,24)\}.$ [definition of group transitivity] It follows fairly easily from the definitions that $M_{22}$ is $3$ -transitive, $M_{11}$ and $M_{23}$ are $4$ -transitive, $M_{12}$ and $M_{24}$ are $5$ -transitive. I haven't been able to see this apparently-simple proof after thinking about things for a while; it's not apparent to me that a given Steiner system should have any particular symmetries, or indeed that any of these groups are nontrivial. Any insights on how to see these transitivity properties would be welcome. The writeup contains some more information on the construction of these Steiner systems above the quoted section, but at least in the $M_{24}$ case, nothing that provides any obvious group-theoretic symmetries, since it uses the binary lexacode construction. I would therefore assume that the reader is intended to prove that the automorphism group of a Steiner system $S(t,k,n)$ is always $t$ -transitive, but this is not the case when $t=2$ (as discussed in Clapham 1976 ), so the proof must use $t>2$ in some crucial way. I'm at a loss as to what this proof method is, however. (As a side note, this writeup seems to disagree with Wikipedia on whether $M_{22}$ is the automorphism group of the Steiner system, or an index- $2$ subgroup of said automorphism group. Any clarification on this front would be welcome.)","['permutations', 'combinatorial-designs', 'automorphism-group', 'group-theory', 'symmetry']"
4182365,Solution set to non-linear equation of $3$ variables,"I have the following trigonometric equation of $3$ variables: $$f(\theta,\lambda,\phi)=3 \cos (\theta ) \cos (\lambda ) \cos (\phi
   )-(\cos (\theta )+3) \sin (\lambda ) \sin
   (\phi )$$ $$-\sin (\theta ) \cos (\lambda )+\sin
   (\theta ) \cos (\phi )+3 \cos (\theta )+\cos
   (\lambda ) \cos (\phi )-7$$ I want to prove that the solution set to the equation $f(\theta,\lambda,\phi)=0$ is $$S_f=\{(2\pi k,\lambda+2\pi m,-\lambda+2\pi n):k,m,n\in\mathbb{Z},\lambda\in\mathbb{R}\}$$ Graphically, it is easy to see that this is the case but everything I have tried so far has failed to prove the conjecture. Perhaps my best attempt was extrapolating this equation into an unwieldy polynomial of $3$ variables $$P(x,y,z)=9 x^4 y^2+16 x^4-18 x^3 y^2-192 x^3+9 x^2
   y^4+z^4 \left(\left(x^2-1\right)
   y^2+1\right) \left(16 \left(x^2-1\right)
   y^2+(3 x+5)^2\right)-64 x^2 y^2+z^2
   \left((x-1) (x+1) \left(9 x^2+30 x+41\right)
   y^4+2 x (x (48 (x-5) x+187)+234) y^2+x (x (9
   (x-2) x-64)-78)-74 y^2+151\right)+736 x^2+2
   y z^3 \left(9 x^4+12 x^3-20 x^2+(x-1) (x+1)
   (x (31 x-78)-33) y^2-96 x-33\right)+2 y z
   \left(31 x^4-270 x^3+560 x^2+(x (x (3 x (3
   x+4)-20)-96)-33) y^2-210 x-111\right)+30 x
   y^4-78 x y^2-960 x+25 y^4+151 y^2+400$$ over the domain $(x,y,z)\in [-1,1]^3$ . If I can prove that the solution set to $P(x,y,z)=0$ over this domain is $$S_P=\{(1,y,y):y\in[-1,1]\}$$ then the original conjecture would be solved. The motivation behind this is actually proving a certain type of quantum error detection encoding exists. It's a little difficult to explain (although if anyone wants details I am more than happy to provide them) but suffice to say that after a lot of work I managed to whittle my existence proof down to the conjecture above.","['algebra-precalculus', 'polynomials', 'trigonometry']"
4182377,Connections between topology and combinatorics,"I recently learned that both Brouwer's fixed point theorem and Borsuk-Ulam admit fairly simple graph-theoretic proofs. While I am able to follow the gist of both proofs, I find myself yearning for some kind of larger context to fit them in. Is there any sort of general machinery for converting topological problems (or, at least, some class of topological problems) into combinatorial ones that these follow from?","['graph-theory', 'general-topology', 'combinatorics']"
4182381,Finding out the perimeter of the ellipse?,"I was just messing around with the maths when I realised a way of finding the perimeter of the ellipse. First you start by representing any given point in the ellipse in this way: $$ \vec{r} = \begin{pmatrix} 
a \cos{\theta} \\
b \sin{\theta}
\end{pmatrix}
$$ Taking in account that a represents the semi-major axes of the ellipse and b the semi-minor axes. Then, you find the distance between two points $$
\Delta s^2 = \Delta x^2 + \Delta y^2 \\
$$ Take the sum of all of this distances: $$
\lim_{n \to \infty} \sum_{i=1}^{n} \sqrt{\Delta x_i^2 + \Delta y_i^2} \equiv \int_{a}^{b} \sqrt{dx^2 + dy^2}
$$ Parametrize the curve with respect to $\theta$ : $$
\int_{a}^{b} \sqrt{\left ( \frac{dx}{d\theta} \right ) ^2 + \left (\frac{dy}{d\theta} \right ) ^2 } \;d\theta
$$ $$
\frac{dx}{d\theta} = a \frac{d \cos{\theta}}{d \theta} = -a \sin{\theta}
$$ $$
\frac{dy}{d\theta} = b \frac{d \sin{\theta}}{d \theta} = b \cos{\theta}
$$ And finally you get this formula: $$
\int_{0}^{\varphi} \sqrt{ a^2 \sin^2({\theta}) + b^2 \cos^2({\theta})} 
  \;d\theta  
$$ Taking in account that $\varphi$ represents until what ""angle"" you want to find the perimeter (if you could talk about angles in an ellipse) I used an online numerical integrator to try to find a solution. This is my formula compared to Ramanujan's one: https://www.desmos.com/calculator/f3qcjauuq3 . Both values are pretty close to each other. Also I've tried to solve this, when $a = b$ . In other words, when the ellipse is no more an ellipse and it is a circunference. Giving me a good and coherent result: $$a = b = r$$ $$
\int_{0}^{2 \pi} \sqrt{ r^2 \sin^2({\theta}) + r^2 \cos^2({\theta}) }  \;d\theta = \int_{0}^{2 \pi} r\sqrt{\sin^2({\theta}) + \cos^2({\theta}) }  \;d\theta =
\int_{0}^{2 \pi} r d\theta = \theta r \Big|_0^{2 \pi} = 2 \pi r - 0 r = 2 \pi r
$$ $$
\therefore \int_{0}^{2 \pi} \sqrt{ r^2 \sin^2({\theta}) + r^2 \cos^2({\theta}) }  \;d\theta = 2 \pi r
$$ The thing is, I have trouble in finding the antiderivative of the ellipse formula. What are the methods that I could use to find it? Could you give me any exact solutions? I could give further explanation if needed, with pictures and animations, etc. Note : Correct me if I made any mistake or if I forgot something in the process. Thanks for the help :)","['integration', 'calculus', 'conic-sections', 'geometry']"
4182382,Polynomials with integer coefficients that can be iterated for infinitely many times and the result is always a prime number,"Denote $f^{(n)}(x)=\underbrace{f(f(f(f(\cdots f}_{n \;\text{times}}(x)))))$ . Does there exists a polynomial function $f(x)=a_nx^n+a_{n-1}x^{n-1}+\cdots+a_1x+a_0$ in $\mathbb{Z}[x]$ such that $\forall n\in \mathbb{N}$ , $f^{(n)}(1)$ are all prime numbers, and $\forall i,j\in \mathbb{N}, f^{(i)}(1)\neq f^{(j)}(1)$ ? I think we can't find a polynomial function that satisfy the above identity. Observation : $f(x)=x^2+x+1$ , $f(1)=3, f^{(2)}(1)=13, f^{(3)}(1)=183$ . $3\mid 183$ . $\;\;\;\;\;\times$ $f(x)=x^2+3x+1$ , $f(1)=5, f^{(2)}(1)=41, f^{(3)}(1)=1805$ . $5\mid 1805$ . $\;\;\;\;\;\times$ $f(x)=3x^2-x+1$ , $f(1)=3, f^{(2)}(1)=25, f^{(3)}(1)=1851$ . $3\mid 1851$ . $\;\;\;\;\;\times$ $f(x)=x^3+2x^2+x+1$ , $f(1)=5, f^{(2)}(1)=181, f^{(3)}(1)=5995445, f^{(4)}(1)=215508505260670237621$ . $5\mid f^{(3)}(1)$ . $\;\;\;\;\;\times$ $f(x)=x^3+3x^2+5x+2$ , $f(1)=11, f^{(2)}(1)=1751, f^{(3)}(1)=5377774511, f^{(4)}(1)=155527705682516977725188134751, f^{(5)}(1)=3762039024097970068792517560792353596800152524223246433216608553915834472630739126136511.$ $11\mid f^{(5)}(1)$ . $\;\;\;\;\;\times$ After my observation, I found that at some moment, $f(1)$ always divide $f^{(n)}(1)$ . How can I prove it? (Not a full) Proof : BWOC. If there exists a polynomial function $f(x)=a_nx^n+a_{n-1}x^{n-1}+\cdots+a_1x+a_0$ in $\mathbb{Z}[x]$ such that $\forall n\in \mathbb{N}$ , $f^{(n)}(1)$ are all prime numbers, then $\displaystyle{f(1)=\sum_{i=0}^{n}a_i}$ is a prime number. Notice if $f(1)=2$ , then $2\nmid f^{(2)}(1)$ , so $2\mid f^{(3)}(1)$ , so $f(1)$ should be an odd prime number. ....... Actually, I have no idea what to do next, but I guess that proof should be highly correlated with ""iteration"". Note : $\mathbb{N}=\mathbb{Z}^+$ If there are mistakes in my question, I'm willing to edit it. Hope it's not a duplication.","['number-theory', 'polynomials', 'prime-numbers']"
4182391,Draw the Ellipsoid $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1$ a tangent plane which cuts off equal segments on the coordinate axis.,"Draw the Ellipsoid $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1$ a tangent plane which cuts off equal segments on the coordinate axis. First, I want to find the normal line of the plane which intersects the $x,y,z$ at some distance $h$ away from the origin. $$<-h,h,0> \times<-h, 0, h> = <h^2, h^2, h^2> = h^2<1,1,1>$$ So I now need to find when there is a point where the gradient is pointing in that direction $$\triangledown f = <\frac{2x}{a^2},\frac{2y}{b^2},\frac{2z}{c^2}>$$ and the point must also satisfy the ellipsoid equation. $x = ka^2/2, y = kb^2/2,z=kc^2/2$ plugging into the ellipsoid equation yields a $$k = \frac{2}{\sqrt{a^2+b^2+c^2}}$$ which makes the gradient $$\triangledown f = <\frac{a^2}{\sqrt{a^2+b^2+c^2}},\frac{b^2}{\sqrt{a^2+b^2+c^2}},\frac{c^2}{\sqrt{a^2+b^2+c^2}}>$$ knowing $k$ I can also find the $<x,y,z>$ position and use $\triangledown f \cdot<x-x_o,y-y_o,z-z_o> =0$ . I get a horrific result of : $$a^2x+b^2y+c^2z=\frac{a^4+b^4+c^4}{\sqrt{a^2+b^2+c^2}}$$ Which is not correct. Could someone aid in creating a solution?","['multivariable-calculus', 'vector-analysis']"
4182451,"Let $f : R → R$ be a continuous decreasing function. Prove that the system $x = f(y),$ $y = f(z),$ $z = f(x)$ has a unique solution.","Let $f : R → R$ be a continuous decreasing function. Prove that the system $$x = f(y),$$ $$y = f(z),$$ $$z = f(x)$$ has a unique solution. Here using the intermediate value theorem, it can clearly be seen that when $\displaystyle{\lim_{x \to -\infty}}f(x)-x$ = $\infty $ & $\displaystyle{\lim_{x \to \infty}}f(x)-x$ = - $\infty $ & we get some $x_0$ such that $f(x_0) = x_0$ But how or why does that imply that $x_0 = x = y = z$","['limits', 'calculus']"
4182453,Is my proof for $(A \cap B) \cup C = A \cap (B \cup C)$ only when $C \subseteq A$ correct?,"I was working through the Chapter 3 supplementary exercises in Grimaldi's textbook, and I wanted to see if my way of answering a problem is correct.  The exercise asks you to prove $(A \cap B) \cup C = A \cap (B \cup C)$ if and only if $C \subseteq A$ .  The book answer has a rather involved proof, but in my mind it's rather simple.  I'm fairly positive that I'm missing something, though, so I wanted to share my proof for others' thoughts. Beginning with $(A \cap B) \cup C$ , by the distributive law we get $(A \cup C) \cap (B \cup C)$ .  Now, if $(A \cup C) \cap (B \cup C) = A \cap (B \cup C)$ , that can only be true when $A \cup C = A$ .  Otherwise, the intersection of $A \cup C$ and $B \cup C$ would include any elements in $C$ but not in $A$ , which means the intersection of $(A \cup C) \cap (B \cup C)$ would necessarily have more elements (because it would include other elements in $C$ ) than what is in $A \cap (B \cup C)$ alone. Therefore, for those two intersections to be equal, $A \cup C = A$ .  And if $A \cup C = A$ , then by definition $C \subseteq A$ .  Thus, we're done. Any thoughts would be greatly appreciated.  Thank you!",['elementary-set-theory']
4182509,Are there arbitrarily long prime gaps in which each number has at least three distinct prime factors?,"Definition: Highly composite prime gap The three composite numbers between the consecutive primes $643$ and $647$ each have at least three distinct prime factors. This is the first occurrence of prime gap of length $> 1$ where each composite number in the gap has at least $k = 3$ distinct prime factors. We call prime gap between $643$ and $647$ as the highly composite prime gap of order $3$ . We have the highly composite prime gaps for $k = 3,4,5$ and $6$ as follows: $k = 3; p = 643$ $k = 4; p = 51427$ $k = 5; p = 8083633$ $k = 6; p = 1077940147$ $k = 7; p = 75582271489$ Question 1 : Are there infinitely many highly composite prime gaps of order $k \ge 3$ ? Question 2 : Given $k$ is there always a highly composite gap of order $k$ ? An ordinary linear regression between $k$ and $\log p$ gives a surprisingly strong fit with $R^2 \approx 0.99915$ . Although it is based on only six data points, this suggests a relationship of the form $p \sim ab^k$ forsome fixed $a$ and $b$ . Definition : Maximal highly composite gap The maximal highly composite gap is defined as a prime gap which is longer than any previous gap and each composite in the gap has at least $3$ distinct prime factors. Update : The longest such gap I have found is of $75$ consecutive composite between the primes $535473480007$ and $535473480083$ . Question 3 : Are there arbitrarily long prime gaps in which each composite number in the gap has at least three distinct prime factors? Update : Posted in MO since it is unanswered in MSE.","['number-theory', 'analytic-number-theory', 'computational-mathematics', 'sequences-and-series', 'prime-numbers']"
4182523,"Prove that there exists $k>1$ and $x_1, x_2, \dots ,x_k$ such that $\sum_{i=1}^k\phi(x_i)$ and $\sum_{i=1}^kx_i$ are both $n$th powers of an integer","I got an interesting question which asks Consider a fixed positive integer $n>1$ . Prove that there exists an
integer $k>1$ and distinct integers $x_1, x_2, \dots ,x_k$ , all
greater than $1$ , such that $\sum_{i=1}^k\phi(x_i)$ and $\sum_{i=1}^kx_i$ are both $n$ th powers of an integer. Here $\phi$ is the Euler
totient function. I am familiar with the basic elementary properties of the $\phi$ function, and I tried to write down the elaborate formula of $\phi(n)$ and manipulate the two sums to arrive at a conclusion. But that didn't help. Things became too ugly too soon. I also tried to look at $\sum_{i=1}^k(\phi(x_i)-x_i)$ but couldn't proceed anywhere. Any help would be appreciated.","['number-theory', 'algebraic-number-theory', 'elementary-number-theory']"
4182586,Set of all circles in cartesian plane without intersection is denumerable,"I want to prove this problem: Prove the set of all circles in cartesian plane(Call that P) that they have not any intersection or none of them is in another one is denumerable . And I must state that our coordinates is in R not Q! For rational numbers we can denote a function such that all circles map to their center coordinates $(x,y)$ and radius $(z)$ so finally every circle will be mapped into $(x,y,z)$ : $$f:P→ Q\times Q\times Q$$ $$f(c)=(x,y,z) ~~s.t~~c∈P$$ Its obvious that f is bijection.
So the function range is in $Q\times Q\times Q$ . And we know that $Q\times Q\times Q\sim N$ . So set of all circles with rational coordinates is denumerable and because we want no intersection , that is a subset of all circles and we know infinite subset of denumerable set is denumerable. As you see I know how to prove it using rational numbers but for real numbers I have no any ideas.",['elementary-set-theory']
4182591,New non-overlapping finite cover,"I need help in the following problem Let $B_{i}$ be a finite family of open balls in $\mathbb{R}^n$ . Show that there exists a finite family of disjoint open balls $B'_{k}$ such that $\bigcup_{k}{B'_{k}} ⊃ \bigcup_{i}{B_{i}}$ and $\sum_{k} {r(B'_{k})} ≤ \sum_{i} r(B_{i})$ where $r$ is the radius. For the disjoint balls $B_{i}$ will keep them, what about the overlapping ones? How I can find this new cover? and does the result remain valid if we have a countable family of open balls with finite sum of the radii instead?","['measure-theory', 'real-analysis']"
4182646,"If $\langle u(t), \phi \rangle \in L^1\ \forall \phi \in S(\Bbb R^d)$, does $\langle u(t), \phi(t) \rangle \in L^1\ \forall \phi \in S(\Bbb R^{1+d})$?","Let $S(\mathbb R^d)$ be the space of Schwartz functions and $S'(\mathbb R^d)$ be the space of tempered distributions (the continuous linear functionals from $S(\mathbb R^d)$ to $\mathbb R$ ) endowed with the weak topology. Assume that $I \subset \mathbb R$ is a compact interval and that $u : I \rightarrow S'(\mathbb R^d)$ is a family of tempered distributions such that $$\langle u(t), \phi \rangle \in L^1(I) \ \forall \phi \in S(\mathbb R^d)$$ Does it follow that $\langle u(t), \phi(t, \cdot) \rangle \in L^1(I) \ \forall \phi \in S(\mathbb R^{1+d})$ ? I've already asked this question in mathoverflow , but I've got no answer so far. This question is also related to this other question of mine . I'm asking this question because it is proved in Chapter 4 of ""Eléments de distributions et d'équations aux dérivées partielles"" by Claude Zuily that if $$\langle u(t), \phi \rangle \in C^0(I) \ \forall \phi \in S(\mathbb R^d)$$ then $\langle u(t), \phi(t, \cdot) \rangle \in C^0(I) \ \forall \phi \in S(\mathbb R^{1+d})$ as well. It is a consequence of Banach-Steinhaus. For the sake of completeness, I'm rewriting the proof for the $C^0$ case below. I was wondering if we can do something similar for $L^1$ space. Any reference is welcomed. One last remark : Observe that $\langle u(t), \phi(t, \cdot) \rangle$ is always the pointwise limit of $L^1(I)$ functions, hence it is Lebesgue measurable. Indeed, if $\phi(t,x) = a(t)b(x)$ for $a \in S(\mathbb R)$ , $b \in S(\mathbb R^d)$ , then $$\langle u(t), \phi(t, \cdot) \rangle = \langle u(t), b \rangle a(t) \in L^1(I)$$ For a general $\phi(t,x)$ , it can be shown (see here for example) that it is the limit in $S(\mathbb R^{1+d}$ ) of a sequence $\phi_n(t,x)$ , where each $\phi_n$ is a finite linear combination of elements of the form $a(t)b(x)$ , $a \in S(\mathbb R)$ , $b \in S(\mathbb R^d)$ . Proof of Claude Zuily for the $C^0$ case : We prove continuity of $\langle u(t), \phi(t, \cdot) \rangle$ at $t_0 \in I$ . Let $(t_n)_{n=1}^{\infty} \subset I$ converge to $t_0$ . We define $u_j := u(t_j)$ and $\phi_j := \phi(t_j, \cdot)$ . By assumption, $u_j$ converges to $u(t_0)$ in $S'(\mathbb R^d)$ . In particular $$\sup_{j \in \mathbb N} | \langle u_j, \psi \rangle | < +\infty \ \forall \psi \in S(\mathbb R^d)$$ Banach-Steinhaus theorem implies that there exists a continuous semi-norm $\rho : S(\mathbb R^d) \rightarrow \mathbb R$ such that $$| \langle u_j, \psi \rangle | \leq \rho(\psi) \ \forall j \in \mathbb N \ \forall \psi \in S(\mathbb R^d)$$ Moreover, it is easy to check that $\phi_j$ converges to $\phi(t_0, \cdot)$ in $S(\mathbb R^d)$ . Hence, $$| \langle u_j, \phi_j \rangle - \langle u(t_0), \phi(t_0, \cdot) \rangle | \leq | \langle u_j, \phi_j - \phi(t_0, \cdot) \rangle | +  | \langle u_j, \phi(t_0, \cdot) \rangle - \langle u(t_0), \phi(t_0, \cdot) \rangle| $$ and everything goes to zero as $j \to +\infty$ .","['schwartz-space', 'lebesgue-integral', 'functional-analysis', 'distribution-theory']"
4182757,Proj of Almost Same Graded Rings are Isomorphic (Exercise from Vakil's FOAG),"I'm trying to solve Exercise 6.4.F from Vakil's FOAG : 6.4.F. Exᴇʀᴄɪsᴇ. $\quad$ Show that if $R_\bullet$ and $S_\bullet$ are the same finitely generated graded rings except in a finite number of nonzero degrees (make this precise!), then $\operatorname{Proj} R_\bullet \cong \operatorname{Proj} S_\bullet$ . First, I think what Vakil means when he says same finitely generated rings except in a finite number of nonzero degrees is that $R_{\bullet}$ and $S_{\bullet}$ have all their homogeneous pieces ( $R_n$ and $S_n$ respectively) the same, except for finitely many $n$ . Could someone please verify if this is right? Second, assuming the above, here's how I would solve this exercise: since $S_n$ and $R_n$ are identical for all large $n$ , if we consider $m \gg 0$ , then we have that $R_{m\bullet}$ and $S_{m\bullet}$ are identical. Thus $\operatorname{Proj} S_{m\bullet}\cong\operatorname{Proj} R_{m\bullet}$ . But Exercise 6.4.D shows that $\operatorname{Proj} S_{m\bullet}\cong \operatorname{Proj} S_{\bullet}$ , and we have a similar isomorphism for $R_{\bullet}$ . Thus, $\operatorname{Proj} S_{\bullet}\cong \operatorname{Proj} R_{\bullet}$ and we are done. Now I'm also not sure if this solutions works, because I don't seem to be using the ""finitely generated"" hypothesis anywhere in my above ""proof"". I would be glad if someone would point out what I am missing.","['commutative-algebra', 'projective-schemes', 'algebraic-geometry', 'solution-verification', 'schemes']"
4182764,"If $x^{19}+x^{17}+x^{13}+x^{11}+x^{7}+x^{5}+x^{3}$ is divided by $(x^2 +1)$, then find the remainder","If the polynomial $x^{19}+x^{17}+x^{13}+x^{11}+x^{7}+x^{5}+x^{3}$ is divided by $(x^ 
2
 +1)$ , then the remainder is: How Do I solve this question without the tedious long division? Using remainder theorem , we can take $x^3$ common and put $x^2 =-1$ although $x$ is not a real number. By this method, I got the right answer as $-x$ . Is it the right way? Because $x$ comes out to be $i$ which is not real. Also , can I apply remainder theorem to quadratic divisor polynomials in this way?","['elementary-number-theory', 'algebra-precalculus', 'polynomials']"
4182792,Prove that $\bigcup\limits_{i=1}^{n} B_{i}=\bigcup\limits_{i=1}^{n} A_{i}$,"It is given that $\{{A_n\}}$ is a monotone nondecreasing sequence of sets while $\{{B_n\}}$ is defined as: $B_1=A_1,B_2=A_2-A_1$ or in general: $B_n=A_n-A_{n-1}$ . Prove that $\bigcup\limits_{i=1}^{n} B_{i}=\bigcup\limits_{i=1}^{n} A_{i}$ for any $n\in\{2,3,\dots\}$ by mathematical induction. I was already able to prove that $\bigcup\limits_{i=1}^{2} B_{i}=\bigcup\limits_{i=1}^{2} A_{i}$ by using the definitions & properties of sets. For step 2, I have $\bigcup\limits_{i=1}^{k} B_{i}=\bigcup\limits_{i=1}^{k} A_{i}$ where $B_k=A_k-A_{k-1}$ .
For step 3, show $\bigcup\limits_{i=1}^{k+1} B_{i}=\bigcup\limits_{i=1}^{k+1} A_{i}$ that is, $\bigcup\limits_{i=1}^{k+1} B_{i}= A_{k+1}$ . $$\bigcup\limits_{i=1}^{k+1} B_{i}=(A_{k+1}-A_k)$$ I get lost with how to show that this is equal to $A_{k+1}$ . $$(A_{k+1}-A_k)=A_{k+1}~\cap~A_k^c$$ Would it be right to assume that $A_k^c=\Omega$ ? Please let me know if I've missed something.","['elementary-set-theory', 'proof-writing']"
4182845,"Basic workings of probability theory on $\mathcal{C}([0,1])$","I am confused as to how exactly the underlying probablity space $(\Omega,\mathscr{F},\mathbb{P})$ relates to a probability measure $\mu:\mathcal{B}(\mathcal{C})\longrightarrow[0,1]$ where $\mathcal{C}:=\mathcal{C}([0,1])$ is the class of continuous functions $f:[0,1]\longrightarrow\mathbb{R}$ and $\mathcal{B}(\mathcal{C})$ is the Borel $\sigma$ -field of subsets of $\mathcal{C}$ . As per the excellent question and answer here Natural & important probability measures on $\mathcal{C}[0,1]$, in particular the Wiener measure , I have a natural preference to avoid, at least initially, the stochastic process viewpoint. However in case this is relevant to my questions, my understanding is as follows: for each $f\in\mathcal{C}$ and for fixed $t\in[0,1]$ , $f_{t}(\omega):\Omega\longrightarrow\mathbb{R}$ is a random variable as $\omega$ runs through $\Omega$ if $f$ is $\mathscr{F}/\mathscr{R}$ measurable ( $\mathscr{R}$ being the linear Borel sets). I think each $f$ are measurable since continuity implies measurability. On the other hand for fixed $\omega\in \Omega$ , $f_{t}(\omega):[0,1]\longrightarrow\mathbb{R}$ is a path as $t$ runs through $[0,1]$ , i.e. a realisation of a real valued function on $[0,1]$ . Thus using the dot notation to signify a variable, $f_{t}(\cdot)$ is a random variable at $t$ and $f_{(\cdot)}(\omega)$ is the entire path of $f$ at $\omega$ . Furthermore $(f_{t}(\omega))_{\omega,t}:=(f_{t}(\omega))_{\omega\in\Omega,t\in[0,1]}$ is a stochastic process $f:\Omega\times[0,1]\longrightarrow\mathbb{R}$ . Random elements in $\mathcal{C}$ I now, perhaps naively, apply my knowledge of how the basics of probability theory work for $X$ - i.e. a random variable in a standard probability set-up) Suppose $\tilde{F}:\Omega\longrightarrow\mathcal{C}$ is a $\mathscr{F}/\mathcal{C}$ -measurable mapping between the measurbale spaces $(\Omega,\mathscr{F})$ and $(\mathcal{C},\mathcal{B}(\mathcal{C}))$ , i.e. for each $\omega\in\Omega$ , $\tilde{F}(\omega)\mapsto \tilde{f}$ for some $\tilde{f}\in\mathcal{C}$ . Then $\tilde{F}$ is a random element in $\mathcal{C}$ and is analagous to a $\mathscr{F}/\mathscr{R}$ -measurable mapping $X(\omega)\mapsto x$ for some $x\in\mathbb{R}$ - i.e. a standard random variable. According to Billinglsey (Convergence of Probability Measures, 2nd Ed, 1999) elements can mean scalars, vectors, sequences as well, and I am trying to follow this general language so I understand probability theory applied to metric spaces (of which $(\mathcal{C},d_{\infty})$ where $d_{\infty}$ is the sup metric is the most general I wll probably ever have the need to consider). The open $d_{\infty}$ -balls $B_{\epsilon}(f)=\{g\in\mathcal{C}:d_{\infty}(f,g)<\epsilon\}$ centred at $f$ of radius $\epsilon$ are conceptually the same as in Euclidean space: i.e. the class of $d_{\infty}$ -open sets of $\mathcal{C}$ denoted $\mathcal{U}(\mathcal{C})$ satisfy each $U\in \mathcal{U}(\mathcal{C})$ being a union of open balls, and the metric space $(\mathcal{C},d_{\infty})$ is separable and complete. The Borel $\sigma$ -field $\mathcal{C}$ is crucially also generated by $\mathcal{U}(\mathcal{C})$ (this is crucial in the sense that oftentimes proving a result on the generating class then extends to the entire $\sigma$ -field). However here I do not need to use such properties here. Let $\require{enclose}
     \enclose{horizontalstrike}{\mu_{\tilde{F}}:\mathcal{C}\longrightarrow[0,1]}$ $\mu_{\tilde{F}}:\mathcal{B}(\mathcal{C})\longrightarrow[0,1]$ be the probability law of $\tilde{F}$ : what does this mean and how does $\mu_{\tilde{F}}$ assign probabilities to elements in $\mathcal{C}$ ?; $$\mu_{\tilde{F}}(B)=\mathbb{P}\left\{\omega\in\Omega:\tilde{F}(\omega)\in B\right\}\hspace{20pt}\text{ for all }B\in\mathcal{B}(\mathcal{C})$$ In words: $B$ is a set of continous real-valued functions on $[0,1]$ and $\tilde{F}(\omega)\in B$ means the entire path $\tilde{F}(\omega):=\tilde{f}_{(\cdot)}(\omega)$ at $\omega$ lies in $B$ . The above often gets shortened to $\mu_{\tilde{F}}(B)=\mathbb{P}\{\tilde{F}\in B\}=\mathbb{P}\{\tilde{F}^{-1}(B)\}$ . Measurable mappings and intergals of Measurable mappings of random elements in $\mathcal{C}$ If my understanding about random elements is OK, how are measurable mappings of these random elements defined, and furthermore how are intergals of these mappings of elements defined? Once again I now perhaps naively apply my knowledge of how this works for $X$ ) . Let $\phi$ be a positive-valued $\mathcal{C}/\mathscr{R}$ -measurable function (i.e. function of functions such as $\phi(\tilde{f})\mapsto\text{max}\left\{|\tilde{f}_{t}|:t\in[0,1]\right\}$ ). Then $\phi(\tilde{F})=\phi\circ \tilde{F}$ is a $\mathscr{F}/\mathscr{R}$ -measurable mapping (theorem 13.1 (ii), Billingsley 1995, Proabability and Measure, 3rd Ed), and as far as I can tell, I am free now to apply standard measure theory pertaining to the integral of random variables to this mapping - i.e. no new theory of integration required (Billingsley 1995 uses a quite general set-up so perhaps here I am benefiting from investing in this extra generality). Using the defintion of expectation of random variables, the change of variable theorem (lines 5 and 6), and the definition of the integral for positive mappings; $$\begin{align*}
\require{enclose}
     \enclose{horizontalstrike}{\mathbb{E}_{\mu_{\tilde{F}}}\left[\phi(\tilde{F})\right]
}\mathbb{E}_{\mu_{\tilde{F}}}\left[\phi\right]&:=\int_{\mathcal{C}}\phi(g)\mu_{\tilde{F}}(dg)\\
&:=\text{sup}\sum_{i}\left\{\left[\underset{g\in B_{i}}{\text{inf}}\phi(g)\right]\mu_{\tilde{F}}(B_{i})\right\}\\
&=\text{sup}\sum_{i}\left\{\left[\underset{g\in B_{i}}{\text{inf}}\phi(g)\right]\mathbb{P}\left[\omega:\tilde{F}(\omega)\in B_{i}\right]\right\}\\
&=\text{sup}\sum_{i}\left\{\left[\underset{g\in B_{i}}{\text{inf}}\phi(g)\right]\mathbb{P}\left[\tilde{F}^{-1}(B_{i})\right]\right\}\\
&=\int_{\mathcal{C}}\phi(g)\mathbb{P}\circ\tilde{F}^{-1}(dg)\\
&=\int_{\Omega}\phi(\tilde{F}(\omega))\mathbb{P}(d\omega)\\
&:=\text{sup}\sum_{i}\left\{\left[\underset{\omega\in F_{i}}{\text{inf}}\phi(\tilde{F}(\omega))\right]\mathbb{P}(F_{i})\right\}\\
&:=\mathbb{E}_{\mathbb{P}}\left[\phi(\tilde{F})\right],
\end{align*}$$ where the supremums extend over all finite decompositions $\{B_{i}\}$ of $\mathcal{C}$ into $\mathcal{B}(\mathcal{C})$ -sets and all finite decompositions $\{F_{i}\}$ of $\Omega$ into $\mathscr{F}$ -sets. This all seems perfectly reasonable to me but please do point out where I have gone wrong.","['measure-theory', 'borel-measures', 'metric-spaces', 'functional-analysis', 'probability-theory']"
4182854,How do I prove this inequality $3(a+b+c+1)\ge 4 \left( \sqrt{\frac{a^2+1}{a+1}}+\sqrt{\frac{b^2+1}{b+1}}+\sqrt{\frac{c^2+1}{c+1}} \right)$?,"If $a,b,c\gt 0$ and $abc=1$ , how do I prove the following inequality $3(a+b+c+1)\ge 4 \left( \sqrt{\frac{a^2+1}{a+1}}+\sqrt{\frac{b^2+1}{b+1}}+\sqrt{\frac{c^2+1}{c+1}} \right)$ ? My version: \begin{gathered}
\sqrt{\frac{a^{2}+1}{a+1}} \leq \sqrt{1+\frac{a^{2}-a}{a+1}} \leq 1+\frac{1}{2} \frac{a^{2}-a}{a+1}=\frac{a}{2}+\frac{1}{a+1} \\
\Leftrightarrow 4\left(\frac{1}{1+a}+\frac{1}{1+b}+\frac{1}{1+c}\right) \leq 3+(a+b+c), a b c=1 \\
a+b+c=3 t^{2}=p \geq 3, a b+b c+c a=q \geq \sqrt{3 p r}, a b c=r=1 \\
4\left(1+\frac{1+p}{2+p+q}\right) \leq 4\left(1+\frac{1+p}{2+p+\sqrt{3 p}}\right) \leq 3+p \\
(t-1)\left(3 t^{3}+6 t^{2}+3 t+2\right) \geq 0
\end{gathered}","['algebra-precalculus', 'inequality']"
4182875,Inverse function of $g(x) = x/\sqrt{\log(1/x)}$.,"Is there a known inverse function for $$ g(x) = x/\sqrt{\log(1/x)}, \quad x \in (0,1) $$ at least for part of its domain? At a first glance this looks suspiciously like the definition of the Lambert W function but this does not seem so simple. From numerical evidence, I can tell that the inverse behaves essentially like the identity function for small values of $y$ in $g(x) = y$ . Thank you in advance.","['functions', 'inverse']"
4182895,"$T_1$ spaces and ""rank"" of a set",For a $T_1$ space $X$ define $\text{rank}(A) = \min\{\kappa : A\text{ is union of }\kappa\text{ closed sets}\}$ . Clearly $\text{rank}(A) \leq 1$ iff $A$ is closed and $\text{rank}(A) \leq \aleph_0$ iff $A$ is a $F_\sigma$ set. It follows that $\text{rank}(\mathbb{R}\setminus\mathbb{Q}) \geq \aleph_1$ for $X = \mathbb{R}$ . Can we find rank of irrationals in ZFC?,"['general-topology', 'set-theory']"
4182937,"Conditional distribution for uniform $U$ given $\max\{U_1,U_2,\dots,U_n\}$","Let $U_1,U_2,\dots,U_n$ be i.i.d. uniform random variables on the interval $[a,b]$ . Set $Z := \max\{U_1,U_2,\dots,U_n\}$ . How can I determine the conditional distribution $\mathbb{P}(U_i \mid Z = x)$ . I think it must be the uniform distribution on the interval $[a,x]$ . If so, how can I prove this formally?","['conditional-probability', 'probability-distributions', 'uniform-distribution', 'probability-theory']"
4183024,A series representation for $e^x$,"I want to show that for $x \notin \{2,3,4,\dots\}$ $$e^x = \frac{2+x}{2-x} + \sum_{k=2}^{\infty} \frac{-x^{k+1}}{k!(k-x)(k+1-x)}$$ It is pretty clear that why $x \notin \{2,3,\dots\}$ . I proved that the series converges for all other $x$ by using the ratio test. My initial thoughts were to take $$ f(x) = \frac{2+x}{2-x} + \sum_{k=2}^{\infty} \frac{-x^{k+1}}{k!(k-x)(k+1-x)}$$ and find $f'(x)$ by doing term by term by differentiation and since we know that $f(0) = 1$ we may be able to find $f(x)$ . I suspect that we can not do this without proving that term by term differentiation is valid. Neverthless I tried to do that also but got stuck here. $$ f'(x) = \frac{4}{{\left(x - 2\right)}^{2}} - \sum_{k=2}^{\infty}\left( \frac{{\left(k + 1\right)} x^{k}}{{\left(k - x + 1\right)} {\left(k - x\right)} k!} + \frac{x^{k + 1}}{{\left(k - x + 1\right)} {\left(k - x\right)}^{2} k!} + \frac{x^{k + 1}}{{\left(k - x + 1\right)}^{2} {\left(k - x\right)} k!} \right)$$ How can I show that this term by term differentiation is valid and how can I proceed further? Can someone suggest some other ways to do it?","['calculus', 'exponential-function', 'sequences-and-series']"
4183028,Sensitivity of geometric median to moving one point,"The geometric median of a finite set $S$ of points in the Euclidean plane is defined as the point $m$ for which $\sum_{s\in S} d(m,s)$ is minimized, where $d(x,y)$ is the Euclidean distance between $x$ and $y$ . As long as $S$ is not colinear, the geometric median is unique. My question is this; if you replace a point $s\in S$ with some $s'$ for which $d(s,s')<\epsilon$ , will the geometric median of the new set be at most $\epsilon$ away from the geometric median of the original? The reason I want to know is that I could use this result in order to solve the puzzle here: Algorithm for people to congregate with limited relative location information . I have verified this is true when $|S|=3$ and $|S|=4$ , where the geometric median can be described explicitly. For example, when $S$ is the set of vertices of a convex quadrilateral, it can be shown the median of $S$ is the intersection of the diagonals. This intersection moves by less than $\epsilon$ when any vertex moves by $\epsilon$ .","['convex-optimization', 'statistics', 'geometry']"
