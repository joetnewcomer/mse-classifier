question_id,title,body,tags
3419972,Artin Schreier equation,Let $K$ be the field obtained by adjoining to $\mathbb{Q}_p$ a root $\alpha$ of the polynomial $f(x)=x^p-x-\dfrac{1}{p}$ . I should prove that $K \supset \mathbb{Q}_p$ is a Galois extension of degree $p$ . I managed to prove that $K$ has degree $p$ :taking $\dfrac{1}{\alpha}$ we see that is a root of $x^p-px^{p-1}-p$ which is an Eisenstein polynomial. I do not know how to show that we have all roots of $f$ .,"['galois-theory', 'number-theory', 'p-adic-number-theory']"
3419973,Mean value of the determinant of a $2n \times 2n$ skew-symmetric matrix with random entries,"Let $A$ be a $2n \times 2n$ matrix with entries chosen independently at random. Each entry is chosen to be $0$ or $1$ , each with probability $1/2$ . Find the expected value of $\det(A - A^T)$ as a function of $n$ . Appeared in a class that prepares students for the Putnam exam, but we did not get to it.","['contest-math', 'determinant', 'matrices', 'random-matrices', 'probability']"
3419988,Implicit differentiation of a trivariate function,"In this post: Deriving the Formula of Total Derivative for Multivariate Functions , it is stated that the first derivative of a trivariate function $f(x,y(x),z(x))$ with respect to $x$ is $$\large \frac{df}{dx}=\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{dy}{dx}+\frac{\partial f}{\partial z}\frac{dz}{dx}$$ I have an implicit equation $f(x,y(x),z(x))=0$ As part of a calculation for $\frac{dy}{dx}$ , I differentiate both sides.  Doing this I get: $\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{dy}{dx}+\frac{\partial f}{\partial z}\frac{dz}{dx}=0$ which gives: $\large\frac{dy}{dx}=-\frac{\frac{\partial f}{\partial x}+\frac{\partial f}{\partial z}\frac{dz}{dx}}{\frac{\partial f}{\partial y}}$ To make sure I have the correct expression, I check this with a simple example: $f(x,y,z)=x+xy+yz-z^2=0$ where: $y=2x$ ; $z=5x$ So: $\frac{\partial f}{\partial x}=1+y$ ; $\frac{\partial f}{\partial y}=x+z$ ; $\frac{\partial f}{\partial z}=y-2z$ ; $\frac{dz}{dx}=5$ It is easy to solve $f(x,y,z)=x+xy+yz-z^2=0$ giving $x=\frac{1}{13}$ , $y=\frac{2}{13}$ and $z=\frac{5}{13}$ So $\frac{dy}{dx}=-\frac{(1+y)+5(y-2z)}{x+z}=4.16666$ , but we know it should be $2.0$ . Problem, so let's check the original equation: Substituting into: $\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{dy}{dx}+\frac{\partial f}{\partial z}\frac{dz}{dx}$ , I get: $(1+y)+(x+z)*2+(y-2z)*5$ which equals $-1$ , ie. not the $0$ I was expecting. What am I missing?","['partial-derivative', 'implicit-function', 'multivariable-calculus', 'implicit-differentiation']"
3420003,Prove this estimator is not sufficient,"I have the following problem. Let $X_1,...,X_n$ be a random sample of i.i.d random variables with density $$f_\theta=\begin{cases}
\theta x +\frac{1}{2},&\text{if }-1\le x\le 1\\
0 ,&\text{elsewhere}
\end{cases}$$ Let $\hat\theta(X)=\frac{3\bar{X}}{2}$ an estimator of $\theta$ . I want to prove that $\hat\theta(X)$ is not sufficient by giving a specific example where $P(X=x|\hat\theta(X)=t)$ depends on $\theta$ . Any help would be appreciated.","['statistics', 'parameter-estimation', 'maximum-likelihood', 'estimation']"
3420007,Is Loewner order equivalent to inequality of eigenvalues?,"I know from this previous question that if $A \geq B$ then $\lambda_k(A) \geq \lambda_k(B)$ , where $\lambda_k$ denotes the $k$ th ordered eigenvalue. Now I would like to know if the reverse holds, i.e., if the Loewner order can be defined in terms of inequalities between eigenvalues. Any book or reference where this issue is explained in detail would be appreciated.","['eigenvalues-eigenvectors', 'reference-request', 'matrices', 'linear-algebra', 'inequality']"
3420066,"Find the smallest $n$ in series, such that $a_n>10^5$","I'm trying to solve the following problem: We have a series ${a_n}^{\infty}_{n=0}$ with relation: $a_{n+2}=5a_{n+1}+3a_n$ ; $a_0=a_1=1$ . Find smallest $n$ , such that $a_n>10^5$ . This is a preparation for exam - no calculators allowed. I tried to create a characteristic polynome, so I created an equation $-x^2+5x+3=0$ , however I got roots $x_1=\frac{5-\sqrt{37}}{2}, x_2=\frac{5+\sqrt{37}}{2}$ , which I think is wrong, as the numbers don't seem right for an exam. Did I make a mistake somewhere when creating the equation or is there any other (simpler) way to solve this? Thanks","['recurrence-relations', 'discrete-mathematics', 'sequences-and-series']"
3420084,Integral of sequence of functions,Let be $f_n(x)=\frac{nx}{1+nx^3}$ a sequence of functions Prove: $$\lim_{n\to\infty} \int_{0}^{1}f_n(x) dx =+\infty$$,"['integration', 'improper-integrals', 'sequence-of-function', 'analysis', 'real-analysis']"
3420086,Definition of Regular function,"In Harsthone page 15, the notion of  regular function of a quasi affine variety $Y$ is defined as followds. A function $f:Y \to k$ is regular at a point $P \in Y$ if there is an
  open neighborhood $U$ with $p \in U \subset Y$ and polynomial $g,h \in
 A = k [x_1,...,x_n]$ , such that $h$ is nowhere zero on $U$ and $f=g/h$ on $U$ . We say that $f$ is regular on $Y$ if it is regular at every
  point of $Y$ . We denote by $\mathcal{O}(Y)$ the ring of all regular
  functions on $Y$ . On the other hand, in the page 17,  THEOREM 3.3 in the book said that $\mathcal{O}(Y)$ is isomorphic to the affine coordinate ring of $Y$ , $\mathcal{O}(Y)\cong A(Y)=k[x_1,...,x_n]/I(Y)$ . But I cannot understand it. Because any element of $A(Y)$ can be represented by the polynomial and not its ratio. So why is $\mathcal{O}(Y)$ defined by using ratio $g/h$ of polynomials $g,h$ ?",['algebraic-geometry']
3420090,What is the Chern class of a line bundle over a number ring?,"Question: Let $F$ be a finite extension over $\def\q{\mathbb Q}\q$ . Let $\mathcal O_F$ be the integral closure of $\mathbb Z$ in $F$ . Then if I am not mistaken, a line bundle (an invertible sheaf) over $X:=\operatorname{Spec}(\mathcal O_F)$ is a fractional ideal of $\mathcal O_F$ . Let $\mathcal L$ be such a line bundle. Then what is $c_1(\mathcal L)\cap[X]$ as defined in stacks project 02SJ ? My thoughts: Since the dimension of $X$ is $1$ , this should be a $0$ -cycle, i.e. a finite linear combination of points of $\operatorname{Spec}(\mathcal O_F)$ , modulo principal divisors, namely, an element of the ideal class group of $\mathcal O_F$ . Write $\mathcal L$ , regarded as a fractional ideal as $\prod_i\mathfrak p_i^{n_i},\,n_i\in\mathbb Z\,\forall i$ , where $\mathfrak p_i$ are prime ideals of $\operatorname{Spec}(\mathcal O_F)$ . Then by definition $c_1(\mathcal L)\cap[X]$ is represented by $\sum_i\text{ord}_{\mathfrak p_i, \mathcal{L}}(s) [\mathfrak p_i]$ for some $s\in\mathcal L$ . Here $\text{ord}_{\mathfrak p_i, \mathcal{L}}(s)$ is defined as follows. Take an $s_i\in\mathfrak p_i$ such that $(s_i)=\mathfrak p_i\mathcal O_{F,\mathfrak p_i}$ . Then $s=s_i^{n_i}\cdot t_i$ for some $t_i\in\mathcal O_{F,\mathfrak p_i}$ . And $\text{ord}_{\mathfrak p_i, \mathcal{L}}(s):=\text{length}(\mathcal O_{F,\mathfrak p_i}/t_i\mathcal O_{F,\mathfrak p_i})$ . I think this is equal to $\text{ord}_{p_i}(t_i)$ . So if $\mathcal O_F$ is a P.I.D., then we can choose $s$ such that each $t_i$ is a unit in $\mathcal O_{F,\mathfrak p_i}$ , and hence $c_1(\mathcal L)\cap[X]=0,\,\forall\mathcal L$ . In general, I have no idea what do the orders of these $t_i$ mean, in terms of $\mathcal L$ . If I might guess, then I would surmise that the Chern class of a line bundle is just the class of the fractional ideal in the ideal class group. But I do not know if this is correct. Any references or hints are welcomed. If the above (strange) argument is flawed, please point the flaws out. Thanks in advance.","['algebraic-number-theory', 'ideal-class-group', 'affine-schemes', 'algebraic-geometry', 'line-bundles']"
3420139,Why is this function continuous on $\mathbb R$?,"Let $f:\Bbb{R}\to\mathbb R$ be a function with $f(0) = 1$ and $f(x+y) \le f(x)f(y)$ for all $x, y \in \Bbb{R}$ . Prove that if $f$ is continuous at $0$ , then $f$ is continuous on $\Bbb{R}$ ? THOUGHTS :
I know the definition of $f$ being continuous at $0$ , that is, for any $\epsilon>0$ , there exists $\delta>0$ such that for all $x$ with $|x-0|<\delta$ , $$
|f(x)-f(0)|<\epsilon\tag{1}
$$ But I don't know how to use (1) and the assumptions in the statement to show the continuity of $f$ on $\mathbb{R}$ , how would (1) be helpful for showing $$
|f(x)-f(x_0)|<\epsilon
$$ for general $x_0\in\mathbb{R}$ ? Could you please help me?","['real-numbers', 'continuity', 'functions', 'real-analysis']"
3420171,Convergence in measure for bounded f and uniformly bounded $g_n$ implies $f_n \cdot g_n \to f \cdot g$ in measure,"Let $f_n$ be convergent in measure to some bounded $f$ ( $\exists_{M \in \mathbb{R}} \text{ }|f(x)| \leq M $ ) and $g_n$ uniformly bounded (there exists $E \in \mathcal{F}, \mu(E)=0$ and $\forall_{x \notin E} \exists_{N\in \mathbb{R}} \forall_{n} |g_n(x)| \leq N$ ) and convergent to $g$ . Prove that $f_n \cdot g_n \to f\cdot g$ in measure. I tried bounding: $$
\forall_{x \notin E} \mu\left(x: |f_n(x)g_n(x) - f(x)g(x)| > \epsilon\right) \leq
$$ $$
\forall_{x \notin E} \mu\left(x: |f_n(x)N - M g(x)| > \epsilon\right)
$$ But that did not lead me far. I have to show the limit of the above measure is 0 as n goes to infinity. How can I tackle this problem?","['measure-theory', 'real-analysis']"
3420186,How to make the function of the planes from the lorenz attractor,"How can I make the equation for the planes of the wings of the lorenzattractor, 
I know that the critical point should be used. But I don't know how I should make this plane, the figure shows in yellow which plane I want.","['chaos-theory', 'ordinary-differential-equations', 'plane-geometry']"
3420258,Why are differential forms so convenient? (In applied mathematics),"This is a soft question to understand better why differential forms are so useful, especially in applied mathematics. I own Tu's - Introduction to Manifolds , and I'll quote the introduction of section 18: Differential forms are generalizations of real-valued functions on a manifold. Instead
  of assigning to each point of the manifold a number, a differential k-form assigns to
  each point a k-covector on its tangent space. For k = 0 and 1, differential k-forms are
  functions and covector fields respectively Differential forms play a crucial role in manifold
  theory. First and foremost, they are intrinsic objects
  associated to any manifold, and so can be used to
  construct diffeomorphism invariants of a manifold.
  In contrast to vector fields, which are also intrinsic
  to a manifold, differential forms have a far richer algebraic structure. Due to the existence of the wedge
  product, a grading, and the exterior derivative, the
  set of smooth forms on a manifold is both a graded
  algebra and a differential complex. Such an algebraic structure is called a differential graded algebra. Moreover, the differential complex of smooth
  forms on a manifold can be pulled back under a
  smooth map, making the complex into a contravariant functor called the de Rham complex of the manifold. We will eventually construct the de Rham cohomology of a manifold from the de Rham complex.
  Because integration of functions on a Euclidean space depends on a choice of
  coordinates and is not invariant under a change of coordinates, it is not possible to
  integrate functions on a manifold. The highest possible degree of a differential form
  is the dimension of the manifold. Among differential forms, those of top degree turn
  out to transform correctly under a change of coordinates and are precisely the objects
  that can be integrated. The theory of integration on a manifold would not be possible
  without differential forms. From this introduction, and plus what the text actually covers on the subject, it seems to me that the main advantage of the differential forms over vector fields is their algebraic structure (which is much richer than the vector fields one). However I do struggle to understand if once I get used to the formalism it actually makes easier to model certain geometric problems. There's another book from the same author ( Tu's - Differential geometry ) where the formalism of forms is used to derive the structure equations, which are still differential forms. I own such book as well, but I also own Do Carmo's - Riemannian Geometry , and I've been reading the three of them lately. The thing is Do Carmo doesn't seem to make use of differential forms at all to develop his theory (except in the exercises), Tu instead uses differential forms as I mentioned. This difference strikes me, since I can't quite figure what sort of ""modelling"" advantage differential forms give over normal vector fields to model certain problems. There're also few papers (Computer Graphics papers) that I've been reading through lately where the differential forms formalism is actually used to design certain algorithms to process meshes, so they must have some modelling advantage that I don't understand. Can anyone clarify? Update : As further clarification I think I can give, I'm asking from the applied mathematics point of view. For example in triangular meshes processing, tools from differential geometry are used to compute laplacian, gradient etc, but the methods I'm aware don't really use differential forms, but I'm aware from some papers I've been reading in the last few months that differential forms are somehow used to model some problems, I can't understand though why such formalism is actually better in some situations. Please let me know if you like some quotes from some of the paper I'm talking about.","['riemannian-geometry', 'smooth-manifolds', 'soft-question', 'differential-forms', 'differential-geometry']"
3420267,Band of thieves - escape locations if some of them are compromised,"I am currently reading a book in which we follow a band of 5 thieves which are about to do something big and some of them might be captured. They would like to meet up afterwards but have to expect that some of their previously shared locations are compromised by being pried out of the caught ones of their group. All of them would know which ones of them have been caught. While the book certainly does not solve this problem mathematically how could the band of thieves make sure the locations are distributed in a way that they can not be compromised by losing some of their group. Things that don't work: Have each thief only know 2 locations like here: In this case if thieves A, C and E would be caught thieves B and D would not have a shared set of locations to check anymore and could not meet up. What about greater groups of thieves? How many locations would they have to share beforehand?","['elementary-set-theory', 'puzzle', 'combinatorics']"
3420271,Show that relative entropy in terms of integrals is always non-negative,"I'm looking to prove that relative entropy (or Kullback–Leibler divergence) is always non-negative (i.e. Gibbs' inequality) in terms of integrals, that is show that for any two probability PDF's $P$ and $Q$ in $\Omega$ $$ \int_\Omega P(x) \log \left(\frac{P(x)}{Q(x)}\right) \, \mathrm{d}x \geq 0. $$ I know how to show this with Jensen's inequality, but I'm having trouble showing that if the integral in question diverges. As an example, setting $P(x)=\frac{1}{\pi  \left(x^2+1\right)}$ , $Q(x)=\frac{e^{-\frac{x^2}{2}}}{\sqrt{2 \pi }}$ and $\Omega=\mathbb{R}$ , the integral diverges to $\infty$ . In this case, as I'm only trying to show that the integral can never be negative, it's enough to show the integral can only diverge to positive infinity. While this seems heuristically correct, I don't know how to go on proving that. My first approach was to look at the integrand itself, but it can be negative at times. Also, I can't just look at some cutoff value from which on all the values would be positive, as I'm only guaranteed that $$\lim_{x\to \infty} \frac{P(x)}{Q(x)}>0,$$ which still allows for negative values inside the logarithm. Any ideas or references to some literature? All the proofs I've found seem to ignore the possibility of divergence, maybe there's a reason I could do that too?","['entropy', 'probability-theory', 'information-theory']"
3420315,"Find the minimum value of $f(x,y) = x + y^2$ given the constraint $2x^2 + y^2 = 1$","Find the minimum value of $x + y^2$ subject to the condition $2x^2 + y^2 = 1$ . 1) I find $\nabla f$ and $\nabla g$ to get $$\nabla f(x,y) = (1, 2y) \\ \nabla g(x,y) = (4x, 2y)$$ Then I set up the system of equations \begin{align}
\nabla f(x,y) &= \lambda g(x,y) \\
1 &= \lambda 4x \\
2y &= \lambda 2y \\
2x^2 + y^2 &= 1
\end{align} I am having difficulties solving the system of equations. I got $\lambda = 1$ from $2y = \lambda 2y$ and from there I obtain $x = \frac{1}{4}$ . However I don't understand how I am supposed to solve for a value of $y$ . I tried to plug in $x = 1/4$ into $2x^2 + 2y^2 = 1$ but I got $y = \sqrt{7/4}$ which does not fit the system of equations. I think my setup is correct and I am doing the calculations wrong. I think I should be getting $x = 1/4$ and $y = \sqrt{7}/4$ . What do you guys think?","['qcqp', 'lagrange-multiplier', 'maxima-minima', 'multivariable-calculus', 'optimization']"
3420420,recursive sequence with continuous function,"Let $f:\mathbb{R}\to\mathbb{R}$ be a continuous function, and let $\{x_{n}\}$ be a sequence defined by $x_{0}=0$ and $$x_{n+1}=f(x_{n})\quad(n\ge0).$$ Prove that if $\{x_{n}\}$ is bounded, then there exists $c\in\mathbb{R}$ such that $f(c)=c$ . Is it possible to show using the Bolzano-Weierstrass theorem? Give some advice or hint. Thank you!","['continuity', 'sequences-and-series', 'fixed-points', 'real-analysis']"
3420425,Two ways to compute divergence do not agree,"We can compute the divergence of a vector filed $X$ on $\mathbb{R}^2$ expressed in polar coordinates $(r,\theta)$ in two ways: the first one is the classical formula $$\text{div}(X)=\frac{1}{r}\frac{\partial(rX^r)}{\partial r}+\frac{1}{r}\frac{\partial X^\theta}{\partial\theta},$$ and the second one is the formula given by the Riemannian definition of the divergence (here $(x^1,x^2)=(r,\theta)$ ) $$\text{div}(X)=\frac{\partial X^i}{\partial x^i}+\Gamma_{ij}^iX^j=\frac{\partial X^r}{\partial r}+\frac{\partial X^\theta}{\partial\theta}+\frac{1}{r}X^r.$$ The two expression are not the same: the term $\frac{\partial X^\theta}{\partial\theta}$ is rescaled differently. Why? I suppose that it has something to do with some kind of renormalisation. For the Christoffel symbols I looked here .","['geometry', 'riemannian-geometry', 'differential-geometry']"
3420548,Properties of $\lim \sup$,"Suppose { $a_n$ } and { $b_n$ } are bounded sequences and that lim $b_n =b$ . Prove
  that $$\lim \sup (a_n + b_n) = \lim \sup a_n + b. $$ Here is what I  tried: Consider $$\sup (a_k) + b$$ Since $b$ is a fixed number, and $\sup (a_k) = \sup \{ a_n : n\geq k \} $ , then it follows that $$\sup (a_k) + b  = \sup (a_k + b)$$ (is this correct?) Then since $b_k \rightarrow b $ , for $k \geq N$ for some $N \in \mathbb{N}$ large enough, $ b_k - \epsilon \leq b \leq b_k + \epsilon$ . So $$ \sup (a_k + b_k - \epsilon) \leq   \sup (a_k + b) \leq \sup (a_k + b_k + \epsilon)$$ Since this holds for all $k \geq N$ , we can take the limit as $k$ approaches $\infty$ , $$\lim \sup (a_k + b_k - \epsilon) \leq \lim  \sup (a_k + b) \leq \lim \sup (a_k + b_k + \epsilon)$$ Since $\epsilon$ was arbitrary, we can conclude that $$\lim \sup (a_n + b_n) = \lim \sup a_n + b. $$","['limsup-and-liminf', 'proof-verification', 'real-analysis', 'complex-analysis', 'limits']"
3420551,Is $f(x)=\frac{x^2-1}{x+1}$ continuous at $x^*=-1$?,"Is $f(x)=\frac{x^2-1}{x+1}$ continuous at $x^*=-1$ ? For $f(x)=\frac{x^2-1}{x+1}$ there exists $\lim\limits_{x\to -1^-}f(x)=-2$ and $\lim\limits_{x\to -1^+}f(x)=-2$ but it is not continuous at $x=-1$ , but that would be a contradiction to the definition of continuity which states that for every function where both $\lim_{x\to x^{*-}}=\lim_{x\to x^{*+}}$ are equal then the function is continuous at the point $x^*$ . What is wrong with my thoughts? (Or do I remember that incorrectly?)","['continuity', 'functions', 'real-analysis']"
3420555,Intuition for this tricky puzzle,"Problem : $13$ Apples, $15$ Bananas and $17$ Cherries are put in the magic hat. When ever a collision of two different fruits occurs, they both get converted into the third type. For example $1$ Apple and $1$ Banana can collide to form $2$ cherries. No other collision is holy. Can a sequence of such magical collisions lead all $45$ fruits to give just one type? Solution provided : Create the invariant function $f(A,B,C) = (0A+1B+2C)mod3$ , this function remains constant during a collision. But $f(13,15,17) = 1$ is not same as any of final states $f(45,0,0)=f(0,45,0)=f(0,0,45)=0$ . Hence this can not be done. Query : I understood the solution but it seems non-intuitive to me. Is there any better solution to this problem?","['puzzle', 'discrete-mathematics']"
3420582,Importance of the structure sheaf of the spectrum,"I'm currently reading scheme theory and i'm trying to figure out the reason of introducing additionally the sheaf of a ring A together with its spectrum.I can understand all the basic properties of the spectrum,understand all the proofs but i can't see the importance of the sheaf.Is there any historical example that led to the definition of the spectrum?Is there any example of two spectrums of rings A,R such that SpecA is homeomorphic to SpecR but with the corresponding sheaves to be ""different"" in the sence that they encode a different information from the rings and thus showing the importance of introducing the sheaf of the ring together with its spectrum?","['algebraic-geometry', 'soft-question', 'schemes']"
3420677,Is there a relationship between the second derivative and the quadratic term of a cubic equation?,"I have been studying on cubic equations for a while and see that the cubic equation needs to be in the form of $mx^3+px+q=0$ so that we can find the roots easily. In order to obtain such an equation having no quadratic term for a cubic equation in the form of $x^3+ax^2+bx+c=0$ , $x$ value needs to be replaced with $t-\frac a{3}$ . I realised that the second derivative of any cubic equation in the form of $x^3+ax^2+bx+c=0$ is $y''=6x+2a$ , and when we equalize $y$ to $0$ , $x$ is equal to $-\frac a{3}$ . This had me thinking about any possible relation between the quadratic term and the second derivative. It is also sort of the same in quadratic functions in the form of $x^2+ax+b=0$ . When we replace $x$ with $-\frac a{2}$ , which is the first derivative of a quadratic function and the vertex point of it, we obtain the vertex form of the equation which does not have the linear term, $x^1$ . Is there a relation between the quadratic term and the second derivative of a cubic equation? If the answer is yes, what is it and how is it observed on graphs, in equations?","['cubics', 'calculus', 'derivatives', 'roots']"
3420726,Why can't Wolfram Alpha compute this integral?,$$\int_0^\infty \int_0^\infty \lambda_1\lambda_2 \mid{x - y} \mid e^{-\lambda_1x - \lambda_2y} dy dx$$ $$= \int_0^{\infty} e^{-\lambda_1x}\bigg[ \int_0^x(x - y)e^{-\lambda_2y}dy + \int_x^\infty(y - x)e^{-\lambda_2y}dy\bigg] dx$$ It seems like a straight forward but tedious integral to compute. Is there a way I can input this so that Wolfram is less confused by $x$ and $y$ being treated as variables and constants in different situations? Also Is there a way to specify that $\lambda_1$ and $\lambda_2$ are positive constants? I replaced $\lambda_1$ and $\lambda_2$ with $\pi$ and $e$ and it gave me an answer.,"['integration', 'statistics', 'wolfram-alpha', 'expected-value']"
3420741,"Show that if X and Y are i.i.d random variables such that $\mathbb{E}(X-Y)^{2}<\infty$, then $\mathbb{E}X^{2}<\infty$","As stated in the title, I'd like to show If X and Y are i.i.d random variables such that $\mathbb{E}(X-Y)^{2}<\infty$ , then $\mathbb{E}X^{2}<\infty$ My attempt: We prove that if $\mathbb{E}X^{2}=\infty$ , then $\mathbb{E}(X-Y)^{2}=\infty$ . To show this, we compute \begin{align*}
\mathbb{E}(X)^{2}&=\int_{0}^{\infty}2x\mathbb{P}\Big(|X|>x\Big)dx\\
&=\int_{0}^{\infty}2x\mathbb{P}\Big(|X-Y|>x, Y=0\Big)dx\\
&\leq\int_{0}^{\infty}2x\mathbb{P}\Big(|X-Y|>x\Big)dx\\
&=\mathbb{E}(X-Y)^{2},
\end{align*} where the inequality is because $\{|X-Y|>x, Y=0\}\subset\{|X-Y|>x\}.$ However, I am really not sure if I am correct, because $(1)$ I am not sure if the inequality is corret $(2)$ I am not sure cannot I do the last equality, since $x$ corresponds to the distribution of $X$ , so perhaps $\mathbb{E}(X-Y)^{2}=\int_{0}^{\infty}2z\mathbb{P}\Big(|X-Y|>z\Big)dz$ ? for some other $z$ ? $(3)$ I did not use the $i.i.d$ property at all... Could somebody have a check? Thank you! If I was incorrect, it will be the best if one could provide an alternative, or a hint. Thank you! Edit 2: As ""Kimchi Lover"" pointed out, we don't need to assume the integrability of $X$ and $Y$ to prove my problem. He proved the second moment, but in the book from Feller, the result holds for any moment $\alpha>0$ . The proof can be found exactly following the post of ""Kimchi Lover"", but I still think it will be the best if I put the proof here, for other users to look up easily. Below is the proof from Feller, and it combined several lemmas: Firstly, we claim that if $u$ is bounded and has a continuous derivative $u'$ , then $$(***)\ \ \int_{a}^{b^{+}}udF=u(b)F(b)-u(a)F(a)-\int_{a}^{b}u'(x)F(x)dx.$$ Indeed, rearrange the above equality, we have $$\int_{a}^{b^{+}}[u(b)-u(x)]dF-\int_{a}^{n}u'(x)[F'(x)-F(a)]dx=0.$$ Suppose $|u'|<M$ and partition $a,b$ into congruent intervals $I_{k}$ of length $h$ . It is easily seen that the contribution of $I_{k}$ to the LHS is in absolute value less than $2MhF\{I_{k}\}$ . Summing over $k$ we find that the LHS is $<2Mh$ , which can be made as small as we possible. Thus the LHS is $0$ , as desired. Now, apply the above claim, we can conclude that for any $\alpha>0$ , we have $$(****)\ \ \int_{0}^{\infty}x^{\alpha}dF=\alpha\int_{0}^{\infty}x^{\alpha-1}[1-F(x)]dx,$$ in the sense that if one side converges, so does the other. Indeed, because of the infinite interval of $(***)$ does not apply directly, but for every $b<\infty$ , after rearrangement, we have $$\int_{0}^{b^{+}}x^{\alpha}dF=-b^{\alpha}[1-F(b)]+\alpha\int_{0}^{b}x^{\alpha-1}[1-F(x)]dx.$$ Suppose firstly that the integral on the left converges as $b\longrightarrow\infty$ . The contribution of $\overline{(b,\infty)}$ to the infinite integral is $\geq b^{a}[1-F(b)]$ , and thus this quantity tends to $0$ . In this case, passage to the limit $b\rightarrow\infty$ leads the results. On the other hand, the integral on the left is smaller than the integral on the right and hence the convergence of the second imply the convergence of the first, which concludes the proof of $(****)$ . An analogue to $(****)$ holds for the left tail. Combining the two formulas, we get that the distribution $F$ possess an absolute moment of order $\alpha>0$ if and only if $|x|^{\alpha-1}[1-F(x)+F(-x)]$ is integrable over $\overline{(0,\infty)}$ . (Conclusion 1) Now, with the above conclusion, we can prove that if $X$ and $Y$ are independent random variables and $S=X+Y$ , then for $\alpha>0$ , $\mathbb{E}|S|^{\alpha}$ exists if and only if $\mathbb{E}|X|^{\alpha}$ and $\mathbb{E}|Y|^{\alpha}$ exists. Indeed, since the variables $X$ and $X-c$ possess exactly the same moments, we can WLOG assume that $0$ is the median for both $X$ and $Y$ . Then, we have $$\mathbb{P}(|S|>t)\geq\dfrac{1}{2}\mathbb{P}(|X|>t),$$ and by the (Conclusion 1), we know that $\mathbb{E}|S|^{\alpha}<\infty$ implies $\mathbb{E}|X|^{\alpha}<\infty$ , which prove the direct $(\Rightarrow)$ . To prove the converse assertion, we just use the inequality $|S|^{\alpha}\leq 2^{\alpha}(|X|^{\alpha}+|Y|^{\alpha})$ .","['expected-value', 'proof-verification', 'probability-theory']"
3420745,Complexity Of A Recurrence Summation.,"I was trying to find all possible full trees as a recurrence formula and I found it but now I want to find the complexity of it as $\theta$ relation. I found that it is something like this. $$f(n)=\sum_{i=0}^{n-1}f(i)*f(n-i-1) \text{ and } f(0)=1,f(1)=1$$ I know it is not exactly this function (I simplified it) but if I can learn how to solve this I can apply it to all possible full trees formula. TLDR; I want to know how I can find time complexity of $f(n)$","['trees', 'summation', 'recurrence-relations', 'discrete-mathematics', 'computational-complexity']"
3420749,"An assertion in Euler's first paper on Elliptic integrals, E28","In his first paper on Elliptic Integrals, Euler calculates the arc length of an ellipse by solving a non-separable differential equation arising from it. He previously thought no non-separable differential equation could be solved. He says: This case seemed exceedingly paradoxical to me at first; but after studying the solution more carefully I realized easily not only that a separation could not be deduced from it, but also, that if a separation were to succeed by another method, far greater absurdities would follow. One might find a comparison of the perimeters of different ellipses, which, it surely seems to me, would overturn all of analysis. My question: How might one find a comparison of perimeters of different ellipses if the differential equation were separable & why would this overturn all of analysis? Links to the original latin & English translation of Euler's article: http://eulerarchive.maa.org/pages/E028.html More context: I believe that the separation of variables in differential equations is so carefully sought because
  a solution of the equation follows directly from that discovery, which is evident to one practiced
  enough in these matters. Moreover the integration of differential equations, if indeed it succeeds,
  is begun best by separating variables. Though certainly innumerable equations have been given,
  whose integrals can be found without such a separation – the Celebrated Johann Bernoulli exhibited
  a method of this type in our Comm. Tom. I page 1672 – yet all of these equations have been
  arranged in such a way, so that either the separation of variables is obvious by itself, or that at
  least the separation may be derived from that integration. It is indeed likely that the computation
  of solutions that Analysts have found up until now are all of this type of equation, that, even if
  the variables can be separated in no other way, a separation still arises from that solution. For this
  reason, I have believed until now that no solvable differential equation could be produced whose
  separation would elude all men. Recently however while engaged in the rectifying of an ellipse, I unexpectedly came upon a
  differential equation by which I was able to solve the rectification of the ellipse, yet a separation of
  variables could not be found, not even from the method of solution. In fact the equation I obtained
  was $dy + \frac{y^2dx}{x} = \frac{x dx}{x^2−1}$ which closely resembles the Riccati equation, and as it happens it is as
  difficult to separate as $dy + y^2 dx = x^2 dx$ . This case seemed exceedingly paradoxical to me
  at first; but after studying the solution more carefully I realized easily not only that a separation
  could not be deduced from it, but also, that if a separation were to succeed by another method,
  far greater absurdities would follow. One might find a comparison of the perimeters of different
  ellipses, which, it surely seems to me, would overturn all of analysis. This solution moreover
  is extremely easy, it is completed indeed by the elongation of infinite ellipses having one of two
  axes in common, and for this reason it must be substantially preferred to the usual way of solving
  quadratures.","['math-history', 'analysis', 'ordinary-differential-equations']"
3420759,"Why do we use combination, not permutation, in binomial distribution?","I am aware there is a question similar to mine here and here , however I'm asking because I have a more specific question regarding this issue. Some explanations I've gotten as to why we use combination is that the order doesn't matter. HTHT and HHTT is the same number of heads. But then wouldn't this mean that the ""number"" of ways we get two heads is just one? Obviously not because ${4\choose2}$ is not 1. Furthermore, can someone explain what $\frac{n!}{(n-k)!}$ would mean in the context of the ""4 trials, 2 heads"" example? In permutations since order matters, doesn't this mean we count HHTT, HTHT, HTTH, and so on? But then when I add this up this comes to a total of 6, which is actually ${4\choose2}$ !","['combinations', 'binomial-theorem', 'combinatorics']"
3420844,In how many ways can we distribute $r$ distinct balls in $n$ identical boxes so that none is empty?,"In how many ways can we distribute $r$ distinct balls in $n$ identical
  boxes so that none is empty? Note that, At once, a ball can only lie in exactly one box. If at first, we consider boxes to be different, let the boxes be $\{b_1\ldots , b_n\}$ and let us call the action of putting balls in these boxes as $f$ , then $f$ is a function since all balls must be put (domain must used completely), and a ball can be put in exactly one of the boxes (well defined) Now note, that $f^{-1}(b_i)\neq \phi$ since $f$ is onto, i.e, all boxes should be non empty. Moreover, $f^{-1}(b_i)\cap f^{-1}(b_j)= \phi \; \forall \; i\neq j$ as $f$ is well defined. and $\cup_{i=1}^{n}f^{-1}(b_i ) = \text{the set of all r balls}$ So, we are basically partitioning the set of $r$ balls into $n$ non empty subsets. But since our boxes are identical, we are looking for unordered partitions which is given by $S(r,n)$ where $S$ is Stirling's Number of $2nd$ kind. Is my interpretation correct?  Please correct me wherever I am going wrong. This is very important to me","['elementary-set-theory', 'number-theory', 'combinatorics', 'elementary-number-theory']"
3420882,Are all conjugacy classes in $\text{GL}_n(\mathbb R)$ path-connected?,"Suppose $A$ and $B$ are conjugate invertible real $n \times n$ -matrices. Does there always exist a path from $A$ to $B$ inside their conjugacy class? I thought I had an easy proof for odd $n$ which goes as follows, but it was incorrect as pointed out in this answer. To show where my misunderstanding arised, here is the wrong argument. Suppose there exists a real matrix $P$ such that $B = PAP^{-1}$ . By replacing $P$ with $-P$ if necessary, we can assume that $\det P > 0$ (this is what goes wrong in even dimensions, see this question).
Then we have that $P = e^Q$ for some real matrix $Q$ (since the image of the exponential map is the path-component of the identity in $\text{GL}_n(\mathbb R)$ ). But now the path $$t \mapsto e^{tQ}Ae^{-tQ}$$ is a path connecting $A$ to $PAP^{-1} = B$ .","['matrices', 'general-topology', 'abstract-algebra', 'linear-algebra']"
3420898,Justification for Uniqueness of Solutions to Dispersive PDE,"For the sake of concreteness, we consider the linear Schrodinger equation $$
\partial_t u = i\Delta u, \ \ \ \ u(0, x) = u_0(x).
$$ The solution is typically (at least, how I've seen it)  obtained by taking the Fourier transform of both sides, giving $\widehat{\partial_t u}(t, \xi) = -i|\xi|^2 \hat{u}(t,  \xi)$ . The next step is where I have questions. Assuming that everything is nice enough (for instance, in Tao's book, he assumes $u_0$ is Schwartz), dominated convergence gives $\widehat{\partial_t u}(t, \xi) =  \partial_t \hat{u}(t, \xi)$ , and then we get an ODE that solves to $$
\hat{u}(t, \xi) =e^{-i|\xi|^2}\hat{u}_0(\xi) \implies u(t, x) = e^{it\Delta}u_0(x).
$$ This is then referred to as "" the solution to the Schrodinger equation, with initial data $u_0$ ."" My question : How do we know that there are no other solutions, that may not satisfy the right decay/smoothness criteria to  justify pulling the Fourier transform into the time derivative of $u$ ? I agree that there are no other solutions $u$ that are ""nice enough"" to justify this. But how do we rule out the existence of solutions $u$ such that $\partial_t \hat{u} \neq \widehat{\partial_t u}$ ? Any help is much appreciated!","['dispersive-pde', 'analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
3420908,Solve : $\sin\left(\frac{\sqrt{x}}{2}\right)+\cos\left(\frac{\sqrt{x}}{2}\right)=\sqrt{2}\sin\sqrt{x}$,"Solve : $\sin\left(\dfrac{\sqrt{x}}{2}\right)+\cos\left(\dfrac{\sqrt{x}}{2}\right)=\sqrt{2}\sin\sqrt{x}$ $$\dfrac{1}{\sqrt{2}}\cdot\sin\left(\dfrac{\sqrt{x}}{2}\right)+\dfrac{1}{\sqrt{2}}\cos\left(\dfrac{\sqrt{x}}{2}\right)=\sin\sqrt{x}$$ $$\sin\left(\dfrac{\sqrt{x}}{2}+\dfrac{\pi}{4}\right)=\sin\sqrt{x}$$ $$\sin\left(\dfrac{\sqrt{x}}{2}+\dfrac{\pi}{4}\right)-\sin\sqrt{x}=0$$ $$2\sin\left(\dfrac{\dfrac{\sqrt{x}}{2}-\dfrac{\pi}{4}}{2}\right)\cos\left(\dfrac{\dfrac{3\sqrt{x}}{2}+\dfrac{\pi}{4}}{2}\right)=0$$ $$\left(\dfrac{\sqrt{x}}{2}-\dfrac{\pi}{4}\right)=2n\pi \text { or } 
 \left(\dfrac{3\sqrt{x}}{2}+\dfrac{\pi}{4}\right)=(2n+1)\pi$$ $$\sqrt{x}=4n\pi+\dfrac{\pi}{2} \text { or } \sqrt{x}=\dfrac{4n\pi}{3}+\dfrac{\pi}{2}$$ $$x=\left(4n\pi+\dfrac{\pi}{2}\right)^2 \text { or } x=\left(\dfrac{4n\pi}{3}+\dfrac{\pi}{2}\right)^2 \text { where n $\in$ I}$$ But actual answer is $$x=\left(4n\pi+\dfrac{\pi}{2}\right)^2 \text { or } x=\left(\dfrac{4m\pi}{3}+\dfrac{\pi}{2}\right)^2 \text { where n,m $\in$ W}$$ I tried to find the mistake but didn't get any breakthroughs.",['trigonometry']
3420913,why do I get different derivatives?,When I implicitly differentiate with respect to $x$ for the equation $y^{-2}=xy$ I get a different result than when I divide $y$ on both sides first to get $y^{-3}=x$ and then differentiate. For the first I get $dy/dx=-\frac{y^4}{xy^3+2}$ but after dividing both sides by $y$ and then differentiating I get $dy/dx=-\frac{y^4}{3}$ did I break a rule or something or if they are is there a way to show these are equivalent?,"['calculus', 'implicit-differentiation']"
3421042,Solving Strong Mathematical Induction Sequence,"I'm trying to work on the problem below, though I've hit a wall on how to proceed to prove the inductive step. Suppose that $c_0,c_1,c_2\ldots$ is a sequence defined as follows: $$c_0=2,\, c_1=2,\, c_2=6,\, c_k=3c_{k-3}\,(k\geq3)$$ Prove that $c_n$ is even for each integer $n\geq0$ . Here is what I have so far: Show that $P(0)$ and $P(1)$ are true. $c_0=2$ and $2\geq0$ and $2\mid2$ , so this is even. $c_1=2$ and $2\geq0$ and $2\mid2$ , so this is even. Show that for every integer $k\geq1$ , if $P(i)$ is true for each integer $i$ with $0\leq i\leq k$ , then $P(k+1)$ is true. Let $k$ be any integer with $k\geq1$ , and suppose $c_i$ is even for each integer $i$ with $0\leq i\leq k$ [inductive hypothesis]. I must show that $c_{k+1}$ is even for each integer $k\geq0$ . Now $c_{k+1}=3c_{k-2}$ ... ...and this is where I do not understand how to proceed. Any tips on how I can finish solving this problem are greatly appreciated.","['induction', 'discrete-mathematics']"
3421067,"Midpoints, bisectors, orthocenter, incenter and circumcenter","In triangle $ABC$ , let $r_A$ be the line that passes through the midpoint of $BC$ and is perpendicular to the internal bisector of $\angle{BAC}$ . Define $r_B$ and $r_C$ similarly. Let $H$ and $I$ be the orthocenter and incenter of $ABC$ , respectively. Suppose that the three lines $r_A$ , $r_B$ , $r_C$ define a triangle. Prove that the circumcenter of this triangle is the midpoint of $HI$ Solution: Construct the medial triangle of $ABC$ , $DEF$ , with $D, E, F$ the midpoints of $BC, CA, AB$ . Note the angle bisector of $\angle BAC$ is parallel to the angle bisector of $\angle EDF$ . Thus, the triangle formed by $r_A, r_B, r_C$ is the excentral triangle of the medial triangle. Let $S$ , $N$ denote the incenter and circumcenter of the medial triangle. Then $S$ is the orthocenter of the triangle formed by $r_A, r_B, r_C$ with $N$ the Nine-Point Center of the same triangle, so the reflection of $N$ across $S$ , $N'$ is the circumcenter of this triangle. Also, $H$ is the reflection of $O$ , the circumcenter of $ABC$ , about $N$ . Thus $HN'$ is parallel to $OS$ , and $HN' = OS$ . Now consider a homothety about $G$ , the centroid of $ABC$ , of factor $-2$ . $O$ is mapped to $H$ . Since this maps the medial triangle $DEF$ to $ABC$ , $S$ , the incenter, maps to the incenter $I$ of $ABC$ . Then $HI$ is parallel to $OS$ , so it follows that $H, I, N'$ are collinear. $HN' = OS$ from before, and $HI$ = $2OS$ , so it follows that $N'$ is the midpoint of $HI$ , as desired. What would the design of this problem look like?","['contest-math', 'euclidean-geometry', 'problem-solving', 'geometry']"
3421069,Minimize $x+2y$ subject to $x^2+y^2\le1$ and $3x+4y\le-5$.,"I want to minimize $x+2y$ subject to $x^2+y^2\le1$ and $3x+4y\le-5$ . I found the gradient conditions: $1+2x\lambda_1+3\lambda_2=2+2y\lambda_1+4\lambda_2=0$ . Then, by complementary slackness: $\lambda_1(x^2+y^2-1)=\lambda_2(3x+4y+5)=0$ . Wolfram-Alpha gives $(x,y,\lambda_1,\lambda_2) = (\frac1{\sqrt5},\frac2{\sqrt5},-\frac{\sqrt5}2,0)$ or $(-\frac1{\sqrt5},-\frac2{\sqrt5},\frac{\sqrt5}2,0)$ . The second one gives an objective value of $-\sqrt5$ , but both of these violate the second constraint. The actual mimimum is $-\frac{11}5$ at $(x,y) = (-\frac35,-\frac45)$ . Where did I go wrong?","['optimization', 'multivariable-calculus']"
3421088,Set notation clarification.,"It may sound like a silly question, but I really want to make sure that I'm getting this right.
I am somewhat ambiguous about what $$\{(x_1,\ldots, x_n)\in \mathbb{N}^n\mid x_i = x_j\} , 1\leq i<j\leq n$$ stands for.
In the case of $n=3$ , for example, do we mean the set $\{(1,1,1), (2,2,2), (3,3,3), \ldots\}$ or the set $\{(1,1,1), (1,1,2), (1,1,3), (1,2,1), (1,2,2),\ldots\}$ ? In other words, do we need $all$ coordinates to be equal or at least two?",['elementary-set-theory']
3421094,Confusion about second covariant derivatives,"Let $M$ be a smooth manifold, $E\to M$ a smooth vector bundle equipped with a linear connection $\nabla$ . Then we have the exterior covariant derivative $$\nabla^2:\Gamma(M,E)\to\Gamma(M,\Lambda^2T^*M\otimes E).$$ If moreover $M$ has a Riemannian metric and thus $T^*M$ has an induced Levi-Civita connection, then we can form the second covariant derivative $$\nabla\nabla:\Gamma(M,E)\to\Gamma(M,T^*M\otimes E)\to\Gamma(M,T^*M\otimes T^*M\otimes E).$$ My question is: How are these two operators related? For example, $\nabla^2$ is in fact the alternating part of $\nabla\nabla$ . Although this can be proved by direct calculations (i.e. they are both the curvature $2$ -form), but why ? Is there any intuition for this? Furthermore, it is strange that $\nabla\nabla$ actually depends on the Riemannian metric, whereas its alternating part does not.","['tensors', 'riemannian-geometry', 'differential-geometry']"
3421101,Poisson mixture process independence used to devastating effect on the Coupon collectors problem,"Proposition 5.2 of the book, Introduction to probability models by Sheldon Ross says that if we have a Poisson process and each event in the process is of type-1 with probability $p$ and type-2 with probability $1-p$ , then the number of type-1 and type-2 events are independent Poisson processes with rates $\lambda p$ and $\lambda (1-p)$ respectively. The independence is key here. It is then used as a powerful tool in example 5.17, where Ross addresses the coupon collectors problem. Quoting: There are $m$ different types of coupons. Each time a person collects a coupon it is, independently of ones previously obtained, a type $j$ coupon with probability $p_j$ , $\sum\limits_{j} p_j = 1$ . Let $N$ denote the number of coupons one needs to collect in order to have a complete collection of at least one of each type. Find $E[N]$ . In the solution, he starts with the straightforward approach, denoting by $N_j$ the number of coupons that must be collected to obtain a type $j$ coupon. We can then express $N$ as: $$N = \max_{1\leq j \leq m} N_j \tag{1}$$ He notes that the $N_j$ are geometric, but this method runs into a wall when we realize that the $N_j$ 's aren't independent. And this makes sense. If there were only two types of coupons, they would be competing each time we collected a coupon. So, if we need very few coupons to collect one for the first kind, it tells us it's a common coupon and so, we now know that we'll have to wait a long time to see the second coupon (meaning $N_1$ and $N_2$ are negatively correlated). Now, Ross considers the coupons arriving according to a Poisson process with rate $1$ . By proposition 5.2, the counting processes defining the arrivals of each of the coupon types (say $j$ ) are independent Poisson process with rates $1 . p_j$ . Now, define $X$ the time at which all coupons are collected and $X_j$ the time at which the first type $j$ coupon is collected. We get an equation very similar to (1): $$X = \max_{1\leq j \leq m} X_j \tag{2}$$ Now, we don't run into the wall since by proposition 5.2, the $X_j$ 's are independent. However, I haven't been convinced by the arguments presented for this. Why does the reasoning we used to conclude that the $N_j$ 's are negatively correlated not apply to the $X_j$ 's as well?","['poisson-distribution', 'coupon-collector', 'poisson-process', 'probability']"
3421129,Vague convergence of absolutely continuous measure to absolutely continuous measure,"Suppose we have a sequence of absolutely continuous measure $\mu_n$ converges vaguely to $\mu$ , which is also absolutely continuous. Generally, we have $$\int fd\mu_n \to \int fd\mu,\ \forall f\in C_B(\mathbb{R})$$ where $C_B$ stands for the space of continuous bounded functions. How to find a counterexample if we take $f$ only bounded and Borel-measurable?",['probability-theory']
3421187,"In the derivative notation $f'(x)$, does the $(x)$ mean ""with respect to $x$"" or something else?","I know $d/dx$ means derivative with respect to x (perhaps I am a little unclear on what precisely that means - I'm not quite sure - but I do think I have at least some sense of its meaning). I know (I think!) that... $$\frac{d}{dx} f(x)=f'(x)$$ Here is my question (it's been bugging me incessantly for the last couple weeks): What does $f'(x)$ mean? Does it mean the derivative of $f$ with respect to x ? Or, rather, does it mean the derivative of $f$ with respect to x , evaluated at the point x ? Or something totally different? To make my question more clear, let me ask this too: What does $f'(a)$ mean? Does it mean the derivative with respect to $a$ ? Or the derivative with respect to $x$ evaluated at some point $a$ ? (so that the ""with respect to x "" is actually ""encoded"" in the $f'$ part of the notation!). Etc. Even worse, take $f'(ax)$ , which appears in some derivative computation rules. Does that mean derivative with respect to $ax$ ?? I suspect this confusion may be somewhat related to the maddeningly persistent confusion -- both for me and virtually everyone else -- between a function and its value at a point . Also, teachers tend to use somewhat imprecise notation and language, so I, being someone who likes precision, can sometimes get confused. While I'm at it, I'll note that this confusion may be related to my additional confusion over language like ""the derivative of the sum of two functions,"" where the two ""functions"" are, say, $x^2$ and $x^3$ . But I thought those were mere polynomials , not functions. One might say $f(x)=x^2$ , but one would never say the function itself , $f$ , was equal to $x^2$ , right? I'm confused. We shouldn't say things like ""the function $x^2$ ,"" right? :(","['notation', 'calculus', 'derivatives']"
3421361,I have a number that represents 40%. How do I get the whole?,"I know 8,000 represents 40% of X and I'd like to figure out X. I can start by doubling 8,000 and that gets me 80% of the total. 10% of the 8,000 is 1,000 and I can add that twice to get the remaining 20%. If 8,000 is 40% of X, then X is 18,000. Is that right?",['discrete-mathematics']
3421515,Question about $\operatorname{Aut}(D_\infty)\cong D_\infty$,"Infinite dihedral group $D_\infty:=\langle a,b|\ a^2=b^2=1\rangle = \mathbb Z_2 * \mathbb Z_2$ . For $x\in D_\infty$ , define $\psi_a, \psi_b\in\operatorname{Inn}(D_\infty)$ by $\psi_a(x)=axa^{-1}, \psi_b(x)=bxb^{-1}$ . Define $\omega\in \operatorname{Aut}(D_\infty)$ by $\omega(a)=b,\ \omega(b)=a$ . $\psi_a^2=\psi_b^2=\omega^2=1,\psi_a\omega=\omega\psi_b$ . This can be reduced to $\psi_a^2=\omega^2=1$ . If we can show $\operatorname{Aut}(D_\infty)$ is generated by $\psi_a,\psi_b$ and $\omega$ , then $\operatorname{Aut}(D_\infty)\cong D_\infty$ . My question: How can we prove $\operatorname{Aut}(D_\infty)$ is generated by $\psi_a,\psi_b$ and $\omega$ ? Update: Thanks to Derek Holt and Unit , I gave an answer below, using nearly the same notation, except for $\psi_a$ being relaced by $\sigma$ and without using $\psi_b$ .","['automorphism-group', 'dihedral-groups', 'abstract-algebra', 'free-groups', 'group-theory']"
3421547,How do I calculate the expected value given this density function?,"I want to find the expected value of a random variable whose density function is $$f(x) = \begin{cases}
2xe^{-x^2}, & x > 0 \\
0, &x \leq 0
\end{cases}.$$ For what's worth, all I know is the way the expected value should be found: that is, I need to put an $x$ beside $f(x)$ and then integrate it (probably, from $0$ to $\infty$ ). Here's where the problem turns up. I tried using Wolfram, and it shows me some weird output. In classes, we haven't covered the material concerning this, but I'm expected to be capable of doing it. But I don't know how.","['statistics', 'probability']"
3421574,A question from Spivak's Calculus on Manifolds,"$$
\begin{array}{l}{\text { EXERCISE } 32(2-2) . \text { A function } f: \mathbb{R}^{2} \rightarrow \mathbb{R} \text { is independent of the second vari- }} \\ {\text { able if for each } x \in \mathbb{R} \text { we have } f\left(x, y_{1}\right)=f\left(x, y_{2}\right) \text { for all } y_{1}, y_{2} \in \mathbb{R} . \text { Show that } f} \\ {\text { is independent of the second variable if and only if there is a function } g: \mathbb{R} \rightarrow \mathbb{R}} \\ {\text { such that } f(x, y)=g(x) . \text { What is } f^{\prime}(a, b) \text { in terms of } g^{\prime} ?}\end{array}
$$ My question is short: What is the ''independent'' mean in the question? Thanks...","['multivariable-calculus', 'real-analysis']"
3421636,Proof that the quotient space of $\mathbb{R}^2/L$ where $L$ is a line passing through the origin is not first countable,"Let $L$ be a line in the plane $\Bbb R^2$ passing through origin. How would you prove that the quotient space of $\frac{\mathbb{\Bbb R}^2}{\sim}$ where $\sim$ is equivalence relation defined by $a\sim b$ iff $a=b$ or $a,b\in L$ . How do you prove that this space is not first countable (not every point has a countable fundamental system of neighborhoods)? My idea was to consider the point $L$ collapses to and the fact that every one of it's open neighborhoods is an open set of $\mathbb{R}^2$ containing $L$ . Then I would need to show that for every countable set $S$ of open sets containing $L$ I can always find an open set $A$ that is not the superset of any of the elements of $S$ . How do I show this?","['general-topology', 'first-countable', 'quotient-spaces']"
3421665,Do the vertical bar and semicolon denote the same thing in the context of conditional probability?,"In the CMU Machine Learning Lecture , likelihood function is denoted by $P(D|\theta)$ In the Cornell lecture note , likelihood function is denoted by $P(D;\theta)$ are semicolon and vertical bar the same here?","['notation', 'probability']"
3421689,Covariance zero does not imply independence - but what does it imply?,"As is well known , there are many examples of (pairs of) random variables which have covariance zero, but which are not independent. However, I'm wondering whether there are any general theorems about what having covariance zero does imply.   Are there theorems that say something like, ""Covariance zero, under additional hypothesis H, implies the following relationship between $X$ and $Y$ ,"" or is there essentially no possible general relationship? By the way, although for certain types of distributions/families the implication ""Covariance zero implies independent"" is valid, that is not the kind of theorem I am asking about.  What relationship can there be that is short of full independence?  Any reasonable hypothesis H would be interesting to me, as would references to online resources or standard references.","['independence', 'covariance', 'probability-theory']"
3421795,Disagreement with professor about Cauchy Sequence,"Remark: in this post, all limits are infinite limits so I'll write just $\lim$ instead of $\lim_{n \rightarrow \infty}$ to save time and notation. Also I want to say that I already wrote the proof to the problem below using the definition but I want to discuss this particular proof. Problem: If $\lim x_{2n} = a$ and $\lim x_{2n -1} = a$ , prove that $\lim  x_n = a$ . Attempt: Define $X_n = \{x_{2n}\} \cup \{x_{2n - 1}\}$ . Take two elements from $X_n$ and let's calculate the limit of their difference: $$ \lim (x_{2n} - x_{2n - 1}) = a - a = 0 $$ Therefore $X_n$ is a Cauchy sequence. Now we use the result that if a Cauchy sequence has a subsequence converging to $a$ , then $\lim X_n = a$ . End of proof. Discussion: I showed this proof to my professor. He said it's wrong because in the limit $$ \lim_{n \rightarrow \infty, m \rightarrow \infty} x_n - x_m = 0$$ $n$ and $m$ cannot be related. I accepted his argument. Later, out of curiosity I looked up again the definition of Cauchy sequence and nowhere does it say that $n$ and $m$ cannot be related. Then I brought it up to my professor, and this time he said that I'm choosing a particular $n$ and $m$ and that I cannot do that in proving that $X_n$ is a Cauchy sequence. I thought that I'm not fixing anything since $2n$ and $2n - 1$ are not fixed, and even then I could write $$ \lim (x_{2n} - x_{2m}) = a - a = 0  $$ and what I wrote would still hold true.. However, I chose not to continue the discussion as I felt that if I insisted on this proof, my professor would feel antagonized and some negativity would be created. However I still fail to see why the proof is incorrect. Personally it felt like my professor just didn't accept that I came up with a clever proof.","['sequences-and-series', 'cauchy-sequences', 'real-analysis']"
3422018,Find the number of solutions of $\sin x+2 \sin 2x- \sin 3x=3$,"In $(0 \:\:\pi)$Find the number of solutions of $$\sin x+2 \sin 2x- \sin 3x=3$$ The equation can be written as $$\sin x+4 \sin x \cos x=3+\sin 3x$$ i.e. $$\sin x(1+4\cos x)=3+\sin 3x$$ i.e., $$\sin x(1+4\cos x)=3+\sin x(3-4\sin^2 x)=3+\sin x(4\cos ^2x-1)$$  so $$\sin x(4\cos^2 x-4\cos x-2)=-3$$ Any hint from here?","['trigonometric-series', 'trigonometry', 'inverse-function']"
3422098,"Minimize $f(x, y, z) = \frac{a}{x} + \frac{b}{y} +\frac{c}{z}$ with $x+y+z=1$","Problem: Minimize the function $$f(x, y, z) = \frac{a}{x} + \frac{b}{y} +\frac{c}{z}$$ where $a, b, c$ are constants and $a, b, c, x, y, z > 0$ . In addition, $$x+y+z=1$$ . I have a solution, but I'm not sure if it's right - in particular, what can I do to check it? (I can't plot something like this, and I can't seem to make Wolfram accept $a, b, c$ as constants) Solution: $$f(x, y, z) = f(x, y) = \frac{a}{x} + \frac{b}{y} +\frac{c}{1 - x - y}$$ Now finding when the partial derivatives vanish will give the turning points. $$f_x = -\frac{a}{x^2} + \frac{c}{(1-x-y)^2}$$ $$f_y = -\frac{b}{y^2} + \frac{c}{(1-x-y)^2}$$ If $f_x = f_y = 0$ , then $$\frac{a}{x^2} = \frac{b}{y^2} = \frac{c}{(1-x-y)^2} = \frac{c}{z^2}$$ So $$\begin{cases}
x = \frac{\sqrt{a}}{\sqrt{a}+\sqrt{b}+\sqrt{c}}\\
y = \frac{\sqrt{b}}{\sqrt{a}+\sqrt{b}+\sqrt{c}}\\
z = \frac{\sqrt{c}}{\sqrt{a}+\sqrt{b}+\sqrt{c}}
\end{cases}$$ and $f_{min}=(\sqrt{a}+\sqrt{b}+\sqrt{c})^2$","['optimization', 'multivariable-calculus']"
3422124,Questions on probability distributions,"(1) Let $X_1,X_2$ be two independent gamma-distributed random variables: $X_1 \sim \Gamma(r,1), X_2 \sim \Gamma(s,1)$ . Are $Z_1:=\frac{X_1}{X_1+X_2}$ and $Z_2:= X_1+X_2$ independent? if yes, I have to find their density. I have already found that $X_1=Y_1Y_2$ and $X_2=Y_1(1-Y_2)$ . But I am not done. What is the domain of $Y_1$ and $Y_2$ ? Since $X_1,X_2>0$ I have that $Y_1>0$ and $0<Y_2<1$ . (2) If $X_1 \sim B (a,b), X_2 \sim B(a+b,c),$ prove $X_1 X_2 \sim B(a,b+c)$ (3) If $X \sim N(0,\sigma^2),$ calculate $E(X^n).$ What I know is that $$E(x^n) = \int_{-\infty}^{\infty}x^n\frac{1}{\sqrt{2\pi t}}e^{-x^2/2t}\;dx$$ I've tried solving numerous times it by parts and then taking limits but I keep getting $0$ and not $3t^2$ ! 
Can somebody give me a better direction?","['statistics', 'probability-distributions']"
3422134,How to solve this integral equation,"While solving a question, I got stuck at $$\dfrac{\int_{0}^{1}{\dfrac{\ln^2\left(x\right)}{\left(1-x\right)^2}}dx}{\int_{1}^{\infty}{\dfrac{\ln\left(x\right)}{\left(1-x\right)x}}dx}$$ How should I proceed, what I want is to convert $\mathcal{N}$ as $k\mathcal{D}$ .","['integration', 'definite-integrals']"
3422228,Interesting identity for the value of an integral involving complex-valued square root.,"I am looking for a simple proof of the following identity which holds by numerical tests: $$
\int_{-\infty}^\infty x\left(\sqrt{1+\frac{a}{1-x^2-2i\gamma x}}-\sqrt{1+\frac{a}{1-x^2+2i\gamma x}
}\right)dx={i\pi a}\operatorname{sgn}\gamma,
$$ where $a$ and $\gamma$ are real numbers. The result suggests using the residue theorem, but the residues at singularities of the integrand seem to be zero... Any hint or suggestion are appreciated.","['complex-analysis', 'complex-integration']"
3422253,Confusion regarding the proof that a matrix that preserves angles between vectors from Poole's Linear Algebra: A Modern Introduction,"I'm working my way through Poole's Linear Algebra: A Modern Introduction , and I'm having some trouble fully wrapping my head around his solution to a problem regarding orthogonal matrices and their preservation of angles. The problem states: If $Q$ is an $n\times n$ matrix such that the angels $\angle(Q\mathbf{x},Q\mathbf{y})$ and $\angle(\mathbf{x},\mathbf{y})$ are equal for all vectors $\mathbf{x}$ and $\mathbf{y}$ in $\mathbb{R}^n$ , prove that $Q$ is an orthogonal matrix. Poole's solution is as follows: The given condition means that for all vectors $\mathbf{x}$ and $\mathbf{y}$ , $$\frac{\mathbf{x}\cdot\mathbf{y}}{\Vert\mathbf{x}\Vert\,\Vert\mathbf{y}\Vert}=\frac{Q\mathbf{x}\cdot Q\mathbf{y}}{\Vert Q\mathbf{x}\Vert\,\Vert Q\mathbf{y}\Vert}$$ Let $\mathbf{q}_i$ be the $i^\text{th}$ column of $Q$ . Then $\mathbf{q}_i=Q\mathbf{e}_i$ , where the $\mathbf{e}_i$ are the standard basis vectors. Since the $\mathbf{e}_i$ are orthonormal and the $\mathbf{q}_i$ are unit vectors, we have $$\mathbf{q}_i\cdot \mathbf{q}_j=\frac{\mathbf{q}_i\cdot \mathbf{q}_j}{\Vert\mathbf{q}_i\Vert\,\Vert\mathbf{q}_j\Vert}=\frac{\mathbf{e}_i\cdot \mathbf{e}_j}{\Vert\mathbf{e}_i\Vert\,\Vert\mathbf{e}_j\Vert}=\mathbf{e}_i\cdot \mathbf{e}_j=\begin{cases}0 & i\neq j \\ 1 & i=j\end{cases}$$ It follows that $Q$ is an orthogonal matrix. Nearly all of this reasoning makes sense to me; given that the $\mathbf{q}_i$ are unit vectors, the fact that $Q$ follows logically from the requirement that $$\frac{\mathbf{e}_i\cdot\mathbf{e}_j}{\Vert\mathbf{e}_i\Vert\,\Vert\mathbf{e}_j\Vert}=\frac{Q\mathbf{e}_i\cdot Q\mathbf{e}_j}{\Vert Q\mathbf{e}_i\Vert\,\Vert Q\mathbf{e}_j\Vert}=\frac{\mathbf{q}_i\cdot\mathbf{q}_j}{\Vert\mathbf{q}_i\Vert\,\Vert\mathbf{q}_j\Vert}$$ and the definition of an orthogonal matrix. However, I'm not sure what actually requires that the $\mathbf{q}_i$ be unit vectors. The only other stipulation on $Q$ in the premise for the problem was that it be a square matrix, so I'm assuming that the condition that the $\mathbf{q}_i$ be unit vectors is somehow implicit in the relationship $\frac{\mathbf{x}\cdot\mathbf{y}}{\Vert\mathbf{x}\Vert\,\Vert\mathbf{y}\Vert}=\frac{Q\mathbf{x}\cdot Q\mathbf{y}}{\Vert Q\mathbf{x}\Vert\,\Vert Q\mathbf{y}\Vert}$ for all $\mathbf{x}$ and $\mathbf{y}$ . Can anybody help me understand Poole's justification for the $\mathbf{q}_i$ being unit vectors?","['orthogonality', 'matrices', 'orthogonal-matrices', 'linear-algebra', 'linear-transformations']"
3422269,Bounded in Probability and smaller order in probability,"I wanted to prove that if $X_n$ is bounded in probability and $Y_n = o_p(X_n)$ , then $Y_n \rightarrow 0$ in probability I  know the following definitions that is $X_n$ is bounded in probability meaning that $P(|X_n|<M)>1-\epsilon$ and I know that $Y_n = o_p(X_n)$ implies that $Y_n/X_n \rightarrow0$ in probability","['statistics', 'probability-theory', 'asymptotics']"
3422272,Picking two cards from a box randomly,"There are N cards (numbered from 1 to N) in a box. N is a positive integer. We choose two cards randomly, and we divide the the smaller number on the cards with the other number (so the smaller one is in the numerator...). $Z$ denotes the ratio calculated as we discussed before. What is $\mathbb{E}(Z)$ ? Here is what I've done so far: $\mathbb{E}(X_{1})=\mathbb{E}(X_{2})=N/2$ where $X_{1},X_{2}$ are the numbers on the cards. $\xi=min(X_{1},X_{2})$ and $\eta=max(X_{1},X_{2})$ The question is: $\mathbb{E}(Z)=\mathbb{E}(\frac{\xi}{\eta})$ I think the following is right: $\mathbb{E}(\xi)+\mathbb{E}(\eta)=\mathbb{E}(X_{1})+\mathbb{E}(X_{2})=N$ What should I do next? I dont even know how to calculate $\mathbb{E}(Z)=\mathbb{E}(\frac{\xi}{\eta})$ easily, because $\xi$ and $\eta$ aren't independent, are they?","['probability-distributions', 'maxima-minima', 'probability-theory', 'probability', 'random-variables']"
3422307,Unifying the product of odd powers of function with the product of even powers of the same function in one product.,"Assume that \begin{equation}
\begin{split}
f_n(x)&= \begin{cases} \big(g(x)\big)\cdot \big(g(x)\big)^3\cdot \big(g(x)\big)^5\cdots \big(g(x)\big)^{n-1},& n \ \text{even},\\
\big(g(x)\big)^2\cdot \big(g(x)\big)^4\cdot \big(g(x)\big)^6\cdots \big(g(x)\big)^{n-1},& n \ \text{odd}, \end{cases}\\
&=\begin{cases}\displaystyle\prod^{n-1}_{k\ \ odd} \big(g(x)\big)^{k}  ,& n \ \text{even},\\
\displaystyle\prod^{n-1}_{k\ \ even}\big(g(x)\big)^{k},& n \ \text{odd}, \end{cases}
\end{split}
\end{equation} where $g(x)$ is any polynomial of order $m$ (with real coefficient). I beleive that it's possible to unify the two cases( $n$ odd and even) in one product like $\prod^{n-1}_{k=1}(\cdots)$ or $\prod^{n-2}_{k=0}(\cdots)$ $\big($ or like $\prod^{n/2}_{k=1}(\cdots)$ or $\prod^{(n/2)-1}_{k=0}(\cdots)$$\big)$ which gives the desired $f_n(x)$ for each $n$ . I hope someone helps me to figure it out.","['products', 'functions', 'functional-analysis', 'real-analysis']"
3422363,How do I denote and handle the restriction of function from a disjoint union of the same set?,"I need to construct a homeomorphism between $\mathbb{R}^2\sqcup\mathbb{R}^2$ and $\mathbb{S}=\{(x,y,z)\in \mathbb{R}^2| x^2-y^2-z^2=1\}$ . The general idea I have is to define $g:\mathbb{R}^2\rightarrow \mathbb{S}$ to have the rule of assignment $g:(y,z)\longmapsto (\sqrt{y^2+z^2+1},y,z)$ and define $h:\mathbb{R}^2\rightarrow \mathbb{S}$ to have the rule of assignment $h:(y,z)\longmapsto (-\sqrt{y^2+z^2+1},y,z)$ . But how do I denote the restrictions of my new continuous function $f:\mathbb{R}^2\sqcup \mathbb{R}^2\rightarrow \mathbb{S}$ formed on the basis of the Universal Property of Disjoint Unions? And have I even defined the proper functions $g$ and $h$ to get a homeomorphism? Thank you. Let me know if you need any clarifications. Edit: I realized that I said something incorrect and deleted that part.","['analytic-geometry', 'general-topology', 'functions']"
3422428,Left regular action of $L^1(G)$ on $L^2(G)$ is given by convolution.,"This seems basic and I couldn't find it by googling so I thought I'd post it along with an answer. If $G$ is a locally compact topological group and $\pi:G\rightarrow U(H)$ is a unitary representation of $G$ on a Hilbert space $H$ , we can talk about an action, $\pi'$ , of $L^1(G, \mathbb{C})$ on $H$ . Namely for $f\in L^1(G, \mathbb{C})$ , and $v\in H$ , define $$\pi'(f)v := \int_G f(y)\cdot(\pi(y)(v))dy$$ where $dy$ is Haar measure and the integral requires the theory of integration of Banach space valued functions.  That is, we define the integral as a limit of integrals of step functions which form a Cauchy sequence in $L^1(G,H)$ and which converge pointwise almost everywhere to $y \mapsto f(y)\cdot(\pi(y)(v))$ . In particular if $\pi$ is the left-regular representation of $G$ on $L^2(G, \mathbb{C})$ , and $g\in L^2(G, \mathbb{C})$ ,   one gets $\pi'(f)(g)$ that looks a lot like the ordinary old convolution of $f$ and $g$ .  But how do you actually know they're the same?  That is how do you know $$x\mapsto \bigg((f\ast g)(x)=\int_Gf(y)g(y^{-1}x)dy\bigg)$$ is the same element of $L^2(G, \mathbb{C})$ as $$\int_G f(y)(L_yg)dy$$ since the former is a family of ordinary integrals parametrized by $x$ and the latter is a single integral of an $L^2(G, \mathbb{C})$ valued function? More generally, let $X$ and $Y$ be measure spaces.  Suppose $$\Phi:X\times Y\rightarrow \mathbb{C}$$ has the property that $$\Phi_x:Y\rightarrow \mathbb{C}, y\mapsto \Phi(x,y)$$ is in $L^1(Y, \mathbb{C})$ for every $x\in X$ , and $$\Phi^y:X\rightarrow \mathbb{C}, x\mapsto \Phi(x,y)$$ is in $L^p(X)$ for all $y$ and some fixed $p\in [1,\infty)$ .  Suppose further that the map $$Y\mapsto L^p(X, \mathbb{C}), y \mapsto \Phi^y$$ lies in $L^1(Y,L^p(X, \mathbb{C}))$ in the sense of the theory of integration of functions taking values in a Banach space.  Denote by $\int_Y \Phi^y dy\in L^p(X, \mathbb{C}) $ its integral in the sense of this theory.  How do we know that $$ (\int_Y \Phi^y dy)(x) = \int_Y\Phi(x,y)dy~~~~~~~~~~~~~~(1)$$ holds for almost all $x\in X$ ?",['functional-analysis']
3422459,Proving that a finite simple group (order < 100) is either abelian or has order 60 [duplicate],"This question already has answers here : $G$ is non abelian simple group of order $<100$ then $G\cong A_5$ (3 answers) Closed 4 years ago . As the title suggests, I'm asked to Prove that a finite simple group $G$ of order less than $100$ is either abelian or has order $60.$ I approached the problem by saying that $G$ could either have a prime order or a non-prime order and have already proven that, if $G$ has order prime, it has to be abelian. However, I'm stuck on what to do for the second case. I've seen examples online where they prove that the order of a finite simple nonabelian group $G$ is less than $60$ , but how do I prove that there is no other order that $G$ can be if it is nonabelian that is between $61$ and $100?$","['simple-groups', 'group-theory', 'abstract-algebra', 'proof-writing']"
3422525,Proving Convergence Using Cauchy Sequences,"Today I learned about Cauchy Sequences, defined as the following: A sequence $(x_n)$ is a Cauchy Sequence if $\forall\varepsilon>0,\exists N\in\mathbb{N}\ \forall n,m\geq N: |x_n-x_m|<\varepsilon$ . Assuming we are dealing with a complete metric space, all Cauchy Sequences converge, right? If so, for what sequences is it easier to show they are Cauchy in order to show they converge (as opposed to the limit definition of a convergent sequence)? My professor said they tend to be sequences whose limit is not immediately clear but I can't think of any such examples. Could someone please share an example of a convergent sequence whose limit and perhaps bounds are non-trivial but can be proven to be convergent by proving they are Cauchy? Thank you!","['cauchy-sequences', 'sequences-and-series']"
3422549,Polyhedra that cover the sphere more than once,"Spherical polyhedra can be thought of as tilings of the sphere. I am interested in the possibility of double covering or multiple covering tilings of the sphere, but I can't find much information about them and would like to know where I can learn more. (It might be that they are known by a different terminology.) A ""double covering tiling"" would mean that as you add tiles to the sphere there will be a place where they don't line up, but if you keep adding tiles on top of the existing ones and keep going, they will line up once every part of the sphere is covered by exactly two tiles. This idea can be extended to triple cover tilings and so on, assuming those exist. This idea has to be made precise in the right way - that's also part of the question. In the comments, Ivan Neretin gives an example that would make the question trivial, but it can be excluded by stipulating that the angles have to add up to $2\pi$ at each vertex, so let's add that as a requirement. (edit: or in fact let's not, since that would exclude multiple coverings entirely.) It seems that double cover tilings exist and are known. Wikipedia's page on uniform polyhedra says ""There are some non-orientable polyhedra that have double covers satisfying the definition of a uniform polyhedron,"" but doesn't give further details except to say that they aren't usually counted as uniform polyhedra. A web page on tiling the sphere with triangles states that the spherical triangle with angles 90 $^\circ$ , 75 $^\circ$ and 45 $^\circ$ will give a double covering of the sphere and that a 75 $^\circ$ -60 $^\circ$ -60 $^\circ$ triangle gives a five-fold tiling. In general I'm looking for more information about multiple covering sphere tilings, but here are my specific questions about them: Are there any/many other known examples of double coverings, aside from the 90 $^\circ$ -75 $^\circ$ -45 $^\circ$ triangle? Are there any examples of double covering tilings that are made of regular polygons and are vertex transitive, as hinted at on the Wikipedia page? I would like to see a specific example. What about triple and higher order coverings - are there known examples beside the 75 $^\circ$ -60 $^\circ$ -60 $^\circ$ triangle, and are there any composed of regular polygons? Do there exist $n$ -fold coverings for every $n$ ? Are there ""infinite covering tilings"" in the sense that you can keep adding tiles in the same repeated pattern but the edges will never quite line up, so each part of the sphere will be covered by an infinite number of tiles? If so, are there such tilings where all the tiles are regular polygons? Is there a set of tiles such that you can always add more tiles, but not in a repeating pattern? This would be a spherical analog of aperiodic tilings of the plane, such as Penrose tiles.","['polyhedra', 'tiling', 'geometry', 'spherical-geometry']"
3422567,Find the sum of series: $\frac{1}{\sqrt{1}+\sqrt{2}}+\frac{1}{\sqrt{3}+\sqrt{4}}+...+\frac{1}{\sqrt{97}+\sqrt{98}}+\frac{1}{\sqrt{99}+\sqrt{100}}$,Find the sum of series: $$\frac{1}{\sqrt{1}+\sqrt{2}}+\frac{1}{\sqrt{3}+\sqrt{4}}+\frac{1}{\sqrt{5}+\sqrt{6}}+...+\frac{1}{\sqrt{97}+\sqrt{98}}+\frac{1}{\sqrt{99}+\sqrt{100}}$$ My Attempt: I tried to go by telescopic method but nothing appears to be cancelling. Something similar was given in a book by Titu Andreescu. I will try to reproduce Let $S=\frac{1}{\sqrt{1}+\sqrt{2}}+\frac{1}{\sqrt{3}+\sqrt{4}}+\frac{1}{\sqrt{5}+\sqrt{6}}+...+\frac{1}{\sqrt{97}+\sqrt{98}}+\frac{1}{\sqrt{99}+\sqrt{100}}$ Further let $T=\frac{1}{\sqrt{1}+\sqrt{2}}+\frac{1}{\sqrt{3}+\sqrt{4}}+\frac{1}{\sqrt{5}+\sqrt{6}}+...+\frac{1}{\sqrt{97}+\sqrt{98}}+\frac{1}{\sqrt{99}+\sqrt{100}}$ Clearly $S+T=\sqrt{100}-1=9$ Also $S-T=2S+1-\sqrt{100}$ and $S>T$ $\Rightarrow 2S>S+T$ $\Rightarrow S>4.5$ This is the best I could come up with,"['algebra-precalculus', 'telescopic-series', 'summation', 'sequences-and-series']"
3422613,Geodesics in the Hyperbolic Plane,"In my the class the definition given for a curve $c(t)$ to be a geodesic is that $c''(t)$ is orthogonal to the surface at the point $c(t)$ . The hyperbolic plane is the upper half plane with the metric: \begin{equation}
ds^2 = \frac{1}{y^2}(dx^2 + dy^2)
\end{equation} I have a fundamental misunderstanding somewhere, because I cannot think why the following curve is not a geodesic in $\mathbb{H}$ : $c(t) = (t, t)$ for $t > 0$ . This curve has second derivative as zero, but we have learnt the geodesics of $\mathbb{H}$ and this is not one of them.  Where am I reasoning this incorrectly?","['geodesic', 'hyperbolic-geometry', 'geometry', 'differential-geometry']"
3422635,Geometrically Constructible Angles,"Angle 30 can be constructed, through drawing an equilateral triangle, constructing angle 120, bisecting it multiple times and getting angle 30. Is it possible to contruct 3 degrees using geometric theorems and how?",['geometry']
3422840,Proof of convolution theorem for Laplace transform,"The convolution theorem for Laplace transform states that $$\mathcal{L}\{f*g\}=\mathcal{L}\{f\}\cdot\mathcal{L}\{g\}.$$ The standard proof uses Fubini-like argument of switching the order of integration: $$\int_0^\infty d\tau \int_{\tau}^\infty  e^{-st}f(t-\tau)g(\tau)\,dt=\int_0^\infty dt\int_0^te^{-st}f(t-\tau)g(\tau)\,d\tau$$ Fubini's theorem says that one can switch the order of integration . But what we have in the iterated integrals are not integrals, but limits of integrals (i.e., improper integrals). Are we justified to treat them like ""proper"" integrals and switch their order?","['integration', 'improper-integrals', 'convolution', 'laplace-transform', 'fubini-tonelli-theorems']"
3422890,On the derivative of a function that is its own inverse,"I was recently doing the following question: Let $f$ be a differentiable function such that $f(f(x))$ = $x$ for all $x\in[0,1]$ . Suppose $f(0)=1$ . Determine the value of $$\int_0^1(x-f(x))^{2016} dx$$ Now, in the light of the fact that $f(f(x))=x$ I thought the substitution $t=f(x)$ may be something to investigate. Of course, if $t=f(x)$ then $dx = {dt\over f’(x)}$ so we may write the integral as $$\int_1^0(f(t)-t){dt\over f’(x)}$$ Now, I noticed that we might be able to exploit the fact* that $f(x)$ is its own inverse to resolve $f’(x)$ . Notice that $${f^{-1}}’(x) = \frac 1{f’(x)}$$ But, since $f(x)=f^{-1}(x)$ we have $$(f’(x))^2 = 1$$ save for the degenerate case that $f’(x) = 0$ — which if true means the function isn’t invertible in the first place (even if it’s only at some certain points those may be extrema and may challenge the invertibility of the function, but I digress). But obviously it is not necessary that a function’s derivative to be $1$ or $-1$ for it to be its own inverse, right? I thought of a function on $[0,\infty)$ $$g(x)=\left(a-x^n\right)^\frac 1n$$ (where $n$ is a natural number) whose derivative is $(1-n)x^{n-1}(a-x^n)^{\frac 1n -1}$ (clearly not $\pm 1$ ) and its inverse is $g^{-1}(x)=(a-x^n)^\frac 1n$ (clearly $g(x)$ , not even making an attempt to disguise itself !). So what’s the deal here? Please help me make sense of it. *I figured that it isn’t actually necessarily true that $f(f(x)) = x$ means $f \equiv f^{-1}$ , since $f^{-1}$ may not even exist, and I ended up solving that problem by instead trying the substitution $x=f(t)$ . Interestingly, continuing with the original substitution and assumption that $f(x)=f^{-1}(x)$ and writing $f’(x)$ as a function of $t$ is consistent with the other substitution. However, this has no actual bearing on the question.","['contest-math', 'calculus', 'derivatives', 'real-analysis']"
3423011,$\sum_{n=1}^{\infty} 1/\sqrt[n]{n}$ converge,"Does the series: $$
\sum_{n=1}^{\infty} \frac{1}{\sqrt[n]{n}}
$$ converge or diverge? I'm unsure where to start with this question. I know that $n$ th root of $n$ converges to $1$ but not sure about its reciprocal.","['calculus', 'convergence-divergence', 'analysis', 'sequences-and-series']"
3423047,"If $\mathfrak{c} = 2^\mathbb{N}$ why $|D^\mathfrak{c}|=\mathfrak{c}$ with $D = \{0, 1\}$.","Let $\mathfrak{c} = 2^\mathbb{N}$ and $D^\mathfrak{c}$ be the topological product of $\mathfrak{c}$ copies
of the discrete Abelian group $D = \{0, 1\}$ . If $D^\mathfrak{c}$ taken as a group is denoted by $G$ ,
and $D^\mathfrak{c}$ taken as a topological space is denoted by $X$ . Why $|G| = |X| = c$ ? I think maybe it has to do with Boolean but I'm not sure. On the other hand I am correct or I am wrong that $|G| = |D^\mathfrak{c}|=|D|^\mathfrak{c} =2^\mathfrak{c}> \mathfrak{c}$ ? I am studying this example of Korovin Orbits in the book Topological groups and related structures of Mikhail Tkachenko. Thank you for your Help!","['elementary-set-theory', 'general-topology', 'cardinals', 'topological-groups']"
3423057,Probability of a difference in a normal distribution,"I have a normal distribution of cabbage with $\sigma=0.7$ and $\mu=2.4$ kg. The question is; what is the probability that $2$ randomly chosen cabbage has a weight-difference of more than $1$ kg? I don't know where to start, other than $P(|X-Y|>1)$","['statistics', 'normal-distribution', 'probability']"
3423097,Average value of $\ln(1+e^x)$ when $x$ is normally distributed,"Does the following integral admit a closed form answer: $$\int_{-\infty}^\infty\mathrm d x \exp\left(-\frac{(x-\mu)^2}{2\nu}\right) \ln(1+e^x)$$ where $\nu>0$ and $\mu$ are finite real parameters. This is just the average value of $\ln(1+e^x)$ , when $x$ is normally distributed with mean $\mu$ and variance $\nu$ (forgetting the normalization constant). I tried with symbolic processing software (Mathematica and WolframAlpha), and both return the input expression unevaluated, suggesting that a closed form solution does not exist, or at least it is not obvious.","['integration', 'definite-integrals', 'normal-distribution']"
3423281,"Evaluating $\cos^{-1}\left(\,\sin\left(\pi^2\right)\,\right)$ without a calculator","This is one of the questions I got in my math exam: Evaluate without a calculator: $$\cos^{-1}\left(\,\sin\left(\pi^2\right)\,\right)$$ I just can't figure out how to evaluate $\sin(\pi^2)$ without using a calculator. Thanks!",['trigonometry']
3423375,"Contour integration with a branch cut on [0,1]","I am asked to evaluate this integral using residues. $$\int_0^1 \frac{1}{x^\omega(1-x)^\omega} \, dx$$ where $$0<\omega<1$$ I'm thinking since I have branch points at $0$ and $1$ that I ought to integrate over a contour that goes around a branch cut $[0,1]$ . However, I'm confused as to how to decide on a function and the values of the arguments above and below the cut. Any help is appreciated!","['complex-analysis', 'contour-integration', 'branch-cuts']"
3423393,Coefficient of generating function (hard),"How to find the coefficient of $x^{46}$ in $\dfrac{1}{1 - x^3 -x^4 -x^{20}}$ without software like Maple?
I tried everything... :(","['permutations', 'combinations', 'discrete-mathematics', 'generating-functions']"
3423409,"What is the purely synthetic construction for producing a projective collineation, given two lines and three points on each line?","It is a well-known fact (sometimes called the fundamental theorem of projective geometry) that given two lines and three points on each of the two lines, there is a unique projectivity between the two lines. It also appears to be a known fact that this projectivity can be extended to a collineation of the entire real projective plane. I'm looking for a specific recipe (draw these lines here, construct the intersection, draw these other lines, etc.) for how to actually implement the collineation, without needing to pass to coordinates. It feels like there's almost such a recipe in various sources, (e.g., Coxeter, partly Richter-Gebert) but I haven't been able to quite make it work. (Specifically, I want to be able to select a line, three points on the line, another line, three points on that line, and then any other 7th point, and be able to construct the image point via a tool in GeoGebra.)","['projective-geometry', 'geometry']"
3423543,"Probability of guessing $0 \le k \le 6$ numbers and their positions correctly, if 6 numbers are drawn from a pool of 49 numbers.","Suppose 6 numbers are drawn one after another from a pool of 49 distinct  numbers. Once a number has been drawn, it is not put back in the pool. The number itself and its position are noted. You are now asked to guess 6 distinct numbers from 1 to 49. What is the probability that $0 \le k \le 6$ of the numbers you guessed are the correct number and in the correct position and that the other $6-k$ numbers are either in the wrong position, or not among the drawn numbers? I'm not sure what the combinatorial argument is, to calculate this probability correctly. There are $49^\underline{6} = 49*48*\dotsc*44$ possible ways to choose 6 numbers and hence the probability of guessing all six numbers correctly (in the correct order) is $1/49^\underline{6}$ . However, what happens, for example, if $k=4$ ? Do I have to simply multiply this probability by the number of ways to select 4 out of 6 numbers and the number of ways to choose 2 numbers from the remaining 43 numbers or do I still have to take the ordering of these 4 or 2 numbers into account? I'm not sure which of the following two terms is correct or if they are both equally wrong (for $k = 4$ ): \begin{align*}
\frac{{6 \choose 4}{43 \choose 2}}{49^\underline{6}}, \quad \frac{{6 \choose 4}4!{43 \choose 2}2!}{49^\underline{6}}
\end{align*} For clarification: Let 1,2,3,4,5,6 be the numbers drawn from the pool in this order. If $k=6$ then the only valid guess is 1,2,3,4,5,6 ( 2,1,3,4,5,6 would be not) If $k=3$ then 1,2,3,6,4,5 , 1,2,3,43,20,10 , 43,2,3,4,20,10 , 43,20,10,4,5,6 are all valid but a guess like 1,2,4,43,20,10 is not (but would be for $k=2$ ). If $k=0$ then all numbers from 1 to 6 must be at different positions or not included in the guess.","['combinatorics', 'probability']"
3423547,Further simplifying geometric algebraic equations,"In the Figure below, the line segments $OA$ and $OB'$ make  angle $\theta$ and $-\theta$ respectively with the positive x axis.  Similiarly $AB$ is orthogonal to the x axis with $D$ the point of intersection and let $d$ be the distance between point $O$ and $D$ . $AB$ is then tilted about point $D$ by an unknown angle $\beta$ (in ccw direction)   to form $A'B'$ as shown in the figure. My question is given that the variables $\theta,\, d, \, y_1$ and $y_2$ are known (experimentally), how do I find the value of $\beta$ ? By projecting $y_1$ and $y_2$ on the x axis and using simple algebraic and trigonometric manipulation I obtain following 2 equations for $y_1$ and $y_2$ as a function of $\beta$ . $$y_1(\beta) = \dfrac{d}{\sin(\beta) + \dfrac{\cos(\beta)}{\tan(\theta)}} \, $$ $\qquad$ and $$y_2(\beta) = \dfrac{d}{-\sin(\beta) + \dfrac{\cos(\beta)}{\tan(\theta)}}$$","['trigonometry', 'calculus', 'geometry']"
3423595,Show that every group of order 1965 is isomorphic to $\mathbb{Z}/393\mathbb{Z} \rtimes \mathbb{Z}/5\mathbb{Z} $,"Where do I start in such problem? $1965=3\times 131\times 5.$ I think was able to show $n_3=1$ so $S_3$ is normal and G is not simple. $n_{131}=1$ What else can I do there? Should I split in the abelian and non-abelian cases or something like it?
Should I use that to show that G is abelian somehow? 
Would you know where I can find similar exercises? Thank you for your time and patience.","['semidirect-product', 'group-theory', 'sylow-theory']"
3423655,Is Schrodinger operator with potential self adjoint,"Consider real-valued one-dimensional Schrodinger operator with potential $V(x)$ , s.t. $L: H^2(\mathbb{R})\rightarrow L^2(\mathbb{R})$ $$L(u)=-u''+V(x)u$$ with $V(x)$ bounded
Is this operator self-adjoint?","['functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
3423755,Suppose that the prime $p$ is totally ramified in $O_{K_1}$ and unramified in $O_{K_2}$. Prove that $K_1 \cap K_2=\Bbb Q$.,"Let $K_1$ and $K_2$ be algebraic number fields. Suppose that the prime $p$ is totally ramified in $O_{K_1}$ and unramified in $O_{K_2}$ . Prove that $K_1 \cap K_2=\Bbb Q$ . For unramified $<p>=P_1^{e_1}$ where all $e_1=[K_1:\Bbb Q]$ in $K_1$ and $<p>=Q_1\cdots Q_s$ in $K_2$ . Do we get anything from here? You can downvote me but pls give some hint! Edit: After putting some thought and reading comments(thanks to Jyrki) I thinjk that I have to consider the tower $ K_1K_2
  /     \
 /       \ 
K_1       K_2
\         /
 \       /
   K_1\cap K_2
       |
       |
      \Bbb Q$ Then we have to consider some fixed field...","['number-theory', 'abstract-algebra', 'algebraic-number-theory', 'dedekind-domain']"
3423870,Mind-boggling pattern based virtuoso conundrum,"My math teacher is a very funny guy. He gave us this "" virtuoso "" math problem: $$\frac{1}{x(x+1)} + \frac{1}{(x+1)(x+2)} + \frac{1}{(x+2)(x+3)} + ... + \frac{1}{(x+99)(x+100)} $$ I like math, but I only stick to the what I'm learning. My math teacher teaches our Honours Algebra II classes, our Math Team and our Math Research classes, but I only have him for regular Algebra II. I suspect this is the type of problems he shows to his more advanced classes. I can't help but look at this and feel utterly dumbfounded. I've been trying to solve this problem for 35 minutes, but to no avail. My math teacher loves problems like this. I understand that this is a very advanced, truly virtuoso math forum, that this problem is a very straight-forward problem for mathematicians of your calibre, and that it could very much be wasting your time but I have no choice but to ask for help. Thank you so much for reading this everyone! God speed and quod erat demonstrandum! ∎
p.s. I know that there is a pattern between the denominators but I have no idea how to solve it without writing everything out by hand and working with a least common denominator",['algebra-precalculus']
3423927,Does the tensor product respect semidefinite ordering in this way?,"I'll use $\succeq$ to denote the positive semidefinite ordering: for square matrices $X,Y$ , one has $X \succeq Y$ iff $X - Y$ is positive semidefinite. It's a well known fact that if $X, Y \succeq 0$ then $X \otimes Y \succeq 0$ . However, if one has two pairs of matrices with $X \succeq Y$ and $X' \succeq Y'$ , it isn't necessarily the case that $X \otimes X' \succeq Y \otimes Y'$ (for instance, if $X = X' = I$ and $Y = Y' = -2I$ ). My question is: if we add that $Y, Y' \succeq 0$ , so our assumptions are $$ X \succeq Y \succeq 0,\ \ \ X' \succeq Y' \succeq 0 $$ Is it necessarily true that $X \otimes X' \succeq Y \otimes Y'$ ? I personally feel that this should be true (and I could swear I've seen this result before but can't find it anywhere), but I'm struggling to prove it. Does anyone have a reference for this fact, and/or know how to prove (or disprove) it?","['positive-semidefinite', 'linear-algebra', 'tensor-products', 'quantum-information', 'quantum-computation']"
3423936,elliptic curve analogue of ambiguous class number formula,"The ambiguous class number formula (first proven by Chevalley) gives the
number of (strongly) ambiguous ideal classes in terms of the class number $h(K)$ of
the base field $K$ , the number $t$ of ramified primes (including those at infinity), and the index of the units that are norms (of integers and units, respectively) inside the unit group $E_K$ of $K$ . (see for example this article by F. Lemmermeyer ) Since it is well known that Selmer groups of elliptic curves (over a base field $K$ ) and the class group of $K$ are 'closely' related, is there an analogue of this formula for Selmer groups of elliptic curves.","['class-field-theory', 'number-theory', 'algebraic-number-theory', 'elliptic-curves']"
3424001,What are all the possibilities of $A$ s.t. $\det(A)=k$?,"Suppose we have $A \in M_3(\Bbb N\cup\{0\})$ s.t. sum of the elements of each row is $k $ for some fixed $k\in \Bbb N\cup\{0\}$ . What are all the possibilities of $A$ s.t. $\det(A)=k$ ? We can start from $k=0$ here we have to have the matrix to be zero. For $k=2$ , I am getting $$
    \begin{pmatrix}
    1 & 1 & 0 \\
    0 & 1 & 1 \\
    1 & 0 & 1 \\
    \end{pmatrix}
$$ as one such matrix then what are the other possibilities and can you give me a general question. Then what about $k=1,3$ And so on for any $k$ can we give a general structure? Then this question can be extended to $A\in M_4(\Bbb N \cup{0})$ . By the way, what I want is if someone can give me some of the partial answers as the general answer might be too strong to expect!","['contest-math', 'determinant', 'matrices', 'combinatorics', 'discrete-mathematics']"
3424030,Understanding proof when equality holds in Minkowski's inequality,"I have a simple question about a fact that is constantly mentioned when we have $\|f+g\|_p = \|f\|_p + \|g\|_p$ in $L^p(\mu)$ space for $1<p<\infty$ (I hope someone find this useful). I read the arguments given here , and here and a proof almost related here . In all of these proofs it's mentioned that when the two Hölder's inequalities below that result on the steps of Minkowski's inequality: $$\int |f| \cdot |f+g|^{p-1} \, d\mu \leq \|f\|_p \cdot \|f+g\|_p^{p-1}$$ $$\int |g| \cdot |f+g|^{p-1} \, d\mu \leq \|g\|_p \cdot \|f+g\|_p^{p-1}$$ These two holds with equality thanks to the given hypothesis, and that's clear to me. However, then comes a fact that is not as clear to me as it should be (since I found no justification for this) that is: these two equalities implies that there is $\alpha,\beta \geq 0$ such that $|f|^p = \alpha |f+g|^p$ and $|g|^p = \beta |f+g|^p$ almost everywhere. Any hint or reference is appreciated. Thanks!","['measure-theory', 'lebesgue-integral', 'lp-spaces', 'inequality', 'holder-inequality']"
3424050,Discontinuities of $\sum_{n=2}^{\infty} \frac{\sin(nx)}{\ln(n)}$,"Let us consider  the trigonometric series $\phi(x)=\sum_{n=2}^{\infty} \frac{\sin(nx)}{\ln(n)}$ . It is every where convergent. However it is not Lebesgue inntegrable in $[-\pi,\pi]$ . What is the set of continuous points of $\phi$ ?","['fourier-series', 'real-analysis']"
3424092,Why does $g$ have a global min at $x=0$ while $f$ does not and yet $g$ is defined in terms of $f$? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question $1.$ Let $f(x) = x^2\sin(1/x)$ for $x\neq 0$ and $f(0)=0.$ Show that $0$ is a critical point of $f$ that is not a local maximum nor a local minimum nor an inflection point. $2.$ Let $g(x) = 2x^2+f(x).$ Show that $g$ does have a global minimum at $0,$ but $g'(x)$ changes sign infinitely often on both $(0,\epsilon)$ and $(-\epsilon,0)$ for any $\epsilon >0.$ Here's what I've done. $1.$ By the product rule and chain rule, $f'(x) = 2x\sin(1/x)-\cos(1/x).$ We will show that $f'(0)=0$ is undefined at $x=0,$ which will show that $f$ has a critical point at $x=0.$ $f'(0)=\lim\limits_{h\to 0}\dfrac{f(h)}{h}=\lim\limits_{h\to 0}\dfrac{h^2\sin(1/h)}{h}\\
=\lim\limits_{h\to0} h\sin(1/h)=\lim\limits.$ Since $-h\leq h\sin(1/h)\leq h\;\forall h\geq 0$ and $h\leq h\sin(1/h)\leq -h\;\forall h\leq 0,$ by the Squeeze Theorem, $\lim\limits_{h\to0}h\sin(1/h)=0=\lim\limits_{h\to 0}h=0.$ (by the Squeeze Theorem). We first prove that $(0,0)$ is neither a local minimum nor a local maximum of $f.$ Notice that whenever $x$ is of the form $\dfrac{1}{-\frac\pi2+2n\pi},n\in\mathbb{Z},$ $\sin(1/x)=-1.$ We know this because $\sin(1/x)=-1$ whenever $1/x$ is of the form $-\frac\pi2+2n\pi,n\in\mathbb{Z}.$ As well, $\sin(1/x)=1$ whenever $x$ is of the form $\dfrac{1}{\frac\pi2+2n\pi},n\in\mathbb{Z}\Rightarrow f(x)=x^2>0$ (as $x\neq 0$ ). Let $x_n = \dfrac{1}{-\frac\pi2+2n\pi},n\in\mathbb{N}$ and $y_n = \dfrac{1}{\frac\pi2+2n\pi},n\in\mathbb{N}.$ Note that $\lim\limits_{n\to\infty}f(x_n)=\lim\limits_{n\to\infty} -x_n^2=\lim\limits_{n\to\infty} f(y_n)=\lim\limits_{n\to\infty} y_n^2=0.$ Then $\forall \epsilon>0,$ there exists $N\in\mathbb{N}$ such that $\forall n\geq N, |x_n|<\epsilon$ and $\exists N_2\in\mathbb{N}$ such that $\forall n\geq N_2, |y_n|<\epsilon.$ Thus, when $N_3=\max\{N,N_2\},$ we have that $-\epsilon < x_n,y_n<\epsilon.$ So there exist $x_n,y_n \in (-\epsilon,\epsilon)\;\forall \epsilon >0$ for some $n\in\mathbb{N}$ such that $f(x_n) = -x_n^2 < 0 <f(y_n) = y_n^2.$ A similar proof for $x<0$ can be shown by using the opposite values of $x_n$ and $y_n.$ Thus $f$ does not have a local maximum or minimum at $x=0.$ By the product rule and chain rule, $f''(x)=2\sin(1/x)-\dfrac{2}{x}\cos(1/x)-\dfrac{\sin(1/x)}{x^2}.$ When $x=\dfrac{1}{2n\pi},n\in\mathbb{Z}\backslash\{0\},f''(x)=-4n\pi.$ When $x=\dfrac{1}{-\frac\pi2+2n\pi},n\in\mathbb{Z},f''(x)=-2+(-\frac\pi2+2n\pi)^2>0.$ So, using a similar approach to above, with $x_n = \dfrac{1}{2n\pi}$ and $y_n=\dfrac{1}{-\frac\pi2+2n\pi},n\in\mathbb{N},$ we can show that $f$ does not have an inflection point when $x>0.$ The proof for $x<0$ also involves using negative values for $x_n$ and $y_n.$ $2.$ Since $f(0)=0, g(0)=2(0)^2+f(0)=0.$ We have that since $\sin(1/x)\geq -1\;\forall x\in\mathbb{R}\backslash\{0\}, g(x)=2x^2+x^2\sin(1/x)\geq x^2>0\;\forall x\in\mathbb{R}\backslash \{0\}.$ Hence $(0,0)$ is a global minimum for $g.$ Also, $g'(0)=0,$ so it is indeed a minimum. Here is the proof:
By the derivative definition, $$g'(0) =\lim\limits_{h\to 0}\dfrac{g(h)-g(0)}{h}\\
=\lim\limits_{h\to 0}\dfrac{2h^2+h^2\sin(1/h)}{h}.$$ From above, $\lim\limits_{h\to 0}\dfrac{h^2\sin(1/h)}{h}=0,$ and so since $\lim\limits_{h\to 0}\dfrac{2h^2}{h}=\lim\limits_{h\to 0}2h=0,g'(0)=0.$ Now, $g'(x) = 4x+2x\sin(1/x)-\cos(1/x).$ We show that $g'(x)$ changes sign infinitely often on $(-\epsilon,0)$ and $(0,\epsilon)$ for any $\epsilon >0.$ We first show this result for $(0,\epsilon)$ . Whenever $x=\dfrac{1}{n\pi},n\in\mathbb{N},g'(x)=\dfrac{4}{n\pi}+1>0.$ Whenever $x=\dfrac{1}{2n\pi},n\in\mathbb{N},g'(x)=\dfrac{2}{n\pi}-1<0.$ Let $(y_n)_{n=1}^\infty$ be the sequence defined by $y_n=\dfrac{1}{2n\pi}$ and let $(x_n)_{n=1}^\infty$ be the sequence defined by $x_n=\dfrac{1}{n\pi}.$ We have that $\lim\limits_{n\to\infty} x_n = \lim\limits_{n\to\infty} y_n=0.$ So $\exists N_1\in \mathbb{N}$ such that $n \geq N_1 \Rightarrow |x_n| < \epsilon, \forall \epsilon >0$ and $\exists N_1\in\mathbb{N}$ such that $n \geq N_2 \Rightarrow |y_n| < \epsilon, \forall \epsilon >0.$ Pick $N_4=\max\{N_1,N_2\}.$ So for all $n \geq N_4, 0< |y_n| = y_n <\epsilon$ and $0<|x_n|=x_n<\epsilon.$ Since $g'(y_n)<0$ at every $y_n$ and $g'(x_n)>0$ at every $x_n,$ we have that $g'(x)$ changes sign infinitely many times on $(0,\epsilon)$ . The proof that $g'(x)$ changes sign inifinitely often on $(-\epsilon,0)$ is very similar. Notice that when $x=-\dfrac{1}{2n\pi},n\in\mathbb{N},g'(x)<0$ and when $x=-\dfrac{1}{n\pi},n\in\mathbb{N},n>1,g'(x)>0.$ We set $y_{n_2} =-\dfrac{1}{2n_2\pi}$ and $x_{n_2}=-\dfrac{1}{n_2\pi},n_2\in\mathbb{N}$ and obtain the same result as above. Edit: some notes: the work above can definitely be simplified. Also, I need to show that $f'(0)=0$ using the definition of a derivative and the Squeeze Theorem.","['real-analysis', 'calculus', 'functional-analysis', 'limits', 'derivatives']"
3424111,When is Euler's totient function for two different integers equal?,"Let $\varphi(\cdot)$ denote the Euler totient function. We know that if $n = p_1^{k_1}\cdot p_2^{k_2} \cdots p_l^{k_l}$ is the unique (up to ordering) representation of a positive integer in terms of primes, then $$\varphi(n) = p_1^{k_1-1}\cdot p_2^{k_2-1} \cdots p_l^{k_l-1} \cdot (p_1-1) \cdot (p_2-1)\cdots(p_l-1).$$ Using this formula, or otherwise, can we say anything about the cases when $m \neq n$ but $\varphi(m) = \varphi(n)$ (for instance, $\varphi(6) = \varphi(3) = 2$ )? Can we characterise all the pairs of integers for which this will happen? An easy case is if $q$ is an odd prime, then $\varphi(q) = \varphi(2q)$ . Therefore, if we have two odd integers $m$ and $n$ , both of which are not divisible by $q$ , and $\varphi(m) = \varphi(n)$ , then $\varphi(qm) = \varphi(qn) = \varphi(2qm) = \varphi(2qn)$ since the totient function is multiplicative.","['number-theory', 'prime-factorization', 'elementary-number-theory', 'reference-request']"
3424119,Show that the minimum of $f$ does not occur at an endpoint.,"Let $f$ be a differentiable function on $[a,b]$ but the derivative may be discontinuous. Suppose that $f'(a)<0<f'(b).$ Show that the minimum of $f$ does not occur at an endpoint. What can you conclude? Here's my work so far Since $f$ is differentiable on $[a,b],$ it is continuous on $[a,b].$ We want to show that neither $f(a)$ nor $f(b)$ is a minimum. Since $f$ is continuous and $f'(a)<0<f'(b),$ there exists $\delta>0$ such that $\exists x\in [a,a+\delta)$ such that $f(x)<f(a)$ . By the definition of a derivative, since $f'(a)=\lim\limits_{h\to 0}\dfrac{f(a+h)-f(a)}{h}.$ We thus have that $\forall \epsilon >0,\exists \delta >0$ such that $0<h<\delta\Rightarrow \left|\dfrac{f(a+h)-f(a)}{h}-f'(a)\right|<\epsilon$ (I'm pretty sure this is correct). So we have that $f(a+h)<f(a)+h(f'(a)+\epsilon).$ In particular, this is true if $\epsilon +f'(a)<0\Rightarrow f(a+h)<f(a).$ Thus there exists an $x\in [a,a+\delta)$ such that $f(x)<f(a).$ Similarly, $f'(b)=\lim\limits_{h\to 0}\dfrac{f(b+h)-f(b)}{h}$ and so by the $\epsilon-\delta$ limit definition, $\forall \epsilon >0,\exists \delta >0$ such that $-\delta < h <0\Rightarrow\left|\dfrac{f(b+h)-f(b)}{h}-f'(b)\right|<\epsilon\Rightarrow f(b+h)>f(b)+h(f'(b)-\epsilon).$ In particular, this holds for $\epsilon \leq f'(b)\Rightarrow b+h < b$ and $f(b+h)<f(b).$ Thus $f(x)$ cannot have a local minimum at $b.$ Thus, I can conclude by the Extreme Value Theorem that the minimum is attained in $(a,b).$","['continuity', 'calculus', 'derivatives']"
3424120,Inverse image of compact set under open mapping contains compact,"Let $\mathbb{D}$ denote the unit disc. Say $U\subset \mathbb{D}$ is an open set such that for every $r\in [0,1)$ there exists $z\in U$ such that $|z| = r$ . Given $0<\rho<1$ is it then possible to find a compact subset of $U$ denoted $K$ such that $\forall r\in[0,\rho]$ there exists $z\in K$ such that $|z| = r$ ? If $U$ is connected then this is trivially true since $U$ is pathwise connected and we need only pick a point $\zeta\in U$ with $|\zeta| = \rho$ and then connect it via a curve to $0$ and consider the trace of the curve as our compact set. A similar construction is valid if $U$ consists of finitely many components. However if $U$ consists of infinitely many components is it still possible to find such a compact set?",['general-topology']
3424198,Points of continuity is a Borel set?,Let $f:X \to \mathbb R$ be a function where $X$ is a metric space. Is the set of points at which $f$ is continuous a Borel set? i.e. Is the set $\{ x \in X : f$ is continuous at $x \in X$ $\}$ a Borel set in $X$ ? (Maybe separability of $X$ is needed?),"['measure-theory', 'real-analysis']"
3424234,A graph with radius three and diameter four.,"I was trying to construct the graphs $G$ and $H$ by using the cycle graph $C_9$ (or $C_n$ ) and $H$ and $G$ are induced in $C_n$ . Graph $G$ is formed by attaching a vertex $x$ to $C_9$ (or $C_n\geq 7$ ) and by making it adjacent to vertex $1$ and all the vertices $j$ , where $3\leq j\leq 8$ (or $3\leq j\leq n-1$ ). Here, in $G$ , vertices $1$ and $x$ have eccentricity two and rest of the vertices have eccentricity three, shown in the following figure. Similarly, I want to draw a graph $H$ , where exactly two vertices have eccentricity three and rest of the vertices have eccentricity four. The graph $H$ must be formed by appending exactly one vertex to $C_n$ . Kindly help.
Any hint will be of great help. P.S. The construction for graph $G$ is for all cycles for $n\geq9$ .","['graph-theory', 'combinatorics', 'discrete-mathematics']"
3424427,Between every two Riemann-integrable functions there is at least one Riemann-nonintegrable function,"I ""created"" this problem approximately 10-14 hours ago. I think that this problem could be used as some undergraduate exercise in the chapter on Riemann integrability. The meaning of between is the following: Suppose that $f$ and $g$ are two Riemann-integrable functions defined on the closed interval $I$ and that $f<g$ . $f<g$ means that for every $x \in I$ it is true that $f(x)<g(x)$ . The function $h$ is between $f$ and $g$ if and only if $f<h<g$ . I think that different approaches exist to solve this exercise, and, which one would be yours? One intuitive idea is to choose for every $f$ and $g$ some $h$ which is very wildly discontinuous in such a way for Riemann integral of $h$ to not exist. But, I didn´t turn this idea into a concrete proof yet. How to prove this?","['functions', 'riemann-integration', 'real-analysis']"
3424567,Finding the quotient of this free abelian group,"I have the group $\langle a,b,c\rangle/\langle -b+c-a,b+c-a\rangle$ . I know this is $\mathbb{Z}\oplus\mathbb{Z_2}$ . However, I tried doing it like this and got something else : I have $$-b+c-a=0, b+c-a=0$$ Which gives me $2c=2a$ and $2b=0$ .
Then my group is the same as $$\langle a-c,b,c\rangle/\langle 2(a-c),2b\rangle$$ which is again $$\langle d,b,c\rangle/\langle2d,2b\rangle\simeq\mathbb{Z}\oplus\mathbb{Z_2}\oplus\mathbb{Z_2}$$ Which step was wrong?","['free-abelian-group', 'quotient-group', 'abstract-algebra', 'free-groups', 'group-theory']"
3424584,Find all analytic functions $f: \mathbb{C} \longrightarrow \mathbb{C}$ such that f(3z)-f(2z)=5f(z),as the title states: Find all analytic functions $f: \mathbb{C} \longrightarrow \mathbb{C}$ such that $f(3z)-f(2z)=5f(z)$ where $z \in \mathbb{C}$ and $f(1)=3$ Starting with the taylor series of expansion of $f(z)$ we have $$f(z) = \sum_{n=0}^{\infty}a_n z^n$$ then substituting the above we have $$f(3z)-f(2z)=5f(z) \implies \sum_{n=0}^{\infty}a_n3^nz^n - \sum_{n=0}^{\infty}a_n2^nz^n = 5\sum_{n=0}^{\infty}a_nz^n$$ then via the uniqueness of power series coeffecients we have $$a_n3^n - a_n2^n = 5a_n$$ which has a single solution at $n=2$ then $a_n = 0~\forall n \in \mathbb{N}\setminus \{2\}$ otherwise. then we have $$f(z)=\sum_{n=0}^{\infty}a_nz^n = a_2z^2$$ from here i'm tempted to use the formula for $a_2$ ie $$a_n = \frac{1}{2 \pi i}\int_{S_{r}^{+}(0)}\frac{f(w)}{w}dw $$ but since it's been given to me that $f(1)=3$ i'm tempted to just state that $a_2 = 3$ . as i can't really think of any other way of determining f. Thoughts? is this correct?,"['complex-analysis', 'complex-numbers', 'analytic-functions']"
3424649,Extending a differential form from a subset of $S^2$ to $S^2$ and then integrating it,"Let $X = \{(x,y,z) \in S^2 \ \mid \ z \neq 0 \} \subset S^2$ and let $$\omega = \frac{1}{z} dx \wedge dy $$ be a differential $2$ -form on $X$ . $(i)$ How can we extend $\omega$ from $X$ to a differential $2-$ form $\theta$ on $S^2$ such that $\theta |_{X} = \omega$ ? $(ii)$ What is then $i_V(\theta)$ (the interior product of $\theta$ into $V$ ), with $$V = y \cdot \frac{\partial}{ \partial x} - x \cdot \frac{\partial}{\partial y}.$$ $(iii)$ How can we compute $\displaystyle \int_{S^2} (x+y)\cdot \theta$ ? I am not really sure how to solve the above questions. This post - Differential Forms on submanifolds - has an answer on how to extend $\omega$ to $\theta$ , but it uses a decomposition of and tangent bundle. Is there a way to do $(i)$ using partitions of unity? The problem with partitions of unity, however, is that we have no control over them. Wouldn't using partitions of unity make $(ii)$ and $(iii)$ very difficult to solve (assuming that we can write $\theta = \displaystyle \sum_{i=1}^n (f_i \cdot \omega)$ , where $\{f_i \}_{i=1}^n$ is the smooth parittion of unity)? A nicer way would be to multiply $\omega$ with a bump function that is $1$ on $X$ and $0$ on $S^2 \setminus X$ (although such function does not exist, but maybe we can use some variation of this).","['integration', 'differential-forms', 'smooth-manifolds', 'differential-geometry']"
3424657,Verification of a process is Markovian or not?,"Let $\left(X_t\right)_{t \in \mathbb{N}}$ is a $(\lambda,P)$ markov chain on $\mathbb{Z}$ , where $\lambda$ is the initial propability and $P$ is the transition matrix. Now define the following processes as $~(1)~Q_t:=\left(X_t\right)^3~,~(2) ~Y_t :=\left(X_t\right)^2~,~(3)~Z_t:=e^{X_t}~,~ (4)~W_t:=\left| X_t\right|$ Are these valid markov chains ? what is the initial probability and the transition matrix ? For the first case i tried like this : $$\begin{align}\mathbb{P}&(Q_{t+1}=j|Q_t=i,Q_{t-1}=i_{t-1},\ldots,Q_0=i_0)\\&=\mathbb{P}(X^3_{t+1}=j|X^3_t=i,X^3_{t-1}=i_{t-1},\ldots,X^3_0=i_0)\\&=\mathbb{P}(X_{t+1}=j^{1/3}|X_t=i^{1/3})\end{align}$$ So this is a markov chain and the initial probability will be $\mathbb{P}(Q_0=i)=\mathbb{P}(X_0=i^{1/3})=\lambda^{1/3}$ and the transition matrix would be then $p_{i^{1/3}j^{1/3}}$ . Is it correct ?
And i'm confused about the other ones $Y_t;=(X_t)^2$ and $W_t;=|X_t|$ is  problematic i think as these maps are not injective...how to prove or disprove this formally and find out $(\lambda,P)$ ?","['stochastic-processes', 'statistics', 'markov-chains', 'probability']"
3424676,"Double integral $\int_0^1 \int_{\sqrt{x}}^1 \sin(\pi y^3)\,dy\,dx$","Calculate the integral: $$\int_0^1(\int_{\sqrt{x}}^1\sin(\pi y^3)dy)dx$$ I have come up with this solution: If $\sqrt{x} \leq y \leq 1$ then $x \leq y^2 \leq 1$ and since $0 \leq x \leq 1$ we have that $0 \leq y \leq 1$ and $0 \leq x \leq y^2$ . If we insert this into the integral we get: $$\int_0^{y^2}(\int_{0}^1\sin(\pi y^3)dy)dx = \int_{0}^1(\int_0^{y^2}sin(\pi y^3)dx)dy$$ This I can easily calculate using variable substitution ( $u = y^3; du = 2y^2dx $ giving me the result $\frac{2}{3\pi}$ , I dont have the key). However I'm not sure what I'm doing with my integration limits is ""legal"", so to say. There is also a follow up question: ""Is the following true or false? Motivate your answer (without long calculations)."" $$\int_0^1(\int_{\sqrt{x}}^1\sin(\pi y^4)dy)dx = 0$$ Just looking at it my answer would be false, but I have no good reason for that. I feel like there is some theory I'm missing here. Thanks in advance and sorry for my poor english.","['integration', 'definite-integrals', 'calculus', 'multiple-integral', 'trigonometry']"
3424708,"Calculating the value of various functions where one is dependent upon others - ZIO $2010$, P$3$","Hello everybody! The problem you see is an algorithmic problem I could not solve. :( This is ZIO $2010$ , P $3$ . The first thing that I thought when I saw the problem was that - ""Umm... this sounds easy, just go step by step"" and I did try to go step by step on the first part.. BUT this problem is too long and the fact that there are a lot of functions this takes too long. Of course, if the problem could be solved manually one by one, why would this be on a high school competition? I somehow managed to calculate a few values and tried to find a pattern but no use. The real difficulty of this problem in my opinion is the fact that all functions depend on each other somehow (leave the function $f$ ) which should probably be cleverly used to find the solution, I guess. Moreover, $g(n)$ and $h(n)$ give values $\pmod{10}$ . Would some sort of a recursive algorithm work? (Note that in this competition, you are given $30$ minutes to solve one problem or $10$ minutes to solve a sub-case. That's pretty low.) The answers are $3,0,2$ . Any help would be appreciated. Thanks!","['elementary-number-theory', 'recursion', 'functions', 'combinatorics', 'algorithms']"
