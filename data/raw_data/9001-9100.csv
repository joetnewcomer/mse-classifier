question_id,title,body,tags
68364,Integral of the derivative of a function of bounded variation,"Let $f\colon [a,b] \to \mathbb R$ be of bounded variation. Must it be the case that $|\int_a ^b f' (x) |\leq |TV(f)|$, where $TV(f)$ is the total variation of $f$ over $[a,b]$? If so, how can one prove this? In the standard proof of the monotone differentiation theorem, it is shown tat this holds for increasing functions: if $f$ is increasing, then $\int_a ^b f'(x) \leq f(b) - f(a) = TV(f)$. I am trying to generalize this to functions of bounded variation.","['bounded-variation', 'measure-theory', 'calculus', 'functional-analysis']"
68370,The complement of a Cartesian product  for characterizing the closed sets of a product space,"I have this basic question, I want to write the closed sets of a product space (under the product topology) in a general way. This problem is more related to set theory than to topology, so if someone does't knows topology but can ""evalf"" this expression, also can help me. Any open set in this topology is a union of product of open sets, i.e., 
$$
J = \bigcup _{\alpha  \in A} \prod_{\beta \in B} X_{\alpha, \beta} 
$$
and every closed set must be of the form 
$$
J^c  = \bigcap_{\alpha \in A} \left( \prod_{\beta \in B} X_{\alpha,\beta}  \right)^c 
$$
so I want to rewrite this part $$
\left( \prod_{\beta \in B} X_{\alpha, \beta} \right)^c .
$$ It's easy to see that 
$$
(A\times B)^c  = (A^c \times B^c) \cup ( A^c\times B) \cup (A\times B^c),
$$
but for arbitrary products, can I do something? Or at least, can I write the closed sets of the product in a general way, just like can I with open sets?","['general-topology', 'elementary-set-theory']"
68371,Orientability implies separation of space?,"If a hypersurface in a manifold separates the ambient space into two disconnected pieces, is the surface necessarily orientable? This seems to be true when one considers the Jordan Brouwer theorem which implies the sphere $S^n$ embedded in $\mathbb{R}^n$ separates space into two disconnected components. But does the requirement of orientability extend to hypersurfaces in any manifold? A counter-example would show a non-orientable hypersurface separating the ambient space into two disconnected regions. (edit: statement on Jordan Brouwer theorem refined per George Lowther's comment)","['general-topology', 'algebraic-topology']"
68377,summation identity - statistics,"What does this summation simplify to? $$ \sum_{x=0}^{y} \frac{1}{x!(y-x)!} $$ I tried applying common formulas (Maclaurin series, binomial coefficients, etc. but nothing seems to match up to it). Any tips would be appreciated. Thanks.",['statistics']
68384,summation of x * (y choose x) binomial coefficients,"What does this summation simplify to? $$ \sum_{x=0}^{y} \frac{x}{x!(y-x)!} $$ I was able to realize that it is equivalent to the summation of $x\dbinom{y}{x}$ if you divide and multiply by $y!$, but I am unsure of how to further simplify. Thanks for the help!","['statistics', 'binomial-coefficients']"
68395,Determine if circle is covered by some set of other circles,"Suppose you have an existing set of circles $\mathcal{C} = {C_1, .., C_n}$ each with a fixed radius $r$ but varying centre coordinates. Next, you are given a new circle $C_{n+1}$ with the same radius $r$ as the previous circles but with a new centre coordinate. How can you determine whether the area covered by the $C_{n+1}$ is fully covered by $\mathcal{C}$? How to do this if the circles can have varying radii? Note : I couldn't yet work out a nice mathematical solution for this. Coming from computer science, the best I could come up with is solving this in a brute force way using some sort of Monte Carlo sampling, i.e., draw a large number of random points from the area of $C_{n+1}$ and then checking for each point if it is enclosed by at least one circle in $\mathcal{C_{intersecting}}$ (subset of $\mathcal{C}$ with circles that are within $2r$ of $C_{n+1}$).","['geometry', 'computational-geometry', 'circles']"
68401,Sequence of measurable functions such that $a_nf_n \rightarrow 0$,"Let $\{f_n\}_{n=1}^{\infty}$ be a sequence of real valued measurable functions on $[0,1]$.  Show that there is a sequence of positive real numbers $\{a_n\}_{n=1}^{\infty}$ such that $a_nf_n \rightarrow 0$ a.e. on $[0,1]$. Here is my idea.  Let $E=[0,1]$.  Let $F_n \subseteq [0,1]$ and closed such that $m(E\setminus F_n)<\epsilon$.  Also, let $g_n$ be a continuous function on $F_n$, and thus bounded since $F$ is closed. Each $g_n$ is bounded by a corresponding $M_n$. My goal was to use Lusin's theorem, but I'm starting to think that my idea is a dead end.",['measure-theory']
68409,system of ode with non-constant coefficient matrix,I am sorry but I haven't learn any method to solve this kind of problem if the given matrix is non-constant. $$\begin{pmatrix}x\\y\end{pmatrix}^\prime=\begin{pmatrix} 1&-\cos t \\ \cos t & 1\end{pmatrix}\begin{pmatrix}x\\y\end{pmatrix}$$ Thanks,['ordinary-differential-equations']
68411,Showing that $\sum_{n \leq x} \frac{\mu(n)}{n} \log \frac{x}{n} = O(1)$.,"So I am trying to show the following:
$$\sum_{n \leq x} \frac{\mu(n)}{n} \log{\frac{x}{n}}=O(1) $$ so I tried partial summation as following: Let $A(x)=\sum_{n \leq x} \frac{\mu(n)}{n}$, then we have $$\sum_{n \leq x} \frac{\mu(n)}{n} \log{\frac{x}{n}}= \int_1^{x} \frac{A(t)\mathrm dt}{t},$$ and $A(t)$ is clearly very small, or $o(1)$ for all $t \in [1,x]$. My question is how to go from here to conclude that the error term is $O(1)$?","['analytic-number-theory', 'number-theory']"
68422,fractional linear transformations,"From my research, I have figured out that this is a Möbius transformation. The respective wiki page helped me understand a bit more, however I can't figure out how to obtain the image. So lets talk about what I do know. Well we are describing the set of all values $f(z)$ where $|z| < 1$. I also know that any three points. I also know that Given a set of three distinct points z1, z2, z3 on the Riemann sphere and a second set of distinct points w1, w2, w3, there exists precisely one Möbius transformation f(z) which maps the zs to the ws and from help from another forum I got the following: To decide which part is the image of the interior |z| < 1 of the disc,
  figure out which point f sends to infinity.",['complex-analysis']
68432,Circle group addition of fractional parts is well-defined,"Let $G=\{x \in \mathbb{R}\mid 0 \leq x < 1 \}$ and for $x,y \in G$ let $x\star y$ be the fractional part of $x+y$ (i.e $x\star y=x+y-\lfloor x+y \rfloor$ where $\lfloor a \rfloor $ is the greatest integer less than or equal to a). Then, how do I prove that $\star$ is a well defined binary operation on $G$ and that $G$ is an abelian group under $\star$? Thank you.I have just started group theory.My progress on this is minimal and next to 0.
Edited .","['quotient-group', 'group-theory']"
68439,Finding Floquet Multipliers,"So I'm curious how to actually find Floquet multipliers given some differential equation $y'=A(t)y$. It's (usually) easy to find the fundamental matrix $\Phi(t)$ by other methods. And I feel like I'm missing something dumb, but what can I do from there to get the eigenvalues for the matrix $C$? For a more concrete example at hand, consider $y'=(a+b\cos t)y$. It is easy to see that $y=(C_0e^{at})e^{b\sin t}$ and the period of the periodic matrix is $p=2\pi$. So now I have:
$$\Phi(t+p)C=\Phi(t)\implies(C_0e^{A(t+2\pi)})e^{B\sin{(t+2\pi)}}C=(C_0e^{At})e^{B\sin{t}}$$
How do I get $C$, or more specifically, the eigenvalues of C once I have this setup? I'm pretty sure I did something incorrectly. EDIT: Sorry, figured it out with anon's help. I completely mixed up my notions for computing the fundamental matrix.",['ordinary-differential-equations']
68446,Equivalence of Cauchy's and Sylow's theorem,"There were two question of group theory posted recently to prove something without Sylow theorem (see 1 , and 2 ) . Both questions have some answer which use Cauchy's theorem . But it seems that any one of these theorem can be proved using another. In the proof of Sylow theorem given by J. B. Fraleigh (Abstract Algebra), he uses Cauchy's theorem as first step, and after existence of Sylow subgroups (or $p$-subgroups), other parts of Sylow theorem (their conjugacy, their cardinality etc.)  can be proved easily. While Cauchy's theorem can be considered as a special case of Sylow theorem. Question: Can we say that both these theorems are equivalent? Sylow theorem: 1) If $G$ is a finite group and $p^n$ divides $|G|$ but $p^{n+1}$ does not, then $G$ has subgroups of order $p^i$ for ($1\leq i\leq n$), 2) any two subgroups of order $p^n$ are conjugate, 3) number of subgroups of order $p^n$ is $1$(mod $ p$).",['group-theory']
68447,A Inequality between Bhattacharyya distance and KL divergence,"I was trying to solve the following problem. But I dont know how to proceed. I would be really grateful if anybody would point me in the right direction. Let $P = (p_1,p_2, \cdots, p_n)$ be a probability vector (That is $\sum p_i = 1$ and $p_i  \geq 0$ ). Let $Q = (q_1,q_2, \cdots, q_n)$ be a permutation of the vector $P$ . If $I = \frac14 \left(\sum p_i \log \frac{p_i}{q_i}\right) + \frac14 \left(\sum q_i \log \frac{q_i}{p_i}\right)$ (involves the Kullback - Leibler divergence ) And $Z = \sum \sqrt{p_iq_i}$ (called the Bhattacharyya distance ) Prove that $I^2 + Z^2 \leq 1$ Thank you","['inequality', 'probability']"
68454,Quadratic iterative system,"A general linear iteative system can be represented as a matrix: $$(x,y)\mapsto(ax+by,cx+dy)$$ is essentially the same as $$\left[\begin{array}{cc}
a&b\\
c&d\\
\end{array}\right]
\left[\begin{array}{c}
x\\
y\\
\end{array}
\right]$$ which is useful because it can be iterated quickly (matrix exponentiation) and enables various matrix techniques for determining asymptotic behavior and the like. (Of course the number of variables can be increased as needed.) Is there a similar tool for quadratic iterative systems like $$(x,y)\mapsto(ax^2+bxy+cy^2,dx^2+exy+fy^2)$$
? I'm interested in computing the $n$th iterate ($n$ not too small), finding asymptotic behavior, and any other interesting things that can be determined for a given collection of constants $a,b,\ldots$. My immediate interest (genetics, oddly enough) does not use any of the diagonal terms $x^2,y^2$ so a treatment that ignores them would be fine (though I suspect including is more natural).","['dynamical-systems', 'real-analysis']"
68457,"Number of connected graphs on labeled vertices, counted according to parity","While trying to derive some formula, I encountered the following problem. Consider the set $C_n$ of all connected graphs on $n$ vertices. What is
$$ \sum_{G \in C_n} (-1)^{|G|} ? $$
In other words, if we give a graph with an even number of edges a weight of $+1$, and a graph with an odd number of edges a weight of $-1$, what is the total weight of all connected graphs? Here is an example: when $n=3$, any two edges form a connected graph, and the complete graph is also connected, so the value of the sum is $3 - 1 = 2$. Surprisingly, the answer is
$$ \sum_{G \in C_n} (-1)^{|G|} = (-1)^{n-1} (n-1)!. $$
This can be proved using the ""exponential formula"" (see e.g. the freely-available generatingfunctionology ). The exponential generating function of all graphs, counted according to their parity, is $1+x$ (since for $n \geq 2$, the even and odd graphs cancel). Hence the exponential generating function for all connected graphs (counted according to their parity) is
$$ \log (1+x) = \sum_{n \geq 1} (-1)^{n+1}\frac{x^n}{n} = \sum_{n \geq 1} (-1)^{n+1}(n-1)!\frac{x^n}{n!}. $$ My question is: Is there an enumerative proof of the formula $$ \sum_{G \in C_n} (-1)^{|G|} = (-1)^{n-1} (n-1)! \quad ? $$","['graph-theory', 'generating-functions', 'combinatorics']"
68468,example of a non-continuous function that is open and not closed,"Can you help me find an example of a function from a subset of $\mathbb{R}^2$ to a subset of $\mathbb{R}^2$ that is not continuous nor closed, but open? and another one that is not continuous but both open and closed? I could only find one that is not continuous nor open but closed.
Thank you.","['general-topology', 'multivariable-calculus', 'analysis']"
68484,uniqueness of a direct limit,"DEFINITIONS: $(I,\leq)$ is a preordered set when $I$ is a set and $\leq$ is a reflexive and transitive binary relation on $I$ , i.e. $\forall i\!\in\!I\!: i\!\leq\!i$ and $\forall i,j,k\!\in\!I\!: i\!\leq\!j,j\!\leq\!k\Rightarrow i\!\leq\!k$ . An directed set is a preordered set $I$ such that $\forall i,\!j\!\in\!I\,\exists k\!\in\!I\!:\,i,\!j\!\leq\!k$ . Let $I$ be a directed preordered set and $\underline{C}$ a category. A direct system in $\underline{C}$ is a pair $((A_i)_{i\in I},(\alpha_{i,j})_{i,j\in I,i\leq j})$ , denoted shorter with $(A_i,\alpha_{i,j})_{i\leq j\in I}$ , that consists of a family $A_i$ of objects of $\underline{C}$ and a family $\alpha_{i,j}$ of morphisms of $\underline{C}$ , such that: $\alpha_{i,j}\!:A_i\!\rightarrow\!A_j$ for $i\!\leq\!j$ ; and $\alpha_{i,i}$ is the identity morphism on $A_i$ ; and $\alpha_{j,k}\!\circ\!\alpha_{i,j}\!=\!\alpha_{i,k}$ for $i\!\leq\!j\!\leq\!k\!\in\!I$ . In praxis, the most usual cases are $\underline{C}$ $=$ SET, GRP, RING, $R$ -MOD, $R$ -ALG, i.e. the category of sets, groups, rings, $R$ -modules, $R$ -algebras. An object $A$ of $\underline{C}$ , together with morphisms $\alpha_i\!:A_i\!\rightarrow\!A$ of $\underline{C}$ , is a direct limit (or inductive limit , or directed colimit ) of the direct system $(A_i,\alpha_{i,j})_{i\leq j\in I}$ in $\underline{C}$ , denoted $(A,\alpha_i)_{i\in I}\!=\!\varinjlim(A_i,\alpha_{i,j})_{i\leq j\in I}$ or just $A\!=\!\varinjlim(A_i,\alpha_{i,j})_{i\leq j\in I}$ or $A\!=\!\varinjlim(A_i)_{i\in I}$ , if $\alpha_j\!\circ\!\alpha_{i,j}\!=\!\alpha_i$ for $i\!\leq\!j\!\in\!I$ ; and the universal property is satisfied: for any other object $A'$ of $\underline{C}$ and morphisms $\alpha'_i\!:A_i\!\rightarrow\!A'$ of $\underline{C}$ satisfying $\alpha'_j\!\circ\!\alpha_{i,j}\!=\!\alpha'_i$ , there exists a unique morphism $\alpha\!:A\!\rightarrow\!A'$ of $\underline{C}$ such that $\alpha\!\circ\!\alpha_i\!=\!\alpha'_i$ for all $i\!\in\!I$ . QUESTION: how can I prove the statement: ""In any category, if a direct limit of a direct system exists, then it is unique up to isomorphism."" My attempt: If $(A,\alpha_i)$ and $(A',\alpha'_i)$ are both direct limits of $(A_i,\alpha_{i,j})_{i\leq j\in I}$ , then $\exists!\alpha\!:A\!\rightarrow\!A'$ such that $\alpha\!\circ\!\alpha_i\!=\!\alpha'_i$ , and $\exists!\alpha'\!:A'\!\rightarrow\!A$ such that $\alpha'\!\circ\!\alpha'_i\!=\!\alpha_i$ . We wish to show $\alpha'\circ\alpha=1_A$ and $\alpha\circ\alpha'=1_{A'}.$ We have $\alpha'\circ\alpha\circ\alpha_i=\alpha'\circ\alpha'_i=\alpha_i$ and $\alpha\circ\alpha'\circ\alpha'_i=\alpha\circ\alpha_i=\alpha'_i$ . Also $\alpha\circ\alpha'\circ\alpha=\alpha$ and $\alpha'\circ\alpha\circ\alpha'=\alpha'$ by the uniqueness of the universal property. This is where I run out of ideas.","['homological-algebra', 'category-theory', 'abstract-algebra']"
68495,difference between join and union,"Is there a difference between the join operator, $\wedge$, and the union of a set? In particular, what is the join of $a \wedge b $ and $b \wedge c$? Is it $a\wedge b \wedge c$ or is it $0$? I seem to have read both answers (in physics textbooks where they have skimmed over the details of how they define their operators). The answer $0$ comes from a geometric algebra book studying projective geometry, where they identify the geometric exterior product (= Grassmann's exterior product) with the join operator.  Since the exterior product is anti-commutative and associative it follows that for vectors $a$, $b$, $c$, $a\wedge a=0 \implies (a\wedge b)\wedge(b\wedge c) = 0$. They went on to define the meet in terms of the exterior product $(a\vee b)^* = a^*\wedge b^*$ where the star denotes the dual of the (in this case) vectors $a$ and $b$.
(see for example Universal Geometric Algebra by David Hestenes) The set union answer comes from a discussion of lattices and probabilities (different book) where they identify join and meet with set union and intersection.  So for example, (since my terminology might be wrong), they drew a lattice such as follows, {a,b}
   /    \
 {a}    {b}
   \    /
     {} So in this case the join is $a \cup b$. Does join/meet have a strict definition that is distinct from union/intersection - or can you define it however you like given the circumstances?  If its the latter, which is the more usual definition?","['lattice-orders', 'elementary-set-theory']"
68496,"Permutation on a set from $\{1, 2, \ldots, 2n\}$","Fix a positive integer $n$ . A permutation $(x_1,x_2,\ldots,x_{2n})$ of the set $\{1,2,\ldots,2n\}$ satisfies the property $A$ iff for at least one $i$ in $\{1,2,\ldots,2n-1\}$ , $|x_i - x_{i+1}|=n$ . I.e. If the difference between adjacent numbers in a permutation is equal to $n$ , then this permutation has the property in the $i^\text{th}$ position. Example: For $n = 3$ , $(1,5,2,3,4,6)$ has the property (because $x_2=5$ and $x_3 = 2$ are next to each other and $|x_2 - x_3| = |5 - 2| = 3$ ); similarly $(6,3,1,2,4,5)$ has the property. However, $(1,2,3,4,5,6)$ does not have the property. I need to prove the following statement: There are at least as many permutations with the property than permutations without it. Can someone please point me towards how to go about this? I tried this for small cases and can see how it is true. I had the idea to first define an algorithm which generates all the possible permutations. Then show that the transposition from one permutation to another produces elements which are adjacent to each other and the difference between them is $n$ . The question suggests to show a bijection between the permutations that do not have this property to the permutations that do have this property, for exactly one $i\geq 2$ .","['permutations', 'contest-math', 'combinatorics']"
68505,Is there a rule of integration that corresponds to the quotient rule?,"When teaching the integration method of u-substitution, I like to emphasize its connection with the chain rule of integration. Likewise, the intimate connection between the product rule of derivatives and the method of integration by parts comes up in discussion. Is there an analogous rule of integration for the quotient rule? Of course, if you spot an integral of the form $\int \left (\frac{f(x)}{g(x)} \right )' = \int \frac{g(x) \cdot f(x)' - f(x) \cdot g(x)'}{\left [ g(x)\right ]^2 }$, then the antiderivative is obvious. But is there another form/manipulation/""trick""?","['calculus', 'integration', 'indefinite-integrals']"
68508,Question about normed vector spaces and quotients,"If you have a norm defined on a vector space $V$ you can define the norm for the quotient by a subspace $W$: $$ || v + W|| := \inf_{w \in W} || v + w || $$ My question is: why does $W$ have to be a closed subspace? In $\mathbb{R}^n$, as it happens, any subspace is closed, i.e. if something is a subspace then it's closed (e.g. lines or planes in $R^3$). Is this true for any vector space? I find it difficult to visualise subspaces of e.g. $L^1$. Many thanks for your help!! (as always ; ) )","['linear-algebra', 'functional-analysis']"
68522,Ordering of a deck of cards,"If you shuffle n cards as follows: Go through the deck one card at a time and at each card, flip a fair coin.  If the coin comes up head, then leave the card where it is, and if it comes up tails move that card to the end of the deck. For example, if n =4,  and the initial ordering is 1,2,3,4, and outcome of the successive flips is h,t,t,h, then the ordering at the end of the round is 1,4,2,3. Assuming all possible outcomes of the coin flips are equally likely, what is the probability that the ordering after one round is the same as the initial ordering? Attempt: If the ordering after n flips is the same as the initial ordering, then you would have to roll all heads since rolling heads doesn't change the ordering. So the answer would be (1/2)^n?",['probability']
68523,Overlap volume of three spheres,"Given three spheres and their coordinates with equal radii that are known to have a triple overlap (a volume contained within all three spheres), is there a known closed form for the calculation of this volume? Relevant: The two sphere case on Mathworld. Update : There seems to be a paper that discuss this exact question in general for $n$ three-dimensional spheres. Unfortunately it is behind a paywall that I don't have access to. I'm putting the link here for future reference, please correct me in the comments if the article does not answer the question: ANALYTICAL TREATMENT OF THE VOLUME AND SURFACE-AREA OF MOLECULES FORMED BY AN ARBITRARY COLLECTION OF UNEQUAL SPHERES INTERSECTED BY PLANES Molecular physics [0026-8976] DODD yr:1991 vol:72 iss:6 pg:1313-1345 Lawrence R. Dodda & Doros N. Theodoroua","['geometry', '3d', 'solid-geometry']"
68533,P(X + Y < a | Y < b) where X and Y are independent continuous random variables,"I’m looking for an online source/article/lecture notes/ text book that would contain a detailed/rigorous discussion/explanation/proof of the following result, which was used in Conditional normal distribution $P(X+Y<a,\,Y<b)=\int\limits_{-\infty}^bg_Y(y)\int\limits_{-\infty}^{a-y}g_X(x)\mathrm dx\,\mathrm dy$ Many thanks","['probability-theory', 'probability-distributions', 'probability']"
68538,Is there a division algorithm for any Euclidean Domain?,"Any Euclidean domain satisfies the division ""algorithm"": For any $x,d$ there exists $q,r$ such that $x = qd+r$ with $\sigma(r)<\sigma(d)$ or $\sigma(r)=0$ With $\sigma$ a ""size function."" I'm wondering if what I would call an algorithm (i.e. a discrete series of steps to get to an answer) exists for division. Specifically: Suppose +, -, and * are defined in some Euclidean Domain. Is there a mechanism to find $x/y$ (beyond brute force)? Repeated subtraction works in the integers, but not the polynomials, and this was my first thought. (I realize that I'm ignoring the problem of how to do subtraction in the ring, which is quite similar.)",['abstract-algebra']
68541,Probability question about married couples,"If four married couples are arranged in a row, what is the probability 
  that no husband sits next to his wife? Would it be $1- \frac{2(4!)}{8!}$?","['probability', 'combinatorics']"
68553,Distribution of maximum of partial sums of independent random variables,"I have been looking all over the net to find a way to work out a probability distribution of a maximum of partial sums of independent random variables, but to no avail. So I have decided to try to work it out for myself. Here are the results of this endeavour and I would appreciate if people with better understanding of probability than me would give it a look over to see if I’ve made a mess of it somewhere. Many thanks.  Here it goes. Given a set $\{X_i:i=0,1,\ldots,n \}$ of independent random variables with $X_0 = 0$ and with given p.d.f.'s $f_{X_i}(x)$ and corresponding c.d.f's $F_{X_i}(x)$, define $S_k=\Sigma_{i=0}^k\,X_i$, and $M_k=\max\{S_0,S_1,\ldots,S_k \}$, and we want to find a distrinution of $M_n$. We have $$
\begin{eqnarray*}
P(M_n<m)&=&P(\max\{M_{n-1},S_n\}<m)\\
&=&P(M_{n-1}<m,S_n<m)\\
&=&P(S_n<m|M_{n-1}<m)P(M_{n-1}<m),\\
\end{eqnarray*}
$$ where $$
\begin{eqnarray*}
P(S_n<m|M_{n-1}<m)
&=&P(S_n<m|\max\{M_{n-2},S_{n-1}\}<m)\\
&=&P(S_n<m|S_{n-1}<m)\\
&=&P(S_n<m,S_{n-1}<m)/P(S_{n-1}<m),
\end{eqnarray*}
$$ where $$
\begin{eqnarray*}
P(S_n<m,S_{n-1}<m)
&=&P(S_{n-1}+X_n<m,S_{n-1}<m)\\
&=&\int\limits_{-\infty}^m f_{S_{n-1}}(s)\int\limits_{-\infty}^{m-s}f_{X_n}(x)\mathrm dx\,\mathrm ds\\
&=&\int\limits_{-\infty}^m f_{S_{n-1}}(s)F_{X_n}(m-s)\mathrm ds,
\end{eqnarray*}
$$ where $$
\begin{eqnarray*}
f_{S_k}(s)&=&\int\limits_{-\infty}^{\infty} f_{X_k}(x)\,f_{S_{k-1}}(s-x)\mathrm dx,
\end{eqnarray*}
$$ with $$
\begin{eqnarray*}
f_{S_0}(s)=\delta (s),
\end{eqnarray*}
$$ so that putting it all together gives a recursion formula $$
\begin{eqnarray*}
F_{M_n}(m) = \frac{F_{M_{n-1}}(m)}{F_{S_{n-1}}(m)} \int\limits_{-\infty}^m f_{S_{n-1}}(s)F_{X_n}(m-s)\mathrm ds
\end{eqnarray*}
$$ with $$
\begin{eqnarray*}
F_{M_0}(m) = H(m)
\end{eqnarray*}
$$ the Heaviside step function. Added 1: Using another approach, I have obtained the following result
$$
f_{M_n}(m) = f_{M_{n-1}}(m)F_{X_n}(0) + \int\limits_0^m f_{X_n}(m-x)f_{M_{n-1}}(x)\mathrm dx
$$ which for 2 normal r.v. seems to give a result that agrees with the one put forward in one of the answers by Sasha. Added 2: Finally I got some free time to look at this problem again and here are my thought on it.  Once again, I would appreciate any comments on it. We begin by considering a joint distribution of $f_{S_1,S_2}(s_1,s_2)$ where $S_1 = X_1$,  $S_2 = S_1 + X_2$, $X_1 \sim X_2 \sim X$, and $X_1$ and $X_2$ are independent. We have $$
f_{S_1,S_2}(s_1,s_2) = f_{S_2 \mid S_1}(s_2 \mid s_1)f_{S_1}(s_1) = f_{X_2}(s_2 - s_1)f_{X_1}(s_1) = f_{X}(s_2 - s_1)f_{X}(s_1)
$$ Next, we have $
\begin{eqnarray*}
F_{M_n}(m)&=&P(M_n<m)\\
&=&P(\max\{S_{n},S_{n-1},...,S_1\}<m)\\
&=&P(S_{n}<m,S_{n-1}<m,...,S_1<m)\\

&=&\int\limits_{-\infty}^m ... \int\limits_{-\infty}^m f_{S_n,S_{n-1},...,S_1}(s_n,s_{n-1},...,s_1)\mathrm ds_n \mathrm ds_{n-1} ... \mathrm ds_1 \\

&=&\int\limits_{-\infty}^m ... \int\limits_{-\infty}^m 
f_{S_n \mid S_{n-1},...,S_1}(s_n \mid s_{n-1},...,s_1)...
f_{S_2 \mid S_1}(s_2 \mid s_1)f_{S_1}(s_1)
\mathrm ds_n \mathrm ds_{n-1} ... \mathrm ds_1 \\

&=&\int\limits_{-\infty}^m ... \int\limits_{-\infty}^m 
f_{S_n \mid S_{n-1}}(s_n \mid s_{n-1})...
f_{S_2 \mid S_1}(s_2 \mid s_1)f_{S_1}(s_1)
\mathrm ds_n \mathrm ds_{n-1} ... \mathrm ds_1 \\

&=&\int\limits_{-\infty}^m ... \int\limits_{-\infty}^m 
f_X(s_n - s_{n-1})...
f_X(s_2 - s_1)f_X(s_1)
\mathrm ds_n \mathrm ds_{n-1} ... \mathrm ds_1 \\

&=& \prod_{i=1}^n \int\limits_{-\infty}^m f_X(s_i - s_{i-1}) \mathrm ds_i
\end{eqnarray*}
$$ where $s_0 \equiv 0$. I hope the above notation is clear enough. Now $$
\begin{eqnarray*}
F_{M_n}(m) = \mathbb E[\mathbb I_{ M_n \leq m}] = \prod_{i=1}^n \int\limits_{-\infty}^{-\infty} f_X(s_i - s_{i-1}) \mathbb I_{s_i \leq m} \mathrm ds_i.
\end{eqnarray*}
$$ The characteristic function $\varphi_{M_n}(t) = \mathbb E[\mathbb e^{i t M_n}]$ is then $$
\begin{eqnarray*}
\varphi_{M_n}(t) = \prod_{i=1}^n \int\limits_{-\infty}^{-\infty} f_X(s_i - s_{i-1}) \mathbb e^{i t s_i} \mathrm ds_i
\end{eqnarray*}
$$","['probability-distributions', 'probability']"
68554,Solving differential equations with Fourier Transformation,"I'm looking for some documentation about solving DEs using Fourier Transformation (not Fourier Series). In particular, I have this one DE that I solved using another technique. Somebody mentioned that it can be solved using FT as well: $$\varepsilon - \frac{1}{2}l^2 \frac{d^2 \varepsilon}{dx^2} = \delta(x)$$ Where $\varepsilon$ is a function of $x$ only, $l$ is a scalar constant and $\delta(x)$ is the Dirac ""function"". The solution for the homogeneous case is $$\varepsilon = A \; exp \left[ -\frac{|x|\sqrt2}{l} \right]$$ If I remember correctly, taking some derivatives resulted in the final solution of $A = \dfrac{1}{l \sqrt{2}}$. Ok, now with FT. I found this site , the example looks a little like my DE -- with in my case $g(t) = \delta(x)$, which is convenient considering the convolution in the end. Without the constant $\frac{1}{2}l^2$ I get $$Y(f) - (2\pi i f)^2 Y(f) = \left( 1 - (2 \pi i f)^2 \right)\;Y(f) = G(f)$$ So $$Y(f) = \frac{G(f)}{1+4 \pi^2 f^2}$$. According to the site (would like to know how to compute it myself), $$F^{-1}\left( \frac{1}{1+4 \pi^2 f^2} \right) = \frac{e^{-|t|}}{2}$$ Taking the convolution with the Dirac ""function"" is again the same function, $\dfrac{e^{-|t|}}{2} = exp \left[ \dfrac{-|t|}{2} \right]$. So to conclude: Any documentation about solving DEs using FT is more than welcome How to do the same, now with the constant $\frac{1}{2}l^2$ How to compute the inverse Fourier Transform of expressions like above (I know it involves an integral to infinity, but I never learned how to solve integrals like that)","['ordinary-differential-equations', 'fourier-analysis']"
68556,Universal Definition for Pullback,"The concept of ""pullback"" has several definitions depending on the context in which it is applied, e.g., smooth functions on manifolds, differential forms, multilinear forms and so forth. See, for example, the Wikipedia Page for an enumeration of these definitions. Is there a ""universal definition"" of pullback from which these various specialized definitions can be derived or should these definitions be viewed as intrinsic and independent? I am aware of the categorical representation as discussed here , but I don't believe (perhaps I'm mistaken) that these various specialized definitions can be derived from the categorical one. Also, is it a coincidence that the adjoint of an operator and the pullback operation both share the exponentiated $*$ as an indicator or is it reflective of a deeper relationship?","['multilinear-algebra', 'differential-geometry']"
68560,Water and wine mixing problem,"This is a well-known problem involving a water barrel and a wine barrel, described here . The trick to solving the puzzle is that one need not make the calculations for each stage of the liquid movement. But if one does... The units here are the quantity of liquid moved each time, e.g. cupfuls. At the end of each ""take out/put back"" movement, the amount of wine in one barrel is the same as the amount of water in the other, so we can consider just the two volumes of liquid without caring about what they are made from. At the beginning the two volumes are $0$ and $n$, say. After the first movement the volumes are $$\frac{n}{n+1}\text{ and }\frac{n^2}{n+1}.$$ 
After the second the volumes are $$\frac{n(2n)}{(n+1)^2}\text{ and }\frac{n(n^2+1)}{(n+1)^2}.$$ 
After the third the volumes are $$\frac{n(3n^2+1)}{(n+1)^3}\text{ and }\frac{n(n^3+3n)}{(n+1)^3}.$$ 
There is a pattern here suggesting that after $t$ goes the volumes are $$\frac{n}{(n+1)^t}\sum_{k=0}^{t/2-1}{\binom{t}{t-2k}n^{2k}}$$ and $$\frac{n}{(n+1)^t}\sum_{k=0}^{t/2}{\binom{t}{t-2k-1}n^{2k+1}}.$$ 
As more liquid is moved the two volumes will each start to approach $\dfrac{n}{2}$. To my question: how do we show that the two volumes above both tend to $\dfrac{n}{2}$ as $t\rightarrow\infty$?","['algebra-precalculus', 'binomial-coefficients']"
68563,Cardinality of a certain set,"I was wondering if there's a formula for the cardinality of the set $A_k=\{(i_1,i_2,\ldots,i_k):1\leq i_1<i_2<\cdots<i_k\leq n\}$ for some $n\in\mathbb{N}$. I calculated that $A_2$ has $n(n-1)/2$ elements, and $A_3=\sum_{j=2}^{n-2}\frac{(n-j)(n-j+1)}{2}$. As you can see, the cardinality of $A_3$ is already represented by a not so nice formula. Is there a general formula?","['elementary-set-theory', 'combinatorics']"
68564,Is this binomial coefficient identity known?,"I stumbled across this identity involving binomial coefficients this morning: If $n$, $k$, $a$, and $b$ are positive integers and $n=a+b$, then $$ \binom{n}{k} =\sum_{i=0}^k \binom{a}{k-i}\binom{b}{i}.$$ The proof is trivial. (Partition a set $N$ of $n$ things into disjoint sets $A$ of $a$ things and $B$ of $b$ things. Now to pick $k$ items from $N$, either all $k$ are from $A$, or $k-1$ are from $A$ and $1$ is from $B$, or...) Has this identity appeared before (I expect the answer to be ""Yes!"") and if so, does it have a name?","['binomial-coefficients', 'reference-request', 'combinatorics']"
68568,Does every field have a non-trivial Galois extension?,"Clearly a field $F$ with no Galois extensions must have only non-separable elements in any extension (otherwise, take the minimal polynomial of some separable element over $F$ - its splitting field will be a Galois extension of $F$). This argument reduces the possible examples to non-perfect fields, or in other words - infinite fields of positive characteristic. However, I can't come up with an example of such a field with no separable elements over it. Are there any such examples? EDIT: I should have been clearer - I'm looking for a field that has non-trivial algebraic extensions, but has no non-trivial Galois extensions.","['galois-theory', 'abstract-algebra', 'field-theory']"
68587,Nonexistence of certain holomorphic function with periodicity,"Sorry lads, but my analysis is long in the past... Given a lattice $\Gamma$ in the complex plane (or more generally: in a complex vector space $V$), why does there not exist a holomorphic function $f$ on the whole plane (or $V$) which satisfies: $f(z+\gamma)=f(z)-\bar{\gamma}$ for all $z \in \mathbb C$ (or $V$) and $\gamma \in \Gamma$ ? (here $\bar{\gamma}$ denotes the complex conjugate).",['complex-analysis']
68590,Can f not be in L1 if its Fourier transform is in L-infinity?,"Can $f\notin L^1$ if its Fourier transform $\hat f \in L^\infty$  ? This is a question about the endpoint of Pontryagin duality.  We know that if a function is in $L^1$, then its Fourier transform lies in $L^\infty$. This is very easy to show.  But what about the converse: Can the $L^1$ norm of $f$ be infinite even if the $L^\infty$ norm of its Fourier transform is finite? I assume that the answer to my question is YES, but I do not see how to handle this case. Any references?",['real-analysis']
68607,Textbook for Projective Geometry,"So while not actually a specific problem I'm struggling with, I was hoping for some of your insight!  For a course, I'm currently reading Stillwell's Four Pillars of Geometry.  While it does a nice job of motivating the theory, I find it a little sparse with regard to challenging/engaging problems.  Any suggestions as to where I can turn to find a nice supplementary text on projective/hyperbolic geometry?","['geometry', 'reference-request', 'projective-geometry']"
68610,Tangent bundle of the projective space.,"I would like to know how does one imagine/write-down the tangent bundle of the real/complex projective space. Is there something simplifying that happens especially for $\mathbb{RP}^1$? Isn't there some relationship of this tangent bundle to spheres and Möbius bands? I can write down transition functions on these but that doesn't answer ""what"" is the tangent bundle. I vaguely know that Möbius band can be imagined as the anti-podal identification of the normal bundle on the circle. What is the exact statement and how to see that? Since projective spaces are easier to think of as quotients of simpler spaces like spheres I am motivated to ask the following question, In general if a group action on a manifold is such that the quotient space is again a manifold then the same action will also do a quotient of the tangent bundle. Now is the quotient of the tangent bundle (will that be again a vector bundle?) anyway related to the tangent bundle of the quotient space?",['differential-geometry']
68622,Doubling Money Game,"The casino offers a certain win-lose game, where you have $p$ chance of winning. You can bet any amount of money, and if you win you get twice your bet; otherwise, you lose your bet. If you use the optimal strategy, what is your chance of doubling your money, as a function of $p$? I came up with the following incorrect solution: You have a 100% chance of winning if $p>\frac{1}{2}$ and $p$ chance of winning if $p<\frac{1}{2}$. Suppose that $p>\frac{1}{2}$. Then each time you bet exactly half your money. If you have $x$ dollars, you end up with $\frac{3}{2}x$ if you win and $\frac{1}{2}x$ if you lose, hence your expected outcome is $\frac{3}{2} xp + \frac{1}{2}x(1-p)$ which equals $xp + \frac{1}{2}x$. So if $p>\frac{1}{2}$, then the total is greater than $x$. Given that each game on average gains you money and you can play an arbitrary number of games, of course you should have 100% chance of doubling your money. Similarly, if $p<\frac{1}{2}$, it can be shown that no matter how much we bet, we lose money on average. Then on average our money will tend to go toward zero, so we're better off just going all-in at the start, with $p$ chance of doubling our money. I do not understand the proper solution, but I think this solution is incorrect; however, I'm having trouble pinpointing where my proof falls apart. Thanks. Edit : for reference I've included a screenshot of the given solution.",['probability']
68625,Manipulating harmonic series,"Given the harmonic series has the summation $$
\sum_{k=1}^n\frac1k=\ln|n| + O(1)$$ How do we show that: $$
\sum_{k=1}^n\frac{1}{2k-1}=\ln\left|\sqrt{n}\right| + O(1)$$",['sequences-and-series']
68635,Banach Indicatrix Function,"Let $f: [a,b] \rightarrow \mathbb R$ be of bounded variation. For $y \in \mathbb R$, define the Banach Indicatrix of $y$ by $N(y) = \# f^{pre} (y)$, ie. $N(y)$ is the cardinality of the pre-image of $y$ under $f$. I seek to prove the following: . (a) N(y) is finite for almost every $y \in \mathbb R$ (b) The function $y \mapsto N(y)$ is measurable. (c) The total variation of $f$ is given by $TV(f) = \int_c ^d N(y) dy$. I have tried partitioning $[a,b]$ and looking at the variation over them in order to bound the size of the set where $N(y)$ is infinite, but this hasn't yielded much success.","['measure-theory', 'functional-analysis', 'real-analysis']"
68638,Totally ramified cyclic extensions of degree $p^a$ of $\mathbb{Q}_p$,It's quite easy to show that the totally ramified extension $\mathbb{Q}_p(\zeta_{p^{a+1}})/\mathbb{Q}_p$ contains a unique subextension $E$ s.t. $E/\mathbb{Q}_p$ is a cyclic extension of degree $p^a$ and this is given by the fixed field of the unique subgroup of order $p-1$ of the Galois group. My questions is: can we explicitly write down what this field is?,"['algebraic-number-theory', 'number-theory']"
68645,Can all subseries of an infinite series be pairwise independent over $\mathbb{Q}$?,"I'm wondering about a simple question that has multiple possible variants depending on a few parameters. The prototypical one would be: Does there exist an infinite series such that any two subseries  are linearly independent over $\mathbb{Q}$? Assume that the two subseries (sums of subsets of terms from the original series) in question are to be summed in order of increasing index - this allows one to put in place a restriction on absolute or conditional convergence. One can add a combination of restrictions from {finite, cofinite, infinite} to each of the two subseries, e.g. ""any finite subseries $\circ$ and cofinite subseries $\bullet$."" Lastly, one can replace linear independence with algebraic independence - much tougher, I imagine - or replace indepenence with de $\text{}$pendence. Are there any general results on these sorts of questions?","['sequences-and-series', 'reference-request', 'real-analysis', 'analysis']"
68650,Finding a smooth function less than some given (positive) continuous function,"Let $M$ be a smooth manifold ($dim\ge 1$). Let $f:M\to\mathbb{R}$ be a positive continuous function. Prove there is a smooth map $g\in C^{\infty}(M)$ such that $0<g<f$. I knew this would involve a partition of unity. I took a locally finite cover of $M$ by precompact open sets ${\bf U}=\{U_i\}_{i\in I}$. Since $f$ is continuous I know $f$ attains a minimum on each ${\rm cl}(U_i)$, say at $x_i$, and that this minimum is still positive. So I defined maps $g_i: U_i\to\mathbb{R}$ given as a constant maps: $g_i(x)=\epsilon_i$, where $\epsilon_i$ is chosen to be $0<\epsilon_i<f(x_i)$. But each $x\in M$ is contained in only finitely many $U_i$, so I defined $h(x)=\min{\{g_i(x)\;|\; x\in U_i\}}$. Now I take a partition of unity $\{\psi_i:M\to\mathbb{R}\}_{i\in I}$ subordinate to ${\bf U}$. My thinking is to smoothly stitch together these constant maps... but I'm not sure what to do from here. Should I define $g(x)=\sum_{i\in I}{\psi_i(x)\,h(x)}$? I know each of these sums will be finite because $supp(\psi_i)\subseteq U_i$, but does this function actually do what I want it to? I thought at first it would, but now it just looks like I'm multiplying by $1$ in a really fancy way... Thanks for your help",['differential-geometry']
68660,Why does the Riemann zeta function have zeros in the complex plane? How is it possible to find them?,"I ask this because, according to Euler's product formula, Riemann's zeta function =(1/something), so how could that be zero?
Also, how could one find zeros that are on the negative side and find a zero in the ""critical strip"", that is, when the real part of the input is between 0 and 1?","['analytic-number-theory', 'riemann-zeta', 'complex-analysis']"
68694,Does $F(t)F'(t) \le 0$ imply that $F$ does not change signs?,"The question is in the title, really. More precisely, suppose $F:[0,1] \to \mathbb{R}$ is continuously differentiable and satisfies (i) $F(t)F'(t) \le 0$ for all $t \in [0,1]$ (ii) $F(0) = 1$ Does this imply that $F(t) \geq 0$ for all $t \in [0,1]$? My intuition is that (i) implies that $F$ and $F'$ have different signs. Since $F(0) = 1$, this means that $F$ must cross the $t$-axis at some point, but the instant it becomes negative it would have to immediately shoot up again. I think this is not possible, but I have not been able to come up with a proof or counter-example. Thanks for your help!","['calculus', 'functions']"
68697,The composition of a convex function with an integrable function is integrable,"If $f$ is (Riemann) integrable on $[0,1]$ and $\varphi$ is convex on $\mathbb{R}$, are there any simple or elegant proofs that $\varphi\circ f$ is also integrable on $[0,1]$? This paper by J. Lu answers the question (and other similar questions) in a bit more generality--basically whenever $\varphi$ is merely continuous rather than convex--but I'm wondering if there's a simpler method here.","['integration', 'real-analysis', 'analysis']"
68701,Puzzle: Can arithmetic be axiomatized with a single two-term relation?,"Following my question about defining multiplication in terms of divisibility , can all of arithmetic be axiomatized with a single two-term relation?  Asaf Karagila comments on my question that the three-term relation Remainder is sufficient.  I have since discovered that a two-term relation suffices, and this Wikipedia article agrees (""first-order logic is undecidable [...] provided that the language has at least one predicate of arity at least 2"").  What is the binary predicate?  How can arithmetic be axiomatized in terms of it?  I have already discovered the answer, so I have tagged this as a puzzle.  However, my construction is difficult and I am wondering if there are any easier ways. EDIT: No answers yet so I'm adding some more details about my own solution.  A binary predicate that I believe suffices is: R(2x+1,y) := Divides(x,y), R(2x,y) := Testbit(x,y).  A next step is defining equality as x=y := ∀z:R(z,x)↔R(z,y).  Still missing are the definitions of 0, 1, addition, and multiplication. EDIT: As requested I'll include the rest of my attempted solution and expand the question somewhat.  Is my solution correct?  Can it be simplified in any significant way?  Are there other binary predicates that work?  What is a more precise and correct way to state the problem?  Sorry if that is too much.  Anyway, here is my solution.  Start with defining equality as: x=y := ∀z:R(z,x)↔R(z,y) which means equal things have the same divisors and also the same bits set.  Next define 0 and 1: x=0 := ∀y:R(x,y)↔R(y,x) Odd(x) := ∃y:y=0∧R(y,x) x=1 := Odd(x)∧∀y:¬Odd(y)∧R(y,x)↔y=0 Also with Odd, divisibility can be unzipped: Divides(x,y) := ∀z:Odd(z)→(R(z,x)→R(z,y)) Now we can define 2: PowerOf2(x) := ∃y:¬Odd(y)∧R(y,x)∧∀z:¬Odd(z)∧R(z,x)→y=z Prime(x) := ¬(x=1)∧∀y:Divides(y,x)↔(y=x∨y=1) x=2 := PowerOf2(x)∧Prime(x) And also multiplication by 2: F(x,y,w) := PowerOf2(w)∧Divides(w,x)∧¬Divides(w,y) x=2*y := (∀z:Odd(z)→(Divides(z,y)↔Divides(z,x)))∧(∃w:F(x,y,w)∧∀z:F(x,y,z)→z=w) Having multiplication by 2 allows Testbit to be unzipped: Testbit(x,y) := ∃z:z=2*y∧R(z,x) Testbit gives us powers of 2: x=Power(2,y) := ∀z:Testbit(z,x)↔z=y Which finally yields the successor function: y=Successor(x) := ∃z:∃w:z=Power(2,x)∧w=Power(2,y)∧w=2*z And also less-than: x<y := ¬(x=y)∧∃z:∃w:z=Power(2,x)∧w=Power(2,y)∧Divides(z,w) Now addition can be defined: a+b=c := ∀i:Testbit(i,c)↔(Testbit(i,a)⊕Testbit(i,b)⊕
∃j:j<i∧Testbit(j,a)∧Testbit(j,b)∧∀k:(k<i∧j<k)→(Testbit(k,a)⊕Testbit(k,b))) Finally, with a definition of a+b=c and Divides(x,y), follow the same construction given in the accepted answer to my prior question to define multiplication in terms of divisibility and addition, then it is done.","['logic', 'puzzle', 'number-theory']"
68704,"Given $a$, $b$, and a prime $p$, how fast can we solve $(a \cdot c) - (b \cdot d) \equiv 1 \bmod p$?","If we're given two naturals, $a$ and $b$, and a prime $p$, how fast can we find two more naturals such that $(a \cdot c) - (b \cdot d) \equiv 1 \bmod p$? Additionally, you are allowed to precompute anything you like, but the memory space you are given can be no larger than the time it takes to solve this problem.  In other words, how fast can we solve for these two variables in equal time and space?","['prime-numbers', 'number-theory']"
68726,A closed subset of an algebraic group which contains $e$ and is closed under taking products is a subgroup of $G$,"A closed subset of an algebraic group which contains $e$ and is closed under taking products is a subgroup of $G$. Denote this set as $X$. If the condition of $X$ being closed is dropped, this statement does not hold. The set of nonzero integers in $\mathbb{G}_m$ over $\mathbb{C}$ is a counterexample. It suffice to prove that for any $x \in X$, $x^{-1}X = X$. As $X$ is closed under taking products, it is clear that $X \subseteq x^{-1}X$. In order to prove the inverse inclusion, the closedness of $X$ (under Zariski topology) must be used. Let $\phi: G \rightarrow G, y \mapsto x^{-1}y$ is a homoemorphism of $G$ as an algebraic variety. So $x^{-1}X = \phi(X)$ is a closed subset of $G$ containing $X$. But Why are they equal? Thanks very much.","['zariski-topology', 'algebraic-groups', 'group-theory']"
68732,Is the problem asking to show that $r\times \nabla \psi$ satisfies wave equation wrong?,"Considering the wave equation in spherical coordinates, if we know that $\psi(\vec{r})$ is a solution, then $\vec{r}\times \nabla \psi$ is also a solution. (The hint is to take the difference between $\psi(r,\theta,\phi)$ and $\psi(r,\theta ',\phi ')$) If I interpreted it correctly, it says that if $\psi $ solves
$$\nabla^2\psi  - \frac{1}{c^2}\frac{\partial^2 \psi}{\partial t^2} =0$$ Then show that: $$\nabla^2(\vec{r}\times \nabla\psi)  - \frac{1}{c^2}\frac{\partial^2 (\vec{r}\times \nabla\psi)}{\partial t^2} =0$$ This question seems outright wrong. As the argument of the Laplacian is a vector. Or am I misinterpreting it?","['multivariable-calculus', 'vector-analysis', 'partial-differential-equations']"
68738,Smoothness of level curves,"Lets $f:\mathbb{R}^2 \to \mathbb{R} $, where $f$ is harmonic, continuous and non-constant. How do I go about showing that the level curves of $f$ are smooth? Thanks!","['differential-geometry', 'real-analysis']"
68740,"Showing that $f: \mathbb{R} \rightarrow (-1,1): x \mapsto \frac{x}{1+|x|}$ is surjective","I am having real difficulty to show that $f: \mathbb{R} \rightarrow (-1,1): x \mapsto \frac{x}{1+|x|}$  is a surjective function. I have trouble seeing that every $y \in (-1,1)$ can be written as $\frac{x}{1+|x|}$ for some $x$. Could anyone give me a little hint?",['functions']
68746,General question on relation between infinite series and complex numbers,"This is a strictly preliminary question. 
I hope to elicit some discussion/s which will lead to a more acceptable form for the question on this site. I'm trying to understand how the study of the following infinite series: $1 + x^2 + x^4 + x^6 + \cdots$                     [1] and complex numbers in general are related. Specifically, given the relation: $1 + x^2 + x^4 + x^6 + \cdots$ = $(1 - x^2)^{-1}$ [2], and $1 + 2^2 + 2^4 + 2^6 + \cdots$ = -$\frac{1}{3}$    [3] in what way do complex number concepts come into play when trying to understand the 'intricacies' of equation [3] above? (I've come to gain some understanding of the equations above; namely: (1) the relation [2] is only applicable when |x| < 1; (2) Euler was generally happy to use relations as [3] in his studies. But, specifically, in trying to study this problem, where and how are complex numbers employed to come to a deeper understanding of the question?) Request to potential answerers : I have a very minimal background in real analysis, and almost none of complex analysis. If you may be kind enough to answer / respond so I may understand your answer / response that would be greatly appreciated. :)","['complex-numbers', 'self-learning', 'sequences-and-series']"
68747,Injective map between power series ring,"Suppose $k$ is a field and let $n > m$. Does there exist injective homomorphisms 
$$ k [[x_1, x_2, \ldots, x_n]] \rightarrow k[[x_1, x_2, \ldots, x_m]]\ ?$$","['commutative-algebra', 'power-series', 'abstract-algebra']"
68749,Difference of order statistics in a sample of uniform random variables,"Let $U_{1}, \, ... \, ,U_{n}$ be a random sample of uniform random variables $U_i \sim \mathrm{Uniform}(0,1)$. Let $U_{(1)}, \, ... \, , U_{(n)}$ be the order statistics of the sample. The goal is to prove that: $$
W = U_{(s)}-U_{(r)} \sim \textrm{Beta}(s-r, \, n - s + r +1) \qquad 1 \leq r < s \leq n
$$ My ""proof"" The joint pdf of $U_{(r)}, U_{(s)}$ is: $$
f_{U_{(r)}, U_{(s)}} (u, v) = \frac{n!}{(r-1)!(s-1-r)!(n-s)!} u^{r-1} (v-u)^{s-1-r} (1 -v)^{n-s} \cdot \textbf{1}_{\{u < v\}} 
$$ Consider the transformation: $$ 
\begin{Bmatrix}
W = U_{(s)} - U{(r)}  & U_{(r)} = Z \\
Z = U_{(r)} & U_{(s)} = W + Z \\
\end{Bmatrix}
$$ The absolute value of the Jacobian determinant is 1. Therefore, the joint pdf of the transformation is: $$
f_{W,Z}(w,z) = \frac{n!}{(r-1)!(s-1-r)!(n-s)!} z^{r-1} w^{s-1-r} (1 - w  - z)^{n-s} 
$$ $$
f_W(w) = \int_{S} f_{W,Z}(w,z) \, \textrm{d}z
$$ For fixed $w$, we have  $S = \{z  \, : \, 0 \leq z \leq 1 - w\}$. Thus: $$
f_W(w) = \frac{n!}{(r-1)!(s-1-r)!(n-s)!} w^{s-1-r} \int_{0}^{1-w} z^{r-1}(1-w-z)^{n-s} \, \textrm{d} z
$$ The problem is that I do not know how to evaluate the integral, but Wolfram Alpha does: $$
\int_{0}^{1-w} z^{r-1}(1-w-z)^{n-s} \, \textrm{d} z= \frac{\Gamma(r)\Gamma(n-s+1)}{\Gamma(n+r-s+1)} (1- w)^{n+r-s}
$$ Now it is really easy to figure out the distribution of $W$. Indeed, the only thing that has to be done is to rewrite the factorials using gamma functions. Initially I was going to ask how to evaluate the integral without the help of Wolfram Alpha, but then I realized that maybe there is a ""better"" proof that avoids it. I have tried to find an alternative proof by using multiple transformations which involved $W$ and another transformation, but it was useless. Edit I have corrected some mistakes I think I made: 1) The joint pdf I wrote was incorrect. 2) I wrote that I the pdf of $W$ could be found as a convolution, but $U_{(r)}, U_{(s)}$ are clearly dependent. What I actually did was an implicit change of variables.","['statistics', 'probability-distributions']"
68752,Abelianization of GL_n,"Suppose $k$ is a field. How to prove that the abelianization of $GL_2(k)$ is $GL_1(k)$ ? Ditto for $GL_n(k)$. Can we say the same thing, were we to replace $k$ with $\mathbb Z$ ?","['linear-algebra', 'group-theory']"
68755,How to intercept someone moving in a 2-dimensional grid world?,"Suppose you have a discrete 2D grid where each point represents a person, for example $\mathbf{d} = (x_d, y_d)$. Each person can only move exactly one square up, left, right or down (or stay put) per tick of the clock. Let's say that $\mathbf{d}$'s velocity is $(c_x, c_y)$, which means that on average after $t$ ticks he will end up to point $\mathbf{e} = (x_e, y_e)$, which lies $t \cdot c_x$ squares left or right on the $x$-axis and $t \cdot c_y$ squares up or down on the $y$-axis. Therefore, $$x_e = x_d + c_x \cdot t$$
$$y_e = y_d + c_y \cdot t$$ Let another person, $\mathbf{o}$, who is chasing after $\mathbf{d}$ and knowing his velocity tries to intercept him. He has to calculate the point $\mathbf{e}$ where $\mathbf{d}$ will end up after $t$ moves and then follow the shortest path of length $t$ there. Since no diagonal moves are allowed, $t = |x_e-x_o| + |y_e-y_o|$. So, to find $\mathbf{e}$ we end up with a system with two equations and two unknowns, like so: $$x_e = x_d + c_x \cdot (|x_e-x_o| + |y_e-y_o|)$$
$$y_e = y_d + c_y \cdot (|x_e-x_o| + |y_e-y_o|)$$ Is this whole reasoning correct or am I missing something? If so, how is this system solved? Additional details: We have four predators $\mathbf{o_i}$ which are chasing after a single prey $\mathbf{d}$. To capture the prey, they need to surround it from all directions, ie up, down, left and right. The predators' movement can be defined as we like, except for the usual constraint, that each predator can move up to one square and diagonal moves are not allowed. The prey moves randomly, with $p=0.2$ to conduct any of its available move (up, down, left, right, stay still). However, if its path is blocked from one side by a predator, then it will never move towards that square. I'm not sure of the details but I assume ""the dice"" are thrown again and again until a non-obstructed move is selected. Motivation for the question above: The point is that after some but not all the predators have reached their position beside the prey to use our knowledge of the prey's behaviour to predict its movement so that the remaining predators can move towards where it will be in the future, not where it is right now. Hence, I said ""on average"" above instead of saying all this. Full disclosure: I'm working on a homework assignment for the pursuit domain (multiagent systems). This question is part of my latest approach which involves herding the prey. The previous approached involved surrounding it without getting too close and then moving in for the kill.","['computer-science', 'algebra-precalculus']"
68762,"$A$ is normal and nilpotent, show $A=0$","Given a matrix $A \in R^{n \times n}$ which is normal ( $AA^H=A^HA$ where $A^H$ is hermitian of $A$ ) and nilpotent ( $A^k=0$ for some $k$ ). Now we need to show that $A=0$ . (This is essentially exercise 5(b) in sec. 80 on p.162 of Paul R. Halmos' Finite-Dimensional Vector Spaces .) I tried to show in the following way, we know that, $AA^H=A^HA$ pre-multiply by $A^{k-1}\implies A^kA^H=A^{k-1}A^HA$ Now, we have $0 = A^{k-1}A^HA$ , since $A$ nilpotent. I am not sure how to proceed from here to show $A=0$ . Can someone help me in this problem?","['matrices', 'nilpotence', 'linear-algebra']"
68777,Voronoi diagram with different metric functions,"Given a metric space $(X,d)$ and finite number of points $(x_i)_{i=1}^n$ the Voronoi diagram (or the Dirichlet cell) $C_i$ is given by
$$
C_i = \{x\in X:d(x,x_i)<\min\limits_{j\neq i}d(x,x_j)\}.
$$ I have two question: Does anybody know if it was considered the case with different distance function, namely
$$
C'_i = \{x\in X:d_i(x,x_i)<\min\limits_{j\neq i}d_j(x,x_j)\}.
$$
where $(d_i)_{i=1}^n$ are all metric on $X$. For the latter case - is it possible in general to find a metric $d'$ on $X$ such that $C'_i$ define above admits the representation
$$
C'_i = \{x\in X:d'(x,x_i)<\min\limits_{j\neq i}d'(x,x_j)\}.
$$
for any $i$. Edited : the trivial answer on 2. is to set $d'(x,x) = 0$, $d'(x,y)=1$ if $x,y$ are from one cell and $d'(x,y)=2$ if they are from the different cells. I wonder if 2. can be solved without construction $C'_i$ first. Since the question now is not mathematically correct, I'll put a reference request tag.","['optimization', 'geometry', 'metric-spaces', 'reference-request']"
68788,"idea of the star position in pullback, pushforward notation","i would like to know if there is some idea behind the position of the star in the pullback, pushforward notation or if it is just some notation without background? What's the reason for the star to be up at the pullback and not down?* I've read that there is no real standard notation (Spacetime and Geometry, Sean Carroll) and i wanted to find out what the original idea behind the notation was. I didn't find anything useful, so maybe you know? Edit: *","['notation', 'differential-geometry']"
68791,Differential geometry text with categorial flavor,"I am blown away by exposition of analytic manifolds in Serre's Lie algebras and Lie groups , I want more! Is there a text that treats classic topics of differential geometry like connections, Riemannian metrics, holonomy, de Rham cohomology, (almost) complex manifolds, homogeneous spaces, symmetric spaces in such a manner? Or maybe some more analytic manifolds, towards the modern research topics?","['book-recommendation', 'reference-request', 'differential-geometry']"
68793,Integration by substitution for line integrals,"Let $P, Q: D \rightarrow \mathbb{R}$ be continuous functions on an open set $D \subset \mathbb{R}$, let a curve $\gamma: [a,b] \rightarrow \mathbb{R}^2$ be of class $C^1$ such that $\gamma([a,b]) \subset D$. Let $\Phi=(\phi, \psi): D' \rightarrow D$ be a diffeomorphism of class $C^1$ from open $D' \subset \mathbb{R}^2$ onto $D$. Let $x=\phi(x',y')$, $y=\psi(x',y')$. I search a formula for a line integral $I:=\int_{\gamma} P(x,y)dx+Q(x,y)dy$   in coordinates $x',y'$. If we calculate differentials $dx$ and $dy$ and formally put it in integral $I$ we obtain that $\int_{\gamma} P(x,y)dx+Q(x,y)dy=\int_{\Phi^{-1} \circ \gamma} [P(\phi(x',y'), \psi(x',y')) \frac{\partial \phi}{\partial x'}+Q(\phi(x',y'), \psi(x',y')) \frac{\partial \psi}{\partial x'}] dx'$+$[P(\phi(x',y'), \psi(x',y')) \frac{\partial \phi}{\partial y'}+Q(\phi(x',y'), \psi(x',y')) \frac{\partial \psi}{\partial y'}]dy' $. Does it formula indeed hold? How to show it? (maybe by changing the both line integrals on Riemann integrals?) Thanks.",['analysis']
68800,"Functions which are Continuous, but not Bicontinuous","What are some examples of functions which are continuous, but whose inverse is not continuous? nb: I changed the question after a few comments, so some of the below no longer make sense. Sorry.","['general-topology', 'examples-counterexamples']"
68812,Does every Jacobian over $\overline{\mathbf{Q}}$ have everywhere good reduction?,Let $J$ be the Jacobian of a smooth projective connected curve of genus $g>1$ over the field $\overline{\mathbf{Q}}$ of algebraic numbers. Does $J$ have everywhere good reduction? I know that there are abelian varieties of any dimension which do not have good reduction everywhere. Just take a suitable elliptic curve $E$ and consider the product $E^g$.,['algebraic-geometry']
68814,How do we show every linear transformation which is not bijective is the difference of bijective linear transforms?,"I have been reviewing some ideas about vector spaces and came upon a surprising fact. I am not quite sure how to begin the argument because the problem requires one to construct two bijective linear transformations whose difference is equal to a given linear transformation. Let $V$ be a vector space over a filed $F$.  Suppose $\phi:V \rightarrow V$ is a linear transformation that is not a bijection. How do we show $\exists f,g :V \rightarrow V$ that are both bjiective linear transformations such that $\phi = f - g$. I tried proving the fact using contradiction but have not been able to get to far so I am wondering if there is a standard constructive proof that applies directly.","['vector-spaces', 'linear-algebra']"
68817,Absolute error loss for a gamma random variable,"Let $X \sim \operatorname{Gamma}(2,1)$, I would like to minimize with respect to $a$
$$E|aX-1|=\int_0^{1/a}(1-ax)xe^{-x}dx+\int_{1/a}^\infty (ax-1)xe^{-x}dx$$ Is there some neat way to do this?
The only way I know is to use calculus on the RHS to find the minimum with respect to $a$.
By neat, I mean a way that use facts from probability or gamma function? Thanks.","['gamma-function', 'probability']"
68832,Showing $R(m\otimes n)$ is free if $Rm$ and $Rn$ are free,"Let $R$ be a commutative ring with identity. The following is a statement I came across about the submodule $Rt$ generated by a decomposable tensor $t=m\otimes n$ being free, given that $Rm$ and $Rn$ are free.  I am not sure if the converse is true but I would be interested in seeing a counterexample. Let $M, N$ be $R$-modules, and let $m$ be in $M$ and $n$ be in $N$.  Suppose also that $Rm$ and $Rn$ are free. Is $R(m \otimes n)$ free?","['modules', 'commutative-algebra', 'abstract-algebra']"
68841,"How does one move a point in $B(0,1)$ to the origin with a Möbius transformation","Let $z_0$ be in the open unit disc $B(0,1)\subset \mathbf{C}$. Is there a general formula for an automorphism of $B(0,1)$ which sends $z_0$ to the origin? I find it easier to think about the complex upper half plane $\mathcal{H}$ so I guess one could do the following. Map $B(0,1)$ bijectively to $\mathcal{H}$ via $$\varphi:z\mapsto \frac{z+1}{iz+1}.$$ Let $\tau_0$ be the image of $z_0$ under this isomorphism. Find a Möbius transformation $\mu$ sending $\tau_0$ to $i$ and define $$f = \varphi^{-1} \circ \mu \circ\varphi.$$ This is the automorphism we're looking for. The only problem is finding $\mu$. How can I do this explicitly?","['differential-geometry', 'riemann-surfaces', 'riemannian-geometry', 'complex-analysis']"
68852,Is there a neat way to show $\int_{-1}^1 \frac{ U_n(z) U_n(z)}{\sqrt{1-z^2}} \mathrm{d} z = \pi (n+1)$,"In answering a question on math.SE, I attempted to find integral of Fejér kernel by using $$
   K_n(t) = \frac{1}{n}  U_{n-1}^2\left( \cos \frac{t}{2} \right)
$$
where $U_n(z)$ stands for the Chebyshev polynomial of the second kind. Then 
$$
 \frac{1}{2 \pi} \int_{-\pi}^\pi K_n(t) \mathrm{d} t = \frac{1}{\pi n} \int_{-1}^1 \frac{U_{n-1}^2(z)}{\sqrt{1-z^2}} \mathrm{d} z
$$
Also it is known that the left-hand-side integral is one, I could not find any neat way of showing that the right-hand-side integral equals one. Note that, by orthogonality property for $U_n(z)$:
$$
  \int_{-1}^1 U_{n-1}^2(z) \sqrt{1-z^2} \mathrm{d} z = \frac{\pi}{2}
$$ Thanks for reading.","['orthogonal-polynomials', 'special-functions', 'integration', 'polynomials']"
68864,Trigonometric identity proof,"I’ve got to proof that: $$\tan\left(\frac{A}{2}\right) = \sqrt{\frac{1 -\cos(A)}{1 + \cos(A)}}$$ My attempt was (tried with right side): $$= \pm \sqrt{\frac{1 -\cos(A)}{1 + \cos(A)} \cdot \frac{1 -\cos(A)}{1 - \cos(A)}}$$
$$= \pm \sqrt{\frac{\left(1 -\cos(A)\right)^2}{1 - \cos^2(A)}}$$
$$= \pm \frac{1 -\cos(A)}{\sin(A)}$$ They are not even similar. I tried Wolfram Alpha online calculator and it showed one of the alternate answers as: $$\sqrt{\tan^2\left(\frac{A}{2}\right)}$$ That’s the answer I suppose I should get to, but I’ve tried it many times and I can’t find (imagine) a possible route. Please, if you could point me in the right direction, I’d greatly appreciate it. Thank you very much in advance.",['trigonometry']
68866,An unbounded convex polyhedron realizing the primes?,"Does there exist an unbounded convex polyhedron with faces that have 3, 5, 7, 11, 13, ...
edges, i.e., such that the number of edges of each face realize exactly the odd primes, with each prime realized exactly once (i.e., there is just one face with that number of edges)?  If so, can this be achieved with adjacent primes realized by adjacent faces?
Here is a start. :-) Update . Now answered negatively by Ed Pegg!  For a related question, which does have a positive solution, see the MO question, "" Polyhedra that combinatorially shadow a sequence "": There is a polyhedron whose shadows combinatorially mimic the primes when the polyhedron is appropriately continuously reoriented.","['prime-numbers', 'geometry', 'polyhedra']"
68872,"if a property holds on closed points of an algebraic variety, does it hold over all geometric points?","Say I've got a variety X (or a scheme locally of finite type) over an algebraically closed field k.
Then closed points of X correspond to k-points of X. (correct?) Let's define a geometric point of X as a morphism from an algebraically closed field into X. (thus for example the morphism from k[x] to the algebraic closure of the field of fractions of k[x] is a geometric point of the line) If a (reasonable!) property P holds for all k-points of X does it then hold for all geometric points? My question comes from moduli stuff. For example, if E is a flat family of sheaves on X parameterised by some base S, such that the fibre of E has some behaviour over all k-points of S, will this behaviour persist on geometric points?",['algebraic-geometry']
68879,Formula for working out the number of dice combinations resulting in a given value,"It's been about nine years since I last undertook any formal mathematical training, and I am wrestling with generating probability curves in R. What I want to know is, for a dicepool of an arbitrary number of d10s, how many combinations will result in a given value? At the beginning and end of the range, this is easy to work out since with 5d10, there's only one combination which will result in 5: All 1s. For 6, once dice needs to be 2, so there are five combinations. That part, I'm not having issues with. But as you get into the midrange, the number of possible combinations which could result in a given total increases exponentially. There must be some formula which can let me calculate this. I've been attempting to work it out from wikipedia pages for most of the afternoon, but my grasp of mathematical jargon is now non-existent, so I am having some issues. If someone has a formula I can plug numbers into, that would be fantastic.",['combinatorics']
68886,Vector Theory Question,"I am having trouble getting started on this multi-part problem. Could anyone take a look and provide some insight on how I might go about coming to a solution for the first part. Q: Assume for two arbitrary vectors v and u that || u || = 3 and || v || = 5. What is the maximum value of || v + u ||? I've rewritten the magnitudes in their equation form but still am not having any light bulbs go off: || u ||=$\sqrt{(u_1)^2+(u_2)^2}$
|| v ||=$\sqrt{(v_1)^2+(v_2)^2}$ Any help is appreciated, thanks.","['optimization', 'multivariable-calculus']"
68906,Unbiased estimator questions,"If $X_1,X_2,\ldots,X_n$ are i.i.d. $\mathrm{B}(1,p)$, find the best unbiased estimator of $p^n$. Attempt: Use indicator functions to show every observation has mean equal to 1 so this is the same as the summation of the xi's = n. So this is a sufficient statistic and best unbiased estimator. Let $X_1,X_2,\ldots,X_n$ be i.i.d. $\mathcal{N}(\mu,1)$. If $\bar{x}$ attains the lower bound as an unbiased estimator of $\mu$, find the Fisher Information in $(X_1,X_2,\ldots,X_n)$. Fisher information = 1/CR-lower bound. If $X_1,X_2,\ldots,X_n$ are i.i.d. Uniform on $(\theta-\frac{1}{4},\theta + \frac{1}{4})$, find the sufficient statistic for theta and find an unbiased estimator of $\theta$ based on $\bar{x}$ and determine if you can improve it. So x_bar is a sufficent statistic and E[Unbiased Estimator|Sufficent Statistic] is the best unbiased estimator.","['statistics', 'parameter-estimation', 'estimation', 'random-variables']"
68924,Finding sum form for a particular recursive function,"Consider a finite sequence of zeros and ones of length $3n$, with $n$ an integer. We write an element of this sequence as $a_i$. How many sequences are there such that there exists an integer $k$, $0<k\le n$, such that $\sum^{3k}_{j=1}a_j=2k$? Here is what I have as of now: let $x_n$ be this number. We notice that $x_1=\binom{3}{2}=3$, and $x_n=\binom{3n}{2n}-\binom{3}{2}x_{n-1}+2^{3}x_{n-1}=\binom{3n}{2n}+5x_{n-1}$. How would I find a sum form solution to this? Also, does this seem correct? I got this because $\binom{3n}{2n}$ counts the total number of sequences satisfying the condition for $k=n$, $\binom{3}{2}x_{n-1}$ is the number of sequences satisfying it for both $k=n$ and $k=n-1$, and the number of sequences satisfying $k=n-1$ should be $x_{n-1}$, and we can choose the last three elements at random, so we multiply by $2^3$.","['sequences-and-series', 'combinatorics']"
68927,Self homeomorphisms are homotopic?,"Let $X$ be a path connected topological space. Is it always true that any two homeomorphisms $f$ and $g$ from $X$ to itself are homotopic? If not, is there a minimal condition on $X$ which guarantees that this will be the case for all possible $f$ and $g$? I know that this is true for a contractible space (indeed, and two maps on such a space are homotopic). However, if it is not true in general, I want to determine a necessary and sufficient condition for this property to hold.","['general-topology', 'algebraic-topology']"
68941,Terminology: A homomorphism factors,"I've never heard the term, a homomorphism ""factors"" before, and it's on my current assignment, so I was hoping someone could explain. The problem: Let $\pi:G\rightarrow G/G'$ be the canonical homomorphism and let $A$ be an abelian group. Show that every group homomorphism $\phi:G\rightarrow A$ factors as $\phi = \phi'\circ\pi$ where $\phi':G/G'\rightarrow A/A'$ is the induced group homomorphism. (Where $G'$ is the commutator subgroup of $G$.) So far, what I've poked around with... As $A$ is abelian, we note for any elements $a_1,a_2\in A$, $a_1^{-1}a_2^{-1}a_1a_2 = e_A$. So $A' = \{e_A\}$ and $A/A' \cong A$. Thus if we let $\phi:G\to A$ be a group homomorphism, for every $g,h\in G$, we must have $\phi(g)\phi(h) = \phi(gh)$. Now, as $A$ is abelian, we must also have $\phi(g)^{-1}\phi(h)^{-1}\phi(g)\phi(h) = e_A$. But using the fact that $\phi$ is a homomorphism again, we have $\phi(g^{-1}h^{-1}gh) = e_A$, and hence, every element of $G'$ is mapped by $\phi$ to $e_A$. Thus, if we have any element $g\in G$, $\phi'\circ\pi(g) = \phi'(gG') = \phi(g)A'$, which is simply $\{\phi(g)\}$. Is this what the question was asking for? Thanks! Edit: New version, As it's always a good idea, we start by showing the map $\phi':G/G'\to A/A'$ given by $\phi'(hG') = \phi(h)A'$ is well defined. For any $g,h\in G$ we note $\phi(g^{-1}h^{-1}gh) = \phi(g)^{-1}\phi(h)^{-1}\phi(g)\phi(h)\in A'$. So $\phi(G')\subset A'$. Now, if we have two elements $h_1,h_2\in G$ such that $[h_1] = [h_2]$, then by definition, we have $h_1 = h_2c$ for some $c\in G'$. So $\phi(h_1) = \phi(h_2c) = \phi(h_2)\phi(c)\in \phi(h_2)A'$. Hence $[\phi(h_1)] = [\phi(h_2)]$ in $A/A'$, and $\phi'$ is well defined. To see that $\phi'$ is indeed a group homomorphism, let $g,h\in G$. Then 
\begin{align*}
 \phi'([g][h]) &= \phi'([gh])\newline
               &= \phi(gh)A'\newline
               &= \phi(g)\phi(h)A'\newline
               &= (\phi(g)A')(\phi(h)A') = \phi'([g])\phi'([h]).
\end{align*}
As $A$ is abelian, we note for any elements $a_1,a_2\in A$, $a_1^{-1}a_2^{-1}a_1a_2 = e_A$. So $A' = \{e\}$ and $A/A' \cong A$. So although $\phi'$ maps to $A/A'$, we may simply say $\phi'$ maps to $A$ in the obvious way, so from here we say $\phi'([h]) = \phi(h)$ for any $h\in G$. Given this, for any element $g\in G$, we have $\phi'\circ\pi(g) = \phi'(gG') = \phi(g)$, and so any group homomorphism $\phi:G\to A$ factors as $\phi'\circ\pi$.","['group-theory', 'abstract-algebra']"
68946,Is every countable dense subset of $\mathbb R$ ambiently homeomorphic to $\mathbb Q$,"Let $S$ be a countable dense subset of $\mathbb R$. Must there exist a homeomorphism $f: \mathbb R \rightarrow \mathbb R$ such that $f(S) = \mathbb Q$? More weakly, must $S$ be homeomorphic to $\mathbb Q$?","['general-topology', 'real-analysis']"
68958,"Why is $(.5, 1]$ considered an open set in $[0, 1]$?","Why is $(.5, 1]$ considered an open set in $[0, 1]$? This is from a topology textbook.",['general-topology']
68964,2-transitive action on vector space of char. 2,"Suppose $V$ is an elementary abelian 2-group of order $2^k$, with $k>2$.  Let $H\le GL(k,2)$ be a solvable group of automorphisms of $V$.  How does one prove that $H$ cannot act 2-transitively on the non-zero vectors of $V$? It seems counterintuitive for such an action to exist, since a point stabilizer in $H$ would have an orbit of size $|V|-2$. And of course $GL(2,k)$ is simple in this case.  But I don't see how to proceed. I actually wonder if the solvability of $H$ is necessary; in particular: What would happen if we dropped the solvability hypothesis?","['finite-groups', 'group-theory']"
68971,"$A$ is skew hermitian, prove $e^A$ is unitary","Given $A$ is a skew-hermitian, ( i.e $A^H = -A$ ) then how do you prove the matrix exponential $e^A$ is unitary. To prove the unitary property of the matrix, I need to show $(e^A)^{*}(e^A) = (e^A)(e^A)^{*}= I$. Can any one help me how to proceed and prove the result?",['linear-algebra']
68977,A coordinate free proof of the identity $Tr(A \otimes  B)= Tr(A) \otimes Tr(B)$,"I was going over some of the definitions of trace and was trying to find a way to prove one of the facts stated about the trace listed in the referenced Wikipedia article . Let $M, N$ be finitely generated projective $R$-modules over a commutative ring $R$ with identity. In particular I was wondering if there was a slick proof using the coordinate free language of the trace for the following fact. How do we show that for any $f \in End_R(M), v \in End_R(N)$ then $ Tr( f \otimes_R g) = Tr(f) \otimes_R Tr(g)$?","['modules', 'linear-algebra', 'abstract-algebra']"
68988,A general formula for the $n$-th derivative of a parametrically defined function,"Letting $$\begin{align*}x&=f(t)\\y&=g(t)\end{align*}$$ the following expressions are well known: $$\begin{align*}\frac{\mathrm dy}{\mathrm dx}&=\frac{g^\prime (t)}{f^\prime (t)}\\\frac{\mathrm d^2 y}{\mathrm dx^2}&=\frac{f^\prime (t)g^{\prime\prime} (t)-g^\prime (t)f^{\prime\prime} (t)}{f^\prime (t)^3}\end{align*}$$ With some effort, we can derive the expression for the third derivative: $$\frac{\mathrm d^3 y}{\mathrm dx^3}=\frac{f'(t) \left(g^{(3)}(t) f'(t)-3 f''(t) g''(t)\right)+g'(t) \left(3 f''(t)^2-f^{(3)}(t)f'(t)\right)}{f'(t)^5}$$ After deriving expressions for the next higher derivatives, I am unable to detect any particular pattern in the expressions, save for the denominator $f'(t)^{2n-1}$ of the $n$-th derivative. I've also tried to search around for information on the derivatives of parametrically-defined functions, but no dice. Here then is my question: is there a general formula for $\dfrac{\mathrm d^n y}{\mathrm dx^n}$ in terms of $f(t),g(t)$ and their derivatives?","['calculus', 'reference-request']"
68995,Number of combinations with repetitions (Constrained),"I would like to calculate the number of integral solutions to the equation $$x_1 + x_2 + \cdots + x_n = k$$ where $$a_1 \le x_1 \le b_1, a_2 \le x_2 \le b_2, a_3 \le x_3 \le b_3$$ and so on. How do we approach problems with variables constrained on both sides $(a_1 \le x_1 \le b_1)$ or with constraints like $x_1 \le b_1$? I know that the same equation with constraints like $x_1 \ge a_1, x_2 \ge a_2$ and so on can be solved using a slight modification of the formula $\binom{n + k - 1}{ k}$. Is it possible to tweak the same formula to suit the given problem?","['discrete-mathematics', 'combinatorics']"
68999,Why is the determinant of the following matrix zero,"Let $a_1,\ldots,a_n$ be complex numbers and let $b_1,\ldots,b_n$ be complex numbers. Let $A$ be the matrix whose $(i,j)$-th entry is $A_{ij} = a_i b_j$. Then I think $\det A = 0$ when $n>1$. This is easy to compute when $n=2$. Question. Why is $\det A =0$?","['matrices', 'linear-algebra']"
69009,All positive integer solutions to $\frac{1}{x_1}+\frac{1}{x_2}+\cdots+\frac{1}{x_n}+\frac{1}{x_1 x_2 \cdots x_n}=1$,"As the title states, how would I go about finding the positive integer solutions of $$\frac{1}{x_1}+\frac{1}{x_2}+\cdots+\frac{1}{x_n}+\frac{1}{x_1 x_2 \cdots x_n}=1$$? Thank you for your help. Edit: I think I've made some progress. I conjecture that you can expand the set {a, a+1, a*(a+1)+1, a*(a+1) (a (a+1)+1)+1, ...} (i.e. multiply all previous solutions and add 1), where a=2, to size n, and that will always be a solution in the case of n variables. This is far from finding all positive integer solutions, though. Edit 2: For example {2}, {2,3}, {2,3,7}, {2,3,7,43}, {2,3,7,1807}, {2,3,7,43,1807,3263443} are all solutions in the case where $n=$ size of the solution set respectively.","['diophantine-equations', 'number-theory']"
69015,Exercise in Hartshorne,"I started reading Hartshorne. Already in the first exercises I stumble across problems. Basically excerise 1.1 ask to prove that $k[x,y]/(y-x^2)$ is isomorphic to a polynomial ring in one variable. Well ok, so I tried the following, define $k[x,y]\to k[t]$ by $x \mapsto t$
and $y \mapsto t^2$. This is obviously a homomorphism and also $y-x^2$ is in the kernel. My problem was to show that the kernel is nothing more than $(y-x^2)$ though this seems kind of clear but I had some trouble proving it rigorously.
I did the following Let $\sum_{i,j} a_{i,j}x^iy^j$ be in the kernel. Then the image is
$\sum_{i,j} a_{i,j}t^it^{2j}=0$. First note that $\sum_{i,j} a_{i,j}x^ix^{2j}$ is also in the kernel, since it has the same image as $\sum_{i,j} a_{i,j}x^iy^j$. But we also see that
$\sum_{i,j} a_{i,j}x^ix^{2j}$ is actually zero, since it is the same as $\sum_{i,j} a_{i,j}t^it^{2j}=0$ only with variables renamed. Thus
$$\sum_{i,j} a_{i,j}x^iy^j=\sum_{i,j} a_{i,j}x^iy^j-\sum_{i,j} a_{i,j}x^ix^{2j}=
\sum_{i,j} a_{i,j}x^i(y^j-x^{2j})$$, and it is well known that $a-b$ divides $a^j-b^j$.
Thus the sum is divisble by $y-x^2$ and we are done. Pretty complicated proof for something
that seems almost obvious. So my problem comes when doing exercise 1.2
I have to show that $k[x,y,z]/(x^2-y,x^3-z)$ is isomorphic to a polynomial ring in one variable.
So again I define a homomorphism $k[x,y,z]\to k[t]$ by $x \mapsto t$, $y \mapsto t^2$ and
$z\mapsto t^3$. The ideal $(x^2-y,x^3-z)$ is obviously contained in the kernel. But how do I show that it is all of that. Taking the same approach as above seems to lead to an even more complicated proof of an ""obvious fact"". So how can I prove 1.1 in a nicer technique that also applies to 1.2",['algebraic-geometry']
69024,High-dimensionality and intuition,"Over the years I've come across (usually as a tangential remark in a lecture) examples of how our intuitions (derived as they are from the experience of living in 3-dimensional space) will lead us badly astray when thinking about some $n$-dimensional Euclidean space, for some $n > 3$, especially if $n \gg 3$. Does anyone know of a compendium of these ""false intuitions"" (in high-dimensional Euclidean space)? Thanks! P.S. The motivation for this question is more than amusement.  In my line of work, the geometrization of a problem by mapping it onto some Euclidean $n$-space is often seen as a boon to intuition, even when $n$ is huge.  I suspect, however, that the net gain in intuition resulting from this maneuver may very well be negative!  In any case, it seems like a good idea to be conversant with those intuitions that should be disregarded.","['geometry', 'general-topology', 'reference-request', 'soft-question', 'euclidean-geometry']"
69027,Tricky contour integral resulting from the integration of $\sin ax / (x^2+b^2)$ over the positive halfline,"I am trying to evaluate the definite integral
$$\int_0^\infty \frac{\sin ax\ dx}{x^2+b^2}$$
where $a,b>0$. This is a problem on an assignment for a class in complex variables. I understand the mechanics of contour integration, but I am stuck. (I have spoken to four classmates who are also stuck.) I would appreciate a small hint, such as a hint to point me toward the right contour, or a suggestion for how to modify one of my abandoned ideas (below) to make it work. (But please do not work out the whole integral; I want to do that myself.) Here is what I have tried so far: The integral from $-\infty$ to $\infty$ is zero because the function is odd, so the semicircular contour in the upper half-plane won't do anything. Viewing the integral as 
$$
\mathrm{Im}\, \int_0^\infty \frac{e^{aiz}dz}{z^2+b^2},
$$ 
I tried integrating around the contour from $0$ to $R>0$, along the quarter-circle to $iR$, and back down to $0$, with an indentation at the pole at $bi$. The pole is simple so I can get the contribution from the indentation via the residue, and the contribution from the quarter-circle $\to 0$ as $R\to \infty$, but the problem is that I believe the integral along the segment of the imaginary axis from $iR$ to $0$ is imaginary (so contributes to the imaginary part) and blows up around the pole, and generally seems harder to deal with than the original integral. Viewing the integrand with either $\sin az$ or $e^{iaz}$ in the numerator, I tried integrating along the rectangle with vertices $0,R,R+ib,ib$, with an indentation at the vertex $ib$ due to the pole. Again, I don't know how to get control of the integral along the imaginary axis. I tried using integration by parts to get a cosine to come out, but the integrand remains odd so the upper half-plane semicircular contour still does no good. I had a crazy idea that I was unable to carry out. There exists some path thru the origin along which the integrand 
$$
\frac{e^{aiz}dz}{z^2+b^2}
$$ 
is pure real.  This path is the solution to an ordinary differential equation with boundary condition $y(0)=0$. If my calculations are right the equation is 
$$
y'=\frac{2xy\cos ax-(b^2+x^2-y^2)\sin ax}{2xy\sin ax+(b^2+x^2-y^2)\cos ax}.
$$
The idea was to integrate along $0$ to $R$, counterclockwise along the circle with radius $R$ till it hits this curve, and back to the origin along this curve. By construction, the integral along this last part doesn't contribute anything to the imaginary part; meanwhile the part of the circle in the upper half plane $\to 0$ as $R\to \infty$. The curve must lie entirely outside the upper half-plane or this would show that the original integral I'm trying to evaluate is zero (since the imaginary part of $2\pi i$ times the residue is zero), which isn't plausible. Evaluating the desired integral then rests on: (a) whether the pole in the lower half-plane ever gets enclosed by this contour (and I'm pretty sure it doesn't), and (b) if I can figure out what is going on with the integral along the part of the circle $|z|=R$ in the lower half-plane before it hits the curve; in particular, what its imaginary part is doing asymptotically as $R\to \infty$. However, I wasn't sure how to pursue these goals any further. Update: As it turns out, the assignment contained a typo and the integral was supposed to be
$$\int_0^\infty \frac{\sin ax\ dx}{x(x^2+b^2)}$$
This makes the integrand even, so it is done easily with the half-circle contour in the upper half-plane.  I definitely learned more because of the typo.  Thanks all for your answers and comments.","['integration', 'complex-analysis', 'contour-integration']"
69030,Confusion about a specific notation,"In the following symbolic mathematical statement $n \in \omega $, what does $\omega$ stand for? Does it have something to do with the continuum, or is it just another way to denote the set of natural numbers?","['notation', 'elementary-set-theory']"
69033,Help me understand limits,"Good day I'm currently doing some math homework (don't worry I won't ask anyone to solve anything) and I don't think I'm understanding limits correctly. More precisely how the l'Hôpital rule works. I know I can/should be able to apply it if it is either $\infty/\infty$ or $0/0$ but I was wondering does it have anything to do with $0/\infty$ or $\infty/0$? Anything you think might help me better understand limits would be appreciated. P.S.
I'm sorry if this is a simple question, math is not my strongest point.","['calculus', 'limits']"
69044,Inequalities involving the probability density function and variance,"I am wondering whether anyone knows of any any inequalities involving the probability density function of an unknown distribution (as opposed to the cumulative distribution function) and its known variance. (We also assume that the mean is zero.) Background: I have an upper bound on Kullback–Leibler divergence from $Z$ to $Y=X+Z$ $$D(p_Z\|p_Y)=\int_{-\infty}^{\infty}p_Z(x)\log\frac{p_Z(x)}{p_Y(x)}dx \leq\epsilon$$ where $Z\sim \mathcal{N}(0,\sigma^2)$.  $X$ has a symmetric density and mean zero, but is otherwise unknown, thus I am treating $Y$ as an unknown.  I am trying to bound the variance of $Y$ given $\epsilon$.  To that end I've considered applying the $\mathcal{L}_1$ distance between the two densities, as per this , half of its square bounds KL divergence from below: $$\frac{1}{2}\left(\int_{-\infty}^{\infty}|p_Z(x)-p_Y(x)|dx\right)^2 \leq D(p_Z\|p_Y)$$ As I understand, $\mathcal{L}_1$ norm relates to the total variation distance .  I do not completely understand why or how, and I posted in a separate question about that after helpful comments to this question.  But this question is about the need for the characterizing pdf with variance.","['probability-theory', 'information-theory', 'inequality']"
69050,Why the term and the concept of quotient group?,"The basic concept of Quotient Group is often a confusing thing for me,I mean can any one tell the intuitive concept and the necessity of the Quotient group,
I thought that it would be nice to ask as any basic undergraduate can learn the intuition seeing the question.
My Question is : Why is the name Quotient Group kept,normally in the case of division, let us take the example of $\large \frac{16}{4}$ the Quotient of the Division is '$4$' which means that there are four '$4$'s in $16$, I mean we can find only $4$ elements with value $4$ So how can we apply the same logic in the case of Quotient Groups,like consider the Group $A$ and normal subgroup $B$ of $A$, So if $A/B$ refers to ""Quotient group"", then does it mean: Are we finding how many copies of $B$ are present in $A$??, like in the case of normal division, or is it something different ?? I understood the Notion of Cosets and Quotient Groups,b ut I want a different Perspective to add Color to the concept. Can anyone tell me the necessity and background for the invention of Quotient Groups? Note: I tried my level best in formatting and typing with proper protocol,if in case, any errors still persist, I beg everyone to explain the reason of their downvote (if any), so that I can rectify myself, Thank you.","['quotient-group', 'group-theory', 'abstract-algebra']"
69061,Probability Constructions,"$y_n$ is a sequence of probability measures on $\mathbb{R}$ such that $y_n\rightarrow y$ where $y$ is another probability measure on $\mathbb{R}$. Construct an example where: $\int x \; dy_n$ exists for each $n$ and has a finite limit but $\int x \; dy$ is $+\infty$. $\int x \; dy_n$ exists for each $n$ and $\lim_{n \to \infty }\int x \; dy_n=+\infty$, but $\int x \; dy$ is finite.",['probability-theory']
69066,Tangential component of a Riemannian connection,"Let $f: M \to N$ be an immersion of a differentiable manifold $M$ into a Riemannian manifold $N$ . Assume that $M$ has the
Riemannian metric induced by $f$ .
Let $p \in M$ and let $U \subset М$ be a neighborhood of $p$ such that $f(U) \subset N$ is a submanifold of $M$ . Further, suppose that $X, Y$ are differentiable vector fields on $f(U)$ which extend to
differentiable vector fields $X^*, Y^*$ on an open set of $N$ . Define $$(\nabla_x Y)(p) =\text{ tangential component of }(\overline{\nabla}_{x^*} Y^*)(p),$$ where $\overline{\nabla}$ is the
Riemannian connection of $N$ . Prove that $\nabla$ is the Riemannian
connection of $M$ .","['submanifold', 'riemannian-geometry', 'connections', 'differential-geometry']"
69070,Locally free sheaves on locally ringed spaces,"One can define the notion of a locally free sheaf (of finite rank) on any locally ringed space. If you restrict to the category of (noetherian?) schemes, this category is equivalent to the category of vector bundles. (This is an exercise in Hartshorne.) If you restrict to the category of complex manifolds, this category is equivalent to the category of complex vector bundles. Can one unify these two observations? That is, can one describe the category of locally free sheaves on a locally ringed space $(X,\mathcal{O}_X)$ as a category of ""vector bundles""?","['manifolds', 'sheaf-theory', 'algebraic-geometry', 'differential-geometry']"
69075,Spectrum of a linear operator on a vector space of countable dim,"How would you prove that if V is a vector space over $\mathbb{C}$ of countably infinite dimension, and $T$ is a linear operator on V, then Spectrum($T$) is non-empty?",['linear-algebra']
69082,Help with commutative property of matrices problem,"Given matrices $A$ and $B$ , where that $AB = A + B$ , prove $AB = BA$ . I keep coming up with AB = AB. It seems like basic algebra, but for the life of me, I'm getting nowhere :/. Someone help please?","['matrices', 'linear-algebra']"
