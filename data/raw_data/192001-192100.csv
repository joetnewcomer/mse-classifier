question_id,title,body,tags
3650283,Double integral over odd region,"Let $(x,y)$ be Cartesian coordinates, let $\mathbf{i}$ and $\mathbf{j}$ be unit vectors in the $x$ - and $y$ -directions, let $(r,\theta)$ be polar coordinates, and let $\mathbf{e}_{\theta} = \left(-\sin\theta\right)\mathbf{i}+\left(\cos\theta\right)\mathbf{j}$ . Let $R$ be the region of points $(x,y)$ satisfying \begin{equation*}\left\lvert x\right\rvert^{\frac{1}{2}} + \left\lvert y\right\rvert^{\frac{1}{2}} \leq 1\end{equation*} I need to find the integral \begin{equation*}\iint\limits_{R}x\left(x^2+y^2\right)^{\frac{1}{2}}\mathbf{e}_{\theta} \mathop{}\!\mathrm{d}A\end{equation*} I tried using polar coordinates as suggested by the problem statement, but (for the $\mathbf{j}$ -component) I end up with the integral \begin{equation*}\int_{0}^{\frac{\pi}{2}}\frac{\cos^{2}\theta}{\left(\sqrt{\cos\theta}+\sqrt{\sin\theta}\right)^8}\end{equation*} which seems intractable. Any assistance would be much appreciated.","['multivariable-calculus', 'vector-analysis']"
3650286,"Differentiability properties of $\psi(x)\cos(\phi(x)),\,\, \psi(x)\sin(\phi(x))$ at $x=0$","Let $\psi:[0,\infty) \to [0,\infty)$ be a  smooth strictly increasing function  satisfying $\psi(0)=0$ , $\psi'(0)>0$ and let $\phi:(0,\infty) \to \mathbb R$ be smooth. Suppose that $\lim_{x \to 0^+}\phi'(x)\psi(x)=0$ . Define $$f_1(x)=\psi(x)\cos(\phi(x)), \, \,\,\,f_2(x)=\psi(x)\sin(\phi(x))$$ on $(0,\infty)$ , and extend them continuously to zero by setting $f_i(x)=0$ . Can the following properties hold simultaneously? $\,f_i$ are infinitely (right) differentiable at $x=0$ . $\,$ All the (right) derivatives of $f_i$ of even order vanish at zero. At least one of the $\,f_i'(0)$ is non-zero. Comment: The assumptions imply that $\alpha:=\lim_{x \to 0^+} \phi(x)$ exists. Indeed, $$ \frac{f_1(x)-f_1(0)}{x}=\frac{\psi(x)-\psi(0)}{x}\cos(\phi(x))\Rightarrow \\ \cos(\phi(x))=\frac{f_1(x)-f_1(0)}{x} \frac{1}{\frac{\psi(x)-\psi(0)}{x}} \Rightarrow \\ 
\lim_{x \to 0^+} \cos(\phi(x))=\frac{f_1'(0)}{\psi'(0)}, $$ and similarly $\lim_{x \to 0^+} \sin(\phi(x))=\frac{f_2'(0)}{\psi'(0)}$ . So, both $\lim_{x \to 0^+} \cos(\phi(x)), \lim_{x \to 0^+} \sin(\phi(x))$ exist, and hence so does $\lim_{x \to 0^+} \phi(x)$ . Now, a direct calculation shows that $$ f_1'(x)=\begin{cases} \psi'(x)\cos(\phi(x))-\psi(x)\phi'(x)\sin(\phi(x)) & \text{if $x > 0$} \\ \psi'(0)\cdot  \cos(\alpha)  & \text{if $x=0$}\end{cases}$$ Now, I am not sure how to proceed from here. for $x>0$ , we have $$  f_1''(x)=\psi''(x)\cos(\phi(x))-2\psi'(x)\phi'(x)\sin(\phi(x))-\psi(x)\phi''(x)\sin(\phi(x))-\psi(x)(\phi'(x))^2\cos(\phi(x)),$$ but since we don't know whether $\phi'(x),\phi''(x)$ have limits when $x \to 0$ , it is not clear to me what to do next.","['singularity', 'real-analysis', 'calculus', 'limits', 'derivatives']"
3650289,"Trace norm of rank one operator $x\otimes y$ for $x,y\in H$","Let $H$ be a Hilbert space. The trace norm on $B(H)$ is defined as $$\|u\|_{1}:=\operatorname{tr}(|u|):=\sum_{e\in E}\langle|u|(e),e\rangle,$$ where $|u|:=(u^{*}u)^{1/2}$ and $E$ is (any) orthonormal basis for $H$ . This may be $+\infty$ . It can be shown that this definition is independent of $E$ . For $x,y\in H$ we have a rank one operator $x\otimes y\colon H\to H$ defined by $(x\otimes y)(h):=\langle h, y\rangle x$ . I want to show that $\|x\otimes y\|_{1}=\|x\|\|y\|$ . I have computed $$(x\otimes y)^{*}(x\otimes y)=\|x\|^{2}(y\otimes y).$$ So if $y\neq0$ , then $$(x\otimes y)^{*}(x\otimes y)=\|x\|^{2}\|y\|^{2}(u\otimes u),\quad\text{where}\quad u:=y/\|y\|.$$ But how do I proceed? I think that I have to pick a specific orthonormal basis $E$ , but I don't see how. Any help would be greatly appreciated! Thanks in advance!","['orthonormal', 'operator-algebras', 'operator-theory', 'trace', 'functional-analysis']"
3650380,"If $\|\langle x_1, x_2, \dotsc, x_n\rangle\|\le \delta$, can I say that $x_i \le \delta$ for each $i$?","If I have a vector $x \in \mathbb{R}^n$ that is in a neighborhood of $0$ that is large $\delta$ , i.e. $\left \| x \right \| \le \delta$ can I say that all of its components must be $x_i \le \delta$ for each $i =1,\dots,n$ ?","['multivariable-calculus', 'calculus', 'linear-algebra']"
3650426,Combinatorial proof that $\sum_0^n {n+k \choose n}{2n-k-1 \choose n-1} = {3n \choose n}$,"I am trying to derive combinatorial proof of the following: $$\sum_{k=0}^n {n+k \choose n}{2n-k-1 \choose n-1} = {3n \choose n}$$ I attempted the construction of argument of type ""split the $3n$ elements into $n$ sections of $3$ elements each"", and count how many ways it can be made, but I do not reach the desired statement. So I am not sure it is the right approach. I also fail to reach the conclusion using a grid path approach. Any help is appreciated!","['summation', 'combinatorial-proofs', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
3650551,Why does this sequence of random variables almost surely converge?,"Let $(\Omega,\mathcal A,\operatorname P)$ be a probability space and $\tau$ be a measurable map on $(\Omega,\mathcal A,\operatorname P)$ with $\operatorname P\circ\:\tau^{-1}=\operatorname P$ . $(Y_n)_{n\in\mathbb N}\subseteq L^1(\operatorname P)$ is called subadditive if $$Y_{m+n}\le Y_m+Y_n\circ\tau^m\;\;\;\text{for all }m,n\in\mathbb N\tag1$$ and additive if equality holds. Assume $(Y_n)_{n\in\mathbb N}$ is subadditive and let $$S_n:=\sum_{i=0}^{n-1}Y_1\circ\tau^i\;\;\;\text{for }n\in\mathbb N,$$ which is easily seen to be additive. Let $$\tilde Y_n:=Y_n-S_n\le0\;\;\;\text{for }n\in\mathbb N,$$ which is additive and nonpositive. Let $$\mathcal I:=\{A\in\mathcal A:\tau^{-1}(A)=A\}.$$ By Fekete's lemma , $$\operatorname E\left[\frac{\tilde Y_n}n\mid\mathcal I\right]\xrightarrow{n\to\infty}\inf_{n\in\mathbb N}\operatorname E\left[\frac{\tilde Y_n}n\mid\mathcal I\right]\tag2.$$ By Birkhoff's ergodic theorem , $$S_n\xrightarrow{n\to\infty}\operatorname E\left[Y_1\mid\mathcal I\right]\;\;\;\text{almost surely}\tag3.$$ Why are we able to conclude that $\lim_{n\to\infty}n^{-1}Y_n$ exists almost surely?","['measure-theory', 'ergodic-theory', 'stochastic-processes', 'probability-theory', 'dynamical-systems']"
3650601,prove $Ax_n - \lambda x_n \to 0$,"Given a Hilbert space $H$ over $\mathbb{C}$ and a linear self-adjoint operator $A: H \to H$ . Let $(x_n) \subset H$ with $\|x_n \|= 1$ such that $\langle Ax_n,x_n \rangle \to \lambda$ where $|\lambda| = \| A \|$ . I need to prove $Ax_n - \lambda x_n \to 0$ I started by $|\lambda| = \| A \|=\sup_{\|x\|=1}\langle Ax,x \rangle \geq \langle Ax_n, x_n \rangle = \langle x_n,Ax_n \rangle$ but not sure how it helps. How can we prove this?","['operator-theory', 'functional-analysis', 'self-adjoint-operators']"
3650650,Why do we need to determine the definiteness of the Hessian to decide what a critical point is?,"In univariate calculus, if we know that $f'(c)=0$ , we can determine if the function $f$ has a minimum at $c$ by checking that $f''(c) > 0$ . The multivariate analogue of the second derivative is the Hessian matrix. I now learned that to decide between extreme and saddle points in this case, it has to be checked whether the Hessian is positive definite, negative definite or indefinite. This can be achieved by checking its eigenvalues. I have several questions regarding this: Why is it not sufficient to check the sign of the values in the Hessian, but we need to check for definiteness? Does the definiteness just make sure some convexity or concavity properties check out, or is there a more meaningful interpretation of that? How do the eigenvalues of a matrix tell us its definiteness? Addendum: What do the off-diagonal entries in the Hessian even mean? How the slope in a certain dimension changes by making changes in a different dimension?","['maxima-minima', 'multivariable-calculus', 'calculus', 'optimization', 'hessian-matrix']"
3650652,Is an infinite direct product or sum of non-trivial modules not finitely generated?,"Based upon these two older questions: Show a direct product is not finitely generated. and $R^\mathbb{N}$ is not finitely generated as an $R$-module , I would like to know the answer to the following questions. Given an infinite family $\{M_i\}_{i \in I}$ of non-trivial left $R$ -modules (where $R$ is a ring), can $\prod M_i$ be a finitely generated $R$ -module ? Given an infinite family $\{M_i\}_{i \in I}$ of non-trivial left $R$ -modules (where $R$ is a ring), can $\bigoplus M_i$ be a finitely generated $R$ -module ? It is suggested in the second link to prove that $\prod_{i \in P_n} M_i$ ( $P_n$ is a subset of $I$ of cardinality $n$ ) requires at least $n$ generators.","['direct-product', 'abstract-algebra', 'direct-sum', 'modules']"
3650763,Find $\lim_{k \to \infty}\int_{0}^{\infty}ke^{-kx^2}\arctan(x)dx$,"Find $\lim_{k \to \infty}\int_{0}^{\infty}ke^{-kx^2}\arctan(x)dx$ . I think that the limit is infinity. $ke^{-kx^2}\leq ke^{-kx^2}\arctan(x)$ for $[\tan(1),\infty)$ , but by integrating we know that $\int_{0}^{\infty}ke^{-kx^2}\to \infty$ and so out sequence of original integrals diveres too. Is this correct?","['lebesgue-measure', 'lebesgue-integral', 'real-analysis']"
3650852,find $\lim_{n\to\infty}\int^n_0t^{x-1}(1-t/n)^ndt$,"I am asked to find with the help of this theorem/property the limit: $$\lim_{n\to\infty}\int^n_0t^{x-1}(1-t/n)^ndt$$ My attempt: For my $f$ I will use $t^{x-1}$ since I clearly see that $f_n=t^{x-1}(1-t/n)^n \to f$ uniformly. But how does the integral $\int^{\infty}_0t^{x-1}dt$ converge? Am I doing something wrong? Also, how do I pick a function that is greater than $f_n$ and for which I can show converges? I was thinking of picking $t^x$ for the function $g(x)$ but then $t^x$ is not finite. Any help is appreciated","['integration', 'limits', 'real-analysis']"
3650863,"Exponential bound for tail of exit time from [-b,b] of martingale","The problem: Let $(X_n, \mathcal{F}_n)$ be a martingale such that $|X_n-X_{n-1}| \leq c$ , $\mathbb{E}(|X_n-X_{n-1}|^2|\mathcal{F}_{n-1}) \geq \delta > 0$ , $\sup X_n = -\inf X_n = \infty$ and $X_0 = 0$ . Let $b>0$ and define the stopping time $\tau = \inf\{n : X_n \not\in (-b,b)\}$ , we want to show that exists $C = C(\delta, c)$ such that for all $b\geq 10c$ , $$\forall k\in\mathbb{N}: \mathbb{P}(\tau \geq kCb^2) \leq e^{-k}.$$ My partial solution: Let $Y_n = X^2_{n} - \delta n$ , then $Y_n$ is a submartingale and so is $Y_{n\wedge \tau}$ . It is true that $\sup \mathbb{E}Y_{n\wedge\tau}^+ \leq \sup \mathbb{E}X_{n\wedge\tau}^2 \leq (b+c)^2$ , then $Y_{n\wedge \tau} \to Y_\tau$ a.s and $EY_\tau < \infty$ and by the dominated convergence theorem \begin{align}
    \mathbb{E}( X_{n\wedge \tau}^2 - (n\wedge \tau)\delta   ) \geq 0 \Rightarrow \mathbb{E}( X_\tau^2)  \geq \delta  \mathbb{E}(\tau)
\end{align} Using Markov: \begin{equation}
    \mathbb{P}( \tau \geq k C b^2 ) \leq \frac{\mathbb{E}\tau}{ kCb^2 } \leq \frac{\mathbb{E}X_\tau^2}{ \delta kCb^2 } \leq \frac{(b+c)^2}{ \delta kCb^2 } \approx \frac{1}{k}.
\end{equation} Giving a bound of order $\frac{1}{k}$ , but far way from the goal that is a bound of order $e^{-k}$ , how to improve it?","['random-walk', 'martingales', 'inequality', 'stopping-times', 'probability-theory']"
3650883,linear independence of vector functions,would the below vector functions be linearly independent? Even though the wronskian is equal to 0? $$\begin{bmatrix}0 & 0 & 0\\ 1 & t & t\\ 2t^2 & 2t^3 & 3t^2\\ \end{bmatrix}$$,"['linear-algebra', 'ordinary-differential-equations']"
3650940,Periodic sequence problem,"Given sequence $a_n$ defined such that $a_1=3$ , $a_{n+1}=\begin{cases}\frac{a_n}{2},\quad 2\mid a_n\\ \frac{a_n+1983}{2},\quad 2\nmid a_n\end{cases}$ . Then prove that the sequence $a_n$ is periodic and find the period. It's easy to prove that $0<a_n<1983$ by induction. By pigeonhole principle, there exist $i,j$ such that $a_i=a_j\implies a_{i+1}=a_{j+1}$ . By induction, we can prove $a_{i+k}=a_{j+k},\forall k\in\mathbb{N}$ . Otherwise, $a_n\begin{cases}2a_{n+1}, \quad a_{n+1}\le 991\\ 2a_{n+1}-1983, \quad a_{n+1}\ge 992\end{cases}$ . So we can prove also $a_{i-k}=a_{j-k} $ for $min(i,j)>k, \forall k\in\mathbb{N}$ . So it's periodic. But I can't find the period. In my opinion, the period is $660$ . Because $3\mid a_n$ and $0<a_n<1983$ . But I can't prove $\forall k, \exists i$ such that $a_i=3k$ , Can anyone help me?",['sequences-and-series']
3651023,maximum inscribed angle within an ellipse,"Consider the figure of an ellipse below: where the coordinate pair $ (x_e , y_e) $ denote a point on the positive half of the ellipse, the red stars denote the foci of the ellipse (at $x = \pm c$ ) , and $d$ denotes a distance centered on the foci as shown. If we draw two lines which connect the points $(-c - \frac{d}{2}, 0)$ and $(-c+\frac{d}{2},0)$ to the point $(x_e,y_e) $ these lines will form an angle $\theta$ between them. The question is: For a given d, for what pair $(x_e, y_e)$ does the angle $\theta$ maximize? My work so far I use the law of cosines to find $\theta$ : $$ d^2 = s_1^2 + s_2^2 - 2 \cdot s_1 \cdot s_2 \cdot cos(\theta)$$ where $$ s_1 = \sqrt{(x_e - (-c + \frac{d}{2}))^2 + y_e^2} $$ and $$s_2 = \sqrt{(x_e - (-c - \frac{d}{2}))^2 + y_e^2}$$ so $$cos(\theta) = \frac{s_1^2 + s_2^2 - d^2}{2 \cdot s_1 \cdot s_2}$$ I get that $s_1^2 + s_2^2 -d^2$ becomes: $$2\cdot \left((x_e + c)^2 + y_e^2 - (\frac{d}{2})^2 \right)$$ and the full expression becomes: $$cos(\theta) = \frac{(x_e + c)^2 + y_e^2 - (\frac{d}{2})^2}{\sqrt{\left((x_e + c - \frac{d}{2})^2 + y_e^2\right)\left((x_e + c + \frac{d}{2})^2 + y_e^2\right)}}$$ because $y_e = b \cdot \sqrt{1 - (\frac{x_e}{a})^2} $ it means that (for a constant $d$ ) $cos(\theta)$ is a function of only $x_e$ so finally I got: $$\theta(x_e) = \cos^{-1}\left(\frac{(x_e + c)^2 + (b^2 \cdot (1 - (\frac{x_e}{a})^2) - (\frac{d}{2})^2}{\sqrt{\left((x_e + c - \frac{d}{2})^2 + (b^2 \cdot (1 - (\frac{x_e}{a})^2)\right)\left((x_e + c + \frac{d}{2})^2 + (b^2 \cdot (1 - (\frac{x_e}{a})^2)\right)}}\right)$$ $x_e$ goes from $[-a,a]$ and I think that $d \le 2\cdot (a - c)$ so that $-c - \frac{d}{2}$ lies within [-a,a] Going back to the question posed, I have two sub-questions: 1) Is this the correct equation for $\theta (x_e)$ ? 2) If this equation is correct, would I just have to set the derivative equal to $0$ to find the critical points (i.e. $x_e$ which gives the maximum $\theta$ ) ? Extra plot: I plotted the above function $\theta(x)$ (with the given constants for a, b, c, and d and $\theta(x)$ is given in degrees) in python, where the red vertical lines show the x values of the foci ( $x =  \pm c$ ), and I got this: what prompted me to ask this question is that it seems the maximum inscribed angle occurs at an x position which is not the focus $ x = - c$ , even though intuitively, I would have thought it would have been at that position. Did I just mess up in the derivation of $\theta(x)$ ?","['angle', 'conic-sections', 'geometry']"
3651067,Proving $\int_0^{\infty} \frac{\sin^3(x)}{x^2} dx = \frac{3\ln(3)}{4} $,"So this integral reminds me of the Dirichlet integral but I am not sure if I can use similar methods to solve it. I want to prove $$\int_0^{\infty} \frac{\sin^3(x)}{x^2} dx = \frac{3\ln(3)}{4} $$ I tried parameterizing with $$ I(a) := \int_0^{\infty} \sin(ax)\frac{\sin^2(x)}{x^2}dx$$ or $$ I(a) := \int_0^{\infty} \frac{\sin^3(x)}{x^2}e^{-ax}dx$$ But none of them worked out for me. Not sure what to do. I would really like to use real methods and not complex analysis, since I haven’t learned it yet.","['integration', 'calculus', 'definite-integrals', 'real-analysis']"
3651070,Help solving $y^{\prime\prime}(1+2\ln(y^\prime)) = 1$,"Let $p = y^\prime$ , then we get $p^\prime(1+2\ln(p))=1$ , so $x + c_1 = \int(1+2\ln(p))dp = p(2\ln(p)-1)$ . But then I'm stuck because I don't know what to do next.",['ordinary-differential-equations']
3651156,Calculating the size of a set.,"If there was a set i.e. $S = \{2, 4, 6\}$ I understand that the size of the set would be $3$ . But what would be the size if there was a set within a set and another within? For example: $X = \{2,4,6,\{8\}\}$ and $Y = \{2,4,6,\{8\}\,\{\{10\}\}, 12\}$",['elementary-set-theory']
3651171,Prove by definition that $ \int_a^b fdf = {f^2(b)-f^2(a) \over{2}}$ when $f$ is continuous,"Let $f :[a,b] \rightarrow \mathbb{R}$ be a continuous function. Prove that $f$ is Riemann Stieltjes integral with respect to itself that is: $f\in RS_a^b(f)$ by definition and $ \int_a^b fdf = {f^2(b)-f^2(a) \over{2}}$ I can't use the Cauchy criterion nor integration by parts to solve this problem: My definition: Let $f,g:[a,b]\rightarrow \mathbb{R}$ bounded functions. $f$ is Riemann Stieltjes integrable with respect to $g$ iff there exists a real number $I$ such that $\forall \epsilon > 0$ there exists a partition $P_{\epsilon}$ in $[a,b]$ such that for every other partion $P$ finer than $P_\epsilon$ , $|S(P,f,g)-I|<\epsilon$ for every choice of numbers $c_i \in [x_{i-1},x_i]$ where $S(P,f,g)=\sum_{i=1}^nf(c_i)(g(x_i)-g(x_{i-1}))$ . In this case we define $\int_a^bfdg = I$ My attempt: Let $\epsilon > 0$ . By uniform continuity of $f$ (because $f$ is continuous on a compact set) there exists $\delta_\epsilon > 0$ such that for every $x,y \in [a,b]$ and $|x-y| < \delta$ then $|f(x)-f(y)|< \epsilon$ . We can construct a partition $P_{\delta_\epsilon}$ so that $||P_{\delta_\epsilon}|| < \delta_\epsilon$ . Take any other partition $P$ finer thatn $P_{\delta_\epsilon}$ and any choice of numbers $c_i\in [x_{i-1},x_i]$ subinterval of the partition $P$ ; then: \begin{align}
|S(P,f,f)-{f^2(b)-f^2(a) \over{2}}| &= \frac{1}{2}|2 \sum_{i=1}^nf(c_i)(f(x_i)-f(x_{i-1}))-f^2(b)+f^2(a)| \\
&= \frac{1}{2}|\sum_{i=1}^nf(c_i)(f(x_i)-f(x_{i-1})) +\sum_{i=1}^nf(c_i)(f(x_i)-f(x_{i-1})) -f^2(b)+f^2(a)| \\ 
&= \frac{1}{2}|\sum_{i=1}^n(f(c_i)-f(x_{i-1})+f(x_{i-1}))(f(x_i)-f(x_{i-1})) +\sum_{i=1}^n(f(c_i)-f(x_i)+f(x_i))(f(x_i)-f(x_{i-1})) -f^2(b)+f^2(a)| \\ 
&=\frac{1}{2}|\sum_{i=1}^n(f(c_i)-f(x_{i-1}))(f(x_i)-f(x_{i-1})) +\sum_{i=1}^n(f(c_i)-f(x_i))(f(x_i)-f(x_{i-1})) +\sum_{i=1}^nf(x_{i-1})(f(x_{i})-f(x_{i-1})) +\sum_{i=1}^nf(x_{i})(f(x_{i})-f(x_{i-1}))-f^2(b)+f^2(a)| \\ 
&=\frac{1}{2}|\sum_{i=1}^n(f(c_i)-f(x_{i-1}))(f(x_i)-f(x_{i-1})) +\sum_{i=1}^n(f(c_i)-f(x_i))(f(x_i)-f(x_{i-1}))| \\ 
&<\frac{1}{2}(\sum_{i=1}^n\epsilon^2 +\sum_{i=1}^n\epsilon^2) \\
&= \epsilon^2(n)
\end{align} The problem is that the last part depends on $n$ so I can´t conclude that this is less than $\epsilon$ because $n$ depends on the partion $P$ . But I don´t know how to solve this part. I would really appreciate any hints or suggestions with this problem.","['integration', 'stieltjes-integral', 'real-analysis', 'continuity', 'riemann-integration']"
3651213,Experiences with Folland's Real Analysis Textbook,I am thinking of self studying the first six chapters of Folland's Real Analysis: Modern techniques and Their Applications . I had read the first six chapters of Baby Rudin in the first real analysis course I had taken and would love to hear what people think of Folland's book for a second real analysis course. Has anyone read this book as an undergraduate? Is it too challenging for an undergraduate student? Bonus: Does anyone have any other suggestions for a different textbook that can be used in a  second semester of real analysis? I've read about Spivak's book but I don't think a physics-based analysis course is relevant to me (I want to pursue graduate-level statistics in a couple years). Edit: I would love to learn some measure theory.,"['book-recommendation', 'real-analysis']"
3651219,"Given $ I_n = \int_0^1 \frac{(x^2 + x + 1)^n - x}{x^2 + 1} dx$ show that we have $I_{4n+1} \in \mathbb{Q}$, for any $n \in \mathbb{N}$.","Consider the integral: $$I_n = \int_0^1 \frac{(x^2+x+1)^n - x}{x^2 + 1} dx$$ I have to show $$I_{4n+1} \in \mathbb{Q}$$ for any $n \in \mathbb{N}$ . I wasn't able to see any ""direct"" way of doing this, so I thought about induction. However, I couldn't find a way of expressing $I_{4n+1}$ in terms of $I_{4n-3}$ in order to use the induction step, so maybe induction is not the way either. So how should I approach this?","['integration', 'calculus', 'sequences-and-series']"
3651227,Is there any significance to the product $\prod_\limits{n=1}^{\infty} \left(1+\frac{1}{n^x}\right)$?,"Is there any significance to this product? $$\prod\limits_{n=1}^{\infty} \left(1+\frac{1}{n^x}\right)$$ Basically taking the Riemann zeta function and trying to make it into a convergent product, because if you have a convergent sum then you can take the infinite product of 1+the sum and it will also converge.","['complex-analysis', 'riemann-zeta', 'infinite-product']"
3651240,Algebraic Topology Book for the Analyst,"I'm looking for a graduate level book on topology that takes most of its motivation from analysis and applied mathematics. I'm currently in an algebraic topology course but other than basic definitions and intuition I have learned absolutely nothing about algebraic topology! Now I need to go and relearn most of the topic but it's quite challenging because I find most books on topology unmotivated and uninteresting. Most examples in my class are showing that one sphere is not a different sphere or otherwise use a collection of classic topological objects which I take very little interest in. I am not trying to diss algebraic topology in any way, but what are some books on topology that stress spaces that are of greater interest to problems in analysis and probability theory? Ghrist's book ""Elementary Applied Topology"" looks good, but too cursory for what I'm after. And references on topological data analysis use persistent homology and other topics that are currently above my head. Some books that I'm aware of but have not read that seem like they may be good are: Lee ""Introduction to Topological Manifolds"", Dold ""Lectures in Algebraic Topology"", Rotman ""An Introduction to Algebraic Topology"", Edelsbrunner ""Computational Topology"", and Kaczynski ""Computational Homology"" If one of the above texts stands out as a good candidate for what I'm interested in, please let me know (It's impossible to read all of them before deciding). Books that I have read parts of and dislike include: Bredon, Massey, Hatcher, and May.","['book-recommendation', 'algebraic-topology', 'analysis', 'reference-request']"
3651246,Definition of polycyclic groups,"I'm trying to understand the definition of polycyclic groups . A solvable group $G$ has two equivalent definitions: $G$ has a subnormal series like $$G = H_n \rhd H_{n-1} \rhd \cdots \rhd H_0 = 1$$ s.t. each $H_{i-1}$ is normal in $H_i$ and $H_{i}/H_{i-1}$ is an abelian group for all $i \in \{1, \ldots, n\}$ . $G$ has a normal series like $$G = H_n \rhd H_{n-1} \rhd \cdots \rhd H_0 = 1$$ s.t. each $H_i$ is normal in $G$ and $H_{i}/H_{i-1}$ is an abelian group for all $i \in \{1, \ldots, n\}$ . Now Wikipedia says a polycyclic group is a solvable group in which the factors $H_{i}/G_{i-1}$ are cyclic but there is no requirement that each $H_i$ be normal in $G$ : In another direction, a polycyclic group must have a normal series with each quotient cyclic, but there is no requirement that each $H_{i}$ be normal in $G$ . As every finite solvable group is polycyclic, this can be seen as one of the key differences between the definitions. I don't understand this. If each $H_i$ is not normal in $G$ then the group $G$ doesn't even satisfy the definition of solvable groups. Furthermore, in a normal series , each $H_i$ is normal in $G$ by definition (cf. this )! Could someone please explain what I'm missing here?","['group-theory', 'abstract-algebra', 'definition', 'solvable-groups']"
3651268,Proving Lipschitz gradient of $f(x)=\sqrt{1+\|x\|^2}$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Given $f(x)=\sqrt{1+\|x\|^2}$ and $f:\mathbb{R}^n\to\mathbb{R}$ . Prove $f\in C_1^{1,1}$ , meaning $$\|\nabla f(x)-\nabla f(y)\|\leq\|x-y\|.$$ I've tried to see how to prove it for $n=2$ but got stuck at computing the norm of the hessian. I know if I can prove $\|\nabla^2f(x)\|_2\leq 1$ will be enough.
I know that $\nabla f(x)=\frac{x}{\sqrt{1+\|x\|^2}}$ i thought maybe i can assume w.l.o.g that $\|x\|\geq \|y\|$ to prove this.","['multivariable-calculus', 'lipschitz-functions']"
3651442,Pointwise convergence of holomorphic functions on a dense set,"Let $G$ be an open connected set and let $D \subset G$ be a dense set. Let $(f_n)$ be a sequence of holomorphic functions in $G$ and assume $f_n \rightarrow 0$ pointwisely on $D$ . Can we deduce that $f_n$ converges pointwisely to a function $f$ ? If this is true, by Osgood theorem ( Convergence of a sequence holomorphic functions ) $f_n$ converges uniformly in a dense open set $D'$ of $G$ , so we deduce $f$ is holomorphic in $D'$ . Since $f\mid_D = 0$ it will follow that $f\mid_{D'} = 0.$ Now, assume we do not know that $f_n$ converges pointwisely but instead $f_n$ is locally bounded. By Vitali-Porter theorem ( https://mathoverflow.net/questions/82787/vitalis-theorem-on-convergence-of-holomorphic-functions ) $f_n$ converges uniformly on compacts subsets of $G$ to an analytic function, but it only 'needs' $D$ to have an accumulation point. My question is the following: could we deduce something like Vitali or Osgood theorem using only pointwise convergence on a dense set $D$ ? Thank you very much!","['complex-analysis', 'pointwise-convergence', 'analytic-functions']"
3651476,Why would the function $f(x)= \frac{1}{x^5(\exp(\frac{1}{x^5})-1)}$ not be continous?,"I was graphing the function $$f(x)=\frac{1}{x^5(\exp(\frac{1}{x^5})-1)}$$ and I noticed that somewhere around $x=1124.925$ and $x=1124.926$ the function stopped being continuous. I was initially graphing it with Desmos but repeating it with other graphing software (GeoGebra) and the calculator in my phone, (FSC) since I was finding an error of the software easier to believe, yielded the same results. Right around that same value, all of those jumped from $f_{(1124.925)}\approx 0.83$ to $f_{(1124.926)}\approx 1.25$ . Now I have no idea if there is some kind of problem in my maths or if it has to do with how computers calculate things. If there is no problem in the software, why is this happening, and why is this function not continuous? ( Here is the Desmos link )","['continuity', 'functions', 'graphing-functions']"
3651513,"The outside of a $180$-sheet roll of toilet paper is covered by two sheets; the inner cylinder, by one. What's wrong with how I counted the layers?","Puzzle: A roll of toilet paper has 180 sheets on it. The outside is covered with exactly two sheets. The inside around the cardboard cylinder is covered by exactly one. Question of the puzzle: how many layers of toilet paper are on the roll of toilet paper? The given solution: One way to solve this is by saying that the average round is covered by 1.5 sheets, so therefore the answer is $180\times\frac{2}{3}=120$ I tried a similar (but wrong) reasoning: ""the average sheet makes an average of $\frac{3}{4}$ rounds (first sheet makes one round and the last sheet makes $\frac{1}{2}$ rounds), so the answer is $180\times\frac{3}{4}=$ 135"" QUESTION: Apparently my answer is wrong. But since it seems analogical to the given solution I don't understand what error I made. Possibly the growth of sheets per round is constant? While the (negative) growth of rounds per sheet is not constant? What are the related functions? Put in another way: if $\frac{dSheets}{dRounds}=Constant$ isn't also $\frac{dRounds}{dSheets}=Constant$ ? This question is linked to this question: Using differential equations to determine the number of rolls on a roll of toilet paper","['calculus', 'puzzle']"
3651544,"Is $f(x)=\begin{cases} \frac{1}{n} & x\in(\frac{1}{n+1},\frac{1}{n}]\\ 0 & x=0 \end{cases}$ differentiable at 0?","I have the function $f:\mathbb{[0,1]\to R}$ defined by $$f(x)=\begin{cases} \frac{1}{n} & x\in(\frac{1}{n+1},\frac{1}{n}]\\ 0 & x=0 \end{cases}$$ and I must show whether or not $f$ is differentiable at $0$ . My first idea was to see if it was discontinuous at $0$ , however I found that it was in fact continuous, so now I know that $\underset{x\to 0}{\lim} f(x)=0$ but this doesn't help me to see if it is differentiable at $0$ . I know I must show whether the limit $$\underset{x\to 0}{\lim} \frac{f(x)}{x}$$ exists but I have no idea where to begin with this as my intuition would be to check both sides of the limit, however, $f$ is not well defined for $x<0$ so I can't do that. Is there something obvious I'm missing? This has been bugging me for a while now.","['limits', 'derivatives', 'analysis', 'real-analysis']"
3651553,What is internal direct sum or internal direct product in Dummit and Foote?,"I refer to Dummit and Foote Chapter 10.3 specifically pages 351 , 353 , 354 , 356 and 357 . Does Exercise 10.3.21 on pages 357 (By the way, there's some errata here. Condition (iii) should be $i_1,...,i_k$ ) define a notion of internal direct sum (of unital $R$ -submodules of a unital $R$ -module over a unital, but not necessarily commutative, ring $R$ )? I think this is an internal direct sum for an infinite or a finite index set that generalises the notion of internal direct sum for a finite index set given in page 354 . Do we have a notion of ' internal direct product '? For the finite case, I believe this is the ' $N_1 + ... + N_k$ ' part of Proposition 10.5 in page 353 . For the finite or infinite case, I believe this is the 'the (unital $R$ -)submodule of $M$ generated by (the union of) all the $N_i$ 's' part of Condition (i) of Exercise 10.3.21 because ' $N_1 + ... + N_k$ ' in Proposition 10.5 is actually equal to (see page 351 ) the (unital $R$ -) 'submodule of $M$ generated by (the union of) all the $N_i$ 's' such that Condition (i) generalises the '(1)' in Proposition 10.5. Therefore : I think of internal direct product of $N_i$ 's of $M$ as $\sum_{i \in I} N_i = R\{\bigcup_{i \in I} N_i\}$ , which like external direct product and external direct sum, is always defined. And then I think of internal direct sum as not always defined but, whenever defined, as equal to internal direct product. Possibly relevant: 'Semidirect product'. This wikipedia page: https://en.wikipedia.org/wiki/Direct_sum_of_groups#Generalization_to_sums_over_infinite_sets Context: I'm trying to understand the direct sum parts of graded rings and graded ideals in later in Chapter 11.5 . I'm hoping these can be internal instead of just external. I ask more here . Edit 1: Thank you for the upvotes or views. I feel like all the hours I spent trying to understand this seemingly minor thing was really worth it. Edit 2: For (not necessarily Abelian) groups: Internal direct product/sum in groups: Is join and independent equivalent to unique expression?","['direct-product', 'graded-rings', 'direct-sum', 'modules', 'abstract-algebra']"
3651555,Prove $\sum_{n\geq1}\frac{2^n (1-\cos\frac{x}{2^n})^2}{\sin\frac{x}{2^{n-1}}}=\tan\frac{x}{2}-\frac{x}{2}$,How to prove for $|x|<\pi$ : $\sum_{n\geq1}\frac{2(1-\cos(\frac{x}{2^n}))}{\sin(\frac{x}{2^{n-1}})}=\tan(\frac{x}{2})$ $\sum_{n\geq1}\frac{2^n (1-\cos(\frac{x}{2^n}))^2}{\sin(\frac{x}{2^{n-1}})}=\tan(\frac{x}{2})-\frac{x}{2}$ Any help will be appreciated.,"['proof-without-words', 'trigonometry', 'summation']"
3651688,Proving a map in a commutative diagram is continuous.,"Suppose $G$ and $H$ are topological groups, with $H \subset G$ . I have the following commutative diagram with $f$ being continuous and $p$ being an open surjection (canonical projection). Does this imply that $h$ is continuous? $\require{AMScd}$ \begin{CD}
G @>{f}>> G\\
@VpVV @VVpV\\
G/H @>{h}>> G/H
\end{CD} Take $U$ an open set in $G/H$ . I have to show $h^{-1}(U)$ is open in $G/H$ , i.e. $p^{-1}(h^{-1}(U))$ is opened in G. I know $p^{-1}(h^{-1}(U))=(h \circ p)^{-1}(U)=(p \circ f)^{-1}(U)=f^{-1}(p^{-1}(U))$ .
But does this help to prove the claim?","['general-topology', 'topological-groups', 'quotient-spaces']"
3651816,"$g$ not continuous in $(0,0)$, differentiable in every direction AND $|D_vg(x)| \leq |v|$","I have found plenty of simliar questions to mine, but in this case there is one more condition that needs to be satisfied, this is the problem: ""Find a function $g:\mathbb{R}^2 \rightarrow\mathbb{R}$ , so that all directional derivatives $D_v g(x)$ exist ( $v\in\mathbb{R}^2$ ) but $g$ isn't continuous in $(0,0)$ AND $|D_vg(x)| \leq |v|$ ."" It's easy to find a function that satisfies the first two conditions, but I just don't know how to use the third one. I also have a problem in understanding it. For example, I can have 2 vectors which point in the same direction but have different length, for example: $\left(
\begin{array}{c}
1\\
0\\
\end{array}
\right)$ and $\left(
\begin{array}{c}
0.01\\
0\\
\end{array}
\right)$ , both point into the same direction, but their length isn't the same. With this in mind, $|v|$ can become arbitrary small, meaning that $|D_vg(x)|$ has to be $0$ , so $g$ has to be a constant function. But that doesn't really help, since there is no constant function that is uncontinuous in $(0,0)$ (or is there?), so I guess that I didn't understand the $|D_vg(x)| \leq |v|$ condition properly. Could you give me some advice?","['derivatives', 'real-analysis']"
3651828,Prove that a parametric curve does not intersect itself,"Consider the following parametric curve $(x(t), y(t))_{t \in \mathbb{R}_+}$ : $$x(t) = \frac{(1+e)(1 - \cos(t)) - t(e-1) \sin (t)}{e t^2 - t^2 \cos(t) - t \sin(t)} $$ $$y(t) = \frac{e(1+t^2)(1 - \cos(t))}{e t^2 - t^2 \cos(t) - t \sin(t)}, $$ where $e = \exp(1)$ .
How to prove that this curve has no multiple points, except the two ones located on the axis $x = 0$ ?","['plane-curves', 'curves', 'real-analysis']"
3651884,Abel summability and Fourier series.,"Recently I'm studying something about Fourier series on the space $\mathbb{T}$ and I find this particular question: Is possible to prove that the Fourier series of an integrable function, is summable by Abel at almost every point? Now, I know that Abel summability is a method to regularize divergent series and that makes finite sums that would otherwise be infinite as the limit of partial sums and I also know that exist some theorems about the convergence according to Abel of the Fourier series. But here, for my question, how can I use these informations? Thanks.","['integration', 'complex-analysis', 'summation', 'fourier-series']"
3651899,Convergence of probabilistic power tower $e^{\pm e^{\pm e^{...}}}$,"While pondering over this question , I came across another interesting one. I am familiar with infinite tetration and its convergence over the reals. Nevertheless, when I saw this power tower, I couldn't help but wonder which distributions of $\pm$ signs make this converge or diverge. For instance: $$e^{-e^{-e^{...}}}={}^\infty(e^{-1})< \infty \\ e^{e^{e^{...}}}={}^\infty e \to \infty$$ Let's define $\forall n\geq 1,\epsilon_n \in {\pm 1}$ as the n th sign of the power tower $$P_\epsilon=e^{\epsilon_1e^{\epsilon_2e^{...}}}$$ defined recursively as $$
[P_\epsilon]_1(x) =e^{\epsilon_1 x}\\ [P_\epsilon]_{n+1}(x) = [P_\epsilon]_{n}(e^{\epsilon_{n+1} x})\\ [P_\epsilon]_n(e) = [P_\epsilon]_n\\
$$ Then, evidently, the first terms of $\epsilon_n$ are irrelevant, only the assymptotic behaviour is important. If we take $$\epsilon_n=\begin{cases}1&\text{for } n\equiv 0 \mod k \\-1 &\text{else }\end{cases}$$ would this converge for some $k$ ? Lastly, one can conjure of all sorts of patterns for these $(\epsilon_n)_{n\in \mathbb{N}}:$ what if the $(-1)$ s are only for prime indexes? What if $\epsilon$ is $−1$ with probability $(1−p)$ and $1$ with probability $p$ ? I think this last question is very interesting, but probably hard to solve. It would seem an important threshold occurs if the expected value $\mathbb{E}(\epsilon)=0$ , or $p=\frac{1}{2}$ , as the limit case for convergence is $${}^\infty(e^{e^{-1}})<\infty$$ My guess is, for $\epsilon$ with $\mathbb{E}(\epsilon)>0$ it will diverge a.s. and for $\mathbb{E}(\epsilon)<0$ it will converge a.s., but I have no idea on how to prove it. This reminds me a lot of Kolmogorov's three series theorem, although I doubt it can be solved in a similar manner. I hope I haven't missed something that would make this problem trivial, that would be very disappointing. Thanks! (feel free to edit to make it look better, or to add more appropriate tags) EDIT :This question has been edited to account for the non-associativity of exponentiation, a fact I, somehow, seemed to have momentarily forgotten.","['convergence-divergence', 'power-towers', 'probability']"
3652039,Show a parallelogram with angle $60^\circ$ is a rhombus,"If $ABCD$ is a parallelogram with $\angle BAD=60 ^\circ$ and $\dfrac{AC^2}{BD^2}=\dfrac31$ , show $ABCD$ is a rhombus. We have the squares of $AC$ and $BD$ so MAYBE it is a good idea to construct right triangles. Let $DD_1\perp AB$ and $CC_1 \perp AB$ . Now we have the right triangles $BD_1D$ and $AC_1C$ with hypotenuses $BD$ and $AC$ , respectively. By the Pythagorean theorem we can get $AC^2=AC_1^2+CC_1^2$ and $BD^2=BD_1^2+DD_1^2$ . This does not seem to help. Can you give me some hints? Thank you in advane! :) I am trying to solve it by the Pythagorean theorem.","['trigonometry', 'geometry']"
3652054,How is independence established and applied in probability?,"Events $A$ and $B$ are said to be independent if and only if $$\mathbb{P}(A\cap B)=\mathbb{P}(A)\cdot \mathbb{P}(B).$$ There are also well-known definitions of a tuple of events being pairwise or mutually independent (which are not equivalent). In some elementary texts, I have seen that it is claimed without justification that $A$ and $B$ are independent (often in a context where such a claim is sufficiently obvious to not raise questions, like a dice roll and a coin toss which do not ""affect"" each other), and then the equation in the definition is used to compute $\mathbb{P}(A\cap B)$ by instead computing $\mathbb{P}(A)\cdot \mathbb{P}(B).$ However, it seems to me that before we can claim that $A$ and $B$ are independent, we need to prove that the equation holds, which just might involve computing all three quantities, thus rendering the concept of independence as baggage in this scenario. Questions: Are there implicit ways of proving independence that are applicable in some general scenarios? By implicit, I mean either establishing $\mathbb{P}(A\cap B)=\mathbb{P}(A)\cdot \mathbb{P}(B)$ without computing the three probabilities, or establishing independence without proving this equation. If so, what are they? In these two cases, it would then make sense to me that the equation could be used as a consequence of having proven independence, making the concept of independence useful in that scenario. If the answer is negative (so implicit methods are unknown), then I have to wonder what purpose this abstraction of independence serves in general. Does it maybe facilitate some proofs that are unrelated to independence, such as restricting to independent cases first and then using it to generalize the argument? Or is independence a  concept that exists for its own sake? Restricting the answer to discrete/finite probability spaces is not a problem.","['discrete-mathematics', 'combinatorics', 'probability']"
3652080,Concluding that sine and cosine are $2\pi$ periodic from definitions,"Let $(f,g)$ be a pair of real-valued $C^1$ functions on $\mathbb{R}$ satisfying $$\forall x\in\mathbb{R}\left(f'(x)=g(x)\quad\text{and}\quad g'(x)=-f(x)\right)$$ $$f(0)=0\quad\text{and}\quad g(0)=1$$ Then it is pretty immediate that $f$ and $g$ are both $C^\infty$ on $\mathbb{R}$ , this determines a power series which determines uniqueness of the pair. A cute argument (without power series) shows that these functions  satisfy $f(x)^2+g(x)^2=1$ . Here's my question: how do we deduce that $f(x)$ and $g(x)$ are $2\pi$ periodic? I understand that we could have started the entire axiomatic system by defining $f(x)$ and $g(x)$ in terms of triangles, but since we already have uniqueness from this setup I wonder how we deduce periodicity from here.","['trigonometry', 'ordinary-differential-equations']"
3652087,how to structure an example of a sequence,I need to find an example of a sequence that on the one hand is divergent and on the other hand has two subsequences that $$\lim_{n\to \infty} \mathit{a}_{3n} = \lim_{n\to \infty} \mathit{a}_{5n}.$$ Thanks a lot.,"['limits', 'calculus', 'sequences-and-series']"
3652088,"Prove that for sets $A,B,C$, if $C \subseteq B$, then $(A\setminus B)\cap C = \varnothing$.","I just need the proof of this. How does one prove that given $A, B, C$ , if $C\subseteq B$ , then $(A\setminus B)\cap C$ is equal to an empty set.",['elementary-set-theory']
3652102,"$P$ be a poset with more than min$\{rs-r,rs-s\}$ elements where $r,s\in\Bbb{N}$. Prove that $P$ has an anti-chain of size $r$ or a chain of size $s$","Let, $(P,\le)$ be the poset. I have begun to solve this in the following way-
Note that, $rs-r\le rs-s\iff r\ge s$ So, without loss of generality assume that $r\ge s$ , then $\operatorname{min}(rs-r,rs-s)=r(s-1)$ As per the question $P$ has elements $\ge r(s-1)$ . So, let number of elements of $P$ is $r(s-1)+n$ where $n\in\Bbb{N}$ . Let us assume on contrary, $P$ neither has an anti-chain of size $r$ nor a chain of size $s$ i.e. for any $A$ of $P$ with $r$ elements, $\exists a,b\in A$ such that either $a\le b$ or $b\le a$ . And for any $C$ of $P$ with $s$ elements, $\exists x,y\in A$ such that neither $x\le y$ nor $y\le x$ . Now, I cannot use the number of elements of $P$ to get a contradiction from the above assumption. Can anybody help me with this? Thanks for assistance in advance.","['elementary-set-theory', 'order-theory']"
3652117,"Prob. 10, Sec. 30, in Munkres' TOPOLOGY, 2nd ed: A countable product of separable spaces is also separable","Here is Prob. 10, Sec. 30, in the book Topology by James R. Munkres, 2nd edition: Show that if $X$ is a countable product of spaces having countable dense subsets, then $X$ has a countable dense subset. My Attempt: Let $X_1, X_2, X_3, \ldots$ be any countably many topological spaces having countable dense subsets $D_1, D_2, D_3, \ldots$ , respectively, and let us put $$ \mathbf{X} \colon= X_1 \times X_2 \times X_3 \times \cdots. \tag{Definition 0} $$ For each $n \in \mathbb{N}$ , let $p_n$ be some given point of $X_n$ . Now let us put $$ D_{n_1, \ldots, n_r}^\prime \colon= D_1^\prime \times D_2^\prime \times D_3^\prime \times \cdots, \tag{Definition 1} $$ where $D_n^\prime \colon= D_n$ for finitely many $n = n_1, \ldots, n_r$ , and $D_n \colon= \left\{ p_n \right\}$ for all other valuse of $n$ . Then let $$ \mathbf{D} \colon= \bigcup_{r \in \mathbb{N}} \bigcup_{n_1, \ldots, n_r \in \mathbb{N}} D_{n_1, \ldots, n_r}^\prime. \tag{Definition 2} $$ The set $\mathbf{D}$ is a countable union of countable subsets of $\mathbf{X}$ and is thus itself a countable subset of $\mathbf{X}$ . We now show that $\mathbf{D}$ is dense in $\mathbf{X}$ , that is, we show that $$ \overline{\mathbf{D}} = \mathbf{X}. \tag{0} $$ Let $\mathbf{x} \colon= x_1 \times x_2 \times x_3 \times \cdots$ be any point of $X$ , and let $$ \mathbf{B} \colon= B_1 \times B_2 \times B_3 \times \cdots \tag{Definition 3} $$ be any basis set for the product topology on $X$ such that $\mathbf{x} \in \mathbf{B}$ ; let $n = n_1, \ldots, n_r$ be the finitely many indices for which $B_n$ is an open set of $X_n$ and let $B_n = X_n$ for all other values of $n$ . For each $i = 1, \ldots, r$ , as $D_{n_i}$ is dense in $X_{n_i}$ that is $\overline{D_{n_i}} = X_{n_i}$ , so $x_{n_i} \in \overline{D_{n_i}}$ , and since $B_{n_i}$ is an open set of $X_{n_i}$ containing $x_{n_i}$ , therefore we can conclude that $$ B_{n_i} \cap D_{n_i} \neq \emptyset, \tag{1} $$ and thus there exists a point $y_{n_i} \in X_{n_i}$ such that $$ y_{n_i} \in B_{n_i} \cap D_{n_i}. \tag{Definition 4*} $$ Now let $$ \mathbf{y} \colon= y_1^\prime \times y_2^\prime \times y_3^\prime \times \cdots \in X, \tag{Definition 4} $$ where $y_{n_i}^\prime \colon= y_{n_i}$ for $i = 1, \ldots, r$ , and $y_n^\prime \colon= p_n$ for all other values of $n$ . This point $\mathbf{y} \in \mathbf{B} \cap \mathbf{D}$ . [Please refer to (Definition 2), (Definition 3), and (Definition 4), and (Definition 4*)  above. ] Thus $$ \mathbf{B} \cap \mathbf{D} \neq \emptyset. \tag{2} $$ Thus for any basis set $\mathbf{B}$ for the product topology on $\mathbf{X}$ such that $\mathbf{x} \in \mathbf{B}$ , the relation (2) above holds. Thus we can conclude that $$ \mathbf{x} \in \overline{\mathbf{D}}. $$ But as $\mathbf{x} \in \mathbf{X}$ was arbitrary, we can conclude that (0) above holds. Thus $\mathbf{X}$ has a countable dense subset $\mathbf{D}$ . [Please refer to (Definition 0) and (Definition 2) above.] Is this proof correct? If so, is my presentation clear enough? Or, are there errors or issues?","['general-topology', 'solution-verification', 'separable-spaces']"
3652180,Let $D$ be an integral domain which is not field and $Q=\text{Frac}(D)$. Then $Q$ has not a projective cover as $D$-module.,"Let $D$ be an integral domain which is not a field and $Q=\text{Frac}(D)$ the field of fractions of $D$ . Then $Q$ as a $D$ -module has not a projective cover. By Corollary 5.35 of Rotman's Homological Algebra we got that : If $D$ is an integral domain and $Q=\text{Frac}(D)$ , then $Q$ is a flat $D$ -module . So we got it, $Q$ is a flat $D$ -module, now my idea is to use Bass Theorem to kill this one, this theorem states (among other things) that $M_{R}$ has projective cover ( $M_{R}$ is perfect) iff every $M_{R}$ flat module is projective. So I reduced (or complicated :S) the problem to prove that the flat module $Q_{D}$ is not projective, which is the part I cannot prove. I tried to study the proof of $\mathbb{Q}_{\mathbb{Z}}$ is a flat module that is not projective but they use the fact $\mathbb{Q}_{\mathbb{Z}}$ is finitely generated and that $\mathbb{Z}$ is DIP but in my case I dont know if $Q_{D}$ is finitely generated, also $D$ is not be DIP. Any help in order to prove this problem in the direction I propose or any other will be apreciated. Thanks!","['homological-algebra', 'modules', 'algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
3652205,"Evaluating $\iint_{[0,1]^2} \frac{2-4xy}{(9-xy)(8+xy)}dxdy$","I am trying to compute the following double integral: $$I=\iint_S \frac{2-4xy}{(9-xy)(8+xy)}dxdy$$ with $S=[0,1]\times[0,1].$ What I have tried: I have written the integral as follows: $$I=I_1+I_2=-2\iint_S \frac{1}{(9-xy)}dxdy+2\iint_S \frac{1}{(8+xy)}dxdy$$ When trying to compute $I_1$ , I encountered this integral $\int_0^1-\frac{1}{x}\ln(1-\frac{x}{9})dx$ . I used an online calculator to solve it and the result involves the function $\operatorname{Li}(z)$ , which I am not familiar with. Another thing I have tried is a change of variables: $$u=9-xy$$ $$v=8+xy$$ The problem is that the Jacobian associated to this change of variables is null, so I cannot use it. My question: Could someone show me a way of computing this integral without having to use $\operatorname{Li}_2(z)$ ?","['integration', 'multivariable-calculus', 'change-of-variable']"
3652334,Why is this $L^1$-sequence relatively weakly sequentially compact?,"Let $(E,\mathcal E,m)$ be a probability space, $\theta$ be a measurable map on $(E,\mathcal E)$ with $m\circ\theta^{-1}=m$ , $s_n$ be a real-valued nonpositive integrable random variable on $(E,\mathcal E,m)$ for $n\in\mathbb N$ with $\lambda:=\inf_{n\in\mathbb N}\int s_n\:{\rm d}m>-\infty$ and $$\varphi_n:=\frac1n\sum_{i=1}^n(s_i-s_{i-1}\circ\theta)\;\;\;\text{for }n\in\mathbb N.$$ It's easy to see that $$\int\varphi_n\:{\rm d}m=\frac{\int s_n\:{\rm d}m}n\xrightarrow{n\to\infty}\lambda\tag1.$$ Moreover, $$Tf:=f\circ\theta\;\;\;\text{for }f\in\mathcal L^1(m)$$ is a linear isometry on $L^p(m)$ for all $p\in[1,\infty]$ . In particular, it is continuous with respect to the weak topology on $L^1(m)$ . How can we show that, for all $i\in\mathbb N_0$ and $p\in\mathbb N$ , there is an increasing $(n_k)_{k\in\mathbb N}\subseteq\mathbb N$ with $$\max\left(\varphi_{n_k}\circ\theta^i,-p\right)\xrightarrow{k\to\infty}\lambda_{i,\:p}\tag3$$ with respect to the weak topology on $L^1(m)$ for some $\lambda_{i,\:o}\in L^1(m)$ ? And how can we show that $\lambda_{i,\:p}$ is nondecreasing in $p$ almost surely? These claims are made in the proof of Theorem 6.7 (in Chapter 4, Paragraph 6) of Revuz' Markov Chains book: I don't understand his arguments. For example, I guess he's talking about relative sequential compactness (I'm not sure, but may it be that this is equivalent to relative compactness in the weak topology?). It seems like he's using that $(\varphi_n)_{n\in\mathbb N}$ is contained in a relatively sequentially compact set; but why is that the case? And why do the $\lambda_{i,\:p}$ need to satisfy the claimed monotonicity in $p$ ? Or does he mean that they can be chosen such that they satisfy this condition?","['measure-theory', 'weak-convergence', 'ergodic-theory', 'functional-analysis', 'probability-theory']"
3652344,Proving that the solution to $f^{\prime}(x) = \frac{1}{x^{2} + (f(x))^{2}}$ is bounded above.,"I am given that $f:[0,\infty)\to \mathbb{R}$ is the unique solution to the ODE: $$f^{\prime}(x) = \frac{1}{x^{2} + (f(x))^{2}}$$ with $f(0)=1$ and I must prove that it is bounded. I have already proven (using the fact that the derivative is always positive and the Mean Value Theorem) that it is monotonically increasing so is bounded below, specifically by $f(0)=1$ . However, I am struggling to figure out how to show it is bounded above. One thought I had was to rearrange the ode to get $(f(x))^2$ the subject, however this doesn't seem like a valid method for some reason, I may just be overthinking it though.","['ordinary-differential-equations', 'analysis', 'real-analysis', 'calculus', 'integral-inequality']"
3652383,Example of “almost metric” topological space,"It is well known, that every subspace of separable metric space is separable. It is also known, this statement not to be true, if space is topological and not necessary metric. But I cannot find an example of topological uncountable and non-metrizable space and topology $\tau$ is infinite, such that every subspace is still separable.","['general-topology', 'separable-spaces', 'examples-counterexamples']"
3652419,"Solve $x^4y^{\prime\prime} = (y-xy^\prime)^3, y(1) = y^\prime(1) = 1$","$\lambda^4x^4\lambda^{n-2}y^{\prime\prime} = (\lambda^ny-\lambda x\lambda^{n-1}y^\prime)^3 \Rightarrow \lambda^{n+2}x^4y^{\prime\prime} = (\lambda^n(y-xy^\prime))^3$ . $\lambda^{n+2} = \lambda^{3n} \Rightarrow n+2 = 3n \Rightarrow n = 1$ . Let $x = e^t, y = ue^{nt} = ue^t$ . Now, $\frac{dy}{dx} = \frac{\frac{dy}{dt}}{\frac{dx}{dt}} = \frac{u^\prime e^t + ue^t}{e^t} = u^\prime + u$ . And, $\frac{d^2y}{dx^2} = \frac{\frac{d}{dt}}{\frac{dx}{dt}}(\frac{dy}{dx}) = \frac{\frac{d}{dt}(u^\prime + u)}{e^t} = e^{-t}(\frac{d^2u}{dt^2} + \frac{du}{dt})$ . Thus, $e^{4t}(e^{-t}(\frac{du^2}{dt^2} + \frac{du}{dt})) = (ue^{t}-e^t(u+\frac{du}{dt}))^3 \Rightarrow e^{3t}(\frac{d^2u}{t^2} + \frac{du}{dt}) = e^{3t}(u-(u+\frac{du}{dt}))^3 \Rightarrow (\frac{d^2u}{dt^2} + \frac{du}{dt}) = (\frac{du}{dt})^3$ . Let $p = \frac{du}{dt}, p^\prime = \frac{d^2u}{dt^2} = \frac{dp}{du}\frac{du}{dt} = p\frac{dp}{du}$ . Thus, $(p + p\frac{dp}{du}) = p^3 \Rightarrow 1 + \frac{dp}{du} = p^2 \Rightarrow \frac{dp}{du} = p^2-1 \Rightarrow u + c_1 = \int\frac{dp}{p^2-1} = \int\frac{dp}{(p-1)(p+1)} = \int(\frac{1}{2(p-1)} - \frac{1}{2(p+1)})dp = \frac{1}{2}\ln(p-1)-\frac{1}{2}\ln(p+1) = \ln(\frac{\sqrt{p-1}}{\sqrt{p+1}}) \Rightarrow c_1e^u = \frac{\sqrt{p-1}}{\sqrt{p+1}} \Rightarrow c_1e^{2u} = \frac{p-1}{p+1} \Rightarrow p-1 = c_1e^{2u}p + c_1e^{2u} \Rightarrow p = \frac{c_1e^{2u}+1}{1-c_1e^{2u}} = \frac{du}{dt}$ . But then I think that I have to use the initial conditions to get $c_1$ , but I don know how to do that.",['ordinary-differential-equations']
3652428,"If $f$ and $g$ are differentiable at $x_{0}$, and $g$ is non-zero on $X$, then $f/g$ is also differentiable at $x_{0}$ and . . .","If $f$ and $g$ are differentiable at $x_{0}$ , and $g$ is non-zero on $X$ , then $f/g$ is also differentiable at $x_{0}$ and \begin{align*}
\left(\frac{f}{g}\right)' = \frac{f'(x_{0})g(x_{0}) - f(x_{0})g'(x_{0})}{g(x_{0})^{2}}
\end{align*} MY ATTEMPT According to the definition of derivative, we have that \begin{align*}
\lim_{x\rightarrow x_{0}}\frac{(f/g)(x) - (f/g)(x_{0})}{x-x_{0}} = \lim_{x\rightarrow x_{0}}\frac{f(x)g(x_{0}) - f(x_{0})g(x)}{(x-x_{0})g(x)g(x_{0})}
\end{align*} On the other hand, we have that \begin{align*}
f(x)g(x_{0}) - f(x_{0})g(x) & = f(x)g(x_{0}) - f(x_{0})g(x_{0}) + f(x_{0})g(x_{0}) - f(x_{0})g(x)\\\\
& = [f(x) - f(x_{0})]g(x_{0}) - f(x_{0})[g(x) - g(x_{0})]
\end{align*} Consequently, we have that \begin{align*}
\lim_{x\rightarrow x_{0}}\frac{(f/g)(x) - (f/g)(x_{0})}{x-x_{0}} & = \lim_{x\rightarrow x_{0}}\frac{[f(x) - f(x_{0})]g(x_{0}) - f(x_{0})[g(x) - g(x_{0})]
}{(x-x_{0})g(x)g(x_{0})}\\\\
& = \lim_{x\rightarrow x_{0}}\frac{[f(x) - f(x_{0})]g(x_{0})}{(x-x_{0})g(x)g(x_{0})} - \lim_{x\rightarrow x_{0}}\frac{f(x_{0})[g(x) - g(x_{0})]}{(x-x_{0})g(x)g(x_{0})}\\\\
& = \frac{f'(x_{0})g(x_{0}) - f(x_{0})g'(x_{0})}{g(x_{0})^{2}}
\end{align*} and we are done. I would like to know if someone could provide an alternative solution to this problem. Any contribution is appreciated.","['alternative-proof', 'solution-verification', 'derivatives', 'real-analysis']"
3652444,Area of triangle inscribed in a circle with a specific position,"Triangle $ABC$ inscribed in a circle. Versine (green)are drawn from the midpoints of the sides of the triangle perpendicular to them. They have lengths as shown in this figure. find the area of $\Delta ABC$ honestly, I got stuck on this problem and I was far from geometry for years. Please show me a clue or guide me to get over on this problem. Thanks in advance. I just find out the lines(green) must cross at one point, because of the middle and perpendicularly. but not go more ...","['euclidean-geometry', 'geometry']"
3652455,Computing the differential of the coadjoint representation of a Lie group at origin ( $ad^*$),"Let $G$ be a Lie group and $Ad^*: \mathfrak{g} \rightarrow GL(\mathfrak{g}^*)$ , $[Ad^*(g)(\xi)](x) \stackrel{def}{=} \xi(Ad(g^{-1})x)$ , the coadjoint representation. I am trying to compute $ad^*: \mathfrak{g} \rightarrow \mathfrak{gl}(\mathfrak{g}^*)$ , $ad^* = d_eAd^*$ . Supposedly, it is $ad^*(x)(\xi)(y)=-\xi([x,y]_G)$ . I know that $ad(x)(y)=[x,y]_G, \forall x,y \in \mathfrak{g}$ . Intuitively, it seems like what is happening is that the differential of $Ad^*$ ""passes over"" each 1-form $\xi$ and simply differentiates $Ad(a^{-1})$ . Since $Ad(a^{-1})=(Ad(a))^{-1} = (i \circ Ad)(a)$ , where $i$ is the inverse in $GL(\mathfrak{g})$ , and $d_ei=-id$ , the result would follow. But I can't see how to prove this ""passing over"" rigorously. Also, $[\cdot, \cdot]_G$ can be seen as the linear term of the function $\nu(x,y)$ which makes it that $exp(x)exp(y)=exp(\nu(x,y))$ on a neighborhood of $0_{\mathfrak{g}}$ . Based on this and the fact that the diagram: $$
\require{AMScd}
\begin{CD}
\mathfrak{g} @>{ad}>> \mathfrak{gl}(\mathfrak{g})\\
@VV{exp_G}V @VV{exp_{GL(\mathfrak{g})}}V \\
G @>{Ad}>> GL(\mathfrak{g})
\end{CD}
$$ commutes, it is shown that $ad(x)(y)=[x,y]_G$ . (A more general proposition holds, namely that if $\Phi:G \rightarrow H$ is a lie group homeomorphism, then the diagram: $$
\require{AMScd}
\begin{CD}
\mathfrak{g} @>{d_e\Phi}>> \mathfrak{h}\\
@VV{exp_G}V @VV{exp_H}V \\
G @>{\Phi}>> \mathfrak{h}
\end{CD}
$$ commutes.) That is, it suffices to calculate $Ad$ and then use the theorem to get to $ad$ . However, the argument begins by exploiting the definition of $Ad(a) = d_e(b \rightarrow aba^{-1})$ in the sense that $Ad(exp(x))(y)$ can be seen as $\gamma'(0)$ for $\gamma(t)=exp(x)exp(ty)exp(x)^{-1}$ and proceeds from there. I tried reproducing the argument for $Ad^*: G \rightarrow GL(\mathfrak{g}^*)$ and $ad^*: \mathfrak{g} \rightarrow \mathfrak{gl}(\mathfrak{g}^*)$ instead of $Ad: G \rightarrow GL(\mathfrak{g})$ and $ad: \mathfrak{g} \rightarrow \mathfrak{gl}(\mathfrak{g})$ respectively ( $Ad^*$ and $ad^*$ are also Lie group homeomorphisms), but I can't find a way to exploit the definition of $Ad^*$ in the same way. Thank you for your time.","['representation-theory', 'lie-algebras', 'lie-groups', 'differential-geometry']"
3652459,"Prob. 11, Sec. 30, in Munkres' TOPOLOGY, 2nd ed: A continuous image of a Lindelof (separable) space is Lindelof (separable)","Here is Prob. 11, Sec. 30, in the book Topology by James R. Munkres, 2nd edition: Let $f \colon X \rightarrow Y$ be continuous. Show that if $X$ is Lindelof, or if $X$ has a countable dense subset, then $f(X)$ satisfies the same condition. My Attempt: Let $X$ and $Y$ be topological spaces, and let $f \colon X \rightarrow Y$ be a continuous map. Case 1. Suppose that $X$ is Lindelof. Let $\mathscr{A}$ be an open covering of $f(X)$ regarded as a subspace of $Y$ . For each $V \in \mathscr{A}$ , we can find an open set $V^\prime$ of $Y$ such that $$ V = f(X) \cap V^\prime. \tag{0} $$ Let $\mathscr{A}^\prime$ be the open covering of $Y$ given by $$ \mathscr{A}^\prime \colon= \left\{ \, V^\prime \, \colon \, V^\prime \mbox{ is open in $Y$ and } f(X) \cap V^\prime \in \mathscr{A} \, \right\}. \tag{Definition 0} $$ [Please refer to (0) above.] Let $V \in \mathscr{A}$ and $V^\prime \in \mathscr{A}^\prime$ for which (0) above holds. Then we find that $$
\begin{align}
f^{-1}(V) &= f^{-1} \left( f(X) \cap V^\prime \right) \\
&= f^{-1} \big( f(X) \big) \cap f^{-1} \left( V^\prime \right) \\
&= X \cap f^{-1} \left( V^\prime \right) \\
&= f^{-1} \left( V^\prime \right). \tag{1}
\end{align}
$$ Moreover, as $V^\prime$ is an open set of $Y$ and as the mapping $f \colon X \rightarrow Y$ is continuous, so the inverse image $f^{-1} \left( V^\prime \right) = f^{-1} (V)$ is an open set of $X$ . We note that \begin{align} 
f(X) &= \bigcup_{V \in \mathscr{A}} V \\ 
&= \bigcup_{V^\prime \in \mathscr{A}^\prime} \left( f(X)\cap V^\prime \right) \\ 
&= f(X) \cap \left( \bigcup_{V^\prime \in \mathscr{A}^\prime} V^\prime \right) \\ 
&\subset \bigcup_{V^\prime \in \mathscr{A}^\prime} V^\prime,
\end{align} which implies that $$
f(X) \subset \bigcup_{V^\prime \in \mathscr{A}^\prime} V^\prime.
$$ Now since $$ \bigcup_{V \in \mathscr{A}} V = f(X) \subset \bigcup_{V^\prime \in \mathscr{A}^\prime} V^\prime, $$ therefore we obtain \begin{align} 
f^{-1} \left( \bigcup_{V \in \mathscr{A}} V  \right) &= f^{-1}\big( f(X) \big) \\ 
&\subset f^{-1} \left( \bigcup_{V^\prime \in \mathscr{A}^\prime} V^\prime \right) \\ 
&\subset X, 
\end{align} [Of course all the inverse images are subsets of the domain.] which simplifies to $$
\bigcup_{V \in \mathscr{A}} f^{-1} \left( V  \right) = X = \bigcup_{V^\prime \in \mathscr{A}^\prime}  f^{-1} \left( V^\prime \right). \tag{2} 
$$ Thus the collection $$ 
\mathscr{A}_X \colon= \left\{ \, f^{-1} (V) \, \colon \, V \in \mathscr{A} \, \right\} = \left\{ \, f^{-1} \left(V^\prime \right) \, \colon \, V^\prime \in \mathscr{A}^\prime \, \right\}
$$ is an open covering of the Lindelof space $X$ , and therefore some countable subcollection of $\mathscr{A}_X$ also covers $X$ ; let one such countable subcollection be $$
\left\{\, f^{-1} \left( V_n \right) \, \colon \, n \in \mathbb{N} \, \right\} = \left\{\, f^{-1} \left( V_n^\prime \right) \, \colon \, n \in \mathbb{N} \, \right\}.
$$ [Please refer to (1) above.] Finally since $$ 
X = \bigcup_{n \in \mathbb{N} } f^{-1} \left( V_n \right),
$$ therefore we obtain \begin{align} 
f(X) &= f \left( \bigcup_{n \in \mathbb{N} } f^{-1} \left( V_n \right) \right) \\ 
&= \bigcup_{n \in \mathbb{N} } f \left(  f^{-1} \left( V_n \right) \right) \\ 
&\subset \bigcup_{n \in \mathbb{N}} V_n \\ 
&\subset f(X),
\end{align} [The last inclusion follwos from the fact that the sets $V_n$ are in the covering $\mathscr{A}$ of $f(X)$ .] and hence $$
\bigcup_{n \in \mathbb{N} } V_n = f(X). 
$$ Thus the collection $$
\left\{ \, V_n \, \colon \, n \in \mathbb{N} \, \right\}
$$ is a countable subcollection of $\mathscr{A}$ that also covers $f(X)$ . This shows that every open covering $\mathscr{A}$ of $f(X)$ has a countable subcollection also covering $f(X)$ . Hence $f(X)$ is Lindelof (as a subspace of $Y$ ) whenever $X$ is a Lindelof space and $f \colon X \rightarrow Y$ is a continuous mapping. Am I right? Case 2. Next, suppose that $X$ is separable. Let $D$ be a countable dense subset of $X$ . Then $D \subset X$ such that $\overline{D} = X$ , and since $f \colon X \rightarrow Y$ is continuous, therefore by Theorem 18.1 (2) in Munkres we obtain $$
f(X) = f\left( \overline{D} \right) \subset \overline{ f(D) },
$$ and hence by Theorem 17.4 in Munkres $$
\left(\overline{f(D)}\right)_{\mbox{in } f(X)} = f(X) \cap \overline{f(D)} = f(X),
$$ that is, $$
\left(\overline{f(D)}\right)_{\mbox{in } f(X)} = f(X). \tag{3} 
$$ Here $\overline{f(D)}$ denotes the closure of $f(D)$ in the topological space $Y$ . Moreover, as $D$ is a countable subset of $X$ and as $f \colon X \rightarrow Y$ is a single-valued map, so we can conclude that $f(D)$ is also a countable subset of $f(X)$ . From (3) above and what has been stated in the preceding paragraphs, we can conclude that $f(X)$ has a countable dense subset $f(D)$ whenever $X$ has a countable dense subset $D$ . Hence $f(X)$ is separable (as a subspace of $Y$ ) whenever $X$ is separable and $f \colon X \rightarrow Y$ is continuous. Am I right? Are both parts of my proof correct? If so, are my presentations of both proofs also clearly enough understandable? Or, are there any issues with either proof?","['separable-spaces', 'continuity', 'solution-verification', 'general-topology', 'lindelof-spaces']"
3652486,Evaluating : $\int \frac{\sec x-\tan x}{\sqrt{\sin^2x-\sin x}} \mathrm{d}x$,"As a part of a bigger question, I was asked to evaluate the integral : $$\int \frac{\sec x-\tan x}{\sqrt{\sin^2x-\sin x}} \mathrm{d}x$$ Here's what I tried :
(Please bear with me, it gets quite lengthy) $$\int \frac{\sec x-\tan x}{\sqrt{\sin^2x-\sin x}} \mathrm{d}x$$ $$=\int \frac{1-\sin x}{\cos x \sqrt{\sin^2x-\sin x}}\mathrm{d}x$$ $$=\int \frac{(1-\sin x) \cos x }{\sqrt{\sin^2 x-\sin x}(1-\sin^2 x)}\mathrm{d}x$$ $$=\int \frac{\cos x}{(\sqrt{\sin^2x -\sin x}(1+\sin x)}\mathrm{d}x$$ Substituting $\sin x= t$ , we're left with a comparatively good-looking integral: $$\int \frac {\mathrm{d}t}{(1+t)\sqrt{t^2-t}}$$ Well, this integral looks simple and maybe is, but I'm having real trouble evaluating it : $$\frac12\int \frac{t+1-(t-1)}{(1+t)\sqrt{t^2-t}}\mathrm{d}t$$ $$=\frac12\left[\int \frac{\mathrm{d}t}{\sqrt{t^2-t}}-\int \frac{t-1}{\sqrt{t^2-t}}\mathrm{d}t\right]$$ Now this is getting longer than I expected it to. Can anyone help me find a shorter and quicker solution to this problem? Thanks in advance.","['integration', 'calculus']"
3652501,"If $f$ is monotone increasing and $f$ is differentiable at $x_{0}$, then $f'(x_{0}) \geq 0$.","Let $X$ be a subset of $\textbf{R}$ , let $x_{0}\in X$ be a limit point of $X$ , and let $f:X\rightarrow\textbf{R}$ be a function. If $f$ is monotone increasing and $f$ is differentiable at $x_{0}$ , then $f'(x_{0}) \geq 0$ . If $f$ is monotone decreasing and $f$ is differentiable at $x_{0}$ , then $f'(x_{0})\leq 0$ . MY ATTEMPT Lemma Let $X\subseteq\textbf{R}$ , $f:X\rightarrow\textbf{R}$ , $g:X\rightarrow\textbf{R}$ , $x_{0}\in X$ is an adherent point, $f(x) \leq g(x)$ for every $x\in X$ and $\displaystyle\lim_{x\rightarrow x_{0}}f(x) = L$ and $\displaystyle\lim_{x\rightarrow x_{0}}g(x) = M$ . Then we have that $L \leq M$ . Proof According to the definition of limit, for every $\varepsilon > 0$ , there are $\delta_{1} > 0$ and $\delta_{2} > 0$ such that \begin{align*}
\begin{cases}
0 < |x - x_{0}| < \delta_{1}\\\\
0 < |x - x_{0}| < \delta_{2}
\end{cases} \Longrightarrow
\begin{cases}
|f(x) - L| < \varepsilon\\\\
|g(x) - M| < \varepsilon
\end{cases} \Longrightarrow L - \varepsilon < f(x) \leq g(x) < M + \varepsilon
\end{align*} Let us assume that $L > M$ . In this case, we can choose $\displaystyle\varepsilon = \frac{L - M}{3}$ , whence we get that \begin{align*}
M - L + 2\varepsilon > M - L + \frac{2(L - M)}{3} = \frac{M - L}{3} > 0 \Longrightarrow M > L
\end{align*} which leads to a contradiction. Therefore the original claim is true and $L \leq M$ . Solution Assuming that $f$ is monotone increasing at $x_{0}$ , we have that \begin{align*}
\frac{f(x) - f(x_{0})}{x - x_{0}} \geq 0
\end{align*} Taking the limit from both sides to $x_{0}$ we conclude that \begin{align*}
\lim_{x\rightarrow x_{0}}\frac{f(x) - f(x_{0})}{x - x_{0}} = f'(x_{0}) \geq 0 = \lim_{x\rightarrow x_{0}}0
\end{align*} simliar reasoning applies to the monotone decreasing case, and we are done. Could someone please verify if I am arguing correctly? Any other solution is welcome.","['alternative-proof', 'solution-verification', 'derivatives', 'real-analysis']"
3652565,n-th power of a matrix using the division of polynomials.,"Consider the matrix $$
A=\begin{pmatrix}
0 & 0 & 0\\
-2 & 1 & -1\\
2 & 0 & 2
\end{pmatrix}
$$ Calculate $A^3-3A^2+2A$ . What is the remainder of the division of the polynomial $X^n$ by the polynomial $X^3-3X^2+2X$ . Calculate $A^n$ for every natural number $n$ . I was solving the following problem and I was stuck in it. For part 1) the answer was the zero matrix. In part 2) I use the usual division and i get the following $$
X^n=X^{n-3}(X^3-3X^2+2X)+3X^{n-1}-2X^{n-2}.
$$ When I pass to part 3) and using part 1) and 2), we obtain $$
A^n=3A^{n-1}-2A^{n-2}.
$$ Using the fact that $A^3-3A^2+2A=O_{3\times 3}$ . but if I use this answer for calculating $A^2$ the answer is not correct, so I think $A^n$ obtained is not correct. Now, one can use the diagonalization of the matrix $A$ and obtain $$
A^n=\begin{pmatrix}
0 & 0 & 0\\
-2^n & 1 & 1-2^n\\
2^n & 0 & 2^n
\end{pmatrix}
$$ Can you help me in proving part 2 (if not correct) and part 3 without using the diagonalization method.","['matrices', 'matrix-calculus', 'linear-algebra', 'polynomials']"
3652603,Calculate the perimeter of a circle with a continuously increasing radius,"I have a circle (if that would even be the correct name for this shape), with a radius function equal to $R=\frac 2\theta + 1$ , where $\theta$ is the angle in radians. The domain is between $\theta = 0.25$ and $\theta = 2\pi$ . How do I calculate the total outside perimeter of this shape? I tried to divide it into individual sectors, and find the arc length, but couldn't find a way due to the two radii (in each sector) being different. The main question is what is a generalised method that could be used to calculate the perimeter of the shape, with an increasing radius (which increases with the angle relative to a starting direction)? The exact function isn't too important.","['algebra-precalculus', 'circles', 'trigonometry']"
3652618,$\operatorname{Hom}_B$ of flat modules is flat over $A$?,"Let $(A, \mathfrak{m}) \rightarrow (B,\mathfrak{n})$ be a local homomorphism of noetherian local rings and let $M$ be a finitely generated $B$ -module flat over $A$ . Suppose moreover that $B$ is also of finite type and flat over $A$ . Q: Is it true that $\operatorname{Hom}_B(M,B)$ is flat over $A$ ? I've tried to use the flatness of $M$ and $B$ to show that the natural map $$\mathfrak{m}\otimes_A \operatorname{Hom}_B(M,B) \longrightarrow \operatorname{Hom}_B(M,B)$$ is injective (to apply the Local Criterion for flatness) but I could not prove that neither find a counterexample. ADDED: Here follows an naive idea I had. Consider the free resolution of $A/\mathfrak{m}$ : $$
\cdots\longrightarrow A^k  \overset{R}{\longrightarrow} A^n  \overset{G}{\longrightarrow} A \longrightarrow A/\mathfrak{m} \longrightarrow 0
$$ where $G= (a_1, \dots, a_n)$ is given by the generators of $\mathfrak{m}$ and $R= (r_{ij})$ is given by the first syzygies. Then we have $$
\operatorname{Hom}_B(M,B)^k  \overset{R}{\longrightarrow} \operatorname{Hom}_B(M,B)^n  \overset{G}{\longrightarrow} \operatorname{Hom}_B(M,B)
$$ and $\operatorname{Tor}^A_1 \left( A/\mathfrak{m}, \operatorname{Hom}_B(M,B)\right)= \ker G / \operatorname{im} R$ . We just need to see if $\ker G \subset \operatorname{im} R$ . Let $(f_1, \dots, f_n)\in \ker G$ i.e. $\sum_j a_jf_j =0$ . Then for every $x\in M$ we have $\sum_j a_jf_j(x) =0$ and since $B$ is flat there exist $(g_1(x), \cdots, g_k(x)) \in B^k$ such that $$
f_i(x) = \sum_j r_{ij}g_j(x)
$$ and we have $g_j \colon M \longrightarrow B$ maps of sets. The problem is reduced to show whether we can produce $B$ -homomorphisms this way. Note that when $M$ is free (as a $B$ -module) then we only need to define $g_j$ on generators but in general it is not true. Also note that the flatness of $M$ was not used so far.","['algebraic-geometry', 'flatness', 'commutative-algebra', 'modules']"
3652668,Definition of the sinusoid in terms of differential equation,"Let us define $\sin(x)$ as the solution to $$y'' = -y$$ with initial conditions $y(0) = 0$ and $y'(0) = 1$ . From this definition, we should be able to deduce from first principles that $\sin(x)$ is periodic, and reconcile this definition with the right-triangle definition of opposite over hypotenuse. Any ideas on how to do this?","['trigonometry', 'axiomatic-geometry', 'geometry', 'ordinary-differential-equations']"
3652778,Definite Integral of $\int_{\frac{-1}{2}}^\frac{1}{2}\int_{\frac{-1}{2}}^\frac{1}{2} \sqrt{x^2+y^2} dxdy$,"Here's my attempt at trying to evaluate the integral. Let $x = y tan\theta$ $$\frac{dx}{d\theta} = \frac{y}{cos^2\theta}$$ $$dx = \frac{y}{cos^2\theta}d\theta$$ The new bounds of inner integral would be $\theta = tan^-(\frac{1}{2y}) $ and $\theta = tan^-(\frac{-1}{2y})$ $$\int_{\frac{-1}{2}}^\frac{1}{2}\int_{tan^-(\frac{-1}{2y})}^{tan^-(\frac{1}{2y})} y^2 sec^3{\theta} dy$$ Evaluating the innermost integral $$\int_{tan^-(\frac{-1}{2y})}^{tan^-(\frac{1}{2y})} sec^3{\theta} dy =  {\huge|} \frac{sec\theta tan\theta + ln|sec\theta + tan\theta|}{2}{\huge|}_{tan^-(\frac{-1}{2y})}^{tan^-(\frac{1}{2y})}$$ $$\begin{multline} = \left( \frac{sec(tan^-(\frac{1}{2y})) tan(tan^-(\frac{1}{2y})) + ln|sec(tan^-(\frac{1}{2y})) + tan(tan^-(\frac{1}{2y}))|}{2}\right)  
 - \\ \left( \frac{sec(tan^-(\frac{-1}{2y})) tan(tan^-(\frac{-1}{2y})) + ln|sec(tan^-(\frac{-1}{2y})) + tan(tan^-(\frac{-1}{2y}))|}{2}\right)  \end{multline}$$ $$\begin{multline} = \left( \frac{\frac{\sqrt{4y^2 + 1}}{4y^2} + ln|\frac{\sqrt{4y^2 + 1} + 1}{2y}|}{2}\right)  
 - \left( \frac{-\frac{\sqrt{4y^2 + 1}}{4y^2} + ln|\frac{\sqrt{4y^2 + 1} - 1}{2y}|}{2}\right)  \end{multline}$$ $$\begin{multline} = \frac{1}{2}\left[\left(\frac{\sqrt{4y^2 + 1}}{4y^2} + ln|\frac{\sqrt{4y^2 + 1} + 1}{2y}|\right)  
 - \left(-\frac{\sqrt{4y^2 + 1}}{4y^2} + ln|\frac{\sqrt{4y^2 + 1} - 1}{2y}|\right) \right] \end{multline}$$ $$\begin{multline} = \frac{1}{2}\left[\left(\frac{\sqrt{4y^2 + 1}}{2y^2} + ln|\sqrt{4y^2 + 1} + 1| 
 - ln|\sqrt{4y^2 + 1} - 1| \right) \right] \end{multline}$$ $$\begin{multline} = \frac{1}{2}\left[\left(\frac{\sqrt{4y^2 + 1}}{2y^2} + ln|\frac{\sqrt{4y^2 + 1} + 1}{\sqrt{4y^2 + 1} - 1}| \right) \right] \end{multline}$$ Evaluating the outermost integral $$\begin{multline} \int_{\frac{-1}{2}}^\frac{1}{2}\frac{\sqrt{4y^2 + 1}}{4} + y^2 \frac{ln{\large|}\frac{\sqrt{4y^2 + 1} + 1}{\sqrt{4y^2 + 1} - 1}{\large|}}{2} dy \end{multline}$$ I am kinda stuck at this point, any help is greatly appreciated -:). I may have done something wrong in the steps above.","['multivariable-calculus', 'definite-integrals']"
3652843,Incorrect theorem: $\lambda$-system implies $\sigma$-algebra. What's wrong?,"I am currently taking a course on probability theory for mathematicians where we're doing some measure theory. I've been thinking about how it is that if $\mathcal{L}$ is a $\lambda$ -system and a $\pi$ -system, then $\mathcal{L}$ is a $\sigma$ -algebra (the converse is very straightforward). Before explaining what I mean, I should point out that we have taken the following definition of $\lambda$ -system. $\mathcal{L} \in \mathcal{P}(\Omega) $ is a $\lambda$ -system iff $\Omega \in \mathcal{L}$ $A, B \in \mathcal{L}$ and $A \subseteq B \Rightarrow B \setminus A \in \mathcal{L}$ $A_1, A_2, \ldots \in\mathcal{L}$ such that $A_n \uparrow A \Rightarrow A \in \mathcal{L}$ Suppose $\mathcal{L}$ is a $\lambda$ -system. For $\mathcal{L}$ to be a $\sigma$ -algebra, in addition to the easy-to-check fact that for any set in $\mathcal{L}$ , its complement is also in $\mathcal{L}$ , the enumerable union of an arbitrary collection of sets in $\mathcal{L}$ must also be in $\mathcal{L}$ . So I started by taking two arbitrary sets. Let $A, B \in \mathcal{L}$ . Suppose $D_1 = A, D_j = A \cup B, \forall j \in \mathbb{N}-\{1\}$ . Then $D_1 \subseteq D_2 \subseteq D_3 \subseteq \ldots $ and clearly $\cup_{j \in \mathbb{N}} D_j = A \cup B$ . This would mean that $D_j \uparrow (A \cup B)$ , so property $(3)$ above would imply that $A \cup B \in \mathcal{L}$ . But then if I already had this for two sets, I could generalize for finite unions. Moreover, if $A_1, A_2, \ldots \in L$ , then $$\bigcup\limits_{j=1}^n A_j \uparrow \bigcup\limits_{j=1}^\infty A_j. $$ Again, property $(3)$ would imply that $\bigcup\limits_{j=1}^\infty A_j \in \mathcal{L}$ . Clearly there is something wrong, since not all $\lambda$ -systems are $\sigma$ -algebras . I would really appreciate that you point out any mistakes in the reasoning above.","['measure-theory', 'probability']"
3652930,"Constructing a nonzero bounded linear functional on $L^\infty[0,1]$ that vanishes on $C[0,1]$","I want to provide an example of a nonzero bounded linear functional on $L^\infty[0,1]$ that vanishes on $C[0,1]$ . I know that the existence of such a functional is guaranteed by the Hahn-Banach Theorem. My issue is with actually constructing one. For example, I know $C[0,1]$ is a subspace of $L^\infty[0,1]$ , so I tried defining a functional $f$ on $C[0,1]$ as $f(u)=\lim_{x\to\frac{1}{2}^+}u(x)-\lim_{x\to\frac{1}{2}^-}u(x)$ . This vanishes on $C[0,1]$ but the one-sided limits are not necessarily well-defined on $L^\infty[0,1]$ . I also thought of considering $L:=$ the span of $C[0,1]$ and some element in $L^\infty[0,1]\backslash C[0,1]$ like $\chi_{[0,\frac{1}{2}]}$ . Then for $y=u+\lambda\chi_{[0,\frac{1}{2}]}\in L$ , I can define $f(y)=\lambda$ . $f$ vanishes on $C[0,1]$ and $||f||\neq0$ and I know I can extend $f$ to $\phi$ on $L^\infty[0,1]$ but I want to be able to know what $\phi$ looks like explicitly.
Maybe that is not possible for this $\phi$ , but is there an example of some other $\phi$ that is explicitly defined for every element in $L^\infty[0,1]$ ?","['hahn-banach-theorem', 'functional-analysis', 'examples-counterexamples']"
3653021,"For how many $b\in \{1,\dots,a\}$ is $1,11,111,\ldots$ (in base $b$) a complete residue system mod $a$? For what fraction of $a$ is the answer 1?","Disclaimer: I am definitely a number theory novice (the vague memory of one college course comprises the totality of my knowledge on the subject). As such, I apologize in advance for any mistakes or oversights. This question is based on a blog post I wrote, ending in a question I wasn't able to solve. The description there is a little bit circuitous, so I will try to describe things more succinctly here. For any integers $k,b>0$ , let $c(k,b)=11\dots11_b=\sum_{i=0}^{k-1}b^i$ . For any integer $a>0$ , let $R(a)$ denote the number of $b\in \{1,\dots,a\}$ such that $\{c(1,b),\dots,c(a,b)\}$ is a complete residue system (mod $a$ ). In particular, this requires that $\gcd(a,b)=1$ , so we have $1\leq R(a)\leq \varphi(a)$ . So a natural question seems to be, when are these bounds achieved? On one end, we have $R(a)=\varphi(a)$ if and only if $a=2$ . But the question of when $R(a)=1$ seems to be harder. We can show that $R(p)=1$ for any prime $p$ (this follows from Problem 3 in my post), so there are infinitely many minimizing values. But I'm curious about what fraction of positive integers (if it converges) satisfy $R(a)=1$ . More specifically, does the limit $$\lim_{n\rightarrow \infty}\frac{\#\big(R^{-1}(1)\cap \{1,\dots,n\}\big)}{n}$$ converge? If so, what does it converge to? Using Matlab, I was able to calculate up to $n=5000$ and it seems to tend towards about $71\%$ . But I have no intuition as to why, or if it will continue…","['number-theory', 'elementary-number-theory']"
3653029,Interchanging an integrand and a measure,"Suppose that $f$ and $g$ are continuous functions on a compact set $K$ and $\mu$ is a (complex) measure. How does the equality $$\int_K f\ \mathrm dg \cdot \mu = \int_K fg\ \mathrm d \mu$$ can be justified, i.e., what does "" $\cdot$ "" mean here? I stumbled upon this issue when studying the proof of the auxiliary result d) here . What I already found is that it is possible to multiply a function with a measure, as explained in this question. Am I right that the above equality is a consequence of this?","['measure-theory', 'lebesgue-integral', 'real-analysis', 'complex-analysis', 'functional-analysis']"
3653082,Logarithmic Laplace transform of the uniform measure on a convex set,"This question comes from a lemma (whose proof is left as an exercise) which I came across when reading the continuous exponential-weighting algorithm. Let $\mathcal K\subset\mathbb R^d$ be a compact convex set with finite volume: $|\mathcal K|<\infty$ . Let $u\in\mathbb R^d$ be a fixed vector and define $x^*\in\arg\min_{x\in\mathcal K}\langle x, u\rangle$ . Prove that $$
-\log\left(\frac{1}{|\mathcal K|}\int_{\mathcal K}e^{-\langle x-x^*, u\rangle}dx\right)\leq 1+\max\left(0, d\log\left(\sup_{x,y\in\mathcal K}\langle x-y, u\rangle\right)\right).
$$ Can someone give me some hint how to prove this inequality? I feel confused how $d$ appears in the bound. Thanks!","['convex-geometry', 'measure-theory', 'functional-analysis', 'probability']"
3653091,If $0<t<1$ then $0<t^n<1$,"Let $(F, +, \cdot, -,^{-1},0,1,<)$ be an ordered field, that is: There exists a binary operation $+$ on $F$ such that $\forall x, y\in F, x+y\in F$ $\forall x,y\in F, x+(y+x)=(x+y)+x$ $\forall x\in F, \forall y\in F, x+y=y+x$ $\exists 0\in F: \forall x\in F, x+0=x$ $\forall x\in F, \exists -x\in F: x+(-x)=0$ There is a binary operation $\cdot$ on $F$ such that: $\forall x,y\in F, x\cdot y\in F$ $\forall x,y\in F, x\cdot y=y\cdot x$ $\forall x,y\in F, x\cdot (y\cdot z)=(x\cdot y)\cdot z$ $\exists 1\in F,:\forall x\in F, x\cdot 1=x$ $\forall x\in F\exists x^{-1}\in F: x\cdot x^{-1}=1$ $\forall x,y,z\in F, x\cdot (y+z)=x\cdot y+x\cdot z$ Let $<$ be a total and transitive order on $F$ , such that if $x,y,z\in F$ and $y<z$ then $x+y<x+z$ if $x,y\in F$ , $x>0$ and $y>0$ then $xy>0$ Is it possible to prove that, for every $t\in F$ , if $0<t<1$ then $t^n<1$ for every integer $n>1$ ? EDIT: if necessary, suppose that $F$ has the least-upper-bound property.","['algebra-precalculus', 'analysis']"
3653132,"Soft Question - Book Recommendations (Diff Geo, Bose-Einstein stats etc.)","I apologise immediately for the soft question but I will still ask it. I feel there may be a lot of people in the same boat so it may be relevant to a large number of others. With context of this coronavirus stuff going on, it's the perfect time to get some extra reading done. I'm currently a second-year maths student, with exams in the next few weeks, and am looking for some recommendations on some 'mathematical physics' type books for after. I'm pretty far through Wald's GR (some of the questions are a pain but that's another story), and am wondering if anyone could suggest some books on, for example, Differential Geometry or Bose-Einstein Statistics (personally I have a book on QM that I'm getting through, albeit slower than GR due to interest reasons, however it may be helpful for others if you have a suggestion for that area too). I would love to have some recommendations on generally anything that you may have wished you'd read about during your undergrad, or even just a fantastic book on an area of mathematics in general. Again, apologies for the soft question.","['statistical-mechanics', 'soft-question', 'reference-request', 'differential-geometry']"
3653142,"Non equivalent colourings of regular hexagon( Brualdi Chapter-14 , Exercise -32)","I have a question in this exercise of Richard Brualdi's Introductory Combinatorics. Exercise is -> Determine the number of non equivalent colourings of corners of regular hexagon with colours red, white and blue. Now, Taking motivation from this example solved in text
Adding image of example-> What I calculated  for regular hexagon $ N(D_6, C) $ = $\frac { 3^6 + 5×3 + 6 × 3^3} {12}$ =75.5 . It seems I am making some mistake as non equivalent colouring comes out to be fractional. I tried to solve it again, but I am getting same answers. Can some please tell what I am doing wrong.","['coloring', 'polya-counting-theory', 'combinatorics', 'discrete-mathematics']"
3653203,"Why can a linear ""ordinary"" differential equation have non-linear coefficients of independent variable?","The confusion origins from the fact that $y$ = $x^2$ + $x$ + $1$ is a non linear equation but $y\,'$ = $x^2$ + $x$ + $1$ is a linear differential equation. Why is the non-linearity in independent variable not significant in the case of differential equations? Does the word linear have different meanings for ""normal"" (not differential) equations and differential equations? What would be the best way to make some geometric sense of linear differential equations? (like in the case of linear equation in two variable it is a line.) Mentions of ""differential equation(s)"" in the above questions refer to only the subset of ""ordinary differential equation(s)""","['calculus', 'derivatives', 'ordinary-differential-equations', 'differential-geometry']"
3653212,Question about number of non equivalent colourings of corners of a regular tetrahedron with k colours,"Due to Covid -19 , in our university quizzes are held online and it's hard to ask questions. 3 Days back in my Combinatorics quiz this question was asked on which I am struck. I couldn't solve it in the time alloted and struggled to find a proper strategy. Question is ->Determine the number of non equivalent colourings of the corners of regular tetrahedron with k different colours. My attempt -> I am trying to solve it by Burnside Theorem ( Number of non equivalent colourings in C are given by N(G, C) = 1/ |G| $\sum_{f \epsilon G } | C(f) | $ . [C(f) = set of all colourings in C that are fixed by f ] Group of permutations is $S_4$ and all $ (k^4)$ will be fixed by identity . But I am not able to think how to find colourings fixed by each permutation caused due to rotations and reflections. I have done it for pentagon which was easy. Can someone please tell a way on how to efficiently and elegentally compute the value of C(f) in case of rotations and reflections. I will be really thankful for the ideas.","['coloring', 'polya-counting-theory', 'combinatorics', 'discrete-mathematics']"
3653249,Prove that $\sum_{n=2}^{\infty} \frac{(-1)^{n}}{n}\zeta(n) = \gamma$,"How do you prove that $$\sum_{n=2}^{\infty} \frac{(-1)^{n}}{n}\zeta(n) = \gamma$$ where $\gamma$ is the Euler-Macheroni constant ? This series kind of appeared in one of the questions I asked earlier ; you just need to do some rearranging to get to this series. Here is WolframAlpha calculating the series. I believe I have almost proved it (my calculations below), but I'm unsure at the end. I would also like to see if there is some other way of proving them (I don't think there is, but it would be cool). My ""proof"" (not sure if it right): \begin{align} \sum_{n=2}^{\infty} \frac{(-1)^{n}}{n}\zeta(n) & =\sum_{n=2}^{\infty}\frac{(-1)^{n}}{n}\sum_{k=1}^{\infty}\frac{1}{k^n} \\\\ & = \sum_{n=2}^{\infty}\sum_{k=1}^{\infty}\frac{(-1)^{n}}{n}\frac{1}{k^n}\end{align} Here I interchange the summations (is it possible to do so?): \begin{align} \sum_{n=2}^{\infty}\sum_{k=1}^{\infty}\frac{(-1)^{n}}{n}\frac{1}{k^n} & = \sum_{k=1}^{\infty}\sum_{n=2}^{\infty}\frac{(-1)^{n}}{n}\frac{1}{k^n} \\\\ & = \sum_{k=1}^{\infty} \left(\frac{1}{k} + \sum_{n=1}^{\infty}\frac{(-1)^{n}}{n}\frac{1}{k^n}\right)\end{align} Recall the Taylor series for $\ln(x)$ : $$\ln(1+x) = \sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n}x^n$$ By plugging in $\frac{1}{x}$ , changing $x$ to $k$ and multiplying by $-1$ on both sides we get: $$-\ln\left(\frac{k+1}{k}\right) = \sum_{n=1}^{\infty}\frac{(-1)^{n}}{n}\frac{1}{k^n}$$ ...which is exactly what we need. So plugging the result into the series above we get: \begin{align} \sum_{k=1}^{\infty} \left(\frac{1}{k} + \sum_{n=1}^{\infty}\frac{(-1)^{n}}{n}\frac{1}{k^n}\right) & = \sum_{k=1}^{\infty} \left(\frac{1}{k} - \ln\left(\frac{k+1}{k}\right)\right) \\\\ & = \sum_{k=1}^{\infty} \frac{1}{k} - \sum_{k=1}^{\infty}\ln\left(\frac{k+1}{k}\right) \end{align} Recall the definition of the Euler-Macheroni constant: $$\gamma = \lim_{n\to\infty}(H_n - \ln(n))$$ Now clearly the term $\sum_{k=1}^{\infty} \frac{1}{k}$ is the $H_n$ part of the definition, but here's is where I get a little stuck; how does $$\sum_{k=1}^{\infty}\ln\left(\frac{k+1}{k}\right) = \lim_{k\to\infty}\ln(k)$$ Otherwise I think my proof is quite correct, but can anybody help at the end of it?","['riemann-zeta', 'harmonic-numbers', 'sequences-and-series']"
3653277,$\nabla_a \nabla_b v^c$ abstract index notation,"I am having some trouble with the equivalence between abstract index notation (AIN) and standard tensor component notation (TCN, for short). Let us consider a covariant derivative $\nabla$ . In TCN, we can define it, for a given basis of tangent vectors $e_i$ , by the relation $$
\nabla_i e_j = \Gamma_{ij}^k e_k\,,
$$ where $\nabla_i=\nabla_{e_i}$ .
Then, for any vector field $v=v^i e_i$ , we have $$
\nabla_i v = (\partial_iv^j + \Gamma^j_{ik}v^k)e_j \implies (\nabla_i v)^j=\partial_iv^j + \Gamma^j_{ik}v^k
$$ since $\nabla_i(v^j e_j)=(\nabla_i v^j) e_j + v^j(\nabla_i e_j)$ and $\nabla_i v^j=\partial_i v^j$ .
In AIN, this translates to $$
\nabla_a v^b = \partial_a v^b + \Gamma_{ac}^b v^c\,,
$$ where however $a, b,c,\ldots$ are just abstract indices and not components with respect to a specific basis. Let us now consider the second derivative. In TCN, we have $$
\nabla_i \nabla_j v = \nabla_i((\partial_j v^k+ \Gamma^k_{jl}v^l)e_k)=(\partial_i(\partial_j v^k+ \Gamma^k_{jl}v^l)+(\partial_j v^m+ \Gamma^m_{jl}v^l)\Gamma_{im}^k)e_k\,.
$$ Therefore, $$
(\nabla_i \nabla_j v)^k=\partial_i \partial_j v^k + v^l \partial_i \Gamma^k_{jl}+\Gamma^k_{jl}\partial_i v^l + \Gamma_{il}^k \partial_j v^l+ \Gamma^k_{im}\Gamma^{m}_{jl}v^l\,.
$$ On the other hand, in AIN, $$
\nabla_a \nabla_b v^c = \partial_a \nabla_b v^c + \Gamma^c_{ad}\nabla_b v^d-\Gamma_{ab}^d \nabla_d v^c\,,
$$ because we need to treat the lower $b$ index in $\nabla_b$ according to its covariant nature, and therefore $$
\nabla_a \nabla_b v^c = \partial_a \partial_b v^c + v^d\partial_a\Gamma^c_{bd} + \Gamma^c_{bd}\partial_a v^d + \Gamma^c_{ad}\partial_b v^d + \Gamma^c_{ad}\Gamma^{d}_{be}v^e-\Gamma_{ab}^d(\partial_d v^c + \Gamma_{de}^c v^e)\,.
$$ But the last term is not there in TCN! Do I have to conclude that somehow these two notations are not really equivalent? (I always assumed they were...)
The unwanted piece cancels in the calculation of the commutator $[\nabla_a,\nabla_b]v^c$ , provided the connection is symmetric, so this is not really an issue in GR, but still I would like to understand what goes wrong.","['connections', 'tensors', 'index-notation', 'differential-geometry']"
3653385,Explicit description of all extensions of $\mathbf{Z}/n\mathbf{Z}$ by $\mathbf{Z}$,"In an exercise in my syllabus on homological algebra, I need to explicitly describe what are the $n$ isomorphism classes of extensions of $\mathbf{Z}/n\mathbf{Z}$ by $\mathbf{Z}$ for the cases $n=p$ prime, $n=pq$ with $p,q$ distinct primes and $n=4$ . (The group $\operatorname{Ext}_\mathbf{Z}^1 (\mathbf{Z}/n\mathbf{Z},\mathbf{Z})$ .) For $n=p$ , I succeeded in classifying these: there are $p$ short exact sequences of the form $$0\longrightarrow \mathbf Z\stackrel{\times p}{\longrightarrow} \mathbf Z\stackrel{f}{\longrightarrow} \mathbf Z/p\mathbf Z\longrightarrow 0$$ with $f:1\mapsto \overline{a}$ with $a\in \{1,\ldots,p-1\}$ and we have the split extension $$0\longrightarrow \mathbf Z\stackrel{}{\longrightarrow} \mathbf Z \oplus \mathbf Z/p\mathbf Z\stackrel{}{\longrightarrow} \mathbf Z/p\mathbf Z\longrightarrow 0$$ which are clearly distinct isomorphism classes. For $n=4$ , I can only find three: $$0\longrightarrow \mathbf Z\stackrel{\times 4}{\longrightarrow} \mathbf Z\stackrel{\pi_i}{\longrightarrow} \mathbf Z/4\mathbf Z\longrightarrow  0$$ where $\pi_1:x\mapsto \overline{x}$ and $\pi_2:x\mapsto \overline{-x}$ , and the split extension $$0\longrightarrow \mathbf Z\stackrel{}{\longrightarrow} \mathbf Z \oplus \mathbf Z/4\mathbf Z\stackrel{}{\longrightarrow} \mathbf Z/4\mathbf Z\longrightarrow  0.$$ For $n=pq$ , we again have the split extension and we can mimic what we did for $p$ prime to obtain $(p-1)(q-1)$ non-equivalent extensions of the form $$0\longrightarrow \mathbf Z\stackrel{\times pq}{\longrightarrow} \mathbf Z\stackrel{f}{\longrightarrow} \mathbf Z/pq\mathbf Z\longrightarrow 0$$ with $f:1\mapsto \overline{a}$ with $a\in \{1,\ldots,p-1\}\times \{1,\ldots,q-1 \}$ . Any help is appreciated!","['homological-algebra', 'group-theory', 'abstract-algebra', 'exact-sequence']"
3653415,Good books on stacks,"Can somebody give me a good reference book on stacks? I learned my algebraic geometry mostly with Görtz and Wedhorn's book, and I wonder if there's a book on the theory of stacks that is equally comprehensive?","['algebraic-stacks', 'algebraic-geometry', 'reference-request']"
3653419,"If $f$ increasing, analytic on $\mathbb{R}$ and $\lim_{x\to +\infty}f(x)=1$, does it follows that $\lim_{x\to +\infty}f'(x)=0$?","Question: If $f$ strictly increasing, analytic on $\mathbb{R}$ and $\lim_{x\to +\infty}f(x)=1$ , does it follows that $\lim_{x\to +\infty}f'(x)=0$ ? If we drop the assumption that the function is increasing, an easy counterexample is $a(x)=\frac{\sin(x^2)}{x}+1$ . If we drop the analyticity requirement (but keep $C^{\infty}$ ) a counterexample can be constructed from $$h(x)=\begin{cases}0&x\le 0\\\exp\left(\frac{-\exp(-1/{(x-1)^2})}{x^2}\right)&x\in (0,1)\\
1&x\ge 1\end{cases}$$ by setting $$b(x):=\text{sign}(x)\sum_{n=0}^{+\infty}\frac{h(2^n(|x|-n))}{2^{n+1}}$$ It is clear that, if $\lim_{x\to +\infty} f'$ exists, it must be $0$ : In fact, since $0=\lim_{x\to +\infty}\frac{f(x)-1}{x}=\lim_{x\to +\infty}f'(x)$ . Otherwise one can prove it by noting that, since $f'\ge 0$ and $1=\int_0^\infty f'(x)dx$ it is impossible to have $\lim_{x\to +\infty}f'(x)>0$ . However, I do not see how to prove the existence of the limit of how to construct a counterexample (as $b$ is not analytic)","['analytic-functions', 'monotone-functions', 'real-analysis']"
3653425,Question Regarding Jet Bundle,"I am reading an appendix on jet bundles, and I am confused on the following question. 
The note I am reading (Singularity of Mappings by Mond and Nuno-Ballesteros, Appendix A) says the following: There is also a jet bundle $J^k(X, Y)$ over any pair
of (complex or real) manifolds $X$ and $Y$ , whose fibre over $(x,y)\in X\times Y$ , which we denote by $J^k(X, Y )_{(x,y)}$ , is the set of $k$ -jets of germs of maps $(X,x)\to (Y,y)$ . My question is, how is this jet bundle $J^k(X, Y)$ defined ? And why this is a locally trivial fiber bundle? I think this is a principle $G$ bundle, where $G$ denotes the group of coordinate changing of $\mathbb{C}^n\times \mathbb{C}^p$ . How should I understand this?","['jet-bundles', 'differential-geometry']"
3653467,Vector fields on a sphere,"I am interested in what smooth vector fields on spheres look like. I am aware of the hairy ball theorem prohibiting the existence of nowhere vanishing smooth vector fields on a sphere, but I would like to permit zeros of the vector field. Are there any theorems on the number of zeros are allowed? Ultimately I would like to be able to write down the general form of a smooth vector field as a sum of basis functions (much like sines and cosines provide a basis for piecewise continuous functions on an interval). Any help e.g. answers with explanations or relevant references would be greatly appreciated. Thanks.","['vector-fields', 'differential-topology', 'spheres', 'differential-geometry']"
3653500,Solve $x(1-x)y''+2(1-2x)y'-2y=0$ by the Frobenius Method,"Find the second solution. First solution is $\dfrac 1 {1-x}$ .  Solve by Frobenius Method : $$x(1-x)y''+2(1-2x)y'-2y=0\,.$$ The first solution I am able to get is $\dfrac{1}{1-x}$ . Other solution is $\dfrac{1}{x}$ , but I am getting $-\dfrac{1}{x(1-x)}$ . Where am I going wrong?","['power-series', 'frobenius-method', 'calculus', 'ordinary-differential-equations']"
3653523,Equivalent definitions of total variation of a complex measure,"Let $(X,M,\mu)$ be a complex measure space. Then for $E \in M$ we have $|\mu|(E)=sup\{\Sigma_k| \mu(E_k)|: E = \sqcup_{k=1}^\infty E_k $ where $ E_k \in M\}$ Prove that for each $E\in M$ $$\begin{align*} 
|\mu|(E)&=\sup\left\{\sum_{k=1}^N |\mu(E_k)| : \{E_k\}_{k=1}^N \ \text{is a finite partition of} \ E\right\}\\
&=\sup\left\{\Big|\int_E f\,d\mu\Big| : f \ \text{is measurable and} \ |f|\leq 1\right\}\end{align*}$$ I've been having a real rough time trying to show these inequalities... I feel like I'm running into some sort of tunnel vision, I've been stuck on this for awhile now, I'd really appreciate it if someone could walk me through this one.. Thanks","['complex-analysis', 'measure-theory', 'analysis', 'real-analysis']"
3653570,Why does $f(z) = z^n$ have no antiderivative only for $n=-1$? [duplicate],"This question already has answers here : Antiderivative $1/z$ on $\mathbb C$ (3 answers) Closed 4 years ago . The complex valued function $f(z) = z^n$ has an analytic antiderivative on $\mathbb{C} \setminus \{0 \}$ for every $n$ except for $n=-1$ . What is so special about $-1$ ? To show why this is such an anomaly, imagine if $z^n$ had an analytic antiderivative on $\mathbb{C} \setminus \{0 \}$ for every $n$ except for $n=3456$ . People would demand to know what is so special about $3456$ . However, it seems like no one feels the need to explain the anomaly at $n = -1$ . What is going on at $-1$ ?","['complex-analysis', 'soft-question', 'intuition', 'analytic-functions']"
3653621,Does the ring of analytic functions have zero divisors?,"Question I have to show that the ring of complex analytic functions on open unit disk has no zero divisors. My attempt let suppose $fg≡0$ such that $f≢0$ and $g≢0$ on open unit disk $U$ then $f$ and $g$ have finitely many zeros on $U$ and so that $fg$ have finitely many zeros on $U$ and hence $fg≢0$ . Hence we must have either $f≡0$ or $g≡0$ . Hence given ring has no zero divisors. I am not that good in complex analysis. However i am familiar with abstract algebra. So please give details. Is my attempt correct? I didnt know, why $f$ and $g$ have finitely many zeros on $U$ ?  please elaborate this point too. Please help...","['complex-analysis', 'ring-theory', 'abstract-algebra', 'analytic-functions']"
3653623,How to construct an equilateral triangle on 2 concentric circles,"Construct an equilateral triangle with the given vertex so that the other vertices lie on the concentric circles respectively. I constructed the triangle, but I don't  know how it works. How does this construction work?  Is there any proof? My construction. Let the smaller circle be $a$ , the larger circle $b$ , and  the point $c$ . Step 1:  Construct a circle with radius of $b$ at the point $c$ . Step 2: The circle will intersect circle $a$ at $2$ points.  Let the two points be $x$ and $y$ .   Construct a perpendicular bisector of the line connecting $x$ and the common centre of circle $a$ and $b$ . Step 3:  The bisector intersect the circle $a$ at a point which is another vertex of the equilateral triangle. For more context, this is from the game "" Euclidea "" level 13.3.  Video solutions can be found here .","['euclidean-geometry', 'puzzle', 'geometric-construction', 'geometry', 'plane-geometry']"
3653634,Maximal operator inequality $P^*$,"The maximal operator function $P^*$ is defined in this way: DEF: If $f\in L^{p}(\mathbb{T})$ , $P^*f(x)=sup_{0<r<1}|P_{r}*f(x)|$ .Where $P_{r}(t)=\sum_{I=-\infty}^{\infty}r^{|i|}\phi_{i}(t)$ is the kernel of Poisson. But now, with this maximal operator, is possible to demonstrate that, for $1\le p<\infty$ , there is a constant $c_{p}>0$ such that, for all $f\in L^p(\mathbb{T})$ : $||P^*f||_{p}\le c_{p}||f||_{p}$ ? Thanks.","['complex-analysis', 'inequality', 'convolution', 'harmonic-functions']"
3653677,Factorise $x^4+y^4+(x+y)^4$,"I think this problem involves making use of symmetry in some way but I don't know how. I expanded the $(x+y)^4$ term but it din't help in the factorization. I am very bad at factorizing, so I didn't work too long on this problem. Any help would be appreciated, I want hints rather than a full solution. But even a full solution is okay.","['contest-math', 'factoring', 'polynomials', 'algebra-precalculus', 'binomial-theorem']"
3653681,How to prove ${n+2 \choose 3}=1\cdot n + 2 \cdot (n - 1) + \ldots + n \cdot 1$?,"I saw this problem as an exercise in Combinatorial Identities :- Prove that $${n+2 \choose 3}=1\cdot n + 2 \cdot (n - 1) + \ldots + n \cdot 1\,.$$ After giving some time to this, I think that it is quite similar to the identity :- ${n \choose k}  = {n - 1 \choose k - 1} + {n - 1 \choose k}$ But I don't know how to prove this algebraically , anyone please help me with this. (Note that I am still not sure whether we can use that identity or not , I can also guess we can use Vandermonde's Identity here) .","['summation', 'binomial-coefficients', 'combinatorics', 'combinatorial-proofs']"
3653683,Differentiating $y=\exp(x+\exp(x+\exp(x+\exp(x+\cdots))))$ and writing derivatives of $\exp$ functions in terms of $y$ only,"I want to differentiate $$y=\exp(x+\exp(x+\exp(x+\exp(x+\cdots)))).$$ Is the following substitution and differentiation appropriate? \begin{align}
y&=\exp(x+\exp(x+\exp(x+\exp(x+\cdots))))\\
y&=\exp(x+y)\\
\frac{dy}{dx}&=\exp(x+y)\left(1+\frac{dy}{dx}\right)\\
\frac{dy}{dx}&=y\left(1+\frac{dy}{dx}\right)\\
\frac{dy}{dx}&=\frac{y}{1-y}\\
\end{align} As well, I notice that I can write the derivative (and therefore any higher order derivative) in terms of $y$ only.  What is required of the $\exp$ function such that I will not be able to write the derivative in terms of $y$ only?  It does not seem like the reason is that the $x$ in the argument is just $x$ and not some function of $x$ :  for example, \begin{align}
y&=\exp x^2\qquad\quad\implies x=\log y-\log2\\
\frac{dy}{dx}&=2x\exp x^2\\
\frac{dy}{dx}&=2xy\\
\frac{dy}{dx}&=2y\,(\log y-\log2)\\
\end{align} and now the derivative is free of $x$ .  Knowing when this is the case would be useful in constructing problems where we can find the slope of the tangent line given only a $y$ value.","['calculus', 'implicit-differentiation', 'derivatives', 'exponential-function']"
3653689,"Discrete Mathematics, proving a composed function is bijective","Given a function $f : X \rightarrow X $ It is known that $f^5 = I$ where $I(x) = x$ (The identity function) I need to prove that $f$ is bijective (Injective and Surjective) $f^n = f \circ f \circ f ...\circ f$ I've tried many ways such as finding out whether or not $x_1, x_2 \in X$ exist such that $f(x_1) = f(x_2)$ but it did not help as the function was composed with itself (5 times) and I got stuck...","['functions', 'discrete-mathematics']"
3653702,Sum over invertible 0-1 matrices,"I stumbled across the following formula when working on a research problem in theoretical computer science. I checked its correctness up to $N=5$ with a computer. I am looking for a simple proof of it. This question has been transfered to MathOverflow . Basic version Let $\mathcal M_N$ be the set of all invertible 0-1 square matrices of size $N$ . More formally, one could write $\mathcal M_N = \{0,1\}^{N\times N} \cap GL_N(\mathbb R)$ . $$
\sum_{M \in \mathcal M_N}
\frac{\det(M)^2 \cdot (-1)^{\|M\|_0 - N}}
{\prod_{i=1}^N\Big(\sum_{j=1}^N M_{i,j}\Big)\prod_{j=1}^N\Big(\sum_{i=1}^N M_{i,j}\Big)} = 1
$$ where $\|M\|_0 = \sum_{i,j} M_{i,j}$ is the number of non-zero entry of $M$ . Weighted gerenalization Note that the formula is also true when a ""positive weight"" is associated to every coefficient. Let $P$ be any matrix with positive coefficients. Let $P \circ M$ be the elementwise product of $P$ and $M$ . Redefine $\mathcal M_N$ to be the set of all 0-1 square matrices without any row/column of zeros (one can also define $\mathcal M_N$ to be A227414 ) $$
\sum_{M \in \mathcal M_N}
\frac{\det(P \circ M)^2 \cdot (-1)^{\|M\|_0 - N}}
{\prod_{i=1}^N\Big(\sum_{j=1}^N [P \circ M]_{i,j}\Big)\prod_{j=1}^N\Big(\sum_{i=1}^N [P \circ M]_{i,j}\Big)} = 1
$$ Here is some python code to check (empirically) my claim (slow when $N > 4$ ). from sympy import Matrix
from itertools import product
N = 2
result = 0
P = Matrix([[4,2],[13,37]])
for p in product([0,1], repeat=N**2):
  M = Matrix(p).reshape(N, N).multiply_elementwise(P)
  val = (-1) ** (sum(p)-N) * M.det() ** 2
  if val != 0:
    for i in range(N):
      val /= sum(M[:,i])
      val /= sum(M[i,:])
    print(M, val)
    result += val
print(result) And for those of you who don't want to run this program, here is the output. Matrix([[0, 2], [13, 0]]) 1
Matrix([[0, 2], [13, 37]]) -1/75
Matrix([[4, 0], [0, 37]]) 1
Matrix([[4, 0], [13, 37]]) -74/425
Matrix([[4, 2], [0, 37]]) -74/117
Matrix([[4, 2], [13, 0]]) -13/51
Matrix([[4, 2], [13, 37]]) 3721/49725
1","['matrices', 'summation', 'combinatorics']"
3653715,How to prove that we can solve limits by substitution?,"I am currently learning analysis and my professor used substitution to solve a lot of limit problems, so I want to know under what circumstances can we use substitution and how to prove it. Example: $\lim_{x\rightarrow 0}\frac{\sin x^2}{x^2}=\lim_{u\rightarrow 0}\frac{\sin u}{u}$ by substitute $u=x^2$ Here is my attempt. My understanding of limit solving by substitution is that $\lim_{x\rightarrow a}u(x)=b\implies\lim_{x\rightarrow a}f(u(x))=\lim_{u\rightarrow b}f(u)$ Proof(probably wrong): Suppose $\lim_{x\rightarrow a}u(x)=b$ and $\lim_{x\rightarrow a}f(u(x))=L$ then $\forall\epsilon\gt 0 \exists\delta_1$ s.t $0\lt|x-a|\lt\delta_1\implies|f(u(x))-L|\lt\epsilon$ then $\forall\delta_1\gt 0 \exists\delta_2$ s.t $0\lt|x-a|\lt\delta_2\implies|u(x)-b|\lt\delta_1$ then fix $\delta=\min(\delta_1,\delta_2)$ we have $\forall\epsilon\gt 0 \exists\delta$ s.t $0\lt|x-a|\lt\delta$ implies $|f(u(x))-L|\lt\epsilon$ and $|u(x)-b|\lt\delta_1$ Since $P\wedge Q\implies(P\implies Q)$ we have $\lim_{u\rightarrow b}f(u)=L$ and do the same thing for the reverse case then the statement is proved.","['limits', 'functional-analysis', 'analysis', 'real-analysis']"
3653805,Why are the integral points on elliptic curve preserved under this substitution?,"I have the following elliptic curve: $$3b^2-b=a^3+a^2$$ When I subsitute $\left(a,b\right)=\left(\frac{X-1}{3},\frac{Y+2}{9}\right)$ , I get the folloing minimal Weierstrass equation: $$Y^2+Y=X^3-3X+4$$ Question: how do I now know that I have all the integral points for the elliptic curve with $a$ and $b$ ? My prof. told me that under my substitution the integral points are preserved but why is that the case?","['elliptic-curves', 'number-theory', 'elementary-number-theory', 'integers', 'problem-solving']"
3653892,"Why are all functions $\{0,1\}^t \rightarrow \{0,1\}$ expressable as polynomials?","Siegelmann, Neural Networks and Analog Computation , p. 41, considers an arbitrary function $\beta(d_1,\ldots,d_t)$ : $\{0,1\}^t \rightarrow \{0,1\}$ , and assumes that it can be expressed as a polynomial--a linear combination of products of zero or more $d_i$ 's: $$\beta(d_1,\ldots,d_t) = c_1 + c_2d_1 + \cdots + c_{t+1}d_t + c_{t+2}d_1d_2 + \cdots + c_{2t}d_1d_2\cdots d_t$$ I don't yet understand why every such function $\beta$ can be expressed in this way.  Why does this follow merely from the fact that $\beta$ is a map from $\{0,1\}^t$ to $\{0,1\}$ ?","['functions', 'combinatorics', 'polynomials']"
3653912,Trigonometry: is there intuitive proof of $\sin(x + y)= \sin x\cos y+ \sin y\cos x$ when $x + y > 90^\circ$?,"I saw nice geometric proof of $\sin{(x + y)} = \sin{x}\cos{y} + \sin{y}\cos{x}$ using unit circle. But I can't find proof when $x + y > 90^\circ.$ Is there intuitive, ""simple"" or geometric way to prove this? Can we maybe transform unit circle or what is going on here?","['geometry', 'real-analysis', 'triangles', 'education', 'trigonometry']"
3653991,Finding unbiased point estimate of population variance,"Q.The contents of each of a random sample of 100 cans of a soft drink are measured. The results have a mean of 331.28 ml and a standard deviation of 2.97 ml. Show that an unbiased estimate of the population variance is 8.91 ml. I'm only able to get 2.97^2 which is 8.8209 but that is not what the question wants. How do I obtain 8.91? From my knowledge, an unbiased estimate of population variance is the same as sample variance, so 8.8209 should have been the correct answer, but it isn't. Why?","['statistics', 'parameter-estimation']"
3654055,How can we prove that $\int_0^\infty e^{-ix}x^{s-1} \ \mathrm{d}x = i^{-s}\Gamma(s)?$ [duplicate],This question already has an answer here : Can we find a closed form for this integral? (1 answer) Closed 4 years ago . I have seen $$\int_0^\infty e^{-ix}x^{s-1} \ \mathrm{d}x = i^{-s}\Gamma(s)$$ in a few posts regarding Mellin tranforms or a few difficult integrals. How can we prove this equality? I assume it can only be proven by residue theory.,"['integration', 'definite-integrals', 'complex-analysis', 'gamma-function', 'mellin-transform']"
3654132,A particularly tricky integral: $ \lim_{n \to \infty} n^{\frac{3}{2}} \int_0^1 \frac{x^2}{(1+x^2)^n} dx. $,"I encountered the following, deceptively simple looking integral - $$ \lim_{n \to \infty} n^{\frac{3}{2}} \int_0^1 \frac{x^2}{(1+x^2)^n} dx. $$ My goal was to hopefully somehow use the Lebesgue dominated convergence theorem; I've tried every elementary bound that I can think of to no avail. One approach suggested to me is to write $$ \int_0^1 \frac{x^2}{(1+x^2)^{n+1}}dx = \int_0^1 \frac{1}{(1+x^2)^n} - \int_0^1 \frac{1}{(1+x^2)^{n+1}}dx, $$ then try and use an induction and partial fraction decomposition argument. Doing this, I didn't find anything useful. When I tried plugging into wolfram/mathematica I got roughly the behavior (this could be wrong, so take it with a grain of salt) $$\int_0^1 \frac{x^2}{(1+x^2)^n} dx =  {}_2 F_1\left(\frac{1}{2}, n, \frac{3}{2}, -1\right). $$ Other calculations by trying to play with Gaussian bounds led to the result $ \frac{\sqrt{\pi}}{4} $ but this was done on a computer, not with an explicit method. There's also been some hints from mathematica that $$ \lim_{n \to \infty} n^{\frac{3}{2}} \int_0^1 \frac{x^2}{(1+x^2)^n} dx $$ is somehow related to $$ \sqrt{\pi}\frac{\Gamma\left(n-\frac{1}{2}\right)}{2\Gamma(n)}. $$ I believe that there is an elementary argument and that I just haven't found it. Any tips would be appreciated.","['calculus', 'definite-integrals', 'analysis', 'real-analysis']"
3654139,Why Mathematicians and Physicists Approach Integration on Manifolds Differently?,"I have been attempting to find an answer to this for a few weeks, and decided to finally ask. I'll ask the questions at the beginning and then give the necessary background below: (1) Are volume forms $dx^1\wedge\ldots\wedge dx^n$ $n$ -forms and tensors? (2) How do I integrate on Lorentzian manifolds? (3) Is some pull-back on a manifold related to the metric of that manifold? I'm a novice to general relativity, and I am teaching myself through Sean Carroll's An Introduction to General Relativity: Spacetime and Geometry (see here for essentially a pre-print version). Simultaneously, I am looking at Jon Pierre Fortney's A Visual Introduction to Differential Forms and Calculus on Manifolds in order to try to understand the mathematical infrastructure of GR in a somewhat visual way. I am very much confused by how the two authors handle integration on manifolds. I understand that Fortney is trying to deal with somewhat more primitive analysis using $\mathbb{R}^n$ manifolds, while Carroll tries to deal more specifically with Lorentzian manifolds, but there is no indication in the text of either that the formulas derived for integration are not general. Carroll (in 89-90 of his book; or page 53-54 of the linked pre-print) argues that volume forms $d^n x=dx^0\wedge\ldots\wedge dx^{n-1}$ are not tensors, i.e. they are not an $n$ -forms. He then goes through to sort of prove that the proper, coordinate-invariant method of integration of a scalar function $\phi$ on a (Lorentzian??) manifold is given by: $I=\int \phi(x) \sqrt{|g|}d^nx$ , where $|g|$ is the determinate of the metric on the manifold. Fortney (in chapter 3 and Appendix A) indicates that forms such as $dx^i \wedge dx^j$ are $n$ -forms and are a subset of all tensors on a manifold (specifically, the set of skew-symmetric ( $n$ ,0) tensors). In Chapter 7, he derives the integration formula for integration under coordinate change $\theta: \mathbb{R}^n \rightarrow \mathbb{R}^n$ as: $\int_R f(x_1,\ldots,x_n) dx^1\wedge \ldots \wedge dx^n = \int_{\phi(R)} f\circ\theta^{-1}(\theta_1,\ldots,\theta_n)T^*\theta^{-1}\cdot(dx^1 \wedge\ldots \wedge dx^n)$ , where $[\theta_1,\ldots,\theta_n]$ are the transformed coordinates and $T^*\theta^{-1}$ is the pullback induced by $\theta$ on $T^*$ . I realize, in a very real sense, these expressions are addressing two different aspects of integration. In particular, Fortney is specifically interested in change in coordinates while Carroll is interested in a coordinate-invariant expression for integration. However, I can't help but think these ideas should be related. In particular, it seems like the pull-back of some transformation is related to the metric on the manifold. I'm particularly concerned by the discrepancy between Carrol and Fortney. Carroll argues volume forms are not tensors, while Fortney argues they are. Either one of them is wrong or I'm misunderstanding the objects they are talking about.","['integration', 'manifolds', 'general-relativity', 'differential-geometry']"
3654144,100 spanning trees for graph G - prove existent of vertex v with degree at least 200,"In not directed and connected graph $ G=(V,E) $ there are $100$ spanning trees $ T_1,T_2,...T_{100} $ . Vertex $v$ in tree $T_i$ is signed $ d(v,T_i)$ for $ i=1,2,...100 $ . I need to prove the existent of vertex $v$ which its degree's sum in $100$ trees ( $ \sum_{i=1}^{100} d(v,T_i) $ ) is smaller than $200$ , and the existent of vertex $v$ which its degree's sum in $100$ trees is at least $200$ . $ d(v,T_i)$ is the vertex's degree in the $i$ 's tree ( $T_i$ ) Thanks!","['graph-theory', 'trees', 'combinatorics', 'discrete-mathematics']"
3654148,Proving that similarity transformation of state-space preserves the Euclidean norm,"I am aware that the state space realization of a dynamical system is not unique. So if we have a dynamical system: $\dot{x} = Ax + Bu$ $y = Cx$ Then we can write it as $\begin{bmatrix}
\dot{x}\\y
\end{bmatrix}
= G
\begin{bmatrix}x\\u\end{bmatrix}
$ where G is $G = \begin{bmatrix}
A \ \ \ \ | & B\\ 
\hline
C \ \ \ \ | & 0
\end{bmatrix}
$ but one can also formulate it as $G = \begin{bmatrix}
TAT^{-1} \ \ | & TB\\ 
\hline
CT^{-1} \ \  \ \ \ | & 0
\end{bmatrix}$ for any invertible matrix $T$ . I need to show that these two realizations of $G$ have the same Euclidean norm. I have found a video showing how similarity transformations preserve the trace and the determinant, but they did just an example problem, without doing the proof. Edit: It has been given that $G \in \mathcal{RH}_2$ , where $\mathcal{RH}_2$ is a set of all real rational, strictly proper and stable transfer matrices. It is a Hilbert Space with the inner product: $\langle F,G \rangle = \sup_{\sigma > 0} \left\{ \frac{1}{2\pi}\int_{-\infty}^{\infty} trace \left\{ F^* (\sigma+j\omega) G(\sigma+j\omega) \right\} d\omega \right\}$ and the corresponding norm given by ${{\left\Vert F \right\Vert}_{2}}^2 = \frac{1}{2\pi} \int_{-\infty}^{\infty} trace \left\{ 
F^*(j\omega)F(j\omega) \right\}d\omega$","['linear-algebra', 'linear-control', 'control-theory', 'dynamical-systems']"
3654164,"A Question about rationality , irrationality or Transcendence of definite integral",( this is my first question on the site so please forgive any possible mistake ) Consider integral of the form : $$\int_0^\infty f(x)dx$$ Can we have a set of conditions of $f(x)$ such that we can assure the value is rational or irrational( transcendental ) ? Is/are there such type of standard result in literature ?,"['definite-integrals', 'irrational-numbers', 'analysis', 'reference-request', 'transcendental-numbers']"
3654284,About weak convergence and derivative,"The following question had appeared in so many places, but none justify it, I tried a lot but . If someone can give me a hand. Let $X$ be a Hilbert space and $I:X\rightarrow \mathbb{R}$ a functional. If $(u_n)\subset X$ is such that $u_n$ converges weakly to $u$ in $X$ and $I'(u_n)\varphi \rightarrow 0$ for every $\varphi \in X$ , then $I'(u)=0$ , where $I'$ denotes the derivative of $I$ . Observations: 1) we know that $\|I'(u_n)\|\rightarrow 0$ (this is a fact in all the cases where this problem happened, so it can be used). 2) we don't know nothing special about $I$ besides being of class $C^1$ . 3) I tried to prove that I get convergence $I(u_n)\varphi \rightarrow I(u)\varphi$ on $\mathbb{R}$ so I could use weak lower semicontinuity (w.l.s.) of the module function. I also tried something about weak converge on $X^\ast$ so I could use w.l.s. of the dual norm, but once again I wasn't sucessful. I appreciate any help. Thanks in advance.","['weak-convergence', 'analysis', 'hilbert-spaces', 'functional-analysis', 'derivatives']"
