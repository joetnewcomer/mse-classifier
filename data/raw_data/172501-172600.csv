question_id,title,body,tags
3064108,Can we find element of order $q^2-1$ in $\text{GL}_2(\mathbb{F}_q)$?,"How to find element of order $q^2-1$ in $\text{GL}_2(\mathbb{F}_q)$ ? I am hoping to find field $\mathbb F_{q^2}$ as subalgebra of $2\times 2$ matrices over field $\mathbb F_q$ where $q$ is power of prime number. I was trying with element $\pmatrix {n&1 \\ 1&0}$ but it works only for $q=2,3,4,8,16$ . It is suggested that this is duplicate of question $GL_n(\mathbb F_q)$ has an element of order $q^n-1$ But I don't know how to find $\mathbb F_{q^2}$ in $M_2(\mathbb F_q)$ . Here is test in GAP for above matrix - $n$ is generator of the field multiplicative group. gap> List([2,3,4,5,7,8,9,11,13,16,17,19,23,25,27,29,31,32,37,41],
>         k->Order([[Z(k),1],[1,0]]*Z(k)^0));
[ 3, 8, 5, 12, 16, 9, 20, 24, 28, 17, 16, 40, 22, 52, 56, 20, 64, 31, 76, 40 ] To give my motivation - I want to prove that algebra $M_2(\mathbb F_q)$ can be represented as ${a+bj}$ for $a,b$ belonging to $\mathbb F_{q^2}$ and multiplication given by Cayley-Dickson $$(a+bj)(c+dj)=ac+\bar db + (da+b\bar c)j,$$ where $j$ is matrix satisfying $\bar j=-j$ .","['general-linear-group', 'finite-fields', 'finite-groups', 'abstract-algebra', 'group-theory']"
3064150,Show that $\psi(t)=e^{\lambda (\varphi(t)-1)}$ is infinitely divisble for any characteristic function $\varphi$,"I am given a function $$e^{\lambda(\varphi(t) -1)} \tag{1},$$ where $\varphi(t)$ is a characteristic function. I managed to show that $(1)$ is a characteristic function too. Now I am to show that $(1)$ is an infinitely divisible function. What does it mean? I know that a distribution is infinitely divisible if it can be expressed as the probability distribution of the sum of an arbitrary number of independent and identically distributed random variables. Do I have to find the distribution of my characteristic function and then show that it is infinitely divisible?","['characteristic-functions', 'probability-theory']"
3064190,What is $\int_0^{\pi/2}\sin^7(\theta)\cos^5(\theta)d\theta$,"I have to integrate the following: $\int_0^\limits\frac{\pi}{2}\sin^7(\theta)\cos^5(\theta)d\theta$ I decided to use a $u$ substitution of $u=\sin^2(\theta)$ , and $\frac{du}{2}=\sin(\theta)\cos(\theta)$ and arrived at this integral $\int_\limits{0}^{1}u^3(1-u)^2du$ From here I decided to use integration by parts using $g=u^3$ and $dv=(1-u)^2du$ I get the following: $$\biggl[\frac{u^3*(1-u)^3}{3}\biggr]_0^1-\int_\limits{0}^{1}(u^2*(1-u)^3)du$$ Repeated again $g=u^2$ , and $dv=(1-u)^3du$ $$\biggl[\frac{u^3*(1-u)^3}{3}\biggr]_0^1-\biggl[\frac{u^2*(1-u)^4}{4}\biggr]_0^1+\frac{1}{2}\int_\limits{0}^{1}u(1-u)^4$$ Repeating again $g=u$ , and $dv=(1-u)^4$ $$\biggl[\frac{u^3*(1-u)^3}{3}\biggr]_0^1-\biggl[\frac{u^2*(1-u)^4}{4}\biggr]_0^1+\frac{1}{2}\biggl[\frac{u(1-u)^5}{5}\biggr]_0^1-\frac{1}5\int_\limits{0}^{1}(1-u)^5$$ and I get $$\biggl[\frac{u^3*(1-u)^3}{3}\biggr]_0^1-\biggl[\frac{u^2*(1-u)^4}{4}\biggr]_0^1+\frac{1}{2}\biggl[\frac{u(1-u)^5}{5}\biggr]_0^1-\frac{1}{30}\biggl[(1-u)^6\biggr]_0^1$$","['integration', 'calculus', 'definite-integrals', 'trigonometry']"
3064218,Evaluate $\sum_{n=1}^\infty \arctan\left(\frac{1}{8n^2} \right)$,"I found this series in Jack D'Aurizio's Superior Mathematics from an Elementary Point of View on his user page. So I've seen similar series to this, so I figured I tried to make it telescope. I managed to write it using the difference formula for $\arctan\left(x\right)$ so, \begin{align}
\sum_{n = 1}^{\infty}
\arctan\left(\frac{1}{8n^{2}}\right) & =
\sum_{n = 1}^{\infty}
\left[\vphantom{\large A}\arctan\left(4n + 1\right) -\arctan\left(4n - 1\right)\right]
\\[1mm] & =
\sum_{n = 1}^{\infty}\left(-1\right)^{n}\arctan\left(2n + 1\right)
\end{align} Writing the series that though does not seem to help since none of the terms cancel with each other. What should I do find the answerÂ ?.","['trigonometry', 'sequences-and-series']"
3064281,Manipulating this $\frac{x-y}{z-y}$ to $1+\frac{x-z}{z-y}$,"There is probably a very easy explanation for this that is lost on me. Came across a formula that was manipulated into another form and it was presented as a given, so I am trying to figure out how that was done. Original: $\frac{x-y}{z-y}=1+\frac{x-z}{z-y}$ So I began to break it down: $\frac{x-y}{z-y}=\frac{x}{z-y}-\frac{y}{z-y}$ $\frac{x-y}{z-y}=\frac{x}{z-y}-(\frac{z}{z-y}-\frac{y}{y})$ <-- maybe this is wrong, but it works and I don't know why. Opening up the bracket: $\frac{x-y}{z-y}=\frac{x}{z-y}-\frac{z}{z-y}+1$ $\frac{x-y}{z-y}=1+\frac{x-z}{z-y}$ As I typed through all this, I see that $\frac{y}{z-y}=\frac{z}{z-y}-1$ is just true, and I get it when I simplify. I don't get how someone could see that to begin and wish to expand a formula like that. Thank you for your time and patience with my high school level math question.",['algebra-precalculus']
3064314,On the number of roots of the polynomial $x^3+Ax^2+1=0$,"I have the following cubic equation $$x^{3}+Ax^{2}+1=0$$ where $A$ is an arbitrary (real) number. I know that either: The 3 roots will be real. One root will be real and the other two will
be complex conjugates of each other. I would like to find out For what value/values of A the roots change from 3 real roots to one
real and two complex roots. The signs of each of the real roots (both when they are all real and when there is only one real root) Is there an analytical way of finding this as a function of $A$ or the only option is to solve the cubic numerically?","['cubics', 'calculus', 'roots', 'numerical-methods']"
3064316,General distributional solution of the Airy Equation,How can I prove that the Airy equation $$ \frac{d^2u}{dx^2}-xu = 0 $$ has at least two linear independent solutions? Once I've found it how can I prove the existence of two independent DISTRIBUTIONAL solutions,['ordinary-differential-equations']
3064320,Find Levi-Civita Connection in Hyperbolic Space,"With: $$
\mathbb{H}^n=\left\{ (x_0,x_1,\dots, x_n)\in \mathbb{R}^{n+1}: \; x_0^2=1+x_1^2+\cdots +x_n^2,\; x_0>0\right\}.
$$ and the form $$
\langle\langle(u_0,u_1,\dots, u_n),(v_0,v_1,\dots,v_n)\rangle\rangle=-u_0v_0+u_1v_1+\cdots+u_nv_n,
$$ I have to prove that $$
 \nabla_X Y =\overline{\nabla}_X Y -\langle\langle X,Y\rangle\rangle P,
$$ where $\overline{\nabla}$ is the connection in $\mathbb{R}^{n+1}$ and $P(p)=p$ . I have tried what follows:
I know that the normal vector to a point $p=(p_0,p_1,\dots,p_n)\in\mathbb{H}^n$ is $(-p_0,p_1,\dots,p_n)$ and also that ( $\langle, \rangle$ is the usual product): $$
\nabla_X Y =\overline{\nabla}_X Y -\langle \overline{\nabla}_XY,N\rangle N.
$$ Since $\langle \overline{\nabla}_XY,N\rangle=X(\langle Y,N\rangle)-\langle Y,\overline{\nabla}_X N\rangle=-\langle Y,\overline{\nabla}_X N\rangle$ . Now $\overline{\nabla}_X N=\sum X(N_i)\frac{\partial}{\partial x_i}=X'$ , where $X'$ is $X$ , but with a change of sign in the first coordinate. Now $\langle Y,\overline{\nabla}_X N\rangle=\langle Y,X'\rangle=\langle \langle Y,X \rangle\rangle$ .
So I get $$
 \nabla_X Y =\overline{\nabla}_X Y -\langle\langle X,Y\rangle\rangle N,
$$ which is the same as above but with a change in a sign. Where is my mistake?","['geometry', 'riemannian-geometry', 'differential-geometry']"
3064390,"What condition on $f$ to be differentiable on $(0,0)$?","Let $a > 0$ , $b > 0$ and $f_{a,b} :\mathbb{R}^2 \rightarrow \mathbb{R} $ defined as: $$f_{a,b} (x,y) = x +y + |x|^a |y|^b$$ Give a necessary and sufficient condition on $ (a,b) $ for $f_{a,b}$ to be differentiable at (0,0). Using the definition of the differentiation on the point $(0,0)$ , I need to get : $$\lim_{(x,y) \to (0,0)} \frac{|f_{a,b}(x,y) - f_{a,b}(0,0)|}{\sqrt{x^2 + y^2}} = 0$$ We have: $$ \frac{|f_{a,b}(x,y) - f_{a,b}(0,0)|}{\sqrt{x^2 + y^2}} = \frac{|x + y + |x|^a|y|^b|}{\sqrt{x^2 + y^2}} \leq \frac{|x|^a|y|^b}{\sqrt{x^2 + y^2}} $$ As a sufficient condition, the last term needs to tend to zero to get the differentiability, for that to happen, I do not how to derive the condition on $a$ and $b$ . I don't know how to get the necessary condition. Thank you.","['limits', 'multivariable-calculus', 'derivatives', 'analysis']"
3064391,Simplify $\sqrt[4]{\frac{162x^6}{16x^4}}$ is $\frac{3\sqrt[4]{2x^2}}{2}$,"(I've been posting a lot today and yesterday, not sure if too many posts are frowned upon or not. I am studying and making sincere efforts to solve on my own and only post here as a last resort) I'm asked to simplify $\sqrt[4]{\frac{162x^6}{16x^4}}$ and am provided the text book solution $\frac{3\sqrt[4]{2x^2}}{2}$ . I arrived at $\frac{3\sqrt[4]{2x^6}}{2x^4}$ . I cannot tell if this is right and that the provided solution is just a further simplification of where I've gotten to, or if I'm off track entirely. Here is my working: $\sqrt[4]{\frac{162x^6}{16x^4}}$ = $\frac{\sqrt[4]{162x^6}}{\sqrt[4]{16x^4}}$ Denominator: $\sqrt[4]{16x^4}$ I think can be simplified to $2x^4$ since $2^4$ = 16 Numerator: $\sqrt[4]{162x^6}$ I was able to simplify (or over complicate) to $3\sqrt[4]{2}\sqrt[4]{x^6}$ since: $\sqrt[4]{162x^6}$ = $\sqrt[4]{81}$ * $\sqrt[4]{2}$ * $\sqrt[4]{x^6}$ = $3 * \sqrt[4]{2} * \sqrt[4]{x^6}$ Thus I got: $\frac{3\sqrt[4]{2}\sqrt[4]{x^6}}{2x^4}$ which I think is equal to $\frac{3\sqrt[4]{2x^6}}{2x^4}$ (product of the radicals in the numerator). How ca I arrive at the provided solution $\frac{3\sqrt[4]{2x^2}}{2}$ ?","['algebra-precalculus', 'radicals']"
3064468,"A function with a non-zero derivative, with an inverse function that has no derivative.","While studying calculus, I encountered the following statement:
""Given a function $f(x)$ with $f'(x_0)\neq 0$ , such that $f$ has an inverse in some neighborhood of $x_0$ , and such that $f$ is continuous on said neighborhood, then $f^{-1}$ has a derivative at $f(x_0)$ given by: $${f^{-1}}'(x_0)=\frac{1}{f'(x_0)}$$ My questions is - why does $f$ have to be continuous on a whole neighborhood of $x_0$ and not just at $x_0$ ? Is there some known counter-example for that?","['proof-explanation', 'inverse-function', 'inverse-function-theorem', 'calculus', 'derivatives']"
3064470,Projective-invariant differential operator,"This question has been cross-posted to MathOverflow . Suppose we want a differential operator $T$ acting on functions $\mathbb{R}^n \rightarrow \mathbb{R}^n$ such that \begin{align*}
&T(g) = 0 \Longleftrightarrow g \in G \\
&g \in G \Longrightarrow T(g \circ f) = T(f)
\end{align*} where $G = \text{Aff}(n, \mathbb{R})$ is the affine group . Consider the operator $$T(f) = (\nabla f)^{-1} \cdot \nabla \nabla f$$ where $\nabla f$ is the gradient of $f$ and $\nabla \nabla f$ is its Hessian . This seems to satisfy the criteria since $$\nabla \nabla f = 0 \Longleftrightarrow f(x) = A \cdot x + b$$ and \begin{align*}
T(A \cdot f + b)
&= (\nabla (A \cdot f + b))^{-1} \cdot \nabla \nabla (A \cdot f + b) \\
&= (\nabla A \cdot f)^{-1} \cdot \nabla \nabla A \cdot f \\
&= (A \cdot \nabla f)^{-1} \cdot \nabla A \cdot \nabla f \\
&= (\nabla f)^{-1} \cdot A^{-1} \cdot A \cdot \nabla \nabla f \\
&= (\nabla f)^{-1} \cdot \nabla \nabla f \\
&= T(f)
\end{align*} My question is this: Is there a similar operator that is invariant under the projective group $G = \text{PGL}(n, \mathbb{R})$ ? For $G = \text{PGL}(1,\mathbb{R})$ , an example is the Schwarzian derivative $$S(f) = \frac{f'''}{f'} - \frac{3}{2} \left(\frac{f''}{f'}\right)^2$$ Projective differential geometry old and new by Ovsienko and Tabachnikov states in chapter 1.3 page 10 that $S(g) = 0$ iff $g$ is a projective transformation and $S(g \circ f) = S(f)$ if $g$ is a projective transformation. They also give a multidimensional generalization of the Schwarzian derivative in equation 7.1.6 page 191: $$L(f)_{ij}^k = \sum_\ell \frac{\partial^2 f^\ell}{\partial x^i \partial x^j} \frac{\partial x^k}{\partial f^\ell} - \frac{1}{n+1} \left(\delta_j^k \frac{\partial}{\partial x^i} + \delta_i^k \frac{\partial}{\partial x^j}\right) \log J_f$$ where $J_f = \det \frac{\partial f^i}{\partial x^j}$ is the Jacobian. However, Schwarps by Pizarro et al. states in section 3.3 page 97 that this ""cannot be used to ensure infinitesimally homographic warps as it also vanishes for other functions than homographies"" (are there examples?). Instead, they give a system of 2D Schwarzian equations that ""vanish if and only if the warp is a homography"" (page 94). These are given in section 4.2 equation 29 page 98: \begin{align*}
S_1[\eta] &= \eta^x_{uu} \eta^y_u - \eta^y_{uu} \eta^x_u \\
S_2[\eta] &= \eta^x_{vv} \eta^y_v - \eta^y_{vv} \eta^x_v \\
S_3[\eta] &= (\eta^x_{uu} \eta^y_v - \eta^y_{uu} \eta^x_v) + 2(\eta^x_{uv} \eta^y_u - \eta^y_{uv} \eta^x_u) \\
S_4[\eta] &= (\eta^x_{vv} \eta^y_u - \eta^y_{vv} \eta^x_u) + 2(\eta^x_{uv} \eta^y_v - \eta^y_{uv} \eta^x_v)
\end{align*} What is the geometric intuition behind these equations? Can they be stated more compactly/concisely? Also, how can we normalize them so that $S_i[\eta]$ is actually invariant under projective transformations of $\eta$ ? Finally, is there a compact expression for the $n$ -dimensional generalization of this derivative?","['differential-operators', 'projective-geometry', 'differential-geometry', 'invariance', 'affine-geometry']"
3064488,"Why is ""antiderivative"" also known as ""primitive""?","If I had to guess, I would say that calling the antiderivative as primitive is of French origin. Is one term more popular than the other?","['calculus', 'math-history', 'analysis', 'terminology']"
3064491,Why $\mathbb{Z}_{\lambda(n)}^* \le \mathbb{Z}_{\phi(n)}^*$ does not hold?,"Let $\lambda(n) = \lambda(\mathbb{Z}_n^*)$ be the Carmichael function and $\phi(n) = | \mathbb{Z}_{n}^{*} |$ be the Euler function. I need to show: $$\mathbb{Z}_{\lambda(n)}^* \le \mathbb{Z}_{\phi(n)}^*$$ But why should $\mathbb{Z}_{\lambda(n)}^* \le \mathbb{Z}_{\phi(n)}^*$ one be a subgroup of the other? This was stated in one course on cryptography but I cannot find a good justification for it. The claim was wrong In general this only holds for the subset relation. Still it would be great to find a counterexample. In return, I leave here an argument why the subset relation holds.","['number-theory', 'group-theory', 'abstract-algebra']"
3064515,Maximizing the area of a cyclic trapezoid whose long base is the circumdiameter. Non-trigonometric solution?,"A half circle with a radius of R encompasses an isosceles trapezoid such that the large base of the trapezoid is the diameter of the circle encompassing it. In terms of R, what is the length of the smaller base of all the possible trapezoids as described, whose area is maximal? After some attempts at the problem, I managed to solve it using unit circle trigonometry, but I am curious if there are purely geometric solutions for this (which is what I was trying to find when I first attempted the problem). Here is my solution: Let $x$ be $\measuredangle AOD$ Let $h$ be the height of the trapezoid Assume $0 < x < 90^\circ$ $$h = AO\sin x = R\sin x$$ $$AB = 2AO\cos x = 2R\cos x$$ Trapezoid area formula: $$\frac{AB + DC}{2} \cdot h $$ $$\downarrow$$ $$S_{(x)} = \frac{2R\cos x  + 2R}{2} \cdot R\sin x$$ From here, we find our function's derivative, get the $x$ for which there is a maxima, and plug it into our definition of AB to get it in terms of R, which would be AB = R. Is there an alternative?","['euclidean-geometry', 'trigonometry', 'derivatives']"
3064530,Proving this mapping is equal to the identity,"Question: Let $f, g$ be mappings of a set $S$ into itself. Assume that $f^2 = g^2 = I$ and that $f \circ g = g \circ f$ . Prove that $ (f \circ g)^2 = I $ . Prove that $ (f \circ g)^3 = I $ . My Answer: $ (f \circ g)^2 =  (f \circ g) \circ (f \circ g) = f \circ g\circ f\circ g = f^2 \circ g^2 = I \circ I= I $ $ (f \circ g)^3 =  (f \circ g) \circ (f \circ g) \circ (f \circ g) = f \circ g\circ f\circ g \circ f\circ g = f^2 \circ g^2  \circ f \circ g= I \circ I \circ f\circ g= f \circ g \; ...$ I've worked out the first part of the question and reduced the second part of a the question to the case of showing that $ (f \circ g) = I $ , but I can't get any further than that. I also noted down that $ f = f^{-1} $ and $ g = g^{-1} $ . I'm not sure whether this plays into it. Could someone point me in the right direction or point out if I'm making a mistake. I almost want to say this is a mistake in the book as I know there a couple in this book but I don't want to give up without knowing for sure. If it's of any help, this is from Serge Lang's Basic Mathematics. Thank you","['algebra-precalculus', 'functions']"
3064544,Do general discrete subgroups of $\operatorname{SL}_2(\mathbb R)$ have fundamental domains in the upper half plane?,"Let $\Gamma$ be a congruence subgroup of $\operatorname{SL}_2(\mathbb Z)$ .  The quotient $\Gamma \backslash \mathbb H$ has the structure of a one dimensional complex manifold, such that the quotient map $\pi: \mathbb H \rightarrow \Gamma \backslash \mathbb H$ is holomorphic.  There is a nice fundamental domain $D \subset \mathbb H$ coming from the usual fundamental domain for $\operatorname{SL}_2(\mathbb Z)$ which, up to some boundary identification, gives us a copy of $\Gamma \backslash \mathbb H$ inside $\mathbb H$ . The Borel measure $\mu = \frac{dx dy}{y^2}$ on $\mathbb H$ descends to a Borel measure $\overline{\mu}$ on $\Gamma \backslash \mathbb H$ which we may define using the fundamental domain: if $U \subset \Gamma \backslash \mathbb H$ is Borel, then we set $$\overline{\mu}(U) := \mu \bigg(\pi^{-1}(U) \cap D \bigg) \tag{1}$$ Now, assume $\Gamma$ is an arbitrary discrete subgroup of $\operatorname{SL}_2(\mathbb R)$ . Is there a canonical measure $\bar{\mu}$ on $\Gamma \backslash \mathbb H$ coming from $\mu = \frac{dx dy}{y^2}$ ? Does there always exist a fundamental domain $D$ for $\Gamma$ ? Can $\overline{\mu}$ arise from a differential form on $\Gamma \backslash \mathbb H$ ?  That is, does $\overline{\mu}$ come from a (unique?) smooth differential $2$ -form $\overline{\omega}$ on $\Gamma \backslash \mathbb H$ (thought of as a smooth manifold) which pulls back to the differential form on the smooth manifold $\mathbb H$ corresponding to $\mu = \frac{dx dy}{y^2}$ ? For intuition, I'm thinking of $\mathbb R$ modulo the action of $\mathbb Z$ .  Up to boundary identification, $[0,1]$ is a fundamental domain for the action of $\mathbb Z$ on $\mathbb R$ .  For the Haar measure $\bar{\mu}$ on $\mathbb R/\mathbb Z$ , we can get it in two ways.  First, if $\pi: \mathbb R \rightarrow \mathbb Z$ is the quotient map, we can measure subsets of $\mathbb R/\mathbb Z$ by pulling them back to $\mathbb R$ , intersecting them with $[0,1]$ , then measuring.  Second, $\bar{\mu}$ comes from the unique invariant nonvanishing $1$ -form on $\mathbb R/\mathbb Z$ which pulls back to the top form $dx$ on $\mathbb R$ giving the Lebesgue measure on $\mathbb R$ .","['measure-theory', 'number-theory', 'complex-analysis', 'modular-forms', 'differential-geometry']"
3064552,Calculate $\int_0^1 \frac{1}{\sqrt{x(1-x)}} dx$ using residue calculus,"I'd like to calculate $$\int_0^1 \frac{1}{\sqrt{x(1-x)}} dx,$$ using residue calculus. I was given a hint to consider the function $$f(z) = \frac{1}{z\sqrt{1-\frac{1}{z}}}. $$ I thought I was relatively comfortable solving residue calculus problems, until I saw this one. Because I am already stuck before I have even begun. I'm thinking that $f(z)$ is analytic when $z \neq 0$ and when $1-1/z > 0. $ I'm also assuming I should make a branch cut of some sort because of the aforementioned condition on $1-1/z$ . But it occured to me that $1-1/z \leq 0$ precisely when $0 < z < 1$ , and therefore this makes no sense to me since the limits of the integral that I am trying to solve are $0$ and $1$ ... So in conclusion, I need a lot of help.","['complex-analysis', 'residue-calculus']"
3064553,Is it true that $\lim_{x \to a}\frac{f(x)-f(a)}{x-a}=\pm\infty\ \implies\ \lim_{x \to a}f'(x) = \pm\infty$?,"Is it true that $\lim_{x \to a}\frac{f(x)-f(a)}{x-a}=\pm\infty \implies  \lim_{x \to a}f'(x) = \pm\infty$ ? Here, $f$ is a function defined on some open interval $I$ , and $a\in I$ . Assume $f$ is continuous at $a$ and differentiable around $a$ . I can't for the life of me see how to prove\disprove this implication, but my gut feeling is that it's false. Any guidance is greatly appreciated.",['calculus']
3064580,Rings with 'non-harmless' zero-divisors,"The following excerpt is from pp. 246â247 of Paolo Aluffi's Algebra: Chapter 0: 1.2. Prime and irreducible elements. Let $R$ be a (commutative) ring [with $1$ ], and let $a,b\in R$ . We say that $a$ divides $b$ , or that $a$ is a divisor of $b$ , or that $b$ is a multiple of $a$ , if $b\in(a)$ , that is $$
(\exists c\in R), \quad b = ac.
$$ We use the notation $a \mid b$ . Two elements $a,b$ are associates if $(a) = (b)$ , that is, if $a\mid b$ and $b\mid a$ . Lemma 1.5. Let $a,b$ be nonzero elements of an integral domain $R$ . Then $a$ and $b$ are associates if and only if $a = ub$ , for $u$ a unit in $R$ . [Proof omitted.] Incidentally, here the reader sees why it is convenient to restrict our attention to integral domains. This argument really shows that if $(a) = (b) \ne (0)$ in an integral domain, and $b = ca$ , then $c$ is necessarily a unit. Away from the comfortable environment of integral domains, even such harmless-looking statements may fail: in $\Bbb Z/6\Bbb Z$ , the classes $[2]_6,[4]_6$ of $2$ and $4$ are associates according to our definition, and $[4]_6 = [2]_6\cdot[2]_6$ , yet $[2]_6$ is not a unit. However, $[4]_6 = [5]_6\cdot [2]_6$ and $[5]_6$ is a unit, so this is not a counterexample to Lemma 1.5. In fact, Lemma 1.5 may fail over rings with 'non-harmless' zero-divisors (yes, there is such a notion) [emphasis added]. Since at this point, Aluffi does not say what such rings are called, I was hoping someone might know what type of rings Aluffi is referring to. (And hopefully provide a little context as to why they are interesting!)","['ring-theory', 'abstract-algebra']"
3064586,How to Prove It - Ch2 Sec 2 Exercise 2a,"Question: I am uncertain if my initial formulation of the logical form of the below statement is correct. How to Prove It (Velleman) Chapter 2, Section 2, Exercise 2a Negate these statements and then reexpress the results as equivalent positive statements. (it is implied that you put into logical form first based on worked examples in the chapter) 2(a) ""There is someone in the freshman class who doesn't have a roommate."" I have written this (prior to negation) $$\begin{equation}\begin{aligned}
\exists x[F(x) \rightarrow \forall y \neg R(x,y)]
\\ \\
F(x): \text{x is in the freshman class}\\
R(x, y): \text{x is roommates with y}\\
\end{aligned}\end{equation}\tag{1}$$ I thought this was the correct formulation however, I found two blogs online that formulate the phrase as follows (again, before negation) $$\begin{equation}\begin{aligned}
\exists x[F(x) \wedge \neg\exists yR(x,y)]
\end{aligned}\end{equation}\tag{2}$$ I checked to make sure equations (1) and (2) are not equivalent $$\begin{equation}\begin{aligned}
\exists x[F(x) \rightarrow \forall y \neg R(x,y)] \qquad &\text{(1)}\\
\exists x[F(x) \rightarrow \neg \exists yR(x,y)] \qquad &\text{quantifier negation}\\
\exists x[\neg F(x) \vee \neg \exists yR(x,y)] \qquad &\text{conditional law}\\
\exists x\neg[F(x) \wedge \exists yR(x,y)] \qquad &\text{DeMorgans}\\
\neg\forall x[F(x) \wedge \exists yR(x,y)] \qquad &\text{quantifier negation}\\
\end{aligned}\end{equation}$$ I am trying to convince myself that (2) is correct, but can't see why (1) is incorrect. Is it because it's somewhat speculative (""There exists someone x, where if that person x is a freshman then for all people y, person x and person y are not roommates"") vs declarative (""There is someone who is a freshman and for all people y, that person and person y are not roommates"")?","['elementary-set-theory', 'quantifiers', 'proof-writing', 'logic']"
3064590,Derivative of metric along curve,"Let $(M,g)$ be a semi-Riemannian manifold with Levi-Civita connection $D$ . Let $\alpha : [a,b] \rightarrow M$ be a smooth curve on $M$ , and let $\frac{D}{dt}$ be the induced covariant derivative on $\alpha$ . I want to prove that for all $X,Y$ smooth vector fields on $\alpha$ , $$\frac{d}{dt}\big{(}g(X,Y)\big{)} = g(\frac{D}{dt}X,Y) + g(X,\frac{D}{dt}Y) $$ (Proposition 3.18(4) from O'Neill's Semi-Riemannian Geometry ). I've tried using substituting coordinates (for a chart with coordinate functions $x^1,...,x^n$ ), which is what the book suggests: $g(X,Y) = g_{ij}X^iY^j$ , and $\frac{D}{dt}X = \left( \frac{dX^i}{dt} + \Gamma^i_{jk}\frac{d(x^j\circ\alpha)}{dt}X^k\right)\partial_i$ . But I am stuck: $$\frac{d}{dt}\big{(}g(X,Y)\big{)} = \frac{dg_{ij}}{dt}X^iY^j + g_{ij}\frac{dX^i}{dt}Y^j + g_{ij}X^i\frac{dY^j}{dt}$$ but $$ g(\frac{D}{dt}X,Y) + g(X,\frac{D}{dt}Y) = g_{ij}\frac{dX^i}{dt}Y^j + g_{ij}X^i\frac{dY^j}{dt} + g_{ij}(\Gamma^i_{kl}\frac{d(x^k\circ\alpha)}{dt}X^lY^j + \Gamma^i_{kl}\frac{d(x^k\circ\alpha)}{dt}X^jY^l) $$ I do not see how these two expressions are equal. Is there something wrong? Is there something I am missing? Thanks!","['semi-riemannian-geometry', 'general-relativity', 'riemannian-geometry', 'differential-geometry']"
3064606,"""Fragmentation"" of a distribution (from paper)","I've been reading a paper by Robert Morris (""Sets, Scales and Rhythmic Cycles; A Classification of Talas in Indian Music"") and came across a formula that I've found a bit tricky. He is referring to the ""fragmentation"" of a distribution and includes the formula below without derivation or reference. I'm pretty new to statistics, so this may be a standard formula that I'm just unaware of. However, I haven't been able to find it in the same format online. One feature for use in ordering talas is fragmentation. We have
  already grouped talas into partition classes. All talas in a
  particular partition class have the same fragmentation. We use the
  partition P as the input to a function that yields the fragmentation
  of the partition. Fragmentation varies between 0 and 1 and is a
  measure of the uniformity of a distributionâthe higher the
  fragmentation, the more even the distribution. We calculate the
  fragmentation of a partition of the number N into z parts using the
  following formula...: $FRAG(P)=1 - \frac{\sum_{k=1}^{z}{PAIRS(p_{k})}}{PAIRS(N)}$ where $PAIRS(s)=\frac{{s^2}-s}{2} \:, \: P=\{{p_{1},p_{2}, p_{3}},...p_{z}\},\\ N = sum(P), and \: z = card(p) . $ I found the formula to be much more readable in this format: $Let \: P = \{p_{1}, p_{2}, p_{3},..., p_{z}\}, \: z = card(P),\: and \: N = sum(P).\\
FRAG(P)=1- \frac{\sum_{k=1}^{z} \frac{p_{k}^{2}-p_{k}}{2}}{\frac{N^{2}-N}{2}}=1-2\frac{\sum_{k=1}^{z}\frac{p_{k}^2-p_{k}}{2}}{N^2-N}$ The author uses the formula with the example $P=\{2, 2, 4\} \rightarrow N = 2 + 2 + 4 = 8$ and $z = 3.$ This returns $FRAG(P)=1-2(\frac{8}{56})=0.714285714...$ Does this formula (or a similar one) have a name? Are there any places where I can find some further information? More generally, what does this mean? Thanks for the help!","['music-theory', 'statistics', 'uniform-distribution']"
3064610,Trying to simplify $\frac{\sqrt{8}-\sqrt{16}}{4-\sqrt{2}} - 2^{1/2}$ into $\frac{-5\sqrt{2}-6}{7}$,I'm asked to simplify $\frac{\sqrt{8}-\sqrt{16}}{4-\sqrt{2}} - 2^{1/2}$ and am provided with the solution $\frac{-5\sqrt{2}-6}{7}$ I have tried several approaches and failed. Here's one path I took: (Will try to simplify the left hand side fraction part first and then deal with the $-2^{1/2}$ later) $\frac{\sqrt{8}-\sqrt{16}}{4-\sqrt{2}}$ The root of 16 is 4 and the root of 8 could be written as $2\sqrt{2}$ thus: $\frac{2\sqrt{2}-4}{4-\sqrt{2}}$ Not really sure where to go from here so I tried multiplying out the radical in the denominator: $\frac{2\sqrt{2}-4}{4-\sqrt{2}}$ = $\frac{2\sqrt{2}-4}{4-\sqrt{2}} * \frac{4+\sqrt{2}}{4+\sqrt{2}}$ = $\frac{(2\sqrt{2}-4)(4+\sqrt{2})}{16-2}$ = (I become less certain in my working here) $\frac{8\sqrt{2}*2(\sqrt{2}^2)-16-4\sqrt{2}}{14}$ = $\frac{8\sqrt{2}*4-16-4\sqrt{2}}{14}$ = $\frac{32\sqrt{2}-16-4\sqrt{2}}{14}$ = $\frac{28\sqrt{2}-16}{14}$ Then add back the $-2^{1/2}$ which can also be written as $\sqrt{2}$ This is as far as I can get. I don't know if $\frac{28\sqrt{2}-16}{14}-\sqrt{2}$ is still correct or close to the solution. How can I arrive at $\frac{-5\sqrt{2}-6}{7}$ ?,"['algebra-precalculus', 'radicals']"
3064636,What is the solution to $y*yââ=\sin(x)$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I saw this differential equation in a Khan Academy video. Iâm still in high school and I have just began to learn a lot about differential equations, so I wanted to see if i could solve this. I donât really know how to though. Apologies if itâs a stupid question, thatâs just me. The differential equation is $$y\frac{d^2y}{dx^2}=\sin(x)$$ . If anybody can tell me the answer to this, and how to get it, if it is solvable, Iâd love to know. Link to the video, the uploader mentions it just at the end but doesnât really say much about it: https://youtu.be/-_POEWfygmU",['ordinary-differential-equations']
3064685,A generalization of maximum modulus principle,"Let $ \emptyset \neq U \subset \mathbb{C} $ be a bounded open connected set and let $ f_1, \dots, f_n $ be analytic in $ \overline{U} $ . Prove that $$ \max_{z \in \overline{U}} \sum_{j=1}^n |f_j(z) | = \max_{z \in \partial U} \sum_{j=1}^n |f_j(z) |. $$ Clearly we have "" $\geq$ "" but I don't know how to reduce to the $ n = 1 $ case to use the usual maximum modulus principle. Help is appreciated.",['complex-analysis']
3064730,$A$ nilpotent and $A+c_iB$ is nilpotent then $B$ is nilpotent.,"Let $A$ and $B$ be $n \times n$ matrices over some field with $A$ nilpotent. Now let $c_1,\ldots,c_{n+1}$ be $n+1$ distinct scalars such that $A+c_i B$ is nilpotent for all $i=1, \ldots,n+1$ . Then how can I show that $B$ is also nilpotent? Thanks","['matrices', 'nilpotence', 'abstract-algebra', 'linear-algebra', 'linear-transformations']"
3064753,Funky function-composed within itself umpteen thousand times,"$f(x)$ is a differentiable function satisfying the following conditions: $$
 0 < f(x) < 1 \quad \text{for all $x$ on the interval $0 \le x \le 1$.} \\
 0 < f'(x) < 1 \quad \text{for all $x$ on the interval $0 \le x \le 1$.}
$$ How many solutions does the equation $$
\underbrace{f(f(f( \ldots f}_{2016~\text{times}}(x) \ldots) =x
$$ have on the interval $0\leq x\leq 1$ ? This seems to be looking like chain rule
And since $f'$ is positive on the interval $[0,1]$ it seems to be increasing what does it do for $x>1$ ?
And what is the significance of 2016? 
I dont think that matters. The function is composed within itself that many times but I think maybe it doesnt matter if its 2016 or 2019 !ð","['contest-math', 'functions', 'functional-analysis']"
3064768,"If $f(x)=\int_{x-1}^x f(s)ds$, is $f$ constant? Periodic?","I was thinking of periodic functions, and in particular the following type of condition: If a function $f:\mathbb{R}\to\mathbb{R}$ always ""tends to its average"", then it should be periodic. To make things more formal, by ""tending to the average"" we could say something like $f(x)=\int_{x-1}^x f(s)ds$ . This is only the average depending on a previous time interval of length $1$ , but it seems an interesting enough property. However, the only type of functions which I could find that satisfies this property are the constant ones! Question: If $f:\mathbb{R}\to\mathbb{R}$ is continuous (or more generaly measurable) and $f(x)=\int_{x-1}^x f(s)ds$ for (almost) every $x\in\mathbb{R}$ , then is $f$ constant (a.e.)? Periodic (a.e.)? Here is a first try for $C^1$ functions (see edit below!): If $f$ is $C^1$ and $x$ is fixed, we can use Taylor expansion $f(s)=f(x)+O(s-x)$ (and similarly for $x-1$ ) to obtain \begin{align*}
f(x+t)-f(x)&=\int_x^{x+t}f(s)ds-\int_{x-1}^{x-1+t}f(s)ds\\
&=\int_x^{x+t}f(x)+O(s-x)ds-\int_{x-1}^{x-1+t}f(x-1)+O(s-x+1)ds\\
&=t(f(x)-f(x-1))+O(t^2),
\end{align*} so $f'(x)=f(x)-f(x-1)$ . This is obviously true if $f$ is constant, but the converse is not clear to me at the moment. Edit : From a comment and answer below, the equation $f'(x)=f(x)-f(x-1)$ has non-periodic solutions on $\mathbb{R}\setminus\mathbb{Z}$ , so this should not be the way to go for $C^1$ functions. However, even in this case it is not clear that any solution of this equation will satisfy $f(x)=\int_{x-1}^x f(s)ds$ , which is the question: All I can obtain, in principle, is $f(x)-f(x-1)=\int_x^{x-1}f(s)ds-\int_{x-2}^{x-1}f(s)ds$ .","['integration', 'calculus', 'average', 'recreational-mathematics']"
3064771,"Need Help : Proving polynomials are continuous, without circular reasoning","I know there are a lot of answers regarding continuity of polynomials. But, this question is different. We need to have $ \lim_{x\to a} {x^n} = a^n$ , $n \in N$ , to be able to prove that polynomials are continuous. This fact is derived from the product rule (or may be it can't be, which is my question). The product rule is proved using square roots, so it assumes the existence of square roots.  The fact that For every non negative number $x$ , it's $n^{th}$ root exists, i.e. $x^n$ is invertible, assumes the continuity of $x^n$ because this is proven using Intermediate value Theorem. Bam - Circular reasoning ! Or am I Wrong ? 
Here's the only proof of product rule which I know :
Assume $ \lim_{x\to a} {f(x)} = L$ and $ \lim_{x\to a} {g(x)} = K$ Let $Ïµ > 0$ be any positive number
Hence, $â\delta_1> 0 â¶ 0<|x-a|<Î´_1â¹|f(x)-L|<\sqrt{\epsilon}$ And $âÎ´_2>0 â¶ 0<|x-a|<Î´_2 â¹ |g(x)-K|<\sqrt{\epsilon}$ Let $Î´=\min\{Î´_1,Î´_2\}$ Hence, $0<|x-a|<Î´$ â¹ $|(f(x)-L)(g(x)-K)-0|<\sqrt{\epsilon} \sqrt{\epsilon} = Ïµ$ Hence, $\lim_{x \to a} {(f(x)-L)(g(x)-K)} = 0$ â¹ $\lim_{x \to a} {(f(x)g(x)-Kf(x)-Lg(x)+KL)} = 0$ And then the result follows. 
Even if we use $\epsilon$ in place of $âÏµ$ , we end up with $0<|x-a|<Î´â¹|(f(x)-L)(g(x)-K)-0|<\epsilon\cdot \epsilon = Ïµ^2$ , and then we have to prove that the range of $\epsilon^2$ is $[0,\infty]$ , which amounts to proving that for each number in $[0,\infty]$ , a corresponding square root exists.","['epsilon-delta', 'limits', 'continuity', 'real-analysis']"
3064845,Function almost always periodic for every period is constant,"Let be $(X, \mathcal{A}, \mu)$ a measure space and $f: X \to [0, +\infty]$ a measurable function such that for every $x \in X$ we have $f(t + x) = f(t)$ for almost every $t \in X$ . Is true that $f$ is constant? or constant almost everywhere? Thanks in advance.","['measure-theory', 'measurable-functions', 'almost-everywhere']"
3064853,Derivatives of inverse matrix,"In the matrix cookbook , Eq. 59, the formula: $$
\partial_x Y^{-1}=-Y^{-1}(\partial_x Y) Y^{-1}
$$ is provided without proof. How to prove this formula?","['derivatives', 'linear-algebra']"
3064867,Cayley Graphs of Product groups is product of Cayley Graphs,"Let us have two Groups $G$ and $H$ . Then, is the Cayley graph associated with $G\times H$ , the direct product of the groups with respect to some generating set with cardinality $mn$ ;  a product of the Cayley graphs associated to groups $G$ and $H$ with generating sets of cardinalities $m$ and $n$ respectively? If so, is the graph product a cartesian product? I think yes, because the product group acts transitively on the product of Cayley graphs, I think? Thanks beforehand.","['graph-theory', 'group-theory', 'combinatorics']"
3064905,For which $p$ primes is $p^{p-1}$ a divisor of $(p-1)^p + 1$?,"$p = 2$ and $p = 3$ definitely are solutions. I think these are all the solutions, but how can I prove it?","['number-theory', 'divisibility', 'prime-numbers']"
3065007,Residue of $f(z)=\frac{z}{\sin{\left(\frac{\pi}{z+1}\right)}}$ in all isolated singularities,"I have this complex function: $$f(z)=\frac{z}{\sin\left(\frac{\pi}{z+1}\right)}$$ I'd like to compute residues in all isolate singularities.
If I'm not mistaken $f$ has poles in $z=\frac{1}{k}-1$ and a non isolated singularity in $z=-1$ , because it is an accumulation point of poles.
I tried to do something similar to this answer , but I don't seem to get a clean expression in terms of $\xi$ , where $\xi$ is $z-\frac{1}{k}+1$ .
The best I can obtain is this: $$\frac{z}{\sin\left(\frac{k\xi+1-k}{k\xi+1}\right)}$$ Can you help me?","['complex-analysis', 'residue-calculus', 'laurent-series']"
3065012,Volume of intersection of a sphere and a paraboloid,"Could I calculate the volume of the intersection of $x^2+y^2+z^2=8$ and $4z=x^2+y^2+4$ using spherical coordinates and treating the paraboloid as a function, as in the following example? $f(x,y)=\frac{1}{4}(x^2+y^2)+1=\frac{1}{4}(r^2\sin^2(\theta)\cos^2(\varphi))+r^2\sin^2(\theta)\sin^2(\varphi))+1=\frac{1}{4}(r^4\sin^2(\theta)) + 1$ Volume = $\int_0^{2\pi} \int_0^\pi \int_0^\sqrt{8} (\frac{1}{4}(r^4\sin^2(\theta))+1)\cdot r^2\sin(\theta) drd\theta d\varphi$ Should that work? I tried it and didn't get the correct result but then I'm sick and might have seen my error. Edit: I think I am mixing something up here. Basically I just add up the values of $f$ but still integrate over a spherical volume. So the idea to restrict the integrated volume with a function doesn't work - at least not like that. Right?","['integration', 'multivariable-calculus', 'calculus', 'volume']"
3065015,How to find which percentile is a value in a skewed normal distribution,"I have a skewed normal distribution for which I know the average, standard deviation, skewness & kurtosis (which is different from zero). Given a number $X,$ how can estimate which percentile corresponds to that value? (I'm ok with getting an approximate value of this percentile.) I used z-score tables in the past (before having skewed distributions), but they seem to apply only to non-skewed distributions. Thanks for your help.",['statistics']
3065071,Maclaurin expansion of $\arctan(x)/(1 â x).$,"How was this Maclaurin expansion derived? For each $|x|<1,$ \begin{align} \left( \frac{\arctan(x)}{1-x} \right)&=\left( \sum^{\infty}_{k=0}x^k\right)\left(\sum^{\infty}_{j=0}\dfrac{(-1)^j x^{2j+1}}{2j+1}\right)\\&= \sum^{\infty}_{k=0}\left(\sum_{j\in D_k} \dfrac{ (-1)^{j} }{2j+1}\right)x^k,\;\text{where}\; D_k=\{j\in \Bbb{N}:0\leq j\leq (k-1)/2\}.\end{align} HERE'S MY TRIAL \begin{align} \left( \frac{\arctan(x)}{1-x} \right)&=\left( \sum^{\infty}_{k=0}x^k\right)\left(\sum^{\infty}_{j=0}\dfrac{(-1)^j x^{2j+1}}{2j+1}\right)\\&= \sum^{\infty}_{k=0}\left(\sum^{k}_{j=0} x^j\dfrac{ (-1)^{(k-j)} x^{2(k-j)+1}}{2(k-j)+1}\right),\;\text{for}\; k\in \Bbb{N}\\&\stackrel{\text{how?}}{=} \sum^{\infty}_{k=0}\left(\sum_{j\in D_k} \dfrac{ (-1)^{j} }{2j+1}\right)x^k.\end{align}","['analysis', 'real-analysis', 'calculus', 'taylor-expansion', 'sequences-and-series']"
3065089,Total variation distance bounds on multivariate normals with different means and variances,"I'm trying to find a way to obtain an upper bound on the total variation distance between two multivariate normal distributions, i.e. $$
\vert \vert \mathcal{N}(\theta_1, a I) - \mathcal{N}(\theta_2, b I) \vert \vert_{TV} \le k \cdot \vert \vert \theta_1 - \theta_2 \vert \vert
$$ where $\theta_1 \ne \theta_2$ and $a \ne b$ are constants and $k$ is some constant. Alternatively, some other bound that results in a scalar times any normed distance of $\theta_1$ and $\theta_2$ works. The problem I'm running into is I can't seem to find many ways to bound total variation distances between multivariate normal distributions. There are plenty of toy examples where either both distributions have the same mean or both distributions have the same variance in which it's easy to get an exact value, and I've computed the exact total variation distance between these two distributions, but I need something cleaner. Any help would be greatly appreciated!","['statistics', 'markov-chains', 'probability-theory', 'probability']"
3065123,Integrating rational function by multiplying with a high-power polynomial term,"When evaluating the integral $\int \frac{x^2 + 8}{x^3 + 9x} \textrm{d}x$ , WolframAlpha gave an interesting step-by-step suggestion. Here is the suggested method: $$\int \frac{x^2 + 8}{x^3 + 9x} \textrm{d}x = \int \frac{x^{17} + 8x^{15}}{x^{18} + 9x^{16}} \textrm{d}x=\frac{1}{18} \int \frac{18(x^{17} + 8x^{15})}{x^{18} + 9x^{16}} \textrm{d}x$$ Let $u = x^{18}+9x^{16} \implies \textrm{d}u=(18x^{17}+144x^{15}) \, \textrm{d}x = 18(x^{17} + 8x^{15}) \, \textrm{d}x $ $$\frac{1}{18} \int \frac{\textrm{d}u}{u} = \frac{1}{18} \log \vert u \vert + C $$ This indeed works, giving the correct answer (after substituting $x$ back and simplifying). But I've never seen this kind of an integration technique, and I'm wondering if there's a name for this method and perhaps a systematic way of determining which degree of polynomial to use? (The polynomial in this example happened to be $x^{15}$ , something that I would've never though of)","['integration', 'calculus', 'substitution']"
3065135,Can't find a seemingly simple limit $\lim_{n\to\infty}\frac{(n+k)!}{n^n}$,"Evaluate the limit: $$
\lim_{n\to\infty}\frac{(n+k)!}{n^n}, \ n,k\in\Bbb N
$$ I would like to avoid Stirling's approximation, derivatives and Cesaro-Stolz, since none of them has been yet introduced. I've tried to apply the old ratio test in order to show the sequence is bounded and monotone, hence convergent, but that leads to nowhere. At least I couldn't find the  appropriate bounds: $$
\frac{x_{n+1}}{x_n} = \frac{(n+k+1)!}{(n+1)^{n+1}} \cdot \frac{n^n}{(n+k)!} \\
=\frac{n+k+1}{n+1}\cdot \frac{n^n}{(n+1)^n} \\
=\underbrace{\left(1+{k\over n+1}\right)}_{>1}\underbrace{\frac{n^n}{(n+1)^n}}_{<1}
$$ This is not conclusive at all. Some further thoughts are: $$
\begin{align}
x_n &= \frac{n!}{n^{n-k}}\cdot \frac{n+1}{n}\cdot \frac{n+2}{n} \cdots\cdot \frac{n+k}{n} \\
&= \frac{n!}{n^{n-k}} \left(1+{1\over n}\right)\left(1+{2\over n}\right)\cdots\left(1+{k\over n}\right) \\
&\le \frac{n!}{n^{n-k}} \left(1+{k\over n}\right)^k \\
&\le \frac{e^kn!}{n^{n-k}}
\end{align}
$$ Not sure how to squeeze it though. I know the limit is $0$ since $x_n$ is decreasing starting from some $N$ towards $0$ . But how do I rigorously show that? I would prefer a hint rather than a full answer. Thank you!","['limits', 'calculus', 'factorial', 'sequences-and-series']"
3065136,"Given $T \in L(X,Y)$, show the equivalence between: existence of $S$ such that $S(T(x))=x$, and $T$ being injective with $T(X)$ complemented in $Y$","Given $X,Y$ Banach spaces and $T \in L(X,Y)$ , show that the following sentences are equivalent: A) there exists $S \in L(Y,X)$ such that $S(T(x))=x$ for all $x \in X$ . B) $T$ is injective and $T(X)$ is a complemented space of $Y$ . Context : I was given this exercise in my functional analysis course but I donât know how to solve this. All I have understood so far in this exercise are the following: I was given the following definition of ""complemented space'': a closed subspace $M$ is complemented in $N$ if exists a topological complement of $M$ in $N$ or equivalently if there exists a linear continuous projection $P$ in $N$ such that $ð(N)=M$ ; $L(X,Y)$ means the set of all continuous linear operators from $X$ to $Y$ .","['banach-spaces', 'complete-spaces', 'normed-spaces', 'functional-analysis', 'linear-transformations']"
3065161,Automorphism of an open subset with completement of codimension $2$,"Let $\mathbb P^n=\mathbb {CP^n}$ , I guess the following is true: Let $D\subset \mathbb P^n$ be a closed subscheme of codimension $2$ . Then every automorphism of $\mathbb P^n-D$ is linear, i.e. ${\rm Aut}(\mathbb P^n-D)\subset {\rm Aut}(\mathbb P^n)$ . More generally, I guess the following is also true: Let $D\subset V$ be a closed subscheme of codimension $2$ , here $V$ is an arbitrary variety. Then ${\rm Aut}(V-D)\subset {\rm Aut}(V)$ . Is it correct? Could someone give a reference or counter example about this? I think the codimension $2$ condition is for Hartog's theorem, but I do not know how to apply it. Clearly we can not expect any morphism to extend to $D$ , as there may be base locus, so we really need automorphisms. Also, it is not enough if $D$ is of codimension $1$ , since there is trivial counter-example $\mathbb P^n-D=\mathbb A^n$ .","['automorphism-group', 'complex-geometry', 'algebraic-geometry', 'reference-request']"
3065170,"Are there two functions $f, g$ such that $f(g(x)) = x^3$ and $g(f(x)) = x^5$?","Question. Are there two functions $f, g: \mathbb{R}\rightarrow\mathbb{R}$ that satisfy $f(g(x)) = x^3 \enspace\forall x\in\mathbb{R}$ and $g(f(x)) = x^5\enspace\forall x\in\mathbb{R}$ ? This is an extension to this question , where I proved that there are no two functions such that $f(g(x)) = x^{2018}$ and $g(f(x))=x^{2019}$ (my proof can easily be extended to any two powers where one power is odd and the other power is even, instead of just $2018$ and $2019$ .) Remark $1$ . If there are two such functions, then they satisfy the following properties: $f, g$ are bijective; $f(x^5) = f(x)^3\enspace\forall x\in\mathbb{R}$ ; $g(x^3) = g(x)^5\enspace\forall x\in\mathbb{R}$ ; $f(i), g(i)\in\{-1, 0, 1\}\enspace\forall i\in\{-1, 0, 1\}$ ; $x^9 = f(g(x))^3 = f(g(x^3)) \enspace\forall x\in\mathbb{R}$ ; $g^{-1}(x)=\sqrt[3]{f(x)}, f^{-1}(x)=\sqrt[5]{g(x)}\enspace\forall x\in\mathbb{R}$ . Remark $2$ . A similar question would be if there are two functions such that $f(g(x)) = x^2 \enspace\forall x\in\mathbb{R}$ and $g(f(x)) = x^4 \enspace\forall x\in\mathbb{R}$ , or more generally: For what $i, j\in\mathbb{N}$ are there functions $f,g:\mathbb{R}\rightarrow\mathbb{R}$ such that $f(g(x)) = x^i, g(f(x)) = x^j\enspace\forall x\in\mathbb{R}$ ?","['function-and-relation-composition', 'real-analysis']"
3065177,Solution of the differential equation $\ddot{x}=e^{-x}$,"I would like to know how to get the solution of the following differential equation: $$\frac{d^2x}{dt^2}(t)=e^{-x(t)}$$ A differential equation of similar form appears in a paper (Rapp & Kassal 1968) that I'm studying. The authors show us its solution (it involves an hyperbolic secant function), but don't tell us how did they get it. So, at least I know that this differential equation has an analytical solution.",['ordinary-differential-equations']
3065195,"Can you have solution for the equation $|\nabla f|^2=f\,\Delta f$, for a homogeneous polynomial $f$ with $\deg(f)>2$?","Consider the following equation \begin{equation}
|\nabla f|^2=f\,\Delta f,
\end{equation} where $\nabla f$ is the gradient of $f$ and $\Delta f$ is the Laplacian of $f$ .
Does this equation have a solution $f\colon \mathbb{R}^n\to \mathbb{R}$ such that $f$ is a homogenous polynomial with $\deg(f)>2$ and $n>1$ ?","['ordinary-differential-equations', 'partial-differential-equations']"
3065246,Prove $F(t)^2-G(t)^2=2tF(t)G(t)$,"Let $F, G:]0,\infty[ \to \mathbb R$ be $$F(t):=\int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x)$$ and $$G(t):=\int_{[0,\infty[}e^{-tx^2}\sin{x^2}d\lambda(x)\,.$$ Prove that $$F(t)^2-G(t)^2=2tF(t)G(t)\,.$$ Note: $F(t)^2-G(t)^2=(\int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x))^2-(\int_{[0,\infty[}e^{-tx^2}\sin{x^2}d\lambda(x))^2$ Specifically looking at $(\int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x))^2=\int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x)\times \int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x)$ am I allowed to state: $\int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x)\times \int_{[0,\infty[}e^{-tx^2}\cos{x^2}d\lambda(x)=\int_{[0,\infty[^2}e^{-tx^2}\cos{x^2}d\lambda^{2}(x)$ ? And then I assume I should substitute $y = x^2$ $\int_{[0,\infty[}2x(\int_{[0,\infty[}e^{-ty}\cos{y}dy)dx$ I can integrate $e^{-y}\cos{y}$ tediously by parts, however, I am not sure what to do with the parameter $t$ in $e^{-ty}\cos{y}$ , as in the above case. Any support is greatly appreciated.","['integration', 'measure-theory', 'lebesgue-integral', 'real-analysis', 'multivariable-calculus']"
3065266,Non zero solution of $3x\cos(x) + (-3 + x^2)\sin(x)=0$,How can I find exact non-zero solution of $3x\cos(x) + (-3 + x^2)\sin(x)=0$ . Simple analysis and the below plot show that the equation has an infinite number of non-zero solutions.,"['trigonometric-series', 'elementary-functions', 'calculus', 'trigonometry']"
3065275,Evaluate the limit $\lim_{n\to\infty}\log_a\left(\frac{4^nn!}{n^n}\right)$,"Evaluate the limit: $$
\lim_{n\to\infty}\log_a\left(\frac{4^nn!}{n^n}\right)\\
a>0\\
a \ne 1
$$ I've started with defining another sequence. Let: $$
y_n = a^{x_n} = \frac{4^nn!}{n^n}
$$ Consider the fraction: $$
\frac{y_{n+1}}{y_n} = \frac{4^{n+1}(n+1)!}{(n+1)^{n+1}} \cdot \frac{n^n}{4^nn!}\\
= \frac{4n^n}{(n+1)^n}
$$ Consider the limit: $$
\begin{align}
\lim_{n\to\infty}\frac{y_{n+1}}{y_n} &= \lim_{n\to\infty}\frac{4n^n}{(n+1)^n} \\
&= \lim_{n\to\infty}4\left(\frac{n}{n+1} \right)^n \\ 
&= {4\over e} > 1
\end{align}
$$ So by this $y_n$ is divergent. Which means: $$
\lim_{n\to\infty}y_n = \infty
$$ Now I'm having difficulties translating it in a backward direction. We have that: $$
\lim_{n\to\infty}y_n = \lim_{n\to\infty}a^{x_n} = \infty
$$ Or: $$
\log_a \lim_{n\to\infty}a^{x_n} = \log_a(\infty)
$$ The answer suggests that: $$
\lim_{n\to\infty}x_n = 
\begin{cases}
+\infty,\ a > 1\\
-\infty,\ 0 < a < 1
\end{cases}
$$ And I don't see where this appears when going backward from $a^{x_n}$ to $x_n$ . Could you please explain that to me?","['limits', 'calculus', 'sequences-and-series']"
3065296,What is the operator norm of $Tf(x) = x^2f(x)$?,"Let $H = L^2([0,1],\mathbb{R})$ and $T : H \to H,\, Tf(x) = x^2f(x) $ . $T$ is linear. $$\|Tf\|_{L^2([0,1],\mathbb{R})} = \sqrt{\int_0^1x^4f^2(x)dx} \leq\sqrt{\int_0^1f^2(x)dx} = \|f\|_{L^2([0,1],\mathbb{R})} $$ $T$ is linear and bounded therefore it's continuous. Also $\|T|| \leq 1$ . I tried finding solution to $\|Tf\|_{L^2([0,1],\mathbb{R})}  = \|f\|_{L^2([0,1],\mathbb{R})} $ I found $$f(x) = \sqrt{\frac{2x-1}{x^4-1}}$$ but it's not in $L^2$ so it doesn't work. anyone knows an $f$ to reach $1$ , I'm not even sure it's $1$ . any help will be greatly appreciated !","['operator-theory', 'normed-spaces', 'hilbert-spaces', 'lp-spaces', 'functional-analysis']"
3065317,Every differential form $\omega$ of degree 1 in the sphere $S^m\subset\mathbb{R}^{m+1}$ that is closed is also exact.,"I am in a series of self-studies on differential forms on $m$ -dimensional surfaces in Euclidean space. I'm two days thinking about the exercise below. The book that proposes this exercise gives no hint of how to solve it. And it seems to me that there is nothing in the text of the book that can help. This exercise is in a chapter preceding the chapter on the Stokes' theorem ( $\int_{\partial M} \omega = \int_{M} d\omega $ ). So I am trying a solution that does not use Stokes' theorem. Exercise. For every differential form of class $C^k$ in  sphere $S^m$ there exists a differential $r$ -form $\Omega$ of class $C^k$ in $\mathbb{R}^{m + 1}-\{0\}$ such that $\Omega|_{S^m} = \omega $ . If $m>1$ , conclude that every closed differential form of degree $1$ in the sphere $S^m$ is the differential of a function $f:S^m \to \mathbb{R}$ . In particular, any closed form of degree $1$ in $S^m$ ( $m>1$ ) must be nullified by at least two points. My job. Let $\omega$ be a differential form of class $C^k$ and degree $r$ in sphere $$S^m = \{(x_1,\ldots, x_{m + 1}) \in \mathbb{R}^{m + 1}: x_1^2 +\ldots + x_{m + 1}^2 = 1\}.$$ Let us obtain a differential form $ \Omega $ of class $ C^k $ and degree $ r $ in $ \mathbb{R}^{m + 1}$ such that $\Omega|_{S^m}=\omega$ . Let $ x \in \mathbb{R}^{m+1}-\{0\}$ be fixed. Given any other vector $ v \in \mathbb{R}^m$ there exists a unique vector $P_x(v)$ perpendicular to $x$ such that $$
v=c_v\cdot x + P_x(v)
$$ where $ c_v $ is a constant that depends on $ v $ . Note that $P_x(v)\in T_xS^m$ . Since the projection $ P_x $ is a linear application we can define $$
\Omega(x)(v_1,\ldots,v_r)=\omega({x}/{|x|})(P_x(v_1),\ldots,P_x(v_r)).
$$ I believe that this first part of the exercise I did correctly.Now let's go to the second part of the exercise which is proof that $ \omega $ is exact. For every point $ x $ of the sphere $ S^m $ there exists a parametrization $\varphi: \overline{U} \to \tilde{U} $ with $x = \varphi (u)$ from which we can obtain the base $\left\{\frac{\partial\varphi(u)}{\partial u_1},\ldots,\frac{\partial\varphi(u)}{\partial u_m}  \right\}$ of $ T_xS ^ m $ and its dual base $\{du_1,\ldots, du_m\}$ of $ (T_xS^m)^\ast $ . In addition, there are coordinate functions $a_j^{\varphi}:\overline{U}\to \mathbb{R}$ such that $$
\omega(x)(v) = a_1^{\varphi}(u) du_1\cdot v+\ldots+ a_m^{\varphi} du_m\cdot v
$$ for all $x=\varphi(u)\in \tilde{U}$ and all $v\in T_xS^m$ . On the assumption that $ \omega $ is closed we have $$
d\omega(x)(v_1,v_2) =\sum_{i<j} \left(\frac{\partial a_j^{\varphi}}{\partial u_i}(u)-\frac{\partial a_i^{\varphi}}{\partial u_j}(u) \right)  du_i\wedge du_j ( v_1,v_2)=0
$$ for all $x=\varphi(u)\in \tilde{U}$ and all $v_1,v_2\in T_xS^m$ . This means that for every point $x\in S^m$ there is a parametrization $\varphi $ , as $\varphi(u)=x$ for some $u \in \overline{U} $ , which provides coordinate functions $a_j^{\varphi}:\overline{U}\to\mathbb{R}$ of $ \omega $ that satisfies for all $i<j$ in $\{1,\ldots,m\}$ $$
\frac{\partial a_j^{\varphi}}{\partial u_i}(u)-\frac{\partial a_i^{\varphi}}{\partial u_j}(u)=0
$$ for all $u\in \overline{U}$ . I think of using the coordinate functions $a_i^{\varphi}$ to define a vector field in $\overline{U}$ or $\tilde{U}$ and achieve some geometric result with line integrals. But I have no idea how to define this field of vectors. Maybe I have not noticed any geometrical property of the sphere that can be used.","['differential-topology', 'differential-forms', 'differential-geometry']"
3065332,"Why does the Euler's totient function $\phi(n)$ give the minimum of exponent s.t. $a^{\phi(n)}\equiv 1\pmod n$, given that $(a,n)=1$?","I want to know whether it's possible that there would exist $1\lt k\lt\phi(n)$ s.t. $a^{\phi(n)}\equiv 1\pmod n$ , for a given $a$ and $n$ ? I need to prove/disprove it. I need some hints. (For title I meant minimum mod n.) OK, seems it may be too easy for my question but may I also ask that how to find the minimum even if I know that $(a,n)=1$ ? i.e. I have to know the $k$ s.t. $a^k\equiv 1\pmod n,$ given $(a,n)=1$ .",['number-theory']
3065343,"In Leibniz notation, how do you write the second derivative of y with respect to the square of x?","I know how to write in Leibniz notation for more plain-vanilla expressions like the the second derivative of y with respect to x. But I am not sure how to write, in Leibniz notation, the second derivative of y with respect to expressions more complex than a single variable. In, for example, the second derivative of y with respect to the square of x, would I just use parentheses and write $dy^2/d(x^2)^2$ ?","['notation', 'calculus', 'derivatives']"
3065389,Notation: gradient as vector field,"Consider the tangent space $T_p\mathbb{R}^n$ , and suppose $\{\big(\frac{\partial }{\partial x^i}\big)_p\}$ is a basis. So my textbook says that the gradient of a function $f$ , $f\in C^\infty(U)$ , $U\subseteq\mathbb{R}^n$ with $U$ open, is defined to be: $$\text{grad}(f):=\sum_{i=1}^n \frac{\partial f}{\partial x_i}$$ but I am failing to see why it would not be $$:=\sum_{i=1}^n \frac{\partial f}{\partial x^i}\frac{\partial }{\partial x^i}$$ so that evaluated at $p\in U$ , we get the gradient vector at $p$ . 
In other words, how is $\frac{\partial f}{\partial x^i}$ a vector field? Thanks","['differential-forms', 'tangent-bundle', 'smooth-manifolds', 'differential-geometry']"
3065395,Bifurcation points of differential equation (example),"Assume the differential equation: $$
x'=\lambda^2-8a\lambda x+2x^2, \quad a\in \mathbb{R}.
$$ The critical points are the solutions to the equation: $$
x'=0 \iff 2x^2-8a\lambda x +\lambda^2=0\tag{1}
$$ which admits solutions: $$
x=\lambda \cdot \frac{8a \pm \sqrt{64a^2-8}}{2}
$$ Now, having in mind that by bifurcation point , we mean a point where change of the number of equilibrium points occur, it seems to me that the number of $(1)$ 's solutions depends only on $a$ and not on $\lambda$ . Is this the case or did I get something wrong?","['stability-in-odes', 'bifurcation', 'ordinary-differential-equations', 'dynamical-systems']"
3065410,"Are these upper and lower bounds for $\frac{x!}{\left\lfloor{x}\right\rfloor!}$ useful? If so, are they already known?","Truncating the infinite series for the derivative of the Digamma function $$
\psi'(x) = \sum_{n=0}^\infty\frac{1}{(x + n)^2}
$$ after $m-1$ terms, where $m$ is a positive integer (the case $m=2$ answered the question How do we prove that $(x-1)!\leq{(\frac{x}{2})^{x-1}}$ ? ), finding upper and lower bounds for the remainder, and integrating twice between the limits $2$ and $2+x$ (at least, I think that's what I did, but it was a long slog, and my notation has changed several times since then), one arrives at the inequalities $$
\left(\frac{m+1+x}{m+1}\right)^{m+1+x}
\!\!\! < \frac{e^x(m+x)!}{e^{(H_m-\gamma)x}m!} <
\left(\frac{m+x}{m}\right)^{m+x}
\quad (x > 0;\ m = 1, 2, 3, \ldots).
$$ This seems most useful (if useful at all!) for smallish $x$ . Replacing $m+x$ by $x$ and $m$ by $\left\lfloor{x}\right\rfloor$ , we get $$
\left(\frac{x+1}{\left\lfloor{x}\right\rfloor+1}\right)^{x+1}
\!\!\! <
\frac{e^{x-\left\lfloor{x}\right\rfloor}}{e^{(H_m-\gamma)(x-\left\lfloor{x}\right\rfloor)}}
\cdot \frac{x!}{\left\lfloor{x}\right\rfloor!}
<
\left(\frac{x}{\left\lfloor{x}\right\rfloor}\right)^x
\quad(x > 1,\ x \notin \mathbb{N}).
$$ This seems to give sharper bounds than the following simple exact form of Stirling's approximation : $$
\sqrt{2\pi}n^{n+\frac{1}{2}}e^{-n} \leqslant n! \leqslant en^{n+\frac{1}{2}}e^{-n}.
$$ On the other hand, it seems to be generally inferior to the full version of Robbins's bounds: $$
\sqrt{2\pi}n^{n+\frac{1}{2}}e^{-n}e^{\frac{1}{12n+1}} < n! < \sqrt{2\pi}n^{n+\frac{1}{2}}e^{-n}e^{\frac{1}{12n}}.
$$ For small values of $x - \left\lfloor{x}\right\rfloor$ , my formula does sometimes give better results. For example, $7.04! \bumpeq 5463.7647$ , and in this case my formula gives the strict bounds $(5463.7292, 5463.8071)$ , whereas Robbins's formula gives $(5463.0514, 5463.8080)$ , and the simplified version of his formula gives the distinctly worse estimates $(5399.5135, 5855.4353)$ . Might my horrid formula therefore have some actual use? If so, has it been published already? Does it have a less nasty proof than the one I've sketched?","['alternative-proof', 'calculus', 'reference-request', 'inequality']"
3065421,Ant climbing on bush,"An ant is on the ground and trying to climb on a (straight) ivy bush 10m high. It crawls up 0.1m each night, but at day, the bush grows uniformly by 0.5m (in its entire height). Will the ant ever reach the top of the ivy? If yes, in how many days? If no, justify why. Let's convert everything in centimeters.
I have made a table with the values of the bush height every night and every day, basis the rules.
The ant starts with 1000 cm height above and 0 cm below. Then it advances by 10 cm so it has 990 cm above and 10 cm below. Total height is (still) 1000 cm.
In the morning, the height of the bush is proportionally extended by 5% so the up value is now $990*(5/1000)+990 = 1039.5$ and the down is $10*(5/1000)+10 = 10.5$ Total height is of course increased by 50 cm.
Then at night, first value is decreased by 10 and second is increased by 10. Height remains the same.
We continue this way and have the following values: Bush up (cm)    Bush down (cm)  Total Initially   1000    0   1000 1st night   990 10  1000 1st day     1039,5  10,5    1050 2nd night   1029,5  20,5    1050 2nd day     1078,52381  21,47619048 1100 3rd night   1068,52381  31,47619048 1100 3rd day     1117,093074 32,90692641 1150 4th night   1107,093074 42,90692641 1150 4th day     1155,227555 44,77244495 1200 5th night   1145,227555 54,77244495 1200 We continue this way and now we make a graph of the 3 columns, bush up, bush down and Total. We notice that the 3 curves are declining, which means that they will never intersect. This means that the ant will never reach the top. However, this is not correct. I was told that the problem has a positive reply. Where am I wrong? Thank you very much in anticipation!",['calculus']
3065444,Possible to prove that a particular trigonometric expression is always positive?,"This is a continuation of the an earlier post where the geometric motivation was presented. Here I'd like to ask: is it possible to prove $\Delta > 0$ always? $$\begin{align} 
\Delta &\equiv \sin(t) \sin\left(r+ (2 \pi -2 r - t)\frac{\epsilon}4 \right) \sin\left( \frac{2 - \epsilon}2 (\pi-r-t)\right) \\
&\quad {} - \frac{2 - \epsilon}2 \sin(r) \sin\left(t-\frac{\epsilon \; t}{4}\right) \sin(r+t)
\end{align}$$ where $$0<r<\frac{\pi}{4} \qquad 0<t<\frac{\pi}{4} \qquad 0<\epsilon <1$$ Some relevant posts include this one that renders the final form of $\Delta$ , which hasn't gotten satisfactory answers.","['trigonometry', 'optimization', 'calculus', 'algebra-precalculus']"
3065476,Line Integral Shift,"How does the integration identity $\int_{a + c}^{b + c} f(x - c)\ dx = \int_a^b f(x)\ dx$ extend to line integrals?  Would a line integral of $f(x(t),\ y(t))$ shifted to $a + c \leq t \leq b + c$ become $f(x(t) - c,\ y(t) - c)$ , $f(x(t - c),\ y(t - c))$ , or something different?  Does this depend on whether the line integral is with respect to $x$ or $y$ (scalar field), arclength (scalar field), or $\vec r$ (vector field), and in each case, which aspects of the problem's geometry would be altered by the shift, and which would be invariant?","['integration', 'multivariable-calculus', 'calculus', 'geometry']"
3065480,why is the Lebesgue-Stieltjes integral well-defined?,"A function $g: [a,b] \rightarrow \mathbb{R}$ a said to be of bounded variation on the interval $[a,b]$ if $$  \sup_{P: a=x_0 < x_1 \ldots < x_i < \ldots < x_{n_P}=b}   \sum_{i=1}^{n_P} |g(x_{i}) -g(x_{i-1}))| < \infty,  $$ where the supremum if taken over all partitions $P$ of the interval $[a,b]$ . One can show that a (right-continous) function $g: [a,b] \rightarrow \mathbb{R}$ is of bounded variation if and only if there exist two monotone non-decreasing (right-continuous) functions $g^+$ and $g^-$ such that $g= g^+ -g^-$ . (However, this decomposition is not unique, as one can for example add any monotone non-decreasing (right-continuous) function to both $g^+$ and $g^-$ ). This is called a Jordan decomposition of $g$ . Moreover, given any $g: [a,b] \rightarrow \mathbb{R}$ which is monotone, non-decreasing and right-continuous function, there exists a unique measure $dg$ on $[a,b]$ such that $$    dg((c,d])=g(d)-g(c)   $$ for all $(c,d] \in [a,b]$ and $dg(\{a\})=0$ . This measure is called the Lebesgue-Stieltjes measure of $g$ . Given a bounded function $f: [a,b] \rightarrow \mathbb{R}$ and a right-continuous function of bounded variation $g : [a,b] \rightarrow \mathbb{R}$ , one can define $$     \int_{[a,b]} f\,dg :=   \int_{[a,b]} f\,dg^+  -  \int_{[a,b]} f\,dg^-,     $$ where $g= g^+ -g^-$ is a Jordan decomposition as above. This is called the Lebesgue-Stieltjes integral of $f$ w.r.t. $g$ . My question is, why is $\int_{[a,b]} f\,dg$ well-defined, i.e. independent of the chosen Jordan decomposition?","['measure-theory', 'stieltjes-integral', 'bounded-variation']"
3065484,"Proof that $\sin {x}$ is infinitely continuously differentiable over $[m,n]$","I am trying to prove that $\sin {x}$ is infinitely continuously differentiable over $[m,n]$ where $m$ and $n$ are real numbers. Here is my attempt at doing so. Is my proof complete? If not, what can I do to improve it? Thank you in advance. Since, $\frac{d}{dx}\sin{x} = \cos{x}$ , $\frac{d^2}{dx^2}\sin{x} = -\sin{x}$ , $\frac{d^3}{dx^3}\sin{x} = -\cos{x}$ , and $\frac{d^4}{dx^4}\sin{x} = \sin{x}$ , the derivatives of $\sin{x}$ , are periodic. Since the first four derivatives of $\sin{x}$ are continuous over $[m,n]$ where $m$ and $n$ are real numbers, $\sin{x}$ must be differentiable an infinite amount of times over $[m,n]$ .","['calculus', 'proof-verification', 'trigonometry', 'proof-writing']"
3065516,"Zero conditional mean, and is regression estimating population regression function?","I am relearning econometrics to get a better understanding of it, and to clear the confusions when I had in college. Using the simple regression model, we have a population model equation as: $$ y = \beta_{0} + \beta_{1}x + u\tag{1}$$ In the SLR assumption 3, we have the zero conditional mean assumption . Are we assuming this statement because in reality, y can take many values given x taking a single value, so that we hope, given x, the expected value of y is center around E[y|x], is this understanding of SLR Assumption 3 correct ? This means, if we take the expected value of equation (1) conditioned on x: $$ E[y|x] = \beta_{0} + \beta_{1}x + E[u|x]\tag{2}$$ Because any deviation can be absorbed by the intercept item, we lose nothing by assuming E[u] = 0. By assuming SLR 3 E[u|x] = E[u] = 0, we are implying that: $$ E[u|x] =\sum{}u P_{u|x}(u) = \sum{}u\dfrac{P_{u,x}(u,x)}{P_{x}(x)}=\sum{}u\dfrac{P_{u}(u)P_{x}(x)}{P_{x}(x)} = \sum{}uP_{u}(u) = E[u]\tag{3}$$ In order to get the above equation, we are implying that x and u are independent of each other, so we are also implying that from the covariance formula of x and u, we can get the following equation E[ux] = E[u] = 0: $$ Cov(u, x) = E[ux] - E[u]E[x] = E[ux] - 0 = E[u]E[x] = 0 \tag{4}$$ Thus: $$ E[ux] = 0 \tag{5}$$ And, because of this implied uncorrelated relationship between u and x, the equation (2) above can be viewed as when E[u|x] = 0, so we have the population regression function by taking expectation conditioned on x for equation (1), as: $$ E[y|x] = \beta_{0} + \beta_{1}x \tag{6}$$ This is a linear relationship between x and expected value of y, by the change of 1 unit in x leads to beta1 unit change in y. And the distribution of y is centered at E[y|x]. So my question is that, when we are estimating using OLS, is the sample regression function estimating the population model equation (1) or estimating the population regression function equation (6) and why? Also, in multiple regression function, we also assume zero conditional mean as: $$ E[u|x_{1}, x_{2}, x_{3},...,x_{k}] = 0 $$ Here are we saying that u is uncorrelated with the group of (x1,...xk), or can we say that u is uncorrelated with each of xi respectively, for i = 1,...,k? Thank you for your help and time! Much obliged.","['regression', 'statistics']"
3065525,Simple Statistics/Probability Problem,"I have used a python script to identify target sequences in a DNA sequence file. There are two classes of sequence: coding and non-coding. I have identified $728$ sequences of interest. $597$ of these fall into the coding regions and $131$ of these fall into the non-coding regions. This is the equivalent of $18\%\,$ non-coding, but the total non-coding region in the sequence file is $13\% $ . Is there a statistical tool to demonstrate the python script identified target sequences in a non-random fashion way? If the script identified sequences that were randomly distributed then $13\% $ of them would have been found in the non-coding region, from a total of $728$ sequences. This seems like it should be reliable. I hope my question is clear.","['biology', 'statistics', 'probability']"
3065572,$\lim\limits_{n\to\infty}f\left(\frac{x}{n}\right)=0$ for every $x > 0$. Prove $\lim\limits_{x \to 0}f(x)=0$,"Function $f: (0, \infty) \to \mathbb{R}$ is continuous. For every positive $x$ we have $\lim\limits_{n\to\infty}f\left(\frac{x}{n}\right)=0$ . Prove that $\lim\limits_{x \to 0}f(x)=0$ . I have tried to deduce something from definition of continuity, but with no effect.","['limits', 'continuity']"
3065579,Understanding the Legendre transform,"In physics, I've seen the Legendre transform motivated by ""changing the variable $x$ of a function $x \mapsto f(x)$ to the variable $u = \frac{df}{dx}$ ."" I don't quite see what that means and why the Legendre transform is the answer to this heuristic. I'd understand it as follows: Let $f: \mathbb{R} \to\mathbb{R}$ be strictly convex and differentiable. Then for every $x_0 \in \mathbb{R}$ the slope $\frac{df(x_0)}{dx}$ is unique. We want to find a function $f^*: f'(\mathbb{R}) \to \mathbb{R}$ such that $f(x_0)=f^*(\frac{df(x_0)}{dx})$ for every $x_0 \in \mathbb{R}$ . In fact we can view the function $f^*$ as a function on a subset of the dual space $\mathbb{R^*}$ such that $f^*(df(x_0))=f(x_0)$ . However, this doesn't seem to capture the Legendre transform, for example by the above the Legendre transform of the exponential function should be the identity function. How can the physicists heuristic be made precise?","['functions', 'real-analysis']"
3065588,Computing the Laplacian in Polar Coordinates [duplicate],"This question already has an answer here : Computing second partial derivative with polar coordinates (1 answer) Closed 5 years ago . Similar questions have been asked on this site but none of them seemed to help me. I'm asked to compute the Laplacian $$\frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}$$ in terms of polar coordinates. I did do it, but I don't understand why what I did is correct, and I don't understand the more ""brute force"" way to do it at all. Here is what I did: I calculated $\frac{\partial}{\partial r}$ and $\frac{\partial}{\partial \theta}$ in terms of $r,$ $\theta,$ $\frac{\partial}{\partial x}$ and $\frac{\partial}{\partial y}.$ This gave me a system of linear equations which I wrote as $$\begin{pmatrix}\frac{\partial}{\partial r} \\\frac{\partial}{\partial \theta}\end{pmatrix} = \begin{pmatrix}\cos\theta & \sin\theta \\\ -r\sin\theta & r\cos\theta \end{pmatrix}\begin{pmatrix}\frac{\partial}{\partial x} \\\frac{\partial}{\partial y}\end{pmatrix}.$$ I inverted to get $$\begin{pmatrix}\frac{\partial}{\partial x} \\\frac{\partial}{\partial y}\end{pmatrix} = \frac{1}{r}\begin{pmatrix}r\cos\theta & -\sin\theta \\\ r\sin\theta & \cos\theta \end{pmatrix}\begin{pmatrix}\frac{\partial}{\partial r} \\\frac{\partial}{\partial \theta}\end{pmatrix},$$ and then I simply wrote \begin{align}\frac{\partial^2}{\partial x^2} &= \frac{\partial}{\partial x} \left( \frac{\partial}{\partial x}\right)\\ &= \left(\cos\theta \frac{\partial}{\partial r}-\frac{1}{r}\sin\theta\frac{\partial}{\partial \theta}\right)\left(\cos\theta \frac{\partial}{\partial r}-\frac{1}{r}\sin\theta\frac{\partial}{\partial \theta}\right)\\ &= \cos\theta\frac{\partial}{\partial r}\left(\cos\theta \frac{\partial}{\partial r}-\frac{1}{r}\sin\theta\frac{\partial}{\partial \theta}\right) -\frac{1}{r}\sin\theta\frac{\partial}{\partial \theta}\left(\cos\theta \frac{\partial}{\partial r}-\frac{1}{r}\sin\theta\frac{\partial}{\partial \theta}\right)\\ &= \cos^2\theta \frac{\partial^2}{\partial r^2} - \frac{2}{r}\sin\theta\cos\theta\frac{\partial^2}{\partial r
\partial\theta} +\frac{1}{r^2}\sin^2\theta\frac{\partial^2}{\partial \theta^2}.\end{align} I similarly got $$\frac{\partial^2}{\partial y^2} = \sin^2\theta \frac{\partial^2}{\partial r^2} + \frac{2}{r}\sin\theta\cos\theta\frac{\partial^2}{\partial r
\partial\theta} +\frac{1}{r^2}\cos^2\theta\frac{\partial^2}{\partial \theta^2}.$$ Adding the two yields $$\frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2} = \frac{\partial^2}{\partial r^2}+\frac{1}{r^2}\frac{\partial^2}{\partial \theta^2},$$ which Spivak says is correct. Explicitly, here are my questions: In my solution, when I found $\frac{\partial^2}{\partial x^2},$ I simply ""multiplied"" the expressions in the second line of the large aligned equation (treating multiplication of the partial operators as composition). Why am I allowed to do this? Why does the expression on the left not act on the thing on the right, forcing me to do the product rule and other nonsense to get the answer? My original idea was just to compute the Laplacian using the chain rule. That is, write $\frac{\partial}{\partial x}=\frac{\partial}{\partial r} \frac{\partial r}{\partial x}$ and compute $\frac{\partial^2}{\partial x^2}$ from there. My problem with this is that I keep getting confused about in which variables I should be writing everything, and how the partial derivative operators act on these other expressions. For example, I compute $\frac{\partial}{\partial x}=\frac{\partial}{\partial r} \frac{\partial r}{\partial x} + \frac{\partial}{\partial \theta} \frac{\partial \theta}{\partial x} = \frac{x}{\sqrt{x^2+y^2}}\frac{\partial}{\partial r} - \frac{y}{x^2+y^2} \frac{\partial}{\partial \theta},$ but then I don't know where to go. Help understanding this method would be greatly appreciated. If anything is unclear let me know and I'll make the necessary edits.","['partial-derivative', 'polar-coordinates', 'derivatives']"
3065598,Singular locus of dual hypersurfaces,"Everything is over field $\mathbb C$ . Let $X$ be a hypersurface of degree $d$ in $\mathbb P^n$ . We know that if $X$ is smooth, then its dual $X^\vee$ is still a hypersurface in $(\mathbb P^n)^\vee$ , but not necessarily smooth. I want to know that, for a general choose of $X$ , can we compute the dimension of the singular locus $X^\vee_{sing}$ ? Where can I find a discussion of this? For some reason, I believe in general it is of codimension $1$ In $X^\vee$ , or it is empty. But I donât know how to prove this. Thanks in advance.","['complex-geometry', 'algebraic-geometry', 'intersection-theory']"
3065604,Proof that $fg \geq 1 \implies \int_E f\ d\mu\int_E g\ d\mu \geq \mu^2(E)$,"$(E,A,\mu)$ is a finite measure space, $f$ and $g$ are both positive and measurable functions from $E$ to $\mathbb{R}$ such that $fg \geq 1$ by holder we have $$(\int_E f^2 \ d\mu)(\int_E g^2\ d\mu) \geq (\int_E fg \ d\mu)^2 \geq (\int_E \ d\mu)^2 \geq \mu^2(E)$$ how about this one though : $\int_E f\ d\mu\int_E g\ d\mu \geq \mu^2(E)$ if $f = h^2, \, g = w^2$ then $|h||w| \geq 1$ and $|h|,\,|w| \geq 0$ and measurable therefore by what precedes $\int_E f\ d\mu\int_E g\ d\mu = (\int_E h^2 \ d\mu)(\int_E w^2\ d\mu)  \geq  \mu^2(E)$ Is my approach correct?","['integration', 'measure-theory', 'proof-verification', 'analysis']"
3065610,Bounding expectation of stopping time,Let $(X_{t})_{t\ge0}$ be adapted to $(\mathcal{F}_{t})_{t\ge0}$ with continuous trajectories. Assume that $X_{0} = 0$ and $X_{t}^{4} - 3t^{2}$ is a martingale with respect to $(\mathcal{F}_{t})$ . Let $$\tau = \inf\left\{t>0 : |X_{t}| = \sqrt[4]{(t^2+9)}\right\}.$$ Prove that $\mathbb{E}\tau^2 < \infty$ and compute $\mathbb{E}\tau^2$ . I know that if $\mathbb{E}\tau^2 < \infty$ then by Doob we get $\mathbb{E}\tau^2 = 4.5$ . The problem is that I cannot prove $\mathbb{E}\tau^2 < \infty$ . I was thinking of a function $g(t) = |X_{t}| - \sqrt[4]{(t^2+9)}$ which is continuous a.s. and $g(0) < 0$ . Then if I find any $t_{0}$ such that $g(t_{0}) > 0$ it will be proved. Is there any other way of doing that?,"['expected-value', 'martingales', 'stopping-times', 'probability-theory']"
3065639,If an immersion $X$ maps circles into planes then its image $X(\mathbb{D})$ is homeomorphic to the cylinder.,"Let $X:\left( u,v\right)\in \mathbb{D}\backslash \left\{ 0\right\}\subseteq\mathbb{R}^2 \mathbb{%
\longmapsto }\left( x\left( u,v\right) ,y\left( u,v\right) ,z\left(
u,v\right) \right) \in \mathbb{\mathbb{R}}^{3}$ an minimal immersion, where $\mathbb{D=}\left\{ p\in \mathbb{R}^{2};\left\Vert p\right\Vert <1\right\} $ is unitary open disc. If $X$ maps circles into planes (coordinate function $z$ constant) then $X(\mathbb{D})$ is homeomorphic to cylindric. I do not know if all these assumptions are necessary, but that's what I have. Can you help me prove that? I studied geometry and topology a long time ago.","['geometry', 'analysis', 'general-topology', 'differential-topology', 'differential-geometry']"
3065643,Prove Fibonacci Sequence Property: $x^2_n + x^2_{n+1}=x_{2n+1}$,"For Fibonacci Sequence , We know that the recursive difference equation is: $$
x_{n+2} = x_n + x_{n+1}\ \ \ \ n\ \geq 0
$$ And that the closed form solution is: $$
x_n = \frac{1}{\sqrt{5}}\left[ \left(\frac{1+\sqrt{5}}{2}\right)^{n} - \left(\frac{1-\sqrt{5}}{2}\right)^{n}  \right]\ \ \ \  n \geq 0
$$ But how to prove that this fibonacci property is true using this information?: $$x^2_n + x^2_{n+1}=x_{2n+1}$$ Hint: substitute the closed form expression for the fibonacci sequence into difference equation and verify that its true. well...I've tried this at least 3 different ways of doing exactly as hinted without much luck...the equation always blows up on me.  Any ideas how to prove  it?","['fibonacci-numbers', 'discrete-mathematics']"
3065678,Maximizing $f$ in $\mathbb{R}^3$,"Find the domain and the maximum value that the function $$f(x,y,z)=\frac{x+2y+3z}{\sqrt{x^2+y^2+z^2}}$$ may attain in its domain. I have found the domain of the function to be $\mathbb{R^3\backslash\mathbf{0}}$ . To maximize I differentiated in terms of $x,y,z$ having $$f_x=\frac{-2 x y-3 x z+y^2+z^2}{\left(x^2+y^2+z^2\right)^{3/2}},\quad f_y=\frac{2 x^2-x y+z (2 z-3 y)}{\left(x^2+y^2+z^2\right)^{3/2}},\quad f_z=\frac{3 \left(x^2+y^2\right)-z (x+2 y)}{\left(x^2+y^2+z^2\right)^{3/2}}$$ But to solve the system $f_x=0,f_y=0$ and $f_z=0$ is rather hard. What are the plausible values of $x,y,z$ ?","['maxima-minima', 'multivariable-calculus', 'cauchy-schwarz-inequality', 'partial-derivative', 'optimization']"
3065679,Maximum runs of composites in arithmetic progressions,"Is there a proof that every arithmetic progression of gap $p$ has a prime in the interval $[p, p^2)$ ? Put another way, can you prove the following: For all primes $p$ , and all integers $0 \le m <p$ , there exists $n\in\Bbb{Z}$ with $1\le n < p$ such that $np+m$ is prime. I think I have a proof sketch but it's fiddly and I don't want to reinvent the wheel (likely badly). To illustrate, here's the result checked for $p = 5$ : Arithmetic progressions with gaps $5$ from $5$ to $24$ ( $p$ to $p^2-1$ ): $\boldsymbol{5},10,15,20$ $6,\boldsymbol{11},16,21$ $\boldsymbol{7},12,\boldsymbol{17},22$ $8,\boldsymbol{13},18,\boldsymbol{23}$ $9,14,\boldsymbol{19},24$ As you can see, in bold , all of these arithmetic progressions have at least one prime. If one of these sequences didn't have a prime the theorem would be disproven for $p = 5$ . Note that proving that every arithmetic progression of gap $p$ and length $(p-1)$ has at least one number coprime to $(p-1)!$ should also prove the above, and indeed is more general, as if $n$ is coprime to $(p-1)!$ and is less than $p^2$ then it has to be prime.","['number-theory', 'prime-gaps', 'elementary-number-theory', 'prime-numbers']"
3065686,How do the roots behave asymoptotically?,"Let $g, h \in \mathbb C[x]$ and $$ f(x, a) = (x-x_0)^m g(x) + h(x) (a-a_0),$$ where $m \ge 2$ , $a \in \mathbb R$ and $a_0$ is a fixed real number. Suppose $g(x_0) \neq 0$ and $h(x_0) \neq 0$ . By this setup, we should be able to have $n$ continuous functions $\alpha_1, \dots, \alpha_n: \mathbb R \to \mathbb C$ such that for each $t \in \mathbb R$ , $\alpha_1(t), \dots, \alpha_n(t)$ constituents the zeros of $f(x, t)$ . If $a \to a_0$ , then we should have $m$ functions converges to $x_0$ . I am wondering how these functions behave. More specifically, it seems to me: we can set $$ (x-x_0)^m g(x) + h(x)(a-a_0) = 0.$$ As $a \to a_0$ , $g(x) \to g(x_0)$ and $h(x) \to h(x_0)$ . If I am allowed to hand wave a little bit, then in a neighborhood of $a_0$ $$ \alpha_j(a) \approx x_0 + \left( \frac{-h(x_0)}{g(x_0)} (a-a_0) \right)^{1/m} \omega_j^m, \text{ for } j=1, \dots, m,$$ where we assume $\alpha_1, \dots, \alpha_m$ are functions converging to $x_0$ and $\omega_j^m$ are solutions to $x^m =1 $ . Is there a way to a rigorous statement on the asymptotic behavior of $\alpha_j$ 's?","['complex-analysis', 'roots', 'polynomials']"
3065712,Let K be the splitting field of the polynomial $x^4-2x^2-2$. Find an automorphism $\sigma \in Gal(K/\mathbb{Q})$ of order 4.,"My attempt:
The roots of the polynomials are $x=\sqrt{1+\sqrt{3}}, -\sqrt{1+\sqrt{3}}, -\sqrt{1-\sqrt{3}}, -\sqrt{1-\sqrt{3}}$ . and the automorphisms $\sigma \in Gal(K/\mathbb{Q})$ permutes the root of the polynomial. But none of such automorphisms are of order 4! I don't know how to proceed further. Thanks for any help!","['field-theory', 'galois-theory', 'group-theory']"
3065720,Laplace Transform of $y^n$,"When the Laplace transform of $y$ is denoted $Y(s)$ , we have formulas for the derivatives of $y$ without actually knowing what $y$ is. Is there an explicit formula for $y^2$ ? More generally, $y^n$ ?","['laplace-transform', 'ordinary-differential-equations']"
3065723,Infinite Rubik's Cube,"Is there an infinite analog to the Rubik's Cube?  What does its solution-algorithm look like?  For illustration, consider the Rubik's cube with infinite tiles to a side, on all sides, with sides of finite length.","['group-theory', 'puzzle', 'combinatorics']"
3065747,Solving for $x$ in $\sin^{-1}(2x) + \sin^{-1}(3x) = \frac \pi 4$,"Given an equation: $$\sin^{-1}(2x) + \sin^{-1}(3x) = \frac \pi 4$$ How do I find $x$ ? I tried solving by differentiating both sides, but I get $x=0$ . How do you solve it, purely using trigonometric techniques?","['trigonometry', 'inverse-function']"
3065769,Find maximum value of $\frac xy$,"If $x^2-30x+y^2-40y+576=0$ , find the maximum value of $\dfrac xy$ . First I completed the squares and got $(x-15)^2+(y-20)^2=7^2$ , which is the equation of a circle. I think I need to use some properties but I don't know what to do next.","['optimization', 'algebra-precalculus', 'maxima-minima', 'discriminant']"
3065819,Is it possible to multiply a set by a natural number? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Say I have a set $S=\{1, 4, 10, 7\}$ . Could I then multiply $S$ by $3$ ? Would that then look like $3S=\{3, 12, 30, 21\}$ ? Any help would be really appreciated.","['elementary-set-theory', 'discrete-mathematics']"
3065865,Count the number of sets of subsets of (Steiner?) triples,"Given a set of $v$ numbers. Fix $v$ . How many sets of $v$ sorted triples can be created, matching the following conditions: two triples shall have atmost one number in common over all triples each number shall appear three times? A representative example set of triples with $v=8$ : $$
S_A=(\color{red}1,\color{blue}2,\color{green}5)\;;\;S_B=(\color{blue}2,\color{grey}3,\color{purple}6)\;;\;S_C=(\color{grey}3,\color{pink}4,\color{orange}7)\;;\;S_D=(\color{red}1,\color{pink}4,8)
$$ $$
S_E=(\color{blue}2,\color{orange}7,8)\;;\;S_F=(\color{grey}3,\color{green}5,8)\;;\;S_G=(\color{pink}4,\color{green}5,\color{purple}6)\;;\;S_H=(\color{red}1,\color{purple}6,\color{orange}7)
$$ Can this be seen as $(v, b, r, k, Î»)$ block designs ? When I read the definition there I find: $v(=8)$ :   points, number of elements of X $b(=8)$ :   number of blocks $r(=3)$ :   number of blocks containing a given point $k(=3)$ :   number of points in a block $\lambda(=1)$ :   number of blocks containing any 2 distinct points But obviously this doesn't fulfill the condition ${\displaystyle \lambda (v-1)=r(k-1)}$ ... UPDATE My example looks like a subset of elements of the Steiner Triple System $STS(9)$ , where subsets that contain $9$ are omitted. The question is motivated by bipartite cubic graphs. Bipartiteness forces $v=b$ , cubicity asks for $r=k=3$ . $\lambda =1 $ means that the graph is simple and has no squares. The example above is derived from the graph shown here . It has 6 octagons living on a double torus. Black points are labelled with numbers $1$ to $6$ ; white points are labelled with characters $A$ to $H$ .","['combinatorial-designs', 'algebraic-graph-theory', 'combinatorics']"
3065885,Turning $\mathbb R^n$ into field,"I am reading Apostol's fascinating text Mathematical Analysis . In a footnote on P117, he writes: If it were possible to define multiplication in $\mathbb R^3$ so as to make $\mathbb R^3$ a field including $\mathbb C,$ we could argue as follows: for every $\bf x$ in $\mathbb R^3$ , the vectors $1,\bf x,\bf x^2,\bf x^3$ would be linearly dependent. Hence for each $\bf x$ in $\mathbb R^3,$ a relation of the form $a_0+a_1{\bf x}+a_2{\bf x^2}+a_3{\bf x^3}=0$ would hold, where $a_0,a_1,a_2,a_3$ are real numbers.But every polynomial of degree three with real coefficients is a product of a linear polynomial and a quadratic polynomial with real coefficients. The only roots such polynomials can have are either real numbers or complex numbers. I have these couple of questions: Does above argument show that $\mathbb R^3$ can't be made a field?  Or just that $\mathbb R^3$ can't be field such that $\mathbb C$ is its subfield? How are we so sure that there are no other roots than complex numbers? Maybe we have not explored enough!","['real-analysis', 'complex-analysis', 'field-theory', 'extension-field', 'complex-numbers']"
3065902,Help with filling in the details to show that $\lim\limits_{n\to\infty} \sum\limits_{k=1}^{n}\left(\frac{k}{n}\right)^n=\frac{e}{e-1}$,"So we have, $$\begin{align}
\lim_{n\to\infty} \sum_{k=1}^{n}\left(\frac{k}{n}\right)^n &= \lim_{n\to\infty} \sum_{j=0}^{n-1}\bigg(\frac{n-j}{n}\bigg)^n \\
&= \lim_{n\to\infty} \bigg(1+\bigg(1-\frac{1}{n}\bigg)^n+...+\bigg(1-\frac{n-1}{n}\bigg)^n\bigg) \\
&= 1+e^{-1}+e^{-2}+... \\
&= \frac{e}{e-1}
\end{align},$$ but my problem is going from the second to the third line. As the limit involves both the summand and the sum and I am not sure how this line is ""legal"", that is what is it that allows is to apply the limit first to each term then to the sum, viz why is that $\lim\limits_{n\to\infty} \sum\limits_{k=0}^{n-1}\bigg(1+\frac{-k}{n}\bigg)^n=\sum\limits_{k=0}^\infty\lim\limits_{n\to\infty}\bigg(1+\frac{-k}{n}\bigg)^n$ ? Is there a double limit? But can you have a double limit involving the same variable? Is this some special case of Fubini's Theorem, if so I am really struggling to see how? Or maybe it is a Riemann sum? If so so I'm not sure how to show t manipulate it to show that.  Any help will be greatly appreciated. Thanks in advance.","['limits', 'real-analysis']"
3065909,Find $(1-p)\log\left [\int^{\infty}_{0} [f(x)]^pdx\right]$,"I want to find \begin{align}(1-p)\log\left [\int^{\infty}_{0} [f(x)]^pdx\right],\end{align} given that $\alpha,\beta, \theta$ and $p$ are constants and \begin{align} f(x)=\dfrac{ \frac{ \theta\alpha \beta}{x^2}\left( 1+\frac{ \beta}{x}  \right) ^{-(1-\alpha)} }{\Big\{ 1-(1-\theta)\left[1-\left( 1+\frac{ \beta}{x}  \right) ^{-\alpha}   \right] \Big\}^2},\;x\in\Bbb{R}\end{align} MY TRIAL By substitution, let $u=1+\frac{ \beta}{x}, $ then $du=-[(u-1)^2/\beta]dx$ \begin{align} f(u)&=\dfrac{ \frac{ \theta\alpha }{\beta}(u-1)^2u ^{-(1-\alpha)} }{\Big\{ 1-(1-\theta)\left[1-u ^{-\alpha}   \right] \Big\}^2}\\&=\dfrac{ \frac{ \theta\alpha }{\beta}(u-1)^2u ^{-(1-\alpha)} }{ 1-2(1-\theta)\left[1-u ^{-\alpha}   \right]+(1-\theta)^2\left[1-u ^{-\alpha}   \right] ^2}\end{align} I'm stuck here, please, how do I continue?","['integration', 'probability-distributions', 'analysis']"
3065914,Is there a function $f: \mathbb{R} \to \mathbb{R}$ such that $\lim\limits_{x\to p}f(x)=\infty$ for every $p \in \mathbb{R}$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I was wondering if there is a function $f: \mathbb{R} \to \mathbb{R}$ such that its limit at every point is infinite. I guess not, because what would its graph look like, but then again, I don't know how to prove it.",['limits']
3065950,Algebraic Closed Form for $\sum_{n=1}^{k}\left( n- 3 \lfloor \frac{n-1}{3} \rfloor\right)$,"Let's look at the following sequence: $a_n=\left\{1,2,3,1,2,3,1,2,3,1,2,3,...\right\}$ I'm trying to calculate: $$\sum_{n=1}^{k} a_n$$ Attempts: I have a Closed Form for this sequence. $$a_n=n- 3 \bigg\lfloor \frac{n-1}{3}  \bigg\rfloor$$ The problem is, I'm looking for a closed form for this summation: $$\sum_{n=1}^{k}\left( n- 3 \bigg\lfloor \frac{n-1}{3}  \bigg\rfloor\right)$$ Is it possible?","['algebra-precalculus', 'closed-form', 'summation', 'ceiling-and-floor-functions']"
3065969,Expected number of consecutive guesses to get a given sequence of numbers,"I have a lock on my dorm door that's really stupid. Basically, it just checks whether or not the sequence of numbers I've put in is the combo, whether or not the lock was reset between guesses. So let's say my combo is 5556. Then I can input 555555556 into my lock and it'll unlock, without having to reset after inputting the first four numbers. I tried to calculate the expected number of random number guesses to eventually input the right combo by assuming each ""guess"" was independent. For example, the input of 123454321 has 6 ""guesses"": 1234, 2345, 3454, 4543, 5432, 4321. Assuming this, the expected length of input required would be 10,000, for 10^4 permutations of a 4 digit combo. However, to check my work, I created a simulation with a queue object and random number generators and ran 100 trials per experiment over 100 experiments in Python. In every experiment, the average was always above 10,000 by a significant margin, ranging from 500-2000. I'm wondering, are the guesses really independent? What is the actual expected value?","['permutations', 'expected-value', 'combinatorics']"
3065973,Infinite summation formula for modified Bessel functions of first kind,"I was trying to find a closed form for the integral $$4\int_0^{\pi/2} t \, I_0(2\kappa\cos{t}) dt \; ,$$ where $$I_{\alpha}(z) := i^{-\alpha}J_{\alpha}(iz) = \sum_{m=0}^{\infty}\frac{\left(\frac{z}{2}\right)^{2m+\alpha}}{m! \Gamma(m+1+\alpha)} = \frac{1}{2\pi} \int_{-\pi}^{\pi} e^{i\alpha \tau + z \sin{\tau}} d\tau$$ are the modified Bessel functions. This integral popped up when I was trying to find the average difference of two points on a circle, where these points are assumed to be drawn independently from a von Mises distribution. It was noted by Robert Israel that this integral can be reduced to $$ \int_0^\pi t I_0(2\kappa \cos(t/2)) \; dt = \frac{\pi^2}{2} I_0(\kappa)^2 - 4 \sum_{r=0}^\infty \frac{I_{2r+1}(\kappa)^2}{(2r+1)^2} \; .$$ So I was wondering, if we can further simplify  this expression, or to stated more clearly: Is there a closed formula for the following sum of modified Bessel functions of the first kind? $$\sum_{r=0}^\infty \frac{I_{2r+1}(\kappa)^2}{(2r+1)^2}$$ A lot of remarkable identities in terms of infinite sums of Bessel functions are known. E.g. Abramowitz and Stegun list in Â§9.6.33ff. a few of them, like: $$\begin{align}
1        &= I_0(z) + 2\sum_{r=1}^{\infty} (-1)^{r}I_{2r}(z) \\
e^z      &= I_0(z) + 2\sum_{r=1}^{\infty} I_{r}(z) \\
\cosh{z} &= I_0(z) + 2\sum_{r=1}^{\infty} I_{2r}(z) \\
\end{align}$$ WolframResearch lists another bunch of infinite series identities.
Also, Neumann's addition theorem seems to work wonders <1> <2> <3> . Regarding the integral itself, Gradshteyn and Ryzhik mention in 6.519.1 that $$\int_0^{\pi/2} J_{2r}(2\kappa\cos{t}) = \frac{\pi}{2} J_r^2(\kappa) \; ,$$ where $J_r(x) = i^rI_r(-ix)$ . So there might be a chance to expect something along this line. Going back to the original problem "" What is the expected value of a distribution $\Delta$ with the following density function "" $$f_{\Delta}(t) := \frac{I_0 \left( 2\kappa \cos{\frac{t}{2}} \right)}{\pi I^2_0(\kappa)} \; ,$$ a straightforward integration leads to the integral mentioned above. Using some probability theory voodoo we can make use of the fact that $$\mathbb{E}[\Delta] = -i \varphi'_{\Delta}(0) 
= -i \left[\frac{d}{d\omega} 
\mathcal{F}(f_{\Delta})(\omega)
\right] \Bigg|_{\omega=0} 
= -i \left[\frac{d}{d\omega} 
\int_{-\infty}^{\infty} e^{it\omega}f_{\Delta}(t) dt 
\right] \Bigg|_{\omega=0} $$ where $\varphi_{\Delta}$ is the characteristic function of $f_{\Delta}$ and $\mathcal{F}$ the (properly scaled) Fourier transform. Now with $\varphi(-\omega) = \overline{\varphi(\omega)}$ , we could further rewrite $$\mathbb{E}[\Delta] = -i\varphi'_{\Delta}(0) 
= \lim_{\omega \rightarrow 0} \frac{\varphi_{\Delta}(\omega) - \varphi_{\Delta}(-\omega)}{2i\omega} 
= \lim_{\omega \rightarrow 0} \frac{\mathcal{Im}\left(\varphi_{\Delta}(\omega)\right)}{\omega} \,$$ to (by plugging in the integral representation of $I_0$ ) obtain $$\mathbb{E}[\Delta] 
= \frac{\pi}{2} - \frac{4}{\pi I_0^2(\kappa)} \sum_{r=0}^\infty \left( \frac{I_{2r+1}(\kappa)^2}{2r+1} \right)^2 
= \frac{1}{\pi^2 I_0^2{\kappa}} \cdot \lim_{\omega \rightarrow 0} \int_0^{\pi/2} \int_{-\pi}^{\pi} \frac{\sin(t\omega)}{\omega} e^{2\kappa\cos{t}\sin{\tau}} d\tau \, dt \; ,$$ but this will essentially lead to the same integral we started with. The promising part about this approach is that the Fourier transform pops up, which might leave some room for the harmonic analysis people among you to do your magic.","['bessel-functions', 'definite-integrals', 'closed-form', 'sequences-and-series']"
3066000,"How to solve for $\theta$ in terms of $a,b,c$ in this expression?","I am trying to design a cam profile like in the above image. There are 2 equations I have come up with that define variable ""c"". The problem, however, is that variables a, b & c are known values and it is angle $\theta$ that I need to know. The 2 arcs on either end of the straight line are equal. The radius of the arcs and the angle $\theta$ change when any of the variables are changed. The 2 equations I have so far: $c = \frac{2a}{\sin \theta}(1-\cos \theta)+\frac{b.\sin \theta}{\cos \theta} $ $c = 2 \left[ \frac{a}{\sin \theta} - \sqrt{ \left( \frac{a}{\sin \theta} \right)^2 - a^2}\, \right] + \frac {b.\sin \theta}{\cos \theta} $ I can't for the life of me manage to rearrange the equation to make $\theta$ the subject! If anyone is an algebra whiz, any help would be much appreciated! Cheers. EDIT Ok, so after a bit of geometric shenanigans, I have come up with some different equations to help solve this but and still struggling to rearrange it. (However, I had to replace $\theta$ with $x$ as I couldn't get $\theta$ to work with the program I drew this with!) By drawing a chord for the arc, we can show that the angle of the chord is half that of the slope. What I really need to find is the length of either $e$ or $d$ , where $d + e = \frac{c}{2}$ Using the half angle formula I get: $\tan x = \frac{2 \tan \frac{x}{2} }{1 - \tan^2 \frac{x}{2}}$ Which can be written as: $\frac{c-2e}{b} = \frac{\frac{2e}{a}}{1-\frac{e^2}{a^2}} $ Simplifying to: $\frac{c-2e}{b} = \frac{2ae}{a^2-e^2}$ My problem now, is that I can't simplify it to get $e$ on its own! If anyone has any ideas, that would be awesome!","['algebra-precalculus', 'trigonometry']"
3066012,Finding irrational entries such that the determinant will never be zero,"Context. The main goal is to find whether or not a subspace of $\mathbb R^5$ of dimension $3$ intersects a rational subspace of dimension $2$ . By rational subspace, we mean a subspace of $\mathbb R^5$ which admits a rational basis ( i.e. a basis formed with vectors with rational entries). This is what motivates this question. The question. Let $Y_1,Y_2,Y_3$ be three vectors of $\mathbb R^5$ such that all the coordinates of the $Y_i$ are in $$\mathbb Q(\sqrt 2,\sqrt 3,\sqrt 6),$$ i.e. the coordinates of the $Y_i$ are of the form $$a+b\sqrt 2+c\sqrt 3+d\sqrt 6,\qquad a,b,c,d\in\mathbb Q.$$ Let $X_1,X_2\in\mathbb Q^5$ be two vectors with rational entries such that $(X_1,X_2)$ is free over $\mathbb R$ . Does there exist such $(Y_1,Y_2,Y_3)$ such that for all such $(X_1,X_2)$ , the matrix $M\in\mathrm M_5(\mathbb R)$ with columns $Y_1,Y_2,Y_3,X_1,X_2$ , i.e. $$M:=(Y_1\vert Y_2\vert Y_3\vert X_1\vert X_2),$$ satisfies $$\det M\ne 0\quad ?$$ Remarks. I have tried many choices of vectors $Y_1,Y_2,Y_3$ , but it always result in a system of four rational equations that I can not solve. The goal would be to show that the system has no rational solution. Any ideas or references which would be related to this matter would be of great help.","['determinant', 'vector-spaces', 'diophantine-equations', 'linear-algebra', 'rational-numbers']"
3066037,Let A be a denumerable subset of an uncountable set X. Prove that X/A is uncountable,"I'd like to know if my proof is correct: Assume $A$ is denumerable and and $A\subseteq X$ Assume X is uncountable
(X uncountable means X is not finite and X is not denumerable) $X=(X\backslash A) \cup (X \cap A)=(X\backslash A)\cup A$ (as $A \subseteq X$ ) Therefore $X$ is uncountable $\implies (X\backslash A) \cup (X \cap A)$ uncountable. Now using the Theorem: If A and B are denumerable sets then $A\cup B$ is denumerable.
The contrapositive of this statement is: If $A\cup B$ is not denumerable then A is not denumerable or B is not denumerable. Now applying the contrapositive to what we have so far $(X\backslash A) \cup A)$ uncountable(not denumerable) $\implies(X\backslash A)$ uncountable or $A$ is uncountable. (by the contrapositive of the theorem) By assumption A is denumerable and therefore countable hence X\A is uncountable as required.","['elementary-set-theory', 'proof-verification']"
3066059,Compact subset of space of Continuous functions,"Let $\mathscr{C}[0,1]$ denote the set of continuous functions with bounded supremum and let $K=\{f\in\mathscr{C}[0,1]|\int_0^1f(t)dt=1\}$ . Then is $K$ compact in the space $\mathscr{C}[0,1]$ ? Typically how do we characterize the compact spaces in the space of continuous functions? Will Heine-Borel property work here? I think Heine-Borel would work, as $[0,1]$ is a compact Hausdorff space. Then, by using a function similar to spikes, or, somewhat like Dirac-Delta function, I think the space $K$ is not compact. Is my argument true? Any hints? Thanks beforehand.","['general-topology', 'functional-analysis', 'real-analysis']"
3066067,There exists a linear operator with no proper invariant subspaces,"Let $A$ be a bounded operator on a Hilbert space $H$ with two invariant subspaces $M$ and $N$ s.t. $N \subset M$ , dim $(M \cap  N^{\perp})> 1$ , and have no invariant subspaces between $N$ and $M$ . Then, show that, there exists an operator $B$ on $H$ which has no proper invariant subspace. All I want a hint for constructing $B$ with the help of $A$ and given conditions, even a little hint will be appreciated. Thanks in advance.","['linear-algebra', 'functional-analysis', 'operator-algebras']"
3066076,"If two rotation matrices commute, do their infinitesimal generators commute too?","Suppose that $e^A$ and $e^B$ are two rotations in $\mathrm{SO}(n)$ . If $e^{A}e^{B} = e^{B}e^{A}$ , can we conclude that $e^{A+B}=e^Ae^B$ ? More importantly, can we say that $AB=BA$ ? I'm particularly interested in the cases when $n=2,3,4$ because I was working on a physics problem that this question was brought up. $n=3$ is the most important case for me.","['lie-algebras', 'matrix-exponential', 'linear-algebra', 'lie-groups', 'rotations']"
3066079,Expectation of the function of multiple random variables,"I am working through some basic probability stuff and have a question regarding functions of multiple variables. If I have two random variables $X,Y$ which have some joint probability distribution $P_{XY}(x,y)$ I can obtain expected value of some function $f(x,y)$ by integrating across both variables: $$
E[f(x,y)] = \int \int f(x,y) P_{XY}(x,y) \ dx \ dy
$$ we can also obtain the expected value of a function of a single variable by following the workings here $$
E[g(x)] = \int g(x) P_{X}(x) \ dx
$$ If the integration across a function of a single value has meaning I am wondering how we would interpret the integration of a function of multiple variables across a single value $$
\int f(x,y) P_X(x)dx
$$ and wether this expression has any meaning?","['probability-theory', 'probability', 'random-variables']"
3066123,Can Bessel functions be represented as a single function with two variables?,"The typical way to represent a Bessel function of first kind is $ J_{\alpha}(z)$ , i.e. $ J_{\alpha}: \mathbb{C}\to \mathbb{C}$ . Is there any good reason that prevents us to write it as a function of two variables $J(\alpha,z)$ , i.e. $ J: \mathbb{C}^2\to \mathbb{C}$ ?","['complex-analysis', 'ordinary-differential-equations', 'bessel-functions']"
3066169,Why needed Open disk in domain?,"Why is in above theorem, it is assumed that $D$ an open disk? Is it to make sure that we can surely apply mean value Theorem, ie, walking parallel to x or y axis, we are not moving out of domain? Can a convex open set do that job? Also, this may sound silly, but why open disk? Is it so that derivative is defined without fuss, that is, without considering boundary of that disk??","['complex-analysis', 'holomorphic-functions', 'derivatives']"
3066180,Actual computational complexity of solving a linear system accounting for numerical accuracy (digit),"Solving a system of linear equations is solving for $n \times 1$ vector $x$ out of $Ax = b$ , where $A$ is $n \times n$ matrix. Suppose that $A$ 's entries have $k$ digits at maximum, in binary or decimal. Similarly, entries of $b$ also has $k$ digits at maximum. It is said that computational complexity of solving lienar equation is $O(n^3)$ , but this does not account for numerical issues - such as actually turning out $x$ to be accurate up to $y$ digits. What would be actual computational complexity accounting for $k$ and $y$ ?","['numerical-linear-algebra', 'computational-complexity', 'linear-algebra']"
3066201,Computing the product $xy$,"$$\dfrac{|x|}{2}=y$$ $$|y|-x=4$$ Compute $xy$ Here I would have been able to solve this question in such case there was only one unknown. I, however, cannot solve this type of questions when there are more than one unknown so let me at least share my thoughts with you. $$|x| = 2y \implies  \dfrac{1}{2}|x| = y $$ Plugging into the second equation and we get that $$\biggr |\dfrac{1}{2}|x|\biggr |-x = 4 $$ This is where I'm stuck. Regards","['algebra-precalculus', 'systems-of-equations', 'absolute-value']"
3066203,A convergent sequence has precisely one accumulation point,"As a step in a proof I've been trying to show that convergence of a sequence implies it must have precisely one accumulation point. This is the definition we use for an accumulation point Let $S$ be a set of real numbers. A real number is an accumulation point $s_0$ of $S$ if and only if for any $\epsilon > 0$ , there exists at least one point $t$ of $S$ such that $0 < |t-s_0| < \epsilon$ . I wish to prove: Lemma: A convergent sequence has precisely one accumulation point. My thoughts: Since the limit of the sequence exists we know that the set cannot have 2 accumulation points or more, we will show by contradiction. If we would have multiple accumulation points the limit does not exist because we can never get arbitrarily close to a single point (within Ïµ), because there exist certain subsequences that each get arbitrarily close to at least two accumulation points, which determine the minimum distance a sequence can be from an accumulation point (so we get a lower bound and therefore we do not get convergence). There must be precisely one accumulation point. I do not know how to make this more precise of a statement, there are a lot of words and it's not really structured. I'm looking for some help making a more rigourous argument.","['limits', 'sequences-and-series', 'real-analysis']"
