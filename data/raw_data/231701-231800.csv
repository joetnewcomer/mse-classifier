question_id,title,body,tags
4817809,Show that $L^p$ convergence is unique almost surely,"It'd be of great help if someone could double-check if my proof is correct/rigorous. Problem: For any $p>0$ , if $X_n\to X$ and $X_n\to Y$ in $L^p$ , then $X=Y$ almost surely My proof: We start with the case of $p\geq 1$ . We have that $\lVert X_n -X \rVert_p\to 0$ , therefore by Markov's inequality: $$\mathbb{P}(\lvert X_n -X\rvert>\epsilon)\leq \frac{\mathbb{E}\lvert X_n-X\rvert}{\epsilon}\to 0$$ as $n$ goes to $\infty$ . Same thing with $Y$ . So then we have $$\mathbb{P}(\lvert X-Y\rvert >\epsilon)\leq \frac{\mathbb{E}(\lvert X-Y\rvert)}{\epsilon}= \frac{\mathbb{E}(\lvert X-X_n + X_n -Y\rvert)}{\epsilon}\leq \frac{\mathbb{E}(\lvert X-X_n\rvert + \lvert X_n -Y\rvert)}{\epsilon}=\frac{\mathbb{E}(\lvert X-X_n\rvert) + \mathbb{E}(\lvert X_n -Y\rvert)}{\epsilon}$$ which converges to $0$ as $n$ goes to infinity. Therefore, we have that $\mathbb{P}(X\neq Y)=0$ and we are done. As for $0<p<1$ , a similar (or rather generalized) argument can be made through showing that $L_p$ convergence yields convergence in probability.","['measure-theory', 'expected-value', 'convergence-divergence', 'probability-theory', 'probability']"
4817838,"Is it possible to ""sort"" a continuous function?","I was motivated for this question while seeking for a new sorting algorithm. Suppose a continuous function $f : [a, b] \to \mathbb{R}$ is given. I wanted to define the sorted version $g$ of $f$ , which shall satisfy the following properties: $g$ monotone increases $g(a)$ is the global minimum of $f$ $g(b)$ is the global maximum of $f$ And I eventually came up with the following equation: $$
\int_a^x g = \int_{\{y : f(y) \leq g(x)\}} f
$$ And I tried to construct $g$ from $f$ explicitly. Simple argument: Since $\{y : f(y) \leq g(x)\}$ is a closed subset of the compact set $[a, b]$ , it is the union of some finitely many closed intervals (and possibly some isolated points), say $[a_1(x) = a, b_1(x)], [a_1(x), b_1(x)], \cdots, [a_n(x), b_n(x) = b]$ . That gives the following equation: $$
\int_a^x g = \sum_{k=1}^n\int_{a_k(x)}^{b_k(x)} f
$$ By taking the derivative of both sides of the equation above, I get this: $$
g(x) = \sum_{k=1}^n (f(b_k(x)) \cdot b_k'(x) - f(a_k(x)) \cdot a_k'(x))
$$ By definition of $a_k$ and $b_k$ , $f \circ a_k = f \circ b_k = g$ , and thus: $$
g(x) = \sum_{k=1}^n (g(x) \cdot b_k'(x) - g(x) \cdot a_k'(x)) = \sum_{k=1}^n g(x) (b_k'(x) - a_k'(x)) = g(x) \sum_{k=1}^n (b_k'(x) - a_k'(x))
$$ ...which doesn't give any information of $g$ . Does this mean this equation is under-determined? Does it even make sense to ""sort"" a function?","['calculus', 'sorting', 'real-analysis']"
4817850,Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$.,"Let $f$ be a real-valued function continuous on $[a,b]$ and differentiable on $(a,b)$. Suppose that $\lim_{x\rightarrow a}f'(x)$ exists. Then, prove that $f$ is differentiable at $a$ and $f'(a)=\lim_{x\rightarrow a}f'(x)$. It seems like an easy example, but a little bit tricky. I'm not sure which theorems should be used in here. ============================================================== Using @David Mitra's advice and @Pete L. Clark's notes I tried to solve this proof.
I want to know my proof is correct or not. By MVT, for $h>0$ and $c_h \in (a,a+h)$
$$\frac{f(a+h)-f(a)}{h}=f'(c_h)$$ and $\lim_{h \rightarrow 0^+}c_h=a$. Then $$\lim_{h \rightarrow 0^+}\frac{f(a+h)-f(a)}{h}=\lim_{h \rightarrow 0^+}f'(c_h)=\lim_{h \rightarrow 0^+}f'(a)$$ But that's enough? I think I should show something more, but don't know what it is.","['limits', 'derivatives', 'continuity', 'real-analysis']"
4817857,How to apply queuing theory to find the long run proportion of customers who leave the system?,"I am trying to apply queuing theory / birth and death process to the following. Suppose customers arrive in a restaurant according to a Poisson process with rate $\lambda = 1$ . Suppose there are $2$ counters in the restaurant. Customers are served in counter $1$ in Exponential $(1)$ amount of time, and customers are served in counter $2$ in Exponential $(2)$ amount of time. Usage of counters by each customer is independent of the usage of counters by other customers.  When a customer comes, s/he chooses counter $2$ if it's free/available, otherwise the customer goes to counter $1$ . If both counters are occupied, the customer leaves. Here, I am trying to calculate the long run proportion of customers that leave because both counters are occupied. I have been trying to set up the balance equations but I am not sure that's the correct approach.  I'm preparing for the upcoming finals and I'd be grateful for any help on how to solve this.  Thank you so much. $EDIT:$ I tried to model this as an $M/M/2$ queue, and I get the following results: I take the state space to be $S = (0, 1, 2),$ where each state refers to the number of customers in the system. The balance equations are $\lambda P_0 = (\mu_1 + \mu_2)P_1$ $\lambda P_1 = (\mu_1 + \mu_2)P_2$ And the normalizing equation is: $P_0 + P_1 + P_2 = 1$ Solving this gives me $P_0 = \frac{9}{13}, P_1 = \frac{3}{13},$ and $P_2 = \frac{1}{13}$ Is this correct?  I am doubtful about the logic I have used to arrive at the balance equations, especially because of the fact that customers prefer counter $2$ over counter $1$ . If this is correct, then, I have $P_2$ , which is the long run probability that a customer leaves because both counters are occupied.  With this, how can I find the long run proportion of customers that leave because both counters are occupied?  Thank you for any help.","['stochastic-processes', 'exponential-distribution', 'queueing-theory', 'probability', 'random-variables']"
4817894,"Given a representation of $C_0(Y)$ on $C_0(X)$, trying to find a continuous function that satisfies a certain condition.","Let $X$ and $Y$ be locally compact Hausdorff, consider an algebra homomorphism $\phi:C_0(Y) \rightarrow C_b(X)$ . We have a representation, which is an algebra homomorphism, $\pi$ of $C_0(Y)$ on $C_0(X)$ defined by $$\pi(g)(f) := \phi(g) \cdot f$$ Assume that this representation is non degenerate i.e. the linear span $\text{span}\{\pi(g)(f):g \in C_0(Y), f \in C_0(X)\}$ is dense in $C_0(X)$ . I am trying to find a continuous function $F:X \rightarrow Y$ such that $\phi(g) = g \circ F$ . For the reverse, if $F$ is continuous and define $\phi(g) := g \circ F$ , then this representation $\pi$ is non degenerate using Stone-Weierstrass theorem. However I am having trouble showing the reverse.","['banach-spaces', 'general-topology', 'linear-algebra', 'analysis']"
4817896,"Prove that if $f: A \to B$ is an injective function and $h: A \to C$ is any function, then there always exists a function $g: B \to C$ with $h = g∘f$.","I'm not an incredibly well versed proof writer and I would like some help with this proof. I really want to know if I'm along the right track or if I need to be going in a different direction. The question does not come from our textbook, but we use A Concise Introduction to Pure Mathematics By: Martin Liebeck. I found a similar question answered here but, I do not belive the proof is exactly the same. (let me know if this question is answered already and I'll delete the post.) Here is the question and my proof. Prove that if $f: A \to B$ is an injective function and $h: A \to C$ is any function, then there always exists a function $g: B \to C$ with $h = g\circ f$ . Assume for contraduction that $f$ is not an injective function, so $a_{1} \neq a_{2}$ but $f(a_{1}) = f(a_{2})$ . Thus $g\circ f = g(f(x))$ and $g(f(a_{1})) = g(f(a_{2}))$ , wich cannot happen in a well defined function. Thus a contradiction. Therefore $f$ must be injective for $g$ to exist and be well defined. Again I really just want to know if I'm going in the right direction and if not point me the right way please. From what I understand it seems like I might be skipping a few steps on the proof.","['proof-writing', 'solution-verification', 'discrete-mathematics']"
4817901,Depiction of convergence in categorical sense,"It is well-known that for a product space $\Pi_{\alpha \in A} X_{\alpha}$ , a sequence of points $\{\mathbf{x}_n\}$ converges if and only if for each $\alpha \in A$ , $\{\pi_{\alpha}(\mathbf{x}_n)\}$ converges in $X_{\alpha}$ . The traditional proof relies heavily on arguments of neighborhoods, and it is in some sense long and laborious. My question is that, since the product space is a product in categorical sense, would a depiction of convergence in categorical sense neatly prove the assertion?","['general-topology', 'category-theory']"
4817906,Exercise 2.5.8 Pedersen's Analysis Now,"I'm trying to do the following problem, and I'm getting hung up on one part.  Here is the problem: Let $f: X \longrightarrow \mathfrak{X}$ be a continuous map from a compact Hausdorff space $X$ into a Banach space $\mathfrak{X}$ , with $\mu$ a Radon measure on $X$ .  Consider elements of the form: $$I_{\lambda}(f) = \sum_{k=1}^n f(s_k)\mu(E_k)$$ where the $E_k$ 's are Borel sets partitioning $X$ and $s_k \in E_k \subset \{s \in X \: | \: \lvert f(s)-f(s_k) \rVert \leq \epsilon \}$ for $\epsilon > 0$ .  With $\lambda = \{E_1,\dots,E_n,\epsilon\}$ , prove that $(I_{\lambda}(f))_{\lambda \in \Lambda}$ is a convergent net.  We denote the limit by: $$\int_{X} f(s) \: d\mu(s).$$ My main issue I'm having is what the ordering needs to be on the set $\Lambda$ .  I figured at first that I would want something like $\lambda \leq \mu$ if and only if $\mu$ contains a refinement of the partition in $\lambda$ , and $\epsilon_{\mu} \leq \epsilon_{\lambda}$ .  However, this doesn't seem to work, as the smaller the $\epsilon$ I stipulate, this seems to directly affect how many $E_k$ 's I would need in my partition since $E_k \subset \{s \in X \: | \: \lVert f(s)-f(s_k) \rVert \leq \epsilon\}$ .  Hence if $\epsilon > 0$ was really small, I should expect there to be many more $E_k$ 's as $\mu(\{s \in X \: | \: \lVert f(s)-f(s_k) \rVert \leq \epsilon\}) \longrightarrow 0$ as $\epsilon \rightarrow 0^{+}$ . Thus, my point is that I shouldn't be able to just change $\epsilon$ independently of changing the $E_1,\dots,E_n$ .  But this goes against what my ordering is supposedly okay with. Is there another ordering I'm supposed to be using?","['integration', 'banach-spaces', 'functional-analysis', 'analysis']"
4817907,"Notation used within ""Prime ideal structure in commutative rings"" by M. Hochster","I was reading through this paper and I came across some rather confusing notation in the proof of Theorem 4. It says Proof. ... $Y$ is finite, and it is clear that the image of the restriction of any element of $[b_1,b_2]$ to $d(b_1)\cup d(b_2)$ is the same as the image of its restriction to $Y$ . Thus, we need only find $c\in I=(b_1,b_2)\cap[b_1,b_2]$ such that $c$ does not vanish on $Y$ ... My question is what is the $[b_1,b_2]$ notation? My familiarity with its notation is as the Lie bracket, or perhaps the commutator. But neither of which make sense here since we want $I$ to be an ideal. Online searches have also yielded no help. Any help is appreciated!","['notation', 'ring-theory', 'algebraic-geometry']"
4817913,Number of natural numbers with an odd sum of number of divisors till that number,"Find the number of natural numbers $n<2023$ such that $\sum_{i=1}^n \sigma_0(i)$ is odd, where $\sigma_0(x)$ denotes the number of divisors of $x$ . First, I know that $\sigma_0(x) = (n_1 +1)(n_2 +1)(n_3+1)\dots$ where $x = p_1^{n_1}\cdot p_2^{n_2} \cdot p_3^{n_3}\dots$ with all $p$ being primes. Hence, it can be observed that $\sigma_0(x)$ is odd if $x$ is a perfect square. Also, $\sum_{i=1}^n \sigma_0(i)$ is odd if there are an odd number of odd terms in $\sigma_0(1)+\sigma_0(2) + \dots + \sigma_0(n)$ . In other terms, that means there must be an odd number of perfect squares from $1$ till $n$ . So, the values of possible $n$ that satisfy the given constraint are: $1,2,3,9,10,11,12,13,14,15,25,26,\dots$ Now I am having some difficulty in counting this till $2022$ . We need to remove all numbers between even squares and the next square, and including the even square to find the number of values till $2022$ . I know that $(n+1)^2-n^2=2n+1$ but I always get errors in my count. Help is appreciated.","['elementary-number-theory', 'algebra-precalculus']"
4817926,Understanding an inequality in the proof,"I believe my question is on some very elementary computations (i.e., this is probably answerable even if you don't know what is the subgaussian norm $\| \cdot \|_{\psi_2}$ etc. However, do let me know if you want clarifications on the definitions. I will update the post if needed). Nevertheless, I will post the statement and the proof that I am trying to understand here for context. This is the statement of the proof: This is the proof that I am trying to understand (I will state my specific question after this picture): In particular , I am confused on how to check the very last inequality $\| X_{uv} - X_{wz} \|_{\psi_2} \leq C \| Y_{uv} - Y_{wz} \|_2$ for some constant $C$ large enough. Attempts : Through computation (squaring both sides of the $\| X_{uv} - X_{wz} \|_{\psi_2}$ inequality and expand the quadratic terms on the right hand side), I noticed that it suffices to show $$
\| u - w \|_2 \| v - z \|_2 \operatorname{rad}(S)\operatorname{rad}(T) \leq C \| u - w \|_2^2 \operatorname{rad}(S)^2 + \| v - z \|_2^2 \operatorname{rad}(T)^2 = C\| Y_{uv} - Y_{wz} \|_2^2
$$ for some $C$ large uniformly for all $u,w,v,z$ . However, I do not see why this would be true. Perhaps we can do something with the boundedness of the set $S$ and $T$ ? I am not entirely sure how to proceed from here.","['proof-explanation', 'a.m.-g.m.-inequality', 'real-analysis', 'inequality', 'probability-theory']"
4817973,Can a pointwise product of $\mathbb{R}\to\mathbb{R}$ bijections be also a bijection?,"It's easy to see that if $f,g:\mathbb{R}\to\mathbb{R}$ are continuous bijections, then their pointwise product $fg$, defined by $x\mapsto f(x)g(x)$ cannot be a bijection. My question : does it exist a pair $(f,g)$ of bijections from $\mathbb{R}$ to $\mathbb{R}$ such that $fg$ is also a bijection ?","['elementary-set-theory', 'functions']"
4817993,Lang exercise 50 on Witt vectors,"I'm reading the construction of the Witt ring from Lang's algebra. This is a series of exercises in chap. VI. In exercise 50 he says: If $x\in W_n(k)$ show that there exists $\xi\in W_n(\bar{k})$ such that $\mathfrak{p}(\xi)=x$ . Do this inductively solving first for the first component and then showing that a vector $(0,a_1,...,a_{n-1})$ is in the image of $\mathfrak{p}$ if and only if $(a_1,...,a_{n-1})$ is in the image of $\mathfrak{p}.$ $\mathfrak{p}$ denotes the Artin-Schreier operator on $W_n(A)$ : $\mathfrak{p}(a)=a^p-a$ .
I don't see how his hint gives the solution for a general $x$ . I see that it gives the solution only for $x=(0,...,0,x_0)$ .","['number-theory', 'field-theory', 'galois-theory', 'local-field', 'class-field-theory']"
4818001,Proof a sequence is convergent,"Given 2 sequences $\{x_n\}$ , $\{y_n\}$ such that: $${y_n^{2} \leq \frac{1}{n} + {x_n}{y_n} \sqrt[3]{x_n}}$$ with ${\forall} n \in$ $	\mathbb{N}$ . Suppose that ${x_n} \to 0$ . Prove that $\{y_n\}$ converges and calculate ${\lim_{n\to\infty} {y_n} }$ I tried to do something like this: $${y_n^{2} \leq \frac{1}{n} + {x_n}{y_n} \sqrt[3]{x_n}}$$ $$\equiv {y_n}({y_n} - x_n^{\frac{4}{3}}) \leq {\frac{1}{n}}$$ Because ${x_n} \to 0$ , so that $x_n^{\frac{4}{3}} \to 0$ , so that the left side of the inequality ${y_n}({y_n} - x_n^{\frac{4}{3}}) \to {y_n^2}$ . And, because ${y_n^2} \leq \frac{1}{n}$ so that $y_n$ is bounded and exist ${\lim_{n\to\infty} {y_n} }$ . Is that a good way to solve the first part of problem ? And how to calculate the limit ? P/s: Sorry because my problem with English and thanks for helping !!","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
4818006,Solving logistic regression equation for slope,"I've calculated a logistic regression model involving two variables $X_1$ and $X_2$ and their interaction $X_1 \times X_2$ and obtained regression coefficients for each. The equation takes following linear form: $$
\text{logit }p(X_1,X_2) = \beta_0 + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + \beta_{\text{interaction}} \cdot X_1 \cdot X_2
$$ If got the mathematics correct this should correspond to the non-linear form: $$p(X_1,X_2) = \frac{1}{1 + \exp\left(-\left(\beta_0 + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + \beta_{\text{interaction}} \cdot X_1 \cdot X_2\right)\right)}$$ The regression coeffiecients I obtained are the following: $\beta_0$ = 0.31, $\beta_1$ = -5.63, $\beta_2$ = -0.02, $\beta_\text{interaction}$ = 0.45; Based on the parameters I can plot the probability against a range of value for $X_1$ (in my case this is named DeltaT ) and across values of $X_2$ (in the plot as single panels and called Tamb ) which looks like: From the plot I can see that there is an point of inflection at appr. $X_2$ = 12.5, i.e. where the slope of the line changes from negative to positive (or vice versa). Now, I'd like the get the exact point when this happens. In other words at which value of $X_2$ does the slope change (at $X_1$ = 0). From manual simulations and numerically obtaining the point where the slope changes, it seems this is at $\frac{-\beta_1}{\beta_\text{interaction}} = \frac{-1*(-5.63)}{0.45} = 12.51 $ .
[As a note why I am confused: I'd have set $X_1$ in the first linear equation to 0 and solved for $X_2$ which would then have been $X_2 = \frac{-\beta_0}{\beta_2}$ ?, but that's not how it works.] I guess obtaining the slope (and where it changes) can be done by calculating the first derivative of the equation with respect to $X_2$ . Setting the derivative equal to zero should obtain the point where the slope becomes zero? However, I don't get the mathematical steps behind that, as in the end this seems to be solved by just two regression coefficients(?).","['derivatives', 'slope', 'logistic-regression']"
4818019,Expected winning score in a round robin tournament,"Suppose you have a tournament consisting of 100 players in which each player plays each other player exactly once. In any given match, the chance of either playing winning is equally likely, and there are no ties. Winning a match gives $1$ point, and losing a match gives no points. After all matches have been played, we rank the players according to their score (from highest to lowest), with ties broken at random. For each $n=1,2,\dots, 100$ , let $S_n$ be the score of the player ranked $n$ out of $100$ . Question: What is the expected value of $S_1$ ? If it's difficult to find this analytically, how can we approximate the value? The same question holds for other variables, e.g. $S_{10}$ , $S_{50}$ . Each individual player's score follows a binomial distribution, but these aren't independent: for instance, the total number of points is fixed. One can bound $S_n$ through a bit of combinatorics: e.g., given that there are $4950$ points in total, we must have $S_1 \geq 50$ , and we can construct a scenario in which $S_1 = 99$ (which is maximal), so perhaps $S_1 \approx 75$ , the midpoint? If I'm looking at $S_{10}$ instead, similar considerations give what I think is $45 \leq S_{10} \leq 94$ . But I'm not sure how to evaluate $\mathbb E(S_{10})$ without further considerations.","['expected-value', 'combinatorics', 'probability']"
4818076,Expected number of strikes to kill a $3$-headed dragon,"You want to slay a dragon with $3$ heads. There is $0.7$ chance of destroying a head and $0.3$ chance of missing. If you miss, a new head will grow. $X$ is a random variable for the number of rounds until you slay all $3$ heads. Find $E[X]$ . I get the following pmf that $P(X = n) = {?}\ 0.7^{k} 0.3^{k-3}$ where $n$ is the number of slays ( $3$ , $5$ , $7$ ...) and $k$ is the number of strikes that destroy a head. I am struggling with coming up with a coefficient for the expression or number of ways to permute successes and failures. I understand that the missing strikes cannot be at the end, and there also cannot be more than $2$ successful strikes before the 1st miss. How to think of a expression to capture the coefficient?","['random-walk', 'expected-value', 'stochastic-processes', 'stopping-times', 'probability']"
4818097,Residue involving Lambert function: $\underset{z=\frac{2}{\pi}W(\frac{\pi}{2})}{\text{Res}}\dfrac{1}{(e^{-\pi x}-x^2)^n}$,"Context I was tring to find a way to evaluate the infinite tetration of $i$ defined as: $${}^{\infty}i:=i^{i^{i^{.^{.^{.}}}}}$$ 1° attempt Considering $z=i^{i^{i^{.^{.^{.}}}}}$ we can notice that $z=i^z$ , solving this equation we have that: $$z=\dfrac{2i}{\pi}W\left(-\dfrac{\pi i }{2}\right)$$ Where $W(x)$ is the Lambert function. 2° attempt I tried to solve it by separating the absolute value and the argument: $z=\rho e^{i\theta}=x+iy$ $$x+iy=i^{x+iy}\\
\rho e^{i\theta}=e^{\frac{\pi}{2} i(x+iy)}\\
\rho e^{i\theta}=e^{\frac{\pi}{2} ix-\frac{\pi}{2} y}\\
\rho=e^{-\frac{\pi}{2} y}\qquad \theta=\dfrac{\pi}{2} x
$$ I tried to graphically look at their representation considering $\rho=\sqrt{x^2+y^2}$ and $\theta=\arctan\left(\dfrac{y}{x}\right)$ Now I have this system: $$\begin{cases}\sqrt{x^2+y^2}=e^{-\frac{\pi}{2} y}\\\arctan\left(\dfrac{y}{x}\right)=\dfrac{\pi}{2} x\end{cases}\quad\Rightarrow\quad\begin{cases}x^2+y^2=e^{-\pi y}\\y=\tan\left(\dfrac{\pi x}{2}\right)x\end{cases}$$ I tried to calculate the inverse function that appears in the first equation: $x=\sqrt{e^{-\pi y}-y^2}$ and after several steps I found that I had to calculate the following residue: $$\underset{x=\frac{2}{\pi}W\left(\frac{\pi}{2}\right)}{\text{Res}}\dfrac{1}{\left(e^{-\pi x}-x^2\right)^n}\qquad\text{for }n\in\mathbb{N}^{+}$$ Aspect of interest Is that both functions are:
In addition to both being even functions, it turns out that although these two functions are not periodic, they intersect infinitely with tangents that are always perpendicular: Question How can I calculate the closed form of this residue? $$\color{blue}{w_n:=\underset{x=\frac{2}{\pi}W\left(\frac{\pi}{2}\right)}{\text{Res}}\dfrac{1}{\left(e^{-\pi x}-x^2\right)^n}\qquad\text{for }n\in\mathbb{N}^{+}}$$ Where $W(z)$ is the Lambert function. Considerig $\color{red}{k:=W\left(\frac{\pi}{2}\right)}$ for brevity, the first cases are: $w_1=-\left(\dfrac{\pi}{k(k+1)}\right)^1\cdot\dfrac{1}{4}$ $w_2=+\left(\dfrac{\pi}{k(k+1)}\right)^3\cdot\dfrac{2k^2-1}{32}$ $w_3=-\left(\dfrac{\pi}{k(k+1)}\right)^5\cdot\dfrac{8k^4-4k^3-12k^2+3}{512}$ $w_4=+\left(\dfrac{\pi}{k(k+1)}\right)^7\cdot\dfrac{48k^6-64k^5-132k^4+40k^3+90k^2-15}{12288}$ $w_5=-\left(\dfrac{\pi}{k(k+1)}\right)^9\cdot\dfrac{384k^8-928k^7-1448k^6+1424k^5+1980k^4-420k^2-840k^2+105}{393216}$ So in general I think the general solution has this form: $$\color{blue}{w_n=(-1)^n\left(\dfrac{\pi}{k(k+1)}\right)^{2n-1}\cdot\dfrac{P_{n}(k)}{2^{3n-1}\cdot(n-1)!}}$$ Where $P_{n}(x)$ is a degree $(2n-2)$ polynomial with the following properties: $P_n(-1)=(2n-3)!!\qquad$ ( $x!!$ is the double factorial) The term of degree $0$ is $(-1)^{n-1}\cdot (2n-3)!!$ The term of degree $1$ is zero. The term of degree $n$ is $(2n-2)!!=2^{n-1}(n-1)!$ The first polynomials I was able to compute are: $P_{1}(x)=1$ $P_{2}(x)=2x^2-1$ $P_{3}(x)=8x^4-4x^3-12x^2+3$ $P_{4}(x)=48x^6-64x^5-132x^4+40x^3+90x^2-15$ $P_{5}(x)=384x^8-928x^7-1448x^6+1424 x^5+1980x^4-420x^3-840x^2+105$ $P_{6}(x)=3840x^{10}-14208x^9-15488 x^8+37424 x^7+36880x^6-26544 x^5-31080x^4+5040x^3+9450x^2-945$ $P_{7}(x)=46080 x^{12}-237312 x^{11}-138624 x^{10}+903424x^9+567408 x^8-1116480x^7-901040x^6+497952 x^5+529200 x^4-69300 x^3-124740x^2+ 10395$ $P_{8}(x)=645120x^{14}-4349952x^{13}-118016x^{12}+21436160 x^{11}+4877280x^{10}-39785600 x^9-20113072x^8+30796704 x^7 + 22251600x^6-9868320x^5-9854460x^4+1081080x^3+1891890 x^2 - 135135$ $P_{9}(x)=10321920 x^{16}- 87330816 x^{15}+ 50932224 x^{14}+ 512583680 x^{13}- 135690880 x^{12}- 1302740160 x^{11}- 221455936 x^{10}+ 1534016384 x^9+ 699496560 x^8- 845079840 x^7 - 569451960 x^6+ 209729520 x^5+200540340 x^4- 18918900 x^3- 32432400 x^2+2027025$ Etc... I put the coefficients from $P_1(x)$ to $P_{13}(x)$ in this triangular pattern to better visualize the values: $$\begin{array}{rrrrrrrrrrrrcllllllllllll}
&&&&&&&&&&&&_1\\ \hline
&&&&&&&&&&&_2&_0&_{\color{red}{-1}}\\ \hline
&&&&&&&&&&_8&_{\color{red}{-4}}&_{\color{red}{-12}}&_0&_3\\ \hline
&&&&&&&&&_{48}&_{\color{red}{-64}}&_{\color{red}{-132}}&_{40}&_{90}&_0&_{\color{red}{-15}}\\ \hline
&&&&&&&&_{384}&_{\color{red}{-928}}&_{\color{red}{-1448}}&_{1424}&_{1980}&_{\color{red}{-420}}&_{\color{red}{-840}}&_0&_{105}\\ \hline
&&&&&&&_{3840}&_{\color{red}{-14208}}&_{\color{red}{-15488}}&_{37424}&_{36880}&_{\color{red}{-26544}}&_{\color{red}{-31080}}&_{5040}&_{9450}&_0&_{\color{red}{-945}}\\ \hline
&&&&&&_{46080}&_{\color{red}{-237312}}&_{\color{red}{-138624}}&_{903424}&_{567408}&_{\color{red}{-1116480}}&_{\color{red}{-901040}}&_{497952}&_{529200}&_{\color{red}{-69300}}&_{\color{red}{-124740}}&_0&_{10395}\\ \hline
&&&&&_{645120}&_{\color{red}{-4349952}}&_{\color{red}{-118016}}&_{21436160}&_{4877280}&_{\color{red}{-39785600}}&_{\color{red}{-20113072}}&_{30796704}&_{22251600}&_{\color{red}{-9868320}}&_{\color{red}{-9854460}}&_{1081080}&_{1891890}&_0&_{\color{red}{-135135}}\\ \hline
&&&&_{10321920}&_{\color{red}{-87330816}}&_{50932224}&_{512583680}&_{\color{red}{-135690880}}&_{\color{red}{-1302740160}}&_{\color{red}{-221455936}}&_{1534016384}&_{699496560}&_{\color{red}{-845079840}}&_{\color{red}{-569451960}}&_{209729520}&_{200540340}&_{\color{red}{-18918900}}&_{\color{red}{-32432400}}&_0&_{2027025}\\ \hline
&&&_{185794560}&_{\color{red}{-1911767040}}&_{2380548096}&_{12435495936}&_{\color{red}{-11230101760}}&_{\color{red}{-40444523520}}&_{10605697600}&_{67185673856}&_{11246571936}&_{\color{red}{-56339106560}}&_{\color{red}{-24293530800}}&_{23789010960}&_{15281426160}&_{\color{red}{-4797833040}}&_{\color{red}{-4443238800}}&_{367567200}&_{620269650}&_0&_{\color{red}{-34459425}}\\ \hline
&&_{3715891200}&_{\color{red}{-45403176960}}&_{88809394176}&_{304618168320}&_{\color{red}{-542481679360}}&_{\color{red}{-1196513206784}}&_{1056393994880}&_{2677512110336}&_{\color{red}{-555243289280}}&_{\color{red}{-3204955762240}}&_{\color{red}{-568295633664}}&_{2050613136000}&_{855124189920}&_{\color{red}{-696743087040}}&_{\color{red}{-432280648800}}&_{118062584640}&_{106686379800}&_{\color{red}{-7856748900}}&_{\color{red}{-13094581500}}&_0&_{654729075}\\ \hline
&_{81749606400}&_{\color{red}{-1163994071040}}&_{3157404549120}&_{7404607168512}&_{\color{red}{-22961450293248}}&_{\color{red}{-33019002361856}}&_{63005932451840}&_{97343743979776}&_{\color{red}{-75013307410688}}&_{\color{red}{-161351897989888}}&_{23949914036224}&_{147655256436480}&_{27950317851456}&_{\color{red}{-75531748131840}}&_{\color{red}{-30823056744480}}&_{21371092142400}&_{12914289788400}&_{\color{red}{-3117557963520}}&_{\color{red}{-2762956696500}}&_{183324141000}&_{302484832650}&_0&_{\color{red}{-13749310575}}\\ \hline
_{1961990553600}&_{\color{red}{-32062337187840}}&_{112598807740416}& _{171315761872896}&_{\color{red}{-929295126183936}}&_{\color{red}{-785427471704064}}&_{3224048625086464}&_{3108838134571008}&_{\color{red}{-5607501784821504}}&_{\color{red}{-7203781137012736}}&_{4632255496628736}&_{9229708405761024}&_{\color{red}{-892152626613248}}&_{\color{red}{-6733346574864000}}&_{\color{red}{-1350185114678400}}&_{2848465773753600}&_{1145257280687520}&_{\color{red}{-688389482420640}}&_{\color{red}{-407477187117000}}&_{88068917336400}&_{76831147493100}&_{\color{red}{-4638100767300}}&_{\color{red}{-7589619437400}}&_0&_{316234143225}
\end{array}$$ I highlighted the negative numbers with red, at the beginning I thought it changed sign every $2$ but going beyond $P_9(x)$ you see that this is not the case","['lambert-w', 'calculus', 'polynomials', 'residue-calculus', 'limits']"
4818128,Variational Calculus Confusion,"In the variational calculus, we find the Euler-Lagrange equation by using the following: $S[x(t) + \epsilon f(t)] = S[x(t)] + \delta S + O(\epsilon^2)$ (1) $S[x(t) + \epsilon f(t)] = S[x(t)] + \frac{d}{d\epsilon}S[x(t) + \epsilon f(t)]\Bigr|_{\epsilon=0}\epsilon + O(\epsilon^2)$ Now, I understand that in (1), the LHS's value has to be less (over all - including all orders) than the RHS's value because $x(t)$ is what minimizes the action, but I also understand that $\delta S$ must be $0$ . So, even if $\delta S = 0$ , (1) still obeys the principle such that the LHS is less than the RHS because the RHS also has $O(\epsilon^2)$ . So we say actions are equal in first order. Now, if I look at i.e $\delta S = \frac{d}{d\epsilon}S[x(t) + \epsilon f(t)]\Bigr|_{\epsilon=0}\epsilon$ , what this tells me in words is how much action changes between our paths, so since we set this to $0$ , then we're officially saying that action values are the same on both paths, even though, on true path, the value has to be less. What's going on? Can this be explained in layman's terms?","['integration', 'derivatives', 'calculus-of-variations']"
4818129,If $F$ and $f$ are continuous show that $g$ is continuous,"Let $F:[a,b]\times [a,b] \rightarrow \mathbb{R}$ and $f:[a,b] \rightarrow \mathbb{R}$ be two continuous functions, let $g:[a,b] \rightarrow \mathbb{R}$ defined as: $$ g(x) = \int_a^b F(x,y) f(y) dy$$ Show that $g$ is continuous My attempt Since $F$ is continuous and it is defined on a compact subset of $\mathbb{R}^2$ then $F$ is uniformly continuous, so it is easy to see that for each $x,y \in [a,b]$ the functions $F_x, F_y:[a,b] \rightarrow \mathbb{R}$ defined as: $$ F_x(y) = F(x,y)$$ $$F_y(x) = F(x,y)$$ are also uniformly continuous. We can write $g$ as: $$ g(x) = \int_a^b F_y(x) f(y) dy$$ Note that: $$|g(x_1)-g(x_2)| = \left|\int_a^b F_y(x_1) f(y) dy - \int_a^b F_y(x_2) f(y) dy \right| = \left|\int_a^b (F_y(x_1)-F_y(x_2)) f(y) dy \right|$$ Let $\epsilon > 0$ , there exists $\delta >0$ such that if $|x_1-x_2| < \delta$ then $|F_y(x_1)-F_y(x_2)| < \dfrac{\epsilon}{b-a}$ $\Rightarrow \int_a^b |F_y(x_1)-F_y(x_2)|dy < \epsilon$ So we have: $$\left|\int_a^b F_y(x_1)-F_y(x_2) dy \right| \leq \int_a^b |F_y(x_1)-F_y(x_2)| dy < \epsilon$$ But the left side of the inequality is not exactly $|g(x_1)-g(x_2)|$ , so I'm having trouble trying to find an expression to prove the continuity of $g$ . Is there an easy way to prove it?","['continuity', 'multivariable-calculus', 'compactness', 'real-analysis']"
4818152,Can infinite summation be defined without using limits?,"Binary addition on natural numbers is defined using simple statements: $$n + 0 = n$$ $$a + succ(b) = succ(a + b)$$ Binary addition of real numbers may be again defined using simple statements (I won't list them here). Could we come up with a list of similar simple statements that would define summation of infinite sequences of real numbers, in a way that does not, even intutitively, rely on the notion of limit or convergence? These are examples of such ""simple, not-limit-using"" statements (though I'm not claiming they are necessarily good candidates): Some base cases: $sum(0, 0, 0, ...) = 0$ $sum(0.5^1, 0.5^2, 0.5^3, ...) = 1$ Respects binary sum: $sum(n_0, n_1, n_2, ...) = r$ implies $r = n_0 + sum(n_1, n_2, ...)$ $sum(n_1, n_2, ...) = r$ implies $n_0 + r = sum(n_0, n_1, n_2, ...)$ Respects binary multiplication: $sum(n_0, n_1, n_2, ...) = r$ implies $sum(n_0 * c, n_1 * c, n_2 * c, ...) = r * c$ Zigzaging rule: $sum(A) = a$ and $sum(B) = b$ and $zigzag(A,B,C)$ implies $sum(C) = a + b$ Helper definitions: For sequences of reals $A$ , $B$ and $C$ , $zigzag(A,B,C)$ holds iff there exists a sequence of naturals $I$ such that $A = (C_{I_0}, C_{I_1}, ...)$ and $B$ equals $C$ with the elements at indices $I$ removed. I'm afraid that if this question does not have an affirmative answer, it would be very hard to give any, because I have not given precise enough description of what is simple and non-limit-using. So I'll be happy to also accept counter-examples that cannot be proven to equal to their limits using my example statements (or other simple statements of the answerer's choosing, if they wish to add some). I'm also aware none of my examples can be used to prove a particular sum diverges. Perhaps we could define summation as a least fixed point? But feel free to restrict yourself to converging sequences, I guess. This question was motivated by this one , where the asker has trouble accepting the conventional definition of infinite summation, which got me thinking about whether one may avoid using limits for defining it.","['limits', 'sequences-and-series']"
4818160,"Prove that $[0,1) \times [0,1)$ is homeomorphic to $[0,1] \times [0,1)$","I need to prove that: $[0,1) \times [0,1)$ is homeomorphic to $[0,1] \times [0,1)$ I am not confident in writing homeomorphisms, but I picture the situation as: I noticed that the each side of the half open (on the left) and U-shaped (on the right) squares is homeomorphic to another, then the interior of these squares are the same so one can use the identity map in there. However, I could not manage to write a specific homeomorphism $f$ for the mentioned sides.","['general-topology', 'intuition']"
4818163,Can pointwise convergence in random variables guarantee some kind of convergence in a related stopping time?,"Suppose we have a family (indexed by $n$ ) of discrete-time random process $\{X_{t}^n\}, {t\geq0} $ , taking values in $\mathbb{R}$ . For each $n,t$ , $X_{t}^n\geq0$ . And $X_{t}^n$ converges pointwise towards a discrete-time random process $X_{t}$ . Define the stopping times correspondingly, $A$ is a constant: $T_n = \inf[t:X_{t}^n\geq A]$ , $T = \inf[t:X_{t}\geq A]$ . (Sorry I cant type ""{"" for some unknown reason) My ultimate goal is to give a lower bound for $\liminf\limits_{n\to\infty} E\{T_n\}$ , and now I have already had a lower bound for $E\{T\}$ . I am wondering how to relate $E\{T\}$ and $\liminf\limits_{n\to\infty} E\{T_n\}$ ?","['stopping-times', 'probability-theory']"
4818176,Prove that Gradient points to Steepest Ascent,"I’m following a course on Machine Learning at uni at the moment. It is the first mathematics course I’ve followed in a while, so I’ve had some difficulty getting back into the mathematics ‘way of thinking’ if you know what I mean. The exercise I’m having difficulty with at the moment is as follows: Let $f : \mathbb{R}^d \to \mathbb{R}$ be differentiable, show that the largest possible increase of the functions $$ \phi_v(t) = f(x+tv) : v \in \mathbb{R}^d, \|v\|_2 = 1$$ in $t=0$ is $\|\nabla f(x)\|_2$ and is assumed for $v = \nabla f(x) / \|\nabla f(x)\|_2$ . A hint was given of using Cauchy-Schwarz (recalling when this is an equality), but I have no idea how I would even start at solving this problem. Any help at all would be greatly appreciated! Thanks in advance!","['optimization', 'functions', 'vector-analysis', 'real-analysis']"
4818201,Rolle's theorem? [duplicate],"This question already has answers here : Prove that $f$ on $[a,b]$ has only a finite number of zeros. (3 answers) How to show the set is finite (1 answer) Closed 7 months ago . The function $f$ is differentiable in $[0,1]$ and $f$ has infinite
roots in $[0,1]$ . Prove that there exists $c\in [0,1]$ such that $f(c)=f'(c)=0.$ My attempt: Assume $x_1, x_2,...,x_n,...$ are roots of $f(x)=0$ . Since $(x_n)$ is bounded, there exists $({x_n}_k)$ converging to $c\in [0,1]$ . Since $f$ is continuous in $[0,1]$ , we deduce that $f(c)=0$ . Can $f'(c)=0$ ?","['calculus', 'rolles-theorem', 'real-analysis']"
4818235,Does a 3D random walk visit infinitely many primes almost surely?,"It is well known that a 2D random walk visits any square infinitely often almost surely, while a 3D random walk visits the origin finitely often almost surely. However, a 3D random walk does visit the line $L(\mathbb{Z}):=\{(0,0,k):k\in\mathbb{Z}\}$ infinitely often almost surely, as this practically reduces to a 2D random walk visiting the origin infinitely often almost surely. For any subset $S\subset\mathbb{N}$ of positive density, it also feels intuitively clear that the set $L(S):=\{(0,0,n):n\in S\}$ is visited infinitely often almost surely, as any time $L(\mathbb{Z})$ is visited, there is a positive probability that $L(S)$ is visited as well. Since the density of primes converges to $0$ very slowly, an interesting follow up question is thus the following. Question: Is the set $L(\mathbb{P}):=\{(0,0,p):p\in\mathbb{N}\ \mbox{prime}\}$ visited infinitely often almost surely? My guess is yes, but barely, in a way. Because I expect the set $\{(0,0,p,q):p,q\in\mathbb{N}\ \mbox{prime}\}$ is visited finitely often almost surely in a 4D random walk. Estimating the probability a 1D random walk is at the origin after $N$ steps as $N^{-1/2}$ and the probability of being prime after $N$ steps as $1/\log N$ gives either the divergent sum $\sum 1/N\log N$ or the convergent sum $\sum 1/N\log^2N$ . I am wondering if this argument can be made precise and whether there is more literature on those sets which are visited infinitely often almost surely.","['random-walk', 'probability-theory', 'asymptotics']"
4818236,Closed form for the $n$-th derivative of $\csc^2$?,"To find a closed representation for the $n$ -th derivative of $f(t)=\csc^2(t)$ , I started with the Ansatz: $$
f^{(n)}(t) = \frac{p_n(\sin t, \cos t)}{\sin^{2+n}(t)} \tag1
$$ where $p_n$ is a polynomial (with integer coefficients) in two variables.
This leads to the recursion $$
p_{n+1}=xy\frac{\partial p_n}{\partial x}
-x^2\frac{\partial p_n}{\partial y}
-(2+n)\,y\,p_n~;\qquad p_0=1 \tag2
$$ and I am stuck. Likely, someone has solved this before and has a pointer to the $n$ -th derivative of $\csc^2$ ? With ""closed representation"" , I mean something like a finite sum and without recursion. Note: This question is on $\cot$ and only comes up with a recursion. (It's a polynomial in just 1 variable so maybe simpler to turn into an explcit form.) Edit That question leads to Lemma 2.1 in: V.S. Adamchik, On the Hurwitz function for rational arguments , Applied Mathematics and Computation , Volume 187, Issue 1, 1 April 2007, Pages 3–12. It states for the $n$ -th derivative of $\cot z$ , $n>1$ : $$
\frac{d^n}{dz^n} \cot z
= (2i)^n (\cot(z)-i)\sum_{j=1}^n \frac{k!}{2^k} \left\{ {n \atop k} \right\} (i\cot(z)-1)^k \\
$$ $$
\text{where } \left\{ {j \atop k} \right\} \text{ is the Stirling subset}\dots
$$ but after the $\dots$ the text is truncated I cannot make 100% sense of it and how $k$ is computed etc. Maybe someone kows the missing bits.","['derivatives', 'closed-form']"
4818244,Geometry Challenge: Parallelogram in $\triangle$ $ABC$ with circumscribed Circle,"I hope this message finds you in high spirits. I am writing to seek your expertise in solving a captivating geometry problem that I recently encountered in a competitive exam. Despite my best analytical efforts, the solution has proven elusive. Your expertise in this matter would be greatly appreciated, and I look forward to the insights that will undoubtedly enhance my understanding of this geometric challenge. $ \textbf{Problem Description:} $ Consider $\triangle$$ ABC $ , where $ M $ is the midpoint of side $ AC $ , and $ O $ is the center of a small circle with the diameter $BM$ . Let $ BPTQ $ be a parallelogram formed by joining points $ B $ , $ P $ , $ T $ , and $ Q $ . This question explores a geometric relationship within this configuration.
Given the setup in $\triangle$$ ABC $ and parallelogram $ BPTQ $ , we want to find an expression for $ \left(\frac{BT}{BM}\right)^8 $ . My Efforts: I've tried various analytical approaches, but my results have been inconsistent. A more systematic strategy involving geometric properties, algebraic manipulation, or other mathematical tools seems necessary. I'm sincerely thankful for your time and insights. Feel free to ask if you need further details. Appreciate your help greatly!","['euclidean-geometry', 'analytic-geometry', 'circles', 'geometry', 'trigonometry']"
4818296,Most likely configuration of coins flips given their value,"Suppose I have 500 silver coins and 500 gold coins, and all are fair. If a heads on a silver coin gives me 1 dollar, and a heads on a gold coin gives me 3 dollars. Then we know the expected number of dollars received after flipping all 1000 coins is $1\cdot250 + 3\cdot250=1000$ . The question: Suppose I flipped all the coins and got 1100 dollars. What is the most likely configurations of the number of silvers and number of golds? Thoughts: I know there are many possible ways this could happen. The extra \$100 outside of the expected 1000 could come from, say, 100 extra silver heads, or 33 golds and 1 silver. Intuitively, both of these configurations are unlikely because there is only way of scoring 100 extra silvers, e.g. So something in between all silvers and (almost) all golds should be more likely. I could brute force this problem by using the binomial distribution to calculate the probability of any number of heads: $P(k \text{ heads}) = (500 \text{ choose } k)\left(\frac12\right)^{500} $ . So we could find the probability of any configuration ( $j$ silver heads, $k$ gold heads). But I would have to plug in every possible combination that adds up to 100. Note: this was an interview question, and was meant to be solved intuitively rather than with equations.","['statistics', 'puzzle', 'probability']"
4818369,Showing $E\operatorname{Tr}(AAA^TA^T)=n^3 + 2n$,"Suppose $A$ is an $n\times n$ matrix with IID standard Normal entries. What is the easiest way of showing the following? $$E\operatorname{Tr}(AAA^TA^T)=n^3 + 2n$$ What is the value of the following as a function of $n$ ? $$E\operatorname{Tr}(AAAA^TA^TA^T)$$ The first few values appear below: $$
\left(
\begin{array}{ccc}
  & \text{n} & \text{value} \\
  & 1 & 15 \\
  & 2 & 64 \\
  & 3 & 183 \\
  & 4 & 432 \\
  & 5 & 895 \\
\end{array}
\right)
$$ Context: Terry Tao's book under ""2.3.4. The moment method""  has an involved derivation of upper bound for generic symmetric $A$ , I'm curious if it can be made much simpler for Gaussian $A$ . Notebook","['combinatorics', 'probability-theory']"
4818456,Trying to calculate a limit with a finite product and WolframAlpha disagrees with my logic.,"I want to calculate $$\lim_{N \to \infty} \prod_{j=1}^{N} \frac{2j}{N + j + 1} \,.$$ Here is my logic: For the range of the product operator ( $1 \leq j \leq N $ ) the inequality $2j < N + j + 1$ is always true. Therefore $\frac{2j}{N + j + 1} < 1 $ , and so an upper boundary can be estabilished using the largest term of the product: $$\lim_{N \to \infty} \prod_{j=1}^{N} \frac{2j}{N + j + 1} \leq \lim_{N \to \infty}\frac{2N}{2N + 1} = 1 \,.$$ However, when I use WolframAlpha I get $\infty$ for an answer. I have seen other questions that challenge the reliability of WolframAlpha, so I am tempted to dismiss it. My questions: Is my logic correct or is WolframAlpha correct? (and why?) If the limit is indeed finite, does it go to zero or does it simply coneverge to some value smaller than 1? How can I show it? Thank you very much.","['limits', 'wolfram-alpha', 'products']"
4818471,Euler's proof of $\frac{\pi}{6}=1-\frac{1}{2}-\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+\frac{1}{6}-\cdots$,"Euler proved $$\frac{\pi}{6}=1-\frac{1}{2}-\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+\frac{1}{6}-\cdots$$ where the reasoning of the signs thus is prepared, so that of the second may be had as $-$ , prime numbers of the form $4m-1$ have a $-$ sign, prime numbers of the form $4m+1$ a $+$ sign, but composite numbers have that sign which agrees with the account of the multiplication from the primes. He proved it by combining $$\frac{\pi}{4}=\prod_{p\,\text{prime},p\gt 2}\frac{1}{1-(-1)^{\frac{p-1}{2}}p^{-1}}$$ (which is a special case of the Euler product of the Dirichlet beta function)
with $$\frac{1}{(1-az)(1-bz)(1-cz)\cdots}=1+Az+Bz^2+\cdots$$ where $$A=\text{sum of the individual terms},$$ $$B=\text{sum of the two factors at a time},$$ etc. with the same factors not excluded, and set $z=1$ . But, there is one problem with Euler's proof. He is multiplying infinitely many geometric series and collecting like powers – treating it like polynomial multiplication, this surely requires some justification; can we prove that the factors in $$\frac{1}{(1-az)(1-bz)(1-cz)\cdots}$$ (where $a,b,c,\ldots$ are the signed reciprocal primes) can be arbitrarily rearranged? Euler is not very rigorous and I can't figure it out myself – how to save Euler's proof? Added: GH from MO has already provided an answer on MO: https://mathoverflow.net/questions/459675/eulers-proof-of-frac-pi6-1-frac12-frac13-frac14-frac15","['pi', 'sequences-and-series', 'absolute-convergence', 'real-analysis']"
4818515,Finding minimum $\sqrt{a(2b+1)}+\sqrt{b(2c+1)}+\sqrt{c(2a+1)}$?,"Recently, my friend has sent me an excessively hard inequality problem, it's If $a,b,c\ge 0: ab+bc+ca=3$ then find the minimum $$P=\sqrt{a(2b+1)}+\sqrt{b(2c+1)}+\sqrt{c(2a+1)}.$$ I try to test some specific value. $a=b=c=1$ then $P=3\sqrt{3}\approx 5.1962$ $a=b=\sqrt{3}; c=0$ then $P\approx 4.0967:= P_{0}$ Hence, we will prove $$\sqrt{a(2b+1)}+\sqrt{b(2c+1)}+\sqrt{c(2a+1)}\ge P_{0}.$$ After squaring both side and use the hypothesis we obtain $$a+b+c+2\sum_{cyc}\sqrt{ab(2b+1)(2c+1)}\ge P^2_{0}-6.$$ I don't know how to prove the last inequality. Could you help me make it clear two question? Is $P_{0}$ good enough to be the desired minimum? Is there some popular ways for the square root cyclic inequality? Thank you for your help. Any ideas and comments are welcome and appreciate. Update $1$ . As Michael Rozenberg pointed out, I checked $$\sqrt{a(2b+1)}+\sqrt{b(2c+1)}+\sqrt{c(2a+1)}\ge 4.$$ Equality occurs iff $a=3;b=1;c=0$ and and its cyclic permutations. Update $2$ . It seems that the following inequality is true. $$\color{black}{\sqrt{ab+ka}+\sqrt{bc+kb}+\sqrt{ca+kc}\ge \sqrt{q+qk}+\sqrt{k}}$$ holds for all $a,b,c\ge 0$ satisfying $q=ab+bc+ca>1; k=\dfrac{1}{q-1}.$ Equality holds at $(a,b,c)=(q,1,0)$ and its cyclic permutations. For $k=2,$ see also here. I hope the proof in link is useful for general cases.","['algebra-precalculus', 'inequality']"
4818516,Surjective local homeomorphisms from X to Y and from Y to X; are X and Y homeomorphic?,"Problem Let $(X, \mathcal{T}_X)$ and $(Y, \mathcal{T}_Y)$ be topological spaces, and $f : X \to Y$ and $g : Y \to X$ be surjective local homeomorphisms . Is $\mathcal{T}_X$ homeomorphic to $\mathcal{T}_Y$ ? Background For background, see here . Since $f$ and $g$ are surjections, we have $|X| = |Y|$ , so at least there exists a bijection. I wonder whether König's proof of the Schröder-Bernstein theorem could be modified to prove this.",['general-topology']
4818536,Why does the bar construction model the classifying space in both topology and AG?,"For a topological group $G$ , we can construct the classifying space $BG$ as the geometric realization of the nerve of $G$ . I have seen a very similar assertion in the context of algebraic geometry: For a reductive group $G$ , the quotient stack $BG:=[*/G]$ can be represented as the homotopy colimit $$ BG = \mathrm{hocolim} (* \leftleftarrows G \cdots ),$$ and this classifies principal $G$ -bundles on schemes. In this answer , this construction is called the bar construction.  Why does this construction give the classifying space in both the topological and algebraic contexts? Is there a way to view these two situations as essentially the same?","['classifying-spaces', 'principal-bundles', 'algebraic-geometry', 'homotopy-theory', 'simplicial-stuff']"
4818590,Normal approximation for sum of dependent indicators from triangular array,"Suppose there is a triangular array of random variables $X_{k,n}$ with the following properties: $$
\mathbb{P}(X_{k,n} = 1) = p_n, \quad \mathbb{P}(X_{k,n} = 0) = 1 - p_n, 
\quad \lim_{n\to\infty}p_n = p > 0,\\
\mathrm{cov}(X_{i,n}, X_{j,n}) = \begin{cases}
c_n \neq 0, & |i - j| \leq 1, \\
d_n, & |i - j| > 1.
\end{cases}, \quad
\lim_{n\to\infty}\mathrm{cov}(X_{i,n}, X_{j,n}) = \begin{cases}
c \neq 0, & |i - j| \leq 1, \\
0, & |i - j| > 1.
\end{cases}
$$ To be exact, $d_n$ decays as $O(n^{-1})$ . Let's define $S_n = \sum_{k=1}^n X_{k,n}$ , $\mu_n = \mathbb{E}S_n$ and $\sigma_n^2 = \mathrm{var}S_n$ . It's also known that $\mu_n$ and $\sigma_n^2$ are both $O(n)$ . From numerical simulations, I can see that $\frac{S_n - \mu_n}{\sigma_n}$ converges in distribution to $\mathcal{N}(0,1)$ . I'm looking for a reference to some version of Central Limit Theorem for this setup. I'm not sure if given properties are enough to prove the convergence, so having a theorem that requires additional facts about dependence is fine as well.","['central-limit-theorem', 'probability-limit-theorems', 'weak-convergence', 'reference-request', 'probability-theory']"
4818628,"How to integrate $I = \int_{0}^{\infty} \frac{e^{-x}\sin^4(x)}{x^{2}}\,dx$","Question: $$I = \int_{0}^{\infty} \frac{e^{-x}\sin^{4}(x)}{x^{2}}dx$$ My Approach, I tried using feynman technique to get rid of the denominator $$I(a) = \int_{0}^{\infty} \frac{e^{-ax}\sin^{4}(x)}{x^{2}}dx$$ From this we have our boundary condition $I(0) = \frac{\pi}{4}, \hspace{1mm}I(\infty) = 0 \hspace{1.5mm} and \hspace{1.5mm}I'(\infty)=0$ $$I'(a) = \int_{0}^{\infty} \frac{-e^{-ax}\sin^{4}(x)}{x}dx$$ $$I''(a) = \int_{0}^{\infty} e^{-ax}\sin^{4}(x)dx$$ So integrating this we get: $$I''(a) = \frac{24}{a^{5}+20a^{3}+64a}$$ So integrating this twice ad applying boundary conditions we get $$I(a) = \int \int  \left(\frac{24}{a^{5}+20a^{3}+64a}da \right)da = \frac{a}{16}\ln{\left(\frac{a^{6}(a^{2}+16)}{(a^{2}+4)^{4}}\right)} + 8\arctan\left(\frac{a}{4}\right)-16\arctan\left(\frac{a}{2}\right)+\frac{\pi}{4}$$ Since $I(1)$ is are target integral we get $$\int_{0}^{\infty} \frac{e^{-x}\sin^{4}(x)}{x^{2}}dx = \frac{-1}{16}\ln{\left(\frac{5^{4}}{17}\right)} + 8\arctan\left(\frac{1}{4}\right)-16\arctan\left(\frac{1}{2}\right)+\frac{\pi}{4} \approx -4.89841$$ But the actual answer seems to be diffrent, it is $$\int_{0}^{\infty} \frac{e^{-x}\sin^{4}(x)}{x^{2}}dx \approx 0.218965$$ So, My question is where have I gone wrong or is their another way to solve this.","['integration', 'improper-integrals', 'calculus', 'definite-integrals']"
4818640,"Take a random walk on Pascal's triangle, without revisits: Does the final number have infinite expectation?","Let's take a random walk on Pascal's triangle, starting at the top. Each number is in a regular hexagon. At each step, we can move to any adjacent hexagon with equal probability, but we cannot revisit a hexagon. The walk ends when we cannot move. Here is an example in which the final number is $15$ . Does the final number have infinite expectation? ( Paradoxically , a random variable whose possible values are all finite, can have infinite expectation.) My thoughts I suspect the answer is yes, because as we move down Pascal's triangle, the numbers quickly become very large. But I don't know how this could be proved. After some step, if we find ourselves in ""open territory"" (no previously visited hexagons nearby), then there is a sequence of five moves that would end the walk. This is illustrated by this image provided by @Stef in the comments: (The first red line segment after the blue line segment could have been in any other direction, so we don't count that one.) The probability of such a sequence would be $\left(\frac{1}{5}\right)\left(\frac{1}{5}\right)\left(\frac{1}{5}\right)\left(\frac{1}{5}\right)\left(\frac{1}{4}\right)=\frac{1}{2500}$ . Using the expectation of a geometric distribution , the expected number of steps in a walk would be roughly $2500$ . I believe, but am not sure, that the expected row number at the end would be on the order of $\sqrt{2500}=50$ . (This rough analysis ignores several factors, for example the effect of the boundaries of the triangle.) But sometimes our walk would go much deeper than the expected depth. The numbers in the triangle increase very quickly. Generally speaking, if the numbers increase faster than the probabilities of reaching them decrease, then the expectation of the final number could be infinite. Context This is one of several questions about Pascal's triangle that I've asked recently, for example: Does the interior of Pascal's triangle contain three consecutive integers? I tried to kill the central binomial coefficients, but they came back Conjecture: In Pascal's triangle with $n$ rows, the proportion of numbers less than the centre number approaches $e^{-1/\pi}$ as $n\to\infty$ . . Sometimes I am more interested in the methods used to answer the questions, than the answers themselves (but I am still curious what the answer is!). Many of my questions initially seemed almost unapproachable to me, but then turned out to have a rational explanation.","['conjectures', 'random-walk', 'expected-value', 'binomial-coefficients', 'probability']"
4818659,Fisher Information Matrix for Weibull Distribution...,"I wish to find the Fisher Information Matrix for the Weibull Distribution... I face two difficulties, I can't find any sufficient guide in internet to lead me to derive the Fisher Information Matrix... if have, could you share it to me... I not able to ensure that everything I have proved below is valid or not... but the below are my workings.. Given that the pdf of Weibull Distribution is $$f(x;k,\lambda)=\frac{k}{\lambda}\left(\frac{x}{\lambda}\right)^{k-1}e^{-\left(\frac{x}{\lambda}\right)^k} $$ Likelihood function : $$ L(k,\lambda|x_i)=k^n\lambda^{-nk}\,\text{exp}\left[(k-1)\sum_{i=1}^n \text{ln}(x_i)-\sum_{i=1}^n \left(\frac{x_i}{\lambda}\right)^k\right]$$ Log-likelhood function : $$\frac{\delta L}{\delta k}=\frac{n}{\lambda}-n\text{ln}\lambda+\sum_{i=1}^n \text{ln}\left(x_i\right)-\sum_{i=1}^n \left( \frac{x_i}{\lambda}\right)^k$$ $$\frac{\delta L}{\delta \lambda}=-\frac{nk}{\lambda}+\frac{k}{\lambda^{k+1}}\sum_{i=1}^n \left(x_i\right)^k$$ Second Difference... $$\frac{\delta^2 L}{\delta k^2}=-\frac{n}{k^2}-\sum_{i=1}^{n}\left(\frac{x_i}{\lambda}\right)^k \left( \text{ln}\left( \frac{x_i}{\lambda}   \right)\right)^2$$ $$\frac{\delta^2 L}{\delta \lambda^2}=\frac{nk}{\lambda^2}+ \frac{k(k+1)}{\lambda^{k+2}} \sum_{i=1}^n \left(x_i \right)^k$$ $$ \frac{\delta^2 L}{\delta k \delta \lambda}=\frac{\delta^2 L}{\delta \lambda \delta k}=-\frac{n}{\lambda}+\frac{1}{\lambda} \sum_{i=1}^n \left[ \left(\frac{x_i}{\lambda}\right)^k  \left( 1+k\, \text{ln}\left( \frac{x_i}{\lambda}  \right)  \right)  \right]  $$ Given that the Fisher Information Matrix have to be in the form of $$ I(k,\lambda) = -E \begin{bmatrix}
\frac{\delta^2 L}{\delta k^2} & \frac{\delta^2 L}{\delta k \delta \lambda}\\
\frac{\delta^2 L}{\delta \lambda \delta k} & \frac{\delta^2 L}{\delta \lambda^2}
\end{bmatrix}$$ Next, $$E\left(\frac{\delta^2L}{\delta k^2}  \right)\\=E\left(-\frac{n}{k^2}-\sum_{i=1}^{n}\left(\frac{x_i}{\lambda}\right)^k \left( \text{ln}\left( \frac{x_i}{\lambda}   \right)\right)^2 \right)
\\=E\left(-\frac{n}{k^2}\right) - E\left( \sum_{i=1}^{n}\left(\frac{x_i}{\lambda}\right)^k \left( \text{ln}\left( \frac{x_i}{\lambda} \right)\right)^2\right)\\=-\frac{n}{k^2} - \frac{1}{\lambda^k} \sum_{i=1}^nE\left( (x_i)^k \left( \text{ln}\frac{x_i}{\lambda}  \right)^2
\right)\\=...$$ And I am stuck for $E\left(\frac{\delta^2 L}{\delta \lambda^2}\right)$ and $E\left(\frac{\delta^2 L}{\delta \lambda \delta k}\right)$ as well... Should I find the derivation for $E\left[X^{p}\left( \text{ln}\frac{X_i}{\lambda}\right)^q  \right]$ ? Which lead to $\int_0^\infty X^{p}\left( \text{ln}\frac{X_i}{\lambda}\right)^q \frac{k}{\lambda}\left(\frac{x}{\lambda}\right)^{k-1}e^{-\left(\frac{x}{\lambda}\right)^k} $ ? Need your help as I have tried my very best to figure it out for weeks... or does it have a better approach for everything...","['fisher-information', 'statistics', 'probability-distributions', 'log-likelihood']"
4818701,Fixing a proof involving surjective and injective functions,"I'm trying to prove that there exists an injective function $f: A \to B$ if and only if there exists a surjective function $g : B \to A$ . I'm fine with the [⇐] direction (which requires the Axiom of Choice), but stuck with part of the [⇒] direction. The theorem is discussed here There exists an injection from $X$ to $Y$ if and only if there exists a surjection from $Y$ to $X$. but the answers there do not touch upon the issue I have. Here is the start of my proof: [⇒] Let $f: A \to B$ be an injection. We need to prove there exists a surjective function $g: B \to A$ . Cases: $A \neq \emptyset = B$ . $A = \emptyset = B$ . $A \neq  \emptyset \neq  B$ . $A = \emptyset \neq B$ . In case 1 the implication follows because the antecedent is false (there cannot be a function from a non-empty set into the empty set). In Case(2), $f^T$ is trivially a surjective function. For Case 3 we take some arbitrary but fixed element $z \in A$ and
then set $$g =_{df} f^T \cup (B \setminus \operatorname{range}(f) \times \{z\})$$ If $\operatorname{range}(f) = B$ then $g = f^T \cup (B \setminus \operatorname{range}(f) \times \{z\}) = f^T \cup (\emptyset \times \{z\}) = f^T$ . And $g$ is a surjective function. But now consider Case 4. In this case $f : \emptyset \to B$ is clearly an injective function. But since there can be no function from a non-empty set into the empty set, there cannot be a function from $B$ to $A$ , so there cannot be a surjective function. So how can I find a surjective function in Case 4?","['elementary-set-theory', 'axiom-of-choice', 'functions', 'logic']"
4818704,What exactly is $X^2$ and what is $E[X^2]$?,I am very new to Probability theory and there are a few things that really confuse me. Suppose $M$ is a finite Subset of $\mathbb{N}$ and $X:\Omega \rightarrow M$ a random variable. My first question is: What is the Image of $X^2$ ? Because it cannot be $M$ . So is it just all values in $M$ squared? My second question is : why is $E[X^2] = \sum_{x \in M} x^2*P(X=x)$ and not $\sum_{x \in M} x^2*P(X^2=x^2)$ ? That is what I get when following the definition of $E[X]$ . The definition for E[X] for finite Random variables I use is: $E[X]=\sum_{x \in M} x*P(X=x)$,"['definition', 'probability-theory', 'probability']"
4818717,Solve equation $(3x^2-2x-1)^2-(6x^2-4x-5)^2+x=0$,"I tried to expand and I had $27x^4-36x^3-42x^2+35x+24=0$ I don't know how to solve this equation, but I sketch on graph and I have four irrational roots.
Which method I need to use in this equation?? $x_1= \frac{\sqrt{41}}{6}+\frac{1}{2}$ $x_2= -\frac{\sqrt{41}}{6}+\frac{1}{2}$ $x_3= \frac{1-\sqrt{37}}{6}$ $x_4= \frac{1+\sqrt{37}}{6}$ This is roots of equaition, which I tried to find","['algebra-precalculus', 'quartics']"
4818719,Does there exist research about equation like $u_{tt}=\det(D_{x}^{2}u)+\cdots$?,"Does there exist research about equation like $$u_{tt}=\det(D_{x}^{2}u)+\cdots$$ ? That is to say, it contains second order time term $u_{tt}$ and the determination of Hessian of solution $\det(D_{x}^{2}u)$ . Recently, I have read some papers concerning Hyperbolic meaning curvature flow like the paper""The hyperbolic mean curvature flow"" by K.Smoczyk and philippe G.Lefloch on JMPA. But does there exist some research like the above question, if yes, can you offer me some paper? Thank advance.","['partial-differential-equations', 'reference-request', 'differential-geometry']"
4818737,"Prove that $d(x_n,x_{n+1})→0$ $\iff$ $(x_n)$ ia a Cauchy sequence.","Let $X\neq\varnothing$ and let $d:X\times X→\mathbb{R}$ be a function such that $$d(x,y)=0 \iff x=y,$$ $$d(x,y)=d(y,x),$$ $$d(x,z)≤\max⁡\{d(x,y),d(z,y)\}.$$ Let us say that $d$ is a metric. Let $(x_n)$ be a sequence in $X$ . Prove that $$\lim_{n\to\infty}d(x_n,x_{n+1})=0$$ if and only if $(x_n)$ is a Cauchy sequence. I proved that if $(x_n)$ is a Cauchy sequence then $d(x_n,x_{n+1})\to0$ as $n\to\infty$ , but I could not prove that if $d(x_n,x_{n+1})\to0$ as $n\to\infty$ then $(x_n)$ is a Cauchy sequence. I know that it is not true for arbitrary spaces. I tried to prove that just for this special metric.","['cauchy-sequences', 'metric-spaces', 'analysis', 'functional-analysis', 'limits']"
4818774,Exercise 5.18 Isaacs’ Character Theory of Finite Groups,"The exercise is as follows: Prove that the following polynomial takes integer values when evaluated at integer points: $$f(x) = \frac{1}{|G|} \sum_m a(m) x^{\frac{|G|}{m}}$$ where $a(m) = |\{g \in G \mid o(g) = m\}|$ He suggests using that the function $\theta_n(g) = n^{m(g)}$ is a character of $G$ , where $m(g) = \langle \chi_{\langle g \rangle}, 1_{\langle g \rangle} \rangle$ and $\chi = (1_H)^G$ , for some subgroup $H$ of $G$ . This makes it obvious to me that $f(x)$ is going to be some inner product of some character and $\theta_x$ . That said, it’s unclear why $m(g) = |G|/m$ for some $m$ , and, most importantly, that, for every $m$ which has an element in $G$ of that order, we can find some $g$ that makes it work. $m(g)$ is the number of orbits of $\langle g \rangle$ under the multiplication action on the cosets of $H$ in $G$ , so I have to find a proper $H$ , which I couldn’t do… I’m also unsure about what to do with $a(m)$ . The inclusion-exclusion principle yields that it’s a $\mathbb{Z}$ -linear combination of $R_n(1)$ , with $R_n(1) = |\{x \in G \mid x^n = 1\}|$ , as $n$ varies through the divisors of $m$ . That said, I don’t know if this helps at all… Does anyone have any hints? Thanks in advance!","['group-theory', 'finite-groups', 'polynomials', 'characters']"
4818788,Limits inferior of differentiable function,"I am asking myself if for every differentiable function $f\colon[0,\infty)\to\mathbb R$ with $f(0)=0$ and $f(x)>0$ for every $x>0$ in a neighborhood of zero, we have $$\liminf_{x\to 0^+} \frac{f(x)}{f'(x)}=0?$$ Consider $\displaystyle\lim_{\delta\to 0+} \inf\Big\{\frac{f(x)}{f'(x)}:x\in (0,\delta)\Big\}$ . As the function $f$ is continuous, the numerator goes to $0$ . Since $f$ is continuous at $0$ and $f(x)>0\ \forall x>0$ , we have by the $\varepsilon,\delta$ -criterion: $$\forall \varepsilon >0 \ \exists \delta>0 \text{ such that } \forall x \in [0,\delta) \text{ we have } |f(x)|=f(x)<\varepsilon.$$ Hence, the difference quotient $\frac{f(x)- f(0)}{x-0}=\frac{f(x)}{x}>0 \ \forall x>0$ . My thought is to use the following intuitive argument: If $f(0)=0$ and $f(x)>0$ for every $x>0$ , then $f'(x)>0$ for $x$ sufficiently close to $0$ .","['continuity', 'derivatives', 'limsup-and-liminf', 'real-analysis']"
4818798,Does $\ln(\frac{2x}{\ln(2x+1)})\sim W(x)$?,"The function $f(x)= \ln(\frac{2x}{ln(2x+1)})$ when plotted is similar to the plot of the Lambert W function. The Wolfram Alpha says that the limit $\lim_{x\rightarrow\infty} \frac{W(x)}{f(x)}=1$ , but on the graph it seems that the limit doesn't reach one. Someone can explain if the limit is really equal to $1$ or WFA is incorrect? An approach is to consider the function $y=\frac{W(x)}{f(x)}$ and see that $$yf(x)e^{yf(x)}=x$$ We can do the limit as $x\rightarrow\infty$ but it seems to make the problem a little bit too complex. I will be grateful if someone can help me.","['lambert-w', 'limits']"
4818840,"How many subsets of $\{1, 2, \dots , 10\}$ don't have two numbers difference between which is $2, 3$ or $5$?","I probably need to use inclusion-exclusion lemma. The total number of subsets of a $10$ element set is $2^{10} = 1024$ . Now I calculate the number of subsets for which the statement holds individually for $2, 3, 5$ (in that order): for $2$ it's $169$ , for $3$ it's $200$ and for $5$ it's $243$ . To get the number of subsets of $S=\{1 \ldots,n\}$ with no two numbers difference between which is $m$ we'll divide all numbers form the $S$ into $m$ groups by their modulo $m$ . After that we'll count number of subsets of these groups without two consecutive numbers (because difference between two consecutive numbers in these groups is exactly m). The number is actually $F_{r+2}$ if the group has $r$ elements.  See How many subsets contain no consecutive elements? . Using product rule we multiply those to get the numbers I mentioned. For example let's count it for $2$ : we have groups $\{1,3,5,7,9\}$ and $\{2,4,6,8,10\}$ . Each has $5$ elements, so the answer is $F_7 \cdot F_7=169$ . However I struggle to find the number of intersections between those sets.","['elementary-set-theory', 'combinatorics']"
4818850,ODE for variance of linear SDE,"I have an SDE $$d X_t = \left(a(t) + b(t) X_t\right) dt + \left(c(t) + e(t) X_t\right) dW_t $$ where $a, b, c, e$ are deterministic functions. I would like to derive the ODE for $s(t) = \mathrm{Var}(X_t)$ . I have come up with a derivation, but I don't know how to formalize it. $$
\begin{aligned}
\mathrm{Var}(X_{t+dt}) &= \mathrm{Var}(X_{t+dt}) \\
&= \mathrm{Var}(X_t + dX_t) \\
&= \mathrm{Var}(X_t +  \left(a(t) + b(t) X_t\right) dt + \left(c(t) + e(t) X_t\right)dW_t) \\
&= \mathrm{Var}(X_t) + b(t)^2 dt^2 \mathrm{Var}(X_t) +  c(t)^2 dt + e(t)^2 \left(\mathrm{Var}(X_t) + E(X_t)^2\right)dt + \text{cov terms}
\end{aligned}
$$ If I continue by dropping the $dt^2$ and $dW_t dt$ terms above, I'm able to derive an ODE. Is it okay to do this? If so, is there a theorem I could use here?","['stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
4818927,A question on the proof of Proposition 1A.1 of Hatcher's Algebraic Topology,"I'm reading Hatcher's Algebraic Topology and I have a question regarding the proof of Proposition 1A.1. I will outline the proof here and highlight the part I don't understand. Note: I'm sorry if this is a trivial question. I'm self-reading the book and have no other place to ask this. Proposition 1A.1: Every connected graph contains a maximal tree. Proof: Let X be a connected graph. Hatcher starts with an arbitrary subgraph $X_0\subset X.$ The idea is to embed $X_0$ as a deformation retract of a subgraph $Y\subset X$ that contains all vertices of $X$ i.e., a maximal tree. First, the author constructs a sequence of subgraphs $X_0\subset X_1\subset X_2...$ by obtaining $X_{i+1}$ from $X_i$ by adjoining the closures $\bar{e_\alpha}$ of all edges $e_\alpha\subset X-X_i$ having at least one endpoint in $X_i.$ And then the author argues that the union $\bigcup_{i} X_i$ is both open and closed, and hence $X = \bigcup_{i} X_i,$ since $X$ is connected. I understand everything so far. What I don't understand is the part of the following paragraph below that I have typed in bold letters. ""Now to construct $Y$ we begin by setting $Y_0 = X_0.$ Then inductively, assuming that $Y_i\subset X_i$ has been constructed as to contain all the vertices of $X_i,$ let $Y_{i+1}$ be obtained from $Y_i$ by adjoining one edge connecting each vertex of $X_{i+1}-X_i$ to $Y_i,$ and let $Y = \bigcup_i Y_i.$ It is evident that $Y_{i+1}$ deformation retracts to $Y_{i},$ and we may obtain a deformation retraction of $Y$ to $Y_0=X_0$ by performing the deformation retraction of $Y_{i+1}$ to $Y_{i}$ during the time interval $[1/2^{i+1},1/2^i].$ "" I understand the remainder of the proof. Could someone please help me with this part? Any help is truly appreciated. Thank you for your time.","['proof-explanation', 'general-topology', 'algebraic-topology']"
4818942,Probability of one correct answer among 10 questions [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 7 months ago . Improve this question I'm not sure how to obtain the answer to problem 6.5.30 R Johnsonbaugh, Discreete Mathematics, 8 ed: ""An unprepared student who takes a 10-question true–false quiz and guesses at the answer to every question. What is the probability that the student answers exactly one question correctly?"" ChatGPT suggests using the binomial probability formula, but that hasn't been covered in this chapter. How should I think about solving this?","['discrete-mathematics', 'combinatorics', 'problem-solving', 'probability']"
4819014,There is a compass-like tool that can draw $y=x^2$ on paper. Is there one for $y=x^3$?,"Is there a tool that can draw $y=x^3$ on paper? I'm referring to low-tech tools, e.g. not computers. I only know of tools that can draw $y=x^2$ . The YouTube video ""Conic Sections Compass"" from khosrowsadeghi shows one; the Instructables entry ""The Lost Art of the Conic Section Compass"" by hombremagnetico gives a closer look. It is very simple. The axis of the ""compass"" is the axis of a cone, the pencil is a slanting ""edge"" of the cone, and the paper is a plane. The pencil traces the intersection of the plane and the cone - literally, a conic section. In this answer to my previous question ""How to draw a parabola using basic equipment?"" , I show another tool, as described in ""A Geometrical Treatise on Conic Sections"" (via archive.org) by W. H. Drew, that draws $y=x^2$ . I am looking for a tool that can draw $y=x^3$ . It would surely exploit some property of the curve $y=x^3$ . But how?","['cubics', 'geometric-construction', 'curves', 'conic-sections', 'geometry']"
4819036,Finding covering spaces of $\mathbb RP^2\vee \mathbb RP^2$,My question is connected (pun intended) to the Exercise 1.3.14 of Allen Hatcher's Algebraic Topology. The question is to find all connected covering spaces of $\mathbb RP^2\vee \mathbb RP^2.$ I understand how to solve the problem by first finding the fundamental group of $\mathbb RP^2\vee \mathbb RP^2$ and then computing the covers corresponding to the subgroups of the fundamental group. This provides a detailed solution as of how to find the connected covering spaces. My question is this: how do we find the covering spaces of $\mathbb RP^2\vee \mathbb RP^2.$ that are not connected? Are there any general techniques or methods that can be used to answer this question? Thank you!,"['general-topology', 'covering-spaces', 'algebraic-topology', 'group-theory']"
4819061,Solving a second order differential equation using the method of undetermined coefficients,"Problem: Find the general solution of the following differential equation. $$ y'' + 2y' + 4y = 13 \cos(4x) $$ Answer: So the first step is to find the complementary solution, $y_c$ . \begin{align*}
m^2 +2m + 4 &= 0 \\
m &= \dfrac{ -2 \pm \sqrt{ 4 - 4(1)(4)} }{ 2(1)} \\
m &= \dfrac{ -2 \pm \sqrt{ -12 } }{ 2 } \\
m &= -1 \pm \sqrt{3} i \\
\end{align*} Now we have: $$ y_c = e^{-x} ( c_1 \sin( \sqrt{ 3 }x ) + c_2 \cos( \sqrt{ 3 } x ) )$$ .
Now we need to find $y_p$ . \begin{align*}
y_p &= A \cos(4x) + B \sin(4x) \\
y'_p &= - 4A \sin(4x) + 4B \cos(4x) \\
y''_p &= - 16A \cos(4x) - 16B \sin(4x) \\
\end{align*} Let $LHS = y'' + 2y' + 4y $ . We have: \begin{align*}
LHS &= - 16A \cos(4x) - 16B \sin(4x) + \\
    &2 ( - 4A \sin(4x) + 4B \cos(4x)  )
	+ 4( A \cos(4x) + B \sin(4x) ) \\
%
LHS &=
	- 16A \cos(4x) - 16B \sin(4x) - 8A \sin(4x) + \\
    & 8B \cos(4x)	+ 4A \cos(4x) + 4B \sin(4x) \\
\end{align*} \begin{align*}
- 16A \cos(4x) - 16B \sin(4x) - 8A \sin(4x) + 8B \cos(4x) + \\
	& 4A \cos(4x) + 4B \sin(4x) \\
    &= 13 \cos(4x) \\
\end{align*} \begin{align*}
- 12A \cos(4x) - 16B \sin(4x) - 8A \sin(4x) + 8B \cos(4x)
		+ 4B \sin(4x) &= 13 \cos(4x) \\
%
- 12A \cos(4x) - 12B \sin(4x) - 8A \sin(4x) + 8B \cos(4x) &= 13 \cos(4x)
\end{align*} Now we set up the following system of linear equations: \begin{align*}
-12A + 8B &= 13 \\
-12B - 8A &= 0 \\
\end{align*} We rewrite as: \begin{align*}
-12A + 8B &= 13 \\
2A + 3B &= 0 \\
\end{align*} Now we solve the system of equations: \begin{align*}
A &= \left( - \dfrac{ 3 }{2} \right) B \\
-12 \left( \dfrac{-3B}{2} \right) + 8B &= 13 \\
18B + 8B &= 13 \\
B &= 2 \\
A &= -3 \\
\end{align*} Hence our answer is: $$ y = e^{-x} ( c_1 \sin( \sqrt{ 3 }x ) + c_2 \cos( \sqrt{ 3 } x ) ) +
  + 48 \cos(4x) - 32 \sin(4x)   $$ However, the book's answer is: $$ y = e^{-x} ( c_1 \sin( \sqrt{ 3 }x ) + c_2 \cos( \sqrt{ 3 } x ) ) +
		\left( \dfrac{1}{2}\right) \sin(4x) - \dfrac{3 }{4} - \cos(4x) $$ Where did I go wrong? Based upon the comments made by user170231 I have updated my answer. However, it is still wrong. Here is the updated answer: So the first step is to find the complementary solution, $y_c$ . \begin{align*}
m^2 +2m + 4 &= 0 \\
m &= \dfrac{ -2 \pm \sqrt{ 4 - 4(1)(4)} }{ 2(1)} \\
m &= \dfrac{ -2 \pm \sqrt{ -12 } }{ 2 } \\
m &= -1 \pm \sqrt{3} i \\
\end{align*} Now we have: $$ y_c = e^{-x} ( c_1 \sin( \sqrt{ 3 }x ) + c_2 \cos( \sqrt{ 3 } x ) )$$ .
Now we need to find $y_p$ . \begin{align*}
y_p &= A \cos(4x) + B \sin(4x) \\
y'_p &= - 4A \sin(4x) + 4B \cos(4x) \\
y''_p &= - 16A \cos(4x) - 16B \sin(4x) \\
\end{align*} Let $LHS = y'' + 2y' + 4y $ . We have: \begin{align*}
LHS &=
- 16A \cos(4x) - 16B \sin(4x) + 2 ( - 4A \sin(4x) + 4B \cos(4x)  ) \\
	&+ 4( A \cos(4x) + B \sin(4x) ) \\
%
LHS &=
	- 16A \cos(4x) - 16B \sin(4x) - 8A \sin(4x) \\
   &+ 8B \cos(4x) \\
	& + 4A \cos(4x) + 4B \sin(4x) \\
\end{align*} \begin{align*}
- 16A \cos(4x) - 16B \sin(4x) - 8A \sin(4x) \\
   & + 8B \cos(4x) \\
	&+ 4A \cos(4x) + 4B \sin(4x) \\
 &= 13 \cos(4x) \\
%
- 12A \cos(4x) - 16B \sin(4x) \\
    &- 8A \sin(4x) + 8B \cos(4x) \\
		+ 4B \sin(4x) &= 13 \cos(4x) \\
\end{align*} \begin{align*}
- 12A \cos(4x) - 12B \sin(4x) - 8A \sin(4x) \\
 &+ 8B \cos(4x) \\
&= 13 \cos(4x)
\end{align*} Now we set up the following system of linear equations: \begin{align*}
-12A + 8B &= 13 \\
-12B - 8A &= 0 \\
\end{align*} We rewrite as: \begin{align*}
-12A + 8B &= 13 \\
2A + 3B &= 0 \\
\end{align*} Now we solve the system of equations: \begin{align*}
A &= \left( - \dfrac{ 3 }{2} \right) B \\
-12 \left( \dfrac{-3B}{2} \right) + 8B &= 13 \\
18B + 8B &= 13 \\
26B &= 13 \\
B &= \dfrac{1}{2} \\
A &= \left( \dfrac{-3}{2}\right) \left( \dfrac{1}{2} \right) \\
A &= - \dfrac{ 3 }{4 }
\end{align*} Hence our answer is: $$ y = e^{-x} ( c_1 \sin( \sqrt{ 3 }x ) + c_2 \cos( \sqrt{ 3 } x ) )
  - \dfrac{ 3 }{4 } \cos(4x)  + \dfrac{1}{2} \sin(4x)   $$ However, the book's answer is: $$ y = e^{-x} ( c_1 \sin( \sqrt{ 3 }x ) + c_2 \cos( \sqrt{ 3 } x ) ) +
		\left( \dfrac{1}{2}\right) \sin(4x) - \dfrac{3 }{4} - \cos(4x) $$ Where did I go wrong?",['ordinary-differential-equations']
4819062,Does this local ring have a name?,"I came up with this: Let $S=\{(a_n)_{n\geq 1}\:|\:a_n\in \mathbb{C}, (a_n)_{n\geq 1}\text{ converges}\}$ be the set of convergent complex sequences. Then this set forms a ring under pointwise operations, with the multiplicative identity given by the constant sequence $1,1,\dots$ , and the additive identity given by $0,0,\dots$ . Let $S_0=\{(a_n)_{n\geq 1}\in S\:|\:\text{for sufficiently large }n, a_n=0\}$ be the set of sequences that are ""eventually $0$ "". Then $S_0$ is an ideal in $S$ . Define $\overline{S}=S/S_0$ , and for each sequence $s\in S$ , write $\overline{s}=s+S_0\in\overline{S}$ . Then two sequences $s_1,s_2\in S$ are equal in $\overline{S}$ if they are ""eventually identical"". Define $L:\overline{S}\rightarrow\mathbb{C}$ by $L(\overline{s})=\lim_{n\rightarrow\infty}s$ . This map is well-defined, because if $\overline{s_1}=\overline{s_2}$ , then $s_1$ and $s_2$ are ""eventually"" the same, so they have the same limit. By the familiar limit rules, $L$ is a ring homomorphism that maps unity to unity. Let $I\subseteq\overline{S}$ be the kernel of this map. Of course, $I$ is a proper ideal. Let $s\in S$ be a sequence that does not converge to $0$ , so that means $\overline{s}\in\overline{S}\setminus I$ . Then for all sufficiently large $n$ , $s_n\neq 0$ . So it is possible to form a sequence $s'$ such that for all sufficiently large $n$ , $s'_n=1/s_n$ . This means that the sequence $ss'$ is ""eventually"" $1$ , and thus $\overline{s}$ is a unit in $\overline{S}$ . What I have just shown is that every element in $\overline{S}\setminus I$ is a unit, and since $I$ is a proper ideal, every element in $I$ is not a unit. Thus, $I$ is the unique maximal ideal in $\overline{S}$ , which means that $\overline{S}$ is actually a local ring, which is nice. Is there a name for this construction? I don't think it tells us anything new about convergent sequences of complex numbers, it's just a nice way to ""repackage"" what we know about convergent sequences.","['local-rings', 'ring-theory', 'sequences-and-series']"
4819080,How many Katamino solutions are there on a $5 \times 12$ board?,"Katamino is the puzzle of placing twelve polygonal pieces so as to form a $5\times 12$ rectangular array. The pieces consist of all possible arrangements of five connected $1\times 1$ squares. For example, there is the $5\times 1$ piece, the T piece, and so forth. Here is one possible solution of the Katamino puzzle: Question. How many Katamino solutions are there on a $5 \times 12$ board? Answers are also welcome for other sized boards (but specify the piece set). I know that there must be at least four ways to arrange the pieces because I have successfully arranged them in one way, and this arrangement can be flipped horizontally, vertically, or in both directions to create new arrangements. For the same reason, because of this symmetry, the total number of arrangements must be a multiple of four. But how many solutions are there?","['polyomino', 'combinatorics', 'tiling', 'discrete-mathematics']"
4819083,"Two element subset of $\{1,2,\dots,100\}$ with sum of elements being a square","Prove that every 50-element subset of $\{1,2,\dots,100\}$ contains two
elements $a,b$ such that $a+b$ is a square of integer. Any 50-element subset of the set $\{1,2,\dots,100\}$ has $\binom{50}{2}=2205$ 2-element subsets.
We want to prove that one of them is $\{a,b\}$ , where $a+b=k^2$ .
Here $k$ can be one of numbers $2,3,\dots,14$ and here are all ""good"" subsets $\{a,b\}$ : $2^2 \dashrightarrow \{1,3\}$ $3^2 \dashrightarrow \{1,8\},\ \{2,7\},\ \{3,6\},\ \{4,5\}$ $\dots$ $13^3 \dashrightarrow \{69,100\},\ \dots,\ \{84,85\}$ $14^2 \dashrightarrow \{96,100\},\ \{97,99\}$ Counted together, there are $1+4+7+12+17+24+31+40+49+40+28+16+2=261$ such subsets.
Does this approach make any sense and get me closer to the solution?","['contest-math', 'elementary-number-theory', 'pigeonhole-principle', 'combinatorics']"
4819091,Understanding the Likelihood Principle - What is the Big Deal here?,"I am reading about the Likelihood Principle ( https://en.wikipedia.org/wiki/Likelihood_principle ). In short, I think the Likelihood Principle discusses the phenomena that if two experiments have the same underlying likelihood function, then even if these experiments result in different observed data - the parameter estimates will be the same in both experiments ... regardless of the fact that both experiments produced different data. That is, if two likelihoods are identical and only differ by some constant: optimizing these two likelihoods will give you the same parameter estimates. I am trying to understand why the Likelihood Principle is important. Part 1: Here is an example from the Wikipedia Page: Suppose we have two experiments involving independent Bernoulli trials with a probability of success on each trial given by $\theta$ . In both experiments, we are interested in estimating $\theta$ based on the data we observe. In the first experiment, $X$ is the number of successes in twelve trials. In the second experiment, $Y$ is the number of trials needed to get three successes (Negative Binomial Distribution https://en.wikipedia.org/wiki/Negative_binomial_distribution ) The likelihood functions for these two experiments are given by: For $X = 3$ , the likelihood function is: $$\mathcal{L}(\theta \mid X=3) = \binom{12}{3} \theta^3 (1-\theta)^9 = 220\theta^3(1-\theta)^9$$ For $Y = 12$ , the likelihood function is: $$\mathcal{L}(\theta \mid Y=12) = \binom{11}{2} \theta^3 (1-\theta)^9 = 55 \theta^3 (1-\theta)^9$$ We can see that one of these likelihoods is 4 times the other likelihood. This means that both likelihoods are essentially identical and only differ by a constant term. Part 2: I tried to continue these examples by myself: For $X = 3$ , the likelihood function is: $\mathcal{L}(\theta \mid X=3) = 220 \theta^3 (1-\theta)^9$ For $Y = 12$ , the likelihood function is: $\mathcal{L}(\theta \mid Y=12) = 55 \theta^3 (1-\theta)^9$ To find the MLE, take the derivative of the likelihood function with respect to $\theta$ , set it equal to zero, and solve for $\theta$ . For both likelihood functions, the derivative is (not that the constant terms in both likelihoods 220 and 55 would cancel out when you would try to solve for $\theta$ ): $$\frac{d}{d\theta} \mathcal{L}(\theta) = 3\theta^2(1-\theta)^9 - 9\theta^3(1-\theta)^8$$ Setting this equal to zero gives: $$3\theta^2(1-\theta)^9 - 9\theta^3(1-\theta)^8 = 0$$ We can factor out $\theta^2(1-\theta)^8$ from both terms: $$\theta^2(1-\theta)^8 (3(1-\theta) - 9\theta) = 0$$ Setting each factor equal to zero gives the possible solutions for $\theta$ : $\theta^2 = 0 \Rightarrow \theta = 0$ $(1-\theta)^8 = 0 \Rightarrow \theta = 1$ $3(1-\theta) - 9\theta = 0 \Rightarrow \theta = \frac{1}{4}$ The possible solutions for $\theta$ are 0, 1, and 1/4. However, $\theta=0$ and $\theta=1$ are likely not valid solutions  because they imply that the event (observing a head) is either impossible or certain to happen. Therefore, the only likely valid solution is $\theta=1/4$ . Thus, we can see that both likelihoods produce the same estimate for $\theta$ , thus demonstrating the Likelihood Principle. My Question: But why is the Likelihood Principle important? The above exercise just showed me that the same likelihood function produces the same parameter estimates in different situations - Is this problematic? Is this useful? What is the big deal here? What is the big deal about the Likelihood Principle? I have a feeling that the Likelihood Principle is trying to highlight some flaw about Likelihood theory - perhaps something about the uniqueness of parameter estimates from Maximum Likelihood Estimation ... or that the design of experiments is more important than the data collected from the experiments. But I am not sure. Can someone please help me understand why the Likelihood Principle is important? Thanks!",['probability']
4819169,How solve the problem $f(x+2)=f(x)+4x+4$ for any $x$,"Find $f(2012)$ , when $f(2)=0$ and $$f(x+2)=f(x)+4x+4$$ for any $x$ I tried to find $f(4), f(6), f(8),....$ \begin{align*}
f(4)&=f(2)+4 \cdot 2 +4 \\
f(6)&=f(4)+4 \cdot 4 + 4 = f(2)+4 \cdot (2 + 4) + 2 \cdot 4 \\
f(8) &= f(2) + 4 \cdot (2 + 4 + 6) + 3 \cdot 4 \\
\vdots \\
f(2012) &= f(2) + 4 \cdot (2 + 4 + 6 + … + 2010) + 1005 \cdot 4 = 4 048 140
\end{align*} And it is correct answer. But my question is how they found $f(2012)$ as $$f(2012)=2012^2 - 4 = 4048140 \, ?$$ How they found proper function $?$","['functional-equations', 'algebra-precalculus', 'functions']"
4819173,"Some easy examples of operations that don't work like you'd expect them to, i.e. not being commutative or associative etc.","I'm currently teaching some undergrads in their first / second semester about fields, groups, rings and vector spaces. I often see them argue that something is definitely a field, since addition / multiplication is commutative, associative and distributive in general. This argument obviously doesn't hold for arbitrary addition / multiplication but it's kinda hard to get them to see this if they only know these operations from the real numbers.
There are obviously many counterexamples but I'm trying to find some that aren't just constructed as counterexamples but actually used in everyday mathematics. I've gotten so far matrix multiplication and the composition of functions as non-commutative operations. Do you know any basic examples of operations that aren't associative or distributive?","['field-theory', 'ring-theory', 'abstract-algebra', 'binary-operations', 'group-theory']"
4819186,Solving $\lim_{x \to 0} \frac{\cos{(\sin{x})}-\cos{x}}{x^2}$,"My friend got this limits question on his midterm exam that neither I nor did anyone in his class that I asked was able to solve it without using L'Hôpital's rule. $$\lim_{x \to 0} \frac{\cos{(\sin{x})}-\cos{x}}{x^2}$$ I tried using all the trigonometric identities I know of, but it always comes back to the original form. How would we go about solving such a problem? Keep in mind that it's a Mathematics 1 course.","['limits', 'trigonometry']"
4819192,Is the subset of symmetric $ 3\times 3 $ matrices with given eigenvalues a manifold?,"For $ \mu_1>\mu_2>\mu_3 $ are three real numbers. Consider th set $$
S(\mu_1,\mu_2,\mu_3)=\{A\in M(3,\mathbb{R}):,A^T=A,\,\,\lambda_1(A)=\mu_1,\lambda_2(A)=\mu_2,\lambda_3(A)=\mu_3\},
$$ where $ \lambda_1(A)>\lambda_2(A)>\lambda_3(A) $ are eigenvalues of $ A $ and $ M(3,\mathbb{R}) $ denotes the sets of $ 3\times 3 $ matrices. Then the questions is that if $ S(\mu_1,\mu_2,\mu_3) $ is a manifold. Here is my try, define $$
f:\{A\in M(3,\mathbb{R}):A^T=A\}\to\mathbb{R}^3\quad(f(A)=(\lambda_1(A),\lambda_2(A),\lambda_3(A))).
$$ I want to show that for any $ \mu_1>\mu_2>\mu_3 $ the point $ (\mu_1,\mu_2,\mu_3) $ is a regular point of $ f $ and then $ S(\mu_1,\mu_2,\mu_3) $ is a manifold. However I cannot deal with the derivatives of $ f $ . Can you give me some hints or references?","['matrices', 'linear-algebra', 'manifolds', 'differential-topology', 'algebraic-topology']"
4819210,On the nonempty intersection of almost convex sets,"Let $(X,\|\cdot\|)$ be a Banach space and $U_{X}$ its closed unit ball. According to  Himmelber, Fixed points of compact multifunctions , a subset $B \subset X$ is said to be almost convex if given $\varepsilon >0$ if given $\{x_{1},\ldots x_{n}\}\subset B$ there are $\{z_{1},\ldots, z_{n}\}\subset B$ such that $\|x_{i}-z_{i}\|\leq \varepsilon$ and $\mathrm{co}(z_{1},\ldots, z_{n})\subset B$ (""co"" denotes the convex hull). Of course, a convex set is almost convex. It is not very hard to show (for instance, in the Euclidean plane) that in general, the nonempty intersection of almost convex sets is not an almost convex set. I am looking for an example of a bounded and almost convex (but not convex) subset of $X$ (infinite dimensional), say $B$ , such that if $C$ is any family of almost convex subsets de $B$ with nonempty intersection, then $\cap_{A\in C}A\neq \emptyset$ is an almost set. I have tried with the following example. Example : Let $X$ be the Banach space, endowed its usual supremum norm, of real continuous functions defined in $[0,1]$ ,  and $P[0,1]$ the Berstein polynomials of the functions belonging to $U_{X}$ , recall $$
P[0,1]:=\big\{ \sum_{i=0}^{n} \binom{n}{i}  x\big( \frac{i}{n} \big) t^{i}(1-t)^{n-i}: x\in U_{X}, n\in\mathbb{N}  \big\} ,
$$ which is a dense (and convex) subset of $U_{X}$ . It  Also, consider the sets $E[0,1]:=\{\exp(-nt):n\in\mathbb{N}\} $ and $B:=P[0,1]\cup E[0,1]$ . Then, it is easy to check that $B$ is almost convex but not $B$ convex. Now, let $C\subset B$ be any family of almost convex sets such that $A^{*}:=\cap_{A\in C}A\neq \emptyset$ . We consider 3 cases: Case 1:  There is $P_{0}[0,1]\subset P[0,1]$ such that $P_{0}[0,1]\subset A$ , $P_{0}[0,1]$ convex and dense in $A$ for each $A\in C$ . Then, $P_{0}[0,1]\subset A^{*}$ and therefore $A^{*}$ is almost convex. Case 2: $  A_{0}\subset E[0,1] \setminus P[0,1]$ for some $A_{0}\in C$ . Then, it is easy to check that $A_{0}$ is almost convex if, and only if, $A_{0}=\{x_{0}\}$ for some $x_{0}\in E[0,1]$ . As we are assuming that $A^{*}\neq\emptyset$ , we have that $A=\{x_{0}\}$ for each $A\in C$ and then, trivially, $A^{*}$ is almost convex. Case 3: There are $P_{0}\subset P[0,1]$ and $E_{0}[0,1]\subset E[0,1]$ such that $A_{0} = P_{0}[0,1]\cup E_{0}[0,1]$ for some $A_{0}\in C$ . I don't know how to prove (if it is true) that in Case 3 $A^{*}$ is almost convex. We can ""relax"" the assumptions by taking $S[0,1]:=\sin(2\pi t)$ (or any other non-polynomial function) ¿Some body can help? Somebody know an example of such set $B$ ? Many thanks in advance for your comments.","['convex-geometry', 'banach-spaces', 'functional-analysis']"
4819249,Why is the noetherian ring property not definable in first-order logic?,"I am reading this paper on the connection between model theory and algebraic geometry. https://math.uchicago.edu/~may/REU2015/REUPapers/Zhang,Victor.pdf On page 9, I have trouble understanding Example 19: it says: This shows the property of being a noetherian ring is not expressible
as a set of sentences in first-order logic, otherwise $\text{Th}(\mathbb Z)$ would model
such a set and our example model would be a noetherian ring. I do not know why ""otherwise $\text{Th}(\mathbb Z)$ would model
such a set and our example model would be a noetherian ring"". May I please ask for a more expanded explanation? Thank you for any help!","['affine-varieties', 'model-theory', 'algebraic-geometry', 'noetherian']"
4819300,What information is extracted by completions of rings?,"I have finished reading Commutative Algebra / Atiyah-Macdonald. Reviewing, I found that I don't know what completions are good for. As a comparison, localization at a prime ideal $\mathfrak{p}$ of a ring $R$ allows me to focus attention on prime ideals contained in $\mathfrak{p}$ . This was clearly useful when proving Going-up and Going-down, and in other places. But what are completions of rings good for (I'm talking about $\mathfrak{a}$ -adic completions of rings or modules for an ideal $\mathfrak{a}$ )? Do they also allow me to focus attention on some information? If so, what information? An answer does not have to be completely general (i.e. you can assume the ring if noetherian or local or whatever helps, or maybe that $\mathfrak{a}$ is a maximal ideal). I've studied classical algebraic geometry and some scheme theory. I prefer an answer of an algebraic nature, but a geometric answer is acceptable as well. What (algebraic) information about a ring or module becomes more accessible after $\mathfrak{a}$ -adic completion? Maybe the answer is that I'm looking for the wrong type of utility from completions. Maybe it's not about focusing attention on some information. That's an answer too if true (Maybe completions are more like algebraic closures of fields? That is, map a ring to a nicer ring, where more things work, and hopefully conclude something about the original ring?)","['formal-completions', 'algebraic-geometry', 'commutative-algebra']"
4819318,Abstract formulation of associativity,"Say we are given a binary operation $f$ on a set $X$ , that is, $$
f : X \times X \to X.
$$ Denote by $\text{Id}$ the identity map on $X$ .
We say that $f$ is associative if, for all $x, y, z \in X$ , we have $$
f(f(x, y), z) = f(x, f(y, z)).
$$ I was wondering if there is a more abstract way of formulating this relationship, i.e. in a coordinate-free way. After looking at a couple of commutative diagrams, I came up with the following: $$
f \circ (f \otimes \text{Id}) = f \circ (\text{Id} \otimes f),
$$ where $\otimes$ is defined through $$
(f \otimes g)(x, y) = f(x)g(y).
$$ Is this a senisble abstract definition of associativity? Can it be simplified somehow? My goal ultimatiely us to understand associativity as a ""form of higher-order commutativity"", if that makes sense. Am I onto something here?","['binary-operations', 'functions', 'associativity', 'category-theory']"
4819321,How to calculate this improper integral?,"The problem is to solve this integral: $$\int_{-\infty}^{+\infty}\frac{\cos^3x}{x^2+1}\mathrm dx$$ I have an idea that use this formula: $$\cos3x=4 \cos^3x -3\cos x$$ It might be possible to split the above integral into two parts, and for each of these integrals the Feynman integral can be calculated separately by introducing the parameter $$I(a)=\int_{-\infty}^{+\infty}\frac{\cos ax}{x^2+1}\mathrm dx$$ or $$I(a)=\int_{-\infty}^{+\infty}e^{-ax}\frac{\cos x}{x^2+1}\mathrm dx$$ Also I wonder if the Residue Theorem could be used to solve this integral.","['integration', 'calculus']"
4819349,Showing that an upper sum is an upper bound of the improper integral,"The question states to show that for $N = 1,2,3,...$ we have: $$\sum_{k=1}^{N} \frac{1}{\sqrt{k^2+1}+k} \color{green}> \frac{1}{2}\ln{\frac{2N + 1}{3}}$$ My idea is to show the following $$\sum_{k=1}^{N} \frac{1}{\sqrt{k^2+1}+k} \color{red}{\geq} \int_{0}^{N} \frac{dx}{\sqrt{x^2+1}+x}$$ $$\left.\sum_{k=1}^{N} \frac{1}{\sqrt{k^2+1}+k} \color{red}{\geq} \frac{\sqrt{x^2+1}x - x^2}{2} + \frac{\ln{(\sqrt{x^2+1}+x})}{2} \right|_{0}^{N} = \frac{\sqrt{N^2+1}N - N^2}{2} + \frac{\ln{(\sqrt{N^2+1}+N})}{2}$$ Notice the strict inequality in the first inequality and then in the last inequality we can remove the first term on the RHS to have only the logarithm as follows. $$\sum_{k=1}^{N} \frac{1}{\sqrt{k^2+1}+k} \color{green}> \frac{\ln{(\sqrt{N^2+1}+N})}{2} \color{green}> \frac{\ln{(\sqrt{2N^2+1}})}{2} \color{green}> \frac{1}{2}\ln{\frac{2N + 1}{3}}$$ $\blacksquare$ Now my concern is that can I do it any simpler way than having to compute that horror of an integral? Thank you for any insights!","['integration', 'improper-integrals', 'definite-integrals', 'analysis', 'real-analysis']"
4819384,Is Fubini’s theorem behind this equality?,"I consider $H$ and $h$ two non negative functions. I had hard time to understand this equality $$
\int_{0}^{t}h(t-s)\left(\int_{0}^{s}H(s-u)udu\right)ds = \int_{0}^{t}u\left(\int_{u}^{t}h(t-s)H(s-u)ds\right)du
$$ I think it is just an application of Fubini theorem, however I don’t succeed to get the same expression. Indeed I get $$
\int_{0}^{t}h(t-s)\left(\int_{0}^{s}H(s-u)udu\right)ds = \int_{0}^{s}u\left(\int_{0}^{t}h(t-s)H(s-u)ds\right)du
$$ I tried some change of variable ( introduce $v = s - u$ ), it was not successful. Am I missing something ? Thank you a lot Edit : use of the hint given by Bruno.B \begin{split}  
= & \int_{0}^{t}h(t-s)\left(\int_{0}^{s}H(s-u)udu\right)ds \\
= & \int_{0}^{\infty}h(t-s)1_{u\leq s\leq t}\left(\int_{0}^{\infty}H(s-u)u1_{0\leq u \leq s\leq t}du\right)ds \\
= & \int_{0}^{\infty}u1_{0\leq u \leq t}\left(\int_{0}^{\infty}H(s-u)h(t-s)1_{u\leq s\leq t}ds\right)du \\
= & \int_{0}^{t}u\left(\int_{u}^{t}h(t-s)H(s-u)ds\right)du
\end{split} Where the second equality follows because we have $0\leq u\leq s\leq t$ but since $s$ varies between $u$ and $t$ we must have that $u$ varies between $0$ and $t$ that is $1_{0\leq u\leq t}$ .","['calculus', 'change-of-variable', 'multiple-integral', 'fubini-tonelli-theorems']"
4819519,Why is this approximate solution correct?,"Consider the following differential equation $$ y''=-y + \alpha y |y|^2, $$ where $y=y(x)$ is complex in general and $\alpha$ is a real constant such that the second term is small compared to $y$ ( $||^2$ is the absolute square). Numerically I find that a good approximation for this is found by taking the solution for $\alpha=0$ , which is $ae^{ix}+be^{-ix}$ (with $a$ and $b$ determined by the initial conditions) and applying a little shift to the exponents: $ae^{i(1+\Delta_a)x}+be^{-i(1+\Delta_b)x}$ , where $\Delta_a=-\alpha(|a|^2+2|b|^2)/2$ and $\Delta_b=-\alpha(2|a|^2+|b|^2)/2$ . I want to find a simple justification for this approximate solution. My attempt so far is this: assuming the solution $ae^{i(1+\Delta_a)x}+be^{-i(1+\Delta_b)x}$ , and inserting it in the differential equation, we find (neglecting the $\Delta^2$ terms, and expanding the absolute square): $$\Delta_a ae^{i(1+\Delta_a)x}+\Delta_b be^{-i(1+\Delta_b)x}=-\alpha/2 \bigg [ (|a|^2+2|b|^2)ae^{i(1+\Delta_a)x} + (2|a|^2+|b|^2)be^{-i(1+\Delta_b)x} +ab^*ae^{i(3+2\Delta_a+\Delta_b)x}+ba^*be^{-i(3+\Delta_a+2\Delta_b)x}  \bigg ]$$ By matching the exponentials of same power, we directly find the expression of the $\Delta$ s. However there are those two (roughly-) third power terms left, that seem to indicate at first that the assumed form is not correct (because these are not small compared to the first two terms). And yet the $\Delta$ s found in this way give a very good approximation. (You can choose initial conditions such that the last two terms are small compared to the first two, but my approximate solution is good even when this is not the case.) How can we somehow disregard the last two terms? Or can we justify the $\Delta$ s in a different way?","['nonlinear-system', 'nonlinear-dynamics', 'ordinary-differential-equations', 'perturbation-theory']"
4819523,Showing the isomorphism between (the geometric lattice)$\pi_n$ and the lattice $\mathcal{L}(M(K_n)).$,"Here is the question I am trying to solve: Show that the lattice of flats of $M(K_n)$ is isomorphic to the partition lattice $\pi_n$ . Definition: $\pi_n$ is the set of partitions of $[n]$ , partially ordered by $\tau \leq \pi$ if every block of $\tau$ is contained in a block of $\pi$ . I was thinking about trying to guess the map from the following figure (in Oxley book, second edition): But still I am not quite sure how to guess it, I know that for the edge 6, we picked the block $\{c,d\}$ because this edge attached these two vertices and then we will take all the other vertices in the top diagram as singletons to form a map at the edge $6$ between $\pi_4$ and the lattice $\mathcal{L}(M(K_4)).$ but how can I conclude a pattern for the map incase of $\pi_n$ ? And how to show the lattice homomorphism of this map? Could anyone clarify this for me please?","['graph-theory', 'functions', 'combinatorics', 'algebraic-combinatorics', 'lattice-orders']"
4819528,How to get rid of large numbers in a function?,"I study mathematics, and I have a question: I have this math function: f(x) = floor(x^99999 / 10^(floor(log10(x^99999+0.1)) + 1 - 5)) ( floor() is rounding down) It gets x , increases it highly and outputs it's first 5 left digits. For greater simplicity, my function can be written like this: f(x) = floor(Increase(x) / 10^(Length(x) - 5)) where: Length(x) = floor(log10(x+0.1)) + 1 is function outputs length (num of digits) of given x . and Increase(x) = x^99999 is function increases given x . The problem is that I insert small numbers into this function ( x is up to 1000), and it also produces small numbers (5-digit), but when it works, it has to internally generate very large numbers, the number of characters in which is so large that my computer cannot cope  with calculation of the result. Understanding that there can only be small numbers at the input and output leads me to believe that huge numbers are not needed when calculating the result.  This comes from the fact that, for example, such a function f() can be rewritten as: f(x) = IsEqual(x; 2)*y2 + IsEqual(x; 3)*y3 + IsEqual(x; 4)*y4 + ... +
IsEqual(x; 1000)*y1000 where IsEqual() outputs 1 if its two arguments are equal or otherwise 0 , and y1 , y2 , y3 and others are 5-digit hypothetical outputs of function f() . This example shows the possibility of getting rid of too large numbers in functions when calculating them.  Yes, the function itself turns out to be quite cumbersome, but at least it is not as demanding in terms of performance as it was originally. At its core, my question can be reduced to the problem of getting rid of large powers, but the problem may not necessarily lie in the powers, because raising to a power is not the only mathematical way to greatly increase a number (there are also, for example, sums of series, products, multiplication and  other) Question: how can I mathematically simplify/transform such functions so that they do not have such large numbers inside? EDIT : in my exampme-function f() argument x is an integer greater than 1 . Thanks @gnasher729 for this.","['functions', 'transformation', 'special-functions', 'big-numbers']"
4819542,Scalar integrals in higher dimensions,"The thing I want to do The typical vector calculus course defines: A bunch of integrals of vector fields in $\mathbb R^2$ and $\mathbb R^3$ : line integrals of a vector field along a curve, flux integrals of a vector field across a curve in $\mathbb R^2$ , and flux integrals of a vector field across a surface in $\mathbb R^3$ . A bunch of integrals of scalar functions in $\mathbb R^2$ and $\mathbb R^3$ : here, we can just integrate any scalar function over any curve or surface. I know that the integrals of vector fields can be generalized to integrals of differential forms. For example, the flux of a vector field $\mathbf F = M\,\mathbf i + N\,\mathbf j + P\,\mathbf k$ across a surface is equivalent to integrating $M\,\mathrm dy \wedge \mathrm dz + N\,\mathrm dz \wedge \mathrm dx + P \, \mathrm dx \wedge \mathrm dy$ over the surface. If the surface is given a parameterization, writing $x(u,v)$ , $y(u,v)$ , and $z(u,v)$ as functions of $u$ and $v$ , then we can expand $\mathrm dx$ as $\frac{\partial x}{\partial u}\,\mathrm du + \frac{\partial x}{\partial v}\,\mathrm dv$ , do the same for $\mathrm dy$ and $\mathrm dz$ , and simplify the wedge products to get something we can integrate with respect to $u$ and $v$ . When I try to understand how to generalize scalar integrals, I run into trouble, because then I have to understand what a ""metric tensor"" or ""Riemannian volume form"" is, and I don't really understand those. However, I have come up with an approach that I do understand, and which seems to correctly handle all the special cases I am confident in. The approach I'd like to verify All the cases I understand seem to be based on the Jacobian determinant, which I'll write $\frac{\partial (x_1, x_2, \dots, x_n)}{\partial (u_1, u_2, \dots, u_n)}$ , and it is the determinant of the matrix whose $(i,j)$ entry is $\frac{\partial x_i}{\partial y_j}$ . When integrating by substitution in higher dimensions, we multiply by the absolute value of this integral. When integrating over a surface in $\mathbb R^3$ , we multiply by the norm of a cross product of partial derivatives, but it simplifies to the expression $$\sqrt{\left(\frac{\partial(x,y)}{\partial(u,v)}\right)^2 + \left(\frac{\partial(x,z)}{\partial(u,v)}\right)^2 + \left(\frac{\partial(y,z)}{\partial(u,v)}\right)^2}.$$ When integrating over a curve in $\mathbb R^n$ parameterized by $\mathbb r(t)$ , we multiply by $\left\|\frac{\mathrm d\mathbf r}{\mathrm dt}\right\|$ . But we can think of the components of $\frac{\mathrm d\mathbf r}{\mathrm dt}$ as $1\times 1$ Jacobian determinants of each of $x_1, x_2, \dots, x_n$ individually with respect to $t$ . So what I'd like to do in general, to integrate over a $k$ -dimensional object in $\mathbb R^n$ on which the variables $x_1, x_2, \dots, x_n$ are parameterized in terms of $u_1, u_2, \dots, u_k$ , is: Write out all $\binom nk$ Jacobian determinants $\frac{\partial(x_{i_1}, x_{i_2}, \dots, x_{i_k})}{\partial(u_1, u_2, \dots, u_k)}$ . Compute the norm of this $\binom nk$ -dimensional vector: the square root of the sum of squares of these determinants. Integrate my scalar function, multiplied by this norm, with respect to $u_1, u_2, \dots, u_k$ . If this works, I would be very happy, because I would not need to know anything more than how to parameterize my $k$ -dimensional object, and how to take partial derivatives. I looked up a question about how to integrate over a surface in 4 dimensions , which is a special case of what I want to know. The answer there gives a formula that looks very different, but I checked in Mathematica and the contents of the square root simplify to the same thing. That's reassuring, but it doesn't tell me that my approach will continue working when integrating over a $5$ -dimensional object in $17$ dimensions. My question about this approach Most importantly: does my approach work in general? If it does work: am I overcomplicating things - is there a simpler way to compute the same quantity? If it doesn't work: is there a correct method that's as concrete as my approach?","['integration', 'scalar-fields', 'surface-integrals', 'multivariable-calculus', 'differential-forms']"
4819565,$\lim\limits_{x\to+\infty}\sum_{k=0}^\infty \left[\lambda f\left(\frac{x}{2^k}\right)-f\left(\frac{x}{3\cdot 2^k}\right)\right]$?,"Some time ago I saw a problem somewhere (I cannot remember where) that goes something like this. Let $f:[0,+\infty)\to\mathbb{R}$ be a real function with $f(x)=f(\lfloor x\rfloor)$ for all $x\geq0$ , $f(0)=0$ and such that $\exists\lim\limits_{x\to+\infty}f(x)\neq0$ . Can the following limit converge (in $\mathbb{R}$ ) for some constant $1\neq\lambda\in\mathbb{R}$ ? $$\boxed{\lim_{x\to+\infty}\sum_{k=0}^\infty \left[\lambda f\left(\frac{x}{2^k}\right)-f\left(\frac{x}{3\cdot 2^k}\right)\right]}$$ Note that the sum on the limit is actually finite as, given $x\geq0$ , there exists $k_0\in\mathbb{N}$ such that $\forall k\geq k_0:f(x/2^k)=f(x/(3\cdot2^k))=f(0)=0$ . I have noticed that, if $f:[0,+\infty)\to\mathbb{R}$ satisfies the above-mentioned hypotheses, then the limit $L = \lim\limits_{x\to+\infty}\sum_{k=0}^\infty f\left(\frac{x}{2^k}\right)$ cannot exist since $$\Rightarrow L = \lim_{x\to+\infty}\sum_{k=0}^\infty f\left(\frac{x/2}{2^k}\right) =\lim_{x\to+\infty}\sum_{k=0}^\infty f\left(\frac{x}{2^{k+1}}\right) =L-\lim_{x\to+\infty}f(x)\neq L$$ But it could so happen that the former limit exists (?). In fact, if we denote $F(x)=\sum_{k=0}^\infty f\left(\frac{x}{2^{k+1}}\right)$ , we can restate the troublesome condition as $\exists\lambda\neq1:\exists\lim\limits_{x\to+\infty}\lambda F(x)-F(x/3)$ . Note that this condition of $F(x)$ alone can't imply the existence of $\lim\limits_{x\to+\infty}F(x)$ (and, thus, it can't lead us to a contradiction by itself) since for example, considering the 3-adic valuation , we have for $\lambda\neq0$ that $F(x)=\lambda^{-\nu_3(\lfloor x\rfloor)}$ (with $F(x):=0$ for $\lfloor x\rfloor = 0$ ) is divergent while $\lambda F(3n)-F(n)=F(n)-F(n)=0$ which I think implies $\lambda F(x)-F(x/3)\to0$ (?). So it could still happen that $\lim\limits_{x\to+\infty}\lambda F(x)-F(x/3)$ exists.","['analysis', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
4819569,$\zeta(z)$ Identities and a Conjecture,"Brief Introduction I found something quite interesting and would like to share it here. The surface area and volume of a unit radius $n$ -sphere can be expressed in terms of the gamma function $\Gamma(z)$ as follows. Let's define the surface area $$S(n) = \frac{2\sqrt{\pi^n}}{\Gamma\left(\frac{n}{2}\right)}$$ and the volume $$V(n) = \frac{\sqrt{\pi^n}}{\Gamma\left(\frac{n}{2}+1\right)}$$ Now let's define a function $\theta_k(n)$ where $k$ is a 'dimensional shift' such that $S(n-k)$ . $\theta_k(n)$ can be expressed as a ratio as follows $$\theta_k(n) = \frac{S(n-k)}{V(n)}$$ $$ = \frac{n\Gamma(\frac{n}{2})}{\sqrt{\pi^k}\Gamma\left(\frac{n-k}{2}\right)}$$ There are many potentially interesting properties that $\theta_k(n)$ exhibits. Many identities like $$\theta_0(n)=n$$ $$\theta_{2k}(n)\theta_{-2k}(n) = n(n-2k)\prod_{m=1}^{k-1}\frac{n-2m}{n+2m}$$ $$\theta_{-k}(n)= \pi^k \theta_{k}(n)$$ There is one that we will use for the next few parts and has many interesting relations, namely $$\theta_{2n}(2n-1) = -\frac{\Gamma(n+\frac{1}{2})}{\pi^n \sqrt{\pi}}$$ $\theta_k(n)$ in general appears in many different areas of mathematics An example: $$\sqrt{\pi}\int_{-\infty}^\infty x^{2n+2}e^{-x^2}\,dx=-\frac{\theta_{2n}(2n-1)}{2\left(-\pi\right)^n}$$ Identities With The Riemann-Zeta Function $$\frac{\zeta (\frac{1}{2}-n)}{\zeta(n+\frac{1}{2})} = -i^{n(n+1)}\frac{\theta_{2n}(2n-1)}{2^n}$$ $$\theta_{2n}(2n-1) = -\frac{(2n-1)!!}{(2\pi)^n}$$ which therefore implies $$\frac{\zeta (\frac{1}{2}-n)}{\zeta(n+\frac{1}{2})} = i^{n(n+1)}\frac{(2n-1)!!}{(4\pi)^n}$$ Next I will present a conjecture based off of a single observation regarding the ratio of $\theta_k(n)$ and $\zeta(n)$ . Conjecture Let's start off with the following equation $$\frac{2^{n}}{\theta_{2n}(2n-1)} = \phi(n) \zeta(n)$$ We are interested in finding the values of $\phi(n)$ , namely for $\text{parity}(n) := \text{even}$ as these are rational. (I managed to derive $\phi(2n) = (-1)^n\frac{2(4n)!!}{B_{2n}(4n-1)!!})$ however am unsure of whether this is correct or not for all $n \in \mathbb{Z}$ A few of the first values of $\phi(2n)$ (ignoring the $(-1)^n$ term) $$\phi(0) = 2$$ $$\phi(2) = 32$$ $$\phi(4) = \frac{1536}{7}$$ $$\phi(6) = \frac{4096}{11}$$ and strangely $$\theta_3(12) = \frac{1536}{7\pi^2}$$ This seems like quite a big coincidence.
The conjecture states that $$\theta_k(n) = \pi^{-\alpha}\phi(\beta)$$ for some $n, k$ and $\alpha, \beta \in \mathbb{N}$ This conjecture being true would imply that any $\zeta(2n)$ can be expressed in terms of $\theta_k(n)$ (and $\pi$ ) alone, like demonstrated below $$\frac{\theta_{8}(7)\theta_3(12)\pi^{2}}{2^{4}} = \frac{1}{\zeta(4)}$$ My question is simply: Is the conjecture true or not? This is quite a big post and there are bound to be some mistakes here and there, so please correct any if you see some. The objective of this post is 1. to share a potentially interesting result, and 2. Ask whether the conjecture is true or not. If anyone has anything to contribute regarding the post, perhaps an application of $\theta_k(n)$ in a completely different field of mathematics, some identities with other well-known functions, etc then feel free to post it below. Thank you everybody for your patience Edit: Thanks to Steven Clark for the corrections (I have shortened the post by removing all of the lengthy proofs) Bounty (Expired) Bounty for anyone who can prove (or disprove) this conjecture. Good luck!","['riemann-zeta', 'number-theory', 'calculus']"
4819632,Intersection multiplicities of varieties expressed with curves,"Let $X,Y$ be different irreducible projective varieties in $\mathbb P^n$ (over an algebraically closed field). Let $Z$ be an irreducible component of $X\cap Y$ . Then the intersection multiplicity of $X$ and $Y$ along $Z$ is an (non-negative) integer defined in literatures such as Fulton's Intersection theory . Let $m$ be the intersection multiplicity of $X$ and $Y$ along $Z$ . Suppose also that $X$ , $Y$ and $X\cap Y$ are non-trivial, $X$ and $Y$ are of dimensions $\ge 1$ and do not contain each other. Can we intuitively interpret the intersection multiplicity the following way: For every $P\in Z$ and every curve on $Y$ passing $P$ and not
contained in $X$ , the intersection multiplicity of the curve with $X$ at $P$ is $\ge m$ . Furthermore, there is such a curve such that
equality holds.","['algebraic-geometry', 'intersection-theory', 'projective-varieties']"
4819644,How to prove a result about the accumulation point / cluster point in Brownian Motion?,"I have a standard Brownian motion, $B(t)$ , $t\ge0$ . I am trying to prove the following result: Every $t > 0$ is an accumulation point (i. e., cluster point) of $(s: B(s) = B(t))$ from the right, with probability $1$ . That is, I would like to prove that, for any fixed $t > 0,$ $P[$ inf $(s > t : B(s) = B(t)) = t] = 1$ I am new to analysis and preparing for finals that's scheduled for next week.  Any insight or advice on this proof will be very helpful.  Thanks a lot! $EDIT:$ I have a follow-up question that kind of takes off from the above, and is posted here: Proof regarding cluster point in Brownian motion","['real-analysis', 'calculus', 'probability-theory', 'probability', 'random-variables']"
4819680,"For an arbitrary continuous function $f$, is the Stieltjes integral $\int_0^1(df(x))^3=0$?","Suppose $f:[0,1]\to\mathbb R$ is continuous, possibly with unbounded variation. We consider sums of the form $$\sum_{i=1}^n\Big(f(x_i)-f(x_{i-1})\Big)^3$$ where $0=x_0<x_1<x_2<\cdots<x_{n-1}<x_n=1$ is a partition of the unit interval. As the norm $\max_i|x_i-x_{i-1}|$ approaches $0$ , do these sums also approach $0$ ? For any $p>0$ , there are continuous functions without finite $\int_0^1|df(x)|^p$ (for example $x^{1/p}\cos x^{-1}$ , or $|\ln x/2|^{-1}\cos x^{-1}$ ). On the other hand, without the absolute value, we always have $\int_0^1df(x)^1=f(1)-f(0)$ . It's not clear whether $\int_0^1df(x)^3$ always exists.","['stieltjes-integral', 'partitions-for-integration', 'analysis', 'bounded-variation']"
4819682,A question on commutative diagram of abelian groups [duplicate],"This question already has an answer here : A question on diagram of groups [closed] (1 answer) Closed 7 months ago . I am trying to solve the following problem. Question: Consider the commutative diagram of abelian groups given below, where the rows in the diagram are exact. Prove or disprove: if $p,q,s$ and $t$ are zero homomorphisms, then so is $r.$ My attempt: At first I thought this was false, so I tried to find a counterexample. I thought I had one with the sequence $0\longrightarrow Z/2Z\longrightarrow Z/4Z \longrightarrow Z/2Z \longrightarrow 0$ in both rows, but the diagram ended up being not commutative. After several failed attempts to find a counterexample, I tried to prove the statement, which also failed. Could someone please help? Thank you!","['group-theory', 'abelian-groups', 'category-theory', 'diagram-chasing']"
4819722,For which values of $\alpha \in \mathbb{R}$ does $\sum_{n=1}^{\infty}n^\alpha \left (\frac{1}{\sqrt{n}}-\frac{1}{\sqrt{n+1}} \right )$ converge,"I have a question in my analysis course that I have been trying for a long time and I'm not quite sure how to answer it. I have an idea for an answer but I'm not sure if it's right. Also note the only theorems we have learned for determining the convergence of series so far is the comparison test and the theorem that $\sum_{k=1}^{\infty}\frac{1}{k^\alpha}$ is convergent iff $\alpha > 1$ for $\alpha \in \mathbb{R}$ . Question For what values of $\alpha \in \mathbb{R}$ does the following series converge? $$\sum_{n=1}^{\infty}n^\alpha \left (\frac{1}{\sqrt{n}}-\frac{1}{\sqrt{n+1}} \right )$$ My solution (so far) First \begin{align}
n^\alpha \left (\frac{1}{\sqrt{n}}-\frac{1}{\sqrt{n+1}} \right ) &= n^\alpha \left (\frac{\sqrt{n+1}-\sqrt{n}}{\sqrt{n} \cdot \sqrt{n+1}} \right ) \\ &= n^{\alpha-\frac{1}{2}} \left (\frac{(\sqrt{n+1}-\sqrt{n})(\sqrt{n+1}+\sqrt{n})}{\sqrt{n+1} \cdot(\sqrt{n+1}-\sqrt{n})} \right ) \\ &= n^{\alpha-\frac{1}{2}} \left (\frac{1}{n+1+\sqrt{n^2+n}} \right ) \\ &= \frac{1}{n^{\frac{3}{2}-\alpha}} \left (\frac{1}{1+\frac{1}{n}+\sqrt{1+\frac{1}{n}}} \right )
\end{align} Now my idea was that you notice the term in the brackets is bounded for $n \in \mathbb{N}$ $$\frac{1}{4} \leq \left (\frac{1}{1+\frac{1}{n}+\sqrt{1+\frac{1}{n}}} \right ) \leq \frac{1}{2}$$ And so as a result, the convergence of the series of a whole depends on the term $$\frac{1}{n^{\frac{3}{2}-\alpha}}$$ and so I can apply the second theorem I know to put a bound on $\alpha$ for convergence. But this does not seem like an acceptable argument, because it is such a loose statement and not based on any actual results we have learned. Alternatively I thought about the significance of the $1+\frac{1}{n}$ since it appears twice, but I didn't really get anywhere with that. So is my answer at all going in the right direction or not? And if it is how can I continue? Otherwise what alternative approach is there?","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
4819729,Chebyshev's Inequality Improvement,"I'm having trouble improving the upper bound on Markov/Chebyshev's inequality in this particular example: Show that $$\lim_{n\to\infty} n\mathbb{P}(|X_1|\geq \epsilon\sqrt{n})=0$$ Clearly, Markov's inequality yields the upper bound $n\mathbb{E}[|X_1|^2]/\epsilon^2$ which is not good enough. I'd really appreciate any hints or direction.","['measure-theory', 'concentration-of-measure', 'probability-theory', 'probability', 'random-variables']"
4819749,"Substitution in double integrals. When can I substitute the same way as in one-variable integrals, and when do I need to use the jacobian?","I'm just learning (multivariable) calculus and tried solve the following double integral: $$\int_0^2 \int_0^1 x y e^{x y^2} d y d x$$ I approached it initially using traditional u-substitution similar to one-dimensional integrals. Surprisingly, it yielded the correct result, but as I delved deeper into my textbook, I discovered a more rigorous method involving the Jacobian in multivariable integrals. So my question is: When does traditional u-substitution work in multiple-dimensional integrals, and when do I need to use the more rigorous approach with the Jacobian? Below I have explained my initial approach to solving the integral and the more rigorous approach using the Jacobian. My initial attempt: $$
\int_0^2 \int_0^1 x y e^{x y^2} d y d x
$$ I would then make u-substitution like this, where I treat x as a constant: $u=xy^2$ , $\frac{du}{dy}=2xy \implies du=2xy\;dy$ (I now realise this is wrong, since this is not the total derivative, but the partial derivative $\partial u/\partial y$ ). $$
=\int_0^2 \int_0^1 \frac{1}{2} e^{ \overbrace{x y^2}^{u}} \underbrace{2x y \;d y}_{du} \; d x = \int_0^2 \int_0^x \frac{1}{2}e^u \;du \;dx = =\frac{1}{2} \int_0^2\left[e^u\right]_0^x d x=\frac{1}{2} \int_0^2\left(e^x-e^0\right) d x=\frac{1}{2}\left[e^x-x\right]_0^2
$$ $$=\frac{e^2-3}{2}$$ Now this gives the correct result, even though the way I'm doing substitution seems to be inconsistent with how my book treats substitution of multivariable integrals.
Why does this work? Was it a coincidence that it worked, or can I always make u-substitution like this in multivariable integrals? How my book treats substitution in multivariable integrals: Substitution in my book is presented as: $$
\iint_R f(x, y) d x d y=\iint_G f(g(u, v), h(u, v))\left|\frac{\partial(x, y)}{\partial(u, v)}\right| d u d v .
$$ With the Jacobian determinant or Jacobian of the coordinate transformation $x=g(u, v), y=h(u, v)$ defined as $$
J(u, v)=\left|\begin{array}{ll}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{array}\right|=\frac{\partial x}{\partial u} \frac{\partial y}{\partial v}-\frac{\partial y}{\partial u} \frac{\partial x}{\partial v} .
$$ The Jacobian can also be denoted by $$
J(u, v)=\frac{\partial(x, y)}{\partial(u, v)}
$$ How I should solve it rigorously So I imagine that in my case if I want to make proper substitution with, I should write the above equation as: $$
\iint_R f(g(x, y), h(x, y))\left|\frac{\partial(u, v)}{\partial(x, y)}\right| d x d y=\iint_G f(u, v) d u d v
$$ If I choose $u=xy^2$ and $v=xy$ , then I get: $$
\left|\frac{\partial(u, v)}{\partial(x, y)}\right|=\left|\left|\begin{array}{ll}
\frac{\partial u}{\partial x} & \frac{\partial u}{\partial y} \\
\frac{\partial v}{\partial x} & \frac{\partial v}{\partial y}
\end{array}\right|\right|=\left|\left|\begin{array}{ll}
y^2 & 2 x y \\
y^{x^2} & x
\end{array}\right|\right|=|y^2 x-2 x y^2|=x y^2
$$ Since $x\geq 0$ on the interval we are considering in the integral. So I could rewrite the integral: $$
\int_0^2 \int_0^1 x y e^{x y^2} d y d x =\int_0^2 \int_0^1\frac{\overbrace{xy^2}^{\left|\frac{\partial(u, v)}{\partial(x, y)}\right|}}{\underbrace{xy^2}_{u}} \underbrace{x y}_{v} e^{\overbrace{x y^2}^{u}} d y d x  =\begin{cases}
    \int_0^2 \int_{v^2/2}^{2v} \frac{v}{u} e^u d u d v \\
    \int_0^2 \int_{u}^{\sqrt{2u}} \frac{v}{u} e^u d v d u \\
  \end{cases}
$$ Where the integration boundaries come from sketching the domain transformation from the cartesian coordinate system y(x) (enclosed rectangle) to the new domain $u(v)$ or $v(u)$ (new enclosed shapes) by using the above definitions for $u$ , $v$ . $u(v)=v^2/x = vy$ or $v(u)=u/y=\sqrt{ux}$ The second integral is solvable analytically and leads to the same result as the initial approach: $$=\int_0^2 \int_{u}^{\sqrt{2u}} \frac{v}{u} e^u d v d u=\int_0^2\left[\frac{1}{2} v^2\right]_u^{\sqrt{2u}} \frac{e^u}{u} d u = \int_0^2\left(u-\frac{u^2}{2}\right) \frac{e^u}{u} d u =\int_0^2\left(e^u-\frac{1}{2} u e^u\right) d u $$ $$\underbrace{=}_{\text{partial integration}}e^2-e^0-\frac{1}{2}\left(\left[u e^u\right]_0^2-\int_0^2 e^u d u\right)=\frac{e^2-3}{2}$$ But as you can tell, it is much more work using the rigorous method and I obtain the same result anyhow. How can I in general tell if it works correctly to use the initial dummy approach, and when do I need to use the rigorous approach with the Jacobian?
What also seems to be an issue, is that substitution in multivariable integrals is not in our curriculum. So it's only something I figured myself by reading outside our curriculum.","['jacobian', 'multivariable-calculus', 'calculus', 'multiple-integral', 'substitution']"
4819763,Can an unstable focus be an $\omega$-limit point?,"Consider the ODE $\dot x=F(x)$ with $F:U\to\mathbb R^2$ ( $U\subset \mathbb R^2$ open) a $C^1$ function. Assume we have an equilibrium point $(x_0,y_0)$ ( $F(x_0,y_0)=0$ ) that is an unstable focus (i.e., $(x_0,y_0)$ is an unstable focus in the linearized system). I am wondering if it's possible that $(x_0,y_0)\in\omega(x,y)$ for some $(x,y)\neq (x_0,y_0)$ , where by $\omega(x,y)$ I mean the $\omega$ -limit set. Phrased differently: Is the stable manifold of $(x_0,y_0)$ equal to $\{(x_0,y_0)\}$ ? My intuition (to the rephrased question) says yes: close to $(x_0,y_0)$ the solution behaves as a solution for the linearized system, and in the linear case we know the solution has to spiral outwards. Is my intuition correct? If so, how to prove this?",['ordinary-differential-equations']
4819783,Can you critique my exposition of $\int_{-\infty}^{+\infty}e^{-x^2}dx = \sqrt \pi$?,"Prove $$\int_{-\infty}^{+\infty}e^{-x^2}dx = \sqrt \pi$$ using Fubini's Theorem. My solution is below. Proof is Correct. What I want to know is : Is it well written? How could the writing be improved, made clearer, or more rigorous? Solution: We first show that $$\int_{-\infty}^{+\infty}\left [e^{-y^2}\int_{-\infty}^{+\infty}e^{-x^2}dx \right ]dy = \pi.$$ By Fubini's Theorem, $$\int_{-\infty}^{+\infty}\left [e^{-y^2}\int_{-\infty}^{+\infty}e^{-x^2}dx \right ]dy = \iint_{(x,y) \in \mathbb R^2}e^{-(x^2+y^2)}dxdy$$ which, by changing to spherical coordinates, equals $$\begin{align*}\int_{0}^{2\pi}\int_0^\infty re^{-r^2}dr d \theta &= \int_{0}^{2\pi} \Biggr|_0^\infty \frac {-e^{-r^2}}{2} d\theta\\ &=\pi.\end{align*}$$ Now, by linearity of integrals, $$\int_{-\infty}^{+\infty}\left [e^{-y^2}\int_{-\infty}^{+\infty}e^{-x^2}dx \right ]dy = \left [ \int_{-\infty}^{+\infty}e^{-x^2}dx  \right ] \left [ \int_{-\infty}^{+\infty}e^{-y^2}dy \right ]$$ and since $e^{-x^2} > 0$ for all $x$ , $$\int_{-\infty}^{+\infty}e^{-x^2} dx = \sqrt \pi$$ as desired.","['proof-writing', 'real-analysis', 'multivariable-calculus', 'solution-verification', 'fubini-tonelli-theorems']"
4819800,Counting measure and integrability condition,"Let $\mathcal{A}$ be the $\sigma$ -field on $[0,1]$ that consists of all subsets of $[0,1]$ that are (at most) countable or whose complement is (at most) countable. Let $\mu$ be the counting measure on $([0,1], \mathcal{A})$ .
(1) Verify that a function $f:[0,1] \longrightarrow \mathbb{R}$ is in $L^1([0,1], \mathcal{A}, \mu)$ if and only if the set $E_f=\{x \in[0,1]: f(x) \neq 0\}$ is at most countable, and $$
\sum_{x \in E_f}|f(x)|<\infty
$$ Furthermore, we have $\|f\|_1=\sum_{x \in E_f}|f(x)|<\infty$ in that case. (2) For every $f \in L^1([0,1], \mathcal{A}, \mu)$ , set $$
\Phi(f)=\sum_{x \in E_f} x f(x) .
$$ Prove that $\Phi$ is a continuous linear form on $L^1([0,1], \mathcal{A}, \mu)$ . (3) Show that there exists no function $g \in L^{\infty}([0,1], \mathcal{A}, \mu)$ such that $\Phi(f)=$ $\int f g \mathrm{~d} \mu$ for every $f \in L^1([0,1], \mathcal{A}, \mu)$ . Can someone give me idea on how to solve it, main problem of mine is how to use counting measure on real line and do manipulation with it. Thank you.
In case, I start with Indicator function of $A$ , we know it $f$ will be integrable if $A$ is finite and in that case, $\{ x \in [0, 1]: f(x) \neq 0 \}$ is countable.","['integration', 'measure-theory', 'lp-spaces', 'functional-analysis']"
4819801,Question regarding Mean value theorem and a monotonically increasing function $f'$,"I was given the following information about a function:
Let $f$ : [0, $\infty$ ) be a function so that $f(0)=0$ $f$ is continuous on [0, $\infty$ ) $f$ is differentiable on (0, $\infty$ ) the function $f'$ : [0, $\infty$ ) is monotonically increasing Let $x, y \in (0, \infty)$ where $x < y$ .
We know that f is differentiable on (0,x) and (0,y) as well.
By Mean Value Theorem there exists a $c \in (0, x)$ such that $f'(c) = \frac{f(x) - f(0)}{x}$ = $\frac{f(x)}{x}$ .
And, there exists a $d \in (0, y)$ such that $f'(d) = \frac{f(y) - f(0)}{y}$ = $\frac{f(y)}{y}$ . I was confused with the values $c,d$ I defined, Since $x<y$ does this mean that $c<d$ ? I have no idea if this is true but I want to know if theres any sort of relationship between $c$ and $d$ using this function $f$ ?","['mean-value-theorem', 'derivatives', 'real-analysis']"
4819826,Is every tiling pattern of $S$ connected by this simple flipping rule?,"Let $S$ be a shape made out of a finite number of squares, equilateral triangles, and rhombi with angles $30^\circ, 150^\circ$ , all having unit length sides. Often there are multiple ways to compose/tile the same shape $S$ . Here are $3$ different tilings of the dodecagon. We add the rule, anywhere in $S$ a square plus triangle appears, it can be swapped to a triangle plus two rhombi as follows (and vice versa) $\textbf{Question}$ : is every tiling of $S$ connected by a finite number of applications of this rule? The three tilings of the dodecagons above are connected. I have looked at many, many shapes and haven't found a counterexample, but haven't been able to prove it either. $\textbf{New evidence}$ : $(1)$ : It was proved every tiling of $S$ has a specific number of triangles. Furthermore, those triangles have a specific orientation. $(2)$ : Suppose $A$ and $A'$ are two tilings of $S$ we are trying to show are connected. So far it has appeared the case, if certain pieces line up between $A$ and $A'$ , they can be left alone without loss of generality while applying the rule to the remaining pieces.","['geometry', 'tiling']"
4819854,Do the endpoints of a catenary have to be horizontally aligned?,"Nothing I see in the definition of a catenary says this must be the case, but every illustration I've seen draws it that way.  I'm assuming that a cable hanging from two points at different heights off the ground is still a catenary but how does that change the equation and properties?",['ordinary-differential-equations']
4819875,What does this sequence of integral converge to?,"I am not a mathematician, I'm a chemist who happened to also be a math enthusiast. I was playing with error function the other day, and just out of curiosity compute these integrals using Desmos. $$ \int _{-\infty}^{\infty} e^{-(x^2+x+1)} dx \approx 0.837247915445$$ Then, I just add more terms to it $$ \int _{-\infty}^{\infty} e^{-(x^4+x^3+x^2+x+1)} dx \approx 0.710530779033$$ $$ \int _{-\infty}^{\infty} e^{-(x^6+x^5+x^4+x^3+x^2+x+1)} dx \approx 0.682543666557$$ $$ \int _{-\infty}^{\infty} e^{-(x^8+x^7+x^6+x^5+x^4+x^3+x^2+x+1)} dx \approx 0.671744903773$$ And so on. At 20th term (order 40 polynomial), the result is $0.655405978856$ One of the thing that I realize is the difference between terms became smaller, and I think the limit is $\frac{\pi^{0.5}}{e} \approx 0.652049332173292$ . But I have no idea why, or if this is even true. I hope someone can provide me with what the limit is.","['improper-integrals', 'definite-integrals', 'sequences-and-series']"
4819889,Finding integration bounds for the volume between ellipsoid and plane,"Let $S$ be the region between $x+z=1$ , and ellipsoid $x^{2}+2y^{2}+z^{2}=1$ ( $S$ is above $x+z=1$ ). Find the volume of $S$ . I'm having trouble to find the bound for $x,y$ , and $z$ . At first I tried to identify the bound of $z$ as follows: $1-x\le z\le \sqrt{1-x^{2}-2y^{2}}\quad$ and for $x$ is $0\le x\le1\quad$ . But I struggled to find the bound for $y$ . My idea  for this is to find the ellipse equation on $x+z=1$ plane to substitute $z=1-x$ to the ellipsoid equation to get $(2x-1)^{2}+(2y)^{2}=1$ . So I get: $-\sqrt{-(x-\frac{1}{2})^{2}+\frac{1}{4}}\le y\le\sqrt{-(x-\frac{1}{2})^{2}+\frac{1}{4}}$ Is this right or am I doing it wrong?","['integration', 'analytic-geometry', 'multivariable-calculus', 'calculus', 'multiple-integral']"
4819898,What is the limit $\lim_{n\to \infty} \cos(n)^{n^2} $,"Two years ago I found this question and wasn't able to make the slightest advancement. $\lim_{n\to \infty} \cos(n)^{(n²)} $ The question is does the limit exist, where n is natural only (of course it does not exist for n being real). Up until now, I noticed the limit does not exist only if there are peaks of cosine in natural numbers, that peaks ""more"" than raising to n² power. Since cosine peaks in $2\pi k$ , the question arising is ""how good can we approximate multiples of $\pi$ using a natural number, or asking what is the best rational approximation for $\pi$ .","['irrational-numbers', 'calculus', 'pi', 'limits', 'trigonometry']"
4819911,Brezis' exercise 8.24.2: do we need the assumption that $k$ is sufficiently large?,"Let $I$ be the open interval $(0, 1)$ . I'm trying to solve a problem in Brezis' Functional Analysis , i.e., Exercise 8.24 Prove that for every $\varepsilon>0$ there exists a constant $C_{\varepsilon}$ such that $$
|u(1)|^2 \leq \varepsilon\left\|u^{\prime}\right\|_{L^2}^2+C_{\varepsilon}\|u\|_{L^2}^2 \quad \forall u \in H^1(I) .
$$ Prove that if the constant $k>0$ is sufficiently large, then for every $f \in L^2(I)$ there exists a unique $u \in H^2(I)$ satisfying $$
(1) \quad
\begin{cases}
-u^{\prime \prime}+k u=f \quad \text{on} \quad I, \\
u^{\prime}(0)=0 \quad \text{and} \quad u^{\prime}(1)=u(1).
\end{cases}
$$ What is the weak formulation of problem (1)? What is the associated minimization problem? In below attempt of (2.), I don't need that $k$ is sufficiently large. I only need $k \ge 0$ . Could you verify if my attempt contains some subtle mistakes? Let $K := \{v \in H^1(I) : v'(0)=0, v'(1)=v(1)\}$ . Then $K$ is a closed subspace of $H^1(I)$ . If $u$ is a classical solution to $(1)$ , then $$
\int_I [-u''v + kuv] = \int_I fv,
\quad \forall v \in K,
$$ which (by integration by parts) is equivalent to $$
(2) \quad 
u(1) v(1) + \int_I [u'v' + k uv] = \int_I fv,
\quad \forall v \in K.
$$ Then $(2)$ is the weak forlulation of $(1)$ . We define a symmetric bilinear form $a$ on $K$ by $$
a(u, v) := u(1) v(1) + \int_I [u'v' + k uv].
$$ It follows from (1.) that $a$ is continuous. Because $k$ is non-negative, $a$ is coercive. We define $\varphi \in (H^1(I))^*$ by $$
\varphi (v) = \int_I fv,
\quad \forall v \in K.
$$ By Lax-Milgram theorem, $(2)$ has a unique solution $u \in K$ . The associated minimization is $$
u= \operatorname{argmin}_{v \in K} \left \{ \frac{1}{2} a(v, v) -  \varphi (v)\right \}.
$$ Notice that $(2)$ implies $$
\int_I u'v' = -\int_I (ku-f)v,
\quad \forall v \in C^\infty_c (I),
$$ which implies $u \in H^2(I)$ with $u''=ku-f$ .","['sobolev-spaces', 'functional-analysis', 'ordinary-differential-equations']"
4819923,Stuck on system of equations,"What are the solutions of this system of equations, where $x,y \in \mathbb{R}$ ? $\begin{cases}
\frac{1}{x} + \frac{1}{2y}  = (x^2+3y^2)(3x^2+y^2)\\
\frac{1}{x} - \frac{1}{2y}  = 2(y^4-x^4)
\end{cases}$ First I rewrote the equations as $\begin{cases}
\frac{1}{x} + \frac{1}{2y}  = 3x^4+3y^4+10x^2y^2\\
\frac{1}{x} - \frac{1}{2y}  = 2(y-x)(y+x)(y^2+x^2)
\end{cases}$ then $\begin{cases}
\frac{1}{x} + \frac{1}{2y}  = 3(x^2+y^2)^2+4x^2y^2\\
\frac{1}{x} - \frac{1}{2y}  = 2(y-x)(y+x)(y^2+x^2)
\end{cases}$ I tried to add the two equations and I got: $\frac{2}{x}=2(y-x)(y+x)(y^2+x^2)+3(x^2+y^2)^2+4x^2y^2$ we can rewrite this as $\frac{2}{x}=(y^2+x^2)[2(y-x)(y+x)+3(x^2+y^2)]+4x^2y^2$ , $\frac{2}{x}=(y^2+x^2)[2y^2-2x^2+3x^2+3y^2]+4x^2y^2$ $\frac{2}{x}=(y^2+x^2)(5y^2-x^2)+4x^2y^2$ Im stuck from here, how should I proceed? Or is there any trick to solving this system?","['algebra-precalculus', 'systems-of-equations', 'linear-algebra']"
4819930,A system of six equations involving $a^3+b^3+c^3 = (c+1)^3$,"Given six distinct integers $(a_1, a_2, a_3,\dots,a_6)$ , after observing patterns in the computer data of this post , why is it possible to solve a system of six cyclic equations, call it $C_6$ , $$a_1^3+a_2^3+z_1^3 = (z_1+1)^3\\
a_2^3+a_3^3+z_2^3 = (z_2+1)^3\\
a_3^3+a_4^3+z_3^3 = (z_3+1)^3\\
\vdots\\
a_6^3+a_1^3+z_6^3 = (z_6+1)^3$$ for infinitely many sets of 6 $a_k$ ? For example, let $a_k = (-9, 16, 51, 82, 57, 22)$ . Then, $$
22^3 + (\color{blue}{-9})^3 + 57^3 = 58^3\\
(\color{blue}{-9})^3 + \color{blue}{16}^3 + 33^3 = \color{red}{34}^3\\
\color{blue}{16}^3 + \color{blue}{51}^3 + 213^3 = 214^3\\
\color{blue}{51}^3 + \color{blue}{82}^3 + 477^3 = \color{red}{478}^3\\
\color{blue}{82}^3 + \color{blue}{57}^3 + 495^3 = 496^3\\
\color{blue}{57}^3 + \color{blue}{22}^3 + 255^3 = \color{red}{256}^3\\
\color{blue}{22}^3 + (-9)^3 + 57^3 = 58^3
$$ and the cycle repeats, akin to the closed-chain of a cycle graph with 6 vertices.  Or a second set $a_k = (15, -2, 9, 58, 75, 64)$ . Then, $$
64^3 + \color{blue}{15}^3 + 297^3 = 298^3\\
\color{blue}{15}^3 + (\color{blue}{-2})^3 + 33^3 = \color{red}{34}^3\\
(\color{blue}{-2})^3 \,+ \color{blue}{9}^3 + 15^3 \,=\, 16^3\\
\;\color{blue}{9}^3 + \color{blue}{58}^3 + 255^3 = \color{red}{256}^3\\
\color{blue}{58}^3 + \color{blue}{75}^3 + 453^3 = 454^3\\
\color{blue}{75}^3 + \color{blue}{64}^3 + 477^3 = \color{red}{478}^3\\
\color{blue}{64}^3 + 15^3 + 297^3 = 298^3
$$ The fact that three pairs of sums (in red) are the same suggests these numbers are not random. They are members of an infinite family given by, \begin{align}\qquad
a_1 &= 3n + 3n^2 + 9n^3\\
a_2 &= 1 + 6n^2 - 9n^3\\
a_3 &= -3n + 3n^2 - 18n^3 + 27n^4\\
a_4 &= 1 - 3n + 15n^2 - 9n^3 + 54n^4\\
a_5 &= 12n^2 + 9n^3 + 54n^4\\
a_6 &= 1 + 3n + 15n^2 + 18n^3 + 27n^4
\end{align} where the first set was just $n =-1,$ and the second set was $n=1.$ Note that, $$a_1^3+a_2^3+(6n^2 + 27n^4)^3 = (1 + 6n^2 + 27n^4)^3$$ which explains why (for this pair) two terms are immune to sign changes. The polynomials for the other $b_k$ can be found by substituting into the equations above and factoring. Question: Why is it possible to solve the cyclic system $C_6$ in the first place? (It is hard enough to solve a Diophantine system of deg- $2$ equations. One would expect a system of deg- $3$ equations would be harder, unless there was a simplifying principle at work -- which I'm looking for.)","['number-theory', 'systems-of-equations', 'diophantine-equations']"
4819951,Which composite number in a prime gap has the most divisors?,"Let $p_n$ be the $n$ -th prime. We define Most composite number : The composite number $m_n$ between $p_n$ and $p_{n+1}$ such that $m_n$ has the more divisors than any other composites in this prime gap. Least composite number : The composite number $l_n$ between $p_n$ and $p_{n+1}$ such that $l_n$ has less divisors than any other composite in this prime gap. If two or more composites in a prime gap have the highest or the lowest number of divisors then we define $m_n$ or $l_n$ to be the smallest among them. Motivation : In this prime gap, where are $m_n$ and $l_n$ likely to occur? Intuitively, bigger numbers are expected to have more divisors and so we might expect $m_n$ to be closer to $p_{n+1}$ and $l_n$ to be closer to $p_n$ . Experimental data shows that this naive intuition is only true for $l_n$ but for $m_n$ is more likely to be halfway between $p_n$ and $p_{n+1}$ . Experiment : Divide the gap between $p_n$ and $p_{n+1}$ into $2k$ equal parts for some fixed $k > 1, i = 0, 1, \ldots, 2k-1$ . Note that every composite in the prime gap will fall in one of these intervals however, not every interval will contain a composite especially if $2k > p_{n+1} - p_n$ however this does not impact our experiment. $$
\left(p_n + \frac{i(p_{n+1} - p_n)}{2k}, p_n + \frac{(i+1)(p_{n+1} - p_n)}{2k} \right]
$$ We plot a graph with $i$ on the $X$ -axis and the frequency the above interval contains $m_n$ on the $Y$ -axis for different values of $n$ . Observation : The data shows that given any $k$ , the most composite number $m_n$ has the highest frequency in the interval $$
\left(p_n + \frac{(k-1)(p_{n+1} - p_n)}{2k}, p_n + \frac{k(p_{n+1} - p_n)}{2k} \right] 
\tag 1
$$ and lowest frequency in in the very next interval $$
\left(p_n + \frac{k(p_{n+1} - p_n)}{2k}, p_n + \frac{(k+1)(p_{n+1} - p_n)}{2k} \right] 
\tag 2
$$ The experiment was run twice, first for $n \le 10^8$ and second for $2.8 \times 10^{16} < n < 2.8 \times 10^{16} + 10^7$ . In both cases, consistent result was observed. Numerically, for $k=5$ , and $2.8 \times 10^{16} < n < 2.8 \times 10^{16} + 10^7$ about $17.7\%$ of $m_n$ occurred in $(1)$ the middle decile; the next highest was $11.2\%$ which occurred in the first decile and only $5.98\%$ fell in $(2)$ ; the middle decile. This shows that the mid-point $\displaystyle \frac{p_n + p_{n+1}}{2}$ is the top candidate for being the most composite number. Also since interval $(1)$ had the highest frequency while $(2)$ had the lowest frequency, in case the mid-point did not have the highest number of divisors, the next best candidate would be the composite preceding the mid-point. Question : Is it true that the mid-point and the number preceding the mid-point is more likely to have more divisors and any other composites in a prime gap?","['analytic-geometry', 'number-theory', 'elementary-number-theory', 'distribution-theory', 'prime-numbers']"
4820061,Proving an inequality of a function given its derivative's values,"Let $f(x)$ be differentiable twice and for which it's true that $f(0) = f'(0) = 1$ and $f''(x) > \frac{1}{x^2 + 1}$ for all values of $x$ . Prove that $\forall x \ge 1: f(x) > 2$ . I thought this might be solvable using Taylor's theorem. Around the point $0$ , $f$ can be written as $f(x) = f(0) + f'(0)x + \frac{f''(0)}{2}x^2 + R(x)$ where $R(x)$ is the error term. This simplifies down to $f(x) = 1 + x + \frac{f''(0)}{2}x^2 + R(x)$ for which it's true that $f(x) = 1 + x + \frac{f''(0)}{2}x^2 + R(x) > 1 + x + \frac{f''(0)}{2}x^2 > 1 + x + \frac{x^2}{2}$ , therefore for $x\ge1$ , $f(x) > 2$ . Now, I'm very sure this is not how you solve this but it's the only thing I've been able to find any success with. Any help would be appreciated.","['derivatives', 'real-analysis']"
4820110,Why do we sometimes lose solutions when solving equations?,"$\forall f:x=y \implies f(x)=f(y)$ which means that any operation can be done on both sides of an equation. When we solve equations we do one operation after the other: $$
x=y \implies f_1(x)=f_1(y) \implies f_2(f_1(x))=f_2(f_1(y)) \implies \dots \\
\implies f_n\circ f_{n-1}\circ\dots \circ f_1(x) = f_n\circ f_{n-1}\circ\dots \circ f_1(y)
$$ The solutions for the new equation (after the operations) are the solutions for $x=y$ and extraneous solutions (some operations don't produce extraneous solutions). If that is the case why do we sometimes lose solutions when solving equations? Edit: Note that $x$ is the left side of the equation and $y$ is the right side of the equation. $x$ and $y$ are expressions.",['algebra-precalculus']
4820113,Proving that a complicated complex expotential is periodic or not.,"I'm trying to prove that $x[n]=\exp \left(j\left(\frac{\pi}{24} n^2+\frac{\pi}{36} n^3\right)\right), n \in \mathbb{Z}$ is not periodic. I thought of proving that there is no $N \in \mathbb{Z}$ such that $x[n] = x[n + N]$ . I've reached this point: \begin{align*}
    \exp \left(i\left(\frac{\pi}{24} n^2+\frac{\pi}{36} n^3\right)\right) &= \exp \left(i\left(\frac{\pi}{24} (n + N)^2+\frac{\pi}{36} (n + N)^3\right)\right) \\
    \frac{\pi}{24} n^2+\frac{\pi}{36} n^3 &= \frac{\pi}{24} (n + N)^2+\frac{\pi}{36} (n + N)^3 + 2\pi m \tag{  $e^{ia} = e^{ib} \Leftrightarrow a - b = 2\pi m, m \in \mathbb{Z}$} \\[15pt]
     -2\pi m &=  \frac{\pi\left(2 N^2+6 N^2 n+3 N^2+6 N n^2+6 N n\right)}{72}  \\
     -2\pi m &= \frac{\pi N^2(6 n+5)+\pi N\left(6 n^2+6 n\right)}{72}
\end{align*} This seems very complicated to me and as it is part of a question of a 2nd-year level assignment, I hardly believe that this is the way to go. Any help would be greatly appreciated. UPDATE : Moreover, I tried plugging the equation into $\texttt{SymPy}$ and solving as to N, what I got was: \begin{equation}  N= \pm \frac{3\left(-n(n+1)+\sqrt{-96 m n-80 m+n^4+2 n^3+n^2}\right)}{6 n+5}
\end{equation} Not sure if this helps in any way. Intuitively I'd expect $N$ not to be a function of $n$ , but rather a constant. Maybe I'm wrong in thinking of $m$ also as a constant and not a function of $n$ , because that way I could argue that the RHS of N is always a function of $n$ and therefore $x(n)$ is not periodic since $N$ is not fixed. EDIT: Turns out the function is periodic. Huge thanks to @Conrad for pointing it out.","['periodic-functions', 'complex-analysis', 'discrete-mathematics', 'trigonometry', 'signal-processing']"
4820137,How to prove/check a random generator is a unifrom random distribution?,"If I have access to a random integer generator that produces values in the range from $0$ to $N-1$ , how can I verify whether this generator truly produces a uniform random distribution? Simply analyzing the frequencies of generated numbers might not be sufficient, as the sequence could be cyclic, repeating in the order of $0, 1, \cdots, N-1$ , and then starting over from $0$ . I believe that examining only $M$ samples ( $M >> N$ is some fixed large number) for this purpose might be inadequate. Perhaps considering the time series of the generated numbers is crucial. Are there any advanced techniques available for accurately assessing this random number generator?","['statistics', 'probability', 'algorithms']"
4820151,What are the rules for solving differential inequalities using Laplace Transforms?,"I am looking to solve a non-autonomous differential inequality of the form $$\frac{dV}{d\tau}-\epsilon V-\frac{E^2}{2\epsilon}\leq\frac{A^2}{2\epsilon}\tau^2+\frac{2AE\tau}{\epsilon},$$ where $V:\mathbb{R}_{\geq0}\to\mathbb{R}_{\geq0}$ , and $\epsilon$ , $E$ , and $A$ are positive (non zero) scalar constants. When I take the Laplace transform, and after some simple algebraic manipulation I obtain $$(s-\epsilon)\mathcal{V}(s)\leq\frac{A^2}{\epsilon s^3}+\frac{2AE}{\epsilon s^2}+\frac{E^2}{2\epsilon s}+V(0).$$ Now, the question is about how to properly dived both sides by $(s-\epsilon )$ since this, as far as I can tell, is sign indefinite. Now, if I assume that that there is no sign change when I do this operation, followed by partial fraction decomposition and then taking the inverse Laplace transform, I obtain $$V(\tau)\leq \left(\frac{A^{2}}{\epsilon^{4}}+\frac{E^{2}}{2\epsilon^{2}}+V(0)+\frac{2AE}{\epsilon^{3}}\right)\text{e}^{\epsilon\tau}-\left(\frac{A^{2}}{\epsilon^{3}}+\frac{2AE}{\epsilon^{2}}\right)\tau-\frac{E^{2}}{4\epsilon^{2}}\tau^{2}-\frac{E^{2}}{2\epsilon^{2}}-\frac{A^{2}}{\epsilon^{4}}-\frac{2AE}{\epsilon^{3}}.$$ This seems fine on initial inspection, since $V(\tau)=V(0)$ when $\tau=0$ , however, things get odd when you change the value of $\epsilon$ . For example, let $A=0.1$ , $E=0.2$ , $V(0)=0$ , then when $\epsilon =0.1$ the value of $V(\tau)<0$ for $\tau\in(1,9)$ (rough bounds). This is a problem since $V$ is a positive definite function (time derivative is positive definite and initial condition here is zero). There is clearly an issue here, and my suspicion is in what I am doing while I am working in the Laplace domain. Now, I see that I can divide equation two by $(s-\epsilon)$ and take it in piecwise form such that $$\mathcal{V}_{u}(s)\leq\begin{cases}
\frac{1}{s-\epsilon}\left(\frac{A^{2}}{\epsilon}\frac{1}{s^{3}}+\frac{2AE}{\epsilon}\frac{1}{s^{2}}+\frac{E^{2}}{\epsilon2}\frac{1}{s}+V_{u}(t_{0})\right), & s-\epsilon\geq0\\
-\frac{1}{s-\epsilon}\left(\frac{A^{2}}{\epsilon}\frac{1}{s^{3}}+\frac{2AE}{\epsilon}\frac{1}{s^{2}}+\frac{E^{2}}{\epsilon2}\frac{1}{s}+V_{u}(t_{0})\right), & s-\epsilon<0
\end{cases},$$ but how would one take the inverse Laplace transform of this while taking the conditions of the piecewise function into account? Would I do the following $$\mathcal{L}^{-1}\{s-\epsilon\}$$ and start to look at the time derivatives of the Dirac-delta function? That seems very bizarre to me, so any help on this issue would be greatly appreciated. perhaps I shouldn't use Laplace transforms and I should solve this differential inequality by a different method? Suggestions are welcome! Thanks,
Sage","['calculus', 'laplace-transform', 'ordinary-differential-equations', 'dynamical-systems']"
