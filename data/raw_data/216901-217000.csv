question_id,title,body,tags
4415229,Conditional expectation with Bernoulli distribution,"Suppose the number of children in a family is a random variable $X$ with expectation $\mu$ , and given $X = n$ for $n\ge1$ , each of the $n$ children in the family is a girl with probability $p$ and a boy with probability $1 − p$ . Let $Y$ be the number of girls in this family. Compute $\mathbb{E}[Y|X]$ and $\mathbb{E}[Y]$ . I would say that $Y$ is a Bernoulli random variable with probability $p$ , i.e. if we say that $Z=1$ is ""child is a girl"" and $Z=0$ is ""child is a boy"", we have $\mathbb{P}(Z=1)=p$ and $\mathbb{P}(Z=0)=1-p$ . Then we know $\mathbb{E}[Y]=p$ , is it so trivial? Then $\mathbb{E}[Y|X]$ is defined as $\mathbb{E}[Y|X]=\frac1{\mathbb{P}(X)}\int_X Yd\mathbb{P}$ , but $X$ is just the number of the children, so $\mathbb{P}(X)=1$ ? What about the integral of $Y$ ? Sorry but i got confused in this problem..","['probability-distributions', 'conditional-expectation', 'expected-value', 'probability-theory', 'probability']"
4415279,How to prove following Generalization of Hilbert Nullstellansatz,"This question is from assignment 1 of my algebraic geometry course. For theory, I have been following my class notes. Question: Let K be an arbitrary field, S be the set of all polynomials in $K[X_1, ..., X_n]$ which have no zeroes in $K^n$ and let A be an ideal in $K[X_1,...,X_n]$ . If $S\cap A =\varnothing$ , then show that $V_K(A)\neq \varnothing$ . Thoughts: $S\cap A=\varnothing$ implies that all elements of A have at least 1 zero in $K^n$ . I have to prove that all elements of A have at least 1 common zero. But, why should all elements have at least one common zero? I am not sure what from the information given in the question would be useful to proceed foreward. Can you please give some hints so that I could able to proceed to proving this question?","['algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
4415357,A ‘quite’ elementary function $f(x)=\frac{x^{s-1}}{s^{x}-1}$ related to Riemann-Zeta function,"So I was studying the following function: $f(x)=\frac{x^{s-1}}{s^{x}-1}$ where $s$ is any natural number greater than $1$ by playing with Wolfram-Alpha a bit I observed that by taking the integral of $f(x)$ from $0$ to $\infty$ or $\int_{0}^{{\infty}}\frac{x^{s-1}}{{s}^{x}-1}$ and setting $s=2n+1$ We get the following results provided by Wolfram: $$\int_{0}^{{\infty}}\frac{x^{3-1}}{{3}^{x}-1}=2\frac{\zeta(3)}{\ln^{3}(3)}$$ $$\int_{0}^{{\infty}}\frac{x^{5-1}}{{5}^{x}-1}=24\frac{\zeta(5)}{\ln^{5}(5)}$$ $$\int_{0}^{{\infty}}\frac{x^{7-1}}{{7}^{x}-1}=720\frac{\zeta(7)}{\ln^{7}(7)}$$ .
.
.
And so on up to some odd numbers, I know that there is no elementary way of evaluating this integral but I wanted to see a hindsight of how this results came about and how is it related to the Riemann-Zeta function!","['integration', 'number-theory', 'real-analysis', 'calculus', 'riemann-zeta']"
4415364,$K$-theory of $S^2$: spinor bundle vs tautological bundle over $\mathbb{C}P^1$,"I'm trying to understand the relationship between different generators of the $K$ -theory group of $S^2$ . Part of my curiosity comes from reading this discussion about characteristic classes. The $K$ -theory group $K^0(S^2)$ is isomorphic to $\mathbb{Z}\oplus\mathbb{Z}$ . We may choose the first copy of $\mathbb{Z}$ to be generated by the trivial bundle $\varepsilon_{\mathbb{C}}^1=\mathbb{C}\times S^2$ . For the second generator, there are two choices that I have seen in the literature, and I would like to understand how they relate to each other: The tautological bundle $H$ over $\mathbb{C}P^1\cong S^2$ ; The positive and negative spinor bundles $\mathcal{S}^+$ and $\mathcal{S}^-$ over $S^2$ . The first one is discussed in some detail here on nLab. The second one is used for example in section 2.16 of this paper by Baum and van Erp. Let us fix a diffeomorphism $\psi\colon S^2\to\mathbb{C}P^1$ . Then both $H$ and $\mathcal{S}^{\pm}$ can be viewed as line bundles over $S^2$ , all with non-trivial first Chern classes I believe. Question 1: Is $H$ isomorphic either to $\mathcal{S}^+$ or $\mathcal{S}^-$ ? Is there a canonical way to identify them, starting from a choice of $\psi$ ? Looking at the answers to this question , it seems that isomorphism classes of line bundles correspond precisely to their first Chern classes. So perhaps this boils down to: Question 2: Is $c_1(H)$ always equal to either $c_1(\mathcal{S}^\pm)$ ? How does this depend on $\psi$ ? Comment: I think perhaps something similar can be done for $S^{2n}$ , or maybe even $S^n$ for all $n$ , but for now I'm just trying to understand $S^2$ . I wonder if there is a reference discussing all this.","['algebraic-topology', 'characteristic-classes', 'vector-bundles', 'differential-forms', 'differential-geometry']"
4415408,Confidence interval for Poisson distribution,"$X_{1}, X_{2}, ..., X_{n}$ is a random sample from $Poisson(\lambda)$ population.
I need to show that when sample size n is large, the approximate two-sided (1- $\alpha$ )% C.I. is $$
\left[
  \bar{x}
+ \frac{1}{2n} z_{\alpha/2}^2
- \frac{1}{2}
  \sqrt{\left(2\bar{x}+\frac{1}{n}z_{\alpha/2}^2\right)^2-4\bar{x}^2},
  \bar{x}
+ \frac{1}{2n} z_{\alpha/2}^2
+ \frac{1}{2}
  \sqrt{\left(2\bar{x}+\frac{1}{n}z_{\alpha/2}^2\right)^2-4\bar{x}^2}
\right]
$$ I got $\frac{\bar{x}-\lambda}{\sqrt{\lambda/n}}\sim N(0,1)$ and $\bar{x}\pm z_{\alpha/2}\sqrt{\bar{x}/n}$ by myself and found from a paper that $\frac{1}{2n}z_{\alpha/2}^2$ is the tail probability for small sample size. Now I'm confused with $\frac{1}{2}\sqrt{(2\bar{x}+\frac{1}{n}z_{\alpha/2}^2)^2-4\bar{x}^2}$ . From the structure of the equation, I guess it is the error? But I'm not sure about it and have no idea how to get it. Can anyone give me some hints or guide for this part please?","['statistics', 'confidence-interval', 'poisson-distribution']"
4415411,Formula for the interior product of a p-form,"Let $X: \mathbb{R}^n \to \mathbb{R}^n$ be a vector field and let $$\omega = \sum_{i_1 < \dots < i_p} f_{i_1\dots i_p} dx_{i_1} \wedge \dots \wedge dx_{i_p}$$ be a $p$ -form over $\mathbb{R}^n$ . I am interested in a formula for the interior product $i_X\omega$ . (Choose $n=3$ and $X(x,y,z) = (z,y,-x)$ for all examples below.) $p = 1$ : \begin{align}
\omega &= \sum_i f_i dx_i &\Rightarrow \quad i_X\omega &= \sum_i f_iX_i\\
\omega &= dx + zdy + zdz &\Rightarrow \quad i_X \omega &= X_x + zX_y + zX_z\\
& & &= z + yz - xz
\end{align} $p = 2$ : \begin{align}
\omega &= \sum_{i < j} f_{ij} dx_i \wedge dx_j &\Rightarrow \quad i_X\omega &= \sum_{i < j} f_{ij} \left( X_i dx_j - X_j dx_i \right)\\
\omega &= dx \wedge dy + dx \wedge dz &\Rightarrow \quad i_X \omega &= X_xdy - X_ydx + X_xdz - X_zdx\\
& & &= zdy - ydx + zdz + xdx \\
& & &= \left( x - y \right)dx + zdy + zdz
\end{align} $p = 3$ : \begin{align}
\omega &= \sum_{i < j < k} f_{ijk} dx_i \wedge dx_j \wedge dx_k &&\Rightarrow \quad i_X\omega =\ ???\\
\omega &= 2zdx \wedge dy \wedge dz &&\Rightarrow \quad i_X\omega =\ ???
\end{align} Questions Are my formulas and examples for $p=1$ and $p=2$ correct? What is the formula for $p \geq 3$ ?","['vector-fields', 'differential-forms', 'differential-geometry']"
4415430,Finding a maximal vector space in a finite set,"Let $X \subseteq \{0,1\}^n$ be a nonempty set of vectors of length $n \geq 1$ with binary components such that the zero vector $\vec{0}$ having all components equal to $0$ belongs to $X$ . Along with the sum $\oplus$ defined as the component-wise xor (modulo 2 sum) of two vectors (i.e. consider the usual GF(2) field), i.e. let $x_i, y_i$ be the $i$ -th component of $\vec{x}, \vec{j}$ respectively, then $$
(\vec{x} \oplus \vec{y})_i :=\quad  x_i + y_i\quad\text{(mod 2)}
$$ I stumbled upon the problem of finding (one of) the biggest (w.r.t. set inclusion) vector spaces contained in $X$ . I tried to look around but I don't seem to be able to find any mention of this problem in literature, not even in its more general formulation involving arbitrary vectors (thus not restricting to binary components and the component-wise modulo 2 sum). (the considerations below on how I feel about the problem being NP-complete, and about the bounds are inexact and refer to a slightly different problem, which I asked here: Finding a biggest (in size) vector space in a finite set ) While the ""dual"" problem of finding the smallest vector subspace containing $X$ is well-known, and easy to solve, I feel like this problem of finding the biggest subset which is a vector space constitutes a harder problem, in terms of computational complexity.
What can we say about the complexity of this problem? Is there any reduction from a NP-complete problem to the above problem? Is there any efficient way to solve this problem? Any information about this problem is welcome , I find no mentions of it in literature. Update with some more (useful) informations: The existence of a solution is obvious, since the set $\{\vec{0}\}$ is a vector space contained in $X$ by definition, and there are finitely many subsets of $X$ (since $X$ is finite). Note that it might not be unique, if multiple maximal spaces exist in $X$ , finding one of them is enough. It would also be useful to find some bounds on the size of the solution, for example, if $|X| \geq 2$ then we can infer that any solution has at least dimension 1. Maybe this reasoning extends to a general bound? If I were to guess such a bound would probably end up being very loose.","['finite-fields', 'np-complete', 'vector-spaces', 'linear-algebra', 'computational-complexity']"
4415463,Show that zeros of a differentiable function is countable when the the intersection of the set of zeros and the set of critical point is empty.,Let $f:\mathbb{R}\to\mathbb{R}$ be a differentiable function such that $\mathbf{A}=\{x\in\mathbb{R}:f(x)=0\}$ and $\mathbf{B}=\{x\in\mathbb{R}:f'(x)=0\}$ and $\mathbf{A}\cap\mathbf{B}=\phi$ . Then show that $\mathbf{A}$ is countable. What I know is $sin{x}$ is an example of such a function. The cardinality of $\mathbf{A}$ for $sin{x}$ countable. But I don't how to prove this in general for any function satisfying these conditions?,"['calculus', 'measure-theory', 'analysis', 'real-analysis']"
4415580,"Draw phase diagram of $x''+\sin x=0$ around $(0,0)$ manually","I am going to draw the phase digram of $x''(t)+\sin(x(t))=0$ around point $o=(0,0)$ manually . My ideas so far: Let $y=\frac{dx}{dt}$ . Then : $$\begin{align}
\frac{dx}{dt}=y \qquad \frac{dy}{dt}=-\sin x
\end{align}$$ If I use first order approximation, then I have: \begin{equation}
\frac{dx}{dt}=y \qquad \frac{dy}{dt}=-x
\end{equation} This coefficient matrix  of this equation set is: \begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix} and it's eigenvalue is $i,-i$ . So, the null solution of the approximation equation set is a center point. But the null solution of the orgin equation set can be center point or focus point. How can I distinguish them? P.S., by appling Lyapunov function $V(x,y)=\frac{1}{2}y^2+1-\cos x$ to the orign equation set, I know $(0,0)$ is stable. Also, by coding from wolframalpha, I know $(0,0)$ is a center point, but I would like to know how to prove it.",['ordinary-differential-equations']
4415690,Expectation of maximum from $n$ draws.,"We have $n$ guys walking down the street, and each can find $1, 2, \ldots $ or $n$ dollars in the street. ( $n$ is the same number of guys and the same number of dollars in the problem). Each of them finds $1$ dollar with probability $\dfrac{1}{2}$ , $2$ dollars with probability $\dfrac{1}{4}$ , $3$ dollars with probability $\dfrac{1}{8}$ , and so on. This is, he finds $x$ number of dollars with probability $\dfrac{1}{2^x}$ . (These probabilities are independent between guys, think that they are walking down different streets; the remaining $1/2^n$ probability can be arbitrarily assigned to getting $n$ (or 0) dollars, whatever is more convenient). In expectation, every agent finds less than $2$ dollars. But: what is the expected number of dollars that the luckiest guy gets? This is, if $x_1, \ldots, x_n$ is the number of dollars that people find, what is $E[\max(x_1, \ldots, x_n)]$ ? Edit: I am interested in obtaining a simple expression for the expectation OR a good upper bound. Idea: Simulations suggest that the answer is around $\log(n)$ . Edit: I tried solving it using Lulu's suggestion. Let $M=\max(x_1, \ldots, x_n)]$ . Then $E[M]=\sum_{i=1}^n Pr(M=i)\cdot i$ . $Pr(M=1)=\frac{1}{2^n}$ . $Pr(M=2)=\frac{3^n}{4^n}-\frac{1}{2^n}$ . $Pr(M=3)=\frac{7^n}{8^n}-\frac{3^n}{4^n}+\frac{1}{2^n}$ . $Pr(M=4)=\frac{15^n}{16^n}-\frac{7^n}{8^n}+\frac{3^n}{4^n}-\frac{1}{2^n}$ . This implies that $E[M]=1 \cdot \frac{1}{2^n} + 2 \cdot (\frac{3^n}{4^n}-\frac{1}{2^n}) + 3 \cdot (\frac{7^n}{8^n}-\frac{3^n}{4^n}+\frac{1}{2^n}) + 4 \cdot (\frac{15^n}{16^n}-\frac{7^n}{8^n}+\frac{3^n}{4^n}-\frac{1}{2^n}) + \ldots$ . But I do not see how that expression converges to something easy to work with - or close to $\log (n)$ . Edit 2: A different approach, using $E[M]= \sum_{k=0}^\infty Pr(M\geq k)$ , gives $E[M]=\sum_{k=0}^\infty 1- \left( \frac{2^{k-1}-1}{2^{k-1}}\right)^n$ , but I am not sure how to expand this term to make it approximately equal to $\log(n)$ .","['expected-value', 'order-statistics', 'probability']"
4415708,Finding the side length of a triangle type problem,"Find $x$ in the diagram Man.. I bet this is really easy but I can't seem to figure out what to do. Law of cosines won't work because I don't know the angle across from $x$ . Those two angles are the same, but I don't know how to make that useful. Help appreciated here for a geometry noob! Thanks a ton I really appreciate the help.","['triangles', 'geometry']"
4415749,"For $n \geq 2$, show that $\sum_{r = 1}^{n} r \sqrt{\binom{n}{r}} < \sqrt{2 ^ {n - 1} n ^ 3}$","Good day, Can someone help me with giving hints for this problem: Show that for $n \geq 2, n \in \mathbb{Z}$ , $$\sum_{r = 1}^{n} r \sqrt{\binom{n}{r}} < \sqrt{2 ^ {n - 1} n ^ 3}$$ I tried $$\sum_{r = 1}^{n} r \sqrt{\binom{n}{r}} = \sum_{r = 1}^{n} \sqrt{r ^ 2\binom{n}{r}}$$ $$= \sqrt{n} \sum_{r = 1}^{n} \sqrt{r\binom{n - 1}{r - 1}}$$ So, we need to prove $$\sum_{r = 1}^{n} \sqrt{r\binom{n - 1}{r - 1}} < n \sqrt{2 ^ {n - 1}}$$ This reminds me of $$\sum_{r = 0}^{n} (r + 1) \binom{n}{r} = n 2 ^ {n - 1} + 2 ^ n = 2 ^ {n - 1}(n + 2)$$ but I can't see how to use it. Also, the only complex math inequality I know is AM-GM, so it's quite possible that it is my theoretical knowledge that is lacking. So, if a well-known inequality is used, please also mention it. Thanks","['binomial-coefficients', 'combinatorics', 'sequences-and-series', 'inequality', 'binomial-theorem']"
4415750,How to find a bend curve in 2D.,"I am looking for a solution on how to find the (approxmiate) shape when bending a rigid-flex circuit board. Please see the abstract sketch below. I have two solid objects ( $A$ , $B$ ) which are connected by a thin and flexible but non-stretchable strip of material. Given that I know (i) the position snf rotation of the fixed parts $A$ and $B$ in 2D space and (ii) the length $d$ and thickness $w$ of the flexible connecting strip how can I calculate the shape of the flexible strip when it is bent? Any help would be highly appreciated, approximate solutions (which may assume $w=0$ ) are fine.",['geometry']
4415782,Extending an automorphism of the upper-half plane to the whole Riemann sphere,"We know the automorphism group of the upper-half plane $\mathbb{H} \subset \mathbb{C}$ is given by $\Big\{ \frac{az+b}{cz+d} \quad \big|\quad a,b,c,d \in \mathbb{R} \quad \text{and } ad-bc>0 \Big\}$ . This is usually proved in textbooks by mapping this set to $\text{Aut}(\mathbb{D})$ and then using our explicit knowledge of $\text{Aut}(\mathbb{D})$ . I was trying to prove this result using the fact that $\text{Aut}(\mathbb{C}_{\infty})=\Big\{ \frac{az+b}{cz+d} \quad \big| \quad a,b,c,d \in \mathbb{C} \quad \text{and } ad-bc \neq 0 \Big \}$ , where $\mathbb{C}_{\infty}$ is the Riemann sphere. The idea is: It is clear that $\Big\{ \frac{az+b}{cz+d} \quad \big| \quad a,b,c,d \in \mathbb{R} \quad \text{and } ad-bc>0 \Big\}$ are in $\text{Aut}(\mathbb{H})$ . For the other way, let $\varphi \in \text{Aut}({\mathbb{H}})$ .  Suppose we can extend $\varphi$ to an automorphism of $\mathbb{C}_{\infty}$ and call this extension $\Phi$ . Now, \begin{equation}
\Phi(z)=\frac{az+b}{cz+d} \quad a,b,c,d \in \mathbb{C} \quad \text{and } ad-bc \neq 0 
\end{equation} As $\Phi \big| _{\mathbb{H}}=\varphi \in \text{Aut}(\mathbb{H})$ , we must have $\Phi(\mathbb{R}) \subseteq \mathbb{R}$ . So $a,b,c \text{ and }d$ are in fact real. Now \begin{equation}
\text{Im}(\Phi(z))= \text{Im}\left(\frac{az+b}{cz+d}\right)=\text{Im}\left(\frac{(az+b)(c\bar{z}+d)}{|cz+d|^2}\right)=\text{Im}\left(\frac{adz+bc\bar{z}}{|cz+d|^2}\right)=\frac{(ad-bc)y}{|cz+d|^2}
\end{equation} where $y=\text{Im}(z)$ . We need $\text{Im}(\Phi(z))>0$ whenever $\text{Im}(z)>0$ . Hence, $ad-bc>0$ . But it is not at all clear to me how to extend $\varphi$ to an automorphism of $\mathbb{C}_{\infty}$ . I know it can be continuously extended to $\mathbb{R}$ and then to the whole of $\mathbb{C}$ by a reflection but this is not enough. Any help would be appreciated.","['complex-analysis', 'riemann-surfaces', 'mobius-transformation']"
4415821,What distribution over X yields uniformly distributed distances between elements of X?,"I'm curious if you can find a probability distribution over $\mathcal{X} \subseteq \mathbb{R}^d$ , such that the random variable $D = d(X,Y)$ is uniformly distributed over the interval $[0, diam(\mathcal{X})]$ , where $X,Y$ are i.i.d. If this is even possible, what would that distribution look like? Are there constraints on $\mathcal{X}$ that make this possible/impossible?","['probability-theory', 'probability', 'random-variables']"
4415849,Proof that Mellin transform of random variable determines distribution,"For $X$ a nonnegative real random variable, define the Mellin transform of the distribution $\mathbb P_X$ to be the function $m_X : [0,\infty) \to [0,\infty]$ given by $$
m_X(s) = \mathbb E[X^s]
$$ I'm trying to prove the following: Theorem. Assume there is an $\epsilon_0 > 0$ with $m_X(\epsilon_0) < \infty$ . Then for any $\epsilon > 0$ , the distribution $\mathbb P_X$ is characterized by the values $m_X(s)$ over $s \in [0,\epsilon]$ . My reference text suggests proving this result in the following steps: Conclude the statement for a random variable $X$ with a continuous density. For $\sigma > 0$ , let $Y_\sigma$ be uniformly distributed over $[1-\sigma, 1]$ and be independent of $X$ . Show that $XY_\sigma$ has a continuous density. Compute $m_{XY_\sigma}$ , and show that $m_{XY_\sigma} \to m_X$ for $\sigma \searrow 0$ . Show that $XY_\sigma$ converges in distribution to $X$ as $\sigma\searrow 0$ . Step 1 is easy using the inverse Mellin transform law for continuous functions.
Step 2 I'm having some trouble with. Actually I don't think it's true as stated; suppose $X = 1$ almost surely. Then $\mathbb P_{XY_\sigma} = \mathbb P_{Y_\sigma}$ is the normalized Lebesgue measure on $[1-\sigma, 1]$ , which does not have a continuous density with respect to Lebesgue measure on $[0,\infty)$ . Question 1. For which $Y_\sigma$ independent of $X$ and for which $Y_\sigma$ converges in distribution to $1$ do we have that $XY_\sigma$ has a continuous density function? I tried showing Step 2 assuming instead that the distribution $Y_\sigma$ has a continuous density given by $f_\sigma$ and that the distribution $\mathbb P_{Y_\sigma}$ converges weakly to the Dirac measure at $1$ . Even then, I'm having trouble showing the density would be continuous: by independence of $X$ and $Y_\sigma$ , the density of $XY_\sigma$ would be given by \begin{align*}
\frac{d}{dx} \mathbb P[XY_{\sigma} \leq x] = \frac{d}{dx} \int_{\mathbb R} \mathbb P[X \leq x/y | Y_\sigma = y] f_\sigma(y) \, dy = \frac{d}{dx} \int_0^\infty \mathbb P[X \leq x/y] f_\sigma(y) \, dy
\end{align*} but without knowing if the CDF of $X$ is differentiable, I'm not convinced we can easily compute this derivative. Assuming we could show $XY_\sigma$ has a continuous density, Steps 3 and 4 should be straightforward with something like dominated convergence and the Portemanteau theorem. But if we know the values of $m_X(s)$ , without knowing anything about the continuity of the Mellin transform itself, I'm not seeing how to conclude that $\mathbb P_X$ is determined by $m_X$ . Question 2. Assuming $m_{XY_\sigma} \to m_X$ pointwise as $\sigma \to 0$ and that $XY_{\sigma} \to X$ in distribution, given that the distributions $\mathbb P_{XY_\sigma}$ are determined by the functions $m_{XY_\sigma}$ , how do we conclude that $\mathbb P_X$ is determined by $m_X$ ? Any ideas? I'd also accept a reference to the Mellin transform for random variables as an answer.","['mellin-transform', 'random-variables', 'probability-theory', 'reference-request']"
4415850,Convergence of $M$-estimators when the argmin is not unique,"Let $(X_i)_{1\le i\le n}$ be i.i.d. random variables taking values in a compact set $\mathcal X\subseteq \mathbb R^d$ , and let $\mathcal P_n = \mathcal P_n(\cdot\mid X_1,\ldots,X_n)$ and $\mathcal P$ be real-valued objective functions defined on a compact set $\Theta\subseteq \mathbb R^p$ , such that the following holds : $$\mathbb P\left(\sup_{\theta\in\Theta}|\mathcal P_n(\theta) - \mathcal P(\theta)|>\epsilon\right)=O(a_n) $$ Where $a_n$ is a deterministic sequence that goes to zero when $n\to\infty$ . Now let $\hat \theta_n \in \arg\min_{\theta\in\Theta} \mathcal P_n(\theta)$ . My question is : can the following relation hold for some $\theta^*\in\arg\min_{\theta\in\Theta} \mathcal P(\theta)$ ? : $$\mathbb P\left(\|\hat\theta_n - \theta^*\|_2 >\epsilon\right) = O(b_n) $$ Where $b_n$ is a deterministic sequence that goes to zero when $n\to\infty$ , which can (ideally) be expressed as $g(a_n)$ with $g$ a positive function that vanishes at $0$ . For the application I have in mind, I am thinking of Empirical Risk Minimization, so the maps $\mathcal P_n$ and $\mathcal P$ have the following expressions : $$\mathcal P_n(\theta) :=\frac 1 n\sum_{i=1}^{n} L(X_i,\theta),\quad \mathcal P(\theta) = \mathbb E[L(X_1,\theta)] $$ $L(x,\cdot)$ can be assumed to be Lipschitz and differentiable, but not convex. (I'd still be interested in a positive result with proof in the convex case though, or any other setting really.) My thoughts : The difficulty lies in the fact that $\arg\min_{\theta\in\Theta} \mathcal P(\theta)$ is not reduced to a singleton. Indeed, if that were the case, it is shown in chapter 5 of Van der Vaart's Asymptotic Statistics (1996) that $\sqrt n(\hat\theta_n - \theta^*)$ would be asymptotically normally distributed (up to additional assumptions). I would thus like to know if something similar could be said (again, up to additional assumptions) in the case where the argmin has multiple elements. Maybe in this case one should consider $\text{dist}(\hat\theta_n,\arg\min\mathcal P) $ and show that it is asymptotically normal too, but I don't really see how to adapt Van der Vaart's argument in that case.","['statistics', 'machine-learning', 'optimization', 'convergence-divergence', 'probability-theory']"
4415870,Bayesian analysis for multiple unknowns,"I've encountered this problem but my stats is rusty and I've been having trouble formalizing it enough to tackle it. I have something like a family tree where each node in the tree has two parents and can be one of two parents for some other node(s) (or not if it's a leaf node).  Each node is either red or black and inherited its color from one of its parents, chosen randomly with equal probability of being from either parent. Each node has a prior probability for being red (based on some educated guesses).  I'm then told the color of a certain subset of nodes.  I want to update my tree with posterior probabilities given the new information.  That is, the nodes I'm given have their prosterior probabilities to be red set to either 1 or 0 and the unknown node probabilities are updated to incorporate the new information. As an example, this tree has some known nodes (the ones colored red or black) and some unknown nodes (the blank ones).  I want to update the probabilities of the unknown nodes given the known nodes. This feels like an obvious problem for Bayesian analysis, and it's very similar to something like this , but the simultaneous nature of updating the posterior probabilities for all nodes in the tree is confusing me and I haven't figured out how to set up the update.","['statistical-inference', 'statistics', 'bayesian']"
4415907,Evaluate $\int_{-\infty}^{\infty}\frac{\sinh\left(y\sqrt{\alpha^2-\omega^2}\right)}{\sinh\left(H\sqrt{\alpha^2-\omega^2}\right)}e^{i\alpha x}d\alpha$,"I need to evaluate the Fourier inverse integral $\displaystyle \int_{-\infty}^{\infty}\frac{\sinh\left(y\sqrt{\alpha^2-\omega^2}\right)}{\sinh\left(H\sqrt{\alpha^2-\omega^2}\right)}e^{i\alpha x}d\alpha \tag*{}$ which arose while solving a PDE. Here, $H>0,x\in\mathbb{R},y\in[0,H]$ . The domain of $\omega$ was not given in the original problem, but I am going to assume $\omega>0$ for simplicity. The problem asks us to introduce proper branch cuts for the square root function before evaluating the integral. For reference, this is the original question . My attempt up until I get the integral is also shown in the link. My Attempt The branch points of the square root functions are at $\alpha =\pm\omega$ . So, I considered the following branch cuts and contours. (Here $\omega$ has an absolute value, but you can ignore it and assume $|\omega|=\omega$ ) We, firstly, need to find the poles of the integrand in the upper half-plane. Those are given by the equation $H\sqrt{\alpha^2-\omega^2}=n\pi i\tag*{}$ Solving this, we obtain $\displaystyle \alpha =\pm\sqrt{\omega^2-\frac{n^2\pi^2}{H^2}}\tag*{} $ where $n=0,1,2,\cdots$ . The problem is that some of those poles are on the branch cuts depending on the parameters. I have been told this is not permissible, so I am not sure how to proceed. Edit: The statement that ""some of those poles are on the branch cuts"" is not correct. It is on the real axis but between $[-\omega,\omega]$ . Edit2: Tables of Fourier Transforms and Fourier Transforms of Distributions by Fritz Oberhettinger states(p37,7.48) that if $a<b$ , we have $\displaystyle \int_{0}^{\infty} \frac{\sinh{(a\sqrt{k^2+x^2}})}{\sinh{(b\sqrt{k^2+x^2}})}\cos{xy}dx = -\pi b^{-1} \sum_{n=0}^{\infty}(-1)^nc_n\sin{(ac_n)}v_n^{-1}e^{-yv_n}$ where $c_n=n\pi/b$ , $v_n = (k^2+c_n^2)^{1/2}$ . I would guess our integral would have a similar form.","['complex-analysis', 'fourier-transform', 'branch-cuts']"
4415910,Is there a simple lower bound or approximation for the Bell numbers?,"I'm not quite certain what descriptors to use to describe the solution I'm looking for, but is there an approximation or useful lower bound for the Bell numbers, for which the amount of terms used in the calculation does not grow with $n$ ? The ordered Bell numbers have this approximation: $$a(n)\approx\dfrac{n!}{2(log2)^{n+1}}$$ which is nice and tidy, and allows one to compute the ordered Bell number $n$ to a ballpark of order of magnitude, which is useful for my purpose. I mostly wanted to reason about the difference in the order of magnitude for search spaces for a problem where the solution is either a single partition or single ordered partition of a set of size $n$ . This is, of course, the difference between the ordered Bell numbers and the Bell numbers. I realize now that it is probably easier and more accurate to simply enumerate the sequences, and their difference, over the domain in which I expect the $n$ might be for my problem ( $1\le n\le 50$ ). However, I am still curious about how the above approximation was derived, and if the same derivation would be impossible for the Bell numbers. I've read what I could find and partially understand about the subject, but it seems most lower bounds or approximations introduce either a number of terms that grows with $n$ , or functions or ideas whose approximation themselves aren't immediately clear to me, or may involve an expansion of terms as well. e.g. this approximation with the Lambert W function: $$B_n\sim \dfrac{1}{\sqrt{n}}\bigg(\dfrac{n}{W(n)}\bigg)^{n+\frac{1}{2}}\text{exp}\bigg(\dfrac{n}{W(n)}-n-1\bigg)$$ It seems to be no less trivial for computation than any other exact form, or Dobiński's formula for that matter. Below I've made a loose attempt at using Dobiński's reduced formula to derive an expression that meets my needs, but I haven't been able to quite get there, and I might have made some errors along the way. $$B_n = \Bigg\lceil\dfrac{1}{e}\sum_{k=0}^{K-1}\dfrac{k^n}{k!}\Bigg\rceil, \quad \dfrac{K^n}{K!} \le 1$$ $$0\lt B_n-\dfrac{1}{e}\sum_{k=0}^{K-1}\dfrac{k^n}{k!}\lt1$$ $$\dfrac{1}{e}\sum_{k=0}^{K-1}\dfrac{k^n}{k!} \lt B_n$$ At this point it seems we could choose some expression for $K$ such that the inequality holds, and only take the largest term of the sum as an approximation to have some bound. But I'm not sure where to go from here, or if the bound would be useful for my purposes. $K=2n$ satisfies $\dfrac{K^n}{K!} \le 1$ , but in that case the summation doesn't make sense for $k=0$ , so we'll assume the inequality doesn't hold for $n=0$ and push forward. Subbing $2n$ for $K$ we get the inequality: $$\dfrac{1}{e}\sum_{k=0}^{2n-1}\dfrac{k^n}{k!} \lt B_n, \quad \text{for } n \ge 1$$ But now on further examination, the value of $k$ for which $\dfrac{k^n}{k!}$ is the largest for a given $n$ is not obvious to me. Observing the graph of $y=\dfrac{x^n}{x!}$ and varying $n$ leads me to believe that finding the root of $k$ for a given $n$ , for the following partial derivative will allow us to find the largest term(s). $$\dfrac{\partial}{\partial k}\dfrac{ k^n}{k!}=0, \quad k,n \gt 0$$ But I think this is as far as I can get with my current knowledge. Once an expression for that root is found in terms of $n$ , you could round it to get (what I would imagine) is the largest term in the summation, or get two terms by applying the floor and the ceiling function to the expression, getting you the largest two terms, and a hopefully simple expression for the lower bound. But all of the solutions to the above equation seem to involve the inverse digamma function, or something I'm equally unsure of how to approximate simply. Although it seems like the root is able to be approximated by some software.","['bell-numbers', 'combinatorics']"
4415922,Why are Sylow Theorems and Sylow subgroups significant?,"If one read's Gallian's Abstract Algebra book then they would find that the chapter for Sylow Theorem's is quite hyped up. However, I am unable to understand the big picture of why Sylow subgroups and sylow theorems are important in group theory as a whole. Could some explain the big picture in simple terms?","['group-theory', 'sylow-theory', 'soft-question']"
4415935,Approximating CDF for an aggregate loss model using FFT,"For a class assignment, I was asked to calculate the VaR at levels 95% and 99.9% for an aggregate loss random variable $S=\sum_{i=1}^{N}{X_i}$ , where $N \sim \mathrm{NB}(r=5,\beta=\frac{1}{5})$ , and $X_i \overset{iid}{\sim} \exp{(\theta=1)}$ . We were to do this by using an FFT on a discretization of the continuous RV $X$ . The way to do this is by choosing a grid width $h$ and an integer $n$ such that $r:=2^n$ is the number of elements to discretize $X$ on, discretizing $X$ and calculating the probabilities of being in equally spaced intervals of width $h$ , applying the FFT to the discretized $X$ , applying the PGF of $N$ to the elements of the Fourier-transformed $X$ , applying the inverse FFT to this vector. The resulting vector should be an approximation for the probability masses of each such interval for $S$ . Below I attach my Python code. I know from previous methods that the 95% VaR ought to be ~4 and the 99.9% VaR ought to be ~10. But my code returns nonsensical results. Generally speaking, my index where the ECDF reaches levels $>0.95$ is way too late, and even after hours of debugging I have not managed to find where I am going wrong. The probability generating function for this parametrization of a negative binomial distribution is $P(z)=[1-\beta(z-1)]^{-r}$ . I hope that this is the correct place to ask this question, as I find it is mostly math- and only minorly programming related. import numpy as np
from scipy.stats import expon
from scipy.fft import fft, ifft

r, beta, theta = 5, .2, 1
var_levels = [.95, .999]


def discretize_X(h: float, m: int):
    X = expon(scale=theta)
    f_X = [X.cdf(h / 2),
           *[X.cdf(j * h + h / 2) - X.cdf(j * h - h / 2) for j in range(1, m - 1)],
           X.sf((m - 1) * h - h / 2)]
    return f_X


# Probability generating function of N ~ NB(r, beta)
def PGF(z: [float, complex]):
    return (1 - beta * (z - 1)) ** (-r)


h = 1e-2
n = 10
r = 2 ** n

VaRs, TVaRs = [], []

# discretize X with (r-1) cells of width h and one final cell with the survival function at h*(r-1)
f_X = discretize_X(h, r)
phi_vec = fft(f_X)
f_tilde_vec_fft = np.array([PGF(phi) for phi in phi_vec])
f_S = np.real(ifft(f_tilde_vec_fft))
ecdf_S = np.cumsum(f_S)  # calc cumsum to get ECDF

for p in var_levels:
    var_idx = np.where(ecdf_S >= p)[0][0]  # get lowest index where ecdf_S >= p
    print(""p ="", p, ""\nVaR idx:"", var_idx)
    var = h * var_idx  # VaR should be this index times the cell width
    print(""VaR:"", var)
    tvar = 1 / (1 - p) * np.sum(f_S[var_idx:] * np.array([i * h for i in range(var_idx, r)]))  # TVaR should be each cell's probability times the value inside that cell

    VaRs.append(var)
    TVaRs.append(tvar)

return VaRs, TVaRs ```","['actuarial-science', 'statistics', 'python', 'stochastic-analysis', 'random-variables']"
4415960,Determining $\frac{\pi^2}{4^22!}-\frac{\pi^4}{4^44!}+\frac{\pi^6}{4^66!}-\frac{\pi^8}{4^88!}+\cdots$,"I want to determine the sum of the series $$\frac{\pi^2}{4^22!}-\frac{\pi^4}{4^44!}+\frac{\pi^6}{4^66!}-\frac{\pi^8}{4^88!}+\cdots$$ What I am trying to do is to consider that $$\cos(x)=1-\frac{x^2}{2!}+\frac{x^4}{4!}-\frac{x^6}{6!}+...$$ then $$-\cos(x)+1=\frac{x^2}{2!}-\frac{x^4}{4!}+\frac{x^6}{6!}-...$$ Up to this point the expression resembles the one I am looking for, however I have not been able to find the final result, any help? thanks!","['power-series', 'trigonometry', 'taylor-expansion']"
4415968,Solving for $\cos(\frac{\pi}{x}) = \frac{x - 2}{x + 2}$,"As the question says, I was wondering how you'd go about solving the equation \begin{align*} 
\cos(\frac{\pi}{x}) = \frac{x - 2}{x + 2}
\end{align*} for $x$ . I've tried various methods and I'm feeling kind of dumb, cause I'm not sure where I'm messing up and I'm sure it's something obvious! For starters, I set up a triangle like this: https://i.sstatic.net/sODTm.png since $\cos(x) = \frac{adjacent}{hypotenuse}$ . Then I solved for the missing angle and tried using Law of Sines, but that just resulted in me getting \begin{align*}
\frac{1}{x + 2} = \frac{1}{x + 2}
\end{align*} which was a major facepalm moment for me, because of course. Then I tried setting \begin{align*}
-1 \le \frac{n-2}{n+2} \le 1
\end{align*} since $cos(x)$ is between -1 and 1. That resulted in me getting two answers, \begin{align*}
0 \le x, 0 \le 4 
\end{align*} Then I tried to solve \begin{align*}
0 \le \frac{\pi}{x} \le \pi
\end{align*} and \begin{align*}
\pi \le \frac{\pi}{x} \le 2\pi
\end{align*} using the same logic as the above. Then, I tried just using algebra: \begin{align*}
\cos(\frac{\pi}{x}) = \frac{x - 2}{x + 2} \\
(x + 2)\cos(\frac{\pi}{x}) = (x - 2) \\
x\cos(\frac{\pi}{x}) + 2\cos(\frac{\pi}{x}) = (x - 2) \\
x\cos(\frac{\pi}{x}) - x = -2 - 2\cos(\frac{\pi}{x}) \\
x ( \cos(\frac{\pi}{x}) - 1) = -2(1 - \cos(\frac{\pi}{x}))\\
-\frac{x}{2} = \frac {1 - \cos(\frac{\pi}{x}) }{ \cos(\frac{\pi}{x}) - 1 }\\
-\frac{x}{2} = \frac {-\cos(\frac{\pi}{x}) + 1 }{ \cos(\frac{\pi}{x}) - 1 }\\
-\frac{x}{2} = \frac {(-1)(\cos(\frac{\pi}{x}) - 1) }{ \cos(\frac{\pi}{x}) - 1 }\\
-\frac{x}{2} = -1\\
-x = -2\\
x = 2
\end{align*} But that solution, I realized, was identifiable by inspection (I facepalmed again). Looking at the graph on Desmos, I saw a bunch of solutions (including the inspection solution of $x = 2$ ). https://i.sstatic.net/eAZUS.png How would I find out those solutions? I can't think of any way to do it and I'm feeling a bit silly, haha. Any help would be appreciated! Apologies about the images, I don't have enough reputation to post them!","['trigonometry', 'recreational-mathematics']"
4416098,Maximum error bound for ODE,"For $f\in C([0,T]×\mathbb{R})$ , let $|f(t,x)-f(t,y)|\leq L|x-y|$ for a $L\geq0$ . Also suppose we have a differential equation \begin{align}
y'(t) &= f(t,y(t))\\
y(0)&=y_0
\end{align} with $y_1,\dots, y_N$ the solutions given by the Forward Euler method. So far I found that a bound for the maximum error is given by $$\max_{0\leq n \leq N} e_n \leq \frac{M}{2L}(e^{LT}-1)h$$ where $M=\max_{0\leq t \leq T}|y''(\tau)|$ for a $\tau\in[0,T]$ , and $h=T/N$ the step size. My question is: how do I find this bound for the following initial value problem: \begin{align}
y'(t)&=\arctan(y(t))\\
y(0)&=y_0?
\end{align} I am stuck at finding $M$ and $L$ . How could I find the maximum of the double derivative of $y(t)$ if I do not know anything about the function?","['numerical-methods', 'error-function', 'ordinary-differential-equations']"
4416136,"Recursion for the number of words of length $n$ over the alphabet $\{0, 1, 2\}$ such that there are neither $11$ nor $22$ blocks","Let $a_n$ be the length of words over an alphabet $\{0,1,2\}$ such that there are neither $11$ nor $22$ ""blocks"". I. e. $001020$ would be allowed, $001120$ wouldn't because we have a $11$ block. I have to show that $$a_n=2a_{n-1}+a_{n-2}$$ It looks like this recursion dissects the question into three cases (and adds them all up): The word of length $n$ ending with $0$ : We have: $\underbrace{\cdots \cdots \cdots }_{a_{n-1}}0$ The word of length $n$ ending with $1$ : We have $\cdots \cdots \cdots 1$ . Now we'll have to be careful though, since the digit that preceeds $1$ can't be $1$ . It has to be a $0$ or a $2$ . Do I have to distinguish these cases as well? Say, it is $0$ , then $\underbrace{\cdots \cdots \cdots}_{a_{n-2}} 01$ . Likewise if it was $2$ . How do I take into account this ambiguity? The word of length $n$ ending with $2$ is essentially the same as the one ending with $1$ . It would make sense to me if it'd be $a_n=a_{n-1}+2\cdot a_{n-2}$ .","['recursion', 'combinatorics', 'recurrence-relations', 'sequences-and-series']"
4416162,"Normal bounded operator in Hilbert space, whose spectrum is real, is self-adjoint","Let $T$ be a bounded normal operator in Hilbert space such that the spectrum $\sigma(T)$ is contained in the real axis. By the Gelfand-Naimark theorem for commutative $C^*$ -algebras the $C^*$ -algebra generated by $T$ and $I$ is isometrically isomorphic to $C(\sigma(T)),$ the algebra of complex valued  continuous functions on $\sigma(T)\subset \mathbb{R}.$ The operator $T$ corresponds to multiplication by $x$ in $C(\sigma(T)),$ therefore $T$ is self-adjoint . I would like to prove that fact  in a straightforward way, but I could not come up with any idea. When $T$ is a compact operator the proof is relatively easy. Assume by contradiction that $T^*-T\neq 0.$ Then one of the numbers $\lambda:=\pm{1\over 2}\|T^*-T\|\neq 0$ is the eigenvalue of the self-adjoint operator $B:={i\over 2}(T^*-T).$ Let $V_\lambda$ denote the eigenspace of the operator $B$ corresponding to $\lambda.$ As $T$ and $T^*$ commute with $B,$ the subspace $V_\lambda$ is invariant for $A={1\over 2}(T+T^*).$ The operator $T=A+iB$ restricted to $V_\lambda$ is of the form $A+i\lambda I.$ Therefore $\sigma(T)\subsetneq \mathbb{R},$ which gives a contradiction.","['operator-theory', 'functional-analysis']"
4416307,Can I get the number of integer partitions into k parts using the stars and bars technique?,"I was learning integer partition of a number n into k parts. My initial solution was $\frac{\binom{n-1}{k-1}}{k}$ and later found out the actual formula is $p(n,k)=p(n-k,k) + p(n-1,k-1)$ Is my solution correct? $\binom{n-1}{k-1}$ is from the stars and bars problem - the number of k-tuples of positive integers whose sum is n and I divide by k because we are counting some more than we should. Here is an example. $p(5,3)=2$ $x_1+x_2+x_3=5$ where $x\in \mathbb{N}\setminus\{0\}$ $\binom{5-1}{3-1}=6$ because we count 1+1+3,1+3+1,3+1+1 as three different and same as 2+2+1,2+1+2,1+2+2 So that's why I divide by k, which brings us to $\frac{6}{3}=2=p(5,3)$ Of course this is only one example and not a proof and that's why I want to hear your opinion if it's a solution.","['combinatorics', 'discrete-mathematics']"
4416347,Why co-domain of a function needs to be defined beyond the function's range?,"I've recently watched this video and struggle to wrap my mind around difference between codomain and range of a function. I understand that range is a subset of the codomain, but at the same time if there is no any restrictions on how the codomain is defined, why it needs to be defined beyond the range? Is it author of a function who defines the codomain? And if so, what makes him define the codomain beyond the range of a function (as the author, he is supposed to know all possible restrictions). If it's not ""authors"" but rather for consumers of a function, who have to deduce the codomain, how can they even approximate the codomain without knowing entire range? And if they know the range, why again codomain needs to be defined beyond range?",['functions']
4416377,Modelling difference in stock prices after a Brownian motion,"Question Stocks $A$ and $B$ open on trading day at the same price. Let $X(t)$ denote the dollar amount by which stock $A$ 's price exceeds stock $B$ 's price when $100t\%$ of the trading day has elapsed. $X(t)\ \forall\ t \in [0, 1]$ is modelled as a Brownian motion process with $\mu = 0$ and $\sigma^2 = 0.3695$ . After $75\%$ of the trading day has elapsed, stock $A$ 's price is $39.75$ and stock $B$ 's price is $40.25$ . Find the probability that $X(1) \geq 0 $ . My working With $X(0) = 0$ and $\mu = 0$ , we have $$\begin{aligned}
X(t) & = X(0) + \mu t + \sigma W_t\\
& = \sigma W_t,
\end{aligned}$$ where $W_t \sim \mathcal{N}(0, t)$ .
We can also obtain the following quantities: $$\begin{aligned}
X(0.75) & = -\frac 1 2,\\
\mathbb{E}[X(0.75)] & = \mathbb{E}[X(1)]\\
& = 0,\\
Var[X(0.75)] & = 0.75\sigma^2,\\
Var[X(1)] & = \sigma^2,\\
\rho & = Corr[X(0.75), X(1)]\\
& = \frac {Cov[X(0.75), X(1)]} {\sqrt {\{Var[X(0.75)]\}\{Var[X(1)]\}}}\\
& = \frac {\sigma^2 Cov[W(0.75), W(1)]} {\sigma^2 \sqrt {0.75}}\\
& = \frac {\min\{0.75, 1\}} {\sqrt {0.75}}\\
& = \sqrt {0.75}.
\end{aligned}$$ Now, let $$X(1) \mid X(0.75) = -\frac 1 2 \sim \mathcal{N}(s, t),$$ where $$\begin{aligned}
s & = \mathbb{E}[X(1)] + \sqrt {\frac {Var[X(1)]} {Var[X(0.75)]}}(\rho)\{X(0.75) - \mathbb{E}[X(0.75)]\}\\
& = -\frac 1 2
\end{aligned}$$ and $$\begin{aligned}
t & = (1 - \rho^2)Var[X(1)]\\
& = \frac 1 4 \sigma^2.
\end{aligned}$$ With $\sigma^2 = 0.3695$ , $$X(1) \mid X(0.75) = -\frac 1 2 \sim \mathcal{N}\left(-\frac 1 2, \frac {739} {8000}\right).$$ $$\begin{aligned}
\therefore \mathbb{P}[X(1) \geq 0] & = 1 - \mathbb{P}[X(1) < 0]\\
& = 1 - \mathbb{P}\left(Z < \sqrt {\frac {2000} {739}}\right)\\
& = 0.04997
\end{aligned}$$ As I have just covered Brownian motion in class, I am wondering whether my answer is correct and in particular, whether my working makes sense. Any comments will be greatly appreciated :)","['stochastic-processes', 'statistics', 'finance', 'probability']"
4416395,Is the cycle graph of a group unique?,"I was perusing the cycle graphs for small groups on Wikipedia and something bothers me: is the cycle graph of a finite group actually unique (up to isomorphism)? For example, if there are any cyclic subgroups of order $5$ , the cycle graph is drawn by picking one primitive generating element $a$ , and drawing a $5$ -cycle in the graph between $e, a, a^2, a^3, a^4$ . But a priori, this means the graph will depend on the choice of $a$ . Can this result in multiple cycle graphs for the same group? It doesn't seem obvious that these different graphs would be isomorphic in general. Definition: A cycle graph of a finite group $G$ is a simple undirected graph defined as follows: first, the vertex set of the graph is taken to be the set of elements $g \in G$ . Then, for each maximal cyclic subgroup of $G$ (cyclic subgroup not fully contained in a larger cyclic subgroup), pick a generator $a$ of the subgroup, and draw undirected edges $e \to a \to a^2 \to a^3 \to \cdots \to a^{k-1} \to a^k = e$ (ignoring any duplicate edges), where $k$ is the order of the subgroup. My question is whether the cycle graph of $G$ is unique up to isomorphism, regardless of the choices of generator for each maximal cyclic subgroup. Notice that for the purposes of this question, the graph is completely unlabeled -- the original vertex labels (elements of the group) are ignored, and edges are not labeled with the cyclic subgroup they correspond to. Strangely, I can't find a previous thread on this: Do cycle graphs determine groups up to isomorphism? asks the converse question of whether the cycle graph uniquely determines the group; Chris Culter asks my question in the comments but is unanswered. How in general does one construct a cycle graph for a group? asks for how to construct the cycle graph, but the top answer suffers from the same problem that the choice of primitive element for a cycle is not unique. I also searched on Google Scholar. I found an interesting paper, The Cyclic Graph of a Finite Group (Ma, Wei, Zhong) , but it defines the cycle graph differently, where $x, y$ share an edge if $\langle x, y \rangle$ is cyclic. In this definition the graph is clearly unique. This also seems to me a much more sensible definition, but I don't have an example where Wikipedia's definition actually leads to ambiguity in the resulting graph, up to isomorphism.","['graph-theory', 'group-theory', 'finite-groups']"
4416432,Would you recommend Advanced Calculus: A geometric View by Callahan as a self-study book for multivariable analysis?,"I have background in calculus, linear algebra, single variable analysis, topology, ode and some abstract algebra. So I've decided to study multivariable analysis before/alongside Lee's smooth manifolds. But since I have had trouble with Rudin and Spivak, I have done some research to find different textbook. After some researching, I have narrowed down to 3 books: Zorich, Hubbard, and Callahan's book. However, I have some question regarding Callahan's book. Is the book rigorous enough to be used in a multivariable analysis course or should it be used as a supplement instead of main textbook? Thanks in advance.","['analysis', 'reference-request', 'real-analysis', 'multivariable-calculus', 'calculus']"
4416457,Decay of convolution with measure,"In order to get some estimates of certain integral my teacher has used the following lemma that, in his opinion it is trivial: Let us consider $\nu$ a radon measure  in $\mathbb{R}^2$ such that $\int_{\mathbb{R}^2}d\nu=0$ . If we define $F(x)=g*\nu(x)$ with $g(x)=\ln|x|$ , then there exists $C>0$ such that $|F(x)|$ deacays like $C\frac{1}{|x|}$ . Unfortunately, it doesn't seem so obvious to me. Does anyone know of any proof  a result like this?
I thought that I would get it by using Poincare's inequality, but it seems to me that it doesn't work. Thanks in advance.","['integration', 'measure-theory', 'real-analysis', 'functional-analysis', 'partial-differential-equations']"
4416516,Find the variance $Var[\int_0^t(aB_s+bs)^2dB_s]$,"How to find the variance $Var[\int_0^t(aB_s+bs)^2dB_s],$ where $B_s$ are standard Brownian motion. My thoughts: $Var[\int_0^t(aB_s+bs)^2dB_s] = E[(\int_0^t(aB_s+bs)^2dB_s)^2] - E[\int_0^t(aB_s+bs)^2dB_s]^2.$ For the first term, it seems that we need to use Ito's isometry. Then $E[(\int_0^t(aB_s+bs)^2dB_s)^2] = E[\int_0^t (aB_s+bs)^4ds].$ I don't know how to do the next step. Do I need to expand the integrand $(aB_s+bs)^4$ and calculate the integral term by term? If so,
How to calculate $\int_0^t s^i B_s^j dB_s$ ? (Let $f(t,x)=t^ix^j$ and then use ito's formula?) For the second term, do we just expand the integrand to calculate the integral and then calculate the expectation? Am I on the right track to solve this problem? Are there an easier way to calculate the variance?","['stochastic-integrals', 'stochastic-analysis', 'stochastic-processes', 'probability-theory', 'stochastic-calculus']"
4416637,$\overline{\mathcal{P}(A)} = \mathcal{P}(U)\setminus\mathcal{P}(A)$,"I've been trying to prove $\overline{\mathcal{P}(A)} = \mathcal{P}(U)\setminus\mathcal{P}(A)$ Here is what I tried: $$\overline{\mathcal{P}(A)} \subseteq \mathcal{P}(U)\setminus\mathcal{P}(A)$$ Let $x$ be an element such that: $$x\in \overline{\mathcal{P}(A)} \implies x \notin \mathcal{P}(A) \implies x \not\subseteq A$$ By that we can conclude by definition of Universal set that $x \subseteq U$ so that means $x \in \mathcal{P}(U)$ so by definition of ""set difference"": if $x \in\mathcal{P}(U)$ and $x \notin \mathcal{P}(A) \implies x\in (\mathcal{P}(U)\setminus \mathcal{P}(A) ).$ $$\mathcal{P}(U)\setminus \mathcal{P}(A) \subseteq \overline{\mathcal{P}(A)}$$ Let $x$ be element such that $x \in (\mathcal{P}(U)\setminus\mathcal{P}(A))$ so by definition of ""set difference"": $$x \in \mathcal{P}(U) \text { and } x \notin \mathcal{P}(A)$$ $$x\notin \mathcal{P}(A) \implies x\in\overline{\mathcal{P}(A)}$$ or everything completely wrong and there no such thing and its impossible to define $\overline{\mathcal{P}(A)}$ ?","['elementary-set-theory', 'discrete-mathematics']"
4416706,How to find the $2d$ cross-sections of the $n$-hypercube that are regular $2n$-gons?,"I'm currently trying to do something that sounds doable, but on which I've now been stuck for a while.
The idea is that, if you consider an $n$ -hypercube, the polygon found by doing a $2d$ cross-section of it can have between 3 and $2n$ edges, with a mean value at $4$ for random cuts. In particular, it seems that you should be able to find a regular $2n$ -gon for a family of  very specific cross-sections, in the same way that a cut of the $3d$ cube gives a regular hexagon if you do a central cut perpendicular to a long diagonal.
However, in general dimension $n$ , it is not obvious to me that there is a simple construction to find these sections: there is no obvious $n-2$ dimensional generalization of the long diagonal of the cube. My question is: is there a general way to find the $2d$ planes that contain these special sections for $n>3$ ?","['euclidean-geometry', 'analytic-geometry', 'geometry', 'polygons', 'cross-sections']"
4416712,"Computing the fiber over $(0,0)$ for the normalization $\operatorname{Spec} \Bbb C[t]\to\operatorname{Spec} \Bbb C[x,y]/(y^2-x^2(x+1))$","Let $R=\mathbb{C}[x,y]/(y^2 -x^2 (x+1))$ . Setting $t:=y/x$ , we can show that $R[t]=\mathbb{C}[t]$ and use it to conclude that $R[t]$ is the normalization of $R.$ Now consider the corresponding normalization map $\phi:\operatorname{Spec}(\mathbb{C}[t])\rightarrow \operatorname{Spec}(R)$ . My goal is to show that the fiber of $(x,y)\in \operatorname{Spec}(R)$ has exactly two points. My idea was to make use of the fact that there is a natural bijection between $\phi^{-1}((x,y))$ and $\operatorname{Spec}(k((x,y))\otimes_R R[t])$ , where $k((x,y)):=R_{(x,y)}/(x,y)_{(x,y)}$ is the residue field of $(x,y)$ . But I'm currently stuck on computing $\operatorname{Spec}(k((x,y))\otimes_R R[t])$ . So any hint/help will be extremely useful. Also, is there a different approach to prove the same result? Thanks in advance.","['maximal-and-prime-ideals', 'algebraic-geometry', 'ring-theory', 'abstract-algebra', 'commutative-algebra']"
4416738,Bell-Shaped functions parametrization,"While doing some exercises from a supplement to Rudin's ""Principles of Mathematical Analysis"" , I have stumbled upon the following interesting problem: Notice that $\frac{A}{(Bx+C)^2+1}$ and $Ae^{-(Bx+c)^2}$ are examples of classes of Ball-Shaped functions which depend on three real parameters each. Can there exist a class of Bell-Shaped functions that depends on more than three parameters? First of all I shall provide my definition of Bell-Shaped function: Definition : $f: R \rightarrow R$ is said to be a Bell-Shaped function iff: $f \in C^{+ \infty}(\Bbb R)$ ( $f$ is smooth) $\lim_{x \to +\infty} f(x) = \lim_{x \to -\infty} f(x) = 0$ $f^{(n)}$ has exactly n zeros (let them be denoted in ascending order by $\{z_n^i\}_{1 \leq i \leq n}$ ) MY ATTEMPT : I have tried using vector spaces, but I immediately realized that a class of same-form Bell-Shaped functions at most constitute an affine subspace (and I am not completely sure of that). Then I started finding necessary conditions for $f$ to be a Bell-Shaped function, e.g.: $\lim_{x \to +\infty} f^{(n)}(x) = \lim_{x \to -\infty} f^{(n)}(x) = 0$ $z_n^i$ is an inversion point for $f^{(n)}$ However, even this time i didn't find anything useful in order to solve the problem. Finally, I begun playing with the functions given as examples and i noticed that the parameters seemed to control specific elements of the graphs: $A$ defines the maximum height (in absolute value) reached by $f$ $B$ defines the width of the central Bell $C$ defines the position of the point of maximum height Therefore, I am now inclined to think that 3 is actually the maximum number of parameters, but I have no idea on how to prove that, or on how to rigorously define $A,B,C$ . As always, any form of help is highly appreciated!","['roots', 'real-analysis', 'functions', 'limits', 'derivatives']"
4416747,Is there a way to find the $n$th term of a Farey sequence?,"I was doing some reading on Farey Fractions and was curious if there is a method to find the $n$ th term in a particular Farey sequence? I know you could do this with a computer search, but at large denominator values this quickly becomes tedious. Here's a random example: Find the $81$ st term of the Farey sequence with denominator 1500.","['farey-sequences', 'fractions', 'discrete-mathematics', 'sequences-and-series']"
4416757,"What are the geometric, harmonic, and quadratic averages of a function?","In Mean of a function , they describe the arithmetic mean of a function and at the bottom of the article they said: There is also a harmonic average of functions and a quadratic average (or root mean square) of functions. My question is what is the form of these averages?","['average', 'multivariable-calculus', 'calculus', 'means']"
4416775,Combinatorial interpretation of a sum,I would like to know if there exists a way to interpret this sum by a combinatorial argument $$\sum _ { k = 0 } ^ { n } \frac { ( - 4 ) ^ { k } k ! } { ( 2 k + 1 ) ! ( n - k ) ! } = \frac { 1 } { ( 2 n + 1 ) \cdot n ! }$$ As of now I have tried coming up with some inclusion exclusion argument by multiplying the sum on the LHS by $(2n+1)n!$ but have failed in doing so. Would appreciate any guidance or help  towards a solution Edit: I found this identity from this mathematical reflections paper I came across https://drive.google.com/file/d/1Q_TdS1btxtMd-wkanMPGSHeXg3Wq2y-P/view?usp=drivesdk,"['summation', 'combinatorial-proofs', 'inclusion-exclusion', 'binomial-coefficients', 'combinatorics']"
4416802,Area calculation for transformed rectangle seems too small.,"I came upon this problem in Khan Academy precalculus, in the unit on matrices.  In the video , he shows this image of a rectangle and asks you to determine its area after being transformed by a matrix.  The determinant of the transformation, multiplied times the original area, should equal the transformed area. I followed the instructions and got his answer. $Area = 7 * 5 = 35$ $det=(5*8)-(9*4) = 4$ $Area' = 35 * 4 = 140$ But I didn't believe the answer, because the matrix seems to transform the unit vectors by a large factor.  So I reframed the problem to imagine the rectangle beginning with its corner on the origin. I defined a new vector for this rectangle, $A$ .  If I then apply the same transformation matrix ... $$ \begin{pmatrix}5 & 9\\\ 4 & 8\end{pmatrix}\begin{pmatrix}7\\\ 5\end{pmatrix}$$ $$7\begin{pmatrix}5 \\\ 4 \end{pmatrix}+5\begin{pmatrix}9 \\\ 8 \end{pmatrix}=\begin{pmatrix}35 \\\ 28 \end{pmatrix}+\begin{pmatrix}45 \\\ 40 \end{pmatrix}=\begin{pmatrix}80 \\\ 68 \end{pmatrix}$$ I get a rectangle of the dimensions $80*68=5440$ I'm certain that Khan is correct and I am wrong, but I don't understand why.  How can the area of the transformed image be so small when I think it should be very large?","['matrices', 'area', 'determinant', 'vectors']"
4416885,Can you generalise the Chinese Remainder Theorem to noncommutative rings without identity?,"Ultimately, my question is: does the following theorem hold? Let $I_1, ..., I_n$ be ideals of some ring $R$ , with $R = I_i + I_j$ for $1 \leq i < j \leq n$ . Then for any $r_1, ..., r_n \in R$ there exists $x \in R$ such that: $$\begin{align*}
x &\equiv r_1 \pmod{ I_1 }\\ &\vdots \\ x &\equiv r_n \pmod{ I_n}
\end{align*} $$ It's easy enough to show if two such solutions $x$ and $x'$ exist, then $x' \equiv x \pmod{I_1 \cap ... \cap I_n}$ . Moreover, this shows the ring homomorphism $\phi: R \rightarrow R / I_1 \times \cdots \times R / I_n$ , given by $r \mapsto \left ( r + I_1, \ldots, r + I_n \right )$ is surjective. Hence, with $\ker(\phi) = I_1 \cap \cdots \cap I_n$ , the first isomorphism theorem tells us: $$ R / \left ( I_1 \cap \cdots \cap I_n \right ) \cong R / I_1 \times \cdots \times R / I_n$$ A few more comments about this question: Theorem 1.11 of these lecture notes gives a sketch proof with the added condition $R^2 + I_i = R$ for $i = 1, ..., n$ . If $R$ contains a multiplicative identity, then $R^2 = R$ , and our requirement immediately holds. Is this added condition necessary for $n > 2$ ? Judson's Abstract Algebra textbook poses the $n=2$ case as an exercise with no added assumption about multiplicative identities etc... which is answered in this question .","['ring-theory', 'abstract-algebra', 'chinese-remainder-theorem', 'noncommutative-algebra', 'ideals']"
4416889,Ways to choose $4$ pieces from $4$ B's and $3$ H's and $2$ S's?,"One plays $4$ pieces out of $9$ . $4$ of them are (of type) B, $3$ are H and $2$ are S. How many ways the pieces can be chosen if there must be at least 1 of each type? I figure out 2 methods.
1 is laying out all possible scenarios:
1 2B, 1H, 1S
2 1B, 2H, 1S
3 1B, 1H, 2S That gives me $72$ . 2nd is choose one each first, then choose $1$ out of $6$ for the remaining i.e. $C(4,1) · C(3,1) · C(2,1) · C(6,1)$ . That gives me $143$ . 2nd method yields double the first. I think 2nd method may double counted something but I can't figure out. Can anyone help?","['combinations', 'combinatorics']"
4416891,Find all angles $\theta$ such that $x_1\cos(0)+x_2\cos(\theta)+x_3\cos(2\theta)+\dots + x_{n+1}\cos (n\theta)=0$,"Find all angles $\theta$ such that $\exists x_1,x_2,\dots x_{n+1}, n\in \mathbb{Z}$ such that $$x_1\cos(0)+x_2\cos(\theta)+x_3\cos(2\theta)+\dots + x_{n+1}\cos (n\theta)=0,\\x_1\sin(0)+x_2\sin(\theta)+x_3\sin(2\theta)+\dots + x_{n+1}\sin(n\theta)=0.$$ Here is my progress. If $\theta$ satisfies then $180-\theta$ satisfies too. Clearly, angles of the form $\frac{360}{r}$ satisfy ( simply consider $r$ regular polygon with external angle $\frac{360}{r}$ ). For example, $\cos(0)+ \cos(60)+\cos(120)+\cos(180)+\cos(240)+\cos(300)=0$ I also got $\theta= 109.47$ or $\cos(\theta)=-1/3 $ satisfying with $3cos(0)+2\cos(\theta)+3\cos(2\theta)=0.$","['trigonometry', 'algebra-precalculus', 'geometry', 'contest-math']"
4416900,Double integral of $1/(x^2+y^2)$ restricted to $x^2+y^2\leq2$ and $x\leq1$,"Find $$\iint_D \frac{1}{(x^2+y^2)^2}dA$$ where $$D = \left\{ (x,y): x^2 + y^2 \leq 2 \right\} \cap \left\{ (x,y): x \geq 1 \right\}$$ Because of the prevalence of $x^2+y^2$ terms here, I figured we would be using a change of variables to polar coordinates with $dA = rdrd\theta$ . However, I ran into trouble when finding the bounds of the integral. I know $r$ goes from $0$ to $\sqrt 2$ , but I got stumped when considering $\theta$ . Solving for $\theta$ using the substitution $x=r\cos\theta$ into $x\leq 1$ , I got $\theta =\arccos(\frac{1}{r})$ . This seems ok, but it resulted in an integral that is impossible to solve by hand (maybe not technically impossible, but clearly I did something wrong here). I also don't think using Cartesian coordinates would be the right approach, since the polar coordinate substitution results in very nice cancelling and easy integration.","['integration', 'multivariable-calculus', 'multiple-integral', 'bounds-of-integration']"
4416937,Closed form for definite integrals invovling Jacobi elliptic functions,"In a 1879 work, Glaisher proves the following closed forms $$\int_{0}^{K\left(k\right)}\log\left(\text{sn}\left(z;k\right)\right)dz=-\frac{1}{4}\pi K^{\prime}\left(k\right)-\frac{1}{2}K\left(k\right)\log\left(k\right)$$ $$\int_{0}^{K\left(k\right)}\log\left(\text{cn}\left(z;k\right)\right)dz=-\frac{1}{4}\pi K^{\prime}\left(k\right)+\frac{1}{2}K\left(k\right)\log\left(\frac{k}{k^{\prime}}\right)$$ $$\int_{0}^{K\left(k\right)}\log\left(\text{dn}\left(z;k\right)\right)dz=\frac{1}{2}K\left(k\right)\log\left(k^{\prime}\right)$$ where, $\text{sn}\left(z;k\right),\,\text{cn}\left(z;k\right),\,\text{dn}\left(z;k\right)$ are the Jacobi elliptic functions, $K(k)$ is the complete elliptic integral of the first kind and, as usual, $K^{\prime}(k)=K(k^{\prime})$ , where $k^{\prime}=\sqrt{1-k^{2}}.$ For the proof he use a product formula for the elliptic functions; I tried to understand what he did but the steps don't have many explanations and therefore I struggle to understand how to prove these identities. Question 1. How we can prove the previous identities? This is the link to the paper of Glaisher: https://royalsocietypublishing.org/doi/pdf/10.1098/rspl.1879.0056 I need to understand these identities because I would like to find a closed form for the following definite integrals: $$\int_{0}^{K\left(k\right)/2}\log\left(\text{sn}\left(z;k\right)\right)dz,\,\int_{0}^{K\left(k\right)/2}\log\left(\text{cn}\left(z;k\right)\right)dz,\,\int_{0}^{K\left(k\right)/2}\log\left(\text{dn}\left(z;k\right)\right)dz\tag{1}$$ Question 2. Is it possible to evaluate in a closed form (in the sense of Glaisher) the integrals in $(1)$ ? I tried some identites, like half argument formulas, hoping to fall back into one of the cases already considered by Glaisher but it seems that this approach does not work. Thank you.","['definite-integrals', 'real-analysis', 'elliptic-functions', 'closed-form', 'elliptic-integrals']"
4416991,Combinatorial argument why is the following below true?,"$\frac{12!}{2^6 \cdot 3! \cdot 3!} = {12 \choose 6} \cdot 5^2 \cdot 3^2 \cdot 1^2 $ I'm trying to formulate a combinatorial argument and have singled out a case of the two formulas above hoping to see if anyone can explain why the above is true. If so it may help me solve the argument for a general case. Any insight or tips would be much appreciated. Here is what I know so far from the left side of the equation: $\frac{12!}{2^6 \cdot 3! \cdot 3!}$ it seems like we are dispersing $12$ distinct objects to two bins of size $3$ but also dividing all the possible subsets of size $6$ (denoted by $2^6$ )? Don't know if that is true but this is what I'm inferring right now. And the right side seems to first pick out a subset of size $6$ , and a bit lost from there on what the $5^2 \cdot 3^2 \cdot 1^2$ denotes and how it can be interpreted with the subset of size $6$ using the product rule.","['combinatorics', 'combinatorial-proofs', 'discrete-mathematics']"
4417024,"Given $u(x,y) = e^x (x \cos y - y \sin y)$ for analytic $f(z) = u + iv$, find $v(x,y)$","Given $u(x,y) = e^x (x \cos y - y \sin y)$ for analytic $f(z) = u + iv$ , find $v(x,y)$ . I know that there is an answer to this question in For an analytic function $f(z)=u+iv$, if $u=e^x(x \cos y-y \sin x)$ find $v$. .
But it seems to me that approach used there is specific to the problem (the choice of the integration path). Is it? If not, could you exlain how the integration path is chosen?
If it is, could you exlain what the general approach is to solve such problems? I know, I should check if $u$ is harmonic, then I should find the harmonic conjugate of $u$ but I don't know what path to choose (usually I'd take the line connecting $(0,0)$ and $(x,y)$ . In this case it results in rather complicated integrals so I think there has to be a better path to integrate over. A clarification would be very helpful. Thanks in advance.","['complex-analysis', 'harmonic-functions', 'analytic-functions']"
4417062,A Formula of $E(X^n)$ [duplicate],"This question already has answers here : Expected value of $X^n$ (2 answers) Closed 2 years ago . The problem 1.1 on p.46 of Stochastic Processes, Sheldon M. Ross, The Second Edition said: If $X$ is nonnegative with distribution $F$ , then $$
E[X^n]=\int_0^\infty{nx^{n-1}\bar{F}(x)\mathrm{d}x}.
$$ Here $\bar{F}(x) := 1-F(x)$ . Here's my solution: \begin{equation}
\begin{split}
E(X^n)
&= \int_0^\infty{x^n\mathrm{d}F(x)}\\
&= -\int_0^\infty{x^n\mathrm{d}\bar{F}(x)}\quad(F(x)+\bar{F}(x)=1)\\
&= -\biggl.x^n\bar{F}(x)\biggr|_0^\infty+\int_0^\infty{\bar{F}(x)\mathrm{d}x^n}\\
&= -\biggl.x^n\bar{F}(x)\biggr|_0^\infty+\int_0^\infty{nx^{n-1}\bar{F}(x)\mathrm{d}x}\\.
\end{split}
\end{equation} I'm almost there, but it suffices, if all above are right, to show $$
\lim_{x\to\infty}{x^n\bar{F}(x)}=0,\tag{1}
$$ which I'm afraid is wrong. Can someone figure out my problem, or just show that (1) is actually right?","['stochastic-processes', 'probability-theory', 'probability']"
4417118,Find the negative logarithm of $(1+vt^a)^{-1}$,"I’m trying to find the logarithm of this function, but I think I have a mistake with my rules. What I did is: I want to take the $-\log(S(t))$ :- $S(t) = (1+vt^a)^{-1}, v,a>0$ . After applying the rules I got: $\log(1+vt^a)$ Is that correct? Or did I make a mistake? My calculations:","['calculus', 'discrete-mathematics', 'logarithms']"
4417128,"Show the limit $\lim_{\delta \to 0}\int_{\ln(1+\delta)}^{\delta} \frac{e^{-x}}{x}\,dx \to 0 $ in a proof of the Digamma function","I want to show that $$\lim_{\delta \to 0}\int_{\ln(1+\delta)}^{\delta} \frac{e^{-x}}{x}\,dx \to 0 \tag{1}$$ Intutitively I could take the limits before integration, then I would get $$\int_{0}^{0} \frac{e^{-x}}{x}\,dx=0 $$ My question is whether this procedure is valid, or a more rigorous proof should be provided in he lines of Edit : Being more careful in the calculations I could show the following estimate: $$
\begin{align*}
\lim_{\delta \to 0}\int_{\ln(1+\delta)}^{\delta} \frac{e^{-x}}{x}\,dx & \leq \left| \lim_{\delta \to 0}\int_{\ln(1+\delta)}^{\delta} \frac{e^{-x}}{x}\,dx \right|\\
& \leq  \lim_{\delta \to 0}\int_{\ln(1+\delta)}^{\delta}\left| \frac{e^{-x}}{x}\right|\,dx \\
& \leq  \lim_{\delta \to 0}\int_{\ln(1+\delta)}^{\delta} \frac{dx}{x} \\
&=\lim_{\delta \to 0} \, \ln(x)\Big|_{\ln(1+\delta)}^{\delta}\\
&=-\lim_{\delta \to 0} \, \ln\left(\frac{\ln(1+\delta)}{\delta}\right)\\
&=- \ln\left(\lim_{\delta \to 0}\frac{\ln(1+\delta)}{\delta}\right) & \text{by continuity of log}\\
&=- \ln\left(\lim_{\delta \to 0}\frac{1}{\delta}\sum_{n=1}^\infty \frac{(-1)^{n+1} \delta^n}{n}\right)\\
&=- \ln\left(\lim_{\delta \to 0}\sum_{n=1}^\infty \frac{(-1)^{n-1} \delta^{n-1}}{n}\right)\\
&=-\ln(1)\\
&=0
\end{align*}
$$ Motivation: The motivation behind this limit comes from a proof of an integral representation of the Digamma function. If we define the Gamma function by the integral $(2)$ below and the digamma function by $\psi(z)=\frac{z}{dz}\ln\left(\Gamma(z) \right)$ $$\Gamma(z)= \int_0^\infty e^{-t}t^{z-1}\,dt \qquad \operatorname{Re}(z)>0\tag{2}$$ Differentiating $(2)$ w.r. to $z$ we obtain: $$
\begin{align*}
\Gamma^\prime(z)&= \int_0^\infty e^{-t}t^{z-1} \ln(t)\,dt \qquad \operatorname{Re}(z)>0\\
&= \int_0^\infty e^{-t}t^{z-1} \left(\int_0^\infty \frac{e^{-x}-e^{-xt}}{x}\,dx \right)\,dt\\
&= \int_0^\infty \left(\int_0^\infty (e^{-x}-e^{-xt})e^{-t}t^{z-1}\,dt \right)\,\frac{dx}{x}\\
&= \int_0^\infty \left(e^{-x}\int_0^\infty e^{-t}t^{z-1}\,dt-\int_0^\infty e^{-t(1+x)}t^{z-1}\,dt \right)\,\frac{dx}{x}\\
&= \int_0^\infty \left(e^{-x}\Gamma(z)-\frac{1}{(1+x)^z}\int_0^\infty e^{-t}t^{z-1}\,dt \right)\,\frac{dx}{x}\\
&= \int_0^\infty \left(e^{-x}\Gamma(z)-\frac{1}{(1+x)^z}\Gamma(z)\right)\,\frac{dx}{x}\\
&=\Gamma(z) \int_0^\infty \left(e^{-x}-\frac{1}{(1+x)^z}\right)\,\frac{dx}{x}\\
\end{align*}
$$ Therefore we have $$\psi(z)=\int_0^\infty \left(e^{-x}-\frac{1}{(1+x)^z}\right)\,\frac{dx}{x} \tag{3}$$ Than $$
\begin{align*}
\psi(z)&= \lim_{\delta \to 0}\int_{\delta}^\infty \left(e^{-x}-\frac{1}{(1+x)^z}\right)\,\frac{dx}{x}\\
&= \lim_{\delta \to 0}\left[\int_{\delta}^\infty \frac{e^{-x}}{x}\,dx-\int_{\delta}^\infty \frac{1}{x(1+x)^z}\,dx\right]\\
&= \lim_{\delta \to 0}\left[\int_{\delta}^\infty \frac{e^{-x}}{x}\,dx-\int_{\ln(1+\delta)}^\infty \frac{e^{-xz}}{1-e^{-x}}\,dx\right] & (x+1 \to e^{x})\\
&= \lim_{\delta \to 0}\left[\int_{\ln(1+\delta)}^\infty \frac{e^{-x}}{x}\,dx-\int_{\ln(1+\delta)}^\infty \frac{e^{-xz}}{1-e^{-x}}\,dx+\int_{\delta}^{\ln(1+\delta)} \frac{e^{-x}}{x}\,dx\right] \\
&= \lim_{\delta \to 0}\left[\int_{\ln(1+\delta)}^\infty \frac{e^{-x}}{x}- \frac{e^{-xz}}{1-e^{-x}}\,dx-\int_{\ln(1+\delta)}^{\delta} \frac{e^{-x}}{x}\,dx\right] \\
&=\int_{0}^\infty \frac{e^{-x}}{x}- \frac{e^{-xz}}{1-e^{-x}}\,dx, \qquad \operatorname{Re}(z)>0
\end{align*}
$$ Provided $(1)$ holds.","['integration', 'digamma-function', 'complex-analysis', 'gamma-function', 'limits']"
4417153,A quick proof for $\mbox{rank}(A + B) \leq \mbox{rank}(A) + \mbox{rank}(B)$,"A recent question in this forum led me to recall the linear algebra result: $$
\mbox{rank}(A + B) \leq \mbox{rank}(A) + \mbox{rank}(B), \ \ \left( A, B \in \mathbf{R}^{n \times n} \right)
$$ (I corrected my first posting after Ben Grossmann corrected it and cited an important inequality in linear algebra. I like to thank him first!) Define $U = \mbox{Range}(A)$ , $V = \mbox{Range}(B)$ . Then $U$ and $V$ are subspaces of $\mathbf{R}^n$ . Clearly, $\mbox{rank}(A) = \mbox{dim}(U)$ , $\mbox{rank}(B) = \mbox{dim}(V)$ . Also, $\mbox{rank}(A + B) \leq \mbox{dim}(U + V)$ . (Thanks to Ben Grossmann for correcting my original statement!) We know the theorem from linear algebra: $$
\mbox{dim}(U + V) = \mbox{dim}(U) + \mbox{dim}(V) - \mbox{dim}(U \cap V)
$$ which implies that $$
\mbox{dim}(U + V) \leq \mbox{dim}(U) + \mbox{dim}(V).
$$ Thus, $\mbox{rank}(A + B) \leq \mbox{dim}(U + V) \leq \mbox{rank}(A) + \mbox{rank}(B)$ .","['matrices', 'solution-verification', 'linear-algebra']"
4417155,Proving the identity of $\sum_{k = 0}^n{4n \choose 4k} = 2^{4n - 2} + (-1)^n2^{2n - 1}$ combinatorially,"I want to prove the following identity combinatorially: $$\sum_{k = 0}^n{4n \choose 4k} = 2^{4n - 2} + (-1)^n2^{2n - 1}$$ Here's my attempt so far: The left hand side is counting the number of teams among $4n$ people where the size of the team is a multiple of 4. Now let $A$ and $B$ be the last two people among these $4n$ people. Now let $T$ be a team such that $|T| = 4k$ . Then we have 4 cases: $A, B \in T$ $A, B \notin T$ Only $A \in T$ Only $B \in T$ Now I will construct all teams among the $4n - 2$ people after removing $A$ and $B$ (I don't care about their size) and convert them to a team of size $4k$ by adding a subset of $\{A, B\}$ to them. Clearly there are $2^{4n - 2}$ teams that don't contain $A$ or $B$ . Let $T$ be one of these teams. Then we have the following cases: $|T| = 4k$ . Teams of this kind correspond to teams of size $4k$ that don't contain $A$ and $B$ . $|T| = 4k + 1$ . Teams of this kind will just be discarded. $|T| = 4k + 2$ . Teams of this kind correspond to teams of size $4k'$ that contain both $A$ and $B$ . We will just add $A$ and $B$ to $T$ . $|T| = 4k + 3$ . Teams of this kind will be converted to teams of size $k'$ that contain either $A$ or $B$ . Then for each team of this kind, we should add $1$ to $2^{4n - 2}$ because we have 2 cases here. Either add $A$ or add $B$ . Based on these cases, we will have $2^{4n - 2} - \sum_{i = 0}^{n - 1}{4n - 2 \choose 4i + 1} + \sum_{i = 0}^{n - 2}{4n - 2 \choose 4i + 3}$ . If I've made no mistakes, then I should show that $- \sum_{i = 0}^{n - 1}{4n - 2 \choose 4i + 1} + \sum_{i = 0}^{n - 2}{4n - 2 \choose 4i + 3} = (-1)^n2^{2n - 1}$ but I don't know how to do this.","['combinatorics', 'combinatorial-proofs']"
4417180,Integral with respect to convolution of measure,"Let $E$ be a $\mathbb R$ -Banach space, $$\theta_n:E^n\to E\;,\;\;\;x\mapsto\sum_{i=1}^nx_i$$ for $n\in\mathbb N$ and $\lambda$ be a measure on $\mathcal B(E)$ . Remember that the $n$ -fold convolution of $\mu$ with itself is given by the pushforard measure $$\lambda^{\ast n}:=\theta_n\left(\lambda^{\otimes n}\right)$$ for $n\in\mathbb N_0$ ; where $\lambda^{\ast0}:=\delta_0$ . Assume $\lambda$ is translation invariant; i.e. $$\lambda(B-x)=\lambda(B)\;\;\;\text{for all }B\in\mathcal B(E)\tag1.$$ Let $F$ be a $\mathbb R$ -Banach space, $f\in\mathcal L^1(\lambda;F)$ and $n\in\mathbb N$ . I would like to derive a formula for $$\lambda^{\ast n}f=\int f\:{\rm d}\lambda^{\ast n}.$$ If $f=\sum_{i=1}^kf_i1_{B_i}$ for some $k\in\mathbb N$ and disjoint $B_1,\ldots,B_k\in\mathcal B(E)$ , then $$\lambda^{\ast n}(B_i)=\lambda(E)^{n-1}\lambda(B_i)\tag2$$ for all $i\in\{1,\ldots,k\}$ and hence $$\lambda^{\ast n}f=\lambda(E)^{n-1}\lambda f\tag2.$$ However, if we take $f=\operatorname{id}_E$ , then we easily see (even without assuming $(1)$ ) $$\lambda^{\ast n}f=\sum_{i=1}^n\int x_i\:\lambda^{\otimes n}({\rm d}x)=n\lambda(E)^{n-1}\int x\:\lambda({\rm d}x)\tag3.$$ So, it seems like my formula $(2)$ is off by a factor of $n$ . But why? Is $(2)$ wrong? It should be correct, since \begin{equation}\begin{split}\lambda^{\otimes2}(B)&=\lambda^{\otimes 2}(\{x\in E^2:x_1+x_2\in B\})\\&=\int\lambda({\rm d}x_1)\lambda(B-x_1)=\lambda(E)\lambda(B)\end{split}\tag4\end{equation} for all $B\in\mathcal B(E)$ . So, what am I missing?","['convolution', 'measure-theory', 'probability-theory']"
4417201,Double limit of the nth derivative of $f(x)=\exp(\sqrt{x})+\exp(-\sqrt{x})$,"I have tried to compute $$\lim\limits_{n\to \infty}\lim\limits_{x\searrow 0}f^{(n)}(x)$$ for $f:[0,\infty)\to \mathbb{R},~f(x)=\exp(\sqrt{x})+\exp(-\sqrt{x})$ . By noticing that $f$ and its derivatives satisfy $2f'(x)+4xf''(x)=f(x)$ and differentiating successively in this equality one can deduce a recurrence relation for $\lim\limits_{x\searrow 0}f^{(n)}(x)$ and thus obtain $\lim\limits_{x\searrow 0}f^{(n)}(x)=2\cdot\frac{n!}{(2n)!}$ , but only assuming that the limit $\lim\limits_{x\searrow 0}~xf^{(n)}(x)$ is $0$ for any $n\in \mathbb{N}$ , which I could not prove rigorously, but I ""feel"" that is true. Of course, any other idea for the computation of the above double limit is welcomed.","['limits', 'derivatives', 'recurrence-relations', 'real-analysis']"
4417214,On Cardinalities of Finite Sets,"In mathematics often when something ""doesn't make sense"", it turns out that it exists anyway. For example we now know there are numbers whose squares are negative. There are geometric sets with fractional dimension, etc. So if we have a finite set, ""common sense"" dictates that it's cardinality is some non-negative integer. I'm wondering if mathematicians work with sets with negative cardinalities or non-integral rational number cardinalities (how about irrational, complex?) etc. Or if to our knowledge these concepts do not make sense.",['set-theory']
4417229,"Given a normal variable $X\sim N(\mu,\sigma)$ and another variable $Y|X \sim N(f(X),g(X))$, under what conditions is $Y$ normal?","Given a normal variable $X\sim N(\mu,\sigma)$ and another variable $Y|X \sim N(f(X),g(X))$ , under what conditions is $Y$ normal? I believe that $g(X)$ has to be constant and $f(X)$ has to be a linear function $a+bX$ . I am struggling to prove it. $$
\Phi_Y(t)=\mathbb{E}[e^{\mathbf{i}tY}]\\
    =\mathbb{E}[\mathbb{E}[e^{\mathbf{i}tY}|X]]\\
    =\mathbb{E}[e^{\mathbf{i}tf(X)-\frac{1}{2}g(X)t^2}]$$ Now it is clear that if $f$ is linear and $g$ is constant then this reduces to the characteristic function of a normal distribution. But how would I go about proving that if $f,g$ are non-linear functions then this cannot be normal.","['probability-theory', 'normal-distribution']"
4417304,"Derive density of $Z=XY$, $X\sim U(0,1)$ and $Y\sim\mathcal N(0,1)$.","I have been stumped for a few days on this. I have two random variables $X\sim U(0,1)$ and $Y\sim\mathcal N(0,1)$ , which are independent. How can I get the density of $Z = XY$ ? I put $Z = XY, W = Y$ i.e. $X=Z/W, Y=W$ and achieved the Jacobian as $J={1\over|W|}$ ,
but I've got $f_{Z,W}(z,w)={1\over\sqrt{2\pi}|w|}{\exp(-w^2/2)}$ and I don't know how to integrate this w.r.t $w$ and get the (marginal) density of $Z$ .
Could you please help me with this problem? I tried partial integration too but it doesn't work.","['statistics', 'probability-distributions', 'change-of-variable', 'probability', 'density-function']"
4417359,Why is $\lim\limits_{n\to\infty} \prod\limits_{k=1}^n \left(1 + \frac{k+1}{n^2}\right) = \sqrt e$?,"Mathematica returns these somewhat striking (to me, at any rate) infinite product identities: $$\lim_{n\to\infty} \prod_{k=1}^n \left(1 + \frac{k+1}{n^2}\right) = \sqrt e$$ and $$\lim_{n\to\infty} \prod_{k=1}^n \left(1 + \frac{(k+1)^2}{n^3}\right) = \sqrt[3]{e}$$ With larger exponents, checking with software seems to take forever. I'm making a bit of a leap here, but these results suggest the following closed form for positive integer $p$ : $$\lim_{n\to\infty} \prod_{k=1}^n \left(1 + \frac{(k+1)^p}{n^{p+1}}\right) = e^{\frac1{p+1}}$$ How does one compute either or both of the first two limits? Does the conjecture hold? In the general case, we have $$\prod_{k=1}^n \left(1 + \frac{(k+1)^p}{n^{p+1}}\right) = \frac{n^{p+1}+(n+1)^p}{n^{p+1}+1} \prod_{k=1}^n \left(1 + \frac{k^p}{n^{p+1}}\right)$$ and the coefficient of the product converges to $1$ . From here, I rewrite the product as a sum of logarithms and attempt to rearrange terms to reveal a Riemann sum, but I have had no luck so far. For instance, in the case of $p=1$ , $$\begin{align*}
\lim_{n\to\infty} \prod_{k=1}^n \left(1 + \frac k{n^2}\right) &= \exp\left(\lim_{n\to\infty}\sum_{k=1}^n \ln\left(1 + \frac k{n^2}\right)\right)\\[1ex]
&= \exp\left(\lim_{n\to\infty}\sum_{k=1}^n \left(\ln\left(n+\frac kn\right) - \ln(n)\right)\right)\\[1ex]
&= \exp\left(\lim_{n\to\infty}\left(\sum_{k=1}^n \ln\left(n+\frac kn\right) - n\ln(n)\right)\right)\\[1ex]
&= \exp\left(\lim_{n\to\infty} n \left(\frac1n \sum_{k=1}^n \ln\left(n+\frac kn\right) - \ln(n)\right)\right)\\[1ex]
\end{align*}$$","['infinite-product', 'limits', 'exponential-function']"
4417411,Is it true that $P(A)=1$ if and only if $A=\Omega$?,"I was wondering if the following statement is true: $P(A)=1$ if and only if $A=\Omega$ . Of course we know that $P(\Omega)=1$ . But if $P(A)=1$ , does it necessarily mean $A=\Omega$ ? Why?","['analysis', 'probability-theory', 'probability']"
4417420,Minimize total area bounded by $N$ lines in general position,"Suppose we have $N$ lines in general position (any two lines, but no three lines, meet at a point) ( $N\geq 3$ ). Let the smallest bounded region have area $1$ . Determine the minimum (or possibly infimum) of the total bounded area. For example, $3$ lines create one triangular region while $4$ create one quadrangular region and two triangular regions (the complete region thus bounded is called a complete quadrilateral). I know that $N$ lines in general position will create ${N+1\choose 2}+1$ regions, ${N+1\choose 2}+1-2N=\frac{(N-1)(N-2)}{2}$ of those bounded on all sides by line segments, but I wanted to figure out how to make the regions maximally “equal” and how close it’s possible to get to the ideal minimum of $\frac{(N-1)(N-2)}{2}$ for a given $N$ (where all bounded regions have area $1$ ). I have little to no experience in the field of maximizing or minimizing geometrical quantities (especially in such a large mathematical space in which to minimize/maximize as this problem gives), so solving this is entirely beyond my experience. It is easy to show that there is no maximal bounded area for $N\geq 4$ by drawing some three lines arbitrarily close to meeting at a point. In this case, the resulting triangular region becomes arbitrarily close to a point while maintaining an area of $1$ in the problem's scaling, and the area of the remaining regions grows arbitrarily large by comparison. But I really don't know how to do any of this when it comes to minimizing the total bounded area. For $N=3$ , with only one region, the minimum area is trivially $1$ . For $N=4$ , some fiddling around on Desmos has convinced me that the minimum area is indeed the ideal of $3$ . For $N=5$ , I believe that the minimum area still remains the ideal of $6$ (although here my fiddling around on Desmos gets much more nonrigorous and guesswork-y). For $N\geq 6$ , however, any proofs or even good guesses of any sort elude me entirely. If finding the exact minimum area (or infimum in case there’s somehow a minimum that can be approached but not actually reached, which I would not expect) is in fact too difficult to do well, I would also be interested in the growth rate of the actual minimal area relative to the ideal as $N\to\infty$ . Thank you for your help! :)","['optimization', 'analytic-geometry', 'maxima-minima', 'geometry']"
4417432,Area of a semicircle minus a segment,How can you find the area of a semicircle minus a segment?,"['trigonometry', 'geometry']"
4417451,Spectral radius of a finitely generated group,"Let $G$ be a finitely generated group and $\Gamma$ be its Cayley graph with the usual word metric. Let $\mu$ be a non-degenerate measure on $G$ , and define $p(x, y) = \mu(x^{-1} y)$ . As is well-known, $p$ generates a random walk on $\Gamma$ with transition probability $p(x, y)$ . Now, define the quantity $\displaystyle\rho := \limsup_{n} p_n(x, y)^{1/n}$ , where $p_n(x, y)$ is the probability of hitting $y$ starting from $x$ at time $n$ (recall that $p_n$ can also be obtained from the $n$ -fold convolution $\mu^{(n)}$ ). It it a well-known result that $G$ is amenable if and only if $\rho = 1$ . A priori, though the definition of $\rho$ seems to depend on the measure $\mu$ , the above result seems to suggest that it is a purely metric concept (since amenability is a variant of nice isoperimetric behavior and is a purely metric construct), and does not depend on the specific non-degenerate measure under consideration. My question is, is the last statement correct? And if yes, can it be seen directly, or does it follow after the fact? Edit: Posted now on MO too!","['amenability', 'metric-geometry', 'reference-request', 'geometric-group-theory', 'group-theory']"
4417479,What's special about the number $1.000000015047466$E+$30$?,"I'm a programmer by trade by I've run into a weirdly special number and need some help deciphering its significance. I was writing some machine learning code that compiles into GPU kernel code and the compiler output the number 1.000000015047466E+30 as part of its generated code. It doesn't seem to be a common number of interest from the field of CS (e.g. it's not a power of $2$ ). Google searching for this constant just yields a handful of results that seem to suggest it may be a physical constant of some kind: This MATLAB forum link suggests it may be a default value for some physical phenomena. This Github repo on whole cell electrophysiology seems to use it as a max limit of some kind. This Opensea NFT link shows that the number is somehow used as a parameter for some kind of fractal art? I'm absolutely baffled I have absolutely no clue why this number is appearing in my ML code and why it's also being used in hard science research and fractal art. It lends me to believe this number has some special properties, but I'm not sure what. Any leads would be greatly appreciated.","['programming', 'floating-point', 'discrete-mathematics']"
4417498,Minimal symmetry group of polygon in $\mathbb{R}^2$,"Is there a polygon in $\mathbb{R}^2$ whose symmetry group is isomorphic to $\mathbb{Z}\backslash 3\mathbb{Z}$ ? I believe I found such a polygon, it’s an equilateral triangle with $3$ smaller equilateral triangles cut out on each side. My idea was to maintain the rotational symmetries, and eliminate the reflectional symmetries. This shape has $12$ vertices. I am wondering if there is a polygon with less than $12$ vertices that has symmetry group isomorphic to $\mathbb{Z}\backslash 3\mathbb{Z}$ . Any help would be appreciated.","['group-theory', 'abstract-algebra', 'symmetry']"
4417513,show that $h$ is a differentiable and $h(-1)=h(0)=h'(0)=0$ and $h(1)=1$ then $h^{(3)}\geq 3$,"If $h$ is an real function, differentiable three times on $[-1,1]$ , such that $h(-1)=h(0)=h'(0)=0$ and $h(1)=1$ . Prove that exist an real number $r\in(-1,1)$ such that $h^{(3)}(r)\geq 3$ For this I used Taylor's theorem so \begin{eqnarray}
h(x)= h(a)+h'(a)(x-a)+\frac{h''(a)}{2!}(x-a)^{2}+\frac{h^{(3)}(a)}{3!}(x-a)^{3}
\end{eqnarray} for $a\in[-1,1]$ , then, for $a=0$ \begin{eqnarray}
h(x)= \frac{h''(0)}{2!}x^{2}+\frac{h^{(3)}(0)}{3!}x^{3}
\end{eqnarray} and for $a=1$ , and $a=-1$ \begin{eqnarray}
h(x)= 1+h'(1)(x-1)+\frac{h''(1)}{2!}(x-1)^{2}+\frac{h^{(3)}(1)}{3!}(x-1)^{3}\\
h(x)=h'(-1)(x+1)+\frac{h''(-1)}{2!}(x+1)^{2}+\frac{h^{(3)}(-1)}{3!}(x+1)^{3}
\end{eqnarray} but I don't the way to continue. Do you know some hint to continue?","['calculus', 'taylor-expansion', 'analysis', 'real-analysis']"
4417550,Why does $f(x_i)-f(x_{i-1})$ not go to $0$ when finding arc length/surface area?,"I didn't really know what to title the question, but in class, we found the formula for surface area was: $A=\sum\limits_{i=1}^n\pi[f(x_i)+f(x_{i-1})]\sqrt{(x_i-x_{i-1})^2+(f(x_i)-f(x_{i-1}))^2}$ because surface area of a frustum is $A=\pi(R+r)l$ where $l$ is slant height. This makes sense to me but then the next step says that because $f(x_i)\approx f(x_i*)$ and $f(x_{i-1})\approx f(x_i*)$ . Then they say $A=2\pi f(x_i*)\sqrt{1+f'(x_i*)^2}$ . The square root was replace because of MVT which made sense to me, but if the approximation with $f(x_i*)$ is valid, then why wouldn't the square root just have the $f(x)$ part just go to $0$ and it'd just be $\Delta x$ . If that substitution is invalid, then why does it work in the first part of the formula to give us $2f(x_i*)$ . Essentially what I'm asking is why are we able to say $f(x_i)\approx f(x_i*)$ and $f(x_{i-1})\approx f(x_i*)$ for the first part of the formula, but can't put those in for the part under the square root. I know this result would give us $\int2\pi f(x)dx$ which is wrong because it should be $ds$ , but it feels wrong to apply the substitution in one place but not the other. Thank you in advance for your help!","['integration', 'limits', 'calculus']"
4417611,Need help solving this set of differential equations of motion,"I'm a masters student and I have a set of Differential equations I need to solve for my research. For context they are equations of motion for scalar fields coupled to gravity, though I suppose that's not really relevant to the maths involved. Specifically, they take the form $$\chi''(\rho)+2A'(\rho)\chi'(\rho)-(\chi'(\rho))^2-12=0$$ $$A''(\rho)-A'(\rho)\chi'(\rho)+2(A'(\rho))^2-24=0$$ $$2(A'(\rho))^2-2(\chi'(\rho))^2-24=0$$ Now, I already know that there are solutions where $\chi'$ and A' are constant ( $\chi'=2$ , $A'=4$ ). However, I know from existing literature using a different number of dimensions that there should also be another solution taking the form of a sum of (natural) logarithms of hyperbolic functions, such that when $\rho\rightarrow\infty$ , $\chi$ and A tend to the constant solutions. The problem is that for the life of me I cannot figure out how to find this additional solution. I have tried writing out trial solutions of the form $$\chi=\chi_0+\chi_1\log(\cosh(\chi_2\rho))+\chi_3\log(\sinh(\chi_4\rho))$$ $$A=A_0+A_1\log(\cosh(A_2\rho))+A_3\log(\sinh(A_4\rho))$$ But despite my best efforts I haven't been able to decipher any kind of solution or determine the constants so far. I feel a little embarrassed because I should probably be able to solve this kind of thing by now but I desperately need some help here. Im not even really sure where to start, apart from redefining $\chi_4$ and $A_4$ in terms of the other constants. I'm using mathematica if that helps at all. Apologies if this is a stupid question.","['nonlinear-system', 'systems-of-equations', 'ordinary-differential-equations']"
4417652,Galois group $G$ where every element fixes a root is trivial,"Let $K$ be the splitting field of a separable irreducible polynomial $f(x) \in F[x]$ of degree $n$ and let $G = Gal(K/F)$ . If for each $g  \in G$ , there is a root $\alpha$ of $f$ such that $g(\alpha) = \alpha$ , prove that $K=F$ . We know that $G$ permutes the roots of $f$ . Since $f$ is irreducible separable over $F$ , $G$ is isomorphic to a transitive subgroup of $S_n$ , i.e., $G$ has some $g$ that is an $n$ -cycle. But at the same time, $g$ also fixes some root of $f$ and so $n=1$ . Is that all? I've thought long and hard about this only to come up with a literal one-liner.","['galois-theory', 'group-theory', 'solution-verification', 'group-isomorphism']"
4417690,If a point is selected inside a rectangle what's the probability that the point is closer to center than vertex?,"If a point is selected inside a rectangle what's the probability that the point is closer to center than vertex? I thought of working on this problem using the concept of area. If I draw two concentric rectangles where length and width of inner rectangle are half of the outer one, then the probability should be the ratio of areas of both rectangles. Therefore $P(E) = \dfrac{l\times B}{2l\times 2b} = 1/4$ But the answer given in my book is $1/2$ . What's the problem here?",['probability']
4417695,"Does this series $L_{i+1}(G)=\{g\in G\mid \varphi(g)g^{-1}\in L_i(G), \forall\varphi\in\operatorname{Aut}(G)\}$ have a name?","From the comment to this other question of mine, I have learned that the series ( $i=0,1,2,\dots$ ): $$Z_{i+1}(G)=\{g\in G\mid \varphi(g)g^{-1}\in Z_i(G), \forall\varphi\in\operatorname{Inn}(G)\} \tag 1$$ where $Z_0(G)=1$ (and hence $Z_1(G)=Z(G)$ , the center of $G$ ) is named the upper central series of $G$ . Does this other series: $$L_{i+1}(G)=\{g\in G\mid \varphi(g)g^{-1}\in L_i(G), \forall\varphi\in\operatorname{Aut}(G)\} \tag 2$$ where $L_0(G)=1$ (and hence $L_1(G)=L(G)$ , the absolute center of $G$ ) have a name?","['group-theory', 'definition', 'terminology']"
4417698,What exactly is the relationship between an Ehresmann connection and splitting of the jet sequence.,"An Ehresmann connection on a vector bundle $\pi : E \to X$ is a splitting of the sequence, $$(1) 0 \to V \to TE \to \pi^* TX \to 0 $$ which respects the linear structure on $E$ (meaning the section is invariant under the induced automorphism of $T E$ induced by scaling). In algebraic geometry, I am familiar with the equivalence between connections on a locally free sheaf $\mathcal{E}$ and splitting of the sequence $$(*) 0 \to \Omega_X^1 \otimes \mathcal{E} \to J^1(\mathcal{E}) \to \mathcal{E} \to 0 $$ I have heard it said that this is a version of the Ehresmann connection formalism but I am not able to make this precise. If I dualize the top sequence and use the fact that $V \cong \pi^* E$ then I recover, $$ 0 \to \pi^* T^* X \to T^* E \to \pi^* E^* \to 0 $$ which looks similar to the jet bundle sequence. However, I am not sure how to directly compare these two sequences. Furthermore, the notion of an Ehresmann connection makes perfect sense in the algebraic category. However, (at least without directly relating it to the jet bundle sequence) I do not see how to show that the data of a splitting recovers an (algebraic) connection. The usual construction of a connection from an Ehresmann connection goes through algebraically. Call the splitting $v : T E \to \pi^* E$ . Then given a section $s : X \to E$ we get $\mathrm{d}{s} : T X \to s^* T E$ and then $s^* v \circ \mathrm{d}{s}$ is a linear map $T X \to E$ defining $X \mapsto \nabla_X s$ thus defining the connection. However, to reverse this process, it seems that I need to be able to choose, locally, flat sections for a connection $\nabla$ in order to define the kernel of $v$ which is the horizontal subspace. Does this mean the Ehresmann connection is really a transcendental object?","['connections', 'algebraic-geometry', 'differential-geometry']"
4417703,What is the intuition behind a G-matrix? Any important applications in science and engineering?,"In 2012, Miroslav Fiedler and Frank J. Hall (Linear Algebra) introduced a new class of matrices called as ""G-matrices"". In fact, a real square matrix $A$ is called a $G$ -matrix, if $A$ is nonsingular and there exist nonsingular diagonal matrices $D_1$ and $D_2$ such that $$
(A^{-1})^\top = D_1 A D_2
$$ They present a potpourri of results like all orthogonal matrices, diagonal matrices are $G$ -matrices. Otherwise, they don't give much insight into how they came up with this new class of matrices. The conclusion in the paper gives a summary of mathematical properties but not any ideas like where these $G$ -matrices can be applied. Is it correct to say that $G$ -matrices are some generalization of orthogonal matrices? G-matrix (2012) Paper Link","['matrices', 'linear-algebra']"
4417758,Why does squaring $\sqrt{x+2}\ge x$ misses an interval? [duplicate],"This question already has answers here : How to solve $\sqrt{x+2}\geq x$? (9 answers) Closed 2 years ago . If I square both sides of $\sqrt{x+2}\ge x$ I get, $$x^2-x-2\le0\quad\Rightarrow\quad (x-2)(x+1)\le0\quad\Rightarrow\quad x\in[-1,2]$$ But the interval $[-2,-1)$ should be included in the solution which is missed here. I'm wondering what's going wrong in the above approach that misses an interval?","['algebra-precalculus', 'inequality']"
4417762,What is the shortest way to enclose $n$ circles?,"What is the shortest way to enclose $n$ circles of radius $1$ ? Is there a formula for the length of the shortest path? I have tried to find the shortest way, up to $n\leq10$ . Below are the shortest ways of those I have found, but there may be shorter ways than these. n length 1 $2\pi$ 2 $2\pi+4$ 3 $2\pi+6$ 4 $2\pi+8$ 5 $2\pi+10$ 6 $2\pi+8+2\sqrt{3}$ 7 $2\pi+12$ 8 $2\pi+14$ 9 $2\pi+12+2\sqrt{3}$ 10 $2\pi+16$","['circles', 'geometry']"
4417796,Integration of power law distribution and negative arguments in the lower incomplete gamma,"Dear mathematical acolytes, I am working as a materials scientist and my current topic is related to some probabilistic considerations of the microstructures of metals. I have the probability of something occuring $ P_\mathrm{occur} $ which is dependent on the diameter, $ c $ , of something in the microstructure. This occurance probability is then modified with the probability density distribution of $c$ as $ P_\mathrm{occur}\left( c \right) p_\mathrm{size}\left( c \right) $ where $$
P_\mathrm{occur}\left( c \right) = 1 - \exp{\left( -B\left( \frac{c}{c_\mathrm{N}} \right)^3 \right)}, 
\\
p_\mathrm{size}\left( c \right) = \frac{m-1}{c_\mathrm{min}}\left( \frac{c}{c_\mathrm{min}} \right)^{-m}.
$$ The probability density $ p_\mathrm{size}\left( c \right) $ is a power-law distribution with a lower bound at $ c_\mathrm{min} $ (a positive number) and upper bound at $ \infty $ . The exponent is subject to $ m > 1 $ . The parameter $ B $ in the occurance probability assumes a value between $ 0 $ and $ 1 $ , and $c_\mathrm{N} $ is a positive number of the same order of $ c_\mathrm{min} $ . The modified occurance probability is then to be integrated over the size $ c $ as: $$
\int_{c_\mathrm{min}}^\infty{P_\mathrm{occur}\left( c \right) p_\mathrm{size}\left( c \right) \mathrm{d}c} = \\
\int_{c_\mathrm{min}}^\infty{ \left( 1 - \exp{\left( -B\left( \frac{c}{c_\mathrm{N}} \right)^3 \right)} \right) \frac{m-1}{c_\mathrm{min}}\left( \frac{c}{c_\mathrm{min}} \right)^{-m} {d}c}
$$ Here the first question appears, how is this to be integrated? Can it be shown using common mathematical functions? As I am of little mathematical prowess, I tried using the symbolical library SymPy in Python to carry out the integration, and also to compare with a numerical integration using the trapezoidal rule. What happens is that the symbolical and the numerical integrations yields the same result, which is great, but that the symbolical integration returns a function named lowergamma(a,x) , SymPy Docs , with a negative argument a evaluated at x . This is supposed to be the incomplete lower gamma Wikipedia , however, the incomplete lower gamma should not take negative arguments. What puzzles me is that the symbolical integration with inserted numerical values in SymPy gives the same result as the trapezoidal numerical integration, even though the symbolical result uses the lower incomplete gamma with negative values, which should not be possible. Here a cluster of questions appears, how can SymPy use negative values in its lower incomplete gamma? Neither SciPy nor Matlab can do this. What is different? How can I account for this in another library/language? Thankful for comments and answers! Update after answer from K.defaoite The negative values in the argument of the incomplete gamma function also appears in the accepted answer below. This can be computed using 8.5.1 in the DLMF where a confluent hypergeometric function is used. Using this along with the solution presented below gives the same answer as both the symbolical and the numerical integration. The code is updated. The code I used for my symbolical/numerical comparison outputs this: Symbolic integral before evaluation = Symbolic integral before evaluation = 0.279982455532194*gamma(2/3)*lowergamma(-2/3, 0.5)/gamma(5/3) + 1.0 - 0.419973683298291*gamma(-2/3)
Value of symbolical integration =  0.744656471805332
Value of numerical integration  =  0.7446564718064296
Integral by K.defaoite          =  0.7446564718053317 The code itself, including the parameters used for evaluation: import sympy as sym
import numpy as np
import scipy.special

# ---- Symbolical integration ----

c = sym.symbols('c', positive=True)
cmin = sym.symbols('cmin', positive=True)
cmax = sym.symbols('cmax', positive=True)
cN = sym.symbols('cN', positive=True)
B = sym.symbols('B', positive=True)
m = sym.symbols('m', positive=True)

func = (m-1)/(cmin**(1-m))*c**-m*(1 - sym.exp(-B*(c/cN)**3))
integrated = sym.integrate(func,(c, cmin, sym.oo), conds='separate')

# ---- Evaluation and comparison to numerical integration ----

Bnum = 0.5          # Somewhere between 0 and 1
cminnum = 0.1       # Small number, lower bound of power law dist.
cNnum = 0.1         # Parameter on the order of cmin
mnum = 3            # Exponent of power law dist, must be larger than 1, realistic up to 20

func = integrated[0].subs(B, Bnum).subs(cN, cNnum).subs(cmin, cminnum).subs(m, mnum)

cnum = np.logspace(np.log10(cminnum), 82, 100000000)
func_num = (mnum-1)/(cminnum**(1-mnum))*cnum**-mnum*(1. - np.exp(-Bnum*(cnum/cNnum)**3.))
trapz_func_num = np.trapz(func_num, cnum)

# ---- Integral by K.defaoite ----

a = (mnum - 1.)/3.
z = Bnum*(cminnum/cNnum)**3.
upper_gamma = scipy.special.gamma(-a) - z**(-a)/(-a)*scipy.special.hyp1f1(-a, -a+1., -z, out=None)
int_Kdefaoite = 1. - a*z**a*(upper_gamma)

# ---- Compare ----

print('Symbolic integral before evaluation =', func)
print('Value of symbolical integration = ', sym.N(func))
print('Value of numerical integration = ', trapz_func_num)
print('Integral by K.defaoite = ', int_Kdefaoite)","['integration', 'probability-distributions', 'probability', 'gamma-function']"
4417896,Solve $ \int_0^\infty x^n e^{-\lambda x} dx $ by differentiating under integral sign,"I have only found information regarding doing this by integration by parts. By differentiating under the integral sign, I let $$I_n = \int_0^\infty x^n e^{-\lambda x} dx $$ and get $\frac{dI_n}{d\lambda} = -I_{n+1} $ and therefore $\frac{dI_n}{d\lambda} = -\frac{n+1}{\lambda} I_n$ . Proceeding from here I solve the ODE to get $I_n = Ae^{-\frac{n+1}{\lambda}x}$ . This is clearly wrong. What went wrong? I am unsure how to proceed with this differentiation of the integral approach to solve this problem.","['integration', 'calculus', 'derivatives']"
4417915,Proof of the Density Theorem using contradiction?,"I just started studying Introduction to Real Analysis for the first time. I'm sorry if the following attempt contains elementary mistakes. The Density Theorem states that if $x$ and $y$ are any real numbers with $x < y$ , then there exists a rational number $r$ such that $x < r < y$ . In the book, they use the Archimedean Property to prove the theorem, but I'm thinking of trying to solve it using contradiction as follows: """"Suppose, by contradiction, that $x < y$ but for all rationals $r$ , $r \geq y$ or $r \leq x$ (simply the negation of the wanted conclusion). Case 1: If $r \geq y$ , then $y$ is a lower bound of $\mathbb{Q}$ . Since $\mathbb{Q}$ is unbounded, then this case is impossible. Case 2: If $r \leq x$ , then $x$ is an upper bound of $\mathbb{Q}$ . Similarly, $\mathbb{Q}$ being unbounded implies this is impossible. Case 3: If $ r \geq y$ and $r \leq x$ then, since $x < y$ , $r \geq y > r$ which is impossible (also note that Cases 1 and 2 eliminate Case 3 completely). Since all cases are impossible, the Density Theorem must be true."""" Is this a valid proof? If not, why? Thank you very much.","['proof-writing', 'analysis', 'real-analysis', 'alternative-proof', 'solution-verification']"
4417922,Distributing colored balls in 80 cells so that no two balls of the same color are in the same cell,"An urn contains 50 white balls and 50 black balls(balls are indistinguishable).
We want to distribute all the balls into 80 numbered cells, where at-most 1 ball of the same color is allowed to occupy any cell.
Find the probability that all the cells are occupied. We all got different answers for this problem. Some answer was only a simple fraction using binomials, some had big summations involving product of three binomials in the denominator. I would be grateful if someone outlines the solution and if possible provides the correct answer. My attempt At first I counted the number of ways to place the balls so that all the cells are occupied.
By PHP, we know that a cell can not contain $\ge 3$ balls. Let there be $a$ cells containing 2 balls.
Then we know that $100=2a+(80-a)\implies a=20$ . Now we choose which 20 cells contains 2 balls.
This can be done in $\binom{80}{20}$ ways. Now we have to distribute the remaining 60 balls in 60 cells. But these remaining 60 balls consist of 30 black and 30 white balls.
Thus we only need to choose which 30 cells we want to put the white balls in. So in total there are $\binom{60}{30}\cdot \binom{80}{20}$ ways. Now I did this same procedure assuming that the balls will e distributed in $k$ cells, $50\le k\le 80$ . This gave me really huge sum which I could not simplify and don't remember now. If I recall it, I will add it asap. But I wanted to know whether my method is at-leats correct? Thanks in advance.","['discrete-mathematics', 'probability']"
4418016,Is OEIS A046346 sequence a subset $S\subset\mathbb{N}$ s.t. $\sum_S\frac{1}{n-\pi(n)}=1$?,"OEIS A046346 sequence lists composite numbers divisible by the sum of their prime factors, counted with multiplicity. $$S=\Big\{n\in\mathbb{N}\space not\space prime,\space n=\prod_k p_k^{\alpha_k}\space:\space\sum_k\alpha_k p_k\space|\space n\Big\}=\Big\{4,16,27,30,60,70,72,84,105,150,\dots\Big\}$$ I've considered over $S$ the series $$\sum_{n\space\in\space S}\frac{1}{n-\pi(n)}$$ where $\pi(n)$ denotes the prime counting function. After the first $296000$ terms, the sum of the series amounts to $0.9956237272160026\dots$ Is it realistic to think that $$\sum_{n\space\in\space S}\frac{1}{n-\pi(n)}=1\space?$$ May this conjecture somehow be related to the one presented in this previous question of mine?","['number-theory', 'elementary-number-theory', 'arithmetic-functions', 'sequences-and-series', 'prime-numbers']"
4418023,Proof of cos(A-B) and geometric intuition,"In the above figure, $a,b$ are unit vectors. The angle between them is $A-B$ . It is easy to see the green highlighted part is: $$\cos A \cos B$$ This means the magenta part must be $$\sin A \sin B$$ Is there a way to prove this elegantly? My work: I tried various similar triangle ratios, then a lot of trig simplification and arrived at the answer. I don't even want to attempt it again. It's a lot of mess. I was hoping to get a more geometric intuition for the magenta segment. Any help? Why I'm looking for geometric intuition: The product $\cos A \cos B$ changes when the plane rotates The product $\sin A \sin B$ changes when the plane rotates But their sum $\cos A \cos B + \sin A \sin B $ doesn't change! I  fully understand the algebra - rotation matrix is orthonormal and preserves the dot product: $$(Ra)' Rb = a'R'Rb = a'Ib = a\cdot b$$ Seeking geometric intuition if possible...",['trigonometry']
4418038,What is an example of a measure without a density that is useful for some application?,"I've been self-studying measure theory from Richard Ash's textbook, and am close to completing Chapter 2 (so, for instance, I've covered the Radon-Nikodym Theorem). What is an example of a measure which is useful for some application for which there is not a density with respect to either counting measure or Lebesgue measure? I would be especially interested to see an example that is a probability measure. I've seen it claimed (e.g. by David Pollard) that such examples are prevalent in practice (and that the intuition of statisticians can be unnecessarily restricted by considering probability distributions merely in terms of those with probability density functions or probability mass functions).
But I can think of no such example at this point in my study.","['measure-theory', 'probability-theory']"
4418041,"A General Continous Mapping or ""Slutsky"" Theorem For Stochastic Processes","While standard Slutsky or Continuous Mapping Theorems apply to non-random functions, are there versions that hold for random functions under suitable conditions? For example, consider a stochastic process $X_n: \Omega \times D_1 \to D_2$ , where $D_1,D_2$ are metric spaces. Suppose that for all $u$ in $D_1$ , $X_n(u) \stackrel{a.s.}{\to} X(u)$ . If $Y_n \in D_1$ , and $Y_n \stackrel{a.s.}{\to } Y$ (we might even imagine $Y$ as being deterministic/constant), then does it follow (under suitable conditions) that $X_n(Y_n) \stackrel{a.s.}{\to} X(Y)$ ? As a simple example of what I'm talking about, imagine the following extremely over complicated proof that for iid RV's $X_1,...,X_n$ with $E(X_i)=0$ , $Var(X_i)=\sigma^2$ , $$S_n^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2 \stackrel{a.s.}{\to} \sigma^2:$$ Define $$S_n^2(u )  =  \frac{1}{n} \sum_{i=1}^n (X_i - u)^2,  $$ so that $S_n^2 = S_n^2(\bar{X})$ . By the SLLN, $S_n^2(u) \stackrel{a.s.}{\to} E(X_0-u)^2$ for each fixed $u \in \mathbb{R}$ . Moreover, $\bar{X} \stackrel{a.s.}{\to}0$ , hence we conclude $S_n^2(\bar{X}) \stackrel{a.s.}{\to} EX_0^2 = \sigma^2$ . What extra details need to be given to make this argument rigorous?","['probability-theory', 'real-analysis']"
4418043,Terminology confusion in complex geometry (hermitian geometry),"I was reading the book by Huybrechts and he writes the definition of an Hermitian structure as follows: Let $X$ be a complex manifold and $I$ be the induced almost complex structure. A Riemannian metric $g$ on $X$ is said to hermitian structure on $X$ if for any point $x \in X$ , the scalar product $g_x$ on $T_x X$ is compatible with the almost complex structure $I$ (i.e $g_x(v,w) = g_x(I(v),I(w)$ for all $v,w \in T_xM$ ). And I found the definition of Hermitian metric in Griffiths and Harris's Principles of Algebraic geometry as follows: Let $X$ be a complex manifold of dimension $n$ . A hermitian metric on $X$ is given by a positive definite hermitian inner product $$ (\cdot, \cdot)_z : T^{'}_{z}(X) \otimes \overline{T^{'}_{z}(X)} \longrightarrow \mathbb{C} $$ on the holomorphic tangent space at $z$ for each $z \in X$ , depending smoothly on $z$ . My question: I am facing difficulty in understanding the following terminologies: Hermitian metric Hermitian structure Hermitian form Hermitian inner product Giving really stupid input, I think 1) and 2) must be the same things (if it is, still I don't get how). Can anyone please clarify these terminologies? I am really confused looking at different other sources (in fact, I am getting more confused after referring to other references). Thanks!","['complex-geometry', 'differential-geometry']"
4418049,weak-star limit of a sequence of integrable functions is an integrable function,"Let $(f_n)_{n\geq1}\subset L^1(\mathbb{R}^d,\mathbb{C})$ be a sequence of Lebesgue integrable functions, and let $g\in L^1(\mathbb{R}^d)$ be a non-negative integrable function be such that \begin{equation}
\vert f_n(x)\vert\leq g(x)
\end{equation} holds for every $x\in\mathbb{R}^d$ and every $n\geq 1$ .
Since the sequence $(f_n)_n$ is bounded in $L^1(\mathbb{R}^d)\subset \mathcal{M}(\mathbb{R}^d)=C_{0}(\mathbb{R}^d)^*$ , there is a finite complex measure $\mu\in\mathcal{M}(\mathbb{R}^d)$ and a subsequence $(n_k)_k$ such that $f_{n_k}\overset{\ast}{\rightharpoonup} \mu$ as $k\rightarrow\infty$ , i.e. such that $$\lim_{k\to\infty}\int_{\mathbb{R}^d}f_{n_k}(x)\varphi(x)dx = \int\varphi(x)d\mu(x)$$ holds for every $\varphi\in C_{0}(\mathbb{R}^d)$ , continuous function vanishing at infinity. How can we deduce in this particular case that $\mu$ is absolutely continuous with respect to Lebesgue measure, or equivalently that that $\mu=fdx$ for some $f\in L^1(\mathbb{R}^{d})$ ?","['integration', 'measure-theory', 'lebesgue-integral', 'analysis', 'real-analysis']"
4418067,Calculating sum formulas $\sum_{i=1}^{x} i^n$,"I think I've found a way to calculate a general formula for $\displaystyle\sum_{i=1}^{x} i^n$ .
Here is how I did it: Notice that $\displaystyle\sum_{i=1}^{x} i^n-{(i-1)}^n=x^n$ Here the difference of polynomials will be very useful, if we want $i^n$ to appear here all we have to do is to take the polynomials of degree $n+1$ , let's see that in two examples. A) Let's calculate $\displaystyle\sum_{i=1}^{x} i$ , notice that we want the degree $1$ to appear, so we will take difference of polynomials of degree $2$ here as follows: $\displaystyle\sum_{i=1}^{x} i^2-{(i-1)}^2=x^2$ Notice that this is the same as: $\sum_{i=1}^{x} (2i-1)=x^2$ Here we can do some arrangements to get: $2\displaystyle\sum_{i=1}^{x}i-\displaystyle\sum_{i=1}^{x}1=x^2$ . It is easy to see that $\displaystyle\sum_{i=1}^{x}1=x$ , if we substitute this in we get: $\displaystyle\left(2\sum_{i=1}^{x} i\right) -x=x^2$ and if we do the algebra here we finally get: $\displaystyle\sum_{i=1}^{x}i=\frac{x^2+x}{2}$ B) Let's calculate $\displaystyle\sum_{i=1}^{x} i^2$ , but for the sake of the post let's do that inline. We take the difference of polynomials of degree $3$ here to get $\displaystyle\sum_{i=1}^{x} i^3-{(i-1)}^3=x^3$ , expanding the polynomials and simplifying gives us the equation $\displaystyle\sum_{i=1}^{x} 3i^2-3i+1=x^3$ , this is the same as $3\displaystyle\sum_{i=1}^{x} i^2-3\displaystyle\sum_{i=1}^{x} i+\displaystyle\sum_{i=1}^{x}1=x^3$ , here we have found at A) that $\displaystyle\sum_{i=1}^{x}i=\frac{x^2+x}{2}$ , if we substitute this in and substitute $\displaystyle\sum_{i=1}^{x}1=x$ we get $3\displaystyle\sum_{i=1}^{x} i^2-3.\frac{x^2+x}{2}+x=x^3$ , from this we get $\displaystyle\sum_{i=1}^{x} i^2=\frac{x^3-x}{3}+\frac{x^2+x}{2}$ . This method can be generalized for any $n$ as follows: $\sum_{i=1}^{x}i^n=\frac{x^{n+1}+\sum_{i=1}^{x}\left( \sum_{j=2}^{n+1}\left({n+1 \choose j}i^{n+1-j}.(-1)^j\right)\right)}{n+1}$ . I also made a script to run the algorithm until $n=100$ , and all the formulas seem to work. I was wondering if this approach is correct and is it something new. Thanks in advance!",['number-theory']
4418090,summing over a new variable in joint distribution,"I have noticed some examples when evaluating a joint probability they introduce a new variable by summing over it. For example: $P(A,B) = \sum_{C} P(A,B,C) = \sum_{C} P(A|B,C) P(B,C)= \sum_{C} P(A|B,C) P(B|C)$ What is the name of this approach ? and when do we use it ? My second question is that shouldn't the above equation be further factored to P(C)? like this: $P(A,B) = \sum_{C} P(A,B,C) = \sum_{C} P(A|B,C) P(B,C)= \sum_{C} P(A|B,C) P(B|C) P(C)$ Or is it ok to stop whenever we want ?","['statistics', 'probability']"
4418096,Grothendieck Group of a Nonsingular Curve (Hartshorne Exercise II.6.11).,"I have copied the exercise below for reference. I was able to figure out how to do (a) and (d). Essentially the isomorphism in (d) is given by rank and determinant respectively. So let me focus on (b) and (c) in this post. Please do not provide me with full solutions, but hints (and if solutions are easier, please hide the details for some of them). For (b), the existence of the two step locally free resolution is doable. The statement that $\operatorname{det}(\psi(D))=\mathscr{L}(D)$ is also easy. What is stumping me is how to prove the independence of the choice of resolution. Checking that this defines a homomorphism follows from Hartshorne Exericse II.5.16(d) and by taking a simultaneous resolution. There are two points in this long exercise that I am stuck. Where I am stuck: How can I show that the definition is independent of choice of resolution? An idea is to consider the two resolutions and use a result from homological algebra to extend the map $\mathscr{F}\rightarrow\mathscr{F}$ to a chain map $\mathscr{E_i}\rightarrow \mathscr{E_i'}$ between the two resolutions. This doesn't seem to do anything for me. I also did some computations in the easy case where $X$ is affine and $\mathscr{F}$ is free, but then were unenlightening. Where I am stuck #2: How should I approach (c)? I know there are answers circulating around, but I would like only a hint. Since we want a torsion sheaf at the end, I should try to to find $\mathscr{L}(D)^{\oplus r}$ such that the stalk at the generic point $\xi$ gives an isomorphism $\mathscr{L}(D)^{\oplus r}_\xi\rightarrow \mathscr{F}_\xi$ . Since $\mathscr{F}_\xi$ has rank $r$ , I really only need $\mathscr{L}(D)_\xi\cong \mathcal{O}_\xi$ . So maybe part (b) with $\operatorname{det}(\psi(D))=\mathscr{L}(D)$ could be helpful here? Unfortunately, I do not know... Exercise II.6.11 The Grothendieck Group of a Nonsingular Curve Let $X$ be a nonsingular curve over an algebraically closed field $k$ . We will show that $K(X) \cong \operatorname{Pic} X \oplus \mathbb{Z}$ , in several steps. (a) For any divisor $D=\sum n_{i} P_{i}$ on $X$ , let $\psi(D)=\sum n_{i} \gamma\left(k\left(P_{i}\right)\right) \in K(X)$ , where $k\left(P_{i}\right)$ is the skyscraper sheaf $k$ at $P_{i}$ and 0 elsewhere. If $D$ is an effective divisor, let $\mathcal{O}_{D}$ be the structure sheaf of the associated subscheme of codimension 1 , and show that $\psi(D)=\gamma\left(\mathcal{O}_{D}\right)$ . Then use (6.18) to show that for any $D, \psi(D)$ depends only on the linear equivalence class of $D$ , so $\psi$ defines a homomorphism $\psi: \operatorname{Cl} X \rightarrow K(X)$ . (b) For any coherent sheaf $\mathscr{F}$ on $X$ , show that there exist locally free sheaves $\mathscr{E}_{0}$ and $\mathscr{E}_{1}$ and an exact sequence $0 \rightarrow \mathscr{E}_{1} \rightarrow \mathscr{E}_{0} \rightarrow \mathscr{F} \rightarrow 0$ . Let $r_{0}=$ rank $\mathscr{E}_{0}$ , $r_{1}=\operatorname{rank} \mathscr{E}_{1}$ , and define $\operatorname{det} \mathscr{F}=\left(\bigwedge^{r_{0}} \mathscr{E}_{0}\right) \otimes\left(\bigwedge^{r_{1}} \mathscr{E}_{1}\right)^{-1} \in \operatorname{Pic} X .$ Here $\bigwedge$ denotes the exterior power (Ex. 5.16). Show that det $\mathscr{F}$ is independent of the resolution chosen, and that it gives a homomorphism $\operatorname{det}: K(X) \rightarrow$ Pic $X$ . Finally show that if $D$ is a divisor, then $\operatorname{det}(\psi(D))=\mathscr{L}(D)$ . (c) If $\mathscr{F}$ is any coherent sheaf of rank $r$ , show that there is a divisor $D$ on $X$ and an exact sequence $0 \rightarrow \mathscr{L}(D)^{\oplus r} \rightarrow \mathscr{F} \rightarrow \mathscr{T} \rightarrow 0$ , where $\mathscr{T}$ is a torsion sheaf. Conclude that if $\mathscr{F}$ is a sheaf of rank $r$ , then $\gamma(\mathscr{F})-r \gamma\left(\mathcal{O}_{X}\right) \in \operatorname{Im} \psi$ . (d) Using the maps $\psi$ , $\det$ , $\operatorname{rank}$ , and $1 \mapsto \gamma\left(\mathcal{O}_{X}\right)$ from $\mathbb{Z} \rightarrow K(X)$ , show that $K(X) \cong$ $\operatorname{Pic} X \oplus \mathbb{Z}$ .","['divisors-algebraic-geometry', 'algebraic-geometry', 'sheaf-theory']"
4418150,Iwasawa decomposition of the general symplectic groups over the ring of adeles,"This post is going to be quite a silly one, as its subject is something I believe to be the case, but which I am unable to prove to be the case (due to my lack of experience with algebraic and specifically reductive groups); nor have I been able to find a reference that asserts this to be the case. I will begin by defining my terms: Let $G := \textrm{GSp}_{2n}(\mathbb{A})$ denote the rank $2n$ general symplectic group over the ring of adeles. Let $$
P := \left\{
\left(\begin{array}{cc}
A & B\\
C & D
\end{array}\right) \in G\quad
\middle|\quad C=0 \right\}
$$ denote the Siegel parabolic subgroup of $G$ . The unipotent subgroup of $P$ can then be seen to be of the form $$
U = \left\{
\left(\begin{array}{cc}
\textrm{Id}_n & X\\
0 & \textrm{Id}_n
\end{array}\right) \in G\quad
\middle|\quad X \in \textrm{Sym}_n(\mathbb{A}) \right\},
$$ where $\textrm{Sym}_n(\mathbb{A})$ denotes the additive group of $n \times n$ matrices with entries in $\mathbb{R}$ , and $\textrm{Id}_n$ denotes the $n \times n$ identity matrix. We define the standard Levi component of $P$ to be: $$
M := 
\left\{
\left(\begin{array}{cc}
Y & 0\\
0 & u (Y^t)^{-1}
\end{array}\right) \in G\quad
\middle|\quad Y \in \textrm{GL}_n(\mathbb{A}),\ u \in \mathbb{A}^{\times} \right\},
$$ We define the standard maximal compact subgroup of $\textrm{Sp}_{2n}(\mathbb{R})$ as: $$
K_{\infty} := \left\{ g \in \textrm{Sp}_{2n}(\mathbb{R})\ \middle|\ g \cdot i \textrm{Id}_n = i \textrm{Id}_n \right\},
$$ where the group action of $\textrm{Sp}_{2n}(\mathbb{R})$ is the usual one. Lastly, for any natural number $q$ , we define the following family of open subgroups of the $\textrm{GSp}_{2n}(\mathbb{Z}_p)$ : $$
K_p(q) := \left\{ g \in \textrm{GSp}_{2n}(\mathbb{Z}_p)\ \middle|\ 
g \equiv
\left(\begin{array}{cc}
\textrm{Id}_n & 0\\
0 & a \textrm{Id}_n
\end{array}\right)\quad
(\textrm{mod}\ q),\quad\textrm{for some}\ a \in \mathbb{Z}_p^{\times} \right\},
$$ and hence: $$
K_0(q) := \prod_p K_p(q),
$$ with the product taken over the set of all rational primes $p$ . My question is whether or not we have the following ""Iwasawa decomposition"" of $G$ : $$
G = U M K_{\infty} K_0(q),
$$ for any natural number $q$ . My reason for believing the above to be true is that it is a natural generalisation of a statement about $\textrm{GSp}_4(\mathbb{A})$ that certainly is true. I would much appreciate it if someone could refer me to a place where the above is demonstrated to be true, or if you could let me know if and why it isn't true.","['algebraic-number-theory', 'reductive-groups', 'algebraic-geometry', 'abstract-algebra', 'linear-algebra']"
4418168,Volume between paraboloid $x^2 +y^2 -4a(z+a)=0$ and sphere $x^2 + y^2 +z^2 =R^2$,"I'm trying to obtein the volume via triple integral but think I'm setting the wrong radius. The solid in particular is bounded by the sphere $x^2 + y^2 +z^2 =R^2$ and above the parabolloid $x^2 +y^2 -4a(z+a)=0$ (consedering $R>a>0$ ). I'm setting cylindrical coordinates and I do eventually get $\theta \in [0,2\pi[$ and $(\rho^2 / 4a) -a\leq z \leq R^2-\rho^2$ . I deduce that the radius must be limited in between $0$ and $4Ra-4a^2$ (Intersection is at height $z=R-2a$ ), so the volume should be: $$\int_{0} ^{2\pi} \int_0 ^{4Ra-4a^2} \int_{(\rho^2 /4a)-a} ^{R^2 -\rho^2} \mathrm{d}V$$ , but this integral results in a different expression from the original solution. My teacher told us the solution is $2\pi \left ( \frac{a^3}{3} - aR^2 + \frac{2R^3}{3} \right )$ .","['integration', 'spheres', 'volume', 'multivariable-calculus', 'calculus']"
4418177,What are the possible expansions of f(z) = $\frac{e^{1/z}}{z}$ about z = i,"I am trying to find all the possible expansions of f(z) = $\frac{e^{1/z}}{z}$ about  = i, and I got something but I am not sure if my reasoning is right. We know we have a singularity at z = 0, so we are going to have two domains.
The first one is |z| < 1, while the second one is |z| > 1.
If  understand it correctly, the first domain (inside the disk of radius 1), will be expanded with a Taylor series, due to the absence of singularities, while the second one (|z| > 1) will need a Laurent series to be computed, as we have a singularity at z = 0. This is what I have done so far: $$\frac{e^{1/z}}{z} = \frac{e^{-i}}{z} + \frac{e^{-i}(z-i)}{z} + \frac{(e^{-i}+2)(z-i)^2}{2z} + . . .$$ But I am not sure if that is correct, and I am not sure how to account for the different domains.","['complex-analysis', 'complex-integration']"
4418252,How to prove by induction $\frac 12 + \frac 14 + ... + \frac {1}{2^n} = 1- \frac {1}{2^n} $ [duplicate],"This question already has an answer here : How do I prove this by mathematical induction? [closed] (1 answer) Closed 2 years ago . I'm trying to solve this problem about mathematical induction but every time I try to solve it I end up with an incorrect answer.
The problem is as follows: Prove by induction : $$\frac 12 + \frac 14 + ... + \frac {1}{2^n} = 1- \frac {1}{2^n} $$ my work:
first I prove that it holds for n=1: $\frac 12 = 1- \frac {1}{2^1} = \frac 12$ Now assume it holds for n=k, prove for n=k+1 the sum is equal to $1 - \frac{1}{2^{k+1}}$ then $\frac 12 + \frac 14 +...+ \frac {1}{2^k} + \frac {1}{2^{k+1}} = 1-\frac {1}{2^k} + \frac {1}{2^{k+1}} = 1 - \frac {2^{k+1}+2^k}{2^{k+1}2^k} = 1- \frac {2(2^k)+2^k}{2(2^k)2^k} = 1 - \frac {3(2^k)}{2(2^k)2^k} = 1- \frac {3}{2(2^k)} = 1- \frac {3}{2^{k+1}}  $ I would very much appreciate if at least someone can tell me where I'm wrong so I could try to solve it by myself. I think the mistake is at factoring $2^{k+1}+2^k$","['algebra-precalculus', 'induction']"
4418256,Trigonometric and exponential integral $\int _0^{\pi }\frac{\cos \left(a\sin x\right)}{1+a\cos x}e^{a\cos x}dx$,"How can we prove this Integral relation? $$\int _0^{\pi }\frac{\cos \left(a\sin x\right)}{1+a\cos x}e^{a\cos x}dx=\frac{\pi }{e}\cdot \frac{e^{\sqrt{1-a^2}}}{\sqrt{1-a^2}}$$ where $\text{  }a\in(-1,1)$ .","['integration', 'complex-analysis', 'calculus']"
4418260,Regular Conditional Probability vs Regular Conditional Distribution,"Wikipedia (and different books too) seem to give two different definitions of what a regular conditional probability is. What is the correct definition and how do they relate? It seems to me that the first definition is the correct one, while the second is actually the definition of a regular conditional distribution ? Definition 1 can be found here . Definition 1 : Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and $\mathsf{A}\in\mathcal{F}$ . Let $\mathbb{1}_{\mathsf{A}}:\Omega\to\{0, 1\}$ be the indicator random variable. A conditional probability of $\mathsf{A}$ given $\mathcal{G}$ is defined as a version of $\mathbb{E}[\mathbb{1}_{\mathsf{A}} \mid \mathcal{G}]$ and denoted $\mathbb{P}(\mathsf{A} \mid \mathcal{G})$ $$
\int_{G} \mathbb{P}(\mathsf{A}\mid \mathcal{G}) d\mathbb{P} = \mathbb{P}(\mathsf{A}\cap G)  \qquad \forall \, G\in\mathcal{G}. \qquad \qquad \qquad (1)
$$ A conditional probability is said to be regular if $\mathbb{P}(\cdot \mid \mathcal{G})(\omega)$ is a probability measure for any $\omega\in\Omega$ . Essentially it is a markov kernel satisfying condition $(1)$ . Definition 2 can be found here . This involves an additional random variable $X$ . Definition 2 : Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, $(E, \mathcal{E})$ be a measurable space, and $X:\Omega\to E$ be a random variable with distribution $\mathbb{P}_X = X_*\mathbb{P}$ . Let $\nu:\Omega\times\mathcal{E}\to [0, 1]$ be a markov kernel satisfying $$
\mathbb{P}(\mathsf{A}\cap X^{-1}(\mathsf{B})) = \int_{\mathsf{B}} \nu(x, \mathsf{A}) \,d \mathbb{P}_X(x).
$$ In general, I could not find anywhere the difference or relationship between regular conditional probability and regular conditional distribution","['conditional-probability', 'measure-theory', 'conditional-expectation', 'geometric-measure-theory']"
4418270,Is there a function whose graph intersects every tangent line at exactly 2 points?,"Is there some $f:\mathbb{R}\to\mathbb{R}$ differentiable at every point such that $\forall x$ , the tangent to $f$ at $(x,f(x))$ intersects the graph of $f$ at $2$ points (counting (x,f(x)))? This problem is delicate, as shown by the example $f(x)=x^3$ , where the condition only fails at $x=0$ . Remark: if $f$ is $C^1$ , then the function $g:\mathbb{R}\to\mathbb{R}$ which sends $x$ to the only point $y$ such that $(y,f(y))$ is in the tangent line to $x$ seems to be continuous at almost every point. I could not use this effectively though, in the non continuous case maybe you could get some weak version of this using the intermediate value property of the derivative?","['tangent-line', 'real-analysis', 'continuity', 'functions', 'derivatives']"
4418298,"In every non-separable incomplete inner product space, is there a maximal orthonormal set which is not an orthonormal basis?","Let $X$ be an inner product space. An orthonormal subset $B \subseteq X$ is called: (i) a maximal orthonormal set if there is no other orthonormal subset of $X$ that contains $B$ (ii) an orthonormaml basis if $\text{span}(B)$ is dense in $X$ Consider the following statements about $X$ . (a) $X$ is complete (b) every maximal orthonormal subset of $X$ is an orthonormal basis for $X$ Note 1. The converse of (b) (i.e., every orthonormal basis for $X$ is a maximal orthonormal subset of $X$ ) is always true and easy to prove. Note 2. The proof that (a) implies (b) is in essentially every textbook that covers Hilbert spaces. Note 3. If $X$ is separable, then (b) implies (a). See: Every incomplete inner product space has a maximal but incomplete orthonormal system Example of complete orthonormal set in an inner product space whose span is not dense Non total orthonormal set in a non Hilbert inner product space Question. If X is not separable, does (b) imply (a)?","['hilbert-spaces', 'inner-products', 'functional-analysis']"
4418331,Find the value of $c$ that minimizes the sum of the areas of two regions bounded by $f(x)=x^3-x^2$ and $y=c+x$,"Let $f(x)=x^3-x^2$ . For a given value of $x$ , the graph of $f(x)$ , together with the graph of the line $c+x$ , split the plane up into regions. Suppose that $c$ is such that exactly two of these regions have finite area. Find the value of $c$ that minimizes the sum of the areas of these two regions. My Attempt: Let $x_1,x_2,x_3$ be the roots of the equation $x^3-x^2=c+x$ So, $x_1+x_2+x_3=1$ ; $x_1x_2+x_2x_3+x_3x_1=-1$ ; $x_1x_2x_3=c$ Also, $x^4=2x^2+(c+1)x+c$ Let S be the sum of the two areas. $$S=\int_{x_1}^{x_2}(x^3-x^2-x-c)dx+\int_{x_2}^{x_3}(c+x-x^3+x^2)dx=1-x_2^2+\frac{9c+1}{12}(1-3x_2)$$ I am not able to get $S$ entirely as function of $c$ or $x_2$","['definite-integrals', 'real-analysis', 'maxima-minima', 'calculus', 'algebra-precalculus']"
4418336,Asymptotic formula for twice iterated factorial $(n!)!$,"Being familiar with Stirling's formula for factorial: $$n!\sim\sqrt{2\pi n}\left(\frac n e\right)^n,\quad\color{gray}{n\to\infty}$$ I naïvely assumed that for twice iterated factorial we can simply substitute the right-hand side into itself and write $$(n!)!\,\stackrel{\color{red}{\small\text{wrong}}}\sim\,\sqrt{2\pi \sqrt{2\pi n}\left(\frac n e\right)^n}\left(\frac{\sqrt{2\pi n}\left(\frac n e\right)^n}e\right)^{\sqrt{2\pi n}\left(\frac n e\right)^n},$$ but, as it turns out, I was wrong. Actually, it grows faster than that: $$(n!)!\,\succ\,\sqrt{2\pi \sqrt{2\pi n}\left(\frac n e\right)^n}\left(\frac{\sqrt{2\pi n}\left(\frac n e\right)^n}e\right)^{\sqrt{2\pi n}\left(\frac n e\right)^n}.$$ Can we express the correct asymptotic growth rate of twice iterated factorial $(n!)!$ using only elementary functions and, possibly, also their inverses, such as the Lambert W-function ? Update: Apparently, the same issue arises for simpler functions like $2^{n!}$ . If we simply substitute Stirling's formula for the exponent $n!$ , we will get an incorrect asymptotic.","['factorial', 'number-theory', 'asymptotics', 'analysis', 'lambert-w']"
4418346,How many sub-sums of the harmonic series converge to a given $x$?,"Pick some positive $x$ . $\hspace{0.5mm}$ Let $\mathcal{C}_x$ be the set of all subsets $S \subset \mathbb{Z}^{+}$ such that $\displaystyle{\hspace{1mm} \sum_{n \in S} \frac{1}{n} = x}$ . Is $\mathcal{C}_x$ countable? (There are follow-ups and generalizations of this question one might ask, too, which I omit here to keep it neat. $\hspace{0.5mm}$ However, answers to more general versions of this question are welcome.)","['cardinals', 'sequences-and-series', 'real-analysis']"
