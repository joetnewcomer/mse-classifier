question_id,title,body,tags
4290758,Factoring $4x^2-2xy-4x+3y-3$. Why isn't it working?,"I want to factorise the following expression. $$4x^2-2xy-4x+3y-3$$ Here are the ways I tried $$4x^2-2xy-4x+3y-3=\left(2x-\frac y2\right)^2+3y-3-\frac{y^2}{4}-4x=\left(2x-\frac y2\right)^2+\frac{12y-12-y^2}{4}-4x=\left(2x-\frac y2\right)^2-\frac 14(y^2-12y+12)-4x$$ Now I need to factor the quadratic $y^2-12y+12$ . So, I calculated discriminant $$D=12^2-4\times 12=96\implies \sqrt D=4\sqrt 6.$$ This means that the multipliers of quadratic are not rational.  So I don't know how to proceed anymore.","['contest-math', 'factoring', 'polynomials', 'algebra-precalculus', 'quadratics']"
4290784,Proof the existence of two roots in $(x-1)\ln y=\ln x$ when $y \neq e$,"Problem statement Proof that $y^x\cdot x$ and $yx^2$ for $x, y \ge 1$ have a single intersection point when $y \ge e$ or $y=1$ and at least two when $1<y<e$ . I am not sure if my reasoning has flaws, how to finish it or if there is any other more elegant/simpler way of proving it. I am studying the behavior of these two functions, i.e. when is $y^x\cdot x>yx^2$ . After graphing it, I have seen that in some regimes, one is greater than the other but not after a specific point. I can prove that there is a single solution for $y=e$ but I struggle to prove the other fact. Attempt $$y^x\cdot x=yx^2\ \overset{x\ge1}{\rightarrow}\ y^x=yx$$ Fixing $y$ to $k$ (for clarity of the reasoning): $$k^x=kx\ \rightarrow x\ln k=\ln k + \ln x$$ $$(x-1)\ln k = \ln x$$ And this is not further simplificable, so we have to find where a line intersects the logarithm function. Case 1: $y=e$ Observing that the derivative of $\ln x$ at $x=1$ is $(x-1)\ln y$ for $y=e$ , and the fact that the derivative of a function at a point is the tangent line at that point, this is the only intersection point. Proof: $f(x)=\ln x,\ f(1)=0 \rightarrow f'(x)=\frac{1}{x};\ f'(1)=1 \rightarrow \int f'(x)|_{x=1}\mathrm{d}x=x+c\overset{f(1)=0}{=}x-1 = (x-1)\ln e\quad \blacksquare$ Case 2: $y\neq e$ Here I tried to find a closed expression of the points by Taylor expansion and then find the roots via Ruffini but $f(x)=\ln x$ is not well defined at $x=1$ . Another idea was to try random points below and above $x=1$ and see what approximate root I found via Newton, but neither option seemed feasible. After thinking and playing with the graph a bit more, I realized that $x=1$ is like the z-rotation axis of the line $(x-1)\ln y$ so I decided to go this way: Fact #1: $x=1$ is a trivial solution of $(x-1)\ln y = \ln x\quad\forall\ y\in[1,\infty)$ Proof: $\ln x|_{x=1}\ln 1 = 0 = 0\ln y=(x-1)\ln y|_{x=1}\quad \blacksquare$ . Fact #2: both functions are strictly monotonically increasing.... and I guess this is relevant? And here is pretty much where I am stuck.","['multivariable-calculus', 'proof-writing', 'solution-verification']"
4290786,For complex measure $\mu$ and $0< f \le g$ is it true that $\Big|\int_{\Bbb R^n } f d\mu\Big| \le \Big|\int_{\Bbb R^n } g d\mu\Big|$?,"Let $\mu$ be a complex measure on $\Bbb R^{n}$ and $f,g \in L^1(\mu)$ such that $0< f(x) \le g(x)$ for a.e. $x \in \Bbb R^n$ . Then is it true that $$\Big|\int_{\Bbb R^n } f d\mu\Big| \le \Big|\int_{\Bbb R^n } g d\mu\Big| \text{ ? }$$ I was trying to break each integral first in its real and imaginary part and then their corresponding positive and negative parts, i.e. $$\int_{\Bbb R^n } f d\mu = Re\Big(\int_{\Bbb R^n } f d\mu\Big) +i Im \Big(\int_{\Bbb R^n } f d\mu\Big)$$ $$=Re^+\Big(\int_{\Bbb R^n } f d\mu\Big)-Re^-\Big(\int_{\Bbb R^n } f d\mu\Big)+iIm^+\Big(\int_{\Bbb R^n } f d\mu\Big)-iIm^-\Big(\int_{\Bbb R^n } f d\mu\Big)$$ and then wanted to look at corresponding decompositions of the measure and the integral $\int_{\Bbb R^n } g d\mu$ and obtain inequalities. But I can't figure them out and proceed from here. Can someone please help?","['integration', 'measure-theory', 'analysis']"
4290810,Find inverse of a function $f(x)=x^3+3x^2+4x-1$,"If $f(x)=x^3+3x^2+4x-1$ ,  then $f(x)$ is one to one as it is increasing function for $x \in R$ . I want to find $x$ such that $\frac{-1}{4}x-\frac{7}{16}=2f^{-1}(\frac{-x}{2}-1)$ my attempt: One option is that calculate $f^{-1}$ , inverse of $f$ , and then changes $x$ to $\frac{-x}{2}-1$ in $f^{-1}$ and then solve the equation. I don't know how to find inverse here Could someone help me with this?","['calculus', 'functions', 'inverse-function']"
4290812,low $p$-value and low explained variation connection in multiple regression analysis,"I've just started studying multiple linear regression and I'm stuck at creating a dataset for which a multiple regression model would have a low $p$ -value (the coefficients are non zero) and also low explained variation $R^2$ . The following is what I've come up. To create a dataset with low explained variation it is necessary that the output variable is not in linear correspondence with the predictor variables. very low $p$ -value of coefficients it is necessary that the coefficients are non zero. If predictors are independent the coefficient $i$ th $a_i$ is equal to $\rho(X_i,Y)$ (correlation between $X_i$ and $Y$ ). If $X_i$ and $Y$ are not connected by linear relation, the coefficient $\rho$ is almost zero so this scenario has to be excluded. On the other hand, if I define $Y$ so has to have linear relation to $X_i$ for every $i$ , the condition on explained variation not holds anymore. However, if the predictors are not independent, although I know the formula for the coefficients, it's not very clear to me how to proceed.","['linear-regression', 'statistics', 'regression-analysis']"
4290814,"Prove that $f(x)^2+f(x+1)^2 = f(2x+1)$ for the function $f$ satisfying $f(x)=f(x-1)+f(x-2), f(1)=f(2)=1.$","Prove that $f(x)^2+f(x+1)^2 = f(2x+1)$ for the function $f$ satisfying $f(x)=f(x-1)+f(x-2), f(1)=f(2)=1.$ I know how to prove it, but it is interesting so I am posting it. The first hint is: \begin{align} &\text{Try to prove this first: } \\ &f(x)=f(x-1)+f(x-2) \\ &=2f(x-2)+f(x-3) \\ &=3f(x-3)+2f(x-4) \\ &= 5f(x-4)+3f(x-3) \\ &= \cdot \cdot \cdot \end{align} The second hint is: $\text{According to the first hint: } f(x)=f(k+1)f(x-k)+f(k)f(x-k-1). $ Check this to see that your proof is the same as mine. \begin{align} &f(x)=f(x-1)+f(x-2) = f(2)f(x-1)+f(1)f(x-2). \ \\ \ \\ &f(x-1)=f(x-2)+f(x-3) \\ &\Rightarrow f(x)=f(2)(f(x-2)+f(x-3))+f(1)f(x-2)=(f(1)+f(2))f(x-2) \\ &+f(2)f(x-3)=f(3)f(x-2)+f(2)f(x-3). \ \\ \ \\ &f(x-2)=f(x-3)+f(x-4). \\ &\Rightarrow f(x)=f(3)(f(x-3)+f(x-4))+f(2)f(x-3)=(f(2)+f(3))f(x-3) \\ &+f(3)f(x-4)=f(4)f(x-3)+f(3)f(x-4). \\ &\cdot \\ &\cdot \\ &\cdot \\ &\therefore f(x)=f(k+1)f(x-k)+f(k)f(x-k-1). \\ &x=2k+1; \ f(2k+1)=f(k)^2+f(k+1)^2. \\ \ \\ &\therefore f(x)^2+f(x+1)^2=f(2x+1).\end{align} p.s. if you have another solution, please post it as an answer with a spoiler. (>! another answer with no enter )","['functional-equations', 'functions']"
4290837,A proof of the Cantor-Schroder-Bernstein theorem,To prove this theorem is valid and sufficient use the argument that if there is an injection function $f$ between the sets A and B and there is an injection function $g$ between B and A. The bijection function is $f\bigcup g$ ?,['elementary-set-theory']
4290850,Doubts on roster notation of sets,"Simple examples of sets are often described via roster notation (aka enumeration notation) like $\{0,1,4,9\}$ -  simply write down the elements of the set between the two set delimiters (curly brackets) and separate them by commas. In axiomatic set theory this notation also occurs in the Axiom of Pairing: For any $a$ and $b$ there exists a set $\{a, b\}$ that contains exactly $a$ and $b$ . It is then easy to see that $\{a, b\} =  \{b, a\}$ . More generally one can usually read that ordering and multiple listing of the elements in roster notation is irrelevant, for example we have $\{0,1,2\} = \{1,0,2,1,0\}$ . In axiomatic set theory the concept of an ordered pair $(a,b)$ is not introduced by a separate axiom, but it is somehow defined based on the existing axioms, for example by $(a,b) = \{\{a\},\{a,b\}\}$ . More generally, the concept of an ordered tuple $(a_1,\ldots,a_n)$ can be introduced in that way. Here are my doubts: It seems to me that roster notation is based on an intuitively pre-existing concept of an ordered tuple . Writing down elements sequentially from left to right does not really produce a set, but a tuple. We then have a certain equivalence relation for such tuples which says us when to regard two tuples as the ""same set"". But set theory should not be based on such an intuitive prerequisite. In fact even the axiom of pairing is based on an intuitive concept of a pair. Later re-introducing this concept via a definition seems to be circular. One might argue that $\{a,b\}$ is short for $\{x : x = a \vee x = b\}$ . But even in this notation we actually have a pair of conditions, separated by a $\vee$ instead of a comma. Yes, $x = a \vee x = b$ and $x = b \vee x = a$ are equivalent, but alone to state this fact requires an intuitive understanding of ""ordering"" which axiomatic set theory claims to be reducible to more elementary concepts. In other words: We start with an intuitive concept of pairs or tuples as ingredients of the axioms and later ""cleanly"" re-introduce these concepts using the axioms based on the same (but beforehand unclean) concept. My question: Usually roster notation occurs at the very beginning of texts on set theory. But it involves ingredients which are not really available in a precise sense at that point: Finite sequences of elements, i.e. ordered tuples Permutations of ordered tuples A reduction process eliminating duplicates in tuples One can argue that at the beginning an intuitive understanding of these ingredients is sufficient to get the idea what roster notation means. The precise meaning will be clear after having introduced the necessary ingredients via an axiomatic approach. But the Axiom of Pairing involves roster notation before the above ingredients have been properly defined. Is this a vicious circle? Do we need a new axiomatic approach avoiding this problem? More precisely, do we need an additional axiomatic concept of an ordered pair?","['elementary-set-theory', 'motivation']"
4290928,Orthogonal vectors in complex vector space,"Consider $u,v\in \mathbb{R}^2$ where $u=(2,1), v=(-1, 2)$ . $u$ and $v$ are orthogonal since $u\cdot v=0$ . If we put them in $\mathbb{C}$ , they should be still orthogonal. However, $ \langle u,v \rangle=(2+i)(-1+2i)\ne 0$ . I must misunderstand something here. Could anyone explain this? Thanks in advance. EDIT: $v=(-1, 2)$ should be converted to $-1 + 2i$ not $-1-2i$ .",['linear-algebra']
4290945,Express $\sin(\theta)$ in terms of spherical harmonics,"So we can express trigonometric quantities in terms of Spherical harmonics, for example $\cos(\theta)\propto Y^{0}_{1}(\theta,\phi)$ . Is there a closed expression for $\sin(\theta)$ ? If not, is there at least some kind of convoluted, numerically accessible expression? Best, v. PS: It seems to be an infinite sum of $Y_{l}^0$ with even $l$ , have a look at my numerical estimate:","['spherical-trigonometry', 'trigonometry', 'spherical-harmonics']"
4290994,"General form for $\sum_{n=1}^{\infty} (-1)^n \left(m n \, \text{arccoth} \, (m n) - 1\right)$","I'm wondering if there is a general form for the following sum: $$\sum_{n=1}^{\infty} (-1)^n \left(m n \, \text{arccoth} \, (m n) - 1\right)$$ for $m \in \mathbb{N}$ I have obtained the following closed-forms for these special cases: Where $G$ is Catalan's constant and $\text{Cl}_2$ is the Clausen function of order 2. $$\sum_{n=1}^{\infty}(-1)^n \left(2n \, \text{arccoth} \, (2n) - 1\right)  = \frac{1}{2} - \frac{2G}{\pi}$$ $$\sum_{n=1}^{\infty} (-1)^{n}\left( 3n \, \text{arccoth} \, (3n)-1\right)  = \frac{1}{2} - \frac{5}{2\pi} \, \text{Cl}_2 \, \left(\frac{\pi}{3}\right) + \frac{1}{4} \ln (3)$$ $$\sum_{n=1}^{\infty} (-1)^n \left(4n \, \text{arccoth} \, (4n) - 1\right) = \frac{1}{2}+ \frac{G}{\pi} - \frac{4}{\pi} \, \text{Cl}_2 \, \left(\frac{\pi}{4}\right) - \frac{1}{4} \ln \left(3- 2\sqrt{2}\right)$$ etc. Given that $G = \text{Cl}_2 \left(\frac{\pi}{2}\right)$ , I am curious to know if the general sum is expressible in terms of the Clausen function. These above sums were determined by using the Mittag-Leffler expansion of $\csc (z)$ , i.e $\csc(z) = \frac{1}{z} + 2z \sum_{n=1}^{\infty} (-1)^n \frac{1}{z^2 - \left(\pi n\right)^2}$ and substituting it into the integral $\int_{0}^{\pi/m} x \csc (x) \, dx$ If one uses the following other method, we can determine the odd and even terms of the sums $$\sum_{n=1}^{\infty} \left( 4n \, \text{arccoth} \, (4n)-1\right) = \frac{1}{2} - \frac{G}{\pi}- \frac{1}{4} \ln (2)$$ $$\sum_{n=1}^{\infty} \left( (4n-2) \, \text{arccoth} \, (4n-2) - 1\right) = \frac{G}{\pi} - \frac{1}{4} \ln (2)$$ $$\sum_{n=1}^{\infty} \left(6n \, \text{arccoth} \, (6n) - 1\right) = \frac{1}{2} - \frac{3}{2\pi} \, \text{Cl}_2 \left( \frac{\pi}{3}\right)$$ $$\sum_{n=1}^{\infty} \left( (6n-3) \, \text{arccoth} \, (6n-3) - 1\right) = \frac{1}{\pi} \, \text{Cl}_2 \, \left(\frac{\pi}{3}\right) - \frac{1}{4} \ln(3)$$ $$\sum_{n=1}^{\infty} \left( 8n \, \text{arccoth} \, (8n) - 1\right) = \frac{1}{2} - \frac{2}{\pi} \, \text{Cl}_2 \, \left(\frac{\pi}{4}\right) - \frac{1}{4} \ln (2-\sqrt{2})$$ $$\sum_{n=1}^{\infty} \left( (8n-4) \, \text{arccoth} \, (8n-4) - 1\right) = \frac{2}{\pi} \, \text{Cl}_2 \, \left(\frac{\pi}{4}\right) - \frac{G}{\pi} - \frac{1}{4} \ln(2+\sqrt{2})$$ EDIT I am now interested in $$\sum_{n=1}^{\infty} \left(m n \, \text{arccoth} \, (m n) - 1\right)$$ for $|m|>1$ as asked here. Here is how I originally proved it for the odd terms: For the first odd term sum, begin with the known result $$2G = \int_{0}^{\frac{\pi}{2}} x \csc (x) \, dx$$ then integrate by parts to get: $$2G = \int_{0}^{\frac{\pi}{2}} \ln \left(\cot (x) + \csc (x) \right) \, dx = \int_{0}^{\frac{\pi}{2}} \ln (\cos (x) + 1) \, dx - \int_{0}^{\frac{\pi}{2}} \ln (\sin (x)) \,dx$$ Then use the well-known result $\int_{0}^{\frac{\pi}{2}} \ln (\sin(x)) \,dx = - \frac{\pi}{2} \ln (2)$ and use the identity $\cos (x) + 1 = 2 \cos^2\left( \frac{x}{2}\right)$ and make the substitution $\frac{x}{2} = u$ . $$\implies 2G - \pi \ln (2) = 4 \int_{0}^{\frac{\pi}{4}} \ln (\cos (u)) \, du$$ Now use the Weierstrass product for $\cos (z)$ , namely $\cos(z) = \prod_{n=1}^{\infty} \left(1-\frac{4z^2}{\pi^2 (2n-1)^2}\right)$ to obtain: $$2G - \pi \ln (2) = 4 \sum_{n=1}^{\infty} \int_{0}^{\frac{\pi}{4}} \ln \left( 1-\frac{4u^2}{\pi^2 (2n-1)^2}\right) \, du$$ After integrating, obtain $\pi \sum_{n=1}^{\infty} \ln \left(1-\frac{1}{4(1-2n)^2}\right) = -\frac{\pi}{2} \ln (2)$ and the result quickly follows. The other odd sums are the same idea. The even sums just comes from combining the two results from the alternating sum and the odd term sum.","['integration', 'calculus', 'analysis', 'sequences-and-series']"
4291001,Riordan numbers recurrence,"Let be $C_n$ the $n^{th}$ Catalan's number.
Well, I have the following relation: $$f(n)=\sum_{k=0}^{n}(-1)^{n-k}\binom{n}{k}C_k\text{.}$$ I would like to know, if there is a way to obtain the recurrence: $$f(n)=\frac{n-1}{n+1}(2f(n-1)+3f(n-2))$$ just by the first identity. Good evening! PS: The problem I'm trying to solve il the following: Let be $f(n)$ number of non-crossing partitions of $[n]={1,2,...,n}$ without singeltons, find a recurrence for $f(n)$ . By using the inclusionâ€“exclusion principle, I've obtained the first relation. Now, I would like to find a recurrence.","['catalan-numbers', 'extremal-combinatorics', 'recurrence-relations', 'combinatorics', 'discrete-mathematics']"
4291002,Elementary proof that there is no field with 6 elements,"I am a T.A at an introductory linear algebra course this semester, and before getting into vector spaces, we give them some basic examples and properties of fields. Since this is a first semester course, everything is very elementary and we do not have too many tools. I'd be happy to give them some elementary proof that there is no field of size $6$ (or some other small, concrete number, like 10... I just do not really care about the general case). They already know that $\mathbb Z _6$ is not a field, but I want to show that there is no other possible field of this size. By elementary, I mean something which can be taught to math students on the second week to their first semester (so no arguments using groups and such). I can start talking about characteristics of fields and then show that the size has to be some power of the characteristic, but I'd prefer not to get into all of this. I am simply wondering if anyone here knows of an elementary proof for some concrete size (like $6$ ). Thank in advance!","['field-theory', 'finite-fields', 'abstract-algebra', 'alternative-proof']"
4291023,$f^{-1}(c)$ is measurable for each $c.$ Is $f$ necessarily measurable? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I am trying to answer the following question: Suppose $f$ is a real-valued function on $\mathbb R$ such that $f^{-1}(c)$ is measurable for each $c.$ Is $f$ necessarily  measurable? Here is the solution I found online: ""Let $E$ denote a non-measurable subset of $(0,1).$ We know such a set exists from Theorem $17$ of Chapter $2.$ Consider the function $f$ defined as $$f(x) = e^x . (2 \chi_E - 1)$$ where $\chi_E$ is the characteristic function of the set $E.$ Then $\{x\in \mathbb R: f(x) > 0 \} = E$ is not a measurable set. However $f$ is one-to-one, so $f^{-1}(c)$ is either empty or a singleton set and therefore measurable."" My questions are: I am not sure what is the intuition behind the author defining $f(x)$ in terms of the the characteristic function of the non-measurable function $E$ . And what is the relation between $c$ and the function given in the example. Any elaboration will be greatly appreciated! Also, is there a proof for that the Vitali characteristic function is a nonmeasurable function? or just it is because its domain is nonmeasurable?","['measure-theory', 'lebesgue-measure', 'analysis', 'real-analysis', 'measurable-functions']"
4291052,Is there a (non probability) example of a lambda-system that is not a sigma algebra?,Most the examples given here What is an example of a lambda-system that is not a sigma algebra? are probability examples. Are there any non probability examples?,"['elementary-set-theory', 'measure-theory']"
4291063,Find the negation of the following quantified sentence $(âˆ€x)(p(x)âˆ¨q(x)â†’Â¬q(x))$,"I'm trying to deny a quantified sentence. My attempt: To negate a quantified sentence, I just need to change the quantifier and connectives. So: $$(âˆ€x)(p(x)âˆ¨q(x)â†’Â¬q(x))$$ Denying: $$(âˆƒx) (p(x) âˆ§ ~q(x) âˆ§ q(x))$$ Notes: $\lnot(âˆ€)$ = $âˆƒ$ $\lnot(âˆ¨)$ = $âˆ§$ $\lnot(â†’)$ = $âˆ§$ $\lnot(Â¬q(x))$ = $q(x)$ I'm pretty sure that my solution is correct, the problem is that the denial of the conditional connective is causing me doubts.","['quantifiers', 'predicate-logic', 'logic', 'discrete-mathematics']"
4291076,An explicit relation betwen Riemannian metric and an associated Hermitian metric?,"I am trying to explicitly find the relationship between a Hermitian metric on a complex manifold and the Riemannian metric on the underlying real manifold -- and specifically on how the determinants of both are related. Sorry for the length of what follows. Consider a metric on a (patch of) an even dimensional real manifold $\mathcal{M}$ of dimension $2n$ , where the Riemannian metric is represented as the $(0,2)$ tensor $G_{MN}$ , with $M, N = 1, \dots, 2n$ , and coordinates labelled $X^M$ . Letting $\mu, \nu = 1, \dots, n$ , with \begin{eqnarray}
\label{eq:G-def}
G_{MN} = 
\begin{pmatrix}
g_{\mu\nu}^{(1)} & \vert & g_{\mu\nu}^{(2)} \\
\hline
g_{\mu\nu}^{(3)} & \vert & g_{\mu\nu}^{(4)} 
\end{pmatrix}
\end{eqnarray} $g_{\mu \nu}^{(2)} = g_{\nu \mu}^{(3)}$ , $g_{\mu \nu}^{(1)} = g_{\nu \mu}^{(1)}$ , $g_{\mu \nu}^{(4)} = g_{\nu \mu}^{(4)}$ . Let $X^M = (x^{\mu}, y^{\nu})$ , giving us bases $\{\dfrac{\partial}{\partial x^{\mu}}, \dfrac{\partial}{\partial y^{\nu}} \}$ for $T_p\mathcal{M}$ , $\{ dx^{\mu}, dy^{\nu}\}$ for $T_p^*\mathcal{M}$ , and suppose that $\mathcal{M}$ has an integrable almost complex structure $J$ . Let $z^{\mu} = x^{\mu} + i y^{\mu}$ , $\bar{z}^{\mu} = x^{\mu} - i y^{\mu}$ , and with the $dz^{\mu}$ , $d\bar{z}^{\mu}$ , $\frac{\partial}{\partial z^{\mu}}$ , $\frac{\partial}{\partial \bar{z}^{\mu}}$ defined as above. \begin{eqnarray}
\label{metric-complexified}
ds^2 &=& G_{MN} dX^M \otimes dX^N
\\
%
&=& g_{\mu \nu}^{(1)} dx^{\mu} \otimes dx^{\nu} + g_{\mu \nu}^{(2)} dx^{\mu} \otimes dy^{\nu}
+ g_{\mu \nu}^{(3)} dy^{\mu} \otimes dx^{\nu} + g_{\mu \nu}^{(4)} dy^{\mu} \otimes dy^{\nu}
%
\\
%
&=&
 \dfrac{1}{4}\left(g_{\mu \nu}^{(1)} - ig_{\mu \nu}^{(2)} - ig_{\mu \nu}^{(3)} - g_{\mu \nu}^{(4)}\right)(dz^{\mu} \otimes dz^{\nu})
+ 
 \dfrac{1}{4}\left(g_{\mu \nu}^{(1)} + ig_{\mu \nu}^{(2)} + ig_{\mu \nu}^{(3)} - g_{\mu \nu}^{(4)}\right)(d\bar{z}^{\mu} \otimes d\bar{z}^{\nu})
\\
&&
+
 \dfrac{1}{4}\left(g_{\mu \nu}^{(1)} + ig_{\mu \nu}^{(2)} - ig_{\mu \nu}^{(3)} + g_{\mu \nu}^{(4)}\right)(dz^{\mu} \otimes d\bar{z}^{\nu})
+
 \dfrac{1}{4}\left(g_{\mu \nu}^{(1)} - ig_{\mu \nu}^{(2)} + ig_{\mu \nu}^{(3)} + g_{\mu \nu}^{(4)}\right)(d\bar{z}^{\mu} \otimes dz^{\nu})
 \\
%
&=&
g_{\mu \nu}(dz^{\mu} \otimes dz^{\nu}) + g_{\bar{\mu} \bar{\nu}}(d\bar{z}^{\mu} \otimes d\bar{z}^{\nu})+ g_{\mu \bar{\nu}}(dz^{\mu} \otimes d\bar{z}^{\nu})
 + g_{\bar{\mu} \nu}(d\bar{z}^{\mu} \otimes dz^{\nu})
\end{eqnarray} (I suppose this can be described as the $\mathbb{C}$ -extension of the original metric.) A Riemannian metric $g$ is Hermitian if for any $X,Y \in T_p\mathcal{M}$ , $p\in \mathcal{M}$ , we have $g(J(X),J(Y)) = g(X, Y) $ . We assume that the almost complex structure acts as $J\left(\frac{\partial}{\partial x^{\mu}}\right) = \frac{\partial}{\partial y^{\mu}}$ , $J\left( \frac{\partial}{\partial y^{\mu}}\right) = -\frac{\partial}{\partial x^{\mu}}$ , so that $
J\left(\dfrac{\partial }{\partial z^{\mu}} \right) = i \dfrac{\partial}{\partial z^{\mu}} \,,\quad
J\left(\dfrac{\partial}{\partial \bar{z}^{\mu}}\right) = -i \dfrac{\partial}{\partial \bar{z}^{\mu}}
$ The above condition then forces $g_{\mu \nu} = g_{\bar{\mu}\bar{\nu}} = 0$ . Looking at the metric expansion, we see that this means $g_{\mu \nu}^{(1)} = g_{\mu \nu}^{(4)}$ , while $g_{\mu \nu}^{(2)} = - g_{\mu \nu}^{(3)}$ , in this basis. Using these results, we find that \begin{eqnarray}
G_{MN} \, dX^M \otimes dX^N
&=&
\frac{1}{4}\left(g_{\mu \nu}^{(1)} + g_{\mu \nu}^{(4)} + i\left(g_{\mu \nu}^{(2)} - g_{\mu \nu}^{(3)} \right)\right) dz^{\mu} \otimes d\bar{z}^{\nu}
\\
&&
+
\frac{1}{4}\left(g_{\mu \nu}^{(1)} + g_{\mu \nu}^{(4)} - i\left(g_{\mu \nu}^{(2)} - g_{\mu \nu}^{(3)} \right)\right)
d\bar{z}^{\mu} \otimes dz^{\nu}
%
\\
%
&=& \frac{1}{2}\left(g_{\mu \nu}^{(1)} + ig_{\mu \nu}^{(2)}\right) dz^{\mu} \otimes d\bar{z}^{\nu}
+ \frac{1}{2}\left(g_{\mu \nu}^{(1)} - ig_{\mu \nu}^{(2)}\right) d\bar{z}^{\mu} \otimes dz^{\nu}
%
\\
%
&=& \frac{1}{2} \left( h_{\mu \bar{\nu}}\, dz^{\mu} \otimes d\bar{z}^{\nu} + h_{\bar{\nu} \mu} \, d\bar{z}^{\nu} \otimes dz^{\mu} \right)
\end{eqnarray} Where we call $h_{\mu \bar{\nu}} = \overline{h_{\bar{\nu}\mu}}$ the Hermitian metric on $\mathcal{M}$ . QUESTION Is the above relationship between $h_{\mu \bar{\nu}}$ and $G_{MN}$ correct, and why is it that $\det(G_{MN}) = (\det(h_{\mu \bar{\nu}}))^2$ ? Here is an attempt at an answer. Let $A$ , $B$ , $C$ , $D$ be $n \times n$ matrices.
Then if $C$ , $D$ commute , we have the identity \begin{equation}
\label{det:identity}
\det
\begin{pmatrix}
    A & B \\
    C & D
\end{pmatrix}
= \det(AD - BC)
\end{equation} Using the decomposition of $G_{MN}$ , we find that \begin{eqnarray}
\label{det-G}
\det\left(G_{MN}\right)
&=& \det\left(g_{\mu\nu}^{(1)}g_{\nu\lambda}^{(4)} - g_{\mu\nu}^{2}g_{\nu\lambda}^{3}\right)
= \det\left( \left(g_{\mu\nu}^{(1)}\right)^2 + \left(g_{\mu\nu}^{(2)}\right)^2\right)
\\
&=& \det\left( g_{\mu\nu}^{(1)} + i g_{\mu\nu}^{(2)}\right) \det\left( g_{\mu\nu}^{(1)} - i g_{\mu\nu}^{(2)}\right)
\end{eqnarray} The second equality above comes from the identifications due to hermitian condition described above. The RHS of the last equation is $\det\left(h_{\mu\bar{\nu}}\right) \det\left(h_{\bar{\nu}\mu}\right)$ and we have seen that $h_{\bar{\nu}\mu}=\overline{h_{\mu \bar{\nu}}}$ . So we find that $$ \det\left(G_{MN}\right) = \left|\det\left(h_{\mu\bar{\nu}}\right)\right|^2 $$ or $\det\left(h_{\mu \bar{\nu}}\right) = \sqrt{\det\left(G_{MN}\right)}$ if $h_{\mu \bar{\nu}}$ is real. Problems: $g^{(2)}$ , $g^{(3)}$ do not necessarily commute. But perhaps it is true that these components are somehow required to vanish -- maybe due to the requirement that $\{ \frac{\partial}{\partial x^{\mu}}, \frac{\partial}{\partial y^{\nu}} \}$ form an /orthonormal basis/ -- so that $h_{\mu \bar{\nu}} = g_{\mu \nu}^{(1)}$ .","['complex-geometry', 'almost-complex', 'kahler-manifolds', 'differential-geometry']"
4291109,"Given IID normal random variables $X_1, \ldots, X_n$, show that $(X_1 - \bar{X})/S$ is ancillary.","This has been bugging me for the past day, and I just can't seem to figure it out. Suppose $X_i \sim N(\mu, \sigma^2)$ for $i=1, \ldots, n$ are IID, where $\mu \in \mathbb{R}$ and $\sigma^2>0$ are unknown. I want to show that $Z = \frac{X_1 - \bar{X}}{S}$ is an ancillary statistic (its distribution is independent of $\mu$ and $\sigma$ ). Basically, I need to calculate the distribution of $Z$ . I know that the formula for the density of $Z$ is given by (Shao, Mathematical Statistics , pg. 165 eq (3.1)): $$
 f(z) = \frac{\sqrt{n}\Gamma(\frac{n-1}{2})}{\sqrt{\pi}(n-1)\Gamma(\frac{n-2}{2})} \bigg[ 1 - \frac{nz^2}{(n-1)^2}\bigg]^{(n/2) - 2} I_{(0, (n-1)/\sqrt{n})}(|z|)
$$ but I have no idea how to derive this result. Presumably you use some transformation, but I just can't seem to find the right one. I don't need a full solution, mostly just some help getting started. Thanks! Oh, and $\bar{X}$ and $S^2$ are the sample mean and sample variance, respectively.","['statistics', 'probability-distributions', 'normal-distribution']"
4291110,Solving $\sqrt{x+4}-2 =x$ for $x â‰  0$.,"For $x \neq 0$ , solve $\sqrt{x+4}-2 =x$ Attempt: $x+4=x^2+4x+4 \implies x^2+3x=0$ , $x=0$ or $x=-3$ , but doing $x+2=-1$ , then we just find $x=0$ . So for nonzero $x$ is this an absurd fact?","['algebra-precalculus', 'solution-verification']"
4291112,Relations Between Probability Distributions and Physical Phenomena,"I have created a $3$ -dimensional visualization of the Central Limit Theorem in Mathematica... However, when flipped upside down, from below it looks suspiciously like light being emitted from a flashlight... Common sense might suggest to write this off as a coincidence.  However, I recently stumbled across a link between probability distribution and physical behavior in this video on the heat equation and this video on the Fokker-Planck equation.  I was kind of surprised by this, because probability distributions are abstract, mathematical concepts, while heat evolution and gas diffusion exist in the physical world. How should this fact that partial differential equation solutions describe both probability distributions and physical phenomena such as heat and gas be understood?  Does this occurrence indicate that the evolution of the physical phenomenon is being driven by that probability distribution under the hood?  Is it a coincidence that they both exemplify the same behavior?  Would this be a prime example of ""The Unreasonable Effectiveness of Mathematics"" as described by Eugene Wigner? How seriously should one take finding something like the above ""flashlight"" pattern when playing around with probability distributions?  Might this discovery indicate something substantial about light, or is it just a coincidence dissimilar to the heat-Gaussian and gas-Gaussian relation?","['probability-distributions', 'geometry', 'partial-differential-equations', 'physics', 'probability-theory']"
4291208,How to solve $\left(\sqrt{\frac{x-1}{x}}\right)^{x^2}=\left(\frac{1}{x}\right)^{x+1}$?,"I need help to solve this equation, please. $$\left(\sqrt{\frac{x-1}{x}}\right)^{x^2}=\left(\frac{1}{x}\right)^{x+1}$$ I know that the solution is $x=\varphi$ (the golden ratio). I got this result by equating the bases and the exponents. I want to know if there are another way to obtain this. Thank you.","['golden-ratio', 'real-analysis', 'calculus', 'radicals', 'algebra-precalculus']"
4291240,"Triple integral evaluation, I'm having difficulty understanding the change of the order of integration.","$$\int_{0}^1\int_{0}^z\int_{z}^1e^{y^3}dydxdz + \int_{0}^1\int_{z}^1\int_{x}^1e^{y^3}dydxdz$$ is the problem. The solution states that the result of the sum of the two is $$\int_{0}^1\int_{0}^y\int_{0}^ye^{y^3}dxdzdy$$ which I can't wrap my head around.
Exactly how is the 'conversion' from dydxdz to dxdzdy achieved?","['integration', 'multivariable-calculus', 'multiple-integral']"
4291260,Defining a log spiral from three points,"Let us have three points: $p_0 = (0,0)$ , $p_1 = (a,0)$ and $p_2=(b,c)$ . (We can assume that $b<a$ .) I want to define a log spiral (in polar space) of the form $r=r_0e^{k\theta}$ from some $center=(x,y)$ that passes through the three points. We can assume the spiral passes through $p_0$ when $\theta = 0$ , so that $r_0^2=x^2+y^2$ . We must compute values for $r_0$ , $k$ and another parameter that would allows us to find $centre$ . It is easy to setup a system of equations to represent the problem, but it is very ugly, and I am not sure how to solve it computationally. For example, if $\theta_0 = cos^{-1}[(r_0^2 + r_1^2 - a^2)/2r_0r_1]$ and $\theta_1 = cos^{-1}[(r_0^2 + r_2^2 - d^2)/2r_0r_2]$ , where $d^2=b^2+c^2$ , we can have equations, such as: $r_1 = r_0e^{k\theta_0} = r_0e^{k\times cos^{-1}[(r_0^2 + r_1^2 - a^2)/2r_0r_1]}$ $r_2 = r_0e^{k\theta_1} = r_0e^{k\times cos^{-1}[(r_0^2 + r_2^2 - d^2)/2r_0r_2]}$ and another uglier equation involving the angle $\theta_2$ between $p_1$ and $p_2$ : $r_2 = r_1e^{k\theta_2}$ . There must be a better way of setting the equations to solve the problem, involving a better representation of the problem, for example a complex representation, or some clever geometry. EDIT: Complex Representation We can represents the points in the complex plan as: $p_0 = 0$ , $p_1 = a$ and $p_2=b + ci$ . Then, the problem is equivalent to finding two complex numbers z and s such that the graph of the complex-valued function over the reals $z^\theta + s$ passes through the points. z should not be a muliple of $e^i$ , in which case it will reduce to a circle. In this case, s is the centre of the spiral.","['nonlinear-system', 'trigonometry', 'interpolation', 'complex-numbers']"
4291290,What determines how to treat single variable PDEs and thus their constants of integration?,"When solving a first-order ODE (perhaps there is also a way to extend this to a higher order ODE) for $y(x)$ , it is possible to shift perspective and consider $x$ to be a function of $y$ by manipulating the differential forms.  For example, $\frac{dy}{dx} = h(y)$ can be solved as $\frac{dx}{dy} = \frac{1}{h(y)}$ , which may be easier.  The moral of the story, very crudely stated, seems to be that derivatives are slopes, and we can inquire about the slope of any variable with respect to any other, regardless of which variables are ""actually independent"" at the end of the day. When solving a first-order PDE such as $u_t + 2xu_x = 0$ , one of the steps involves considering either $\frac{dt}{dx}$ or $\frac{dx}{dt}$ , even though both $x$ and $t$ are independent variables, which I'm fine with due to the above paragraph.  However, either of these derivatives are situated within a horizontal slice of $\mathbb{R}^3$ , where $u$ is the dimension orthogonal to that slice.  Thus these derivatives should technically be partials, and the ""ODEs"" they create technically single variable PDEs, and more importantly, the constant of integration encountered when we solve either of these ""ODEs"" should be a function of $u$ .  In this particular problem (solved using the choices made in the above timestamped video), we would get $x = K(u)e^{2t}$ , which produces the correct solution $u = f(xe^{-2t})$ after dividing through by $e^{2t}$ and taking $K^{-1}$ of both sides.  In other solution methods for first-order PDEs, constants are explicitly written as functions . Does thinking about the constant of integration as a function in the context of first-order PDEs always work as it does here, or is this a coincidence?  If not, then why is it correct to use a genuine constant here instead of a function of the dimension not involved in the derivative?  Again, I'm drawing the conclusion from the first paragraph that $u$ not ""actually"" being an independent variable makes little difference in this context. For separation of variables problems, it is similarly common to shorthand partial derivatives as ordinary derivatives once things are separated.  For example, if the PDE is $u_{xx} + u_{yy} = 0$ , and thus one of the separated equations is $Y_{yy} = \lambda Y$ , we often pretend this is the same thing as $Y'' = \lambda Y$ .  However, we are really solving the former, which takes place within a $2$ -dimensional slice of $\mathbb{R}^3$ , not the latter, which takes place within $\mathbb{R}^2$ .  Thus, if we solve it by guessing the particular solution $Y = e^{\sqrt\lambda y}$ and finding the general solution via reduction of order , it seems like we should, for both integrations that occur within reduction of order, write the constants as arbitrary functions of $x$ , since integrating within a ""constant $x$ slice"" of $\mathbb{R}^3$ should yield that arbitrary function.  Why is it correct to use a genuine constant here? Is there a systematic way to decide when integration constants in a three-dimensional problem should be functions, or does it come down to memorizing the decisions presented along with each solution method?","['surfaces', '3d', 'functions', 'partial-differential-equations', 'constants']"
4291304,Find angle between diagonal of a quadrilateral knowing two sides are equal.,"In the following problem, I am trying to find angle $x$ : All the ugly drawn parts are of course my attempts to solve the problem from the data given. I have tried finding similar triangles or anything that can help, to no avail. I seem to have practically every angle, but I'm unable to make progress. The only other idea I've come up with from here is to make a circle with center $C$ in order to apply some circle theorems, but that also led nowhere. Please help me.","['triangles', 'euclidean-geometry', 'trigonometry', 'geometry']"
4291312,Seating $3$ couples around a circular table with husband and wife opposite,"Find the number of ways in which three couples can be seated around a circular table such that husband and wife are always diametrically opposite to each other?
How do I approach this problem? I think the answer should be $2!*2!*2!=8$ but my friend told me that the answer is $3*2!*2!*2!=24$ . I think it should be $8$ only. Can anyone help? PleaseðŸ™ðŸ™. Thank you","['algebra-precalculus', 'combinatorics']"
4291323,"Proving that $y''(x)=\cos(x)+y^3(x)+3x\,y^2(x)\,y'(x)>0.$","Let $y'(x)=\sin(x)+x\,y^3(x)$ and $y(0)=0$ . How can I prove that $y''(x)>0$ for small $x$ ? Got the information $v'=\sin(x), v(0)=0$ and $w'=x\,y^3(x), w\left(\dfrac{\pi}{2}\right)=\dfrac{\pi}{2}.$ My way was: $$y''(x)=\cos(x)+y^3(x)+3x\,y^2(x)\,y'(x).$$ It's $\mathrm{cos}(x)>0$ for $x \in \left[0,\dfrac{\pi}{2}\right]$ . Now I don't know how to show that it's still positive for $y''(x)$ since I don't know what $y(x)$ is. How can it be shown?",['ordinary-differential-equations']
4291326,Proving the identity $e^{J_nA} = cos(A) + J_nsin(A)$ where $J_n^2=-I_n$,"My professor has told me that the following identity is true: $$e^{J_nA} = \cos(A) + J_n\sin(A)$$ where $e^A$ , $\sin(A)$ , and $\cos(A)$ are all matrix functions defined by their Taylor series, $A$ is a $n\times n$ matrix, and $$J_n^2=-I_n$$ I know one possible value of $J_2$ here: $$J_2 = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$$ As a sidenote, n must be even if we want $J_n$ to have only real entries. I am having trouble deriving the identity. I know that the proof for $e^{it}=\cos(t)+i\sin(t)$ involved separating the Taylor series for sin and cos depending on the behavior of $i^n$ . I tried a similar method for the matrix exponential and got here. $$e^{J_nA}=I + \frac{J_nA}{1} + \frac{J_nAJ_nA}{2!} + \frac{J_nAJ_nAJ_nA}{3!}+\dotsb $$ I would like to write it as: $$e^{J_nA}=I + \frac{J_nA}{1} + \frac{-A^2}{2!} + \frac{-J_nA^3}{1}...$$ The identity only works if $J_n$ and $A$ commute, though I can't see how this can be true. It could work if $J_n$ and $A$ are simultaneously diagonalizable, but that would impose restrictions on what matrix $A$ is. Is there a way to prove the identity? Is the identity just not true? I know there are multiple possible values for $J_n$ . Would I need to impose restrictions on what the value of $J_n$ is to get the identity to work? Thank you","['matrix-exponential', 'linear-algebra']"
4291396,Stokes' theorem on the 2-sphere,"Consider the 2-sphere $\mathbb{S}^2\subseteq \mathbb{R}^3$ so that it can be parametrized locally by the spherical coordinates $(\theta,\phi)$ , i.e., $\hat{n} = (\sin \theta \cos \phi, \sin \theta \sin \phi, \cos\theta)$ . Let me define locally a 1-form by $$
A = \frac{1}{2}(1-\cos\theta )d\phi
$$ It's clear (I think) that $A$ can be extended to a smooth 1-form on $\mathbb{S}^2$ .  Now let's say I have a simple closed contour $\gamma$ in $\mathbb{S}^2$ such that $\gamma (s) = (\theta_0, s)$ where $\theta_0$ is constant in this context and $s\in [0,2\pi]$ . Then it's not hard to see that $$
\oint_\gamma A= \pi(1-\cos\theta_0)
$$ Now by Stokes' theorem, it seems that $\gamma$ is the boundary of the ""upper-sphere"" parametrized by $0<\theta < \theta_0$ and $\phi \in (0,2\pi)$ and thus we can apply Stokes' theorem so that $$
\oint_\gamma A = \int_\text{upper} dA = \frac{1}{2} \int_\text{upper} \sin \theta d\theta d\phi
$$ Notice that $\sin \theta d\theta d\phi$ is the induced surface measure and thus the RHS is just the solid angle $\Omega(\text{upper})$ enclosed by $\gamma$ divided by 2, i.e., $=\Omega(\text{upper})/2$ . Now this is all good so far. However, one can although think of $-\gamma$ (reversed orientation) as the boundary of the ""lower sphere"", i.e., that parametrized by $\theta_0 <\theta<\pi$ . In this case, you would think that you could apply Stokes' theorem again so that $$
\oint_\gamma A = -\oint_{-\gamma} A = -\int_\text{lower} dA = -\frac{\Omega(\text{lower})}{2}
$$ However, this obviously can't be true since this would imply that the solid angle of the entire 2-sphere is $$
\Omega(\text{upper}) +\Omega(\text{lower})=0
$$ I think this is somehow related to the Chern number, but I don't quite understand where I went wrong in my reasoning. EDIT . After a closer look, it seems that $A$ might not be well-defined near the south pole, i.e., $\theta = \pi$ . Indeed, near the south pole, it seems that $A\sim d\phi$ which would mean any sufficiently small contour around the south pole would give $\oint_\gamma  A = 2\pi$ and does not converge to zero. Is this the reason why the above reasoning doesn't work?","['stokes-theorem', 'spherical-coordinates', 'differential-forms', 'differential-geometry']"
4291419,"Is $T_1+T_2$ planar if $T_1,T_2$ are trees with the same vertices?","I have two trees $T_1=(V,E_1)$ and $T_2=(V,E_2)$ . My question is, supposing that $G=(V,E_1\cup E_2)$ , can we conclude that $G$ is planar? I think it's planar, because $G$ is created by the union of two trees.
Also i read this post that shows union of two planer graph my not be planer.","['graph-theory', 'trees', 'discrete-mathematics', 'planar-graphs']"
4291421,Is this a mutually exclusive event?,"$\begin{array}{|c|c|c|c|}
         \hline
         & X = a & X = b & \text{Total}  \\
         \hline
         Y = c & 20 & 70 & 90 \\
         \hline
         Y = d & 15 & 45 & 60 \\
         \hline
         \text{Total} & 35 & 115 & 150 \\
         \hline
\end{array}$ $P(X = a\hspace{.5 em} OR\hspace{.5 em} Y \ne c)$ Since X can be and Y can be not c at the same time, I'm guessing they are not mutually exclusive so I cannot use the following formula. $P(X = a\hspace{.5 em} OR\hspace{.5 em} Y \ne c) = P(X = a) + P(Y \ne c) = \frac{95}{150}$ I just need to verify this is wrong or right.",['statistics']
4291427,Cut two squares into rectangles to reassemble a single square.,"This problem would belong to puzzling SE, except that I suspect it to be impossible.  So I post it here, to see if someone can provide an argument proving the impossibility. The problem: Is it possible to cut two unit squares into a finite number of rectangles and rearrange these into a solid square of side length $\sqrt 2$ ?  (without overlap, discarding of parts, holes in the assembly, or such.) A more general version of the problem has been posted already Can a square be cut parallel to its sides to make a rectangle of non-square-rational proportion? but it didn't receive an answer.","['dissection', 'geometry']"
4291429,"solve for $20x+15 \equiv 47 \pmod{4},$ if there are no solution why? [duplicate]","This question already has answers here : Noninvertible scalings of congruences may yield extraneous roots (4 answers) Solving linear congruences by hand: modular fractions and inverses (5 answers) Closed 2 years ago . solve for $20x+15 \equiv 47 \pmod{4},$ if there are no solution why? I tried to do it the following way, but i'm wondering if it is the way it should be done. Is it correct? $$20x+15 \equiv 47 \pmod{4},$$ subtract $15$ from $47$ , $$20x \equiv 32\pmod{4},$$ divide both sides by $4$ , $$5x \equiv 8\pmod{4},$$ $$x \equiv 0.$$","['modular-arithmetic', 'discrete-mathematics']"
4291464,Derivation of Mehler kernel,"I'm trying to get an intuitive feel for Fourier multipliers and using it to derive formulas for kernels. What I do know for instance is that the Laplacian $\Delta$ is multiplication by $M = -\lvert{\xi\rvert}^2$ in Fourier space, in the sense $\Delta = F^{-1} \circ M \circ F$ where $M$ is the multiplication operator, and that this idea can be used to find the heat kernel: the action of $e^{t\Delta}$ is then multiplication by $M' = e^{-t\lvert{\xi\rvert}^2}$ in Fourier coordinates and so $$ e^{t\Delta}f(x) = (F^{-1} \circ M'\circ F)f(x) = (2\pi)^{-n} \int\int e^{i\langle{x-y, \xi\rangle}}e^{-t\lvert{\xi\rvert}^2}f(y)dyd\xi. $$ We can then swap the order of integration to get $$\int e^{i\langle{x-y, \xi\rangle}}e^{-t\lvert{\xi\rvert}^2}d\xi = \pi^{n/2}t^{-n/2} e^{-\lvert{x-y\rvert}^2/(4t)} $$ which multiplied by $(2\pi)^{-n}$ gives the correct normalisation factor $(4\pi t)^{-n/2}$ out front. For the Ornstein-Uhlenbeck operator $L = -\frac{1}{2}\Delta + x \cdot \nabla$ , I am trying to apply the same ideas to get the Mehler kernel which according to http://www.math.chalmers.se/Math/Research/Preprints/2012/12.pdf (p14) is given by $$ M_t(x, y) = \frac{1}{\pi^{n/2}(1 - e^{-2t})^{n/2}} \exp\left(-\frac{\lvert{y-e^{-t}x\rvert}^2}{1-e^{-2t}}\right). $$ If I apply the same ideas, $L$ in Fourier coordinates should be multiplication by $\frac{1}{2}\lvert{\xi\rvert}^2 + i\langle{x, \xi\rangle}$ so I should be able to recover the kernel of $e^{-tL}$ through the integral $$ \int e^{i\langle{x-y, \xi\rangle}}e^{-\frac{t}{2}\lvert{\xi\rvert}^2 - it\langle{x, \xi\rangle}} d\xi $$ but I can't get this to agree even in dimension $1$ . What am I doing wrong?","['functional-analysis', 'analysis', 'partial-differential-equations']"
4291546,The ODE modeling of gradient descent with diminishing stepsize,"The gradient descent (GD) with constant stepsize $\alpha^{k}=\alpha$ takes the form $$x^{k+1} = x^{k} -\alpha\nabla f(x^{k}).$$ Then, by constructing a continuous-time version of GD iterates satisfying $X(k\alpha)=x^{k}$ and taking $\alpha\to 0$ , we could obtain a limiting ode for constant-stepsize GD of the form $$\lim_{\alpha\to 0}\frac{X(t+\alpha)-X(t)}{\alpha} = \nabla f(X(t))\Rightarrow\frac{dX(t)}{dt} = -\nabla f(X(t)).$$ My question is that if we use the diminishing stepsize with the form $\alpha^k = \alpha/(k+1)$ , could we derive the corresponding ODE as $\alpha\to 0$ .
I guess, the limiting ode for diminishing-stepsize GD might take the form of $$\frac{dX(t)}{dt} = -\frac{1}{t+1}\nabla f(X(t)).$$ Thanks!","['gradient-flows', 'ordinary-differential-equations', 'discrete-optimization', 'optimization', 'gradient-descent']"
4291569,"Relation between ""$\lim_{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}$"" and ""$\lim_{x \rightarrow a} f(x)$""?","Perhaps I'm not understanding of it myself but to me there is a 'disconnect' between limit and derivative definition of the limit: Let a function f(x) be defined on a deleted neighborhood around x = a.
Then we say that, $\lim_{x \rightarrow a} f(x) = L$ if for every $\varepsilon > 0$ , there is some $\delta > 0$ s.t. $|f(x) - L| < \varepsilon $ whenever $0 < |x-a| < \delta$ definition of derivative: $$ f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}$$ My problem/question is that the definition of limit is to a function and in a derivative which has a limit built-in into it, it is to a variable, h, not related to a function. Am I wrong in this thinking? (feel free to answer in terms of analysis if necessary) From what I understand, a limit has you take closer and closer approximations of a function around a particular value. A derivative has you take slopes whose points get closer and closer hence why the h goes to 0.","['limits', 'calculus', 'definition', 'derivatives']"
4291652,Matrices and influence of small error on their inverse matrices.,"Find (nontrivial) matrix $A\in M_n$ such, that only small error in its element has small influence on inverse matrix, it means that this matrix will be different from $A^{-1}$ only little bit. Then find (nontrivial) matrix $B\in M_n$ such, that only small error in its element has bigg influence on inverse matrix, it means that this matrix will be different from $B^{-1}$ a lot. I have already find these matrices. Consider matrix real matrix $A$ $$A=\begin{pmatrix}0 & 1\\
1 & 1\end{pmatrix},$$ then the inverse matrix $A^{-1}$ is $$A^{-1}=\begin{pmatrix}-1 & 1\\
1 & 0\end{pmatrix}.$$ Now consider matrix $A+\Delta A$ in the form $$A+\Delta A=\begin{pmatrix}\frac{1}{1000} & 1\\
1 & 1\end{pmatrix},$$ then the inverse matrix $A^{-1}$ is $$(A+\Delta A)^{-1}=\begin{pmatrix}-\frac{1000}{999} & \frac{1000}{999}\\
\frac{1000}{999} & -\frac{1}{999}\end{pmatrix}.$$ It means that $A^{-1}$ and $(A+\Delta A)^{-1}$ differ only little bit. But when I consider matrix $B$ $$B=\begin{pmatrix}\frac{1001}{1000} & 1\\
1 & 1 \end{pmatrix},$$ then the inverse matrix $B^{-1}$ is $$B^{-1}=\begin{pmatrix}1000 & -1000\\
-1000 & 1001\end{pmatrix}.$$ Now consider matrix $B+\Delta B$ in the form $$B+\Delta B=\begin{pmatrix}\frac{1002}{1000} & 1\\
1 & 1 \end{pmatrix},$$ then the inverse matrix $A^{-1}$ is $$(B+\Delta B)^{-1}=\begin{pmatrix}500 & -500\\
-500 & 501\end{pmatrix}.$$ It means that $B^{-1}$ and $(B+\Delta B)^{-1}$ differ a lot. But furthermore what I should do, it is not only to find matrices, but also formally justify it for any matrix. The hint is that I should use knowledges about decompositions of matrices and about condition number, also I can use this relation $$\frac{||(A+\Delta A)^{-1}-A^{-1}||}{||A^{-1}||}\leq \kappa(A)\frac{||\Delta A||}{||A||},$$ where $\kappa(A)$ is condition number of matrix $A$ . I think that it can be something with regularity and singularity of matrix. My matrix $A$ is  ""strong"" regular and difference between these two inverse matrices was small. On the other hand the matrix $B$ was near to singular and the difference between inverse matrices was bigg. I have not idea how to deal with formally and more generally. Any help will be appreciated. Thank you very much.","['matrices', 'condition-number', 'linear-algebra', 'matrix-decomposition']"
4291668,"Proof verification: if $f$ is a solution of $(y')^2+y^2=1$ that is defined over $\mathbb{R}$, then $f$ attains at least one of $1,-1$ as a value.","Context : I'd like to prove that for any pair of numbers $(x_0,y_0)$ with $|y_0|\leq 1$ , the initial value problem $$(y')^2+y^2=1,y(x_0)=y_0$$ will have infinitely many solutions that are defined over $\mathbb{R}$ . In my analysis of the differential equation, I've determined that it locally has exactly two solutions when $|y_0|<1$ , each a sine wave, and that these ""branch off"", so to speak, to infinitely many when their graphs reach $\pm 1$ . Since I'm interested in solutions defined over $\mathbb{R}$ , I can prove the infinitude of solutions by showing that any $\mathbb R$ -defined solution attains either $1$ or $-1$ as a value. Problem : if $f$ is a $\mathbb R$ -defined solution of $(y')^2+y^2=1$ , is it true that $f(x_0)=1$ or $f(x_0)=-1$ for some $x_0\in\mathbb R$ ? I've been thinking about this problem for a while now, and believe I have conceived of an argument that proves the answer is yes. Here it is: Proof : suppose, for the sake of finding a contradiction, that $f$ is a $\mathbb R$ -defined solution of $(y')^2+y^2=1$ that never attains $1$ nor $-1$ as a value. Since the differential equation implies that $f$ must satisfy $-1\leq f(x)\leq 1$ for every $x\in\mathbb R$ , it follows from our assumption that $-1<f(x)<1$ for every $x$ , or $\left|f(x)\right|<1$ . Note that the ODE is equivalent to $|y'|=\sqrt{1-y^2}$ , so $\left|f(x)\right|<1$ and the fact that $\sqrt{1-t^2}>0$ when $|t|<1$ implies that $|f'(x)|>0$ for every $x$ . This yields the following result: Result 1 : For any $x$ , either $f'(x)>0$ or $f'(x)<0$ . For what follows, we'll need to strengthen Result $1$ to the following: Result 2 : $f'$ is either strictly positive or strictly negative. Proof : From Result $1$ , we have that either $f'(0)>0$ or $f'(0)<0$ . Suppose $f'(0)>0$ and pick any $x_1\neq 0$ . From Result $1$ , it follows that either $f'(x_1)>0$ or $f'(x_1)<0$ . Notice that it is not possible for $f'(x_1)<0$ to be true because Darboux's theorem would then imply that $f'(c)=0$ for some $c$ between $0$ and $x_1$ , contradicting Result $1$ . Thus, $f'(x_1)>0$ ; since $x_1$ was arbitrary, this proves that $f'$ is strictly positive. The argument for $f'(0)<0$ is pretty much identical, so it is omitted. Thus, $f'$ is either strictly positive or strictly negative. With this established, it follows that either $$f'(x)=\sqrt{1-[f(x)]^2}\text{ for every }x\text{, or }f'(x)=-\sqrt{1-[f(x)]^2}\text{ for every }x$$ or, because $\sqrt{1-[f(x)]^2}>0$ , $$\frac{f'(t)}{\sqrt{1-[f(t)]^2}}=1\text{ for every }t\text{, or }\frac{f'(t)}{\sqrt{1-[f(t)]^2}}=-1\text{ for every }t$$ Integrating each case from $x_0$ to $x$ , using $\int 1/\sqrt{1-x^2}dx=\sin^{-1}(x)$ , and rearranging, we get $$f(x)=\sin\left(\sin^{-1}\left(f(x_0)\right)+x-x_0\right)\text{ for every }x$$ $$\text{or}$$ $$f(x)=\sin\left(\sin^{-1}\left(f(x_0)\right)-(x-x_0)\right)\text{ for every }x$$ If the correct expression for $f$ is the first one, then we can evaluate it at $\frac{\pi}{2}+x_0-\sin^{-1}\left(f(x_0)\right)$ and $-\frac{\pi}{2}+x_0-\sin^{-1}\left(f(x_0)\right)$ . This is justified because the domain of $f$ is $\mathbb{R}$ . \begin{align}
f\left(\frac{\pi}{2}+x_0-\sin^{-1}\left(f(x_0)\right)\right) &= \sin\left(\sin^{-1}\left(f(x_0)\right)+\frac{\pi}{2}+x_0-\sin^{-1}\left(f(x_0)\right)-x_0\right)\\
&= \sin\left(\frac{\pi}{2}\right)\\
&= 1
\end{align} \begin{align}
f\left(-\frac{\pi}{2}+x_0-\sin^{-1}\left(f(x_0)\right)\right) &= \sin\left(\sin^{-1}\left(f(x_0)\right)-\frac{\pi}{2}+x_0-\sin^{-1}\left(f(x_0)\right)-x_0\right)\\
&= \sin\left(-\frac{\pi}{2}\right)\\
&= -1
\end{align} This contradicts the assumption that $f$ never attains $1$ nor $-1$ . We can reach a contradiction from the second expression too; just evaluate $f$ at $\frac{\pi}{2}+x_0\color{red}{+}\sin^{-1}\left(f(x_0)\right)$ and $-\frac{\pi}{2}+x_0\color{red}{+}\sin^{-1}\left(f(x_0)\right)$ instead. Thus, our original assumption must be false, so any $\mathbb R$ -defined solution always attains at least one of $1,-1$ as a value. $\blacksquare$ I appreciate any and all feedback.","['calculus', 'solution-verification', 'ordinary-differential-equations']"
4291705,What is the meaning of integral structure and generic fibreï¼Ÿ,"In Peter Scholze's perfectoid spaces , Almost mathematics , I have difficulty understanding the sequence of localization functors $$
K^{\circ}-\textrm{mod}\rightarrow K^{\circ}-\textrm{mod}/(\mathfrak{m}-\textrm{torsion})\rightarrow K^{\circ}-\textrm{mod}/(p-\textrm{power torsion}),
$$ which says that it is the functor of passing from an integral structure to its generic fibre. Here are my questions: What do ''integral structure'' and ''generic fibre'' mean in this context? Why $K^{\circ}-\textrm{mod}/(\mathfrak{m}-\textrm{torsion})\rightarrow K^{\circ}-\textrm{mod}/(p-\textrm{power torsion})$ is a localization functor? And why $K^{\circ}-\textrm{mod}/(p-\textrm{power torsion})$ is equivalent to $K-\textrm{mod}$ ?","['homological-algebra', 'algebraic-geometry', 'abstract-algebra', 'arithmetic-geometry']"
4291723,Is this Proof clear? Proof that $a^{1/n}$ converges to $1$ wherer $n$ is a natural number and $a>0$ is a real number,"Proof that $a^{1/n}$ converges to $1$ wherer $n$ is a natural number and $a>0$ is a real number Is this proof well written? Probably not. If not can anyone help me how to improve it? Step 0 Let $n$ denote a natural number. We are going to use the lemma that $a \leq b$ $\iff $ $a^n \leq b^n$ where $a,b$ are two real numbers $>0$ . We are also going to use the squeeze theorem and the binomial theorem. We will first construct a lower bound then an upper bound for $a \geq 1$ and then we are going to generalize it for $0<a<1$ . Step 1 Let $a \geq 1$ . We will show that $1 \leq a^{1/n}$ . This is equivalent to $1 \leq a$ . But this is true by assumption and thus we have a lower bound. Step 2 Lets consider $a^{1/n} \leq 1 +\frac{a}{n}$ . This is equivalent to $a \leq (1+\frac{a}{n})^{n}$ . We can use the binomial theorem here and we find that it is equal to $1+n \frac{a}{n}1^{n}+...=1+a+...$ . Then we combine this and we have that $a \leq 1+a+...$ but this is obviously true. Step 3 We now have achieved the following for a real number $a \geq 1$ we have that $1 \leq a^{1/n} \leq 1+a/n$ . And by the squeeze theorem this converges to 1. Step 4 Last and least we need to show the same thing for $0 <a <1$ . We can rewrite this into $a^{1/n}=\frac{1}{(\frac{1}{a})^{1/n}}$ . But because if $a <1$ then $1/a > 1$ but because this is also a real number we can use the theorem we just proved and apply the quotient rule for limits. Thus we are done.","['limits', 'proof-writing', 'analysis']"
4291760,Prove uniqueness of solution in ODE,"I need to find function $f(x)$ , such that $x > 0$ and $f(x) > 0$ , so that the area limited by the function, the $y$ and $x$ axes and $x = a$ ( $a > 0$ ) is $f^3(x)$ for every $a$ , and prove that there is only one such function. Let $y' = f(x)$ and $y = \int^{x}{f(t)dt}$ I get the ODE $(y')^3 = y \Longrightarrow y'=\sqrt[3]{y}$ which is separable thus I get $y = \sqrt[3]{(\frac{2}{3}(C+x))^2}$ . But I'm not sure how I'm supposed to prove uniqueness. The existence and uniqueness theorem is defined for $D = \{(x, y)| \alpha < x <\beta, \gamma < y < \delta \}$ where $y$ is continuous and the $y$ partial derivative exists if $(x_0, y_0)\in D$ then there is a solution for the initial value problem with $(x_0, y_0)$ and that solution is unique, but can I use this here? and if so, how?",['ordinary-differential-equations']
4291771,"If $a$ is not a power of $10$, when $\lim\limits_{n\to\infty} S(a^n)=\infty$? ($s(m)$ is the sum of the digits of $m$.) [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question If $a$ is not a power of $10$ , when $$\lim\limits_{n\to\infty} S(a^n)=\infty?$$ ( $s(m)$ is the sum of the digits of $m$ .) (I don't have any ideas to prove this.)","['limits', 'number-theory']"
4291773,"If you sum all possible subsets of peoples' birthday dates in a group of 7 people, will two sets always have the same sum?","Suppose for a set $S$ of 7 people, $f(s)=$ the birthday date of the person for each person in $S$ . For example if their birthday is May 17th then $f(s)=17$ . Now for every subset $T \subset S$ let $g(T)$ be the sum of all $f(s)$ with $s \in T$ . Is the function $g$ ever an injection? In other words, prove whether or not two distinct subsets of $S$ will always have the same $g$ value. This was a bonus problem on the final exam of an introductory proofs class I took last year, and it has been bothering me ever since then because no solution was ever posted. I think this would be reasonably simple to check with a computer using brute force, but that was besides the point of the class. A similar problem where $S$ is instead a set of 8 people is trivial to prove using the Pigeonhole Principle. In that case, the codomain of $g$ will have $8 \cdot 31 =248$ elements while $\mathcal{P}(S)$ will have $2^8 = 256$ elements, so therefore $g$ is not injective. Clearly this same strategy does not apply to a group of 7 people, but is there another way of approaching this problem in order to prove whether or not $g$ is injective? My first thought was that even though $2^7 = 128 < 217=31 \cdot{7}$ , there could maybe be some way to analytically reduce the 217 possible sums until there are less than 128? It seems intuitively like there should exist an $S$ where $g$ is injective, but without using brute force I couldn't produce one.","['pigeonhole-principle', 'proof-writing', 'functions', 'probability']"
4291774,Caratheodory's theorem with two vectors,"Let $S\subset \mathbb R^n$ be given. Take $x\in conv(S)$ . Then by Caratheodory's theorem, we can find $n+1$ vectors in $S$ such that $x$ is in the convex hull of these vectors, i.e., there is $S'\subset S$ with $|S'|\le n+1$ such that $x\in conv(S')$ . Now suppose we have $x,y\in conv(S)$ . Then Caratheodory's theorem implies there is $S'\subset S$ with $|S'|\le 2n+2$ such that $x,y\in conv(S')$ . My question is: can this upper bound on $|S'|$ be reduced to $2n+1$ or $2n$ ? If $n=1$ then clearly $|S'|=2=2n$ suffices. For $n=2$ it seems to be the case, too (no proof). The example in the answer to the question Generalization of Caratheodory's theorem shows that $|S'|< 2n$ is not possible in general.","['polytopes', 'convex-hulls', 'convex-geometry', 'geometry', 'convex-analysis']"
4291783,"How many 4-permutations of the positive integers not exeeding 100 contain three consecutive integers k, k + 1, k + 2, in the correct order","There is a discrete math question I am strugguling to understand: ""How many 4-permutations of the positive integers not exceeding 100
contain three consecutive integers k, k + 1, k + 2, in the correct
order"" a) where these consecutive integers can perhaps be separated by other integers in the permutation? b) where they are in consecutive positions in the permutation? We know that the number of permutations with repetition for a is $98*97*4$ since for every 98 possible choices of k we have 97*4 possible 4-permutations, (arrangements) of k,k+1,k+2 and another number different from those 3. But how can I compute the repreated permutations? For instance, consider k=1. Then some of the possible permutations are $""1,2,3,4"",
""4,1,2,3"",
""1,4,2,3""$ which are also permutations for k=2. I think that given a number k I should find the 4-permutations of k that are repeated in other permutations. Then i will need to multiply this value for 98, the number of total possible Ks and subtract this from the first result. But I do not know how can I compute them. Sorry if the question has already been made, but my point of interest regards how to find the repetitions I am interested in. Thanks in advance","['permutations', 'combinatorics', 'discrete-mathematics']"
4291794,Problem on counting,"Let $S_1,...,S_{10}$ be 10 sets with exactly 8 elements, and $|S_i\cap S_j|\le 2$ . Problem: what is $\min(|S_1\cup...\cup S_{10}|)$ In general, if we have n sets $S_1,...,S_n$ with $|S_i|=N$ and $|S_{i_1}\cap...\cap S_{i_k}|\le m$ for every $\left\{i_1,...,i_k\right\}\subset\left\{1,...,n\right\}$ , is there a way to find $\min(|\cup_{j=1}^n S_j|)$ , or, to find an upper bound or lower bound for it? Background: I am trying to prove that every group with 180 elements is not simple. By Sylow theorem, the number of Sylow-5 subgroups of a simple group with 180 elements can only be 6 or 36.If it contains exactly 6 Sylow-5 subgroups, then it can be embedding into $A_6$ , which is impossible. Hence I tried to prove that it can not contain 36 Sylow-5 subgroups. I tried to make a contradiction by counting the number of elements. If it contains 36 Sylow-5 subgroups, it must contain 144 elements with order 5. And it turns out that such a group can only contain exactly 10 Sylow-3 subgroups. Let $S_1,..., S_n$ be these groups, if $|S_1\cup ...\cup S_{10}|\ge 37$ , then the proof was finished. At first, I think the minimum takes place when $|S_1\cap...\cap S_{10}|=3$ , but I can't prove it ( and it doesn't seem right ). To begin with the problem, I tried to consider the problem without group structure, and got the above problem.",['combinatorics']
4291842,"How can we calculate the derivative of Hessian, namely, the third derivative?","I have a problem when calculate the following derivative which is supposed to be $\mathbf{R}^{2\times 2}$ since $t\in \mathbf{R}$ . According to the chain rule, $$\tag{1}
\frac{d}{dt}\nabla^2f(x+tv)|_{t=0}=\nabla^3f(x+tv)|_{t=0}\cdot \frac{d(x+tv)}{dt}=\nabla^3f(x)\cdot v
$$ where $x,v\in \mathbf{R}^2$ . The gradient of $f(x)$ w.r.t $x$ is $$
\nabla f(x)=\left[\begin{array}{c}\frac{\partial f}{\partial x_1}\\
\frac{\partial f}{\partial x_2}\end{array}\right]
$$ The Hessian is $$
\nabla^2 f(x)=\nabla\nabla^T f(x)=\left[\begin{array}{cc}\frac{\partial^2 f}{\partial x_1^2}&\frac{\partial^2 f}{\partial x_2\partial x_1}\\
\frac{\partial^2 f}{\partial x_1\partial x_2}&\frac{\partial^2 f}{\partial x_2^2}\end{array}\right]
$$ However, how can I calculate $\nabla^3f(x)$ ? My strategy is to take derivative on each entry of $\nabla^2f(x)$ w.r.t $x$ , i.e., a column vector $\frac{\partial[\nabla^2f]_{ij}}{\partial x}=[\frac{\partial[\nabla^2f]_{ij}}{\partial x_1};[\frac{\partial[\nabla^2f]_{ij}}{\partial x_2}]$ , $$
\nabla^3f(x)=\frac{\partial \nabla^2f(x)}{\partial x}=\left[\begin{array}{cc}
\frac{\partial^3 f}{\partial x_1^3}&\frac{\partial^3 f}{\partial x_2\partial x_1^2}\\
\frac{\partial^3 f}{\partial x_1^2\partial x_2}&\frac{\partial^3 f}{\partial x_2^2\partial x_1}\\
\frac{\partial^3 f}{\partial x_1^2\partial x_2}&\frac{\partial^3 f}{\partial x_2^2\partial x_1}\\
\frac{\partial^3 f}{\partial x_1\partial x_2^2}&\frac{\partial^3 f}{\partial x_2^3}
\end{array}\right]$$ The result is $\mathbf{R}^{4\times 2}$ .Continue our calculation $$
\nabla^3f(x)\cdot v=\left[\begin{array}{cc}
\frac{\partial^3 f}{\partial x_1^3}&\frac{\partial^3 f}{\partial x_2\partial x_1^2}\\
\frac{\partial^3 f}{\partial x_1^2\partial x_2}&\frac{\partial^3 f}{\partial x_2^2\partial x_1}\\
\frac{\partial^3 f}{\partial x_1^2\partial x_2}&\frac{\partial^3 f}{\partial x_2^2\partial x_1}\\
\frac{\partial^3 f}{\partial x_1\partial x_2^2}&\frac{\partial^3 f}{\partial x_2^3}
\end{array}\right]\left[\begin{array}{c}v_1\\v_2\end{array}\right]=
\left[\begin{array}{c}
\frac{\partial^3 f}{\partial x_1^3}v_1+\frac{\partial^3 f}{\partial x_2\partial x_1^2}v_2\\
\frac{\partial^3 f}{\partial x_1^2\partial x_2}v_1+\frac{\partial^3 f}{\partial x_2^2\partial x_1}v_2\\
\frac{\partial^3 f}{\partial x_1^2\partial x_2}v_1+\frac{\partial^3 f}{\partial x_2^2\partial x_1}v_2\\
\frac{\partial^3 f}{\partial x_1\partial x_2^2}v_1+\frac{\partial^3 f}{\partial x_2^3}v_2
\end{array}\right]
$$ The result is $\mathbf{R}^4$ not $\mathbf{R}^{2\times 2}$ . Where is wrong? Let's think about in the view of linear approximation $$\tag{2}
\nabla^2f(x+tv)\approx\nabla^2 f(x) + \nabla^3f(x)v\cdot t
$$ (2) does not make sense due to the inconsistent dimension on the RHS. Specifically, the first term is $2\times 2$ but the second term is $4\times 1$ . Where is wrong with my derivations?
Still, $$\tag{3}
\frac{d}{dt}\nabla^2f(x+tv)|_{t=0}=\nabla^3f(x)v
$$ Who can help me point out the bugs, fix the bugs and get the correct form? Any instruction will be appreciated.","['partial-derivative', 'hessian-matrix', 'derivatives']"
4291857,"An exercise in ""Mathematical Statistics Jun Shao"" about the completeness of a 'modified' exponetial family","It is not the first time meeting this problem in StackExchange and I have read the answer to it(the original solution is copied at the bottom, also available in Show a statistic is complete but not suffcient , the idea is checking the completeness by definition). But it seems that he wrongly uses the inverse property of the two-sided Laplace transformation because now the transformation equality does not hold for every real value(not for $0$ ). So what should I do now because Laplace transformation is a direct and powerful tool to solve this kind of problem(like proving completeness in the general exponential family) but it does not work here, Are there any related properties that can fix it? Or other ideas? Let $X_{1}, \ldots, X_{n},(n \geq 2)$ be i.i.d. random variables having the normal distribution $N(\theta, 2)$ when $\theta=0$ and the normal distribution $N(\theta, 1)$ when $\theta \in \mathbb{R}-\{0\}$ . Show that the sample mean $\bar{X}$ is a complete statistic(sorry for missing this one before, this is the main concern of the question) but not a sufficient statistic for $\theta$ . $$
\begin{aligned}
\mathrm{E}(g(\bar{X})) &=\int_{\mathbb{R}} g(u) f_{\bar{X}}(u) d u=\int_{\mathbb{R}} g(u) \frac{1}{\sqrt{2 \pi}} \exp \left(\frac{-1}{2} \cdot \frac{(u-\theta)^{2}}{2 / n}\right) d u \\
&=\frac{1}{\sqrt{2 \pi}} \int_{\mathbb{R}} g(u) \exp \left(\frac{-1}{2} \cdot \frac{u^{2}-2 u \theta+\theta^{2}}{2 / n}\right) d u \\
&=\frac{1}{\sqrt{2 \pi}} \cdot \exp \left(\frac{-n \theta^{2}}{4}\right) \int_{\mathbb{R}} \overbrace{\left(g(u) \exp \left(\frac{-n u^{2}}{4}\right)\right)}^{\text {Call this } h(u)} \exp \left(\left(\frac{n \theta}{2}\right) u\right) d u
\end{aligned}
$$ (The factor that does not depend on $u$ has been pulled out.) $$
=0 \text { only if } \int_{\mathbb{R}} h(u) \exp (\eta u) d u=0
$$ i.e. $\quad(\mathcal{L} h)(\eta)=0$ for every value of $\eta$ , where $\mathcal{L}$ is a two-sided Laplace transform. Recall the inverse property: if $\quad(\mathcal{L} f)(s)=\quad(\mathcal{L} g)(s)$ for every value of $s$ in $\mathbb{R}$ , then $f = g$ almost everywhere. Hence we conclude that $h = 0, a.e$ and therefore $g = 0, a.e$ , which complete the proof of completeness.","['inverse-laplace', 'statistics', 'normal-distribution', 'sufficient-statistics']"
4291876,Sequence $A_n$ converges to $\phi$ iff the sequence of symmetric difference $B_n=B_{n-1}\Delta A_n$ is convergent,"Let $B_1=A_1$ and $B_n=B_{n-1}\Delta A_n$ for $n=1,2,...$ . Then $B_n$ is convergent iff $\lim_{n\rightarrow \infty}A_n=\phi$ The definition for convergence $$\lim_{n\rightarrow\infty}\inf A_n
=
\bigcup_{n=1}^\infty\bigcap_{k=n}^\infty A_k
=
\bigcap_{n=1}^\infty\bigcup_{k=n}^\infty A_k
=
\lim_{n\rightarrow\infty}\sup A_n$$ I know that $B_n=(B_{n-1}|A_n)\cup(A_n|B_{n-1})$ but I don't know where to start. Could you please give a small hint on how to start to prove this.","['elementary-set-theory', 'measure-theory']"
4291881,Looking for a discrete mathematics course by programming,"Background: I am a totally blind programmer. That is, I cannot see at all. I am using a screen reading software to read texts, but not images. I am unable to get a computer science degree. Now I am a full-time web developer but I am still very interested about computer science so I am learning as much as I could on my spare time. This post was inspired by section 2.3.3 of SICP https://sarabander.github.io/sicp/html/2_002e3.xhtml#g_t2_002e3_002e3 . For many years I am trying to find ways to study discrete math. There are many free wonderful resources and courses. Unfortunately, everything I could find (E.G. MIT OCW, Book of Prufe) uses math rendering that can't be read by my screen reader. After much searching, I believe in the web at least, the mathjax library is the only library that allows my screen reader to read equations (there could be more, but I am not sure). Unfortunately, mathjax is not available on pdf format. And all lecture notes I could find are all in pdf format and have equations that is ignored by my screen reader. Then, I have thought of bypassing my notation problem completely. Maybe there is a course that teaches discrete math through programming just like how SICP did it with sets. I know SICP does not intend to teach discrete math, but I feel that the same idea could be applied when actually learning discrete math. I am hoping that by using this approach, even though the mathematical notation are invinsible to my screen reader, I still have some code (maybe written in a functional language?) that I could play with to understand the concepts. I am counting on abstraction here (E.G. I am confident that objects in graph theory can be represented as data structures). I would like to ask: is there such a course? Can anyone recommend a course like this? Like a discrete math and programming combination? Topics I am interested in are combinatorics, asymptotic notation, number theory, graph theory Also, I am aware that bypassing math notation is extremely dangerous to do. After all, reading and writing prufes almost involves reading math notation. I have come to accept that this might be my (or my screen reading software)'s limitation that cannot be fixed at the moment. I will settle for learning the concepts enough to at least being able to pruve the time complexity of my own programs.","['self-learning', 'programming', 'learning', 'discrete-mathematics', 'online-resources']"
4291890,"Evaluation the Elsasser function:$\text E(y,u)=\int_{-\frac12}^\frac12e^{\frac{2\pi uy\sinh(2\pi y )}{\cos(2\pi x)-\cosh(2\pi y)}}dx$ from MathWorld","Here is another uncommon special function which is officially recognized inspired by: Series Representation of the Glasser function: $$\text G(x)\mathop=\limits^\text{def} \int_0^x \sin(t\sin(t))dt\sim2\sqrt{\frac x\pi}$$ This time we would like to evaluate the Elsasser function which I cannot find any primary source for $$\text E(y,u)\mathop=^\text{def}\int_{-\frac12}^\frac12 e^{\frac{2\pi uy\sinh(2\pi y)}{\cos(2\pi x)-\cosh(2\pi y)}}dx$$ Here is an interactive graph of the function. I was able to get the following series representation using software and substitution of variables where appears the Gauss Hypergeometric function : $$\text E(y,u)=\int_{-\frac12}^\frac12 e^{\frac{2\pi uy\sinh(2\pi y)}{\cos(2\pi x)-\cosh(2\pi y)}}dx= \int_{-\frac12}^\frac12 e^{\frac{a}{\cos(2\pi x)-b}}dx=\int_{-\frac12}^\frac12 \sum_{n=0}^\infty \frac{a^n}{n!}(\cos(2\pi x)-b)^{-n}dx\ne \frac{i}{\sqrt{\pi (b^2+1)}}\sum_{n=0}^\infty\frac{a^n\color{red}{(-n)!}}{n!\left(\frac12-n\right)!}\left((-1)^{1-n}(1+b)^{1-n}\,_2\text F_1\left(\frac12,1-n,\frac32-n,\frac{b+1}{b-1}\right)+(1-b)^{1-n} \,_2\text F_1\left(\frac12,1-n,\frac32-n,\frac{b+1}{b-1}\right)\right)$$ The 4th step can be evaluated using the Appell $\text F_1$ function and then plug in the bounds to get the hypergeometric functions. The problem of $\cos(2\pi x)$ in the denominator creates the $(-n)!$ in the expansion and therefore in the integral. Another idea is to use one of many other expansions for $e^y$ . How can we find a series representation for the Elsasser function as seen in Wolfram Mathworld? We can use it to put complicated sums into closed Elsasser form. The sum representation should have a single sum with closed form coefficients, so no nth derivatives please. You can also have 2 infinite series if really needed. Please correct me and give me feedback!","['definite-integrals', 'special-functions', 'trigonometric-integrals', 'sequences-and-series', 'bessel-functions']"
4292012,Finding covariance from a transformed random variable given its covariance matrix,"Let the bivariate random variable $A=(A_1,A_2)^T$ have a Gaussian distribution on $\mathbb{R}^2$ with zero mean and covariance matrix be given by $$\begin{pmatrix} 1 & -0.4\\-0.4 & 1\end{pmatrix}$$ . Let $B$ = $\begin{pmatrix} 1 \\ 2 \end{pmatrix}$ and $C$ = $\begin{pmatrix} 2 \\ 1 \end{pmatrix}$ . Define $X=B^TA,Y=C^TA$ . How do I find the covariance of X and Y? I know that $cov(X,Y) = E(XY)-E(X)E(Y)$ . I don't quite understand how to read a covariance matrix.","['statistics', 'covariance', 'variance', 'matrices', 'expected-value']"
4292020,How to evaluate the integral $I = \int_o^{\infty} \frac{x}{\sqrt{e^{2\pi\sqrt{x}}-1}}dx$?,"$$I = \int_{0}^{\infty} \frac{x}{\sqrt{e^{2\pi\sqrt{x}}-1}}\,dx$$ . I tried to solve by substituting $t = 2\pi\sqrt x \implies t^2 = 4{\pi}^2x \implies tdt = 2\pi^2dx$ $$I = \frac1{8\pi^4}\int_0^{\infty}\frac{t^3}{\sqrt{e^t-1}}dt.$$ But now I'm unable to solve from here.","['integration', 'definite-integrals', 'gamma-function', 'calculus', 'beta-function']"
4292089,Geometric intuition behind Cauchy not having a mean,"I'm trying to follow the geometric intuition behind the Cauchy distribution in the book ""An introduction to probability theory and its applications"" by Feller volume 2, first edition. On page 51, he describes the density of the Cauchy distribution as: $$\gamma_t = \frac{t}{(t^2+x^2)\pi}$$ He then describes an experiment where a ray of light is emitted horizontally onto a vertical mirror from a source O. The light strikes the mirror at point A and the mirror can rotate about a vertical axis passing through A. This is shown in the figure below. The light reflects off the mirror and strikes the wall O is on at a distance $X$ from O. The first assertion is that if $\phi$ is uniformly distributed between $(-\frac{\pi}{2}, \frac{\pi}{2})$ , $X$ has the density given by $\gamma_t$ . I managed to prove this. Then he says that its apparent that if this experiment is repeated $n$ times and the average taken, then this average $\frac{X_1+X_2+\dots X_n}{n}$ will have the same distribution as $X_1$ . I didn't follow this part. I guess I could derive it from the density, but Feller seems to be hinting at some kind of obvious geometric intuition which I'm completely missing.","['convergence-divergence', 'probability']"
4292108,Are there any examples of non-linear functions whose contour plot is made up of ALL parallel lines? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I know planes are made up of all parallel lines but what about functions in 3-space?",['multivariable-calculus']
4292129,"Milnor: Morse Theory Theorem 3.1 why do we need $f^{-1}[a,b]$ to be compact?","In Milnor's Morse Theory the Theorem 3.1 is given as follows: Let f be a smooth real valued function on a manifold $M$ . Let $a < b$ and
suppose that the set $f^{âˆ’1}[a,b]$ , consisting of all $p \in M$ with $a \le f(p) \le
 b$ , is compact, and contains no critical points of $f$ . Then $M^a$ is
diffeomorphic to $M^b$ . Furthermore, $M^a$ is a deformation retract of $M^b$ ,
so that the inclusion map $M^a \to M^b$ is a homotopy equivalence. Edit: The set $M^a$ is defined as $M^a:= f^{-1}(-\infty,a]$ . After the proof, he states that The condition that $f^{âˆ’1}[a,b]$ is compact cannot be omitted. For example
Figure 3.2 indicates a situation in which this set is not compact. The
manifold $M$ does not contain the point $p$ . Clearly $M^a$ is not a
deformation retract of $M^b$ . Figure 3.2 is the following: But i dont understand why $f^{âˆ’1}[a,b]$ is supposedly not compact and how is it possible that $p \not\in M$ ? What am i missing? Why does $p \not\in M$ imply that $f^{âˆ’1}[a,b]$ is not compact and why is it clear that $M^a$ is not a deformation retract of $M^b$ ? Could someone elaborate? Thanks for any help!","['smooth-manifolds', 'morse-theory', 'manifolds', 'differential-topology', 'differential-geometry']"
4292132,"If $|f_n|\leq g \in L^1$, and $f_n\rightarrow f$ in measure, then $f_n \rightarrow f \in L^1$ (Folland chp 2 ex. 34)","In Folland's Real Analysis, 1999 2nd edition. So the precise statement is actually: Let $(X, M, \mu)$ be a measure space. If $|f_n|\leq g \in L^1$ , and $f_n\rightarrow f$ in measure, then  (1) $\lim_{n\rightarrow \infty} \int f_n = \int f$ and (2) $f_n \rightarrow f \in L^1$ , however unless I am seriously mistaken 2 implies 1... So I have attempted to solve this problem and I am stuck, and want to ask for some help. My approach is contradiction. I will detail the work I have so far. I would prefer help which builds on my current work, however if my approach is futile then of course starting over is preferable, but in the end how you help is up to you. Suppose that $f_n \rightarrow f$ in $L^1$ does not hold, then $\exists\; \epsilon > 0$ s.t. $\forall$ $N$ , $\exists$ $n\geq N$ s.t. $\int_P |f_n -f| \geq \epsilon $ (where $P$ is a set of positive measure). Now, we know that $f_n$ converges in measure, denote $\{x\;|\;|f_m(x)-f(x)|\geq \epsilon\}=F_{m,\epsilon}$ , then $\exists$ M s.t. $\forall$ $m\geq M$ , we have $\mu(F_{m,\epsilon})<\epsilon$ . Now, at this point I start to become unsure of what to do. THe most interesting thing I can come up with is this: $$\sup_P |f_n -f| \mu(P)\geq \int_P |f_n-f| \geq \epsilon \mu(P) > 0$$ Also, we have that |f_n|\leq g, so we need to use this, but I am not sure how. Of course $\sup_P |f_n -f| \mu(P)\leq \sup_p 2g \mu(P)$ but I am not sure how this helps. Another interesting thing I found was that, $f_n \rightarrow f$ in $L^1$ is equivalent to $|f_n - f|\rightarrow 0$ in $L^1$ , so if we could show that $\int_P |f_n-f|\rightarrow 0$ somehow, than perhaps this would say something close to "" $f_n\rightarrow f$ in $L^1$ ""... (but I don't think it would say exactly that. Thanks!","['integration', 'measure-theory', 'analysis', 'real-analysis']"
4292143,"If $\mathcal{P}(A \cup B) = \mathcal{P}(A) \cap \mathcal{P}(B)$, then $A = B$","I'm confused trying proving this problem. I tried the following: Suppose $A\neq B$ , then $\mathcal{P}(A) \cap \mathcal{P}(B)=\emptyset\implies \mathcal{P}(A \cup B) =\emptyset \implies A \cup B =\emptyset \implies A=B$ , but this is a contradiction. Thus, $A = B$ . The thing with this though is that I'm not quite sure about $\mathcal{P}(A) \cap \mathcal{P}(B)=\emptyset$ , because $A$ and $B$ can have a common element and still be different.",['elementary-set-theory']
4292188,The monotonicity of a function that is of complex form,"Consider the following function in $x\geq0$ with a parameter $q\in[0,1]$ : $f(x;q):=\frac{(1-e^{-x})^2}{e^{-x}(1+x)^2(1+qx-e^{qx}(1+x))^2}\bigg[(1+qx)^2+(2+qx)q^{2}x(1+x)^2+e^{qx}(1+x)\big\{(qx+1)(x-1)+q^{2}x(1+x)(qx(1+x)-2)\big\}\bigg]$ For a given $q\in[0,1]$ , when I plot the function $f(x;q)$ in $x\geq0$ , I can see that the function is monotonically (strictly) increasing in $x\geq0$ , so I suspect that this monotonicity holds for $f(x;q)$ in general. I've tried to prove it by taking the first derivative, but as you can see, it is very complicated... I used the symbolic calculation/calculus in Mathematica, but it was not able to resolve the issue. I am just wondering if there could be any way to prove that $f(x;q)$ increases in $x\geq0$ for a given $q\in[0,1]$ . By the way, if $q=0$ or $q=1$ , then the function $f(x;q)$ reduces to a quite simple form and I can easily show its monotonicity. Any suggestions or thoughts would be really appreciated. Thank you so much for your help in advance!","['calculus', 'monotone-functions', 'analysis']"
4292247,An Simple Analysis Problem,"$\varphi:\textrm{R}\rightarrow\textrm{R}$ is continuous, $\lim_{x\rightarrow\infty}\varphi(x)-x=\infty$ , and $\{x\in\textrm{R}|\varphi(x)=x\}$ is a finite non-empty set. If $f:\textrm{R}\rightarrow\textrm{R}$ is continuous and $f\circ\varphi=f$ , prove that $f$ is a constant function. I think maybe we can prove it by contradiction, assume that $f$ isn't a constant function, considering $f$ is continuous, it's easy to see that $f$ has an infinite number of different values in some $[a,b](a,b\in\textrm{R})$ . Note that $\varphi$ has finite fixed points, perhaps contradictions can be launched from here. But I failed to make any further progress.",['analysis']
4292259,Prove that $f(x) = x^2 + x$ is uniformly continuous.,"Using the definition of uniform continuity, prove that $f(x) = x^2 + x$ is uniformly continuous on $(0,1)$ . Proof: We have a function $f:(0,1)\to\mathbb R$ . Let $\epsilon > 0$ . Note $\forall x,y\in\mathbb (0,1)$ that $|x+y| \le |x| + |y| < 2$ . Set $\delta = \frac \epsilon 3$ . Then for $x,y\in(0,1)$ , if $|x-y|<\delta$ , implies \begin{split}
|x^2+x-(y^2+y)|&\le|x^2-y^2|+|x-y| \\ 
&= |x-y||x+y| + |x-y|\\
&=|x-y|(|x+y|+1)\\
&< |x-y|(2+1)\\
&= 3|x-y| \\
&<\epsilon.
\end{split} Is this an ok proof?","['solution-verification', 'real-analysis']"
4292260,Can I Plug In Infinity to Verify the Equation: $\tan \left( \frac{\pi}{2} - x\right) = \cot x$,"When working on the following equation for my pre-calculus class a couple nights ago Verify that: $$
\tan \left(\frac{\pi}{2} - x\right) = \cot x
$$ I decided to answer the question by using the fact that $$
\tan \left(\frac{\pi}{2} - x\right) = \frac{\tan \frac{\pi}{2} - \tan x}{1 + \tan \frac{\pi}{2} \tan x}.
$$ I realized that using this formula, I would not help me answer the question for my math class, because, as we learned in class, $\tan \frac{\pi}{2}$ is undefined. However, I (knowingly erroneously) proceeded with this method, plugging in $\pm \infty$ for $\tan \frac{\pi}{2}$ yielding $$
\frac{\pm \infty - \tan x}{1 + \pm \infty \tan x}
$$ which can be simplified down to $$
\frac{1 - 0}{0 + \tan x} = \frac{1}{\tan x} = \cot x
$$ by dividing the numerator and the denominator by $\pm \infty$ . In an attempt to justify the fact that $\frac{\infty}{\infty}$ should be 1 and $\frac{1}{\infty}$ and $\frac{\tan x}{\infty}$ should be 0, I turned to limits. Whilst not knowing too much calculus, I learned last year in Algebra II that $\infty$ can be somewhat approximated with limits. As such, I defined a new constant $\beta$ as $$
\beta = \lim_{x \rightarrow \infty} x
$$ and $\alpha$ as $\beta$ 's reciprocal. From my understanding of limits, that just means $\beta$ is essentially some very large number, so I should still be able to do arithmetic with it ( $\alpha \beta = 1$ , $\beta - \beta = 0$ , etc.). So then my equation becomes: $$
\frac{\beta - \tan x}{1 + \beta \tan x} = \frac{\beta - \tan x}{1 + \beta \tan x} \cdot \frac{\alpha}{\alpha} = \frac{1 - \alpha \tan x}{\alpha + \tan x}
$$ which is essentially $\cot x$ at the end, because $\alpha \approx 0$ . I have been constantly told not to use infinities in equations since elementary school, yet here it seems to yield the correct answer. Why is that, and when can these infinities be used in equations? If I got this wrong, where did I err?","['limits', 'algebra-precalculus', 'trigonometry', 'infinity']"
4292313,Find $\int_0^\infty \frac{dx}{x^{n-1}+x^{n-2}+\cdots +x+1}$ using contour integral,"I have solved the problem of integration $$\int_0^\infty \frac{1}{x^n+1}dz$$ using the contour as an arc of a circle. But I don't know how to  approach this problem: $$I=\int_0^\infty\frac{1}{x^{n-1}+x^{n-2}+\cdots +x+1}dz$$ Please Help me with this problem Edit: With the help of geometric series (suggested in comment), I can reduce the integral to the evaluation of $$\int_0^\infty\frac{1-x}{1-x^n}dx$$ Now it's $1-x$ that's causing the problem if I choose the contour to be arc with angle $2\pi /n$ . I still need little help.","['complex-analysis', 'contour-integration']"
4292315,Component wise convergence,"Attempt: Comparing real and imaginary parts we get: $r_n \cos \theta_n \to r \cos \theta$ $r_n \sin \theta_n \to r \sin \theta$ $\,\,$ Thus, we get: $r_n \cos \theta_n \, . \,r_n \cos \theta_n \to r \cos \theta \, . \, r \cos \theta$ $r_n \sin \theta_n \, . \,r_n \sin \theta_n \to r \sin \theta \, . \, r \sin \theta$ $\implies r_n^2 \sin^2 \theta_n \, + \,r_n^2 \cos^2 \theta_n \to r^2 \sin^2 \theta \,+ \, r^2\cos^2 \theta$ $\implies r_n^2  \to r^2$ How do I conclude from here that $r_n \to r$ ? Should I approach this differently? Edit: I just realized I could just compose with square root function. $\lim_{n \to \infty} r_n=\lim_{n \to \infty} \sqrt{{r_n}^2}= \sqrt{\lim_{n \to \infty} {r_n}^2} = \sqrt{r^2}=r$","['complex-analysis', 'sequences-and-series']"
4292326,Mittag-Leffler Expansion of gamma function,"Mittag-Leffler's expansion theorem states that $$f(z)=f(0)+\sum_{n=1}^\infty \text{Res}_{z=z_k}\left\{\frac{1}{z-z_k}+\frac{1}{z_k}\right\}$$ For gamma function $\Gamma(z)$ $$\Gamma(z)+\frac{1}{z}=1+\sum_{n=1}^\infty \frac{(-1)^n}{n!}\left(\frac{1}{z+n}-\frac{1}{n}\right)=1+\sum_{n=0}^\infty\frac{(-1)^n}{n!(z+n)}-\sum_{n=1}^\infty \frac{(-1)^n}{n(n)!}$$ But this doesn't simplifies to what I have to prove. That is $$\Gamma(z)=\sum_{n=0}^\infty\frac{(-1)^n}{n!(z+n)}+\int_1^\infty t^{z-1}e^{-t}dt$$ I get the first term right!, but I'm not sure how does this integral shows up here. Please help me with this","['complex-analysis', 'contour-integration', 'gamma-function']"
4292381,"Spivak Chapter 1, Problem 19(a)","Per Spivak The great granddaddy of all inequalities is the Schwarz inequality $${x}_{1}{y}_{1}+{x}_{2}{y}_{2} \le \sqrt{{x}_{1}^{2} + {x}{_2}^{2}}\sqrt{{y}_{1}^{2} + {y}{_2}^{2}}$$ Prove that if ${x}_{1} = \lambda {y}_{1}$ and ${x}_{2} = \lambda {y}_{2}$ for some number $\lambda \ge 0$ , then equality holds in the Schwarz inequality. There is an answer here , but it seems to follow the pattern elsewhere of simply substituting on both sides of the given inequality, and changing the $\le$ to an $=$ , unless I am misunderstanding it. Amusingly, having invested in Spivak's answer book, his answer is "" The proofs for .... are straightforward "" :-) Here is my attempt. Given ${x}_{1} = \lambda y_{1}$ and ${x}_{2} = \lambda y_{2}$ then $$\sqrt{{x}_{1}^{2} + {x}_{2}^{2}}\sqrt{{y}_{1}^{2} + {y}_{2}^{2}} = \sqrt{(\lambda {y}_{1})^{2} + (\lambda {y}_{2})^{2}}\sqrt{{y}_{1}^{2} + {y}_{2}^{2}}$$ $$= \sqrt{\lambda^{2} {y}_{1}^{2} + \lambda^{2} {y}_{2}^{2}}\sqrt{{y}_{1}^{2} + {y}_{2}^{2}}$$ $$=\sqrt{\lambda^{2}( {y}_{1}^{2} +  {y}_{2}^{2})}\sqrt{{y}_{1}^{2} + {y}_{2}^{2}}$$ $$=\sqrt{\lambda^{2}}\sqrt{( {y}_{1}^{2} +  {y}_{2}^{2})}\sqrt{{y}_{1}^{2} + {y}_{2}^{2}}$$ Note: $\sqrt {\lambda^{2}} = \lambda \iff \lambda \ge 0 $ as asserted in the problem. $$=\lambda ({y}_{1}^{2} +  {y}_{2}^{2})$$ $$=\lambda {y}_{1}^{2} +  \lambda{y}_{2}^{2}$$ $$=\lambda {y}_{1}{y}_{1} +  \lambda{y}_{2}{y}_{2}$$ $$=(\lambda {y}_{1}){y}_{1} +  (\lambda{y}_{2}){y}_{2}$$ $$=x_{1}{y}_{1} +  x_{2}{y}_{2}$$","['calculus', 'proof-writing', 'algebra-precalculus', 'inequality']"
4292393,$\frac{3}{4} \lim_{n \to \infty}\left(\frac{\sum_{r=1}^{n}\frac{1}{\sqrt{r}} \sum_{r=1}^{n}\sqrt{r} }{\sum_{r=1}^{n}r }\right)$,"$$\frac{3}{4} \lim_{n \to \infty}\left(\frac{\sum_{r=1}^{n}\frac{1}{\sqrt{r}} \sum_{r=1}^{n}\sqrt{r}  }{\sum_{r=1}^{n}r    }\right)$$ Apparently, the answer is 2. My try: $$\frac{3}{4} \lim_{n \to \infty}\left(\frac{n^2}{\frac{n(n+1)}{2}    }\right)$$ $$\frac{3}{2} \lim_{n\to \infty}\left(1-\frac{1}{n+1}\right)$$ Which is $\frac{3}{2}$ . Surely I did something stupid or illegal here. What was it? Also on the forum where I found this, It was said it can be done by converting into integrals, A push in the right direction on that too would be pretty rad. Hope I am not asking too much.","['integration', 'limits', 'summation']"
4292419,Why must we consider seemingly unnecessary cases for combinatorics problems?,"Consider the following question: ""A committee of 7 students must be selected from 4 tenth-grade students, 6 eleventh-grade students, and 8 twelfth-grade students. How many committees are possible if there must be at least 5 twelfth-grade students on the committee?"" My solution: There are 8 twelfth-grade students, and at least five must be selected. Five will be selected from a group of 8, and the remaining 3 will be put into a separate group with the tenth and eleventh-graders, to form a group of (3+4+6) = 13 students. From this group, 2 will be selected, to form a committee of (2+5) = 7 students. (â‚ˆCâ‚…)(â‚â‚ƒCâ‚‚) = 4368 possible committees. Official solution: ""There are 8 twelfth-grade students, of which five, or six, or seven (but not eight because there are only 7 members of the committee), can be selected. Case 1: Five twelfth-grade students are selected from a group of eight. The 10 remaining students from the other two grades are grouped together, and two of them are chosen to complete the committee. (â‚ˆCâ‚…)(â‚â‚€Câ‚‚) Case 2: Six twelfth-grade students are selected from a group of eight. The 10 remaining students from the other two grades are grouped together, and one of them is chosen to join the twelfth-graders in the committee. (â‚ˆCâ‚†)(â‚â‚€Câ‚) Case 3: Seven twelfth-grade students are selected from a group of eight. Seven students have been selected, and the committee is complete. (â‚ˆCâ‚‡)(â‚â‚€Câ‚€) Case 1 + Case 2 + Case 3 =
(â‚ˆCâ‚…)(â‚â‚€Câ‚‚) +
(â‚ˆCâ‚†)(â‚â‚€Câ‚) + (â‚ˆCâ‚‡)(â‚â‚€Câ‚€) = 2808 possible different committees."" I understand the approach to the official solution. However, it is unclear where the unselected twelfth-grade students disappear to. Why are they not included in the other group? In my solution, there is no need to consider cases, because the ""at least five twelfth-grade students"" condition is already satisfied. Five are guaranteed from the original group, and it is entirely possible that one or two more will join the committee, as they are included in the separate expression (â‚â‚ƒCâ‚‚). I expected the two solutions to be equivalent, and reach the same integral solution. However, they did not. Surely there is a mathematical explanation for this.","['combinations', 'combinatorics']"
4292425,Why does the Complex Modulo Function $1 \bmod z$ Look Very Similar to the Riemann Zeta Function When Graphed?,"I recently watched 3Blue1Brown's video on the Riemann Zeta Function . At around the 11:00 minute mark, there's an interesting visualization of the function graphed in the complex plane. As an amateurish interest, I built up a complex graphing calculator using HSL graphing in JavaScript, only considering hue for the complex argument. I defined the complex modulo operation as $z_1 \bmod z_2 = z_1 - z_2 \left\lfloor \frac{z_1}{z_2} \right\rfloor$ and the complex floor function as $\lfloor z \rfloor = \lfloor \Re(z) \rfloor + \lfloor \Im(z) \rfloor$ . Here is the function $1 \bmod z$ graphed vs. 3Blue1Brown's graph: I don't know much about Complex Analysis, but these two functions seem to have the same underlying structure, at least to my eyes. What connection is there between the modulo function and the Riemann Zeta Function? I also don't think I've seen this shape in mathematics before. Are there any other places where this shape shows up? Forgive my ignorance if the answers are trivial. Also, thank you very much for spending the time to answer questions from newbies to math like me.","['complex-analysis', 'riemann-zeta', 'graphing-functions', 'modular-arithmetic']"
4292436,maximum of $\frac{1}{(1+(x- \frac{1}{2})^2 )^{\frac{3}{2}}}+\frac{1}{(1+(x+ \frac{1}{2})^2 )^{\frac{3}{2}}}$,"How to find this function's maximum? $$\frac{1}{(1+(x- \frac{1}{2})^2 )^{\frac{3}{2}}}+\frac{1}{(1+(x+ \frac{1}{2})^2 )^{\frac{3}{2}}}$$ I think it has a maximum value at x=0. A search on Wolfram Alpha revealed that this is correct. However, the result in wolframalpha was to solve the 12th-order equation by differentiating and then performing general differentiation, I wonder if there is an easier way.","['maxima-minima', 'derivatives', 'real-analysis']"
4292438,How to solve the recurrence relation $ a_{n}=\sum_{i=1}^{n-1}a_{i}a_{n-i} $,"The relation is: $$
a_{n}=\sum_{i=1}^{n-1}a_{i}a_{n-i}\\n>1,a_1=2
$$ I know I probably should create a generating function for $a_n$ , like $G(x)=f(x)*g(x)$ , but I'm not sure what $f(x)$ and $g(x)$ exactly should be, are they just $\sum a_ix^i$ and $\sum a_{n-i}x^{n-i}$ ? And are there any other methods to solve it? Any hint would be useful to me.","['combinatorics', 'recurrence-relations', 'discrete-mathematics', 'generating-functions']"
4292445,Self-composition of extended Motzkin numbers vs. doubled Catalan numbers,"I am looking for a combinatorial proof (or a reference to such) of the following fact related to Catalan and Motzkin numbers. Consider the extended Motzkin number sequence, where the $n$ th term ( $n\ge 0$ ) counts the number of nonempty Motzkin paths (see A001006 ) either consisting of a single point or starting with a level step. Its generating function is $$
\tilde{m}(z)=1+zm(z)=\frac{1+z-\sqrt{1-2z-3z^2}}{2z}.
$$ It appears that the coefficient $$[x^n]\bigl(\tilde{m}(z)\tilde{m}(z\tilde{m}(z))\bigr)\ge 2C_n \qquad \text{for}\ n\ge 1,$$ where $C_n$ is the $n$ th Catalan number ( A000108 ) and $$C(x)=\frac{1-\sqrt{1-4z}}{2z}$$ is the Catalan generating function. In fact, $$[x^n]\bigl(\tilde{m}(z)\tilde{m}(z\tilde{m}(z))\bigr)=[x^n]\bigl(2C(x)-1\bigr) \qquad \text{for}\ 0\le n\le 7,$$ and it appears that $$[x^n]\bigl(\tilde{m}(z)\tilde{m}(z\tilde{m}(z))\bigr)>[x^n]\bigl(2C(x)-1\bigr) \qquad \text{for}\ n\ge 8.$$ (Cf. OEIS sequences A348197 and A068875 .) I understand that the left-hand side grows asymptotically faster than the right-hand side (with rates of growth of $\frac{3}{2}(1+\sqrt{3})\approx 4.098$ vs. $4$ ) and so should be eventually greater. It is less clear to me why the weak inequality holds from the very beginning.","['generating-functions', 'catalan-numbers', 'combinatorics', 'reference-request']"
4292450,Expected value of a stopping time of the sum of exponential random variables,"Let $X_1, X_2,...$ be a sequence of independent exponential random variables, each with mean 1. Given a positive real number $k$ , let $k$ be defined by $N=\min\left\{ n: \sum_{i=1}^n X_i >k \right\}$ .
That is, $N$ is the smallest number for which the sum of the first $N$ of the $X_i$ is larger than $k$ . I want to compute $E[N]$ . I attempt to apply Wald's equation $E[\sum_{i=1}^N X_i]=E[N]E[X]$ , but I have no idea to obtain $E[\sum_{i=1}^N X_i]$ .","['stochastic-processes', 'probability']"
4292462,Prime $p$ does not divide $\dbinom{n}{p^e}$,"I encountered this statement as a lemma in the course of proving the first Sylow theorem. If $|G|=n=p^em$ , $p$ is a prime and $m$ and $p$ are coprime, show that $p$ does not divide $\dbinom{n}{p^e}$ . I tried expanding to factorials, but it didnâ€™t work. A solution or directive towards the proof is appreciated.","['group-theory', 'sylow-theory', 'combinatorics']"
4292464,What is the value of $\ln(e^{2i\pi})$?,"According to Euler's formula, $e^{i\theta} = \cos\theta + i\sin\theta;$ so, $e^{2\pi i} = 1.$ And $\ln(1) = 0.$ On the other hand, $\ln(e^{i\theta}) = i\theta,$ so $\ln(e^{2\pi i}) = 2\pi i$ . Is $\ln(e^{2i\pi})$ equal to $0$ or $2\pi i$ ?","['complex-analysis', 'logarithms']"
4292468,Find the Closed form of $\sum_{i=0}^k {{2k+2}\choose{i}} (2k)^i$,"I am trying to solve an integer equation which involves the expression $$\sum_{i=0}^k \binom{2k+2}{i}(2k)^i,$$ where $k\in\mathbb{N}$ . It would be helpful if there exists a closed form for this expression. Any ideas as to how I could achieve this? Thanks.","['summation', 'combinatorics', 'combinations']"
4292502,Reference for $\operatorname E\left[f\circ X\mid X^{-1}(\mathcal F)\right]=\operatorname E_{\mathcal L(X)}\left[f\mid\mathcal F\right]\circ X$,"Let $(\Omega,\mathcal A,\operatorname P)$ be a probability space; $(E,\mathcal E)$ be a measurable space; $X$ be an $(E,\mathcal E)$ -valued random variable on $(\Omega,\mathcal A,\operatorname P)$ ; $\mathcal L(X)$ denote the distribution of $X$ with respect to $\operatorname P$ ; $\operatorname E_{\mathcal L(X)}$ denote the expectation with respect to $\mathcal L(X)$ ; $\mathcal F\subseteq\mathcal E$ be a $\sigma$ -algebra on $E$ ; $f:E\to\mathbb R$ be bounded and $\mathcal E$ -measurable. We can easily show that $$\operatorname E\left[f\circ X\mid X^{-1}(\mathcal F)\right]=\operatorname E_{\mathcal L(X)}\left[f\mid\mathcal F\right]\circ X\tag1,$$ but I wasn't able to find this result in any textbook. Is there a commonly used name for the identity $(1)$ and does anyone have a reference for this result?","['conditional-expectation', 'measure-theory', 'probability-theory', 'reference-request']"
4292508,Find random variable given joint law,"Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space (if necessary we can assume to work with the unit interval with the Lebsegue measure) and let $X: \Omega \to \mathbb{R}$ be a random variable with law $\mu$ (meaning that $X_\sharp \mathbb{P}=\mu$ ). Suppose it is given a probability measure $\gamma$ on $\mathbb{R}^2$ with first marginal $\mu$ (meaning that $\pi^1_\sharp \gamma = \mu$ , where $\pi^1(x,y)=x$ is the projection on the first factor from $\mathbb{R}^2$ to $\mathbb{R}$ ). Can we find a random variable $Y:\Omega \to \mathbb{R}$ s.t. $(X,Y)_\sharp \mathbb{P}=\gamma$ ? I think that the problem is equivalent to the following: given the disintegration of $\gamma$ w.r.t. $\mu$ , call it $\{\mu_x\}_{x \in \mathbb{R}}$ , can we find a random variable $Y$ s.t. $$\mathbb{P}(Y \in B \mid X=x) = \mu_x(B)$$ for $\mu$ -a.e. $x \in \mathbb{R}$ and every $B \in \mathcal{B}(\mathbb{R})$ ? Edit: I add the following comment that may be helpful. It seems (see Bogachev 10.7.7 Corollary) that a sufficient condition is the existence of a random variable $Z: \Omega \to [0,1]$ with uniform distribution independent from the given $X$ . Is it always possible if the probability space is the unit interval with the Lebesgue measure?","['measure-theory', 'probability-distributions', 'probability-theory', 'random-variables']"
4292545,Problem when calculating vaccine effectiveness,"When calculating vaccine effectiveness against hospitalization from publicly available data, I came across a strange (mathematical) problem, which I do not know how to interpret. The problem occurs on real-world data, though for this question I hand-crafted the data to better illustrate the problem. I calculate the vaccine effectiveness against hospitalization by comparing numbers of hospitalized in the group of positive. Here are my data: Note that while the effectiveness of the vaccine for the whole population is negative, for individual age ranges it is positive. Mathematically it is clear - young people are often positive but rarely hospitalized, while old people are rarely positive (because they are more vaccinated) however they are hospitalized more often. Question 1: How to interpret the fact that the effectiveness for the whole is negative, while for parts it is positive? When I perform the categorization differently than by age range, I can get completely different effectiveness for the parts: Note that the total numbers are the same as before, only the distribution of hospitalized among positive is different. My impression is that by carefully choosing the category, I can get any results I want. Question 2: I am sure this phenomenon is well known and studied in statistics. Can you point me to the right topic I can look at? Question 3: As shown above, the calculated vaccine effectiveness depends substantially (and can give completely different results) on the division into categories chosen. Why is categorization by age considered better (and correct) than categorization e.g. by colour (as in my example)? IMO, it's just a wishful thinking. For instance, how can we be sure that if we sub-divide the age ranges more finely (either by individual years or by another criteria, e.g. type of vaccine, factory where it was made, region where the patients live etc.), the effectiveness won't be negative again?",['statistics']
4292583,How should I prove this map to be injective and surjective,"I am reading from course notes on Smooth Manifolds and I was unable to prove this which Define $T_v : C^{\infty}(U) \to \mathbb{R} $ by $T_v(f)= v(f) = \frac{d}{dt} f(p+tv)|_{t=0}=df_p(v)=\frac{df}{dx_1}(p) v_1 +...+ \frac{df}{dx_n} (p)v_n$ . Let $M, M_1, M_2$ be manifolds and $M= M_1 \times M_2$ , $P= (P_1,P_2)$ Then Prove that $T_p M \approx T_{P_1} M_1 \oplus T_{P_2}M_2$ If $\pi_1 : M \to M_1$ , $\pi_2 : M\to M_2$ , $(d\pi_1)_P: T_P M \to T_{P_1}M_1$ , $(d\pi_2)_P: T_PM \to T_{P_2}M_2$ , $\alpha :T_P M \to T_{P_1} M_1 \oplus T_{P_2}M_2$ , I thought of the map $\alpha(v)=((d\pi_1)_P(v), (d\pi_2)_P (v))$ I need help in checking map 1-1 and onto. Let $\alpha(v)=0$ , to show v=0. $\alpha(v)=0$ => $(d\pi_1)_P(v)=0 $ and $ (d\pi_2)_P(v)=0$ but  how does definition of $df_p$ now implies that v=0. Can you please tell ? Similarly , I am not able to think how should  I approach the surjective part. Kindly give a rigorious proof so that I can learn the method for smooth manifolds as I think I am not much comfortable in this.","['manifolds', 'tangent-spaces', 'analysis', 'smooth-manifolds']"
4292610,Numbers from $1$ to $n$ are permuted such that the sum of any three consecutive numbers is divisible by the leftmost of them,"Numbers from $1$ to $n$ are permuted such that the sum of any three consecutive numbers is divisible by the leftmost of them. Example for $n=6$ : $1,2,3,5,4,6$ . The questions are: For which values of $n$ exists such permutation? Is it possible for $n$ to be arbitrarily large? What is the biggest possible value of $n$ ? Examples for $n=15$ : $$ \begin{aligned} &15,11,4,7,5,9,1,8,3,13,14,12,2,10,6 \\ &3,14,13,15,11,4,7,5,9,1,8,6,2,10,12 \\ &15,1,14,3,11,13,9,4,5,7,8,6,2,10,12 \\ &14,9,5,4,1,3,13,11,15,7,8,6,2,10,12 \end{aligned} $$ Example for $n=38$ : $$ \begin{gathered} 35,3,32,1,31,33,29,4,25,11,14,19,23,15, \\ 8,37,27,10,17,13,21,5,16,9,7,38,18,20, \\ 34,6,28,2,26,22,30,36,24,12 \end{gathered} $$ And two examples for $n=51$ : $$ \begin{gathered} 46,45,1,44,49,39,10,29,21,37,5,32,23,41,51,31,20, \\ 11,9,2,43,35,8,27,13,14,25,3,47,40,7,33,16,17, \\ 15,19,26,50,28,22,6,38,34,4,30,18,42,48,36,12,24 \end{gathered} $$ and $$ \begin{aligned} &46,45,1,44,49,39,10,29,21,37,5,32,23,41,51,31,20, \\ &11,9,2,43,35,8,27,13,14,25,3,47,40,7,33,16,17, \\ &15,19,26,50,28,22,6,38,34,4,30,42,18,24,12,36,48 \end{aligned} $$ P.S. At the moment, this is a record! These are the only two solutions for $n$ between $39$ and $64$ , inclusive. It was checked by brute force. I would also be glad if someone could add more appropriate tags to this question. P.S.S. There is also an idea to generate chains of length $\frac{n}{2}$ (with values from $1$ to $n$ ), and then ""pair"" them. I do not know to what extent this will speed up the process. P.S.S.S. Here's a nifty Sage program using recursively enumerated sets and MapReduce. For example, $n=21$ is taken, but you can replace it with any other.
Especially the power of this approach can be felt on a multiprocessor system, where MapReduce is automatically parallelized and everything is greatly accelerated. The program is not mine!","['number-theory', 'arithmetic', 'divisibility', 'algorithms']"
4292617,Prove that $x<y\implies x^{y^{x^{y^x}}}<y^{x^{y^{x^y}}}$,"I was looking for pairs $(x,y)$ of positive real numbers which satisfy equations of the form $x^y=y^x, x^{y^x}=y^{x^y}, x^{y^{x^y}}=y^{x^{y^x}},\dots$ etc. I was able to find solutions for $x^y=y^x$ , which can be parametrized as $\left(\left(\frac{n+1}{n}\right)^n,\left(\frac{n+1}{n}\right)^{n+1}\right).$ I used a graphing calculator (Desmos) and it seems that all the equations with odd terms, like $x^{y^x}=y^{x^y}, x^{y^{x^{y^x}}}=y^{x^{y^{x^y}}},\dots$ etc. are only possible for $x=y$ . Using methods of calculus, I was able to show that $x<y\implies x^{y^x}<y^{x^y}$ , which proves this for the equation with 3 terms. But I can't seem to generalize this further. I am trying to prove $x<y\implies x^{y^{x^{y^x}}}<y^{x^{y^{x^y}}}$ , and to generalize this to all odd term equations of this type.","['real-analysis', 'calculus', 'algebra-precalculus', 'exponential-function', 'parametrization']"
4292624,What arangement of $n$ points in the plane minimizes the dispersion of the distances between them?,"If $n=3$ the solution is an equilateral triangle where all sides have the same length. In general, there are $N = \frac{n}2(n-1)$ distances between the points. Use the empirical coefficient of variation as a relative measure of dispersion of the distances $x_i$ $$v = \frac1{\bar{x}} \sqrt{\frac1N \sum_{i=1}^N (x_i - \bar{x})^2} \qquad \bar{x} = \frac1N \sum_{i=1}^N x_i$$ Below are some arangements of four points and their values of $v$ (distances with the same length are coloured). I have not found an arangement for $n=4$ with a smaller variation than the square. It is possible to show that for two points at $(\pm 1,0)$ and the two other ones at $(0,\pm y)$ the minimum of $v$ occurs at $y = 1$ . $\hspace{4cm}$ For $n < 8$ the regular polygons seem to minimize the variation $v = P(n)$ . After that it is better to place one point at the center and arange the others evenly around it like in the arangement on the right. In that case, call $v=Q(n)$ $$P(n) = \tan{\left( \frac\pi{2n} \right)} \sqrt{N - \cot^2{\left( \frac\pi{2n} \right)}} \qquad
Q(n) = \frac{\sqrt{\frac{n^2}2 - \left( \cot{\left( \frac\pi{2(n-1)} \right)} + 1 \right)^2}}{\cot{\left( \frac\pi{2(n-1)} \right)} + 1}$$ $$\begin{array}{c|cccc|ccc} n & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline P(n) & 0.172 & 0.236 & 0.277 & 0.3066 & 0.328 & 0.345 & 0.359 \\ Q(n) & 0.268 & 0.269 & 0.287 & 0.3068 & 0.324 & 0.339 & 0.351 \end{array}$$ Are there better strategies for $n \to \infty$ ? And is there a way to prove an arangement is optimal for a given $n$ ?","['optimization', 'statistics', 'geometry']"
4292642,Offline manual backpropagation - Is it correct?,"I want to understand the algorithm of backpropagation so I created this example by myself: I want to backpropagate through the network offline when Additionally in last layer we consider linear activation function and MSE as loss function. My solution I generally understand the idea behind backpropagation, I want just to assure if I understand how it works in offline case. I have question about two derivatives: First is about backpropagating error with respect to weight 9 And second is about backpropagation MSE with respect to weight 6: where neth_2 means input in node h2. Additionally: Can I please ask you to check whether those derivatives make sense? I'm really not sure how backpropagation works in offline case so it would be very handy for me if you could check it!","['statistics', 'gradient-descent', 'neural-networks', 'vector-analysis']"
4292693,Why do we need Fubini's theorem in this proof of Minkowski's inequality for integrals,"I'm reading Theorem 6.19 in textbook Real Analysis: Modern Techniques and Their Applications by Gerald B. Folland. Suppose that $(X, \mathcal{M}, \mu)$ and $(Y, \mathcal{N}, \nu)$ are $\sigma$ -finite measure spaces, and let $f$ be an $(\mathcal{M} \otimes \mathcal{N})$ -measurable function on $X \times Y$ . a. If $f \geq 0$ and $1 \leq p<\infty$ , then $$
\left[\int\left(\int f(x, y) d \nu(y)\right)^{p} d \mu(x)\right]^{1 / p} \leq \int\left[\int f(x, y)^{p} d \mu(x)\right]^{1 / p} d \nu(y)
$$ b. If $1 \leq p \leq \infty, f(\cdot, y) \in L^{p}(\mu)$ for a.e. $y$ , and the function $y \mapsto\|f(\cdot, y)\|_{p}$ is in $L^{1}(\nu)$ , then $f(x, \cdot) \in L^{1}(\nu)$ for a.e. $x$ , the function $x \mapsto \int f(x, y) d \nu(y)$ is in $L^{p}(\mu)$ , and $$
\left\|\int f(\cdot, y) d \nu(y)\right\|_{p} \leq \int\|f(\cdot, y)\|_{p} d \nu(y).
$$ In the proof of (b), he said that Assertion (a) therefore follows from Theorem 6.14. When $p<\infty$ , (b) follows from (a) (with $f$ replaced by $|f|$ ) and Fubini's theorem; when $p=\infty$ , it is a simple consequence of the monotonicity of the integral. Could you please explain? Why do we need to use Fubini's theorem? I could not see the need of swapping differential operators here. How is the monotonicity of the integral used to obtain the result for $p = \infty$ here?","['proof-explanation', 'measure-theory', 'fubini-tonelli-theorems', 'inequality']"
4292720,Why am I allowed to cancel terms inside an integral?,"Given the definite integral $$
\int_0^{\pi} \frac{\cos \theta}{\cos \theta} \ \text{d}\theta
$$ When $\theta = \frac{\pi}{2}$ , the integrand becomes $$
\frac{0}{0}
$$ Since the integral can be thought of as the limit of a Riemann sum and each $\theta$ in the interval $[0,\pi]$ will eventually be substituted into the integrand, then would it still be correct to write $$
\int_0^{\pi} \frac{\cos \theta}{\cos \theta} \ \text{d}\theta = \int_0^{\pi} 1 \ \text{d}\theta
$$ If so, why?","['integration', 'calculus']"
4292750,"linear independence of $\{x_j-x_i\}_{j=1 ,\\j \ne i}^{k+1}$? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Show that if the set $\{x_2-x_1,...,x_{k+1}-x_1\} \subset \mathbb{R}^n$ is linearly independent then the following set is also linearly independent: $$\{x_1-x_i,...,x_{i-1}-x_i,x_{i+1}-x_i,...,x_{k+1}-x_i\}$$","['convex-analysis', 'linear-algebra', 'vector-spaces', 'convex-hulls']"
4292774,Is the independence number of the (strong) product of two graphs equal to the product of the independence numbers?,"Consider two graphs $G=(V,E)$ and $H=(W,F)$ . For the sake of simplicity, say that both $G$ and $H$ contain all self-loops. Let $N_G(v)$ and $N_H(w)$ be the first neighborhoods of any vertex $v\in V$ (in $G$ ) and $w\in W$ (in $H$ ), respectively. Then the strong product $G\boxtimes H$ of $G$ and $H$ is a graph with set of nodes $V\times W$ in which a pair $(v,w)\in V\times W$ is adjacent to another one $(v',w')\in V \times W$ if and only if $(v,w) \in N_G(v')\times N_H(w')$ . It is easy to see that the independence number $\alpha(G\boxtimes H)$ of $G\boxtimes H$ is always larger than or equal to the product $\alpha(G)\alpha(H)$ of the independence numbers $\alpha(G)$ of $G$ and $\alpha(H)$ of $H$ . This is because the Cartesian product $I\times J$ of an independent set $I$ in $G$ with an independent set $J$ in $H$ is an independent set in $G\boxtimes H$ . I assume it is not always the case that $\alpha(G\boxtimes H) = \alpha(G)\alpha(H)$ (not even up to constants) because I keep finding the upper bound $\alpha(G\boxtimes H) \le \alpha^\star(G)\alpha(H)$ , where $\alpha^\star(G)$ is the fractional packing (or Rosenfeld) number of $G$ , which I know can be much bigger than $\alpha(G)$ .
This got me wondering: Question: Is it possible that $\alpha(G\boxtimes H) \gg \alpha(G)\alpha(H)$ ? E.g., can I find two sequences of graphs $(G_n)_{n\in\mathbb N},(H_n)_{n\in\mathbb N}$ with $\alpha(G_n)\alpha(H_n)= const$ but $\alpha(G_n\boxtimes H_n) \to \infty$ as $n\to \infty$ (where the divergence is, say, faster than logarithmic)?","['graph-theory', 'combinatorics', 'discrete-mathematics']"
4292775,How to find the exact value of the integral $ \int_{0}^{\infty} \frac{d x}{\left(x^{3}+\frac{1}{x^{3}}\right)^{2}}$?,"$\textrm{I first reduce the power two to one by Integration by Parts.}$ $\begin{aligned}\displaystyle \int_{0}^{\infty} \frac{1}{\left(x^{3}+\frac{1}{x^{3}}\right)^{2}} d x &=\int_{0}^{\infty} \frac{x^{6}}{\left(x^{6}+1\right)^{2}} d x\\&=\displaystyle -\frac{1}{6} \int_{0}^{\infty} x d\left(\frac{1}{x^{6}+1}\right)\\&
=\displaystyle -\left[\frac{x}{6\left(x^{6}+1\right)}\right]_{0}^{\infty}+\frac{1}{6} \int_{0}^{\infty} \frac{1}{x^{6}+1} d x \quad \textrm{ (Via Integration by Parts})\\&=\displaystyle \frac{1}{6} \int_{0}^{\infty} \frac{1}{x^{6}+1} d x\end{aligned}$ $\textrm{Then I am planning to evaluate }\displaystyle I= \int_{0}^{\infty} \frac{1}{x^{6}+1}\text{ by resolving }\frac{1}{x^{6}+1} \text{ into partial fractions.}$ But after noticing that $$I=\int_{0}^{\infty} \frac{d x}{x^{6}+1}\stackrel{x\mapsto\frac{1}{x}}{=} \int_{0}^{\infty} \frac{x^{4}}{x^{6}+1} d x,$$ I changed my mind and started with $3I$ instead of $I$ as below: $$
\begin{aligned}
3 I &=\int_{0}^{\infty} \frac{x^{4}+2}{\left(x^{2}+1\right)\left(x^{4}-x^{2}+1\right)} d x \\
&=\int_{0}^{\infty}\left(\frac{1}{x^{2}+1}+\frac{1}{x^{4}-x^{2}+1}\right) d x \\
&=\left[\tan ^{-1} x\right]_{0}^{\infty}+\int_{0}^{\infty} \frac{\frac{1}{x^{2}}}{x^{2}+\frac{1}{x^{2}}-1} d x \\
&=\frac{\pi}{2}+\frac{1}{2} \int_{0}^{\infty} \frac{\left(1+\frac{1}{x^{2}}\right)-\left(1-\frac{1}{x^{2}}\right)}{x^{2}+\frac{1}{x^{2}}-1} d x\\ 
&=\frac{\pi}{2}+\frac{1}{2}\left[\int_{0}^{\infty} \frac{d\left(x-\frac{1}{x}\right)}{\left(x-\frac{1}{x}\right)^{2}+1}-\int \frac{d\left(x+\frac{1}{x}\right)}{\left(x+\frac{1}{x}\right)^{2}-3}\right] \\
&=\frac{\pi}{2}+\frac{1}{2}\left[\tan ^{-1}\left(x-\frac{1}{x}\right)\right]_{0}^{\infty}-0 \\
&=\frac{\pi}{2}+\frac{1}{2}\left[\frac{\pi}{2}-\left(-\frac{\pi}{2}\right)\right] \\
&=\pi \\ \therefore I &=\frac{\pi}{3}
\end{aligned}
$$ Now I can conclude that $$\boxed{\displaystyle \quad \int_{0}^{\infty} \frac{1}{\left(x^{3}+\frac{1}{x^{3}}\right)^{2}} d x=\frac{\pi}{18} }.$$ :|D Wish you enjoy the solution! Opinions and alternative  methods are welcome.",['integration']
4292887,"$\left \langle f,g \right \rangle$ defines an inner real product in $\mathcal{C}[-\frac{\pi}{2},\frac{\pi}{2}]$","Consider $\mathcal{C}[-\pi/2,\pi/2]$ the vectorial $\mathbb{R}$ space of continuous real functions in $[-\pi/2,\pi/2]$ . For $f,g \in \mathcal{C}[-\pi/2,\pi/2]$ define $$\left \langle f,g \right \rangle=\int_{-\pi/2}^{\pi/2}\cos(t)f(t)g(t)dt.$$ Prove $\left \langle f,g \right \rangle$ defines an inner real product in $\mathcal{C}[-\frac{\pi}{2},\frac{\pi}{2}]$ . Attempt: $\bullet$ Linearity follows from the linearity of the integral. $\bullet$ Conjugate symmetry follows from $$\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\cos(t)f(t)g(t)dt=\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\cos(t)g(t)f(t)dt.$$ $\bullet$ If $0 \neq f \in \mathcal{C}[-\frac{\pi}{2},\frac{\pi}{2}]$ , then $$\left \langle f|f \right \rangle=\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\cos(t)f^{2}(t)dt>0$$ because of the continuity of $f$ , $f^{2}>0$ , the continuity of $\cos$ and $\cos >0$ in $[-\frac{\pi}{2},\frac{\pi}{2}]$ . Since $\left \langle 0,0 \right \rangle=0$ holds, positive definiteness follows from $\bullet$ Since $f\in \mathcal{C}[-\frac{\pi}{2},\frac{\pi}{2}]$ , $f^{2} \geq 0$ . Since $\cos >0$ in $[-\frac{\pi}{2},\frac{\pi}{2}]$ , $\left \langle f,f \right \rangle=0\implies f=0$ . $\bullet$ If $f \in \mathcal{C}[-\frac{\pi}{2},\frac{\pi}{2}]$ , $f^{2} \geq 0$ . Since $\cos>0$ in $[-\frac{\pi}{2},\frac{\pi}{2}]$ , $\left \langle f,f \right \rangle\geq 0,\forall f \in \mathcal{C}[-\frac{\pi}{2},\frac{\pi}{2}]$ follows. $\therefore$ $\left \langle f,g \right \rangle$ defines an inner real product in $\mathcal{C}[-\frac{\pi}{2},\frac{\pi}{2}]$ .","['integration', 'inner-products', 'solution-verification', 'linear-algebra', 'trigonometry']"
4292904,Angle between tangent on circle and a line to a point on a larger concentric circle,"I have two concentric circles, the smaller one has radius $r$ and the larger one radius $r+a$ . I am trying to calculate the angle between the tangent line at a point $A$ on the smaller circle and the line from point $A$ to point a $B$ on the larger circle. I made this diagram to illustrate my problem. The angle I want to find is angle $\beta$ . Distances $a$ (between $B$ and $D$ ), $d$ (along the circumference of the inner circle), and $r$ are known. With this information, I can calculate angles $\alpha$ , $\delta$ , and distance $b$ , angles $\epsilon$ and $\theta$ , as well as the supplementary angles $\delta'$ and $\theta'$ (not drawn to avoid clutter). Intuitively I see that triangles $ABD$ and $ABE$ are now fully defined but I am not able to work out angles $\beta$ and $\eta$ . How do I solve this problem? I want to code this problem with single-precision floating-point numbers in C++ so a computationally efficient solution is preferred. EDIT: In this example point $B$ lies ""above the horizon"" as seen from point $A$ . Is it also possible to calculate angle $\beta$ when $B$ is below the horizon?",['trigonometry']
4292918,Induced $\frak{g}$-action on exterior power and symmetric power?,"What to show . Let $\frak{g}$ be a Lie algebra and $V$ a $\frak{g}$ -representation. I am supposed to show that for $r \geq 0$ there exists a unique action of $\frak{g}$ on the exterior power $\bigwedge ^r V$ such that  the canonical projection $V^{\otimes r} \rightarrow \bigwedge ^r V$ is a $\frak{g}$ -intertwiner. Here, we consider the tensor product of representations on the left-hand side. Afterwards, I am asked to show the above statement, where the exterior power is replaced by the symmetric power $S^rV$ . What I think . Consider the exterior power.
Uniqueness seems easy: The canonical projection is supposed to be a $\frak{g}$ -intertwiner. This forces us to define on equivalence classes of homogenous tensors $x.[v_1 \otimes ... \otimes v_r] := [x. (v_1 \otimes ... \otimes v_r)]$ for $x \in \frak{g}$ . As homogenous tensors span $V^{\otimes r}$ there image under the canonical projection spans $\bigwedge ^r V$ . Hence, the above definition determines the action uniquely. What remains to show is that this map is well-defined, and indeed gives a $\frak{g}$ -action. That is where I am struggling. I have no experience in working with exterior powers. I donâ€™t know what relations I can use. Questions . Any hints? Are well-definedness and the two action properties (bilinearity and the â€œLie algebra action propertyâ€) the only thing left to show? Many thanks!","['lie-algebras', 'representation-theory', 'abstract-algebra', 'tensor-products', 'exterior-algebra']"
4292924,Initial conditions for which these two recursive sequences converge,"The following problem is a generalization of an exercise that the professor give me and that I have already solved. In the initial statement $\alpha=0$ and given $0<a_0<1$ , the limit of the first sequence  is $\dfrac{a_0}{1-a_0}$ . I was wondering what happen if I change a bit the exercise, with $\alpha>1$ it seems to me that the sequence diverges, but what happens if $0<\alpha<1$ ? Given $N\in\mathbb{N}$ and $0<\alpha<1$ , we define for each $0\leq n \leq N-1$ $$a_{n+1,N}=a_{n,N}+\dfrac{1}{N}a_{n,N}^2\left(\dfrac{2}{b_{n,N}}\right)^\alpha\\
b_{n+1,N}=b_{n,N}-\dfrac{1}{N}a_{n,N}^2\left(\dfrac{2}{b_{n,N}}\right)^\alpha.$$ There exist any values of $a_0=a_{0,N}$ and $b_0=b_{0,N}$ such that the sequences $\{a_{N,N}\}$ and $\{b_{N,N}\}$ converge to positive numbers? Attempt: I have realized that $a_N+b_N=a_0+b_0$ for every $N$ , so it suffices to see that $\{b_{N,N}\}$ is bounded below or $\{a_{N,N}\}$ is bounded above. I don't know how to make rigorous calculations, but I have found out by using the software Mathematica that apparently  with $\alpha=1/2$ there is convergence for some pairs $(a_0,b_0)$ like $a_0=0.004; b_0=0.003$ $a_0=0.008 ; b_0=0.004$ $a_0=0.003 ; b_0=0.001$ $a_0=0.32 ; b_0=0.24$ $a_0=0.021 ; b_0=0.017$ Thanks.","['real-analysis', 'calculus', 'numerical-methods', 'sequences-and-series', 'convergence-divergence']"
4292928,How does Brownian motion exit a square? Or the harmonic measure on a square.,"Consider a circle $\mathcal{C}$ in $\mathbf R^2$ of unit radius and a Brownian motion $\{X_t\}_{t\in \mathbf R^+} $ starting from its center. Let $\mu$ be the measure on $\mathcal{C}$ defined on arcs $I$ as $$
\mu(I) = Prob(X_\tau \in I), 
$$ where $\tau = \text{inf}_t (X_t \in \mathcal{C})$ . Namely, $\mu(I)$ is the probability that $X$ leaves the circle through the arc $I$ . It is well known that $\mu$ is just Lebesgue measure on $\mathcal{C}$ , simply by isotropy of Brownian motions. My question is: what happens if we replace the circle with a square? Is there any closed formula for the probability density of the exit point of a Brownian motion from a square? Guess: a friend of mine proposed this function $$
p(x) = \frac{\frac{1}{1+x^2} - \frac{1}{2}}{\frac{\pi}{2} -  1}
$$ as probability density on a single side (perhaps a factor of $1/4$ is needed to normalize), where $x$ is distance from the middle point of the square side. EDIT: I leave you some nice Python simulations","['stochastic-processes', 'brownian-motion', 'probability-theory', 'polygons']"
4292930,Does the inclusion of schemes affine over $S$ to $S$-schemes have a left adjoint?,"Let $S$ be a scheme. Consider the category $\mathrm{Aff}_{/S}$ of schemes affine over $S$ , by which I mean $S$ -schemes $X$ such that the structure morphism $X \to S$ is affine. For simplicity, I'll also call those affine $S$ -schemes (though they don't have to be affine as an absolute scheme!). Consider the inclusion functor $\mathrm{Aff}_{/S} \to \mathrm{Sch}_{/S}$ . Does this have a left adjoint? The question is motivated by the ""absolute analogue"". It's well-known for $S=\mathrm{Spec}(\Bbb Z)$ that a left adjoint to $\mathrm{Aff} \to \mathrm{Sch}$ exists and it is given by $X \mapsto \mathrm{Spec}(\Gamma(X,\mathcal O_X))$ . The same construction works as long as $S$ is affine. One naÃ¯ve way one might try to do a similar thing in the general setting is to use the relative Spec. The relative Spec construction has a nice universal property, so that seems promising. The idea is to define a functor $\mathrm{Sch}_{/S} \to \mathrm{Aff}_{/S}$ by sending $X \xrightarrow{f} S$ to $\underline{\mathrm{Spec}}_S(f_* \mathcal O_X) \to S$ . The problem with this is that $\underline{\mathrm{Spec}}_S(f_* \mathcal O_X)$ is not even defined, because for general $f$ , there's no reason that $f_* \mathcal O_X$ is quasicoherent! (This is however true if $f$ is qcqs.) So that doesn't work. Of course, via the relative Spec functor, the category of affine $S$ -schemes is anti-equivalent to the category of quasicoherent $\mathcal O_S$ -algebras, so the question could be recast in terms of asking for a functor from $S$ -schems to quasicoherent $\mathcal O_S$ -algebras with certain properties. Certainly taking the pushforward of the structure sheaf gives us a canonical $\mathcal O_S$ -algebra, that may however not be quasicoherent, as mentioned before. So I guess if there was a sufficiently canonical way to associate to each $\mathcal O_S$ -algebra a quasicoherent $\mathcal O_S$ -algebra, applying that to the pushforward of the structure sheaf and then taking the relative Spec would give us our desired functor. Actually, there are some situations where one can associate to some non-quasicoherent module a ""canonical"" quasicoherent module with a suitable universal property, see here . I don't see how that lemma is applicable in this situation, however. One could of course try to use adjoint functor theorems, however the original motivation for the question was figuring out whether the inclusion $\mathrm{Aff}_{/S} \to \mathrm{Sch}_{/S}$ preserves limits. (Note that $\mathrm{Aff}_{/S}$ actually has all limits because the category of quasicoherent $\mathcal O_S$ -algebras has all colimits.) That's of course asking less than the existence a left adjoint, but in practice exhibiting a left adjoint is often a good way to show that a functor preserves limits.","['adjoint-functors', 'algebraic-geometry', 'schemes']"
4292931,Probability - Am I understanding the problem correctly (Part II)?,"The problem It is given that a certain type of battery has a mean shelf life of $30$ months with a standard deviation of $3$ months. For a randomly selected battery let ð‘‹ denote the lifetime of the battery. Assuming a normal distribution: $$X\sim\mathcal{N}(30,9).$$ The question asks to estimate how long $90\%$ of batteries are expected to last. The solution(?) The probability that a randomly selected battery has a lifetime of $\approx2.18$ years can be evaluated as $$p=\mathbb P(X>2.18)=\mathbb P(Z>-1.28)=0.9$$ where $Z$ is a standard normal random variable. I'm uncertain as to why (or why not) $2.18$ years is the estimate of how long $90\%$ of batteries are expected to last. Please advise. Thanks.","['statistics', 'normal-distribution', 'probability']"
4292933,$A$ has property of Baire if and only if $A=B \sqcup Q$ where $B$ is a $G_{\delta}$ set and $Q$ is of first category,"Over $\Bbb{R}$ , A set $A$ with a property of Baire is defined in our notes as the symmetric difference $A=G\triangle Q$ where $G$ is open and $Q$ is of first category. I am asked to show that a set $A$ has property of Baire if and only if $A=B \sqcup Q$ where $B$ is a $G_{\delta}$ set and $Q$ is of first category. I can show that for $A=G\triangle Q$ with open $G$ and first-category $Q$ , one gets $N=\overline{G}\setminus G$ is closed (and nowhere dense), and $N\triangle Q$ is of first category, hence $A={\overline{G}\triangle N \triangle Q}=\overline{G}\triangle (N \triangle Q)$ which means it is also a symmetric difference of closed set and a first category set. But for the countable intersection of open sets $B=\bigcap B_i$ , I am not sure what to do similar to the above method. Is $B$ nowhere dense? If so, then $A $ is a first category set, but is it still the symmetric difference between an open set and a first-category set? I am not sure what to say about such $B$ .","['solution-verification', 'baire-category', 'functional-analysis', 'real-analysis']"
4292977,"Is $\mathbb{R}$ as a complete field a/the ""smallest"" complete field containing the field $\mathbb{Q}$?","I know that the metric space of real numbers equipped with the Euclidean metric $(\mathbb{R}, d_{\mathrm{Eu}})$ is the Euclidean- , not a p-adic , completion of the metric space of rational numbers equipped with the Euclidean metric $(\mathbb{Q}, d_{\mathrm{Eu}})$ . Then... Question Out of Curiosity: Is $\mathbb{R}$ as a ( Cauchy- ) complete field (in terms of the Euclidean metric) a/the "" smallest "" ( Cauchy- ) complete field containing (an isomorphic copy of) the field $\mathbb{Q}$ (in terms of the Euclidean metric)? By "" smallest "" I mean two things out of curiosity: (1) If $\mathbb{F}$ is some ( Cauchy- ) complete field (in terms of the Euclidean metric), must $\mathbb{F}$ contain (an isomorphic copy of) the field $\mathbb{R}$ as a subfield? (2) If $\mathbb{F}$ is some ( Cauchy- ) complete field (in terms of the Euclidean metric) containing (an isomorphic copy of) the field $\mathbb{Q}$ (in terms of the Euclidean metric), must $\mathbb{F}$ have cardinality strictly greater than $\mathbb{Q}$ (i.e., must $\mathbb{F}$ be uncountable)? Actually, leading by my own curiosity, I have found this paper "" Analysis in the Computable Number Field "" by Oliver Aberth published in 1968: https://sci-hub.se/10.1145/321450.321460 . After giving it a read, I think Oliver Aberth constructed a countable ( Cauchy- ) complete field (in terms of the Euclidean metric) containing (an isomorphic copy of) the field $\mathbb{Q}$ as a subfield, hence the field $\mathbb{R}$ is not a/the "" smallest "". But an online friend who is much more mathematically matured than me said (nearly a month ago) the paper is misleading so I am misunderstanding the facts. Until now I still don't know why, so I am asking here and wishing someone can help about my curiosity and/or clarify what Oliver Aberth actually did in the paper.","['field-theory', 'complete-spaces', 'abstract-algebra', 'analysis']"
4292979,Signed Borel Measures and Functions of Bounded Variation,"Let $\nu$ be a finite signed Borel measure on the closed interval $[a,b]$ . We can define a function $F_\nu : [a,b]$ by $$F_\nu(x) = \nu([a,x]).$$ It can be shown that $F_\nu$ has bounded variation and is right-continuous. An exercise in my class notes asks to prove that $|\nu|([a,b]) = V(F_\nu, [a,b])$ , where the right-hand side denotes the total variation of $F_\nu$ . Immediately after this, the notes state the following theorem: ""The map $\nu \mapsto F_\nu$ is a bijection from the set of finite signed Borel measures on $[a,b]$ to the set of functions of bounded variation that take value $0$ at $a$ and that are right-continuous."" My problem is that neither of these statements are true. Both break when we consider the measure $\delta$ given by $$\delta(A) = \begin{cases} 1, &\quad a \in A \\ 0, &\quad a \notin A
\end{cases}.$$ In this case, $F_\delta$ is identically $1$ , and so $V(F_\delta,[a,b]) = 0$ while $|\delta|([a,b]) = \delta([a,b]) = 1$ . Moreover, $F_\delta$ does not take the value $0$ at $a$ , so the second statement is false too. How do we fix this? Can we just limit ourselves to the meausures $\nu$ for which $\nu(\{a\}) = 0$ ? Or is there a change we can make so that the results still apply to all signed measures?","['measure-theory', 'signed-measures', 'borel-measures', 'real-analysis', 'total-variation']"
4292984,"Are a (co)tangent vector's ""coordinate components"" equivalent to the ""coordinate components of its pushfoward to R^n""?","[All equation numbers reference Wald, Robert M. , General relativity , Chicago-London: The University of Chicago Press. XIII, 491 p. 34.50 (1984).] Consider a subset $O$ of an $n$ -dimensional, $C^{\infty}$ , real manifold $M$ .  Let $\psi:O\to V\subset\mathbb{R}^n$ be a chart and let $x^\mu:\mathbb{R}^n\to\mathbb{R}$ denote coordinates corresponding to some choice of basis $\{e_\mu\}$ on $\mathbb{R}^n$ .  Composition yields the smooth maps $x^\mu\circ \psi:O\to\mathbb{R}$ . Now, the corresponding coordinate components $v^\mu$ of any tangent vector $v\in V_p$ at $p\in O$ are given by \begin{align}
v^\mu=v(x^\mu\circ\psi(p)), \tag{2.2.6}\label{v mu}
\end{align} with $v(f)$ viewed as a linear map $v:C^{\infty}(M)\to\mathbb{R}$ . On the other hand, the push-forward map $\psi^*:V_p\to V_{\psi(p)}$ associated with $\psi$ is defined via \begin{align}
(\psi^*v)(f)\equiv v(f\circ\psi) \tag{C.1.1}\label{push v},
\end{align} for all smooth $f:\mathbb{R}^n\to\mathbb{R}$ .  So, it would seem the coordinate components of $\psi^*v$ at $\psi(p)$ are given by the same expression as the coordinate components of $v$ at $p$ : \begin{align}
(\psi^*v)^\mu = (\psi^*v)(x^\mu) \overset{\eqref{push v}}{=} v(x^\mu\circ\psi(p)) \overset{\eqref{v mu}}{=}v^\mu \label{*}\tag{$\star$}
\end{align} Likewise, in the basis dual to \begin{align}X_\mu(f)\equiv \left.\frac{\partial}{\partial x^\mu}(f\circ \psi^{-1})\right|_{\psi(p)} \tag{2.2.1} \label{X mu},
\end{align} the coordinate components of any dual vector $w\in {V}^*_p$ are given by \begin{align}
w_\mu=w(X_\mu), \tag{c.f. Ch2 Problem 6.b} \label{w mu}
\end{align} Since $\psi$ is bijective, its inverse ${\psi^{-1}}:V\to O$ can provide a push-forward $(\psi^{-1})^*:{V}^*_p \to {V}^*_{\psi(p)}$ for cotangent vectors by requiring that for every $v\in V_{\psi (p)}$ , \begin{align}
((\psi^{-1})^*w)(v)=w((\psi^{-1})^*v) \tag{c.f. C.1.2} \label{push w}
\end{align} The coordinate components of $(\psi^{-1})^*w$ at $\psi(p)$ are given by, \begin{align}
((\psi^{-1})^*w)_\mu = ((\psi^{-1})^*w)\left(\frac{\partial}{\partial x^\mu} \right) \overset{\eqref{push w}}{=} w\left((\psi^{-1})^*\frac{\partial}{\partial x^\mu}\right)\overset{\eqref{X mu}}{=}w(X_\mu)\overset{\eqref{w mu}}{=}w_\mu. \label{**}\tag{$\star\star$}
\end{align} Since tensors at $p$ are defined as multilinear maps over $V_p$ and ${V}^*_p$ , one should straightforwardly obtain analogous formulas for tensors of any type using $\eqref{*}$ and $\eqref{**}$ . Question(s): Are formulas \eqref{*} and \eqref{**} and their derivations correct? Is it valid to view these formulas as saying: "" The coordinate components (associated with chart $\psi$ ) of a tensor at $p\in M$ are equal the coordinate components of the tensor's push-forward via $\psi$ to $\psi(p)\in\mathbb{R}^n$ "" ?","['manifolds', 'diffeomorphism', 'tangent-spaces', 'differential-geometry']"
