question_id,title,body,tags
3787228,"How to show a given function satisfies the Cauchy Riemann equation at a point, but is not differentiable at that point.","This is my question: Prove that the function $f(z) = u+iv$ , where $$ f(z) = \begin{cases} \frac{x^3(1+i) - y^3(1-i)}{x^2+y^2}, &\text{ if }z\neq 0,\\ 0 &\text{if }z=0.\end{cases}$$ satisfies the Cauchy-Riemann equation at the origin, but the derivative of $f$ at $z=0$ does not exists. I solved the second part about the derivative but was not able to prove the first part, that is, $u_x=v_y$ at $z=0$ . Please kindly direct me how to solve it: I got \begin{align*}
u_x&=\frac{x^4 +3x^2x^2 + 2xy^3}{(x^2 + y^2)^2}, \\ 
u_y&=\frac{ -y^4 -2x^3y -3x^2y^2}{(x^2 + y^2)^2}, \\
v_x&= \frac{x^4 - 2xy^3 + 3x^2y^2}{(x^2 + y^2)^2}, \\
v_y&=\frac{y^4+2x^2y^2-2x^3y}{(x^2 + y^2)^2}.
\end{align*} and there remains the question of how to prove $u_x=v_y$ at $(x,y)=(0,0)$ .","['complex-analysis', 'cauchy-riemann-equations', 'derivatives']"
3787271,Is there a way to prove that this function crosses the $x$ axis only twice? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I have this function ( $x>0$ ): $$   f(x)=\left(x^2+2\right)^2 \cosh (x)-\left(2-x^2\right)^2 \cosh (3 x)-\left(\frac{x}{2}\right)^2      $$ Here is the picture of the function and its first derivative I want to prove that this function crosses the $x$ axis only twice. Any hints or suggestions are welcome.","['hyperbolic-functions', 'functions', 'solution-verification', 'real-analysis']"
3787274,Additive function differentiable at 0 [duplicate],"This question already has answers here : Proving that an additive function $f$ is continuous if it is continuous at a single point (4 answers) Closed 3 years ago . Let $f:\mathbb{R}\to \mathbb{R}$ be a function such that $f(x+y)=f(x)+f(y)$ where $x,y\in \mathbb{R}$ . If $f(x)$ is differentiable at $x=0$ , then $f(x)$ is differentiable only in a finite interval containing zero $f’(x)$ is constant for all $x\in \mathbb{R}$ $f(x)$ is constant for all $x\in \mathbb{R}$ $f(x)$ is differentiable except at finitely many points To pick the right option, the function must be determined first. However, I have not been able to do from the given data, not even a bit. I need a hint to begin the solution process Multiple correct",['functions']
3787276,Generalized (Gnedenko Kolmogorov) limit theorem for distribution with inverse cube law,"If $X_1,...,X_n$ are i.i.d.:s with finite variance $\sigma^2$ (and zero mean, for simplicity), the central limit theorem tells us that the stochastic variable $S_n$ , defined by $$S_n = (X_1+...+X_n)/\sqrt{n},$$ tends to the Gaussian $N(0,\sigma^2)$ in distribution, as $n \to \infty$ . There is a generalized limit theorem, due to Gnedenko and Kolmogorov, which deals with the case when the $X_i$ are i.i.d.:s with undefined variance (i.e. infinite), see e.g. the following wikipedia article on Stable distribution but also the book ""Econophysics"" by Stanley and Mantegna. Hence let $X_1,...,X_n$ be i.i.d.:s, drawn from a symmetric distribution around the origin, and with power law tails, such that the pdf $f$ satisfies $$f(x) \sim |x|^{-(1+\alpha)},$$ where $\alpha$ is a parameter. Clearly we must have $\alpha > 0$ , in order for the distribution to be normalizable. Further, if $\alpha > 2$ , the variance is finite, so the ordinary central limit theorem applies. Hence consider the case $\alpha \in (0,2]$ . According to the generalized limit theorem, the stochastic variable $S_n$ , defined by $$S_n = (X_1+...+X_n)/n^{1/\alpha},$$ tends to a Lévy $\alpha$ -stable distribution, with stability parameter $\alpha$ (the skewness and location parameters are both zero due to the symmetry assumption), as $n \to \infty$ . My question concerns the specific case when $\alpha=2$ and whether the theorem actually applies in this case. Indeed, when $\alpha = 2$ the Lévy $\alpha$ -stable distribution reduces to a Gaussian. The generalized limit theorem, as stated above, would therefore imply that if $X_1,...,X_n$ are i.i.d.:s, either with finite variance or with inverse cube tails, then the stochastic variable $S_n$ , defined by $$S_n = (X_1+...+X_n)/\sqrt{n},$$ tends to the Gaussian $N(0,\sigma^2)$ , as $n \to \infty$ . Is this correct, i.e. does the conclusion of the central limit theorem remain valid, even when the variance of the $X_i$ is undefined, provided they are drawn from a distribution with inverse cube tails?","['probability-distributions', 'probability-theory', 'probability']"
3787332,Orbit-stabilizer for algebraic groups,"I'm currently trying to studying quotients of algebraic groups, but I find this topic very confusing. I think all my doubts boil down to the following question: Let $G$ be an algebraic group, and suppose it acts non-trivially on a projective variety $X$ , that is I have a map $G\times X\to X$ . Given a point $p\in X$ , I can consider the isotropy group $$Iso(p)=\{g\in G\mid gp=p\}$$ and the orbit of $p$ under $G$ , i.e. $$Gp=\{gp\mid g\in G\}.$$ Then when, and if yes under which hypothesis, do the following isomorphisms hold? $$\frac{G}{Iso(p)}\overset{(1)}{\simeq} Gp \overset{(2)}{\simeq} X?$$ What are the condition on $G$ , $X$ in order that $(1)$ and $(2)$ holds? I apologize for the vagueness of the question and I hope this naive question.","['group-actions', 'algebraic-geometry', 'algebraic-groups']"
3787422,Derivative of distance function on a Riemannian manifold.,"While reading Andrews and Hopper's book on Ricci flow, I found the following computation which I am not able to verify. $M$ is a compact Riemannian manifold and $p \in M$ and $r>0$ . We are interested in the function $\psi(x) = \phi\left(\frac{d_{g}(x,p)}{r}\right)$ . Here $\phi$ is a smooth bump function $\phi: [0,\infty) \to \mathbb{R}$ with the following properties. $\phi = 1$ on $[0,1/2]$ . $\phi = 0$ on $[1,\infty)$ . $|\phi'| \leq 3$ on $[1/2,1]$ . Now we want to compute the derivative of $\psi$ . Claim is that $|\nabla \psi| \leq \frac{1}{r} \sup |\phi'|$ . I assumed $r$ is small enough so that $B(p,r)$ lies in a normal neighborhood around $p$ , then $d_{g}(p,x)= \sqrt{x_{1}^{2} + \dots + x_{n}^{2}}$ in normal coordinates around $p$ . Then I see that $$
\frac{\partial \psi}{\partial x_{i}} = \frac{1}{r}\phi'\left( \frac{d_{g}(x,p)}{r}\right) \frac{x_{i}}{\sqrt{x_{1}^{2} + \dots + x_{n}^{2}}}.
$$ Thus $$
|\nabla \psi|^{2} = g^{ij}(x)\frac{\partial \psi}{\partial x_{i}} \frac{\partial \psi}{\partial x_{j}} = \frac{1}{r^{2}}\phi'\left(\frac{d_{g}(x,p)}{r}\right)^{2}\frac{g^{ij}(x)x_{i}x_{j}}{x_{1}^{2} + \dots + x_{n}^{2}}.
$$ I don't know how to go forward. I tried working in normal coordinates around $x$ , that didn't seem to work either. Also, I am not sure how to deal with the derivative of the distance function if $x$ is not in a normal neighborhood of $p$ .","['riemannian-geometry', 'differential-geometry']"
3787499,Is There a Connection Between Minimum $ {L}_{1} $ Norm Solution and LASSO?,"I am reading a book about sparsity Statistical Learning with Sparsity:
The Lasso and Generalizations . I want to know the relationship between the following two optimization problem: $$\min_{\beta} \| \beta\|_{1} \;  s.t. X\beta=y$$ and $$\arg \min_{\beta}  \frac{1}{2n}\|X\beta-y \|_{2}^{2}+\lambda \| \beta\|_{1}$$ where $X \in \mathbb{R}^{n\times d}$ , $y\in\mathbb{R}^{n}$ , $\beta \in\mathbb{R}^{d}$ , d>n and $\lambda > 0$ . Can we say that Lasso is the relaxed version of the minimum $L_{1}$ norm solution?","['statistics', 'convex-optimization', 'sparsity', 'machine-learning', 'optimization']"
3787504,Find shortest path which intersects a point in a polygon,"Sorry if I'm explaining this badly, math in English can be a bit troublesome. I have a polygon, I have a random point inside that polygon. From this point I want a line ""drawn"" edge-to-edge and to intersect the point, but I want this line to be the shortest possible. See my image below: The red dot indicates the random point inside polygon.
The green dotted line is the shortest path/line (which I'm looking for)
The blue vague line is just example of longer lines that does not match the criteria (shortest path of all paths).
And, obviously I want the path to intersect the red point. (My real problem is that I want to find the line AND all the coordinates above that line, but that can be another problem for another day unless someone's feeling really ambitious) Edit:
I want to do this because I want to somewhat (not true physics) simulate the (2D) behavior of cracking a rock and thus want to know what piece of the rock that should split away. Also, a solution for a convex-polygon would suffice (even though the image shows a non-convex).","['optimization', 'geometry']"
3787513,Checkerboard Infection Problem minimal initial infection needed [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question This problem has been brought up here a couple of times so I'm doing a copy and paste for the introduction: “There is a dangerous bacteria, Checkerichia Coli, (or C. Coli for short) which lives in the squares of checkerboards. Squares with this bacteria living in their digestive systems are said to have ""C-sickness"". Once a square has ""C-sickness,"" it is stuck with it forever! Excessive contact with C-sick squares will cause a square to become C-sick. Specifically, if a healthy square has at least two C-sick neighbors (orthognal, not diagonal), then that square becomes C-sick the next day. For example, on the left we the photo of a see a partially infected checkerboard which was taken yesterday. On the right is a photo taken today.” enter image description here The original question under this problem was ""If the terrorist has only 7 samples, can he still infect the whole board?“ which has been proven by comparing the maximal parameter of each case and thus finding out the minimal number of initially infected squares required to eventually infect the entire board. Is there a way to generalise this problem(proving this mathematically, finding out a formula for the process, etc)? Could we then extend this generalisation to a 3D cube(the infection rule could tweak a little, for instance requiring a block to border 3 instead of 2 already infected blocks in order to be infected) and find out the minimal number of initially infected blocks required in order to eventually infect the entire cube? Context: this is a task given by my teacher in high school.
My tries: I couldn't figure out a mathematical proof but have proved through exhaustion that the minimal initially infected blocks needed to eventually infect the entire cube is $2n-1$ if a blocks gets infected when bordering two already infected blocks, and $n^2$ if it needs to border three. I haven't studied discrete mathematics so inputs on this regard(proof by case analysis) would be greatly appreciated as well:) Original post: https://puzzling.stackexchange.com/questions/18074/checkerboard-infection","['game-theory', 'discrete-mathematics']"
3787529,if $a^5+b^5<1$ and $c^5+d^5<1$ then prove that ${a^2}{c^3}+{b^2}{d^3}<1$,"if $a^5+b^5<1$ and $c^5+d^5<1$ then prove that ${a^2}{c^3}+{b^2}{d^3}<1$ given $a,b,c,d$ are non  negative real numbers My try: It is easy to deduce that $a,b,c,d<1$ ,thus $$a^2c^3+b^2d^3<a^5c^5+b^5d^5<(a^5+b^5)(c^5+d^5)<1$$ I want to know if there is a more cleaner method to solve this problem and if possible with calculus. I would also  want to know if my proof has a mistake anywhere.","['multivariable-calculus', 'proof-writing', 'inequality']"
3787556,Show $\sum_k \frac{1}{4^k|x-b_k|}$ converges in many points.,"Consider the following problem from Axler's ""Measure, integration and real analysis"" (p72): Honestly, I'm not even sure where to begin. This problem appears in the section that proves Lusin's and Egorov's theorem. The abstract Lebesgue integral is not yet introduced so I can not use that. Otherwise I could just show that the sum has finite integral and could conclude. I think we can write $f= \lim_k f_k$ as a pointwise limit of the partial sums but this probably does not quite help. Further attempt: I tried to show $$\{x: f(x) < 1\}$$ contains a set of infinite measure but was unsuccesful.
A hint to get started is appreciated!","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3787562,Drunk man at the cliff problem: expected reward while staying alive? & unequal step size,"There are several variations of this Drunk man (or a monkey) at the cliff problem. I wonder what will be the expected sum of rewards if the drunk man gets paid while he stays alive. The setup is like this. At period zero, a man is standing at $n$ step from the edge of a cliff. In each period from period $1$ , he takes independent steps; $k_1$ steps forward with probability $p$ , $k_2$ steps backward with probability $q$ , and he doesn't move with probability $1-p-q$ . We assume $k_1,k_2>0$ . So, in each period, the random walk takes value of $-k_1$ w/p $p$ , $k_2$ w/p $q$ and $0$ w/p $1-p-q$ . At the end of each period, if he's still alive, he gets paid \$1 as a reward. The future payoff is discounted by $\delta$ . So, if he is expected to fall off the cliff after 3 periods, his sum of expected rewards is $$1+\delta+\delta^2.$$ My question is what is the sum of the expected rewards? There are some relevant questions and answers. Here Q1 , Q2 , Q3 ,... but because of the discount factor and the unequal step sizes, I can't figure out how to apply the identity... Any other approach or an answer?","['random-walk', 'probability-distributions', 'markov-chains', 'markov-process', 'probability']"
3787573,How to prove $\frac{a^{n+1}+b^{n+1}+c^{n+1}}{a^n+b^n+c^n} \ge \sqrt[3]{abc}$?,"Give $a,b,c>0$ . Prove that: $$\dfrac{a^{n+1}+b^{n+1}+c^{n+1}}{a^n+b^n+c^n} \ge \sqrt[3]{abc}.$$ My direction: (we have the equation if and only if $a=b=c$ ) $a^{n+1}+a^nb+a^nc \ge 3a^n\sqrt[3]{abc}$ $b^{n+1}+b^na+b^nc \ge 3b^n\sqrt[3]{abc}$ $c^{n+1}+c^na+c^nb \ge 3c^n\sqrt[3]{abc}$ But from these things, i can't prove the problem.","['summation', 'a.m.-g.m.-inequality', 'multivariable-calculus', 'muirhead-inequality', 'inequality']"
3787576,Prove $\int_0^1 \frac{dx}{(x-2) \sqrt[5]{x^2{(1-x)}^3}} = -\frac{2^{11/10} \pi}{\sqrt{5+\sqrt{5}}}$,"$$
\mbox{Prove}\quad
\int_{0}^{1}{\mathrm{d}x \over
\left(\,{x - 2}\,\right)\,
\sqrt[\Large 5]{\,x^{2}\,\left(\,{1 - x}\,\right)^{3}\,}\,}
=
-\,{2^{11/10}\,\pi \over \,\sqrt{\,{5 + \,\sqrt{\,{5}\,}}\,}\,}
$$ Being honest I havent got a clue where to start.  I dont think any obvious substitutions will help ( $x \to 1-x, \frac{1}{x}, \sqrt{x},$ more). The indefinite integral involves hypergeometric function so some miracle substitution has to work with the bounds I suspect. Maybe gamma function is involved some how ??. If anyone has an idea and can provide help I would appreciate it.","['integration', 'calculus', 'definite-integrals', 'special-functions']"
3787609,Fair coin toss and Bayes,"You have a coin and your prior assumption is that its probability of heads $\theta$ is chosen from a uniform distribution on $[0, 1]$ . You toss the coin 10 times and get 6 heads. What is the estimate of $\theta$ ? I figured that it has to be $\frac{6}{10}$ but is there a theorem or rule that can upend my guess?","['statistical-inference', 'statistics', 'parameter-estimation', 'bayesian']"
3787611,Number of integer solutions to $x^2 + xy + y^2 = c$,"Inspired by this question on Mathematica StackExchange: Consider the set of integer solutions $x, y \in \mathbb{Z}$ to the equation $x^2 + xy + y^2 = m$ , for $m \in \mathbb{N}$ . Conjecture: the number of distinct solutions to this equation is divisible by 6 for all integer $m$ . I have done a brute-force calculation in Mathematica for $m \leq 10^4$ , and have not found any counterexamples. For example, there are: 0 integer solutions for $m = 2$ ; 6 distinct solutions for $m = 3$ [ $(x,y) = \pm (2, -1)$ , $(x,y) = \pm (-1, 2)$ , and $(x,y) = \pm (1, 1)$ ]; 12 distinct solutions for $m = 7$ ; 18 distinct solutions for $m = 49$ ; and so forth.  The largest number of solutions Mathematica found was 54 solutions, for $m = 8281$ .  All of these are divisible by 6. Is there a counterexample to this conjecture for some larger value of $m$ ?  Or can the conjecture be proven? I suspect that a proof will involve some kind of hidden symmetry of the polynomial $x^2 + xy + y^2$ that maps integer solutions to integer solutions;  but I haven't been able to nail it down.  It's not hard to see that the number of solutions must be even (if $(x,y)$ is a solution, then so is $(-x, -y)$ , and these are distinct unless $x = y = 0$ );  but the divisibility by 6 is much more mysterious to me.","['number-theory', 'symmetry', 'elementary-number-theory', 'diophantine-equations']"
3787646,Prove two series are equal,"Prove $I=J$ , where: $$I=\left\{\frac{4}{\pi^6}\displaystyle\sum_{n=1}^{\infty}\displaystyle\sum_{m=1}^{\infty}\frac{1}{n^2m^2\sqrt{n^2+m^2}}\left(\pi\frac{e^{\pi\sqrt{n^2+m^2}}+e^{-\pi\sqrt{n^2+m^2}}}{e^{\pi\sqrt{n^2+m^2}}-e^{-\pi\sqrt{n^2+m^2}}}-\frac{1}{\sqrt{n^2+m^2}}\right)\right\}^{-1},$$ and $$J=12\pi^2\left(\displaystyle\sum_{n=0}^{\infty}\frac{5}{(3n+1)^2}-\frac{4}{(6n+1)^2}\right)^{-1}.$$ My atempt :  We have \begin{align*}
\displaystyle\sum_{n=0}^{\infty}\left\{\frac{1}{(3n+1)^2}+\frac{1}{(3n+2)^2}\right\}&=\displaystyle\sum_{n=1}^{\infty}\left\{\frac{1}{(3n-2)^2}+\frac{1}{(3n-1)^2}+\frac{1}{(3n)^2}\right\}-\displaystyle\sum_{n=1}^{\infty}\frac{1}{(3n)^2}\\
&=\displaystyle\sum_{n=1}^{\infty}\frac{1}{n^2}-\displaystyle\sum_{n=1}^{\infty}\frac{1}{(3n)^2}\\
&=\frac{8}{9}\displaystyle\sum_{n=1}^{\infty}\frac{1}{n^2}=\frac{4\pi^2}{27}\\
\end{align*} and: \begin{align*}
\displaystyle\sum_{n=0}^{\infty}\frac{1}{(3n+2)^2}&=4\displaystyle\sum_{n=0}^{\infty}\frac{1}{(6n+4)^2}\\
&=4\displaystyle\sum_{n=0}^{\infty}\left\{\frac{1}{(6n+1)^2}+\frac{1}{(6n+4)^2}\right\}-4\displaystyle\sum_{n=1}^{\infty}\frac{1}{(6n+1)^2}\\
&=4\displaystyle\sum_{n=0}^{\infty}\left\{\frac{1}{(3n+1)^2}-\frac{1}{(6n+1)^2}\right\}\\
\end{align*} So : \begin{align*}
\frac{4\pi^2}{27}&=\displaystyle\sum_{n=0}^{\infty}\frac{1}{(3n+1)^2}+4\displaystyle\sum_{n=0}^{\infty}\left(\frac{1}{(3n+1)^2}-\frac{1}{(6n+1)^2}\right)\\
&=\displaystyle\sum_{n=0}^{\infty}\frac{5}{(3n+1)^2}-\frac{4}{(6n+1)^2}\\
&\implies \frac{1}{12\pi^2}\displaystyle\sum_{n=0}^{\infty}\frac{5}{(3n+1)^2}-\frac{4}{(6n+1)^2}=\frac{1}{81}\\
&\implies 12\pi^2\left(\displaystyle\sum_{n=0}^{\infty}\frac{5}{(3n+1)^2}-\frac{4}{(6n+1)^2}\right)^{-1}=81\\
\end{align*} Finally $ J=81$ , but how to prove $I=81$ ?
Any help for how to calculate $I$ ? Thanks in advance","['convergence-divergence', 'calculus', 'sequences-and-series', 'real-analysis']"
3787662,Definition of a presheaf on a topological space with values in an abelian category,"As far as I know if $C$ be a category, then a presheaf on $C$ is simply a functor $F:C^{op}\longrightarrow Set$ . Now, let $X$ be a topological space and let $O(X)$ be the set of opens of $X$ . Now, if we look at $O(X)$ as a category, then, as far as I know a presheaf on X, is simply a presheaf on $O(X)$ which is a functor $F:O(X)^{op}\longrightarrow Set$ . For some people, for a presheaf on a topological space, it is also needed that $F(\emptyset)=\{ * \}$ . Can we obtain it from the definition that I know? If not, why and how it is true that we need also this condition? Actually, I am looking to see why in the definition of a presheaf on a topological space with values in an abelian category, it is needed that $F(\emptyset)=0$ ?","['abelian-categories', 'algebraic-geometry', 'category-theory', 'sheaf-theory']"
3787689,What is the intuition behind the trace of an endomorphism?,"Looking back on my math education, I noticed that even though the trace of an endomorphism came up a lot, I'd be hard pressed to give a good description of what the trace really means. I'm looking for some good intuition about its meaning. To elaborate on what I'm looking for: if I forgot the rigorous definition of the determinant, I could rebuild it from scratch without reference to a basis because I know that it's supposed to measure the change in volume and orientation of a parallelepiped under a linear transformation. For the same reason, I can quickly tell that it is independent of basis, multiplicative, and determines wether the endomorphism is injective or not, all without doing any calculations. I want something similar for the trace. It doesn't need to be geometric, but I want to know what the trace tells us about how the endomorphism acts .","['trace', 'linear-algebra', 'intuition']"
3787747,Calculate $\lim_{n \to\infty}\sqrt[n]{\{\sqrt{2}\}\{2\sqrt{2}\}\{3\sqrt{2}\}\cdots\ \{n\sqrt{2}\} }$,"$$\text{Calculate :}\lim_{n \to\infty}\sqrt[n]{\{\sqrt{2}\}\{2\sqrt{2}\}\{3\sqrt{2}\}\cdots\{n\sqrt{2}\} } . $$ Note: Weyl's equidistributed criterion. The following are equivalent: $$x_n\quad\text{is equidistributed modulo 1}$$ $$\forall~ \text{continuous & 1-peridic} f: \quad\frac{1}{N}\sum_{n=1}^Nf(x_n)\rightarrow\int_0^1f $$ $$\forall~ k\in \mathbb Z^*:\quad \frac{1}{N}\sum_{n=1}^Ne^{2πikx_n}\rightarrow 0 $$ Background: Im trying to approach this problem by weyl's criterion, so my thoughts so far are: \begin{align}
&\sqrt[n]{\{\sqrt{2}\}\{2\sqrt{2}\}\{3\sqrt{2}\}\cdots\{n\sqrt{2}\} }\\
&=\big(\{\sqrt{2}\}\{2\sqrt{2}\}\{3\sqrt{2}\}\cdots\{n\sqrt{2}\}\big)^{1/n}=\\
&=\exp\left(\frac{1}{n}\log\big(\{\sqrt{2}\}\{2\sqrt{2}\}\{3\sqrt{2}\}\cdots\{n\sqrt{2}\}\big) \right)=\\
&=\exp\left(\frac{1}{n}\sum_{k=1}^n \log\big(\{k\sqrt{2}\}\big)\right)\\
\end{align} So, since $\sqrt{2}$ is irrational  then it's simple to prove that the sequence $x_n=\{ n\cdot \sqrt{2}\}$ is equidistributed $\text{mod}\ 1$ .
Let us define the continuous & $1$ -periodic function $f(x):=\log(x-[x])$ by the weyl's criterion we get: $$\begin{align*}
\frac{1}{n}\sum_{k=1}^n \log(\{k\sqrt{2}\})&\longrightarrow\int_0^1\log(\color{black}{\underbrace{\{x\}}_{=x-[x]}})dx\\[5pt]
&=\int_0^1\log(x)dx\\[5pt]
&=\bigg[x\log(x)\bigg]_0^1-\int_0^1dx\\[5pt]
&=-1\\[5pt]
\end{align*}$$ Hence $$\lim\limits_{n \to\infty}\sqrt[n]{\{\sqrt{2}\}\{2\sqrt{2}\}\{3\sqrt{2}\}\cdots\{n\sqrt{2}\} }=e^{-1}  $$ Is there something wrong? Also, can we find this limit with some other way? Let me know, thank you.","['uniform-distribution', 'ergodic-theory', 'real-analysis', 'calculus', 'limits']"
3787759,Understanding the formula for curvature,"In this khan academy article , they discuss how can define curvature as, $$ \bigg\| \frac{dT}{dS} \bigg\| = \kappa$$ In the post they write, ""However, we don't want differences in the rate at which we move along the curve to influence the value of curvature since it is a statement about the geometry of the curve itself and not the time-dependent trajectory of whatever particle happens to be traversing it. For this reason, curvature requires differentiating T(t) with respect to arc length, S(t), instead of the parameter t"" I feel this is not a sufficient explanation and more explanation is needed to clarify the formula. As it just states a reason that 'the curvature should related to the arclength (geometrical quantity) rather than velocity or time'. This doesn't really help because that is ruling out other quantities which we could have taken derivative with respect to. How would we motivate that when speaking of curvature of the intuitive idea of curvature (how much you need to turn) as the above equatoion? And, even after all this one issue remains for me still, we define unit tangent vector using parameterizations, so the tangent vector in itself is reliant on a property outside the curve. So technically speaking curvature is not fully made of properties intrinsic to curve. Refrence: $$ T = \frac{ v(t)}{|v(t)|} $$","['arc-length', 'curvature', 'differential-geometry']"
3787760,Combinatorial Fibonacci proof :$F_1 + F_2 + F_3 + \dots + F_n = F_{n + 2} - 1.$,"Question: Use a tiling argument to give a combinatorial proof that $$F_1 + F_2 + F_3 + \dots + F_n = F_{n + 2} - 1.$$ What I did: First off I found out that the number of ways of tiling a $1 \times n$ rectangle with $1 \times 1$ and $1 \times 2$ tiles is $F_{n + 1}.$ Therefore, $F_{n + 2}$ is the number of ways of tiling a $1 \times n+1$ rectangle. The subtract 1 is taking away one of the cases and so I decided to take away the case which was created with all $1 \times 1$ . I also realized that the LHS is just basically a bunch of cases together and since the RHS is just the arrangements of $1 \times 1$ and $1 \times 2$ tiles but at least one $1 \times 2$ tile in the arrangement. But however, I am having trouble finding out how to make these cases.","['summation', 'fibonacci-numbers', 'combinatorial-proofs', 'combinatorics', 'tiling']"
3787786,Asymptotic Estimate of Vector Function,"I would like to compute the asymptotic limit of the of the following function $$f(x,\omega) = \frac{x - \omega\sqrt{1+|x|^2}}{x\cdot \omega - \sqrt{1+|x|^2}}$$ Where $x\in \mathbb{R}^3$ and $\omega \in \mathbb{S}^2 = \{y\in\mathbb{R}^3 \ | \ |y| = 1\}$ is a point on the unit sphere. More precisely, I need to estimate $||f(x,\cdot)||_{L^\infty(\mathbb{S^2)}} := \sup_{\omega\in\mathbb{S}^2}|f(x,\omega)|$ for large $|x|$ . I have the rough estimate \begin{align}
|f(x,\omega)| &\leq \bigg|\frac{\frac{x}{\sqrt{1+|x|^2}} - \omega}{\frac{x\cdot\omega}{\sqrt{1+|x|^2}} - 1}\bigg| \leq \frac{2}{1 - \frac{|x|}{\sqrt{1+|x|^2}}}
\\
&= \frac{2\sqrt{1+|x|^2}}{\sqrt{1+|x|^2} - |x|} = \mathcal{O}(|x|^2)
\end{align} as $|x|\rightarrow \infty$ . But I am wondering if I can do better than this and obtain a sharper estimate (possibly $\mathcal{O}(|x|)$ or even $\mathcal{O}(1)$ )? Edit: To elaborate I am looking for a better function $g(|x|)$ such that \begin{align}
\sup_{(\hat{x},\omega)\in\mathbb{S}^2\times\mathbb{S}^2}|f(|x|\hat{x},\omega)| \leq C g(|x|)
\end{align} I suspect something like linear in $|x|$ like $g(|x|) = a|x| + b$ .","['multivariable-calculus', 'parameter-estimation', 'asymptotics', 'estimation']"
3787827,Monotone functions and Borel sets,"I'm studying measure theory and two question came to my mind: If $f:\mathbb{R}\to\mathbb{R}$ is monotone and $B\subseteq\mathbb{R}$ is borel, is the image $f(B)$ borel? If $f:\mathbb{R}\to\mathbb{R}$ is a monotone function (say, non-decreasing), does there exist a sequence of continuous functions $f_n:\mathbb{R}\to\mathbb{R}$ converging pointwise to $f$? Here's the motivation for those questions: Let $(X,M)$ and $(Y,N)$ be measure spaces. It's known that if $\mu$ is a measure on $(X,M)$ and $f:X\to Y$ is measure, then we have the pushforward measure $f_*\mu(A)=\mu(f^{-1}(A))$ on $(Y,N)$. What if we were to define a ""pullback measure""? Given a function $f:X\to Y$ such that $f(M)\subseteq N$ (i.e. $f$ maps measurable sets to measurable sets) and a measure $\nu$ on $N$, the natural formula would be $f^*\nu(A)=\nu(f(A))$. So we ask if is there a good amount of functions which map measurable sets to measurable sets in $\mathbb{R}$, and monotone functions seem like good candidates (for strictly monotone, continuous functions, the result is valid and there are several answers on the web). If this were true, maybe we could use some convergence argument to solve the problem above.","['measure-theory', 'real-analysis']"
3787840,Intuition behind sums of sums of whole numbers,"So I was playing around, and all this is just a curiosity and nothing serious. Anyway, most readers probably know: $$1+2+3+4+5+...+(n-1)+n=\frac{1}{2}n^{2}+\frac{1}{2}n=\binom{n+1}{n-1}$$ I started playing around, adding the individual sums of whole numbers instead of whole numbers alone. Words aren't very helpful to describe this process, instead consider the sum of sums for $n=4$ , which we shall call $N_2(4)$ for simplicity: $$\left ( 1+2+3+4 \right ) + \left ( 1+2+3 \right ) + \left ( 1+2 \right ) + \left ( 1 \right ) = 20$$ Remarkably, there is a simple formula (I did the math): $$N_{2}(n)=\binom{n+2}{n-1}$$ Where $N_2(n)$ is the sum of sums as above. Formally, $N_2(n)=\sum_{1\leq i}^{n}\sum_{1\leq j\leq i}j$ . Now imagine going further, with sums of sums of sums, for instance: $$N_3(4) = \left ( \left ( 1+2+3+4 \right ) + \left ( 1+2+3 \right ) + \left ( 1+2 \right ) + \left ( 1 \right ) \right ) + \left ( \left ( 1+2+3 \right ) + \left ( 1+2 \right ) + \left ( 1 \right ) \right ) + \left ( \left ( 1+2 \right ) + \left ( 1 \right ) \right ) + \left ( \left ( 1 \right ) \right ) = 35$$ Again, this seems to follow the pattern (I haven't explicitly checked): $$N_3(n)=\binom{n+3}{n-1}$$ And we might conjecture: $$N_k(n)=\binom{n+k}{n-1}$$ One angle of attack is this: realizing that the previous series always adds up to that of the differences between successive elements of the next series, and so verifying that: $$\binom{n+k}{n-1} - \binom{(n-1)+k}{(n-1)-1}=\binom{n+(k-1)}{n-1}$$ I.e. that $N_{k}(n)-N_{k}(n-1)=N_{k-1}(n)$ for any suitable $n$ and $k$ . My question is if there's some intuition behind all this. Maybe an alternative way of looking at this, or proving it. Why are the sums so neatly expressible?","['summation', 'binomial-coefficients', 'combinatorics', 'intuition', 'recreational-mathematics']"
3787871,Prove Uncountability of Reals in an infinite base number system (without diagonalization),"So I was trying to study some infinite dimensional spaces when I came across the idea:
What if I didn't choose a base, or rather, what if my base was infinite? I did some thinking and decided to let each number be the most simplified positive fraction $\left[\frac{p}{q}\right]$ . This makes talking about rational numbers easy, since all rational numbers are now ""whole"" numbers in this system. But then I started thinking about how to make decimals and representing any real number. I used the following example as motivation on how to consistently write/represent real numbers: $$\pi = [3].[3.1][3.14][3.141]...$$ So, basically I'm representing irrational numbers as their unique rational decimal expansion sequence. Clearly all operations compute in the same fashion as decimal. But this makes me wonder: When you express real numbers in this fashion, it makes it to where the diagonalization argument doesn't work because, for example: $[3].[3.1][\mathbf{4.14}][3.141]...$ isn't a number because it's not a a rational decimal expansion of any number. Obviously the diagonalization argument is constructive and fails to work here because I'm not representing things in exactly the same way as you do in typical numerical systems with finite bases. I guess I'm wondering if this means that real numbers are countable/listable in this numerical system, and if not, why not? Since every real number can be written as a limit of rational numbers, and you can't use a diagonalization argument to show that you can construct a number not listed in the "" set of real numbers "" like you do in the normal case.",['elementary-set-theory']
3787881,Analyzing the Collatz Conjecture using function definitions,"Apologies for the length of my definition.  If anyone has suggestions for shortening it, I am glad to update. Does it follow that for all positive integers $x_1, x_2$ where $x_1 \ne x_2$ , there exists $n$ such that $h_n(x_1) \ne h_n(x_2)$ ?  ( Note: See below for the definition $h_n(x)$ My thinking is yes.  My reasoning is below.  My argument is incomplete.  So, I would be very interested if the answer is yes, no, or whether it is an open question. Let: $g(x)= 
\begin{cases}
    1,& \text{if } x = 1\\
    3x+1,              & \text{otherwise}
\end{cases}$ $f^{a,b,c,\dots}(x) = g\left(\dfrac{g\left(\dfrac{g\left(\dfrac{g\left(\dfrac{x}{2^a}\right)}{2^b}\right)}{2^c}\right)}{\vdots}\right)$ $h_n(x) =$ the sequence of numbers generated from applying the rules of the Collatz Conjecture where each number is the maximum power of $2$ that divides the even result generated by adding $1$ after multiplying $3$ to the previous odd result. Example: $h_1(3) = 0$ with $f^{0}(3) = 10$ $h_2(3) = 0,1$ with $f^{0,1}(3) = 16$ $h_3(3) = 0,1,4$ with $f^{0,1,4}(3) = 1$ For integers $x_1 > 0, x_2 > 0, h_n(x_1) = h_n(x_2)$ if and only if each element at the same position in the difference sequences are equal. Note 1: If $x$ is odd, then $h_1(x) = 0$ Note 2:  For all positive $n$ , there exists a nonnegative integer $t$ such that $f^{h_n(x)}(x) = 3t + 1$ Note 1:  For all nonnegative integers $t,u$ , $h_1(2t+1) = h_1(2u+1) = 0$ Note 2:  If $h_2(x) = 2,2$ and $h_2(y) = 2,3$ , then $h_2(x) \ne h_2(y)$ . Example $h_4(17) = 0, 2, 3, 4$ $f^{h_4(17)} = f^{0,2,3,4}(17) = g\left(\dfrac{g\left(\dfrac{g\left(\dfrac{g\left(\dfrac{17}{2^0}\right)}{2^2}\right)}{2^3}\right)}{2^4}\right) = 1$ $f^{h_3(17)} = f^{0,2,3}(17) = 5$ $f^{h_5(17)} = f^{0,2,3,4,0} = 1$ Examples : For $x_1 = 3, x_2 = 4$ , $n=1$ and $h_1(3) = 0$ and $h_1(4) = 2$ For $x_1 = 3, x_2 = 5$ , $n=2$ and $h_2(3) = 0,1$ and $h_2(5) = $ 0,4$ It seems to me that it follows that for all positive integers $x_1, x_2$ where $x_1 \ne x_2$ , there exists $n$ such that $h_n(x_1) \ne h_n(x_2)$ .  Here's my thinking for why this is true. (1)  Assume that there are two positive integers $x_1 \ne x_2$ but for all $n > 0, h_n(x_1) = h_n(x_2)$ . (2)  Case 1: There exists a minimum $n$ such that $f^{h_n(x_1)}(x_1) = f^{h_n(x_2)}(x_2)$ Define $F^{a,b,c,\dots}(y)$ as the inverse of $f^{a,b,c,\dots}(x)$ so that if $y = f^{a,b,c,\dots}(x)$ , then $x = F^{a,b,c,\dots}(y)$ Let $i = f^{h_n(x_1)}(x_1)$ Since the inverse of each function is itself a function, it follows that it is impossible that $x_1 = F^{h_n(x_1)}(i) \ne F^{h_n(x_1)}(i) = F^{h^n(x_2)}(1) = x_2$ (3)  Case 2: There is never a case where $f^{h_n(x_1)}(x_1) = f^{h_n(x_2)}(x_2)$ even while for all $n$ , $h_n(x_1) = h_n(x_2)$ Let $a_0 = x_1, b_0 = x_2$ Define $c_i, d_j$ such that: $a_i = 2a_{i+1} + c_{i+1}$ and $b_j = 2b_{j+1} + d_{j+1}$ where each $c_i, d_j \in \{0,1\}$ Since $a_0 \ne b_0$ , there exists $n$ where $c_n \ne d_n$ . Let $m$ be the first such time so that $c_m \ne d_m$ but $c_{m-1} = d_{m-1}$ To complete the argument, I need to show that since $m$ exists, $h_m(x_1) \ne h_m(x_2)$ .  If I can figure this out, I will update. Edit: I have made an attempt to complete case 2.  It is not complete but I think that the argument is valid if I can add a lemma.","['collatz-conjecture', 'functions']"
3787984,"Generalized Likelihood Ratio Test for $p_1=p_2$ when $X_1\sim \text{Bin} (n_1,p_1)$ and $X_2\sim\text{Bin}(n_2,p_2)$","Let $X_1\sim \text{Bin} (n_1,p_1)$ , $X_2\sim\text{Bin}(n_2,p_2)$ be two independent random variables. I am trying to find the Generalized Likelihood Ratio Test for the the null hypothesis: $$H_{0}: p_1=p_2$$ The only thing I could come up with is under the null I know that $X_1+X_2\sim \text{Bin}(N=n_1+n_2,p)$ . Then I can find my size $\alpha$ test by finding the  values $K_1, K_2$ such that $$P(X_1+X_2\le K_1)\le \frac{\alpha}{2}$$ and $$P(X_1+X_2\ge K_2)\le \frac{\alpha}{2}.$$ I am just wondering  if this  is the right approach or if is there another approach that gets me my GLT. Update from comments below: Then my ratio becomes: $$\frac{(1-\bar{X})^{n_1+n_2-2x}\bar{X}^{2x}}{(1-\bar{X}_1)^{n_1-x}(1-\bar{X}_2)^{n_1-x}(\bar{X}_1\bar{X}_2)^x}$$","['statistical-inference', 'statistics', 'hypothesis-testing']"
3788002,Inverse image of compact is compact,"Let $f : X\to Y$ be a closed map of topological spaces, such that the inverse image of each point in $Y$ is a compact subset of $X$ . Is it true that the pre-image of a compact set $K$ is compact? The answer is yes but I’m not sure how to show it. I just know that the inverse image of a singleton, say $y,$ is contained in the union of finitely many sets which we can call $U_y$ . That’s it. Why is the statement of the question true? I saw this Why is the inverse image of a compact set under a special sort of function compact? But it’s too old to revive. Can someone post an easy to follow solution. I’ve just read about the finite intersection property but to understand it I had to come read it here: http://mathonline.wikidot.com/finite-intersection-property-criterion-for-compactness-in-a",['general-topology']
3788040,Prove the number of computations produced by the Nth fibonacci number,"Given the generic Fibonacci function: procedure fib(integer: n):
    if n = 0  return 0
    else if n = 1  return 1
    else return fib(n - 1) + fib(n - 2) Prove the number of computations $C(n) = F(n + 2) + F(n - 1) - 1$ by induction. The problem also gives: $C(0) = C(1) = 1$ (base case) and $C(n) = C(n - 1) + C(n - 2) + 1$ (The number of computations recursively) The basis step is given for $C(0)$ and $C(1)$ . The induction step is where I am confused. The answer in the back of the book says something along the lines of ""the proof is simple"", and offers no further explanation. I am used to straightforward induction proofs and I'm not sure how to go about solving a recursive algorithm through induction.","['recursive-algorithms', 'proof-writing', 'discrete-mathematics', 'algorithms', 'induction']"
3788084,Applications of Signed/Complex Measures,"While first studying measure theory, I found signed and complex measures to be quite exciting; I recognized complex analysis as a powerful tool, and figured complex measures would be important as well. Now, after three courses in measure theory, I find myself realizing I have never once seen an application of either signed or complex measures (beyond perhaps the space of signed measures naturally being a vector space, which can allow some additional proof techniques for the Radon-Nikodym theorem) . One reason for this is obvious: Hahn-Jordan Decomposition : Given any complex measure, we can decompose it into its real and imaginary parts and then (canonically)
decompose it into a linear combination of four non-negative measures. Using this theorem as justification, every source I have found treats non-negative measures as the fundamental concept, and relegates signed and complex measures to the wayside, except perhaps for a complex version of Radon-Nikodym. This has never sat well with me, as I would think there should be contexts in which signed (or, more generally, complex) measures should arise naturally, in which case decomposing the measure could be detrimental . With this in mind, I would ask the following related questions: Question 1 : What contexts, if any, are there in which signed or complex measures arise naturally? Question 2 : Regardless of naturality, have signed/complex measures found any nontrivial uses beyond first decomposing into a sum of non-negative measures? I recognize these questions are rather open-ended and would thus accept any insight, be it large or small.",['measure-theory']
3788103,"IF $\mu_n \rightarrow \mu$ Show that , $\sup _{A\in \mathbb{R}}|\mu_n -\mu |\rightarrow0$","Let $\mu_n ,  $ be probability measures  on $( \mathbb{R}, \mathcal{R})$ with $n \geq 1$ with charachterstic functions ${\Phi}_n$ . $\mu$ is also a probability measure with function $g$ Given that $|{\Phi}_n (t)| \leq  g(t)$ $\forall t \in  \mathbb{R}$ and $\int_{-\infty}^\infty g(t)dt< \infty $ If $\mu_n \rightarrow \mu$ Show that , $\sup _{A\in \mathbb{R}}|\mu_n  -\mu |\rightarrow0$ (i.e $\mu_n$ converges in $\mu$ in total variation norm) My thought was to use try to use Levy continuity theorem. Or maybe sheffe's Theorem (See below).
But I am not sure how.","['self-learning', 'measure-theory', 'probability-theory', 'characteristic-functions']"
3788121,Let $AD\cap (BFC) $ in points $P$ and $Q$ and let $AD\cap (ABE)=M$ then $MP=MQ$.,Let $\triangle DEF$ be the medial triangle of $\triangle ABC$ with standaring notations. Let $AD\cap (BFC) $ in points $P$ and $Q$ and let $AD\cap (ABE)=M$ then $MP=MQ$ . Here is the diagram: There was a non synthetic solution given here https://artofproblemsolving.com/community/c1213795h2168723p16462131 But I am interested in a synthetic solution. Thanks in advance.,"['contest-math', 'euclidean-geometry', 'circles', 'geometry', 'power-of-the-point']"
3788129,Potential function with a fixed set of minima and no poles.,"Can we find a continuous, almost-everywhere smooth, potential function with no poles, such that the set of local minima for the sum of these potentials around $n$ points are exactly those $n$ points? We are given a set of $n$ points $(x_0, \ldots, x_{n-1})$ in $\mathbb{R}^d$ . We consider the additive potential function $$g(x) = \sum_{i=1}^n f(x, x_i)$$ as a sum of potentials $f : \mathbb{R}^d \times \mathbb{R}^d \mapsto \mathbb{R}$ , where $f$ is continuous and almost everywhere smooth. We would like $g$ ’s minima to be exactly the $x_i$ . That is, any point $x$ where the gradient of $g$ vanishes is either one of the $x_i$ , a saddle point, or a maximum. $$\forall x, \nabla g(x) = \vec{0} \implies x \in \{x_1, \ldots, x_n\}~\vee~ \exists v, \lambda < 0, H(g)(x) v = \lambda v$$ By way of example, we can construct such a function with the gravitational potential. If we let $$f(x,y) = -\frac{1}{||x-y||},$$ I believe the property is respected. A point mass will always end up “falling” towards one of the attractors, or at least I believe that’s implied by Earnshaw’s theorem. I’m not 100% sure it works for all norms, but the Euclidian distance will do. For this potential, the $x_i$ aren’t just local minima, they are poles. We would like to find a better-behaved function which does not exhibit poles. One way to “cheat” is to cut-off the poles. Once we get close enough to one of the $x_i$ , the influence of the other points is low enough that the potential can be approximated as just the potential coming from that $x_i$ . This means that, for a given set of $x_i$ , there will always be $\epsilon > 0$ s.t. $f(x,y) = -\frac{1}{\epsilon + ||x-y||}$ works as intended. However, a third constraint is that we would like $f$ to not depend on the $x_i$ . Are there such functions? A famous non-example would be a standard multivariate Gaussian. If you sum three of those on the vertices of an equilateral triangle, counter-intuitively, the center is a stable equilibrium. In general, I suspect that $f$ cannot be smooth at $f(x,x)$ . Otherwise, $f$ would be locally quadratic and if three points in $x_i$ are sufficiently close to each other, we would expect their barycenter (w.r.t the norm induced by the Hessian of $f$ ) to be a minimum.
However, this does not rule out solutions where $f$ is non-differentiable around $f(x,x)$ .","['maxima-minima', 'multivariable-calculus', 'physics', 'hessian-matrix', 'gradient-descent']"
3788134,Can eigenvectors be scaled and still be eigenvectors?,"Given the matrix: $$
A = 
\begin{bmatrix}
2 & 0 \\
1 & 4
\end{bmatrix}.
$$ The eigenvalues are: $$λ_1 = 2,$$ $$λ_2 = 4.$$ To find the eigenvectors : $$v_1 = \operatorname{nullity}(A - λ_1I),$$ $$v_2 = \operatorname{nullity}(A - λ_2I).$$ Which yielded the following eigenvectors for $v_1$ : $$
v_1 = 
\begin{bmatrix}
-\frac{2}{3}\\
1
\end{bmatrix}.
$$ However, I have seen this alternative eigenvector for $v_1$ : $$
v_1 = 
\begin{bmatrix}
-2\\
3
\end{bmatrix}.
$$ Question: Are both eigenvectors for $v_1$ correct? In other words, can you scale an eigenvector by any real number and it still be an eigenvector of that matrix? This would make sense, as I believe we are only really interested in the direction (eigenvectors) and the factor by which the matrix scaled (eigenvalues). Thus, I would argue that the magnitude of the values within an eigenvector are meaningless. For example, the following eigenvector for $v_1$ is just as correct: $$
v_1 = 
\begin{bmatrix}
-44444\\
66666
\end{bmatrix}
$$ Which perhaps begs the question: Why aren't these called eigenvalues and eigendirections?","['vectors', 'eigenvalues-eigenvectors', 'vector-spaces', 'matrices', 'linear-algebra']"
3788141,Holomorphic maps of the Riemann sphere,"I'm trying to understand a proof that the holomorphic maps of the Riemann sphere are the rational functions from this book : All rational maps are analytic from the extended plane to itself. For the converse, suppose $f(z) \in \mathbb{C}$ for all $z \in \mathbb{C}_\infty$ . Then $f$ is entire and bounded and thus constant. We can therefore assume that $f(z_0) = \infty$ for some $z_0 \in \mathbb{C}$ (consider $f(1/z)$ if necessary). By continuity of $f$ the point $z_0$ cannot be an essential singularity of $f$ . In other words, $z_0$ is a removable singularity or a pole. By the uniqueness theorem, the poles cannot accumulate in $\mathbb{C}_\infty$ . Since the latter is compact, there can thus only be finitely many poles. Hence, after subtracting the principal part of the Laurent series of $f$ around each pole in $\mathbb{C}$ from $f$ , we obtain an entire function which grows at most like a polynomial. By Liouville's theorem, such a function must be a polynomial and we are done. I have a few questions regarding this: Using the continuity argument can we also conclude that $z_0$ must be a pole? Why does the uniqueness theorem imply the poles cannot accumulate in $\mathbb{C}_\infty$ ? Perhaps slightly less relevant to this proof, does $f : \mathbb{C}_\infty \to \mathbb{C}_\infty$ holomorphic imply $f$ is meromorphic? I'm using Conway's definition that $f$ is meromorphic iff it is analytic except for poles.",['complex-analysis']
3788142,Evaluate $\lim _{n\to \infty }\int _{0}^{1}nx^ne^{x^2}dx$,"Evaluate $\lim _{n\to \infty }\int _{0}^{1}nx^ne^{x^2}dx.$ I applied the mean value thorem of integral to $\int _{0}^{1}nx^ne^{x^2}dx.$ We get $c\in (0,1):$ $$\int _{0}^{1}nx^ne^{x^2}dx=(1-0)nc^ne^{c^2}.$$ Taking limit ( $\lim_{n\to \infty}$ )on the both side, We get, $$\lim_{n\to \infty}\int _{0}^{1}nx^ne^{x^2}dx=\lim_{n\to \infty} nc^ne^{c^2}=0.$$ My answer in the examination was wrong. I don't know the correct answer. Where is my mistake?","['integration', 'calculus', 'fake-proofs', 'real-analysis']"
3788244,"Is there any meaning to this ""Super Derivative"" operation I invented?","Does anyone know anything about the following ""super-derivative"" operation? I just made this up so I don't know where to look, but it appears to have very meaningful properties. An answer to this question could be a reference and explanation, or known similar idea/name, or just any interesting properties or corollaries you can see from the definition here? Is there perhaps a better definition than the one I am using? What is your intuition for what the operator is doing (i.e. is it still in any sense a gradient)? Is there a way to separate the log part out, or remove it? Or is that an essential feature? Definition: I'm using the word ""super-derivative"" but that is a made-up name. Define the ""super-derivative"", operator $S_x^{\alpha}$ , about $\alpha$ , using the derivative type limit equation on the fractional derivative operator $D_x^\alpha$ $$
S_x^{\alpha}  = \lim_{h \to 0} \frac{D^{\alpha+h}_x-D^{\alpha}_x}{h}
$$ then for a function $$
S_x^{\alpha} f(x) = \lim_{h \to 0} \frac{D^{\alpha+h}_xf(x)-D^{\alpha}_x f(x)}{h}
$$ for example, the [Riemann-Liouville, see appendix] fractional derivative of a power function is $$
D_x^\alpha x^k = \frac{\Gamma(k+1)}{\Gamma(k-\alpha+1)}x^{k-\alpha}
$$ and apparently $$
S_x^{\alpha} x^k =  \frac{\Gamma (k+1) x^{k-\alpha} (\psi ^{(0)}(-\alpha+k+1) - \log (x))}{\Gamma (-\alpha+k+1)} = (\psi ^{(0)}(-\alpha+k+1) - \log (x)) D_x^\alpha x^k
$$ a nice example of this, the super-derivative of $x$ at $\alpha=1$ is $-\gamma - \log(x)$ , which turns up commonly. I'm wondering if this could be used to describe the series expansions of certain functions that have log or $\gamma$ terms, e.g. BesselK functions, or the Gamma function. Potential relation to Bessel functions : For example, a fundamental function with this kind of series, (the inverse Mellin transform of $\Gamma(s)^2$ ), is $2 K_0(2 \sqrt{x})$ with $$
2 K_0(2 \sqrt{x}) = (-\log (x)-2 \gamma )+x (-\log (x)-2 \gamma +2)+\frac{1}{4} x^2 (-\log (x)-2 \gamma +3)+\\
+\frac{1}{108} x^3 (-3 \log (x)-6 \gamma +11)+\frac{x^4 (-6 \log (x)-12 \gamma +25)}{3456}+O\left(x^5\right)
$$ in the end, taking the super-derivative of polynomials and matching coefficients we find $$
S_x^1[2 \sqrt{x}I_1(2\sqrt{x})] + I_0(2 \sqrt{x})\log(x) = 2K_0(2 \sqrt{x})
$$ which can also potentially be written in terms of linear operators as $$
[2 S_x x D_x + \log(x)]I_0(2 \sqrt{x}) = 2K_0(2 \sqrt{x})
$$ likewise $$
[2 S_x x D_x - \log(x)]J_0(2 \sqrt{x}) = \pi Y_0(2 \sqrt{x})
$$ I like this because it's similar to an eigensystem, but the eigenfunctions swap over. Gamma Function: We can potentially define higher-order derivatives, for example $$
(S_x^{\alpha})^2  = \lim_{h \to 0} \frac{D^{\alpha+h}_x-2 D^{\alpha}_x + D^{\alpha-h}_x}{h^2}
$$ and $$
(S_x^{\alpha})^3 = \lim_{h \to 0} \frac{D^{\alpha+3h}_x-3 D^{\alpha+2h}_x + 3 D^{\alpha+h}_x - D^{\alpha}_x}{h^3}
$$ this would be needed if there was any hope of explaining the series $$
\Gamma(x) =  \frac{1}{x}-\gamma +\frac{1}{12} \left(6 \gamma ^2+\pi ^2\right)
    x+\frac{1}{6} x^2 \left(-\gamma ^3-\frac{\gamma  \pi ^2}{2}+\psi
    ^{(2)}(1)\right)+
\\+\frac{1}{24} x^3 \left(\gamma ^4+\gamma ^2 \pi ^2+\frac{3
    \pi ^4}{20}-4 \gamma  \psi ^{(2)}(1)\right)+O\left(x^4\right)
$$ using the 'super-derivative'. This appears to be $$
\Gamma(x) = [(S^1_x)^0 x]_{x=1} x^{-1} + [(S^1_x)^1 x]_{x=1} x +  \frac{1}{2}[(S^1_x)^2 x]_{x=1} x^2 + \frac{1}{6} [(S^1_x)^3 x]_{x=1} x^3 + \cdots
$$ so one could postulate $$
\Gamma(x) = \frac{1}{x}\sum_{k=0}^\infty \frac{1}{k!}[(S^1_x)^k x]_{x=1} x^{k}
$$ which I think is quite beautiful. Appendix: I used the following definition for the fractional derivative: $$
D_x^\alpha f(x) = \frac{1}{\Gamma(-\alpha)}\int_0^x (x-t)^{-\alpha-1} f(t) \; dt
$$ implemented for example by the Wolfram Mathematica code found here FractionalD[\[Alpha]_, f_, x_, opts___] := 
  Integrate[(x - t)^(-\[Alpha] - 1) (f /. x -> t), {t, 0, x}, 
    opts, GenerateConditions -> False]/Gamma[-\[Alpha]]

FractionalD[\[Alpha]_?Positive, f_, x_, opts___] :=  Module[
  {m = Ceiling[\[Alpha]]}, 
  If[\[Alpha] \[Element] Integers, 
    D[f, {x, \[Alpha]}], 
    D[FractionalD[-(m - \[Alpha]), f, x, opts], {x, m}]
  ]
] I'm happy to hear more about other definitions for the fractional operators, and whether they are more suitable.","['gamma-function', 'derivatives', 'fractional-calculus', 'bessel-functions']"
3788267,Why is $\frac{\partial x}{\partial y} = \frac{dx/dt}{dy/dt}$ if $x$ and $y$ are unknown?,"Given $\frac{dx}{dt} = f(x,y,t)$ and $\frac{dy}{dt} = g(x,y,t)$ . Suppose the explicit form of $x$ and $y$ are unsolvable, then the phase plane is usually considered by looking at $\frac{dx}{dy}$ . This represents how much $x$ varies with respect to $y$ , which is also $\frac{\partial x}{\partial y}$ . Thus, $$\frac{\partial x}{\partial y} = \frac{dx/dt}{dy/dt}.$$ This means that without knowing $x$ and $y$ , I can find the derivative of $x$ with respect to $y$ and vice versa. Is there any problem with this? In a hand-wavy manner, I am wondering if the last equality in the following makes sense $$\frac{\partial x}{\partial y} = \frac{\partial x}{\partial t}
\frac{\partial t}{\partial y} = \frac{\partial x/\partial t}{\partial y/\partial t}
= \frac{dx}{dy}.$$ As Lee Mosher points out, the case of $dy/dt=0$ (or even $dy/dt \equiv 0$ ) needs to be considered separately. For this question, I am most interested in the case when $dy/dt >0$ .","['partial-derivative', 'calculus', 'dynamical-systems', 'real-analysis']"
3788280,Why do we need continuity in this theorem of path independence of limit of multi variable function?,"Let $f:D\to \mathbb{R}$ , where $D\subset\mathbb{R}^2$ such that a deleted neighborhood of a point $(a,b)$ is contained in $D$ . If $\lim_{(x,y)\to(a,b)}f(x,y)=L$ exists then For any real continuous function $g$ if $\lim_{x\to a}g(x)=b$ then $\lim_{x\to a}f(x,g(x))=L$ My question is,
Why is the continuity of $g$ required ? Here is how I attempted proof : For any $\epsilon>0$ , $\exists\delta_1>0 : |x-a|<\delta_1 , |u-b|<\delta_1, (x,u)\neq (a,b)\implies|f(x,u)-L|<\epsilon$ And for the function $g$ , $\exists\delta_2>0 : 0<|x-a|<\delta_2 \implies|g(x)-b|<\delta_1$ Hence, $\exists\delta=\min(\delta_1,\delta_2) > 0 : 0<|x-a|<\delta$ $\implies 0<|x-a|<\delta_1$ and $0<|x-a|<\delta_2$ $\implies 0<|x-a|<\delta_1$ and $|g(x)-b|<\delta_1$ $\implies |x-a|<\delta_1$ and $|g(x)-b|<\delta_1$ ; $(x,g(x))\neq (a,b)$ $\implies|f(x,g(x))-L|<\epsilon$ Which step from the above requires continuity ?","['real-analysis', 'continuity', 'multivariable-calculus', 'calculus', 'limits']"
3788301,Trace-logarithm inequality $\operatorname{tr}\log (A) \leq \operatorname{tr}(A-I)$ for matrices $A$ with strictly positive eigenvalues,"I recently came across Klein's inequality , which states that for any Hermitian matrices $A, B$ of the same size and any differentiable concave function $f :(0,\infty) \to \mathbb R$ , we have $$\operatorname{Tr}\left[f(A)-f(B)-(A-B) f^{\prime}(B)\right] \leq 0$$ where $f(A)$ is the induced map
defined on the eigenvalues and corresponding projectors $P$ as $f(A) \equiv \sum_{j} f\left(\lambda_{j}\right) P_{j},$ given the spectral decomposition $A=\sum_{j} \lambda_{j} P_{j}$ . When $f= \log$ and $B=I$ Klein's identity seems to give $$\operatorname{Tr}\left[\log(A)-(A-I)\right] \leq 0$$ This resembles the usual identity that we have in $1D$ namely, $\log x \leq x-1$ . Similarly, for any matrix $A$ satisfying $\|A-I\|<1$ in operator norm we have, by the power series expression of $\log$ around $I$ : $$\log (A)=\sum_{k=1}^{\infty}(-1)^{k+1} \frac{(A-I)^{k}}{k}=(A-I)-\frac{(A-I)^{2}}{2}+\frac{(A-I)^{3}}{3} \cdots$$ Thus we can see in this case as well that $\operatorname{Tr}\log (A) \leq \operatorname{Tr}(A-I)$ . Recall that a complex matrix has a logarithm if and only if it is invertible. When the matrix has no negative real eigenvalues, then there is a unique logarithm that has eigenvalues all lying in the strip $\{z \in \mathbf{C} \mid-\pi<\operatorname{lm} z<\pi\}.$ This logarithm is known as the principal logarithm. My question is: to what extent can we generalise this result? For example, does it hold that for any matrix $A$ with strictly positive eigenvalues we have $\operatorname{Tr}\log (A) \leq \operatorname{Tr}(A-I)$ ? If it helps we can add the requirement that $A$ is a product of two positive definite matrices.","['logarithms', 'trace', 'matrices', 'linear-algebra', 'inequality']"
3788304,Let $p=1+\frac{1}{\sqrt 2}+\cdots\frac{1}{\sqrt {120}}$ and $q=\frac{1}{\sqrt 2}+\frac{1}{\sqrt 3}+\cdots\frac{1}{\sqrt {121}}$ then,"Let $$p=1+\frac{1}{\sqrt 2}+\frac{1}{\sqrt 3}+\cdots\frac{1}{\sqrt {120}}$$ and $$q=\frac{1}{\sqrt 2}+\frac{1}{\sqrt 3}+\frac{1}{\sqrt 4}+\cdots\frac{1}{\sqrt {121}}$$ then which of the following is true: A) $\ p>20$ B) $\ q<20$ C) $\ p+q<40$ D) $\ p+q>40$ The answer is ABD. I was able to obtain A and B, but was unable to obtain D. My Attempt: Consider the function $f(x)=\frac{1}{\sqrt x}$ when $x\in [a,a+1]$ , then we have: $$\frac{1}{\sqrt {a+1}}\leq\frac{1}{\sqrt x}\leq \frac{1}{\sqrt a}$$ Thus, $$\int_a^{a+1}\frac{dx}{\sqrt {a+1}}<\int_a^{a+1}\frac{dx}{\sqrt x}< \int_a^{a+1}\frac{dx}{\sqrt a}$$ Or, $$\frac{1}{\sqrt {a+1}}<\int_a^{a+1}\frac{dx}{\sqrt x}< \frac{1}{\sqrt a}$$ Summing the above inequality from $a=1$ to $a=120$ gives $$q<\int_1^{121} \frac{dx}{\sqrt x}<p$$ That simplifies to $q<20<p$ . But that doesn't help me comment on $p+q$ . Any help would be awesome.","['integration', 'calculus', 'definite-integrals']"
3788305,Analytical formula for all combinations of n items without repetition where order is not important,"I do have n items and would like to get the number of all possible combinations whereby the order can be ignored and repetitions are not allowed. For example, for n = 3 I expect x1, 
x2, 
x3,
x1, x2
x1, x3
x2, x3
x1, x2, x3 so the number should be 7 . I can calculate this number, I think, by summing up the binomial coefficients: $$\sum_{k=1}^{n} \binom{n}{k}$$ . I can calculate this in Python as follows from scipy.special import binom

n = 3
sum(binom(n, k) for k in range(n)) which indeed returns 7 . What I am wondering is whether there is an analytical equation for this. The closest I could find is $$\binom{n + r - 1}{r} = \frac{(n+r-1)!}{r!(n-1)!}$$ , but that allows for repetition.",['combinatorics']
3788309,Basic questions about the sobolev space $H^\infty(\mathbb{R})$,"Let's consider $H^\infty(\mathbb{R})$ to be the intersection of all Sobolev spaces $H^s$ for $s\geq0$ , that is, $$
H^\infty(\mathbb{R}):=\bigcap_{s\geq 0}H^s(\mathbb{R}).
$$ I am wondering some trivial questions about this space, like for example, is this space different from the space of Schwartz functions $\mathcal{S}$ ? Or maybe do we have an inclusion like $$
H^\infty\subset\mathcal{S} \quad \hbox{or} \quad \mathcal{S}\subset H^\infty?
$$ If not, I was wondering if even possible to prove that any function $f\in H^\infty$ belongs to $f\in L^1$ . This last question arises to me because I know that by Sobolev's embedding we have that $f$ belongs to any $L^p$ space for $p\geq 2$ , but what about $p<2$ ? Since we have a ""super"" regularity, I guess this doesn't sound crazy right? Finally, does $f\in H^\infty$ implies (for example) exponential decay?","['examples-counterexamples', 'real-analysis', 'lp-spaces', 'sobolev-spaces', 'weak-derivatives']"
3788377,Is there a differentiable function such that $f(\mathbb Q) \subseteq \mathbb Q$ but $f'(\mathbb Q) \not \subseteq \mathbb Q$?,"Is there a differentiable function $f:\mathbb R \rightarrow \mathbb R$ such that $f(\mathbb Q) \subseteq \mathbb Q$ , but $f'(\mathbb Q) \not \subseteq \mathbb Q$ ? A friend of mine asserted this without giving any examples. I seriously doubt it, but I had hard time trying to disprove it since analysis isn't really my thing. I can't even think of any class of differentiable functions with $f(\mathbb Q) \subseteq \mathbb Q$ other than the rational functions.",['real-analysis']
3788387,Where Does the Hessian Matrix Come from (Why Does it Work)?,"Why does the Hessian matrix $$\left( {\begin{array}{cc}
\frac{\partial^2f}{\partial x^2} & \frac{\partial^2f}{\partial x \partial y} \\
\frac{\partial^2f}{\partial y \partial x} & \frac{\partial^2f}{\partial y^2} \\
\end{array} } \right)$$ work and where does it come from? I just recently came across this in a multivaraible calculus course. It was used to determine whether an extremum of a function with 2 variables is a maximum or minimum or ""saddle point"". Can anyone explain why it pops up here and how it helps understand the properties of an extremum?","['multivariable-calculus', 'linear-algebra', 'hessian-matrix']"
3788402,Weak convergence of product of sequences that are weakly convergent in $H^1$ and weak-* convergent in $L^\infty$,"I'm currently trying to understand a step of a proof in a paper. Suppose that we have two sequences $(n_k), (V_k)\subset H^1(\Omega)\cap L^\infty(\Omega)$ that converge weakly in $H^1(\Omega)$ to some limit elements $n,V\in H^1(\Omega)$ , where $\Omega\subset\mathbb{R}^d$ is a bounded Lipschitz domain $(d\in\{1,2,3\})$ . Further, assume that the sequences are uniformly bounded in $H^1(\Omega)$ and $L^\infty(\Omega)$ as well as weak-* convergent in $L^\infty(\Omega)$ to $n,V$ . The authors now argue that this is more than sufficient to show $$
\lim_{k\to\infty}\int_\Omega n_k\nabla V_k\cdot\nabla \varphi\;\text{d}x=\int_\Omega n\nabla V\cdot\nabla\varphi\;\text{d}x\quad\forall\varphi\in H^1(\Omega)
$$ without giving more details. Here, $\nabla u$ denotes the weak derivative of $u$ . As I'm quite new to the topic, I tried to understand why this holds true. My approach was to consider the following: $$
\int_\Omega (n_k\nabla V_k-n\nabla V)\cdot\nabla\varphi\;\text{d}x=\int_\Omega n_k(\nabla V_k-\nabla V)\cdot\nabla\varphi\;\text{d}x+\int_\Omega (n_k-n)\nabla V\cdot\nabla\varphi\;\text{d}x.
$$ We have that $\nabla V\cdot\nabla\varphi\in L^1(\Omega)$ and therefore the second integral tends to $0$ by the weak-* convergence of $(n_k)$ . However, the first integral is tricky for me. If there wasn't the term $n_k$ , one could see that the integral tends to $0$ because of the weak convergence of $V_k$ in $H^1(\Omega)$ . I could apply the Hoelder inequality to estimate $$
\left\vert\int_\Omega n_k(\nabla V_k-\nabla V)\cdot\nabla\varphi\;\text{d}x\right\vert\leq\Vert n_k\Vert_{L^\infty(\Omega)}\Vert \nabla V_k-\nabla V\Vert_{L^2(\Omega)}\Vert\nabla\varphi\Vert_{L^2(\Omega)}
$$ but I don't know if $(\nabla V_k)$ strongly converges in $L^2(\Omega)$ . Could someone provide me with a hint on how to tackle the problem? Thanks. To give a little more context: I'm given a weakly convergent sequence $(n_k,V_k)$ that is a weak solution of a semilinear elliptic PDE system. I want to show that I can pass to the limit in the weak formulation where the term above appears. For the other terms in the weak formulation it was easy to verify that this is possible as they were linear.","['sobolev-spaces', 'functional-analysis', 'weak-convergence', 'partial-differential-equations']"
3788407,Bartle's Elements of Integration Exercise 6.T,"Let $(X,\mathcal{M},\mu)$ be a  measure space. Bartle defines $$\|f\|_\infty = \inf\{S(N);N\in\mathcal{M}, \mu(N)=0\}$$ where $$S(N) = \sup \{|f(x)|;x\not\in N\}$$ How do I show $|f(x)|\leq \|f\|_\infty$ a.e.? Also, that this is the infimum of real constants with this property, that is, if $A<\|f\|_\infty$ , then $\exists E\in\mathcal{M}$ with $\mu(E)>0$ such that $|f(x)|>A,\,\forall x\in\ E$ . I've actually seen this as the definition of $\|\cdot\|_\infty$ in some other books, but I can't see how to prove they are equivalent. I've tried proving it by contradiction but I seem to get nowhere. Any tips?","['measure-theory', 'lp-spaces', 'measurable-functions']"
3788424,Partial reciprocal sum,"How can I show that $$\sum_{k=1}^{n}\frac{1}{n+k}\leq\frac{3}{4}$$ for every integer $n \geq 1$ ?
I tried induction, estimates with logarithms and trying to bound the sum focusing on the larger terms or things like $\frac{1}{n+1}+\frac{1}{n+2}\leq\frac{2}{n+1}$ but nothing seems to work. Do you have any suggestion? Thanks","['summation', 'sequences-and-series']"
3788516,Nested functions,"Assuming that we have some function $L(x)$ such that $L(x) = x - \frac{x^2}{4}.$ Now, define $a_n$ as $$L \Bigl( L \Bigl( L \Bigl( \cdots L \Bigl( \frac{17}{n} \Bigr) \cdots \Bigr) \Bigr) \Bigr),$$ where we have $n$ iterations of $L.$ My question here is, what value does $n \cdot a_n$ approach as $n$ approaches infinity? I tried to find some sort of pattern but it got nasty fast. I than tried to find a few small values and test them out, but they didn't quite work. How should I approach this problem? Thanks.",['functions']
3788524,Solving a non linear differential equation,"This is a first order differential equation: $$
\frac{df_1}{dx} + \frac{(f_1)^2}{h^2} - \frac{2m}{h^2} \lambda \delta(x-pa)=-\frac{2mE_1}{h^2}
$$ Where, h, $\lambda $ and $E_1$ are constants and and pa lies in [0,a] as 0<p<1 . I have not been taught how to handle differential equations with a Dirac Delta function in it. Moreover, this is a non linear one. I came across this in a research paper and the answer is given but the method to solve it isn't. I have tried learning to use Laplace transform to solve this, but got stuck again because I didn't know how to do Laplace transform of the second term of the equation.
Any help will be appreciated. Please, help me out. The answer is: $f_1=√2mE_1[cot (\frac{√2mE_1}{h} (x-b))]$ Where b is constant of integration P.s.: I know this might be rude but please don't vote this as a homework question because it isn't one. If you can't help just ignore.","['nonlinear-analysis', 'laplace-transform', 'ordinary-differential-equations', 'dirac-delta']"
3788528,Definition of tensor product of rings,"Let $X=\operatorname{Spec} A,Y=\operatorname{Spec}B$ and $Z=\operatorname{Spec}C$ be affine schemes, with $A,B,C$ commutative rings. According to Wikipedia, the following holds: $X \times_Y Z\cong \operatorname{Spec}\left( A\otimes_B C \right)$ . Question: What is the tensor products of rings? Do we view $A$ and $C$ as $B$ -algebras in some kind of way?","['algebraic-geometry', 'ring-theory', 'abstract-algebra', 'tensor-products', 'schemes']"
3788540,Euler-Mascheroni constant as local minima of a function [closed],Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 years ago . Improve this question Let define the function $x>0$ : $$f(x)=\Gamma\Big(\frac{1}{x}\Big)-x$$ Then $x_0$ such that : $$f(x_0)=-\gamma$$ Define a local minima ( see here ) of the function $f(x)$ .(We have the Gamma function and the negative Euler-Mascheroni constant) To prove it I have tried derivative we have $x>0$ : $$f'(x)=-\frac{\Gamma\Big(\frac{1}{x}\Big)\psi\Big(\frac{1}{x}\Big)}{x^2}-1$$ But we want : $$f'(x)=0$$ Or : $$\Gamma\Big(\frac{1}{x}\Big)\psi\Big(\frac{1}{x}\Big)=x^2$$ And now the best things I can do is to use Newton's method to check my result . Question Is there an hidden closed form here ? How to solve it ? Thanks in advance .,"['digamma-function', 'gamma-function', 'numerical-methods', 'constants', 'derivatives']"
3788598,"If $H$ is a subgroup of a finite abelian group $G$, then $G$ has a subgroup that is isomorphic to $G/H$.","I know Is every quotient of a finite abelian group $G$ isomorphic to some subgroup of $G$? has two answers. I don't understand how the first answer works and I have doubt about that answer. The second answer uses character theory which I don't intend to use since I am practicing for my qualifying exam. This problem showed up in our last exam. None of my classmates can solve this problem and the professors in our department just provided us with some hints saying look the homomorphism and do induction which helped a little but didn't lead me to an answer. Now I list the two approaches I have tried and the reasons why they don't work. The first approach I tried is the following: Suppose $G$ is finite abelian. Let $|G|=p_1^{\alpha_1}p_2^{\alpha_2}\cdots p_k^{\alpha_k}$ and $|H|=p_1^{\beta_1}p_2^{\beta_2}\cdots p_k^{\beta_k}$ . By the elementary divisor decomposition, we have $G\cong G_1\times G_2\times\cdots\times G_k$ and $H\cong H_1\times H_2\times\cdots\times H_k$ where $G_i$ is a Sylow subgroup of $G$ and $H_i$ is a Sylow subgroup of $H$ for all $i=1,2,...,k$ . Hence $G/H\cong(G_1/H_1)\times(G_2/H_2)\times\cdots\times(G_k/H_k)$ . So it suffices to show the result when $G$ is a $p$ -group. Suppose $|G|=p^n$ . Now by the invariant factor decomposition, $G\cong\mathbb{Z}_{p^{a_1}}\times\mathbb{Z}_{p^{a_2}}\times\cdots\times\mathbb{Z}_{p^{a_t}}$ with $a_1\geq a_2\geq\cdots\geq a_t$ and $a_1+a_2+\cdots+a_t=n$ . For $i=1,...,t$ , let $e_i=(...,0,1,0,...)$ . Then $G/H=\left<e_1H,...,e_tH\right>$ . WLOG, let $\{e_1H,e_2H,...,e_sH\}$ , $s\leq t$ , be a smallest set of generators for $G/H$ . Claim: $G/H\cong\left<e_1H\right>\times\cdots\times\left<e_sH\right>$ . So $G/H\cong\mathbb{Z}_{p^{b_1}}\times\cdots\times\mathbb{Z}_{p^{b_s}}$ with $b_i\leq a_i$ for all $i=1,...,s$ . (This is due to the canonical projection $G\mapsto G/H$ and thus $|e_iH|\mid |e_i|$ for all $i=1,...,s$ .) Therefore, $G/H$ is isomorphic to a subgroup of $G$ . This looks promising, but the claim is false. A counterexample is this: Let $G=\mathbb{Z}_4\times\mathbb{Z}_4$ . Then $G/\left<(2,2)\right>\cong\mathbb{Z}_4\times\mathbb{Z}_2$ , but $(0,1)+\left<(2,2)\right>$ and $(1,0)+\left<(2,2)\right>$ both have order 4. The second aprroach I tried is the following: Suppose $G$ is finite abelian and $H\leq G$ . Let $|G|=p_1^{\alpha_1}p_2^{\alpha_2}\cdots p_k^{\alpha_k}$ and $|H|=p_1^{\beta_1}p_2^{\beta_2}\cdots p_k^{\beta_k}$ where $p_1,...,p_k$ are distinct primes. By the elementary divisor decomposition, we have $G\cong G_1\times G_2\times\cdots\times G_k$ and $H\cong H_1\times H_2\times\cdots\times H_k$ where $G_i$ is a Sylow $p_i$ -subgroup of $G$ and $H_i$ is a Sylow $p_i$ -subgroup of $H$ for all $i=1,2,...,k$ . Since $H_i\unlhd G_i$ for all $i=1,...,k$ , $G/H\cong(G_1/H_1)\times(G_2/H_2)\times\cdots\times(G_k/H_k)$ . So it suffices to show the result when $G$ is an abelian $p$ -group. We proceed by induction. If $|G|=p$ , then $H=1$ or $G$ , so $G/H\cong1$ or $G$ . Suppose the result holds for all abelian $p$ -groups of order less than $|G|$ . Now by the fundamental theorem of finite abelian groups, $G=\mathbb{Z}_{p^{\alpha_1}}\times\cdots\times\mathbb{Z}_{p^{\alpha_n}}=\left<x_1\right>\times\cdots\times\left<x_n\right>$ . Consider $\varphi:G\to G$ such that $x\mapsto x^p$ . Since $G$ is abelian, $\varphi$ is a group homomorphism with $\ker\varphi=\left<x_1^{p^{\alpha_1-1}}\right>\times\cdots\left<x_n^{p^{\alpha_n-1}}\right>$ . By Cauchy's theorem, $H':=\ker\varphi\cap H\neq1$ and it is elementary abelian. So WLOG, $H'\cong\left<x_1^{p^{\alpha_1-1}}\right>\times\cdots\left<x_m^{p^{\alpha_m-1}}\right>$ where $m\leq n$ . It follows that $G/H'$ is isomorphic to a subgroup of $G$ . By the third isomorphism theorem, we have $G/H\cong(G/H')/(H/H')$ . Since $|G/H'|<|G|$ , by the induction hypothesis, $(G/H')/(H/H')$ is isomorphic to a subgroup of $G/H'$ and thus it is isomorphic to a subgroup of $G$ . Therefore, $G/H$ is isomorphic to a subgroup of $G$ . Now in this proof I assume that if $B$ and $C$ are isomorphic subgroups of a finite abelian group $A$ , then $A/B\cong A/C$ which is not true. A counterexample is $A=\mathbb{Z}_4\times\mathbb{Z}_2$ , $B=\left<(2,0)\right>$ and $C=\left<(0,1)\right>$ . Here $A/B\cong\mathbb{Z}_2\times\mathbb{Z}_2\not\cong\mathbb{Z}_4\cong A/C$ . Now my question is does anyone know how to fix either of the problems in my two attempts above to make it work? Or have a better explanation for the first answer in the post Is every quotient of a finite abelian group $G$ isomorphic to some subgroup of $G$? ? This problem have haunted me for months. I would really appreciate the help.","['direct-product', 'group-isomorphism', 'quotient-group', 'group-theory', 'abelian-groups']"
3788612,Find limit of $f(x)$ as $x$ tends to $0$,I need some help answering this question: $$f(x) = \frac{\cosh(x)}{\sinh(x)} - \frac{1}{x}$$ find the limit of $f(x)$ as $x$ tends to $0$ by writing $f(x)$ as a quotient of two powers series. I have so far: $$\frac{(x(1+\frac{x^2}{2!}+\frac{x^4}{4!}+\cdots))-(x + \frac{x^3}{3!} + \cdots)}{x(x + \frac{x^3}{3!}+\cdots)}= \frac{(x+\frac{x^3}{2!}+\frac{x^5}{4!}+\cdots))-(x + \frac{x^3}{3!} + \cdots)}{(x^2 + \frac{x^4}{3!}+\cdots)}$$ but I don't know how to reduce this further.,"['power-series', 'limits', 'trigonometry', 'sequences-and-series']"
3788621,An interesting limit: $\lim_\limits{n\to\infty}\frac{\sin 1\sin\sqrt{1}+\sin 2\sin\sqrt{2}+\sin 3\sin\sqrt{3}+\cdots+\sin n\sin\sqrt{n}}{n}$,"I would like to prove that $$\lim_\limits{n\to\infty}\frac{\sin 1\sin\sqrt{1}+\sin 2\sin\sqrt{2}+\sin 3\sin\sqrt{3}+\cdots+\sin n\sin\sqrt{n}}{n}=0$$ but I am stuck. I tried to solve it by using Euler-Maclaurin formula , but I could not to. Euler-Maclaurin formula applied to the function $f(x)=\sin x \sin\sqrt{x}\;\;$ is the following: $$\sum_{h=1}^n\sin h\sin\sqrt{h}=\int_\limits{0}^n\left[\sin x\sin\sqrt{x}+\left(x-\lfloor x\rfloor\right)\left(\cos x\sin\sqrt{x}+\frac{\sin x\cos\sqrt{x}}{2\sqrt{x}}\right)\right] \, dx$$ but I could not manage to prove that $$\frac{1}{n}\int_\limits{0}^n\left(x-\lfloor x\rfloor\right)\left(\cos x \sin\sqrt{x} \right) \, dx\rightarrow 0 \text{ as } n\to\infty.$$ Moreover I tried to write the limit as a limit of a Riemann sum, but I did not manage to. Furthermore I tried to prove the following inequality: $$\left|\sin 1\sin\sqrt{1}+\sin 2\sin\sqrt{2}+\cdots+\sin n \sin\sqrt{n} \right|\le\sqrt[4]{n^3}\\\text{for all }\;n\in\mathbb{N},$$ but it was not successful. I managed to prove that $$\lim_{n\to\infty}\frac{\sin 1+\sin 2 +\sin 3+\ldots+\sin n}{n}=0$$ and $$\lim_{n\to\infty}\frac{\sin\sqrt{1}+\sin\sqrt{2}+\sin\sqrt{3}+\cdots+\sin\sqrt{n}}{n}=0.$$ Is it possible to use these last two limits in order to prove that $$\lim_{n\to\infty}\frac{\sin 1\sin\sqrt{1}+\sin 2\sin\sqrt{2}+\sin 3 \sin\sqrt{3}+\cdots+\sin n\sin\sqrt{n}}{n}=0\text{ ?}$$ I tried to use Cauchy-Schwartz inequality, but I got $$\lim_{n\to\infty}\frac{\sin^21+\sin^22+\cdots+\sin^2n}{n}$$ and $$\lim_{n\to\infty}\frac{\sin^2\sqrt{1}+\sin^2\sqrt{2}+\cdots+\sin^2\sqrt{n}}{n}$$ and these last two limits are not zero in fact there are both $\frac{1}{2}$ .","['limits', 'sequences-and-series', 'real-analysis']"
3788654,Reconstructing sets of sets from reduced membership information,"Let $[n] = \{1,2,\dots,n\}$ and $\mathcal{P}(n)$ be the power set of $[n]$ . Let $\sigma \subseteq \mathcal{P}(n)$ be a set of subsets of $[n]$ . For each $\sigma$ one can define the square matrix $\{a_{ik}\}_{i,k =1,\dots,n}$ with $$a_{ik} = \Big|\big\{s \in \sigma\ \big|\ i \in s \wedge  |s| = k \big\}\Big|,$$ i.e. the number of $k$ -element sets in $\sigma$ that contain $i$ . For fixed $i$ , the vector $[a_{i1},\dots,a_{in}]$ gives something like an inverted degree sequence: the element (or node) $i$ is contained in $a_{i2}$ pairs, in $a_{i3}$ triples, in $a_{i4}$ quadruples, and so on. It's obvious that $a_{ik} \leq \binom{n-1}{k-1}$ . And I'm quite – but not absolutely – sure that $\sum_i a_{ik}  = 0 \text{ mod } k$ (a generalization of the handshaking lemma ). For lack of another name , let me call this matrix the stats matrix of $\sigma$ . [Side question: Does the so defined matrix have an official name?] Now let a square matrix $\{a_{ik}\}_{i,k =1,\dots,n}$ with $a_{ik} \leq \binom{n-1}{k-1}$ and $\sum_i a_{ik}  = 0 \text{ mod } n$ for each $k$ be given. My question is fourfold: How can it be checked if there is a $\sigma \subseteq \mathcal{P}(n)$ with stats matrix $\{a_{ik}\}$ ? What are necessary and/or sufficient conditions? How can the number of $\sigma$ with stats matrix $\{a_{ik}\}$ be determined? Or maybe the fraction of stats matrices (among all as defined in the premise) in the limit $n \rightarrow \infty$ ? How can one (random) $\sigma$ with stats matrix $\{a_{ik}\}$ effectively be constructed? How can all $\sigma$ with stats matrix $\{a_{ik}\}$ effectively be constructed (to perform some statistics on this set)? The column $k=2$ reminds me of the configuration model for random graphs, maybe this can help to answer question 3 for general $k$ (which to be honest interests me most).","['elementary-set-theory', 'graph-theory', 'random-graphs', 'combinatorics']"
3788665,"If $f(x)$ is differentiable for all real numbers, then what is the value of $\frac{a+b+c}{2}$?","If $f(x)=\begin {cases} a^2 + e^x & -\infty <x<0 \\ x+2 & 0\le x \le 3 \\ c -\frac{b^2}{x} & 3<x<\infty \end{cases}$ , where $a,b,c$ are positive quantities. If $f(x)$ is differentiable for all real numbers, then value of $\frac{a+b+c}{2}$ is Left hand derivative at $x=0$ $$Lf’(0) =\lim_{h\to 0} \frac{2 - (a^2 +e^{-h})}{h}$$ For limit to exist, $2-a^2=0 \implies a=\pm \sqrt 2$ $$L f’(0)=1$$ Right hand derivative at $x=0$ $$R f’(0) =\lim_{h\to 0} \frac{ h+2 -2}{h} =1$$ Left hand derivative at $x=3$ $$Lf’(3) =\lim_{h\to 0} \frac{5- (3-h+2)}{h}=1$$ And $$Rf’(3) =\lim_{h\to 0} \frac{ c -\frac{b^2}{3+h}-5}{h}$$ For limit to exist, $c=h$ $$Rf’(3) =\lim_{h\to 0} \frac{b^2}{(3+h)(h)}=\infty$$ Where am I going wrong?","['limits', 'calculus', 'derivatives']"
3788668,"The minimum value of $P(A \cup B )$, if $P(\bar B)={ \{P(A \cup B)}\}^2$","What I tried: $$1-P(B)=\{P(A)+P(B)-P(A \cap B)\}^2$$ Now since $0\le P(B) \le1 \\
$ : $\therefore 1-P(B) \in\  [0,1]\\$ Also, the R.H.S is also the square value of a probability hence it is also in $[0,1]$ . Next, I decided to check the options given if any of them has the range $[0,1]$ and apparently there were two of them. Looking at the options I got an intuition that the answer probably is the root of a quadratic equation but I am unable to form that quadratic equation. ( I will attach the image for reference ) I also tried using Venn diagram and had an idea that $1=\text{Probability of the sample space}$ and then I could write some expression for $P(\bar{B})$ by equating it with $P(A)-P(A\cap B)\  \mathbf{\text{along with something more}}$ that something more is what I am unable to find out. If I get that then I think I can let some terms as $x$ and form and equation. I also made an approach using conditional probability but that didn't give any progress. OR Maybe I am wrong all along. This is the Question: I know B can't be the answer A and C hint at it being a quadratic function but if I don't get an absolute value then, option D could also be correct.","['elementary-set-theory', 'conditional-probability', 'probability']"
3788694,Does the existence of a minimal cover for a subset of reals need some form of choice?,"In Kanamori's book, ""The Higher Infinite"" p. 376, he defines a minimal cover of some $A \subseteq \omega^\omega$ , to be any $B \subseteq \omega^\omega$ , such that $A\subseteq B$ and that $B$ is Lebesgue measurable and if $Z \subseteq B-A$ is Lebesgue measurable, then $m_L(Z) = 0$ . And he claims that picking some $B$ with $A\subseteq B$ and $m_L(B)$ minimal, does the job. [Here $m_L$ denotes the Lebesgue measure.] Now here is my problem. The whole premise of this chapter is that we don't want to use choice to do these things. But any way I try to construct such a $B$ , I inevitably use some form of choice. The best I can do is $\mathsf{AC}_\omega(\omega^\omega)$ . Is there some choice-free way to do this? A sketch of a proof with $\mathsf{AC}_\omega(\omega^\omega)$ : Let $x = \inf\{m_L(B): A\subseteq B \text{ and } B \text{ is Lebesgue measurable}\}$ . By $\mathsf{AC}_\omega(\omega^\omega)$ , let $\langle B_n: n<\omega\rangle$ be a sequence such that $A\subseteq B_n$ and $m_L(B_n) \rightarrow x$ as $n \rightarrow \infty$ . Now $B = \bigcap_n B_n$ is the desired minimal cover. $\square$","['axiom-of-choice', 'measure-theory', 'set-theory', 'real-analysis']"
3788750,Order of the group of commuting elements in a finite group,"Let $G$ be a finite group of order $n$ and class number $k.$ Show that $$
\left|\left\{(a,b)\in G^2: ab=ba\right\}\right|=nk.
$$ I considered $G$ acting on itself by conjugation, and then applied Burnside's lemma to give me $$ nk = \sum_{a\in G}o(Z(a)).$$ This is where I'm stuck. I just need to show that there are $k$ centralizers of $a \in G$ of order $n$ . Any help would be appreciated.","['group-theory', 'group-actions', 'finite-groups']"
3788761,Linear approximation of $\cos\big(\frac{\pi}{5}+0.07\big)-\cos\big(\frac{\pi}{5}\big)$,"Find the approximate value of $$\cos\bigg(\frac{\pi}{5}+0.07\bigg)-\cos\bigg(\frac{\pi}{5}\bigg)$$ using linear approximation. My attempt: The tangent line approximation of $f(x)=\cos(x)$ at $\displaystyle x=\frac{\pi}{5}$ is $$f(x)\approx f\bigg(\frac{\pi}{5}\bigg)+\bigg(x-\frac{\pi}{5}\bigg)f'\bigg(\frac{\pi}{5}\bigg).$$ Putting $x=\frac{\pi}{5}+0.07$ , we get $$f(x)-f\bigg(\frac{\pi}{5}\bigg)\approx -\sin\bigg(\frac{\pi}{5}\bigg)(0.07)=0.0411$$ Is my solution is right? If not, then how do I solve it?","['trigonometry', 'derivatives', 'approximation', 'tangent-line']"
3788781,What groups have a homomorphic image $\Bbb{Z}/2\Bbb{Z}$?,"Is there a simple characterization for all the groups $G$ so that there exists an epimorphism $\varphi:G\to\Bbb{Z}/2\Bbb{Z}$ ? First assume there exists a nontrivial homomorphism $\varphi:G\to\Bbb{Z}/2\Bbb{Z}$ . Denote $K=\operatorname{ker}(\varphi)$ and $H=\varphi^{-1}(1)=G\setminus K$ . I understand that any such group must have the following properties: $G/K\cong\Bbb{Z}/2\Bbb{Z}$ . If $a,b\in H$ and $x\in K$ , then $ab\in K$ (since $\varphi(ab)=\varphi(a)+\varphi(b)=1+1=0$ ) and similarly $ax\in H$ . Is there a more complete characterization of such groups? Edit: I extend my previuos question and ask the following: Is there a characterization of all groups $G$ such that there exists a nontrivial homomorphism $\varphi:G\to\Bbb{Z}/n\Bbb{Z}$ for a given $n\in \Bbb{N}$ ?","['group-theory', 'group-isomorphism']"
3788787,Continuity concerning family of projections,"Let $X$ be a compact topological space, $H$ be a complex Hilbert space and endow $F(H)$ , the space of bounded Fredholm operators in $H$ , with the uniform norm topology (inherited from $B(H)$ ). Let $T: X\to F(H)$ , $x\mapsto T_x$ , be a continuous map.
There exists a closed subspace $V\subseteq H$ of finite codimension, i.e. $\dim H/V<\infty$ , such that $V\cap \ker T_x = \{0\}$ for all $x\in X$ . I have proved that $H/T(V) = \bigsqcup\limits_{x\in X} H/T_x(V)$ is a vector bundle over $X$ (of finite rank).
In particular, $\dim H/T_x(V)$ is independent of $x$ (here we can assume connectedness of $x$ ). For $x\in X$ , let $P_x: H\to H$ be the orthogonal projection onto $T_x(V)$ .
In order to induce a specific map of bundles (see here for details), I need to check the continuity of the map $X\times H\to H$ given by $(x,u)\mapsto P_x(u)$ . Question: Is $(x,u)\mapsto P_x(u)$ continuous? Looking at the inequality $$ \|P_y(v)-P_x(u)\| \leq \|P_y(v-u)\| + \|(P_y-P_x)(u)\|$$ we conclude it suffices to prove that $x\mapsto P_x$ is continuous when one gives $B(H)$ the strong operator topology, but I could not prove it. Any help is appreciated. Thanks in advance!","['hilbert-spaces', 'continuity', 'operator-theory', 'functional-analysis']"
3788788,Convergence of $\max$ and tail events,"Let $(X_n)_n$ be a sequence of real random, and consider the tail $\sigma$ -algebra generated by $(X_n)_n: \mathcal{F}=\bigcap\limits_{n \in \mathbb{N}}\sigma\left(\bigcup_\limits{k \geq n}\sigma(X_k)\right).$ Prove or disprove the following statement: $1) \left\{w;\max_{1 \leq k \leq n} X_k(w) \text{ converges }\right\} \in \mathcal{F},$ $2)\left\{w;\dfrac{\max_{1 \leq k \leq n} X_k(w)}{n} \text{ converges }\right\}\in \mathcal{F}.$ So, since $\mathbb{R}$ is complete, and taking $r \in \mathbb{N},$ $$\left\{w;\max_{1 \leq k \leq n} X_k(w) \text{ converges }\right\}=\bigcap_{p \in \mathbb{N}}\bigcup_{n_0 \in \mathbb{N}}\bigcap_{n \geq n_0}\bigcap_{q \in \mathbb{N}}\left\{w;|\max_{1 \leq k \leq n+r+q} X_k(w)-\max_{1\leq k \leq n+r}X_k(w)|\leq\frac{1}{p}\right\}$$ $$=\bigcap_{p \in \mathbb{N}}\bigcup_{n_0 \in \mathbb{N}}\bigcap_{n \geq n_0}\bigcap_{q \in \mathbb{N}}\left\{w;\max(\max_{n+r+1 \leq k \leq n+r+q} X_k(w)-\max_{1\leq k \leq n+r}X_k(w),0)\leq\frac{1}{p}\right\}$$ Any ideas how to continue?","['measure-theory', 'convergence-divergence', 'probability-theory']"
3788790,Relationship between Product of $L^p$ spaces an product measures,"Let $(X,\Sigma,\mu)$ and $(Y,\Gamma,\nu)$ be $\sigma$ -finite measure spaces.  Is there a relationship between $L^p_{\mu}(\Sigma)\times L^p_{\nu}(\Gamma)$ , equipped with the product of their $L^p$ -topologies, and the space $L^p_{\mu\otimes \nu}(\Sigma \otimes \Gamma)$ ? More generally, let $(X_i,\Sigma_i,\mu_i)$ be a countably infinite familly of $\sigma$ -finite measure spaces, then is $$
\prod_{i=1}^{\infty} L_{\mu}^p(\Sigma_i) \mbox{ dense in } L^p_{\bigotimes_{i=1}^{\infty} \mu_i}\left(\bigotimes_{i=1}^{\infty}\Sigma_i\right)?
$$ Ideas and Update: Initially, my intuition was (as noted in the comment below) to pass through the completed injective tensor product $L_{\mu}^p(\Sigma)\otimes_{\epsilon} L^p_{\nu}(\Gamma)$ . Though now I doubt that works since according to this post and some results of Grothendieck, if $p=1$ then $L_{\mu}^p(\Sigma)\otimes_{\epsilon} L^p_{\nu}(\Gamma)\cong L_{\mu}^p(X;L_{\nu}^p(\Gamma))$ (the Pettis-Gel'fand integrable functions).","['integration', 'measure-theory', 'functional-analysis', 'general-topology', 'fubini-tonelli-theorems']"
3788796,Proved $|B|=2+\frac{C_{2p}^{p}-2}{p}$,"Let $p$ be an odd prime number and $\mathbb{Z}_{2p}=\left\{0,1,2,\ldots,2p-1\right\}$ .
Let $$B=\left\{A\subset\mathbb{Z}_{2p}: 
  |A|=p\;\; \wedge \;\;\displaystyle\sum_{k\in A}{}k=0\pmod p\right\}$$ Prove that: $\quad |B|=2+\frac{C_{2p}^{p}-2}{p}$ This question was suggested to me by my friend after trying to solve it, but I did not reach any results after several attempts. Is there any idea or suggestion for how to solve this problem? Thank you in advance..","['calculus', 'algebra-precalculus', 'prime-numbers', 'real-analysis']"
3788811,confusion regarding treatment of $dx$ in a physics problem,"Consider a fixed, positive Point charge $q1$ , kept at the origin. Another (positive) charge, $q2$ , is being brought from $\infty$ to the point $(r,0)$ , by an external agent slowly . We wish to calculate the work done by the external agent (and thus derive the ""potential"" of the point charge $q1$ , being defined as $w_{ext}/q2$ (or as $-w_{electric}/q2$ )). Suppose we consider a position $(x,0)$ : The magnitude of force is going to be $kq1q2/x^2$ . We will thus have, $\vec{F}=\dfrac{-kq1q2}{x^2}\hat{i}$ . When we displace it from a position $(x,0)$ to $(x-dx,0)$ , the displacement vector $(\vec{ds})$ will be $(x-dx)\hat{i}-x\hat(i)=-dx\hat{i}$ . Using $dw$ = $\vec{F}.\vec{ds}$ , we will get $dw=\dfrac{kq1q2}{x^2}dx$ . Upon integrating from $\infty$ (initial position) to $r$ (final position), we get : $$w_{ext}=-\dfrac{kq1q2}{r}$$ and thus $$V(r)= w_{ext}/q2 =-\dfrac{kq1}{r}$$ which is completely absurd. I tried to be as rigorous as possible with the definitions, vectors etc and yet a -ve sign has crept in somewhere. The only issue seems to be with the treatment of $dx$ . Although , I took $dx$ to be the magnitude of displacement, and accounted for the direction by using $-\hat{i}$ .A possible argument seems to be "" $x$ decreases, so $dx$ is a negative quantity. So the ""magnitude"" should be $-dx$ . My two concerns: What is then, the issue with displacement= $\vec{r_{final}} - \vec{r_{initial}}$ that simply yields $-dx\hat{i}$ ? Simply ""putting"" a - sign before $dx$ after claiming "" $dx$ is negative"" seems to be arbitrary. There should be an argument (like I presented in the previous bullet point) that will produce the - sign for the magnitude, and thus making the vector $(-dx)(-\hat{i})$ . The main essence of this problem seems to be rigorously defining what $dx$ actually represents, for a quantity $x$ . I believe the entire thing can be summarized by one question: What is wrong in writing displacement= $\vec{r_{final}} - \vec{r_{initial}}$ that simply yields $-dx\hat{i}$ ? If I had $(x+dx)$ instead of $(x-dx)$ ,then the derivation would be correct. But why is this the Case?","['physics', 'derivatives', 'convention']"
3788826,The role of Injectivity and Surjectivity on Equivalence Classes,"This is a problem from Proofs and Fundamentals by Ethan D. Bloch that I’m struggling to solve: Let $f:A \to B$ a map. Define a relation $\sim$ on $A$ by letting $x \sim y$ iff and only if $f(x) = f(y)$ , for all $x, y \in A$ . What can be said about the equivalence classes of $\sim$ , depending upon whether $f$ is injective but not surjective, surjective but not injective, neither or both? So far, I realize that if $f$ is injective, then all the equivalence classes of $\sim$ will have exactly one element (and this comes also from the fact that $f$ is a map). Although, I don’t think that the fact that $f$ is surjective or not will alter the equivalence classes. Can someone please help me understanding what is the “relation” (if any) between surjectivity and equivalence classes? Thank you in advance for your attention!","['equivalence-relations', 'relations', 'functions', 'elementary-set-theory', 'problem-solving']"
3788839,calculate: $\int_{-\infty}^{\infty}\frac{\cos\frac{\pi}{2}x}{1-x^{2}}dx$ using complex analysis ; detect my mistake,"calculate: $\int_{-\infty}^{\infty}\frac{\cos\frac{\pi}{2}x}{1-x^{2}}dx$ using complex analysis.
My try: $\int_{-\infty}^{\infty}\frac{\cos\frac{\pi}{2}x}{1-x^{2}}dx$ symetric therefore : $ \int_{-\infty}^{\infty}\frac{\cos\frac{\pi}{2}x}{1-x^{2}}dx=2\int_{0}^{\infty}\frac{\cos\frac{\pi}{2}x}{1-x^{2}}dx$ calculate instead: $2Re\int_{0}^{\infty}\frac{e^{\frac{\pi}{2}zi}}{1-e^{\pi zi}}dz$ use pizza slice: $2Re\int_{0}^{\infty}\frac{e^{\frac{\pi}{2}zi}}{1-e^{\pi zi}}dz=\int_{0}^{2\pi}\frac{e^{\frac{\pi}{2}\theta i}}{1-e^{\pi\theta i}R^{2}}d\theta+\int_{0}^{R}\frac{e^{\frac{\pi}{2}\theta i}}{1-e^{\pi\theta i}R^{2}}dR+\int_{0}^{R}\frac{e^{\frac{\pi}{2}\theta i}}{1-e^{\pi\theta i}R^{2}}dR$ take limits: $2Re\int_{0}^{\infty}\frac{e^{\frac{\pi}{2}zi}}{1-e^{\pi zi}}dz=Lim_{R\rightarrow\infty}\int_{0}^{2\pi}\frac{e^{\frac{\pi}{2}\theta i}}{1-e^{\pi\theta i}R^{2}}d\theta+Lim_{\theta\searrow0}\int_{0}^{R}\frac{e^{\frac{\pi}{2}\theta i}}{1-e^{\pi\theta i}R^{2}}dR+Lim_{\theta\nearrow0}\int_{0}^{R}\frac{e^{\frac{\pi}{2}\theta i}}{1-e^{\pi\theta i}R^{2}}dR$ $2Re\int_{0}^{\infty}\frac{e^{\frac{\pi}{2}zi}}{1-e^{\pi zi}}dz=0+\int_{0}^{R}\frac{1}{1-e^{\pi\theta i}R^{2}}dR+\int_{0}^{R}\frac{1}{1-e^{\pi\theta i}R^{2}}dR$ According the residue theorem at $ \int_{0}^{\infty}\frac{e^{\frac{\pi}{2}zi}}{1-e^{\pi zi}}dz=2\pi iRes_{z=-1}\frac{e^{\frac{\pi}{2}zi}}{1-e^{\pi zi}}=0
$ therefore: $2Re\int_{0}^{\infty}\frac{e^{\frac{\pi}{2}zi}}{1-e^{\pi zi}}dz=0$","['integration', 'complex-analysis', 'contour-integration', 'residue-calculus']"
3788844,Proof Verification: Baby Rudin Ch. 6. Ex. 7a,"I want to follow up on my previous question . Based on the comments and responses I got for my previous question, I developed a new proof for Baby Rudin Ch. 6. Ex. 7a. The exercise is: Suppose $f$ is a real function on $(0, 1]$ and $f \in \mathscr{R}$ on $[c, 1]$ for every $c>0$ . Define \begin{equation}\tag{7.0}
    \int_0^1 f(x) \, dx = \lim_{c \to 0} \int_{c}^1 f(x) \, dx
\end{equation} if this limit exists (and is finite). If $f \in \mathscr{R}$ on $[0, 1]$ , show that this definition of the integral agrees with the old one. My latest attempt: Firstly, we state a modified version of Theorem 6.12 (c) in the text; call it Theorem 6.12 (c $^*$ ). Theorem 6.12 (c) states that: If $f \in \mathscr{R}(\alpha)$ on $[a, b]$ and if $a<c<b$ , then $f \in \mathscr{R}(\alpha)$ on $[a, c]$ and on $[c, b]$ and \begin{equation*}
            \int_a^c f \, d\alpha + \int_c^b f \, d\alpha = \int_a^b f \, d\alpha
\end{equation*} Now, if we allow for the possibility of $c=b$ in Theorem 6.12 (c), we obtain  Theorem 6.12 (c $^*$ ). To wit, take $c=b$ in Theorem 6.12 (c). Then, If $f \in \mathscr{R}(\alpha)$ on $[a, b]$ and if $a<c\le b$ , then $f \in \mathscr{R}(\alpha)$ on $[a, c]$ and on $[c, b]$ and \begin{equation*}
        \int_a^c f \, d\alpha + \int_c^b f \, d\alpha = \int_a^b f \, d\alpha + \int_b^b f \, d\alpha = \int_a^b f \, d\alpha
    \end{equation*} In other words, we can safely replace $a<c<b$ with $a<c\le b$ in Th. 6.12. Now, we prove the original question.
Suppose $f$ is a real function and is Riemann integrable on $[0, 1]$ in the sense of the definition of Riemann integrals given on Pg. 121 in Rudin. Let $0<c\le1$ and that the limit on the right-hand side of (7.0) exists. By Theorem 6.12 (c $^*$ ), $f$ is Riemann integrable on $[c, 1]$ . We want to show that \begin{equation}
\lim_{c \to 0} \int_{c}^1 f(x) \, dx = \int_0^1 f(x) \, dx = \lim_{c \to 0} \int_{0}^1 f(x) \, dx 
\end{equation} Let $\epsilon> 0$ be given. It suffices to that there exists a $\delta>0$ such that \begin{equation}\tag{7.1}
    0< |c-0| < \delta \implies \left|\int_{c}^1 f(x) \, dx- \int_{0}^1 f(x) \, dx \right| < \epsilon
\end{equation} By Theorem 6.12 (c $^*$ ), we know that $f$ is Riemann integrable on $[0, c]$ and \begin{equation}\tag{7.2}
    \int_0^c f(x) \, dx =  \int_0^1 f(x) \, dx - \int_c^1 f(x) \, dx
\end{equation} Since $f$ is Riemann integrable on $[0, c]$ , it must also be bounded on $[0, c]$ , for if $f$ is not bounded on $[0, c]$ , then it cannot be Riemann integrable on $[0, c]$ . Put $M = \sup\limits_{0 \, \le x \, \le c} |f(x)|$ . Clearly, $M \ge 0$ . If $M =0$ , then $f(x) = 0 \, \forall x$ , which implies \begin{equation}
     \int_0^1 f(x) \, dx =0= \int_{c}^1 f(x) \, dx 
 \end{equation} which means that the implication in (7.1) will hold for any $\delta$ . So, assume WLOG that $0 < \epsilon\le M$ . By Theorem 6.12 (d),we have \begin{equation*}
    \left| \int_0^c f(x) \, dx\right| \le M(c-0) = Mc \stackrel{(7.2)}{\implies} \left| \int_0^1 f(x) \, dx - \int_c^1 f(x) \, dx\right| \le Mc
\end{equation*} Put $\delta = \dfrac{\epsilon}{M}$ . Notice that $c\in(0, 1]$ and $\dfrac{\epsilon}{M}\in(0, 1]$ . If $c=\dfrac{\epsilon}{M}, Mc=\epsilon$ and \begin{equation*}
    \left| \int_0^1 f(x) \, dx - \int_c^1 f(x) \, dx\right| \le Mc = \epsilon
\end{equation*} If $c<\dfrac{\epsilon}{M}$ , we have \begin{equation*}
    \left| \int_0^1 f(x) \, dx - \int_c^1 f(x) \, dx\right| \le Mc < M \cdot \frac{\epsilon}{M} = \epsilon
\end{equation*} If $c>\dfrac{\epsilon}{M}, \dots$ My question : Is Theorem 6.12(c $^*$ ) actually correct? If my proof correct, or is there something in the proof that can be improved? Can someone please suggest how the proof can be completed? Thanks!","['integration', 'proof-writing', 'analysis', 'real-analysis', 'solution-verification']"
3788847,Find (with a proof ) lim$_{n \rightarrow \infty} \int_0^n \frac{(1 -\frac{x}{n})^n}{ne^{-x}}dx$,"So I am pretty sure I want to start off with defining $f_n = \frac{(1 -\frac{x}{n})^n}{ne^{-x}}$ if $x >0$ , and then use something like the bounded convergence theorem or dominated convergence theorem.","['integration', 'functions', 'functional-analysis', 'real-analysis']"
3788853,Inscribed Circles in a Quadrilateral,"I found this problem online, where it was asked to prove EF = GH. I was able to prove that, but got intrigued by how the four smaller inscribed circles could be constructed in the first place. That is, given the fact that AD + BC = AB + CD (i.e. an inscribed circle can be constructed for ABCD), how can we contruct EF and GH, such that for each smaller quadrilateral, an incribed circle can be constructed? Also, given a fixed quadrilateral with AD + BC = AB + CD, are the positions of EF and GH unique? Any hints would be appreciated.","['quadrilateral', 'circles', 'geometry', 'geometric-construction']"
3788889,Struggling on how to prove an inequality using the Mean Value Theorem,"Prove the following using the mean value theorem: $ln(1+x)$ $\lt$ $x$ , for all $x \gt 0$ . So far, I have tried the following: I have created a function $$f(x) = ln(1+x) - x , $$ $x \gt 0$ . I have differentiated this $f(x)$ to give : $$f'(x) = \frac{1}{1+x} -1 = \frac{-x}{1+x}$$ which is clearly less than $0$ for all $x \gt 0$ . But I am struggling to apply the Mean Value Theorem here. I have noticed that $log(1) = 0$ and so: $$log(1+x) - x = f(x) - f(0) = x (f'(c))$$ for some $c \in (0,x)$ but I don't know where to go from here.","['calculus', 'derivatives', 'inequality', 'real-analysis']"
3788904,Finding a variable $n$ that satisfies the functional composition in which $f(f(f(f(n))))=3.$,"Let $f(x) = x^2-2x$ . How many distinct real numbers $c$ satisfy $f(f(f(f(n)))) = 3$ ? I have not found any good way to do this problem. I have just resorted to start with brute force: We start with $$f(f(f(f(x))))=f(f(f(x^2-2x)))=f(f((x^2-2x)^2-2(x^2-2x)))\cdots$$ on and on and on, until we finish our composition. However, this will make our problem extremely messy and is very inefficient. Is there any slicker way to start with, and how would I do it. Thanks in advance.","['functional-equations', 'algebra-precalculus', 'functions', 'function-and-relation-composition']"
3788918,Integral inequality - École Polytechnique International Entrance Exam,"(The exercise shown below is from a past admission exam taken in order to get accepted at the École Polytechnique, France, as an international student.) In that exercise, I was able to solve the first two items (1.1 and 1.2), but I'm having some problems in getting the right answer in the last one, 1.3. To solve item 1.3, I'm using the fact that $$ \left| \int_{a}^{b} f(t) dt \right|  \leq \int_{a}^{b} \left| f(t) \right| dt \qquad (*)$$ Furthermore, I'm using the first result presented in the item 1.1. By integrating the inequality shown in that result from $a$ to $b$ , I was able to get that $$ \int_{a}^{b} \left| f(t) \right| dt \leq K\frac{(b-a)^2}{2}$$ Then, it follows from (*) that $$ \left| \int_{a}^{b} f(t) dt \right|  \leq K\frac{(b-a)^2}{2} $$ However, the answer expected is $ \left| \int_{a}^{b} f(t) dt \right|  \leq K\frac{(b-a)^2}{4} $ It seems to me that I should also use the result obtained at item 1.2, but it isn't really clear to me how it would help me solve the last item. Could someone help me identify where my (possible) mistake is, or suggest a way to consider the result from item 1.2? Thanks. (P.S. My english isn't that good -- Sorry)","['integration', 'calculus', 'integral-inequality', 'real-analysis']"
3788981,Is the inverse of the differentiation chain rule true?,"I know that if $f$ and $g$ are differentiable $g ◦ f$ is differentiable but is the other way around true? That is, if $g ◦ f$ is differentiable then $f$ and $g$ are both differentiable I suspect this isn't true, but couldn't come up with a counter-example","['functions', 'real-analysis']"
3788986,Properties of $X′(t)=AX(t)$,"I am studying some properties of the initial value problem \begin{equation*}
   \begin{cases}
	\frac{d}{dt}X(t)=A(t)X(t),\\
		X(t_0)=I.
		\end{cases}
\end{equation*} where $A(t)$ is a real-valued $n\times n$ matrix. If we denote as $X(t,t_0)$ the solution of this problems, we have the following properties: $X(t,r)=X(t,s)X(s,r)$ $X(s,t)=X(t,s)^{-1}$ I want to show this two properties and I know how to proof the first one using the second. Basicly, if you define $Y(t)=X(t,s)X(s,r)$ , you can do the following calculation: \begin{equation*}
    \begin{split}
        \frac{d}{dt}Y(t)&=\frac{d}{dt}X(t,s)X(s,r)
        \\
        &=A(t)X(t,s)X(s,r)
        \\
        &=A(t)Y(t).
    \end{split}
\end{equation*} It is also easy to see that $Y(r)=I$ using the second property and we have the first property. My problem is, how can I prove the second property?","['matrix-equations', 'ordinary-differential-equations']"
3788988,"Stuck on how to approach integral $\int_0^1x^n\log(x)\,dx$","I am being asked the following: For $n \gt 0$ , and by considering an appropriate limit, find: $$\int_0^1x^n\log(x)\,dx$$ I want to try integration by parts, but I am confused about what the question means when it states by considering an appropriate limit.","['integration', 'limits', 'calculus', 'definite-integrals']"
3788993,How do we decide whether to visualize a matrix with its rows or columns?,"Should one visualize a matrix by its rows, columns, or both depending on the situation? I see both used and it seems arbitrary. It would be nice if only one was used consistently. Shouldn't a graph of a matrix be denoted as being a row or column representation somehow to avoid confusion? Example where author switches: https://intuitive-math.club/linear-algebra/matrices [Example I] Given the transformation: $$
\begin{bmatrix}
1 & 1\\
2 & 0
\end{bmatrix}
+
\begin{bmatrix}
2 & 1\\
1 & 1
\end{bmatrix}
=
\begin{bmatrix}
3 & 2\\
3 & 1
\end{bmatrix}
$$ The author represents the matrix after the transformation visually by its rows , using the following row vectors: $$
v_1 = 
\begin{bmatrix}
3\\
2
\end{bmatrix}
v_2 = 
\begin{bmatrix}
3\\
1
\end{bmatrix}
$$ [Example II] Given the transformation: $$
\begin{bmatrix}
0 & 1\\
-1 & 0
\end{bmatrix}
\begin{bmatrix}
3 & 1\\
1 & 1
\end{bmatrix}
=
\begin{bmatrix}
1 & 1\\
-3 & 1
\end{bmatrix}
$$ The author represents the matrix after the transformation visually by its columns , using the following column vectors: $$
v_1 = 
\begin{bmatrix}
1\\
-3
\end{bmatrix}
v_2 = 
\begin{bmatrix}
1\\
-1
\end{bmatrix}
$$ Question: Why is did they author seemingly arbitrarily switch from a row → column visual representation? What is the intuition behind this – if any?","['linear-algebra', 'linear-transformations']"
3789062,Convergence rate of Ito process path,"Let $|t-t_0|=O(\alpha_n)$ and $dX_t = \mu_t dt + \sigma_t dB_t$ with $X_0=0$ .
Then what is the convergence rate of the Ito process for a given realized path $\omega$ , i.e., $$
|X_t(\omega) - X_{t_0}(\omega)| = O(?).
$$ What I have tried. Using the mean value theorem, we have $|X_t(\omega)-X_{t_0}(\omega)|=\left|X'_{t^*}(\omega)(t-t_0) \right|$ .
So If $X'_{t_0}(\omega)$ is well-defined,  due to $|t^*-t_0|\le|t-t_0|=O(\alpha_n)$ , we have $O(\alpha_n)$ unless $X'_{t_0}(\omega)=0$ .
However, it is well known that the Brownian motion is nowhere differentiable, so $X'_{t_0}(\omega)$ is not defined for all $t_0$ .
The only thing that I can know is that since $X_t(\omega)$ is continuous, $X_t(\omega)\rightarrow X_{t_0}(\omega)$ anyway.
So is it possible to obtain the convergence rate of the above Ito process? Thanks,","['pointwise-convergence', 'brownian-motion', 'probability-theory', 'asymptotics']"
3789086,Finding n-th derivative,"How can I find the n-th derivative of $\frac{1}{x(x+1)}$ ? I tried expressing it as such, but I'm not sure about it. $\frac{1}{x(x+1)} = \frac{1}{x}-\frac{1}{x+1}$ edit: I think I found it, can anyone confirm? Let $f_n$ be the n-th derivative of $f$ $f_1\left(x\right)=-\frac{1}{x^2}+\frac{1}{\left(x+1\right)^2}$ $f_2\left(x\right)=\frac{2}{x^{\text{3}}}-\frac{2}{\left(x+1\right)^3}$ $f_3\left(x\right)=-\frac{6}{x^4}+\frac{6}{\left(x+1\right)^4}$ $f_4\left(x\right)=\frac{24}{x^5}-\frac{24}{\left(x+1\right)^5}$ We can then guess that the n-th derivative would be: $f_n\left(x\right)=\left(-1\right)^n\left(-\frac{n!}{x^{n+1}}+\frac{n!}{\left(x+1\right)^{n+1}}\right)$ However is this enough to ""proof"" the result? Like is it acceptable if I was in, say, an exam? $f_n$ is simply based on an assumption?... Also, how can I find x such as $f_n(x) = 0$ ?",['derivatives']
3789109,Ordered Factorization of positive integers with two prime factors,"The bottom of this page provides a solution to the number of ordered factorizations of a positive integer with two prime factors: https://mathworld.wolfram.com/OrderedFactorization.html i.e. if $n = p_1^{\alpha_1} p_2^{\alpha_2}$ Then numbers of ordered factorizations is $$H(n) = 2^{\alpha_1 + \alpha_2 - 1} \sum_{k=0}^{\alpha_2} {\alpha_1 \choose k}{\alpha_2 \choose k} 2^{-k}$$ And I am actually only interested in an easier version $m = \alpha_1 = \alpha_2$ . In which case $$G(p^m q^m) = 2^{2m - 1} \sum_{k=0}^{m} {m\choose k}{m\choose k} 2^{-k}$$ Is there a combinatoric argument to prove that result?  I found the paper that proved it using generating function, but I'd be curious to see a combinatoric argument. Also, how do I write $G(p^m q^m)$ as a generating function suppose we are able to infer this series through combinatoric argument?","['combinatorial-proofs', 'number-theory', 'elementary-number-theory', 'combinatorics', 'generating-functions']"
3789119,Deciding if $\mathbb{Z}\ltimes_A \mathbb{Z}^5$ and $\mathbb{Z}\ltimes_B \mathbb{Z}^5$ are isomorphic or not,"I have the two following groups $G_A=\mathbb{Z}\ltimes_A \mathbb{Z}^5$ , where $A=\begin{pmatrix} 1&0&0&1&0\\0&-1&0&0&0\\0&0&-1&0&0\\0&0&0&0&-1\\0&0&0&1&0\end{pmatrix}$ and $G_B=\mathbb{Z}\ltimes_B \mathbb{Z}^5$ , where $B=\begin{pmatrix} 1&0&0&1&0\\0&-1&0&1&0\\0&0&-1&0&0\\0&0&0&0&-1\\0&0&0&1&0\end{pmatrix}$ . The product is given (for example in $G_A$ ) by $(k,m)\cdot(\ell,n)=(k+\ell, m+A^k n)$ . Problem : Decide if $G_A$ is isomorphic to $G_B$ or not. My thoughts : I think strongly that they are not isomorphic. The matrices $A$ and $B$ are both of order 4, they're not conjugate in $\mathsf{GL}(n,\mathbb{Z})$ (neither $B$ and $A^{-1}$ ) but they are conjugate in $\mathsf{GL}(n,\mathbb{Q})$ . In some other cases, I've seen that they're not isomorphic by computing the abelianization, but in this case both have the same abelianization, namely $\mathbb{Z}\oplus\mathbb{Z}\oplus\mathbb{Z}_2\oplus\mathbb{Z}_2$ . Even worse, both have 1 as an eigenvalue. In my previous MO question there is a counterexample for the implication "" $G_A\cong G_B\Rightarrow A\sim B^{\pm 1}$ "" so I cannot use that. Can anyone give me any other ideas to prove that they aren't isomorphic? Or maybe to prove that they are isomorphic (if they were). Now I posted it in this MO question Thanks!","['group-isomorphism', 'matrices', 'semidirect-product', 'abstract-algebra', 'linear-algebra']"
3789135,If $f$ is entire can $e^f$ have a pole at infinity?,"Didn't see any like this in the similar questions, so hopefully it isn't a repeat. There was a question on a past qual that asked if $f$ is entire, can $e^f$ have a pole at infinity. I think the answer is going to be no. But would like confirmation. If possible it would be much appreciate if someone could tell me why my logic is correct or incorrect. Suppose $e^f$ has a pole at infinity. Clearly we cannot have $f$ be constant, hence as a consequence of Louisville's theorem we must have $\infty$ be either a pole or an essential singularity of $f$ . If $\infty$ were essential, then we could find a sequence $z_n\to\infty$ such that $f(z_n)\to c$ for some constant $c$ . Then $e^{f(z_n)}\to e^c$ and so $\lim_{z\to \infty}e^{f(z)}\not=\infty$ and $e^f$ does not have a pole at $\infty$ . The last case we have to consider is were $f$ has a pole at infinity. Let $g(w)=f(1/w)$ . Let $D$ an exterior domain, and define $1/D=\{1/z:z\in D\}$ . Then as $g$ has a pole inside $1/D$ it follows that either $$\int_{\partial 1/D} d\arg(g)\not=0\text{ }(*)$$ or $$\int_{\partial 1/D}d\arg(g)=0\text{ }(**)$$ If $(*)$ then by the argument principle modulo $2\pi$ it follows that $\arg(g)$ takes on every value in $[0,2\pi)$ , in particular we can choose $w\in 1/D$ with $g(w)\in i\mathbb{R}$ . If $(**)$ then as $g$ has a pole in $1/D$ at $0=1/\infty$ it follows that $g$ must have a zero in $1/D$ as $$\int_{\partial1/D}d\arg(g)=\#\{\text{ zeros of }g\text{ in }1/D\}-\#\{\text{ poles of }g\text{ in }1/D\}$$ In this case we have $g(w)=0\in i\mathbb{R}$ for some $w\in 1/D$ . We conclude that regardless of our situation we can choose $w\in 1/D$ with $g(w)\in i\mathbb{R}$ . As a consequence we can choose $z\in D$ with $f(z)\in i\mathbb{R}$ . Well then as $D$ was an arbitrary exterior domain we can choose $z_n\to\infty$ with $f(z)\in i\mathbb{R}$ , so $|e^{f(z_n)}|=1$ , and we cannot have $$\lim_{z\to\infty}e^{f(z)}=\infty$$ Consequently $e^f$ does not have a pole at infinity.","['complex-analysis', 'solution-verification']"
3789140,"Is the derivative of Riemann integral $\int_0^xf$, if it exists, always equal to $f(x)$?","Let $f:[0,1]\to\mathbb{R}$ be a Riemann integrable function. Define a function $F:[0,1]\to\mathbb{R}$ by $$F(x)=\int_0^xf.$$ Suppose that $F$ is differentiable at $c\in(0,1)$ . Then is it necessarily true that $F'(c)=f(c)$ ?
Note that it is true if $f$ is continuous at $c$ .","['riemann-integration', 'calculus', 'derivatives']"
3789143,If $\gamma_n$ are roots of $\tan x = x$ can every function be expanded in form of $\sum_n a_n \sin(\gamma_n x)$?,"Solving a linear PDE, I got the general solution $$f(x,t)=\frac{e^{-t/\tau}}x\sum^\infty_{n=1}a_n\sin(\gamma_n x)$$ where $\gamma_n$ is the nth positive root of $\tan x=x$ . To satisfy the initial condition we require $$\phi(x)=\frac1x\sum^\infty_{n=1}a_n\sin(\gamma_n x)$$ From a physical point of view, $\phi(x)=f(x,0)$ represents the initial temperature profile of the object, which can be arbitrarily defined. Hence, I assumed such a expansion form is possible for most sufficiently well-behaving $\phi(x)$ , but I cannot prove it. What are the necessary and sufficient conditions for expanding a function $\phi(x)$ into $\frac1x\sum^\infty_{n=1}a_n\sin(\gamma_n x)$ ? Noteworthily, $\gamma_n\sim\frac{\pi}{2}+n\pi$ thus this series is ‘almost’ a Fourier sine series. When this expansion turns out to be mathematically possible, how could one extract the $a_n$ s? One possible way is performing fourier transform on both sides, so that $$\int^\infty_{-\infty}x\phi(x)e^{-i\omega x}dx = \sum^\infty_{n=1}a_n\delta(\omega-\gamma_n)$$ assuming $\omega>0$ , but often does one have knowledge of $\phi(x)$ only on a physically meaningful region $[0,a]$ .",['real-analysis']
3789163,almost complex actions on $\mathbb{R}^{6}$,"As far as I understand, there are many almost complex structures on the $6$ -torus. A class $(a_{1},...,a_{15}) \in \mathbb{Z}^{15} = H^{2}(T,\mathbb{Z})$ is $c_{1}$ of an almost complex structure iff all $a_{i}$ are even, since the tangent bundle is trivial hence the second steifel whitney class vanishes. My question is, pick a non-standard almost complex structure (i.e. $c_{1} \neq 0$ ). We can lift the almost complex structure to $\mathbb{R}^{6}$ , which will be invariant by some action of $\mathbb{Z}^{6}$ , is it possible to do this explicitly? Edit: This question has now been answered at mathoverflow by Robert Bryant. https://mathoverflow.net/questions/377163/almost-complex-mathbbz6-action .","['complex-geometry', 'algebraic-topology', 'differential-geometry']"
3789171,how to solve $\int _0^1\frac{\ln \left(1+x\right)}{a^2+x^2}\:\mathrm{d}x$,"how to solve $$\mathcal{J(a)}=\int _0^1\frac{\ln \left(1+x\right)}{a^2+x^2}\:\mathrm{d}x$$ i used the differentiation under the integral and got \begin{align}
\mathcal{J(b)}&=\int _0^1\frac{\ln \left(1+bx\right)}{a^2+x^2}\:\mathrm{d}x
\\[3mm]
\mathcal{J'(b)}&=\int _0^1\frac{x}{\left(a^2+x^2\right)\left(1+bx\right)}\:\mathrm{d}x
\\[3mm]
&=\frac{a^2b}{1+a^2b^2}\int _0^1\frac{1}{a^2+x^2}\:\mathrm{d}x+\frac{1}{1+a^2b^2}\int _0^1\frac{x}{a^2+x^2}\:\mathrm{d}x-\frac{b}{1+a^2b^2}\int _0^1\frac{1}{1+bx}\:\mathrm{d}x
\\[3mm]
&=\frac{ab}{1+a^2b^2}\operatorname{atan} \left(\frac{1}{a}\right)+\frac{1}{2}\frac{\ln \left(1+a^2\right)}{1+a^2b^2}-\frac{\ln \left(a\right)}{1+a^2b^2}-\frac{\ln \left(1+b\right)}{1+a^2b^2}
\end{align} But we know that $\mathcal{J}(1)=\mathcal{J(a)}$ and $\mathcal{J}(0)=0$ \begin{align}
\int_0^1\mathcal{J'(b)}\:\mathrm{d}b&=a\:\operatorname{atan} \left(\frac{1}{a}\right)\int _0^1\frac{b}{1+a^2b^2}\:\mathrm{d}b+\frac{\ln \left(1+a^2\right)}{2}\int _0^1\frac{1}{1+a^2b^2}\:\mathrm{d}b-\ln \left(a\right)\int _0^1\frac{1}{1+a^2b^2}\:\mathrm{d}b
\\
&-\int _0^1\frac{\ln \left(1+b\right)}{1+a^2b^2}\:\mathrm{d}b
\\[3mm]
\mathcal{J(a)}&=\frac{1}{2a}\operatorname{atan} \left(\frac{1}{a}\right)\ln \left(1+a^2\right)+\frac{1}{2a}\ln \left(1+a^2\right)\operatorname{atan} \left(a\right)-\frac{1}{a}\ln \left(a\right)\:\operatorname{atan} \left(a\right)-\underbrace{\int _0^1\frac{\ln \left(1+b\right)}{1+a^2b^2}\:\mathrm{d}b}_{\mathcal{I}}
\end{align} but how to calculate ${\mathcal{I}}$ , i tried using the same technique but it didnt work","['integration', 'definite-integrals', 'leibniz-integral-rule']"
3789175,"Prove that $P=RA'\cap EF$, then $DP\perp EF$.","In triangle $ABC$ , let $DEF$ be the contact triangle, and let $(M)$ be the midpoint of the arc $(BC)$ not containing $(A)$ in $(ABC)$ . Suppose ray $MD$ meets $(ABC)$ again at $R$ . If $I$ is the incenter of $(ABC)$ and ray $RI$ intersects $(ABC)$ again at $A'$ , then $A'$ is the antipode of $A$ . If $P=RA'\cap EF$ , then $DP\perp EF$ . My Progress till now: tough problem ! Lemma :Let $ABC$ be a triangle with incenter $I$ , incircle $\omega$ , and circumcircle $ \Omega $ , and suppose that $\omega$ meets $BC, CA$ , and $AB$ at $D, E,$ and $F$ . Suppose that the circle with diameter $AI$ and $\Omega $ meet at two points $A$ and $R$ . Show that $RD$ bisects angle $\angle BRC$ . Proof : Note that the circle with diameter $AI$ will contain $E$ and $F$ .(Since $AI$ is the angle bisector and $IE=IF \implies \angle AFI=\angle AEI=90^{\circ}$ ) Note that there is a spiral symmetry $S$ centered at $R$ dilating $\Delta RFB$ to $\Delta REC$ ( considering the circle with diameter $AI$ and the circumcircle of $ABC$ ) . So we have $\Delta KFB$ similar to $\Delta REC \implies \frac{RB}{BC}= \frac{BF}{CE}= \frac{BD}{CD}$ ( as $D,F,E$ are intoch points ) . Hence we have , $\frac{RB}{BC}=\frac{BD}{CD}$ and by angle bisector theorem , we get that $RD$ bisects angle $\angle BRC$ . So, by this lemma, we get that $RD$ bisects arc $BC$ ( let's say at $M$ ). Moreover, since $\angle AFI=\angle AEI=90^{\circ}$ , we get that $\angle ARI=90^{\circ} \implies RIA'$ are collinear , where A' is the antipode of A . But I am stuck with point $P$ . Hope someone can give hints. Thanks in advance.","['contest-math', 'euclidean-geometry', 'geometry']"
3789188,"Solving the system $A = \alpha\cot\theta + \beta\cot\varphi$, $B = \gamma\cos\theta + \delta\cos\varphi$ for $\theta$ and $\varphi$","I am looking to solve the following system of equations for $\theta, \varphi \in (0, \pi/2]$ . Please note that both the trigonometric cotangents and cosines are involved. $$
A = \alpha\cot(\theta) + \beta\cot(\varphi) \\
B = \gamma\cos(\theta) + \delta\cos(\varphi)
$$ with $A, B, \alpha, \beta, \gamma, \delta \in \mathbb{R}$ all known. If simplification is needed, I can allow the assumptions $B = 0$ $A, \alpha, \beta, \gamma > 0$ $\delta < 0$ $\left|\gamma\right| > \left|\delta\right|$ I have the following questions: Does this system of equations permit an analytic solution? If so, how? What would be the easiest method for solving this with a scientific programming language? It's an algebraic nightmare, and I think I'll just be trying to implementing a computational solution (still working on) until someone more clever than me (not hard to find) solves this. Or I'm not going crazy haha.","['trigonometry', 'systems-of-equations']"
3789216,How many different ways can you go about completing a course/ class at university?,"How many different combinations of results in assignments are possible in a University course? I am interesting in calculating the number of unique ways I can finish this course that I am doing. To make things easier, there are no partial marks. Here are some conditions: There are four assessable tasks Assignment 1: Weighted 10% Assignment 2: Weighted 15% Assignment 3: Weighted 15% Assignment 4: Weighted 60% Possible marks for each assessment: Assignment 1: /10 Assignment 2: /10 Assignment 3: /10 Assignment 4: /100 I am a little rusty on my combinations discrete mathematics. Can this be viewed as a pigeonhole principle problem or would this be permutations/combinatrix? From an algorithmic point of view, how would you go about solving this? TL/DR: How many combinations of indiviudal graded assessments can you get in a course? What is the range of end of semester marks possible? Thank you for your time.","['permutations', 'combinations', 'pigeonhole-principle', 'combinatorics', 'discrete-mathematics']"
3789241,Define polar coordinates of circle at origin and circle with radius $R$.,"Question: (i) Define in polar coordinates $r = f(\alpha)$ the origin-centred circle with radius $R$ . Specify the domain range for the polar coordinate $\alpha$ . (ii) Define in polar coordinates $r = f(\alpha)$ a circle with radius $R$ and the centre at the Cartesian coordinates $(R, 0)$ . Specify the domain range for the polar coordinate $\alpha$ . My Answer for (i): $x = r \cdot \cos \alpha$ $y = r \cdot \sin \alpha$ $x^2 + y^2 = R^2$ (equation for circle, radius R centered at origin) $r^2(\cos^2 \alpha + \sin^2 \alpha) = R^2 $ $r^2 = R^2$ $r = R$ The domain range of $\alpha$ is $(-\infty, \infty) $ ? Is my answer correct? I don't think it's correct because my RHS doesn't have $\alpha$ at all. I've learnt that domain is the set valid input values and range is the set valid output values. However, I don't understand what the question means by ""domain range"" of $\alpha$ ? For part (ii) I don't understand how do you shift a circle centered at origin to become the radius. I am sorry if this seems like a trivial question, please don't close this question. I haven't done any serious math for the last 2 years, especially geometry. The textbook that he had written (and made us buy) didn't really ease us into the topic but expected us to already know all these formulas, I only managed to do these much after 6 hours of googling and reading up on functions, polar equations, and so on. Please please please help, thanks!!","['circles', 'geometry', 'calculus', 'polar-coordinates', 'trigonometry']"
3789271,Deciding the range of a rational function with undefined points,"Consider the function, $$ f(x) = \frac{\sin x}{x}$$ now, we know that x=0 evaluation is undefined but , $$ \lim_{x \to 0} f(x) =  \lim_{x \to 0} \frac{\sin x}{x}$$ Now, the limit evaluates to x=1, so is {1} included in the range of this function or not? as in if we define the function from $ \mathbb{R} \to \mathbb{R}$ , is the $ x=0$ point defined or not. As in, the limit exists but the not the function at the point, therefore is 0 included in domain or not?",['limits']
3789295,How to interpret the meaning of $\mathbf n d\sigma$ in terms of differential forms?,"We often write stokes's formula in $\mathbb R^n$ as $$
\int_\Omega \nabla\cdot \mathbf f d\mu=\int_{\partial \Omega} \mathbf f \cdot \mathbf n d\sigma.
$$ My questions is: what does $\mathbf n d\sigma$ mean? it is written as if it is a vector, but $d \sigma$ is essentially a $(n-1)$ -covector.  Multiplying a covector by $\mathbf n$ does not make sense. Interpreting $\mathbf f \cdot \mathbf n d\sigma$ as the product of a function and a $(n-1)$ -covector does not work as well, because when I calculate it in two dimensions, it does not work. Let's say that the normal vector is $(\sin \theta, -\cos \theta)$ , then $\mathbf f \cdot \mathbf n = f_1 \sin \theta -f_2 \cos \theta$ , and $d \sigma = \cos \theta dx_1 + \sin \theta dx_2$ . Multiplying $\mathbf f \cdot \mathbf n$ and $d \sigma$ does NOT lead to the expected expression $f_1 dx_2 -f_2 dx_1$ or something similar. So how could I translate $\mathbf n d\sigma$ to the language of differential form?","['stokes-theorem', 'differential-forms', 'differential-geometry']"
3789347,"How many nonnegative integers $x_1, x_2, x_3, x_4$ satisfy $2x_1 + x_2 + x_3 + x_4 = n$?","Can anyone give some hints about the following question? How many nonnegative integers $x_1, x_2, x_3, x_4$ satisfy $2x_1 + x_2 + x_3 + x_4 = n$ ? Normally this kind of question uses stars and bars but there are $2x_1$ , which I don’t know how to handle. Help please! Ps :I think may be we can use recurrence relation.","['combinatorics', 'recurrence-relations']"
3789355,Double integration with Indicator function,"The integral of interest is: $$Q = \int_{\mathbb{R}^2}\int_{\mathbb{R}^2} I\left(\frac{1}{2}\frac{x_2^2 - x_1^2 + y_2^2 - y_1^2}{x_2-x_1} \in [0,1]\right) \nonumber \\
\times I\left(2\arcsin\left(\frac{\sqrt{(x_2-x_1)^2 + (y_2-y_1)^2}}{2\sqrt{y_1^2 + \left(x_1 - \frac{1}{2}\frac{x_2^2 - x_1^2 + y_2^2 - y_1^2}{x_2-x_1}\right)^2}}\right) > \theta\right) \\ \times \exp\left(-C\left(y_1^2 + \left(x_1 - \frac{1}{2}\frac{x_2^2 - x_1^2 + y_2^2 - y_1^2}{x_2-x_1}\right)^2\right)\right)\mathrm{d}Z_1\mathrm{d}Z_2,$$ where $C > 0$ , $Z_1 = (x_1, y_1)$ , $Z_2 = (x_2,y_2)$ , $I(\cdot)$ is an indicator function, and $\theta \in (0, \pi]$ .",['integration']
3789373,Discrete Polynomial Ham-Sandwich Theorem proof,"Discrete Polynomial Ham-Sandwich Theorem: Consider an algebraic hypersurface of degree $k$ in $\Bbb R^n$ ; let $Q$ be a polynomial of degree $k$ in $n$ variables. Define $m := \binom {n + k} {n}$ - 1. Let $A_1, \ldots, A_m$ be disjoint finite sets in $\Bbb R^n$ . Then there exists $Q$ s.t. $$
\forall i \in [m], \quad \left| A_i \cap \{ Q<0 \} \right|, \left| A_i \cap \{ Q > 0 \} \right| \le \frac {\left| A_i \right|} 2
$$ Proof: I read a proof sketch for the Discrete Ham-Sandwich Theorem (see at the bottom) in which the ""cutting""/bisecting surface is (degenerate to) only being a hyperplane (rather than an algebraic hypersurface). In this proof I got familiar with the ""trick of balls with volume"" (do you have a better name for that?). My question is why not using the same ""trick of balls with volume"" to reduce the discrete case into the continuous case (in which $A_1, \ldots, A_m \subset \Bbb R^n$ are disjoint bounded open sets), to which I already have a proof sketch (see below)? Thank you. Proof sketch for Continuous Polynomial Ham-Sandwich Theorem: Let $Q$ be a polynomial of degree $k$ in $n$ variables. We want to show that $\exists Q$ s.t. $$
\forall i \in [m], \quad vol \left( A_i \cap \{ Q<0 \}\right) = vol \left( A_i \cap \{ Q > 0 \}\right) = \frac {vol(A_i)} 2
$$ Note that $m$ is the number of non-constant monomials in $n$ variables of degree $\le k$ . Thus we can make polynomials of degree $≤ k$ from points of the $m$ -sphere $\Bbb{S^m}$ by identifying the vector $b = (b_0 , \ldots , b_m) ∈ \Bbb S^m$ with the polynomial $Q_b$ whose coefficients are precisely the values $b_0 , \ldots , b_m$ . Define $f : \Bbb S^m \rightarrow \Bbb R^m$ as follows $$
f_i (b_0 , \ldots , b_m) = vol \left( A_i \cap \{ Q_b<0 \}\right) - vol \left( A_i \cap \{ Q_b > 0 \}\right)
$$ In other words, the $i$ -th coordinate of $f$ measures how good the hyperplane $\{Q_b = 0\}$ is at bisecting the set $A_i$ ; In particular, this hyperplane bisects $A_i$ if and only if $f_i(b_0 , \ldots, b_m ) = 0$ . $f$ can be shown to be continuous using the dominated convergence theorem. Intuitively, if we vary $b_0 , \ldots , b_m$ by just a little bit, then the polynomial $Q_b$ will only change a little bit, which means that the location of its zero-set $\{Q_b = 0\}$ will only change a little bit, so the value of each $f_i$ will only change a little bit; then since every $f_i$ is continuous, we get that $f$ is continuous. Also, $f$ is odd since every coordinate satisfy $$
\begin{split}
f_i(-b_0 , \ldots, -b_m ) &= vol \left( A_i \cap \{ Q_{-b}<0 \}\right) - vol \left( A_i \cap \{ Q_{-b} > 0 \}\right) \\
&= vol \left( A_i \cap \{ Q_{b} > 0 \}\right) - vol \left( A_i \cap \{ Q_{b} < 0 \}\right) \\
&= -f_i(b_0 , \ldots, b_m )
\end{split}
$$ By the Borsuk–Ulam theorem, $\exists (a_0 , \ldots , a_m) ∈ \Bbb S^m$ s.t. $f (a_0 , \ldots , a_m) = 0$ . This implies that the hypersurface $\{Q_a = 0\}$ bisects each $A_i$ . Proof sketch for Discrete Ham-Sandwich Theorem: For each $i \in [m]$ we define $A_i'$ to be the set of balls of radius $\epsilon$ , centered around every point in $A_i$ , namely $A_i' := \bigcup_{x \in A_i} B(x, \epsilon)$ . We choose $\epsilon$ s.t. the hyperplane intersects in total at most $n$ balls. Therefore whenever $n$ is odd, we know that on one hand the hyperplane must intersect at least one ball for each $A_i'$ , but on the other hand it can't intersect more than $n$ balls; concluding that the hyperplane intersect exactly one ball for each $A_i'$ , and by symmetry the hyperplane must pass through the ball's center in order to bisect its volume. It means that the hyperplane bisects the original $A_i$ as required. Otherwise (i.e $n$ is even), it's possible to remove any point, do what we did for odd $n$ , then return the point, since we are guaranteed that on each side of the hyperplane there won't be more than half of the points (after adding back the removed point).","['general-topology', 'lebesgue-measure', 'algebraic-topology', 'measure-theory']"
3789381,Compute the value of the integral $\int_1^{\infty} \lfloor x^2 \rfloor e^{-x} \ \mathrm d x $,"I want to compute the value of the integral $$\int_1^{\infty} \lfloor x^2 \rfloor e^{-x} \ \mathrm d x $$ where $\lfloor \  \rfloor$ denotes the floor function. My try: I split into a series: since $\lfloor x^2 \rfloor = n$ is equivalent to $\sqrt{n} \le x < \sqrt{n+1}$ , I get $$\int_1^{\infty} \lfloor x^2 \rfloor e^{-x} \ \mathrm d x = \sum_{n=1}^{\infty} n \int_{\sqrt{n}}^{\sqrt{n+1}} e^{-x} \ \mathrm d x = \sum_{n=1}^{\infty} n (e^{-\sqrt{n}}-e^{-\sqrt{n+1}})$$ Now I write down the first few terms of the series and write it in another way: $$\sum_{n=1}^{\infty} n (e^{-\sqrt{n}}-e^{-\sqrt{n+1}}) = 1 e^{-\sqrt{1}} - 1 e^{-\sqrt{2}} +2 e^{-\sqrt{2}} - 2 e^{-\sqrt{3}} + 3 e^{-\sqrt{3}} - 3 e^{-\sqrt{4}} + \cdots$$ and note that terms cancel. Thus the series is equal to te following limit: $$\lim_{n \to \infty} ( e^{-\sqrt{1}} +  e^{-\sqrt{2}} +  e^{-\sqrt{3}} + \cdots +e^{-\sqrt{n}}) - n e^{-\sqrt{n+1}}$$ And here I get stuck. Maybe I should use some integration by parts, but as $\lfloor x^2 \rfloor$ is discountinuous I don't know if I'm allowed to use it.","['integration', 'calculus', 'definite-integrals', 'sequences-and-series']"
3789419,Do finite linear combinations of sine functions always have infinitely many zeros?,"To determine the sine-like function(ex: sound, electromagnetic wave), there are three factors: amplitude, freq and phase. While I study about composition of waves, I have one question - can we say that the composed wave has infinitely many '0-displacement points'? In formally, let the function $f(x)$ defined as below, while $n \in \mathbb{N}$ is the given constant. $f(x) = \sum_{i=1}^{n}{a_i \sin(b_i x + c_i)}$ To make clear statement, let's assume that sequence $\{a_i\}$ doesn't contain any 0. Also, $f(x)$ is not always zero - for example, if $\{a_i\} = \{2, 1, 1\}, \{b_i\} = \{1, 1, 1\}, \{ c_i \} = \{\pi, 0, 0\}$ , $f(x) = 2\sin(x+\pi) + \sin(x) + \sin(x)=0$ always. So my question is, does the function $f$ has infinitely many zeros, regardless to given sequence $\{a_i\}, \{b_i\}, \{c_i\}$ and $n$ ? If it is, how can I prove it with mathmatical way? I strongly believe that it has inf. many solutions because wave without 0-displacement is unnatural, but I want to prove it with formal way. If not, then does it have at least one solution? Is there counterexamples? Thank you for reading my long and not well-organized questions. Thank for TheSilverDoe, $b_i \neq 0$ for all $i = \{1, ..., n\}$","['summation', 'functions']"
