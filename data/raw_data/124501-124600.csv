question_id,title,body,tags
1882886,Can we extend Young's convolution inequality with $BMO$ instead of $L^\infty$,"Obviously $\|f*g\|_{L^\infty}\leq\|f\|_{L^1}\|g\|_{L^\infty}$. Do we have the stronger bound $\|f*g\|_{L^\infty}\leq C\|f\|_{L^1}\|g\|_{BMO}$? Or almost as good, $\|f*g\|_{L^\infty}\leq C\|f\|_{H^1}\|g\|_{BMO}$? I think this might follow from the fact that interpolation still works when you replace $L^\infty$ with $BMO$. Edit: It seems to me that the first statement is false for example if you take $f=1_{[0,1]}\in L^1$ and $g(x)=\log|x|\in BMO$. Then for $x>1$,
\begin{align*}
f*g(x)=x\log x-(x-1)\log(x-1)-1
\end{align*}
is not $L^\infty$. For the second claim, I'm tempted to use the duality inequality for $H^1$ and $BMO$ to say something like
$$|\int f(t)g(x-t)dt|\leq\|f\|_{H^1}\|g\|_{BMO}$$
but I know that this is only really supposed to hold for $f\in H_0^1$.","['harmonic-analysis', 'real-analysis']"
1882963,strictly concave function whose third derivative is negative?,"Consider a continuous function $u(q)\geq 0$ whose domain is $[0,\infty)$ that satisfies the following conditions: $u^{\prime }>0$ for all $q \in [0,\infty)$, $u^{\prime \prime }<0$ for all $q \in [0,\infty)$, $u(0)=0$, $u^{\prime }(0)<\infty $ Now I am wondering whether $\frac{u^{\prime \prime }(q)}{u^{\prime }(q)}$ is weakly increasing in $q$ always hold ? Is there any function $u(q)$ making $\frac{u^{\prime \prime }(q)}{u^{\prime }(q)}$decreasing in $q$ or non-monotonic in $q$? Here is what I have done:
$\frac{d\Big(\frac{u^{\prime \prime }(q)}{u^{\prime }(q)}\Big)}{d q}=\frac{u^{\prime \prime \prime}(q)u^{\prime}(q)-[u^{\prime \prime}(q)]^{2}}{[u^{\prime}(q)]^{2}}$. So if $u^{\prime \prime \prime}(q)<0$ or $u^{\prime \prime \prime}(q)u^{\prime}(q)<[u^{\prime \prime}(q)]^{2}$ holds for some $q$, I can conclude the above conjecture is not true. But I cannot figure out a such a function. Can you help to find out such a function? thank you !",['functions']
1882973,Need help with O'Neill's proof that partial derivative operators form a basis for TpM,"I am trying to understand a proof in B. O'Neill's Semi-Riemanian Geometry . Specifically, I am trying to understand 12. Theorem which in my book is on page 8. The proof is to the theorem that the set $\{\partial_1, \ldots \partial_n \}$ forms a basis in $TpM$. Let $\xi(\mathcal U)$ be a chart defined on an open set $\mathcal U$, and let $\xi(\mathcal U) = \{q \in \mathbb{R}^n : |q| < \epsilon \}$ for some $\epsilon > 0$. Let $g$ be a smooth function. Suppose $0 \leq t \leq 1$. My problem begins right at the start of the proof where he says to define $$g_i(q) := \int_0^1 \frac{\partial g}{\partial u^i}(tq) dt$$
Where did this function come from? What does it even mean? What is the reason for the $tq$ argument and not just $t$? Then he states that using the Fundamental Theorem of Calculus, one can obtain: $$g = g(0) + \sum g_i u^i$$ And the proof goes on from there. I think I understand how to get the second equation using the Fundamental Theorem of Calculus, but I have no idea where that integral came from or what it means and that is where I need help. (I know I can just treat it as a definition, but it drives me crazy if I can't see where it came from and understand it.)","['smooth-manifolds', 'differential-geometry']"
1882982,Why are we allowed to multiply a 1x1 matrix by any matrix? [duplicate],"This question already has answers here : Multiplying by a $1\times 1$ matrix? (3 answers) Closed 7 years ago . So, in order to multiply 2 matrices, there must be the same number of columns in the left matrix as there are rows in the right matrix. So if $A$ is an $m \times n$ matrix and $B$ is a $p \times q$ matrix, must not $n = p$ in order for $AB$ to exist and mustn't $q = m$ in order for $BA$ to exist. By this logic, we should only be allowed to multiply a $1 \times 1$ matrix by either a $1 \times n$ matrix on the right or a $n \times 1$ matrix on the left. However, if $C$ is a $1 \times 1$ matrix and $D$ is a $m \times n$ matrix, where neither $m$ nor $n = 1$, we're allowed to multiply the 2 matrices simply by multiplying each entry in $D$ by the entry in $C$. Why? Shouldn't the rules for a $1 \times 1$ matrix be the same as for all the other matrices?","['matrices', 'linear-algebra']"
1882999,Representable functor from scheme to Groups,"I want to do the following question (6.6k) in Vakli's Foundations of algebraic geometry Suppose we have a contravariant functor $F$ from $Sch$ to $Groups$. Suppose further that $F$ composed  with the forgetful functor is represented by the object Y. Show that the group operations on $F(X)$ uniquely determine $m:Y×Y \rightarrow Y$, $i:Y \rightarrow Y$, $e:Spec\mathbb{Z} \rightarrow Y$. And the group operation on $Mor(X,Y)$ is the same as on $F(X)$ Suppose $F$ composites with the forgetful functor is equal to $h_Y$
I know that $m:Y×Y \rightarrow Y$ must be the product of  the projection morphism $π:Y×Y \rightarrow Y$ with itself and $e$ must be identity element of the group $Mor(spec\mathbb{Z} ,Y)$ For the morphism $i$, it composites with $f \in Mor(Y,Y) $ to give $f^{-1}$. But I have no idea why this morphism is unique. Also I don't know why $F(X)$ should have group structure induced by $Y$. I'm sorry if this is a stupid question. Any help or hints are appreciated.","['group-schemes', 'algebraic-geometry']"
1883015,Why does this trick gives precisely the formulas for the sum of the $n$ first naturals and the $n$ first squares?,"I've learned a cool trick several days ago. Suppose I want to find a polynomial that gives me: $$f(1)=1, \quad f(2)=5,\quad  f(3)=14,\quad f(4)=30\tag{1}$$ I could do the following: Take a polynomial of degree $3$ and make the following system with it: \begin{eqnarray*}
  {ax^3+bx^2+cx+d}&=&{1} \\ 
  {ax^3+bx^2+cx+d}&=&{5} \\ 
  {ax^3+bx^2+cx+d}&=&{14} \\ 
  {ax^3+bx^2+cx+d}&=&{30} 
\end{eqnarray*} I just need to substitute the $x^n$'s according to $(1)$. This gives me: \begin{eqnarray*}
  {a1^3+b1^2+c1+d}&=&{1} \\ 
  {a2^3+b2^2+c2+d}&=&{5} \\ 
  {a3^3+b3^2+c3+d}&=&{14} \\ 
  {a4^3+b4^2+c4+d}&=&{30} 
\end{eqnarray*} Now this is easy, we need only to find the coefficients $a,b,c,d$, that is, find the inverse of a matrix $A$ in the matricial equations $Ax=b$: $$\left(
\begin{array}{cccc}
 1 & 1 & 1 & 1 \\
 2 & 4 & 8 & 16 \\
 3 & 9 & 27 & 81 \\
 4 & 16 & 64 & 256 \\
\end{array}
\right)$$ And then: $x=A^{-1}b$. This gives me: $$a=\frac{1}{3}\quad\quad  b= \frac{1}{2}\quad\quad  c= \frac{1}{6}\quad\quad   d= 0$$ That is, our polynomial is: $\cfrac{x^3}{3}+\cfrac{x^2}{2}+\cfrac{x}{6}$ and this is - at least to me - surprising because it is the formula for the sum of the first $n$ square numbers. What baffles me more is that if I do the same with a polynomial of degree $4$ and a $5\times 5$ matrix and $f(5)=55$ it will give me the same formula. So I have three questions: When I did this trick, I used only $4$ and $5$ values of the sum of the first $n$ squares. So why does it give me exactly the polynomial for the sum of the first $n$ squares instead of any other polynomial? There is an infinite abyss of possible polynomials, why exactly this one? For example: Why didn't it give a polynomial that gives me: $$f(1)=1, \quad f(2)=5,\quad  f(3)=14,\quad f(4)=30 \quad f(5)=55\quad  f(6)=2\quad ?$$ Why does the result holds for a higher $n$, that is: Why doing the same thing with $n+1$ gives me the same polynomial it gave me for $n$ instead of any other polynomial in the behemothic chasm of possible polynomials? I've tried for other formulas in the past, just like the sum of the first $n$ positive integers. Why does it hit exactly the polynomial for the sum of the first $n$ positive integers instead of any other polynomial in the humongous crevasse of possible polynomials? Sorry if the question is too stupid, but I've figured this trick some time ago and couldn't find a clue to why this is hapenning. The main drama of the thing for me is that it seems as if I were doing incomplete induction and then BAM! it just hits exactly where I wanted it to hit.","['polynomials', 'linear-algebra']"
1883017,Does there exist a metrizable space of which do we not know its metric?,"It seems that in topology, we have very good ways to deduce whether a space is metrizabe or not. For example, if a space $(X, \mathcal{T})$ is metrizable, then it is Hausdorff, $T_3$, $T_4$, $\ldots$, $T_6$, first countable, separable $\Leftrightarrow$ second countable, and we can do even further by applying metrizability theorems like Urysohn's theorem. Then by negation, if we are missing some of those properties, then it is not metrizable. But how do people come up with the actual metric on those spaces? It seems to me as far a $\mathbb{R}$ is concerned, there is sort of this intuitive/common sense feeling what the metric should be. For example, the absolute value of distance between two points. And lo and behold, it just happens to satisfy the metric axioms. So to me it is not very clear if this way will always work for weirder topological spaces that happens to be metrizable. Are there metrizable spaces for which we do not know what metric actually generates the topology?","['general-topology', 'metric-spaces', 'soft-question']"
1883026,How to solve $\int_0^{2\pi} \frac{dt}{3+\sin(t)}$?,"Got stuck with the integral. I rewrote it as $$I=\int_0^{2\pi} \frac{2idt}{6i+e^{it}-e^{-it}},$$
then I took $z:=e^{it}\implies dz=ie^{it}dt\implies dt = -\frac{idz}{z}$, so I'd get: $$\int_\gamma \frac{2dz}{6i+z-\frac{1}{z}}=\int_\gamma \frac{2zdz}{6iz+z^2-1}.$$ It can be observed that $-i(2\sqrt{2}\pm 3)$ are the singular points of the integrand. Then the residues are $\pm \frac{i}{4\sqrt{2}}$. And the integral would become: $$ I=0 $$ as per the Cauchy Residue Theorem. Where am I mistaken? Appreciate your input.","['complex-analysis', 'integration', 'contour-integration', 'residue-calculus']"
1883064,Can two different topological spaces cover each other?,"I.e. do there exist non-homeomorphic $X$, $Y$ and covering maps $f:X\rightarrow Y$, $g: Y\rightarrow X$? I have a basic understanding of covering space theory as its taught in school. I was inspired by this question Two covering spaces covering each other are equivalent? I have done some of the things you would do, looking at what happens to fundamental groups.  And I've tried to find counter examples by drawing  graphs.","['algebraic-topology', 'general-topology', 'covering-spaces']"
1883094,Every finite-dimension subspace of $\mathcal{X}$ is closed.,"Background Information: (Folland)Theorem 5.8 - Let $\mathcal{X}$ be a normed vector space. a.) If $M$ is a closed subspace of $\mathcal{X}$ and $x\in \mathcal{X}\setminus M$ , there exists $f\in\mathcal{X}^*$ such that $f(x)\neq 0$ and $f(M) = \{0\}$ . In fact, if $\delta = \inf_{y\in M}\|x - y\|$ , $f$ can be taken to satisfy $\|f\| = 1$ and $f(x) = \delta$ . Question: Let $\mathcal{X}$ be a normed vector space. a.) If $M$ is a closed subspace and $x\in\mathcal{X}\setminus M$ then $M + \mathbb{C}x$ is closed. (Use Theorem 5.8a.)). b.) Every finite-dimensional subspace of $\mathcal{X}$ is closed. Proof a.) - Let $M$ be a proper closed subspace of $X$ and let $x\in X\setminus M$ . There existts $f\in X^*$ such that $f(x)\neq 0$ and $f(M) = \{0\}$ . Let $\{u_n + a_nx \}_{1}^{\infty}$ be a sequence in $M + Kx$ that converges to $y\in X$ . Then $$f(y) = \lim\limits_{n\rightarrow \infty}f(u_n + a_n x) = \lim\limits_{n\rightarrow \infty} a_nf(x)$$ since $f$ is continuous, so $\{a_n\}_{1}^{\infty}$ converges to $a:= f(y)/f(x)$ , which implies that $\{a_nx \}_{1}^{\infty}$ converges to $ax$ . Therefore $$\{u_n\}_{1}^{\infty} = \{(u_n + a_nx) - a_nx\}_{1}^{\infty} \to  (y - ax)$$ which lies in $M$ because $M$ is closed. It follows that $y\in M + Kx$ , which shows that $M + Kx$ is closed. Proof b.) - I have spent a considerable amount of time thinking about this but I am not sure how to proceed or how to show this result. Any hints or suggestions are greatly appreciated.",['functional-analysis']
1883120,Bayesian approach to polling sample uncertainty,"Suppose I have a three-way election between candidates A, B, and C, with 1,000,000 voters. A national poll on a fair sample of N=1000 yields the following breakdown, with a margin of error at ±3%: A - 50%
B - 40%
C - 10% Now suppose I pick a voter at random from the whole population, not necessarily the sample. What prior probabilities do I assign to that voter choosing A, B, or C? The reason I'm hung up here is that the prior probabilities given to the individual voter are related to the posterior probability distribution of the entire population after taking the sample - e.g. the population parameter. If we knew the exact population parameter, we'd know what prior probability distribution to assign to the individual voter - but we don't. Instead, we have a probability distribution of population parameters consistent with the sample, so we have a probability distribution of probability distributions to give to the voter. The way I'm doing it in my head is to create a massive joint probability distribution - one random variable is the population parameter, the other is the voter breakdown for each hypothetical population. Then, to figure out the overall prior probability I should assign to a random person in the population picking ""A"", ""B"", or ""C"", I just compute the marginal probabilities and call it a day. However, is this the right approach? These samples are usually done using the frequentist ""confidence interval"" approach to quantify error, whereas I'm thinking in Bayesian terms which uses ""credible intervals"" instead. Is there some frequentist/Bayesian subtlety here that can throw me off by mixing them?","['bayesian', 'statistics', 'probability', 'statistical-inference']"
1883127,$L^2$ operator norm and convergence,"Question : Consider $T_n : L^2[-\pi,\pi] \to L^2[-\pi,\pi]: u(x) \mapsto  \displaystyle \int_{-\pi}^{\pi} \left( e^{\frac{x}{n} - \frac{y}{n}} \right) u(y) \; dy$, for $n \in \mathbb{Z}^{\ge 1}$. (a) Is $T_n$ compact? Self-adjoint? (b) Calculate $\|T_n\|_{op}$. (c) Does $T_n$ converge? If so, to what and in what (operator) sense? Solution : (a) Compactness - Notice that $T_n (u) (x) = e^{\frac{x}{n}} \displaystyle \int_{-\pi}^\pi e^{-\frac{y}{n} } u(y) dy$, so ran$(T_n)$ is contained in the finite dimensional subspace spanned by $e^{x/n}$, meaning it's compact. I calculated $T_n^*(u)(x) = \displaystyle \int_{-\pi}^{\pi} \left( e^{\frac{y}{n} - \frac{x}{n}} \right) u(y) \; dy$, so it's not self-adjoint. (b) I'm stuck here. I can use Cauchy Schwarz to show that $\|T_n\|_{op} \le \|K\|_{L^2[-\pi,\pi]}$, where $K(x,y) = e^{x/n-y/n}$. But any attempts at the reverse inequality have not been successful . . . (c) It converges to the identity . . . I think strongly. Because of LDCT I can push a limit inside the integral. Uniformly, I'm unsure. This is an old test problem, so part of why I ask this question is because I will have to answer similar questions on the exam in a timely manner. I could maybe mess around with part (c) for a while and come up with some function $u(y)$ that demonstrates non-uniform convergence, but I'm curious if there's a better way to go about it. I can't think of any examples now. Please let me know if my work is correct or you can offer help. Thank you","['functional-analysis', 'compact-operators', 'lp-spaces', 'operator-theory']"
1883131,Center of Lie group and Lie algebra,"Let $G$ be a Lie group and $\mathfrak{g} = T_eG$ its Lie algebra (where $e \in G$ is the neutral element). Denote $Z(\mathfrak{g})$ the center of $\mathfrak{g}$ and $Z(g)$ the center of $G$. I've read the following statement but don't see how to prove it:
$$Z(\mathfrak{g}) = 0 \Longleftrightarrow Z(G) \text{ is zero dimensional.}$$
I'm mainly interested in the ""$\Leftarrow$"" direction, as this is used in a corollary of the Bonnet-Myers theorem in the text I'm reading. Now, my knowledge of Lie groups is very basic. For instance, I guess that the dimension of $Z(G)$ means its dimension as a manifold. So I would think that $Z(G)$ is a submanifold of $G$. But how do I even see this in general? According to this question When is the Lie algebra of the center of Lie group the center of its Lie algebra , it seems to be difficult to find conditions when $Z(\mathfrak{g}) = Z(G)$ holds in general, however the statement above looks more simple. Would someone be able to prove the above equivalence?","['riemannian-geometry', 'differential-geometry', 'lie-algebras', 'lie-groups']"
1883140,An integration of product $(1-x^n)$,"Prove $$\int_0^1\prod_{n=1}^\infty(1-x^n)dx=\frac{4\pi\sqrt3}{\sqrt{23}}\frac{\sinh\frac{\pi\sqrt{23}}3}{\cosh\frac{\pi\sqrt{23}}2}.$$
In fact, product $(1-x^n)$ is difficult to compute, I hope you can show me some ideas, thank you!","['products', 'real-analysis', 'integration', 'calculus']"
1883159,To find solution of $y''(x)=ay^3(x)+by^2(x)+cy(x)+d$,"We know that
$$g'(x)=g(x)$$
$g(0)=1$
Then $g(x)=e^x$ If we want to solve $y'=ay$, We can write general solution $y'=ay$ (or if derivative both sides $y''=a^2y$)
 $$y=k.g(a x)=k.e^{ax}$$
 where $k$ is a constant. We can express particular solution of many differential equation via using $e^{\beta x}$ but some we cannot.
For example, we can find a particular solution for $y'=1+y^2$  (if we apply derivative both sides then we get  $y''=2y^3+2y$) and particular solution for $y''=2y^3+2y$ can be written: $$y=\tan(x)=-i\frac{g(ix)-g(-ix)}{g(ix)+g(-ix)}=-i\frac{e^{ix}-e^{-ix}}{e^{ix}+e^{-ix}}$$ but as I have known that we cannot solve the general differential equation ,$y''(x)=ay^3(x)+by^2(x)+cy(x)+d$  (where a,b,c are constants) via only using  $e^{\beta x}$ . We need to define some non-elementery functions (elliptic functions) to find the solution. I thought to select an elliptic function to find a particular solution of general differential equation ,$$y''(x)=ay^3(x)+by^2(x)+cy(x)+d$$ by using the selected elliptic function and elementary function $e^{\beta x}$ (my aim to find a closed form with limited terms in combination of both functions or only with the selected elliptic function). Let's select an elliptic function and try to use it a base function for this aim. $$f''(x)=-2f^3(x)$$
$f(0)=0$ $f'(0)=1$ $$(f'(x))^2=1-f^4(x)$$
The defined base function can be expressed
$$x= \int_{0}^{f(x)} \frac{du}{\sqrt{1-u^4}}$$ Euler found an addition formula for this selected $f(x)$. The formula at page 6 in article that was written by Jose Barrios $$f(x+y)=\frac{f(x)\sqrt{1-f^4(y)}+f(y)\sqrt{1-f^4(x)}}{1+f^2(x)f^2(y)}$$ or we can rewrite it as: $$f(x+y)=\frac{f(x)f'(y)+f(y)f'(x)}{1+f^2(x)f^2(y)}$$ Is it possible to find a particular solution of the general differential equation $y''=ay^3+by^2+cy+d$ by using the combination of the selected  elliptic function $f(x)$  and $e^{x}$ in a closed form ? Note:Maybe we need to select another base $f(x)$ that would be easier for this aim. I selected randomly. I need references and hints about this idea.
Thanks a lot for your helps EDIT:  15th August ,2016 I would like to  share my results: Let's assume that we have the equation: $$ y'=\frac{dy}{dx} = p (y^2 + my + n)$$ If we solve the differential equation: General solution of the differential equation can be written as $y=-\frac{m}{2}-\frac{\sqrt{m^2-4n}}{2}+\cfrac{\sqrt{m^2-4n}}{1+ke^{xp\sqrt{m^2-4n}}}$ where $k$ is a constant. If we derivative both side
$$ y'=\frac{dy}{dx} = p (y^2 + my + n)$$
$$ y''=\frac{d^2y}{dx^2} = p (2y + m)y'$$
$$ y''= p^2 (2y + m)(y^2 + my + n)$$ $$ y''= 2p^2y^3+3p^2my^2+p^2(m^2+2n)y+p^2mn$$ If we compare with our general equation , We can find a condition that it has solution via $e^{\beta x}$. $$y''= ay^3+by^2+cy+d$$ The condition:
$$2b^3+27a^2d=9abc$$ If the general equation satisfy this condition , the general equation has a solution with $e^{\beta x}$ but if the condition is not satisfy , we should express the solution with elliptic functions. It is a known fact that elliptic functions cannot be expressed by $e^{\beta x}$ in closed form. $p=\sqrt{a/2}$ ; $m=\frac{2b}{3a}$;$n=\frac{3d}{b}$ $y_p(x)=-\frac{m}{2}-\frac{\sqrt{m^2-4n}}{2}+\cfrac{\sqrt{m^2-4n}}{1+ke^{xp\sqrt{m^2-4n}}}$
where $k$ is a constant. I have noticed that we can express the solvable condition as: $y_p=A\cfrac{e^{\alpha x} +B}{e^{\alpha x} +C}$ Let's transform the general diff equation $y=A\cfrac{u +B}{u +C}$ and to check a condition for u $$y'=A\cfrac{C-B}{(u +C)^2}u'$$ $$y''= ay^3+by^2+cy+d$$
If we integrate both side, we get: $$ \frac12 \left( \frac{dy}{dx} \right)^2 =  \frac{a}{4} y^4+\frac{b}{3} y^3+\frac{c}{2} y^2+d y + e$$  where $e$ is a constant. $$  \frac{A^2(C-B)^2}{2(u +C)^4} u'^2 =  \frac{a}{4}\frac{(u +B)^4}{(u +C)^4}+\frac{b}{3} \frac{(u +B)^3}{(u +C)^3}+\frac{c}{2} \frac{(u +B)^2}{(u +C)^2}+d \frac{(u +B)}{(u +C)} + e$$ $$ \frac{A^2(C-B)^2}{2} u'^2=  \frac{a}{4}(u +B)^4+\frac{b}{3} (u +B)^3(u +C)+\frac{c}{2} (u +B)^2(u +C)^2+d (u +B)(u +C)^3 + e(u +C)^4$$ I have been looking for a condition if we can transform it to one equation
such as $$ u'^2=  a_4u^4+a_3u^3+a_2u^2+a_1u+a_0$$ where $a_n$ is any selected constant. In my question , I selected to convert it to $$ u'^2=  s(1-u^4)$$ but I am not sure if we can transform the general equation into this type. Can anybody prove that it is possible or not to convert it to a selected form such as $ u'^2=  a_4u^4+a_3u^3+a_2u^2+a_1u+a_0$?
(where $a_n$ are selected constants.) EDIT:7th Sep 2016 $$ \frac{A^2(C-B)^2}{2} u'^2=  \frac{a}{4}(u +B)^4+\frac{b}{3} (u +B)^3(u +C)+\frac{c}{2} (u +B)^2(u +C)^2+d (u +B)(u +C)^3 + e(u +C)^4$$ If We select $e=-(\frac{a}{4}+\frac{b}{3}+\frac{c}{2}+d)$ then $u^4$ term is canceled so we can get 3 degree polynomial form. $$ \frac{A^2(C-B)^2}{2} u'^2= b_3 u^3+b_2u^2+b_1u+b_0$$ We can do $x=\alpha t$ then  we can get the form, $$  u'^2=  u^3+k_2(A,B,C)u^2+k_1(A,B,C)u+k_0(A,B,C)$$ We still have 3 parameters ($A,B,C$) to select . We can transform into a selected form $ u'^2=  u^3+a_2u^2+a_1u+a_0$. (where $a_n$ are  selected constants.) Can anybody help me to show if the transform to a selected form $ u'^2=  u^3+a_2u^2+a_1u+a_0$  is possible or not? 
Thanks a lot for helps","['ordinary-differential-equations', 'elliptic-integrals', 'elliptic-functions']"
1883236,"Weak net convergence in $\ell_p$, where $1 < p < \infty$.","EDIT: The question is edited after an error pointed out by gerw. There are the following two results regarding weak convergence in $\ell^p$ spaces: Let $((\beta_n^{(\alpha)}))_{\alpha \in I} \subseteq \ell_p (\mathbb{N})$ be a net and $(\beta_n) \in \ell_p (\mathbb{N})$ , where $1 < p < \infty$ . Then (i). $\beta_n^{(\alpha)} \to \beta_n$ for each $n \in \mathbb{N}$ whenever $(\beta_n^{(\alpha)})  \xrightarrow[]{w} (\beta_n)$ . (ii). $(\beta_n^{(\alpha)})  \xrightarrow[]{w} (\beta_n)$ whenever the net $((\beta_n^{(\alpha)}))_{\alpha \in I}$ is bounded and $\beta_n^{(\alpha)} \to \beta_n$ for each $n \in \mathbb{N}$ Unfortunately I am not able to prove both implications. Any help is highly appreciated.","['banach-spaces', 'nets', 'functional-analysis', 'weak-convergence', 'lp-spaces']"
1883251,How to understand immersed submanifold?,"I read this from Loring Tu's book: I am so confused what ""the topology and differentiable structure inherited from $f$"" really means. This one doesn't stuck me when I first went here. But in the following chapter of Lie groups, I run through this again: I wonder if there is a given smooth structure on $H$ itself. I can hardly understand ""an immersed submanifold via the inclusion map"". If here we can define ""immersion"", we must first give $H$ a smooth structure. I am so confused. Any help will be appreciated.","['differential-topology', 'smooth-manifolds', 'manifolds', 'differential-geometry', 'lie-groups']"
1883293,Sum of remainders of Triangular numbers,"Let $p=8k+7$ be a prime number, prove that: $$\sum_{k=1}^{p-1} \left\{ \frac{2T(k)}{p}\right\} = \sum_{k=1}^{p-1} \left\{ \frac{k}{p}\right\}$$ , where $T(k)$ is the $k-$th Triangular number and $\{\frac{k}{p}\}$ is the decimal part of $\frac{k}{p}$. It's fairly easy to notice that the RHS is equal to $\frac{p-1}{2}$, as all the fractions are less than 1, but I'm having troubles with the LHS. It's easy to notice that the terms are symmetric wrt the $\frac{p-1}{2}$ term, but that's all I found. I tried writing $2T(n) = n(n+1) = n^2 + n = (n+1)^2 - (n+1)$, but none of this helped me. Also it's easy to notice that: $\{\frac{i}{p}\} = \frac{x}{p}$, where $x$ is the remainder when $i$ is divided by $p$. I tried a lot of examples, but I can't see a correlation between these numbers. Also as the sum of the $x$'s is less than $p^2$ I tried working modulo $p^2$ and proving that their sum is equal to $\frac{p(p-1)}{2}$, but again to no avail. In fact I can't make use of the fact that $p$ is of the given form, as it seems that for primes other than those from the given form the identity doesn't hold.","['number-theory', 'fractions', 'elementary-number-theory']"
1883307,Is my own proof of the Bolzano-Weierstrass Theorem correct?,"I wonder my own proof of the Bolzano-Weierstrass theorem is correct or rigorous enough, and I also find a proof of the relevant theorem in Tao's Analysis weird. Here is my proof of the Bolzano-Weierstrass theorem. Let $\{a_n\}$ be a bounded sequence which is bounded in $[a,b]$. Let $D=\big\{x\in[a,b]~|~[a,x]$ contains infintely many terms of $\{a_n\}\big\}$, in which the predicate ""$[a,x]$ contains infintely many terms of $\{a_n\}$"" can be transfered into a more rigorous manner, ""$\big\{n|a_n\in [a,x]\big\}$ is infinite."" Hence $D=\Big\{x\in[a,b]\Big|\big\{n|a_n\in [a,x]\big\}\text{ is infinite}\Big\}$. Here we first clarify some important observation of $D$. First, is $D$ non-empty? Yes, because $b$ must in $D$. Second, does $D$ have the supremum and infimum? Yes, since $b\in D$ and $a$ is definitely a lower bound of $D$(but not neccessarily in $D$), by the Least Upper Bound property, $D$ has the supremum and infimum. Last, must $D$ have the least element in it? No, for example, if $\{a_n\}$ is defined strictly decreasing and converges to some point $x\in [a,b)$, then $D$ has no least element. Now we start the proof. Let $x=\inf D$. If $x=a$, the proof is trivial(the subsequence is $a,a,a,a,a,\dotsc$). If $x=b$, the proof is similar to the case $x\in(a,b)$, so I just prove the case $x\in(a,b)$. Because $x$ might not in $D$, so we can't directly conclude that $[a,x]$ contains infinitely many points of $\{a_n\}$. But let $\varepsilon >0$, then $[a,x-\varepsilon]$ must contains finite points of $\{a_n\}$, that is, $\big\{n|a_n\in [a,x-\varepsilon]\big\}$ is finite. By the Approximation Property for Infimum, there also exists a point $y\in D$ such that $x\leq y<x+\varepsilon$, hence $[a,y]$ has infinitely many points of $\{a_n\}$, that is, $\big\{n|a_n\in [a,y]\big\}$ is infinite. Now we claim that $[a,y]\setminus[a,x-\varepsilon]=(x-\varepsilon,y]$ has infinitely many points of $\{a_n\}$ in a rigorous manner, rather than an intuitive argument. Since $a_n\in [a,x-\varepsilon]\Rightarrow a_n\in [a,y]$, so $\big\{n|a_n\in [a,x-\varepsilon]\big\}\subseteq\big\{n|a_n\in [a,y]\big\}$. So for any $\varepsilon>0$, the difference of an infinite set and finite set, $\big\{a_n|a_n\in(x-\varepsilon,y]\big\}$, is infinite. We're now going to construct our subsequence. Let $\varepsilon>0$, we can pick an $a_N\in(x-\varepsilon,y]\subsetneq(x-\varepsilon,x+\varepsilon)$. Next, using $\varepsilon/2>0$, there must exists a $N'>N$ such that  $a_{N'}\in(x-\varepsilon/2,y]\subsetneq(x-\varepsilon/2,x+\varepsilon/2)$. Continuing on this way, we can construct a subsequence, and it is easy to check  that this subsequence converges to $x$, and we omit the detail. $\blacksquare$ Here are my questions. Is my proof correct? Is any part of this proof redundant? Can any part of this proof be simplified? In Terence Tao's Analysis 1, he stated a theorem(Prop. 6.6.6) that any sequence has a subsequence converges to its limit point. He then gave the hint of the proof in the Exercis 6.6.5. He defined a sequence $n_j=\min\{n|a_n\in[L-\frac{1}{j},L+\frac{1}{j}]\}$. Of course, by the Well-Ordering Principle of natural numbers, we know the $\{n_j\}$ is well-defined. However, I think $\{n_j\}$ is not guaranteed to be strictly increasing simply by letting the value of $n_j$ to be the minimum index of $\{a_n\}$ that falled in the interval. (Because, for example, when $j=10$, $a_{100}$ is the mininum-index term in $[L-\frac{1}{10},L+\frac{1}{10}]$, however, when $j=11$, $a_{100}$ is still in $[L-\frac{1}{11},L+\frac{1}{11}]$, so it will be chosen duplicately again.) And we know that in order to make $\{a_{n_j}\}$ to be a so-called ""subsequence"", $\{n_j\}$ must be strictly increasing. So we can't pick a subsequence like this. Am I correct? -------------UPDATED--------------- I made mistakes on discussing the endpoints. Edited like this: if $\inf D=b$, since $b\in D$, then $\inf D\in D$. For any $\varepsilon>0$, $[a,b-\varepsilon]$ has finite points of $\{a_n\}$, hence $[a,b]\setminus[a,b-\varepsilon]=(b-\varepsilon,b]$ has infinitely many point of $\{a_n\}$. So it's easy to construct a subsequence. If $\inf D=a$, and if $\inf D=a\in D$, then $[a,a]$ has infinitely many points of $\{a_n\}$, so the subsequence is trivially $a,a,a,a,a,\dotsc$. Otherwise if $\inf D=a\not\in D$, then $[a,a]$ has only finite points of $\{a_n\}$. For any $\varepsilon>0$, by the Approximation Property for Infimum, there exists $y\in D$ such that $a\leq y<a+\varepsilon$. So $[a,y]$ has infinitely many point of $\{a_n\}$. Therefore, $[a,y)\setminus[a,a]=(a,y]\subsetneq(a,a+\varepsilon)$ has infinitely many point of $\{a_n\}$, then we can construct the subsequence. The proof became a bit lengthy. However, I just now came up with a brilliant idea! As my proof stated in the beginning, I suppose the sequence to be in $[a,b]$. Now, we redefine $D$ to be $\mathfrak{D}=\big\{x\in[a-1000,b+1000]~|~[a-1000,x]$ contains infintely many terms of $\{a_n\}\big\}$. Then $\inf\mathfrak{D}$ must not be the endpoint of this new and bigger interval, so we need not to discuss the case when $\inf\mathfrak{D}=a-1000$ or $\inf\mathfrak{D}=b+1000$ anymore!","['general-topology', 'real-analysis', 'analysis']"
1883345,Find the point that minimises the sum of the angles from three points to that point on the surface of a sphere,"Let $A,B,C$ be three points on the surface of a unit sphere with centre $O$, such that they are not all three on the same maximal circle.
There is a plane $P$ that contains all three points and on it they define a triangle.
Let $X$ be a point (to be determined) on the surface of the sphere and let $XA$ be the angle between the line segments $OA$ and $OX$ and similarly for $XB$ and $XC$. Q1: Find the point $X$ on the surface of the sphere such that the sum of the three angles $XA+XB+XC$ is minimal. Q2: Is the problem of Q1 equivalent to finding a point on the plane $P$ that minimises the sum of the (Euclidean) distances from the vertices to that point? (Eg if that point on the plane is found, call it $Y$, then one can extend the line segment from $O$ to $Y$ until the surface of the sphere is intersected) If one can show that Q2 is equivalent to Q1 then the point of minimal sum can be determined by first finding the Fermat point on the plane. Thanks in advance!","['triangles', 'spheres', 'optimization', 'geometry']"
1883382,Why is $r^2 + 2rs + s^2$ composite for all positive integer $r$ and $s$?,"Why is $r^2 + 2rs + s^2$ composite for all positive integer $r$ and $s$? By definition, A integer $n > 1$ is composite if and only if there exist positive integer, $r$ and $s$, so that $n = r \times s$ and $r$ does not equal $1$ and $s$ does not equal $1$. So from the question above, it says for all positive integer $r$ and $s$. All positive integers includes $1$, however, by definition, it says $r$ and $s$ must not be equal to $1$. Therefore, I conclude they are not composite. Unfortunately, the answer said that they are composite. How is this so and how do I prove that they are composite even it says ""for all positive integer $r$ and $s$""?","['integers', 'discrete-mathematics']"
1883385,what is the meaning of 2 in group SO(2)?,"I am a beginner in group theory. Now reading a note of Lie group. I am confusing about the dimension of the group and the notation. For example, from my understanding, the dimension of SO(2) is 1 because only one parameter (rotation angle) is used to parameterise the group. Then what is the meaning of 2 in the notation of SO(2)? And In the case of SU(2), we have three parameters, correspond to three generators if I understand correctly, then why it is called SU(2)?","['notation', 'group-theory']"
1883424,"Semisimple, connected Lie groups generated by unipotent elements.","Let $G$ be a linear, semisimple Lie group with no compact factors.
The unipotent elements of $G$ are those that have only eigenvalue 1. I've seen it asserted that $G$ is generated by its unipotent elements: see Exercise #2 $\S 4.5$ in Dave Witte Morris' book on Arithmetic Groups .
The hint in the book is that you consider the simple factors.
But even considering $SL(2, \mathbb{R})$, it is unclear to me why it is generated by its unipotent elements. I am aware that any matrix in $G$ can be written as a product of commuting hyperbolic, elliptic, and unipotent element, but I am unsure of how you might generally express the hyperbolic and elliptic elements as unipotent elements.","['matrices', 'lie-groups']"
1883434,"Find all maximal ideals of $\mathbb{R}[x,y]/(x^2+y^2)$","How to find all maximal ideals of $\mathbb{R}[x,y]/(x^2+y^2)$? I have no idea because $\mathbb{R}$ is not algebraically closed. Is there any good way?","['abstract-algebra', 'ring-theory', 'maximal-and-prime-ideals', 'ideals']"
1883436,"Is $C([0,1])$ strictly convex?","Is $C([0,1])$ equipped with the supremum-norm strictly convex? With strict convexity given by: if $f\neq g$ and $\Vert f\Vert=1=\Vert g\Vert$ then $\Vert f+g\Vert<2$. I guess it is not, but how can I prove it?","['functional-analysis', 'convex-analysis']"
1883449,"When randomly selecting n unique edges from a complete graph, what is the probability distribution for the numbers of covered vertices?","Assume that we have a complete graph $G = \{V,E\}$, where $V$ is the vertices set and $E$ is the edges set. We randomly select $n$ unique edges from $G$ ($n \le |E|$), and these edges cover $m$ vertices. The problem is: what is the probability distribution for $m$? ========================================= I read a helpful post: Given a random labelled simple graph with n edges, when is it more likely to get a graph with more edges than vertices? ========================================== Now I am trying to transform this problem to picking random element pairs from a set, and I find this post could be helpful since it asked a very similar question. https://stackoverflow.com/questions/15793172/efficiently-generating-unique-pairs-of-integers Could anyone please help me?","['graph-theory', 'probability-theory', 'probability-distributions', 'probability', 'combinatorics']"
1883479,"Find all prime numbers $p$, for which $2x^{p-1} + 2009= y ^{p-1}$ has an infinite number of solutions in non-negative integers?","Find all prime numbers $p$, for which $$2x^{p-1} + 2009= y ^{p-1}$$ has an infinte number of solutions in non-negative integers?
This is a contest problem. Any ideas? EDIT:
Here is the progress i made. Help would be welcomed.
Let's rewrite the equation as $$2x^{p-1} + 2009-y ^{p-1}=0$$ From Fermat's theorem we have that $0\equiv 2x^{p-1} + 2009-y ^{p-1}\equiv 2008,\ 2009$ ,$2010$ or $2011\pmod p$ So obviously $p$ can only be a prime divisor of  $2008,\ 2009$ ,$2010$ or $2011$, $=>p=2, 3, 5, 7, 41,67, 251$ or $2011$ $p=2$ we get $2x+2009-y=0$ which has infinite number of solutions. $p=5\ => 2x^4+2009=y^4$. $y^4\equiv {0,1,15} \pmod {16},\  2x^4+2009\equiv{8,9,10} \pmod {16} =>$ no solutions. $p=7\ => 2x^6+7.7.41=y^6$  $=> 2x^6-y^6\equiv 0\pmod 7$. From Fermat's theorem $2x^6\equiv 0$ or $2 \pmod 7$, $y^6 \equiv 1$ or $0 \pmod 7$
=>$x=7k\ ,\  y=7l $  => $2009\equiv 0 \pmod {7^6}$ which is false => no possible solutions for $p=7$. The case for $p=41$ is solved analogously to $p=7$. I have no ideas for $p=3, 5 ,67, 251$ or $ 2011$ yet.","['number-theory', 'elementary-number-theory', 'contest-math', 'prime-numbers', 'exponentiation']"
1883490,What is the error for the estimate of the probability of a binomial distribution?,"I want to estimate the probability $p$ of a Binomial distribution $B(n,p)$. I draw $n$ samples and get $s$ successes. The estimate for $p$ is $$\hat{p} = s/n.$$ What is the variance of $\hat{p}$?","['statistics', 'binomial-distribution', 'parameter-estimation']"
1883508,differential-difference equation,"In my work I've arrived at the equation: $x y'(x)+ay(x)+b y(cx-1)=0$, where $a,b,c$ are given constants ($c\ge1$). I suspect this equation has been studied before. Can anyone suggest a reference? In the special case of $c=1$, the equation reduces to $x y'(x)+ay(x)+b y(x-1)=0$ and this equation has already been studied extensively in the literature, especially on number theory.","['recurrence-relations', 'real-analysis', 'ordinary-differential-equations', 'delay-differential-equations']"
1883518,If $A\subseteq C$ then $AB\cap C=A(B\cap C)$?,"$A,B,C$ can be anything (e.g. I proved it for Galois extensions of $\Bbb Q$).  Can someone find a proof or a counterexample (for any class of $A,B,C$, e.g. groups, rings) that if $A\subseteq C$ then $AB\cap C\subseteq A(B\cap C)$?  (since the $\supseteq$ is obvious). My proof is really really long, by induction on the power of prime factors dividing $[B:B\cap C]$; however, I'm pretty sure I've seen an elementary proof back in high school (which obviously wasn't in the context of Galois extensions). EDIT: AB is the compositum of A and B.","['algebraic-number-theory', 'group-theory', 'elementary-set-theory']"
1883532,A horrid-looking integral $\int_{0}^{5} \frac{\pi(1+\frac{1}{2+\sqrt{x}} )}{\sqrt{10}\sqrt{\sqrt{x}+x}} $,"$$
\mathbf{\mbox{Evaluate:}}\qquad
\int_{0}^{5}  \frac{\pi(1+\frac{1}{2\sqrt{x}} )}{\sqrt{10}\sqrt{\sqrt{x}+x}} 
\,\,\mathrm{d}x
$$ This is a very ugly integral, but appears to have a very simple closed form of: $$\Gamma(\frac15)\Gamma(\frac45)$$ Mathematica can evaluate this integral, but WolframAlpha doesn't even give a correct numerical answer. I have tried many techniques on this integral but have not been able to crack it at all. Any help on this integral would be greatly appreciated. Thank you!","['integration', 'definite-integrals', 'calculus']"
1883542,An application of Hall's marriage theorem.,"I am reading the Wikipedia article entitled Hall's marriage theorem.  It states we can use the theorem to prove the following: ""Take a standard deck of cards, and deal them out into 13 piles of 4 cards each. Then, using the marriage theorem, we can show that it is always possible to select exactly 1 card from each pile, such that the 13 selected cards contain exactly one card of each rank (Ace, 2, 3, ..., Queen, King)."" The article doesn't say HOW Hall's theorem can be used to prove this. I think we could create a bipartite graph with one partite set consisting of each of the 13 ranks and the other partite set consisting of the 13 piles. An edge joins a rank to a pile if that rank is in the pile.  Each rank would be joined to at least one pile and at most 4 piles. 
This is as far as I have gotten.","['combinatorics', 'graph-theory', 'probability']"
1883564,How to show that $\chi_R(x) \nabla \frac 1{|x|} \in L^1(\mathbb R^3) \cap L^{3/2}(\mathbb R^3)$,"Let $\chi_R(x)$ be a smooth function such that $\chi_R(x)=1$ if $|x| \le R$, $\chi_R(x) = 0$ if $|x| \ge 2R$, and $0 \le \chi_R(x) \le 1$ for all $x \in \mathbb R^3$. How can we show that
 $\chi_R(x) \nabla \frac 1{|x|} \in L^1(\mathbb R^3) \cap L^{3/2}(\mathbb R^3)$? In particular, I'm stuck at the $\chi_R(x) \nabla \frac 1{|x|} \in L^{3/2}(\mathbb R^3)$ part, I got
$$\int_{\mathbb R^n} \left|\chi_R(x) \nabla \frac 1{|x|}\right|^{3/2} dx \le \int_{B(0,2R)} \left|\nabla \frac 1{|x|}\right|^{3/2} dx = \int_{B(0,2R)}\frac 1{|x|^3} dx =\infty$$ so I got $\infty$ but obviously I'm trying to get less than $\infty$. I think my inequality might be too crude. Edit: This is more precise and seems to me more plausible, but I'm still not sure if this answers my question:
$$\int_{\mathbb R^n} \left|\chi_R(x) \nabla \frac 1{|x|}\right|^{3/2} dx \color{red}{<} \int_{B(0,2R)} \left|\nabla \frac 1{|x|}\right|^{3/2} dx = \int_{B(0,2R)}\frac 1{|x|^3} dx =\infty$$
(I figured it was a strict inequality because $\chi_R(x) < 1$ as $|x|$ is close enough to $2R$. Wishful thinking, though: the first integral is still equal to $\infty$.)","['functional-analysis', 'normed-spaces', 'integration', 'lebesgue-integral']"
1883628,Tessellating the sphere,"It is a famous result that the plane can be tessellated by regular triangles, squares, and hexagons. Which regular polygons can tessellate the sphere?",['geometry']
1883629,tensor product of *-algebra as a *-algebra: well-definedness of involution,"I have a question regarding the proof of the following proposition: Proposition: Let $A$ and $B$ two $\mathbb{K}$-algebras with involution. Let $A\otimes B$ be the algebraic tensor product of $A$ and $B$. Then $A\otimes B$ is a involutive $\mathbb{K}$-algebra with $$(a\otimes b)(c\otimes d)=ac\otimes bd,$$
$$(a\otimes b)^*=a^*\otimes b^*.$$
Proof (only the passage where I'm stuck): 
I'm stuck with the well-definedness of $$\sum_i a_i\otimes b_i\mapsto \sum_i a_i^*\otimes b_i^*:$$
 1. Why one has to check for well-definedness the following: if $\sum_i a_i\otimes b_i=0 \Rightarrow \sum_i a_i^*\otimes b_i^*=0$ ? 2.how to prove that/ Is my solution for 2. (see below) correct? My idea for 2. is to use the fact if $\{x_1,..,x_n\}\subseteq A$ is linearly independent and $\{y_1,...,y_n\}\subseteq B$ is arbitrary and such that $$\sum_{i=1}^nx_i\otimes y_i=0,$$ then $y_1=...y_n=0$.
Now, let $\{x_k\}_{k\in I}$ be a basis for $A$ and $a_i=\sum_{k\in I}\lambda_{i,k}x_k$ (this sum is finite). Furthermore let $\sum_i a_i\otimes b_i=0$ Then we have $$0=\sum_i a_i\otimes b_i=\sum_i \sum_{k\in I}\lambda_{i,k}x_k \otimes b_i=\sum_{k\in I} x_k\otimes (\sum_i \lambda_{i,k}b_i)$$
i.e. (with the fact above) $\sum_i \lambda_{i,k}b_i=0$ for all $k\in I$.
It follows $$\sum_i a_i^*\otimes b_i^*=\sum_i \sum_{k\in I}\overline{\lambda_{i,k}}x_k^* \otimes b_i^*=\sum_{k\in I} x_k^*\otimes (\sum_i \overline{\lambda_{i,k}}b_i^*)=\sum_{k\in I} x_k^*\otimes (\sum_i \lambda_{i,k}b_i)^*=0.$$ But I have no idea why 1. proves well-definedness of the involution.","['functional-analysis', 'operator-algebras', 'involutions']"
1883634,Proofs for complete + totally bounded $\implies$ compact.,"The proof one usually sees for this (by my knowledge) is by proving that the metric space in question is sequentially compact, as seen here: totally bounded, complete $\implies$ compact I am interested in a more direct proof involving the open covering definition of compactness. Using the equivalence of countable compactness and limit point compactness (every infinite subset admits a limit point) for $T_1$-spaces as a starting point, I came up with the following proof, but it still feels a bit indirect. Is anyone aware of a different and/or more direct proof? Thank you! Here is my proof: Assume $X$ to be a complete and totally bounded metric space. To prove that it is compact, we need to show that it is both countably compact and Lindelöf and both of these properties are equivalent to limit point compactness and separability, respectively, so we prove those two properties. To see that $X$ is separable, note that for each $n\in\mathbb{N}$, there exists a finite $(1/n)$-net $\{x_{n,1},\dotsc,x_{n,r_n}\}$ for $X$. Then the countable set
\begin{equation}
S=\bigcup_{n=1}^{\infty}\{x_{n,1},\dotsc,x_{n,r_n}\}
\end{equation}
is dense in $X$, which proves that $X$ is separable. To prove that $X$ is limit point compact, let $F\subset X$ be an infinite subset. Then as $X$ is covered by finitely many balls of radius $1$, it follows that one such ball contains infinitely many members of $F$, i.e. $F\cap B_{1}(x_1)$ is infinite for some $x_1\in X$. Similarly, as $F\cap B_{1}(x_1)$ is infinite and $X$ is covered by finitely many balls of radius $1/2$, we have that $F\cap B_{1}(x_1)\cap B_{1/2}(x_2)$ is infinite for some $x_2\in X$. Continuing inductively, we obtain a sequence $\{x_n\}_{n=1}^{\infty}$ in $X$ such that
\begin{equation}
E_n:=F\cap B_{1}(x_1)\cap\dotsb\cap B_{1/n}(x_n)
\end{equation}
is infinite for every $n\in\mathbb{N}$. I claim that $\{x_n\}_{n=1}^{\infty}$ is Cauchy. If $y_n\in E_n$ is arbitrary for $n\in\mathbb{N}$ then $\{y_n\}_{n=1}^{\infty}$ is Cauchy since the $E_n$ are nested and $\mathrm{diam}(E_n)\leq 2/n$, which in turn implies that $\{x_n\}_{n=1}^{\infty}$ is Cauchy as $d(x_n,y_n)<1/n$ for all $n$. Thus $\lim_{n\to\infty}x_n=x$ for some $x\in X$ by completeness. It remains to show that $x$ is a limit point of $F$, for which it is sufficient to show that $F\cap B_{\varepsilon}(x)$ is infinite for all $\varepsilon>0$. To see this, let $\varepsilon>0$. Choose $n\in\mathbb{N}$ such that $1/n<\varepsilon/2$ and $d(x_n,x)<\varepsilon/2$. Then $B_{1/n}(x_n)\subset B_{\varepsilon/2}(x_n)\subset B_{\varepsilon}(x)$, so we obtain
\begin{equation}
E_n\subset F\cap B_{1/n}(x_n)\subset F\cap B_{\varepsilon}(x).
\end{equation}
Hence $F\cap B_{\varepsilon}(x)$ is infinite as $E_n$ is, which completes the proof.","['general-topology', 'metric-spaces']"
1883655,Quaternions Group Homomorphic to Klein Group,"I'm currently studying some stuff about group theory and I came to problem of showing that $$\displaystyle\frac{Q_8}{\langle-1\rangle}\cong V_4,$$ so I checked on this link: Quaternions Group and Klein Group , which seems to clarify somehow what I wanted to know. But now I'm curious about how to prove the statement using the first isomorphism theorem. Here is what I have: Setting the map $\varphi:Q_8\to V_4$ such that $\varphi(a)=|a|$ we find that $\varphi$ is a morphism of groups with $Ker(\varphi)=\langle -1\rangle$, so applying the theorem the statement holds. My question is about the legality of $\varphi$, I think it's ok but I'd like another opinion, approach, thought about it.","['abstract-algebra', 'quaternions', 'group-homomorphism']"
1883658,"$f$ Lebesgue integrable, showing $f$ is constant almost everywhere","$f\in L^1([a,b])$ satisfies $\lim_{h\rightarrow 0}\frac{1}{h}\int_{a}^{h}|f(t+h)-f(t)|dt = 0$ then $f$ is constant almost everywhere. I tried using the mean value theorem, to somehow get that $f'=0$ almost everywhere, but can't seem to prove it is.","['real-analysis', 'lebesgue-integral', 'measure-theory']"
1883659,Relationship between dimension of a manifold M and dimension of the Lie group G = Sym(M),"What is the relationship between the dimension of the manifold of a lie group and the dimension of the manifold the lie group describes the symmetries of? For example, the dimension of the sphere, S^2, is 2. And the dimension of the Lie group, SO(3), that describes the symmetries of the sphere, is 3. Is there a general relationship between the dimension of the two manifolds, i.e. that of the manifold of a Lie group and that of the manifold which the Lie group 'acts on'? Thanks.","['riemannian-geometry', 'differential-geometry', 'lie-groups']"
1883663,Name of technique for determining the number of eigenvalues larger than some limit,"Suppose $A$ is a real symmetric positive definite matrix with eigenvalues $\lambda_1,...,\lambda_n > 0$ (which we do not know). If one wants to know how many eigenvalues $A$ has above some limit $s \neq \lambda_i$, one can study the quadratic form defined by the matrix $(A-sI_n)$, diagonalise it to find the number of positive eigenvalues and use Sylvester's law of inertia. For example, if $(A-sI_n)$ has one positive eigenvalue, then $A$ has exactly one eigenvalue larger than $s$. That is, the technique allows you to find the number of eigenvalues above some limit without actually computing the eigenvalues, and in order to work, the limit itself must not be an eigenvalue. The literal translation of the name of this technique in my native language would be ""spectral cleaving"" or ""spectral splitting"". However, this doesn't seem to be the correct terminology in English. Any suggestions?","['matrices', 'reference-request', 'terminology', 'linear-algebra', 'analysis']"
1883693,How to define the inverse of a vector?,"Most physical situations in mechanics can be modeled using a combination of derivatives - specifically, derivatives of position: velocity and acceleration. But physical situations can also be modeled other ways. Consider the scalar equation for velocity in one dimension: $v = \frac{dx}{dt}$ it can be modeled just as well by a different quantity, called ""slowness"" which is described as: $s = \frac{dt}{dx} = \frac{1}{v}$ Which is used commonly in day-to-day life. For example, runners usually measure distance in minutes per mile, or minutes per km. Walkers go slow enough such that it doesn't make sense to measure speed in miles/hour or a similar unit, but instead to say they take ~20mins per mile. However, this is rarely, if ever, used in physics. In order to model slowness physically, there are a few basic things to know. Firstly, when we want to add velocities, they add quite nicely. If I stand on top of a flat bed truck and run at 5m/s while the truck is going 10m/s (disregarding special relativity) my total speed is 5+10=15m/s. For a slowness, we have to take advantage of knowing this fact to figure out how to ""add"" slownesses together. We know $v_1+v_2=v_t$ , so if $s_1=\frac{1}{v_1}$ and $s_2=\frac{1}{v_2}$ and $s_t=\frac{1}{v_t}$ then $\frac{1}{s_1}+\frac{1}{s_2}=\frac{1}{s_t}$ and therefore: $\frac{1}{\frac{1}{s_1}+\frac{1}{s_2}}=s_t$ defining an operation associated with this (called ""oplus"" - discussed in detail here ): $x \oplus y := \frac{1}{\frac{1}{x}+\frac{1}{y}}$ and its inverse (""ominus"") $x \ominus y := \frac{1}{\frac{1}{x}-\frac{1}{y}} = x \oplus (-y)$ we have $s_1 \oplus s_2 = s_t$ , so while velocities add, slownesses oplus. It seems slowness is not easily measured in vector form, even though it represents the same physical quantity as velocity and therefore has both magnitude and direction. The vector version of slowness should (I think) fulfill three requirements: Preserve direction (point same direction as velocity vector) Invert magnitude (magnitude of slowness should be 1/speed) Coordinate independence (slowness in the x-direction doesn't effect y-direction, etc) There is only one vector which satisfies the first two is the vector $\frac{\vec{v}}{|\vec{v}|^2}$ which unfortunately doesn't also satisfy the third requirement, because the magnitude of $\vec{v}$ is affected by all coordinates of $\vec{v}$ . For example, if $\vec{v}=\langle1,2\rangle$ then $\frac{\vec{v}}{|\vec{v}|^2}=\langle\frac{1}{\sqrt{5}},\frac{2}{\sqrt{5}}\rangle$ but if $\vec{v}=\langle1,3\rangle$ then $\frac{\vec{v}}{|\vec{v}|^2}=\langle\frac{1}{\sqrt{10}},\frac{3}{\sqrt{10}}\rangle$ which means just changing the y coordinate also changed the x coordinate of the slowness. In order to preserve coordinate independence, as well as stay consistent with the one-dimensional definition of slowness, one can define a ""vector"" for slowness as taking the reciprocal of each component. So if $\vec{v}=\langle x,y,z\rangle$ then $\vec{s}=\langle\frac{1}{x},\frac{1}{y},\frac{1}{z}\rangle$ . The initial problem is that it appears to not preserve direction or invert the magnitude of the velocity vector. However it requires changing a fundamental vector property. It comes down to how we measure distance in a coordinate system, and the operations we use on vectors. We all know that vectors add together, which makes sense since velocity and position do the same, and those things add when they are scalars. One problem with defining slowness as a vector may be that slowness does not satisfy vector properties, even in one dimension! Slownesses do not add, they oplus (as shown above). So instead of defining slowness, which is the reciprocal of slowness, as a vector, why not define it as something else? Something which is like a vector, but more readily taking advantage of its properties. For example, it could be different in how we measure its magnitude, as well as other properties: Given $\vec{v}=\langle x,y,z \rangle$ $\frac{1}{\vec{v}} := [x^{-1},y^{-1},z^{-1}]$ <-- not a vector, instead could be called an ""inverse vector"" or ""invector"", denoted by square brackets $|\frac{1}{\vec{v}}| := \sqrt{(\frac{1}{x})^2\oplus(\frac{1}{y})^2\oplus(\frac{1}{z})^2}$ This appears to behave quite nicely, since $|\frac{1}{\vec{v}}|=\sqrt{(\frac{1}{x})^2\oplus(\frac{1}{y})^2\oplus(\frac{1}{z})^2} = \sqrt{\frac{1}{x^2+y^2+z^2}} = \frac{1}{\sqrt{x^2+y^2+z^2}} = \frac{1}{|\vec{v}|}$ Which satisfies requirement #2. Through this definition, requirement #1 can also be satisfied, provided we change how we measure distance. Most people are familiar with a graph on a logarithmic scale. This helps visualize data which grows exponentially by changing where the numbers are located geometrically on an axis. In our new idea of distance, we will work with reciprocal space (I am aware that term is used to describe a crystal lattice, but I am not using the same thing here). BOTH the x and y axes (we'll start working in 2 dimensions) will be re-scaled such that x=1/x and y=1/y. The origin will be replaced with a single ""point at infinity"" similar to projective geometry. Graphing reciprocal vectors in a space which is measured this way satisfies requirement #1 - preserving direction. So all three requirements are satisfied with this new definition, provided we say that this isn't a vector, and it lives in a different space. The vector $\langle 2,3 \rangle$ (shown right) when graphed in Cartesian space looks the same as the vector $\langle \frac{1}{2},\frac{1}{3} \rangle$ (shown left) graphed in reciprocal space. An amazing thing about this space is that the geometric ""tip to tail"" addition of vectors applies to this new space as well, except it corresponds to oplussing of invectors instead of addition of vectors. And, assuming this represents a slowness, it is entirely consistent with vector addition of velocity! We define  (invectors will be denoted with a *) given $\vec{a}* = [a_1,a_2]$ and $\vec{b}* = [b_1,b_2]$ $\vec{a}* \oplus  \vec{b}* := [a_1 \oplus b_1, a_2 \oplus b_2]$ For instance: $\vec{v_1} = \langle x_1,y_1 \rangle$ and $\vec{v_2} = \langle x_2,y_2 \rangle$ $\vec{v_1}+\vec{v_2} = \vec{v_t} = \langle x_1+x_2,y_1+y_2 \rangle$ as a slowness invector, that would be $\vec{s_1}* = [\frac{1}{x_1},\frac{1}{y_1}] $ and $\vec{s_2}* = [\frac{1}{x_2},\frac{1}{y_2}]$ $\vec{s_1} \oplus \vec{s_2} = \vec{s_t} = [\frac{1}{x_1+x_2},\frac{1}{y_1+y_2}]$ which is consistent because $\frac{1}{[\frac{1}{x_1+x_2},\frac{1}{y_1+y_2}]} = \langle x_1+x_2,y_1+y_2 \rangle = \frac{1}{\vec{v_t}}$ Using the fact that vector addition is the same as invector oplussing, it is possible to prove that since lengths in reciprocal space are 1 divided by their geometric lengths, there is a new Pythagorean theorem for reciprocal space. $c^2=a^2 \oplus b^2$ because we know that $\frac{1}{length(x)}=x$ in reciprocal space (where length(x) is the length you measure normally, if you took out a ruler, for example and x is the ""actual length"") and we know that $length(c)^2 = length(a)^2+length(b)^2$ so $\frac{1}{c^2} =\frac{1}{a^2}+\frac{1}{b^2}$ and $c^2 =\frac{1}{\frac{1}{a^2}+\frac{1}{b^2}} = a^2 \oplus b^2$ which explains the equation above for magnitude of an invector. We can also see that if we use the inverse operation of $\oplus$ , $\ominus$ (o-minus) we can define a linear distance function along the axes in one dimension. We can call this function ""closeness"" because it is how close one object is to another. A small closeness is a large distance and a large closeness is a small distance (because they are reciprocals). $closeness(x,y) = c(x,y) := |x \ominus y|$ and for two dimensions $c((x_1,y_1),(x_2,y_2)) := \sqrt{(x_1 \ominus x_2)^2 \oplus (y_1 \ominus y_2)^2}$ The three dimensional formula is similar. We can see that under this closeness (distance) formula in 1 dimension, the distance between the reciprocal of any integer and the reciprocal of the next is 1. The distance between 1 and 1/2 is $|1\ominus\frac{1}{2}|=1$ . The distance between 1/2 and 1/3 is 1, the distance between 1/3 and 1/4 is 1, and so on. The distance function here is translation-invariant - if we move the axes the lengths of lines do not change. I have skirted along without mentioning the fact that the identity under the oplus operation is $\infty$ . I've found that $\infty$ is an integral part of this system. It effectively works just like zero in the Cartesian system. $a\oplus\infty=a\ominus\infty=a$ for all a and in general, $\infty = -\infty$ . As far as I can tell, this makes sense physically - a body with $0$ velocity has $\infty$ slowness. A body which has $0$ distance between it and something else has $\infty$ closeness to it. The invector of acceleration can be found to have physical meaning as well, and to work perfectly well in reciprocal space. I have not included everything which I have found about this system, including the dot product of two invectors $(\vec{a}*\cdot\vec{b}*=[a_1\cdot b_1,a_2\cdot b_2])$ and how they relate to derivatives describing motion in a reciprocal way. As an amateur with only high school level math training, I'd simply like to ask, does this make any sense? Is anybody aware of a vector way to describe slowness or other inverse vector quantities which is different (or the same) from my own work? I'd like to understand how this relates to mathematics in general, and if my ideas and work are valid. A vector is generally defined as a mathematical object with both magnitude and direction, but it seems to me that even though that suits this type of idea, a vector is unable to describe the type of object I'm dealing with here. Is there another way to do this that is already accepted by the math community? Is my work new or does this exist somewhere? Is defining the inverse of a vector even possible? If vectors are independent of coordinate system, why does changing to reciprocal space change anything? Basically, I would just like to know more details about how to define the inverse of a vector, in a mathematical sense and a physical sense.","['reference-request', 'physics', 'calculus', 'algebra-precalculus', 'vectors']"
1883701,Interpretation of stopping time sigma algebra (as explained in Durret),"I'm reading Rick Durret's ""Probability Theory and examples"". The book is available online here [ https://services.math.duke.edu/~rtd/PTE/PTE4_1.pdf ]. Please refer to the definition cum interpretation of the sigma algebra associated with a stopping time which appears in the book  following Example 4.1.4 on page 156. I understand the mathematical definition of $F_N$, but I wish to know the interpretation of this. I don't quite understand Durret's explanation. Thanks!",['probability-theory']
1883707,Integrate squared sum,"This is a mathematical question, but with a statistical flavour. A little background:
I have $N$ IID observations denoted by $w[n]$, from a uniform distribution $\mathcal{U}[0,\beta]$. Now, I know an unbiased estimator of $\beta$: $$\hat{\beta} = \frac{2}{N}\sum_{n=0}^{N-1}w[n]$$ and the variance of a uniform distribution is $\frac{\beta}{12}$ so a method to compute an estimator of $E[\hat{\sigma^2}]$ is to do this: $$E[\hat{\beta^2}] = E[\hat{\beta}]^2 + \operatorname{Var}(w[n])$$ It's simpler because I know the variance of the sum is the sum of the variances. But I'm trying to prove it using the expectation integral, and then I must solve: $$\frac{1}{12}E[\hat{\beta^2}]=\frac{2}{12N^2\beta} \int_0^\beta \left(\sum_{n=0}^{N-1}w[n]\right)^2 \, dw$$ where $\beta$ is a constant. How does one integrate a squared sum like this? The substitution method didn't seem to work very well. I tried to set $u=\sum_{n=0}^{N-1}w[n]$ such that $du=N$, and I integrate, and get the anti-integral $\frac{1}{3}\Big(\sum_{n=0}^{N-1}w[n]\Big)^3$. This gives a solution: $E[\hat{\beta^2}]=\frac{2N\beta^2}{36}$, but the answer is supposed to be $$E[\hat{\beta^2}]=\frac{1}{12}\Big(\beta^2+\frac{\beta^2}{36N}\Big)$$ Has anyone solved this before?","['statistics', 'calculus']"
1883719,Correction factor for Hyperbolic Curve,"I have generated several data sets under varying experimental conditions, that are plotted as hyperbolic curves. I have two experiments that were done under identical conditions, but the curve is not the same. I'll call experiment A the ""ideal"". The equation for this line is: y=(435.6*S)/(0.333*S). In experiment B, I would expect the same result but instead the equation I get is: y=(390.1S)/(0.3176+S) I'd like to generate a correction factor to shift equation B to match equation A, and then apply that correction factor to other data sets within experiment B. Is this possible? How would I go about finding the correction factor?","['statistics', 'curves']"
1883733,What is the expected number of red apples left when all the green apples are picked?,We have 4 green apples and 60 red apples. Each time we pick one out without replacement. Then what is the expected number of red apples left when all 4 green apples are picked?,"['probability', 'discrete-mathematics']"
1883735,"How to show $\int_{E} x \, d\mu \geq \mu(E)^2 / 2$","Let $E \subseteq [0,1]$ , $\mu$ the Lebesgue measure. I would like to show that $\int_{E} x \, d\mu \geq \frac{1}{2} \mu(E)^2$ . Lemma: $$ \int_{0}^{\mu(E)} x \, d\mu \leq \int_{E} x \, d\mu $$ This lemma seems pretty reasonable, in fact, I would expect it to hold for any monotone function $f : [0, 1] \to \mathbb{R}$ : $$ \int_{0}^{\mu(E)} f \, d\mu \leq \int_{E} f \, d\mu $$ But I'm not sure how to prove it rigorously. If the lemma holds, the result follow as $\int_{0}^{\mu(E)} x \, d\mu = \frac{1}{2} \mu(E)^2$ . Idea of a proof: \begin{align*}
\int_{0}^{1} 1_{E} f \, d\mu
&= \left[ f(x) \int_{0}^{x} 1_{E} \, d\mu \right]_{0}^{1}
- \int_{0}^{1} \left( \int_{0}^{x} 1_{E} \, d\mu \right) f'(x) \, d\mu(x) \\
&= f(1) \mu(E) - \int_{0}^{1} \left( \int_{0}^{x} 1_{E} \, d\mu \right) f'(x) \, d\mu(x) \\
&\geq f(1) \mu(E) - \int_{0}^{1} \left( \int_{0}^{x} 1_{[0, \mu(E)]} \, d\mu \right) f'(x) \, d\mu(x)  \\
&= \int_{0}^{1} 1_{[0, \mu(E)]} f \, d\mu
\end{align*} Again, I'm not sure if integration by parts is valid in this context, and I'm pretty sure that $f$ needn't be differentiable for the lemma to hold.","['real-analysis', 'measure-theory']"
1883743,Evaluate $\sqrt[2]{2} \cdot \sqrt[4]{4}\cdot \sqrt[8]{8}\cdot \dots$,"Evaluate: $$\lim_{n\to \infty }\sqrt[2]{2}\cdot \sqrt[4]{4}\cdot \sqrt[8]{8}\cdot \dots \cdot\sqrt[2^n]{2^n}$$ My attempt :First solve when $n$ is not infinity then put infinity in. $$2^{\frac{1}{2}}\cdot 4^{\frac{1}{4}}\cdot \dots\cdot (2^n)^{\frac{1}{2^n}}$$ $$=2^{\frac{1}{2}}\cdot 2^{\frac{2}{4}}\cdot \dots\cdot 2^{\frac{n}{2^n}}$$ Now calculate the sum of the powers: $$\frac{1}{2}+\frac{2}{4}+\frac{3}{8}+\dots+\frac{n}{2^n}$$ $$=\frac{2^{n-1}+2\cdot2^{n-2}+3\cdot2^{n-3}+\dots+n\cdot2^0}{2^n}$$ Now calculate the numerator: $$2^0+2^1+2^2+\dots+2^{n-1}=2^n-1$$ $$+$$ $$2^0+2^1+\dots+2^{n-2}=2^{n-1}-1$$ $$+$$ $$2^0+2^1+\dots+2^{n-3}=2^{n-2}-1$$ $$+$$ $$\vdots$$ $$+$$ $$2^0=2^1-1$$ $$=2^1+2^2+2^3+\dots+2^n-n=2^{n+1}-n-1$$ Now put the numerator on the fraction: $$\frac{2^{n+1}-n-1}{2^n}=2-\frac{n}{2^n}-\frac{1}{2^n}$$ Now we can easily find $\lim_{n \to \infty}\frac{1}{2^n}=0$ Then we just have to find $\lim_{n \to \infty }\frac{n}{2^n}$ , that by graphing will easily give us the answer zero. That gives the total answer is $4$ . But now they are two problems: 1.I cannot find $\lim_{n \to \infty }\frac{n}{2^n}$ without graghing. 2.My answer is too long. Now I want you to help me with these problems.Thanks.","['fractions', 'calculus']"
1883751,Strong form of Dirichlet prime number theorem and the 'smallest' large set,"I have no background in number theory, but the statement of the Dirichlet's theorem from Wikipedia is easy enough to understand. However, I'm confused about so called strong form (or 'stronger form') of this theorem: Stronger forms of Dirichlet's theorem state that for any such arithmetic progression, the sum of the reciprocals of the prime numbers in the progression diverges and that different such arithmetic progressions with the same modulus have approximately the same proportions of primes . Is this 'strong form' proved? If so, please give me the link. The strong form of Dirichlet's theorem implies that $${\frac  {1}{3}}+{\frac  {1}{7}}+{\frac  {1}{11}}+{\frac  {1}{19}}+{\frac  {1}{23}}+{\frac  {1}{31}}+{\frac  {1}{43}}+{\frac  {1}{47}}+{\frac  {1}{59}}+{\frac  {1}{67}}+\cdots $$ is a divergent series. This is the sum of reciprocals of the primes in the form $4n + 3$ . The second question (provided that this stronger form of the theorem is true, are the sets of primes of this form the 'smallest' (in terms of asymptotic density) among the large sets (i.e., sets of positive integers whose reciprocals sum to infinity)? (To clarify, we can take $4n+3$ as a sole example, because if the second part of the 'stronger form' is true, all of the primes $an+b$ are of the same size, i.e. asymptotic density) There was a similar question as my first one before, but the answers are not really informative. If the 'usual proof' of the Dirichlet's theorem also proves the 'stronger form', then why is it called a 'stronger form'? And is it actually called that, or is it just Wikipedia authors' invention?","['number-theory', 'prime-numbers', 'elementary-set-theory', 'elementary-number-theory']"
1883790,Is every ideal class represented by a prime?,"Suppose $K$ a number field. Does every element of the class group $C_K$ have a prime ideal in its class? More generally, is this true for ray class groups?",['number-theory']
1883792,"Let $f$ be entire such that $f(x+ix)\in\mathbb{R}$ for all $x\in \mathbb{R}$. If $f(2)=1-i$, find $f(2i)$.","Let $f$ be entire such that $f(x+ix)\in\mathbb{R}$ for all $x\in \mathbb{R}$. If $f(2)=1-i$, find $f(2i)$. So far, I have that since f is entire , $f(z)=\sum_{n=0}^\infty f^n(2)(z-2)^n/n!=\sum_{n=0}^\infty f^n(2i)(z-2i)^n/n!$. So $f(2+2i)=\sum_{n=0}^\infty f^n(2)(2)^n i^n/n!=\sum_{n=0}^\infty f^n(2i)(2)^n/n!=a\in \mathbb{R}$. I'm not sure if this is heading in the right direction.",['complex-analysis']
1883808,"Let $A_1, A_2,\dots$ be a sequence of disjoint, finite subsets of $\mathbb{N}$. How can $\bigcup_{n=1}^\infty A_n$ be either finite or infinite?","Let $A_n$ be finite subsets of $\mathbb{N}$ that are not $\emptyset$, and $\forall i,j, i\not = j$, $A_i, A_j$ are disjoint, then must 
$$\bigcup_{n=1}^\infty A_n = \mathbb{N}$$
and thus be (countably) infinite? So basically I am looking for an example of $A_1,A_2,\dots$ such that
$$\bigcup_{n=1}^\infty A_n$$
is finite. Or, is the only way for the infinite union to be finite for $A_i =A_j = \emptyset$? Thanks. Edit: A comment has answered how $\bigcup_{n=1}^\infty A_n \not = \mathbb{N}$ is possible. I am not sure if it must be infinite, though, when $A_n \not = \emptyset$",['elementary-set-theory']
1883845,"Disprove the statement: If a function is twice differentiable at a local maximum point, then its second derivative is negative at that point","I have to develop a counter example that disproves this statement but I am not to sure on how to go about this. Is it something simple that I am forgetting about? If a function is twice differentiable at a local maximum point, then its second derivative is negative at that point","['analysis', 'functions']"
1883851,Properties of local ring of restriction sheaf,"Let $X$ be a scheme and $\mathcal{F}$ a sheaf on $X$. I am trying to figure out if it is true that for any open affine $U \subset X$ such that $x \in U$, we have that $$\mathcal{F}_x = (\mathcal{F} \vert_U)_x$$ To show this I would need to show that every equivalence class of $\mathcal{F}_x$ contains $(W, s)$ where $W \subset U$. I am having a hard time showing this, maybe because it isn't even a true statement. I was thinking that maybe it is possible to show the existence of an affine open so that we have the above equality. Are either of these ideas correct?",['algebraic-geometry']
1883882,"Prove via differentiation that integral $\int_0^\pi \frac{\log(1+\cos\alpha\cos\theta)}{\cos\theta}\,d\theta = \pi(\frac{\pi}{2}-\alpha) $","Prove via differentiation that integral 
  $$\int_0^\pi \frac{\log(1+\cos\alpha\cos\theta)}{\cos\theta}\,d\theta$$
  is equal to
  $$ \pi\left(\frac{\pi}{2} - \alpha\right) $$
  where $0\leq \alpha\leq \frac{\pi}{2}.$ After partially differentiating wrt $\alpha$ I have tried via substitution; $$u=\log(1+\cos\alpha\cos\theta)$$ Then I get; $$\frac{dI(\alpha)}{d\alpha}=-\sin\alpha\int_0^\pi \frac{1}{1+\cos\alpha\cos\theta}\,d\theta$$ I can not currently see how to progress this to stated answer. Please show full working. The question is from G Stephenson's 'Mathematical Methods for Science students' - page 172.","['derivatives', 'definite-integrals', 'calculus']"
1883896,Values of b for which this series will converge,"I'm trying to solve the following problem: Find the positive values of $b$ for which the series $\sum_{n=1}^\infty b^{\ln(n)} $ converges. I started by doing the integral test with the function $b^{\ln(x)}$. To integrate this: $\int b^{\ln(x)} \, dx  $ I used integration by parts: I picked $u=b^{\ln(x)}$ $du= b^{\ln(x)} \ln(b)(1/x)dx$ $v=\int dx = x$ Doing the substitution I get: $$\int b^{\ln(x)} \, d x= b^{\ln(x)}x-\int x b^{\ln(x)}\ln(b)(1/x)\,dx$$ $$\int b^{\ln(x)} dx= b^{\ln(x)}x-\int b^{\ln(x)}\ln(b)(x/x)dx$$ $$\int b^{\ln(x)} dx= b^{\ln(x)}x-\int b^{\ln(x)}\ln(b)dx$$ $$\int b^{ln(x)} dx= b^{ln(x)}x-ln(b)\int b^{ln(x)}dx$$ $$\int b^{\ln(x)} dx + \ln(b)\int b^{\ln(x)}dx= b^{\ln(x)}x$$ $$[1 + \ln(b)]\int b^{\ln(x)} dx = b^{\ln(x)}x$$ $$\bbox[5px,border:2px solid red]{\int b^{\ln(x)} dx = \frac{b^{\ln(x)}x}{[1 + \ln(b)]}}\qquad$$ Then I solve evaluate the improper integral between $1$ and $\infty$ $$\int_1^\infty b^{\ln(x)} dx =\lim_{t\rightarrow \infty} \int_1^t b^{\ln(x)} dx =\lim_{t\rightarrow \infty} \left| \begin{array}{c} \frac{ b^{\ln(x)}x }{[1 + \ln(b)]} \end{array}  \right|_1^t$$ $$\lim_{t\rightarrow \infty} \left| \begin{array}{c} \frac{b^{ln(x)}x}{[1 + \ln(b)]} \end{array}  \right|_1^t =\lim_{t\rightarrow \infty} \frac{b^{\ln(t)}t}{[1 + \ln(b)]} -\frac{b^{\ln(1)}1}{[1 + \ln(b)]}$$ $$=\lim\limits_{t\rightarrow \infty} \frac{b^{\ln(t)}t}{[1 + \ln(b)]} -\frac{b^{0}1}{[1 + \ln(b)]}$$ $$=\lim\limits_{t\rightarrow \infty} \frac{b^{\ln(t)}t}{[1 + \ln(b)]} -\frac{1(1)}{[1 + \ln(b)]}$$ $$=\lim\limits_{t\rightarrow \infty} \frac{b^{\ln(t)}t}{[1 + \ln(b)]} -\frac{1}{[1 + \ln(b)]}$$ $$=[\frac{1}{[1 + \ln(b)]}]\lim\limits_{t\rightarrow \infty} b^{\ln(t)}t - 1$$ Here is where I get a little confused: I assume that $0<b < 1$ because that way I have an indeterminate form $0\cdots\infty$ which allows me to apply l'Hôpital's  rule as following: $$=\left[\frac{1}{[1 + \ln(b)]}\right] \lim_{t\rightarrow \infty} \frac {b^{\ln(t)}}{t^{-1}} - 1$$ I differentiate numerator and denominator $$=\left[\frac{1}{[1 + \ln(b)]}\right] \lim_{t\rightarrow \infty} \frac {b^{\ln(t)}\ln(b)t^{-1}}{-t^{-2}} - 1$$ And I end up getting the same indeterminate form again ($0\cdot\infty$) $$=\left[\frac{1}{[1 + \ln(b)]} \right] \lim_{t\rightarrow \infty} -b^{\ln(t)} \ln(b)t - 1$$ What should I do? The book gives $b<(1/e)$ as an answer, which makes sense because if $b=1/e$ then $\ln(b) = -1$ and the denominator of $[\frac{1}{[1 + \ln(b)]}]$ would be zero. But I'm stuck here. How do I solve evaluate the limit? I'm assuming that my mistake is not in the indefinite integral because Mathematica and other CAS give the same answer. So the problem is likely to be in the improper integral and the limit.","['limits', 'sequences-and-series', 'calculus', 'improper-integrals', 'convergence-divergence']"
1883900,Is this an exponential distribution? (Variant of Gumbel distribution),"The distribution function is $$F_x(x\mid\lambda) = e^{-e^{-\lambda x}}, \qquad \lambda > 0$$ Is this an exponential family? The pdf that I obtained was $$f(x\mid\lambda) = \lambda e^{-\lambda x} e^{-e^{-\lambda x}}$$ and the joint pdf of an iid sample is $$f(x^n\mid\lambda) = \lambda^n e^{-\lambda \sum_{i=1}^n x_i} e^{-\sum_{i=1}^n (e^{-\lambda x_i})}$$ I don't see how I can bring this to the form that is required for it to be an exponential family. Am I right in concluding that this isn't an exponential family? Also, is it possible to use the Karlin Rubin theorem to obtain a UMP test and thus a  confidence interval for $\lambda$?",['statistics']
1883904,A time parameterization of geodesics on the sphere,"So I know that all the geodesics on the sphere ($\mathbb{S}^2\subseteq\mathbb{R}^3$) lie on great circles. However, I have been having a bit of trouble coming up with a time parameterization of these great circle arcs. Specifically, if I have 2 points $(\theta_1,\phi_1)$ and $(\theta_2,\phi_2)$ that lie on $\mathbb{S}^2$ what is the function $\gamma:[0,1]\to\mathbb{S}^2$ such that $\gamma([0,1])$ is the geodesic that connects these two points? This doesn't seem like it should be terribly difficult; however, I've been getting stuck. The reason that I care about this parameterization is that I am trying to get some visualization working in Mathematica.","['geodesic', 'differential-geometry', 'geometry']"
1883915,Closed connected integral submanifold is maximal,"I'm having some problems to prove the following assertion: Let $\mathscr{D}$ be an involutive distribution of dimension $k$ in a
  manifold $N$. Let $(M,\varphi)$ be a integral connected submanifold,
  such that $\varphi(M)\subseteq N$ is a closed subset. Show that $M$ is
  a maximal connected integral submanifold of $\mathscr{D}$ (a leaf ,
  say). I've tried using the local version of Frobenius Theorem (each involutive distribution is integrable and locally its integral connected submanifolds are slices). However, the problem talks of something that is rather global (?), and I can't manage to complete the idea. I'd thank any kind of help.",['differential-geometry']
1883920,Are two compact Hausdorff spaces homeomorphic if their algebras of continuous functions are isomorphic?,Suppose that $X$ and $Y$ are two compact Hausdorff spaces and $F\colon C(X) → C(Y)$ is a continuous isomorphism of algebras. Can I say $X$ and $Y$ are homeomorphic? The key words always lead me to other questions. Can anyone give me some reference? Thanks a lot!,"['general-topology', 'banach-algebras']"
1883948,"Meaning Confused "" Theorem Schema""","I was reading Enderton's ""Elements of Set Theory"", and came upon, Transfinie Recrursion Theorem Schema: For any formula $\gamma(x,y)$ the following is a theorem. Assume $<$ is a well ordering on $A$. Assume that for any $f$ there is a unique $y$ such that $\gamma (f,y)$. Then there exists a unique function $F$ with domain $A$ such that 
  $$ \gamma (F \upharpoonright \text{seg }t , F(t)) $$
  for all $t \in A$. I am confused what is meant by Theorem Schema here. Enderton explains this as an ""infinite package of theorems"". But what is the point? When we write out a theorem, don't we just write a statement? Why restrict ourselves to a specific form? I feel like the Theorem Schema is limiting our scope rather than helping - or maybe I completely misunderstood its purpose. May someone explain? Thanks!","['set-theory', 'logic', 'functions', 'definition']"
1883951,Supplementary Books to Complex Analysis of Rudin's RCA?,"I will be doing a reading course in the complex analysis starting on this Fall Semester. The assigned book is Rudin's Real and Complex Analysis. From my understanding, Rudin treats complex analysis very elegantly, but very terse. I am curious if you could suggest some books in the complex analysis that can accomodate Rudin, with particular emphasis on the extensive treatment and/or clear explanations. I am embarrassed to ask my professor as I do not want to impose a bad impression on me. Also, are previous chapters in Rudin-RCA a must requirement for later chapters in the complex analysis? I am currently reading through Berberian and Kolmogorov/Fomin to learn some basics of measure theory and banach space, but I have not completely learned them yet.","['complex-analysis', 'book-recommendation', 'reference-request']"
1883952,Theorem name or source: a (ruled) surface with a certain number of non-parallel lines is a plane,"I vaguely remember from my youth  a result concerning scrolls or ruled surfaces . Here is what I remember: A ruled surface containing at least $n$ non-parallel lines is a plane In my memory $n=27$, but I am not very sure. Does this remind someone of a correctly stated theorem, does it have a name?","['differential-geometry', 'surfaces', 'geometry']"
1883984,Expressing $\mathbb{P}_A^n$ as $n+1$ glued copies of $\mathbb{A}_A^n$ -- what is the inclusion map?,"I wish to define an inclusion morphism $\varphi_i:\mathbb{A}_A^n\to\mathbb{P}_A^n$, where $\mathbb{A}$ and $\mathbb{P}$ are defined as schemes, and $A$ is some commutative ring. I'm struggling to define the mapping of topological spaces. Here are my thoughts so far: I define $A_i:=A[x_0,\ldots,\hat{x_i},\ldots,x_n]$ for convenience, and set $U_i:=\mathrm{Spec}\ A_i=\mathbb{A}_A^n$. I also set $S=A[x_0,\ldots,x_n]$ as a graded ring, defining $S_+$ and $S_d$ as usual. For any prime ideal $\mathfrak{p}\subset A_i$, I have to define $\varphi_i(\mathfrak{p})$ as some homogeneous prime ideal of $S$ not containing$S_+$. I have a few thoughts about possible ways to do this, but I'm not sure which one is the correct one, and I don't see any obvious way to check. Firstly, for $\mathfrak{p}=\langle f_1(x_0,\ldots,\hat{x_i},\ldots,x_n),\ldots, f_m(x_0,\ldots,\hat{x_i},\ldots,x_n)\rangle$, I might define $$\varphi_i(\mathfrak{p})=\left\langle x_i^{e_1}f_1(\frac{x_0}{x_i},\ldots,\hat{\frac{x_i}{x_i}},\ldots,\frac{x_n}{x_i}),\ldots, x_i^{e_m} f_m(\frac{x_0}{x_i},\ldots,\hat{\frac{x_i}{x_i}},\ldots,\frac{x_n}{x_i})\right\rangle$$ where $e_j$ is minimal such that $$x_i^{e_j} f_j(\frac{x_0}{x_i},\ldots,\hat{\frac{x_i}{x_i}},\ldots,\frac{x_n}{x_i})\in S.$$ Alternatively, I might define $$\varphi_i(\mathfrak{p})=\{x_i^a f(\frac{x_0}{x_i},\ldots,\hat{\frac{x_i}{x_i}},\ldots,\frac{x_n}{x_i}) \bigg|\  f\in\mathfrak{p},a\in\mathbb{N}\}\cap S.$$ Unfortunately, I'm having a hard time showing that either of these definitions give a prime ideal not containing $S_+$, and furthermore, both of them are incredibly clunky to work with. Is there any better way of going about this?","['schemes', 'algebraic-geometry']"
1883997,Convergence of the series $\sum_{n=1}^{\infty}\frac{x_n}{n^b}$,"Assume the sequence $y_n=\dfrac{x_1+x_2+\cdots+x_n}{n^a}$ is bounded, where $a>0$ and $x_n$ is a sequence of real numbers. Prove that if $b>a$ then $\sum\limits_{n=1}^{\infty}\dfrac{x_n}{n^b}$ is convergent. I have tried to solve in the case a=1, b=2 by calculating x with respect to y, we get $$\sum\limits_{n=1}^{\infty}\dfrac{x_n}{n^b}=\sum\limits_{n=1}^{\infty}\dfrac{x_n}{n^2}=\dfrac34 y_1+\cdots+\dfrac{2n+1}{(n+1)^2}y_n+\cdots$$ However, even in this simple case I cannot get the conclusion. Could you please help me with this? Thank you very much for your help.","['real-analysis', 'convergence-divergence', 'sequences-and-series', 'calculus']"
1884009,When to use the modulus symbol and when not to use the modulus symbol in integration and differentiation?,"I am facing this conceptual doubt for quite some time now. We know $$\frac{d}{dx}{(\sec^{-1}{x})}=\frac{1}{|x|\sqrt{x^2-1}}$$ whereas $$\frac{d}{dx}{(\csc^{-1}{x})}=\frac{-1}{|x|\sqrt{x^2-1}}$$ Now suppose I need to find the integral 
$$\int\frac{1}{x\sqrt{x^2-1}}dx$$ then will the answer be $\sec^{-1}{x}$ or $\csc^{-1}{x}$ in case the modulus function is not used for $x$ in the denominator? Why? Another similar doubt I have is that $$\int{\frac{1}{x^2-a^2}} \, dx$$ equals $$\frac{1}{2a}\ln\left(\frac{x-a}{x+a}\right)+C$$ or $$\frac{1}{2a}\ln\left|\frac{x-a}{x+a}\right|+C \text{?}$$ Some books use the former formula and some use the latter.Which one is correct and why? Pardon me if you find this question too trivial.But really I'm confused with this thing from the past few months!","['algebra-precalculus', 'integration', 'calculus', 'derivatives']"
1884020,Sylow Conjugation Theorem is False,"We have the following theorem: Theorem. Let $p$ be a prime and $G$ be a finite group whose order is divisible by $p$. Then the number of Sylow-$p$ subgroups of $G$ is congruent to $1$ modulo $p$. We are going to find a counterexample to this theorem. Let $G=SL_2(F_3)$, where $F_3$ denotes the finite field of order $3$.
Then $|G|=24$. Let $n_3$ denote the number of Sylow-$3$ subgroups of $G$.
Then the number of elements of order $3$ in $G$ is $2n_3$. Let $M\in G$ be a matrix of order $3$.
Then $M$ satisfies the polynomial $x^3-1=(x-1)^3$.
Thus the minimal polynomial of $M$ is either $x-1$, or it is $(x-1)^2=x^2+x+1$.
But the minimal polynomial cannot be $x-1$ since the order of $M$ is $3$.
So the minimal polynomial of $M$ has to be $x^2+x+1$.
Since the characteristic polynomial of $M$ has degree $2$, we see that the minimal and the characteristic polynomial of $M$ coincide, and thus $M$ is a cyclic.
This means that there is a vector $v\in F_3^2$ such that $\{v, Mv\}$ is a basis of $F_3^2$.
So $M$ is similar to the matrix $N:=\begin{bmatrix} 0 & -1 \\ 1 & -1\end{bmatrix}$. What we have shown is that all matrices of order $3$ in $G$ are similar to $N$.
It is clear that all matrices similar to $N$ are of order $3$. So the set of all the elements of order $3$ in $G$ is same as the conjugacy class of $N$. So now we set out to find the size of the conjugacy class of $N$. This is same as the index of the centralizer $C_G(N)$ of $N$.
Let $A=\begin{bmatrix} a & b\\ c & d\end{bmatrix}$ be in $C_G(N)$.
Then using $ANA^{-1}=N$, we have $b=-c$ and $d=a+b$, and using $\det(A)=1$ we have $a^2+ab+b^2=1$.
The solutions to this are $(a, b)=(0, 1), (0, 2), (1, 0), (1, 2), (2, 0), (2, 1)$.
So $|C_G(N)|=6$. Therefore $\text{conj}(N)=|G:C_G(N)|=4$. So we have shown that there are $4$ elements of order $3$ in $G$, which yields there are $2$ Sylow-$3$ subgroups in $G$. This contradicts the theorem quoted in the beginning of the theorem. Can somebody please point out my mistake? I know Sylow theorems are not wrong :)","['abstract-algebra', 'group-theory', 'linear-algebra', 'sylow-theory']"
1884031,How rigorous are pictorial proofs?,"The standard proof that $|\mathbb{Q}| = \mathbb{|N|}$ is pictorial. I am sure everyone here has seen it. The ""zig-zag"".  I must admit, however, that, although I was ""intuitively"" convinced by it, I was never entirely satisfied with it because it is not an explicit bijection $f:\mathbb{N} \to \mathbb{Q}$ given by an actual formula. The fact that the proof is correct seems ""clear"" to us, but this is, again, merely an appeal to intuition. One should note that some of these ""proofs by picture"" are simply incorrect: see Russell O'Connor's answer here . I have two questions Is the pictorial proof that $|\mathbb{Q}| = \mathbb{|N|}$ rigorous by the standards of modern pure mathematics? For the sake of this question, suppose that there isn't an explicit formla, or that it's too unwieldy to use in practice. After all, even if there is a formula, most of the people who've seen the pictorial argument do not know of it. Is there an explicit formula for the ""pictorial"" proof? There's some minor issues, of course, namely the inclusion of $0$ and variations of the ""zig-zag"" path, but these are no big deal. A bijection $f:\mathbb{N} \to \mathbb{N} \times \mathbb{N}$ suffices; dealing with negatives, equivalent fractions, etc is trivial.","['soft-question', 'elementary-set-theory']"
1884082,Do we have to consider different ranges when we a using trigonometric substitution?,Several times during integration we use trigonometric substitutions like $x=\sin(y)$ or $x=\tan(y)$ but in such cases do we have to take care of the sign of $x$ by considering different ranges for $y$? For example when we are integrating $$\frac{1}{x\sqrt{x^2-1}}$$ we may use $x=\sec(y)$.In case $0\leq y<\dfrac\pi2$ answer will be $\sec^{-1}{x}$ but if $\pi/2< y\leq\pi$ answer will be $\sec^{-1}{(-x)}$.Morever they do not differ by a constant as $\sec^{-1}{x}\ne \sec^{-1}{(-x)}+C$. However in all books I have seen till now during trigonometric substitution range is never taken care of.My question is that is necessary and correct to take care of the range of the trigonometric variable substituted?Why or why not?,"['integration', 'trigonometry', 'calculus']"
1884088,How can I prove: $\sum_{n\leq x}\frac{\phi(n)}{n^2} = \frac{\log x}{\zeta(2)}+\frac{C}{\zeta(2)} + A + O\left(\frac{\log x}{x}\right)$,"The problem is that prove that $$\sum_{n\leq x}\frac{\phi(n)}{n^2} = \frac{\log x}{\zeta(2)}+\frac{C}{\zeta(2)} + A + O\left(\frac{\log x}{x}\right)$$ 
where $C$ is Euler's constant and $A = \sum_{n \geq 1}\frac{\mu(n)\log n}{n^2}$ The following is things I did try: $$\sum_{n\leq x}\frac{\phi(n)}{n^2} = \sum_{n\leq x}\frac{1}{n}\frac{\phi(n)}{n} =\\ 
\sum_{n\leq x}\frac{1}{n}\sum_{d\mid n} \frac{\mu(d)}{d} = \sum_{q\leq x}\frac{1}{q} \sum_{d\leq x} \frac{x}{q}\frac{\phi(d)}{d^2} =\\
\sum_{q\leq x}\frac{1}{q}\left( \frac{1}{\zeta(2)} + O\left(\frac{q}{x}\right) \right) = \frac{\log x}{\zeta(2)} + \frac{C}{\zeta(2)}  + O\left(\frac{1}{x}\right) + \sum_{q\leq x}\frac{1}{q} O\left(\frac{q}{x}\right) $$ Here $\mu$ is the Möbius function and $\phi$ is the Euler totient function.",['number-theory']
1884094,Is $\emptyset$ bounded? Why then $\inf \emptyset = \infty$ is reasonable?,"I know the proof that $\emptyset$ (empty set) has $\infty$ as infimum. This seems to imply to me that $\emptyset$ is unbounded, I mean a set with $\infty$ as lower bound: how can this be bounded?! But if $\emptyset$ is unbounded then $\forall M>0$ $\exists x \in \emptyset $ s.t. $x > M$, which is impossible because there can't exist an element in $\emptyset$. How to resolve this paradox? thanks.","['real-analysis', 'supremum-and-infimum', 'elementary-set-theory']"
1884096,Calculus of Variations. Understand Epsilon's Need?,"What purpose is Epsilon in Calculus of Variations?
My question being what is the need for Epsilon in the equation 
$y(x)+\epsilon n(x)$ when we assume this as the neighborhood curve of $y(x)$? When we try to find out the Euler Lagrange condition, we take $y(x)$ to be the curve and $y(x)+\epsilon n(x)$ to be the neighboring curve.",['multivariable-calculus']
1884140,almost everywhere differentiable but not almost everywhere continuously differentiable,"The common example for differentiable but not continuously differentiable is $x^2\sin \frac{1}{x}$. This still looks almost everywhere continuously differentiable to my not very trained eyes. Are there examples of almost everywhere differentiable but not almost everywhere continuously differentiable? If so, are there any straightforward conditions (possibly to do with one-sided derivatives) that can be combined with almost everywhere differentiable to give almost everywhere continuously differentiable? (I am trying to show that the Lipschitz continuous function I am working with is almost everywhere continuously differentiable. Rademacher's theorem says that a Lipschitz continuous function is almost everywhere differentiable. )","['derivatives', 'lipschitz-functions', 'measure-theory']"
1884143,About the distribution of Gaussian prime pairs,"The primes in the ring of Gaussian integers $\mathbb Z[i]$ has a simple connection to the ordinary primes: either one of the components is zero and the other is a prime of type $4n+3$ or the norm $\|a+ib\|=a^2+b^2$ is a prime $a\ne 0\ne b$, that is a prime of the type $4n+1$ . All $n\in\mathbb Z^+$ with $n=a^2+b^2$, e.g. all ordinary primes of the form $4n+1$, are composites in $\mathbb Z[i]$:
$n=(a+ib)(a-ib)$. There is essentially one pair of Gaussian primes such that $|z_1-z_2|=1$, and that is $(1+i,2+i)$. If defining prime pairs in $\mathbb Z[i]$ with 
$|z_1-z_2|=2$, then it seems like for each integer $n>0$ there is a prime $n+im\in\mathbb Z[i]$ with a prime as a pair. Can this be proved? Verified for all $a+ib\in\mathbb Z[i]$ with $0<a,b\leq1000$. I'll elaborate. If $|z_1-z_2|=2$ one can assume e.g. that $\Re (z_1-z_2)=0$ and $|\Im (z_1-z_2)|=2$. Hence, the conjecture can be formulated as: For each $a\in\mathbb N^+$ it exists $b\in\mathbb Z$ such that $a+ib$ and $a+i(b+2)$ are Gaussian primes. Equivalent conjecture: 
  $\forall n\in\mathbb N^+\exists m\in\mathbb Z: n^2+m^2,n^2+(m+2)^2\in\mathbb P$.","['number-theory', 'conjectures', 'prime-numbers', 'gaussian-integers']"
1884160,Integration of trigonometric function $\int\frac{\sin(2x)}{\sin(x)-\cos(x)}dx$,"$$\int\frac{\sin(2x)}{\sin(x)-\cos(x)}dx$$ My attempt: Firstly, $\sin(2x)=2\sin(x)\cos(x)$. After that, eliminate the $\cos(x)$ seen in both the numerator and denominator to get $$2\int\frac{\sin(x)}{\tan(x)-1}\ dx.$$ From here onwards, should I convert $\sin(x)$, $\tan(x)$ to half-angles and use $\tan(x/2)=t$? But this would be a time consuming method. Any suggestions?","['indefinite-integrals', 'real-analysis', 'integration', 'trigonometry']"
1884178,Derivative of $\frac{\ln{x}}{\ln{a}}$,"I'm asked to prove that the derivative of $$\frac{\ln{x}}{\ln{a}}$$ is $$\frac{1}{x\cdot\ln{a}}$$ My attempt: $$\frac{d}{dx}(\frac{\ln{x}}{\ln{a}}) = \frac{\frac{1}{x}\cdot \ln{a} - \frac{1}{a}\cdot \ln{x}}{(\ln{a})^2}$$
which is 
$$\frac{1}{x\cdot\ln{a}} - \frac{\ln{x}}{a\cdot (\ln{a})^2}$$ which is not equal to $$\frac{1}{x\cdot\ln{a}}$$ What am I doing wrong?",['derivatives']
1884253,"Computing the Lie bracket on the Lie group $GL(n, \mathbb{R})$","Consider the Lie group $GL(n, \mathbb{R})$. Since $GL(n, \mathbb{R})$ is an open subset of the space $M_{n,n}(\mathbb{R})$ of $n \times n$ matrices, we can identify the tangent space (Lie algebra) $T_{e}GL(n, \mathbb{R})$ with $M_{n,n}(\mathbb{R})$, where $e$ is the identity matrix. I'm trying to understand the following: For all $x, y \in M_{n,n}(\mathbb{R}) = T_{Id}GL(n, \mathbb{R})$ we have $[x, y] = xy -yx$ where $[\cdot, \cdot]$ is the Lie algebra bracket. According to the question Is the Lie bracket of a Lie Algebra of a Matrix Lie Group always the commutator? , this seems to be the case for matrix Lie groups in general. So far, I've worked out the following: Let $x \in M_{n,n}(\mathbb{R}) = T_{Id}GL(n, \mathbb{R})$ and let $X \in \Gamma^{inv}(TG)$ be the left invariant vector field corresponding to $x$, i.e. with $X(e) = x$. By openness, we can choose $\epsilon > 0$ such that the curve $(-\epsilon, \epsilon) \ni t \mapsto e + tx$ through the identity is contained in $GL(n, \mathbb{R})$. For $g \in GL(n, \mathbb{R})$ let $L_g$ be the left multiplication by $g$ in $GL(n, \mathbb{R})$. Using left invariance of $X$ calculate:
$$X(g) = ((L_g)_{*}X)(g) = D_eL_g(X(e)) = D_eL_g(x) = \frac{d}{dt}|_{t=0}g(e+tx) = gx$$
Now, the above implies for $f \in C^{\infty}(GL(n, \mathbb{R}))$ and $g \in GL(n, \mathbb{R})$:
$$X(g)(f) = (D_gf)(gx)$$
where we identify the tangent vector $X(g)$ with the corresponding derivation at $g$. Now how do I calculate $[x,y] \stackrel{def.}{=} [X, Y](e) \stackrel{\text{WHY?}}{=}xy-yx$ from this?","['vector-fields', 'differential-geometry', 'lie-algebras', 'lie-groups']"
1884271,Evaluate $\prod_{i=1}^{89} \sin (i)$ [duplicate],"This question already has answers here : Prove that $\prod_{k=1}^{n-1}\sin\frac{k \pi}{n} = \frac{n}{2^{n-1}}$ (3 answers) Closed 7 years ago . Evaluate: $$\prod_{i=1}^{89} \sin (i)$$ My attempt :We know that $\sin \alpha \cdot \sin(90- \alpha)=\sin \alpha \cos \alpha=\frac{1}{2}\sin 2\alpha$ Then we have: $$
(\sin1\cdot\sin89)(\sin2\cdot\sin88)\cdots (\sin44\cdot\sin46)\sin45
=\left(\frac{1}{2}\sin2\right)\cdots\left(\frac{1}{2}\sin88\right)\frac{\sqrt{2}}{2}
$$ Factorize from $\frac{1}{2}$ then do the same thing again You will get: $$
\frac{\sqrt{2}}{2^{67}}(\sin4\cdot\sin8\cdots\sin88)
$$ Now what to do?Any hints?","['trigonometry', 'calculus']"
1884274,Can (linear) differential equations of infinite order be recast into equations of first order?,"In most analysis courses one sees that differential equations of order $n$ are basically a subset of higher dimensional differential equations of order $1$ , for example the equation: $$f^{(n)}(t)=F\left(f(t),f'(t),...,f^{(n-1)}(t),t\right)$$ is the same as: $$\frac{d}{dt}\,\begin{pmatrix}g_0(t)\\g_1(t)\\\vdots\\g_{n-1(t)}\end{pmatrix}=\begin{pmatrix}g_1(t)\\\vdots\\g_{n-1}(t)\\F\left(g_0(t),g_1(t),...,g_{n-1}(t),t\right)\end{pmatrix}.$$ This is especially useful, as it allows one to write down explicitly the solutions to linear differential equations of finite order, if we have: $$f^{(n)}(t)=\sum_{k=0}^{n-1}a_k\, f^{(k)}(t)$$ Then the corresponding $n$ -dimensional equation is of the form: $$\frac{d}{dt} g(t) = A\cdot g(t)$$ for some matrix $A$ and the solution is $g(t)=\exp(A\,t)g(0)$ . It is possible to generalise to time dependent coefficients $a_k$ . Is there a way to implement this trick for differential equations that are essentially of infinite order? For example the equation $$f=\sum_{k=1}^\infty f^{(k)}(t),$$ of which the solution space is $f=\{C\exp(\frac t2)\mid C\in\mathbb R$ (or $\mathbb C$ ) $\}$ . More generally I would like to put something of the form $$\sum_{k=0}^\infty a_k\, f^{(k)}(t)=0$$ (where $a_k$ are as regular as needed (but with infinite non-zero terms)) into the form $$\frac{d}{dt} u = A(u)$$ Where $u$ is a map $C^\infty(\mathbb R,X)$ with $X$ a Banach space and $A\in \mathcal L(X)$ .","['real-analysis', 'functional-analysis', 'ordinary-differential-equations', 'sequences-and-series', 'analysis']"
1884280,Expectation of the maximum absolute value of gaussian random variables,"Let $\{X_i\}_{i=1}^n$ be an i.i.d. sequence of $\mathcal{N}(0, \sigma^2)$ variables, and consider the random variable 
$$Z_n : = \max_{i=1,\ldots,n}|X_i|.$$ I need to prove the bound $$ E[Z_n] \leq \sqrt{2\sigma^{2}\log{n}} + \frac{4 \sigma}{\sqrt{2\log{n}}} \quad \text{for all } n \geq 2. $$ I know how to prove the bound 
$ E[Z_n] \leq \sqrt{2\sigma^{2}\log{n}} $ 
(using the moment generating function),
which is even better, but the hint in the exrcise says that I should use the tail bound 
$$
P[|U| \geq x] \leq \sqrt\frac{2}{\pi}\frac{1}{x} e^{-\tfrac{x^2}{2}},
\quad \text{where $U$ is a standart normal r.v.} \qquad (1)
$$ My idea. Since $Z_n$ is a non-negative r.v., then 
$$ E[Z_n] = \int_{0}^{\infty} P[Z_n \geq x] \ dx = \int_{0}^{\infty} 
\Bigl( 1 - \bigl(1 - P[|X_1| \geq x] \bigr)^n \Bigr) dx. $$
I tried to use the tail bound (1), but it was unsuccessful. I even don't understand why this integral converges. I would appreciate any ideas. Thanks!","['probability', 'concentration-of-measure', 'normal-distribution']"
1884294,Basic Joint probability distribution calculation example?,"(X, Y) has the Joint probability distribution: \begin{array}{|c|c|c|}
\hline
 X / Y & 1 & 2 \\ \hline
 1& \frac{1}{4}& \frac{1}{3}  \\ \hline
 2&  \frac{1}{6}& a \\ \hline
\end{array} So $P(X=1\mid Y=1)=\frac{3}{5}$ and $P(X=2\mid Y=2)=\frac{3}{7}$. My challenge is via calculation, how $\frac{3}{5}$ and $\frac{3}{7}$
  was calculated in this example?","['probability-theory', 'probability', 'statistics', 'probability-distributions']"
1884297,Prove: A triangle inscribed in a rectangular hyperbola has its orthocenter on that hyperbola,"Let $A$, $B$, and $C$ be three points on the curve $xy = 1$ (which is a rectangular hyperbola). Prove that the orthocenter of $\triangle ABC$ also lies on the curve $xy = 1$. I have given up on this problem...",['geometry']
1884303,The $n$-th number that contains the digit $k$ or is divisible by $k$ ($2 \le k\le 9$),"I need to find the $n$-th number that contains the digit $k$ or is divisible by $k$, where $2 \leqslant k \leqslant 9$. Example: If $n = 15$ and $k = 3$, then the answer is $$ 33\quad (3, 6, 9, 12, 13, 15, 18, 21, 23, 24, 27, 30, 31, 32, 33)$$ I started following the sequence but couldn't formulate it: for multiples of $3 \to 3+3+3+4+3+3+4+3+3+4$ for containing digit $3 \to 1+1+1+10+1+1+1+1+1+1$ I have to answer in better than $\mathcal O(n)$ or in $\mathcal O(1)$ if possible. Please don't suggest methods that checks every number in a for loop. Thanks. Edit: I have searched everywhere but couldn't find it answered anywhere. So, it's not a duplicate.","['recreational-mathematics', 'contest-math', 'algorithms', 'discrete-mathematics']"
1884307,Composition relation of R1 ∘ R2,"Let $R_1$ and $R_2$ be the relations on $\{1, 2, 3, 4, 5\}$ defined by $$R_1 = \{(1,1),(2,3),(2,4),(3,5),(5,2),(5,5)\}$$ $$R_2 = \{(1,1),(2,2),(2,3),(2,5),(4,3),(5,5)\}$$ The answer for this is below but I'm not sure how they arrived at this answer. Answer: $$R_2 \circ R_1 = \{(1,1),(2,3),(2,4),(2,5),(4,5),(5,5)\}$$ What I got: $$R_2 \circ R_1 = \{(1,1),(2,3),(2,4),(2,5),(2,2),(4,5),(5,5)\}$$","['relations', 'function-and-relation-composition', 'elementary-set-theory', 'discrete-mathematics']"
1884318,Find permutation matrix that minimizes a Frobenius norm,"Is there an efficient way to solve (maybe approximately) the following optimization problem $$\left\| A - X B X^T \right \|_F \xrightarrow[]{X}\min$$ where $X$ is a permutation matrix and $A$ , $B$ are dense square symmetric real matrices?","['permutation-matrices', 'optimization', 'matrices', 'discrete-optimization', 'permutations']"
1884366,"Given any function $g(x): \mathbb{R} \rightarrow \mathbb{R}$, find a function $f(x)$ such that $f(f(x)) = g(x)$.","This is a problem I was casually discussing with friends: Given any function $g(x): \mathbb{R} \rightarrow \mathbb{R}$, find a
  function $f(x): \mathbb{R} \rightarrow \mathbb{R}$ such that $f(f(x)) = g(x)$. Is it possible to find a solution for $f(x)$ or prove that there isn't a solution for $f(x)$ such that $f(f(x)) = g(x)$ for any given $g(x)$? For example, if $g(x) = x + 2$, then $f(x)$ could easily be $f(x) = x + 1$. But if $g(x) = x^2 - 1$, then it's not clear to see what $f$ can satisfy such solution. What approach might be promising? My first gut reaction would be some analytical method but can't think of a way still...Discussions are welcomed!",['analysis']
1884417,What does mean weaker and stronger terms in logic?,"I want to understand what does mean the terms stronger and weaker which I'm reading in Programming: The Derivation of Algorithms : When $[P\Rightarrow Q]$ holds then $P$ is called stronger than $Q$ and $Q$ is called weaker than $P$. For example, $x\geqslant 2$ is stronger than $x\geqslant 1$ and $x^{2}\geqslant 0$ is weaker than $x\geqslant 0$. The weakest predicate is predicate true, since $[P\Rightarrow true]$ for all $P$, and the strongest predicate is predicate false, since $[false \Rightarrow P]$ for all $P$. I would like to hear examples from daily common language which apply these two terms.",['discrete-mathematics']
1884418,"How to evaluate $\int_0^1\int_0^1 \frac{1}{1-xy} \, dy \, dx$ to prove $\sum_{n=1}^{\infty} \frac{1}{n^2}=\frac{\pi^2}{6}$.","I've read somewhere on this site that if you consider: $$\int_0^1 \int_0^1 \frac{1}{1-xy} \,dy\,dx$$ Then using the power series, we have this is equal to $\sum_{n=1}^{\infty} \frac{1}{n^2}$ which I decided to try and was able to show. Apparently we can show this is equal to $\frac{\pi^2}{6}$, and using what little I know about double integrals from a few khan academy videos (I haven't taken multivariable  calculus yet), I tried to evaluate this double integral by techniques of single variable calculus $u=xy$..and I got this: $$-\int_{0}^{1} \frac{\ln (1-x)}{x}dx$$ The usual way I would evaluate this is with a Taylor series, but that just that just leads us in circles. So I want to know how can I evaluate this, so we can prove $\sum_{n=1}^{\infty} \frac{1}{n^2}=\frac{\pi^2}{6}$.","['multivariable-calculus', 'riemann-zeta', 'calculus']"
1884421,What is the expected number of suits in a hand of 5 cards?,"Suppose we draw 5 cards out of a deck of 52.What is the expected number of
different suits in our hand? For example, if we draw K♠ 3♠ 10♥ 8♥ 6♣, there
are three different suits in our hand.
Answer: 3.114 Here’s what I’ve tried. N = number of suits E[X] = (1*P(N=1) + 2*P(N=2) + 3*P(N=3) + 4*P(N=4))/52C5 Now to calculate the probability of N suits is where I get a problem. P(N=1) = (4C1 *13C5)/52C5 My Reasoning: (Pick a suit * choose 5 cards from it) P(N=2) = (4C2 *13C1*13C1*24C3)/52C5 My Reasoning: (Pick two suits * pick 1 card from each * choose 3 from the remaining cards in those two suits) P(N=3) = (4C3 *13C1*13C1*13C1*36C2)/52C5 My Reasoning: (Pick three suits * pick 1 card from each * choose 2 from the remaining cards in those three suits) P(N=4) = (4C4 *13C1^4*48C1)/52C5 My Reasoning: (Pick four suits * pick 1 card from each * choose 1 from the remaining cards in those four suits) This leaves me with: 1(5148)+2(2052336)+3(5536440)+4(1370928)/2598960 Which equals 10.002 They’re aren’t even 10 suits in a deck so I’ve done something very wrong.","['statistics', 'probability']"
1884474,support on a triangle and uniform marginal distributions,"Suppose $X$ and $Y$ are jointly distributed on the support $\operatorname{conv} \{(0,0),(0,1),(1,0)\}$ with the joint PDF $f>0$ everywhere on the support. Is it possible to find $f$ such that the marginal PDFs are given by $f_X(x)= 1$ for all $x\in[0,1]$ and $f_Y(y)=1$ for all $y\in [0,1]$? Thanks","['multivariable-calculus', 'statistics', 'probability', 'probability-distributions']"
1884500,How to solve this equation for $x$ with XOR involved?,"I have the following equation I need to solve for $x$: $$y = (21306107x - 27776373) \oplus (i - 29799480x) $$ Where: $\oplus$ is the XOR operator for 32-bit integers. Both $y$ and $i$ are known 32-bit integers. $x$ is some unknown 32-bit integer [Edit: $y$ and $i$ actually do vary, but are fixed for each unknown value of $x$]","['abstract-algebra', 'computational-mathematics']"
1884533,Double dual space is isomorphic to vector space - Intuition,"The recent topics I studied were linear functionals and dual spaces. I like to think about a linear functional as a stack of hyperplanes like it is described here . In ""Finite dimensional vector spaces"" by Paul Halmos I read that every vector space is isomorphic to its double dual. I wonder if there is an intuitive way to see that they are isomorphic? Also I am not sure how I could graphically or geometrically think about the double dual space (in the sense I think about the dual space as a stack of hyperplanes in every direction). Is there a way to visualize the double dual space? Maybe then the isomorphism would become clearer. I guess there is some intuition behind it since someone had to think about it first before he or she invented the concept (double dual space). I hope my question makes sense? Thanks for any responses!","['intuition', 'linear-algebra', 'dual-spaces', 'duality-theorems']"
1884536,Does the matrix square root have directional derivatives at semipositive points?,"$\newcommand{\psym}{\operatorname{P}_{\ge 0}}$ Let $\psym$ be the set of symmetric positive semidefinite (real) matrices. Let $\sqrt \cdot :\psym  \to \psym $ be the unique positive semidefinite square root. Let $A \in \psym$ be a matrix on the boundary , i.e $\det A=0$. Let $B$ be a symmetric matrix such that $A+tB$ is positive semidefinite for $t>0$ small enough**. Quesion: Characterize all such pairs $A,B$ where the one sided directional derivative $$  (d\sqrt{\cdot})_A(B)=\left. \frac{d}{dt}\right|_{t=0}  \sqrt{A+tB}:=\lim_{t \to 0^+} \frac{ \sqrt{A+tB}-\sqrt{A}}{t} \, \, \text{ exist.}$$ Partial results: (1) For $A=0$ the limit exists only for $B=0$. (2) For every $A=B$ the limit exists (and equals $\frac{1}{2}\sqrt{A}$). (3) For every $A$, there are a ""lot"" of matrices $B$ for which the limit does not exist (see the answer of loup blanc): By orthogonal diagonalization, after noting that $(d\sqrt{\cdot})_A(B)$ exists $\iff (d\sqrt{\cdot})_{V^TAV}(V^TBV)$ exists for every $V \in O_n$, we reduce to the case $A=diag(0,a_2,\cdots,a_n)$ where $a_i\geq 0$: 
For every matrix $B$ of the form $B=diag(\lambda,C)$ where $\lambda > 0,C\in M_{n-1}$ is symmetric s.t. $diag(a_2,\cdots,a_n)+C\geq 0$, 
$(d\sqrt{\cdot})_A(B)$ does not exist. In this question , it is shown that $\sqrt{\cdot} $ is not differentiable in the standard sense. In fact, this answer shows $\psym$ is not a manifold (with boundary) at all, so ""standard differentiability"" does not really make sense here. **
Actually, as noted by loup blanc, if $B$ is a symmetric matrix such that $A+B \in \psym$, then $A+tB \in \psym$ for small enough $t$. Indeed, $A,A+B \in \psym$ and $\psym$ is convex, hence $(1-t)A+t(A+B)=A+tB \in \psym$. Note that the reverse implication is false in general; It can happen that $A+tB \in \psym$, but $A+B \notin \psym$, see here .","['derivatives', 'real-analysis', 'matrix-calculus', 'positive-definite', 'linear-algebra']"
1884540,similar matrices in analysis,"Suppose $A$ and $B$ are two matrices or bounded operators such that $A=\lim_{n\to\infty} A_n$ and $B=\lim_{n\to\infty} B_n$ for a sequence of matrices or operators $A_n$ and $B_n$. (The limit is to be understood as the matrix or operator norm.) Suppose in addition that for each $n$, $A_n$ is similar (i.e. conjugate) to $B_n$. Does it follow that $A$ is similar to $B$? I think the answer is positive at least in some cases, for instance when $A_n$ and $B_n$ are matrices and are orthogonally similar. I wonder what general statements can be proved (even by modifying the conditions, modes of convergence, etc) for this setting and in what cases counterexamples can be constructed. Any help is appreciated.","['functional-analysis', 'operator-theory']"
1884546,Components vs Quasicomponents,"Forgive me if this question has been asked before, but I did a quick search and nothing came up. My book ( Geometry and Topology by Bredon) Defines components of a topological space $X$ as The collection of equivalence classes of the equivalence relation ""$p$ and $q$ belong to a connected subset of $X$"" while it defines quasicomponents as The collection of equivalence classes of the equivalence relation ""$d(p)=d(q)$ for every discrete valued map $d$ on $X$"" While I understand these definitions, I do have one misunderstanding: I don't understand how these concepts are different. The theorem A topological space is connected iff every discrete valued map is constant seems to show that they are one and the same: Let $[\,\cdot\,]_C$ be the equivalence classes defined in the former definition and let $[\,\cdot\,]_Q$ be the equivalence classes defined in the latter definition. $(\Rightarrow)$ (this direction is apparently true) If $[x]_C$ is the component in $X$ with $x$ as its representative, then for any $y\in [x]_C$, $x,y$ both belong to a connected subset of $X$. By the above we must then have $d(x)=d(y)$ for all discrete valued maps $d$. Thus $y\in [x]_Q$ and $[x]_C\subseteq [x]_Q$. $(\Leftarrow)$ If $z,w\in [x]_Q$ are chosen arbitrarily, then $d(z)=d(x)$ and $d(w)=d(x)$ for all discrete valued maps $d$. But then $d(z)=d(w)$ for all discrete valued maps, so the above shows that $[x]_Q$ is connected, and hence $[x]_Q\subseteq [x]_C$. The $(\Leftarrow)$ direction is evidently wrong. But why? A few months ago I encountered this same issue and apparently overcame it, but I forgot how. Thanks in advance.","['general-topology', 'connectedness']"
1884558,Is there a simple geometric example of unequal left and right cosets?,"Does anyone know a simple geometric example of unequal left and right cosets? Specifically I am looking for a non-abelian group which can be realized in $\mathbb{R}^2$ or $\mathbb{R}^3$ whose subgroups and thus cosets are easy to pictorially visualize. Such a visualization hopefully would make the fact that cosets partition the group, even in the non-abelian case, easier to internalize. What I have tried so far: I suspect that something like this should be possible to draw simply for one of the symmetry/dihedral groups, although I am not familiar with them enough to think off the top of my head how this would be done. For example, all of the cyclic groups are abelian. The best candidate seems like the dihedral group of order 6, since it is the smallest non-abelian group. However, I do not really know how to visualize it, its elements, or its cosets. https://en.wikipedia.org/wiki/Dihedral_group_of_order_6 The examples given here for non-abelian cosets are all non-geometric, as opposed to the affine space example which presents itself readily for abelian cosets. Perhaps something using the cosets of SO(3) (rotations in $\mathbb{R}^3$) might also be possible. The abelian case is very simple (and hopefully will clarify somewhat what I am looking for): Affine spaces are precisely the cosets of vector spaces when considered as abelian groups under their addition operation. (See e.g. Wikipedia: https://en.wikipedia.org/wiki/Affine_space#Examples ) http://groupprops.subwiki.org/wiki/Left_coset_of_a_subgroup#Examples_in_abelian_groups This leads to a nice visual way to think of (abelian) cosets -- as the algebraic generalization of (hyper)planes or lines which don't pass through the origin. (image from Wikipedia) I am looking for a similarly nice visual/geometric way to think of non-abelian cosets.","['examples-counterexamples', 'finite-groups', 'abstract-algebra', 'soft-question', 'group-theory']"
1884573,"If $V\subset L^\infty[0,1]$ with $\|f\|_\infty \leq c\|f\|_2$, then $V$ is finite dimensional","If $V$ is a linear subspace of $L^\infty[0,1]$ with $\|f\|_\infty \leq c\|f\|_2$ for all $f\in V$, then $V$ is finite dimensional. The proof is an explicit calculation: Since $L^\infty[0,1] \subset L^2[0,1]$, take $e_1,\cdots , e_n$ to be $L^2$-orthonormal vectors in $V$. Fix some $x$ in $[0,1]$. We have, for all $y\in[0,1]$,
$$\left|\sum e_i(x)e_i(y)\right| \leq \left\|\sum e_i(x) e_i(\cdot) \right\|_\infty \leq c\left\|\sum e_i(x)e_i(\cdot)\right\|_2 = c \sqrt{ \sum e_i^2(x)},$$
take $y = x$ this implies 
$$\sum e_i^2(x) \leq c^2.$$
Integrate both side and we get
$$  n=\int_0^1 \sum e_i^2(x) \leq c^2.$$
This proof is simple but it is not really intuitive for me. Could you guys help me with a more functional analysis argument of this? Since for functions in $L^\infty[0,1]$ we always have $\|f\|_2 \leq  \|f\|_\infty$ , paired with $\|f\|_\infty \leq c \|f\|_2$, this means that the identity map is a continuous bijection between $(V, \|\cdot \|_2)$ and $(V, \|\cdot \|_\infty)$. Also $\|\cdot\|_2$ and $\|\cdot\|_\infty$ are equivalent on $V$. I know any two norms are equivalent in a finite dimensional space, but I dont know if there is anything special about the $L^2$ and $L^\infty$ norm to make the converse of that true as well.","['functional-analysis', 'normed-spaces', 'real-analysis', 'lp-spaces']"
1884586,Cipolla's algorithm for finding square roots - why is first step fast?,"I'm reading about the Cipolla algorithm to find the square root of some quadratic residue n in $\mathbb{F}_p$ - https://en.wikipedia.org/wiki/Cipolla%27s_algorithm . The first step of the algorithm is to keep picking a random $a \in \mathbb{F}_p$ until you find one with $a^2 - n$ not a square. The couple of references I've found online suggest this should be quick, and that about half of all possible $a$ will satisfy this. I can't see why this is necessarily the case, as the distribution of $a^2 - n$ isn't going to be uniform across $\mathbb{F}_p$, so might not be distributed as an even split across quadratic residues / non-residues. Can anyone give me a hint as to why this holds? Thanks.","['number-theory', 'abstract-algebra']"
1884607,"""Does everyone want coffee?"" propositional logic problem","When three professors are seated in a restaurant, the hostess asks them: “Does everyone want coffee?” The first professor says: “I do not know.” The second professor then says: “I do not know.” Finally, the third professor says: “No, not everyone wants coffee.” The hostess comes back and gives coffee to the professors who want it. How did she figure out who wanted coffee? The solution is that the first two professors who say they don't know if everyone wants coffee are the ones who want coffee. However, I don't understand the logic. For example, the first professor may not want coffee, but does not know if the other professors want coffee, and thus can answer ""I do not know."" So how can the hostess take an answer of ""yes"" from that?","['propositional-calculus', 'logic', 'discrete-mathematics']"
1884664,"Is $C(X,S)$ $\sigma$-compact ($X$ compact, $S$ is a complete metric space)?","Suppose $X$ is a compact metric space and $S$ is a complete separable metric space. Let $\mathcal{C}(X,S)$ be the space of all continuous functions from $X$ into $S$. Prove that $\mathcal{C}(X,S)$ is $\sigma$-compact.
(I read this statement on Kallenberg's book Foundations of Modern Probability , where it is stated as a consequence of Arzela-Ascoli's theorem. A proof of this escapes me.)","['stochastic-processes', 'general-topology', 'analysis']"
1884694,Is $\ 7!=5040\ $ the largest highly composite factorial?,"A highly composite number is a natural number $\ n\ge 1\ $ that has more divisors than any smaller natural number $\ m\ge 1$. Checking the $\ 1000\ $ entries in the OEIS-sequence, I noticed that only the following factorials are highly composite : $$[1, 2, 6, 24, 120, 720, 5040]$$ Is it known whether $\ 7!=5040\ $ is the largest highly composite factorial ? The factorials have some conditions necessary for a number to be highly composite : They contain the first $k$ prime numbers as prime factors. The exponents are non-increasing. The exponent corresponding to the largest prime factor is $\ 1$. So, it is not obvious whether the above list is complete.","['number-theory', 'factorial', 'divisor-counting-function']"
