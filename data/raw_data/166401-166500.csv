question_id,title,body,tags
2899139,A property of the x-ray transform,"Problem: Let $P_{\theta}f(x) = \int_{\mathbb{R}}f(x + s \theta) ds$ be defined as the x-ray transform, where $\theta \in S^{n-1}$, and $x$ belongs to $\Theta^{\perp}$, the hyperplane that passes through the origin and is orthogonal to $\theta$. Now, why is it that $\widehat{P_{\theta}f}(\xi) = 0$ for $\xi \in \Theta^{\perp}$ implies that $P_{\theta}f = 0$ ($\hat{f}$ means Fourier transform of $f$)? This result occured in the paper ""Practical and Mathematical Aspects of the Problem of Reconstructing Objects from Radiographs"" (Theorem 4.2 proof), the author did not give any explanation for it. Attempt at an explanation: I was originally thinking maybe the injectivity of the Fourier transform was playing a role here, but I can't see how it can actually be true on a hyperplane instead of $\mathbb{R}^n$. Using the Fourier inversion formula also doesn't seem to give me $P_{\theta}f = 0$, since $\widehat{P_{\theta}f}(\xi)$ only vanishes on $\Theta^{\perp}$ instead of on all of $\mathbb{R}^n$. Am I missing something very obvious here? Any help is appreciated!","['fourier-analysis', 'image-processing', 'analysis', 'real-analysis']"
2899169,"Is $\prod_{1\leq i< j\leq n} \frac{a_i - a_j}{i-j}$, with distinct integers $a_i$, an integer?","It is known that for every $n$ consecutive integers, their product is divisible by $n!$, since $${{m}\choose{n}} = \frac{m!}{n!(m-n)!}$$ is also an integer. So is it true that for every distinct integer $a_1, a_2, ..., a_n$, the number $$S = \prod_{1\leq i< j\leq n} \frac{a_i - a_j}{i-j}$$ is also an integer? (Sorry for my grammar mistakes, English is my second language)","['number-theory', 'algebra-precalculus', 'combinatorics', 'geometry']"
2899202,Find the expected value of the additional pickups?,I find a probability problem in a computer game. The problem is as following. The player has a passive ability that has $28$% chance every $27$ second to grant random buff. There are 6 types of buff and they have equal chance to occur. The desired buff lasts $120$ second and it doubles($\times2$) the pickups of the player during the period. The desired buff effect can be piled and the player earns $\times2^n$ pickups for n numbers of the desired buff. What is the expected value of the additional pickups? Someone has given the following solution: Chance of random buff$ = 28$% Time between trying buff$ = 27$s Number of buff types$ = 6$ Desired buff$ = $“Double Pickups” Duration of the desired buff$ = 120$s Average time it takes to proc “Double Pickups”$ = (27$s$ \times 1/0.28) \times 6 = 578$s$ = 9.64 $min How often is the “Double Pickups” active $ = 120$s$ / 578$s$ = 20.76$% of the time How much pickups does one gain over all $ = 1 + (1 \times 0.2) = 1.2 = 120$%$ = 20$% more pickups overall I don't think this is the right solution. May someone tell me if it is correct or not? What is the correct answer? Please explain.,"['statistics', 'proof-verification', 'probability-distributions', 'expected-value', 'probability']"
2899209,Sampling error of correlation coefficient (Phi coefficient) for binary variables,"Suppose I have two correlated binary variables (A and B) with known probabilities ($p_a$ and $p_b$) and correlation ( Phi ) coefficient in population - $\rho$ . Is there any analytic function for sampling error of correlation coefficient if I have finite sample from the population? I've tried to use Fisher z-transformation, with sampling variance estimation of $1/N$ but it gives inappropriate (lower than actual) estimation of variance. As my simulations show, sampling error of Phi coefficient is a function of population correlation coefficient, frequencies of both binary variables and of course sample size. Thanks in advance!","['statistical-inference', 'statistics', 'correlation']"
2899219,Prove that there are infinitely many primes $p$ such that $x^{10} + x + 1 \equiv 0 \mod p$ has at least one solution $x\in\mathbb{Z}$. [duplicate],"This question already has answers here : $x^2-x+1$ has a root $\!\bmod p\,$ for infinitely many primes $p$ (3 answers) Closed 4 years ago . Prove that there are infinitely many primes $p$ such that $$x^{10} + x + 1 \equiv 0 \mod p$$ has at least one solution $x\in\mathbb{Z}$. I believe I should be doing a proof by contradiction but I cannot figure out where it arises. Any help will be appreciated! thank you!",['number-theory']
2899233,Conclusions about derivability of a function $f(x)=x\left|{\log{x}}\right|$,"I want to study the derivability of this function $$f(x)=x\left|{\log{x}}\right|$$ My textbook says the function is defined for $x>0$ (easy to understand for me, the argument of the logarithm must be positive) and it says: ""it can certainly be derived for $x\neq 1$"". I wonder how my textbook reached this conclusion without deriving the function first. I'm aware derivatives are defined like this: $$\lim_{h\rightarrow0}{\frac{f(x_0+h)-f(x_0)}{h}}$$ Although I can't understand how we can reach conclusions about derivability just by looking at the function. Any hints?",['derivatives']
2899285,"Let $f : R \to R$ be twice differentiable, $f (x+\pi$) = $f(x)$, $f''(x) + f(x) \geq 0$ $∀$ $x\in {R}$. Prove that $f(x)\geq0$ $∀$ $x\in {R}$","I came across this question: Let $f : R \to R$ be a twice differentiable function such that $f (x+\pi$) =  $f(x)$ and $f''(x) + f(x) \geq 0$ $∀$ $x\in {R}$. Prove that $f(x)\geq0$ $∀$ $x\in {R}$ My teacher suggested me to define a function $g: R\to R$ 
for a real number $a\in R$, where $g(x) = f'(x+a) \sin x - f(x+a)\cos x$ $\therefore g'(x) = \sin x(f(x+a) + f''(x+a))\geq 0$ $∀$ $x\in [0,\pi]$ Hence for every $a\in R$ $g(\pi) - g(0)\geq 0$ $\because$ $g(\pi) -g(0) = f(a+\pi)+f(a) = 2f(a)$ $\therefore$ $2f(a) \geq0$ $\therefore$ $f(a) \geq0$ My Problem: Everything seems okay with the method he suggested, but how do I know how to define $g(x)$ the way he did. Is there any other way to do this question? If not, can someone please help me in identifying why my teacher went for that particular definition of $g(x)$? Thanks for comments/suggestions.","['calculus', 'functions', 'derivatives']"
2899380,Let $f(x) = x^3-3x^2+6$ for $x\in \mathbb{R}$ and $g(x) = \max\{f(t); x+1\leq t\leq x+2\}$ for $-3\leq x\leq0$. Need help in understanding $g(x)$.,"I came across this question in my book: Let $f(x) = x^3-3x^2+6$  for $x\in {R}$ and $g(x) = \max\{f(t); x+1\leq t\leq x+2\}$ , for  $-3\leq x \le0$ and $1-x, x\geq 0$. Test continuity of $g(x)$ for $x\in [-3,1]$. My Problem I have not been able to attempt the problem since I am stuck on the first step: Identifying the function. I have absolutely no idea how $g(x)$ looks like for $x\in [-3,0]$. I need help in understanding this part:
$$g(x)=\max\{f(t); x+1\leq t\leq x+2\}$$ Thanks for comments/suggestions.","['continuity', 'calculus', 'functions', 'derivatives']"
2899395,Can we characterize the equivalence classes of matrices up to left multiplication by an orthogonal matrix?,"Let $M_n$ be the space of $n \times n$ real matrices, and consider the following equivalence relation on $M_n$: $A \sim B$ if there exist $Q \in O(n)$ such that $A=QB$. Can we characterise nicely the equivalence classes of this relation? By comparison, the usual ""orthogonal equivalence"", i.e. $A \sim B \iff A=QBU$ for some $Q,U \in O(n)$ gives rise to the notion of SVD-i.e. each equivalence class corresponds to a specific finite sequence of singular values . Thus, the singular values are the ""invariants"" which classify the equivalence classes. Is there any sensible way to associate a list of classifying invariants to the new relation in a similar way? Of course, we shall need ""more invariants"" to distinguish between different classes: It will be something like ""the same singular values+something else"". In particular, we have $A \sim B \implies \ker A=\ker B$, but for invertible matrices this does not really add any information. Even finding some non-trivial sufficient conditions for when $A \sim B$ would be interesting: Having the same singular values is certainly necessary , but is far from sufficient, which can be checked directly even at dimension $2$. ($\Sigma$ and $\Sigma Q$ are not equivalent generically).","['equivalence-relations', 'orthogonal-matrices', 'linear-algebra', 'invariant-theory', 'singular-values']"
2899422,Non-Linear (Second Order) Differential Equation,I need some hints for solving $yy''-(y')^2=xy^2$. I noticed that the left hand side is close to $(yy')'$: $yy''-(y')^2=xy^2\ \Leftrightarrow\ yy''+(y')^2-2(y')^2=xy^2\ \Leftrightarrow\ (yy')'-2(y')^2=xy^2$. But I don't know how to continue expressing the terms as derivatives of some functions. Thanks,"['ordinary-differential-equations', 'real-analysis']"
2899428,solving the Thomas-Fermi-Dirac equation for planetary interior structure modeling,"I am currently working on implementing high pressure equations of state to model interiors of planets and exoplanets. To this end, what I need to do is to numerically solve the Thomas-Fermi-Dirac equation for a given material. The problem I have is purely mathematical, i.e. I am having trouble solving the differential equation. For the simpler case of the Thomas-Fermi equation I successfully proceeded in analogy to S. Esposito 2001 (link 1). After lengthy research I found in F. Ren 2014 (link 2) that they transformed the TFD equation and applied a 4th order Runge-Kutta scheme to solve it. I am, however, struggling to practically apply this idea my self. The equation I seek to solve is: $$ {d^2 \phi}/{d x^2} = x(\epsilon + (\phi/x)^{1/2} )^3$$ With the initial conditions: $$ d^2\phi/dx^2|_{x_0} = \phi(x_0)/x_0, \ \ \ \phi(0) = 1 $$ The first problem I have is, that the rhs depends on x and $\phi$ itself rather than on x and the first derivative, which would probably be easier. The second thing is that the initial conditions are given for the second derivative and the function itself at two different points. Of course, transforming the equation into a first order one would considerably simplify things. The simplest substitution $ f (\phi,x) = d \phi /dx$ would yield an integral term on the rhs so another approach seems to be required. Since I have no experience with 2nd order nonlinear differential equations I would appreciate it if some one could guide me through this one. 1: https://core.ac.uk/download/pdf/25325537.pdf 2: https://pdfs.semanticscholar.org/f0b8/0210ae2c2da807cd874c5e8c4c37091a4a46.pdf","['numerical-methods', 'ordinary-differential-equations']"
2899437,"Why is $[fX,gY]=fg[X,Y]$ wrong? ( $[]$ denotes the Lie bracket)","It is well-known that $[fX,gY]=fg[X,Y]+fX(g)Y-gY(f)X$  where $f,g$ are scalar functions and $X,Y$ are vector fields. However, it is also easy to demonstrate that the Lie bracket is anti-commutative and linear in the first argument. Why is this reasoning wrong? $$[fX,gY] = f[X,gY] = -f[gY,X] = -fg[Y,X] = fg[X,Y]$$ I'm sure I'm making a mistake, but I can't see where. I am using only anti-commutativity and linearity in the first argument. Where is the mistake?","['lie-derivative', 'smooth-manifolds', 'differential-geometry']"
2899438,Blow-up and gluing coordinates,"I am reading the book ""Algebraic Geometry and Statistical Learning Theory"" by Sumio Watanabe and have a question regarding Remark 3.16 (1) on page 95. He defines the blow-up of $ \mathbb{R}^2$ with center $V=\lbrace 0\rbrace$ as $B_V(\mathbb{R}^2)=\overline{\lbrace (x,y,(x:y))\in\mathbb{R}^2\times\mathbb{P}^1\vert (x,y)\in\mathbb{R}^2\setminus V\rbrace }$ where $\mathbb{P}^1$ denotes the real projective line. Then after using the typical identification of $(x:y)$ with $(1:z)$ where \begin{align} z=\begin{cases}\frac{y}{x}\,,\quad x\neq 0\\ \infty\,,\quad x=0\end{cases}\end{align} some easy computation shows that $B_V(\mathbb{R}^2) = \lbrace (x,y,z)\vert y=zx\rbrace\subset\mathbb{R}^2\times\mathbb{P}^1$. This approach is understandable for me. However in the remark the author writes that the above blow-up is equivalent to the substitution
\begin{align}
x=u=st\,,\quad y=uv=s
\end{align}
and then gluing the two coordinates $(u,v)$ and $(s,t)$. Unfortunately there are no more comments regarding this procedure. As far as I understand this, he defines two sets \begin{align}
U_1=\lbrace (u,v)\vert u,v\in\mathbb{R}\rbrace\,\quad U_2=\lbrace(s,t)\vert s,t\in\mathbb{R}\rbrace
\end{align} and then sets $U=U_1\sqcup U_2/\sim$ where $\sim$ is defined by the above two equations. But I have no idea how $U$ is the same as $B_V(\mathbb{R}^2)$. Could someone please explain me this approach? Best regards","['blowup', 'coordinate-systems', 'algebraic-geometry', 'quotient-spaces', 'manifolds']"
2899450,Prove that the standard deviation is less than half the range,"Taken from ""An Introduction to Quantum Physics"" by Stefanas Trachanas (Question 2.4). I need to prove that for an arbitrary statistical distribution: $$
\Delta A ≤ \frac{|a_{max}-a_{min}|}{2}
$$ Where $a_{min}$ & $a_{max}$ are the minimum & maximum possible values of the distribution,respectively. I have proved this for 2 special cases: The case in which the mean is at the centre of the range, & the case with only two possible outcomes. How do I prove the general case?","['statistics', 'standard-deviation', 'inequality']"
2899455,Distance between incentre and excentre of a triangle,"I am unable to get anywhere regarding the distance between the incentre and an excentre of $\triangle ABC$. I do know that it is generalised as: $$IJ_c=c\sec\left(\frac C2\right)$$
  For the incenter $I$ and excenter $J_c$, where side $c$ is opposite $\angle C$. But how can I prove this? Kindly assist.",['trigonometry']
2899480,Why the fact that surjective map has an inverse at right is equivalent to axiom of choice?,"Suppose $f:A\to B$ is surjective. By definition for all $b\in B$, there is $a_b\in A$ s.t. $b=f(a_b)$. Define $g:B\to A$ by $$g(b)=a_b.$$
Clearly, $g$ is a well defined function, because $g(b)$ has no two different value, and $g(b)\in A$ for all $b$. Therefore, for all $b\in B$,
$$f(g(b))=f(a_b)=b,$$
and thus $g$ is invertible at right. Where did I used Axiom of choice ?","['elementary-set-theory', 'axiom-of-choice', 'functions']"
2899491,"Bernoulli trials (n,p) - probability for even/odd number of successes","I came across this problem. It asks what is the probability of even number of successes in a series of $n$ Bernoulli trials with probability of success in each trial equal to $p \neq \frac{1}{2}$. Knowing the formula for $k$ number of successes I was able to immediately write down this: $$ p_{\ even} = \sum_{k=0}^{\lfloor\frac{n}{2}\rfloor}{n \choose 2k} p^{2k}(1-p)^{n-2k} $$ I think that is correct, right? But after that when I saw the solution which they give in the book, I found (as I suspected by the way) that there's a closed form formula and it goes. $$ p_{\ even} =  \frac{1}{2} + \frac{1}{2} (1-2p)^{n}$$ The proof is simple, one basically argues inductively over $n$. This made me think (and this is my question here)... Is there an algebraic way to prove the following equality? I mean some way which simply manipulates these binomial coefficients and uses some of their known properties. $$ \sum_{k=0}^{\lfloor\frac{n}{2}\rfloor}{n \choose 2k} p^{2k}(1-p)^{n-2k} = \frac{1}{2} + \frac{1}{2} (1-2p)^{n} $$","['combinations', 'binomial-coefficients', 'combinatorics', 'probability-theory', 'probability']"
2899500,One dimension equivalent to area,Some say that length is the $\mathbb R^1$ equivalent of $\mathbb R^2$ area or $\mathbb R^3$ volume but you can substract areas or have intersections of areas. I need to do the same in one dimension but I don't which term to use. Is length really the good term for that?,['geometry']
2899523,Confusion with the proof that the Cantor set is closed,"I have encountered the definition of Cantor set and its property. Cantor set is constructed by removing the middle third open set of each interval . So each time we get some union of closed sets. As $F_0=[0,1]$ $F_1=[0,1/3]\cup [2/3,1]$ $F_2=[0,1/9]\cup [2/9,1/3]\cup [2/3,7/9]\cup [8/9,1]$ So on , That for $F_n$ we get the Union of $2^n$ closed interval . I know that finite union of closed interval is closed, but this is not true for arbitarly union. As I had a counterexample:  $\cup$ {{1/n}|for $n$ $\in N$} as this is not closed as $0$ is not in  that union. So what is the argument here to say that $F_n$ is closed for any $ n$ . As we are using this fact to prove the Cantor set to be closed as $F=\cap_{n\to \infty} F_n$ i.e arbitarly intersection of closed set is closed . Where am I misunderstanding? Any help will be appreciated.","['general-topology', 'cantor-set', 'analysis', 'real-analysis']"
2899559,"What are the gradient, divergence and curl of the three-dimensional delta function?","The three-dimensional delta function is defined as follows: $$\delta(\mathbf{r}-\mathbf{r'})= 0 \;\; \mathrm{for} \;\;\mathbf{r}\neq\mathbf{r'} $$
$$\delta(\mathbf{r}-\mathbf{r'})= \infty \;\; \mathrm{for} \;\;\mathbf{r}=\mathbf{r'} $$ $$\int_V\delta(\mathbf{r}-\mathbf{r'})\;dV= 1 .$$ By definition also:
$$\int_V f(\mathbf{r})\delta(\mathbf{r}-\mathbf{r'})\;dV= f(\mathbf{r'}) $$. I am wondering what the vector calculus operators are for the delta function. For example: Curl: $\nabla \times \delta(\mathbf{r}-\mathbf{r'})$ Divergence: $\nabla \cdot \delta(\mathbf{r}-\mathbf{r'})$ Gradient: $\nabla \delta(\mathbf{r}-\mathbf{r'})$ Any insight or references are appreciated. Please note that I am a geophysicist by training and it has been a few years since I have taken any vector calculus classes!","['divergence-operator', 'dirac-delta', 'curl', 'multivariable-calculus', 'vector-analysis']"
2899664,Finding equivalence classes under a relation,"The question is: Define a relation $\rho$ on $\mathbb{Z}$ such that $a\ \rho \ b\ $ iff $a=b$ or $a,b$ are positive. Show that the relation is an equivalence and find the equivalence classes. My solution: Let us write $\mathbb{Z}$ as $\mathbb{Z^+ \cup {Z^-}^*}$
.
If $a \in \mathbb{Z^+}$ then $a \ \rho \ b$ implies $b \in \mathbb{Z^+}$. Similarly for other set. We then proceed by showing the reflexivity [ $a \ \rho\ a \ \forall a \in \mathbb{Z}$ since if $a \in \mathbb{Z}$ then a is positive and relates to itself and if $a \in \mathbb{{Z^-}^*}$, $a=a$ ] , symmetry [ leaving out the case when $a \in \mathbb{Z^+}$ and  $b \in \mathbb{{Z^-}^*}$ as in this case a does not relate to b ], and transitivity [ following similar methods]. The disjoint classes I end up with are  $\mathbb{Z^+}$, $\{i:i \in  \mathbb{{Z^-}^*}$} i.e $\{0\}, \{-1\}...,\{-n\},...$","['elementary-set-theory', 'equivalence-relations', 'relations']"
2899679,How can one define ordinary derivative of a vector field along a curve?,"Definition 1: A curve $C$ on a manifold $M$ is a smooth function $C:(a,b) \rightarrow M$. Definition 2: A vector field $v^a=v^a(t)$ along a curve $C$ is an assignment of vectors on the tangent space at each point $C(t)$ on the curve. Definition 3: Let $v^b$ be a vector field on $M$. The derivative operator $\partial_a v^b$ is defined by taking partial derivative at each component of $v^b$, given that a fixed coordinate system is chosen. Definition 4: $ v^a$ is said to be parallelly transported along the curve $C$ if $ t^a \nabla_a v^b=0$. In Wald's General Relativity, the derivative operator $\nabla_a$ is defined on vector field on the manifold, but still the above is valid by defining similar thing for vector fields along a curve. Theorem: Suppose a coordinate system is fixed. The following are equivalent. (1) A vector $ v^a$ is parallelly transported along a curve $C$. (2) $t^a\partial_a v_b+t^a {\Gamma^c}_{ab} v_c=0$ [Tensor form] (3) $\displaystyle\frac{dv^\nu}{dt}+\sum_{\mu,\lambda=1}^n t^\mu {\Gamma^\lambda}_{\mu\nu}v_\lambda=0$ [Component form] This confuses me a lot. In particular, we are unable to define the derivative operator on vector field along a curve in the manner in definition 3. To make sense of taking partial derivatives at each component, formally speaking, it is defined as partial derivatives of the composition of the components and the parametrization of the manifold (which makes sense because the components themselves are formally functions from $M$ to $\mathbb{R}$). However, the curve may have self-intersection and two different tangent vectors at the same point might be defined for a vector field along a curve, which means the components now cannot be a function from $M$ to $\mathbb{R}$. So we cannot make composition with parametrizations now in order to allow us to define partial derivatives. Therefore, I am unable to understand the meaning of statement (2) in the above theorem. Can anyone give me some clue?","['derivatives', 'general-relativity', 'vector-fields', 'differential-geometry']"
2899699,Number of n long sequences using combinatorics,"The prompt asks us to find the number of n-long sequences of letters 'a', 'b', 'c' , where letter 'a' cannot appear on odd numbered positions, positions start from number 1. The way I tried solving it was,
Since there are $\lceil \frac{n}{2} \rceil$ odd positions and $n - \lceil \frac{n}{2} \rceil$ or $\lfloor \frac{n}{2} \rfloor$ even positions. Even positions can be occupied in $3^{\lfloor \frac{n}{2} \rfloor}$ ways and odd positions can be occupied in $2^{\lceil \frac{n}{2} \rceil}$ ways, putting both together we have $$3^{\lfloor \frac{n}{2} \rfloor} \cdot 2^{\lceil \frac{n}{2} \rceil} \text{ ways of choosing n sequences}$$ Is this the right way to solve this problem? Can this be solved using inclusion-exclusion principle? should it?","['combinatorics', 'discrete-mathematics']"
2899702,Question on Hartshorne II.6.1,"I found the proof of Hartshorne II.Lemma 6.1 very confusing. I will break down places I'm confused by. First, (*) means we have a noetherian, integral, separated scheme which is regular in codimension one. The Lemma states: Let $X$ satisfy (*) and let $f \in K^{*}$ be a regular function on $X$. 
  Then $v_{Y}(f) = 0$ for all but finitely many prime divisors on $Y$. The statement makes sense. The proof goes: Let $U$ be an open affine subset of $X$ on which $f$ is regular. Then $Z = X - U$ is a proper closed subset of $X$. Since $X$ is noetherian, $Z$ only contains at most finitely many prime divisors $Y$. Question 1: Is this because $X$ is irreducble and $U$ is open, thus dense. Then every prime divisor contained $Y$ must be contained in an proper open set of $Z$, which must intersect $U$. Hence if there where infinitely many prime divisors in $Z$, then we could have constructed an open set not intersecting $U$? It will be sufficient to show that there are only finitely many prime divisors $Y$ of $U$ for which $v_{Y}(f) \ne 0$. Since $f$ is regular on $U$, $v_{Y}(f) \ge 0$ in any case. Question 2: Why so? We only know that either $f$ has non-negative valuation or its inverse. And $v_{Y}(f) > 0$ iff $Y$ is contained in a proper closed subset of $U$ defined by the ideal $Af$ in $A$. Since $f \ne 0$, this is a proper closed subset of $U$, hence only contain finitely many closed irreducible subsets of codimension one in $U$. Question 3: I'm not following either of the sentences here. Many thanks!","['divisors-algebraic-geometry', 'algebraic-geometry']"
2899735,On the translation between a morphism of schemes and its geometric description,"Within the theory of schemes, rather than formally defining a morphism of schemes (saying which prime goes to which, and what the pullback map looks like), we often give the geometric intuition behind it. I am having difficulties translating between these two perspectives. Easier example. Let's work in $\mathbb{R}$. I want to formalize what 'the projection of the parabola $y = x^2$ to the $y$-axis' means. Within the world of schemes, I guess that means you want a morphism from $\operatorname{Spec} \mathbb{R}[x,y] / (y - x^2)$ to $\operatorname{Spec} \mathbb{R}[y]$. Which one? It suffices to pick the right ring map $\mathbb{R}[y] \to \mathbb{R}[x,y] / (y-x^2)$. The obvious choice would be the inclusion map, so that's going to be what you want. To be honest the only reason I 'get' this is because this is the only reasonable choice. Harder example. Consider the circle of radius $1$ around the origin. We want to find rational points on this circle. One of them is the point $p = (1,0)$. If $q$ is another rational point, consider the line going through $p$ and $q$, and compute the slope of this line. We end up with a map taking the coordinates $q = (x,y)$ and sending it to the slope $y/(x-1)$. Somehow this should yield a rational map from the circle to the affine line, i.e. a map $\operatorname{Spec}\mathbb{Q}[x,y] / (x^2 + y^2 - 1) \dashrightarrow \mathbb{A}_{\mathbb{Q}}^1$. Which rational map should it be? It beats me. Intuitively I understand what the map should do. I also understand that the formula we described gives the behaviour on closed points. But I'm not able to translate the intuition to a full fledged rational map. Question. What should the map be in the harder example? What I tried. The problem that I'm facing is that we are looking for a rational map, and not a morphism, so we cannot consider a corresponding map of rings. In this case, we're lucky that the birational 'inverse' of this map is an actual morphism, and this morphism should correspond to a ring map $\mathbb{Q}[x,y] / (x^2 + y^2 - 1) \to \mathbb{Q}[z]$. I can't think of a reasonable choice. One might also try and define the desired rational map on small affines and patch them together but this seems more tedious, and I end up getting stuck with the same problem. Follow-up question. What is the general procedure for going from a geometric description to an actual morphism, and how do we know when
  it suffices to describe the behaviour on the closed points? Edit. I would also be very happy with a reference. Unfortunately no book I know of seems to refer to this 'bridge'. Vakil formally defines morphisms of schemes but then proceeds to give 'examples' of morphisms of schemes by just providing coordinate maps.* Hartshorne and Mumford give examples in the world of varieties (where coordinate maps are fine because you only have the closed points to deal with) but forget about them when moving to schemes. Eisenbud gives only very basic ones where there's really only one reasonable choice of ring map. Liu does not really bother with examples. *: That said, he is the only one who acknowledges the existence of the 'bridge'. I quote: ""Suppose you are looking for rational points on the circle $C$ given by $x^2 + y^2 = 1$. One rational point is $p = (1,0)$. If $q$ is another rational point, then $pq$ is a line of rational (non-infinite) slope. This gives a rational map from the conic $C$ to $\mathbb{A}^1_{\mathbb{Q}}$, given by $(x,y) \mapsto y / (x-1)$."" He then mentions: "" Something subtle just happened: we were talking about $\mathbb{Q}$-points on a circle, and ended up with a rational map of schemes. "" But he doesn't say what this 'something subtle' is supposed to be, and that's precisely the detail that I've been desperately looking for for a long time. What is it?","['algebraic-geometry', 'schemes', 'geometry', 'commutative-algebra']"
2899757,Is this complex function differentiable at 0?,"I'm moving my first steps in Complex Analysis. I can't tell whether the function
$$f(x+iy) = 2|xy|+i(y^2-x^2)$$
is differentiable at $0$ or not. I tried using the limit of the difference quotient :
$$\lim_{z\to 0}\frac{2|xy|+i(y^2-x^2)}{x+iy}$$ The limit is $0$ when $xy>0$ because $x=-iy$ is a root of the numerator, but I can't compute the limit in general...",['complex-analysis']
2899765,Finding the value of $\frac{13}{a-13}+\frac{23}{b-23}+\frac{42}{c-42} $ given a system of three equations,"Let $a, b, c, x, y, z$ be real numbers that satisfy the three equations $$ 13x+by+cz=0 $$
$$ ax+23y+cz=0 $$
$$ ax+by+42z=0 $$ Suppose that $ a\neq13 $ and $x\neq0$. What is the value of $$\frac{13}{a-13}+\frac{23}{b-23}+\frac{42}{c-42} $$ I tried $$ (13-a)x+(b-23)y=0 $$
$$ (23-b)y+(c-42)z=0 $$
$$ (13-a)x+(c-42)z=0 $$ $$ (a-13)x=(b-23)y=(c-42)z $$ But I don't know how to continue further Maybe $$ \frac{1}{(a-13)x}=\frac{1}{(b-23)y}=\frac{1}{(c-42)z} $$ But any hint will be appreciated",['algebra-precalculus']
2899778,Sufficient statistic based on a random sample of size n where the pmf is $P_{\theta}(X=x) = c(\theta)2^{-x/\theta}$,"so the question asks to find a sufficient statistic based on a sample of size n, where $X \sim f_{\theta}(x)$ and 
\begin{equation}
f_{\theta}(x) = P_{\theta}(X=x) = c(\theta) 2^{-x/\theta}, \quad x = \theta, \theta+1,..., \theta >0
\end{equation}
and 
\begin{equation}
c(\theta) = 2^{1-1/\theta}(2^{1/\theta}-1)
\end{equation}
I derived that the joint pmf is 
\begin{equation}
P_{\theta}(X_1 = x_1,...,X_n = x_n) = c(\theta)^{n}2^{-1/\theta \sum x_{i}}\mathbb{I}_{\{X_{(1)}\geqslant \theta\}}
\end{equation}
My question is, in this case, is a sufficient statistic $T(x) = (\sum x_{i}, X_{(1)})$ or both $\sum x_{i}$ and $X_{(1)}$ can serve as a sufficient statistic? 
Thanks so much for your time and consideration.",['statistics']
2899829,"Evaluating $\int \cot x \csc^2x \,\mathrm{d}x$ with $u=\cot x$","$\newcommand{\d}{\mathrm{d}}$ Evaluate the integral using the indicated substituion. $$\int \cot x \csc^2x \,\d{x}, \qquad u= \cot x .$$ Differentiating both sides of $u$, then making the substitution: $$
\begin{align}
u &= 
\phantom{-}\cot x, \\
\d u &= -\cot x\csc x \,\d{x}, \\
\d x &= -\frac{\d u}{u \csc x}.
\end{align}$$
$$\int -\frac{u\csc^2 x \,\d{u}}{u\csc x} = \int -\csc x \,\d{u}. $$ Apparently, this was not an adequate approach, because $x$ is still part of the integrand. What should be done instead?","['integration', 'indefinite-integrals', 'calculus', 'trigonometry']"
2899910,Matrix Multiplication not associative when matrices are vectors?,"Wikipedia states: Given three matrices A, B and C, the products (AB)C and A(BC) are
  defined if and only the number of columns of A equals the number of
  rows of B and the number of columns of B equals the number of rows of
  C (in particular, if one of the product is defined, the other is also
  defined) Row and column vectors can be thought of as just special cases of matrices. So given the above I would expect: $$(a^Tb)c = a^T(bc)$$ However the right side is undefined because you can’t multiply two column vectors, seemingly contradicting Wikipedia. Am I mistaken? If not, can we only consider matrix multiplication to be associative in contexts where we know no intermediate matrix becomes 1x1?","['matrices', 'linear-algebra', 'vectors', 'associativity']"
2899921,Unique coverage,"Given a collection $A$ of $m$ sets $S_1,\ldots,S_m$ (i.e., $A=\{S_1,\ldots,S_m\}$) such that $|S_1 \cup \ldots \cup S_m| =n$. In other words, $S_1,\ldots,S_m$ covers a ground set of $n$ elements. Is there a constant $0<c<1$ such that for arbitrary $A=\{S_1,\ldots,S_m\}$,  one can always find $B \subseteq A$ such that the number of elements that belong to exactly one set in $B$ is at least $c n$? Conjecture: for $c=1/100$, we can always find a $B \subseteq A$ such that the number of elements that belong to exactly one set in $B$ is at least $n/100$.",['combinatorics']
2899959,Find all $n\in\mathbb{N}$ such that the nonzero roots of $(z+1)^n-z^n-1$ are all on the unit circle.,"I try to use Mathematica to find such $n$, and I believe that only $n=2,3,4,5,6,7$ meet the requirement but I don't know how to prove that for $n\ge 8$, there is always some root lying outside the unit ball by using the basic complex analysis knowledge. In fact, I do have a dirty way to deal with this problem, which only involves the property of polynomial. But I don't think it is an efficient and elegant solution.","['complex-analysis', 'polynomials']"
2899964,"Partial derivatives of $f$ exist, but only $n-1$ of them are continuous, implies differentiability","The problem I am working on is stated as follows. Let $A \subset R^2$ be open and $(a_1,a_2)\in A$. Suppose $f:A\to R$ satisfies that $\frac{\partial f}{\partial x_1}(a_1, a_2)$ exists,
  and that $\frac{\partial f}{\partial x_2}$ is continuous in $A$. Prove that $f$ is differentiable at $(a_1, a_2)$. My professor gave the title as a remark to the proof that if all partials exist and are continuous in $A$ then $f$ is differentiable. I am having trouble proving it even for this specific case that I am working with. I used an argument involving the mean value theorem to show that 
$|f(y)-f(x)-\sum_{i=1}^2\frac{\partial f}{\partial x_i}(x_1,x_2)*(y_i-x_i)|$
$\leq ||y-x||*[|\frac{\partial f}{\partial x_1}(u_1,y_2)-\frac{\partial f}{\partial x_1}(x_1,x_2)|+|\frac{\partial f}{\partial x_2}(x_1,u_2)-\frac{\partial f}{\partial x_2}(x_1,x_2)|]$ ,where $u_i$ is between $x_i$ and $y_i$. From here I can use the continuity of $\frac{\partial f}{\partial x_2}$ to show that the second term in the inequality is bounded by $\frac{\epsilon}{2}$ but I am unsure of how I can get a nice upper bound for the first term. I realize I skipped a lot of intermediate steps but I am not sure if this is even the correct way to proceed with this proof. If someone could help me with a hint or maybe some intuition on why this statement should be true I would really appreciate it.","['multivariable-calculus', 'real-analysis']"
2899965,Integral of $\int_{-\infty}^{\infty} \frac {dk}{ik+1}$,"I came across this integral today in the context of inverse fourier transforms: $$ R(x)={1 \over 2\pi}\int_{-\infty}^{\infty} \frac {e^{ik(x-1)}}{ik+1}dk$$ I know the solution is supposed to be $$ R(x) = \theta(x-1)e^{-(x-1)} $$ Where $\theta(x)$ is the Heaviside step function. I have worked out the integral with contour integration and residual theorem for $x \gt 1$ and $x \lt 1$, wich work out as $e^{-(x-1)}$ and $0$ respectively. My problem is for $x=0$, where I expect to be $R(0)={1 \over 2}$. The integral would then be: $$R(0)={1 \over 2\pi}\int_{-\infty}^{\infty} \frac {dk}{ik+1}$$
Wich i don't know how to calculate. Wolfram alpha tells me that the integral is in fact ${1\over2}$. My first instinct was to multiply and divide the integrand by $e^{ik}$ and then solve the integral by closing the contour and using the residual theorem; but the residual is $-i$, so it would be  $R(0) = 1$. I know there are different definitions of the Heaviside function, and in some $\theta(0) =1$, but we used $\theta(0) ={1 \over 2}$ the whole course so I find it improbable my professor would use it differently here. Also, Wolfram seems to agree that it should be ${1 \over 2}$. First time posting, so I hope I'm following all the rules.","['complex-analysis', 'contour-integration', 'definite-integrals', 'fourier-transform']"
2899966,Combinatorics distribution problem indistinguishable items in distinguishable boxes,"In how many ways can you put $10$ identical gold coins into four colored boxes so that at least $1$ goes into the blue box, at least $1$ into yellow, at most $2$ into red and at least $3$ into green? The way I solved this was by writing down all the restrictions, 
$$B \geq 1, \space G \geq 3, \space Y \geq 1, \space R \leq 2$$ Where each letter corresponds to the first letter of each colour box. Dividing the problem into 3 cases where there's no coin in red box, 1 coin and 2 coins in red box seems to be the most logical, giving the following results using bars and stars : C1: R box has zero coins $\binom{3 + 5-1}{5} = \binom{7}{5}$ C2: R box has one coins $\binom{3 + 4 - 1}{4} = \binom{6}{4}$ C3: R box has two coins $\binom{3 + 3 - 1}{3} = \binom{5}{3}$ Giving us a final result of $$\binom{7}{5}+\binom{6}{4}+\binom{5}{3}$$ I wanted to know if this is the correct way of solving a problem like this? What if we had bigger numbers including a box having at most say a 1000 coins, would we need to make a 10000 cases? what if there's multiple at most restrictions? like 2 boxes have to have at most 3 gold coins?","['combinatorics', 'discrete-mathematics']"
2899971,"can polynomial functions with rational coefficients approximate any continuous function on $[a,b]$.","The Stone-Weierstrass theorem states that for any continuous function $f$ on $[a,b]$ there exists a polynomial function $p$ such that $\|f-p\|<\varepsilon$ for any $\varepsilon>0$. Where $\|\cdot\|$ is the sup norm. If I replace ""a polynomial function"" with ""a polynomial function with rational coefficient"" from the above statement, will the statement still hold true?",['real-analysis']
2899991,"if $m<\frac{1}{2}$, what is $-\frac{1}{m}$ equal to?","I start out with: $$m<\frac{1}{2}$$
Taking the reciprocal of both side flips inequality sign: $$\frac{1}{m}>2$$
Multiply both side by -1 flips the sign yet again: $$-\frac{1}{m}<-2$$. But the result $-\frac{1}{m}<-2$ is not valid. If $m=-100$, then $-\frac{1}{m}=\frac{1}{100}$ What did I do wrong?",['algebra-precalculus']
2900002,"Simple probability question - how to approach systematically, not overthink, or misinterpret?","I have been studying some probability 'story problems' and surprisingly the ones with very simple solutions tend to confuse me. Particularly, the question below. The question: Losses covered by an insurance policy are modeled by a uniform distribution on the interval [0,1000]. An
  insurance company reimburses losses in excess of a deductible of 250. Calculate the difference between the median and the 20th percentile of
  the insurance company reimbursement, over all losses. (A) 225 (B) 250 (C) 300 (D) 375 (E) 500 If you are young like me and don't know what a deductible is, it means for a loss $L > 0$ the insurance company will pay you a reimbursement $R = \mathrm{max}\{ L -250, 0 \}$ . The solution provided: Before applying the deductible, the median is 500 and the 20th percentile is 200. After applying
  the deductible, the median payment is 500 – 250 = 250 and the 20th percentile is max(0, 200 –
  250) = 0. The difference is 250. I have a strong background in math but not a good foundation in probability. The solution seems so simple, but without seeing the solution I would not have been confident that method is correct and justified. Part of this may be the wording of the questions - does the part 'over all losses' make a difference? I could not figure out what that meant. Here is how I interpreted the question as I read it for the first time: Losses $L$ are a random variable with distribution $L$ ~ $U[0,1000]$ . (This makes the first part of the solution make sense since median and percentile of uniform distribution are trivial.) Reimbursements $R$ are a new random variable and to find the median and 20th percentile I should find how $R$ is distributed. For some particular loss $\ell > 0$ , the reimbursement is given by $r = \mathrm{max}\{\ell - 250, 0 \}$ . Then a PDF for $R$ should be $$p(r) = \cases{ .25, \hspace{5mm} \text{ if } r = 0 \\ 750^{-1}, \hspace{2mm} \text{ if } r \in (0,750]} \, $$ Using this, I would have found a median to be 281.25 by solving for a number $k$ such that $p(0 < r < k) = p(k < r < 750)$ which is inconsistent with the solution provided. My question for you: It seems I am overthinking or misunderstanding. The solution provided seems much simpler. Is there a concept I am missing that I can learn about that would allow me to confidently and quickly answer this question?  Am I misinterpreting the question? Am I misunderstanding the definition of something? It is very important to me to feel like I understand the definitions and justifications. I do not understand why the solution provided is justified, nor would I have solved it that way on my own - so I am unsatisfied with my understanding. Any guidance appreciated.","['statistics', 'mathematical-modeling', 'descriptive-statistics', 'probability-distributions', 'probability']"
2900006,Graph with conditions,"G is a graph whose vertex set is $\{1,2, ... , 82\}$, vertices $i$ and $j$ are adjacent iff $|i-j| \mod 4 = 0 \text{ and } i \neq j$. (a) Calculate the chromatic number of $G$. (b) Is $G$ Eulerian? (c) Is $G$ planar? I'm not sure how to go on about part (a) or part (c), for part (b), I tested cases manually like adjacent vertices of $1$ are $\{5, 9, ... , 81\}$ which are $20$ vertices, since degree of each vertex is even, it is Eulerian. For part (c) I believe it should be enough to prove that there is a $K_5$ or $K_{3,3}$ subgraph but I'm not sure how to do this.","['graph-theory', 'eulerian-path', 'combinatorics', 'discrete-mathematics', 'planar-graphs']"
2900028,Change of angle inside a quirky hexagon,"So I am dealing with the hexagon as shown in the picture below and I need to find out how one angle depends on another angle. More specifically, I need $\frac{d\psi}{d\varphi}$ at $\varphi=0$. Note that except for $\psi$ and $\varphi$, all angles are fixed and that $K$ and $L$ are fixed lengths. The vertex at the green angle $\gamma$ is uniquely defined since we are given four lengths and all six angles of a hexagon. Note that this is true since I know (from context) that the angles are chosen so that the green angle $\gamma$ is not equal to $\pi$ (which would mean that the hexagon is actually a pentagon in which case that point would of course not be unique). Clearly if $\varphi=0$, then the picture is symmetric and $\psi=\gamma/2$. That one is easy. What I need, though, is the rate of change $\frac{d\psi}{d\varphi}$ at $\varphi=0$. I found a solution but it is terribly ugly, so I was wondering if I ignored some basic trigonometry trick that simplifies this issue. Here is my solution. Consider this picture with some added info: By the law of cosines, $c^2=L^2+K^2-2KL\cos(\pi/6+\varphi)$, so we get $c$. By the law of cosines, $d^2=L^2+K^2-2KL\cos(\pi/6-\varphi)$, so we get $d$. By the law of sines, $\frac{\sin(\delta)}{K}=\frac{\sin(\varphi+\pi/6)}{c}$, so we get $\delta$. By the law of sines, $\frac{\sin(\epsilon)}{K}=\frac{\sin(\varphi-\pi/6)}{d}$, so we get $\epsilon$. Now we can combine the following two equations from the law of sines: $\frac{\sin \psi}{c}=\frac{\sin(\alpha-\delta)}{\ell}$ $\frac{\sin (\gamma-\psi)}{d}=\frac{\sin(\alpha-\epsilon)}{\ell}$ Combining them by eliminating $\ell$, we get $\frac{\sin(\alpha-\delta)}{\sin\psi}c=\frac{\sin(\alpha-\epsilon)}{\sin(\gamma-\psi)}d$. Now $\psi$ is the only unknown and I can implicitly differentiate the hell out of these equations and get some unwieldly formula for $d\psi/d\phi$ at $\phi=0$. I had to deal with a similar problem a while ago and at first I had a horrible formula only to later realize if only I had used the law of sines differently in one step, the equation would be much simpler. So I am wondering if there is a better way to do this besides the one above.","['euclidean-geometry', 'angle', 'geometry', 'polygons', 'trigonometry']"
2900029,Question on local coordinates in the moduli space $\mathcal{H}_g(1)$,"Fix $g\ge 2$ and denote by $\mathcal{H}_g(1)$ the moduli space of holomorphic 1-forms on a closed Riemann surface of genus $g$ with one zero of order $2g-2$ . Given any point $\omega\in \mathcal{H}_g(1)$ , the corresponding tangent space $T_{\omega}\mathcal{H}_g(1)$ can be identified with the cohomology group $H^1(X,Z(\omega),\mathbb{C})$ , where $X$ is the Riemann surface where $\omega$ is holomorphic and $Z(\omega)$ is the zero of $\omega$ . Let $B=\{\phi_1,\dots,\phi_{2g}\}$ be a basis of closed complex forms on $X$ which vanish in a neighborhood of $Z(\varphi)$ , from what we just said one can deduce that each $\phi_i$ represents a tangent direction to $\mathcal{H}_g(1)$ in $\omega$ . It is possible to endow $\mathcal{H}_g(p)$ of an hermitian metric $h$ which on each tangent space $T_{\omega}\mathcal{H}_g(1)$ is defined as $h_{\omega}(\phi,\psi):=\frac i 2 \int_X\phi\wedge \overline \psi$ for every $\phi,\psi \in T_\omega\mathcal{H}_g(1)$ . Since the local period maps give local coordinates on $\mathcal{H}_g(1)$ , we can write all elements of $\mathcal{H}_g(1)$ sufficiently close to $\omega$ as $\omega_z:=\omega+ \sum_{i=1}^{2g}z_i\phi_i$ and so $z=(z_1,\dots,z_{2g})\in \mathbb{C}^{2g}$ are complex coordinates near $\omega$ . I would like to write down explicitly the hermitian metric $h$ in these coordinates $z$ , i.e. I would like to compute the coefficients $h_{ij}(\omega_z)$ of $h=\sum_{i,j}h_{ij}(\omega_z)dz_idz_j$ . I've tried to do so, and I obtain constant coefficients $h_{ij}(\omega_z)=\int_X\phi_i\wedge \overline{\phi_j}$ , since the wedge product is the same in each point $\omega_z$ . Am I right to say that the coefficients $h_{ij}(\omega_z)$ are constant? If not, can you point me out where I'm wrong? Thank you!","['algebraic-geometry', 'moduli-space', 'riemannian-geometry', 'differential-geometry']"
2900032,Norm of a functional defined by an integral,"Show that $\phi: L^{\frac{3}{2}}\left(\left(0,\frac{1}{2} \right] \right) \to \mathbb{R}$ defined by $$u\mapsto\int_0^{\frac{1}{2}}u \, dx$$ is linear. Find $\|\phi\|_{\mathcal{L}}$. My try Clearly $\forall u, v\in L^{\frac{3}{2}}\left(\left(0,\frac{1}{2}\right]\right)$ and $\lambda\in\mathbb{R}$ $\\ \phi(u+v)=\int_{0}^{\frac{1}{2}}(u+v)dx=\\ =\int_{0}^{\frac{1}{2}} u \, dx+\int_0^{\frac{1}{2}} v \, dx=\\ =\phi(u)+\phi(v)$ $\phi(\lambda u)=\int_{0}^{\frac{1}{2}}\lambda u \, dx = \lambda \int_0^{\frac{1}{2}} u \, dx=\lambda\phi(u)$ so the linearity of the functional is easy to prove (right?), but what can I do to evaluate its norm? Maybe it can be useful the Holder Inequality? I think it's not the right idea. Any help would be appreciated.","['measure-theory', 'definite-integrals', 'functional-analysis']"
2900040,Compactness / sequentially compact / reflexive space / separable spaces / weak topology,"I'm a bit confuse with all theses notions. Let $E$ a normed vector space of infinite dimension (also Banach, but it's probably not important). The theorem of Eberlin Smulian theorem says that : all bounded sequence that has a subsequence that converge weakly $\iff$ it's reflexive. (In fact it just says implication, but the converse is also true)... anyway. Q1) Does it mean that if $E$ is reflexive, then instead of the fact that the weak topology is not metrizable, the property $C\subset E$ is compact $\iff$ $C$ is sequentially compact hold ? Because in reflexive spaces, $\{x\in E\mid \|x\|\leq 1\}$ is compact. Eberlin Smulian theorem says that $\{x\in E\mid \|x\|\leq 1\}$ is compact $\iff$ it's sequentially compact. Can this be generalized for any compact $C$ ? Q2) If a set is separable, we know that $\{x\in E\mid \|x\|\leq 1\}$ is metrizable for the weak topology. In particular, can we conclude from this that If $E$ is separable, a set $C\subset E$ is compact $\iff$ it's sequentially compact.","['functional-analysis', 'weak-convergence', 'weak-topology']"
2900047,"Using $\epsilon$-$\delta$ approach, prove that $\lim_{(x,y)\rightarrow (1,3)}\frac{x}{y}=\frac{1}{3} $","How to prove that $$ \lim_{(x,y)\rightarrow (1,3)}\frac{x}{y}=\frac{1}{3}$$
By $\epsilon$-$\delta$ definition. What i did is this: Let $\epsilon$ be grater than zero; i need to find a $\delta(\epsilon)>0$ such that $|x/y-1/3|<\epsilon$ whenever $0<\sqrt{(x-1)^2+(y-3)^2}<\delta$. Then, rewriting  $$\bigg|\frac{x}{y}-\frac{1}{3}\bigg|=\bigg|\frac{3x-y}{3y}\bigg|= \bigg|\frac{3x-y+3-3}{3y}\bigg|=
\bigg|\frac{3(x-1)+(-y+3)}{3y}\bigg| $$ Now, by triangle inequality $$\bigg|\frac{3(x-1)+(-y+3)}{3y}\bigg|\leq \frac{3|x-1|+|y-3|}{3|y|}$$
since $$|x-1|<\sqrt{(x-1)^2+(y-3)^2}<\delta \text{ and } |y-3|<\sqrt{(x-1)^2+(y-3)^2}<\delta$$ then $$\frac{3|x-1|+|y-3|}{3|y|}<\frac{3\delta+\delta}{3|y|}=\frac{4\delta}{3|y|}$$ But i have that $|y-3|<\sqrt{(x-1)^2+(y-3)^2}<\delta$, then $y\in (3-\delta,\delta+3)$. So i think the next step is to take $\delta =1/3$; if i do that i get $8/3<y$ that is $1/|y|<3/8$ 
then 
$$\frac{4\delta}{3|y|}<\frac{3}{8}\frac{4\delta}{3}=\frac{\delta}{2}.$$ So, taking $\delta=\min\{1/3, 2\epsilon\}$, we have $$\bigg|\frac{x}{y}-\frac{1}{3} \bigg|< \frac{3}{8}\frac{4\delta}{3} = \frac{\delta}{2}= \frac{1}{2}2\epsilon = \epsilon. $$ is this reasoning right?","['limits', 'multivariable-calculus', 'epsilon-delta']"
2900073,Differentiable Convex Function with multiple global minimum. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Does there exist Differentiable Convex Function with multiple global minimum (except for constant function)?","['convex-optimization', 'functions', 'derivatives']"
2900082,Maximum Modulus Principle Intuition [duplicate],"This question already has answers here : Intuition Behind Maximum Principle (Complex Analysis) (3 answers) Closed 5 years ago . I understand that the Maximum Modulus Principle works, but I'm a little baffled as to why. To be more precise, the picture I have in my head is something like this: for a compact set $K \subset \mathbb{C}$, since $|f|$ (for $f$ holomorphic) can only attain its maximum on the boundary $\partial K$, if you consider a disk centered on the origin, no matter what value $|f|$ obtains on the boundary of this disc, you can just increase the radius of the disc a little bit and find a higher value of $|f|$; in other words, the absolute value of function just keeps growing without bound. What exactly is driving/forcing this growth? I'm looking for some kind of explanation, geometric or otherwise, that could aid my intuition. In particular, why does $\mathbb{C}$ behave so differently from $\mathbb{R}$ here? EDIT. In response to the ""duplicate"" tag: I am looking for something a little deeper than the answers there. As mentioned above, an explanation of the difference in behaviours of $\mathbb{C}$ and $\mathbb{R}$ in this regard, perhaps with a tie-in to the Cauchy-Riemann equations...it seems like there's something that causes functions to behave fundamentally differently over $\mathbb{C}$, and I would like to understand why, with the Maxiumum Modulus Principle as a concrete example.",['complex-analysis']
2900094,Nonstandard Complex Numbers,"There is an aspect of Non-Standard Analysis that for quite some time I cannot get my head around. I have done some research on this platform to avoid an exact double answer, or one that could be derived, but I still find my question unanswered. I'm happy to receive links to the threads I missed. So, one can construct Non-Standard reals, say via an ultrafilter. Then, infinitesimals in the nonstandard realm are the equivalence classes of null sequences. Infinite nonstandard reals are equivalence classes of real sequences that are unbounded (positive and negative sign) For alternating sequences like (-1, +1, -1, +1,...), there is an infinite set in the ultrafilter that does determine which of the +1, -1 it is. So all hyperreals are (representatives of equivalence classes of) sequences. Now, as so many other questions ask, what happens with $\mathbb{C}$?
It is clear that the hyperreals are elementary equivalent to the reals, and categoricity for the complex numbers make them isomorphic to any completion of the hyperreals in that same sense. So what happens to all the infinitesimals? Do the hyperreal non-zero infinitesimals suddenly become hyperreal complex null-sequences? While when going the 'standard way' they should be zero all along? In particular, what is the function $f$ in the following diagram: $\require{AMScd}$
\begin{CD}
    \mathbb{R} @>N>> \mathbb{R}^*\\
    @Vc  V V @VV  CV\\
    \mathbb{C} @>>f> \mathbb{C}\equiv\mathbb{C}^{*}
\end{CD} Here, $N$ is the usual ultrapower embedding, $c$ and $C$ are the respective algebraic completions. The particular question is, what happens to $(1/n)_{n\in\mathbb{N}}$? That is a non-zero infinitesimal, equivalent to zero taking standard parts, so it should be a zero non-standard complex number, but cannot be going the non-standard complex way. What is wrong with this? Or rather, what implicit projection are we missing? And for the set theorists, what kind of cardinal do we need to exist for this to work?","['logic', 'nonstandard-analysis', 'real-analysis', 'complex-analysis', 'set-theory']"
2900106,Coefficient of $x^{27}$ in $(1-x^{10})^6/(1-x)^6$.,It's been a while since I have been playing with these so excuse me if it is too obvious. How can I represent $$ f(x) = \frac{(1-x^{10} ) ^6}{(1-x)^6} $$ as sum of powers of $x$ I am especially interested in the coefficient in front of $x^{27} $ in that sum. The book I am reading gives this coefficient as obviously being $${32 \choose 5}- {6 \choose 1} {22 \choose 5}+{6 \choose 2}{12 \choose 5}$$ but I don't know where this comes from. Many thanks in advance.,"['real-analysis', 'calculus', 'combinatorics', 'sequences-and-series', 'power-series']"
2900112,How many partials need to be continuous to imply differentiability?,"Let $f:\mathbb{R}^n \to \mathbb{R}$. Let $x_0 \in \mathbb{R}^n$. Assume $n-1$ partials exist in some open ball containing $x_0$ and are continuous at $x_0$, and the remaining $1$ partial is assumed only to exist at $x_0$. A well known result states that this implies $f$ is differentiable at $x_0$. My question is whether or not this can be strengthened. Can we replace ""$n-1""$ in the above theorem with some function $g(n)$ ""smaller"" than $n-1$, and replace ""remaining $1$ partial"" with ""remaining $n-g(n)$ partials""? Feel free to play with assumptions slightly. For instance, you can replace ""continuous at $x_0$"" with ""continuous at $x_0$ and in some open ball containing $x_0$"".","['partial-derivative', 'multivariable-calculus', 'real-analysis']"
2900117,Does this manifold have a name?,"EDIT: In an attempt to not let the bounty go to waste, I will consider responses that give reasonable guesses of what the involved surfaces are, WITHOUT requiring parametrizations. While using Mathematica to alter manifolds and numerically verify the Gauss-Bonnet Theorem, I generated the figure whose pictures I've attached in this post (multiple perspectives of the same shape are provided). Does anyone know whether it is a known manifold with a name? I generated this surface by using $h_{x}, h_{y}, h_{z}$ below, setting $t_{2}=t=1, M=0, \phi=\pi/2$ on the interval $0 \leq k_{x} \leq 2\pi$ and $0 \leq k_{x} \leq 2\pi$ : After much reading, my biggest guess is that this is a pseudosphere glued to a duplin cyclide - however, I am having trouble knowing this for sure. I do not even know how I would go about proving this. Note that I plotted some of the normal vectors on its surface and it appears that normal vectors in the pseudosphere-like region point INTO the surface, whereas other normal vectors point out. Perhaps this could be useful information. Since this is a question with a bounty on it now, I should know what this 2D surface is with certainty. For your convenience, I have a link to MATLAB and Mathematica files that have manipulable figures here: https://1drv.ms/f/s!Ak6chxAgMs9Pg_tYixvFy6MfO9531A Here are some animations that show sections of the surface (I change parameters in a fixed frame so that you get certain perspectives inside the surface as it leaves the fixed frame). Here are some static images (essentially screenshots from the models in the link above):","['differential-geometry', 'surfaces', 'non-orientable-surfaces', 'manifolds', 'terminology']"
2900164,Is the following Area of Crescent all right?,"In the figure below. There are two overlapping circles and the area of Crescent in Red that I have found is $A_{C} = \frac{\pi rw}{2}$, where $w$ is the shift from center $'X'$ in blue to $'X'$ in red. Details:
$$A_C = \frac{A_{elipse} - A_{circle}}{2} = \frac{[\pi r^2 + \pi r w] - \pi r^2}{2}$$","['area', 'geometry']"
2900176,How to verify whether or not there exists a vector $x$ such that $Ax > 0$?,"Given a matrix $A$ in $\mathbb R^{m\times n}$, if I want to know whether or not there is an $x \in \mathbb R^{n}$ such that 
$$
Ax > 0,
$$
(meaning all elements in $Ax$ are positive). Is there some easy way to verify whether or not such $x$ exists?","['linear-algebra', 'linear-programming']"
2900211,"If $x=\cos a + i \sin a$, $y=\cos b + i \sin b$, $z=\cos c +i \sin c$, and $x+y+z = xyz$, then show that $\cos(a-b) + \cos(b-c) + \cos(c-a) + 1=0$","If $x=\cos a + i \sin a$, $y=\cos b + i \sin b$, $z=\cos c +i \sin c$, and $x+y+z = xyz$, then show that $$\cos(a-b) + \cos(b-c) + \cos(c-a) + 1=0$$ Here's how I tried it $$x+y+z=xyz $$ So, by De Moivre's Theorem,
$$(\cos a + \cos b + \cos c)  + i(\sin a + \sin b + \sin c) = \cos(a+b+c)  + i \sin(a+b+c) 
$$ Equating real and imaginary parts, 
$$\cos a + \cos b + \cos c= \cos(a+b+c)$$
and similarly for sine. Now, $$(a-b)  + (b-c)  + (c-a)  =0$$ What to do now?  Please help. And please use De Moivre Theorem!",['trigonometry']
2900226,Clarification on countability of $\mathbb{N}^2$ and $2^{\mathbb{N}}$,"I would like some help with developping a better intuitive understanding of why $\mathbb{N}^2$ is countable while $2^\mathbb{N}$ is not. As far as I understand we can write a 1:1 enumerating function for both. I am able to follow Cantor's theorem proof. But I don't know how to directly compare the power set result to that of $\mathbb{N}^2$. The proof is shown in multiple other questions but I cannot find any answer or textbook that compares $\mathbb{N}^2$ with $2^\mathbb{N}$ the way I would like. I understand that for infinite countable $\mathbb{N}$, the Cantor pair function becomes surjective while a 1:1 enumeration of the power set does not. It may be injective but not surjective. Why does it not become surjective the same way as the Cantor pair as we move from a finite subset of $\mathbb{N}$ to the countable infinite superset? For $\mathbb{N}^2$, I can use the Cantor pair function (a bijection) to enumerate all possible pairs. For a finite set the function is not surjective but for an infinite set it is. Now, for $2^\mathbb{A}$, we could build a 1:1 function, also not surjective for finite sets, as follows (consider $A=\{c,b,a\}$):
$$
\begin{array}{c c}
0 \mapsto \{\} & 000\\
\hline
1 \mapsto \{a\} & 001\\
2 \mapsto \{b\} & 010\\
3 \mapsto \{b,a\} & 011 \\
\hline
4 \mapsto \{c\} & 100\\
5 \mapsto \{c,a\} & 101 \\
6 \mapsto \{c,b\} & 110 \\
7 \mapsto \{c,b,a\} & 111
\end{array}
$$
We simply convert the enumerating integer to binary and each zero or one indicates presence of the element in that position (of $A$).
In the above, we would quickly need huge numbers ($2^n$) but so what?The Cantor pair function is also not surjective (we need $n^2$) for a finite set. I am probably overlooking something.. please point it out to me EDIT:
Thanks to the answer and comments below, there is actually a simple example I see now: with Cantor's function we prove surjectivity going from $\mathbb{N}^2 \rightarrow \mathbb{N}$ by showing we can find some pair $\in \mathbb{N}^2$ to pruduce any $n \in \mathbb{N}$. But in the above power set scheme, all we have to do is show one example where it does not work. So consider for example the infinite set of all even numbers in $2^{\mathbb{N}}$. It is a subset of $\mathbb{N}$ so it is in the power set. But we cannot use the above enumeration when the set has infinite length.","['elementary-set-theory', 'real-analysis']"
2900235,Prove that a function is not differentiable with $\epsilon - \delta$,"We have this function $$f(x) =
\begin{cases}
x^2,  & \text{if $x$ $\in \mathbb{Q}$} \\[2ex]
0, & \text{if $x$ $\in\mathbb{I}$}
\end{cases}$$ We have to determine where the function is differentiable and calculate the derivative. Then we need to determine where is not differentiable and prove this with $\epsilon - \delta$ Attempt: We define:
$$\text{$h(x)=0$ and $g(x)=x²$, then $h(0)=0=g(0)=f(0)$}$$
Now we have:
$$\text{$h(x)$ ≤ $f(x)$ ≤ $g(x)$, for all x$\in\mathbb{R}$}$$
$$\text{that is equal to: $h(x)-h(0)$ ≤$ f(x) - f(0) $≤$ g(x) - g(0)$}$$ 
$$\text{And without losing generality assuming that $x>0$:}$$
$$\text{$\frac{h(x)-h(0)}{x}$ ≤ $\frac{f(x)-f(0)}{x}$ ≤ $\frac{g(x)-g(0)}{x}$}$$
$$\text{Finally: $\lim_{x\to 0} \frac{h(x)-h(0)}{x}$ ≤ $\lim_{x\to 0} \frac{f(x)-f(0)}{x}$ ≤ $\lim_{x\to 0} \frac{g(x)-g(0)}{x}$}$$
$$\text{By the squeeze theorem we have that:}$$
$$\lim_{x\to 0} \frac{f(x)-f(0)}{x}=0$$
$$\text{So $f$ is differentiable at $c=0$ and $f'(0)=0$}$$ The problem comes when we have to show that is not differentiable at any other point. I managed to do it with limits but I don't know how to put in into $\epsilon - \delta$ $$\text{Let $x\in\mathbb{Q}$ and $x≠0$. Then $f(x)=x²$. Now we know that exist a sequence $(y_n)_{n\in\mathbb{N}}$ of irrational numbers such that:}$$
$$\lim_{n\to \infty}y_n = x$$
$$\text{Also we have that $f(y_n)=0$ for all $n$, because $y_n$ is irrational, but:}$$
$$\lim_{n\to \infty}f(y_n) = f(x) = x²$$
$$\text{So we can see that:}$$
$$\lim_{n\to \infty}f(y_n) ≠ f(y_n)$$
$$\text{This implies that $f$ is not continuous at $\mathbb{I}$, therefore $f$ is not differentiable at $\mathbb{I}$}$$
$$\text{The same thing works to prove that $f$ is not continuous at $\mathbb{Q}$ so $f$ is not differentiable at any point, except $x=0$.}$$ As I said the problem comes when I have to write that second thing with $\text{$\epsilon - \delta$ }$, cause I don't really know how to start. Someone has any ideas? Thanks to everyone.","['epsilon-delta', 'continuity', 'calculus', 'limits', 'derivatives']"
2900263,Holomorphic function having finitely many zeros in the open unit disc,"Suppose $f$ is continuous on the closed unit disc $\overline{\mathbb{D}}$ and is holomorphic on the open unit disc $\mathbb{D}$. Must $f$ have finitely many zeros in $\mathbb{D}$? I know that this is true if $f$ is holomorphic in $\overline{\mathbb{D}}$ (by compactness of the closed unit disc), but I'm not sure of what happens when I just consider $\mathbb{D}$.",['complex-analysis']
2900320,When does a bijection of topologies induce a homeomorphism of spaces?,"If two topological spaces $(X, \tau)$ and $(Y, \tau')$ are homeomorphic, we have a bijective correspondence between $\tau$ and $\tau'$ via $U \in \tau \mapsto f(U) \in \tau'$ where $f: X \to Y$ is a homeomorphism. Are the reasonable conditions to be imposed to a bijection $\Gamma : \tau \to \tau'$ so that it implies that $X$ and $Y$ are homeomorphic? I'm not necessarily asking for a homeomorphism $g : X \to Y$ to verify $\Gamma(U) = g(U)$, but for conditions on $\Gamma$ that imply the existence of some homeomorphism.","['general-topology', 'soft-question']"
2900406,"Understanding the concept of isomorphism between Hom(V,W) and $M_{m\times n}$","I'd appreciate a clarification about the following issue.
It's known that Hom(V,W) is isomorphic to $M_{m\times n}$
Correct me if I'm wrong, but as I get it, the meaning of the above statement is that every linear transformation from V to W is represented uniquely by an mxn matrix, and vice versa.
However, I'm having a hard time understanding something. Since we are free to choose any bases for V and W, consequently we get different representation matrices. How does it not contradict the statement mentioning the isomorphism, according to which, as I get it (and probably not correctly), there is a unique respective matrix? Thanks in advance!","['matrices', 'vector-space-isomorphism', 'linear-transformations']"
2900414,Why I can't divide $\frac{d^3}{dx^3}$ and $\frac{d^2}{dx^2}$,"I had an differential equation exam last semester. There was a question I never found out how I could answer. At a part of the question I faced: $\frac{u'''}{u''} = \cot(x)u+x$ And it was asked to make it first-order Linear and then solve it. I simply wrote $\frac{u'''}{u''}= u'$. Then wrote it like $u' - \cot(x)u = x$, 
but I know the first part is not correct at all. I never had the chance to ask the teacher or anyone what should one do in this situation? I think I had a confusion and it's root is the notation
when you could write $\frac{dy}{dx}$ as $(D)y$ so $\frac{d^3y}{d^3x}$ could be $(D^3)y$. Is there a way to lower this equations order?",['ordinary-differential-equations']
2900416,What's the difference between almost surely and surely?,"It might be a duplicate but what's the difference between almost surely and surely? I have heard that the concept of ""almost"" is relatively new in probability theory, but I never understood the core concept that differences it. From what I have heard, it's linked to the measure in the probability tribu that defines a probability function, but it doesn't help me much. Is ""surely"" even defined with new probability theory? Why did they added an extra word? In this article , Wikipedia states that : ""In probability experiments on a finite sample space, there is no difference between almost surely and surely. However, the distinction becomes important when the sample space is an infinite set, because an infinite set can have non-empty subsets of probability zero."" I don't understand this sentence. Maybe someone can help me understand  this concept and maybe provide an example?","['measure-theory', 'probability-theory', 'probability']"
2900420,probability of concordance and discordance,"Define the probability of concordance $(\pi_c)$ and probability of discordance $(\pi_d)$. Obtain an unbiased estimate of $\tau = \pi_c-\pi_d$ I know what is concordance and discordance (usually use it to find Kendall's tau). Knowing that, I guessed that probability of concordance is $P(X_1>X_2, Y_1>Y_2)$ or something like this. But I don't really rely on guess. What I want is a article/book where it is documented or if someone gives answer of this specific question here (that will be very helpful), so that I ca answer the question clearly. Anyway, thanks for any help.","['statistical-inference', 'statistics', 'probability', 'reference-request']"
2900452,Partition of Set: Proof,"Let $f : A\to B$. If $\{B_1,B_2,\dots,B_n\}$ is a partition of $B$, prove that $\{f^{-1}(B_1),f^{-1}(B_2),\dots,f^{-1}(B_n)\}$ is partition of $A$. I approached it like following:
Since $f^{-1}$ exists $f$ must be one one and onto. So for each $x$ in $A$ there is one distinct image in $B$ under $A$. Converse, ""for each $y$ in $B$ there is distinct pre-image under $f$"". This implies that we can define a set $A_i \subseteq A$ which is the set of all elements in A whose image lies in $B_i$ under the $f$ i.e., $A_i=_f^{-1}(B_i)$. Since $f$ is onto $f$ covers all of $B$ and hence union of $A_i$s is equal to $A$. Since $B_i$ is non-empty set, there exist a $y$ in $B_i$ such that $f(x)=y$. This means that $x$ is pre-image of $y$ under $f$. Hence $x$ belongs to $A_i$. So $A_i$ is non empty set. Now since $B_i \cap B_j$ is empty if $i\ne j$, them there is no $y$ such that it belongs to both $B_i$ and $B_j$. Now $f$ is one one so there is no pre-image $x$ such that it belongs to both $A_i$ and $A_j$. So $A_i \cap A_j$ is empty. Hence set of all $A_i$s form a partition of $A$. I think my logic is correct but can someone help me writing it in formal way. Also since there is one one correspondence between equivalence relations and partitions, how to approach this using relations.","['set-partition', 'equivalence-relations', 'functions', 'discrete-mathematics', 'elementary-set-theory']"
2900453,Why is $\left(I - M M^T \right)^{-1} M = M\left(I - M^T M \right)^{-1}$,"While trying to generalize a certain formula from the scalar case to the matrix case, I came across the following curious observation: Let $M$ be a real, not necessarily square matrix, then
$$\left(I -  M M^T \right)^{-1} M = M\left(I -  M^T M \right)^{-1}$$
which seems to be true in general. It holds, for example, for this randomly chosen case
$$M=\left(
\begin{array}{ccc}
 -1 & 7 & -4 \\
 -1 & 5 & 7 \\
 -4 & 7 & 4 \\
 -2 & -5 & 0 \\
 2 & -1 & -6 \\
\end{array}
\right)$$ Although numerical examples convinced me that this is true, I do not understand why. Any hints?","['matrices', 'matrix-equations']"
2900468,Why does this ln function inverse?,"I'm going through applications of separable equations and came across an example of half-lives: $$M(t)=\frac{M}{2}=Me^{-kt}$$ Factoring out $M$, $\frac{1}{2}=e^{-kt}$. To solve for $t$, $\ln\left(\frac{1}{2}\right)=-kt$. And then the answer is $t=\frac1k{\ln 2}$. Why isn't the value for $t=-\frac1k{\ln \frac12}$? Thanks:)","['calculus', 'logarithms']"
2900519,Limiting the expected number of geometrically distributed variables I need to look at,"I am given a series $X_1, X_2, \dots X_n$ of geometrically distributed (with $p = 0.5$), independent random variables, for some $n$. Since there seem to be multiple definitions around: Think of each $X_i$ as the number of coin flips until you see ""heads"". Let $\hat{X} = \max_i(X_i)$. What I want is an upper bound (in $n$) on the expected number of geometrically distributed random variables (with $p = 0.5$) $X'_1, X'_2, X'_m$ that I need to look at before I see a value of at least $\hat{X} + 1$. In other words: I draw $n$ such random variables and determine the maximum. Now, in the second step, I keep on drawing such random variables until I see a value larger than the previously determined variables. How many variables do I expect to draw in the second step? My assumption is: $m \leq 2n$ or $m \le 3n$. I have two approaches of which I feel like they should show this, but for both I'm not sure whether they hold / how to correctly bound $m$. Approach 1 : Symmetry and Handwaving I feel like there should be symmetry at play here. Say I wasn't looking for a value larger than $\hat{X}$, but at least as large as $\hat{X}$. Concatenate both series together to form $X_1, X_2, \dots X_n, X'_1, X'_2, \dots, X_m$. Now, the values of the individual $X_i$ (resp. $X'_i$) are indepentent of my choice of $n$ - so why should the distance ""to the left"" from $X_n$ to $\hat{X}$ be larger than the distance ""to the right"" from $X_n$ to the next element of at least the same value as $\hat{X}$? For symmetry reasons, we should expect $m = n$ here. Now I'm not looking for an element of the same value, but of strictly greater value. Since $P[X_i = \hat{X}] = 2 \cdot P[X_i = \hat{X} + 1]$, the values I'm looking for should be halve as densely distributed… thus we should expect $m = 2n$ … right? At this point I'm waving my hands really hard and hope for the best. Is this approach valid? Would you (as a reviewer / reader) accept this in a paper, if I used it to prove some property of some data structure? Approach 2 : Actually do the Maths One more rigorous approach I thought of is this: From this answer I know that the expected maximum of $n$ such $X_i$ (let's call it $M(\{X_i\})$) is $$E(M(\{X_1, \dots X_n\})) = \sum_{k \geq 0}\left( 1 - \left(1 - \frac{1}{2^k}\right)^n\right)$$ I feel like if I find an $m$ such that $E(M(\{X'_1, \dots X'_m\})) - E(M(\{X_1, \dots X_n\})) \ge 1$, then I should have proven my point - right? I chose $m = 3n$. The resulting sum from the above can then be simplified to: $$\sum_{k \ge 0}\left( \left(1 - \frac{1}{2^k} \right)^n - \left(1 - \frac{1}{2^k} \right)^{3n} \right) \ge 1$$ However, at this point, I seem to not have paid attention in my calculus classes, or just don't see how to bound this series. Does anybody see a way of showing the above? I would be happy with any $m = cn$ for a constant $c$. I plotted it for $m = 3n$ (and $0 \le k \le 1000$), and the left side is well above $1$ for all $n \geq 1$. Even if I could show that: Is my approach (""if I chose $m$ such that the expected values differ by at least $1$, I have found the upper bound I'm looking for"") valid? Thanks a lot!","['probability-distributions', 'calculus', 'sequences-and-series', 'limits', 'probability']"
2900521,Counting Hamiltonian cycles in a graph,"Given a graph $G = (W \cup U, E)$ where $W = \{w_1, w_2, ..., w_n\}$, $U = \{u_1, u_2, ..., u_n\}$ and $E = \{\{w_i, u_j\}: 1 \leq i, j \leq n\}$. The task is to calculate the number of Hamiltonian cycles in the graph G. The way I tried solving was, Our vertex set $V = \{w_1, w_2 ... w_n, u_1, u_2, ... u_n\}$ Since we know 2 vertices $w_i \in W \text{ and } u_j \in U$ are adjacent if $1 \leq i, u \leq n$, this makes every vertex in W adjacent to every vertex in U giving us a bipartite graph. Also by Dirac's theorem
$deg(v) \leq \frac{|V|}{2}$ for a graph to be Hamiltonian. So the task at hand condenses down to finding the number of hamiltonian cycles in a complete bipartite graph with n and n vertices i.e. $K_{n,n}$. Which is $\frac{n!(n-1)!}{2}$, taken from here . Is this the right way to solve this problem or am i missing something?","['graph-theory', 'proof-verification', 'hamiltonian-path', 'combinatorics', 'discrete-mathematics']"
2900526,What is the intuitive meaning of the Heyting algebra?,"I'm curious about this. As we all know, classical logic can be described as what is called the Boolean Algebra , in particular, you can think of figuring out the truth of a classical composite statement like ""P and Q"" from the truth values of its components or atomic statements, by applying truth tables to look up the values of the connectives. However, one with a bit more initiation will realize that classical logic is, of course, not the only logic out there - there are others as well, such as the intuitionistic logic in which we try to reject the law of the excluded middle, i.e. that ""P OR NOT P"" must be tautological, and that there may be $P$ where it may not attain truth (Actually, it is one such logic, there are others you can make as well that do the same.). In particular intuitionistic logic is strictly weaker than classical logic meaning that it will never affirm anything that classical does not and anything it does affirm will be affirmed in classical - which also implies that the law of excluded middle is never actually falsified , just not always affirmed , for falsifying the law is equivalent to affirming its negation, i.e ""NOT (P OR NOT P)"", and thus it is tautological in IL that ""NOT NOT (P OR NOT P)"", but the double negation doesn't cancel. Given the ease of the ""truth table"" paradigm, a natural question arises as to whether you can do the same thing for the intuitionistic logic, and it turns out that the answer is, in a sense, yes: but it requires what amounts to a truth table involving not just an infinite, but an uncountably infinite (namely $\beth_1$ - same as the real line!) amount of truth values - which are not , as you might naively think, reals between 0 and 1 as though representing a ""middling"" truth like in another non-classical system called ""fuzzy logic"", at least not in any straightforward way, but rather are something muucchh weerder : in particular, the ""truth values"" are effectively all open sets of the real line (though this may not be the only model but I believe any model has to be at least this big!), where ""false"" is the empty set and ""true"" the whole line. The great stringy middle is just that - the ordering is by inclusion, and this leads to a complex, fibrated structure (in infinitiye-dimensional infinityey infispace!) instead of the naively-expected continuum ""in the middle"". (E.g. one ""fiber"" of truth values is the one formed by the sets $(-a, a)$ with $a \in [0, \infty]$ (no the last bracket is NOT a typo and YES that makes sense) while a different ""fiber"" would be $(100 - a, 100 + a)$ with the same range of $a$ after at least a suitably great ""depth"" $\frac{1}{a}$.) So my question is why is this? Why is it that the structure of truth values that we must accept, following some relatively simple modifications or ""weakenings"" of the classical logic, to make the truth tables, must ""suddenly"" explode to not just a large or even infinite but this enormously infinite set that even more has a structure which to me it seems at least you could never guess a priori? Moreover, is there any ""meaning"" that can be assigned to these weird truth values?","['general-topology', 'heyting-algebra', 'logic', 'intuitionistic-logic']"
2900545,Construct Two Linearly Independent Power Series Solutions to $(1+z^2)u''+3zu'+u=0$,"I am trying to construct two linearly independent, power series solutions to the ODE $$(1+z^2)u''+3zu'+u=0$$ My attempt: Let $$u=\sum_{k=0}^{\infty}A_kz^k\implies u'=\sum_{k=1}^{\infty}kA_kz^{k-1}\implies u''=\sum_{k=2}^{\infty}k(k-1)A_kz^{k-2}$$ Substituting this into the ODE, I find that 
$$\sum_{k=0}^{\infty}\left((k+2)(k+1)A_{k+2}+(k+1)^2A_k\right)z^k=0$$
Hence $u$ is a solution iff $$A_{k+2}=-\frac{k+1}{k+2}A_k \ \ \ k\geq 0$$
I do not see how to proceed, especially considering there are no intial conditions to find $A_0$ and $A_1$. Any advice would be greatly appreciated.","['power-series', 'proof-verification', 'ordinary-differential-equations']"
2900638,Glueing of algebraic surface along two intersecting curves,"My question arises as part of understanding an analogue of the normalization of singular curves. Assume that $C$ is such a curve and that $p\in C$ is a singular point of $C$ (and the only one for simplicity). Then the normalization $\widetilde C\to C$ glues two points of $\widetilde C$, or, to rephrase this a little, it glues $\widetilde C$ along two points. (This is what happens to a nodal curve. If $C$ is cuspidal, then a point together with a tangent direction is mapped to a point but let us stick to the nodal case, again for simplicity). Now, let us consider the following: Let $S$ be a surface having a curve singularity along a smooth curve $C$, say a node . Then the normalization of $C$ is a map from a smooth surface $\widetilde S$ to $S$ that maps two curves $C_1,C_2\cong C, C_1\cap C_2=\emptyset$ in $S$ to the curve $C$. Again, we may rephrase this and say that it glues $\widetilde S$ along the curves $C_1,C_2$. The condition $C_1\cap C_2=\emptyset$ comes from the fact that the curve $C$ in $S$ is smooth. As just indicated, if the curves $C_1,C_2$ on $\widetilde S$ intersect, then I expect the curve $C$ which is the singular locus of $S$, to be singular as well. Also the condition $C_1,C_2\cong C$ should become $C_1,C_2\cong \widetilde C$ in this case so that we still have smooth curves in $\widetilde S$. ($\widetilde C$ denotes the normalization of $C$.) My question now is the following: 0) Following the suggestion by @Get Off The Internet, given a smooth surface $S'$ and a smooth curve $C'$ together with a $2-1$ map from $C'$ to another curve $C$, which may have singularities, does there exist a surface S having singularities along C, whose normalization is the original surface $S'$? 1) Can we say anything about the singularity of $S$ at the image of the intersection point $C_1\cap C_2\in \widetilde S$? If not, how about the singularity of $C$ in this point? 2) The normalization $\widetilde S\to S$ provides a desingularization of $S$ and it is minimal due to the universal property of the normalization. If we embedded $S\hookrightarrow V$ where $V$ is a smooth variety, then we can blow-up $V$ several times and end up with $\widetilde S\hookrightarrow \widetilde V$ where $\widetilde V$ denotes the blown-up $V$. It is clear that over $C_1,C_2$ there lies a $\mathbb P^1$-bundle that is contracted. Just think of the case of a nodal curve $C$. However, this is much less clear over the point of intersection of $C_1,C_2$. I expect that over this point the $\mathbb P^1$-bundles just intersect and nothing bad happens but I can't see any formal reason for that. Can anyone help me with that? 3) Is there a way to make our lifes simpler, i.e. to make the curves $C_1,C_2$ disjoint again? Probably by blowing up $\widetilde S$ in the point of intersection of this curves? Any help will be greatly appreciated. Comments on the problem are also very welcome. Here is a kind of minimal example that I have in mind: Let $C$ be a smooth curve and consider the product $\widetilde S = C\times C$ together with the projection $S\to C$. Whether on the first or the second factor doesn't matter but let us say its on the first factor. Then we have two sections of $S$. The first one is the section $s_p: q\mapsto (q,p)$ (for $p\in C$) and the other one is $d: q\mapsto (q,q)$. Let us denote their images by $S_p,D$, respectively. Obviously, $S_p,D\cong C$ and they intersect in the point $(p,p)$ and nowhere else. If we glue them, we get a surface $S$ that we may still consider as sitting over $C$. The fibres of $S\to C$ are then nodal curves degenerating to a cuspidal one (=the fibre over $p$). However, I'm really uncertain about singularities of $S$ and how its singular locus looks like. My impression is that I have nodes along $C\setminus\{p\}$ and that the singular locus is a cuspidal curve whose normalization is $C$. But I am unable to give a formal argument for this and therefore I am not even sure whether this is correct. Also whether this reflects the general situation is not clear to me. I just thought this example might help. This does also lead me to the formulation of part 3) of my question. Blowing up $\widetilde S$ in $(p,p)$ separates the two sections $S_p$ and $D$. But then I added a curve in $\widetilde S$ which comes from the exceptional divisor of the blow-up and looks more complicated to me. At least my confusion increases. Even partial answers or some intuition what might be happening will be useful to me.","['algebraic-geometry', 'deformation-theory']"
2900639,Prove that $t\mapsto \frac{d}{dt}\left[\rho(t)\right]^{-1}=-\rho(t)^{-1}\rho'(t)\rho(t)^{-1}$,"Let $\rho:(a,b)\to \operatorname{ISO}\left(\Bbb{R}^n\right)$ be differentiable. I want to prove that 
\begin{align}\frac{d}{dt}\left[\rho(t)\right]^{-1}=-\rho(t)^{-1}\rho'(t)\rho(t)^{-1}\end{align} 
$\operatorname{ISO}\left(\Bbb{R}^n\right)$ is a space of continuous linear maps from $\Bbb{R}^n$ to $\Bbb{R}^n$. I have proven before that \begin{align}f:\operatorname{ISO}\left(\Bbb{R}^n\right)\to \operatorname{ISO}\left(\Bbb{R}^n\right)\end{align} 
\begin{align}u\mapsto u^{-1}\end{align}is differentiable and
\begin{align}f'(u)(h)=-u^{-1}h u^{-1}\end{align} 
 Along the line, I used Neumann's Lemma but I got stuck when proving this. Here is my work: I tried by using definition: Let $h\in(a,b),$ then
\begin{align}\rho(t+h)-\rho(t)&=\rho(t+h)^{-1}-\rho(t)^{-1}\\&=\left[\rho(t+h)-\rho(t)+\rho(t)\right]^{-1}-\rho(t)^{-1}\\&=\left[\left[\left(\rho(t+h)-\rho(t)\right)\left(\rho(t)\right)+I\right]^{-1}-I\right]\rho(t)^{-1}\end{align}
I'm thinking of applying Neumann's Lemma but I can't move on. I want someone to help see my fault. Otherwise, alternative methods will be highly regarded. Thanks!","['analysis', 'real-analysis', 'multivariable-calculus', 'uniform-convergence', 'convergence-divergence']"
2900657,"$I$ is infinite, $A_k$ is countably infinite, and $A_i$ is countable for all $i \neq k$. Is $\prod\limits_{i\in I}A_i$ countable?","The Cartesian product of a family $(A_i\mid i\in I)$ is defined as $$\prod\limits_{i\in I}A_i=\{f:I\to\bigcup A_i\mid f(i)\in A_i \text{ for all } i \in I\}$$ Let $(A_i \mid i \in I)$ be a family of non-empty indexed sets where $I$ is infinite, $A_k$ is countably infinite, and $A_i$ is countable for all $i \neq k$. Is $\prod\limits_{i\in I}A_i$ countable? I found that it's not too hard to conclude when $I$ is finite or when $A_k$ is uncountable. Please give me some hints in this case!",['elementary-set-theory']
2900668,How many ways a tie breaker happens in a football game?,"Assumptions are: One team wins, maximum of 5 kicks for each team,  team A and B, team A do the first kick. My Try : Case 1- Team A wins, further sub-cases as all 10 kicks are made, only 9, only 8, only 7, only 6. 
If all 10 kicks are made, that means Team B failed to score in the final kick and Team B leads by 1 goal after 9 kicks. So the possibilities are 5-4,4-3,3-2,1-0 and in each of the possibility we have to find number of ways the goals are distributed among the 5 and 4 chances of Team A and B respectively. Next sub-case is only 9 kicks are made, again find all possibility and so on Same for case 2. Is there any better way to solve it?","['combinatorics', 'discrete-mathematics']"
2900676,When is $ 999\cdots$ a perfect square?,"I'm interesting to look more about property of  this number $ 999\cdots$ , for even digits which form that number it's clear that's not a perfect square for example :$ 99=10^2-1,9999=10^4-1,\cdots$ , Now my question is: what about if the numbers of digits of  $ 999\cdots$ is odd ? how i proof or disproof that is a perfect square or nor ? Note: The trivial case is for $n=1$  which is $9$","['square-numbers', 'elementary-number-theory', 'algebra-precalculus', 'discrete-mathematics']"
2900722,Proof that $\frac{1 + \sqrt{5}}{2}$ is irrational.,"I'm trying to prove that $\frac{1 + \sqrt{5}}{2}$, the ""Golden ratio,"" is irrational. I have only been able to do so thus far by taking as given (which I haven't had any trouble proving) exercise 1.1. in Rudin: Lemma. (Taken as proved) If $r \neq 0$ is rational and $x$ is irrational, then $r + x$ is irrational and $rx$ is irrational. I'd be very interested if someone knows of an alternate approach. Since the proof I have written was far from elegant, I'd appreciate any critiques on it as well. Here's what I came up with: Proof. Let's first establish that $\sqrt{5}$ is irrational. Assume to the contrary that $\sqrt{5} \in \mathbb{Q}$. Then, $\exists m, n \in \mathbb{Z}, \left(\sqrt{5} = \frac{m}{n} \wedge n \neq 0\right)$. Without loss of generality, let $m$ and $n$ be coprime. Squaring both sides and algebraically rearranging this equation yields 
\begin{align}
5n^2 = m^2,
\end{align} 
in which case $5 \mid m^2$. We can therefore deduce that $5 \mid m$. (Side note: the only explanation I was able to come up with, which I will leave as a conceptual argument for the moment, is the fundamental theorem of arithmetic: if we attempt to establish the contrapositive, that $5$ does not divide $m$ means that for all $p_i$ in the prime factorization of $m$, $p_i \neq 5$. In squaring $m$, we double the exponents, but $5$ is still not a factor, and because it's prime it can't be generated from any of the other factors. So, $5$ also doesn't divide $m^2$. This is far from as elegant as establishing, say, that if $m$ is odd, $m^2$ is also odd. If there is a better way to establish this fact, though, I'd be very interested in hearing it.) So, since $5 \mid m$, we can write $m = 5a$ for some $a \in \mathbb{Z}$. Substituting into our equation gives
\begin{align}
5n^2 = (5a)^2 = 25a^2,
\end{align}
and simplifying gives 
\begin{align}
n^2 = 5a^2,
\end{align}
so $5 \mid n^2$ and thus $5 \mid n$, a contradiction, as we assumed $m$ and $n$ were coprime. Thus, $5$ is irrational. From here, since $1 \in \mathbb{Q} - \{0\}$, we can use the fact that $r + x$ is irrational with $r = 1$ and $x = \sqrt{5}$ to deduce that $1 + \sqrt{5}$ is irrational. Similarly, using the fact that $rx$ is irrational, we can set $x = 1 + \sqrt{5}$ and $r = \frac{1}{2}$ to deduce that $rx = \frac{1 + \sqrt{5}}{2}$ is irrational, which is our goal. How does this look? I'd be very interested in any critiques of this or alternate methods of proof. Thanks.","['proof-verification', 'real-analysis']"
2900724,Sampling Distribution of Variances,"Please consider this problem and my solution to it. I get the feeling, my approach is way off. Problem: A normal population has a variance of $15$. If samples of size $5$ are
drawn from this population, what percentage an be expected to have
variances less than $10$. Answer The sample variance has a chi-square distribution with $4$ degrees of freedom. Also observe that $\frac{10}{15} = 0.666667$. That is, you need to adjust for the population variance. I then went to this website: https://stattrek.com/online-calculator/chi-square.aspx and I entered $4$ for the degrees of freedom and $0.666667$ for the Chi-Square
critical value. I then got an answer of $0.05$ but the book gets an answer of
$0.50$. What did I do wrong? Thanks, Bob","['statistics', 'probability']"
2900737,Evaluating $\lim_{x\to 1}{\left(\frac{(1-x)(1-x^2)\cdots(1-x^{2n})}{({(1-x)(1-x^2)\cdots(1-x^n)})^2}\right)}$,"$$\lim_{x\to 1}{\left(\frac{(1-x)(1-x^2)\cdots(1-x^{2n})}{({(1-x)(1-x^2)\cdots(1-x^n)})^2}\right)}$$ 
i am a 12th class student and this question was given by our mathematics teacher a year ago.
this is part of JEE EXAM syllabus ,
i tried in this way..
$$\lim_{x\to 1}{\left(\frac{(1-x)(1-x^2)\cdots(1-x^{2n})}{({(1-x)(1-x^2)\cdots(1-x^n)})^2}\right)}$$
$$ =\lim_{x\to 1}{\left(\frac{(1-x^{n+1})(1-x^{n+2})\cdots(1-x^{2n})}{{(1-x)(1-x^2)\cdots(1-x^n)}}\right)}$$
then knowing that 
$$\lim_{x\to 1}{(\frac{1-x^n}{1-x})}=n$$  hence $$\lim_{x\to 1}{\left(\frac{(1-x)(1-x^2)\cdots(1-x^{2n})}{({(1-x)(1-x^2)\cdots(1-x^n)})^2}\right)}
= \binom{2n}{n}$$ 
Is this right? 
Is there any other way to do it?","['limits', 'calculus']"
2900753,Atlas of a regular surface,"I have the following set of $R^3$: $$
S=\{(x,y,z) \in \mathbb{R}^3: \, x^2+y^2-z^3=1\}
$$ It is immediate to see that $S$ is a regular surface. How may I find an atlas? When  $z \neq 0$, as $z=\sqrt[3]{x^2+y^2-1}$, we have the parameterization $x(u,v)=(u,v,\sqrt[3]{u^2+v^2-1})$ but what happens when $z=0$?",['differential-geometry']
2900803,How to prove that if $a_n=o(n) $ then $\sum_{k=0}^n\frac{a_n^k}{k!}\sim e^{a_n} $,"I recall a question where it was proved that $$\lim_{n\to\infty}e^{-n}\sum_{k=0}^n\frac{n^k}{k!}=\frac12.$$This seems to remain true when $n$ is replaced by $n+c $ where $c$ is some constant, while apparently, if $a_n=o(n)$, $$\sum_{k=0}^n\frac{a_n^k}{k!}\sim e^{a_n}.$$How to prove it? Maybe the method used in the related question is also useful here, but I couldn't find it","['asymptotics', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
2900804,How can I prove that $a\implies b$ equals $\neg b\implies\neg a$ with truth tables?,"How can I prove that $a\implies b$ equals $\neg b\implies\neg a$ with truth tables? I've made a separate truth table for each but I am unsure on how to make it into a single/combined truth table.. EDIT: would this then be the correct way to prove it?
Suggestion 1: Suggestion 2:","['logic', 'discrete-mathematics']"
2900855,Let $\Omega \subseteq \Bbb{C}$ is open and $K\subset \Omega$ be compact.,"I'm trying to prove the following statement:
""Let $\Omega \subseteq \Bbb{C}$ is open and $K\subset \Omega$ be compact. Prove that there exists $\epsilon >0$ independent of $z\in K$ where $D_\epsilon(z)\subset \Omega$ for all $z\in K$"" I wanted to use a general definition of open cover compactness and do a similar proof to showing that compact subspace of a metric space is bounded. However, I haven't had much success. I was told that sequential compactness may be easier to use in this proof however, I don't know where to start with that. I understand the definition is similar to the Bolzano-Weierstrass Theorem: $K\subseteq\mathbb{C}$ is sequentially compact if every $\{z_{n}\}\subseteq K$ has a convergent subsequence where the limit is in $K$.","['complex-analysis', 'general-topology', 'compactness']"
2900889,Convergence of sample mean using CLT,"Assume $X_i$ s are i.i.d. random variables with mean $\mu$ and variance $\sigma^2$ . Prove: $$\lim_{n\to\infty}n^2\mathbb{P}\left(\left|\frac{\sum_{i=1}^{n} X_i}{n}-\mu\right|>n^{-1/4}\right)=0.   \,\,\,\,\,\,\  (1)$$ My effort: I used Chebyshev inequality but did not work. Then, I thought about using the central limit theorem (CLT). By CLT: $$\lim_{n\to\infty}\mathbb{P}\left(\sqrt{n}\left|\frac{\sum_{i=1}^{n} X_i}{n}-\mu\right|>\gamma\right)=1-\mathrm{erf}\left(\frac{\gamma}{\sigma}\right).$$ The problem is that $\gamma$ cannot be a function of $n$ . Otherwise, I could let $\gamma=n^{1/4}$ and prove what I need. Is there a trick I can use here? How can I prove (1)? If (1) only holds for certain conditions, please let me know. Lastly, if we know $X_i=A_i^2 B_i^2$ where $A_i$ and $B_i$ are Gaussian random variables with non-zero means, can we prove (1)?","['concentration-of-measure', 'central-limit-theorem', 'convergence-divergence', 'probability-theory', 'probability']"
2900906,Are all Projective Space Bundles on Schemes Projectivizations of Vector Bundles?,"I'm taking a course on complex manifolds, and in class we saw this fact for complex manifolds. I have a moderate background in algebraic geometry, and was interested in the same question for $\mathbb{P}^n$ bundles over a scheme $X$, where now we interpret the transition functions on $\operatorname{Spec} A \times \mathbb{P}^n$ as $A-$linear automorphisms of the homogeneous ring $A[x_0, ..., x_n]$. Maybe this is a good formulation of the precise question: is every such bundle isomorphic to $\mathbb{P}(\mathscr{E})$ for some locally free sheaf $\mathscr{E}$? I'm sure that there is some really high-powered proof using tools that are above my paygrade though like GAGA or something. Personally I'd just like to see to what extent this is still true in the algebraic setting, although not necessarily over the complex numbers, or if possible, even projective space over a ring would be nice to know some things about.",['algebraic-geometry']
2900949,Durrett's way of defining Markov Property for Markov Chains,"I've seen several questions posted here, but they were not precisely what what I'm looking for, either in the answer which was given, or in the question itself. 1. If $P_{x}=P_{\delta_{x}}$, then why $P_{\mu}(A)=\int P_{x}(A)\, \mu(dx)$ ? I'm looking at this as if 
\begin{split}
P_{\mu}(\prod ( X_i \in B_i)) & =\int \mu(dx) \int_{B_0} \delta_x(dx_0) \int_{B_1} p(x_0,dx_1) \cdots \int_{B_n} p(x_{n-1},dx_n) \\
 & = \int \mu(dx) \mathbf{1}_{B_0}(x) \int_{B_1} p(x,dx_1) \cdots \int_{B_n} p(x_{n-1},dx_n) \\
 & = \int_{B_0} \mu(dx) \int_{B_1} p(x,dx_1) \cdots \int_{B_n} p(x_{n-1},dx_n)
\end{split} 2. Let $Y:\Omega\rightarrow\mathbb{R}$ is measurable and bounded. Then 
  $$E_{\mu}(Y\circ\theta_{n} \mid \mathcal{F}_{n})=E_{X_{n}}(Y)$$ So, according to a remark in the book, page 283, $E_{X_{n}}(Y)=g(X_n)$. The way I'm interpreting this is $Y\circ\theta_{n}=Y(X_{n+1},X_{n+2},...)$ and $E_{\mu}(Y(X_{n+1},X_{n+2},\ldots) \mid \mathcal{F}_{n})=g(X_n)$. However, I think I'm losing information. I think I should interpret it as $$E_{\mu}(Y(X_{n+1},X_{n+2},\ldots) \mid \mathcal{F}_{n})=E(Y(X_{n+1},X_{n+2},\ldots) \mid X_{n}).$$
How can I ? In wikipedia , the property is stated w.r.t. conditional probabilities. I get how from Durrett's definition we get the conditional probability definition using indicator functions. But how do we do the other direction? or are the definitions not equivalent?","['stochastic-processes', 'probability-theory', 'markov-chains']"
2900950,"What is the relation between 'the order of $x^k = n/{\gcd(k,n)}$' and Lagrange's Theorem?","Algebra by Michael Artin Cor 2.8.11 to Lagrange's Theorem (Theorem 2.8.9). Question: What is the relation between Cor 2.8.11 and the order of $x^k$ given by $n/{\gcd(k,n)}$ (in the text, this is in the 3rd bullet point of Prop 2.4.3 )? (*) I was thinking something like $1=\gcd(k,p)$ for all $1 \le k < p$, so somehow the order of the non-identity elements of a group of prime order would be shown to be $p/{\gcd(k,p)} = p/1 = p$. I think this would be a way to show Cor 2.8.11 without Cor 2.8.10 to Lagrange's Theorem (Theorem 2.8.9), Lagrange's Theorem itself (Theorem 2.8.9) or even the Counting Formula (Formula 2.8.8). Or perhaps it's more the converse: that Cor 2.8.11 implies 3rd bullet of Prop 2.4.3 in the case that $n$ is prime? (*) Prop 2.4.3 Proposition 2.4.3 Let $x$ be an element of finite order $n$ in a group, and let $k$ be an integer that is written as $k = nq + r$ where $q$ and $r$ are integers and $r$ is in the range $0 \leq r < n$. $x^k = x^r$. $x^k = 1$ if and only if $r = 0$. Let $d$ be the greatest common divisor of $k$ and $n$.
  The order of $x^k$ is equal to $n/d$.","['elementary-number-theory', 'cyclic-groups', 'alternative-proof', 'abstract-algebra', 'group-theory']"
2900960,"As n approaches infinity, the likelihood function at true parameter values is maximum among any other feasible choice of the parameters","Assume that a density $f(x;\theta)$ is distinct for different values of parameter $\theta$ and has common support for all $\theta$. To show that for $\theta \neq \theta_0$ (=true value that generates samples),
$$\lim_{n \to \infty} P\left(L(\theta_0;x_1,...,x_n)>L(\theta;x_1,...,x_n)\right)=1$$ where $L$ is a likelihood function of the density $f$, my textbook proved it by showing that 
$$\frac{1}{n}\sum_{i=1}^{n} \log\left(\frac{f(x;\theta)}{f(x;\theta_0)}\right)<0 $$
which by the WLLN, as $n \longrightarrow \infty$, the left hand side is equal to $$E\left[\log\left(\frac{f(x;\theta)}{f(x;\theta_0)}\right) \right] < \log E\left[\frac{f(x;\theta)}{f(x;\theta_0)}\right]=\log(1)=0$$ by Jensen's inequality. But I would like to know whether the following solution can also be used. From AM-GM inequality, 
$$\left(\prod_{i=1}^{n} f(x_i;\theta)\right)^{1/n} = L(\theta;x_1,...,x_n)^{1/n} \leq \frac{\sum_{i=1}^{n} f(x_i;\theta)}{n}$$
As $n \longrightarrow \infty$, by the WLLN
$$L(\theta;x_1,...,x_n)^{1/n} \leq E[f(X;\theta)]= \int f(x;\theta)f(x;\theta_0)dx = \left<f(x;\theta),f(x;\theta_0)\right>$$
From Cauchy-Schwarz inequality, the inner product between two vectors is maximized when they are in the same direction. So the strict maximum of $L(\theta;x_1,...,x_n)$ is attained when $f(x;\theta)=f(x;\theta_0)$ by the assumption of the distinct density for different $\theta$, which implies $\theta = \theta_0$. Hence, as $n \longrightarrow \infty$, $P(L(\theta;x_1,...,x_n) < L(\theta_0;x_1,...,x_n))=1$.","['statistics', 'maximum-likelihood']"
2900966,Elementary Symmetric Polynomials and Determinant,"I am trying to show that
\begin{equation}
\sqrt{\Delta} = \prod\limits_{1 \leq i < j \leq n} \left( x_i - x_j \right)
= \det \begin{pmatrix} \dfrac{\partial \sigma_{n,1}}{\partial x_1} & \cdots & \dfrac{\partial \sigma_{n,n}}{\partial x_1} \\ \vdots & \ddots & \vdots \\ \dfrac{\partial \sigma_{n,1}}{\partial x_n} & \cdots & \dfrac{\partial \sigma_{n,n}}{\partial x_n} \end{pmatrix}
\end{equation}
where $\sigma_{n,j}$ are the elementary symmetric polynomials of $n$ variables $x_1, x_2, · · · , x_n$ Does anyone know of another method to do so other than induction since I am not sure how the inductive step works out~","['galois-theory', 'abstract-algebra', 'linear-algebra', 'symmetric-polynomials']"
2900982,"Approximating smooth function on $[0,1]$ by Bernstein polynomial (interested in approximation rate in $L^2$ norm)","Consider a smooth function $f$ on $[0,1]$ and its Bernstein polynomial of power $n$: 
$$B_n(f)=\sum_{k=0}^n f\left(\frac{k}{n}\right) b_{n,k}(x)$$
with
$$b_{n,k}(x) = \binom{n}{k}x^k (1-x)^{n-k}.$$ It is well known that 
$$\sup_{x \in[0,1]} |B_n(f)(x)-f(x)|=O\left(\frac{1}{\sqrt{n}}\right).$$ But what if one considers the $L^2$ norm instead of $L^{\infty}$ norm? Will the approximation power be better in $L^2$ norm? In particular, is it possible to conclude that 
$$\left(\int_{0}^1 (B_n(f)(x)-f(x))^2 dx \right)^{\frac{1}{2}}=O\left(\frac{1}{n}\right)?$$","['approximation-theory', 'reference-request', 'real-analysis', 'functions', 'polynomials']"
2901069,"Centre of the special linear group $SL_2(\mathbb R)$ or $SL(2,\mathbb R)$","Algebra by Michael Artin Def 2.5.12 Obviously $I$ and $-I$ are in the centre: $AI=IA,A(-I)=(-I)A$. How exactly do I go about doing this? I was thinking to solve for $j,f,g,h$ below $$\begin{bmatrix}
a & b\\ 
c & d
\end{bmatrix} \begin{bmatrix}
j & f\\ 
g & h
\end{bmatrix} = \begin{bmatrix}
j & f\\ 
g & h
\end{bmatrix} \begin{bmatrix}
a & b\\ 
c & d
\end{bmatrix}, ad-bc=1=jh-fg.$$ So, I plug in b*g=c*f, a*f+b*h=b*j+d*f, c*j+d*g=a*g+c*h, j*h-f*g=1, a*d-b*c=1 in Wolfram Alpha or here or here (or here ) to get a bunch of complicated solutions sets, some of which include the desired $\pm I$. Ugh, where can I find a proof online? Or if this is still folklore , how do I begin? Do I for example take cases $c=0, c \ne 0$ and then solve for the centre in each case('s subcases)? Another thing I thought was to suppose on the contrary that $f \ne 0$ and then arrive at a contradiction and then assume $f=0$ when supposing on the contrary that $g \ne 0$ and then assuming $f=g=0$ when supposing on the contrary that $j \ne \pm 1$ and then assuming $f=g=0, j = \pm 1$ when supposing on the contrary that $h \ne \pm 1$. Not looking for a full solution, just a little nudge in the right direction. I've been lost in subcases of subcases of cases (like here ) that I think I'm missing something elegant or simple. I guess I've been doing maths for quite awhile that I've forgotten how to do arithmetic . To clarify, I am looking for a way to do this by systems or at least nothing high level like using facts like these About the Center of the Special Linear Group $SL(n,F)$ Note that this chapter is on homomorphisms. The reader just finished cyclic groups. The reader didn't reach Lagrange's Theorem, fields, rings or even isomorphisms.","['linear-groups', 'matrices', 'abstract-algebra', 'linear-algebra', 'group-theory']"
2901075,Why doesn't the limit $\lim_{r \to 0} \frac{r (\cos^2\theta\sin\theta)}{r^2\cos^4\theta + \sin^2\theta}$ exist?,"I have two questions regarding this subject. Hope you can help me. Consider the limit of the function $f(x,y) = \frac{x^2y}{x^4+y^2}$ as (x,y) approaches (0,0):
$$\lim_{(x,y) \to (0,0)} \frac{x^2y}{x^4 + y^2} = \lim_{r \to 0}\frac{r^2\cos^2\theta(r\sin\theta)}{r^4\cos^4\theta + r^2\sin^2\theta}$$
I tried the paths $y=x^2$ and $x=0$ and found different limits which means that the limit of the function doesn't exists. Now, I want to verify that through the polar coordinates. After the simplification, the polar equation reduces to:
$$\lim_{r \to 0} \frac{r (\cos^2\theta\sin\theta)}{r^2\cos^4\theta + \sin^2\theta}$$
At first I thought that this limit is equal to $0$ for all lines except the lines $\theta = 0$ and $\theta= \pi$ because I thought these lines would make $sin \theta$ equal to $0$ and that would cause the indeterminate form $0/0$, but after I thought about it, I came to another conclusion: Since this is a limit, $r$ only approaches to $0$ and it is never actually $0$, then the numerator of the limit is $0$ because of the $sin\theta$ part, and its denominator is not $0$ since $r$ is not $0$. Then, the limit should be just $0$. Is that right ? My other question is about using different paths on polar coordinates. For example, consider the function $r = sin\theta$. If I use this path, the equation becomes:
$$\lim_{r \to 0} \frac{sin^2\theta cos^2\theta}{sin^2\theta \cos^4\theta + \sin^2\theta}$$
Simplifying ${sin^2\theta}/{sin^2\theta}$, we get,
$$\lim_{r \to 0} \frac{cos^2\theta}{cos^4\theta + 1}$$
My question is: Can I use the possible $\theta$ values in this expression? For example, Because $r=sin\theta$ and $r$ approaches to $0$, $\theta$ can either be $0$ or $\pi$. Since we only have the even powers of $cos\theta$, we can just assume that $cos^2\theta$ is $1$, which makes the limit equal to: 
$$\lim_{\theta \to 0} \frac{1}{1 + 1}= 1/2$$
Am I allowed to use $\theta$ value in the limit or is my work after the limit $\lim_{r \to 0} \frac{cos^2\theta}{cos^4\theta + 1}$ just wrong ? Edit: I was told that my question is a possible duplicate of another question. I am very new at this website, so I'm not sure if this is the right way to explain why my question is not a duplicate but  I'm gonna try to explain why. Mine and the other question is about the same limit; however, in the other question, the user asked why the limit doesn't exist, while I already know why, I just want to verify this in terms of polar coordinates. I also want to know if using the value of $\theta$ in different paths is valid, which is a topic the other question doesn't mention.","['limits', 'multivariable-calculus', 'polar-coordinates']"
2901083,X-Y representation,"I have a question about plotting two functions that have a common argument, e.g. time, in a single figure, such that the values of one function are used for the x-axis and the those of the second function for the ordinary y-axis. How can we for example specify two functions $x=f_1(t)$ & $y=f_2(t)$ such that when plotting these two functions in a single figure, a circle is created by varying the variable t? Any help would be really appreciated.","['functions', 'graphing-functions']"
2901096,Calculate $\lim_{x\to{0^+}}\frac{\log_{\sin{x}}{\cos{x}}}{\log_{\sin{\frac{x}{2}}}\cos{\frac{x}{2}}}$,"Calculate
  $$\lim_{x\to{0^+}}\frac{\log_{\sin{x}}{\cos{x}}}{\log_{\sin{\frac{x}{2}}}\cos{\frac{x}{2}}}$$ My Attempt: $$\lim_{x\to{0^+}}\frac{\ln{(1-2\sin^2{\frac{x}{2}})}}{\ln{(1-2\sin^2{\frac{x}{4}})}}\cdot\frac{\ln{\sin{\frac{x}{2}}}}{\ln{\sin{x}}}$$ $$\lim_{x\to{0^+}}\frac{\ln{(1-2\sin^2{\frac{x}{2}})}}{\ln{(1-2\sin^2{\frac{x}{4}})}}\cdot\frac{\ln{\sin{\frac{x}{2}}}}{\ln{2\sin{\frac{x}{2}}\cos{\frac{x}{2}}}}$$ $$\lim_{x\to{0^+}}\frac{\ln{(1-2\sin^2{\frac{x}{2}})}}{\ln{(1-2\sin^2{\frac{x}{4}})}}\cdot\frac{\ln{\sin{\frac{x}{2}}}}{\ln{2\sin{\frac{x}{2}}\cos{\frac{x}{2}}}}$$ $$\lim_{x\to{0^+}}\frac{\ln{(1-2\sin^2{\frac{x}{2}})}}{\frac{-2\sin^2{\frac{x}{2}}}{-2\sin^2{\frac{x}{2}}}}\cdot\frac{\frac{-2\sin^2{\frac{x}{4}}}{-2\sin^2{\frac{x}{4}}}}{\ln{(1-2\sin^2{\frac{x}{4}})}}\cdot\frac{\ln{\sin{\frac{x}{2}}}}{\ln{\sin{\frac{x}{2}}}+\ln{2}}$$ $$\frac{1}{4}\cdot 16\cdot 1$$ $$=4$$ Am I solving this correct?","['limits', 'limits-without-lhopital', 'logarithms']"
2901121,Find the function $f(x)$ if $f(x+2)=f(x+1)+2f(x)$ for all $x\in \mathbb{Z}^{+}$,"Here is my problem: If $x\in \mathbb{Z}^{+}$ and $f(x+2)=f(x+1)+2f(x)$
  find the function $f(x)$ where, $f(1)=4, f(2)=2.$ When I encounter this problem, I thought the question is wrong. I obviously, I would expect a question like this, for example: $f(5)+f(17)=?$ Anyway, I did something like this: $f(3)=f(2)+2f(1)$ $f(4)=f(3)+2f(2)=f(2)+2f(1)+2f(2)=3f(2)+2f(1)$ $f(5)=f(4)+2f(3)=3f(2)+2f(1)+2f(2)+4f(1)=5f(2)+6f(1)$ $f(6)=f(5)+2f(4)=5f(2)+6f(1)+6f(2)+4f(1)=11f(2)+10f(1)$ $f(7)=f(6)+2f(5)=11f(2)+10f(1)+10f(2)+12f(1)=21f(2)+22f(1)$ $f(8)=f(7)+2f(6)=21f(2)+22f(1)+22f(2)+20f(1)=43f(2)+42f(1)$ $A_{f(2)} :=\left\{ 1,3,5,11,21,43 ... \right\}$ $B_{f(1)} :=\left\{ 2,2,6,10,22,42 ... \right\}$ Here,  $A_{f(2)}$ and $B_{f(1)}$ are coefficients. $A_{f_1(2)}=2^1-1; A_{f_2(2)}=2^2-2+1; A_{f_3(2)}=2^3-2^2+2-1;...; A_{f_n(2)}=2^n-2^{n-1}+2^{n-2}-2^{n-3}+2^{n-4}-2^{n-5}+...-...+ (-1)^n \Longrightarrow A_{f_n(1)}=\frac {2^{n+1}-2(-1)^{n+2}}{3}=\frac {2^{n+1}+2(-1)^{n+3}}{3}$ $B_{f_1(1)}=2^1; B_{f_2(1)}=2^2-2; B_{f_3(1)}=2^3-2^2+2;...;B_{f_n(1)}=2^n-2^{n-1}+2^{n-2}-2^{n-3}+2^{n-4}-2^{n-5}+...-...+2 (-1)^{n+1}  \Longrightarrow B_{f_n(2)}=\frac{2^{n+1}-(-1)^{n+1}}{3}=\frac{2^{n+1}+(-1)^{n+2}}{3}$ $f(n+2)=2A_{f_n(1)}+4B_{f_n(2)}=\frac {2^{n+1}+2(-1)^{n+3}}{3} ×2+ \frac{2^{n+1}+(-1)^{n+2}}{3} ×2^2=2^{n+2}-2(-1)^{n+2} \iff f(x)=2^x-2(-1)^x$ Finally, I got an answer like this: $$f(x)=2^x+2(-1)^{x+1} , x\in \mathbb Z^{+}$$ Honestly, I don't know. Maybe, this is a ridiculous way. Because, it might be an intelligent way for to do it. And You know what I will actually asking. Is my solution correct? Or,was there an intelligent way to do it? Thank you!","['recurrence-relations', 'functions', 'sequences-and-series', 'algebra-precalculus', 'exponential-function']"
2901129,What is the fastest way to estimate the Arc Length of an Ellipse?,"To estimate the circumference of an ellipse there are some good approximations. $a$ is the semi-major radius and $b$ is the semi-minor radius. $$L \approx \pi(a+b) \frac{(64-3d^4)}{(64-16d^2 )},\quad \text{where}\;d =  \frac{(a - b)}{(a+b)}$$ Are there any similar formulas to approximate the arc length of an ellipse from $\theta_1$ to $\theta_2$? If not what are some computationally fast ways to approximate the arc length to within about $1\%$ to $0.1\%$ of $a$?","['elliptic-curves', 'geometry', 'calculus', 'numerical-methods', 'elliptic-integrals']"
2901189,Further examples of stalks,"I am currently learning about stalks for the first time. In my exploration online about the topic, I routinely run into the same three examples: In a constant sheaf associated with an abelian group $A$, any stalk is isomorphic to $A$. In the sheaf of real-valued continuous functions, stalks are all germs at the given point. In the sheaf of complex-analytic functions, stalks are all germs at the given point. I understand why these examples are ubiquitous. Stalks are about local behavior, and germs are an example of functions locally behaving the same being identified. Great! That said, I would love to see more examples of stalks outside of these.","['algebraic-geometry', 'category-theory', 'sheaf-theory']"
2901192,"How can it be that the Chern class fully determines a line bundle, but having Chern class zero doesn't imply a line bundle is trivial?","It is well-known that the Chern class of a line bundle in $H^2(M,\mathbb Z)$ fully determines the bundle up to isomorphism. However, in this wikipedia entry on Calabi-Yau manifolds it is stated that there are manifolds that have a non-trivial canonical bundle, but a vanishing integral Chern class. What is happening here? The only thing I can think of is that a differentiably trivial line bundle doesn't have to be holomorphically trivial. Is that the case? How should one think of such bundles that are topologically but not analytically trivial?","['complex-geometry', 'algebraic-geometry', 'characteristic-classes']"
2901196,Bounds on moments of sample mean,"Let $X_i$s be i.i.d zero mean random variables whose $p$-th moments are finite. Prove
  $$E\left[\left(\sum_{i=1}^{n}X_i\right)^p\right]\leq C_p n^{p/2}$$
  where $C_p$ is a constant independent of $n$. My effort: I think we have to expand the sum and then say each term is of $k$-th moment, count the number of them and the upper bound. This seems to be messy. Is there a better solution? Or, is it a name of a theorem?","['expected-value', 'probability-theory', 'probability']"
2901199,How to prove $\lim_{t\rightarrow \infty} \frac{e^{i x t}}{x- {i\mkern1mu} 0^+} =2 \pi {i\mkern1mu} \delta(x)$,"Recently I meet following two equations in physics. Their proof is like a magic and I can't understand.
$$\lim_{t\rightarrow -\infty} \frac{e^{i x t}}{x- {i\mkern1mu} 0^+} =0$$
$$\lim_{t\rightarrow \infty} \frac{e^{i x t}}{x- {i\mkern1mu} 0^+} =2 \pi {i\mkern1mu} \delta(x)$$ I'm very confused about in what sense these equations are hold? Could you recommend some literatures about these part of math? According to my knowledge(e.g. mathematical analysis or calculus) there should be no limit. PS: I think the $0^+$ here means $$\lim_{\epsilon\rightarrow 0^{+}}\lim_{t\rightarrow -\infty}\frac{e^{i x t}}{x- {i\mkern1mu} \epsilon} =0$$
$$\lim_{\epsilon\rightarrow 0^{+}}\lim_{t\rightarrow \infty} \frac{e^{i x t}}{x- {i\mkern1mu} \epsilon} =2 \pi {i\mkern1mu} \delta(x)$$ Actually I don't know which order of taking limit is correct: 
$\lim_{\epsilon\rightarrow 0^{+}}\lim_{t\rightarrow -\infty}$ or $\lim_{t\rightarrow -\infty}\lim_{\epsilon\rightarrow 0^{+}}$ or the order doesn't matter.","['calculus', 'analysis', 'distribution-theory']"
2901265,"$R$ is commutative, $I$,$J$ are ideals, $I+J=R$, then $IJ=I\cap J$ [duplicate]","This question already has answers here : If $I+J=R$, where $R$ is a commutative rng, prove that $IJ=I\cap J$. (2 answers) Closed 4 years ago . If $R$ is a commutative ring and $I$ and $J$ are ideals s.t. $I+J=R$ then show that $IJ=I\cap J$. I've already shown that $IJ \subset I\cap J$, now I need to show the reverse inclusion. I'm a bit lost, so far i'm just figuring out what pieces I have to work with. Such as: $\forall r\in R$ $\exists i\in I ,j\in J$ s.t. $i+j=r$ $\forall ij\in IJ$, $ij=i_1$ and $ij=j_1$ for some $i_1\in I$, $j_1\in J$. Also, if I let $x\in I\cap J$, then $x=i_2=j_2=i+j$ for some $i_2\in I$, $j_2\in J$ Anyone, having problem getting to the conclusion here, thanks in advance","['ring-theory', 'abstract-algebra']"
2901300,"What is the relationship between 1/7, 1/11, 1/13, and the number 1001?","Here is what we have: 1/7 = 0.142857...
1/11 = 0.090909...
1/13 = 0.076923... Notice that if you add the first three digits to the next three digits, you always get 999: 142 + 857 = 999
090 + 909 = 999
076 + 923 = 999 Oddly, the same thing happens with 2/7 , 2/11 , 2/13 , and so on, as the numerator increases: adding the first three digits to the next three digit results in 999. OK. Multiplying the denominators 7 x 11 x 13 results in 1001. What is the relationship between the fraction series x/7, x/11, x/13 , and the result of multiplying the denominators? (This is taken from an example in the book ""Ten Ways to Destroy the Imagination of Your Child"" by Anthony Esolen . The examples above are presented separately, and the reader is encouraged to be imaginative in discovering how they are related. After almost 20 years of playing math/CS games, I suppose I still have no imagination to solve this kind of thing.)","['recreational-mathematics', 'puzzle', 'sequences-and-series']"
2901319,A tricky combinatorial sum,"I'm looking for a clean expression of the following combinatorial sum : $$\sum\limits_{k=0}^{n}\frac{{n \choose k}^2}{{{2n} \choose {2k}}}$$
I recall being told it does have a neat expression. However, I'm not familiar with combinatorics or anything related to evaluating non trivial finite sums such as this, so I basically lack methods to tacke this. Any insight would be great !","['summation', 'binomial-coefficients', 'combinatorics', 'generating-functions']"
