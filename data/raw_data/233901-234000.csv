question_id,title,body,tags
4883109,How is the sheafy De Rham cohomology functorial?,"$\newcommand{\T}{\mathscr{T}}\newcommand{\C}{\mathscr{C}^\infty}$ I've been enjoying Iversen's book on sheaf cohomology. He briefly mentions De Rham cohomology and a sheafy perspective on it but he doesn't mention if the canonical isomorphism is functorial; everyone says that it is, but with the sheaf perspective I'm not so sure. It's not been clear to me or people I've talked to how to even define sheafy De Rham cohomology as a functor. To explain the problem I need to explain his definition first. Let ""smooth manifold"" mean the usual but emphasise second-countable, Hausdorff and being without boundary (the latter for convenience and the first two because we need our manifolds to be $\sigma$ -compact). All sheaves and sheaf morphisms considered will be of $\Bbb R$ -sheaves. For a smooth manifold $X$ , let $\C_X$ denote the sheaf of smooth $\Bbb R$ -valued functions. Let $\T_X$ denote the tangential sheaf; this is the sheaf with sections $\T_X(U)$ the vector space of all sheaf morphisms $\phi:\C_X|_U\to\C_X|_U$ with the property that for all open $V\subseteq U$ , $\phi_V:\C_X(V)\to\C_X(V)$ is a derivation of $\Bbb R$ -algebras. The sheaf action is defined by obvious restriction, and it is obviously a true sheaf. Now for $p\ge0$ let $\Omega_X^p$ denote the De Rham sheaf, the ""sheafy"" notion of differential forms. This is the sheaf on $X$ with sections $\Omega^p_X(U)$ defined as the vector space of alternating morphisms of $\C_X|_U$ -modules: $$\omega:\T_X|_U\otimes_{\C_X|_U}\T_X|_U\otimes_{\C_X|_U}\cdots\otimes_{\C_X|_U}\T_X|_U\to\C_X|_U$$ Where $p$ sheaf tensor products are taken. The exterior derivative may be defined in the usual way and we have a complex of sheaves $\Omega^\bullet_X$ and an obvious injection $\underline{\Bbb R}\to\Omega^\bullet_X$ . It can be argued with a local bump function perspective that this is a complex of soft sheaves, and since $X$ is $\sigma$ -compact and LCH this means soft sheaves are acyclic for cohomology (as well as compactly supported cohomology). Then standard arguments involving finding a basis for forms on $\Bbb R^n$ and getting antiderivatives find that $\Omega^\bullet_X$ is a resolution of $\underline{\Bbb R}$ (exact iff. exact on stalks). By abstract nonsense we conclude $H^\bullet(X;\Bbb R)\cong H^\bullet_{dR}(X)$ and $H^\bullet_C(X;\Bbb R)\cong H^\bullet_{dR,C}(X)$ . Now my question is: with this sheafy perspective on $H^\bullet_{dR}$ , how is it a functor ? Because I would obviously want the sheaf theory to produce a natural isomorphism between the two theories. I know this should be possible since the high and mighty speak of this e.g. Clausen refers to (some similar variant of) sheafy De Rham cohomology as a functor, but doesn't explain how that actually works. I don't know if Iversen's formulation (the presentation of which I've also slightly tweaked) is standard or not. Classically if I have a smooth $f:X\to Y$ it's very easy to define the derivative $df$ ""pointwise"". If you have a derivation $D:\C(f^{-1}(W))\to\Bbb R$ you promote it to one $\C(W)\to\Bbb R$ just by assigning $g\mapsto D(g\circ f)$ , for any open subset $W$ of $Y$ . However Iversen's formulation is founded on vector fields, rather than tangent vectors. Looking around, we can't generally perform a similar construction for pushing forward vector fields; if I have a section $\phi$ of $\T_X(f^{-1}(W))$ , I can define a derivation $\C(W)\to\C(f^{-1}(W))$ by precomposition again but I can't push that to something $\C(W)\to\C(W)$ . Ideally I should have a sheafy derivative $df:f_\ast\T_X\to\T_Y$ , or $\T_X\to f^\ast\T_Y$ or $\T_Y\to f_\ast\T_X$ or whatever the correct variance should be, but I'm struggling to get anything. And then of course if I don't have a derivative I don't have a hope of defining a pullback-of-forms map $\Omega_Y^p\to f_\ast\Omega_X^p$ . And that's what I really need; it can be shown that $f^\ast:H^\bullet(Y;\Bbb R)\to H^\bullet(X;\Bbb R)$ would be computable by (the global sections of) any chain map $F:\Omega^\bullet_Y\to f_\ast\Omega_X^\bullet$ which, in degree zero, equals the precomposition map $f^\ast:\C_Y\to f_\ast\C_X$ . But how do I find such a chain map $F$ ?","['de-rham-cohomology', 'sheaf-cohomology', 'sheaf-theory', 'differential-geometry']"
4883112,four-digit number is equal to the product of the sum of its digits multiplied by the square of the sum of the squares of its digits,"I'm trying to find a four-digit number that is equal to the product of (the sum of its digits) multiplied by (the square of the sum of the squares of its digits). I've tried running all combinations in Python and found two solutions (2023 and 2400). However, my maths teacher gave it to me and said there was a way to solve it analytically. $\sum_{i=0}^{3}10^{3-i}a_i = (\sum_{i=0}^{3} a_i) \times \left(\sum_{i=0}^{3} a_i^2\right)^2$ The only thing I found is that since $6^5 = 7776$ , no $a_i$ for $i \in \{0, 1, 2, 3\}$ cannot be greater or equal than six because otherwise $a_0$ would be greater or equal than 7 and $7^5 > 10000$ .","['elementary-number-theory', 'discrete-mathematics']"
4883196,Are there two contradictory definitions of the limit of a function at a point?,"I have found two different definitions of the limit of a function f at a point a in very popular calculus/analysis texts. In one, the function f is asked to be defined on an open interval containing the point a , possibly excluding said point ( Stewart, Wade ). The other definition, more popular in analysis books, simply asks that point a be a limit point (also called a cluster point) of the domain of the function ( Bartle and Sherbert, Abbott ). In both cases the epsilon-delta condition is requested.  In the first definition, for the limit to exist it is necessary that x can tend to a from both sides, that is, with values greater and less than a . In the second version, since any extreme of an interval is also a limit point, x is not required to tend to a on both sides.  The latter also implies that the sided limits are particular cases of the general definition. Furthermore, the theorem which is usually stated as "" There exists the limit of f at a point a and is equal to L if and only if both sided limits exist and are equal to L "" only makes sense in that case for interior points of the domain. My question is: Which of the two definitions is correct or, in any case, more rigorous?  Are they equivalent and am I missing something? Any clarification on this is welcome. Thanks!","['limits', 'calculus', 'analysis']"
4883202,How to approach an Hyperbolic Integral that doesn't appear to be solvable in closed form.,"I'm interested in tackling the following integral: $$\int_{-\ln (2+\sqrt 5)}^{\ln (2+\sqrt 5)} \sqrt{4+\sinh^2(x)} dx$$ While I've attempted various techniques, it appears challenging to find a closed-form solution for this integral. I'm beginning to suspect that it might not have one. Do you have any insights into expressing it as an infinite series or in terms of special functions? Any guidance or suggestions on alternative approaches would be greatly appreciated. Thank you for your assistance!","['integration', 'calculus', 'hyperbolic-functions']"
4883251,Is there a closed-form expression for the series $\sum_{n=0}^\infty \frac{n x^n}{1 - \left( \frac{x}{1+x} \right)^n}$?,"During a homework assignment, I ran into this series: $$S(x) = \sum_{n=0}^\infty \frac{n x^n}{1 - \left( \frac{x}{1+x} \right)^n}$$ Graphing $S(x)$ shows that it converges for some values of $x$ , mainly near $x=0$ and near $x=-1$ . I was wondering if $S$ has a closed form. Despite my best efforts, I was not able to find one online or through my own analysis. It should be noted that the summand is ill-defined for $n=0$ . It does, however, possess a limit. In particular: $$\lim_{n \to 0} \! \left ( \frac{n x^n}{1 - \left( \frac{x}{1+x} \right)^n} \right ) = \frac{1}{\ln \! \left( 1+\frac{1}{x} \right)}$$ This limit should be taken as the $n=0$ term.","['closed-form', 'sequences-and-series']"
4883331,When does a finite group $G$ only have a single maximal subgroup containing given $A<G$?,"Let $G$ be a finite group. Let $A<G$ . When is it true that $G$ has only a single maximal subgroup containing (or equal to) $A$ ? The necessary condition is that there must be $x \in G$ such that $\left<A \cup \{x\}\right>=G$ and the order of $\left<x\right>$ is a power of a  prime. In many cases, this is also sufficient -- for instance, if $G$ is abelian. But this is not generally true. The simplest counterexample is provided by $G=S_2 \times C_3$ , $A=\left<(e,v)\right>$ where $e$ is the identity and $v$ is any $2-$ cycle permutation. Unless I'm missing something, it is sufficient for there to be $x \in G$ such that $\left<A \cup \{x\}\right>=G$ , the order of $\left<x\right>$ is a power of a prime, and either $\left<x\right>$ or $A$ are normal subgroups of $G$ . But this condition is not necessary if $G$ is simple nonabelian and $A$ is any of its maximal subgroups, for instance. EDIT: a slightly weakened version of the sufficient condition presented above is that there is $x \in G$ such that $\left<A \cup \{x\}\right>=G$ , the order of $\left<x\right>$ is a power of a prime, and $A$ commutes with $\left<x\right>$ under the product of groups. However, this is still not necessary. In particular, there are examples where the supergroups of $A$ do not form a chain, for instance if $A≅C_2$ is a non-normal subgroup of $G=C_2^2 ⋊C_4$ .","['maximal-subgroup', 'group-theory', 'finite-groups']"
4883445,Proving $I(E(\Bbb C))=\left< y^2-x^3-ax-b\right>$,"Let $E/\Bbb C : y^2=x^3+ax+b$ be an elliptic curve over $\Bbb C$ . Let $$E(\Bbb C)=\{(x,y)\in\Bbb C^2:y^2=x^3+ax+b\}$$ be the affine variety generated by $E$ . Finally, let $$I(E(\Bbb C))=\{f(x,y)\in\Bbb C[x,y]: f(p)=0,\, \forall p\in E(\Bbb C)\}.$$ I am trying to show that $I(E(\Bbb C))=I_E$ is the ideal generated by $E$ , that is, $$I_E=\left< y^2-x^3-ax-b\right>\subset\Bbb C[x,y].$$ My attempt so far has been the following. Suppose $f\in\Bbb C[x,y]$ . Then we can divide by $y^2-x^3-ax-b$ and get $$f(x,y)=q(x,y)(y^2-x^3-ax-b)+r(x,y),$$ where the total degree of $r\in\Bbb C[x,y]$ is less than $3$ . Suppose then, that $f$ vanishes on $E(\Bbb C)$ . That is, $f\in I_E$ . From the uniformalization theorem, there is a lattice $L\subset \Bbb C$ such that $a=-15G_4(L),$ and $b=-35G_6(L)$ , where $G_4,G_6$ are Eisenstein series corresponding to $L$ . Because of this, the point $(\wp(z,L),\tfrac12\wp'(z,L))$ is in $E(\Bbb C)$ for all $z\in\Bbb C$ . Here, $\wp(z,L)$ is the Weierstrass Elliptic function corresponding to the lattice $L$ . Thus, $$0=f(\wp,\tfrac12\wp')=q(\wp,\tfrac12\wp')(\tfrac14\wp'^2-\wp^3-a\wp-b)+r(\wp,\tfrac12\wp').$$ Due to the differential equation $$\frac14\wp'(z,L)^2=\wp(z,L)^3-15G_4(L)\wp(z,L)-35G_6(L),$$ we then have that $$r(\wp,\tfrac12\wp')=0.$$ All I need to show now is that $r(x,y)$ is identically equal to $0$ , but I am not quite sure how. One thing I considered is to write $$r(x,y)=\beta_1x^2+\beta_2xy+\beta_3y^2+\beta_3x+\beta_5y+\beta_6,$$ so that $$\begin{align}
r(\wp,\tfrac12\wp')&=\beta_1\wp^2+\tfrac12\beta_2\wp\wp'+\tfrac14\beta_3\wp'^2+\beta_4\wp+\tfrac12\beta_5\wp'+\beta_6\\
&=\beta_1\wp^2+\tfrac12\beta_2\wp\wp'+\beta_3(\wp^3+a\wp+b)+\beta_4\wp+\tfrac12\beta_5\wp'+\beta_6\\
&=\beta_3\wp^3+\beta_1\wp^2+(a\beta_3+\beta_4)\wp+\tfrac12(\beta_2\wp+\beta_5)\wp'+\beta_6+b\beta_3\\
&= 0.
\end{align}$$ Then using the expansion $$\wp(z,L)=\frac1{z^2}+\sum_{n\ge1}(2n+1)G_{2n+2}(L)z^{2n},$$ the expansion of $\beta_3\wp^3+\beta_1\wp^2+(a\beta_3+\beta_4)\wp$ would only contain even powers of $z$ , while the expansion of $\tfrac12(\beta_2\wp+\beta_5)\wp'$ would only contain odd powers of $z$ , making the coefficient of $z^n$ of $r(\wp,\tfrac12\wp')$ be $0$ for all $n\ne0$ , and the constant term of $r(\wp,\tfrac12\wp')$ equal to $-\beta_6-b\beta_3$ . I was hoping that this would then imply that $r(x,y)\equiv 0$ , but I am not exactly sure if that works. Is there a way to show that $r\equiv 0$ from here? Or perhaps an easier way to show that $f\in\left<y^2-x^3-ax-b\right>$ ? Thanks. Edit for context: I encountered this problem by looking through my old notes from my undergrad algebraic geometry class. I was wondering how I could apply what I learned from the class to elliptic curves.","['complex-analysis', 'algebraic-geometry', 'elliptic-curves', 'ideals']"
4883477,"How to find all positive integers $n,k$ such that ${n\choose k}=m$ for a given $m$?","This question is motivated by a simple exercise in Peter Cameron's Combinatorics: Topics, Techniques, Algorithms : A restaurant near Vancouver offered Dutch pancakes with ‘a thousand and
one combinations’ of toppings. What do you conclude? The intended solution (according to Cameron's website ) is the following: since ${14 \choose 4}=1001$ , most likely there were $14$ possible toppings and each serving of pancakes allowed the patron to choose $4$ toppings. However, this begs a much more general question: For a given positive integer $m$ , how can we find all positive integers $n$ and $k$ such that ${n \choose k}=m$ ? This seems like a very natural and obvious question to ask, but preliminary searches didn't yield much apart from specific values of $m$ . Any insight to this question for more general classes of positive integers $m$ , or links to relevant references, would be appreciated.","['combinations', 'reference-request', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
4883501,Inconsistent Function Monotonicity from hand and Mathematica image,"$g(x)=\frac{\phi(x)}{1-\Phi(x)}$ , where $\phi(x)$ and $\Phi(x)$ are p.d.f and c.d.f of standard normal distribution respectively. $g'(x)=\frac{\phi'(x)(1-\Phi(x))+\phi^2(x)}{(1-\Phi(x))^2}=\frac{\phi(x)}{(1-\Phi(x))^2} \left( \phi(x)-x(1-\Phi(x)) \right)$ since $\phi'(x)=-x\phi(x)$ . Let $h(x)=\phi(x)-x(1-\Phi(x))$ , $h'(x)$ is thus $\phi'(x)-(1-\Phi(x))+x\phi(x)=\Phi(x)-1 \leq 0$ $h(x)$ is non-increasing, and $\lim_{x\to \infty} h(x)=0 $ , thus $h(x)\geq0$ and since $\frac{\phi(x)}{(1-\Phi(x))^2}\geq 0$ then $g'(x)\geq0$ . $g(x)$ must be non-decreasing. But in Mathematica, I have this like a wavy line. Mathematica Image What's the problem please!","['calculus', 'derivatives', 'normal-distribution', 'real-analysis']"
4883503,Expected value and variance of Sigmoid and SiLU on a normally distributed random variable for variational approximation,"I am trying to apply Assumed Density Filtering (ADF) according to the paper Lightweight Probabilistic Deep Networks to my own model, and I need to implement the variational approximation layer of Sigmoid and SiLU function. I tried to look for the equations for Sigmoid layer. In the paper Variational Learning in Nonlinear Gaussian Belief Networks , the authors mentioned that they have a closed form solution for calculating the expected value for Sigmoid layer with the equation: $$
M(μ,σ) = Φ(\frac{μ}{\sqrt{1+σ^2}})
$$ However, according to this question , there is only an approximation solution. Did I miss out some assumptions from the paper or misunderstood either of them? For SiLU, I am unable to find out resources for it so far. Would appreciate if anyone could provide some guidance or point me to some resources for it.","['machine-learning', 'statistics']"
4883514,If $x^2-16\sqrt x =12$ what is the value of $x-2\sqrt x$?,"I saw this problem:If $x^2-16\sqrt x =12$ what is the value of $x-2\sqrt x$ ? To make notations simpler Let $x=t^2$ and $t^4-16t=12$ and $l= t^2-2t$ . I tried to use the fact the $\frac{12} l =\frac{t^4-16t}{t^2-2t}= t^2+2t +4 + \frac{8}{t-2}= (t-2)^2 + 6t+ \frac{t}{8l}= \frac{l^2}{t^2}+6\frac{l}{t-2}+  \frac{t}{8l}$ I tried to find some similarities between the $l$ cubic polynomial and the original  quartic polynomial but I didn't find anything useful, needless to say that solving the quartic polynomial is not an option as the general solution is too complicated. Btw the answer is $2$ and since this is a ""clean"" and ""nice"" answer for a quartic polynomial there must be some trick",['algebra-precalculus']
4883527,A guess on two increasing rational approximations to $\frac{\pi}{4}$,"When investigated the Wilf function $$W(z)=\frac{\arctan\sqrt{2\operatorname{e}^{-z}-1}\,}{\sqrt{2\operatorname{e}^{-z}-1}\,},$$ see the preprint [1] below, I proposed a guess which reads that the sequences \begin{equation}\label{approx2pi-guess-qi}\tag{QG}
\frac{1}{2^k}\sum_{j=0}^{k-1}\frac{(-1)^j}{2j+1} \sum_{\ell=0}^{k-j-1}\binom{k}{\ell}
=-\frac{1}{\binom{2k}{k}}\sum_{\ell=1}^{k} (-1)^{\ell} \binom{2k-\ell}{k} \frac{2^{\ell/2}}{\ell} \sin\frac{3\ell\pi}{4}
\end{equation} are increasing in $k\in\mathbb{N}$ and tend to $\frac{\pi}{4}$ as $k\to\infty$ . If this guess were verified to be true, then the sequences in \eqref{approx2pi-guess-qi} would be two increasing rational approximations of the irrational constant $\frac{\pi}{4}$ . This guess is also related to a closed-form formula of the Gauss hypergeometric series ${}_2F_1\bigl(k+\frac12,k+1;k+\frac32;-z^2\bigr)$ for $k\in\mathbb{N}$ , which is established in a forthcoming paper authored by me. Question : is the above guess true? Feng Qi and Mark Daniel Ward, Closed-form formulas and properties of coefficients in Maclaurin’s series expansion of Wilf’s function composited by inverse tangent, square root, and exponential functions , arXiv (2022), available online at https://arxiv.org/abs/2110.08576v2 .","['approximation', 'approximation-theory', 'sequences-and-series', 'limits', 'hypergeometric-function']"
4883541,What's relationship between ST and TS's eigenvalues,"this comes from a comment of this If 𝑆
and 𝑇
are 𝑛×𝑚
and 𝑚×𝑛
matrices, 𝑆𝑇
and 𝑇𝑆
have the same nonzero eigenvalues: 𝑢
is an eigenvector of 𝑆𝑇
iff 𝑇𝑢
is an eigenvector of 𝑇𝑆
, with the same nonzero eigenvalue. I try to proof this if 𝑢
is an eigenvector of 𝑆𝑇 => 𝑇𝑢
is an eigenvector of 𝑇𝑆 $\begin{align}
STu &= \lambda u \tag{1}
\end{align}$ left multiply by T, then $\begin{align}
TSTu &= \lambda Tu \tag{2}
\end{align}$ but I'm blocked (2)=>(1)
if $TSTu=\lambda Tu$ , I can't multiply left with $T^-1$ since
T is not square matrix, even T is square matrix, it still may
not be invertible. and also why OP in that thread says have the same nonzero eigenvalues ,
from (1)=>(2) it seemed the zero eigenvalues are also corresponding?","['matrices', 'eigenvalues-eigenvectors']"
4883593,prove the existence of a fixed point,"Prove that every holomorphic function $f$ on the closed disk $\overline{D}(0,1)$ with $|f(z)|<1$ when $z\in
 \overline{D}(0,1)$ has at least one fixed point in $D(0,1)$ . My attempt: Since $f$ is holomorphic on $\mathbb{D}$ , $f$ is either constant or attains its maximum on boundary. If $f$ is constant, we finish. If $f$ is not constant, then $f$ attains its maximum on $\partial\mathbb{D}$ . According to Maximum modulus principle, we have $$|f(z)|\leq |f(z_0)|\quad\forall z\in\mathbb{D}\quad (\text{for some }|z_0|=1).$$ I don't know how to continue. I haven't used the hypothesis $|f(z)|<1\quad\forall z\in\overline{D}(0,1)$ . Could someone have any idea how to solve this problem?",['complex-analysis']
4883620,Schur's Multiplier exercise (Problem 5A.7 Isaacs' Finite Group Theory),"I have a question about the following problem [Finite Group Theory, Martin Isaacs, Chapter 5]: Let $ B $ and $ C $ be cyclic subgroups of a finite group G, and suppose that $ BC = G $ and $ B \cap C > 1 $ . Assume that $ C \lhd G $ and that $ |C:G^{'}|=n $ . Show that the order of $ M(G) $ is strictly less than $ n $ . I report my reasoning: For the third theorem of homomorphism, I know that $ B/B \cap C \cong BC/C=G/C $ and since $ B $ is cyclical, every quozient is also cyclical. So I'm in the hypothesis of the exercise 5A.5 from M.Isaacs Finite Group Theory and then I have to $ |M(G)| $ divides $ |C:G^{'}|=n $ . Therefore $ |M(G)| $ is less than $ n $ . I can't justify the fact that it has to be strictly less than $ n $ .  (I think it depends on the fact that $ C $ is a normal subgroup proper to $ G $ ).","['transfer-theory', 'cyclic-groups', 'finite-groups', 'abstract-algebra', 'group-theory']"
4883633,Number Fields Generated by Fourier Coefficients of Modular Forms,"We know that for every normalized Hecke eigenform $f$ , its Fourier coefficients generate a number field. I wonder if we have an ""inverse Galois"" type conjecture regarding which number fields can be generated in this way. Moreover, do we have some sort of statistical results/conjectures regarding the frequency of a number field appearing as a field generated by the Fourier coefficients of an eigenform? I would like to apologize in advance if this is a naive question.","['number-theory', 'modular-forms', 'algebraic-number-theory']"
4883646,"Integrate : $\int e^{x+e^{x+e^x}}\, dx$","Question : $$\int e^{x+e^{x+e^x}} dx$$ source : Integration Techniques and Tricks
University of Miami Mathematics Union
Trevor Birenbaum My attempt: can be rewritten as $$\int e^x (e^{e^x})^{(e^{e^x})} dx$$ let $z=e^{e^x} \implies dz  = e^{e^x} e^x dx$ so the integral transforms to $$\int \frac{z^z}{z}dz$$ now is it even possible to solve this","['integration', 'indefinite-integrals']"
4883671,Integration by parts on surfaces in weighted $L^2$ spaces,"I am on a closed and bounded surface $S$ without boundary.
Then it holds that $$
\int_S \nabla f \cdot \nabla f  \mathrm dA = -\int_S f\Delta f  \mathrm dA.
$$ This is the $H^1(S)$ seminorm, meaning that I can express the $H^1(S)$ norm as $$
\|f\|_1 =  \int_S f(1-\Delta) f  \mathrm dA.
$$ Imagine now I am on a weighted $L^2$ space with weight function $w$ . What can be said for $
\|f\|_1^w
$ ?
The problem becomes to integrate $$
\int_S w\nabla f \cdot \nabla f  \mathrm dA. 
$$ I tried to play around with various integration by parts, but I end up getting gradients of the weights and so on.
I see no reason why it shouldn't hold that $$
\|f\|_1^w = \int_S wf(1-\Delta)f \mathrm dA. 
$$ Here $\mathrm d A$ is just the Lebesgue measure, so if one says that we are instead integrating against $\mathrm dW = w\mathrm d A$ , then it just seems like it would hold, but I don't know.
Is it at all possible to rewrite in terms of only Laplacians? $$
\int_S w \nabla f \cdot \nabla f \mathrm d A? 
$$","['multivariable-calculus', 'sobolev-spaces']"
4883752,Must a countable disjoint union of closed balls in $\mathbb{R}^n$ with positive radius be disconnected?,"A disjoint union of open balls is of course disconnected. Here it is proved that a locally compact, connected, Hausdorff space is not a countable disjoint union of compact subsets, so a countable disjoint union of closed balls in $\mathbb{R}^n$ can't be connected and locally compact (hence cannot be open or closed connected), which rules out the connected sets for $n=1$ . But what if we just ask this disjoint union to be connected? Please forgive me if this question turns out to be trivial. Thank you in advance for any help. Edit. Actually I should have asked about something more general, like Is there a connected subset $\mathbb{R}^n$ that can be written as a countable disjoint union of closed sets in $\mathbb{R}^n$ ? If not, is there a connected subset $\mathbb{R}^n$ that can be written as a countable disjoint union of closed sets in the subspace topology? And the answer to the second question above is still negative for $n=1$ , or for open subsets of $\mathbb{R}^n$ (the link requires also local connectedness which is satisfied for open connected sets). However, I will keep the question as it was given the discussion already presented. Edit 2. A connected subset of $\mathbb{R}^3$ as a countable disjoint union of closed sets has be constructed in the existing answer. I would still be interested in the closed ball case: Must a countable disjoint union of closed balls in $\mathbb{R}^n$ not be connected?","['general-topology', 'analysis', 'real-analysis']"
4883765,A triangle whose area is four times the area of ​Napoleon's triangle,"There's a nice feature I came up with using GeoGebra about four years ago. I have some kind of argument for it but it's not entirely rigorous, I hope to formalize it in the answers. I would also be happy if anyone could provide a proof of pure Euclidean geometry Given: $∆ABC$ is a triangle, the equilateral triangles $∆BCX,∆CAY,∆ABZ$ are constructed towards the outside of the triangle $∆ABC$ , point $L$ is the center of $∆BCX$ , point $M$ is the center of $∆CAY$ , point $N$ is the center of $∆ABZ$ , and $L',M',N'$ are three points such that $L'M'∥LM$ , $M'N'∥MN$ and $N'L'∥NL$ such that $M',A,N'$ lie on one line and $N',B,L'$ lie on one line and $L',C,M'$ lie on one line.
Required: Prove that the area of the triangle $∆LMN$ is equal to a quarter of the area of the triangle $∆L'M'N'$ . An idea I have is to use the tiling property of Napoleon's triangle... Adding this network of triangles we get a network like this: Now it is clear that the large network is a withdrawal of a double network from the network of infinite Napoleon's triangles, and therefore each triangle has an area equal to four times the area of ​​Napoleon's triangle. I think the argument needs some modification to become rigorous. What should we do? Is the property I discovered already known? If so, please provide a reference that mentions it Edit: Regarding the tiling, it is copied from this video: https://youtu.be/KQ8cSuoopyc?si=rzq4soIOjSbBdasu","['euclidean-geometry', 'geometry']"
4883824,Why does T = S1 work for conics?,"If we have a conic with equation: $ax^2 + 2hxy + by^2 + 2gx + 2fy + c = 0$ or S =0, and a point $P(x_1, y_1)$ . Then the equation of the chord with P as its midpoint is given by T = $S_1$ . $S_1$ is obtained by plugging point P in S.    ( $S_1$ = $ax_1^2 + 2hx_1y_1 + by_1^2 + 2gx_1 + 2fy_1 + c = 0$ ) The 'T' form of an equation can be obtained by replacing: $$x^2 \rightarrow xx_1$$ $$y^2 \rightarrow yy_1$$ $$x \rightarrow \frac{x + x_1}{2}$$ $$y \rightarrow \frac{y + y_1}{2}$$ $$xy \rightarrow \frac{xy_1 + x_1y}{2}$$ I don't understand how this works ! Could someone help me understand why it does? Edit: The slope of the chord somehow equals the derivative of the conic at point $(x_1, y_1)$ . Can someone explain how that happens?","['conic-sections', 'geometry']"
4883836,"How many different totals can be obtained in a test with $20$ questions if one can obtain $-1$, $0$, or $4$ for a question?","In an exam there are $20$ questions, one can obtain either $-1,0$ or $4$ in each question based on marking scheme. How many different totals can he obtained in the test? One thing I observed different that even without repetition there is repetition as different permutation can obtain same number. I tried several ways like beggar method and others, but tackled the same above problem. But one thing I deduced that the lowest he can get is -20 and highest he can get is $80$ . So total $101$ number but in this some numbers are there he can't obtain so answer must lie between $1-101$ But certainly unable to find the answer.","['permutations', 'combinations', 'combinatorics']"
4883853,Finding functions $h$ satisfying $h(h(n))+h(n+1) = n+2$ for all natural $n$. Why does the Golden Ratio appear?,"Find all functions $h$ , on natural numbers satisfying the functional relation, $$h(h(n))+h(n+1) = n+2$$ for $n$ being a natural number. Answer : So, the function is $h(n) = [n\alpha]-n+1$ where $\alpha$ is the golden ratio. Doubt :
In the solution,the author goes backwards,i.e they already assume it and try to prove that it is true. But what I need is an alternate solution where we discover that the function has this golden ratio in it. There solution is like not even intuitive.","['contest-math', 'functional-equations', 'golden-ratio', 'functions', 'algebra-precalculus']"
4883869,Prove that $\lim_ {x\to \infty}\min(|x-\sqrt{m^2+2n^2}|)=0$,"For $x\gt0$ , let $f(x)$ be the minimum value of $|x-\sqrt{m^2+2n^2}|$ for all integers $m, n$ . Then Prove that $$\lim_ {x\to \infty} f(x)= \; 0$$ I am completely stuck with this question, minimum value of modulus is zero that's fine, but how $f(x)$ will be zero when $x$ approaches to $\infty$ . If I take $\sqrt{m^2+2n^2}$ to be some nearest value of $x$ Then it takes $|\infty -\infty|$ form; which can take finite values except zero too. How to solve this question","['limits', 'calculus', 'algebra-precalculus']"
4883872,Seeking clarification: Convergence of the series $\sum_{n=1}^{\infty} \frac{1}{e^{n^2}}$,"I was asked to show whether the following series converges: $$\sum_{n=1}^{\infty}\frac{1}{{e^{n^2}}}$$ Here's my first attempt which I thought was pretty straightforward: $$e^{n^{2}}=(e^{n})^{n}\ge e^{n} \ ,\ \forall \ n \in \Bbb{N}$$ $$\implies (e^{n})^{-n} \le e^{-n}$$ $$\implies\sum_{n=1}^{\infty}\frac{1}{{e^{n^2}}} \le \sum_{n=1}^{\infty}\frac{1}{{e^{n}}} $$ $$\sum_{n=1}^{\infty}\frac{1}{{e^{n}}} = \sum_{n=1}^{\infty}(\frac{1}{e})^{n}=\frac{\frac{1}{e}}{1-\frac{1}{e}}=\frac{1}{e-1}$$ Therefore the series in question converges.
I also thought about showing this in another way using the integral test. I have only read about the error function briefly so it is likely I did in fact make a mistake here: $$\frac{2}{\sqrt{\pi}}\sum_{n=2}^{\infty}\frac{1}{{e^{n^2}}}\le \frac{2}{\sqrt{\pi}}\int_1^\infty{e^{-t^{2}}}dt =\operatorname{erfc}(1) = 1-\operatorname{erf}(1)$$ $$\implies \sum_{n=2}^{\infty}\frac{1}{{e^{n^2}}} \le \frac{\sqrt{\pi}}{2}(1-\operatorname{erf}(1))$$ Adding the first term back into the series we see: $$\sum_{n=1}^{\infty}\frac{1}{{e^{n^2}}} = \frac{1}{e} + \sum_{n=2}^{\infty}\frac{1}{{e^{n^2}}} \le \frac{1}{e} + \frac{\sqrt{\pi}}{2}(1-\operatorname{erf}(1))$$ And since $\operatorname{erf}(1)$ and $\frac{1}{e}$ are both finite-valued this shows the series converges by the integral test. My professor told me both of these are wrong, but I am struggling to see why. Specifically that the statement $e^{n^{2}}=(e^{n})^{n}\ge e^{n} \ ,\ \forall \ n \in \Bbb{N}$ is incorrect. She also said that $e^{-x^{2}}$ is non-integrable, so using the error function is not valid. I am confused why I can't use the error function if it's ok to use other transcendental functions like log(x), exp(x), etc.","['transcendental-functions', 'calculus', 'sequences-and-series', 'convergence-divergence', 'error-function']"
4883874,The normal cone to a ball at a boundary point of the ball,"I got stuck in a problem that in the end I needed to know what is the normal cone to a ball $B = \left\{ x \in \mathbb{R}^n : \lVert x \rVert \leq 1 \right\}$ (here $\lVert \cdot \rVert$ is an arbirtrary norm), at a point at the boundary $\partial B$ . Recalling that the normal cone to a closed convex set $C$ at a point $x$ in $\mathbb{R}^n$ is the set $$N_C(x) = \left\{ y \in \mathbb{R}^n : \langle y,z-x \rangle \leq 0 \ \forall z \in C \right\}.$$ I manage to compute it for $n = 1$ , you can assume $B = [-1,1]$ , if I'm not mistaken, it is $$N_B(x) = \begin{cases} 
[0,+\infty), & \ \text{if} \ x=1, \\
(-\infty,0], & \ \text{if} \ x=-1.\end{cases}$$ Intuitively, I think the normal cone $N_B(x)$ will always be a half space at boundary points, but I don't know how this in a more general setting, is my intuition correct? If not, what is it? Any help, hints or references would be appreciated. $\textbf{Edit}:$ Actually when the norm is differentiable at the point I managed to see that the normal cone is a line (using e.g. Theorem 23.7 of Rockafellar's Convex Analysis book), so the problem actually lies in points where the norm is not differentiable.","['convex-geometry', 'convex-cone', 'convex-analysis', 'analysis']"
4883881,What are the possible dimensions of self-anti-involution matrices?,"I am considering $2^n\times 2^n$ complex matrices and a general anti-involution $A^{**}=A$ , $(AB)^*=B^* A^*$ . To my knowledge all anti-involutions are a combination of transpose, field involution and similarity transform (is that correct?). What are the possible dimensions of the space created by all matrices which are it's own anti-involution $$A=A^*$$ I am considering a complex number to be 2 dimensions (i.e. in real vector space). For example for $4\times 4$ matrices and the Hermitian conjugate for the anti-involution, you have 16 independent degrees of freedom (4 reals on diagonal and 6 complex numbers above diagonal). Is calling this a 16-dimensional vector space correct? If you were to choose the transpose as the anti-involution, you would have 20 dimensions (4 complex on diagonal and 6 complex numbers above diagonal). However, if you chose the anti-involution $$A^*=RA^TR^{-1}$$ with $$R=
\begin{pmatrix}
0&1&0& 0\\
-1&0&0&0\\
0&0&0&-1\\
0&0&1&0
\end{pmatrix}$$ then the matrices with $A=A^*$ would form only a 12-dimensional space (correct?). Is this the smallest dimension that self-anti-involution matrices can have? Which other values are possible? I think, generally, for $2^n\times 2^n$ matrices you can find anti-involutions which would make the space of self-anti-involution matrices $d=2^n(2^n+(-1)^{\lfloor n/2\rfloor})$ dimensional. My main motivation is to understand if these particular anti-involutions are special in some sense. Maybe one would restrict to not using complex conjugation (the field involution) for that.","['matrices', 'abstract-algebra', 'linear-algebra']"
4883900,A sequentially continuous linear operator on tempered distributions is continuous,"The following proposition is true. My question is whether we can find a direct, pedagogical proof that can be delivered in class without having to introduce new concepts from functional analysis. In the following, let $\mathcal S'=\mathcal S'(\mathbb R)$ denote the space of tempered distributions, i.e., the space of continuous linear functionals on the Schwartz class $\mathcal{S}=\mathcal{S}(\mathbb R)$ . Proposition . Let $T\colon \mathcal S'\to \mathcal S'$ be linear and sequentially continuous. Then $T$ is continuous. Remark . In practice, $T$ will be the Fourier transform. I find this proposition useful for teaching, since it allows for a streamlined proof that the Fourier transform $\mathcal F\colon \mathcal S\to \mathcal S$ extends to a continuous linear map $\mathcal S'\to \mathcal S'$ by taking adjoints. Proof . This is actually true with $\mathcal S$ replaced by a general separable locally convex space: see, for example, https://math.stackexchange.com/a/2099312/8157 . $\Box$ Attempt of a direct proof . A sub-base of neighborhoods of the origin in $\mathcal S'$ is given by the family $V_{f, \epsilon}$ , where $\epsilon >0$ , $f\in \mathcal S$ , and $$
V_{f, \epsilon}:=\{\psi\in\mathcal S'\ :\ \lvert\langle \psi, f\rangle\rvert<\epsilon\}.$$ In particular, $T$ is continuous if and only if, for all $f\in\mathcal S$ and $\epsilon>0$ there are $g_j\in\mathcal S$ and $\delta_j>0$ , with $j\in\{1, \ldots N_{f, \epsilon}\}$ , such that $$
\psi\in \bigcap_j V_{g_j, \delta_j} \Rightarrow T\psi\in V_{f, \epsilon}.$$ So, assuming this does not hold, i.e. $T$ is not continuous, we need to construct a sequence $\psi_n\in\mathcal S'$ such that $\psi_n\to 0$ but $T\psi_n\not\to 0$ , thus proving the contrapositive of the required proposition. The fact that $\mathcal S$ is separable must enter the picture here.","['harmonic-analysis', 'general-topology', 'functional-analysis', 'distribution-theory']"
4883929,Application of the residue theorem: how to use angular sectors,"I am studying the residue theorem applications to physics and in particular the angular sector method. I understand the concept of finding an angle such that the infinite half-segment along it acts as the real axis, but I found an integral that I do not know how to solve with this method: $$\int{\frac{1-x}{1+x^3}}$$ I tried the following: $$ \oint{\frac{1-z}{1-z^3} dz}=\int_{0}^{\infty}{\frac{1-x}{1-x^3}dx} + \int_{\infty}^0{\frac{1-z}{1+z^3}dz}=2\pi i \Sigma_jRes(f, z_j)$$ And taking $z=re^{2\pi i/3}$ in the last integral, I get $$ \oint{\frac{1-z}{1-z^3} dz}=\int_{0}^{\infty}{\frac{1-x}{1-x^3}dx} - \int^{\infty}_0{\frac{1-re^{2 \pi i /3}}{1+r^3}dr}=2\pi i \Sigma_jRes(f, z_j)$$ What I wanted to get is something of the form $(1-a)I=2\pi i \Sigma_j Res(f,z_j)$ , but I dont see anyway of factorizing. What should I do ? By the way, the integral should vanish, maybe it helps...","['complex-analysis', 'residue-calculus']"
4883933,Unique group law on cubic,"I'm looking at the following problem from Pg. 47 of Miles Reid's Undergraduate Algebraic Geometry ( https://homepages.warwick.ac.uk/staff/Miles.Reid/MA4A5/UAG.pdf ): 2.11 (Group law on  cuspidal cubic.) Consider the curve $$C:(z=x^3)\subset k^2;$$ $C$ is the image of the bijective map $\varphi\colon k\to C$ by $t\mapsto (t,t^3)$ , so it inherits a group law from the additive group $k$ . Prove that this is the unique group on $C$ such that $(0,0)$ is the neutral element and $$P+Q+R=0\iff P,Q,R\text{ are collinear}$$ for $P,Q,R\in C$ . [Hint: You might find useful the identity $$\left|\begin{array}{ccc}
1 & a & a^3\\
1 & b & b^3\\
1&c&c^3\end{array}\right| = (a-b)(b-c)(c-a)(a+b+c).]$$ In projective terms, $C$ is the curve $(Y^2Z=X^3)$ , our old friend with a cusp at the origin and an inflexion point at $(0,1,0)$ , and the point of the question is that the usual construction gives a group law on the complement of the singular point. There is a typo in the equation defining $C$ ; it should be $y=x^3$ . The group operation on $C$ should be $$
(a,a^3)+(b,b^3):=(a+b, (a+b)^3), 
$$ if I understand correctly. Showing that this operation satisfies the above biconditional is, I think, fairly straightforward. We can use the fact that the above determinant is zero iff the points $P$ , $Q$ , and $R$ are collinear. But I'm having trouble with the uniqueness part. Here's what I have so far: Suppose $\odot$ is a group operation on $C$ satisfying the relevant properties. We wish to show that this operation equals $+$ as defined above. Take $(s,s^3)$ and $(t,t^3)$ on the curve, and write $$
(s,s^3)\odot(t,t^3)=(x_{s,t},x_{s,t}^3). 
$$ We want $x_{s,t}$ to equal $s+t$ . If these points are distinct, then the line connecting them intersects $C$ at the point $(-s-t,(-s-t)^3)$ , so, using the collinearity condition, we get $$
(x_{s,t},x_{s,t}^3)\odot(-s-t,(-s-t)^3)=(0,0). 
$$ So we're almost there! This implies that $(-s-t,(-s-t)^3)$ is the inverse of $(x_{s,t},x_{s,t}^3)$ in the operation $\odot$ . But can we conclude that $$
(x_{s,t},x_{s,t}^3)=(s+t,(s+t)^3)?
$$ Thank you.",['algebraic-geometry']
4883938,"In geometric algebra, what is the dot product of a vector and a scalar? what is the wedge product of a vector and a scalar?","I am watching a series on geometric calculus by Alan Mcdonald and in the first episodes he states that for any vector u and multivector M: $uM = u \cdot M + u \land M$ This doesn't really take into account what happens with the 0-grade i.e scalar part of M, let's call it s. For the identity to be true either $u \cdot s$ or $u \wedge s$ has to be zero, and from what i read previously in some instances $u \land s = us$ , which implies $u \cdot s = 0$ , which kind of makes intuitive sense i guess?","['differential-geometry', 'multivariable-calculus', 'bilinear-form', 'geometric-algebras', 'clifford-algebras']"
4883948,Scrabble probability of Player A playing a U word?,"so I was recently involved in a probability debate with a friend involving Scrabble: I had been selected to be Player 1. It was my turn to play (first play of the game) and I had a q and was looking for a word that would have a U in it. I was contemplating skipping my turn in the hopes that my adversary would play a U. However, he told me that, from my perspective, it would be about a 4 * 4/93 chance that he would play a word with a U in it, since 4 was the average length of a scrabble word and there are 4/93 possible tiles in the bag. I disagreed, but could not come up with a valid counter, aside from some vague musings about dependent probabilities. Later on, I did some digging. It looks like you'd have to figure out the probability that  both that my friend (Player 2) has a U and will play it. I did some math and it seems like one approach could be to use a hypergeometric solution, like that posited here: Scrabble Probability to figure out the probability that he has a u. I followed this approach to figure out that the probability that my friend had at least one U (given that I don't have one), which came out to about 25.54%. I then multiplied this by the (number of words that are seven letters or less and have at least one u in them in the Scrabble dictionary)/(total number of words that are seven letters or less in the dictionary) to arrive at at about a 4.21% probability. My main question is where did I err? Would appreciate any and all insights!","['card-games', 'statistics', 'probability-theory', 'probability']"
4883953,"How to construct a concrete sequence $\{(a_i,b_i]\}$ such that the inequality $\sum_i(F(b_i)-F(a_i)) < \epsilon$ holds for all $\epsilon>0$?","I got stuck on the following problem: Let $F:\mathbb{R}\to\mathbb{R}$ be a bounded, nondecreasing, and right-continuous function that satisfies $\lim_{x\to-\infty}F(x)=0$ . Define a function $\mu^*:\mathcal{P}(\mathbb{R})\to[0,+\infty]$ by letting $\mu^*(A)$ be the infimum of the set of sums $\sum_{n=1}^{\infty}(F(b_n)-F(a_n))$ , where $\{(a_n,b_n]\}$ ranges over the set of sequences of half-open ivtervals that cover $A$ , in the sense that $A \subseteq \bigcup_{n=1}^{\infty}(a_n,b_n]$ . Prove that $\mu^*(\emptyset) = 0$ . I would like to show it in this way: For each positive number $\epsilon$ there is a sequence $\{(a_i,b_i]\}$ of half-open intervals (whose unions necessarily includes $\emptyset$ ) such that $\sum_i(F(b_i)-F(a_i)) < \epsilon$ . However, I failed to give a concrete argument showing that the above claim is true; for example, I couldn't construct such a concrete sequence $\{(a_i,b_i]\}$ such that the inequality $\sum_i(F(b_i)-F(a_i)) < \epsilon$ holds for all $\epsilon>0$ . Could someone please help me out? Thanks a lot in advance!","['measure-theory', 'proof-writing', 'analysis', 'real-analysis', 'sequences-and-series']"
4884037,Strongly convex sets are totally normal,"Let $(\mathcal{M},g)$ be a smooth Riemannian manifold. We say that an (open) subset $U \subset \mathcal{M}$ is strongly convex if and only if every pair $p$ , $q \in U$ has a unique
geodesic of minimum length connecting them, and this geodesic lies completely in $U$ . Note that besides the unique minimizing geodesic, there may be other geodesics between $p$ and $q$ which do not minimize length. I have seen multiple posts which claim that any strongly convex set is totally normal, i.e. for every $p \in U$ we have that $\exp_p: \exp_p^{-1}(U) \subset T_p\mathcal{M} \to U$ is a diffeomorphism, see for example here and here . However, I can't find a reference where this is shown and failed to prove it myself. I am aware that it suffices to show the injectivity of $\exp_p$ on $\exp_p^{-1}(U)$ for every $p \in U$ . In fact, assuming we already know that $\exp_p$ is locally injective $^1$ , we can deduce that any geodesic $\gamma : \lbrack 0, b \rbrack \to \mathcal{M}$ lying in $U$ cannot seize to be minimizing: Otherwise, there would exist a cut time $0 < c < b$ such that $\gamma$ is no longer minimizing past $\lbrack 0, c\rbrack$ and one can construct at least two distinct minimizing geodesics connecting $p$ with $\gamma(c)$ . $^2$ Therefore, all geodesics lying in $U$ must be minimizing and an argument similar to this answer then yields the claim. In summary : I am looking for an argument as to why $\exp_p$ is (locally) injective on the preimage $\exp_p^{-1}(U)$ if $U$ is a strongly convex set containing $p$ . $$ $$ $\small{^1 : \text{I.e. for every} \ v \in \exp_p^{-1}(U) \ \text{there exists an open neighborhood} \ V \subset \exp_p^{-1}(U) \ \text{of} \ v \ \text{such that} \ \ \\exp_p\vert_V \ \text{is injective}.}$ $\small{^2 : \text{C.f. Proposition} \ 10.32 \ \text{b)} \ \text{in Jack Lee's }}$ Introduction to Riemannian Manifolds","['geodesic', 'riemannian-geometry', 'differential-geometry']"
4884063,Suppose $A$ is a $7\times 7$ matrix with all entries less than $1$ in magnitude. Prove that $\det (100I+A) > 0$,"Suppose $A$ is a $7\times 7$ matrix with all entries less than $1$ in magnitude. Prove that $\det (100I+A) > 0$ . This is my first post on the mathematics stack exchange, so forgive me if it’s not in the right format or if it’s been asked before (I couldn’t find anything like it, though). My elementary linear algebra professor posted this question in the beginning of class this morning and left it up to us to discuss independently. I can gather that the determinant of $100 I_7$ is $100^7$ and every element in $A$ is in between $-1$ and $1$ (given) but other than that, I’ve got no clue where to start. Thank you!","['determinant', 'proof-writing', 'linear-algebra']"
4884068,The positive Laplacian is indeed the negative Laplacian,"I know this question sounds like a joke. And it probably is:). I found it kind of annoying, but also interesting, to call $-\Delta=-\sum_{j=1}^n\partial^2_{jj}$ ""the positive Laplacian"" as it is the positive operator with respect to standard $L^2$ -pairing. But in the mean time it is also regarded as the ""negative of Laplacian"" since we have a minus sign. I think there are two different questions related to this topic. What is a more clear way to explain the difference on what positive and negative is for a Laplacian? And also how to use such terminologies appropriately in different circumstances. I believe such clarification is important, as the Hodge-Laplacian $\Box=dd^*+d^*d$ (let me not using $\Delta$ here) can be called a ""positive Laplacian"" with slightly less ambiguity. Also the word ""subharmonic functions"" refers to those $f$ such that $-\Delta f\le0$ , but not $\Delta f\le0$ . Are other similar (but also interesting) confusions happen in other fields of mathematics? If I remember correctly, the meaning of covariant tensors and contravariant tensors in commutative algebra and in differential geometry are flip. They seem less ""annoying"" than Laplacian though.","['education', 'laplacian', 'calculus', 'analysis']"
4884079,Euclidean Geometry problem regarding the line passing through incenter,"This is from the exercise problems(not the live contest) for regional math competition. Let triangle $ABC$ satisfy $\angle C > 90^\circ$ and let its incenter be $I$ . Let $\ell$ be the line parallel to $BC$ passing through $I$ , and call $\ell \cap AB = D$ , $\ell \cap BC = E$ . We know that $AI = 3$ and the radius of inscribed circle of the triangle $ABC$ is $1$ . Finally, $(\text{area of }ABC) = 5\sqrt{2}$ is given. The problem is to find $DE$ . Below is my attempt to do something but I am stuck. How can I proceed to get $DE$ ? Thank you in advance for any form of help, hint, or solution.","['contest-math', 'euclidean-geometry', 'geometry']"
4884092,"""Nice"" values of x such as sin(x) is transcendental","First time posting here so please forgive any lack of adherence to best practices. sin(x) is a transcendental function. However most common values for the angle x will yield an algebraic number result. In particular, sin(q) (in degrees) or sin(qπ) (in radians) will always be algebraic. I can reverse-engineer a transcendental result by making y=sin(x), assigning a transcendental number to y in [-1, 1], and then solving for x. For example, sin(arcsin(1/e)) must be transcendental.  But x=(arcsin(1/e)) is not what I would call a ""niece value of x"". After a couple of hours researching the internet I didn't find much. I have the intuition that sin(q) (q rational, not zero and in radians), as well as sin(k) (k irrational and in degrees, or in radians but not of the form qπ) would most of the times (or always?) be transcendental. And these would be ""nice"". But I know it can be extremely difficult to prove. So the question is, are there ""nice"" values of x such as sin(x) is confirmed to be transcendental? I tried sin(1 radian) in Wolfram Alpha and it said it is transcendental. Is that true? How does it even know? What about other integers or π (in degrees)? Or 1/π, e, Φ, ln(2) or √2 (either in radians or degrees)? Would the sine of these numbers be transcendental?","['trigonometry', 'transcendental-functions', 'transcendental-numbers']"
4884093,Is my trigonometric proof of the Pythagorean theorem correct?,"Actually, this is more of a proof for the trigonometric identity, $$\sin^2\theta + \cos^2\theta = 1.$$ First use right triangle, $ABC$ with angle $C$ being the right angle. Drop an altitude from point $C$ to line $AB$ making the point $D$ . Let's name the segments: $AB = c$ , $AC = b$ , and $BC = a$ . Also, $AD = c_1$ , $BD = c_2$ , so $c_1 + c_2 = c$ . Then, $b\cos A = c_1$ , $a\cos B = c_2$ , so $b\cos A + a\cos B = c$ . Since we know that $\angle A + \angle B = 90^\circ$ , we also know that $\cos B = \sin A$ . Thus, $b\cos A + a\sin A = c$ , or $$
\frac{b}{c}\,\cos A + \frac{a}{c}\, \sin A = 1.
$$ Since $\frac{b}{c} = \cos A$ and $\frac{a}{c} = \sin A$ , we have $$
\cos^2 A + \sin^2 A = 1.
$$","['euclidean-geometry', 'geometry', 'solution-verification', 'trigonometry', 'algebra-precalculus']"
4884121,"Expectation of maximum number of non-overlapping random intervals inside $[0,1]$ out of $n$ intervals","Choose two random numbers from $[0,1]$ and let them be the endpoints of a random interval. Repeat this independently for $n$ times to get $n$ random subintervals (denoted as $U_1,\dots,U_n$ ). Let $T_n = \max\{|S|: S\subset\{U_1,\dots,U_n\}, U_i\cap U_j=\emptyset \mbox{ for any }U_i\not =U_j\in S\}$ , where $|S|$ denote the number of intervals in $S$ . I'm wondering what is the expectation of $T_n$ ? Or how fast does the expectation of $T_n$ increase with $n$ ? ( is it $O(n)$ , $O(\sqrt{n})$ or $O(\log n)$ ?)","['statistics', 'probability']"
4884200,"If $m\mid n$ can we find a group $G$ of order $n$ with a subgroup $H$ of order $m$, i.e. can every divisibility be proved group theoretically?","I came across the following problem: Let $p$ be a prime, prove that $$(p-1)^np^{(n^2-n)/2}\mid \prod_{k=1}^n(p^n-p^{k-1})$$ for any $n\in \mathbb{N}$ . It is quite hard to prove it using elementary number theory. However, the problem becomes trivial if we notice that $|\operatorname{GL}_n(\mathbb{F}_p)|=\prod_{k=1}^n(p^n-p^{k-1})$ and $|\operatorname{B}_n(\mathbb{F}_p)|=(p-1)^np^{(n^2-n)/2}$ , where $\operatorname{B}_n(\mathbb{F}_p)$ is the set of $A\in \operatorname{GL}_n(\mathbb{F}_p)$ that is upper triangular. And the proof follows immediately from Lagrange's theorem. So that got me thinking, can every divisibility problem $m\mid n$ be solved using group theory and Lagrange? More precisely, if $m\mid n$ , can we always find a group $G$ of order $n$ and a subgroup $H$ of $G$ of order $m$ ? I know that the above is true for $m=p$ a prime by Cauchy's theorem or more generally for $m=p^k, k=\operatorname{ord}_p(|G|)$ by Sylow. But I also know that the converse of Lagrange's theorem is false in general. I have tried a few different pairs of small $n,m$ and it seems to work out but I can't prove of disprove the above statement.","['group-theory', 'abstract-algebra', 'finite-groups']"
4884222,Can an ODE system never converge to its stable equilibrium in the long run?,"I have the following coupled non-linear ODE system, which describes a biological system: $$
\begin{cases}
\dfrac{dp}{dt} = -\gamma p f,\\
\\
\dfrac{df}{dt} = \gamma p f,\\
\\
\dfrac{dT}{dt} = \left( 1 - \dfrac{k}{T} \right) \left\{ b (1 - T) \dfrac{r^n}{f^n + r^n} - m \dfrac{f^n}{f^n + r^n} \right\},
\end{cases}
$$ where the parameters are: $\gamma = 0.5432$ , $k = 0.0026$ , $b = 0.0885$ , $n = 14.9832$ , $r = 0.8265$ , and $m = 0.9780$ . I want to investigate the stability of the system. To this end, first, one observes that the first two equations imply that $p + f$ must be a constant, let's call it $w$ , and in our case, it must be non-negative (for the chosen parameters' values of the system in the above, it's $0.801$ ). Now, by excluding $p$ from the above system, we can write: $$
\begin{cases}
\dfrac{df}{dt} = \gamma (w - f) f,\\
\\
\dfrac{dT}{dt} = \left( 1 - \dfrac{k}{T} \right) \left\{ b (1 - T) \dfrac{r^n}{f^n + r^n} - m \dfrac{f^n}{f^n + r^n} \right\}.
\end{cases}
$$ The second system has four equilibria, $(f, T)$ : $$(0, k), (0, 1), (w, k), \left( w, 1- \frac{m}{b} (w / r)^n \right).$$ Now, by calculating the eigenvalues of the Jacobian, it turns out that the first two equilibria are unstable, and the last two are stable equilibria. My question is as follows: If we simulate our original, i.e., the first system, using Mathematica for some initial conditions: s = NDSolve[{p'[t] == -gamma p[t] f[t], f'[t] == gamma p[t] f[t], 
T'[t] == b (1 - T[t]) (r^n/(f[t]^n + r^n)) (1 - 0.0026/T[t]) - m (f[t]^n/(f[t]^n + r^n)) (1 - 0.0026/T[t]), 
p[0] == p0, f[0] == 0.001, T[0] == T0}/.{gamma -> 0.5432, b -> 0.0885, m -> 0.9780, r -> 0.8265, n -> 14.9832, p0 -> 0.8, T0 -> 0.8}, {p, f, T}, {t, 0, 720}];

Plot[Evaluate[{f[t], T[t]}/.s], {t, 0, 720}, PlotStyle -> {Blue, Red}, AxesOrigin -> {-1, 0}, PlotRange -> All] we obtain: From the above plot it's clear that solutions are converging to $(w, k)$ in the long run, which here $w$ is around $0.8$ . However, it seems that the system never converges to $\left( w, 1- \frac{m}{b} (w / r)^n \right)$ , for any initial conditions. What is the reason behind that? Can a system never converge to one of its stable equilibria? EDIT The reason for transforming the original system: If one wants to do the stability analysis for the original system, one runs into a problem. It has two equilibria: $(f \to 0, T \to k)$ and $(f \to 0, T \to 1)$ , regardless of the value of $p$ . Since $p$ can be anything, it isn't clear how one can calculate the Jacobian. However, by transforming the system, we can eliminate this issue.","['stability-theory', 'ordinary-differential-equations']"
4884256,Can I avoid differentiation to find the remainder when $x^{73}-2x^{15}+3x-1$ is divided by $(x-1)^2$,"The problem states: Find the remainder when $x^{73}-2x^{15}+3x-1$ is divided by $(x-1)^2$ . So, what I did is , Assume, the remainder to be linear, i.e $r(x) = ax+b$ By Eucild''s, $x^{73}-2x^{15}+3x-1 = q(x)(x-1)^2+r(x)$ Putting x = 1, We get, $a+b = 1$ . Then I differentiated it, and the rest is trivial. How can I skip differentiation and get the answer by some other method/way.
I was thinking perhaps something involving number theory?
I want to know is it possible to skip calculus in such questions?","['calculus', 'derivatives', 'algebra-precalculus', 'polynomials']"
4884274,Evaluate $\int_{0}^{c} \int_{x}^{c} e^{x^2+y^2}dydx$. Given $\int_{0}^{c}e^{s^2}ds = 3$,"Evaluate $$\int_{0}^{c} \int_{x}^{c} e^{x^2+y^2}dydx$$ Given $$\int_{0}^{c}e^{s^2}ds = 3$$ where c is a positive constant. The solution to this question mentioned that: $$\int_{0}^{c} \int_{x}^{c} e^{x^2+y^2}dydx = \frac{1}{2}  \int_{0}^{c} \int_{0}^{c} e^{x^2+y^2}dydx$$ Because the integral in the RHS is evaluated on a triangular region bounded by: $$ y = x \,\,\,\&\,\,\, y = c$$ whereas the one on the LHS is a square region whose area is double the triangular region. Though I can observe that the region on XY - plane is indeed double, I'm not sure if the relationship would hold while taking a double integral over a non-constant function.","['integration', 'iterated-integrals', 'multivariable-calculus', 'definite-integrals']"
4884311,Wasserstein-2 distance between $\mu_1$ and $\mu_2$ equals the $L^2$ distance between the Fourier transforms $\hat{\mu}_1$ and $\hat{\mu}_2$,"Plancherel's theorem states that the Fourier transform preserves the $L^2$ norm, meaning that if $f$ and $g$ are functions whose Fourier transforms are denoted by $\hat{f}$ and $\hat{g}$ respectively, then: $$
\| f \|_{L^2} = \| \hat{f} \|_{L^2}
$$ Similarly, is it true that: if $\mu_1$ and $\mu_2$ are probability measures and $\hat{\mu}_1$ and $\hat{\mu}_2$ are their respective Fourier transforms, then the $L^2$ distance between $\hat{\mu}_1$ and $\hat{\mu}_2$ is equal to the Wasserstein-2 distance between $\mu_1$ and $\mu_2$ ? Mathematically, this can be expressed as: $$
W_2(\mu_1, \mu_2) = \left( \int_{X \times X} |x - y|^2 d\pi(x,y) \right)^{1/2} = \left( \int_{\mathbb{R}} |\hat{\mu}_1(\xi) - \hat{\mu}_2(\xi)|^2 d\xi \right)^{1/2}
$$","['measure-theory', 'statistics', 'fourier-analysis', 'complex-analysis', 'probability-theory']"
4884321,ODE and semi flow,"I am new to ODE and semi-flows. So, it is a beginner question. If an ODE $x^{\prime} = h(x)$ admits a unique solution, is it right to say that the flow generated by this equation, i.e. $\Phi_t(x) = x + \int_0^t h(s) \,\mathrm{d}s$ satisfies those properties : (a) $\Phi_0 = Id$ ; (b) $\Phi_t$ is continuous $\forall t$ ; (c) $\Phi_{t+s} = \Phi_t \circ \Phi_s$ $\forall s,t \geq 0$ . I can prove the first two points, but I fail to prove the third one. I try to prove this point in a particular case where $T$ is a contraction on $\mathbb{R}^d$ and $h = T - I_d$ . Thanks in advance",['ordinary-differential-equations']
4884362,Equation 8.39 to 8.41 in the book Neuronal Dynamics,"In the book neuronal dynamics chapter 8.4 ( https://neuronaldynamics.epfl.ch/online/Ch8.S4.html ), the authors derive the Fokker-Planck equation for a noisy neuron. Starting from equation 8.39: $ p(u, t+\Delta t) = [1- \Delta t \sum_{k} v_k(t)] e^{\Delta t / \tau} p(e^{\Delta t / \tau} u, t) + \Delta t \sum_k v_k(t) [p(u-w_k, t) - p(u,t)] $ First, they do a Taylor approximation of $ \Delta t  $ around 0. Doing it myself, $ p(e^{\Delta t / \tau} u, t) \approx p(u, t) + u \Delta t \frac{\partial}{\partial u} p(u, t) $ $ e^{\Delta t / \tau} \approx 1 + \frac{\Delta t}{\tau} $ Ignoring all the terms with $ \Delta t^2 $ , I reach the following result $ \frac{\partial}{\partial t} p(u, t) = \frac{1}{\tau} p(u, t) + u \frac{\partial}{\partial u} p(u,t) - \sum_k v_k ( p(u-w_k, t) - p(u, t)) $ However, the book claims that there is a factor of $ \frac{1}{\tau} $ multiplying $u \frac{\partial}{\partial u} p(u,t)$ . In equation 8.40, $ \frac{\partial}{\partial t} p(u, t) = \frac{1}{\tau} p(u, t) + \frac{1}{\tau} u \frac{\partial}{\partial u} p(u,t) - \sum_k v_k ( p(u-w_k, t) - p(u, t)) $ More interestingly, this term somehow disappears when going from 8.40 to 8.41. The goal is to approximate the equation once more with Taylor expansion with $w_k$ going to 0, keeping the terms up to $w_k^2$ . When I assume 8.40 to be true and use the expansion: $p(u-w, t) = p(u, t) - w\frac{\partial}{\partial u} p(u,t) + \frac{1}{2} w^2 \frac{\partial^2}{\partial u^2} p(u,t)$ I reach the following solution: $\tau \frac{\partial}{\partial t} p(u, t) \approx u \frac{\partial}{\partial u} p(u, t) - \frac{\partial}{\partial u} [-u + \tau \sum_k v_k(t) w_k] p(u, t) + \frac{1}{2}[\tau \sum_k v_k(t) w_k^2] \frac{\partial^2}{\partial u^2} p(u, t) $ The solution in the book (equation 8.41) omits the term $u \frac{\partial}{\partial u} p(u, t)$ : $\tau \frac{\partial}{\partial t} p(u, t) \approx - \frac{\partial}{\partial u} [-u + \tau \sum_k v_k(t) w_k] p(u, t) + \frac{1}{2}[\tau \sum_k v_k(t) w_k^2] \frac{\partial^2}{\partial u^2} p(u, t) $ I would appreciate any help showing where I'm being mistaken.",['multivariable-calculus']
4884375,Integration of hypergeometric function on complex plane,"I have come across an integral that involves a hypergeometric function, which can be expressed as follows: $$I = \int_0^1 x^{1/2}(1-x)^{\epsilon-1}  {_{2}F_1}(\frac{1}{2}+\epsilon,1+\epsilon;\frac{3}{2};x) dx.$$ Here, $\epsilon$ is a small complex quantity where $|\epsilon|\ll1$ . I found an integral formula in ""Table of Integrals, Series, and Products,"" ET II 399(4), as follows: $$\int_0^1 x^{\gamma-1}(1-x)^{\rho-1}F(\alpha,\beta,;\gamma;x)dx=\frac{\Gamma(\gamma)\Gamma(\rho)\Gamma(\gamma+\rho-\alpha-\beta)}{\Gamma(\gamma+\rho-\alpha)\Gamma(\gamma+\rho-\beta)}$$ ,
for $Re \ \gamma\gt0, Re\ \rho\gt0, Re\ (\gamma +\rho -\alpha - \beta)\gt0$ . As one can see, for my case, $\alpha=1/2+\epsilon, \beta=1+\epsilon, \gamma=3/2, \rho=\epsilon$ , and if assume $Re\ (\epsilon) \gt 0$ (this is not necessarily true), the third condition can not be satisfied as $Re\ (\gamma +\rho -\alpha - \beta)= Re\ (-\epsilon)\lt0$ . I have a question regarding the integral $I$ . Does the given case imply that $I$ diverges, or is there any other way to determine the result? If possible, I would appreciate it if someone could guide me on how to solve this integral. Thank you all for taking the time to read my post! EDIT: A little background for the problem set: I intend to compute this complex integral along the path $C$ on the complex plane as follows: $$K=\int_C\frac{z^2}{1+z^2} {_2F_1(\frac{1}{2}+\epsilon,\frac{1}{2}-\epsilon;\frac{3}{2};-z^2)}dz,$$ where $z=x/\delta$ , $x$ is real, $\delta=\sqrt{(4\epsilon^2-1)\eta}$ , here, $\epsilon$ is a small complex quantity ( $|\epsilon|\ll1$ )(so $Re (\epsilon)$ can be positive or negative, whatever helps, we can assume that) and $0<\eta\ll1$ is a real parameter; the integration path $C$ is determined by $z=x/\delta$ , where x is taken from $(0,+\infty)$ . My attempt is first taking a transform of variable, $t=z^2$ , and the integral $K$ is converted into the following form: $$K=\frac{1}{2}\int_{C'} t^{1/2}(1+t)^{-1} {_2F_1(\frac{1}{2}+\epsilon,\frac{1}{2}-\epsilon;\frac{3}{2};-t)}dt$$ , then making use of the relation: $$F(a,b;c;z)=(1-z)^{-a}F(a,c-b;c;\frac{z}{z-1})$$ , yielding $$K=\frac{1}{2}\int_{C''} t^{1/2}(1+t)^{-3/2-\epsilon} {_2F_1(\frac{1}{2}+\epsilon,1+\epsilon;\frac{3}{2};\frac{t}{t+1})}dt$$ .
At this point, I took a transform of the variable as $l=t/(t+1)$ and ended up something like this: $$K=\frac{1}{2}\int_{C'''}l^{1/2}(1-l)^{\epsilon-1} {_2F_1(\frac{1}{2}+\epsilon,1+\epsilon;\frac{3}{2};l)dl},$$ where the paths of integration, $C'$ , $C''$ and $C'''$ change accordingly with the transformation of variables. Finally, if $x: 0 \to +\infty$ , $l$ varies from $0 \to 1$ along the path $C'''$ , therefore I thought it is safe to say(I think I made a serious mistake here, even though I am not quiet sure why): $$K=\frac{1}{2}\int_0^1l^{1/2}(1-l)^{\epsilon-1} {_2F_1(\frac{1}{2}+\epsilon,1+\epsilon;\frac{3}{2};l)dl}.$$ Obviously, from the comment by @Conrad, at $l=1$ , the integrant is singular, so I need to be careful by changing the integral path from $C'''$ to the real axis. What should I do at this point? I would appreciate it so much if someone could tell me how to deal with this problem; thank you all for your time reading my post.","['definite-integrals', 'special-functions', 'complex-analysis', 'contour-integration', 'hypergeometric-function']"
4884383,"Generalization of Geometric Mean, Standard Deviation, etc.","The geometric mean can be thought of as the exponential of the arithmetic mean of the logarithms of your dataset $\{a_i\}_{i=1}^n$ : $$GM = \exp\left(\dfrac{1}{n}\sum_{i=1}^{n}\ln(a_i)\right)$$ Similarly, the standard deviation of a dataset is the square root of the arithmetic mean of the squares of your errors $\{e_i\}_{i=1}^{n}$ : $$SD = \sqrt{\dfrac{1}{n}\sum_{i=1}^{n}(e_i)^2}$$ I find it interesting that both of these ideas seems to follow of more general pattern: $$f^{-1}\left( \dfrac{1}{n} \sum_{i=1}^{n} f(x_i) \right),$$ where the function in question is $f(x) = x^2$ for the standard deviation, and $f(x) = \ln x$ for the geometric mean. Even the arithmetic mean is trivially of this form, just with $f(x)= x$ as the identity function. Are there other widely-used variations of these kinds of ""functional"" averages? And is there anything we can say, more universally, about these kinds of averages as a whole? In particular, I find it interesting that all three of the averages I mentioned above (when applied to two values) all give values that are between the two data points. What would have to be true about a function $f(x)$ for this ""midpoint"" property to hold? For example, $\arcsin\left( \frac{1}{2} (\sin a + \sin b)\right)$ is most certainly not between $a$ and $b$ in most cases.","['average', 'statistics', 'functions', 'standard-deviation']"
4884395,"Without choice, what can be the (finite) automorphism groups of $\mathbb F_2$-vector spaces?","My motivation for this question is similar to the one in this question . However, that question only asks about the possibility of an infinite $\mathbb F_2$ -vector space having trivial automorphism group. What other finite automorphism groups can occur? In particular, can $\mathbb Z/(3)$ occur? (see also this question for extra motivation).","['automorphism-group', 'group-theory', 'linear-algebra', 'axiom-of-choice', 'set-theory']"
4884443,Extrema of derivate are where tangent crosses the curve.,"In this article https://www.jstor.org/stable/2310782 i found this proposition: Let $f$ be a differentiable function defined on an open interval $(a, b)$ containing
the point $x_0$ . Let: (B) There exists an open interval $I\subset (a, b)$ , $x_0\in I$ , such that on $I$ the function $f'$ attains a (local) maximum or minimum at $x_0$ . (C) There exists an open interval $I\subset (a, b)$ , $x_0\in I$ such that on I we have $T\ge f$ on one side of $x_0$ and $T\le f$ on the other side. Here $T$ is the tangent to the
graph of $f$ at the point $x_0$ . Then (B) implies (C). I ask help for proof thats if $x_0$ is local minimum of $f'$ (i.e. $\exists \delta>0$ : $f'(x)\ge f'(x_0)$ for $x_0-\delta<x<x_0+\delta$ ), then $f\le T$ in $(x_0-\delta,x_0)$ and $f\ge T$ in $(x_0,x_0+\delta)$ . $[T(x)=f(x_0)+f'(x_0)(x-x_0)]$ . Ps. On Wikipedia ( https://en.wikipedia.org/wiki/Inflection_point ) i found that ' If all extrema of $f'$ are isolated (that is, in some neighborhood, x is the one and only point at which f' has a (local) minimum or maximum), then an inflection point is a point on the graph of f at which the tangent crosses the curve.'","['inflection-point', 'tangent-line', 'real-analysis', 'maxima-minima', 'derivatives']"
4884494,Show that this wreath product is representable by matrices.,"We know that every finitely generated metabelian group $\Gamma$ has a faithful representation over a finite product of fields.
Denoting $C_n$ the cyclic group of order $n$ ,   the restricted wreath product $C_6\wr C_\infty$ is linear over a product of two fields ( a field of char. 2 and a field of char. 3). May I ask how to construct this injective homomorphism from $C_6\wr C_\infty$ to the product of general linear group? Let $t$ be the generator for $C_\infty$ and $s$ be the generator for $C_6$ , I know that $C_6\wr C_\infty = \langle s, t \rangle$ , but I am not sure where the generator should be mapped to. Any hint/reference would be really appreciated.","['wreath-product', 'group-theory', 'abstract-algebra']"
4884509,"Global version of an ideal being ""finitely generated up to radical""","Let $X$ be a quasicompact, quasiseparated scheme and $Z$ a closed subset of $X$ with quasicompact complement. Is there a finite type quasicoherent ideal sheaf $\mathcal{J}$ on $X$ which cuts out $Z$ ? (more formally, such that $\mathcal{O}_X/\mathcal{J}$ has support $Z$ ?) In the affine case the answer is yes: $V(\mathfrak{a})$ has quasicompact complement iff $\mathfrak{a}$ is finitely generated up to radical (edit: meaning there are finitely many $f_1,\ldots,f_n$ such that $\sqrt{(f_1,\ldots,f_n)} = \sqrt{\mathfrak{a}}$ ). But I'm having trouble globalizing this. We can take a finite cover of $X$ by affines (and cover all pairwise intersections by finitely many mutually distinguished opens) and take arbitrary finitely generated defining ideals on each affine, but I don't see how we can modify these such that they glue together to a global ideal sheaf. Here is what I've thought about so far: write $X = U_1 \cup \ldots \cup U_n$ for $U_i$ affine and $U_{ij} = V_{ij}^1 \cup \ldots \cup V_{ij}^{k_{ij}}$ . Let $\mathfrak{a}_i \leq \mathcal{O}_X(U_i)$ be a finitely generated ideal with $V(\mathfrak{a}_i) = Z \cap U_i$ . We can choose $N$ uniform such that for any $i, j$ we have $(\mathfrak{a}_i \mathcal{O}_X(U_{ij}))^N \subseteq \mathfrak{a}_j \mathcal{O}_X(U_{ij})$ . In fact more is true: write $U_{ij} = D(f_i)$ for $f_i \in \mathcal{O}_X(U_i)$ and $\mathfrak{a}_i = (a_{i,1},\ldots,a_{i, r_i})$ ; then there is some uniform $K$ such that for any $i, j$ and any product of $N$ generators of $ \mathfrak{a}_i$ there is a relation showing that that product lies in $\mathfrak{a}_j \mathcal{O}_X(U_{ij})$ whose coefficients' denominators have power of $f_j$ at most $K$ . But I don't see how any of this actually helps. I would prefer a ""global"" or ""all at once"" answer rather than one which uses the induction principle for qcqs schemes. Edit: Here is a construction which I think works, but I'm unsure. Cover $X$ by finitely many affine opens and cover the pairwise intersections by finitely many simultaneously distinguished opens. Further iterated intersections will be affine (they're distinguished in some cover element) and there are only finitely many of them overall. So we have a finite set $\mathcal{P}$ of affine opens which cover $X$ and where the pairwise intersection of any two elements is covered by elements. Since $\mathcal{P}$ is finite it's in particular well founded and so we can define functions on it by recursion; we use this to define a family of finitely generated ideals $I(U) \leq \mathcal{O}_X(U)$ such that $I(U)$ has vanishing locus $Z \cap U$ and $I(U) \mathcal{O}_X(V) \subseteq I(V)$ whenever $V\subseteq U$ . Let $U\in \mathcal{P}$ be arbitrary and suppose $I(V)$ has been defined for all $V \subsetneq U$ in $\mathcal{P}$ . Because $U$ is affine and $Z\cap U$ is a closed subset with quasicompact complement there is a finitely generated ideal $J \leq \mathcal{O}_X(U)$ with vanishing locus $Z\cap U$ . Then for each $V \subsetneq U$ the ideal $J \mathcal{O}_X(V)$ is finitely generated and contained in $\sqrt{I(V)}$ , which implies there is some $n$ such that $J^n \mathcal{O}_X(V) \leq I(V)$ . Since there are only finitely many $V$ we can find a uniform $N$ such that for any $V$ we have $J^N \mathcal{O}_X(V) \leq I(V)$ . Define $I(U) = J^N$ . This construction gives us a diagram of ideals of $\mathcal{O}_X(U)$ for $U\in \mathcal{P}$ . If we let $i_U : U \to X$ be the inclusion then we can also view $U \mapsto (i_U)_*\widetilde{I(U)}$ as a functor $\mathcal{P}^{\mathrm{op}} \to \mathcal{O}_X-\mathsf{Mod}$ equipped with a monomorphism to the functor $U \mapsto (i_U)_*i_U^*\mathcal{O}_X$ . Since $X$ is qcqs all these pushforwards are in fact quasicoherent modules, and the sheaf condition tells us the limit of that latter functor is $\mathcal{O}_X$ . Then $\mathcal{J} = \lim_{U\in\mathcal{P}} (i_U)_*\widetilde{I(U)}$ is a quasicoherent sheaf equipped with a monomorphism to $\mathcal{O}_X$ (the limit of a family of monomorphisms between two diagrams is a monomorphism) ie is a quasicoherent ideal sheaf. We built this $\mathcal{J}$ by locally constructing a fg ideal cutting out the right subset, so it feels like it should still be fg with the right support. But maybe taking this limit destroys any hope of finiteness? I guess the hope is that a finite limit of (sheaves of) rings equipped with finite type ideal sheaves is still finite type?","['algebraic-geometry', 'schemes', 'quasicoherent-sheaves']"
4884530,The integral of the absolute value of the Gaussian curvature of a compact surface,"I want to prove the following theorem: Let $S$ be a compact surface, and $N:S\rightarrow \Bbb{S}^2$ the Gauss map, then we have $$ \int_{S} |K| \,dA = \int_{\Bbb{S}^2} \#N^{-1} \,dA $$ where $K$ is the Gaussian curvature of the surface $S$ , and $\#N^{-1}$ is the number of the preimages of the Gauss map. I know that by the stack of record theorem and the fact that $\Bbb{S}^2$ is connected, the value $\#N^{-1}$ should be a constant locally. Also, I know that locally (within a local chart of $S$ ), we should have $ \int_{U} |K| \,dA = \int_{N(U)} \,dA $ But I don't know how to prove the theorem globally by involving the number of preimages of the Gauss map. Any hints would be helpful.","['curvature', 'differential-geometry']"
4884550,Prove that $f(A \cap f^{-1}(B))=f(A) \cap B$,"I'm attempting to prove that for a function $f: X \rightarrow Y$ and $A \subset X$ , $B \subset Y$ , the following equality holds: $$ f \left( A \cap f^{-1}(B) \right) = f(A) \cap B. $$ "" $\mathbf{\subset}$ "": Using that $f(A \cap A') \subset f(A) \cap f(A')$ and $f(f^{-1}(B)) \subset B$ , we get $$ f \left( A \cap f^{-1}(B) \right) \subset f(A) \cap f\left(f^{-1}(B)\right) \subset f(A) \cap B. $$ "" $\mathbf{\supset}$ "": For the reverse direction, I derived the following proof: $$ \begin{align*}
y \in f(A) \cap B &\Longleftrightarrow y \in f(A) \wedge y \in B \\
&\Longleftrightarrow (\exists x \in A : f(x) = y) \wedge y \in B \\
&\Longleftrightarrow \exists x \in A : f(x) = y \wedge x \in f^{-1}(B) \\ &\Longleftrightarrow \exists x \in A \cap f^{-1}(B) : f(x) = y \\
&\Longleftrightarrow y \in f \left( A \cap f^{-1}(B) \right)
\end{align*} $$ While I'm aware from other questions that the overall direction is correct, there seems to be a flaw in how I've articulated the proof since it demonstrates equality, which I suspect might not be accurate. I also apologize for the formatting issues but I couldn't manage the equation to be displayed as multi-line equation. Update I'm not a big fan of simply chaining equations without much explanation. However on of my professors uses this extensively in his course. Therefore I though this would be a good exercise. Furthermore I'm interested in whether the equivalency is true or not. I suspect it's not but am not sure where my error is.","['elementary-set-theory', 'proof-writing', 'functions', 'solution-verification']"
4884626,Value of derivative of complex function,"Let $f:\mathbb{C} \to \mathbb{C} \\ \,\ \ \ \ \ z \to (|z|^2-2)\overline{z}$ Determine the points where f is differentiable, presenting the due expression $f'(z)$ . $u(x,y)=Re((x^2+y^2-2)(x-yi))=x^3+xy^2-2x; \\ v(x,y)=Im((x^2+y^2-2)(x-yi))=-yx^2-y^3+2y $ $u_x=3x^2+y^2-2; \,u_y=2xy \\
v_x= \ \ -2xy;\qquad  \ \ v_y=-x^2-3y^2+2$ The CR equality happens when: $$4(x^2+y^2)=4$$ And as all partial derivatives are continuous, f is differentiable at $\partial{D(0,1)}$ And now, for $z_{0} \in \partial{D(0,1)}$ and knowing $\textit{'a priori'}$ that the limit exists: $$ lim_{z \to z_0; z \in \partial{D(0,1)}} \frac{f(z)-f(z_0)}{z-z_0} = lim_{z \to z_0; z \in \partial{D(0,1)}}\frac{(|z|^2-2)\overline{z}+\overline{z_0}}{z-z_0}=lim_{z \to z_0; z \in \partial{D(0,1)}}-\frac{z-z_0}{z-z_0}=-1$$ So $f'(z_0)=-1$ for $z_0 \in \partial{D(0,1)}$ Is this correct? Any further suggestions?",['complex-analysis']
4884631,Does this series violate the integral test condition for convergence/divergence?,"My professor is asking me to determine if the following series is convergent or divergent using the integral test specifically. $$
\sum^{\infty}_{n=1}n^2e^{-n}
$$ However, this seems to violate the requirement that $f(x)$ should be decreasing over $[1,\infty)$ since $f(1)<f(2)$ . The rest of the function is decreasing over $(2,\infty)$ which makes me think that I'm missing some special trick. I can't tell if I'm missing something or if my professor just did this on purpose to make sure we check our conditions...","['calculus', 'sequences-and-series']"
4884635,An attempt for approximating the logarithm function $\ln(x)$: Could be extended for big numbers?,"An attempt for approximating the logarithm function $\ln(x)$ : Could be extended for big numbers? PS: Thanks everyone for your comments and interesting answers showing how currently the logarithm function is numerically calculated, but so far nobody is answering the question I am asking for, which is related to the formula \eqref{Eq. 1} : Is it correctly calculated?, Could a formula for the logarithm of large numbers be found with it? Here with ""big/large numbers"" I am meaning in the same sense of how the Stirling's approximation formula approximates the factorial function at large values. Intro__________ On a previous question I found that the following approximation could be used: $$\ln\left(1+e^x\right)\approx \frac{x}{1-e^{-\frac{x}{\ln(2)}}},\ (x\neq 0) \quad \Rightarrow \quad \dfrac{\ln\left(1+x^{\ln(2)}\right)}{\ln\left(x^{\ln(2)}\right)} \approx \frac{x}{x-1}$$ And later I noted that I could do the following: $$\dfrac{\ln\left(1+x^{\ln(2)}\right)}{\ln(2)} \approx \frac{x\ln\left(x\right)}{x-1}$$ by differentiating both sides: $$\frac{x^{\ln(2)-1}}{1+x^{\ln(2)}} \approx \frac{1}{x-1}-\frac{\ln(x)}{(x-1)^2}$$ From where I could isolate the logarithm as an approximation function: $$\ln(x) \approx f_1(x) = \frac{\left(x-1\right)\left(x+x^{\ln(2)}\right)}{x\left(1+x^{\ln(2)}\right)}$$ which end been a quite good approximation: sacrificing little accuracy it follows the logarithmic curve for longer than Taylor expansions or Padé approximants of similar order: as example the 2nd order Taylor series center at $x=1$ equals $T(x)=(x-1)-\frac12 (x-1)^2$ , and the Pade approximant of 2nd order in both numerator and denominator at $x=1$ is $P(x)=\frac{3(x^2-1)}{x(x+4)+1}$ are shown in the following figures 1 and 2 compared with $f_1(x)$ : It follows the graph quite good and way longer that the classic approximations just with some little crossovers, but as can be seen at the end, it still detaches from the logarithmic curve as $x$ increases. Looking at how the approximation was obtained, you can notice that further differentiation could be used to obtain other approximations for the logarithm function, since for every differentiation I will get the form: $$\mathbb{P}_1(x) \approx \mathbb{P}_2(x)+\ln(x)\cdot \mathbb{P}_3(x) \Rightarrow \ln(x) \approx \frac{\mathbb{P}_1(x)-\mathbb{P}_2(x)}{\mathbb{P}_3(x)}$$ with $\mathbb{P}_i(x)$ some polynomial-alike functions (with roots and fractions of combination of roots and polynomials). As example, the next step of differentiation leads to the approximation: $$\ln(x)\approx f_2(x) = \frac12 (x-1)^3 \left( \frac{x}{(x-1)^3}-\frac{1}{x(x-1)^3}+\frac{x^{\ln(2)-2}\left(\ln(2)-1-x^{\ln(2)}\right)}{\left(1+x^{\ln(2)}\right)^2}\right)$$ which can be seen in the following figure 3 , sacrificing accuracy at the beginning, looks like it is converging for larger values, so I wonder if this process could be indeed giving approximations for the logarithmic function. Main question_______________ Considering that the derivative of the logarithm is $\ln'(x) = \frac1x$ , that $\frac{d^n}{dx^n}\left(\frac{x}{x-1}\right) = \frac{n!}{(x-1)(1-x)^n}$ and $\frac{d^n}{dx^n}\left(\frac{1}{x}\right) = \frac{(-1)^{n}\ n!}{x^{n+1}}$ , and the product rule for higher derivatives $d^{(n)}(uv) =\sum\limits_{k=0}^n {n \choose k}d^{(n-k)}(u)\ d^{(k)}(v)$ , if I didn't messed up the nth-order approximation is given by: $$\ln(x) \approx f_n(x) = \dfrac{ \sum\limits_{k=0}^{n-1}{{n-1}\choose k} \dfrac{d^{k}}{dx^{k}}\left(\dfrac{1}{x}\right)\dfrac{d^{n-1-k}}{dx^{n-1-k}}\left(\dfrac{1}{1+x^{-\ln(2)}}\right) - \sum\limits_{k=1}^{n-1}{{n-1}\choose k} \dfrac{d^k}{dx^k}\left(\dfrac{1}{x}\right)\dfrac{d^{n-1-k}}{dx^{n-1-k}}\left(\dfrac{x}{x-1}\right)}{\dfrac{n!}{(x-1)(1-x)^n}} \label{Eq. 1} \tag{Eq. 1}$$ But here I got stuck: It is possible to use this for finding a formula for large $x$ values? I was aiming to form a fraction keeping only the higher order polynomials in $n$ and later replace it with $x$ , but I couldn't find a formula clearer than this one. Here assuming the formula is right, hope you can review it. I don't know if is possible to do factorization on the $\{n-1-k\}$ terms, as example. It is converging to the logarithm for large $x$ values as $n$ increase? I don't really know if this is true. Motivation____________ After receiving a comment criticizing this answer , I realized that approximating $\ln(x) \approx x(x^{\frac1x}-1)$ for large numbers don't going to be useful since looks like $x^{\frac1x}$ is even harder to calculate (even when it looks accurate at large values ). So assuming that $x^b$ with $b>0$ is not difficult in this sense ( I hope it is not, I am not really sure if make sense the use of $x^{\ln(2)}$ ), I was aiming to find some ration of polynomials $\ln(x) \approx f_n(x) = \dfrac{\mathbb{P}_1(x^n)}{\mathbb{P}_2(x^n)}$ and later see how behaves replacing $n \leftarrow x$ such $\ln(x) \approx f(x) = \dfrac{\mathbb{P}_1(x^x)}{\mathbb{P}_2(x^x)}$ . Added later Using Stirling's approximation I could make the following: $$\begin{array}{r c l} \ln(x+1) & = &\ln\left((x+1)!\right)- \ln(x!) \\
\overset{\text{Stirling's}}{\Rightarrow} \ln(x+1)\left(x+\frac12\right) & \approx & 1 + \ln(x)\left(x+\frac12\right) \\
\Rightarrow \ln(x+1)-\ln(x) = \ln\left(1+\frac{1}{x}\right) &\approx & \frac{2}{2x+1} \end{array}$$ But unfortunately I haven't found yet a better simple approximation than $f_1(x)$ , neither something useful for large $x$ values. 2nd later comments The formula \eqref{Eq. 1} was corrected since it was originally wrong. I still without found the formula at $n$ , but by mistake I recovered in Wolfra-Alpha other known expansion that works better than Taylor's: $$\ln(x) = \sum\limits_{k=1}^\infty \frac{1}{k}\left(\dfrac{x-1}{x}\right)^k$$ and I am afraid that the second sum of  \eqref{Eq. 1} lead to that canceling the logarithm on the approximation ""equality"", but I still missing with constants matching. Last update I give up, after trying much more I have should I found, no after complications because of $\dfrac{d^n}{dx^n} \ln(x)$ broke its form when $n=0$ compared with $n>0$ , that the approximation take the form: $$n=0:\qquad \dfrac{\ln(1+x^{\ln(2)})}{\ln(2)}\approx \ln(x)\dfrac{x}{x-1}$$ $$n=1:\qquad \dfrac{1}{x+x^{1-\ln(2)}}\approx \dfrac{1}{x-1}-\dfrac{\ln(x)}{(x-1)^2}\quad \Rightarrow f_1(x)$$ $$\begin{array}{r c l}
n>1: \ln(x) & \approx & \sum\limits_{k=1}^n \frac{1}{k}\left(\frac{x-1}{x}\right)^k +\frac{1}{n}\left(\frac{x-1}{x}\right)^n \frac{(x-1)}{(1+x^{\ln(2)})}\sum\limits_{q=0}^{n-1} \sum\limits_{k=0}^q \sum\limits_{j=0}^k \dfrac{(-1)^{j+k-q}{k \choose j}}{(1+x^{-\ln(2)})^k}{(k-j)\ln(2) \choose q} \\
\Rightarrow n\geq 1: \ln(x) & \approx  & \sum\limits_{k=1}^{n-1} \frac{1}{k}\left(\frac{x-1}{x}\right)^k + \frac{1}{n}\left(\frac{x-1}{x}\right)^n \left(\dfrac{x+x^{\ln(2)}}{1+x^{\ln(2)}}\right) + \frac{1}{n}\left(\frac{x-1}{x}\right)^n \frac{(x-1)}{(1+x^{\ln(2)})}\sum\limits_{q=1}^{n-1} \sum\limits_{k=0}^q \sum\limits_{j=0}^k \dfrac{(-1)^{j+k-q}{k \choose j}}{(1+x^{-\ln(2)})^k}{(k-j)\ln(2) \choose q} \end{array}$$ Unfortunately I wasn't able to reduce it more neither related it to $f_1(x)$ (which works amazing for small $n$ ), and also from the graph in Desmos looks like the approximation works good at low $n$ values for $x<1000$ but later it don't gain too much power, but I believe it is converging to the logarithm: the first part do approach the logarithm, and the awful second part, if I am not mistaken, should go to zero for large $n$ and $x$ since are proportional to $\frac{1}{n}$ and because the degree of the polynomials on each term's numerator and denominator matches (hope someone could prove it - I wasn't able to show it). Another interesting thing is that using the approximation: $$\ln(x) \approx \frac{(x-1)}{x^{\ln(2)}}\ln(1+x^{\ln(2)})$$ and introducing there the classic approximation $\sum_{k=1}^n \frac{1}{k}\left(\frac{x-1}{x}\right)^k$ you get something that arrives a bit faster than the original one: $$\ln(x) \approx \frac{(x-1)}{x\ln(2)}\sum\limits_{k=1}^n \frac{1}{k}\left(\dfrac{x^{\ln(2)}}{1+x^{\ln(2)}}\right)^k$$ after many answers Testing some answers I have noticed that many functions like $f_0(x) = x-1$ , $f_2(x) = \frac{x-1}{x}$ , $f_3(x)= \frac{x-1}{x}\frac{(x+x^2)}{(1+x^2)}$ , $f_1 = \frac{x-1}{x}\frac{(x+x^{\ln(2)})}{(1+x^{\ln(2)})}$ , $f_5(x) = \sum\limits_{k=1}^n \frac{1}{k}\left(\frac{x-1}{x}\right)^k$ , and others, fulfill that $\lim_{n\to\infty}nf_i(x^{\frac{1}{n}})=\ln(x)$ (you could check their convergence in Desmos ), I don't fully understand why.... I think is related to the property used in the numerical examples given on the answers which logarithms fulfill $f(x)=nf(x^{\frac{1}{n}})$ . So far I have been unable to find a good approximation for large numbers without falling into more troublesome terms like $x^{\frac{1}{x}}$ . But which I think is somehow interesting is that $f_1(x)$ shows to converge faster that any other attempts, so maybe something interesting is hidding there. As example $\theta(x) = xf_1(x^{\frac{1}{x}})$ is practically the logarithm function when $x>0.65$ ( in my humble opinion : the max difference is less than $0.0011$ ).","['approximation-theory', 'logarithms', 'real-analysis', 'functions', 'solution-verification']"
4884651,solution using symmetry to probability question involving 3 jurors,"Consider the following problem: Alice has decided to participate in a jury with three members, with the verdict decided by majority. To express her disinterest in the case, she decides to vote by flipping a fair coin. The other two members make the correct decision with probability $p \in (0, 1)$ . How does this arrangement compare to a judge who makes the correct decision with probability $p$ ? The probability the correct decision is made in the former scenario can be calculated as $\frac{p^2}{2} + \frac{p^2}{2} + 2\left(\frac{p(1-p)}{2}\right) = p$ , so the probabilities are the same. Because they are the same, I am wondering if there is a more elegant solution that argues by symmetry?","['recreational-mathematics', 'puzzle', 'problem-solving', 'probability']"
4884678,Fibonacci but with ratio of sum and difference of the the previous two terms,"I have discovered a pretty weird sequence $$a_0=1$$ $$a_1= 2$$ $$a_n=\frac{a_{n-1}+a_{n-2}}{a_{n-1}-a_{n-2}}$$ The first few terms are: $$1,\ 2,\ 3,\ 5,\,\ 4,\ -9,\ \frac{5}{13},\ -\frac{56}{61},\ \frac{423}{1033},...$$ The sequence is very unpredictable and looks very random. I wonder if there is a closed-form formula, perhaps analogous to Binet's formula, that calculates the $n$ -th element of this sequence. To simplify the problem, i tried splitting the sequence to its numerator and denominator $$a_n = \frac{A_n}{B_n}$$ Combining with the recursive equation yields, $$\frac{A_n}{B_n} = \frac{\frac{A_{n-1}}{B_{n-1}} +\frac{A_{n-2}}{B_{n-2}}}{\frac{A_{n-1}}{B_{n-1}} -\frac{A_{n-2}}{B_{n-2}} } = \frac{A_{n-1} B_{n-2}+A_{n-2}B_{n-1}}{A_{n-1} B_{n-2}-A_{n-2}B_{n-1}} $$ Then we set, $$A_n = A_{n-1}B_{n-2} + A_{n-2}B_{n-1}$$ $$B_n = A_{n-1}B_{n-2} - A_{n-2}B_{n-1}$$ $$A_0 = B_0 = B_1 = 1$$ $$A_1 = 2$$ While i fell short of my goal of finding a closed-form formula, i think this is a great start. Can anyone find a closed-form formula?","['number-theory', 'fibonacci-numbers']"
4884717,Why $ \sum_{p \leq N \ and \ p \ prime} \frac 1 p $ counts prime factors of $N$?,"I came across this sum $ \sum_{p \leq N \ and \ p \ prime} \frac 1 p $ in page 22 of Concrete Mathematics by Graham, Knuth and Patashnik. The authors denote that: Incidentally,  this  sum  gives  the  approximate  average  number  of  distinct  prime
factors  of  a  random  integer  near  N, since  about $1/p$ of  those  integers  are
divisible  by  p . I don't understand the latter part of this sentence at all. I tried a few examples and found it sometimes work (N=12 works while N=49 isn't). Then I searched this sum on google and this site and found nothing related. Can someone give me a more detailed explanation of why it can count prime factors?","['discrete-mathematics', 'sequences-and-series']"
4884722,"How do we know the space of distributions is ""big""?","Consider the set of compactly supported smooth functions $C^\infty_c(\mathbb{R})$ . The dual of this space, $(C^\infty_c(\mathbb{R}))'$ is often referred to as a very large space. Since every $f \in C^\infty_c(\mathbb{R})$ can be mapped to an element in the dual space by integration we know that $C^\infty_c(\mathbb{R}) \subset (C^\infty_c(\mathbb{R}))'$ and that this inclusion is strict since non-regular distributions are known to exist. But how do we know that the dual space is much larger than the original space? That is, how do we know the non-regular distributions are not rare? Is there a way to quantify how much bigger the dual space is compared to the original space?","['dual-spaces', 'functional-analysis', 'distribution-theory']"
4884734,"Generalizing the property of two parabolas tangent to a circle, each of which touches it at two points","About five years ago, I discovered a property of a parabola, a circle that touches it at two points, and two tangents to it that are parallel to the index of the parabola. In the previous picture $AC⊥BD$ After some trying, I was able to prove this with analytical geometry But my happiness was soon complete until I was able to generalize this property, as it was not required for two parallel lines to be parallel to the index of the parabola. I was also able to prove this with analytical geometry But after a while I noticed that I could consider two parallel lines to be a special case of a parabola when the distance between the focus and the guide approaches zero and they are located at infinity.
The property is actually valid for two parabolas Then, after a while, I found that this property was already known and was present in Arseniy Akuban’s book This was frustrating, as I had used the previous properties in many applications and theorems But after a while I was able to generalize this feature more: I discovered that these two perpendicular lines are nothing but the interior bisector and exterior bisector of the angle between the diagonals of the quadrilateral whose vertices are the four points of contact, as shown in the picture: Reaching this stage took almost a year of coming up with one generalization after anotherTheorems have been used along the way in many engineering constructions But I still do not know how to prove this last general case. It is really difficult. I would be grateful for any help in proving this, and I prefer engineering methods to analytical methods. Is the latest circular known?","['euclidean-geometry', 'conic-sections', 'geometry']"
4884746,Evaluating limit involving integrals,"I recently came across a challenging limit problem involving integrals of power functions during an examination, and I’m having trouble figuring out how to solve it. I would greatly appreciate any help or insights you can provide. The problem is to evaluate the following limits: $$
	\lim_{n\rightarrow +\infty} \frac{\int_0^{1/2}x^{nx}dx}{\int_{1/2}^1x^{nx}dx},\quad \lim_{n\rightarrow +\infty} \frac{\int_0^{1}\frac{x^{nx}}{1+x^2}dx}{\int_{0}^1x^{nx}dx}.
$$ I am aware that we might need to consider the function $f(x)=x^x=e^{x\ln x}$ , which attains its maximum value of 1 at 0 and 1, and its minimum value at $e^{-1}$ . However, I’m unsure how to utilize these facts to tackle the problem. Any hints, suggestions or explanations on how to approach these types of limit problems would be greatly appreciated. Thank you in advance for your assistance.","['integration', 'limits', 'calculus', 'real-analysis']"
4884748,Hyperbolic trigonometry NOT in Poincare disk,"There are a lot of hyperbolic trigonometric identities derived in Poincare disk model that resemble similar identities from the Euclidean geometry. For example, the analogue of the Pythagoras theorem is $\cosh(c) = \cosh(a)\cosh(b)$ , the analogue of the sine law is $\frac{\sin(A)}{\sinh(a)}$ = $\frac{\sin(B)}{\sinh(b)}=\frac{\sin(C)}{\sinh(c)}$ and so on. After reading the proofs of those identities it made me wonder whether they are specific for the Poincare disk model, or they can be arrived at in other models, e.g. Beltrami-Klein model. I suppose in other models those identities look differently, but I'm not sure. If so, how does Pythagoras theorem looks in Beltrami-Klein model?","['trigonometry', 'hyperbolic-geometry']"
4884772,Confusion about the definition of Radon Measure,"From wikipedia , Radon measure is defined as Radon measure is a measure on the $\sigma$ -algebra of Borel sets of a Hausdorff topological space $X$ that is finite on all compact sets, outer regular on all Borel sets , and inner regular on open sets . I am reading a proof on the $\sigma$ -compact, locally compact metric space, that a local finite Borel measure is a Radon measure. It seems to me the proof shows it is inner regular on all Borel sets , which is stronger than the definition of Radon measure. Am I correct? What is the correct definition of inner regularity? I think the correct statement is If $X$ is a locally compact, $\sigma$ -compact metric space and $\mu$ is a locally finite Borel measure on $X$ , then it is both inner and outer regular (or simply regular).",['measure-theory']
4884795,Are linearly ordered topological spaces well-based?,"A linearly-ordered topological space or LOTS is one whose topology admits a basis generated by open intervals of a total ordering of its points. A well-based space is one which admits a local basis of each point that is totally ordered by set inclusion. It appeared to me that the former implied the latter, and I attempted to prove it via the following: for any point $p$ in a LOTS take (possibly transfinite) sequences $a:\Gamma\rightarrow[-\infty,p)$ and $b:\Gamma\rightarrow(p,\infty]$ for some ordinal $\Gamma$ which are monotonic and surjective with $a_0=-\infty$ and $b_0=\infty$ . Then for any open interval $(c,d)$ containing $p$ there is an ordinal $\beta$ such that $c\le a_\beta<p<b_\beta\le d$ , so $\{(a_\alpha,b_\alpha)\mid\alpha\in\Gamma\}$ is a local basis of $p$ . However, I got a response that a LOTS can be not well-based if there exists a point with different cofinalities on the left and the right, giving the example of $\omega_1+1+\omega*$ with the order topology (where $+$ denotes order concatenation and $\omega*$ the reverse order of $\omega$ ). This doesn't jibe with my understanding though, since there should exist exist monotonic surjections from $\omega_1$ to $\omega$ to make my argument work. Is this response wrong, or is there some shortcoming of my argument?","['order-theory', 'general-topology']"
4884822,Almost sure convergence in terms of sets,"Let $(\Omega,\mathcal{F},p)$ be a probability space, and let $X_n$ be a sequence of random variables.
Suppose that for a random variable $X$ we have that for all events $F\in\mathcal{F}$ , $$
\lim_{n\to\infty}\int_F X_n \, dp = \int_F X \, dp .
$$ Can we conclude that $X_n\to X$ almost surely?","['convergence-divergence', 'probability-theory', 'random-variables']"
4884829,Constructive proof of a statement about a property of a Lebesgue-null set,"I was studying some measure theory and upon searching for additional exercises on the internet I came upon one that said: Given $H$ a Borel set subset of the real numbers, prove that if $\lambda(H)=0$ (its Lebesgue measure) then there exists a number $\alpha$ such that $\alpha + H$ , the set of all the reals of the form $\alpha + h$ where $h \in H$ , is a subset of the irrational numbers. I had no problems in proving this statement, but the proof that I came up with is by contradiction and thus does not find the wanted number (which is not even unique, since it's provable that the set of all such real numbers is a Borel set with measure $+\infty$ ). My question is, is it possible to find a constructive proof of this statement? Or maybe is it possible to find such a number for a special non trivial (like one made only of irrational or rational numbers) null set, for example the Cantor set?","['measure-theory', 'analysis']"
4884835,"$Ham(M, \omega)$ acts transitively on $(M,\omega)$","Let $M$ be a  compact and connected smooth manifold with a symplectic form $\omega$ . $Ham(M, \omega)$ denotes the space of hamiltonian symplectomorphisms of $(M,\omega)$ . I have the following statement in my lecture notes: Using Darboux’s theorem one can show that the action of $Ham(M, \omega)$ on M is transitive, that is: for any pair of points $p, q \in M$ , there exists $\Phi \in Ham(M, \omega)$ such that $\Phi(p) = q$ . The idea  is that Darboux's theorem to go from local to global, i.e. to show that points that are close to each other in the symplectic manifold can be mapped to each other via a Hamiltonian diffeomorphism. ...which I unsuccesfully tried to prove. How is it done? I know that a symplectomorphism of $(M, \omega)$ is a diffeomorphism $\Phi : M \to M$ such that $\Phi ^∗\omega = \omega$ . $\Phi$ is Hamiltonian if there exists a Hamiltonian isotopy $\phi_t$ such that $\Phi=\phi_1$ . And that $ Ham(M, \omega)$ is a normal subgroup of $Symp(M, \omega)$ (the space of symplectomorphisms ), if that is useful. I am not sure what ""points close to each other"" means here and how to use to arrive to the thesis Following the suggestion: Let p, q be in M then By Darboux theorem there are open charts $(U_p,\phi)$ and $(Uq,\psi)$ with $\phi$ and $\psi$ symplectomorphisms from $\Bbb R^{2n}$ But what do I do with this?","['differential-forms', 'symplectic-geometry', 'smooth-manifolds', 'differential-geometry']"
4884922,"One of the numbers $\zeta(5), \zeta(7), \zeta(9), \zeta(11)$ is irrational","I am reading an interesting paper One of the numbers ζ(5), ζ(7), ζ(9), ζ(11) is irrational by Zudilin. We fix odd numbers $q$ and $r$ , $q\geq r+4$ and a tuple $\eta_0,\eta_1,...,\eta_q$ of positive integer parameters satisfying the conditions $\eta_1\leq \eta_2\leq...\leq \eta_q<\eta_0/2$ and $$ \eta_1+\eta_2+...+\eta_q\leq \eta_0\left(\frac{q-r}{2}\right)\tag{1}$$ Define $$F_n:=\frac{1}{(r-1)!}\sum_{t=0}^\infty R_n^{(r-1)}(t)\tag{2}$$ and note that $R_n(t)=O(t^{-2})$ . We put $m_j=\max\{\eta_r,\eta_0-2\eta_{r+1},\eta_0-\eta_1-\eta_{r+j}\}$ for $j=1,2,...,q-r$ and define the integer $$\Phi_n:=\prod_{\sqrt{\eta_0 n}<p\leq m_{q-r}n}p^{\varphi(n/p)}$$ where only primes enter the product and $$\varphi(x)=\min_{0\leq y<1}\left(\sum_{j=1}^{r}([y]+[\eta_0x-y]-[y-\eta_j x]-[(\eta_0-\eta_j)x-y]-2[\eta_j x])+\sum_{j=r+1}^{q}([(\eta_0-2\eta_j)x]-[y-\eta_j x]-[(\eta_0-\eta_j)x-y])\right)$$ where [.] denotes the ceiling function. Let $D_N$ denote the lcm of $1,2,...,N$ . Lemma $1$ : ( $2$ ) defines a linear form of $\zeta(r+2),\zeta(r+4),...,\zeta(q-2)$ with rational coefficients; moreover, $$ D_{m_1n}^r D_{m_2n... D_{m_{q-r}n}}.\Phi_n^{-1}.F_n\in\mathbb{Z}+\mathbb{Z}\zeta(r+2)+\mathbb{Z}\zeta(r+4)+...+\mathbb{Z}\zeta(q-2) \tag{3}$$ By Prime Number Theorem, $$\lim_{n\to\infty}\frac{\log D_{m_j n}}{n}=m_j,\ \ \ j=1,...,q-r.$$ We introduce the auxiliary function $$f_0(\tau)=r\eta_0\log(\eta_0-\tau)+\sum_{j=1}^{q} (\eta_j\log(\tau-\eta_j)-(\eta_0-\eta_j)\log(\tau-\eta_0+\eta_j)) -2\sum_{j=1}^r \eta_j\log \eta_j+\sum_{j=r+1}^q (\eta_0-2\eta_j)\log(\eta_0-2\eta_j)$$ defined in the $\tau$ -plane with the cuts $(-\infty,\eta_0-\eta_1]$ and $[\eta_0,+\infty)$ Lemma $2$ : Let $r=3$ and $\tau_0$ be a zero of the polynomial $$(\tau-\eta_0)^r(\tau-\eta_1)...(\tau-\eta_q)-\tau^r(\tau-\eta_0+\eta_1)...(\tau-\eta_0+\eta_q) $$ with Im $\tau_0>0$ and the maximum possible value of Re $\tau_0$ . Assume Re $\tau_0<\eta_0$ and Im $f_0(\tau_0)\notin \pi \mathbb{Z}.$ Then $$\overline{\lim}_{n \to \infty} \frac{\log |F_n|}{n}=Re f_0(\tau_0)$$ If the sequence of linear forms on the left side of ( $3$ ) assumes non-zero arbitrarily small values as $n$ increases, then in the case $r=3$ there are irrational numbers among $$ \zeta(5),\zeta(7),...,\zeta(q-4),\zeta(q-2) \tag{4}$$ Therefore the following holds: Lemma $3$ : Suppose that $r=3$ and in the above notation $C_0=-Re f_0(\tau_0)$ , $$C_1=rm_1+m_2+...+m_{q-r}-\left(\int_0^1\varphi(x)\frac{\Gamma'(x)}{\Gamma(x)}dx-\int_0^{1/m_{q-r}}\varphi(x)\frac{dx}{x^2}\right)$$ If $C_0>C_1$ , then at least of the numbers ( $4$ ) is irrational.
The line below Lemma $3$ reads: we put, $r=3,q=13$ , $$\eta_0=91,\eta_1=\eta_2=\eta_3=27,\eta_4=29,\eta_5=30,\eta_6=31,...,\eta_{12}=37,\eta_{13}=38.$$ Then $$C_0=227.58019641...,C_1=226.24944266...$$ Question: Is the above result true only for $r=3$ or for other odd values of $r>3$ also? For example can we take $r=5,q=13$ and if we show that $C_0>C_1$ and with suitable choice of $\eta's$ , can we say at least one of the four numbers $$ \zeta(7),\zeta(9),\zeta(11),\zeta(13)$$ is irrational? Rephrasing my question, Is Proposition $5$ in Arithmetic of linear forms involving odd zeta values true for $r=5$ and $q=13$ ? Any help would be highly appreciated. Thank you!","['number-theory', 'computational-number-theory', 'analytic-number-theory', 'computational-mathematics', 'riemann-zeta']"
4884934,How to prove the two answers to an integral are equivalent,"I'm trying to do the integral: $$\int{\frac{1}{\sqrt{e^{-2x}-1}}}dx$$ So I try two ways to do it, the first method I used is to multiply $e^x$ on both sides first. $$\int{\frac{1}{\sqrt{e^{-2x}-1}}}dx$$ $$=\int{\frac{e^x}{\sqrt{1-e^{2x}}}}dx$$ $$=\int{\frac{\sin{\theta}}{\sqrt{1-\sin^2{\theta}}}\times\frac{\cos{\theta}}{\sin{\theta}}}d\theta$$ $$=\theta+C$$ $$=\arcsin{(e^x)}+C$$ The second method is to do substitution directly. $$\int{\frac{1}{\sqrt{e^{-2x}-1}}}dx$$ $$=-\int{\frac{1}{u}\times\frac{u}{u^2+1}}du$$ $$=-\arctan{u}+C$$ $$=-\arctan{\sqrt{e^{-2x}-1}}+C$$ $\text{After input them into desmos, I found out that the distances betweens the graphs is constant}\frac{\pi}{2}$ $\text{So I want to ask that, can you prove that,} $ $$\arcsin{(e^x)}-(-\arctan{\sqrt{e^{-2x}-1}})\text{ is a constant?}$$","['integration', 'inverse-trigonometric-functions', 'calculus', 'indefinite-integrals', 'trigonometry']"
4884967,limit of convergent series in point wise convergent series of continuous functions,"Hi I was looking for $f_n :  [0,1] \to \mathbb{R}$ such that every $f_n$ is continuous and $f_n$ converges pointwise to a function $f : [0,1] \to \mathbb{R}$ that is continuous as well. Then I wanted to find a series with $x_n \to a \in [0,1] $ such that : $\lim_{n \to \infty} f_n(x_n) \neq f(a)$ . I was wondering if $f_n : [0,1] \to \mathbb{R} : x \to 
  2nx \space if \space 0 \leq x < \frac{1}{2n} , 2-2nx \space if \space \frac{1}{2n} \leq x \leq \frac{1}{n},  \space else \space 0
 $ ( $f_n \to f =0$ ) and $x_n = \frac{1}{2n}$ ( $x_n \to 0$ ) would work. Because (correct me if I am wrong) $f_n(x_n) = 1$ and $f(x) = 0$ . Are there any simpler examples is my example correct?","['real-analysis', 'continuity', 'functions', 'sequences-and-series', 'pointwise-convergence']"
4885022,Is there a simple geometric proof of why two simple harmonic oscillators draw a circle?,"We all know that a circle can be drawn with the trigonometric functions $x=\cos(t),  y=\sin(t)$ . If we define the sine and cosine functions in terms of triangles (like we do in high school), then this is quite obvious. But then later on in our education, we learn that the solution to a simple harmonic oscillator is the sine function. A weight on an undamped spring goes back and forth following a sine wave over time, and that this is the intuition behind a lot of wave motion (like sound waves). However, it's not generally taught why the sine wave solution to simple harmonic motion is the same function as the sine wave as defined by triangles or circles. Or in other words, when you take two harmonic oscillators and plot their outputs as $x=\cos(t),  y=\sin(t)$ , why should they make a perfect circle? More specifically: it's intuitive enough that they must form a shape that makes a full loop of some kind. But why does it happen to be a perfect circle , as opposed to an alternative shape like a larger ""squircle"", or something smaller similar to a rhombus with rounded corners? After doing a bit of research, the best answer I've found is that if you independently derive the Taylor series for each of: $\sin(t)$ as defined by simple harmonic oscillator (as the negative of its second derivative) $\sin(x)$ as defined trigonometrically (as the opposite side of a right triangle over the hypoteneuse) Then you discover they're the same Taylor series and therefore the same function and therefore simple harmonic oscillators can be used to draw a circle. But I'm wondering two things. First, is there some kind of more intuitive, direct, ideally geometric explanation? Resorting to Taylor series certainly works as a formal proof, but it doesn't build any kind of intuitive understanding. And for something so foundational to math and engineering as circles and sine waves, it seems like there should be a more obvious link. And second, if there is no such intuitive explanation and we're forced to resort to the Taylor series equivalence, is there an easy yet rigorous way to derive that that doesn't involve mixing up the two origins/definitions of the sine function to do so? E.g. every example I can find of deriving the Taylor series expansion of a simple harmonic oscillator involves using a trigonometric identity at some point, e.g. to show that the derivative of sine is cosine. But this defeats the whole purpose because it assumes what I'm trying to prove -- that the simple harmonic oscillator sine wave is the same as the trigonometric sine wave in the first place.","['geometry', 'calculus', 'taylor-expansion', 'intuition', 'trigonometry']"
4885050,Prove that a critical graph $G$ satisfies $\chi(G) \leq \delta + 1$,"Let $G = (V, E)$ a critical graph; i.e. a graph s.t. for any subgraph $H \subseteq G$ we have $\chi(H) < \chi(G)$ . I was requested to prove that $\chi(G) \leq \delta + 1$ , where $\delta$ is the degree of the vertex (or vertices) of lesser degree in $G$ . I presume that I need to relate a sub-graph $H$ that involves the vertex (or vertices) of $G$ whose degree is $\delta$ and somehow prove that the chromatic number of this subgraph is $\delta$ . Then the property follows from the fact that $G$ is critical. However, I was unable to construct such subgraph. Am I taking the wrong approach? Any hints/suggestions are appreciated.","['graph-theory', 'coloring', 'discrete-mathematics']"
4885101,Behavior of function $\sum_{j = n}^\infty \frac{\sin^2((2j-1) \pi x)}{(2j-1)^2}$,"For a positive integer $n$ , define the function $$
F_n(x) = n^2 \sum_{j = n}^\infty \frac{\sin^2((2j-1) \pi x)}{(2j-1)^2}.
$$ I am trying to understand the behavior of $F_n(x)$ in the following sense. For a positive exponent $\alpha$ , I would like to
compute the limit $$
L_\alpha = \lim_{n \to \infty} F_n(1/n^\alpha). 
$$ Based on plotting in Mathematica, it appears that $L_\alpha$ has the following behavior: it seems to be $0$ for $\alpha > 2$ , infinite for $\alpha < 2$ and some finite number for $\alpha = 2$ . I tried to simplify the summation but I am having difficulty passing to the limit.","['trigonometric-series', 'limits', 'inequality', 'asymptotics']"
4885118,Sum of angles in a $1$-by-$3$ rectangle [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 months ago . Improve this question This problem was in a competition for a job. It seems simple BUT the challenge is you cannot use trigonometry. Let there be 3 squares with side length of $\ell$ arranged in such a way that it forms a rectangle with length $3\ell$ and width $\ell$ . So $ABCD$ , $CEHD$ , and $EFGH$ are squares. (This is notation because it's easier to tell you this way.) If $\alpha = m(\angle AED)$ and $\beta = m(\angle AFD)$ , then what is $\alpha + \beta$ ? I tried to use angles of quadrilaterals but so far I didn't find anything useful. Have fun solving it! Non-OP edit : added diagram to confirm construct or clarify","['quadrilateral', 'angle', 'geometry']"
4885156,Image of a two-parameter function over a set,"I have the following problem: Let $B$ be the set $B=\{(x_1 , x_2) \in \mathbb{R}^2\mid x_1^2 + x_2^2 \leq 1\text{ and }x_1 \geq 0\}$ . A function $f:B \rightarrow \mathbb{R}$ is given by $$
f(x_1 ,x_2 )=x_1^2 x_2^2 +3x_1 x_2 +x_2 -4.$$ State the image of $f$ . To find the image, I want to do an extrema investigation. First, I find all the stationary points. There is one at $\left(-\frac13, 0\right)$ , but based on the Hessian matrix, I conclude it is a saddle point. Next, I want to do a boundary investigation. I parametrize the circular part of the boundary of the set as $$s(u) = (\cos u,\sin u),\quad u \in\left[-\frac\pi2,\frac\pi2\right].$$ However, this is where my problem starts. I make a composite function $f \circ s$ - this describes the values of $f$ along the circular boundary. I then differentiate this composite function and get $$- 2 \sin^3u\,\cos u- 3 \sin^2u+ 2 \sin u\,\cos^3u+ 3 \cos^2u+ \cos u.$$ Now, I want to set it equal to $0$ and solve, since the stationary points on the composite function constitute possible extrema of $f$ on $B$ . However, I am not able to solve this. It there any trick or another way to find these extrema?","['convex-optimization', 'multivariable-calculus', 'calculus', 'linear-algebra']"
4885163,"Find the radius of the circle tangent to $x^2, \sqrt{x}, x=2$","I'm taking up a post that was closed for lack of context because I'm very interested in it : Let $(a,b)$ be the center of this circle. It seems intuitive that $b=a$ , but I have not been able to prove it formally, although I know that two reciprocal functions are symmetric with respect to the first bisector $y=x$ . Then let $(X,X^2)$ the point of tangency with $y=x^2$ . I think we're going to use the formula for the distance from $(a,a)$ to the line $y-X^2=2X(x-X)$ . We have obviously the relation $r=2-a$ . The normal to $(X,X^2) $ passes through $(a,a)$ I'm not sure if my notations are the best to elegantly solve the exercise. I hope you will share my enthusiasm for this lovely exercise that I have just discovered thanks to MSE.","['analytic-geometry', 'functions', 'geometry']"
4885169,"Expected number of ""survivors"" of independent Brownian motions on the line","[Edited since originally posted]
I'm trying to find a closed form for the expected number of ""survivors"" of $n$ independent standard Brownian motions in $\mathbb{R}$ up to time $t > 0$ where $t$ is scaled as $\lambda/n^2$ for some positive $\lambda$ ?. By ""survivor"", I mean a Brownian motion $W_s^x$ that does not intersect any other Brownian motion up to time $t$ . I'm well aware of the Karlin-McGregor formula which gives the asymptotics of non-intersecting Brownian motions as $t\to\infty$ . I have been looking mostly at the literature for coalescing Brownian motions, but the asymptotics are not very precise. Any suggestions? If we considered three Brownian motions in $d=1$ , it's actually possible to write an explicit formula for the first hitting time. Let $\tau = \inf\left\{t\geq 0: W_t^{x_2} - W_t^{x_1} = 0 \text{ or } W_t^{x_3}-W_t^{x_2} = 0\right\}$ be the first time one of the two increment processes hit $0$ , ( $x^3 > x^2 > x^1$ ), which becomes a sort of hitting time for a cone. From a paper of O'Connell ""Collision times and exit times from cones"", the tail of this hitting time has the distribution $$P(\tau \leq t) = 2(1-\Phi(\frac{x^{2}-x^{1}}{\sqrt{2t}})) + 2(1-\Phi(\frac{x^{3}-x^{2}}{\sqrt{2t}})) - 2(1-\Phi(\frac{x^{3}-x^{1}}{\sqrt{2t}}))$$ which gives the asymptotics $$P(\tau \geq t) \sim \frac{(x^{3}-x^{2})(x^{2}-x^{1})^2+(x^{3}-x^{2})^2(x^{2}-x^{1})}{\sqrt{4\pi}}t^{-3/2}(1+o_t(1)).$$ That's cool, but I'm trying to deal with multiple Brownian motions at the same time, where neighbors from further away can intersect in finite time.","['stochastic-analysis', 'brownian-motion', 'probability']"
4885201,Extenstion of Caputo's fractional derivative to distribution.,"Let us start with the definitions that $\frac{d}{dx}\theta(x)=\delta(x)$ where $\theta(x)$ and $\delta(x)$ are Heaviside and delta functions. Now, with the definition: $^c D^\alpha f(t)=\frac{1}{\Gamma(n-\alpha)}\int_0^t\frac{\frac{d^n}{dx^n}f(x)}{(t-x)^{\alpha-n+1}}\mathrm d x$ I want to understand what is the half-derivative of the $\theta(x)$ such that applying another half derivative on the result gives the delta function. For a simple function like say $f(t)=sin(x)$ one can look at the Taylor expansion and for each power of x, the formula gives the half derivative to be $$^c D^\frac12 x^p=\frac{p x^{p-\frac{1}{2}} \Gamma (p)}{\Gamma \left(p+\frac{1}{2}\right)}$$ such that application of another half derivative gives the same result as the full derivative i.e. $px^{p-1}$ . However, for the Heaviside theta function, the first application gives $$^c D^\frac12 \theta(x)=\frac{2 \theta (x)-1}{\sqrt{\pi } \sqrt{x}}$$ and now I am usure how to show that the second application gives the Delta function.","['calculus', 'derivatives', 'fractional-calculus', 'real-analysis']"
4885212,defective renewal equation,"I am reading this paper of Lin and Willmot. I dont understand how they come up with formula $2.7$ and why $\tilde{\phi}(s)=\frac{\tilde{H}(s)}{1+\beta-\tilde{g}(s)}$ . Can someone help me? So i want to show that $$\phi(u)=\frac{1}{1+\beta}\int_0^u\phi(u-x)dG(x)+\frac{1}{1+\beta}H(u)$$ is the same as $$\phi(u)=\frac{1}{\beta}\int_0^uH(u-x)dK(x)+\frac{1}{1+\beta}H(u)$$ where $\beta>0, G(x)=1-\overline{G}(x)$ is a distribution function with $G(0)=0$ and $H(u)$ continuous for $u\geq 0$ and $K(u)=1-\overline{K}(u)$ with $\overline{K}(u)=\sum_{n=1}^\infty\frac{\beta}{1+\beta}\left(\frac{1}{1+\beta}\right)^n\overline{G}^{*n}(u)$ . $f^{*n}(x)$ is the $n$ fold convolution of $f$ . \
Can someone explain me why they are the same?","['integration', 'convolution', 'probability-theory', 'laplace-transform']"
4885216,Are quasicomponents connected in a compact non-Hausdorff space?,"Are quasicomponents connected in a compact space? Background: Quasicomponents are connected in a compact Hausdorff space . A non-compact locally compact Hausdorff space may not have connected quasicomponents . It can be shown that a subset is an open connected component if and only if it is an open quasicomponent. (*) In a finitely generated space (aka Alexandrov discrete space), every intersection of open subsets is open. Hence every quasicomponent is open as an intersection of clopen subsets. By (*), the quasicomponents are connected. More generally, a space with open components (a sumconnected space) has connected quasicomponents by (*). This includes locally connected spaces, which include finitely generated spaces. So this question is about whether the first result can be generalized to non-Hausdorff spaces.",['general-topology']
4885307,Can I analytically integrate a high dimensional Normal distribution? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 months ago . Improve this question Let's say $X \sim \mathcal{N}(\mu, \Sigma)$ , where $\mu \in \mathbb{R}^{3}$ . If I have two half spaces: $H_{1} = \{x: a^{T}x \geq 0\}, H_{2} = \{x: b^{T}x \geq 0\}$ , is there a way I can compute the total probability mass in the area $H_{1} \cap H_{2}$ ?","['integration', 'calculus', 'statistics', 'analysis']"
4885308,convolution of characteristic functions,"Suppose $A$ and $B$ are measurable subsets of $\mathbb{R}$ of finite  positive measure. Show that the convolution $\chi_A*\chi_B$ is continuous and not identically $0$. Use this to prove that $A+B$ contains a segment. We have $$(\chi_A*\chi_B)(x)=\int_{-\infty}^{\infty} \chi_A(x-y)\chi_B(y)~dm(y)$$ For $y\notin B$ we have $\chi_B(y)=0$. So, $$(\chi_A*\chi_B)(x)=\int_B\chi_A(x-y)~dm(y)$$ For $x-y\notin A$ we have $\chi_A(x-y)=0$ $x-y\in A\Rightarrow y\in x-A$ so, we have $y\in B\cap (x-A)$. So, $$(\chi_A*\chi_B)(x)=\int_{B\cap(x-A)}~dm(y)=m(B\cap(x-A))$$ So, $$(\chi_A*\chi_B)(x)=m(B\cap(x-A))$$ EDIT 1 : Consider $x_n\rightarrow x $ ( converging sequence) then i see that $$\cdots (B\cap(x_n-A))\supseteq (B\cap(x_{n+1}-A))\supseteq(B\cap(x_{n+2}-A))\supseteq\cdots (B\cap(x-A))$$ So, $m(B\cap (x_n-A))\rightarrow m(B\cap (x-A))$ (EDIT : i have said this before but then some one said that convergence may not imply that containment but now i tried and i strongly feel it holds if it converges) I guess this would be sufficient to say that convolution is continuous.... $\epsilon-\delta$ way of proving continuity.. Given $\epsilon >0$ i have to choose $\delta>0$ such that $|x-y|<\delta$ implies $$\big|(\chi_A*\chi_B)(x)-(\chi_A*\chi_B)(y)\big|=\big|m(B\cap(x-A))-m(B\cap(y-A))\big|<\epsilon$$ I know $m(A)-m(B)=m(A\cap B^c)$ if $B\subset A$. I am not so sure if that holds in general or if it holds at least in this case... Suppose every thing is super perfect then i would get $$\big|m(B\cap(x-A))-m(B\cap(y-A))\big|=\big|m(B\cap(x-A))\cap (B\cap(y-A))^c\big|=\big| m(B\cap (x-A)\cap (y-A)^c)\big|$$ I do not know where to go from here.. Even if i prove this is continuous i am not very sure how to use this and prove that $A+B$ contains a segment... Please suggest some path...","['measure-theory', 'lebesgue-measure', 'convolution']"
4885359,A group generated by three rational points has actually two generators?,"Consider three points on the plane: $(1,0), (0,1) $ and $(\frac{p}{q},\frac{m}{n})$ , where $p,q,m,n\in\mathbb Z, q,n\neq 0,\gcd(p,q)=\gcd(m,n)=1.$ The group generated by $(1,0), (0,1) $ and $(\frac{p}{q},\frac{m}{n})$ with respect to addition can be denoted as $$\Gamma=\{k_1(1,0)+k_2(0,1)+k_3(\frac{p}{q},\frac{m}{n})\mid k_1,k_2,k_3\in \mathbb Z\}\\=
\{(k_1+k_3\frac{p}{q},k_2+k_3\frac{m}{n})\mid k_1,k_2,k_3\in \mathbb Z\}.$$ After trying some examples, I feel that $\Gamma$ can actually be generated by its two elements(e.g., when $(\frac{p}{q},\frac{m}{n})=(\frac{2}{5},\frac{4}{5})$ , $\Gamma=<(\frac{1}{5},\frac{2}{5}),(\frac{2}{5},-\frac{1}{5})>$ ), but the general proof may require some knowledge of number theory(?) which is my weakness. I was wondering if my guess is correct and if anybody would be willing to share his or her ideas about this question. Thanks in advance!!","['number-theory', 'abstract-algebra']"
4885394,"How to evaluate $\int_{-\infty}^\infty \sin^3(x)/x^3 dx$ without finding an analytic continuation, still using complex analysis.","There is a similar post regarding the integral $\int_{-\infty}^\infty \sin^3(x)/x^3 dx$ on Stack. The reason I have some trouble with this post is because it gives an analytic continuation of the function $\sin^3(x)/x^3=\left(\frac{e^{iz}-e^{-iz}}{2iz}\right)^3$ , namely $h(z)=-\frac{e^{3iz}-3e^{iz}}{4z^3}-\frac{1}{2z^3}$ . Firstly, I'd like to know how such an analytic continuation is obtained. Regarding the sinc function, I know that sine has a zero of order 1, and the denominator also does, so at $z=0$ , there is a removable singularity and thus an analytic continuation. I also know that $\lim_{z\to 0}\text{sinc}(z)=1$ (meaning 1 is an analytic continuation to $\mathbb{C}$ ?) Regarding the actual integral itself, I have considered the usual indented semicircle, of radius $R$ , with an $\epsilon$ semicircle about the essential singularity 0. Without using that analytic continuation $h(z)$ , this gives, letting $f(z)=\frac{e^{3iz}-3e^{iz}+3e^{-iz}-e^{-3iz}}{z^3}$ $$0=\frac{i}{8}\left[\int_{[-R,-\epsilon]\cup [\epsilon, R]}  f(z)dz+\oint_{\gamma_R}f(z)dz+\oint_{\gamma_\epsilon}f(z)dz\right]$$ where $\gamma_R$ is the contour of the large semicircle of radius $R$ , and $\gamma_\epsilon$ is the contour around the singularity $0$ of radius $\epsilon$ . Using Jordan's Lemma, I have managed to show that the integral over $\gamma_R$ goes to 0 as $R\to\infty$ . The trouble comes in with the integral over $\gamma_\epsilon$ as $\epsilon\to 0$ . I have come across a lemma here on Stack that says if $f$ is some meromorphic function (which, by definiton means that if $z_0$ is an isolated singularity of $f$ there exists some $r>0$ such that $1/f$ extends to a holomorphic function on $B_r(z_0)$ ) that has a simple pole at a point $z_0$ , then $\lim_{\epsilon\to z_0}\oint_{\gamma_\epsilon} f(z)d{z} = i(\gamma_\epsilon(b)-\gamma_\epsilon(b))\text{Res}(z_0,f(z))$ . This is in fact the reason I asked about the analytic continuation; since the singularity of sinc is essential, this lemma does not apply unless we extend it to some other function that has a pole, at least as I understand it. I also know the answer should be $3\pi/4$ . If anyone could help, I would appreciate it. The link to the post is: $ \int_{-\infty}^{\infty} \frac{\sin^3 x}{x^3} \, dx$ using contour integration.","['integration', 'analytic-continuation', 'real-analysis', 'complex-analysis', 'contour-integration']"
4885435,Is there a way to study the relationship between the category of finite groups and their conjugacy classes categorically?,"I asked this question on MO here From The influence of conjugacy class sizes on the
structure of finite groups . My question is as follows: Is there a way to study the relationship between the category of finite groups and their conjugacy classes categorically? Could you please provide references for this? Clarification of the question: we know the impact of conjugasy classes in classification the finite groups but however there are open questions dose not have answers yet so my main gaol is to find a way to connect category of finit groups and conjugacy classes categorically to solve such questions. I found The category of groups and conjugacy classes of homomorphism Is it relevat to my question from algebraic topology Perspective?","['group-theory', 'category-theory', 'reference-request']"
4885436,What is the exact form of the matrix algebra generated by a Jordan matrix?,"In a paper it is claimed that the matrix algebra generated by a matrix $$J=\oplus_i  J_{\lambda_i,m_i}$$ in Jordan normal form ""is the algebra of block diagonal matrices with the same block partition, where each block contains an arbitrary upper triangular Toeplitz matrix "". I take issue with the arbitrary Toeplitz blocks - an explicit calculation for any polynomial $$p(J)=\sum_k p_k J^k=p\left(\bigoplus_i J_{\lambda_i,m_i}\right)=\bigoplus_i p\left(J_{\lambda_i,m_i}\right) \ ,$$ and thus any element of the generated matrix algebra yields $$\sum\limits_k\begin{bmatrix}p_k\lambda^k&p_kk\lambda^{k-1}&p_k(k^2-k)\lambda^{k-2}&\ddots&\ddots &\ddots&\ddots\\0&p_k\lambda^k&p_kk\lambda^{k-1}&\ddots&\ddots& p_kk!\lambda^{k-l}/(l!(k-l)!)&\ddots\\0&0&p_k\lambda^k&\ddots&\ddots&\ddots&\ddots\\0&0&0&\ddots&\ddots&\ddots&\ddots\\0&0&0&0&p_k\lambda^k&p_kk\lambda^{k-1}&p_k(k^2-k)\lambda^{k-2}\\0&0&0&0&0&p_k\lambda^k&p_kk\lambda^{k-1}\\0&0&0&0&0&0&p_k\lambda^k\end{bmatrix}$$ for the Toeplitz blocks. But the same polynomial has to be applied to all blocks and so how can the full matrix be composed of arbitrary upper triangular matrix blocks ? For example, I fail to see how one would obtain a single diagonal block with all other blocks being trivial zero Toeplitz submatrices. What am I missing here?","['matrices', 'jordan-normal-form', 'linear-algebra', 'linear-transformations']"
4885475,zero locus of irreducible polynomial,"I am learning Riemann surfaces and one of the books I use is Algebraic curves and Riemann surfaces by Rick Miranda. He mentions if $f(z,w)\in \mathbb{C}[z,w]$ is irreducible the zero locus is connected. He doesn't prove it. Do you guys have tips how to do this or some literature? Kind regards","['curves', 'algebraic-geometry', 'riemann-surfaces']"
4885515,"Proving function is differentiable at $(0,0)$ using total derivative definition","I am looking at the function: $$\frac{x\sin(xy)}{\sqrt{x^2+y^2}}$$ at $(0,0)$ , where the function at $(0,0)$ is defined to be $0$ . I understand that I need to check that: $$\lim_{{h \to 0}} \frac{{f(\mathbf{a} + \mathbf{h}) - f(\mathbf{a}) - A\mathbf{h}}}{{\|\mathbf{h}\|}} = \lim_{{h \to 0}} \frac{{f(\mathbf{h})}}{{\|\mathbf{h}\|}} = 0 \ .$$ Since $D_1f$ and $D_2f$ are equal to zero, I believe that A is the matrix $(0, 0)$ . I've tried going through different paths to see if I get a different limit when doing: $$\lim_{{h \to 0}} \frac{{f(\mathbf{h})}}{{\|\mathbf{h}\|}} \ ,$$ and all of them give me $0$ . Therefore, I believe that function is differentiable at $(0,0)$ . But how can I prove it more soundly? Thank you!","['multivariable-calculus', 'derivatives']"
4885524,Finding the first term of a Goodstein sequence whose expression has maximum exponent,"Let $n$ be a natural number. When constructing the Goodstein sequence $(n)_{k}$ , we start with $(n)_{1}=n$ written in complete base $2$ , and we proceed recursively. That is, if $(n)_{k}$ has been defined and it is not $0$ , $(n)_{k+1}$ is obtained from $(n)_{k}$ by bumping the base by $1$ in the complete base $k+1$ representation of $(n)_{k}$ and then subtracting $1$ . Now, let $n\ge 4$ and let $2^{m_{1}}+\cdots 2^{m_{r}}$ , with $m_{1}>\cdots >m_{r}\ge 0$ be the standard base $2$ representation of $n$ . As we write down the sequence $(n)_{1}$ , $(n)_{2}$ , etc. we notice that the exponents of the first terms (in bases $2$ , $3$ , etc., respectively) increase up to a certain base, say $b$ , at which point we will have $(n)_{b-1}=(b)^{b}$ . Next, we have $(n)_{b}=b\cdot(b+1)^{b}+\cdots+b\cdot(b+1)+b$ , and the exponents never increase again as we continue the process. That is, for $k\ge b-1$ the exponents of the expression of $(n)_{k}$ are all $\leq b$ . For example, take $n=7=2^{2}+2+1$ . We have $$\begin{array}{c|c}
    b & (n)_{b-1}\\
    \hline
    2 & 2^{2}+2+1\\
    3 & 3^{3}+3\\
    4 & 4^{4}+3\\
    5 & 5^{5}+2\\
    6 & 6^{6}+1\\
    7 & 7^{7}\\
    8 & 8^{8}-1 = 7\cdot 8^{7}+\cdots+7\cdot 8+7\\
    \vdots & \vdots\\
   \end{array}$$ Thus, for $n=7$ , the exponent reaches its maximum for the first time for base $b=7$ (i.e. for the $6^{th}$ term of the sequence). Up to $n=7$ the base $b$ is easy to find. But for $n=8$ it is a very large number, and things explode for larger values of $n$ (as one would expect). How do we find this $b$ in general? Is there an explicit expression for $b$ perhaps in terms of the Hardy functions?","['number-theory', 'ordinals', 'set-theory']"
4885542,Is subset relation preserved under limit for Hausdorff metric?,"Let $X$ be a metric space. I consider elements in $Y=2^X\setminus \emptyset$ and use the Hausdorff metric for $Y$ . Suppose that $A_n \subseteq B_n$ for $A_n,B_n \in Y$ and $A_n \rightarrow A$ and $B_n \rightarrow B$ , where $A,B \in Y$ as well. Is it true that $A \subseteq B$ ?","['hausdorff-distance', 'metric-spaces', 'real-analysis', 'order-theory', 'general-topology']"
4885570,"If you lived in a 4-torus, what would the doughnut hole look like from the inside?","I'm not just curious; it refers to general relativity. Specifically, would the hole in  the torus' center look to us like a sphere, one you cannot enter because you always slip across the side and go around it instead of through?",['general-topology']
4885643,"What is the number of ways of dividing $2n$ distinct balls into $n$ identical bins, such that each bin contains two balls?","What is the number of ways of dividing $2n$ distinct balls into $n$ identical bins, such that each bin contains two balls? My working was as follows: First select $n$ balls in $\binom{2n}{n}$ ways, then put one ball into each bin, and arrange it in a total of $n!$ possible ways, then, put the rest of the $n$ balls into the $n$ bins, in one way. This can be done in a total of $\binom{2n}{n} n!$ ways. But there seems to be double counting for each bin, so we need to divide by $2^{n}$ to get the correct answer, i.e, $\binom{2n}{n} \frac{n!}{2^n} $ . My question is how is this double counting happening?
Also, more generally, is there a way to determine how to divide $kn$ distinct balls into $n$ identical bins such that each bin has $k$ balls?",['combinatorics']
4885649,Empirical process symmetrization lower bound,"This is part of Exercise 7.1.9 from Vershynin's book ""High Dimensional Probability"": Suppose $X_1(t),\dots,X_N(t)$ are $N$ independent, mean zero random processes indexed by points $t\in T$ . Let $\varepsilon_1,\dots,\varepsilon_N$ be independent symmetric Bernoulli random variables (Rademacher variables). Prove that $$\frac{1}{2}\mathbb{E}\sup_{t\in T}\sum_{i=1}^N \varepsilon_i X_i(t) \leq \mathbb{E}\sup_{t\in T}\sum_{i=1}^N X_i(t)$$ By some standard symmetrization arguments (introducing ghost processes $X_i'(t)$ and subtracting their expectation in the LHS, I arrived at $$\mathbb{E}\sup_{t\in T}\sum_{i=1}^N \varepsilon_i X_i(t) \leq \mathbb{E}_{X,X'}\left[\sup_{t\in T} \sum_{i=1}^N X_i(t)-X_i'(t)\right]$$ Without any other assumption (or the presence of an absolute value) I don't think we can conclude that the LHS of the above inequality is bounded by $2\mathbb{E}\sup_{t\in T}\sum_{i=1}^N X_i(t)$ . Is there any other argument that circumvents this obstacle or is it possible that this inequality does not hold without further assumptions or absolute values?","['empirical-processes', 'stochastic-processes', 'probability-theory']"
4885657,"Local trivialization of $\mathcal O(-1)$, proposition 2.2.6, complex geometry by Huybrechts","I was reading Complex Geometry by Daniel Huybrechts. On page 68, section 2.2 we have a proposition of holomorphic line bundle over $\mathbb P^n$ , Proposition 2.2.6: The projection $\pi:\mathcal O(-1)\rightarrow\mathbb P^n$ is given by projecting to the first factor. Let $\{U_i\}_{i=0}^n$ be an open covering of $\mathbb P^n$ . A canonical trivialization of $\mathcal O(-1)$ over $U_i$ is given by, $$\psi_i:\pi^{-1}(U_i)\rightarrow U_i\times\mathbb C,\quad(\ell,z)\mapsto(\ell,z_i)$$ The transition maps $\psi_{ij}(\ell):\mathbb C\rightarrow\mathbb C$ are given by $w\mapsto \frac{z_i}{z_j}\cdot w$ , where $\ell=(z_0:\cdots,z_n)$ . Suppose we have $(\ell,z^*)$ where $\ell$ belongs to $U_i$ and $z^*\in\mathbb C\setminus\{0\}$ . In this scenario, I assumed that if we map $(\ell,z^*)$ using $\psi_j^{-1}$ , it would look like this: $(\ell,z_0,\cdots,z_{j-1},z^*,z_{j+1},\cdots,z_n)$ , inserting $z^*$ at position $j$ . However, if this option doesn't hold, what alternatives should we consider? Because we need also to satisfy, $z\in \ell$ . Now, $\psi_i(\ell,z_0,\cdots,z_{j-1},z^*,z_{j+1},\cdots,z_n)=(\ell,z_i)$ . When we apply $\psi_i$ to $(\ell,z_0,\cdots,z_{j-1},z^*,z_{j+1},\cdots,z_n)$ , fixing $\ell$ , I noticed that the transition function $z^*\mapsto z_i$ . Question : What I got doesn't match with the book. Where did I make the mistake? Similarly, I want to tackle the same issue for sections: $\sigma_i$ in $\Gamma(U_i,\mathcal O(-1))$ and $\sigma_j$ in $\Gamma(U_j,\mathcal O(-1))$ . Here, we're lifting $\ell$ from $\mathbb P^n$ to $\mathcal O(-1)$ . So, we have $$U_j\times\mathbb C\stackrel{\psi_j}{\leftarrow}\pi^{-1}(U_i\cap U_j)\stackrel{\psi_i}{\rightarrow}U_i\times\mathbb C$$ and, \begin{align}
\sigma_i&:U_i\rightarrow\pi^{-1}(U_i)\stackrel{\psi_i}{\cong}U_i\times\mathbb C\\
\sigma_j&:U_j\rightarrow\pi^{-1}(U_j)\stackrel{\psi_j}{\cong}U_j\times\mathbb C
\end{align} Now, I couldn't get how the section $\sigma_i$ lift $\ell=(z_0:\cdots,z_n)\in U_i\cap U_j$ , one possible way maybe \begin{align}
\sigma_i(\ell)&=\left((z_0:\cdots,z_n),\frac{z_0}{z_i},\cdots,\frac{z_n}{z_i}\right)\stackrel{\psi_i}{\rightarrow}\boxed{(\ell,1)}\\
\sigma_j(\ell)&=\left((z_0:\cdots,z_n),\frac{z_0}{z_j},\cdots,\frac{z_n}{z_j}\right)\stackrel{\psi_j}{\rightarrow}\boxed{(\ell,1)}
\end{align} Because $\mathcal{O}(-1):=\{(\ell,z)\in \mathbb{CP}^n \times \mathbb{C}^{n+1}: z\in \ell\}$ , that's why I think the lifting might be $\in\mathbb{CP}^n \times \mathbb{C}^{n+1}$ (Let me correct if my understanding is wrong). Then I assume the transition map might be $\frac{z_j}{z_i}$ because $\sigma_i\mapsto \frac{z_j}{z_i}\cdot\sigma_j$ while fixing $\ell$ . I think I do mistake on the trivialization part $\boxed{(\ell,1)}$ (Does the $1$ reflect the ""matrix representation"" mentioned here ?). Question : I'm unsure if my computation is correct, can anyone verify that? And does the trivialization part play any role to get the transition maps? My confusion arise since I haven't found any information on section for holomorphic bundles up until section 2.2, though I haven't covered everything since I'm studying the book on my own and skip most of the part which seems unfamiliar to me. It will be a great help if anyone suggest an answer or resource from where I can clear my understanding. TIA","['complex-geometry', 'reference-request', 'algebraic-geometry', 'online-resources', 'line-bundles']"
4885726,Clarification on Sign in Mean Curvatures of Parallel Surfaces,"For my Differential Geometry class, I've encountered an issue with a problem from do Carmo. Although I'm aware that similar questions have been previously addressed, my issue differs. I understand how to solve the problem but am encountering a specific challenge. Firstly, let me outline the problem: Let $\mathbf{x} = \mathbf{x}(u,v)$ be a regular parametrized surface. A parallel surface to $\mathbf{x}$ is a parametrized surface $$\overline{\mathbf{y}}(u,v) = \mathbf{x}(u,v) + a \mathbf{N}(u,v),$$ where $a$ is a real constant and $\mathbf{N}$ denotes the unit normal to $\mathbf{x}$ . (b) Show that the Gaussian and mean curvatures $\overline{K}$ and $\overline{H}$ of $\mathbf{y}$ are respectively given by $$\overline{K} = \frac{K}{1 - 2Ha + Ka^2},$$ and $$\overline{H} = \frac{H - Ka}{1 - 2Ha + Ka^2}.$$ From question (a), we derived: $$\overline{\mathbf{y}}_u \times \overline{\mathbf{y}}_v = (1 - 2 H a + K a^2) (\mathbf{x}_u \times \mathbf{x}_v).$$ This implies that the unit normal $\overline{\mathbf{N}}$ to $\overline{\mathbf{y}}$ is given by $$\overline{\mathbf{N}} = \frac{\overline{\mathbf{y}}_u \times \overline{\mathbf{y}}_v}{||\overline{\mathbf{y}}_u \times \overline{\mathbf{y}}_v||} = \frac{1 - 2 H a + K a^2}{|1 - 2 H a + K a^2|} \frac{\mathbf{x}_u \times \mathbf{x}_v}{||\mathbf{x}_u \times \mathbf{x}_v||} = \text{sgn}(1 - 2 H a + K a^2) \mathbf{N}.$$ Thus, $\overline{\mathbf{N}}$ and $\mathbf{N}$ are parallel as expected. However, the term $\text{sgn}(1 - 2 H a + K a^2)$ suggests they may not always point in the same direction. My confusion arises from this sign, as other solutions I've reviewed do not include the sign function, which doesn't make sense to me. When I account for this term throughout my proof, I arrive at the given formula for $\overline{K}$ , which makes sense since Gaussian curvature is independent of orientation. However, for the mean curvature $\overline{H}$ , I derive: $$\overline{H} = \text{sgn}(1 - 2 H a + K a^2) \frac{H - Ka}{1 - 2Ha + Ka^2} = \frac{H - Ka}{|1 - 2Ha + Ka^2|}.$$ I am wondering if I've made an error in my reasoning. Note that in class we are using the convention where the ordering of the pair $(u,v)$ matters in defining the normal unit vector. If I don't use this convention, I can get the right result by choosing the orientation that suits me, but it will depend on $u$ and $v$ , which would be a bit like cheating.","['surfaces', 'curvature', 'differential-geometry']"
4885735,Lower bound of sum of cosine of angle difference,"Let $n\geq 4$ . (1) why $$\sum_{i\neq j\in[n]}\cos^2(\theta_i-\theta_j)\geq n^2/2$$ holds? Note that this corresponds to the formula $$n^2/2\leq \|X\|_F^2$$ in the paper, where $X$ is matrix with $X_{ij}=\cos(\theta_i-\theta_j)$ . Note that $i<j$ , $i>j$ are considered separatly thus there are $n^2-n$ terms in the summation. (2) does it also hold $$\sum_{i\neq j\in[n]}\cos(\theta_i-\theta_j)\geq n^2/2$$",['analysis']
4885756,Matrices and differentiation commute,"Suppose for simplicity we have a plane curve $\gamma(t)=(f(t),g(t))$ . I'm just curious exactly what is the property responsible for the fact that, if $R_{90}$ is the two dimensional rotation matrix by $90$ degrees counterclockwise, then $$\frac{d}{dt}\left(R_{90}\left(\gamma(t)\right)\right)=R_{90}\left(\frac{d}{dt}(\gamma(t))\right).$$ I'm thinking this only applies to matrices $R$ that are linear isomorphisms (and for when the multiplication makes sense). What is the general property at work here?","['calculus', 'matrix-calculus', 'linear-algebra']"
