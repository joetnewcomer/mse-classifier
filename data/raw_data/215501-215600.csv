question_id,title,body,tags
4373230,Is a compactly supported function on a locally compact Hausdorff space uniformly continuous?,"Consider the following fragment from Folland's book ""A course in abstract harmonic analysis"": Let $G$ be a locally compact Hausdorff group. I want to prove that the representation $$\pi: G \to B(L^2(\mu))$$ defined by $(\pi(x)f)(s) = f(x^{-1}s)$ is strongly continuous. I.e. if $f \in L^2(\mu)$ and $\{g_\alpha\} \to 1$ , we must show that $$\int_G |f(g_\alpha^{-1}s)-f(s)|^2 d \mu(s) = \|\pi(g_\alpha)f-f\|_2^2 \to 0.$$ My attempt : A routine density argument shows that it is sufficient to prove this for $f \in C_c(S)$ . In the hope of accomplishing this, I try to prove the following lemma (I try to mimique the proof of proposition 2.42, as suggested by Folland): Lemma : Let $X$ be a locally compact Hausdorff space and $G$ a locally compact Hausdorff topological group together with a continuous
action $G\times X \to X$ . If $f \in C_c(X)$ , then the following
property is satisfied: For every $\epsilon >0$ , there exists an open subset $V\subseteq G$ containing $1$ such that for all $g \in V$ , we have $\sup_{x \in
 X}|f(g^{-1}x)-f(x)| <\epsilon$ . Given this lemma, I can finish, but the classical proof (where $X=G$ and we consider the translation action) does not generalise to general actions. To prove the lemma, I tried to modify the proof of proposition 2.6 in Folland's book, but it becomes clear that this proof no longer works, though I still hope that there exists some modification that saves the argument. Any help/hint/suggestion/answer is highly appreciated! Thanks in advance for any help!","['measure-theory', 'harmonic-analysis', 'topological-groups', 'general-topology', 'locally-compact-groups']"
4373233,Why will the bundle homomorphism $\hat{g}:TM\to T^* M$ be bijective if each restriction $\hat{g}:T_p M\to T_p^* M$ is bijective?,"The following material is based on Lee's Introduction to Smooth Manifolds . Let $g$ be a Riemannian metric on a smooth manifold $M$ . We define a bundle homomorphism $\hat{g}:TM\to T^* M$ over $M$ as follows. For each $p\in M$ and each $v\in T_p M$ , $\hat{g}(v)$ is the covector in $T_p^* M$ defined by $$\hat{g}(v)(w)=g_p(v,w)$$ for $w\in T_p M$ . Using some linear algebra, I have been able to show each restriction $\hat{g}:T_p M\to T_p^* M$ is bijective. However, I have no idea why this result will in turn make the bundle homomorphism become bijective. Does anyone get to see this? Thank you.","['riemannian-geometry', 'differential-geometry']"
4373250,"Let $E$ and $F$ be normed spaces. Is it true that $\mathcal{L}(E,F)$ contains an isometric copy of $E'$?","Let $E$ and $F$ be normed spaces. Is it true that $\mathcal{L}(E,F)$ contains an isometric copy of $E'$ ? I want to use this idea in my proof, but I don't know if it's true. How could I define a function to check the isometric isomorphism between $E'$ and $\mathcal{L}(E,F)$ . Note: $E'$ is the topological dual of $E$ .","['general-topology', 'functional-analysis', 'analysis']"
4373278,Choice principles implied by antichain principle in ZF - Foundation,"It is known that in ZF, the axiom of choice and the
antichain principle (every partially ordered set has a maximal antichain) are equivalent. However, in ZF-Foundation, while AC still implies the antichain (maximality) principle (which I'll call AMP), the converse is not necessarily true. This also implies that the proof that AMP => AC in ZF is not ""trivial"". Are there known choice principles which are implied by AMP in ZF-Foundation, something like ""AMP => BPI"" for example. Or ""AMP => DC"", etc ... Also, is there a choice principle weaker than AC which is known to not be implied by AMP (In ZF - Foundation)?","['elementary-set-theory', 'axiom-of-choice', 'set-theory']"
4373306,Find $f:f(xf(x)+f(y))=f(x)^2+y$,"Find $f:f(xf(x)+f(y))=f(x)^2+y$ Domain and co-domain is real numbers I did the following: Let $s=f(0)$ Then $f(f(y))=s^2+y$ so $f$ is surjective Also, $f(x)=f(y)\implies f(xf(x)+f(y))=f(xf(x)+f(x))\implies x=y$ so $f$ is injective So, $f$ is bijective. Letting $f(x)=0, y=0$ we get $f(f(0))=0 \implies s=f(0)=0$ In fact $f(f(x))=x$ for ALL $x$ Letting $x=1,y=0$ we get $f(f(1))=f(1)^2\implies 1=f(1)^2 \implies f(1)=1$ or $f(1)=-1$ If $f(1)=1$ we let $x=1$ to get $f(y+1)=f(y)+1$ By induction this leads to $f(x)=x$ for all integers If $f(1)=-1$ we let $x=1$ to get $f(y-1)=f(y)+1$ By induction we get $f(x)=-x$ for all integers How to extend the domain over real numbers I don't know, any help would be appreciated",['functional-equations']
4373321,Notion of maximal atlases in differential geometry,"I'm following a course on differential geometry and from some background I've gotten used to the fact that a topological manifold $M$ is called a smooth/differentiable manifold if we can equip $M$ with some smooth atlas $\mathcal{A}$ , then the pair $(M, \mathcal{A})$ is called a differentiable manifold. Now reading Lee's book I've found out that we actually require $\mathcal{A}$ to be something called maximal. He first states that Our plan is to define a “smooth structure” on $M$ by giving a smooth atlas, and to
define a function $f : M \to \Bbb R$ to be smooth if and only if $f \circ \varphi^{-1}$ is smooth in the sense of ordinary calculus for each coordinate chart $(U, \varphi)$ in the atlas. There is one minor technical problem with this approach: in general, there will be many possible atlases that give the “same” smooth structure, in that they all determine the same collection of smooth functions on $M$ . Then he goes on to state that However, it is more straightforward to make the following definition: a smooth atlas $\mathcal{A}$ on $M$ is maximal if it is not properly contained in any larger smooth atlas. This just means that any chart that is smoothly compatible with every chart in $\mathcal{A}$ is already in $\mathcal{A}$ . The lecturer in the course I'm taking says that in practice we only need to consider some atlas $\mathcal{A}$ for $M$ instead of the maximal one which is causing my confusion. Why is this true? Lee also  gives the proposition $1.17$ which states that Let $M$ be a topological manifold, then every smooth atlas $\mathcal{A}$ for $M$ is contained in a unique maximal smooth atlas, called the smooth structure determined by $\mathcal{A}.$ and presumably this is the reason why we can consider some arbitary atlas $\mathcal{A}$ instead of the maximal one? Why does this imply that we don't need to consider the maximal one? I don't think it's obvious.","['manifolds', 'smooth-manifolds', 'differential-geometry']"
4373339,Show that $\int_0^1 |t-z|^{-1/2}\ \mathrm{d}t < c(1 + |z|)^{-1/2}$,"I want to show that there is a constant $c > 0$ such that $$
\int_0^1 |t-z|^{-1/2}\ \mathrm{d}t < c(1 + |z|)^{-1/2}
$$ for any $z \in \mathbb{C}$ .
I found the assertion in a paper I'm reading and I have not succeeded in proving it. Perhaps it uses a common technique in complex analysis (my complex analysis is rusty). Power series expansions came to mind, but I still don't see how to make use of it.","['complex-analysis', 'inequality']"
4373372,Why does $\frac{(x+2)\cdot (2x+\frac{1}{x^2})-(x^2-\frac{1}{x})\cdot 1}{(x+2)^2}$ simplify to $\frac{x^4+4x^3+2x+2}{x^2(x+2)^2}$?,"I have been burdened with an unclear textbook. The exercise is to differentiate the fraction of two functions. Here it is: $$f(x)=\frac{x^2-\frac{1}{x}}{x+2}$$ I understand how to do this: $\left ( \frac{a}{b} \right )'=\frac{ba'-ab'}{b^2}$ . I have that memorised, and for step one me and the textbook agree. Here's the textbook solution: $$f'(x)=\frac{(x+2)\cdot (2x+\frac{1}{x^2})-(x^2-\frac{1}{x})\cdot 1}{(x+2)^2}=\frac{x^4+4x^3+2x+2}{x^2(x+2)^2}$$ Now here's my solution: $$f'(x)=\frac{(x+2)\cdot (2x+x^{-2})-(x^2-x^{-1})\cdot 1}{(x+2)^2}=\frac{2x^2+x^{-1}+4x+2x{-2}-x^2+x^{-1}}{x^2+4x+4}=\frac{x^2+4x+2x^{-1}+2x^{-2}}{x^2+4x+4}$$ My approach is simple: I write instances of $1/x^n$ with negative exponents, multiply everything in brackets until there's no more brackets, then I add together the terms with the same exponent and order them from high to low. I understand that brackets are necessary if you want to keep multiplication factors intact (for instance to figure out for what values of x the result is zero) but then I might as well not have done anything. The textbook clearly goes for something else entirely, and its approach I truly cannot devise. I know that these two answers are equivalent. Wolfram Alpha agrees . My question is, why would one go for the textbook notation when simplifying the immediate solution (the step me and my textbook agree on). This is a consistent problem I have with the textbook; my answers end up equivalent to the textbook answer but entirely differently notated. I feel like there's something I am missing, for when I ask Wolfram to differentiate the original fraction, it too ends up with the textbook notation . I don't have a tutor to ask; I am working my way through the book on my own in preparation for a programming course. This feels critical to me; what are the steps to reach the textbook solution, and most importantly why are those the steps to take?","['algebra-precalculus', 'derivatives']"
4373376,Why are they called Rules of Differentiation and not Rules of Derivation,"Regarding Differentiation, Derivation, and Integration. I think semantics are SUPER important and I'm confused on the precise scope of these three words. Is derivation a type differentiation, or visa versa? Is integration considered a type differentiation or is it something completely different? The following is my understanding so far. The non-productive suffix -ation is used to form nouns meaning ""the action of (a verb)"" or ""the result of (a verb)"". I used multiplication as a control. Verb Adjective Noun w/ non-productive suffix Multiply Multiple Multiplication Differentiate Differential Differentiation Derive Derivative Derivation Integral Integrate Integration My main problem is this. My textbook gives me The Rules of Integration to find the Integral of a function. So why does it give me The Rules of Differentiation to find the Derivative of a function. As far as I am aware a Differential is NOT the same as a Derivative, but that a Differential is a part Derivative. That is to say every Derivative has a Differential as a part of it, but not every Differential is part of a Derivative. TLDR; Why are they called Rules of Differentiation and not Rules of Derivation?","['integration', 'calculus', 'definition', 'derivatives']"
4373378,Showing a simple example of uniform convergence in probability,"Let $\{y_1,\dots,y_n\}$ a random sample. Let $f_n(\mu) = \frac{1}{n}\sum_{i=1}^n (y_i - \mu)^2, \quad f(\mu) = E[(y_i - \mu)^2].$ Let $\Theta \subset \mathbb{R}$ be a compact set. I want to show that $\sup_{\mu \in \Theta} |f_n(\mu) - f(\mu)| \to^p 0$ I know that since $f$ and $f_n$ are continuous for all $n$ the $\sup$ exists and is attained in $\Theta$ . But I am not sure how to proceed further and show convergence in probability for that object.","['probability-theory', 'uniform-convergence']"
4373453,Sum of the square harmonic series,"I stumbled across the following series reviewing some HW from a few years ago $\sum_{i=1}^{n}\left(\sum_{j=i}^{n}\frac{1}{j}\right)^2$ i.e. $(\frac{1}{1}+\frac{1}{2}+\ldots+\frac{1}{n})^2+(\frac{1}{2}+\ldots+\frac{1}{n})^2+\ldots+(\frac{1}{n})^2$ This series equals $2n-\sum_{i=1}^{n}\frac{1}{i}$ , which I have confirmed with some code.  I am curious if anyone can give a hand in trying to show this relation.  So far, writing $\sum_{i=1}^{n}\frac{1}{i}$ as $S_n$ , I have rewritten the sum as $S_n^2+(S_n-S_1)^2+(S_n-S_2)^2+\ldots +(S_n-S_{n-1})^2$ But have been stuck at dead ends using this approach.  Any thoughts or hints would be greatly appreciated.",['analysis']
4373468,Clarification about solving cubic equations,"I'm trying to understand a statement written on Wikipedia about solving cubic equations. In particular this part: A ''cubic formula'' for the roots of the general cubic equation (with $a\neq 0$ ) $$ax^3 + bx^2 + cx + d = 0$$ can be deduced from every variant of Cardano's formula. The formula being rather complicated, it is worth splitting it in smaller formulas.Let $\begin{align} \Delta_0 &= b^2 - 3ac,\\ \Delta_1 &= 2b^3 - 9abc + 27a^2d. \end{align}$ Then $$C = \sqrt[3]{\frac{\Delta_1 \pm \sqrt{\Delta_1^2 - 4 \Delta_0^3}}2}$$ where the symbols $\sqrt{{~}^{~}}$ and $\sqrt[3]{{~}^{~}}$ are interpreted as ""any"" square root and ""any"" cube root, respectively. The sign $\pm$ before the square root is either $+$ or $-$ ; the choice is almost arbitrary, and changing it amounts to choosing a different square root.
Then, one of the roots is $x = - \dfrac{1}{3a}\left(b+C+\dfrac{\Delta_0}{C}\right).$ The other two roots can be obtained by changing the choice of the cube root in the definition of $C$ , or, equivalently by multiplying $C$ by a primitive root of unity, that is $\dfrac{–1 \pm \sqrt{–3}}{2}$ . In other words, the three roots are $x_k = - \frac{1}{3a}\left(b+\xi^kC+\frac{\Delta_0}{\xi^kC}\right), \qquad k \in \{0,1,2\}$ , where $ξ = \dfrac{–1 \pm \sqrt{–3}}{2}.$ First question: what does it mean ""the choice is ALMOST completely arbitrary?"" In what sense is ALMOST? Second question: I tried to apply this to the following cubic $x^3 - 3x^2 + 2x - 2 = 0$ . I calculated what necessary that is $\Delta_0 = 3$ , $\Delta_1 = -54$ . Then $C = \sqrt[3]{\dfrac{-54 \pm \sqrt{54^2 - 12}}{2}}$ . If I choose the plus sign, then $C \approx 0.391$ and it results $x_0 \approx 3.68$ . If I choose the minus sign, then $C \approx 3-77$ and it results $x_0 \approx 2.52$ . Only the second choice leads to a very good solution for the cubic (the two others are real). So it appears that if I chose the plus sign I would have not gotten the right solution. How can I know what sign to choose, because it is not arbitrary, and that ""almost"" arbitrary needs clarification.","['cubics', 'algebra-precalculus', 'roots']"
4373487,Følner net on a direct union of amenable groups,"Let $G$ be an amenable group and let $(H_i)_{i\in I}$ be the poset of all the countable (or, if simpler, even finitely generated) subgroups of $G$ so, in particular, we have $G=\bigcup_IH_i$ . Furthermore, each $H_i$ is amenable (as it is a subgroup of an amenable group) and countable, so it has a Følner sequence $(F_{n,i})_{n\in\mathbb N}$ . In this situation, is it possible to see that $(F_{n,i})_{(n,i)\in \mathbb N\times I}$ (with $\mathbb N\times I$ endowed with the product order) is a Følner net for $G$ ? If this is not true in general, is it possible to choose the Følner sequences $(F_{n,i})_{n\in\mathbb N}$ in some special way that makes the result true? Finally,  if also my second question has a negative answer, is there some other canonical way to build a Følner net for $G$ out of Følner sequences (or nets) for the $H_i$ 's?","['amenability', 'group-theory', 'ergodic-theory']"
4373495,Why is it true that the solution to this IVP is always $<3$ when $x\geq 0$ (without solving the equation)?,"I'm doing some more practice problems for my upcoming DEs test, and I tried this true/false question: The solution to the initial value problem $dy/dx=(x-2)(y-3)^2,y(0)=0$ , will always be less than $3$ ; that is, $y(x)<3$ for $x\geq 0$ . Bear in mind that the question assumes the person doing the problem doesn't know how to solve DEs yet, since that is covered in the subsequent chapter. Using the uniqueness and exactness theorem, given that $dy/dx=F(x,y)$ , I checked if $F$ and $\partial F/\partial y$ were continuous at the point $(0,0)$ by inputting the point $(0,0)$ into both. If they are continuous, there exists a rectangle $R$ that includes the point $(0,0)$ in which $F$ and $\partial F/\partial y$ are continuous. $F(0,0)=-2(-3)^2=-18$ $\displaystyle\frac{\partial F}{\partial y}=2(x-2)(y-3)$ $\displaystyle\frac{\partial F}{\partial y}\bigg|_{(0,0)}=2(-2)(-3)=12$ Since the limits of $F$ and $\partial F/\partial y$ can be solved via substitution, the limits are the same as the evaluations of the points, and they are both continuous at any point. Therefore, there exists a rectangle $R$ that includes $(0,0)$ where both are continuous, meaning there is a unique solution to the initial value problem. However, I at first concluded that since $\partial F/\partial y$ is positive, $y$ is increasing at $(0,0)$ , and the statement is false. However, the answer is that the statement is true. I've thought about it a bit more and realized that since $dy/dx$ is negative at that point, $y$ is actually decreasing at $(0,0)$ , not increasing! Also, we can't conclude anything about the statement from the values at $(0,0)$ besides the fact that only one unique solution exists. So I tried setting $dy/dx$ to $0$ to see if I could find any critical points, in an attempt to see if there's a maximum at $y=3$ . $dy/dx=(x-2)(y-3)^2=0 \implies x=2\;\text{or}\;y=3\implies$ Critical value at $y=3$ . But the second derivative test was inconclusive: $\begin{align}\displaystyle\frac{d^2 y}{dx^2}&=(y-3)^2+2(x-2)(y-3)\displaystyle\frac{dy}{dx}\\
&=(y-3)^2+2(x-2)(y-3)(x-2)(y-3)^2\\
&=(y-3)^2+2(x-2)^2(y-3)^3\\
\displaystyle\frac{d^2y}{dx^2}\bigg|_{y=3}&=(3-3)^2+(x-2)^2(3-3)^3\\
&=0
\end{align}$ So that didn't work! However, I just tried solving the DE to see what would happen since the test I'm taking covers solving separable DEs anyway: $\begin{align}
\displaystyle\frac{dy}{dx}&=(x-2)(y-3)^2 \\
\displaystyle\int\frac{dy}{(y-3)^2}&=\int(x-2)dx\\
-\displaystyle\frac{1}{y-3}&=x^2-2x+C \\
\end{align}$ Applying the initial condition: $-\displaystyle\frac{1}{0-3}=(0)^2-2(0)+C\implies C=\displaystyle\frac{1}{3}$ . Therefore the unique solution is $y=-\displaystyle\frac{1}{x^2-2x+\frac{1}{3}}+3$ It can now be easily seen that the maximum of this expression is $y=3$ since $\lim_{x\to \infty} y(x)=3$ . But is there any way to determine this without solving the equation?","['initial-value-problems', 'ordinary-differential-equations']"
4373503,Inequality of Incomplete Sum for $e^n$ Versus $e^{n}/2$,"Several years ago, I encountered a problem: Prove that, for all natural numbers $n\ge 1$ we have $$\sum_{k=0}^{n-1} \frac{n^k}{k!}<\frac{e^n}{2}<\sum_{k=0}^{n} \frac{n^k}{k!}.$$ The original solution is using induction, but that solution is false. Recently I am revisiting the problems and I found this one, so I am asking here. P.S. I have read the post how to ask a good question . I really don't know how to go on with this problem. Please don't regard this question as no-clue questions, thanks. To answer the commonly asked question ""what attempts have I tried."" I would like to say that I have tried to use a different induction, but it gives no result. Also, by graphing, the solution of $\frac{e^x}{2}=\sum_{k=0}^{n} \frac{x^k}{k!}$ I have observed is around $n+\frac 23$ (I know that it is not allowed to ask two questions in one post, but I will put this observation here and if this question is proved, I am going to ask a separate question for this observation.)","['inequality', 'sequences-and-series', 'real-analysis']"
4373515,Limit of $\sum_{i=1}^k a_iz_i^n$ as $n\to\infty$ where $|z_i| = 1$,"Let $z_1, ..., z_k \in S^1\setminus\{1\}$ , $a_1, ..., a_k\in\mathbb{C}\setminus \{0\}$ with $z_i\neq z_j$ for $i\neq j$ , and consider the sequence of the form $b_n = \sum_{i=1}^k a_iz_i^n$ . Is it true that $\lim_{n\to\infty} b_n$ can never exist? I think that if we assume $z_1, ..., z_k$ are all roots of unity, then $b_n$ must be non-constant and periodic, so the limit doesn't exist. If $z$ isn't a root of unity, it's known that $\{z^n : n\in\mathbb{N}\}$ is dense in $S^1$ , so I think that in this case the limit also won't exist, but I'm not sure if the density of two such terms won't ""cancel out"".","['limits', 'complex-numbers', 'sequences-and-series']"
4373593,"Given $f \in L^2(\mathbb{R})$, show that $\operatorname{span}\{f(x-n)\}_{n \in \mathbb{Z}}$ is not dense in $L^2(\mathbb{R})$?","Given any $f \in L^2(\mathbb{R})$ , show that the span of $\{f(x-n)\}_{n \in \mathbb{Z}}$ is not dense in $L^2(\mathbb{R})$ ? This problem comes from this presentation with slides here . It is claimed under the ""Remark"" section that this set cannot be dense. They actually deal with a more general case but I choose $p=2$ for simplicity. Since $L^2(\mathbb{R})$ is a Hilbert space, then the sequence above being dense in the space is equivalent to the following condition: If we have $g \in L^2(\mathbb{R})$ with $(g,f(x-n)) = 0$ (inner-product) for every $n \in \mathbb{Z}$ , then $g = 0$ (in the $L^2(\mathbb{R})$ - sense). The idea is to find a nonzero $g \in L^2(\mathbb{R})$ that satisfies the hypothesis above. However, I am unable to show this. Is there a simple function that gets the job done? Thanks :)!","['hilbert-spaces', 'sequences-and-series', 'fourier-analysis', 'real-analysis']"
4373595,Looking for a function which is convex when x is between 0 and 1 but concave when x is greater than 1 [closed],Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question I am looking for such a function for plotting a figure like in the picture in Python. Many thanks!,['algebra-precalculus']
4373657,Why are there more prime factors of the form $4k-1$ than of the form $4k+1$?,"The density of primes of the form $4k-1$ and $4k+1$ is equal although slight discrepancy exists in the exact numbers also known as Chebyshev bias. All odd prime factors of a number will be either of the form $4k-1$ or $4k+1$ . If we take all natural numbers $n \le x$ and look at the prime factors of each of these numbers, the density of prime factors of $4k-1$ significantly higher than that of the form $4k+1$ , i.e. the bias is much stronger than Chebyshev bias. Experimental data shows that the difference between their densities is a constant. Let $a(n)$ = no. of distinct primes factors of $n$ which are of the form $4k-1$ and $b(n)$ = no. of distinct primes factors  of $n$ which are of the form $4k+1$ . Data at every checkpoint from $x = 10^6$ to $x = 10^9$ shows a consistent trend that $$
\frac{1}{x}\sum_{n \le x} [a(n) - b(n)] \approx 0.83498
$$ At a more granular level, we consider only odd numbers then the above constant is $\approx 0.33498$ and if we consider only even numbers then it is $\approx 1.33498$ . Question : What is the source of this bias and is there a closed form of $$
\lim_{x \to \infty}\frac{1}{x} \sum_{n \le x} [a(n) - b(n)]
$$","['elementary-number-theory', 'number-theory', 'analytic-number-theory', 'limits', 'prime-numbers']"
4373662,"Continuity of this function as $(x,y)$ tends to $(0,0)$","Here's a function in $x$ and $y$ defined piecewise as $$f(x,y)=
\left\{\begin{array}{ll}
      0 & (x,y)=(0,0)\\
 \frac{x^2y}{x^4+y^2} & (x,y) \neq (0,0) \\
\end{array}\right.$$ Examine its continuity as the ordered pair tends to $(0,0)$ . Okay, so I first tried this by assuming $x=\frac{1}{n}$ and $y=\frac{1}{n^2}$ , where $n\rightarrow \infty$ . The limit of the function as $(x,y) \rightarrow (0,0)$ came out to be $\frac{1}{2}$ , and since this is not equal to the value of the function at the said point, its discontinuous at the origin. But when I assumed $x=\frac{1}{n}$ and $y=\frac{1}{n}$ , I got the limit zero: $$f(\frac{1}{n},\frac{1}{n})= \frac{\frac{1}{n^3}}{\frac{1}{n^4}+ \frac{1}{n^2}} = \frac{\frac{1}{n}}{\frac{1}{n^2}+ 1} $$ Since $x,y \rightarrow 0$ , $f(\frac{1}{n},\frac{1}{n}) \rightarrow \frac{0}{0+1}=0$ Where am I going wrong?","['continuity', 'multivariable-calculus']"
4373734,Can you prove this equality? Binomial coefficients and probability frequency,"Here's the equality : $$\frac{\sum_{k=0}^n \binom{n}{k}(k/n)}{2^n} = 1/2$$ I've tried for 1, 2, 3 and found the equality was right.
I don't know how to prove it (by induction, maybe, but the formula looks too complicated for induction to me) and I'd also like to know (if there is any) the name of this equality. I found it when looking at the probability of a coin (2 sides) and I took the mean of all the frequencies weighted by their amount of occurence. Could have been a dice (6 sides) so I feel like this formula could exist for 1/6 = .... That's why I am looking for the generic name of it ! Thank you in advance for your help in this problem","['binomial-coefficients', 'probability']"
4373737,Lebesgue Measure on $\mathbb{R}^n$,"I know that the n-dimensional Lebesgue-Measure $\lambda^n$ is defined at least on all Borel Sets $\mathcal{B}^n= \mathcal{B}(\mathbb{R}^n)$ . Let's assume $A=(a,b) \subseteq \mathbb{R}^n$ . (i) My first question is whether it is possible to define $\lambda^n(A)$ ? I suppose that this leads to $\lambda^n(A)=0$ ? (ii) Is it allowed to use the 1-dimensional Lebesgue-Measure here or can we do no other than using $\lambda^n$ since the space is $\mathbb{R}^n$ ? May anybody have an answer/explanation to this?","['measure-theory', 'lebesgue-measure']"
4373791,"$A \in M_{n\times n}(\mathbb{Q})$ with $A\neq I_n$, $A^p=I_n$ then $p\leq n+1$.","Let $A \in M_{n\times n}(\mathbb{Q})$ with $A\neq I_n$ . I want to prove the following: if $A^p =I_n$ for a prime $p$ , then $p\leq n+1$ . Hmm, At this moment, I have no good idea for this problem.  I just list some of my ideas (which seem not useful).... If $A^p=I_n$ , then $\det(A) \neq 0$ so $A$ is invertible. For matrix over $\mathbb{C}$ , $A^p-I=0$ implies $m_A(t)|t^p-1$ , and for this case since $\mathbb{C}$ is algebraically closed, minimal polynomial factors out to monic polynomial and hence $A$ is diagonalizable. So $\lambda^p=1$ ...
(This approach seems not good.)","['matrices', 'matrix-equations', 'linear-algebra']"
4373871,Branching process: The expected value of number of cells,"Suppose that there are two kinds of cells, type $A$ and type $B$ . Let $A_t$ and $B_t$ be the number of cells (i.e. $\mathbb{N}_{0}$ -valued) of type $A$ and type $B$ , respectively, at time $t\in\mathbb{R}_{+}$ . Each cell of type $A$ , independent of all other cells is, after a time that is exponential distribution with parameter $\lambda>0$ , dived into two cells of type $A$ with probability $p_1> 0$ , two cells of type $B$ with probability $p_2> 0$ ,and one cell of type $A$ and one cell of type $B$ with probability $p_3> 0$ , where $p_1+p_2+p_3 = 1$ . Each cell of type $B$ , independent of all other cells, after a time that is exponential distribution with parameter $\gamma>0$ , dies. i.e. the number of cells of type $B$ decreases by one. Let $a\in\mathbb{N}$ and $b\in\mathbb{N}_{0}$ . What is \begin{align}
\mathbb{E}[A_t\mid A_0=a]
\end{align} and \begin{align}
\mathbb{E}[B_t\mid A_0=a,B_0=b]
\end{align} for each time $t\in\mathbb{R}_+$ ? My attempt based on the suggestion by mjqxxxx : I'd argue that something like \begin{align}
\partial_t\mathbb{E}[A_t\mid A_0=a] = \lambda(p_1-p_2) \mathbb{E}[A_t\mid A_0=a]
\end{align} should hold and therefore the solution is \begin{align}
\mathbb{E}[A_t\mid A_0=a] = a \exp(\lambda(p_1-p_2)t)
\end{align} for each time $t\in\mathbb{R}_+$ .  Moreover, \begin{align}
\partial_t\mathbb{E}[B_t\mid A_0=a,B_0=b] = -\gamma \mathbb{E}[B_t\mid A_0=a,B_0=b] + \lambda(2p_2 + p_3)\mathbb{E}[A_t\mid A_0=a]
\end{align} and therefore \begin{align}
\mathbb{E}[B_t\mid A_0=a,B_0=b] = 
\begin{cases}
 b \exp(-\gamma t) + \frac{\lambda(2p_2+p_3)a\exp(-\gamma t)}{\gamma +\lambda(p_1 - p_2)}\left(\exp((\gamma +\lambda(p_1 - p_2))t)-1\right) & \text{if } \gamma +\lambda(p_1 - p_2) \neq 0 \\
(b+\lambda(2p_2+p_3)a t)\exp(-\gamma t) & \text{otherwise}
\end{cases}
\end{align} for each time $t\in\mathbb{R}_+$ . Is this true? How do you formalize this?","['stochastic-processes', 'mathematical-modeling', 'probability-theory', 'ordinary-differential-equations']"
4373882,"Prove that $f$ is a contraction iff $\exists K\in (0,1):\forall x \in \mathbb R, |f'(x)|\leq K$","In an exercise my teacher asked the following: Let $f:\mathbb R\to \mathbb R $ be a function of class $C^1$ . Prove that $f$ is a contraction iff $\exists K\in (0,1):\forall x \in \mathbb R, |f'(x)|\leq K$ . I was able to prove this but I didn't use anywhere the fact that $f'$ is a continuous function and because of that, I'm starting the question of the validity of my proof. This is what I did: (1. contraction $\implies$ $|f'(x)|\leq K$ ) If $f$ is a contraction, then $\exists K\in (0,1)$ such that: $\forall x,y\in \mathbb R, |f(x)-f(y)|\leq K|x-y|$ , and this is the same as: $$\left|\frac{f(x)-f(y)}{x-y}\right|\leq K$$ Let $x \in \mathbb R$ , then: $$|f'(x)|=\left|\lim _{\alpha\to 0} \frac {f(x+\alpha ) - f(x)}{(x+\alpha)-x}\right|$$ $$ =\lim_{\alpha \to 0}\left| \frac {f(x+\alpha ) - f(x)}{(x+\alpha)-x}\right|\leq K$$ (2. $|f'(x)|\leq K \implies$ contraction) Let $f$ be a function such that $\forall x \in \mathbb R, |f'(x)| \leq K$ with $K \in (0,1)$ . Let $x,y \in \mathbb R$ with $x < y$ . Then, $\exists c \in (x,y)$ such that: $$\left| \frac{f(y)-f(x)}{y-x} \right|=|f'(c)|\leq K$$ So: $$|f(y)-f(x)|\leq K|y-x|$$ In the proof, I never used the fact that the function $f$ has a continuous derivative. Is the proof correct or did I make some kind of mistake that considering that $f'$ is continuous would fix?","['solution-verification', 'derivatives', 'lipschitz-functions', 'real-analysis']"
4373889,Why is this substitution in the double integral effective?,"I'm working with double integrals, and came about a problem where I had to calculate the following integral: $$ \int \int_D \left | 3x+4y \right|dx dy$$ where D is the region contained within the circle whose equation is given by $x^2+y^2 \leq 2$ . At first I thought I directly might use polar coordinates, but that seemed to complicate things when we have absolute values in the integrand. I went to check on if there was any hints to the problem, and there was. The book hinted that a substitution of the form $$\begin{pmatrix}
u \\v \end{pmatrix} = \begin{pmatrix}3/5 & 4/5 \\-4/5& 3/5 \\\end{pmatrix} \begin{pmatrix}x \\y \end{pmatrix}$$ And when I made that substitution, I realized that the new region, let's call it $A$ became $u^2+v^2 \leq 2$ , the Jacobian became 1, and the double integral became: $$\int \int_A 5 |u| du dv$$ which then easily can be calculated through polar coordinates. It was the first time seeing the substitution, so I tried to generalize it for a double integral of the form: $$I := \int \int_D \left | \alpha x +\beta y \right | dx dy$$ where $D$ is a region given by the circle whose equation is $x^2+y^2 \leq \gamma^2$ . I later used the same method. First I let: $$\begin{pmatrix}
u \\v \end{pmatrix} = \frac{1}{||(\alpha, \beta)||}\begin{pmatrix}\alpha & \beta \\- \beta& \alpha \\\end{pmatrix} \begin{pmatrix}x \\y \end{pmatrix}$$ Then, as previously, I get the equivalent region $A$ given by $u^2+v^2\leq \gamma^2$ , and the Jacobian $J = 1$ , just as in the previous numerical example. We also realize that $\left| \alpha x +\beta y \right | = ||(\alpha, \beta)|| \cdot|u|$ , and so our double integral just becomes: $$||(\alpha, \beta)|| \int \int_A |u| dudv$$ which then, through polar coordinates, gives us that $I$ can be evaluated to $\frac{4 ||(\alpha, \beta)|| \gamma^3}{3}$ . What I realized from this, is that this substitution is really efficient for calculating double integrals with the given integrand. However, I can't really geometrically see how these transformations look like. I think that if someone could provide any geometrical intution on what these transformations look like between the $xy$ - plane, to the $uv$ - plane and to the $r\theta$ - plane, this would really deepen my understanding on why this substitution is efficient in the way it is. Thank you in advance for any such contributions.","['integration', 'multivariable-calculus', 'linear-transformations']"
4373909,Have I determined the inverse element for this group correctly?,"The operation $\circ $ has been defined on the set $S=\{ (a,b)|a, b\in \mathbb{R} \}$ in the following manner: $(a,b)\circ (c,d)=(a+c+1,b+d)$ . Show that $(S,\circ)$ is a group. I've already finished showing closure, associativity and identity, but I'm not sure if the inverse element is correct. More specifically, is $(a,b)^{-1}$ in this case, the same as $(a^{-1},b^{-1})$ ? Here's what I've tried doing so far: Let $(a,b)\in S$ , then there exists an $(a,b)^{-1}\in S$ such that: $(a,b)\circ (a,b)^{-1}=(a,b)^{-1}\circ (a,b)=(-1,0)$ (the identity element) From here, we conclude that: $a^{-1}=-a-2\\ b^{-1}=b$ And therefore the inverse element of $S$ is $(-a-2,-b)$ .","['group-theory', 'solution-verification']"
4373926,Find all functions $f:\mathbb R \rightarrow \mathbb R$ that have following two properties,Find all functions $f:\mathbb R \rightarrow \mathbb R$ that have following two properties (i) $f(f(x))=x$ $\;$ $\forall \in \mathbb R$ (ii) $x \geq y$ then $f(x)\geq f(y)$ My Approach: $f(f(x))=x$ $\implies$ $f(x)$ is one-one and onto. So let $f(x)=t$ From $(i)$ property $f(y)=t$ $\implies$ let $t\neq x$ Case $(1)$ $x<t$ $\implies$ $f(x)\leq f(t)$ $\implies$ $t\leq x$ Hence Contradiction. Case $(2)$ $t<x$ $\implies$ $f(t)\leq f(x)$ $\implies$ $x\leq t$ Hence again contradiction From  Case $(1)$ and Case $(2)$ $x=t$ $\implies$ $f(x)=x$ $\;$ $\forall x \in \mathbb R$ My doubt: Is my conclusion $f(x)$ is bijective correct just by seeing $f(f(x))=x$ ? Am i missing anything? Other way to solve this Problem is also appreciated,"['functional-equations', 'algebra-precalculus', 'functions']"
4373931,"If $ \lim_{x\to\infty} \frac{f(x)}{x} = 1$, then $\exists x_n \rightarrow \infty$ such that $\lim_{n\to\infty} f'(x_n) = 1$.","Let $f: \mathbb{R} \longrightarrow \mathbb{R}$ be a differentiable function. If $$
\lim_{x\to\infty} \frac{f(x)}{x} = 1,
$$ then there exists a sequence $(x_n)$ such that $x_n \rightarrow \infty$ as $n \rightarrow \infty$ and $$
\lim_{n\to\infty} f'(x_n) = 1.
$$ We tried to use the Mean Value Theorem. For each $n$ , there exists $x_n \in [0,n]$ such that $$
f'(x_n) = \frac{f(n) - f(0)}{n}.
$$ Hence, $\lim_{n\rightarrow \infty} f'(x_n)=1$ . But, we are not sure that $x_n \rightarrow \infty$ . Thank you for any hint.","['functions', 'derivatives', 'real-analysis']"
4373943,Martingale is Uniformly Integrable,"Let $(Z_n)_{n\ge0}$ be a sequence of i.i.d. r.v.'s with $P(Z_1=1)=P(Z_1=-1)=\frac{1}{2}$ . Let $S_0=0$ and $S_n=Z_1+\dots+Z_n$ . Let $\mathcal{F}_0=\{\emptyset,\Omega\}$ and $\mathcal{F}_n=\sigma(Z_1,\dots,Z_n)$ . Let $a>0$ , $0<\lambda<\frac{\pi}{2a}$ , and $T=\inf\{n\ge1:|S_n|=a\}$ . So far, I have shown that $$ X_n=(\cos{\lambda})^{-n}\cos{(\lambda S_n)} $$ is a martingale and that $T$ is almost surely finite. Now, I must further prove that $(X_{n\wedge T})_{n\ge0}$ is uniformly integrable, but I don't see how. Could someone give me a hint?","['random-walk', 'uniform-integrability', 'martingales', 'stopping-times', 'probability-theory']"
4373997,"Can there be ""polynomial spaces""?","I don't know how to better frame this question. Thinking about vector spaces and their role in basically everything Calculus touched, I can understand why they are so central, especially in areas like differential geometry. But Taylor's Theorem got me thinking; my personal view of what Taylor's Theorem tells us is that the derivative is not a ""one ocurrence"" phenomenon, it is a part of a larger set of objects that approximate the function to a polynomial. That is, the derivative is not the most central object, in a fundamental way, just the most convenient I guess? Since it is easier to deal with linear functions than with, say, quadratic or cubic functions, and of course, it is easier to work with vector spaces than with other kinds of spaces. Considering that vector spaces' structure is, in some sense, fine tuned, so that linear applications preserve their structure, what space would be such that quadratic applications preserve its structure? Or cubic applications? Are there such ""polynomial"" spaces? What properties might they have? Are they useful? My guess is that in principle they would be useful, because just as we can approximate functions to linear or quadratic functions and build powerful analysis from that, one might guess that we could approximate non-linear spaces with ""sums"" of these ""polynomial spaces"", perhaps even yielding a notion of ""derivatives for a space""? This might be a meaningless question but I would appreciate some insight. EDIT : For illustrating what I mean. Consider a vector space $V$ . Then a linear map $T$ from $V$ to another vector space preserves the structure of $V$ a vector space, that is, $$T(\alpha u+\beta v)=\alpha T(u)+\beta T(v), u,v\in V$$ Or in other words, the image of the vector space $V$ is also a vector space. What I'm looking for is a space $Q$ such that quadratic applications would have the property: $$T(\alpha u+\beta v)=\alpha^2 u^2+2\alpha \beta uv+\beta^2v^2, u,v \in Q$$ Or in other words, the image of Q by T is also a ""----"" space, that is, it preserves its structure, whichever it might be.",['linear-algebra']
4374007,"Periodic solution of ODE, why does my counterexample not work?","I am considering the ordinary differential equation of form $\frac{dN}{dt}= f(N)$ , for example $\frac{dN}{dt} = N$ . I know that this kind of ode has no non-constant periodic solution. However, I am thinking about $N(t)=\sin(t)$ . Since I can write $$ \frac{dN}{dt} = \cos(t) = \sqrt{1-N^2} :=f(N).$$ So the above differential equation does have periodic solution. I don't understand why my example is not a good counterexample. My feeling is that my example has restriction on N ( $-1\leq N\leq1$ ). Am I correct?",['ordinary-differential-equations']
4374081,"In a triangle ABC, if certain areas are equal then P is its centroid","Let $P$ be a point in the interior of $\triangle ABC$ . Extend $AP$ , $BP$ , and $CP$ to meet $BC$ , $AC$ , and $AB$ at $D$ , $E$ , and $F$ , respectively. If $\triangle APF$ , $\triangle BPD$ , and $\triangle CPE$ , have equal areas, prove that $P$ is the centroid of $\triangle ABC$ . I am trying to do with Ceva's theorem: $\frac{AF}{FB}\cdot\frac{BD}{DC}\cdot\frac{CE}{EA}=1$ and also with the result $\frac{AP}{PD}=\frac{AF}{FB}+\frac{AE}{EC}$ but having some difficulties. Please give any hint.","['euclidean-geometry', 'area', 'centroid', 'geometry', 'triangles']"
4374105,"Evaluating $\int_{0}^{\infty} \frac{\ln^3 (1+x^2)}{1+2x^2} \, dx$ with contour integration","I’m trying to evaluate the following integral through a contour method: $$\int_{0}^{\infty} \frac{\ln^3 (1+x^2)}{1+2x^2} \, dx$$ Defining $$f(z)=\frac{\ln^3 (1+z^2)}{1+2z^2}$$ I’ve tried to use a semi-circular contour from $-R$ to $R$ as the integrand is even and have calculated the residue at $z=\frac{i}{\sqrt{2}}$ which gives: $$\text{Res} \left(f(z),\frac{i}{\sqrt{2}}\right) = \frac{\ln^3 (2)}{2\sqrt{2}} i$$ However, I’m struggling because there are branch cuts that I’m not sure how to deal with here. I can evaluate the related integral: $$\int_{0}^{\infty} \frac{\ln (1+x^2)}{1+2x^2} \, dx = -\frac{\pi\ln(6-4\sqrt{2})}{2\sqrt{2}}$$ Since $\ln (1+x^2)=\ln(1+i x)+\ln(2-i x)$ allowing the integral to be separated and each new integral to be integrated along a semi-circle without running into a branch cut. However, I’m not sure about the squared or cubed case. EDIT With the help of Mathematica, I can determine $$\int_{0}^{\infty} \frac{\ln^3 (1+x^2)}{1+2x^2} \, dx = \pi\left(3\sqrt{2}\operatorname{Li}_3\left(\frac{1}{4}\left(2+\sqrt{2}\right)\right)-3\sqrt{2}\operatorname{Li}_3\left(\frac{1}{4+2\sqrt{2}}\right)-6\sqrt{2}\operatorname{Li}_2\left(\frac{1}{\sqrt{2}}\right)\ln(2)+\frac{3\operatorname{Li}_2\left(\frac{1}{2}-\frac{1}{\sqrt{2}}\right)\ln(2)}{\sqrt{2}}+3\sqrt{2}\operatorname{Li}_2\left(\frac{1}{4}\left(2+\sqrt{2}\right)\right)\ln\left(4-2\sqrt{2}\right)+\frac{3\operatorname{Li}_2\left(-2+2\sqrt{2}\right)\ln(2)}{\sqrt{2}}-3\sqrt{2}\operatorname{Li}_2\left(\frac{1}{4+2\sqrt{2}}\right)\ln(2)-3\sqrt{2}\operatorname{Li}_2\left(\frac{1}{4+2\sqrt{2}}\right)\ln\left(2+\sqrt{2}\right)+\frac{3\ln^3(2)}{8\sqrt{2}}-\frac{\ln^3\left(3-2\sqrt{2}\right)}{\sqrt{2}}-\frac{\ln^3\left(4-2\sqrt{2}\right)}{\sqrt{2}}-\sqrt{2}\ln^3\left(2-\sqrt{2}\right)-\sqrt{2}\ln^3\left(1+\sqrt{2}\right)-\sqrt{2}\ln^3\left(2+\sqrt{2}\right)-\frac{3\ln(2)\ln^2\left(3-2\sqrt{2}\right)}{\sqrt{2}}-\frac{9\ln^2(2)\ln\left(2-\sqrt{2}\right)}{4\sqrt{2}}+\frac{3\ln^2\left(4-2\sqrt{2}\right)\ln\left(2-\sqrt{2}\right)}{\sqrt{2}}+3\sqrt{2}\ln(2)\ln^2\left(2-\sqrt{2}\right)-\frac{75\ln^2(2)\ln\left(1+\sqrt{2}\right)}{4\sqrt{2}}+6\sqrt{2}\ln^2\left(2-\sqrt{2}\right)\ln\left(1+\sqrt{2}\right)-\frac{3\ln\left(\frac{577}{2}-204\sqrt{2}\right)\ln^2\left(1+\sqrt{2}\right)}{2\sqrt{2}}-\frac{3\ln(2)\ln^2\left(2+\sqrt{2}\right)}{\sqrt{2}}+3\sqrt{2}\ln(2)\ln^2\left(3+2\sqrt{2}\right)-\frac{3\ln\left(1-\frac{1}{\sqrt{2}}\right)\ln^2\left(3+2\sqrt{2}\right)}{2\sqrt{2}}+\frac{3\ln\left(1+\frac{1}{\sqrt{2}}\right)\ln^2\left(3+2\sqrt{2}\right)}{2\sqrt{2}}+\frac{3\ln^2(2)\ln\left(1393+985\sqrt{2}\right)}{2\sqrt{2}}-\frac{3\ln\left(12-8\sqrt{2}\right)\ln\left(3-2\sqrt{2}\right)\ln\left(1+\sqrt{2}\right)}{\sqrt{2}}+\frac{3\ln(2)\ln\left(1+\sqrt{2}\right)\ln\left(2+\sqrt{2}\right)}{\sqrt{2}}-\frac{3\ln\left(1+\sqrt{2}\right)\ln\left(3+2\sqrt{2}\right)\ln\left(136+96\sqrt{2}\right)}{\sqrt{2}}\right)-\frac{\pi^3\left(\ln\left(\frac{577}{8}-51\sqrt{2}\right)-2\ln\left(3-2\sqrt{2}\right)\right)}{4\sqrt{2}}$$ It may be possible to simplify this answer with polylogarithm identities, but I have not yet determined how to. As @Random Variable suggested, we can use a keyhole contour to determine $$\int_{-\infty}^{\infty} \frac{\ln^3 (1+x^2)}{1+2x^2} \, dx = \frac{\pi^3 \ln \left(3-2\sqrt{2}\right)}{\sqrt{2}} - \frac{\pi \ln^3 (2)}{\sqrt{2}} - 6 \pi \underbrace{\int_{1}^{\infty} \frac{\ln^2 (t^2-1)}{1-2t^2} \, dt}_{I}$$ A method of evaluating $I$ is as follows: $$I = \int_{0}^{1} \frac{\ln^2 (1-x)}{x^2-2} \, dx + 2\int_{0}^{1}\frac{\ln(1-x) \ln(1+x)}{x^2-2} \, dx + \int_{0}^{1}\frac{\ln^2 (1+x)}{x^2-2} \, dx + 4 \int_{0}^{1}\frac{\ln^2 (x)}{x^2-2} \, dx - 4 \int_{0}^{1} \frac{\ln (1-x^2) \ln (x)}{x^2-2} \, dx$$ for which each of these integrals, Mathematica is able to evaluate in terms of polylogarithms. Are there any further simplifications, rigorous proofs of the integrals Mathematica is able to determine, or any alternate methods at arriving at the answer?","['integration', 'analysis', 'complex-analysis', 'contour-integration', 'complex-integration']"
4374106,"Find the limiting distribution of $\frac{\sum_{i=1}^{n}(Y_i^2-\sigma Z_i^2)}{\sum_{i=1}^{n}Z_i^2}$, where $Z_i\sim N(0,1)$ and $Y_i\sim N(0,\sigma^2)$","Let $Y_1,\ldots,Y_n \overset{\text{iid}}{\sim} N(0,\sigma^2)$ , where $\sigma > 0$ , and let $Z_1,\ldots,Z_n \overset{\text{iid}}{\sim} N(0,1)$ . Define $$U_n = \frac{\sum_{i=1}^{n} (Y_i^2 - \sigma Z_i^2)}{\sum_{i=1}^{n} Z_i^2}. $$ What is the limiting distribution of $\sqrt{n} \,U_n$ as $n \rightarrow \infty$ ? My attempt: I started by writing $U_n$ as \begin{align*}
   \frac{\frac{1}{n} \sum_{i=1}^{n} (Y_i^2 - \sigma Z_i^2)}{ \frac{1}{n}\sum_{i=1}^{n} Z_i^2},
\end{align*} and then attempted to treat the numerator and denominator individally. First computing some relevant information: From the fact that the mean and variance of a chi-squared random variable with $p$ degrees of freedom are $p$ and $2p$ , respectively, we have $E(Z_i^2) = 1$ and $\text{Var}(Z_i^2) = 2$ . We also have $E(Y_i^2) = V(Y_i) + (E(Y_i))^2 = \sigma^2 + 0^2 = \sigma^2$ . Noting that $\frac{1}{\sigma}Y_i \sim N(0,1)$ , we have $(\frac{1}{\sigma} Y_i)^2 = \frac{1}{\sigma^2} Y_i^2 \sim \chi^2_1$ . Hence, $\text{Var}(\frac{1}{\sigma^2} Y_i^2) = \frac{1}{\sigma^4} \text{Var}(Y_i^2) = 2$ , and so $\text{Var}(Y_i^2) = 2\sigma^4 < \infty$ . Therefore, by the Weak Law of Large Numbers, $$\frac{1}{n} \sum_{i=1}^{n} Y_i^2 \overset{p}{\longrightarrow} \sigma^2 \text{ as } n \to \infty $$ (where $\overset{p}{\longrightarrow}$ denotes convergence in probability). Then by Slutsky's Theorem, if $\sqrt{n}\left(\frac{1}{n} \sum_{i=1}^{n} (Y_i^2 - \sigma Z_i^2)\right)$ converges to $W$ in distribution, then $U_n$ converges to $\sigma^2 W$ in distribution. I'm currently stuck on how to find the limiting distribution of the numerator. My idea is to apply the Central Limit Theorem somehow, maybe to the random variables $X_i := Y_i^2 - \sigma Z_i^2$ . I've computed that $E(X_i) = \sigma^2 - \sigma$ and $\text{Var}(X_i) = 2\sigma^4 + 2\sigma^2$ . Then by the CLT, $$ \frac{\frac{1}{n} \sum_{i=1}^{n} (Y_i^2 - \sigma Z_i^2) - (\sigma^2 - \sigma)}{\sqrt{\frac{2\sigma^4 + 2 \sigma^2}{n}} } \longrightarrow N(0,1) \text{ in distribution}.$$ But I'm not sure where to go from here...Any suggestions for how to progress?","['statistics', 'central-limit-theorem', 'probability-distributions', 'probability', 'random-variables']"
4374132,Permutations of alphabet with X following Y *and* vowels in alphabetical order,"Exercise Find the number of arrangements of the $26$ letters of the English alphabet that have both Y occurring before X all $5$ vowels (A, E, I, O, U) occurring in alphabetical order Warm-up First I count how many arrangements meet the first condition. Probably not necessary, but I am wondering if I am using the right reasoning. Such an arrangement would look like $$\begin{matrix}[\alpha \text{ of 24 letters}] & \text{Y} & [\beta \text{ of }24-\alpha\text{ letters}] & X & [\gamma \text{ of }24-\alpha-\beta\text{ letters}]\end{matrix}$$ I think the number of permutations of the letters surrounding Y and X will then be $$\sum_{\alpha=0}^{24} \alpha! \binom{24}\alpha \sum_{\beta=0}^{24-\alpha} \beta! \binom{24-\alpha}\beta \sum_{\gamma=0}^{24-\alpha-\beta} \gamma! \binom{24-\alpha-\beta}\gamma = 506\,809\,361\,586\,340\,667\,433\,417\,385$$ By extension, an arrangement in which the vowels are alphabetized might look like $$\begin{matrix}[\alpha \text{ consonants}] & \text{A} & [\beta \text{ consonants}] & \text{E} & [\gamma \text{ consonants}] & \text{I} & [\delta \text{ consonants}] & \text{O} & [\epsilon \text{ consonants}] & \text{U} & [\phi \text{ consonants}]\end{matrix}$$ with a total count of $$\sum_{\alpha=0}^{21} \alpha! \binom{21}\alpha \sum_{\beta=0}^{21-\alpha} \beta! \binom{21-\alpha}\beta \sum_{\gamma=0}^{21-\alpha-\beta} \gamma! \binom{21-\alpha-\beta}\gamma \sum_{\delta=0}^{21-\alpha-\beta-\gamma} \delta! \binom{21-\alpha-\beta-\gamma}\delta \sum_{\epsilon=0}^{21-\alpha-\beta-\gamma-\delta} \epsilon! \binom{21-\alpha-\beta-\gamma-\delta}\epsilon \sum_{\phi=0}^{21-\alpha-\beta-\gamma-\delta-\epsilon} \phi! \binom{21-\alpha-\beta-\gamma-\delta-\epsilon}\phi = 11\,369\,881\,057\,625\,718\,121\,105$$ Am I counting these individual arrangements correctly? Is there a more elegant way of counting them? Possible solution In the sum for the second count above, replace $21$ with $19$ . Then multiply the resulting sum by the number of ways one can tack on Y and X to the arrangement. For a given alphabetical-vowel arrangement, Y can be placed in $25$ different positions. If we place Y at the $k$ -th position $(1\le k\le25)$ , then that leaves $26-k$ possible positions for X. The total number of ways to do this would be $$25 \sum_{k=1}^{25} (26-k) = 8\,125$$ Then the total number of arrangements satisfying both criteria is $$92\,380\,283\,593\,208\,959\,733\,978\,125$$ Is this count correct?",['combinatorics']
4374164,Brownian motion and stopping time,"Let $(W_r)_r$ be a Brownian motion and $\alpha>0.$ Let $\theta_{\alpha}=\inf\{ r \geq 0,W_r=r\alpha+\alpha \}$ and $U_{\alpha}=\inf \{r \geq 0,W_r \geq r\alpha+\alpha\}.$ I want to know (prove rigorously) that $W_{\theta_\alpha \wedge r} \leq \alpha+\alpha(\theta_\alpha \wedge r)$ and why on the event $\{\theta_\alpha<\infty\},$ we have $W_{\theta_{\alpha}}=\theta_{\alpha}\alpha+\alpha.$ These two properties, are they true for $U_{\alpha}?$","['stochastic-processes', 'stopping-times', 'brownian-motion', 'probability-theory']"
4374209,Why doesn't the gamma distribution have the memoryless property?,"The gamma distribution essentially tells us the probability of $k$ events happening in a given amount of time, $t$ . It seems to me that there are certain examples of the gamma distribution where it behaves memoryless. For example, the probability of 2 customers entering a store in 3 hours GIVEN that no customer has entered in the first hour is equivalent to the probability of 2 customers entering a store in 2 hours. Is my example properly demonstrating ""memoryless-ness""? What would be an example of it having memory using my store scenario?","['statistics', 'probability-distributions', 'gamma-distribution']"
4374215,"Why is the probability of drawing two cards, one of a specific suit and one of a specific rank equal to the probability of drawing a specific card?",(Suppose the specific suit is Diamond and the specific rank is Queen.) Suppose we draw two cards without replacement. We are asked to calculate the probability that the first one is a Queen and the second one is a Diamond. To calculate this there are two cases: First card is Queen of Diamond. First card is a Queen but not a Diamond. In the first case the probability of drawing first card Queen of Diamond then second card a Diamond is $$\frac{1}{52} \cdot \frac{12}{51}.$$ In the second case the probability of drawing first card a Queen but not a Diamond then second card a Diamond is $$\frac{3}{52} \cdot \frac{13}{51}.$$ Add them up and we get $$\frac{1}{52}.$$ This is equal to drawing just one card of the Queen of Diamond. Why is that?,"['card-games', 'probability']"
4374255,Proof that integration is reverse-differentiation,"The following section is the start of a proof to show that integration is the inverse of differentiation: Consider an integral $F(x) = \int_{b}^{a} f(x) \ dx$ . Let $F(x) = \int_{a}^{x} f(u) \ du $ . It is apparent that $F(x)$ is a continuous function of $x$ and $F(x+h) = \int_{a}^{x+h} f(u) \ du = \int_{a}^{x} f(u) \ du + \int_{x}^{x+h} f(u) \ du = F(x) + \int_{x}^{x+h} f(u) \ du$ Rearranging and dividing through we get $\frac{F(x+h) - F(x)}{h} = \frac{1}{h}\int_{x}^{x+h} f(u) \ du$ .
Finally, taking the limit as $h \rightarrow 0$ the LHS becomes $\frac{dF}{dx}$ and the RHS becomes $f(x)$ . I have several questions about this proof I don't understand: From line one to line two, for what substitution $u$ where $u$ is some function of $x$ is this true? The transformation from [1] to [2] is not obvious to me. How is it apparent $F(x)$ is continuous (intuitively, only the case if $f(x)$ is continuous)? The final line appears to me to hand-wave away the limit as $h \rightarrow 0$ of $\frac{1}{h}$ which should be undefined to my knowledge. In any case, it seems invalid unless $\lim_{h\rightarrow 0} \frac{1}{h} = 1$ which I find unlikely. Thanks!","['limits', 'calculus']"
4374277,"Show that the Lorentz boosts, $\begin{pmatrix}\gamma & -\beta\gamma\\ -\beta\gamma & \gamma\end{pmatrix}$ form a one-parameter Lie group.","In the following post, I am questioning certain parts of the author's solution for the beginning of a proof that the Lorentz boosts form a one-parameter Lie group: Show that the Lorentz boosts, \begin{pmatrix}\gamma & -\beta\gamma\\  -\beta\gamma & \gamma\end{pmatrix} form a one-parameter Lie group. Is this group Abelian? Here is the author's solution: The Lorentz boosts, $\Lambda$ , are defined by $$\Lambda(\beta)=\begin{pmatrix}\gamma & -\beta\gamma\\  -\beta\gamma & \gamma\end{pmatrix}=\gamma(\beta)\begin{pmatrix}1 & -\beta\\  -\beta & 1\end{pmatrix}$$ where $\gamma(\beta)=\frac{1}{\sqrt{1-{\beta}^2}},\quad \beta=\frac{v}{c}, \quad$ and $c$ is the speed of light, so $\lvert v \rvert\lt c$ . To show that these matrices form a Lie group, we must demonstrate that these matrices under matrix multiplication satisfy the group axioms. Closure . The product of two boosts $\Lambda(\beta )$ and $\Lambda({\beta}^{\prime})$ is $$\begin{align}\Lambda(\beta )\Lambda({\beta}^{\prime})&=\gamma({\beta})\gamma({\beta}^{\prime})\begin{pmatrix}1 & -\beta\\  -\beta & 1\end{pmatrix}\begin{pmatrix}1 & -{\beta}^\prime\\  -{\beta}^\prime & 1\end{pmatrix}\\&=\gamma({\beta})\gamma({\beta}^{\prime})\begin{pmatrix}1+\beta{\beta}^\prime & -\beta-{\beta}^\prime\\  -\beta-{\beta}^\prime & 1+\beta{\beta}^\prime\end{pmatrix}\end{align}$$ For this product to have the form $$\Lambda(\beta^{\prime\prime})=\gamma(\beta^{\prime\prime})\begin{pmatrix}1 & -{\beta}^{\prime\prime}\\  -{\beta}^{\prime\prime} & 1\end{pmatrix}\tag{1}$$ we must require that $$\color{red}{\gamma(\beta^{\prime\prime})=\gamma(\beta)\gamma({\beta}^\prime)\left(1+\beta{\beta}^\prime\right)}\tag{2}$$ $${\begin{align}\color{red}{{\beta}^{\prime\prime}\gamma({\beta}^{\prime\prime})=\gamma(\beta)\gamma({\beta}^\prime)\left(\beta+{\beta}^\prime\right)}\tag{3}&\\=[\cdots]\end{align}}$$ I would like to understand the reasoning behind the need for the red equations, $(2)$ and $(3)$ . Taking equation $(2)$ for now, and plugging it back into $(1)$ , $$\begin{align}\Lambda(\beta^{\prime\prime})&=\gamma(\beta^{\prime\prime})\begin{pmatrix}1 & -{\beta}^{\prime\prime}\\  -{\beta}^{\prime\prime} & 1\end{pmatrix}\\&=\gamma(\beta)\gamma({\beta}^\prime)\left(1+\beta{\beta}^\prime\right)\begin{pmatrix}1 & -{\beta}^{\prime\prime}\\  -{\beta}^{\prime\prime} & 1\end{pmatrix}\\&=\gamma(\beta)\gamma({\beta}^\prime)\begin{pmatrix}1 +\beta{\beta}^\prime & -\left(1 +\beta{\beta}^\prime\right){\beta}^{\prime\prime}\\  -\left(1 +\beta{\beta}^\prime\right){\beta}^{\prime\prime} & 1 +\beta{\beta}^\prime\end{pmatrix}\\&=\gamma(\beta)\gamma({\beta}^\prime)\left(1+\beta{\beta}^\prime\right)\begin{pmatrix}1 & -{\beta}^{\prime\prime}\\  -{\beta}^{\prime\prime} & 1\end{pmatrix}\end{align}$$ So I see why the factor of $\color{red}{\left(1+\beta{\beta}^\prime\right)}$ was needed in equation $(2)$ , but there was no way I could intuit the need for this $\color{red}{\left(1+\beta{\beta}^\prime\right)}$ factor without direct substitution as I did above. Is there some deeper technique going on here that eludes me, like matrix diagonalization? The other equation (in red) I am questioning is $(3)$ and I really don't understand what the author is trying to do with this. So, if anyone has any insights or thought's on why this is being done please let me know. N.B .
There is more to equation $(3)$ than I have written above, but I didn't want to ask too many questions at once, so for now I am just questioning the need for $\color{red}{{\beta}^{\prime\prime}\gamma({\beta}^{\prime\prime})=\gamma(\beta)\gamma({\beta}^\prime)\left(\beta+{\beta}^\prime\right)}$ .","['representation-theory', 'matrices', 'abstract-algebra', 'group-theory', 'lie-groups']"
4374280,Normal numbers have a measure of 1,"So, I have a problem with the following proof from my course notes: ""Let us say a number $x \in [0, 1]$ is (weakly) normal if any finite sequence of digits occurs infinitely often in its decimal expansion. We claim the set E of all such numbers has $m(E) = 1$ . To see this, let $E_i$ be the set of $x \in [0, 1]$ such that the $i$ -th digit of $x$ is $1$ . Then
the $E_i$ are independent, and $m(E_i) = 1/10$ , so $\sum_i E_i = \infty$ . Thus the digit 1 occurs infinitely often for almost every $x \in [0, 1]$ . The same reasoning applies to any finite sequence of digits. Intersecting these countably many sets of measure one again yields a set of full measure."" I understand everything in this proof except for the fact that $E_i$ and $E_j$ for $i \neq j$ are indeed independent (which was defined as $m(E_i \cap E_j) = m(E_i) m(E_j)$ ). Intuitively, it is very clear, however, I don't know how I could prove that rigorously. I see that $E_i$ is measurable as a countable union of intervals, and I also intuitively understand that the measure of $E_i \cap E_j$ should be equal to the measure of $E_i \cap F_j$ (where $F_j$ represents the set of numbers with, say, 2 at the $j$ -th place after comma), since we have a bijective mapping between intervals which unions constitute $E_j$ and $F_j$ , however, I don't know how to make this rigorous. Could someone please help me with that?","['number-theory', 'measure-theory', 'probability-theory', 'real-analysis']"
4374304,"Please tell me what the author wants to say. (""Topics in Algebra 2nd Edition"" by I N. Herstein)","I am reading ""Topics in Algebra 2nd Edition"" by I. N. Herstein. The following sentences are in this book. I cannot understand what the author wants to say: Let $G$ be a cyclic group of order $7$ , that is, $G$ consists of all $a^i$ , where we assume $a^7=e$ . The mapping $\phi:a^i\to a^{2i}$ , as can be ckecked trivially, is an automorphism of $G$ of order $3$ , that is $\phi^3=I$ . Let $x$ be a symbol which we formally subject to the following conditions: $x^3=e,x^{-1}a^ix=\phi(a^i)=a^{2i}$ , and consider all formal symbols $x^ia^j$ , where $i=0,1,2$ and $j=0,1,2,\dots,6$ . We declare that $x^ia^j=x^ka^l$ if and only if $i\equiv k\mod 3$ and $j\equiv l\mod 7$ . We multiply these symbols using the rules $x^3=a^7=e,x^{-1}ax=a^2$ . For instance $(xa)(xa^2)=x(ax)a^2=x(xa^2)a^2=x^2a^4$ . The reader can verify that one obtains, in this way, a non-abelian group of order $21$ . What is the symbol $x$ ? From which algebraic system did this symbol $x$ come? What is the product of $x$ and $a$ ? Please tell me what the author wants to say.","['automorphism-group', 'group-theory', 'abstract-algebra', 'cyclic-groups']"
4374326,Number of arrangement of the word JANUARY such that N is before Y and no two vowels are next to eachother,"My attempt to the question is no vowel next to eachother: 4!/2!*(6P3)/2!.
How do I find that N is left to Y?",['discrete-mathematics']
4374350,How do I solve for y in $y=\text{tanh}(\frac{x}{y})$?,I want to solve the following equation as a function of purely $x$ : $$y=\text{tanh}\left(\frac{x}{y}\right)$$ My best guess up to this point has been to rearrange the equation using inverse hyperbolic tangent $$x = y\text{tanh}^{-1}(y)$$ and plugging in the definition for hyperbolic tangent from Wolfram Mathworld $$\text{tanh}^{-1}(z) = \frac{1}{2}\left[\text{ln}(1+z)-\text{ln}(1-z)\right]$$ Which I've taken as far as $$e^{2x}=\left(\frac{1+y}{1-y}\right)^y$$ But seems like a dead end. Any insight into this problem is super appreciated!,"['algebra-precalculus', 'hyperbolic-functions']"
4374351,Pigeon hole problem unknown pigeons numbers,"I am trying to solve an exercise but I am not sure if I handle it right. let's suppose we have 40 letters of the alphabet. The 20 of those are
with small letters and 20 are with capital.What are the minimum numbers
of the letters that have a text with capital letters, so that the text
to contain at least 10 same letters? what I have tried:
The 20 smaller letters can be ignored because we only want to learn about 20  capital letters.The text is in capital so we only care about 20. Now I have lets call it a variable n.The variable n represents the pigeons(which is the text),the text is unknown the length it has.What we know is ,that we care about pigeonholes that are 20.We need to place the n into 20 so that we will take 10 at least. what I think is $n/20$ =10 that means n= 20*10 =200.
Am I right?","['pigeonhole-principle', 'discrete-mathematics']"
4374413,Big intersection operation,"According to ""Elements of Set Theory"" 30p, $$\emptyset \neq A \subseteq B \;\,\Rightarrow \;\, \bigcap B \subseteq \bigcap A.$$ In each case, the proof is straightforward. For example, in the last case, we assume that every member of $A$ is also a member of $B$ . Hence if $x \in \bigcap B$ , i.e., if $x$ belongs to every member of $B$ , then a fortiori $x$ belongs to every member of the smaller collection $A$ . And consequently $x \in \bigcap A$ . I wonder if the last part is really a proof. It is okay to assume $x \in \bigcap B$ and arrive at $x \in \bigcap A$ . But can I use this reasoning ""then a fortiori $x$ belongs to every member of the smaller collection"" in a proof exercise? It looks like just using intuition (it does not seem rigorous).","['elementary-set-theory', 'proof-explanation', 'discrete-mathematics']"
4374505,How do I show that the following map is surjective?,"Helly I have the following problem. We have $f:T\rightarrow S$ a map an $\mathfrak{R}$ a relation on $S$ . We define a relation $\mathfrak{P}$ with $$x~\mathfrak{P}~y \Leftrightarrow f(x)~\mathfrak{R}~f(y)$$ Now we assume that $\mathfrak{R}$ is an equivalence relation and we define $$g:T/\mathfrak{P}\rightarrow S/\mathfrak{R};~~~~~T\supset B\mapsto A\subset S~~~s.t.~~~f(B)\subset A$$ I just know that $g$ is always injective and I know want to show that if $f$ is surjective then $g$ is bijective. So my Idea was the following. I remarked that it is enough to show that if $f$ is surjective then $g$ is surjective. I wanted to assume that $g$ is not surjective then $$\exists A\in S/\mathfrak{R}~~~s.t.~~~\forall B\in T/\mathfrak{P}~~~g(B)\neq A \Rightarrow f(B)\not\subset A$$ In my opinion this shows that $f$ is not surjective and thus the claim follows by contraposition, but I'm not really sure if this is correct so? Could someone take a look and help me? Thank you a lot.","['equivalence-relations', 'functions', 'linear-algebra']"
4374518,Exponential growth of a cow farm with constraints in Minecraft,"This question is distinct from Exponential growth of cow populations in Minecraft in that an important constraint present in Minecraft is missing from that post. Here are the following constraints: We start with $2$ adult cows, and $0$ baby cows. Any given pair of adult cows can only be bred to form $1$ baby cow every $5$ minutes. ( not included in the other post ) A baby cow becomes an adult after $20$ minutes. My first (failed) attempt at solving this was to intialise the following functions $A(0)=2, B(0)=0$ , where $A$ and $B$ are funtions of $t$ (time, in minutes) representing the number of Adult and Baby cows respectively. Naively, I represented this via a system of two linear differential equations given the above initial conditions: $$\frac{dA}{dt}=\frac{B}{20}$$ $$\frac{dB}{dt}=\frac{A}{5}$$ Where my thought process was that every 20 minutes there would be an addition of $B$ adults, and every 5 minutes there would be an addition of $\frac{A}{2}$ baby cows. By use of linear algebra, it can be shown that the equations of $A$ and $B$ are as follows: $$A(t)=e^{-\frac{t}{10\sqrt{2}}}+e^{\frac{t}{10\sqrt{2}}}$$ $$B(t)=\frac{1}{\sqrt{2}}e^{\frac{-t}{10\sqrt{2}}}\left(e^{\frac{t}{5\sqrt{2}}}-1\right)$$ Where the total number of cows, call this $C(t)$ is some rounded form of $A(t)+B(t)$ . The problem with this: Given that at any given time, the 'age' of the babies within the pool will be different (non-uniform), the formation of adults will be staggered. This is the point I'm struggling to continue. I've tried to think of $B$ as forming subsequences every $5$ minutes, but I am really unsure how to implement this in any meaningful way. So how can this formula be generated? Edit : On second thoughts, this might be reducible to the post linked at the start. Not too sure though. Edit 2 : As pointed out in the comments by Daniel Mathias, this question is distinct from the linked post.","['ordinary-differential-equations', 'recurrence-relations', 'linear-algebra', 'recreational-mathematics', 'exponential-function']"
4374535,"How are we supposed to show existence of a delta function on a linear subspace of $C[0,1]$ without reference to the Riesz representation?","Royden leaves the following as an exercise: Let $X$ be a linear subspace of $C[0,1]$ that is closed as a subset of $L^2[0,1]$ . $X$ is closed, and there is a constant $M$ such that $\|f\|_\infty\le M\|f\|_2,\,\|f\|_2\le\|f\|_\infty$ for all $f\in X$ . Show that for all $y\in[0,1]$ , there is a function $k_y\in L^2[0,1]$ with $f(y)=\int_0^1k_y(x)f(x)\,\mathrm{d}x$ for all $f\in X$ . This comes after a chapter on basic linear operator theory, with theorems like the open mapping and closed graph theorems covered, and some lemmas on when we can know if an operator is continuous/open, and also some theorems on the isomorphy of finite dimensional linear spaces with $\Bbb R^n$ . I have studied more measure theory than is covered thus far in Royden's book, and I have seen the proof of the Riesz representation theorem and I can tell you that since $T_y\in L^2[0,1]^*$ , $T_y:f\mapsto f(y)$ is a continuous linear functional on the subset $X$ (by extreme value theorem), it must have the representation $k_y\in L^2[0,1]$ as the $L^p$ -conjugate of $2$ is again $2$ - I hope I'm using this theorem right. So there is an immediate and rather too powerful solution. Royden does not cover this theorem until much later in the book I believe, yet he expects students to find, or show the existence of, such a delta-esque function $k_y$ using basic linear operator theory. What was the solution he had in mind? I'd be happy with any linear theoretic solution really, since knowing exactly what Royden intended is hard! I just expect one can get away with arguments weaker than the Riesz Representation theorem.","['measure-theory', 'riesz-representation-theorem', 'operator-theory', 'real-analysis', 'linear-transformations']"
4374579,"Check: $\sum_{n=1}^{\infty}a_n$ converges, $\sum_{n=1}^{\infty}a^2_n$ diverges, then $\sum_{n=1}^{\infty}|a_n|$ diverges","Please do check my proof, and in case, tell me how to make it more professional. Additionally I'm not sure if the statement ( $A$ if $B$ ) means $A\implies B$ or $B\implies A$ . Problem: Let $\sum_{n=1}^{\infty}a_n$ be a convergent series. Show that $\sum_{n=1}^{\infty}|a_n|$ diverges if $\sum_{n=1}^{\infty}a^2_n$ diverges. My Solution: Since $\sum_{n=1}^{\infty}a_n$ converges, it must be true that $\lim_{n\to \infty}a_n=0.$ Then for some $n_0$ : for all $n>n_0$ , $|a_n|<1$ . Then it is true that $a^2_n<|a_n|$ for all $n>n_0$ . Hence $\sum_{n=n_0+1}^{\infty}a_n^2 < \sum_{n=n_0+1}^{\infty}|a_n| \implies \sum_{n=n_0+1}^{\infty}|a_n|$ diverges.","['solution-verification', 'sequences-and-series']"
4374611,Limit $\lim_{n\to \infty} n\cdot \sin(\frac{n}{n+1}\pi) = \pi$,"Problem: $\lim_{n\to \infty} n\cdot \sin(\frac{n}{n+1}\pi) = \pi$ So far I have: I think that $\lim_{n\to \infty}\frac{ \sin(\frac{n}{n+1}\pi)}{\frac{n}{n+1}\pi} = 1$ , but I'm not sure. It would be similar to $\lim_{x\to 0}\frac{\sin(x)}{x}=1$ , but that doesn't make a load of sense to me after trying to finish it.","['limits', 'trigonometry']"
4374616,"Must a monotone, differentiable function $[a, b] \rightarrow \mathbb{R}$ be absolutely continuous?","If we assume that the derivative is continuous the answer is clearly yes. I suspect however that now the answer is no. This question arose when I wanted to investigate for which non-decreasing, right-continuous functions $g: \mathbb{R} \rightarrow \mathbb{R}$ the Lebesgue-Stjeltes measure associated to $g$ , $\mu_g$ , is given by $\mu_g(A)=\int_A g' d\mu$ for all Borel sets $A$ (1). If such a $g$ is automatically absolutely continuous then the second theorem of calculus for Lebesgue integration would imply that $\int_{[a,b]}g'(x)d\mu(x)=g(b)-g(a)$ and the uniqueness in Caratheodory's theorem would imply (1).","['measure-theory', 'absolute-continuity', 'real-analysis']"
4374624,What are some face and vertex transitive polyhedra that are not edge transitive?,"A way I know to define the Platonic solids is that they are the only convex polyhedra that are edge, face, and vertex transitive. If we retain only the vertex transitivity, one finds a new family of solids, the 13 (14?) Archimedean solids + the infinite series of prisms and antiprisms. Similarly, retaining only face transitivity, one finds the 13 Catalan solids  and the infinite series of bipyramids and trapezohedra. These solids are the duals of the vertex transitive solids. 2 Catalan solids and 2 Archimedean solids are also edge transitive. But the definition of the Platonic solids as edge, face and vertex transitive seems to imply that there are face+vertex transitive solids that are not edge transitive. Something in between Catalan and Archimedean. If that is not the case, face and vertex transitivity would suffice to define the Platonic solids. Which are these face & vertex transitive solids? (excluding the Platonic solids)","['polyhedra', 'geometry']"
4374668,Bounding the norms of the powers of a $2\times 2$ matrix,"Let $\|.\|_2$ denote the matrix norm induced from the Euclidian vectornorm and let \begin{align}
M=\left(\begin{array}{cc}
a+b & -b \\
1 & 0
\end{array}\right).
\end{align} I need to bound $\|M^n\|_2$ for any $n\in \mathbb{N}$ . It is assumed that $a\in[0,1]$ and $b\in(0,1)$ . I tried to use Gelfand's formula from which we obtain: $$\|M^n\|_2\leq (\rho(M)+o(1))^n,$$ where $\rho(M)$ denotes the spectral radius of $M$ . It is easy to see why $\rho(M)\leq \max\left\{1-\frac{1-a}{2},b\right\}$ . Therefore $\|M^n\|_2<c$ for any n, if $\, a\leq q<1$ .
In the case $a=1$ we see that: \begin{align*}
\|M^n\|_2=\left\|\frac{1}{b-1}\left(\begin{array}{cc}
b^{n+1}-1 & -b-b^{n+1} \\
b^{n}-1 & -b-b^{n}
\end{array}\right)\right\|_2\leq c.
\end{align*} But I have no idea how to bound $\|M^n\|_2$ when $a \rightarrow 1$ , since I haven't found any results on the convergence of Gelfand's formula. Any suggestions how to prove this?
Also according to many plots, it should hold that \begin{align*}
\|M^n\|_2\leq\left\|\frac{1}{b-1}\left(\begin{array}{cc}
b^{n+1}-1 & -b-b^{n+1} \\
b^{n}-1 & -b-b^{n}
\end{array}\right)\right\|_2\leq c,
\end{align*} for any $a\in[0,1]$ . Any idea how to prove the last inequality?","['spectral-radius', 'matrix-norms', 'linear-algebra', 'functional-analysis', 'sequences-and-series']"
4374669,The set is meager if it has a cover of clopen meager sets,"Let $X$ be a topological space such that there exists a collection of meager clopen sets $(C_i)_{i \in I}$ such that $X = \bigcup_{i \in I} C_i$ . I want to prove that $X$ is then meager itself. As each $C_i$ is meager there is a sequence of nowhere dense sets $(N_{i,n})^\infty_{n=1}$ such that $C_i = \bigcup^\infty_{n=1} N_{i,n}$ . For natural $n$ define set $M_n = \bigcup_{i\in I} N_{i,n}$ . Then by basic set theory $$
X = \bigcup_{i \in I} C_i = \bigcup_{i \in I} \bigcup_{n=1}^\infty N_{i,n} = 
 \bigcup_{n=1}^\infty \bigcup_{i \in I} N_{i,n} = \bigcup_{n=1}^\infty  M_n.
$$ So, we need to show that each $M_n$ is meager and we are done. Initaly I thought that it may be possible to use clopennes to write something like $$
\mathrm{int}\;\mathrm{cl}\;M_n = \mathrm{int}\;\mathrm{cl}\;\bigcup_{i \in I} N_{i,n} =
\bigcup_{i \in I} \mathrm{int}\;\mathrm{cl}\; N_{i,n} = \bigcup_{i \in I} \emptyset =\emptyset,
$$ where $\mathrm{int}$ and $\mathrm{cl}$ stand for interior and closure operations respectively, so $M_n$ is nowhere dense. But this works only if $(C_i)_{i \in I}$ are disjoint. This won't work in general, for example take $(C_i)_{i \in I}$ to be a collection of all clopen sets in $X=\mathbb{Q}$ , and choose $N_{i,n}$ to be a collection of singletons so each $M_n = \mathbb{Q}$ . I know that in this example sets $X$ is meager, but I wanted to show that argument above may fail in general. My first idea was to transform $(C_i)_{i\in I}$ into a different disjoint clopen cover $(C'_j)_{j \in J}$ , such that for each $j\in J$ there is $i\in I$ with $C'_j\subset C_i$ . But how to do so? In case $X$ is locally connected one can just take its connected components, but this won't work in general. I thought that a transfinite induction may also work. By abuse of notation assume that $I$ is an ordinal, let $\mathcal{C}_{1}$ be an well-order by $I$ version of $(C_i)_{i\in I}$ . then for a non-limit ordinal $\kappa + 1$ we may construct a Set $\mathcal{C}_{\kappa + 1} = \{ C^\kappa_{1}, \ldots, C^\kappa_\kappa \} \cup \{ C^\kappa_{\kappa +1} \setminus C^\kappa_\kappa , \ldots,  C^\kappa_{I} \setminus C^\kappa_\kappa   \}$ , which is an ordered set of clopen sets, where each set in the first brackets is disjoint from any other set in $\mathcal{C}_{\kappa + 1}$ . And for the limit ordinals $\lambda$ just write $\mathcal{C}_\lambda = \{ C^i_i | i < \lambda\} \cup \left\{ \bigcap_{i < \lambda} C^i_j |\lambda \le j \le I \right\}$ ? I don't think this will work, as clopeness may be lost. But in the end I hoped $\mathcal{C}_I$ to be a disjoint cover I seek. Please, help me to prove this statement.","['general-topology', 'transfinite-recursion', 'baire-category']"
4374680,Algebraic Geometry problem with Nullstellensatz,"I am right now battling with algebraic geometry and can't wrap my head around one problem. Problem: Assume that the field $K$ is the field of complex numbers. Let $V \in \mathbb{A}^n_K$ be an affine variety, let $f \in K[V]$ . Suppose that for all $P \in V$ we have that $f(P)\neq 0$ . If $x_1, ..., x_n$ denote the coordinate functions on $V$ , prove that there exists a unique $g \in K[V]$ such that there is an equality $$(f \cdot g) = \sum^n_{m = 1} (-1)^m \cdot \frac{m!}{(2m)!}\cdot x_m$$ in $K[V]$ . You should use the Nullstellensatz in your solution. Unfortunately, I have no idea even where to start with this and would appreciate any help. Thank you.",['algebraic-geometry']
4374717,"Evaluating $\lim_{(x,y)\to(0,0)}\dfrac{x^2+y^2}{x^4+y^4}$ [duplicate]","This question already has answers here : Limit $(x,y) \to (0,0)$ (2 answers) Closed 2 years ago . Evaluate the limit: $\displaystyle\lim_{(x,y)\to(0,0)}\dfrac{x^2+y^2}{x^4+y^4}$ To solve this, I converted it to polar coordinate and got: $\displaystyle\lim _{r\to0}\left(\frac{1}{r^2(\sin^4\theta+cos^4\theta)}\right)=\infty$ But after putting this on WolframAlpha, it tells me that this limit does not exist.
Who is wrong here?","['limits', 'multivariable-calculus']"
4374723,"How many $10$ character words formed from the alphabet $\{a, b, c\}$ have exactly six $a$s?","You have $\{a,b,c\}$ as your character set.
You need to create words having $10$ characters that must be chosen from the character set. Out of all the possible arrangements, how many words have exactly $6$ $a$ s? My logic:
We need $6$ $a$ s in our words. The rest $4$ words must come from $\{b,c\}$ . Final answer $= \dfrac{2C1 \cdot 4 \cdot 10!}{6!}$ The $6!$ comes from the $6$ duplicate $a$ s in our words.
Is my logic fine?","['combinatorics', 'discrete-mathematics']"
4374758,Transitive Lie group action on the Hantzsche-Wendt Manifold,"Does there exists a smooth transitive action of a (finite dimensional) Lie group $ G $ on the Hantzsche-Wendt manifold? In other words, does there exists a Lie group $ G $ and a closed subgroup $ H $ such that $ G/H $ is diffeomorphic to the Hantzsche-Wendt manifold? If such a transitive actions exists my guess is that the group $ G $ is the Euclidean group $ E_3 $ or some subgroup of $ E_3 $ . Note that $ G $ must be noncompact as all three manifolds with transitive action by a compact group are already given here https://math.stackexchange.com/a/4364430/758507 Also the group must be at least dimension 4 since all manifolds which are the quotient of a three dimensional Lie group by a cocompact lattice are given here https://www.sciencedirect.com/science/article/pii/0166864181900183 Some background: The Hantzsche-Wendt manifold is a compact connected flat orientable 3 manifold. Like all compact flat manifolds it is normally covered by a torus, in this case $ T^3 $ . And  moreover (like all flat manifolds) it is aspherical. So it is determined by its fundamental group which is presented in https://arxiv.org/abs/math/0311476 as $$
\pi_1(M) \cong <X,Y:X=Y^2XY^2,Y=X^2YX^2>
$$ where $ X,Y,Z=(XY)^{-1} $ are the generating screw motions which square to the translations $ t_1= X^2, t_2=Y^2,t_3=Z^2 $ given in Wolf theorem 3.5.5. Since $ M $ is compact and flat $ \pi_1 $ is a Bieberbach group, indeed it fits into the short exact sequence $$
1 \to \mathbb{Z}^3 \to \pi_1(M) \to C_2 \times C_2 \to 1
$$ so $ M $ has holonomy $ C_2 \times C_2 $ . Abelianizing $ \pi_1 $ we can see that
the first homology is $$
H_1(M,\mathbb{Z})\cong C_4 \times C_4 
$$ From the short exact sequence above we can see that $ \pi_1 $ is virtually abelian and solvable.","['riemannian-geometry', 'homogeneous-spaces', 'differential-topology', 'lie-groups', 'differential-geometry']"
4374767,Show that $\frac{d}{dx}\tan x=\sec^{2}x$ (by using infinitesimal approach to the derivative),"I couldn't prove the derivative of $\tan x$ by using the infinitesimal approach to the derivatives. Here's my solution, but I stuck at the last step: \begin{align}
\tag{1}\frac{\mathrm{d} }{\mathrm{d} x}\tan x &= \frac{\tan (x+dx)- \tan x}{dx} \\
\tag{2} &=\frac{\frac{\tan x + \tan dx}{1-\tan x . \tan dx}-\tan x}{dx}\\
\tag{3}&=\frac{\frac{\tan x + dx - \tan x + dx\tan^2 x}{1 - dx\tan x}}{dx}\\
\tag{4} &=\frac{dx(1+\tan^2 x)}{1 - dx\tan x} \cdot \frac{1}{dx}=\frac{1+ \tan^2 x}{1-dx\tan x}\\
\tag{5} &=\frac{\sec^2 x}{1-dx\tan x}\neq \sec^2 x. 
\end{align}","['real-analysis', 'calculus', 'trigonometry', 'derivatives', 'infinitesimals']"
4374789,Determining the Lie algebra $\frak g$ to a given Lie group $G$,"In an old exercise sheet there was the following Lie group: $$G = \left\{ \begin{pmatrix} a & b \\ 0 & a^{-1}\end{pmatrix} \Big|\ a> 0\ \text{and}\ b\in \mathbb R\right\}$$ and in order to see whether i understood things correctly, I wanted to determine its corresponding Lie algebra $\frak g$ . However, I don't exactly know how to approach this problem. What I've tried: My first naive approach was to consider $G$ as a Lie subgroup of $\operatorname{GL}_2(\mathbb R)$ and use the one-parameter subgroup $$\gamma(t) = e^{tA}$$ since $$A \in {\frak{g}} \iff e^{tA} \in G\quad \forall t\in \mathbb R$$ However, I'm not sure how to proceed from here since, unlike in the examples of the common Lie subgroups $O(n), \operatorname{SO}(n)$ or $\operatorname{GL}_n(\mathbb R)$ , I don't see a straight forward property of elements of $G$ that would be of help in order to use the exponential map accordingly. My second thought was choosing a one-parameter subgroup $$\gamma\colon \mathbb R\to G$$ s.t. $\gamma(0) = E$ , one possible one-parameter subgroup would be $$\gamma(t) = \begin{pmatrix} 1+t & 0 \\ 0 & (1+t)^{-1}\end{pmatrix}$$ Now clearly $\gamma(0) = E$ . But now I'm not entirely sure. I thought about considering $\gamma'(0)$ in order to obtain a tangent vector at $E$ and then determine the unique corresponding left-invariant vector field $\widetilde X$ via $$\widetilde X(H) = dL_H(\gamma'(0))$$ where $H$ denotes a fixed but arbitrary element of $G$ and $dL_H$ is the push-forward (the differential) of the left-translation map $$L_H\colon G\to G,\ A\mapsto HA$$ My question: Would my second attempt somehow be reasonable? Or do I confuse some things? Additionally, I'd like to ask whether someone could give me a hint regarding my first approach (embedding $G\hookrightarrow \operatorname{GL_2}(\mathbb R)$ ) Thanks for any help.","['lie-algebras', 'vector-fields', 'smooth-manifolds', 'lie-groups', 'differential-geometry']"
4374804,How to evaluate $\int_0^\infty\frac{ \sin{x}}{x^2+1}dx$?,"For some reason I couldn’t find an answer to this online even though it seems very basic. I am trying to evaluate the following improper integral. $ \int_0^{\infty} \frac{ \sin{x}}{x^2+1}dx$ . I consider the following function on nonnegative reals. $I(a) = \int_0^{\infty} \frac{ \sin{ax}}{x^2+1}dx$ . Differentiating under the integral sign I get $I'(a) = a\int_0^{\infty} \frac{ \cos{ax}}{x^2+1}dx$ . The rhs I evaluate with a contour integral to get $I'(a) = \frac{\pi}{2}ae^{-a}$ . With the boundary condition $I(0)=0$ , I get $I(a) = \frac{\pi}{2}(1-e^{-a} -ae^{-a} )$ . $a=1$ corresponds to the integral I want to evaluate, which is equal to $$\frac{\pi}{2} - \frac{\pi}{e}$$ This doesn’t quite match the answer I am getting on [Wolfram Alpha][1].
Can someone please help me find out where I am going wrong? Thanks.","['integration', 'contour-integration', 'improper-integrals']"
4374814,Limit for $t\to+\infty$ of a solution to an ODE,"Consider the Cauchy problem: \begin{cases}
y'(t)=\frac{t^2}{2+\mathrm{sin}(t^2)}cos(y(t)^2) \\
y(0)=0
\end{cases} I have to prove that $\mathrm{lim}_{t\to+\infty} y(t)=\sqrt{\pi/2}$ . I have tried to solve it by separating the variables $t$ and $y$ :  dividing by $\cos(y(t)^2)$ both sides of the equation and integrating, one obtains: $$
\int^{y(t)} _0 \frac{1}{\cos(u^2)}\mathrm{d}u=\int^{t} _0 \frac{s^2}{2+\sin(s^2)}\mathrm{d}s, 
$$ but it does not look very useful. Does anyone know how to solve it?","['indefinite-integrals', 'ordinary-differential-equations', 'real-analysis']"
4374821,How to use Calculus of Variation with Lagrange Multipliers when the marginal conditions only depend on the multipliers?,"I am trying to solve an optimization problem: $$\begin{aligned}&\max_{G(v)}\int_p^1 \frac{G(v)}{v} dv\\
s.t.& \int_0^1(1+\log u)G'(u)du=-\overline v<0\\
&\int_0^x(1-\frac{G(v)}{v})dv\leq\int_0^xH(v)dv, \text{  for any } x\in[0,1]\\
&\frac{G(v)-v*G'(v)}{v^2}>0, \text{  for any }v\in(0,1]\\
&G(1)=0 \text{ and } G'(p)=0,
\end{aligned}
$$ where $p\in[0,1]$ and $H(v)$ is a weakly increasing function on $[0,1]$ with $H(0)=0$ and $H(1)=1$ . (Actually, this problem is a second version of the problem in my another question Functional Extremum Problem with inhomgeneous boundary condition , just simply let $G(v)=v(1-F(v))$ to avoid inhomgeneous boundary condition.) Now with the constraint I describe above, the new objective function should be $$\begin{aligned}&\int_p^1 \left[\frac{G(v)}{v}+\lambda(1+\log v)G'(v)+\mu(v)\frac{G(v)-v*G'(v)}{v^2}\right]dv\\+&\int_0^1\gamma(x)\int_0^x(1-\frac{G(v)}{v})dvdx\\ =&\int_p^1 \left[\frac{G(v)}{v}+\lambda(1+\log v)G'(v)+\mu(v)\frac{G(v)-v*G'(v)}{v^2}+\int_v^1\gamma(x)dx\times (1-\frac{G(v)}{v})\right]dv, \end{aligned}$$ where $\lambda,\gamma(\cdot),\mu(\cdot)$ are the Lagrange Multiplier (Function), and the equality is from the exchange of integral sequence. By the Calculus of Variations, the marginal condition is $$\frac{1}{v}+\lambda\cdot(-\frac{1}{v})+\int_v^1\gamma(x)dx\cdot(-\frac{1}{v})+\left[\mu(v)\cdot(\frac{1}{v^2}-\frac{1}{v^2})+\mu'(v)\cdot\frac{1}{v}\right]=0,$$ or, $$1-\lambda-\int_v^1\gamma(x)dx+\mu'(v)=0.$$ Neither $G(v)$ nor $G'(v)$ disappears in the condition! I know that this is the similar case when we use Lagrange Multiplier in linear programming problem. But in linear programming, there is other ways. In my case, I don't know how to solve this problem at all without the Lagrange method.","['optimization', 'euler-lagrange-equation', 'functional-analysis', 'calculus-of-variations']"
4374838,Rock-Paper-Scissors and Hadamard matrices,"While playing ``rock paper scissors'' with my daughter it became obvious that she wanted to add a few elements to the game, Specifically the fox and the well.
A bit later we added the match and the fly. Since I wanted it to be a balanced tournament, the completed game looked like this: Although there is only one orientation of the $K_3$ (complete graph on three vertices) which is balanced, and the same seems to hold for $K_5$ , this is definitively not the case for $K_7$ .
In fact, writing the above graph as a matrix: $
\left(\begin{array}{rrrrrrr}
\mathit{Rock} & \mathit{Paper} & \mathit{Scissors} & \mathit{Fox} & \mathit{Well} & \mathit{Match} & \mathit{Fly} \\
0 & -1 & 1 & 1 & -1 & 1 & -1 \\
1 & 0 & -1 & -1 & 1 & -1 & 1 \\
-1 & 1 & 0 & 1 & -1 & 1 & -1 \\
-1 & 1 & -1 & 0 & 1 & -1 & 1 \\
1 & -1 & 1 & -1 & 0 & 1 & -1 \\
-1 & 1 & -1 & 1 & -1 & 0 & 1 \\
1 & -1 & 1 & -1 & 1 & -1 & 0
\end{array}\right)$ one sees that many chunks of columns are similar: the three entries of the fox, well, match and fly columns are either identical or reversed. In order to make things more random, I thought of using a Hadamard matrix of size 8.
First remove first row and column (those which are, for the standard way of writing Hadamard matrices full with 1).
Then permute the rows so that the diagonal entries are all -1.
Finally set the diagonal to 0 (i.e. add the identity matrix).
There are a few candidates (I guess they are all isomorphic), one looks like this: $
\left(\begin{array}{rrrrrrr}
0 & -1 & 1 & -1 & 1 & 1 & -1 \\
1 & 0 & -1 & -1 & -1 & 1 & 1 \\
-1 & 1 & 0 & -1 & 1 & -1 & 1 \\
1 & 1 & 1 & 0 & -1 & -1 & -1 \\
-1 & 1 & -1 & 1 & 0 & 1 & -1 \\
-1 & -1 & 1 & 1 & -1 & 0 & 1 \\
1 & -1 & -1 & 1 & 1 & -1 & 0
\end{array}\right)
$ The resulting matrix gives an orientation of $K_7$ which is balanced.
Furthermore, the columns do not have this tendency to have many entries which are identical or reversed.
Here is the corresponding graph: There are [up to isomorphisms] only 3 possible games on 7 elements: the number of non-isomorphic balanced orientation of $K_{2n+1}$ , is given here . See the paper Rock-Paper-Scissors and borromean rings by Chamberland and Herman. Although it seems to me that this solution is optimal (see below) for the 7 elements game, moving up to a nine elements game, I was wondering how to quantify the quality of the graph.
Basically if $F$ is the matrix of the first game, then $F^tF$ is not very uniform.
Whereas if $S$ is the matrix of the second game, then $S^tS$ takes the same value everywhere except on the diagonal.
Hence a measure of how optimal (how unpredictable ) the game is, would be to look at how the matrix $M^tM$ deviates from a matrix with all non-diagonal coefficients equal. My primary question is: Question: What are [other] possible ways to measure the deviation from [some form] of ideal game? i.e. what are possible definitions of optimal ? Actually, a friend of mine pointed out, that, when the number of elements is big, it is still a fun game even when the nodes are not all balanced. Indeed, if one element is stronger (in the sense that it wins against more elements than it loses), than one might be tempted to pick it.
But then, it still loses against a few others, so you might tempted to pick them. And so on.
Still I don't want to go primarily in this direction because: (1) I don't think it's easy to quantify; (2) My feeling is that such games reduce to a part of the whole. So a secondary question would be: Question: What are possible ways to measure the deviation from [some form] of ideal game in the unbalanced version? I was thinking about stationary measures for a random walk, but I guess there are more clever ways to go around...","['directed-graphs', 'graph-theory', 'combinatorics', 'game-theory', 'recreational-mathematics']"
4374847,Differentiable norm preserving map is a linear isometry?,Suppose $\Phi: \mathbb{R}^n \to \mathbb{R}^n$ is differentiable and $||\Phi(x)|| = ||x||$ for all $x \in \mathbb{R}^n$ . Does it already follow that $\Phi$ is a linear isometry? If $\Phi$ is only continuous we could simply take $\Phi(x) = |x|$ in case $n = 1$ for a counterexample. Do you know any counterexamples or a proof? Is surjectivity maybe the point? Does it hold if we require a priori surjectivity?,"['linear-algebra', 'differential-geometry', 'real-analysis']"
4374857,A proposition concerning the position of Pascal points.,"While answering a recent question I have found algebraically the following nice proposition related to Pascal's theorem . Let $ABCD$ be a cyclic quadrilateral and let $E$ and $F$ be arbitrary diametrically opposite points on its circumcircle. Further let $G=(AE)\cap(DF),H=(BE)\cap(CF),I=(AC)\cap(BD)$ , and $O$ be the center of the circumcircle. Then: $$IG:IH=\tan\frac{\angle AOD}2:\tan\frac{\angle BOC}2.\tag1$$ The proposition reveals that the ratio $IG:IH$ does not depend on the choice of the antipode points $E,F$ , i.e. it is an ""internal"" property of cyclic quadrilateral. My algebraic proof is very lengthy and cannot be reproduced here (essentially it is the same computation as reported in the linked answer). Is this proposition known and can it be easily proved? The collinearity of the points $G,I,H$ can be assumed as it follows from Pascal's theorem.","['quadrilateral', 'euclidean-geometry', 'circles', 'geometry']"
4374890,Area Under The Curve of CDF and Alpha Value - Chi-Sq Test Interpretation,"Can you help me understand the meaning of the area under the curve of a PDF in the interpretation of a Chi-Square test? This is what I think I know: If my test statistic (i.e. the chi-square value) increases for a given Degree of Freedom, so does my cumulative distribution function (CDF) value of the Chi-Square CDF(Chi). This value represents the area under the curve of the Chi-Square probability density function (PDF). CDF(Chi) is also the significance level or ""critical region"" called $\alpha$ (often used with 0.05). Is it correct, that if for a given test value the CDF(Chi) > 0.05 my H0 can be rejected when only interested in a one-tailed test - i.e. are two distributions different or not? This approach seems to work if I do a one-tailed test. This is however currently counterintuitive to me, because as far as I understand, p = 1 - CDF(Chi) and H0 is rejected if p < 0.05. Therefore I would believe that the above statement is not true and I would need to reject H0 if CDF > 0.95. That would also make more sense to me, as it means that H0 is only rejected if there is more than 95% ""evidence"" that it is not random or less than a 5% chance of wrongfully rejecting H0. Can someone help clear things up - I assume my last thought makes more sense and in my example, I am getting results that seem to conform to what I want to see but are actually not statically significant. Thanks!","['statistics', 'cumulative-distribution-functions', 'probability-distributions', 'chi-squared', 'hypothesis-testing']"
4374905,"What is the point of the idea of ""contact"" in differential geometry?","I've been having introductory lectures on differential geometry and we came to the idea of ""contact"". There are two definitions: Let $\alpha: I \to \Bbb{R}^3$ and $\beta: \overline{I} \to \Bbb{R}^3$ be regular curves such that $\alpha(t_0)=\beta (t_0)$ with $t_0 \in I \cap \overline{I}$ . We say $\alpha,\beta$ have contact of order $n$ in $t_0$ if all derivatives of order $\leq n$ of $\alpha,\beta$ coincide in $t_0$ and the derivatives of order $n+1$ in $t_0$ are distinct. Let $\alpha:I\to \Bbb{R}^3$ a regular curve and $\pi$ a plane in $\Bbb{R}^3$ with a point $p=\alpha(t_0)$ for some $t_0\in I$ . We say $\alpha$ and $\pi$ has contact of order $n$ in $p$ if there exists a regular curve $\beta: \overline{I} \to \Bbb{R}^3$ such that $\beta(\overline{I})\subset \pi$ and $\alpha,\beta$ haver contact of order $n$ in $t_0$ . And two theorems asserting that: The only straight line with contact $1$ in a point of a curve is the tangent line. The only plane with contact $2$ in a point in a curve is the osculating plane. But at least in the book I am reading, it seems it stops there. We just define ""contact"" and check that there are special lines and planes with a certain degrees of ""contact"". So I am curious: What is the point of the idea of ""contact""?",['differential-geometry']
4374928,Order of learning measure theory and functional analysis,Guiding question :Should measure theory be learned before functional analysis or should it be the other way around? Perhaps there is no largely agreed upon answer to this so I'll ask: More specific question : What connections are there between the two subjects that might make a person choose to study one before the next? All feedback is appreciated.,"['banach-spaces', 'measure-theory', 'lebesgue-measure', 'hilbert-spaces', 'functional-analysis']"
4375029,First Fundamental Form and the Metric,"On the upper half-plane with standard $x,y$ coordinates we have the hyperbolic metric given by $g_{11}=g_{22}=1/y^2$ and $g_{12}=0$ . In terms of the first fundamental form, we write $$ds^2=\frac{dx^2+dy^2}{y^2}=
\frac{-4\,dz\,d\bar{z}}{(z-\bar{z})^2}.$$ I can interpret $dz$ and $d\bar{z}$ as (complex-valued) $1$ -forms on the upper half plane. Suppose I now want to show that a fractional linear transformation $z\mapsto z'$ where $z'=\frac{az+b}{cz+d}$ and $ad-bc=1$ gives an isometry. By definition of an isometry, I would want to show that the pushforward of two tangent vectors in the domain yield the same result evaluated under the metric. But when I see other people approach the problem, what they do is calculate $dz'$ and $d\bar{z'}$ , and then prove that $$\frac{-4\,dz'\,d\bar{z'}}{(z'-\bar{z'})^2}=\frac{-4\,dz\,d\bar{z}}{(z-\bar{z})^2}.$$ I'm a little confused by what this calculation is actually doing. I would think that what I need to do is evaluate the same fundamental form (without the 's in the numerator) at the point $z'$ and $\bar{z'}$ on the pushforward of some tangent vectors in the domain.  I am not sure why it is valid to simply substitute in $dz'$ and $\bar{dz'}$ and compute ""the same thing"", and along the way I seem to have lost any intuition of what $dz'$ and $d\bar{z'}$ really mean as $1$ -forms.","['complex-analysis', 'riemannian-geometry', 'differential-geometry']"
4375049,Cauchy's integral formula and essential singularities,"Let $f$ be holomorphic at $z_0\in\mathbb C$ . I would like to compute the integral $$\oint_{\gamma_{z_0}} f(z)\, e^{\frac{1}{z-z_0}}dz,$$ where $\gamma_{z_0}$ is a small circle around $z_0$ .
By expanding the exponential, I find that $$\oint_{\gamma_{z_0}} f(z)\, e^{\frac{1}{z-z_0}}dz=\sum_{n=0}^\infty \frac{1}{(n+1)!}\oint_{\gamma_{z_0}} \frac{f(z)}{(z-z_0)^{n+1}}dz.$$ The integral can be evaluated using Cauchy's integral formula, $$\oint_{\gamma_{z_0}} f(z)\, e^{\frac{1}{z-z_0}}dz=2\pi i\sum_{n=0}^\infty \frac{f^{(n)}(z_0)}{n!(n+1)!},$$ with $f^{(n)}$ the $n$ -the derivative of $f$ . It seems that the result could be related more directly to $f$ , having similarities with the Taylor series $f$ . To be precise, if the $(n+1)!$ was removed, the result would be $2 \pi i f(z_0+1)$ . I was wondering a) if such Borel-like sums have been studied for Taylor series of holomorphic functions, and b) if there is a more direct relation between the period integral and $f$ that would allow to compute the sum. Many thanks for sharing your ideas!","['cauchy-integral-formula', 'complex-analysis', 'contour-integration', 'sequences-and-series', 'residue-calculus']"
4375138,"$A+B+C+D=\pi$, and $0\leq A,B,C,D \leq \frac{\pi}{2}$. Prove that $\sin^4(A)+\sin^4(B)+\sin^4(C)+\sin^4(D)\leq 2$","I tried to simplify it from some ways. (1). $\sin^4(A)+\sin^4(B)+\sin^4(C)+\sin^4(D)$ $=\left(\frac{1-\cos(2A)}{2}\right)^2+\left(\frac{1-\cos(2B)}{2}\right)^2+\left(\frac{1-\cos(2C)}{2}\right)^2+\left(\frac{1-\cos(2D)}{2}\right)^2$ $=1-\frac{1}{2}\left(\cos(2A)+\cos(2B)+\cos(2C)+\cos(2D)\right)+\frac{1}{4}\cdot\left(2+\frac{\cos(4A)+\cos(4B)+\cos(4C)+\cos(4D)}{2}\right)$ $=\frac{3}{2}-\frac{1}{2}\left(\cos(2A)+\cos(2B)+\cos(2C)+\cos(2D)\right)+\frac{1}{8}\left(\cos(4A)+\cos(4B)+\cos(4C)+\cos(4D)\right)$ (2). $\cos(2A)+\cos(2B)+\cos(2C)+\cos(2D)$ $=2\cos(A+B)\cos(A-B)+2\cos(C+D)\cos(C-D)$ since $A+B=\pi-(C+D)$ , thus $\cos(A+B)=-\cos(C+D)$ . It follows that $=2\cos(A+B)\left(\cos(A-B)-\cos(C-D)\right)$ $=2\cos(A+B)\cdot\left(-2\sin(\frac{A-B+C-D}{2})\sin(\frac{A-B-C+D}{2})\right)$ $=2\cos(A+B)\cdot\left(-2\sin\left(\frac{\pi}{2}-(B+D)\right)\sin\left(\frac{\pi}{2}-(B+C)\right)\right)$ $=2\cos(A+B)\cdot\left(-2\cos(B+D)\cos(B+C)\right)$ $=-4\cos(A+B)\cos(A+C)\cos(A+D)$ (3).Similarly, $\cos(4A)+\cos(4B)+\cos(4C)+\cos(4D)$ $=2\cos(2A+2B)\cos(2A-2B)+2\cos(2C+2D)\cos(2C-2D)$ $=2\cos(2A+2B)\cos(2A+2C)\cos(2A+2D)$ Am I on the right track? Maybe it is a Jensen's-inequality problem but it seems not always concave up or down in the interval $[0,\frac{\pi}{2}]$ . I am stuck here. Please help, and thank you.","['trigonometry', 'triangle-inequality', 'inequality']"
4375230,At what point is the line $y = mx + b$ tangent to $y = \sin (mx + b)$?,"I was playing around on Desmos to investigate the visual relationship between $y = mx + b$ and $y = \frac{x + m}{b}$ . I noticed that the effect of $m$ and $b$ on these two equations were opposite: scaling $m$ increases the slope of $y = mx + b$ while shifting $y = \frac{m + x}{b}$ from left to right, while scaling $b$ does the inverse. Out of curiosity, I applied the sine function to each of these equations and discovered that they respond similarly: scaling $m$ and $b$ contracts or shifts the two waves. I also noticed that the lines were tangent to their respective sine wave, shifting and sloping to, on a visual level, remain tangent to the same location on the wave (e.g., decreasing $b$ causes both the line $y = mx + b$ and its corresponding sine wave $y = \sin (mx + b)$ to visually move together, with the line appearing to ""move"" with the same wavefront). I'm wondering how to calculate exactly where the line $y = mx + b$ is tangent to $y = \sin (mx + b)$ .","['trigonometry', 'calculus', 'derivatives', 'algebra-precalculus']"
4375263,Operators similar to operators with spectral radius 1,"Let $A$ be a linear bounded operator acting on a Banach space $X.$ Assume the spectral radius of $A$ is equal $1.$ Do there exist  invertible operators $U_n:X\to X,$ such that $$\|U_n^{-1}AU_n\|<1+{1\over n},\quad n\ge 1\ ?$$ I can do it for Hilbert space $X$ (see Szwarc, Ryszard;
Problems and Solutions: Solutions of Advanced Problems: 6496.
Amer. Math. Monthly 94 (1987), no. 2, 197–199).
For the Banach space $X,$ I am able to construct a new norm $\|\cdot\|_n$ on $X,$ equivalent to the original norm $\|\cdot \|,$ such that the norm of $A:(X,\|\cdot\|_n)\to (X,\|\cdot\|_n)$ is less than $1+1/n.$ But I am unable to decide whether it can be done by similarity, as described above. I have checked (to be on the safe side) it works for two-dimensional space with norm $\|(z_1,z_2)\|=|z_1|+|z_2|.$ In general, the conclusion is true for any finite-dimensional normed space $X=\mathbb{C}^n,$ once it holds for the euclidean norm. This should follow from Jordan's decomposition of matrices and the fact that any two norms on finite-dimensional space $M_{n\times n}(\mathbb{C})$ are equivalent.","['spectral-radius', 'operator-theory', 'functional-analysis']"
4375285,Reference for Complex Manifolds for Great Picard Theorem and Julia Sets,"I’ve recently been looking to understand the proofs of some theorems about complex analysis and Julia sets that require results on complex manifolds, especially The Great Picard Theorem. I need The Uniformization Theorem, then I need to understand why the plane with two points removed can’t have the entire complex plane as its cover. I know basic complex analysis, real differential topology and results on coverings from algebraic topology. Where can I find a text with the sort of results I need?","['complex-analysis', 'complex-manifolds', 'reference-request']"
4375309,Feynman-Kac proof,"I have seen numerous proofs, showing that if $u\left(x,t\right)$ satisfies a PDE of the form: $$\frac{\partial u}{\partial t}+\mu\left(x,t\right) \frac{\partial u}{\partial x}+\frac{1}{2}{\left(\sigma\left(x,t\right)\right)}^2 \frac{\partial^2 u}{{\partial x}^2}-V\left(x,t\right) u+f\left(x,t\right)=0$$ then, $M_s$ defined as: $$M_s=e^{-\int\limits_{t}^{s}{V\left(\tau,X_{\tau}\right)}d\tau}u\left(s,X_s\right)+\int\limits_{t}^{s}{e^{-\int\limits_{t}^{r}{V\left(\tau,X_{\tau}\right)d\tau}}f\left(r,X_r\right)dr}$$ where $X_t$ is a random process, with evolution defined as: $$dX\left(x,t\right)=\mu\left(x,t\right) dt+\sigma\left(x,t\right) dW$$ is a martingale, and as such, its average at multiple realizations at an arbitrary point in time (i.e. terminating time), could be used as an estimator for the solution. However, I could not find any proof that such a solution actually satisfies the original equation. Any good reference or tip would be greatly appreciated. Thanks in advance!","['monte-carlo', 'analysis', 'reference-request', 'stochastic-processes', 'partial-differential-equations']"
4375341,Solutions of $\tan(2x+3) = -1/2$,"this is a silly question but here I go. I was solving a question that required evaluating the derivative of an equation, which would result in finding a local min and a local max. The two points are in Quadrant 2 and 3,respectively. The original equation is $\frac{\cos(2x+3)}{2e^x}$ . It came down to $\tan(2x+3) = -1/2$ . After this, $\arctan(-1/2) = -0.463647..$ . This lies in Q4, but I was wondering if I could use its complimentary angle (i.e., $2π - 0.463657$ ). The answer would be same, since $\tan(2π - \theta) = - \tan(\theta)$ . However, both yields different $x$ values and thus different answers. I would be appreciated if you could explain the problem in my reasoning to me.",['trigonometry']
4375358,The derivative of flow for an autonomous system,"We are considering an autonomous equation $$\dot x=f(x),$$ where $f: \mathbb{R}^n\rightarrow\mathbb{R}^n$ is a $C^1$ vectorfield. Let $\phi^t$ be the corresponding flow. Is it true that $$D\phi^t(x)f(x)=f(\phi^t(x)),$$ why? $D$ stands for the Jacobian matrix of $\phi^t(x)$ . This is a question I encountered while I was reading researching articles: Coomes, B. A., Koçak, H., Palmer, K. J. (1995). A shadowing theorem for ordinary differential equations. Zeitschrift für angewandte Mathematik und Physik ZAMP , 46(1), 85-106. K. J. Palmer (1996) Shadowing and Silnikov Chaos. Nonlinear Analysis, Theory, Methods & Applications , 27(9), 1075-1093. Palmer, K. J. (2008). Transversal periodic-to-periodic homoclinic orbits. Handbook of Differential Equations: Ordinary Differential Equations , 4, 365-439. The authors simply state this is a fact on Page 96, 1077, 382 of the articles above, but without any derivation process. So I think this might come from some fundamental theory in dynamical systems. So I calculate like this: $$D\phi^t(x(s))f(x(s))=D\phi^t(x(s))\dot x(s)=\frac{d}{ds}[\phi^t(x(s))]=f(\phi^t(x)).$$ I'm not sure if this is correct or in a standard way. Any help will be deeply appreciated.","['ordinary-differential-equations', 'dynamical-systems']"
4375377,Find the limit of $(1+\frac{1}{n^2})(1+\frac{2}{n^2})...(1+\frac{n}{n^2})$.,To find the limit of $(1+\frac{1}{n^2})(1+\frac{2}{n^2})...(1+\frac{n}{n^2})$ . First notice that $(1+\frac{1}{n^2})$ is the smallest term in the product and $(1+\frac{n}{n^2})$ is the greatest. So: $ (1+\frac{1}{n^2})^n \leq (1+\frac{1}{n^2})(1+\frac{2}{n^2})...(1+\frac{n}{n^2}) \leq (1+\frac{n}{n^2})^n$ . First this seemed like a good idea but then I found $ 1 \leq (1+\frac{1}{n^2})(1+\frac{2}{n^2})...(1+\frac{n}{n^2}) \leq e$ . Which is not really helpful. How can this problem be done? P.S: In previous questions of the problem I had to prove that $\sum n^2 = \frac{n(n+1)(2n+1)}{6}$ and $x-\frac{x^2}{2} \leq  \ln(x+1) \leq x$ .,"['limits', 'real-analysis']"
4375419,"How to show that $E[\phi(X,Y)|G]=E[\phi(x,Y)]$ for all $x$?","The question is, Given a probability space $(\Omega, F, P)$ , a sigma field $G \subset F$ , random variables $X,Y$ and a measurable function $\phi$ such that $X \in G$ , $Y$ is independent with $G$ and $E[|\phi(X,Y)|] < \infty$ . Then, Show that $E[\phi(X,Y)|G]= g(X)$ , where $g(x) = E[\phi(x,Y)]$ for all $x$ . I proved this result when $X,Y$ are independent. What is direction to prove this if independence condition is not given?","['conditional-expectation', 'probability-theory']"
4375493,Mutual information is maximized when Poisson-Binomial reduces to Binomial?,"I have two random variables $X$ and $Y$ . $X$ follows Poisson-Binomial distribution with parameters $\{q_1, \ldots, q_k\}$ . Thus, $X$ can take values in the set $\{0,1,\ldots,k\}$ . $Y$ is a binary random variable. $Y=0$ with probability $\frac{0.1}{1+x}$ when $X=x$ .  Thus, the transition probabilities $\frac{0.1}{1+x}$ is decreasing in $x$ . I am interested in the mutual information between $X$ and $Y$ denoted by $I(X;Y)$ . Specifically, I have a conjecture that the mutual information $I(X;Y)$ is maximum when all $q_i$ 's are equal which leads to a Binomial
distribution for $X$ . Also, my intuition is that any transition probability which is decreasing in $x$ ( Similar to $\frac{0.1}{1+x}$ ) leads to this conjecture. Can someone help me prove this? Or provide a counter example? $\underline{\text{Some useful facts that I noted}}$ : For a given $\sum q_i$ and $k$ , the Binomial distribution has the maximum variance. The entropy of Poisson- Binomial distribution is bounded above by the entropy of a binomial distribution with the same $k$ and $\sum q_i$ . I have asked this question in cstheroySE as well to gather attention as the topics on Information Theory don't have a large audience comparatively.","['mutual-information', 'binomial-distribution', 'optimization', 'probability', 'random-variables']"
4375534,Can the exponential map of $SO(3)$ be expressed as a limit?,"According to this textbook (page 14), the exponential map of some Lie algebras (like $SO(2)$ ) can be thought of as repeated multiplication of matrices with infinitesimally small rotations. $$
\lim_{k\to\infty} \left(\mathbf{I} + \alpha_a\frac{X_a}{k}\right)^k = \sum_{m=0}^{\infty} \frac{1}{m!}\left(\alpha_aX_a\right)^m \equiv e^{\alpha_aX_a},
$$ where $X_a$ is the $a$ -th generator (basis vector) of the Lie algebra and $\alpha_a$ is the coefficient of the $a$ -th basis vector (the textbook seems to be using Einstein's summation notation). I feel like this derivation of the exponential map for ""sphere-like"" manifolds like $SO(2)$ or $SO(3)$ makes sense and thus I wanted to test whether this intuitive definition via a limit, when simplified, can indeed lead to the analytical version of the exponential map. I therefore first decided to use Python's symbolic math package SymPy to verify whether the equation holds for the Lie algebra of $SO(2)$ and SymPy was able to simplify the symbolic limit into the analytical form of the exponential map (3.6) in the textbook. I plugged the generator $X_{SO(2)}$ into the limit and after simplification, I got the expected analytical form $\exp_{SO(2)}(x)$ . $$
\begin{aligned}
X_{SO(2)} &= \begin{bmatrix}0 & -x\\x & 0\end{bmatrix}\\
\exp_{SO(2)}(x) &= \begin{bmatrix}\cos x & -\sin x\\\sin x & \cos x\end{bmatrix}
\end{aligned} 
$$ However, when I tried to simplify the limit with generators for $SO(3)$ , the analytical expression and the limit were not the same! I wondered whether the same limit could be used to derive the analytical form of the exponential map of $SO(3)$ , so I found the generator $X_{SO(3)}$ of $SO(3)$ and the expected analytical exponential map $\exp_{SO(3)}(X)$ in this paper (page 5, Example 3 and 4): $$
\begin{aligned}
X_{SO(3)} &= \begin{bmatrix}0 & -z & y\\ z & 0 & -x\\ -y & x & 0\end{bmatrix}\\
\exp_{SO(3)}(X) &= \mathbf{I} + X_{SO(3)} \sin{\theta} + X^2_{SO(3)} (1 - \cos{\theta})
\end{aligned} 
$$ I assume that $\theta = \sqrt{x^2+y^2+z^2}$ . I expected the following to be true: $$
\lim_{k\to\infty} \left(\mathbf{I} + \frac{X_{SO(3)}}{k}\right)^k \stackrel{?}{=} \mathbf{I} + X_{SO(3)} \sin{\theta} + X^2_{SO(3)} (1 - \cos{\theta})\\
\lim_{k\to\infty} \left(\mathbf{I} + \frac{X_{SO(3)}}{k}\right)^k - \left(\mathbf{I} + X_{SO(3)} \sin{\theta} + X^2_{SO(3)} (1 - \cos{\theta})\right) \stackrel{?}{=} \begin{bmatrix}0 & 0 &0\\0 & 0 &0\\0 & 0 &0\end{bmatrix}
$$ However, after attempting to use SymPy to simplify the expression above, I did not get a $\mathbf{0}$ matrix. It seems like one of the following is true: I might be using a wrong analytical form $\exp_{SO(3)}(x,y,z)$ . I might misunderstood what $\theta$ stands for in $\exp_{SO(3)}(x,y,z)$ . I made a mistake when re-writing the expressions into SymPy. SymPy is unable to correctly simplify the expression. The limit does not hold for $SO(3)$ . What did I do wrong? And if the limit does not hold for $SO(3)$ , what are some other ways to derive the exponential map for $SO(3)$ in an intuitive and satisfying way? Below is the Python/SymPy code I used: from sympy import *
init_printing()


x, y, z = symbols('x y z')
G = Matrix([
    [0, -z, y],
    [z, 0, -x],
    [-y, x, 0]
])

# The analytic expression of the exponential map
omega = sqrt(x*x + y*y + z*z)
analytic_expression = Identity(3).as_explicit() + sin(omega) * G + (1 - cos(omega)) * G**2

# Expressing the exponential map as a limit
n = symbols('n', integer=True)
limit_expression = (Identity(3).as_explicit() + G/n)**n
limit_expression = limit_expression.limit(n, oo)

# If both expressions are exactly the same, their difference should be the zero matrix
diff_matrix = simplify(analytic_expression - limit_expression)

# diff_matrix is NOT zero. Why?
print(diff_matrix) EDIT: To rule-out the possibility of a bug in SymPy, I have also tried to simplify the expression using Wolfram Mathematica (I assume Mathematica is more reliable than SymPy). However, not even Mathematica simplified the difference between the limit and the analytical solution into the $\mathbf{0}$ matrix. The Mathematica code: Clear[""Global`*""]
X = {
    {0, -z, y},
    {z, 0, -x},
    {-y, x, 0}
}

omega = Sqrt[x^2 + y^2 + z^2]

analyticExpression = IdentityMatrix[3] + Sin[omega]*X + (1 - Cos[omega]) * (X.X)
limitExpression = Limit[MatrixPower[IdentityMatrix[3] + X/n, n], n -> Infinity]

(analyticExpression - limitExpression) // FullSimplify // MatrixForm","['limits', 'lie-algebras', 'lie-groups', 'rotations']"
4375537,Derivatives. Getting things into and out of d(),In a textbook I encountered this: $$\frac{H}{\omega_s}2\omega_r\frac{d\omega_r}{dt} = (P_m-P_e)\frac{d\delta}{dt}$$ The left-hand side of this equation can be rewritten to give $$\frac{H}{\omega_s}\frac{d\left(\omega^2_r\right)}{dt} = (P_m-P_e)\frac{d\delta}{dt}$$ But to me it seems that $\frac{d(w_r^2)}{dt}$ is the same as $\frac{d}{dt}w_r^2$ which is just $2w_r$ and not $2w_r\frac{dw_r}{dt}$ What am I missing here? EDIT: Got it! If it were $\frac{d(t^2)}{dt}$ then it would be $2t$ But $w_r$ is a function of $t$ so you need to use the chain rule!,"['notation', 'calculus', 'derivatives']"
4375554,Decide whether this integration $\displaystyle \int ^\infty _ 0 \dfrac{x}{1+x^2\cos x} dx$ converges or diverges,"\begin{equation}
\int ^\infty _ 0 \dfrac{x}{1+x^2\cos x} dx
\end{equation} I don't know how to prove if this integration is convergent or divergent. My first idea is to find another function and do a limit comparison with $\dfrac{x}{1+x^2\cos x}$ at $x= \infty$ , but I couldn't find a suitable function to do that. Then, I tried to find whether $\dfrac{x}{1+x^2\cos x}$ this function is always less than another function or whether it is always greater than another function. However, since $\cos $ is oscillating between 1 and -1, I failed I also tried to ""split"" this integral by only considering some intervals on which $\cos$ is greater than 0 or is less than zero (something like $\int ^\frac{\pi}{2} _ 0 \frac{x}{1+x^2\cos x} dx + \int ^\frac{3\pi}{2} _ \frac{\pi}{2} \frac{x}{1+x^2\cos x} dx+...$ ) As a result, I still can't figure it out. Even though I can prove this the absolute value of $\dfrac{x}{1+x^2\cos x}$ is divergent, but it also seems not to be helpful. Thus, any helps? Thanks! Edit:(The integration is wrong, please dismiss this edition)
I tried integration by parts: \begin{equation}
\displaystyle \int ^\infty _ 0 \dfrac{x}{1+x^2\cos x} dx = \dfrac{x^2}{2(1+x^2\cos x)}|^{\infty} _{0} + \dfrac{1}{2}\int^\infty_0\frac{x^2}{(1+x^2\cos x)^2} dx
\end{equation} This function's absolute value is divergent, by writing the right hand side integration to be \begin{equation}
\int^\infty_0\frac{x^2}{(1+x^2\cos x)^2} dx =\int^\infty_0|\frac{x}{(1+x^2\cos x)}|^2 dx
\end{equation} Thus $\int^\infty_0\frac{x^2}{(1+x^2\cos x)^2} dx $ is divergent to infinity at $x= \infty$ And $\dfrac{x^2}{2(1+x^2\cos x)}$ oscillates at positive infinity and negative infinity As a result, I think this integration doesn't converge Is this a reasonable argument?","['integration', 'convergence-divergence', 'solution-verification', 'analysis']"
4375559,Heuristic reason for Rademacher Theorem,Loosely speaking Rademacher Theorem states that Lipschitz functions are almost everywhere differentiable. Is there a heuristic justification for this?,"['continuity', 'derivatives', 'lipschitz-functions', 'real-analysis']"
4375568,Prove that $x^{13}-y^{14}-x+y$ is divisble by $157$.,"Natural numbers $x,y$ are such that $x^{12}-y^{13}$ is divisible by $157$ . Prove that $x^{13}-y^{14}-x+y$ is also divisble by $157$ . Off the cuff I tried to multiply first expression by some quantities to get to the $x^{13}-y^{14}-x+y$ , but that would demand from $x^{12}-y^{13}$ to contain $\pm 1$ to obtain terms $x$ or $y$ . The key fact to me seems that $157$ is prime and can be expressed as $12 \times 13 +1$ . I checked by hand, in search of patterns, few cases in more general statement $n^2+n+1|x^n-y^{n+1} \rightarrow n^2+n+1|x^{n+1}-y^{n+2}-x+y$ and for $n=1,2$ it seems true.","['number-theory', 'divisibility', 'elementary-number-theory']"
4375662,Weak convergence implies weak* convergence?,"In lecture my professor said that ""weak convergence implies weak* convergence"" but gave no explanation or proof, and ended class there.  I'm trying to make sense of this statement but can't even see what is really being claimed here. So first of all, what is the assumption of the claim?  Clearly we have to be talking about a normed linear space $(X,\|\cdot\|)$ .  Are we assuming that there exists a weakly convergent sequence $\{f_n\}\rightharpoonup f$ in $X$ ?  I feel like this must be the assumption, since any other quantifier seems silly. Now what is the conclusion of the claim?  That there exists a weak* convergent sequence?  Or that $f_n$ determines a particular sequence $\varphi_n\in X^*$ , and $f$ determines $\varphi$ , such that $\varphi_n\overset w \to \varphi$ in $X^*$ ?  I know that we can define a natural element in $X^{**}$ corresponding to any $g\in X$ by the definition $\psi_{g}(\phi') = \phi'(g)$ , and this has the property that $\|\psi_{g}\|_{**}=\|g\|$ but it's not clear to me how this would determine some particular sequence of $\{\varphi_n\}\subseteq X^*$ .","['normed-spaces', 'functional-analysis', 'weak-convergence']"
4375666,"$\max\{ aX_n, b Y_n\} \to \max\{aX,bY\}$","Suppose $(X_n,Y_n)$ is a sequence of bivariate random variables. Suppose there exists a bivariate random variable $(X,Y)$ such that $X_n\to X$ and $Y_n\to Y$ in distribution. Suppose further that for all $a,b\in \mathbb{R}$ we have $$\max\{aX_n, bY_n\} \stackrel{d}{\to} \max\{aX,bY\}.$$ Then does $(X_n,Y_n)\stackrel{d}{\to} (X,Y)$ ? I am not sure if it is at all true. The question is pretty similar to the Cramer Wold device but with the max operator. Note that we may write $$aX_n+bY_n= \max\{aX_n, bY_n\}+\min\{aX_n, bY_n\}=\max\{aX_n, bY_n\}-\max\{-aX_n, -bY_n\}.$$ However, this does not quite help. Any thought or help is appreciated.","['weak-convergence', 'probability-limit-theorems', 'probability']"
4375695,"Does $\exists\ X\subset [0,1],\ a\neq 0,b\in\mathbb{R},\ $ such that $\ (Y:=) \{ ax+b: x\in X \} \cap X = \emptyset;\ X\cup Y = [0,1]\ $ or $(0,1)\ ?$","If $ X\subset [0,1],\ a\neq 0,b,\in\mathbb{R} $ then define $\ Y:= \{ax+b: x\in X \}.$ Does there exist $X\subset [0,1],\ a,b\in\mathbb{R},\ $ such that $\ Y
 \cap X = \emptyset;\ X\cup Y = [0,1]\ $ or $(0,1)\ ?$ If the question was to make $X\cup Y = [0,1)\ $ then $ X = \left[0,\frac{1}{2}\right)\ $ will do, with $a=1, b = \frac{1}{2}.$ But if the question is to make $X\cup Y = [0,1]\ $ or $X\cup Y = (0,1)\ $ then I am not sure how to do it/ if it can be done. This question made me think of my question from a few months ago , but I don't see how alternating between rationals and irrationals helps with this question. For $X\cup Y = (0,1),\ $ can we arctan the function in Troposphere's answer to that question? I don't think that works.","['elementary-set-theory', 'examples-counterexamples', 'real-analysis']"
4375704,Solve $\log(z^2-1)=\frac{i\pi}{2}$,"I was wondering if someone could help me solve the following complex logarithmic equation, such that $$
\text{Log}(z):=\log(z) \iff\arg z=\theta_p
$$ $$
\forall z \in \mathbb{R} :\text{Log}(z^2-1)=i\pi/2
$$ So far, I have $$
w=e^z \implies w^2+w+1=0.
$$ Solving for $w$ using quadratic $$
w=-1/2+i\sqrt3/2
$$ or $$
w=-1/2-i\sqrt3/2
$$ From the text, I know a few things $$
\text{Log}(z)=\text{Log}(r)+i\theta_p : r=|z|>0, \theta_p=\arg(z), -\pi<\theta_p<=\pi
$$ $$
\text{Arg}(z)=\theta_p+2\pi k : k=..., -3, -2, -1, 0, 1, 2, 3, ...
$$ $$
\log(z)=\text{Log}(r)+i(\theta+2 \pi k)
$$ Thank you in advance for your help!","['complex-analysis', 'trigonometry', 'complex-numbers', 'logarithms']"
4375782,Representing the cube of any natural number as a sum of odd numbers,"I'm expanding my notes on exercises from Donald Knuth's The Art of Computer Programming, and found something rarely mentioned in the Internet, but still useful to prove Nicomachus' Theorem about the sum of cubes. Knuth phrases this in the following way in exercise 8(a) to chapter 1.2.1: Prove the following theorem of Nicomachus (A.D. c. 100) by induction: $1^3=1$ , $2^3=3+5$ , $3^3=7+9+11$ , $4^3=13+15+17+19$ , etc. In the answers to exercises, the author gives the following formula: $(n^2-n+1)+(n^2-n+3)+...+(n^2+n-1)=n^3$ My question is, how do I get to that formula from the sample sums given in the problem? It looks kind of odd, especially because the last summand doesn't give me any idea on how it is connected with first ones. Usually one is able to see this clearly, but not here. I tried to get to that formula by the following set of thoughts: To get $n^3$ one must sum up $n$ odd numbers, starting from the $(n-1)$ th central polygonal number, meaning that a cube of number $n$ is made by summing up odd numbers, starting from $(n-1)$ th central polygonal number to $n$ th triangular number . Since odd numbers form arithmetic progression with $a_1=1, d=2$ , it's possible to use the following formula of summing up $p$ th to $q$ th member of this progression: $$S_{p,q}=\dfrac{a_p+a_q}2\cdot(q-p+1)$$ We can simplify $\dfrac{a_p+a_q}2$ , putting $a_p=2p-1$ and $a_q=2q-1$ : $$\dfrac{a_p+a_q}2=\dfrac{(2p-1)+(2q-1)}2=\dfrac{2p+2q-2}2=p+q-1$$ Thus, the formula of summing up $p$ th to $q$ th member of this progression is: $$(p+q-1)(q-p+1)=(q+(p-1))(q-(p-1))=q^2-(p-1)^2$$ Substituting in $p=\dfrac{n(n-1)}{2}+1=\dfrac{n^2-n+2}{2}$ , and $q=\dfrac{n(n+1)}{2}$ , we get the formula: $$\left(\dfrac{n(n+1)}{2}\right)^2-\left(\dfrac{n(n-1)}{2}+1-1\right)^2=\left(\dfrac{n(n+1)}{2}\right)^2-\left(\dfrac{n(n-1)}{2}\right)^2=\\
\dfrac{(n(n+1))^2-(n(n-1))^2}{4}=\dfrac{(n(n+1)-n(n-1))(n(n+1)+n(n-1))}{4}=\\
\dfrac{n^2(n+1-n+1)(n+1+n-1)}{4}=\dfrac{4n^3}{4}=n^3$$ This... kind of... proves the sum formula for any $n$ , really. But it doesn't give out the formula in question, i.e. $(n^2-n+1)+(n^2-n+3)+...+(n^2+n-1)$ . What is the correct way to get this formula? Any hints are greatly appreciated.","['summation', 'elementary-number-theory', 'triangular-numbers', 'combinatorics', 'induction']"
4375832,Proving Tietze's theorem on metric spaces,"Let $(X,d)$ be a metric space and $A$ be a non-empty closed subset of $X$ . If $f\colon A\to \mathbb{R}$ is an application continuous and bounded, then there exists a continuous map $g\colon X\to \mathbb{R}$ such that $g(x)=f(x)$ for all $x\in A$ and $$\inf_{x\in X}g(x)=\inf_{y\in A}f(y) \quad \text{ y } \quad \sup_{x\in X}g(x)=\sup_{ y\in A}f(y).$$ My attempt We can decompose $X = A^{\circ} \cup (X\smallsetminus A) \cup \partial A$ . Let $m=\inf_{y\in A}f(y)$ and $M = \sup_{y\in A}f(y)$ . I first consider the case when $m=1$ and $M=2$ . Let us define $h\colon X\smallsetminus A \to \mathbb{R}$ by $h(x)=\inf\{f(y)d(x,y)\colon y\in
A\}$ and $g\colon X\to \mathbb{R}$ by \begin{equation}\label{def1}
g(x)=
\left\{
\begin{array}{ccl}
\dfrac{h(x)}{d(x,A)} & \text{si} & x\in X\smallsetminus A,\\
f(x) & \text{if} & x \in A.
\end{array}
\right.
\end{equation} It is easy to prove that $1 \leq g(x) \leq 2$ for all $x \in X\smallsetminus A$ and $g$ extends $f$ to all $X$ . It remains for me to prove that $g$ is continuous on $X$ ; however, I have proven that $g$ is continuous on $A^{\circ}$ and on $X\smallsetminus A$ . It only remains for me to show that it is continuous on $\partial A$ . To do this, suppose $x_0\in \partial A=A\cap \overline{X\smallsetminus A}$ and let $\varepsilon \in (0,1)$ ; since $f$ is continuous on $A$ and $x_0\in A$ , there exists $\eta>0$ such that \begin{equation}
 y\in A\cap B(x_0, \eta) \quad \text{implies} \quad |f(y)-f(x_0)|\le \varepsilon. \quad (*)
\end{equation} Let $\delta\colon=\eta/3$ and $x\in B(x_0, \delta)$ . There are two cases. Case 1: If $x\in A$ , it follows from $(*)$ that $|g(x)-g(x_0)|=|f(x)-f(x_0)|\le \varepsilon$ . Case 2: If $x\notin A$ . Choose $a\in A$ such that $d\left(x,a\right)\leq 2d\left(x,A\right)$ ,
and note that $$
d\left(a,x_{0}\right)\leq d\left(a,x\right)+d\left(x,x_{0}\right)\leq2 d\left(x,A\right)+d\left(x,x_{0}\right)\leq3\cdot d\left(x,x_{0}\right)\leq\eta.
$$ Therefore, $\left|f\left(a\right)-f\left(x_{0}\right)\right|\leq\frac{\varepsilon}{2}$ ,
and thus $$
g\left(x\right)\leq \dfrac{f(a)d(x,a)}{d(x,A)} \leq \dfrac{(\frac{\varepsilon}{2}+f(x_0))d(x,a)}{d(x,A)} \leq \dfrac{(\frac{\varepsilon}{2}+f(x_0))2d(x,A)}{d(x,A)} = 2f(x_0) + \varepsilon.
$$ Precisely here is my mistake, because I need to prove that $ g(x) \leq f(x_0) + \varepsilon$ but I got $ g(x) \leq 2f(x_0) + \varepsilon$ . So I want to know where I am failing. I think if I can show this, the inequality $f(x_0) -\varepsilon \leq g(x) $ would be analogous. Finally, for the general case $m<M$ , I suspect that I should define the function $F(x) = \alpha f(x) + \beta$ , but I'm not sure. I need help with this exercise, please. Any help is appreciated.","['continuity', 'functions', 'metric-spaces']"
4375861,Weak convergence in the Hilbert cube,"I'm very curious about the following problem: How can I show that in the Hilbert cube defined as $$C=\{x=(x_1,x_2,\dots) \in l^p: |x_n|\leq \frac{1}{n}\,\,\, \forall n \in \mathbb{N}\}, 1\leq p < \infty$$ the weak convergence implies strong convergence? I just need some reference or hints on how to attack the problem. I know that for every $\varphi \in (l^p)^*$ , it follows that if $(x_n)$ is a sequence in $C$ that converges weakly to some $x$ , then $\varphi(x_n) \to \varphi(x).$ I'm hoping to find some specifics $\varphi's$ that is going to help me conclude strong convergence, that is: $||x_n-x||_p \to 0$ . I feel that the duality relation between $l^p$ and $l^q$ , where $\frac{1}{p}+\frac{1}{q}=1$ , can be helpful. Thank you in advance, every help will be very much appreciated.","['general-topology', 'functional-analysis', 'weak-convergence', 'weak-topology']"
4375866,Limit of a sequence of conditional expectations,"I was reading a proof of the property of conditional expectations that says that if $X_n \uparrow X$ then $\mathbb E[X_n|\mathcal G] \uparrow E[X|\mathcal G]$ in a given probability space $(\Omega, \mathcal F, \mathbb P)$ where $\mathcal G \subset \mathcal F$ . How can we ensure that $\lim_{n\to\infty} \mathbb E[X_n|\mathcal G]$ exists? I know that we have to use the monotonicity of the conditional expectations, but isn't it possible that such limit be infinity?","['conditional-probability', 'measure-theory', 'probability-theory']"
4375870,Set-theoretic notation: cardinality of infinite sets,"It seems nonstandard to use vertical bars $| \cdot|$ to denote the cardinality of a potentially infinite set, but the lecture notes I'm working through do so. It also uses $\leq$ , $>$ , and $<$ notation, but doesn't necessarily define them. My understanding from reading some questions on here is that the notation can be read as follows. We say $|S| \leq |T|$ if there is an injection $S \hookrightarrow T$ . This then means $|S| < |T|$ or $|S| = |T|$ , where the latter means means that there is a bijection between $S$ and $T$ . The former means there is no bijection; since there is an injection $S \to T$ , it must mean that there is no surjection from $S \to T$ . In this sense, $T$ is ""larger"" than $S$ , but in the infinite sense, it doesn't really make sense to talk about the size of a set in this way. The notes then flip the symbol around and write $|S| > |T|$ . I assume this just means exactly the same thing, but in reverse. Namely, $|S| < |T|$ if and only if $|T| < |S|$ . So there is an injection $T \to S$ but no surjection $T \to S$ . Are all of these correct?",['elementary-set-theory']
4375929,Minimizing the number of characters required to write a set of numbers,"Let $n$ be an integer, and $f(n)$ be the number of characters write $n$ in decimal number. Eg. $f(13) = 2$ while $f(-13) = 3$ since we need a an extra minus sign for the negative number. Given a set of positive integers $S$ , $\sum_{n \in S}f(n)$ is the total number of characters required to write all the numbers of $S$ . Eg. If $S = \{3, 4, 9, 10, 12, 19, 102 \}$ , then $\sum_{n \in S}f(n) = 12$ . In the above example, if we subtract $3$ from all numbers in $S$ we get a new set If $S_3 = \{0, 1, 6, 7, 9, 16, 99 \}$ , then $\sum_{n \in S_3}f(n) = 9$ . On the other hand if we $4$ from each number in $S$ , we get $S_4 = \{-1, 0, 5, 6, 8, 15, 98 \}$ and $\sum_{n \in S_4}f(n) = 10$ . Thus subtracting $4$ results in a set that requires more characters to write because of the negative sign in $-1$ . Now consider a sufficiently large set where subtracting a suitable number $c$ leads to one or more negative numbers but this tradeoff results in minimization of $\sum_{n \in S_c}f(n - c)$ . Trivially $c \ge min(S)$ . Question 1 : Let $S$ be a set of positive integers in $[a,b]$ . How can we find a integer $c$ such that the sum $\sum_{n \in S_c}f(n - c)$ is minimum? Question 2 : Can we do better than $c = min(S)$ if the distribution of the numbers in $S$ is known say the numbers are approximately normally distributed in $[a,b]$ or has a left/right skew?","['statistics', 'number-theory', 'elementary-number-theory', 'maxima-minima', 'optimization']"
4375939,Inverse of a 2x2 matrix with an example,"Need to find inverse of this matrix: $
\begin {bmatrix}
1 & 3/5\\
0 & 1\\
\end {bmatrix}
$ This is how it has been solved: $
\begin {bmatrix}
1 & 3/5\\
0 & 1\\
\end {bmatrix}
$ $
\begin {bmatrix}
x_1 & x_2\\
x_3 & x_4\\
\end {bmatrix}
$ = $
\begin {bmatrix}
1 & 0\\
0 & 1\\
\end {bmatrix}
$ - step 1 $
\begin {bmatrix}
1 & 0\\
0 & 1\\
\end {bmatrix}
$ $
\begin {bmatrix}
x_1 & x_2\\
x_3 & x_4\\
\end {bmatrix}
$ = $
\begin {bmatrix}
1 & -3/5\\
0 & 1\\
\end {bmatrix}
$ - step 2 From step 1 to step 2, -3/5 added to first row, second column of $
\begin {bmatrix}
1 & -3/5\\
0 & 1\\
\end {bmatrix}
$ Also, -3/5 to first row,second column of $
\begin {bmatrix}
1 & 0\\
0 & 1\\
\end {bmatrix}
$ My query is if it is correct to add or deduct a particular number from say first column, first row on the right side and on the left side and both leave the equation intact?","['matrices', 'gaussian-elimination', 'linear-algebra', 'inverse']"
4375944,Characterization of basis in terms of universal property: axiom of choice,"I wonder if the proof of the following statement requires the axiom of choice: (Characterization of basis in terms of universal property) Let $V$ be a vector space, and let $S$ be a non-empty subset of $U$ . Show that $S$ is a basis for $V$ if and only if for every vector space $W$ and every function $f : S → W$ , there exists a unique linear transformation $\tilde{f} : V → W$ such that $\tilde{f}(x) = f(x)$ for all $x \in S$ . The proof of the 'if' direction given in this answer certainly uses the axiom of choice, specifically this part: On the other hand, suppose $E$ is linearly independent but not spanning. $\require{color} \colorbox{yellow}{Extend $E$ to a basis $E'$}$ , with $x\in E'\setminus E$ .
Any $f:E\to Y$ extends to distinct functions $f_0,f_1:E'\to Y$ defined by $f_0(e)=f_1(e)=f(e)$ for $e\in E$ , $f_0(e)=f_1(e)=0$ for $e\in E'\setminus (E\cup\{x\})$ , and $f_0(x)=0$ , $f_1(x)=1$ . Any linear extension of either $f_1$ or of $f_2$ will be a linear extension of $f$ . By the forward direction you know that each $f_1$ and $f_2$ have linear extensions. Since $f_1\ne f_2$ , these will be distinct linear extensions of $f$ . I don't see any way to circumvent it. However, there is an obvious argument using Yoneda lemma that seemingly does not use the axiom of choice: Let $F,G: \operatorname{Set}\rightleftarrows \operatorname{Vec}_k$ be the free-forgetful adjunction.The universal property implies that $\operatorname{Set}(S,G(W))\cong \operatorname{Vec}_k(V,W)$ for every vector space $W$ naturally in $W$ (naturally follows from uniqueness of $\tilde{f}$ ). On the other hand, $\operatorname{Set}(S,G(W))\cong \operatorname{Vec}_k(F(S),W)$ . Thus, the representable functors $\operatorname{Vec}_k(V,-)$ and $\operatorname{Vec}_k(F(S),-)$ are naturally isomorphic and hence $V\cong F(S)$ by a corollary of Yoneda lemma and it follows that $S$ is a basis of $V$ . I would like to know whether the proof of the statement requires axiom of choice and if yes, when does the above argument uses the axiom of choice, and if no, how to modify the first proof to get around it.","['category-theory', 'universal-property', 'linear-algebra', 'axiom-of-choice', 'set-theory']"
