question_id,title,body,tags
1224472,Does concentration of measure inequality for a random matrix is enough to determine bounds on extremal singular value tail probabilities?,"First of all I do not know if this even qualifies to be a proper question, but I have a rather trivial doubt which I somehow could not resolve for the past few hours. So here it is. Let $A\in \mathbb{R}^{m\times N}$ be a random matrix which satisfies some concentration inequality of the form $$P\left(\left|\|Ax\|^2-E(\|Ax\|^2)\right|\ge \delta\right)\le 2\exp(-cf(\delta,m))\quad \forall x\in \mathbb{R}^N$$ for some $1>\delta>0$ and some function $f:\mathbb{R}\to \mathbb{R}$. From here it seems to me that this statement implies that $$P(\sigma_{max}(A)\le \sqrt{1+\delta})\ge 1-\exp(-cf(\delta,m))\\ P(\sigma_{min}(A)\ge \sqrt{1-\delta})\ge 1-\exp(-cf(\delta,m))$$ But I am not sure about it and in fact have seen papers where they talk about methods like Slepin-Gordon to use to find the extremal value probability bound. I think I am missing something. I suspect I am not considering the fact that for getting the tail bounds the former inequalities have to be satisfied $for\  all \ x\in \mathbb{R}^n$. Though this seems to me quite a trivial issue but it is bothering me constantly. So my question is, is my doubt correct or something more sinister is going on here? Thanks in advance.","['probability-theory', 'concentration-of-measure', 'random-matrices']"
1224479,Convergence of double-infinite series,"Let $r\in\mathbb{N}_{0}$ and $0<\lambda<1$. Prove that double-infinite series
$$\sum_{j=0}^{\infty}\sum_{k=j}^{\infty}k^r\lambda^k$$
is convergent.","['sequences-and-series', 'calculus']"
1224483,What is the cardinality of an infinite set of closed disjoint intervals?,"The question is : Let A be a set of a closed disjoint intervals, what is the cardinality of A ? I need to prove it using the Density property of the rational numbers in the real numbers. So i know that the cardinality of every interval is c, and it is closed under union so it has go be c as well, but i'm not sure that it is a countable union, kinda stuck. Thanks in advance !",['elementary-set-theory']
1224502,Limit of a sequence of random variables,"Suppose $Z_n$ is a sequence of independent random variables, which are uniformly picked from the interval $[1,2]$. Show that:
$$
\lim_{n_\rightarrow \infty}P\left(\left|\sqrt[n] {Z_1 Z_2\cdots Z_n}-\frac{4}{e}\right|>\epsilon\right)=0.
$$
I know I should probably apply the weak laws of large numbers, but I'm not sure how.","['probability-theory', 'limits', 'law-of-large-numbers', 'random-variables', 'probability']"
1224507,Exercise 2.12 in Harris - Algebraic Geoemetry: a first course,"Consider the three lines of $\mathbb{P}^3$ given by $L: \, z_0 = z_1 = 0 \\
M: \, z_2=z_3 = 0 \\
N: \, z_0 = z_2, \, z_1 = z_3.$ It is claimed in Exercise 2.12 of Harris (a first course) that the union of all lines that meet all $L,M,N$ is projectively equivalent to the segre variety $\Sigma_{1,1} = \sigma(\mathbb{P}^1 \times \mathbb{P}^1)$, where $\sigma$ is the Segre embedding. I have been thinking about this problem but i am still confused about some important things: 1) Conceptually, a line of $\mathbb{P}^3$ can be represented by a point of $\mathbb{P}^3 \times \mathbb{P}^3$. On the other hand $\Sigma_{1,1}$ lives inside $\mathbb{P}^3$. So i don't see how a point of $\Sigma_{1,1}$ may correspond to a line. 2) If i take a line that meets all three $L,M,N$, then the coefficients of the equations of the line satisfy three quadratic equations and that's precisely the space of lines that meet all three $L,M,N$. Now, these are 
three equations in $8$ unknowns and i can't see how they could be replaced by one equation in $4$ unknowns, which is the equation that describes $\Sigma_{1,1}$. Any insights?",['algebraic-geometry']
1224513,"If some of your boomerangs don't come back, how many throws will you get?","Let's say you're practicing throwing boomerangs. You're not an expert, and only 50% of the time does a boomerang return to you. So you stand out in a field with 16 boomerangs and start throwing them. After the first pass, 8 have come back, and you can throw them again. Out of those, 4 will come back, etc.  When you throw the last one, you will have made 31 throws altogether. If N is your starting number of boomerangs and P is the percent that come back, what is the formula for T, the total number of throws you can make? EDIT: Apparently I needed to add ""more details"", but since this is a problem that I kind of made up, I'm not sure what I could have left out. This situation was inspired by an online game I'm playing. My archer buys arrows by the stack. Normally each arrow in the stack can only be fired once, but there are also arrows which have a 75% chance of returning. I thought ""A stack of 100 of these arrows must be the equivalent of some-big-number of regular arrows."" I can get that equivalent number by using a spreadsheet. Rather than roll a die for each arrow, I assume that 75% of the stack would return the first time, then 75% of the remainder the next time, and so on. Although the spreadsheet works, I believed that there was some formula that would apply to any number of arrows with any specific return rate. Probably involving integrals, since (as was pointed out here) this was the sum of a bunch of related steps. In fact, I suspected that my hypothetical formula was already well-known, like all those in the textbooks involving coin-flipping and dice-throwing. But other than googling ""Boomerangs"" I couldn't be sure where to start. It seemed to make more sense to ask people who post about math for fun.","['statistics', 'algebra-precalculus']"
1224531,Closed form of the sum $\sum\limits_{n=0}^\infty \exp(-n^3)$,I am trying to calculate the sum of the series $$\sum_{n=0}^\infty \exp(-n^3)$$ Can it be expressed in terms of known mathematical functions?,"['closed-form', 'sequences-and-series', 'exponential-sum']"
1224581,If $p>3$ what are two solutions of $x^2 ≡ 4 \pmod p$?,"Theorem used: ""Suppose that $p$ is an odd prime. If $p \nmid a$, then $x^2 ≡ a \pmod p$ has exactly two solutions or no solutions."" Question: If $p>3$ what are two solutions of $x^2 ≡ 4 \pmod p$? Solution given in back: $2$ and $p-2$. I am unable to figure out how we get this answer? So far what I can think of is that: as $p>3$ and $p$ is a prime so $p$ must be $5$ or $7$ or $11$ something.
So all of them are greater than $4$. So $x^2 ≡ 4 \pmod p$ must have solutions like $4$ and $p-4$. How can we get $2$ there?","['number-theory', 'quadratic-forms', 'congruences', 'elementary-number-theory']"
1224620,Cone into cartesian coordinates,"Let $K$ be the surface of an inﬁnite cone with circular cross section, vertex at the origin and axis lying along the positive $z$-axis. If the angle between the $z$-axis and the surface of the cone is $α$, ﬁnd expressions for $K$ in terms of Cartesian, spherical and cylindrical polar coordinates. Solution: In terms of spherical polars, $K = \{(ρ,ϕ,θ) : 0 \leq ρ < ∞,ϕ = α,0 \leq θ < 2π\}$. By simple trigonometry, $tanα = r/z = (\sqrt{x^2 + y^2})/z$. In terms of Cartesian coodinates then, $K = \{(x,y,z) : x^2 +y^2 = z \tan α\}$ and, in terms of cylindrical polars, $K = \{(r,θ,z) : r = z tanα,0 \leq θ < 2π\}$. For the Cartesian, shouldn't it say $(z \tan α)^2$ instead, since it is the radius?",['multivariable-calculus']
1224672,Comparing limits of integrals,"If $$f_n:X\rightarrow [0,\infty]$$ is a sequence of measurable functions and we know that $$\lim_{n\rightarrow \infty }\int_X f_n \,d\mu=0,\qquad \qquad \tag{$\star$}$$ then can we conclude that for any measurable set $Y\subset X$ we have $$\lim_{n\rightarrow \infty }\int_Y f_n \,d\mu=0$$ because $0\leq\int_Y f_n \,d\mu\leq\int_X f_n \,d\mu$? Also, does the limit $(\star)$ imply that $\lim_{n\rightarrow \infty}f_n(x)=0$ almost everywhere?","['real-analysis', 'lebesgue-integral', 'measure-theory']"
1224698,Parametric differentiation,"The parametric equations of a curve are
$$\begin{cases}x(t)=e^{-t}\cos t\\y(t)=e^{-t}\sin t\end{cases}$$ Show that 
$$\frac{dy}{dx}= \tan\left(t-\frac{\pi}{4}\right)$$ I did the differentiation correct which is $$\frac{\sin t-\cos t}{\cos t+\sin t}$$ but I don't know how can I reach the final answer? how can this be changed to $\tan\left(t-\dfrac{\pi}{4}\right)$","['parametric', 'derivatives']"
1224703,"Example of Field Extension $E/F$ with $Char(F)=2$ and $[E:F]=2$, but is not Galois","I understand that for a field extension $E/F$, if $Char(F)\neq 2$ and $[E:F]=2$ then it must be a Galois Extension. I have proved this, but I am having trouble finding a counterexample when the characteristic requirement is dropped. My first thought was something like $\mathbb{F}_{2}(\sqrt{t})/\mathbb{F}_{2}(t)$. I am not sure, however, if this is a degree 2 extension. Regardless, the minimum irreducible polynomial of $\sqrt{t}$ over $\mathbb{F}_{2}(t)$ is $f(x) = x^{2}-t = 0$. Clearly, $(f,f')=f\neq1$ and, in $\mathbb{F}_{2}(\sqrt{t})$, $f$ splits into $(x-\sqrt{t})^{2}$, so it is purely inseparable. Is this enough to show that $\mathbb{F}_{2}(\sqrt{t})/\mathbb{F}_{2}(t)$ is an inseparable extension? And is it indeed degree 2?","['abstract-algebra', 'galois-theory', 'irreducible-polynomials']"
1224710,"How to prove $(\forall x,y\in\mathbb{Z})(5\nmid xy\to(5\nmid x\land 5\nmid y))$","Question: Prove $x,y\in\mathbb{Z},\Bigl((5\nmid xy)\to(5\nmid x\land 5\nmid y)\Bigr)$ where $\forall a,b\in\mathbb{Z},\bigl((a\nmid b)\leftrightarrow(\forall k\in\mathbb{Z},b\neq ak)\bigr)$ and $\forall a,b\in\mathbb{Z},\bigl((a\mid b)\leftrightarrow(\exists k\in\mathbb{Z},b=ak)\bigr)$. So my question is, what in the world is this problem asking and how do I begin to solve it? I know I have to use either proof by contradiction or contrapositive, but I'm struggling so much with this question. Could someone please help me?","['logic', 'quantifiers', 'discrete-mathematics', 'predicate-logic']"
1224747,Formal proof that $N$ is the smallest infinite set [duplicate],"This question already has answers here : Proving Dedekind finite implies finite assuming countable choice (2 answers) Closed 9 years ago . I wish to write a formal proof of the following statement: For any infinite set $X$, there exists an injection $f:\mathbb{N}\to X$. I'd like the proof to explicitly use the full axiom of choice (for every family of sets $\{S_\alpha\}$ there exists a family of elements $\{x_\alpha\}$ such that each $x_\alpha\in S_\alpha$). When this was asked before , none of the answers were explicit about where choice is invoked. Motivation: I'm TAing a course in discrete math and was embarrassed to find that I can't prove this homework question.","['elementary-set-theory', 'axiom-of-choice']"
1224748,Multivariate Delta Method,"If I have a $\sqrt{N}$ asymptotic normal estimator (call it $\boldsymbol{\theta}$, possibly a vector). Say I want to find the asymptotic distribution of $g(\boldsymbol{\theta})$ and suppose $g'(\boldsymbol{\theta})=0$ (assume $g(\cdot)$ real valued). How would the second order delta method derivation go along in this case? Can easily argue that the quadratic form in the Taylor expansion is asymptotically chi-square? In the univariate case this is trivial, but in the multivariate case there is dependency issues which does not seem obvious. Also, would the estimator converge at $\sqrt{N}$ speed? Thanks.","['asymptotics', 'statistics', 'convergence-divergence']"
1224750,All real values $a$ for a $2$-dimensional vector?,"Find all real numbers $a$ for which there exists a $2D$, nonzero vector $v$ such that:
$\begin{pmatrix} 2 & 12 \\ 2 & -3 \end{pmatrix} {v} = a {v}$. I substituted $v$ with $\begin{pmatrix} c \\ d \end{pmatrix}$ and multiplied to obtain the system of equations: $2x+12y = ax$ $2x-3y= ay$ Since the value $a$ is only of importance, I added the two equations to obtain $4x+9y = ax + ay$. Would that mean that $a = 4, 9$ is correct?","['geometry', 'calculus', 'algebra-precalculus', 'trigonometry']"
1224802,Sections of associated bundles,"Let $\pi:P\rightarrow M$ be a Principal bundle and $\pi_V:P\times_G F\rightarrow M$ be its associated bundle via the representation $\rho:G\rightarrow GL(V)$. Fact: $\Gamma(P\times_G V)\simeq\{f:P\rightarrow V: f(pg)=\rho(g^{-1})f(p)\}=:A$ I am trying to get a better understanding of this statement. Suppose $f\in A$. We want to show that this $f$ corresponds to a unique $\sigma\in \Gamma(P\times_G V)$. So unless I'm mistaken, given $f$, we want to construct a map $\sigma:M\rightarrow P\times_G V$ such that $\pi_V\circ \sigma=\text{id}$. Based on lecture notes found here , I think the first step in this process is to define the map $F:P\rightarrow P\times_G V$ by $F=[\text{id}\times f]$. Then $F(pg)=[pg,\rho(g^{-1})f(p)]=[p,f(p)]=F(p)$ The lecture notes then say this map 'descends' to a section $\sigma:M\rightarrow P\times_G V$. I am wondering how to construct this 'descent'? Is it done by defining $\sigma=F\circ \pi^{-1}$? Does this make sense? It seems to be 'well defined' since we just take any $x\in M$ to its fiber which is then mapped uniquely to an equivalence class. Just seems a bit dodgy as $\pi^{-1}$ isn't a function per say. Conversely, if $\sigma\in \Gamma(P\times_G V)$, then $\sigma$ 'lifts' to a map $F:P\rightarrow P\times_G V$ and since $\pi_F\circ \sigma=\text{id}$, it follows that $F=\text{id}\times f$. I can't seem to get my head around this explanation of the converse argument and I am hoping someone is able to help me.","['principal-bundles', 'lie-groups', 'differential-geometry', 'manifolds']"
1224839,How can I prove the inverse of a function is odd?,"Given function $f$, where $A⊆ \mathbb{R} $ is a symmetric domain with respect to 0,$ \;\; f:A \rightarrow\mathbb{R}$ and $f$ is an odd one-to-one function, I need to prove $f^{-1}$ is odd. I was originally thinking of using the integral and proving that the integral over a domain symmetric to 0 would be 0, but now I'm thinking that doesn't imply that the function is odd. How else could I go about proving $\;f^{-1}$ odd?","['calculus', 'functions']"
1224868,"Why, although these functions have the same derivative, do they not differ by a constant?",I calculated the derivative of $\arctan\left(\frac{1+x}{1-x}\right)$ to be $\frac{1}{1+x^2}$. This is the same as $(\arctan)'$. Why is there no $c$ that satisfies $\arctan\left(\frac{1+x}{1-x}\right) = \arctan(x) +c$?,"['derivatives', 'calculus', 'trigonometry']"
1224900,A $G_δ$ subset of $2^ω$ that is homeomorphic to $ω^ω$,"How do I show that there is a $G_δ$ subset of the Cantor space $2^ω$ that is homeomorphic to the Baire space $ω^ω$? I've been given the hint to consider $G = \{x ∈ 2^ω : x\text{ is not eventually constant}\}$, but I'm not entirely sure what to do with it. Any help is appreciated.","['descriptive-set-theory', 'the-baire-space', 'cantor-set', 'general-topology']"
1224916,"Find all Gaussian integers $α, β, γ$ such that $αβγ = α + β + γ = 1$","I tried to solve for this by assuming $α=a+bi$, $β=c+di$, and $γ=e+fi$, and explicitly solving this by equal $a+c+e=1$, $b+d+f=0$, and similarly for $αβγ=1$. Is there any other easier approach for this problem? I know $(1, i, -i)$  is a pair of solution. But is there any other?","['number-theory', 'algebraic-number-theory']"
1224918,"Show that the metric space C[a,b] is complete. [duplicate]","This question already has answers here : How to show that $C=C[0,1]$ is a Banach space (3 answers) Closed 9 years ago . Prove that the metric space $C[a,b]$ is complete. Where $C[a,b]$ is the collection of continuous $f:[a,b] → R$ and $||f|| = sup_{x \in [a,b]} |f(x)|$, such that $\rho (f,g) = ||f - g||$ is a metric on $C[a,b]$. attempt in proof: Recall that a metric space X is said to be complete if and only if every Cauchy sequence $x_n \in X$ converges to some point in $X$. Let $f_n$ be a Cauchy sequence in $ C[a,b]$, then $\forall \epsilon > 0,$ there is $N$ such that $||f_n - f_m|| < \epsilon$ for $n , m \geq N$ implies $||f_n - f_m || = sup |f_n - f_m | < \epsilon$. Now for $x \in [a,b]$, $|f_n - f_m | \leq sup_{x \in [a,b]} |f_n(x) - f_m(x)| < \epsilon$ for $n \geq N$. Thus $f_n(x)$ converges uniformly to $f(x)$ . And each $f_n$ is continous on $[a,b]$, and $f_n → f$ uniformly on $[a,b]$. Thus, $f \in C[a,b]$. So $C[a,b]$ is complete. Can someone please give some feedback? I don't know if I can conclude that $f_n$ converges uniformly to $f$. Can someone please help? Thank you in advance.","['metric-spaces', 'uniform-continuity', 'real-analysis', 'functions']"
1224919,"double decker, 13 card flush vs. 18+ of each color card. Who has the better odds of winning?","Two people, call them $A$ and $B$, decide to play a card game. They take 2 standard decks of playing cards and combine them into a ""superdeck"" of 104 cards, shuffle them well, and then draw 1 card at a time randomly without replacement. $A$ immediately wins if 13 cards of any single suit are drawn.  $B$ immediately wins if at least 18 black and at least 18 red cards are drawn. If they both ""win"" on the same card draw it is a tie (no decision) and they start over with all 104 cards after reshuffling. They bet even money—dollar for dollar matching. Who, if anyone, has the advantage of winning and by how much? Note that all cards are shared ""community"" cards so the players are not drawing their own set of cards. If someone would like to comment on if and how the probability changes if they draw their cards in tandem, that would be very interesting and informative. For example, two cards would be drawn at a time, giving one to $A$ and one to $B$, and then checked if anyone won or if they tied. This is an optional bonus question and not required to earn the checkmark for best answer. Simulating 1,000,000,000 (1 billion) decisions (including ties) of the original question (shared cards), I got the following winning percentages:
$$
A\quad 38.7855918\%\\
B\quad 59.7770491\%\\
\textrm{Tie}\quad 1.4373591\%
$$ Minimum # of cards : $14$ (A disappointment as I was expecting it to be $13$) Maximum # of cards : $42$ Average # of cards : $37.1902$ Note that at ""first glance"", some people might think that $A$ has the advantage because $A$ can win with as few as 13 cards but B needs 36 cards at a minimum to win. However, look at how much $B$ is actually favored to win. Another thing that somewhat surprised me is the average number of cards for someone to win (or tie) is about 37, only about 1 card more than $B$'s minimum needed to win. That might also imply that $A$ has an advantage. What if I reworded the question to also state that the average number of cards for a decision is slightly over 37, would most people then think at first glance that $A$ has an advantage, possibly a huge one? Yet another thing that might mislead people into thinking that $A$ has a large advantage is that $A$ is guaranteed not to lose if at least 42 cards are drawn. At that point either $A$ has won or tied. Kudos to the person that suspected that $B$ was the favorite from the start because he didn't let these false biases sway him towards $A$ being the favorite. UPDATE: I ran the alternate simulation giving each player their own cards and got some interesting results. $B$ remains the favorite at about $57.1\%$ to $A$'s $36.8\%$ which means the ties more than quadrupled to about $6.1\%$. The average number of cards slightly more than doubled from about 37 to about 75 with a low of 28 and a high of 94 (in ten million decisions). Perhaps one way to simplify getting the answer to this original problem is to start the game by immediately turning over 35 cards since $B$ cannot win with less than 36 cards. Check if $A$ won at 35 cards. If not, then draw cards 36 thru 42 one at a time checking for a winner.  By draw 42 someone will have won or there is a tie. So would the prob of $A$ winning on a 35 card draw be:
$$
\large
\frac{4\times {26 \choose 13} \times {78 \choose 22}}{{104 \choose 35}}
$$ That is about $10\%$. There may be two 13 card flushes in that but we don't care as long as there is at least 1. Also it is interesting to note that in the original game with shared cards, $42$ cards is the max for a win or tie because of the $25+17$ situation but with separate hands, that doesn't happen so the max # of cards drawn can exceed $84$ (which is $2$ * $42$) and in my simulation is did exceed it as it maxxed out at $94$ cards, not $84$.  For example, B could get $29$ black cards and $17$ red cards but A could have $12, 12, 11,$ and $11$ of each suit (respectively) so at that point, neither is a winner and $2$ more cards would be drawn (almost exhausting the deck of $104$).  The max number of cards ever for the separate card variation of this game seems like $94$ but I am not certain yet. Another interesting point is that simulation of the separate card variation is considerably slower because the average approximate number of cards needed is about double ($75$ vs. $37$) and can go as high as $94$ which is almost the entire deck so it becomes ""harder"" (slower).",['probability']
1224955,Proving that the second derivative of a convex function is nonnegative,"My task is as follows: Let $f:\mathbb{R}\to\mathbb{R}$ be a twice-differentiable function,
  and let $f$'s second derivative be continuous. Let $f$ be convex with
  the following definition of convexity: for any $a<b \in \mathbb{R}$:
  $$f\left(\frac{a+b}{2}\right) \leq \frac{f(a)+f(b)}{2}$$ Prove that
  $f'' \geq 0$ everywhere. I've thought of trying to show that there exists a $c$ in every $[a,b] \subset \mathbb{R}$ such that $f''(c) \geq 0$, and then just generalizing that, but I haven't been able to actually do it -- I don't know how to approach this. I'm thinking that I should use the mean-value theorem. I've also thought about picking $a < v < w < b$ and then using the MVT on $[a,v]$ and $[w,b]$ to identify points in these intervals and then to take the second derivative between them, and showing that it's nonnegative. However I'm really having trouble even formalizing any of these thoughts: I can't even get to a any statements about $f'$. I've looked at a few proofs of similar statements, but they used different definitions of convexity, and I haven't really been able to bend them to my situation. I'd appreciate any help/hints/sketches of proofs or directions.","['convex-analysis', 'calculus', 'real-analysis', 'derivatives']"
1225041,Entropy upper bound inequality for Sub-Gaussian Random Variable,"We say that the random variable $Z$ is $\sigma^2$-subGaussian if $\mathbb{E} \exp(tZ) \leq \exp(t^2\sigma^2)/2$. Define the $(x\log x)$-entropy (or simply the entropy) of a nonnegative random variable $Z$ by $\text{Ent}(Z):= \mathbb{E}(Z\log Z)- \mathbb{E}Z \log (\mathbb{E}Z)$. Here $\log$ is the natural logarithm. I am interested to get the following bound: If $X-\mathbb{E}(X)$ is $\sigma^2$-subGaussian, then $\text{Ent}(\exp (t X))\leq t^2\alpha \;\mathbb{E}(\exp(tX)) $ for any $t\geq 0$, and $\alpha$ is some constant depending on $\sigma$. How do I get the above bound? I have tried using Jensen's inequality and various manipulations but could not get the above.","['probability-theory', 'expectation', 'random-variables', 'probability', 'inequality']"
1225049,RH would follow from $\displaystyle \frac{p_{n+1}}{p_{n+1}-1}<\frac{\log\log N_{n+1}}{\log\log N_n} $ for all $n>1$; what is my mistake?,"Let $N_n=\prod_{k=1}^np_k$ be the primorial of order $n$,$\gamma$ be the Euler-Mascheroni constant and $\varphi$ denote the Euler phi function. Nicolas showed that if the Riemann Hypothesis is true, then $$\forall \ n\in\mathbb{Z^+}, \ \frac{N_n}{\varphi(N_n)}=\prod_{k=1}^n\frac{p_k}{p_k-1}>e^\gamma \log\log N_n.\tag{NI}$$He also proved that the falsity of RH would imply the existence of infinitely many $n$ violating NI, as well as infinitely many $n$ satisfying it. Now, the combination of the following statement, equivalent to PNT:$$\lim_{n\to\infty}\frac{p_n}{\log N_n}=1$$ and Mertens' third theorem $$\lim_{n\to\infty}\frac{1}{\log p_n}\prod_{k=1}^n\frac{p_k}{p_k-1}=e^\gamma$$ yields $$\lim_{n\to\infty}\frac{1}{\log\log N_n}\prod_{k=1}^n\frac{p_k}{p_k-1}=e^\gamma,$$ thus NI would follow from the decreasingness, even only for large enough $n$, of the sequence $$u_n=\frac{1}{\log\log N_n}\prod_{k=1}^n\frac{p_k}{p_k-1}.$$ Therefore, since $\displaystyle \log \log 2 $ is negative, let us consider for $n>1$ $$\frac{u_{n+1}}{u_n}=\frac{\log\log N_n}{\log\log N_{n+1}}\frac{p_{n+1}}{p_{n+1}-1}<1 $$ $$\frac{p_{n+1}}{p_{n+1}-1}<\frac{\log\log N_{n+1}}{\log \log N_n}=\frac{\log\log N_{n+1}}{\log(\log N_{n+1}-\log p_{n+1})}.\tag{$\star$}$$ 
We have $\log N_{n+1}<p_{n+1}$, wherefrom, for any real $r>\log p_{n+1}+1,$ $$\frac{1}{\log(r-\log\log N_{n+1})}<\frac{1}{\log(r-\log p_{n+1})},$$ so as long as $\log N_{n+1}>\log p_{n+1}+1$ for $n>1$, $(\star)$ is weaker than  $$\frac{p_{n+1}}{p_{n+1}-1}<\frac{\log\log N_{n+1}}{\log(\log N_{n+1}-\log \log N_{n+1})}.\tag{1}$$ But $\displaystyle f(x)=\frac{\log x}{\log(x-\log x)}$ is decreasing on $(1,+\infty)$: $$\require\cancel f'(x)= \frac{\log(x-\log x)/x - \log x \cdot D(\log(x-\log x))}{\cancel{\log^2(x-\log x)}}<0 \\ \frac{\log(x-\log x)}{x}<\frac{(1-1/x)\log x}{x-\log x}=\frac{(x-1)\log x}{{x}(x-\log x)} \\ (x-\log x)\log(x-\log x)<(x-1)\log x \\ (x-\log x)^{x-\log x}<x^{x-1},$$ which follows from $$(x-\log x)^x < x^{x-1} \\ \left(1-\frac{\log x}{x}\right)^x<x^{-1} \\ \left(1-\frac{\log x}{x}\right)^{x/\log x}<x^{-1/\log x} =e^{-1},$$ which, setting $\displaystyle t=\frac{x}{\log x}$, reduces to the familiar $$\left(1-\frac{1}{t}\right)^t<e^{-1}.\tag{2}$$ With $x$ in place of $t$ we would say $(2)$ holds for $x>1$, but given that the range of $\displaystyle \frac{x}{\log x}$ does not include $[0,e)$, we really need $t>0$, or equivalently, again, $x>1$. Hence $(1)$ is in turn implied by $$\frac{p_{n+1}}{p_{n+1}-1}<\frac{\log p_{n+1}}{\log(p_{n+1}-\log p_{n+1})} \\ p_{n+1}\log(p_{n+1}-\log p_{n+1})<(p_{n+1}-1)\log p_{n+1} \\ (p_{n+1}-\log p_{n+1})^{p_{n+1}}<p_{n+1}^{p_{n+1}-1},$$ and this we have already showed to ensue from $(2)$. Thence, $(\star)$ is true and so is NI. However, not only such a result would prove RH, but in this paper it is shown that such a result would confute Cramér's conjecture . On account of this I expect to have made some stupid mistake, I just cannot see it. Thank you for any observations.","['riemann-hypothesis', 'prime-numbers', 'number-theory', 'proof-verification', 'real-analysis']"
1225112,"Prove that if $a,b \in \mathbb{R}$ and $|a-b|\lt 5$, then $|b|\lt|a|+5.$","I'm trying to prove that if $a,b \in \mathbb{R}$ and $|a-b|\lt 5$, then $|b|\lt|a|+5.$ I've first written down $-5\lt a-b \lt5$ and have tried to add different things from all sides of the inequality. Like adding $b+5$ to get $b\lt a+5 \lt 10+b$ but am just not seeing where that gets me.","['arithmetic', 'absolute-value', 'algebra-precalculus', 'inequality']"
1225117,Differential identity $\left(x^2\frac{d}{dx}\right)^nf(x)=x^{n+1}\frac{d^n}{dx^n}\left(x^{n-1}f(x)\right)$,"I have found the following differential identity:
$$\left(-x^2\frac{d}{dx}\right)^nf(x)=(-1)^n x^{n+1}\frac{d^n}{dx^n}\left(x^{n-1}f(x)\right)$$
I have used it to find an alternative Rodrigues representation for Bessel polynomials. My proof we have placed: $D=\frac{d}{dx}$ in order to save space. We start observing that, for n=1 the following is trivial that: \begin{equation}
\left(x^{2}D\right)=x^{n+1}D^{n}x^{n-1},n=1
\end{equation} Therefore for n=1 the identity holds. For induction we proof that,
if the identity holds for n, then it holds also for n+1. Applying
the operator$\left(x^{2}D\right)$ to both members: \begin{equation}
\left(x^{2}D\right)\left(x^{2}D\right)^{n}=\left(x^{2}D\right)\left(x^{n+1}D^{n}x^{n-1}\right)
\end{equation} \begin{equation}
\left(x^{2}D\right)^{n+1}=x^{2}Dxx^{n}D^{n}x^{n-1}=x^{2}Dx^{n}xD^{n}x^{n-1}
\end{equation} remembering the following commutation rules: \begin{equation}
Dx^{n}\centerdot-x^{n}D=nx^{n-1}
\end{equation} \begin{equation}
D^{n}x\centerdot-xD^{n}=nD^{n-1}
\end{equation} we can write: \begin{equation}
\left(x^{2}D\right)\left(x^{2}D\right)^{n}=x^{2}\left[Dx^{n}\centerdot\right]\left[xD^{n}\right]x^{n-1}=x^{2}\left[x^{n}D+nx^{n-1}\right]\left[D^{n}x\centerdot-nD^{n-1}\right]x^{n-1}
\end{equation}
\begin{equation}
=x^{n+2}D^{n+1}x^{n}\centerdot+x^{2}\left[nx^{n-1}D^{n}x\centerdot-x^{n}DnD^{n-1}-nx^{n-1}nD^{n-1}\right]x^{n-1}
\end{equation} Now we can show that the term in square parentheses is null, indeed
applying above seen commutation rules: \begin{equation}
=x^{n+2}D^{n+1}x^{n}\centerdot+x^{2}\left[nx^{n-1}xD^{n}+nx^{n-1}nD^{n-1}-nx^{n}D{}^{n}-nx^{n-1}nD^{n-1}\right]x^{n-1}=
\end{equation} \begin{equation}
=x^{n+2}D^{n+1}x^{n}\centerdot+x^{2}\left[nx^{n}D^{n}-nx^{n}D{}^{n}+nx^{n-1}nD^{n-1}-nx^{n-1}nD^{n-1}\right]x^{n-1}=x^{n+2}D^{n+1}x^{n}\centerdot
\end{equation} Therefore: \begin{equation}
\left(x^{2}D\right)^{n+1}=\left(x^{2}D\right)\left(x^{2}D\right)^{n}=x^{n+2}D^{n+1}x^{n}\centerdot
\end{equation} Questions Is my proof correct? Is this identity already known? If yes, what are the references? Where can I find a possibly more elegant proof? Edit So, it seems the identity was true, and already known.
In ""Mathematical Analysis I"" (Springer), Zorich V.A, in chapter ""The Basic Rules of Differentiation"", exercise 3 at page 213, the identity of $D^nf(1/x)=(-1)^nx^{n+1}D^n x^{n-1}f(1/x)$ has to be demonstrated. Unfortunately no hint was given and no proof is sketched, so I remain without a more straightforward proof ...)",['derivatives']
1225122,Ideals Generated by polynomials,"So I am currently studying a course in commutative algebra and the main object that we are looking at are ideals generated by polynomials in n variables. But the one thing I don't understand when working with these ideals is when we reduce the generating set to something much simpler. For e.g. Consider the Ideal   $I$ = $<x^2-4x + 3, x^2 +x -2>$, then since $x -1$ is a common  factor of both the polynomials in the generating set we deduce that I is infact $<x-1>$. So my question is what is the criteria that applies when we are reducing the generating set to something much simpler. Based on what I understand, I am guessing in the above example that since every polynomial is divisible by $x-1$ we can say the ideal is generated by $x-1$ (wouldn't this result in the loss of any elements?). But I am not entirely convinced by my reasoning and would prefer to hear it from someone who understands this stuff better. Also using the same reasoning as above can we then say that the ideal $I$ = $<x^3 - x^2 + x>$ = $<x>$ ?","['abstract-algebra', 'polynomials', 'ideals']"
1225138,Comparing Eigenvalues of Positive Semidefinite Matrices,"If $B\succeq A$, show that $\lambda_n(B)\ge\lambda_n(A)$, where $\lambda_n$ is the $n$th largest eigenvalue. It is Theorem 6 in this paper but the proof is only given as ""by characterization."" Do they mean the min/max characterization of eigenvalues,
$$\lambda_k=\min\max\frac{x^TAx}{x^Tx}$$
and if so, how is that applied? Thanks.","['linear-algebra', 'matrices']"
1225148,Differentiating matrix exponential,"I know that $$\frac{d}{dt}e^{At} = Ae^{At}$$ However, in one lecture, I found the following $$\frac{d}{dt}e^{A^Tt} = e^{A^Tt}A^T$$ The lecture is as follows How to show the second case? Why not $A^Te^{A^Tt}$ ?","['matrices', 'matrix-exponential', 'matrix-calculus', 'linear-algebra', 'derivatives']"
1225152,Expected number of rolls on a die until each face has appeared at least twice,"Note: The die is fair, normal 1 - 6 die. So I understand that the expected number of rolls until each face occurs is 14.7 by the following post: Expected time to roll all 1 through 6 on a die but what is the expected number of rolls until each face has appeared at least 2 times?","['probability-theory', 'probability']"
1225216,Mean Value Theorem Inequality Contradiction(?),"I am trying to show: $e^x > 1+x+\frac{x^2}{2}$ for $x>0$ using Mean Value Theorem (MVT). My method is as follow: consider the function $f(x)=e^x - (1+x)$. We know $f'(x) = e^x -1 > 0 $ for $x>0$. Thus, by MVT, exists a $c \in ]0,x[$ such that, $$f'(c)=\frac{f(x)-f(0)}{x-0}=\frac{e^x-(1+x)}{x}$$ 
Since we know $f'(c) > 0$, the statement is equivalent to,
$$ \frac{e^x -(1+x)}{x} > 0 \Rightarrow e^x - 1> x \Leftrightarrow f'(x)>x $$
for $x>0$. Now we can update the original inequality of $f'(c)$, giving
 $$f'(c)=\frac{e^x-(1+x)}{x} > x \Leftrightarrow e^x > 1+x+x^2 $$
for $x>0$. However, this is not the same as the intended inequality, and is certainly not true, letting $x=1$ yields $e > 3$. What happened here? May someone explain? Thank you so much!","['inequality', 'calculus', 'limits', 'functions']"
1225243,"Prove that $\dfrac{g(x,u_{n})}{\left\Vert u_{n}\right\Vert ^{p-1}}\rightarrow g_{0}$ weakly in $L^{\overline{p}}$ for some $\overline{p}>p*'$","Let $\Omega
 \subset
  \mathbb{R}^{N}$
  be a smooth bounded domain , $g:\Omega\times\mathbb{R}\rightarrow\mathbb{R}$
  is a Caratheodory function such that $g(x,t)=0$
  for $t\leq0$
 . Suppose that there exist function $a\in L^{r}$
  and $d\in L^{p'}$
  such that $\left|g(x,t)\right|\leq a(x)t^{p-1}+d(x)$ with $r>N/p$
  if $1<p\leq N$
  and $r=1$
  if $p>N$
 ; $p'$
  is Holder conjugate of $p$ Let $\left\{ u_{n}\right\} \subset W_{0}^{1,p}$
  be a sequence such that $\left\Vert u_{n}\right\Vert \rightarrow\infty$
  as $n\rightarrow\infty$
 . Let us define $v_{n}=u_{n}/\left\Vert u_{n}\right\Vert $
 . Hence $\left\Vert v_{n}\right\Vert =1$
  and we may assume that $v_{n}\rightarrow v$
  weakly in $W_{0}^{1,p}$
 . Prove that $\dfrac{g(x,u_{n})}{\left\Vert u_{n}\right\Vert ^{p-1}}\rightarrow g_{0}$
  weakly in $L^{\overline{p}}$
  for some $\overline{p}>p*'$
  if $p<N$
  and $\overline{p}=1$
  if $p\geq N$
 . Here are my efforts: Firstly consider $p<N$
  , my intension is: prove that $\dfrac{g(x,u_{n})}{\left\Vert u_{n}\right\Vert ^{p-1}}$
  is bounded in $L^{\overline{p}}$
  for some $\overline{p}>p*'$ $\dfrac{g(x,u_{n})}{\left\Vert u_{n}\right\Vert ^{p-1}}\leq a(x)\left|v_{n}\right|^{p-1}+\dfrac{d(x)}{\left\Vert u_{n}\right\Vert ^{p-1}}$ ${\displaystyle \int_{\Omega}}\left|a(x)\left|v_{n}\right|^{p-1}\right|^{\delta}dx\leq{\displaystyle \int_{\Omega}}\left|a(x)\right|^{\delta}\left|v_{n}\right|^{(p-1)\delta}dx$ $\leq\left\Vert a(x)^{\delta}\right\Vert _{L^{\frac{N}{p\delta}}}\left\Vert \left|v_{n}\right|^{(p-1)\delta}\right\Vert _{L^{\frac{N}{N-p\delta}}}$ $\leq\left\Vert a(x)^{\delta}\right\Vert _{L^{\frac{N}{p\delta}}}\left\Vert v_{n}\right\Vert _{L^{\frac{N(p-1)\delta}{N-p\delta}}}^{(p-1)\delta}$ I expect that by Sobolev embedding, $\left\Vert a(x)^{\delta}\right\Vert _{L^{\frac{N}{p\delta}}}\left\Vert v_{n}\right\Vert _{L^{\frac{N(p-1)\delta}{N-p\delta}}}^{(p-1)\delta}\leq C\left\Vert a(x)^{\delta}\right\Vert _{L^{\frac{N}{p\delta}}}\left\Vert v_{n}\right\Vert ^{(p-1)\delta}=C\left\Vert a(x)^{\delta}\right\Vert _{L^{\frac{N}{p\delta}}}$
 , so we are done. Thus we have to pick $\delta$
  such that ${\displaystyle \frac{N(p-1)\delta}{N-p\delta}}<\dfrac{Np}{N-p}
 \Longleftrightarrow\delta<\dfrac{pN}{Np-N+p}=p*'$ My trouble is when choose $\overline{p}=\delta$
 , we are done that $\dfrac{g(x,u_{n})}{\left\Vert u_{n}\right\Vert ^{p-1}}$
  is bounded in $L^{\overline{p}}$
  with $\overline{p}<p*'$
  (not $\overline{p}>p*'$
  ). On the other hand if choose $\overline{p}'=\delta$
 , we are done that $\overline{p}>p*'$
  but $\dfrac{g(x,u_{n})}{\left\Vert u_{n}\right\Vert ^{p-1}}$
  is bounded in $L^{\overline{p}'}$
  , not $L^{\overline{p}}$
  . Please help me to take out that trouble. I appreciate your help.","['sobolev-spaces', 'real-analysis', 'functional-analysis']"
1225256,Indefinite integrals of trigonometry,"I have big problems with the following integrals: $$\int\frac{dx}{\sin^6 x+\cos^6x}$$ $$\int\frac{\sin^2x}{\sin x+2\cos x}dx$$ It isn't nice of me but I almost have no idea, yet I tried the trigonometric substitution $\;t=\tan\frac x2\;$ , but I obtained terrible things and can't do the rational function integral. Perhaps there is exist some trigonometry equalities? I tried also $$\frac1{\sin^6x+\cos^6x}=\frac{\sec^6x}{1+\tan^6x}=\frac13\frac{3\sec^2x\tan^2x}{1+\left(\tan^3\right)^2}\cdot\overbrace{\frac1{\sin^2x\cos^2x}}^{=\frac14\sin^22x}$$ and then doing parts with $$u=\frac14\sin^22x\;\;:\;\;u'=\sin2x\cos2x=\frac12\sin4x\\{}\\v'=\frac13\frac{3\sec^2x\tan^2x}{1+\left(\tan^3\right)^2}\;\;:\;\;v=\arctan\tan^3x$$ But it is impossible to me doing the integral of $\;u'v\;$  . Any help is greatly appreciated","['calculus', 'integration']"
1225277,Any subfield of $\mathbb{C}$ must contain every rational number,"I tried to prove that any subfield of $\mathbb{C}$ must contain every rational number by contradiction. Proof: Let $\mathbb{F}$ be any subfield of $\mathbb{C}$. Thus, $\mathbb{F}$ is itself  a  field  under  the  usual  operations  of  addition  and  multiplication  of  complex  numbers. For $\mathbb{F}$ to be a field, $0$ and $1$ must belong to $\mathbb{F}$. We have to prove that any subfield of $\mathbb{C}$ must contain every rational number. Let us assume on the contrary that there exists at least one rational number $q \neq 0$ such that $q \notin \mathbb{F}$. Let $-q$ and $\frac{1}{q}$ are in $\mathbb{F}$. Then since $\mathbb{F}$ is a field, it must contain additive inverse of $-q$ and multiplicative inverse of $\frac{1}{q}$; i.e. $q$ must be in $\mathbb{F}$. But, according to our assumption, $q \notin \mathbb{F}$; which implies that $\mathbb{F}$ is not a field. Thus we arrive at a contradiction. Now, let $-q$ and $1/q$ are also not in $\mathbb{F}$ ( edit - here I mean to say all elements of $\mathbb{Q}$, except $q$, $-q$ and $\frac{1}{q}$, are in $\mathbb{F}$). Then $\mathbb{F}$ does not satisfy closure under addition and multiplication, which again leads to the contradiction that $\mathbb{F}$ is not a field. Hence any subfield of $\mathbb{C}$ must contain every rational number. Q.E.D. I just need feedback on whether it is correct and how I can improve it (especially the last portion). Also, can we modify the statement of the result into - The set of rational numbers is the smallest subfield of $\mathbb{C}$ ? Thanks. Edit: Thank you all of you for your valuable feedback. I think I did not write my arguments in the second part of the proof quite clearly. Here is how my chain of thoughts were - Since I assumed that at least one element $q$ of rational number is not in $\mathbb{F}$, so there may be more than one element of $\mathbb{Q}$ that are not in $\mathbb{F}$. This is why I assumed in the second part the non presence of $−q$ and $q^{−1}$. Now, consider for example all rational numbers, except $2$, $−2$ and $0.5$, are in $\mathbb{F}$ (as $q$, $-q$ and $q^{-1}$ in second part). Then $1−3=−2$ , such that $1,3 \in \mathbb{F}$ implies that closure does not hold. May be I should have added something like - Since $1, -(q+1)\in\mathbb{F}$, then by closure $1+{-(q+1)} = -q \in \mathbb{F}$. But $-q \notin \mathbb{F}$ which implies closures does not hold. I hope my arguments are convincing enough. I look forward to more comments.  Thanks.. Edit 2 - I realized where I made mistake in the above proof. Thank you all of you for your comments...:)",['abstract-algebra']
1225287,"There are n balls and m colors, calculate the ways that color 1 appear most","Also, color 1 can be as many as others. for example: there 2 balls and 3 colors, we can color like that: 1 1, 1 2, 1 3, 2 1, 2 2, 2 3, 3 1, 3 2, 3 3 and 1 1, 1 2, 1 3, 2 1, 3 1 are the valid answer, so the answer is 5. but how to calculate with n,m?","['discrete-mathematics', 'combinatorics']"
1225288,Number of permutations with subset distance constraint,"The problem is to calculate the number of all unique permutations of a string with repetitions. There is also a constraint for one subset elements to be spaced from each other. Typical input data is AAAAAABBBCCCC . Lets assign to L the length of input string. The goal is to find the number of all possible permutations of this string, in which any two C 's must not to be placed next each other. As I understand, the problem can be splitted into 2 subproblems: Count number of unique permutations of AAAAAABBB (N1); Count number of correct possible placements of all (4 in our case) C 's in L (N2); Multiply N1 and N2 to get the answer. N1 can be calculated as: L!/(La! * Lb!) . How should I calculate N2? I'm not interested in direct answer, just looking for some clarification. My attempt on manual synthesis of the formula was very awful (because we have multiple intervals, and at every element task splits into two). Do this kind of task is likely to be solved by dynamic programming? Thanks in advance!","['constraints', 'combinatorics', 'permutations']"
1225309,$A$ is a sum of two postive integer squares?,"if $x,y,z,w$ be postive integer,and such $x^2+y^2$ is prime number,and  $A=\dfrac{w^2+z^2}{x^2+y^2}\in N^{+}$ show that
$A$ is a sum of  two  postive integer squares? maybe 
$$A=\dfrac{(w^2+z^2)(x^2+y^2)}{(x^2+y^2)}=\dfrac{(wx+zy)^2}{(x^2+y^2)^2}+\dfrac{(wy-xz)^2}{(x^2+y^2)^2}$$
But I can't prove $\dfrac{wx+zy}{x^2+y^2},\dfrac{(wy-xz)}{x^2+y^2}\in Z$","['prime-numbers', 'number-theory', 'square-numbers', 'elementary-number-theory']"
1225340,$F(x)+G(y)= e^{x+y}?$,"Are there functions $F(x)$, $G(y)$, such that
 $F(x)+G(y)=e^{x+y}$ , where $x,y$ are real numbers? I have been trying all elementary functions, and have no clues on what else I could do.","['exponential-function', 'functions']"
1225355,"Show that the substitution $t=\tan\theta$ transforms the integral ${\int}\frac{d\theta}{9\cos^2\theta+\sin^2\theta}$, into ${\int}\frac{dt}{9+t^2}$","To begin with the $d\theta$ on the top of the fraction threw me off but I'm assuming it's just another way of representing: $${\int}\frac{1}{9\cos^2\theta+\sin^2\theta}\,d\theta$$ I tried working backwards $$\frac{d}{d\theta}\tan\theta=\sec^2\theta\,\,\,\,{\Rightarrow}\,\,\,\,d\,\tan\theta=\sec^2\theta\,d\theta$$
$${\Rightarrow}\,{\int}\frac{\sec^2\theta\,d\theta}{9+\tan^2\theta}$$ $$\tan\theta=\frac{\sin\theta}{\cos\theta}\,\,\,\,{\Rightarrow}\,\,\,\,\tan^2\theta=\frac{\sin^2\theta}{\cos^2\theta}$$ $${\Rightarrow}\,{\int}\frac{\sec^2\theta\,d\theta}{\left(9+\dfrac{\sin^2\theta}{\cos^2\theta}\right)}$$ $$9=\frac{9\cos^2\theta}{\cos^2\theta}\,\,\,\,{\Rightarrow}\,\,\,\,{\int}\frac{\sec^2\theta\,d\theta}{\left(\dfrac{9\cos^2\theta}{\cos^2\theta}+\dfrac{\sin^2\theta}{\cos^2\theta}\right)}\,\,\,\,{\Rightarrow}\,\,\,\,{\int}\frac{\sec^2\theta\,d\theta}{\left(\dfrac{9\cos^2\theta+\sin^2\theta}{\cos^2\theta}\right)}$$ $${\Rightarrow}\,\,\,\,{\int}\frac{\color{red}{\cos^2\theta\,\sec^2\theta}\,d\theta}{9\cos^2\theta+\sin^2\theta}$$ Now I have to prove that $$\cos^2\theta\,\sec^2\theta=1$$
but I don't think it is... What have I done wrong? Regards Tom",['trigonometry']
1225377,"English for ""prolongement"" or ""Fortsetzung""?","I'm sorry if that's not the right place to ask for, wikipedia failed to give me the correct word... What's the English for a function that is defined on a larger domain than the original function and coincides with it on the original domain? Is it ""extension"" or ""continuation"" or something else?","['terminology', 'mathematical-german', 'functions', 'translation-request', 'mathematical-french']"
1225389,What is the difference between ring homomorphism and module homomorphism?,"I know that $\mathbb{Z}$ and $2\mathbb{Z}$ are isomorphic as Z-modules. But they are not isomorphic as ring. I used to think that I can look at isomorphism as being ""equal"". However, now it seems that it is not really expressing ""equality"" as there are different kinds of homomorphism. What then does isomorphism actually expresses. How can I think about it intuitively? I know by definition they are different, I just want to have an intuition of their difference.","['abstract-algebra', 'modules', 'ring-theory']"
1225405,If $K$ and $H$ are subgroups of $G$ and $H \triangleleft K$ then $K \subseteq N(H)$.,We can easily prove the truth of this statement. My question is that why do we not simply say $K=N(H)$ $?$ I'd be really grateful for an elaboration on this.,"['abstract-algebra', 'group-theory', 'normal-subgroups']"
1225435,Dimension of set of Hermitian matrices commuting with a given matrix,"Given a Hermitian matrix $A$, what is the dimension of the set of all other Hermitian matrices $B$ such that $[A,B] = 0$. It is clearly not the same for all $A$, but how can one find it for a given $A$.","['lie-algebras', 'matrices']"
1225445,Abelian groups axioms with minus in place of plus,"An abelian group is a set equipped with a binary operation $+$, a unary operation $-$ and a nullary operation (constant) $0$, satisfying certain axioms (associativity, unity, etc).
I wonder if it is possible to describe the same structure, that is that of abelian group, using a binary operation $-$ and a constant $0$. I think it is possible by setting $a-b=a+(-b)$ where in the LHS there is the binary $-$ I am defining, while in the RHS we have the usually binary $+$ and the unary ""inverse"" operator $-$. But which should be the axioms for $-$ in order to have an abelian group?",['abstract-algebra']
1225451,Exercise in R.Vakil 18.4.L: Ample line bundles and finite morphism,"Here's the question: R.Vakil, Exercise 18.4.L : Suppose $\mathcal{L}$ is a base-point free invertible sheaf on a proper variety $X$, and hence induces some morphism $\phi: X\rightarrow\mathbb{P}^n$. Then $\mathcal{L}$ is ample if and only if $\phi$ is finite. So suppose that $\phi$ is finite, then by the hint in Ex 16.6.G, $\phi^{*}\mathcal{O}_{\mathbb{P}^n}(1)$ is ample and this is $\mathcal{L}$ (hope I am correct here). But for the converse here is the hint: If $\phi$ is not finite, show that there is a curve $C$ contracted by $\pi$, using theorem 18.1.9. Show that $\mathcal{L}$ has degree 0 on $C$. I am completely lost here. What is $\pi$? So I looked up at the theorem and it says Theorem 18.1.9 Suppose $\pi:X\rightarrow Y$ with $Y$ noetherian. Then $\pi$ is projective and has finite fibers if and only if it is finite. Equivalently $\pi$ is projective and quasifinite if and only if it is finite. Question 1 : How should I make sense of the ''contractible'' part and how do I use the theorem to prove it? Question 2 : Assume that the contractible part is done. I need to show that $\mathcal{L}$ has degree zero on $C$. Why? I tried Riemann-Roch but at least I can't get anything out of it. Any hints?","['algebraic-geometry', 'curves', 'projective-schemes']"
1225458,In a transitive relation does x and z have to be the same element?,"I am new to relations on sets and am trying to get my head around transitive relations. I understand the definition of $(x,y) \in R, (y,z) \in R$ and $(x,z) \in R$
However what i am not sure about is if $x$ and $z$ need to be the same number. For example if I have the following relation:
$R = \{(1,2), (1,3), (1,4), (2,1), (2,3), (2,4), (3,1), (3,2), (3,4), (4,1), (4,1), (4,3)\}$ Answer 1:
Can you this be transitive if you say $(2,1)$ is an element of $R$ and $(1,4)$ is an element of $R$ so is $(2,4)$ which makes this transitive. Answer 2:
Or does the definition mean that if $(2,1)$ is there $(1,2)$ is there but $(2,2)$ is not there and it is not transitive. I am not sure of which one of the answers is correct and why?","['elementary-set-theory', 'relations']"
1225471,Why is Green's theorem asymmetric in $x$ and $y$?,"Green's theorem is $$\oint_{\partial D} (P\, dx+Q\, dy) = \iint_D dx\,dy \: \left ( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}\right)$$ where one can see that the RHS is asymmetric in $x$ and $y$. Why is this, and what is the physical significance? I suspect the answer has to do with using a right-handed coordinate system (e.g. one can use Stokes' theorem to show the above, which involves a vector product, which is right-handed), or possibly that the boundary is traversed anti-clockwise, but I'm unable to make any deeper or more accurate statement than that...","['vector-analysis', 'greens-theorem', 'calculus', 'multivariable-calculus']"
1225559,How to eliminate other possibilities of $p$? [duplicate],"This question already has an answer here : Qualifying Exam Question On Elementary Group Theory (1 answer) Closed 9 years ago . Let $G$  be a finite group, $p$ be  the smallest prime divisor of $|G|$, and $x \in  G $ an element of order p. Suppose
$h \in  G $ is such that $hxh ^{−1} = x ^{10} $. Show that $p=3$ My try: Since $o(x)=p$ and $hxh^{-1}$ and $x$ being conjugates have the same order we have $o(hxh^{-1})=o(x^{10})=p$ Also $o(x^{10})=\dfrac{p}{\gcd(10,p)}=p$ Then $\gcd(10,p)=1$ then $10^{p-1}\equiv 1(\mod p)$ I can see that $p=3$ satisfies the equation .Also $p\neq 2,5$.How to eliminate other possibilities of $p$?",['group-theory']
1225602,Identifying a sequence of numbers from an optimization problem in $L^1$,"Question Does there exist general closed form solutions (or some sort of recurrence relation) to the system of equations:
$$\begin{align}
x_0 &= -1\\
x_{k+1} &= 1\\
\sum_{j = 0}^k (-1)^j (x_{j+1} - x_{j}) & = 0 \\
\sum_{j = 0}^k (-1)^j (x_{j+1}^2 - x_{j}^2) &= 0 \\
\vdots \\
\sum_{j = 0}^k (-1)^j (x_{j+1}^k - x_{j}^k) &= 0
\end{align}$$
with $-1 \leq x_1 \leq x_2 \leq \ldots \leq x_k \leq 1$? For $k$ small the solutions can be computed by hand to be $k = 1$: $\{0\}$ $k = 2$: $\{\pm \frac12\}$ $k = 3$: $\{0, \pm\frac12\}$ $k = 4$: $\{(\pm 1\pm\sqrt{5})/4\}$ $k = 5$: $\{0, \pm\frac12, \pm\frac{\sqrt{3}}2\}$ The non-trivial part of the system can be summarised as
$$  \begin{pmatrix}
x_0 & x_1 & x_2 & \ldots & x_k & x_{k+1} \\
x_0^2 & x_1^2 & x_2^2 & \ldots & x_k^2 & x_{k+1}^2 \\
x_0^3 & x_1^3 & x_2^3 & \ldots & x_k^3 & x_{k+1}^3 \\
& \vdots \\
x_0^k & x_1^k & x_2^k & \ldots & x_k^k & x_{k+1}^k \end{pmatrix}
\begin{pmatrix}
-1 \\
2\\
-2 \\
2 \\
-2 \\
\vdots \\
(-1)^{k-1} 2\\
(-1)^k\end{pmatrix} = 0 $$
if it helps. Motivation This question asked for the minimizer among linear functions of the functional $\int_0^1 |e^x - \ell(x)|dx$. The solution given by anonymous user surprised me slightly: it turns out that there is a universal construction that is independent of the target function (in this case, $e^x$). Namely, the user has actually proven that Theorem Let $f\in C^2([-1,1])$ such that $f''$ is strictly signed. Then the minimizer of the functional $\int_{-1}^1 |f(x) - \ell(x)|dx$ among linear functions $\ell(x)$ is attained when 
$$ \ell(x) = (x + 1/2) \left( f(1/2) - f(-1/2)\right) + f(-1/2) $$
in other words, it is the linear interpolation function between $(-1/2, f(-1/2))$ and $(1/2, f(1/2))$. I gave another write-up of the proof here on my blog . It turns out that the theorem extends to the case of ""higher order convexity"", in the following sense: Theorem There exists a list of numbers $\{x_1, \ldots, x_k\}\subset [-1,1]$ such that for every $f\in C^k([-1,1])$ such that $f^{(k)}$ is strictly signed, the minimizer of the functional $\int_{-1}^1 |f(x) - p(x)|dx$ among polynomials of degree at most $k-1$ is attained by the $(k-1)$-order polynomial interpolation based on the points $(x_j, f(x_j))$. I wrote up a proof again on my blog . The question above is asking for general closed form expressions of the numbers $\{x_1, \ldots, x_k\}$ in terms of $k$. (If someone happens to know a literature reference for this result, that would also be very nice.)","['systems-of-equations', 'sequences-and-series', 'reference-request', 'polynomials']"
1225626,Solving second order differential equation in a hurry,"During last test I had to solve second order ODE: $y'' + 3y = e^x + 1$. I managed to write the general solution of homogenous equation (with RHS replaced by zero):
$$y = c_1 \cos \left(\sqrt{3} x\right) + c_2 \sin \left(\sqrt{3} x\right).$$ That wasn't hard. But how to find (at least one) the general solution? Using sosmath (Method of Variation of Parameters) I found out that:
\begin{align}
	u_1 & = \sqrt{3} \int \sin \left(\sqrt{3} x\right) (e^x + 1) \,\textrm{d}x =  \frac{\sqrt{3}}{12} \cdot \left(3e^x \sin \left(\sqrt 3 x\right) - \sqrt 3 (4+3e^x) \cos \left(\sqrt 3 x\right) \right) + C_1 \\
	u_2 & = \sqrt{3} \int \cos \left(\sqrt{3} x\right) (e^x + 1) \,\textrm{d}x = \frac{\sqrt{3}}{12} \cdot \left(3e^x \cos \left(\sqrt 3 x\right) + \sqrt 3 (4+3e^x) \sin \left(\sqrt 3 x\right) \right) + C_2,
\end{align}
after what I don't know how to proceed. There must be a simpler way, because one can guess the solution to be $$y(x) = \frac{4 + 3e^x}{12}.$$",['ordinary-differential-equations']
1225630,Rule of 72 doubling time,"I need some help understanding this. So as far as I can tell. The rule of 72 is used to determine when prices will double in years. This is done by 72 divided by the rate, or interest. So it would look like 72/6 if the percent was only 6. Which would equal 12 years. But here's my issues, there is the rule of 72 doubling time and actual doubling time. Can someone please explain this part. Like if the percent is one then the rule of 72 doubling time is 72 years but actual doubling time in only 70 years. How is this actually calculated? Can someone show me and actually equation. I will love you long time lol.","['finance', 'algebra-precalculus']"
1225633,Is the isosurface of a smooth function a smooth surface?,"suppose $ f(x)\in C_c^{\infty}(R^n, R)$, an infinitely differentiable function with compact support, from $R^n$ to $R$. If $f\not\equiv 0$, is the boundary of its support, i.e. $\partial\{x\in R^n: f(x)\neq 0\}$, always a smooth surface of $n-1$-dimension in $R^n$? Or at least piece-wise smooth?
(of course it may comprise of disjoint components) Thanks a lot :-D",['multivariable-calculus']
1225657,Representations of a quiver and sheaves on P^1,We know from Beilinson that there's an equivalence of derived categories $D^b Rep(Q) \simeq D^b Coh(\mathbb{P}^1)$ where the lefthandside is the derived category of bounded complexes of representations of the Kronecker quiver $* => *$ and the righthandside is the derived category of bounded complexes of coherent sheaves on projective space. My question is: Is there a proof that $Rep(Q) \not \simeq Coh(\mathbb{P}^1)$ as abelian categories?,"['quiver', 'algebraic-geometry', 'representation-theory']"
1225659,"Why is the relation ""$f=g $ almost everywhere"" transitive?","In Rudin's Real and Complex Analysis, it says on pg 27 that If $\mu$ be a measure, define $f\sim g$ iff $\mu(\{x|f(x)≠g(x)\})=0$, where $f,g$ are measurable functions from $X$ to a topological space $Y$. Then, $f\sim g$ is an equivalence relation. But, I do not see why $f\sim g$ is transitive. If I assume $f\sim g$ and $g\sim h$, then by definition, $\{x|f(x)≠g(x)\}$ and $\{x|g(x)≠h(x)\}$ are measurable sets with measure zero. But then, how do I know that the set $\{x|f(x)≠h(x)\} \subset \{x|f(x)≠g(x)\} \cup  \{x|g(x)≠h(x)\} $ is measurable? Here $Y$ is a topological space, not necessarily $R$ or extended reals.",['measure-theory']
1225665,Does $[G:Z(G)] =n \implies x \mapsto x^{n}$ a group homomorphism?,"Let $G$ be a finite group, and let $Z(G)$ denote the center of $G$. Its a known result that if $G/Z(G)$ is cyclic then $G$ is Abelian. So we see that if $|G|/|Z(G)|=p$, where $p$ is a prime, then $G$ becomes an Abelian group and so the map $f: G \to G$ defined by $f(x)=x^{p}$ is a group homomorphism. Can the question be generalized for any positive integer $n$? That is: If $|G|/|Z(G)|=n$, then does it imply that the map $f: G \to G$ defined by $f(x)=x^{n}$ is a group homomorphism??","['abstract-algebra', 'group-theory']"
1225667,The maximum value of expression $ \sqrt{\sin^2x+ 2a^2} - \sqrt{-1 -\cos^2x+ 2a^2} $,"If $a,x\in\Bbb R$, what is the maximum value of the expression 
$ \sqrt{\sin^2x+ 2a^2} - \sqrt{-1 -\cos^2x+ 2a^2} $? I tried to differentiate but it became messy.","['algebra-precalculus', 'trigonometry']"
1225717,square numbers whose only digits are 0 and 1,Does there exist any perfect square integer (other than $10^{2k}$) whose digits are only $0$ and $1$ in base 10 expression? This just comes up in a leisure talk with my friends. Is that elementary or hard to prove? Any comment will be helpful. some discussion i found on mathoverflow: https://mathoverflow.net/questions/22/can-n2-have-only-digits-0-and-1-other-than-n-10k,['number-theory']
1225729,Factor out the following expression $ -5 a^3 b^3 c + 125abc $,Recently I was doing some factoring exercises and encountered the following problem. The idea is to simplify the expression. $$ -5  a^3  b^3  c + 125abc $$ We find the GCD and bring it outside $$ 5abc(-a^2b^2+25) $$ Now we apply Binomial Decomposition $$ -a^2b^2+25 = (5+ab)(5-ab) $$ And here is the part I don't get. Could you please explain why it became $ (5+ab)(5-ab) $ in the expression? And not $ (-ab+5)(ab+5) $ What is the rule that allows us to swap expression/change the sign? I hope you understand what I don't understand ? Thank you,['algebra-precalculus']
1225762,"Can we recover étale, fpqc etc. morphisms of schemes from the affine versions?","$\DeclareMathOperator{\Spec}{Spec}$
Consider the following procedure for defining a class of morphisms of schemes: Take a suitable class of homomorphisms of rings (e.g., canonical maps to localization at an element, standard étale homomorphisms, faithfully flat homomorphisms). Identify the category of rings with the opposite category of affine schemes.  This identifies our chosen ring homomorphisms with certain morphisms of affine schemes. Consider the morphisms $f\colon X\to Y$ of schemes with the following property: whenever $\Spec A$ is an affine scheme with a morphism $\Spec A\to Y$, the fibre product $\Spec A\times_Y X$ is affine and the natural map $\Spec A\times_Y X\to\Spec A$ corresponds to one of our chosen ring morphisms from step (1). I'm really interested in this process because I think it might give a way to define the Zariski/étale/flat topologies by defining a topology on the opposite category of commutative rings and then extending it as follows: Identify the category of schemes with the full subcategory of the topos of sheaves on $\mathbf{CRing}^{\text{op}}$ given by the functor of points (assuming that these functors of points are indeed sheaves). Take the canonical topology on the topos of sheaves, and restrict to the category of schemes. Some pages on the nLab seem to suggest that this should yield certain important topologies on the category of schemes. If we start with the Grothendieck pretopology on $\mathbf{Aff}$ given by jointly surjective families of morphisms corresponding to localization-at-a-single-element maps, do we get the Zariski topology? If we start with the pretopology given by jointly surjective families of formal duals of standard étale homomorphisms, do we get the étale topology? If we start with the pretopology given by jointly surjective families of formal duals of faithfully flat homomorphisms, do we get the flat topology? At the ring level, jointly surjective should correspond to jointly injective , since localization maps, standard étale homomorphisms etc. behave nicely topologically (they give closed maps). Pretopologies for the Zariski, étale and flat topologies are given by jointly surjective families of Zariski embeddings, étale morphisms and fpqc/fppf morphisms respectively. In this case, the procedure defined at the start of the question does the following (replacing [...]/[***] with localization map/Zariski embedding , standard étale/étale and faithfully flat/fpqc ): Take the topology on $\mathbf{Aff}$, given by jointly surjective families of [...] homomorphisms. Form a pretopology generating the canonical topology on the topos of presheaves on $\mathbf{Aff}$ in a standard way, and restrict it to the category of schemes. This pretopology should correspond to jointly surjective families of certain morphisms of schemes.  Then the [***] morphisms are those morphisms that appear in a covering family. Is this correct?  For example, are the étale morphism of schemes precisely those morphisms $f\colon X\to Y$ that appear in some covering family in the canonical extension to the category of schemes of the 'standard étale topology' on the opposite category of commutative rings?  More concretely, are the étale morphisms precisely those morphisms $f\colon X\to Y$ such that whenever $\Spec A$ is an affine scheme with a morphism $\Spec A\to Y$, the fibre product $\Spec A\times_Y X$ is affine (say $\Spec A\times_Y X\cong\Spec B$) and the natural map $\Spec A\times_Y X\to\Spec A$ corresponds to a standard étale morphism $B\to A$ ?","['algebraic-geometry', 'topos-theory', 'schemes', 'grothendieck-topologies']"
1225779,Conceptual question on Probability,"today I've stumbled in a interesting question about probabilities while talking to a friend, I don't know whether here is the best place to ask, but as it involves probabilities, I'll try. Given a set of numbers $1,\ldots,N$ as a sample space in a lottery ticket, we have to mark a subset of $M$ numbers. Why do we have a common sense that we have better chances choosing a sparse subset of numbers instead of a contiguous subset. Isn't the probability the same, or is there deeper math involved that I'm not aware of? Thanks in advance.","['probability-theory', 'probability']"
1225797,solving PDE with state-dependent boundary conditions,"I am interested in solving the following PDE (heat equation):
$$\frac{\partial u}{\partial t} = \kappa \frac{\partial ^2 u}{\partial x^2}$$
In order to solve it, I discretize space uniformly into $N$ segments and convert the PDE into $N+1$ ODEs:
$$ \frac{du_k}{dt} = \kappa\frac{u_{k+1} + u_{k-1} - 2u_k}{(\Delta x)^2} \quad k=0,1\ldots.N$$
which I can solve using any ODE routine (such as ode45 or ode15s in MATLAB).
For the particular case of my problem, the Neumann boundary conditions for the PDE are dependent on the values of $u_k$ itself, i.e 
$$ \frac{du_b}{dx}= \left\{
\begin{array}{cc}
\alpha & u_b < u_{critical}, \\
-\gamma u_b & u_b \geq u_{critical}
\end{array} \right. $$
where $\alpha$ and $\gamma$ are constants and $b=\{0,N\}$. Assuming that I am solving the above problem using some numerical ODE solver (such as ode45 in MATLAB), and there are two routines: odefun (which I provide to the solver) to evaluate the derivatives and odestep which i define and the solver calls it after every successful integration step, where should I check and change the boundary conditions, in odefun or odestep ? I am asking this because the odefun routine might be used internally by the solver for evaluation of jacobian etc and ideally, we should not change boundary conditions in the middle of an integration step, right? PS. I was not sure which was the better place to ask this question, programming stack exchange or maths stack exchange. I asked it here because its essentially a math problem.","['heat-equation', 'boundary-value-problem', 'matlab', 'ordinary-differential-equations', 'partial-differential-equations']"
1225810,"Lengths of the shortest ""simple"" equation, that use only the number '1', equal to a given natural numbers.","Is there a formula, for determining the length of the shortest formula, that uses only the number ' 1 ', parenthesis , and the hyper operations $\{\{+, - \}, \{\times, / \}, \{\text{^}, \log_N,\text{nth root}\}, \{↑↑,\ldots\}, \ldots \}$ , that is equal to a given natural number? The trend is simple at first, before gradually becoming very complex, below is a list of what I think should be the first few (first 400) elements, followed by a list of their lengths. $$\small{1\\1+1\\1+1+1\\1+1+1+1\\1+1+1+1+1\\1+1+1+1+1+1\\1+1+1+1+1+1+1\\(1+1)\text{^}(1+1+1)\\(1+1+1)\text{^}(1+1)\\(1+1+1)\text{^}(1+1)+1\\(1+1+1)\text{^}(1+1)+1+1\\(1+1+1+1)\times(1+1+1)\\(1+1+1+1)\times(1+1+1)+1\\(1+1)↑↑(1+1+1)-1-1\\(1+1)↑↑(1+1+1)-1\\(1+1)↑↑(1+1+1)\\(1+1)↑↑(1+1+1)+1\\(1+1)↑↑(1+1+1)+1+1\\(1+1)↑↑(1+1+1)+1+1+1\\(1+1)↑↑(1+1+1)+1+1+1+1\\(1+1)↑↑(1+1+1)+1+1+1+1+1\\(1+1+1)↑↑(1+1)-1-1-1-1-1\\(1+1+1)↑↑(1+1)-1-1-1-1\\(1+1+1)↑↑(1+1)-1-1-1\\(1+1+1)↑↑(1+1)-1-1\\(1+1+1)↑↑(1+1)-1\\(1+1+1)↑↑(1+1)\\(1+1+1)↑↑(1+1)+1\\(1+1+1)↑↑(1+1)+1+1\\(1+1+1)↑↑(1+1)+1+1+1\\(1+1)\text{^}(1+1+1+1+1)-1\\(1+1)\text{^}(1+1+1+1+1)\\\ldots\\(1+1+1)↑↑(1+1)\times(1+1)\\\ldots\\(1+1)\text{^}(1+1+1+1+1+1)\\\ldots\\(1+1+1)\text{^}(1+1+1+1)\\\ldots\\(1+1+1)↑↑(1+1)\times(1+1+1+1)\\\ldots\\(1+1+1+1+1)\text{^}(1+1+1)\\(1+1+1+1+1)↑↑(1+1+1)+1\\(1+1+1+1)↑↑(1+1)/(1+1)-1\\(1+1+1+1)↑↑(1+1)/(1+1)\\\ldots\\(1+1+1+1)↑↑(1+1)\\\ldots\\(1+1+1+1)↑↑(1+1)*(1+1+1)/(1+1)\\(1+1)↑↑(1+1+1)\times(1+1+1+1+1)\text{^}(1+1)\\\ \\1, 3, 5, 7, 9, 11, 13, 13, 13, 15, 17, 17, 19, 17, 15, 13\\15, 17, 19, 21, 23, 23, 21, 19, 17, 15, 13, 15, 17, 19, 19, 17\\\ldots}$$ In answer to, Elaqqad, if so restrict operations to the first few , perhaps with some info about why this is necessary. Thanks to Ross Millikan for showing that with just ' 1 ', parenthesis , $+$ and $\times$, the answer is the sequence at oeis.org/A005245 , and additionally with powers is oeis.org/A025280 . Thease sequences decelerate fantastically :) Thease sequences take only the number of $1$s into account and ignore factors such as parenthesis complexity. From the $2$nd sequence, numbers that are more complicated to construct than any smaller number include: $1, 2, 3, 4,\\5, 7, 11, 13, 21, 23, \ldots$ Of which are not prime: $4, 21, \ldots$ From my sequences above: Numbers that are more complicated to construct than any smaller number include: $1, 2, 3, 4, 5, 6,\\7, 10, 11, 13, 20, 21, \ldots$ Of which are not prime: $4, 6, 10, 20, 21\ldots$ The table below shows the first appearance of operations, although the larger values may yet prove inaccurate: $+\ 2\\-\ 14\\\times\ 12\\/\ 384\\\text{^}\ 9\\↑↑\ 16$",['combinatorics']
1225833,Proximal operator for the nuclear matrix norm of Hankel matrix,"I have a problem in hand for which I need to compute the proximal operator of the composite function $ {\left\| \mbox{Hankel} (x) \right\|}_{\ast} $ where $ x \in \mathbb R^N $ and $ \left\| \cdot \right\|_{\ast} $ denotes the matrix nuclear norm. For a general matrix $X$ , the proximal map of the $\| X \|_{\ast}$ becomes a soft-thresholding of the singular values. I'll be grateful if somebody help me to evaluate the proximal map of $\| \mbox{Hankel} (x)\|_{\ast}$ .","['nuclear-norm', 'matrices', 'hankel-matrices', 'proximal-operators', 'convex-analysis']"
1225870,Find the Laurent Expansion of $f(z)$,"Find the Laurent Expansion for
$$f(z)=\frac{1}{z^4+z^2}$$
about $z=0$. I have found the partial fraction decomposition
$$f(z)=\frac{1}{z^4+z^2}=\frac{1}{z^2}-\frac{1}{2i(z-i)}+\frac{1}{2i(z+i)}.$$ Next I wanted to expand each of the three terms separately.  I have $$\frac{1}{z^2}=\frac{1}{z^2},$$
$$\frac{1}{2i(z-i)}=-\frac{1}{2z}i\sum_{n=0}^{\infty}\left(\frac{i}{z}\right)^n,\quad |z|>1$$
$$\frac{1}{2i(z+i)}=-\frac{1}{2z}i\sum_{n=0}^{\infty}\left(-\frac{i}{z}\right)^n,\quad |z
|>1.$$ Therefore, I believe that my Laurent expansion should be
$$\frac{1}{z^4+z^2}=\frac{1}{z^2}+\frac{1}{2z}i\sum_{n=0}^{\infty}\left(\frac{i}{z}\right)^n-\frac{1}{2z}i\sum_{n=0}^{\infty}\left(-\frac{i}{z}\right)^n,\quad |z|>1.$$ I had a few questions, though. 1) What about the $z$ in the denominators outside the sums?  What's that all about? 2) Does the same radius of convergence $|z|>1$ apply for $\frac{1}{z^2}$ as did for the other two series?  What does it mean to expand about $z=0$, and yet the radius of convergence for those two expansions above are $|z|>0$? 3) Can I do anything to clean up this answer?","['power-series', 'laurent-series', 'calculus', 'complex-analysis']"
1225899,Convolution with Heaviside function (integration),"To clarify notation, I use $u_n = 1$ when $x>n$, and $0$ otherwise. I am having troubles with the following convolution/integration: $u_2(t) \ast sin(\sqrt{2}t) = \int^t_0u_2(\tau) \cdot sin(\sqrt{2}(t-\tau))\ d\tau$. At first I thought of splitting the integral up so that I can make the Heaviside function some definitive value (0 or 1) on an interval, but I do not know how that would work since $t$ has no specific value. That leads me to think perhaps my problem is that I am not very good at integration. Any hints or tips will be appreciated.","['convolution', 'ordinary-differential-equations', 'integration']"
1225906,Finding Equation of tangent line,"Can someone double check my work to see if I'm doing it correctly? Find the equation of the line tangent to the graph of $(2,1)$ where $f$ is given by $f(x) = 2x^3 - 2x^2 + 1$ 1) $f'(x) = 6x^2-4x$ (First I found derivative) 2) $f'(2) = 6(2)^2-4(2) = 16$ (Then found slope by plugging $x$ coordinate into derivative) 3) $y-1 = 16(x-2) =$ (Then I plugged slope, $x$, and $y$ into point slope formula and solved) $y = 16x - 31$","['calculus', 'derivatives']"
1225933,Wedge Product Formula For Sine. Analogous Formula Generalizing Cosine to Higher Dimensions?,"So I was day dreaming about linear algebra today (in a class which had nothing to do with linear algebra), when I stumbled across an interesting relationship. I was thinking about how determinants are really the area spanned by column vectors, and I had the thought that one could measure linear independence (in $R^2$ in this case) on a scale from 0 to 1 by taking the ratio $\frac{|det(\vec{a},\vec{b})|}{|\vec{a}||\vec{b}|}$ where $det(\vec{a},\vec{b})$ is the determinant of a matrix whose column vectors are $\vec{a}$ and $\vec{b}$. Then I remembered the definition of the cross product in $R^2$,  $\vec{a}\times\vec{b}= \begin{vmatrix}i&j&k\\a_1&a_2&0\\b_1&b_2&0
\end{vmatrix} = \begin{vmatrix}a_1&a_2\\b_1&b_2\end{vmatrix}k = det(\vec{a},\vec{b}) k$ . Taking the absolute value, we have $|\vec{a}\times\vec{b}| = |\vec{a}||\vec{b}|sin(\theta) = |det(\vec{a},\vec{b})|$. Finally, rearranging we have $sin(\theta) = \frac{|det(\vec{a},\vec{b})|}{|\vec{a}||\vec{b}|}$. This can be thought of as comparing actual area spanned to maximum possible area spanned.
With this definition, we can generalize this notion of measuring linear independence to higher dimensions (e.g. for $R^3$, $sin(\theta) = \frac{|det(\vec{a},\vec{b},\vec{c})|}{|\vec{a}||\vec{b}||\vec{c}|}$). Now clearly this isn't ""really"" $sin(\theta)$, but the notion should be the same, i.e. comparing actual spanned volume to maximum possible spanned volume. Finally we note that $|det(\vec{a},\vec{b},\vec{c})|$ may also be written as $|\vec{a}\wedge\vec{b}\wedge\vec{c}|$, allowing us to write $sin(\theta) = \frac{|\vec{a}\wedge\vec{b}\wedge\vec{c}|}{|\vec{a}||\vec{b}||\vec{c}|}$. Which Leads Us To My Real Question: Now, in $R^2$, we have the equation for cosine, $cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{|\vec{a}||\vec{b}|}$, which looks an awful lot like the corresponding equation for sine, $sin(\theta) = \frac{|\vec{a}\wedge\vec{b}|}{|\vec{a}||\vec{b}|}$. The difference is, we have no problem generalizing the equation for sine to an arbitrary amount of dimensions. We simply need to tack on another wedge product, i.e. $sin(\theta) = \frac{|\vec{v_1}\wedge\vec{v_2}\cdots\wedge\vec{v_n}|}{|\vec{v_1}||\vec{v_2}|\cdots|\vec{v_n}|}$. My question is: Is there a similar way to generalize cosine using a simple combination of products in the numerator? Perhaps in geometric algebra? Maybe it would somehow extend the analogy of cosine being the ratio of actual projection to maximum possible projection? Thanks in advance! Edit: To anyone who is interested, I figured out since the denominator of the cosine generalization would be $|\vec{v}|^3$ for 3 vectors $\vec{v}$ that are all in the same direction. Therefore, the numerator would need to equal  $|\vec{v}|^3$ to yield an answer of 1 in the case that all the vectors are parallel.","['correlation', 'determinant', 'linear-algebra', 'trigonometry']"
1225935,Two Definitions of Critical Points,"Let $f:\mathbb{R}^n\to\mathbb{R}^m$ be given such that it is continuously differentiable. According to Wikipedia , a ""critical point"" of $f$ is a point $p\in\mathbb{R}^n$ such that: 1) According to one paragraph it is the condition that $\partial_j f_i = 0$ for all $j\in\{1,\dots,n\}$ and $i\in\{1,\dots,m\}$. 2) According to another paragraph, it is the condition that the matrix with the $(i,j)$ component $\partial_j f_i$ (i.e. the Jacobian matrix) has rank less than $m$. It seems like these two definitions are not equivalent. Why are there two different definitions? I can understand why the first one is called ""critical point"": it gives exactly the points which are local maximum, minimum or saddle points. Why is the second one called ""critical point""? What does it give, except for saying that the function at this point is not surjective?","['analysis', 'multivariable-calculus']"
1226026,Where can I find Wielandt's original proof of Sylow's Theorem?,"I have seen several proofs of Sylow's Theorem based on Wielandt's method. Everyone gives credit to Wielandt's proof of Sylow's theorem, but ironically everyone puts their own spin on it. Where can I find the original Wielandt's proof of Sylow's theorem?","['alternative-proof', 'soft-question', 'abstract-algebra', 'group-theory', 'finite-groups']"
1226029,Prime ideals in $\mathbb{Q}(\sqrt{-5})$ above rational prime ideals,"I'm really struggling to understand the concept of prime ideals lying above and below a given prime ideal. For example taking the extension $\mathbb{Q}(\sqrt{-5})\big/\mathbb{Q}$, how do we know $(2, 1+ \sqrt{-5})$ the only ideal in the bigger field lying above $(2)$? And why do $(3, 1+\sqrt{-5})$ and $(3, 1-\sqrt{-5})$ lie above $(3)$? (Sorry I appreciate that this is probably very elementary but I'm new to algebraic number theory and all the books I have just skip over this point assuming it is obvious to the reader which sadly it's not...)","['maximal-and-prime-ideals', 'number-theory', 'algebraic-number-theory']"
1226030,Odd divisibility induction proof,Prove that for odd $n>3$ $$64\ | \ n^4-18n^2+17$$ I checked that for $n=5$ it works. I think I need to assume that for $2n+1$ it holds and show that $2n+3$ also holds. Any ideas?,"['number-theory', 'divisibility', 'induction']"
1226045,Clear up definition of cayley graph,"I have come across two definitions of Cayley graphs, both very similar but one being more general. I have been working with the more general definition which is: A Cayley graph of a group 􏰎$X$ with a subset􏰐 $S \subset X$ 􏰏, is defined by taking X to be the vertex set of the Cayley graph, with directed edges $(g,h)$ whenever $gh^{-1} \in S$. However in other texts i have read that $S$ needs to be a generating set of $X$, this stronger version implies that the Cayley graph will be connected. I understand that the cayley graph depends on the choice of $S$ as this defines the edges and intuitively get why the graph would be connected if the set generates the group, however i am struggling to prove it formally. I want to be able to link the two definitions in my notes by proving the graph is connected. any help on providing a proof as to why the graph would be connected would be much appreciated, thank you.","['cayley-graphs', 'graph-theory', 'group-theory']"
1226051,To whom do we owe this construction of angles and trigonometry?,"I've come across what is, to me, the most precise, beautiful and thorough definition of what we know of as the angle between two vectors. I say this because most literature either skims over things and starts talking about angles all of the sudden, or uses a contrived definition like the unique $\theta\in[0,\pi]$ such that $\|u\|\|v\|\cos\theta = u\cdot v$ . Yes it works fine, but it leaves me quite unsatisfied; I should like to already have an ""intrinsic"" definition of an angle, and then let the cosine function be defined to tell me things about angles. Why, the other way around we have to define cosine by some magical power series, and then it turns out that the above definition makes angles behave as they should! To me it seems disingenuous, but thoughts on this are welcome. Anyway, the construction comes from a book called Àlgebra Lineal i Geometria (Castellet/Llerena), and I'd like to know if anyone has seen it, other than in this book of course. I'll post the beginning of the section (translated from Catalan, and paraphrased): Let $(E,\langle\cdot,\cdot\rangle)$ be a $2$ -dimensional Euclidean space. In the set of pairs of unit vectors, define the equivalence relationship $$(u,u')\sim(v,v') \iff \exists f\in SO(2) : f(u)=v,\, f(u') = v'$$ This condition is proven to be equivalent to $\exists g\in SO(2) : g(u)=u',\, g(v) = v'$ . We define an angle as one of these equivalence classes. We denote the class represented by $(u,u')$ as $[(u,u')] = \widehat{uu'}$ . This can be easily extended to $E\times E$ by defining $\widehat{uv}$ as the angle defined by $\frac{u}{\|u\|},\frac{v}{\|v\|}$ . Call the set of angles $A =E\times E_{\large{/\sim}}$ and for any $u\in E$ define a map $$SO(2)\longrightarrow A \atop \qquad\qquad\, f\mapsto \widehat{uf(u)}$$ This is in fact a bijection, and allows us to transport the operation in $SO(2)$ to $A$ : given $\alpha,\beta\in A$ with preimages $f,g$ respectively, define the sum $\alpha+\beta$ as the image of $f\circ g$ . In class notation: $$\left.\begin{align}&\alpha = [(u,f(u))] \\ &\beta = [(f(u),gf(u))]\end{align}\right\}\Rightarrow \alpha+\beta = [(u,gf(u))]$$ Naturally, this sum has the same properties in $A$ as does the operation in $SO(2)$ . $A$ is then an abelian group, whose identity is $0 = \widehat{uu}$ . The inverse, or opposite angle of $\widehat{uf(u)}$ is $\widehat{f(u)u}$ . Here comes the fun part. By fixing an orientation on $E$ , each $f\in SO(2)$ has a corresponding matrix $$\left(\begin{array}{ccc}a & -b \\ b & a\end{array}\right)\text{ with } a^2+b^2 = 1$$ Let $\alpha$ be the angle corresponding to $f$ . We define the cosine , and the sine , of $\alpha$ by $$\cos\alpha = a\qquad \sin\alpha = b$$ Too clever. And it gets better: We'll make a couple of observations now about this definition. Firstly, in changing the orientation of $E$ the sign of $\sin\alpha$ changes, but not $\cos\alpha$ . One has $$\cos0 = 1\qquad \sin0 = 0$$ since the angle $0$ corresponds to $\mathrm{id}$ . The angle $-\alpha$ (the opposite wrt the sum) corresponds to $f^{-1}$ , whose matrix is the transpose of $f$ 's matrix; therefore: $$\cos(-\alpha) = \cos\alpha\qquad \sin(-\alpha) = -\sin(\alpha)$$ The angle $\alpha + \beta$ corresponds to the composition of their respective maps. Thus, matrix multiplication gives: $$\begin{align}\cos(\alpha+\beta) = \cos\alpha\cos\beta-\sin\alpha\sin\beta \\ \sin(\alpha+\beta) = \sin\alpha\cos\beta+\cos\alpha\sin\beta\end{align}$$ To finish I'll put some subsequent propositions without proofs. There exists one, and only one angle $\pi$ such that $\pi+\pi = 0$ . $\pi$ is the angle such that $\cos\pi = -1$ and $\sin\pi = 0$ . There exist two, and only two angles $\delta_1,\delta_2$ such that $\delta_i + \delta_i = \pi$ . $\delta_i$ are the angles such that $\cos\delta_1 = \cos\delta_2 = 0$ and $\sin\delta_1 = -\sin\delta_2 = 1$ . We call these right angles. $\widehat{uv}$ is a right angle iff $\langle u,v\rangle = 0$ The text continues proving things like these. My question is whether anyone has seen this, or a similar extensive treatment. Also though, I'm interested in finding any text that formally links the primitive angles, sine and cosine from geometry to the sine and cosine we now all know and love from calculus, or even complex analysis, preferably with geometry as a starting point. Notes: Obviously there are well definedness issues to address. It seems this is left to the reader. Should I post this on mathoverflow? I've never used it but something tells me a bibliographic inquiry like this one could fit. Unfortunately, angles are now abstract objects, and we haven't at all defined the sine or cosine of a real number, so I'm thinking of a way to map real numbers to angles. Any comment on this would be appreciated!","['euclidean-geometry', 'definition', 'geometry', 'trigonometry', 'reference-request']"
1226078,The elements in the composite field $FK$,"Where $F$,$K$ are two fields. What does the element in the composite field $FK$ look like? All the elements are generated by the elements of $F$ and $K$? (combination of the elements of $F$ and $K$) I think $FK$ is pretty close to the free product of $F$ and $K$; is $FK \subsetneq F*K$? Thank you!","['abstract-algebra', 'field-theory']"
1226098,"Generalisation of $\gcd\left(\frac{a^n-b^n}{a-b},a-b\right)=\gcd(n\gcd(a,b)^{n-1},a-b)$ to $\gcd\left(\frac{a^n-b^n}{a-b},a^m-b^m\right)$?","We have the identity
$$\gcd\left(\frac{a^n-b^n}{a-b},a-b\right)=\gcd(n\gcd(a,b)^{n-1},a-b).$$
(see here ) This appears to be a quite useful result with various applications. I wonder whether there is a similar identity for $$\gcd\left(\frac{a^n-b^n}{a-b},a^m-b^m\right),$$ preferably telling what this $\gcd$ is in terms of $n,m,a-b$ or $a^{\gcd(m,n)}-b^{\gcd(m,n)}$ or so.","['number-theory', 'gcd-and-lcm', 'divisibility']"
1226110,Is the sequence defined by the recurrence $ a _ { n + 2 } = \frac 1 { a _ { n + 1 } } + \frac 1 { a _ n } $ convergent? [duplicate],"This question already has answers here : Proof of existence of a limit for the sequence recursively-defined with $a_1=1$, $a_2=1$ and $a_n=\frac{1}{a_{n-1}}+\frac{1}{a_{n-2}}$ for $n\ge2$ (5 answers) Closed 7 years ago . Let $ a _ 0 = 1 $ , $ a _ 1 = 1 $ and $ a _ { n + 2 } = \frac 1 { a _ { n + 1 } } + \frac 1 { a _ n } $ for every natural number $ n $ . How can I prove that this sequence is convergent? I know that if it's convergent, it converges to $ \sqrt 2 $ since if $ \lim \limits _ { n \to \infty } a _ n = a $ then: $$ \lim _ { n \to \infty } \left ( a _ { n + 2 } - \frac 1 { a _ { n + 1 } } - \frac 1 { a _ n } \right) = a - \frac 2 a = 0 \text ; $$ $$ \therefore \quad a ^ 2 = 2 \text . $$ Now it's easy to see that every $ a _ n $ is positive, so $ a \ge 0 $ and thus $ a = \sqrt 2 $ . Assuming the sequence is convergent, I can calculate an estimation of the rate of convergence too. Let $ \epsilon _ n := a _ n - \sqrt 2 $ . We have: $$ \epsilon _ { n + 2 } = \frac 1 { a _ { n + 1 } } - \frac 1 { \sqrt 2 } + \frac 1 { a _ n } - \frac 1 { \sqrt 2 } = - \frac { a _ { n + 1 } - \sqrt 2 } { \sqrt 2 a _ { n + 1 } } - \frac { a _ n - \sqrt 2 } { \sqrt 2 a _ n } = - \frac { \epsilon _ { n + 1 } } { \sqrt 2 a _ { n + 1 } } - \frac { \epsilon _ n } { \sqrt 2 a _ n } \text . $$ Now because $ a _ n \sim \sqrt 2 + \epsilon _ n $ and $ \lim \limits _ { n \to \infty } \epsilon _ n = 0 $ , therefore from the above equation: $$ \epsilon _ { n + 2 } \lesssim - \frac { \epsilon _ { n + 1 } + \epsilon _ n } 2 \text , $$ which yields $ \epsilon _ n \lesssim \alpha \left ( \frac { - 1 - \sqrt 7 i } 4 \right) ^ n + \beta \left( \frac { - 1 + \sqrt 7 i } 4 \right) ^ n $ for some complex constants $ \alpha $ and $ \beta $ , using induction on $ n $ . Equivalently, we have $ \epsilon _ n \lesssim \left( \frac 1 { \sqrt 2 } \right) ^ n \bigl( A \cos ( n \theta ) + B \sin ( n \theta ) \bigr) $ for $ \theta = \arctan \frac { \sqrt 7 } 4 $ and some real constants $ A $ and $ B $ , since $ \left| \frac { - 1 \pm \sqrt 7 i } 4 \right| = \frac 1 { \sqrt 2 } $ and $ \arg \frac { - 1 \pm \sqrt 7 i } 4 = \pi \mp \theta $ . Hence we get the rough estimation $ | \epsilon _ n | \lesssim C 2 ^ { - \frac n 2 } $ for some real constant $ C $ , and $ \frac 1 { \sqrt 2 } $ is a good guess for the rate of convergence. ( Edit: Thanks to Alex Ravsky for the confirming graphs in his answer .) Edit (some more of my thoughts): Let $ b _ n := \min \left\{ a _ n , a _ { n + 1 } , \frac 2 { a _ n } , \frac 2 { a _ { n + 1 } } \right\} $ . It's easy to see that $ b _ n \le a _ n \le \frac 2 { b _ n } $ and $ b _ n \le a _ { n + 1 } \le \frac 2 { b _ n } $ . Now using induction we can prove that $ b _ n \le a _ { n + m } \le \frac 2 { b _ n } $ . Especially, $ a _ { n + 2 } \ge b _ n $ and $ \frac 2 { a _ { n + 2 } } \ge b _ n $ which yields $ b _ { n + 1 } \ge b _ n $ . The problem can be solved if I show that the sequence $ ( b _ n ) _ { n = 0 } ^ \infty $ increases to $ \sqrt 2 $ .","['limits', 'recurrence-relations', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
1226135,"How can hypersurfaces ""know"" the degree of their defining polynomials?","I'm currently trying to learn some complex and projective geometry.
There is one issue bugging me again and again, from different perspectives, and I just can't get my head around it.
One incarnation of my problem: In the book by Griffiths and Harris, they define the degree of a variety in chapter 1.3.
One of their definitions is: In case [the variety] $V \subset \mathbb P^n$ is a hypersurface, we have seen that it may be given in terms of homogeneous coordinates $X_0 \dots X_n$ as the locus $V = \big( F(X_0 \dots X_n) = 0 \big)\;$ of a homogeneous polynomial $F$. If $F$ has degree $d$, then [...] $V$ has degree $d$. A consequence would be for example that the canonical bundle of $V$ is $\mathcal O(d - n - 1) \big|_V$. My problem is:
As far as I understand it, the object $V$ here is just the hypersurface as a geometrical object (an algebraic variety was defined as the locus of a collection of polynomials and nothing more).
Hence $F$ and $F^2$ would define the same object $V$.
But using $F^2$ instead of $F$ gives us a different degree / a different canonical bundle, which doesn't make sense...","['projective-geometry', 'complex-geometry', 'algebraic-geometry']"
1226139,Calculate number of (four-letter) strings that contain exactly two matching characters (s),"The following problem refers to strings in A, B, ..., Z. Question: How many four-letter strings are there that contain exactly two S's? I used the formula in this answer to come up with the following: $\left(26^4-25^4\right)\cdot 2\space =132,702$ However, this seems to be incorrect... any tips?","['discrete-mathematics', 'combinatorics']"
1226156,Convergence of Cesàro means for a monotonic sequence,"If $(a_n)$ is a monotonic sequence and $$
\lim_{n \to \infty} \frac{a_1 + a_2 + \cdots + a_n}{n}
$$ exists and is finite, does $a_n$ converge?  If so, does it converge to the same limit? I claimed that this was true in an old answer of mine .  I think I had convinced myself of it at the time but I can't seem to now.",['sequences-and-series']
1226162,Lyapunov functions and basins of attraction,"\begin{align}
    x' &= -x^3 + x^5 + (x^4)(y^5)\\[.7em]
    y' &= -8y^3 + y^5 - 10(y^4)(x^5)
\end{align} $(0,0)$ is obviously a critical point of the system, and we are given that it is asymptotically stable, but have to show it. I have tried to make a Lyapunov function $V(x,y) = ax^2 + cy^2$ , with $a,c > 0$ but I am having trouble to prove that $\frac{d}{dt} V(x,y)$ is negative definite. I get some complicated polynomial I can't use logic to finalize. How can I change the Lyapunov to come up with a meaningful conclusion? \begin{align}
    \frac{d}{dt}V(x,y) = 2ax(-3x^2 + 5x^4 + 4x^3y^5) + 2cy(-24y^2+5y^4-40y^3x^5)\\[.7em]
\end{align}","['stability-in-odes', 'ordinary-differential-equations', 'lyapunov-functions']"
1226180,Parallelizing lines,"Let $n \geq 1$ be an integer, and $L_1,\ldots,L_n$ be $n$ lines in $\mathbb{R}^3$ which are pairwise disjoint. Is it possible to move all $n$ lines continuously so that they never cross, and so as to send the line $L_k$ to $\{k\} \times \mathbb{R} \times \{0\}$? This seems instinctively true, but I don't see any slick proof right now. The problem is that the space of affine lines is $4$-dimensional, while ""crossing a given line"" is $3$-dimensional, so codimension $1$. Hence, if I try to move each line once at a time, it seems entirely possible that a set of badly placed lines cages off a portion of space, and that the remaining lines can't be moved around freely. Bonus question : what is the least number of pairwise disjoint lines $(L_k)$ I need so that the space of lines which cross no $L_k$ is disconnected?","['projective-space', 'geometry', 'general-topology', 'algebraic-topology', 'geometric-topology']"
1226193,Nature of the serie $\sum\prod_{k=2}^n (2-e^{\frac{1}{k}})$,"I'd like to determine the nature of the following serie : $$\sum_{n\ge 2}\prod_{k=2}^n (2-e^{\frac{1}{k}})$$ Let $u_n = \prod_{k=2}^n (2-e^{\frac{1}{k}})$. So I ""have"": 
$$\begin{aligned}
\ln(u_n) &= \sum \ln(2-e^{1/k}) 
\\& \approx \sum \ln(1-1/k + o(1/k))\\
& \approx \sum 1/k- o(1/k))\\
& \approx -\ln(n) = \ln(1/n)\end{aligned}$$
So I guess that $u_n = \Theta (1/n)$ and so $\sum u_n$ diverge. But all those calculations are not correct since $k$ is not always ""big"" and we can not sum ""$o$"" arbitrarily.",['sequences-and-series']
1226206,"If $n\ge2$, prove that $\frac {n!}{n^n} \le ({\frac 1 2})^k$, where $k$ is the greatest integer $\le \frac n 2$.","Using only precalculus knowledge , if $n\ge2$, prove that $\frac {n!}{n^n} \le ({\frac 1 2})^k$, where $k$ is the greatest integer $\le \frac n 2$. (taken from Apostol's Calculus I, page 46) I don't have much of a clue, I tried substituting $k$ with the maximum value it can assume ($\frac n 2$)
$$\frac {n!}{n^n} \le \left({\frac 1 2}\right)^{\frac n 2}$$
then squaring the expression
$${{(n!)}^2 \over {n}^{2n}} \le \frac 1 {2^n}$$
and obtaining
$$2^n(n!)^2\le n^{2n}$$
I mean, I can intuitively see that
$$2n^2\cdot2(n-1)^2\cdots2\cdot2^2\cdot2\cdot1^2$$
(n times)
is less than
$$n^2\cdot n^2 \cdots n^2$$
(n times), but I'd like to have a more logically sound explanation, given that I didn't delve into calculus yet. Thanks in advance!","['factorial', 'algebra-precalculus', 'inequality']"
1226224,Find the sum of all odd numbers between two polynomials,"I was asked this question by someone I tutor and was stumped. Find the sum of all odd numbers between $n^2 - 5n + 6$ and $n^2 + n$ for $n \ge 4.$ I wrote a few cases out and tried to find a pattern, but was unsuccessful. Call polynomial 1, $p(n) = n^2 - 5n + 6,$ then $p(4)=2.$  Next, call polynomial 2, $q(n)=n^2 + n,$ then $q(4)=20.$  Then adding all the odd numbers between 2 and 20 gives the following sum: $3+5+7+9+11+13+15+17+19= 99. \\$ $p(5)= 6$ and $q(5)=30.$ Then adding all the odd numbers between 6 and 30 gives the following sum:  $7+9+11+13+15+17+19+21+23+25+27+29=216 \\$ $p(6)=12$ and $q(6)= 42.$  Then adding all the odd numbers between 12 and 42 give the following sum:  $13+15+17+19+21+23+25+27+29+31+33+35+37+39+41 = 405.$ From here I do not see any apparent patters.  This problem was given in a Pre-Calculus course, so clearly only elementary methods are expected by the students. Any help or advice would be much appreciated.  Thank you!!!!!",['algebra-precalculus']
1226228,Infinite direct product of fields.,"Let $F$ be a field, and consider the infinite direct product$$F \times F \times F \times F \times \dots,$$i.e. $\prod_{i=0}^\infty F$, i.e. the direct product of a countable number of copies of $F$. The theory of bases for vector spaces shows that this direct product has a basis, and so is isomorphic to a direct sum of copies of $F$. How many copies of $F$ do I need $($i.e. what is the dimension of this direct product as an $F$-vector space$)$? Is it possible to exhibit this isomorphism between an infinite direct product and an infintie direct sum explicitly?","['abstract-algebra', 'field-theory', 'vector-spaces', 'linear-algebra']"
1226232,"How do I evaluate $\mathbb E(X\log(X))$ if $X$ has a binomial distribution, for large $n$ values?","$X\sim\mathcal {Bin}(n,p)$ I want to evaluate $\sum\limits_{x=0}^n {^n\mathrm C_x} p^x(1-p)^{n-x}x\log(x)$. Is there any way to avoid the sum because my $n$ can be very large (around $10^6$)?","['entropy', 'summation', 'statistics', 'probability', 'binomial-distribution']"
1226288,Residue of 1/sin^3(z),What are the residues of $ \frac{1}{sin^{3}z} $? From the residue theorem the residues are at $$\lim_{z \rightarrow z_{0}} \frac{1}{(n-1)!} \frac{d^{n-1}}{dz^{n-1}} (z-z_{0})^{n} f(z)$$ $$\lim_{z \rightarrow n\pi} \frac{1}{2!} \frac{d^{2}}{dz^{2}}  \frac{(z-n\pi)^{3}}{sin^{3}z}$$ $$=\lim_{z \rightarrow n\pi} \frac{1}{2}   (z-n\pi)(3csc^{5}(z)+9cot^{2}(z)csc^{3}(z))-6cot(z)csc^{3}z$$ But what is the $\lim_{z \rightarrow n\pi} \frac{1}{2}   (z-n\pi)(3csc^{5}(z)+9cot^{2}(z)csc^{3}(z))-6cot(z)csc^{3}z$?,['complex-analysis']
1226334,Ways to form a committee - why is this approach incorrect?,"I have the following question: If there are 7 women and 9 men, how many ways are there to select a committee of 5 members if at least 1 man and 1 woman must be on the committee? I have found the solution as follows: Let $C_r = $ $ 16 \choose{5}$ = the number of ways to select 5 members with no restrictions Let $C_w = $ $ 7 \choose{5}$ = the number of ways to select all women (i.e. no men) Let $C_m = $ $ 9 \choose{5}$ = the number of ways to select all men (i.e. no women) $\therefore$ the number of ways to choose a committee of atleast one man and one woman is $$C_r - C_w - C_m = 4368 - 21 - 126 = 4221$$ I understand this to be the correct answer, but I'm trying to understand why the following approach is incorrect: First, we select 1 man, there are 9 ways to do this. Secondly, we select 1 woman, there are 7 ways to do this. Then, we select the rest of the committee, and we don't care whether they are men or women. There are $14 \choose{3}$ ways to do this. $\therefore$ the number of ways to choose a committee of at least one many and one woman is: $$9 \times 7 \times 364 = 22932 \ne 4386$$ So obviously this reasoning is incorrect. What am I missing out on in the second approach?",['combinatorics']
1226354,"Proving gcd($a,b$)lcm($a,b$) = $|ab|$","Let $a$ and $b$ be two integers. Prove that $$ dm = \left|ab\right| ,$$ where $d = \gcd\left(a,b\right)$ and $m = \operatorname{lcm}\left(a,b\right)$ . So I went about by saying that $a = p_1p_2...p_n$ where each $p_n$ is a prime. Same applies to $b = q_1q_2 ... q_c$ . So then $m = (u_1u_2...)(p_1p_2...p_n)$ and $m = (t_1t_2...)(q_1q_2...q_c)$ since $a|m$ and $b|m$ . $m$ has a unique factorization, so the primes $(u_1u_2...)(p_1p_2...p_n) = (t_1t_2...)(q_1q_2...q_n)$ and the gcd(a,b) = $(p_1p_2...p_n) \cap (q_1q_2...q_n)$ (I know this is not mathematically correct, so is there a correct way to express this?). So $dm = |ab| \iff d= \frac{|ab|}{m}$ . And by the definition above, $\frac{ab}{(t_1t_2...)b} = \frac{a}{(t_1t_2...)}$ . And this is where I get stuck. Is my proof right? Am I going in the right direction? Thanks PS. I am trying to do this using only the prime factorization theorem and the definitions of the gcd and the lcm.","['abstract-algebra', 'proof-verification', 'divisibility', 'prime-factorization']"
1226362,limits of r in cylindrical coordinates,"Find the volume of a sphere $x^2+y^2+z^2\leq 1$ contained between planes $z=1/2$ and $z=1/\sqrt2$ using cylindrical coordinates. So the limits of $\theta$ would be $0$ to $2\pi$. Limits of $z$ would be the given planes. But why cant the limits of $r$ simply be $0$ to $1$? Is it because, if this were the case, we would be finding the volume of a cylinder and not a spherical type of object?",['multivariable-calculus']
1226405,Show that a subset $V$ of $X$ is open iff every point $x \in V$ has a neighborhood $V_x \subseteq V$,Show that a subset $V$ of $X$ is open iff every point $x \in V$ has a neighborhood $V_x \subseteq V$. Any help would be appreciated. I am working in topological spaces right now.,"['real-analysis', 'general-topology']"
1226421,Defining the area under an oscillating function,"I was curious about taking a definite integral of an oscillating function. For example, $$\lim_{a\to 0} \int_a^1 \sin \frac1x\,dx$$ I know that there is some area under the function, but since it oscillates infinitely is it possible to define it? Do you use the limit superior and the limit inferior? I know it is probably possible (somehow) to take the antiderivative, FTC, etc. but I am wondering what this really intuitively means, given that this function oscillates (so we can't really ""see"" the area under the function).","['calculus', 'definite-integrals']"
1226434,Exponential of a matrix always converges,"I am trying to show that the exponential of a matrix converges for any given square matrix of size $n\times n$: $M\mapsto e^M$ e.g. $\displaystyle e^M = \sum_{n=0}^\infty \frac{M^n}{n!}$ Can I argue that: Since $n!$ necessarily grows faster than $k^n$ will, that this converges. This seems to be an obvious fact, since: $$n!=1\times 2\times 3\times \cdots \times k\times (k+1)\times (k+2)\times \cdots$$
$$k^n=k\times k\times k \times\cdots\times k \times k\times \cdots$$ If we have some $q\times q$ matrix, with $a$'s in each position(which will grow as fast as we make our $a$ and $q$ large) we still only get increasing at a rate of $q^{n-1}\times a^n$ In light of the comments, I know that in this banach space, I need only show that $\displaystyle e^M = \sum_{n=0}^\infty \frac{||M||^n}{n!}$ converges. Now I have many matrix norms to choose from, and I can't seem to get a good argument going rigorously. Any ideas?","['convergence-divergence', 'matrices']"
1226445,$E(X^2)=E(X)=1$. Find $E(X^{100}).$,"$X$ is a random variable such that $E(X^2)=E(X)=1$. Find $E(X^{100}).$ My attempt: Assuming $X$ is discrete, we have $\sum x_i\mathbb P(X=x_i) = \sum x_i^2\mathbb P(X=x_i) = 1.$ We have something like $x_1p_1+\cdots+x_np_n=x_1^2p_1+\cdots+x_n^2p_n=1$. How do I find out $\sum x_i^{100}\mathbb P(X=x_i)$? I am completely blank. And, I am feeling uneasy as $X$ could well be continuous. How do I proceed then?","['statistics', 'expectation']"
1226455,What does a positive definite matrix have to do with Cauchy-Schwarz Inequality?,"In my text book, Cauchy-Schwarz Inequality is extended to a positive definite matrix. But I neither understand what the relationship between Cauchy-Schwarz Inequality and a positive definite matrix nor the sentence underlined in red, I am not a strong linear algebra person, I have just started studying it. I'd like post the page here so that I might get a better explanation.
Thank you.",['linear-algebra']
1226475,Chern class of complex vector bundles,"Let $\xi$ be an $n$-dimensional complex vector bundle. It is claimed that the Chern class of $\xi$ is $$
c(\xi)=(1+x_1)\cdots (1+x_n),
$$
$|x_k|=2$, $c_j(\xi)$ is the $j$-th symmetric polynomial of $x_1,\cdots,x_n$. Is this claim true? In the case $\xi=L_1\oplus\cdots\oplus L_n$, Whitney sum of complex line bundles, I have obtained the claim. How about general case?","['vector-bundles', 'characteristic-classes', 'algebraic-geometry', 'differential-geometry', 'algebraic-topology']"
1226497,Can you verify this proof of the Schroeder-Bernstein theorem?,"I'm a freshman in college and my professor challenged us to find a proof of this theorem. Please don't give me the answer but please verify if this proof works or, if not, if it is the start of a proof that needs to be filled in, or if you think I should abandon this line of thought. If it doesn't work, please explain why. This is kind of a sketch of a proof, really, and (I think) it uses the axiom of choice which I don't like, but it's a start. (I'm not sure if it actually uses the AoC. Does it?) Theorem: Let $A$ and $B$ be sets. Let $f:A\to B$ and $g:B\to A$ be injections. Then there is a bijection from $A$ onto $B$. Lemma: Let A and B be sets such that there is no bijection from A onto B. Let $f:A\to B$ and $g:B\to A$ be injections. Then there is at least one element $x\in A$ such that $f(x)\not= g^{-1}(x)$. Proof: By contradiction. Assume that $f(a)=g^{-1}(a)$ for every $a\in A$ where $g^{-1}(a)$ is defined. Either $A=g[B]$ or $g[B]\subset A$ (strictly contained). If $g[B] = A$, then $f=g^{-1}$ and $f$ is a bijection, which is a contradiction. So $g[B]\subset A$. But since $g^{-1}$ is a surjection, every element of $B$ is mapped to an element of $A$, so for any $x\in A-g[B]$, $f(x)=f(y)$ for some $y\in g[B]$ and $f$ is not injective, which is a contradiction. proof of theorem: By contradiction. Assume there is no bijection from $A$ onto $B$. By the lemma, since f and g are injections, there must be a $a\in A$ such that $f(a)\not=g^{-1}(a)$. Similarly, there must be a $b\in B$ such that $g(b)\not=f^{-1}(b)$.
  Let $A'$ and $B'$ be the subsets of A and B where $a\in A',b\in B'$ iff $f(a)\not=g^{-1}(a), g(b)\not= f^{-1}(b)$ Define $h:A\to B$ where 
  $$h(x)=f(x)$$ whenever $f(x)=g^{-1}(x)$ 
  and $$h(a)=b $$where $a$ is some element of $ A'$ and $b$ is some element of $ B'$. Clearly, $h:A-A'\to B-B'$ is a bijection, since $f=g^{-1}$ is a bijection. Also, $h:A'\to B'$ is a bijection because And then something about how there is always a $d\in B'$ for any $c\in A'$, meaning they come in pairs, but then we can define $h(c)=d$ and get our bijection again. So $h:A\to B$ is a bijection, which is a contradiction.","['elementary-set-theory', 'proof-verification', 'axiom-of-choice']"
1226499,How do I use residue theorem to evaluate this improper integral to get a good looking solution?,"The problem is $\int_{0}^{\infty} \frac{\sqrt{x}}{x^2+2x+5}dx$ 
I replace x with z, and did some algebra, but the solution was rather nasty. it contains exponential and arctan such and such. However, the solution given was $\frac{\pi}{2\sqrt{2}}{\sqrt{\sqrt{5}-1}}$. It was pretty neat and I have no idea how I could convert my solution to that form.","['complex-integration', 'complex-analysis', 'residue-calculus']"
