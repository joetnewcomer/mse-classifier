question_id,title,body,tags
4170133,Do you pick a delta or do you pick an epsilon beginner confusion,"Let $f : \mathbb R \to\mathbb R$ be a continuous function such that $\lim_{x\to0} f(x) = 2.$ Prove that there
exists a $δ > 0$ such that, on the interval $(−δ, δ)$ , the function f is bounded. Hello I am a beginner in college mathematics, here is my attempt. I know that for every $ε>0$ , there exist a $δ>0$ such that $|x-a| < δ$ implies that $|f(x) - L| < ε$ $|x| < δ$ = $- δ < x < δ$ $|f(x) - 2| < ε$ Then use the 2nd triangle inequality?? (Sorry I am just trying to create something) $|f(x)| - |2| < ε$ = $|f(x)| < ε$ +  |2| = $-ε- 2<f(x) < ε$ + 2 So here what do I do ? Do I pick a ε? So let $ε = 1$ and now the function f is bounded? Do I pick the delta or epsilon first? Thanks -Alice","['functions', 'solution-verification']"
4170150,Strongly bounded set is weakly bounded,"I have the following two definitions: Let $S$ be a subset of a Banach space $X$ . We say that $S$ is weakly bounded if $l\in X^*$ , the dual space of $X$ , then $\sup\{|l(s)|:s\in S\}<\infty$ . We say that $S$ is strongly bounded if $\sup\{||s||:s\in S\}<\infty$ . I need to prove those definitions are equivalent. I have already proved that 2 implies 1, but I'm stuck with the other direction. I suspected that I have to use Hahn-Banach's theorem, or maybe some of their corollaries, but I don't know how to proced. Thanks to all of you.","['banach-spaces', 'operator-theory', 'hahn-banach-theorem', 'functional-analysis']"
4170182,Show that $\int_0^1 f(x)dx\int_0^1\frac{1}{f(x)}\geq 1$,"Let $f:[0,1]\to\Bbb R$ be a measurable function satisfying $0<f(x)<\infty$ for each $x\in [0,1]$ . Show that $\int_0^1 f(x)dx\int_0^1\frac{1}{f(x)}\geq 1$ . $Attempt$ . Since $f>0$ there exists a sequence $\{\phi_n\}_{n=1}^\infty$ of simple functions on $[0,1]$ which converges pointwise on $[0,1]$ to $f$ and such that $0<\phi_1\leq\phi_2\leq \dots$ We observe that if the statement is true for any simple function $\phi:[0,1]\to\Bbb R$ with $\varphi>0$ then $1\leq \int_0^1\phi_n(x)dx\int_0^1\frac{1}{\phi_n(x)}dx$ implies that $$1\leq \lim_{n\to\infty}\int_0^1 \phi_n(x)dx\lim_{n\to\infty}\int_0^1 \frac{1}{\phi_n(x)}dx\stackrel{?}{=}\int_0^1 f(x)dx\int_0^1\frac{1}{f(x)}dx$$ by Monotone Convergence Theorem. But, I couldn't prove it for any positive simple function. Any help would be appreciated. Thanks!","['measure-theory', 'real-analysis']"
4170209,Student's t-distribution calculation,"How to calculate this Student's t-distribution? $X $ ~ $ t(17)$ I have to find $P(X>8)$ and $a$ where $P(X>a)=0.1$ . I used standardizing for calculating similar probability for normal distribution $X $ ~ $ N(18,3)$ . How can I do that kind of thing here in Student's? I know there exists the formula of $Z$ but don't know how to use it. $\frac{\bar X-\mu}{S/\sqrt n}$ Thanks.","['statistics', 'probability-distributions', 'probability']"
4170235,proving an integral inequality_,"I'm having problem proving the following integral inequality. Let $f$ be a continuous function on $[0,1]$ such that $f(0)=0$ and $f'(x)\geq1$ for all $x\in(0,1)$ . Show the following inequailties hold. $$\left( \int^{1}_{0} f(x) dx \right)^2 \leq \int ^{1} _{0} (f(x))^3 dx$$ I have tried using C-S inequalities of integrals, and also integration by parts, but failed. Please help thank you!","['integration', 'calculus', 'definite-integrals', 'integral-inequality']"
4170314,Find a recurrence relation for the number of bit strings of length $n$ by Goulden-Jackson,"I am working over Goulden -Jackson Method, I tried to undergo every possible question type. I obtained the following questions from Rosen's Discrete Mathematics and Its Applications . I solved them by classical way, but when I tried to solve them via Goulden - Jackson, I got stuck. a-) Find a recurrence relation for the number of bit strings of length $n$ which do not contain three consecutive zeros My work = The bad word is $000$ , but it has two overlapping such that $0,00$ . Unfortunately, I do not know how to approach when there are two different overlapping words. I got stuck there. b-) How many bit strings of length $n$ contain either five consecutive zeros or five consecutive ones. My work = I thought that I can reach the solution by all cases - $(00000,11111)$ are bad words. In this situation, I reached $\frac{1}{1-2x} - \frac{1}{1-2x+2x^5}$ , but when I convert it into recurrence form, it does not satisfy the desired result. c-) How many bit strings of length $n$ contain either three consecutive zeros or four consecutive ones. My Work = It has the same logic as part $b$ I hope to find answers for my questions. Thanks for your works.","['combinatorics-on-words', 'combinatorics', 'recurrence-relations', 'discrete-mathematics']"
4170326,"""Find the volume of the ellipsoid $x^2+\frac{y^2}{100}+\frac{z^2}{4} = 1$"" was the question and here's my attempt.","So, I asked this question and got downvotes and someone saying I should present my attempt. Here lies my thought process: I know the volume is $\iiint dxdydz$ on any W $\subset R^3$ and I ""just"" had to find the proper values for the W, namely x, y, z, their range. I said $-1≤x≤1$ , $-10≤y≤10$ and $-2≤z≤2$ . Expressing them on the integral, for every variable we set the next one to 0 and the previous as constant so it's: $-1≤x≤1$ $-10 \sqrt{1-x^2} ≤y≤10 \sqrt{1-x^2}$ $-2 \sqrt{1-x^2-\frac{y^2}{100}} ≤ z ≤ 2 \sqrt{1-x^2-\frac{y^2}{100}}$ and it actually went from $dxdydz$ to $dzdydz$ Any thoughts? Thanks in advance.","['integration', 'volume', 'multivariable-calculus', 'solution-verification', 'multiple-integral']"
4170331,Relationship between the inverse of the right eigenvector of a matrix $A$ and the transpose of the right eigenvector of $A^T$,"I should start by stating that this question is related to the Koopman's theory in dynamical system, which gathers growing interests in the fluid dynamics/data science areas such as dynamic mode decomposition (DMD). Suppose $A$ has the eigen decomposition $P_1DP_1^{-1}$ where the columns of $P_1$ are the right eigenvectors of $A$ . Then $A^{\text{T}}$ has an eigen decomposition $P_2DP_2^{-1}$ where columns of $P_2$ are the right eigenvectors of $A^{\text{T}}$ . We can write down these relations explicitly \begin{align}
AP_1&=P_1D\,,\\
A^{\text{T}}P_2&=P_2D\,.
\end{align} From these equations, there are two ways of getting the left eigenvectors, namely \begin{align}
P_1^{-1}A&=DP_1^{-1}\,,\\
P_2^{\text{T}}A&=DP_2^{\text{T}}\,.
\end{align} Therefore, the rows in both $P_1^{-1}$ and $P_2^{\text{T}}$ are the left eigenvectors of $A$ , but $P_1^{-1}\neq P_2^{\text{T}}$ in general. I believe \begin{equation}
P=aP_1^{-1}+bP_2^{\text{T}}\,,
\end{equation} is also a left eigenvector of $A$ for any scalars $a$ and $b$ that are not both zeros. My questions are why the matrix $P$ is singular for $a=-b$ even though they are not equal (my guess is something to do with linear dependency), and what is the relationship between $P_1^{-1}$ and $P_2^{\text{T}}$ since both were derived using valid approaches for finding the left eigenvectors of the matrix $A$ . I asked this question because the Koopman eigenfunction is found by multiplying the left eigen vectors of the Koopman operator with its invariant subspace (linear variables derived from the original nonlinear variables). And I wish to understand whether the approach for obtaining the left eigenvectors of the Koopman operator matters in terms of the final Koopman eigenfunction. I would appreciate if anyone could help me on this. Thanks a lot!","['eigenvalues-eigenvectors', 'data-analysis', 'linear-algebra', 'eigenfunctions', 'dynamical-systems']"
4170352,"Defining Connection in Hom Vector Bundle without Coordinates, using Dieudonne's Definition","In Dieudonne's Treatise on Analysis (Volume III Section 17.16) the following definition for a linear connection in a vector bundle is given: A linear connection $C$ , in a vector bundle $(E,\pi, M)$ is defined to be a smooth map $C:TM\oplus E\to TE$ which satisfies for each $(k_x,u_x)\in T_xM\oplus E_x$ , the following conditions: $\pi_E(C(k_x,u_x))=u_x$ , where $\pi_E:TE\to E$ is the standard projection $T\pi (C(k_x,u_x))=k_x$ where $T\pi:TE\to TM$ is the tangent mapping The mapping $k_x\mapsto C(k_x,u_x)$ is a linear mapping of $T_xM$ into $T_{u_x}E$ The mapping $u_x\mapsto C(k_x,u_x)$ is a linear mapping of $E_x$ into $(TE)_{k_x}$ , which is the fiber of $T\pi:TE\to TM$ lying over the base point $k_x\in TM$ . Geometrically this definition makes sense; I interpret $C(k_x,u_x)$ as ""the direction I should move within $E$ in order to reach an infinitesimally close fiber, provided that I start at the point $u_x$ of the fiber $E_x$ , and move in the direction $k_x$ in the base manifold"" (picture below for one-dimensional base and fibers). With the usual abuse of notation, we have local coordinates $(x,\dot{x},u,\dot{u})\in \Bbb{R}^n\times \Bbb{R}^n\times\Bbb{R}^p\times\Bbb{R}^p$ on $TE$ , where $x$ describes the base point, $\dot{x}$ describes the vectors in the fiber tangent to $M$ , $u$ describes points in the fibers of $E$ and $\dot{u}$ describes the points tangent to $E$ . Relative to such adapted coordinates, the mapping $C$ takes the form $C: (x,\dot{x},u)\mapsto (x,\dot{x},u,-\Gamma_x(\dot{x},u))$ , where $\Gamma_x:\Bbb{R}^n\times\Bbb{R}^p\to\Bbb{R}^p$ is bilinear (the minus sign is merely convention to make things match up with the usual Christoffel symbols). Question. Suppose we are given two vector bundles $E,F$ over $M$ , with respective linear connections $C_E,C_F$ . How can I define abstractly (i.e without any references to coordinates) a connection $C_{\text{Hom}(E,F)}$ in the bundle $\text{Hom}(E,F)$ ? I have already managed to define it in coordinates, meaning I figured out how, given local coordinates, to define the $\Gamma$ for $\text{Hom}(E,F)$ in terms of $\Gamma$ for $E$ and $F$ . Then, I used the appropriate transformation laws for $\Gamma$ 's to check that everything works out so that we do indeed obtain a globally well-defined mapping $\text{C}_{\text{Hom}(E,F)}$ . I will include this local definition if anyone requests it; but the notation is a little heavy so I suppress it for now. The reason I ask this question is because defining things locally and then verifying the compatibility conditions is a little tedious, especially since $\Gamma$ has a somewhat complicated transformation behavior. By the way, I have already determined how to define connections on $E\oplus F$ and $E\otimes F$ , in an abstract manner (and also locally). Also, I'm aware that $\text{Hom}(E,F)\cong E^*\otimes F$ , so if someone can provide an abstract definition for $C_{E^*}$ in terms of $C_E$ that is also sufficient. Thank you. Picture:","['connections', 'vector-bundles', 'differential-geometry']"
4170366,Solve $e^x \cdot 2y' = y^2 + y'^2$,"Solve $$e^x \cdot 2y' = y^2 + y'^2.$$ My Attempt Let $p = y'$ then $2e^x \cdot p=y^2+p^2$ . No differentiate both sides to get: $$2e^x \cdot p + 2e^x \frac{dp}{dx} = 2yp + 2p\frac{dp}{dx}$$ $$\frac{dp}{dx}(2e^x-2p) = 2yp-2e^xp \rightarrow \frac{dp}{dx}(e^x-p) = yp-e^xp$$ I can not easily identify this form so I make the substitution $y = ux$ and $y' = u'x + u$ $$(u'x+u)(e^x-ux) = yux-e^xux$$ Later manipulation does not really get me to an easier form, where should I go from here?",['ordinary-differential-equations']
4170409,A group $G$ with a subgroup $H$ of index $n$ has a normal subgroup $K\subset H$ whose index in $G$ divides $n!$,"I would be very thankful if someone could give me a hint with proving this. It's a very common exercise in abstract algebra textbooks. If $G$ is a group with a subgroup $H$ of finite index $n$, then $G$ has a normal subgroup $K$ contained in $H$ whose index in $G$ is finite and divides $n!$. I found the proof on this Wikipedia page at some point (although the proof appears to be no longer there), but I got lost in one of the details.","['normal-subgroups', 'group-theory', 'abstract-algebra']"
4170506,How do you find (continuous) bounds on the matrix exponential,"Let $A$ be a $n \times n$ real or complex matrix. I am interested in bounds on the matrix exponential $e^{A t}$ , for $t \geq 0$ . In particular: is there a continuous function $C: M_{n\times n} \rightarrow \mathbb{R}_+$ such that $$\|e^{At}\| \leq C(A) e^{\Lambda(A) t} (1 + t^{n-1})$$ As some background: for any individual $A$ , I know how to find the big- $\mathcal{O}$ behavior of $e^{A t}$ . Namely, re-write $A$ as $T B T^{-1}$ where $B$ is in Jordan canonical form, compute the matrix exponential of $B$ , then translate back to the matrix exponential of $A$ . This gives the upper bound: $$\|e^{A t} \| = \mathcal{O}(e^{\Lambda(A) t} (1+t^{n-1}))$$ where $\Lambda(A)$ is the largest real part of any eigenvalue of $A$ . Now, the eigenvalues of $A$ depend continuously on $A$ , so $\Lambda(A)$ is continuous. However the big- $\mathcal{O}$ constant in this expression depends on the matrix $T$ used above. However, $T$ will not vary continuously with $A$ , and may even be arbitrarily large when $\|A\|$ is bounded. So, this doesn't give a continuous bound on the matrix exponential. Hence my question: is there a continuous function $C(A)$ such that: $$\|e^{At}\| \leq C(A) e^{\Lambda(A) t} (1 + t^{n-1})$$","['matrices', 'inequality', 'matrix-exponential', 'linear-algebra']"
4170549,"Given $2^k$ random binary sequences of length $k$, how many are expected to be distinct?","For example, say you get a uniformly random batch of $256$ sequences of $8$ bits each, and count the number of distinct sequences (i.e. discard duplicates then count), and repeat this experiment indefinitely. In the limit, what's the ratio of distinct sequences to total sequences? Empirically, irrespective of choice of $k$ , this seems to be converging to something around $0.63214$ , so as a pure guess, I'm thinking maybe $1-\frac{1}{e}$ , but I'm sure someone on here will immediately know the answer to this one.","['random', 'probability-distributions', 'probability']"
4170557,Can we determine higher powers of a matrix in terms of lower powered matrices?,"Consider a n-ordered square matrix A. Using Cayley-Hamilton Theorem, I can represent the matrix $A^n$ as a matrix polynomial P(A) of degree n-1. Further any matrix $A^k$ where $k>n$ can also be represented as follows: $$A^k= a_{k,n-1} A^{n-1} + a_{k,n-2} A^{n-2}+a_{k,n-3} A^{n-3} ... + a_{k,2} A^{2}+a_{k,1} A^{1} + a_{k,0}I$$ What I want to know that is there any way to determine the coefficients as a function of k?","['matrices', 'functions', 'cayley-hamilton']"
4170615,How do I deal with finding a partition for a specific epsilon?,"Hello everyone my name is Alice and I am new to this website. Here is the problem I am working on: Let the function f : [0, 1] → R be defined by f(x) = x. Find a
partition P of the interval [0, 1] such that the upper sum U(f, P) and
the lower sum L(f, P) satisfy the inequality $U\left(f,\:P\right)-L\left(f,P\right)\:<\:\frac{1}{3}$ and I have to show that how I compute or estimate $U(f, P) − L(f, P)$ . Here is my attempt: Let $P = \{x_0, ..., x_i\}$ be any partition of $[0, 1]$ Using the definition of upper sum and lower sum: $
L(f,P) = \sum^n_{i=1}m_i(x_i - x_{i-1})$ here $m_i$ is the infimum of $f(x)$ in the interval $[x_{i-1}, x_i].$ And same thing for upper sum but we have supremum instead. Since we have $f(x) = x$ , which is a increasing function, so the infimum of the interval $[x_{i-1}, x_i]$ will always be $x_{i-1}$ and the supremum of the interval will always be $x_i$ so rewriting the definition of the upper sum and the lower sum, I get: $ L(f,P) = \sum^n_{i=1} x_{i-1}(x_i - x_{i-1})$ $ U(f,P) = \sum^n_{i=1} x_i(x_i - x_{i-1})$ $ U(f,P) - L(f,P) = \sum^n_{i=1} x_i(x_i - x_{i-1}) - \sum^n_{i=1} x_{i-1}(x_i - x_{i-1})$ here I am allowed to factor out the summation symbol right? $= \sum^n_{i=1}(x_i - x_{i-1})(x_i - x_{i-1})$ This is where I am stuck, I need to show that this expression is less than $\frac{1}{3}$ but I am not sure how to create an inequality. Maybe I need this: $ 0 < x_0 < x_i < x_n = 1$ ? I am not sure. It will be really nice if someone can help me tq- Alice","['functions', 'solution-verification', 'real-analysis']"
4170616,Showing $\lim_{h \to 0} \frac{\|a+h\| - \bigg(\|a\| + \frac{a\cdot h}{\|a\|}\bigg)}{\|h\|} = 0$,"Show for any $a \neq 0 $ we have: $$\lim_{h \to 0} \frac{\|a+h\| - \bigg(\|a\| + \frac{a\cdot h}{\|a\|}\bigg)}{\|h\|} = 0$$ Note: all terms are vectors of any dimension. Hint: remember that $x - y = \frac{x^{2} - y ^{2}}{x+y}$ Attempt : I've gotten this far $$\frac{\|a+h\|^{2} - \bigg(\|a\| + \frac{a\cdot h}{\|a\|}\bigg)^{2}}{\|h\| \bigg(\|a+h\| + \bigg(\|a\| + \frac{a\cdot h}{\|a\|}\bigg)\bigg)}$$ I expanded it form here, but I don't see anything that remotely looks like it will help in cancelling out the $\|h\|$ term. I do have Cauchy Schwartz available, but it won't make any inequality true if I apply it to $a \cdot h$","['multivariable-calculus', 'real-analysis']"
4170674,Finding a closed form for $1-2\cos\alpha+3\cos2\alpha+\cdots+(-1)^{n-1}n\cos((n-1)\alpha)$,"I would like to find a closed form for the following Sum: $$C = 1-2\cos(\alpha)+3\cos(2\alpha)+\cdots+(-1)^{n-1}n\cos((n-1)\alpha)$$ I've tried adding it to the sum: $$S =0-2\sin(\alpha)+3\sin(2\alpha)+\cdots+(-1)^{n-1}n\sin((n-1)\alpha) $$ Then, $$C+iS = 1-2e^{i\alpha}+3e^{i2\alpha}-...+(-1)^{n-1}ne^{in\alpha} $$ We can multiply by $e^{i\alpha}$ to get: $$(C+iS)e^{i\alpha} = 0+e^{i\alpha}-2e^{i2\alpha}+3e^{i3\alpha}-...+(-1)^{n-2}(n-1)e^{in\alpha}+(-1)^{n-1}ne^{i\alpha(n+1)}  $$ Adding the above two equations, $$(C+iS)(1+e^{i\alpha}) = 1-e^{i\alpha}+e^{i2\alpha}-e^{i3\alpha}+...+(-1)^{n-1}e^{in\alpha} +(-1)^{n-1}ne^{i\alpha(n+1)}$$ Which is then a geometric series and hence: $$(C+iS)(1+e^{i\alpha}) =\frac{1((-e^{i\alpha})^{n}-1)}{(-e^{i\alpha}-1)}+(-1)^{n-1}ne^{i\alpha(n+1)}$$ I have trouble continuing forward,as the expression gets complicated.I was wondering if there is an alternative way or if you could simplify the above expression?","['trigonometry', 'sequences-and-series']"
4170693,"Generic Points, Associated Points on $\operatorname{Spec} A$ for an Integral","I'm reading section 5.5 from Vakil's FOAG, and I got a bit confused about the associated points of a module $M$ . The way he develops this theory, is by first stating some axioms and then proving that they hold. The first axiom states the following: (A) The associated primes/points of $M$ are precisely the generic points of irreducible
components of the support of some element of $M$ (on $\operatorname{Spec} A$ ). Now, the very next exercise states the following: Suppose $A$ is an integral domain. Show that the
generic point is the only associated point of $\operatorname{Spec} A$ . This is supposed to be done assuming property A. Now, the exercise itself doesn't state what module $M$ we are working with, as in property (A). Does this mean $M = A$ , or do we have to show this for all $A$ modules $M$ ?
If $M=A$ , I think that the exercise is trivial. However, I am unable to show this holds for all $A$ modules $M$ . In fact I think that this latter statement is false. Consider $A = \mathbb{Z}$ and $M = \mathbb{Z}/4\mathbb{Z}$ . As far as I can see, the ideal $(2)$ is in the support of $\overline{1}\in M$ , whereas $(3)$ isn't, because $(3)$ doesn't contain $4$ , and $4\cdot \overline{1} = 0\in M$ means $\overline{1}$ is $0$ in $M_{(3)}$ . This shows that the support of $\overline{1}$ is nonempty, while the it isn't the entirety of $\operatorname{Spec} A$ , and this means that $(0)$ can't be the generic point of the support. I would be very grateful if someone could point out where I am going wrong here.","['algebraic-geometry', 'commutative-algebra']"
4170757,Double integral of an off-centered circle.,"I'm new to double integral and doing this problem: (from the Supplemental Problems, MIT) Express each double integral over the given region R as an iterated integral in polar
coordinates. Use the method described in Notes I to supply the limits of integration. For
some of them, it may be necessary to break the integral up into two parts. In each case,
begin by sketching the region. b) The circle of radius 1, and center at (0, 1). What I have done so far: Since I know the area of a radius 1 circle is $\pi$ , the result must also be $\pi$ . I know the bounds must be from 0 to $\pi$ . Then, I do the following double integral: $$\int_0^{\pi}\int_0^{2sin\theta} 1 drd\theta $$ But the result is 4, which is a wrong answer. Therefore, my question is, where is the mistake? I do try to replace the $drd\theta$ term to $rdrd\theta$ and it gives me the correct answer, which is $\pi$ . But there are some problems, which do not use the $rdrd\theta$ term. For example, this problem: a) The region lying inside the circle with center at the origin and radius 2. and to the left of the vertical line through (−1, 0). The answer of this problem is: $$\int_{2\pi/3}^{4\pi/3} \int_{-sec\theta}^2 1 drd\theta$$ which does not include a $rdrd\theta$ term Furthermore, I will appreciate if anyone can help me to point out when to use the $drd\theta$ term and when to use $rdrd\theta$ term. P/S: Sorry for my bad English. I'm not a native speaker.","['multivariable-calculus', 'multiple-integral', 'polar-coordinates']"
4170835,Measure-Theoretic Importance Sampling: Do we need equivalence of measures?,"Let $\pi$ and $\mu$ be the target and proposal measures on $(X, \mathcal{X})$ respectively, with $\pi \ll \mu$ . Suppose $\lambda$ is the reference measure on $(X, \mathcal{X})$ and that $\pi\ll \lambda$ and $\mu\ll\lambda$ with densities $$
\frac{d \pi}{d\lambda} = \frac{\tilde{p}_\pi}{\displaystyle \int \tilde{p}_\pi \, d\lambda} \qquad \qquad \frac{d \mu}{d \lambda} = \frac{\tilde{p}_\mu}{\displaystyle \int \tilde{p}_\mu d\lambda}
$$ I would now like to find the usual expression for the Importance Sampling weights. For this, I consider the Radon-Nikodym derivative of $\pi$ with respect to $\mu$ $$
\begin{align}
W = \frac{d \pi}{d \mu} 
&= \frac{d \pi}{d \lambda} \frac{d \lambda}{d \mu} && \text{Chain rule requires $\pi \ll\lambda \ll \mu$} \\
&= \frac{d \pi}{d\lambda} \left(\frac{d \mu}{d \lambda}\right)^{-1} && \text{This requires $\lambda \ll \mu$ and $\mu \ll \lambda$ i.e. equivalence} \\
&= \frac{\tilde{p}_\pi}{\displaystyle \int \tilde{p}_\pi \, d\lambda} \cdot \left(\frac{\tilde{p}_\mu}{\displaystyle \int \tilde{p}_\mu d\lambda}\right)^{-1} \\
&= \frac{\tilde{p}_\pi}{\tilde{p}_\mu} \cdot \left(\frac{\displaystyle \int \tilde{p}_\mu d \lambda}{\displaystyle \int \tilde{p}_\pi d\lambda}\right)
\end{align}
$$ Which makes sense because this agrees with the usual set-up. However, for this to work, we must have $\mu$ and $\lambda$ equivalent, but surely this is never the case in practice? Is this derivation correct?  I.e. do we need $\mu\ll\lambda$ and $\lambda \ll\mu$ to get our usual Importance Sampling weights?","['measure-theory', 'statistics', 'sampling', 'probability-theory', 'probability']"
4170914,$\sum_{d|n} \phi (\tau (d))=?$,"So I have just taken a course on elementary number theory and started playing around with Eulers totient function $\phi (n)$ and $\tau (n):= \#\{\text{divisors of } n\}$ . I read about Gauss theorem which states that for positive integers $n\geq 1$ $$\sum_{d|n} \phi (d)=n$$ and then thought of a ""similar"" question which is stated in the title, that is $$\boxed {\sum_{d|n} \phi (\tau (d))=?}$$ I tried out some case that would be simpler than finding a general formula for this sum, which is when $n=p^{k}$ where $p$ is prime and $k$ is a positive integer. Reason being that there are simpler indentities to work with regarding $\phi (n)$ and $\tau (n)$ when $n$ is a power of a  prime. I will present what I found to be true on a more heuristic/evidential basis. Meaning that I have not yet proven the results. Here are my results: Let $p$ be a prime and $k$ a positive integer. Then we have that: $\boxed{\sum_{d|p^{k}} \phi (\tau (d))=\sum\limits_ {i=1}^ {k+1} \phi (i)=\sum\limits_ {i=1}^ {k} \phi (i)+\phi (k+1)}.$ $\boxed{\sum_{d|p^{k}} \phi (\tau (d))\stackrel{?}{\equiv} 0 \pmod 2}$ , if $k\geq 1$ . Proving the first identity is rather simple. Simply note that the divisors of $p^{k}$ are the following list of integers: $\underbrace{p^{0}, p^{1}, \ldots , p^{k-1}, p^{k}}_{k+1}$ . Meaning in particular that $p^{k}$ has exactly $k+1$ divisors. Thus $$\sum_{d|p^{k}} \phi (\tau (d))= \phi (\tau (p^{0}))+\phi (\tau (p^{1}))+\ldots +\phi (\tau (p^{k})) = \phi (1)+\phi (2)+\ldots +\phi(k)+\phi (k+1)=\sum\limits_ {i=1}^ {k+1} \phi (i)$$ which follow from the fact that $\tau (p^{k})=k+1$ . Thus $$\sum_{d|p^{k}} \phi (\tau (d))=\sum\limits_ {i=1}^ {k+1} \phi (i)=\sum\limits_ {i=1}^ {k} \phi (i)+\phi (k+1).$$ That $\boxed{\sum_{d|p^{k}} \phi (\tau (d))\equiv 0 \pmod 2}$ I found by experimentation but I dont know exactly how to prove this, given that I have not made any mistakes, that is... Since $\sum_{d|p^{k}} \phi (\tau (d))=\sum\limits_ {i=1}^ {k+1} \phi (i)$ one finds that: If $n=p^{1}$ , then $\sum\limits_ {i=1}^ {1+1} \phi (i)=\phi (1)+\phi (2)=1+1=2$ . If $n=p^{2}$ , then $\sum\limits_ {i=1}^ {2+1} \phi (i)=\phi (1)+\phi (2) + \phi (3)=2+2=4$ . If $n=p^{3}$ , then $\sum\limits_ {i=1}^ {3+1} \phi (i)=\phi (1)+\phi (2)+\phi (3)+\phi (4)=4+2=6$ If $n=p^{4}$ , then $\sum\limits_ {i=1}^ {4+1} \phi (i)=\sum\limits_ {i=1}^ {4} \phi (i)+\phi (5)=6+4=10$ , so on and so forth. I made calculations up to $n=p^{6}$ which gives $\sum\limits_ {i=1}^ {6+1} \phi (i)=18$ , which suggests that $$\sum_{d|p^{k}} \phi (\tau (d))\stackrel{?}{\equiv} 0 \pmod 2$$ if $k\geq 1$ . How would one go about proving this?","['number-theory', 'recreational-mathematics', 'solution-verification', 'elementary-number-theory']"
4170970,Why does the formula for $\tan 3\theta$ work at $\theta=\frac\pi4$ even though its proof involves cancelling $1-\tan^2\theta$?,"Here's a proof for the $\tan 3\theta$ formula: According to the procedure for the proof, when we cancel out $1-\tan^2\theta$ , the value of $1-\tan^2\theta$ should not be equal to $0$ (in particular, $\theta$ should not be $\frac{\pi}{4}$ ) as $\frac{ax}{bx}=\frac{a}{b}$ only if $x$ isn't $0$ . But, when we substitute $\theta=\frac{\pi}{4}$ in the final formula, we still get the correct answer: $$\frac{3\tan\frac{\pi}{4}-\tan^3\frac{\pi}{4}}{1-3\tan^2\frac{\pi}{4}}=\tan\frac{3\pi}{4}=-1$$ How is this possible?","['trigonometry', 'solution-verification']"
4170981,"If $X+Y$ follows exponential distribution with parameter $2 \lambda$, is it necessary $X$ and $Y$ follow exponential with parameter $\lambda$?","Let $Z=X+Y$ , $Z \backsim exp(2 \lambda)$ , with $X, Y$ i.i.d. Then, $$f_Z(Z=z)=2 \lambda e^{-2 \lambda(x+y)}$$ By the convolution function, $$f_Z(Z=z) = \int\limits_{- \infty}^{\infty}f_Y(z-x)f_X(x)dx $$ $$ \implies 2 \lambda e^{-2 \lambda(x+y)} = \int\limits_{- \infty}^{\infty}f_Y(z-x)f_X(x)dx$$ Differentiating both sides, $$-4 {\lambda}^2e^{-2 \lambda (x+y)} = f_Y(z-x)f_X(x)$$ Since $Y=Z-X$ and $X, Y$ are independent, $f_Y(z-x)f_X(x) = f(x,y)$ , the joint pdf. Then, $$f_X(x)=\int\limits_{- \infty}^{\infty}f(x,y)dy$$ $$f_X(x)=\int\limits_0^{\infty}-4 {\lambda}^2e^{-2 \lambda (x+y)}$$ $$f_X(x)= -4 {\lambda}^2e^{-2 \lambda x}\int\limits_0^{\infty}e^{-2 \lambda y}dy$$ Put $2 \lambda y = u,$ then, $dy = \frac{1}{2 \lambda}du$ $$f_X(x)= -4 {\lambda}^2e^{-2 \lambda x}.\frac{1}{2 \lambda}$$ $$f_X(x) = -2{\lambda}e^{-2 \lambda x}$$ What am I doing wrong that my marginal function for $X$ is not integrating to $1$ ? More importantly, is the general approach that I have taken to solving this problem correct?","['statistics', 'probability-distributions', 'solution-verification']"
4170986,subset of a basis of a separable metric space,"Show that if $X$ is a separable metric space, every basis for the topology on $X$ contains a finite or countable subset that's also a basis. I know how to show that every separable metric space has a countable basis, but here the problem is showing that every basis for the topology on $X$ has a finite or countable subset that's also a basis. Let $S$ be a basis for the topology on $X$ . Assume $S$ is uncountably infinite; if it's finite or countable, we can take $S$ itself to be the finite or countable subset that's a basis. Let $B=\{b_1,b_2,\cdots\}$ be a countable dense subset of $X$ . Then every open ball in $X$ has a point in $B$ . Also, I know how to show that the set of open balls centred at points in $B, $ defined by $S' :=\{B(b_i, \frac{1}{a}) : i,a\in\mathbb{Z}^+\}$ is finite or countable and is a basis for $X$ (i.e. a set $O$ is open in $X$ iff for every $o\in O, \exists U\in S'$ so that $a\in U\subseteq O$ ). I need to find a subset $S_0$ of $S$ so that $S_0$ is a basis and it is finite or countable. I think I should intuitively find some way to ""filter out"" unnecessary basis elements so that we only have countably many basis elements and this ""filtering"" will likely involve using the countable dense subset $B$ .","['calculus', 'general-topology', 'metric-spaces', 'real-analysis']"
4171000,"Holomorphic n-th covering map between annulus must be $z^n$, essentially.","Let $A_R$ denotes the annulus $\{ z\in \mathbb{C}:1<|z|<R \}$ . I guess and tend to prove the following(may not true): If $f:A_r\mapsto A_R$ is a holomorphic covering map with degree $n$ , then $R=r^n$ and $f$ must has the form $$f=\phi\circ z^n\circ \psi.$$ Where $\psi\in \text{Aut}A_r$ , and $\phi \in \text{Aut}{A_R}$ . Is this true? If not, does $R=r^n$ still holds(I strongly believe this is true)? This question rises when I look this post . Which says about $A_r$ and $A_R$ are conformal equivalent iff $R=r$ (they have the same moduli). So how about the covering map? I think may it should first prove $$\text{Mod}(A_R)=n\text{Mod}(A_r).$$ Here $\text{mod}$ means the moduli. I want use $$\text{Mod}(A_r)=\frac{1}{\lambda(\Gamma_r)},$$ where $\lambda$ is the extremal length. Give a metric on $A_R$ , pull back by $f$ to obtain a metric on $A_r$ I can prove $$\lambda(\Gamma_r)\geq n \lambda(\Gamma_R).$$ Hence $$\text{Mod}(A_R)\geq n \text{Mod}(A_r).$$ But for another direction, I don’t know how to continue.","['complex-analysis', 'covering-spaces']"
4171069,On the linearity of metric projections,"Let $X$ be a reflexive and strictly convex Banach space. If $V$ is a closed subspace of $X$ then for each $x \in X$ there exists a unique vector $P_V(x) \in V$ that solves the feasibility problem $ \inf_{v \in V} ||x-v|| $ .  The map $P_V \colon X \to V ,  ~ x \mapsto P_V(x) ,$ is called the   metric projection onto $V$ . In general, such a map need not be linear. It is linear if $X$ is a Hilbert space, since it coincides with the orthogonal projection on $V$ (which is linear) I was wondering if Hilbert spaces are the only ones with the property that each of its metric projection onto closed subspaces is linear. To be more exact, is the following statement true? Let $X$ be a reflexive and strictly convex Banach space. Suppose  that
each of its
metric  projections onto closed subspaces is linear. Then $X$ is a Hilbert  space. Any help is appreciated!","['hilbert-spaces', 'banach-spaces', 'functional-analysis', 'approximation-theory']"
4171139,Help in understanding a proof that $\mathbb{Q}_p^a$ is not complete,"I am trying to learn about $p$ -adic numbers using Robert's A Course in $p$ -adic Analysis .
In section II.1.4 he proves that the algebraic closure, $\mathbb{Q}_p^a$ , of $\mathbb{Q}_p$ is not complete by showing it's not a Baire space. The proof goes as follows: Let us define the sequence of subsets $$X_n= \{x\in\mathbb{Q}_p^a\mid [\mathbb{Q}_p(x):\mathbb{Q}_p]=n\},\quad n\geq 1\quad(1)$$ so that $\mathbb{Q}_p^a=\bigcup X_n$ . It is also obvious that $\lambda X_n\subset X_n$ ( $\lambda\in\mathbb{Q}_p$ ), $X_m+X_n\subset X_{mn}$ , and in particular $$X_n+X_n\subset X_{n^2}.\quad (2)$$ $\quad$ (a) These subsets are closed. If $x\neq 0$ is in the closure of $X_n$ , say $x=\lim x_i$ with a sequence $(x_i)$ in $X_n$ , then for each $x_i$ , let $f_i(X)\in\mathbb{Q}_p[X]$ be a polynomial of least degree with $x_i$ as a root and coefficients scaled so they lie in $\mathbb{Z}$ and at least one of them is in $\mathbb{Z}_p^\times$ . Extracting if necessary a subsequence of $(f_i)$ , we can assume that it converges (in norm, coefficient-wise), say $f_i\to f$ , so $f\in\mathbb{Z}_p[X]$ has degree less than or equal to $n$ and at least one coefficient in $\mathbb{Z}_p^\times$ , so $f(X)\neq 0$ . By the ultrametric property, the convergence $f_i\to f$ is uniform on all bounded sets of $\mathbb{Q}_p^a$ . Since the convergent sequence $(x_i)$ is bounded, we have $$f(x)-f_i(x_i)=\underbrace{f(x)-f(x_i)}_{\to 0}+\underbrace{f(x_i)-f_i(x_i)}_{\to 0}\to 0.$$ This implies $f(x)=\lim f_i(x_i)=0$ and $x\in X_n.\,\,(3)$ $\quad$ (b) The subset $X_n$ have no interior point. Since for any closed ball $B$ of positive radius in $\mathbb{Q}_p^a$ we have $\mathbb{Q}_p^a=\mathbb{Q}_p\cdot B$ $(4)$ , such ball cannot be contained in a subset $X_n$ , and no translates can be contained in $X_n$ . It seems like the definition in $(1)$ should be $X_n =\{x\in\mathbb{Q}_p^a\mid [\mathbb{Q}_p(x):\mathbb{Q}_p]\leq n\}$ so the equality in $(2)$ holds. Otherwise for $x\in X_n$ , $2x\in X_n+X_n$ but is not in $X_{n^2}$ since the degree of $x$ and $2x$ will be the same. Moreover, in $(3)$ Robert claims $x\in X_n$ since $f(x)=0$ and $\deg f\leq n$ , which only works if $X_n$ is the set of all elements of degree at most $n$ over $\mathbb{Q}_p$ . The problem with defining $X_n$ in this way is that it no longer has an empty interior since $\mathbb{Q}_p\subset X_n$ . So I am not sure what am I missing here. I'm also unsure why the equality in $(4)$ holds ( $\mathbb{Q}_p^a=\mathbb{Q}_p\cdot B$ ). Any help is appreciated!","['proof-explanation', 'general-topology', 'p-adic-number-theory', 'analysis']"
4171221,"For a normed vector space, is $\|x-y\| \leq \|x\|+\|y\|$ true?","I have a question about an inequality in normed vector spaces and I want to know if my proof is correct. Claim: Let $X$ be a normed vector space. Then \begin{equation} \|x-y\| \leq \|x\|+\|y\|\end{equation} for all $x,y \in X$ . Proof: Using the triangle inequality and the fact that $\|z\|=\|-z\|$ , we have \begin{equation}
\begin{split}
\|x-y\|&= \|x+(-y)\|\\
&\leq \|x\|+\|-y\| \\
&= \|x\|+\|y\|
\end{split}
\end{equation} Thanks for your answers!","['normed-spaces', 'solution-verification', 'vector-spaces', 'functional-analysis']"
4171223,Complex integration $\int_{0}^{1}\frac{\sqrt[3]{4x^{2}\left(1-x\right)}}{\left(1+x\right)^{3}}dx$,"I have integral $$I_1=\int_{0}^{1}\frac{\sqrt[3]{4x^{2}\left(1-x\right)}}{\left(1+x\right)^{3}}dx.$$ I tried something like this: $$f\left(z\right)=\frac{\sqrt[3]{4z^{2}\left(1-z\right)}}{\left(1+z\right)^{3}}=\frac{\left(1-z\right)}{\left(1+z\right)^{3}}\left(\frac{2z}{1-z}\right)^{\frac{2}{3}}$$ Using the figure I have $$I=\int_{C_{R}}f\left(z\right)dz+\int_{C_{r1}}f\left(z\right)dz+\int_{z_{1}}f\left(z\right)dz+\int_{C_{r2}}f\left(z\right)dz+\int_{z_{2}}f\left(z\right)dz.\tag1$$ Since I have only one singularity in my contour: $$\operatorname {Res}\left(f,z=-1\right)	=\frac{1}{\left(3-1\right)!}\cdot\lim_{z\rightarrow\left(-1\right)}\left[\left(z+1\right)^{3}\cdot f\left(z\right)\right]^{''}\\
	=\frac{1}{2}\cdot\lim_{z\rightarrow\left(-1\right)}\frac{-2\cdot4^{\frac{1}{3}}z^{2}}{9\left(-z^{2}\left(-1+z\right)\right)^{\frac{5}{3}}}\\
	=\frac{1}{2}\cdot\frac{-2\cdot4^{\frac{1}{3}}\left(-1\right)^{2}}{9\left(-\left(-1\right)^{2}\cdot\left(-1-1\right)\right)^{\frac{5}{3}}}\\
	=-\frac{2^{\frac{2}{3}}\cdot1}{9\left(2\right)^{\frac{5}{3}}}=-\frac{1}{9}2^{\frac{2}{3}-\frac{5}{3}}=-\frac{1}{9}2^{-1}=-\frac{1}{18}.$$ Thus I got $$I=2\pi i\cdot\left(-\frac{1}{18}\right)=-\frac{\pi i}{9}.$$ Now, I use Jordan's lemma and get: $$\lim_{z\rightarrow0}z\cdot f\left(z\right)=0$$ $$\lim_{z\rightarrow1}\left(z-1\right)f\left(z\right)=0$$ $$\lim_{z\rightarrow\infty}z\cdot f\left(z\right)=0$$ So I conclude that $$\lim_{r\rightarrow0}\int_{C_{r1}}f\left(z\right)dz=0$$ $$\lim_{r\rightarrow0}\int_{C_{r2}}f\left(z\right)dz=0.$$ $$\lim_{R\rightarrow\infty}\int_{C_{R}}f\left(z\right)dz=0.$$ When I put $r\rightarrow0$ and $R\rightarrow\infty$ into (1) and put that on $z_1$ the argument of the function  is 0, and on $z_2$ the argument is $2\pi \cdot \frac{2}{3}$ then I have: $$I_{1}\left(1-e^{i\frac{4\pi}{3}}\right)=-\frac{\pi}{9}i.$$ Now I have the problem because on the left side I have real and imaginary part and on the right hand side I only have imaginary part and I don't know what to do.","['complex-analysis', 'contour-integration', 'complex-integration']"
4171226,Is this a bundle?,"In Frederic Schuller's lecture series Lectures on The Geometrical Anatomy of Theoretical Physics, he gives an example of a bundle $E\overset{\pi}{\rightarrow}M$ where different points of the base manifold have different fibres: Or, as spelled out in Simon Rea's transcription of the lectures, found here (p. 39): $$F_p:=\mathrm{preim}_\pi(\{p\}) \cong_\mathrm{top}
\begin{cases} 
 S^{1} & p < 0\\
 \{ p \} & p = 0\\
 [0,1] & p > 0\\
\end{cases}$$ The example is great for intuition. But I don't see how E can be a manifold with such an odd structure (as a reminder, the definition of $E\overset{\pi}{\rightarrow}M$ requires E to be a topological manifold).",['differential-geometry']
4171233,$\int_0^\pi f(x)^2dx = \int_0^\pi f'(x)^2dx$?,"Let $f \in C_{ℝ}([0, \pi])$ satisfy $f(0) = f(\pi) = 0$ and $f' \in L^2([0, \pi])$ . It can be shown (using Parseval's Theorem and the fact that for the Fourier coefficients we have $c^{f'}_n = inc^f_n$ ), that $$\int_0^π f(x)²dx ≤ \int_0^π f'(x)²dx.$$ (Ask me for a proof.) Now I'm asked to determine when actually equality holds, with the hint: ""extend $f$ to an odd function on $[-π, π]$ "". Without that hint, I arrive at some answer, namely those functions $2ic · \sin(x)$ for any $c ∈ ℝ$ . However I never used the hint, really.. So that throws me off. The solutions I have seem to satisfy, but have I missed others? (In terms of that hint, I do know that for odd functions $f$ , the Fourier coefficients satisfy $c^f_{-n} = -c^f_n$ , which will probably come into play at some point.)","['fourier-analysis', 'analysis']"
4171249,Computing the limit $\lim_{k \to \infty} \int_0^k x^n \left(1 - \frac{x}{k} \right)^k \mathrm{d} x$ for fixed $n \in \mathbb{N}$,"I'm working on a problem that asks to compute $$\lim_{k \to \infty} \int_0^k x^n \left(1 - \frac{x}{k} \right)^k \mathrm{d} x$$ for fixed $n \in \mathbb{N}$ . What I've tried so far is to do a $u$ -substitution for $u = \frac{x}{k}$ , so I have $$\int_0^k x^n \left(1 - \frac{x}{k} \right)^k \mathrm{d} x = k^{n + 1} \int_0^1 u^n (1 - u)^k \mathrm{d} u .$$ Using the Binomial Theorem to break up the $(1 - u)^k$ term, I get $$k^{n + 1} \int_0^1 u^n (1 - u)^k \mathrm{d} u = k^{n + 1} \int_0^1 \sum_{j = 0}^k \binom{k}{j} \frac{(-1)^j}{n + j + 1} .$$ However, I don't know how to compute the limit of this expression as $k \to \infty$ . I assume that I should recognize it as some kind of Taylor series that's somehow $O \left( k^{-(n + 1)} \right)$ , but I'm not seeing it. Note: When looking at other posts, I found an integral that looked similar to this one, and the only answer on that post involved something called a beta function. I have never heard of a beta function, and would like to find a solution here that doesn't rely on whatever a beta function is. Another idea I considered was to use the Dominated Convergence Theorem, since $e^{-x} = \lim_{k \to \infty} \left( 1 - \frac{x}{k} \right)^k$ , so I figured I could use DCT to say that \begin{align*}
\lim_{k \to \infty} \int_0^k x^n \left( 1 - \frac{x}{k} \right)^k \mathrm{d} x & = \lim_{k \to \infty} \int_0^\infty \chi_{[0, k]}(x) x^n \left( 1 - \frac{x}{k} \right)^k \mathrm{d} x \\
& = \int_0^\infty x^n e^{-x} \mathrm{d} x & (\textrm{DCT used here})\\
& = n ! ,
\end{align*} assuming I didn't mess up any of my integration by parts. However, I couldn't find a choice of dominator that would work on all of $[0, \infty)$ , so I'd also be interested in a solution that uses DCT as well. Perhaps I'm being naive, but the pointwise limit is just so convenient that I have to imagine that DCT can be used here. EDIT: Thanks to some inspiration from a comment by user Mars Plastic, I thought to consider some other integral convergence theorems. I came up with this, which I think works. I'm still interested to see if the Dominated Convergence Theorem argument can be made to work, perhaps a bit more smoothly than this. Let $f_k(x) = \chi_{[0, k]}(x) x^n \left( 1 - \frac{x}{k} \right)^k$ . I claim that the sequence is monotone increasing on $[0, \infty)$ . Fix $x \in (0, \infty)$ , and let $K = \lfloor k \rfloor + 1$ , so that $K = \min \{ k \in \mathbb{N} : f_k(x) \neq 0 \}$ . Obviously if $k < K$ , then $f_k(x) = 0 \leq f_{k + 1}(x)$ . So consider the case where $k \geq K$ . Then $f_k(x) = x^n \left(1 - \frac{x}{k} \right)^k$ , and based on answers to this question , it seems this sequence would be monotone increasing. Therefore, I can apply the Monotone Convergence Theorem to say that $f_k(x) \nearrow x^n e^{-x}$ , so $$\int_0^k x^n \left( 1 - \frac{x}{k} \right)^k \mathrm{d} x = \int_0^\infty f_k(x) \mathrm{d} x = \int_0^\infty x^n e^{-x} = n! .$$","['integration', 'riemann-integration', 'lebesgue-integral', 'real-analysis']"
4171294,Angle at intersection by diagonals of a quadrilateral,"Suppose ABCD is a quadrilateral such that $ \angle BAC=50, \angle CAD=60,  \angle CBD=30$ and $\angle BDC = 25$ . If $E$ is the point of intersection of $AC$ and $BD$ ,then the value of $ \angle AEB $ is __ Source Here is a picture I've drawn of the set up, I find that in $\triangle BCD$ , we can find $ \angle BCD=125$ . I am not sure how to proceed after that. The most theorems relating angles of a quadrilateral are for cyclic quadrilaterals and this quadrilateral here is not cyclic so I am not sure how to proceed.","['contest-math', 'quadrilateral', 'problem-solving', 'geometry']"
4171306,Showing a series is finite a.e. given an $L^p$ function.,"Q: Suppose $f \in L^2 (\mathbb{R})$ . Show that $\sum_{n = 1}^\infty \frac{f(x + n)}{n^{3/4}}$ is finite a.e. $x \in \mathbb{R}$ . It seems that we have to show the sum is in $L^p (\mathbb{R})$ for some $p \geq 1$ . Let's try $p = 2$ , applying Cauchy-Schwarz inequality, we have $$\int \left|\sum_{n = 1}^\infty \frac{f(x + n)}{n^{3/4}}\right|^2 dx \leq \left(\sum_{n = 1}^\infty \frac{1}{n^{3/2}}\right) \int \left(\sum_{n = 1}^\infty (f(x + n))^2\right) dx = C \sum_{n = 1}^\infty \|f\|_2^2 = \infty .$$ It seems that this estimate is too large. For $p = 1$ , it seems that a similar situation occurs when CS-inequality is applied. I am thinking if we can show $$\int \left|\sum_{n = 1}^\infty \frac{f(x + n)}{n^{3/4}}\right| dx \leq \sum_{n = 1}^\infty \int \frac{[f(x + n)]^2}{n^{3/2}} dx,$$ then we are done. However I have no idea how to get this (or a similar) estimate.","['measure-theory', 'real-analysis']"
4171347,Wasserstein distance between two empirical measures,"Suppose that $\{X_i\}_{1\leq i\leq N}$ and $\{Y_i\}_{1\leq i\leq N}$ are $\mathbb{R}$ -valued random variables. I found in some manuscripts the inequality $$\mathbb{E}\left[W^2_2\left(\frac{1}{N}\sum_{i=1}^N \delta_{X_i}, \frac{1}{N}\sum_{i=1}^N \delta_{Y_i}\right)\right] \leq \mathbb{E}\left[\frac{1}{N}\sum_{i=1}^N |X_i-Y_i|^2\right] \quad (*).$$ I believe this is not that obvious... May I know how can one justify this upper bound on the squared Wasserstein distance (with exponent $2$ )?","['inequality', 'optimal-transport', 'probability-theory']"
4171355,An integration on the sphere (a rescaling problem),"Let $\mathbb{S}^{n-1}$ be the unit sphere in $\mathbb{R}^{n}$ and
let $f$ be smooth on $\mathbb{R}^{n}$ . Is it  possible to express the integral $$I(\lambda):=\int_{\mathbb{S}^{n-1}}f(\lambda x_1,x_2,\cdots,x_n) d\sigma(x),\qquad \lambda >0$$ in terms of the integral $$J:=\int_{\mathbb{S}^{n-1}}f(x_1,x_2,\cdots,x_n) d\sigma(x).$$ This is a very simple special case of the my old question here Changing variables in integration over spheres Some thought: If the point $(x_1,x_2,\cdots,x_n)$ lives on the sphere $x_1^2+\cdots+x_n^2=1$ then $(\lambda x_1,x_2,\cdots,x_n)$ lives on
the ellipsoid $E_{\lambda}:=\{(x_1,x_2,\cdots,x_n)\in \mathbb{R}^n: x_1^2/\lambda^2+\cdots+x_n^2=1\}$ . So, I am guessing $$I(\lambda):=\int_{E_{\lambda}}f( x_1,x_2,\cdots,x_n) d\tilde{\sigma}(x)\qquad (1)$$ where $d\tilde{\sigma}$ is the surface measure on the ellipsoid $E_\lambda$ . An edit : Thanks to the comments below I realize the question so put does not make sense. So, I modify:
Can we express $I(\lambda)$ in terms of $J$ where $$J:=\int_{\mathbb{S}^{n-1}}f(x_1,x_2,\cdots,x_n) d\widetilde{\sigma}(x)$$ where $d\widetilde{\sigma}$ is some kind of a weighted surface measure on the sphere, obviously realted to the natural measure $d{\sigma}$ ?","['multivariable-calculus', 'differential-geometry', 'spherical-harmonics', 'real-analysis']"
4171362,ODE's Hale's Book pg. 27 Lemma 4.1 Exercises 4.1,"I am studying with Hale's book on ODE's. There it does not have the demonstration of the following motto: LEMMA 4.1 If $f$ is either independent of $t$ or periodie in $t$ , then the solution $x=0$ of $\dot{x}(t)=f(t,x(t))$ being stable (asymptotically stable) implies the solution $x=0$ of $\dot{x}(t)=f(t,x(t))$ is uniformly stable (uniformly asymptotically stable). could you help me with the proof ? I couldn't develop anything. Any script or hints would be helpful","['analysis', 'ordinary-differential-equations']"
4171366,set of invertible diagonal matrices,"Let $\mathcal{T}$ be the set of invertible diagonal matrices. Show that for any invertible matrix $B$ such that $B\mathcal{T}B^{-1} = \mathcal{T}, B=PT$ for some permutation matrix $P$ and invertible diagonal matrix $T.$ Here, $AB := \{ab: a\in A, b\in B\}$ when $A$ and $B$ are sets. I'm not sure how to show this result, though I'm pretty sure I need to consider eigenvalues, diagonalizable matrices, and change of basis matrices. Using the definition alone, I get stuck quite easily; I only know that for any invertible diagonal matrix $T, BTB^{-1}$ is an invertible diagonal matrix and for any invertible diagonal matrix $T', T' = BT''B^{-1}$ for some invertible diagonal matrix $T''.$ I know how to show that if an $n\times n$ matrix has $n$ distinct eigenvalues, then it has $n$ distinct eigenvectors (an eigenvector can correspond to only one eigenvalue) and thus these $n$ eigenvectors are linearly independent, so the matrix is diagonalizable, but I'm not sure if this is useful. Clarification: Here $B\mathcal{T}B^{-1} := \{BTB^{-1} : T\in \mathcal{T}\}$","['permutations', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'linear-transformations']"
4171385,Proving Set Theory Basics,"Quick introduction, I am a just graduated high school senior working on logic and proofs just to get a better feel for my mathematics degree this fall. My book does not have a solutions page. Here is my proof to the following equality. $$
A \cup(B \cap C)=(A\cup B)\cap (A\cup C)
$$ Suppose $x \in (A\cup B)\cap (A\cup C)$ . This means $x$ must lie in $A\cup B$ and $A\cup C$ . Since $x \in A \cup B$ and $x \in A \cup C$ , then $x \in A$ . Since $x \in A$ , then $x \in A \cup (B\cap C)$ . I am unsure if the last step is strong enough to show equivalence. Thanks for the help!",['elementary-set-theory']
4171419,Can we say that a function is increasing/decreasing at a point where tangent is vertical?,"I am analyzing this function: $f(x)=\sqrt[3]{x^3-x}$ . The derivative is $f'(x)=\frac{3x^2-1}{3\sqrt[3]{(x^3-x)^2}}$ . I found that $f'(x)>0$ when $x<-1 \text{ or } -1<x<-\frac{1}{\sqrt 3}$ . Can I say that the function is increasing on interval $(-\infty, -\frac{1}{\sqrt 3})$ (include $x=-1$ even though the derivative does not exist at that point? I am thinking that the function continues to increase. It's been awhile since I did any analysis like this but I believe the definition of increasing function does not involve the derivative. Am I right?","['functions', 'derivatives', 'real-analysis']"
4171436,Example of a measure space with constraints on values of measure,"I need to find a measure space $(X,S,\mu)$ such that $$
\{\mu(E)|E\in S\} = [0,1]\cup[3,\infty] .
$$ My idea: I know the result for $[0,1]\cup[3,4]$ in the right hand side because in that case if we consider any probability measure $P$ on $(-\infty, 0)$ and we add $P$ with point measure $3\delta_2$ , then it will give the measure with range $[0,1]\cup[3,4]$ but I have no idea what to do if the range is $[0,1]\cup[3,c]$ where $c>4$ . If we get above then in limiting sense we can get a measure  with range $[0,1]\cup[3,\infty]$ . Am I  right? Please give me some hint.","['measure-theory', 'lebesgue-measure', 'real-analysis']"
4171437,"Surface area of revolution $y=\sin{x}$ for $x\in[0,\pi]$ , I always get half of the correct answer","So we know the formula for the surface area of revolution: $$\left|S\right|=2\pi\displaystyle\int\limits_a^b\left|f(x)\right|\sqrt{1+\left(f'(x)\right)^2} 
\space dx$$ For this question: $$\left|S\right|=2\pi\displaystyle\int\limits_0^{\pi}\left(\sin{x}\right)\sqrt{1+\cos^2{x}} 
\space dx$$ So I decided to use substitution method for $\underline{\cos{x}=a}$ : $$-2\pi\displaystyle\int\sqrt{1+a^2}\space da$$ After this part, it gets more complicated. Now I know about hyperbolic trigonometric functions, however, I must solve this without using them. Hence I searched and found this and this videos where the integration is solved the supposed way, the answer being $\dfrac{1}{2}\sqrt{1+x^2}\cdot x+\dfrac{1}{2}\ln{\left|\sqrt{1+x^2}+x\right|}+C$ , which for me, is: $$-\dfrac{\pi}{2}\left[\sqrt{1+a^2}\cdot a+\ln{\left|\sqrt{1+a^2}+a\right|}\right]+C=-\dfrac{\pi}{2}\left[\sqrt{1+\cos^2{x}}\cdot\cos{x}+\ln{\left|\sqrt{1+\cos^2{x}}+\cos{x}\right|}\right]\Bigg|_0^{\pi}$$ When we calculate, we get: $-\dfrac{\pi}{2}\left[-2\sqrt{2}+\ln{\left(\dfrac{\sqrt{2}-1}{\sqrt{2}+1}\right)}\right]\approx7.21$ The problem is, when I calculate this problem on WolframAlpha , it gives me about $14.42$ , which is twice my answer. I am unsure where did I make the mistake.","['integration', 'definite-integrals', 'surface-integrals', 'analysis', 'calculus']"
4171446,Use of choice function in Urysohn's lemma?,"Let $A = \mathbb Q \cap [0,1]$ . In the proof of Urysohn's lemma, we construct a family of open set such that: $\{V_q\}_{q \in A}$ , where if $r_1 < r_2$ , then $V_{r_1} \subset \subset V_{r_2}$ . The construction was made possible by some choice $\alpha: A \rightarrow \{\text{open set}\}$ . Is there a particular reason why we chose $A = \mathbb Q \cap [0,1]$ , i.e. a countable, dense set of $[0,1]$ ? Why can't we choose $A = [0,1]$ ? We still have a choice function $\alpha: [0,1] \rightarrow \{\text{open sets}\}$ such that if $r_1 < r_2$ , then $\alpha(r_1) \subset\subset \alpha(r_2)$ .",['general-topology']
4171475,"Finding $\int^1_0 \frac{x\,\mathrm dx}{((2x-1)\sqrt{x^2+x+1} + (2x+1)\sqrt{x^2-x+1})\sqrt{x^4+x^2+1}}$","$$\int^1_0 \frac{x\,\mathrm dx}{((2x-1)\sqrt{x^2+x+1} + (2x+1)\sqrt{x^2-x+1})\sqrt{x^4+x^2+1}}$$ I've tried : Substitution: $t = x^2$ $x = \cos{t}$ Definite Integral Properties like $a+b-x$ (which messes up the integral symmetry) Partial Integration by breaking integral into into $\frac{x}{\sqrt{\left(x^4+x^2+1\right)}}$ Partial Fractions which are very ugly As per wolframAlpha, a beautiful elementary closed form exists $$
\frac{\sqrt{\left(x^4+x^2+1\right)}}{3}\left(\frac{1}{\sqrt{\left(x^2-x+1\right)}}- \frac{1}{\sqrt{\left(x^2+x+1\right)}}\right)
$$ I'll be happy with solution of just definite integral, bonus points if you get closed form solution too.","['integration', 'indefinite-integrals', 'definite-integrals']"
4171548,In how many ways 5 different rings can be worn on 4 fingers?,"Is the answer $4^5$ or $4 \times 5 \times 6 \times 7 \times 8$ ? The rationale behind $4 \times 5 \times 6 \times 7 \times 8$ is that first ring has four options, the second ring has $5$ options, since the second ring can come underneath the first, and so on. I think $4^5$ is correct since $4 \times 5 \times 6 \times 7 \times 8$ is over counting the number of possibilities.","['permutations', 'combinatorics']"
4171651,"A prime numbers property: $4/\pi=e^{1/2}\Big(1+e^{1/3}\big(1+e^{1/5}(\dots)\big)\Big)/p_n, \ \ n\to\infty$","It is possible to show that the limit of the following fraction is a constant $$ \frac {e ^ {1/2} \Big (1 + e ^ {1/3} \big (1 + e ^ {1/5} (1 + e ^ {1/7} (\dots (1 + e ^ {1 / p (n)})} {p(n)} \sim c $$ How can I determine the value of the constant $ c $ ? $ \ $ From the prime number theorem we have that $ \pi (x) \sim \text {li} (x): = \int_0 ^ x \frac {dt} {\ln (t)} $ , where $ \pi (x ) $ is the prime counting function. $ \pi (n) $ can be thought as the inverse of the function $ p (n) $ which gives the $n$ -th prime number, such that $ p (n) \sim \text {ali} (x) = \text {li} ^ {- 1} (x) $ . Calling $ \text {ali} (x) $ the inverse function of the logarithmic integral: $$\text{li}(\text{ali}(x))=x \ \ \Rightarrow \ \ \frac{d}{dx}\Big[\text{li}(\text{ali}(x))\Big]=1\Rightarrow \text{ali}'(x)=\ln(\text{ali}(x))$$ $$\text{ali}''(x)=\frac{d}{dx}\Big[\ln(\text{ali}(x))\Big]=\frac{\text{ali}'(x)}{\text{ali}(x)} \Rightarrow \text{ali}'(x)=\text{ali}(x) \text{ali}''(x)\sim p(x) \ \text{ali}''(x) $$ Solving the differential equation $ y '(x) = p (x) y' '(x) $ we obtain: $$y(x)=c_2+c_1\int_{1}^{x}\exp \Big(\int_1^{t}\frac{du}{p(u)}\Big)dt$$ Interpreting the integral as the area under a broken line, we can write for the integer $ n $ : $$y(n)=c_2+c_1\sum_{v=1}^n \exp\Big(\sum_{w=1}^{v}\frac{1}{p(w)}\Big)\sim p(n)$$ $$y(4)=c_2+c_1 \Big( e^{\frac{1}{2}}+e^{\frac{1}{2}+\frac{1}{3}}+e^{\frac{1}{2}+\frac{1}{3}+\frac{1}{5}}+e^{\frac{1}{2}+\frac{1}{3}+\frac{1}{5}+\frac{1}{7}}\Big)=c_2+c_1 \Big( e^{1/2}\Big(1+e^{1/3}\Big(1+e^{1/5}\Big(1+e^{1/7}\Big)\Big)\Big)\Big)\approx 7$$ This means that $ \big (y (n) -c_2 \big) / p (n) \sim c_1 $ . Setting $ c_2 = 0 $ , the trend of $ y (n) / p (n) $ is as follows: The orange line represents $ \frac {4} {\pi} $ but there is no reason for it to be the limit, I did it for clickbait   :-\","['number-theory', 'pi', 'euler-product', 'sequences-and-series', 'prime-numbers']"
4171662,What is the benefit of defining a positive norm for vectors?,"I read that the reason we have the property $\langle A|B\rangle=\langle B|A\rangle^*$ is to make define a positive norm with the formula $\langle A|A\rangle$ . But I do not understand how having this norm benefits us. I guess we're doing this to make an analogy with arrows, which also have a positive norm. But this can't be the only reason. After all, a lot of things which are true for arrows are not true for general vectors. For instance, angle values of $-2\pi$ to $2\pi$ are not carried over from arrows to general vectors. The formula for the angle between general vectors, $\cos \theta=\frac{\langle A|B\rangle}{|A||B|}$ , can result in complex values of $\theta$ . The commutativity of the inner product isn't carried over from arrows to general vectors either (though this is the very reason a positive norm gets carried over). Keeping a positive norm for general vectors must be allowing us to carry over some nice properties from the world of arrows to general vectors. What are those nice things? Like, even if we drop this property, we'd still be able to prove the existence of an orthonormal basis, as Gram Schmidt does not require $\langle A|B\rangle=\langle B|A\rangle ^*$ . So at least that stuff still works out. EDIT- I just realised that, while Gram Schmidt may not require $\langle V|V\rangle$ to be strictly positive, it does require $\langle V|V\rangle$ not to be 0 for non-zero vectors $|V\rangle$ , because only then can we rescale the basis vectors by their norm to get a unit vector. EDIT- I also realised that the Cauchy Schwarz and Triangle Inequalities would no longer make sense without this norm. Maybe these are useful results too.","['inner-products', 'normed-spaces', 'linear-algebra']"
4171719,"$f(x)=\begin{cases} 0 & \text{ if } x\in (-\infty,0) \\ 1 & \text{ if } x\in [0,+\infty] \end{cases}$, continuous?","Question: is $f: (\mathbb{R},τ) \rightarrow (Y,σ)$ with $f(x)=\begin{cases}
0 & \text{ if } x\in (-\infty,0) \\ 
1  & \text{ if } x\in [0,+\infty] 
\end{cases}$ , continuous? Where $τ$ is the usual topology of $\mathbb{R}$ , $Y=\{0,1,2\}$ and $σ=\{\emptyset,Y,\{0\}\}$ my answer would that its continuous since $f^{-1}(\emptyset) \in τ$ , $f^{-1}(Y)\in τ$ and $f^{-1}(\{0\})= (-\infty,0)\in τ$ Is my answer correct? If not, how can I properly answer and prove it?",['general-topology']
4171729,Why does the MOG provide the octads of a Steiner system,"I have been reading about the Mathieu finite simple groups, which has taken me on an interesting detour through coding theory, and the Miracle Octad Generator (MOG). There are a lot of instructions online on how the MOG works, but much less on why it works, and it's the why I'd like some pointers on. In terms of the how , I understand that you've got 24 points laid out onto a grid, and obeying some rules, you can extend any 5 of them to an octad of the steiner system S(5,8,24). The rules are based around parity of the rows and columns, and scoring the columns to give hexacode words , which are all words from a 6-dimensional codebook over the finite field $F_4$ . Although a tad fiddly, this is all well and good. In order to try and find out why it works, I have read the reference everyone gives: chapter 11 of Conway/Sloane's Sphere Packing, Lattices and Groups. But (at least it seemed to me) this source only described how to use the MOG, not why it worked. Another source that looked promising was a master's thesis by Sitt Chee Keen ( http://eprints.usm.my/6567/1/ON_STEINER_SYSTEM_S%285%2C_8%2C_24%29_AND_THE_MIRACLE_OCTAD_GENERATOR_%28MOG%29.pdf ) but I could only find the first half online. Like everything Conway was interested in, it still just feels a bit magic! Could anyone give me any intuition as to what is going on behind the scenes in the MOG to make it work? Why the hexacode? How might you prove it? Any good references? Any help much appreciated, thank you.","['finite-groups', 'simple-groups', 'coding-theory', 'combinatorics']"
4171733,Invertibility of compact operator on infinite dimensional Hilbert space,"I am trying to solve an exercise from the book Analysis for Applied Mathematics by Ward Cheney. Exercise 2.3.25 states: ""Prove that if $X$ is an infinite-dimensional Hilbert space, then a compact operator on $X$ cannot be invertible."" I interpret the question to mean that the operator maps $X$ to a general normed linear space $Y$ . I have been able to show that if the inverse exists, then it cannot be bounded/continuous. This is also covered in several questions on this site. This also takes care of the case where the operator maps to a Banach space because the interior/open mapping theorem then implies that if the inverse exists then it must be bounded/continuous. Is the assertion in the exercise true if the operator maps $X$ to $Y$ , where $Y$ is a normed linear space? After failing to prove this myself I have tried searching both this site, as well as google, but still have not found a proof. As I am learning functional analysis for the first time, I realize I might have overlooked something I did not quite understand. I have seen some mentions of spectral theory when searching, but this is not covered before the next chapter in the book. Thanks in advance to anyone that took the time to read this post.","['hilbert-spaces', 'compact-operators', 'functional-analysis']"
4171737,What's the third derivative of $\log( \det X)$ with respect to $X$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question If $X$ is positive definite matrix, what is the third derivative of $\log(\det X)$ with respect to $X$ ? We know the first derivative of $\log(\det X)$ is $X^{-1}$ and the second derivative (hessian) is $X^{-1} \otimes X^{-1}$ but I don't know the third derivative of $\log(\det X)$ with respect to $X$ . Thanks in advance.","['multivariable-calculus', 'statistics', 'matrix-calculus', 'derivatives']"
4171765,Is a function differentiable if its directional derivatives converge uniformly on the unit vectors?,"Given a function $f:\mathbb{R}^n \to \mathbb{R}$ , we say that its directional derivatives converge in $\mathbf{a}\in \mathbb{R}^n$ uniformly with respect to the unit vectors if $\forall \varepsilon >0$ there exists $\delta >0$ such that, for every $t \in B_\delta(0)$ , $$
\left|\frac{f(\mathbf{a}+t\mathbf{v})-f(\mathbf{a})}{t}-D_\mathbf{v}f(\mathbf{a})\right|<\varepsilon \quad \forall\, \mathbf{v}\,:\,\|\mathbf{v}\|=1
$$ where $D_\mathbf{v}f(\mathbf{a})$ denotes the direction derivative with respect to $\mathbf{v}$ . Now,
I know that the existence of all the directional derivatives doesn't guarantee the differentiability of a function, but I was wondering, what can we say if its directional derivatives converge uniformly on the unit vector? Intuitively I'd say that the function is also differentiable in $\mathbf{a}$ , but I'm not exactly sure how to show it. Here's my attempt : I'd like to show that $\lim_{{x} \to {a}} 
\frac{{f}({x})-{f}({a}) - {T}({x}-{a})}{\|{x}-{a}\|} = 0$ for some linear transformation $T$ . Ignoring for a moment the part about the linear transformation, I can rewrite the first inequality as: $$
\left|\frac{f(\mathbf{x})-f(\mathbf{a})-D_\mathbf{v}f(\mathbf{a})\|\mathbf{x}-\mathbf{a}\|}{\|\mathbf{x}-\mathbf{a}\|}\right|<\varepsilon \quad \forall\, \mathbf{x}\,:\,\|\mathbf{v}\|=1
$$ with $\mathbf{x}=\mathbf{a}+t\mathbf{v}$ and $\|t \mathbf{v}\|=t=\|\mathbf{x}-\mathbf{a}\|$ . At this point I'm stuck; maybe I'm missing something trivial. Did I do something wrong? Is the claim even true?","['calculus', 'derivatives', 'real-analysis']"
4171814,Expected square of frog's distance to the centre in the limit of infinite number of jumps: $\lim\limits_{n\to\infty}E[|r_n|^2]$,"Consider a unit circle on a plane and a frog in its centre. The frog
makes infinite series of jumps. For the $n^{th}$ jump, the frog
chooses a random point $x_n$ on the circle and jumps to the middle of the segment connecting its current position $r_n$ with the point $x_n$ . Find the expected square of the frog's distance to the centre in the limit of an infinite number of jumps: $\lim\limits_{n\to\infty}E[|r_n|^2]$ I'm kind of lost in this problem and don't know where to start. Albeit, I understand a few things, e.g. the area of the unit circle will be $\pi$ and the segment that will be created by $x_n$ and $r_n$ , the centre of the segment would be $\frac{1}{2}$ units from the origin (unit circle). But that's it, any help is appreciated!","['expected-value', 'statistics', 'probability']"
4171853,Variational Problem has no minimizer,"I am given the problem $$
\min_{u \in H_0^1([0, 1])} \int^1_0 xu'(x)^2~\mathrm{d}x
$$ and supposed to prove that the problem has no minimizer. But the integrand is bounded below by zero, and thus so is the functional that we want to minimize. The zero function is admissible and therefore $0$ is a minimizer. Where is my mistake? Or is there a problem with the task?","['calculus-of-variations', 'real-analysis', 'sobolev-spaces', 'optimization', 'derivatives']"
4171862,Periodic solutions are Lyapunov Stable,"Consider the ODE $$
\left\{ \begin{align*}
\dot{x}=&y\\
\dot{y}=&-x^2-bx-c.
\end{align*}\right.
$$ Under the assumption that $b^2-4c>0$ , we find the equilibria $P_1=\left(\frac{-b+\sqrt{b^2-4c}}{2},0\right)$ and $P_2=\left(\frac{-b-\sqrt{b^2-4c}}{2},0\right)$ . Around $P_1$ , the linearized system allow us to conclude that $P_1$ is a center and orbits around $P_1$ are periodic. Using the first integral $F(x,y)=\frac{y^2}{2}+\frac{x^3}{3}+\frac{bx^2}{2}+cx$ around $P_1$ , we can use Morse Lemma to prove that these periodic solutions also occur on the non linear system as well, and therefore, stable. I want to prove, however, that these periodic solutions (for the non linear system) are Lyapunov stable. Looking at the system on polar coordinates gave me nothing. Can someone give me a hint (not an answer)? As reference, this is from exercise 5.4 from Verhulst's ""Nonlinear Differential Equations and Dynamical Systems"". Thanks in advance!","['morse-theory', 'ordinary-differential-equations', 'dynamical-systems']"
4171903,Why do we use $...$ with irrational numbers?,"That $...$ in title means the $...$ used at the end of example in question body. So my question is you must have numbers/patterns like $$ W={1,2,3 ...} $$ Here $ .. $ makes sense. Also $$ \frac{1}{3}=0.33... $$ Here also $...$ makes sense. However when we take an irrational numbers we write it as $ 1.43857358357385... $ Now if we asked to find the fifth term of first $2$ examples we can easily tell this that is why we use point. However we don't know the next digit which will we used. Then why do we use $...$ ? Background - Today I was learning set theory and there $...$ came which made me think and ask this question. I have tried fully to add context but sorry if it lacks context.","['pattern-matching', 'irrational-numbers', 'decimal-expansion', 'elementary-set-theory', 'pattern-recognition']"
4171907,"If $3\sin x +5\cos x=5$, then prove that $5\sin x-3\cos x=3$","If $3\sin x +5\cos x=5$ then prove that $5\sin x-3\cos x=3$ What my teacher did in solution was as follows $$3\sin x +5\cos x=5 \tag1$$ $$3\sin x =5(1-\cos x) \tag2$$ $$3=\frac{5(1-\cos x)}{\sin x} \tag3$$ $$3=\frac{5\sin x}{(1+\cos x)} \tag4$$ $$5\sin x-3\cos x=3 \tag5$$ However this should not be true when $\sin x=0$ , as division by zero is not defined; and also, if $\sin x=0$ , then the expression we have to prove evaluates to $-3$ . In other words question is incomplete but my teacher denied it. Am I correct in my reasoning?",['trigonometry']
4171933,How to solve the following Laplace equation,"Im struggling to find a solution for the following problem $$ \begin{cases}
\varDelta u\left(x,y\right)=0 & \forall\left(x,y\right)\in\left(0,2\right)\times\left(0,2\right)\\
u\left(x,0\right)=\sin\left(\pi x\right) & \forall x\in[0,2]\\
u\left(x,2\right)=\sin\left(3\pi x\right) & \forall x\in[0,2]\\
u\left(0,y\right)=0 & \forall y\in[0,2]\\
u\left(2,y\right)=\min\left\{ y,\left(2-y\right)\right\}  & \forall y\in[0,2]
\end{cases} $$ I'll explain what I have tried. I dont know to solve this equation as is. What I do know is to solve Laplace equation's with homogenous boudary conditions. So I tried to find a function $ w(x,y) $ such that either $$ \begin{cases}
u\left(x,0\right)-w\left(x,0\right)=0, & \forall x\in[0,2]\\
u\left(x,2\right)-w\left(x,2\right)=0 & \forall x\in[0,2]
\end{cases} $$ Or $$ \begin{cases}
u\left(0,y\right)-w\left(0,y\right)=0, & \forall y\in[0,2]\\
u\left(2,y\right)-w\left(2,y\right)=0 & \forall y\in[0,2]
\end{cases} $$ So I defined $ w\left(x,y\right)=\sin\left(\pi x+\pi xy\right) $ , and then I defined $ v\left(x,y\right)=u\left(x,y\right)-w\left(x,y\right) $ So that $$  \begin{cases}
\varDelta v\left(x,y\right)=\left(\left(\pi+\pi y\right)^{2}+\left(\pi x\right)^{2}\right)\sin\left(\pi x+\pi xy\right) & \left(x,y\right)\in\left(0,2\right)\times\left(0,2\right)\\
v\left(x,0\right)=0 & \forall x\in[0,2]\\
v\left(x,2\right)=0 & \forall x\in[0,2]\\
v\left(0,y\right)=0 & \forall y\in[0,2]\\
v\left(2,y\right)=\min\left(y,\left(2-y\right)\right)-\sin\left(\pi x+\pi xy\right) & \forall y\in[0,2]
\end{cases}$$ I do know how to solve this equation, but it requires too much work and Im sure there is a simpler solution. I'll show my way to solve this equatoin: I'll use the Fourier series by the orthogonal system $ \left\{ \sin\left(\frac{\pi n}{2}y\right)\right\} _{n=1}^{\infty} $ (which consists of eigenfunctions of the Laplace eigenvalue equation in the $ y $ direction in the homogenous case) And I'll write each function as its generlized Fourier series: $ \begin{cases}
v\left(x,y\right)=\sum_{n=1}^{\infty}Y_{n}\left(x\right)\sin\left(\frac{\pi n}{2}y\right)\\
\frac{\partial^{2}v}{\partial x^{2}}\left(x,y\right)=\sum_{n=1}^{\infty}X_{n}\left(x\right)\sin\left(\frac{\pi n}{2}y\right)\\
\frac{\partial^{2}v}{\partial y^{2}}\left(x,y\right)=\sum_{n=1}^{\infty}Z_{n}\left(x\right)\sin\left(\frac{\pi n}{2}y\right)\\
\left(\left(\pi+\pi y\right)^{2}+\left(\pi x\right)^{2}\right)\sin\left(\pi x+\pi xy\right)=\sum_{n=1}^{\infty}A_{n}\sin\left(\frac{\pi n}{2}y\right)\\
\min\left(y,\left(2-y\right)\right)-\sin\left(\pi+\pi xy\right)=\sum_{n=1}^{\infty}C_{n}\sin\left(\frac{\pi n}{2}y\right)
\end{cases} $ One can check that the following holds: $ \begin{cases}
X_{n}=\frac{d^{2}}{dx^{2}}Y_{n}\left(x\right)\\
Z_{n}=-\frac{\pi^{2}n^{2}}{4}Y_{n}\left(x\right)\\
\\
\end{cases} $ So we get $$ \begin{cases}
v\left(x,y\right)=\sum_{n=1}^{\infty}Y_{n}\left(x\right)\sin\left(\frac{\pi n}{2}y\right)\\
\frac{\partial^{2}v}{\partial x^{2}}\left(x,y\right)=\sum_{n=1}^{\infty}\frac{d^{2}}{dx^{2}}Y_{n}\left(x\right)\sin\left(\frac{\pi n}{2}y\right)\\
\frac{\partial^{2}v}{\partial y^{2}}\left(x,y\right)=-\sum_{n=1}^{\infty}\frac{\pi^{2}n^{2}}{4}Y_{n}\left(x\right)\sin\left(\frac{\pi n}{2}y\right)\\
\left(\left(\pi+\pi y\right)^{2}+\left(\pi x\right)^{2}\right)\sin\left(\pi x+\pi xy\right)=\sum_{n=1}^{\infty}A_{n}\sin\left(\frac{\pi n}{2}y\right)\\
\min\left(y,\left(2-y\right)\right)-\sin\left(\pi+\pi xy\right)=\sum_{n=1}^{\infty}C_{n}\sin\left(\frac{\pi n}{2}y\right)
\end{cases} $$ And we now how to calculate $ A_n $ and $ C_n $ because its gonna be the inner product in $L^2 $ between $\sin\left(\frac{\pi n}{2}y\right) $ and the compatible function. So if we'll insert each series into Laplace equation, and use the uniquness of the coefficients, we'll get an ordinary differential equation $ \frac{d^{2}}{dx^{2}}Y_{n}\left(x\right)-\frac{\pi^{2}n^{2}}{4}Y_{n}\left(x\right)=A_{n} $ And we have intial condition because $$ 0=v\left(0,y\right)=\sum_{n=1}^{\infty}Y_{n}\left(0\right)\sin\left(\frac{\pi n}{2}y\right)\to Y_{n}\left(0\right)=0 $$ And $ v\left(2,y\right)=\sum_{n=1}^{\infty}Y_{n}\left(2\right)\sin\left(\frac{\pi n}{2}y\right)=\sum_{n=1}^{\infty}A_{n}\sin\left(\frac{\pi n}{2}y\right)\to Y_{n}\left(2\right)=A_{n} $ So if I'll work hard enough and calculate all those integrals, I will get a solution for $Y_n $ and therefore also for $ v(x,y) $ and then for $ u(x,y) $ . But Im looking for much simple way. Thanks in advance. UPDATE: I tried to define $ w\left(x,y\right)=\frac{\left(2-y\right)}{2}\sin\left(\pi x\right)+\frac{y}{2}\sin\left(3\pi x\right) $ And solve as I explained, but it also lead me to an endless algebra calculations.","['ordinary-differential-equations', 'partial-differential-equations']"
4171963,Question on convergence of difference between two real sequences.,Let $\{x_n\}_{n \geq 1}$ and $\{y_n\}_{n \geq 1}$ be two sequences of real numbers such that $x_n^3 - y_n^3 \to 0.$ Can we always conclude that $x_n - y_n \to 0\ $ ? I think it is true. But I can't able to prove it. I tried by factorizing $$x_n^3 - y_n^3 = (x_n - y_n) (x_n^2 + x_n y_n + y_n^2).$$ Now how do I proceed? Any help will be appreciated. Thanks!,"['limits', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
4171972,"Calculation of $f(\frac{3}{5})$, given that $f:D(0, 2)\to\,\mathbb{C}$ is analytic so that $f(\frac{2}{n})=\frac{1}{n+1}, n = 2, 3,...$","I have the following exercise: Let $f:D(0, 2)\to\,\mathbb{C}$ be an analytic function so that $f(\frac{2}{n})=\frac{1}{n+1}, n = 2, 3,...$ Calculate $f(\frac{3}{5})$ . And this is my approach. I replaced $\frac{2}{n}$ with $m$ . So the expression above transforms to $f(m)=\frac{m}{m+2}$ . If I consider $f(z)=\frac{z}{z+2}$ , this function is analytic in the domain and satisfies the condition above. So if I use this function, I calculate $f(\frac{3}{5})=\frac{3}{13}$ . My question is, why would this $f$ be the only one that satisfies those criteria, so that I can use it to calculate $f(\frac{3}{5})$ ? Thank you in advance!",['complex-analysis']
4171984,Geometry problem about angles in triangles,"In the triangle $ABC$ , the point $D$ in $BC$ is so that $\overline{AC}=\overline{BD}$ . Let $\angle ABD = \theta$ , $\angle ACD = 4\theta$ and $\angle CAD = 90^\circ-\theta$ . How can I calculate $\theta$ ? I tried using Stewart's theorem on $\triangle ABC$ and the law of cosines on some angles. This resulted in a system of 5 equations, which I wasn't able to solve for $\theta$ . Wolfram|Alpha says the solution is $\theta=20^\circ$ . Is there a simpler way to solve this problem?","['triangles', 'angle', 'geometry']"
4172006,What is $\tan x\cdot \tan y$ when given $\sin(x+y)=2/3$ and $\sin(x-y)=-1/3$?,"I tried solving it by using the formulas for $\sin(x+y)$ and $\sin(x-y)$ but I got stuck with $\tan y=6\tan x$ .
I then tried another approach, using $\sin^2x+\cos^2x=1$ and found the values of $\cos(x+y)$ and $\cos(x-y)$ . Then I wrote $\tan(x+y)$ and $\tan(x-y)$ with the extended formulas $(\tan x+\tan y)/(1-\tan x\tan y)$ and the one with $-$ , and matched them with the formula $\tan(x+y)=\sin(x+y)/\cos(x+y)$ and the one with $-$ respectively, because above I calculated the values for the terms of the fraction. However, I only got $\tan x\tan y$ equalling a fraction containing $\tan x$ and $\tan y$ , so got nowhere in the end. Any ideas? Sorry for the bulk of text, it’s my first post and thanks in advance!",['trigonometry']
4172014,Why discrete set must be countable?,"I want to show a set, which every point of it is an isolated point. Then this set must be countable. How to show it? I find this from Wikipedia's article Isolated point , but I don't understand: A set that is made up only of isolated points is called a discrete set (see also discrete space). Any discrete subset S of Euclidean space must be countable, since the isolation of each of its points together with the fact that rationals are dense in the reals means that the points of S may be mapped into a set of points with rational coordinates, of which there are only countably many.","['analysis', 'real-analysis']"
4172028,"If every $k$-vertex induced subgraph of $G$ has $m$ edges, then $G$ is either complete or empty.","Let $e(G)$ be the number of edges in $G$ . I've been trying the following exercise from Doug West's Graph Theory, and would like an answer for part (b) : 13.26) Suppose $n, k \in \mathbb{Z}$ satisfy $1 < k < n-1$ and $n\geq 4$ . Suppose $G$ is a simple $n$ -vertex graph and that every $k$ -vertex induced subgraph of $G$ has $m$ edges. a) Suppose $G'$ is an induced subgraph of $G$ with $j$ vertices, where $j>k$ . Prove that $$e(G') = m\cdot \binom{j}{k}\cdot \frac{1}{\binom{j-2}{k-2}} $$ b) Use (a) to prove that $G=K_n$ or $G=\overline{K_n}$ . Hint: use (a) to compute the entry in the adjacency matrix for the vertex pair $uv$ ; the formula is independent of the choice of $u$ and $v$ . Part (a) is pretty simple. Count the number of pairs $(e, H)$ where $H$ is an induced $k$ -vertex subgraph of $G$ and $e$ is an edge of $G$ . Then divide by the number of ways to make $H$ for a given edge $e$ . But part (b) I have no idea. I'd prefer an answer that makes use of the hint.","['graph-theory', 'combinatorics', 'discrete-mathematics']"
4172116,Probability logic that I don't understand,Here is the full question: $5$ balls are drawn in succession without replacement from an urn containing $5$ red balls and $6$ blue balls. How many possible outcomes are there? I've been pondering whether or not I should solve it this way. Let $n$ be the number of possible outcomes. Then: $$n = 2\cdot 2\cdot 2\cdot 2\cdot 2\cdot 2$$ but isn't there one outcome that would use up all the red balls before drawing for the sixth sample space? Wouldn't it make it like this? $$n = 2\cdot 2\cdot 2\cdot 2\cdot 2\cdot 1$$ (my reasoning: the sixth sample space would only be equal to one since all the red balls have been drawn and only one color is left) I'm terribly confused on how I should tackle this. Will it affect the final answer? I would be grateful if someone could give me the correct answer and an explanation.,['combinatorics']
4172173,Differential Equation - I'm missing something stupid,"I have the follwoing ODE to solve: $\dfrac{dy}{dx}+3y=28e^{2x}y^{-3}$ This is a Bernoulli equation so I make the substitution $ u=y^{1-n} \; \;u=y^4 \; \;y=u^{1/4}$ . $$
\begin{split}
\frac{du^{{1/4}}}{dx} + 3u^{{1/4}} &= 28e^{2x}u^{-3/4} \\
u^{{-3/4}}\frac{du}{dx}+3u^{1/4} &= 28e^{2x}u^{-3/4} \\
\frac{du}{dx} + 3u &=28e^{2x}
\end{split}
$$ So we have a linear differential equation $\dfrac{dy}{dx}+p(x)y=f(x)$ with $p(x)=3$ . Multiplying through by ${e}^{\int{{3}dx}}\;\;$ $$
\begin{split}
e^{3x}\frac{du}{dx}+3ue^{3x} &= 28e^{5x}
 \quad \text{(expanded form from the product rule)} \\
\frac{d\left[u e^{3x}\right]}{dx} &= 28e^{5x} \\
{u}{e^{3x}} &= \int{28e^{5x}dx} \\
u &=\frac{28}{5}e^{2x} + \frac{c}{e^{3x}} \\
y &= \left(\frac{28}{5}e^{2x}+\frac{c}{e^{3x}}\right)^{1/4}
\end{split}
$$ However, the answer I am given is $$
y = \left(8e^{2x}+\frac{c}{e^{12x}}\right)^{1/4}
$$ I've tried this about 10 times now and can't see where I am making a mistake. Note:  I'm an amateur doing self study so there's nobody I can ask.  I'm sure there is an obvious error but I can't find it.","['substitution', 'solution-verification', 'ordinary-differential-equations']"
4172252,Can you integrate a function of two variables with respect to one variable?,"Similar to how we take the partial derivative of a function $f(x,y)$ by holding one of the variables constant, why can we not take the integral of a function $f(x,y)$ by holding one variable constant? For example why don't these equal $\int xy \, dx = \frac{x^2}{2}y$ , assuming $y$ is a variable?","['multivariable-calculus', 'calculus']"
4172256,Finding a $3 \times 3$ matrix that only has 1 non-trivial invariant subspace of $\mathbb{R}^3$.,"Does such a matrix exist, and if so how can I find it? My intuition was the matrix $$M = \begin{bmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{bmatrix}$$ as the only eigenspace is $\text{span} \{\mathbf{e}_1\}$ , but I'd also need to prove that there's no other 2-dimensional invariant subspace. If we let $\mathbf{u}$ and $\mathbf{w}$ be arbitrary linearly independent nonzero vectors in $\mathbb{R}$ , we want $M\mathbf{u} = \alpha \mathbf{u} + \beta \mathbf{w}$ and $M\mathbf{w} = \gamma \mathbf{u} + \delta \mathbf{w}$ . We then get the system of equations $$\begin{align*} u_1 + u_2 &= \alpha u_1 + \beta w_1 \\ u_2 + u_3 &= \alpha u_2 + \beta w_3 \\ u_3 &= \alpha u_3 + \beta w_3 \\ w_1 + w_2 &= \gamma u_1 + \delta w_1 \\ w_2 + w_3 &= \gamma u_2 + \delta w_3 \\ w_3 &= \gamma u_3 + \delta w_3 \end{align*}$$ I'm sure we could solve this by turning it into a matrix and RREF and finding values of the constants to make it nonzero, but I don't feel like this is the way. With some research I found the theorem: Let $T: V \to V$ be a linear operator on a finite dimensional vector space. If $T$ is diagonalizable, and $W$ is a $T$ -invariant subspace of $W$ , then the restriction of $T$ to $W$ , $T_W$ , is also diagonalizable. Does this mean, because the only eigenspace is the above and is one-dimensional, any 2-dimensional subspace cannot be invariant because the eigenspaces of $T_W$ must be eigenspaces of $T$ ? Also, does this imply that any $n \times n$ Jordan matrix only has a single non-trivial invariant subspace? And does this all generalise to $\mathbb{C}$ as well? Thanks for any help!","['matrices', 'invariant-subspace', 'linear-algebra']"
4172376,What does this property of the basis try to generalize?,Given a topological space $X$ and $x  \subset B_1 \cap B_2$ where $B_1$ and $B_2$ are basis elements then there exists a basis $B_3$ such that $ x \in B_3 \subset B_1 \cap B_2$ .Is it trying to generalize the connectedness property of a topological space ?,"['general-topology', 'analysis']"
4172450,Theorem about series of functions,"If both series $\sum_{n=1}^{\infty} f_{\mathrm{n}}(x)$ and $\sum_{n=1}^{\infty} f_{\mathrm{n}}^{\prime}(x)$ converge uniformly on I and $f_{n}^{\prime}$ is continuous for every $n \in \mathbb{N}$ , then the function $f(x) = \sum_{n=1}^{\infty}f_{n}(x)$ is differantiable on I and we have $f^{\prime}(x)=\sum_{n=1}^{\infty}f_{n}^{\prime}(x)$ My Work: We want to prove that $f^{\prime}(x) =\displaystyle \sum_{n=1}^{\infty}f_{n}^{\prime}(x)$ in other words $\displaystyle\lim_{n \to \infty} f_{n}(x) = \displaystyle\sum_{n=1}^{\infty}f_{n}(x)$ we know that $\displaystyle\sum_{n=1}^{\infty}f_{n}(x)$ converge uniformly. Then $\displaystyle\lim_{n \to \infty}f_{n}(x)=f(x)$ exist. Also we can say that from the information given in question, $f_{n}$ is differantiable because $f_{n}^{\prime}$ is continuos. And from above we can say that $\displaystyle\lim_{n \to \infty}f_{n}^{\prime}=f^{\prime}$ since $f_{n}^{\prime}$ is continuous. Therefore, that is, $f^{\prime}=\displaystyle\sum_{n=1}^{\infty}f_{n}^{\prime}$ . Does it sufficient to prove? I am not sure...","['sequence-of-function', 'solution-verification', 'derivatives', 'real-analysis']"
4172514,Comparing mean squared errors for estimators,"Let random sample $(X_{1},...X_{n})$ is taken from a population with mean $\mu $ and variance $\sigma ^{2}$ . Compare suggesting estimators for $\mu $ according to mean squared error. Suggesting estimators are , $T_{1}=\frac{3X_{1}-X_{2}-X_{3}+4X_{n}}{5}$ and $T_{2}=\frac{X_{1}}{2}-\frac{X_{2}}{3}+\frac{X_{n}}{6}$ Here is my solution : $E(X)=\mu $ and $var(X)=\sigma ^{2}$ $E(T_{1})=E(\frac{3X_{1}-X_{2}-X_{3}+4X_{n}}{5})=\frac{5\mu }{5}=\mu $ and $var(T_{1})=var(\frac{3X_{1}-X_{2}-X_{3}+4X_{n}}{5})=\frac{27}{25}\sigma ^{2}$ $MSE(T_{1};\mu )=Var(T_{1})+(E(T_{1})-\mu ))^{2}=\frac{27}{25}\sigma ^{2}$ $E(T_{2})=E(\frac{X_{1}}{2}-\frac{X_{2}}{3}+\frac{X_{n}}{6})=\frac{1}{3}\mu $ and $var(T_{2})=var(\frac{X_{1}}{2}-\frac{X_{2}}{3}+\frac{X_{n}}{6})=\frac{7}{18}\sigma ^{2}$ $MSE(T_{2};\mu )=Var(T_{2})+(E(T_{2})-\mu ))^{2}=\frac{7}{18}\sigma ^{2}+\frac{4}{9}\mu ^{2}$ But how can I compare these two mean squared error. I do not know the value of $\mu ^{2}$ $MSE(T_{1};\mu )<MSE(T_{2};\mu )$ or $MSE(T_{2};\mu )<MSE(T_{1};\mu )$ ?","['statistics', 'variance', 'mean-square-error', 'means', 'probability']"
4172577,A particular solution for $y''+a_1y' + a_2y = Ae^{i\omega x}$,"I'm trying to show that for the equation $y''+a_1y' + a_2y = Ae^{i\omega x}$ , there is a solution of $\phi(x)$ of the form $$\phi(x)=\frac{A}{\vert p(i\omega)\vert}e^{i(\omega x - \alpha)}$$ where $ p(i\omega)=\vert p(i\omega)\vert e^{ix}$ , and $p(i\omega) \neq 0$ . The last statement is confusing and I'm unsure how to interpret it. Why would $p(i\omega)=\vert p(i\omega)\vert e^{ix}$ ? I think that $$p(i \omega)=\omega^2+a_1i\omega+a_2$$ which is a complex number with $Re = \omega ^2+a_2$ , and $Im = a_1\omega$ . It does not seem like we will ever end up to such an identity, with the standard definition of complex norm. To be precise, we have that $$\vert p(i\omega)\vert=\sqrt{(\omega^2 +a_2)^2 + (a_1\omega)^2}$$ As for validating the solution, one would compute $L(\frac{A}{\vert p(i\omega)\vert}e^{i(\omega x - \alpha)})$ , take the term with the norm as a common factor. I imagine the norm is cancelled at some point. What am I missing here?","['complex-numbers', 'ordinary-differential-equations']"
4172584,Characterization of the subspaces of $\mathbb R^{m\times n}$ induced by rank-1 matrices?,"Consider a linear subspace $S$ of the space of $m\times n$ real valued matrices $\mathbb R^{m\times n}$ . When does $S$ admit a basis consisting only of rank-1 matrices? I.e. is there a simple criterion or algorithm to check for the existence of such a basis? Background: I would also be interested in the case when we also allow diagonal, permutation, or generally sparse matrices. This is because the matrix vector product $A\cdot x$ , which usually costs $O(n^2)$ to evaluate, can be computed in $O(n)$ for the aforementioned matrix types. Thus, if we have a subspace $S\le \mathbb R^{m\times n}$ with $\dim(S)\ll m\times n$ we can potentially achieve huge performance gains by representing parametrized  matrices in $S$ using a basis which only consists of these ""cheap"" matrices. Formulation as an optimization problem: Given a finite set of $m\times n$ real-valued matrices $\mathcal A = \{A_1,\ldots, A_d\}\subset \mathbb R^{m\times n}$ , find $$ \min_{\mathcal B = \{B_1, \ldots, B_d\}\subset \mathbb R^{m\times n}}  \sum_{i=1}^d \text{rank}(B_i) \qquad\text{s.t.}\quad\big\langle \mathcal A\big\rangle = \big\langle \mathcal B\big\rangle $$ Is there any hope of solving this problem exactly? If not, what types of optimization algorithms are suitable for this kind of problem, given that both the objective function and the constraint are discontinuous? Example: given $$S= \left\langle
\underset{A_1}{\begin{bmatrix}4 & 5 & 6 \\ 7 & 9 & 11 \end{bmatrix}},
\underset{A_2}{\begin{bmatrix}2 & 3 & 4 \\ 5 & 7 & 9 \end{bmatrix}}
\right\rangle
$$ We can represent $S$ equivalently as $$S= \left\langle 
\underset{B_1}{\begin{bmatrix}3 & 4 & 5 \\ 6 & 8 & 10 \end{bmatrix}},
\underset{B_2}{\begin{bmatrix}1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}}
\right\rangle
= \left\langle
\underset{u_1v_1^T}{\begin{bmatrix} 1 \\ 2 \end{bmatrix}\begin{bmatrix}3 & 4 & 5\end{bmatrix}},
\underset{u_2v_2^T}{\begin{bmatrix} 1 \\ 1 \end{bmatrix}\begin{bmatrix} 1 & 1 & 1\end{bmatrix}}\right\rangle 
$$ Because $A_1=u_1v_1^T + u_2v_2^T$ and $A_2 = u_1v_1^T - u_2v_2^T$ . Now consider a parametrized matrix $C$ in $S$ . If we express $C$ in the basis $A$ , i.e. $C = \lambda_1 A_1 + \lambda_2 A_2$ , then computing the matrix vector product $C\cdot x$ will cost us at least 16/10 multiplications/additions, whereas if we represent it in the basis $B$ , i.e. $C=\lambda_1 u_1v_1^T + \lambda_2 u_2v_2^T$ , then the matrix-vector product $C\cdot x$ can be done in 12/8 multiplications/additions. More generally, for a space of dimension $d$ the scaling is $O(dmn)$ for a dense basis and $O(d(m+n))$ for a rank-1 basis in terms of both elementary multiplications and additions. (using elementary matrix multiplication)","['matrices', 'optimization', 'matrix-rank', 'linear-algebra']"
4172607,Are pointwise finitely divisible subgroups of $\mathbb Q^n$ finitely generated?,"I have a question regarding additive subgroups of $\mathbb Q^n$ . Suppose we have such a guy $M$ , and suppose we know it is pointwise finitely divisible. That is, for every $m \in M$ , there are only finitely many integers $n$ such that $m/n \in M$ . Does it follow that $M$ is finitely generated? Some thoughts: It is well-known that a subgroup of $\mathbb Q^n$ is finitely generated if and only if it is discrete, i.e. the denominators are ""globally bounded"", whereas the given property only says each element has bounded denominators. Thus it seems weaker, but I haven't been able to find an example of an $M$ with this property and which is not finitely generated. If $n = 1$ , the statement is true: we may assume that $1 \in M$ , and one can then show that $1/N = \text{min}\{m \in M \ | \ nm = 1 \text{ for some }n \geq 1\}$ generates $M$ . By the above point, an equivalent way to state the question is this: suppose $M \subset \mathbb Q^n$ is a subgroup whose intersection with any line through the origin is finitely generated. Does it follow that $M$ is finitely generated?","['finitely-generated', 'abelian-groups', 'abstract-algebra', 'integer-lattices']"
4172669,"book recommendation for functions, function spaces, properties, approximation, embeddings, densensess etc.","I am trying to learn more about function spaces, properties and structure of the functions in each space but also properties and structures of the function spaces themselves , with different metrics maybe, embeddings and dense embeddings (with the classical or even different metrics for each space), and approximation arguments for each space. This would include, spaces of functions on manifolds,on $\mathbb{R}$ , on $\mathbb{R}$ , on $\mathbb{R}^n$ , on $\mathbb{C}^n$ , on subsets of manifolds or $\mathbb{R}^n$ or $\mathbb{C}^n$ , even more general metric or topological sets. Spaces of differential functions, of continuous functions (even almost everywhere differential or alamost everywhere continuous etc.), integrable functions, covex functions, real analytic functions, holomorphic functions, Sobolev spaces etc. It would be really nice, if your recommendation are from the more classical to the more general case. For example I think it would be more constructive for me if I'd start with some more classical spaces, for example spaces on $\mathbb{R}^n$ and then go to manifolds. But my reasoning for that could be of course false. Furthermore, it could be books, lecture notes or whatever youy may think would be helpful. Lastly, the more theorems, propositions, properties etc. the better, but I would also like to see many examples, useful methods and tricks, intuitional approaches and explanations and many exercises, if possible. Of course not only the ""trivial"" ones . I have basic (to maybe somewhat advanced knowledge for some of them) knowledge of one-dimensional analysis, multivariable analysis, cmplex analysis, measure theory, functional analysis, Fourier analysis, Banach algebras, operator theory, spectral theory, Sobolev spaces etc. Some books I have already in mind are for example, Differential Topology, Hirsch, Theory of Function Spaces, Triebel, Measure Theory and Fine Properties of Functions, Evans & Gariepy It is probably much to ask and maybe very vague what I am looking for but any help and recommendations are welcome. Thank you!","['book-recommendation', 'real-analysis', 'complex-analysis', 'functions', 'functional-analysis']"
4172684,How to solve system of set-theoretic equations?,"Consider the next setup: $S = \{a,b,c,d,e\}, A_1,A_2,B_1,B_2,X_1,X_2 ⊂ S$ $(A_1\setminus X_1) \cup X_2 = B_1$ $(A_2\setminus X_1) \cup X_2 = B_2$ \ is relative complement. $\{a,b\} \setminus \{a\} = \{b\} $ How to find all possible solutions for $X_1, X_2$ ? What is the right mathematical name for such equations? Example: $(\{a,b,c\}\setminus X_1) \cup X_2 = \{b,c\}$ $(\{a,b,c\} \setminus \{a\}) \cup \{c\} = \{b,c\}$ $(\{a,d,e\} \setminus \{a\}) \cup \{c\} = \{d,e,c\}$ My idea was to use equation 1 to express $X_2$ in terms $X_1$ and substitute $X_2$ in equation 2. But I am stuck with what to do next. $X_2 = B_1 \setminus (A_1 \setminus X_1)$ $X_1 = A_2 \setminus (B_2 \setminus (B_1 \setminus (A_1 \setminus X_1)))$","['elementary-set-theory', 'boolean-algebra']"
4172734,Orthogonality of generalized eigenbases of self-adjoint operators,"As mentioned in another of my recent questions , given some operator on a Hilbert space, we can construct a Gelfand triple in order to ""recover"" eigenfunctions corresponding to elements of the spectrum which do not strictly have corresponding eigenfunctions in the Hilbert space ""proper."" For example, the momentum operator $i\frac{d}{dx}$ has no ""proper"" eigenfunctions in $L_2(\mathbb{R})$ , but it does have sinusoidal eigenfunctions in the Gelfand triple $H^s(\mathbb{R}) \subseteq L_2(\mathbb{R}) \subseteq H^{-s}(\mathbb{R})$ . In the finite-dimensional case, we have a theorem that every Hermitian operator has an orthogonal eigenbasis. Does this generalize to the infinite-dimensional case of self-adjoint operators? Immediately, we encounter a problem: the sinusoids $t \to e^{ikt}$ are not ""proper"" elements of $L_2(\mathbb{R})$ (they fail to be square-integrable).  So, we cannot say precisely that these eigenfunctions are orthogonal with respect to the inner product on the Hilbert space.  Can we state something weaker? Perhaps we can recover a notion of ""limiting orthogonality"" by approximating our eigenfunctions with sequences of functions that are elements of $L_2(\mathbb{R})$ (e.g. approximating a sinusoid by a sequence of sinc functions with slower and slower decay)?  Maybe there's some other approach entirely?  Or is this a doomed endeavor, and there is no useful generalization to be found?","['hilbert-spaces', 'spectral-theory', 'functional-analysis']"
4172756,Sectional curvature of Lie group,"Good time of day. I try to prove that the sectional curvature for Lie group with bi-invariant metric is equal $K(\sigma)= \frac 14 |[X,Y]|^2$ My attempt is the following I have proved that $\nabla_X Y = \frac 12 [X,Y]$ I know that $K_{XY}(p)=\frac{R(X,Y,X,Y)}{||X||^2 ||Y||^2 - \langle X,Y\rangle^2}=R(X,Y,X,Y)=\langle R(X,Y)X,Y\rangle $ If $X,Y$ are orthonormal fields $\langle R(X,Y)X,Y\rangle =\langle (\nabla_Y \nabla_X X - \nabla_X \nabla_Y X + \nabla_{[X,Y]} X),Y\rangle $ first term is equal $0$ since $\nabla _X X=0$ second term $-\langle\nabla_X \nabla_Y X,Y\rangle=-X\langle\nabla_Y X,Y\rangle+\langle\nabla_Y X,\nabla_X Y\rangle$ $\langle\nabla_Y X,Y\rangle=Y\langle X,Y\rangle-\langle X,\nabla_Y Y\rangle=0$ And $-\langle \nabla_X \nabla_Y X,Y\rangle =\langle \nabla_Y X,\nabla_X Y\rangle=\langle\frac12[Y,X],\frac12[X,Y]\rangle=-\frac 14 |[X,Y]|^2$ third term is equal to $0$ since $\langle\nabla_{[X,Y]} X,Y\rangle=\frac12[X,Y]\langle X,Y\rangle=0$ And $K(\sigma)= -\frac 14 |[X,Y]|^2$ Please help me. I don't know where I have made mistake. Thank you","['lie-algebras', 'lie-groups', 'riemannian-geometry', 'differential-geometry']"
4172799,"Product of characteristic functions, counter example","We know that for independent random variables $X,Y$ we have: $\varphi_{X+Y}(t)=\varphi_X(t)\cdot \varphi_Y(t)$ . Now I'm searching for an example that shows that the reversal of the statement is not true, so I want $X$ and $Y$ which fulfill $\varphi_{X+Y}(t)=\varphi_X(t)\cdot \varphi_Y(t)$ but are dependent. I can't think of an example that shows this.","['characteristic-functions', 'statistics', 'probability-theory']"
4172835,Cubic Bézier spline multiple split,"I have a cubic Bézier spline with four points: P1, C1, C2 and P2. I'm using De Casteljau method to split the spline to two parts at a given point x, 0 <= x <= 1. The question is, can I use the same algorithm to split the spline twice? Say, that I want to split spline in points x1 = 0.25 and x2 = 0.75. First I split the spline S to S1 and S2 at the point 0.25. Next, I re-evaluate x2, since now it's on the segment [0.25, 1]. If I then subtract 0.25 from everything, I get 0.5 on [0, 0.75], which, scaled up to [0, 1] gives me x2'=0.(6). Then I split S2 on point x2 to get another two splines and the center one is the one I'm seeking. Is the re-calculation of split point on the second spline valid? I mean, will 0.(6) on the second spline will match the original 0.75 on the initial one?","['bezier-curve', 'spline', 'geometry']"
4172845,"How can I know if a pile in ""Grundy's game"" have a winning strategy?","Assume that I'm playing Grundy's game and I have pile of $n$ coins.
I want to know if I have a winning strategy (no matter what is it). How can I know if there is? I understand the transform to Nim heap, but I don't understand how it can tell me if there is a winning strategy or not: For example: Wikipedia says that pile of 18 equals to Nim heap at size 4, but if I have one pile at Nim, no matter what the size is - I can win at the first move. So my question is: for a pile of size $n$ at Grundy's game how can I know if there a winning strategy or not? (for any size of pile) and if someone can explain me the idea of transform it to Nim heap it will be great (as I said - I don't understand the transform idea, because pile of any size at Nim have a winning strategy). Thank you!","['discrete-mathematics', 'combinatorial-game-theory']"
4172859,Number of sequences length n in which neighboring elements differ by 1 [duplicate],"This question already has answers here : In how many sequences of length $n$, the difference between every 2 adjacent elements is $1$ or $-1$? (3 answers) Closed 3 years ago . Suppose we're given a set $S=\{1,2,3,4,5\}$ .
From elements in $S$ , we form an array/sequence of length $n$ , such that difference between neighboring elements is strictly $1$ . For example, one valid sequence of length $n = 5$ is $32345$ . What is the generating function for the number of such sequences? My approach is to let $f(n)$ be a number of such sequences starting with 1 or 5 (only one neighboring number). Then, $f(n) = 2 * n-1$ .
Similiarly, let $g(n)$ be a number of sequences staring with $2,3,4$ .
Unfortunately, I can't find a proper formula for $g(n)$ , although I'm quite convinced it's in the form of $a\times g(n-1) + b\times f(n-1)$ .","['generating-functions', 'combinatorics', 'sequences-and-series']"
4172891,"Problem $3.18$, Rudin's RCA (Convergence in Measure)","Let $\mu$ be a positive measure on $X$ . A sequence $\{f_n\}$ of complex measurable functions on $X$ is said to converge in measure to the measurable function $f$ if to every $\epsilon > 0$ there corresponds an $N$ such that $$\mu(\{x: |f_n(x) - f(x)| > \epsilon) < \epsilon$$ for all $n > N$ . Assume $\mu(X) < \infty$ and prove the following statements: If $f_n(x)\to f(x)$ a.e., then $f_n\to f$ in measure. If $f_n \in L^p(\mu)$ and $\|f_n-f\|_p\to 0$ , then $f_n\to f$ in measure; here $1\le p\le \infty$ . If $f_n\to f$ in measure, then $\{f_n\}$ has a subsequence which converges to $f$ a.e. Investigate the converses of $(a)$ and $(b)$ . What happens to $(a)$ , $(b)$ , and $(c)$ if $\mu(X) = \infty$ , for instance, if $\mu$ is the Lebesgue measure on $\mathbb R$ ? My (incomplete) work: I used Egoroff's theorem to complete the proof. Fix some $\epsilon > 0$ . My approach is similar to the one above: I'm trying to show that $\mu(\{x: |f_n(x) - f(x)| > \epsilon) \xrightarrow{n\to\infty} 0$ . (Completed for $1 \le p < \infty$ with help of the hint in the answer. What about $p = \infty$ ?) I see that this part has some discussion here. (Completed). Follow-up Problem: Are the converses of (a) and (b) true? If not, what are some counterexamples? What happens if $\mu(X) = \infty$ ? Comment : In many places, I've seen convergence in measure defined as: $f_n\to f$ in measure if for every $\epsilon > 0$ , $$\lim_{n\to\infty} \mu(\{x: |f_n(x) - f(x)| > \epsilon) = 0$$ While this implies the definition used by Rudin, I do not see how the two are equivalent (i.e. why does the other implication hold). Thanks!","['measure-theory', 'probability-theory', 'real-analysis']"
4172893,"Proving $f(x)=\frac{1}{x}$ is continuous on the interval $(0,1)$","Prove $f(x)=\frac{1}{x}$ is continuous on the interval $(0,1)$ . My attempt: $$
\left|{\frac{1}{x}-\frac{1}{c}}\right|<\epsilon\\
\left|{\frac{c-x}{xc}}\right|<\epsilon\\
\frac{\left|c-x\right|}{xc}<\epsilon \qquad\text{as $x$ and $c$ are both positive} 
$$ Suppose $|x-c|<\frac{c}{2}$ so that $\frac{c}{2}<x<\frac{3c}{2}$ . Then we could obtain $\frac{1}{x}<\frac{2}{c}$ $$
\frac{1}{x}<\frac{2}{c}\implies \frac{1}{xc}<\frac{2}{c^2}\implies \frac{\left|c-x\right|}{xc}<\frac{2}{c^2}|c-x|<\epsilon.
$$ By observation, setting $|c-x|<\frac{c^2\epsilon}{2}=\delta$ . Is this proof looks right to you? Any comment and help are much appreciated!","['analysis', 'real-analysis', 'continuity', 'solution-verification', 'limits']"
4172905,Is there a way to get trig functions without a calculator?,"In school, we just started learning about trigonometry, and I was wondering: is there a way to find the sine, cosine, tangent, cosecant, secant, and cotangent of a single angle without using a calculator? Sometimes I don't feel right when I can't do things out myself and let a machine do it when I can't. Or, if you could redirect me to a place that explains how to do it, please do so. My dad said there isn't, but I just had to make sure. Thanks.","['computational-mathematics', 'functions', 'algorithms', 'trigonometry', 'calculator']"
4172923,Prime numbers theorem via Mertens’ function,"I have read in many places that $M(x)=o(x)$ (where $M(x):= \sum_{n\leq x}\mu(n)$ and $\mu$ is the Mobius function ) implies the prime numbers theorem . However, I am yet to find one readable proof of this statement. Can anyone please explain how these two properties are related? (Supposedly elementarily).","['number-theory', 'elementary-number-theory']"
4172927,Show that $(a^2-b^2)(a^2-c^2)(b^2-c^2)$ is divisible by $12$,"Let $a,b,c\in\Bbb N$ such that $a>b>c$ . Then $K:=(a^2-b^2)(a^2-c^2)(b^2-c^2)$ is divisible by $12$ . My attempt : Since each $a,b,c$ are either even or odd, WLOG we may assume $a,b$ are both even or odd. For both cases, $a+b$ and $a-b$ are divisible by $2$ so $K$ is divisible by $4$ . Note that any $n\in\Bbb N$ is one of $\overline{0},\overline{1},\overline{2}$ in $\operatorname{mod}3$ . Well from this, I can argue anyway but I want to show $K$ is divisible by $3$ more easier or nicer way. Could you help?",['number-theory']
4172957,Defining the sheaf of the maximal spectrum $\mathcal{O}_{Spm(R)}$,"Let $R$ be a finitely generated algebra over an algebraically closed field $k$ and consider $\operatorname{Spm}(R)$ the maximal spectrum of $R$ equipped with the Zariski topology. For an ideal $J$ of $R$ we denote $V(J)$ the closed set of $\operatorname{Spec}(R)$ and $D(J)$ the open set. Similarly we denote $V(J)_m,D(J)_m$ the closed and open sets of $\operatorname{Spm}(R)$ . Question : i) Prove that $V(J)_m=V(J)\cap \operatorname{Spm}(R)$ and $D(J)_m=D(J)\cap \operatorname{Spm}(R)$ . ii) Show that the sheaf of commutative rings $\mathcal{O}_{Spm(R)}$ can be defined as $$\Gamma(D_m(J),\mathcal{O}_{\operatorname{Spm}(R)})=\Gamma(D(J),\mathcal{O}_{\operatorname{Spm}(R)})$$ I can do i) because $V_m(I)=\{\mathfrak{p}\in \operatorname{Spm}(R):\mathfrak{p}\supset I\}=\{\mathfrak{p}\in \operatorname{Spec}(R)\cap \operatorname{Spm}(R):\mathfrak{p}\supset I\}=V(I)\cap \operatorname{Spm}(R)$ and similarly for $D_m(J)$ but I am not sure how to proceed for ii). Why would this equality of ringed spaces define the sheaf $\mathcal{O}_{\operatorname{Spm}(R)}$ ? How would I use the finitely generated property of $R$ ?","['algebraic-geometry', 'sheaf-theory']"
4172974,Fourier coefficients and series for $x\sin(x)$,"Let $$g(x)=x \cdot \sin x,$$ $x\in [-\pi,\pi)$ ( $2\pi$ -period). Find the Fourier coefficients $c_n(g)$ for all $n\in\mathbb{N}$ and write out the Fourier series for $g$ . I normally understood this as $c_n$ would be defined as $$c_n=\left\{\begin{matrix}
{a_0}/{2} & n=0 \\ 
(a_n-ib_n)/2 & n=1,2,...\\ 
(a_{-n}+ib_{-n})/2 & n=-1,-2,... 
\end{matrix}\right.$$ for the Fourier series $$f(t)=\frac{a_0}{2}+\sum_{n=1}^{\infty}\left [ a_n\cos nt +b_n\sin nt \right ]=\sum_{n=-\infty}^{\infty}c_ne^{int}$$ Now I am stuck. I am not sure on how to find these coefficients for all $n\in\mathbb{N}$ and afterwards writing up the series when from the definition of $c_n$ it appears that $c_n$ will vary.","['fourier-analysis', 'analysis', 'real-analysis', 'sequences-and-series', 'fourier-series']"
4172977,Finding all the subgroups of $\mathbb{Z}_2*\mathbb{Z}_2$,"Consider the group that is the free product of two copies of $\mathbb{Z}$ , that is $G=\mathbb{Z}_2 * \mathbb{Z}_2=\left\langle  a,b\;\vert \;a^2=b^2=e \right\rangle$ . I'm trying to find all of its proper and non trivial subgroups. I can find at least one subgroup, ie I think that there is a subgroup of $G$ generated by $ab$ , which is isomorphic to $\mathbb{Z}$ . But that's about all the obvious one I can see straight away and I'm not sure how to go about finding the rest. The reason I'm asking this is because I'm trying to find the all of the connected covering spaces of $\mathbb{R}P^2\vee\mathbb{R}P^2$ . If I'm not mistaken, this space has $G$ as its fundamental group, hence identifying subgroups of $\pi_1(G)$ would allow me to classify the covering spaces (right?). If anyone can provide some insight I'd appreciate that, thank you! EDIT: I forgot to write it down but of course by definition $H*H'$ contains both $H$ and $H'$ as subgroups so here we also have $\mathbb{Z}_2$ as a subgroup. $G$ even has two such subgroups, the one generated by $a$ and the one generated by $b$ (right?).","['group-theory', 'free-product', 'algebraic-topology']"
4173002,If $\cos\alpha+\cos\beta+\cos\gamma=\sin\alpha+\sin\beta+\sin\gamma=0$ and $\cos3\alpha=\dfrac34$ then find $\cos^8\alpha+\cos^8\beta+\cos^8\gamma$,"If $\cos\alpha+\cos\beta+\cos\gamma=\sin\alpha+\sin\beta+\sin\gamma=0$ and $\cos3\alpha=\dfrac34$ then $\cos^8\alpha+\cos^8\beta+\cos^8\gamma=$ A) $\dfrac3{128}$ B) $\dfrac{27}{32}$ C) $\dfrac3{16}$ D) $\dfrac5{128}$ ATTEMPT $1$ : $e^{i\alpha}=\cos\alpha+i\sin\alpha$ , $e^{i\beta}=\cos\beta+i\sin\beta$ , $e^{i\gamma}=\cos\gamma+i\sin\gamma$ Adding, we get $e^{i\alpha}+e^{i\beta}+e^{i\gamma}=0$ Now, if $a+b+c=0\implies a^3+b^3+c^3=3abc$ Therefore, $e^{i3\alpha}+e^{i3\beta}+e^{i3\gamma}=3e^{i(\alpha+\beta+\gamma)}$ $\implies \cos3\alpha+i\sin3\alpha+\cos3\beta+i\sin3\beta+\cos3\gamma+i\sin3\gamma=3\cos(\alpha+\beta+\gamma)+3i\sin(\alpha+\beta+\gamma)$ Sure, $\cos3\alpha$ is given but what about the rest? Also, we need $\cos^8\alpha$ . How to get that? On RHS, can we use the following formula? $\cos(A+B+C)=\cos A\cos B\cos C-\cos A\sin B \sin C-\cos B\sin C\sin A-\cos C\sin A\sin B$ ATTEMPT $2$ : $$\cos3\alpha=\frac34\\4\cos^3\alpha-3\cos\alpha=\frac34\\\cos\alpha(4\cos^2\alpha-3)=\frac34\\4\cos^2\alpha-3=\frac34\sec\alpha$$ Also, $0\le4\cos^2\alpha\le4\implies-3\le4\cos^2\alpha-3\le1\implies-3\le\frac34\sec\alpha\le1\implies-4\le\sec\alpha\le\frac43$ But $\sec\alpha\ge1\implies1\le\sec\alpha\le\frac43$ Does that help? ATTEMPT $3$ : Hit and try. If $\beta=\frac{\pi}2\implies\alpha+\gamma=\pi$ But it doesn't fit sine equation. Have tried with $0,\pi$ etc. Not working.","['trigonometry', 'complex-numbers', 'inequality']"
4173018,Think independent random variables defined on a duplicated product space,"Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space on which defines two random variables $$
X \colon \Omega \to \mathbb{R}\\
Y \colon \Omega \to \mathbb{R}
$$ Now suppose that $X$ and $Y$ are independent meaning that $\mathbb{P}(\omega \in \Omega \colon X \in A, Y \in B) = \mathbb{P}(\omega \in \Omega \colon X \in A)\, \mathbb{P}(\omega \in \Omega \colon Y \in B)$ for every $A,B \in \mathcal{B}$ . Now let us duplicate the space $(\Omega, \mathcal{F}, \mathbb{P})$ yielding $(\Omega', \mathcal{F}', \mathbb{P}')$ such that $\Omega = \Omega'$ , $\mathcal{F}=\mathcal{F}'$ , and $\mathbb{P} = \mathbb{P}'$ . We consider the  product space $(\Omega \times \Omega', \mathcal{F}\otimes \mathcal{F}', \mathbb{P}\,\mathbb{P}')$ . We then extend those random variables to that product space by $$
\overline{X}(\omega,\omega')\colon \Omega \times \Omega' \to \mathbb{R} = X(\omega), \\
\overline{Y}(\omega,\omega')\colon \Omega \times \Omega' \to \mathbb{R} = Y(\omega')
$$ We can verify that $\overline{X}$ and $\overline{X}$ are random variables on that product space, and they are independent. Am I on a correct path to think about the independence of $X$ and $Y$ in this product measure sense? In short, suppose we have two indepdendent random variables on a probaiblity space, can I think them as the sliced random variables on their product measure space?","['measure-theory', 'independence', 'probability-theory', 'probability', 'random-variables']"
4173043,A uniform bound on a family of complex integrals,"In my research, I have encountered the following integrals, which I want to bound. For $0 < t < 1$ , let $C_t$ denote the portion of the complex unit circle consisting of $z$ for which $\Re (z) \ge t$ and define $$
I_t (n,k) := \int_{C_t} \left( 
\frac{1-tz}{|1-tz|} \right)^{1+i+n} \frac{1}{(1-tz)^{1+i}} z^k  dz =  \int_{C_t} \left( 
\frac{1-tz}{|1-tz|} \right)^{n} \cdot z^k \cdot \frac{1}{|1-tz|^{1+i}} dz.
$$ I want to see that there exists $C>0$ such that $|I_t(n,k)| \leq C$ for all $1/2 < t < 1$ and $(n,k) \in \mathbb{Z}_{\ge 1} \times \mathbb{Z}_{\ge 1}$ . Any ideas will be appreciated.","['complex-analysis', 'fourier-series', 'contour-integration', 'complex-integration']"
4173045,If$ \{\tau_{\alpha}\} $be a family of topology on $X$ then the smallest topology containing all $\{\tau_{\alpha}\}$,"If $\{\tau_{\alpha}\}$ be a family of topology on $X$ then the smallest topology containing all $\{\tau_{\alpha}\}$ The basic intuition that I have is the smallest topology will be the topology generated by $\cup_{\alpha}\{B_{\alpha}\}$ where $B_{\alpha}$ is the basis of $\tau_{\alpha}$ . Edit 1:I need to prove that it is the smallest topology. We denote by $\tau$ the topology generated by $\cup \{B_{\alpha}\}$ . We consider $\tau' \subset \tau$ and $\tau'$ containing all the family of topologies . Then, we know that for each $x \in X$ and each element $B' \in \cup\{B'_{\alpha}\}$ there is an element $B \in \cup\{B_{\alpha}\}$ such that $x \in B \subset B'$ .Can we conclude from here that $B'_{\alpha} \subset \cup\{B_{\alpha}\}$ ? Since the family of topologies $\{\tau_{\alpha}\}$ is a subset of $\tau'_{\alpha}$ then we can conclude that $\cup\{B_{\alpha}\}  \subset B'_{\alpha}$ . So $\tau=\tau'$ . Is this proof OK? And if it is OK then is my topology  unique? Is my intuition correct? Also my answer has been without consulting the other answers on the net, it  would be rather helpful if someone points out my mistake as in why this topology will not hold rather than suggesting an answer.","['general-topology', 'solution-verification', 'analysis']"
4173065,"What sorts of (sets of) equations are ""approximately compatible"" with the $2$-sphere?","Given a metric space $\mathcal{X}=(X,d)$ and an equational theory (in the sense of universal algebra) $\mathsf{E}$ , say that an approximate model of $\mathsf{E}$ on $\mathcal{X}$ is a sequence $(\mathcal{M}_i)_{i\in\mathbb{N}}$ of structures in the signature of $\mathsf{E}$ with domain $X$ such that: in each $\mathcal{M}_i$ , each function symbol gets interpreted as a continuous function $X^{arity}\rightarrow X$ in the sense of $d$ ; and for each $i\in\mathbb{N}$ , each equation $t(x_1,...,x_n)=s(x_1,...,x_n)$ in $\mathsf{E}$ , and each $a_1,...,a_n\in X$ , we have $d(t(a_1,...,a_n), s(a_1,...,a_n))<2^{-i}.$ Say that $\mathsf{E}$ is approximately compatible with $\mathcal{X}$ iff $\mathsf{E}$ has an approximate model on $\mathcal{X}$ . Compare this with the notion of (genuine) compatibility discussed here , where there is no ""error"" permitted. In the above-linked paper, a generalization of Adams' theorem that there is no group structure on the $n$ -sphere for $n\not\in\{0,1,3,7\}$ is proved: for $n\not\in\{0,1,3,7\}$ there is no ""interesting"" algebraic structure compatible with the $n$ -sphere at all. On the other hand, approximate compatibility seems like an extremely weak notion, and I don't see any nontrivial examples of equational theories which are not approximately compatible with (for example) the $2$ -sphere: Is there an equational theory $\mathsf{E}$ which has a model with more than one element but is not approximately compatible with $S^2$ ? Note that for equational theories, ""has a model with more than one element"" is equivalent to (for example) ""has a model of size continuum,"" so once we rule out the genuinely trivial theories there is no cardinality obstacle.","['universal-algebra', 'general-topology', 'logic', 'metric-spaces']"
4173126,When does the equation $f(x)=x\cdot\sqrt{2x+6}=ax$ have 2 real solutions?,"My question is the following: Let $f\colon[-3,\infty)\to \mathbf{R}$ with $f(x)=x\sqrt{2x+6}$ .
Is there a value $a\in\mathbf{R}$ for which $f(x)=ax$ has 2 real solutions? I tried the following: With some caculations we find $f'(x)=\frac{3x+6}{\sqrt{2x+6}}$ . Since $(0,0)$ is on the line $y=ax$ , we find $f'(0)=\sqrt{6}$ and therefore a = $\sqrt{6}$ .
By using the schetch of the graph I find the answer $a\in[0,\sqrt{6}$ ). I feel this is kind of cheating, since I used the graph and 'cleverly guessed' the value of $a=\sqrt{6}$ . Is there a more mathematical rigous way to solve this equation? Any help is greatly valued.","['calculus', 'solution-verification', 'derivatives', 'real-analysis']"
4173136,Time derivative of a constant vector in rotating frame,"Recently I'm learning rotating frame kinematics, but I have come up with a question that confuses me a lot. Let's say a vector $P$ connects the origin and a fixed point in the rotating frame, so it is constant in the rotating frame (local frame, $LF$ ), so that $P_{x,LF}$ , $P_{y,LF}$ , and $P_{z,LF}$ never change. The coordinates of $P$ in the inertial frame ( $IF$ ) can be expressed as: $P_{IF} = R_{LF2IF}P_{LF} $ where $R_{LF2IF}$ denotes the rotation matrix from the local frame to the inertial frame. Based on the above equation, the time derivative of $P_{LF}$ is: $\dot{P_{IF}} = \dot{R_{LF2IF}}P_{LF} + R_{LF2IF}\dot{P_{LF}}$ ------------- (1) According to the rotating frame kinematics Rotating Reference Frame , the time derivative of $P_{LF}$ is: $\frac{dP_{LF}}{dt} = (\frac{dP_{LF}}{dt})_{LF} + \omega\times P_{LF}$ Because $P_{LF}$ is constant in the local frame, therefore: $\frac{dP_{LF}}{dt} = 0 + \omega\times P_{LF} = \omega\times P_{LF}$ However I have seen in many places where people consider $\dot{P_{LF}}$ to be zero, which makes me rather confused. It seems that the following condition is true: $\frac{dP_{LF}}{dt} = (\frac{dP_{LF}}{dt})_{LF}$ But, why is this, why the cross product is not calculated in this case? When should the cross product be used and when should not? Many thanks to anyone who would kindly help me to clear my confusion.","['kinematics', 'vectors', 'cross-product', 'derivatives', 'rotations']"
4173137,Riemannian metric with tensor products [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 3 years ago . Improve this question I have been studying differential geometry and I came across two different definitions for the Riemannian metric given by: $$ds^2 = G_{ij} \, dx_i \, dx_j, \tag 1$$ $$ds^2 = G_{ij} \, dx^i \otimes dx^j. \tag 2 $$ Based on the above, I have a few questions: What is the difference between them? What are the advantages of the second definition? What are the properties of the operation $\otimes$ ? Does the fact of $dx^i$ and $dx^j$ being orthogonal imply $dx^i\otimes dx^j$ be zero? Based on your vast experience, what are great introductory books that discuss the second definition in the framework of physics? Thanks in advance.","['differential-forms', 'metric-spaces', 'riemannian-geometry', 'differential-geometry']"
4173180,The quadratic equation whose roots are $\sec^2\theta$ and $\csc^2\theta$ can be,"The quadratic equation whose roots are $\sec^2\theta$ and $\csc^2\theta$ can be A) $x^2-2x+2=0$ B) $x^2-5x+5=0$ C) $x^2-7x+7=0$ D) $x^2-9x+9=0$ Method $1$ : $$\sec^2\theta+\csc^2\theta=\frac1{\cos^2\theta}+\frac1{\sin^2\theta}\\=\frac1{\sin^2\theta\cos^2\theta}\\=\frac4{\sin^22\theta}\ge4$$ Also, $\sec^2\theta\csc^2\theta=\dfrac1{\sin^2\theta\cos^2\theta}$ So, options $B),C),D)$ are correct. Method $2$ : Let the quadratic equation be $x^2-px+q=0$ So, $\sec^2\theta+\csc^2\theta=p, \sec^2\theta\csc^2\theta=q\implies \csc^2\theta=\dfrac{q}{\sec^2\theta}$ Putting that in the sum of roots, we get $$\sec^2\theta+\frac{q}{\sec^2\theta}=p\\\implies\sec^4\theta-p\sec^2\theta+q=0\\\implies\sec^2\theta=\frac{p\pm\sqrt{p^2-4q}}2\ge1\\\implies p\pm\sqrt{p^2-4q}\ge2\\\implies\pm\sqrt{p^2-4q}\ge2-p\\\implies p^2-4q\ge4+p^2-4p\\\implies p-q\ge1$$ What's wrong in this method?","['trigonometry', 'quadratics', 'roots']"
4173207,How to find a bezier curve between two points and two tangents without anchor points,"I was wondering how do you find the best fit bezier curve between two points with known tangents as in the most minimum curve of which the two handle points are not known. Most guides explain how to calculate the curve of something like this: But in my scenario i don't know P1 or P2 i only know P0 and P3 and their tangents that govern the direction of travel like so: Is it possible to calculate a minimum bezier curve with such information, how would you find the curve in this situation ?","['curves', 'bezier-curve', 'geometry']"
