question_id,title,body,tags
2613401,Construct a non trivial homomorphism $\mathbb Z_{14} \to\mathbb Z_{21}$,"Question : Construct a non trivial homomorphism from the group $\mathbb Z_{14}$ to the group $\mathbb Z_{21}$. Discussion : So, to start off, we see that the orders of the given groups for the homomorphism, are : $$|\mathbb Z_{14}|=14$$ $$|\mathbb Z_{21}|=21$$ Then, if $φ : \mathbb Z_{14} \to \mathbb Z_{21}$ is a homomorphism, $|φ[\mathbb Z_{14}]|$ should divide both $|\mathbb Z_{14}|$ and $|\mathbb Z_{21}|=21$. The common divisors of $14$ and $21$ are $1$ and $7$ which means that there exists a non-trivial homomorphism. Would this also mean that $|φ[\mathbb Z_{14}]| = 7$ ? How would one proceed after the initial criteria elaborated, to construct a homomorphism as asked though and give a complete answer to the question ? I can't seem to determine a specific one. I know that $\mathbb Z_{14}$ is the group of integers $\mod 14$ and $\mathbb Z_{21}$ is the group of integers $\mod 21$.","['abstract-algebra', 'group-homomorphism', 'number-theory', 'integers', 'group-theory']"
2613403,Why $\displaystyle\lim_{n\to+\infty}x_n\otimes y_n=x\otimes y\;?$,"Let  $(E,\langle \cdot,\cdot\rangle_1)$, $(F,\langle \cdot,\cdot\rangle_2)$ be two complex Hilbert spaces. We recall
$$E \otimes F:=\left\{\xi=\sum_{i=1}^dv_i\otimes w_i:\;d\in \mathbb{N},\;\;v_i\in E,\;\;w_i\in F
\right\}.$$ We endow $E \otimes F$, with the following inner product
$$
\langle \xi,\eta\rangle=\sum_{i=1}^n\sum_{j=1}^m \langle x_i,z_j\rangle_1\langle y_i ,t_j\rangle_2,
$$
for $\xi=\displaystyle\sum_{i=1}^nx_i\otimes y_i\in E \otimes F$ and $\eta=\displaystyle\sum_{j=1}^mz_j\otimes w_j\in E \otimes F$. Let $(x_n)_n\subset E$ and $(y_n)_n\subset F$ such that $\displaystyle\lim_{n\to+\infty}x_n=x$ and $\displaystyle\lim_{n\to+\infty}y_n=y$. Why
  $$\displaystyle\lim_{n\to+\infty}x_n\otimes y_n=x\otimes y\;?$$ Thank you.","['functional-analysis', 'tensor-products', 'linear-algebra', 'hilbert-spaces']"
2613439,How to compare competing players with different number of games?,"I hope this is the right place, because the basic problem is mathematical. Regularly my flatmates and I play a card game. But we are playing this in different player constellations and not always with the same number of players.
But I want to compare how good or bad a player was and to choose a winner e.g. after X games. What would be a good and fair way to calculate this? My base idea was to weight the victories with the relative number of games they played.
But I didn't come up with a good solution. Is there an existing formula or a similar problem already solved?","['game-theory', 'statistics']"
2613453,"how many ways are there to order the word LYCANTHROPIES when C isn't next to A, A isn't next to N and N isn't next to T","a. total number of ways to order 13 letters in a word: 13! b. Number of ways for CA/AC: 2*12! Number of ways for AN/NA: 2*12! Number of ways for NT/TN: 2*12! Total: 3*2*12! c. Number of ways for CAN/NAC: 2*11! Number of ways for ANT/TNA: 2*11! Total: 2*2*11! d. Number of ways for CANT/TNAC: 2*10! using the inclusion–exclusion principle I got:
13! -(3*2*12!) + (2*2*11!)-(2*10!). $$-$$
the textbook solution is: 13! -(3*2*12!) + (2*2*11!) +(2*2*11!) -(2*10!).
I dont know what I am missing. Any help is greatly appreciated.","['inclusion-exclusion', 'combinatorics', 'discrete-mathematics']"
2613456,Area of a spherical rectangle,"I would like to know how one can calculate the area of a spherical rectangle which is defined by two longitudes and latitudes on the unit sphere. I am well aware of answers like this question here , but I would like to do it using multidimensional integration. My approach so far I know I can parameterize the points on a unit sphere 
$$\partial\mathbb{S}^2 = \{(x,y,z) \in \mathbb{R}^3 : x^2 + y^2 + z^2 = 1\}$$
by using spherical coordinates: $[0,\pi]\times[0,2\pi]$ $$\Omega= \begin{bmatrix}
\sin\theta\cos\phi \\
\sin\theta\sin\phi \\
\cos\theta
\end{bmatrix}$$ If I integrated over all of the unit sphere's area, I would do the following:
$$\int_F do = \int_0 ^\pi\int_0 ^{2\pi}|\partial_{\theta}\Omega\times \partial_{\phi}\Omega|d\phi d\theta$$ Now, however, I do not need to integrate over the whole unit sphere so I must change my area of integration. Furthermore, I believe that I would have to change my parametrization slightly. Let's say the rectangle is $b$ high (distance between two latitudes), and $c$ wide, (the distance between two latitudes) as well as $a$ above the equator. Since $\phi$ ""is symmetrical"" instead of integrating from $[0,2\pi]$ we can integrate from $[0,c]$, (right?), but how do I integrate over $\theta$, since not only the height of the rectangle is important but also how far away it is from the equator. Your help is greatly appreciated. (Sorry for the bad picture)","['multivariable-calculus', 'integration', 'analysis']"
2613457,A compass moving on two straight lines,"Let $a$ and $b$ be two straight lines intersecting at $O$ with an angle $\theta$ between them. Two sequence of points $\{A_n\} \in a$ and $\{B_n\} \in b$ are such that $A_nB_n=A_{n+1}B_n=A_{n+1}B_{n+1}=d$ for a constant $d$ and all $n \geq 1$ and that $A_n \neq A_{n+1}$, $B_n \neq B_{n+1}$ for all $n$. Question Under what conditions (on $\theta$, $A_1$, and $d$) is $\{A_n\}$ a finite set? How to work out $card(\{A_n\})$ and $card(\{B_n\})$, the number of points in those two sets? Under what conditions is $\{A_n\}$ an infinite set? How is the arrangement of $\{A_n\}$ and $\{B_n\}$ changed when the position of $A_1$ changes?","['discrete-mathematics', 'euclidean-geometry', 'geometry']"
2613461,subset with a special property,"I was reading a book and there was a theorem about all the subsets of $\mathbb{R}$ with the following property: $a+b\not \in A \Leftarrow a,b \in A$. I tried to think of a simple subset with the following property, but my creativity has failed me. I am looking for the simplest subset. Edit: I didn't understand the property right. The set $\{1\}$ is a good example. Edit: The other thing that I didn't understand is how to prove that there is an maximum set out of all those subsets of $\mathbb{R}$ with the mentioned property.",['elementary-set-theory']
2613463,solving third order nonlinear ordinary differential equation.,"Can anybody give any hints how to solve any kind of particular solution involving at least one arbitrary constant of this ode:
$p^2y^{\prime \prime \prime}+ yy^{\prime}+y+ax+b=0$, where $a$,$b$,$p$ are constants.",['ordinary-differential-equations']
2613481,Use Gershgorin's theorem to show that a matrix is nonsingular,"Given two matrices $$A = \begin{pmatrix}
    3 & -\frac{1}{2} & 0 \\
    -\frac{1}{2} & 1 & -\frac{1}{2} \\
    0 & -\frac{1}{2} & 1 \\
    \end{pmatrix}, \qquad D = \begin{pmatrix}
    \alpha & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1 \\
    \end{pmatrix}, \quad\alpha \neq 0$$ I'm first asked to bound the eigenvalues of $\mathrm{A}$. And then using the fact that $\mathrm{D^{-1}AD}$ has the same eigenvalues as A finding a better bound for the eigenvalues of $\mathrm{A}$. I did this also with applying Gershgorin's theorem on $\mathrm{D^{-1}AD}$. Finally I'm asked to conclude that $\mathrm{A}$ is nonsingular. I don't know why I can conclude that $\mathrm{A}$ is nonsingular after applying Gershgorin's theorem on $\mathrm{D^{-1}AD}$. I know that a strictly diagonally dominant matrix is nonsingular and positive definite, but this is not the case for $\mathrm{A}$.","['matrices', 'numerical-linear-algebra', 'linear-algebra']"
2613531,"If A is such that $(A+2I)^2=0$, prove that $A+I$ is invertible.","I'm stuck on the second question of this matrix exercise: Consider a matrix $A$ $\in$ $\mathcal{M}_n(K)$ such that $(A+2I)^2=0$ , where $I$ is the identity matrix. Prove that $A$ is invertible and describe the $A^{-1}$ matrix as a function of $A$ . Prove that $(A+I)$ is invertible. Solution for 1.: $(A+2I)^2=0$ $\Rightarrow$ $\Rightarrow A^2 + 4AI +4I^2 = 0 \Rightarrow$ $\Rightarrow A(A+4I) = -4I \Rightarrow$ $A(-\frac {A+4I}{4}) = I$ , therefore A is invertible, and $A^{-1} = -\frac 14A-I.$ I've tried pretty much every replacement I could think of, but no matter what, I couldn't come to the form $(A+I)B=I$ . Many thanks in advance.","['matrices', 'inverse']"
2613552,Prove that the set of invertible elements in a Banach algebra is open,"I am reading Banach algebras. There is a question in the text that show that set of all invertible elements is an open set. I have thought a lot about it, considering it as a normed space but am not exactly sure how to show that it has an open ball around any invertible element.","['general-topology', 'banach-algebras', 'inverse', 'vector-spaces']"
2613559,Show that Minkowski functional is a sublinear functional,"A set $C\subseteq X$ is convex if for any $x,y\in C$ and any $0\leq t \leq 1,$ we have $tx+(1-t)y\in C.$ A set $C\subseteq X$ is absorbing if for any $x\in X,$ there exists $t>0$ such that $tx\in C.$ Let $X$ be a vector space over $\mathbb{R}$ and let $C$ be a convex and absorbing subset of $X.$ Define $\rho:X\to\mathbb{R}$ by $$\rho(x) = \inf\{t>0: \frac{x}{t}\in C \}.$$ Show that (a) $\rho$ is well-defined and that $\rho(x)\geq 0$ for all $x\in X.$ (b) $\rho(ax) = a\rho(x)$ for any $x\in X$ and any $a\geq 0.$ (c) $\rho(x+y) \leq \rho(x)+\rho(y)$ for any $x,y\in X.$ Moreover, $$\{x\in X:\rho(x)<1 \} \subseteq C \subseteq \{ x\in X:\rho(x)\leq 1\}.$$ My attempt: part (a): Fix $x\in X.$ Clearly $\{t>0:\frac{x}{t}\in C\}$ is bounded below by $0.$ Since $C$ is absorbing, there exists $t>0$ such that $$tx = \frac{x}{\frac{1}{t}} \in C.$$ Therefore, the set $\{t>0:\frac{x}{t}\in C \}\neq \emptyset$ and hence $\rho$ is well-defined. Fix $\varepsilon>0.$ Then there exists $t>0$ such that $\frac{x}{t}\in C$ and $$\rho(x) +\varepsilon>t>0.$$ Since $\varepsilon >0$ is arbitrary, therefore $\rho(x)\geq 0.$ part (b): Fix $x\in X$ Clearly $\rho(ax)=a\rho(x)$ holds if $a=0$ as $\rho(0)= 0.$ Assume that $\alpha>0.$ Then $$\rho(ax) = \inf\{a\frac{t}{a} : \frac{ax}{t} = \frac{x}{\frac{t}{a}} \in C\} = a\inf\{\frac{t}{a}: \frac{x}{\frac{t}{a}}\in C \} = a\rho(x).$$ part (c): Fix $\varepsilon>0.$ Then there exist $t_x$ and $t_y$ such that $\frac{x}{t_x} \in C, \frac{y}{t_y}\in C,$ $$t_x<\rho(x)+\frac{\varepsilon}{2} \text{ and } t_y < \rho(y) + \frac{\varepsilon}{2}.$$ It follows that $$t_x+t_y < \rho(x) + \rho(y) + \varepsilon.$$ By convexity of $C,$ $$\frac{x+y}{t_x+t_y} = (\frac{t_x}{t_x+t_y})\frac{x}{t_x} + (\frac{t_y}{t_x+t_y})\frac{y}{t_y} \in C.$$ Therefore, $$\rho(x+y) \leq t_x+t_y < \rho(x)+\rho(y)+\varepsilon.$$ Since $\varepsilon>0$ is arbitrary, hence $$\rho(x+y)\leq \rho(x)+\rho(y).$$ 'Moreover' part: Let $x\in \{x\in X:\rho(x)<1\}.$ Let $\varepsilon = 1 - \rho(x)>0.$ Then there exists $t>0$ such that $\frac{x}{t}\in C$ and $$1 = \rho(x)+\varepsilon > t.$$ Note that If $C$ is absorbing, then $0\in C.$ By convexity of $C,$ $$x = t(\frac{x}{t}) + (1-t)0\in C.$$ Finally, let $x\in C.$ Then $\frac{x}{1} = x \in C.$ Therefore, $\rho(x)\leq 1.$ Are my proofs correct?","['functional-analysis', 'real-analysis', 'supremum-and-infimum', 'proof-verification']"
2613617,A recursive divisor function,"Question: Function definition : $$f(1)=1$$
$$f(p)=p$$ where $p$ is a prime, and $$f(n)=\prod {f(d_n)}$$ where $d_n$ are the divisors of $n$ except $n$ itself. End result : The end result of the function is when all divisors have been reduced to primes or 1. Example:$$f(12)=f(2)f(3)f(4)f(6)=f(2)f(3)f(2)f(2)f(3)=f(2)^3f(3)^2=72$$ Question parts : (a) Find a general formula for $f(a^n)$ where $a$ is a prime and $n$ is a natural number. (b) Find a general formula for $f(a^nb^m)$ (following same notation). [Note: $a$ and $b$ are unique primes. $n$ and $m$, however, may be equal.] Attempts at solutions : (a) We have solved it. The solution is: $a^{2^{n-2}}$ if $n≥2$, $a$ if $n=1$. (b) As of yet, none of us (me and my colleagues) have come up with a solution. We have solved the special cases
$$f(ab^m)=a^{2^{m-1}} \times b^{(2^{m-2})(m+1)}$$
$$f(a^2b^m)=a^{(2^{m-1})(m+2)} \times b^{(2^{m-2})(m^2+5m+2)/2}$$
$$f(a^3b^m)=a^{(2^{m-1})(m^2+7m+8)/2} \times b^{(2^{m-2})(m^3+12m^2+29m+6)/6}$$ Update 1 : $f(a^4b^m)$ has been solved as well. $$f(a^4b^m)=a^{(2^{m-1})(m^3+15m^2+56m+48)/6} \times b^{(2^{m-2})(m^4+22m^3+131m^2+206m+24)/24}$$ An answer to the above questions is needed. A general formula for $f(n)$ is appreciated, along with an explanation.","['recurrence-relations', 'recursion', 'functions']"
2613643,"Better way to define this bijection [0,1) to (0,1) [duplicate]","This question already has answers here : How to define a bijection between $(0,1)$ and $(0,1]$? (9 answers) Closed 6 years ago . I have been trying to construct a bijection from $[0,1)$ to $(0,1)$ that my professor was showing me in class.  I think that the function works in terms of being one-to-one and onto. Here's the function $$f(x)=\Bigl(\dfrac{2^{n+1}-3}{2^n}\Bigr)-x$$
but in order for this function to work (i.e. be 1-1 and onto), we have to have $$x<\dfrac{2^{n}-1}{2^{n}}$$
This is what the graph looks like. My question is whether or not there a better way to construct $f$ so that $x$ doesn't depend on values of $n$.","['real-analysis', 'proof-writing', 'cardinals', 'elementary-set-theory']"
2613666,Convergence of a series involving $\sin(n)^n$ [duplicate],"This question already has answers here : Does the sequence $\{\sin^n(n)\}$ converge? (1 answer) Is $ \sum\limits_{n=1}^\infty \frac{|\sin n|^n}n$ convergent？ (3 answers) Closed 6 years ago . How does one determine whether or not the infinite series
$$\sum_{n=1}^\infty \sin^n(n)$$
converges? I suspect that it doesn't converge absolutely, but I have no idea how to prove/disprove convergence or prove/disprove absolute convergence. Help?","['real-analysis', 'trigonometry', 'sequences-and-series', 'convergence-divergence']"
2613728,Generating function of the sequence $\binom{2n}{n}^3H_n$,"Generating functions of the sequences $\binom{2n}{n}^2H_n$ and $\binom{2n}{n}^2H_{2n}$, where $H_n$ is $n$-th harmonic number , are known in terms of elliptic integrals
$$
\sum_{n=1}^\infty\binom{2n}{n}^2\frac{H_n}{16^n}k^{2n}=K(\sqrt{1-k^2})+\frac{1}{\pi}K(k)\log\frac{k^2}{16(1-k^2)},\tag{1}
$$
$$
\sum_{n=1}^\infty\binom{2n}{n}^2\frac{H_{2n}}{16^n}k^{2n}=\frac12K(\sqrt{1-k^2})+\frac{1}{\pi}K(k)\log\frac{k}{4(1-k^2)}.\tag{2}
$$ Clausen's formula written in the form
$$
\left[{}_2F_1\left({2a,2b\atop a+b+\tfrac12};x\right)\right]^2={}_3F_2\left({2a,~2b,~a+b\atop a+b+\tfrac12,2a+2b};4x(1-x)\right)\tag{3}
$$
allows one to find the generating function of the sequence $\binom{2n}{n}^3(H_{2n}-H_n)$ by differentiating $(3)$ with respect to $a$ at $a=b=\tfrac14$:
$$
\sum_{n=1}^\infty \binom{2n}{n}^3\frac{H_{2n}-H_n}{64^n}(4x(1-x))^n=\frac{2}{3\pi}K(\sqrt{x})\sum_{n=1}^\infty \binom{2n}{n}^2\frac{4H_{2n}-3H_n}{16^n}x^n.\tag{4}
$$
Thus according to $(1)$ and $(2)$ this gf is known in terms of elliptic integrals. Q: What can be said about the generating functions $\displaystyle f(x)=\sum_{n=1}^\infty \binom{2n}{n}^3\frac{H_n}{64^n}x^n$ and $\displaystyle g(x)=\sum_{n=1}^\infty \binom{2n}{n}^3\frac{H_{2n}}{64^n}x^n$ of the sequences $\binom{2n}{n}^3H_n$ and $\binom{2n}{n}^3H_{2n}$ separately? In view of $(4)$ one can find closed form of $f(1)-g(1)$ and Whipple's sum http://dlmf.nist.gov/16.4.E7 (after differentiating it wrt $c$ at $a=c=\tfrac12,~d=1$) gives closed form of $2g(1)-3f(1)$. This means that at least the closed forms of $f(1)$ and $g(1)$ can be found.","['special-functions', 'hypergeometric-function', 'generating-functions', 'sequences-and-series']"
2613738,"Find a sequence in $[0,\ln 4]$","Find an $a_1, a_2, \dots$ sequence in the interval $[0,\ln 4]$, such that for any $x<y$ positive integers
$$|a_x-a_y|\geq \dfrac{1}{y}$$ I know the well known
$$a_n=\dfrac{(-1)^{n+1}}{n}$$
sequence, and I tried making new sequences from that, but none of them worked.","['integration', 'limits']"
2613750,$\frac{d}{dx} \sqrt{x+2}$,"Forgive me for my simple question, calculus from Engineering school is about ten years in the past for me. $f(x) = \sqrt{x+2}$ What is $f'(x)$? If it were just $f(x) = \sqrt{x}$ it would be easy, because $\sqrt{x} = x^\frac{1}{2}$, but the case of $f(x) = \sqrt{x+2}$ is different because of $+2$, correct? Or am I wrong? :/","['derivatives', 'calculus']"
2613771,Drawing points and straight lines in the plane,"I select points and draw lines in the plane according to the following rule : I start with an initial set of five distinct points, no three of which are on the same line, and at each step I either draw a new line passing through two distinct already selected points, or select an intersection point between two already drawn lines. How to prove (or find a counterexample) that the process can be continued indefinitely, that there always will be new points to be selected or new lines to be drawn ? Note that the process may stop when we start with only four points.","['combinatorics', 'euclidean-geometry']"
2613781,Line Cylinder Intersection,"I am looking for an efficient way of finding the intersection of a line with a cylinder. Several answers are suggested on this site and other places, however I found this answer the most efficient one. The only challenge I am facing is that the given question and its answer only apply to a case where the line originates from the cylinder axis. 
Is there any similar approach that can be generalized to lines starting and ending at any arbitrary position? To be specific, I am looking for finding intersection of any arbitrary line with a cylinder where the cylinder axis is assumed to lie on one the axes (say Z-axis). The given cylinder doesn't have caps and therefore, the line may or may not intersect with the cylinder. If it does intersect, it may have one or two intersection(s) of course.","['linear-algebra', 'geometry']"
2613917,Are most linear operators invertible?,"Are most matrices invertible? discusses this question for matrices. All the answers implicitly used the (unique) vector topology on the space of $n \times n$ matrices. But my understanding (correct me if I'm wrong) is that infinite-dimensional linear operators can have multiple vector norms which induce inequivalent topologies, so the question becomes trickier. My (not entirely precise) question is, for an infinite-dimensional vector space, is the set of isomorphisms a dense open set under every ""reasonable"" operator topology? Under every operator norm topology induced by a ""reasonable"" norm on the vector space? My intuition says yes, but I'd be curious if anyone can make the words ""reasonable"" more precise. (I assume that there's no natural way to generalize the Lebesgue-measure sense of ""almost all matrices are invertible"" to the infinite-dimensional case, due to the absence of an inifinite-dimensional Lebesgue measure , but correct me if I'm wrong.)","['functional-analysis', 'topological-vector-spaces', 'measure-theory', 'inverse']"
2613919,"$\mathbb{C}[x,y]$ is the sections of Spec $\mathbb{C}[x,y]$ minus the origin?","Consider the scheme $X:=$ Spec $ \mathbb{C}[x,y]$ and delete the closed point $(x,y)$ (i.e. the ideal generated by $x,y$). This gives an open set $U \subset$ Spec $\mathbb{C}[x,y]$.  I'm trying to prove that $\mathcal{O}(U)$ is isomorphic to $\mathbb{C}[x,y]$. My strategy was to take the basic-open cover $X_x \cup X_y = U$,  since I know that by definition $\mathcal{O}(X_x) = \mathbb{C}[x,y]_{(x)}$, $\mathcal{O}(X_y) = \mathbb{C}[x,y]_{(y)}$ and I believe that then we have $$\mathcal{O}(U) = \mathcal{O}(X_x)\times_{\mathcal{O}(X_{xy})} \mathcal{O}(X_y) = \mathbb{C}[x,y]_{(x)}\times_{\mathbb{C}[x,y]_{(xy)}} \mathbb{C}[x,y]_{(y)}$$ where by this notation I mean the set of pairs which restrict to the same thing in $\mathcal{O}(X_{xy})$. Please let me know if this is incorrect. Now I certainly seem to have an injective ring hom $$\mathbb{C}[x,y] \rightarrow \mathbb{C}[x,y]_{(x)}\times_{\mathbb{C}[x,y]_{(xy)}} \mathbb{C}[x,y]_{(y)}.$$ So I need to prove its surjective.  The only thing I know to do is to take 
a pair $$\big(\frac{p(x,y)}{q(y)+ x\cdot r(x,y)}, \frac{p'(x,y)}{q'(x)+ y\cdot r'(x,y)}\big)\in \mathbb{C}[x,y]_{(x)}\times_{\mathbb{C}[x,y]_{(xy)}} \mathbb{C}[x,y]_{(y)}$$ and use the fact that the first entry is equal to the second entry (in the fraction field, say) to prove that in fact the denominators are in fact just elements of $\mathbb{C}$. I tried playing around with degrees but to no avail. So Am I on the right/easiest track? and is there a simple way to finish the argument?","['schemes', 'abstract-algebra', 'algebraic-geometry']"
2613937,Arrangements of a given word with specific properties,"How many arrangements of MATHEMATICS are there that have all of the following properties: (a) TH appears together in the that order (b) E appears somewhere before C My attempt at solving this went like this: Note: we have 2 M's and 2 A's and since TH has to appear in that order, it can be considered a single element instead of two elements. Mathematics has 11 letters but we have 10 spots to fill since TH is again, one element and appears together every time. so the first thing I considered is the case where C is the second element making E the first element. I came up with the equation $${ \frac {8!}{2!2!}} $$ since we have 8 elements after choosing E and C and then the $2!$ is there since we have 2 A's and 2 M's. After this I thought of the next possibility which was C is the third letter and that makes E either the first or second letter, which gave me the following: $$(2 \times 8) + 7!$$ The left side is that E can be in either position and that the other position will be filled with one of the remaining 8 letters. When I got to the right side is when I realized that I was gonna get stuck. How do I account for the fact that I might not need to have the $2!$ in the denominator if either A or M is in the left side of the arrangement next to E and the cases where it isn't? My guess is it would be something like $$\frac{7!}{2!2!} + (2 \times \frac{7!}{2!}) $$ which includes the case where both A and M are on the right side of C and then the case where only A and only M are on that side. As C moves farther down the arrangement and there is less space for elements to be there, I would assume I would have to take into consideration the fact that the left side of C could and will have two A's and/or two M's. maybe I am over complicating this problem but this is what I could think of. If I'm doing something wrong here please let me know and point me in the right direction.","['combinatorics', 'discrete-mathematics']"
2613964,What precisely is the Friendship Paradox (and is Wikipedia wrong?),"Friendship paradox is the somewhat well-known statement that ""statistically speaking, your friends have more friends than you do"". To my mind, which is surely ignorant of any complexities of social sciences, it seems that this should translate into the following statement: Friendship Paradox Theorem I. Let $G = (V,E)$ is an undirected graph. Then the average degree of a vertex sampled uniformly at random from the neighbourhood of a vertex sampled uniformly at random from $V$ is at least as large as the average degree of a vertex sampled uniformly at random from $V$, i.e., $$
\frac{1}{|V|} \sum_{v \in V} \frac{1}{\deg(v)} \sum_{u : uv \in E} \deg(u) \geq \frac{1}{|V|} \sum_{v \in V} \deg(v).\tag{1}
$$ Hence, I was somewhat to see that Wikipedia justifies the friendship by a different inequality. Friendship Paradox Theorem II. Let $G = (V,E)$ is an undirected graph. Then the average degree of a vertex sampled by choosing a random endpoint of an edge sampled uniformly at random is at least as large as the average degree of a vertex sampled uniformly at random from $V$, i.e., $$
\frac{1}{2|E|} \sum_{v \in V} \deg(v)^2 \geq \frac{1}{|V|} \sum_{v \in V} \deg(v). \tag{2}
$$ Now, both inequalities are true, and friendship paradox is an empirical observation, so there is not much of a problem. However, I would be grateful if someone could explain to me the intuitive appeal of (2) as a justification of said observation (right now, it seems to me that it's just obtained by choosing the distribution on $V$ so as to make computations easier). Of course, it could be the case that no such justification exist, in which case I would be grateful for references (and moral support) to edit the relevant Wikipedia page.","['applications', 'graph-theory', 'probability']"
2613967,Faster way of determining the coefficient of a polynomial function?,"Question: Determine if the leading coefficient of the function ""a"", is positive or negative. a) $$f(x)=(x-3)^2(x+1)(x+2)^3$$ In my notes I stated the sign of the leading coefficient without work but in order to get the answer now I had to expand the polynomial function out. Any help would be appreciated. -Jack","['algebra-precalculus', 'functions']"
2613969,"Is there an algorithm for the ""zig-zag"" proof that the rationals are countable?","I'm sure that we've all seen diagrams such as this to show how we can order sets such as the rationals in order to show that they are countable, however, is there a way to find the nth term of such a list? Can it be expressed as a formula? Or does this require some sort of algorithm? Furthermore, can this be done without any informal appeal to ""the nth line""?","['algorithms', 'elementary-set-theory', 'proof-explanation']"
2613979,Is complex analysis same difficulty as vector analysis/multivariable calculus? [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 6 years ago . Improve this question In vector calculus, you need some geometric intuition and find some integrals. I scanned a book on complex analysis and realized that they're aren't many pictures as we see in a multivariable textbook and there is always some substitution of variables going on when computing some integral. Is learning complex analysis in terms of computing integrals and derivatives just as easy as learning computational calculus? Why don't complex analysis books have as many pictures as calculus books do? Because they're are less pictures in complex analysis books and tons of substitutions I can't understand, I'm guessing complex analysis isn't as easy as calculus.","['multivariable-calculus', 'complex-analysis', 'soft-question', 'vector-analysis']"
2613999,"Eisenstein Series, discriminant and cusp forms","I'm working on the following exercise: Let $E_k=\frac{G_k(z)}{2\zeta(k)}$ be the normalized Eisenstein series of weight $k$. Calculate a)$E_4=1+240\sum_{n=1}\sigma_3(n)q^n$ and b)$E_6=1-504\sum_{n=1}\sigma_5(n)q^n$, where $q=\exp(2\pi i z)$ c) Show that $\sigma_3(n)=\sigma_5(n)\,\text{mod}(12)$, where $\sigma_k(n)=\sum_{d|n}d^k$ d) Finally show that $\Delta=\frac{1}{1728}(E^3_4-E^2_6)$ is a non-zero cusp form of weight $k=12$ and integer fourier coefficients. My solution attempt: a&b)The Fourier transform for $G_k$ is $G_k(z)=2\zeta(k)+\frac{2(2\pi i)^k}{(k-1)!}\sum_{n=1}^{\infty}\sigma_{k-1}(n)q^{n}$, where $q=\exp(2\pi i z)$.
Further $\zeta(2k)=\sum_{n=1}n^{-2k}=\frac{(-1)^{k+1}(2\pi)^{2k}}{2(2k)!}B_{2k}$ from the Euler formula, which gives $\zeta(4)=\frac{\pi^4}{90}$ and $\zeta(6)=\frac{\pi^6}{945}$. Hence $$E_4=\frac{G_4}{2\zeta(4)}=\left(\frac{\pi^4}{45}+\frac{2(2\pi i)^4}{3!}\sum_{n=1}^{\infty}\sigma_3(n)q^n\right)\frac{\pi^4}{45}=1+240\sum_{n=1}\sigma_3(n)q^n$$ Similarly for $E_6$. c) is where I'm starting to have problems, I have honestly no idea how to approach this part. d) First we have that for $G_{k}(\frac{az+b}{cz+d})=(cz+d)^{k}G_k(z)$ and further that $G_k$ is analytic and possesses a Fourier transform, as such it is a modular form of weight $k$. By definition of $E_k$ this property is ""inherited"" obviously and we also immediately get that $E_k^j$ is a modular form of weight $kj$.  As such $\Delta'=E^3_4-E^2_6$ is the difference of two modular forms of weight $kj=12$ and hence a modular form of weight $12$ itself. Thus $\Delta$ is one as well. To verify that $\Delta$ is indeed a cusp form we have to examine the behavior at $\infty$. We have that $\lim_{z\to\infty}G_k(z)=2\zeta(k)$ and hence that $\Delta=\frac{1}{1728}(E^3_4-E^2_6)=\frac{1}{1728}\left(\left(\frac{G_4(z)}{2\zeta(4)}\right)^3-\left(\frac{G_6(z)}{2\zeta(6)}\right)^2\right)$ tends to $0$ as $z\to\infty$, as such $\Delta$ is a cusp form of weight $12$. For the fourier coefficients I'm having problems again. Calculating some of the coefficients, it's somewhat obvious that they have to be whole numbers, but I'm unsure how to prove this. As such I guess I'm hoping for some help with c) and the second part of d). Thanks in advance! Edit: And if someone could confirm that my reasoning at all is sound would be amazing as well! I'm not entirely sure with my arguments in d). Edit2: I think I've solved it: \begin{align*}
E_4^3-E_6^2&=(1+240\sum_{n=1}\sigma_3(n)q^n)^3-(1-504\sum_{n=1}\sigma_5(n) q^n)^2\\
&=12^2\left(5\sum_{n=1}\sigma_3(n)q^n+7\sum_{n=1}\sigma_5(n)q^n\right)+12^3\left(100(\sum_{n=1}\sigma_3(n)q^n)^2-147(\sum_{n=1}\sigma_5(n) q^n)^2+8000(\sum_{n=1}\sigma_3(n) q^n)^3\right) \\
&=12^2\left(\left(5\sum_{n=1}\sigma_3(n)q^n+7\sum_{n=1}\sigma_5(n)q^n\right)\right)+\left(12\left(100(\sum_{n=1}\sigma_3(n)q^n)^2-147(\sum_{n=1}\sigma_5(n) q^n)^2+8000(\sum_{n=1}\sigma_3(n) q^n)^3\right)\right)
\end{align*} With $\Delta=\frac{1}{1728}(E^3_4-E^2_6)=\frac{1}{2^63^3}(E^3_4-E^2_6)$ and the above it remains to show that $5\sum_{n=1}\sigma_3(n)q^n-7\sum_{n=1}\sigma_5(n) q^n$ is divisible by 12. We have $$5\sum_{n=1}\sigma_3(n)q^n-7\sum_{n=1}\sigma_5(n) q^n=\sum_{n=1}(5\sigma_3(n) +7\sigma_5(n))q^n=\sum_{n=1}\sum_{d|n}(5d^3+7d^5)q^n$$ for $d\in\mathbb{N}$. This leaves to prove $5d^3+7d^5$ is divisible by 12. We have $$5d^3+7d^5=d^3(5+7d^2)=
\begin{cases}
d^3(1-d^2)&=0 (\text{ mod }4)\\
d^3(-1+d^2)&=0 (\text{ mod }3)
\end{cases}$$ This proves c) and what was left of d)","['complex-analysis', 'analytic-number-theory', 'modular-forms']"
2614000,Possible condition on locally Euclidean subset of Euclidean space to be embedded submanifold II,"Given a locally Euclidean (locally homeomorphic to some Euclidean space) subset $X\subset\mathbb R^n$ and $p\in X$, let $\widetilde{\mathrm{T}}_pX$ denote the tangent set of $X$ at $p$, namely the set of derivatives of differentiable curves in $X$ based at $p$. Suppose for all $p\in X$ we have that $\widetilde{\mathrm{T}}_pX$ is a linear subspace of $\mathbb R^n$ of dimension $\dim_pX$. Does it follow that $X\subset\mathbb R^n$ is an embbeded differentiable submanifold? Added. Here's a thought. Perhaps we can locally construct an exponential map which is a diffeomorphism between a neighborhood of the tangent plane and a neighborhood of $p$ in $X$. Using this diffeomorphism we can move between a differentiable structure on $X$ and the differentiable functions defined on the aforementioned neighborhood of the tangent plane.","['manifolds', 'smooth-manifolds', 'differential-geometry', 'calculus']"
2614004,homeomorphism between Ring (Annulus) and Cylinder,How to prove that a Ring (Annulus) in $R2$ is homeomorphic to a Cylinder in $R3$? I understand that I need to find a function that maps every point in the ring to a point on the cylinder.,['general-topology']
2614053,"Japanese theorem, in its extended form","""Japanese theorem"" is the name given to the following result (see Fig. 1 below) :  if $A_1A_2A_3A_4$ is a cyclic quadrilateral, the incenters of the $4$ triangles $A_pA_qA_r$ with all triples $\{p,q,r\}$ are the vertices of a rectangle $R$ . Moreover, the medians of rectangle $R$ intersect circular arcs $A_{k}A_{k+1}$ (with $A_5:=A_1$ ) in their midpoints. In fact, there is a second level of properties that is seldom given ( see references and comments at the end of the text ): Let $\frak{E}$ denote the set of the $4 \times 3 = 12$ excenters of all triangles $A_pA_qA_r$ : The convex hull of $\frak{E}$ is a rectangle $R'$ with sides parallel to sides of rectangle $R$ . The 8 other points of $\frak{E}$ are situated on the sides of rectangle $R'$ at the intersection of the extended sides of rectangle $R$ . Fig. 1 : Rectangles R (blue vertices) and R' (red vertices). Midpoints of arcs $A_kA_{k+1}$ in black. Fig. 2 enriches Fig. 1 with the four circles associated with triangle $A_1A_2A_3$ (its incircle and its three excircles) [Drawing all the incircles and excircles would give a too messy picture]. Fig. 2 : Incircle and the three excircles of triangle $A_1A_2A_3$ . Some supplementary properties exist. I will not give them here. My questions dealing with this second part of the japanese theorem : is there is a simple proof of it ? The proof I have done is rather complicated. I have the feeling that there must be more direct proofs. are there known consequences of this rather surprizing result ? References and comments : The detailed MAA article here ""The Penguin Dictionary of Curious and Interesting Geometry"" by David Wells (Penguin, 1991) p. 43, a very interesting book, full of ill-known results: ( https://archive.org/details/ThePenguinDictionaryOfCuriousAndInterestingGeometry ). The Wolfram corresponding article. Strangely, neither Wikipedia nor Cut-the-knot sites on ""Japanese theorem"" mention the second part. See also this site. Similar properties exist for tangential quadrilaterals. See for example ( http://forumgeom.fau.edu/FG2011volume11/FG201108.pdf ) cited in ( https://en.wikipedia.org/wiki/Tangential_quadrilateral ). See as well this blog .","['circles', 'triangles', 'geometry']"
2614055,Proving that $\lim\limits_{x \to 0} x \cdot (\log(x))^\alpha = 0$ for every $\alpha > 0$?,"$\lim_{x \to 0} x \cdot (\log(x))^\alpha = 0$ for every $\alpha > 0$? It should tend to zero for every $\alpha \in \Bbb R_+$, though I can't find a rigorous way to prove it.","['real-analysis', 'limits', 'logarithms', 'calculus', 'algebra-precalculus']"
2614100,proving that a curve with constant curvature contained in a sphere its a circle,"i have a curve $\alpha:I\rightarrow \mathbb{R}^3$ such that his curvature $k$ is constantand $\alpha$ is entirely contained in a sphere, i must prove that this curve is a cirlce. My try: I need to prove that $\alpha$ has zero torsion, i supose the sphere with center at origin and radius $r>0$, so i got $|T^{'}(s)|=k$ for all $s$ where $T$ is the tangent line of $\alpha$ and $|\alpha(s)|^2=r^2$ for all $s$, i wanted to prove that the binormal $B$ of $\alpha$ is contants, but i derivatite these two equation a lot and i couln't conclude anything, anyone can help?","['curves', 'differential-geometry']"
2614107,Another attempt at solving a PDE with the method of characteristics,"I want to use the method of characteristics to obtain the solution to this PDE, $$\frac{\partial F}{\partial t}=\left(z-t\right)\left(\beta z-\gamma\right)\frac{\partial F}{\partial z}$$ which I've seen is of the form $$F\left(t,z\right)=F\left(\left(\frac{\beta\left(z-1\right)}{\beta z-\gamma}\right)e^{\left(\beta-\gamma\right)t}\right).$$ Attempt: I was trying by identifying the PDE with one of the form $$a\left(t,z\right)F_{z}+b\left(t,z\right)F_{t}=g\left(t,z,F\right),$$ deriving the solution $h\left(t,z\right)=c_{1}$ for $$\frac{dz}{dt}=\frac{a\left(t,z\right)}{b\left(t,z\right)}$$ then the solution $j\left(t,z,F\right)=c_{2}$ for $$\frac{dF}{dt}=\frac{g\left(t,z,F\right)}{b\left(t,z\right)}$$ and writting $j\left(t,z,F\right)=K\left(h\left(t,z\right)\right)$. However, I got stuck when I stomped upon the following ODE to solve, $$z'\left(t\right)=-\beta z^{2}\left(t\right)+\beta tz\left(t\right)+\gamma z\left(t\right)-\gamma t.$$ How can I solve the problem?","['ordinary-differential-equations', 'characteristics', 'partial-differential-equations']"
2614116,"Why is ""greater than"" not a total function?","I was just doing some fun reading, and I stumbled upon the following: A total function $f: X \mapsto Y$ is a binary relation on $X \times Y$ that satisfies the following two properties: $\forall x \in X$, there is a $y \in Y$ such that $[x, y] \in f$ if $[x, y_1] \in f$ and $[x,y_2] \in f$, then $y_1 = y_2$ That's all nice and dandy, however, the text then goes on to state A relation on $\mathbb{N} \times \mathbb{N}$ representing greater than fails to satisfy either of the conditions. I understand why it fails the second condition, but why does it fail the first condition? Isn't it a similar idea to $f: X \mapsto X : x \mapsto x+1$, which would pass the first condition. In other words, isn't there alwasy a number in $\mathbb{N}$ that is greater than the current one? Cheers!","['elementary-set-theory', 'relations', 'functions', 'discrete-mathematics']"
2614123,Question on calculation of integral.,"I need to show this proof $$\int_0^1\int_0^xF(z)dzdx\le \int_0^1\int_0^xG(z)dzdx$$ if and only if $F$ has Lower variance than $G$ for the same mean of $F$ and $G$ and with support $[0,1]$ . Also I know that by the second order stochastic dominance definition $\int_0^x F(T)dT\le \int_0^x G(T)dT \iff \int u(x) dF(x) \ge \int u(x) dF(x)$ The solution is as follows: But I cannot reach from this integral $$\int_0^1\int_0^xF(z)dzdx\le \int_0^1\int_0^xG(z)dzdx$$ to this integral $U(F)= \int u(x)dF(x) \ge U(G)= \int u(x)dG(x)$ How can I get this last integral from the integral in yellow box? I asked this way.","['game-theory', 'self-learning', 'integration', 'calculus']"
2614156,The limit of $2\frac{\sin\left(\frac{M\theta_{k}}{2}\right)\cos\left(\frac{1}{2}(1+M)\theta_{k}\right)}{\sin\left(\frac{\theta_{k}}{2}\right)} $,"I have $$\lambda_{k,N}=2\dfrac{\sin\Big(\dfrac{M\theta_{k}}{2}\Big) \cos\Big(\dfrac{1}{2}(1+M)\theta_{k}\Big)}{\sin\Big(\dfrac{\theta_{k}}{2}\Big)} $$ where $\theta_{k} \in [0, 2\pi]$, I can then let  $\theta_{k}=\dfrac{2\pi k}{N}$ for $k=0...N$, I would like to find the limit of $\lambda_{k,N}$ when $N \to +\infty$ knowing that $M=M(N)$ and $\dfrac{M}{N}=0$ when $N \to +\infty$. it would have been more interesting if I can compute the limit of $\lambda_{k,N}$ for any $k$ but it's not computable for some values of $\theta_{k}$. Is it possible to find a closed form expression for the limit ?","['limits', 'trigonometry', 'probability', 'sequences-and-series', 'fourier-transform']"
2614160,Calculate limit with L'Hopital's rule,"I want to calculate the limit $\displaystyle{\lim_{x\rightarrow 0}\frac{x^2\cos \left (\frac{1}{x}\right )}{\sin x}}$ . I have done the following: It holds that $\lim_{x\rightarrow 0}\frac{x^2\cos \left (\frac{1}{x}\right )}{\sin x}=\frac{0}{0}$ . So, we can use L'Hopital's rule: \begin{align*}\lim_{x\rightarrow 0}\frac{x^2\cos \left (\frac{1}{x}\right )}{\sin x}&=\lim_{x\rightarrow 0}\frac{x^2\cos \left (\frac{1}{x}\right )}{\sin x} \\ &=\lim_{x\rightarrow 0}\frac{\left (x^2\cos \left (\frac{1}{x}\right )\right )'}{\left (\sin x\right )'} =\lim_{x\rightarrow 0}\frac{2x\cdot \cos \left (\frac{1}{x}\right )+x^2\cdot \left (-\sin \left (\frac{1}{x}\right )\right )\cdot \left (\frac{1}{x}\right )'}{\cos x} \\ &=\lim_{x\rightarrow 0}\frac{2x\cdot \cos \left (\frac{1}{x}\right )-x^2\cdot \sin \left (\frac{1}{x}\right )\cdot \left (-\frac{1}{x^2}\right )}{\cos x} \\ & =\lim_{x\rightarrow 0}\frac{2x\cdot \cos \left (\frac{1}{x}\right )+\sin \left (\frac{1}{x}\right )}{\cos x}=\lim_{x\rightarrow 0}\left (2x\cdot \cos \left (\frac{1}{x}\right )+\sin \left (\frac{1}{x}\right )\right ) \\ & =\lim_{x\rightarrow 0}\left (2x\cdot \cos \left (\frac{1}{x}\right )\right )+\lim_{x\rightarrow 0}\left (\sin \left (\frac{1}{x}\right )\right )\end{align*} We calculate the two limits separately $\lim_{x\rightarrow 0}\left (2x\cdot \cos \left (\frac{1}{x}\right )\right )$ : \begin{equation*}\left |\cos \left (\frac{1}{x}\right )\right |\leq 1 \Rightarrow -1\leq \cos \left (\frac{1}{x}\right )\leq 1  \Rightarrow -2x\leq 2x\cdot \cos \left (\frac{1}{x}\right )\leq  2x\end{equation*} we consider the limit $x\rightarrow 0$ and we get \begin{equation*}\lim_{x\rightarrow 0} \left (2x\cdot \cos \left (\frac{1}{x}\right ) \right )=0\end{equation*} How can we calculate the limit $\lim_{x\rightarrow 0}\left (\sin \left (\frac{1}{x}\right )\right )$ ?","['calculus', 'limits']"
2614201,On the equivalence between the Kullback-Leiber divergence and the $L^2$ distance.,"There is a question very similar to this one here , matter of fact I was looking to see if this equivalence was true and I stumbled upon that question. For the sake of completeness I quote the question: Let $P$ and $Q$ be two probability measures with densities $p$ and $q$ with respect to the Lebesgue measure on [0,1] such that: $0<a\leq p(x)\leq b$ , $0<a\leq q(x)\leq b$ $\forall x\in $ [0,1] $a$ , $b>0$ are constants. Show that the Kullback distance $K(P,Q)$ is equivalent to the squared $L^2$ distance between densities $p$ and $q$ . where I assume the Op is talking about the standard Kullback leiber divergence $$K(P,Q):= \int_{- \infty}^{+\infty} p(x) \log \frac{p(x)}{q(x)} \, dx$$ that is not, in fact, a distance (it is missing the symmetry property). Nonetheless we want to show there are two constants $\alpha$ and $\beta$ such that $\alpha||p-q||_{L^2}^{2}\leq K(P,Q)\leq \beta ||p-q||_{L^2}^{2}$ . How does one show the lower bound? I assume what the OP of the linked question is doing for the upper bound is that he is expanding $\log(q(x))$ at the point $p(x)$ resulting in $$\int_{0}^{1} p(x) \log \frac{p(x)}{q(x)} \, dx \le \int_{0}^{1}-p(x) ( q(x) - p(x)) \frac{1}{p(x)} + \frac{1}{2 p(x)^2}( q(x) - p(x))^2 \, dx \le $$ $$\int_{0}^{1} ( q(x) - p(x)) \, dx + \frac{1}{2 a^2} \int_{0}^{1}(q(x) - p(x))^2 \, dx = \frac{1}{2 a^2} \int_{0}^{1}(q(x) - p(x))^2 \, dx $$ where we have been assuming continuity of $q(x)$ . But the Op of the linked question obtains some extra constants before the $L^2$ norm so I am afraid my calculation are incorrect (as they often have been) could somebody point out my mistake?","['functional-analysis', 'real-analysis', 'measure-theory', 'probability-theory']"
2614231,Help verifying the Lindeberg condition,"Suppose $X_1, X_2,...$ is a sequence of independent, identically distributed
random variables with $\mathbb{E}[X_n] = 0$ and $\mathbb{E}[X_n^2] = 1$ . Find  sequences of constants $a_n$ and $b_n$ such that, $$ \frac{\left(\sum\limits_{1 \leq i < j \leq n}X_iX_j - a_n\right)}{b_n} $$ converges in distribution and find the limit distribution. My approach was to try and use the Lindeberg condition. Let $S_n = \sum\limits_{1 \leq i < j \leq n}X_iX_j = \sum\limits_{j=1}^n\sum\limits_{i=1}^{j-1}X_iX_j$ . and define, $Y_j = \sum\limits_{i=1}^{j-1}X_iX_j$ . Then $\mathbb{E}[Y_j] = 0$ and $\mathbb{E}[Y_j^2] = j-1$ . Let $b_n^2 = \sum\limits_{j=1}^n\mathbb{E}[Y_j^2] = \frac{n(n+1)}{2} - n$ . To verify the Lindeberg condition, we need to investigate the asymptotic behaviour of, $$ \frac{1}{b_n^2}\sum\limits_{j=1}^n\mathbb{E}[Y_j^2\mathbb{1}_{\{|Y_j|>\epsilon b_n\}}] \;\;\;\;\; \forall \epsilon > 0 $$ If the above quantity goes to $0$ as $n\rightarrow\infty$ , then $S_n/b_n$ will converge in distribution to $N(0,1)$ . Since we're not given any other information about $X_1, X_2,...$ , can we possibly use this approach? It makes no assumption of boundedness.","['probability-theory', 'convergence-divergence', 'central-limit-theorem']"
2614239,"Why is $\int_0^t t \, dW_s$ not a martingale?","Assuming $W_t$ is the standard Brownian motion, then $\int_0^t t  \, dW_s=tW_t$ is not a martingale since $\mathbb{E}(tW_t \mid \mathcal{F}_s)=tW_s$. However, by martingale property of Ito's integral it seems that $\int_0^t t \, dW_s$  should be a martingale... The only explanation I can think of is that the ""stochastic process"" $t$ looks like not adapted to $\mathcal{F}_s$. But this sounds very strange because $t$ is actually a constant in that integral. And if that is the case, the answer here that uses Ito's isometry to compute a variance is also problematic because there $(t-s)$ is also not adapted to $\mathcal{F}_s$ but Ito's isometry requires such adaptation.","['brownian-motion', 'probability-theory', 'stochastic-calculus', 'martingales']"
2614260,The integral of an elliptic integral: $\int_{0}^{1}\frac{x\mathbf{K}^2\left ( x \right )}{\sqrt{1-x^{2}}}\mathrm{d}x$,"Prove: $$\mathcal{I}=\int_{0}^{1}\frac{x\mathbf{K}^2\left ( x \right )}{\sqrt{1-x^{2}}}\mathrm{d}x=\frac{\pi ^{4}}{16}\,_7F_6\left ( 
\dfrac{1}{2},\dfrac{1}{2},\dfrac{1}{2},\dfrac{1}{2},\dfrac{1}{2},\dfrac{1}{2},\dfrac{5}{4};\dfrac{1}{4},1,1,1,1,1;1 \right )$$ I found this beautiful integral in Yury A.Brychkov's book Handbook of Special Functions,Derivatives, Integrals, Series and Other Formulas p.278. My attempt: With the representation of elliptic integral by hypergeometric function below $$\mathbf{K}\left ( x \right )=\frac{\pi }{2}\, _2F_1\left ( \frac{1}{2},\frac{1}{2};1;x^{2} \right )=\frac{\pi }{2}\, _2F_1\left ( \frac{1}{4},\frac{1}{4};1;4x^{2}\left ( 1-x^{2} \right ) \right )$$ $$\mathbf{K}^{2}\left ( x \right )=\frac{\pi^{2} }{4}\, _3F_2\left ( \frac{1}{2},\frac{1}{2},\frac{1}{2};1,1;4x^{2}\left ( 1-x^{2} \right ) \right )$$ hence \begin{align*}
\mathcal{I}&=\frac{\pi ^{2}}{4} \int_{0}^{1}\frac{x}{\sqrt{1-x^{2}}}\sum_{k=0}^{\infty }\frac{\Gamma^{3} \left ( k+\dfrac{1}{2} \right )\left [ 4x^{2}\left ( 1-x^{2} \right ) \right ]^{k}}{\Gamma^{3} \left ( \dfrac{1}{2} \right )\Gamma^{3} \left ( k+1 \right )}\mathrm{d}x\,\,\left ( x^{2}\rightarrow x \right )\\
&=\frac{\pi ^{2}}{8} \int_{0}^{1}\frac{1}{\sqrt{1-x}}\sum_{k=0}^{\infty }\frac{\Gamma^{3} \left ( k+\dfrac{1}{2} \right )\left [ 4x\left ( 1-x \right ) \right ]^{k}}{\Gamma^{3} \left ( \dfrac{1}{2} \right )\Gamma^{3} \left ( k+1 \right )}\mathrm{d}x\\
&=\frac{\pi ^{2}}{8}\sum_{k=0}^{\infty }\frac{\Gamma^{4} \left ( k+\dfrac{1}{2} \right )4^{k}}{\Gamma^{3} \left ( \dfrac{1}{2} \right )\Gamma^{2} \left ( k+1 \right )\Gamma \left ( 2k+\dfrac{3}{2} \right )}\\
&=\frac{\pi ^{2}}{8}\sum_{k=0}^{\infty }\frac{\Gamma^{4} \left ( k+\dfrac{1}{2} \right )4^{k}}{\Gamma^{2} \left ( \dfrac{1}{2} \right )\Gamma^{2} \left ( k+1 \right )\Gamma \left ( k+\dfrac{3}{4} \right )\Gamma \left ( k+\dfrac{5}{4} \right )2^{2k+\frac{1}{2}}}\\
&=\frac{\pi ^{2}}{8\sqrt{2}}\sum_{k=0}^{\infty }\frac{\Gamma^{4} \left ( k+\dfrac{1}{2} \right )}{\Gamma^{2} \left ( \dfrac{1}{2} \right )\Gamma^{2} \left ( k+1 \right )\Gamma \left ( k+\dfrac{3}{4} \right )\Gamma \left ( k+\dfrac{5}{4} \right )}\\
&=\frac{\pi ^{2}\Gamma ^{2}\left ( \dfrac{1}{2} \right )}{8\sqrt{2}\Gamma \left ( \dfrac{3}{4} \right )\Gamma \left ( \dfrac{5}{4} \right )}\, _4F_3\left ( \frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{1}{2};1,\frac{3}{4},\frac{5}{4};1 \right )\\
&=\frac{\pi ^{2}}{4}\, _4F_3\left ( \frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{1}{2};1,\frac{3}{4},\frac{5}{4};1 \right )
\end{align*} But the numerical result doesn't match.Is there something wrong with my solution? Any help will be appreciated! Another solution (by Editor):
The originial integral equals to $\int_0^1 K'(x)^2dx$ . Now use that $$\frac{4}{\pi^2}K'(x)_=\sum_{n=0}^\infty (-1)^n (4n+1) \left(\frac{\binom{2n}n}{4^n}\right)^3 P_{2n}(x)$$ and Parseval theorem of FL expansion. The result immediately follows.","['calculus', 'hypergeometric-function', 'integration', 'definite-integrals', 'elliptic-integrals']"
2614261,Is this example of an inductive (sub)set correct?,"Note: It seems that the term inductive set has varying definitions . I am not referring (I think) to the definition used here , so this question is not a duplicate of that one. Specifically, consider the definition found on p. 12 of Dudley, Real Analysis and Probability : More generally, let $(X, <)$ be any partially ordered set. A subset $Y \subset X$ will be called inductive if, for every $x \in X$ such that $y \in Y$ for all $y \in X$ such that $y<x$, we have $x \in Y$. The author then goes on to state that the set $( -\infty, 0)$ in $\mathbb{R}$ is inductive (presumably $\mathbb{R}$ with the standard partial order is meant), but: Question: Is that a typo? I.e. how is $(-\infty, 0)$ inductive using the above definition? It seems like $(-\infty, 0]$ might be inductive using the above definition, but not $(- \infty, 0)$. Either a correction of my flawed reasoning, or a sanity check that it is indeed a typo, would help. (Flawed) reasoning: If we take $X = \mathbb{R}$ and $Y = (-\infty, 0)$, then seemingly $0 \in X$ is such that for all $y \in \mathbb{R}$ with $y < 0$, $ y \in (-\infty, 0)$. (That's literally the definition of that half-open interval, right?) So then seemingly by the definition of inductive set, if $(-\infty, 0)$ were inductive, we would have $0 \in (-\infty,0)$, which is obviously untrue. However, the way the definition is worded in Dudley is very confusing to me and it seems likely that I am messing up the order of logical quantifiers and connectives to get a non-equivalent statement in my mind, in particular how I am thinking of the definition is as follows: A subset $Y \subset X$ is inductive if, for all $x \in X$ such that ($y<x \implies y \in Y$) , $x \in Y$. (Now that I write it out my wording of the definition, in addition to most likely being wrong, is also rather opaque.) Anyway, $y <0$ clearly implies $y \in (-\infty, 0)$, hence my ""reasoning"" above.","['order-theory', 'elementary-set-theory']"
2614268,Why is a discrete topology called a discrete topology?,"I'm not looking for the definition of discrete topology given in textbooks; I'm wondering why the word 'discrete' was chosen. I mean, the concept of discrete topology is built up from sets, which are built from objects--which are discrete. So, if we're looking for a word to differentiate power sets as topologies from other topologies, and we use the adjective 'discrete' to accomplish that differentiation because the power set is composed of discrete objects--then, by similar reasoning, couldn't we call all topologies on sets 'discrete'. Because they're built from discrete objects and compositions, too. All of them. All topologies. There must be some other reason we call discrete topologies 'discrete'. What is it?","['terminology', 'general-topology']"
2614302,Initial Distribution of Stochastic Differential Equations,"consider the SDE \begin{align}
\begin{cases}
X_t= \mu (t,x_t)dt + \sigma(t,X_t) d W_t \quad \forall t\in [0,T] \ (\text{or } t\geq 0),\\ 
X_0 \sim \xi.
\end{cases}
\end{align} Suppose that, somehow, I could show (weak or strong) existence of a solution for some initial distribution $\xi$. Now I consider the same SDE with the same coefficients but a different initial distribution $\tilde{\xi}$. Which techniques are available to infer on the (weak or strong) existence of this SDE? (If necessary, feel free to impose existence of densities or moments for the initial distribution.) In particular, if I could show that we have (weak or strong) existence of a solution for $\xi=\delta_x$ for all $x\in \mathbb{R}^n$, how can I show (weak or strong) existence for any $\xi$  (on which you can impose assumptions). I am also interested in the relation of pathwise uniqueness and uniqueness in law for two SDEs with the same coefficients but different initial conditions (i.e. if I have uniqueness for the SDE for one initial condition, do I have it also for the same SDE with another initial condition?). But that is maybe a different question and I don't want to overplay my hand. I am happy for any answer which only considers existence, not uniqueness or vice versa. You don't have to spell things out if it's done elsewhere, I am also happy with a reference to a book, paper, lecture notes or whatever. Thank you in advance!","['stochastic-processes', 'probability-theory', 'stochastic-analysis', 'brownian-motion', 'stochastic-differential-equations']"
2614313,Equation with huge number of nested square roots:,"Find all real roots of equation:
$$
\underbrace{\sqrt{x+2\sqrt{x+2\sqrt{x+...+2\sqrt{x+2\sqrt{3x}}}}}}_\text{2018 roots}=x
$$ I know that the answers are 0 and 3. What is common about those values is that each time, the square root is applied, the result is exactly $x$, therefore the number of iterations doesn't really matter. To clarify that, declare the sequence of $a_i(x)$ as:
$$
a_0(x)=x;\quad a_n(x)=\sqrt{x+2a_{n-1}(x)}
$$
So, the equation of the problem can be written as $a_{2018}(x)=x$. If it could be proven that for given $x$ the sign of $a_n-a_{n-1}$ is same for any $n$ (i.e. the sequence is monotonously increasing, decreasing or constant) then, since $a_{2018}(x)=a_{0}(x)$ the sequence must be constant, in particular $a_1(x)=a_0(x)$, ie. $\sqrt{3x}=x$, from which it follows that $x$ must be either $0$ or $3$. And indeed, it can be easily checked that the sequence is constant for those values. To my disappointment, I can't find a way to prove that sign of $a_n-a_{n-1}$ is same. It may be really wrong.","['radicals', 'sequences-and-series']"
2614316,Conjugate function of log-sum-exp?,"In Boyd's Convex Optimization book , Example 3.25 finds the conjugate function $f^*(y):=\sup_{x\in\text{dom}(f)}(y^Tx-f(x))$ of the log-sum-exp function $f(x):=\log(\sum_{i=1}^ne^{x_i})$. First, the gradient of $y^Tx-f(x)$ is taken to yield the condition: $$
y_i=\frac{e^{x_i}}{\sum_{j=1}^ne^{x_j}}\quad i=1,...,n
$$ where we see that a solution for $y$ exists if and only if $y\succ 0$ and $\textbf{1}^Ty=1$. Then the book simply says: By substituting the expression for $y_i$ into $y^Tx-f(x)$ we obtain $f^*(y)=\sum_{i=1}^ny_i\log(y_i)$. So far I've been unsuccessful in deriving this. How does one proceed? All I see is: $$
y^Tx-f(x)=\sum_{i=1}^ny_ix_i-\log(\sum_{i=1}^ne^{x_i})=\frac{\sum_{i=1}^nx_ie^{x_i}}{\sum_{j=1}^ne^{x_j}}-\log(\sum_{i=1}^ne^{x_i})
$$ But from here on I do not knonw how to proceed.","['linear-algebra', 'proof-verification', 'convex-analysis']"
2614326,Convergence of $\sum_{n=1}^{\infty}\frac{\mu(n)\chi(n)}{n^s}$ on $\Re{s}=1$,"Let $\chi$ be a dirichlet character mod $q$ . Does the dirichlet series for $$\frac{1}{L(s,\chi)}=\sum_{n=1}^{\infty}\frac{\mu(n)\chi(n)}{n^s}$$ converge for any $s$ on the line $\Re(s)=1$ and is this value equal to $\frac{1}{L(s,\chi)}$ ? I can see that it does converge for the principle character using an estimate for $M(x)$ but still dont know whether this value is $\frac{1}{L(s,\chi_0)}$","['number-theory', 'complex-analysis', 'analytic-number-theory', 'dirichlet-series']"
2614330,Explain a proof about the norm of derivation of operators,"Let $E$ be a complex Hilbert space. Let $A\in \mathcal{L}(E)$. Consider
\begin{eqnarray*}
W_{0}(A)
&=&\{\alpha\in \mathbb{C}:\;\exists\,(z_n)\subset E\;\;\hbox{such that}\;\|z_n\|=1,\displaystyle\lim_{n\rightarrow+\infty}\langle A z_n,z_n\rangle=\alpha,\\
&&\phantom{++++++++++}\;\hbox{and}\;\displaystyle\lim_{n\rightarrow+\infty}\|Az_n\|= \|A\| \}.
\end{eqnarray*} If $0\notin W_{0}(A)$,  by the transformation $Ae^{i\alpha}$ of $A$, why we can suppose that $\Re e( W_{0}(A))\geq\tau>0$. Notice that this result figures in the proof of THEOREM $2$. ( 1 )","['functional-analysis', 'operator-theory', 'hilbert-spaces']"
2614345,The sequence $\{X_n\}$ obeys weak law of large numbers if,"Let $\{X_n\}$ is a sequence of independent random variables with  $P\{X_n=\pm n^\alpha\}=\frac{1}{2}$, $n=1,2,\cdots$. The sequence $\{X_n\}$ obeys weak law of large numbers if 1) $\alpha <1/2$ 2) $\alpha =1/2$ 3) $1/2<\alpha \leq 1$ 4) $\alpha >1$ I find $E(S_n)=0, Var(X_n)=n^{2 \alpha}$ and I have to show $\frac{E(S_n^2)}{n^2}$ tend to zero as $n \rightarrow \infty$ for satisfying WLLN. So $\frac{E(S_n^2)}{n^2}=\frac{Var(S_n)}{n^2}=\frac{1}{n^2}\sum_{i=1} ^n Var(X_i)$ after that how to handle to find the condition on $\alpha$, Please help.","['law-of-large-numbers', 'statistics', 'probability']"
2614371,Number of possibilities to choose class representatives,"Question : A school has 4 classes. Each class consists of 25 students. The school needs to choose 10 class representatives, in which there is a minimum of one student from each class. How many ways there are to do so? I get why the final answer, using inclusion/exclusion, will be: $${{100}\choose{10}} - {{4}\choose{3}}{{75}\choose{10}} + {{4}\choose{2}}{{50}\choose{10}} - {{4}\choose{1}}{{25}\choose{10}}$$ But, why couldn't I do something like the following: First, I need to assure that there 4 students, each representing a separate class. In total, there are $25^{4}$ ways of doing so. After that, I can choose the 6 available slots for the class representatives from the $100-4 = 96$ students left, meaning that there are ${96}\choose{6}$ ways of doing so. In total, from what I just described, where would be a total of: $${25^{4}}*{{96}\choose{6}}$$ possibilities for the selection. Conceptually I must be getting something wrong. What am I over-counting/under-counting?","['combinatorics', 'discrete-mathematics']"
2614401,"Prove that $\forall k>0,\ \int_0^{+\infty}\frac{\sin x}{x+k}dx<\frac1k$.","Let $k>0$. Try1:
\begin{align}
\int_0^{+\infty}\frac{\sin x}{x+k}dx&=\int_k^{+\infty}\frac{\sin (t-k)}{t}dt\tag{$t=x+k$}\\
&=\int_k^{+\infty}\frac{\sin t\cos k-\sin k\cos t}{t}dt\\
&=\cos k\int_k^{+\infty}\frac{\sin t}{t}dt-\sin k\int_k^{+\infty}\frac{\cos t}{t}dt\\
&=\cdots
\end{align}
Maybe then we can study this integral as a function of $k$ with the help of $\operatorname{Si}$ and $\operatorname{Ci}$? I gave up. Try2: We define
$$f:z\mapsto\frac{e^{iz}}{z+k}.$$
Let $R>0$, we have
$$0=\int_0^{R}\frac{e^{ix}}{x+k}dx+\int_0^{\frac{\pi}2}\frac{e^{iRe^{i\theta}}}{Re^{i\theta}+k}iRe^{i\theta}d\theta-\int_0^{R}\frac{ie^{-x}}{ix+k}dx.$$
For all $\theta\in(0,\frac{\pi}2)$,
\begin{align}
\left|\frac{e^{iRe^{i\theta}}}{Re^{i\theta}+k}iRe^{i\theta}\right|&=\left|\frac{Re^{iR\cos\theta-R\sin\theta}}{Re^{i\theta}+k}\right|\\
&=\left|\frac{Re^{-R\sin\theta}}{Re^{i\theta}+k}\right|\\
&\underset{R\to+\infty}{\to}0,
\end{align}
thus,
$$
\int_0^{+\infty}\frac{e^{ix}}{x+k}dx=\int_0^{+\infty}\frac{ie^{-x}}{ix+k}dx=\int_0^{+\infty}\frac{e^{-x}(x+ik)}{x^2+k^2}dx.
$$
Finally,
\begin{align}
\int_0^{+\infty}\frac{\sin x}{x+k}dx&=\int_0^{+\infty}\frac{ke^{-x}}{x^2+k^2}dx\\
&<\frac1k\int_0^{+\infty}e^{-x}dx\\
&=\frac1k
\end{align} So here are my questions: Is there a simpler/smarter proof (without using the residue theorem)? Can we express the integral as a series of $k$ (distinguishing cases of $k<1$ and $k>1$)? A wiki page which may help.","['trigonometric-integrals', 'integration', 'sequences-and-series', 'analysis']"
2614425,"Evaluating $\iint xy\,\mathrm{d}x\,\mathrm{d}y$ over an elliptical disk","Evaluate $$I=\iint_A xy\,\mathrm{d}x\,\mathrm{d}y$$ where $$A=\{(x,y): ax^2+2hxy+by^2\le r^2, a>0, ab-h^2>0\}$$ I tried diagonalising the quadratic form $h(x,y)=ax^2+2hxy+by^2$ to some $$g(u,v)=u^2+v^2$$ where $$\begin{pmatrix}u\\v\end{pmatrix}=\begin{pmatrix} \sqrt{a} &  \frac{h}{\sqrt{a}}\\\ 0 &  \sqrt{\frac{ab-h^2}{a}} \\\end{pmatrix}\begin{pmatrix}x\\y\end{pmatrix}$$ The Jacobian of the transformation $(x,y)\to(u,v)$ is $$\det(J) = \frac{1}{\sqrt{ab-h^2}}$$ The integral then reduces to $$I=\iint_{u^2+v^2\le r^2}\left(\frac{uv}{\sqrt{ab-h^2}}-\frac{hv^2}{ab-h^2}\right)\frac{1}{\sqrt{ab-h^2}}\,\mathrm{d}u\,\mathrm{d}v$$ Now I could apply a polar transformation to get the answer. But this method seems a bit cumbersome, especially solving for $x$ and $y$ from $u,v$. Is there an obvious alternate solution? The problem can also be seen as finding $\mathbb{E}(XY)$ where $(X,Y)$ is jointly uniform over $A$. EDIT. Proceeding in the way I have done so far, I get the final answer as $$I=-\frac{\pi r^4h}{4(ab-h^2)^{3/2}}$$ Is this indeed the correct answer?","['multivariable-calculus', 'integration', 'definite-integrals']"
2614442,Proving the error estimate of Simpson's rule using the Euler–Maclaurin sum formula,"I am trying to prove the following lemma for the Simpson's rule. Suppose $g\in C^4([-1,1],\mathbb{R})$. Then $$\bigg|\int_{-1}^1g(x)dx-\frac{g(-1)+4g(0)+g(1)}{3}\bigg|\leq\frac{1}{90}\|g^{(4)}\|_\infty.$$ Here $\|\!\cdot\!\|_\infty$ is the supremum norm defined by $\|f\|_\infty:=\sup_{x\in[-1,1]}|f(x)|$. My attempt : Applying the Euler–Maclaurin sum formula, we get $$\int_0^1g(x)dx-\frac{2g(0)+g(1)}{3}=\frac{g(1)-g(0)}{6}-\frac{g'(1)-g'(0)}{12}-\frac{1}{6}\int_0^1B_3(x)g^{(3)}(x)dx,$$ where $B_3(x):=x^3-3x^2/2+x/2$ is the third Bernoulli polynomial. Now integrate by parts to get $$\int_0^1B_3(x)g^{(3)}(x)dx=\frac{x^2(x-1)^2}{4}g^{(3)}(x)\bigg|_0^1-\int_0^1\frac{x^2(x-1)^2}{4}g^{(4)}(x)dx.$$ The first term equals $0$, while for the second we have, by the mean value theorem for integrals, $$\int_0^1\frac{x^2(x-1)^2}{4}g^{(4)}(x)dx=\bigg(\int_0^1\frac{x^2(x-1)^2}{4}dx\bigg)g^{(4)}(\xi)=\frac{1}{120}g^{(4)}(\xi)$$ for some $\xi\in[0,1]$. The problem is how to estimate the other two terms. If this approach were to work, then we must show that $$\bigg|\frac{g(1)-g(0)}{6}-\frac{g'(1)-g'(0)}{12}\bigg|\leq\frac{1}{240}\|g^{(4)}\|_\infty.$$ But I don't know how to relate these to the fourth derivative. I tried Lagrange's mean value theorem but got stuck. How should I proceed? This appears as an exercise VI.6.7. from Analysis II by Amann and Escher, where the whole section is devoted to applications of the Euler–Maclaurin sum formula. So if you have other proofs, please make use of this formula. Thanks in advance!","['numerical-methods', 'real-analysis', 'analysis']"
2614458,Proving that a set of 2016 natural numbers contain a non-empty set with a sum divisible by 2016 [duplicate],"This question already has answers here : Consider a set $A$ of $10$ integers, $A = \{a_1,a_2, \ldots ,a_{10}\}$. Prove that there is at least one subset of $A$ whose sum is divisible by $10$. (2 answers) Closed 6 years ago . Prove: Each set which consists of 2016 natural numbers contains a non-empty set that has a sum divisible by 2016 My attempt: Use the pigeonhole principle. Let $A$ be a set of 2016 natural numbers. I'll split the answer to two different possibilities: $A$ contains at least one number which is divisible by 2016. In this case the claim is true. $A$ does not contain numbers that are divisible by 2016. In this case, we'll apply the pigeonhole principle. Holes : $\{1,2,3,4,...,2016\}$, A set of all possible remainders when dividing a number (which is not divisible by 2016) by 2016. In total there are 2015 holes. Pigeons : 2016 numbers. Each number will be divided by 2016 and put into the appropriate hole according to the that number's division remainder. By the pigeonhole principle, at least in one hole will be $\lceil \frac{2016}{2015} \rceil$. objects. In this case, the claim is also true. Is my proof sufficient? Can I prove this not using 2 different possibilities, or maybe not using the pigeonhole principle? Edit : As stated in the comments and in the answers, my proof is false. The proof shows that there exists at least 2 numbers in the set $A$ with a difference divisible by 2016. It does not prove that there is a non-empty set with a sum divisible by 2016.","['pigeonhole-principle', 'discrete-mathematics']"
2614515,"$\forall\epsilon>0, \exists N\in\Bbb N$ such that $\forall n>m\ge N, a_n-a_m<\epsilon$. Prove that $a_n$ converges to a real limit or to $-\infty$","Assume $(a_n)_{n=1}^\infty$ has the following property: For all $\epsilon>0$ exists some $N\in\Bbb N$ such that for every $n>m\ge N$, $a_n-a_m<\epsilon$. We want to prove that $a_n$ converges to a real limit or to $-\infty$. This condition implies that $a_n$ is bounded above.
So, I tried to show what happens if it's bounded below or unbounded below.
When bounded below, by Bolzano-Weierstrass $a_n$ has some convergent subsequence, $a_{n_k} \rightarrow L$, so for all $k\ge K$ for some $K\in \Bbb N$, $|a_{n_k}-L|<\epsilon \iff -\epsilon<a_{n_k}-L<\epsilon$ . I tried to use this to prove the convergence of the entire sequence but it didn't work -  I tried to take some $n>n_k>max(N, n_K)$, $a_n-L=a_n-a_{n_k}+a_{n_k}-L <2\epsilon$, but now I can't get the left inequality right. This makes me question my entire process. Maybe I should've gone about this a different way?","['sequences-and-series', 'calculus', 'limits']"
2614527,Hensel Lemma and cyclotomic polynomial,"I'm trying to prove the following equivalence Let $p\neq 3$, then $f(X)=X^3-1$ splits completely in $\mathbb{Z}_p$ ($p$-adic integers) iff $p\equiv1 \bmod 3$. This is my attempt: first I noticed that $\mathbb{Z}/p\mathbb{Z}$ contains a primitive 3rd root of unity iff $p\equiv1\bmod3$. Then if $p\equiv1\bmod3$ I have 3 different roots $x_1,x_2,x_3$ of $f(X)$ in $\mathbb{Z}/p\mathbb{Z}$ s.t. $f'(x_i)\neq0$ and by Hensel Lemma I can lift them to 3 different roots in $\mathbb{Z}_p$, so $f(X)$ splits completely. I have a doubt on the reverse implication: if $f(X)$ splits completely in $\mathbb{Z}_p$ then it splits completely in $\mathbb{Z}/p\mathbb{Z}$. I want to deduce that $\mathbb{Z}/p\mathbb{Z}$ contains a primitive root of unity and I think this is true since if $p\neq 3$ then $f(X)\neq (X-1)^3$ and so if it splits is has 3 different roots. Is it true?","['number-theory', 'hensels-lemma', 'p-adic-number-theory', 'commutative-algebra']"
2614545,Self-convolution of $f(\vec{r}) = e^{-x^2-y^2}/r^2$,"I wonder if the self-convolution of
$$ f(\vec{r}) = \frac{e^{-(x^2+y^2)}}{r^2}, $$
(where $\vec{r} = (x,y,z)$, and $r^2 = x^2+y^2+z^2$),
$$ (f*f)(\vec{r}') = \iiint_{-\infty}^\infty d^3\vec{r}\, f(\vec{r}) f(\vec{r}'-\vec{r}), $$
can be evaluated or simplified in any way? Thanks. Edit: Using the identity
$$ \frac{1}{A} = \int_0^\infty d\nu\, e^{-A\nu} \text{ for } A>0, $$
the integration dimension can be decreased by 1:
$$
\begin{align*}
\iiint_{-\infty}^\infty d^3\vec{r}\, f(\vec{r}) f(\vec{r}'-\vec{r}) 
&= \iiint_{-\infty}^\infty d^3\vec{r}\, \frac{e^{-x^2-y^2}}{r^2} \frac{e^{-(x'-x)^2-(y'-y)^2}}{|\vec{r}' - \vec{r}|^2} \\
&= \iint_0^\infty d^2\vec{\nu}\iiint_{-\infty}^\infty d^3\vec{r}\, e^{-x^2-y^2-(x'-x)^2-(y'-y)^2- r^2\nu_1 - |\vec{r}' - \vec{r}|^2 \nu_2} \\
&= \pi^{3/2} \iint_0^\infty d^2\vec{\nu} \frac{\exp \left(-\frac{(\nu_1+1) (\nu_2+1) (x'^2+y'^2)}{\nu_1+\nu_2+2}-\frac{\nu_1 \nu_2 z'^2}{\nu_1+\nu_2}\right)}{(\nu_1+\nu_2+2)\sqrt{\nu_1+\nu_2}}.
\end{align*}
$$ Can this integral be further reduced?","['multivariable-calculus', 'improper-integrals', 'convolution', 'fourier-analysis']"
2614566,Prove that the projection from a square to a torus is not open,"I'm stuck with this exercises of my notes: Let's consider a square $Q=[0,1]\times[0,1]\subset \mathbb{R^2}$, with the usual topology. Let's $p: Q \to T$ be the canonical projection on the torus $T$. Prove that $p$ is not open. I think that maybe I'm not correct about my way of thinking on the ""canonical projection"", because if I think that the torus is constructed identifying the sides of that square and doing continuously deformations to it (see the image below), every open set on $[0,1]\times[0,1]$ leads me to an open set on $T$. What am I doing wrong? Thanks for your time.","['algebraic-topology', 'abstract-algebra', 'general-topology']"
2614598,When is the pointwise convergence on $A$ equivalent to the uniform one?,"Let $A$ be a finite set of $[0,1]$ and $F$ a sub-vector-space of $C([0,1],\mathbb R^n)$. When is the pointwise convergence on $A$ equivalent to the uniform one. And with $A$ be countable? I suppose the finite dimension of $F$ is sufficient but is it necessary? Not sure for the second case. Is there a more general condition to have this equivalence (with any vector-space)?","['uniform-convergence', 'functional-analysis', 'general-topology', 'convergence-divergence', 'vector-spaces']"
2614612,Continuous group homomorphism between normed vector spaces are linear?,"The following is exercise III.1.13 from Analysis I by Amann and Escher. Suppose that $V$ and $W$ are normed vector spaces and $f : V → W$ is a continuous
  group homomorphism from $(V, +)$ to $(W, +)$. Prove that $f$ is linear. (Hint: If $\mathbb{K = R}$, $x \in V$ and $q ∈ \mathbb{Q}$, then $f(qx) = qf(x)$. See also Exercise $6$.) Here $\mathbb{K}$ means the base field for $V$ and $W$ and $\mathbb{K=C}$ or $\mathbb{R}$ (which is a convention stated in the text). Now if $\mathbb{K=C}$ then by the hint we can only show that $f(a+bi)=af(1)+bf(i)$ for all $a,b\in\mathbb{R}$, but I think $f(i)$ need not equal $if(1)$, and so the result is false. For instance, take $V=W=\mathbb{C}$, regarded as vector spaces over $\mathbb{C}$ and equipped with the usual Euclidean norm. Let $f$ be the conjugation, i.e., $f:a+bi\mapsto a-bi$. I do think this is a continuous group homomorphism, but it isn't linear. Am I right?","['normed-spaces', 'group-homomorphism', 'continuity', 'linear-transformations', 'analysis']"
2614626,Convergence of the sequence $ \sqrt {n-2\sqrt n} - \sqrt n $,"Here's my attempt at proving it: Given the sequence $$ a_n =\left( \sqrt {n-2\sqrt n} - \sqrt n\right)_{n\geq1} $$ To get rid of the square root in the numerator: \begin{align}
\frac {\sqrt {n-2\sqrt n} - \sqrt n} 1 \cdot \frac {\sqrt {n-2\sqrt n} + \sqrt n}{\sqrt {n-2\sqrt n} + \sqrt n} &= \frac { {n-2\sqrt n} - \ n}{\sqrt {n-2\sqrt n} + \sqrt n} = \frac { {-2\sqrt n}}{\sqrt {n-2\sqrt n} + \sqrt n} \\&= \frac { {-2}}{\frac {\sqrt {n-2\sqrt n}} {\sqrt n} + 1}
\end{align} By using the limit laws it should converge against: $$
\frac { \lim_{x \to \infty} -2 } { \lim_{x \to \infty} \frac {\sqrt {n-2\sqrt n}}{\sqrt n}  ~~+~~\lim_{x \to \infty} 1}
$$ So now we have to figure out what $\frac {\sqrt {n-2\sqrt n}}{\sqrt n}$ converges against: $$
\frac {\sqrt {n-2\sqrt n}}{\sqrt n} \leftrightarrow \frac { {n-2\sqrt n}}{ n} = \frac {1-\frac{2\sqrt n}{n}}{1}
$$ ${\frac{2\sqrt n}{n}}$ converges to $0$ since: $$
2\sqrt n = \sqrt n + \sqrt n \leq \sqrt n ~\cdot ~  \sqrt n = n
$$ Therefore $~\lim_{n\to \infty} a_n = -1$ Is this correct and sufficient enough?","['real-analysis', 'convergence-divergence', 'analysis', 'limits']"
2614630,"Prove or disprove: $\exists x \forall y \,\,\varphi \models \forall y \exists x \,\ \varphi$","Prove or disprove: $\exists x \forall y \,\,\varphi \models \forall y
\exists x \,\ \varphi$ where $\varphi$ is a first-order-logic formula About notation: I call LHS as $A$ and RHS as $B$. Then $A \models B$ means that $B$ is true in every structure in wich $A$ is true. Now question is if this is the case here, with proof. Let's say we have $\varphi : = R(x,y)$ where $R$ stands for relation. Then LHS says that For some $x$, all $y$ are such that $R(x,y)$ RHS says that For any $y$, there is a $x$ such that $R(x,y)$ Now if we expand those two sentences, we have for LHS: There exists a $x \in \mathbb{N}$, such that for any number $y$, we have that $x
>y$. This is wrong because there can't be number that is greater than
  all numbers. For RHS we have: For any $y \in \mathbb{N}$, there exists a number $x$ such that $x >
y$. This is correct because for any number there is always a bigger
  number. For this reason we have no model here and this means the statement is false. I like to know if this is correct pls because I need it for exam and I would do it like that in exam if they ask similar question?","['first-order-logic', 'model-theory', 'quantifiers', 'logic', 'discrete-mathematics']"
2614636,"If $f \in C([a, b],\mathbb{R})$ is differentiable on $(a,b]$ and $\lim_{x\to a}f'(x)$ exists, then $f\in C^1([a, b],\mathbb{R})$?","The following is exercise IV.2.3. from Analysis I by Amann and Escher. Let $-\infty < a < b < \infty$ and $f \in C([a, b],\mathbb{R})$ be differentiable on $(a, b]$. Show that, if $\lim_{x\to a} f'(x)$ exists, then $f$ is in $C^1([a, b],\mathbb{R})$ and $f'(a) = \lim_{x\to a} f'(x)$. ( Hint : Use the mean value theorem.) I think this is false and found a counterexample: Set $b=0$ and define $f:[a,0]\to\mathbb{R}$ by
$$
f(x):=
\begin{cases}
x^2\sin(1/x),&x\in[-a,0);\\
0,&x=0.
\end{cases}
$$
The function $f$ satifies all the hypotheses but $f'$ is not continuous at $x=0$. Am I right? Edit: My point is that $\lim_{x\to a} f'(x)$ does not imply $f\in C^1([a, b],\mathbb{R})$, or, roughly speaking, the continuity of $f'$ at $x=a$ does not imply its continuity on the whole interval. However, the second conclusion, $f'(a) = \lim_{x\to a} f'(x)$, is correct.","['real-analysis', 'analysis']"
2614646,I cannot find boundaries in Divergence and Stokes's Theorems,"I have two questions from my exam. I'm sorry for not remembering what was F. 1) $F$ is 3-dimensional vector field. $\delta$ is a surface which is bounded by $z=x^2+y^2-3$ and $z=1$ and oriented in negative $z$ $$ \iint_{\delta}curlF.\hat N.ds $$ Should I do $z=x^2+y^2-3 = 1$ $\Rightarrow$ $z=x^2+y^2=4$ and use polar coordinates? And I know  we can write $\hat N.ds$ = $\pm \frac{\nabla G}{G_3}dxdy$ when $G(x,y,z)=0$ is equation of surface with (1-1) projection onto a domain in the $xy$-plane. How can I decide what is $\hat N.ds$ when we have two or more equations like $x^2+y^2+z^2=a^2$ and $x+y+z=0$ 2) It was more confusing me. I have no idea how can I determine boundaries. I have found $divF=3y$ $$ \iiint_D 3ydV $$ Equations : $z=1-x^2$ parabolic cylinder and $z=0$, $y=0$, $y+z=2$ What should I say for boundaries? As you can see I have some problems about these theorems. Can someone suggest some tutorials or books etc for catching them pratically? Thanks","['multivariable-calculus', 'integration', 'calculus']"
2614656,Understanding the density function and expected value of an estimator.,"Suppose that we have a sample $X_1,\ldots,X_n$ from the distribution with density function $$f(x\mid\theta) = \dfrac{2x}{\theta^2}\mathbb{1}_{(0,\theta)}(x)$$ w.r.t. the Lebesgue measure. I recently learned that if we use $T(X) =\max(X_1,\ldots,X_n)$ as an estimator for $\theta$, the density function of $T(X)$ equals $$f_n(x\mid\theta) = n(F(x\mid\theta))^{n-1}f(x\mid\theta) = \dfrac{2n}{\theta^{2n}}x^{n-1}\mathbb{1}_{(0,\theta)}(x)$$ and thus that $$\operatorname{E}_\theta(T(X)) = \displaystyle\int_0^\theta x\dfrac{2n}{\theta^{2n}}x^{n-1}\mathbb{1}_{(0,\theta)}(x) \, dx.$$ What I don't understand is when the density function of an estimator differs from the density function of the data. In my book the following example is given: Suppose $X\sim\operatorname{Geom}(\theta)$, with $\theta\in(0,1)$. We seek for an unbiased estimator for $\theta$. We have that $$\operatorname{E}_\theta d(X) = \sum\limits_{i = 1}^\infty d(i)\theta(1-\theta)^{i-1}$$ where $d(X)$ is an estimator for $\theta$. Here the density function of the data is used to calculate the expected value of the estimator. Why is this case different to the case with the order statistic? This computation of the expected value would surely be wrong if $d(X)$ equals the order statistic right? Question: When calculating the expected value of an estimator, how do you determine which density function you should use? Is the order statistic a special case? I would really like to know what the underlying theory behind all this is, because now it comes across as quite random. Thanks in advance!","['statistical-inference', 'probability-theory', 'probability-distributions', 'statistics', 'probability']"
2614657,Only bijective mappings are invertible. Clarifying proof.,"Suppose that $f : A \mapsto B$ is invertible with inverse $g : B \mapsto A$. Then $g \circ f = \operatorname{id}_A$ which means $\forall a \in A : g(f(a)) = a$. Let's take some $a_1, a_2 \in A$ with $f(a_1) = f(a_2)$; then $a_1 = g(f(a_1)) = g(f(a_2)) = a_2$. The thing which I can't get is $g(f(a_1)) = g(f(a_2))$ equality. Where it comes form? Clearly, $a_1 = g(f(a_1))$ and $a_2 = g(f(a_2))$, but why it implies $a_1 = a_2$? Why can't they be non-equal, i.e. different, but mapped to the same $b = f(a_1) = f(a_2)$? The only assumption I came up with is that having $b = f(a_1) = f(a_2)$ seems to violate mapping requirement for $g : B \mapsto A$, since it ""maps"" exactly the same $b$ twice: $(b, a_1)$ and $(b, a_2)$... nevertheless, still having a feeling like I do miss something important. Do I?","['elementary-set-theory', 'inverse-function', 'functions', 'proof-explanation']"
2614704,Equivalent definition of a parabola as a locus of points given its focus and vertex,"Consider the following alternative definition of a parabola: Given two points $F$ and $O$ in the plane, the parabola having focus $F$ and vertex $O$ is the locus of points $P$ of the plane such that
  $$(FP - OF)(FP + 3 OF) = OP^2.$$ Using coordinates it is easy to see that the definition is equivalent to the usual one. Indeed, if we let $O = (0, 0)$, $F = (0, f)$ for some $f > 0$ and $P = (x, y)$, then the given equation simplifies to $x^2 = 4 f y$, which is precisely the equation of the parabola having focus $F$ and vertex $O$ as it is usually defined. What I am interested in is a geometric proof that any parabola satisfies the above property, which should hopefully give some insight on why such an equality must hold. I have attempted to prove it in two ways: As it is written, the equality seems to say that a certain rectangle (or maybe parallelogram?) has the same area as the square on the line segment $OP$. I have noticed that $FP - OF$ is the distance from $P$ to the tangent line to the parabola at $O$, but I don't know what to do with $FP + 3 OF$. The equality can be rewritten as
$$OP^2 + (2 OF)^2 = (FP + OF)^2.$$
Now it looks as though it could be proven using the Pythagorean theorem. But I haven't been able to draw a triangle having sides $OP$, $2OF$ and $FP+OF$ so that it can be seen that it is indeed a right triangle. Any help would be highly appreciated. ( Background : this problem came up while trying to prove a similar property about the cissoid of Diocles, see this other question of mine. The two properties are related through inversion with respect to the unit circle centered at $O$.)","['locus', 'conic-sections', 'geometry']"
2614758,"Sparse sets $S$ of integers such that $\frac{1}{x^2}\sum_{s\in S, s\le x}s\to 0$","Let $A$ be a set of positive integers such that
$$
\lim_{x\to \infty}\frac{1}{x^2}\sum_{a\in A \cap [1,x]}a=0.
$$
Fix now a set $B$ of positive integers such that $|B\cap [1,x]| \le |A \cap [1,x]|$ for all $x\ge 1$. Question. Is it true that
  $$
\lim_{x\to \infty}\frac{1}{x^2}\sum_{b \in B \cap [1,x]}b=0\,\,\,?
$$","['discrete-mathematics', 'real-analysis', 'integers', 'limits']"
2614808,"Prove that if all of the roots of a polynomial $P(z)$ are real, then for any $b$ the roots of the polynomial $R(z)=P(z+ib)+P(z-ib)$ are also real.","Prove that if all of the roots of a polynomial $P(z)$ are real, then for any $b$ the roots of the polynomial $R(z)=P(z+ib)+P(z-ib)$ are also real. I'm just trying take the polynomial $$P(z)=a_0+a_1z+\dotsb+a_nz^n$$ and look at $$P(z+bi)=a_0+a_1(z+bi)+\dotsb+a_n(z+bi)^n$$ and $$P(z-bi)=a_0+a_1(z-bi)+\dotsb+a_n(z-bi)^n$$ and then considering $$P(z+ib)+P(z-ib)=2a_0+2a_1z+2a_2(z^2-b^2)+\dotsb$$ we can see that every term hasn't the imaginary part, i.e all of $i$ are missing and we get new polynomial $G$ which have coefficients as in a polynomial $P(z)$ multiplying on $b$ . But how to explain that all of the roots of $R(z)$ are real?","['complex-analysis', 'roots', 'polynomials']"
2614828,Continuous functions whose Riemann sums are zero,"here is my question : Let's denote $F$ the following set 
$$
F=\left\{ f \in \mathcal{C}^0([0,1],\mathbb{C}) / \forall n \in \mathbb{N}^*, \sum_{k=0}^{n-1}f \left( \dfrac{k}{n} \right) =0 \right\}.
$$
$F$ contains the line spanned by $f_0 : x \in [0,1] \mapsto \sin (2 \pi x)$ (but not $\cos$ because of $n=1$), but is it bigger ?
I hope not, but I may be wrong.
Thanks in advance.","['real-analysis', 'calculus', 'analysis']"
2614845,Can there be a magic square with equal diagonal sums different from equal row and column sums?,"I got a task in programming a program that can detect whether a 4x4 square is a magic square or not. At first, I wrote code that met the requirements for all given examples but I noticed one flaw. I used 2 variables to indicate sums. I used them once to calculate the sums of the rows and columns and compare them, then I reset them back to 0 and used them to calculate the diagonal sums and check if they were equal. The thing was, is that I did not actually compare the diagonal sums to the original row and column sums, and that got me thinking. Can there exist a ""magic square"" where the diagonal sums are equal and the row and column sums are equal, but the diagonal sums are different from the row and column sums? Is there any actual way to prove this? I tried to come up with examples but nothing came to me. An example would disprove this and make me rewrite my code. For simplicity, I would rather know about a 4x4 square, but if you can I'll be happy to hear a proof for any $n$ x $n$ square. Thanks in advance. Edit: I already check to see if the integers are all different, so I'd rather know if one exists where all of the integers are different.","['recreational-mathematics', 'magic-square', 'discrete-mathematics']"
2614886,"$f\in K[X]$ irreducible odd degree, $\alpha, \beta\in\bar K$ distinct roots of $f$ $\Rightarrow$ neither $\alpha+\beta\in K$ nor $\alpha\beta\in K$","My task is to show following: Let $ f \in K[X]$ irreducible with odd degree. If $\alpha , \beta \in \bar K$ are distinct roots of $f$, where $ \bar K$ is an algebraic closure of $K$, neither $\alpha + \beta \in K$ nor $ \alpha \beta \in K$. I have no idea, how to go on. Some hints or (partial) solutions would be really nice.","['irreducible-polynomials', 'galois-theory', 'abstract-algebra', 'extension-field', 'field-theory']"
2614921,Bounded projection,"I have a problem proving the following: Consider a banach space $X$. A projection is bounded if and only if $\mathrm{Im} (P)= \{ Px : x \in X\}$ nd $\mathrm{Ker}(P) = \{x \in X: Px =0\}$ are closed subspaces of $X$. So we know that a linear operator,say P is a projection if $P^2=P$. 
Assume first that the projection is bounded. Then $\exists M$  s.t $||Px||\leq M||x||$ for all $x \in X$. Then, since we know that $X$ is a Banach space, take a sequence say $\{x_n\}_n$ then we know that it is convergent. Say x is the limit. Apply P on $\{x_n\}_n$. Assume $\{Px_n\}$ sonverges to y. Then $x_n - Px_n = (I-P)x_n$ converges to $x - y$. It follows that $y \in \mathrm{Im}(P)$ and $x-y \in \mathrm{Ker}(P)$, so $y = Py = Px$. Hence the conclusion follows from the closed graph theorem. Is it true what I wrote? I mean does it really proof the theorem? How to proceed next?","['functional-analysis', 'projection', 'operator-theory']"
2614940,"Prove that $\mathbb{Q}[\sqrt[4](7), i] = \mathbb{Q}[\sqrt[4](7)+ i]$","I want to prove that $\mathbb{Q}[\sqrt[4](7), i] = \mathbb{Q}[\sqrt[4](7)+ i]$. Of course $""\supseteq""$ is clear. Do you have any hints how I can show that $\sqrt[4](7), i \in \mathbb{Q}[\sqrt[4](7)+ i]$","['abstract-algebra', 'galois-theory', 'extension-field']"
2614955,Which Matrix is an Inner Product,"We define the inner product of square matrices to be $\langle\vec x, \vec y\rangle_A=\vec x^TA\vec y$. One of the matrices $$\begin{bmatrix}
1 & 2 \\
2 & 1 \\
\end{bmatrix}$$ $$\begin{bmatrix}
2 & 1 \\
1 & 2 \\
\end{bmatrix}$$ violates the requirement $\langle\vec x, \vec x\rangle_A > 0$ for $\vec x \neq \vec 0$. I don't really understand the inner product in general, and I am really struggling to understand and prove which matrix isn't an inner product. Any help would be greatly appreciated.","['matrices', 'linear-algebra', 'inner-products']"
2614960,A question related to Kolmogorov's $0$-$1$ law,Let $\{X_n\}_{n=1}^\infty$ be a sequence of mutually independent random variables such that for each $n$ there exists a finite set $F_n$ such that $\mathbb{P}(X_n\in F_n)=1$; $S=\lim_{n\rightarrow\infty}\sum_{i=1}^n X_i$ exists and is finite almost surely. Prove that either there is a countable set $A$ such that $\mathbb{P}(S\in A)=1$ or there is no $\alpha\in\mathbb{R}$ such that $\mathbb{P}(S=\alpha)>0$. It seems that we need to use Kolmogorov's $0$-$1$ law to solve above problem but I have no idea how to construct corresponding tail $\sigma$-algebra. Any help is appreciated.,"['real-analysis', 'probability', 'measure-theory']"
2614969,Estimating the limit $x_{n+1} =x_n - x_{n}^{n+1} $,"I wonder whether there is a general method for accurately estimating the limit of the sequence: \begin{equation}
x_{n+1} = x_n - x_{n}^{n+1}, \forall x_1 \in (0,1)
\end{equation} After showing that the limit exists, since $ x_n $ is decreasing and bounded, I managed to derive a lower-bound. In particular, I used the fact that: \begin{equation}
\frac{x_{n+1}}{x_n} = 1-x_{n}^n \tag{1}
\end{equation} Using $(1)$ we obtain: \begin{equation}
\frac{x_N}{x_{N-1}}...\frac{x_2}{x_1}=\prod_{n=1}^{N} (1-x_{n}^n)=\frac{x_N}{x_1} \tag{2}
\end{equation} From this we deduce: \begin{equation}
\begin{split}
\lim_{N \to \infty} x_N & = \lim_{N \to \infty}x_1 \prod_{n=1}^{N} (1-x_{n}^n) \\  & = x_1 (\lim_{N \to \infty} \prod_{n=1}^{N} e^{\ln (1-x_{n}^n)}) \\ & = x_1 (\lim_{N \to \infty} e^{\sum_{n=1}^N\ln (1-x_{n}^n)})
\end{split}
\tag{3}\end{equation} Using the following facts: \begin{cases}
\sum_{n=1}^{N} \ln(1-x_{n}^n) \geq \sum_{n=1}^{N} \ln(1-x_{1}^n),\\
x \approx 0 \implies \ln(1+x) \approx x \\
\tag{4}\end{cases} We may deduce that for $M$ sufficiently large: \begin{equation}
\sum_{n=1}^{\infty}\ln (1-x_{n}^n) \geq \sum_{n=1}^{M} \ln(1-x_{1}^n)-\sum_{n=M}^\infty x_{1}^n \tag{5}
\end{equation} And using $(5)$ we have a useful lower-bound. However, I wonder whether there's a more direct integration technique which can give me a good approximation to $(3)$.","['real-analysis', 'integration']"
2615000,Stochastic decision problem with normal distribution,"Suppose the decision maker receives a piece of information (signal) $s=\theta+e$, where the true parameter $\theta$ and error $r$ are normally distributed, and makes decision $d\ge 0$ in order to maximize 
$$-Pr(\theta<T-d|s)W-d,$$
with $W>0$ and $T\in\mathbb{R}$ being scalars. Suppose the optimal decision is expressed as function $d(s)$:
$$d(s):=\arg\max_{d\ge 0} -Pr(\theta<T-d|s)W-d.$$
Then here is my problem: Is the term $s+d(s)$ always (i.e., for all $W,T$) strictly increasing, and how to show this? The problem is nasty because the optimization problem is not globally concave in $d$, so the first order condition does not always determine the optimum. What can I show? Well, if $W$ is small enough, then the corner solution is $d=0$ for all $s$, and then $s+d(s)=s$ is always invertible. However, if $W$ is large enough, then for some interval an interior solution $d(s)>0$ holds. I plotted an example of $d(s)$ in such a case: The downward sloping part is the interval where the first order condition holds
$$1=-\frac{\partial Pr(\theta<T-d|s)}{\partial d}W.$$
I was able to get an explicit expression for this interior $d(s)$:
$$d(s)=T-PDF^{-1}(1/W)$$
$$=T+\sqrt{-2\sigma^2 \log(1/W \sqrt{2\pi\sigma^2})}-\mu(s),$$
where $PDF^{-1}$ is the inverse of the left side of the normal probability density function, $\sigma^2$ is the variance and $\mu(s)$ is the mean of the posterior normal distribution which depends on information $s$. Because $s$ enters only in the mean, the slope of $d(s)$ is greater than $-1$, so $s+d(s)$ is always strictly increasing whenever $d(s)$ is determined by the first order condition. But perhaps it could still be that there is a discontinuity when $d(s)>0$ switches from the interior solution to a corner solution $d(s)=0$ (around $s=9$ in the plot) for some parameter values $W,T$, which would then introduce a downward jump in $s+d(s)$, making it non-monotone. This is obviously not the case in the plot, but can this happen, or if not, how to show it cannot happen (so that $s+d(s)$ really is always strictly increasing)? I would appreciate any help, either with another approach or by finishing that (hopefully) last step in my approach. Thanks!","['statistics', 'decision-theory', 'probability', 'optimization']"
2615012,Evaluating $\lim\limits_{x \to a} \frac{x^a - a^x}{x^a - a^a}$ without L'Hopital,"I'm trying to solve this limit without using L'Hopital rule. I already tried multiplying up and down by $x^a+a^a$, finding bounds for squeeze theorem, substitution of variables, but got nothing... $a$ is a positive real number different than $1$. Any help would be appreciated.","['derivatives', 'real-analysis', 'exponential-function', 'calculus', 'limits-without-lhopital']"
2615072,computing $A_2=\sum_{k=1}^{n}\frac{1}{(z_k-1)^2} $ and $\sum_{k=1}^n \cot^2\left( \frac{k\pi}{n+1}\right)$,"Assume that  $z_1,z_2,...,z_n$ are  roots of the equation $z^n+z^{n-1}+...+z+1=0$. I was asked  to compute the expressions 
  $$A_1=\sum_{k=1}^{n}\frac{1}{(z_k-1)} ~~~~~~and~~~~~~A_2=\sum_{k=1}^{n}\frac{1}{(z_k-1)^2} $$
  Then deduces  $$B_2=\sum_{k=1}^n \cot^2\left( \frac{k\pi}{n+1}\right)$$ I managed with $A_1$ and proved that $$A_1=\sum_{k=1}^{n}\frac{1}{(z_k-1)}=-\frac{n}{2}$$
I  used the fact that $$\frac{z^{n+1}-1}{z-1}=z^n+z^{n-1}+...+z+1$$ Actually I couldn't see an apparent link between  $A_1$,  $A_2$ and $B_2$. 
Can anyone help with $A_2$ and $B_2$. k","['complex-numbers', 'roots', 'algebra-precalculus', 'complex-analysis', 'summation']"
2615108,What is tricky about proving the Nielsen–Schreier theorem?,"The Nielsen–Schreier theorem states (in part): Let $F$ be a free group, and $H\le F$ be any subgroup. Then $H$ is isomorphic to a free group. I have seen the topological proof of this theorem using the correspondence between coverings and subgroups of the fundamental group. This has always struck me as using rather strong theory for what it is being used to prove (though I do appreciate the beauty of the argument). In my head, I see the following (loose) argument: Let $H$ be a subgroup of $F$, and assume that $H$ is not free. Then there exists some nontrivial relation $h_1h_2\dots h_n = 1$. But then this is also a nontrivial relation in $F$ implying that $F$ is not free, which is absurd. Thus $H$ is free. Clearly, there must be some problem with this. What are the stumbling blocks here? An issue I see is that the exact notation a relation has always seemed a little vague to me (some reduced word equal to the identity?), but that doesn't seem like it ought to be a large enough problem to invalidate the argument.","['free-groups', 'group-theory', 'proof-verification']"
2615158,"Mathematics for Computer Science, Problem 2.6. WOP","This is a problem from Mathematics for Computer Science book and the same MIT course . You are given a series of envelopes, respectively containing $1,2,4,...,2^m$ dollars. Define Property $m$: For any nonnegative integer less than $2^{m+1}$, there is a selection of envelopes whose contents add up to exactly that number of dollars. Use the Well Ordering Principle (WOP) to prove that Property $m$ holds for all nonnegative integers $m$. Hint : Consider two cases: first, when the target number of dollars is less than $2^m$ and second, when the target is at least $2^m$. I see how it this can be proved by induction but got really stuck with proving it using WOP. For example, I consider a case when $n<2^m$ . I look for contradiction => some number n which is not the sum of envelopes. 0,1,2 can be made from envelopes (ok, I have some doubts about $0$, but I ignore them for now) so $n>2$. Here I totally stuck:
if I try (n-1) = sum, then I can't find a way to show than { (n-1) +1 = sum of envelopes } or find any contradiction. Please, help!:)","['computer-science', 'discrete-mathematics']"
2615179,"Existence and uniqueness of limit cycle for $r' = μr(1-g(r^2)), \text{and} θ' = p(r^2)$","Exercise : Consider the dynamical system :
  $$r' = μr(1-g(r^2))$$
  $$θ' = p(r^2)$$
  where $μ>0$ is a constant and $g,p: \mathbb R_+ \to \mathbb R \space$ are smooth functions with $p(1) > 0$ and $g(1) = 1$. Show the existence of a limit cycle. Is the limit cycle unique ? Attempt : So, first of all, we have to check the sign of $r'$. Since $μ>0$, we observe that the points $r=0$ and $r=1$ are stationary points, since $g(1) = 1$. In previous cases, one would have to study the changes of the  sign of $r'$ depending on $r$ and would then fix an one-dimension phase portrait with arrows denoting the monotonic domains. If the arrows would ""converge"" $(\rightarrow \space \leftarrow$) around an equilibria, then this is enough to show that a unique cycle exists. In this particular case though, I am unable to conduct such a result, since we are not sure of the sign of the expression $(1-g(r^2))$ but we only know that it has a zero point at $r=1$. I'm pretty sure there's another way around, especially since we're given that $p(1) >0$ which I am not sure where to use yet (I guess it shows a clockwise ""movement"" of the phase portrait). I would appreciate any hints or help !","['dynamical-systems', 'polar-coordinates', 'stability-theory', 'stability-in-odes', 'ordinary-differential-equations']"
2615190,Derivative of $f \left( g^{-1} (x) \right)$ using only the first principle,I am trying to  find the derivative of $f \left( g^{-1} (x) \right)$ . I know that it is given by $$f \left( g^{-1} (x) \right)'=\frac{f' \left( g^{-1} (x) \right)}{ g' \left( g^{-1} (x) \right)}$$ using chain rule. But I am not able to prove it using first principle. How can I do that using the first principle ?,"['derivatives', 'real-analysis', 'calculus', 'functions', 'algebra-precalculus']"
2615198,Theorem 9.17 from Baby Rudin,"I'm a bit confused about the final step in the proof the the theorem below from Baby Rudin. This theorem tells precisely that the value of the linear transformation $f'(x)$ at $e_j$ is the column-vector in $R^m$ which is the partial derivative of the vector $f=(f_1,\dots,f_m)^t$ with respect to the $j^{th}$ variable. Now, doesn't Eq. $(28)$ already imply this claim? The left-hand side of this equation is precisely the directional derivative of $f$ at $x$ in the direction of $e_j$, which is precisely what we need. Do I miss something? Furthermore, it seems to me that Rudin's reference to Theorem 4.10 is imprecise. This theorem says the following: However, I don't understand how this theorem is used to conclude the existence of the limits of each quotient.","['real-analysis', 'calculus', 'analysis']"
2615236,Fixed points of simplicial maps,"I want to prove a statement about the fixed points of simplicial maps. If $f: |K|\to |K|$ is a simplicial map prove that the set of fixed points of $f$ is the polyhedron of a subcomplex of $K^1$ (where $K^1$ denotes the first barycentric subdivision of the simplicial complex $K$ ), though not necessarily of a subcomplex of $K$ . My ideas: If a fixed point is a vertex of $K$ it is also a barycentre of a $0$ -simplex in $K$ . If a fixed point lies in the interior of a simplex $S$ of $K$ then $f$ must take that simplex to itself. Since this induces a permutation on the vertices of $S$ I can prove that the barycentre $A_S$ is a fixed point too. I can collect all the barycenters in a set $M$ . Then $M$ is a subset of the vertices of $K^1$ . If I define $L$ to be the simplex that has the elements of $M$ as its vertices then each element of $L$ is a fixed point of $f$ . My problems with this construction are: i) I don't think I can prove that each fixed point of $f$ is also in $L$ . ii) I don't see how to prove that $L$ is a simplex of $K^1$ . If $M=\{A_0,...,A_k\}$ and $S_0,...,S_k$ are the corresponding simplexes I would have to prove that there is a permutation $\sigma$ of the integers $0,1,2,...,k$ such that $S_{\sigma(0)} \lt S_{\sigma(1)} \lt ... \lt S_{\sigma(k)}$ where the notation $A  \lt  B$ means that the simplex $A$ has to be a face of the simplex $B$ . If I succeed in proving i) and ii) I can define the set $H=\{L\}$ . By the above results this would be a subcomplex of $K^1$ such that the polyhedron $|H|$ is the set of fixed points of $f$ . But I do not know how to prove i) and ii).","['algebraic-topology', 'general-topology', 'simplicial-complex']"
2615268,Is $XX^T$ invertible?,"In one of the lectures today, the professor said that if $X \in \mathbb{R}^{m \times n}$ matrix, and the columns of $X$ span $\mathbb{R}^m$, then the matrix $XX^T$ is invertible. I am not sure why this is the case unless $m=n$?","['matrices', 'linear-algebra']"
2615287,"If $\Bbb E[{X \choose r}]=\frac{\lambda^r}{r!}$ for all positive integer $r$, show $\Bbb E[X^r]$ is the $r$th moment of a Poisson random variable","Let $X$ be a random variable with support over positive integers. If $\Bbb E[{X \choose r}]=\frac{\lambda^r}{r!}$ for every integer $r\ge 1$, how to show that $\Bbb E[X^r]$ equals the $r$th moment of Poisson distribution for every $r$? The condition is equivalent to $\Bbb E[X(X-1)...(X-r+1)]=\lambda^r$ for $r=1,2,...$. We can check indeed $\Bbb E X=\lambda,\Bbb E[X^2-X]=\lambda^2\Rightarrow \Bbb E[X^2]=\lambda^2+\lambda$, but then it looks very hard to prove it by simple induction as the higher moments of Poisson distribution look complicated. Do we need to use properties of Poisson distribution or some polynomial techniques?","['probability-theory', 'probability', 'statistics']"
2615324,Linearity of the directional derivative,"I'm confused by the highlighted remark in the text quoted below. Isn't $df_x$ linear by the very definition? At least Rudin and Apostol define derivative to be a linear map. Actually, as I write this question I perhaps came up with an answer, but since I'm not sure whether I'm right, let me state my version here and see whether it is correct. I think what is meant in the text below is that their definition of the derivative at $x$ evaluated at $h$ does not assume linearity. Whereas the definitions of Rudin and Apostol assume linearity, and then they (at least Apostol) prove (see Theorem 12.3 here Expressing directional derivative in terms of partial derivatives ) that the derivative of $f$ at $x$ evaluated at $h$, $df_x(h)$, actually equals the directional derivative of $f$ at $x$ in the direction of $h$. Am I interpreting what is going on correctly? Also, if we assume the definition given in the text below, how to prove that $df_x$ is linear? I don't see how it follows after playing with the quotient inside the $\lim$ sign. (I don't have Spivak at hand.)","['multivariable-calculus', 'real-analysis', 'analysis', 'derivatives']"
2615354,How can one show that $\int_{0}^{1}{x\ln{x}\ln(1-x^2)\over \sqrt{1-x^2}}\mathrm dx=4-{\pi^2\over 4}-\ln{4}?$,How can one show that $$\int_{0}^{1}{x\ln{x}\ln(1-x^2)\over \sqrt{1-x^2}}\mathrm dx=4-{\pi^2\over 4}-\ln{4}?\tag1$$ I have tried IBP but it is seem too complicate. I am not sure what to do or how to tackle this problem. Please any help? Thank!,"['improper-integrals', 'integration', 'calculus', 'closed-form']"
2615372,Lagrange multiplier for the Stokes equations,"I'm trying to understand the following: Let $\Omega \subset \mathbb{R}^2,\ V$ = space of (vectorvalued) continuous piecewise linear functions with zero boundary, $W = $ space of continuous piecewise linear (scalar) function. We're looking for solutions $(u,p) \in V \times W$ of the following discrete formulation of the Stokes equations: $\begin{eqnarray}
a(u,v) - (p,\text{div}v) &=& (f,v) \ \ \ \forall v \in V 
\\ (q, \text{div} u) &=& 0\ \ \ \forall q \in W
\end{eqnarray}$ with $a(u,v):= \int_\Omega \sum\limits_i Du^iDv^idx$ and the $L^2$-scalar product $(\cdot,\cdot).$ Now the notes say, these equations are a necessary condition for the minimization problem $E(v) := \frac{1}{2}a(v,v) - (f,v) \rightarrow \text{Min}$ in $V_\text{div}$ with $\ \ \ V_\text{div}:= \{ v \in V : (q, \text{div}v) = 0 \ \ \ \forall q \in W  \}$,
since the Lagrangian of the problem is given by $L(v,q) = \frac{1}{2}a(v,v) - (f,v) - (q,\text{div} v)$ and the above equations are a necessary condition for a stationary point in this function. Now my questions are: 1.) Why is this the Lagrangian for the Minimization problem? I only know the Lagrange multipliers for functions of real numbers and I'm not sure how to apply this to such a functional. The first two terms are obvious, but I can't see how exactly we obtain the term $-(q, \text{div}u)$ from the above constraint. Is $q$ the Lagrange mulitplier? 2.) Why are the equations for $u$ and $p$ the conditions for a stationary point? Here I have no idea, in what way do we take the derivatives of $L$? 3.) The lecture notes say further, that this is the reason, why this discretization does not work, because $V_\text{div}$ won't contain a good approximation for the classical solution $u$ anymore. This seems logic, but i don't see why we need the minimization problem to see this, since we already have the same restriction $(q,\text{div}v) = 0 \ \ \ \forall q \in W$ in the discrete formulation above? I would be very thankful for any hint to these questions!","['finite-element-method', 'partial-differential-equations', 'numerical-methods', 'lagrange-multiplier', 'analysis']"
2615373,"What is it, intuitively, that makes a structure ""topological""? [duplicate]","This question already has answers here : Intuition behind topological spaces (4 answers) Closed 6 years ago . With most concepts in mathematics I've learned about, I have an intuitive feeling of what their meaning is, and why we have them. Topology is not one of them. I understand that the concept of a topology emerged from the study of continuous functions. Essentially as far as I understand it, the starting point historically was the epsilon delta definition of continuity on real functions, and the topology axioms are ""what remained after stripping away everything not required for the definition of continuity"". So far so good. But then it turns out that various mathematical structures have topologies defined on them, while these have (seemingly to me at least), absolutely nothing to do with continuity, such as topologies defined on sets of first-order sentences, or on ""sets of mathematical structures"" (in mathematical logic), or on any kind of other set which as far as I see has no relation to $\mathbb R$. Of course what these all have in common, is the topology axioms. But while I know what the axioms are and can apply them, I have no intuitive understanding of what it is that all these different structures have in common. What, intuitively, does it mean for a structure to be ""topological""? I intuitively know what the set of vector spaces have in common, or the set of measure spaces. What , intuitively, does the set of topological spaces have in common?","['intuition', 'general-topology']"
2615382,$AB$ is compact iff $BA$ is,"Is it in general true or false that for $A, B \in \mathcal L (\mathcal H)$, $A B \in \mathcal S^\infty (\mathcal H) \Longleftrightarrow B A \in \mathcal S^\infty (\mathcal H )$? I think it holds only for $A, B$ normal.
Could someone tell me if I'm right? Proof (sketch) for normal operators:
$A B \in \mathcal S^\infty (\mathcal H)$, then $\psi_n \rightharpoonup 0$ implies $A B \psi_n \to 0$, because $A, B$ normal, this implies $A^* B^* \psi_n = (BA)^* \psi_n \to 0$, i.e. $(BA)^* \in \mathcal S^\infty(\mathcal H)$ and this is equivalent to $BA \in \mathcal S^\infty (\mathcal H)$. Counterexample for the general case (I think this is one): $\mathcal H = \ell^2(\mathbb N)$ and denote projections in Bra-Ket notation like $\vert n \rangle \langle n \vert$.
$$A := \operatorname{s-}\lim_{N\to\infty} \sum_{n=1}^N \frac{1}{2n+1} \vert 2n+1\rangle \langle 2n+1\vert + \operatorname{s-}\lim_{N\to\infty} \sum_{n=1}^N \vert 2n \rangle \langle 2n \vert$$
$$B := \operatorname{s-}\lim_{N\to\infty} \sum_{n=1}^N \vert 2n+1\rangle \langle n \vert$$
Then $AB$ is compact but $BA$ is not (since it maps norm-preserving on the infinite-dimensional subspace of even $n$s)...","['functional-analysis', 'compact-operators', 'operator-theory']"
2615408,The integral of the standard mollifier function,"Firstly, I would like to say that I know that it was asked before here , but the question wasn't answered. I'm self studying PDE by Evans's book and I'm trying understand why the integral of a mollifier is $1$. I will put the definition of a mollifier below according Evans's book. $\textbf{Definition:}$ Let $\eta: \mathbb{R}^n \longrightarrow \mathbb{R}$ defined by $\eta(x) := \begin{cases}
C \exp \left( \frac{1}{|x|^2 - 1} \right) \hspace{1.0cm} \text{if} \ |x| < 1\\
0 \hspace{3.6cm} \text{if} \ |x| \geq 1,
\end{cases}$ the constant $C > 0$ selected so that $\int_{\mathbb{R}^n} \eta \ dx = 1$. The function $\eta$ is called $\textit{the standard mollifier}$. I'll be gratefull if someone can tells me how can I compute $\int_{\mathbb{R}^n} \eta \ dx$.","['multivariable-calculus', 'integration', 'partial-differential-equations']"
2615425,How do I solve this specific ordinary differential equation?,"I have no clue how to solve this ode , I've been watching videos, searching online, but I just can't, someone please help me. $$2xyy' = x^2 + xy$$","['ordinary-differential-equations', 'calculus', 'analysis']"
2615426,Find a prime factor of $7999973$ without a calculator,How would you go about finding prime factors of a number like $7999973$? I have trivial knowledge about divisor-searching algorithms.,"['number-theory', 'prime-factorization']"
2615465,$n_{th}$ derivative of function?,"I have to take the nth derivative of a function (below). 
The research paper, which I have taken it from calls it the composite function. Any idea how to find the solution? $$L_d(s) = exp\{-C_d s^{2/\alpha_N}\}$$ I can take as many derivates as I want manually, but cannot find the $nth$ derivate or any formula for it, For example see below: $$F' = -\dfrac{2C_dx^{\frac{2}{\alpha_N}-1}\mathrm{e}^{-C_dx^\frac{2}{\alpha_N}}}{\alpha_N}$$ $$F'' = \dfrac{2C_dx^{\frac{2}{\alpha_N}-2}\left(2C_dx^\frac{2}{\alpha_N}+\alpha_N-2\right)\mathrm{e}^{-ax^\frac{2}{\alpha_N}}}{\alpha_N^2}$$ Any guidance would be appreciated for $F^n$",['derivatives']
2615466,Puzzling Dice Roll Probability Problem,"Mark and Jacob are taking turns rolling a fair die. Mark rolls first. What is the probability that Mark will roll an odd number before Jacob rolls a $4$? This just has me stumped... I'm not too familiar with summations, so simple answers are appreciated","['probability', 'dice']"
2615469,A nilpotent matrix A implies AB is nilpotent?,"Problem: Show that if A and B are n×n nilpotent matrices that commute, then AB is also nilpotent I ended up proving that if either A or B is nilpotent, then AB is nilpotent, so you don't need both matrices to be nilpotent. Is my proof correct? My proof seems kind of trivial, so I'm not sure if it's correct. Let m >= 1. If A is nilpotent then $A^m = 0$. Hence $A^m*B^m=0$ Hence $(AB)^m=0$ (Shown from previous question.)","['matrices', 'linear-algebra', 'proof-verification']"
