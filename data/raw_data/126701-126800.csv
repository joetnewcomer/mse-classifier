question_id,title,body,tags
1929698,"Exercise 9, Chapter 2 of Stein's Fourier Analysis. Showing that a fourier series does not converge absolutely but converges conditionally.","Let $f(x)=\chi_{[a,b]}(x)$ be the characteristic function of the interval $[a,b]\subset [-\pi,\pi]$. Show that if $a\neq -\pi$, or $b\neq \pi$ and $a\neq b$, then the Fourier series does not converge absolutely for any $x$. [Hint: It suffices to prove that for many values of $n$ one has $|\sin n\theta_0|\ge c \gt 0$ where $\theta_0=(b-a)/2.$] However, prove that the Fourier series converges at every point $x$. I've computed the Fourier series and got $\frac{b-a}{2\pi}+\sum_{n\neq 0}\frac{e^{-ina}-e^{-inb}}{2\pi in}e^{inx}.$ Also, $|e^{-ina}-e^{-inb}|=2|\sin n\theta_0|$, and $\theta_0\in (0,\pi)$, so I can see that for infinitely many values of $n$, we have $|\sin n\theta_0|\ge c \gt 0$. But this does not guarantee $\sum_{n\neq 0}|\frac{e^{-ina}-e^{-inb}}{2\pi in}e^{inx}|\ge \sum \frac{c}{n}$, and in fact we might have this inequality only for the squares of integers, in which case the right hand side converges. So how does the hint solve the problem? Moreover, for the second problem, to show that the Fourier series converges at every point, I think I need to use Dirichlet's test, using $1/n$ as the decreasing sequence to $0$, but how can I show that $\frac{e^{-ina}-e^{-inb}}{2\pi in}e^{inx}$ has bounded partial sums? I would greatly appreciate any help.","['conditional-convergence', 'fourier-series', 'fourier-analysis', 'convergence-divergence', 'analysis']"
1929717,Looking for counter-intuitive example for independence of random variables,I am looking for a simple example of two independent discrete random variables that one would not expect to be independent because one knows that these two quantities have a causal relationship in real life.,"['independence', 'examples-counterexamples', 'probability']"
1929731,Does $\sum_{n = 2}^{\infty} [\zeta(n) - 1]$ converge?,"Question in the title. Does $\sum_{n = 2}^{\infty} [\zeta(n) - 1]$ converge? If not, how about $\sum_{n = 1}^{\infty} [\zeta(2n) - 1]$?","['complex-analysis', 'real-analysis', 'riemann-zeta', 'analytic-number-theory']"
1929748,How many 4 letter words can we make from BANANAS,"As the title suggests, I want to know the value of possible permutations without repetition given the word BANANAS BA 1 N 1 A 2 N 2 A 3 S so for example to word A 1 N 1 A 2 S is the  A 3 N 1 A 2 S and we want to exclude it from our total value. How would I calculate this, I've tried 7*6*5*4 /(3!)(2!) but I think this value is too small.","['permutations', 'combinatorics', 'combinations']"
1929754,Number of ways $6$ persons of different height be seated So that every one in front is shorter?,"In a jeep there are $3$ seats in front & $3$ at back.
Number of ways $6$ persons of different height
be seated So that every one in front is shorter
than the person directly behind him MY ATTEMPT: Suppose the jeep is like this with the upper row as the front row. The tallest three among the 6 can be told to enter the back row.Morever they can be arranged in $3!$ ways.Similarly the remaining 3 people can be told to occupy the front row.They too may be arranged in $3!$ ways.So answer should be $3!*3!=6*6=36$ However this answer is incorrect as per the answer key in my book.Where did I go wrong?","['algebra-precalculus', 'combinatorics']"
1929780,Reference request: Values of $a$ for which $\frac{\sin\theta}{a + \cos\theta} = \sin\eta$ implicitly defines $\theta\mapsto \eta$,"$$ \theta \mapsto \frac{\sin \theta}{a + \cos \theta} \tag 1 $$ Everybody knows the right side of $(1)$ is equal to $\tan(\theta/2)$ if $a=1$ and to $\tan\theta$ if $a=0$.  Obviously it's unbounded if $|a| \le 1$ and bounded otherwise. In the course of thinking about map projections I stumbled across a proof that this has maximum value $1$ and minimum $-1$ precisely if $|a|=\sqrt 2$. A consequence is that a function $\theta\mapsto\eta$ is implicitly defined by $$ \frac{\sin\theta}{a + \cos\theta} = \sin\eta \tag 2 $$ precisely if $|a|=\sqrt 2$. My question is: Is this a standard result found in the literature somewhere --- something everyone except me knows (for appropriate values of ""everybody"", thus excluding those whose interests would not bring this to their attention if it's widely known)?  Or to be more precise: Is this something standard that is in the literature somewhere?  Are other things of interest known about this? Appendix: One way of showing that The maximum and minimum are equal to $1$ and $-1$ respectively when $a=\sqrt 2$ is to observe that
$$
\left( \frac{\sin\theta}{\sqrt2+\cos\theta} \right)^2 + \left( \frac{\sqrt 2\cos\theta + 1}{\sqrt 2 + \cos\theta} \right)^2 = 1
$$
and the value $1$ is attained when $\sin\theta = -\cos\theta>0$.","['reference-request', 'trigonometry']"
1929790,The Klein bottle and its Topology,"I have read in several places that the Klein bottle is a 2-manifold, but I cannot find an explicit proof anywhere. How would you show it is locally Euclidean, Hausdorff and second countable (I think this is what defines something as a manifold...)? And why can it not be embedded in $\mathbb R^3$? Any help would be greatly appreciated.","['manifolds', 'general-topology', 'klein-bottle']"
1929801,Why are $E/F$ and $E^\ast$ smooth manifolds?,"In chapter $5$ of Morita's Geometry of Differential Forms , he defines a real vector bundle over a smooth manifold $M$ as a triple $(E,\pi,M)$, where $E$ is another smooth manifold, $\pi \colon E \to M$ is smooth, each fiber $E_p = \pi^{-1}[p]$ is a real vector space, and there's the trivialization condition (which we won't need here I guess). Then he procceds to do a lot of cool constructions such as the quotient bundle and the dual bundle. I understand everything except why $$E/F = \bigsqcup_{p \in M} E_p/F_p \quad\mbox{and}\quad E^\ast =\bigsqcup_{p \in M}E_p^\ast$$ are smooth manifolds. Here, of course, $(E,\pi,M)$ is a vector bundle over $M$ and $(F,\pi,M)$ is a subbundle. I think I must be missing some basic fact about smooth manifolds, but anyway, I need some push here.","['vector-bundles', 'smooth-manifolds', 'differential-geometry', 'differential-topology']"
1929814,Which solvable ODE's correspond to algebraic curves?,"This is a follow-up to my previous question: Are there periodic functions satisfying a quadratic differential equation? Let $P(u,v)$ be a bivariate polynomial. Then its zero set $\{(u,v): P(u,v)=0\}$ is an algebraic curve. It might make it easier to assume in what follows that the curve has no singularities. If we substitute $y$ and $y'$ into $P(u,v)$ to take the places of $u$ and $v$ respectively, then we get a possibly non-linear first-order ODE. I am curious about how many ODEs of this type have solutions. 1. How many/which first order ODE's formed by substituting into a bivariate polynomial have solutions? 2. Does any solution to such an ODE parametrize the algebraic curve that characterizes the defining ODE? EDIT: I realize now that the answer to this question is obviously yes, because it follows directly from the fact that $y$ and $y'$ are both functions of the same single variable (say $t$ ) and that they satisfy the equation $P(y,y')=0$ , so if they exist they must parametrize the algebraic curve, as a result of their definition. Some examples might help. For second-degree algebraic curves, we have that the equation of the unit circle corresponds to a first order ODE which has both sine and cosine as solutions, and that these two functions parametrize the unit circle as well: $$(y')^2 + y^2 -1=0  $$ Likewise, the hyperbolic sine and hyperbolic cosine parametrize the unit hyperbola, and both correspond to solutions of a first order ODE whose form is that of the unit hyperbola: $$(y')^2 - y^2  -1=0$$ Finally, smooth cubic curves in Weierstrass normal form can be parametrized by the Weierstrass $\wp$ functions, and the (complex) differential equations characterizing the Weierstrass $\wp$ functions have the form of a smooth cuvic curve in Weierstrass normal form: $$(\wp')^2 = 4[\wp^2] -g_2 \wp -g_3  $$ Then my question is essentially: how far does this go? For examples, if I were to choose the algebraic curve $$ v^4 - v^3 = u^5 +u^2 -7$$ would the first-order ODE $$(y')^4 - (y')^3 = y^5 + y^2 -7$$ have a solution? And would the solution to this ODE (if it existed) parametrize the original algebraic curve $v^4 - v^3 = u^5 + u^2 -7$ ? This example is completely arbitrary, but hopefully it makes clear to some extent the level of generality I am interested in. Note: I am not sure how to properly tag this question. This question is related but addresses second order ODE's -- however, I am not interested in second order ODE's, only first order: Algebraic Curves and Second Order Differential Equations I think that this question is probably the most related, assuming that epicycloids can be defined by first-order ODEs (I don't know either way). Proof that Epicycloids are Algebraic Curves?","['algebraic-curves', 'parametric', 'ordinary-differential-equations', 'algebraic-geometry']"
1929855,"GRE question: Evaluate $\int_0^\infty \left \lfloor x \right \rfloor e^{-x} \,\mathrm{d}x$","I have the following GRE question that I have no idea how to solve. Let $\left \lfloor x \right \rfloor$ denote the greatest integer not exceeding $x$. Evaluate $\int_0^\infty \left \lfloor x \right \rfloor e^{-x} \,\mathrm{d}x$. The answer says it should be $\frac{1}{e-1}$, and a hint that they give is $$
\int\limits_0^\infty \left \lfloor x \right \rfloor e^{-x} \,\mathrm{d}x 
= 
\sum_{n=1}^\infty \int\limits_n^{n+1} ne^{-x} \,\mathrm{d}x\,.
$$ I don't really see how we go from the integral to the summation.","['integration', 'gre-exam']"
1929884,Finding the $ n $-th derivative of $ {\cos^{n}}(x) $.,"Problem. Find the $ n $-th derivative of $ {\cos^{n}}(x) $. What I’m doing is substituting $ t = \cos(x) $ and then finding the $ n $-th derivative of the new function, but I’ve a feeling that this is wrong. Could anyone please point out the correct method? Thank you!","['derivatives', 'trigonometry', 'calculus']"
1929937,Is the first chern class of an algebraic line bundle an algebraic cohomology class?,"Let $X$ be a smooth projective variety over $\mathbb{C}$. For $L$ a line bundle, one can define the first Chern class in $H^2(X; \mathbb{Z})$ (for example using the exponential exact sequence, a generic smooth section, or the Euler class - I think they are all the same.) Moreover, given a smooth closed subvariety $Y \subset X$, one can associate to a fundamental class in the cohomology of $X$, by extending the Thom class of a tubular neighborhood / Normal bundle. Such cycles are said to be algebraic. (I think.) (There is a similar procedure when this subvariety is singular, by stratifying it.) Question: For $L$ an algebraic line bundle (the transition maps are polynomial), is $c_1(L)$ an algebraic cycle? I would also like to know if the collection of algebraic cycles and cocycles is closed under cup and cap products. I guess so, but I could use a reference. Partial answer: Let $O(1)$ denote some very ample line bundle on X. Then $L(n))$ for some $n$ is globally generated, and hence defines a map to projective space $\phi : X \to P^N$, so that $\phi^*(O_{P^N}(1)) = L(n)$. Provided that the pullback of an algebraic cycle is algebraic (which I don't know to prove still), we are done, because then the first chern of $L(n)$ is algberaic, being the pullback of the Chern class of $O_{P^N}(1))$ which is a hyperplane, and hence algebraic. Now we just use that $c_1(O_X(1))$ is a hyperplane section of $X$, and hence algebraic, and that $c_1(L(n)) = c_1(L) + c_1(O_X(n))$ to conclude. So now I need: Lemma: Let $\phi : X \to Y$ be a regular map of smooth complex projective (algebraic) varieties. Let $Z \subset Y$ be a subvariety, and $[Z]$ the fundamental class in $H^*(Y)$. Then $[\phi^{-1}(Z)] = \phi^*([Z])$. Rmk: In the case that $Z$ is a smooth subvariety, I understand reasonable well how to define this fundamental class - for example, it can be defined by pulling back forms to on $Y$ to $Z$, and then integrating them, and then using Poincare duality and universal coefficients. In the case of singular varieties, this is less clear -- I expect that one can pullback to the generic smooth open (or to a resolution of singularities) and use the same trick, though.","['homology-cohomology', 'complex-geometry', 'algebraic-geometry']"
1929946,"Homeomorphism from (a,b) to reals","Question:
  Show that if $a,b \in \mathbb{R}$ and $a < b$ then $\left ( a,b \right ) $ is homeomorphic to $\mathbb{R}$. [Hint: Let 
$$f:\left( -1,1\right)\rightarrow \mathbb{R},\ \ \ f\left( x \right)=\frac{x}{ 1-x^{2} } ;$$ 
use standard results from calculus to prove continuity] $f\left ( x \right )=\frac{x}{\left ( 1-x^{2} \right )}$
and the inverse is $f^{-1}\left ( x \right )=\frac{y}{1-y^{2}}$
every element in $\left ( a,b \right )$ is the image of exactly one element in $\mathbb{R}$.
We have bijection. At this point, my lack of formal education in real analysis is posing some problems but let's give it a shot. Definition:
  Let $A \subseteq \mathbb{R}$, let $f:A\rightarrow \mathbb{R}$ and let $c \in$ A. 
  We say that f is continuous at c if given any neighbourhood $V_{\varepsilon }\left ( f\left ( c \right ) \right )$ of $f\left ( c  \right )$ there exists a neighbourhood $V_{\delta }\left ( c \right )$ of c such that if x is any point of $A \cap V_{\delta}\left ( c \right )$, then
  $f\left ( x \right )$ belongs to $V_{\varepsilon }\left ( f\left ( c \right ) \right )$ Any help is appreciated. Thanks in advance.","['general-topology', 'real-analysis', 'functions']"
1929988,What is the geometric meaning of $f(x)=f(c)+f'(c)(x-c)+r(x)(x-c)$?,"I see a few, but not many, books give a characteristic theorem about derivatives of real-valued real functions. That is the following: If $f$ is differentiable at $c$, then there is a function $r$
  continuous at $c$ such that $f$ can be written as
  $f(x)=f(c)+f'(c)(x-c)+r(x)(x-c)$ in an appropriate neighborhood of $c$, where $r(x)=\begin{cases}\frac{f(x)-f(c)-f'(c)(x-c)}{x-c},\qquad x\neq c \\0,\qquad x=c\end{cases}$ However, I can't find any explanation that says what is the geometric meaning of $f$ being written in this way and how did we come up with this somewhat technical function $r$. And what can we do with this function $r$? Does it have any theoretical benefit?","['derivatives', 'real-analysis']"
1930062,Estimating the radius of the Earth from a plane trip [closed],"Closed. This question is off-topic . It is not currently accepting answers. Closed 7 years ago . This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. This question is not about mathematics, within the scope defined in the help center . Improve this question My friend had an interview at Cambridge. He was asked the following question, and was stumped: I fly to Chicago. The plane trip is $8$ hours. I look at the time and then set my watch back $6$ hours. Knowing that the Earth rotates $360^\circ$ in $24$ hours, what is the radius of the Earth?",['geometry']
1930174,Kodaira dimension of $X \subset \mathbb{P}^n$ hypersurface of degree $d$,"Let us fix some notations: $X = \{ F = 0 \} \subset \mathbb{P}^n$ is a non singular hypersurface of degree $d$ $K_X$ is the canonical bundle of $X$ $Q(X, K_X) = \{f/g \; \text{such that} \; f,g \in \Gamma(X, K_X^{\otimes p}) \; \text{for some $p > 0$}, g \neq 0 \}$ $K(X) = deg tr_{\mathbb{C}}Q(X,K_X)$ is the Kodaira dimension of $X$ What I want to show is that 
$$K(X) = 
\left\{
\begin{array}{lr}
-\infty & \text{if $d < n+1$}\\
0 & \text{if $d = n+1$}\\
dim_{\mathbb{C}}(X) & \text{if $d > n+1$}
\end{array}
\right.
$$ From adjunction formula we know that $K_X = \mathcal{O}_{\mathbb{P}^n}(d-n-1) \rvert_X$, so the first two cases are easy: if $d < n+1$ we have that $\Gamma(X, K_X^{\otimes p}) = 0$ for all $p >0$; if $d = n+1$ we have that $\Gamma(X, K_X^{\otimes p}) = \mathbb{C}$ for all $p >0$. I am now dealing with the last case. I know that $\Gamma(X, K_X^{\otimes p}) =$ {Homogeneous polynomial of degree $p(d-n-1)$ restricted to $X$}, so Initial post $$Q(X,K_X) = \{ f/g \; \text{with $f,g \in \mathbb{C}[z_0, \dots, z_{n-1}]$ of the same degree} \}$$ (I deleted the n-th coordinate because without loss of generality we can suppose that $\partial F / \partial z_n \neq 0$) At this point I notice that $Q(X,K_X) = Q(\mathbb{P}^{n-1}, \mathcal{O}_{\mathbb{P}^{n-1}}(1))$ and from this I obtain 
$$K(X) = deg tr_{\mathbb{C}}(Q(\mathbb{P}^{n-1}, \mathcal{O}_{\mathbb{P}^{n-1}}(1)) \leq n-1 = dim_{\mathbb{C}}(X)$$ Now I am searching for $n-1$ elements in $Q(X,K_X)$ algebraically independent. I guess the elements $\{z_i/z_0 \; i=1, \dots, n-1\}$ should work. My attempt to show they are algebraically independent: let $p \in \mathbb{C}[X_1, \dots, X_{n-1}]$ be such that $p(z_1/z_0, \dots, z_{n-1}/z_0) \equiv 0$ and let $q$ be the maximum exponent of the monomials in $p$. Then $z_0^q \cdot p(z_1/z_0, \dots z_{n-1}/z_0)$ is a homogeneous polynomial of degree $q$ in the variables $z_0, \dots, z_{n-1}$ and it's identically zero. Then by the polynomial identity principle its coefficients must be all zero, consequently $p \equiv 0$. Is it correct?
(In truth I am only changing name to the variables, so I think it should works. Does it?) Edit $$Q(X,K_X) = \{ f/g \; \text{with $f,g$ homogeneous polynomials of the same degree}\}$$ I don't ask $f,g$ to be of degree $p(d-n-1)$ for some $p>0$ because if $f,g$ are any two homogeneous polynomials of the same degree $s$ then I can consider $$\frac{f}{g} = \frac{z_0^{p(d-n-1)-s} f}{z_0^{p(d-n-1)-s} g}$$ Where $p(d-n-1)-s >0$. With a friend of mine we tried to conclude the proof but we didn't make it right. Here's what we have done so far: Let $z_i$ be a coordinate in $\mathbb{P}^n$ such that $z_i$ is not identically zero on $X$. Without loss of generality we can suppose $i=0$. Let $z_j$ be a coordinate that is in $F$. Without loss of generality we can suppose $j=1$. Consider the functions $z_i/z_0$ for $i>1$ and suppose it exists a polynomials in $\mathbb{C}[t_1, \dots, t_{n-1}]$ such that $p(z_2/z_0, \dots, z_n/z_0) = 0$. Let $s$ be the degree of $p$, then $q(z_0, \dots, z_n) = z_0^s p(z_2/z_0, \dots, z_n/z_0)$ is a homogeneous polynomial of degree $s$ that vanishes on $X$. Being $F$ non singular and homogeneous it is irreducible, so $F \mid q$ but this is impossible because $F$ depends on $z_j$ and $q$ does not. Does this proof work? If not, can someone give me a hint? Thank you in advance!","['extension-field', 'complex-geometry', 'algebraic-geometry']"
1930185,Why does this series expansion for $\frac{1}{\cos(x)}$ fail using $\frac{1}{1-x}$?,"I'm trying to do (2nd order) Taylor expansion for 
$$\frac{1}{\cos(x)}$$ using $$\frac{1}{1-x}$$ What I do is write $$\cos(x)=1-\frac{x^2}{2!}+\frac{x^4}{4!}+ \cdot\cdot\cdot, \text{ at } x_0=0$$ and $$\frac{1}{1-x}=1+x+x^2+\cdot\cdot\cdot$$ so $$\frac{1}{\cos(x)}=\frac{1}{1-(1-\cos(x))}=1+(1-\cos(x))+(1-\cos(x))^2$$
$$=1+(1-1+\frac{x^2}{2!})+(1-1+\frac{x^2}{2!})^2$$
$$=1+\frac{1}{2}x^2+\frac{1}{4}x^4$$ which is correct up to order 2 ( or order 1? ), but the coefficient of $x^4$ (is that 3rd or 4th order?) is wrong. Why? How can I get the correct coefficients for higher orders? And if I do want only 2nd order Taylor polynomial, then what is the term $\frac{1}{4}x^4$?","['taylor-expansion', 'trigonometry']"
1930190,How do I find the angles of a polygon if I only have the lengths of the sides?,"I have the length of n sides $[l_1,l_2,...,l_n]$ and it is possible to form a polygon using these sides. The sides of the polygon will be in the order as given so the adjacent sides of $l_1$ are $l_n$ and $l_2$ and like that. How do I find the interior angles of the polygon? I know there can be multiple lists of angles for 1 list of sides, I just need any one.","['trigonometry', 'polygons', 'geometry']"
1930232,Prove that $2^n-3$ is squarefree,"Let $n$ be a natural number with $n \equiv 1 \pmod{4}$. Is it true that $2^n-3$ is not divisible by $p^2$ for any prime $p$? I am conjecturing the above to be true, so I was wondering if there were any counterexamples to it since I checked it up to $n = 101$ and it was true up to then.",['number-theory']
1930244,Solve $\iint_D\sqrt{9-x^2-y^2}$ Where $D$ is the positive side of a circle of radius 3,"Solve $\displaystyle\iint_D\sqrt{9-x^2-y^2}$ Where $D$ is the positive
  side of a circle of radius 3 ($x^2+y^2=9,x\ge0,y\ge0$) I tried to subsitute variables to $r$ & $\theta$: $$x = r\cos\theta$$
$$y = r\sin\theta$$
$$E = \{0\le r\le3,0\le\theta\le\pi\}$$
$$\displaystyle\iint_D\sqrt{9-x^2-y^2} = \displaystyle\iint_EJ\sqrt{9-(r\cos\theta)^2-(r\sin\theta)^2}=\displaystyle\iint_Er\sqrt{9-(r\cos\theta)^2-(r\sin\theta)^2}$$ But have no clue on how to solve this new integral.","['multivariable-calculus', 'substitution', 'integration']"
1930272,How do I visualize differential equations?,"OK, I got an exam in about a week, and there is a point that I don't really got my head around yet. Our professor likes to give for example three pictures and one differential equation. The question now is which of these pictures approximates the given equation. How would I go and start such a task? These are very simple equations and most will probably find this trivial, but as someone in second semester I always get stuck on these kind of questions. Examples would be: $$ y^\prime = y + 1 $$ $$ y^\prime = -2xy^2 $$ Example:","['visualization', 'ordinary-differential-equations']"
1930307,Showing A Certain Subgroup of $\mathbb{Q}$ must actually be $\mathbb{Q}$,"I asked to show that if $H$ is a subgroup of the additive group $\mathbb{Q}$ with the property that $\frac{1}{x} \in H$ if $0 \neq x \in H$, then $H=0$ or $H = \mathbb{Q}$>. I already ruled out the first case by supposing the $H$ only contained the identity. I am wondering if my proof of the second part is correct. Here it is: Suppose that $H$ has at least one other element besides $0$, call it $\frac{r}{s}$. By closure, we know $\frac{r}{s} + \dots + \frac{r}{s}$ (s-times is $r$, which is in $H$. Then $\frac{1}{r} \in H$. By closure, $\frac{1}{r} + \dots + \frac{1}{r}$ ($r$-times) is $1$. Since $H$ contains $1$, by $H$'s closure it must contain all integers. Now, let $y \in H$ be some arbitrary integer. Then $\frac{1}{y} \in H$, and so $\frac{1}{y} + \dots + \frac{1}{y}$ ($x$-times, where $x$ is some arbitary integer) is $\frac{x}{y}$, an arbitrary rational number which must be in $H$ by closure. Does this seem right?","['abstract-algebra', 'rational-numbers', 'group-theory', 'proof-verification']"
1930315,"If $f(x)=f'(x)+f''(x)$ and $f(a)=f(b)=0$, then $f(x)=0\quad\forall x\in[a,b]$","A real-valued function $f(x)$ defined on a closed interval $[a,b]$ has the properties that $f(a) = f(b) = 0$ and $f(x) = f'(x)+f''(x)$ for all $x$ in $[a,b]$. Show that $f(x) = 0$ for all $x$ in $[a,b]$. I'm not sure how to tackle this problem. Would integration help? I see that second order linear differential equations require exponential functions. But wouldn't it be possible to have $f(x)$ as a trigonometric function? If not, please state why I'm wrong.","['ordinary-differential-equations', 'calculus', 'functions']"
1930354,Finding a Power Series representation for the function $f(x) = \frac{2}{3-x}$,"Let's say I want to find a Power Series representation of the function $f(x) = \frac{2}{3-x}$ Now I know we can write this as a geometric series $$\sum_{n=0}^{\infty}ar^n = \frac{a}{1-r}$$ But I see two possible ways two write it as a geometric series. $(1)$ and $(2)$ below Method $(1)$: We let $a=2$, and $r = x-2$, thus we transform $\frac{2}{3-x}$ into the form $\frac{a}{1-r}$ and we write the Power Series for $f$ as follows: $$f(x) = \sum_{n=0}^{\infty}\ 2(x-2)^n$$ Method $(2)$: We divide both numerator and denominator by a factor of $3$ to go from $\frac{2}{3-x}$ to $$\frac{\frac{2}{3}}{1-\frac{x}{3}}$$ and we can then write $f$ as follows: $$\begin{aligned}f(x) &= \sum_{n=0}^{\infty}\ \frac{2}{3}\left(\frac{x}{3}\right)^n
\\
&= \sum_{n=0}^{\infty}\left(\frac{2}{3^{n+1}}\right)x^n
\end{aligned}$$ But only $(2)$ is correct, and $(1)$ is incorrect, but I can't seem to see why. What I did in $(1)$ seemed like perfectly valid algebraic manipulations, so why does $(1)$ result in an erroneous answer?","['power-series', 'sequences-and-series', 'functions']"
1930381,Changing a non-function into a function by changing the codomain/range?,"My test book says that $y=\sqrt{x}, x\in\ \Bbb R$ is not a function because x values less than zero are not mapped onto anything, and that it can be made into a function by restricting the domain to $x\in \Bbb R, x\geqslant 0$. I was wondering if you could make it into a function by changing the range instead, to $y=\sqrt{x}, x\in \Bbb R, y\in \Bbb C\;?$ Then each element of the domain does have an output... [NOTE: I am also unsure about the distinction between range and codomain. I think- though I really am not sure- that the term codomain should be used here instead, and the range is smaller than the codomain because the possible outputs exist solely on the real axis or imaginary axis, but not elsewhere on the complex plane?]","['complex-numbers', 'functions']"
1930388,Polar to cartesian form of r=sin(3θ),"$$\sin(2θ) = \sinθ\cosθ + \cosθ\sinθ = 2\sinθ\cosθ $$
$$\cos(2θ) = \cosθ\cosθ - \sinθ\sinθ = \cos²θ - \sin²θ $$ $$\sin(3θ) = \sin(2θ+θ) $$
$$= \sin(2θ)\cosθ + \cos(2θ)\sinθ $$
$$= 2\sinθ\cos²θ + \cos²θ\sinθ - \sin³θ 
$$ $$= 2\sinθ(1-\sin²θ) + (1-\sin²θ)\sinθ - \sin³θ 
$$ $$= 2\sinθ - 2\sin³θ + \sinθ - \sin³θ - \sin³θ 
$$ $$r=\sin(3θ)= 3\sinθ - 4\sin³θ$$
How to deal with the differing exponents? I could add an $r$ to both sides or $r^3$ but not both. 
 $$r^4= 3r^3\sinθ - 4r^3\sin^{3}θ   $$
 $$r^4= 3r^2y - 4y^3$$
 $$(x^2+y^2)^{4/2}= 3(x^2+y^2)^{2/2}y - 4y^3$$
 $$(x^2+y^2)^{2}= 3(x^2+y^2)y - 4y^3$$
 $$(x^2+y^2)^{2}= 3yx^2-y^3$$",['trigonometry']
1930391,difference between function and relation?,"What is the difference between function and relations ? What are the characteristics of function and relations? What are the similarities and contrasts between relations and functions ?
Examples will be much appreciated , thanks !",['functions']
1930410,What is the number of n node simple labeled graphs without endpoints?,"Here, an endpoint is a vertex of degree $1$.  For $n=1,2,3,\ldots$ the number of such graphs is $1,1,2,15,314,\ldots\;$.  This is sequence A059167 in Sloane's OEIS.  The exponential generating function (e.g.f.) is given in the OEIS entry: $$\exp\left(\frac{x^2}2\right)\sum_{n=0}^\infty\frac1{n!}\left(\frac{x}{e^x}\right)^n2^{\binom{n}2}$$ Is there some way to easily arrive at the generating function by applying symbolic derivation methods such as those described in Flajolet and Sedgewick Analytic Combinatorics? I understand the factor $\exp(\frac{x^2}{2})$.  I also see that the summation WITHOUT THE FACTOR $\exp(-x)$ would be the e.g.f. for the total number of simple labeled graphs.","['generating-functions', 'combinatorics', 'graph-theory']"
1930455,"Proving that if $A$ is infinite, and $A \subseteq X$, then at least one set of $X$ is infinite.","Here is the original question: Let $A$ be an infinite set. Prove that if $$A \subseteq X,$$ and $X$ can be written as $$X=\bigcup^n_{k=1}X_{k}$$ then at least one of the sets $$X_{k}\cap A$$ is an infinite set. 
My book doesn't make any mention of infinite sets and I couldn't really figure out to answer this question. It makes sense to me that for an infinite set to be inside another set, that set would have to be infinite too, but I am struggling with formulating a formal proof to show this.",['elementary-set-theory']
1930460,Proof of norm product in Hilbert spaces,"Prove that, in a Hilbert space,
$$
\Vert A+B\rVert \lVert A-B\rVert \le \lVert A\rVert^2 + \lVert B\rVert^2
$$ I was asked to do it without using the Cauchy inequality I don't know which property to use, any help would be appreciated","['functional-analysis', 'multivariable-calculus', 'linear-algebra']"
1930461,Find an integrable dominated function for a convolution,"Let $h\in L^1(\mathbb{R}^n)$. Let $\varphi\in S(\mathbb{R}^n)$, $\int_{\mathbb{R}^n}\varphi(x) dx=1$, where $S(\mathbb{R}^n)$ is the Schwartz function space and $\varphi$ is nonnegative, radial, and radially decreasing. Let $\varphi_k(x)=k^n\varphi(kx)$, $k=1,2,...$, which is a sequence of function approximations to the Dirac delta function $\delta_0$. Recall that $||h\ast \varphi_k||_1\le ||\varphi_k||_1||h||_1=||h||_1$ by Young's inequality. Then is there a function $g\in L^1(\mathbb{R}^n)$ such that $${\rm{sup}}_{k\ge1}|h\ast\varphi_k|(x)\le g(x),\ a.e. \ ?$$ Remark: (1)Using the Hardy-Littlewood maximal function we have ${\rm{sup}}_{k\ge1}|h\ast\varphi_k|(x)\le Mh(x)$, but unfortunately $Mh\notin L^1(\mathbb{R}^n)$ whenever $h\ne 0$ on a set with positive measure. (2)By the Hardy-Littlewood maximal theorem, if $h\in L^p(\mathbb{R}^n)$, $1<p\le \infty$, then $${\rm{sup}}_{k\ge1}|h\ast\varphi_k|(x)\le Mh(x)\in L^p(\mathbb{R}^n).$$","['functional-analysis', 'harmonic-analysis', 'real-analysis']"
1930471,"Irreducible polynomial such that its roots satisfy a given relation $x_3 = f(x_1,x_2)$","My general question is: What are the polynomials $f \in \Bbb Q[X,Y]$ such that there exists an irreducible polynomial $P \in \Bbb Q[X]$ having three distinct zeros $x_1,x_2,x_3$ that satisfy $x_3 = f(x_1,x_2)$ ? It is probably too broad, so I'd like to begin with the case $f(X,Y)=aX+bY+c$ where $a,b,c$ are rational numbers. Apparently , we can't have $ab=0$ (because $X \mapsto aX+c$ is an automorphism of $\Bbb Q[X]$ if $a\neq 0$, and clearly if $a=0$, we can't have $x_3=c \in \Bbb Q$). Moreover, the cases $a=1,b=0=c$ and $b=1,a=0=c$ are forbidden. It is less trivial, but the case $c=0,a=b=1/2$ can't happen, see here . The main point of the answer is that the Galois group of the splitting field of $P$ over $\Bbb Q$ acts transitively on the finite set $R \subset \Bbb C$ of the roots of $P$, so that any root $x_j$ can be expressed as $x_j=f(a_j,b_j)$ where $a_j \neq b_j \in R$. We can call this property the $f$-transitivity: a non-empty set $E \subset \Bbb C$ is $f$-transitive if any $x \in E$ can be written as
$x=f(y,z)$ for distinct elements $y,z \in E$.
Maybe for many $f \in \Bbb Q[X,Y]$, an $f$-transitive set must be infinite, as this is the case for $f(X,Y)=(X+Y)/2$.
Even for $f(X,Y)=X+Y$, I wasn't able to prove it, but I believe it's true for $f(X,Y)=aX+bY+c$, and maybe for higher degree… However, $f(X,Y)=XY$ is possible, using some cyclotomic polynomial. Any hint is welcome. Thank you!","['irreducible-polynomials', 'abstract-algebra', 'roots']"
1930477,In how many ways can a number be written as a product of two different factors?,"In how many ways can a number be written as a product of two different
  factors? MY APPROACH : FOR EXAMPLE:Let the number be $8100$=$2^23^45^2$. Number of divisors = $(2+1)(4+1)(2+1) = 45$ The divisors can be written as product of two numbers like. $1 \times 8100$ $2 \times 4050$ $3 \times 2700$ $4 \times 2025$ $5 \times 1620$ . . . $8100 \times 1$ Due to repetition a top-bottom symmetry can be observed.Hence the actual number of ways of representing as a product should be $$\frac{45+1}{2}=\frac{46}{2}=23$$ Is my approach correct?Can someone please verify whether my argument that the final answer should be $\frac{45+1}{2}$ is valid? EDIT:As @mathlove pointed out we need to subtract 1 as $90*90$ is not allowed as the question asks for different factors.Final result $\frac{45-1}{2}=22$.","['combinatorics', 'elementary-number-theory']"
1930480,Show that the dual space of the vector space of all polynomials is isomorphic to the infinite-dimensional Euclidean vector space over the reals,May I please ask how to show the dual space of the vector space of all polynomials is isomorphic to the infinite-dimensional Euclidean vector space over the reals? (i.e. Show that $(\Bbb{R}[X])^*$ and $\Bbb{R}^{∞}$ are isomorphic where $\Bbb{R}$ means the set of all reals),"['polynomials', 'vector-space-isomorphism', 'linear-transformations', 'linear-algebra', 'vector-spaces']"
1930485,rank of a orthogonal matrix,"My question is, Is every orthogonal matrix is full rank ? Is there any specific thorem to answer this question ?
I didnt find any","['matrices', 'orthogonal-matrices', 'matrix-rank', 'linear-algebra']"
1930526,Continuity and limits at end point of interval,"I am bad a calculus and I have question about continuity. If I have a polynomial, then the function is continuous on $\mathbb{R}$ because $\lim_{x\to a} f(x) = f(a)$ for all $a\in \mathbb{R}$. My question is if $f(x) = \sqrt{x}$ is continuous at $0$. My text book doesn't say if this is continuous at $0$. It does say that the function is continuous from the right . But I have also heard that continuity is a more general concept. My question is: under the real definition of continuity, if $f(x) = \sqrt{x}$ continuous at $0$? Is it correct to say that $\lim_{x\to 0} \sqrt{x} = 0$ without specifying that $x$ is approaching from the right?","['continuity', 'calculus', 'limits']"
1930558,Textbook recommendation for discrete Mathematics,I want a good textbook covering elemenents of discrete mathematics Average level.Im a mathematics undergraduate so i dont want it to be towards Computer science that much.Interested in combinatorics and graph theory .But also covering enumeration and other stuff.One book i found is https://www.amazon.com/Elements-Discrete-Mathematics-C-Liu/dp/0071005447 .,"['reference-request', 'book-recommendation', 'discrete-mathematics']"
1930588,Calculate the expectation of the product of two random variables,"There is a sequence of independent and identically distributed continuous random variables $X_1,X_2,\ldots$ with common density function $f(x)$. We say that a record occurs at time $n$ if $X_n > \max(X_1,X_2,\ldots,X_{n-1})$. Consider the random variable $Y_i$ defined as: $Y_i = 1$ if a record occurs at time $i$, $Y_i = 0$ otherwise. Compute $E[Y_i \cdot Y_j]$ where $i < j$. I compute like this:
$$E[Y_i \cdot Y_j] = P(Y_i = Y_j = 1) = P(Y_i=1 \mid Y_j=1)P(Y_j=1).$$
Since $P(Y_j=1)=1/j$ and $$P(Y_i=1 \mid Y_j=1) = P(Y_i=1) = 1/i,$$ then $E[Y_i \cdot Y_j]=1/(ij)$.
It seems not quite right and I am not very sure about it. The reason why I'm not so sure about it is that, it seems by this way, the events ""the record occurs at the time $i$"" and ""the record occurs at the time $j$"" would be independent. But given ""the record occurs at the time $j$"", would it be harder for ""the record occurs at the time $i$"" to happen? Because $X_j > X_i$, and $X_i$ cannot be very big to be the record. Thank you for your help!",['probability-theory']
1930607,Maximum area enclosure given side lengths,"A peer of mine gave me the following problem : Given a sequence of $n$ lengths (i.e.,$L_1, L_2, .., L_n$ ) where each is the length of the side, find a sequence of $n$ points (where $p_k = (x_k, y_k)$) such that $dist(p_k, p_{k+1}) = L_k$ and $dist(p_1, p_n) = L_n$ where $dist(a, b)$ is the Euclidean distance between points $a$ and $b$. Note that the first point , $p1$,  must be $(0, 0)$ and the second point , $p_2$,  must be $(0, L_1)$ . These points must correspond to the ordered clockwise vertices of the simple polygon having the maximum possible area for the given side lengths. Determine the coordinates of the points that correspond to a polygon of maximal area. Now, I've seen some similar problems, but this one I don't even know how to begin with. Determining the maximum area is quite easily done by Brahmagupta's formula, but the actual finding of the points seems really hard. The only thing I could think of was finding some sort of enclosing circle and then trying to determine the points, but that's not even a starting point. Any ideas on how to solve this?","['area', 'geometry']"
1930627,Why is the sample space no longer important once a discrete random variable is introduced?,"F.M. Dekking, C. Kraaikamp, H.P. Lopuhaa, L.E. Meester, "" A Modern Introduction to Probability and Statistics "", Page-43. Once a discrete random variable X is introduced, the sample space Ω is
  no longer important. It suffices to list the possible values of X and
  their corresponding probabilities. This information is contained in
  the probability mass function of X. Can you explain this? Why is the sample space no longer important once a discrete random variable is introduced?","['probability-theory', 'probability', 'probability-distributions']"
1930635,When do limits at infinity not exist?,"Here is a limit at infinity$$\lim_{x \to \infty}f(x)$$ A limit fails to exist for one of the four reasons: The one-sided limits are not equal The function doesn't approach a finite value The function oscillates The $x$ value is approaching the endpoint of a closed interval Here are my questions: How does this work for limits at infinity? I am under the impression that we can only approach infinity from one side, therefore causing all limit at infinity to fail this test. So the only case where this rule applies is if the limit evaluates to either $+\infty$ or $-\infty$? No questions. Is this case essentially a subset of case 1?","['calculus', 'limits']"
1930682,Finite-dimensional algebras which do not satisfy Wedderburn's principal theorem,"Let $k$ be a field, and let $A$ be a finite-dimensional associative $k$-algebra.  I assume that $A$ is basic (that is, in any decomposition of $A$ into a direct sum of indecomposable projective $A$-modules, these are pairwise non-isomorphic). Wedderburn's principal theorem states that If $A/rad(A)$ is separable, then there is a subalgebra $B$ of $A$ such that $B$ is semisimple and $A=B\oplus rad(A)$ as $k$-vector spaces. In this post, I will say that Wedderburn's principal theorem holds for an algebra $A$ when the conclusion of the above theorem is true for $A$ (even if $A$ does not satisfy the assumptions of the theorem). Here are some families of cases where the theorem does holds: if $k$ is algebraically closed, or even a perfect field; if $A$ is the path algebra of some finite quiver $Q$ modulo an admissible ideal (or, more generally, the tensor algebra of a $k$-species modulo an admissible ideal), without assumptions on the field $k$. An example where it does not hold is the $\mathbb{F}_2(t)$-algebra $\mathbb{F}_2(t)[X]/(X^2-t)^2$  (Exercise 6.4 of Drozd and Kirichenko's book Finite Dimensional Algebras ).  Similar examples are obtained by replacing $\mathbb{F}_2$ by any field of characteristic $2$, and also in other positive characteristics. The above examples are all local algebras.  From these, we can build new examples by taking direct products or extensions. My question is: What are other families of examples where Wedderburn's principal theorem does not hold? The bigger the family the better! The above examples were all I could find in the literature.  Without the assumption that $k$ is a field, there is also the case of endomorphism rings of finite abelian groups, mentionned in Auslander, Reiten and Smalø's book Representation Theory of Artin Algebras .","['abstract-algebra', 'ring-theory', 'examples-counterexamples', 'representation-theory']"
1930743,Show that a function $f:P(X)\to P(X)$ preserving the subset relation has a fixed point,"We have a map $f:P(X)\to P(X)$ , where $P(X)$ means the part of $X$ and the function is  monotone (by considering inclusion "" $\subseteq$ ""). So $\forall \space A\subseteq B $ we have $f(A)\subseteq f(B)$ .
Show that this map has a fixed point. This claim is used in some proofs of Cantor–Schröder–Bernstein theorem , for example, see proof 3 on ProofWiki ( current revision ).","['order-theory', 'fixed-point-theorems', 'elementary-set-theory']"
1930766,finite flat subgroup schemes of smooth group schemes,"Let $G$ be a smooth group scheme over $S$, and let $H\subset G$ be a finite flat closed subgroup scheme (hence it's a Cartier divisor). Let $nH$ be the closed subscheme corresponding to multiplying the Cartier divisor by the integer $n$. Is $nH$ also a subgroup scheme? My intuition is that the answer is no. If that's the case, what breaks?",['algebraic-geometry']
1930795,Showing that if $AB=BA$ then $A$ and $B$ are simultaneously diagonalizable [duplicate],"This question already has an answer here : Prove that simultaneously diagonalizable matrices commute (1 answer) Closed 4 years ago . Suppose $A \in M_n$ has distinct eigenvalues $a_1,\dots,a_n$ and that $A$ commutes with a given matrix $B \in M_n$ so that $AB=BA$ . a. Prove A and B are simultaneously diagonalizable I was able to show that $B$ is diagonalizable but I cannot figure out how to show that $A$ and $B$ are simultaneously diagonalizable.",['linear-algebra']
1930851,"Terrence Tao Measure Theory, hash symbol",This is the first time I encounter this hash symbol. What is it supposed to mean?,"['notation', 'measure-theory']"
1930874,"Extending representation of $\operatorname{GL}(n,\mathbb{Z})$ to $\operatorname{GL}(n,\mathbb{Q})$","Given a representation $\rho\colon\operatorname{GL}(n;\mathbb{Z}) \rightarrow\operatorname{GL}(V)$ on a finite-dimensional, complex vector space $V$ , I am trying to understand a condition that would let me find a representation $$\tilde \rho\colon\operatorname{GL}(n;\mathbb{Q}) \rightarrow \operatorname{GL}(V)$$ with $\tilde \rho|_{\operatorname{GL}(n;\mathbb{Z})} = \rho.$ This is easy enough when $n=1$ . Any representation $\rho$ can be extended to $\mathbb{Q}^{\times}$ by defining $$\rho(x) := \begin{cases} \rho(1): & x > 0; \\ \rho(-1): & x < 0. \end{cases}$$ In other words, pulling back along the sign homomorphism $$\mathrm{sgn} : \mathbb{Q}^{\times} \rightarrow \mathbb{Z}^{\times}, \; x \mapsto \begin{cases} 1: & x > 0; \\ -1: & x < 0. \end{cases}$$ I can't think of a surjective homomorphism $\operatorname{GL}(n;\mathbb{Q}) \rightarrow\operatorname{GL}(n;\mathbb{Z})$ for $n > 1$ so I can't use this argument anymore.","['matrices', 'representation-theory', 'group-theory']"
1930877,Number of sequences of length $m$ such that $1$ is never followed immediately by $0$,"Find the number of sequences of length $m$ consisting of only digits, where each digit is in $\{0,1,2\}$ and such that $1$ is never followed immediately by $0$. Attempt: I tried taking 10 as a block, but I got nowhere. I found the Mathematics question Number of permutations of thet set $\{1,2,...,n\}$ in which $k$ is never followed immediately by $k+1$ . Using that, since I need to remove only 1 followed by 0, I get only one set $A_1$ so, $3^m$ - 6 (3*2). Pick the other number in three ways, and we can permute 10 and other number in two ways). Is this right?
I am from a non-math background; kindly consider this if possible.","['generating-functions', 'permutations', 'combinatorics', 'sequences-and-series', 'discrete-mathematics']"
1930892,Prove that if $X$ is standard normal then $|X|$ and $1_{X>0}$ are independent,"Given $X$ is standard Normal distributed ($X\sim N(0,1)$). Show that $|X|$ and $1_{X>0}$ are independent. My attempt $|X| = X$ for $X>0$, and $|X| = -X$ for $X\leq 0$. Each of these possibilities have $p=\frac{1}{2}$ as $X$ ~ $N(0,1)$. Same probabilities work for $1_{X>0} = 1$ for $X>0$ and $0$ for $X<0$. But now, we need to find $P(|X|\cap 1_{X>0})$. This intersection turns out to be $1$ if $X>0$, and $0$ if $X\leq 0$. We have: $P(1) = P(0)=\frac{1}{2}$ (by definition of $1_{X>0}$). But $\frac{1}{2}$ is not equal to $P(X)P(1)$ for $X>0$ or $X\leq 0$, isn't it? My question I think I messed up something here on the product of the two probabilities. Could someone please help correct it?",['probability-theory']
1930933,is $\sqrt[n]{n!}$ ever an integer?,"Does there exist an $n \in \mathbb{N}$ greater than $1$ such that $\sqrt[n]{n!}$ is an integer? The expression seems to be increasing, so I was wondering if it is ever an integer. How could we prove that or what is the smallest value where it is an integer?","['number-theory', 'factorial', 'radicals', 'elementary-number-theory']"
1930936,"""Statistics"": How many of these begin and end with the letter ""S""?","How many distinct permutations are there of the letters
  in the word “ statistics”? How many of these begin
  and end with the letter s? The first part of the question I do understand. You have to use permutation with identical items. This is based on the number of letters. $$\binom{10}{3,3,1,2,1} = 50400$$ Yet for the second part I am confused as to what the directions means. It says that how many letters begin and ends with letters s so does one eliminate $2$ s and calculate this problem normally? BONUS:
 If so using a similar example how does one find out if how many of these begin and end with the letters m for the word ""mathematicsman"" ?","['permutations', 'statistics']"
1930944,Clarification needed for proof of Theorem $17.2$ from Munkres Topology regarding necessary and sufficient conditions for a set to be closed.,"Without a picture, how do we know: 1.) $(X-C)\cap Y = Y-A$? 2.) $A = Y \cap (X-U)$? Please help me understand these two. Thanks.","['intuition', 'general-topology', 'elementary-set-theory', 'proof-explanation']"
1930946,Why wouldn't this proof be correct for ℘(A ∪ B) = ℘(A) ∪ ℘(B) ↔ (A ⊂ B) ∨ (B ⊂ A) ∨ (A = B),"Let U={1, 2, 3, ... 24, 25, 26, a, b, c, ... x, y, z}, A = {1, 2, 3, ... 24, 25, 26} and B = {a, b, c ... x, y, z}, then → A ∪ B = U → U ⊆ A ∪ B    Definition of a union → U ∈ ℘(A ∪ B)     Definition of a Power Set However,  U ⊈ A ∧ U ⊈ B → U ∉ ℘(A) ∧ U ∉ ℘(B)      Definition of Union & Power Set → ~(U ∈ ℘(A) ∨ U ∈ ℘(B))  DeMorgan's Law → U ∉ ℘(A) ∪ ℘(B)     Definition of Union & Power Set → ℘(A ∪ B) ⊈ ℘(A) ∪ ℘(B)  Definition of a subset ∴ ℘(A ∪ B) ≠ ℘(A) ∪ ℘(B) However, Let U, A and B be arbitrary sets, then → U ⊆ A ∪ B     Definition of a union → U ∈ ℘(A ∪ B)     Definition of a Power Set Then Let (A ⊂ B) ∨ (B ⊂ A) ∨ (A = B) → U ⊆ A ∨ U ⊆ B (Definition of Union & Power Set) → U ⊆ A ∪ B (Definition of Union) → U ∈ ℘(A) ∪ ℘(B) (Definition of Union & Power Set) ∴ ℘(A ∪ B) = ℘(A) ∪ ℘(B) ↔ (A ⊂ B) ∨ (B ⊂ A) ∨ (A = B)","['proof-verification', 'elementary-set-theory', 'discrete-mathematics']"
1930953,Give an example of two distinct sets $A$ and $B$ such that $A \times B = B \times A$,"This is a question from my textbook. The book gives the answer $A = \varnothing$ and $B = \{1\}$. The definition of $A \times B$ is $\{(a,b): a \in A \land b \in B\}$. But if $A = \varnothing$, what could be in it? Even if it's $(\varnothing, 1)$ & $(1, \varnothing)$, how do you compare $\varnothing$ and $1$?",['elementary-set-theory']
1931013,Prove $\sqrt{2}$ is between $\dfrac{a}{b}$ and $\dfrac{a+2b}{a+b}$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $a$ and $b$ be positive integers. Show that $\sqrt{2}$ always lies between $\dfrac{a}{b}$ and $\dfrac{a+2b}{a+b}$. Please give the easy solution as possible.","['algebra-precalculus', 'elementary-number-theory']"
1931025,Complex-Orthogonal Similarity,"First, a quick definition: I say that a matrix $M$ with complex entries is complex-orthogonal if $MM^T = I$.  Note that $T$ here refers to the transpose, not the conjugate transpose .  That is, we are taking the adjoint with respect to a bilinear (as opposed to sesquilinear) dot-product over $\Bbb C$.  A complex-orthogonal matrix need not be unitary, and vice versa (unless of its entries are in $\Bbb R$). Consider the following claim: Claim: For any $A \in \Bbb C^{n \times n}$, there exists a complex-orthogonal $U$ such that $UAU^T$ has equal diagonal entries. My Question: Does this claim hold? It may be helpful to take a look at this paper which proves that for any $A$, there is an $S$ such that $SAS^{-1}$ has equal diagonal entries.  It goes on to show that such an $S$ may be taken to be a unitary matrix .  Another proof of the same fact is presented in p. 77 of R.A. Horn and C.R. Johnson’s Matrix Analysis (1985/7, Cambridge Univ. Press). Notably, both of these proofs exploit facts for which an analog fails to exist in my case.  For Horn and Johnson's proof, we need the fact that the unitary matrices form a compact subset of $\Bbb C^{n \times n}$.  For the proof linked, we need the numerical range to have certain ""nice properties"", which I believe fail in my case. Any insight here is appreciated.","['matrices', 'complex-analysis', 'linear-algebra']"
1931036,"bijection from $(a,b)$ to $\Bbb R$","I need to construct a bijection from an arbitrary interval $(a,b) \to\Bbb R$ . I was thinking of somehow using the tangent function because it's asymptotic, but i'm not sure how to get started.","['elementary-functions', 'general-topology', 'functions']"
1931143,On the existence of semisimple (irreducible) characters of simply connected algebraic groups,"I'm learning Deligne-Lusztig theory of complex characters of finite groups of Lie type and there are some difficulties for me in understanding the theory. Let $\mathcal{G}$ be a simple simply-connected algebraic group over the algebraic closure of a finite field of characteristic $p$ , and $F$ be a Frobenius endomorphism. Let $\mathcal{G^*}$ be a group in duality with $\mathcal{G}$ with corresponding Frobenius endomorphism $F^*$ . (For example, we can take $\mathcal{G}=SL_n(\overline{F_p})$ , $F:(a_{ij})\mapsto (a_{ij}^q)$ , and $\mathcal{G^*}=PGL_n(\overline{F_p})$ , where $q$ is a $p$ -power). Question: Is it true that for every $\mathcal{G^*}^{F^*}$ -conjugacy class of a semisimple element $s\in\mathcal{G^*}^{F^*}$ , there is a semisimple irreducible character $\chi_s \in \mathrm{Irr}(\mathcal{G}^F)$ of degree $[\mathcal{G^*}^{F^*}: C_{{\mathcal{G^*}}^{F^*}}(s)]_{p^{'}}$ ? I wonder whether or not ""the existence"" of such a semisimple irreducible character depends on connectedness of $Z(\mathcal{G})$ or the centralizer $C_\mathcal{G^*}(s)$ . I know that the aswer of the above question is affirmative when $Z(\mathcal{G})$ is connected. But I don't know whether it is true for groups with disconnected center, specially when $\mathcal{G}=SL_n(\overline{F_p})$ . It seems that the above question has been investigated in a classical paper of Lusztig: On the representations of reductive groups with disconnected center . But the paper seems to be unavailable on the web. So I would appreciate it if you provide an accessible resource. Example: I saw the following argument in a paper recently (note that in the following argument $q$ is a power of $p$ and $\mathrm{gcd}(4, q-1)\neq 1)$ : As far as I checked, $C_{PGL_4(\overline{F_p})}(\overline{s})$ in the example above is disconnected! In fact, it has two connected components: one of them is $\frac{C_{GL_4(\overline{F_p})}({s})}{Z(GL_4(\overline{F_p}))}$ and the other one consisting of the images of matrices $\begin{bmatrix} 0&a&0&0\\ b&0&0&0\\0&0&0&c\\0&0&d&0 \end{bmatrix}$ under the canonical projection $GL_4 \rightarrow PGL_4$ . This example shows that the question above might have affirmative answer even if $Z(\mathcal{G})$ and $C_{\mathcal{G^*}}(s)$ are disconnected. But this is just an example. I need a verified reference. Update Description: I found out that I misunderstood the concepts of regular and semisimple characters. So I deleted my previous wrong arguments and rewrite the question in a more efficient format. Now I would be grateful for any help on the question above.","['abstract-algebra', 'characters', 'representation-theory', 'algebraic-groups', 'group-theory']"
1931152,What are all the positive integers $n$ such that $2^n$ ends in $n$?,What are all the positive integers $n$ such that $2^n$ ends in $n$? Example: $2^{36} = 68719476736$ ends in $36$.,['number-theory']
1931172,Which calculus textbook is this?,I found an image of a calculus textbook which implies this textbook must be a fun read. If anyone has gone through this book or remember reading please let me know. Thanks. Here is the image:,"['reference-request', 'book-recommendation', 'calculus']"
1931189,The Concept of Isometry under Riemannian Metric's Context,"While reading the book Modern Geometry — Methods and Applications
Part I , I have a problem reconciling the definition of isometries with the usual version (an isometry preserves the distance between two points). Quoted from the book, an isometry (or a motion of the given metric ) is a transformation preserving the Riemannian metric (Def. 4.1.1., page 24). That is, $x^i = x^i(z^1, \dots, z^n)$ is called an isometry if $g'_{ij}(z^1, \dots, z^n) = g_{ij}(x^1(z), \dots, x^n(z))$, where the $g_{ij}$ and $g'_{ij}$ are Riemannian metrics of corresponding coordinates systems. My problem is, how is the concept of distance connected with that of $g_{ij}$? Assuming that the distance between $x_1 = (x_1^1, \dots, x_1^n)$ and $x_2 = (x_2^1, \dots, x_2^n)$ is defined as $d(x_1, x_2) = \sqrt{\left< \xi, \xi \right>} = \sqrt{\sum_{i, j} g_{ij}(x_1) \xi^i \xi^j}$, where $\xi$ is the vector originating at $x_1$ and terminating at $x_2$. Let $z_1, z_2$ be points corresponding to $x_1, x_2$ under an arbitrarily chosen transformation, respectively, and let $\eta = \vec{z_1z_2}$. Then, by the definitions of vector and inner product in this book, $d(z_1, z_2) = \sqrt{\left< \eta, \eta \right>} = \sqrt{g'_{ij}(z_1) \eta^i \eta^j} = \sqrt{ \left[\frac{\partial x^k}{\partial z^i} g_{kl}\frac{\partial x^l}{\partial z^j}\right]_{z = z_1} \eta^i \eta^j} = \sqrt{g_{ij}(x_1) \xi^i \xi^j} = d(x_1, x_2)$. This suggests that every transformation is an isometry, which is obviously absurd. Thus, either my assumed definition of distance is false or I have misunderstood the definitions of vectors and inner products. Can anyone point out my fallacy? *This question is coming out when I am tackling with an exercise stating that every isometry of Euclidean n-space is affine , for which I have known a proof like in page 7~8 of this document . Unfortunately, I am unable to ""translate"" it into the context of Riemannian metric due to my shortage in concepts of distance.",['differential-geometry']
1931192,Playing with numbers form $1$ to $n^2$ in an $n\times n$ grid,"Suppose we write each number from $1$ to $n^2$ in an $n\times n$ grid exactly once. Prove there are two adjacent cells such that the absolute value of their difference is not less than $n$. I think it should be a pigeonhole problem, but the work I did so far was basic observations like if we assume all differences are less than $n$, then neither of $1,2,\dots,2n-2$ could be in the row or the column $n^2$ is placed. Any hints would be appreciated.","['combinatorics', 'contest-math']"
1931193,Convergence of series based on iteratively applying the logarithm function [duplicate],"This question already has an answer here : Does the series converge (1 answer) Closed 6 years ago . Context From the Riemann criterion, we know that $$\sum \frac 1{n^s}$$ converges when $s>1$ . Continuing in the same path, we know thanks to Bertrand series that $$\sum \frac 1{n\ln (n)^s}$$ converges when $s>1$ . And so on, i.e. if we note $\ln^{(k)}$ the $k$ -th iteration of the logarithm: $$\ln^{(k)}:=\underbrace{\ln\circ\ln\circ\cdots\circ\ln}_{k \text{ times}}$$ then for all $\ell$ $$\sum \left(\ln^{(\ell)}(n)^s\prod_{k=0}^{\ell-1} \ln^{(k)}(n)\right)^{-1}$$ converges when $s>1$ . The problem So I was wondering what happens at the limit . Does it go infinity or does it converge ? Let's consider the following series: $$\mathscr S:=\sum_{n=1}^\infty \left(\prod_{\substack{k=0 \\ \ln^{(k)}(n)\geqslant 1}}^{\infty} \ln^{(k)}(n)\right)^{-1},$$ where the apparently infinite product is in fact finite for all $k$ . Basically, we take as much logarithm as we can so it doesn't get smaller than $1$ . Concretely We have $\ln^{(0)}(1)=1\geqslant 1$ and $\ln^{(1)}(1)=0< 1$ so we stop there. And $\ln^{(0)}(2)=2\geqslant 1$ and $\ln^{(1)}(2)\approx 0.69< 1$ so we stop there. And $\ln^{(1)}(3)\approx 1.1\geqslant 1$ and $\ln^{(2)}(3)\approx 0.1< 1$ so we stop there. $\vdots$ And $\ln^{(1)}(15)\approx 2.7\geqslant 1$ and $\ln^{(2)}(15)\approx 0.996< 1$ so we stop there. And $\ln^{(2)}(16)\approx 1.02\geqslant 1$ and $\ln^{(3)}(16)\approx 0.02< 1$ so we stop there. $\vdots$ So it starts like this: $$\mathscr S=\frac 11+\frac 1{2}+\frac 1{3\ln (3)}\ldots+\frac 1{15\ln (15)}+\frac 1{16\ln (16)\ln\ln (16)}+\ldots.$$ The question Does $\mathscr S$ converge ? What could work We can prove Riemann criterion and Bertrand's one using the Cauchy condensation test: $$\sum f(n)<\infty \iff \sum 2^nf(2^n)<\infty.$$ But I didn't get anywhere. We can also notice that since we want $n$ such that $$1\leqslant \log^{(k)}(n)<e$$ we want $n$ sucht that $$e^{(k)}\leqslant n < e^{(k+1)}.$$ So we can rewrite the series: $$\mathscr S:=\sum_{k=0}^\infty \sum_{n=[e^{(k)}]+1}^{[e^{(k+1)}]}\left(\prod_{\ell=0}^{k} \ln^{(\ell)}(n)\right)^{-1}.$$ Therefore, if we take $n\in \{[e^{(k)}]+1, \ldots, [e^{(k+1)}]\}$ we have: $$\sum_{n=[e^{(k)}]+1}^{[e^{(k+1)}]}\left(\prod_{\substack{\ell=0 \\ \ln^{(k)}(n)\geqslant 1}}^{k} \ln^{(\ell)}(n)\right)^{-1}\leqslant \frac{e^{(k+1)}-e^{(k)}}{\displaystyle\prod_{\ell=0}^{k} \ln^{(\ell)}(e^{(k+1)})}=\frac{e^{(k+1)}-e^{(k)}}{\displaystyle\prod_{\ell=0}^{k} e^{(\ell)}}.$$ Now we only need to understand whether or not $$\sum_{k=0}^{\infty} (e^{(k+1)}-e^{(k)})\left(\prod_{\ell=0}^{k} e^{(\ell)}\right)^{-1}$$ converges.","['divergent-series', 'sequences-and-series']"
1931222,Why are these limits finite? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I am solving a test and have trouble solving this particular task: given a differentiable function $f : \mathbb{R} \rightarrow \mathbb{R} $ and $ f(0) = f(1) = 0$, are the following limits finite: $$
(a) \lim_{n \rightarrow +\infty} nf\left(\frac{1}{n}\right)
$$ $$
(b) \lim_{n \rightarrow +\infty} nf\left(\frac{n+1}{n}\right)
$$ $$
(c) \lim_{n \rightarrow +\infty} nf\left(\frac{n}{n+1}\right)
$$ Why are those limits finite (I know they all are)?","['derivatives', 'limits']"
1931227,$X_n \to 0$ in probability iff $E\left( \frac{|X_n|}{1+|X_n|}\right) \to 0$,"Good day, In class we had the exercise as stated in the title: Show that $X_n \to 0$ in probability for $n\to \infty$ iff $E\left( \frac{|X_n|}{1+|X_n|}\right) \to 0$ for $n \to \infty$. And the solution is the following: $""\Leftarrow""$ Assume that $E\left( \frac{|X_n|}{1+|X_n|}\right)\to 0$ and let $\epsilon>0$. Then, according to Markovs inequality, we have 
$$P(|X_n|>\epsilon)\color{red}{=}P\left(\frac{|X_n|}{1+|X_n|}>\frac{\epsilon}{1+\epsilon}\right) \leq \frac{\epsilon}{1+\epsilon} E\left( \frac{|X_n|}{1+|X_n|}\right) \to 0 $$ $""\Rightarrow""$ Assume $X_n \to 0$ in probability, then
$$E\left( \frac{|X_n|}{1+|X_n|}\right)\color{orange}{\leq}\frac{\epsilon}{1+\epsilon}\cdot P(|X_n|\leq \epsilon)+1\cdot P(|X_n|>\epsilon) \to \frac{\epsilon}{1+\epsilon} \to 0$$ To be honest I have some problems with the solution. They are as following: $""\color{red}{=}""$ How? My try: $P(|X_n|>\epsilon)=P\left(\frac{|X_n|}{1+|X_n|}>\frac{\epsilon}{1+|X_n|}\right)$ but how do I get this to above formulation? $""\color{orange}{\leq}""$ It seems a bit like $E(X)=\sum x_i p_i$ but I don't get how this inequality is proved. As a hint to the exercise there was: $f(u)=\frac{u}{1+u}$ is strictly increasing on $[0,\infty)$. So this should come into play somewhere in the answer to my questions I guess. Can someone please hep me with these two questions? Thanks a lot,
Marvin","['probability-theory', 'probability', 'proof-verification']"
1931236,Convergent or divergent: $\sum_{k=1}^{\infty}\frac{2^{k}\cdot k!}{k^{k}}$ [duplicate],"This question already has answers here : Simple series convergence/divergence: $\sum_{k=1}^{\infty}\frac{2^{k}k!}{k^{k}}$ (4 answers) Closed 7 years ago . Convergent, absolutely convergent or divergent:
  $$\sum_{k=1}^{\infty}\frac{2^{k}\cdot k!}{k^{k}}$$ I have used ratio test because we got a fraction here and I think I did alright till the end: $$\lim_{k\rightarrow \infty}\frac{\frac{2^{k+1}\cdot (k+1)!}{(k+1)^{k+1}}}{\frac{2^{k}\cdot k!}{k^{k}}} = \lim_{k\rightarrow \infty}\frac{2^{k+1}\cdot (k+1)! \cdot k^{k}}{(k+1)^{k+1}\cdot 2^{k}\cdot k!} = \lim_{k\rightarrow \infty}\frac{2^{k}\cdot 2^1\cdot k! \cdot (k+1)\cdot k^{k}}{(k+1)^{k}\cdot (k+1)\cdot 2^{k}\cdot k!} = \lim_{k\rightarrow \infty}\frac{2k^{k}}{(k+1)^{k}}$$ $$=2\lim_{k\rightarrow \infty} \left( \frac{k}{k+1}\right)^{k}$$ Now I don't know (without a calculator) to what this would converge / diverge to... In the exam we are not allowed to use a calculator... So what to do? The denominator will be greater than the enumerator by 1, so dividing each other we got something $<1$. We take exponent $k$ which is $\geq 1$, so we will end up with $<1$ again. Multiply this with 2 we get something $< 1$ but $> 0$ and thus the series is convergent...? I hope I have described it well? Would you give me full points to this task? :D Edit: I haven't described it well in the end. See the accepted answer and its comments! Thanks a lot to everyone - from every question I ask here, I always learn new things :-)","['convergence-divergence', 'sequences-and-series', 'calculus', 'analysis']"
1931246,"Line bundles on Grassmannians $Gr(m, n)$ and $Gr(n-m, n)$","Let $Gr=Gr(m, V)$ be a Grassmannian of $m$-dimensional vector subspaces in the $n$-dimensional vector space $V$. There is a Plücker embedding $p_1: Gr \hookrightarrow \mathbb P(\Lambda^m V)$ mapping an $m$-dimensional vector subspace $U \subset V$ to the point $p_1(U)=\mathbb P(\Lambda^m U)$. It gives a series of line bundles $p_1^* \mathcal O(k)$. One can prove that they are the only line bundles on $Gr$, thus $Pic(Gr)=\mathbb Z$. But there is another way to see $Gr$: as $Gr(n-m, V^*)$, replacing $U \subset V$ by $\ker(V^* \to U^*) \subset V^*)$. It gives another Plücker embedding $p_2: Gr \hookrightarrow \mathbb P(\Lambda^{n-m} V^*)$ and another series $p_2^* \mathcal O(l)$ of line bundles. But Picard group is the same. So what $p_2^* \mathcal O(l)$ corresponds to $p_1^* \mathcal O(k)$?","['grassmannian', 'projective-geometry', 'algebraic-geometry']"
1931248,"Calculate the ""inverse"" of the Ricci tensor",Let $R_{\mu\nu}$ be the coordinates of the Ricci tensor. Using that $R^{\mu\alpha}R_{\alpha\nu}=\delta_\nu^\mu$ and $R_{\mu\nu}=R_{\nu\mu}$ it is possible to get the expressión of $R^{\mu\nu}$? Many thanks!,"['tensors', 'differential-geometry']"
1931287,What is the saturation of the function?,"In the field of deep learning, people often use the term function saturation . There are many examples, but here is a one related to sigmoid : The initial stage of growth is approximately exponential; then, as
  saturation begins, the growth slows, and at maturity, growth stops None of the books/tutorials/papers explain what is the definition of the saturation. I also have not found anything by googling the term. It is not hard to guess that it means that with the growth of $x$, the $y$ grows significantly slower (almost does not grow at all). My another idea is that this is something similar to horizontal asymptote. So my questions are: Is there such a term function saturation or something similar to it? If so, what is the definion can I say that $\arctan$ saturates (what about $\tan$ or $arcsec$)?","['terminology', 'functions']"
1931313,Show that function with compact support are integrable.,"I'm trying to show that measurable function with compact support are integrable. Can I do as following : Let $f$ with compact support. Then there is $K$ compact s.t. $supp(f)\subset K$. Then, there is $M$ s.t. $|f|<M$ (because the support is compact) and thus $$\int |f|<M\int_K=Mm(K)<\infty $$
where $m$ is Lebesgue measure. Does it work ?","['lebesgue-integral', 'measure-theory']"
1931320,Prove that $\|A\|_2 = \sqrt{\|A^* A \|_2}$,"This is a homework exercise, so hints are welcome. Prove $\|A\|_2 = \sqrt{\|A^* A \|_2}$. The $\|\cdot\|_2$ is the induced 2-norm, and $A \in M_{n,m}(\mathbb{C})$. $A^*$ is the complex conjugate. Note that the matrix $2$-norm is defined as $$
 \sup_x\{\|Ax\|_2 \,\big|\,\|x\|_2 \leq 1 \}\text{,}
$$ where $\|x\|_2$ is the Euclidean norm. So far, I tried expanding the norms, resulting in $$
\sup_x\{\sum_i|(Ax)_i|^2\,\big|\, \|x\|_2 \leq 1 \} = \sup_x\{\sqrt{\sum_i |(A^* A x)_i|^2} \,\big|\,\|x\|_2 \leq 1 \}\text{.}
$$ Unfortunately, I don't have a clue where to go from here.","['matrices', 'normed-spaces', 'numerical-linear-algebra', 'linear-algebra']"
1931322,Finding mixed strategy Nash equilibria for a game with infinite strategies,"I run into this problem in my attempt to answer another question on math.stackexchange (In the original problem the strategies can be made discrete and finite, but I am interested in the infinite version here) We have a 2-player game with an infinite set of strategies. The strategies are essentially the same basic strategy with a parameter that tweaks it. The parameter takes values between $0$ and $1$, hence infinite strategies are born. Let's name these strategies $H_a$ where $a$ is the parameter that can change. We also know the payoff function that provides the gain of one strategy over another. Let's denote the gain of strategy $H_b$ over strategy $H_a$ as $G(H_b, H_a)$. The payoff is symmetrical, so one's gain is the other one's loss. For our particular game the payoff function looks like this
$$G(H_b, H_a) = \begin{cases}
G^-(H_b, H_a), & \text{if $a \le b$} \\
G^+(H_b, H_a), & \text{if $a>b$}
\end{cases}$$ with
$$G^-(H_b, H_a) = b\left[\frac{a(b-a)}{b} - (1-a)\frac{a-b+2}{2}\right] + (1-b)\left[a + (1-a)\frac{2(b-a)}{1-a}\right]$$
$$G^+(H_b, H_a) = b\left[\frac{a(b-a)}{a} - (1-a)\right] + (1-b)\left[a\cdot\frac{b-a+2}{2} + (1-a)\frac{2(b-a)}{1-b}\right]$$ Simplifying them so that their quadratic form becomes apparent:
$$G^-(H_b, H_a) = (a^2b - ab^2 + 5ab - 2a^2 - 3b^2 +2b -2a)/2$$
$$G^+(H_b, H_a) = (a^2b - ab^2 - 5ab + 3a^2 + 2b^2 +2b -2a)/2$$ From either of these formulas we can also calculate that $G(H_a, H_a)=0$ There is no dominant strategy. For any strategy $H_a$ you can find strategies that are better, and strategies that are worse. To get a better a sense have a look at the following two graphs. They show the gain of two strategies $H_{0.5}$ and  $H_{0.8}$ against any possible strategy. The $x$ axis on both graphs is just the parameter $a$ going from $0$ to $1$ while the $y$ axis is $G(H_{0.5}, H_a)$ for the graph on the left and $G(H_{0.8}, H_a)$ for the graph on the right. You can notice for example that $H_{0.5}$ is better than $H_{0.2}$, $H_{0.8}$ is better than $H_{0.5}$, but $H_{0.2}$ is better than $H_{0.8}$. How do we find the mixed strategy Nash equilibrium for our problem? In other words, what is the p.d.f. $f_M$ of random variable M that we use to define our Nash Equilibrium mixed strategy $H_M$? When I was first faced with this problem I did not have any formal knowledge of mixed strategies or Nash equilibria, but I did try intuitively the simplest mixed strategy: $M$ being uniformly distributed in the interval $[0,1]$. I realised that this does not lead to an equilibrium and then I stopped because I was not sure whether this meant that no equilibria exist, or that I needed to find a different distribution for $M$. I read a bit on game theory and mixed strategy equilibria before posting this question and now I believe that a mixed strategy should exist . The infinite number of strategies might complicate things but this is how I am seeing the problem: Since this is a symmetrical zero-sum game I should find the distribution of $M$ such that whatever strategy $H_a$ my opponent chooses, my expected gain will be zero. More formally, we need to find the p.d.f. for $M$ such that $\underset{M}{\mathbb E}[G(H_M, H_a)]=0, \forall a \in [0,1]$. (where $\underset{X}{\mathbb E}[F(X)]$ is the expected value of function $F()$ over random variable $X$ ) Is this right? If so, then I believe we can proceed like so: $$\underset{M}{\mathbb E}[G(H_M, H_a)]=0, \forall a \in [0,1] \Leftrightarrow$$
if $f_M$ is the probability density function of random variable M we get 
$$\int_0^1 G(H_x, H_a)f_M(x)dx =0, \forall a \in [0,1] \Leftrightarrow$$ $$\int_0^a G^+(H_x, H_a)f_M(x)dx + \int_a^1 G^-(H_x, H_a)f_M(x)dx=0, \forall a \in [0,1] \Leftrightarrow$$ I am stuck at this point, I am not sure how to solve this parametric differential equation. If an analytic solution is not possible, can I apply numerical methods? Finally, a small side note because I got it wrong initially: In the equation above notice how $G^+$ goes with the integral that spans $[0,a]$, and how $G^-$ goes with the integral that spans $[a,1]$. This is because gain is seen from the viewpoint of the first argument of $G()$. So if the first argument is $H_x$, with $x \le a$, while the second argument is $H_a$, then we should use the $G^+$ branch/case.","['game-theory', 'nash-equilibrium', 'ordinary-differential-equations']"
1931487,Understanding the use of continuity and 'derivative change' in finding Green functions,"Below is a university question and the corresponding solution for which I do not understand small parts of: Question: Show that the Green’s function for the range $x \ge 0$, satisfying $$\frac{\partial^2 G(x,z)}{\partial x^2}+G(x,z)=\delta(x-z)$$ with boundary conditions:$$G(x,z)=\frac{\partial G(x,z)}{\partial x}=0\quad\text{at}\quad x=0$$ is $$G(x,z)=\begin{cases}\cos z\sin x-\sin z\cos x, &\text{for} & x\gt z \\ 0 &\text{for} & x\lt z\end{cases}$$ Solution: For $x \ne z$ 
  $$\frac{\partial^2 G(x,z)}{\partial x^2}+G(x,z)=0$$ with solution
  $$G(x,z)=\begin{cases}A(z)\sin x+B(z)\cos x, &\text{for} & x\lt z \\ C(z)\sin x+D(z)\cos x &\text{for} & x\gt z\end{cases}$$
  $$G(x=0,z)=0\implies B(z)=0$$ The derivative is (for $x\le z$)
  $$\frac{\partial G(x,z)}{\partial x}=A(z)\cos x$$
  Since this is zero at $x=0$, we conclude that $A(z)=0$. Hence, $$G(x,z)=0\qquad (x\lt z)$$
  $\color{red}{\mathrm{For}}$ $\color{red}{x\gt z,}$ $\color{red}{\text{we use continuity of}}$ $\color{red}{G(x,z)}$ $\color{red}{\mathrm{at}}$ $\color{red}{x=z,}$ $\color{red}{\mathrm{ie.}}$
  $$\color{red}{C(z)\sin z+D(z)\cos z=0\tag{1}}$$
  $\color{red}{\text{The first derivative changes by}}$ $\color{red}{1,}$ $\color{red}{\text{so since}}$ $\color{red}{G(x,z)=0}$ $\color{red}{\text{for}}$ $\color{red}{x\lt z,}$
  $$\color{red}{C(z)\cos z -D(z)\sin z=1\tag{2}}$$
  We can eliminate $D$ (or $C$) from equations $(1)$ and $(2)$:
  $$C(z)\cos z + C(z)\frac{\sin^2z}{\cos z}=1\tag{3}$$
  Multiplying equation $(3)$ by $\cos z$ gives $$C(z)\cos^2 z+C(z)\sin^2 z=\cos z$$ and since $\cos^2 z+\sin^2 z=1$,
  $$C(z)=\cos z$$ and we then find $$D(z)=-\sin z$$
  Thus,
  $$G(x,z)=\begin{cases}\cos z\sin x-\sin z\cos x, &\text{for} & x\gt z \\ 0 &\text{for} & x\lt z\end{cases}$$
  $\fbox{}$ I understand everything apart from the part marked red; Why is equation $\color{red}{(1)}$ equal to zero? What does this have to do with continuity? Also equation $\color{red}{(1)}$ has products: $C(z)\sin z$ and $D(z)\cos z$ since $C(z)$ and $D(z)$ are both functions of $z$. So why isn't the product rule being used to yield $$C'(z)\sin z + C(z)\cos z + D'(z)\cos z -D(z)\sin z\text{?}$$ Why is it that the ""first derivative changes by $1$""?","['derivatives', 'intuition', 'continuity', 'greens-function', 'proof-explanation']"
1931509,How to find area under sines without calculus?,"In the section establishing that integrals and derivatives are inverse to each other, James Stewart's Calculus textbook says (pp325--pp326, Sec.4.3, 8Ed): When the French mathematician Gilles de Roberval first found the area under the sine and cosine curves in 1635, this was a very challenging problem that required a great deal of ingenuity. If we didn’t have the benefit of the Fundamental Theorem of Calculus, we would have to compute a difficult limit of sums using obscure trigonometric identities. It was even more difficult for Roberval because the apparatus of limits had not been invented in 1635. I wonder how Gilles de Roberval did it. Wikipedia and MacTutor do not contain much info on that. How to apply the method of quadrature is exactly the real challenge I suppose. This is mainly a history question, but I'm also curious as to how one would approach this in modern days. Thank you.","['math-history', 'trigonometry', 'calculus', 'algebra-precalculus', 'sequences-and-series']"
1931541,"Which version of the total derivative formula is ""more correct""?","For $y=f(t,u_1,\ldots,u_m)$ with $u_i$ depending on $t$, i.e. $y$ not only directly but also indirectly depending on $t$: Wolfram says: $\frac{\partial y}{\partial t}=\frac{\partial f}{\partial t}+\frac{\partial f}{\partial u_1}\frac{\partial u_1}{\partial t}+ \ldots+\frac{\partial f}{\partial u_m}\frac{\partial u_m}{\partial t}$ while Wikipedia says: $\frac{df}{dt}=\frac{\partial f}{\partial t}+\frac{\partial f}{\partial u_1}\frac{du_1}{dt}+ \ldots+\frac{\partial f}{\partial u_m}\frac{du_m}{dt}$ Is it not unambiguous at Wolfram that partial and total derivative are used with the same symbols? From what I have read, total derivatives take into account the indirect effects of $t$, while partial derivatives do not. Even if the expression left to the equal sign here could be chosen freely (i.e. $\frac{df}{dt} \equiv \frac{\partial y}{\partial t}$): Does Wolfram not disregard that also $u_i$ could be some function that, again, depends indirectly on $t$? If I understand correctly, the partial derivative would neglect that, the total would not. Is this correct? Is Wolfram unprecise here because it uses $\frac{\partial u_1}{\partial t}$ instead of $\frac{du_1}{dt}$? Also, the Wolfram expression for the total derivative confuses me... Using the partial derivative on a variable that is the result of a function seems even more unprecise. $\frac{\partial y}{\partial t}$ is the total derivative and $\frac{\partial f}{\partial t}$ is the partial derivative... what is the intuition/reasoning for this?","['derivatives', 'partial-derivative']"
1931575,Solving an $n$-dimensional integral [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $p> - 1$. Find the integral
$$ \int\limits_{(0,1)^n} \left( \frac{\min(x_1,...,x_n)}{\max(x_1,...,x_n)} \right)^p dx_1...dx_n. $$","['real-analysis', 'integration', 'calculus']"
1931605,Simplifying inverse trigonometric expressions such as $\cos^{-1}\left(\frac{x^2-1}{x^2+1}\right)+\tan^{-1}\left(\frac{2x}{x^2-1}\right)$,"Okay so I'm just looking for a short cut method or a method that is not so long for simplifying expressions like this $$\cos^{-1}\left(\frac{x^2-1}{x^2+1}\right)+\tan^{-1}\left(\frac{2x}{x^2-1}\right)$$ Here is how I do it: I take the first term of the equation and convert it to $\tan^{-1}$ form, and then I apply the formula of $\tan^{-1}a +   \tan^{-1}b$. But this method is really time consuming; plus, it results in mistakes most of the times. Any alternatives or how you would do it, let me know, I'll be grateful. Thanking in anticipation. Edit. Do I put $x= \tan y$ and solve? That should make it easy.",['trigonometry']
1931620,"$\mathbb{R}$ is uncountable, Abbott's proof","There is this proof in Abbott's book (Understanding Analysis) on page 25, that I am failing to understand. Theorem is that $\mathbb{R}$ is uncountable. And here is how the author proceeds to prove it (I know there is an easier proof by Cantor): To show that a set is uncountable, we need to show that there is no 1-1,onto function of the form: $f:\mathbb{N} \rightarrow \mathbb{R}$.
The author uses proof by contradiction, therefore assume there is a 1-1, onto function. This implies that no two ""input"" values from $\mathbb{N}$ will map us to the same value in $\mathbb{R}$ (1-1) and also that every element maps onto a unique value in $\mathbb{R}$, i.e. $x_1=f(1)$,$x_2=f(2)$ and so on, so we can write: $$\mathbb{R} = \{x_1,x_2,x_3, ... \}$$ Would be the set of all the reals. Now the author proceeds to use the Nested Interval Property (Theorem 1.4.1 in the book) to produce a real that is not in the list. Let $I_1$ be a closed interval that does not contain $x_1$ and $I_2$ be a closed interval that is contained in $I_1$ and which does not contain $x_2$. (1) My first understanding hurdle : ""Certainly $I_1$ contains two smaller disjoint closed intervals"", I cannot see which two disjoint intervals he has in mind . So I am starting to struggle at this point. He then proceeds to say that $x_{n_0}$ is some real number from the list, then $x_{n_0} \notin I_{n_0}$ (fair enough, this is due to the way we constructed the intervals) and he then proceeds saying that: $$x_{n_0} \notin \bigcap_{n=1}^{\infty}I_n$$ I do not see where the $I_{n_0}$ is in the above... The above has an intersection of the following intervals: $I_1,I_2,I_3,..$ and there is no $I_{n_0}$. (2) Finally , by construction there is a real that is not in the interval, $x_n \notin I_n$ is not in there at all. So by definition, we are omitting a real number from the interval, even though we are meant to prove it (I realize that this is an interval in the first place; My thinking was that if we prove it on the interval then we can extend the conclusions from that interval to the whole of the real line; But if we are omitting a real from the number line, then how can we make any conclusions at all).",['real-analysis']
1931631,Why am I getting different results while applying similarity and Pythagoras theorem?,"AE=7.2 cm,AD=7.6 cm,BE=4.2 cm and BC=8.4 cm. Now,find DE. So,if we apply Pythagoras theorem in AED we have $ED=\sqrt {7.6^2-7.2^2}=2.433$. But,if we apply similarity between AED and ACB, we have  $\frac {AD}{DE}=\frac {AB}{BC}$ and thus solving we get DE=5.6 cm. So,why this inconsistency in results?",['geometry']
1931633,Integers $n$ such that $n^2$ is of the form $3k+2$,"Find all integer $n$ such that $n^2$ is of the form $$3k+2.$$ In observing I find that it has no integer solution, but I can't solve it mathematically. Any hint regarding this is appreciated. Thanks in advance.","['algebra-precalculus', 'induction', 'gcd-and-lcm']"
1931643,Every discrete subgroup is unimodular,"Let $G$ be a locally-compact topological group. Let $\Gamma$ be a discrete subgroup [Perhaps $G$ is hausdorff in order for $\Gamma$ to be closed and therefore have a haar-measure, but It was not mentioned in the origin book].
A book I'm currently reading states trivially that $\Gamma$ must be unimodular, but I fail to understand it. It is clear to me that finite sets $E_{1,2}\subset\Gamma$ with $|E_1|=|E_2|$ must have the same measure (trivial for singletons by left-invariance of $\mu_\Gamma$, and by additivity also for finite sets). therefore $\mu_\Gamma(Eh)=\mu_\Gamma(E)$ for all finite $E\subset \Gamma$). Now, if $\Gamma$ is finite we are done.
If it is countable, we will get by finiteness of haar measure that the measure must be the zero measure, which is a troubling result by itself.
If it is more than countable, I don't know what to do but maybe follow the same argument for countable and get the same contradiction. p.s: I also tried using outer-regularity, but it seemed much less useful.","['topological-groups', 'measure-theory', 'haar-measure']"
1931685,Are the iterates of $f(x) = \frac{3}{2-x}$ bounded?,"Let $\displaystyle u_0=\frac 32$ and $\displaystyle u_{n+1} = \frac{3}{2-u_n}$ Is $(u_n)$ bounded ? Since $f(x) = \frac{3}{2-x}$ has no fixed point, $(u_n)$ does not converge. Observation with a computer suggests that this sequence is not bounded. Indeed, it seems that there are terms that get closer to $2$ as $n$ grows. (e.g $u_{829}\approx 2.001)$ Is there a way to prove this formally ? I haven't been able to make significant progress with this question, so any help is appreciated. Progress: with $\alpha = 1 + \frac{i}{2\sqrt 2}$ and $\theta  = \arg (1+i\sqrt 2)$ , I've proved that $$u_n = \sqrt 3 \frac{\operatorname{Re}(e^{i(n-1)\theta} \alpha)}{\operatorname{Re}(e^{in\theta} \alpha)}$$ Is that any good ?","['recurrence-relations', 'real-analysis', 'sequences-and-series']"
1931705,Solve $\cot^2x=\csc x$ in degrees,"$$\cot^2x = \csc x$$
I have to solve for $x$ in degrees. Here's what I did:
$$\cot^2x=\csc^2x - 1$$
$$\csc^2x -\csc x - 1 = 0$$
If $y=\csc x$:
$$y^2-y-1=0$$
and now I cannot proceed to the given answers of 38.2° and 141.2°.","['algebra-precalculus', 'trigonometry']"
1931724,"A different way to prove that $\int_{1+\sqrt{2}}^{\infty} \frac{\ln(1+x)}{1+x^{2}} \, dx = \frac{3 G}{4} + \frac{\pi}{16} \, \ln 2$","From the fact that $$\int_{0}^{1} \int_{0}^{1} \frac{1}{2-x^{2}-y^{2}} \, dx \, dy $$ is an integral representation of Catalan's constant ($G$) , I was able to deduce that $$\int_{1+\sqrt{2}}^{\infty} \frac{\ln(1+x)}{1+x^{2}} \, dx = \int_{0}^{\pi/8} \ln(1+ \cot u) \, du = \frac{3G}{4} + \frac{\pi}{16} \, \ln 2\tag{1}.$$ What is another way to prove $(1)$ that preferably doesn't involve the dilogarithm function ? EDIT : In response to Dr. MV's comment, the following is how I deduced $(1)$ from that integral representation of Catalan's constant. $$ \begin{align}  G&= \int_{0}^{1} \int_{0}^{1} \frac{1}{2-x^{2}-y^{2}} \, dx \, dy \\ &= \int_{0}^{1} \frac{1}{2-y^{2}} \int_{0}^{1} \frac{1}{1-\frac{x^{2}}{2-y^{2}}} \, dx \, dy \\ &= \int_{0}^{1} \frac{1}{\sqrt{2-y^{2}}} \,  \text{artanh} \left(\frac{1}{\sqrt{2-y^{2}}} \right) \, dy \\ &= \int_{0}^{\pi/4} \text{artanh} \left(\frac{1}{\sqrt{2} \cos \theta} \right) \, d \theta \\ &= \frac{1}{2} \int_{0}^{\pi/4} \ln \left(\frac{\sqrt{2} \cos \theta +1}{\sqrt{2} \cos \theta -1} \right) \, d \theta \\ &= \frac{1}{2} \int_{0}^{\pi/4} \ln \left(\frac{(\sqrt{2} \cos \theta+1)^{2}}{2  \cos^{2} \theta -1} \right) \, d \theta \\ &=\int_{0}^{\pi/4} \ln (\sqrt{2} \cos \theta +1) \, d \theta - \frac{1}{2} \int_{0}^{\pi/4} \ln(\cos 2 \theta) \, d \theta \\ &= \int_{0}^{\pi/4} \ln\left(\sqrt{2} \cos \left(\frac{\pi}{4}  - \phi\right)+1\right) \, d \phi - \frac{1}{4} \int_{0}^{\pi/2} \ln( \cos \tau) \, d \tau \\ &= \int_{0}^{\pi/4} \ln \left(\sin(\phi) + \cos(\phi)+1\right) \, d \phi - \frac{1}{4} \left(- \frac{\pi}{2} \,  \ln 2 \right) \\ &= \int_{0}^{\pi/4} \ln (\sin \phi) \, d \phi + \int_{0}^{\pi/4} \ln \left(1+ \frac{1+ \cos \phi}{\sin \phi} \right) \, d \phi + \frac{\pi}{8} \,  \ln 2 \\ &= - \frac{G}{2} - \frac{\pi}{4} \,  \ln 2 + \int_{0}^{\pi/4} \ln \left(1+ \cot \frac{\phi}{2} \right) \, d \phi + \frac{\pi}{8} \, \ln 2 \\ &=  - \frac{G}{2} - \frac{\pi}{8} \, \ln 2 + 2 \int_{0}^{\pi/8} \ln (1 + \cot u) \, du \end{align}$$","['integration', 'definite-integrals']"
1931769,Calculate the upper boundary of the given sum,"How to find upper boundary of $\sum^{\infty}_{k = 1}\frac{1}{k^{3/2}}$? I know that it can be solved using integrals, but can someone explain how this sum is connected with integral of $f(x) = \frac{1}{k^{3/2}}$?","['limits', 'supremum-and-infimum', 'functions', 'integration', 'summation']"
1931771,Find sum of first $n$ terms of the series : $1+\frac{1^3+2^3}{1+2}+\frac{1^3+2^3+3^3}{1+2+3}+\dots$,"The main question is: Find sum of first $n$ terms of the series : $1+\frac{1^3+2^3}{1+2}+\frac{1^3+2^3+3^3}{1+2+3}+\dots$ My approach: Initially, nothing clicked, so I went forward with simplifying the series.
So, we get, after simplifying, $S$(The sum of series) = $1+3+6+10+15+21+\dots$ So, I try to write $S$ as, $1+(4-1)+(9-3)+(16-6)+\dots$ So what I finally get is, $S$=$(1+4+9+16+25+\dots) - (1+3+6+10+\dots)$ Therefore, $$2S=\sum_{n=1}^n{n^2}$$ Thus, $S$ = $\frac{n(n+1)(2n+1)}{12}$ But, the answer given in my textbook is $\frac{n(n+1)(n+2)}{6}$ My answer is not matching. Please help me by pointing out my mistake or providing a new approach and solution altogether.","['algebra-precalculus', 'sequences-and-series']"
1931797,Why is Area of a triangle like this?,I had just now seen that the following is actually an axiom. $$\text{The area of a triangle is }\;\frac 12\text{ base } \times \text{ height }$$ But how did we come up with the idea that the area of a triangle is given as by that? Why not something else?,"['axioms', 'area', 'triangles', 'geometry']"
1931798,Do limits evaluated at infinity exist?,"Here is some limit:
$$\lim_{x \to b} f(x)$$ We know that for a limit to exist, we must have
$$\lim_{x \to b+} f(x) = \lim_{x \to b-} f(x)$$ So I am confused because, when $b=+\infty$ we can only evaluate this limit from the left side and not the right side.  We can't approach infinity from a higher infinity. Does this mean that limits evaluated at infinity don't exist and therefore none of the limit laws like addition apply to limits evaluated at infinity? EDIT: So does that mean I can use the limit laws such as addition, composition, etc on limits evaluated at infinity as long as the limits tend to a finite value? I.e. $$\lim_{x \to \infty} [f(x) + g(x)] = \lim_{x \to \infty} f(x) + \lim_{x \to \infty} g(x)$$ as long as both of the separate limits are some finite value? And so on, for multiplication, composition, etc?",['calculus']
1931800,inconsistency in Cauchy Integral Formula for matrices?,"I noticed that for scalars, Cauchy's integral formula $$
\oint_{C}dz\frac{f(z)}{z-z_{0}}=2\pi if(z_{0})
$$ requires that $f(z)$ be analytic on and inside the contour $C$ (correct me if I'm wrong). Now there seems to also be a similar relation that works for matrices, as seen at the top of p. 8 in Higham : $$
\oint_{C}dzf(z)\left(zI-A\right)^{-1}=2\pi if(A).
$$ However, this relation requires that $C$ does enclose all the eigenvalues of $A$. This is perplexing to me, because on one hand you do not want the singularities inside the contour, and on the other, you do . Why are the requirements for the scalar and matrix versions the total opposite of each other?","['cauchy-integral-formula', 'complex-analysis', 'complex-integration', 'functional-calculus']"
1931810,Definition of an affine set,"Some resource says that a set $A \subset V$ is affine if $\forall x,y \in A, t \in F$, $tx+(1-t)y \in A$ while others say that $A$  is affine if $\forall x_1,\dotsb,x_n \in A, a_1, \dotsb a_n, \in F, a_1+\dotsb+a_n = 1$, $a_1x_1+\dotsb+a_nx_n \in A$. I am trying to show that this two is equivalent, suppose that I have the first one holds. Then I want to show $\forall x_1,\dotsb,x_n \in A, a_1, \dotsb a_n, \in F, a_1+\dotsb+a_n = 1$, $a_1x_1+\dotsb+a_nx_n \in A$. I first consider a simple case where there are only three elements, ${x_1,x_2,x_3}$,but I do not think it is viable. So maybe the first one is wrong? Because the second definition includes the interior of a triangle while the first does not.","['linear-algebra', 'analysis']"
1931812,Order of zero of a function,"1) Suppose I want to find the order of the zero of the following function $$ f(z) = (e^z - 1)^3$$ at $z=0$ . I first find the Taylor expansion for $e^z - 1$ , and then write $$e^z - 1 = zg(z),$$ where $g$ is analytic and $g(0) \ne 0$ . Next, I say $f(z) = z^3 [g(z)]^3$ , so it has a zero of order 3 at 0. Is that right? The reason I'm asking this is because in complex analysis, it is not true that $(z_1 z_2)^\alpha = z_1^\alpha z_2^\alpha$ . If it's not right, is there any way I can do it fast without expanding everything? 2) How can I find the order of the zero of $f(z) = [\log(1 + \sin z)]^2$ at $z = 2\pi$ without applying the formula to the whole function $f$ to find the Taylor series? Will the multi-valued $\log$ function affect how I do it? Thank you.","['complex-analysis', 'roots']"
1931816,Ramanujan's Nested Square Roots,"Well, while my friend was watching ""The Man Who Knew Infinity""(Story of The Great Mathematician - THE Ramanujan), he came across the following equation -> $3 = \sqrt{1+2{\sqrt{1+3\sqrt{1+4\sqrt{1+5.....\infty}}}}}$ (I hope you understand the series ) I actually found 1 method of proving this -> We know that $(x+1)^{2}=x^{2}+2x+1
          =1+x(x+2)$ -------------------> (1) Substituting $2$ in the above identity $(2+1)^{2}=1+2(2+2)$ $=>(3)^{2}=1+2(4)$ $=>3=\sqrt{1+2(4)} $ -------------------> (2) Similarly , substituting $3$ in the above identity $(3+1)^{2}=1+3(3+2)$ $=>(4)^{2}=1+3(5)$ $=>4=\sqrt{1+3(5)}$ -------------------> (3) Similarly , substituting $4$ in the above identity $(4+1)^{2}=1+4(4+2)$ $=>(5)^{2}=1+4(6)$ $=>5=\sqrt{1+4(6)}$ -------------------> (4) Substituting value of (3) & (4) in (2) $(2+1)^{2}=1+2(2+2)$ $=>(3)^{2}=1+2(4)$ $=>3=\sqrt{1+2(4)}$ $=>3=\sqrt{1+2\sqrt{1+3(5)}}$ $=>3=\sqrt{1+2\sqrt{1+3\sqrt{1+4(6)}}}$ We notice we can do this FOREVER , thus , arriving at $3 = \sqrt{1+2{\sqrt{1+3\sqrt{1+4\sqrt{1+5.....\infty}}}}}$ Is there any other method to arrive at the given solution , in fact , PROVE the given equation ??",['number-theory']
1931970,Matrix to a power,"Let $k$ be fixed natural number and let $$A=\begin{bmatrix}a_1&*&*&...&*\\0&a_2&*&...&*\\0&0&a_3&...&*\\ \vdots & \vdots & \vdots & \ddots & \vdots\\0&0&0&...&a_n\end{bmatrix}$$ where $a_i^k=1$ for every natural $i$. The $a_i$ are pairwise distinct and $*$ denotes a random complex number. My question is: Is it true that $A^k=E$? I've tried for some matrices and its seems to be true, but I have no idea how to start solving this. Thanks for any idea!","['matrices', 'linear-algebra', 'complex-numbers']"
1931973,"When adding up random variables, how is the 'new sample space' and probability function implicitly defined?","When adding up two random variable with probability space $(\Omega_1, B_1, P_1)$ and $(\Omega_2, B_2, P_2)$, it seems that a new probability space $(\Omega', B', P')$ is implicitly defined by having a surjective function $\pi:\Omega'\mapsto\Omega_1$ which is measurable and $P'(\pi^{-1}(E)) = P_1(E)\;\; \forall E \in B_1$. In this Terrance Tao's note , it shows how one can extends a sample space from 'throwing one dice' to 'throwing two dice' by explicitly defining $\pi$, $\Omega'$ and $B'$. But in a lot of textbooks, when random variables are added, multiplied or divided together, these are not defined explicitly. A frequently seen example is the Binomial Distribution (from here , but I have seen the same thing in my textbook as well): Let $X_1$, $X_2$ ..., $X_n$ be identical Bernoulli Distributed random variables with $P(X=1) = p;\; P(X=0) = 1-p$. And their corresponded Binomial random variable is $X = X_1 + X_2 + ... + X_n $ Here it seems to me that the above equation magically defined $\Omega'$ to be the Cartesian product of the sample spaces of all the $X_i$, and it also defined the probability of each sample points in the new $\Omega'$. How does the addition formula uniquely determine the new $\Omega'$ and $P'$? In general, what are the rules?","['probability-theory', 'random-variables']"
1931976,How to prove polynomials with degree $n$ does not form a vector space?,"This is one of my linear algebra problems: Prove that polynomials of degree $n$ does not (The professor made these words bold intentionally) form a vector space. From what I read, the set of polynomials of degree $n$ should be a vector space, because: There is an ""One"" and a ""Zero"" in this set; We can find inverse for addition and multiplication from this set; It follows all the axioms of addition. It follows all the axioms of scalar multiplication. Then can someone give me some hints to prove it does not form a vector space?","['polynomials', 'vectors', 'field-theory', 'linear-algebra', 'vector-spaces']"
1931999,Milne's Intersection theory simple example,"In Milne's Divisors and intersection theory , Example 12.3a) computes the intersection number of the curves $Z_1: Y=X^2$ and $Z_2: Y^2=X^3$ at the intersection point $P=(0,0)$ in the affine plane over a base field $k$. By definition $(Z_1 \cdot Z_2)_P = $ $\dim_k O_P/(Y-X^2, Y^2-X^3) = 
\dim_k k[X,Y]_{(X,Y)}/(Y-X^2, Y-X^3) $ The author states that this dimension is equal to $\dim_k k[X]/(X^4-X^3)$ (so, equal to 3). Why is that true? I would say at most that $k[X,Y]_{(X,Y)}/(Y-X^2, Y-X^3) = k[X]_{(X)}/(X^4-X^3) $ but this has dimension greater than 3, since contains for example powers like $1/(X-1)$ which are linearly independent from $1, X, X^2$.","['localization', 'intersection-theory', 'algebraic-geometry', 'commutative-algebra']"
1932006,characteristic polynomial of an element of a finite flat $R$-algebra,"A book I'm reading says this: Let $R$ be a noetherian ring, and $B$ a finite locally free $R$-algebra. Since $B$ is locally free, for every $b\in B$, multiplication by $b$ gives an $R$-linear endomorphism of $B$ as a locally free $R$-module, and consider its characteristic polynomial $$P_b(T) := \text{det}(T-b)\in R[T]$$ Ie, we cover Spec $R$ with open neighborhoods $U_i$ where $B$ is free, and on each $U_i$ we pick a basis, compute the characteristic polynomial there, and using basis-invariance of the char poly, we glue them together to obtain the char poly ""$P_b(T) := \text{det}(T-b)$"" in $R[T]$. Is there a good reference/book that discusses the properties of this characteristic polynomial? (I'm of course familiar with the situation where $R$ is a field. I'm just wondering what carries over from the field case to our case here). For example, are the roots of $P_b(T)$ precisely the values of $R$ for which there exists a nonzero $x\in B$ with $bx = rx$? Is $P_b(b) = 0$? Intuitively in terms of algebraic geometry, what does this characteristic polynomial represent/tell you?","['abstract-algebra', 'commutative-algebra']"
1932023,"Is $C^1[a,b]$ with the norm $\left \| f \right \|_1=(\int_{a}^{b}\left | f(t) \right |dt)+(\int_{a}^{b}\left | f´(t) \right |dt)$ a complete space?","Is $C^1[a,b]$ with the norm   $\left \| f \right \|_1=(\int_{a}^{b}\left | f(t) \right |dt)+(\int_{a}^{b}\left | f´(t) \right |dt)$ a complete space? I thought with parabolas based on this link , but the area is infinite $C([0, 1])$ is not complete with respect to the norm $\lVert f\rVert _1 = \int_0^1 \lvert f (x) \rvert \,dx$ . Thanks.","['functional-analysis', 'real-analysis', 'analysis']"
1932029,How to evaluate $\sum\limits_{k=0}^{n-1} \sin^t(\pi k/2n)$?,"How to evaluate $\displaystyle\sum_{k=0}^{n-1} \sin^t\left(\frac{\pi k}{2n}\right)$? $t,n$ are constants $\in \Bbb{Z}$. My try: $$\begin{align}
\zeta:=e^{i\pi/2n} \implies & \sum_{k=0}^{n-1}\sin^t\left(\frac{\pi k}{2n}\right) \\
= &\frac{1}{2^t}\sum_{k=0}^{n-1}\left(\zeta^k- \zeta^{-k}\right)^{t} \\
= &\frac{1}{2^t}\sum_{k=0}^{n-1}\left(\zeta^{-kt}\right)\left(\zeta^{2k}-1\right)^{t}\\
\end{align}$$ How to proceed from here? If a closed-form solution is not possible, can we still work out cases such as $t = n, t = 2n, $ etc? EDIT: Sangchul Lee provided an answer for even t. Still looking for a solution for odd t as well.","['trigonometry', 'sequences-and-series', 'complex-numbers', 'trigonometric-series']"
1932085,Proof of the inequality $2^{n} < \binom{2n}{n} < 2^{2n}$?,"As review for a midterm I am asked to prove the inequality:
$2^{n} < \binom{2n}{n} < 2^{2n}, n > 1.$ What I have is a two-part inductive proof. It is not hard to show for $2^{n} < \binom{2n}{n}$: Base step: Let $n=2$: $2^{2} < \frac{(2n)!}{2!2!} < 2^{2*2}$ $4 < 6 < 16$ Inductive Step: Show $2^{k+1} < \frac{[2(k+1)]!}{(k+1)!(k+1)!}.$ We have
$2^{k+1} = 2^{k}*2,$ so $2^{k+1} < 2 * \frac{2k!}{k!k!}.$ Since $\frac{(2k+2)!}{(k+1)*k!*(k+1)*k!} = \frac{2(k+1)*(2k+1)*2k!}{(k+1)k!(k+1)k!} = (2k+1) * \frac{2k!}{k!k!}$ and $(2k+1) > 2, k \geq 2,$ we can conclude the first part of the inequality. However, I can't make the second part work. By similar algebra I arrive at: $(2k+1) * \frac{2k}{k!k!} < 2^{2(k+1)} = 2^{2k}*2^{2},$ but $ (2k+1) \nless 4, k \geq 2.$ What have I done wrong?","['number-theory', 'proof-verification']"
