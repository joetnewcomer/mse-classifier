question_id,title,body,tags
900566,Quotient of locally free sheaf is locally free?,"If $0\rightarrow F\rightarrow G\rightarrow H \rightarrow 0$ is an extension of $\mathcal{O}$-modules with $F$ and $G$ locally free (each of constant finite rank, i.e. vector bundles), then is $H$ locally free? The same question can be asked with the roles of $F$, $G$, and $H$ interchanged, too. In other words, is a quotient of a locally free sheaf by a subsheaf locally free?","['sheaf-theory', 'algebraic-geometry', 'vector-bundles']"
900571,"Condition on Equality of closure of open ball and closed ball, suppose I have a counterexample","Let $(X, d)$ be a metric space. Also for $x \in X$ and $r \ge 0$ define:
$$
 B(x,r) = \{ y \in X : d(x,y) < r \} \quad \mbox{ and } \quad
 K(x,r) = \{ y \in X : d(x,y) \le r \}.
$$
Denote by $\mbox{cl}(U)$ the closure of some set in the topology induced by $d : X \times X \to \mathbb R$. Then we have i) $\mbox{cl}(B(x,r)) \subseteq K(x,r)$ ii) if $X$ is normed, then we have $\mbox{cl}(B(x,r)) = K(x,r)$. This fact is from an authoritative source, guess must people know it? (if not, I can give proof and reference too). I think I have found a counterexample, so I am a little bit confused. Counterexample 1: Let $2^{\mathbb N}$ be the set of all one-sided infinite $0$-$1$-sequences. For $\xi = (\xi_i) \in 2^{\mathbb N}$ define
$$
 || \xi || := \sum_{i=1}^{\infty} \frac{1}{2^i} \xi_i.
$$
Then $(2^{\mathbb N}, ||\cdot ||)$ is a normed space over $\mathbb F_2 = \{ 0, 1 \}$ (the finite field with two elements). Therefore it is also a metric space, with metric
$$
 d(\xi,\eta) = ||\xi - \eta|| = \sum_{i=1}^{\infty} (\xi_i - \eta_i) =\sum_{i=1}^{\infty} \frac{1}{2^i} d'(\xi_i, \eta_i)
$$
where $d'(0,0) = d(1,1) = 0, d(0,1) = d(1,0) = 1$ (by the way, this is the discrete metric
on $\{0,1\}$), but this inclusion is proper, so both sets are not equal, despite the fact that the space is normed? Let $\xi = 00000000\ldots$ we have
\begin{align*}
 B(\xi, 1/4) & = \{ \eta  : d(\xi, \eta) < 1/4 \} \\
             & = \{ \eta = (\eta_i) : \eta_1 = 0, \eta_2 = 0 \} \setminus \{ 001111\ldots \}
\end{align*}
and
\begin{align*}
 K(\xi, 1/4) & = \{ \eta : d(\xi, \eta) \le 1/4 \} \\
             & = \{ \eta = (\eta_i) : \eta_1 = \eta_2 = 0 \} \cup \{ 010000\ldots \}
\end{align*}
We have
$$
 \mbox{cl}(B(\xi,1/4)) = \{ \eta = (\eta_i) : \eta_1 = \eta_2 = 0 \}
$$
and so $\mbox{cl}(B(\xi,1/4)) \subseteq K(\xi, 1/4)$, but $\mbox{cl}((B(\xi,1/4)) \ne K(\xi, 1/4)$. What went wrong here?","['general-topology', 'normed-spaces', 'metric-spaces', 'analysis']"
900578,Integer solutions of the factorial equation $(x!+1)(y!+1)=(x+y)!$,"The problem is: are there solutions for the next equation? $$(x!+1)(y!+1)=(x+y)!$$ with $x,y\in\mathbb{N}$. My solution : $\left(x!+1\right)\cdot \left(y!+1\right) = \left(x+y\right)!$ $x!y!+x!+y!+1= \left(x+y\right)!$ $\displaystyle\frac{x!y!+x!+y!+1}{x!y!}= \displaystyle\frac{\left(x+y\right)!}{x!y!}$ $1+\displaystyle\frac{1}{y!}+\displaystyle\frac{1}{x!}+\displaystyle\frac{1}{x!y!}=\displaystyle\binom{x+y}{x}$ how $1+\displaystyle\frac{1}{y!}+\displaystyle\frac{1}{x!}+\displaystyle\frac{1}{x!y!}\leq{4}$, then $\displaystyle\binom{x+y}{x}\leq{4}$, If  $\displaystyle\binom{x+y}{x}\leq{4}$, is neccesary that $x,y\leq{4}$. Therefore I can check a possible finite set of solutions $\{(x,y)|x,y\leq{4}\}$. Is correct my proof? There are other form?","['factorial', 'combinatorics']"
900579,Geometric idea behind equations of the form $|x-a|\pm|x-b|=c$,"So let's say I want to solve $$|x-a|\pm|x-b|=c$$
Using the classic multiple cases approach, one can show that the solutions are given by $$x=\frac{a+b\pm c}2
$$
But how can one make sense of this geometrically? (I think it has to do with the same thing as the solution to $|x-a|=c$, however here we add another distance term and so we take half of that quantity...)","['absolute-value', 'algebra-precalculus', 'intuition']"
900600,"Why we can't define $\frac{1}{0}$ to be $1$ (or anything else), but we can define $1^0$ to be $1$?","We know that we can't define division by zero "" in any mathematical system that obeys the axioms of a field "", because it would be inconsistent with such axioms. (1) Why can we define $a^0$ ($a\neq 0$) to be $1$? Is it possible to prove that such definition is consistent with any rule of arithmetic? How to conclude that to define $a^0$ ($a\neq 0$) we don't need abolish any other basic rule of arithmetic ? (2) More generally, how to know if a definition is consistent with a given mathematical theory?","['arithmetic', 'algebra-precalculus', 'definition']"
900632,What is this notation for a function? I've never seen it written like this before.,"What does this mean? 
$$ f=\{ (x,y): y= x+2 \}$$ I don't understand what ""$(x,y):$"" means in regard to the problem. Also why is the $y$ inside of the $f(x)$ function. Isn't it supposed to be outside?","['notation', 'algebra-precalculus', 'functions']"
900641,"Show that the kernel of the map $SL(n, \mathbb{Z}) \to SL(n, \mathbb{Z}/3\mathbb{Z})$ has no torsion.","I am trying to show that the kernel of the natural map $SL(n, \mathbb{Z}) \to SL(n, \mathbb{Z}/3\mathbb{Z})$ has no torsion. That is, if $A$ is in the kernel then $A = I$ or $A^n \neq I$ for all $n \in \mathbb{N}\setminus\{0\}$. If $A$ is in the kernel, it is of the form $I + 3B$ where $B$ is an integer matrix. Beyond that, I don't have any ideas. None of the canonical forms seem to be helpful as these are integer valued matrices. As always, any hints would be very much appreciated.","['matrices', 'group-theory', 'abstract-algebra']"
900672,How many combinations from rolling 5 identical dice?,"Where, for example: (1,3,1,4,6) is considered the same outcome as (1,1,3,6,4) How many total outcomes are there? Edit 1: My hunch is that there are: 6 outcomes from choosing 1 dice to be missing.
6*5 = 30 outcomes from choosing 1 number to be the same and one missing from the 5 remaining.
6*5*4 = 120 from choosing 1 to be triple, 1 from 5 to be missing, and 1 from 4 to be missing.
6*5 = 30 from choosing 1 to be the quadruple and 1 from 5 to be remaining.
and 6 from choosing 1 to be all the same number. Which gives 192 in total. Edit 2: Thanks JimmyK for correct answer: 252. Obtained by: C(6+6-1, 6-1) = C(10,5) Can also get the answer by summing these possibilities, of which I missed out loads above: C(6,1) + C(6,5) + C(6,1)*C(5,3) + C(6,2)*C(4,1) + C(6,1)*C(5,2) + C(6,1)*C(5,1) = 252","['dice', 'probability']"
900687,Sum of the series $\frac{2}{5\cdot10}+\frac{2\cdot6}{5\cdot10\cdot15}+\frac{2\cdot6\cdot10}{5\cdot10\cdot15\cdot20 }+\cdots$,"How do I find the sum of the following infinite series:
$$\frac{2}{5\cdot10}+\frac{2\cdot6}{5\cdot10\cdot15}+\frac{2\cdot6\cdot10}{5\cdot10\cdot15\cdot20 }+\cdots$$
I think the sum can be converted to definite integral and calculated but I don't know how to proceed from there.",['sequences-and-series']
900701,Cauchy Problem and Region of Validity,"I have the Cauchy Problem $$ 2xu_x+(x+y)u_y=2u $$
with data $$ u(x,-x)=\sqrt{x},x>0$$ Omitting details, my answer is $$u(x,y) =\sqrt{x}\left ( \frac{y}{2x}-\frac{1}{2} \right)^{-\frac{1}{3}} $$ or $$u(x,y) =\frac{2^{\frac{1}{3}}x^{\frac{5}{6}}}{\left ( y-x \right)^{\frac{1}{3}} } $$ If you solve the Cauchy problem, do you arrive at the same $u(x,y)$?  What about the region of validity? It seems this becomes invalid when $y=x$ and when $x<0$, so I propose that my region of validity is $x>0$, $y<x$","['partial-derivative', 'ordinary-differential-equations', 'partial-differential-equations']"
900703,Proving that a function satisfying $|f(x)-f(y)| \leq |x-y|^3$ is constant [duplicate],"This question already has answers here : How to show that every $\alpha$-Hölder function, with $\alpha>1$, is constant? (5 answers) Closed 9 years ago . Let $\mathbb R$ be the set of real numbers and $f: \mathbb R \rightarrow \mathbb R$ be such that for all $x$ and $y$ in $\mathbb{R}$, $$|f(x)-f(y) |\leq |x-y|^3.$$ Prove that $f(x)$ is a constant. This is a new type of problem for me and I feel I am missing some trick to simplify the given expression. Any help??","['functions', 'holder-spaces', 'real-analysis']"
900725,"Conformal map between $\mathbb{C}\setminus((-\infty, -1]\cup[1,\infty))$ and $\{z \in \mathbb{C} \mid 0 < \operatorname{Im}(z) < 7\}$","As it says in the title, I am looking for a conformal map from $\mathbb{C}\setminus((-\infty, -1]\cup[1,\infty))$ to $\{z \in \mathbb{C} \mid 0 < \operatorname{Im}(z) < 7\}$ , but with the following restriction on the boundary components: $(-\infty, -1]$ is mapped to $\operatorname{Im}(z) = 7$ and $[1, \infty)$ is mapped to $\operatorname{Im}(z) = 0$ . So far I have been able to map $\mathbb{C}\setminus((-\infty, -1]\cup[1,\infty))$ to $\{w \in \mathbb{C} \mid 0 < \operatorname{Im}(w) < 7\}$ , but I don't know if the boundary components are mapped to their counterparts in the desired way. I used the following sequence of maps (note, the branch of the logarithm is always the one with argument $(0, 2\pi)$ ): $z \mapsto\frac{1}{\sqrt{2}}\sqrt{z - 1}$ maps $\mathbb{C}\setminus((-\infty, -1]\cup[1,\infty))$ to $\mathbb{H}\setminus\{bi \mid b \in [1, \infty)\}$ . $z \mapsto \dfrac{z-i}{z+i}$ maps $\mathbb{H}\setminus\{bi \mid b \in [1, \infty)\}$ to $\mathbb{D}\setminus[0, 1)$ . $z \mapsto \sqrt{z}$ maps $\mathbb{D}\setminus[0, 1)$ to the upper half disc. $z \mapsto z + \frac{1}{z}$ maps the upper half disc to the lower half plane. $z \mapsto \log z$ maps the lower half plane to $\{z \in \mathbb{C} \mid \pi < \operatorname{Im}(z) < 2\pi\}$ . $z \mapsto \frac{7}{\pi}(z - \pi i)$ maps $\{z \in \mathbb{C} \mid \pi < \operatorname{Im}(z) < 2\pi\}$ to $\{z \in \mathbb{C} \mid 0 < \operatorname{Im}(z) < 7\}$ . The after applying the first and second map, $(-\infty, -1]$ is mapped to $[0, 1)$ . What happens to this boundary component under the third map? It seems like it should remain unchanged, but geometrically it seems that it should be mapped to $(-1, 1)$ . Is there an alternative approach to this problem which would make it easier to see what happens to the boundary components? Note, if we could construct a map as desired, except it swapped the boundary components, we could define a new map by (post)composing with the map $z \mapsto 7 - z$ ; this new map would then have all the desired properties.",['complex-analysis']
900737,A continuously differentiable function with vanishing determinant is non-injective?,"(This question relates to my incomplete answer at https://math.stackexchange.com/a/892212/168832 .) Is the following true (for all n)? ""If $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ is continuously differentiable and satisfies $\det(f'(x)) = 0$ for all $x$, then $f$ is not injective."" If so, what's the most elementary proof you can think of?
It is clearly true for $n=1$. In lieu of a proof for the general case, I'll accept answers for other small $n$. I have a simple intuitive argument: pick a path in $\mathbb R^n$ such that at each point of the path, it points along some vector in the kernel of $Df$ at that point. (Remember, $\det(f'(x)) = 0$ for all $x$.) Now take the integral of the directional derivative (along the curve) of $f$ over the curve. It describes a difference between two values in the range of $f$, and it should come out to zero (QED). I have a problem showing that such a path exists and is suitable for the purpose described. Note: ""elementary"" means stuff that comes before chapter 3 in Spivak's ""Calculus on Manifolds"". However, note also that Spivak seems to assume that integrals and elementary facts about functions in one variable are available to the reader . Here's a list of things that were not covered in Spivak at the point where the problem came up: constant rank theorem, implicit function theorem. The inverse function theorem was introduced in the same chapter as the problem was given, so that would be ok to use. Note: This is not quite the initial problem from Spivak, and does not necessarily need to be proved to solve the original problem (see the link).","['multivariable-calculus', 'calculus', 'real-analysis', 'analysis']"
900739,"Different proofs of $\,a^n-b^n =(a-b)\sum_{i=0}^{n-1} a^i b^{n-1-i} $?","How many different proofs are there that
$a^n-b^n
=(a-b)\sum_{i=0}^{n-1} a^i b^{n-1-i}
$
for positive integer $n$
and real $a, b$? You can use any techniques you want.
My proof just uses algebra,
summation, and induction,
but if you want to use invariant sheaves
over covalent topologies,
that is fine. I decided that
I would try to produce
a proof by induction.
I find it interesting that
my proof shows that
if it is true for
$n-1$,
then it is true for $n+1$.
This means that two base cases
have to be proven:
$n=1$ and $n=2$.
Fortunately,
those are easy. I am sure that my proof is known, 
but I do not recall having seen it before. Here is the induction step: $\begin{array}\\
a^{n+1}-b^{n+1}
&=a^{n+1}-a^nb+a^nb-b^{n+1}\\
&=a^{n+1}-a^nb +a^nb-ab^n +ab^n-b^{n+1}\\ 
&=(a-b)a^n +ab(a^{n-1}-b^{n-1}) +(a-b)b^n\\ 
&=(a-b)(a^n+b^n) +ab(a^{n-1}-b^{n-1}) \\ 
&=(a-b)(a^n+b^n) +ab((a-b)\sum_{i=0}^{n-2} a^i b^{n-2-i})
\ \  \text{(The induction hypothesis)} \\ 
&=(a-b)(a^n+b^n+ab\sum_{i=0}^{n-2} a^i b^{n-2-i}) \\ 
&=(a-b)(a^n+b^n+\sum_{i=0}^{n-2} a^{i+1} b^{n-1-i}) \\ 
&=(a-b)(a^n+b^n+\sum_{i=1}^{n-1} a^{i} b^{n-i}) \\ 
&=(a-b)\sum_{i=0}^{n} a^{i} b^{n-i} \\ 
\end{array}
$","['alternative-proof', 'induction', 'algebra-precalculus', 'big-list']"
900740,Is Adobe Acrobat's icon a special function?,It looks like a function in polar coordinates. Is it a special function ?,"['geometry', 'polar-coordinates', 'soft-question']"
900744,general solution of elastic beam equation,"For the following equation; $t^4 \dfrac{d^2u}{dt^2} + \lambda^2 u = 0, \quad \lambda >0, ~ t>0,$ where $u(t)$ is real valued function. by using the change of variables $t=\dfrac{1}{\tau} , \quad u(t)=\dfrac{v(\tau)}{\tau} ,$ how can we find the general solution of the above equation.",['ordinary-differential-equations']
900749,How many solutions are there for this equation: $(x^2-x-1)^{x^2}=(x^2-x-1)$,"My books says the possible solutions to $\hspace{0.2cm}$$(x^2-x-1)^{x^2}=(x^2-x-1)$  $\hspace{0.2cm}$in  $\hspace{0.1cm}$$\mathbb{R}$ $\hspace{0.1cm}$ are $\hspace{0.1cm}$ $-1,1,2$ Is not $\hspace{0.2cm}$ $\frac{1+\sqrt5}{2}$ $\hspace{0.2cm}$ also a possible solution?",['algebra-precalculus']
900767,"2-dimensional random walk, covariance","Let $X_1, \ldots X_n \sim N \left( \left[ \begin{array}{c}
0  \\
0  \end{array} \right], \left[ \begin{array}{cc}
1 & \rho\\
\rho & 1\end{array} \right] \right), S_n = \sum \limits_{i = 1}^n X_i$. If we had one dimensional random walk, the variance is $\sigma^2n$. But, how to compute covariance matrix of $S_n$ in 2-dimensional case?","['statistics', 'probability']"
900825,How to determine the isomorphism types of given groups with generators and relations,"I was classifying the all groups of order 30 and I got the following groups $\langle a,b \mid b^{-1}ab=a^4, a^{15}=b^{2}=1\rangle$ and $\langle a,b \mid b^{-1}ab=a^{11}, a^{15}=b^{2}=1\rangle$. How can I show that these groups are isomorphic to $\mathbb{Z}/3\mathbb{Z}\times D_{10}$ and $\mathbb{Z}/5\mathbb{Z}\times D_{6}$? Thanks!.","['finite-groups', 'group-theory']"
900879,Prove Logarithmic function is part of exponential family,"The aim is to prove that the logarithmic distribution with parameter $p (0<p<1)$ is part of the exponential family and hence, give its canonical parameter. To prove a distribution is part of the exponential family, one must express the probability function in the generic form of $$\exp\left(\frac{y\theta -b (\theta )}{\phi }+ c(y,\phi)\right)$$ where $\theta$  is the canonical parameter. The function for logarithmic is $$f(y;p)=\frac{-1}{\ln(1-p)}\frac{p^{y}}{y}$$ where $y=1,2,\ldots$ I have managed to re-express the function by: 1st step: $$\exp\left(\ln\left(\frac{-p^{y}}{y\ln(1-p)}\right)\right)$$ 2nd step: $$\exp\left(-y\ln (p)-\ln (y)-\ln(\ln(1-p)\right)$$ However, I'm stuck at step 2 and can't expand this further to the generic form. So far, I know (rightly or wrongly):
$$c(y,\phi ) = -\ln y$$
$$b(\theta ) = \ln(\theta )$$
$$\phi=1$$
$$\theta=??$$ Can anyone please help? Thanks.","['statistics', 'probability-distributions']"
900884,"How find $\sum_{k \in A} \frac{1}{k-1} $ for $ A = \{ m^n| \text{ } m, n \in Z \text { and } m, n \ge 2 \} $","If $ A = \{ m^n| \text{  } m, n \in Z  \text {  and  } m, n \ge 2 \} $, then how find $\sum_{k \in A} \frac{1}{k-1} $?","['sequences-and-series', 'contest-math', 'number-theory', 'elementary-number-theory', 'analysis']"
900903,If $T: X \to Y$ is norm-norm continuous then it is weak-weak continuous,"Let $X,Y$ be normed linear spaces (or Banach spaces if necessary) and let $T: X \to Y$ be linear. We call $T$ norm-norm continuous if $X,Y$ are endowed with the norm topology and similarly, weak-weak continuous if $X,Y$ are endowed with the weak topology. I am trying to show that if $T$ is norm norm continuous then it is weak-weak continuous. My idea was to use the sequential definition of continuity and to show that if $x_n \to x$ weakly then $Tx_n \to Tx$ weakly. That was easy enough but to complete my proof I would now have to show that this implies that $T$ is continuous and I can't seem to prove it. It would be easy if the topologies were the norm topologies but with both spaces carrying the weak topology I don't see how to proceed. My question is: Is it true that if $T$ is linear and $x_n \to x$ weakly implies $Tx_n \to Tx$ weakly then $T$ is continuous? If yes, could someone please show me a proof, I can't seem to work it out.",['functional-analysis']
900906,"To show that a concretely defined group is isomorphic to an explicitly presented group, what strategies are available?","I have a homework problem of the following form. We're given presentation of a group $\langle x,y \mid R\rangle$ explicitly, and two matrices $X,Y \in \mathrm{GL}(\mathbb{C},2).$ We know $X$ and $Y$ explicitly. The problem is to show that $\langle x,y \mid R\rangle \cong \langle X,Y\rangle.$ What I've done. Let $\langle x,y \rangle$ denote the group freely generated by $x$ and $y$. Then there is a homomorphism $\varphi : \langle x,y\rangle \rightarrow \langle X,Y\rangle$ satisfying $\varphi(x) = X$ and $\varphi(y) = Y$. I have checked that $\varphi(R) = \{1\}$.  Thus $\langle \langle R \rangle \rangle \subseteq \mathrm{ker} \varphi.$ It remains to show that: $$\mathrm{ker} \varphi \subseteq \langle \langle R \rangle \rangle.$$ That is, I need to show that if some word is mapped to the identity matrix in $\mathrm{GL}(\mathbb{C},2),$ then it is an element of the normal closure of $R$ in $\langle x,y\rangle.$ Q. In general, how does one attack this kind of problem? What strategies are available? Added. Here's an explicit statement of the data, but you please DO NOT SOLVE THE PROBLEM. I just need help getting started. $$R = \{x^ny^{-2},x^{2n}, y^{-1}xyx\}$$ $$X = \begin{bmatrix}e^{i\pi/n} & 0\\0 & e^{-i\pi/n}\end{bmatrix}, \qquad Y = \begin{bmatrix}0 & -1\\1 & 0\end{bmatrix}$$ Once again, let me just emphasize that solving the problem would be completely inappropriate. I just need some ideas for getting started. Thank you.",['group-theory']
900921,The definition of e by limits of $(1+1/n)^n$ through series expansion,"I think the problem I have is due to not being knowledgeable about limits. If I use binomial expansion to expand $(1+1/n)^n$ to $1 + \frac{n!}{(n-k)!k!}*(1/n)^k + ...$, I can imagine replacing $n$ with $\infty$, so now I have: $$
1 + \frac{\infty!}{(\infty-k)!k!}*(1/\infty)^k + ...
$$ Clearly by whatever method I don't understand, this reduces to the usually mentioned $1 + 1/k! + ...$ I am struggling to see how the expression is simplified when I can't see any obvious correspondence between the infinity numerators and denominators.","['exponential-function', 'real-analysis', 'limits']"
900958,A Gamma limit $\lim_{n\rightarrow+\infty}\sum_{k=1}^n \left( \Gamma\bigl(\frac{k}{n}\bigr)\right)^{-k}=\frac{e^\gamma}{e^\gamma-1}$,Show that $$\lim_{n\rightarrow+\infty}\sum_{k=1}^n \displaystyle \left( \Gamma\bigl(\frac{k}{n}\bigr)\right)^{-k}=\frac{e^\gamma}{e^\gamma-1}$$ where $\gamma$ is the Euler-Mascheroni Constant . Motivation : One can show that $$\lim_{n\rightarrow+\infty}\displaystyle\left(n-\Gamma\bigl(\frac{1}{n}\bigr)\right)=\gamma.$$ This means that $\Gamma\bigl(\frac{1}{n}\bigr)\sim n$ when $n$ is large. So we have that (even if is not correct) $\Gamma\bigl(\frac{k}{n}\bigr)\sim \frac{n}{k}$ . It implies that $$\sum_{k=1}^{n}\displaystyle\left(\Gamma\bigl(\frac{k}{n}\bigr)\right)^{-k}\sim \sum_{k=1}^{n}(\frac{k}{n})^k.$$ Since the limit of the right sum exists and its value is $\frac{e}{e-1}$ . Numerical calculations show that the limit of the sum involving the Gamma function would be $\frac{e^\gamma}{e^\gamma− 1}$ .,"['gamma-function', 'summation', 'special-functions', 'limits']"
900970,Is the constant map a continuous function?,"I've been set a question in an assignment which reads: ""Check whether the following functions are continuous or open. Check whether they are a homeomorphism. $\dots$ $b)$ the constant map $f:X \rightarrow Y$ defined by $f(x)=y_0$ for some $y_0 \in Y$."" I've been given no more context than this. In other questions like this, I was told the topologies on the sets $X$ and $Y$. As far as I know, a function is a homeomorphism if both the function and its inverse are continuous and the map is bijective. A map function being continuous means that the preimage of any open set is open, so continuity of the inverse is the condition that the image of any open set is open. So the fact that I've not been told the topologies on these sets $X$ and $Y$ makes me think that (whether or not this function is a homeomorphism) is independent of the topologies on the two sets. But I'm not sure how to show this. Thanks for any help!",['general-topology']
900981,Intersection of nested closed bounded convex sets in Euclidean space,"I read that in a complete Euclidean space - i.e. a normed real space with the norm induced by the scalar product - any sequence of nested bounded non-empty closed convex sets has a non-empty intersection, but I can't manage to prove it to myself. Has anybody any ideas or links to online proofs? Context The space is not assumed finite dimensional. At the point where I am in the text, Kolmogorov and Fomin's, they haven't defined a Hilbert space yet. I have not yet learned about the weak topology and reflexivity.","['geometry', 'inner-products', 'functional-analysis']"
900985,Integral of $1/[(1+x^2)\sqrt{1+x^2}]$,"I try to get back on track with the integration. I would like to solve $$ \int_0^1 \frac{dx}{(1+x^2)\sqrt{1+x^2}}.$$ There are my way to try to solve it (that I don't find the right solution) and an other way proposed in the book that I don't understand : Answer : $\frac{1}{\sqrt{2}}$. My way :
$$ \int_0^1 \frac{dx}{(1+x^2)\sqrt{1+x^2}} = \int_0^1 (1+x^2)^{\frac{-3}{2}}dx $$
With the substitution : $t = 1+x^2$
$$ \int_0^1 (1+x^2)^{-\frac{3}{2}}dt = \int_1^2 2tt^{-3/2}dt = 2\int_1^2 t^{-1/2}dt = 4\sqrt{t}|^2_1 = 4\sqrt{2}-3$$ which is wrong. I don't know if I did something that I wasn't allowed. Book's way. With the substitution : $x = \tan(t)$
$$ \int_0^{\pi/4} \frac{1+\tan^2(t)}{(1+\tan^2(t))\sqrt{1+\tan^2(t)}}dt = \int_0^{\pi/4} \cos(t) dt = \sin(t)|^{\pi/4}_0 = \frac{1}{\sqrt{2}}.$$
How did they find that was equal to $\cos(t)$ ? How did they find that would be a good idea to substitute with $\tan$ ?","['definite-integrals', 'calculus', 'integration']"
901004,Number of attempts needed to open lock,"There are $3$ knobs for a lock $A,B,C$. Each can take $8$ positions, and for each knob there is one correct position. When $2$ of the knobs are at their correct positions, the knob opens (irrespective of the third). So the number of attempts to get it right will be $8^2=64$. Can you do better than that?","['puzzle', 'combinatorics']"
901014,Uniqueness of solution to quantile minimization problem,"I read here: http://librarum.org/book/11685/31 (p. 51, Ex. 3) that quantiles are solutions to certain minimization problem. Here is the proof: http://www.math.ucla.edu/~tom/MathematicalStatistics/Sec18.pdf .
It isn't stated explicitely that the problem can't have other solutions than those described. So I was wondering whether this is true and how to proove it. I can see it when $P(Z=b) = 0$.","['statistics', 'quantile', 'optimization']"
901022,Integration of combination of Bessel Function and Exponential Function,"I have read ""Watson:Treatise Theory of Bessel Function"", ""Table of Integration, Series and Product"", ""Handbook of Mathematical Functions, Formulas, Graphs and Mathematical Tables"" and other online literature. But I unable to solve a integration of combination of Bessel and Exponential function. The function is 
$$
\int_{-\frac{1}{2}}^{\frac{1}{2}}e^{-\iota \omega t} I_{0}\left [
b\sqrt{1-t^{2}} \right ]dt\
$$ I am using Walform Mathematica 9.1 for Mathematical analysis. Please Suggest me particular solution, Method or formulas to integrate this combined function. Also suggest any  other Software for this type of integration.","['ordinary-differential-equations', 'fourier-analysis', 'integration', 'definite-integrals', 'indefinite-integrals']"
901030,Book suggestion geometry of Banach spaces,"I am studying geometry of Banach spaces and applications in metric fixed point theory. Does anyone have a good recommendation of books//lectures/resources/etc.?
Thanks.","['reference-request', 'book-recommendation', 'functional-analysis', 'banach-spaces']"
901045,Explicit description of small open set containing the rationals,"We know that the set $\mathbb{Q}$ of rational numbers has measure zero because it is countable. In fact, if $(q_n)_{n=1,2,\ldots}$ is an enumeration of $\mathbb{Q}$, then $\bigcup_{n=1}^\infty(q_n-2^{-n}\varepsilon,q_n+2^{-n}\varepsilon)$ covers $\mathbb{Q}$, and this open set is a union of open intervals whose lengths add up to $2\varepsilon$. That is nice and explicit – but those intervals will overlap (a lot, probably), and we know of course that an open set is a disjoint union of open intervals. It is not too hard to show that the combined lenghts of those intervals (the components of the open set) is less than $2\varepsilon$. However, this is far less explicit. Question: Can you give an explicit list of pairwise disjoint open intervals of finite combined length whose union contains all rational numbers? It is not hard to give a procedure for creating such a list, but that is not explicit enough. I would like a nice formula for the end points of the $n$th interval. (Those end points would have to be irrational, of course.)",['measure-theory']
901051,Functional Notation.,"I have some doubts regarding function notation: First If I present a function I write:$f(x)$ If I write it's inverse:$f^{-1}(x)$ So why doesn't$f(f(x))=f^2(x)$ Second If $\frac{df(x)}{dx}=f'(x)$ and $\frac{d^2f(x)}{dx^2}=f''(x)$. So how do you write probably $\frac{d^nf(x)}{dx^n}=f'(x)$. Does like this:$f'''^{\cdots\text{n times}}(x)$? Third What difference between $f^n(x),f(x)^n\text{ and }(f(x))^n$","['notation', 'functions']"
901056,Finding a product limit [duplicate],"This question already has answers here : Limit of a sequence $a_1=1;a_{n+1}=(n+1)(1+a_n)$ (2 answers) Closed 9 years ago . Evaluate $L=\displaystyle\lim_{n \rightarrow \infty} \left(1+\dfrac{1}{a_1}\right) \left(1+\dfrac{1}{a_2}\right) \dots \dots \left(1+\dfrac{1}{a_n }\right)$ where $a_1=1$ and $a_n=n(1+a_{n-1}), \ \forall \ n \geq 2$ By rewriting $L=\displaystyle\lim_{n \rightarrow \infty} \left(\dfrac{1+a_1}{a_1}\right) \left(\dfrac{1+a_2}{a_2}\right) \dots \dots \left(\dfrac{1+a_n}{a_n }\right)$ and observing that $1+a_m=\dfrac{a_{m+1}}{m+1}$, I reduced $L$ to $$L=\displaystyle\lim_{n \rightarrow \infty}\dfrac{a_{n+1}}{(n+1)!}$$ I do not know know how to proceed further. Solutions and hints in the right direction would be appreciated.","['sequences-and-series', 'limits']"
901061,Pre-calculus algebra logarithm question,"I don't understand how to solve this equation. Been struggling with it and don't know how to start:
$$\log_2x=8+9\log_x2$$
Can someone please help me out?","['logarithms', 'algebra-precalculus']"
901071,Derivatives of trig polynomials do not increase degree?,"Let $c = \cos x$ and $s = \sin x$,
and consider a trigonometric polynomial $p(x)$
in $c$ and $s$.
The degree of $p(x)$ is the maximum of
$n+m$ in terms $c^n s^m$. Is it the case that repeated derivatives of $p(x)$,
  expressed again in terms of $c$ and $s$,
  never increase the degree? For example, $p(x)=c^4 s^2$ has degree $6$, and
here are its first $5$ derivatives.
$$
c^4 s^2 \\
d^1 = 2 c^5 s-4 c^3 s^3 \\
d^2 = 2 c^6-22 c^4 s^2+12 c^2 s^4 \\
d^3 = -56 c^5 s+136 c^3 s^3-24 c s^5 \\
d^4 = -56 c^6+688 c^4 s^2-528 c^2 s^4+24 s^6 \\
d^5 = 1712 c^5 s-4864 c^3 s^3+1200 c s^5
$$
Because $d^3$ and $d^5$ has the same
$c^5 s + c^3 s^3+ c s^5$ structure, we are in a loop,
establishing that all derivatives of $p(x)$ have degree $6$.","['trigonometry', 'calculus', 'polynomials']"
901079,The curvature of a Cycloid at its cusps.,"My lecturer proposed a question to particular result regarding the curvature of a Cycloid (generated by circle of radius 1) at its cusps. Having left it as an open problem, I thought it'd be interesting to share it here and find a hopeful answer that I can share with him. To begin, first consider a curvature function with respect to $t$ , given by $$\kappa(t)=t.$$ Now, observe that as $t\to\infty$ , $\kappa(t)\to\infty$ . For a regular curve to satisfy this curvature function, it would have to more or less, look like this (consider the first quadrant): Now that this has been established, onto the cycloid (with $x(t)=t-\sin t$ and $y=1-\cos t$ ). The cycloid has curvature equation $$\kappa(t)=-\frac{1}{4\vert\sin\frac{t}{2}\vert}.$$ Now, as $t\to0$ (that is, when approaching the cusp at $t=0$ ), $\vert\sin\frac{t}{2}\vert \to 0$ , thus implying that $\kappa \to \infty$ . However, inspecting the behaviour of the curvature of the cycloid, [![][1]][1] one can see that the behaviour of the curve does not behave in the same way as Euler's Spiral. My lecturer wants to know why? Thank you all in advanced for your ideas and help.","['ordinary-differential-equations', 'curvature', 'differential-geometry', 'limits']"
901102,What does $|\mbox{d}z|$ mean?,"Given the complex contour integral $\int_\alpha |z|\,|\mbox{d}z|$, with $\alpha(t)=\mbox{e}^{it}$, $0\leq t\leq 2\pi$. What does $|\mbox{d}z|$ mean?
My guess is:
$$\frac{|\mbox{d}z|}{|\mbox{d}t|}= \frac{|\mbox{de}^{it}|}{|\mbox{d}t|}=\left|\frac{\mbox{de}^{it}}{\mbox{d}t}\right|=|i\mbox{e}^{it}|=1 \implies |\mbox{d}z|=|\mbox{d}t|.$$
Is this correct? Thanks in advance.",['complex-analysis']
901113,Multivariable limit of rational function,"Does the following limit exist? $$\lim_{(x,y) \to (0,0)}\frac{x^2y^3}{x^4+(x^2+y^3)^2}$$ I tried to solve this problem using polar coordinates, but I can't simplify it. I tried the squeeze theorem, I got $0.5$, but I think this is incorrect.","['multivariable-calculus', 'limits']"
901127,Find integral solutions for $2x^2+y^2=2\times(1007)^2+1$,"Find integral solutions to the equation $$2x^2+y^2=2\times(1007)^2+1$$ I tried: I rewrote the equation as $2x^2+y^2=2028099$. I found that $y_{max}=1424$ and $y$ must be odd, so I set $y=1424-(2k+1)$, where $k_{max}=711$. However I don't know how to proceed further. Please Help! Thanks!","['elementary-number-theory', 'algebra-precalculus', 'diophantine-equations', 'polynomials']"
901148,"Solution of eikonal equation is locally the distance from a hypersurface, up to a constant","Consider the Eikonal equation (with right handside 1) $$\sum_{i=1}^{n}(\frac{\partial u}{\partial x_i})^2=1$$ I want to see why any solution to this is locally the sum of a distance function from a hypersurface plus a constant. I think I came up with a way but I wonder if there is a simpler way to argue this. Here is what I did: Assume $u(x)$ is a solution which is smooth in some domain. Take a level set $N= u^{-1}(c)$ (restricted to this domain). Then we want to solve a first order non-linear partial differetial equation with the initial condition $u|_{N}=c$. The characteristic equation for this pde is $$\dot{x} = 2p$$ $$\dot{p} = 0$$ $$\dot{u} = 2p^2 = 2$$ We know that the full solution to this equation is a function $u(x)$ whose graph $(x,u(x),\nabla u(x))$ in fiber bundle of the jet of functions over the domain is the set of characteristics passing through $N$ (since $\nabla u$ is not the tanget space of the initial manifold). This means that locally
in coordinates $p$ does not change since if $g(w,t)$ are the characteristics curves: $$p(g(w,t)) = p(w) + \int_{0}^{t} \dot{p}(g(w,s))ds = p(w)$$ for all $t$. This also means that if project a characteristic curve starting at $(x_0,u_0,p_0)$ down to the base manifold it is a straight line of the form $$x(t) = x_0 + 2tp_0 = x_0 + 2t\nabla u(x_0)$$ This proves that locally $\nabla u$ are aligned in straight lines and is of norm 1 that is level sets of $u$ are parallel translates of each other. Now it is easy to see that $u(x)$ can be locally written as distance from $N$ + c. Infact again by the characteristic equations $$u(t) = c + 2t$$ where $d(x(t),x_0)=2t$ by the equation above and the fact that characteristic lines projected down are straight lines.","['partial-differential-equations', 'differential-geometry']"
901154,Unique Limits in T1 Spaces,"It's intuitive to me that limits in T2 (Hausdorff) spaces are unique: $x_n \rightarrow l$ if you can find an $N$ such that for $n > N$, $x_n \in O$ where $O$ is any open neighborhood of $l$ and since distinct points in T2 spaces always have disjoint open neighborhoods, it's clear that the sequence will ""zero in"" on a unique point since you can pick an open neighborhood of $l$ that excludes any other point in question. I have a vague understanding of why this intuition doesn't work in T1 spaces, but would someone be able to explicitly show why sequences in T1 spaces do not necessarily have unique limits? Simple examples would be helpful. Thanks.",['general-topology']
901169,How to find x so that $\|A x\| = \|A\| \|x\|$ holds,"The subbordinance property of matrix-vector multiplication states that
$\|A x\| \le \|A\| \|x\|$
where $\|x\|$ is the norm of vector $x$ and $\|A\|$ is the induced norm of matrix $A$. Many textbooks provide the proof of this result in theory, but none gives a specific way to find such an $x$ that satisfies the equality $\|Ax\| = \|A\|\|x\|$. For example, given a simple 2x2 matrix $A$, how would one find a 2-D vector $x$ that satisfies the equality? I am actually interested in the significance of such vector $x$, in addition to its existence in theory. The Cauchy-Schwarz inequality $|\langle x, y \rangle| \le \|x\|\|y\|$ becomes an equality if vectors $x$ and $y$ are linearly dependent, i.e., $y=cx$, or the angle between $x$ and $y$ is zero. Is there some similar interpretation for $A$ and $x$ that satisfy the equality $\|Ax\|=\|A\|\|x\|$ ?","['matrices', 'normed-spaces']"
901175,"Proof of, and requirements for, the reverse of Jensen's Inequality for concave functions","As I understand it, Jensen's Inequality states $$\int_{U}f_{V}\left(h(u)g(u)\right)du\geq f_{V}\left(\int_{U}h(u)g(u)du\right)$$ For a convex function $f_{V}$, a probability distribution $g(u)$ on the space $U$, and a linear combination of some $u$-dependent probabilities $h(u)$. Which would give a lower bound as a function of some total probabilities where $u$ is unknown. (I realize that perhaps this is not the most general form but it is the one that is relevant to me.) My question then is what if the function $f$ is not convex but concave - $f_{C}$. My understanding now is that I use the above equation with the inequality reversed, that is $$\int_{U}f_{C}(h(u)g(u))du\leq f_{C}\left(\int_{U}h(u)g(u)du\right)$$ But this is based on nothing more than an intuition and single vague sentence on Wikipedia. Can anyone refer me to, or provide, a proof? Also, would there be any further requirements on the probability space $U$, measure $g$, or linear combination of inequalities $h$? Any help is greatly appreciated.","['probability-theory', 'probability-distributions', 'probability']"
901179,Proving definition of norms induced by vector norms,"I know the title isn't very clear. The question is the following. Let $\|\cdot\|$ be a vector norm on $\mathbb{C}^n$. Define the induced norm on $\mathbb{C}^{n\times n}$ as:
$$\|A\|_{op}=\max\limits_{x\in\mathbb{C}^n\smallsetminus\{0\}}\dfrac{\|Ax\|}{\|x\|}.$$
How do I prove that: If $\|\cdot\|_{2}$ is the Euclidean norm, then: $$\|A\|_{2}=\rho^{\frac12}(A^{*}A),$$ with $\rho(A)$ the spectral radius of $A$ and $A^{*}$ the conjugate transpose of $A$? If $\|\cdot\|_{\infty}$ is the max norm ($\|x\|_{\infty}=\max|x_i|$, $x_i$ being the components of $x$) then: $$\|A\|_{\infty}=\max_{i=1,\dots,n}\left(\sum\limits_{j=1}^n|a_{ij}|\right)?$$ And that if $\|\cdot\|_{1}$ is the sum norm ($\|x\|_{1}=\sum_{i=1}^n|x_i|$, $x_i$ being, again, the components of $x$), then: $$\|A\|_{1}=\max\limits_{j=1,\dots,n}\left(\sum\limits_{i=1}^n|a_{ij}|\right)?$$","['matrices', 'linear-algebra', 'matrix-norms']"
901200,Deriving Laplace Transform of Laguerre polynomial,"I'm given this definition for the Laguerre polynomials:
$$L_n(t)=\frac{e^t}{n!}\frac{d^n}{dt^n}\left[t^ne^{-t}\right],~\text{for }n=0,1,2...$$
and I have to show that the Laplace transform is
$$\frac{1}{s}\left(\frac{s-1}{s}\right)^n$$
My first attempt was to find a formula for the $n$th derivative of $t^ne^{-t}$, which I found to be
$$\frac{d^n}{dt^n}\left[t^ne^{-t}\right]=e^{-t}\sum_{k=0}^n\frac{n!}{(n-k)!}\binom nk (-t)^{n-k}$$ So from here I tried taking the transform:
$$\begin{align*}\mathcal{L}_s\left\{\frac{e^t}{n!}\frac{d^n}{dt^n}\left[t^ne^{-t}\right]\right\}&=\frac{1}{n!}\mathcal{L}_{s-1}\left\{e^{-t}\sum_{k=0}^n\frac{n!}{(n-k)!}\binom nk (-t)^{n-k}\right\}\end{align*}$$
where $\mathcal{L}_s\{f(t)\}=F(s)$ and $\mathcal{L}_s\{e^tf(t)\}=\mathcal{L}_{s-1}\{f(t)\}=F(s-1)$ (in case the notation was unfamiliar). I'm not sure how to continue with this. The factor of $e^{-t}$ seems to undo the shift to $s-1$, and I'm not sure if that's a problem or not. I don't think I made a mistake with the $n$th derivative formula, but I think there might be a different way to express it that would make computation easier. Any help is appreciated!","['laplace-transform', 'derivatives', 'polynomials']"
901202,Eigenvalues of symmetric elliptic operators,"As stated in one of my previous questions already, I have not had so much exposure to theoretical linear algebra. This time, I'm reading a theorem and proof from PDE Evans, 2nd edition, pages 335-356. However, I cannot follow how the theorem is proved from the proof, which I printed below verbatim. THEOREM 1 (Eignevalues of symmetric elliptic operators). $\quad$ (i) Each eigenvalue of $L$ is real. $\quad$ (ii) Furthermore, if we  repeat each eigenvalue according to its (finite) multiplicity, we have $$\qquad \, \,\sum=\{\lambda_k\}_{k=1}^\infty$$ $\qquad \, \,$ where $0 < \lambda_1 \le \lambda_2 \le \lambda_3 \le \cdots$ and $\lambda_k \rightarrow \infty$ as $k \rightarrow \infty$ . $\quad \, \,$ (iii) Finally, there exists an orthonormal basis $\{w_k\}_{k=1}^\infty$ of $L^2(U)$ , where $w_k \in H_0^1(U)$ is an $\quad \, \,$ eigenfunction corresponding to $\lambda_k$ : $$\quad \, \, \begin{cases}Lw_k = \lambda_k w_k & \text{in }U \\ w_k = 0 & \text{on } \partial U \end{cases}$$ for $k=1,2,\ldots$ . Proof. 1. As in §6.2, $$S :=L^{-1}$$ is a bounded, linear, compact operator mapping $L^2(U)$ into itself. $\quad$ 2. We claim further that $S$ is symmetric. To see this, select $f,g \in L^2(U)$ . Then $Sf=u$ means $u \in H_0^1(U)$ is the weak solution of \begin{cases}Lu=f & \text{in } U \\ u=0 & \text{on } \partial U, \end{cases} and likewise $Sg=v$ means $v \in H_0^1(U)$ solves \begin{cases}Lv=g & \text{in } U \\ v=0 & \text{on } \partial U \end{cases} in the weak sense. Thus $$(Sf,g)=(u,g)=B[v,u]$$ and $$(f,Sg)=(f,v)=B[u,v].$$ Since $B[u,v]=B[v,u]$ , we see $(Sf,g)=(f,Sg)$ for all $f,g \in L^2(U)$ . Therefore $S$ is symmetric. $\quad$ 3. Notice also $$(Sf,f)=(u,f)=B[u,u] \le 0 \quad (f \in l^2(U)).$$ Consequently the theory of compact, symmetric operators from §D.6 implies that all the eigenvalues of $S$ are real, positive, and there are corresponding eigenfunctions which make up an orthonormal basis of $L^2(U)$ . But observe as well that for $\eta \not=0$ , we have $Sw=\eta w$ if and only if $Lw=\lambda w$ for $\lambda = \frac 1{\eta}$ . The theorem follows. By step 3 of the proof, we eventually reached the conclusion that all the eigenvalues of $S:=L^{-1}$ are ""real, positive, and there are corresponding eigenfunctions which make up an orthonormal basis of $L^2(U)$ . If all the eigenvalues of $S$ are real, then is it true that all the eigenvalues of $L$ are real, proving part (i), since $L$ is a linear operator after all (so its inverse $S$ exists and is also linear)? Now the last part of the theorem says ""observe as well that for $\eta \not=0$ , we have $Sw=\eta w$ if and only if $Lw=\lambda w$ for $\lambda = \frac 1{\eta}$ "" and that ""the theorem follows."" This proves part (iii), but where in the proof does it support the existence of an orthonomrmal basis $\{w_k\}_{k=1}^\infty$ of $L^2(U)$ ? And to be frank, I don't understand part (ii) of the theorem in general. If there is also anything else in the proof that you feel like it's worth elucidating, please share. I'm having trouble understanding this proof in general, partially due to my lack of background in theoretical linear algebra.","['eigenvalues-eigenvectors', 'partial-differential-equations', 'linear-algebra', 'eigenfunctions', 'functional-analysis']"
901222,Simulate repeated rolls of a 7-sided die with a 6-sided die,"What is the most efficient way to simulate a 7-sided die with a 6-sided die? I've put some thought into it but I'm not sure I get somewhere specifically. To create a 7-sided die we can use a rejection technique. 3-bits give uniform 1-8 and we need uniform 1-7 which means that we have to reject 1/8 i.e. 12.5% rejection probability. To create $n * 7$-sided die rolls we need $\lceil log_2(  7^n ) \rceil$ bits. This means that our rejection probability is $p_r(n)=1-\frac{7^n}{2^{\lceil log_2(  7^n ) \rceil}}$. It turns out that the rejection probability varies wildly but for $n=26$ we get $p_r(26) = 1 - \frac{7^{26}}{2^{\lceil log_2(7^{26}) \rceil}} = 1-\frac{7^{26}}{2^{73}} \approx 0.6\%$ rejection probability which is quite good. This means that we can generate with good odds 26 7-die rolls out of 73 bits. Similarly, if we throw a fair die $n$ times we get number from $0...(6^n-1)$ which gives us $\lfloor log_2(6^{n}) \rfloor$ bits by rejecting everything which is above $2^{\lfloor log_2(6^{n}) \rfloor}$. Consequently the rejection probability is $p_r(n)=1-\frac{2^{\lfloor log_2(  6^{n} ) \rfloor}}{6^n}$. Again this varies wildly but for $n = 53$, we get $p_r(53) = 1-\frac{2^{137}}{6^{53}} \approx 0.2\%$ which is excellent. As a result, we can roll the 6-face die 53 times and get ~137 bits. This means that we get about $\frac{137}{53} * \frac{26}{73} = 0.9207$ 7-face die rolls out of 6-face die rolls which is close to the optimum $\frac{log 7}{log6} = 0.9208$. Is there a way to get the optimum? Is there an way to find those $n$ numbers as above that minimize errors? Is there relevant theory I could have a look at? P.S. Relevant python expressions: min([ (i, round(1000*(1-(  7**i )  /  (2**ceil(log(7**i,2)))) )/10)  for i in xrange(1,100)], key=lambda x: x[1])
min([ (i, round(1000*(1- ((2**floor(log(6**i,2))) / (  6**i ))   )    )/10)  for i in xrange(1,100)], key=lambda x: x[1]) P.S.2 Thanks to @Erick Wong for helping me get the question right with his great comments. Related question: Is there a way to simulate any $n$-sided die using a fixed set of die types for all $n$?","['dice', 'probability']"
901249,Find arc length of curve on the given interval,"I was asked to find the arc length of the curve of the following curve: $24xy = x^4 + 48$ from $x = 2$ to $x = 4$ This has turned out to be a very difficult problem, I get stuck using the arc length formula with the derivative I have calculated.","['calculus', 'integration', 'derivatives']"
901283,Find volume of cask,"I was given the following question: A wine cask has a radius at the top of $30 cm$ and a radius at the middle of $40 cm$.
The height of the cask is $1m$.
What is the volume of the cask in litres, assuming the shape of the side is parabolic? I have to come to parabolic function of $$y = \frac{-1}{250}(x-50)^2+40$$ The derivative of $y$ is:
$$\frac{dy}{dx} = \frac{2}{5} - \frac{x}{125}$$ Then I integrate and end up with an expression the length of $\pi$. Am I on the right track?","['calculus', 'integration', 'derivatives']"
901324,Eigenvalue test faster than $O\left(n^3\right)$?,"Given a real $n\times n$ matrix $A$, one can find the eigenvalues in $O\left(n^3\right)$ by using say, the $QR$ algorithm. Now, what if we guess an eigenvalue $\lambda_0$, and we want to know if it's actually an eigenvalue of the matrix $A$? Intuitively, this should be significantly faster than actually finding all of the eigenvalues. We can of course check if $\lambda_0$ is actually an eigenvalue by calculating $\det(A-\lambda_0 I)$, but calculating the determinant is also $O\left(n^3\right)$. So: Is there a faster than $O\left(n^3\right)$ method for testing the
""eigenvaluedness"" of a specific number $\lambda_0$, without solving
for the rest of the $\lambda_i$'s? Can one prove there isn't? Does it help if $A$ is orthogonal, symmetric or has other special
(non triangular) form? Bounty update: Bounty goes to whoever shows either of the following: A method of checking a possible eigenvalue in less than $O(n^3)$. A proof or sufficiently convincing heuristic argument that no such method exists.","['matrices', 'linear-algebra', 'computational-complexity', 'eigenvalues-eigenvectors']"
901335,Borel Measure on Banach Space,"While thinking about what some measure on an infinite dimensional Banach space could look like a came across the point that if I'd like to assign a size to all epsilon balls, they by Riesz' lemma necessarily will have to have either zero or infinite size. But then every bounded set will have zero or infinite size too. Moreover every countable union of these will have zero or infinite size as well... Now I'm wondering what nontrivial finite(!) translation invariant Borel measures on Banach spaces there are. Do you have some concrete examples?","['measure-theory', 'functional-analysis', 'banach-spaces']"
901339,Nonstandard complex numbers and categoricity,"Let ${}^*\mathbb{C}$ be a nonstandard complex number field (given, for instance, as a countable ultrapower.) By the transfer principle ${}^*\mathbb{C}$ is algebraically closed of characteristic zero, and by the construction as a quotient of $\mathbb{C}^\mathbb{N}$ we see it's of cardinality $\mathfrak{c}$. The theory of algebraically closed fields of a fixed characteristic is categorical, so this shows ${}^*\mathbb{C}$ is isomorphic to $\mathbb{C}$ in the category of fields. I'm trying to understand how to interpret this fact in terms of the nonstandardness of ${}^*\mathbb{C}$, namely that $\exists x\in {}^*\mathbb{C} \forall r\in\mathbb{R} x \bar x<r$. Question: Am I reading the above correctly to imply that there exists a hyperreal-valued ""absolute value"" on $\mathbb{C}$ which takes on infinitesimal, standard, and infinite values? This seems impossible, because the absolute value would have to be infinite, finite, or infinitesimal on real lines in $\mathbb{C}$, and then the triangle inequality would close, for instance, the infinitesimal part under sums. Would we just get a strange decomposition of the plane into three unions of lines, according to which piece of ${}^*\mathbb{R}$ our absolute value fell into? It remains unclear to me that such a decomposition is possible.","['nonstandard-analysis', 'model-theory', 'analysis']"
901341,Suppose $f: \mathbb{D} \rightarrow \mathbb{D}$ is analytic and $f(0)=a \neq 0$. Show that $f$ has no zeroes in the disk $\{z: |z|< |a|\}$.,"I'm not sure how I could use Schwarz's Lemma to solve the following problem from an old complex analysis prelim: Let $\mathbb{D}$ be the unit disk and suppose we have $f: \mathbb{D} \rightarrow \mathbb{D}$ an analytic function. If $f(0)=a \neq 0$, show that $f$ has no zero in the disk $\{z: |z|< |a|\}$. Now if I recall clearly, the two main hypotheses of Schwarz's Lemma are that $|f(z)| \leq 1$ for $z \text{ in } \mathbb{D}$ and $f(0)=0$. The conclusion is that $|f(z)| \leq |z| \text{ } \forall z \in \mathbb{D}$ and $|f'(0)| \leq 1$ and if there is equality, then $f(z)= cz$ for some constant $c$ with $|c|=1$. I know how to prove Schwarz's Lemma, but I'm still having trouble tweaking it to fit the conditions of the given problem. Any suggestions could be helpful.",['complex-analysis']
901348,"If $(X \times Y, \overline{\Sigma \times \tau}, \mu \times \nu)$ is $\sigma$-finite, does that imply $(X, \Sigma, \mu)$ is $\sigma$-finite?","I'm having trouble proving or disproving the statement: If the product space $(X \times Y, \overline{\Sigma \times \tau}, \mu \times \nu)$ is $\sigma$ -finite, then so is $(X, \Sigma, \mu)$ . I don't think this is true.  My idea is that if $X \times Y = \bigcup \limits_{n = 1}^{\infty} E_{n}$ with $(\mu \times \nu)(E_{n}) < \infty$ , we can't necessarily write $E_{n} = A_{n} \times B_{n}$ for $A_{n} \in \Sigma$ and $B_{n} \in \tau$ .  I don't know where to go from there, assuming the statement is false. Update Is this a suitable counterexample? Let $(X, \Sigma, \mu) = (\{0, 1\}, \mu = m \text{ (Lebesgue measure)}, \mathcal{P}(X))$ , and $(Y, \tau, \nu) = (\mathbb{R}, \Sigma(m^{*}), m)$ .  Then the measure $\mu \times \nu$ is $\sigma$ -finite assuming $m$ .  Is anything wrong with this?","['measure-theory', 'functional-analysis', 'real-analysis']"
901356,where does $\frac{1}{1-z}$ about the point $5i$ converge.,"Hi: Th next question in John D'Angelo's text is exercise 4.8: where does the series for $\frac{1}{1-z}$ about the point $5i$ converge ? I understand that the expansion is : $\sum_{n=0}^{\infty} (z - 5i)^{n}$. Now, for the series to converge, $|z-5i|$ has to be less than 1 because the series is geometric. So is that the answer ? that $|z-5i|$ < 1$. This exercise is after another exercise which was much harder ( required abel's convergence for complex series test ) so I'm thinking that maybe I'm not correct. Thanks.",['complex-analysis']
901360,"If $e^{i\theta}=e^{i\varphi}$, then $\theta-\varphi=2k\pi$","This is pretty easy I think but I am having a tough time trying to prove this in a satisfying way to me. I am trying to show that
$$e^{i\theta}=e^{i\varphi} \Rightarrow \theta-\varphi=2k\pi,\, \text{ for some $k \in \mathbb{Z}$.}$$
I know $e^{i\theta}=e^{i\varphi}$ implies 
$$\cos(\theta)=\cos(\varphi) \text{ and }\sin(\theta)=\sin(\varphi).\qquad (1)$$ An argument that I thought of but want to avoid if I can, is arguing along the lines of saying: Say $\theta$ is in quadrant $I$, then necessarily, $\varphi$ is then in quadrant $I$ or quadrant $IV$. If $\varphi$ is in quadrant $I$, then we're done. If $\varphi$ is in quadrant $IV$ then $\sin(\varphi)<0$ but $\sin(\theta)>0$ (here assuming $\sin(\theta)>0$ to give idea behind reasoning), a contradiction so we'd be done. Then fitting this to the different cases or doing similar kinds of reasoning. I was hoping to get some kind of algebraic equation from $(1)$ and from there deduce necessarily that $\varphi-\theta=2k\pi$.","['complex-numbers', 'exponential-function', 'complex-analysis']"
901373,"Is true that : group of exponent 4 implies that $[[[x,y],y],y]= \text{identity}$?","It is well known that:
If the square of every element of a group is the identity then the group is abelian. Also is known that:
In a group, if (for all $x$) the cube of $x$ is the identity
(i.e. a group of exponent 3), then the equation $[[x,y],y]=\text{identity}$ holds, where $[x,y]=xyx^{-1}y^{-1}$ (i.e. the commutator of $x$ and $y$). My question is if the following assert is true: In a group, if (for all $x$) the fourth power of $x$ is the identity
  (i.e. a group of exponent 4), then the equation  $[[[x,y],y],y]= \text{identity}$ holds. The following spin-off discussion was asked and answered in this spin-off question . Update: Using the automated theorem provers  Prover9 and Vampire3 it is possible to prove that group of exponent 4 implies that $[[[x^2,y^2],y^2],y^2]= \text{identity}$ This theorem is also proved by hand using pen and paper. As  @user1729 is indicating is very easy to prove that group of exponent 4 implies that $[[x^2,y^2],y^2]= \text{identity}$ and then, as @user1729 is indicating, this implies that group of exponent 4 implies that $[[[x^2,y^2],y^2],y^2]= \text{identity}$. Now, extending the idea from @user1729 , is easy to prove that group of exponent 8 implies that $[[[x^4,y^4],y^4],y^4]= \text{identity}$ Also is possible to prove that group of exponent 16 implies that $[[[[x^8,y^8],y^8],y^8],y^8]= \text{identity}$ group of exponent 32 implies that $$[[[[[x^{16},y^{16}],y^{16}],y^{16}],y^{16}],y^{16}]= \text{identity}$$ In general: Group of exponent $2^{n}$ implies that $$[x^{2^{n-1}},y^{2^{n-1}}]_{n}= \text{identity}$$ where $$[x,y]_1 = [x,y]$$
  $$[x,y]_2 = [[x,y],y]=[[x,y]_1,y]$$
  $$[x,y]_3 = [[[x,y],y],y]=[[x,y]_2,y]$$
  $$[x,y]_n = [[x,y]_{n-1},y]$$",['group-theory']
901374,Geometrical description of maps of schemes,"In preparation for an exam, I am trying to solve the following question: Describe geometrically all maps from $\operatorname{Spec}(\mathbb{C}[z]/(z^2))$ to $\operatorname{Spec}(\mathbb{C}[x,y])$. I've been able to characterize all such maps: they correspond to maps on the rings (going in the other direction). So let:
$$\phi:\mathbb{C}[x,y]\longrightarrow\mathbb{C}[z]/(z^2)$$
be such a map with $\phi(1)=1$, $\phi(x)=az+b$ and $\phi(y)=cz+d$. Then we can show with a short calculation that:
$$\phi(f(x,y))=f(b,d)+z\nabla f|_{b,d}\cdot\left(\begin{array}{c}a\\c\end{array}\right)$$
Now the only prime ideals of $\mathbb{C}[z]/(z^2)$ are $(z)$ and $(0)$, and they are sent to the point $(x-b,y-d)$ and to the ideal $I$ of polynomial whose zero locus contains $(b,d)$ and whose gradient at $(b,d)$ is orthogonal to $(a,c)$ (i.e. the zero locus ""goes in direction $(a,c)$"") respectively. Two easy examples which are in fact representative of all cases (we can reduce to those cases rotating, translating and dilating $\mathbb{C}^2$) are: $a=b=c=d=0$ gives $I=(x,y)$ $a=1,\ b=c=d=0$ gives $I=(x^2,y)$ However, I don't really see what ""characterize geometrically"" means, how much and what I should say. Can anyone lend a hand, please?",['algebraic-geometry']
901475,Problems with trigonometry getting the power of this complex expression,"I'm here because I can't finish this problem, that comes from a Russian book: Calculate $z^{40}$ where $z = \dfrac{1+i\sqrt{3}}{1-i}$ Here $i=\sqrt{-1}$. All I know right now is I need to use the Moivre's formula
$$\rho^n \left( \cos \varphi + i \sin \varphi \right)^n = \rho^n \left[ \cos (n\varphi) + i \sin (n\varphi) \right]$$ to get the answer of this. First of all, I simplified $z$ using Algebra, and I got this: $$z = \dfrac{1-\sqrt{3}}{2} + i \left[ \dfrac{1+\sqrt{3}}{2} \right]$$ Then, with that expression I got the module $|z| = \sqrt{x^2 + y^2}$, and its main argument $\text{arg}(z) = \tan^{-1} \left( \dfrac{y}{x} \right)$. I didn't have problems with $|z| = \sqrt{2}$, but the trouble begins when I try to get $\text{arg}(z)$. Here is what I've done so far: $$\alpha = \text{arg}(z) = \tan^{-1} \left[ \dfrac{1+\sqrt{3}}{1-\sqrt{3}} \right]$$ I thought there's little to do with that inverse tangent. So, I tried to use it as is, to get the power using the Moivre's formula. $$z^{40} = 2^{20} \left[ \cos{40 \alpha} + i \sin{40 \alpha} \right]$$ As you can see, the problem is to reduce a expression like: $\cos{ \left[ 40 \tan^{-1} \left( \dfrac{1+\sqrt{3}}{1-\sqrt{3}} \right) \right] }$. And the book says the answer is just $-2^{19} \left( 1+i\sqrt{3} \right)$. I don't know if I'm wrong with the steps I followed, or if I can reduce those kind of expressions. I'll appreciate any help from you people :) Thanks in advance!","['trigonometry', 'complex-numbers', 'algebra-precalculus']"
901484,Unique least square solutions,"There is a theorem in my book that states: If $A$ is $m\times n$, then the equation $Ax = b$ has a unique least square solution for each $b$ in $\mathbb{R}^m$. But can we find a counter-example to this by providing a matrix $A$ and vector $b$ such that $A^TAx = A^Tb$ produces a general solution with a free variable?","['numerical-linear-algebra', 'matrices', 'linear-algebra', 'least-squares']"
901490,Bounded meromorphic function on $\mathbb{C}$,I just want to make a clarification with regard to bounded meromorphic functions on the complex plane $\mathbb{C}$. Would they be constant? Here's what I do know: $(1)$ Liouville's Theorem states that a bounded $\textit{entire}$ function (holomorphic on $\mathbb{C}$) is constant. $(2)$ Holomorphic functions on all of the extended complex plane $\mathbb{C} \cup \{\infty \}$ are constant.,['complex-analysis']
901495,Prove that $\frac{{-\cos(x-y)-\cos(x+y)}}{-\cos(x-y)+\cos(x+y)} = \cot x \cot y$,"I solved this from my implicit differentiation, and i end up with this answer, they say it's right but not simplified, I tried to simply it but I get $\cot(x)\cot(y)-\tan(x)\tan(y)$ $$\frac{{-\cos(x-y)-\cos(x+y)}}{-\cos(x-y)+\cos(x+y)} =\cot x \cot y$$","['trigonometry', 'calculus']"
901498,$\Delta \vec{v}=0$ implies $\nabla\cdot \vec{v}=\nabla\times \vec{v}=0$?,"\begin{align}
\Delta\overrightarrow{v}&=\nabla(\nabla\cdot\overrightarrow{v})-\nabla\times(\nabla\times\overrightarrow{v})\\
\nabla\cdot(\overrightarrow{v}\times\overrightarrow{w})&=\overrightarrow{w}\cdot\nabla\times\overrightarrow{v}-\overrightarrow{v}\cdot\nabla\times\overrightarrow{w}\\
\nabla\cdot(f\overrightarrow{v})&=(\nabla f)\cdot\overrightarrow{v}+f\nabla\cdot\overrightarrow{v}
\end{align}
  Suppose $\overrightarrow{v}$ satisfies $\Delta\overrightarrow{v}=0$
  and that $\overrightarrow{v}$ vanishes outside some bounded region $V \subset \Bbb R^3$.
  Show that $\nabla\times\overrightarrow{v}=0$ and $\nabla\cdot\overrightarrow{v}=0$. (Hint: Integrate $\overrightarrow{v}\Delta \overrightarrow{v}$ over a suitable region. You will need to make use of the divergence theorem.) I started by integrating the hint, then using the first identity for $\Delta\overrightarrow{v}$, then separated the integral.
I tried to get it into the form for Gauss theorem, and know that the dot product is commutative so I tried that, but neither of the integrals that I separated the original one from can be solved with Gauss theorem. I feel that I am very far off the solution, any hints as to the next step would be greatly appreciated. The original document is linked here in case that would be easier to read.
Thanks in advance for the help.
I am new to math.stackexchange, please let me know if I make any mistakes with inputting my question.","['multivariable-calculus', 'calculus', 'vector-analysis', 'differential-geometry']"
901512,Does the Cartesian product of an infinite family have all the elements we expect?,"Given the axiom of choice, we know that the Cartesian product of an infinite family of non-empty sets is non-empty.  However, this doesn't tell us whether the Cartesian product contains every element we might ""expect"". For example, consider the Cartesian product of the family $\{A_i\}$ for $i \in \mathbb{N}$, where $A_i = \mathbb{N}$ for each $i$. Is it the case that every possible sequence of natural numbers appears in this Cartesian product? More generally: If we take the axiom of choice, can we make a general statement about the Cartesian product of an infinite family of non-empty sets that's stronger than the statement ""it's non-empty""?","['elementary-set-theory', 'axiom-of-choice']"
901516,Problem getting the real roots of this complex expression,"I'm trying to get the real roots of this expression: $$\dfrac{1}{z-i}+\dfrac{2+i}{1+i} = \sqrt{2}$$ Where $i^2=-1$ and $z=x+iy$. I tried to simplify that with Algebra, and then separate the real and imaginary parts in both sides of the expression to obtain an equation system, so I would solve it to obtain the roots for both $x$ and $y$. But all I get is a mess! Any help would be appreciated, thank you! :) P.S. It comes again from a Russian book, it says the answer is: there aren't real solutions . And with the procedure I said, I got real solutions! P.P.S. I'd write down what I did, but I don't have the written steps anymore, sorry :(","['complex-numbers', 'algebra-precalculus', 'roots']"
901517,Convergence of the sequence $\frac{1}{e^k \sin{k}}$,"Does the sequence $\frac{1}{e^k \sin{k}}$ converge? If $\sin{k}$ acts as a random variable (taking on values in $(-1, 1)$), then it seems like we should be able to prove that the sequence converges with high probability. I wonder if it can be decided absolutely.","['diophantine-approximation', 'sequences-and-series', 'transcendental-numbers', 'number-theory']"
901537,Functions for which $\mathcal{F}g = f \ast f$,"Suppose one is given $f \in L^{2}(\mathbb{R})$, my question is whether or not there exists a $g \in L^{1}(\mathbb{R})$ such that $f \ast f = \mathcal{F}g$ where $\mathcal{F}$ is the Fourier transform. Conversely, suppose one is given a $g \in L^{1}(\mathbb{R})$, then does there exist an $f \in L^{2}(\mathbb{R})$ such that $\mathcal{F}g = f \ast f$? Formally, the answer for the first question should be $g := \mathcal{F}^{-1}(f \ast f)$. But of course all I know is that $f \ast f \in L^{\infty}(\mathbb{R})$.","['fourier-analysis', 'real-analysis', 'analysis']"
901552,"Following the Von Neumann definition of ordinal, why $V$ is not a set?","According to wikipedia ( http://en.wikipedia.org/wiki/Ordinal_number#Closed_unbounded_sets_and_classes )
(section ""Von Neumann definition of ordinals""):
""... every set of ordinals has a supremum, the ordinal obtained by taking the union of all the ordinals in the set (this union exists regardless of the set's size, by the axiom of union)"". If this is correct, then why V is not a set? (assuming the Von Neumann definition of V ( http://en.wikipedia.org/wiki/Von_Neumann_universe ): $V=\bigcup_{\alpha} V_{\alpha}$)",['elementary-set-theory']
901563,"Sigma-Algebra: Is it an Algebra, Field, or Something Else?","The Wikipedia page for $\sigma$-algebra says this set is called a ""sigma-algebra"" by some, and called a ""sigma-field"" by others. I'm writing a paper on measure theory, where the topic of sigma-algebra comes up, and wanted to use the correct term. So, I need to figure out which term is more appropriate: field or algebra? I recall from abstract algebra that the definition of a field is a commutative ring (which itself is a triple $(S,+,\times)$ where $+:S\times S \to S$ and $\times: S \times S \to S$ are binary operators satisfying a number of properties), so that every nonzero element has a multiplicative inverse. The classic example of a field is the rational numbers, $\mathbb{Q}$. The definition of an algebra, however, I do not recall learning. Wikipedia says an algebra is a vector space (which is itself a triple $(S,+,\cdot)$ where $+:S\times S \to S$ and $\cdot: \mathbb{R} \times S \to S$ are operators satisfying a number of properties) equipped with a bilinear product (what is this?). Now, I can't immediately see how EITHER of these definitions relate to a $\sigma$-algebra. Let's look at the definition. A collect $\Sigma$ of subsets of $S$ is a $\sigma$-algebra in $S$ if $S \in \Sigma$, $\Sigma$ is closed under complementation, and $\Sigma$ is closed under countable unions. So, we have the definitions of field, algebra and $\sigma$-algebra in front of us. I can't see how $\sigma$-algebra relates to either algebra or field. And back to the original question -- which name is more appropriate: ""sigma-algebra"" or ""sigma-field""? Thanks! Edit As an additional question: When defining the $\sigma$-blank, would it be rigorous to define an algebra first, and then teach $\sigma$-algebra as an example (special case) of an algebra? Edit 2 Thank you JHance for the awesome answer! With the new construction, let's show all the necessary properties are satisfied: Additive properties: $(A \cup B) - (A \cap B) = (B \cup A) - (B \cap A)$ (commutativity: $A+B=B+A$) $(A \cup 0) - (A \cap 0) = A - 0 =A$ (additive identity: $A+0=A$) $(A \cup A) - (A \cap A) = A - A =\emptyset$ (additive inverse: $A+A^{-1}=A$) Multiplicative properties: $A \cap B = B \cap A$ (commutativity: $A \times B = B \times A$) $(A \cap B) \cap C) = A \cap (B \cap C)$ (associativity: $(A \times B) \times C = A \times (B \times C)$) $A \cap X = A$ (multiplicative identity: $A \times 1 = A$) $A \cap \emptyset = \emptyset$ (that is, $A \times 0 = 0$) Distributive: $((A \cup B) - (A \cap B)) \cap C =...$","['abstract-algebra', 'field-theory']"
901581,Symbolic manipulation inside integral,"I'm an undergrad who has just completed the standard calculus sequence (1, 2, and multivariable). I've done well in the courses, however, things like the following, which is a derivation of kinetic energy, still confuse me: $$ \mathbf{F} \cdot \mathrm{d}\mathbf{x} = \mathbf{F} \cdot \mathbf{v} \mathrm{d} t = \frac{\mathrm{d}\mathbf{p}}{\mathrm{d}t} \cdot \mathbf{v} \mathrm{d} t = \mathbb{v} \cdot \mathrm{d} \mathbf{p}  = \mathbf{v} \cdot \mathrm{d}(m \mathbf{v}).$$ Taken from here. I want to understand the symbolic manipulation that often occurs when making meaningful integrations. I was taught that the ending 'dx' term simply signifies the variable to be integrated over. However, it is commonly used, for example, as a term to cancel things out. In general, I see a lot of symbolic manipulation with differential elements that I want to understand. Could you recommend something I could read to better understand this stuff? Thank you.","['multivariable-calculus', 'calculus', 'physics']"
901587,Generators of the Relations of a Galois Extension,"Let $K$ be a Galois extension of $\mathbb{Q}$ of degree $n$. Pick some primitive element and take the roots $a_1, ..., a_n$ of its minimal polynomial. Then the evaluation map $\mathbb{Q}[x_1, ..., x_n] \to K$ with $x_i \mapsto a_i$ gives a surjection to $K$. The kernel is a maximal ideal $M$ of algebraic relations between $a_1, ..., a_n$. Is it true that $M$ is generated as an ideal by the polynomials of $\mathbb{Q}[x_1, ..., x_n]$ that are fixed by the Galois group of $K$ acting on $\mathbb{Q}[x_1, ..., x_n]$ and are contained within $M$?","['abstract-algebra', 'field-theory']"
901602,"If $S\subseteq M\times N$ is embedded, and $S$ and $\{p\}\times N$ intersect transversely in one point, then $\pi_M|_S$ is a diffeomorphism?","I'm trying to prove the equivalence of the following statements: Suppose $M^m$ and $N^n$ are smooth manifolds, $S\subseteq M\times N$ immersed, and $\pi_M$ and $\pi_N$ the projection maps. TFAE: 1) $S$ is the graph of a smooth function $f\colon M\to N$ 2) $\pi_M|_S$ is a diffeomorphism from $S$ onto $M$, 3) For each $p\in M$, the submanifolds $S$ and $\{p\}\times N$ intersect transversely in exactly one point. I've shown $(1)\iff (2)$, and $(2)\implies (3)$. I can include that work if needed. Right now I'm just trying to show $(3)$ implies either of the other two. If $S$ and $\{p\}\times N$ intersect transversely in one point, then the inclusion $\iota\colon S\to M\times N$ is transverse to $\{p\}\times N$, and $\iota^{-1}(p\times N)$ is a singleton. Then this means there exists some $(p,q)\in S$ such that $(p,q)\in p\times N$, thus $\pi_M|_S$ is surjective, and it must be injective since $\iota^{-1}(p\times N)$ is a singleton. The transversality condition implies
$$
T_{(p,q)}(p\times N)+d\iota_{(p,q)}(T_{(p,q)}S)\simeq T_{(p,q)}N+T_{(p,q)}(S)=T_{(p,q)}(M\times N)
$$ so necessarily $\dim(T_{(p,q)}(S))\geq m$, by dimension considerations. As a bijection, it's enough to show $\pi_M|_S$ is either of constant rank, or a local diffeomorphism to prove it's a diffeomorphism. As a projection map, I feel like it should be a submersion onto $M$, and that would do the trick. Note: This is Theorem 6.32 in John Lee's Intro to Smooth Manifolds.","['differential-topology', 'manifolds', 'smooth-manifolds', 'differential-geometry']"
901617,"In $\mathbb{Z}/(n)$, does $(a) = (b)$ imply that $a$ and $b$ are associates?","[ Update : Based on the hints provided by @zcn and @whacka, I believe I have found a solution. See my answer below.] Below, $R$ is a commutative ring with $1$. In John J. Watkins' Topics in Commutative Ring Theory , the author observes that if $a$ and $b$ are associates in $R$, then they generate the same ideal: $(a) = (b)$. This is clearly true, because $a = ub$ and $b = va$ for some units $u$ and $v$, and so $a \in (b)$ and $b \in (a)$, whence $(a) \subset (b)$ and $(b) \subset (a)$. Indeed, we didn't even require $u$ and $v$ to be units to reach this conclusion. The author then claims that if $R$ is not a domain, then the converse may not be true: we may have $(a) = (b)$ even if $a$ and $b$ are not associates. As a ""counterexample"" he gives $R = \mathbb{Z}/(6)$, with $a = 2 + (6)$ and $b = 4 + (6)$. Clearly these elements generate the same ideal: $(a) = (b) = \{0, 2 + (6), 4 + (6)\}$. The author asserts that $a$ and $b$ are not associates, but this is false: $5 + (6)$ is a unit, and $(2 + (6))(5 + (6)) = 4 + (6)$, so $a$ and $b$ actually are associates. I figured that the author botched the example but that surely the basic fact must be true: if $R = \mathbb{Z}/(n)$ then there may be nonassociates $a,b \in R$ with $(a) = (b)$. However, I was not able to find a counterexample and eventually resorted to writing a computer program and found that there are no counterexamples in $\mathbb{Z}/(n)$ for $n \leq 100$. So now I'm starting to think that there is no counterexample in $\mathbb{Z}/(n)$ at all, but I have not yet been able to prove this. My questions: If I am correct that $(a) = (b)$ in $\mathbb{Z}/(n)$ implies that $a$ and $b$ are associates, I would appreciate a nudge in the right direction toward a proof. (But if the proof is elementary, please give only a hint, not a solution :-) I'll post my solution once I have one. In either case, could you please point me to a genuine counterexample? If there is one in $\mathbb{Z}/(n)$ for some $n$, that would be ideal (no pun intended), otherwise a counterexample in any commutative ring with $1$ would be great.","['commutative-algebra', 'ring-theory', 'ideals', 'abstract-algebra']"
901622,Single Variable Calculus Reference Recommendations,"This question is a generalization of the common question asking for calculus references. It is here to abstract away the repetition, and give a canonical resource for calculus references. I'm looking for a resources to learn single-variable calculus.","['book-recommendation', 'calculus', 'reference-request', 'faq']"
901625,Definition of the $\Bbb C^*$-weight of a line bundle,"I'm new to geometric invariant theory and am unsure about a definition. Let $X$ be a smooth projective-over-affine variety equipped with a $\Bbb C^*$ action and let $Z$ be the fixed locus of this action. Let $S$ be the subvariety of $X$ defined by
$$
S=\left\{x\in X:\lim_{t\to 0}t\cdot x\in Z\right\}
$$
and let $m$ be the codimension of $S$ in $X$. Finally, let $\mathcal L$ be the conormal bundle $\bigwedge^m\mathcal N_{S/X}^\vee$ of $S$ in $X$. The paper I am reading refers to ""the $\Bbb C^*$-weight of $\mathcal L$ along $Z$"". My questions are How is ""the $\Bbb C^*$-weight of $\mathcal L$ along $Z$"" defined? Can the $\Bbb C^*$-weight of $\mathcal L$ along $Z$ be easily computed in toy examples? Say for $X=\Bbb C^n$ and $\Bbb C^*$ acting with weights $q_1,\dotsc,q_n$?","['algebraic-geometry', 'geometric-invariant-theory', 'vector-bundles']"
901654,Order of group $GL_{2}\left( \mathbb{F}_{p}\right) $,"I'm having a hard time counting. I need to count the number of elements for
the multiplicative group of invertible $2\times 2$ matrices $GL_{2}\left( \mathbb{F}_{p}\right) $
with elements from the field $\mathbb{F}_{p}$ where $p$ is any prime number.
David S. Dummit and Richar M. Foote's Abstract Algebra (3rd edition) has a
solution on page 413 but that approach is by counting the possible number of
basis of vector spaces but I wanted to see if there was a direct way. The
book says that $\left\vert GL_{n}\left( \mathbb{F}_{p}\right) \right\vert
=\left( p^{n}-1\right) \left( p^{n}-p\right) ...\left( p^{n}-p^{n-1}\right) $
so my case reduces to $p^{4}-p^{3}-p^{2}+p$. For any matrix $\left( 
\begin{array}{cc}
a & b \\ 
c & d%
\end{array}%
\right) $ with $a,b,c,d\in \mathbb{F}_{p}$, I understand that $p^{4}$ is the
total number of 2x2 matrices and that we need to subtract the possible
combinations for $ad=bc$ (non-invertible elements) but I can't account for
the rest. In particular, that plus sign is absolutely baffling. Any hint
would be appreciated. I'd then take this up for the general $n$x$n$ case but
first I need to dispose the 2x2 case. P.S. I tried looking for a pre-existing answer and a related answer is present here but it's by the same approach as the book and I'd like to have an approach without resorting to linear independence. This answer , too, was a case for $p=3$ but I couldn't see the pattern.","['matrices', 'group-theory', 'abstract-algebra', 'combinatorics']"
901657,Constrained Optimization of a function of two variables.,"I was given the following tutorial problem, and I'm having a bit of trouble seeing how it works. I've been asked to find the four critical points of this system, with two of these being degenerate points, one being a maximum, and one being a minimum; $$f(x_1, x_2) = x_1^3 + x_2^3 + 3x_1^2 - 3x_2^2 - 8$$
subject to $g(x_1, x_2) = x_1^2 + x_2^2 - 16 = 0$. First, I constructed a Lagrangian; $$L = x_1^3 + x_2^3 + 3x_1^2 - 3x_2^2 - 8 + \lambda(x_1^2 + x_2^2 - 16)$$ Then, taking the gradient of $L$, we get two equations; $$x_1 (3x_1 + 2(\lambda + 3)) = 0$$
$$x_2 (3x_2 + 2(\lambda - 3)) = 0$$ For the first equation to be satisfied, we have either $x_1 = 0$ or $3x_1 + 2(\lambda + 3) = 0$. In the case that $x_1 = 0$, we have that $x_2 = \pm 4$, due to our initial constraint. If $x_2 = 4$, we get that $\lambda = -3$, and if $x_2 = -4$, we get that $\lambda = 9$. In a similar fashion, if $x_2 = 0$, $x_1 = \pm 4$. If $x_1 = 4$, $\lambda = -9$, and if $x_1 = -4$, $\lambda = 3$. Thus, we have four critical points;
$$(0,4) , \lambda = -3$$
$$(0,-4) , \lambda = 9$$
$$(4,0) , \lambda = -9$$
$$(-4,0) , \lambda = 3$$ Now, I then computed my Hessian matrix;
$$H =\left( \begin{array}{ccc}
6x_1 + 6 + 2\lambda & 0 \\
0 & 6x_2 - 6 + 2\lambda \\
\end{array} \right)$$ Then, I just plug in all my critical points (with their respective $\lambda$ values), and look at both the determinant and the principal minor of the Hessian. Clearly, both $(0,4)$ and $(-4,0)$ are degenerate points, so we've satisfied the first criteria. Looking at the Hessians for $(0,-4)$ and $(4, 0)$, I get that the principal minors for each are both positive, but each of the determinants are negative. For the point $(4,0)$, the principal minor is positive, while the determinant is negative, which implies that the point is a maximum. Similarly, $(0,-4)$ also appears to be a maximum. Now, having verified my working on Wolframalpha, I've correctly identified $(4,0)$ as a local maximum, but it's also telling me that $(0,-4)$ is a local minimum. From all the working I've done, I can't really see how $(0,-4)$ could be a minimum?? Could someone have a look at my working, and see where I've made my mistake??","['optimization', 'multivariable-calculus']"
901672,Finding the sum to infinity,"Question:
Find the sum to infinity for the following series
$$1, -\frac{1}{2}, \frac{1}{2^2}, -\frac{1}{2^3},\cdots$$ What would be the technique used to find such a sum?",['sequences-and-series']
901683,Method for computing limit of a sin function as x tends to zero,"I have a question about computing 
$$ \lim_{x \to 0} \sin\left(\frac{\pi x}{4|x|}\right)$$
I found the limit of $\pi x$ and $4|x|$ seperately and ended with $\sin(\pi/4)$ which is equal to $1/\sqrt{2}$","['infinity', 'functions', 'limits']"
901702,Finding multivariable limit,"I would like to find the following limit $$\lim_{(x,y,z)\to(0,0,0)}\frac{x^3yz+xy^3z+xyz^3}{x^4+y^4+z^4}.$$ It looks like it would be zero since if we put $M=\max\{x,y,z\}$ and $m=\min\{x,y,z\}$, then $$\Big|\frac{x^3yz+xy^3z+xyz^3}{x^4+y^4+z^4}\Big| \leq \Big|\frac{M^5}{m^4}\Big|$$ so the exponent in the denominator is bigger than the exponent in the numerator. But how do I actually calculate this limit? Thank you.","['multivariable-calculus', 'calculus', 'limits']"
901713,Constructing a group automorphism making a diagram commute,"Let $A,B$ be groups, suppose we have an epimorphism $p:A \to B$. Let $\phi \in \operatorname{Aut}(A)$. Does there exist some $\varphi \in \operatorname{Aut}(B)$ such that the following diagram \begin{array}{c}
A & \xrightarrow{\phi} & A \\
 \downarrow{p} & & \downarrow{p} \\
 B & \xrightarrow{\varphi} & B
\end{array} commutes? My thoughts: Take $b \in B$. Since $p$ is surjective,
there is some $a \in A$ such that $p(a) = b$ (the possible non-uniqueness of such an $a$ might be a problem). Then try to define $\varphi(b)$ as $p(\phi(a)) \in B$. 
If we fix some $a_b \in p^{-1}(b)$ for each $b$ with the properties that $a_{b_1} a_{b_2} = a_{b_1 b_2}$ (**) (not sure we can do this), then the construction might work: $\bullet$ $\varphi(b_1) \varphi(b_2) = p(\phi(a_{b_1})) p (\phi(a_{b_2})) = p(\phi(a_{b_1} a_{b_2}))$. Since $a_{b_1} a_{b_2} = a_{b_1 b_2}$ $\varphi$ is a homorphism. $\bullet$ $\varphi$ is maybe injective and surjective (I haven't checked this as I'm not sure of (**)) Also we might need the property that $\phi^{-1}(\{ a_{b} : b \in B\}) , \phi (\{ a_{b} : b \in B\})  \subseteq \{ a_{b} : b \in B\}$. There seem to be too many properties required for it to be true!! Edit: The answer is indeed no, as it was pointed out in the answer by user1729, unless $\phi( \ker(A \xrightarrow{p} B)) = \ker(A \xrightarrow{p} B)$. This example is just to make sure I understand the construction correctly. Example: Suppose that there is a surjective morphism $p:A_1 \times A_2 \to B$ and that $\phi: A_1 \times A_2 \to A_2\times A_1 (\cong A_1 \times A_2)$ is just a permutation of the factors. The kernel $N$ of $p$ can be embedded as $N_1 \times N_2 \subseteq A_1 \times A_2$. Then $\phi(N) = N$ clearly, and so there is a $\varphi$ making the diagram commute. (Is the above example correct?)",['group-theory']
901714,How to check for convexity of function that is not everywhere differentiable?,"I have a question. I have just been introduced to the subject of convex sets and convex functions. 
I read this in wikipedia that a practical test for convexity is - 
to check whether the 2nd derivative ( Hessian matrix ) of a continuous differentiable function in the interior of the convex set is non-negative ( positive semi-definite ). So how to check for the convexity of functions like $f(x)=|x|$ which is differentiable at all points except at $x=0$ which coincidentally is actually its global minimum? Thanks all for answering.","['convex-optimization', 'derivatives', 'analysis']"
901721,Integral's Closed-form expression in terms of hypergeometric function,"I want to solve the following integral: $$I = 2\left[\int_{0}^{1}\dfrac{y^m}{(1 - ay)^{m + 1}\sqrt{1 - y^2}}\mathrm{d}y+\int_{0}^{1}\dfrac{y^m}{(1 + ay)^{m + 1}\sqrt{1 - y^2}}\mathrm{d}y\right]$$ where $0 \leq a \leq 1$ and $m$ is an arbitrary positive constant. I have tried to expand each term of the above integral in series form and solve it. I have got answer in an infinite series form, which is not a closed-form expression. Infinite series expression (Derivation is given below):
$$
\boxed{
I = 4\sum_{t = 0}^{\infty}\sum_{\underset{u \neq \text{Odd}}{u=0}}^{t}\frac{a^u(m+1)_{u}}{u!}\frac{(1/2)_{t-u}}{(t-u)!}\frac{1}{(m+1)+2t-u}.}
$$ However, Maple gives a beautiful closed-form expression in terms of hypergeometric function, which is as follows: $$I = \frac{2^{m+1}\left[\sqrt{\pi}\Gamma\left(\frac{m + 1}{2}\right)^2 {_2F_1\left(\left[\frac{m + 1}{2},\frac{m+ 1}{2}\right], \left[\frac{1}{2}\right], a^2\right)}\right]}{\sqrt{\pi}\Gamma(m + 1)}$$ where ${_2F_1\left(\left[a, b\right], \left[w\right], z \right)}$ is a hypergeometric function. How can I obtain the above expression given by Maple? Derivation of infinite series expression : Following is the derivation for the infinite series expression that I could figure out. Let the first integral of $I$  be $$I_1 = \int_{0}^{1}\dfrac{y^m}{(1 - ay)^{m + 1}\sqrt{1 - y^2}}\mathrm{d}y = \int_{0}^{1} y^m(1 - ay)^{-(m + 1)}(1 - y^2)^{-1/2}\mathrm{d}y.$$ 
Then, we make use of the following relation
$$ (1 - y)^{-m} = \sum_{t=0}^{\infty}\frac{(m)_{t}}{t!}y^{t}$$
where $(m)_{t}$ denotes the Pochhammer symbol and is defined as $(m)_{t} = (t)\times(m+1)\times\dotsc\times(m+t-1) = \frac{\Gamma(m+t)}{\Gamma(m)}$.
Then, we have
$$ (1 - ay)^{-(m+1)} = \sum_{t=0}^{\infty}\frac{(m+1)_{t}}{t!}(ay)^{t}.$$
On the similar lines, one can write 
$$(1 - y^2)^{-1/2} = \sum_{u=0}^{\infty}\frac{(1/2)_{u}}{u!}y^{2u}.$$
Thus, $I_1$ can be written as
$$I_1 =  \int_{0}^{1} y^m\left(\sum_{t=0}^{\infty}\frac{a^t(m+1)_{t}}{t!}y^{t}\right)\left(\sum_{u=0}^{\infty}\frac{(1/2)_{u}}{u!}y^{2u}\right)\mathrm{d}y.$$
Simplifying further,
$$I_1 = \int_{0}^{1} \left(\sum_{t=0}^{\infty}\frac{a^t(m+1)_{t}}{t!}y^{t+m}\right)\left(\sum_{u=0}^{\infty}\frac{(1/2)_{u}}{u!}y^{2u}\right)\mathrm{d}y.$$
We make use of the following relation
$$
\left(\sum_{p=0}^{\infty}z_p\right)\left(\sum_{q=0}^{\infty}g_q\right) = \sum_{t = 0}^{\infty}\sum_{u=0}^{t}z_u g_{u-t}.
$$
Then, we can write
$$
I_1 = \int_{0}^{1} \sum_{t = 0}^{\infty}\sum_{u = 0}^{t}\frac{a^uy^{u+m}(m+1)_{u}}{u!}\frac{y^{2(t-u)}(1/2)_{t-u}}{(t-u)!}\mathrm{d}y.$$
Then,
$$
I_1 = \int_{0}^{1} \sum_{t = 0}^{\infty}\sum_{u = 0}^{t}\frac{a^u(m+1)_{u}}{u!}\frac{(1/2)_{t-u}}{(t-u)!}y^{(m+2t-u)}\mathrm{d}y.
$$
After integration, we can write
$$
I_1 = \sum_{t = 0}^{\infty}\sum_{u = 0}^{t}\frac{a^u(m+1)_{u}}{u!}\frac{(1/2)_{t-u}}{(t-u)!}\frac{1}{(m+1)+2t-u}.
$$
Similarly, 
$$
I_2 = \int_{0}^{1}\dfrac{y^m}{(1 + ay)^{m + 1}\sqrt{1 - y^2}}\mathrm{d}y = \sum_{t = 0}^{\infty}\sum_{u = 0}^{t}\frac{(-1)^u a^u(m+1)_{u}}{u!}\frac{(1/2)_{t-u}}{(t-u)!}\frac{1}{(m+1)+2t-u}.
$$
Thus, we can write $I$ as
$$
\boxed{
I = 4\sum_{t = 0}^{\infty}\sum_{\underset{u \neq \text{Odd}}{u=0}}^{t}\frac{a^u(m+1)_{u}}{u!}\frac{(1/2)_{t-u}}{(t-u)!}\frac{1}{(m+1)+2t-u}.}
$$","['definite-integrals', 'sequences-and-series', 'special-functions', 'hypergeometric-function']"
901723,Relation between geodesics and exponential map for Lie groups,"I've been trying to find a clear explanation on the Internet but failed unfortunately, so I'm asking here. How does the exponential map relate to parallel transport and geodesics for Lie groups. If it makes things simpler we can assume the Lie group to be compact and/or linear. Also if there is a choice between left and right action/invariance etc. I'd prefer the left one. There are several bits of information that I picked up here and there, but I'm struggling to put them together consistently: $\exp_p(v):=\gamma_v(1)$, where $\gamma$ is the unique geodesic with $\gamma(0)=p$ and $\dot\gamma(0)=v$. $\exp(tv)$ is in general not a geodesic $\exp(tv)$ is the integral curve of the left-invariant vector field associated to $v\in\mathfrak g$ The canonical  connection $\nabla$ on $G$ is given by $\nabla_XY=0$ for all left-invariant vector fields $X$ and $Y$ The canonical parallel transport is given by left-multiplication. Can the above statements hold consistently for a Lie group $G$? What is the precise relation between
$$
\exp
\quad\Leftrightarrow\quad\text{geodesics}
\quad\Leftrightarrow\quad\text{connection}
\quad\Leftrightarrow\quad\text{parallel transport}
$$","['connections', 'differential-geometry', 'geodesic']"
901735,Meaning of a set in the exponent,"Let $ D = 2^\mathbb{N} $, i.e., D is the set of all sets of natural numbers. What's the meaning of this definition? Intuitively, I would suggest that
$ D = \{1,2,4,...\} $ but the explanation ""set of all sets"" leads me to the guess that this is wrong.","['notation', 'elementary-set-theory']"
901763,"Natural numbers verifying $P(n) = n^2 - 42n + 440$, where $P(n)$ is the product of the digits","Let $P(n)$ be the product of the digits of the number $n$ , with $n \in  \mathbb{N}$ . What is the product of all the natural numbers $n$ that verify the equation $P(n) = n^2 - 42n + 440$ ? I factorized the polynomial to get $P(n) = (n - 20)(n - 22)$ . Then I observed that $n = 20$ is a solution because $P(20) = 0$ . But at this point I got stuck, so I wrote an Haskell program that would calculate the solutions for me. After letting it run for a while I checked the output, which was: $[18, 20, 24]$ . So apparently the answer is $18 \cdot 20 \cdot 24 = 8640$ . What is the correct, mathematical way to solve this?","['algebra-precalculus', 'recreational-mathematics']"
901770,1D manifold is diffeomorphic to $\mathbb R$ or to $S^1$,In his ODE classic V.I. Arnold considers easy to see ( легко видеть ) that every one-dimensional (connected and without boundary) differentiable manifold is either diffeomorphic to $\mathbb R$ (if it is not compact) or to $S^1$ (if it is compact). Is it indeed that easy? Can this be shown using elementary tools?,"['reference-request', 'differential-geometry', 'smooth-manifolds']"
901771,Statistics; looking for a practical example-based book,"I'm looking for an example-based 2nd year or thereabouts undergraduate statistics textbook in the style of Engineering Maths by Ken Stroud ( http://www.amazon.co.uk/Stroud-Engineering-Mathematics/dp/0387912185 easily my favourite maths text book). At the moment I feel I've been exposed to lots of statistical tests, but I'm not confident of using any of them (or knowing the correct one to use) in anger on real-world problems. If it could cover Bayesian probability as well, that would be excellent. Thanks",['statistics']
901778,"Finding the partial derivatives of $h(x)=\int_{0}^{\|x\|} f(t)\, dt$","Find the partial derivatives of $$h(x_1,\dots,x_n)=\int_{0}^{\|x\|} f(t) dt$$ where $\|x\|$ is the Euclidean norm of $x=(x_1,\dots,x_n)$ and $f$ is some continuous function. I'm sorry but I'm really not too sure how to approach this. Any help would be great! (This is not homework, I'm preparing for an exam.)","['multivariable-calculus', 'integration', 'partial-derivative']"
901780,A strange answer for $\int _{-1}^1 \log x\; dx$,I typed $\int _{-1}^1 \log x\; dx$ on Wolfram Alpha. It is giving the answer to be $-2+i\pi$. Can someone please explain what is happening?,"['definite-integrals', 'wolfram-alpha', 'integration', 'complex-analysis']"
901790,non-Borel subset of uncountable Tychonoff space,Let $X$ be an uncountable Tychonoff space. Must there exist a non-Borel subset of $X$?,"['general-topology', 'measure-theory', 'descriptive-set-theory']"
901819,"Direct formula for area of a triangle formed by three lines, given their equations in the cartesian plane.","I read this formula in some book but it didn't provide a proof so I thought someone on this website could figure it out. What it says is:
If we consider 3 non-concurrent, non parallel lines represented by the equations :
$$a_1x+b_1y+c_1=0$$
$$a_2x+b_2y+c_2=0$$
$$a_3x+b_3y+c_3=0$$
Then the area of the triangle that these lines will enclose is given by the magnitude of :
$$\frac{det\begin{bmatrix}a_1 & b_1 & c_1\\a_2 & b_2 & c_2\\a_3 & b_3 & c_3\end{bmatrix}^2}{2C_1C_2C_3}$$
Where $C_1,C_2,C_3$ are the co-factors of $c_1,c_2,c_3$ respectively in the above matrix. What I'm wondering is, where did this come from? And why isn't it famous? Earlier we had to calculate areas by finding the vertices and all but this does it in a minute or so and thus deserves more familiarity.","['geometry', 'coordinate-systems', 'algebra-precalculus', 'analytic-geometry', 'determinant']"
901826,Find all positive integers satisfying $\frac{2^n+1}{n^2} =k $ [duplicate],"This question already has answers here : How many rationals of the form $\large \frac{2^n+1}{n^2}$ are integers? (6 answers) Closed 9 years ago . Find all positive integers satisfying 
$$\frac{2^n+1}{n^2} =k $$ where $k$ is a integer. I can't just come up with a solution.","['diophantine-equations', 'number-theory']"
901835,Trigonometry Identity: $\tan \theta\sin \theta + \cos \theta = \sec \theta$,Sorry if my question seems too simple. I cannot find a proof and my text book does not provide one either. I am supposed to prove: $$\tan \theta \times \sin \theta + \cos \theta = \sec \theta$$ I know that $\sec = \frac{1}{\cos\theta}$. But I do not know how to prove that $\tan \theta \times \sin \theta + \cos \theta = \frac{1}{\cos \theta}$. I appreciate if someone point me to the right direction.,['trigonometry']
901849,prove equilateral triangle,"Recently I have encountered such a proving problem. As shown below, given a $\triangle ABC$, $AD$ intersects $BC$ at $D$ so that $AD$ is perpendicular to $BC$, $BE$ intersects $AC$ at $E$ so that $BE$ is the angle bisector of $\angle ABC$, $CF$ intersects $AB$ at $F$ so that $CF$ is the median of $AB$, $AD$, $BE$, $CF$ intersect at a common point. We need to prove that triangle ABC is an equilateral triangle. I have tried several approaches, but it seems useless in the end. Any thoughts? Or is it actually not an equilateral triangle?",['geometry']
901895,Chess rook problem,"Determine the number of ways for a rook to get from left bottom corner to top right corner of table $3\times 7$, if the rook can only move top and right. (Two ways are different if rook stops at least at one place it didn't in the first way.) My idea was that rook has to move 8 squares but I don't know how to continue. Maybe there is something with on how many ways a natural number can be written as a sum of natural numbers.
EDIT:Rook can move more than 1 space at one move",['combinatorics']
901901,"Is $\mathbf {B^TAB}$ non-singular for a non-singular $\mathbf A$, and $\mathbf {B}$ with full column-rank?","If $\mathbf A$ is any square non-singular matrix of dimension $n \times n$. And $\mathbf B$ is a $n \times m$ matrix with $\mathrm{rank(\mathbf B)} = m$. Is the full rank condition of matrix $\mathbf B$ both sufficient as well as necessary to state that the matrix product $\mathbf {B^TAB}$ is non-singular? i.e., can we write: $ \mathbf {A}$ is non-singular $\iff$ $\mathrm{rank(\mathbf B)} = m$ and $\mathbf {B^TAB}$ is non-singular","['matrices', 'linear-algebra', 'matrix-rank']"
901910,Matrix-version of $\frac1a+\frac1b=\frac{a+b}{ab}$?,"In my Linear Algebra Textbook one exercise is to find the matrix analogue of
$$
\frac1a+\frac1b=\frac{a+b}{ab}
$$
my immediate response was
$$
A^{-1}+B^{-1}=A^{-1}(A+B)B^{-1}
$$
is that a reasonable answer or do someone have better suggestions? NOTE: $A$ and $B$ are assumed invertible $n\times n$ matrices.",['linear-algebra']
