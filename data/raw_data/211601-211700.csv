question_id,title,body,tags
4259576,Question about the proof of Ulam's theorem,"According to Bogachev's book , Ulam's theorem (Theorem 1.12.40) states that if a finite countably additive measure $\mu$ is defined on all subsets of the set $X$ of cardinality $\aleph_1$ and vanishes on all singletons, then it is identically zero. In the proof, by hypothesis, $X$ can be well-ordered in such a way that, for every $y\in X$ , the set $\{x:x<y\}$ is at most countable. Then, define an injective function $f$ from this set into $\mathbb{N}$ . This means that for every pair $(x,y)$ with $x<y$ , $f(x,y)=n$ for some natural number $n$ . Bogachev then defined, for every $x\in X$ and every natural $n$ , $$A_{x}^{n}=\{y\in X:x<y;f(x,y)=n\}$$ I know that $A_{x}^{n}\cap A_{z}^{n}=\emptyset$ where $x\ne z$ . But is $A_{x}^{n}\cap A_{x}^{n'}=\emptyset$ where $n\ne n'$ ? I'm thinking that it is because if there were a $y\in A_{x}^{n}\cap A_{x}^{n'}$ , then $x<y$ , $f(x,y)=n$ , and $f(x,y)=n'$ . That's a contradiction. Am I right?","['measure-theory', 'cardinals', 'set-theory']"
4259609,Why do we add a negative sign in front of a square root of a (negative number squared)?,"Recently while learning calculus in High school, my teacher mentioned how to do limits at infinity  to find horizontal asymptotes by dividing by the largest degree of $x$ . For some limits, there is a square root so you have to divide inside the square root, for example if you have the limit: $\lim_{x \to -\infty} \frac{\sqrt{4x^2+3}}{3x+2}$ . The way my teacher taught it was you divide by $x$ , so you get $3 + \frac{2}{x}$ on the bottom. On the top, you are supposed to get $-\sqrt{4 +\frac{3}{x^2}}$ and plug in negative infinity. However, I don't get why you have to add the negative sign in front of the numerator, although I understand it has something to do with the limit being at negative infinity (as this doesn't happen when the limit goes to infinity). My teacher mentioned something about negative numbers squared and how they work when square rooted, and he mentioned this definition: $\sqrt{x^2}$ = $-x$ if $x<0$ $\sqrt{x^2}$ = $x$ if $x>0$ However, this doesn't make sense to me. If $x = -5$ for example, wouldn't it be $\sqrt{(-5)^2}$ , which is $\sqrt{25}$ which is 5 instead of -5? I'm not sure why the above ""definition"" or equation exists?","['limits', 'limits-without-lhopital']"
4259731,"Prove that $M_{n , k} = \frac{1}{n} \sum_{i = 1}^{n} (X_{i} - \bar{X_{n}})^{k} \overset{p}\longrightarrow \mu_{k}$","I want to prove that $M_{n , k} = \frac{1}{n} \sum_{i = 1}^{n} (X_{i} - \bar{X_{n}})^{k} \overset{p}\longrightarrow \mu_{k}$ Where $X_{1} , X_{2} , ... , X_{n}$ are i.i.d with expected value $E(X_{i}) = \mu$ Also let the k-th central moment be $$\mu_k = E(X - \mu)^{k}$$ A natural estimator is the k-th sample moment,given by $$M_{n , k} = \frac{1}{n} \sum_{i = 1}^{n} (X_{i} - \bar{X_{n}})^{k}$$ I know that for $k = 2$ due to the law of large numbers and the fact that $g(x) = x^{2}$ is a continuous function. That $$M_{n , 2} = \frac{1}{n} \sum_{i = 1}^{n}(X_{i} - \bar{X_{n}})^{2} = \frac{1}{n} \sum_{i = 1}^{n} X_{i}^{2} - \bar{X_{n}}^{2} \overset{p}\longrightarrow \mu_{2}$$ But how do I prove this for any k since. $$M_{n , K} = \frac{1}{n} \sum_{i = 1}^{n}(X_{i} - \bar{X_{n}})^{K} \ne \frac{1}{n} \sum_{i = 1}^{n} X_{i}^{K} - X_{n}^{k}$$","['statistics', 'convergence-divergence', 'probability-theory']"
4259765,Proving $\frac{\cos A - \cos B}{\sin A + \sin B} = \frac{\sin B - \sin A}{\cos A + \cos B}$,Prove $$\dfrac{\cos A - \cos B}{\sin A + \sin B} = \dfrac{\sin B - \sin A}{\cos A + \cos B}$$ I tried as shown below and am not sure how to do it. Your help is appreciated. Thanks. Proving from left hand side: $$\dfrac{\cos A}{\sin A + \sin B} - \dfrac{\cos B}{\sin A + \sin B}$$ $$=\dfrac{\dfrac{\cos A}{\sin A}} {\dfrac{\sin A+\sin B}{\sin A}} - \dfrac{\dfrac{\cos B}{\sin B}}{\dfrac{\sin A+\sin B}{\sin B}}$$ $$= \dfrac{\dfrac{1}{\tan A}}{1+\dfrac{\sin B}{\sin A}}- \dfrac{\dfrac{1}{\tan B}}{1+\dfrac{\sin A}{\sin B}}$$,['trigonometry']
4259783,"Is $(a_{j})_{j=0}^{\infty}$ with $a_{j+1} = a_{j}^2\sin(a_{j})$ bounded, divergent or convergent?","Can we choose $a_{0} \in \mathbb{R}$ so that the sequence $(a_{j})_{j=0}^{\infty}$ defined via $a_{j+1} = a_{j}^2\sin(a_{j})$ is bounded but not convergent? Note that if $p$ is a fixed point of the function $f(x) = x^2\sin(x)$ then $p, f(p), f(f(p)),...$ converges to $p$ . If $p < 1$ then the sequence mentioned in the previous sentence converges to 0. I don't know whether we can choose $a_{0} \in \mathbb{R}$ so that the above sequence is divergent. Edit: it seems achille hui has found an $a_{0}$ which is the root of $x\sin (x) = -1$ , this $a_{0}$ satisfies the first question. Can we now choose an $a_{0}$ so that the above sequence is unbounded (divergent)?","['fixed-point-theorems', 'real-analysis', 'functional-analysis', 'sequences-and-series', 'trigonometry']"
4259799,Equivalent definitions of n-acyclic morphisms / Problem with spectral sequence,"I'm trying to understand section (VI.4) on smooth base change in Milne's Étale Cohomology.
He defines a morphism $g \colon Y \longrightarrow X$ to be $n$ -acyclic (I only care about the case $n \geq 0$ ) if for any scheme $X'$ étale and of finite type over $X$ and any appropriate torsion sheaf $\mathcal{F}$ on $X'$ , $H^i(X', \mathcal{F}) \longrightarrow H^i(Y', g'^*\mathcal{F})$ is bijective for all $0 \leq i \leq n$ and injective for $i=n+1$ .
Here $g' \colon Y' = Y \times_X X' \longrightarrow X'$ is the base change of $g$ . It is stated that this is equivalent to the condition $(*)$ that for any $X'$ and $\mathcal{F}$ as above, the map $\mathcal{F} \longrightarrow g'_*g'^*\mathcal{F}$ is an isomorphism and $R^jg'_*(g'^*\mathcal{F}) = 0$ for $1 \leq j \leq n$ . I see why the definition implies the condition $(*)$ .
However, I'm a bit confused about the other direction.
I don't have a lot of experience with spectral sequences yet, I think that's where the problem lies. The Leray spectral sequence for $g'$ reads $H^i(X',R^jg'_*(g'^*\mathcal{F})) \Longrightarrow H^{i+j}(Y',g'^*\mathcal{F})$ .
Now, using $(*)$ , we find that, since the differentials on the $r$ -th page have grading $(r,1-r)$ , the entries at $(i,0)$ with $0 \leq i \leq n+1$ in the spectral sequence never have any non-zero arrows coming in our out of them on any page. Hence, they are already stable from the second page on, i.e. $H^i(X', \underbrace{g'_*g'^*\mathcal{F}}_{\mathcal{F}}) \cong H^i(Y',g'^*\mathcal{F})$ for $0 \leq i \leq n+1$ .
But this is not what we want for $i = n+1$ . It would be nice if someone could point out what I'm misunderstanding.","['etale-cohomology', 'spectral-sequences', 'algebraic-geometry', 'sheaf-cohomology']"
4259805,What is the number of transitive relation containing exactly three ordered pairs?,"On a set $A=\{1,2,3,\cdots,n\}$ , what is the number of transitive relations, $t_{n,3}$ that contain exactly $3$ ordered pairs? An example is the relation $\{(1,2),(2,3),(1,3)\}$ I calculated the same for some values of $n$ and the same are tabulated under: \begin{array}{|c|c|}
\hline
\color{red}n&\color{red}{t_{n,3}}\\
\hline
0&0\\
\hline
1&0\\
\hline
2&2\\
\hline
3&43\\
\hline
4&276\\
\hline
\end{array} What would be a general formula for $t_{n,3}$ ?","['order-theory', 'combinatorics', 'relations']"
4259836,Proving $\frac{2\sin x+\sin 2x}{2\sin x-\sin 2x}=\csc^2x+2\csc x \cot x+\cot^2x$,"Prove $$\dfrac{2\sin x+\sin 2x}{2\sin x-\sin 2x}=\csc^2x+2\csc x \cot x+\cot^2x$$ Proving right hand side to left hand side: $$\begin{align}\csc^2x+2\csc x \cot x+\cot^2x 
&= \frac{1}{\sin^2x}+\dfrac{2\cos x}{\sin^2x}+\dfrac{\cos^2x}{\sin^2x} \tag1 \\[0.8em]
&=\frac{\cos^2x+2\cos x+1}{\sin^2x} \tag2 \\[0.8em]
&=\frac{1-\sin^2x+1+\dfrac{\sin 2x}{\sin x}}{\sin^2x} \tag3 \\[0.8em]
&=\frac{\;\dfrac{2\sin x-\sin^3x+\sin 2x}{\sin x}\;}{\sin^2x} \tag4 \\[0.8em]
&=\frac{2\sin x-\sin^3x+\sin2 x}{\sin^3x} \tag5
\end{align}$$ I could not prove further to the left hand side from here. I would need help. Thank you in advance.",['trigonometry']
4259851,$f(x)=\lim\limits_{t\rightarrow0}\frac{\sin(\sin(k\pi/e^{1/t})e^{1/t}[x^2-x+\pi])}{\ln (k+[x]^2)}$ where k is an integer and $x\in \mathbb R$?,"$f(x)=\displaystyle\lim\limits_{t\rightarrow0}\frac{\sin(\sin(k\pi/e^{1/t})e^{1/t}[x^2-x+\pi])}{\ln
 (k+[x]^2)}$ where [.] denotes the greatest integer function, and $k$ is
an integer. For what values of $k$ will $f(x)$ be continuous $\forall x\in \mathbb R$ For what values of $k$ will $f'(x)$ be continuous $\forall x\in \mathbb R$ For what values of $k$ will $f''(x)$ be continuous $\forall x\in \mathbb R$ $f(x)=\displaystyle\lim\limits_{t\rightarrow0}\frac{\sin(\sin(k\pi/e^{1/t})e^{1/t}[x^2-x+\pi])}{\ln (k+[x]^2)}=\lim\limits_{t\rightarrow0}\frac{\sin(k\pi[x^2-x+\pi])}{\ln (k+[x]^2)}$ , Using property $\displaystyle\lim\limits_{x\rightarrow0}\frac{\sin(x)}{x}=1$ $f(x)$ will be a continuous constant function equal to zero $\forall x\in \mathbb R$ if $k>0$ (since log function is not defined for negative numbers or zero). There is a chance that $f(x)$ can be discontinuous at $k=1,x=0$ , so we need to check the function at that point. When $\displaystyle k=1^+,\lim\limits_{t\rightarrow0}\frac{3\sin(k\pi)}{\ln (1^+)}=0/h=0$ where $h$ is an infinitesimally small positive number Similarly when $\displaystyle k=1^-,\lim\limits_{t\rightarrow0}\frac{3\sin(k\pi)}{\ln (1^-)}=0/h=0$ where $h$ is an infinitesimally small negative number So $f(x)$ should be continuous and differentiable at at $k=1$ , but the answer says $1$ is excluded and $k>1$ . What am I missing? Also can we say the second derivative doesn't exist for a continuous function.","['limits-without-lhopital', 'continuity', 'functions', 'limits', 'derivatives']"
4259896,Computation of Lie Derivative using Cartan's Magic Formula,"In section 2.6 of the notes by Natario, he uses Cartan's magic formula $$\mathcal{L}_V\omega = i_V(d\omega) + d(i_V\omega)$$ to compute the second fundamental form of timelike hypersurface $\sigma = \sigma_0$ in $$g_1 = -d\tau^2 + a^2(\tau) \big[ d\sigma^2 +\sigma^2(d\theta^2 + \sin^2\theta d\varphi^2) \big]$$ This surface has unit normal $\frac{1}{a}\frac{\partial}{\partial \sigma}$ . I am confused however about how to compute $\mathcal{L}_n(a^2(\tau)d\sigma^2)$ term because it seems to give $-\dot{a}\ d\tau d\sigma$ , but this term does not appear in claimed second fundamental form $K = a(\tau)\sigma_0 (d\theta^2 + \sin^2\theta d\varphi^2)$ . I am sure I have done something foolish, and would be very grateful to be pointed in the right direction! $\frac{1}{2}\mathcal{L}_n (a^2 d\sigma^2) = a(\mathcal{L}_na)d\sigma^2 + a^2 (\mathcal{L}_nd\sigma)d\sigma\\
	= a^2 d(d\sigma(n))d\sigma\\ 
	= a^2 d( \frac{1}{a(\tau)})d\sigma\\$","['semi-riemannian-geometry', 'general-relativity', 'differential-geometry']"
4259923,Solving a system of quadratic inequalities,"I have the following equation: $$\bigg\lfloor \sqrt {c^2(x^2+y^2)+2c(x+y)+2} +\frac{1}{2} \bigg\rfloor = c$$ For a given positive integer $c$ , I am trying to define algebraically, in terms of $x$ & $y,$ the region in 2D space for which the above equation and the following conditions hold: $c\in \mathbb N$ $0\leq x, y \leq 1$ I.E, which constraints/inequalities $x$ & $y$ must fulfill (in terms of $c$ ) This is what I have done so far: Getting rid of the floor yields $2$ quadratic inequalities: $$(1)\quad \sqrt {c^2(x^2+y^2)+2c(x+y)+2} +\frac{1}{2}\geq c$$ $$(2)\quad \sqrt {c^2(x^2+y^2)+2c(x+y)+2} +\frac{1}{2}\lt c + 1$$ It is possible to substitute $u = x^2+y^2, v=x+y$ (and adjust constraints accordingly), and after some algerbra arrive at the $2$ linear inequalities (assuming I haven't made any silly algebraic mistakes): $$v\geq \frac {4c^2-4c-4c^2u-7}{8c}=-\frac c2u+\frac c2-\frac 7{8c}-\frac 1 2$$ $$v\lt \frac {4c^2+4c-4c^2u-7}{8c}=-\frac c2u+\frac c2-\frac 7{8c}+\frac 1 2$$ So I have $2$ lines with equal negative slope $=-\frac c2,$ and $y$ -interception differing by $1$ unit. Fulfilling just these $2$ inequalities yields a region of interest bounded between the lines. Adding the constraints $0\leq u,v \leq 2,$ which stem from $0\leq x,y \leq 1$ results in a yet more bounded region. Sketching this allows me to define the desired region (in 2D space of $u$ & $v$ ) graphically: The picture was created in powerpoint so it is not 100% precise but the idea is clear. It is important to note: The desired region is colored in RED The square has dimensions $2\times 2$ The axis represent $u$ & $v,$ rather than the originally desired $x$ & $y$ The line with larger $y$ -interception itself is NOT part of the region (if my logic is correct) The line with smaller $y$ -interception itself IS part of the region (again, if my logic is correct) Different values of $c$ result in different polygons of the desired region, which stem from alternating boundaries of $u$ and $v$ - either the polygon is bounded by $0$ & $2,$ or by the inequalities - depending on which is maximal/minimal. For example, for some values of $c,$ a trapezoid would symbolize the region of interest. I was hoping I could keep working my way from here to algebraic constraints regarding $x$ & $y$ as is originally intended. But this leads to a system of many inequalities and a big algebraic mess. Perhaps this is possible to do, but I am not sure how to do this. And perhaps a different approach to the original equation yields a more appealing, simpler, solution. I also tried feeding this into Mathematica, but I think I did it wrong, and it produced constraints for $y$ which depend on $x,$ not what I am looking for. I want to find constraints on $x$ and $y$ only in terms of $c.$ I can assume WLOG $x\geq y$ because the original equation is symmetric, but again I am not sure where this leads me. Any guidance and algebraic help / the solution of $x$ and $y$ is appreciated. Would also like to hear if anybody spots any algebraic mistakes and logic faults!","['systems-of-equations', 'number-theory', 'geometry', 'linear-algebra', 'inequality']"
4259962,Extremising $\int_0^1 f(x) f(1-x) \ \mathrm{d}x$ subject to length of $f$ and endpoints,"I have recently learnt some Calculus of Variations and was trying to apply this to a question I made: Over all functions $f: [0, 1] \to \mathbb{R}$ satisfying $f(0) = f(1) = 0$ with fixed curve length $\ell \geq 1$ (i.e. $\int_0^1 \sqrt{1 + (f'(x))^2} \ \mathrm{d}x = \ell$ ), find $f$ which maximise and minimise \begin{align*}
\int_0^1 f(x) f(1 - x) \ \mathrm{d}x.
\end{align*} Ordinarily, I would proceed by Lagrange Multipliers and use Euler-Lagrange equations to solve for $f$ , but I'm not sure how this would work with $f$ being shifted above. I considered rederiving the Euler-Lagrange equation for this as well, but the fact that it is a shifted argument makes me think this would likely not be nice to work with. Any help would be appreciated, thanks!","['calculus-of-variations', 'euler-lagrange-equation', 'calculus', 'optimization', 'constraints']"
4259963,Questions about a linear PDE,"Consider the following linear PDE: $$u\frac{\partial}{\partial u}\left(u\frac{\partial f}{\partial u}\right) + v\frac{\partial}{\partial v}\left(v\frac{\partial f}{\partial v}\right)=\frac{\partial^2 f}{\partial u^2} + \frac{\partial^2 f}{\partial v^2}\tag{1}$$ where we have $f(u,v)$ . Has anyone studied this PDE in the literature? Are there weak solutions to $(1)?$ I constructed this PDE by deriving the Laplacian in two different coordinate systems and then made the coordinate variables the same and equated the expressions, which is what you see on the LHS and RHS. I'm not sure what this process is called or if anyone has used it before.","['coordinate-systems', 'multivariable-calculus', 'laplacian', 'partial-differential-equations']"
4260011,Is there any situation where it is interesting to rewrite the terms of a sequence as the partial sums of a series?,"Any sequence $(a_n)\in\mathbb C^\mathbb N$ can always be re-written as a sequence of partial sums $\left(\sum_{k=0}^n u_k\right)$ for some sequence $(u_k)\in\mathbb C^\mathbb N$ (for instance, take $u_0=a_0$ and $u_n=a_n-a_{n-1}$ when $n\geq 1$ ). My question is, are there any situations where this transformation would make it easier to study the sequence $(a_n)$ ? For example, are there any cases where this method would make it easier to determine if $(a_n)$ is convergent or divergent, or to compute its limit?","['limits', 'analysis', 'sequences-and-series']"
4260015,"Geometrically relating the solution to $a_1x+b_1y+c_1=0$ and $a_2x+b_2y+c_2=0$, and the cross product of vectors $(a_1,b_1,c_1)$ and $(a_2,b_2,c_2)$","I recently came across two lines of the form $a_{1}x+b_{1}y+c_{1}=0$ and $a_{2}x+b_{2}y+c_{2}=0$ . I had noticed that, assuming the two lines intersect, if we multiply the cross product of the vectors $(a_{1},b_{1},c_{1})$ and $(a_{2},b_{2},c_{2})$ by a scalar such that the $z$ component of that vector is $1$ , then the resulting first two elements of that vector is the solution to this system of equations. I was wondering why this is true or if there exists a proof regarding this.","['systems-of-equations', 'linear-algebra', 'geometry']"
4260065,"""Local parametrizations"" implying ""Local flat"" in the two definitions of submanifolds of Euclidean spaces","I am trying proving the four definitions for submanifolds of Euclidean space providing in https://www.mathematik.uni-muenchen.de/~tvogel/Vorlesungen/TMP/skript-TMP.pdf are equivalent (I came to this reference for reading the book Vector Calculus, Linear Algebra, and Differential Forms of J.H. Hubbard and B.B. Hubbard.): For equivalent definitions of the notion of submanifold of dimension $k\in \mathbb{N}^+ = \{1,2,...\}$ . In all four of them, $M \in \mathbb{R}^n$ . And in all all four of them, smooth means belonging to $C^l$ for some positive integer $l$ , or infinitely differentiable. Condition (a) Local parametrizations: For all $p \in M$ there is an open set $U \in \mathbb{R}^k$ , a neighborhodd $V \in \mathbb{R}^n$ of $p$ and a smooth map $\varphi: U \rightarrow \mathbb{R}^n$ such that $\varphi$ is a homeomorphism onto $V \cap M$ , and for all $x \in U$ the differential $D_x\varphi: \mathbb{R}^k \rightarrow \mathbb{R}^n$ is injective. Condition (b) Locally flat: For all $p \in M$ there are an open neighbourhood $V \subset \mathbb{R}^n$ of $p$ and $W \subset \mathbb{R}^n$ and a diffeomorphism $\phi: V \rightarrow W$ such that $\phi(p) = 0$ and $\phi(V\cap M) = (\mathbb{R}^k \times \{0\in \mathbb{R}^{n-k}\}) \cap W$ Condition (c) Locally regular level set: For all $p \in M$ there is an open neighborhood $V$ and a smooth function $F: V \rightarrow \mathbb{R}^{n-k}$ such that $F^{-1}(0) = V\cap M$ , and for all $q \in M \cap V$ the differential $D_q F : \mathbb{R}^n \rightarrow \mathbb{R}^{n-k}$ is surjective Condition (d) Locally a graph: For all $p \in M$ there is an open neighbourhood $V \in \mathbb{R}^{n}$ and an open subset $U \in \mathbb{R}^k$ and a smooth function $g: U \rightarrow \mathbb{R}^{n-k}$ and a permutation $\sigma \in S_n$ such that $V\cap M = \{(y_{\sigma(1)}, y_{\sigma(2)}, \dots, y_{\sigma(n)})| y = (x, g(x)) \text{ where } x \in U\}$ I found that proving their equivalence in details seems quite tedious. I tried to restate the proof of $a \Rightarrow b$ providing on the book Calculus on manifolds by Michael Spivak as follows, because I think the proof in it is missing some details. Could someone please check if the proof is missing any details or if there is some more elegant proof? Thanks! Proof (a) => (b) in details Denote $x_0 = \varphi^{-1}(p)$ .
By item 2 of Condition (a), $D_p \varphi : \mathbb{R}^k \rightarrow \mathbb{R}^n$ is injective, hence its Jacobian matrix: $$
D_x\varphi(x_0)=
\left(\begin{array}{cccc}
\frac{\partial \varphi_{1}}{\partial x_{1}} & \frac{\partial \varphi_{1}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{1}}{\partial x_{k}} \\
\frac{\partial \varphi_{2}}{\partial x_{1}} & \frac{\partial \varphi_{2}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{2}}{\partial x_{k}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial \varphi_{n}}{\partial x_{1}} & \frac{\partial \varphi_{n}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{n}}{\partial x_{k}}
\end{array}\right) \bigg\rvert_{(x_1, x_2, \cdots, x_k) = x_0}
$$ has rank $k$ . Hence there are positive integers $1 \leq r_1 \leq r_2 \leq \dots \leq r_k \leq n$ such that $$
\left(\begin{array}{cccc}
\frac{\partial \varphi_{r_1}}{\partial x_{r_1}} & \frac{\partial \varphi_{r_1}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_1}}{\partial x_{k}} \\
\frac{\partial \varphi_{r_2}}{\partial x_{1}} & \frac{\partial \varphi_{r_2}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_2}}{\partial x_{k}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial \varphi_{r_k}}{\partial x_{1}} & \frac{\partial \varphi_{r_k}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_k}}{\partial x_{k}}
\end{array}\right) \bigg\rvert_{(x_1, x_2, \cdots, x_k) = x_0}
$$ has nonzero determinant. Let $\sigma$ be any permutation of $\{1,2,\cdots, n\}$ such that $\sigma(1) = r_1, \sigma(2) = r2, ...,\sigma(k) = r_k$ . And define $P: \mathbb{R}^n \rightarrow \mathbb{R}^n$ by $P(y_1, y_2, \cdots, y_n) = (y_{\sigma(1)}, y_{\sigma(2)}, \cdots, y_{\sigma(n)}) = (y_{r_1}, y_{r_2}, \cdots, y_{r_k}, \cdots)$ . Then obviously(In fact I think the proof of it is quite tedious. Though it do be just some permutation of the coordinates.) $P$ is a diffeomorphism of $\mathbb{R}^n$ . And $$
 D_x(P \circ \varphi) = \left(\begin{array}{cccc}
\frac{\partial \varphi_{r_1}}{\partial x_{1}} & \frac{\partial \varphi_{r_1}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_1}}{\partial x_{k}} \\
\frac{\partial \varphi_{r_2}}{\partial x_{1}} & \frac{\partial \varphi_{r_2}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_2}}{\partial x_{k}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial \varphi_{r_k}}{\partial x_{1}} & \frac{\partial \varphi_{r_k}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_k}}{\partial x_{k}} \\
\vdots & \vdots & \vdots & \vdots
\end{array}\right) .
 $$ The first $k$ row of it's value at $x_0=\varphi^{-1}(p)$ has nonzero determinant by the previous assumption.
Since $\varphi$ is smooth, so is $P \circ \varphi$ . So there is a neighborhood $U_1 \subset U$ of $x_0$ such that $D_x(P \circ \varphi)$ has nonzero determinant is it. Define $\Phi: U_1 \times \mathbb{R}^{n-k} \rightarrow \mathbb{R}^n$ by $$
\Phi(x,y) = P(\varphi(x)) + (0, y)
$$ where $x \in U, y \in \mathbb{R}^k$ and $(0, y)$ means that the first $k$ coordinates are zero and the last $n-k$ coordinates is $y$ . Then $$
D\Phi = 
 \left(\begin{array}{cccccccc}
\frac{\partial \varphi_{r_1}}{\partial x_{1}} & \frac{\partial \varphi_{r_1}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_1}}{\partial x_{k}} & 0 & 0 & \cdots & 0 \\
\frac{\partial \varphi_{r_2}}{\partial x_{1}} & \frac{\partial \varphi_{r_2}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_2}}{\partial x_{k}} & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
\frac{\partial \varphi_{r_k}}{\partial x_{1}} & \frac{\partial \varphi_{r_k}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_k}}{\partial x_{k}} &0 & 0 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots & 1 & 0 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots  & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
\vdots & \vdots & \vdots & \vdots  & 0 & 0 & \cdots & 1 
\end{array}\right)
$$ has nonzero determinant on $U_1 \times \mathbb{R}^{n-k}$ . Define $\Phi_1: U_1 \times \mathbb{R}^{n-k} \rightarrow \mathbb{R}^n$ by $\Phi_1 = P^{-1} \circ \Phi$ , then its Jacobian matrix has nonzero  determinant on $U_1 \times \mathbb{R}^{n-k}$ , and $\Phi_1(x_0, 0) = P^{-1}(P(\varphi(x_0))+(0,0)) = \varphi(x_0) = p$ .( $P^{-1}$ is linear and $\Phi_1(x,y) = P^{-1}(P(\varphi(x))+(0,y)) = \varphi(x)+P^{-1}(0,y)$ . Define $\Phi_1$ directly seems very hard to write the statement.)
By the inverse function theorem, there is neighborhood $W_1 \subset U_1 \times \mathbb{R}^{n-k}$ of $(x_0, 0)$ and neighborhood $V_1$ of $p$ and the restriction $\Phi_1: W_1 \rightarrow V_1$ is a diffeomorphism (Normally the textbook state the inverse function theorem without saying the inverse function is $C^{k}$ if the original function is $C^{k}$ . But it do be seems the Jacobian matrix of the inverse function is a raitional function of the Jacobian matrix of the original function. Ref: Inverse function theorem: how show $F \in C^k \Rightarrow F^{-1} \in C^k$ with this method? maybe.). Since $\varphi: U \rightarrow V\cap M$ is a homeomorphism, and $W_1 \cap (\mathbb{R}^k \times \{0\in \mathbb{R}^{n-k}\})$ is a open neighborhood of $x_0$ (In fact there is still a homeomorphism between them, but stating it clearly is very tedious.) and subset of $U_1 \subset U$ . Hence $\Phi_1(W_1 \cap (\mathbb{R}^k \times \{0\in \mathbb{R}^{n-k}\})) = V_2 \cap M$ for some $V_2$ (Is there some counterexample to show that $V_2$ cannot be $V_1$ ?). Let $V_3 = V_1 \cap V_2$ , and $W_3 = \Phi_1^{-1}(V_3)$ , and $\phi_3: V_3 \rightarrow W_3$ be the restriction of $\Phi_1^{-1}$ on $V_3$ . Then $\phi_3$ satisfies: $\phi_3: V_3 \rightarrow W_3$ is diffeomorphism. $\phi_3(p) = (x_0, 0)$ $\phi_3(V_3 \cap M) = W_3 \cap (\mathbb{R}^k \times \{0\in \mathbb{R}^{n-k}\})$ Take $\phi_4(p) = \phi_3(p) - (x_0, 0)$ . Then $V_3$ , $\phi_4(V_3)$ , $\phi_4$ is the $V, W, \phi$ in condition (b). ⬛ Though the idea seems quite simple. The proof is so tedious, and the part for proving that $\phi_3(V_3 \cap M) = W_3 \cap (\mathbb{R}^k \times \{0\in \mathbb{R}^{n-k}\})$ seems not right or well stated. Could someone help to check the proof or provide some more rigorous proof please? And any hint for proving the other directions is greatly appreciated too! (Though the keypoint as I know is the inverse/implicit theorem, but using it to state a rigorous proof of the equivalent of the above four definition seems still quite hard for me.) Thanks!","['manifolds', 'multivariable-calculus', 'smooth-manifolds']"
4260082,How does this infinite sum series switch between ln(2) and -(1-ln(2))?,"I found this simple infinite series of summed terms:
0/1 + 1/2 - 2/3 + 3/4 - 4/5 + 5/6 - 6/7 ....etc When I tried calculating a bunch of terms, weirdly enough, it seems that an even number of terms produces ln(2) and an odd number of terms produces -(1-ln(2)).
(You can test that out in this codepen I made) My question is: How does this sequence produce the two numbers? Is there a certain property in it that results in this? Can a known sequence be rearranged and simplified to produce this sequence, or maybe this sequence re-written in a different form? Thanks in advance! Edit #1: I found that taking the known series for ln(2), substracting it from 1 and inversing addition and substraction results in this same series, which should calculate -(1-ln(2)) by definition . My question remains: How does it produce ln(2) and how does it alternate between the two?",['sequences-and-series']
4260103,A reference for $\displaystyle \sum_{n\leq X}\mu^2(n)= \frac{X}{\zeta(2)}+O(\sqrt{X}\exp(-c\sqrt{\log X}))$,"There is this result of counting the number of square-free numbers until $X$ , which goes like this: $$\sum_{n\leq X}\mu^2(n)= \frac{X}{\zeta(2)}+O(\sqrt{X}\exp(-c\sqrt{\log X})).$$ Could someone please point me to a reference which contains this result? I'd like to cite it and would prefer to refer the reader to a book or a paper. Thanks!","['analytic-number-theory', 'number-theory', 'reference-request']"
4260143,A CDF related inequality,"Suppose $F$ and $G$ are two cdf over $[0, 1]$ . Let $$z \equiv \frac{\int_{0}^1 x (1 - G(x)) \mathrm{d}F(x)}{\int_{0}^1 (1 - G(x)) \mathrm{d}F(x) }.$$ Does the following inequality hold: $$ \int_z^1 (y - z) \mathrm{d}G(y) \geq \int_0^1 \int_x^1 (y - x) \mathrm{d} G(y) \mathrm{d} F(x) \,\, ?$$ If we define function $r(x) \equiv \int_x^1 (y - x) \mathrm{d} G(y)$ , the desired inequality can be stated as $r(z) \geq \int_0^1 r(x) \mathrm{d} F(x)$ . Although I know that $r$ is decreasing and convex, but yet have no idea how to proceed. Thank you very much!","['inequality', 'analysis', 'probability']"
4260207,Are there extensions of Euler's infinite product for sine function?,"Euler product about sine function is $\frac{\sin(x)}{x} = \prod_{n=1}^\infty \left ( 1- \left(\frac{x}{n\pi}\right)^2 \right)$ I wonder if there is known results about slight modification of above product. Does there exists analytic expression about following infinite product? $\prod_{n=1}^\infty \left( 1- \left( \frac{x}{n\pi + a}\right)^2\right)$ I can't find out what it is, even though it is a slight modification.","['complex-analysis', 'euler-product', 'special-functions', 'analysis']"
4260334,Some intuition behind why we visit $\frac{2}{3}$ of Natural Numbers?,"Consider stating at $0$ on the number line of integers. Toss a fair coin and if heads advance $1$ space and if tails advance $2$ spaces. What number has the highest probability of being visited? What is the probability of visiting very large numbers roughly? First part The following recursive equation can be formed quite nicely conditioning on the first toss: $p_n = \frac{1}{2} p_{n-1} + \frac{1}{2}p_{n-2}$ (for $n \geq 3)$ In other words $p_n$ is the average of $p_{n-1}$ and $p_{n-2}$ . The average of $n$ values is less than or equal to the maximum of the values. Hence, for example, $10$ cannot be the most probably visited square because you visit either $8$ or $9$ with greater probability. Likewise $9$ cannot be for the same logic continuing until we see $3$ cannot be. Hence the only candidates are $1$ or $2$ . The probability of visiting $1$ is $\frac{1}{2}$ and the probability of visiting $2$ is $\frac{1}{2} \cdot \frac{1}{2} + \frac{1}{2} = \frac{3}{4} $ Hence $2$ is the most probably visited square. Second part Solving the recurrence relation formed in the first part (using the intials $p_1$ and $p_2$ )  yields: $p_n = \frac{2}{3} + \frac{1}{3} \cdot (-\frac{1}{2})^n$ This explains the periodicity noticed prior and gives us the long-run average probability  of $\frac{2}{3}$ This number seems way to high for my intuition. I expected it to be $\frac{1}{2} $ can someone provide some intuition behind $\frac{2}{3}$ ?","['statistics', 'probability']"
4260380,Prove that $2-\cfrac{\pi^2}{6-\cfrac{\pi^2}{10-\cfrac{\pi^2}{14-\cfrac{\pi^2}{...}}}} = 0$,Prove that $$2 - \cfrac{\pi^2}{6-\cfrac{\pi^2}{10 - \cfrac{\pi^2}{14-\cfrac{\pi^2}{...}}}} = 0$$ My thoughts: One common approahch is setting $x = \text{LHS}$ and we express LHS as $x = \frac{\pi^2}{f(x)}$ . But it's hard to translate the denominator as a function of $x$ $\pi^2$ also reminds everybody of the basel problem $\sum_{n=1}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6}$ but what can we do about it?,"['sequences-and-series', 'real-analysis']"
4260419,Evaluate $\int_0^\infty \frac{e^{-x}\sin(x)}{\sqrt[3]{x} } dx$,I am having trouble with the following integral Evaluate $$\int_0^\infty \frac{e^{-x}\sin(x)}{\sqrt[3]{x} } dx$$ $$\int_0^\infty \frac{e^{-x}\sin(x)}{\sqrt[3]{x} } dx=\int_0^\infty 3ue^{-u^3}\sin(u^3)du$$ let $u=\sqrt[3]{x}\rightarrow dx=3x^{2/3}du$ How does one proceed from here? Thank you for your time.,"['integration', 'improper-integrals', 'real-analysis', 'complex-analysis', 'calculus']"
4260513,A paradox in probability,"I am trying to calculate the probability of picking perfect squares out of first $n$ positive integers. There are $\operatorname{floor}(\sqrt n)$ number of perfect squares less than $n$ , if we assume picking each number is equally likely then probability of picking perfect squares less than $n$ is $p(n) =\operatorname {floor}(\sqrt n) /n$ But if we consider all positive integers then ( I think but I am not sure) the probability is $\lim_{n\to \infty} p(n) = 0$ . That means the probability of selecting a square is $0$ even though there are infinitely many squares! Does this mean I that I will not get any squares when I pick any positive integers? This appears like a paradox. I think I have not very much understood what it means by $P(A) = 0$ , does this mean the event $A$ is impossible or something else? Can you give some insights about this?","['paradoxes', 'probability']"
4260514,Is $( (\lnot p \lor q ) \lor ( p \lor r ) )$ equivalent to $( \lnot p \lor q \lor p \lor r )$?,Determine whether $( p \land q ) \to ( p \lor ( q \land r ) )$ is tautology or not. In line 5 of my given picture can I write this $( (\lnot p \lor q ) \lor ( p \lor r ) )$ as $( \lnot p \lor q \lor p \lor r )$ If yes then what is this rule called? My solution:,['discrete-mathematics']
4260536,About the asymptotic behavior of specific Jacobi $\theta$ function $\operatorname{\vartheta}_3\left(0;x\right)$ when $x\to{1-}$.,"Since $\displaystyle\sum_{n=1}^\infty{x^{n^2}}=\dfrac{\operatorname{\vartheta}_3\left(0,x\right)-1}2$ for $x\in\left(0,1\right)$ (just in case), it suffices to consider the former below. (Another relevant identity is that $\displaystyle\sum_{n=1}^\infty{x^{n^2}}=\left(1-x\right)\mspace{-2mu}\sum_{n=1}^\infty{\!\left\lfloor\mspace{-1mu}\sqrt{n}\right\rfloor\!x^n}$ .) Many a post (e.g., What is $\lim_{x\to1^{-}} (1-x)\left(\sum_{i=0}^{\infty} x^{i^2}\right)^{2}$? , Why is $\lim_{x\to 1}\sqrt{1-x}\sum_{n=0}^\infty x^{n^2}=\sqrt{\pi}/2\,\,$? , Compute the limit of $\sqrt{1-a}\sum\limits_{n=0}^{+\infty} a^{n^2}$ when $a\to1^-$ , $\lim_{x\to 1^-}\sqrt{1-x}\ \left(1+x+x^4+x^9+x^{16}+x^{25}+\cdots\right)=\sqrt{\pi}/2$ is true? , and How to prove that $\lim_{x\to 1^-} \left(\left(\sum_{n=1}^{\infty}x^n \right)\cdot \log\left(\frac{1}{x}\right) \right)= 1$ WITHOUT computing the sum ) has shown that $$\lim_{\mspace{-1mu}x\to1^-}{\sqrt{1-x\mspace{2mu}}\sum_{n=1}^\infty{\mspace{-1.5mu}x^{n^2}}}=\frac{\mspace{-1.5mu}\sqrt\pi\mspace{1mu}}2\text{.}$$ Nevertheless, none of them evaluated the limit $$\lim_{\mspace{-1mu}x\to1^-}{\mspace{-3mu}\left({\sqrt{\mspace{-0.5mu}\frac\pi{1-x}}-2\mspace{-0.75mu}\sum_{n=1}^\infty{\mspace{-1.75mu}x^{n^2}}}\right)\mspace{-1mu}}\text{,}$$ which is equal to $1$ . how to find the asymptotic expansion of the following sum: sketched out a possible approach to finding this kind of limit. Some other related posts are as follows: Asymptotics of a recurrence relation , Asymptotic behavior of $\sum\limits_{n=0}^{\infty}x^{b^n}$ when $x\to1^-$ , Evaluate $\lim_{x\to1^-}\left(\sum_{n=0}^{\infty}\left(x^{(2^n)}\right)-\log_2\frac{1}{1-x}\right)$ and What's the limit of the series $\log_2(1-x)+x+x^2+x^4+x^8+\cdots$. . In Analysis of Series and Products. Part 1: The Euler–Maclaurin Formula and Analysis of Series and Products. Part 2: The Trapezoidal Rule , the authors show that $${{\sum_{j=1}^\infty\mathrm{e}^{-j^2/x^2}-\frac{x\sqrt\pi}2+\frac12}\to0}\quad\text{as}\quad{x\to{+\infty}}\text{,}$$ and the magnitude of the left-hand side decreases exponentially as $x$ increases. (Then set $u=\exp\left(-x^{-2}\right)$ . How about the Abel–Plana formula?) In addition, in On the asymptotics of some partial theta functions and Some new asymptotic expansions of certain partial theta functions , each author showes that $$\dfrac{\operatorname{\vartheta}_3\left(0,x\right)+1}2=\sum_{n\in\mathbb{N}}{x^{n^2}}=\frac12\sqrt{\frac{-\pi}{\ln{x}}}+\frac12+\operatorname{\mathcal{O}}\left(\ln^q{\!x}\right)\text{,}\quad\forall{q\in\mathbb{N_+}}\text{,}$$ as $x\to1^-$ (after making some substitutions). It is generally known that $\ln{x}\sim{x-1}$ as $x\to1$ ; hence, as $x\to1^-$ , $\displaystyle\sum_{n=1}^\infty{x^{n^2}}\sim\frac12\sqrt{\frac{\pi}{1-x}}$ just means that $\displaystyle\sum_{n=1}^\infty{x^{n^2}}\sim\frac12\sqrt{\frac{-\pi}{\ln{x}}}$ . Yet, if one uses the former in the asymptotic expansion, what about the remainder term? Via a number of numerical experiments, it seems that $$\frac{\operatorname{\vartheta}_3\left(0,x\right)}{\sqrt{\pi\!\left(1-x\right)}}=\frac1{\sqrt{\pi\!\left(1-x\right)}}\sum_{n\in\mathbb{Z}}{x^{n^2}}=\frac1{1-x}-\frac14+\operatorname{\mathcal{o}}\left(1\right)\quad\text{as}\quad{x\to1^-}\text{.}$$ Unfortunately, I have no idea how to prove it. Any idea? Many thanks!","['limits', 'theta-functions', 'asymptotics', 'algorithms']"
4260577,"upper bound on $L_2$-norm of a power series, in terms of coefficients?","Is there any upper bound on $L_2$ -norm of a convergent power series (in R), in terms of coefficients?
I have $f(x) = a_0+a_1\frac{x}{1!}+a_2\frac{x^2}{2!}+a_3\frac{x^3}{3!}+...$ . I need something like: $$
\|f\|^2_{L_2({\mathbb R})}\leq r(a_0, a_1, a_2,\cdots)
$$ where $r(a_0, a_1, a_2,\cdots) = \sum_{ij}c_{ij}a^\ast_i a_j$ is some infinite positive definite quadratic form.","['complex-analysis', 'entire-functions', 'power-series', 'taylor-expansion']"
4260580,"Why is $ \arccos\left(\frac{1-x^2}{1+x^2}\right)= -2\arctan(x)$ true $\forall x\in (-\infty, 0]$?","I'm trying to solve the problem that follows, and I'd appreciate any feedback on my solution in order to improve it. Thank you. $$ \arccos\left(\frac{1-x^2}{1+x^2}\right) = -2\arctan(x) $$ So, I start off with analysing the domain. It's clear that arctan is defined for all values of x that are a member of the real numbers. We can also see that the argument of arccos is less than 1 and greater than -1 for all x's. So that means, our equation is defined for all x's that belongs to the real numbers. $$\Rightarrow \tan(\arccos(\frac{1-x^2}{1+x^2}) = \tan(-2\arctan(x))$$ Using the fact that $\tan(x) = \sqrt{1-\cos^2(x)}/\cos(x)$ as well as $\tan(2x) = \frac{2\tan(x)}{1-\tan^2(x)}$ . I land at this equation: $$\frac{|2x|}{1-x^2} = \frac{-2x}{1-x^2}$$ Which is true for all negative x's, so basically $\forall x\in (-\infty, 0]$ I'm now asking you whether I can deduce this is true. Because, I actually applied tan to both sides, so does this imply that the solution set also holds for our original equation, and is there a way to deduce this, maybe using estimations? Thank you.","['trigonometry', 'real-analysis']"
4260586,Expressing $\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+\frac{p}{q}}$ as a finite sum depending on the parity of $q$,"Suppose $p$ and $q$ are positive integers, $p<q$ . How to prove this identity? $$
\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+\frac{p}{q}}=\frac{\pi}{2}\csc\left(\frac{p\pi}{q}\right)+(-1)^{(p-1)}\sum_{n=1}^{(q-1)/2}2\cos\left(\frac{2\pi np}{q}\right)\ln\left(\cos\frac{n\pi}{q}\right)\quad
\forall\;q \text{ is odd}\\
\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+\frac{p}{q}}=\frac{\pi}{2}\csc\left(\frac{p\pi}{q}\right)+(-1)^{(p-1)}\sum_{n=1}^{q/2}2\cos\left(\frac{(2n-1)\pi p}{q}\right)\ln\left(\cos\frac{(2n-1)\pi}{2q}\right)\quad
\forall\;q \text{ is even}$$ I have proved the first part for $q$ is odd by referring to the proof of Gauss's digamma theorm. Using Abel’s limit theorem $$
\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+\frac{p}{q}}=\lim_{t\rightarrow 1^-}\sum_{n=0}^{\infty}\frac{(-1)^n q}{p+qn}t^{p+qn}=(-1)^{-p}q\lim_{t\rightarrow 1^-}\sum_{n=0}^{\infty}\frac{(-t)^{p+qn}}{p+qn}
$$ Using algorithm for extracting every $n^{th}$ term of a series, let $\omega=e^{2\pi i/q}$ $$
\sum_{n=0}^{\infty}\frac{(-t)^{p+qn}}{p+qn}=\frac{1}{q}\sum_{n=0}^{q-1}\omega^{-np}(-\ln(1+\omega^n t)) \\
\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+\frac{p}{q}}=(-1)^{-p}\lim_{t\rightarrow 1^-}\sum_{n=0}^{q-1}\omega^{-np}(-\ln(1+\omega^n t))=(-1)^{1-p}\sum_{n=0}^{q-1}\omega^{-np}\ln(1+\omega^n)
$$ Replace $p$ by $q-p\quad$ ( $q$ is odd) $$
\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+\frac{q-p}{q}}=(-1)^{1-q+p}\sum_{n=0}^{q-1}\omega^{-n(q-p)}\ln(1+\omega^n)=(-1)^p\sum_{n=0}^{q-1}\omega^{np}\ln(1+\omega^n)
$$ Subtract the two expressions to obtain $$
\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+\frac{p}{q}}-\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+1-\frac{p}{q}}=(-1)^{p-1}\sum_{n=0}^{q-1}2\cos\left(\frac{2\pi np}{q}\right)\ln(1+\omega^n)
$$ Left side is real, so it is equal to the real part of right side. $$
\mathfrak{R}(\ln(1+\omega^n))=\mathfrak{R}\left(\ln\left(2\cos\frac{n\pi}{q}e^{i\frac{n\pi}{q}}\right)\right)=\ln\left | 2\cos\frac{n\pi}{q}\right |  \\
\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+\frac{p}{q}}-\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+1-\frac{p}{q}}=(-1)^{p-1}\sum_{n=1}^{q-1}2\cos\left(\frac{2\pi np}{q}\right)\ln\left | \cos\frac{n\pi}{q}\right | 
$$ Since $$
\sum_{n=0}^{q-1}\cos\left(\frac{2\pi np}{q}\right)\ln(2)=0\quad \text{and}\quad \ln(\cos(0))=0
$$ Using the well-known identity $$
\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+\frac{p}{q}}+\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+1-\frac{p}{q}}=\pi\csc\left(\frac{p\pi}{q}\right)\
$$ Then $$
\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+\frac{p}{q}}=\frac{\pi}{2}\csc\left(\frac{p\pi}{q}\right)+(-1)^{(p-1)}\sum_{n=1}^{q-1}\cos\left(\frac{2\pi np}{q}\right)\ln\left | \cos\frac{n\pi}{q}\right | \\
=\frac{\pi}{2}\csc\left(\frac{p\pi}{q}\right)+(-1)^{(p-1)}\sum_{n=1}^{(q-1)/2}2\cos\left(\frac{2\pi np}{q}\right)\ln\left(\cos\frac{n\pi}{q} \right)\quad\forall\;q \text{ is odd}
$$ But I cannot apply the same proof process for $q$ is even. Can someone help me to complete the proof? Thanks to Mr. metamorphy's answer. The proof is clever but it doesn't explain why my proof process for $q$ is odd can't be applied for $q$ is even. After a few days of trying, I manage to work it out for $q$ is even. The problem is when applying Abel’s limit theorem, different parity of $q$ comes out different result. Using Abel’s limit theorem $$
\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+\frac{p}{q}}=\lim_{t\rightarrow 1^-}\sum_{n=0}^{\infty}\frac{(-1)^n q}{p+qn}t^{p+qn}=(-1)^{-p}q\lim_{t\rightarrow 1^-}\sum_{n=0}^{\infty}\frac{(-1)^n(-t)^{p+qn}}{p+qn} \\
\sum_{n=0}^{\infty}\frac{(-1)^n(-t)^{p+qn}}{p+qn}=\sum_{n=0}^{\infty}\frac{(-t)^{p+2nq}}{p+2nq}-\sum_{n=0}^{\infty}\frac{(-t)^{p+(2n+1)q}}{p+(2n+1)q}
$$ Using algorithm for extracting every $n^{th}$ term of a series, let $\omega=e^{2\pi i/2q}=e^{\pi i/q}$ $$
\sum_{n=0}^{\infty}\frac{q(-t)^{p+2nq}}{p+2nq}=\frac{q}{2q}\sum_{n=0}^{2q-1}\omega^{-np}(-\ln(1+\omega^n t))=-\frac{1}{2}\sum_{n=0}^{2q-1}\omega^{-np}\ln(1+\omega^n t) \\
\sum_{n=0}^{\infty}\frac{q(-t)^{p+(2n+1)q}}{p+(2n+1)q}=\sum_{n=0}^{\infty}\frac{q(-t)^{(p+q)+2nq}}{(p+q)+2nq}=-\frac{1}{2}\sum_{n=0}^{2q-1}\omega^{-n(p+q)}\ln(1+\omega^n t) \\
\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+\frac{p}{q}}=\lim_{t\rightarrow 1^-}(-1)^{-p}\left ( \sum_{n=0}^{\infty}\frac{q(-t)^{p+2nq}}{p+2nq}-\sum_{n=0}^{\infty}\frac{q(-t)^{p+(2n+1)q}}{p+(2n+1)q} \right )\\
=\lim_{t\rightarrow 1^-}\frac{(-1)^{p-1}}{2}\left(\sum_{n=0}^{2q-1}\omega^{-np}\ln(1+\omega^nt)-\sum_{n=0}^{2q-1}\omega^{-n(p+q)}\ln(1+\omega^nt)\right)
$$ For $n$ is odd, $\omega^{-n(q+p)}=-\omega^{-np}$ $$\omega^{-np}\ln(1+\omega^nt)-\omega^{-n(p+q)}\ln(1+\omega^nt)=2\omega^{-np}\ln(1+\omega^nt)$$ For $n$ is even, $\omega^{-n(q+p)}=\omega^{-np}$ $$\omega^{-np}\ln(1+\omega^nt)-\omega^{-n(p+q)}\ln(1+\omega^nt)=0$$ \begin{align}
&\text{ }\quad\sum_{n=0}^{2q-1}\left ( \omega^{-np}\ln(1+\omega^nt)-\omega^{-n(p+q)}\ln(1+\omega^nt) \right )\\
&=\sum_{n=1,3,5\cdots ,2q-1}^{}2\omega^{-np}\ln(1+\omega^nt)=
\sum_{n=1}^{q}2\omega^{-(2n-1)p}\ln(1+\omega^{(2n-1)}t)\\ \\
&\text{ }\quad\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+\frac{p}{q}}=\lim_{t\rightarrow 1^-}\frac{(-1)^{p-1}}{2}\sum_{n=1}^{q}2\omega^{-(2n-1)p}\ln(1+\omega^{(2n-1)}t)\\
&=(-1)^{p-1}\sum_{n=1}^{q}\omega^{-(2n-1)p}\ln(1+\omega^{(2n-1)})
\end{align} The rest of the proof is the same as the process for $q$ is odd, and goes to the desired result: $$
\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+\frac{p}{q}}=\frac{\pi}{2}\csc\left(\frac{p\pi}{q}\right)+(-1)^{(p-1)}\sum_{n=1}^{q/2}2\cos\left(\frac{(2n-1)\pi p}{q}\right)\ln\left(\cos\frac{(2n-1)\pi}{2q}\right)\quad
\forall\;q \text{ is even}$$",['sequences-and-series']
4260591,Solutions to an ODE using Frobenius when roots are repeated,"I have been looking at the following differential equation $$xy^{\prime \prime} + y^{\prime} + xy=0$$ Observe that since $x=0$ is a regular singular point we use the Frobenius method and substitute $y=\sum_{n=0}^{\infty}a_n x^{n+r}$ to get $$r^2a_0x^{r-1} + (r+1)^2a_1x^r + \sum_{n=2}^{\infty}[(n+r)^2a_n+a_{n-2}]x^{n+r-1}=0$$ Since $a_0 \neq 0$ we get $r=0,0.$ giving $y_1 = a_0(1-\dfrac{x^2}{2^2}+\dfrac {x^4}{4^2\cdot2^2}-\dfrac{x^6}{6^2\cdot4^2\cdot2^2}......)$ . Now apparently to obtain the second solution I have to differentiate this solution w.r.t. $r$ and then evaluate this at $r=0$ , but I really don't see where this comes from. Could anyone show me explicitly why exactly we do this?  Or more generally - If the roots of the indicial equation $r_1$ and $r_2$ are repeated,  why is the second solution of the form $$y_2= c\left(\frac {\partial y_1}{\partial r}\right)_{\large {r=r_1}}?$$ All the sources I've looked at just state this fact without really showing where this came from, so either I'm missing something really obvious or there is some other explanation.","['power-series', 'frobenius-method', 'ordinary-differential-equations']"
4260598,Prove the average of iid random variable from this distribution is not convergent to zero in probability,"I'm trying to prove that the probability distribution $P(X=k) \sim \dfrac{1}{k^2}, k \in \mathbb{Z} \setminus \{0\}$ does not hold the weak law of large numbers. it means if $\overline{X}$ is the average of $n$ iid samples from this distribution, then for any $\epsilon>0$ we have a $\delta >0$ in which $P(|\overline{X}|>\epsilon) > \delta$ for any $n \in N$ . I don't even know if this fact is right or not, so I'd also be happy to see why is this fact wrong. I was just thinking that maybe it's true because this distribution is similar to $\text{Cauchy}(0,1)\sim \dfrac{1}{1+x^2}$ and $\text{Cauchy}(0,1)$ is stable on average which means it's average is not convergent to zero in probability (the average of $n$ iid samples from $\text{Cauchy}(0,1)$ also has the $\text{Cauchy}(0,1)$ distribution).","['convergence-divergence', 'probability-distributions', 'probability-theory', 'law-of-large-numbers']"
4260638,Inverse image of a subset of the codomain with elements without corresponding elements in domain,"I consider the function $f(x) = x^2 : \mathbb{R} \rightarrow \mathbb{R}$ whose image is $[0, + \infty)$ . For the sake of simplicity: domain $D = \mathbb{R}$ , codomain $C = \mathbb{R}$ . If I consider $A = [-25, 25]$ subset of the codomain $C$ , this subset contains elements $[-25, 0)$ which don't have a corresponding element in the domain $D$ . In this case is it possible to evaluate the inverse image of $A$ ? I tried to do it in this way. According to the definition of the inverse image: $$
f^{-1}(A) = \lbrace x \in D : f(x) \in A \rbrace
$$ $$
f^{-1}(A) = f^{-1}([0, 25]) = \lbrace x \in [-5, 5] \rbrace
$$ Is it correct? EDIT: the Mathematica software gave me the same result I wrote.",['functions']
4260647,Convergence in probability implies convergence in quantile of inverse quantile.,"I'm having some problems proving the following result. Let $F_{n}, n=0,1,2, \ldots$ , be c.d.f.'s such that $F_{n}
 \rightarrow{ }_{w} F_{0} .$ Let $G_{n}(U)=$ $\sup \left\{x: F_{n}(x)
 \leq U\right\}, n=0,1,2, \ldots$ , where $U$ is a random variable
having the uniform $U(0,1)$ distribution. Show that $G_{n}(U)
 \rightarrow{ }_{p} G_{0}(U)$ . Here I think G represents a sort of inverse quantile functions. If we assume that the r.v. are continuous, then G is just the random variables themselves. However, I have no idea how to prove this general case when it's not assumed that the inverse quantile exists. Any help is appreciated. Thanks.","['measure-theory', 'probability-distributions', 'convergence-divergence', 'probability']"
4260666,"If an event $A$ is independent of the event $B, B\cap C \text{ and } B \cup C.$ Then find $P(A \cap C)$","If an event $A$ is independent of the event $B, B\cap C \text{ and } B
 \cup C.$ Then find $P(A \cap C)$ . My attempt List item event $A$ is independent of the event $B.$ $\therefore P(A\cap B)=P(A)\cdot P(B)$ List item event $A$ is independent of the event $B\cap C.$ $\therefore P(A\cap (B \cap C))=P(A)\cdot P(B \cap C)$ List item event $A$ is independent of the event $B\cup C.$ $\therefore P(A\cap (B \cup C))=P(A)\cdot P(B \cup C)$ But I am not able to conclude the expression for $P(A \cap C).$","['statistics', 'independence', 'probability']"
4260687,periodic function of x with period 2 and f(x) =|x|−x for −1<x≤1.,"Let $f(x)$ be a periodic function in $x$ with period 2, and $f(x) =|x|−x$ for $−1< x≤1$ .  Sketch the graph of the curve $y=f(x)$ in the interval $[−3,3]$ . $f(x) =|x|−x$ seems not to be a periodic function, though, so how can I solve the question? Thank you!","['functions', 'absolute-value', 'graphing-functions']"
4260711,Statistics - What is the random error of a measurement with one reading?,"To calculate the random error in a set of measurements this is what I would do. Get the standard deviation of the measurements: $$ \sigma=\sqrt{\frac{1}{N-1}\Sigma_{i=1}^N(x_i-\bar{x})^2} $$ Where $N$ is the number of repeats, $x_i$ is the value of each sample, $\bar{x}$ is the mean, and $\sigma$ is the standard deviation. The standard error of the mean: $$ s_\bar{x}=\frac{\sigma}{\sqrt{N}} $$ Obtain the random error using a t distribution: $$ \varepsilon_{rnd}=±t_{N-1}*s_\bar{x} $$ The problem is, if I have a measurement with a single reading of a given value, this method for random error calculation is clearly no longer applicable. Therefore, how would you go about calculating the random error in a for a sample set where N=1? Is that even possible? Thanks for your time!","['standard-error', 'statistics', 'standard-deviation']"
4260714,Convergence of a rearrangement of a series,"Let $\sum a_n$ be a convergent series and $f$ is a bijection on $\mathbb{N}$ . Suppose $(f(n) -n)$ is a bounded sequence the rearrangement series $\sum a_{f(n)}$ converges to same limit. Suppose $m_n=\sup\{|a_k| : k> n\}$ . If the sequence $(m_n |f(n) -n|) $ converges to $0$ , then the rearrangement series converges to same limit. Since $(f(n) -n)$ is bounded sequence, the terms can be grouped in some way such that the limit does not change. But how it can be done. $2$ seems consequence of $1$ . How to do it?","['sequences-and-series', 'real-analysis']"
4260750,How to solve this quadratic nonhomogenius differential equation,"I have this differential equation $r'(t) = \sqrt{1-\left(\dfrac{a}{1+a^2t^2}\right)^2 r(t)^2}$ where $r(0)=0, r'(0)=1$ I have no clue how to solve it, my usual techniques fail (such as separation of variables, direct integration, and a lot of things that don't apply because it is nonhomogenius and nonlinear). I also tried to find some substitutions to separate the variables without success. With the rescaling $r\rightarrow r/a,t\rightarrow t/a$ as suggested by JohnBarber and a subsitution $t=\tan(u)$ (where the time domain $[0,\infty)$ maps to $[0,\pi/2)$ ) this can be simplified to $r'(u)^2+r(u)^2=\dfrac{1}{\cos(u)^4}, r(0)=0, r'(0)=1$ Any suggestions would be highly appreciated. As it was asked in the comments: the background is the following geometrical problem: A point $A$ moves along a the line $y=1/2$ with position $x(t)=a⋅t$ with constant velocity $a>0$ . A second point $B$ starting at the origin $O$ tries to ""stick to"" the (moving) line $AO$ while using its remaining velocity component of total velocity $|v|=1$ to move away from $A$ . The movement of $B$ in polar coordinates $(r,\varphi)$ is such that $\varphi(t)$ follows trivially from the problem while $r(t)$ satisfies the above equation. More detailed, one derives that $\varphi'(t)=2a/(1+(2at)^2)$ and plugs it into $|v|=1$ which leads to the above equation. The geometric view suggests that with inreasing distance $r$ there might be a point in time where $B$ cannot compensate the rotational motion of $A$ anymore which is when the term under the squareroot becomes negative.",['ordinary-differential-equations']
4260804,"Proving that there is an infinite number of pairs of prime numbers for which $F(n)F(n+1) =pq $ for no $n>1 \in \mathbb{N}$, $F(n)$ is the GPF function","Proving that there is an infinite number of pairs of prime numbers for which $F(n)F(n+1) = pq $ does not hold for any $n>1 \in \mathbb{N}$ , $F(n)$ is the GPF function I have been trying to solve this problem involving the greatest prime factor function (A006530). Two primes hold the desired property if their product is equal to the product of the GPF of two consecutive natural numbers. A brief look at the function will reveal that many of the small primes will indeed work, but we are supposed to get an infinite number of pairs which cannot have the property. How to approach this problem? OEIS does not provide any useful piece of information regarding the function itself, so perhaps something connected with prime distributions directly? Thank you in advance, looking forward to your insights","['number-theory', 'prime-factorization', 'prime-numbers']"
4260841,How to prove this function is measurable?,"Suppose $\{f_n\}$ is a sequence of measurable functions on $[0,1]$ . For any $x \in [0,1]$ , suppose that $\exists$ $N$ s.t. $f_n(x)=0$ for all $n\ge N$ . Define function $h$ as the smallest index $n$ for which $f_n(x)=0$ , i.e. $h(x) = inf\{n:f_n(x)=0\}$ . If I want to show that h is measurable function, what should I do? Actually, I've tried a lot. I thought my definition of $g_n := \chi_{\{0\}}(f_n(x))$ is good because it used the condition that $\{f_n\}$ is measurable, but not useful to prove it. Or definition? But I still don't know how to show the inverse is a $\sigma$ -algebra.","['measure-theory', 'measurable-functions', 'real-analysis']"
4260843,How to solve $\sqrt{\tan x}=1$ without squaring?,Problem: $$\sqrt{\tan x}=1$$ Solution (with squaring): $$\sqrt{\tan x}=1$$ $$\tan x=1$$ $$x=n\pi+\frac{\pi}{4}\tag{1}$$ 1. $(i)$ contains extraneous roots. How do I filter them out? 2. How do I solve for $x$ without squaring?,"['algebra-precalculus', 'trigonometry']"
4260845,How to prove than $p_{2n}-(p_{2n}\mod p_{n}) = 2p_{n}$ ? where $p_{n}$ is the $_{n}$th prime number ? (for $n$ > 1),"Let the prime function $p_n$ be the $n$ th prime number. For example $p_1$ = 2, $p_2$ = 3, $p_3$ = 5, $p_4$ = 7, $p_5$ = 11 etc. I noticed something with the prime function : it seems than $p_{2n}-(p_{2n}\mod p_{n}) = 2p_{n}$ , for $n$ > 1 For example : $p_{4}-(p_{4}\mod p_{2})$ = 7 - (7 mod 3) = 6 = 2*3 $p_{6}-(p_{6}\mod p_{3})$ = 13 - (13 mod 5) = 10 = 2*5 $p_{8}-(p_{8}\mod p_{4})$ = 19 - (19 mod 7) = 14 = 2*7 $p_{100}-(p_{100}\mod p_{50})$ = 541 - (541 mod 229) = 458 = 2*229 It seems than $p_{2n}$ is connected with $p_n$ . This is something counter-intuitive for me because this is the first time I see a link with $p_{2n}$ and $p_n$ and I thought there was no link about these two numbers. Is there a way to explain that ? I don't know how to start for proving it.","['number-theory', 'prime-gaps', 'prime-numbers']"
4260877,Derivation of the Backpropagation Algorithm,"I'm studying the Backpropagation algorithm and I want to derive it by myself.
Therefore, I've constructed a very simple network with one input layer, one hidden layer and one output layer. You can find the details in the graphic. The network can be found here: $t$ is the true output, $i$ is the input, $z_{hi}$ and $w_{ij}$ are the weights. Moreover I have the activation function $\phi(x)$ . I think that I have understood the algorithm more or less, so I've started with the following error function: $$
E = \frac{1}{2}\sum_{j=1}^2{(t_j-o_j)^2} 
$$ That function should be correct, isn't it?
The next step is to calculate the partial derivative: $$
\frac{\partial E }{\partial w_{ik}} = \frac{\partial E }{\partial o_k}\cdot  \frac{\partial o_k }{\partial w_{ik}} = -(t_k-o_k) \cdot  \frac{\partial o_k }{\partial w_{ik}} =-(t_k-o_k) \cdot  \phi'(\sum_{i=1}^2 w_{i,k}\cdot a_k ) \cdot a_k
$$ For me that seems to be correct, but is that right up to here (Especially the formal math aspects)? I'm asking, because i have so problems with the next steps (derivating to the z's... Thanks for helping!","['partial-derivative', 'derivatives', 'neural-networks']"
4260881,How to find $\lim_{x\to 2}\frac{|x-2|}{2x-x^2}$,"$$\lim_{x\to 2}\frac{|x-2|}{2x-x^2}$$ I know the answer of the left hand limit is $1/2$ ; while the right hand limit is $-1/2$ . But I don't understand how do you get that? If I factor $-x$ from the denominator, I'll get $(-2+x)$ which cancels out with the numerator. Then I'll get $1/-x$ . If I plug in the limit of $2$ from the left hand, it would be $1/2$ . Wouldn't it also be $1/2$ from the right hand, as well? I'm getting confused on how to work this out. Please help, thank you!","['limits', 'calculus', 'factoring', 'algebra-precalculus']"
4260882,Fitting a tangent curve by hand / on paper,"Sorry if anything is unclear or a bit of a stupid question my maths is nowhere near as advanced as yours, I searched quite extensively on stats.stackexchange and stackexchange and couldn't find or at least understand something that helped me do this. I need to fit a single iteration of a transformed tangent function to this data as a form of regression and show how I did it manually. I did something similar with a cubic which was very simple. I've heard of different computational greedy algorithms to do this but I'd need to solve this by hand to show I understand the maths behind it. The equation would be a generic $𝑦=𝐴\tan(𝐵𝑥+𝐶)+𝐷.$ If I understood correctly the difficulty in solving this is the fact that it includes unknown coefficients both inside and outside of the brackets. Is there a way for me to solve it with all four unknowns? If not I'd be ok with knowing how to estimate some of the unknowns on this data and then solving the rest based on those assumptions. It doesn't need to be perfect but I need to know the limitations of the method I use and it needs to be good enough to be representative of the data within the given domain of $0<x<5.34.$ I was able to do this in desmos with an $r^2$ of 0.9958. The equation I got was $$y=0.336305\tan(0.486962x+1.87644)+1.19392$$ which would be $a=0.336305, b=0.486962, c=1.87644, d=1.19392$ Essentially I need a model to fit this rotated s-shape. Doing the inverse and using a logistic function kind of works (see pictures attached) but it really wouldn't be ideal. I look forward to any of your proposals, thank you for your time! The tan regression is shown in blue and the inversed data with the logistif regression is shown in black. Desmos tan and logistic regression: Data: https://pastebin.com/Frf6LZYR","['regression', 'trigonometry', 'systems-of-equations', 'mathematical-modeling']"
4260918,second countable and Hausdorff doesn't imply normal,"We know that if $X$ is a second countable and regular, then it is normal. Now assume that $X$ is second countable and Hausdorff. Are there any examples to show that $X$ need not necessarily be a normal space?",['general-topology']
4260920,"Tied chess matches and the monotonicity of $\sum_{k=0}^n \binom{2n}{k,k,2n-2k} (pq)^k (1-p-q)^{2n-2k}$","In the upcoming World Chess Championship 14 games in the classical time format will be played compared to 12 in the previous matches. This change appears to have been made mainly to reduce the number of draws by allowing the players to take more risks, but it has also been stated that this decreases the chances of a tie in the overall series. Intuitively, this seems clear: the higher the total number of games, the less likely it is that both players win exactly the same number of games. A mathematical proof would be better, of course, and this is what I am after here. I want to consider a simple model for the general case with $2n$ games, $n \in \mathbb{N}$ . Let $p$ and $q$ be the probability for the first and second player to win any given game (independent of previous results, current form, etc.), respectively. Then $0 \leq p, q, p + q \leq 1$ holds and $1-p-q$ is the probability of a draw (we ignore the advantage for the player with the white pieces and interpret $p$ and $q$ as colour averages). Even though the series is not continued if one player takes an unassailable lead, we can pretend that all $2n$ games are played and compute the probability of $k$ wins for the first and $l$ wins for the second player from a trinomial distribution : \begin{equation}
f(p,q,n;k,l) =  \binom{2n}{k,l,2n-k-l} p^k q^l (1-p-q)^{2n-k-l} . 
\end{equation} The probability of an overall tie is \begin{align}
t(p,q,n) &= \sum \limits_{k=0}^n f(p,q,n;k,k) = \sum \limits_{k=0}^n \frac{(2n)!}{k!^2 (2n-2k)!} (pq)^k (1-p-q)^{2n-2k} \\
&= (1-p-q)^{2n} {}_2F_1 \left(\frac{1}{2}-n,-n;1;\frac{4 p q}{(1-p-q)^2}\right) .
\end{align} Provided that this reasoning is correct, my question is: How can we show that $t(p,q,n)$ is decreasing in $n$ for any given $p,q,p+q \in [0,1]$ ? Some thoughts: Numerically, this clearly appears to be true. This is the probability of a tie for $q=p$ : In the special case $p=q=\frac{1}{2}$ we have $$ t\left(\frac{1}{2},\frac{1}{2},n\right) = f\left(\frac{1}{2},\frac{1}{2},n;n,n\right) = \frac{1}{4^n} \binom{2n}{n} = \frac{2}{\pi} \int \limits_0^\infty \frac{\mathrm{d} x}{(1+x^2)^{n+1}} \, , $$ which is clearly decreasing in $n$ . Maybe an integral representation can also be helpful in the general case. I do not know a lot about hypergeometric functions though and cannot find a transformation which would allow us to use Euler's integral formula .","['multinomial-coefficients', 'sequences-and-series', 'multinomial-distribution', 'probability', 'hypergeometric-function']"
4260942,An Exercise from Chapter 2 of Stein's Complex Analysis,"I have been working through Stein and Shakarchi's Complex Analysis and I'm stuck on Exercise 15 from Chapter 2. The question states: Suppose $f$ is a non-vanishing continuous function on $\overline{D}$ that is holomorphic in $D$ . Prove that if $$|f(z)| = 1$$ whenever $|z|=1$ , then $f$ is constant. [Hint: Extend $f$ to all of $\mathbb{C}$ by $f(z) = 1/\overline{f(1/\overline{z})}$ whenever $|z|>1$ , and argue as in the Schwarz Reflection Principle.] I have made some progress with the hint. Namely I have shown that $1/\overline{f(1/\overline{z})}$ is holomorphic whenever $|z|>1$ . Furthermore, since $|f(z)|=1$ whenever $|z|=1$ , we know that the following function is well defined: $$ g(z) = \begin{cases} 
      f(z) & |z|<1 \\
      f(z) = 1/\overline{f(1/\overline{z})} & |z|=1 \\
      1/\overline{f(1/\overline{z})} & |z|>1 
   \end{cases}
$$ However, since we only have continuity at $|z|=1$ , I'm not sure how we can use something like the Symmetry Principle to conclude that $g$ is a holomorphic function on $\mathbb{C}$ . Once we have this, I'm pretty sure that we can deduce by Liouville's Theorem that $g$ is constant since it would be bounded and entire. Any hints or suggestions would be greatly appreciated.","['complex-analysis', 'analytic-continuation']"
4260944,Find the right derivative of $\sin^2\left(x \sin \frac1x\right)$ at $x = 0$.,"$$f(x)=\sin^2\left(x \sin \frac1x\right)$$ I was trying to find the right derivative of $f$ at $x = 0$ . ( In this question, it says to take the f(0) as f is right continuous at x = 0. Therefore, considering the right continuous at x = 0, I obtained f(0) = 0 ) before that, I know,
I should find whether $f$ is the right differentiable at $x = 0$ . Is $f$ right differentiable at $x = 0?$ If so, what is the right derivative of $f$ at $0?$ I think $f$ is not right differentiable at $x = 0$ . It would be great if someone could clarify this. Any help would be appreciated.","['limits', 'functions', 'derivatives']"
4260971,Show that the coefficient of $(x_1+x_2+\dots+x_n)^m \prod_{1\leq j<i<n}(x_i-x_j)$ is $\frac{m!}{m_1! m_2!\cdots m_n!}\prod_{1\leq j<i\leq n}(m_i-m_j)$,"Let $m_1,m_2,\dots,m_n\in \mathbb{Z}_0^{+}$ such that $\sum_{i=1}^nm_i\geq {n \choose 2}$ . Let us write $m=\sum_{i=1}^nm_i-{n \choose 2}$ . Prove that the coefficient of $(x_1+x_2+\dots+x_n)^m \prod_{1\leq j<i<n}(x_i-x_j)$ in the monomial $x_1^{m_1}x_2^{m_2}\cdots x_n^{m_n}$ is $$\frac{m!}{m_1! m_2!\cdots m_n!}\prod_{1\leq j<i\leq n}(m_i-m_j).$$ I noticed that the $$f(x_1,x_2,\dots ,x_n)=\prod_{1\leq j<i<n}(x_i-x_j)$$ is the vandermonde polynomial, and I guess  that $\deg(f)={n \choose 2}$ since $f$ is non zero if all the $x_i\neq x_j$ . Now for the other side I have $$g(x_1,x_2,\dots, x_n)=(x_1+x_2+\cdots+x_n)^m$$ which is pretty simetric and also I guess that a coefficient of $g$ in any monomial is $${m \choose k_1,k_2,\dots, k_n}x_1^{k_1}x_1^{k_2}\cdots x_n^{k_n}.$$ Now I try paste both parts but I can't figure out how I should do it. I think that I should think how should be the $k_i$ since $\deg (g)=m$ and for get the monomial $x_1^{m_1}x_2^{m_2}\cdots x_n^{m_n}$ I need a monomial of $f$ which degree is ${n \choose 2}$ and complete the sum of $m_i$ . Edit: I try do a particular case when $n=3$ and $m_1=3,m_2=1,m_3=1$ and search the monomial $x_1^3x_2x_3$ and then I expand the polynomial $$(x_1-x_2)(x_2-x_3)(x_1-x_3)(x_1+x_2+x_3)^2=$$ $$x_2 x_1^4 - x_3 x_1^4 + x_2^2 x_1^3 - x_3^2 x_1^3 - x_2^3 x_1^2 + x_3^3 x_1^2 - x_2^4 x_1 + x_3^4 x_1 - x_2 x_3^4 - x_2^2 x_3^3 + x_2^3 x_3^2 + x_2^4 x_3$$ and first I try watch a regular expression or a pattern, but it seems like there aren't pattern and second I don't get the term and $x_1^3x_2x_3$ in the expression above, now check my calculations and are right, but in these case I don't know where I made the mistake?. Also I try only expand the vandermonde polynomial for $3$ variables. $$(x_3-x_1)(x_2-x_1)(x_3-x_2)=x_3^2(x_2-x_1)+x_2^2(x_1-x_3)+x_1^2(x_3-x_2)$$ And  I think that for $4$ variables I should obtain $$x_3^3(x_2-x_1)+x_2^3(x_1-x_3)+x_1^3(x_3-x_2)+x^4(x_4-x_1)$$ but it is wrong, and I don't identify a pattern. Any comment, hint or suggestion is useful.
Thanks in advance.","['finite-fields', 'functions', 'combinatorics', 'multinomial-coefficients']"
4261003,Can the LCT and MCT for Lebesgue integrable functions be viewed as a lattice completeness result?,"The set of Lebesgue integrable functions form a lattice under pointwise min and max (also more generally for R, Henstock-Kurzweil integrable functions with an upper or lower bound form a lattice as well). The convergence theorems look a lot like countable completeness for that lattice. Is there an alternative formulation or generalization of them that is exactly that or further in that direction?","['integration', 'lebesgue-measure', 'lebesgue-integral', 'lattice-orders', 'gauge-integral']"
4261005,What is the natural projection of a partition?,"According to the text provided by my teacher, ""any partition $P$ equals the natural partition of its natural projection, $P = P_p$ "" (to be proven as an exercise). Above this line, the text states that ""given any partition $P$ of $X$ , we can induce the natural projection of $X$ onto $P$ , $p$ : $X \rightarrow P$ satisfying $p$ : $x \mapsto [x$ ]."" If the natural projection $p$ requires two sets, an original set X and a partition P such that $p$ : $X \rightarrow P$ satisfying $p$ : $x \mapsto [x$ ], then what does the phrase ""its natural projection"" in the first line refer to – what is the natural projection of a partition? Thanks in advance.",['elementary-set-theory']
4261081,Infinitesimal Generator of semigroup for markov chain,"In Wikipedia the definition of generator of a semigroup is given as follows For a Feller process $(X_t)_{t\ge0}$ with Feller Semigroup $T=(T_t)_{t\ge0}$ and state space $E$ we define the generator $(A,D(A))$ by: $$D(A)=\left\{f\in C_\infty :\lim_{t↓0}\frac{T_tf-f}{t}\text{exists as a uniform limit}\right\},$$ I saw another definition of infinitsimal generator in context of markov chains which says that if $$d\phi(X_t)=A \phi(X_t)dt+dM_t$$ where $M_t$ is a martingale and $\phi \in C_c^{\infty}(\mathcal{X})$ and { $X_t$ } is a markov chain on $\mathcal{X}$ then A is the infinitesimal generator.
Are these two definitions equivalent?","['markov-chains', 'stochastic-processes', 'functional-analysis', 'semigroup-of-operators', 'group-theory']"
4261152,$3^n$ as sum of powers of $2$ [duplicate],This question already has answers here : Representation of powers of three in binary. (2 answers) Closed 2 years ago . Is there a closed form to express $3^n$ as a sum powers of $2$ ? I am interested in the case where all exponents of $2$ are unique. $$3^0 = 2^0$$ $$3^1 = 2^0 + 2^1$$ $$3^2 = 2^0 + 2^3$$ $$3^n = ?$$,"['number-theory', 'modular-arithmetic', 'discrete-mathematics', 'exponentiation']"
4261171,Why $\left(\frac{\partial x}{\partial y}\right)_z=-\left(\frac{\partial x}{\partial z}\right)_y \left(\frac{\partial z}{\partial y}\right)_x$?,"I was reading a statistical mechanics book, the author use: $\left(\frac{\partial x}{\partial y}\right)_z=-\left(\frac{\partial x}{\partial z}\right)_y \left(\frac{\partial z}{\partial y}\right)_x$ so I'm trying to prove this, but I'm stuck. I'm using the theorem: $dF=\frac{\partial F}{\partial x}dx+\frac{\partial F}{\partial y}dy+ \frac{\partial F}{\partial z}dz=0$ with $dz=\left(\frac{\partial z}{\partial x}\right)_y dx +\left(\frac{\partial z}{\partial y}\right)_xdy$ if $z$ is constant then $dz=0$ then $\left(\frac{\partial z}{\partial x}\right)_y dx =-\left(\frac{\partial z}{\partial y}\right)_xdy$ so I get: $\frac{\partial F}{\partial x}dx+\frac{\partial F}{\partial y}dy=0$ but this result doesn't lead me to anything","['partial-derivative', 'multivariable-calculus', 'calculus', 'statistical-mechanics']"
4261186,"Show that the joint distribution of $X_1, X_2, \dots, X_n$ belongs to a two-parameter exponential family.","Let $X_1, X_2, \dots, X_n$ be ind $ \sim\text{Ber}(\theta_i)$ where \begin{equation}
        \theta_i = P(X_i=1)=\frac{\exp(\alpha+\beta t_i)}{1+\exp(\alpha + \beta t_i)}
\end{equation} where $t_1, t_2, \dots, t_n$ are known constants.  Show that the joint distribution of $X_1, X_2, \dots, X_n$ belongs to a two-parameter exponential family. $\bf{My Attempt (WIP) :}$ We can start by finding the joint pmf of $X_1, X_2, \dots, X_n$ \begin{align*}
        f(\mathbf{X}) &= \prod_{i=1}^n \theta_i^{x_i} (1-\theta_i)^{1-x_i}\\
        &=\prod_{i=1}^n \frac{(e^{\alpha+\beta t_i})^{x_i}}{ (1+e^{\alpha+\beta t_i})^{x_i}} (1-\frac{e^{\alpha+\beta t_i}}{1+e^{\alpha+\beta t_i}})^{1-x_i}\\
        &=\prod_{i=1}^n \frac{(e^{\alpha+\beta t_i})^{x_i}}{ (1+e^{\alpha+\beta t_i})^{x_i}} (\frac{1+e^{\alpha+\beta t_i}}{1+e^{\alpha+\beta t_i}}-\frac{e^{\alpha+\beta t_i}}{1+e^{\alpha+\beta t_i}})^{1-x_i}\\
        &=\prod_{i=1}^n \frac{(e^{\alpha+\beta t_i})^{x_i}}{ (1+e^{\alpha+\beta t_i})^{x_i}} (\frac{1}{1+e^{\alpha+\beta t_i}})^{1-x_i}\\
        &=\prod_{i=1}^n \frac{(e^{\alpha+\beta t_i})^{x_i}}{ (1+e^{\alpha+\beta t_i})^{x_i + 1 - x_i}}  \\
        &=\prod_{i=1}^n \frac{e^{\alpha x_i+\beta t_ix_i}}{1+e^{\alpha+\beta t_i}}\\
        &=\frac{e^{\alpha\sum_{i=1}^n x_i+\beta\sum_{i=1}^n t_i x_i} }{\prod_{i=1}^n{(1+e^{\alpha+\beta t_i})}}
    \end{align*} This is exponential family with $T(x) = (\sum x_i, \sum t_i x_i)$ , $\eta= (\alpha, \beta)$ , and $A(\eta) =A(\eta)=\frac{1}{\prod_{i=1}^n{(1+e^{\alpha+\beta t_i})}}$","['statistical-inference', 'statistics', 'probability-distributions', 'logistic-regression']"
4261201,On the matter of removing discontinuities.,"I have been asked to determine whether the function $f(x) = \frac{\sin{x}}{x}$ is continuous on $\mathbb{R}$ . I do understand that the function in continuous everywhere except for the point $x=0$ , where the function is undefined (the $\frac{0}{0} 
$ indeterminacy). I do, however, remember what the textbook told me about removable discontinuities, i.e. that whenever a function possesses a definite limit as its argument approaches a value outside of the function’s domain, we are (somehow) given the right to assign the value of that limit to be the value of the function at that point. The question is why in the world such a feint is even plausible? Of course, we can say that the “modified” function with its discontinuity removed is a whole new function, which possesses a domain different from the initial function, that is perfectly fine, but as far as I understand, we regard such functions in their initial form as the ones in which the discontinuities were already removed? I know that it might be too many words for a question to be even comprehensible, so I am going to provide an example using the function I mentioned already: $f(x) = \frac{\sin{x}}{x}$ . We know that it is undefined at $x=0$ , however we can remove the discontinuity at that point (for the limit is equal to $1$ as $x$ approaches $0$ ), and thus make it continuous on $\mathbb{R}$ . Do we still regard the modified function as $f(x) = \frac{\sin{x}}{x}$ , or do we specifically mention that it is a different function, possessing a distinct value at $x = 0$ ?","['analysis', 'real-analysis', 'continuity', 'calculus', 'limits']"
4261218,Parameter of ellipse in $\mathbb{R}^3$ derived from 2 planar projections,Given is an arbitrarily oriented ellipse in $\mathbb{R}^3$ that is by parallel projection mapped onto $xy$ - and $zy$ -planes.  How can I derive the directions and lengths of the semi-axes of the original ellipse from the $2$ projections? (There might be $2$ solutions that are related by reflection. Knowing a single solution is sufficient.) What is known? (see here ) the projected shapes must be also ellipses the semi-axes of the original ellipses are in general after projection not semi-axes of the planar ellipses projected semi-axes of the original ellipse are conjugated diameters of the planar ellipse and in general not perpendicular Possible solution strategy Consider all conjugated diameters (i.e. tangential parallelograms) of the $2$ planar ellipses to find that combination that is the projection of semi-axes of the original ellipse.,"['trigonometry', 'conic-sections', 'geometry']"
4261376,What does it mean for a correspondence to be single valued?,"Let's say we have a correspondence $f:A \rightarrow B$ , what would it mean for f to be single valued. Does that mean each element of A gets mapped to a unique subset of B or do all elements of A get mapped to one particular subset in B?","['general-topology', 'discrete-mathematics', 'real-analysis']"
4261379,Differential of a trigonometric function,"If $x\sin \left( {\alpha  + y} \right) = \sin y$ and $y' = \frac{m}{{{x^2} + 2nx + 1}}$ , then find the value of $m^2+n^2$ .{where $y' = \frac{{dy}}{{dx}}$ } My approach is as follow $x\cos \left( {\alpha  + y} \right)y' + \sin \left( {\alpha  + y} \right) = y'\cos y \Rightarrow y'\left( {x\cos \left( {\alpha  + y} \right) - \cos y} \right) =  - \sin \left( {\alpha  + y} \right)$ $\Rightarrow y' = \frac{{ - \sin \left( {\alpha  + y} \right)}}{{\left( {x\cos \left( {\alpha  + y} \right) - \cos y} \right)}}$ How will corelate the above equation with $y' = \frac{m}{{{x^2} + 2nx + 1}}$","['calculus', 'trigonometry']"
4261395,Definition of multivariable function,"A multivariable function has been defined as given below: $f(x,y) =\begin{cases} \dfrac{x^3\cos\frac{1}{y} + y^3\cos\frac{1}{x} }{x^2+y^2} &x,y \neq  0,\\
\\0&\text{otherwise}. \end{cases}$ I have to investigate its continuity and differentiability at point $(0,0)$ .
But I am not sure about the definition of the given function.
Does the above definition mean that $f(x,0)=0,x\neq 0$ and $f(0,y)=0,y\neq 0$ ? This is important because when I consider this definition for $f(x,0)$ and $f(0,y)$ then both partial derivatives exist and are equal to $0$ . Otherwise if we define $f(x,y)=0$ only for $(x,y)=(0,0)$ then partial derivatives do not exist and I can readily conclude the non-differentiability of $f(x,y)$ at $(0,0)$ as existence of partial derivatives is necessary for the function to be differentiable. Please help me as to how to interpret this definition. Also is the function differentiable if I interpret this definition in above mentioned manner i.e. $f(x,0)=0,x\neq 0$ and $f(0,y)=0,y\neq 0$ .
Here is my work on differentiability part: $∆z=a∆x+b∆y+k_1∆x+k_2∆y$ where $a$ and $b$ are the partial derivatives w.r.t. $x$ and $y$ respectively.
For differentiability, $k_1$ and $k_2$ should tend to $0$ as $∆x$ and $∆y$ tend to $0$ . $∆z=
\dfrac{(∆x)^3\cos\frac{1}{∆y} + (∆y)^3\cos\frac{1}{∆x} }{(∆x)^2+(∆y)^2} $ .
Thus $k_1=\dfrac{(∆x)^2\cos\frac{1}{∆y}} {(∆x)^2+(∆y)^2} $ and $k_2=\dfrac{(∆y)^2\cos\frac{1}{∆x}} {(∆x)^2+(∆y)^2} $ . Now do $k_1$ and $k_2$ tend to $0$ as $∆x$ and $∆y$ go to $0$ ? If I take $∆x=∆y$ then $k_1$ comes out to be limit of $\frac {1}{2}cos\frac{1}{∆y}$ as $∆y$ goes to $0$ which does not exist.
Am I doing it correctly? Please suggest.","['limits', 'multivariable-calculus', 'derivatives', 'continuity']"
4261410,Isn't a homeomorphism between an open interval and $\mathbb{R}$ a contradiction?,"Let $X = (-\frac{\pi}{2},\frac{\pi}{2})$ . Define $f: X \rightarrow \mathbb{R}$ given by $f(x) = \tan(x), \, \forall x \in X$ . It can be shown that $f$ is a homeomorphism. We have that $X$ is an open set. $\mathbb{R}$ is open and closed. By the homeomorphism, $X$ should be open, once $\mathbb{R}$ is open; but also closed since $\mathbb{R}$ is closed. However, $X$ is not closed. What am I missing here? Thanks!","['general-topology', 'real-analysis']"
4261418,Geometry problem using circular arcs,"The construction for the problem is as follows: Given some circular arc $A$ centred at $C$ with an angle $\theta \geq 180^{\circ}$ and endpoints $a,b$ , take some arbitrary point $t$ inside the region bounded by the arc and the segment $ab$ . Construct two arcs, $A_1, A_2$ within $A$ by taking the intersection of the perpendicular bisector of $at$ / $tb$ and the segment $aC$ / $Cb$ . The endpoints of the arcs are at $a, t$ and $t, b$ respectively, and the centre at the aforementioned intersection point. The problem is to show that $\mathbf{A_1 + A_2 \leq A}$ for any choice of $\mathbf{t}$ . Construction: Some initial points are that clearly if $t$ is on $A$ , then $A_1, A_2$ just make up the entire arc, and then if $t$ is on the segment $ab$ then you can show using similar triangles that $A_1 + A_2 = A$ in this case. While this problem seems intuitively true, I haven't been able to come up with any way of proving it definitively.",['geometry']
4261424,Xor of two binary random variables,"Let $X,Y\in\{0,1\}$ be two dependent binary random variables such that $\Pr[X=0]=\frac{1}{2}+\alpha$ and $\Pr[Y=0]=\frac{1}{2}+\beta$ for $\alpha,\beta\geq 0$ . My question is how to get a lower bound of $\Pr[X\oplus Y=0]$ . Here $X\oplus Y$ is the xor of two binary variables $X,Y$ . In the case that they are independent, $\Pr[X\oplus Y=0]=(1/2+\alpha)(1/2+\beta)+(1/2-\alpha)(1/2-\beta)=1/2+2\alpha\beta$ . When they are dependent, I have $\Pr[X\oplus Y=0]\geq \Pr[X=0,Y=0]\geq 1-(1/2-\alpha)-(1/2-\beta)=\alpha+\beta$ . However, the bound seems weak. I suspect one can get a lower bound greater than $1/2$ as in the independent case, but a counterexample would also be appreciated.","['upper-lower-bounds', 'probability', 'random-variables']"
4261477,Extra solutions when solving $\sin\theta+\cos\theta=\sqrt{2\sin2\theta}$,"The problem: $$\sin\theta+\cos\theta=\sqrt{2\sin2\theta}$$ My solution: $$\sin\theta+\cos\theta=\sqrt{2\sin2\theta}$$ $$\sin\theta+\cos\theta=\sqrt{2\times2\sin\theta\cos\theta}$$ $$\sin\theta+\cos\theta=2\sqrt{\sin\theta\cos\theta}$$ $$\sin\theta-2\sqrt{\sin\theta\cos\theta}+\cos\theta=0$$ $$(\sqrt{\sin\theta}-\sqrt{\cos\theta})^2=0$$ $$\sqrt{\sin\theta}=\sqrt{\cos\theta}$$ Either $\cos\theta$ or $\sin\theta$ can be equal to zero. Both can't be equal to zero at the same time. As we can see that $\sqrt{\sin\theta}=\sqrt{\cos\theta}$ , neither of $\cos\theta$ and $\sin\theta$ is equal to zero. So, dividing both sides by $\cos\theta$ is valid. $$\sqrt{\tan\theta}=1$$ $$\tan\theta=1$$ $$\theta=n\pi+\frac{\pi}{4}\tag{1}$$ My question: We've stumbled upon an interesting solution in (1). Here, $\theta$ satisfies our original equation only when $n$ is even or $0$ . Why is that? Isn't $n$ supposed to belong to the set of integers? PS: This might help you in answering the question.",['trigonometry']
4261484,How to distinguish $y^2=(x+a)(x-b)^2$ from $y^2=(x+a)(x-b)^4$ if a graph is given?,"From the following question, I understood that the answer must be one of  options c and f .
The other options are wrong (I will not explain why). Question How can I decide the correct answer from the left options ( c and f )? Edit I plotted both with Mathematica as follows. However I prefer to solve it analytically without plotting them first if possible.",['algebra-precalculus']
4261496,Peano Axioms and modeling sets.,"I am assuming the standard Peano-Axioms, which can be found here https://en.wikipedia.org/wiki/Peano_axioms under ""Formulation"". Does one assume that a set of objects called $\mathbf{N}$ exists? A set is a collection of objects according to naive set theory, therefore, if not assuming that such objects exists, where are they coming from? I see it often that these axioms are used to define a set of natural numbers and that one then shows that any two sets that satisfy those axioms are isomorphic, which allows to speak of the natural numbers. But again, if not assuming that a set with objects exists, how can I construct a set? I could write down things like $\{0,I,II,III,...\}$ which would satisfy the PA, but is this a set? What are its objects? I think one would have to assume something like: There exists a set $A$ that satisfies the PA and then define $I:=s(0)$ , $II:=s(1)$ and so on. This would fix the problem, since in this case I would have assumed objects. However, this would only give me one set of objects. Meaning, I could also model $\{0,1,2,3,...\}$ by defining it the same way as above. This however would lead to $\{0,1,2,3,...\}=\{0,I,II,III,...\}$ . Is it possible to have two models that are ""only"" isomorphic, meaning not equal but isomorphic? Or more strictly, is it possible to have two models with completely different objects ? As far as I see it, I need to assume the existence of some kind of objects, and then give those objects names such as $1,2,3$ or $0,I,II,III$ , however, can I reverse this by giving names such as $0,I,II,III,...$ and find objects that model this set (in this case this should be possible when assuming the existence of some sort of natural numbers)? Is this possible for every set ? Doesn't this also lead to ambiguity, since one does not know which model is meant?","['elementary-set-theory', 'axioms', 'set-theory', 'peano-axioms']"
4261501,The angle $ \angle APB$ between the tangents to a curve.,"I have been asked to find $\angle{APB}$ in the form: $\tan^{-1}{\alpha}$ + $\tan^{-1}{\beta}$ I got the equations of the lines through differentiating the function $h(x) = {(\ln(x) - 1.5)}^{2} - 0.25$ at $e$ and ${e}^{2}$ respectively The equation of the first line is: $$y = -xe^{-1} + 1$$ The equation of the second line is: $$y = xe^{-2} -1$$ Then after doing some math, I got: $\angle{APB}$ = 180 - (180 - $\tan^{-1}{(\frac{-1}{e})}$ + $\tan^{-1}{(\frac{1}{e^2})}$ ) $\angle{APB}$ = $\tan^{-1}{(\frac{-1}{e})}$ - $\tan^{-1}{(\frac{1}{e^2})}$ However, this is not the correct answer(due to the minus sign). I do not know if I am doing the correct work so far or if I have gone far off. $ \angle APB$","['triangles', 'trigonometry', 'geometry', 'tangent-line']"
4261539,Probability of getting into my favorite PhD,"Suppose there are $n$ potential grad students and $n$ Universities. Each University has one scholarship to do a PhD. Each student has a strict ranking of Universities so that each University is comparable and she is not indifferent between any two Universities.  The preferences of each student are independent and generated uniformly at random. Students are ordered according to their height (or some other irrelevant attribute) and can choose their favourite University to study in based on this order. Universities accept the first student who applies to them and their decision is final and irrevocable. My first question is: for the $k$ tallest applicant, what is the probability that they end up in their $j$ -th best University? I'm thinking for the k-th tallest applicant, the chance of getting into her top choice is $\frac{n+1-k}{n}$ . Second question: Suppose now that, if a student gets her first choice, she completes her PhD with probability 1, if she gets her second choice, she does with probability 1/2 and so on, so in general, if a student gets her k choice, she finishes her PhD with probability $\frac{1}{2^{k-1}}$ . What is the expected number of students that fail to complete their PhD? Simulations tell me it is $0.38 n$ , but I would like to understand why.","['order-statistics', 'probability']"
4261570,Sample size calculation for accuracy,"In a scientific association, an electoral process for the president of the association will take place. There are 2 candidates, A and B. Every member (of the N in total) will vote either for A or for B. We would like to estimate the percentage of A candidate (let’s call it P) and for this, we use a sample of size $n (n<N)$ .
What is the optimum number of n, so that the percentage of voters of A in the sample varies from the real percentage P of voters of A by less than $1%$ (in absolute value), with probability $95%$ ? I have found a wikipedia link https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Normal_approximation_interval but am not sure which formula to use, since there are no numbers for the population $(N)$ and also for $P$ and $(1-P)$ . As I am not at all familiar with statistics and this kind of stuff, I did a bit of research - the only (easy) thing I managed to find is that the value of $z$ for $95%$ is $1.96$ . So we would like $P(|\hat{p}-p| \le 0.01)$ to be $95%$ .
But we don't have any indication of $P$ ! (I am not doing any homework - this is a ""real life"" problem to calculate a suggested ""exit poll"" size). Thank you very much in advance! Edit:
I now found this equation: $n \geq \left(\dfrac{z_{\alpha/2} \sigma}{\delta}\right)^2$ and wonder if I can use it.","['statistics', 'standard-deviation', 'probability']"
4261627,"A closed subspace of $C([0,1])$ with all functions of bounded variation has finite dimension","In several papers on spaceability I found cited the following theorem of Levine and Milman (1940): Theorem: Let $E$ be a closed subspace of $C([0,1])$ (that is $C([0,1],\mathbb{R})$ endowed with the maximum norm $\|\cdot\|_\infty$ ) such that each $f \in E$ is of bounded variation. Then $\dim E < \infty$ . I woluld like to see a proof of this theorem but couldn't find one neither online nor offline. So, I tried
to prove it myself, but I got stuck. Here's my attempt: Let $\|f\|_v := |f(0)| + Var_f([0,1])$ , and note that $\|f\|_\infty \le \|f\|_v$ for each $f \in C([0,1]) \cap BV([0,1])$ . Set $$
 A_n:=\{f \in E: \|f\|_v \le n\}, \quad n \in \mathbb{N}.
 $$ Each $A_n$ is a closed subset of $E$ (note that $E$ is endowed with $\|\cdot\|_\infty$ ) and $E= A_1\cup A_2 \cup A_3 \cup \dots$ .
By Baire's Theorem some $A_n$ contains a closed ball of $E$ . Thus the closed unit ball $B_1(E)$ is contained in some $A_n$ . This implies that $\|\cdot\|_\infty$ and $\|\cdot\|_v$ are equivalent norms on $E$ . Now, I would like to prove that $B_1(E)$ is compact. If you replace for a moment ""of bounded variation"" by ""Lipschitz continuous"", then Arzela-Ascoli could be applied at this point and $\dim E < \infty$ would follow. But a closed and bounded subset $M$ of $C([0,1])$ with the property that $\|f\|_v \le c$ for a constant $c$ and each $f \in M$ is not compact, in general (e.g. $M=\{t \mapsto t^n: n \in \mathbb{N}\}$ ). Thus, in a proof of compactness of $B_1(E)$ the vector space structure of $E$ should play a central role. I can explain by an example what I mean by that: Consider the functions $f_n:[0,1] \to \mathbb{R}$ , $n \ge 2$ defined as $$
f_n(x)=nx ~ (x \in [0,1/n]), ~~ f_n(x)=2-nx ~ (x \in [1/n,2/n]), ~~ f_n(x)=0 ~ (x \in [2/n,1]).
$$ There is a sequence $(n_k)$ such that $\|f_{n_1} + \dots +f_{n_k}\|_\infty$ is uniformly bounded in $k$ and $\|f_{n_1} + \dots + f_{n_k}\|_v \to \infty$ as $k \to \infty$ . Thus $f_n \in E$ $(n \ge 2)$ is impossible. So, I assumed that $\dim E = \infty$ and tried to construct functions in $E$ with $\|f\|_\infty$ small but $\|f\|_v$ big, but didn't succeed. A second idea leading to nowhere up to now was that Helly's First Theorem could help: It's known that each sequence in $B_1(E)$ has a pointwise convergent subsequence. So, three questions: Can somebody finish this proof? Is there a better (easier) proof? Is there an accessible reference with a proof of this theorem? Thanks for any support. Edit: The Lemma mentioned by daw in the comments would indeed finish the proof: Let $(f_n)$ be a sequence in $B_1(E)$ , w.l.o.g. pointwise convergent (Helly). Then it is a Cauchy sequence in $E$ : Otherwise there is some $\varepsilon_0 > 0$ such that $$
\forall n \exists k_n,l_n \ge n: ~ \|f_{k_n}-f_{l_n}\|_\infty \ge \varepsilon_0.
$$ But $f_{k_n}-f_{l_n} \to 0$ pointwise as $n \to \infty$ , hence by the Lemma $\|f_{k_n}-f_{l_n}\|_\infty \to 0$ as $n \to \infty$ , a contradiction. Thus all is reduced to prove the following Lemma : If $(f_n)$ is a bounded sequence in $E$ with $f_n \to 0$ pointwise, then $\|f_n\|_\infty \to 0$ .","['functional-analysis', 'bounded-variation']"
4261677,Solving or estimating $\int_{-\infty}^\infty \frac{\exp(-a/(x^2+b))}{x^2+c}dx$,"Can the integral $$\int_{-\infty}^\infty \frac{\exp(-a/(x^2+b))}{x^2+c}dx$$ be made explicit ( $a,b,c>0$ )? I'm also asking those of you who have access to CAS which can solve it.
In case it can't, is there a very good upper bound? I need a better one than the obvious bound $$\int_{-\infty}^\infty \frac{1}{x^2+c}dx = \frac{\pi}{\sqrt{c}}$$ Thanks a lot!","['integration', 'improper-integrals', 'definite-integrals', 'real-analysis', 'upper-lower-bounds']"
4261735,Some questions about Twistor Space of a closed $4$-manifold,"Let $(M,g)$ be a closed Riemannian manifold of dimension $4$ . We denote its twistor space, the space of almost complex structures on the tangent bundle $TM$ by $Z\xrightarrow{\pi}M$ . At any point the fibre is isomorphic to $S^2$ . Now this can be seen as $S(\Lambda_+^2M),$ i.e., the sphere bundle of the self-dual two forms on $M$ . Self dual means $*\omega=\omega$ . $1.$ I want to know how this description works; i.e., given a self dual two form at a point $m\in M,$ how do we get an almost complex structure on $T_m M?$ $2.$ Is it possible to induce a metric on $Z$ from $M$ , if yes how? $3.$ Once we put a metric on $Z$ and take the corresponding Levi-Civita connection we get a splitting of the tangent bundle in terms of horizontal and vertical vectors: \begin{equation*}
TZ=TV\oplus TH\cong TV\oplus \pi^*(TM)
\end{equation*} Does this give a reduction of the structure group $SO(6)$ to $SO(4)\times SO(2)?$","['riemannian-geometry', 'connections', 'twistor-theory', '4-manifolds', 'differential-geometry']"
4261740,Nonnegativity of the determinant of a commuting matrix,Let $A\in M_n(\mathbb{R})$ such that $A^2=-I_n$ and $AB=BA$ for some $B\in M_n(\mathbb{R})$ . Prove that $\det(B)\geq0$ . All the information I could extract from the relation $A^2=-I_n$ are as follows: $(a)$ $A$ is not diagonalizable. $(b)$ $\det(A)=1$ . $(c)$ $n$ must be even. Now how to conclude that $\det(B)$ is nonnegative using these $3$ informations alongwith $AB=BA$ is not clear to me. Any help is appreciated.,"['matrices', 'linear-algebra', 'contest-math']"
4261820,Questions about infinite dimensional normed spaces,"Problem Show that if $X$ is an infinite-dimensional normed vector space, then there is a sequence $\{x_n\}$ such that $\|x_n\|=1,\|x_n-x_m\|=1\ (n\neq m)$ ． My view I have confirmed that the following two theorems hold. Theorem1 :If $M$ be a close subspace of a normed vector space $X$ which is $X\neq M$ , then for all $\varepsilon>0$ there is $u_{\varepsilon}\in X$ such that $\|u_{\varepsilon}\|=1,d(u,M)>1-\varepsilon$ ． Theorem2 :By Theorem 1, if $M$ is finite dimensional, we can get $u_0\in X$ such that $\|u_0\|=1,d(u_0,M)\geq1$ . I think I will use these theorems, but I can't think of a specific way to do it.
Also, from a geometric point of view, if $X$ is one-dimensional (line), we can get $x_1\in X$ where $\|x_1\|=1$ .
If $X$ is two-dimensional (plane), then by drawing an equilateral triangle with one point at the origin, we can obtain $x_1,x_2\in X$ such that $\|x_i\|=1\ (i=1,2),\|x_1-x_2\|=1$ .
Furthermore, if $X$ is three-dimensional (space), by considering a regular square pyramid with a single point at the origin, we can obtain $x_1,x_2,x_3\in X$ such that $\|x_i\|=1\ (i=1,2,3),\|x_i-x_j\|=1\ (i\neq j)$ .
I was wondering if I could do the same thing in higher dimensions, so that I could do the same thing in infinite dimensional space, but it didn't work. Postscript (Nov. 11, 2021) I still haven't solved this problem yet, so I'll describe another method that I've newly thought of. My view (New) Take $x_1\in X$ such that $\|x_1\|=1$ , and let $M_1$ be a finite-dimensional subspace of $X$ spaned by $x_1$ .
From Theorem 2 , we obtain $x_2\in X$ such that $\|x_2\|=1$ and $d(x_2,M_1)\geq 1$ .
Next, let $M_2$ be a finite dimensional subspace of $X$ spaned by $x_1$ and $x_2$ . From Theorem 2 , we obtain $x_3\in X$ such that $\|x_3\|=1$ and $d(x_3,M_2)\geq 1$ .
Continuing this work, let $M_n$ be a finite dimensional subspace of $X$ spaned by $x_1,\cdots,x_n$ . From Theorem 2 , we obtain $X_{n+1}\in X$ such that $\|x_{n+1}\|=1$ and $d(x_{n+1},M_n)\geq 1$ .
Assume that this task is completed in a finite number of times. In this case, $X$ is a finite dimensional space, which is a contradiction. Therefore, this work can be done infinitely many times.
In this case, $\{x_n\}_{n=1}^{\infty}$ satisfies $\|x_n\|=1\ (n=1,2,\cdots)$ , and from $d(x_{n+1},M_n)\geq 1$ , we can say $1\leq\|x_n-x_m\|$ for $n\neq m$ . My Question Now if I can show that $\|x_n-x_m\|\leq 1\ (n\neq m)$ , I think the proof is done.
Do you have any better ideas?","['normed-spaces', 'vector-spaces', 'functional-analysis']"
4261834,Defining $\frac{\sin x}{x}$ at $x=0$,"This question is based on this post. Here is the question: At $x=0$ , $\frac{\sin x}{x}$ has ____? (Options are maxima , minima , point of inflection , dicontinuity ) I have failed to draw a plausible conclusion from what the answers say and the chat discussion(very interesting) there. Here are some of the points that I wish to clarify: 1) Is it relevant to talk about continuity at a point where a function isn't even defined(first, third answers ( * , *** ) )? 2) How can one extend the domain of the function to remove this discontinuity 3) The original question does not talk about extending the domain of the function, so is answer 2 ( ** ) valid? Pictures of answers for reference: (*) ( ** ) ( *** )","['continuity', 'calculus', 'functions']"
4261876,"If $G$ is abelian and ${\rm Aut}(G)$ is finite, prove that $G$ has finite torsion subgroup.","This is Exercise 4.3.10 of Robinson's ""A Course in the Theory of Groups (Second Edition)"" . According to Approach0 and this search , it is new to MSE. The Details: The torsion-subgroup of an abelian group $G$ is the subgroup of all elements of $G$ of finite order. The Question: Let $G$ be an abelian group such that ${\rm Aut}(G)$ is finite, prove that $G$ has finite torsion-subgroup. Thoughts: Let $G$ be as defined above and $\varphi\in{\rm Aut}(G)$ . I think that, because $\lvert \varphi (x)\rvert=\lvert x\rvert$ , and there are only finitely many permutations of elements of a given order due to $\lvert{\rm Aut}(G)\rvert<\infty$ , there must exist only finitely many elements of finite order. However, I doubt this is as simple as that, and even if the idea is right, I'm not sure how to make the proof more rigorous. Something tells me I'm missing something . . . Please help :)","['automorphism-group', 'torsion-groups', 'finite-groups', 'group-theory', 'abelian-groups']"
4261909,Integrating product of Dirac deltas and step functions,"I have the following integral $$\int d^4\boldsymbol{x}' \,\delta\big[(\boldsymbol{x}-\boldsymbol{x'})^2+\alpha^2\big]\,\Theta(-x_0+x'_0)\,\delta\big[(\boldsymbol{x'})^2+\alpha^2\big]\,\Theta(-x_0'),\tag{1}\label{1}$$ where $\boldsymbol{x}=\{x_0,x_1,x_2,x_3\}$ , the square is $(x')^2=x_\mu'\cdot x'^{\mu}$ , and the metric convention used is $\{-1,1,1,1\}.$ First I wanted to do it in the limit $\alpha\to0$ , where then I used the second dirac delta and theta function to impose $$x_0'=-\sqrt{(x_1')^2+(x_2')^2+(x_3')^2}.$$ Then performing the first integral over $dx_0'$ we would end up with $$\int d^3\boldsymbol{x}'\,\frac{\delta\big[\boldsymbol{x}^2-2x_0\sqrt{(x_1')^2+(x_2')^2+(x_3')^2}-2x^1  x_1'-2x^2  x_2'-2x^3  x_3'\big]}{2\sqrt{2}\sqrt{(x_1')^2+(x_2')^2+(x_3')^2}}\,\times\Theta\bigg(-x_0-\sqrt{(x_1')^2+(x_2')^2+(x_3')^2}\bigg),\tag{2}\label{2}$$ where the denominator came from the expansion of the second dirac delta via $$\int_{\mathbb{R}^n}f(x)\,\delta\big(g(x)\big)dx=\int_{g^{-1}(0)} \frac{f(x)}{|\nabla g|}d\sigma(x).\tag{3}\label{3}$$ This is not a very nice expression to work with so then I tried to go to spherical coordinates, however this ran into some issues. I tried $$\int dr'\int d\phi_1 d\phi_2\,\times\frac{(r')^2\,\text{sin}(\phi_1)}{2\sqrt{2}\,r'}\delta\big[\boldsymbol{x}^2-2r'\big(x_0+x_1 \text{sin}(\phi_1) \text{cos}(\phi_2)+x_2 \text{sin}(\phi_1) \text{sin}(\phi_2)+x_3 \text{cos}(\phi_1)\big)\big]\,\times\Theta\big(-x_0-r'\big),\tag{4}\label{4}$$ I don't think this factor of $r'$ should be there, as the final result should be proportional to ( updated 28/09 ) $\Theta\big(-x_0-\sqrt{x_1^2+x_2^2+x_3^2}\big)$ .
Furthermore, since we normally have $$\delta(\boldsymbol{x}-\boldsymbol{x}_0)=\frac{1}{r^2\text{sin}(\phi_1)}\delta(r-r_0)\delta(\phi-\phi_0)\delta(\theta-\theta_0),\tag{6}\label{6}$$ I am not sure I am even expressing the Dirac delta correctly in \eqref{4}. I am also a bit lost in the difference between the usual coordinate change involving the Jacobian, and the relation \eqref{3}, obviously one is a coordinate change and one is not, but they seem to have very similar effects. So I my main questions are; what is the correct way to transform the expression \eqref{2} into polar coordinates, and am I using \eqref{3} correctly in tandem with the coordinate change?","['integration', 'kinematics', 'quantum-field-theory']"
4262037,Hamiltonian path exists if each two nonadjacent vertices has sum of degrees equal to n-1,"Here is a question from a textbook I’m reading, prove that a simple graph with n vertices has a hamiltonian path if the sum of degree number of each two none adjacent vertices is n-1. I know A graph with n vertices (where n > 3) is Hamiltonian if the sum of the degrees of every pair of non-adjacent vertices is n or greater. I know Dirac's theorem for Hamiltonian graphs tells us that if a graph of order n greater than or equal to 3 has a minimum degree greater than or equal to half of n, then the graph is Hamiltonian How can we use them to achieve the proper answer ?
A hint or an answer would be appreciated. Thank you in advance.","['graph-theory', 'hamiltonian-path']"
4262087,"In precisely what sense is the universal representation of a C*-algebra ""universal""?","Recently I started learning about the universal representation of a C*-algebra $A$ , which is the direct sum the GNS representations corresponding to each state $\rho\in S(A)$ . Let me denote it $\pi_u : A\to\mathcal{B}(H_u)$ . Certainly it's remarkably useful to know that every C*-algebra is *-isomorphic to a norm-closed subalgebra of some $\mathcal{B}(H)$ . However, I have yet to really see in what sense this representation is ""universal"". Why attach that qualifier? The tag ""universal"" suggests this representation has some relationship to all other (faithful?) representations $\pi : A\to\mathcal{B}(H)$ . The wikipedia entry on the universal representation says that if $\pi$ is any other representation, then there exists a ""projection $P$ in the center of $\overline{\pi_u(A)}$ (all closures will be taken wrt the weak operator topology), and a *-isomorphism $\alpha : \overline{\pi_u(A)}P\to\overline{\pi(A)}$ such that $$
\pi(a) = \alpha(\pi_u(a)P)\tag{*}
$$ Wikipedia provides no reference for this result, which annoys me as I'd like to try to understand a few things: Where does this projection $P$ come from? Why is it necessary that it be in the center of $\overline{\pi_u(A)}$ ? Is (*)  the best we can do? The projection $P$ seems crucial, but it also muddles what would've otherwise been a simpler ""universality"" result, akin to the universality of, say, the Stone-Cech compactification. In particular, I was trying to prove that if $\pi_u$ admits a weak expectation (a unital, completely positive contraction $\varphi : \mathcal B(H)\to\overline{\pi_u(A)}$ such that $\varphi(\pi_u(a)) = \pi_u(a)$ for all $a$ ), then $\pi$ also admits a weak expectation. The proof would've been straightforward, if not for the projection $P$ , and I can't see how to ""get around it"". (I know now that you can prove this using Arveson's extension theorem, but $P$ still irks me). Is there a sense in which the universal representation is functorial? Can anyone help me understand what I'm missing?","['c-star-algebras', 'representation-theory', 'functional-analysis', 'operator-algebras']"
4262138,"Range of $\frac{x-y}{1+x^2+y^2}=f(x,y)$","I have a function $\frac{x-y}{1+x^2+y^2}=f(x,y)$ . And, I want to find the range of it. I analyzed this function by plotting it on a graph and found interesting things. Like if the level curve is $0=f(x,y)$ , then I get $y=x$ which is a linear function. But if the level curve is something not 0, then the level curve becomes a circle. And for big values of level curves, the circle disappears. Is there something I can use to find the range of this function?","['algebra-precalculus', 'functions']"
4262180,Proof by induction with inequalities,"Prove $5n + 6 \leqslant n^2$ holds for all $n \geqslant N$ by induction. Here $N$ is the answer you get in (a). For (a) I got $6$ and I proceeded as follows: Base case: $n = 6$ : $5(6)+6 \leqslant 6^22$ , $36 \leqslant 36$ therefore base case is true. Assume $5k + 6 \leqslant k^2$ for $k \geqslant 6$ . Induction: $5(k+1) + 6 \leqslant (k+1)^2$ is true $$5k + 5 + 6  \leqslant k^2 + 2k +1$$ I'm some how confused as to what I need to do next.","['inequality', 'induction', 'discrete-mathematics']"
4262189,When do two triangles reflected over midpoints have the same area?,"Suppose I have a triangle ABC. I have points C', A', and B' on segments AB, BC, and CA, respectively. Suppose I reflect C' about the midpoint of AB to get point C'' (also on AB); similarly for the other two points to get points A'' and B''. Is the area of A'B'C' the same as the area of A''B''C''? I tried some programming and it looks like it is. How can I prove this? I tried proving this showing that Area(AC'B')+Area(BA'C')+Area(CA'B') = Area(AC''B'')+Area(BA''C'')+Area(CA''B''). I did this by saying that AB has length c, BC has length A, and CA has length b. Then I decided to show that the sum of the areas of the three outside triangles of each triangle is the same. For instance the area of AC'B' is $1/2 (1/2 c - \delta_{B'})(1/2 b - \delta_{C'})\sin A$ , where $\delta_{B'}$ is the distance of $B'$ from the midpoint of AC. I wrote out all of the areas of the triangles like this, but I cannot show that they are the same.","['hypothesis-testing', 'geometry', 'linear-algebra', 'physics', 'trigonometry']"
4262190,"Various ways to calculate $\int \sin(x) \cos(x) \, \mathrm{d}x$","Consider the integral $$\mathcal{I} := \int \sin(x) \cos(x) \, \mathrm{d} x$$ $
\newcommand{\II}{\mathcal{I}}
\newcommand{\d}{\mathrm{d}}
$ This is one of my favorite basic integrals to think about as an instructor, because on the face of it, there are a lot of different ways to solve it, many are accessible to Calculus I students, and can give some insights into the nature of integration and to some trigonometry identities. For instance: Substitution with $u = \sin(x)$ gives $$\II =  \frac{\sin^2(x)}{2} + C$$ Substitution with $u = \cos(x)$ gives $$ \II = - \frac{\cos^2(x)}{2} + C$$ (Noted in comments by Koro) Make the substitution $$
u = \sec(x) \implies \d u = \sec(x) \tan(x) \, \d x= \frac{\sin(x)}{\cos^2(x)} \, \d x$$ Then $$\begin{align*}
\II &= \int \sin(x) \cos(x) \frac{\cos^2(x)}{\sin(x)} \, \d u\\
&= \int u^{-3} \, \d u\\
&= - \frac{1}{2\sec^2(x)} + C\\
&= - \frac{\cos^2(x)}{2}  + C
\end{align*}$$ A similarly motivated substitution: $$
u = \csc(x) \implies \d u = -\cot(x) \csc(x) \, \d x = - \frac{\cos(x)}{\sin^2(x)} \, \d x
$$ yields $$\begin{align*}
\II &= -\int \sin(x) \cos(x) \frac{\sin^2(x)}{\cos(x)} \, \d u\\
&= -\int u^{-3} \, \d u\\
&=  \frac{1}{2\csc^2(x)} + C\\
&= \frac{\sin^2(x)}{2}  + C
\end{align*}$$ Using $\sin(2x) = 2 \sin(x) \cos(x)$ readily leads to $$\II = -\frac{\cos(2x)}{4} + C$$ Integration by parts differentiating $\sin(x)$ yields $$\II = \sin^2(x) - \II$$ which will yield a previous solution on solving for $\II$ . Integration by parts differentiating $\cos(x)$ yields $$ \II = -\cos^2(x)- \II$$ which, similarly, yields a previous solution once we solve for $\II$ . Using the Weierstrass substitution $t = \tan(x/2)$ gives $$
\II = \int \frac{2t}{1+t^2} \frac{1-t^2}{1+t^2} \frac{2}{1+t^2} \, \mathrm{d}t = \frac{2t^2}{(1+t^2)^2} + C=\frac{2 \tan^2(x/2)}{(1 + \tan^2(x/2))^2} + C
$$ We can use the complex sine and cosine: $$
\sin(x) = \frac{e^{ix} - e^{-ix}}{2i} \qquad
\cos(x) = \frac{e^{ix} + e^{-ix}}{2}
$$ Then $$
\II = \int \frac{e^{2ix} - e^{-2ix}}{4i} \, \mathrm{d} x = - \frac 1 8 \left( e^{2ix} + e^{-2ix} \right) + C = -\frac 1 4 \cos(2x) + C
$$ ( Warning for Novices: The $C$ constant in each expression may not be the same as in others. These answers are equivalent ones for the integral, but the solutions without the $+C$ terms are not equal expressions. These hint at certain trigonometry identities, which is why I find them interesting.) This has given us a set of solutions to $\II$ via a few basic methods, and a few less-basic but accessible methods. My question is, what other solutions can you come up with for $\II$ ? I'm particularly interested in answers which: are obviously not functionally equivalent to those already given give answers other than those already expressed (perhaps a hint at other identities or concepts of note?) use methods that you don't see in a typical calculus class, or methods that are rarely used use unusual but slick and effective techniques or any combination thereof! I have no real motivation for this but my own curiosity, but I'm curious to see what you guys can think of.","['integration', 'big-list', 'calculus', 'trigonometric-integrals', 'indefinite-integrals']"
4262245,What is the exact relationship between $E(Y\mid X=x)$ and $E(Y\mid X)$?,"I know that $E(Y\mid X)$ is a random variable of $X$ , while $E(Y\mid X=x)$ is a value. However, when they both appear in a problem like the one below, I cannot tell the exact relationship between them. Problem: Assume that $(X,Y)$ has a jointly probability density function $$f(x,y)=
\begin{cases}
\frac{21}{4}x^2y, & \text{if } x^2 \le y \le 1, \\ 
0, & \text{else}.
\end{cases}$$ (1) Show the probability density function of $X$ , i.e. $f_X(x)$ ; (2) Fixed $x\in (-1,1)$ , show the conditional expectation of $Y$ given $X=x$ , i.e. $E(Y\mid X=x)$ ; (3) Show $E(Y|X)$ . I have finished the first 2 subproblems: (1) $\displaystyle
f_X(x)
=\int_{-\infty}^{\infty}f(x,y)\,dy
=\ldots
=\begin{cases}
\frac{21}{8}x^2(1-x^4), & \text{if } x^2 \le 1,\\ 
0, & \text{else}.
\end{cases}$ (2) $\displaystyle
f_{Y|X=x}(y|x)
=\frac{f(x,y)}{f_X(x)}
=\begin{cases}
\frac{2y}{1-x^4}, & \text{if } x^2 \le y \le 1 \text{ and } x \in (-1,0)\cup(0,1),\\ 
0, & \text{else}.
\end{cases}$ $E(Y\mid X=x)=\int_{-\infty}^{+\infty}yf_{Y\mid X=x}(y\mid x)dy=\cdots=\frac{2}{3}\frac{1+x+x^2}{(1+x)(1+x^2)}, x\in (-1,0)\cup(0,1)$ I'm confused with the relationship between $E(Y\mid X=x)$ and $E(Y \mid X)$ , especially when the definition domain is something like the things here. Also, though I finished the subproblem (2), I still cannot understand how the $x=0$ disappears and what the meaning behind it is . Please help me, thanks a lot!","['conditional-probability', 'conditional-expectation', 'probability-theory', 'probability', 'density-function']"
4262247,"Calculate the standard deviation from confidence interval, sample size, and mean, when confidence interval is not symmetric around mean","I have the following data: 300 customers were asked to participate in a survey, and each customer was able to submit his/her response within the first two weeks. The mean time to respond to the survey was 107 hours. Using this data, a statistician was able to conclude that the mean time to respond to the survey for future customers would be between 96.12 and 120.65 hours. Assuming that this is 95% confidence interval, how can we figure out the type of distribution and the standard deviation from this data? Thanks in advance for your help! PS: I tried to use t-statistic, but realized that the 95% confidence interval (96.12,120.65) is not symmetric by the mean (107). This means that the data is not normally distributed.","['statistical-inference', 'statistics', 'confidence-interval', 'standard-deviation', 'means']"
4262266,When a local isometric immersion become global?,"I have a question that I have not been able to answer clearly. When a local isometric immersion become global? For example, I know the following result, if $\phi:M\to N$ be a differentiable mapping and $g$ a metric in $N$ , then $\phi^*g$ is a metric in $M$ if and only if $\phi$ is a immersion (isometric). But that only assures me a local isometric immersion, for example of a pseudosphere and the hyperbolic plane $\mathbb H^2$ , as only one horocircle can be immersed but not the entire hyperbolic plane. But in turn, for example Rozendorn uses this same fact to prove that $\mathbb H^2$ is isometrically immersed in $\mathbb R^5$ with a non-injective immersion. In his article he only explicitly gives such a immersion and the idea is to use the result above, after that he concludes that it is a global immersion. I feel a bit tied in my hands with this and I don't know how to get out. I would appreciate some clarification, thank you. I thought that the definition of immersion was only one, I'm sorry if there are more so:
A differentiable map $\phi:M\to N$ is an immersion if the differential $d\phi_p$ is non-singular for all $p$ .","['isometry', 'hyperbolic-geometry', 'riemannian-geometry', 'differential-geometry']"
4262291,"proof of injection, surjection and bijection of identity functions","Suppose there are two functions $$ f: X \to Y $$ and $$ g: Y \to X$$ and $ g \circ f = id_{X}$ Proof or disproof the following: a) f is injectiv b) f is surjectiv c) g is injective d) g is surjective e) $f\circ g = id_{Y}$ After some research i was able to do a and b myself, however i stuck on c and d because in my head we can just use the answers from a and b and change the domain and codomain and some symbols to get the answers, basically should be the same as a and b. But i thought if it were that easy why would the professor give us the questions to do....So could someone tell me how to proceed or if there is a trick at all? Would be really great if you could give me one example proof of c and d so that i can check whether my answers are correct!! NEW EDIT: I just found some older posts with similar questions where people said f is injective and g is surjective...i thought they were both bijective???? completely confused,,,would really appreciate some help clearing this thing. Thanks a lot","['elementary-set-theory', 'proof-explanation', 'functions']"
4262297,"Is the space $H(\operatorname{curl},\Omega)$ stable under the action of symmetric definite positive matrix?","$\newcommand{\curl}{\operatorname{curl}}$ Let $M$ be a $3 \times 3 $ constant real symmetric positive definite matrix, $\Omega\subset \mathbb{R}^3$ a bounded lipschitz domain and define: $$
H(\curl,\Omega)=\{u\in (L^2(\Omega))^3\,|\,\nabla\times u\in (L^2(\Omega))^3 \}
$$ where $(L^2(\Omega))^3$ is the space of square integrable functions on $\Omega$ .
Let $u$ be a function such that $M\nabla\times u\in H(\curl,\Omega)$ .
Then, does we have $\nabla\times u\in H(\curl,\Omega)$ ? If $M\nabla\times u\in H(\curl,\Omega)$ , then there exists $v\in H(\curl,\Omega)$ such that $M\nabla\times u=v$ . The matrix $M$ is invertible, so we have $\nabla\times u= M^{-1}v$ which is in $(L^2(\Omega))^3$ . But, how to check if $\nabla\times\nabla\times u=\nabla\times  (M^{-1}v)$ is in $(L^2(\Omega))^3$ or not?","['matrices', 'multivariable-calculus', 'sobolev-spaces', 'functional-analysis', 'differential-geometry']"
4262397,Matrix whose spectral moments (traces of its powers) follow a geometric progression,"I am looking for a square $n \times n$ matrix $A$ such that its spectral moments follow a geometric progression: $Tr(A^r) = k^r$ , for some $k$ . I believe this to be impossible since $Tr(A^0) = Tr(I) = n$ . So perhaps I should be looking for something like $$
Tr(A^r) = n^{r+1},
$$ or something like $$
Tr(A^r) = nk^{r}.
$$ I'm interested in complex or real matrices, though other fields would be acceptable. Apologies if the question sounds vague, but I am dealing with an open ended question here! What I have tried: I know that the coefficients of the characteristic polynomial can be expressed in terms of the spectral moments using Newton's identities . In this way, I can fix the traces of $A,\ldots,A^n$ to be whatever I want, find the corresponding polynomial, and find a matrix whose characteristic polynomial matches it. However, this does not give me any control on the spectral moments of powers greater than $n$ . So I'm beginning to wonder if this is at all possible. Also note that for $n=1$ this is trivial since $A$ is just an element of the underlying field and we have $Tr(A^r) = A^r$ . I'm interested in $n>1$ . EDIT: if possible, I would also like $A$ to be invertible...","['matrices', 'characteristic-polynomial']"
4262513,How to solve limit of $(n-1)\left[1-\left(1-\left(\frac{\lambda}{n}\right)^2\right)^{n-1}\right]$?,"I'm trying to find the solution to the following limit: $$
\lim_{n\rightarrow\infty}(n-1)\left[1-\left(1-\left(\frac{\lambda}{n}\right)^2\right)^{n-1}\right]\ \mathrm{.}
$$ I've tried decomposing the limit and writing it as $$
\lim_{n\rightarrow\infty}(n-1)\left[1-\left(1-\frac{\lambda}{n}\right)^{n}\left(1+\frac{\lambda}{n}\right)^n\left(1-\left(\frac{\lambda}{n}\right)^2\right)^{-1}\right]\ \mathrm{,}
$$ so I can use that $$
\lim_{n\rightarrow\infty}\left(1\pm\frac{\lambda}{n}\right)^{n}=e^{\pm\lambda}\mathrm{,}
$$ but the problem seems to be that I cannot split the products or the sums within the limit into separate limits as $\lim_{n\rightarrow\infty}(n-1)$ does not converge. The answer should be $\lambda^2$ (this is also what Wolfram gives me), but I don't see a way to get to that answer myself. I'd rather not resort to the Laurent series, as this is also a nightmare to compute. Can anyone help?","['limits', 'calculus']"
4262514,Opening up a cone,"Refer here Consider the cone in the white rectangle, suppose it is cut through AO, what will the image look like when the cut surface is flattened onto a plane? The answer is the right figure of $OAA$ , but when I try to imagine the cut, my mind tells me it looks something like a PAC-man mouth Both cases seem to look like a sector of a circle but I can't seem to figure out what factors is controlling the central angle which is being subtended. Is there some way to relate cone parameters to the angle subtended once the cone flattened?","['general-topology', 'geometry']"
4262542,Does eigenvector of Stokes operator are eigenvectors of the Laplacian?,"The Stokes operator is defined by $Au=-P_H\Delta u$ , where $u\in D(A)=H^2(\Omega)\cap V$ and $P_H$ is the Leray's projector. From Spectral Theorem, we can obtain an orthonormal basis of eigenvectors $(w_i)_{i\in \mathbb{N}}$ of H, i.e., $$Aw_j=\lambda_iw_i,\ \forall i\in\mathbb{N}.$$ I was reading a paper where the authors uses the following property $$-\Delta w_i=\lambda_i w_i,\tag{1}\label{1}$$ i.e, $w_i$ is an eigenvector of the Laplacian too. Someone told me that's because $P_H$ and $-\Delta$ commutes for the eigenvectors. First I would like to know if they commutes for the eigenvectors of the Stokes operator and why. If they don't, I would like to know if \eqref{1} is true. I will be satisfied with a good reference, I was not able to find one. I've read they do not commute, in general. In the absence of boundaries, for example, they commute, but I couldn't find anything with respect to the eigenvectors.","['fluid-dynamics', 'functional-analysis', 'partial-differential-equations']"
4262545,What is the probability that the sum of six rolls of a fair die is divisible by $7$?,"$$P(E)=\frac{\text{Favourable outcomes}}{\text{Total outcomes}}$$ Total outcomes $=6^6$ Favourable outcomes means the sum must be $7$ , $14$ , $21,28$ or $35$ Assume $a_1$ , $a_2$ , $a_3$ , $a_4$ , $a_5$ , $a_6$ to be the numbers on the top faces of the dice In case of $7$ and $35$ the number of cases should be $6$ as $0<a_i<7$ . When the sum is $14$ , favourable outcomes are: $${14 \choose 6}-6\left({7 \choose5}+{6 \choose 5}+1\right).$$ I tried this by a variation of beggars method. Let's assume there are $14$ coins, and $6$ beggars. There are $14$ places for the beggars to choose, such that there are $13$ places between two coins and one to the left of the first coin. Each beggar gets all the coins between himself and the beggar just to the right of him. This way we ensure that each beggar at least gets one coin. After this I subtracted the number of cases where one beggar gets more than one coin. Now this is solvable up to this point, but I'm getting a very large equation when I do this for $21$ and $28$ . Is there a better method, since I will most probably only have $5$ min in the upcoming exam on $3^d$ October.","['combinatorics', 'probability']"
4262569,"How to prove that the set $S:=\{(u,v,\sin(u)\cos(v)):u,v \in [a,b]\}$ is connected?","Exercise:
Given the Set $S:=\{(u,v,\sin(u)\cos(v)):u,v \in [a,b]\}$ show that $S$ is a connected Set. From a intuitive viewing point the connectedness is obvious.
My first thought was to use the Definition of connectedness but I tried something else first: My idea is to use the fact that for a continuous function $f:X \rightarrow Y$ and a connected set $X$ the set $f(X)$ is also connected. So one could write the Set S as parametrization $f(x,y)=(x,y,\sin(x)cos(y))$ $f$ is obviously continuous
and $f([a,b]^2)=S$ Now $[a,b]$ is path connected so, in particular the set $S$ is connected. Are my calculations correct?
Is there another way to show it? I am especially interested in a more ""topological"" prove. Hope someone could help me out with that.","['general-topology', 'analysis']"
4262579,"$\int_{\cup_i A_i} X \;d\mu =\sum_i \int_{A_i} X \,d\mu$ for $X$ integrable?","Let $(\Omega,\mathcal{F},\mu)$ be a measure space, $X:\Omega \to \mathbb{R} $ integrable and $(A_i)_{i \in I}$ ( $I$ countable indexset) pairwise disjoint sets in $\mathcal{F}$ . Does then $$\int_{\cup_i A_i} X \;d\mu =\sum_i \int_{A_i} X \,d\mu$$ holds? Normally this follows from the monotone convergence theorem. But since $X$ is not non-negative we can't use it here. Is it instead possible to argue with the dominated convergence theorem? Attempt: Let $i_1, i_2, \dots$ be a numbering of the elements of I. Define $f_n:=\sum_{j=1}^{n}X\;\mathbb{1}_{A_{i_j}}$ . Then $f_n(\omega)$ converges to $f(\omega):=\sum_{i=1}^{\infty} X \; \mathbb{1}_{A_{i_j}}(\omega)$ for all $\omega \in \Omega$ . Since $f,f_n$ are measurable and $|f_n|\leq |X|\sum_{i=1}^{n}\mathbb{1}_{A_{i_j}}\leq |X|$ for all $n \in \mathbb{N}$ and $|X|\in \mathcal{L}^1,$ all conditions of the dominated convergence theorem are given. It follows that $$\int_{\cup_i A_i} X \;d\mu = \int \mathbb{1}_{\cup_i A_i} \; X\; d\mu=\int \sum_{i=1}^{\infty} \mathbb{1}_{A_{i_j}} \;X \,d\mu=\int f_  \,d\mu=\lim_{n \to \infty} \int f_n \; d\mu=\lim_{n \to \infty}\sum_{i=1}^{n}\int \mathbb{1}_{A_{i_j}}\; X \;d\mu=\sum_{i=1}^{\infty}\int_{A_{i_j}} X \;d\mu$$","['measure-theory', 'lebesgue-integral']"
4262587,Computing the limit at infinity for multivariable function,"I seem to be having problems understanding the epsilon-N definition of limits when the function takes multiple variables. For example, consider the limit $\lim_{(x,y) \rightarrow (\infty, \infty)} xe^{-y}$ , which has come up in my stats homework. My hunch is that this limit should converge to $0$ , as this yields the correct answer and the graph seems to ""flatten out"" in general when looking far away in the first quadrant. Yet, I can neither confirm nor disprove this guess since I cannot find the definition of limits of multivariable functions at infinity. The only definition I could find are those at finite points, in which case a direct generalization of $\epsilon-\delta$ definition for single variable functions could be applied. Could somebody please explain the rigorous definition of limits at infinity? Also, if possible, could you confirm or disprove my guess about $\lim_{(x,y) \rightarrow (\infty, \infty)} xe^{-y}$ ? Thanks very much.","['limits', 'multivariable-calculus', 'definition', 'epsilon-delta']"
4262593,Why some solution are excluded in solving equality involving absolute value function?,"For example I have to solve this equation $$|x-1|+|2x+4|=5.$$ I have been taught that I have to make an table and find the cases in which $x-1$ and $2x+4$ are going to be positive and negative, both negative for the interval $(-\infty;-2)$ ; first negative and second positive for $(-2;1)$ ; and both positive for $(1;+\infty)$ in this case, and then solve for $x$ in each case and you get three results: $-8/3$ for $x \in (-\infty;-2)$ , $0$ for $x \in (-2;1)$ and $2/3$ for $x \in (1;+\infty)$ . Here is what I don't understand. Why only $-8/3$ and $0$ are supposedly solutions and $2/3$ is not? I was told its because $x \notin (1;+\infty)$ . But that doesn't really make sense for me, can anyone explain this in more detail?","['algebra-precalculus', 'absolute-value']"
4262598,Maximum size of antichain-like collection,"Lets say we have a set $X$ of $n$ elements and a family $S\subseteq\mathcal{P}(X)$ of subsets such that no set $A\in S$ is the union of some of the others, that is, $$\forall S'\subseteq S\setminus\{A\}\quad\text{we have}\quad A\ne\bigcup_{B\in S'}B$$ what is the maximum size of such a collection $S$ in terms of $n$ ? Are there estimates/proofs similar to that of Sperner's theorem? Thanks!","['elementary-set-theory', 'combinatorics', 'upper-lower-bounds']"
4262647,Analytification of a smooth projective variety is a compact Kähler manifold.,"I am reading “Fourier-Mukai transforms in algebraic geometry” by Daniel Huybrecht. On page 130 it is written that by Hodge theory there is a natural direct sum decomposition $$H^n(X,\mathbb{C})=\bigoplus_{p+q=n}H^{p,q}$$ with $\overline{H^{p,q}}=H^{q,p}$ and moreover $H^{p,q}\cong H^q(X, \Omega^p)$ where here $X$ is the analytification of a smooth projective variety over $\mathbb{C}$ . I am agree that by Hodge decomposition, we have this for a compact Kähler manifold. So, if the analytification of a smooth projective variety over $\mathbb{C}$ is a compact Kähler manifold, we should have this decomposition. My question is that is this true and if not why do we have this decomposition?","['hodge-theory', 'algebraic-geometry', 'kahler-manifolds', 'projective-varieties']"
