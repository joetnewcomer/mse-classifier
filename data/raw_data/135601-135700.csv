question_id,title,body,tags
2147380,Evaluate the given limit,Given a function $f : R → R$ for which $|f(x) − 3| ≤ x^2$. Find $$\lim_{ x\to0}\frac{f(x) - \sqrt{x^2 + 9}}{x}$$ Can  the function $f(x)$  be considered as  $x^2 + 3$  and go about evaluating the limit using the Limit laws?,"['calculus', 'limits']"
2147387,Independence of two F-distributed random variables,"Suppose that $U$ and $W$ are subspaces of $\mathbb{R}^n$ such that $U \subseteq W$, with respective dimensions $m = \dim(U)$, $s=\dim(W)$.
Denote the (perpendicular) projection matrices onto $U$ and $W$ as ${\bf P}_U$ and ${\bf P}_W$, respectively. Let us also define the orthogonal complement $W^\perp$, with projection matrix ${\bf P}_W^\perp = {\bf I} - {\bf P}_W$. Then the subspace $Z = W \cap U^\perp$ has projection matrix ${\bf P}_Z = {\bf P}_W - {\bf P}_U$. Let ${\bf x} \in \mathbb{R}^n$ be normally distributed with zero mean and covariance ${\bf I}_n$. One has 
$$ \|{\bf x} \|^2 =  \|{\bf P}_U {\bf x} \|^2 + \|{\bf P}_Z{\bf x} \|^2 + \|{\bf P}_W^\perp{\bf x} \|^2,$$
and each of the three terms in the right-hand side is chi-square distributed with $m$, $s-m$ and $n-s$ degrees of freedom, respectively; moreover, they are statistically independent (as per Cochran's Theorem). Intuitively, this is due to the fact that the subspaces $U$, $Z$ and $W^\perp$ are pairwise orthogonal. My question is the following. Consider the random variables
$$ r_1 = \frac{ \|{\bf P}_W{\bf x} \|^2}{\|{\bf P}_W^\perp {\bf x} \|^2} \sim F_{s,n-s}
 \qquad \mbox{and} \qquad r_2 = \frac{ \|{\bf P}_U{\bf x} \|^2}{\|{\bf P}_Z{\bf x} \|^2} \sim F_{m,s-m}. $$
Are they statistically independent? Empirical evidence is supportive, and I conjecture that they are,  but I have been unable to obtain a formal proof. Note that $r_1$ is the ratio of the energy of $\bf x$ in $W$ to that in $W^\perp$ (its orthogonal complement in  $\mathbb{R}^n$), whereas $r_2$ is the ratio of the energy of $\bf x$ in $U\subseteq W$ to that in $Z$, which is its orthogonal complement in  $W$. I think the question is of independent interest, but in any case, I arrived at this from a hypothesis testing problemin which, on one hand, I test $H_0: {\bf x} \in W$ vs. $H_1: {\bf x} \notin W$ (with test statistic $r_1$) and on the other, $H_0': {\bf P}_W{\bf x} \in U$ vs. $H_1': {\bf P}_W{\bf x} \notin U$ (with test statistic $r_2$). Thanks in advance!","['statistics', 'probability', 'normal-distribution', 'random-variables']"
2147391,Adjoint Norms in Banach Space,"If $T:X\to Y$ is a bounded linear transformation of Banach spaces $X$ and $Y$ , then there is an adjoint transformation $Y^*\to X^*$ that satifies $\langle Tx,y^* \rangle =\langle x,T^{*}y^* \rangle$ for all $x\in X$ and $y^*\in Y^*.$ One standard result is that $\left \| T \right \|=\left \| T^{*} \right \|.$ I have seen several proofs of this fact but I have a question about Rudin's. Rudin writes the following sequence of equalities: $\left \| T \right \|=\sup \left \{\langle Tx,y^{*} \rangle:\left \| x\leq 1 \right \|,\left \|  y^{*}\le 1 \right \| \right \}=\\\sup \left \{\langle x,T^*y^{*} \rangle:\left \| x\leq 1 \right \|,\left \|  y^{*}\le 1 \right \| \right \}=\\ \sup \left \{T^*y^{*}:\left \| y^{*}\le 1 \right \| \right \}=\left \| T^{*} \right \|$ which I take to mean: $\left \| T \right \|=\sup_\left \{ \left \| x \right \|\le 1 \right \}(\sup_\left \{ \left \| y^* \right \|\le 1 \right \}\left \{\langle Tx,y^{*} \rangle  \right \})=\\ \left \| T \right \|=\sup_\left \{ \left \| x \right \|\le 1 \right \}(\sup_\left \{ \left \| y^* \right \|\le 1 \right \}\left \{\langle x,T^*y^{*} \rangle  \right \})=\\ \left \| T \right \|=\sup_\left \{ \left \| y^* \right \|\le 1 \right \}(\sup_\left \{ \left \| x \right \|\le 1 \right \}\left \{\langle x,T^*y^{*} \rangle  \right \})=\left \| T^* \right \|.$ My question is simple: what justifies the interchange of the suprema? The way Rudin writes it, he seems to be claiming in general that that if $f\in \mathscr F, $ and $x\in X$ , then $\sup_{\mathscr F}\sup_{X}f(x)=\sup_{X}\sup_{\mathscr F}f(x), $ which is intuitive enough, but I have not been able to prove it.","['functional-analysis', 'real-analysis']"
2147402,Why doesn't this prove that Cantor's Diagonal argument doesn't work?,"Let us assume that construction of a number, $z$, which cannot be indexed (or counted) in $\mathbb{N}$, proves by contradiction that the cardinality of the set containing $z$ is greater than $|\mathbb{N}|$ . Then for any set $S$ that contains $z$, for which there is no bijection with $\mathbb{N}$, $|S| \gt |\mathbb{N}|$. → (This is the basis for Cantor's Diagonal argument, right?) Now let’s examine a subset of $\mathbb{N}$ in which numbers start with 1, and are non-repeating infinite sequences of integers: $S =$ { non-repeating infinite integer sequences beginning with number 1 } → (the reason i'm starting with 1 is to prevent cases where a majority of the leading digits are 0, and to create a similar situation where we start with 0.xxx) There are infinitely many non-repeating infinite integer sequences that start with 1, just as there are infinitely many non-repeating infinite integer numbers that start with 2, 3, 4 etc. → (isn’t this true?) Elements in this set $S$ can be represented as $1d_1d_2d_3d_4$…. where the sequence of digits $d_i$ is infinite and non-repeating. An infinite subset of $\mathbb{N}$, here called $S$, and the parent set $\mathbb{N}$ have the same cardinality (e.g. odd numbers and even numbers), so there should exist a bijection between $S$ and $\mathbb{N}$. → (isn’t this true?) The elements of $S$ can be listed as $s_1$, $s_2$, $s_3$… and while we cannot tell exactly which number from $S$ got to be the first one, or the second... we do know that every element in $S$ must get an index from $\mathbb{N}$ Let’s construct the number $z$ that will be in $S$, but will differ from each and every number that has received an index. The number $z$ will have the form $1e_1e_2e_3e_4$… where the $i$th digit past the leading 1 in $z$ will differ from the $i$th digit of $s_i$. We can do this by setting $e_i = s_{ii}+1 (mod 10)$ where $s_{ii}$ is the $i$th digit of $s_i$. Now, $z$ is in $S$, but it will not receive an index in $\mathbb{N}$. If it did get one, say $k$, then we have $z = s_k$, but the $k$th digit of $z$ is, by construction, different from the $k$th digit of $s_k$, which is a contradiction. Hence our assumption is incorrect. (here's the core of the Diagonal argument) This means that either… a) Cardinality of a subset of $\mathbb{N}$ (here $S$) is somehow larger than $|\mathbb{N}|$? b) The initial assumption that construction of $z$ proves $S$ has a greater 
cardinality than $\mathbb{N}$ is incorrect? c) I’m missing something else…","['elementary-set-theory', 'proof-verification']"
2147411,Is $f$ continuous at zero?,"$$\require{cancel}$$ $$f(x) =
\begin{cases}
\frac{\sin x}{|x|} &\text{ if }x \neq0
\\
\hspace{0.3cm}1 &\text{ if }x=0.
\end{cases}$$ My Attempt 1)$$\lim_{x\rightarrow0}\frac{\sin x}{|x|} = \lim_{x\rightarrow0}\frac{\sin x}{x} \frac{x}{|x|} = 1\lim_{x \rightarrow0}\frac{x}{|x|} 
\\$$
2)$$\lim_{x\rightarrow0^{-}}\frac{x}{|x|}=-1 \hspace{0.3cm}\text{and}\hspace{0.3cm} \lim_{x\rightarrow0^{+}}\frac{x}{|x|}=1
$$
Therefore:
$$1\lim_{x\rightarrow0}\frac{x}{|x|}=DNE
$$
so, $f$ is not continuous at $0$. My question is does my solution actually prove that $f$ is not continuous at $0$? or is it continuous at zero because $f(x)=1$ when $x=0$?","['continuity', 'calculus', 'limits']"
2147445,Connection between dual maps and bilinear forms?,"Say $\xi$ is a linear map from $V$ to $W$ . Then its dual map is the map $\xi^\star:W^\star \to V^\star$ such that $g\mapsto \xi^\star(g) = g\circ \xi \doteq f$ . If $A_\mathcal V^\mathcal W$ is the matrix associated to $\xi$ in the bases $\mathcal V$ of $V$ and $\mathcal W$ of $W$ , then its transpose $(A_\mathcal V ^ \mathcal W)^T$ is the matrix associated to $\xi^\star$ in the dual bases $\mathcal W^\star$ and $\mathcal V^\star$ . This also means that if $V$ is one-dimensional, the matrix associated to $\xi^\star$ is a row vector (covector), the transpose of a column vector in $\mathbb K^m$ . Now suppose we have a bilinear form $\phi : V\times W \to \mathbb K$ . Then I can represent it through the matrix $$\Phi =  \begin{bmatrix}\phi(v_1,v_1) & \cdots & \phi(v_1,w_m) \\ \vdots & \ddots & \vdots \\ \phi(v_n,w_1) & \cdots& \phi (v_n,w_m) \end{bmatrix}$$ where the $v_i$ 's are the elements of the basis $\mathcal V$ and the $w_j$ 's are the elements of the basis $\mathcal W$ . If $x=(x_1,\dots,x_n)$ and $y=(y_1,\dots,y_n)$ are the coordinates of $v$ and $w$ respectively in the bases $\mathcal V$ and $\mathcal W$ , then I can write $\phi(v,w) = x^T\Phi y$ . This last step is justified in my notes by means of the proposition: Proposition. The bilinear form $\phi$ is completely determined by the $nm$ scalars $a_{ij} \doteq \phi(v_i,w_j)$ . Proof. We can write $v = \sum_{i=1}^n x_iv_i$ and $w = \sum_{j=1}^m y_jw_j$ , so, by iterating bilinearity, we have $$\phi(v,w) = \phi\left(\sum_{i=1}^n x_iv_i,\sum_{j=1}^my_jw_j\right) = \sum_{i,j} a_{ij} x_iy_j $$ The last term corresponds to the expansion of $x^T\Phi y$ , but this seems to fall relatively out of the blue. Indeed, seeing that a row vector is involved would have one think that some dual map is involved – and when learning about matrix congruence, this involvement seems even more evident. Yet nowhere in my notes is the connection directly mentioned and clarified (the two concepts belong to separate chapters). So, is there a way to justify the transpose formula in terms of dual spaces? In other words, does this way of representing bilinear forms (once the bases are chosen) conceal a deeper connection to the concept of dual spaces and dual maps?","['bilinear-form', 'linear-algebra', 'duality-theorems']"
2147449,Identify this Derivation Rule,"$$\frac{d(\ln(f(t))}{dt} = \frac{f'(t)}{f(t)}$$ A Finnish introductory book on differential equations uses the above formula without explanation, simply claiming ""it is known"". What is this equation and why does it hold true for all f ? I went through the entire ""derivation rules"" wikipedia page and could not find it. Searching for the specific formula is hard due to markdown.","['derivatives', 'ordinary-differential-equations']"
2147458,Solve $ \frac{2}{\pi}\int_{-\pi}^\pi\frac{\sin\frac{9x}{2}}{\sin\frac{x}{2}}dx $ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Solve the following integral:
$$
\frac{2}{\pi}\int_{-\pi}^\pi\frac{\sin\frac{9x}{2}}{\sin\frac{x}{2}}dx
$$",['integration']
2147477,Show that the set $A=\{x\in \mathbb{Q}^+| x^2>2\}$ has no least element in $\mathbb{Q}$,"Show that the set $A=\{x\in \mathbb{Q}^+| x^2>2\}$ has no least element in $\mathbb{Q}.$ I assume that $y=\inf A=$ least element. Then $y\in A$ or $y$ does not belongs to A. Case I: Let $y\in A$; Considering $k=\frac{y^2-2}{2y}$, I have shown $y-k \in \mathbb{Q}^+$ (as $y-k>0$ and rational). Also $(y-k)^2>2$ implying $y-k\in A$. But $y-k<y$ and $y=\inf A$, a contradiction. So $A$ has no least element in this case. Case II: Let $y$ that does not belongs to $A$. Then either $y^2=2$ or $y^2<2$. My questions: Please help me to complete case II. Is there any other way to solve it?","['real-analysis', 'real-numbers', 'calculus']"
2147486,Explicit components of $y^x=x^y$ [duplicate],"This question already has answers here : Given that $x^y=y^x$, what could $x$ and $y$ be? (5 answers) Closed 7 years ago . When graphing the implicit equation $y^x=x^y$ it seems like it consists of two possibly explicit curves. On the one hand there is the linear solution $y=x$. But as it appears there is a second curve monotonously decreasing as x increases, intersecting with the curve $y=x$ at coordinate point $(2,2)$. Is there any way to find out the explicit form of this other curve?",['calculus']
2147519,How can we show that $\sum_{n=0}^{\infty}{2n^2-n+1\over 4n^2-1}\cdot{1\over n!}=0?$,"Consider $$\sum_{n=0}^{\infty}{2n^2-n+1\over 4n^2-1}\cdot{1\over n!}=S\tag1$$
  How does one show that $S=\color{red}0?$ An attempt: $${2n^2-n+1\over 4n^2-1}={1\over 2}+{3-2n\over 2(4n^2-1)}={1\over 2}+{1\over 2(2n-1)}-{1\over (2n+1)}$$ $$\sum_{n=0}^{\infty}\left({1\over 2}+{1\over 2(2n-1)}-{1\over (2n+1)}\right)\cdot{1\over n!}\tag2$$ $$\sum_{n=0}^{\infty}\left({1\over 2n-1}-{2\over 2n+1}\right)\cdot{1\over n!}=\color{blue}{-e}\tag3$$ Not sure what is the next step...","['factorial', 'sequences-and-series']"
2147571,What's the chance for the closest whole number to $\frac{A}{B}$ is even?,"Both $A$ and $B$ are a random number from the $\left [ 0;1 \right ]$ interval. I don't know how to calculate it, so i've made an estimation with excel and 1 million test, and i've got $0.214633$. But i would need the exact number.","['contest-math', 'statistics', 'probability']"
2147576,Probability - Expected number of draws to get all 52 cards at least once drawing in groups of size n [duplicate],"This question already has an answer here : Expected number of times a set of 10 integers (selected from 1-100) is selected before all 100 are seen (1 answer) Closed 4 years ago . Imagine you have a deck of cards and want to be fairly sure that you draw each card once (with a perfectly fair, complete, and random deck on each draw, of course). You are drawing cards in groups of size n from the deck. What is the expected number of draws such that each card has been drawn at least once? Similar to the coupon collector's problem , but not quite. How would one go about integrating the math for that algorithm with drawing multiple cards at the same time? Edit: found some duplicate questions. How to calculate the expected value of the coupon collector problem if we are collecting the coupons in groups of k? Coupon Collector Problem with Batched Selections","['coupon-collector', 'probability', 'card-games']"
2147657,Definition of the $l_2$ space,"Why do we define $l_{2}$ to be the space of real sequences $(x_{k})$ such that $\sum_{k=1}^\infty\vert x_{k} \vert^{2}$ converges instead of the space of sequences such that $\sum_{k=1}^\infty x_{k}{}^{2}$ converges? $\vert x_{k}\vert^{2}=x_{k}^{2}$ after all... This question may be silly, but I want to be sure there's nothing mysterious going on here.","['functional-analysis', 'sequences-and-series']"
2147658,Lie's theorem about infinitesimal rigid motions and riemannian manifolds curvature,"The theorem states that in a Riemannian connected finite dimensional manifold of 3 dimensions the existence of a 6-dimensional group of local isometries that determine infinitesimal rigid displacements implies that this manifold must possess a constant curva­ture(one can read the complete statement and proofs in Lie's ""Theorie der Transformationsgruppen, Vol. 3""). I'm interested in the conclusions one can derive from this restriction, for instance does this theorem imply some sort of limitation on Riemannian manifolds of variable curvature with respect to the existence of stationary points of functionals(variational principles) on the manifold? Added: To be more specific, if we use the language of principal G-bundles and consider a tangent frame bundle with the total space as the group of local isometries, the fiber G the point-stabilizer group and the base manifold that on wich the isometry group acts,  does this theorem limit the maximum number of dimensions of the total space to 6 for a 3-dimensional base manifold and to a finite number for any finite-dimensional base manifold? Also, do the infinitesimal rigid motions expressed by the local isometry group in a Riemannian manifold determine the maximum number of dimensions of the group of symmetries, defined as vanishing functional derivatives and corresponding Euler-Lagrange equations routinely used in multidimensional calculus of variations, of any objects defined on the manifold that undergo such rigid motions?","['calculus-of-variations', 'differential-geometry', 'lie-groups']"
2147666,First derivative equation with $\cosh^{-1}$ set equal to zero solving,This is the equation I'm left with when I took the derivative of a function. I would like to optimise so I'm trying to find a min/max by setting equal to zero. I have been having trouble solving. $$\frac{2\text{arcosh}(y)}{\sqrt{y^2-1}}+2y-4=0$$,"['algebra-precalculus', 'hyperbolic-functions', 'ordinary-differential-equations']"
2147693,Regarding $e$ in $\lim\limits_{x \to a}{[\phi(x)]^{\psi(x)}} = e^{\lim\limits_{x \to a}{[\phi(x) - 1]\psi(x)}}$,"I'm currently studying limits with $x$ in the exponent. The following formula simplifies the work to solve limits. If $\lim\limits_{x \to a}{\phi(x)} = 1$ and $\lim\limits_{x \to a}{\psi(x)} = \infty$ , then $\lim\limits_{x \to a}{[\phi(x)]^{\psi(x)}} =$ $\lim\limits_{x \to a}{\{[1+\alpha(x)]^{\frac{1}{\alpha(x)}}\}^{\alpha(x)\psi(x)}} =$ $e^{\lim\limits_{x \to a}{[\phi(x) - 1]\psi(x)}}$ For the most part, I understand how the formula is derived. However, there's one part I don't understand. Why does $\lim\limits_{x \to a}{\{[1+\alpha(x)]^{\frac{1}{\alpha(x)}}\}} = e$ ?","['real-analysis', 'limits', 'exponential-function', 'calculus', 'proof-explanation']"
2147720,Relationship between Cauchy's Integral formula and Poisson kernel,"If we have a function $\tilde{f}\in L^p(T)$ on the unit circle $T$ with $p \geq 1$ we can regain a harmonic function $f$ on the unit disk using the Poisson kernel $P_r$: $$
f\left(re^{i\theta}\right)=\frac{1}{2\pi} \int_0^{2\pi} P_r(\theta-\phi) \tilde f\left(e^{i\phi}\right) \,\mathrm{d}\phi, \quad r < 1
$$ It looks somewhat similar to the Cauchy's Integral formula. The latter states that a holomorphic function defined on a disk is completely determined by its values on the boundary $\gamma$ of the disk. $$
f(a)={\frac {1}{2\pi i}}\oint _{\gamma }{\frac {f(z)}{z-a}}\,dz
$$ Questions: 1) Could you please explain the logic behind the integral transformation with the Poisson kernel? Why does the kernel have the form $P_{r}(\theta )=\operatorname {Re} \left({\frac {1+re^{i\theta }}{1-re^{i\theta }}}\right),\ 0\leq r<1$ ? 2) What is the difference between the integral transformation with the Poisson kernel and the Cauchy's integral formula? Thank you for your help in advance.","['functional-analysis', 'complex-analysis']"
2147735,Well-definition of a function from rationals to rationals,"This is admittedly a homework question, but it seems erroneous to me. Define the rational numbers $\mathbb{Q}$ as the set of equivalence classes on $\mathbb{Z} \times (\mathbb{Z} \backslash \{0\})$ corresponding to the equivalence relation $\sim$ , where $$(a,b)\sim (c,d) \Leftrightarrow ad = bc$$ Show that $f: \mathbb{Q} \rightarrow \mathbb{Q}$ given by $f([(a,b)]_\sim) = [(a+b,b)]_\sim$ is not well-defined. But here is a proof (I think) that $f$ is well-defined: $[(a,b)]_\sim = [(c,d)]_\sim \Rightarrow ad = bc$ by definition of $\sim$ $\Rightarrow ad = cb$ , by commutativity of multiplication in $\mathbb{Z}$ $\Rightarrow ad + bd = cb+db$ , by an addition property of equality and commutativity of multiplication $\Rightarrow (a+b)d = (c+d)b$ , by right distributivity of multiplication over addition $\Rightarrow [(a+b,b)]_\sim=[(c+d,d)]_\sim$ , by definition of $\sim$ $\Rightarrow f([(a,b)]_\sim) = f([(c,d)]_\sim)$ , by definition of $f$ . Therefore $f$ is well-defined by the definition of a well-defined function. $\Box$ Is my proof incorrect?","['equivalence-relations', 'functions']"
2147754,Function $f$ such that $|f(x)-f(y)| \ge \sqrt{|x-y|}$,"After having spent some time on this problem and having found little on this topic in existing articles, I decided to post it here. My question is : Does there exist a bounded (injective) function $f: [0,1] \rightarrow \mathbb{C}$ such that $$\forall (x,y) \in [0,1]^2,\ |f(x)-f(y)| \ge \sqrt{|x-y|}$$ (It is not necessary to assume $f$ to be injective, as it follows from the last hypothesis.) Note that I am not asking the function to be continuous. Indeed, using a result of Besicovitch and Schoenberg, I can prove that such a function can not be continuous (however this is not really easy to prove ; see On Jordan Arcs and... ). By the way, in the same paper, they proved that given any $\varepsilon>0$ there exists a continuous injection $f : [0,1] \rightarrow \mathbb{C}$ such that $\forall (x,y) \in [0,1],\ |f(x)-f(y)| \ge |x-y|^{\frac{1}{2}+\varepsilon}$. I really find this result noteworthy. At first I thought I could directly use it, by taking the limit (in a certain sense) to answer my question, but the supremum of the functions they give goes to $+\infty$ when $\varepsilon$ goes to $0$, and I am requiring boundedness : it did not work. Regarding my problem, I believe that there are no such functions, but did not manage to prove it. I tried to consider the reciprocal function : from this point of view, we must study Holder continuous functions of order $2$ defined on subsets of $\mathbb{C}$. However, not much is known about Holder continuous functions of order $>1$ (when they are not constant), or rather, I did not manage to find articles investigating this matter. I also tried to study the discrete case : assuming such a function exists, and taking the image of $\left \{ \frac{k}{n}\ |\ 0 \le k \le n \right \}$, we have, for all $n \in \mathbb{N}^*$, $n$ complex points $x_1,...,x_n$ such that for all $i,j$, $|x_i-x_j| \ge \sqrt{\frac{|i-j|}{n}}$. All those points are in the same bounded set (because $f$ is bounded). However I think that the diameter of such sets grows unbounded when $n$ approaches $+\infty$. Same problem here, I could not find a proper reference dealing with this topic. I would be glad if someone had an idea on how to tackle this problem, or references to related articles. Edit : I posted an answer, which I hope is correct, but I would still be glad to see suggestions or different solutions.","['reference-request', 'real-analysis', 'holder-spaces', 'measure-theory']"
2147756,Strong upper bound for consecutive p-smooth numbers,"Størmer's Theorem provides a method to find all consecutive p-smooth numbers. In Lehmer's paper On a problem of Størmer , a very weak upper bound is given (Theorem 7). M. F. Hasler observes at A002072 that, for $n$ primes, $10^n/n$ is an upper bound, except for $n=4$. This is in my view a very strong bound. Is there a proof of this result or a stronger bound than Lehmer's somewhere? I made this graph to demonstrate how tight this bound is (extraordinarily close for a few small $n$):","['number-theory', 'reference-request', 'asymptotics']"
2147782,Calculating $\frac{1}{1\cdot 2\cdot 3}+\frac{1}{5\cdot 6\cdot 7}+\frac{1}{9\cdot 10\cdot 11}+\cdots$,"I found this question on an old exam paper - UK GCE A-Level (1972) - equivalent to university entrance level in most countries I believe. The method may be ""standard"" but has left me stumped. Maybe I am missing something obvious. Can someone give me a hint rather than a full worked solution? Question Calculate: $$\dfrac{1}{1\cdot 2\cdot 3}+\dfrac{1}{5\cdot 6\cdot 7}+\dfrac{1}{9\cdot 10\cdot 11}+\cdots$$ What do I notice? It is an infinite series, so one of Geometric, Maclaurin, Taylor Series might be useful. The sum converges because each term is less than geometric series with ratio (0.5). The terms are formed from ""truncated"" factorials (my expression) So the series can be rewritten $$\frac{0!}{3!}+\frac{4!}{7!}+\frac{8!}{11!}+\cdots$$ There are three successive positive integers in the denominators of each term in the original series and the multiples of 4 are missing from the denominators. The integers ""within"" the factorials in the numerator and denominator are (arithmetically) increasing by 4. Because it is an infinite series I can't hope to ""group"" the terms by finding common multiples. So I get stuck. Then I cheat and put: $\displaystyle\sum \frac{(4k-4)!}{(4k-1)!}$ into Wolfram Alpha. The answer $\frac{\ln(2)}{4}$ , pops out. So I feel an approach to solution might have something to do with the Maclaurin expansion of $\ln(1+x)$ but I can't get anywhere with this. Any hints would be gratefully received. Thanks, Clive","['factorial', 'sequences-and-series']"
2147789,Principal ideal property without being integral domain? [duplicate],"This question already has an answer here : Principal ideal rings that are not integral domains (1 answer) Closed 7 years ago . In my algebra course (taught from Artin), a principal ideal domain is defined as an integral domain such that all ideals are principal. This got me wondering: Are there rings for which every ideal is principal, but the ring is not an integral domain?","['abstract-algebra', 'ring-theory']"
2147798,Numerical integration of $\frac{1}{x} \int_0^x \frac{t}{e^t-1}dt$,"I'm trying to compute numerically the non-elementary function below :
$$
\frac{1}{x} \int_0^x \frac{t}{e^t-1}\mathrm{d}t
$$ I tried to evaluate the integral with the quad algorithm, but at values of x < 1, the result is inaccurate compared with the tabled values, maybe because of the indetermination of the integrand at $t=0$. I also tried to compute the integral with the Simpsons method in which I manually replace the first term of the integrand vector by the value of the limit :
$$
\lim_{t\to 0} \frac{t}{e^t-1} = \lim_{t \to 0} \frac{1}{e^t}=1
$$
It takes however a large number of intervals to attain a precision as low as 3 digits, and it has poor performance. Any ideas to compute this function more efficiently? Or maybe this function is already computed in a Scipy algorithm? Thanks for your help","['numerical-methods', 'integration', 'indeterminate-forms']"
2147815,Problem solving a differential equation $y'=y+y^2$,"I tried solving this differential equation: $$y'=y+y^2$$
I tried the substitution $$z=y^{-1}\\z'=-y^{-2}y'$$ and the differential equation becomes $z'+z=-1$ and now I have as solution $$z=ce^{-x}-1$$ and $$y=\frac{1}{ce^{-x}-1}$$ 
But the solution is not correct. Does  someone can find my error?",['ordinary-differential-equations']
2147817,"Let $X, X$ ~ $N (120,4)$ be an independent measure, what is the probability that three measurements are equal, when measured three times?","The voltage (in volts) of a given circuit is a random variable $ X $ that is normally distributed with the parameters $ μ = 120 $ and $ σ ^ 2 = 4 $ If three independent measurements are taken, what is the probability that the three measurements are between $ 116 $ and $ 118 $ volts? My idea is to first get a probability of success $ p $, which I will calculate by standardizing $ X $, and then finding the probability that $ X $ is between $ 116 $ and $ 118 $. Since I need to count the number of measurements, each one with probability of success $ p $ and each measurement attempt is done independently, I would do it with another variable $ Y $ ~ $ B (3, p) $ The answer to the question would be $ P (Y = 3) $, but in this case $ n = y = 3 $ then $ P (Y = 3) = p ^ 3 $ $Z=\dfrac{X-μ}{σ}=\dfrac{X-120}{2}\Rightarrow p = (116<X<118) = P(\dfrac{116-120}{2}<Z<\dfrac{118-120}{2}) = $ $P(-2<Z<-1) = \Phi(-1) - \Phi(-2) = 0,13786 - 0,01831 = 0,11955 \Rightarrow p^3 = 0,001708633$ Is the correct way I'm thinking the solution to the exercise? Thank you very much.","['probability-theory', 'binomial-distribution', 'probability', 'probability-distributions']"
2147859,$x^2 + y^5 = 2015^{17}$,"I've got stuck at this problem, I don't know how to approach it. Prove that the equation doesn't
  have any integer solutions for $$x^2 + y^5 =  2015^{17}$$ I've thought about Fermat's little theorem but it didn't helped.
Just a hint would be really  appreciated. Thanks!","['number-theory', 'prime-numbers', 'diophantine-equations', 'elementary-number-theory']"
2147877,"Prove using mathematical induction: for $n \ge 1, 5^{2n} - 4^{2n}$ is divisible by $9$","I have to prove the following statement using mathematical induction. For all integers, $n \ge 1, 5^{2n} - 4^{2n}$ is divisible by 9. I got the base case which is if $n = 1$ and when you plug it in to the equation above you get 9 and 9 is divisible by 9. Now the inductive step is where I'm stuck. I got the inductive hypothesis which is $ 5^{2k} - 4^{2k}$ Now if P(k) is true than P(k+1) must be true. $ 5^{2(k+1)} - 4^{2(k+1)}$ These are the step I gotten so far until I get stuck: $$ 5^{2k+2} - 4^{2k+2} $$
$$ = 5^{2k}\cdot 5^{2} - 4^{2k} \cdot 4{^2} $$
$$ = 5^{2k}\cdot 25 - 4^{2k} \cdot 16 $$ Now after this I have no idea what to do. Any help is appreciated.","['induction', 'discrete-mathematics']"
2147885,"Reference request: Best way of studying Loring Tu's ""An Introduction to Manifolds"" incompletely, but with restrictions","Motivation of question ahead: I have been accepted to study a Master's degree in pure mathematics at an overseas university starting in August. There are quite a few courses that I am interested in taking there that require prerequisites that I do not yet have. To rectify that I am taking a course in ring and module theory and another in abstract analysis. Unfortunately, however, no one at my institution is offering any course on differential geometry/differential topology, which I also need to learn before I go. To rectify this I've initiated a study group at the university, where we basically work through Tu's book . Unfortunately I am very busy with the other courses mentioned, and with teaching duties, so the pace is only a meagre 10-15 pages a week. This is a problem, because when I arrive at the new institution I'm expected to know the following: ""The notion of manifold, smooth maps, immersions and submersions, tangent vectors, Lie derivatives along vector fields, the flow of a vector field, the tangent space (and bundle), the definition of differential forms, de Rham operator (and hopefully the definition of de Rham cohomology),"" But by my calculations, I will not be able to linearly study the book until I reach the de Rham cohomology. This means that I am going to have to, unfortunately, skip some sections for now. Actual Question: The content page for the book is very detailed (available on the linked Amazon page), and I am hoping that someone can tell me what sections I can safely leave out without excessively hampering my ability to understand the concepts specifically mentioned above, and without interrupting the flow of ideas terribly. Thanks in advance. P.S. I realise that this question does not really comply with the guidelines set out for asking questions, but I am not sure where else to go to find the information I am seeking, and I do require this information for non-trivial reasons. So if you are going to vote to close, I'd very much appreciate guidance as to where I should go to find an answer for this question instead (besides at my university, as I am already trying that route concurrently, but our department has very few people knowledgeable enough in the area of differential geometry to be able to answer this question it seems.)","['manifolds', 'reference-request', 'differential-geometry', 'differential-topology']"
2147965,Absolutely convergent series of continuous functions is continuous,"For $m, d\ge 1$, let $f_n: K\to \mathbb{R}^m$ be a continuous function for every $n\ge 0$, where $K\subset \mathbb{R}^d$ is compact. Show that if $\sum_{n=0}^\infty \|f_n\|_K <\infty$ then $f(x):=\sum_{n=0}^\infty f_n(x) <\infty$ is also a continuous function $f:K\to\mathbb{R}^m$. Proof: $\infty>\sum_{n=0}^\infty \|f_n\|_K =\sum_{n=0}^\infty \max\{\|f_n(x)\|:x\in K\} \ge \>\sum_{n=0}^\infty \|f_n(x)\|$, thus $\sum_{n=0}^\infty f_n(x)$ converges for any $x\in K$, which implies that $f(x)$ exists. This also implies that $\forall \varepsilon > 0, \exists N>0$, such that $k>N\implies \|\sum_{n=0}^k f_n(x) - f(x)\|<\varepsilon/2$. Since $f_n(x)$ is continuous for every $x\in K$, then $\forall \varepsilon >0, \exists \delta > 0$ such that, for $x_0\in K$, $\|x-x_0\|<\delta\implies \|f_n(x)-f_n(x_0)\| < \varepsilon$. Now, let $k>N$, then $\left\| \sum_{n=0}^k f_n(x)-\sum_{n=0}^k f_n(x_0)\right\|
=\left\| \sum_{n=0}^k f_n(x)+f(x_0)-f(x_0)-\sum_{n=0}^k f_n(x_0)\right\|\le$
$\left\| \sum_{n=0}^k f_n(x)-f(x_0)\right\|+\left\|\sum_{n=0}^k f_n(x_0)-f(x_0)\right\|$,
and this is where I get stuck since I do not know what to do with the $\left\| \sum_{n=0}^k f_n(x)-f(x_0)\right\|$ term, so some help would be appreciated. Is there something that I do not know about compact sets which can be applied here? Update: I've just found out that sequences of continuous functions are uniformly convergent on compact sets, so I would change my last inequality as follows: $\left\| \sum_{n=0}^k f_n(x)-\sum_{n=0}^k f_n(x_0)\right\|
=\left\| \sum_{n=0}^k f_n(x)+f(x)-f(x)-\sum_{n=0}^k f_n(x_0)\right\|\le$
$\left\| \sum_{n=0}^k f_n(x)-f(x)\right\|+\left\|\sum_{n=0}^k f_n(x_0)-f(x)\right\|
\le \varepsilon/2+\sup\limits_{x\in K}\left\|\sum_{n=0}^k f_n(x)-f(x)\right\|<\varepsilon/2+\varepsilon/2=\varepsilon$ Do you think this is correct now?","['sequences-and-series', 'functions', 'continuity', 'convergence-divergence', 'analysis']"
2147977,Derivative of a large product,"I need help computing $$
\frac{d}{dx}\prod_{n=1}^{2014}\left(x+\frac{1}{n}\right)\biggr\rvert_{x=0}
$$ The answer provided is $\frac{2015}{2\cdot 2013!}$ , however, I do not know how to arrive at this answer. Does anyone have any suggestions?","['derivatives', 'infinite-product', 'sequences-and-series', 'calculus']"
2147994,Spivak or Apostol?,"Which one of those is the best for a person interested in pure mathematics and who wants to have a deep understanding of calculus? Apostol or Spivak? Could you guys tell me some differences between the approaches of them? What about the exercises? I would like to be challenged, but in a constructive way.","['reference-request', 'book-recommendation', 'calculus']"
2148062,"Sequence $x_n$ has subsequences $\{x_{2n}\}$, $\{x_{2n-1}\}$, $\{x_{3n}\}$ all converging. show that $\{x_{n}\}$ is convergent.","Suppose that $x_n$ is a sequence such that the subsequences $\{x_{2n}\}$, $\{x_{2n-1}\}$, $\{x_{3n}\}$ all converge. Show that $\{x_{n}\}$ is convergent. If there is an actual sequence of $\{x_n\}$, I might be able to solve it. But I don't know how to start and solve this problem.","['real-analysis', 'sequences-and-series']"
2148138,Proving Holder's inequality for sums,"I want to prove the Holder's inequality for sums: Let $p\ge1$
    be a real number. Let $(x_{k})\in l_{p}$
    and $(y_{k})\in l_{q}$
   . Then,
  $$\overset{\infty}{\underset{k=1}{\sum}}\vert x_{k}y_{k}\vert\le\left(\overset{\infty}{\underset{k=1}{\sum}}\vert x_{k}\vert^{p}\right)^{\frac{1}{p}}\left(\overset{\infty}{\underset{k=1}{\sum}}\vert y_{k}\vert^{q}\right)^{\frac{1}{q}}$$
  with $q\in\mathbb{R}$
    such that $\frac{1}{p}+\frac{1}{q}=1$
   . Inspired by a proof I've seen before, I attempted at a solution. I would like you to confirm the ideas and to answer the questions, which correspond to steps of the proof I don't know how to justify. My attempt: Let $p>1$
  be a real number. $Let (x_{k})\in l_{p}$
  and $(y_{k})\in l_{q}$
 . If $\left(\overset{\infty}{\underset{k=1}{\sum}}\vert x_{k}\vert^{p}\right)^{\frac{1}{p}}=0$
  or $\left(\overset{\infty}{\underset{k=1}{\sum}}\vert y_{k}\vert^{q}\right)^{\frac{1}{q}}=0$
  the inequality is trivially true ( Question1: Is it? Why? ) In case both are nonzero, we can define the sequences $(z_{k})$
  and $(w_{k})$
  with $$z_{k}=\frac{x_{k}}{\left(\overset{\infty}{\underset{k=1}{\sum}}\vert x_{k}\vert^{p}\right)^{\frac{1}{p}}}\ \text{ and }\ w_{k}=\frac{y_{k}}{\left(\overset{\infty}{\underset{k=1}{\sum}}\vert y_{k}\vert^{q}\right)^{\frac{1}{q}}}$$ Now, $$\overset{\infty}{\underset{k=1}{\sum}}\vert z_{k}w_{k}\vert\le\overset{\infty}{\underset{k=1}{\sum}}\left(\frac{\vert z_{k}\vert^{p}}{p}+\frac{\vert w_{k}\vert^{q}}{q}\right)$$
  (by Young's Inequality). But $$\overset{\infty}{\underset{k=1}{\sum}}\left(\frac{\vert z_{k}\vert^{p}}{p}+\frac{\vert w_{k}\vert^{q}}{q}\right)=\overset{\infty}{\underset{k=1}{\sum}}\left(\frac{\vert x_{k}\vert^{p}}{p\left(\overset{\infty}{\underset{k=1}{\sum}}\vert x_{k}\vert^{p}\right)}+\frac{\vert y_{k}\vert^{q}}{q\left(\overset{\infty}{\underset{k=1}{\sum}}\vert y_{k}\vert^{q}\right)}\right)\le1$$
 ( Question2: Is this last inequality true? Why? If it is, the result follows smoothly... ) So $\overset{\infty}{\underset{k=1}{\sum}}\vert z_{k}w_{k}\vert\le1$
 . Multiplying both sides by the (positive) term $\left(\overset{\infty}{\underset{k=1}{\sum}}\vert x_{k}\vert^{p}\right)^{\frac{1}{p}}\cdot\left(\overset{\infty}{\underset{k=1}{\sum}}\vert y_{k}\vert^{q}\right)^{\frac{1}{q}}$
 , it's done. $\square$","['inequality', 'sequences-and-series']"
2148192,Map from free product $G*G$ to $G$,"Disclaimer: This came up in working on a homework problem, but this is just my curiosity and won't actually help me solve the homework problem, probably. Let $G$ be a group, and take the free product $G*G$. Define $\phi:G*G \to G$ by taking words in $G*G$ and actually reducing them by multiplication in $G$. Is this a group homomorphism? Is there a canonical name for this homomorphism? Both of the following discuss related questions, but I don't think either discusses my exact question. free product of the same group Finitely generated free group is a cogroup object in the category of groups","['category-theory', 'group-theory']"
2148246,Distribution of the Maximum of a (infinite) Random Walk,"Let $S_0 = 0$ and define $S_n = \sum^n_{i = 1} X_i$ such that
\begin{align*}
\mathbb P(X_i = 1) &= p \\
\mathbb P(X_i = -1) &= 1 - p = q
\end{align*} for $p < \frac{1}{2}$. Find the distribution of $Y = \max \{S_0, S_1, S_2, ...\}$. My attempt at a solution: One known result (and a nice application of path counting/the reflection principle) is that if $Y_n = \max \{S_0, S_1, ..., S_n\}$ then
\begin{equation*}
\mathbb P(Y_n \geq r, S_n = b) =
\begin{cases}
 \mathbb P(S_n = b) & b \geq r \\
 \left(\frac{q}{p}\right)^{r - b} \mathbb P(S_n = 2r - b) & b < r
\end{cases}
\end{equation*} and so, for $r \geq 1$, we find
\begin{align*}
\mathbb P(Y_n \geq r) &= \mathbb P(S_n \geq r) + \sum^{r - 1}_{b = -\infty} \left(\frac{q}{p}\right)^{r-b} \mathbb P(S_n = 2r - b) \\
&= \mathbb P(S_n = r) + \sum^\infty_{c = r + 1} \left[1 + \left(\frac{q}{p}\right)^{c - r}\right] \mathbb P(S_n = c)
\end{align*} However, this was for the maximum over a finite random walk $S_n = \sum^n_{i = 1} X_i$. In the present case we're interested in the maximum over all $n \in \mathbb N$, say $S_\infty$, and it's not immediately obvious to me how to solve such a case. Thank you for any input!","['random-walk', 'probability-theory', 'probability']"
2148259,Limit of a sequence satisfying $(2-a_n)a_{n+1} \rightarrow 1$ as $n\rightarrow\infty$,"This was in one of the calculus textbook exercise problem. Now, I am pretty sure that there was a typo in the problem. Problem Let $a_n$ be a sequence of real numbers satisfying the following:
$$
\lim_{n\rightarrow\infty} (2-a_n)a_{n+1} = 1.
$$
Prove that $\lim\limits_{n\rightarrow\infty} a_n= 1$. My tries The conclusion is true under assumption of $0<a_n<2$ for all but finitely many $n$. The conclusion is false without any further assumption. So, the problem is incorrect as it is stated. Question Is the conclusion true under an additional assumption of boundedness of $a_n$? Remarks I was able to obtain a sequence $a_n$ that the conclusion is false. However, under the assumption of boundedness, I am wondering if we can prove the conclusion or give a counterexample. Proof of My try 1 Let $\overline{a} = \limsup a_n$ and $\underline{a} = \liminf a_n$. Let $\{n_k\}_{k=1}^{\infty}$ be a subsequence of natural numbers such that $a_{n_k}\rightarrow \overline{a}$. Then we have 
$$
(2-a_{n_k}) a_{n_k +1} \rightarrow 1.
$$
Taking a convergent subsequence of $\{a_{n_k +1}\}_{k=1}^{\infty}$ which converges to $b$, we have 
$$(2-\overline{a}) b = 1.$$
This gives $b=1/(2-\overline{a})$. Since $\overline{a}$ is the limsup, we have
$$
\frac1{2-\overline{a}}\leq \overline{a}.
$$
By assumption $0<a_n<2$ for all but finitely many $n$, we have $2-\overline{a} >0$. Thus, 
$$1\leq (2-\overline{a})\overline{a}.$$
Then it follows that $(\overline{a}-1)^2 \leq 0$, which yields $\overline{a}=1$. Similar argument for $\underline{a}$ gives $\underline{a}=1$. Therefore, $a_n\rightarrow 1$. Proof of My try 2 Since $a_n=0$ or $a_n=2$ do not happen infinitely often, we may assume that $a_n\neq 0$, $a_n\neq 2$ for all $n$. Consider $\{1/k \}_{k=1}^{\infty}$, and the sequence given by the recurrence:
$$
a_{n+1} = \frac{1+\frac1 1}{2-a_n}. \ \ \textrm{(Round 1)}
$$
Then there exists $n$ such that $a_n>2$. We will put the first time that it happens as $n=n_1$, and we will say that we exit the Round 1. We have $a_{n_1+1}<0$ by the recurrence. Once we exit the Round 1, we enter into the Round 2 which starts with $n=n_1+1$ and the recurrence: 
$$
a_{n+1} = \frac{1+\frac12}{2-a_n}. \ \ \textrm{(Round 2)}
$$
Then there exists $n$ such that $a_n>2$. We put the first time $n\geq n_1+1$ that it happens as $n=n_2$, and we exit the Round 2. Then $a_{n_2+1}<0$. Continue this process with using the recurrence 
$$
a_{n+1} = \frac{1+\frac1k}{2-a_n}. \ \ \textrm{(Round $k$)}
$$ Then we have the sequcne $a_n$ such that $a_n>2$ infinitely many often and $(2-a_n)a_{n+1}\rightarrow 1$.","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
2148267,Sections of line bundles over $\mathbb{C}P^n$,"Let $M=\mathbb{CP}^n$ be our manifold, and $U_j$ be the standard coordinate charts, i.e. $$U_j=\{[z_0:z_1:\cdots:z_n] : z_j\neq 0\}$$ 
with coordinates $w^{(j)}_i=z_i/z_j$ for $i\neq j.$ Consider the section $s$ of $\mathcal{O}(1)$ which is $s=z_0$ in homogeneous coordinates. In local cordinates: over $U_1$ is equal to $w_1$. Now consider the section $s^*$ of $\mathcal{O}(2)$ which is $s=z_0z_1$ in homogeneous coordinates. In local cordinates: over $U_1$ is equal to $w_1$. Is that correct? The two sections $s$ and $s^*$ are equal on $U_1$? Similarly, what is the local expression in $U_1$ for $t=z_0 z_1 z_2 \cdots z_n$ (as a section of $\mathcal{O}(n+1)$) ? I'm a bit confused. I would honestly appreciate any help.","['complex-geometry', 'kahler-manifolds', 'vector-bundles', 'differential-geometry']"
2148270,Least Positive Residue,"How would I find the least positive residue of
say $6! \bmod 7$ or $12! \bmod 13$ I just learned modular arithmetic and my book doesn't explain what least positive residues are so I'm a bit lost.",['number-theory']
2148286,Almost sure convergence and equivalent definition,"First of all this question has related answers here and here , but I am still struggling to understand the proof and the nuances that go along with it. Show that a.s convergence of a sequence of random variables $X_k$ to a constant $\mu$ is equivalent to the condition that for all $\epsilon > 0$:
$$ \lim_{n\to\infty}\mathbb{P}[\omega:\sup_{k>n}|X_k(\omega) - \mu|>\epsilon] = 0$$ I have interpreted the proof in the second link as follows. I fixed $\epsilon>0$ as given and defined, $${A_k}: = \left\{ {\left| {{X_k} - \mu } \right| \geq \epsilon } \right\}\qquad {\text{and }}\qquad {B_n}: = \bigcup\limits_{k \geqslant n} {{A_k}} $$ Then if $B = \bigcap\limits_{n \geq 1} {{B_n}}$ we have $P\{B\} = 0$ since, 
$$P\left\{ {\mathop {\lim }\limits_{n \to \infty } {X_n} = \mu } \right\} = 1\quad  \Rightarrow \quad P\left\{ {\mathop {\lim }\limits_{n \to \infty } {X_n} \ne \mu} \right\} = 0{\text{ }}\quad {\text{or}}\quad {\text{ }}\mathop {\lim }\limits_{n \to \infty } {X_n}=\; \left({\text{D.N.E}}\right)$$
Then, 
$$\begin{aligned}
P\left\{ B \right\} & = 0 \\
& = P\left\{ {\bigcap\limits_{n \geqslant 1} {\bigcup\limits_{k > n} {{A_k}} } } \right\} = P\left\{ {\mathop {\lim \sup }\limits_{k \to \infty } {A_k}} \right\} = P\left\{ {\mathop {\lim }\limits_{n \to \infty } \mathop {\sup }\limits_{k > n} {A_k}} \right\} \\ &= \mathop {\lim }\limits_{n \to \infty } P\left\{ {\mathop {\sup }\limits_{k > n} \left| {{X_k} - \mu } \right| \geqslant \varepsilon } \right\} \hfill \\ 
\end{aligned}$$
But I am not sure how the last step - pulling the limit out of the probability - is justified. I understand that $B_n$ are decreasing sets, but the continuity of probability result tell us that the limit outside becomes a union or intersection when taken inside. But this seems to be a limit remaining a limit when taken outside. Is it true that we can always pull a limit outside? As for the converse I do not understand it at all, and my attempts to define similar sets $A_k,B_n$ etc. have failed.  I know that the condition gives us, 
$$ \mathop {\lim }\limits_{n \to \infty } P\left\{ {\mathop {\sup }\limits_{k > n} \left| {{X_k} - \mu } \right| < \varepsilon } \right\} = 1 \quad  \forall  \varepsilon  > 0$$
But I don't know how to proceed from here. My attempts have been to take the limit inside and show somehow that $\mathop {\lim \sup }\limits_{k \to \infty } \left| {{X_k} - \mu } \right| \leq 0$. Since $\mathop {\lim \inf }\limits_{k \to \infty } \left| {{X_k} - \mu } \right| \geq 0$ if we show the previous, then the limit exists and the desired equality : $ {\mathop {\lim }\limits_{k \to \infty } \left| {{X_k}} \right| = \mu } $ is achieved. Any help would be much appreciated. I have been mulling over this for three days, and I loathe to post questions that are similar to ones already I asked, but I really have not been able to crack it or find a source that enumerates and explains each step and why it is justified.","['real-analysis', 'probability-theory']"
2148354,"Set builder notation, can I switch what's written on the left and right of the vertical bar?","I've been reading 'How to Prove It' by Velleman, in the book it states that: $\{x\,\in\,U\,|\,P(x)\}$; this is read 'the set of all $x$ in $U$ such that $P(x)$'. It then goes on to give $\{x\,\in\,\mathbb R\,|\,x^2<9\}$ as an example of such a set which would have the real numbers between $-3$ and $3$ as elements. However, in the next chapter the book gives an example of the set of all perfect squares $S$ by specifying the universe of discourse on the right: $S=\{n^2|\,n\,\in\,\mathbb N\}$ So is it acceptable to switch what is on the left side of the vertical bar with what is on the right? The Wikipedia entry on set builder notation , under 'Specifying the domain' uses the former notation, yet under 'More complex expressions on the left side of the notation' they use the latter. From the book and Wikipedia I'd assume it would be valid to use either. That being said, I've seen people claim that by switching them one would be creating a different set.","['notation', 'elementary-set-theory']"
2148360,To find value of $\sin 4x$,Given $\tan x = \frac { 1+ \sqrt{1+x}}{1+ \sqrt{1-x}}$. i have to find value of $\sin 4x$. i write $\sin 4x=4 \frac{ (1-\tan^2 x)(2 \tan x)}{(1+\tan^2 x)^2}$ but it seems very  complicated to do this? Any other methods? Thanks,['trigonometry']
2148361,Stokes' theorem with respect to complex differentials $\partial$ and $\overline{\partial}$ on Riemann surfaces,"Let $X$ be a compact Riemann surface.  Consider the operators $\partial$ and $\overline{\partial}$, where $\partial$ is considered as either an operator taking (complex-valued) functions to $(1, 0)$-forms or an operator taking $(0, 1)$-forms to $2$-forms; and where $\overline{\partial}$ is considered as either an operator taking functions to $(0, 1)$-forms or an operator taking $(1, 0)$-forms to $2$-forms. Let $f$ be a smooth function on $X$; let $\alpha$ be a holomorphic $(1,0)$-form on $X$. In Donaldson's book Riemann Surfaces , he says that the integral $$ \int_X \overline{\partial}(f \alpha) = 0 $$ vanishes by Stokes' theorem.  Moreover, he frequently uses Stokes' theorem as justification for the vanishing of integrals of the form $\int \partial \beta$ or $\int \overline{\partial} \beta$ for appropriate $1$-forms $\beta$.  But the only version of Stokes' theorem he writes down as a theorem is the usual one with the differential $\mathrm{d}$. My question is whether there is indeed a more general version for Stokes' theorem, applicable to the complex differentials $\partial$ and $\overline{\partial}$, or if in each case I need to figure out why Stokes' theorem is applicable.  And, if possible, can you help me understand why it is applicable in the special case mentioned above?","['complex-geometry', 'differential-forms', 'riemann-surfaces', 'differential-geometry']"
2148389,"Why does ""$x^2 - 5x + 6 = 0$"", which is the same as ""$(x-3)(x-2) = 0$"", represent a parabola?","Consider the equation $x^2 - 5x + 6 = 0$. By factorising I get $(x-3)(x-2)  = 0$. Which means it represents a pair of straight lines, namely $x-2 =0 $ and $x- 3 = 0$, but when I plot $x^2 - 5x + 6 = 0$, I get a parabola, not a pair of straight lines. Why? Plotting: x^2 - 5x + 6 = 0 at Wolfram Alpha , I get the result:","['graphing-functions', 'wolfram-alpha', 'geometry']"
2148408,Nonhomogeneous heat equation solution of $u_t - \Delta u = \rm div f$,"For 3D, I have some doubts of the nonhomogeneous heat equation with initial value zero. i.e if $u_t - \Delta u = f$, $$u(x,t)=c\int_{0}^{t} \int_{\mathbb R^3} \frac{1}{(t-s)^{3/2}} e^{\frac{-|x-y|^2}{4(t-s)}}f(y,s)dyds$$. Now if I change $f$ to $\rm div f$, here I mean the divergence. Then, what will $u$ be? Thanks.","['real-analysis', 'partial-differential-equations', 'heat-equation', 'ordinary-differential-equations', 'analysis']"
2148409,How can we show that $\int_{0}^{\infty}x\sin{x}\ln{(1-e^{-x})}\mathrm dx=1-{\pi\over 2\tanh\pi}-{\pi^2\over 2\sinh^2{\pi}}?$,"Consider the integral $(1)$ $$\int_{0}^{\infty}x\sin{x}\ln{(1-e^{-x})}\mathrm dx=I\tag1$$
  How can we show that $$I=1-{\pi\over 2\tanh\pi}-{\pi^2\over 2\sinh^2{\pi}}$$ An attempt: Dealing with indefinite integral $$\int x\sin{x}\ln{(1-e^{-x})}\mathrm dx=J\tag2$$ Apply integration by parts $u=\ln{(1-e^{-x})}$ then $du={e^{-x}\over 1-e^{-x}}\mathrm dx$ $v=-\int x\sin{x}\mathrm dx=-x\cos{x}+\sin{x}$ $$J=(-x\cos{x}+\sin{x})\ln{(1-e^{-x})}-\int{e^{-x}\over 1-e^{-x}}(\sin{x}-x\cos{x})\mathrm dx\tag3$$ $$J=(-x\cos{x}+\sin{x})\ln{(1-e^{-x})}-\int\sum_{n=0}^{\infty}e^{x(1-n)}(\sin{x}-x\cos{x})\mathrm dx\tag4$$ $$J=(-x\cos{x}+\sin{x})\ln{(1-e^{-x})}-\sum_{n=0}^{\infty}\int e^{x(1-n)}(\sin{x}-x\cos{x})\mathrm dx\tag5$$ Let Applying integration by parts $$J_1=\int e^{x(1-n)}\sin{x}\mathrm dx={e^{x(1-n)}[(1-n)\sin{x}-\cos{x}]\over (1-n)^2+1}$$ $$J_2=\int xe^{x(1-n)}\cos{x}\mathrm dx={xe^{x(1-n)}[(1-n)\cos{x}+\sin{x}]\over (1-n)^2+1}-{e^{x(1-n)}[(n^2-2n)\cos{x}-2(1-n)\sin{x}]\over ((1-n)^2+1)^2}$$ So far applying integration by parts seem bit hard to resolve problem $(1)$, how else can we tackle $(1)?$","['calculus', 'closed-form', 'integration', 'definite-integrals', 'sequences-and-series']"
2148424,Integral of product of CDF and PDF of a random variable,"For a continuous real random variable $X$ with CDF $F_X(x)$ and PDF $f_X(x)$ I want to prove the following
$$\int\limits_{-\infty}^{\infty}x(2F_x(x)-1)f_x(x)dx\geq 0$$ 
I was thinking of integration by parts but $x$ complicates it. Any hints?","['probability-theory', 'integration', 'density-function', 'probability-distributions']"
2148431,"I want to change my corporate developer thinking mindset and want to learn Linear Algebra, Statistics and Calculus for improvement [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 5 years ago . Improve this question Hello Math Stackexchangers, I am a typical corporate developer who is bored by doing repetitive work, I need change and I am ready for it. I want to learn Linear Algebra, Statistics and Calculus, help me build my learning path or recommend me some good beginner level book. I can spend up to 8-10 hours per day for learning. Thank you","['career-development', 'statistics', 'linear-algebra', 'calculus']"
2148453,Finding an example of a linear operator,"I am asked to give an example of a linear operator $T$ on a normed space $X$ such that $$\Vert T\Vert = 1$$ and $$\Vert T(x)\Vert < \Vert x\Vert, \;\forall x \in X\setminus\{0\}.$$ I consider $X = (\ell^1, \Vert \cdot \Vert _1)$, and define $T: X\to X$ by $$\forall x = (x_n)_{n=1}^\infty,\;T(x) = \left( \left( 1-\frac1{2^n} \right)x_n\right)_{n=1}^\infty.$$
I can prove that $T$ is in fact a linear operator from $X$ to $X$, but I can't prove that $\Vert T\Vert = 1$ and $\Vert T(x)\Vert < \Vert x\Vert, \;\forall x \in X\setminus\{0\}.$ I would like some tips if my example is correct, and if so how to prove the statement.  Thanks a lot!!!","['functional-analysis', 'examples-counterexamples']"
2148474,How to prove this lemma related to iterated function?,"My teacher gave me the following lemma to prove : Let $P(x)$ be a polynomial with integer coefficients. If some integer $t$ satisfies $$P(P((...P(t)...))) =t$$ for some number of iterations, then prove that $t$ should either satisfy $P(t)=t$ or $P(P(t)) = t$ . $(t \in \mathbb{Z})$ I have tried some usual approaches but failed. My teacher told me that this lemma is related to some IMO problem of the year 2005 or 2006. So I expect some clever and ingenious proof. I have tried my best but it does not yield. Can anyone help me to prove this conundrum ? Any help will be gratefully acknowledged. Thanks in Advance ! :-)","['algebra-precalculus', 'contest-math', 'polynomials', 'integers']"
2148532,Lie bracket of local orthonormal basis of vector fields,"Let $(M, g, \nabla)$ be a Riemannian manifold (with $\nabla$ the Levi-Civita connection of $g$), and let {$E_i$} be a local orthonormal basis of vector fields ($g(E_i, E_j) = \delta_{ij} )$. What can I say about 
$[E_i, E_j]$ or equivalently (using the torsion-free property of Levi-Civita connection) about $\nabla_{E_i} E_j$? Is it true that $[E_i, E_j]$ vanishes?","['manifolds', 'vector-fields', 'riemannian-geometry', 'differential-geometry']"
2148553,Regular in codimension one VS Singular locus is codimension at least two,"In Hartshorne, a scheme is regular in codimension one if the local ring at any (non-closed) point representing a codimension one subscheme is a regular local ring (of Krull dimension one). For varieties, the most naive notion of being regular in codimension one (at least to me!) is just to say that the set of singular points is subvariety of codimension at least two. Is my naive definition of ""regular in codim one"" equivalent to the definition in Hartshorne? (I ask this because my naive definition is easy to verify: for instance,  a surface with only ADE singularities is clearly regular in codimension one by my naive definition - there is no need to do any commutative algebra, which I'm terrible at. But I do need to know if this singular variety satisfies the condition in Hartshorne, because having DVRs in codimension one allows me to define Weil divisors.)",['algebraic-geometry']
2148570,What's the symbol for diffeomorphism? [duplicate],"This question already has answers here : Notations involving squiggly lines over horizontal lines (3 answers) Closed 7 years ago . I know that isomorphism has a well known symbol, but what about diffeomorphism?","['smooth-manifolds', 'notation', 'functions']"
2148582,In how many ways can i obtain 4 using 0.125 and multiples?,"In how many ways can I obtain $4$ as a sum of : $0.125$, $0.25$, $0.5$, $1$, $2$, $4$ with repetitions of values?
The order of the sum is important, for example $(1, 1, 2)$ is different from $(1, 2, 1)$ and different from $(2, 1, 1)$,","['combinatorics', 'integer-partitions']"
2148625,Non-unitary isometries in $B(H)$ are far away from invertible elements,"I am struggling with the second part of this problem: Let $A$ be a unital Banach Algebra, and let $x$ and $y$ be elements in A such that that $xy=1_A$ and $yx\neq1_A$. (i) Let $z$ be an element in $A$, such that $\|x-z\|<\frac{1}{\|y\|}$. Show that $z$ is not invertible. (ii) Let $H$ be an infinite dimensional Hilbert space. Let $S\in B(H)$ be a non-unitary isometry (i.e. $S^*S=I$ and $SS^* \neq I$).
 Show that $$1=\|S\|=\textrm{dist}(S,G(B(H))),$$ where $G(B(H))$ is the subset of invertible elements in $B(H)$. This is the progress, I have made: I have finished the proof of part (i) using the fact that if $\|1_A-x\|<1$, for some $x\in A$, then $x$ is invertible . Further, it is easy to see that $\|S\|=\|S^*\|=1$. It therefore follows from part (i) that $\textrm{dist}(S^*, G(B(H))\geq 1$. Finally, it is evident that neither $S$ nor $S^*$ are invertible. Any hints would be appreciated.","['real-analysis', 'banach-algebras', 'operator-theory', 'functional-analysis', 'operator-algebras']"
2148640,Uniform martingale convergence of Radon-Nikodym derivatives of a convex set of probabilities,"Update : I've cross posted at MO here . Let $(\Omega, \mathcal{F})$ be a measurable space equipped with a filtration $\{\mathcal{F}_n\}_{n \in \mathbb{N}}$ such that $\mathcal{F}_n \uparrow \mathcal{F}$. Let $\mathcal{C}$ be convex set of mutually absolutely continuous probabilities on $(\Omega, \mathcal{F})$ generated by finitely many extreme points $P_1,...,P_n$. Suppose that $\{R_n \}_{n \in \mathbb{N}}$ is a sequence of probability measures defined, respectively, on $(\Omega, \mathcal{F}_n)$, and suppose that for all $Q \in \mathcal{C}$, $R_n \ll Q|_{\mathcal{F}_n}$ for all $n$. Let $Y^Q_n = dR_n/dQ|_{\mathcal{F}_n}$ be the corresponding Radon-Nikodym derivative. Let us assume that, for all $Q \in \mathcal{C}$, $\{Y_n^Q \}_{n \in \mathbb{N}}$ is a martingale in $\{\mathcal{F}_n\}_{n \in \mathbb{N}}$ with respect to $Q$. Since the $Y_n^Q$ are non-negative, the martingale convergence theorem guarantees that $Y_n^Q \to Y^Q_\infty$ almost surely (with respect to any $Q \in \mathcal{C}$, by mutual absolute continuity). Question. Does it follow from our convexity assumptions that the martingale convergence mentioned above is uniform in $Q \in \mathcal{C}$? That is, is it true that $\sup_Q |Y^Q_n - Y^Q_\infty| \to 0 \ $ almost surely as $n \to \infty$? If it helps, we can assume that the filtration is very simple. For instance, we can assume that each $\mathcal{F}_n$ is generated by a finite measurable partition. Also, if it helps, we can assume that for all $Q \in \mathcal{C}$ the sequence $\{ Y_n^Q\}$ is uniformly integrable and so $Y_n^Q \to Y^Q_\infty$ in $L^1$ as well as almost surely.","['martingales', 'probability-theory', 'measure-theory', 'convex-analysis']"
2148666,Compute $5!25! \mod 31$,"For an exercise, I was asked to compute $5!25! \mod 31$. I noticed that $5! = 120 \equiv -4 \equiv 27 \mod 31$. Therefore we have that 
$$5!25! \equiv 27 \cdot 25! \mod 31.$$
Because of the congruence of Wilson, I also know that $30! \equiv -1 \mod 31$. 
We have that $30! \equiv 30 \cdot 29 \cdot 28 \cdot 26 \cdot 27 \cdot 25! \equiv -1 \mod 31$, so I computed 
$$(30 \cdot 29 \cdot 28 \cdot 26) \equiv (-1 \cdot (-2) \cdot (-3) \cdot (-5)) \equiv 30 \equiv -1 \mod 31.$$ Hence we find that $-1 \cdot (27 \cdot 25!) \equiv -1 \mod 31$ and therefore $27 \cdot 25! \equiv 1 \mod 31$. This proves that $5!25! \equiv 1 \mod 31$. Is this correct?","['number-theory', 'proof-verification', 'elementary-number-theory']"
2148688,Is there a bijection $f: \mathbb{N} \rightarrow \mathbb{N}$ such that the series $\sum\limits_n \frac{1}{n+f(n)} $ is convergent?,"Is there a bijection $f: \mathbb{N} \rightarrow \mathbb{N}$ such that the series $\sum_1 ^\infty \frac{1}{n+f(n)} $ is convergent? I could not solve this. I tried to proceed in following lines: 1) Tried to provide a contradiction: First let $n \sim m$ iff $\exists k \in \mathbb{Z}$ such that $f^k(n)=m$. This is an equivalence relation. Consider the orbits. For the finite orbits we can compare the series to $\sum_1^\infty \frac{1}{n+n}$. But then I could not figure out how to proceed for infinite orbits. 2) Tried to prove that there is some function: Let $\{k_n\}$ be a subsequence of $\mathbb{N}$ such that $\sum_0^\infty \frac{1}{k_n}$ converges. Set $f(n)=k_{n}, \forall n \in \mathbb{N}\setminus\{k_n\}$. Then images of each $n$ which are not in the subsequence $k_n$ is defined. Now we have to define images of each $k_n$.
Define $f(k_n)=n$ $ \forall n \in \mathbb{N}\setminus \{k_n\}.$. Could not proceed further. I think My second attempt was going in right direction. My plan was use the fact that all elements here are positive and to construct the function $f$ in such a manner that $\forall n\in \mathbb{N}$ either $n$ or $f(n)$ is in $\{k_n\}$.","['real-analysis', 'sequences-and-series', 'analysis']"
2148717,Prove that $\sum \limits_{n=0}^{\infty} \frac{n!}{(n+1)!+(n+2)!} = \frac{3}{4}$,"I was playing around with factorials on Wolfram|Alpha , when I got this amazing result : $$\sum \limits_{n=0}^{\infty} \dfrac{n!}{(n+1)!+(n+2)!} = \dfrac{3}{4}.$$ Evaluating the first few partial sums makes it obvious that the sum converges to $\approx 0.7$. But I am not able to prove this result algebraically. I tried manipulating the terms and introducing Gamma Function, but without success. Can anyone help me with this infinite sum ? Is there some well-known method of evaluating infinite sums similar to this ? Any help will be gratefully acknowledged. Thanks in advance ! :-) EDIT : I realized that $(n!)$ can be cancelled out from the fraction and the limit of the remaining fraction as $n \to \infty$ can be calculated very easily to be equal to $0.75$. Very silly of me to ask such a question !!! Anyways you can check out the comment by @Did if this ""Edit"" section does not help.","['factorial', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
2148731,Finding the row and column number of the number $20096$,Consider the numbers arranged in the following way $$\begin{array}{ccccccc} 1 & 3 & 6 & 10 & 15 & 21 & \cdots \\ 2 & 5 & 9 & 14 & 20 & \cdots & \cdots \\ 4 & 8 & 13 & 19 & \cdots & \cdots & \cdots \\ 7 & 12 & 18 & \cdots & \cdots & \cdots & \cdots \\ 11 & 17 & \cdots & \cdots & \cdots & \cdots & \cdots \\ 16 & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots \\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \end{array}$$The question is to find the row number and column number in which the number $20096$ occurs. I tried to find a general expression for the number in $k^{th}$ column and $n^{th}$ row.$$\frac{k(k-1)}{2}-(n-1)=20096$$and $$\frac{n(n-1)}{2}+k=20096$$But I am getting fractional value of row which is incorrect.Any ideas?Thanks.,['sequences-and-series']
2148740,Building an Automated Theorem Prover with Machine Learning,"I am very interested in developing a machine-learning algorithm that could learn from the axioms, properties of the mathematical system of interest, and positive examples from the conjecture of interest to infer some key properties of the possible proof to the conjecture.  Such interest led me to the art of automated theorem prover, which seems to exist more than I know. Is ATP applicable to the fields in algebraic geometry and set-theoretic topology?  I am very interested in trying to prove Jacobian Conjecture, which is related to the computation learning theory.  I am lost in the sea of ATP, so I am curious if you could suggest me some books and articles that describe ATP. I am also curious how applicable is ATP to the algebraic geometry.","['reference-request', 'automated-theorem-proving', 'book-recommendation', 'algebraic-geometry']"
2148742,The set of natural numbers is ordinal,"We define $ω$ to be the set of natural numbers, i.e. $ω=$ $\cap$ { $x$ | $0\in x \wedge \forall u\in x$ $ u+1\in x $ } Accordingly, I have managed to show that $ω \subset\text{Ord}$ , where $\text{Ord}$ is the class of all ordinal numbers. Since I was asked to prove that $\omega \in \text{Ord}$ , it is only left to prove that $\omega$ itself is transitive (due to definition which states: $x \in \text{Ord}$ iff x is transitive and every element of $x$ is transitive too.) Does anyone have any idea of how to prove transitivity of $\omega$ , where by transitivity I mean, if $x \in \omega$ , then $x \subset \omega$ ...","['elementary-set-theory', 'ordinals']"
2148759,Evaluate the limit $\lim \limits_{x \to 2} \frac{\cos\frac{\pi}{x}}{x-2}$,"I was asked to determine the limit: $$\lim \limits_{x \to 2}  \frac{\cos\frac{\pi}{x}}{x-2}$$ without using L'Hopital's rule. Also, I was given the hint: let $t = \frac{\pi}{2} - \frac{\pi}{x}$. However, I'm not sure how to begin. A little assistance would be appreciated.","['limits-without-lhopital', 'trigonometry', 'calculus', 'limits']"
2148785,Current being a differential form with distribution coefficients,"It is a standard result that on a domain $\Omega$ in $\mathbb{R}^n$, $T=\sum_{|I|=q}T_{I}dx_{I}$ for any $q$-current $T$, where $T_{I}$ is a $0$ current or a distribution on $\Omega$ for each multi-index $I$. Now, for an $r$-current $T$ and an $s$-form $\alpha$ on $\Omega$, define an $r+s$-current as $(T\wedge\alpha)(\beta)=T(\alpha\wedge\beta)$ for any $n-r-s$-form $\beta$ on $\Omega$. Then, in the above we have $T=\sum_{|I|=q}T_{I}\wedge dx_{I}$ on $\Omega$. At this stage, if we start with a general smooth manifold $M$(orientable) and a $q$-current $T$ on $M$, can we write $T=\sum_{\alpha}T_{\alpha}\wedge\omega_{\alpha}$ on $M~?$ (Here $T_{\alpha}$ and $\omega_{\alpha}$ are distributions and $q$-forms on $M$ for each $\alpha$).","['differential-forms', 'distribution-theory', 'differential-geometry']"
2148802,Can anyone explain how the complex matrix representation of a quaternions is constructed?,I am reading some properties of quaternionic matrices and I am unable to understand how can we got such matrix representation. please help in this regards.,"['complex-analysis', 'quaternions', 'linear-algebra']"
2148815,"Prove that for an open and continuous map $f:X \to Y$ between topological spaces, it is $f(\tau_X)=\tau_Y$","Let $(X,\tau_x),(Y,\tau_Y)$ be topological spaces and let $f:X \to Y$ be a continuous and open function. Prove that $f(\tau_X)=\tau_Y$ This is an excercise of the problems set of my course on Introductory Topology notes. I think what I've been asked to prove is not true since if I chose $X,Y$ both equiped with the discrete topology and $Y$ having more than one element, then any constant function $f$ $(f(x)=c \in Y$ for all $x \in X)$ is continuous and open but $f(\tau_X)=\{ \emptyset, \{c\} \}\not=\tau_Y$ Is this right? Perhaps I'm missunderstanding it. Any clarification or suggestion is welcome. Thanks","['continuity', 'general-topology', 'open-map']"
2148822,Is $\infty \times 0$ just $\frac00$?,"When I solve some limit I get infinity times zero in the answer, but isn't infinity just $\frac10$ and $\frac10 \times 0 = \frac00$. Can I just use L'Hospital's rule there?","['infinity', 'calculus', 'limits']"
2148828,"When does a real, positive definite matrix have positive entries?","Let $A = (a_{ij})_{i,j=1,\dots,n}$ be a real symmetric square matrix. Suppose $A$ is positive definite. 
Are there sufficient conditions that guarantee $a_{ij} > 0$ for all $i,j = 1,\dots, n$? I know that $a_{ii} > 0$ for all $i=1,\dots,n.$ One thing I found is that $a_{ij} > 0$ for all $i,j = 1, \dots, n$ if and only if $A^{-1}$ is monotone , i.e. $A^{-1}x \ge 0$ implies $x \ge 0$ for all $x \in \mathbb{R}^n.$
Is there a nice way to connect this with the fact that $A$ and $A^{-1}$ are positive definite?","['matrices', 'positive-definite', 'positive-semidefinite']"
2148834,Does the preservation of sequential limits imply continuity in non-metrizable spaces?,"I have a question concerning the following theorem in Munkres's Topology : Theorem 21.3. Let $f : X \rightarrow Y$. If the function $f$ is continuous, then for every convergent sequence $x_n \rightarrow x$ in $X$, the sequence $f(x_n)$ converges to $f(x)$. The converse holds if $X$ is metrizable. My question is: Does the converse holds if $X$ not metrizable? I have no idea about where to start, but I know some spaces which are not metrizable. E.g. $\mathbb{R}^\omega$ with the box topology, the topological vector space of all functions $f : \mathbb{R} \rightarrow \mathbb{R}$, the uncountable product of $\mathbb{R}$.","['continuity', 'general-topology']"
2148872,Show that $\sinh x$ is strictly increasing,"Can someone help me with this analysis question: I have to show that $\sinh(x)$ is strictly increasing on all of $\Bbb R$, and that $\cosh(x)$ is strictly increasing on the interval: $[0,\infty)$ and strictly decreasing on the interval: $(-\infty, 0]$. I'm not allowed to differentiate or anything like that. I think I have to do it with inequalities. I've been giving a hint to use the trigonometric addition formulas.","['real-analysis', 'analysis']"
2148906,Fundamental Theorem of Calculus for distributions.,"Consider a function $F \in C^{\alpha}( \mathbb{R})$ for $0 < \alpha < 1.$ Then we can take it's distributional derivative. We can say $f = F' \in C^{\alpha -1}( \mathbb{R} )$. My issue is going back. Say I have a $\alpha -1 -$Hölder function $f$, then how can I ""integrate"" it to get a primitive $F$ such that $f$ is it's distributional derivative? Here we use the following definition (but any other definition can be used as well) for negative $\beta \in (-1,0)$: $$g \in C^{\beta}( \mathbb{R} ) \Leftrightarrow \ |\langle g, \phi_{x}^{\lambda} \rangle| \  ≤ C \lambda^{\beta}$$ where $\phi_{x}^{\lambda}(z) = \lambda^{-d}\phi(\frac{z -x}{\lambda})$ and $C$ is uniform over all $x$ and $\phi \in C^{\infty}_0(-1,1)$ with $||\phi||_{C^1}≤1.$ EDIT: An approach would be to prove that $$C^{\beta} \subseteq L^1_{Loc}$$ which would mean in particular that our distributions are actually functions . This does not work, as pointed out in the comments, and in related questions. My other idea was to define $\langle f, 1_{[0,a]} \rangle $ by using typical approximation of the indicator function of $1_{[0,a]}.$ This cannot work: for example the derivative of a the Brownian motion is not a measure . I saw that this topic is covered only slightly in MathStackExchange. Here are similar questions regarding this topic. Here Is a general question about Holder spaces with negative exponents. Here Is a question that is very similar to mine, maybe just in a slightly different context. Here Is a question by myself regarding the same topic. In fact as you might imagine I am looking into this subject with little success :D Finally let me also give some motivation as to why I ma studying this: these spaces are used in the theory of SPDEs. In particular a reference is Fritz's and Hairer's book: ""A course on rough paths."" Both my questions are exercises in this book.","['functional-analysis', 'distribution-theory', 'holder-spaces', 'analysis']"
2148914,Evaluate the limit at infinity,"Evaluate: $$\lim_{x \rightarrow \infty}x\sin \frac{1}{x}$$ $$\lim_{x \rightarrow \infty}x \times\lim_{x \rightarrow \infty} \sin \frac{1}{x}=\infty \times0=\text{Undefined}$$
Is this the correct way to convey that the limit does not exist? Or is there a mathematical way to show that $$\lim_{x \rightarrow \infty} \sin \frac{1}{x} = 0$$
Other than just knowing that 1 divide by an infinitely large number approaches $0$.","['calculus', 'limits']"
2148951,The 'd' symbol in integration and derivation,"What is a good way to think about the d symbol in derivation and integration? Every time I think I understand what it means, I see it in a new context that is incompatible with my previous ideas. Example: integrate: x dx + 5 . I used to think that the integration symbol is like an opening parenthesis and the d symbol is like the closing parenthesis, telling us that we don't want to integrate the + 5 . I used to think that the x in dx determines the variable that we integrate by. Now take a look at the left hand side of this equation from Khan Academy: Instead of integrating u(x)v'(x) by x , we integrate u by v . How are these things equal? How can we even integrate by v when it isn't a variable, it's a function! u doesn't necessarily even contain v , so how can we integrate u by v ? What kind of black magic happened here?","['derivatives', 'indefinite-integrals', 'integration']"
2148967,Dividing a connected shape into equal-sized connected pieces,"It's nearly pancake day here in the UK and I'll be making my kids pancakes. The last one in the batch is always a little oddly shaped, and I'd like to divide it into two pieces of equal area so they can share it. However, the children are fussy eaters and insist on their pieces being path-connected. Is this possible? Maybe there's an old theorem somewhere that would help me have a stress-free day? (n.b. I'm capable of making very precise and wiggly cuts with the tip of the knife, so model the cut as a continuous map $C:[0,1] \to \mathbb{R}^2$)","['reference-request', 'general-topology', 'measure-theory', 'geometry']"
2148973,Composing outer automorphisms with group representations,"In J.P. Serre's Linear Representations of Finite Groups, two representations $(\rho, V)$ and $(\rho', V')$ of a finite group $G$ are called isomorphic if there exists a linear transformation $T: V \to V'$ such that $T \cdot \rho = \rho' \cdot T$. In the case where $V = V'$, we would call these representations isomorphic if $\rho' = T \cdot \rho \cdot T^{-1} = c_T\circ \rho$, where $c_T$ is conjugation by $T$. Conceptually this makes sense to me because if we identify these two vector spaces using any choice of bases, then this definition essentially says that a change of basis, i.e. how we write the same transformations shouldn't change the representation. However, if we consider $k$-vector spaces where $GL_n(k)$ has non-trivial outer automorphisms, i.e. automorphisms which are not conjugation by some element, we could call two representations $\rho, \rho':G \to GL_n(k)$ isomorphic if $\rho = \phi\circ \rho'$ for any $GL_n(k)$ automorphism $\phi$. This seems similar and a good candidate because in this case too we can always recover a representation from any isomorphic copies, if we know what the isomorphism is, and because the actions of the group elements maintain their relations under such an isomorphism. An example of this sort of outer automorphism is complex conjugation in $GL_2 (\mathbb{C})$; this is not inner because for any matrix with a non-real determinant, the determinant changes, whereas inner automorphisms must preserve determinants. I can see that we don't actually define isomorphisms in this fashion because if we did we would lose the uniqueness of the characters up to isomorphism (at the very least). My questions are the following: If two representations are related in this fashion (i.e. $\rho = \phi \circ \rho'$), what can we say about them? If one is irreducible, is the other irreducible too? If so, how exactly do outer automorphisms act on the set of irreducible representations of a fixed dimension? Can we calculate the decomposition of one representation using the decomposition of the other? Is there any nice correspondence? What does the action of outer automorphisms do at the level of characters? Can this action be described intrinsically on the vector space of class functions? I think that there are a lot of other questions one could ask in this set-up; any remarks about this situation would be welcome. If there are better answers for similar cases (wherever the question makes sense), for instance locally compact or reductive groups, they would be welcome too.","['representation-theory', 'group-theory']"
2149047,Bias-variance decomposition,"I found the following in The Elements of Statistical Learning . Suppose we have 1000 training examples $x_i$ generated uniformly on $[-1,1]^p$. Assume that the true relationship between $X$ and $Y$ is 
$$Y = f(X) = e^{-8||X||^2}$$
without any measurement error. We use the 1-nearest-neighbour rule to predict $y_0$ at the test-point $x_0 = 0$. Denote the training set by $\mathcal T$. We can compute the expected prediction error at $x_0$ for our procedure, averaging over all such examples of size 1000. Since the problem is deterministic, this is the mean squared error (MSE) for estimating $f(0)$: \begin{align}\text{MSE}(x_0) &= E_\mathcal T[f(x_0)-\hat y_0]^2\\
&= E_\mathcal T[\hat y_0 - E_\mathcal T(\hat y_0)]^2 + [E_\mathcal T(\hat y_0)-f(x_0)]^2\\
&= \mbox{Var}_\mathcal T(\hat y_0) + \mbox{Bias}^2(\hat y_0) \end{align} I don't quite understand what they mean by the problem being deterministic. Also, how exactly do they get from the first line to the second line? I played around with the binomial formula but I can't seem to derive this.","['machine-learning', 'statistics', 'variance']"
2149060,The BV minimizing problem depends on a moving weighted parameter,"Let $v\in L^2(Q)$ be given, where $Q:=(0,1)\times(0,1)$ is an unit square. Define a sequence of parameter function $\alpha_s$ by
$$
\alpha_s(x):=
\begin{cases}
1&\text{ if }x\in(1/2+s,1)\times(0,1)\\
2&\text{ if }x\in(0,1/2+s)\times(0,1)
\end{cases}
$$
where $0<s<1/2$. Define 
$$
u_s:=\operatorname{argmin}\{\|u-v\|_{L^2(Q)}^2+|\alpha_su|_{TV(Q)}:\,\,u\in BV(Q)\}\tag 1
$$
where $BV$ denotes the bounded variation space and $TV$ denotes the total variation seminorm. My question: do we have $u_s\to u_0$ in $L^1$ as $s\to 0$? ($u_0$ is defined by letting $s=0$ in $(1)$) I am also wondering what if I change $(1)$ by replacing $BV$ with the Ambrosio-Tortorelli functional, i.e., \begin{multline}
(u_s,z_s):=\operatorname{argmin}\{\|u-v\|_{L^2(Q)}^2+\int_Q |\nabla u|^2(z^2+1)\alpha_sdx+\\
\int_Q[|\nabla z|^2+(1-z)^2]\alpha_s dx:\,\,u,z\in W^{1,2}(Q)\}\tag 2
\end{multline}
Then, do we have $(u_s,z_s)\to (u_0,z_0)$ in $L^1$? or even weakly in $W^{1,2}$? Thank you!","['real-analysis', 'partial-differential-equations', 'functional-analysis', 'sobolev-spaces', 'bounded-variation']"
2149062,Computing $7^{13} \mod 40$,"I wanted to compute $7^{13} \mod 40$. I showed that 
$$7^{13} \equiv 2^{13} \equiv 2 \mod 5$$
and
$$7^{13} \equiv (-1)^{13} \equiv -1 \mod 8$$. Therefore, I have that $7^{13} - 2$ is a multiple of $5$, whereas $7^{13} +1$ is a multiple of $8$. I wanted to make both equal, so I solved $-2 + 5k =  + 8n$ for natural numbers $n,k$ and found that $n = 9, k = 15$ gave a solution (just tried to make $3 + 8n$ a multiple of $5$. Therefore, I have that 
$$7^{13} \equiv -73 \equiv 7 \mod 40.$$ Is this correct? Moreover, is there an easier way? (I also tried to used the Euler totient function, but $\phi(40) = 16$, so $13 \equiv -3 \mod 16$, but I did not know how to proceed with this.)","['number-theory', 'proof-verification', 'elementary-number-theory']"
2149066,Proving $\lim _{ x\to\infty }{ f(x) }=\infty$ if $f'(x)>c$ for every $x$,"Given a differentiable function $f: (0,\infty) \rightarrow \mathbb R $ and $c>0$ such that $f'(x)>c$ for every $x$. Prove: $\lim _{ x\rightarrow\infty  }{ f(x) }=\infty$ Using the MVT, I got to $f(x)>c(x-x_0)+f(x_0)$, and I think I should proceed using the definition of limit, but I got stuck. Any help appreciated.","['derivatives', 'calculus']"
2149074,Locus of the points on complex plane,We have a complex plane and there are inequality on it: $2|z| > |1 + z^2|$. What is the locus of such set of $z$? I tried to solve it by using polar coordinates and got: $r^4 - 2r^2\cos(2\phi ) - 1 = 0$. As well I tried solve it using $z = x + iy$ and got: $4(x^2 + y^2) = (x^2 - y^2 + 1)^2 + 4x^2y^2$ By I still can`t figure out what is the explicit name or at least descritpion for such locus of points. Will be very grateful for any help.,"['complex-analysis', 'plane-geometry', 'complex-numbers', 'plane-curves']"
2149075,Monty hall problem with 4 doors and PLAYER choosing 2 initially,"Question: In a variation of the Monty Hall game show, you now have 4 doors and only one has a car behind it. The other 3 doors have goats. This time you choose (without opening them) 2 doors. Monty Hall opens one of the remaining doors and
shows that it has a goat. He then asks you “If you keep your two choices and the car is behind one of the two doors you chose, you win. But you can give up your two choices and open the remaining door and win the car if it is there.” In order
to maximize your chance of winning the car, should you take Monty’s offer or not? My Attempt: Initially we have ${{4}\choose{2}} = 6$ ways of choosing 2 doors. 
1) We have 3 ways of choosing two doors so that we are on door with car.  If we switch, we lose.
2) We have 3 ways of choosing two doors so that both of the doors have goats.  If we switch we win. Since we are equally likely if we switch or do not (in light of the equal number of ways we can choose doors so that we have the car or do not have the car), it does not make any difference, probability-wise to make the switch.","['monty-hall', 'probability', 'proof-verification']"
2149093,Expectation and orthogonal projection,"Many books while introducing the regression problem, start with the assertion that any random variable $Y$ can be decomposed into two orthogonal terms 
$$
Y= E[Y|X]+\epsilon.
$$
In the classical statistics $E[Y|X]$ is a shorthand for $E[Y|X=x]$ where $X$ is some ""controlled"" (non-random) variable. However in econometric research $X$ is a random variable, thus I guess that $E[Y|X]$ is a shorthand for $E[Y|\sigma(X)]$, where $\sigma(X)$ is a sigma algebra generated by $X$. Is it right interpretation? Another assertion is that $E[Y|X]$ is an orthogonal projection. What space does $Y$ projected onto (on $\sigma(X)$?)? I pretty well understand it from the algebraic point of view when
$$
y = \hat{y} + e,
$$ 
and $HY=X(X'X)^{-1}X'y$. In this case the orthogonality of $e$ w.r.t $\hat{y}$ has clear geometric interpretation ($H$ is an orthogonal projection of $y$ onto $C(X)$ and $e \in C(X)^{\perp}$). However, this is a post-hoc approach when we already observed the data points $\{y_i, x_{1i},...,x_{pi}\}_{i=1}^n$, while I'm interested in the stochastic process that generates it. To sum up, my questions are: If $X$ is random variable and defined on the same probability space as $Y$, why does an orthogonal decomposition of the kind 
$$
Y = E[Y|\sigma(X)]+\epsilon=h(X)+\epsilon
$$ 
exists?
How can I prove its existence (and uniqueness)? (I know it requires squared integrability of $Y$, but I have non-intuitive explanation how it is suffice for the decomposition to exist). Are the projections $E[Y|\sigma(X)]$ or $E[Y|X=x]$ project on $\sigma(X)$? If so, does it have any intuitive meaning (like in the linear Algebra analog) If $\epsilon$ defined on the same probability space, what it means to be orthogonal to $E[Y|\sigma(X)]$? Would appreciate any help.","['regression', 'probability-theory', 'measure-theory', 'statistics']"
2149106,Particular definition of e,"Show that, $$e=\lim_{x\to \infty} \left(1+\frac{1}{x}\right)^x $$ Is the same number that satisfies, 
$$\lim_{h\to0} \frac{e^{h}-1}{h} = 1 \tag{*}$$ You don't have to do that if it's too cumbersome, but (*) is used to find the derivatives of real exponential functions. I know that definitions are not to be proven but I'm looking for some derivation or intution for the claim that (*) is a legitimate definition for e, and furthermore, a proof that limit in (*) exists.","['intuition', 'limits', 'exponential-function', 'calculus', 'definition']"
2149108,Two topologies coincide if they have the same convergent nets,"There are many similar questions to mine in the site, but I'm still not sure.
Let $X$ be a vector space with $T_1$ and $T_2$ two topologies that make $X$ a TVS (Hausdorff). If I want to show that $T_1=T_2$, does it suffice to show that every converegent net $(x_{\lambda})_{\lambda}$ in $X$ to some $x\in X$ w.r.t. $T_1$ converges to $x$ w.r.t. $T_2$ and conversely? Thank you","['functional-analysis', 'general-topology']"
2149112,To prove that a mathematical statement is false is it enough to find a counterexample?,"I am trying to show if the following statements are true or false. Is it true that $|a + b| = |a| + |b|$ for general vectors $a$ and $b$? If $a \cdot b = a \cdot c$ for equally-sized non-zero vectors $a$, $b$, $c$, does it follow that $b = c$? For the first one I found a counterexample that shows that the statement is false. If vector $a=\langle 1,4,5\rangle$ and $b=\langle 2,2,2\rangle$ then $|a|+|b|=\sqrt{42}$+$2\cdot\sqrt{3}=9.945$, and then, $|a+b|=\sqrt{1^2+4^2+5^2+2^2+2^2+2^2}=7.348$, then we can conclude that $9.945 \ne7.348$ and the statement is false. Also by the triangle identity $|a + b| \le |a| + |b|$ For the second statement I also found an counterexample that proves that is false.
If vector $a=\langle 1,0,0\rangle$, $b=\langle 0,1,0\rangle$ and $c=\langle 0,0,1\rangle$, we will obtain the following dot product: $a \cdot b = 0$ $a \cdot c = 0$ then $a \cdot b = a \cdot c = 0$ and $b \ne c$ My question is: To prove the two statements is it enough to find a counterexample and say if it is true or false. Or should I try to provide a more mathematical proof like induction or contradiction?","['induction', 'linear-algebra', 'proof-verification']"
2149124,Linear connection and covariant derivative: help needed to clear up confusion in extension of definitions,"Let $\nabla$ be a linear connection defined on the tangent bundle of a manifold $M$ . We have, with $X(M)$ being the global sections module of $TM$ , $$\nabla: X(M)\to \Omega^1(M)\otimes X(M)$$ We can extend it as a derivation of degree 1 on the $\Omega^*(M)\otimes X(M)$ complex by the formula $$\nabla(\omega\otimes X)=d\omega\otimes X+(-1)^{r}\omega\wedge\nabla(X)$$ where $\omega\in\Omega^r(M)$ and $X\in X(M)$ On the other hand, a covariant derivative defined on $TM$ extends in a unique way to the duals of vector fields (i.e., covector fields), and to arbitrary tensor fields, that ensures compatibility with the tensor product and trace operations (tensor contraction). For instance, for $\omega\in \Omega^r(M)$ and $X, Y\in X(M)$ , we get $$\nabla_Y(\omega\otimes X)=\nabla_Y(\omega)\otimes X+\omega\otimes\nabla_Y(X)$$ I am confused here because I cannot reconcile the two formulas. What am I misunderstanding?",['differential-geometry']
2149146,Holomorphic functions and real functions: continuity of partial derivatives,"Given $f: A \subset \mathbb{C} \rightarrow \mathbb{C}$ a holomorphic function, I can represent the function as $f=u+iv, \  u,v:A \rightarrow \mathbb{R}$ and so $f$ can be seen as a function from $A' \subset \mathbb{R}^2$ to $\mathbb{R}^2$ under the identification of $\mathbb{C} \ni z=x+iy$ with $(x,y) \in \mathbb{R}^2$ and of $f$ with $F=(u,v)$. Now, the problem arise (for me) in two different direction, the first: A theorem in [Markushevich] [""Theory of functions of a complex variable""] 1 says that if we look at $f$ as $F$ (notation above) then $f$ is holomorphic if and only if $F$ is real differentiable and solve Cauchy-Riemann conditions. Now, this gives us a good method to say when $f$ is holomorphic (at least when we can manage somehow the differentiability of $F$), but (and so the book continues) there is a nice sufficent condition on $F$ which is the continuity of partial derivatives of $u(x,y)$ and $v(x,y)$ (plus C-R equations) to imply holomorphicity of $f$. And that's ok, but this is only a sufficent condition. Wikipedia's page on holomorphic functions seems to do confusion: it says (in ""Properties"" ) If one identifies $\mathbb{C}$ with $\mathbb{R}^2$, then the holomorphic functions coincide with those functions of two real variables with continuous first derivatives which solve the Cauchy–Riemann equations, a set of two partial differential equations. Is it me, or this is wrong? Second question: how the fact that $f$ is holomorphic, and so analytical, relates with $u(x,y)$ and $v(x,y)$? The problem, to me, arises when I try to show that a holomorphic function $f=u+iv$ solve the equation $\frac{\partial^2u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} = 0$. i) How can I say that the second (partial) derivatives of $u$ exists?
ii) What about the mixed (partial) derivatives of $u$ which are supposed to cancel? How the symmetry requirements on $u$ are satisfied? In summary: which are some good characterisations of holomorphicity in terms of $u,v$? Thank you in advance","['complex-analysis', 'holomorphic-functions', 'calculus']"
2149149,How does one show that $\sum_{k=1}^{n}\sin\left({\pi\over 2}\cdot{4k-1\over 2n+1}\right)={1\over 2\sin\left({\pi\over 2}\cdot{1\over 2n+1}\right)}?$,"Consider $$\sum_{k=1}^{n}\sin\left({\pi\over 2}\cdot{4k-1\over 2n+1}\right)=S\tag1$$
  How does one show that $$S={1\over 2\sin\left({\pi\over 2}\cdot{1\over 2n+1}\right)}?$$ An attempt: Let $$A={\pi\over 2(2n+1)}$$ $$\sin(4Ak-A)=\sin(4Ak)\cos(A)-\sin(A)\cos(4Ak)\tag2$$ $$\cos(A)\sum_{k=1}^{n}\sin(4Ak)-\sin(A)\sum_{k=1}^{n}\cos(4Ak)\tag3$$ $$\sin(4Ak)=4\sin(Ak)\cos(Ak)-8\sin^3(Ak)\cos(Ak)\tag4$$ $$\cos(4Ak)=8\cos^4(Ak)-8\cos^2(Ak)+1\tag5$$ Substituting $(4)$ and $(5)$ into $(3)$ is going to be quite messy. So how else can we prove $(1)$?","['trigonometry', 'sequences-and-series']"
2149179,Measure theory for Fourier Analysis,"I intend to study Fourier Analysis, but lack the necessary background in measure theory. I have studied basic analysis and know uniform convergence etc. Are there some resources that contain the required measure theory and are brief? Also, do I need to know any functional analysis as well?","['real-analysis', 'fourier-analysis', 'reference-request', 'book-recommendation', 'measure-theory']"
2149233,Mirror Symmetry of Calabi-Yau Surfaces?,"This isn't a terribly refined question, but more broad-brush: are there nice results on explicit mirror pairs of certain Calabi-Yau surfaces?  In particular, I'm curious if we know the mirror partners of the smooth, non-compact resolutions of the singular surfaces $\mathbb{C}^{2}/\mathbb{Z}_{N+1}$, as well as $\mathbb{C}^{2}$ itself.  I was hoping someone could sort of briefly summarize any big results in this area, maybe also with sources.  Also, I can see that there's an extensive literature on mirror symmetry of K3 surfaces, but I'm having a tough time navigating it, for now. (What got me thinking about this is the fact that the elliptic genus apparently satisfies $$\text{Ell}_{y,q}(X) = (-1)^{\text{dim}X}\text{Ell}_{y,q}(\tilde{X})$$ where $X$ and $\tilde{X}$ are mirror pairs.  Hence, for surfaces the elliptic genus coincides for the mirror partners.  By the Dijkgraaf-Moore-Verlinde-Verlinde formula, this would seem to imply that the generating function of the elliptic genera of the Hilbert Schemes of points also coincides for both partners, which seems very non-trivial to me.)","['complex-geometry', 'algebraic-geometry', 'mathematical-physics', 'symplectic-geometry', 'mirror-symmetry']"
2149271,Sufficiently rich probability space,"I have seen the phrase ""sufficiently rich probability space"" used a fair often in probability theory. For example, at the bottom of the first page here . What does the phrase ""sufficiently rich"" mean?",['probability-theory']
2149283,Math behind the match stick game in The Goal,"I just finished reading The Goal by Eliyahu M. Goldratt. It's a good book, and from my understanding, pretty well known. Goldratt is a a physicist who turned his scientific scrutiny to production line management, and his book is about how mathematical thinking can help your business. In it, he describes a game used to illustrate the flow of a production line. It goes like this: There are five people sitting in a line with a few boxes of matches at one end. The goal is to move as many matches as possible to the end of the line. In one turn, the first first person in the line rolls a six sided die and moves that number of matches down the line. The next person then rolls the die and moves that number of matches down the line, and so on. That's one turn. You repeat that process a given number of times and then count the number of matches moved all the way through (""throughput"") and count the matches that are still sitting in between the people waiting to be moved (""inventory""). You want to maximize throughput and minimize inventory build up. To be explicit, if on the first turn the first player rolls a 4 and the second player rolls a 6, they can only move 4 because there's only 4 available to move. But if there are more matches waiting in the inventory (perhaps because the person before them is getting high rolls while they've been getting low rolls), then they take from that pile. So if there are 2 already in inventory, player 1 rolls a 4 and player 2 rolls a 6, there are 6 available matches, so they can move all 6 down the line. Hopefully that's clear. Here is the question, or rather questions: What is the expected value for throughput after $n$ turns? What about the expected inventory waiting for each person after $n$ turns? How does the number of people affect these values? What if the dice are more abstract, like a 17 sided die with four 6s, three 12s, etc. Just an abstract probability distribution? What about an expected value for the amount of rolls that were ""wasted""? So if there was 2 in inventory and you rolled a 5, that would be 3 units ""wasted"" Can we find more details about the distribution, rather than just the expected value?","['probability-distributions', 'statistics', 'probability', 'dice', 'combinatorics']"
2149321,Class group of $\mathbb{Q}(\sqrt{7})$,"Find the class group of $\mathbb{Q}(\sqrt{7})$ . $\textit{Hint}$ : notice that $2=(3+\sqrt{7})(3-\sqrt{7})$ and that $-1+\sqrt{7}=(2+\sqrt{7})(3-\sqrt{7})$ Here's what I've done: the Minkowski bound is $\sqrt{7}\approx 2.65$ , so we need to analyse $2\mathcal{O}$ . Since $x^2-7=(x+1)^2(\text{mod }2)$ , then $(2)=\mathfrak{p}^2$ , where $\mathfrak{p}=(2, 1+\sqrt{7})$ . $[\mathfrak{p}]$ has order $\leq 2$ because $[\mathfrak{p}]^2=[(2)]=e$ . If $\mathfrak{p}=(a+b\sqrt{7})$ for some $a+b\sqrt{7}\in\mathcal{O}$ , then $(2)=\mathfrak{p}^2=((a+b\sqrt{7})^2)$ , so $2=u(a+b\sqrt{7})^2$ for some unit $u$ . The only units in $\mathbb{Z}[\sqrt{7}]$ are $(8+3\sqrt{7})^n$ for some $n\in\mathbb{Z}$ . Obviously $n$ cannot be even, because that would mean $2=w^2$ for some $w\in\mathbb{Z}[\sqrt{7}]$ (absurd), so we must have $2(8+3\sqrt{7})=w^2$ for some $w\in\mathbb{Z}[\sqrt{7}]$ , which is also impossible by simple verification. Therefore $[\mathfrak{p}]\neq e$ and $Cl_{\mathbb{Q}(\sqrt{7})}=\mathbb{Z}/2\mathbb{Z}$ . That took a little bit of work, and I suppose the tip could be useful, but I dont see how.","['number-theory', 'abstract-algebra', 'algebraic-number-theory']"
2149333,How to deduce $\sin A+\sin B+\sin C=4\cos\frac A2\cos\frac B2\cos\frac C2$ from $A+B+C=\pi$?,"If $A+B+C=\pi$,$$\sin A+\sin B+\sin C=4\cos\dfrac A2\cos\dfrac B2\cos\dfrac C2\tag1$$$$\sin A+\sin B-\sin C=4\sin\dfrac A2\sin\dfrac B2\cos \dfrac C2\tag2$$$$\cos A+\cos B+\cos C=4\sin\dfrac A2\sin\dfrac B2\sin\dfrac C2+1\tag3$$$$\cos A+\cos B-\cos C=4\cos\dfrac A2\cos\dfrac B2\sin\dfrac C2-1\tag4$$$$\tan A+\tan B+\tan C=\tan A\tan B\tan C\tag5$$$$\cot\dfrac A2+\cot\dfrac B2+\cot\dfrac C2=\cot\dfrac A2\cot\dfrac B2\cot\dfrac C2\tag6$$ Formulae $(1)$ through $(6)$ were given with the condition that $A+B+C=180^{\circ}$. I'm not sure how to arrive at them. Question: How do you arrive at $(1)$ through $(6)$? I need a place to start. I am well aware that$$\sin A+\sin B=2\sin\dfrac {A+B}2\cos\dfrac {A-B}2$$And$$\cos A+\cos B=2\cos\dfrac {A+B}{2}\cos\dfrac {A-B}2$$
However, I'm not sure how to get $\sin A\pm\sin B\pm\sin C$. I'm guessing it has something to do with the expansion of $\sin(A+B+C)$. Note: In your answer, give a hint on where I can begin, then hide the rest of your answer .","['plane-geometry', 'trigonometry']"
2149340,How to prove this $\pi$ formula? [duplicate],"This question already has answers here : How to sum this series for $\pi/2$ directly? (5 answers) Closed 6 years ago . I am hoping to find out where the formula 
$$\frac{\pi}{2}=\sum_{k=0}^{\infty}\frac{k!}{\left(2k+1\right)!!}$$
comes from. I can't see how one could begin to prove it.","['factorial', 'sequences-and-series', 'pi']"
2149349,Show that $x^e\le e^x$ for all $x\gt 0$ and $x \in \mathbb {R}$,"I understand that this question may seem quite simple, but although I can see different ways of showing this, I don't understand how it follows from the context I was given (i.e why the second part of the question begins with ""hence""). It may also help to bear in mind that I have only just started teaching myself calculus and so far I have only covered differential calculus up to the level taught in secondary schools. The question is in two parts (I understand that to answer (i) you simply find $f'(x)$ and evaluate $f'(e)$ to show that it is equal to zero and is therefore, since there is only one stationary point which is a maximum point as implied by the question, the maximum point): $f(x) = {\ln x\over x}$, $x\gt 0$ (i) Show that the maximum point on the graph of $y = f(x)$ occurs at the point $\left(e,\frac{1}{e}\right)$. (ii) Hence, show that  $x^e\le e^x$ for all $x\gt 0$ Any help would be greatly appreciated.","['derivatives', 'functional-inequalities', 'calculus']"
2149360,Evaluating $\int\sqrt{1-\sin x}\ dx$,"One of the method to find the integral $$\int\sqrt{1-\sin x}\ dx$$ is by multiplying by $\dfrac{1+\sin x}{1+\sin x}$ inside the root. Then, by using the identity $\sin^2x+\cos^2x=1$ , we get $$\int\dfrac{\sqrt{\cos^2x}}{\sqrt{1+\sin x}}\ dx$$ The next step is we remove the square with the root and using the substitution $u=\sin x$. My question is why ? Why don't we put an absolute value of $\cos x$? So, we have two answers. Is this situation always true in any similar situation in indefinite integrals? Sorry, if my question is trivial. Thanks","['integration', 'calculus']"
2149381,Asymptotic to a sequence of algebraic numbers.,Let $f(n)$ be the largest real solution of $$x^n - x^{n-1} = 1 $$ As $n$ grows to positive infinity we get the asymptotic : $$ f(n) = 1 + \frac{\ln(n)}{n} + \frac{\exp(2)}{n^2} + ...$$ Where the value $\exp(2)$ is optimal ! ( and $...$ means smaller term(s) ) Notice $f(2)$ is the golden mean. How to show this asymptotic  ? Edit Corrected the formula.,"['polynomials', 'limits', 'roots', 'exponential-function', 'asymptotics']"
2149389,"Is the growth rate of certain sets in $\mathbb Z$ is eventually linear?,","This question is related to Gromov's theorem in that it is about the growth rate with respect to some set of generators of $\mathbb Z$. Gromov's Theorem, along with the Bass-Guivarc'h formula, tells us that for any finite symmetric set of generators $S$ of $\mathbb Z$, if we let
$$B_n = \{x \in \mathbb Z : \exists k \leq n, s_1, \ldots, s_k \in S \text{ such that }n = s_1 + \cdots + s_k\}$$
and $c(n) = \left|B_n\right|$, then $c(n) \in \Theta(n)$ since $\operatorname{rank} \mathbb Z = 1$. Computational evidence leads me to expect that there is a stronger condition on $c(n)$: I conjecture that $c(n)$ is necessarily linear for large enough $n$, that is, that there are some integers $N, a, b$ such that for all $n \geq N$, we have $c(n) = an + b$. Is this conjecture true or not? Can anyone think of a proof either way? EDIT: Computing the growth rates induced from various generating sets makes me think that in fact we have $c(n)$ linear for all $n$ such that $\{-\max S, \ldots, -1, 0, 1, \ldots, \max S\} \subseteq B_{n-1}$, or equivalently for all $n > \max d^{-1}[\{-\max S, \ldots, 0, \ldots, \max S\}]$, where $d : \mathbb Z \to \mathbb N$ is given by $d(k) = \min\{n \in \mathbb N : k \in B_n\}$; this gives a very precise bound on where the linearity must start, and in most cases this bound is actually sharp. EDIT 2: Allow me to describe some of my observations. The Mathematica code I used is provided below. $S = \pm \{4, 7\}$. Here are some plots which tell about the behavior of the $B_n$ and of $c(n)$: The upper plot, with all the dots, shows the nonnegative elements of each $B_n$ for $n = 0, 1, \ldots, 10$. ($B_n$ is always symmetric, so we do not need to show both sides.) Note that $\max S = 7$, and observe from this plot that $B_n$ contains $\{-7, -6, \ldots, 6, 7\}$ for $n \geq 5$. Now, the lower-left plot is just $c(n)$ vs. $n$, and the lower-right plot is of the differences $c(n) - c(n - 1)$. Gromov's Theorem says that $c(n) \in \Theta(n)$, and indeed the lower-left plot shows that $c(n)$ looks linear. The lower-right plot strongly suggests that $c(n)$ is exactly linear for $n > 5$, with formula determined to be $c(n) = 14n - 9$. Looking back at the various $B_n$, one may observe that the right ""edge"" of each $B_n$ is exactly the same for $n \geq 5$, which is itself close to a proof that $c(n)$ is linear past this point. $S = \pm \{1, 2, 9\}$. Here are the plots: Here again my conjectures seem to be confirmed: the lower-right plot suggests that $c(n)$ is eventually linear with slope 18, and furthermore $\max S = 9$, and we see from the upper plot that for all $n \geq 3$, $\{-9, -8, \ldots, 8, 9\} \subseteq B_n$, and from the lower-right plot again that $c(n)$ is linear on $n \geq 3$. (It in fact seems that the bound on where linearity begins is sharp exactly when $1 \not\in S$, and needs to be shifted left by one when $1 \in S$.) Here is the code. gens specifies the positive portion of the generators you wish to use (e.g. if gens = {2, 3} , then $S = \{-3, -2, 2, 3\}$), while iters and countsiters just say how much data to show: iters is the number of balls $B_n$ to display in the first plot, and countsiters is the number of ball-sizes $c(n)$ to display in the lower plots. (* input parameters *)
gens := {6, 7}
iters := 10
countsiters := 20

(* computation *)
MinkowskiSum[lst__] := DeleteDuplicates[Total /@ Tuples[{lst}]]
MinkowskiProduct[lst__] := 
 DeleteDuplicates[Apply[#1 #2 &, Tuples[{lst}], {1}]]
Clear[b]
allgens := MinkowskiProduct[gens, {-1, 0, 1}]
b[-1] := {}
b[0] := {0}
b[n_] := b[n] = MinkowskiSum[b[n - 1], allgens]
c[n_] := Length[b[n]]

(* plots *)
ListPlot[Table[{#, -n} & /@ Select[b[n], # >= 0 &], {n, 0, iters}], 
 AspectRatio -> 3*iters/(Max[b[iters]] - Min[b[iters]]), 
 GridLines -> {Range[0, iters Max[allgens]], Range[-iters, 0]}, 
 Ticks -> {Range[0, iters Max[allgens], 5], Range[-iters, 0]}, 
 ImageSize -> Full]
{DiscretePlot[c[n], {n, 0, countsiters}, ImageSize -> Large], 
 DiscretePlot[c[n] - c[n - 1], {n, 0, countsiters}, 
  ImageSize -> Large]}","['group-theory', 'geometric-group-theory']"
2149408,Multivariable dual numbers and multivariable automatic differentiation,"I understand the basics of how dual numbers work, as well as how they are used for automatic differentiation, as described here: Dual Numbers & Automatic Differentiation I was wondering, how would you extend this concept to get partial derivatives of a function? Basically I have a multivariable function, and I'd like to calculate it's value and gradient for a specific input. I started off by looking at how multiplication of dual numbers is derived for a function of a single variable of the form $y=f(x)$. (Note the $\epsilon^2$ in the last step turns to 0 which makes that term disappear): $(a+b\epsilon)*(c+d\epsilon) = \\
ac+(bc+ad)\epsilon+bd\epsilon^2 = \\
ac + (bc+ad)\epsilon$ That made me think that maybe I could just have an $\epsilon$ defined per variable in a $z=f(x,y)$ function, so I gave it a shot. (Note that the $x^2$ and $y^2$ terms disappear below for the same reason as above): $
x=\epsilon_x \\
y=\epsilon_y \\
(a+bx+cy)*(d+ex+fy)= \\
ad+(ae+bd)x+(af+cd)y+(bf+ce)xy+bex^2+cfy^2= \\
ad+(ae+bd)x+(af+cd)y+(bf+ce)xy
$ This looks pretty good except for the $xy$ term, which I have no idea how to account for in the gradient, or how to interpret. Can anyone help me out towards understanding how to do multivariable automatic differentiation?","['derivatives', 'partial-derivative']"
