question_id,title,body,tags
2655824,How would I be able to reduce this boolean expression? $(b + d)(a' + b' + c)$,"So, I have this boolean expression and I have to simplify it, here is what I am doing: $(b + d) * (a' + b' + c)$ *Opening the expression by multiplication $= a'b + bb' + bc + a'd + b'd + cd$ $= a'b + bc + a'd + b'd + cd$ But when I go and check the answer using an online simplifier, the answer is: ANS = $a'b + bc + b'd$ But I can't think of any steps to reduce my equation to the required answer. Any help would be appreciated.","['boolean-algebra', 'propositional-calculus', 'logic', 'discrete-mathematics']"
2655859,Number of subsets with subscripts being a multiple of each other?,"Let set $S = \{a_1, a_2, a_3, \ldots, a_{12}\}$, where all 12 elements are distinct. We wish to form subsets, each of which contains one or more of the elements of set $S$ (including the possibility of using all the elements of $S$). The only restriction is that the subscript of each element in a specific set must be an integer multiple of the smallest subscript in that set. For example, $\{a_2, a_4, a_6\}$ is an acceptable set, as is $\{a_6\}.$ How many such sets can be formed? My approach to this problem was to start off with using a1 as the lowest subscript, leading to 2^11 options for the rest of the subsets that use a1. I then repeated the process up to a6, and added 1 for the rest of the subsets a7 through a10. What am I doing wrong?","['combinatorics', 'contest-math', 'elementary-set-theory']"
2655900,Doob's continuous martingale convergence theorem,"I present the theorem and the part of the proof where I am having trouble understanding. This is a from lecture note I found online. Here $\mathcal{D}_n=\{k/2^n|k\leq 0\}$, $\mathcal{D}=\cup_n \mathcal{D}_n$. I don't understand how Fatou's lemma was used to get the second inequality. Any help is appreciated. Thanks!","['stochastic-processes', 'real-analysis', 'probability-theory', 'proof-verification', 'martingales']"
2655934,Proving in Quine's New Foundations,"I'm reading Quine's New Foundations paper. However, there are a lot of questions I do not manage to answer. I would say they all lead to the question: how to prove things in NF? For example, is it provable in NF that $\forall x (x \in \{x\})$ or commutativity of identity, being this latter defined as $x = y \stackrel{def}{=} \forall w (x \in w \rightarrow z \in w)$ and finally how do you prove - if possible - that $x = y \wedge z \in x \rightarrow z \in y$ with just the axioms and definitions Quine provides in his paper? I came up with these questions in connection with something Quine says in his paper. He says that the ""unrestricted"" abstraction principle $\exists x \forall y (y \in x \leftrightarrow \phi)$ for $x$ not occurring in $\phi$, provides a class $x$ about which he says ""viz. $\widehat{y} \phi$"". Now, I think this means that from the abstraction principle you can prove $\widehat{y}\phi$ for any $\phi$ of the required kind, is it so? I do not even manage to prove that $x = y \rightarrow x \subset y \wedge y \subset x$","['logic', 'elementary-set-theory', 'foundations']"
2655942,Test whether we can infer that the population means differ,"Naturally, this is very insignificant and we fail to reject the null hypothesis. Is this the right calculation?","['statistics', 'hypothesis-testing']"
2655973,Intuition with Complex Derivatives of a Complex Variable.,"I'm having some trouble wrapping my mind around derivatives of a complex function of a complex variable.This is a function of two real inputs and two real outputs, correct? What confuses me is that in multivariable calculus, the functions were of multivariable input, and had one real output.  So when calculating the derivative, you'd have to know what vector you're calculating it on. In the complex space however, with two inputs and two outputs, I feel like this would also be the case? But it seems like we can use all the standard rules from single variable calc. What am I misunderstanding?
Thanks!","['intuition', 'complex-analysis']"
2656033,How to find the MLE of multiple parameters [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Suppose $X_1,\ldots,X_n,Y_1,\ldots,Y_n$ are independent exponential r.v., where the density of $X_i$ is $f_i(x)= \lambda_i\theta \exp(-\lambda_i \theta x_i)$ for $x\geq0$, while the density of $Y_i$ is $g_i(x) = \lambda_i \exp(-\lambda_i x_i)$  for $x\geq0$. Find the MLE of theta (based on $X_1,\ldots,X_n,Y_1,\ldots,Y_n$). Find the MLEs of $\lambda_i$ for each $i$.",['statistics']
2656057,How can a cone have non-zero Riemann curvature yet can be made out of a piece of paper?,"A cone with a $90^\circ$ vertex angle can be parameterized by $$(x, y, z) = (z \cos \theta, z \sin \theta, z).$$ The metric on the cone can then be found to be $$dx^2 + dy^2 + dz^2 = 2 dz^2 + z^2 d \theta.$$ The only non-zero Christoffel symbols are $$\Gamma^z_{\space \space \theta \theta} = -\frac{1}{z} \hspace{1 cm}\Gamma^\theta_{\space \space \theta z} = \frac{z}{2}$$ and the only non-zero independent component of the Riemann curvature tensor is $$R_{z \theta z \theta} = 2 z^{-2} + 1.$$ It's possible I made a mistake in the above computations, but I believe they are correct. This shows that a cone is not locally flat. However, a cone in real life can be formed by curving a piece of paper. For example, to compute the surface area of a cone, you can consider the surface area of a pac-man shape laying flat on a table. When the top and bottom of the mouth of the pac-man are brought together, the paper is in the shape of a cone. The Christoffel symbols and Riemann tensor contain no information on the embedding of a manifold in ambient space. Therefore, it would seem to me that the Riemann tensor would have to be $0$, as it would be for flat piece of paper. How can this be?",['differential-geometry']
2656086,When is the Euler-Lagrange equation trivially satisfied?,"In calculus of variations, we seek the functions $y(x)$ extremising the quantity $$I = \int_{x_1}^{x_2} F(x, y, y') \, dx.$$ To find these functions, we solve the Euler-Lagrange equation $$\frac{\partial F}{\partial y} = \frac{d}{dx}\left[\frac{\partial F}{\partial y'} \right].$$ Sometimes the E-L equation is trivially satisfied. This means that given boundary conditions $y(x_1) = y_1$ and $y(x_2)=y_2$, all functions $y(x)$ are extremals, i.e. $I$ is independent of the path $y$. In such a situation, I understand that the Lagrangian $F$ simplifies to a total derivative. Here are two examples: 1) $I = \int_{x_1}^{x_2} (y^2 + 2xyy') dx$. We compute $\frac{\partial F}{\partial y} = 2y + 2xy'$ and $\frac{d}{dx} \left[ \frac{\partial F}{\partial y'} \right] = \frac{d}{dx} \left[ 2xy \right] = 2y + 2xy'$. We then show path-independence by writing $I = \int_{x_1}^{x_2} (y^2 + 2xyy') dx = \int_{x_1}^{x_2} \frac{d}{dx} \left[ xy^2 \right] dx = \left[ xy^2 \right]_{x_1}^{x_2}$. 2) $I = \int_{x_1}^{x_2} (\frac{y'}{y}) dx$. We compute $\frac{\partial F}{\partial y} = -\frac{y'}{y^2}$ and $\frac{d}{dx} \left[ \frac{\partial F}{\partial y'} \right] = \frac{d}{dx} \left[ \frac{1}{y} \right] = -\frac{y'}{y^2}$. We then show path-independence by writing $I = \int_{x_1}^{x_2} \frac{y'}{y} dx = \int_{x_1}^{x_2} \frac{d}{dx} \left[ \log |y| \right] dx = \left[ \log |y| \right]_{x_1}^{x_2}$. I want to know how to figure out what the Lagrangian is the total derivative of. In the two examples above I got the total derivatives by trial and error, but I would like to know what the answer is in general.","['euler-lagrange-equation', 'partial-derivative', 'calculus', 'calculus-of-variations', 'ordinary-differential-equations']"
2656124,"$\dim H^0(X, \mathcal{O}_D) \leq 1 + \deg D$ when $-1 \leq \deg D \leq g - 1$","$X$ is a compact Riemann surface of genus $g$. I'm starting to wonder whether this statement (in my title) is true at all or whether it's a typo...it seems impossible to prove using just the Riemann-Roch theorem, unless I'm not seeing something simple. Do I need to use more than just Riemann-Roch? I can only do the case $\deg(D) = -1$ right now. I'll just explain my notation here, I'm using Otto Forster's notation from his book Lectures on Riemann Surfaces. $D$ is a divisor on $X$. $H^0(X, \mathcal{O}_D)$ is obviously the 0-th cohomology group of X with respect to the sheaf $\mathcal{O}_D$ where $\mathcal{O}_D (U) = \{ f \in \mathcal{M}(U)| \mathrm{ord}_x(f) \geq -D(x) \, \forall x\in U  \}$ and $\mathcal{M}$ is the sheaf of meromorphic functions. The version of Riemann-Roch with which I'm familiar is: for a compact Riemann surface $X$ of genus $g$ and a divisor $D$ on it, $\dim H^0(X, \mathcal{O}_D) - \dim H^1(X, \mathcal{O}_D) = 1 - g + \deg D$. I also tried using the consequence of the Serre Duality which asserts that $\dim H^1(\mathcal{O}_D) = \dim H^0(X, \Omega_{-D})$ where $\Omega_{-D}$ is the sheaf of meromorphic 1-forms that are multiples of $-D$. But I just cannot get the above inequality! Could I please get a hint?","['riemann-surfaces', 'sheaf-theory', 'complex-geometry', 'algebraic-geometry', 'sheaf-cohomology']"
2656227,You are taking a multiple-choice test with n questions each of which has 4 alternatives. You have mastered 60% of the material,"You are taking a multiple-choice test with n questions each of which has 4 alternatives. You have mastered 60% of the material. Assume this means that you have a 0.6 chance of knowing the answer to a random test question, and that if you donâ€™t know the answer to a question then you randomly select among the four answer choices. Assume that this holds for each question, independent of the others, and assume that each correct answer gives 1 point and wrong answers give 0 points, the score is the sum of all points.  For each answer define a random variable Xi (i=1,2,...,n) that takes the value 1 if the  ith answer is correct and 0 otherwise. 
a.What is the probability that you answer a particular question correctly? 
b.What is your expected score on the exam? 
c.Write down a formula for the probability mass function (pmf) for one particular X, obtain the cumulative distribution function (CDF) for Xi and plot the CDF 
WORK: for my work so far I have A = Knowing the answer B = All choices are equal and C = Student answers correctly. P(A) = .6, P(B) = .25 I am looking for P(A|C)? = P(C|A)P(A)/P(C)? Other than that I am kind of lost","['statistics', 'probability']"
2656231,Proving that softmax converges to argmax as we scale x,"For a vector $\mathbb{x}$ , the softmax function $S:\mathbb{R}^d\times \mathbb{R}\rightarrow \mathbb{R}^d$ is defined as $$
S(x;c)_i = \frac{e^{c\cdot x_i}}{\sum_{k=1}^{d} e^{c\cdot x_k}}
$$ Consider if we scale the softmax with constant $c$ , $$
S(x;c)_i = \frac{e^{c\cdot x_i}}{\sum_{j=1}^{d} e^{c\cdot x_j}}
$$ Now since $e^x$ is an increasing and diverging function, as $c$ grows, $S(x)$ will emphasize more and more the max value. At $c \rightarrow \infty$ , $S(x)$ outputs a one-hot vector with 1 at the position of the maximum element. Now this is my intuition, but how do I prove this?","['proof-writing', 'probability']"
2656232,Expected number of turns in dice throwing,"I generated a transition probability matrix for a scenario where I throw five dice and set aside those dice that are sixes. Then, I throw the remaining dice and again set aside the sixes - then I repeat this procedure until I get all the sixes. $X_n$ here represents the number of dices that are sixes after n rolls. $$\begin{pmatrix}\frac{5^5}{6^5} & \frac{3125}{6^5} & \frac{1250}{6^5} & \frac{250}{6^5} & \frac{25}{6^5} & \frac{1}{6^5}\\\ 0 & \frac{625}{6^4} & \frac{500}{6^4} & \frac{150}{6^4} & \frac{20}{6^4} & \frac{1}{6^4} \\\ 0& 0 & \frac{125}{6^3}& \frac{75}{6^3}& \frac{15}{6^3} & \frac{1}{6^3} \\\ 0 & 0& 0& \frac{25}{6^2}& \frac{10}{6^2}& \frac{1}{6^2}& \\ 0 & 0 & 0 & 0 & \frac{5}{6} & \frac{1}{6} \end{pmatrix}$$ I want to figure out how many turns it takes for me on average to get all sixes. I'm not even sure where to start with this problem. Is it a right approach to write a program where I calculate $P^n$ and see when the 6th column all equals to 1? Any pointers would be greatly appreciated.","['markov-chains', 'statistics', 'probability']"
2656286,When does a manifold with boundary admit Riemannian metric?,"A paracompact manifold admits a Riemannian metric. Does this hold for a manifold with boundary? If not, then under what conditions does it admit a Riemannian metric? Say, is it true for a compact manifold (with boundary)? Not a duplicate: In this question, it is said that the metric can be extended from the boundary; however, the question assumes the manifold to be already Riemannian, while my question is whether any (say, para compact) manifold with boundary can be made Riemannian in the first place.","['manifolds', 'metric-spaces', 'riemannian-geometry', 'differential-geometry']"
2656287,Random tree generation probability problem,"Given a tree-structure with a root node, the node can either get zero, one or two children, all with the same probability of 1/3. The children also has the same probability of getting zero, one or two children, which is 1/3. Recursively the algorithm creates the tree. What is the probability that the tree has more than N nodes in total? Came across this question after programming a random tree generator and got really curious, would be cool if someone knew the answer.","['probability-theory', 'probability', 'programming']"
2656293,What does the notation $S^n$ mean?,"I see this notation a lot, but I can't seem to find an answer in my google searches. In particular I am looking at the question Let $\alpha:S^n\rightarrow S^n$ be the antipodal map. Prove that $n$ is odd, then $\alpha\simeq \operatorname{id}.$ If it is an arbitrary symmetric group, then I understand, but I have seen $S^1$ be used for the circle.","['algebraic-topology', 'general-topology', 'notation']"
2656307,Proving this group is Abelian,"Let $(G,.)$ be a group where there exists an element $g \in G$ such that
for any $x \in G$ it is the case that $x^3 = gxg$. I've been stumped on this one. All I have found is that $e = e^3 = geg = g^2$. Does anyone have advice on some starting points to solve this? Thanks",['abstract-algebra']
2656319,Probability Problem based on the a set of numbers. Determine if the number is divisible by three,"Three distinct numbers are selected at random from the set $\{1,2,3,4,5,6\}$. What is the probability that their product is divisible by $3$? I think that since because $3$ and $6$ are the only numbers that divide into three with an integer remainder, the number would have to have $3$ or $6$ in the unit's digit. Since this is the case the probability would be  $2/6$ or $1/3$. Have I done anything wrong?",['probability']
2656320,How do I factorize this exact differential?,I know for a fact that $$(x+y)dx + (x-y)dy$$ is an exact differential since their partial derivatives are the same (both equates to 1). How do I find that $df$ that could capture this entire equation? I can't see how since there is a $-1$ to the $y$. My closest guess is $\frac{1}{2}(x+y)^2$ but I cannot get the negative $y$.,"['derivatives', 'ordinary-differential-equations', 'partial-differential-equations']"
2656368,Converting phrases to First Order Logic,"Assume $a$ is an array of integers of length $100$. Example: All elements of a are 0.

Solution: âˆ€i.((0 â‰¤ i < 100) â†’ a[i] = 0).

(1) For all i in 0..99, the ith element of a is the square of i.
(2) All the elements of a with index a multiple of 3 are 0.
(3) At least one element of a is positive.
(4) At least two elements of a are positive.
(5) Either all elements of a are positive or all elements of a are negative.
(6) The elements of a are non-decreasing. (As an example, the sequence 1,2,2,3,3,4 is non-decreasing
(7) Each element of a (except for the first two elements) is the sum of the previous two elements. The answers below are my own attempts.  I'm not sure that they are correct and I wasn't able to work out anything for problems $2$ and $4$.  I'm still learning FOL and I'm having trouble doing the translations. 1.
âˆ€((0 â‰¤ i < 100 â†’ a[i] = i^2))
2
))
3.
âˆƒi((0 â‰¤ i < 100) âˆ§ (a[i] > 0))
5.
âˆ€i((0 â‰¤ i < 100) âˆ¨ (0 > i > âˆ’100) â†’ (a[i] > 0) âˆ¨ (a[i] < 0)
6.
âˆ€i((0 â‰¤ i < 99) â†’ (a[i + 1]) > a[i]))
1
7.
âˆ€i((2 â‰¤ i < 100) â†’ a[i] = (a[i âˆ’ 1] + a[i âˆ’ 2]))","['propositional-calculus', 'first-order-logic', 'logic', 'discrete-mathematics']"
2656388,Definition of Surjectivity in set-theoretical definition of functions,"I am puzzled by the question of how to define surjectivity in the light of the set-theoretical definition of functions. A function $f : A \rightarrow B$ from a set $A$ to a set $B$ is a subset $f \subseteq A \times B$ of the Cartesian product $A \times B$ such that the following conditions hold: $\forall a \in A : \exists b \in B : (a,b) \in f$ $\forall a \in A : \forall b, b' \in B : (a,b), (a,b') \in f \rightarrow b=b'$ Both conditions encode the idea that for every value $a \in A$ there exists a unique output value $b \in B$. A function $f : A \rightarrow B$ is surjective if $ \forall b \in B : \exists a \in A : f(a) = b$. We note that the notion of a function being surjective depends on the codomain of the function; by enlarging the codomain the function loses any surjectivity and by shrinking the codomain a function may eventually become surjective. However, in the set-theoretical definition of functions we identify functions by their graph, which is enough to recover the domain $A$ and the image $f(A)$, but not enough to recover the codomain $B$. For example, the identity function $f : \{0\} \rightarrow \{0\}$ is surjective and can be identified with its graph: $f = \{ (0,0) \} \in \{0\} \times \{0\}$.
However, this is also the graph of the non-surjective function $f : \{0\} \rightarrow \mathbb R$. It seems intuitive to me the ""same"" function with different codomains is actually different functions. Consequently, the codomain should be included as information when we define a ""function object"" in mathematics.","['foundations', 'elementary-set-theory']"
2656397,Space of closed subsets of locally compact metric space is locally compact,"Let $(X,d)$ be a metric space. Assume that $d$ is a bounded metric. Let $C(X)$ be the collection of closed subsets of $X$. The Hausdorff metric on $C(X)$ is defined by
$$d_H(A,B)= \max \{\sup_{b \in B} d(A,b), \sup_{a \in A} d(a,B)\}.$$
where $d(A,b)= \inf_{a \in A} \{d(a,b) \}$. It is well-known that $(C(X),d_H)$ is compact if $X$ is compact. I wonder if $(C(X),d_H)$ is locally compact given $X$ is locally compact ? Could you give me some hints or reference for this question?",['general-topology']
2656407,Elementary Matrix and Row Operations,"Find an elementary matrix E such that EA = B $$A = \begin{bmatrix} 2 & -1 \\ 5 & 3 \end{bmatrix}$$ $$B = \begin{bmatrix} -4 & 2 \\ 5 & 3 \end{bmatrix}$$ $$\begin{bmatrix} 2 & -1 \\ 5 & 3 \end{bmatrix}*E=\begin{bmatrix} -4 & 2 \\ 5 & 3 \end{bmatrix}$$ So the Elementary Matrix is an Identity Matrix that has one elementary row operation performed on it. Multiplying an Elementary Matrix by A should result in a matrix that is equivalent to having that elementary row operation performed onto A . I can see that only the first row of A is modified to obtain B and I can tell that the first row of A is scaled by a value of -2. Therefore the Elementary Matrix should be the Identity Matrix with the first row scaled by -2.$$\begin{bmatrix} -2 & 0 \\ 0 & 1 \end{bmatrix}$$ However, $$\begin{bmatrix} 2 & -1 \\ 5 & 3 \end{bmatrix}*\begin{bmatrix} -2 & 0 \\ 0 & 1 \end{bmatrix}â‰ \begin{bmatrix} -4 & 2 \\ 5 & 3 \end{bmatrix}$$ What am I doing incorrectly?","['matrices', 'linear-algebra']"
2656423,"$L(X,Y)$ is not complete if $Y$ is not complete","Let $X,Y$ be normed spaces, such that $Y$ is not complete, and $X\neq \left\{{0}\right\}$. I want to show that $L(X,Y)$ is not complete, where $L(X,Y)$ is the space of linear and bounded operators from $X$ to $Y$. My idea of the proof is show the contrapositive. Assuming that $L(X,Y)$ is complete, let $(y_n)_{n \in{} \mathbb{N}} \subseteq Y$ be a Cauchy sequence. I want to find sequences $(x_n)_{n \in{} \mathbb{N}} \subseteq X \setminus \left\{{0}\right\}$, $(l_n)_{n \in{} \mathbb{N}} \subseteq L(X,Y)$ with $y_n=l_n(x_n)$. However for $n,m$, $ \left\|{l_n-l_m}\right\| \geq \left\|{y_n-y_m}\right\|$ , so I can't guarantee that $(l_n)_{n \in{} \mathbb{N}}$ is Cauchy.","['functional-analysis', 'linear-algebra', 'linear-transformations']"
2656431,Zeroes of a non elementary integral,How would one find the zeroes in such cases? I tried some geometric approximations using the graph to no avail. $$\int_{0}^{2\pi} \ln \left(x + \sin t \right)dt$$ Any help or insight would be appreciated.,"['roots', 'integration', 'definite-integrals', 'calculus']"
2656449,Solve a system of equation,"Solve this system of equations.
  $$\begin{cases}
x^{2} + y^{2} = 4 \\ 
z^{2} + t^{2} = 9 \\
xt + yz = 6
\end{cases}$$ My attempts: $(x^{2}+y^{2})(z^{2} + t^{2})=36 \xrightarrow{\text{by applying Lagrange formula}} (xt + zy)^{2} + (zx -ty)^{2}=36 \xrightarrow{xt + yz = 6}  36 + (zx -ty)^{2} =36\Rightarrow (zx -ty)^{2} = 0 \Rightarrow zx = ty \Rightarrow z = \frac{ty}{x}$ $xt + yz = 6 \xrightarrow{z = \frac{ty}{x}}  xt + \frac{ty^{2}}{x} = 6 \Rightarrow x^{2}t + y^{2}t = 6x \Rightarrow t(x^{2} + y^{2})=6x \xrightarrow{x^{2} + y^{2} = 4} t=\frac{3}{2}x$ $z= \frac{ty}{x} \xrightarrow{t=\frac{3}{2}x} z = \frac{3}{2}y$ Can we go further and determine the exact value of variables?",['algebra-precalculus']
2656463,Solve $3\sin^2 x - \cos^2 x - 2 =0$,"Find all the angles between $0$ and $360^\circ$ that satisfy $$3\sin^2 x - \cos^2 x - 2 =0$$ My attempt - $3\sin^2 x - (1-\sin^2x) - 2 =0$ $ 3 \sin^2 x  + \sin^2 x = 3 $ $4\sin^2 x = 3 $ $ \sin x= \frac{\sqrt{3}}{2} $ I found that $x= 60,120 $ Why is the answer for this $60,120,240,300$ ? How do I find 240 and 300?","['trigonometry', 'quadratics']"
2656519,Can there be an onto homomorphism from a ring without unity to a ring with unity?,"Let $R$ be a ring without a unit element and $R'$ be a (non trivial) ring with a unit element. Can there be an onto homomorphism from $R$ to $R'$? Some observations: There cannot be an isomorphism , because then the element of $R$ which maps to $1$ in $R'$ must be a unit element in $R$. Also, we cannot have an onto homomorphism from a ring with unity to a ring without unity as $f(1)$ is going to be a unit element. Edit: I am defining a ring homomorphism as a function $ \phi: R \rightarrow R'$ such that for all $a,b$ in $R$,
$$\phi(a+b) = \phi(a) + \phi(b) $$
$$\phi(ab) = \phi(a)\phi(b)$$","['abstract-algebra', 'ring-theory', 'ring-homomorphism']"
2656531,Find $A$ if $B=A-A^T$,"Suppose $B=A-A^T$ and I know $B\in\mathbb R^{n\times n}$. What is a simple way to get $A$? And what if I have the constraint that $A_{ij}\ge 0$ $\forall i,j$? Clarifications: $B$ is skew symmetric $a_{ii}$ (the diagonal elements of $A$) are zero","['matrices', 'matrix-equations', 'linear-algebra']"
2656537,Making linear groups trivial by adding an equal number of generators and relations,I was told that there is a theorem stating that if $G$ is a finitely presented nontrivial linear group then it is not possible to take a presentation for $G$ and make it a presentation for the trivial group by adding an equal number of generators and relations. Does anyone know a reference for this theorem or how to prove it? Thanks!,"['combinatorial-group-theory', 'group-theory', 'geometric-group-theory']"
2656544,Sum of terms of a sequence,"The question I am trying goes on like this ""Consider the sequence a n given by 
$$ a_1 = \frac{1}{3}$$ 
$$ a_{n+1} = a_n^2 + a_n $$
Let
$$ S = \frac{1}{a_2} + \frac{1}{a_3} + ... + \frac{1}{a_{2008}} $$
Then [S] is equal to _______. (where [.] represents the greatest integer function) Please note that I haven't still learnt convergence and divergence of series, and so any help in formulating a solution not using the above mentioned topics would be appreciated. I have tried calculating the values upto $a_5$ but have not been able to deduce a pattern.","['recurrence-relations', 'sequences-and-series']"
2656558,Find stationary distribution for infinite space Markov chain,"Let their be a Markov chain on state space $S = \{0,1,2,...\}$. Transition probabilities are given as: $p_{0,1} = 1$, $p_{i,i+1} = p$, $p_{i+1, i} = q$ where $0<p,q<1$ and $p+q=1$. For $p<q$ , I need to find the stationary distribution (say $\pi$) of this Markov chain. I got the balance equations: $$\pi_{0}  =q.\pi_{1}$$ $$\pi_{i} = p.\pi_{i-1}+ q.\pi_{i+1}$$
and normalization conditoin: $$\sum_{i=0}^{\infty}\pi_{i} = 1$$ Using these relations, how can I get the stationary distribution?","['markov-chains', 'probability']"
2656593,Minimum value of $h(\theta)= 3 \sin \theta - 4\cos \theta + \sqrt{2} $,Minimum value of $h(\theta)= 3 \sin \theta - 4\cos  \theta + \sqrt{2} $ Find the minimum value of $h(\theta)$ $h(\theta)= 3 \sin \theta - 4\cos  \theta + \sqrt{2} = 5 \sin (\theta + 53.13) + \sqrt{2} $ Minimum value - $5\sin (\theta + 53.13) + \sqrt{2} = -5 $ Therefore min value is = $ -5/5 - \sqrt{2} $ Why am I wrong ? And how should I do this question..,['trigonometry']
2656595,Direction of x-axis with respect to y-axis in Cartesian coordinate system?,What should be the direction of y-axis with respect to x-axis in cartesian coordinate system ? It should be on right hand side or left hand side of x-axis or it can be on both sides as long as it is perpendicular to x-axis ? Plus Angles in counterclockwise rotation are considered positive when they are measured from positive x-axis. What if the rotation is clockwise ? From which axis the angles will be considered positive if the rotation is clockwise ?,"['graph-theory', 'trigonometry', 'geometry']"
2656614,Strong law of large numbers and uniform convergence on compact sets,"Let $f:\mathbb{R}^d\to\mathbb{R}$ be a continuous function and let $(X_i)_{i\in\mathbb{N}}$ be idependent and identical copies of the random variable $X$. Suppose all these random variabes are integrable. I know from the strong law of large numbers that
$$
\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^n f(X_i) = E[f(X)]
$$
with probability one. Suppose we have a sequence of continuous functions $(f_i)_{i\in\mathbb{N}}$ that converges to $f$ uniformly on compact sets. Does it hold that
$$
\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^n f_n(X_i) = E[f(X)]
$$
with probability one? I know that if the convergence to $f$ is uniform, then the above is probably true. I'm curious about if it also holds for uniform convergence on compact sets. Edit: My thoughts. The following is to show that if the convergence to $f$ is uniform, then the above is true.
Define
$$
a_n := \sup_{x\in\mathbb{R}^n} |f(x) - f_n(x)|. 
$$
Observe that
$$
\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^n f(X_i) - f_n(X_i) + f_n(X_i) = E[f(X)]
$$
$$
\implies \lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^n f_n(X_i) - \lim_{n\to\infty} \frac{1}{n}\sum_{i=1}^n f(X_i) + f_n(X_i) = E[f(X)].
$$
Therefore, if we have uniform convergence then $\lim_{n\to\infty} a_n = 0 \implies$ we have the desired result by using Cesaro means to show that the last sum on the left hand side of the equation goes to zero. Does a similar or better argument exist for when there is uniform convergence on compact sets?",['probability-theory']
2656621,Birthday problem- Adam and Eve,Question: Adam and Eve are in a room with $n âˆ’ 2$ other people. Suppose you know that at least two of the people in the room celebrate their birthday on the same day. What is the probability that Adam and Eve celebrate their birthday on the same day? (Assume that a year has 365 days and that the distribution of births over a year is uniform.) MyApproach: I think it is knowing that at least two people have the same birthday that is confusing to me. Does it change anything to have this info?,"['birthday', 'probability']"
2656638,Derivative of a flow diffeomorphism,"Let $M$ be a manifold, $V$ a smooth vector field on it and $\phi_t:M \rightarrow M$ its flow is defined for all $t\in R$. Can $d\phi_t$ be expressed in terms of $V$?","['manifolds', 'differential-geometry']"
2656640,Absolute continuity of a Borel measure,"This is a question from Ph. D Qualifying Exam of real analysis. Let $F$ be an increasing function on $[0,1]$ with $F(0)=0$ and $F(1)=1$ . Let $\mu$ be a Borel measure defined by $\mu((a, b))=F(b-)-F(a+)$ and $\mu(\{0\})=\mu(\{1\})=0$ . Suppose that the function $F$ satisfies a Lipschitz condition $$|F(x)-F(y)|\le A|x-y|$$ for some $A>0$ . Let $m$ be the Lebesgue measure on $[0,1]$ . (a) Prove that $\mu \ll m$ . (b) Prove that $\dfrac{d\mu}{dm} \le A $ a.e. My attempt: (a) Since $F$ is Lipschitz continuous, it is clear that $F$ is absolutely continuous and $F$ is differentiable a.e. and $$\mu((a, b))=\int_{a}^{b}F'dm$$ by absolute continuity. Since $\mu$ is a Borel measure, it extends to $\mu(E)=\int_E F'dm$ for every Borel set $E$ .(I'm not sure for this part. Is there any related theorem or counterexample for this one?) Therefore, it suffices to show that $\int_E F' =0$ whenever $E$ is a Borel set and $m(E)=0$ , and this is obvious from the definition of Lebesgue integral. (b) From (a), $\dfrac{d\mu}{dm}=F'$ and by Lipschitz continuity, $|F'|\le A$ a.e. and the result is obvious. Am I correct? Is there any errors or logical jumps in my attempt?","['real-analysis', 'absolute-continuity', 'lebesgue-measure', 'proof-verification', 'measure-theory']"
2656643,Exceptional solutions to ODE,"I'm trying to find the general solution for $y'=2(1-y^2)$ such that it also contains the solutions $y=\pm1$. I managed to arrive at $$\frac{\ln|y+1|-\ln|y-1|}{2}=2x+C$$ which gives $$\frac{y+1}{y-1}=Ae^{4x}$$ which further gives $$\frac{2}{y-1}=Ae^{4x}-1$$ so $$y=\frac{2}{Ae^{4x}-1}+1$$. 
Now, what I'm unsure about is how to check if this solution satisfies the exceptional
solutions since the derivative is an expression containing only the variable $x$ and I'm trying to check if $y=\pm1$ satisfies the ODE. I'm also not sure how I would modify the solution so that it also contains the exceptional solutions.","['ordinary-differential-equations', 'calculus']"
2656656,Definition of convergence to infinity,"Does the following definition hold true for every series that converges to infinity? Definition- A sequence $x_n$ is said to converge to $\infty$ if, for every $a>0$ and $\epsilon>0$, there exists $N\in \mathbb{N}$, such that for $n>N$ it is true that $|x_n-a|>\epsilon$ Clarification: For example we can use this definition to determine that $x_n=\sqrt{n}$ converges to infinity. But if our sequence is $1,0,2,0,3,0,4,0,5...$ then the above definition can't be used. Can this definition be modified so that oscillating sequences can also be tested for convergence to infinity?","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
2656668,Bump function inequality,"Let $f_1, f_2, f_3\in C_0^\infty(\mathbb{R}^2)$, $\psi\in C_0^\infty(\mathbb{R})$, and $\psi_t(x)=t\psi(tx)$ for all $t>0$. Show that if $\tfrac{1}{p_1}+\tfrac{1}{p_2}+\tfrac{1}{p_3}=1$ for some $1\le p_1,p_2,p_3\le\infty$, then $$\left|\int_{\mathbb{R}^3}f_1(x,y)f_2(y,z)f_3(z,x)\psi_t(x+y+z)dxdydz\right|\le\lVert f_1\rVert_{L^{p_1}(\mathbb{R}^2)}\lVert f_2\rVert_{L^{p_2}(\mathbb{R}^2)}\lVert f_3\rVert_{L^{p_3}(\mathbb{R}^2)}\lVert\psi\rVert_{L^1(\mathbb{R})}$$ for all $t>0$. See https://en.wikipedia.org/wiki/Bump_function and http://mathworld.wolfram.com/Lp-Space.html for notation explanation. My first instinct is to use Holder's Inequality, but the pairings of the variables on the LHS and the additional $\psi$ terms make it difficult to apply. I don't have any other ideas.","['holder-inequality', 'lp-spaces', 'lebesgue-integral', 'analysis']"
2656684,Can $ \boldsymbol{u}' - \boldsymbol{A}(t) \boldsymbol{u} = \boldsymbol{f}(t)$ be solved using an matrix exponential integrating factor?,"Can $$ \boldsymbol{u}' - \boldsymbol{A}(t) \boldsymbol{u} = \boldsymbol{f}(t)$$ be solved using an integration factor method analogous to that used in the non-system case? (i.e. $u'-a(t)u=f(t)$). Giving the following solution: $$ \boldsymbol{u} = e^{-\int^t A(k)dk}\left (\int^t e^{\int^s A(k)dk}\boldsymbol{f}(s) ds + \boldsymbol{c}\right )$$ where the matrix exponential defined in the usual way. If you rearrange the solution and differentiate you find
$$
\frac
{\text{d}e^{-\int^t A(k)dk}}
{\text{d}t}
\boldsymbol{u}
+
e^{-\int^t A(k)dk}
\boldsymbol{u}'
=
e^{-\int^t A(k)dk}
\boldsymbol{f}
$$ If this is indeed the solution, it would then imply that
$$
\frac
{\text{d}e^{-\int^t A(k)dk}}
{\text{d}t}
= 
- e^{-\int^t A(k)dk} \boldsymbol{A}
$$ However this doesn't seem to be the case from Eq. 2.1 of http://aip.scitation.org/doi/pdf/10.1063/1.1705306 referenced on the Wiki page for matrix exponentials . EDIT: Disproved on page 6 here .","['real-analysis', 'ordinary-differential-equations']"
2656690,Is there an intuitive reason for a certain operation to be associative?,"When I read Pinter's A Book of Abstract Algebra , Exercise 7 on page 25 asks whether the operation
$$x*y=\frac{xy}{x+y+1}$$
(defined on the positive real numbers) is associative. At first I considered this to be false, because the expression is so complicated. But when I worked out $(x*y)*z$ and $x*(y*z)$, I found both to be $$\frac{xyz}{xy+yz+zx+x+y+z+1}!$$ Commutativity is easy to see. But associativity can be so counter-intuitive! Can you see this operation is associative without working it out? Are there tricks to do this?","['intuition', 'abstract-algebra', 'associativity']"
2656693,Is it true that an element of a group whose order divides the order a subgroup is an element of the subgroup,"Let $G$ be a group. Suppose that the order of $G$ is finite and that $H$ is a subgroup of $G$. Is it true that an element of $G$ whose order divides the order of $H$ is in $H$? Here is my attempt: Let $|G|=n$ and $|H|=m$. Then $m|n$. Moreover, the order of every subgroup of $G$ divides the order of $G$ and if $g \in G$, then$|g|=|\langle g \rangle|$. Since in itself $H$ is a group (under the same operation in $G$), then the order of its element divides $|H|=m$. Thus, if $|g||m$ then $\langle g\rangle\leq H$ which means that $g \in H$. Can you help me out with this one? I just can find a way put $g$ in $H$.","['abstract-algebra', 'group-theory']"
2656714,Then the maximal possible number of elements in $E?$,"Let $U$ be an orthonormal set in a Hilbert space $H$ and let $x \in H$ be such that $\vert \vert x \vert \vert =2$. Consider the set $$E=\{  u\in U: \vert \langle x, u \rangle \vert \geq \frac{1}{4} \}$$ Then the maximal possible number of elements in $E?$ I use Bessel inequality but didn;t get any result. Please help.","['functional-analysis', 'hilbert-spaces', 'inner-products']"
2656719,Prove that $x \& (x - 1)$ turns off the rightmost bit in a word.,"Prove that for a n-bit word $x$ the operation $$x \& (x - 1)$$ with $\&$ being the bitwise AND-operator, turns off (inverts) the rightmost bit of $x$ (e.g. 0101100 -> 01010000). The example above is is the first bit manipulation presented in Henry Warren's Hacker's Delight and I would like to understand why this works. A concrete mathematical proof or even pointers to a general strategy when undertaking proofs regarding bitstrings would be higly appreciated.","['boolean-algebra', 'computer-arithmetic', 'discrete-mathematics']"
2656724,Probabilistic Proof of Chebyshev Inequality - Step 3,"Here you can find the probabilistic proof of the Chebyshev Inequality. I don't understand Step 3 which uses the following inequality: $$
\mathbb{E}\left[\mathbf{I}_{\{ X^2 >1\}} \right] \leq \mathbb{E}\left[X^2\right]
$$ Can you explain why this is true?","['probability-theory', 'inequality', 'probability', 'integral-inequality']"
2656803,Diagonalization of the Fourier transformation.,"Browsing the following link: https://webusers.imj-prg.fr/~bernard.maurey/agreg/Textes/ag001_a2.pdf , I was able to understand how to diagonalize the Fourier Transform $ \mathcal {F} $, but unfortunately, the article does not tackle the subject of knowing how to find the eigenspaces relative to the eigenvalues â€‹â€‹of $ \mathcal {F} $. Can you please tell me how to find them ? Thanks in advance for your help.","['matrices', 'diagonalization', 'linear-algebra', 'fourier-transform']"
2656851,Understanding the branch cut and discontinuity of the hypergeometric function,"I am going through this paper, and I am having trouble understanding page 20. I am still learning my way around managing multi valued complex functions, so I'd like your help in understanding what's happening there. I have the definition of the hypergeometric series $_2F_1$ as
$$
_2F_1(a,b;c;z)=\sum_{n=0}^\infty \frac{(a)_n(b)_n}{(c)_n}\frac{z^n}{n!}.
$$
Here $(a)_n$ is the rising Pochhammer symbol, $(a)_n=\Gamma(a+n)/\Gamma(a)$ (well defined whenever $a$ is not a negative integer or zero). By elementary computations, the radius of convergence of this series is 1. It is then said that $_2F_1$ can be continued analitically in the whole complex plane along any curve not passing through $[1;+\infty)$, and $1$ itself is a branching point and the function has a cut on the previous segment. Furthermore, equation $2.115$ of the paper computes the discontinuity when crossing the branch cut as (with $x\geq4/3$ to have the real part of the argument bigger than one)
$$
_2F_1\left(\frac16,\frac56;1;\frac{3x}4+i\epsilon\right)-_2F_1\left(\frac16,\frac56;1;\frac{3x}4-i\epsilon\right)=i\, _2F_1\left(\frac16,\frac56,1;1-\frac{3x}4\right).
$$
I am trying to understand those results. It is not clear to me how to understand from the power series form the behaviour on the boundary of the disk of convergence. It is clear to me that the series in $z=1$ diverges, as the coefficients do not go to zero, so there should be no analytical continuation. A way to study that function would be its integral representation:
$$
_2F_1(a,b;c;z)=\frac{\Gamma(c)}{\Gamma(b)\Gamma(c-b)}\int_0^1t^{b-1}(1-t)^{c-b-1}(1-zt)^{-a}dt.
$$
Here it is understood that $\arg t=0=\arg(1-t)$. From this representation, it is clear to me that the function can have branches whenever $a$ is not a positive integer, and if $z\in[1;\infty)$ we have $0$ in the integration path, so we can have multiple branches. And that's exactly my case. The problem is that I can't use this form to prove the previous equation about the discontinuity: all I manage to write is (here I write z=x with x real number bigger than one, and I am neglecting some numerical factors that can easily be reinserted)
$$
\lim_{\epsilon\to0}((1-xt-i\epsilon t)^{-a}-(1-xt+i\epsilon t)^{-a})=-(1-e^{-2i\pi a})(1-xt)^{-a}.
$$
This is different from the line that I would like to prove (as an example, I do not see why I should change the argument, but I also see that with that argument changing the function is at least evaluated in a point where well defineteness is guaranteed). To summarize, I have two questions: How to prove the discontinuity from the integral form? More in general: how to treat power series in order to understand if their singularities are poles or branch cuts, and the discontinuity of their analytic continuations? I know that this is a very broad topic, and I'd also like some references to continue my study. Thanks everybody for the time you took reading this.","['branch-points', 'complex-analysis', 'hypergeometric-function', 'power-series', 'branch-cuts']"
2656902,A hard Cubic Diophantine equation,"This problem is from an Olympiad handout: Show that there exists infinitely many integer triplets $(x,y,z)$ such that 
  $ x^3+y^3+z^3-2xyz=1$. I tried to plug $x=y$ and use tangent lines to find solutions inductively, but the method didn't work well. (It gave $(1,1,1) \rightarrow (13,13,-23)$ ,but after that there was only rational roots) Also I plugged the equation in the cubic formula and tried to delete the cubic root, but it also failed. Help me please :I","['number-theory', 'diophantine-equations']"
2656934,Proof of $\angle$ sum of polygon.,"First, I know this question might have been asked by several times, see here , for an example.
Before someone may want to mark it as dulplicate , I would like to calrify what I want to ask. Mainly, I am asking for a rigorous proof, or why it is rigorous enough ? Notes: I will be considering simple polygon . Consider the answer of this post by Misha Lavrov, What makes things worse is that people often work with polygons on a
  somewhat intuitive level I agree with this. I definitely know that $$Interior\space\angle\space sum\space of\space a\space N-sided\space polygon=(N-2)180^\circ$$ as every high school text shall states. Most of the proofs which I have seen about the problem, has a similar idea as the accepted answer of this post . Main idea of the proof: For a polygon, we will just select points to join segments, and then we can devide the polygon into several pieces, and by $\angle$ sum of $\triangle$, we can find the $\angle$ sum of the polygon . This proof is very intuitive, but  I don't think it is rigorous enough, as I wonder, can we still connect every vertex to the point, even for a extremely ugly concave polygon, to seperate the polygon, into several $\triangle$s, such that each of the interior $\angle$ of each of the $\triangle$s is in an interior $\angle$ of the polygon and won't be counted twice. For an explaination and example for what I said right above, see below. e.g.) For a 'ugly' 23-sided polygon, which I drew 'randomly' : As in the image above, I discovered a way to divide the polygon into 21 pieces of $\triangle$, while it sounds to be eligible. However, I don't think this will certainly happen , if the polygon is even more ugly. How to explicitly consider the case, when different $\triangle$ share a same interior $\angle$ in the proof? Also, as I asked above, can we always find a way to devide it properly? (I think it is important to prove it) I also considered the way in this post . For the induction part, asumming the $\angle$ sum formula for polygon is true for $N$-sided polygon. Then, consider any $N+1$-sided polygon, I used to think that we can select $2$ vertex which is saperate by one vertex in middle (can I present it more precisely?), join them together, and we form a $\triangle$ (Do we need to consider whether 'convex' or 'concave' ?). Then we form a $N$-sided polygon, and by the induction hypothesis and $\angle$ sum of $\triangle$, we can prove the formula holds for $N+1$. But, moreover, do we need to consider this case and/or this case , when the remaining polygon might not be $N$-sided? TO SUM UP, How can we consider all possible cases and make a rigorous proof? Most importantly, I want a justification of the constructability (I mean whether the graph is constructable/valid, not for the Compass-and-straightedge construction ) and generality of a graph, if there is a graph in the proof. Remember, as I said above, I am looking for a rigorous proof, not an usual one . Thank you so much for your answer :) NOTES:Does the ways in this help?","['polygons', 'angle', 'proof-writing', 'euclidean-geometry', 'geometry']"
2656945,Proving a few properties of Bertrand curves,"Here's what I've got so far (and I'm assuming $\alpha$ is a unit speed curve): a) The fact that $\beta(s) = \alpha(s) + r(s)N(s)$ for some scalar function $r$ follows trivially because of the fact that the normal lines of $\beta$ and $\alpha$ are equal (so $\beta(s)$ is a point on the normal line of $\alpha(s)$, which is precisely what the equality states). Then: $$\ r(s) = (\beta(s) - \alpha(s))N(s) \Rightarrow r'(s) = (\beta'(s) -
    \alpha'(s))N(s) + (\beta(s) - \alpha(s))N'(s)  $$ $$r'(s) = (\beta'(s) -\alpha'(s))N(s) + r(s)N(s)N'(s) = (\beta'(s) -
    \alpha'(s))N(s)= \beta'(s)N(s) = 0$$ as desired. b) Let $\theta$ be the angle between the unit tangent vectors mentioned. Then: $$\cos(\theta) = \frac{\beta'(s)\alpha'(s)}{||\beta'(s)||} = \frac{1-r(s)k(s)}{||\beta'(s)||}$$ but $$||\beta'(s)||^2 = ||\alpha'(s)||^2 + r^2(s)||N(s)||^2 = 1 + r^2(s)$$ so $$||\beta'(s)|| = \sqrt{1+r^2(s)}$$ $$\cos(\theta) = \frac{1-r(s)k(s)}{\sqrt{1+r^2(s)}}$$ EDIT: The formula for $||\beta'(s)||^2$ above is wrong. Actually, $||\beta'(s)||^2 = ||\alpha'(s)||^2 + r^2(s)||N'(s)||^2$, which is not as neat as the above. But the hint in the comments is much better to solve this. c) Assuming I had done b) , then the following would be true: $$(\cos(\theta))' = \left( \frac{\beta'(s)\alpha'(s)}{||\beta'(s)||} \right)' = 0 \Rightarrow (\beta'(s)\alpha'(s))' = 0 \Rightarrow \beta''(s)\alpha'(s) + \beta'(s)\alpha''(s) = 0$$ but continuing on this path only leads to $k'(s) = 0$, which, even if I had done b), would not be useful (and I'm aware using this to prove b) is circular). Beyond these (b) and c)), I would also like some help solving d) and $21$ (I don't know where to start on those). Update : A lot of what I wrote above is wrong, but my doubts here are almost completely solved. I just have to finish $21$ and I'll be done. Update 2 : For $20$, see the discussion in comments. For $21$: Since $T_\beta$ is orthogonal to $N_\alpha$, we can write it as a linear combination of $T_\alpha$ and $B_\alpha$. In particular: $$T_\beta = \pm (\cos(\theta)T_\alpha + \sin(\theta)B_\alpha)$$ where $\theta$ is the angle between the unit tangent vectors of $\alpha$ and $\beta$. Then: $$B_\beta = T_\beta \times N_\beta = \pm T_\beta \times N_\alpha = \pm (\cos(\theta)T_\alpha + \sin(\theta)B_\alpha) N_\alpha= \pm(\cos(\theta)B_\alpha - \sin(\theta)T_\alpha)$$
Differentiating and using the fact that $\cos(\theta)$ and $\sin(\theta)$ are constant, we have: $$B_\beta' = \pm(\cos(\theta)B_\alpha' -\sin(\theta) T_\alpha') = \pm(-\cos(\theta)\tau_\alpha N_\alpha - \sin(\theta)\kappa_\alpha N_\alpha)$$
$$B_\beta' = \pm(-\cos(\theta)\tau_\alpha - \sin(\theta)\kappa_\alpha)N_\alpha$$ By the Frenet frame, we also have: $$B_\beta' = -\tau_\beta N_\beta = \pm(-\tau_\beta N_\alpha) $$ So: $$\pm(-\cos(\theta)\tau_\alpha - \sin(\theta)\kappa_\alpha)N_\alpha =  \pm(-\tau_\beta N_\alpha) \Rightarrow (\cos(\theta)\tau_\alpha + \sin(\theta)\kappa_\alpha) = \pm \tau_b $$ By previous work: $$\cot(\theta) = \frac{1-r(s)\kappa_\alpha(s)}{r(s) \tau_\alpha(s)}$$ which is equivalent to:
$$\kappa_\alpha(s) + \cot(\theta) \tau_\alpha = \frac{1}{r(s)} \Rightarrow \kappa_\alpha(s) \sin(\theta) + \tau_\alpha(s) \cos(\theta) = \frac{\sin(\theta)}{r(s)}$$ Then: $$\pm (\cos(\theta)\tau_\alpha + \sin(\theta)\kappa_\alpha) = \pm \frac{\sin(\theta)}{r(s)} = \tau_\beta(s)$$ $\bar{s}$ being the arclength paramater, we also have: $$T_\beta = \frac{d \beta}{ds} \frac{ds}{d\bar{s}}$$ so: $$T_\beta = \left( ( 1 - r(s) \kappa_\alpha(s))T_\alpha + r(s) \tau_\alpha(s) b_\alpha(s) \right ) \frac{ds}{d\bar{s}}$$ comparing the very first expression for the coefficients, we see that: $$\pm \sin(\theta) = r(s) \tau_\alpha(s) \frac{ds}{d \bar{s}}$$ so that: $$\frac{d \bar{s}}{ds} = \frac{r(s)\tau_\alpha(s)}{\sin(\theta)}$$ and then, by previous work: $$\pm \frac{\tau_\alpha(s) \tau_\beta(s) r(s)}{\sin(\theta)} = \pm \frac{\sin(\theta)}{r(s)} \Rightarrow \tau_\alpha(s) \tau_\beta(s) = \frac{\sin^2{\theta}}{r^2(s)}$$ as desired.","['curves', 'frenet-frame', 'differential-geometry']"
2656948,$\lim_{n \rightarrow \infty}\int_0^1f_n(x)$ [duplicate],"This question already has answers here : Limit of $s_n = \int\limits_0^1 \frac{nx^{n-1}}{1+x} dx$ as $n \to \infty$ (5 answers) Closed 2 years ago . For $n=1, 2,...,$ let $f_n(x)=\frac{2nx^{n-1}}{x+1}, x\in [0, 1]$. Then $\lim_{n \rightarrow \infty}\int_0^1f_n(x)$ Function is unbounded at $1$, How do I solve?","['real-analysis', 'analysis']"
2656982,Averaged log-likelihood with a latent variable for mixture models,"In class we've defined the following: $$Q(\theta; \theta^t) = \sum_z P(Z=z\mid X=x; \theta^t) \log P(X=x; Z=z;\theta)$$ It's part of the EM algorithm. Here, $\theta^t$ are the assumed parameters at time $t$. I just wonder, why wasn't it defined as conditional probability, like this: $$\log P(X=x \mid Z=z;\theta)$$ I'm looking for the intuition here, other than ""it's the definition"". Thanks","['maximum-likelihood', 'machine-learning', 'statistics', 'log-likelihood']"
2656985,What will be the derivative $2 (\ln (x))^{x/2}$?,"I'm not sure, that my steps are valid, so $ln(f(x))=ln \cdot (2 \cdot ln(x))^{x/2}=x^{2} \cdot ln \cdot(ln(x))$ so I get $\frac{1}{f(x)} \cdot f'(x)$ and I have to multiply both sides with $f(x)$, am I right?",['derivatives']
2656997,"Complicated recursion formula, seems similar to Bell numbers?","I came up with a recursive formula for a problem I was working on.  It is as follows.
$$a_n = \Big(\frac{1-q^{f \cdot n}}{1-q^n}\Big)\displaystyle\Big(1+\sum_{i=0}^{n-1}\binom{n}{i}p^{n-i}q^ia_i\Big)$$ Here $a_0 = 0$ $p, q\in [0,1],$ $p + q = 1,$ and $f$ is a positive integer.
Is there anyway to give a closed form solution to this equation? Off the top of my head this seems related to Bell Numbers. Other thought were to try setting $$\alpha(n) = \frac{1-q^{f \cdot n}}{1-q^n}$$
and then simplifying $$\frac{a_n}{\alpha(n)} - p\cdot\frac{a_{n-1}}{\alpha(n-1)}$$ using $$\binom{n}{k} = \binom{n-1}{k} + \binom{n-1}{k-1}.$$
I think I keep getting errors, but this should give something close to 
$$\frac{a_n}{\alpha(n)} - p\cdot\frac{a_{n-1}}{\alpha(n-1)} = (1-p) + n\cdot p^1q^{n-1}a_{n-1} + \displaystyle\sum_{i=0}^{n-2}\binom{n-i}{i}p^{n-i}q^ia_i.$$ I think $$\displaystyle\sum_{i=0}^{n-2}\binom{n-i}{i}p^{n-i}q^ia_i = p\cdot\displaystyle\sum_{i=0}^{n-2}\binom{n-i}{i}p^{(n-1) - i}q^ia_i = p\Big(\frac{a_{n-i}}{\alpha(n)} - 1\Big).$$ I should then be able to stick this back in and solve. However, I seem to get a different answer, and regardless, this still is far from being a closed form solution. Any suggestions?  Thank you!","['generating-functions', 'combinatorics', 'bell-numbers']"
2657045,Limit of a sequenced defined by arithmetic mean and geometric mean.,"If $a_1 =\alpha$, $ a_2 = \beta$, and, for every $k$, $$a_{2k+1}= \frac{1} {2k}  \sum_{i=1}^{2k} a_i\qquad a_{2k+2}=\left(\prod_{i=1}^{2k+1} a_i\right)^{1/(2k+1)}$$
  what is the limit of the sequence $(a_k)$?, That is, what is $\lim_{n\to\infty}{a_n}?$ I think this sequence converges, but i can't find a clue to figure out its limit. When $\alpha=\beta$, it is clear that $a_n=\alpha$ However, if $\alpha\neq\beta,$ I think i cannot calculate exactly wht $a_n$is, so I'm currently stuck. I know that for all $k$, $\alpha \le a_k \le \beta$, and I think that this sequence converges towards somewhere close to $\min(\alpha, \beta)$ because
$a_1=\alpha$, $a_2=\beta$, $a_3=\frac{\alpha + \beta}{2}$, $a_4=(\frac{\alpha\beta(\alpha+\beta)}{2})^{1/3}$, but I can't figure out what to do next. For example, here is a sequence when $\alpha=2, \beta=4$:
$$a_1=2, a_2=4, a_3=3, a_4=2.8845, a_5=2.9711, a_6=2.90162, a_7=2.95953, a_8=2.9098, a_9=2.95331, a_{10}=2.91462, a_{11}=2.9494, a_{12}=2.91776, a_{13}=2.9468, a_{14}=2.91998...$$
Edit: I think as $n$ increases, $$a_{2k+2}\le a_{2k+4}\le a_{2k+3}\le a_{2k+1}$$
Could anybody confirm this please? If this is right, $a_n$ converges to $2.9.....$ I found that the above inequality is right using mathematical induction Let $\alpha<\beta$, then $a_3=\frac{\alpha+\beta}{2}$, $a_4=\sqrt[3]{\alpha\beta\frac{\alpha+\beta}{2}}=\sqrt[3]{(\frac{\alpha+\beta}{2}+\frac{\beta-\alpha}{2})(\frac{\alpha+\beta}{2}-\frac{\beta-\alpha}{2})(\frac{\alpha+\beta}{2})}=\sqrt[3]{(\frac{\alpha+\beta}{2})^3-A}<\frac{\alpha+\beta}{2}=a_3$
  If $a_{2k+1}>a_{2k+2}$, 
  $$a_{2k+3}=\frac{1}{2k+2}\sum_{i=1}^{2k+2}{a_i}=\frac{a_{2k+2}+a_{2k+1}+\sum_{i=1}^{2k}{a_i}}{2k+2}=\frac{a_{2k+2}+(2k+1)a_{2k+1}}{2k+2}\\\therefore a_{2k+2}<a_{2k+3}<a_{2k+1}\\a_{2k+4}=(\prod_{i=1}^{2k+3}{a_i})^{\frac{1}{2k+3}}=(a_{2k+3}\cdot (a_{2k+2})^{2k+2})^{\frac{1}{2k+3}}\\\therefore a_{2k+2}<a_{2k+4}<a_{2k+3}\\\therefore a_{2k+2}<a_{2k+4}<a_{2k+3}<a_{2k+1}$$By the mathematical induction, for all natural number $n$, $a_{2n+2}<a_{2n+4}<a_{2n+3}<a_{2n+1}$ This means that the sequence $\{a_{2n}\}$ and $\{a_{2n+1}\}$ converges, since the two sequences are monotone increasing and monotone decreasing, and the two sequences are bounded(since $\alpha<\{a_{2n}\}<\{a_{2n+1}\}<\beta$).
Now I want to know whether the two limits of these two sequences are the same, which means $\{a_{n}\}$ converges.","['means', 'sequences-and-series', 'convergence-divergence']"
2657076,"What's the probability of choosing two numbers from $[0,1]$ and having the difference at least one half? [duplicate]","This question already has answers here : Probability of two uniform random numbers being more than $\frac{1}{2}$ apart (5 answers) Closed 4 years ago . We have the unit interval $[0,1]$ and we want to find the probability of picking two random numbers $a,b$ from that interval with $|a-b|>0.5$. Must I investigate $[0,1]Ã—[0,1]$? I don't have the faintest idea of how to solve this. The problem is that $[0,1]$ has infinite numbers to pick fromâ€¦ so how to calculate a probability with infinitely many items in the sample space? I would be really happy if somebody shed a light on this.",['probability']
2657114,Soft Question: Resources to learn Olympiad Combinatorics,"Combinatorics is my weakest Math Olympiad subject and nine out of ten times I am unable to solve a combinatorics problem from my country's mathematical Olympiads (India). I do not have a tutor nor any resources except the internet and AoPS to learn and therefore I need some introductory problem-solving texts to get me going. I currently know about basic counting principles, linear homogeneous recurrence relations, binomial-theorem, multinomial theorem, basic graph theory, etc. I am referring to Richard A. Brualdi's Introductory Combinatorics as my primary textbook. What I need now is some Olympiad problem-solving texts or readings which cover everything from the basics to advanced problems. Please suggest some reading material assuming I am a beginner in Olympiad Combinatorics and have no prior knowledge about it. Thank you. Edit: I researched and found Chen Chuan Chong's Principles and Techniques in Combinatorics and Titu's Guide to UG Combinatorics. Are these two good books? I am finding the latter one very hard to grasp.","['combinatorics', 'contest-math', 'book-recommendation', 'soft-question']"
2657159,$\exists\alpha\in\mathbf{F}(|\alpha-\lambda|<\frac{1}{1000}\ \text{and}\ T-\alpha I \ \text{is}\ \text{invertible})$,"I was asked to prove the following Theorem. Theorem. Given that $V$ is finite dimensional and $T\in\mathcal{L}(V)$ and $\lambda\in\mathbf{F}$. There exists a
  $\alpha\in\mathbf{F}$ such that $|\alpha-\lambda|<\frac{1}{1000}$ and
  $T-\alpha I$ is invertible. The Following is my attempt any extra results that i make use of have been quoted below the proof. Is my proof correct? Proof. Assume on the contrary that given any $\alpha\in\mathbf{F}$, $|\alpha-\lambda|<\frac{1}{1000}$ implies that $T-\alpha I$ is not invertible or equivalently $\alpha$ is an eigenvalue of $T$. Let $n = \dim V$ and $I_{n+1} = \{1,2,3,...,n,n+1\}$. Consider now the function $\mathcal{V}:I_{n+1}\to \mathbf{F}$ defined by $$\mathcal{V}(x) = \lambda+\frac{1}{10^{3+x}}$$ with the above definition it is not difficult to see that $\forall x\in I_{n+1}\left(|\mathcal{V}(x)-\lambda|<\frac{1}{1000}\right)$. Consequently we have a list of $n+1$ distinct eigenvalues namely $\mathcal{V}(1),\mathcal{V}(2),...,\mathcal{V}(n),\mathcal{V}(n+1)$ taking this together with theorem $5.10$ implies that the corresponding list of eigenvectors namely $v_1,v_2,...,v_n,v_{n+1}$ is linearly independent. However we know that any list of linearly independent vectors in $V$ must have length $m\leq n = \dim V$ resulting in a contradiction. $\blacksquare$ $5.10$ If $T\in\mathcal{L}(V)$ and $\lambda_1,\lambda_2,...,\lambda_n$ is a list of distinct eigenvalues
  of $T$ then the corresponding list of vectors $v_1,v_2,...,v_n$ is
  linearly independent.","['eigenvalues-eigenvectors', 'linear-algebra', 'proof-verification', 'linear-transformations']"
2657170,Understanding this proof of Gronwall's inequality.,"Gronwall's theorem is given as follows: Assume that for $t_0\leq t \leq t_0 + a$, with $a$ a positive constant, we have the estimate $$\phi(t)\leq \delta_1\int_{t_0}^t\psi(s)\phi(s)ds + \delta_3\,\,\,\,\,(1.4)$$ in which, for $t_0\leq t \leq t_0 + a$, $\phi(t)$ and $\psi(t)$ are continuous functions, $\phi(t) \geq 0$ and $\psi(t)\geq 0$; $\delta_1$ and $\delta_3$ are positive constants. Then we have for $t_0\leq t\leq t_0 + a$ $$\phi(t) \leq \delta_3e^{\delta_1\int_{t_0}^t\psi(s)ds}$$ In my book the following proof of this theorem is proof is given: From the estimate $(1.4)$ we derive $$\dfrac{\phi(t)}{\delta_1\int_{t_0}^t\psi(\tau)\phi(\tau)d\tau + \delta_3}\leq 1$$ 
multiplication with $\delta_1\psi(t)$ and integration yields $$\int_{t_0}^t\dfrac{\delta_1\psi(s)\phi(s)ds}{\delta_1\int_{t_0}^t\psi(\tau)\phi(\tau)d\tau + \delta_3}\leq \delta_1\int_{t_0}^t\psi(s)ds$$ so that $$\ln(\delta_1\int_{t_0}^t\psi(s)\phi(s)ds + \delta_3) - \ln\delta_3\leq \delta_1\int_{t_0}^t\psi(s)ds$$ which produces $$\delta_1\int_{t_0}^t\psi(s)\phi(s)ds + \delta_3 \leq \delta_3 e^{\delta_1\int_{t_0}^t\psi(s)ds}$$ Applying the estimate $(1.4)$ again, but now to the lefthand side, yields the required inequality. Question: How does the step from  $$\int_{t_0}^t\dfrac{\delta_1\psi(s)\phi(s)ds}{\delta_1\int_{t_0}^t\psi(\tau)\phi(\tau)d\tau + \delta_3}\leq \delta_1\int_{t_0}^t\psi(s)ds$$ to $$\ln(\delta_1\int_{t_0}^t\psi(s)\phi(s)ds + \delta_3) - \ln\delta_3\leq \delta_1\int_{t_0}^t\psi(s)ds$$ work? I think I understand the rest of the proof but I can't figure out the above step. Thanks!","['numerical-methods', 'ordinary-differential-equations']"
2657194,Poisson sand timer,"Imagine a sand clock that drops grains of sand from an upper bulb to the lower one at random intervals. The probability that $k$ grains of sand are dropped in $t$ seconds is $$\frac{(\lambda t)^k}{k!}e^{-\lambda t}$$ Assume $\lambda$ is known. How can I estimate the elapsed time when I see $n$ more grains of sand in the lower bulb? I guess the problem is very common. What is the canonical approach? Should I compute the a confidence interval for $t$? How? Or, what else?","['statistics', 'statistical-inference', 'probability-distributions']"
2657195,Continuous functions need hint,"We have continuous functions $f,g:[0;\infty)\longrightarrow[0;\infty)$ with the following properties: $f(0)=g(0)=0$ $g(x)\neq0$ , for any $x>0$ $f(x+g(f(x)))=f(x)$ , for any $x$ Prove that $f(x)=0$ for any $x$ . I need only a hint how to start. So far I've tried something with a sequence with positive terms and limit $0$ . I think that somehow we have to get to: $g(f(x))=0$ for any x, from where the conclusion follows.","['continuity', 'real-analysis', 'functions']"
2657197,Is it true that every isomorphism of hypersurface can be lifted to ambient projective space?,"Let $f:X\to X'$ be isomorphism between hypersurfaces of dimension $n-1$ ( in $\mathbb P^n$). Is it true that we can always lift this $f$ to $\mathbb P^n$? I have already seen that for very general case this is true. My approach is using the very ample line bundle $\mathcal O_X(1)=i^*\mathcal O_{\mathbb P^n}(1)$ ($i$ is the embedding in $\mathbb P^n$). If $f^*\mathcal O_{X'}(1)=\mathcal O_{X}(1)$ and $h^0(X,\mathcal O_{X}(1))=n+1$ then obviously we can lift $f$ to $\mathbb P^n$. So we need two conditions: $h^0(X,\mathcal O_{X}(1))=n+1$. Using the exact sequence $0\to \mathcal O(1-d)\to \mathcal O(1) \to \mathcal O_X(1)\to 0$ this is true if $h^0(\mathbb P^n,\mathcal O(1-d))=h^1(\mathbb P^n,\mathcal O(1-d))=0$ , which trivially holds for $d>1$. So this is okay. $f^*\mathcal O_{X'}(1)=\mathcal O_{X}(1)$. I don't know if this is always true? I know for $n>3$ the Picard group of hypersurface is isomorphic to $\mathbb Z$. And the canonical bundle of $X$ is $\mathcal O_X(d-n-1)$. So if $d-n-1\neq 0$ we always have $f^*\mathcal O_{X'}(1)=\mathcal O_{X}(1)$. But I feel very uncomfortable with the condition $d-n-1\neq 0$. Is there any way to avoid it? Or, if there is another way to show this? Thanks in advance.","['algebraic-geometry', 'geometry']"
2657200,Prove a function is differentiable with chain rule,"Let $A$ be an open set of $\mathbb{R}^n$ and let $f : A \rightarrow \mathbb{R}^m$. Fix $u \in \mathbb{R}^m$ and define $g : A \rightarrow \mathbb{R}$ by $g(x) = f(x) \bullet u$ for all $x \in A$, where $\bullet$ is the inner product. Show that if $f$ is differentiable on $A$ then $g$ is differentiable on $A$ with derivative $g'(a) = f'(a) \bullet u$. I am trying to prove this with the chain rule. If $h : \mathbb{R}^m \rightarrow \mathbb{R}$ is defined by $h(x) = x \bullet u$ then $h$ is differentiable, so $h \circ f = g$ is differentiable. I am having a hard time showing that $g'(a) = f'(a) \bullet u$.","['derivatives', 'real-analysis', 'chain-rule']"
2657271,Puzzled for a (supposed) way to show that $\det(e^A)=e^{{\rm tr}(A)}$,"I know how to show that $\det(e^A)=e^{{\rm tr}(A)}$ by elementary methods using the eigenvalues of $e^A$ (that is, if $\lambda$ is an eigenvalue of $A$ then $e^\lambda$ is an eigenvalue of $e^A$). However I found an exercise that want that the identity be proved in a very different way (more analytical). It says: Let $f(t):=\det(e^{tA})-e^{{\rm tr}(tA)}$. Now, using the derivative of the determinant, show that $f'=0$. The derivative of the determinant is stated as: Let $a_1,\ldots,a_m\in C^1(X,\Bbb K^m)$, where $X\subset\Bbb K$ is open, then $\det[a_1,\ldots,a_m]\in C^1(X,\Bbb K)$ and $$(\det[a_1,\ldots,a_m])'=\sum_{k=1}^m\det[a_1,\ldots,a_{k-1},a_k',a_{k+1},\ldots,a_m]$$ Here $\Bbb K\in\{\Bbb R,\Bbb C\}$ and the derivative is clear as the derivative of a $m$-linear function for the $a_k$ as vector columns (or rows) of a $m\times m$ matrix. Trying to use this I can see that $$f'(t)=\partial\det(e^{tA})e^{tA}A-{\rm tr}(A)e^{t\,{\rm tr}(A)}$$ But it is far to be clear how the identity of the question can be proved from here, that is, how to prove that $f'=0$. Anyone knows how to handle this proof? Thank you.","['linear-algebra', 'analysis']"
2657284,Is the equation $A = \frac{d^2}{8}\left(\alpha-\sin\alpha\right)$ solvable for $\alpha$?,"I have been given this equation. $A$ and $d$ are known, and I want to solve to $\alpha$.
$$A=\frac{\pi d^2}{4}\cdot\frac{\alpha}{2 \pi}-\frac{d\,\sin\left(\frac{\alpha}{2}\right)\left[\frac{d}{2}-\frac{d}{2}\left(1-\cos\left(\frac{\alpha}{2}\right)\right)\right]}{2}$$
Until now, I've tried my best to land at this:
$$A=\frac{\pi\,d^2}{4}\cdot\frac{\alpha}{2 \pi}-d^2\sin\left(\frac{\alpha}{2}\right)\left[-\frac{1-4\cos\left(\frac{\alpha}{2}\right)}{8}\right]$$
Now I don't know how to proceed or if this can even be solved. Edit by @Blue. As @AdrianKeister shows, the equation should actually reduce to $$ A = \frac{d^2}{8}\left(\;\alpha - \sin \alpha\;\right)$$ I've used this form to provide a more-informative title for this question.",['trigonometry']
2657289,Proving $p_{n+1}<p_n^2$ without Bertrand's postulate,"How can we prove that 
$$p_{n+1}<p_n^2$$
Where $p_n$ is the nth prime number.
Using Bertrand's Postulate it becomes easy. But how can we prove it without using this deep result?","['inequality', 'number-theory', 'elementary-number-theory', 'prime-numbers', 'discrete-mathematics']"
2657332,Study if a tree represents a tautological proposition,"I have difficulties to solve this excercise: The following path of a given tree in customary infix notation:
$$[(\neg(p\wedge q)\Rightarrow r)\wedge ((r\vee q)\Rightarrow s)]$$
represents a tautological proposition (do not make truth table). It is not possible to go through the 1) tree in preorder. For the first one, I don't know why there is the data of how the tree is traversed (usual infix notation): is it because it is the classical (or only) way to reduce a complex proposition in classical logic? Because with another notation it is not valid (as it says in point 2., which I think is true). However, it is clear that 1. is not a tautology, because if $q$ is true and $s$ is false, the expression is false. So this excercise is solved like this, or do we have to apply logical laws?:
$$\begin{matrix} (\neg(p\wedge q)\Rightarrow r)\wedge ((r\vee q)\Rightarrow s)&\underbrace{\Leftrightarrow}_{\textrm{Conditional equiv.}\\\;\;\textrm{and involution}}\\ ((p\wedge q)\vee r)\wedge (\neg (r\vee q)\vee s)&\underbrace{\Leftrightarrow}_{\textrm{De Morgan}}\\ ((p\wedge q)\vee r)\wedge ((\neg r\wedge \neg q)\vee s)&\underbrace{\Leftrightarrow}_{\textrm{Distributive}}\\ (p\vee r)\wedge (q\vee r)\wedge (\neg r\vee s)\wedge (\neg q\vee s),& \end{matrix}$$
but from here I do not know how to continue. For the second I made the tree path: Looking at the graph the preorder path would be $$\wedge\Rightarrow\neg\wedge pqr\Rightarrow\vee rqs,$$ and this in logic does not represent anything; it has no meaning, therefore we can't go through the tree in preorder. Any help would be appreciate! Thanks!","['trees', 'propositional-calculus', 'discrete-mathematics']"
2657374,$\sigma(n) \equiv 1 \space \pmod{n}$ if and only if $n$ is prime,"For $n>1$ , let $\sigma(n)$ denote the sum of all positive integers that evenly divide $n$ . Then $\sigma(n) â‰¡ 1 \space(mod\space n)$ if and only if $n$ is prime. I've been trying to prove this for a long time, but i can't figure it out. I found a theorem that might be helpful: $\sigma(n) = n + k$ has finitely many solutions for $k > 1$ (more specifically, this equation has no solutions for $nâ‰¥k^2$ ). proof: let $nâ‰¥k^2>1,\space \sigma(n) = n + k$ . Note that $n$ must be a composite number (otherwise $k=1$ ). Therefore, $n$ has a divisor $dâ‰¥\sqrt n$ . From $\sigma(n)$ 's definition: $\sigma(n) â‰¥ n+d+1â‰¥n+\sqrt n + 1 â‰¥ n + k + 1 > n + k$ If anyone can generalize this to $\sigma(n)=qn+k$ , or something like that, it might help.","['number-theory', 'divisor-sum', 'prime-numbers', 'modular-arithmetic']"
2657377,An identity of complete homogeneous symmetric polynomials $\sum_{k=1}^\infty k h_k(X)h_k(Y) t^{k-1}$,"Let $p,q$ be integers, $X, Y$ two groups of different variables, $X=(x_1,\ldots,x_p)$ and $Y=(y_1,\ldots,y_q)$, and $h_r$ a complete homogeneous symmetric polynomials. Is there an identity analogue to Cauchy's identity, in the form:
$$ \sum_{k=1}^\infty k h_k(X)h_k(Y) t^{k-1}? $$","['combinatorics', 'symmetric-polynomials']"
2657380,Understanding why linear independent columns relate to positive definite matrix and invertibility,"OK folks. So, lets say we have a matrix A. The inverse of $A^TA$ exists iff the columns of A are linearly independent. Then, we want to prove this by considering that the product $A^TA$ is positive definite, since, apparently, if $A^TA$ is positive definite and invertible, the columns are linearly independent. I hope I explained this well. Otherwise, would appreciate somebody can provide some insight and intuition of what is going on. Thank you all.","['matrices', 'positive-definite', 'linear-algebra']"
2657403,Taylor's theorem implies existence of n+1 order derivative?,"From my  Calculus textbook: Usually the text is careful to make sure the description above a formula establishes all the necessary preconditions to exist for the formula to be true. I noticed here though that $f^ {(n+1)} $ is referred to, without any mention of a requirement that it exist (f being order n differentiable by itself doesn't imply n+1 differentiability). Does the continuity of all the earlier derivatives imply it or is there an unstated assumption?","['derivatives', 'taylor-expansion', 'calculus']"
2657405,Sufficient conditions for global stability from linear stability.,"Consider
$$ \dot x = -Ax+f(x),$$
where $A>0$ and $f(x)$ smooth. Question: What are sufficient conditions for $f(x)$ such that the origin of the system is globally asymptotically stable? I know that the local version holds. But I am looking for global version. A trivial example would be $f(x)=Bx$ with $-A+B<0$. My approach: Let $x_l(t)$ denote the solution of the linear system and $x(t)$ of the nonlinear. Moreover, suppose that $f(x)$ is uniformly bounded, say $||f(x)||<\beta$ with $\beta>0$, and that $f(x)=0\iff x=0$. It follows that
\begin{equation}
\begin{split}
|| x(t)-x_l(t)|| &= ||\int_0^te^{-A(t-\tau)}f(x(\tau))d\tau||\\
&\leq \int_0^t  ||e^{-A(t-\tau)}||\cdot||f(x(\tau))||d\tau\\
&\leq \beta\int_0^t  ||e^{-A(t-\tau)}||d\tau,\\
\end{split}
\end{equation}
where the second inequality is due to our assumptions. The above error does not converge to $0$ (but to $\beta||A^{-1}||$) as $t\to\infty$. So, the assumptions only lead to stability. Naturally I can pick some specific cases where the integral can be explicitly solved to ensure that the error converges to zero, but I fail to see a general assumption on $f(x)$.","['stability-in-odes', 'ordinary-differential-equations', 'stability-theory']"
2657427,How to win in this game,"You select 10 numbers from the set $\{2,3,\dots,12\}$. 
You then continually roll 2 fair dice and sum them up, until your selection of 10  numbers come up. For example if your selection was 7,7,7,7,8,8,8,6,6,6 (4 7's, 3 8's and 3 6's), and you roll the dice repeatedly and get 7,7,6,5,8,7,7,9,8,3,5,10,12,6,6,3,2,5,7,9,8, you may stop now because 4 7's have come up, 3 8's and 3 6's. What is the best choice of 10 numbers so as to minimise the number of rolls ?","['stochastic-processes', 'probability', 'dice', 'game-theory']"
2657457,"Pythagorean triples that ""survive"" Euler's totient function","Suppose you have three positive integers $a, b, c$ that form a Pythagorean triple:
\begin{equation}
  a^2 + b^2 = c^2. \tag{1}\label{1}
\end{equation}
Additionally, suppose that when you apply Euler's totient function to each term, the equation still holds:
$$
  \phi(a^2) + \phi(b^2) = \phi(c^2). \tag{2}\label{2}
$$
One way this can happen is if $a^2, b^2, c^2$ have the same primes in their prime factorization. (For example, starting from the Pythagorean triple $3,4,5$, we could multiply all three terms by $30$ to get $90, 120, 150$. If we do, then we have $90^2 + 120^2 = 150^2$ and $\phi(90^2) + \phi(120^2) = \phi(150^2)$.) In that case, because all three terms are squares, they all contain these prime factors at least twice, and so we must have
$$
   \phi(\phi(a^2)) + \phi(\phi(b^2)) = \phi(\phi(c^2)). \tag{3}\label{3}
$$
My question is: are there any ""atypical"" solutions to the two equations $\eqref{1}$ and $\eqref{2}$ for which $\eqref{3}$ does not hold? Or at least where $\eqref{1}$ and $\eqref{2}$ hold, but the prime factorizations of $a,b,c$ do not consist of the same primes, even if $\eqref{3}$ happens to hold for a different reason? In the comments, Peter and Gerry Myerson have checked small cases (all triples for $1 \le a \le b \le 10^5$ and primitive triples generated by $(m,n)$ for $1 \le n \le m \le 2000$) without finding any atypical solutions. Here is an in-depth explanation for why typical solutions like $(90,120,150)$ work. By a typical solution, I mean a solution where $a,b,c$ have the same primes in their prime factorization. Such a triple satisfies $\eqref{2}$ and $\eqref{3}$ whenever it satisfies $\eqref{1}$, as shown below. Let $\operatorname{rad}(x)$ denote the radical of $x$: the product of all distinct prime factors of $x$. To get a typical solution, we start with any Pythagorean triple, then scale $(a,b,c)$ so that $\operatorname{rad}(a) = \operatorname{rad}(b) = \operatorname{rad}(c) = r$. It is a general totient function identity that whenever $\operatorname{rad}(x) = r$, $\phi(x) = \frac{\phi(r)}{r} \cdot x$. In other words, $\phi(x) = x \prod\limits_{p \mid x} \frac{p-1}{p}$ where the product is over all primes $p$ that divide $x$. In the case above, we have
$$
   \phi(a^2) + \phi(b^2) = \frac{\phi(r)}{r} \cdot a^2 + \frac{\phi(r)}{r} \cdot b^2 = \frac{\phi(r)}{r} \cdot c^2 = \phi(c^2),
$$
and $\eqref{2}$ holds.
Moreover, since $r \mid a,b,c$, we have $r^2 \mid a^2,b^2,c^2$, so when we multiply by $\frac{\phi(r)}{r}$, we have $r \phi(r) \mid \phi(a^2), \phi(b^2), \phi(c^2)$. Therefore all prime factors of $r \phi(r)$ divide each of $\phi(a^2)$, $\phi(b^2)$, and $\phi(c^2)$. These are all their prime factors, since $r$ contained all the prime factors of $a^2, b^2,c^2$ and since then the only new prime factors introduced came from multiplying by $\phi(r)$. As a result, $\phi(a^2), \phi(b^2), \phi(c^2)$ still have the same set of prime factors: $\operatorname{rad}(\phi(a^2)) = \operatorname{rad}(r \phi(r)) = s$, and similarly $\operatorname{rad}(\phi(b^2)) = \operatorname{rad}(\phi(c^2)) = s$. So $\eqref{3}$ holds, because
$$
   \phi(\phi(a^2)) + \phi(\phi(b^2)) = \frac{\phi(s)}{s} \cdot \phi(a^2) + \frac{\phi(s)}{s} \cdot \phi(b^2) = \frac{\phi(s)}{s} \cdot \phi(c^2) = \phi(\phi(c^2)).
$$","['number-theory', 'diophantine-equations', 'pythagorean-triples', 'totient-function']"
2657501,Infinite product of the form $2-2^{1/k}$,"How can I show that $$\lim_{n\to\infty} \prod_{k=2}^{n} (2-2^{1/k})=0$$ This is an exercise from a college admission exam, and the answer is given as 0.
 I don't understand how infinitely many positive numbers can have a product equal to $0$.
 I tried to take the logarithm of the product and use the fact that $$\lim_{x\to0} \frac{\ln(1+x)}{x}=1$$ and get rid of it, but I am left with $$\sum_{k=2}^{\infty} (1-2^{1/k})$$ Is it correct? Any other approaches would be appreciated.","['infinite-product', 'sequences-and-series']"
2657502,Bose-Einstein function as real part of polylogarithm: $\overline{G}_{s}(x)= \Re \mathrm{Li}_{s+1}(e^x)$,"For real $x<0$ the Bose-Einstein integral of order $s$ is given at https://dlmf.nist.gov/25.12.E15 as $$G_{s}(x)=\frac{1}{\Gamma\left(s+1\right)}\int_{0}^{\infty}\frac{t^{s}}{e^{t-x}
-1}\mathrm{d}t$$ which can be written as a polylogarithm $G_s(x) = \mathrm{Li}_{s+1}(e^x)$. For $x>0$ this value of $G_s$ becomes complex. In an old paper from 1954
( On Bose-Einstein Functions )
J. Clunie defines the Bose-Einstein functions for $x>0$ by the Cauchy principal value $$\overline{G}_{s}(x)=\frac{1}{\Gamma\left(s+1\right)}\lim_{\delta \to 0}\left(\int_{0}^{x-\delta}+ \int_{x+\delta}^{\infty}\right)\frac{t^{s}}{e^{t-x}
-1}\mathrm{d}t$$
(actually the pre-factor is omitted). The table for $\overline{G}_{1/2}(x)$ with $x >0$ can be reproduced numerically with Maple as $\overline{G}_s(x) = \Re \mathrm{Li}_{3/2}(e^x)$. Is there a known proof that $$\overline{G}_{s}(x)=\frac{1}{\Gamma\left(s+1\right)}\lim_{\delta \to 0}\left(\int_{0}^{x-\delta}+ \int_{x+\delta}^{\infty}\right)\frac{t^{s}}{e^{t-x}
-1}\mathrm{d}t = \Re \mathrm{Li}_{s+1}(e^x),\quad s>0, x>0  $$ I searched the internet without success for a relevant link, but perhaps there is a general result from complex analysis. Edit: Thanks to R.J. Mathar's links I have the following argument:
Ng cites Dingle and gives the relation to the Fermi-Dirac functions (see https://dlmf.nist.gov/25.12.E14 )
$G_s(x)=-\Re F_s(x+i\pi).$  If we use the integrals in a heuristic way this gives for $x>0$ $$G_s(x) = -\Re F_s(x+i\pi) \stackrel{?}{=}
 -\Re \left(\frac{1}{\Gamma\left(s+1\right)}\int_{0}^{\infty}\frac{t^{s}}{e^{t-x-i\pi}+1}\mathrm{d}t\right) =
 -\Re \left(\frac{1}{\Gamma\left(s+1\right)}\int_{0}^{\infty}\frac{t^{s}}{1-e^{t-x}}\mathrm{d}t\right)=\Re \mathrm{Li}_{s+1}(e^x)$$ Is the second step justified? From the DLMF notation, I would think that the argument of $F_s(x)$ should be real in the integral form, otherwise they use $z$ for complex variables?!","['special-functions', 'complex-analysis', 'polylogarithm', 'cauchy-principal-value']"
2657506,"If I know the variance of two numbers, is it possible to find their difference?","If I know the variance of two numbers, is it possible to find their difference? As in, the only information I have is the variance and that there are only two numbers. How could I work backwards to find out what the difference between the two numbers is?","['statistics', 'variance']"
2657549,If $f: \mathbb{R}^n \rightarrow \mathbb{R}$ such that $\alpha \|x-y\| \leq \|f(x)-f(y)\|$ then $f(\mathbb R^n)$ is closed.,"Let $f : \mathbb{R}^n \rightarrow \mathbb{R}$ a $\mathcal{C}^1$ function such that there exists $\alpha > 0$: $$ \alpha \|x-y\| \leq \|f(x)-f(y)\|, \forall x,y \in \mathbb{R}^n$$
  1)Show that $f(\mathbb{R}^n)$ is a closed set. I don't know how to approach this exercise. I tried to  take a Cauchy sequence $(y_n) = f(x_n)$, then as $\mathbb R^n$ is a Banach space, I immediately have that $(x_n)$ converges, thus $f(x_n)$ converges in $f(\mathbb{R}^n)$, thus is closed. But I don't know how to show this rigorously, and I am unsure if my idea is correct.","['multivariable-calculus', 'real-analysis', 'calculus']"
2657571,"Probability of picking matching socks, after partitioning the drawer.","Apologies if this is a duplicate. I searched and didn't find anything quite like it. Suppose I have a drawer with an equal number of N black socks and N white socks. They're all mixed up. So, my chances of picking a matching pair in the first two selections is (N-1)/(2N-1), right? Well, what if, before I pick the first sock, I randomly (so I don't know the colors of the socks I'm moving) partition the drawer so that there are N socks on each side, and I draw one sock from each side. Do the chances of drawing a matching pair change ? On the one hand, we can see that selection from one side doesn't change the composition of socks on the other side of the partition. However, whichever color I choose from the ""first"" side, it's likely that there are more of that color on that side. On other words, if I draw a black sock from one side, it's more likely that that side had N-1 blacks and 1 white than it is that that side had 1 black and N-1 whites. My suspicion is that I need to do some kind of hypothesis testing, where I consider the chances of every possible partitioning, but that's way above my skill level.","['statistics', 'probability']"
2657576,$\sum\limits_{k=0}^{\frac{n}{2}} 2k\binom{n}{2k}=n\cdot2^{n-2}$,"Let $n\geq 2$ be even. Show that
  $$
\sum\limits_{k=0}^{\frac{n}{2}} 2k\binom{n}{2k}=n\cdot2^{n-2}
$$
  1) In a combinatorial way. (hint: count pairs $(x,S)$ s.t. $x\in S\subset \{1,2,...,n\}$ where $|S|$ is even) 2) Using binomial theorem. (hint: derivate $(1+x)^n+(1-x)^n$ ) Progress : 1) From the hint I was able to derive that the LHS counts the number of subsets $S_i$ where $|S_i|$ is even for all $i$s, multiplied by its cardinality, though I'm not sure about that. How does one put it in a combinatorial proof? 2) I calculated the derivative, but nothing yet.","['combinatorics', 'binomial-coefficients']"
2657623,Schur polynomials on 1's and -1's,"The Schur polynomial in $d$ variables $s_\lambda (x_1,x_2,\ldots,x_d)$ for an integer partition $\lambda$ of $k$, has a simple form when we evaluate all arguments at one, i.e. $s_\lambda (1,\ldots,1)$. We can write the expression in terms of the character of the identity as
$$
s_\lambda (1,\ldots,1) = \frac{1}{k!} \chi^\lambda(\mathbb{1}) z_\lambda(d)\,, \quad {\rm where} \quad z_\lambda(d) = \prod_{(i,j)\in \lambda} (d+j-i)
$$
with the product taken over the Young diagram of $\lambda$. This is often written more explicitly as
$$
s_\lambda (1,\ldots,1) = \prod_{1\leq i<j \leq d} \frac{\lambda_i - \lambda_j +j-i}{j-i}\,.
$$
For our purposes, given a partition $\lambda$, $s_\lambda (1,\ldots,1)$ is just some polynomial in $d$.
For example, the partition $\lambda=\{2,1\}$ gives $s_\lambda(1,\ldots,1) = \frac{1}{3}d (d^2-1)$. I am interested in Schur polynomials of the form $s_\lambda(1,\ldots,1,-1,\ldots,-1)$, i.e. a polynomial of $d$ arguments, evaluated on $a$ 1's and $b$ -1's, where $a+b=d$. Is there a simple expression for $s_\lambda(1_a,-1_b)$ as a polynomial in $a$ and $b$? I am aware that we may combine Schur functions using the Littlewood-Richardson rule as $s_\lambda(x,y) = \sum_{\mu,\nu} c^\lambda_{\mu\nu} s_\mu (x) s_\nu (y)$. By computing the coefficients and using the expressions above it seems I can find the expression I want. Is there a simpler way to compute $s_\lambda(1_a,-1_b)$? Also, is there a simple way to compute the coefficients $c^\lambda_{\mu\nu}$ in terms of characters $\chi^\lambda$?","['combinatorics', 'representation-theory', 'symmetric-groups', 'symmetric-polynomials']"
2657632,$\mathbb{F}_2[\alpha] \cong \mathbb{F}_2[x]/(x^2+1)$?,"My question involves part (b) of Chapter 11 problem 6.4 in Artin's Algebra textbook. In each case, describe the ring obtained from $\mathbb{F_2}$ by adjoining an element $Î±$ satisfying the given relation: (a) $Î±^2+Î±+1=0$ (b) $Î±^2+1=0$ (c) $Î±^2+Î±=0$ Now, I obtained that the ring in part (a) is isomorphic to $\mathbb{F_4}$ and that the ring in part (c) is isomorphic to $\mathbb{F_2}\times\mathbb{F_2}$ . It seems to me that the ring in part (b) would be isomorphic to $\mathbb{F}_2[x]/(x^2+1)$ , but my teacher doesn't agree. He said, ""Be careful: notice the polynomial $x^2+1$ is not irreducible over $\mathbb{F}_2$ . Adjoining a root of a reducible polynomial is not the same as taking the quotient $\mathbb{F}_2[x]/(x^2+1)$ "" So, is my teacher right, or am I? And why?","['abstract-algebra', 'ring-theory']"
2657644,Integrating a function over a vertical line in the complex plane,"This is quite a simple question, and should hopefully have a simple answer. How do I go about computing $$\int_{2-i\infty}^{2+i\infty} \frac{1}{t} \mathrm{d}t?$$ I've never seen such an integral before and don't know how to approach this.","['complex-analysis', 'analytic-number-theory', 'complex-integration']"
2657664,Find the determinant of the following $5\times 5$ real matrix:,"Let $A \in \mathbb{R}^{5 \times 5}$ be the matrix $$\begin{bmatrix} a&a&a&a&b\\a&a&a&b&a\\a&a&b&a&a\\a&b&a&a&a\\b&a&a&a&a\end{bmatrix}$$ Find the determinant of $A$ . What I've done so far : $$\det\left(\begin{array}{l}a&a&a&a&b\\a&a&a&b&a\\a&a&b&a&a\\a&b&a&a&a\\b&a&a&a&a\end{array}\right) = \det\left(\begin{array}{l}b&a&a&a&a\\a&b&a&a&a\\a&a&b&a&a\\a&a&a&b&a\\a&a&a&a&b\end{array}\right)$$ (since switching two pairs of rows does not change the determinant) $$= \det\left(\begin{array}{l}b-a&0&0&0&a-b\\0&b-a&0&0&a-b\\0&0&b-a&0&a-b\\0&0&0&b-a&a-b\\a&a&a&a&b\end{array}\right)$$ (since adding a multiple of one row to another does not change the determinant) for all $1\le i\le 4 \rightarrow R_i-R_5$ Now I am quite stuck. I wanted to obtain a triangular matrix so I can calculate its determinant by the diagonal entries, but I don't know what to do with the last row. I've tried some column operations as well, but have had no success. Would be happy to get your help, thank you :)","['matrices', 'hankel-matrices', 'linear-algebra', 'determinant']"
2657685,1st Yr Probability: question about marginal and joint pdfs for $3$ uniform continuous independent random variables,"Background I'm trying to improve my understanding of the relationship between marginal and joint pdfs for calculating specific probabilities. The Problem $X$, $Y$, $Z$ are independent and uniformly distributed $(0,1)$. What is $P(X>YZ)$? My question The book solution is below, but I'm wondering if I can solve this with the marginal distribution of $X$ alone. The marginal pdf of $X$ is $f_X(x) = 1$ In theory with $f_X(x)$ I can calculate any probability for $X$. I believe that is the whole point of having a pdf for a random variable. Therefore:$$P(X>YZ) = \int_{YZ}^{1}dx$$ $$=1-YZ$$ But this definitely isn't the right answer (which as you see below is $3/4$). The book solution makes complete sense to me. My question is why can't we get the answer from the marginal pdf of $X$? Shouldn't a marginal pdf for a RV answer all probability statements for that RV? Thanks for your help and patience! Book Solution",['probability']
2657709,How many rational numbers are in the opening of this binomial: $(\sqrt3+\sqrt[3]2)^{100}$,"73) How many rational numbers are in the opening of the binomial? $$(\sqrt3+\sqrt[3]2)^{100}$$ For the first time I solve such a question. My approach: $$
\left\lbrace\begin{array}{ccccccl}
 (\sqrt3)^{2m}Ã—(\sqrt[3]2)^{3n}\in \mathbb{Q} 
\\[1mm]
2m+3n=100
\end{array}\right.\Longrightarrow 2m+3n=100 \Longrightarrow 2m+6k=100 \Longrightarrow m=50-3k,\left\{kâ‰¥0,mâ‰¥0  \right\} \Longrightarrow k=0,1,2,3,..,16  \Longrightarrow 16+1=17
$$ Is this the correct way to find the answer?","['algebra-precalculus', 'binomial-theorem', 'binomial-coefficients', 'rational-numbers']"
2657731,Completeness of Continuous Functions on Uniform Spaces,"So I'm trying to find the most general setting in which I can talk about completeness of function spaces. In metric spaces, it's simple to show that the space $C(X,Y)$ of continuous functions $X\longrightarrow Y$ is complete whenever $X$ is compact and $Y$ is complete. I have a hunch that $C(X,Y)$ is complete if $X$ is compact and $Y,\mathcal U$ is a complete uniform space. First of all, we can generate a uniform structure $\mathcal F$ on $C(X,Y)$ by declaring that for each entourage $U\in\mathcal U$, there is an entourage $F\in\mathcal F$ defined by 
$$(f,g)\in F \iff \forall x\in X, (f(x),g(x))\in U.$$
(Note: I am open to the fact that another formulation of uniform spaces might be more appropriate here, particularly the pseudometric formulation. Feel free to demonstrate this!) My questions are:
(i) under what conditions is $C(X,Y)$ complete, and (ii) does the uniform topology on $C(X,Y)$ coincide with the compact-open topology?","['functional-analysis', 'general-topology', 'metric-spaces', 'uniform-spaces']"
2657837,Applications of Chern class to gauge theories in physics,"There is a statement about the 2nd Chern class $c_2$ concerning complex vector bundles:
$$
1/ (8 \pi^2) \int tr[Fâˆ§F] =c_2 \in \mathbb{Z}
$$ (1) Does this statement depend on the choice of gauge group for the curvature 2-form (or field strength?), say in the case of SO(N), SU(N), or Sp(N)? How does this formula get modified for different gauge groups? (2) Since the trace tr suggests that the we are writing the Lie algebra generators in terms of certain Representations. How do the Representations affect the expression of the above formula? How do we know which normalization to choose for certain Representations? [for example, choosing between Representations of fundamentals, adjoint or vector etc.] (3) How does the formula above change regarding to the spin structures of manifold (spin or not)? [e.g. the normalization $1/ (8 \pi^2)$ changes. How and why?]","['algebraic-geometry', 'characteristic-classes', 'mathematical-physics', 'algebraic-topology', 'differential-geometry']"
2657856,What are some must-know trig identities?,"I'm having a hard time with trigonometry. Not in the sense that I can't solve problems, but that I don't understand what I'm using. I see a trig identity and just sort of accept that it works without necessarily seeing how it was derived. But most of the time when I see the equivalence I have no idea how they got there. Are there a few ""core"" identities from which the rest can be derived? Or am I barking up the wrong tree and just need to suck it up and memorize?","['algebra-precalculus', 'trigonometry', 'proof-explanation']"
2657863,Intuitive explanation of a derivative with an example,"I am having a hard time grasping a concept of derivative intuitively, perhaps due to a lack of a good example of how it can be used in practice. I am looking for an explanation in laymen terms with a practical example that can be deconstructed and would give an idea of how a derivative can be used in practice. I am not looking for mathematical proof or strict mathematical definition. Here is my current understanding, please point out where it is correct or incorrect intuitively: Let's say that we have $y$ (dependent variable or output) and $x$ (independent variable or input).
If we have a function of   $y=x^{3}$. Does derivative tell us by how much the output of a function (dependent variable $y$) when we change input (independent variable $x$) by a certain amount ($dx$)? In other words, derivative tells us how sensitive the function is to the changes in its input. P.S. I could not find a satisfactory explanation of this question anywhere on stack exchange.","['derivatives', 'calculus']"
2657896,"Let $f(x)$ be a differential real function defined on the real line. If $f(0)=0$ and $f'(x)=[f(x)]^2$, then $f(x)=0$ foi any $x$.","Again, $f:\mathbb{R}\to\mathbb{R}$ is differentiable, $f(0)=0$, and $f'(x)=[f(x)]^2$ for every $x$. A friend suggested the following argument: If exists $c$ such that $f(c)\neq0$, there exists an interval $I$ around $c$ such that $f(x)\neq0$ if $x\in I$ (because $f$ is continuous since it is differentiable). In that interval, we could define $g(x)=x+\frac{1}{f(x)}$. This function $g$ would be differentiable and $g'(x)=0$. Then $g(x)$ is constant, for example, $k$. Then, $f(x)=\frac{1}{k-x}$ for $x\in I$ But I don't know where to find an absurd. What should I do next? I think I should use the fundamental theorem of calculus and try to find an absurd with $f(x)=\int_0^x f'(t)dt=\int_0^x [f(t)]^2 dt$, but I also didn't got anywhere. Thanks in advance.","['derivatives', 'real-analysis']"
2657916,Largest size of the automorphism group of a 3-regular graph on 12 vertices,"Let $ G $ be a 3-regular connected graph on 12 vertices. We also assume that $ G $ is undirected, unweighted, and simple. What will be the largest possible size of the automorphism group of $ G $? The example I found with a large automorphism group is a hexagonal prism, whose automorphism group is generated by (1 2 3 4 5 6)(7 8 9 10 11 12), (1 7)(2 8)(3 9)(4 10)(5 11)(6 12), and (2 6)(3 5)(8 12)(9 11), corresponding to a $\pi/6$ - rotation and reflections. This automorphism group has size 24. Can anyone come up with an example with larger automorphism group?","['graph-theory', 'group-theory']"
2657967,How to show that a $30$-gon is constructible?,I have already shown a $10$ -gon is constructible. The I am trying to use the fact that the angle $\cos(2\pi/10)$ is constructible and that $$\cos(2\pi/10) = \cos(3(2\pi/30)) = 4\cos^3(2\pi/30)-3\cos(2\pi/30)$$ and that $\cos(2\pi/10) = (\sqrt5-1)/8$ . After this step I get lost. All help appreciated.,"['geometric-construction', 'euclidean-geometry', 'trigonometry']"
2657972,Question about Hopf invariant from Milnor,"In Milnor's book Topology from the Differentiable Viewpoint , there is a problem concerning the definition of the Hopf invariant.  Let $y\neq z$ be regular values of a smooth map $f:S^{2p-1}\to S^p$, then we want to show that the linking number $\ell(f^{-1}(y),f^{-1}(z))$ is locally constant as a function of $y$.  The Hopf invariant of $f$ is then defined as $\ell(f^{-1}(y),f^{-1}(z))$, after several more parts of this exercise showing that this quantity only depends on the homotopy class of $f$. Here, the linking number $\ell(M,N)$ for compact, oriented, boundaryless submanifolds $M^m,N^n$ of $S^{m+n+1}$ is defined by picking some $p\in S^{m+n+1}\setminus(M\cup N)$, identifying $S^{m+n+1}$ with $\mathbb R^{m+n+1}$.  The linking number is then defined by the degree of the map $\lambda:M\times N\to S^{m+n}$ given by
$$\lambda(x,y)=\frac{x-y}{\|x-y\|}.$$ My idea was to use the framed cobordism theory outlined in $\S7$.  For if we choose a neighborhood $U$ of $y$ consisting of regular values of $f$ with $z\in U$, and if $y_0\in U$, then $f^{-1}(y)$ is framed cobordant to $f^{-1}(y_0)$.  But I don't know where to go from here. Any hints about how I should proceed would be greatly appreciated.","['algebraic-topology', 'differential-geometry', 'differential-topology']"
2657980,"What is the slope of the line tangent to the surface at that point $(2,-1)$ lying in the plane $y=-1$?","For the function $f(x,y)=x(x+y^5)$,What is the slope of the line
  tangent to the surface at that point $(2,-1)$ lying in the plane
  $y=-1$? I know how to find the linear approximation of any function,but i'm not getting any approach of finding the slope. Please suggest any method/formula....","['multivariable-calculus', '3d', 'real-analysis', 'derivatives']"
2657983,"If $n$ is a multiple of $8$, then the number of sets of size divisible by $4$ is $2^{n-2} + 2^{(n-2)/2}$","Question in Cameron, Combinatorics: If $n$ is a multiple of $8$, then the number of sets of size divisible by $4$ is $2^{n-2} + 2^{(n-2)/2}$ We are given a property derived from the Binomial theorem: For $n > 0$ , the number of subsets of an $n$-set of even and of odd cardinality are equal.
Given we know that for a set $A$ with cardinality $n$, its power set has cardinality $2^n$ we can deduce that half the elements in the power set of $A$ are of even cardinality and the rest of odd, hence each is of size $2^{n-1}$ Given even sets are sets with cardinality divisible by $2$, we know $P(A_{\text{ div by 2}}) = P(A_{\text{div by 4}}) + P(A_{\text{4l+2 for some l}}) = a+b = 2^{n-1}$. This part I understand. Now what gets me confused the the continuation of this solution, where we use the binomial theorem as so: $(1+i)^n = 2^{n/2} = \sum_{k=0}^n {n \choose k} i^k$ given $(1+i) = \sqrt 2 e^{i\pi / 4}$ and somehow we obtain $a-b = 2^{n/2}$ allowing us to find $a$. What I do not understand, is how we can get the idea of using complex numbers to find this result. If you could detail the thinking process here, that would be much appreciated! Moreover how do we find the expression for $a-b$? I am also confused about that. Thank you very much!","['combinatorics', 'binomial-theorem', 'binomial-coefficients', 'complex-numbers']"
2658007,Asymptotic Consistency of Estimator,"Let $X_1,X_2,...,X_n$ be a random sample where $X_i$'s are i.i.d. and are from an Exponential distribution with mean $\theta$ and variance $\theta^2$. Define the following estimator of $\theta$: $$\hat\theta=\frac{n\bar X}{n+1}$$ What is the bias of $\hat\theta$?  Is $\hat\theta$ asymptotically consistent?  Is it mean square error (MSE) consistent? Here's my attempt at the solution, but I'm unsure: $Bias(\hat\theta)=E(\hat\theta)-\theta=\theta\left(\frac{n}{n+1}-1\right)=\frac{-\theta}{n+1}$ Here's the place that I'm not sure; does $\lim_{n \to \infty} Bias(\hat\theta)=0$ imply that $\hat\theta \rightarrow^P\theta$?  If so, is this alone enough to show that $\hat\theta$ is asymptotically consistent?  Unfortunatley, I haven't really been able to find a clear definition of ""asymptocially consistent,"" and this is just what I assume it means. Finally, for MSE consistency, I calculated that $\lim_{n \to \infty} V(\hat\theta)=\theta^2\ne 0$, which should imply that $\hat\theta$ is not MSE consistent.  But, once again, I also haven't found a clear definition for this terminology, so I'm not sure of it either. What do you think?","['probability-theory', 'random-variables', 'statistics', 'probability', 'parameter-estimation']"
2658035,Is the Point Estimate for variance of Bernoulli Unbiased?,"Let $X_1,...,X_n$ be a random sample from the Bernoulli family with parameter $p$. Let $\sigma^2$ be the variance of $X_1$. Consider the point estimate for $\sigma^2$ given by $\overline{X}(1-\overline{X})$. Is it unbiased? Since $\overline{X}(1-\overline{X}) = \overline{X}-(\overline{X})^2$, we have $\mathbb{E}(\overline{X}(1-\overline{X})) = \mathbb{E}(\overline{X}-(\overline{X})^2)= \mathbb{E}\overline{X}-\mathbb{E}(\overline{X})^2$. We know that $\mathbb{E}\overline{X}=\mu$ and $\mathbb{E}(\overline{X})^2=\frac{\sigma^2}{n}+\mu^2$. Putting them in $\mathbb{E}\overline{X}-\mathbb{E}(\overline{X})^2$ we find that it equals to $\mu-\frac{\sigma^2}{n}-\mu^2$ which is not equal to $\sigma^2$ and thus it is not unbiased. Is this correct?","['probability-theory', 'statistics']"
