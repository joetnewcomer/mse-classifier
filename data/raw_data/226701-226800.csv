question_id,title,body,tags
4682927,Where does the formula for solving exact differential equations come from?,"My professor put this formula in the lecture notes for solving exact differential equations: $$
\int_{x_0}^x M(x, y)dx + \int^y_{y_0}N(x_0, y)dy = C
$$ He also says that we usually put $x_0 \text{ and } y_0 = 0$ . But I don't understand where it comes from or why does it work.","['calculus', 'ordinary-differential-equations']"
4682980,Proving the solution of the IVP is positive for all $t$,"I have the following problem: \begin{equation}
\frac{d x}{d t}=-\frac{x^3}{1-t} \quad x(0)=x_0>0
\end{equation} How can I show that $x(t)>0$ for $t \in\left[0, \alpha\right)$ , where $\alpha \leq 1$ ? I have some ideas, but have not been able to use them to write/reach a proper conclusion. We can study the sign of $\frac{d x}{d t}$ to conclude that it is negative in the interval $\left[0, \alpha\right)$ , thus implying that $x(t)$ should be decreasing in the same interval. Moreover, from the initial condition, we know that $x(t)$ must be positive in a neighborhood of $x_{0}$ . How can I combine these facts together with the theorem of existence and uniqueness to show that $x(t)$ can never reach zero nor attain negative values? Thanks in advance, Lucas","['calculus', 'derivatives', 'ordinary-differential-equations']"
4682988,gradient of functions on a unit ball,"Assume $\Omega$ is a unit open ball, and $f \in C^1(\Omega) \cap C(\overline{\Omega})$ , satisfies $|f(x)|\leq 1$ for all $x$ . How to prove there exists $x_0$ s.t. $|\nabla f(x_0)|\leq 1$ ? My idea is to assume all the gradient have norm larger than $1$ . Then we create a curve started at origin, and its tangent has direction the same (or opposite direction) as the gradient. Then since the length of the curve must larger than $1$ , then we could find a dot $x_1$ s.t. $|f(x_1)|>1$ , a contradiction. However it's hard to describe the curve, and I don't know if it's right.","['multivariable-calculus', 'vector-analysis']"
4682990,Detecting subtle signals embedded in random patterns?,"Suppose one million random 10 digit binary numbers have been generated: $$1011010101$$ $$1110010100$$ $$0110001010$$ $$...$$ A ""bug"" in the generator code is such that the $3^{rd}$ and $7^{th}$ digit of the sequence have an ""extra"" conditional algorithm tucked in: If the parity of all digits to the left of the $3^{rd}$ digit is odd, and the parity of all digits to the right of the $7^{th}$ digit is even, then there is a 10% chance of the $7^{th}$ digit morphing to match the $3^{rd}$ digit value. $$01\color{red}0010\color{red}1110$$ Suppose this bug is unknown to us. Is there a systematic statistical way to discover the inherent non-randomness in these set of 1 million numbers, and if so, is there a way to discover which digits are problematic and the exact bug algorithm that produces them?","['statistics', 'random-variables']"
4682997,Limit of a sequence of functions $f_n$ defined implicitly from the sequence of functions $F_n$ by the Implicit Function Theorem,"I have a sequence of functions $f_n(x,y)$ where each $f_n$ is defined by the Implicit Function Theorem from a particular $C^1$ function $F_n(x,y,z)$ , that is, $F_n(x,y,f_n(x,y))=0$ . The sequence $F_n$ is converging pointwise to the function $F$ which is also $C^1$ . I wonder if we can conclude that $f_n$ converges pointwise to the function $f$ defined implicitly from $F$ by the Implicit Function Theorem? I haven't been able to find such a result. Thank you for any insight.","['limits', 'multivariable-calculus', 'implicit-function-theorem']"
4683032,Boundedness of Linear Functionals,"I came across the Definition of Bounded linear functionals, that is: A linear functional $f$ defined on a normed space $X$ that is, $f: X\mapsto\mathbb{C}$ is said to be bounded if $\exists\, M>0$ such that $\forall x$ $\in X$ , $$|f(x)|\le M\|x\|$$ And since, the dual space $X^{*}$ of $X$ , is a normed space under the norm, $$\|f\|=\sup_{x\ne 0}\frac{|f(x)|}{\|x\|}$$ Then we have the following inequality $\forall x \in X$ : $$|f(x)|\le \|f\|.\|x\|$$ Comparing this with the above definition, can we actually imply that every linear functional is bounded and therefore continuous? This really confuses me because i have seen some unbounded linear functionals defined in Vector spaces.
I would be grateful to anyone who can clear this confusion.",['functional-analysis']
4683044,Prove a sequence of functions is bounded,"Context I am reading the text book Calculus With Applications, by Peter D. Lax and have a problem in the section of 'Approximating $e^x$ '. The way to do this is to take the functions $e_n(x)=(1+\frac{x}{n})^n$ and prove they converge uniformly to the function $e^x$ . The problem I confront is in the first step, which is to prove the sequence $\{e_n(x)\}$ is bounded. Complete problem description Show that for each $x>0$ , the sequence $\{e_n(x)\}$ is bounded. Hint: For $x<2$ , $e_n(x)<(1+\frac{2}{n})^n$ . Set $n=2m$ to conclude that $e_m(x)<e^2$ . My progress The confused part to me is the hint, and I wrote something like: for $x<c$ $e_n(x)=(1+\frac{x}{n})^n<(1+\frac{c}{n})^n$ let $n=cm$ $e_{cm}(x)=(1+\frac{x}{cm})^{cm}<(1+\frac{1}{m})^{m\cdot c}=e_m^c$ Now I'm stuck, especially don't know how to do with $e_{cm}(x)$ Other thinking Since the book does not clearly define the bound of a sequence of functions and how to find it. I want to make sure if it is the same as I thought: For a sequence of functions to be bounded, we need to prove that for every $x$ (in their common domain D), the sequence of numbers $\{f_n(x)\}$ is bounded.","['calculus', 'sequences-and-series']"
4683051,Conjectured identity for the ratio of Ramanujan theta functions,"Following Ramanujan, we define theta functions as follows $$\chi(q):=\prod_{n = 1}^{\infty}\left(1+q^{2n-1}\right),\\\phi(q)=\sum_{n=-\infty}^{\infty}q^{n^2},\\\displaystyle \psi(q)=\sum_{n = 0}^{\infty}q^{\frac{n(n+1)}{2}}\\\text{and}\\
f(-q)=\sum_{n =-\infty}^{\infty}(-1)^n q^{\frac{n(3n-1)}{2}}$$ where $\displaystyle |q|\lt1$ , based on his general theta function triple product $\displaystyle f(a,b)=\sum_{n =-\infty}^{\infty} a^{\frac{n(n-1)}{2}}b^{\frac{n(n+1)}{2}}=(-a;ab)_\infty(-b;ab)_\infty(ab;ab)_\infty$ The q-pochhammer symbol is customarily defined as $\displaystyle (a;q)_\infty=\prod_{n = 1}^{\infty}\left(1-aq^{n-1}\right)$ How do we prove the conjectured identity $\begin{aligned}\dfrac{\chi^9(q)}{\chi^3(q^3)}+8=9\dfrac{\phi^3(q^3)}{\phi(q)}\cdot\dfrac{\psi(-q^3)}{\psi^3(-q)}\end{aligned}$ Whereby the expansion of the right hand side divided by $9$ can readily be found in this oeis article","['q-series', 'number-theory', 'theta-functions', 'modular-forms']"
4683064,Trace inequality for $\operatorname{Tr}(ABAB)$?,"Is it true that for square matrices $A,B$ it holds that $$\operatorname{Tr}(A^2)\operatorname{Tr}(ABAB)\le (\operatorname{Tr}(A^2 B))^2$$ maybe under some additional assumptions on $A,B$ like positive semidefinite? Background: In a more general setting I obtained $$\operatorname{Tr}(A\bar A)\operatorname{Tr}(AB\bar A \bar B)+\lvert\operatorname{Tr}(AB\bar A)\rvert^2+\cdots\ge 0$$ for complex-symmetric $A$ and general $B$ , where $\bar A$ denotes the entry-wise complex conjugate. I am wondering whether the sum of the two terms above is always non-negative on their own without the rest of the sum. But then I noticed that even in the setting of real-valued matrices I do not know whether such an inequality might hold true.","['matrices', 'inequality', 'linear-algebra', 'trace']"
4683106,Find $\int_0^{\infty}\int_{{x}}^{{x}+1} \sin \left({e}^{{t}}\right) {dt} \ dx$,"Question: Let ${f}({x})=\displaystyle \int_{{x}}^{{x}+1} \sin \left({e}^{{t}}\right) {dt}$ then: (A) $\displaystyle \int_{0}^{\infty} f(x) d x \leq 2
$ (B) $\displaystyle \int_{0}^{\infty} f(x) d x > -e
$ (C) $f(x)$ has local maxima or global maxima at ${x}=\ln \left(\frac{\pi}{1+\mathrm{e}}\right)$ (D) $f(x)$ is continuous By  Leibniz integral rule: $$f'(x)=\sin(e^{x+1})-\sin(e^x)$$ For maxima: $$f'(x)=0\\\sin(e^{x+1})-\sin(e^x)=0\\e^{x+1}=n\pi+(-1)^ne^x$$ For $n=1$ : $${x}=\ln \left(\frac{\pi}{1+\mathrm{e}}\right)\\f''(x)=\mathrm{e}^{x+1}\cos\left(\mathrm{e}^{x+1}\right)-{e}^x\cos\left({e}^x\right)$$ And using calculator(I don't know how to approximate it without it): $$f''\left(\ln \left(\frac{\pi}{1+\mathrm{e}}\right)\right)<0$$ So (C) is correct. For (A) and (B) :
I tried substituting $u=e^t$ $$f(x)=\int_{e^x}^{e^{x+1}}\dfrac{\sin u}{u} du$$ And integrating by parts(taking Ist as $\dfrac1u$ and 2nd as $\sin(u)$ gives: $$f(x)=\left[\dfrac{-\cos u}{u}\right]_{e^x}^{e^{x+1}} - \int_{e^x}^{e^{x+1}} \dfrac{\cos u}{u^2}du
$$ which I am unable to simplify. The given answer is A,B,C,D","['integration', 'calculus', 'functions']"
4683123,Does series $\sum_{n=1}^{\infty}(\frac{1+ \cos n}{2 + \cos n})^{2n}$ converges?,"My idea was to test the necessary condition for the convergence of a series ( $\lim_{n \to \infty} a_n = 0$ ). I've concluded since $\lim_{n \to \infty} \cos n$ doesn't exist, the necessary condition for the convergence can't be calculated, meaning that the series diverges . Is this correct, or am I missing something?","['limits', 'calculus', 'convergence-divergence', 'sequences-and-series']"
4683151,"What is the probability that the distance between any 2 consecutive points on the line is less than the given minimum distance, d?","Question: Given a line segment $L$ and $n$ random points on it. Assuming the points are uniformly distributed over $[0, L]$ . What is the probability that the distance between any 2 consecutive points on the line is less than the given minimum distance, d? Attempt: This question came from my curiosity, but my attempt is as follows: To calculate the probability that the distance between any two consecutive points on the line is less than the given minimum distance $d$ , we need to consider the total number of possible arrangements of the $n$ points on the line, as well as the number of arrangements that satisfy the condition. The total number of possible arrangements of the n points on the line is given by the number of ways to choose $n-1$ points out of $L$ , which is given by the binomial coefficient: $$C = {L-dn+1 \choose n-1}$$ Now, let's consider the number of arrangements that satisfy the condition. To do this, we can imagine dividing the line segment into small intervals of length $d$ . For the distance between any two consecutive points to be less than $d$ , each interval must contain at most one point. Therefore, the number of arrangements that satisfy the condition is given by the number of ways to choose $n$ or fewer intervals out of $(L/d)$ intervals, which is again given by the binomial coefficient: $$S = {L/d + 1 \choose n}$$ Finally, the probability that the distance between any two consecutive points on the line is less than d is given by the ratio of the number of arrangements that satisfy the condition to the total number of possible arrangements: $$P = \dfrac{S}{C}$$ I've tested the theoretical calculation with a Monte Carlo simulation. The results do not match. Hopefully, someone can provide an answer to the query. Thanks. EDIT: For reference, using Monte Carlo simulation with the parameters $n = 10, L = 20, d = 2$ , and $10^6$ trials the estimated probability is 0.002.","['conditional-probability', 'probability-theory', 'probability']"
4683159,"Given a subgroup of a free group, find the associated covering space.","Let $R_2$ the rose with $2$ petals, that is the wedge of $S^1$ with itself. We know its fundamental group is the free group with two elements, $\pi_1(R_2)=F_2=\langle a,b\rangle$ . Now given some subgroup $H=\langle a^3,ba,aba^2,a^2b^2a \rangle$ . I want to find the covering space $X_H$ of $X$ associated to $H$ by the Galois Correspondence of subgroups and covering spaces. I know the universal covering space $\widetilde{X}$ of the rose of $n$ -petals is the Cayley Graph of the free group on $n$ elements. Moreover, I know that to get $X_H$ we quotient the $\widetilde{X}$ by $[\gamma]~[\gamma']$ whenever $\gamma(1)=\gamma'(1)$ and $[\gamma\gamma'^{-1}]\in H$ , since we may see $\widetilde{X}$ as the space of paths to $R_2$ . Still, I was not able to use this fact to obtain a concrete graph $X_H$ . I wonder if there's a general procedure to obtain our space $X_H$ given a graph $H$ , or, at least, in this specific case.","['group-theory', 'free-groups', 'algebraic-topology', 'covering-spaces']"
4683175,Why can't union be recursively defined (Tao's Analysys I),"I'm currently working on Terence Tao's Analysis I, and in page 42, Tao defines pairwise union: Axiom 3.4. Guven any two sets $A,B,$ there exists a set $A\cup B,$ called the union $A\cup B$ of $A$ and $B,$ whose elements consists of all the elements which belong to $A$ or $B$ or both. However, in page 43, he gives an idea to form triplet, quadruplet, etc sets by doing $\{a,b,c\}:=\{a\}\cup\{b\}\cup\{c\}$ and so forth (he begins this Chapter by stating the pair sets axiom), yet he claims that we are not yet in a position to define sets consisting of $n$ objects for any given natural number $n;$ this would require iterating the above construction "" $n$ times"", but the concept of $n$ -fold iteration has not yet been rigorously defined. I'm not sure what he means by that...I looked for that concept and found that it means functions that take other functions as inputs OR have as a result another function...but I don't see the relation... Please advise, thanks a lot!","['elementary-set-theory', 'functions']"
4683213,How to calculate relative performance (non-finance) when lower is better?,"Example: Car 1 goes from 0-60 in 6 seconds. Car 2 goes from 0-60 in 8 seconds. Car 3 goes from 0-60 in 9 seconds. Now, Car 1 is obviously the fastest, so I want to make it my reference and express other cars' performance relative to it. If higher was better this would be a simple value/reference * 100 , however, if I express Car 1 as 100% this formula makes the other two as 133% and 150% respectively, which is mathematically correct, but makes no sense on a scale of 1-100%. I've wasted the last two hours looking for an answer and only come across two: (1 - value/reference) + 1 or 2 - value/reference . Problem with this is: if a car takes 12 seconds the result would be zero. Again no sense. (1/value)/(1/reference) or reference/value . I'm not sure whether it's correct, but it seems to achieve what I want.",['statistics']
4683233,Minimize $x+y$ given $(C - 1)xy + x=D$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Minimize $x+y$ given $(C - 1)xy + x=D$ where $x$ and $y$ are positive reals I am looking to derive an equation such that given a constant $C$ and an objective value $D$ , I am able to determine the minimum needed combined $x$ and $y$ to reach it. objective function with a horizontal plane at D","['multivariable-calculus', 'calculus', 'maxima-minima', 'optimization']"
4683264,"Uniform convergence of $f_n(x) = n^cx(1-x^2)^n$ on $[0,1]$","Assume $f_n(x) = n^cx(1-x^2)^n$ For which values of $c$ does ${f_n(x)}$ converge uniformly on $[0,1]$? This is only part of the problem, I have already proven that ${f_n}$ converges pointwise on $[0,1]$ for all $c$.","['uniform-convergence', 'real-analysis']"
4683277,Why does this substitution give an incomplete particular integral solution for this linear ODE?,"Trying to solve the following linear nonhomogeneous ODE $$ (D + 2)^2y = \cosh 2x $$ We can easily find the complementary function $$ y_c = c_1 e^{-2x} + c_2 {x} e^{-2x} $$ Using the inverse differential operator $$y_p = \frac{1}{(D+2)^2}\cosh 2x$$ I tried two methods to solve it but one of them gives an incomplete particular solution, I first used the fact that $\cosh 2x = ({e^{2x} + e^{-2x}})/{2}$ , and then applied the shift theorem $$y_p = \frac{1}{(D+2)^2} \frac{e^{2x} + e^{-2x}}{2}
= \frac{1}{2}\left(\frac{1}{16}e^{2x} + \frac{1}{(D+2)^2}e^{-2x}\right) = \frac{1}{2}\left(\frac{1}{16}e^{2x} + e^{-2x}\frac{1}{D^2}(1)\right)  = \frac{1}{2}\left(\frac{1}{16}e^{2x} + \frac{x^2}{2}e^{-2x}\right)$$ $$y_p = \frac{1}{32}e^{2x} + \frac{x^2}{4}e^{-2x}$$ which gives a complete particular solution, so the general solution is $$y = c_1 e^{-2x} + c_2 {x} e^{-2x} + \frac{1}{32}e^{2x} + \frac{x^2}{4}e^{-2x}$$ But when I tried to expand $(D+2)^2$ then substitute $2^2$ in place of $D^2$ , and lastly apply the shift theorem, I got $$y_p = \frac{1}{D^2+4D+4}\cosh 2x = \frac{1}{4}\left(\frac{1}{D+2}\cosh 2x\right) = \frac{1}{4}\left(\frac{1}{D+2} \frac{e^{2x} + e^{-2x}}{2}\right) = \frac{1}{32}e^{2x} + \frac{1}{8}\left(\frac{1}{D+2}e^{-2x}\right)$$ $$y_p = \frac{1}{32}e^{2x} + \frac{1}{8}xe^{-2x}$$ but the term $\dfrac{x^2}{4}e^{-2x}$ is missing in this particular solution while the other $\dfrac{1}{8}xe^{-2x}$ is already there in the complementary function. So my question is, what exactly did I do wrong?, and when can I be sure that the method I am using will actually give the right complete answer?","['differential-operators', 'linear-algebra', 'ordinary-differential-equations']"
4683326,What percent of lighted grids are walkable: a trick-or-treating problem,"I am a math teacher that likes to invent fun math problems to explore. Here is one I have been investigating for a little while and have made little progress on because the number of possible $n \times n$ ""grids"" here is always $2^{n^2}$ . I imagine a kid trick-or-treating over a grid of houses on Halloween whose parents tell him he can only keep going so long as one house nearby has their lights on. Each house is either a lit or unlit square on a grid, which can be thought of as $n \times n$ for simplicity's sake, but $n \times m$ explorations are welcome also. The kid can start at any point in the grid, he does not have to begin on the edge. My question(s) are as follows: Given an $n \times n$ grid, is there a closed form for the number of ""walkable"" grids? These are grids where the kid can get to every single house, meaning no house with its lights on is surrounded by houses with their lights off. As $n \to \infty$ , does the percentage of lighted grids that are walkable approach a non-zero value? (My gut feeling here is no, but it would be super cool if it did!) Here is some information I do know: For $2 \times 2$ grids, every one is walkable. For $3 \times 3$ grids, >50% are walkable, because if the middle house has its lights on, the grid is walkable no matter what. For $n \times n$ grids where $n \geq 4$ , a lower bound on the percentage of walkable grids is given by $\frac{100}{(n-2)^2}$ for similar reasons to $3 \times 3$ grids; if all the ""middle"" houses are lighted, the grid is automatically walkable. If this is similar to another problem, please let me know, and if you find out anything cool here, awesome! This is my first post here, so hopefully this will at least spark some curiosity and exploration. I cannot write code very well, but imagine someone could brute force this for small integers for a little bit. Edit: The kid can walk by the same house multiple times as long as it has its lights on. Only interested in visiting lit houses. ""Walkable"" if every lit house is within a king move of another lit house. Daniel Mathias had a worthwhile note simplifying the problem in the comments below, quoted, ""...you have a grid of cells that are either 0 or 1 and you are asking what percentage of these grids have all of the 1's connected.""","['puzzle', 'combinatorics', 'recreational-mathematics', 'percolation', 'probability']"
4683337,Why can the exterior derivative be interpreted as in this picture?,"This paper provides a geometric intuition of differential forms. On page $5$ it reads: ""Consider the case $xdy$ . The number of horizontal lines is roughly proportional to $y$ . In other words the number of lines that 'come to an end' in any area is roughly the same wherever we look. This is just like the example $dx ∧ dy$ above. You may have guessed what is going on here: $d(xdy) = dx∧dy$ . In other words exterior derivative is none other than finding the boundary of the picture."" Why can the exterior derivative of a form $\omega$ , defined as $$d\omega = d\left(\sum_{i_1,\ldots,i_k}\omega_{i_1,\ldots,i_k}dx^{i_1}\wedge\ldots\wedge dx^{i_k}\right)\\
\ \ = \sum_{i_1,\ldots,i_k}d\omega_{i_1,\ldots,i_k}\wedge dx^{i_1}\wedge\ldots\wedge dx^{i_k}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = \sum_{i_1,\ldots,i_k}\sum_{\alpha=1}^nD_{\alpha}(\omega_{i_1,\ldots,i_k})\ dx^{\alpha}\wedge dx^{i_1}\wedge\ldots\wedge dx^{i_k}\\
$$ be interpreted as the quantity of points at which the lines in the picture come to an end?","['exterior-derivative', 'differential-geometry', 'intuition', 'differential-forms', 'exterior-algebra']"
4683397,Any ideas on how to perform this integral?,"I can numerically verify that \begin{equation}
\int_{-\infty}^{\infty}\mathrm{d}p \frac{i \left(e^{c | p| }-1\right) e^{-| p|  \sin (\phi )+i p \cos (\phi )}}{p}
\end{equation} is equal to \begin{equation}
-2 i \tanh ^{-1}\left(1+\frac{2 i e^{i \phi }}{c}\right)+2 i \tanh ^{-1}\left(1-\frac{2 i e^{-i \phi }}{c}\right)-2 \pi
\end{equation} But I do not know how to analytically show it. For context; I happen to obtain this $\int_{-\infty}^{\infty}\mathrm{d}p -\frac{i \left(e^{c | p| }-1\right) e^{-| p|  \sin (\phi )+i p \cos (\phi )}}{p}-2 i \tanh ^{-1}\left(1+\frac{2 i e^{i \phi }}{c}\right)+2 i \tanh ^{-1}\left(1-\frac{2 i e^{-i \phi }}{c}\right)-2 \pi$ as an energy of a particle in a model I am working with for parametric region $0<c$ and $3c<2\sin(\phi)$ . I numurically evaluated the integral and saw it is zero. It happens to be zero not only in this parametric region for for all values of $c>0$ , $0<\phi<\pi/2$ , and $c<\sin(\phi)$ . So, I would like to see how to perform the integral. Moreover, I have one more integral of similar structure \begin{equation}
\int_{-\infty}^\infty \frac{i\left(e^{c|p|}+e^{2|p| \sin (\phi)}\right) e^{-|p|(c+\sin (\phi))+i p \cos (\phi)}}{p\left(e^{c|p|}+1\right)} \mathrm{d} p
\end{equation} which I have not been able to solve.","['integration', 'improper-integrals', 'analysis', 'real-analysis', 'indefinite-integrals']"
4683451,"For any $I = [p_{n} + 2, p_{n+1}^2 - 2]$ there exists an affine function in the first quadrant that lower bounds the twin prime average counter on $I$","Motivation Lemma. $h_n(x) = \sum\limits_{d \mid p_n\#}(-1)^{\omega(d)} \sum\limits_{r^2 = 1 \pmod d} \left\lfloor \frac{x - r}{d}\right\rfloor$ counts the number of twin prime averages in the interval $[p_n +2, x]$ for all $p_n + 2 \leq x \leq p_{n+1}^2 - 2$ . Proof. I can prove the above using elementary definitions and inequalities. Conjecture. An affine function lower-bounding $h_n(x)$ is: $$
f_n(x) = \frac{-n}{2} + x\sum_{d \mid p_n\#}\frac{(-1)^{\omega(d)}{\begin{cases} 2^{\omega(d) -1},2\mid d \\ 2^{\omega(d)}, \text{else} \end{cases}}}{d} 
$$ In other words the slope $m$ of the trend line $f_n$ is precisely the sum of all $(-1)^{\omega(d)} \frac{1}{d}$ such that $r \in$ the group $G_d = \{ r \in \{0, \dots, d-1\} : r^2 = 1\pmod d\}$ which is the same thing as above. Evidence This code which generates the above screenshot showing the trend line above next to what it lower bounds, but zoomed in on the window of $X = p_{n} + 2... 2\cdot( p_{n+1}^2 - 2)$ . from sympy import *

def f(n, x, P):
   S = 0
   R = -(n+1)/2
   Q = 0 
      
   for d in divisors(P):
      T = 0
      U = 0
      V = 0
      for r in range(0, d):
         if (r**2 - 1) % d == 0:
            U += floor((x- r)/d)
            T += (x - r)/d
            V += floor((-r)/d)
      S += (-1)**primeomega(d) * U
      R += (-1)**primeomega(d) * T
      Q += (-1)**primeomega(d) * V
      
   return  R, S, Q

X = Symbol('X')

N =5

M = primorial(N)
S,R, Q = f(N, X, M)

print(S)
print(R)
print(f'Q={Q}')

a = prime(N+1) + 2
b = prime(N + 2)**2 - 2



delta = 1.0
n = int((b-a) / delta)

Xvals = []
Yvals = []
Xvals1 = []
Yvals1 = []

max = 0

for x in range(a, 2*(b+1)):
   Xvals1.append(x)
   z,y,_= f(N, x, M)
   
   if abs(z-y) > max:
      max = abs(z-y)
      
   Yvals1.append(z)
   Yvals.append(y)
   
print(f'Max diff = {max}')
   
import matplotlib.pyplot as plt


plt.plot(Xvals1, Yvals1)
plt.plot(Xvals1, Yvals)
plt.show() It's already been proven that the slope is indeed correct, here by Bryan Moehring , even for more general sums of such step functions.  In other words, any such trend line or bounding line must have a slope of that form, so that is solved for.  I merely seek a constant for $b$ .  I think $b = -n/2$ seems like good choice.  Judging by zooming in on the above $n=5$ screenshot, it's a really tight bound. Question How can I prove that $b = -n/2$ is sufficient so that $f_n(x) = mx + b \leq h_n(x)$ for all $x \in \Bbb{R}$ ?  (For sufficiently large $n$ ) Further Motivation $$
\sum_{d \mid p_n\#}\frac{(-1)^{\omega(d)}{\begin{cases} 2^{\omega(d) -1},2\mid d \\ 2^{\omega(d)}, \text{else} \end{cases}}}{d} = \\
(1-\frac{1}{2})\sum_{d \mid \frac{p_n\#}{2}}\frac{(-2)^{\omega(d)}}{d}
$$ So this seems like something to which we can apply Mertens' Theorems to. If the estimate for $b$ is correct, that would mean we can expect at least: $$
f_n(p_{n+1}^2-2) = -\frac{n}{2} + (p_{n+1}^2 -2) M
$$ twin prime averages to occur in the interval $I = [p_{n} + 2, p_{n+1}^2 - 2]$ .  Where $M = \frac{1}{2}\sum\limits_{d \mid \frac{p_n\#}{2}}\frac{ (-2)^{\omega(d)}}{d}$ .  And $M$ is handled nicely by Mertens application .","['ceiling-and-floor-functions', 'twin-primes', 'number-theory', 'real-analysis', 'prime-numbers']"
4683461,Is $\sec^2 x - \tan^2 x$ a constant function?,"If $f(x)=\sec^2 x$ , $g(x) = \tan^2 x$ then $$p(x)=f(x)-g(x)=1,$$ which is a constant function but its graph is discontinous at $$(2n+1)\frac{\pi}{2}$$ even though $1$ is not discontinuous. So, if given a function like $p(x)=1$ can we rewrite it as $-x+x+1$ , or even $\sec^2(x) - \tan^2(x)$ ? Or does $\sec^2(x) - \tan^2(x)$ have a domain which does not contain odd multiple of $\frac{\pi}{2}$ ??","['trigonometry', 'functions']"
4683472,There is already a theorem that demonstrates this inequality? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I would like to create my first paper on math, this time in geometry, and the intuitive definition of the theorem would be that: A 2D disk with radius $r$ and a continuous function (for example, a mountain) that forms a disk with radius $r$ seen from the top, fixed at the exact centre, will always have a greater surface than the initial disk it is placed above. Here there are two images: a red disk and a red mountain. The point would be that the mountain always has greater surface/area than the disk for a fixed area (marked as red): The Disk: The Mountain: I am thinking of demonstrating it with integrals and polar coordinates. I'll also highen it to $n$ dimensions. Thanks!","['geometry', 'geometric-topology', 'general-topology', 'differential-topology', 'differential-geometry']"
4683521,Finding densities to estimate parameters using the Maximum likelihood technique,"Consider the  following autoregressive process with normal errors: \begin{equation}\label{7YlUV4i8nuO}\tag{I}
    y_t = \phi y_{t-1}+ u_t, \quad u_t \overset{iid}{\sim} N(0,\sigma^2)
\end{equation} We can show: \begin{equation}
    y_1 \sim N\left(0,\frac{\sigma^2}{1-\phi^2}\right), \quad y_t|y_{t-1} \sim N\left(\phi y_{t-1}, \sigma^2 \right)
\end{equation} Denote the density of $y_{1}$ as $f_{y_1}(y_1;\phi,\sigma^2)$ and the conditional density of $y_t|y_{t-1}$ as $f_{y_t|y_{t-1}}(y_t|y_{t-1};\phi,\sigma^2)$ . Given $y_T,...,y_1$ a sample, the likelihood function, and the log-likelihood function are: $$ L(\phi,\sigma^2) = f_{y_1}(y_1;\phi,\sigma^2) \prod_{t=2}^T f_{y_t|y_{t-1}}(y_t|y_{t-1};\phi,\sigma^2)$$ $$\mathcal L(\phi,\sigma^2) = \log f_{y_1}(y_1;\phi,\sigma^2) + \sum_{t=2}^T \log f_{y_t|y_{t-1}}(y_t|y_{t-1};\phi,\sigma^2)$$ See here to obtain the density $f_{y_1}(y_1;\phi,\sigma^2)$ . For more details of all derivations, the reader can refer to this note . Now, I want to find the likelihood functions for a class of stochastic process with compound Poisson distribution, i.e., a generalization of this .  Given $N \sim \hbox{Poisson}(\lambda)$ with $\lambda\geq 0$ and $(y_t)_{t\in \mathbb Z}$ the AR(1) process given in (\ref{7YlUV4i8nuO}), define $x_t = \sum_{j=0}^{N} y_{t;j}$ (note that $N$ does not depend on $t$ ). It is straightforward to show that: $$x_t = \phi x_{t-1} + v_t, \quad v_t:= \sum_{j=0}^{N}  u_{t;j}$$ where $(v_t)_t$ turns out to be white noise. First, suppose the deterministic case $N=n> 0$ (If $N=0$ , by convention $\sum_{j=0}^0 y_{tj} =0$ ) . Denoting $ \sigma_n^2 := n\sigma^2$ , we have: $$x_t = \phi x_{t-1} + v_t^n, \quad v_t^n:= \sum_{j=1}^{n}  u_{t;j}\sim N(0, \sigma_n^2)$$ Setting $\theta= (\phi,\sigma^2)$ , we have that $x_1 \sim N(0,\sigma_n^2/(1-\phi^2))$ , i.e, its denisity is: \begin{equation}\label{2D85niRxr6}\tag{II}
f^n_{x_1}(x_1;\theta)= \frac{1}{\sqrt{ 2 \pi \sigma_n^2/(1-\phi^2) }} e^{ -\tfrac{1}{2} x_1^2/\sigma_n^2/(1-\phi^2) }
\end{equation} Moreover, we have that $x_t|x_{t-1} \sim N( \phi x_{t-1}, \sigma_n^2 )$ , i.e., its density is: \begin{equation}\label{2D85niRxr62}\tag{III}
f^n_{x_t|x_{t-1}}(x_t|x_{t-1};\theta)= \frac{1}{\sqrt{ 2 \pi  \sigma_n^2 }} e^{ -\tfrac{1}{2} (x_t - \phi x_{t-1} )^2/ \sigma_n^2 }
\end{equation} In order to obtain the likelihood function for the random $N \sim \hbox{Poisson}(\lambda)$ case, first note that $x_1 = \sum_{j=0}^N y_{1;j}$ and since $P[N=0]>0$ , we have $P[x_1=0]> 0$ . That is, we have a mixed distribution (a density of a mixed random variable is neither discrete nor continuous) and this can complicate things a bit (Read this post on this point). So, as a first exercise, let's assume that $N>0$ . Setting $\mu  = (\phi, \sigma^2, \lambda)= (\theta, \lambda)$ , we have that: \begin{equation}
\begin{aligned}
f_{x_1}(x_1|N>0; \mu)&= \sum_{n=1}^\infty f_{x_1|N=n}(x_1|N=n;\mu)P(N=n)\\
& = \sum_{n=1}^\infty f^n_{x_1}(x_1;\theta) P(N=n)\\
\end{aligned}
\end{equation} Analogously: \begin{equation}
\begin{aligned}
f_{x_t|x_{t-1}}(x_t|x_{t-1}, N>0; \mu)&= \sum_{n=1}^\infty f_{x_t|x_{t-1},N=n}(x_{t}|x_{t-1},N=n;\mu)P(N=n)\\
& = \sum_{n=1}^\infty f^n_{x_t|x_{t-1}}(x_t|x_{t-1};\theta) P(N=n)    
\end{aligned}
\end{equation} Thus, given $x_T,...,x_1$ , the likelihood function is: \begin{equation}\label{HK518fhA8lu}\tag{II}
\begin{aligned}
L(\mu| N>0)&=f_{x_1}(x_1| N>0; \mu) \prod_{t=2}^T  f_{x_t|x_{t-1}}(x_t|x_{t-1},  N>0; \mu)\\
&= \sum_{n=1}^\infty f^n_{x_1}(x_1;\theta) P(N=n) \prod_{t=2}^T \sum_{n=1}^\infty f^n_{x_1}(x_1;\theta) P(N=n) \\
&= e^{-T\lambda} \left[ \sum_{n=1}^\infty f^n_{x_1}(x_1;\theta) \frac{\lambda^n}{n!}  \right]  \left[ \prod_{t=2}^T\sum_{n=1}^\infty f^n_{x_t|x_{t-1}}(x_t|x_{t-1};\theta) \frac{\lambda^n}{n!}\right]\\
\end{aligned}
\end{equation} The log-likelihood is \begin{equation}\label{HK518fhA8lu2}\tag{IV}
\mathcal L(\mu| N>0) = -T\lambda + \log \left( \sum_{n=1}^\infty f^n_{x_1}(x_1;\theta) \frac{\lambda^n}{n!}  \right) +  \sum_{t=2}^T \log\left(\sum_{n=1}^\infty f^n_{x_t|x_{t-1}}(x_t|x_{t-1};\theta) \frac{\lambda^n}{n!}\right)
\end{equation} where $f^n_{x_1}(x_1;\theta)$ and $f^n_{x_t|x_{t-1}}(x_t|x_{t-1};\theta)$ are respectively given by (\ref{2D85niRxr6}) and (\ref{2D85niRxr62}). The general case is much more dramatic. \begin{equation}
\begin{aligned}
f_{x_1}(x_1; \mu)&= \mathbf{1}_{[x_1 = 0]} (x_1) P(N = 0) + \mathbf{1}_{[x_1 \neq 0]}(x_1)\sum_{n=1}^\infty f_{x_1|N=n}(x_1|N=n;\mu)P(N=n)\\
& = \mathbf{1}_{[x_1 = 0]}(x_1) e^{-\lambda} + \mathbf{1}_{[x_1 \neq 0]}(x_1)\sum_{n=1}^\infty f^n_{x_1}(x_1;\theta) e^{-\lambda}\frac{\lambda^n}{n!}\\
&= e^{-\lambda} \left[\mathbf{1}_{[x_1 = 0]}(x_1)  + \mathbf{1}_{[x_1 \neq 0]}(x_1)\sum_{n=1}^\infty f^n_{x_1}(x_1;\theta) \frac{\lambda^n}{n!}\right]
%\frac{1}{\sqrt{ 2 \pi \Tilde \sigma_n^2 }} e^{ -\tfrac{1}{2} x_1^2/\Tilde \sigma_n^2 }    
\end{aligned}
\end{equation} Analogously, defining $D= [x_t = \phi x_{t-1}]$ : \begin{equation}
\begin{aligned}
f_{x_t|x_{t-1}}(x_t|x_{t-1}; \mu)&= \mathbf{1}_{D}(x_{t}) P(N = 0) + \mathbf{1}_{D^c}(x_{t})\sum_{n=1}^\infty f_{x_t|x_{t-1},N=n}(x_{t}|x_{t-1},N=n;\mu)P(N=n)\\
& = \mathbf{1}_{D}(x_{t}) e^{-\lambda} + \mathbf{1}_{D^c}(x_{t})\sum_{n=1}^\infty f^n_{x_t|x_{t-1}}(x_t|x_{t-1};\theta) e^{-\lambda}\frac{\lambda^n}{n!}\\
&= e^{-\lambda} \left[\mathbf{1}_{D}(x_{t})  + \mathbf{1}_{D^c}(x_{t})\sum_{n=1}^\infty f^n_{x_t|x_{t-1}}(x_t|x_{t-1};\theta) \frac{\lambda^n}{n!}\right]
%\frac{1}{\sqrt{ 2 \pi \Tilde \sigma_n^2 }} e^{ -\tfrac{1}{2} x_1^2/\Tilde \sigma_n^2 }    
\end{aligned}
\end{equation} Thus, given $x_T,...,x_1$ , the likelihood function is: \begin{equation}\label{26Y7lHzJ3}\tag{V}
L(\mu)=   f_{x_1}(x_1; \mu) \prod_{t=2}^T  f_{x_t|x_{t-1}}(x_t|x_{t-1}; \mu)
\end{equation} I'm going to avoid making the respective substitutions so as not to load too many formulas and not write the log-likelihood function. So, I guess my questions boil down to: Is there any way to try to find a more user-friendly form of the likelihood in (\ref{26Y7lHzJ3}) or the log-likelihood function for the general case? More specifically, how to find the $\hat \mu= (\hat \lambda, \hat \phi, \hat \sigma^2)$ that maximizes such functions? If not, can we try to maximize the case $N>0$ given by $\mathcal L(\mu| N>0)$ above in (\ref{HK518fhA8lu2})? If we can't maximize directly, I thought about doing a truncation on $\mathcal L(\mu| N>0)$ , i.e., finding $\hat \mu_M$ that maximizes \begin{equation}
\begin{aligned}
\mathcal L_M(\mu| N>0) = -T\lambda + \log \left( \sum_{n=1}^M f^n_{x_1}(x_1;\theta) \frac{\lambda^n}{n!}  \right) +  \sum_{t=2}^T \log\left(\sum_{n=1}^M f^n_{x_t|x_{t-1}}(x_t|x_{t-1};\theta) \frac{\lambda^n}{n!}\right)\\
\end{aligned}
\end{equation} Perhaps this maximization is feasible and with luck, I might have: \begin{equation}
\hat{\mu}_M \longrightarrow \hat \mu = \hbox{argmax}_{\mu} \mathcal L(\mu| N>0), \quad (M \to \infty)
\end{equation} If this is not a good strategy, what to do?","['statistical-inference', 'probability-distributions', 'maximum-likelihood', 'probability-theory', 'density-function']"
4683541,Can the integral be found without Feynman’s trick?,"When I came across the integral $$J=\int_0^{\infty} \frac{\operatorname{artanh}\left(\frac{1}{\sqrt{1+x^2}}\right)}{\sqrt{1+x^2}} d x=\frac{\pi^2}{4}  $$ whose answer is surprisingly decent, I, as usual, put $x=\tan \theta$ and transform the integral into $$
\begin{aligned}
J & =\int_0^{\frac{\pi}{2}} \frac{\operatorname{artanh}\left(\frac{1}{\sec \theta}\right)}{\sec \theta} \sec ^2 \theta d \theta \\
& =\int_0^{\frac{\pi}{2}} \sec \theta \operatorname{artanh}\left(\frac{1}{\sec \theta}\right) d \theta
\end{aligned}
$$ Feynman’s trick reminds me to deal with its parametrized integral $$
J(a)= \int_0^{\infty} \frac{\operatorname{artanh}\left(\frac{a}{\sqrt{1+x^2}}\right)}{\sqrt{1+x^2}} d x =\int_0^{\frac{\pi}{2}} \sec \theta \operatorname{artanh} \left(\frac{a}{\sec \theta}\right) d \theta
$$ with $|a|<1$ and $J(0)=0$ . Consequently, differentiation under integral make our life easier as $$
\begin{aligned}
 J^{\prime}(a)&=\int_0^{\frac{\pi}{2}} \frac{1}{1-\frac{a^2}{\sec ^2 \theta}} d \theta \\& =\int_0^{\frac{\pi}{2}} \frac{\sec ^2 \theta}{\sec ^2 \theta-a^2} d \theta \\
& =\int_0^{\frac{\pi}{2}} \frac{d(\tan \theta)}{\left(1-a^2\right)+\tan ^2 \theta} \\
& =\frac{1}{\sqrt{1-a^2}}\left[\operatorname{artan}\left(\frac{\tan \theta}{\sqrt{1-a^2}}\right)\right]_0^{\frac{\pi}{2}} \\
& =\frac{\pi}{2 \sqrt{1-a^2}} \\
&
\end{aligned}
$$ Integrating back with $J(0)=0$ yields $$
\boxed{J(a)= \int_0^{\infty} \frac{\operatorname{artanh}\left(\frac{a}{\sqrt{1+x^2}}\right)}{\sqrt{1+x^2}} d x=\frac{\pi}{2} \int \frac{1}{\sqrt{1-a^2}} d a=\frac{\pi}{2} \operatorname{arcsin}a \,} \tag*{(1)} 
$$ In particular, let $a$ approach to $1$ , we get $$
J= \lim _{a \rightarrow 1} J(a)= \lim _{a \rightarrow 1}\frac{\pi}{2} \operatorname{arcsin} 1=\frac{\pi^2}{4}
$$ Inspired by $J(a)$ , I believe that whenever $\left|\frac ab \right| <1$ , we can further evaluate $$
\int_0^{\infty} \frac{\operatorname{artanh}\left(\frac{a}{\sqrt{b^2+x^2}}\right)}{\sqrt{b^2+x^2}} d x
$$ by simply letting $x\mapsto bx$ which transforms $$
\boxed{\int_0^{\infty} \frac{\operatorname{artanh}\left(\frac{a}{\sqrt{b^2+x^2}}\right)}{\sqrt{b^2+x^2}} d x=\int_0^{\infty} \frac{\operatorname{artanh}\left(\frac{\frac{a}{b}}{\sqrt{1+x^2}}\right)}{\sqrt{1+x^2}} d x=J\left(\frac{a}{b}\right )=  \frac{\pi}{2} \operatorname{arcsin}\left(\frac{a}{b}\right) \,}
$$ Latest Edit :After submitting the question, I found that there is a simpler version with Feynman’s trick to share with you. Considering $b$ is a constant and let $$\displaystyle J(a)=\int_0^{\infty} \frac{\operatorname{artanh}\left(\frac{a}{\sqrt{b^2+x^2}}\right)}{\sqrt{b^2+x^2}} d x\tag*{} $$ where $J(0)=0$ .
Differentiating $J(a)$ w.r.t. $a$ yields $$\displaystyle \begin{aligned}J^{\prime}(a) & =\int_0^{\infty} \frac{1}{\left(1-\frac{a^2}{b^2+x^2}\right)\left(b^2+x^2\right)} d x \\& =\int_0^{\infty} \frac{1}{b^2-a^2+x^2} d x \\& =\frac{1}{\sqrt{b^2-a^2}}\left[\tan ^{-1}\left(\frac{x}{\sqrt{b^2-a^2}}\right)\right]_0^{\infty} \\& =\frac{\pi}{2 \sqrt{b^2-a^2}}\end{aligned}\tag*{} $$ Integrating back w.r.t. $a$ with $J(0)=0$ yields $$\displaystyle \boxed{ \begin{aligned}J(a) & =\frac{\pi}{2} \int \frac{d a}{\sqrt{b^2-a^2}}  =\frac{\pi}{2} \arcsin \left(\frac{a}{b}\right)\end{aligned}}\tag*{} $$ Can it be done without Feynman’s trick? Your comments and alternative methods are highly appreciated.","['integration', 'definite-integrals', 'hyperbolic-functions', 'calculus', 'trigonometric-integrals']"
4683567,"If a function's domain is an empty set, how can its derivative still exist graphically?","I read in my textbook that if we add two functions (with domain $D_1$ and $D_2$ ), the domain of resulting function is $\displaystyle{D_1 \cap D_2}$ . Now, if we add $f(x)=\ln(1-x)$ and $ g(x)=\dfrac{1}{\sqrt{x-1}}$ . The domain of $p(x)=f(x)+g(x)$ is an empty set, so there is ""no graph"" for this function $p(x)$ . How to interpret this? However, if I take the derivative of this function, I obtain a graph. How is this possible? The function itself did not exist, but we have a slope(derivative) for that function which is represented graphically?","['functions', 'derivatives', 'graphing-functions']"
4683594,Find the area of hexagon,"Triangle $ABC$ has a right angle at $C$ , and $AC=BC= 1$ . Let $I$ be the incenter of triangle $ABC$ . Let $D$ , $E$ , and $F$ be the midpoints of $AI$ , $BI$ , and $CI$ , respectively. Furthermore, let $J$ be the intersection of $AE$ and $BD$ , $K$ be the intersection of $BF$ and $CE$ , and $L$ be the intersection of $CD$ and $AF$ . Find the area of hexagon $DJEKFL$ . Write your answer in the form $a/b$ , where $a$ and $b$ are positive integers and $\gcd(a, b) = 1$ .​ Answer of this problem is $1/6$ . But my answer is 3/8. Here's My approach Let $M$ be the midpoint of $AB$ . Since triangle $ABC$ is isosceles, $M$ is also the midpoint of $CI$ . Hence, $DM$ is a median of triangle $ADI$ , and $EF$ is a median of triangle $BDI$ . Therefore, by the triangle median property, $DM = 1/2 AD$ and $EF = 1/2 BD$ . Since $I$ is the incenter of triangle $ABC$ , we have angle $AIC = \angle BIC = 90 + \angle C/2$ . Thus, $\angle AID = 180 - \angle AIC = 90 - \angle C/2$ . Similarly, $\angle BID = 90 - \angle C/2$ . Therefore, triangles $AID$ and $BID$ are similar, and we have $AD/BD = AI/BI = \sin(B/2)/\sin(A/2) = \cos(C/2)/\sin(A/2)$ .
Since triangle $ABC$ is isosceles, we have $\angle BAC = 90 - \angle C/2$ . Thus, $\sin(A/2) = \cos(BAC/2) = \cos((90 - \angle C/2)/2) = \sin(C/4)/\sqrt2$ . Therefore, $AD/BD = \cos(C/2)/(\sin(C/4) \sqrt2)$ . It follows that $DM/EF = (1/2 AD)/(1/2 BD) = \cos(C/2)/\sin(C/4) \sqrt2$ . By the angle bisector theorem, we have $AE/BE = AC/BC = 1$ . Thus, $J$ is the midpoint of $AE$ , and $K$ is the midpoint of $BF$ . Therefore, the line segments $DJ$ and $EK$ divide triangle $DEF$ into three triangles, each of which is similar to triangle $BDI$ . The ratio of the area of triangle $DJK$ to the area of triangle $DEF$ is $(DJ/EJ)^2 = (EK/BK)^2 = (BD/2EF)^2 = (2 \cos(C/2)/\sin(C/4))^2 = 4 \cos^2(C/2)/\sin^2(C/4)$ . The ratio of the area of triangle $DJL$ to the area of triangle $DEF$ is $(DJ/DL)^2 = (1/2 DM/DF)^2 = (\cos(C/2)/\sin(C/4) \sqrt2/EF)^2 = 2 \cos^2(C/2)/\sin^2(C/4)$ . The ratio of the area of triangle $EKL$ to the area of triangle $DEF$ is $(EK/EL)^2 = (1/2 EF/EM)^2 = (1/2 EF/CI)^2 = (1/2 EF/(2EF/\cos(C/2)))^2 = 4 \cos^2(C/2)$ . Therefore, the area of hexagon $DJEKFL$ is the difference between the area of triangle $DEF$ and the sum of the areas of triangles $DJK$ , $DJL$ , and $EKL$ . Hence, $Area(DJEKFL) = (1/2) EF^2 (1 - 4 \cos^2(C/2)/\sin^2(C/4) - 2 \cos^2(C/2)/\sin^2(C/4) - 4 \cos^2(C/2))$ . Substituting $\cos(C/2) = \sqrt{1 - \sin^2(C/2)}$ and $\sin(C/2) = 1/\sqrt{1 + \cot^2(C/2)}$ , we obtain $Area(DJEKFL) = (1/4) (1 - \sin^2(C/4)) (1 + \cos(C/2) - \cos^2(C/2) - 2 \cos^4(C/2))$ . Simplifying, we obtain $Area(DJEKFL) = (1/4) (1 - (1 - \cos(C/2))/(2\sin(C/4))) (2 \cos^4(C/2) - \cos^3(C/2) - \cos^2(C/2) - \cos(C/2) + 1)$ . Simplifying, we obtain $Area(DJEKFL) = (1/16) (2\sin(C/4) + \cos(C/2) - 1) (2 \cos^4(C/2) - \cos^3(C/2) - \cos^2(C/2) - \cos(C/2) + 1)$ . Now, we can use the identity $\cos(2x) = 2\cos^2(x) - 1$ to express $\cos^4(C/2)$ in terms of $\cos(2C)$ : $\cos^4(C/2) = (1/4) (2\cos^2(C/2) - 1)^2 = (1/4) (2\cos^2(C) - 3)^2 = (1/4) (\cos^2(C) - 4\cos^2(C) + 4) = (3/4) - (1/4) \cos^2(C)$ . Similarly, we can use the identity $\cos(3x) = 4\cos^3(x) - 3\cos(x)$ to express $\cos^3(C/2)$ and $\cos^2(C/2)$ in terms of $\cos(C)$ : $\cos^3(C/2) = (1/2) \cos(C/2) (1 + \cos(C)) = (1/2) \sqrt{1 - \sin^2(C/2)} (1 + \cos(C))$ , $\cos^2(C/2) = 1 - \sin^2(C/2) = \cos^2(C/2) - \sin^2(C/2) = \cos(C) - \sin^2(C/2)$ .
Substituting these expressions into the formula for the area of hexagon $DJEKFL$ and simplifying, we obtain $Area(DJEKFL) = (3/16) (2\sin(C/4) + \cos(C/2) - 1) (1 - \cos(C) - \cos^2(C/2) - \cos(C/2) + 1)$ . Simplifying further, we obtain $Area(DJEKFL) = (3/16) (2\sin(C/4) + \cos(C/2) - 1) (\sin^2(C/2) - \cos(C/2) - \cos(C) + 1)$ . Finally, we can use the half-angle identity again to express $\sin^2(C/2)$ in terms of $\cos(C/2)$ : $\sin^2(C/2) = (1 - \cos(C))/2$ . Substituting this into the formula for the area of hexagon $DJEKFL$ and simplifying, we obtain $Area(DJEKFL) = (3/32) (\cos(C/2) - 2(8\cos^4(C/4) - 8\cos^2(C/4) + 1) + 2(2\cos^2(C/4) - 1) + 1)$ . Simplifying further, we obtain $Area(DJEKFL) = (3/8) \cos^4(C/4) - (9/8) \cos^2(C/4) + (1/4)$ . Therefore, the area of hexagon $DJEKFL$ is $(3 \cos^4(C/4) - 9 \cos^2(C/4) + 2)/8$ . To write this in the form $a/b$ , we can multiply the numerator and denominator by $8$ to obtain $Area(DJEKFL) = (3 \cos^4(C/4) - 9 \cos^2(C/4) + 2)/8 = (3 \cos^4(C/4) - 9 \cos^2(C/4) + 2)/(2^3) = 3/8 * (\cos^4(C/4) - 3 \cos^2(C/4) + 2/3)$ . Since $\gcd(3, 8) = 1$ , the answer is $3/8 * (\cos^4(C/4) - 3 \cos^2(C/4) + 2/3)$ .",['geometry']
4683596,Generating function and finite difference.,"The above example is from Introduction to Combinatorial Analysis by John Riordan. Here $\Delta$ is the difference operator. But I am unable to understand why $\Delta 0^r = 1^r, \Delta^2 0^r = 2^r-2$ and $\Delta^3 0^r = 3^r - 3 \times 2^r +3$ . What I was trying is $\Delta 0^r = (0+1)^r - 0^r = 1^r$ , but then $\Delta^2 0^r = \Delta 1^r = (1+1)^r - 1^r = 2^r - 1$ , which is not matching with the book.","['self-learning', 'analysis', 'combinatorics', 'discrete-mathematics', 'generating-functions']"
4683620,Does the inequality $c_a \le xy^{y^a/x^a} + yx^{x^a/y^a} - x^a - y^a \le C_a$ hold?,"Update : Posted in MO since it is unanswered in MSE Let $0 \le x,y \le 1$ and $a$ be a real. Consider the function $$
f(x,y,a) = xy^{y^a/x^a} + yx^{x^a/y^a} -  x^a - y^a \tag 1
$$ For a fixed $a$ , the graph of the maximum and the minimum value of $f(x,y,a)$ shown below. The graph shows that for $a \ge 1$ , there is a constant $C_a$ depending only in $a$ such that $xy^{y^a/x^a} + yx^{x^a/y^a} -  y^a - x^a \le C_a$ and similarly for $a \le 2$ , there is a constant $c_a$ such that $c_a \le xy^{y^a/x^a} + yx^{x^a/y^a} -  y^a - x^a$ . Also, $0\ <a < 2$ is the only interval in which $f(x,y,a)$ has non-zero maxima and minima for every $a$ . Some experimentally observed examples of inequalities belonging to this family are $$
-\frac{1}{2} \le xy^{y/x} + yx^{x/y} - x - y < 0 \tag 2
$$ $$
0 \le xy^{y^2/x^2} + yx^{x^2/y^2} -  x^2 - y^2 \le \frac{1}{4} \tag 3
$$ $$
0 \le xy^{y^4/x^4} + yx^{x^4/y^4} -  x^4 - y^4 \le \frac{1}{2} \tag 4
$$ Question 1 : Can we prove that for every real $a$ , there exists $c_a$ and $C_a$ such that $$
c_a \le xy^{y^a/x^a} + yx^{x^a/y^a} -  x^a - y^a \le C_a
$$ Question 2 : Can we express $c_a$ and $C_a$ in terms of $a$ ?","['analysis', 'real-analysis', 'calculus', 'inequality', 'algebra-precalculus']"
4683640,What does a period mean in logic or set theory?,"I studied set theory in high school, but I'm seeing a period in my university course, that I can't recognize to mean something. Here is an example from my professor's slides: The notation $\{ (x, y) \mid Φ \}$ abbreviates $\{ p \mid ∃x. ∃y. p = (x, y) ∧ Φ \}.$","['elementary-set-theory', 'notation', 'predicate-logic', 'logic']"
4683644,Is the relation $e^{At} = Pe^{Jt} P^{-1}$ valid for a Jordan form?,"Imagine you have a system of equations of the form: $$
\mathbf {\dot{x}} = A \mathbf{x}
$$ And $A$ cannot be diagonalized, so you find it's Jordan form: $$
A = P J P^{-1} \\ \; \\
J  = \begin{pmatrix} 
\lambda_1 & 1 & 0 & \dots  & 0 \\
0 & \lambda_2 & 1 & \dots & 0 \\
0 & 0 & \lambda_3 & \dots & 0 \\
\vdots &\vdots & \vdots &  \ddots & \vdots \\
0 & 0 & 0&  \dots & \lambda_n \\
\end{pmatrix}
$$ The solution of the differential equation is: $$
\mathbf{x}(t) = e^{At}\mathbf{x_0}
$$ My question is: If the matrix $A$ was diagonalizable, then: $$
e^{At} = P \begin{pmatrix} 
e^{\lambda_1t} & 0 & 0 & \dots  & 0 \\
0 & e^{\lambda_2 t} & 0 & \dots & 0 \\
0 & 0 & e^{\lambda_3 t} & \dots & 0 \\
\vdots &\vdots & \vdots &  \ddots & \vdots \\
0 & 0 & 0&  \dots & e^{\lambda_n t} \\
\end{pmatrix} P^{-1}
$$ because: $$
e^{At} = \sum_{n = 0}^{\infty} \frac{(At)^n}{n!} = \sum_{n = 0}^{\infty} \frac{(PDP^{-1})^n}{n!}t^n = P \left ( \sum_{n = 0}^{\infty} \frac{D^n}{n!}t^n \right )P^{-1} = P e^{Dt} P^{-1}
$$ but, is this last relation: $$
\sum_{n = 0}^{\infty} \frac{(PJP^{-1})^n}{n!}t^n = P \left ( \sum_{n = 0}^{\infty} \frac{J^n}{n!}t^n \right )P^{-1} = P e^{Jt} P^{-1}
$$ still apply for a Jordan form? I know that when you sum all the terms involved in $e^{Jt}$ , polynomials of $t$ will appear apart from the exponential related to the eigenvalue: $e^{\lambda_k t}$ . But, is this still valid?","['linear-algebra', 'ordinary-differential-equations', 'dynamical-systems']"
4683645,Looking for an optimisation algorithm,"I have an optimisation problem and I'm looking for the best algorithm to solve it. I have a sparse matrix (only 0 and 1, R and C stay for rows and columns) such as: | C1 | C2 | C3 | C4 | C5 | C6 |
----------------------------------
R1 |  1 |  0 |  0 |  1 |  0 |  0 |
R2 |  0 |  1 |  0 |  0 |  0 |  0 |
R3 |  0 |  1 |  1 |  0 |  1 |  1 |
R4 |  0 |  0 |  1 |  0 |  0 |  0 |
R5 |  1 |  0 |  0 |  0 |  0 |  0 | I need to merge the rows so that the ""1"" appears only in at most one of the merged rows (for example, R2 and R3 cannot be merged because both rows have a ""1"" in column C2), and so to minimise the total number of zero in the whole matrix.
The final goal is to find (and merge) the rows that do not share any ""1"" on the same column. Using the above example, I can do it in several ways. Solution 1. | C1 | C2 | C3 | C4 | C5 | C6 |
-------------------------------------
R1+R2 |  1 |  1 |  0 |  1 |  0 |  0 |
R3    |  0 |  1 |  1 |  0 |  1 |  1 |
R4+R5 |  1 |  0 |  1 |  0 |  0 |  0 | Solution 2. | C1 | C2 | C3 | C4 | C5 | C6 |
-------------------------------------
R1+R4+R2 |  1 |  1 |  1 |  1 |  0 |  0 |
R3+R5    |  1 |  1 |  1 |  0 |  1 |  1 | Solution 3. | C1 | C2 | C3 | C4 | C5 | C6 |
-------------------------------------------
R1+R3       |  1 |  1 |  1 |  1 |  1 |  1 |
R2+R4+R5    |  1 |  1 |  1 |  0 |  0 |  0 | All the presented solutions cannot be reduced any more, but the second and the third options are clearly more efficient w.r.t. the first one. In my problem, I need to deal with a matrix MxN where M and N are more than some hundreds. Is there an algorithm that can find the optimal solution?","['matrices', 'optimization', 'coloring']"
4683672,Show by hand $\int_{0}^{\sqrt{5}}x^{x}dx>4$,It's a very Challenging question perhaps a kind of olympiad question : Prove that : $$\int_{0}^{\sqrt{5}}x^{x}dx>4$$ It's pretty sharp since the left hand side is almost $4.0005$ . I recall the Sophomore dream : $$\int_{0}^{1}x^{x}dx=-\sum_{n=1}^{\infty}\left(-n\right)^{-n}$$ For the other part we have : $$\int_{1}^{\sqrt{5}}x^{x}dx\geq \int_{1}^{\sqrt{5}}\left(1-\sqrt{x}+x\right)^{1+x}dx$$ Because I show in another question the inequality $x>0$ : $$\left(1-\sqrt{x}+x\right)^{1+x}\leq x^x$$ But we are far from the goal . We have numerically : $$\int_{1}^{1.5}x^{x}dx>\int_{1}^{1.5}\left(e^{\left(-1+\frac{\left(1+x+x^{2}\right)}{3}\right)}+\frac{1}{6}\left(x-1\right)^{2}+\frac{1}{26}\left(x-1\right)^{4}\right)dx$$ How to show it by hand or make a proof where the final steps is the calculus of some constant ?,"['constants', 'inequality', 'definite-integrals', 'derivatives']"
4683680,Rearrangement of letters in a word,"I have the following exercise, how many ways are there two rearrange the letters in the word “apple”. I know that the answer will be $5!/2$ , as we have $5!$ permutations, but as we have $2$ ""p""s we counted twice every different permutation. I would like to get the same result using a more formal argument. We had in the lecture the following argument (which is I think used implicitly in my argumentation): Let $Y,X$ be sets. Suppose we have a disjoint partition $ Y = \dot\bigcup_{x\in X} Y_x$ with $\#Y_x =m$ . Then $\#X = \frac{\#Y}{m}$ .
Am I right that this is used implicitly and what are my $Y_x$ precisely.","['combinations', 'combinatorics', 'combinatorial-proofs', 'discrete-mathematics']"
4683707,Naive question about product topology vs quotient topology,"I have a very naive question about the product topology. Whenever I see a topological space with a product topology structure, this structure is already explicitly given. That is, the statement will begin something like "" $X\times Y$ be a topological space..."". My question is somehow ""(how) can you know a topological space has a product structure?"". If you are given a topological space X, are there some necessary and/or sufficient conditions which allow you to test whether there is a non-trivial Y and Z such that $X\cong Y\times Z$ ? My naive guess would go something like: Let $X$ be a topological space (maybe with some special structure).
Let $\sim$ be an equivalence relation (maybe with some special structure) and $X/\sim$ be the equivalence classes equipped with the quotient topology.
Now consider each $[x]\in X/\sim$ as a subset of $X$ , and equip each of theses subsets with the subset topology.
Then, if $[x]$ is homeomorphic to some topological space $Y$ , $\forall [x]\in X/\sim$ , then $X \cong X/\sim \times Y$ ? I'd guess this isn't true, but I haven't been able to come up with (what i am sure is) an obvious counter example. I believe my intuition about product spaces vs quotient spaces is deeply flawed, so some thoughts on this would also be appreciated.","['general-topology', 'product-space']"
4683768,Showing that a group of order $72$ has a normal subgroup of order at least $3$,"From Section $2.12$ of Herstein's ""Topics in Algebra"" ( $2^{\text{nd}}$ edition): $\;$ We give one other illustration of the use of the various parts of Sylow's theorem. Let $G$ be a group of order $72$ ; $o(G) = 2^{3}3^{2}$ . How many $3$ -Sylow subgroups can there be in $G$ ? If this number is $t$ , then, according to Theorem $2.12.3$ , $t = 1 + 3k$ . According to Lemma $2.12.5$ , $t \mid 72$ , and since $t$ is prime to $3$ , we must have $t \mid 8$ . The only factors of $8$ of the form $1 + 3k$ are $1$ and $4$ ; hence $t = 1$ or $t = 4$ are the only possibilities. In other words $G$ has either one $3$ -Sylow subgroup or $4$ such. $\;$ If $G$ has only one $3$ -Sylow subgroup, since all $3$ -Sylow subgroups are conjugate, this $3$ -Sylow subgroup must be normal in $G$ . In this case $G$ would certainly contain a nontrivial normal subgroup. On the other hand if the number of $3$ -Sylow subgroups of $G$ is $4$ , by Lemma $2.12.5$ the index of $N$ in $G$ is $4$ , where $N$ is the normalizer of a $3$ -Sylow subgroup. But $72 \nmid 4! = (i(N))!$ . By Lemma $2.9.1$ $N$ must contain a nontrivial subgroup of $G$ (of order at least $3$ ). It is the last part that I don't understand. Since the the normalizer $N$ is of order $18$ in this example, why must the order of the normal subgroup that $N$ contains be at least $3$ ? That is, why can't $N$ only have one normal subgroup, of order $2$ ? Clarifying notes: By $i(N)$ , Herstein means the index of the subgroup $N$ .
For reference, here is Lemma $2.9.1$ , Lemma $2.12.6^{*}$ , and Theorem $2.12.3$ : Lemma $2.9.1$ : If $G$ is a finite group, and $H \neq G$ is a subgroup of $G$ such that $o(G) \nmid i(H)!$ then $H$ must contain a nontrivial normal subgroup of $N$ . In particular, $G$ cannot be simple. Lemma $2.12.6$ : The number of $p$ -Sylow subgroups in $G$ equals $o(G) / o(N(P))$ , where $P$ is any $p$ -Sylow subgroup of $G$ . In particular, this number is a divisor of $G$ . Note that $N(P)$ in the above lemma is the normalizer of $P$ Theorem $2.12.3$ : The number of $p$ -Sylow subgroups in $G$ , for a given prime, is of the form $1 + kp$ . $^{*}$ The book mistakenly references Lemma $2.12.5$ in place of Lemma $2.12.6$ , I think, which is why I include Lemma $2.12.6$ instead of Lemma $2.12.5$ .","['finite-groups', 'normal-subgroups', 'abstract-algebra', 'sylow-theory', 'group-theory']"
4683791,Integral of $\int_0^1 \frac{dx}{(1-x^5)^{1/5}}$,"I've attempted to integrate $I = \int_0^1\frac {dx}{(1-x^5)^{1/5}}$ using residues. I've convinced myself that this integral converges, and am under the impression that $f(z) = \frac{1}{(1-z^5)^{1/5}}$ has branch points at the five fifth roots of unity. I've taken the rays from these points out to infinity as my branch cuts, and attempted to calculate the integral along a contour $0 \to 1 \to e^{2\pi i/5} \to 0$ , where I've made little quarter circles around the singularities. It seems to me (from a calculation) that the little circles around the singularities have zero contribution in the limit $r\to 0$ . It also seems that the integral $e^{2\pi i/5}\to 0$ is just $-e^{2\pi i/5} I$ , and finally the arc between the singularities is proving a little tricky to me (initially I tried to ignore convergence issues and use a binomial series but this may well be where I went wrong...). Any tips are appreciated!","['integration', 'definite-integrals', 'improper-integrals', 'complex-analysis', 'contour-integration']"
4683814,Radon-Nikodym derivative in a compact Hausdorff space,"Let $X$ be a compact Hausdorff space, $m$ be a regular non-atomic probability measure defined on the Borel $\sigma$ -algebra of $X$ , and $g$ an autohomeomorphism of $X$ . Suppose that the image measure $g_{\ast}m$ (defined by $g_{\ast}m(O) = m\big[g^{-1}(O)\big]$ for each Borel subset $O$ ) is equivalent to $m$ . Then, for each $F\in C(X)$ , I will have: $$
\int_X F(x)\frac{d\,g_{\ast}m}{d\,m}(x)\,dm(x) = \int_X F(x)\,d(g_{\ast}m)(x) = \int_X F(x)\, dm(g^{-1}(x)) = \int_X F\circ g(x)\,dm(x)
$$ Define $\phi_m$ to be the positive linear functional defined by $m$ (and defined on $C(X)$ ) and $\phi_{g_{\ast}m}$ similarly, $I = \{F\in C(X): \phi_m(\vert\,f\,\vert)=0\}$ and $I_g = \{F\in C(X): \phi_{g_{\ast}m}(\vert\,f\,\vert) = 0\}$ . Then the image of the following mapping: $$
M: I_g\rightarrow I,\hspace{0.3cm} F(x) \mapsto F(x)\frac{d\,g_{\ast}m}{d\,m}(x)
$$ is equal to the image of the action (restrcited on $I_g$ ) defined by $g\bullet F(x) = F\circ g(x)$ . Obviously both mappings are invertible and the action is a isometric isomorphism between $I_g$ and $I$ . My questions are: Since the action $F\mapsto g\bullet F$ is multiplicative, $M$ will never coincide with the action unless $g$ is the identity. In this case what can we tell about the local range of $(d\,g_{\ast}m/d\,m)$ ? Is it true that, for each probability regular non-atomic measure $m$ , there exists a non-identity element $g\in\operatorname{Homeo}(X)$ (the homeomorphism group of $X$ ) such that $m$ is equivalent to $g_{\ast}m$ ? Given a non-trivial subgroup $G\subseteq\operatorname{Homeo}(X)$ , will there exist a regular non-atomic probability measure $m$ such that $g_{\ast}m\sim m$ for each $g\in G$ ? Update : Let $m$ be a probability regular non-atomic measure and assume $g\in\operatorname{Homeo}(X)$ satisfies $m\sim gm$ . According to this post , we can find a compact set $C_{\epsilon}$ such that both $m(C_{\epsilon})$ and $gm(C_{\epsilon})$ are less than $\epsilon$ . I tried to use this and the Urysohn's Lemma to decide the local behavior of the derivative but failed. Any hints will be appreciated. Update2 : From the Corollary 6.2.2 in Measures on topological space written by V.I.Bogachev ( link ), for a fixed $g\in\operatorname{Homeo}(X)$ , one can find a Radon probability measure $m$ (and hence regular when $X$ is compact) such that $g_{\ast}m = m$ . In such case, compared to my original question, I suppose it would not be too greedy to ask, given $m$ a probability measure, is it possible to find a subgroup $G\subseteq\operatorname{Homeo}(X)$ such that $g_{\ast}m\sim m$ for each $g\in G$ .","['c-star-algebras', 'measure-theory', 'dynamical-systems', 'probability-theory', 'radon-nikodym']"
4683821,How do I calculate the following sum,I'm trying to calculate the sum: $$\sum_{n=1}^\infty \frac{(-1)^{n+1}}{(n)(2n-1)}$$ I tried to write it as follows: $$\sum_{n=1}^\infty (-1)^{n+1}\left (\frac{2}{2n-1}  -    \frac{1}{n} \right) $$ However I did not deduce anything. I would appreciate any suggestion!,"['calculus', 'sequences-and-series', 'real-analysis']"
4683833,Example of minimal parabolic k-subgroups that are not Borel subgroups,"A subgroup $B < G$ of a connected algebraic group $G$ that is maximal among the solvable connected subgroups is called a Borel subgroup . A closed subgroup $P < G$ is parabolic if it contains a Borel subgroup. Let $K/k$ be a field extension with $K$ algebraically closed. Here they say that minimal parabolic $k$ -subgroups (i.e. defined over $k$ ) play the same role over $k$ as the Borel subgroups play over $K$ . How should I think of this? Are there any examples of groups, where the minimal parabolic $k$ -subgroups are not Borel subgroups? I guess this would mean that the Borel subgroups must not be defined over $k$ ? I am mostly interested in $K= \mathbb{C}, k = \mathbb{R}$ .","['group-theory', 'real-algebraic-geometry', 'algebraic-groups']"
4683848,Intuition about existence of directional derivatives doesn't imply differentiability [duplicate],"This question already has answers here : Why does existence of directional derivatives not imply differentiability? (4 answers) Closed last year . I am aware of several examples showing that existence of directional derivative in all directions does not imply differentiability at a point, but why is the existence of directional derivatives not enough to assure differentiability? For example, I understand the idea and relation between smoothness and differentiability, but how come smoothness cannot be guaranteed when we just know directional derivatives exist? Hope I am clear about my concern. Thanks!","['multivariable-calculus', 'intuition']"
4683859,Differential equation in $\mathcal D'$ (generalized function/distributions),"I'm struggling with understending of how should I solve differential equations in distributions. For example,if I'm asked to solve in $\mathcal D'$ the following equation: $$ F'-2F=0$$ Okay, so $F'=2F$ in terms of distributions, which means $F'$ and $2F$ act the same on every test function. Recalling the definition: $$\langle F', \varphi \rangle = - \langle F, \varphi' \rangle \ \ \ \ \ \ \ \langle 2F, \varphi \rangle = \langle F, 2\varphi \rangle$$ So should I write it in the way $$ -\int\limits_{\mathbf R} F\varphi'\ dx =2\int\limits_{\mathbf R} F\varphi\ dx $$ But it feels like I go back from definition and losing solutions which do not have this form. Reading about distributions I found out that sometimes problems are solved with ""well-known"" little facts, perhaps I miss something like that. I'm sure I have to reduce this equation to something known, like, $F'=\delta(x)$ , but I don't see how I can do that. So basically I'm looking for hints in techniques or facts/tricks that could help me to solve this and other linear DE in distributions. Thanks!","['ordinary-differential-equations', 'distribution-theory']"
4683888,Prime divisors of $a^n+b$.,"Let $a,b\in \mathbb N$ , $a>1$ . Prove that the set $M=\{\omega(a^n+b)\mid n\in \mathbb N\}$ is infinite, where $\omega(n)$ is the number of distinct pime divisors of $n$ . First, obviously we have to prove that $P=\{p\in \mathbb P:\exists n\in \mathbb N \text{ s.t. } p\mid a^n+b\}$ is infinite, but since the set of prime divisors of $a^n$ is finite, we have by Kobayashi that $P$ is infinite (If somebody knows how to prove this without Kobayashi feel free to post the proof). So I only managed to solve the case $(a,b)=1$ . Here's the proof, Fist denote $f(n)=a^n+b$ . Assume $M$ is finite and take $m=\max M$ then there would be some $n$ such that $$f(n)=a^n+b=\prod_{i=1}^m{p_i^{c_i}}$$ Consider the number $m=n+\prod\phi(p_i^{c_i+1})$ , by Fermat we have $\forall 1\le i\le m$ , $$f(m)\equiv a^n+b\equiv f(n)\pmod{p_i^{c_i+1}}$$ So $p_i^{c_i+1}\mid f(m)\iff p_i^{c_i+1}\mid f(n)$ this is enough to imply $$\nu_{p_i}(f(m))=\nu_{p_i}(f(n))$$ But $f(m)>f(n)$ so there must be some $p\ne p_i$ such that $p\mid f(m)$ contradicting the maximality of $m$ . There are two main problems when $(a,b)>1$ , the first is you can't directly use Fermat's Theorem and the second is how do you ensure that $\nu_p(f(m))=\nu_p(f(n))$ for $p\mid (a,b)$ ? Certainly you can't use Fermat there.","['number-theory', 'elementary-number-theory', 'prime-numbers']"
4683889,"What's the domain of $x^{2/6}$? $\mathbb{R}$ or $[0, \infty)$?","Is domain of $f(x) = x^{1/3}$ same as $f(x) = x^{2/6}$ ? This is a exam problem in pre-calculus which asked us to write domain of $x^{2/6}$ . I think the answer should be $[0, \infty)$ but the teacher said $\mathbb{R}$ . It's equivalent to how to define the domain of $x^{p/q}$ with $p, q \in \mathbb{N}$ ? Method1 (The teacher uses): We need to let $p, q$ coprime, then compute the domain. In this case, the domain should be same. That is $x \in \mathbb{R}$ . Method2 (I use): $x^{p/q} \equiv \left(\sqrt[q]{x}\right)^p \equiv 
 \sqrt[q]{x^p}$ . Then domain of $f(x) = x^{1/3}$ is $\mathbb{R}$ .
For domain of $f(x) = x^{2/6}$ , there is no universal definition for $x < 0$ . $((-1)^2)^{1/6} = 1$ but $((-1)^{1/6})^2$ is not well-defined in $\mathbb{R}$ or multivalued in $\mathbb{C}$ $e^{i (\frac{1}{3}\pi + \frac{2n}{3}\pi)}$ . Therefore, we need to exclude negative number, that is, domain is $[0, \infty)$ . As shown in wiki and proofWiki , it doesn't require to make $p, q$ coprime first. Which one is correct? That is what's the precedece of viewing the symbol $x^{2/6}$ .","['algebra-precalculus', 'functions']"
4683900,Closure of product with an particular metric,"Let $(\mathbb{R}^2,d)$ a metric space, with $d(x,y)=\|x\|+\|y\|$ when $x\not=y$ and $d(x,y)=0$ when $x=y$ . Find the closure of $(0,1)\times(0,1)$ . My intuition says that the closure is $[0,1]\times[0,1]$ but since we are working with that metric, I don't know if the closure changes, and if it doesn't, I don't know how to formally prove that $[0,1]\times[0,1]$ is the closure. I would appreciate any help.","['euclidean-geometry', 'normed-spaces', 'metric-spaces', 'analysis', 'real-analysis']"
4683921,Find the segment CR in the figure below?,"In the figure below, MD=2, CD=16 is RP=5.calculate CR.( $S:CR=\sqrt3)$ The figure above is not to scale. I try:
Extending PR to A on the circumference we will have $\triangle ARB_{(right)}$ $AR=16 \implies AP = 16 - 5=11$ $ \triangle CPB: PR^2 = CR.BR$ $OA=R \implies (2R)^2=16^2+BR^2$ $O1=r \implies 4R^2 = 256 + (2r-CR)^2$","['euclidean-geometry', 'circles', 'geometry', 'plane-geometry']"
4683994,Why should I care that a matrix is diagonalizable? [duplicate],"This question already has answers here : A matrix is diagonalizable, so what? (5 answers) Closed last year . I was talking to someone about diagonalizability of matrices and they asked me why we even care. For one thing, we can easily raise the matrix to the $n$ -th power. If $A=PDP^{-1}$ then, $A^n = P D^n P^{-1}$ and the diagonal matrix $D$ is very easy to raise to the $n$ -th power (just raise all the diagonals). Ok, he said - so just a computational benefit. Also, you can find the determinant by multiplying the numbers on the diagonal. Ok, again a computational benefit since you could find it through other means too. I realized that I can't think of a fundamental benefit one gets from being able to diagonalize a matrix that extends beyond computational efficiency. For invertibility, there are many things you get that you just can't do otherwise. Is there anything similar one gets for diagonalizability that I'm not aware of? Note another similar question: A matrix is diagonalizable, so what? that asks for a solid perception of diagonalizability. I believe this one is slightly different since it focuses on any utility barring numerical efficiency.","['matrices', 'diagonalization', 'linear-algebra']"
4684015,Maximum number of real roots of $f(x)=\sum_{k=0}^n (\sin{k\theta})x^k$,"What is the maximum number of real roots of $f(x)=\sum\limits_{k=0}^n (\sin{k\theta})x^k$ where $n\in\mathbb{N}, \theta\in\mathbb{R}$ ? Experimenting on desmos, it seems that when $n$ is odd, $f(x)$ can have at most three real roots; and when $n$ is even, $f(x)$ can have at most four real roots. If that is true, it would seem rather strange: why three and four? Possibly related: a polynomial of the form $\sum\limits_{k=0}^n a_k x^k$ , where $a_k=\pm1$ , can have arbitrarily many distinct real roots .","['trigonometry', 'roots', 'polynomials']"
4684033,Proof of k-th Difference operator with sequence relation [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Improve this question https://en.wikipedia.org/wiki/Recurrence_relation#Difference_operator_and_difference_equations I know that $$
\Delta^k a_n = \sum_{t=0}^{k} (-1)^t \binom{k}{t}a_{n+k-t}.
$$ But I don't know how to prove that $$ 
a_{n+k} = a_{n} + \binom{k}{1} \Delta a_n 
+ \cdots + \binom{k}{k} \Delta^{k}(a_n).
$$ How to prove it?","['combinatorics', 'recurrence-relations']"
4684053,Equation $a^a=b^b$,"Here's the problem: Find all pairs of rational numbers $(a,b)$ such that $0<a<b$ with $a^a=b^b$ . Using some calculus, I was able to find that $0<a<\frac{1}{e}<b<1$ , and I also found one solution $(a,b) = (\frac{1}{4}, \frac{1}{2})$ . I thought it would be the only solution, but I couldn't prove it.
How can I go further and solve this problem?","['calculus', 'algebra-precalculus']"
4684136,"Continuity, partial derivatives, differentiability questions for $f(x, y):= \frac{x^2 \sin y^2}{x^2+y^4}$","I would like to know if you think my answers are correct, I am not so sure about $1$ and $3$ and I don't know how to answer $4$ . Define $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ by \begin{align*}
f(x, y):= \frac{x^2 \sin y^2}{x^2+y^4}, & (x, y) \neq(0,0) & \\ f(x,y) := 0, & (x, y)=(0,0)
\end{align*} Is the function continuous in (0,0)? Do the partial derivatives $(D_1f)(0,0)$ en $(D_2f)(0,0)$ exist? For which $u \in \mathbb{R}^2 \backslash \{0\}$ does the directional derivative $(D_uf)(0,0)$ exist? Is $f$ differentiable in $(0,0)$ ? Is $f$ a $C^1$ function? To show that $f$ is continuous at $(0,0)$ , we need to show that $\lim_{(x,y)\to(0,0)}f(x,y)=f(0,0)=0$ . We can do this by using the epsilon-delta definition of a limit. To do this, we need to show that for every $\epsilon > 0$ , there exists a $\delta > 0$ such that if $||(x,y) - (0,0)|| < \delta$ , then $|f(x,y) - f(0,0)| < \epsilon$ . Let $\epsilon > 0$ and take $\delta = \sqrt{\epsilon}$ . If $||(x,y) - (0,0)|| < \delta$ , then we have: \begin{align*}
|f(x,y) - f(0,0)| &= ||\frac{x^2\sin y^2}{x^2+y^4} - 0|| \
&= |\frac{x^2\sin y^2}{x^2+y^4}| \
& \leq |\frac{x^2\sin y^2}{x^2}| \
& \leq |\frac{x^2y^2}{x^2}| = y^2 \
& \leq x^2 + y^2 = \delta^2 = \epsilon.
\end{align*} Here, we used that $x^2 \leq x^2 + y^4$ , since $y^4 \geq 0$ for all $y$ and observed that $0 \leq |\sin y^2| \leq 1$ for all $y$ . Thus, the limit of the function exists at 0 and $f$ is continuous at 0. We prove this using the definition of partial derivative. We have $(D_jf)(a) := \lim_{t \to 0} \frac{f(a + te_j) - f(a)}{t}$ with $a = (0 \quad 0)^T$ . Thus: \begin{align*}
(D_1f)(0,0) & = \lim_{t \to 0} \frac{f\left(\left(\begin{array}{c} 0 \ 0 \ \end{array}\right) + t\left(\begin{array}{c} 1 \ 0 \ \end{array}\right)\right) - 0}{t} \
& = \lim_{t \to 0} \frac{f(t,0) - 0}{t} \
& = \lim_{t \to 0} \frac{\frac{t^2\sin(0^2)}{t^2 + 0^4} - 0}{t} \
& = \lim_{t \to 0} \frac{\frac{t^2 \cdot 0}{t^2}}{t}\
& = \lim_{t \to 0} \frac{0}{t^3} \
& = \lim_{t \to 0} 0 \
& = 0
\end{align*} and \begin{align*}
(D_2f)(0,0) & = \lim_{t \to 0} \frac{f\left(\left(\begin{array}{c} 0 \ 0 \ \end{array}\right) + t\left(\begin{array}{c} 0 \ 1 \ \end{array}\right)\right) - 0}{t} \
& = \lim_{t \to 0} \frac{f(0,t) - 0}{t} \
& = \lim_{t \to 0} \frac{\frac{0^2\sin(t^2)}{0^2 + t^4} - 0}{t}\
& = \lim_{t \to 0} \frac{\frac{0}{t^4}}{t}\
& = \lim_{t \to 0} \frac{0}{t^5} \
& = \lim_{t \to 0} 0 \
& = 0.
\end{align*} So the partial derivatives $(D_1f)(0,0)$ and $(D_2f)(0,0)$ exist. We look at the definition of the directional derivative $(D_uf)(0,0) = \lim_{t \to 0} \frac{f(a + tu) - f(a)}{t}$ . For $a = (0 \quad 0)^T$ , this gives (with $u := (u_1, u_2)$ ): \begin{align*}
\lim_{t \to 0} \frac{f(a + tu) - f(a)}{t} &= \lim_{t \to 0} \frac{f(tu)}{t} \
& = \lim_{t \to 0} \frac{f(t(u_1, u_2))}{t} \
&= \lim_{t \to 0} \frac{f(tu_1, tu_2)}{t} \
& = \lim_{t \to 0} \frac{\frac{t^2u_1^2 \cdot \sin(t^2u_2^2)}{t^2u_1^2 + t^4u_2^4}}{t} \
& = \lim_{t \to 0} \frac{t^2u_1^2 \cdot \sin(t^2u_2^2)}{t^3u_1^2 + t^5u_2^4} \
& = \lim_{t \to 0} \frac{u_1^2\sin(t^2u_2^2)}{tu_1^2 + t^3u_2^4}\
& = \frac{u_1^2\sin(0^2 u_2^2)}{0u_1^2 + 0^3u_2^4} \
& = \frac{0}{0}.
\end{align*} So for no $u \in \mathbb{R}^2 \backslash {0}$ , the directional derivative $(D_uf)(0,0)$ exists. We use the theorem that says when $f$ is differentiable in $a$ , then the partial derivatives $(D_jf_i)(a)$ exist and we have the total derivative $Df(a)$ . ?? To show that $f$ is a $C^1$ function, we need to show that the partial derivatives exist and are continuous on $\mathbb{R}^2$ . The partial derivatives of $f$ are: \begin{align*}
\frac{\partial f}{\partial x}(x,y) &= \frac{2x\sin y^2 (x^2+y^4) - x^2\sin y^2 \cdot 2x}{(x^2+y^4)^2} \
&= \frac{2x\sin y^2 (y^4)}{(x^2+y^4)^2}, \\
\frac{\partial f}{\partial y}(x,y) &= \frac{x^2 \cdot 2y\cos y^2 (x^2+y^4) - x^2\sin y^2 \cdot 4y^3}{(x^2+y^4)^2} \
\end{align*} Both partial derivatives are continuous on $\mathbb{R}^2$ since they are defined as quotients of continuous functions with denominators that do not become zero. Therefore, $f$ is a $C^1$ function on $\mathbb{R}^2$ .","['analysis', 'continuity', 'solution-verification', 'partial-derivative', 'derivatives']"
4684188,Finding sum of zeroes of $f(x)$ from $g'(x)=60f(x)$ .,"Question: Let $f(x)=x^{5}+a x^{4}+b x^{3}+c x^{2}+d x+e$ . If $ad-25e=0$ and $g(x)=10 x^{6}+12 a x^{5}+15 b x^{4}+20 c x^{3}+30 d x^{2}+60 e x+k$ where $a, b, c, d, e, k \in R
$ has six positive real zeros and $f(3)=1$ , then find the sum of the zeros of $f(x)$ . Things I have observed: $$g(x)=10 x^{6}+12 a x^{5}+15 b x^{4}+20 c x^{3}+30 d x^{2}+60 e x+k\\g'(x)=60(x^{5}+a x^{4}+b x^{3}+c x^{2}+d x+e)\\g'(x)=60f(x)$$ $$g'(3)=60f(3)=60$$ Also by Descartes' rule of signs, the sign of $a,b,c,d,e$ must be alternate. Further, I tried using Vieta's formula but it didn't lead me anywhere. I have not been able to extract information from $ad=25e$ . Can someone please give me hints?
(Please give me hints only, it helps me!) Thanks","['functions', 'roots', 'polynomials']"
4684195,Proving $ \cos x + \frac{2p}{\pi}\sin x \geq 1 - 2 \left(\frac{x}{\pi}\right)^p$ for $p>0$,"I came across the following inequality: $$
\cos x + \frac{2p}{\pi}\sin x \geq 1 - 2 \left(\frac{x}{\pi}\right)^p,
$$ which should be valid for any $0\leq x \leq \pi$ and $p\in\mathbb R$ according to the graphs. I managed to prove it myself for $p\leq0$ as follows: If $p=0$ it's trivial, and if $p<0$ , I look at the difference $$f(x) = \cos x + \frac{2p}{\pi}\sin x - 1 + 2\left(\frac{x}{\pi}\right)^p$$ Noticing that $f(\pi)=0$ , it is enough to show $-f'(x)\geq0$ . Then, $$-f'(x) = \sin x - \frac{2p}{\pi}\left(\cos x + \left(\frac{x}{\pi}\right)^{p - 1}\right)$$ and all terms are nonnegative for $x\in[0,\frac{\pi}{2}]$ . If $x\in[\frac{\pi}{2},\pi]$ then still $\cos x + \left(\frac{x}{\pi}\right)^{p - 1}\geq0$ as the latter term is at least $1$ . This method doesn't help me for the case $p>0$ , though, as the function is no longer monotone. I tried to calculate (among other things) a taylor and fourier series, but found it wasn't much use for general $p$ . Any help or hints will be appreciated.","['self-learning', 'trigonometry', 'inequality', 'real-analysis']"
4684219,Concerning this curious sum: $\sum_{n=1}^{\infty}\frac{2n+1}{2^{2n}}\left[\frac{n-1}{2^{2n}}-(n-2)\right]\zeta(2n+1)=\ln2$,How would you go about showing this is correct? $$\sum_{n=1}^{\infty}\frac{2n+1}{2^{2n}}\left[\frac{n-1}{2^{2n}}-(n-2)\right]\zeta(2n+1)=\ln2$$ Any hints may help? It is quite starnge $\zeta(2n+1)$ would cough out $\ln2$,"['riemann-zeta', 'sequences-and-series']"
4684248,How to find the probability of the largest $k$ values from $n$ sorted stacks being in the top $m$ items from each stack?,Let’s say I have a set $U$ containing some values. I randomly distribute these values into $n$ stacks and sort each stack from largest to smallest. Then I take the top $m$ items from each stack to form a set $S$ . What is the probability the largest $k$ values in $U$ are also in $S$ ? I can see trivially that the probability is $1$ if $m=k$ but that’s as far as I’ve got.,['statistics']
4684249,How to prove a trace matrix is positive (semi) definite?,"Assume $X,A,B,C$ are all positive (semi) definite matrices with the same dimension. Define $$
S = \begin{bmatrix}
\mbox{tr}(XAXA) & \mbox{tr}(XAXB) & \mbox{tr}(XAXC) \\
\mbox{tr}(XAXB) & \mbox{tr}(XBXB) & \mbox{tr}(XBXC) \\
\mbox{tr}(XAXC) & \mbox{tr}(XBXC) & \mbox{tr}(XCXC)
\end{bmatrix}.
$$ Can we show that $S$ is a positive (semi) definite matrix? I think probably mathematical induction can be applied since the original problem itself is of $p>3$ dimension, but I still have no specific idea. To validate if it is the truth, I run the R code below. Here I simulated different positive definite matrices $X,A,B,C$ for $1000$ times and the results showed that all the $S$ matrix is positive definite. library(clusterGeneration)
K <- 50
ss <- 0
for(i in 1:1000){
  set.seed(i)
  X <- genPositiveDefMat(K)$Sigma
  A <- genPositiveDefMat(K)$Sigma
  B <- genPositiveDefMat(K)$Sigma
  C <- genPositiveDefMat(K)$Sigma
  S <- matrix(0, nrow = 3, ncol = 3)
  S[1,1] = tr(X%*%A%*%X%*%A)
  S[1,2] = S[2,1] = tr(X%*%A%*%X%*%B)
  S[1,3] = S[3,1] = tr(X%*%A%*%X%*%C)
  S[2,2] = tr(X%*%B%*%X%*%B)
  S[3,2] = S[2,3] = tr(X%*%B%*%X%*%C)
  S[3,3] = tr(X%*%C%*%X%*%C)
  ss <- ss + sum(eigen(S)$values > 0)
}
ss # The result ss = 3000 shows that all S is positive definite Motivation This question comes from the expectation of the 2nd order derivative of a specific log-likelihood function. I just simplified it to a $3$ -dimensional problem with $A$ , $B$ , $C$ matrices.","['probability', 'matrices', 'linear-algebra', 'induction', 'positive-definite']"
4684316,Calculus involved in math-modelling,"Background: I am trying to model my arm as a spring with constant b. As the arm is on the door it compresses and it accelerates the door and decelerates me. I am trying to solve for door angle theta as a function of time, from the moment when the arm touches the door to the moment the arm is fully compressed. The compression is expressed as the difference in tangential velocity of the door at contact point and the velocity of the man. I have worked through the derivation of the formulas but came up with an error function at the end which I think is not reasonable. I may have done this wrong, so please please help if you have any thoughts. I really need your help. the derivation is as follows:
At $t=0$ , Arm just touches the door and being compressed at a rate of $X(t)=\Delta x=(u_{man}-u_{door})t$ Where $u_{door}$ is the tangential velocity of the door at the current position $r$ , which can be expressed as $r=\frac{r_0}{\cos\theta}$ . From rotational kinematics, $u_{door}\cos\theta=\dot{\theta}r \Rightarrow u_{door}(t)=r_0\frac{\dot{\theta}(t)}{\cos^2{\theta(t)}}$ . For the man, he experiences a deceleration due to compression of the spring, $F_{spring}=bX=ma \Rightarrow a(t)=\frac{b}{m}X(t)$ . Hence, $u_{man}=u_0-\frac{b}{m}\int X(t)dt$ . This gives: $X(t)=(u_0-r_0\frac{\dot{\theta}(t)}{\cos^2\theta(t)} -\frac{b}{m}\int X(t)dt)t$ Rearrange and substitute $g(\dot{\theta},\theta)=u_0-r_0\frac{\dot{\theta}(t)}{\cos^2\theta(t)}$ : $t^{-1}\cdot X(t)=g(\dot{\theta},\theta)  -\frac{b}{m} \int X(t)dt$ $\frac{d}{dt}(t^{-1}\cdot X(t))=\frac{d}{dt} (g(\dot{\theta},\theta)  -\frac{b}{m} \int X(t)dt)$ $-t^{-2} X(t)+t^{-1} X'(t)=\frac{d}{dt} (g(\dot{\theta},\theta))-\frac{b}{m} X(t)$ Rearrange: $\frac{1}{t} X'(t)+(\frac{b}{m}-\frac{1}{t^2})X(t)=t\frac{d}{dt} (g(\dot{\theta},\theta))$ $X'(t)+(\frac{bt}{m}-\frac{1}{t})X(t)=t\frac{d}{dt}(g(\dot{\theta},\theta))$ Denote function $f$ , such that $f'(t)=(\frac{bt}{m}-\frac{1}{t})$ $X'(t)+f'(t)X(t)=t\frac{d}{dt}(g(\dot{\theta},\theta))$ Multiply both sides by $e^{f(t)}$ : $e^{f(t)}\cdot X'(t)+e^{f(t)}\cdot f'(t)\cdot X(t)=e^{f(t)}\cdot t\cdot \frac{d}{dt} (g(\dot{\theta},\theta))$ LHS can be rewritten as $e^{f(t)} X'(t)+e^{f(t)} f'(t)X(t)=\frac{d}{dt} (e^{f(t)} X(t))$ Hence, $\frac{d}{dt} (e^{f(t)} X(t))=t e^{f(t)}\frac{d}{dt} (g(\dot{\theta},\theta))$ $e^{f(t)} X(t)=\int t e^{f(t)} dt\cdot (g(\dot{\theta},\theta))$ from here if you substitute them in, you will find an integral of (exp(2t^2/bm))dt which is non solvable.","['integration', 'mathematical-modeling', 'ordinary-differential-equations', 'multivariable-calculus', 'calculus']"
4684419,Lie derivative of a coordinate form into its coordinate direction,"I have to calculate the Lie derivative of $\alpha = d x_1$ with respect to the vector field $X = \partial_1$ , but I cannot use Cartan's magic formula (which would immediatly show that $L_X\alpha= 0$ ). So the flow is given by $\phi_X^t(x) = (x_1+t,x_2,...,x_n)$ , and thus $D\phi_X^t(x) = id_n$ , right? Then it follows that $$\left.\frac{d}{dt}\right|_{t=0}(\phi_X^t(x))^*\alpha = \left.\frac{d}{dt}\right|_{t=0} \alpha\circ id_n = 0,$$ which should be the right answer... but I am not really sure if my linerarization of the flow is actually correct.","['differential', 'lie-derivative', 'differential-geometry']"
4684437,Lagrange Multipliers to solve extrema,"I have a problem here that I can't solve it. Can anyone help me out here, please? Find extrema of function $f(x,y,z)= x^{2} yz$ over the unit sphere $x^{2}+y^{2}+z^{2} = 1$ . Here is my take on it: $$\begin{equation}
\begin{aligned}\bigtriangledown f & = \:< \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}>\\ & =\: < 2xyz, x^{2}z, x^{2} y>
\end{aligned}
\end{equation}$$ $$ \begin{equation}
\begin{aligned}\bigtriangledown g & = \:< \frac{\partial g}{\partial x}, \frac{\partial g}{\partial y}, \frac{\partial g}{\partial z}>\\ & =\: < 2x, 2y, 2z>
\end{aligned}
\end{equation}$$ since there are parallel , so the cross product of them equals 0.
Now, to use the conventional Lagrange Multiplier method, I get : $$
\bigtriangledown f = \lambda \ast \bigtriangledown g $$ $$\begin{equation}
\begin{aligned}
\begin{cases}
 &  2xyz= \lambda \ast 2x\\ 
 &  x^{2} z=\lambda \ast 2y \\ 
 &  x^{2} y= \lambda \ast 2z
\end{cases}
\end{aligned}
\end{equation}$$ I can't solve x, y , and z from the system.
Then I am thinking, what if I factor the common factor from both gradient vectors. $$\begin{aligned} 
\frac{\bigtriangledown f }{x^{2} y z} & = \;<\frac{2}{x},\frac{1}{y},\frac{1}{z}> \\
\frac{\bigtriangledown g }{2} & = \;<x, y, z> \\
\frac{\bigtriangledown f }{x^{2} y z}\times \frac{\bigtriangledown g }{2} & = <\frac{z}{y}-\frac{y}{z},\frac{x}{z}-\frac{2z}{x},\frac{2y}{x}-\frac{x}{y}> \text{Cross Product}
\end{aligned}$$ $$\begin{cases}
 &  \frac{z}{y}-\frac{y}{z}=0 \\ 
 &  \frac{x}{z}-\frac{2z}{x}=0 \\ 
 &  \frac{2y}{x}-\frac{x}{y}=0 
\end{cases}$$ Simplify further: $$ 
\begin{cases}
 &  \frac{z}{y}=\frac{y}{z} \\ 
 &  \frac{x}{z}=\frac{2z}{x} \\ 
 &  \frac{2y}{x}=\frac{x}{y} 
\end{cases}$$ Assume: \begin{aligned}
\frac{y}{z} &= a\Rightarrow a = \frac{1}{a}\Rightarrow a^{2}= 1\Rightarrow a = \pm 1 \\
\frac{z}{x} &= b\Rightarrow 2b = \frac{1}{b}\Rightarrow b^{2}= \frac{1}{2}\Rightarrow b = \pm \sqrt{\frac{1}{2}}\\
\frac{y}{x} &= c\Rightarrow 2c = \frac{1}{c}\Rightarrow c^{2}= \frac{1}{2}\Rightarrow c = \pm \sqrt{\frac{1}{2}}\\
\end{aligned} Then, I have no clue what to do afterwards?","['multivariable-calculus', 'calculus', 'optimization', 'lagrange-multiplier']"
4684441,Minimum number of sets to guess a natural number from 1 to 100. One set contains up to 5 numbers.,"There is a hidden natural number from $1$ to $100$ .
One can make multiple sets of up to $5$ numbers.
Once sets are picked, all the sets containing the hidden number are highlighted.
What is the minimum number of such sets that after first try one can surely know the hidden number? May it be so that the answer is above $20$ , since if at least any number is not present in any of the set there is no way for us to know, if it is the hidden one? Edit: There are $100$ numbers from $0$ to $99$ . One number $x$ was picked, and we need to find $x$ . To find it, we can choose sets of up to $5$ numbers, and when all sets are chosen we are told which sets contain $x$ and which don't. Problem: Choose a smallest number of sets. Important is that we don't get any information until all sets are chosen.","['discrete-mathematics', 'information-theory']"
4684571,Creating an Example to Disprove the Central Limit Theorem?,"In class, we are always told that for the Central Limit Theorem to be applicable, observations have to be IID (Independent and Identically Distributed). However, we are not always told why this IID condition is so important for the Central Limit Theorem. This being said, I am trying to create an example where the IID condition is not met and thus show myself why it is required. Part 1: The first thing that comes to mind is an Autoregressive Process as by definition AR Processes are said not to be IID. For instance, suppose we have an AR(1) Process: $$y_t = \phi y_{t-1} + \epsilon_t$$ Based on this AR(1) process, I know the following: $E(\epsilon_t) = 0$ $E(\epsilon_t^2) = \sigma^2$ $E(\epsilon_t\epsilon_s) = 0$ $Var(y_t) = \frac{\sigma^2}{1-\phi^2}$ $E(y_t) = \phi E(y_{t-1}) = 0$ Part 2: As for the Central Limit Theorem, I know that in when $n$ is large, any Random Variable behaves as: $$\frac{\bar{x} - E(X)}{\sqrt{\frac{Var(X)}{n}}} \approx N(0,1)$$ Part 3: Putting this all together, I would now show that the above AR(1) Process DOES NOT converge to a Standard Normal Distribution: $$\frac{Y_t - E(Y_t)}{\sqrt{\frac{Var(Y_t)}{n}}} = \frac{Y_t - E(Y_t)}{\sqrt{\frac{\sigma^2}{1-\phi^2}\frac{1}{n}}} \not\approx N(0,1) $$ However, I am not sure if I am doing this correctly for the AR(1) Process and have in fact shown that in the absence of IID, the Central Limit Theorem is not necessarily valid. In general, can someone please show me an example where the IID condition is not met and as a result the Central Limit Theorem does not apply? Thanks! Note: I am aware that there are versions of the Central Limit Theorem that do not require the IID Condition (e.g. https://en.wikipedia.org/wiki/Central_limit_theorem#Lyapunov_CLT , https://en.wikipedia.org/wiki/Lindeberg%27s_condition ) - however, I am specifically interested in constructing an example that shows why the Classic Central Limit Theorem requires the IID condition.","['central-limit-theorem', 'probability']"
4684592,"How do I evaluate $\lim_{(x,y)\to (0,0)}\frac{x^2\sin(xy^2)}{x^4+9y^4}$","I have tried multiple paths to see if they have differing limits, but they all lead to 0, so I'm assuming that is the limit. Then, I tried using Squeeze Theorem, but it doesn't seem to work. This is my current progress: $$x^4+9y^4\geq x^4$$ $$\frac{x^2}{x^4+9y^4}\leq\frac{1}{x^2}$$ $$0\leq\left|\frac{x^2\sin(xy^2)}{x^4+9y^4}\right|\leq\left|\frac{\sin(x^2y)}{x^2}\right|$$ and I'm stuck in evaluating the limit of the upper bound.","['limits', 'multivariable-calculus']"
4684622,Can we show without using calculator that in a unit right triangle $1 \le ab^{b^2/a^2} + ba^{a^2/b^2} < 1 + \frac{7}{180}$?,"While working on this related inequality , I found the following inequality about right triangles. Let $a^2 + b^2 = 1$ be the two perpendicular sides of a right triangle then, $$
1 \le ab^{b^2/a^2} + ba^{a^2/b^2} < 1 + \frac{7}{180}
$$ I found this inequality interesting because it is pretty tight in the sense that the upper bound is less than $3.89\%$ higher than the lower bound. Wolfram Alpha gives the upper bound as $1.03888$ and a Monte Carlo simulation gives $1.03888238314$ at $a = 0.18733386311638833$ and $b= 0.9822963013927571$ . Hence I have used $1+\frac{7}{180} = 1.03888888...$ as an elegant estimate for the upper bound. Question : The above upper bound is using brute force methods of calculus, What is the best upper bound that can be obtained using Olympiad level tools and standard well known inequalities? Update 1 : The lower bound follows immediately from Young's inequality. Let $a^2+b^2 = c^2$ . Take $p = \frac{c^2}{b^2}$ and $q = \frac{c^2}{a^2}$ in $(3)$ of Young's inequality . Then, Young's inequality says that $$
\frac{b^2}{c^2}a^{c^2/b^2} + \frac{a^2}{c^2}b^{c^2/a^2} 
= \frac{b^2}{c^2}a^{1+a^2/b^2} + \frac{a^2}{c^2}b^{1+b^2/a^2} 
= \frac{b^2}{c^2}a^{1+a^2/b^2} + \frac{a^2}{c^2}b^{1+b^2/a^2}\ge ab
$$ or equivalently, $\displaystyle ab^{b^2/a^2} + ba^{a^2/b^2} \ge c^2$ .
Scaling the right triangle to a unit circle, we have $a^2+b^2=c^2 =1$ and the lower bound follows.","['inequality', 'algebra-precalculus', 'analysis', 'real-analysis']"
4684651,Chararacterization of a triangle for which $R(b+c)=a\sqrt{bc}$,My approach is as follow It is given that $R(b+c)=a\sqrt{bc}$ We know that $R=\frac{abc}{4\triangle}$ On solving we get $\frac{abc}{4\triangle}(b+c)=a\sqrt{bc}$ Which is $\frac{\sqrt{bc}}{4}(b+c)=\triangle$ but not able to solve.,['trigonometry']
4684658,Prove all Lie group homomorphisms of the circle have certain form,"Prove all lie group homomorphisms $\phi:\mathbb{S}^1\to\mathbb{S}^1$ has the form $z\to z^n$ for some $n\in\mathbb{Z}$ . My idea - first of all, I cannot use the Lie algebra-lie group correspondence as we've yet to learn it. First I proved that every $1-1$ homomorphism of the circle group must be the identity or the inverse map. So it's left for me to prove that given such homomorphism $\phi$ , it's kernel is the roots of unity of order $n$ for some $n\in\mathbb{N}$ . My attempt: Firstly, since $\ker(\phi)\subset\mathbb{S}^1$ is a closed subgroup, it is also a closed Lie subgroup. Assuming $\phi$ is not the trivial homomorphism, it is not both closed and open (as $\mathbb{S}^1$ is connected), and therefore must be $0$ -dimensional submanifold (as $\mathbb{S}^1$ is one dimensional) - and consequently has the discrete topology. So $\ker(\phi)$ is a discrete subset of a compact set and hence finite. Moreover, since every $z$ that is not root of unity has dense orbit in $\mathbb{S}^1$ and $\ker(\phi)$ is a closed proper subgroup, $\ker(\phi)$ can only contain roots of unity. Denote: $$N=max\{ord(z)\mid z\in\ker(\phi)\}$$ I want to claim that $\ker(\phi)$ is all roots of unity of order $N$ . By choice of $N$ , there exists $z\in\ker(\phi)$ with $ord(z)=N$ , i.e: $$z=e^{\frac{2\pi i r}{N}}$$ with $r,N$ coprime. As $\ker(\phi)$ is a subgroup, $e^{\frac{2\pi i}{N}}\in\ker(\phi)$ . This means that every root of unity satisfying $z^N=1$ is in $\ker(\phi)$ . Assume there exists some $w\in\ker(\phi)$ with $z^m=1$ and $m\not\mid N$ . Then $lcm(m,N)>N$ , $zw\in\ker(\phi)$ and: $$(zw)^{lcm(m,N)}=1$$ This however does not lead me to a contradiction as there might be some $N\geq k\mid lcm(m,N)$ such that $ord(zw)=k$ - at least I wasn't able to prove there isn't. How do I continute from here? I can't see the contradiction but if the statement in the question is true then there must be some way to reach contradiction. Any help would be appreciated.","['number-theory', 'gcd-and-lcm', 'roots-of-unity', 'group-theory', 'lie-groups']"
4684667,When does an invariant probability measure exist?,"Let $T : X \to X$ be a measurable automorphism of a standard Borel measurable space $X$ . How can we tell if $T$ has an invariant probability measure? For example, if $T$ is a rotation on the circle, then clearly an invariant probability measure exists. On the other hand, if $T$ is a translation on $\mathbb{R}$ , then none exists. Since I'm not assuming a given topology on $X$ , the kind of answer that I'm looking for will not make reference to a topology on $X$ . So the Krylov-Bogolyubov theorem is not really what I'm looking for. A good answer could for example be that no invariant probability measure exists if and only if a suitably defined cross product von Neumann algebra is properly infinite.","['measure-theory', 'ergodic-theory', 'dynamical-systems']"
4684673,"Given two unbounded subsets $X,Y$ of $\mathbb{R},$ do there exist three points of $X$ whose translation and stretch approximates three points of $Y?$","Suppose $X$ and $Y$ each are subsets of $\mathbb{R}$ that are bounded below and unbounded above (and therefore infinite). Given $\varepsilon>0,\ $ do there exist $\ x_1,\ x_2,\ x_3 \in X;\ x_1 < x_2 < x_3;\ \quad y_1,\ y_2,\ y_3 \in Y;\ y_1 < y_2 < y_3,\ $ such that $$ 1 - \varepsilon < \left\lvert \frac{ \frac{x_2 - x_1}{x_3 - x_1}}{\frac{y_2 - y_1}{y_3 - y_1}} \right\rvert < 1+\varepsilon\quad ?$$ This is equivalent to saying that we can find (many) pairs of triplets $(x_1,x_2,x_3)$ and $(y_1,y_2,y_3),$ such that $\ \frac{x_2 - x_1}{x_3 - x_1}$ and $ \frac{y_2 - y_1}{y_3 - y_1}\ $ are relatively arbitrarily close, which means that each pair of triplets $(x_1,x_2,x_3)$ and $(y_1,y_2,y_3),$ ""look similar to each other"" albeit for a scaling and translation. Is $X = \{2^n:\ n\in\mathbb{N} \};\ Y = \{3^n:\ n\in\mathbb{N} \}\ $ a counter-example? I am not sure... And maybe you construct a back-and-forth counter-example if $X$ and $Y$ are finite sets of arbitrary size, but I'm not sure this construction would extend to $X$ and $Y$ being infinite sets. On the other hand, it seems plausible that there are not good back-and-forth counter-examples if $X$ and $Y$ are finite sets of arbitrary size by some application of the Pigeonhole principle. But I'm not sure about what's true, so it will require more thought. Also, Stolz-Cesàro theorem might play a role.","['real-numbers', 'approximation', 'diophantine-approximation', 'pigeonhole-principle', 'real-analysis']"
4684710,If $f^{-1}(A)=A$ a.e. then show there is $B$ with $f^{-1}(B)=B$,"$f$ is a measurable mapping on $(X,\mu)$ , and is measure-preserving, i.e.  for all measurable set $E$ , $\mu (f^{-1}(E))=\mu(E)$ .
There is a measurable set $A$ ，s.t. $f^{-1}(A) =A$ , a.e. which means $\mu(A\bigtriangleup f^{-1}(A))=0.$ Then there exists $B$ measurable  s.t. $A=B$ , a.e. and $f^{-1}(B)=B$ . I find this problem is similar to Poincare's Theorem, but I was not able to imitate its proof. I tried to let $B= A \cap f^{-1}(A)\cap f^{-2}(A)\cdots$ but failed.","['measure-theory', 'dynamical-systems']"
4684713,"True or false: For every $N\in\mathbb{N}$, there exists $k\in\mathbb{N}$ such that $\sin{(k^n)}<0$ for $n=1,2,3,...,N$.","True or false: For every $N\in\mathbb{N}$ , there exists $k\in\mathbb{N}$ such that $\sin{(k^n)}<0$ for $n=1,2,3,...,N$ . I made up this question. I suspect the statement is true. I have considered complex numbers ( $\sin{(k^n)}=\text{Im}(e^{k^n i})$ ), proof by contradiction, and induction, to no avail. As an example, for $N\le10$ we have $k=539$ . EDIT As @durianice commented, $\sin{(539^7)}>0$ . (I was using desmos, which is not precise enough to calculate the sine of huge numbers .)","['elementary-number-theory', 'pi', 'natural-numbers', 'trigonometry', 'exponential-function']"
4684719,Proving that Fubini metric is well-defined,"Let $$\pi:S^{2n+1}\to S^{2n+1}/S^1=\mathbb{P}^n\mathbb{C}$$ be the Hopf fibration. I already know that $d_z\pi$ is a vector space isomorphism for every $z\in S^{2n+1}$ (when restricted to the orthogonal space to the orbit). We'd like to define a riemannian structure on $\mathbb{P}^n\mathbb{C}$ by push-forwarding the riemannian structure of $S^{2n+1}$ on each tangent space through $d_z \pi$ . Basically we want to define the Fubini metric as: $$\mathsf{fub}_{[z]}(u,v):=\mathsf{sph}_z((d_z\pi)^{-1}(u),(d_z\pi)^{-1}(v)),$$ where $\mathsf{sph}$ is the standard spheric metric. The only problem with this definition is that it may depend on the choice of the representative $z$ of the equivalence class $[z]$ . So let $e^{i\theta}z$ be a new representative: $$\mathsf{sph}_{e^{i\theta}z}((d_{e^{i\theta}z}\pi)^{-1}(u),(d_{e^{i\theta}z}\pi)^{-1}(v))=\langle (d_{e^{i\theta}z}\pi)^{-1}(u), d_{e^{i\theta}z}\pi)^{-1}(v) \rangle$$ where $\langle -,- \rangle$ is the standard euclidean inner product on $\mathbb{C}^{n}\cong \mathbb{R}^{2n}$ (the fixed isomorphism is $(x_1+iy_1,...,x_n+iy_n)\to (x_1,...,x_n,y_1,...,y_n)$ ). My problems would be over if I had an euclidean isometry $I:\mathbb{C}^n\to \mathbb{C}^n$ such that: $$(d_{e^{i\theta}z}\pi)^{-1}=I\circ (d_z\pi)^{-1}$$ so basically I'd like to prove that: $$(d_{e^{i\theta}z}\pi)^{-1}\circ (d_z\pi)$$ is an euclidean isometry. Is there an elegant way of doing this without dwelving too much into coordinates? Or in alternative, is there a better approach?","['differential-geometry', 'kahler-manifolds', 'riemannian-geometry', 'complex-geometry', 'projective-space']"
4684746,integral operator is not continuous,"Let $Y$ be a subset of $[0,1]^{[0,1]}$ of integrable functions. $I: Y \rightarrow \mathbb{R}$ , $f \mapsto \int_{0}^{1}f$ . We use the product topology on $[0,1]^{[0,1]}$ , that is, we consider $f$ as $\prod_{x \in [0,1]}{f(x)}$ and the euclidean metric topology on $\mathbb{R}$ .
How do we prove that $I$ is not continuous? Now for all $f_n$ in $Y$ with $f_n \rightarrow f$ we have $I(f_n) \rightarrow I(f)$ since $f_n$ converges to $f$ uniformly, but I know this doesn't necessarily mean that $I$ is continuous.","['general-topology', 'analysis']"
4684778,Growth rate of $(p-1)/\phi(p-1)$ for large primes,"Was looking into things regarding primitive roots of primes, and wanted to know roughly what fraction of residues are primitive roots. This led me to consider $S_n= (p_n-1)/\phi(p_n-1)$ . $S_n$ is the reciprocal of the primitive root fraction, chosen because ""unbounded"" is easier to think about than ""not bounded away from zero"". And since number theory isn't my forte, I decided to investigate numerically. For a lower bound, numerics suggest $\liminf_{n\rightarrow\infty}S_n = 2$ . I can easily show this follows from the conjecture that there are infinitely safe primes, but I'm not sure about whether the converse is true. More interesting to me is the upper bound. There seems to be no upper bound to $S_n$ , but it grows very slowly. It first exceeds $4$ at $p_{47} = 211$ , $5$ at $p_{4568} = 43891$ , and I gave up searching at $p_{856690} = 13123111$ where it first reaches $5.75$ . It appears that $S_n = O(\ln p_n)$ and that this bound isn't tight, but it's hard to tell since my range is so small for such slow-growing functions. (Also, it amuses me that in experimental number theory, seven orders of magnitude is ""too small"".) Anyways, I'm sure I'm not the first person to consider $S_n$ , but I wanted to take a bit of a stab at it myself first. What results, if any, already exist about $S_n$ ?","['number-theory', 'totient-function', 'prime-numbers']"
4684785,Closed form of integral: $\displaystyle\int_{1}^{\infty}\frac{ax-b+1}{x^{\frac{1}{b}}}e^{-\frac{a}{b}x}\ln\left(ax-b+1\right)\mathrm{d}x$,"Context I was trying to calculate the entropy in the Benktander distribution of the second kind, where: $$f_X(x)=\exp\left(\frac{a}{b}(1-x^b)\right)\cdot x^{b-2}\cdot(ax^b-b+1)\qquad x\geq 1$$ Where $a>0$ and $b\in(0,1]$ In this case the entropy is defined as: $$H[X]=-\int_1^\infty f(x)\cdot\ln(f(x))\mathrm{d}x$$ And after several steps I arrived at this point: $$H[X]=1+\frac{e^{\frac{a}{b}}}{b}\left(E_{\frac{1}{b}}\left(\frac{a}{b}\right)-\int_{1}^{\infty}\frac{ax-b+1}{x^{\frac{1}{b}}}e^{-\frac{a}{b}x}\ln\left(ax-b+1\right)\mathrm{d}x\right)$$ Where $E_s(z)$ is the generalized exponential integral What is the closed form of this integral? $$\displaystyle\int_{1}^{\infty}\frac{ax-b+1}{x^{\frac{1}{b}}}e^{-\frac{a}{b}x}\ln\left(ax-b+1\right)\mathrm{d}x$$ I think it might be useful to consider the following function: $$E_s(z):=z^{s-1}\int_{z}^{\infty}e^{-t}t^{-s}\mathrm{d}t$$ Is integral representation of the exponential integral function, so we can define this other function: $$E^{(1,0)}_s(z):=\frac{\mathrm{d}}{\mathrm{d}s}E_s(z)=z^{s-1}\int_{z}^{\infty}e^{-t}t^{-s}\ln\left(\frac{z}{t}\right)\mathrm{dt}$$ My approach is this: $$\int_{1}^{\infty}\frac{ax+b-1}{x^{\frac{1}{b}}}e^{-\frac{a}{b}x}\ln\left(ax+b-1\right)\mathrm{d}x=\left.\frac{\partial}{\partial s}\int_{1}^{\infty}\frac{(ax+b-1)^s}{x^{\frac{1}{b}}}e^{-\frac{a}{b}x}\mathrm{d}x\right|_{s=1}$$ Then I try to solve $$\int_{1}^{\infty}\frac{(ax+b-1)^s}{x^{\frac{1}{b}}}e^{-\frac{a}{b}x}\mathrm{d}x=\frac{\left(b-1\right)^{s+1-\frac{1}{b}}}{a^{1-\frac{1}{b}}}\int_{\frac{a}{b-1}}^{\infty}\frac{\left(x+1\right)^{s}}{x^{\frac{1}{b}}}e^{-\frac{b-1}{b}x}dx$$ Then I get stuck","['integration', 'statistics', 'improper-integrals', 'definite-integrals', 'probability']"
4684786,When does a maximal sub topology not containing an open set exist?,"The motivation is this question . I was quite optimistic in the formalization I provided of the question, it would indeed be the case that such a maximal sub topology exists. Now, I learn that it's less clear than I initially thought (after seeing comment and answers provided). I first bring in a definition: Stable topology $\tau$ under $U$ : A topology $\tau$ is stable under a non-empty open subset $U$ , if $U$ is an open set in the topology $\tau$ , and, there exists a maximal sub topology of $\tau$ not containing $U$ . This leads to a shorter formulation of my question, Is $\mathbb{R}$ stable under removal of an epsilon interval about a point?",['general-topology']
4684802,"A bounded positive linear mapping $T$ is an orthogonal projection if $\mathrm{dim}(\mathcal{R}(T))=1$ and $\sum_{k}\left<T(e_k),e_k\right>=1$","Let $\mathcal{H}$ be a Hilbert space over the field $\mathbb{K}$ and $T:\mathcal{H}\to\mathcal{H}$ be a bounded positive linear mapping such that $\mathrm{dim}(\mathcal{R}(T))=1$ and $\sum_{k\in\mathcal{I}}\left<T(e_k),e_k\right>=1$ for some Hilbert basis $(e_k)_{k\in \mathcal{I}}$ where the index set $\mathcal{I}$ can be countable or uncountable. I want to show that $T$ is an orthogonal projection onto the span of the single linearly independent element of its range. To be precise, $$\mathcal{R}(T):=\{T(v)\mid v\in\mathcal{H}\}$$ and as $\mathrm{dim}(\mathcal{R}(T)) = 1$ it follows that $\exists w\in\mathcal{H}:\forall v\in\mathcal{H}:\exists c_v\in\mathbb{K}:T(v) = c_vw$ . I have a suspicion that the scalar $c_v$ is given by the inner product $\left<v,w\right>$ with linearity in the first argument. Since $T$ is a bounded positive linear mapping, it suffices to show that $T$ is a projection, i.e. $T^2 = T$ and $T|_S=I$ for some $S\subset\mathcal{H}$ . My problem: Confirming the defining two properties of a projection mapping would be quite easy if we were to know the image of $w$ under $T$ . Namely, if we were to know that $T(w) = 1\cdot w = w$ , then for any $v := cw,c\in\mathbb{K},$ we would have $T(cw) = cT(w) = cw = v$ showing that $T = I$ on the span of $w$ and $T^2 = T$ . My problem is that I don't know how to a.) conclude that $T(w) = w$ b.) use the assumption $\sum_{k\in\mathcal{I}}\left<T(e_k),e_k\right>=1$ to my advantage. Since the $e_k$ s form a Hilbert basis, $w = \sum_{k\in\mathcal{C}}\beta_ke_k,\beta_k\in\mathbb{K}$ for some countable $\mathcal{C}\subset\mathcal{I}$ and $T$ is continuous, I suppose I would have to look at some double series of the form $\sum_{k_1\in\mathcal{I}}\sum_{k_2\in\mathcal{C}}\beta_k\left<T(e_{k_2}),e_{k_1}\right>$ and conclude something from this. But as of now it is not clear to me what I should do.","['hilbert-spaces', 'operator-theory', 'orthogonality', 'functional-analysis']"
4684819,"How can I show $\Bbb{P}(B_t\leq x, B^*_t\leq y)=F\left(\frac{x}{\sqrt t}\right)-F\left(\frac{x-2y}{\sqrt t}\right)$for geometric brownian motion?","Let $B$ be a brownian motion and $B^*_t:=\sup_{s\leq t} B_s$ . I want to show that for $y>0, x\leq y$ , $$\Bbb{P}(B_t\leq x, B^*_t\leq y)=F\left(\frac{x}{\sqrt t}\right)-F\left(\frac{x-2y}{\sqrt t}\right)$$ where $F$ is the distribution function of a normal random variable $N\sim \mathcal{N}(0,1)$ . I somehow don't see how to procedure, I thought about using the strong markov property for $W_t:=B_{t+\rho}-B_{\rho}$ where $\rho$ is a suitable stopping time, because we have done something similar in class. There we took $\rho:=\inf\{t>0: B_t\in (y,\infty\}$ to be the first entry time, which is an $\Bbb{F}^{B_+}$ stopping time, but I think this does not work in our case here. Therefore I wanted to ask if someone could help me futher. What we have proven in class is that for all $y>0$ and $x\leq y$ $$\Bbb{P}(B_t\leq x, B^*_t\geq y)=\Bbb{P}(B_t\geq 2y-x)$$","['stochastic-calculus', 'stochastic-processes', 'brownian-motion', 'probability-theory', 'probability']"
4684898,"Show that for every $\epsilon>0$, there exists $x_{\epsilon}$ real number such that $|f(x_{\epsilon})|<\epsilon$","We have $f:\mathbb R\to \mathbb R$ a function that has a primitive $F:\mathbb R\to (0,\infty)$ . Show that for every $\epsilon>0$ , there exists $x_{\epsilon}$ such that $|f(x_{\epsilon})|<\epsilon$ . I supposed that there exists an $\epsilon>0$ such that for every $x$ real number we have $|f(x)|>\epsilon$ .
So $f(x)\in (-\infty,-\epsilon)\cup(\epsilon,\infty)$ . But f has Darboux property because it has $F$ as a primitive. So $f(x)<-\epsilon$ or $f(x)>\epsilon$ for every $x$ real number. In the case when $f(x)<-\epsilon$ for every $x$ , we have $f(x)<0$ for every $x$ so $F$ is strictly decreasing and it has limits at $\pm\infty$ (I don't know if this helps). I tried using Lagrange theorem. If we take $a<b$ then $F(a)>F(b)>0$ and by Lagrange theorem we get that $\frac{F(b)-F(a)}{b-a}=f(c)<-\epsilon$ . I dont know how to finish the problem and how to get to a contradiction. I feel like I need to use some limit or derivative close to $\epsilon$ to get a contradiction.","['integration', 'mean-value-theorem', 'derivatives', 'epsilon-delta']"
4684936,Show that $x^{\overline {n}}=\sum_{k=0}^n \binom{n-1}{k-1} \frac{n!}{k!}x^{\underline{k}}$,"I want to proof the following: $\displaystyle x^{\overline {n}}=\sum_{k=0}^n \binom{n-1}{k-1} \frac{n!}{k!}x^{\underline{k}}$ where $x^{\overline {n}}=x(x+1)(x+2)...(x+n-1)$ (rising factorial) and $x^{\underline{k}}=x(x-1)...(x-k+1)$ (falling factorial) With the hint:
Consider $a_kx^{\underline{k}}$ . Define $D(p(x)):=p(x+1)-p(x)$ , using $D$ one can calculate a formula for $a_k$ Find the formula for $a_k$ and use this formula to show the above equality. My attempt:
I first tried to calculate $a_k$ $a_kx^{\underline{k}}=a_k x (x-1)(x-2)...(x-k+1)$ Applying $D$ $D (a_k x (x-1)...(x-k+1))=a_k ((x+1) x (x-1)...(x-k+2)-x(x-1)...(x-k+1))=a_k k x^{\underline{k-1}}$ Thus, $D^k (a_k x (x-1)...(x-k+1))=a_k k! x^{\underline{k-k}}=a_k k!$ By this I get the formula $a_k=D^k x^{\underline{k}} \dfrac{1}{k!}$ I did calculate that $Dx^{\overline{n}}=(1-n)x^{\overline{n+1}}$ Now $x^{\overline{n}}=c_0 +c_1 x^{\underline{1}}+...+c_n x^{\underline{n}}$ If I apply $D^n$ I get $x^{\overline{n+n}}=n! c_n x^{\underline{n-n}}$ $x(x+1)...(x+2n-1)=n! c_n$ I don't really know how to continue from here on.","['combinatorics', 'discrete-mathematics']"
4684938,Diffeomorphism between complex hyperbolic space and unit ball,"Let $h$ be the bilinear map: $$h:\mathbb{C}^{n+1}\times \mathbb{C}^{n+1}\to \mathbb{C},\ \ \ \ (z,w)\mapsto -z_0\overline{w_0}+\sum_{i>0} z_i\overline{w_i}$$ and let $\mathfrak{I}:h(z,z)=-1$ . There is an obvious left action of $S^1$ on $\mathfrak{I}$ and I wanna prove that $\mathfrak{I}/S^1$ is diffeomorphic to the open ball in $\mathbb{C}^n$ . My attempt It's pretty clear that every element of $\mathfrak{I}/S^1$ can be written (almost) uniquely as: $$z=(r_0,r_1e^{i\theta_1},...,r_ne^{i\theta_n})$$ so my idea was to define something like $z\mapsto (r_1,...,r_n,\theta_1,...,\theta_n)$ , but this clearly works only if we suppose that $r_i\neq 0$ for every $i>0$ (otherwise things get a bit messy, because $\theta_i$ is not even well defined). Is this the right track?","['kahler-manifolds', 'riemannian-geometry', 'complex-geometry', 'hyperbolic-geometry', 'differential-geometry']"
4684949,Does $f$ belong to $L^2(\mathbb R)$?,"The fourier transform of $f \in L^1(\mathbb R)$ is given by $\displaystyle\hat f ( s) = \int_{\mathbb R} f(x) \exp(-2\pi ix s) \mathrm{d}x.$ Suppose $\hat f \in L^2,$ is it true that $f \in L^2?$ As I did not get any counter examples to disprove, I have tried prove this unsuccessfully : We have $\displaystyle\lVert \hat f \rVert_2 = \int |\hat f(s) |^2\mathrm{d}s = \int \left|\int f(x) \exp(-2\pi ix s) \mathrm{d}x\right|^2 \mathrm{d}s $ is finte. We have to show that the integral $\displaystyle\int |f(x)|^2 \mathrm{d}x$ is finite. If Fourier inversion holds we may write $\displaystyle\int |f(x)|^2\mathrm{d}x = \int \left| \int \hat f (s) \exp(2i \pi s x)\mathrm{d}s\right|^2\mathrm{d}x$ but I am not able to proceed here. Also, I am not sure, if fourier inversion holds for $L^1$ functions.","['measure-theory', 'fourier-analysis', 'fourier-transform']"
4684992,Help Finding a Traveling Wave Solution,"I am looking for traveling wave solutions of \begin{align}
\frac{\partial U}{\partial t} &= AU\left(1-\frac{U}{K}\right)-BUV+D_{1}\nabla^{2}U \\
\frac{\partial V}{\partial t} &= CUV-DV+D_{2}\nabla^{2}V
\end{align} Where $A,B,C,D,K,D_{1}$ ,and $D_{2}$ are constants. After nondimensionalising I arrived at the following system of equations \begin{align}
\frac{\partial u}{\partial t} &= u(1-u-v)+D\frac{\partial^{2}u}{\partial x^{2}} \\
\frac{\partial v}{\partial t} &= av(u-b)+\frac{\partial^{2}v}{\partial x^{2}}
\end{align} I then substituted $u(x,t)=U(z), v(x,t)=V(z), z=x+ct$ to get \begin{align}
cU' &= U(1-U-V)+DU'' \\
cV' &= aV(U-b)+V''
\end{align} The considering the case where $D = 0$ and letting $V' = W$ I get the following system of first order ODEs \begin{align}
U' &= \frac{1}{c}U(1-U-V) \\
V' &= W \\
W' &= cW-aV(U-b)
\end{align} I am now stuck here I am unsure how to handle this system and get a traveling wave solution. I would appreciate any tips or suggestions on methods of solutions.","['ordinary-differential-equations', 'partial-differential-equations', 'nonlinear-system', 'mathematical-biology', 'population-dynamics']"
4685023,How do I check that $x^3 - 3x + 1$ has no roots in $\mathbb{Z}$?,"Serge Lang in Algebra (pg 270) says that $x^3 - 3x + 1$ has no roots in $\mathbb{Z}$ and hence no roots in $\mathbb{Q}$ . I can check using the rational root test that there are no rational roots, and consequently no integer roots either. Lang seems to be going in the opposite direction. He seems to be using Gauss's lemma and that's fine but how is he checking that there are no integer roots? There is another polynomial on the same page $x^3 - x + 1$ that he checks directly to be irreducible using the rational root test. Why is he not doing the same for $x^3 - 3x + 1$ ?","['algebra-precalculus', 'polynomials']"
4685036,How can I show that this subset of $L^2(Y)$ is equal to the zero set if we assume $T:Y\rightarrow Y$ is strong mixing?,"Let $(Y, \mathcal{A}, \mu)$ be a probability space and $T:Y\rightarrow Y$ a measure preseving map. Define $U=\{f\in L^2(Y): f\circ T=e^{-i\theta}f~~\text{a.e.}\}$ where $\theta\not \in 2\pi\Bbb{Z}$ . I want to show that if $T$ is strong mixing, then $U=\{0\}$ . Clearly $\{0\}\subset U$ . So one pick $f\in U$ , i.e. $f\circ T=e^{-i\theta}f$ . Now we can use the following fact with $g=1$ $T$ strong mixing iff for all $f,g\in L^2(Y)$ , $\int_Y f(T^n(y)) g(y)~d\mu=\int_Y f(y)~d\mu \int_Y g(y)~d\mu$ Then we deduce that $$\lim_{n\rightarrow \infty} \int_Y f(T^n(y))~d\mu=\int_Y f(y) ~d\mu$$ or equivalently $$\lim_{n\rightarrow \infty} (e^{-in\theta}-1)\int_Y f(y) ~d\mu=0$$ Since $\theta\not \in 2\pi \Bbb{Z}$ , we deduce that $e^{-in\theta}\neq 1$ , therefore $\int_Y f(y) ~d\mu=0$ . But from here I cannot deduce that $f=0$ . Can someone help me how to conclude? I also thought about working with $A:=\{y\in Y: f(y)\geq0\}$ and then define $g(y)=\Bbb{1}_A$ so that I somehow can guarantee that $f\geq 0$ and deduce $f=0$ but this also did not work.","['measure-theory', 'ergodic-theory', 'analysis', 'functional-analysis', 'dynamical-systems']"
4685086,How to solve $\sec^4(x)=\tan^4(x)+3\tan(x)$,"Background I was trying to solve this purely trigonometric equation: $$\sec(x)^4=\tan(x)^4+3\tan(x)$$ I performed my usual tactic,which is to rewrite everything in terms of sin and cos, and solve the resultant equation. (Works 100% of the time,70% of the time) and got: $$1=\sin(x)^4+3\sin(x)\cos(x)^3$$ Which I could not solve, although if anybody CAN solve it I would love to hear how. So I input the original equation into a computer algebra system and it told me that it is an equivalent problem to the solution of this equation: $$2\sec(x)^2=3\tan(x)+1$$ Now this was solvable with the same approach that failed on the original equation. I just converted it into a quartic polynomial problem at the end, after rewriting it in terms of sin and cos. But a number of questions arose, the answers to which would greatly help my ability to solve trig equations, but for the purpose of this site I think i'll split them up into multiple posts each with a very precise focus, so that they will be more useful to other users. Question: How do I know if a given trigonometric equation can be simplified?
(of course im primarily concerned about non obvious cases) and by simplified I think the example I gave is a good one for what I mean by simplified. Two things happened: (1) The number of terms in the equation that contained trigonometric functions was reduced. it went from 3 in the original to only 2 in the computer algebra system's output (2) The order (natural power) of any trig terms in the equations was reduced. How do I know if it is possible to accomplish one or both of these goals, given a purely trigonometric equation? (note: im not asking for a general procedure to do so, just a method of determining if this can be done given a purely trigonometric equation. I will ask other questions that arose from this problem later. )","['algebra-precalculus', 'trigonometry']"
4685102,Roll a die until the sum of the rolls is prime. Repeat this again and again. Show that the expected value of the number of rolls is $>2.428$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question So, let's denote the number of throws with $X$ .
For example, if the first roll is a $2$ ,then $X=1$ .
If the first roll is a $1$ and the second roll is a $6$ , then $X=2$ .
If the first one is $4$ ,the second is $2$ and the third $5$ , then $X=3$ , etc. Show that the expected value of $X$ is at least $2.428$ . // Edit:
I have first tried to prove it by writing a program that simulates all possible outcome, but even after numerous changes and optimization, it does not run under reasonable time (I am not even sure it would reach $2.428$ under $20$ minutes, and the ideal runtime would be under a minute).
I have no idea what other way I can approach the problem, I have tried to represent the possibilities as a tree, but I haven't figured out any pattern in it yet.","['statistics', 'probability', 'prime-numbers']"
4685131,Prove that $\limsup \ \frac{B\left(t\right)}{\sqrt{t\ }\log t}=0$,Let $B(t)$ be a Standard Brownian Motion. We need to prove that $\underset{t \to \infty}\limsup \dfrac{B\left(t\right)}{\sqrt{t}\cdot\ln(t)}=0$ I was thinking if we could use the Blumenthal's $0-1$ law along the lines of the proof for $\underset{t \rightarrow \infty}\limsup \dfrac{B(t)}{\sqrt{t}}=\infty$ . But I am not sure about how to argue in this case.,"['probability-distributions', 'stochastic-processes', 'brownian-motion', 'probability-theory', 'probability']"
4685134,Proving possibilities of building bipartite 2 colored graphs with n vertices,"I want to show that I can you can build a two-colorable graph for all n that have n vertices and $\left\lfloor \frac{n^2}{4} \right\rfloor$ edges. After this, I want to show that it is impossible to do the same for any graph that has n vertices and more than $\left\lfloor \frac{n^2}{4} \right\rfloor$ edges. For n = 0,1,2,3,4, these are quite simple and I see how it could be done. n=2 is $P_2$ , n = 3 is $P_3$ , and n = 4 is $C_4$ . Making these two-colored is not too difficult. I can find a two-colorable graph at n=5, but this was through trial and error. From n = 5 and further, I can not find a definite strategy for making a two colorable graph, as I cannot even find a single way thorugh trial and error for n = 6. I know that there being $\left\lfloor \frac{n^2}{4} \right\rfloor$ edges is probably the key to this problem, and I would assume knowing why this is the case would help me figure out as to why you cannot do this with any more vertices. EDIT: Okay, I believe I have found a pattern. If you split the colors as evenly as possible for the vertexes (i.e. if n = 9 then 4 red, 5 blue vertices) then you connect each vertex of one color to every vertex of the other (for the n = 9 example, each of the 4 red vertices can each be connected by an edge to the 5 blue vertices for a total of 4 * 5 edges). This has helped me successfully solve, n = 7,8,9,10. Seeing this pattern, I am having a bit of difficulty mathematically describing the importance of $\left\lfloor \frac{n^2}{4} \right\rfloor$ , though I am convinced it works.","['graph-theory', 'coloring', 'bipartite-graphs', 'discrete-mathematics']"
4685175,Show $\lim_{n \to \infty}|\cos nx|^{1/n}=1$ almost everywhere,"This is an old problem in a set of qualifier exams in analysis  which I have not been able to solve completely; hence my decision to post it here. The problem  have us prove that if $f$ is a measurable complex function on $\mathbb{R}$ of period $T>0$ such that $\int^T_0|f(x)|\,dx <\infty$ , then (1) $\lim_{n\rightarrow\infty}\frac{f(nx)}{n^2}=0$ almost everywhere. (2) As an application of (1), show that $\lim_{n\rightarrow\infty}|\cos nx|^{1/n}=1$ . Part (1) is not very complicated. I consider the series $s(x)=\sum^\infty_{n=1}\frac{|f(nx)|}{n^2}$ . Then $$\frac{1}{n^2}\int^T_0|f(nx)|\,dx=\frac{1}{n^3}\int^{nT}_0|f(x)|\,dx=\frac{1}{n^2}\int^T_0|f(x)|\,dx$$ As a consequence, $\int^T_0 s(x)\,dx<\infty$ . From this it follows that the series $s(x)$ converges almost everywhere and (1) follows. Part (2) is where I am struggling, for $f_n(x)=|\cos nx|^{1/n}$ is not quite of the form $n^{-2} g(nx)$ for some periodic function. I would appreciate any hint to finish part (2) off.","['integration', 'lebesgue-integral', 'real-analysis']"
4685200,Determine all numbers n satisfying the given ratio,"Let $n$ be an even natural number. We divide the numbers $1, 2,\dots , n^2$ into two equal sets $A$ and $B$ (that is $|A|=|B|=n^2/2$ ), such that each of the $n^2$ numbers is exactly in one of the sets. Let $S_A$ and $S_B$ be the sum of all elements in $A$ and $B$ . Determine all numbers $n$ so that there is a division satisfying $$\frac{S_A}{S_B}=\frac{39}{64}.$$ I found the value of $S_B$ . $$
\begin{aligned}
&\frac{S_A}{S_B}=\frac{39}{64}\\
&\begin{aligned}
\frac{S_A+S_B}{S_B} & =\frac{39+64}{64} \\
\frac{\frac{\eta^2\left(n^2+1\right)}{2}}{S_B} & =\frac{103}{64} \\
S_B & =64 \cdot \frac{n^2\left(n^2+1\right)}{103 \cdot 2} \\
& =32 \cdot \frac{n^2\left(n^2+1\right)}{103}
\end{aligned}
\end{aligned}
$$ Now this value should be smaller than or equal to sum of last n terms and greater than sum of first n terms. $$
\begin{aligned}
& \text { Sum of first in terms } \leq S_B \leqslant \text { sum of last ' } n \text { ' } \\
& \text { terms } \\
& \frac{n^2/2(n^2/2+1)}{2} \leq \frac{32}{103} n^2\left(n^2+1\right) \leq \\
& \frac{n^2\left(n^2+1\right)}{2}-\frac{n^2/2(n^2/2+1)}{2}
\end{aligned}
$$ I am not able to solve this inequality.Also I am not sure if this is correct direction. Because if I use geogebra then above inequality must have solution in between (0,3).","['elementary-set-theory', 'contest-math', 'sequences-and-series']"
4685255,Prove that either $m(E)=0$ or $m(\mathbb{R}\setminus E)=0$.,"$\textbf{Problem :}$ Let $E\subset\mathbb{R}$ be a Lebesgue measurable set such that $m(E\setminus(E+x))=0$ for all $x\in\mathbb{R}$ ; where $E+x:=\{e+x:e\in E\}$ . Prove that either $m(E)=0$ or $m(\mathbb{R}\setminus E)=0$ . I have been able to prove it partially. $\textbf{My Attempt :}$ Since $E$ is Lebesgue measurable, by definition, we have that, for any $A\subset\mathbb{R}$ , $$ m^{\ast}(E)=m^{\ast}(E\cap A)+m^{\ast}(E\cap A^c). $$ In particular, we have that $m(E)=m(E\cap (E+x))+m(E\cap (E+x)^c)$ for all $x\in\mathbb{R}$ . Since $m(E\cap (E+x)^c)=m(E\setminus (E+x))=0$ , we conclude that $m(E)=m(E\cap (E+x))$ for $x\in\mathbb{R}$ . Now notice that, for any $x\in\mathbb{R}$ , $$m(E\cap (E+x))=\int_{\mathbb{R}}\chi_{E\cap (E+x)}(y)dy=\int_{\mathbb{R}}\chi_{E}(y)\cdot\chi_{(E+x)}(y)dy=\int_{\mathbb{R}}\chi_{E}(y)\cdot\chi_{E}(y-x)dy.$$ Hence, we have that, for any $x\in\mathbb{R}$ , $$m(E\cap (E+x))=\int_{\mathbb{R}}\chi_{E}(y)\cdot\chi_{-E}(x-y)dy=(\chi_E\ast\chi_{-E})(x).$$ Since $m(E)=m(E\cap (E+x))$ for all $x\in\mathbb{R}$ , we conclude that $$(\chi_E\ast\chi_{-E})(x)=m(E)\text{ for all }x\in\mathbb{R} $$ Now there are only two possibilities regarding the Lebesgue measure of $E$ : either $m(E)<\infty$ or $m(E)=\infty$ . Suppose that $m(E)<\infty$ . This implies that $m(-E)<\infty$ . Therefore, we get that $\chi_{E}\in L^2(\mathbb{R})$ and $\chi_{-E}\in L^2(\mathbb{R})$ . So, by a basic result in convolution, we conclude that $\chi_E\ast\chi_{-E}\in C_0(\mathbb{R})$ . This means that we can make the function $\chi_E\ast\chi_{-E}$ , which is the constant function $m(E)$ , as arbitrary small as we would like outside a compact interval. This implies that $m(E)=0$ . Now I don't know how to proceed further when we have $m(E)=\infty$ . Please give me some hint. Thanks in advance.","['measure-theory', 'lebesgue-measure', 'convolution', 'real-analysis']"
4685293,Prove $\int_0^{\pi/4} \frac{x \cos 2x}{1-\sin x} \mathrm dx > \frac{1}{4}$,"How can I prove this inequality : $$\int_0^{\frac{\pi}{4}} \frac{x \cos(2x)}{1-\sin(x)} \mathrm{d}x > \frac{1}{4}.$$ It was given by my teacher as a challenge but I can't figure out that how to solve it. For indefinite integral, Usual substitution fails and the product $x\cos(2x)$ hints integration by part , but it made it only uglier and hence unsuccessful. Please give a hint or an insightful answer through which it can be easily proven. Thanks !","['integration', 'calculus', 'definite-integrals', 'inequality']"
4685355,Are $L^2$-closed subspaces of $C^\infty$ finite-dimensional?,"I have the following question:
Given a closed Riemannian manifold $M$ , let $V\subseteq C^\infty(M)$ be a linear subspace that is closed with respect to the $L^2$ -norm.
The question now is whether $V$ is finite-dimensional. My idea is to use Rellich's embedding theorem together with the open mapping theorem: Denote by $\iota : H^m(M) \rightarrow L^2(M)$ , $m \in \mathbb{N}$ the canonical inclusion, by Rellich this is a compact injective map. The restriction of $\iota$ to $V$ gives a bijection $\iota\lvert_V: (V, \lvert\lvert\cdot\rvert \rvert_{H^m})\rightarrow (V,  \lvert\lvert\cdot\rvert \rvert_{L^2})$ . If I now can show that $V$ is also closed with respect to the $H^m$ -norm, I can use the open mapping theorem to get that the inverse $(\iota\lvert_V)^{-1}:(V,  \lvert\lvert\cdot\rvert \rvert_{L^2})\rightarrow (V, \lvert\lvert\cdot\rvert \rvert_{H^m})$ is continous. In this case, the identity of $V$ factors through $H^m$ , i.e. $id_V = \iota\lvert_V \circ (\iota\lvert_V)^{-1}$ and hence it is a compact map. This then implies that $V$ is finite-dimensional. The missing puzzle piece is therefore to show that $V$ is closed as a subspace of $H^m$ (doesn't matter for which $m$ ). I'm grateful for any hint on how to prove this, less grateful for a counterexample, but at least then I can stop thinking about this ;) Thanks!","['functional-analysis', 'global-analysis', 'partial-differential-equations']"
4685364,"Is there a book on mathematical logic with a similar style to mathematician Terence Tao's book ""Analysis 1""?","I am looking for a mathematical logic textbook that I can study on my own. However, I am not very proficient in English. To give you an idea of my English level, I can read children's books such as ""Matilda"" and math textbooks in English. In particular, I really like Terence Tao's book ""Analysis 1"". This book provides very detailed explanations of proofs. However, the problem is that books with a lot of everyday English language are difficult for me to read. This includes books in the philosophy category. I found that books with titles like ""A Friendly Introduction to Logic"" were actually more difficult for me to read. The more explanations there were, the more difficult it was for me as a non-native English speaker. So, I was wondering if there are any mathematical logic textbooks that are easy for me to read? I've searched Amazon and Google extensively, but I haven't found a book that's perfect for me. I considered purchasing Enderton's ""Mathematical Introduction to Logic,"" which seems to be highly recommended, but the proofs aren't detailed enough. I'm hoping to find a book that's perfect for me, but since Terence Tao's book was so good, I'm looking for something similar. Thank you very much for your help.","['first-order-logic', 'logic', 'book-recommendation', 'reference-request', 'real-analysis']"
4685408,Sign of a function near the boundary,"Let $\Omega \subset \mathbb R^N$ , $N \geq 2$ , be a smooth bounded open set. Let $f \in C^1(\overline \Omega)$ , and suppose that $$
f = 0, \quad \frac{\partial f}{\partial \nu} < 0 \qquad \text{on } \quad \partial \Omega,
$$ where $\nu$ is the outer unit normal vector to $\partial \Omega$ . It is very intuitive that $f > 0$ in an $\varepsilon$ -neighborhood of $\partial \Omega$ , that is, $f > 0$ in the set $$
A_\varepsilon := \{x \in \Omega \ : \ d(x, \partial \Omega) < \varepsilon\}
$$ for some $\varepsilon > 0$ . Nonetheless, I am failing to obtain a proof with sufficient details to really convince me. My attempt begins with taking a neighborhood of each point $x \in \partial \Omega$ where $f > 0$ and then using the compactness of $\overline \Omega$ to find the neighborhood I want. The compactness part is OK, but the beginning is kind of tricky to me. Any suggestions on how to start? Thanks in advance.","['multivariable-calculus', 'real-analysis']"
4685444,Is there a norm for $\nabla$ operator? What is the meaning of $\sqrt{\Delta}$? [duplicate],"This question already has answers here : Square root of a Hermitian operator exists (4 answers) Intuitive understanding of the operator norm? (1 answer) What's the difference between the operator norm and the sup norm (1 answer) Closed last year . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Considering the del operator in cartesian coordinate. $$\nabla=\left(\dfrac{\partial}{\partial x_1},...,\dfrac{\partial}{\partial x_n}\right)$$ Since technically it's a vector, does its norm have any meaning? I considered is ""norm"" as the square root of the Laplacian: $$\left\Vert\nabla\right\Vert_{\mathbb{R}^n}=\sqrt{\Delta}=\sqrt{\dfrac{\partial^2}{\partial x_1^2}+...+\dfrac{\partial^2}{\partial x_n^2}}$$ Is it correct to think of this operator as ""the operator that applied twice behaves like the Laplacian""? If this is wrong, it is possible to make the norm of a vector of operators? If yes, what are the properties of this norm and what normed space are we in? In this case I considered $\mathbb{R}^n$ and technically this is not a norm since the result is not a non-negative number, so I was curious to know: If anyway this operator makes sense (in case what is the meaning). If there is a space that can associate a numeric value to the norm of this operator (even if the norm was infinite causing the operator to be unbounded). If this space exists, does the result depend on the coordinate system i'm using and the length of the vector?","['normed-spaces', 'laplacian', 'functional-analysis', 'vector-analysis', 'derivatives']"
4685449,"Is it possible to partition interval [0,1] into two closed sets with empty interior?","Consider the Eucidean topology over $\mathbb{R}$ . Consider the unit interval [0,1]. I want to partition this interval into sets $A$ and $B$ such that $A \cup B =[0,1]$ Both $A$ and $B$ are closed and both $A$ and $B$ have an empty interior. It might be the case that such a construction is not possible. In that case, I am interested to know if a closed set with non-empty interior cannot be partitioned into closed sets with empty interior? For the case of unit interval, I was considering fat Cantor set ( https://en.wikipedia.org/wiki/Smith%E2%80%93Volterra%E2%80%93Cantor_set ) as a possible candidate for set $A$ . If set $B$ is the closure of complement of $A$ , then $B$ ends up having a non-empty interior.",['general-topology']
4685451,Probability of product of $5$ digits being divisible by $5$ or $7$.,"Seven digit numbers are formed using digits $1,2,3,4,5,6,7,8,9$ without repetition. The probability of selecting a number such that product of any $5$ consecutive digits is divisible by either $5$ or $7$ is $P$ . Then $12P$ is equal to: PS: I know this question has been asked and answered over here as well as on this site.Their answer is $7$ but I've gotten $8$ . I am unable to identify the error in my solution. Any corrections or full solutions would be appreciated First I tried to define $P'$ $P'= 1 - P$ $P'$ = probability of selecting a number such that atleast one of the consecutive $5$ digits aren't divisible by $5$ or $7$ or both. We try to find $P'-$ Counting of Favorable Cases: Case $1$ : Number has no $5$ or $7$ is $7!$ Case $2$ : Number has either $5$ or $7$ . In this case the number $5$ / $7$ can occupy positions $1,2,6$ and $7$ so that atleast one set of consecutive digits will not be divisible by $5$ or $7$ . Hence $4\choose1$ $\times$ $7\choose6$ $\times$ $6!$ $\times$ $2$ Case $3$ :Number has both $5$ and $7$ . In this case $5$ and $7$ occupy either positions $[1,2],[6,7]$ or $[1,7]$ . So $3\choose1$ $\times$ $2!$ $\times$ $7\choose 5$ $\times$ $5!$ Favorable Cases = $7!$ $\times$ $(1 + 8 + 3)$ Total Cases = $9 \choose 7$ $\times$ $7!$ $P'= \frac{1}{3}$ $\therefore P = \frac{2}{3}$ , Hence $\boxed{12P = 8}$","['contest-math', 'probability']"
4685465,"Is my solution a typical solution? Any better solution? Exercise 8 on p.46 in Exercises 2C in ""Measure, Integration & Real Analysis"" by Sheldon Axler.","I am reading ""Measure, Integration & Real Analysis"" by Sheldon Axler. The following exercise is Exercise 8 on p.46 in Exercises 2C in this book. Exercise 8 Give an example of a set $X$ , a $\sigma$ -algebra $\mathcal{S}$ of subsets of $X$ , a set $\mathcal{A}$ of subsets of $X$ such that the smallest $\sigma$ -algebra on $X$ containing $\mathcal{A}$ is $\mathcal{S}$ , and two measures $\mu$ and $\nu$ on $(X,\mathcal{S})$ such that $\mu(A)=\nu(A)$ for all $A\in\mathcal{A}$ and $\mu(X)=\nu(X)<\infty,$ but $\mu\neq\nu.$ I solved this exercise. I guess my solution is ok. Is my solution a typical solution for this exercise? Any better solution? My solution is here: Let $X:=\{1,2,3,4\}.$ Let $\mathcal{A}:=\{\{1,2\},\{1,3\}\}.$ Suppose $\mathcal{S}$ is the smallest $\sigma$ -algebra on $X$ cotaining $\mathcal{A}.$ $\{1,2,3\}=\{1,2\}\cup\{1,3\}\in\mathcal{S}.$ $\{4\}=X\setminus\{1,2,3\}\in\mathcal{S}.$ $\{1,2,4\}=\{1,2\}\cup\{4\}\in\mathcal{S}.$ $\{1,3,4\}=\{1,3\}\cup\{4\}\in\mathcal{S}.$ $\{3\}=X\setminus\{1,2,4\}\in\mathcal{S}.$ $\{2\}=X\setminus\{1,3,4\}\in\mathcal{S}.$ $\{2,3,4\}=\{2\}\cup\{3\}\cup\{4\}\in\mathcal{S}.$ $\{1\}=X\setminus\{2,3,4\}\in\mathcal{S}.$ $\therefore\mathcal{S}=2^X.$ Let $x,y\in [0,\infty)$ and $x\neq y.$ Let $v:X\to [0,\infty]$ be the function such that $v(1)=x,v(2)=y,v(3)=y,v(4)=x.$ Let $w:X\to [0,\infty]$ be the function such that $w(1)=y,w(2)=x,w(3)=x,w(4)=y.$ Let $\mu:\mathcal{S}\to [0,\infty]$ be the function such that $\mu(E)=\sum_{x\in E} v(x)$ for $E\in\mathcal{S}.$ Let $\nu:\mathcal{S}\to [0,\infty]$ be the function such that $\nu(E)=\sum_{x\in E} w(x)$ for $E\in\mathcal{S}.$ Then, $\mu(\{1,2\})=v(1)+v(2)=x+y=y+x=w(1)+w(2)=\nu(\{1,2\}).$ Then, $\mu(\{1,3\})=v(1)+v(3)=x+y=y+x=w(1)+w(3)=\nu(\{1,3\}).$ Then, $\mu(X)=v(1)+v(2)+v(3)+v(4)=2x+2y=w(1)+w(2)+w(3)+w(4)=\nu(X)<\infty.$ Then, $\mu(\{1,4\})=v(1)+v(4)=2x\neq 2y=w(1)+w(4)=\nu(\{1,4\}).$ So, $\mu\neq\nu.$","['measure-theory', 'solution-verification', 'real-analysis']"
4685488,Simplify $\frac{\frac{x+1}{2x}}{\frac{x^2-1}{x}}$,"$\color{white}{\require{cancel}{3}}$ So recently I have been doing math to see if I could still do simple math, mainly focusing on Algebra. So, I decided to see if I was able to simplify $$\frac{\frac{x+1}{2x}}{\frac{x^2-1}{x}}$$ The thing is, I am a little iffy on the answer that I got for it, so I want to verify that my solution is correct. Here is how I got my answer: $$\frac{\frac{x+1}{2x}}{\frac{x^2-1}{x}}$$ $$\iff\frac{(x+1)(x)}{(2x)(x^2-1)}$$ $$\implies\frac{\cancel{x}(x+1)}{\cancel{x}(2x^2-2)}$$ $$\iff\frac{x+1}{2x^2-2}$$ $$\iff\frac{x+1}{2(x^2-1)}$$ $$\iff\frac{x+1}{x^2-1}\cdot\frac{1}{2}$$ $$\implies\frac{\cancel{x+1}}{\cancel{(x+1)}(x-1)}\cdot\frac{1}{2}$$ $$\iff\frac{1}{x-1}\cdot\frac{1}{2}$$ $$\iff\frac{1}{2(x-1)},\text{ }x\neq-1,0,1$$ My question Is my solution correct, or what could I do to attain the correct solution, or if it is correct, what could I do attain the correct solution more easily? To clarify Sorry if this question seems trivial/short This is not a duplicate of any of my other questions Sorry if the ""algebra-precalculus"" tag seems a little out of place, I mean I guess it sort of fits but still.","['algebra-precalculus', 'solution-verification']"
