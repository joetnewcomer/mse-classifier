question_id,title,body,tags
1811271,"Function that turns GCD and LCM into intersections and unions?: $f(a)\cap f(b)=f(\gcd(a,b))$, $f(a)\cup f(b)=f(\operatorname{lcm}(a,b))$","Is there a function $f:\Bbb N_+\to\cal P(\Bbb N_+)$ such that: $f(a)\cap f(b)=f(\gcd(a,b))$, $f(a)\cup f(b)=f(\operatorname{lcm}(a,b))$, $a\in f(a)$, and $f$ is injective? Without the third condition, the function that maps a number to the set of its prime-power divisors works. Without the fourth, the trivial map $x\mapsto\Bbb N$ works. From either the first or the second condition, we get $f(a)\subseteq f(ab)$. Combining with the third, we get that $f(a)$ contains the set of $a$'s divisors. I was also able to show that $6\in f(2)$ iff $6\in f(10)$. But I really have no idea how to go from here, or even if such a function exists.","['number-theory', 'least-common-multiple', 'gcd-and-lcm', 'divisibility']"
1811325,Spivak's Calculus?,"I have seen many users here asking questions about problems in what they call ""Spivak's Calculus Book"". I have never seen the book, and information online is scarce. From what I've gathered, it is just a more rigorous calculus 1-3 book with harder problems. I have already taken calculus, and I am about to take analysis. Is there any point for me to buy Spivak's book, or is reading analysis books better at this point? Is this calculus book better than a standard analysis book? Why don't students who are gifted enough just start off with an analysis book instead of Spivak's calculus book?","['reference-request', 'real-analysis', 'soft-question', 'calculus']"
1811331,Is the sum of the first $n$ primes a prime infinitely many times?,"Define the sequence $P(n)=\sum_{i=1}^{n}p_i$, where $p_i$ is the $i$-th prime number. I observed for some small $n$ that sometimes this sum evaluates to a prime number, for example $P(2)=2+3=5$ and $P(4)=2+3+5+7=17$ and $P(6)=2+3+5+7+11+13=41$. So it is natural to ask: Is it true that there is a sequence of natural numbers $\{n_i:i \in \mathbb N\}$ such that all numbers in the set $\{P(n_i):i \in \mathbb N\}$ are prime numbers?","['number-theory', 'prime-numbers']"
1811332,General form of function given that it is differentiable,Suppose a function satisfies the relation $f(x+y)=f(x)+f(y)$. How can I show that the more general form of $f(x)$ is $f(x)=kx$ where $k$ is a constant given that $f$ is differentiable?,"['functions', 'derivatives', 'calculus', 'analysis']"
1811357,$|X|=|X\cup\{a\}|?$,Let $X$ be an infinite set and $a\notin X$. Prove $$|X|=|X\cup\{a\}|$$ This is so intuitively obvious but upon inspection it appears quite non-obvious. How might one prove this? Do I need the axiom of choice?,"['cardinals', 'elementary-set-theory']"
1811360,A challenging straightedge and compass construction,"Three points $A,O,B$ are given, and $0<\theta=\widehat{AOB}<\frac{\pi}{3}$. It is known that there are two points $A',B'$ on the segments $OA,OB$ such that 
  $$ BB'=B'A'=A'A $$
  holds. How to find them with straightedge and compass? The problem is straightforward to solve through trigonometry: if we set $$OA=A,\;OB=b,\;\cos\theta=c,\; AA'=x$$
it boils down to solving the second-degree equation:
$$ (a-x)^2+(b-x)^2 - 2(a-x)(b-x)c = x^2, $$
but I wasn't able to find an elegant solution through straightedge and compass only.","['euclidean-geometry', 'geometry']"
1811373,the defining polynomials of $PGL_n$ as an affine algebraic group,"I have read this question . Also, there are theorems telling me that $PGL_n$, as the quotient of $GL_n$ by its center, is with no doubt an affine variety (affine algebraic group). But, is it true that every finite dimensional affine variety can be viewed as a closed subset of an affine $m$-space $\mathbb A^m$?  Can $PGL_n$ be viewed as a closed subset of $\mathbb A^m$ for $m \geq \dim PGL_n$? If this is true, then what are the defining polynomials of $PGL_n$ in $k[x_1,x_2,\cdots,x_m]$? Thanks to everyone.","['algebraic-groups', 'algebraic-geometry']"
1811384,How to find $L = \int_0^1 \frac{dx}{1+{x^8}}$,Let $L = \displaystyle \int_0^1 \frac{dx}{1+{x^8}}$ . Then $L < 1$ $L > 1$ $L < \frac{\pi}{4}$ $L > \frac{\pi}{4}$ I got some idea from this video link . But got stuck while evaluating the second integral. Please help!! Thanks in advance!,"['residue-calculus', 'calculus', 'complex-analysis', 'integration', 'contour-integration']"
1811421,Why are anti-diagonal / persymmetric matrices not as important as diagonal / symmetric matrices?,"Diagonal matrices and diagonalizability are key topics in linear algebra as well as numerical linear algebra. Likewise, symmetric matrices have lots of nice properties that make them widely studied and important both theoretically and computationally. However, anti-diagonal matrices seem to be no more than a curiosity in matrix algebra. While symmetry along the main diagonal seems to count for so much, persymmetry does not seem to count for very much at all. Is there a reason for this? After all (and this might sound naive) why should one diagonal (left to right) matter so much more than the other one? Is this an artifact / convention arising from the development of matrix algebra or does it reflect something deeper. Or, are anti-diagonal and per-symmetric matrices of far greater importance than I think? I was thinking about this and was not really able to come up with anything close to a satisfactory answer.","['matrices', 'linear-algebra']"
1811433,Let $M=\big( \begin{smallmatrix} A & B \\ C & D \end{smallmatrix} \big)$. Prove $\det(M)=\det(A)\cdot \det(D-C·A^{-1}·B)$.,"Let  (shown in matrix blocks) $M=\big( \begin{smallmatrix} A & B \\ C & D \end{smallmatrix} \big)$ a square matrix such that $A$ is invertible and $D$ is a square matrix. I have to prove that $$\det(M)=\det(A)\cdot \det(D-C·A^{-1}·B)$$ I also have an indication to use it: Consider previously these cases:
\begin{align*}
A_1 &= \begin{pmatrix} A & 0 \\ 0 & I \end{pmatrix},\\
A_2 &= \begin{pmatrix} I & 0 \\ 0 & D \end{pmatrix},\\
A_3 &= A_1·A_2,\\
A_4 &= \begin{pmatrix} I & A^{-1}·B \\ 0 & I \end{pmatrix},\text{ and}\\
A_5 &= A_3\cdot A_4.
\end{align*} The problem is difficult in general for me and I don't even know how to use the indication.","['matrices', 'determinant']"
1811443,is there any convergent sub-sequence of a sequence of all rational numbers?,"Let $(a_n)$ be a sequence of rational numbers, where all rational numbers are terms . ( i.e. enumeration of rational numbers ) Then, is there any convergent sub-sequence of  $(a_n)$?","['rational-numbers', 'sequences-and-series', 'convergence-divergence']"
1811490,"Family of definite integrals involving Dedekind eta function of a complex argument, $\int_0^{\infty} \eta^k(ix)dx$","The Dedekind eta function is denoted by $\eta(\tau)$, and is defined on the upper half-plane ($\Im \tau >0$). Put $\tau = i x$ where $x$ is 
a positive real number. The function has the following representations: 
$$\eta(ix)= e^{-\pi x/12} \prod_{n=1}^{\infty} (1-e^{-2\pi x n})
\\=\frac{2}{\sqrt{3}}\sum_{n=0}^{\infty} \cos\left(\frac{\pi}{6}(2n+1)\right)e^{-\pi x/12 \, (2n+1)^2}=\sum_{n \in \mathbb{Z}} (-1)^n e^{-\pi x/12 \,(6n+1)^2}.\tag{1}$$
It is not difficult to observe that when $x>0$, $\eta(i x)$ is a real number, and goes to $0$ when $x$ goes to infinity. It also satisfies the functional equation $$\eta\left(\frac{i}{x}\right)=\sqrt{x}\,\eta(i x)\tag{2}$$
Moreover, the Jacobi triple product identity implies that 
$$\eta^3(ix)=\sum_{n=0}^{\infty} (-1)^n (2n+1) e^{-\pi x(n+\frac12)^2}\\=\frac12\vartheta_1'(e^{-\pi x})=\frac12 \vartheta_2(e^{-\pi x})\vartheta_3(e^{-\pi x})\vartheta_4(e^{-\pi x})\tag{3}$$
Where $\vartheta_k$ are the Jacobi theta functions. Now define
$$\large I(k)=\int_0^{\infty} \eta^k(ix)dx.\tag{4}$$
In his paper Some Integrals of the Dedekind Eta-function (2008) ( arxiv link ), Glasser shows
that $\displaystyle I(1)=\frac{2 \pi}{\sqrt{3}},I(3)=1$ (these follow directly from the above series represantations), and also gives the Laplace transform $$\int_0^{\infty} e^{-x y} \eta^3(i x)dx=\operatorname{sech}\sqrt{\pi y}\tag{5}$$
from which I can deduce (by setting $y=\pi(n+1/2)^2$, multiplying by $(-1)^n(2n+1)$ and summing) that 
$$I(6)=\int_0^{\infty} \eta^6(ix)dx=\sum_{n=0}^{\infty} \frac{(-1)^n (2n+1)}{\cosh\left(\pi (n+\frac12)\right)}
\\=\frac12 \vartheta_2^2(e^{-\pi})\vartheta_4^2(e^{-\pi})=\frac{\pi}{4 \Gamma\left(\frac34\right)^4}.\tag{6}$$
By the way, note that the closed form for $I(1)$ is this question of @VladimirReshetnikov in disguise. A numerical computation suggests that we also have $$I(4)\stackrel?=\frac{2^{2/3} \pi}{3 \Gamma\left(\frac23\right)^3}.\tag{7}$$ $$I(8)\stackrel?=\frac23 \left(\frac{2^{2/3} \pi}{3 \Gamma\left(\frac23\right)^3}\right)^3.\tag{8}$$ Using the same procedure I did to evaluate $I(6)$ on another Laplace transform given by Glasser, 
$$\int_0^{\infty} e^{-x y} \eta(i x)dx=\sqrt{\frac{\pi}{y}} \frac{\sinh 2\sqrt{\pi y/3}}{\cosh\sqrt{3 \pi y}},\tag{9}$$
I obtain $$I(4)=\int_0^{\infty} \eta^4(ix)dx=2 \sum_{n=0}^{\infty} (-1)^n \frac{\sinh\frac{\pi}{\sqrt{3}}(2n+1)}{\cosh\frac{\sqrt{3}\pi}{2}(2n+1)}\tag{10}$$
but I could not evaluate this sum in terms of elliptic functions as I did with $I(6)$. The formula for $I(8)$ is even more intriguing, as I cannot even see a reasonable route to start proving it. Q1: Can we find a closed form expression for $I(n)$, at least for small integer $n$? Q2: What is the closed form of $I(12)$?
  Note that we have $$I(12)=\int_0^{\infty} \eta^{12}(ix)dx=\frac1{16} \int_0^{\infty}\vartheta_2^4(e^{-\pi x})\vartheta_3^4(e^{-\pi x})\vartheta_4^4(e^{-\pi x})dx=0.07552061383997469\dots$$
  Based on the other results so far, I believe the closed form may involve the Gamma function. Bonus Q: What would be a way to prove the conjectural closed form for $I(4)$ or $I(8)$? This is interesting to me because as can be seen in this post, a closed form for $I(12)$ may be used to find the closed form
of the integral $\int_0^{\infty} \vartheta_4^{12}(e^{-\pi x})/(1+x^2) dx$. Another interesting aplication of these integrals is giving closed forms for beautiful lattice sums . For instance, by expanding $\eta$ into its series representation, equation $(7)$ can be rewritten
$$\sum_{(a,b,c,d)\in \mathbb{Z}^4} \dfrac{(-1)^{a+b+c+d}}{(6a+1)^2+(6b+1)^2+(6c+1)^2+(6d+1)^2} = \frac{\pi^2}{18 \sqrt[3]{2}\,\Gamma\left(\frac23\right)^3},\tag{11}$$
which I deem visually pleasing. Similarly, equation $(6)$ can be rewritten 
$$\sum_{(a,b,c)\in \mathbb{Z}^3} (-1)^{a+b+c} \operatorname{sech}\left(\frac{\pi}{2\sqrt{3}} \sqrt{(6a+1)^2+(6b+1)^2+(6c+1)^2}\right) = \frac{\pi}{4 \Gamma\left(\frac34\right)^4}.\tag{12}$$ UPDATE. Paramanand Singh was able to prove the closed form for $I(4)$. Using the substitution suggested by him, we obtain new representations for $I(8)$ and $I(12)$: 
$$I(8) = \frac{2^{1/3}}{\pi^3} \int_0^1 x^{1/3}\,(1-x^2)^{-1/3}\,K(x)^2 \,dx \stackrel?= \frac23 \left(\frac{2^{2/3} \pi}{3 \Gamma\left(\frac23\right)^3}\right)^3 \tag{13}$$
$$I(12) = \frac{2}{\pi^5} \int_0^1 x\,K(x)^4\,dx = \quad ? \tag{14}$$
Here $K$ is the complete elliptic integral of the first kind. Furthermore, in view of the definitions of Ramanujan's tau function and the Tau Dirichlet series , it follows that 
$$I(24) = \int_0^{\infty} x^{10} \eta^{24}(i x) dx = \frac{10!}{(2\pi)^{11}} \sum_{n=1}^{\infty} \frac{\tau(n)}{n^{11}}. \tag{15}$$
I am not sure about that one. ONE MORE UPDATE With some educated guesses and Mathematica's 'RootApproximant', I have finally found a possible closed form for $I(2)$ !!!
$$I(2)=\int_0^{\infty}  \eta^2(i x) dx \stackrel?= \log\left(1+ \sqrt{3}+\sqrt{3+2 \sqrt{3}}\right) \tag{16}$$
This one holds for at least 100 digits. This is surprising to me, as it's quite different from the other ones so far. This also gives me hope that a closed form for $I(5),I(7)$ etc. exists.","['theta-functions', 'closed-form', 'integration', 'definite-integrals', 'special-functions']"
1811512,A question about Chebyshev's inequality and why $\operatorname{Var}(\overline X_n) = \sigma^2/n.$,"This is the question: For i.i.d. r.v.s $X_1, . . . ,X_n$ with mean $μ$ and variance $\sigma^2$, give a value of $n$ (as a
  specific number) that will ensure that there is at least a $99\%$ chance that the
  sample mean will be within $2$ standard deviations of the true mean $μ$. This is the solution: We have to find $n$ such that $$P(|\overline{X}_n-\mu|>2\mu) \leq 0.01$$ By Chebyshev's inequality (in the form $P(|Y-EY|>c) \leq \frac{\text{Var}(Y)}{c^2}$), we have $$P(|\overline{X}_n-\mu|>2\mu) \leq \frac{\text{Var}(\overline{X}_n)}{(2\sigma)^2} = \frac{\frac{\sigma^2}{n}}{4\sigma^2}=\frac{1}{4n}$$ So the desired inequality holds if $n \geq 25$. So what I'm confused about is why $\text{Var}(\overline{X}_n)=\frac{\sigma^2}{n}$. What does it mean to take the variance of the sample mean? I did some research and it seems like sample variance (is this even called sample variance?) should be divided $n-1$ and not $n$ as seen in the solution. Unfortunately the course I'm taking has not gone over any of this. I'm hoping someone could point me in the right direction or even providing the necessary terms to search to understand this more.","['statistics', 'probability']"
1811518,"Ultrafilter on $[0,1]$ consisting of closed sets","Today we learned about filters and ultrafilters in the General Topology course. I am trying to play around with these definitions. I wish to ask a question that I am unsure about. Let us say, we have an ultrafilter $\mathcal{F}$ on the closed interval $[0, 1]$. Construct $\overline{\mathcal{F}} = \{ \overline{A} : A \in \mathcal{F}\}$, where the overline denotes closure of a set. My question is, is $\overline{\mathcal{F}}$ an ultrafilter? Is it a filter at all? The intersection property is clear, but it is the superset property of filters that is confusing me. Thanks for any help!","['general-topology', 'reference-request', 'real-analysis', 'filters']"
1811530,How to show that $f(x) = x^2$ is continuous using topological definition?,"I am trying to show that simple continuous functions satisfy
  topological definition of continuity Recall given $(X, \mathcal{T}), (Y, \mathcal{J}), f$ is continuous if $f^{-1}(V) \in \mathcal{T}, \forall V \in \mathcal{J}$ Then given $f:\mathbb{R} \to \mathbb{R}$ equipped with the usual topology $\mathcal{T}$, we wish to show that $f(x) = x^2$ is continuous $\Leftrightarrow$ show that $f^{-1}(V)$ is open  $\forall V \in \mathcal{T}$ Attempt : Given $f: \mathbb{R} \to \mathbb{R}, x \mapsto x^2$ By $\epsilon-\delta$ definition of continuity, we know that $\forall x \in \mathbb{R}, \forall \epsilon > 0, \exists \delta > 0$, such that $\forall x_o \in \mathbb{R}$ whenever $x \in \mathcal{B}_{\delta}(x_o)  \implies f(x) \in \mathcal{B}_{\epsilon}(f(x_o))$ Then given $x_o \in \mathbb{R}, \epsilon >0$, let $V = \mathcal{B}_{\epsilon}(f(x_o))$. Then $x \in f^{-1}(V) = f^{-1}(\mathcal{B}_{\epsilon}(f(x_o)))$ However, since $f$ satisfies $\epsilon-\delta$ version of continuity, $\exists \delta >0$ such that $x \in \mathcal{B}_{\delta}(x_o) \subseteq f^{-1}(V) = f^{-1}(\mathcal{B}_{\epsilon}(f(x_o)))$. This shows $f^{-1}(V)$ is open by definition of open set in $\mathbb{R}$. End of proof. can anyone check if this is correct? my main concern is that not all $V$ is of the form $\mathcal{B}_{\epsilon}(f(x_o))$...","['proof-verification', 'continuity', 'epsilon-delta', 'proof-writing', 'general-topology']"
1811541,Understanding divison by monic polynomial in $R[x]$ where $R$ is an arbitrary ring,"I read ""Algebra: Chapter 0"" by P.Aluffi. I encountered a topic where it says you can divide any polynomial in $R[x]$ ( $R$ is any ring) by a monic polynomial(that is, a polynomial of the form $x^d + \sum\limits_{i=0}^{d-1} a_i x^{i}$ ). It says if $g(x)$ is any polynomial and $f(x)$ is a monic polynomial. Then $\exists \ \ \ q(x), r(x) \in R[x],\: \deg r(x) < \deg f(x): g(x) = f(x)q(x) + r(x).$ I have two questions regarding this: 1) Why $f(x)$ needs to be monic? 2) How can we prove it? The book talk about induction and that if $\deg  g(x) = n > d = deg \ f(x)$ , then $\forall a \in R$ we have $ax^n = ax^{n-d}f(x) + h(x)$ for some $h(x): \deg \ h(x) < n$ . It's true, but how do I take it from here?","['abstract-algebra', 'ring-theory', 'polynomials']"
1811545,Which is the explanation of the identity $\sum_{k=0}^n {n \choose k}k^2 = 2^{n-2}n(n+1)$?,"A friend of mine put me a problem some time ago, and after trying to do it, I finally surrendered. I looked for the answer online so maybe I could guess why is it like it is, but I just can't understand anything about it. Could someone explain, rather using logic, why is the identity as it is? $$\sum_{k=0}^n  {n \choose k}k^2 = 2^{n-2}n(n+1)$$","['combinations', 'combinatorics', 'summation']"
1811556,Jacobian matrix of the parametrization of (part of) a ball,"I read (in E. Sernesi, Geometria 2) that the function $\varphi:\left(-\frac{\pi}{2},\frac{\pi}{2}\right)\times\mathbb{R}^{n}\to\mathbb{R}^{n+1}$ defined by $$\varphi(\theta_1,\ldots,\theta_n,r)=\left(r\prod_{k=1}^n\cos\theta_k,r\sin\theta_n\prod_{k=1}^{n-1}\cos\theta_k, r\sin\theta_{n-1}\prod_{k=1}^{n-2}\cos\theta_k,\ldots ,r\sin\theta_1\right)$$has an invertible Jacobian matrix. 
How can it be proved?
I  have calculated such matrix as $$\begin{pmatrix}\prod_k\cos\theta_k & -r\sin\theta_1\prod_{k\ne 1}\cos\theta_k &\ldots& -r\sin\theta_n\prod_{k\ne n}\cos\theta_k\\\sin\theta_n\prod_{k<n}\cos\theta_k& -r\sin\theta_1\sin\theta_n\prod_{k<n,k\ne 1}\cos\theta_k&\ldots&-\cos\theta_n\prod_{k<n,k\ne n}\cos\theta_k\\\vdots& \vdots &\ddots&\vdots \\ \sin\theta_1&r\cos\theta_1 &\ldots&0\end{pmatrix}$$I have tried to use induction to prove it, and showed that it holds for $n=2$, and $n=1$, by calculating the determinant, but I do not know how to prove it for the general case. Using the determinant in the general case by using the Laplace expansion does not seem applicable to me... verifying the independence of the columns or rows may be an option, but I cannot do so. I thank any answerer very much!","['matrices', 'multivariable-calculus', 'linear-algebra', 'vector-analysis']"
1811576,"Why vectors in Linear Algebra start from point (0,0)?","I learn linear algebra in university and I was wondering why vectors in linear algebra always start from the point $(0,0)$? how many kinds of mathematical vectors out there? Is it legit to use other kind of vectors in linear algebra apart from that vectors that start at $(0,0)$?",['linear-algebra']
1811602,explicit upper bound of TREE(3),"TREE(3) is the famously absurdly large number that is the length of a longest list of rooted, 3-colored trees whose $i$th element has at most $i$ vertices, and for which no tree's vertices can be mapped to the vertices of a subsequent tree preserving color and inf relationships. Some lower bounds of TREE(3) have been proven. Among them are $A^{A(187196)}(1)$, where the superscript denotes function iteration and $A(x) = 2\uparrow^{x-1}x$ (using Knuth up-arrows ) is a version of the Ackermann function. More bounds appear in this Wikia article and here . These bounds are rather unsatisfying because there is no indication of how tight they are. The only upper bound of TREE(3) that I have seen (other than, trivially, TREE($n$) for $n > 3$) has a similar derivation as a longest sequence of a more general type of graph; I can't find a reference at the moment. This upper bound is also unsatisfying because it is non-constructive. Does anyone know of an explicit expression that is an upper bound of TREE(3)? Or is it so large that there is no hope to construct one?","['computability', 'combinatorics', 'graph-theory', 'big-numbers']"
1811612,"When five dice are rolled, what is the chance to get five 6's if you can roll the dice that do not show a 6 on the first roll once more?","We have $5$ normal dice. What is the chance to get five $6$'s if you can roll the dice that do not show a 6 one more time (if you do get a die with a $6$, you can leave it and roll the others one more time. Example: first roll $6$ $5$ $1$ $2$ $3$, we will roll $4$ dice and hope for four $6$s or if we get $6$ $6$ $2$ $3$ $3$ we will roll three dice one more time). I tried to calculate if you get $1$, $2$, $3$, $4$ dice with $6$ but I don't know how to ""sum"" the cases.",['probability']
1811632,How to decompose a 2d shape into sin and cosin modes?,"Assume that you have a circle with radius $r_0$, then you keep adding cosine modes as below: $r=r_0+a_1\cos(1\theta)+a_2\cos(2\theta)+a_3\cos(3\theta)+a_4\cos(4\theta)+~...$ if you plot this as below by matlab: r0=1;
a1=0.2;
a2=0.2;
a3=0.2;
a4=0.2;
th=0:0.01:2*pi;
r=r0+a1cos(th)+a2cos(2*th)+a3cos(3*th)+a4cos(4*th);
x=r*cos(th);
y=r*sin(th);
plot(x,y); You will see that you can get different shapes by changing the number of modes (i.e. here n=4) or coefficients (i.e. $a_i$) and even omitting some modes (i.e. $a_k=0$) you will get different shapes. My question is that how can you decompose for instance an oval or a square in cosine modes, and find their coefficients (somehow in the way of fourier decomposition), is it ever possible? But I think making any shape would be possible by my method mentioned above, but I am interested to know how I can come from an arbitrary shape to its corresponding cosine modes.","['circles', 'fourier-series', 'matlab', 'geometry', 'fast-fourier-transform']"
1811644,Why is that any function from $X$ to the trivial topological space is continuous?,"I think this is a super silly question, but I just can't figure out why is that given any function $f: X \to Y$, where $(X, \mathcal{T})$ is an arbitrary topological space, and $(Y, \mathcal{T}_{trivial})$ where $ \mathcal{T}_{trivial} = \{\varnothing, Y\}$ Okay, so $f^{-1}(\varnothing) = \varnothing \in \mathcal{T}$, but how do we know that $f^{-1}(Y) \in \mathcal{T}?$ Since the preimage of the codomain is not necessarily the domain When is the preimage of codomain not equal to domain? Why couldn't there be a case where $f^{-1}(Y) = U \subset X$, but $U \notin \mathcal{T}?$ Edit: So is it always the case that $f^{-1}(Y) = X$, given $f: X \to Y$?","['general-topology', 'functions']"
1811689,Cardinality of a closed uncountable set.,"I am aware that every closed uncountable subset of reals has cardinality of the continuum. It's easy to use this result to prove that the same is true in $\mathbb R ^n$, for each $n\in\omega$. Question. What are some generalizations of this result? That is, are there general properties that a separable metric space can have, which guarantee every one of its closed uncountable subset has size of the continuum?","['general-topology', 'metric-spaces']"
1811754,Is $\frac{d}{dx}\left(\sum_{n = 0}^\infty x^n\right) = \sum_{n = 0}^\infty\left(\frac{d}{dx} x^n \right)$ true?,"Almost 3 months ago, I asked this question regarding if it's possible to compute the summation of derivatives, as in the example I've given: $$\sum_{n = 0}^\infty \frac{d}{dx} x^n$$ One answer regarded the interchange between summations and derivatives, which got me thinking: does the interchange between the derivative and the summation succeed in this example? In other words, is $$\frac{d}{dx}\left(\sum_{n = 0}^\infty x^n\right) = \sum_{n = 0}^\infty\left(\frac{d}{dx} x^n \right)$$ true? I believe it is, because the summation of the derivatives of $x^n$ from $n = 0 \to \infty$ was: $$1 + 2x + 3x^2 + 4x^3 + 5x^4 + \cdot \cdot \cdot$$ and to evaluate the summation of a series, you take the derivative of each term , which gets me: $$\frac{d}{dx}\left(\sum_{n=0}^\infty x^n\right) = \frac{d}{dx}(1 + x^2 +x^3 + x^4 + x^5 + \cdot \cdot \cdot) = 1 + 2x + 3x^2 + 4x^3 + 5x^4 + \cdot \cdot \cdot $$ Hence, I believe that the interchange succeeds. Am I right? Does the interchange succeed? Notes I implemented the left hand side of the ""interchange equation"" into WolframAlpha, and I got back something ""useful"", but it doesn't really solve my problem. I found This question and this question , but they have nothing to do with my question. Multiple other questions deal with interchanges with summations and integrals. This is about interchanging summations and derivatives.","['derivatives', 'summation', 'sequences-and-series', 'calculus']"
1811755,$\mu * \nu$ a finite Borel measure in $\mathbb{R}$?,"Let $\mu$ and $\nu$ be two finite Borel measures on $\mathbb{R}$. For any Borel set $A \subset \mathbb{R}$, define$$\mu * \nu(A) = \mu \times \nu(\{(x, y) \in \mathbb{R}^2 : x + y \in A\}).$$Is $\mu * \nu$ necessarily a finite Borel measure in $\mathbb{R}$? Thoughts. I know that the set $\{(x, y) \in \mathbb{R}^2 : x + y \in A\}$ is Borel when $A$ is Borel.","['real-analysis', 'measure-theory']"
1811759,what is the graph of $y = x^x$?,Obviously on the RHS this graph is just a really steep exponential graph however problems arise on the LHS and I cannot find any graph sketching programs that can do. Some will give a graph but then simply say that the LHS is undefined which must be incorrect since negative values with odd powers must still work like $-3^{-3} = -1/27$ but then of course values like $(-1/2)^{-1/2}$ do not. I asked my maths teacher about this and my tutor and both didn't seem to have answers.,['functions']
1811766,Divergence Theorem when Surface isn't closed,"So we essentially want to evaluate $$\iint_S \vec{F} \cdot d\vec{S},$$ where $\vec{F} = \langle 2x+y, x^2+y, 3z \rangle$ and $S$ is the cylinder $x^2+y^2=4$ , between the surfaces $z=0$ and $z=5$ . We have that the cylinder is open at the top and the bottom. Therefore, we cannot readily apply Gauss' Divergence theorem. We need to subtract the contributions given by the flux through the top and the bottom, from the volume integral. If we let the closed surface of the cylinder be represented by $S$ , the bottom surface represented by $S_1$ and the top surface by $S_2$ , we have that $$\iint_S \vec{F} \cdot n dS + \iint_{S_1} \vec{F} \cdot ndS + \iint_{S_2} \vec{F} \cdot nds = \iiint_V \nabla \cdot \vec{F} dV.$$ Computing the RHS gives us that $$\iiint_V \nabla \cdot \vec{F} dV = 120 \pi.$$ How do we compute $$\iint_{S_1} \vec{F} \cdot ndS$$ and $$\iint_{S_2} \vec{F} \cdot ndS ?$$ Perhaps $$\iint_{S_1} \vec{F} \cdot ndS = \iint_{S_1} \vec{F} \cdot \langle 0, 0, -1 \rangle dS = \iint_{S_1} -3z dS?$$","['multivariable-calculus', 'divergence-theorem', 'surface-integrals', 'vector-analysis']"
1811779,"For	the	general	ellipse $ \frac{x^2}{a^2} + \frac{y^2}{b^2} = 1$ ,show that the midpoints of the chords lie on a straight line.","Question: A  collection  of  parallel    chords  connect pairs   of  points  on  an  ellipse,    as  shown For  the general ellipse  $ \frac{x^2}{a^2} + \frac{y^2}{b^2} = 1$,    show    that    the midpoints   of  the chords  lie on  a   straight    line. What I have tried/attempted: Say that a line in the form $y=mx+c$ intersects the ellipse so $$\frac{x^2}{a^2} + \frac{y^2}{b^2} = 1$$ $$ \Leftrightarrow x^2b^2 + y^2a^2 = a^2b^2 $$ $$ \Leftrightarrow x^2b^2 + (mx+c)^2a^2 = a^2b^2 $$ $$\Leftrightarrow x^2b^2 + (m^2x^2+2mcx+c^2)a^2=a^2b^2 $$ $$ \Leftrightarrow x^2b^2 + a^2m^2x^2+2mca^2x+c^2a^2 - a^2b^2 = 0 $$ $$ \Leftrightarrow (b^2+a^2m^2)x^2 + (2mca^2)x + (c^2a^2-a^2b^2) = 0 $$ $$\Leftrightarrow (b^2+a^2m^2)x^2 + (2mca^2)x + a^2(c^2-b^2) = 0 $$ Now I am stuck should I be doing something with this equation I have formed? Or is there a more easy geometrical approach to this question I am missing.","['conic-sections', 'geometry']"
1811783,Doubt on proof of equivalance of conditions for uniform integrability in $L^1$,"In a measure space with finite total measure, a family $A$ of r.v's is called U.I if $$
\lim_{N\to\infty}\sup_{X\in A} \int_{|X|>N} |X|\,d\mu=0
$$ I have some doubts on equivalance of this definition in $L^1$ space. The following is equivalent to U.I in $L^1$: A is bounded subset of $L^1$. For every $\epsilon>0$ there exists $\delta>0$ such that for any measurable $E$ if $\mu(E)\le \delta$ then $\int_E |X| d\mu\le \epsilon$ for any $X\in A$. The following proof is given: Proof: Suppose $A$ is U.I. Then for any measurable $E$, $$
\int_E |X| = \int_{E\cap \{|X|>N\}} |X| + \int_{E\cap \{|X|\le N\}} |X|:=B+C
$$ Given $\epsilon>0$ we can choose $N$ such that $B<\epsilon /2$ and obviously $C\le N\mu (E)$. Thus we may choose $\delta = \frac{\epsilon}{2N(\mu(\Omega)+1)}$. And I'm not sure why we can't simply choose $\delta=\epsilon/2N$? And why do we need the condition that $\Omega$ is a finite measure space? I don't particularly see any necessity for it, unless I'm missing something about last line. Probably only place where it might matter is stating $C\le N\mu(E)$, since it might be that $\mu(E)=\infty$. But we are controlling size of $E$ anyway, aren't we? Edit : It seems for condition $1$ to hold, $E$ must be of finite measure so total measure must be finite, as we only control size of $E$ for proof of $2$.","['real-analysis', 'measure-theory', 'uniform-integrability']"
1811800,A little help with $ x_n = \frac{1}{n^2} \sum_{k=1}^{n} \left[ k \alpha \right] $,"So I have this little problem I want to solve that says the following: For every number $\alpha \in \mathbb{R} $, analyze the following sequence
$$ \{ x_n \} = \frac{1}{n^2} \sum_{k=1}^{n} \left[ k \alpha \right] ,$$ where $ \left[ x \right] $ is the floor function. Calculate for which values of $ \alpha $ does $ x_n $ converges, or if it diverges. If it converges, calculate the limit of the sequence. My first thought was to prove that this sequence converges, or diverges, for every possible value of $ \alpha $, that is, when $ \alpha \in \mathbb{Z} ,$ $ \alpha \in \mathbb{Q} ,$ and when $ \alpha \in \mathbb{I} ,$ in other words, $ \alpha $ is irrational. So when $ \alpha \in \mathbb{Z} $ I got the following:
If $ \alpha \in \mathbb{Z} $ then $ \left[ \alpha \right] = \alpha ,$ therefore
$$ \{ x_n \} = \frac{1}{n^2} \sum_{k=1}^{n} k \alpha = \frac{\alpha}{n^2} \sum_{k=1}^{n} k = \frac{\alpha}{n^2} \frac{n(n+1)}{2} = \frac{\alpha (n+1)}{2n} ,$$
thus, $ \{ x_n \} $ converges to $$ \{ x_n \} = \frac{\alpha (n+1)}{2n} ;$$
since it converges, then we can calculate the limit as follows
$$ \lim_{n \to \infty} \{ x_n \} = \lim_{n \to \infty} \frac{\alpha (n+1)}{2n} = \frac{\alpha}{2} \lim_{n \to \infty} \frac{n+1}{n} = \frac{\alpha}{2} \lim_{n \to \infty} \frac{1 + \frac{1}{n}}{1} = \frac{\alpha}{2} \left[ \lim_{n \to \infty} 1 + \lim_{n \to \infty} \frac{1}{n} \right] = \frac{\alpha}{2} .$$ So, first of all, is this proof correct? If so, how can I proceed to prove the sequence when $ \alpha \in \mathbb{Q} $ or when it's irrational? Should I prove the floor function for rational and irrational numbers and then apply this to the sequence?","['ceiling-and-floor-functions', 'sequences-and-series', 'limits']"
1811839,Central Limit Theorem: Show that $\mathrm{Pois}(n)$ distribution if approximately Normal if $n$ is a large positive integer,"Here is what the solution in the textbook says: Let $S_n=X_1+\cdots+X_n$, with $X_1,X_2,\ldots$ i.i.d. $\sim \mathrm{Pois}(1)$. Then $S_n \sim\mathrm{Pois}(n)$ and for $n$ large, $S_n$ is approximately $N(n,n)$ by the CLT. Based on my understanding of the Central Limit Theorem, $\overline{S}_n$ is approximately $N(n,n)$, not $S_n$. Here is my proof: The Central Limit Theorem says
  \begin{align}
\sqrt{n}\left(\frac{\overline{S}_n-\mu}{\sigma}\right)\xrightarrow{n \rightarrow \infty}\mathcal{N}(0,1) \\
\sqrt{n}\left(\frac{\overline{S}_n-n}{n}\right)\xrightarrow{n \rightarrow \infty}\mathcal{N}(0,1) \\
\end{align} With a location scale transformation we get: \begin{align}
\overline{S}_n-n \xrightarrow{n \rightarrow \infty}\mathcal{N}(0,n) \\
\overline{S}_n \xrightarrow{n \rightarrow \infty}\mathcal{N}(n,n) \\
\end{align} And since $\overline{S}_n=\frac{1}{n}S_n$,  $S_n  \xrightarrow{n \rightarrow \infty}\mathcal{N}(n,n^3)$. Is my proof incorrect or is the problem just a difference in notation?","['statistics', 'central-limit-theorem', 'probability', 'poisson-distribution']"
1811853,Prove $(A \cup B)' = A' \cap B'$,"I would like some assistance in verifying this proof? (I understand the last conjecture about ""symmetry"" is probably shaky, but I just want to know if the first part is right since going backwards will more than likely be the same as is usually the case in such proofs). We seek to prove $(A \cup B)' = A' \cap B'$. Suppose we have the universal set $U$. We know $x \in (A \cup B)'$. $\implies x \in U - (A \cup B)$. Thus $x \in U \land x \notin (A \cup B)$. $x \notin A \lor x \notin B \implies x \notin (A \cap B)$. So $x \in U \land (x \notin (A \cap B))$. $\implies x \in U \land (x \notin A \land x \notin B) $. $\implies (x \in U \land x \notin A) \land (x \in U \land x \notin B)$ $\implies x \in (U-A) \land x \in (U-B)$ $\implies x \in A' \land x \in B'$ $\implies x \in A' \cap B'$ By the symmetry of the situation we see that $x \in A' \cap B' \implies x \in (A \cup B)' $. Therefore the sets are subsets of one another, therefore they are equal.","['proof-writing', 'elementary-set-theory', 'proof-verification']"
1811865,Recursive Sequence $a_n = \frac{1}{2} (a_{n-1} + 5) $,"I got this question in which they ask me to explain why it is convergent and evaluate its limit. $$a_1=3\;and\;a_n = \frac{1}{2} (a_{n-1} + 5) \\ n=2,3,4,... $$ To prove it's convergent, I show that it is increasing and bounded above by 5. Also, I find its limit by showing that Let $L=\lim_{n\to\infty} a_n$ Notice that $a_n = \frac{1}{2} (a_{n-1} + 5)$ Hence $\lim_{n\to\infty} a_n = \lim_{n\to\infty} \frac{1}{2} (a_{n-1} + 5)$ $\Rightarrow L = \frac{1}{2}(L+5)$ $\Rightarrow 2L = L+5$ $\Rightarrow L = 5$ As the sequence is non-decreasing, $$L =\lim_{n\to\infty} a_n=5 $$ That's what I got. However, the book's answer for this question's limit is $\frac{5}{2}$ Is there anything wrong with my proof? Thanks in advance.","['recurrence-relations', 'real-analysis', 'sequences-and-series', 'limits']"
1811886,What is the most rigorous definition of a matrix?,"Is matrix just a rectangular array of symbols and expressions, or one can define it in a more formal way?","['matrices', 'linear-algebra', 'definition']"
1811897,Water flowing from a vessel with curved sides,"Suppose a hole is drilled perpendicularly into the side of the beaker which is full to the brim with a fluid (say water). This will result in water spurting out, travelling in a parabolic trajectory and hitting the solid, flat surface (say, a table) the beaker is resting upon. For a cylindrical beaker, the distance the water lands from the base of the beaker depends on the height at which the hole is drilled. It turns out that drilling the hole at half the total height of the water column maximises the distance the water spouts. Suppose now that, instead of a beaker with vertical sides; its vertical cross section is curved (like a wine glass). The distance from the base that the water lands is now harder to calculate, because it leaves the beaker travelling at an angle. Does a beaker exist which the property that, regardless of where the hole is drilled, the water spout will hit the table at the same distance away from the base of the beaker? If so, what algebraic equation (if any) defines the vertical cross section of the beaker? Note: for the purposes of this problem, ignore fluid dynamics of the water and air resistance. I'm envisaging a beaker with a circular horizontal cross section. The problem pertains only to the point the water spout initially hits.","['physics', 'differential-geometry']"
1811907,"How to find $\max\int_{a}^{b}\left (\frac{3}{4}-x-x^2 \right )\,dx$ over all possible values of $a$ and $b$, $(a<b)$?","I tried finding the maxima of $f(x)=\frac{3}{4}-x-x^2$ by taking the derivative and so on and use the fact that $\displaystyle\int_{a}^{b}f(x)\,dx \leq M(b-a)$ where $M$ is the global maximum, but then the maximum value depends on the values of $a$ and $b$.","['maxima-minima', 'integration', 'calculus']"
1811961,Evaluation of $\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n \tanh n}$,"Here is a series that arose while playing around with some differential equations. $$\mathcal{S}=\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n \tanh n}$$ I have a feeling that it has a closed form. Although I am not able to attack it. For example an idea that could be promising would be to use the kernel $\pi \csc \pi z$ and integrate the function $$f(z)=\frac{\pi \csc \pi z}{z \tanh z}$$ over a square, although I am unsure about the vertices. In the mean time Wolfram Alpha is unable to give a close form. Instead it returns $0.98903$ as an approximate result. So, can anyone help me derive the closed form (if that eventually exists?)","['sequences-and-series', 'calculus']"
1811973,Does $\lim_{x\to 0} \frac1{x^2}$equal $\infty$ or does it not exist?,If  $\ f:\mathbb R\rightarrow \mathbb R\ $  where  $f(x)=\frac{1}{x^2}$ then $$\lim_{x\to0}f(x)=?$$ Which of the following option is most correct among these (a)$\infty$ (b) limit does not exists Solution According to me$$\lim_{x\to0^+}f(x)=\lim_{x\to0^-}f(x)=\infty$$ but the limit is not finite so we can say that the limit does not exists I am confused to choose the option among (a) & (b) in single choice type question Can anyone tell me ?,['limits']
1811981,Radius of convergence,What is radius of convergence of $\sum_{n=2}^{\infty} \left(1+\dfrac{1}{n}\right)^{n^2}z^n$? I don't know anything,"['complex-analysis', 'power-series']"
1811983,Diagonalizable vs full rank vs nonsingular (square matrix),"There are many discussions of such type problems (comparison), for example: Diagonalizable vs Normal Today, I want to clearly understand the topic. Suppose the matrix $A\in \mathbb{R}^{n\times n}$ . Since the multiplication of all eigenvalues is equal to the determinant of the matrix, $A$ full rank is equivalent to $A$ nonsingular. The above also implies $A$ has linearly independent rows and columns.  So $A$ is invertible . $A$ is diagonalizable iff $A$ has $n$ linearly independent eigenvectors. ( $A$ is non-defective). Note: $A$ is defective if geo. multiplicity $<$ alge. multiplicity. A diagonalizable matrix does not imply full rank (or nonsingular). My problem is Does full rank matrix (nonsingular) imply it is diagonalizable? Equivalently: Does a matrix with all its columns or rows linear independent imply all its eigenvectors are linear independently?","['matrices', 'diagonalization', 'matrix-rank']"
1811989,Proof about equivalence relations,"Let $R$ be a reflexive and symmetric relation on a set $X$. A pair $x,y ∈ X$
are connected via $R$ if there are elements $x = x_0, x_1, . . . , x_k = y$ such that
$(x_i, x_{i+1}) ∈ R$ for all $i = 0, 1, . . . , k − 1$. Let $S = \{(x, y)|x,y $ are connected via $R$ $\}$. Prove that S is an equivalence relation. Approach: This time I don't have an approach because I don't understand how the relation R works on X. Is it basically saying $(x,y) \in R$ if x and y are different? Proof $(x,x)\in S$ because $(x,x) \in R$ hence  are connected by definition, so S is reflexive if $(x,y) \in S$, there exists a set of ordered pairs in R in the form $(x,z_i)........(z_k,y)$ which implies that there is also a path from y to x so $(y,x) \in S$ which implies that S is symmetric if $(x,y),(y,z) \in S$ then there exists ordered pairs in R such that $(x,z_i),....,(z_k,y),(y_j,i),...,(j_k,z)$. This implies that there is a path from x to z, so $(x,z) \in S$. Therefore, S is transitive. Does that make sense? I feel like I am being a little informal","['relations', 'elementary-set-theory']"
1812114,Proving that if $f(x)=x$ has no real solution then $f(f(x))=x$ has no real solution either,"If $f:\mathbb{R} \to \mathbb{R}$ is a continuous function such that $f(x)=x$ has no real solution, then show that $f(f(x))=x$ has no real solution either. Is the proof trivial as it seems or does it need an analytical approach?","['derivatives', 'real-analysis', 'continuity', 'functions']"
1812115,Jacobian determinant of a map?,"For $m,n\in \mathbb N$, let $f$ is the map given by 
$$\begin{align}
 f: & \quad \mathbb R^m \times \mathbb R^n \longrightarrow \mathbb R^m \times \mathbb R^n \\
& (x,y)\mapsto f(x,y) = (x+x',y+y'+[x,x']); \quad \mbox{for fixed } \, (x',y')\in \mathbb R^m \times \mathbb R^n,  \end{align}$$
where $[.,.]$ is a map $[.,.] : \mathbb R^m \times \mathbb R^m \longrightarrow \mathbb R^n.$ How to prove that the differential of $f$ is lower triangular, and thus the Jacobian determinant is $1$ (i.e., $|J_{f}|=1$) ? Thank you in advance","['real-analysis', 'differential', 'calculus', 'determinant', 'multivariable-calculus']"
1812121,"Evaluating $\int_{0}^\infty \frac{\log x \, dx}{\sqrt x(x^2+a^2)^2}$ using contour integration","I need your help with this integral: $$\int_{0}^\infty \frac{\log x \, dx}{\sqrt x(x^2+a^2)^2}$$ where $a>0$. I have tried some complex integration methods, but none seems adequate for this particular one. Is there a specific method for this kind of integrals? What contour should I use?","['complex-analysis', 'improper-integrals', 'integration', 'definite-integrals', 'contour-integration']"
1812147,Value of $k$ for equation to have no solution,"What are different integer values of $k$ between $1-9$ for which the equation $$|x-1|+|x-2|+|x+1|+|x+2|=4k$$,has no solutions. Now there are 24 different ways of having signs ie the equation after removing mod.solving these $24$ equations and then getting answer is very much time consuming(though I got the answer) So my main problem is how to deal with the mod sign and proceed with it or is there any shortcut.",['algebra-precalculus']
1812151,Can a non complete metric space be homeomorphic to its completion?,Suppose we have a non complete metric space. Can it be homeomorphic to its metric completion?,"['general-topology', 'metric-spaces']"
1812168,Using a Taylor polynomial to approximate $\cos(\frac14)$ with an error no more than $10^{-12}$,"So the Lagrange remainder is given by: $$R_n(x)=\frac{f^{n+1}(c)}{(n+1)!}(x-a)^{n+1}.$$ We want $\cos(\frac14)$ and we can do it around $a=0$ . We know that $f^{n+1}$ is either $\pm\cos x$ or $\pm\sin x$ , which are both bounded with an absolute value of $1$ : $$|\cos x|\le1,\qquad |\sin x|\le 1.$$ So we can say the following is true: $$|R_n(x)|=\frac{f^{n+1}(c)}{(n+1)!}\left(\frac14\right)^{n+1}\le 1\cdot\frac{1}{(n+1)!}\left(\frac14\right)^{n+1} $$ And we get: $$|R_n(x)|\le \frac{1}{(n+1)!}\left(\frac14\right)^{n+1} $$ Let's try $n=6$ : $$\frac{1}{10^{12}}\le \frac{1}{7!}\left(\frac14\right)^{7} =\frac{1}{5040}\cdot\frac{1}{16384}=\frac{1}{82575360} $$ $$\frac{1}{10^{12}}\le \frac{1}{82575360}  $$ Is $n=6$ enough? Or do I have to continue?","['taylor-expansion', 'trigonometry', 'trigonometric-series', 'approximation']"
1812170,Solve $2\cosh z + \sinh z = i$ for $z$,"The equation is $$2\cosh z + \sinh z = i$$
I used the following formulas:
$$\cosh z = \frac{e^z + e^{-z}}{2},
  \sinh z = \frac{e^z - e^{-z}}{2}$$
to reduce this equation to $$3e^z - e^{-z} = 2i$$ but I am not sure now to proceed now. I had idea about replacing $e^z$ with $x$, having $$3e^z -2i - e^{-z} = 0 \implies 3x^2 -2ix - 1 = 0$$
but I am to sure I can do this, and resulting $x$ is not looking good. My teacher said this equation can be reduced to quadratic, but he didn't explain it in detail. How do I solve this?","['trigonometry', 'complex-numbers', 'quadratics']"
1812213,Left ideals of $M_2(K)$ with $K$ a field,"Is it true that the only proper left ideals of $M_2(K)$, the ring of the matrices whose coefficients are in a field $K$, are
$$
\left\{\begin{pmatrix}ah & ak \\ bh & bk\end{pmatrix}: a,b \in K\right\}
$$
for all $h,k \in K$ not both $0$? This is my attempt. Proper ideals don't contain invertible elements, so the determinants of the elements in the ideals are all zeros. $2 \times 2$ matrices with null determinant are those with one of the two columns multiple of the other and one of the two rows multiple of the other, that is, the matrices of the form 
$$\begin{pmatrix}ah & ak \\ bh & bk\end{pmatrix}, \quad a,b,h,k \in K$$
Let $I$ be a proper left ideal. It contains a matrix $\bigr(\begin{smallmatrix}ah & ak \\ bh & bk\end{smallmatrix}\bigl)$ for some $a\neq 0,b\neq 0,h,k \in K$ and $h,k$ not both $0$ and thus, for all $x,y \in K$,
$$\begin{pmatrix}x & 0 \\ 0 & y \end{pmatrix} \begin{pmatrix}ah & ak \\ bh & bk\end{pmatrix} = \begin{pmatrix} ahx & akx \\ bhy & bky\end{pmatrix}$$
which implies that 
$$\left\{\begin{pmatrix}ih & ik \\ jh & jk\end{pmatrix} : i,j \in K\right\} \subseteq I$$
The left hand side is itself a proper left ideal. Wlog $h\neq 0$. If $I$ contained $\bigr(\begin{smallmatrix}ah & al \\ bh & bl\end{smallmatrix}\bigl)$, with $l\neq k$, then it would also contain
$$\begin{pmatrix}1& 0 \\ 0 & 0\end{pmatrix} \begin{pmatrix}ah & ak \\ bh & bk\end{pmatrix} +\begin{pmatrix}0 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix}ah & al \\ bh & bl\end{pmatrix} = \begin{pmatrix}ah & ak \\ bh & bl\end{pmatrix}$$
which is invertible, then $I=M_2(K)$, contradiction. Is this correct?","['matrices', 'ring-theory', 'ideals']"
1812219,"Integrating the following $\int \sqrt{\tan x+1}\,dx$","Question: Integrate the following, $$\int\sqrt{\tan x+1}\;dx.$$ Wolfram Alpha returns a non-elementary answer. Can someone please spot the mistake I have made here: First consider this integral: $$\int \frac{1}{(x+1)\sqrt{x+3}} \, dx = -\sqrt{2}\tanh^{-1}\frac{\sqrt{x+3}}{\sqrt{2}} + c$$ Wolfram Alpha confirms that result. Then, we have $$I=\int \sqrt{\tan x+1} \, dx, \quad \tan x=u+2,
\quad dx=\frac{du}{\sec^{2}x}=\frac{du}{(u+2)^{2}-1}=\frac{dx}{(u+3)(u+1)}$$ So this transforms the integral to the first integral on this post, which we can evaluate. Then after evaluation and resubstitution I get: $$I=-\sqrt{2}\tanh^{-1}\frac{\sqrt{\tan x+1}}{\sqrt{2}}+c$$ However differentiating this with Wolfram Alpha gives me a messy trigonometric expression which doesn't seem to be equal (I tested some values in both expressions and get different answers). I also estimated the area under the integral between some values and also obtained different answers using the closed form. Any ideas why? EDIT: I used the wrong identity. Nevertheless, we can still use this method to integrate sqrt(tanhx integrals). E.g:
$$I=\int \sqrt{\tanh x+1} \, dx, \tanh x=u+2,\quad 
-dx = \frac{du}{\operatorname{sech}^2 x} = \frac{du}{(u+2)^2-1} = \frac{dx}{(u+3)(u+1)}$$ To obtain:
 $\int \sqrt{\tanh x+1} \, dx = I=\sqrt{2}\tanh^{-1} \dfrac{\sqrt{\tanh x+1}}{\sqrt{2}}+c$","['indefinite-integrals', 'integration', 'trigonometric-integrals']"
1812238,help on limit exercise,"I'm having some trouble solving this limit: $$\lim_{x\to\infty} x\left[\left(\cosh x\right)^ \frac1x - \left(1+\frac1x\right)^x\right]$$ It's part of a set of limits wich should be solved using taylor.
I tried this road: $$\lim_{x\to\infty} x\left[e^{\frac1x\ln(\cosh x)}-e^{\ln\left(1+\frac1x\right)x}\right]$$ I then tried with algebraic manipulation using the definition of hyperbolic cosine $\frac12(e^x+e^{-x})$ and then I also played a bit with l'Hopital but it turns into something suspiciously compicated...
I'm taking real analysis 1 and my toolset is: -algebraic manipulation -talyor series -l'Hopital (if and only if all else fails) I can't use more advanced techniques since they are not part of the course. I'm sure there's something obvious I'm missing.
Any idea on how to proceed?","['real-analysis', 'calculus', 'limits']"
1812264,Not integrable although iterated integrals are equal,"How can I show that the function $$f=\begin{cases} 0 & (x,y)=(0,0)\\\frac{xy}{(x^2+y^2)^2} & \mbox{else}\end{cases}$$ is not Lebesgue-integrable, although the iterated integrals exist and are equal: 
$$\int_{-1}^{1}\int_{-1}^{1}f(x,y)dydx=\int_{-1}^{1}\int_{-1}^{1}f(x,y)dxdy?$$","['real-analysis', 'integration', 'lebesgue-integral', 'measure-theory']"
1812329,Galois group of $\overline{\mathbb{F}_{p}}$ gives arithmetical information for finite fields $K/\mathbb{F}_{p}$?,"Let $\mathbb{F}_{p}$ be the field with $p$ elements and $\overline{\mathbb{F}_{p}}$ be its algebraic closure. For some reason, we want to understand the structure of the Galois group of such an extension. We then find that we have an isomorphism of topological groups $G(\overline{\mathbb{F}_{p}}/\mathbb{F}_{p})\cong\hat{\mathbb{Z}}$. My question is: The description of the group $G(\overline{\mathbb{F}_{p}}/\mathbb{F}_{p})$ certainly gives the ""subfield lattice"" of $\overline{\mathbb{F}_{p}}$ and this is nice. On the other hand, such a subfield lattice can be found very easily without using this isomorphism, and in fact most books even use this fact to establish the isomorphism. In view of this comments, why does one want to describe such an absolute Galois group and why it is useful to know that, for instance, $\hat{\mathbb{Z}}\cong\prod_{p}\mathbb{Z}_{p}$, where $\mathbb{Z}_{p}$ denotes the $p$-adic integers? Do we get any arithmetical information about finite fields $K/\mathbb{F}_{p}$? Thanks a lot.","['number-theory', 'abstract-algebra', 'galois-theory', 'algebraic-number-theory']"
1812363,"Invariant vectors of $A^n B^m$ with $A,B$ orthogonal matrices","Let $A$ be the following matrix:$$A=\dfrac{1}{2}\ \left(
\begin{array}{cccccccccc}
 -1 & -1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
 1 & -1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\
 -1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\
 0 & 0 & 1 & -1 & 0 & 0 & 1 & 1 & 0 & 0 \\
 0 & 0 & -1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 \\
 0 & 0 & 0 & 0 & 1 & -1 & 0 & 0 & 1 & 1 \\
 0 & 0 & 0 & 0 & -1 & 1 & 0 & 0 & 1 & 1 \\
 0 & 0 & 0 & 0 & 0 & 0 & 1 & -1 & 1 & -1 \\
 0 & 0 & 0 & 0 & 0 & 0 & -1 & 1 & 1 & -1 \\
\end{array}
\right)$$
and $B=A+C$ with $C$ a matrix full of zeros, except the first $(2,2)$ block equal to $\begin{bmatrix} 1&1 \\ -1&-1\end{bmatrix}$: $$B=\dfrac{1}{2}\ \left(
\begin{array}{cccccccccc}
 \mathbf{1} &  \mathbf{1} & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
  \mathbf{-1} &  \mathbf{-1} & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
 1 & -1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\
 -1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\
 0 & 0 & 1 & -1 & 0 & 0 & 1 & 1 & 0 & 0 \\
 0 & 0 & -1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 \\
 0 & 0 & 0 & 0 & 1 & -1 & 0 & 0 & 1 & 1 \\
 0 & 0 & 0 & 0 & -1 & 1 & 0 & 0 & 1 & 1 \\
 0 & 0 & 0 & 0 & 0 & 0 & 1 & -1 & 1 & -1 \\
 0 & 0 & 0 & 0 & 0 & 0 & -1 & 1 & 1 & -1 \\
\end{array}
\right)$$ Both $A,B$ are both orthogonal matrices with elements in $\{-\frac{1}{2},0,\frac{1}{2}\}$, hence each row and each column has exactly 4 non-zeros entries (the sum of the square of the entries of each row or column is 1). The characteristic polynomial of $A$ is $1+x^N$ and that of $B$ is $-1+x^N$ (here, $N=10$). $A,B,C$ do not commute. $A,B$ are isometries, so is $A^n B^m$. I am trying to characterize $\ker(A^n B^m -I)$ for $(n,m)\in\mathbb{N}^2$. From their characteristic polynomial, $B^N=I$ and $A^{2N}=I$ so it suffices to consider $n\leq 2N$ and $m\leq N$. From experimental computations, it seems that ""something happens"": $A,B$ are made of $(2,2)$ blocks which are multiplied (there seems to be a structure of monoid) and translates to neighbour blocks. The question is how to characterize the kernel of  $A^nB^m-I$ (at least it's dimension) as a function of $n$ and $m$. This could be based on a basis in which $A$ and $B$ have a simple expression, but I did not succeed in identifying such a basis, despite the fact that $C=B-A$ is full of zeros (except 4 entries). Edit Based on Armadillo Jim's answer, the question can also be written as finding the maximal invariant subspace of  $A^n (DA)^m$ ($D$ is a block diagonal matrix of blocks $\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$ and $I_{N-2}$).","['matrices', 'linear-algebra', 'monoid', 'vector-spaces']"
1812383,$\int\frac{\sin x}{\sqrt{1-\sin x}}dx=?$ Calculate this integral,$\displaystyle\int\dfrac{\sin x}{\sqrt{1-\sin x}}dx=?$ Effort; $1-\sin x=t^2\Rightarrow \sin x=1-t^2\Rightarrow \cos x=\sqrt{2t^2-t^4}$ $1-\sin x=t^2\Rightarrow-\cos x dx=2tdt\Rightarrow dx=\frac{2t}{\sqrt{t^4-2t^2}}dt$ $\displaystyle\int\frac{1-t^2}{t}\cdot\frac{2t}{\sqrt{t^4-2t^2}}dt=2\int\frac{1-t^2}{\sqrt{t^4-2t^2}}dt$ $\ = 2\displaystyle\int\frac{1}{\sqrt{t^4-2t^2}}dt-2\displaystyle\int\frac{t}{\sqrt{t^2-2t}}dt$ $\ = 2\displaystyle\int t^{-1}(t^2-2)^{-\frac{1}{2}}dt-2\displaystyle\int t(t^2-2t)^{-\frac{1}{2}}dt$ But after that I don't know how to continue.,['integration']
1812387,Average amplitude of the sum of N sines with random phase differences,"We have N functions of the form $asin(kx+c)$ Their $a$ and $k$ values are the same their $c$ is a random number between $0$ and $2\pi$ $f(x)$ is the sum all N functions I know $f(x)=Asin(kx+c')$ will also be a sine function with the same period but most likely a different amplitude $A$ and phase $c'$. $A$ can be anything between $0$ and $Na$, so $0<A^2<N^2a^2$. But what will be the average the value of $A^2$? I got to this problem when I was studying sound inteference. I suspect from the physics behind it the answer is $Na^2$. I can prove this result for $N=2$. I suppose the harmonic addition theorem could be helpful but I don't quite know how to get to the average.","['trigonometry', 'average']"
1812388,Calculating $\lim \limits_{x \to \infty} \frac{x+\frac12\cos x}{x-\frac12\sin x}$ using the sandwich theorem,"Calculating $\lim \limits_{x \to \infty} \dfrac{x+\frac12\cos x}{x-\frac12\sin x}$ Correct me if I'm wrong: $\cos x$ and $\sin x$ are bounded so that $$|\cos x|\le 1,\qquad |\sin x|\le1$$ Therefore I can say: $$
\frac{x-\frac12}{x+\frac12}\le
\frac{x+\frac12\cos x}{x-\frac12\sin x}\le 
\frac{x+\frac12}{x-\frac12}
$$ the limits of the left and right side are equal to 1, therefore the the limit I'm looking for is also equal to 1 . The answer is correct, but what I'm not sure is $$
\frac{x-\frac12}{x+\frac12}\le\frac{x+\frac12\cos x}{x-\frac12\sin x}
$$ was this step correct?","['trigonometry', 'limits']"
1812391,"Relation between the dual space, transpose matrices and rank-nullity theorem","Summing up, how can one use linear functionals, transpose matrices, row and column rank equality and annihilators to prove the rank-nullity theorem? While studying linear algebra, I'm trying to get the precise relation between the following concepts: given two vector spaces $V$ and $W$ , I can form the dual spaces $V^*$ and $W^*$ by taking all the linear functionals on $V$ and $W$ . The basis of each one of those spaces lifts to the duals (though this demonstration I'm still going to carry on, but I have the idea how to do so). If I have a linear transformation $T: V \to W$ , I have a natural way of defining a transformation $T^*: W^* \to V^*$ , by composition: given a functional $g \in W^*$ , I define $T^*g := f \in V^*$ by: $$f(\alpha) = g(T\alpha)$$ This way of defining $T^*$ is familiar to me, as it is similar a construction commonly done on modules over a ring $R$ . Now, if I represent $T$ as a matrix on a choice of basis for $V$ and $W$ , and $T^*$ as the matrix on the basis given by the lifting, then $T^*$ will be the transpose of $T$ (the exchange of rows and columns of $T$ ). I don't see clearly why this happens. Furthermore, the rank of $T$ is equal to the rank of $T^*$ , what proves that the column rank of a matrix equals it's row rank. Given that the rank is the dimension of the image subspace, I think that this can be show by carrying out the image of $V$ on $W$ to the duals. I also read that there is a relation between the duals and the kernel of a linear transformation (the annihilator (?)), but that isn't very clear to me either. Using that and the facts above, one can prove the rank-nullity theorem. The reason I'm asking this is that I was given a proof of the rank-nullity theorem without using the linear functionals, and that seemed to depend only on $T$ having the same row and column rank (which was proved by smart manipulation of some equations on vectors). That proof didn't gave me a satisfactory intuition on the rank-nullity theorem, specially when $V \neq W$ . I believe that the proof through linear functionals will be more enlightening. EDIT: I was following the treatment given on Hoffman's Linear Algebra, chapters 3.5 through 3.7 (linear functionals, annihilators and transposes), if that's of interest.","['duality-theorems', 'transpose', 'matrix-rank', 'linear-algebra', 'vector-spaces']"
1812401,Simplifying derivative result,"I am doing the derivative of $$f(x) = \frac{x^2 -4x +3}{x^2-1}$$ So my result is the following $$f'(x) = \frac{4x^2 -8x +4}{(x^2-1)^2}$$ I am sure the answer is correct, but in my solutions book and In Wolfram Alpha they simplify until $$f'(x) = \frac{4}{(x+1)^2}$$ And I don't know why, which steps are they doing?",['derivatives']
1812405,Determining the image of a function $\psi:\mathbb{R}^2 \rightarrow \mathbb{R}^2$,"Determining the image of a function $\psi:\mathbb{R}^2 \rightarrow \mathbb{R}^2$, $\psi(x,y) = (x^2 - y^2, x^2 + y^2)$ I made some observations about $\psi$: $\psi$ isn't injective, since $\psi(-x,-y) = \psi(x,y)$.
The restriction $\psi_{|D}$, where $D = \{(x,y) \in \mathbb{R}^2 | x,y>0\}$ makes $\psi$ injective, since:
$(x_{1}^2-y_{1}^2,x_{1}^2+y_{1}^2) = (x_{2}^2-y_{2}^2,x_{2}^2+y_{2}^2)$ occurs when $x_{1}^2-y_{1}^2=x_{2}^2-y_{2}^2$ and $x_{1}^2+y_{1}^2=x_{2}^2+y_{2}^2$, which will lead to $(x_1,y_1)=(x_2,y_2)$. How do I determine the image of  $\psi_{|D}$?  (My objective here is to create a bijection)",['functions']
1812433,"Finding all $f:[0, \infty) \to [0, \infty)$ differentiable and convex with $f(0)=0$ and $f'(x)\cdot f\bigl(f(x)\bigr)=x$","Find all functions $f:[0, \infty) \to [0, \infty)$ , differentiable and convex, such that $$f(0)=0 \tag1\label1$$ and $$ \ f'(x)\cdot f\bigl(f(x)\bigr)=x, \forall x \tag2\label2$$ Obviously, $f(x)=x$ is a solution, so I'm trying to find other solutions. From \eqref{2} we get $f(x) \gt 0, \ f'(x) \gt 0, \forall x \gt 0$ and $f'(0)=0$ therefore $f$ is strictly increasing. So far, I don't know how to use the convexity of $f$ , the definition of convexity doesn't seem to help. UPDATE: From \eqref{2} $f'(x)=\dfrac x{f\bigl(f(x)\bigr)}, \ \forall x \gt 0$ therefore $f$ is twice differentiable on $(0, \infty)$ .","['derivatives', 'functional-equations']"
1812446,$\small| U-\frac{m}{n}\small| \leq \frac{1}{n^3}$,"Let $U$ be uniform distributed in $[0,1]$ . Show that with probability $1$ there's maximum a finite amount of $n \in \mathbb N$, so that the inequality $\small| U-\frac{m}{n}\small| \leq \frac{1}{n^3}$ is true for $m \in \mathbb N$. I have tried for two days and I couldn't solve it.","['stochastic-processes', 'uniform-distribution', 'inequality', 'probability']"
1812448,Convergence of a series with terms in-between harmonic and geometric,"Let $\alpha \in (0,1)$, and let $K \in \mathbb N$. Consider the infinite series as a function a $K$: $$f(K) = \frac{K^\alpha}{(1+K^\alpha)} + \frac{K^\alpha (K+1)^\alpha}{(1+K^\alpha)(1+(K+1)^\alpha)} + \frac{K^\alpha (K+1)^\alpha (K+2)^\alpha}{(1+K^\alpha)(1+(K+1)^\alpha)(1+ (K+2)^\alpha} + \dots $$ My questions are: Is $f(K)$ finite? Is $\lim_{K \to \infty} f(K)$ finite? Note that when $\alpha = 0$, the series always converges to $1$. When $\alpha = 1$, it always diverges. So the question boils down to: what happens when $\alpha \in (0,1)$?","['convergence-divergence', 'sequences-and-series', 'calculus', 'limits']"
1812455,Prove $(a+b)^3(b+c)^3(c+d)^3(d+a)^3\ge 16a^2b^2c^2d^2$,"Let $a,b,c,d>0$ and such $a+b+c+d=1$, show that
$$(a+b)^3(b+c)^3(c+d)^3(d+a)^3\ge 16a^2b^2c^2d^2$$ since
$$a+b\ge 2\sqrt{ab}$$
I think this will not hold.because we have $256abcd\le 1$","['inequality', 'substitution', 'multivariable-calculus', 'symmetric-polynomials', 'uvw']"
1812461,Injection from $\mathcal P \left({\mathbb{N}}\right)$ to derangements of $\mathbb{N}$,Let $S$ be the set of the permutations without fixed points of $\mathbb{N}$. Is there an elegant way to exhibit an injection from the power set $\mathcal P \left({\mathbb{N}}\right)$ to $S$ ? (Clearly such an injection exists because $\left|\mathcal P \left({\mathbb{N}}\right)\right|=\left|S \right|=2^{\aleph_0}$),"['elementary-set-theory', 'combinatorics', 'functions']"
1812495,"How can I show $\lim\limits_{x\to a}e^x=e^a$ just using limit ,without ""continuous""","Effort: We know that $e^x=\lim\limits_{n\to \infty}\left(1+\dfrac{x}{n}\right)^n$ And if we can say $\lim\limits_{y\to b}\left[\lim\limits_{x\to a}f\right]=\lim\limits_{x\to a}\left[\lim\limits_{y\to b}f\right]$  for this problem; $\lim\limits_{x\to a}e^x=\lim\limits_{x\to a} \left[\lim\limits_{n\to \infty}\left(1+\dfrac{x}{n}\right)^n\right]=\lim\limits_{n\to \infty}\left[\lim\limits_{x\to a}\left(1+\dfrac{x}{n}\right)^n\right]=\lim\limits_{n\to \infty}\left[\left(1+\dfrac{a}{n}\right)^n\right]=e^a$ and I proof $\boxed{\boxed{\lim\limits_{x\to a}e^x=e^a}}$ . İs this proof true? And please help ,how I can prove this, with ""accurately"".",['limits']
1812540,How to solve a differential equation with a distributional free term?,"I tried to solve this type of differential equation $$y'' + y = \delta + \delta' .$$ I tried using the Laplace Transform, but I'm stuck at that $\delta$ (Dirac function). The only thing I know is the solution $$(\cos (t) + \sin (t)) \, u(t)$$ with $u(t)$ being Heaviside's step function. EDIT : $$y'' + y = \delta + \delta' .$$ not $$y'' + y' = \delta + \delta' .$$","['dirac-delta', 'distribution-theory', 'ordinary-differential-equations', 'laplace-transform']"
1812622,Prove $\int_{0}^{2\pi}{x\sin^3(x)\over 1+\cos^2(x)}dx=2\pi-\pi^2$,"Integrate $$I=\int_{0}^{2\pi}{x\sin^3(x)\over 1+\cos^2(x)}dx=2\pi-\pi^2$$ $${1\over 1+y}=\sum_{n=0}^{\infty}(-1)^ny^n$$ Setting $y=\cos(x)$ $\sin^3(x)={1\over 4}{(3\sin(x)-\sin(3x))}$ Substitute into I $$I=\sum_{n=0}^{\infty}{(-1)^n\over 4}\int_{0}^{2\pi}x\sin^3(x)\cos^{2n}(x)dx$$ $$I=\sum_{n=0}^{\infty}{(-1)^n\over 4}\int_{0}^{2\pi}x\sin(x)\cos^{2n}(x)-x\sin(3x)\cos^{2n}(x)dx$$ Let $$J=\int_{0}^{2\pi}x\sin(x)\cos^{2n}(x)dx$$
Recall $$\int_{0}^{2\pi}\sin(x)\cos^{2n}(x)dx=0$$
$$\int_{0}^{2\pi}\cos^{2n+1}(x)dx=0$$
Applying by parts $$J=\left.-x{\cos^{2n+1}\over 2n+1}\right|_{0}^{2\pi}-{1\over 2n+1}\int_{0}^{2\pi}\cos^{2n+1}(x)dx$$ $$J={1-2\pi\over 1+2n}$$ Let $$K=\int_{0}^{2\pi}x\sin(3x)\cos^{2n}(x)dx$$ $$\sin(3x)=3\sin(x)\cos^2(x)-\sin^3(x)$$ Substitute into K $$K=\int_{0}^{2\pi}3x\sin(x)\cos^{2n+2}(x)-x\sin^3(x)\cos^{2n}(x)dx$$ Let $$L=3\int_{0}^{2\pi}x\sin(x)\cos^{2n+2}(x)dx$$ Applying by parts $$L=\left.-3x{\cos^{2n+3}(x)\over n+1}\right|_{0}^{2\pi}-{1\over n+1}\int_{0}^{2\pi}\cos^{2n+3}dx$$ $$L={3-6\pi\over n+1}$$ Let $$M=\int_{0}^{2\pi}x\sin^3(x)\cos^{2n}(x)dx$$ Integrate M it is tedious. Anyway can someone show me another easy method to tackle integral $I$? Thank you.","['integration', 'definite-integrals', 'closed-form']"
1812627,modern calculus or analysis text that emphasizes Landau notation?,"Is there a comprehensive calculus or analysis textbook or problem book, written in the last twenty years, that emphasizes the use of Landau notation (big and little oh), especially for making estimates and calculating limits? This emphasis is lacking in most conventional treatments and standard undergraduate coursework, at least in my experience. (I am in the US.) Yet as so many MSE answers illustrate, it is the way many experts approach elementary problems. I feel I have learned more about the power of this approach by reading answers here at MSE than I have from any book. So I'm curious whether some systematic but elementary treatment does exist that I've just happened to miss. If not, where would you point students looking to develop facility in calculation from this point of view? EDIT: well, since there have been no answers, I wonder if there is such a book at all, not necessarily written within the last twenty years. I suppose one reference would be Whittaker and Watson's Course of Modern Analysis. Is there something pitched at a slightly more elementary level, that would be suitable to assign to first or second year undergraduates?","['reference-request', 'calculus', 'analysis']"
1812653,Is the empty set linearly independent or linearly dependent?,Is the empty set linearly independent or dependent?,"['linear-algebra', 'definition']"
1812663,"A ""trick"" with a deck of cards, probability of equal result","I recalled this ""trick"" I read in some magazine years ago and thought I'd try to explore it a bit more. You have standard deck of 52 cards. The cards are on the table, facing up. Your friend secredly selects a number, $n_0$, from 1 to 13. Then your friend lifts $n_0$ cards one by one. Then he selects the value of the $n_0$th card, say $n_1$, as his new number and lifts again $n_1$ cards and takes the value of the $n_1$th as the new value. This continues until all the cards have been lifted. Let the value he has after the final card be $n_v$. All this is done silently. The trick is, the magazine stated, that you can, with somewhat high probability, predict his final number. The idea is to choose your own number, $n_0^*$, and do the above process yourself as your friend does his counting. There is, apparently, high probability that your final value is the same as his. Not a very good trick, in my opinion. I did some simulations. It seems that the probability that the values are the same is a bit lower than $0.7$. How should one analyze this kind of a situation and what is the correct probability? I remember the article mentioned Markov chains, but I cannot see how I could use them in this case. The process is finite and I don't think this has the Markov's property. I tried constructing some large transition matrix, but nothing came of it. I tried some simplifications, like using only three cards with numbers 1,2 and 3, but the calculations become very messy rather quickly. Edit:
I did some more simulations, 200,000 runs for each pair $\{n_0,n_0^*\}$ and got the following matrix for probability of same result, rounded to save space. $\left(
\begin{array}{ccccccccccccc}
 1. & 0.7 & 0.7 & 0.7 & 0.69 & 0.69 & 0.69 & 0.68 & 0.68 & 0.68 & 0.67 & 0.67 & 0.67 \\
 0.7 & 1. & 0.7 & 0.69 & 0.69 & 0.69 & 0.68 & 0.68 & 0.67 & 0.67 & 0.67 & 0.66 & 0.66 \\
 0.7 & 0.7 & 1. & 0.69 & 0.69 & 0.68 & 0.68 & 0.67 & 0.67 & 0.67 & 0.66 & 0.66 & 0.66 \\
 0.7 & 0.69 & 0.69 & 1. & 0.68 & 0.68 & 0.67 & 0.67 & 0.67 & 0.66 & 0.66 & 0.65 & 0.65 \\
 0.69 & 0.69 & 0.69 & 0.68 & 1. & 0.68 & 0.67 & 0.67 & 0.66 & 0.66 & 0.65 & 0.65 & 0.65 \\
 0.69 & 0.69 & 0.68 & 0.68 & 0.68 & 1. & 0.67 & 0.66 & 0.66 & 0.66 & 0.65 & 0.65 & 0.64 \\
 0.69 & 0.68 & 0.68 & 0.67 & 0.67 & 0.67 & 1. & 0.66 & 0.66 & 0.65 & 0.65 & 0.64 & 0.64 \\
 0.68 & 0.68 & 0.67 & 0.67 & 0.67 & 0.66 & 0.66 & 1. & 0.65 & 0.65 & 0.64 & 0.64 & 0.63 \\
 0.68 & 0.67 & 0.67 & 0.67 & 0.66 & 0.66 & 0.66 & 0.65 & 1. & 0.64 & 0.64 & 0.64 & 0.63 \\
 0.68 & 0.67 & 0.67 & 0.66 & 0.66 & 0.66 & 0.65 & 0.65 & 0.64 & 1. & 0.64 & 0.63 & 0.63 \\
 0.67 & 0.67 & 0.66 & 0.66 & 0.65 & 0.65 & 0.65 & 0.64 & 0.64 & 0.64 & 1. & 0.63 & 0.62 \\
 0.67 & 0.66 & 0.66 & 0.65 & 0.65 & 0.65 & 0.64 & 0.64 & 0.64 & 0.63 & 0.63 & 1. & 0.62 \\
 0.67 & 0.66 & 0.66 & 0.65 & 0.65 & 0.64 & 0.64 & 0.63 & 0.63 & 0.63 & 0.62 & 0.62 & 1. \\
\end{array}
\right)$ Actually, as the order doesn't matter, I simulated only cases with $n_0^* > n_0$ and copied the results to the lower triangle. The diagonal elements are all one, for if both players select the same starting number, the results are identical. If I calculated it correctly, for the elements outside the diagonal, a 95% two-sided confidence interval is $\text{value}\pm 0.00219131.$ In any case, the best strategy seems to be to select $n_0^* = 1$.",['probability']
1812691,Definition of Uniform Convergence for vector-valued functions,"Birkhoff and Rota's 'Ordinary Differential Equations' proves that Picard iteration works not just for scalar differential equations but for the systems of scalar DEs that arise from vector-valued differential equations as well.  (That is, DEs of the form $\frac{d\mathbf{f}}{dt} = \mathbf{F}(\mathbf{f},t)$, where $\mathbf{f}$ and $\mathbf{F}$ are vector-valued.)  The proof relies on the notion of uniform convergence, but I'm unable to find a definition for uniform convergence for vector-valued functions.  By analogy with Wikipedia's definition for uniform convergence for functions whose range is simply in $\mathbb{R}$, I've gone ahead and made my own definition: Let $f_i:\mathbb{R}^n \to \mathbb{R}^m$ be functions such that $f_i$ is defined for all $i \in \mathbb{N}$.  We say that $(f_i)_{i \in \mathbb{N}}$ is uniformly convergent with limit $f:\mathbb{R}^n \to \mathbb{R}^m$ if for every $\epsilon > 0$, there exists a natural number $K$ such that for all $\mathbf{x} \in \mathbb{R}^n$ and all $k \geq K$ we have $\|f(\mathbf{x})_k - f(\mathbf{x})\| < \epsilon$. Is this correct, or close to correct?","['real-analysis', 'ordinary-differential-equations']"
1812728,"Can limits be thought of as linear functionals (or operators, depending on context)?","Ok so I just started Calc I this summer and since I already feel pretty comfortable with it from high school, I'm trying to gain a more rigorous perspective on it. I already know that limits behave linearly in the sense that
$$
\lim_{x \to a}[f(x)+g(x)]=\lim_{x \to a}f(x)+\lim_{x \to a}g(x)
$$
and 
$$
\lim_{x \to a}[af(x)]=a \left(\lim_{x \to a}f(x)\right)
$$
but I have never seen them formally described as a linear functional (or linear operator if the output is a function as in the case of the derivative) in the sense that they take an element of a suitable function space (for simplicity, take the continuous functions which form an infinite dimensional normed vector space, lets say $E$) such that $L:E \to \mathbb{R}$ where $L$ is defined by 
$$
L=\lim_{x \to a}
$$
My gut instinct on this is that it may have never been useful to formalize the notion of a limit as a linear functional or that the definition of the derivative operator $D:C^{k} \to C^{k-1}$ as 
$$
Df=\lim_{h \to 0} \frac{f(x+h)+f(x)}{h}
$$
makes this so obvious that no one talks about it explicitly. Another way to phrase my question would be: ""Limits belong to which class of mathematical objects?"" I tried asking my teacher but she didn't even understand what I was asking (she is a TA type who is well intentioned but clearly not comfortable enough with the material to teach) so any additional insights would be of great help here.","['functional-analysis', 'calculus']"
1812735,The point spectrum and residual spectrum of an operator on $l_2$ related to backward shift,"I have a problem with the spectrum of this operator: $(Tx)_1 = x_2$ $(Tx)_2 = x_1$ $(Tx)_n = \frac{1}{n}x_{n+1}$ with $n\ge3$ Find the $||T||$, the point spectrum $\sigma_P(T)$ and $\sigma_P(T^{\dagger})$ and the residual spectrum $\sigma_{\rho}(T)$ and $\sigma_{\rho}(T^{\dagger})$. For the $||T||$ I have found: $||T|| = 1$ Then for the point spectrum I try with: $\lambda x_1 = x_2$ $\lambda x_2 = x_1$ $\lambda x_n = \frac{1}{n}x_{n+1}$ I found easily that some eigenvalues are $\lambda_n =0,\pm1$, but I have a problem, when I study the case $\lambda \neq\lambda_n$, I found the eigenvector: $v_{\lambda} = (x_1,\lambda x_1, x_3, 3\lambda x_3, 3\cdot4\lambda^2x_3,....)$ But I don't understand the condition that the $\lambda$ have to satisfies for $v_{\lambda} \in \ell_2 $ For adjoint $T^{\dagger}$ I have found: $\sigma_P(T^{\dagger}) = \{\lambda = \pm 1 \}$ $\sigma_{\rho}(T^{\dagger}) = \{z = 0\}$ It is correct?","['functional-analysis', 'spectral-theory', 'operator-theory', 'hilbert-spaces']"
1812785,"Does there exist a positive irrational number $\alpha $, such that for any positive integer $n$ the number $\lfloor n\alpha \rfloor$ is not a prime?","Does there exist a positive irrational number $\alpha $, such that for any positive integer $n$ the number $\lfloor n\alpha \rfloor$ is not a prime? My try if $\alpha=\sqrt{17}$ then $\lfloor n\alpha \rfloor=4n$",['number-theory']
1812787,"Why is $\log z = \ln r + i\theta$ ($r>0, \alpha <\theta < \alpha + 2\pi$) discontinuous at $\alpha$?","In one book on complex variables it is written that, given the function $\log z = \ln r + i\theta$ (for proper citation, let's call it function (2), as in the book) ($r>0, \alpha <\theta < \alpha + 2\pi$), where $\alpha \in \mathbb{R}$ and $z\in\mathbb{C}$, it is discontinuous at $\theta = \alpha$ because: Note that if the function (2) were to be defined on the ray $\theta = \alpha$, it would not be continuous there. For if $z$ is a point on that ray, there are points arbitrarily close to $z$ at which the values of $v$ are near $\alpha$ and also points such that the values of $v$ are near $\alpha+2\pi$. [We also note to the above excerpt that $u(r,\theta) = \ln r$ and $v(r,\theta)=\theta$.] Unfortunately, I don't understand what the quote above means, i.e. why this function would be discontinuous at $\theta = \alpha$. Would someone please clarify this for me?","['logarithms', 'complex-analysis', 'continuity']"
1812811,Trivial or not: Dirac delta function is the unit of convolution.,"My task is to prove that the Dirac delta function is the unit of convolution and all I find always is this formula but no further explanation: $$[f*\delta](t)=\int_{-\infty}^{\infty}f(t-\sigma)\delta(\sigma)d\sigma=f(t)$$ Should I see from this that it's true, or should I use it as the beginning of the proof. If it is trivial then why and if not then what should I do?","['complex-analysis', 'dirac-delta', 'convolution']"
1812829,Showing that $\int_{0}^{1}{\sqrt{1-x^4}\over 1+x^4}dx={\pi\over 4}$,"Integrate $$I=\int_{0}^{1}{\sqrt{1-x^4}\over 1+x^4}dx={\pi\over 4}$$ Substitution $x=\sqrt{\tan(u)}\rightarrow dx={\sec^2(u)\over 2\sqrt{\tan(u)}}du$ $x=1\rightarrow u={\pi\over 4}$ $x=0\rightarrow u=0$ $$I={1\over 2}\int_{0}^{{\pi\over 4}}{\sqrt{1-\tan^2(u)}\over 1+\tan^2(u)}\cdot {\sec^2(u)\over \sqrt{\tan(u)}}du$$ $$I={1\over 2}\int_{0}^{{\pi\over 4}}{\sqrt{1-\tan^2(u)\over \tan(u)}}du$$ $$I={1\over 2}\int_{0}^{{\pi\over 4}}{\sqrt{\cot(u)-\tan(u)}}du$$
Recall $$\cot(u)-\tan(u)={\cos^2(u)-\sin^2(u)\over \sin(u)\cos(u)}=2\cot(2u)$$ Substitute back into I $$I={1\over 2}\int_{0}^{{\pi\over 4}}{\sqrt{2\cot(2u)}}du$$ $$I={\sqrt2\over 2}\int_{0}^{{\pi\over 4}}{\sqrt{\cot(2u)}}du$$ Well I know that $$\int{\cos(2u)\over\sin(2u)}du={1\over 2}\ln(\sin(2u))+C$$ but $$\int\sqrt{{\cos(2u)\over\sin(2u)}}du$$ I have not idea, so can anyone please give a hand? Thank.",['integration']
1812869,Could I learn linear algebra before or along with calculus? (Same with differential equations),"I want to plan the next few subjects I learn (by self-study) in mathematics. I have made it through the equivalent of maybe half a Calculus I class so far, but I would like to start a bit on linear algebra and/or differential equations (which I will not do until maybe I work on my calculus some more). Is it a good idea to take this path?","['soft-question', 'ordinary-differential-equations', 'linear-algebra', 'calculus']"
1812922,Help visualizing solutions to the (1D) wave equation.,"I know that the one-dimensional wave equation can be written as $$ \frac{\partial^2 u}{\partial t^2} = c^2 \frac{\partial ^2 u}{\partial x^2}$$ and has solutions of the form $$ u = F(x+ct) + G(x-ct)$$ I'm having trouble developing a proper intuition about the meaning of the solution, though. I superficially understand that it's the sum of two functions ""travelling"" in different directions with time, but that doesn't help me be able to really visualize what solutions look like. More specifically, I'd like to develop an intuitive or visual understanding of what solutions to the wave equation have in common, and what separates them from functions that aren't solutions.","['wave-equation', 'ordinary-differential-equations']"
1812929,Finding the second derivative of$f(x)=x^2\sqrt{4-x}$,"Find the second derivative of the function following: $$f(x)=x^2\sqrt{4-x}$$ Here I go... $$f(x)= x^2(4-x)^{1\over 2}$$
\begin{align*}
f'(x) &= 2x(4-x)^{1\over 2}+{1\over 2}x^2(4-x)^{-{1\over 2}}(-1)\\ 
&= 2x(4-x)^{1\over 2} - {1 \over 2}x^2(4-x)^{-{1\over 2}}\\ 
&={1\over 2}x(4-x)^{-{1\over 2}}[4(4-x)-x]\\ 
&= {1\over 2}x(4-x)^{-{1\over 2}}(16-5x)\\
f''(x)&= {-{1\over 4}}x(4-x)^{-{3\over 2}}(-1)({1\over 2})(16-5x)+(-5)[{1\over 2}x(4-x)^{-{1\over 2}}]\\
&= {-{1\over 8}}x(4-x)^{-{3\over 2}}(16-5x)-{5\over 2}(4-x)^{-{1\over 2}}\\
&= {-{1\over 8}}x(4-x)^{-{3\over 2}}[(16-5x)-20(4-x)]\\
&={-{1\over 8}}x(4-x)^{-{3\over 2}}(-64+15x^2)
\end{align*} I think I messed up on the second derivative. Could anyone show me the steps for the second derivative? Thanks!","['derivatives', 'calculus', 'proof-verification']"
1813997,Different representation of $\sin^2x$,"I'm learning trigonometry, and I was just looking at the $y = \sin^2(x)$ graph. To me it looks the same as a $y = -\cos(x)$ shifted up. More specifically, it looks like $y = -0.5\cos(2x) + 0.5$ . Are these two functions the same? And this is my first post here so I'm not sure how good of a question this is. Thanks.",['trigonometry']
1814010,"If $x$ and $y$ are positive numbers less than $20$ for which $x+y+xy=76$, what is $x+y$?","What is a simple way to solve this problem? I can do it by trying $x$ and $y$, starting from $1$. That does not look like the best way. If $x$ and $y$ are positive numbers less than $20$ for which $x+y+xy=76$, what is $x+y$?","['algebra-precalculus', 'elementary-number-theory']"
1814013,Power sets question about $\subseteq$ and $\in$,"I have a question with regards to this question as follows: Let $A = \{1,2,\{1,2\}\}$. Determine whether the following statements are true or false with a brief explanation of why. a) $\{1,2\} \subseteq P(A)$ b) $\{1,2\} \in P(A)$ In this case, I am not sure why $a)$ false since I wrote out $P(A): \{\emptyset,\{1\},\{2\},\{\{1,2\}\},\{1,2\},\{1,\{1,2\}\},\{2,\{1,2\}\},\{1,2,\{1,2\}\}\}$ Then is it not the case that $\{1,2\} \subseteq P(A)$ ? I think I need some clarification about the difference of $\subseteq$ and $\in$. I had always thought that if it was listed without braces inside an overall brace of the entire set, then it would be written as $\subseteq$ since I thought of it as ""the set containing the set"". In this case, I know that $1 \in A$ and $\{1,2\} \subseteq A$ but I am not sure about the first two statements since the solutions stated that statement $a)$ is false while statement $b)$ is false. Would someone please help me correct my understanding ? Thank you",['elementary-set-theory']
1814024,"Two diagonal matrices each other's entries rearranged (same eigenvalues and multiplicities), are they similar?","Two diagonal matrices each other's entries rearranged (same eigenvalues and multiplicities), are they similar? This seems like such a simple question, but I can't quite see a connection. I want to say that the two diagonal matrices can commute with each other, and this gives us the algebraic edge to show their similarity, but I'm generally bad with abstract algebra proofs.","['matrices', 'abstract-algebra', 'linear-algebra']"
1814027,Derived pushforward of exceptional divisors,"Let $f: X \to Y$ be a birational morphism between projective, smooth varieties. Suppose $E$ is an effective exceptional divisor of $f$. Then it is well known that $f_*(\mathcal{O}_X(E)) = \mathcal{O}_Y$ (see for example Debarre's ""Higher-dimensional algebraic geometry"" Page 177, section 7.12). My question is how does derived pushforward $\mathbf Rf_* \mathcal{O}_X(E)$of $\mathcal{O}_X(E)$ look like? Is it still $\mathcal{O}_Y$?","['homology-cohomology', 'algebraic-geometry', 'birational-geometry']"
1814031,"Find a function that is continuous in usual topology, discontinuous in lower limit topology","I had found a function that was continuous in the lower limit topology but not the usual topology Show the Heaviside step function is continuous in $(\mathbb{R}, \mathcal{T}_\text{lower limit})$ $$h(x):=\begin{cases} 1, & x \geq0\\ 0, & x <0 \end{cases}$$ $h(x)$ is not continuous in the usual topology because the preimage of $1$ is $[0, \infty)$ which is not open. Now I need a function that is continuous in usual topology (meaning: $f: \mathbb{R}_{usual} \to \mathbb{R}_{usual}$ is continuous) But
discontinuous in lower limit topology $(f: \mathbb{R}_{lower limit} \to \mathbb{R}_{lower limit}$ is discontinuous). I was thinking a function whose preimage is of the form $[a,b]$ , is this line of thinking correct? What would be such a function?","['general-topology', 'functions']"
1814092,Reducing $\tan\frac{\pi}{16} + 2\tan\frac{\pi}{8} +4$ to $\cot\frac{\pi}{16}$,"$$\text{The value of}\quad\tan\frac{\pi}{16} + 2\tan\frac{\pi}{8} +4 \quad\text{is equal to _______.}$$
  (Answer: $\cot\frac{\pi}{16}$) I solved the question by the identity 
$$\tan \phi = \cot\phi-2\cot 2\phi$$ 
and got the right answer. However, I want to get some other way so that I can solve the question using basic expansions of tangent, rather than using such an uncommon identity (which I had to look up in the book).",['trigonometry']
1814099,Condition on $a$ for $(x^2+x)^2+a(x^2+x)+4=0$,Find the set of values of $a$ if $$(x^2+x)^2+a(x^2+x)+4=0$$ has $(i)$ All four real and distinct roots $(ii)$ Four roots in which only two roots are real and distinct. $(iii)$ All four imaginary roots $(iv)$ Four real roots in which only two are equal. Now if I set $x^2+x=t$ then even if $t^2+at+4=0$ has real roots in is not necessary that $(x^2+x)^2+a(x^2+x)+4=0$ will have real roots too.  So how to derive the condition on a? Could someone give me some direction?,"['algebra-precalculus', 'quadratics']"
1814118,Solution set for $\lfloor x\rfloor\{x\}=1$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question What is the solution set for $\lfloor x\rfloor\{x\}=1$ , where $\{x\}$ and $\lfloor x\rfloor$ are respectively fractional part and greatest integer function of $x$. P.S.: the answer is $\{m+1/m:m\in\Bbb N\setminus\{1\}\}$. Please explain me the procedure and logic behind these kind of problems .","['relations', 'functions']"
1814187,"If $\sin \theta+\cos\theta+\tan\theta+\cot\theta+\sec\theta+\csc\theta=7$, then $\sin 2\theta$ is a root of $x^2 -44x +36=0$ My own bonafide attempt. [duplicate]","This question already has answers here : If $\sin A + \cos A + \tan A + \cot A + \sec A + \csc A = 7$ then $x^2 - 44x - 36 = 0$ holds for $x=\sin 2A$ (2 answers) Closed 8 years ago . $$ 0<\theta<\pi/2$$
  and $$\sin\theta+\cos\theta+\tan\theta+\cot\theta+\sec\theta+\csc\theta=7$$
  then show that $\sin 2\theta$ is a root of the equation $$x^2 -44x +36=0$$ I tried to use the above given equation of all the trigonometric ratios, though I ended up with an expression of $\sin\theta\cos\theta$. But this too is in the form of $\sin\theta$ and $\cos\theta$ which was $\sin\theta\cos\theta=\frac{1}{6-\sin\theta-\cos\theta}$. When I put that in the quadratic equation, it would again transform into the form of $\sin\theta$ and $\cos\theta$, and hence at last I couldn't prove the thing. Even hints would work, as I would like to solve the question myself.",['trigonometry']
1814197,I need an intuitive explanation of eigenvalues and eigenvectors,"Been browsing around here for quite a while, and finally took the plunge and signed up. I've started my mathematics major, and am taking a course in Linear Algebra. While I seem to be doing rather well in all the topics covered, such as vectors and matrix manipulation, I am having some trouble understanding the meaning of eigenvalues and eigenvectors. For the life of me, I just cannot wrap my head around any textbook explanation (and I've tried three!), and Google so far just hasn't helped me at all. All I see are problem questions, but no real explanation (and even then most of those are hard to grasp). Could someone be so kind and provide a layman explanation of these terms, and work me through an example? Nothing too hard, seeing as I'm a first year student. Thank you, and I look forward to spending even more time on here!","['eigenvalues-eigenvectors', 'intuition', 'linear-algebra']"
1814216,Is $ \sin: \mathbb{N} \to \mathbb{R}$ injective?,"I was trying to show that $\sin(x)$ is non-zero for integers $x$ other than zero and I thought that this result might emerge as a corollary if I managed to show that the result in question is true. I think it's possible to demonstrate this by looking at the power series expansion of $\sin(x)$ and assuming that we don't know anything about the existence of $\pi$. All of the answers below insist that the proposition '$\exists p,q \in \mathbb{Z}, \sin(p)=\sin(q)$' -where $\sin(x)$ is the power series representation-is undecidable without using the properties of $\pi$. If so this is a truly wonderful conjecture and I would like to be provided with a proof. Until then, I insist that methods for analyzing infinite series from analysis should suffice to show that the proposition is false. Note: In a previous version of this post the question said ""bijective"" instead of ""injective"". Some of the answers below have answered the first version of this post.","['number-theory', 'real-analysis', 'trigonometry', 'analytic-number-theory']"
1814247,Open set in $\mathbb{R}^n$ is countable union of rectangles,"Prove that any open set $U \subset \mathbb{R}^n$ can be represented as countable union of open rectangles with sides parallel to the axes. Proof: Let $U$ be an open set in $\mathbb{R}^n$ then for any $x=(x_1,x_2,\dots,x_n)\in U$ exists $\varepsilon_x>0$ such that $N_{\varepsilon_x}(x)\subset U$ . But rectangle $V^{\varepsilon_x}_x=(x_1-\frac{\varepsilon_x}{\sqrt{n}},x_1+\frac{\varepsilon_x}{\sqrt{n}})\times \dots\times(x_n-\frac{\varepsilon_x}{\sqrt{n}},x_n+\frac{\varepsilon_x}{\sqrt{n}})$ lies in $N_{\varepsilon_x}(x)$ . It's obvious that collection $\{V^{\varepsilon}_x: x\in \mathbb{R}^n, \varepsilon>0\}$ is base of $\mathbb{R}^n$ .  Using this problem we conclude that $U$ can be represented as countable union of some subset of this collection. EDIT: Every open set $U\subset \mathbb{R}^n$ we can write in the following way: $U=\bigcup \limits_{x\in U}V_x^{\varepsilon_x}$ Is my proof correct? Would be very thankful for comments and remarks!","['general-topology', 'proof-verification']"
1814293,Does pointwise convergence of continuous functions on a compact set to a continuous limit imply uniform convergence on that set?,"The problem is in Marsden - Elementary Classical Analysis 2nd ed. Ch.5. My approach was Condition : $f_k, f(\in\mathcal C) : A\to N$, $f_k \to f \ \text{(pointwise)}$ and $A$ is compact. Since $A$ is compact, the range of $f_k, f$ are all compact, so bounded. Since $f_k, f\in\mathcal C_b$, if $\|f_k-f\|\to 0$ in $\mathcal C_b$ then $f_k\to f \ \text {(uniformly)}$. when the norm of $\mathcal C_b$ is defined by
$\|f\|=\sup\{\|f(x)\||x\in A\}$. I got stucked in 3, showing $\|f_k-f\|\to 0$. I don't even know the proposition is true. Can anyone give me some hint to solve this or a counterexample for this?",['functional-analysis']
1814339,The way of calculate $\frac{\partial}{\partial x} \ A^x $,"Problem: The square matrix $A\in\mathbf{R}^{n\times n}$ is not function of $x\in \mathbf{N}$. Then, how do to calculate the following matrix derivative?
  $$\frac{\partial}{\partial x} \ A^x $$ I could not find such calculus on any website and materials as far as I know. Do you know any documents, papers or theorems which are related with this? Or how is derivation shown? My main interest: Furthermore, I have already shown a proof when the matrix $A$ is able to diagonalize. But, I cannot find how to approach in other cases or the general case.","['matrices', 'matrix-calculus', 'linear-algebra', 'derivatives']"
1814357,Sorting prime numbers on two sets of equals weights,"Lets denote $(p_n)$ the sequence of all prime numbers $(p_1=2, p_2=3,\ldots)$. The conjecture is the following. For $n$ odd and greater than $2$, $$\exists I \subset \{1,\ldots ,n\} \quad \sum_{i\in I } p_i=\sum_{i\in\{1,\ldots,n\}\setminus I} p_i.$$ For instance, $n=3$ works because $2+3=5$ $(I=\{1,2\})$. But also $n=5$ because $2+5+7=3+11$ $(I=\{1,2\})$. And also $n=7$ because $2+3+7+17=5+11+13$ $(I=\{1,2\})$. We can prove that it can't work for $n$ even by parity (we would have an odd number of odd prime numbers). We can reformulate the problem like this: For $n$ odd and greater than $2$, $$\exists I \subset \{1,\ldots ,n\} \quad \sum_{i\in I } p_i=\frac 12\sum_{i\leqslant n} p_i.$$ I honestly don't know how to start here, and I don't even know if this could be true. Do you have any leads ?","['number-theory', 'prime-numbers', 'arithmetic']"
1814430,"Show that $\int_0^\infty\frac{1}{1+x^n}\,\mathrm dx = \frac{\pi/n}{\sin(\pi/n)}$ for $\mathbb{N}\ni n\geq 2$","Show that $$\int_0^\infty\frac{1}{1+x^n}\,\mathrm dx = \frac{\pi/n}{\sin(\pi/n)}$$ for $\mathbb{N}\ni n\geq 2$. Let $S=\{r\mathrm e^{\mathrm i\varphi}\in\mathbb{C} \mid 0\leq r\leq R,0\leq \varphi\leq (2\pi)/n\}$ and we will integrate along $\partial S$ for $R\to\infty$. With $\gamma(t)=R\mathrm e^{\mathrm it}, t\in[0,(2\pi)/n]$ and $\delta=[R\mathrm e^{\mathrm i \frac{2\pi}{n}}, 0]$ we can write this integral as $$
\oint_{\partial S}\frac{1}{1+z^n}\,\mathrm dz
= \underbrace{\int_0^R \frac{1}{1+z^n}\,\mathrm dz}_{=: I_R}
+ \int_\gamma\frac{1}{1+z^n}\,\mathrm dz
+ \int_{\delta} \frac{1}{1+z^n}\,\mathrm dz.
$$ Fortunately the integral along the outer arc $\gamma$ vanishes since $$
\left|\int_\gamma\frac{1}{1+z^n}\,\mathrm dz\right|
\leq \int_0^{(2\pi)/n}\left|\frac{R\mathrm i\mathrm e^{\mathrm it}}{R^n\mathrm e^{n\mathrm it}+1}\right|\,\mathrm dt
\overset{(*)}{\leq} \frac{R}{(R-1)^n}\frac{2\pi}{n} \overset{R\to\infty}{\longrightarrow} 0.
$$ Furthermore we have $$
\int_{\delta} \frac{1}{1+z^n}\,\mathrm dz
= - \int_0^R \frac{\mathrm e^{\mathrm i \frac{2\pi}{n}}}{(t\mathrm e^{\mathrm i \frac{2\pi}{n}})^n+1}\,\mathrm dt
= - \int_0^R \frac{\mathrm e^{\mathrm i \frac{2\pi}{n}}}{t^n+1}\,\mathrm dt
= - \mathrm e^{\mathrm i \frac{2\pi}{n}}I_R.
$$ The singularities of the integrand are obtained via $z^n+1=0\Leftrightarrow z_k=\mathrm e^{\mathrm i \frac{\pi}{n}}\mathrm e^{\mathrm i k\frac{2\pi}{n}}$ for all $k=0,\ldots,n-1$. Since only $z_0$ lies in $S$ this is the only singularity we have to consider for the residue theorem. It follows that $$
\oint_{\partial S}\frac{1}{1+z^n}\,\mathrm dz
= 2\pi\mathrm i\operatorname{Res}_{z_0}\left(\frac{1}{1+z^n}\right)
= 2\pi\mathrm i\frac{1}{nz_0^{n-1}}
= 2\pi\mathrm i\frac{1}{n\mathrm e^{\mathrm i \frac{(n-1)\pi}{n}}}
= -\frac{2}{n}\pi\mathrm i\mathrm e^{\mathrm i \frac{\pi}{n}}.
$$ We are interested in the value $I=\lim_{R\to\infty} I_R$ hence we can write $$
-\frac{2}{n}\pi\mathrm i\mathrm e^{\mathrm i \frac{\pi}{n}}
= I - \mathrm e^{\mathrm i \frac{2\pi}{n}}I.
$$ Now we can solve for $I$ which yields $$
I = \frac{-\frac{2}{n}\pi\mathrm i\mathrm e^{\mathrm i \frac{\pi}{n}}}{1 - \mathrm e^{\mathrm i \frac{2\pi}{n}}}
= \frac{\pi}{n}2\mathrm i\frac{- \mathrm e^{\mathrm i \frac{\pi}{n}}}{1- \mathrm e^{\mathrm i \frac{2\pi}{n}}}
= \frac{\pi}{n}\frac{2\mathrm i}{\mathrm e^{\mathrm i \frac{\pi}{n}} - \mathrm e^{-\mathrm i \frac{\pi}{n}}}
= \frac{\pi/n}{\sin(\pi/n)}.
$$ I am curious whether this suffices and/or you have other suggestions to further improve my solution. Furthermore I am indecisive about my inequality at $(*)$ regarding the vanishing integral along $\gamma$. This inequality for example does not hold for $R=1/2$ and $n=3$ - is it reasonable to assume that $R\geq 1$ since we are analyzing $R\to\infty$ anyways? If so I would argue that $R\geq 1\implies R^n-1>0$ and therefore $R^n-1\geq (R-1)^n$. Are there alternative justifications for this estimation?","['alternative-proof', 'complex-analysis', 'residue-calculus', 'proof-verification']"
1814438,Order of point in divisor,"Let $y^2=f(x)$ be hyperelliptic curve over $k$ and $(a,\sqrt{f(a)})$ point on the curve and let $b\in k$.
I would like to prove that divisor of the  function $g(x,y)=\frac{x-b}{x-a}$ is equal to $$div(g)=(b,\sqrt{f(b)})+(b,-\sqrt{f(b)})-(a,\sqrt{f(a)})-(a,-\sqrt{f(a)}).$$ It is easy to show that first two points are the only zero points and other two are the only poles, we also have point in infinity $(0:1:0)$ but $g$ has value not equal to $0$ in that point so order of point of infinity is $0$ and is not in divisor. I can  also show that, for example, the uniformizer in $P=(b,\sqrt{f(b)})$ is $y-\sqrt{f(b)}$ bu I don't know how to use it to show that  order of that point is $1$. I also know that $ord_P(g)=ord_P(x-b)+ord_p(x-a)=ord_P(x-b)$ since $x-a$ doesn't vanish in $P$.  If my curve was $\mathbb{P}^2$ I know I could conclude it is $1$ since it is zero point with multiplicity $1$? But what can I conclude here? EDIT: I forgot to write that $(a,\sqrt{f(a)})$ is  the point on curve in field $k$ and that could maybe help for some ideas?","['algebraic-curves', 'riemann-surfaces', 'divisors-algebraic-geometry', 'algebraic-geometry']"
