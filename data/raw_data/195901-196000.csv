question_id,title,body,tags
3773091,"If $n > 1$, there are no non-zero $*$-homomorphisms $M_n(\Bbb{C}) \to \Bbb{C}$","If $n > 1$ , there are no non-zero $*$ -homomorphisms $M_n(\Bbb{C}) \to \Bbb{C}$ . A $*$ -homomorphism is an algebra morphism $\varphi: M_n(\Bbb{C}) \to \Bbb{C}$ with $\varphi(\overline{A}^T) = \overline{\varphi(A)}$ I tried to show that every $*$ -morphism must be zero: if $\varphi$ is such a $*$ -morphism, then $E_{ij}^2 = 0$ for $i \neq j$ (here $E_{ij}$ is the matrix that is $1$ on position $(ij)$ , $0$ elsewhere). Thus $\varphi(E_{ij}) = 0$ for $i \neq j$ . Hence, $$\varphi(A) = \sum_i a_{ii} \varphi(E_{ii})$$ so if I can show that $\varphi(E_{ii}) = 0$ I will be done. Unfortunately, I can't see why this should be true. I did not yet fully use the fact that it is a $*$ -morphism. The only thing I can see is that $\varphi(A) \in \Bbb{R}$ for all $A$ since $\varphi(E_{ii})= \varphi(\overline{E_{ii}}^T) = \overline{\varphi(E_{ii}})$ . If $\varphi$ is non-zero, then there is $A$ with $\varphi(A) \in \Bbb{R}\setminus \{0\}$ . But then $\varphi(iA) = i\varphi(A) \notin \Bbb{R}$ , a contradiction. Thus $\varphi=0$ . Is this correct?","['matrices', 'solution-verification', 'involutions', 'functional-analysis']"
3773116,Show by counting two ways that $\sum_{i=1}^{n}i(n-i)=\sum_{i=1}^{n}{i\choose 2}={n+1 \choose 3}$?,"I'm trying to solve the following problem: I am trying to understand what he is counting in two ways in the first and second equality. I noticed that ${i \choose 2}=\frac{i(i-1)}{2}$ is the sum of the first $(i-1)$ integers. I did the following: I expanded both sums and showed that they are equal. I wrote ${n+1 \choose 3}=\frac{(n+1)n(n-1)}{6}$ and showed it is equal to one of the sums. But judging from the section where the author writes about counting in two ways, it seems something else should be done: I struggled a lot to find a figure such as the one he found in this example. Although I understood that ${i \choose 2}=\frac{i(i-1)}{2}$ and hence that $\sum_{i=1}^{n} {i \choose 2}$ is the sum of the sums of the first $i$ integers, I couldn't match the first sum to it in any meaningful way, I also don't know how to match the sums to ${n+1 \choose 3}$ .","['combinatorics', 'combinatorial-proofs']"
3773117,"Variational derivative of $F_w(f) = \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 f(x_1)w(x_1,x_2)f(x_2)$","I am starting to work with functional derivatives. In Density functional theory an advanced course by Engel and Dreizler they have the example of the functional in the title of this question. The variation of the functional can be calculated to be $$\delta F_w = \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 w(x_1,x_2)[(f(x_1)\varepsilon \eta(x_2) + f(x_2) \varepsilon \eta(x_1) + \varepsilon \eta(x_1) \varepsilon \eta(x_2)] $$ The Taylor Expansion around $\varepsilon = 0$ is $$ \delta F_w = \frac{dF(f + \varepsilon \eta)}{d \varepsilon} \bigg|_{\varepsilon = 0} \varepsilon + \frac{d^2F(f + \varepsilon \eta)}{d \varepsilon^2} \bigg|_{\varepsilon = 0} \varepsilon^2= \\= 
 \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 w(x_1,x_2)(f(x_1)\eta(x_2) + f(x_2) \eta(x_1)) \varepsilon + \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 w(x_1,x_2)2 \eta(x_1) \eta(x_2) \varepsilon^2 $$ By the definition of functional derivative I get that in the first term I need to isolate $\eta(x_1)$ , this can be done by rewriting it in the following way $$\frac{dF(f + \varepsilon \eta)}{d \varepsilon} \bigg|_{\varepsilon = 0} =   \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 (w(x_1,x_2) + w(x_2,x_1))f(x_2)\eta(x_1)$$ Which means that the first functional derivative is $$ \frac{\delta F}{\delta f(x_1)} = \int_{y_1}^{y_2} dx_2 (w(x_1,x_2) + w(x_2,x_1))f(x_2)$$ So far, this agrees with the textbook, however there is something I do not understand in the second derivative, and it might be just some simple algebra issue. We have that $$ \frac{d^2F(f + \varepsilon \eta)}{d \varepsilon^2} \bigg|_{\varepsilon = 0} = \int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 w(x_1,x_2)2 \eta(x_1) \eta(x_2)$$ From the definition of the second variational derivative I conclude from the above that $$ \frac{\delta^2 F_w}{\delta f(x_1) \delta f(x_2)} = 2 w(x_1,x_2)$$ However the textbook writes that $$\frac{\delta^2 F_w}{\delta f(x_1) \delta f(x_2)} = w(x_1,x_2) + w(x_2,x_1)$$ I get that (unless there is something I am not seeing) $$\int_{y_1}^{y_2} dx_1 \int_{y_1}^{y_2} dx_2 w(x_1,x_2)2 \eta(x_1) \eta(x_2) = \int_{y_1}^{y_2} dx1 \int_{y_1}^{y_2} dx_2 (w(x_1,x_2) + w(x_2,x_1)) \eta(x_1) \eta(x_2) $$ However, $2w(x_1,x_2) \neq w(x_1,x_2) + w(x_2,x_1)$ unless $w(.,.)$ is symmetric, so my solution to the second derivative differs from the one in the textbook. This might be just some simple algebra I am not seeing, what am I missing?","['derivatives', 'calculus-of-variations']"
3773120,Missing angle problem,"In a isoceles triangle ABC having equal angles as B=C=80°, B is joined to a point D on AC such that AD=BC. Find angle BDC. By construction, its coming out to be 30°, is there any way to it without direct measuring?","['euclidean-geometry', 'geometry']"
3773124,How to prove that $\lim\limits_{n \to \infty} \frac{(n!)^n}{n^{n^2}}=0$?,I am trying to prove that $$\lim\limits_{n \to \infty} \frac{(n!)^n}{n^{n^2}}=0.$$ Let $a_n= \dfrac{(n!)^n}{n^{n^2}}$ . My initial thought was to use the direct comparison test $$\frac{1}{n^2}< a_n < b_n$$ but I cannot find a $b_n > a_n:  b_n = 0$ . For example I tried $$b_n= \left( \frac{n!}{n} \right)^n =((n-1)!)^n \to \infty.$$ Any ideas on how to find a proper $b_n$ or other approaches to prove this limit?,"['limits', 'calculus', 'convergence-divergence', 'sequences-and-series']"
3773134,Fubini's theorem for integrable functions.,"I have gone through the proof of Fubini's theorem for non-negative measurable functions from the book An Introduction to Measure and Integration by Inder K Rana. The satement of the theorem is as follows $:$ Theorem $1$ $:$ Let $(X \times Y, \mathcal A \otimes \mathcal B, \mu \times \nu)$ be the product measure space induced by the $\sigma$ -finite measure spaces $(X,\mathcal A, \mu)$ and $(Y,\mathcal B, \nu).$ Then for any non-negative $\mathcal A \otimes \mathcal B$ - measurable function $f,$ the following staements hold $:$ $($ i $)$ For any $x_0 \in X,y_0 \in Y$ the maps $x \longmapsto f(x,y_0)$ and $y \longmapsto f(x_0,y)$ are $\mathcal A$ -measurable and $\mathcal B$ -measurable respectively. $($ ii $)$ The map $x \longmapsto \displaystyle {\int_{Y}} f(x,y)\ d\nu(y)$ is $\mathcal A$ -measurable and the map $y \longmapsto \displaystyle {\int_{X}} f(x,y)\ d\mu(x)$ is $\mathcal B$ -measurable. $($ iii $)$ $\displaystyle {\int_{X}} \left ( \displaystyle {\int_{Y}} f(x,y)\ d\nu(y) \right ) d\mu(x) = \displaystyle {\int_{Y}} \left ( \displaystyle {\int_{X}} f(x,y)\ d\mu(x) \right ) d\nu(y) = \displaystyle {\int_{X \times Y}} f(x,y)\ d(\mu \times \nu) (x,y).$ The general version of the above theorem states as follows $:$ Theorem $2$ $:$ Let $(X \times Y, \mathcal A \otimes \mathcal B, \mu \times \nu)$ be the product measure space induced by the $\sigma$ -finite measure spaces $(X,\mathcal A, \mu)$ and $(Y,\mathcal B, \nu).$ Then for any $f \in L_1 (\mu \times \nu),$ the following staements hold $:$ $($ i $)$ The maps $x \longmapsto f(x,y)$ and $y \longmapsto f(x,y)$ are $\mu$ -integrable a.e. $y(\nu)$ and $\nu$ -integrable a.e. $x(\mu)$ respectively. $($ ii $)$ The map $x \longmapsto \displaystyle {\int_{Y}} f(x,y)\ d\nu(y)$ is $\mu$ -integrable a.e. $x(\mu)$ and the map $y \longmapsto \displaystyle {\int_{X}} f(x,y)\ d\mu(x)$ is $\nu$ -integrable a.e. $y(\nu).$ $($ iii $)$ $\displaystyle {\int_{X}} \left ( \displaystyle {\int_{Y}} f(x,y)\ d\nu(y) \right ) d\mu(x) = \displaystyle {\int_{Y}} \left ( \displaystyle {\int_{X}} f(x,y)\ d\mu(x) \right ) d\nu(y) = \displaystyle {\int_{X \times Y}} f(x,y)\ d(\mu \times \nu) (x,y).$ I tried to prove the above theorem with the help of Theorem $1.$ Here's what I did $:$ My attempt $:$ Let $f^+$ and $f^-$ be the positive and the negative part of the function $f$ respectively. Since $f \in L_1(\mu \times \nu),$ $f^+$ and $f^-$ are both non-negative $\mathcal A \otimes \mathcal B$ -measurable functions. Applying Theorem $1$ $($ iii $)$ to $f^+$ and $f^{-}$ we have \begin{align*}\displaystyle {\int_{X}} \left ( \displaystyle {\int_{Y}} f^+(x,y)\ d\nu(y) \right ) d\mu(x) = \displaystyle {\int_{Y}} \left ( \displaystyle {\int_{X}} f^+(x,y)\ d\mu(x) \right ) d\nu(y) & = \displaystyle {\int_{X \times Y}} f^+(x,y)\ d(\mu \times \nu) (x,y) \\ & \leq \displaystyle {\int_{X \times Y}} |f(x,y)|\ d(\mu \times \nu) < +\infty. \end{align*} \begin{align*}\displaystyle {\int_{X}} \left ( \displaystyle {\int_{Y}} f^-(x,y)\ d\nu(y) \right ) d\mu(x) = \displaystyle {\int_{Y}} \left ( \displaystyle {\int_{X}} f^-(x,y)\ d\mu(x) \right ) d\nu(y) & = \displaystyle {\int_{X \times Y}} f^-(x,y)\ d(\mu \times \nu) (x,y) \\ & \leq \displaystyle {\int_{X \times Y}} |f(x,y)|\ d(\mu \times \nu) < +\infty. \end{align*} This shows that the map $x \longmapsto \displaystyle {\int_Y} f^+(x,y)\ d\nu(y)$ is $\mu$ -integrable, the map $y \longmapsto \displaystyle {\int_X} f^+(x,y)\ d\mu(x)$ is $\nu$ -integrable, the map $x \longmapsto \displaystyle {\int_Y} f^-(x,y)\ d\nu(y)$ is $\mu$ -integrable and the map $y \longmapsto \displaystyle {\int_X} f^-(x,y)\ d\mu(x)$ is $\nu$ -integrable. So the map $y \longmapsto f^+(x,y)$ is $\nu$ -integrable a.e. $x(\mu)$ and the map $y \longmapsto f^-(x,y)$ is $\nu$ -integrable a.e. $x(\mu).$ Hence $y \longmapsto f(x,y)$ is $\nu$ -integrable a.e. $x(\mu).$ Similarly, the map $x \longmapsto f^+(x,y)$ is $\mu$ -integrable a.e. $y(\nu)$ and the map $x \longmapsto f^-(x,y)$ is $\mu$ -integrable a.e. $y(\nu).$ Hence $x \longmapsto f(x,y)$ is $\mu$ -integrable a.e. $y(\nu).$ This proves $($ i $).$ Since $f \in L_1(\mu \times \nu)$ it follows that \begin{align*} \int_{X \times Y} f(x,y)\ d(\mu \times \nu) (x,y) & = \int_{X \times Y} f^+(x,y)\ d(\mu \times \nu) (x,y) - \int_{X \times Y} f^-(x,y)\ d(\mu \times \nu) (x,y) \\ & = \int_X \left ( \int_{Y} f^+(x,y)\ d{\nu(y)} \right ) d{\mu}(x) - \int_X \left ( \int_{Y} f^-(x,y)\ d{\nu(y)} \right ) d{\mu}(x) \end{align*} Now how do I proceed? Any help will be highly appreciated. Thanks in advance.","['integration', 'measure-theory', 'product-space', 'proof-writing', 'fubini-tonelli-theorems']"
3773143,"Find $a$, $b$ such that $x^2 - x -1$ is a factor of $ax^9 + bx^8 + 1$","Find $a$ , $b$ such that $x^2 - x -1$ is a factor of $ax^9 + bx^8 + 1$ The second polynomial can be rewritten as $$ax^9 + bx^8 + 1 = f(x)(x^2 - x - 1)$$ The roots of this polynomial are $\frac{1 \pm \sqrt 5}{2}$ . Substituting one of these roots in this equation gives us: $$a\left( \frac{1 + \sqrt5}{2}\right)^9 + b\left( \frac{1 + \sqrt 5}{2}\right)^8 + 1 = 0$$ I was able to solve this far, but I gave up because the calculation past this point gets too tedious. The textbook has gone ahead and simplified this to $$2^9 a + 2^8b(\sqrt 5 - 1) + (\sqrt5 - 1)^9 = 0$$ after which it simplifies to (divide by $2^8$ and solve the binomial expression) $$2a + b(\sqrt 5 -1) = 76 - 34\sqrt5$$ Is there a more elegant way to solve this problem ? Preferably one that does not include the magical use of a calculator or the evaluation of that ugly binomial expansion?","['polynomials', 'sequences-and-series', 'algebra-precalculus', 'binomial-theorem', 'quadratics']"
3773168,Calculation of the Schur Multiplier (Problem 5A.8(b) Isaacs' Finite Group Theory),"Let $A$ and $B$ be arbitrary finite groups. a. Show that $$|M(A \times B)| \geq |M(A)||M(B)|.$$ b. Assuming the $|A|$ and $|B|$ are coprime, show that $$M(A \times B) \cong M(A) \times M(B)$$ (where $M(G)$ denotes the Schur multiplier of the finite group $G$ ). I answered part $a$ by constructing a central stem extension of $A \times B$ , from the direct product of the Schur representation groups for $A$ and $B$ . I am stuck on part b. I'll detail my approach below. Let $\Gamma$ be a Schur representation group for $A \times B$ . Then there is a surjective map $\pi : \Gamma \to A \times B$ such that $\Gamma / ker\;\pi \cong A \times B$ , and $ker\; \pi \subseteq Z(\Gamma) \cap \Gamma'$ . Let $A^* = \pi^{-1}(A)$ and $B^* = \pi^{-1}(B)$ . So then $G/A^* \cong B$ and $G/B^* \cong A$ , since $|\Gamma:A^*|$ and $|\Gamma:B^*|$ are coprime it follows that $\Gamma = A^* B^*$ . Then $|\Gamma| = |A^*||B^*|/|A^* \cap B^*|$ . Since $\Gamma / A^* \cong B$ it follows that $|B^*|/|A^* \cap B^*| = |B|$ , hence $|A^* \cap B^*| = |ker \; \pi|$ . Since $ker \; \pi \subseteq A^* \cap B^*$ it follows that $ker \; \pi = A^* \cap B^*$ . I suspect that I need to assume that $|M(A \times B)| > |M(A)||M(B)|$ and derive a contradiction. But I'm not sure how to proceed. Any help or comments would be appreciated.","['group-theory', 'finite-groups']"
3773194,Active research on the subject of general topology,"I've lately been told that the subject of general topology, like what Hausdorff and Kolmogorov dealt with, is a a dead subject research wise. I have been wondering however whether this is indeed true, and would like to know whether there are people actively researching general topology or adjacent subjects? I would appreciate perhaps some concrete examples of people or institutions dealing with the subject.","['general-topology', 'advice', 'research']"
3773202,How to evaluate $\sum_{n=1}^{\infty}\:\frac{2n+1}{2n(n+1)^2}$?,"Note: Similar questions have been asked here and here , but this is quite different. I am trying to evaluate $$\sum_{n=1}^{\infty}\:\frac{2n+1}{2n(n+1)^2} \quad (1)$$ I re-wrote the fraction as $$ \frac{2n+1}{2n(n+1)^2} = \frac1{2(n+1)} \cdot \frac{2n+1}{n(n+1)}= \frac1{2(n+1)} \left( \frac1n + \frac1{n+1} \right) = \frac1{2} \left( \frac1{n(n+1)} + \frac1{(n+1)^2} \right) = \frac12 \left( \left( \frac1n -\frac1{(n+1)} \right) + \frac1{(n+1)^2} \right)$$ Hence $$(1) = \frac12 \sum_{n=1}^{\infty}\:  \left( \frac1n -\frac1{(n+1)} \right) + \frac12 \sum_{n=1}^{\infty}\:\frac1{(n+1)^2} = \frac12\lim_{n \to \infty}1-\frac1{n+1}+\frac{\pi^2}{12} = \frac{\pi^2}{12} + \frac12$$ Therefore $$\sum_{n=1}^{\infty}\:\frac{2n+1}{2n(n+1)^2} = \frac{\pi^2}{12} + \frac12$$ I am unsure about $$ \sum_{n=1}^{\infty}\:\frac1{(n+1)^2} = \frac{\pi^2}{12} $$ We know the basic p-series $$ \sum_{n=1}^{\infty}\:\frac1{n^2} = \frac{\pi^2}{6} $$ Is this solution correct?","['convergence-divergence', 'sequences-and-series']"
3773215,Why don't we draw modulus bars when we open 'under root' in indefinite integration?,"Is there a way we can ignore absolute value bars in indefinite integration. Below is the solution of a problem that confused me and not just this problem, I recently noticed that all the problem and questions that I've been doing , I was omitting modulus when opening roots. $$\int{\sqrt{1+\sin x}\,\,dx}$$ Multiplying and dividing by $\sqrt{1-\sin x}\,\,$ , we get, $$\int{\frac{\sqrt{(\cos)^2 x}}{\sqrt{1-\sin x}}\,\,dx}$$ Now in the solution, they opened the root of Cos square x and wrote just Cos x and not |cos x|. Please explain me why is it so that in every indefinite problem I am finding that modulus bars are omitted.
And if it is not so, then correct me where am I wrong , also in just another case of my confusion. If we let, $$\sin x = t$$ then what would be $ \cos x$ $$\cos x = \pm \sqrt{1-t^2}$$ But they usually omit -ve sign and take only $ \sqrt{1-t^2}$ These two are the most confusing parts to me.  I  really need these doubts get cleared
Please help me as I am new to calculus and facing problems :)","['integration', 'calculus', 'trigonometric-integrals', 'indefinite-integrals', 'trigonometry']"
3773220,Evaluate integral $\int (x^2-1)(x^3-3x)^{4/3} \mathop{dx}$,"How can I evaluate this integral $$\int (x^2-1)(x^3-3x)^{4/3} \mathop{dx}=\;\;?$$ My attempt : I tried using substitution $x=\sec\theta$ , $dx=\sec\theta\ \tan\theta d\theta$ , $$\int (\sec^2\theta-1)(\sec^3\theta-3\sec\theta)^{4/3} 
  \sec\theta\ \tan\theta d\theta $$ $$=\int \tan^2\theta \sec^4\theta(1-3\cos^2\theta)^{4/3} \sec\theta\ \tan\theta d\theta $$ $$=\int \tan^3\theta \sec^5\theta(1-3\cos^2\theta)^{4/3}\ d\theta $$ $$=\int\dfrac{ \sin^3\theta}{ \cos^8\theta}(1-3\cos^2\theta)^{4/3}\ d\theta $$ I can't see if this substitution will work or not. This has become so complicated. Please help me solve this integral.","['integration', 'indefinite-integrals', 'calculus']"
3773227,Hoeffding/ANOVA decomposition for vector valued $U$-statistics,"Consider a random variable $Z = f(X_1, \dots, X_n)$ , where $f: {\bf R}^n \to {\bf R}$ and $X_1, \dots, X_n$ IID random variables, and assume that $Z$ and $X_1, \dots, X_n$ all have finite variance. It is well known that there exists a collection of projections $\{\pi_A : L^2 \to L^2\}$ , indexed by subsets $A \subseteq \{1, \dots, n\}$ , such that $$Z = \sum_A \pi_A Z$$ and $\pi_A(Z)$ and $\pi_B(Z)$ are uncorrelated ( $\mathop{{\bf Cov}}[\pi_A(Z), \pi_B(Z)] = 0$ ) whenever $A \neq B$ .   This is the Hoeffding decomposition of $Z$ . Is there an analogous decomposition if $f$ is vector valued, that is, if $Z \in {\bf R}^p$ ? That is, does there exist a set of projections $\{\pi_A\}$ such that $Z = \sum \pi_A Z$ and $$
\mathop{{\bf Cov}}[\pi_A(Z), \pi_B(Z)] = \mathbf{0} \in \mathbf{R}^{p \times p}
$$ for all $A \neq B$ ?","['statistics', 'anova', 'probability']"
3773237,"What axiom of ZFC implies that ""sets have no repeated elements""?","For example, the axiom of pairing says: Let $a$ be a set. Let $b$ be a set. If follows that the set $\{a,b\}$ exists. This can be used to prove the existence of singletons, for instance, by setting $b := a$ (in the previous statement). Namely, the axiom of pairing implies the following: Let $a$ be a set. If follows that the set $\{a\}$ exists. This got me thinking. What ZFC axiom implies that, for any set $a$ , the set $\{a,a\}$ equals the set $\{a\}$ ? Equivalently, what axiom of ZFC implies that the sets of ZFC don't behave like multisets ? (I suspect it's extensionality , but I couldn't argue why. So, if it is extensionality, then I'm gonna need some convincing...)","['elementary-set-theory', 'axioms']"
3773252,Why is $f(x)=C_1e^{\lambda_1x}+C_2e^{\lambda_2x}$ the only solution family to $a\cdot f''(x)+b\cdot f'(x)+c\cdot f(x)=0$,"I was taught that $f(x)=C_1e^{\lambda_1x}+C_2e^{\lambda_2x}$ is the only solution family to $a\cdot f''(x)+b\cdot f'(x)+c\cdot f(x)=0$ (provided that $b^2-4ac\neq0$ ). Here is my teacher's proof showing why: Assume that $f(x)=e^{\lambda x}$ for some $\lambda$ . Then the differential equation becomes: $$a\frac{d^2}{dx^2}e^{\lambda x}+b\frac{d}{dx}e^{\lambda x}+ce^{\lambda x}=0$$ $$a\lambda^2e^{\lambda x}+b\lambda e^{\lambda x}+ce^{\lambda x}=0$$ $$e^{\lambda x}(a\lambda^2+b\lambda+c)=0$$ $$a\lambda^2+b\lambda+c=0$$ Then if $\lambda_1, \lambda_2$ are the roots of the equation $a\lambda^2+b\lambda+c=0$ , $f(x)=e^{\lambda_1 x}$ and $f(x)=e^{\lambda_2 x}$ are solutions to the differential equation. Now suppose $g(x)$ is a solution to the differential equation, then we can show $C\cdot g(x)$ is a solution to the differential equation where $C$ is a real number: $$aC\cdot g''(x)+bC\cdot g'(x)+cC\cdot g(x)=0$$ $$C(a\cdot g''(x)+b\cdot g'(x)+c\cdot g(x)=0)$$ $$a\cdot g''(x)+b\cdot g'(x)+c\cdot g(x)=0$$ which is true by definition. Therefore we know $f(x)=C_1e^{\lambda_1 x}$ and $f(x)=C_2e^{\lambda_2 x}$ are solution sets for real $C_1, C_2$ . We can also show $f_1(x)+f_2(x)$ is a solution to the differential equation provided that $f_1(x)$ and $f_2(x)$ satisfy the equation: $$a(f_1(x)+f_2(x))''+b(f_1(x)+f_2(x))'+c(f_1(x)+f_2(x))=0$$ $$a(f_1''(x)+f_2''(x))+b(f_1'(x)+f_2'(x))+c(f_1(x)+f_2(x))=0$$ $$a\cdot f_1''(x)+b\cdot f_1'(x)+c\cdot f_1(x)+a\cdot f_2''(x)+b\cdot f_2''(x)+c\cdot f_2''(x)=0$$ $$a\cdot f_1''(x)+b\cdot f_1'(x)+c\cdot f_1(x)=0, a\cdot f_2''(x)+b\cdot f_2''(x)+c\cdot f_2''(x)=0$$ both of which are true by definition. Therefore $f(x)=C_1e^{\lambda_1 x}+C_2e^{\lambda_2x}$ is a solution family to the differential equation (the only exception being when $\lambda_1=\lambda_2$ ). However, there is an issue with this proof: it merely shows that $f(x)=C_1e^{\lambda_1x}+C_2e^{\lambda_2x}$ is a solution family, but it does not show that $f(x)=C_1e^{\lambda_1x}+C_2e^{\lambda_2x}$ is the only solution family. In other words, the proof does not show that there are solutions to the equation not of the form $f(x)=C_1e^{\lambda_1x}+C_2e^{\lambda_2x}$ . My question is: How can we show that $f(x)=C_1e^{\lambda_1x}+C_2e^{\lambda_2x}$ is the only solution family",['ordinary-differential-equations']
3773255,Finding the limit : $\lim_{x\to0}\ln(e + 2x)^\frac{1}{\sin x}$,"I tried replacing $x$ with $0$ the log returns $1$ and the $1/\sin x$ returns $1/0$ . So I thought the limit should be infinity. However, graphing the function yields undefined value at $0$ , and the result shows that the limit is $e^{2\,e^ {- 1 }}$ . I have no idea how did they reach that conclusion.",['limits']
3773288,"The minimum $a,b \;\in\;\mathbf{N}$ such that $\frac{a}{b}=0.2017...$","Recently, I have found this problem: Find two natural numbers $a,b$ such that $a+b$ is minimum and: $$\frac{a}{b}=0.2017...$$ where $...$ means that there are other decimal digits (periodic or not) that we don't know. In order to solve this problem, I started considering that the generating fraction of a decimal periodic number of the form $A,B$ is given by: $$\frac{N}{D}=\frac{AB-B}{10^b-1}$$ where $AB$ is not the product of $A$ and $B$ , but the number formed by their union and $b$ is the number of periodic digits. If we consider, also, the anti-period (numbers of the form $A,BC$ ), the formula becomes: $$\frac{N}{D}=\frac{ABC-AB}{10^b(10^c-1)}$$ where, again, $ABC$ and $AB$ don't denote the product and $c$ is the number of digits of the period. In this second case, the reasonement becomes more difficult because $2017$ can be entirely the antiperiod and the periodic digits aren't known. So, how can we go on?","['fractions', 'number-theory', 'elementary-number-theory']"
3773337,Family of Integrals $\int_0^{\frac{\pi}{2}} {\left(\frac{\sin{(ax)}}{\sin{(bx)}}\right)}^{2n} \; dx$,"Background : I want to preface this by saying that I'm not sure if these generalized integrals have been brought up before, but I haven't seen anything on them.   I'm creating this post because I am interested to find out more intriguing information about these integrals, to see if there are any mistakes in my following observations, and maybe even to see if anyone has derivations of these observations ( I used Wolfram Alpha to calculate most of the integrals ). Note, for all of the following integrals assume that $\displaystyle\underline{a > b\ \mbox{and}\ a, b, n, k \in \mathbb{Z}^{+}}$ : Powers of 2 : First, for the generalized integrals with a power of $2$ : $$\int_0^{\frac{\pi}{2}} {\left(\frac{\sin{(ax)}}{\sin{(bx)}}\right)}^2 \; dx = \begin{cases}
\frac{a \pi}{2b} & \text{if} \; a \bmod b=0 &\\
\text{Diverges} & \text{if} \; a \bmod b \neq 0
\end{cases}$$ And making the upper bound dependent on $b$ and just substituting $u=bx$ yields: $$\int_0^{\frac{\pi}{b}} {\left(\frac{\sin{(ax)}}{\sin{(bx)}}\right)}^2 \; dx = \begin{cases}
\frac{a \pi}{b^2} & \text{if} \; a \bmod b=0 \\
\text{Diverges} & \text{if} \; a \bmod b \neq 0
\end{cases}$$ Even powers : Then even powers, although this has been a bit challenging. For $a=2b$ , I found that: $$\int_0^{\frac{\pi}{2}} {\left(\frac{\sin{(2bx)}}{\sin{(bx)}}\right)}^{2n} \; dx = \frac{\pi}{2}  \cdot \frac{(2n)!}{{\left(n!\right)}^2}$$ I evaluated the integrals at varying powers of $n$ using Wolfram, and created a sequence using the coefficients of the result of the integrals.  Then, I used OEIS to recognize the sequence, which is the ""central binomial coefficients"", or sum of squares of entries in the $n^{\text{th}}$ row of the triangle of binomial coefficients. For $a=3b$ , I found that: $$\int_0^{\frac{\pi}{2}} {\left(\frac{\sin{(3bx)}}{\sin{(bx)}}\right)}^{2n} \; dx =\frac{\pi}{2} \displaystyle\sum_{k=0}^n {2k \choose k}{2n \choose k}$$ According to OEIS , the sequence is equivalent to the sum of squares of entries in the $n^{\text{th}}$ row of the triangle of trinomial coefficients.  Note that the sequence is every other central trinomial coefficient. For $a=4b$ , I found that: $$\int_0^{\frac{\pi}{2}} {\left(\frac{\sin{(4bx)}}{\sin{(bx)}}\right)}^{2n} \; dx =\frac{ \pi}{2} \displaystyle\sum_{k=0}^{ \lfloor{3n/4} \rfloor} {(-1)}^k {2n \choose k} {5n-4k-1 \choose 3n-4k}$$ According to OEIS , the sequence is equivalent to the ""central quadrinomial coefficients"". For $a=5b$ and indeed it appears to follow this sequence, but I couldn't find a closed form for sum of squares of entries in the $n^{\text{th}}$ row of the triangle of 5-nomial (I'm not sure what it is called) coefficients Conjecture : From these observations, I conjecture the following with the aforementioned conditions: $$\int_0^{\frac{\pi}{2}} {\left(\frac{\sin{(kbx)}}{\sin{(bx)}}\right)}^{2n} \; dx =\frac{ \pi}{2} \rho$$ where $\rho$ is the sum of squares of entries in the $n^{\text{th}}$ row of the triangle of $k^{\text{th}}$ multinomial coefficients.  I believe this is equivalent to the central $k^{\text{th}}$ multinomial coefficients for even valued $k$ , but is the alternating central coefficients for odd valued $k$ . Is there a closed form expression for this (the sum of squares of entries in the $n^{\text{th}}$ row of the triangle of $k^{\text{th}}$ multinomial coefficients) and are my observations correct?","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'solution-verification']"
3773347,The smallest base $b$ for which the fraction $\frac{5445469}{5445468}$ has a finite number of decimal digits,"Recently, I have found this problem: Given the fraction $\frac{5445469}{5445468}$ , find the smallest base $b\;\in\;\mathbf{N}$ such that, in base $b$ , the fraction has a finite number of decimal digits.
To solve this problem, I have splitted the fraction into two terms as follows: $$\frac{5445469}{5445468}=\frac{5445468}{5445468}+\frac{1}{5445468}=1+\frac{1}{5445468}$$ Now, the first term is a $1$ , so in every base it's always $1$ as the numerator of the fraction. We have to find the minimu base for which $\frac{1}{5445468}$ has a finite number of terms. I know the factorization of $5445468$ that is: $$5445468=2^2\cdot3^4\cdot7^5$$ but how can we get $b$ ?
I think that in order to have a finite decimal representation, $5445468$ in base $b$ has to be multiple of $2$ and $5$ . Is it correct?","['number-theory', 'elementary-number-theory']"
3773353,"For what values $x_0$, does the sequence $x_{n+1} = x_n^2 - \dfrac{x_n}{2}$ converge?","This is the question: Let $x_n$ be a sequence of real numbers defined by $x_{n+1} = x_n^2 - \dfrac{x_n}{2}$ , with $n \geqslant 0$ .
For what values $x_0$ , does this sequence converge? And it converges to what? My first idea on how to solve this was to determine for what values the sequence decreases. So, $x_n \geqslant x_{n+1}$ implies $x_n \geqslant x_n^2 - \dfrac{x_n}{2}$ , and from that we get that the sequence decreases if $x_n \in [0,\frac{3}{2}]$ . Testing some values, we see that the sequence does converge on $x_0 = 0$ , $x_n = 1$ , $x_n = \frac{1}{2}$ , $x_n = \frac{3}{2}$ , for the values $0$ , $0$ , $0$ , and $\frac{3}{2}$ respectively. All done, I don't have any clues on how to proceed. Any help will be welcome.","['sequences-and-series', 'real-analysis']"
3773408,Compute $f^{(2001)}(0)$ where $f(x) = e^{-x}\sin(x)$ [duplicate],"This question already has answers here : $100$-th derivative of the function $f(x)=e^{x}\cos(x)$ (7 answers) Closed 3 years ago . The question: Let $f(x) = e^{-x}\sin(x)$ . Calculate $f^{(2001)}(0)$ . Note that in the question, $f^{(n)}(x)$ . means the $n$ -th derivative of $f$ . I calculated the first six derivatives on zero and got: $f^{(0)}(0) = 0$ . $f^{(1)}(0) = 1$ . $f^{(2)}(0) = -2$ . $f^{(3)}(0) = 2$ . $f^{(4)}(0) = 0$ . $f^{(5)}(0) = -4$ . $f^{(6)}(0) = 8$ . So, the pattern isn't pretty clear.
Also, I thought that if $n$ is even, then: $f^{(n)}(x) = -e^{-x}\cos(x) - f(x) + f^{(1)}(x) - f^{(2)}(x) + ... + f^{(n-3)}(x) - f^{(n-2)}(x) - f^{(n-1)}(x)$ . and $f^{(n - 1)}(x) = e^{-x}\cos(x) + f(x) - f^{(1)}(x) + f^{(2)}(x) + ... + f^{(n-4)}(x) - f^{(n-3)}(x) - f^{(n-2)}(x)$ . So by substituting $f^{(n - 1)}(x)$ in the first equation, we get: $f^{(n)}(x) = -2(e^{-x}\cos(x) + f(x) - f^{(1)}(x) + f^{(2)}(x) + ... + f^{(n-4)}(x) - f^{(n-3)}(x))$ I tried to the same thing with $n$ being odd and tried to perform an induction to get a formula for the value of $f^{(n)}(x)$ but couldn't find a way to do so. Any ideas?","['calculus', 'derivatives']"
3773470,Value of $\sum_{n=1}^{\infty} \frac{\cos (n)}{n}$,"I was trying to calculate the value of the series $\displaystyle \sum_{n=1}^{\infty} \dfrac{\cos (n)}{n}$ and I got an answer which I think could be right, but I'm not sure about some of the steps I took to get there. I was wondering if someone could provide some more insight so I can clear my doubts, and also check if I actually got the correct value. First of all, I used Dirichlet's test for the convergence of the series, since $a_n = \dfrac{1}{n}$ is monotonic and $\displaystyle \lim_{n \to \infty} a_n = 0$ , and the cosine partial sums can be bounded by a constant not dependent on $n$ (I'm pretty sure this is right since I looked other ways to do it, so I won't list exactly what I did to get the bound). With that out of the way, I tried taking the expression $\dfrac{\cos(n)}{n}$ and rewriting it as something I could attempt to sum, and got this: $$\displaystyle \int_1^{\pi} \sin(nx) \, dx = \left. -\dfrac{\cos(nx)}{n} \right|_1^{\pi} = \dfrac{(-1)^{n+1}}{n} + \dfrac{\cos(n)}{n}$$ So $$\displaystyle \int_1^{\pi} \sin(nx) \, dx + \dfrac{(-1)^{n}}{n} = \dfrac{\cos(n)}{n}$$ And then $$\displaystyle \lim_{n \to \infty} \displaystyle \sum_{k=1}^{n}\left(\displaystyle \int_1^{\pi} \sin(kx) \, dx + \dfrac{(-1)^{k}}{k}\right) = \displaystyle \lim_{n \to \infty} \displaystyle \sum_{k=1}^{n} \dfrac{\cos(k)}{k}$$ Then I tried separating the left side member into two sums, since $$\displaystyle \sum_{n=1}^{\infty} \dfrac{(-1)^n}{n} = \displaystyle -\sum_{n=1}^{\infty} \dfrac{(-1)^{n+1}}{n} = -\ln (2)$$ I believe the latter equality can be derived using the alternate series test for the convergence of the series, and the Taylor expansion around $x = 0$ of $\ln {(1+x)}$ along with Abel's theorem . As for the other sum, this is the step I'm not sure about. I did $$\displaystyle \lim_{n \to \infty} \displaystyle \sum_{k=1}^{n}\left(\displaystyle \int_1^{\pi} \sin(kx) \, dx\right) = \displaystyle \lim_{n \to \infty} \displaystyle \int_1^{\pi} \left(\displaystyle \sum_{k=1}^{n} \sin(kx)\right) \, dx$$ I'm not sure that's valid, and if it is I'm not sure why: I thought it would be fine since the partial sums could be arranged that way before taking the limit, but I suspect this thinking isn't correct, and I can't just swap the sum and the integral anytime without affecting the result. But anyways, if we take it as valid, then we can get a value for the sum by doing $$\cos {(nx+\dfrac{x}{2})} - \cos {(nx-\dfrac{x}{2})} = -2\sin {(nx)}\sin{\left(\dfrac{x}{2}\right)}$$ So $$\sin{(nx)} = \dfrac{\cos {(nx-\frac{x}{2})} + \cos {(nx+\frac{x}{2})}}{2\sin{\left(\frac{x}{2}\right)}}$$ And then $$\displaystyle \sum_{k=1}^{n} \sin{(kx)} = \displaystyle \sum_{k=1}^{n} \dfrac{\cos {(kx-\frac{x}{2})} + \cos {(kx+\frac{x}{2})}}{2\sin{\left(\frac{x}{2}\right)}}$$ Which telescopes to $$\displaystyle \sum_{k=1}^{n} \sin{(kx)} = \dfrac{\cos {\left(\frac{x}{2}\right)}-\cos {\left(\frac{2n+1}{2} \cdot x\right)}}{2\sin{\left(\frac{x}{2}\right)}}$$ Returning to the integral, we need to evaluate $$\displaystyle \lim_{n \to \infty} \displaystyle \int_1^{\pi} \left(\displaystyle \sum_{k=1}^{n} \sin(kx)\right) \, dx = \displaystyle \lim_{n \to \infty} \displaystyle \int_1^{\pi} \frac{\cos {\left(\frac{x}{2}\right)}-\cos {\left(\frac{2n+1}{2} \cdot x\right)}}{2\sin{\left(\frac{x}{2}\right)}} \, dx$$ I again tried separating it in the sum of the integrals. The first one $$\displaystyle \int_1^{\pi} \frac{\cos {\left(\frac{x}{2}\right)}}{2\sin{\left(\frac{x}{2}\right)}} \, dx = \displaystyle \int_{\sin {\frac{1}{2}}}^1 \dfrac{1}{u} \, du = -\ln({\sin{\frac {1}{2}}})$$ Via substitution $u = \sin{\frac{x}{2}}$ This won't change when $n$ goes to infinity. As for the second one $$-\dfrac{1}{2} \displaystyle \int_1^{\pi} \dfrac{\cos{\left(nx+\frac{x}{2}\right)}}{\sin{\left(\frac{x}{2}\right)}} \, dx = -\dfrac{1}{2}\left(\displaystyle \int_1^{\pi} \dfrac{\cos{(nx)}\cos{\left(\frac{x}{2}\right)}}{\sin{\left(\frac{x}{2}\right)}} \, dx - \displaystyle \int_1^{\pi} \sin(nx) \, dx \right) = $$ $$= -\dfrac{1}{2}\left(\displaystyle \int_1^{\pi} \dfrac{\cos{(nx)}\cos{\left(\frac{x}{2}\right)}}{\sin{\left(\frac{x}{2}\right)}} \, dx + \displaystyle \left. \frac{\cos(nx)}{n} \right|_1^{\pi} \right)$$ Both of these integrals go to 0 as $n$ goes to infinity, applying the Riemann-Lebesgue lemma for the first one, since the function $f(x) = \cot{\left(\frac{x}{2}\right)}$ is continuous on $[1,\pi]$ . Putting it all together gives $$\displaystyle \displaystyle \sum_{n=1}^{\infty} \dfrac{\cos(n)}{n} = -\ln2-\ln{\left(\sin{\frac{1}{2}}\right)} = \boxed{-\ln{\left(2 \cdot \sin{\frac{1}{2}}\right)}} \approx 0.0420195$$ I used Octave to try and check the result: setting $n = 10^6$ gave me $$S_{10^6} \approx 0.042020$$ Because of this, I'm inclined to think I got the correct answer, but I still doubt some of the steps I took (mainly the interchanging sum and integral one). Thanks in advance. I'm sorry if I didn't make myself clear, english isn't my first tongue. I did some search as to find something related to this value, but couldn't find anything. Very sorry if its been answered before.","['sequences-and-series', 'real-analysis']"
3773490,Why is the gradient normal to tangent vectors?,"Suppose $f: \mathbb{R}^n \to \mathbb{R}$ is differentiable at $x$ . Let $d_xf$ denote the derivative of $f$ at $x$ . Let $L$ be the level set through $x$ , $L = \{y \in \mathbb{R}^n: f(y) = f(x)\}$ . Suppose $v$ is a tangent vector at $x$ that is tangent to the level set $L$ . Then the claim is that $d_xf(v) = 0$ . Why is this true? Is there a rigorous justification of this? I have seen various answers, like Why is the gradient normal? and Why gradient vector is perpendicular to the plane , but I couldn't really find a rigorous justification of that particular fact, that $d_xf(v) = 0$ . I can see the intuition but I'd like a proof if possible. Also, what does it mean precisely when we say ""Suppose $v$ is a tangent vector at $x$ that is tangent to the level set $L$ ""? Can all these facts and notions be defined and proved in the usual multivariable context of Euclidean space $\mathbb{R}^n$ (like in a normal or advanced Calc III course) or do we need an excursion into differential geometry or something? I'd just like to know because some of the multivariable calculus texts/resources I've seen, as well as some answers on this site, mostly seem to gloss over the details and just roughly justify it by appealing to geometric intuition, which I think is useful but I would also like a proof.","['multivariable-calculus', 'differential-geometry']"
3773532,Hessian of $f(X)$ when $X$ is a symmetric matrix,"Thanks to the scientific community, things are getting clear relatively to the question: what is the gradient of a function $f(X)$ when $X$ is a symmetric matrix? . In particular, I report here some useful links that addressed this question in the past and can be used as a reference to proceed further on this discussion: Understanding notation of derivatives of a matrix Taylor expansion of a function of a symmetric matrix https://arxiv.org/pdf/1911.06491.pdf In a nutshell, we can say that, when involving a function with matrix argument, we have to distinguish between two ""different"", but related, gradients: the unconstrained gradient $G$ , computed with standard matrix calculus without assuming dependent variables in the matrix $X$ , and used for the computation of the differential of the function, i.e. $G:dX$ the constrained gradient $S$ , that considers only the independent variables of the matrix $X$ . These two gradients are related by the expression: $$S=G+G^{T}-I \circ G $$ and it turns out that the first-order differential of the function $f$ at a given point $X$ after a perturbation $\Delta X$ can be computed as: $$ d f=\sum_{i, j} G_{i j} d X_{i j} = \sum_{i \geq j} S_{i j} d X_{i j}$$ It is important however to note how, in an iterative algorithm that updates a variable $X^{k+1}$ (such as in gradient descent), we have to use the constrained gradient $S$ and not the gradient $G$ , due to the fact that $X$ is symmetric while the gradient $G$ could be not symmetric. More information can be found in the above links, that explain the relation also in terms of $vec(\cdot)$ and $vech(\cdot)$ operators. Coming to my question. I want now to find the Hessian of the function $f(X)$ , that in theory is a $4$ th order tensor and we already know the mangy road crisscrossed to get to the gradient. To start, is it correct to perturb the first-order differential (with the unconstrained gradient)?
If yes, I will reach a scalar quadratic form. For instance, if we consider as function $f(X)=\log \operatorname{det} X$ , we know that the second order approximation with perturbation in $U$ and $V$ is given by (and I reference this question Second order approximation of log det X ): $$-\operatorname{tr}\left(X^{-1} U X^{-1} V\right) = - \operatorname{vec}(U^{\top})^{\top}(X^{-\top} \otimes X^{-1})   \operatorname{vec}(V)$$ We can arrive at the Hessian in matrix form $X^{-\top} \otimes X^{-1}$ . My first question is: how to write it in a tensor form? And second question is: how to reach in this case our constrained Hessian ?","['approximation', 'approximation-theory', 'multivariable-calculus', 'taylor-expansion', 'hessian-matrix']"
3773538,Help with proving/disproving an inequality,"$\textbf{Question:}$ Let $x_1,x_2,x_3,x_4 \in \mathbb{R}$ such that $(x_1^2+1)(x_2^2+1)(x_3^2+1)(x_4^2+1) =16 $ . Is it then true that $x_1x_2+x_1x_3+x_1x_4+x_2x_3+x_2x_4+x_3x_4- x_1x_2x_3x_4 \le 5$ , with equality $\iff x_1=x_2=x_3=x_4=\pm 1$ ? Rough calculations seem to suggest that this is indeed the case, but I am unable to prove it. For some context, this question is actually related to USAMO $2014$ P $1$ . The original question was that: given a polynomial $P(x)=x^4+ax^3+bx^2+cx+d,$ and $ b-d \ge 5$ , where all $4$ roots $x_1,x_2,x_3,x_4$ of $P(x)$ are real,  find the smallest value of the expression $(x_1^2+1)(x_2^2+1)(x_3^2+1)(x_4^2+1)$ . Indeed, I managed to prove that this expression is at least $16$ . But to show that the minimum value of $16$ is actually attainable, I have to find a construction of some polynomial $P(x)$ satisfying the conditions of the question. While it is indeed obvious to see that setting $ x_1=x_2=x_3=x_4=1$ or $ x_1=x_2=x_3=x_4=- 1$ both works ( just expand $(x-1)^4$ and $(x+1)^4$ respectively), are there other non-trivial values that would work too? In particular, using $\textbf{Vieta's formula}$ gives us that $b=x_1x_2+x_1x_3+x_1x_4+x_2x_3+x_2x_4+x_3x_4$ and $d=x_1x_2x_3x_4$ , so the complicated looking expression in the first paragraph is actually just equivalent to $b-d$ .","['contest-math', 'inequality', 'symmetric-polynomials', 'optimization', 'algebra-precalculus']"
3773541,"Let $E_1 \subset E_2$ both be compact and $m(E_1) = a, m(E_2) = b$. Prove there exists a compact set $E$ st $m(E) = c$ where $a < c < b$.","Exercise 1.27 (Stein & Shakarchi): Suppose $E_1$ and $E_2$ are a pair of compact sets in $\mathbb{R}^d$ with $E_1 \subset E_2$ and let $a = m(E_1)$ and $b = m(E_2)$ . Prove that for any $c$ with $a < c < b$ , there is a compact set $E$ with $E_1 \subset E \subset E_2$ and $m(E) = c$ . Hint: As an example, if $d = 1$ and $E$ is a measurable subset of $[0, 1]$ , consider $m(E ∩ [0, t])$ as a function of t. Here is my intuition for tackling this exercise Consider a countable sequence of closed cubes centered at the origin which increase in measure (essentially I imagine that the cubes fill $\mathbb{R}^d$ as they grow infinitesimally in terms of side length). Now, as the cubes grow we should be able to create a measurable set $E$ such that $E_1 \subset E \subset E_2$ where $m(E) = c$ and $a < c< b$ , by taking the intersection of some limit of these closed cubes with $E_2$ . Now, it's clear by construction that this set would be compact, as a closed subsets of a compact set are themselves compact. However, I am having trouble justifying that this $E$ does exist. I'm thinking that it should because $E_1 \subset E_2$ with measures $m(E_1) = a$ and $m(E_2) = b$ remind me of $[a,b]$ some connected interval in $\mathbb{R}$ . Thus, with the hint in mind, we should be able to create a continuous function and use the Intermediate Value Theorem. With all this in mind, I'm having trouble formalizing this intuition, it's that classic tip of the tongue feeling! Any hints or correction of my intuition would be most welcome!","['measurable-sets', 'measure-theory', 'real-analysis']"
3773544,Why is Kantorovich optimal transport dual not smooth?,"Let $X$ and $Y$ be two separable metric spaces such that any probability measure on $X$ (or $Y$ ) is a Radon measure (i.e. they are Radon spaces). Let $c: X \times Y \rightarrow[0, \infty]$ be a Borel-measurable function. The Kantorovich problem is $$\inf \left\{\int_{X \times Y} c(x, y) \mathrm{d} \gamma(x, y) \mid \gamma \in \Gamma(\mu, \nu)\right\}$$ where $\Gamma(\mu, v)$ denotes the collection of all probability measures on $X \times Y$ with marginals $\mu$ on $X$ and $v$ on
Y. It can be shown that a minimizer for this problem always exists when the cost function $c$ is lower semi-continuous and $\Gamma(\mu, v)$ is a tight collection of measures (which is guaranteed for Radon spaces $X$ .
and $Y$ ). Under mild conditions, the minimum of the Kantorovich problem is equal to $$
\sup \left(\int_{X} \varphi(x) \mathrm{d} \mu(x)+\int_{Y} \psi(y) \mathrm{d} \nu(y)\right)
$$ where the supremum runs over all pairs of bounded and continuous functions $\varphi: X \rightarrow \mathbf{R}$ and $\psi: Y \rightarrow \mathbf{R}$ such that $$
\varphi(x)+\psi(y) \leq c(x, y)
$$ I read everywhere that this dual problem is not smooth in $(\varphi, \psi)$ . What does it mean and why is it the case ? By not smooth, do we mean that the Fréchet derivatives do not exist ?","['real-analysis', 'functional-analysis', 'optimization', 'derivatives', 'probability']"
3773576,Separability of a $\sigma$-algebra generated by an algebra,"Let $X:(Ω,\mathcal{F})$ be a measurable set and $\mathcal{F}=\sigma(C)$ where $C$ is an algebra . Now We define $B_\mathcal{w}=\underset{\mathcal{w}∈A,A∈C}{\cap}A$ , which suggests the intersection of all sets in $C$ that contain $\mathcal{w}$ . Similarly, we define $\mathcal{F}_\mathcal{w}=\underset{\mathcal{w}∈A,A∈\mathcal{F}}{\cap}A$ ,which suggests the intersection of all sets in $\mathcal{F}$ that contain $\mathcal{w}$ . If there exists another element $x$ which does not equal to $\mathcal{w} $ and $x$ belongs to $B_\mathcal{w}$ . Now I think its trivial that $x$ belongs to $\mathcal{F}_\mathcal{w}$ but I couldn't give a precise proof. Moreover, could we replace the condition "" $C$ is an algebra"" by "" $C$ is an arbitrary set class which generates $\mathcal{F}$ "" and get the same conclusion?","['measure-theory', 'real-analysis']"
3773605,"How do I find integers $x,y,z$ such that $x+y=1-z$ and $x^3+y^3=1-z^2$? [duplicate]","This question already has answers here : Solve for integers $x, y, z$ such that $x + y = 1 - z$ and $x^3 + y^3 = 1 - z^2$. (3 answers) Closed 3 years ago . This is INMO 2000 Problem 2. Solve for integers $x,y,z$ : \begin{align}x + y &= 1 - z \\ x^3 + y^3 &= 1 - z^2 . \end{align} My Progress: A bit of calculation and we get $x^2-xy+y^2=1+z $ Also we have $x^2+2xy+y^2=(1-z)^2 \implies 3xy=(1-z)^2-(1+z)=z(z-3) \implies y=\frac{z(z-3)}{3x}$ and $x=\frac{z(z-3)}{3y} $ . Note that since $z$ , $x$ , $y$ is an integer, we must have $3\mid z$ . So, let $z=3k$ . So we have $y=\frac{3k(3k-3)}{3x}=\frac{k(3k-3)}{x}$ and $x=\frac{z(z-3)}{3y}=\frac{k(3k-3)}{y}$ . Then I am not able to proceed.
Hope one can give me some hints and guide me.
Thanks in advance.","['contest-math', 'systems-of-equations', 'number-theory', 'elementary-number-theory', 'quadratics']"
3773624,"""Being an algebraic space"" is local property on the target","Let $S$ be a scheme and let $\require{AMScd}$ \begin{CD}
X' @>{p}>> X\\
@V{f'}VV @V{f}VV\\
Y' @>{q}>> Y
\end{CD} be a Cartesian diagram in the category of sheaves on $\mathscr{S}ch/S$ with the etale topology.
Assume that $q$ is epi.
If $f'$ is representable by quasi-separated algebraic spaces, then so is $f$ . (For a morphism of sheaves $f : X \to Y$ over $S$ , we say that $f$ is representable by quasi-separated algebraic spaces if for any $S$ -scheme $T$ and any map $T \to Y$ , $X \times_Y T$ is a quasi-separated algebraic space.) To show it, it suffices to show that $X$ is a quasi-separated algebraic space, assuming that $Y, Y'$ affine schemes, $q$ etale surjective, and $X'$ a quasi-separated algebraic space. And by 1.6. of Laumon, Moret-Bailly's Champs algébriques, to see that $X$ is a quasi-separated algebraic space, it suffices to show that $X' \times_X X' \to X' \times_S X'$ is quasi-compact. But I can't show it. Since $X' \times_S X' \to X \times_S X$ is representable by schemes, and is etale surjective, the quasi-compactness of $X' \times_X X' \to X' \times_S X'$ and the one of $X \to X \times_S X$ are equivalent.
And of course in general $X \to X \times_S X$ is not quasi-compact.
So it seems that this highlighted statement is wrong. Where is wrong in my argument?","['algebraic-geometry', 'category-theory']"
3773633,Euler Lagrange equation assuming once differentiability,"Let $$S=\int_0^T L(\varphi(t),\varphi'(t))dt$$ Assume that $\varphi(0)=0$ is the only constraint. Assume that $\varphi$ is once differentiable but not twice. Is there an equivalent of Euler Lagrange equation for minimizing $S$ ? I know if we have both endpoints and twice differentiability, we get EL equation. But what about this more general situation?","['optimization', 'ordinary-differential-equations', 'calculus-of-variations']"
3773727,When is a limit of measure preserving maps also measure preserving?,"Take some compact space, for simplicity take the interval $[0,1]$ . Let $f_n:[0,1] \to [0,1]$ be measure preserving, i.e. $\mu (f_n^{-1}(A))=\mu(A)$ for all lebesgue measurable $A \subset [0,1]$ . The question is under what kinds of convergence $f_n \to f$ do we also ensure that $f$ is measure preserving? I think I can show that for $f_n \to f$ in $L^2$ this holds. I'm now wondering about weak convergence $f_n \rightharpoonup f$ in $L^2$ ? Anyone know the answer (and maybe a proof/counterexample)?","['measure-theory', 'ergodic-theory', 'weak-convergence', 'real-analysis']"
3773811,Example where $\operatorname{Spec} S^{-1}B$ is neither open nor closed in $\operatorname{Spec} B$,"I know that $\operatorname{Spec} S^{-1}B$ is open in $\operatorname{Spec} B$ with respect to the Zariski topology when $S=\{1,f,f^2,\ldots\}$ for $f\in B$ . However, is this true for every multiplicative subset $S$ of $B$ ? In what I'm reading I see the example of $\operatorname{Spec}\mathbb{Q}\subset\operatorname{Spec}\mathbb{Z}$ . Is this just because $\operatorname{Spec}\mathbb{Q}=(0)$ and $V(I)$ is always contained in some nonzero prime ideal?","['algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
3773829,Evaluating $\int _0^1\frac{\ln ^2\left(x\right)\ln \left(1-x\right)}{1+x^2}\:dx$,"I've been trying to evaluate $$\int _0^1\frac{\ln ^2\left(x\right)\ln \left(1-x\right)}{1+x^2}\:dx$$ With no success, i tried to consider the following integrals $$I=\int _0^1\frac{\ln ^2\left(x\right)\ln \left(1-x\right)}{1+x^2}\:dx,J=\int _0^1\frac{\ln ^2\left(x\right)\ln \left(1+x\right)}{1+x^2}\:dx$$ $$I+J=\int _0^1\frac{\ln ^2\left(x\right)\ln \left(1-x^2\right)}{1+x^2}\:dx=\int _0^1\frac{\ln ^2\left(x\right)\ln \left(1-x^4\right)}{1+x^2}\:dx-\int _0^1\frac{\ln ^2\left(x\right)\ln \left(1+x^2\right)}{1+x^2}\:dx$$ I managed to express that $1$ st integral into somewhat known euler sums but that $2$ nd integral arrived at a sum i didnt know how to evaluate which was $$2\sum _{k=1}^{\infty }\frac{\left(-1\right)^kH_k}{\left(2k+1\right)^3}$$ And it seems this approach wont go smooth, could i tackle the main integral differently? maybe with an easier approach?","['integration', 'improper-integrals', 'definite-integrals', 'harmonic-numbers']"
3773836,Prove complex numbers $a$ and $b$ are antipodal under stereographic projection $\iff a \overline{b} = -1$,"I'm trying to prove the following statement: Given $a, b \in \mathbb{C}$ , prove that $a$ and $b$ correspond to antipodal points on the Riemann sphere under stereographic projection if and only if $a \overline{b} = -1$ My attempt I wanted to make a proof where all my implications were reversible to avoid making a proof of each implication separately. As previous knowledge, I know that if a have a point $a \in \mathbb{C}$ , then the stereographic projection $f: \mathbb{C} \to S^2$ is given by $$
f(a) = \left(\frac{a + \overline{a}}{1 + |a|^2},\frac{a - \overline{a}}{i\left(1 + |a|^2\right)},\frac{|a|^2-1}{|a|^2+1}\right)
$$ Now, given that $P,Q\in S^2$ are antipodal if and only if $P =-Q$ , I get the following: \begin{align}
f(a) = -f(b) &\iff
\begin{cases}
\frac{a + \overline{a}}{1 + |a|^2} = \frac{-b - \overline{b}}{1 + |b|^2} \\
\frac{a - \overline{a}}{i\left(1 + |a|^2\right)} = \frac{\overline{b}-b}{i\left(1 + |b|^2\right)} \\
\frac{|a|^2-1}{|a|^2+1} = \frac{1-|b|^2}{|b|^2+1} \\
\end{cases}\\
&\iff\begin{cases}
a + \overline{a}+a|b|^2 +\overline{a}|b|^2 = -b - \overline{b}-b|a|^2 -\overline{b}|a|^2 \\
a - \overline{a}+a|b|^2 -\overline{a}|b|^2 = -b + \overline{b}-b|a|^2 +\overline{b}|a|^2 \\
|ab|^2+|a|^2-|b|^2-1 =-|ab|^2+|a|^2-|b|^2+1 \\
\end{cases}\\
&\iff\begin{cases}
a +a|b|^2 = -b -b|a|^2  \\
\overline{a} +\overline{a}|b|^2 = -\overline{b} -\overline{b}|a|^2  \\
|ab|^2=1 \\
\end{cases}\\
&\iff\begin{cases}
a +b +a|b|^2+b|a|^2 =0 \\
|a||b|=1 \\
\end{cases}\\
\end{align} Where here I use brackets to indicate that all those equations are true simultaneously. On this last step is where I ran into trouble because I couldn't find a way to show that both conditions in the last step are equivalent to $b =- \frac{1}{\overline{a}}$ . Is my attempt correct (up to what I have already written)? And if so, does somebody know how I could conclude the proof of equivalence? Any help would be greatly appreciated. Thank you!","['analytic-geometry', 'proof-writing', 'complex-analysis', 'solution-verification', 'complex-numbers']"
3774000,Liminf of union of two sequences,"Let $A_n$ and $B_n$ be two sequences of sets. How $(\liminf_n A_n \cup \liminf_n B_n)$ and $\liminf_n (A_n\cup B_n)$ are related? Def . Given a sequence of sets $E_n$ , the limit inferior of $E_n$ is defined as $$\liminf_{n\to\infty} E_n=\bigcup_{n=1}^\infty \bigcap_{k=n}^\infty E_k$$ Some thoughts Write $\liminf_n A_n=\bigcup_{n}C_n$ and $\liminf_n B_n=\bigcup_{n}D_n$ where $C_n=\bigcap_{k=n}^\infty A_k$ and $D_n=\bigcap_{k=n}^\infty B_k$ . I will use a (intutive) result that requires a proof : $(\bigcup_{n\in\mathbb{N}}C_n) \cup (\bigcup_{l\in\mathbb{N}}D_l)=\bigcup_{n\in\mathbb{N}}C_n\cup D_n$ . On the other hand, for each $n$ , $$C_n\cup D_n=\bigcap_{k=n}^\infty A_k \cup \bigcap_{l=n}^\infty B_l=\bigcap_{k=n}^\infty \left[ A_k \cup \left(\bigcap_{l=n}^\infty B_l \right)\right]\subseteq \bigcap_{k=n}^\infty A_k \cup B_k.$$ From these observations, we immediately have $$\liminf_n (A_n\cup B_n)\supseteq \liminf_n A_n \cup \liminf_n B_n $$","['elementary-set-theory', 'limsup-and-liminf', 'measure-theory']"
3774022,Can I use L'Hopital's to show $\lim_{x\to1^-}(1-x)[\frac{d}{dx}(1-x)\sum_{n=1}^\infty a_nx^n]=0$ for $a_n$ a bounded sequence of reals?,"I am attempting to prove that if $a_n$ is a bounded sequence of real numbers then $$\lim_{x\to1^-}(1-x)\left[\frac{d}{dx}(1-x)\sum_{n=1}^{\infty}a_nx^n\right]=0$$ My approach is to first make some algebraic manipulations, namely we see that \begin{align*}
1&=\lim_{x\to1^-}\frac{(1-x)\sum_{n=1}^{\infty}a_nx^n}{(1-x)\sum_{n=1}^{\infty}a_nx^n}\\
&=\lim_{x\to1^-}\frac{1}{(1-x)\sum_{n=1}^{\infty}a_nx^n}\left(\frac{1-x}{\frac{1}{\sum_{n=1}^{\infty}a_nx^n}}\right)\\
\end{align*} The reason I want to do this is that if I were able to apply L'Hopital's rule to $$\frac{1-x}{\frac{1}{\sum_{n=1}^{\infty}a_nx^n}}$$ then I would get that \begin{align*}
1&=\lim_{x\to1^-}\frac{1}{(1-x)\sum_{n=1}^{\infty}a_nx^n}\left(\frac{-1}{-\frac{\sum_{n=1}^{\infty}na_nx^{n-1}}{\left(\sum_{n=1}^{\infty}a_nx^n\right)^2}}\right)\\
&=\lim_{x\to1^-}\frac{\sum_{n=1}^{\infty}a_nx^n}{(1-x)\sum_{n=1}^{\infty}na_nx^{n-1}}\\
\end{align*} From there we can subtract $1$ from both sides and multiply top and bottom by $(1-x)$ to get that $$\lim_{x\to1^-}\frac{\left(1-x\right)\sum_{n=1}^{\infty}a_{n}x^{n}-\left(1-x\right)^2\sum_{n=1}^{\infty}na_{n}x^{n-1}}{\left(1-x\right)^2\sum_{n=1}^{\infty}na_{n}x^{n-1}}=0$$ Since $$\left(1-x\right)^2\sum_{n=1}^{\infty}na_{n}x^{n-1}$$ is bounded, the only way for this quantity to go to zero would be for $$\left(1-x\right)\sum_{n=1}^{\infty}a_{n}x^{n}-\left(1-x\right)^2\sum_{n=1}^{\infty}na_{n}x^{n-1}=(1-x)\left[\frac{d}{dx}(1-x)\sum_{n=1}^{\infty}a_nx^n\right]$$ to go to $0$ , thus yielding what we want. I am not sure if this use of L'Hopitals is (or can be) justified, since the limit of $$\frac{-1}{-\frac{\sum_{n=1}^{\infty}na_nx^{n-1}}{\left(\sum_{n=1}^{\infty}a_nx^n\right)^2}}$$ as $x\to1^-$ is not required to exist. Is there any way I can make this argument rigorous? EDIT: If I had the pair of inequalities $$\limsup_{x\to 1^-}k(x)\frac{f(x)}{g(x)}\leq \limsup_{x\to 1^-}k(x)\frac{f'(x)}{g'(x)}$$ $$\liminf_{x\to 1^-}k(x)\frac{f'(x)}{g'(x)} \leq \liminf_{x\to 1^-}k(x)\frac{f(x)}{g(x)}$$ for differentiable functions $f$ , $g$ and $k$ on $[0,1)$ then I could resolve my issue. On wikipedia it states that $$\liminf_{x\to1^-}\frac{f'(x)}{g'(x)}\leq \liminf_{x\to1^-}\frac{f(x)}{g(x)} \leq \limsup_{x\to1^-}\frac{f(x)}{g(x)}\leq \limsup_{x\to1^-}\frac{f'(x)}{g'(x)}$$ but I can't complete the argument for when the factor of $k(x)$ is added.","['limits', 'calculus']"
3774068,Definite integration $\int _{-\infty}^\infty \frac{\tan^{-1}(2x-2)}{\cosh(\pi x)}dx$,"How do I integrate $$\int _{-\infty}^\infty \frac{\tan^{-1}(2x-2)}{\cosh(\pi x)}dx\quad ?$$ The actual integral that I encountered is: $$\int_{-\infty}^\infty dx \left(\frac{N}{\cosh(\frac{\pi }{c}(x-1))}+\frac{1}{\cosh(\frac{\pi}{c}x)} \right) 2 \tan^{-1}\left(\frac{2x-2}{c} \right)$$ where c is a constant with $$\Re c>0$$ Not sure if these two terms makes it easier. I was trying to solve just the last term, but I couldn't make any progress. Numerical integration gives $\int _{-\infty}^\infty \frac{\tan^{-1}(2x-2)}{\cosh(\pi x)}dx= -1.01334 $ . Any hint on how to do it analytically?","['integration', 'calculus', 'definite-integrals']"
3774089,Is there a continuous one-to-one function from a Luzin space into the reals?,"The definition I am working with is that a topological space is Luzin if it is an uncountable $T_3$ space such that every nowhere dense subset is at most countable. I've been reading the proof of Theorem 6.8 from S. Todorcevic's Partition Problems in Topology: If it is true that every regular hereditarily Lindelof space is hereditarily separable, then there are no Luzin spaces. The proof proceeds by contradiction, assuming there is a Luzin space $X$ and building from it a hereditarily Lindelof space that is not hereditarily separable. Earlier in the book, it is proved that the same hypotheses imply the Suslin Hypothesis, i.e. there are no Suslin trees ( $\omega_1$ -trees with no uncountable chains and no uncountable antichains). At some point in the proof, the author claims that since there are no Suslin trees, in particular, the set of nonempty open sets of $X$ ordered by set-theoretic inclusion is not a Suslin tree. Therefore it is possible to find a continuous function from an uncountable subset $X_0\subseteq X$ into the reals, $f: X_0\rightarrow \mathbb{R}$ , such that $f$ is one-to-one. Finding such a function is the very assertion that I am struggling to see. Since there is no reference to previous results or bibliography about this assertion, I believed it is folklore or something very easy to see. However, I have been struggling to find a proof for this. What I have done: First, I found this theorem in K. Kunnen's paper ""Luzin spaces"" http://www.topology.auburn.edu/tp/reprints/v01/tp01021.pdf : Every Luzin ( $T_3$ ) space is zero-dimensional (in the paper, a Luzin space is a $T_2$ topological space such that every nowhere dense subset is countable). Because of this, instead of considering the whole set of nonempty open subsets of $X$ , I rather work with the clopen basis, ordered by inclusion. Secondly, every Luzin space is hereditarily Lindelof, and by our hypothesis, also hereditarily separable. So the clopen basis ordered by inclusion is $ccc$ . Since it is not a Suslin tree, then either it's height as a tree is countable or there is an uncountable chain. If there is an uncountable chain $\{U_\alpha:\alpha<\omega_1\}$ , then $\bigcup\{X\setminus U_\alpha:\alpha<\omega_1\}$ is not hereditarily Lindelof, for the family $\{X\setminus U_\alpha:\alpha<\omega_1\}$ is an open cover with no countable subcovers. Thus, the tree is countable. From this, I tried to build a continuous one-to-one function from the tree $2^{<\omega}$ into the reals, enumerate the countable dense subset of $X$ , $D=\{d_n:n\in \omega\}$ , and then associate to every $x\in X$ an infinite 0,1 sequence, such that the sequence is the characteristic function of a subset of $D$ converging to $x$ . The only problem here is that I don't know whether $X$ is first countable, or at least Frechet-Urysohn. I believed it was true, for if there was a point $x$ with an uncountable local base, then one should be able to build an uncountable chain of basic clopens of $x$ . However, I am not sure if I am thinking of this in the wrong way. I then pursued another strategy: Using the clopen basis, it is easy to build a Cantor scheme $(A_s,\subseteq)$ , $s\in 2^{<\omega}$ , without the vanishing diameter condition, since we don't know about metrizability in $X$ . Nevertheless, if it is true that the set of chains of this scheme whose intersection is non-empty, then it is possible to define a continuous one-to-one function like the one we want. In other words, if the set $P=\{f\in 2^\omega:\bigcap_{n\in\omega}A_{f|_n}\neq\emptyset\}$ is uncountable, we are done. But I don't know if this is true. Looking up for answers, I found the concepts of ""Suslin representation"" and ""Suslin operation"", which seem to address similar problems. However, they seem to involve too much infinitary combinatorics for Todorcevic to not refer to a particular paper, book, or result that implies the assertion. Is that assertion trivial and I am completely lost? Or is there a reference where to look up for this result?","['general-topology', 'descriptive-set-theory', 'set-theory']"
3774147,Approximating Probabilities of Winning a Game with a 9% Winning Chance,"A game has a winning probability of $9\%$ . If you play $10$ times, what is the probability that you win $$(i)\text{ exactly once}$$ $$(ii) \text{at least once}$$ $$(iii) \text{less than 3 times or more than 5 times}$$ Since this is a binomial distribution $\frac{winning}{losing}$ , I approached (i) the following way: $$\frac{10!}{9!} \times {0.09}^1 \times {0.91}^9 = 38.5\%$$ Is this correct? Because it seems rather high to me that one has almost a $40%$ probability of winning EXACTLY once. But my intuition might be (as is usually is) flawed. For the second subtask now I assume that one could simply calculate $$1 - {0.91}^{10} = 61.1\%$$ Here I wanted to ask, I one were not to use this shortcut, but to actually add all binomial probabilities, if it would be admissible to approximate the result normally in the following way: $$E(x) = np = 100.09 = 0.9~~ \text{and}~~ \sigma = \sqrt {({100.09}\times{0.91})} = 0.905$$ If we include all results from $1$ to $10$ wins, with continuity correction $(1-0.5 = 0.5)$ and standardisation we have $$1 - P(X ≥ 1) = P\left(Z ≥ \frac{(0.5-0.9)}{0.905}\right) = P(Z ≥ -0.442) = 67.1\%$$ However, this approximation seems quite off form the actual solution of $61.1\%$ , where did I do a mistake? Or can't you use a normal approximation for the task in question? For the last subtask I again started by calculating the binomial probability: $$1 - \text{P(three wins) - P(four wins) - P(five wins)} \\=1 - \left(\frac{10!}{3!7!} \times {0.09}^3 \times {0.91}^7 \right) - \left(\frac{10!}{4!6!} \times {0.09}^4 \times {0.91}^6) \right) - \left(\frac{10!}{5!5!} \times {0.09}^5 \times {0.91}^5 \right)= 94.6 \%$$ Would there be a faster way to calculate this? Again, if you would approximate normally: $$1 - P\left(\frac{(2.5-0.9)}{0.905} ≤ Z ≤ \frac{(5.5-0.9)}{0.905}\right) = 1 - P(1.768 ≤ Z ≤ 5.083) = 96.2\%$$ I seem to be doing something wrong with the normal approximation, but I can't figure out what is is. Can someone spot the mistake?","['statistics', 'probability-distributions', 'probability']"
3774225,Weight enumerator classifiers,"Let $f(x,y)$ be a polynomial with integer coefficients. What conditions guarantee that this is the weight enumerator of a binary linear code of size $n$ and dimension $k$ ? I’m almost certain that the answer to this question is unknown...so instead i’ll settle for anything that is conjectural. There’s a list of necessary conditions: $f$ must be homogeneous of degree $n$ with non-negative coefficients. The $x^n$ coefficient has to be $1$ since the zero vector is the unique weight $0$ vector. The $y^n$ coefficient has to be $0$ or $1$ since the all $1$ ’s vector either belongs to the code or doesn’t. The sum of the coefficients has to be $2^k$ since every vector has a unique weight and so is counted exactly once by some coefficient. The MacWilliams transform ( $g(x,y) = \frac{1}{2^k}f(x+y,x-y)$ ) has to have all of the above properties but with coefficient sum $2^{n-k}$ since if $f$ corresponds to a code then $g$ would correspond to the dual code. Are there any more necessary conditions missing?","['coding-theory', 'linear-algebra', 'discrete-mathematics']"
3774244,"Calculus of $ \lim_{(x,y)\to (0,0)} \frac{8 x^2 y^3 }{x^9+y^3} $","By Wolfram Alpha I know that the limit $$
   \lim_{(x,y)\to (0,0)} \dfrac{8 x^2 y^3 }{x^9+y^3}=0.
$$ I have tried to prove that this limit is $0$ , by using polar coordinate, the AM–GM inequality and the change of variable $ x^9= r^2 \cos^2(t) $ and $y^3= r^2 \sin^2(t)$ , but these attempts were unsatisfactory. I also have reviewed the similar questions and their answers but there are difference between those functions and mine one, I think the principal difference is that the powers of the denominators are odd.","['limits', 'multivariable-calculus', 'rational-functions', 'real-analysis']"
3774249,What is the probability that the $i$-th best student in a class of $m$ students do better than the $j$-th best student in a class of $n$ students?,"Suppose I am a student in a class of $m$ students and I rank $i$ -th among them, what is the probability that I do better than a student that ranks $j$ -th in a class of $n$ different students? I modelled this problem assigning to each student his ""incompetency"", measured by a real number (so a student is better than another student if in this scale he scores less) and assuming that these scores are extracted for each student in i.i.d. manner from a fixed continuous distribution. Formally, suppose $X_1,X_2, X_3,\dots,Y_1,Y_2,Y_3,\dots$ are i.i.d. real random variables with common distribution $\mu$ , that we assume continuous, i.e. $\forall x \in \mathbb{R}, \mu(\{x\}) = 0$ . If $m\in \mathbb{N}$ , let $X^1_m,X^2_m,\dots,X^m_m$ denote respectively the reordering from $X_1,\dots,X_m$ such that $X^1_m$ is the smallest in the set $\{X_1,\dots,X_m\}$ , $X^2_m$ is the second smallest in the set $\{X_1,\dots,X_m\}$ , and so on (and so $X^1_m \le X^2_m \le \dots \le X^m_m$ and ties appear with zero probability, since $\mu$ is continuous). If $n \in \mathbb{N}$ , define analogously $Y_n^1,\dots,Y^n_n$ . Given $m,n \in \mathbb{N}, i \in \{1,\dots,m\}, j \in \{1,\dots,n\}$ , what it the probability of the event $$\{X^i_m \le Y^j_n\}?$$ Actually, I have in mind a simple strategy using brute force (i.e. disintegration, independence and integration by parts) to get to the result, but the calculations are a bit cumbersome... has anyone any idea how to get to the result in a simpler manner?","['recreational-mathematics', 'probability-distributions', 'combinatorics', 'probability']"
3774250,"(Verification) If $A \subsetneq B$, and $B \subsetneq C$, then $A \subsetneq C$","Statement to be proved: If $A \subsetneq B$ , and $B \subsetneq C$ , then $A \subsetneq C$ . First method : Inequality : If $A \subsetneq B$ then there exists an $x\in B$ such that $x\notin A$ . Let $z:z\in B \wedge z\notin A$ Similarly, if $x\in B\implies x\in C$ , then $z\in C$ , but if $z\notin A$ , then there is an element in $C$ not in $A$ , so $A\ne C$ . $(x\in A\implies x\in B, x\in B \implies x\in C) \implies (x\in A \implies x\in C)$ . Thus at best $x\subset C$ . But since $A\ne C$ , then $A\subsetneq C$ . Q.E.D. Second method (I couldn't use this in the exercise as cardinality came later): $A\subsetneq B$ means that all elements in $A$ are in $B$ , but some elements in $B$ are not in $A$ . So $n(A)<n(B)$ .  Same can be said about $B$ and $C$ : $n(B)<n(C)$ . Combining the two inequalities: $n(A)<n(C)$ so it follows that $A\neq C$ . The fact that all elements in $A$ are in $C$ is proven the same way as in the first method. I think this is correct, but I'm not entirely sure as to the logical soundess. I also think there is a faster way of doing it (but I mainly want to know if it is correct). EDIT: I use $\subsetneq$ to mean ""A is a subset of B, but A is not equal to B""","['elementary-set-theory', 'solution-verification']"
3774384,Do orthogonal projections play a role in diagonalizability?,"I'm studying Linear Algebra by myself, and the textbook I use is the fourth edition written by Friedberg, Insel, and Spence. For now, I'm trying to get through Section 6.6 that concerns orthogonal projections and the spectral theorem. The following claim embodied in this section really confuses me, and I'm not sure what theorem the authors apply to guarantee diagonalizability of $T$ . Let $V$ be a finite-dimensional inner product space, $W$ be a subspace of $V$ , and $T$ be the orthogonal projection of $V$ on $W$ . We may choose an orthonormal basis $\beta=\{v_1,\ldots,v_n\}$ for $V$ so that $\{v_1,\ldots,v_k\}$ is a basis for $W$ . Then $[T]_\beta$ is a diagonal matrix with $1$ 's as the first $k$ diagonal entries and $0$ 's elsewhere. I have no doubt about the existence of $\beta$ ; in fact, this can be guaranteed by Theorem 6.7. However, I don't know why the authors are confident to say that $[T]_\beta$ is diagonal. They even tell me the explicit form of this matrix representation. Does anyone have an idea? Thank you so much.","['matrices', 'projection', 'projection-matrices', 'linear-algebra']"
3774414,Riemann–Stieltjes Integral for Multivariate Functions,"Given two (sufficiently good) single-variable functions \begin{equation}f, g: [a,b] \mapsto \mathbb{R}, \text{ here } a,b \in \mathbb{R}\end{equation} the Riemann–Stieltjes integral is defined as \begin{equation}
\int_{a}^{b} f \,dg = \lim_{N\to\infty} \sum_{i=1}^N f\left(a+i\Delta_N\right)\left[ g\left(a+i\Delta_N\right) - g\left(a+(i-1)\Delta_N\right) \right], \\\quad \text{here } \Delta_N=\frac{b-a}{N}
\end{equation} I was wondering if there is a Riemann-Stieltjes integral definition for multivariate case, e.g. how to define $\int_S f \, dg$ for multivariate functions, e.g. when both $f, g: S \mapsto \mathbb{R}$ , where $S \subset \mathbb{R}^n $ ? (We can assume $S$ is a hyper-rectangle for simplicity.) This question has arisen from the problem of how to calculate the mean of a function of random vector. E.g., having a random vector $X: \Omega \mapsto \mathbb{R}^n$ with an arbitrary cdf $F_X$ and given a function $g: \mathbb{R}^n \mapsto \mathbb{R}$ , how to numerically approximate $\mathbb{E}[g(X)] = \int_{\mathbb{R}^n} g(x) \, dF_X(x)$ ?","['integration', 'means', 'riemann-integration', 'probability']"
3774415,Find all $n$ which $7(n^2 + n + 1)$ is perfect $4^{th}$ power.,"Find all positive integer $n$ , which $7(n^2 + n + 1)$ is perfect $4^{th}$ power. What I tried Let $7(n^2 + n + 1) = a^4$ $\to$ $ 7 | a$ and $a$ is odd. We then get $(n^2 + n + 1) = 343k^4$ ; $k \in \mathbb Z$ Hence, $ 343 | n^3 - 1$ . I’m stuck here Please help! Thanks in advance. Ps : This problem is from my teacher , in the topic of polynomial and it’s application.","['elementary-number-theory', 'algebra-precalculus', 'polynomials', 'perfect-powers']"
3774437,"Prove that $N,R,F$ are collinear","In a triangle $ABC$ , let $I$ be the incentre. Let $D$ , $E$ , $F$ be the intersections of $(ABC)$ . with the lines through $I$ perpendicular to $BC$ , $CA$ , $AB$ , respectively. Define $O= BC \cap DE$ and $L= AC \cap DE$ . Define $IF\cap AB= R$ . Let $N=(BOF) \cap (LAF)$ .Prove that $N$ , $R$ , $F$ are collinear. My progress: Since $F\in (ABC) $ , I thought of using simson points . So I took points $J$ , $R$ , $K$ as the simson points in $BC$ , $BA$ , $AC$ wrt point $F$ respectively. ( as shown in the diagram ) Then since $NBFO$ and $AFLN$ is cyclic, we get that $180- \angle ONF=\angle OBF=\angle CBF=180- \angle FAC=180 -\angle FAL = \angle FNL $ . Hence points $O$ , $N$ , $L$ are collinear . Now, I am stuck. I tried using phantom points but couldn't proceed. I am thinking of using Radical axis but still confused. Here are some more observations which might be trivial but still, we have $BJFR$ , $RFKA$ , $CJFK$ concyclic. We also have $\Delta JFK \sim \Delta BFA $ . Please post hints if possible. Thanks in advance. PS: This is my own observation, so there is a very high chance that I might be wrong. Below are a few diagrams for the problem.","['contest-math', 'euclidean-geometry', 'geometry']"
3774466,Given two circles externaly tangent to each other and the common tangent line. Draw a third tangent circle.,We are given $\Gamma_A$ centered at $A$ and $\Gamma_B$ centered at $B$ tangent to each other externally at $C$ . Line $DE$ is one common tangent to both not through $C$ . Is there a nice way to draw the red circle externally tangent to both and to line $DE$ without using the complete apollonius solution for the $CCL$ ? Are there any nice symetries in this problem? I only saw the homotheties which would mean a few tangency points would be colinear and of course Monge-D'Alembert theorem implies one more colinearity between the tangency points and the exterior homothetic center,"['euclidean-geometry', 'geometric-construction', 'circles', 'geometry', 'geometric-transformation']"
3774507,How to obtain the sum of the series $\sum_{n=0}^{\infty}\frac{1}{2^{n}(3n+1)}$?,How to prove what follows? $$\sum_{n=0}^{\infty}\frac{1}{2^{n}(3n+1)}=\frac{2^{\frac{1}{3}}}{3}\ln\left(\frac{\sqrt{2^{\frac{2}{3}}+2^{\frac{1}{3}}+1}}{2^{\frac{1}{3}}-1}\right)+\frac{\sqrt[3]{2}}{3}\arctan\left(\frac{2^{\frac{2}{3}}+1}{\sqrt{3}}\right)-\frac{2^{\frac{1}{3}}\pi}{6\sqrt{3}}$$ My attempt: $$\sum_{n=0}^{\infty}\frac{1}{2^n(3n+1)}=\sum_{n=0}^{\infty}\frac{x^{3n+1}}{2^n(3n+1)}|_{x=1}$$ We put $$S(x)=\sum_{n=0}^{\infty}\frac{x^{3n+1}}{2^n(3n+1)}\implies S^{'}(x)=\sum_{n=0}^{\infty}\frac{x^{3n}}{2^n(3n+1)}$$ $$S^{'}(x)=\sum_{n=0}^{\infty}\frac{(\frac{x^3}{2})^n}{3n+1}=\sum_{n=0}^{\infty}(\frac{x^3}{2})^n(1-\frac{3n}{3n+1})=\sum_{n=0}^{\infty}(\frac{x^3}{2})^n-\sum_{n=0}^{\infty}(\frac{x^3}{2})^n\frac{3n}{3n+1}=\frac{1}{2-\frac{x^3}{2}}-\sum_{n=0}^{\infty}(\frac{x^3}{2})^n(\frac{3n}{3n+1})=\alpha-\beta$$ Where $$\beta=\sum_{n=0}^{\infty}(\frac{x^3}{2})^n(\frac{3n}{3n+1})$$ So $$\beta=?$$ Waiting for your help to find a beta or prove equal above.,"['integration', 'definite-integrals', 'calculus', 'sequences-and-series', 'convergence-divergence']"
3774532,Prove that $f(x_0)>\frac{2}{3}$,"It's a problem found with the help of Geogebra. Let $0<x$ be a real number then define the function: $$f(x)=\Big(\frac{x}{x+1}\Big)^{\Gamma(x)}$$ Then let $x_0$ be the maximum of the function on $(0,\infty)$ and  then prove that: $$f(x_0)>\frac{2}{3}$$ See here to compare Well to solve it I have tried logically the use of derivative we have: $$f'(x)=\Big(\frac{x}{x+1}\Big)^{\Gamma(x)} \Bigg(\frac{(x + 1) \Big(\frac{1}{(x + 1)} - \frac{x}{(x + 1)^2}\Big) Γ(x)}{x} + \log\Big(\frac{x}{x + 1}\Big) Γ(x) \psi^{(0)} (x)\Bigg)$$ Where we have the $n^{th}$ derivative of the digamma function. I think that this derivative is not really useful only theoretically, but we can use the Newton's method numerically . I have tried some inequality on the this wiki page notably an inquality due to  Kečkić and Vasić without success. On the other hand the problem with Taylor series is : we get a lot of constant as Euler-Mascheroni constant wich needs to be evaluate with an series or something like that. So it's a little bit make problem on another problem. Maybe spline cubic is the way I don't know... Finally taking the logarithm on both side the derivative is a little bit less tedious. See here Well if you have an issue thanks in advance ...","['gamma-function', 'functions', 'numerical-methods', 'inequality', 'derivatives']"
3774569,"Prove that $L^1\cap L^{\infty }\subseteq L^p$ for all $p\in [1,\infty]$","My work: Let $f\in L^1\cap L^{\infty }$ then $\left \| f \right \|_1,\left \| f \right \|_{\infty}$$<\infty$ $\left ( \int _X\left | f \right |^pd\mu  \right )^{1/p}=\left ( \int _X\left | f \right |^{p-1}\left | f \right |d\mu  \right )^{1/p}\leq \left ( \left \| f \right \|_{\infty}^{p-1}\left \| f \right \|_1 \right )^{1/p}< \infty$ Then $f\in L^p$ Correct ? ( $(X,A,\mu)$ is the given measure space)","['measure-theory', 'lp-spaces']"
3774576,Riemann-Roch theorem for minimal Kähler surface.,"Let $X$ be a compact minimal non-algebraic Kähler surface. In Buchdahl's paper Algebraic deformations of compact Kähler surfaces p458, the author gives an equation: $20=8h^{0,1}(X)+h^{1,1}(X)$ , I don't know how to get this equation. The explanation by the author is: $c_1(X)^2=0$ and $h^{2,0}(X)=1$ , I know $c_1(X)^2=0$ is deduced by minimal and non-algebraic condition, but I can't see why $h^{2,0}(X)=1$ , the author also said from the Riemann-Roch Theorem and the fact that $\mathcal X(X)=c_2(X)[X]$ it follows easily that $20=8h^{0,1}(X)+h^{1,1}(X)$ , unfortunately, for me it's not so easy, so may someone tell me why $h^{2,0}(X)=1$ and give more details about how to use Riemann-Roch to deduce this formula? Thanks!","['complex-geometry', 'complex-manifolds', 'algebraic-geometry', 'kahler-manifolds']"
3774593,"Is it true that $\int \hat{f}g \, dx=\int f\hat{g} \, dx$ for $f,g\in L^2$?","It is a very well know fact that the formula holds in $L^1$ , I guess that the formula still  holds for $f,g\in L^2$ .","['harmonic-analysis', 'fourier-analysis', 'real-analysis']"
3774599,Quotient of a Lie algebra by a subalgebra - what is it?,"The quotient $G/H$ of a group $G$ by its subgroup $H$ has a $G$ -action - every transitive $G$ -set is of this form. However, the quotient space $\mathfrak g/\mathfrak h$ of a Lie algebra $\mathfrak g$ by its subalgebra $\mathfrak h$ is just a vector space. By analogy with the group case, I am trying to figure out whether it is still ""special"" in some way. If $\mathfrak g$ is the Lie algebra of a Lie group or an algebraic group $G$ with some representation $V$ , and $\mathfrak h$ is the Lie algebra of the stabilizer $H=G^v$ of some vector $v\in V$ , then there is a way to identify the tangent space of the orbit $Gv$ at $v$ with the space $\mathfrak gv=\{gv\mid g\in\mathfrak g\}$ , so that the surjective map $\mathfrak g\twoheadrightarrow\mathfrak gv$ sending $g$ to $gv$ has kernel $\mathfrak h$ . Note that although $\mathfrak g$ acts on $V$ , the subspace $\mathfrak gv$ is not in general closed under the $\mathfrak g$ -action (and does not in general contain $v$ ). So one possibility to relate spaces $\mathfrak g/\mathfrak h$ to quotients like $G/H$ would be to ask whether there exists a $\mathfrak g$ -representation $V$ and a vector $v\in V$ such that $\mathfrak h=\{g\in\mathfrak g\mid gv=0\}$ and there is an isomorphism $\mathfrak g/\mathfrak h\cong\mathfrak gv$ compatible with the quotient maps $\mathfrak g\twoheadrightarrow\mathfrak g/\mathfrak h$ sending $g$ to $g+\mathfrak h$ and $\mathfrak g\twoheadrightarrow\mathfrak gv$ sending $g$ to $gv$ . Can this (or maybe something better) be always done?","['lie-algebras', 'algebraic-groups', 'representation-theory', 'algebraic-geometry', 'homogeneous-spaces']"
3774619,An Inversion of the Law of Large Numbers,Assume $(X_{n})$ are i.i.d. and $E[|X_{1}|]=\infty$ . Can we conclude that $\frac{S_{n}}{n}$ does not converge in probability? We certainly know that $\frac{S_{n}}{n}$ cannot converge a.s. since the event $$A=\{|X_{n}|>n \; \infty \text{-often}\}$$ has probability $1$ by Borel-Cantelli and thus $|\frac{S_{n}}{n}-\frac{S_{n+1}}{n+1}|$ will be greater than some $\epsilon>0$ infinitely often.,"['borel-cantelli-lemmas', 'convergence-divergence', 'probability-theory']"
3774621,The set of all vector spaces [duplicate],"This question already has an answer here : Why is the collection of all groups a proper class rather than a set? (1 answer) Closed 3 years ago . How can I prove that the set of all vector spaces doesn't exist? (In other words, if I gather all vector spaces, then it cannot be a set)",['elementary-set-theory']
3774631,Changing order of integration using chart technique error?,Consider: $$ \int_0^3 \int_4^{\sqrt{25-z^2}} \int_{-\sqrt{25-y^2-z^2}}^{\sqrt{25-y^2-z^2}} dxdydz$$ i. Clearly sketch the graph of the solid whose volume this triple integral determines. ii. Present all other triple integrals in rectangular coordinates equivalent to the given triple integral but each of a different order than the others.<br> Here is my graph and chart for changing order of integration any help at all in confirming my answers would be great! I also added my work if that helps as well.,"['integration', 'order-of-integration', 'multivariable-calculus', 'multiple-integral']"
3774637,Proving a homeomorphism between $RP^n$ and $D^n/{\sim}$,"How to prove that $RP^n$ is homeomorphic to quotient space $D^n/{\sim}$ ,where $x\sim -x$ for $x\in\partial D^n$ . Definition: $RP^n=R^{n+1}\smallsetminus\{0\}/{\sim}$ where $\sim$ is defined as : $x\sim\lambda x$ for all $\lambda\in R^x,$ $x\in R^{n+1}\smallsetminus\{0\}.$ $R^x$ is the productive group of scalars that are not $0.$ It is a little weird to me to build a homeomorphism like this.","['general-topology', 'projective-geometry', 'geometry']"
3774663,Prove that $\lim_{x\to 0} \frac{f(x)}{f'(x)} = 0$ for $f\in C^1$ and $f(0)=0=f'(0)$,"Let $f\in C^1(\mathbb{R})$ , with $f(0)=0$ and $f'(0)=0$ . Furthermore, assume that in some neighborhood around $0$ , $f$ and $f'$ have no additional zeros, so $f^{-1}(\{0\})=\{0\}=(f')^{-1}(\{0\})$ .
I want to show that $\lim_{x\to 0} \frac{f(x)}{f'(x)}=0$ . EDIT: According to a comment, this statement might be false. Would it be possible to prove the following, weaker statement: If $\lim_{x\to 0} \frac{f(x)}{f'(x)}=y$ , then, $y\in\{0,+\infty,-\infty\}$ ? My attempt so far is to write $$
\lim_{x\to 0} \frac{f(x)}{f'(x)} = \lim_{x\to 0} \lim_{h\to 0} \frac{hf(x)}{f(x+h)-f(x)}
\stackrel{?}{=} \lim_{h\to 0} \lim_{x\to 0} \frac{hf(x)}{f(x+h)-f(x)}
= \lim_{h\to 0} \frac{h\cdot 0}{f(h)-0} = 0.
$$ As indicated by the ""?"" above the ""="", I am not sure how to prove that I am allowed to exchange these limits. I tried to apply the Moore-Osgood theorem. If I understand the theorem correctly, it boils down to showing: For all $h\neq 0$ , the limit $\lim_{x\to 0} \frac{hf(x)}{f(x+h)-f(x)}$ exists. This limit is always equal to $0$ , by the same calculation as above. For all $x\neq 0$ , the limit $\lim_{h\to 0} \frac{hf(x)}{f(x+h)-f(x)}$ exists. This limit is equal to $\frac{f(x)}{f'(x)}$ and thus exists. One of the limits converges uniformly, i.e., either the first limit converges uniformly for $h\neq 0$ , or the second limit converges uniformly for $x\neq 0$ . Unfortunately, I am stuck at showing uniform convergence of either of the two limits. I have the following questions: Is the statement I am trying to prove correct, or do I need further assumptions? Is my proof strategy correct so far? Is there a simpler way? Is one of the limits actually uniform? If so, can someone give me a hint on how to show it?",['limits']
3774679,"Showing whether an ideal in $\mathbb{Z}[x,y]$ is prime.","The ideal $(1+x^2,1+y^2)$ is prime in $\mathbb{Z}[x,y]$ ?
I have this:
Analogously to $\mathbb{Z}[x]/(1+x^2)\simeq \mathbb{Z}[i]$ , $\mathbb{Z}[x,y]/(1+x^2,1+y^2)\simeq \mathbb{Z}[i]\times \mathbb{Z}[i]$ and $\mathbb{Z}[i]\times \mathbb{Z}[i]$ is not a  integral domain. Therefore $(1+x^2,1+y^2))$ is not prime. This is correct? pd: The ideal $(p)$ , $p$ prime is prime in $\mathbb{Z}[x,y]$ ? I have this: $\mathbb{Z}[x,y]/(p)\simeq (\mathbb{Z}[x]/(p))[y]\simeq (\mathbb{Z}_{p}[x])[y]$ and $\mathbb{Z}_{p}$ is a field then $\mathbb{Z}_{p}[x]$ is a field?","['ring-theory', 'abstract-algebra']"
3774681,What is the connection between the Fundemental group and homotopy equivalent,"I am just beginning learning algebraic topology after finishing point-set topology. I did not have background in group theory(only know some basic fact about it).
For this question, can I just claim if torus and klein bottle do not have same fundemental group,then they are not homotopy equivalent?
This is the first time I have encountered such questions and if anyone can give explanations in details, then it may help me to learn greatly. Thank you.","['group-theory', 'algebraic-topology']"
3774710,Relaxing Statistical Independence,"Given two random variables $x \sim X$ and $y \sim Y$ , for finite sets just to keep thing simple, we say that they are independent if $$
 \Pr[ x = x_0, \; y = y_0 ] = \Pr[x = x_0] \cdot \Pr[y = y_0]
$$ for every $x_0, y_0 \in X \times Y$ (In general one replace singletons with measurable sets of the respective space). For several applications I felt the need to define a relaxed notion of independence, that I called $\varepsilon$ -independence and is satisfied if $$
 I(x,y) = \frac{1}{2} \sum_{x_0 \in X} \sum_{y_0 \in Y} \left| \Pr[x = x_0, \; y = y_0] - \Pr[x = x_0] \Pr[y = y_0] \right| \leq \varepsilon.
$$ For those familiar with the statistical distance it's easily proved that $$
  I(x,y) = \sum_{x_0 \in X} \Delta(y_{|x = x_0}, y) \Pr[x = x_0]
$$ where $y_{|A}$ is the random variable such that $\Pr[y_{|A} = y_0] = \Pr[y = y_0 \mid A]$ for any event $A$ (all of this can be easily generalized for any probability spaces and in that languagge $I(x,y)$ is simply the distance between the joint distribution $(x,y)$ and the product distribution $x \times y$ ). In my opinion the notion fits so well in the general theory that there has to be a reference. Can anyone point this out?","['probability-theory', 'reference-request']"
3774721,Fundamental solution of a first order distributional equation,"Which is a solution of $u^{'}+\alpha u=\delta_0$ ? What about $u^{'}+f(x)u=\delta_0$ ? (Where $u$ is a distribution over an open set $\Omega$ with $\Omega\subseteq\mathbb{R}$ , $\alpha$ a real constant and $f\in C^{\infty}(\Omega)$ and $\delta_0$ is the Dirac's delta in $0$ ). I think i have to use the Fourier's transform but i am not sure about the calculations can someone please give me some advice?","['fundamental-solution', 'ordinary-differential-equations', 'distribution-theory']"
3774734,Low Number of trials for Bernoulli trial - cannot use normal approximation,"I am investigating whether an event that can be treated as a Bernoulli trial is statistically likely. However, I only have $ n = 5 $ trials, and so I don't believe I can approximate a normal distribution and employ a hypothesis test. Is there any way around this? I understand that statistics inherently relies on the law of large numbers so I assume I just have too few trials ultimately.","['statistics', 'normal-distribution', 'hypothesis-testing']"
3774762,How to understand the difference between independent events and independent random variables?,"I'm trying to learn probability on my own and have recently been studying random variables. The book I'm using provides an explanation of why the criterion for event independence is different than the criterion for random variable independence but I just can't get my head around it. ""Definition 3.8.2 (Independence of many r.v.s).
Random variables $X_1 , \ldots , X_n$ are independent if \begin{align}
& P (X_1 \leq x_1 , \ldots , X_n \leq x_n ) \\[6pt]
= {} & P (X_1 \leq x_1 ) \cdots P (X_n \leq x_n ), \text{ for all } x_1 , \ldots , x_n \in\mathbb R.\end{align} For infinitely many r.v.s, we say that they are independent if every finite subset of the r.v.s is independent. Comparing this to the criteria for independence of $n$ events, it may seem strange that the independence of $X_1 , \ldots , X_n$ requires just one equality, whereas for events we needed to verify pairwise independence for all $\binom{n}{2}$ pairs, three-way independence for all $\binom{n}{3}$ triplets, and so on. However, upon closer examination of the definition, we see that independence of r.v.s requires the equality to hold for all possible $x_1 , \ldots , x_n$ -- infinitely many conditions!"" So somehow, the criteria that each r.v. being tested for independence can take on any value and have the equality still hold allows us to infer that there is tuple-wise independence between each r.v. being tested as well, unlike the criteria for events. Can someone help illuminate this for me?","['independence', 'probability', 'random-variables']"
3774765,A question about finding Lebesgue measure of a specific set I am unable to find,"This is a quiz question of previous year asked in my measure theory exam and I am unable to solve it. Let $k$ be a positive integer and let $$S_{k} = \{x \in [0, 1] | \text{ a decimal expansion of $x$ has a prime digit at its $k$-th place}\}.$$ Then the Lebesgue measure of $S_{k} $ is? I know the definition of Lebesgue measure and I self studied it from Tom M Apostol Mathematical analysis and Walter Rudin (very few online classes). But unfortunately, I don't know how this particular question can be approached and I am struck. Any help would be really appreciated.","['measure-theory', 'lebesgue-measure']"
3774774,Show that $(a^3+a+1)(b^3+b+1)(c^3+c+1)\le 27$,"Let $a,b,c\ge 0$ be such that $a^2+b^2+c^2=3$ . Show that $$(a^3+a+1)(b^3+b+1)(c^3+c+1)\le 27$$ I want to consider the function $$f(x)=\ln{(x^{3/2}+x^{1/2}+1)}$$ Maybe it isn't the case $f''(x)\le 0$ , so I can't use Jensen's inequality.","['summation', 'tangent-line-method', 'logarithms', 'multivariable-calculus', 'inequality']"
3774787,Integrals of a Hopf algebra: Why that name?,"1. Context: The notion of an integral Let $H$ be a Hopf algebra over a field $\mathbb k$ . We call its $\mathbb k$ -linear subspace $$
 I_l(H)= \{x \in H; h \cdot x=\epsilon(h)x \quad for \>all\>h\in H\}
$$ the space of left integrals. In other words, $I_l(H)$ is the space of left invariants for $H$ acting on itself by multiplication. In a similar manner one can define (the space of) right (co)integrals. Integrals seem to have a wide range of applications. For instance, they appear in a strong ""(Hopf algebra) version"" of Maschke's theorem, i.e. they are related to the semisimplicity of a Hopf algebra. 2. Question Why are integrals called integrals? Specifically, I think I overheard someone saying that they can be related to the notion of an integral in calculus. How so?","['modules', 'abstract-algebra', 'hopf-algebras', 'soft-question', 'terminology']"
3774804,Justifying $\sum_{n=0}^\infty\log(1+x^{2^n}) = -\log(1-x)$ for $0\le x<1$,"I studied the official solution to a Putnam competition problem and got stuck in a step, which is summarized as follows: For $0\le x<1$ , we have $$
\sum_{n=0}^\infty\log(1+x^{2^n}) = -\log(1-x)\tag{1}
$$ My two closely related questions below are based on the justification of (1). The solution gave the following argument for justifying (1): Due to the uniqueness of binary expansions of nonnegative integers, we have the identity of formal power series $$
\frac{1}{1-x}=\prod_{n=0}^{\infty}\left(1+x^{2^{n}}\right)\,;\tag{2}
$$ the product converges absolutely for $0\le x<1$ . But I don't understand what this means. Question 1 : In particular, how is ""the uniqueness of binary expansions of nonnegative integers"" used here? Naively, if we treat the infinite sum as a finite sum and apply (2), then we have $$
\sum_{n=0}^\infty\log(1+x^{2^n}) 
= \log \prod_{n=0}^{\infty}\left(1+x^{2^{n}}\right) 
= \log \frac{1}{1-x}
= -\log (1-x) \tag{3}
$$ But Question 2 : how can one justify the first equal sign?","['calculus', 'logarithms', 'binary', 'sequences-and-series']"
3774853,Is $L^2(\mathbb R)$ isometrically isomorphic with $\ell^2(\mathbb Z)?$,"Is $L^2(\mathbb R)$ isometrically isomorphic with $\ell^2(\mathbb Z)?$ My thoughts: We can define an operator $\mathcal L:L^2(\mathbb R)\rightarrow \ell^2(\mathbb Z)$ : $\mathcal Lf=\{\hat f(ξ)\}_{ξ\in \mathbb Z}$ (obviously $\mathcal L$ is linear & $1-1$ by uniqueness) and  by the Parseval identity we have that $\lVert f\rVert_{L^2(\mathbb R)}^2=\lVert \hat f\rVert_{\ell^2(\mathbb Z)}^2$ Hence we have an isometry. is that enough? Also, can we claim that $:L^2(\mathbb R)≅ \ell^2(\mathbb Z)?$ Thanks you. EDIT: how about $L^2([a,b])?$","['harmonic-analysis', 'operator-theory', 'fourier-analysis', 'functional-analysis']"
3774859,Global sections of a proper variety over an arbitrary field,"While doing some research, I stumbled upon the following fact that I'd taken for granted. Theorem: Let $ X $ be a proper variety over a field $ k $ (variety = geometrically integral, separated, finite type). Then $ \Gamma(X, \mathcal{O}_X) $ is a finite field extension of $ k $ . The line of proof I have in mind is as follows: Let $ s \in \Gamma(X, \mathcal{O}_X) $ be a global section. It corresponds to a morphism $ s : X \rightarrow \mathbb{A}^1_k $ . This morphism factors as $$ X \xrightarrow{(id_X, s)} X \times_k \mathbb{A}^1_k \xrightarrow{p_2} \mathbb{A}^1_k $$ where $ (id_X, s) $ is a section of the first projection $ p_1 : X \times_k \mathbb{A}^1_k \rightarrow X $ . In the composition, the first morphism is a closed immersion (being a section of the separated morphism $ p_1 $ ) and the second is a closed map by properness. So $ s $ is a closed map. The image of $ s $ is proper, closed and irreducible, hence is a single closed point in $ \mathbb{A}^1_k $ , say given by an irreducible polynomial $ h(T) \in k[T] $ . This shows that $ h(s) = 0 $ in $ \Gamma(X, \mathcal{O}_X) $ and hence the global sections form a field. If $ k $ was assumed algebraically closed, then $ h $ must be of the form $ T - a $ for some $ a \in k $ . In this case, we recover the result that $ \Gamma(X, \mathcal{O}_X) = k $ . However if $ k $ is arbitrary, I don't see how to get finite dimensionality of $ \Gamma(X, \mathcal{O}_X) $ although $ s $ itself lies in a finite extension. Question: How to complete the proof? Does one rely on Grothendieck's (hard) result that for a coherent sheaf on a proper variety, all cohomology groups are finite dimensional? Or is there a proof not using this result? Most references either just cite this fact or deal with the algebraically closed case only.",['algebraic-geometry']
3774889,"Example of a cocommutative, non-unimodular Hopf algebra?","1. Definitions: Unimodularity and cocommutativity Let $H$ be a Hopf algebra over a field $\mathbb k$ . We call $H$ unimodular if the space of left integrals $I_l(H)$ is equal to the space of right integrals $I_r(H)$ . We call $H$ cocommutative if $\tau_{H,H} \circ \Delta = \Delta$ . Here, $\Delta$ denotes the coproduct of $H$ , while $\tau: H \otimes H \rightarrow H \otimes H; v \otimes w \mapsto w \otimes v$ is the twist map. 2. Question In my lecture notes it says that there are cocommutative, non-unimodular Hopf algebras. What would be an example? Apparently, an example is given in Hopf algebras and their action on rings by Susan Montgomery. However, due to the pandemic I am unable to get it from the library. If you have a copy and could write down the relevant section, that would be very much appreciated. 3. My ideas so far The Taft-Hopf algebra $H$ over a field $\mathbb k$ is not an example:
If $H$ is commutative (i.e. root of unity $\zeta =1_{\mathbb k}$ ), then $H$ is unimodular. In this case, it is even isomorphic to the boring group algebra of the zero group. Otherwise, $H$ is not cocommutative (even though it is non-unimodular then). Non-cocommutativity follows easily from the observation that the square of the antipode is not the identity (if $\zeta \neq 1_{\mathbb k} $ ). Group algebras:
As the coproduct of a group algebra is given by the diagonal map any group algebra is cocommutative. However, any group algebra $\mathbb k[G]$ over a finite group $G$ is unimodular, since $$I_l=I_r=\mathbb k \cdot \sum\limits_{g\in G} g$$ What about infinite groups? Regarding the universal enveloping algebra, tensor algebra, symmetric algebra, alternating algebra I am not sure. What can be said here? Maybe the following proposition turns out to be useful: A finite dimensional Hopf algebra $H$ is unimodular iff its distinguished group-like element/modular element $a \in G(H^*)$ is equal to the counit $\epsilon_H$ . Here, the modular element $a$ is the unique linear form such that $t\cdot h = t a(h)$ for all $h\in H, t\in I_l(H)$ . It exists because $t\cdot h \in I_l(H)$ and $I_l(H)$ is one dimensional. It can be shown to be a morphism of algebras, hence a group-like element in $H^*$ .","['representation-theory', 'examples-counterexamples', 'modules', 'abstract-algebra', 'hopf-algebras']"
3774909,Proper Subgroup of $O_2(\mathbb{R})$ Isomorphic to $O_2(\mathbb{R})$,"in his very beautiful answer to my post Subgroup of Plane Isometries Isomorphic to $O_2(\mathbb{R})$ , Angina Seng stated without proof that there exists a proper subgroup of $O_2(\mathbb{R})$ which is isomorphic to $O_2(\mathbb{R})$ .
He suggested to use the usual ""Zorn's Lemma/Hamel basis"" argument, but I cannot see what he really meant, since $O_2(\mathbb{R})$ is not even a vector space. Could anyone help me please?
Thank you very much for your help in advance.","['group-theory', 'set-theory']"
3774917,"Show that in a Hilbert space, $||x+\alpha y|| \ge ||x|| \,\forall\,\alpha\in\mathbb{C}\implies (x,y)=0$","We have a Hilbert space, equipped with the norm $||\cdot||=\sqrt{(\cdot,\cdot)}$ , and we're given the following inequality holds for all $\alpha\in\mathbb{C}: ||x+\alpha y||\ge ||x||$ How would you go about showing this inequality leads to $(x,y)=0$ Squaring both sides I get to $(x,x)+\overline{\alpha}(x,y)+\alpha\overline{(x,y)}+|\alpha|^2(y,y)\ge (x,x)\implies \overline{\alpha}(x,y)+\alpha\overline{(x,y)}+|\alpha|^2(y,y)\ge 0$ I can write this in terms of real or imaginary parts depending on choice of $\alpha$ but I'm perplexed how to make this something meaningful. Any help appreciated.","['inner-products', 'normed-spaces', 'complex-analysis', 'hilbert-spaces', 'functional-analysis']"
3774967,Convex geometric realizability of abstract polyhedron with congruent isosceles obtuse triangular faces,"Question below. Some background: Take an isosceles obtuse triangle of the form with $\alpha = \frac{n-1}{n}\pi$ for some $n \geq 3$ ( $\beta=\frac{\pi}{2n}$ ) If you look at the class of convex polyhedra such that each face is congruent to this triangle, then the vertices are of the following types: $\alpha^2\beta^2$ $\beta^4$ , $\beta^6$ , ..., $\beta^{4n-2}$ $\alpha\beta^{2n}$ A type indicates which face angles go around it. Lets look at what happens if such a polyhedron has a vertex of type $\beta^m$ ( $m=4,6, ..., 4n-2$ ): The vertices colored red, only allow for the type $\alpha^2\beta^2$ and so they can be filled in. The next picture shows this: where lines that are marked similarly should be pasted together. This can be done in all vertices colored red, obtaining This fixes thus a unique abstract polyhedron $P_m$ .
Abstract here also fixes the face angles and the edge lengths (up to scale).
This shape (if it is geometrically realizable) somewhat resembles a trapezohedron where the faces are bent on the long diagonal. Which of the $P_m$ is geometrically realizable as a convex polyhedron? (we do not allow edges with dihedral angle $\pi$ ) And how would you prove this?
After trying out with some paper models, it seems that the only polyhedron that is geometrically realizable is $P_4$ . I can prove that $P_6$ is never geometrically realizable as a convex polyhedron $Q$ :
Any net of $Q$ can be folded to a parallelepiped with kite faces (angles $\alpha$ , $2\beta$ , $\alpha$ , $2\beta$ ). By Alexandrov's uniqueness theorem , this means that $Q$ is a parallelepiped; and thus has dihedral angles $\pi$ (two triangles joined together with their long edge form a face of this parallelepiped) At first I thought I could extend this proof to $m > 6$ by showing that the net folds to a trapezohedron, but proving this is much harder than in case of the parallelepiped; because now we have to show that there exists a geometrically realizable trapezohedron with all faces congruent to a kite with angles $\alpha, 2\beta, \alpha, 2\beta$ . Moreover, it might be that this is not the case; which means we have to find another way to prove this. Question : How to prove that $P_m$ is not geometrically realizable as a convex polyhedron for $m > 6$ ? How to prove that $P_4$ is geometrically realizable as a convex polyhedron? Which general techniques exist for proving similar results?
I would prefer answers that do not use coordinate calculations. Results that I think could help: Lemma 2a p 162 of Convex Polyhedra If two convex polyhedral angles, distinct from dihedral angles
and possibly degenerate, have corresponding planar angles of equal measure
while not all of their dihedral angles are equal, then there are at least four
sign changes in the differences between the corresponding dihedral angles as
we go around the vertices. This can be used to find minimal and maximal dihedral angles. For example, in a $\alpha^2\beta^2$ vertex, if we set the dihedral angle between the two $\beta$ to $\pi$ ; this is still realizable. This angle should decrease. Therefore the opposite angle has to decrease as well; which shows that the dihedral angle between the two $\alpha$ reaches its maximum when the dihedral angle between the two $\beta$ is $\pi$ . This dihedral angle $A$ satisfies the following formula : $\cos(A) = \frac{\cos(2\beta) - \cos^2(\alpha)}{\sin^2(\alpha)} = \frac{-\cos(\alpha) - \cos^2(\alpha)}{\sin^2(\alpha)}$ This question is part of a search to classify the convex polyhedra with congruent isosceles triangular faces (the faces do not need to be transitive).","['polyhedra', 'convex-geometry', 'geometry', 'triangles', 'geometric-realization']"
3775016,Show that $\cos\big(\frac{2\pi}{n}\big)$ is an algebraic number,"$\bullet~$ Problem: Show that $\cos\bigg(\dfrac{2\pi}{n}\bigg)$ is an algebraic number [where $n$ $\in$ $\mathbb{Z} \setminus \{0\}$ ]. $\bullet~$ My approach: Let's consider the following polynomial in $\mathbb{Z}[x]$ in recursive terms. \begin{align*}
    &T_{0}(x) = 1\\
    &T_{1}(x) = x\\
    &T_{n + 1}(x) = 2x T_{n}(x) - T_{n-1}(x)
\end{align*} $\bullet~$ $\textbf{Claim:}$ The polynomial $T_{n}(x)$ for any $n$ $\in$ $\mathbb{N}$ satisfies the following \begin{align*}
    T_{n}(\cos(\theta)) = \cos(n\theta)  
\end{align*} $\bullet~$ Proof: We'll use induction on $n$ for this proof. At first, we easily obtain that for $n = 0$ the given is true. Now for some $n = k$ , we assume that \begin{align*}
    T_{k}(\cos(\theta)) = \cos(k\theta)  
\end{align*} Therefore we need to prove for $n = (k + 1)$ . Now from the recursion relation of $T_{n}(x)$ we have \begin{align*}
    T_{k + 1}(\cos(\theta)) & = 2 \cos(\theta)T_{k}(\cos(\theta)) - T_{k -1}(\cos(\theta))\\   
    & = 2 \cos(\theta) \cos(k\theta) - \cos((k -1)\theta)\\
    & = 2 \cos(\theta) \cos(k\theta) - \cos(k\theta) \cos(\theta) - \sin(k\theta)\sin(\theta)\\
    & = \cos((k + 1)\theta)
\end{align*} Hence by induction hypothesis, we obtain that our claim is true. Therefore we have \begin{align*}
    T_{n}\Bigg(\cos\bigg(\frac{2\pi}{n}\bigg)\Bigg) = \cos(2\pi) = 1
\end{align*} Therefore we just need to consider a polynomial $P(x) = T_{n}(x) - 1.~$ As $T_{n}(x) \in \mathbb{Z}[x]$ it implies $P(x) \in \mathbb{Z}[x]$ Therefore we have $\cos\big(\frac{2\pi}{n}\big)$ is an algebraic number. Please check the solution and point out the glitches. Can you prove this in a different (like a pretty elementary one (by not using the idea of cyclotomic polynomials or Chebyshev's Polynomials)) way? $\bullet~$ $\large{\textbf{Edit:}}$ $\blacksquare~$ Alternate Approach: I have used the expansion of $\cos\bigg( \dfrac{2\pi}{n} \bigg)$ . And obviously which comes from de-Moivre's (simple for $n \in \mathbb{Z}$ ). Can you please try to give a solution not using these arguments? (de-Moivre's, Cyclotomic Polynomial, $\color{blue}{\text{Chebychev Polynomials}}$ , etc etc).","['ring-theory', 'abstract-algebra', 'solution-verification']"
3775049,Expression for the Clarke subdifferential of a weakly convex function,"Let $\gamma\in\left]0,+\infty\right[$ , let $f$ be a proper, convex, lower semicontinuous function from a real Hilbert space $\mathcal{X}$ to $\left]-\infty,+\infty\right]$ , and set $g=f-\frac{\gamma}{2}\|\cdot\|^2$ . Then $g$ is weakly convex. I'm looking for a reference characterizing for which $x\in\mathcal{X}$ the following holds \begin{equation}
\partial_{Clarke} g(x) = \partial_{convex} f(x) - \gamma x. \tag{*}
\end{equation} where $\partial_{Clarke}$ is the Clarke subdifferential, $$\partial_{convex} f(x) = \left\{ u \in \mathcal{X} \mid (\forall y \in \mathcal{X}) \quad \langle y - x \mid u \rangle + f(x)\leq f(y) \right\},$$ and the righthand side in (*) denotes Minkowski subtraction. I know that $(\nabla \frac{\gamma}{2}\|\cdot\|^2) (x) = \gamma x$ and that $\partial_{convex}$ coincides with $\partial_{Clarke}$ on convex functions. However, I am only working with a weakly convex function. I've perused Rockafellar/Wets but not found much. I'm actually not entirely positive that (*) is true everywhere, e.g. it may fail on the boundary of the domain of $g$ . Any relevant info is greatly appreciated! EDIT: I think it would suffice to find a reference for when the sum rule holds for Clarke subdifferentials. I believe that $\partial_{Clarke}g(x)\supset\partial_{convex}f(x)-\gamma x$ , so it would suffice to show the reverse inclusion.","['non-smooth-analysis', 'convex-analysis', 'functional-analysis', 'reference-request']"
3775146,Convergence of series using domination,"Let $(x_n)_n$ be a sequence of $]0,+\infty[,y_n=\sum_{k=1}^nx_k$ such that $\lim_n y_n=+\infty.$ Let $p>1.$ Prove that $$\sum_{n}\dfrac{x_n}{y_n(\ln(y_n))^p}$$ converges. Maybe the easiest way to prove it is to show that $\sum_n\dfrac{x_n}{y_n(\ln(y_n))^p}$ is dominated with a convergent series. Any ideas ?","['sequences-and-series', 'analysis', 'real-analysis']"
3775157,Is the braid group hyperbolic?,"The braid groups satisfy a number of properties that one would expect of a hyperbolic group , liking having a solvable word problem, and having exponential growth. Are the braid groups hyperbolic groups? If not, is there any obvious property of hyperbolic groups showing that they are not?","['geometric-group-theory', 'group-theory', 'hyperbolic-groups', 'braid-groups']"
3775180,Equivalence of sets.,"Prove that if $M$ is an arbitrary infinite set and $A$ is countable, then $M \sim M \cup A$ $M\sim N$ are said to be equivalence if a one-to-one correspondence can be set up between their elements This exercise comes in the book of funtional analysis to Kolmogorov. My doubt goes around how a formal proof would be written, in this book several examples come, but, the functions are not built. I think it is clear how to create function 1-1, assigning the first elements of $M$ to $A$ and the rest of them back to $M$",['elementary-set-theory']
3775230,Solving : $5^{x^2+6x+8}$ = 1,"Solve for $x$ : $$5^{x^2+6x+8} = 1$$ So, I took the natural logarithm on both sides, $$(x^2+6x+8)\ln(5) = \ln(1)$$ then I divide both sides by $\ln(5)$ to set the polynomial to zero because we know $\ln(1) = 0$ .
I will be left with: $$x^2+6x+8 = 0$$ Factoring this will give: $$(x+2)(x+4) = 0 \implies x = -2, -4 $$ Then I checked my $x$ values I got $1$ . So my question is did I do it correctly?","['algebra-precalculus', 'solution-verification']"
3775240,Please check my solution - counting elements in a group,"I'm fairly sure that my basic reasoning is correct, but I'm worried about a lack of rigor in my answers. Nitpicking would be highly appreciated. The exercises are from Aluffi's Algebra Chapter 0. Problems. 1.8 Let $G$ be a finite abelian group, with exactly one element $f$ of order 2. Prove that $\prod_{g\in G}g=f$ 1.9 Let $G$ be a finite group, of order n, and let m be the number of elements $g \in G$ of order exactly 2. Prove that n-m is odd. Deduce that if n is even, then G necessarily contains elements of order 2. Solutions. 1.8 Every other element has a distinct inverse (or is the identity), and so gets cancelled out, but $f$ , being its own inverse, remains uncancelled. Consider a product of elements $g_1 g_2 .. f .. g_{n-1}$ where $n=|G|$ . Now since $G$ is abelian, we may reorder this product as we choose. One of the $g_k$ is the identity, so we may ignore that. Now consider some other $g_k$ which is neither the identity nor $f$ . Thus this $g_k$ has order greater than 2. This means that $g_k$ has an inverse in $G$ which is different than itself. (If it was its own inverse then its order would be 2). So we may place the elements $g_k$ next to their inverses in the product, so that the whole product becomes $eee..f = f$ . 1.9 We will try to pair elements with their inverses. $n-m$ is the number of elements that have order not equal to $2$ . We can split this into two types: elements with order greater than $2$ , and the identity. So it suffices to show that there is an even number of elements with order greater than $2$ , since there is only $1$ identity. Consider the distinct elements of the group with order greater than $2$ . Since they have order greater than $2$ , by the same logic as given in the previous exercise, they must have a inverse which is different than itself and shared by no other element. So we can pair each element with its inverse and obtain a list containing all the elements of the group with order greater than two in pairs. Thus there is an even number of elements in the group with order greater than two.","['group-theory', 'abstract-algebra', 'finite-groups', 'solution-verification']"
3775300,The Mean First Passage Time of a Markov Chain with Infinite Number of States,"Consider the following Markov chain ( $q = 1-p$ ): I want to find the mean first passage time $m(i ,j) (i, j \geq 0)$ , where $m(i, j)$ denotes the expected number of steps to reach state $j$ when the Markov chain starts from state $i$ . But I'm not sure where to start, as there are an infinite number of states, and couldn't deduce a finite number of equations.","['random-walk', 'markov-chains', 'stochastic-processes', 'probability-theory', 'probability']"
3775358,What is $\lim_{N\to\infty}\frac{-2}{\pi}\sum_{n=1}^N \frac{(-1)^n}{n} \sin(n\frac{N\pi}{N+1})$?,"Here is what I have so far: $$\lim_{N\to\infty} f_N \left(\frac{N\pi}{N+1}\right)$$ $$f_N (x) = \frac{-2}{\pi}\sum_{n=1}^N \frac{(-1)^n}{n} \sin(nx)$$ $$\implies \lim_{N\to\infty} f_N \left( \frac{N\pi}{N+1}\right)=\lim_{N\to\infty}\frac{-2}{\pi}\sum_{n=1}^N \frac{(-1)^n}{n} \sin\left(n\frac{N\pi}{N+1}\right)$$ $$=\lim_{N\to\infty}\frac{-2}{\pi}\left(-\sin\left(\frac{N\pi}{N+1}\right)+\frac{1}{2}\sin\left(\frac{2N\pi}{N+1}\right)-\frac{1}{3}\sin\left(\frac{3N\pi}{N+1}\right)+\cdots \pm \frac{1}{N}\sin\left(\frac{N^2 \pi}{N+1}\right) \right)$$ $$=\frac{2}{\pi}\lim_{N\to\infty}\frac{N\pi}{N+1}\left(\frac{\sin( \frac{N\pi}{N+1})}{\frac{N\pi}{N+1}} -\frac{\sin( \frac{2N\pi}{N+1})}{\frac{2N\pi}{N+1}}+\frac{\sin( \frac{3N\pi}{N+1})}{\frac{3N\pi}{N+1}} ... \pm \frac{\sin( \frac{N^2 \pi}{N+1})}{\frac{N^2 \pi}{N+1}}\right)$$ And in the last step I suppose I use the Riemann sum using midpoints to find the corresponding integral and evaluate it however I am a bit confused as to what integral I get. If there is a better way to evaluate this limit, I am open to suggestions. The expected answer is approximately 1.18.","['integration', 'riemann-sum', 'fourier-series', 'limits', 'trigonometry']"
3775422,"Find all $P(x)$ $\in$ $\mathbb{R}[x]$ such that for some $c \in \mathbb{R}$, the functional equation $ (x + 1)P(x - 1) - (x - 1)P(x) = c $ holds.","$\blacksquare~$ Problem: Find all $P(x)$ $\in$ $\mathbb{R}[x]$ such that for some $c \in \mathbb{R}$ , the following functional equation holds \begin{align*}
    (x + 1)P(x - 1) - (x - 1)P(x) = c
\end{align*} $\blacksquare~$ My Attempt: We will check at first for $x = 0 \text{ and } -1$ and by some simple calculation we will obtain that \begin{align*}
    P(0) = P( - 1) = \frac{c}{2}
\end{align*} Let's consider a polynomial $Q(x)$ $\in$ $\mathbb{R}[x]$ such that \begin{align*}
    Q(x) = P(x) - \frac{c}{2}
\end{align*} Therefore our new functional equation becomes \begin{align}
    &(x + 1)Q(x - 1) - (x - 1) Q(x) = 0\\
    \implies & (x + 1)Q(x - 1) = (x - 1) Q(x) \quad \quad \cdots\cdots (1)
\end{align} Then we will make a claim about our new defined polynomial $Q(x)$ . Claim: The polynomial $Q(x)$ follows the following conditions \begin{align*}
Q ( x ) = \begin{cases} 
  0 & \text{ if } Q \text{ is a constant polynomial in } \mathbb{R}[x] \\
 b \cdot x ( x + 1 ) & \text{ otherwise and for some }b \in \mathbb{R} \\
   \end{cases}
\end{align*} $\bullet~$ Proof: From the way we constructed $Q(x)$ we have \begin{align*}
    Q(0) = Q(-1) = 0
\end{align*} therefore let's consider the two cases separately $\bullet~$ Case $1$ : We have $Q$ as a constant polynomial. Say for some $a$ $\in$ $\mathbb{R}$ \begin{align*}
        Q(x) \equiv a \quad \text{for all } x \in \mathbb{R}
    \end{align*} Then, we have for any $x$ in $\mathbb{R}$ \begin{align*}
        &a \cdot (x + 1) = a \cdot (x - 1)\\
        \implies & ax + a = ax - a\\
        \implies & a = 0
    \end{align*} Hence the first part of our claim is proved. $\bullet~$ Case $2$ : If $Q(x)$ is non-constant, then it's obvious that the polynomial $Q(x)$ has roots $0, -1,$ therefore, \begin{align*}
        Q(x) = x(x + 1) M(x) \quad \text{for some polynomial } M(x) \in \mathbb{R}[x]
    \end{align*} such that $\text{deg}(Q) > \text{deg}(M)$ . Therefore, we have from $(1)$ \begin{align*}
        \implies &(x - 1) \cdot x \cdot (x + 1) M(x - 1) = (x - 1) \cdot x \cdot (x + 1) M(x)\\
        \implies & M(x - 1) = M(x) \quad \text{for all } x \in \mathbb{R} - \{-1, 0, 1\} 
    \end{align*} Now on doing $x \mapsto x + 1$ we have \begin{align*}
        M(x) = M(x + 1) \quad \text{and similarly for each map } ( x + j ) \mapsto \{x + (j + 1)\} 
    \end{align*} Hence we have the conclusion \begin{align*}
        &M(x) = M(x + 1) = M(x + 2) = \cdots = M(x + n) = \cdots\\
        \implies & M(x) \equiv b \quad \text{for some }b \text{ in } \mathbb{R} \text{ and } \forall \text{  } x \in \mathbb{R} 
    \end{align*} again from the quotient form of $Q(x)$ we have \begin{align*}
        Q(x) = b \cdot x(x + 1) \quad \text{for some } b \text{ in } \mathbb{R} 
    \end{align*} Hence we have proved the second part of our claim too. Therefore we have proved both our claims! Hence we obatin that the polynomial $Q(x)$ precisely. Which implies that our polynomial $P(x)$ will be \begin{align*}
P ( x ) = \begin{cases} 
  Q(x) + \frac{c}{2} = \frac{c}{2} & \text{ if } Q \text{ is a constant polynomial in } \mathbb{R}[x]\\ 
Q(x) + \frac{c}{2} =  b \cdot x ( x + 1 ) + \frac{c}{2} & \text{ otherwise and for some }b \in \mathbb{R} \\
   \end{cases}
\end{align*} Hence we are done! Please check the solution, and please give some new ideas too :)","['contest-math', 'algebra-precalculus', 'solution-verification', 'polynomials']"
3775432,Evaluating $\int_{0}^{\infty}\frac{\sin^{2n+1}x}{x}\mathrm{d}x$,"I was generalizing the following integral: $$\int_{0}^{\infty}\frac{\sin^{2n+1}x}{x}\mathrm{d}x \hspace{40pt} n\geq 0$$ We can start by noting that $\sin x =\dfrac{e^{ix}-e^{-ix}}{2i}$ and thus $\displaystyle \sin^{2n+1}x=\frac{(-1)^n}{2^{2n}}\sum_{r=0}^n (-1)^r \binom{2n+1}{r}\sin(2r+1)x$ . This implies $$\begin{aligned}\displaystyle \int_{0}^{\infty}\frac{\sin^{2n+1}x}{x}\mathrm{d}x &=\frac{(-1)^n}{2^{2n}}\sum_{r=0}^n (-1)^r \binom{2n+1}{r}\int_{0}^{\infty}\frac{\sin(2r+1)x}{x}\mathrm{d}x \\ &=\frac{(-1)^n\pi}{2^{2n+1}}\sum_{r=0}^n(-1)^r\binom{2n+1}{r} \\ &=\frac{(-1)^n\pi}{2^{2n+1}}\sum_{r=0}^n\left((-1)^r\binom{2n}{r}-(-1)^{r-1}\binom{2n}{r-1}\right)\end{aligned}$$ Where I have used the well known result $\displaystyle \int_{0}^{\infty} \frac{\sin(2r+1)x}{x}\mathrm{d}x=\int_{0}^{\infty}\frac{\sin x}{x}\mathrm{d}x=\frac{\pi}{2}$ and the property of binomial coefficients that $\displaystyle \binom{n}{r}+\binom{n}{r-1}=\binom{n+1}{r}$ Since the above sum telescopes, we have $$\displaystyle \int_{0}^{\infty}\frac{\sin^{2n+1}x}{x}\mathrm{d}x=\frac{(-1)^n\pi}{2^{2n+1}}(-1)^n\binom{2n}{n}=\frac{\pi}{2^{2n+1}}\binom{2n}{n} ~\forall ~ n\in \mathbb{Z^{+}}$$ I would like to know other methods for evaluating this integral.","['integration', 'calculus', 'definite-integrals']"
3775445,Quadratic irrationals with continued fraction of period one,"There are some quadratic irrationals (like $\sqrt{2}, \sqrt{5},\sqrt{10}$ , etc.) that have continued fractions with a period of one (e.g. $\sqrt{2}=[1;2,2,2,2,\dots]$ ). I know the period of the fraction ends whenever $a_i=2a_0$ , but is there any pattern to the quadratic irrationals that exhibit this behavior? The Golden Ratio, for example, also famously has a period of one, with its continued fraction being $\phi = [1;1,1,1,1,\dots]$ .","['number-theory', 'irrational-numbers', 'elementary-number-theory', 'continued-fractions', 'quadratic-forms']"
3775481,Is $\mbox{Rank}(A + A^2) \leq \mbox{Rank} (A)$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Here, $A$ is an $n \times n$ matrix. I am not able to find any counterexample but not able to prove this as well. The examples I have tried so far shows me that $\mbox{Rank} (A + A^2) = \mbox{Rank} (A)$ . I don't know how to show the inequality.","['matrices', 'matrix-rank', 'linear-algebra']"
3775495,Prove that $\Gamma(\operatorname{W}(x))$ is convex $\forall x>0$,"Background : At the begining I was studing a function wich increases slowly and maybe have some property useful in number theory .Particulary I have found : Let $0<x\,$ define the function : $$f(x)=\Gamma(\operatorname{W}(x))$$ Where we see the Gamma function and the Lambert's function Then prove that : $$f''(x)>0\quad\forall x>0$$ Well working with WA wich is a little bit capricious I find that the minimum of the second derivative occurs on $I=[24800,24900]$ I have tried to solve the following expression see here without success . My second strategy is : if we know that mid-point convexity and conitinuity implies convexity we can says that we have : Let $ x,y>0$ then we have : $$f(x)+f(y)\geq 2f\Big(\frac{x+y}{2}\Big)$$ I can solve it for large value but not on $I$ describe above . Update : Following the good start by TheSimpliFire we have to prove : $$\psi(x)+\frac{(\psi(x))'}{\psi(x)}>1+\frac{1}{x+1}\quad \forall x>0$$ From the source we have (see (51) and (52)): $$\frac{\pi^2}{\pi^2x+6-\pi^2}\leq(\psi(x))' \quad \forall x\geq 1$$ And $$\log\Big((t-1)\frac{\pi^2}{6}+1\Big)-\gamma\leq\psi(t)<\log(2t-1)-\gamma\quad \forall t\geq 1$$ Perhaps there is an issue now . So if you have an idea or an approach like a hint it would be nice . Thanks a lot for all your contributions ! Max. Source : https://www.hindawi.com/journals/jam/2014/264652/","['special-functions', 'gamma-function', 'lambert-w', 'derivatives', 'convexity-inequality']"
3775506,Normal distribution sample,"Since I'am beginner in statistics I'm stuck in simple exercise so will appreciate any help. I have mean, standard deviation and probability p(x) and need to get x. Here is the Exercise The patient recovery time from a particular surgical procedure is
normally distributed with a mean of 5.3 days and a standard deviation
of 2.1 days The 90th percentile for recovery times is? I know that it's possible to get x from probability formula but I was wondering if there is easier way to get it.","['statistics', 'probability-distributions', 'normal-distribution', 'gaussian-integral']"
3775552,Find all positive integers $n$ for which $1372n^4 - 3 $ is an odd perfect square.,"Find all positive integers $n$ for which $1372\,n^4 - 3$ is an odd perfect square. I tried $\bmod ,4,5,7$ and failed. Next, I used Vieta’s Theorem and failed again. Any hints, please. Thank you very much! Edit number and parity already. Sorry for typo Edit 2 : This question is related to this question.","['number-theory', 'elementary-number-theory', 'diophantine-equations', 'square-numbers', 'perfect-powers']"
3775592,Determining solution curve of ODE,"I have been introduced to autonomous ODE's and have recently come across the notion of a non-autonomous differential equation. After some reading, I came across this logistic model, $$\frac{dx}{dt}=x(a(t)-b(t)x), \ \ \ \ x(s)=x_0.$$ Where $a(t),b(t)>0$ .
And apparently, this logistic equation has 'relatively straightforward' asymptotic behaviour since there exists an explicit solution. My question is, how would you go about finding this explicit solution? All of the variables in this equation are dependent on time. It doesn't seem to be separable either. A hint would be greatly appreciated, just to get me started at finding a solution $\space x(t) \space$ to the above equation. Thank you in advance!","['integration', 'calculus', 'derivatives', 'ordinary-differential-equations']"
3775595,"The definition of the set of positive integers in ""Topology 2nd Edition"" by James R. Munkres.","I am reading "" Topology 2nd Edition "" by James R. Munkres. In Ch. 1.4, Munkres defines the set of real numbers $\mathbb{R}$ with the field axioms (including completeness), and then defines $\mathbb{Z}_{+}$ as the smallest inductive set in $\mathbb{R}$ , as follows: A subset $A$ of the real numbers is said to be inductive if it contains the number $1$ , and if for every $x$ in $A$ , the number $x+1$ is also in $A$ . Let $\mathcal{A}$ be the collection of all inductive subsets of $\mathbb{R}$ . Then the set $\mathbb{Z}_{+}$ of positive integers is defined by the equation $$\mathbb{Z}_{+} = \bigcap_{A\in \mathcal{A}} A.$$ Munkres didn't define $\mathbb{Z}_{+} := \{1, 1 + 1, 1 + 1 + 1, \dots\}$ . Why?","['elementary-number-theory', 'integers', 'definition', 'elementary-set-theory', 'general-topology']"
3775627,"Why did we call a row operation ""elementary""?","Why we called the three actions of row operation ""elementary""? Is there a thing called ""advanced"" or ""complicated"" row operation? I've seen the word ""non-elementary"" row operation is used to describe things like $R_1-R_2$ , which is not written in the conventional $-1R_2 + R_1$ . Is this usage correct? In particular, what should $R_1-R_2$ be called? \begin{align*}
    &\text{a) A non-elementary row operation} \\
    &\text{b) An elementary row operation} \\
    &\text{c) Just a row operation} \\
    &\text{d) It is not a row operation}
\end{align*}","['matrices', 'linear-algebra', 'terminology']"
3775641,How to find solutions to parametric differential equations as non-parametric level sets?,"Say that we have a parametric differential equation, given $g_k(t)$ and $h_k(t)$ $$\cases{\sum g_k(t)\cdot {f_x}^{(k)}(t) = 0\\\sum h_k(t)\cdot {f_y}^{(k)}(t) = 0}$$ with generalized ""prime"" notation $$ (\cdot)^{(n)}= \frac{\partial^n (\cdot)}{\partial t^n}$$ So that a curve is defined by $(x,y) = (f_x(t),f_y(t))$ . Can we transform this problem to a level set for some basis of functions: $$\phi(x,y) =\sum c_k e_k(x,y) = 0$$ Own work An obvious example where this is possible would be the harmonic equations for sin and cos on the unit circle: $$\cases{f_x(t) = \cos(t)\\f_y(t) = \sin(t)}$$ which solves $$\cases{g_k = \{1,0,1\}, f_x(0) = 1\\h_k = \{1,0,1\},f_y(0)=0}$$ and for which $$x^2+y^2 -1 = 0$$ What I am curious about is of course more general settings. Which areas of mathematics will I benefit from learning more from in order to be able to formulate and solve things like this? A suspicion that I have is that $$(\nabla \phi (f_x(t),f_y(t))) \cdot [f'_x(t),f'_y(t)] = 0$$ In other words, the tangent of the trajectory of the parametric curve needs to be orthogonal to the gradient of the level set function. Although I have no proof of this.","['soft-question', 'geometry', 'ordinary-differential-equations', 'real-analysis']"
3775646,Multiplying an equation by a derivative,"Relevant page print screen Hi All, I'm following a derivation for the Quantum Harmonic Oscillator from the textbook 'Quantum Physics' by Gasiorowicz and have attached a print screen of the page in question for clarity. There is a step in the derivation that I would appreciate if someone could clarify for me.
Starting from $$\frac{ d^{ 2} u_{0}(y)}{ dy^{2}} - y^{2} u_{0}(y) = 0$$ The author then multiplies this equation by $$2\frac{du_0}{dy}$$ (no idea why he scales it by 2?) which allows the equation to be rearranged to $$\frac{d}{dy}\left(\frac{du_0}{dy}\right)^2-y^2\frac{d}{dy}(u_0)^2 = 0$$ I understand how he arrives at the first term but not the second. I would have thought the correct way to multiply $y^2u_0(y)$ by $\frac{du_0}{dy}$ would be as follows: $$\frac{du_0}{dy}y^2u_0(y)=\frac{d}{dy}(u_0^2y^2)=u_0^2\frac{d}{dy}(y^2)+(y^2)\frac{d}{dy}(u_0^2)$$ How does the author obtain the $$y^2\frac{d}{dy}(u_0)^2$$ term in the second equation? How can he choose not to operate on $y^2$ with the differential operator when performing the multiplication? What are the rules for multiplying equations by derivatives? Thanks","['quantum-mechanics', 'derivatives']"
3775647,Interpreting $a^2b \div \frac13a^2b^3$.,"The maths book I'm using shows: $$ a^2b \div \frac13a^2b^3 $$ Which would be something like: $$ a^2 \cdot b ÷ \frac13 \cdot a^2 \cdot b^3 $$ My understanding of order of operations it would equal: $$ 3a^4b^4 $$ However in the book the second term is evaluated before the division (switching it to multiplication) and equaling: $$ a^2b \cdot \frac{3}{a^2b^3} = \frac{3a^2b}{a^2b^3} = \frac{3}{b^2} $$ If a multiplication or division signs shows between terms, should that be taken as multiplying or dividing one term by the other?",['algebra-precalculus']
3775670,Prove that $\frac{1}{2} (x-1) x + y$ is a bijection. (on p.45 Munkres Topology 2nd Edition),"I am reading ""Topology 2nd Edition"" by James R. Munkres. On p.45, Munkres leaves it to the readers to show that $g$ is bijection: Show that $g(x, y) = \frac{1}{2} (x-1) x + y$ is a bijection from $\{(x, y) \in \mathbb{Z}_{+} \times \mathbb{Z}_{+} \mid y \leq x\}$ to $\mathbb{Z}_{+}$ . I proved the above fact, but I am not sure my proof is right or not. And if my proof is right, please give me a better proof. My proof: $g$ is injective: Let $(x, y), (x^{'}, y^{'}) \in \mathbb{Z}_{+} \times \mathbb{Z}_{+}$ and $y \leq x$ and $y^{'} \leq x^{'}$ . Let $(x, y) \neq (x^{'}, y^{'})$ . If $x \neq x^{'}$ , then $x < x^{'}$ or $x > x^{'}$ . Without loss of generality, we can assume $x < x^{'}$ . Then, $x+1 \leq x^{'}$ , because if $x+1 > x^{'}$ , then $0 < x^{'} - x < 1$ and $x^{'} - x \in \mathbb{Z}_{+}$ . But there is no element $x \in \mathbb{Z}_{+}$ such that $0 < x < 1$ . $$\frac{1}{2}(x-1)x+y \leq \frac{1}{2}(x-1)x+x =\frac{1}{2}x(x+1)\leq \frac{1}{2} (x^{'}-1)x^{'}<\frac{1}{2} (x^{'}-1)x^{'}+y^{'}.$$ If $x = x^{'}$ and $y \neq y^{'}$ , then $y < y^{'}$ or $y > y^{'}$ . Without loss of generality, we can assume $y < y^{'}$ . $\frac{1}{2}(x-1)x+y = \frac{1}{2}(x^{'}-1)x^{'}+y <  \frac{1}{2}(x^{'}-1)x^{'}+y^{'}$ . So, $g$ is injective. $g$ is surjective: We prove by induction. $1 = \frac{1}{2} (1 - 1) 1 + 1$ and $1 \leq 1$ . Assume that $n = \frac{1}{2} (x - 1) x + y$ and $y \leq x$ . If $y < x$ , then $n+1 = \frac{1}{2} (x - 1) x + (y+1)$ and $y+1 \leq x$ . If $y = x$ , then $n+1 = \frac{1}{2} (x - 1) x + y+1 = \frac{1}{2} (x - 1) x + x+1 = \frac{1}{2} x (x + 1) + 1$ and $1 < x+1$ . So, $g$ is surjective.","['elementary-set-theory', 'integers']"
3775716,Standard version of covariant derivative properties,"[Throughout we're considering the intrinsic version of the covariant derivative. The extrinsic version isn't of any concern.] I'm having trouble reconciling different versions of the properties to be satisfied by the covariant derivative. Essentially $\nabla$ sends $(p,q)$ -tensors to $(p,q+1)$ -tensors. I'll write down the required properties for $\nabla$ from the two sources. This lecture (relevant timestamp linked) If $X$ is a vector field, $\nabla_Xf=Xf$ , for a scalar field $f$ $\nabla_X(T+S)=\nabla_XT+\nabla_XS$ $\nabla_X(T(\omega,Y))=(\nabla_XT)(\omega,Y)+T(\nabla_X\omega,Y)+T(\omega,\nabla_XY)$ $\nabla_{fX+Z}\ T=f\nabla_XT+\nabla_ZT$ Core principles of special and general relativity (Luscombe): $\nabla_if=\partial_if$ $\nabla(aT+bS)=a\nabla T+b\nabla S$ for real $a,b$ $\nabla(S\otimes T)=(\nabla S)\otimes T+S\otimes (\nabla T)$ $\nabla$ commutes with contractions, $\nabla_i(T^j_{\ \ jk})=(\nabla T)^j_{\ \ ijk}$ At least the second property is consistent. The first property from the book is a more restrictive version of the first property from the lecture. In fact, $\nabla_i$ means $\nabla_{\partial_i}$ and $\partial_i$ isn't even a vector field! As for the last two properties from the two sources, I have no idea on how to relate them. Are these requirements incomplete for either of the sources? If not, how can these two sets of requirements be shown to be equivalent?","['tensors', 'differential-geometry']"
