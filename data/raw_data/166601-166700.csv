question_id,title,body,tags
2903846,How to calculate the limit $\lim_{x\to1}\lfloor\sin^{-1}(x)\rfloor$?,"The question is about finding
$$\lim_{x\to1} f(x)$$
where
$$f(x) = \lfloor\sin^{-1}(x)\rfloor$$ The function takes the value $1$ at $x = 1$ but while approaching $1$ from the left side, it takes the value $0$ at all points. The function is not defined at $x>1$ so the right limit does not exist. The book mentions $1$ as the answer as $f(1)=1$ but i don't understand why. What is the meaning of a limit?","['limits', 'calculus', 'trigonometry', 'ceiling-and-floor-functions']"
2903847,Is this proof for linearity of Expectation correct?,"I'm studying the first properties of expectation and on my notes I have a proof  for its linearity. However my notes arent fully clear so I tried reinterpreting them, and this is the result: Linearity of Expectation (proof): Let $X$ and $Y$ be random variables, and $\text{Image}(X) = \{s_1, s_2, \dots, s_k, \dots \}$, and  $\text{Image}(X) = \{t_1, t_2, \dots, t_k, \dots \}$ where both sets have at most countably many elements (the random variables are discrete). Let's put $P_i = \mathbb{P}( X = s_i)$ and $Q_j = \mathbb{P}(Y = t_j)$ and eventually $\pi_{ij} = \mathbb{P}[ (X = s_i)\cap(Y = t_j)]$ which is the probability that $X = s_i$ and meanwhile $Y = t_j$. We have that $\text{Image}(X+Y) = \{ s_1+t_1,  s_2+t_1, \dots, s_2+t_1, s_2 + t_2, \dots, s_k
+t_1, s_k + t_2, \dots \}$. So by definition of expectation: $E[X+Y] = \sum\limits_{i=0}^{\infty}\sum\limits_{j=0}^{\infty}(s_i+t_j)\mathbb{P}(X+Y = s_i+t_j) = \sum\limits_{i=0}^{\infty}\sum\limits_{j=0}^{\infty}(s_i+t_j)\pi_{ij}$ Using distributive property of multiplication $E[X+Y] = \sum\limits_{i=0}^{\infty}\sum\limits_{j=0}^{\infty}s_i\pi_{ij} + \sum\limits_{i=0}^{\infty}\sum\limits_{j=0}^{\infty}t_j\pi_{ij} =
\sum\limits_{i=0}^{\infty}s_i\sum\limits_{j=0}^{\infty}\pi_{ij} + \sum\limits_{j=0}^{\infty}t_j\sum\limits_{i=0}^{\infty}\pi_{ij}$ We observe now that by definiton $\pi_{ij}$ is $\mathbb{P}[(X = s_i)\cap(Y = t_j)]$ and therefore $\sum\limits_{j=0}^\infty\pi_{ij} = \sum\limits_{j=0}^\infty\mathbb{P}[(X = s_i)\cap(Y = t_j)]$ Where $Y = t_i$ is the same as $T^{-1}(t_j) = \{ \mathcal{D_j}\in\Omega \;\;|\;\; X(\mathcal{D}_j) = t_j\}$ and $\Omega$ is the sample space. Because $Y$ is a function $\mathcal{D_j}$ are disjoint and so they make a partition for $\Omega$, i.e. $\bigcup\limits_{j=0}^\infty\mathcal{D}_j = \Omega$. Using additivity for measures $\sum\limits_{j=0}^\infty\pi_{ij} = \mathbb{P}( \bigcup\limits_{j=0}^\infty[(X = s_i) \cap (Y = t_j) ] = \mathbb{P}[ (X = s_i) \cap\bigcup\limits_{j=0}^\infty Y = t_j ] = \mathbb{P}[ (X = s_i) \cap \Omega ] = \mathbb{P}(X = s_i) $ By reproducing the same procedure for $\sum\limits_{i=0}^\infty\pi_{ij}$ we find out that $\sum\limits_{i=0}^\infty\pi_{ij} = \mathbb{P}( Y = t_j )$ Substituting in $E[X+Y] = \sum\limits_{i=0}^{\infty}s_i\sum\limits_{j=0}^{\infty}\pi_{ij} + \sum\limits_{j=0}^{\infty}t_j\sum\limits_{i=0}^{\infty}\pi_{ij}$ we get: $E[X+Y] = \sum\limits_{i=0}^{\infty}s_i\mathbb{P}(X = s_i) + \sum\limits_{j=0}^{\infty}t_j\mathbb{P}(Y = t_j) = E[X] + E[Y]$. Is it correct? My biggest doubt is where we state that $\mathbb{P}(X+Y = s_i+t_j) = \pi_{ij}$ cause I expect that there could be more than one way to get $X+Y = s_i+t_j$,  and not only if $X = s_i$ and $Y = t_j$.","['expected-value', 'probability-theory']"
2903870,Derivative as a rate measurer,"I studied about the application of derivatives as they help in measuring rate of change .
For example :- Let $A$ be area of a circle of radius $r$ $$A = \pi \cdot r^2$$ then $$\frac {dA}{dr}= 2 \pi \cdot r $$ Suppose we have to find rate of change of area w.r.t to radius 
at $r = 5 \text{ cm}$ . Then $$\left(\frac {dA}{dr}\right)_{r=5}= 10 \pi\text{ cm}^2/\text{cm} $$ My question is: Does our final answer mean that when the radius of the circle changes from $5 \text{cm}$ to $6 \text{cm}$, the change in area is equal to $ 10\pi\text{cm}^2 $,
i.e., the area of circle at $6 \text{cm} = \text{area of circle at }5 \text{cm} + 10\pi\ \text{cm}^2 $?","['derivatives', 'geometry', 'applications']"
2903911,How does the concept of a derivative solve the problem of instantaneous velocity?,"$$ \color{darkcyan}{\frac{dy}{dx}}
= \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}$$
$$ \color{darkcyan}{m}
= \lim_{x \to a} \frac{f(x) - f(a)}{x-a} $$ Text source: https://i.sstatic.net/Kn3Bm.png I think I have a fairly solid understanding of the derivative, but I don't get how it helps us find instantaneous velocity at a point. It only gives us the velocity that we can get infinitely close to, but that's not the velocity at the point. The velocity at the point is undefined as x-x in the denominator = 0. I get the following about limits and derivatives: That the limit is an actual value, not an approximation. The limit is the actual value that we are getting infinitely closer to. That the derivative is the limit of the slope of x and a, as a is moved infinitely closer to a. It is the slope that is being approached, as a gets infinitely close to x. But while this lets us know what the velocity is between two points as they get infinitely close to each other, that still doesn't give the actual instantaneous velocity at that point, because to find the actual velocity at that single instant, you have to do f(x)-f(x)/x-x= 0/0 = undefined. So how does the concept of the derivative give us instantaneous velocity? How can this be explained without epsilon delta proofs, at the level of someone learning Khan Academy calculus?","['limits', 'calculus', 'derivatives']"
2903956,Functional equation related to $\sin$: $f(x+y)=f(x)f'(y)+f'(x)f(y)$,"Find all differentiable $f:\mathbb R \to \mathbb R$ such that $$\forall (x,y)\in \mathbb R^2, f(x+y)=f(x)f'(y)+f'(x)f(y)$$ It's easy to check that the only constant solution is $0$ and the only polynomial solution is $x\mapsto x$. Besides, it's also easy to check that $\sin$ is a solution, as well as $\sinh$. More generally, $x\mapsto \frac {\sin(ax)}{a}$ and $x\mapsto \frac {\sinh(ax)}{a}$ are solutions. Setting $x=y=0$ yields $f(0)(1-2f'(0))=0$. By letting $y=0$, one gets the ODE $ f(0)f'(x)+(f'(0)-1)f(x)=0$. If $f(0)\neq 0$, then $f'(0)=\frac 12$ and this is easily solved as $x\mapsto \frac {\exp(ax)}{2a}$ If $f(0)=0$, either $f'(0)\neq 1$ and then $f=0$ , or $f'(0)=1$ and the ODE is now useless. How should I continue ? Are there other solutions ?","['functional-equations', 'ordinary-differential-equations', 'real-analysis']"
2903959,Smart Integration Tricks [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 years ago . Improve this question I am in the last year of my school 
and studied integration this year I have done several Integration techniques like Integration By substitution By partial fractions By parts Trigo. substitutions Tangent half angle substitution and many other basic methods of integration. So I wanted to ask about some integration tricks that might prove quite helpful. Not something advanced which is taught at higher level of studies But some smart integration tricks at school level only.","['integration', 'indefinite-integrals', 'definite-integrals']"
2903984,Differential equation for logistic population growth with harvesting,"When both parameters are set to $1$, the differential equation for logistic population growth is $$x'=x(1-x)$$ Suppose the population is also harvested at the constant rate $h$. The differential equation then apparently becomes $$x'=x(1-x)-h$$ Here's what I don't understand: If the population is harvested at the constant rate $h$, it means that $x$ is reduced by $h$ periodically. Why would this also mean that $x'$ is reduced by $h$?",['ordinary-differential-equations']
2903991,History and connecting math to real world,"I am software developer with big interest in Mathematics and especially discrete mathematics (which is foundational for computer science). I am currently reading Discrete Mathematics and Its Applications but I feel like I am still lacking connection or history/background between theories and how to apply them. For example, I read about Conjunctive Normal Forms and then I start wondering who created them and what do they solve? what problem they were facing to introduce them? To put it in different terms, most math books seem to miss the narrative or the story or conceptual information on why a certain topic was introduced. They feel more like reference books where they document corollaries, theories or definitions one after the other. Some times the books answer these questions but most of the times they don't
Is there any references for resources that might help in this area other than Googling for it all the time? Am I approaching learning these concepts the wrong way? What do you recommend based on your experience? How do you get real intuition for math concepts?","['applications', 'self-learning', 'discrete-mathematics', 'computer-science']"
2903999,Calculation of flux through sphere when the vector field is not defined at the origin,"I am trying to calculate the flux through the unit sphere centered at the origin given a vector field $F:\mathbb{R}^3 \setminus \{(0,0,0)\} \rightarrow \mathbb{R}^3$ with $\operatorname{div} F=1/(x^2+y^2+z^2)$. I can't apply the divergence theorem directly because of the discontinuity so what I have done instead is to consider an inner sphere of infinitesimal radius $\epsilon>0$ and write $\iiint_V \operatorname{div} F dV$= (flux through unit sphere)-(flux through inner sphere). However, I really don't know how to go on from here.
Can I prove that the flux through the sphere of radius $\epsilon$ is $0$ or is this not even true? I would appreciate some help.","['integration', 'divergence-operator', 'multivariable-calculus', 'vector-analysis']"
2904053,There is no holomorphic function $f$ on the open unit disk such that $f(1/n)=2^{-n}$,"Prove that there is no holomorphic function $f$ on the open unit disk such that $f(1/n)=2^{-n}$ for $n=2,3,..$ I know a similar question was asked on this website before but this is different. I define $g(z)=2^{-\frac{1}{z}}$, Can I use this function and identity theorem to show that $f=g$ on the disk. But $g$ is not analytic at $z=0$, so we get a contradiction? Is there a loophole in my argument. Edited the definition of function $g$",['complex-analysis']
2904089,Does $\frac{n}{\sum\limits_{k=1}^{n}\Big(\frac{k}{k+1}\Big)^k}$ converge?,"Does the sequence $$\displaystyle \frac{n}{\sum\limits_{k=1}^{n}\Big(\frac{k}{k+1}\Big)^k}$$
  converge? Attempt. Since $\Big(\frac{k}{k+1}\Big)^k \rightarrow 1/e\neq 0$ and the terms are positive, the series
$\sum\limits_{k=1}^{\infty}\Big(\frac{k}{k+1}\Big)^k$ diverges to $+\infty$. I find hard to determine if $n$ or the sum goes faster to $+\infty.$ Thanks in advance.","['summation', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
2904090,Identifying and sketching $\operatorname{Re}(z^3)=1$,"Identify and sketch the locus of $\operatorname{Re}(z^3)=1$. I tried solving the exercise like this 
\begin{align*}
  z &= x+iy \\
  z^3 &= (x+iy)^3 = x^3-3xy^2+i3x^2 y-iy^3 \\
  \operatorname{Re}(z^3) &= x^3-3xy^2=1
\end{align*}
But I can't figure out what this equation represents or how to sketch it.",['complex-analysis']
2904095,Nearest commuting matrix,"The space of matrices that commute with a given matrix $A\in \mathbb{C}^{n\times n}$
is a subspace of the vector space of all matrices $\mathbb{C}^{n\times n}$. There must exist a projection operator upon this subspace, some $P_A$ such that
$$\forall M \in \mathbb{C}^{n\times n}: [P_A M,A]=0$$ Question: Is there some useful expression for $P_A$ in terms of $A$? Context I'm looking for a way to interpolate matrices without breaking commutation. For example, I may want to construct
$$f:[0,1]\rightarrow \mathbb{C}^{n\times n}$$
$$g:[0,1]\rightarrow \mathbb{C}^{n\times n}$$
knowing $f(0),f(1),g(0),g(1)$ and $[f(0),g(0)]=[f(1),g(1)]=0$, in such at way that $[f(x),g(x)]=0$ remains true for all $x$.",['matrices']
2904108,Elementary proof that all fields of four elements are isomorphic to each other,"A question in Rotman's Advanded Modern Algebra asks to prove the question in the title. I'm convinced of my proof, but a subquestion asked to prove that $1+1$ is zero, and for this I proceeded on a case by case basis; that is, assuming for the sake of contradiction that $1+1 \ne 0$, given that we now know that the field $F = \{0,1,1+1,a\}$, I proved that this structure cannot be a field. However, I think this solution is quite ugly and I was wondering if is a nicer but still elementary solution (avoiding the fact that char($F$) = $2$). PS I'm not sure if this is a suitable question. If it's not, feel free to remove it.","['finite-fields', 'abstract-algebra', 'proof-verification']"
2904121,Proving that the collection of all maps between two sets is a set [duplicate],"This question already has an answer here : Is the collection of all functions between two sets a set? (1 answer) Closed 5 years ago . I've been just introduced to Zermelo-Fraenkel set theory, and I don't know the proper way to approach this exercises yet. Here I'm being asked to prove that $B^{A}$ (i.e. the collection of all maps between the sets $A$ and $B$) is a set itself. I know the axioms but it's hard for me now to figure out which axiom do I have to start with to attack this exercise. I'd appreciate any hint. Thanks for your time.",['elementary-set-theory']
2904165,Sum of Poisson and geometric distribution,"Suppose that $X$ has Poisson distribution with parameter $λ$ and that $Y$ has
  geometric distribution with parameter $p$ and is independent of $X$. Find a simple formula in terms of $λ$ and $p$ for $P(X + Y = 2)$ This is what I have so far: Based on $$P(X + Y = n) = \sum_{k=0}^nP(X=k)P(Y=n-k) $$ so $$P(X + Y = 2)=\sum_{k=0}^2{{λ^ke^{-λ}}\over k!}p(1-p)^{2-k-1}  $$ $${e^{-λ}}p(1-p)\sum_{k=0}^2{{λ^k}\over k!}(1-p)^{-k}  $$ $${e^{-λ}}p(1-p)\sum_{k=0}^2{\left({{λ}\over (1-p)}\right)^k {1\over k!}} $$ At this point I do not know how to proceed, or were my previous steps incorrect? [The correct solution is ${e^{-λ}}p(1-p)+{e^{-λ}}λp$]","['statistics', 'probability-distributions', 'probability']"
2904182,Sum the series $\frac{3}{1⋅2⋅4}+\frac{4}{2⋅3⋅5}+ \frac{5}{3⋅4⋅6}+...\text{(upto n terms)}$,"$\frac{3}{1⋅2⋅4}+\frac{4}{2⋅3⋅5}+ \frac{5}{3⋅4⋅6}+...\text{(upto n terms)}$ The general term seems to be
$$T_r= \frac{r+2}{r(r+1)(r+3)}.$$ I see no way to telescope this because the factors of the denominator of the general term are not in arithmetic progression. Do I have to use something else? Or am I missing some easy manipulation?","['algebra-precalculus', 'summation', 'sequences-and-series']"
2904194,Variations of $\lim_{n\rightarrow \infty} \left(1 + \frac{1}{n}\right)^n$,"I understand that 
$$
\lim_{n\rightarrow \infty} \left(1 + \frac{1}{n}\right)^n = e
$$ However, how would $ \lim_{n\rightarrow \infty} \left(1 + \frac{1}{3n}\right)^{5n} $ be simplified. The $5$ can be moved outside the limit: $$
{\lim_{n\rightarrow \infty} \left[\left(1 + \frac{1}{3n}\right)^{n}\right]^5}
$$ But how can you simplify it to the form $e^{\frac{m}{n}}$? The answer is $ e^{\frac{5}{3}} $, but could someone help me understand the methodology?","['limits', 'calculus']"
2904213,How can I solve for an unknown vector given an otherwise known cross product?,"Given known vectors $\vec a$ and $\vec b$, is it possible to solve for $\vec u$, given the following equation? $\vec a \times \vec u = \vec b$ So far I have found that the following must be true $\vec u = \vec c + \lambda \hat a$ where $\vec c = \frac{b}{a} (\hat b \times \hat a) = \frac{\vec b \times \vec a}{a^2}$ $\lambda \in \Bbb R $ but I'm unsure whether it is possible to further determine $\vec u$, since any vector $\vec u$ such that $(\vec u \cdot \hat c) \cdot \hat c = \vec c$ should do the trick, which means that $\lambda$ can take any value. However, when developing the cross product $\vec a \times \vec u = \vec b$ by hand, one can reach a matricial equation of the form $ A \cdot (\vec x)^t = (\vec b)^t $ where $A$ is a matrix with coefficients from $ \vec a $ and which is solvable with at least one method, one example thereof being $ (\vec x)^t = A^{-1} \cdot (\vec b)^t $ Am I doing something wrong? Or is this equation truly not solvable using vectorial math?","['matrices', 'cross-product', 'linear-algebra']"
2904218,Proving the Complementation Law for sets,"I am new to Discrete Mathematics, and have been asked to prove the Complementation Law for sets, that is: $\overline {(\overline A)} \equiv A$. Our teacher advised us to turn the sets into propositions, so would it be as simple as this: $\overline {(\overline A)}$ $\equiv \neg (\neg p)$ $\equiv p$ $\equiv A$ We have not really seen what a real proof looks like. Thank you!","['elementary-set-theory', 'proof-writing', 'discrete-mathematics']"
2904274,The closure of the sum is contained in the sum of the closures?,"Let $X=(X,|\cdot|$) be a Banach space and let $A,B \subset X$. Then
$$ \overline{A+B} \subset \overline{A}+\overline{B}\:?$$
Where, $\overline{A+B}$, $\overline{A}$ and $\overline{B}$ denotes the closure of the sets $A+B$, $A$ and $B$, respectively. I do not know if this is true. But I tried to prove it as follows: Let $x \in \overline{A+B}$ then exist $(x_n)_{n \in\mathbb{N}} \subset A+B$ such that $x_n \longrightarrow x$. In addition also, exists $(a_n)_{n \in\mathbb{N}} \subset A$ and $(b_n)_{n \in\mathbb{N}} \subset B$. So,
$$a_n+b_n=x_n \longrightarrow x.$$
Hence, exists $y \in A$ and $z \in B$ such that 
$$ a_n  \longrightarrow y \:\: \text{and} \:\: b_n \longrightarrow z.$$
Therefore, $y+z=x$ and $y+z \in \overline{A}+\overline{B}.$ Therefore, $ \overline{A+B} \subset \overline{A}+\overline{B}$. I was in doubt if I can guarantee the existence of the elements $ y $ and $ z $, in $ A $ and $ B $, respectively. Is that correct?","['banach-spaces', 'general-topology', 'functional-analysis']"
2904297,Tangent spaces to the orbit of a Lie group,"Let $G\subset GL(n)$ be a Lie subgroup and denote 
$$
M:=G x_0 = \{ Ax_0\ :\ A\in G\}\subset \mathbb R^n,$$
where $x_0\ne 0$ is a fixed vector in $\mathbb R^n$. Then $M$ is a smooth submanifold of $\mathbb R^n$. Question . Is it true that $$\tag{1}A(T_{x_0} M)=T_{Ax_0} M,\qquad \forall A\in G\ ?$$ At first sight I would say that (1) is true. However, if the Lie algebra $\mathfrak g$ is given by
$$\mathfrak g = \text{span}\ (g_1, g_2\ldots g_m), $$ 
then 
$$T_{x_0} M = \text{span}\, (g_1 x_0, \ldots ,g_mx_0),$$
and analogously 
$$
T_{Ax_0} M=\text{span}\, (g_1 Ax_0, \ldots, g_mAx_0), $$
while 
$$
A(T_{x_0} M) = \text{span}\, (Ag_1 x_0, \ldots ,Ag_mx_0),$$
and I don't see a reason why the last two vector spaces should coincide. I don't know how to handle the commutators $[g_j, A]$, where $g_j\in\mathfrak g$ and $A\in G$.","['lie-algebras', 'lie-groups', 'differential-geometry']"
2904319,Is chapter 5 of Grothendieck (1955) related to Sheaf Cohomology?,"I'm curious if the fifth and last chapter of  Grothendieck's 1955 paper (which he states in the introduction is the origin of the paper) is describing something related to Sheaf Cohomology? Is he using the notation H1(X,G) as what we usually call H0(X,G) the global sections? In his own (translated) words from the introduction: 'In the last chapter, we define the cohomology set H1(X,G) of X with
  coefficients in the sheaf of groups F, so that the expected
  classification theorem for fibre spaces with structure sheaf G is
  valid. We then proceed to a careful study of the exact cohomology
  sequence associated with an exact sequence of sheaves e->F->G->H->e.' Any help would be gratefully appreciated! PS you can find the mentioned 1955 paper of Grothendieck's General theory of fibre spaces with structure sheaf here as generously pointed to by user Jeroen.","['algebraic-geometry', 'sheaf-cohomology']"
2904333,Prove equality involving binomial coefficients,"I was solving a probability problem and I got a different answer than the one given in the book. Seems the authors were using a different way of counting/arguing. For the two answers to be equal, the following equality should hold true. $$\sum_{k=0}^{b-1} \binom{a+k-1}{a-1} p^a (1-p)^k = \sum_{k=a}^{a+b-1} \binom {a+b-1}{k} p^{k} (1-p)^{a+b-k-1}$$ How can this be proved? And the problem itself was: in a series of Bernoulli trials with probability for success in a single trial equal to $p$, what is the probability to get $a$ successes before getting $b$ failures? I think you guys will reverse engineer how I counted and how the authors counted.","['probability-theory', 'probability']"
2904343,Estimate degrees of freedom in sample variance.,"Given a sequence of independent identically distributed random variables $X_1,\ldots,X_m \sim \chi^2_n / n$ is there literature on estimates for the degrees of freedom $n$? In an attempt to find the MLE I calculated the roots of the log likelihood function assuming continuous values of $n$. This tells us that $n$ should satisfy $$2\frac{1}{n} - \psi(n) = \log \left( \sqrt{2}\prod_{i=1}^m x_i^{\frac{1}{m}} \right) $$ where $\psi$ denotes the digamma function. This estimate is not quite as insightful as I'd hoped. An easy estimate is simply noting that $\operatorname{Var}(\chi^2_n / n) = 2/n$. However, I don't know whether this will actually be a good estimate in practice.","['statistical-inference', 'statistics', 'parameter-estimation', 'reference-request', 'probability']"
2904361,second order derivative of log det of matrix,"I need to find the second order derivative of a matrix. $f(\pmb{\Delta})=\log \det(\pmb{I}+k\pmb{V^T \Delta V})$ Where $ \pmb{\Delta}$ is a triangular matrix. I did the first order derivative but not sure if is correct.
$$\frac{\partial f(\pmb{\Delta})}{\partial  [\pmb{\Delta}]_{ii}} =  
\left[ k\pmb{V} \left( \pmb{I}+k\pmb{V^H}\pmb{\Delta}\pmb{V} \right)^{-1}\pmb{V^H} \right]_{ii}  $$ This is correct? How about the second order derivative?","['determinant', 'matrices', 'matrix-calculus', 'vector-analysis', 'derivatives']"
2904365,Estimate parameters of Wishart matrix.,"Given a sequence of real Wishart matrices $W_1 , \cdots , W_k \sim \mathcal{W}_m(n,\Sigma)$ where $\Sigma$ is a singular matrix. Are there good estimates for the degrees of freedom? The MLE for $\Sigma$ is $\frac{1}{k\cdot n-1}\sum_{i=1}^k W_i$ since this is the MLE if we had access to the samples which gave rise to the observed Wishart matrices. The only estimate I have been able to come up for the degrees of freedom is to take a random vector $Y$ and consider $\frac{Y^T W Y}{Y^T (\frac{1}{k}\sum_{i=1}^k W_i) Y}$. This should be approximately $\chi^2_n /n$ according to theorem 3.2.8 in Muirhead's Aspects of multivariate statistical theory, estimating $n$ now comes down to estimating the shape parameter of the $\chi^2_n /n$.","['statistical-inference', 'statistics', 'parameter-estimation', 'estimation']"
2904397,"How to prove $\lim_{(x,y) \rightarrow (0,0)} \frac{x^{2}y^{2}}{x^{3}+y^{3}}$ doesn't exist [duplicate]","This question already has answers here : Limit of $\lim_{(x,y)\rightarrow(0,0)}\frac{x^2y^2}{x^3+y^3}$ (3 answers) Closed 5 years ago . I have tried using polar coordinates and several polynomial functions, but they all converge to the value of $0$; checking at Wolfram Alpha, however, it confirms that the limit does not exist. How should I approach these kinds of questions and not get stuck? 
$$
\lim_{(x,y) \rightarrow (0,0)} \frac{x^{2}y^{2}}{x^{3}+y^{3}}
$$","['limits', 'calculus']"
2904401,"Prove that $f(x,y)=1- \sqrt{|xy|}$ is not differentiable at $(0,0)$","Let $f(x,y)=1- \sqrt{|xy|}$. Prove that $f$ is not differentiable at $(0,0)$. I don't really know how to tackle this problem. Normally for this kind of problem I would find the candidate to be differential, this is, $L(x,y) = \frac{\partial f}{\partial x}(0,0)x + \frac{\partial f}{\partial y}(0,0)y$ and I would try to find out if $\displaystyle\lim_{(x,y)\to(0,0)}\frac{f(x,y)-f(0,0)-L(x,y)}{ \| (x,y)\|}=0$. But I don't know how to start being $f$ differentiable.","['multivariable-calculus', 'real-analysis']"
2904427,Does the domain of integration have to be convex?,"The following theorem is straightforward to prove if we ignore the convexity of the domain : Suppose that $\text{R}$ is a convex region in the plane and that the function $g : \text{R} \to \mathbb{R}$ has continuous bounded partial derivatives. Show that the surface $S = {(x, y, g(x, y)) \ | \ (x, y) \ \text{in} \ \text{R}}\}$ has area equal to that of $\text{R}$ if and only if the function $g : \text{R} \to \mathbb{R}$ is constant. For the proof I used the fact that $\iint_D \Big(\sqrt{1+f_x^2+f_y^2} -1\Big)\,dA = 0 \iff f_x=0 =f_y \iff f = \text{const.}$ But why it is supposed that the region to be convex? I can't see why it is necessary to use convexity of the region in the proof. Is there a counterexample i.e. a non-convex region that the mentioned theorem fails to hold?","['integration', 'convex-analysis', 'examples-counterexamples', 'real-analysis']"
2904442,Gaussian Curvature of Saddle,"I recently got to grips with simple 2D curvature, and I'm aware that formulae exist for Gauss curvatures of surfaces defined in terms of some $z=f(x,y)$, but I'm finding little progress making sense of these.  Is there a nice way to get at the curvature of such defined surfaces? In particular I'd like to find the curvatures of the saddles defined by $z=xy$ and $z=x^y\;(0^0:=1)$ (intuitively, the latter ""feels"" more negative than the former, but by how much?).","['curvature', 'differential-geometry']"
2904447,An integral map from 3-torus $\mathbb{T}^3$ to 3-sphere $S^3$,"Let $\phi_1, \phi_2, \phi_3, \phi_4 \in \mathbb{R}$ be real valued functions, such that $$\phi_j(x,y,z):(x,y,z) \in \mathbb{T}^3 \to \phi_j(x,y,z) \in  \mathbb{R}.$$ Here $\mathbb{T}^3$ is a 3-torus, with $j=1,2,3,4$ . The $\phi_j(x,y,z)$ satisfies a constraint $$\sum_{j=1}^4 (\phi_j)^2=1,$$ which means that $(\phi_1, \phi_2, \phi_3, \phi_4)$ is a vector on a 3-sphere $S^3$ . Consider the integral computed from the domain $(x,y,z) \in\mathbb{T}^3$ to the target of $(\phi_1, \phi_2, \phi_3, \phi_4) \in S^3$ . We can choose the $\mathbb{T}^3$ has a unit length 1, and the $S^3$ has a unit radius 1. Question 1: Can we show that $$(2/\pi^2) \int_{T^3} (\epsilon^{abc} \phi_1 \partial_a \phi_2 \partial_b \phi_3 \partial_c \phi_4) \;dx dy dz\;\in \mathbb{Z}?$$ is integer valued? (Or up to a front factor to be fixed.)
Is this true or is it wrong? (At least for certain function $\phi_j(x,y,z)$ , I find the integral can be integer valued. ( Bonus, but you can skip this one below to claim the answer. ) Question 2:
More generally, is there some homotopy type of constraint, such that the integral map from the domain $\mathbb{T}^d$ to the sphere $S^d$ , certain integral of the similar form $$\# \int_{T^d} (\epsilon^{\mu_1 \mu_2 \mu_3 \dots \mu_d} \phi_1 \partial_1 \phi_2 \dots \partial_{\mu_{d-1}} \phi_{d} \partial_{\mu_d} \phi_{d+1}) \;d^dx \;\in \mathbb{Z}?$$ where $$\sum_{j=1}^d (\phi_j)^2=1,$$ Up to a proper normalization $\#$ ?","['integration', 'real-analysis', 'partial-differential-equations', 'differential-topology', 'differential-geometry']"
2904466,"Use of the word ""intrinsically"" in mathematics.","I am studying some algebraic geometry and surfaces of Riemann and reading some books... often appears an expression whose meaning is not clear to me. So I would like some help to understand this. The expression in question is ""intrinsically"". I have separated some of the moments in which I have found (and continue) with doubts about what does it mean meant by the use of the word ""intrinsically"". 1) Nevertheless, we have, in fact, gained much, for 
  we have defined multiplicities of $(d — 1)$-dimensional varieties intrinsically , in terms of properties of a ring of the function field 
  of $V$, and this will be found to be of immense value in later developments. 2) Recall that if $M=\mathbb{C}^g/\Lambda$ is any complex torus, then the tangent spaces $\{T'_{\lambda}(M) \}_{\lambda\in M} $ are all naturally identified with $\mathbb{C}^g$. Thus if $X\subset M$ is any analytic subvariety of dimension $k$, $X^* = X — X_{\text{sing}}$ the smooth locus of $X$, we can define the Gauss map 
  $$
\begin{array}{llll}
{\cal{G}}_ {X} :&X^*&\longrightarrow&G(k, g) \\
&p&\longmapsto&T'_{p}(X)\subset T'_{p}(M)= \mathbb{C}^g.\\
\end{array}
$$ We see immediately that $\cal G$ is intrinsically defined and that it does not vary if $X$ is translated in $M$. So that's my question. What do authors mean by intrinsically in these two passages. Thank You!","['proof-writing', 'algebraic-geometry', 'terminology']"
2904470,$f(x)=x^3$ is convex nor concave without to derivate,"Drawing the graph of a function is not intuitive. For example, the function $f(x)=x^2$ is special, we can say that given two points of the function the straight line that join those points is above the function. So we write 
$$
x_2<(x_1+x_0)(x-x_0)+x^2 
$$
How did I obtain the part of the right? 
$$
(Y-x_0^2)/   (x-x_0)  =     (x_1^2 -  x_0^2)   /      (x_1 -   x_0)
$$
Now I just isolate Y in the equation above. My question is, can I do the same with $f(x)=x^3$? 
And how do I prove it? Can I say that $x^3$ is below the straight line that join two points of the function $x^3$? If and only if $x$ is positive.","['real-numbers', 'calculus', 'discrete-mathematics', 'real-analysis']"
2904491,Set of symmetric positive semidefinite matrices is closed,"I am self-studying Boyd & Vandenberghe's Convex Optimization . Example 2.15 (page 43) states that the symmetric positive semi-definite cone $S^n_+$ is a proper cone. This necessitates, amongst other things, that it is closed. I am not sure how to show that $S^n_+$ is closed, particularly because this set consists of matrices, which I am less comfortable working with. The most relevant question I have found that may have some relation to this one is here ; I am not sure how to act on the answer of this question for I am not sure of whether the functions $f_1$ and $f_2$ as defined in the answer are relevant to my task.","['positive-semidefinite', 'matrices', 'linear-algebra', 'symmetric-matrices', 'general-topology']"
2904496,Reference Request: Concentration inequalities/concentration of measure phenomenon,"Is there a good source for concentration inequalities? I've seen the standard ones (Bernstein, Hoeffding, Chernoff, etc.), but I'm hoping to get two things: A ton of exercises. (Still haven't really gotten a great grasp on these inequalities, intuitively, so that's why the exercises help. They build intuition.) Learn some more exotic/specific concentration inequalities (for example, for Gaussian chaos, matrix random variables, etc.) I know of the book, ""Concentration Inequalities: A Nonasymptotic Theory of Independence."" Is that still the best reference out there?","['concentration-of-measure', 'probability-theory', 'reference-request']"
2904536,"Does there exist infinite words using the alphabet $\{A,B,C,D\}$ that avoids patterns $XX,\ XAX,\ XBX,\ XCX,\ XDX$?","Another form of this question is: Does there exist a gap-1 square-free infinite word using the alphabet {A,B,C,D}? Normally square-free in this context means that there are no sub-words twice in a row that follows the pattern XX. For example the words AA, CABABC, ABCABC all aren't square-free. Words that are gap-1 square-free not only avoids sub-words twice in a row but also avoids identical sub-words that are one letter apart for example ABA DCABCAD ABCDABC are all square-free but not gap-1 square-free. All of the patterns that gap-1 square free avoids are XX, XAX, XBX, XCX, XDX. It can be shown by exhaustion that there are finitely many words using the alphabet {A,B,C} That are gap-1 square-free. This is the entire list: A, AB, ABC, ABCA, AC, ACB, ACBA, B, BA, BAC, BACB, BC, BCA, BCAB, C, CA, CAB, CABC, CB, CBA, CBAC A reduction of the problem can be done by first establishing a correspondence between words using the alphabet {A,B,C,D} and four-vertex directed graphs. Where the four vertices are labeled A,B,C,D with $N-1$ edges. Where $N$ is the number of letters in the word that the graph corresponds to. The tail of first edge in the graph starts with the vertex whose label is the same as the first letter in the Word. The head of the first edge ends with the vertex whose label is the same as the second letter in the word. In general the tail of 
the $k^{th}$ edge corresponds to $k^{th}$ letter of the word and the head of the $k^{th}$ edge corresponds to the $k+1^{th}$ letter. Where $1\leq k\leq N-1$ . For example the word ABC would correspond to the four-vertex two-edge graph. Where the first edge has tail on vertex A and head on vertex B. The second edge has tail on vertex B and head on vertex C. All of these corresponding graphs cannot have loops because all words are gap-1 square-free. The tail of the second edge must start at the head of the first edge. The head of the second edge cannot end at the tail of the first edge because the words are gap-1 square free. With these conditions on the graphs there is only one non-isomorphic graph with two edges. This implies that the set of all words that are length three or greater that do not start with ABC are isomorphic to the set of words that start with ABC. So if there are no infinite words it is sufficient to show that no words that start with ABC are infinite. here are a couple of links that might be useful: Does there exist an infinite number string without any 'refrain'? https://en.wikipedia.org/wiki/Square-free_word EDIT: I wrote a computer program to find long words that are gap-1 square-free using the A,B,C,D alphabet. There exists words of this type that are more than $10^6$ letters long. EDIT $2$ : There exists an infinite gap-1 square-free word using the alphabet {A,B,C,D,E,F}. Proof: Let $I$ be an infinite square-free word using the alphabet {A,B,C}. (There are examples of these in the links I have already provided.) Let $I^*$ be defined follows: $I^*$ is obtained by replacing every instance of A in $I$ with AD every instance of B with BE and every instance of C with CF. I will call the AD, BE, CF pairs ""blocks"". Let $S_1$ be the of letters {A,B,C} and let $S_2$ be the set of letters {D,E,F}. Let $x_1$ and $x_2$ be sub-words of $I^*$ that are of the same length and are adjacent or are one letter apart. When comparing the letters of the two words to see if they are the same there are two cases. In the first case the blocks of $x_1$ and $x_2$ could be out of alignment. For example if $x_1$ is ADB and $x_2$ is ECF. The first two letters of $x_1$ is in one block and the last letter is in a second block where as the first letter of $x_2$ is in one block and the last two letters are in a second block. If the blocks are out of alignment none of the letters between $x_1$ and $x_2$ will match each other because the letters will be in opposing sets. (If the first letter of $x_1$ is in $S_1$ then the first letter of $x_2$ will be in $S_2$ , and so on.) In the second case (if the blocks are aligned), The first letter of a block in $x_1$ will match the first letter of the corresponding block in $x_2$ if and only if the second letter of the same block in $x_1$ will match the second letter of the corresponding block in $x_2$ . So $x_1$ will match $x_2$ if and only if the blocks of $x_1$ match the blocks of $x_2$ . In case 2 the blocks of $x_1$ must be adjacent to the blocks of $x_2$ . So it is enough to show that the blocks of $I^*$ are square-free. There is a direct correspondence between the blocks of $I^*$ and the letters of $I$ . $I$ is by definition square-free. QED","['combinatorics-on-words', 'sequences-and-series']"
2904540,Does $x'' = x^3$ have an analytical solution?,"I encountered this differential equation $x'' = x^3$ during one of my work and couldn't find an analytical solution to the above. I've used numerical methods to solve the equation in the end. I was just wondering if there is any way to find an analytical solution or show that an elementary solution doesn't exist. WolframAlpha expresses the answer with Jacobi theta functions, and I was wondering if that's the only way to express the answer.",['ordinary-differential-equations']
2904544,Why do we factor polynomials the way we do?,"Today, I was working on some limit practice problems and came across two that I had to factor. The first limit had this polynomial in the denominator: $$x^2+2x-15$$ which I factored down to: $$(x-3)(x+5)$$ The second limit had this polynomial in the numerator: $$2z^2-17z+8$$ which I factored down to: $$(2z-1)(z-8)$$ As I was looking over these problems, I realized I don't know why polynomials factor down like this. I was just taught what to do when I come across each type. When I factor them down the answers ""make sense"", but I just can't see a reason why they do as I look at it from my current perspective. Could someone shed some light on this? Are there proofs for things like this?","['algebra-precalculus', 'factoring', 'polynomials']"
2904573,Find all connected covers of $\mathbb{RP}^2 \vee \mathbb{RP}^2$,I was trying to find all connected covers of $\mathbb{RP}^2 \vee \mathbb{RP}^2$ . In that regard I got the universal cover which is a disjoint wedge of countably infinite spheres. Also I got covers corresponding to each cyclic subgroup of $\mathbb{Z}_2*\mathbb{Z}_2$ . How do I find covers corresponding to other subgroups if there exists any?,"['group-theory', 'algebraic-topology', 'covering-spaces']"
2904607,If $f\in L^1(\mathbb R)$ and $g\in C_c^\infty(\mathbb R)$ such that $f\star g=0$ a.e. then $f=0$,"Let $f\in L^1(\mathbb R)$ and $g\in C_c^\infty(\mathbb R)$ such that $f\star g(x)=0$ a.e. Then show that $f=0$ a.e. Note, $g$ is a fixed function. So from the given condition I can see $\hat{f}(y)\hat{g}(y)=0$ for every $y\in\mathbb R$ so $\hat{f}(y)=0$ or $\hat{g}(y)=0$ for every $y$. I now thought of applying the uncertainty principle. The set $\{y:g(y)\neq 0\}$ is bounded, hence $\{y:\hat{g}(y)\neq 0\}$ must have infinite Lebesgue measure, in particular must be unbounded. So $\hat{f}(y)\neq0$ over a set with infinite measure. So what?","['fourier-analysis', 'functional-analysis', 'real-analysis']"
2904620,Where can I find out more about the nature of holes in plane regions?,"Over the last few months I have been studying tilings of regions by polyominoes (mostly dominoes). I have been putting my findings together, mostly in the form of proving various things about polyomino regions and their tilings. But a few times now I had to gloss over some details, all which seem to me related. These seem obvious, but I would like to have a list of properties / facts that I can rely on to reason more confidently. Here are some examples of the types of things I am interested in: Distinguishing the outside border from the borders of holes. If we have polyomino region that can be extended arbitrarily at specified edges on the outside border, without any new cells neighboring cells of the original region except at the specified cells, then the extensions can connect with each other only on specific ways without overlap (for example, if we have four extendable edges 0, 1, 2, 3 in clockwise order, and the extensions at 0 and 2 connect somewhere outside the original region, then the extensions at 1, and 3 cannot also connect outside the figure) if extensions connect up in a certain way, it forces some holes to form. Holes are simply-connected. One hole cannot surround another in a connected region (or overlap). Part of it has to do with choosing a suitable definition of polyomino, and then a suitable definition of hole. (In many papers that deal with polyominoes with holes, a more complicated definition is used for polyomino than is necessary for most other purposes.) Some of the types of things where these issues arise can be seen from some of my other questions here, for example: Is every “even” polyomino with one hole tileable by dominoes? , Elementary proof of transformations of domino tilings. . Another example is the following formula that applies to polyomino regions: $$ h = \frac{v - p}{4} +1,$$ where $h$ is the number of holes, $v$ is the number of valleys (sides of the polyomino between concave corners), $p$ is the number of peaks (sides of the polyomino between convex corners). So my question is, where can I find out more about how holes in plane regions (maybe specifically polyomino regions) really work ? Ideally, I would not like to delve to deep into too abstract topics, so the more elementary the better. (I do not really know which are suitable tags to apply. The ones I added seem relevant, but please suggest alternatives if they apply.)","['graph-theory', 'polyomino', 'general-topology']"
2904659,One-sided Taylor's expansion,"Suppose a function $f(t)$ is defined only on $[t_{0},\infty)$. Suppose all ""right'' derivatives $f^{(n)}(t)$ exist, that is, 
$$f^{(1)}(t_{0})=\lim_{\delta\rightarrow 0+}\frac{f(t_{0})-f(t_{0}+\delta)}{\delta}<\infty,$$ and  in general, $$f^{(n)}(t_{0})=\lim_{\delta\rightarrow 0+}\frac{f^{(n-1)}(t_{0})-f^{(n-1)}(t_{0}+\delta)}{\delta}<\infty,$$ for every $n\geq 1$. Is Taylor's expansion,
$$
    f(t_{0})+\sum_{n=1}^{\infty} \frac{f^{(n)}(t_{0})}{n!}(t-t_{0})^n,
$$ defined on $[t_{0},t_{0}+\varepsilon)$ for some $\varepsilon$? I am not asking whether the expansion converges to $f(t)$ (that is a different question) in a neighborhood of $t_{0}$. I am just asking if $f(t)$ can be expanded only to the right hand side without the left hand side having been defined. Intuitively one would think so, but I cannot find any literature items specifically on this point. All Taylor's expansions seem to assume all derivatives exist in an open neighborhood of $t_{0}$. Please enlighten me.",['real-analysis']
2904701,how to draw the space of such linear combinations?,"We have the linear combination $$ {2 \choose 1 } x_1 + {1 \choose 2} x_2 + {1 \choose -2} x_3 + {1 \choose 1} x_4 + {-1 \choose 0 } x_5 + {0 \choose -1 }x_6 $$ As $x_i \geq 0 $ is given, according to the definition, the linear combination above generates a cone. But, how can we draw it? Isnt the linear combination above just the entire $\mathbb{R}^2$ plane? Also, I need to decide whether the vector ${6 \choose 4}$ lies in the cone, but since the linear combination above represesnt the entire plane, then ${6 \choose 4}$ must lie in. Am I missing something here?","['convex-geometry', 'visualization', 'vector-spaces', 'analysis', 'intuition']"
2904722,Probability of getting at least one head given there at least two heads. (solution verification),"A coin with probability, $p$, of coming up heads is flipped three times (the flips are independent).
What is the probability of getting at least one head given there are at least two heads? My attempt at a solution:
First I realize that there are eight possible outcomes: TTT, HHH, THH, HHT, HTH, TTH, THT, HTT. Now consider:
\begin{align*}
&P(1~\text{head} \mid \text{at least $2$ heads}) + P(2~\text{heads} \mid \text{at least $2$ heads})+ P(3~\text{heads} \mid \text{at least $2$ heads})\\
& \quad = \frac{P(1~\text{head}) \cap P(\text{at least $2$ heads})}{P(\text{at least $2$ heads})} + \frac{P(2~\text{heads}) \cap P(\text{at least $2$ heads})}{P(\text{at least $2$ heads})}\\
& \qquad + \frac{P(3~\text{heads}) \cap P(\text{at least $2$ heads})}{P(\text{at least $2$ heads})}\\ 
& \quad = 0 + \frac{P(2~\text{heads})}{P(\text{at least $2$ heads})} + \frac{P(3~\text{heads})}{P(\text{at least $2$ heads})}\\
& \quad = \frac{3p^2(1-p)}{3p^2(1-p)+ p^3} + \frac{p^3}{3p^2(1-p)+ p^3} 
\end{align*} Is this solution correct?  I'm not sure whether the probability of getting two heads is $3p^2(1-p)$ or if it is $(p^2)(1-p)$.  I figured that since there was three possible ways of getting two heads I would multiply by $3$ (add three times).  But I'm not sure if this correct.","['algebra-precalculus', 'statistics', 'probability-theory', 'probability']"
2904743,Can a basis of a tangent space be mapped to a basis of another tangent space if the map between the spaces is a homeomorphism and vice versa?,"If I have an open subset $U$ of a n-dimensional $C^k-$manifold $M$ and a homeomorphism $f:U \to \Bbb R^n$ (Basically I am talking about a chart $(U,f)$) can I say that under this map a basis of $T_pU=T_pM$ can be sent to a basis of $T_{f(p)}\Bbb R^n$ and even vice versa? that is if we have a basis of $T_{f(p)}\Bbb R^n$ can it be pulled back to a basis of $T_pM$? Here $p$ is a point inside $U$ and I am actually talking about the differential $df_p:T_pM \to T_{f(p)}\Bbb R^n.$ So I am seeking answer in the context of differentials only. By a n -dimensional $C^k-$manifold , I mean a topological manifold with a maximal $C^k-$atlas i.e we have a collection of charts $\{(U_i,f_i)\}$ forming a $C^k-$atlas where each $f_i : U_i \to \Bbb R^n$ is a homeomorphism and the transition maps are $C^k-$maps i.e. they are k-times continuously differentiable. By compatibility condition on $C^k-$atlases (Two $C^k-$atlases are compatible if their union is a $C^k-$atlas), we have a maximal $C^k-$atlas. Note that the above definition doesn't have $f_i $ to be a diffeomorphism hence I don't have an isomorphism $df_p :T_pM \to T_{f_i(p)}\Bbb R^n.$","['differential', 'differential-topology', 'tangent-spaces', 'differential-geometry']"
2904760,age-based word problem,"Peter's age is three years more than three times his son's age. After three years, Peter's age will be ten years more than twice his son's age. What is Peter's present age? I have tried to put this into algebra, but not sure if correct? $x =$ Peter's son's age $p =$ Peter's age \begin{align*}
3x + 3 & = p\\ 
10 + 2x & = 3 
\end{align*}","['word-problem', 'algebra-precalculus']"
2904814,Why is this not a choice function from subsets of $\Bbb R$?,"A definition of the Axiom of Choice mentions that: [...] no choice function is known for the collection of all
  non-empty subsets of the real numbers 
  --Wikipedia However, for all non-empty subsets $X$ of the real numbers I can construct a function $f$ which chooses the element closest to $0$, and if there are two such elements, chooses the positive one. Below is my attempt at stating this rigorously: $ \forall X \subset \mathbb{R} | X \neq \emptyset: f(X)=\begin{cases}
  x:\min (|X|),& \text{if } (\max(x):x \in X, x<0) \neq (\min(x):x \in X, x>0)\\
    \min(|X|),& \text{otherwise}
\end{cases}$ This certainly holds for subsets of reals which are positive only $\longrightarrow$ select the smallest are negative only $\longrightarrow$ select the largest contain $0$ $\longrightarrow$ select $0$ are positive and negative $\longrightarrow$ select the element nearest to $0$ contain a positive and a negative element both closest to $0$ $\longrightarrow$ select the positive one Is this a choice function for the collection of all non-empty subsets of the real numbers? And if it's not a breakthrough, what am I missing?","['elementary-set-theory', 'real-numbers', 'axiom-of-choice']"
2904829,"Probability - four random integers between 0-9, that not more than two are the same","Four integers are chosen at random between 0 and 9, inclusive. Find the probability that (a) not more than 2 are the same. What I tried: all unique numbers: 63/125, 
                  two same numbers: 72/1000 And then add them both. But the answer in the book is 963/1000. I'm not getting such a high probability. Where have I made the mistake?",['probability']
2904851,What does a segment in the plane as a metric space defined by a $p$-norm look like?,"In the metric space $\mathbb R^2$ with the metric $d$ defined by $d(x,y)= (|x_1-y_1|^p+|x_2-y_2|^p)^{1/p}$, where $p\gt1$ is a real number, like what does the set of all $m\in \mathbb R^2$ with $d(a,m)+d(m,b)=d(a,b)$ look, where $a$ and $b$ are two arbitrary points of $\mathbb R^2$?
I think for all $p$’s it is a straight line segment as in the obvious case $p=2$ but I do not know how to deal with the other values of $p$.","['metric-spaces', 'real-analysis']"
2904854,What is the meaning of subsampling level in machine learning/statistics?,"I am reading a paper but nowhere it is defined but the term is being used a lot. The paper is - Less is More : Nystrom Computational Regularization -Rudi,Camariano, 
               Rosasco
 Link : https://papers.nips.cc/paper/5936-less-is-more-nystrom-computational-regularization.pdf","['statistics', 'descriptive-statistics', 'learning', 'machine-learning', 'sampling']"
2904863,When The curvature is maximum of $x^\frac{1}{2}+y^\frac{1}{2}=a^\frac{1}{2}$,"QUESTION Find Where The Curvature has an extremum                                                                                                                                                                                                                                                                                                                                                                             ?
$$x^\frac{1}{2}+y^\frac{1}{2}=a^\frac{1}{2}$$
MY APPROACH $$x^\frac{1}{2}+y^\frac{1}{2}=a^\frac{1}{2}.   .   .   .   .   (1)$$ $$\Rightarrow y^\frac{1}{2}=a^\frac{1}{2}-x^\frac{1}{2}$$
now differntiating both sides we will get:
$$\frac{1}{2}{y^\frac{-1}{2}}\frac{dy}{dx}=(-1)\frac{1}{2}x^\frac{-1}{2}$$
$$\Rightarrow \frac{dy}{dx}=-(\frac{y}{x})^\frac{1}{2}.   .   .   .   .   .(2)$$
Now differentiating again with respect to x again:
$$\frac{d^2y}{dx^2}=-\bigg(\frac{1}{2}(\frac{y}{x})^\frac{-1}{2}.\frac{d}{dx}(\frac{y}{x})\bigg)$$
$$=-\frac{1}{2}\Bigg(\frac{x^\frac{1}{2}}{y^\frac{1}{2}}\bigg(\frac{d}{dx}(\frac{1}{x})y-\frac{dy}{dx}(\frac{1}{x})\bigg)\Bigg)$$
$$=-\frac{1}{2}\Bigg(\frac{x^\frac{1}{2}}{y^\frac{1}{2}}\bigg(\frac{-y}{x^2}-\frac{dy}{dx}(\frac{1}{x})\bigg)\Bigg)$$
now put the value of $\frac{dy}{dx}$ :
$$=\frac{1}{2}\Bigg(\frac{x^\frac{1}{2}}{y^\frac{1}{2}}\bigg(\frac{y}{x^2}-(\frac{y}{x})^\frac{1}{2}\frac{1}{x}\bigg)\Bigg)$$
After simplifying i got : $$\frac{d^2y}{dx^2}=\frac{1}{2x}\bigg(\frac{y^\frac{1}{2}}{x^2}-1\bigg).   .   .   .   .(3)$$
but from simplification of equation (2) in terms of $a$ wil result :
$$\frac{dy}{dx}=1-(\frac{a}{x})^\frac{1}{2}$$
here i can easily simlify this to get $\frac{d^2y}{dx^2}$ i.e.
$$\Rightarrow\frac{d^2y}{dx^2}=\frac{\sqrt a}{2x\sqrt x}.   .   .   .   .(4)$$
I dont know why i am unable to reduce (3) to (4).May be there exists some calculation error,even thats not my question.
proceeding to find radius of curvature and curvature : FROM FORMULA
$$\rho=\frac{\bigg(1+(\frac{dy}{dx})^2\bigg)^\frac{3}{2}}{\frac{d^2y}{dx^2}}$$
putting the value of (2) and (4):
$$\Rightarrow \rho=\frac{\bigg(1+\frac{y}{x}\bigg)^\frac{3}{2}2x\sqrt x}{\sqrt a}$$
$$\Rightarrow\rho=\frac{2(x+y)^\frac{3}{2}}{\sqrt a}$$
so curvature is
$$\frac{1}{\rho}=\kappa=\frac{\sqrt a}{2(2x+a-2\sqrt a\sqrt x)^3/2}$$
[notice that i have put y in terms of a nad x] NOW BEGINS THE PROBLEM for being extremum
$\frac{d\kappa}{dx} =0 $ and i have to check the sign of $\frac{d^2\kappa}{dx^2}$ : as you can see in the image :
$$\frac{d\kappa}{dx}=\frac{3\sqrt a(2-\frac{\sqrt a}{\sqrt x})}{4(2x-2\sqrt x\sqrt a+a)^\frac{3}{2}}$$
Now letting this to zero we have :
$$2=\frac{\sqrt a}{\sqrt x}$$
thus i am getting $x=\frac{a}{4}$ as a critical point.
BUT the answer is given as $\frac{\sqrt 2}{a}$
you can even see by inspection $x=\frac{a}{4}$ is not a critical point. please help and let me know where i have made mistake.
THIS IS MY HUMBLE REQUEST.","['curvature', 'multivariable-calculus', 'calculus', 'algebra-precalculus', 'derivatives']"
2904879,Is $S_{10}$ generated by the group of $6$ cycles?,"Is $S_{10}$ generated by the subgroup of $6$ cycles? I'll denote that subgroup by $H$. We know that by conjugation permutations remains with the same cycle structure, and therefore $H$ is closed to conjugation. It is easy to see from here that $H$ is normal subgroup. The only normal subgroup of $S_{10}$ is  $A_{10}$, therefore $H$ must be either  $S_{10}$ or $A_{10}$. How can i determine which one from here? Are the specific numbers $10,6$ even relevant?","['permutations', 'group-theory', 'normal-subgroups']"
2904895,To construct a graph using path graph $P_4$,"I was trying to construct a graph $G_1$ and $G_2$ , wherein both graphs $P_4$ is an induced graph. Graph $G_1$ is such that it contains exactly one vertex of eccentricity two and rest of the vertices with eccentricity three. Similarly,
the graph $G_2$ is such that it contains exactly one vertex of eccentricity three and the rest of the vertices with eccentricity four. In both the cases, $P_4$ is induced in $G_1$ and $G_2$ . I tried in the following manner. For $G_1$ , I added $6$ vertices to $P_4$ and got the result, and for $G_2$ , I added $10$ vertices to $P_4$ and got the result. However, later I got that $G_1$ can be obtained with the fewer number of vertices. Can $G_2$ be also obtained by adding less than $10$ vertices, if possible? 
Kindly help me to get the graph. Any hint or suggestion is helpful. My attempt : (numbers are the eccentricity of the vertices) Graph $G_1$ with less number of vertices: P.S. For $G_2$ , I also got an example. consider $C_6$ with vertices $1,2,3,4,5,6$ . Add $5$ vertices $1′,2′,3′,4′,5′$ and make edges $1′2′,2′3′,3′4′,4′5′$ , and $xx′$ , where $x=1,2,3,4,5$ . This also gives a graph with exactly one vertex with eccentricity $3$ and rest with $4$ . The total number of vertices is $11$ . Can we think of a smaller order? I am wondering that.","['graph-theory', 'combinatorics', 'discrete-mathematics']"
2904977,Compute norm of linear functional,"I was trying to solve the following problem.
$$
\varphi(f) = \int_0^1 t f(t)dt, \quad f \in X = \{g: [0,1]\to \mathbb{R}, g \text{ continuous in }[0,1] \}
$$ 
Consider X with the norm $ \| \cdot \|_1 $. Show that $ \varphi \in X^*  $ and compute its norm. I've done the following: 
$$
|\varphi(f)| \leq \int_0^1 t|f(t)|dt \leq \int_0^1|f(t)|dt=\|f\|_1
$$
$$
\implies \|\varphi\|\leq 1 \implies \varphi \in X^*
$$
Then, I was trying to show that $ \| \varphi \| =1 $. It seemed to me that the sequence $ f_n = n \chi_{(1,1-1/n)} $ did the job, since: 
$$
\| f_n\| = 1 \quad  \forall n \in \mathbb{N} 
$$
and 
$$
\int_0^1 tf_n(t)dt   = n \int_{1-1/n}^1tdt = \frac n 2 t^2 \bigg|_{1-1/n}^1 = \frac n 2 \left[1-\left(1-\frac 1 n \right)^2\right] = \frac n 2 \left(- \frac{1}{n^2} + \frac 2 n  \right) = 1 - \frac{1}{2n}
$$
Hence, $ \forall n \in \mathbb{N} $, 
$$ 
\| \varphi \| = \sup_{f\neq0} \frac{|\varphi(f)|}{\|f\|_1} \ge \frac{|\varphi(f_n)|}{\|f_n\|_1}= 1-\frac{1}{2n}
\implies \| \varphi \| \ge 1
$$ 
However, $f_n$ are not continuous, meaning that $f_n \not\in X$ and I cannot use them. How can I proceed? I was thinking about using the density of $C([0,1])$ in $L^1([0,1])$ but I do not know how. Can someone please help me? Thank you in advance.","['measure-theory', 'operator-theory', 'functional-analysis']"
2905015,If $f:\mathbb{R}\to\mathbb{R}$ is continuous and bijective. Is $f^{-1}:\mathbb{R}\to\mathbb{R}$ continuous?,"I have a function $f:\mathbb{R}\to\mathbb{R}$ that is both continuous and bijective.
My instinct tells me that $f^{-1}$ is also continuous.but I'm not able to show it. All I can show is that f is strictly monotone. Can anyone show how to move on from this point onwards to show the continuity of $f^{-1}$.
If possible you can use topology for the proof. Thanks in advance","['general-topology', 'real-analysis']"
2905016,General formula for Differentiation Operator,"I was considering the Operator
$$
x\,\frac{\rm d}{{\rm d}x}
$$
and applying it $n$ times to an arbitrary function $f(x)$.
Is there a general formula for it? I started with the first few
\begin{align}
\left(x\,\frac{\rm d}{{\rm d}x}\right)^{1} f(x) &= x f' \\
\left(x\,\frac{\rm d}{{\rm d}x}\right)^{2} f(x) &= x f' + x^2 f'' \\
\left(x\,\frac{\rm d}{{\rm d}x}\right)^{3} f(x) &= x f' + 3x^2 f'' + x^3 f''' \\
\left(x\,\frac{\rm d}{{\rm d}x}\right)^{4} f(x) &= x f' + 7 x^2 f'' + 6x^3 f''' + x^4 f'''' \\
\vdots
\end{align}
but I don't see any pattern yet.
Obviously the first and last coefficients are always $1$. I figured if I start with any $n$ of the form
$$
\left(x\,\frac{\rm d}{{\rm d}x}\right)^{n} f(x) = \sum_{k=1}^n a_k \, x^k f^{(k)}(x) \tag{1}
$$
where $a_1=a_n=1$, then recursively
\begin{align}
\left(x\,\frac{\rm d}{{\rm d}x}\right)^{n+1} f(x) &= xf^{(1)}(x) + \sum_{k=2}^n \left(k \, a_k + a_{k-1}\right) x^k f^{(k)}(x) + x^{n+1} f^{(n+1)}(x) \\
&= \sum_{k=1}^{n+1} \left(k \, a_k + a_{k-1}\right) x^k f^{(k)}(x) \tag{2}
\end{align}
where $a_0=a_{n+1}=0$. From (1) and (2) we get for fixed $k$ the coupled recurrence
$$
a_k(n+1) = k\, a_k(n) + a_{k-1}(n)
$$
which can be put in matrix form
\begin{align}
\begin{pmatrix} a_1(n+1) \\ a_2(n+1) \\ a_3(n+1) \\ \vdots \\ a_{n-1}(n+1) \\ a_n(n+1) \end{pmatrix} &= \begin{pmatrix} 1 & 0 & 0 & \dots & 0 & 0 \\ 1 & 2 & 0 &  \dots & 0 & 0 \\ 0 & 1 & 3 &  \dots & 0 & 0 \\ \vdots & \vdots & \vdots & \dots & \vdots & \vdots \\ 0 & 0 & 0 & \dots & n-1 & 0 \\ 0 & 0 & 0 & \dots & 1 & n \end{pmatrix} \begin{pmatrix} a_1(n) \\ a_2(n) \\ a_3(n) \\ \vdots \\ a_{n-1}(n) \\ a_n(n) \end{pmatrix} \\
&=\begin{pmatrix} 1 & 0 & 0 & \dots & 0 & 0 \\ 1 & 2 & 0 &  \dots & 0 & 0 \\ 0 & 1 & 3 &  \dots & 0 & 0 \\ \vdots & \vdots & \vdots & \dots & \vdots & \vdots \\ 0 & 0 & 0 & \dots & n-1 & 0 \\ 0 & 0 & 0 & \dots & 1 & n \end{pmatrix}^n \begin{pmatrix} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \\ 0 \end{pmatrix} \, .
\end{align}
So either I'm too stupid or there is no obvious one.","['differential-operators', 'summation', 'derivatives', 'recurrence-relations']"
2905022,Hints on solving the equation $3\sqrt{x-1}+\sqrt{3x+1}=2$,"I recently stumbled upon the problem $3\sqrt{x-1}+\sqrt{3x+1}=2$, where I am supposed to solve the equation for x. My problem with this equation though, is that I do not know where to start in order to be able to solve it. Could you please give me a hint (or two) on what I should try first in order to solve this equation? Note that I only want hints. Thanks for the help!",['algebra-precalculus']
2905043,Prove the reduced Riesz representation theorem for finite-dimensional vector spaces with using only the concepts in linear algebra,"If someone did not study functional analysis but just studied linear algebra, how to let them understand the idea of Riesz representation theorem for finite-dimensional vector spaces? The Riesz representation theorem Wikipedia Let $H$ be a Hilbert space, and let $H^*$ denote its dual space, consisting of all continuous linear functionals from $H$ into the field $\mathbb{R}$ or $\mathbb{C}$. If $x$ is an element of $H$, then the function $\varphi_{x}$, for all $y$ in $H$ defined by: 
\begin{align*}
\varphi_x (y) = \langle y,x \rangle 
\end{align*}
where $\langle \cdot,\cdot \rangle$ denotes the inner product of the Hilbert space, is an element of $H^*$. The Riesz representation theorem states that every element of $H^*$ can be written uniquely in this form. This description is abstract to me. Since linear algebra is sort of the reduced functional analysis, at the very first step, I am thinking to understand the reduced Riesz representation theorem applied to linear algebra. In linear algebra, we intend to solve the problem of a linear system
\begin{align*}
A x = b
\end{align*}
where $A \in \mathbb{R}^m \times \mathbb{R}^n$ is an $m$ by $n$ matrix, $x \in \mathbb{R}^n$ is an $n$ by $1$ column vector and $b \in \mathbb{R}^m$ is an $m$ by $1$ column vector. The matrix $A$ transforms vectors in $\mathbb{R}^n$ to vectors in $\mathbb{R}^m$, thus we say $A: \mathbb{R}^n \to \mathbb{R}^m$. But the vector $b$ is actually in the column space of $A$, say $C(A) = \mathbb{R}^r \subset \mathbb{R}^m$, which has dimension $r$ that denotes the rank of $A$. Thus we can say $A: \mathbb{R}^n \to \mathbb{R}^r$. If we have an $m$ by $1$ column vector $y$, then we can write
\begin{align}
y^T A x = y^T b
\end{align}
We can rewrite it in the form of inner product
\begin{align}
\langle y,Ax \rangle = \langle y,b \rangle
\end{align}
And if we consider $b$ as a functional in the dual space of $\mathbb{R}^r$, denoted by $\varphi_{Ax}(\cdot) := \langle \cdot,b \rangle$, then
\begin{align}
\varphi_{Ax} (y) = \langle y, A x \rangle
\end{align}
Note that the mapping between $b$ and $\varphi_{Ax}$ is one-to-one. We say, every $b$ in $\mathbb{R}^r$ can be written uniquely in this form. It is very close to the equation in the Riesz representation theorem, but it seems we have to use $Ax$ instead of $x$, unless $A=I$ and $m=n=r$? I am trying to state the reduced version of the Riesz representation theorem in linear algebra, as follows: $\mathbb{R}^r$ is a Hilbert space, and its dual space $(\mathbb{R}^r)^*=\mathbb{R}^r$, consisting of all continuous linear functionals from $\mathbb{R}^r$ into the field $\mathbb{R}$. If $Ax$ is an element of $\mathbb{R}^r$, then the function $\varphi_{Ax}$, for all $y$ in $\mathbb{R}^r$ defined by: 
\begin{align*}
\varphi_{Ax} (y) = \langle y,Ax \rangle 
\end{align*}
where $\langle \cdot,\cdot \rangle$ denotes the inner product of the Hilbert space, is an element of $(\mathbb{R}^r)^*$. The Riesz representation theorem states that every element of $(\mathbb{R}^r)^*$ can be written uniquely in this form. That is, every vector $b$ in $\mathbb{R}^r$ can be represented by $\langle y,Ax \rangle$. This looks like a connection to the ""weak formulation"" of $Ax = b$, namely, we can find the solution $x \in \mathbb{R}^n$ of $Ax = b$, if for every ""test"" vector $y \in \mathbb{R}^m$ there holds $\varphi_{Ax} (y) = \langle y,Ax \rangle$. I am still not fully understand the theorem at this moment, so there might be something wrong stated above. Any comments? Could you provide me with a more clear structure of the reduced Riesz representation theorem in linear algebra? In addition, the proof of the Riesz representation theorem in textbooks usually take with a nullspace of $\varphi$ denoted by $\mathrm{ker}(\varphi)$ and its orthogonal space $\mathrm{ker}(\varphi)^{\perp}$. In linear algebra, we know the row space of a matrix is always orthogonal to its nullspace. Is there any connection between these two ideas? In other words, can we prove the reduced Riesz representation theorem for finite-dimensional vector spaces with using only the concepts in linear algebra?","['analysis', 'linear-algebra', 'functional-analysis', 'linear-transformations', 'partial-differential-equations']"
2905057,Curvature function and rate of change of angle,"Let $\gamma:(a,b)\rightarrow \mathbb{R}^2$ be a smooth curve with $\| \dot{\gamma}(s)\|=1$ for all $s\in (a,b)$. Fix $s_0\in (a,b)$ and let the unit vector $\dot{\gamma}(s_0)$ be represented by $(\cos \phi_0,\sin\phi_0)$.  Then there is smooth function $\phi$ with $\phi(s_0)=\phi_0$  such that 
  $$\dot{\gamma}(s)=(\cos\phi(s),\sin\phi(s))$$
  for all $s\in (a,b)$. The proof goes as follows: let 
$$\dot{\gamma}(s)=(f(s),g(s))$$
so that $f(s)^2+g(s)^2=1$ for all $s$. Define 
$$\phi(s)=\phi_0 + \int_{s_0}^s (f\dot{g}-g\dot{f})du$$
It is then shown that this is required $\phi$ in the theorem. Q. I didn't get intuition for choice (definition) of $\phi$. How do we justify the choice of $\phi$ above? Reference: Elementary differential geometry by Pressley, Proposition 2.2.1 (New edition) Using the explicitly defined angular function $\phi$, the curvature function is given by 
$$\kappa_s =\frac{d\phi}{ds}.$$",['differential-geometry']
2905065,Fitting probability distribution connection to regression,"My question is whether there is some connection between fitting probability distribution on some data set and linear regression? Or this two tools are for different problems? By fitting probability distribution I mean that I have some data $x_{1},...,x_{n}$ and I believe they came from for example normally distributed population with parameters $\mu$ and $\sigma^2$. And I will estimate these parameters with likelihood estimation technique or method of moments technique. By regression, I mean that I have random variable $X$ that is modelled for example as $X \sim a+bY+cZ$ where $Y$ and $Z$ are some random variables and I am estimating parameters $a,b$ and $c$. So my question is whether there is some link between these two ""mathematical topics""? Whether for example, I can look on fitting probability distribution as some specific problem of linear regression? Thank you for your answer.","['regression', 'statistics']"
2905070,Conditional probability involving sum of two independent geometric random variables,"Let $X$ and $Y$ be two independent geometric random variables with common
  parameter $p$. Find $P(Y = y|X + Y = z)$ where $z ≥ 2$ and $y = 1, 2, · · · , z − 1$. I am quite lost with this question.  The correct solution is $1\over z-1$, however I have no idea how they got there.  This is what I started with. $$P(Y = y|X + Y = z)$$ $$ P(Y = y)\cap P(X + Y = z)\over P(X + Y = z)$$ $$ p(1-p)^{y-1}\cap\sum_{n=0}^zp(1-p)^{n-1}p(1-p)^{z-n-1}\over \sum_{n=0}^zp(1-p)^{n-1}p(1-p)^{z-n-1}$$ $$ p(1-p)^{y-1}\cap\sum_{n=0}^zp(1-p)^{n-1}p(1-p)^{z-n-1}\over \sum_{n=0}^zp(1-p)^{n-1}p(1-p)^{z-n-1}$$ $$ p(1-p)^{y-1}\cap\sum_{n=0}^zp^2(1-p)^{z-2}\over \sum_{n=0}^zp^2(1-p)^{z-2}$$ $$ p(1-p)^{y-1}\cap(z+1)p^2(1-p)^{z-2}\over (z+1)p^2(1-p)^{z-2}$$ As you can see, I have made quite a mess and am nowhere near the correct solution. Can somebody please help me out with this?","['statistics', 'probability-distributions', 'probability']"
2905088,Improved intermediate value theorem,"Suppose $f\colon [a,b] \to \mathbb{R}$ is a continuous function with $f(a)<0$, $f(b)>0$. Can it be proved that there exists $s_1\leq s_2$ and $\epsilon>0$  such that $f(s)=0$ for all $s\in[s_1,s_2]$, whilst $f(s)<0$ for all $s\in [s_1-\epsilon, s_1)$ and $f(s)>0$ for all $s\in (s_2,s_2+\epsilon]$? If not, what about in the case that one assumes $f$ is $C^1$, or smooth?","['general-topology', 'real-analysis']"
2905234,Why vacuously true is defined for 'every' and is not defined for 'there exist',"When get general logic statements of vacuous truth, seems that the allowed forms are only for 'all ($\forall$)', and not for 'there exist ($\exists$)'. For example, in wiki , it shows the possible universally quantified statements are: $\forall x:P(x)\Rightarrow Q(x)$, where it is the case that $\forall x:\neg P(x)$. $\forall x\in A:Q(x)$, where the set $A$ is empty. $\forall \xi :Q(\xi )$, where the symbol $\xi$ is restricted to a type that has no representatives. I dont understand why there is no form constructed by $\exists$, for example: $\exists x\in A:Q(x)$, where the set $A$ is empty. In my understanding, this statement is equivalent to say: ""if there exist $x\in A$, then $Q(x)$ is true"". When $A$ is empty, that means the ""there exist $x\in A$"" is false, i.e. the $P$ of ""$P\Rightarrow Q$"" is false, then logically, the statement should be vacuously true. Could someone tell me where is wrong with this statement? (I know it's wrong as it is used to show ""$\varnothing\not\subset A$ is false instead of vacuous true"". See, for example, in discussion : ""$X$ is not a subset of $A$"", in symbols $X\not\subset A,$ means $\exists x(x\in X\text{ and }x\notin A).$ If $X$ has no elements, this existential statement is not true vacuously, it is simply false. ).","['elementary-set-theory', 'logic']"
2905289,Proof about eigenspace of eigenvalue in power of matrix,"Upon studying the Jordan normal form I came across the problem of determining the Jordan normal form of powers of a single base matrix and in that context I was wondering what happens to the eigenspaces. I am not yet entirely sure if the statement below is correct which is why I tried to prove it myself and would now like to know if the below proof is actually correct. Also, since I have not written many proofs yet as of now, please let me know if I have done any notational mistakes or if anything could be stated in a more elegant way. Show that given any square-matrix $A\in \mathbb{K}^{n\times n}, \mathbb{K}$ is a field, $k \in \mathbb{N}$ the following statement holds true: $E_{\mu}(A^k) = \oplus_i E_{\lambda_i}(A)$ where $\lambda_i$ is an eigenvalue of $A$ such that ${\lambda_i}^k = \mu$. ""$\supseteq$"": Let $\lambda_i$ be an eigenvalue of $A$ such that ${\lambda_i}^k = \mu$ and $v \in E_{\lambda_i}(A)$. Then: $Av = \lambda_iv$. $A^kv = A^{k-1}Av = \lambda_iA^{k-1}v = \ldots = {\lambda_i}^kv \implies v \in E_{{\lambda_i}^k}(A^k) = E_{\mu}(A^k)$. ""$\subseteq$"": Assume $E_\mu(A^k) \supset \oplus_iE_{\lambda_i}(A)$. Then $\exists v \in E_\mu(A^k) \setminus \oplus_iE_{\lambda_i}(A)$. $A^kv = {\mu}v \implies A^kv^k = A^kvv^{k-1} = {\mu}vv^{k-1} = {\mu}v^k$ therefore $v^k$ is an eigenvector of $A^k$ and $\mu$. Note that $Av \neq \lambda_iv$ for any eigenvalue $\lambda_i$ of $A$. Thus: $(Av)^k \neq (\lambda_iv)^k \Leftrightarrow A^kv^k \neq \lambda_i^kv^k = \mu v^k$ which means that $v^k$ cannot be an eigenvector of $A^k$ and $\mu$ which is a contradiction. Such a vector $v$ can therefore not exist, implying that $E_\mu(A^k) \subseteq \oplus_iE_{\lambda_i}(A)$.","['matrices', 'exponentiation', 'eigenvalues-eigenvectors']"
2905323,"""Exactly Two"" - Are these statements logically equivalent?","Let P(x,y) = ""x loves y""
Let the Domain of Discourse be all people ""Someone loves exactly two people"" I found that this can be expressed as: ∃x∃y∃z( y≠z ∧ P(x,y) ∧ P(x,z) ∧ ∀w(P(x,w) → (w=y ∨ w=z))) (Person y is not person z, and person x loves y and person x loves z, and for all people, if person x loves person w, then person w must be person y, or person w must be person z) However I also see this expression in many places and in similar circumstances: ∃x∃y∃z( y≠z ∧ ∀w(P(x,w) ↔ (w=y ∨ w=z))) (Person y is not person z, and for all people, person x loves person w if and only if person w is person y and person w is person z) Are these logically equivalent?","['quantifiers', 'discrete-mathematics']"
2905324,Constructive proof of the Cauchy Schwarz inequality,"The famous CS inequality states $$
\left| \left< x , y \right>\right|
  \le \left\| x \right\| \cdot \left\| y \right\|
$$ for $x,y$ in an inner product space $X$ over $\mathbb{K}$. Every proof I found involves some kind of case distinction; namely one may use w.l.o.g $\Vert x \Vert, \Vert y \Vert > 0$ since the inequality is trivial otherwise. However, I was looking for a constructive proof (i.e. without using the law of excluded middle) for the inequality. I will add some remarks to the question. 1) Law of excluded middle: This axiom states that $A \vee \neg  A$ is true. This is not considered an axiom in constructive mathematics. One might interpret this in the following way: Indirect proofs are not allowed. However, I find this not completely accurate. Precisely the implication $A \rightarrow \neg \neg A$ is true in constructive mathematics; the implication $ \neg \neg A \rightarrow A$ not in gerneral. 2) The definition of an inner product is the same as in ""classical"" mathematics. 3) The norm $\Vert \cdot \Vert$ is given by $\Vert x \Vert = \sqrt{\langle x, x\rangle }$.","['constructive-mathematics', 'inequality', 'cauchy-schwarz-inequality', 'analysis']"
2905338,An $n$-norm for the space of continuous functions?,"Given a (real) vector space $F$ of dimension $\geq n$, recall that  $\|\cdot,\ldots,\cdot\|:F^{n}\longrightarrow [0,\infty)$ satisfying the following conditions for each $x_{0},x_{1},\ldots,x_{n}\in F$: (1) $\|x_{1},\ldots,x_{n}\|=0$ if, and only if, $x_{1},\ldots,x_{n}$ are linear dependent. (2) $\|x_{1},\ldots,x_{n}\|$ is invariant under permutation. (3)  $\| \lambda x_{1},\ldots,x_{n}\|=|\lambda | \|x_{1},\ldots,x_{n}\|$ for every $\lambda\in\mathbb{R}$. (4) $\|x_{0}+x_{1},\ldots,x_{n}\|\leq \|x_{0},\ldots,x_{n}\|+\|x_{1},\ldots,x_{n}\|$. is said to be an $n$-norm on $F$, and the pair $(F,\|\cdot,\ldots,\cdot\|)$ a $n$-normed space. For instance, if $F:=\mathbb{R}^{d}$, with $d\geq n$, then
$\|x_{1},\ldots,x_{n}\|:=\sqrt{ \mathrm{det}\big(\langle x_{i},x_{j} \rangle  \big) }$ is an $n$-norm in $F$. How can we define an $n$-norm in the linear space $F$ of the continuous functions $f:[0,1]\longrightarrow \mathbb{R}$? I am thinking about a mapping of the form:
$$\|f_{1},\ldots,f_{n}\|:= \sup\left\{ \left|\, \mathrm{det}\left[ \begin{array}{l} f_{1}(x_{1}) & \cdots &f_{n}(x_{n}) \\ f_{1}(x_{1}) &\cdots &f_{n}(x_{n}) \\ \vdots & \ddots & \vdots \\ f_{1}(x_{1})&\cdots& f_{n}(x_{n})   \end{array}\right] \,\right|:x_{1},\ldots,x_{n} \in [0,1]\right\}$$ which (from the properties of the determinant) it seems that obeys the conditions (2)-(4) of the above definition. I am not sure if the condition (1) holds. What do you think? Do you known some $n$-norm in this space $F$? Many thanks in advance for your comments and suggestions.","['functional-analysis', 'real-analysis']"
2905350,Solving $px^n -x + (1-p)=0$,"I'm interested in solving the following equation in $[0,1]$ $$px^n - x + (1-p)=0$$ where $p \in [0,1]$ and $n \in \mathbb N -\{1,2 \} $ both constant. To start with, we can easily see that $x=1$ is a solution and I also know there is another solution in $[0,1]$ for every $n \in \mathbb N -\{1,2 \}$ and for all $p> p_c (n)$ . I've tried using the Horner method with $(x-1)$ we get : $$(x-1)(px^{n-1} + px^{n-2}+..+ px^2 + px+p-1)=0 $$ So we get $(px^{n-1} + px^{n-2}+..+ px^2 + px+p-1)=0 $ . Then we can do :
$$ x^{n-1} + x^{n-2}+..+ x=\frac{1-p}{p}$$ or $$\frac{x^n-1}{x-1}=\frac{1-p}{p}+1 $$ But this doesn't seem to give anything useful. Any ideas on how we can solve this?","['algebra-precalculus', 'polynomials']"
2905354,"Recursive set $A = \lbrace 1,A \rbrace $","You have given $A := \{ 1,A \}$ (which presents a weird way of defining a set. Is this recursive?). The question is $|A|  = \ldots $ ? Is $|A|= 2$ because $A = \{ 1 , \{ 1 , \{ 1,\{ 1 , ... \} \} \} \}$? What can you say about $A$?","['elementary-set-theory', 'calculus', 'algebra-precalculus']"
2905361,Proof of 1/n (where n is from Natural Numbers) as neither open nor closed? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I have a doubt with the proof of 1/n being neither open nor closed. I have seen various replies to similar queries; however, none of them seems to be speaking of the same reasons. So, I would like to know a conclusive proof of the assertion. Thanks
Jai","['elementary-set-theory', 'general-topology', 'real-analysis']"
2905378,Fractional Brownian Motion and Fractional Laplacian,"It is well known that the Laplacian is the infinitesimal generator of a Brownian Motion, that is, $$
\lim_{t \to 0} \frac{E[f(x+B_t)-f(x)]}{t}= \Delta f(x).
$$ Is it true that for the Fractional Brownian Motion $B_H$ with hurst parameter $H \in (0,1)$ , that is, the continuous-time Stochastic Process with stationary gaussian increments, mean $0$ and covariance $$
\mathbb{E}[(B_H(t)-B_H(s))^2]= |t|^{2H} + |s|^{2H} - |t-s|^2H
$$ has the infinitesimal generator to be a fractional Laplacian $-(-\Delta)^{\alpha}$ for some $\alpha \in (0,\infty)$ ? If so, what is the relation between $H$ and $\alpha$ ?","['stochastic-processes', 'semigroup-of-operators', 'probability-theory']"
2905396,"Given number of regions, find maximum number of boundaries","Problem: Supoose we want to divide $\mathbb{R^2}$ into $N$ regions separated by straight lines. What is the maximum number of boundary lines we could have? I also pose the restriction that between any two regions, there can be only one boundary line (so zigzagging a boundary does not increase the count). For example, here I attempt to find the maximum number of boundaries for $N = 4$ regions in $\mathbb{R}^2$, and I count 6 boundary lines. But is there a general formula for general integer $N$? Can someone point me to a reference on this problem?","['combinatorics', 'geometry']"
2905460,Trouble understanding the $ε$-$N$ proof for limit of sequence,"I'm currently stuck on a math problem proposed by my teacher. For $\varepsilon=0,001$, find $N=N_0$ such that $|a_n-L|\lt \varepsilon$ if $n\geqslant N$ $$a_n = \frac{(n+1)}{3n-1}\, {\rm and} \, L=1/3$$ According to him, the answer could be any $N_0$ greater than $\left[\frac{2}{3\varepsilon}\right]+1$. I have an idea of how the proof works, but I can't figure out how he came up with that specific value for N$_0$. And why the final value has a $+1$.","['limits', 'calculus', 'sequences-and-series']"
2905474,"Regarding a proof of: ""if $A,B \in M_n(\mathbb{k})$ are diagonalizable and commute, they are simultaneously diagonalizable"".","As the title states, I'm looking for a proof of the following, Proposition. Let $A, B \in M_n(\mathbb{k})$ be commuting diagonalizable matrices, so that $AB = BA$. Therefore, $A$ and $B$ can be diagonalized in the same basis. with these additional requirements: no usage of minimal polynomials, and as elementary an argument as possible. Looking for similar questions, I stumbled upon this answer. It proves that eigenvalues of $A$ are $B$-invariant and vice versa. If these were one dimensional, then by restricting $A$ or $B$ as functions to the eigenspaces of the other, we see that they share all eigenvectors (although possibly with different eigenvalues) and thus any base of them will diagonalize both matrices simultaneously. However, the case for eigenspaces of arbitrary dimension is left as an exercise. Any hints on how to proceed? Edit: upon reading this answer, I think the question can be reduced to: how can we show that given an eigenspace $E_\lambda$, $B : E_\lambda \to E_\lambda$ is diagonalizable? If this is answered, then since $$
\mathbb{k}^n = E_{\lambda_1} \oplus \cdots \oplus E_{\lambda_n}
$$ with $\lambda_1, \dots, \lambda_n$ the eigenvalues of $A$, and each restriction of $B$ to $E_{\lambda_i}$ can be diagonalized on a basis $B_i = \{v^i_1 , \dots, v^i_{k_i}\}$, the basis $\mathcal{B} = \cup_{i=1}^nB_i$ of $\mathbb{k}^n$ consists of eigenvectors of $B$ that are also eigenvectors of $A$, precisely because each $v_j^i \in E_{\lambda_i}$. Thus, each element of $\mathcal{B}$ would be an eigenvector for both $A$ and $B$, which implies that $\mathcal{B}$ diagonalizes the matrices at the same time. In short, if I have thought about this correctly, my question reduces to: how can one show that a $B$-invariant eigenspace of $A$ has a basis of eigenvectors of $B$?","['matrices', 'proof-explanation', 'linear-algebra']"
2905482,"Expectation and variance of $Y=\max(X_1,\ldots,X_n)$, where $X$ is uniformly distributed.","I've got a problem, with a solution, in my introduction to mathematical statistics book and I just don't get how they got there. There are follow up questions so I'd like to get insight at how they got to the answer. The problem:
Let $X_1,\ldots,X_n$ be independent random variables with the uniform distribution on the interval $[0,1]$. Determine the expectation and variance of $Y=\max(X_1,\ldots,X_n)$. Hint: Deduce the density of Y from the distriution function $P(Y \leq y)$ of $Y$, which can be determined using the distribution functions of $X_1, \ldots, X_n$. The solution: $\operatorname E[Y]=\frac{n}{n+1}$, $\operatorname{var}[Y]=\frac{n}{n+2}+\left(\frac{n}{n+1}\right)^2$. Now I know that a uniformly distributed random variable on $[0,1]$ has the following probability distribution, expectation and variance; $F(x)=x$ for $x \in [0,1]$,  $\operatorname E[X]=\frac{1}{2}$ and $\operatorname{var}[X]=\frac{1}{12}$.","['expected-value', 'probability-distributions', 'uniform-distribution', 'probability']"
2905486,"Is it possible that $\text{lcm}(zx, zy)$ exists but $\text{lcm}(x, y)$ not?","Is it possible to find an example of an integral domain $D$ and a pair of non-zero elements $x$ and $y$ in $D$ such that $\text{lcm}(zx, zy)$ exists for some non-zero element $z$ in $D$ but $\text{lcm}(x, y)$ does not? Definition of least common divisor (lcm):
Let $a$ and $b$ be elements of a commutative ring $R$. A common multiple of $a$ and $b$ is an element $m$ of $R$ such that there exist elements $x$ and $y$ of $R$ such that $ax = by = m$. A least common multiple of $a$ and $b$ is a common multiple $m$ of $a$ and $b$ that is minimal in the sense that for any other common multiple $n$ of $a$ and $b$, $n=zm$ for some $z$ in $R$.",['abstract-algebra']
2905488,"Closed form of integral over fractional part $\int_0^1 \left\{\frac{1}{2}\left(x+\frac{1}{x}\right)\right\}\,dx$","Recenly, several interesting questions have been posted asking for closed forms of integrals over the fractional part of certain functions.
For me the story started with Evaluation of $\int_{0}^{1}\int_{0}^{1}\{\frac{1}{\,x}\}\{\frac{1}{x\,y}\}dx\,dy\,$ which after a long and instructive journey I could solve completely. Another example was symmetric double-integral on fractional part . These are examples of double integrals. There are as well many single integrals, and, as we can see below, the field of single integrals is by far not exhausted. This time my result is given in the beginning and a proof is asked for. Let $\{z\}$ be the fractional part of $z$.
Prove that: $$i := \int_0^1 \left\{\frac{1}{2}\left(x+\frac{1}{x}\right)\right\} = i_{s} $$ where $$i_{s}=c_{g}-\frac{\gamma }{2}+\frac{3}{4}+\frac{\log (2)}{2} \simeq 0.28000699470709318696$$ Here $\gamma$ is the Euler-Mascheroni constant and $$c_{g} = \int_0^{\infty } \frac{t-2 I_1(t)}{2 \left(e^t-1\right) t} \, dt \simeq -0.52795876312211303745$$ where $I_{n}(t)$ is the modified Bessel function of the first kind. $c_{g}$ is a (probably) new constant which appears in the asymptotic expansion of the sum $$g(n) = \sum _{k=1}^n \sqrt{k^2-1} $$","['integration', 'summation', 'asymptotics', 'fractional-part', 'bessel-functions']"
2905570,Proof for differentiable functions,"Find all positive differentiable functions $f$ that satisfy $$\int_0^x \sin(t) f(t) dt = [f(x)]^2-1$$for all real numbers $x$. So, it appears that this question has been asked in the past (at this link Find all positive differentiable functions $f$ that satisfy $\int_0^x \sin(t) f(t) dt = [f(x)]^2.$ ). However, given the hint that this user provided in the previous post, I believe I am still unsure how to go proceed. Also, as the previous poster said, this is the correct problem and I believed people answered about the incorrect problem. My reasoning is that if you differentiate both sides, you can get a function f such that it will only be positive. However, I’m not sure that this I’m fact works. Can someone please help? Thanks. (I’m also sorry about my previous posts - please don’t downvote anymore)","['ordinary-differential-equations', 'proof-explanation', 'proof-writing', 'calculus', 'derivatives']"
2905593,Stone–Weierstrass theorem with exact matches at a finite number of points,"This is a proof-verification request. Let $(X,d)$ be a compact metric space and let $\mathbf{C}(X)$ denote the set of (bounded and) continuous real-valued functions on $X$. Suppose that $\mathcal A\subseteq \mathbf{C}(X)$ is a subalgebra that contains the constant functions and separates points; that is: $f,g\in\mathcal A$ and $\alpha,\beta\in\mathbb R$ imply $\alpha f+\beta g\in\mathcal A$; $f,g\in\mathcal A$ imply that $fg\in\mathcal A$; all constant functions on $X$ are included in $\mathcal A$; and $x,y\in X$ and $x\neq y$ imply the existence of some $f\in\mathcal A$ such that $f(x)\neq f(y)$. The Stone–Weierstrass theorem implies that $\mathcal A$ is dense in $\mathbf{C}(X)$ (with respect to the supremum metric $d_{\infty}$). I propose a result and a proof showing that one can do even better: approximating any continuous function on $X$ uniformly by members of $\mathcal A$ while reaching exact agreement at any finite number of points! This result was put forth by Boel et al. (2001) for a more general case (locally compact Hausdorff spaces and complex-valued continuous functions vanishing at infinity), but I felt more comfortable with constructing a slightly different proof. Proposition: Suppose that $\varphi\in\mathbf{C}(X)$ and that $x_1,\ldots,x_k$ are distinct points in $X$ ($k\in\mathbb N$). Then, for any $\varepsilon>0$, there exists some $f\in\mathcal A$ such that $d_{\infty}(\varphi,f)<\varepsilon$ and $\varphi(x_i)=f(x_i)$ for all $i\in\{1,\ldots,k\}$. Before proving this proposition, an auxiliary result is needed. Lemma: For any $(c_1,\ldots,c_k)\in\mathbb R^k$, there exists some $g\in\mathcal A$ such that $g(x_i)=c_i$ for $i\in\{1,\ldots,k\}$. Proof of lemma: Define the evaluation functional $\Phi:\mathbf{C}(X)\to\mathbb R^k$ as $$\Phi(\varphi)\equiv(\varphi(x_1),\ldots,\varphi(x_k))$$ for every $\varphi\in\mathbf C(X)$. Letting $$\delta\equiv\min_{\substack{i\in\{1,\ldots,k\}\phantom{\setminus\{i\}}\\j\in\{1,\ldots,k\}\setminus\{i\}}}d(x_i,x_j)>0,$$ one can define $$\varphi(x)\equiv\sum_{i=1}^kc_i\max\left\{1-\frac{d(x,x_i)}{\delta},0\right\}$$ for every $x\in X$. It is not difficult to check that $\varphi\in\mathbf{C}(X)$ and $\varphi(x_i)=c_i$ for every $i\in\{1,\ldots,k\}$. This entails that $\Phi$ is surjective: $\Phi(\mathbf{C}(X))=\mathbb R^k$. The Stone–Weierstrass theorem, in turn, implies that $\Phi(\mathcal A)$ is a dense linear subspace of $\mathbb R^k$. Since $\Phi(\mathcal A)$ is finite-dimensional, it must be closed, so that $\Phi(\mathcal A)=\mathbb R^k$. The result follows. $\blacksquare$ Proof of proposition: I am going to proceed by induction on the number of points. The case $k=1$ is easy: take any $h\in\mathcal A$ such that $d_{\infty}(\varphi,h)<\varepsilon/4$ and denote $\Delta\equiv\varphi(x_1)-h(x_1)$. Defining $$f(x)\equiv h(x)+\Delta$$ for every $x\in X$, one has that $f\in\mathcal A$. Now, for any $x\in X$, $$|f(x)-\varphi(x)|\leq|h(x)-\varphi(x)|+|h(x_1)-\varphi(x_1)|\leq 2d_{\infty}(\varphi,h)<\frac{\varepsilon}{2},$$ so that $d_{\infty}(f,\varphi)\leq\varepsilon/2<\varepsilon$. Clearly, $f(x_1)=\varphi(x_1)$ by construction. Now suppose that the result holds up until $k\in\mathbb N$. For $k+1$, choose some $g\in\mathcal A$ such that
\begin{align*}
g(x_1)=\cdots=g(x_k)=&\;0,\\
g(x_{k+1})=&\;1,
\end{align*}
which is possible by the lemma. Let $$\xi\equiv\sup_{x\in X}|g(x)|\in(0,\infty).$$ By the induction hypothesis, it is possible to choose $h\in\mathcal A$ in such a way that $$d_{\infty}(\varphi,h)<\min\left\{\frac{\varepsilon}{4},\frac{\varepsilon}{4\xi}\right\}$$ and $h(x_i)=\varphi(x_i)$ for $i\in\{1,\ldots,k\}$. Let $\Delta\equiv\varphi(x_{k+1})-h(x_{k+1})$ and define $$f(x)\equiv h(x)+\Delta g(x)$$ for every $x\in X$. Clearly, $f\in\mathcal A$, $f(x_i)=\varphi(x_i)$ for every $i\in\{1,\ldots,k,k+1\}$, and for any $x\in X$, $$|f(x)-\varphi(x)|\leq|h(x)-\varphi(x)|+|g(x)||\Delta|\leq d_{\infty}(h,\varphi)+\xi d_{\infty}(h,\varphi)<\frac{\varepsilon}{2},$$ which implies that $d_{\infty}(f,\varphi)\leq\varepsilon/2<\varepsilon$. The induction thus goes through from $k$ to $k+1$ and the proof is complete. $\blacksquare$","['general-topology', 'proof-verification', 'functional-analysis']"
2905628,Isomorphisms of complex (foliated) n-tori,"From here: https://www.encyclopediaofmath.org/index.php/Complex_torus A complex torus is 
a complex Abelian Lie group obtained from the $n$-dimensional complex space $\mathbb{C}^n$ by factorizing with respect to a lattice $\def\G{\Gamma}\G\subset \mathbb{C}^n $ of rank $2n$. A basis for a lattice $\Gamma\subset \mathbb{C}^n$ can be given by a matrix $\def\O{\Omega}\O$ of dimension $n\times 2n$, called the period matrix of the torus $T=\mathbb{C}^n/\G$. Tori $T_i = \mathbb{C}^n/\Gamma_i$ with period matrices $\O_i$ ($i=1,2$) are isomorphic (as complex Lie groups or as complex manifolds) if and only if there exist matrices $C\in \textrm{GL}(n,\mathbb{C}) $ and $Z\in \textrm{GL}(2n,\mathbb{Z})$ such that $\O_2 = C\O_1 Z$. My questions are: What is the role of these two actions ($\textrm{GL}(n,\mathbb{C}) $ and $\textrm{GL}(2n,\mathbb{Z})$)? Is one of them related to the lattice (isomorphism of the lattices?) and another to the complex structure of the torus itself? Where can I find a detailed proof for the n-dimensional case? Let us now for a moment forget a complex structure and regard $\mathbb{C}^n$ as $\mathbb{R}^{2n}$ with coordinates $\{x^i, y^i\}, i=\overline{1,n}$. Suppose we have a $n$-dimensional foliation on $\mathbb{R}^{2n}$ given as $y^i=const$. This foliation induces foliations on tori $T_i$, obtained from different lattices. As far as I understand biholomorphic isomorphisms mentioned above do not take into consideration the structure of these foliations on tori. Do someone have the idea how to find the relation (isomorphism) between two different foliated n-tori (obtained from two different lattices), in the terms of period matrices of the lattices? I see there perhaps will be much of number theory (in 2-dim case rational and irrational slopes will obviously give closed and dense leaves corresp.), but I am not sure how to express it correctly in n-dim case. Thanks to all of you in advance! ♥","['holomorphic-foliations', 'foliations', 'algebraic-geometry']"
2905636,Let $\lim\limits_{n\to\infty}a_{n+1}-a_{n}=\alpha$. Show that $\lim\limits_{n\to\infty} \frac{a_n}{n}=\alpha$. [duplicate],"This question already has an answer here : Limit of $b_n$ when $b_n=\frac{a_n}{n}$ and $\lim\limits_{n\to\infty}(a_{n+1}-a_n)=l$ [duplicate] (1 answer) Closed 5 years ago . Let $\lim\limits_{n\to\infty}a_{n+1}-a_{n}=\alpha$. Show that $\lim\limits_{n\to\infty} \frac{a_n}{n}=\alpha$. Since $\lim\limits_{n\to\infty}a_{n+1}-a_{n}=\alpha$, we have that for all $\epsilon>0$ there exists $N\in\mathbb{N}$ such that 
$$|(a_{n+1}-a_n) - \alpha| < \epsilon \quad \forall n>N.$$
We wish to show that for all $\epsilon>0$ there exists $M\in\mathbb{N}$ such that 
$$\left|\frac{a_n}{n} - \alpha \right| < \epsilon \quad \forall n>M.$$
So,
\begin{align*}
\left|\frac{a_n}{n} - \alpha \right| &= \left|\frac{a_n}{n} - \frac{a_{n+1}}{n} + \frac{a_{n+1}}{n} - \alpha \right| \\
&= \left| \frac{1}{n}(a_n - a_{n+1} - \alpha) + \frac{a_{n+1}}{n} + \sum_{j=1}^{n+1} \frac{\alpha}{n} \right| \\
\end{align*}
I tried continuing with this pattern, but I did not get anywhere.","['sequences-and-series', 'analysis', 'real-analysis']"
2905639,"A natural isomorphism $\Omega_X \to T_{X,0}^* \otimes _k \mathscr{O}_X$ on an abelian variety $X$ (and the identification of vector fields)","Let $X$ be an abelian variety over a field, $\Omega_X$ the differential sheaf, $\mathscr{T}_X$ the tangent sheaf, i.e., $\mathscr{Hom}(\Omega_X, \mathscr{O}_X)$, and $T_{X,0}$ be the tangent space at $0$ of $X$.
I want a natural isomorphism $\Omega_X \to T_{X,0}^* \otimes _k \mathscr{O}_X$.
($V^*$ is the dual space of a vector space $V$.) The proposition 1.5. in http://page.mi.fu-berlin.de/elenalavanda/BMoonen.pdf proves this, but I don't understand. He says that a vector field on $X$ (i.e., an element of $\Gamma (X, \mathscr{T}_X )$ ) can be identified with an automorphism $X_S \to X_S$ which reduces to the identity on $X$.
($S = k[x]/x^2$) I don't understand this. (This is a similar question to Making rigorous Mumford's argument about the sheaf of differentials on an Abelian Variety )","['algebraic-geometry', 'abelian-varieties']"
2905643,trigonometry axioms,I remember as a college freshman having a math teacher present a short list of simple axioms from which we could then derive all trig functions and  prove all identities.   But I cannot find this list of axioms online.   Perhaps I am using the wrong words.  Can you help me?,['trigonometry']
2905645,Definition of $H^{s}(\mathbb{R^{+}})$ and it's norm,"What's the definition of $H^{s}(\mathbb{R^{+}})$(classical Sobolev space on the half line) and its norm in terms of Fourier Transform? 
I'm aware of the definition of classical Sobolev Space $H^{s}(\mathbb{R})=\{f:(1+|\chi|^{2})^{\frac{s}{2}}\hat{f}(\chi)\in L^{2}(\mathbb{R})\}$ where $\hat{f}$ is the Fourier Transform of $f$. 
Thanks in advance!","['harmonic-analysis', 'fractional-sobolev-spaces', 'sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
2905654,Book suggestion for Differential Geometry after kreyszig.,"Preview of kreyszig's book: https://books.google.com/books/about/Differential_Geometry.html?id=B7yxgFaQKNAC&printsec=frontcover&source=kp_read_button#v=onepage&q&f=false As seen in the link, the presentations in Kreyszig's are limited to 3 dimension. Only calculus, and a bit of linear algebra, and some ODE knowledge are needed to read kreyszig's book. My purpose for studying DG is to understand General Relativity which involve 4 dimension. My math background is, calulus,123,ODE,linear algebra,basic complex integration.  I have read an introductory book on general topology,and functional analysis also by kreyszig. So far, I havn't read any text on abstract algebra. I have also briefly read Real Analysis by terrence tao. Other than all these, I have a little knowledge about PDEs and asymptotic analysis. If I want to learn some more general concepts and theorems about DG, on my level, what book do you recommend? Or perhaps, I should study Abstract algebra first?","['book-recommendation', 'differential-geometry']"
2905701,Finding a combinatorial formula for the following sequence of tables,"While studying a subject in mathematical physics and topology (which is not necessarily relevant to this question anyway), I bumped into the following sequence of tables, let's call them $M_0, M_1, M_2, M_3, M_4, \cdots$. (The table goes on, but it gets a little too big after $M_4$, so I could only write down one quarter of $M_5$.) These tables have some interesting patterns and symmetry, but a simple formula completely characterizing them is still missing. So the question is : what would be a simple (combinatorial) formula for $M_n$? Let me briefly explain the meaning of the entries. Each entry of these matrices is a multiset of integers. I used exponent for multiplicity; e.g. $6,4^2,2^2,0,-6,-8,-10$ actually means $6,4,4,2,2,0,-6,-8,-10$. 
Also in the last two tables overlined numbers $\overline{n}$ just mean $-n$. (I had to introduce this notation to save up space.) Finally, let me list some of obvious patterns you can see : $M_n$ is $(n+1)$-by-$ (n+1)$, and its entries are all even/odd depending on the parity of $\frac{n(n+1)}{2}$. $M_n$ is symmetric under flipping along the diagonal. If you flip $M_n$ either vertically or horizontally, you get $-M_n$. Let's index rows and columns of $M_n$ by numbers $0, 1, \cdots, n$. Then the $(i,j)$-entry of $M_n$ is a multiset of size $\binom{n}{i}\binom{n}{j}$. The very outer entries have a very natural combinatorial description : $(M_n)_{0,j} = \{p(1)+\cdots+p(n) \mid p\in P^{(n)}_{2j-n}\}$ as a multiset where $P^{(n)}_{m}:=\{p : \{0, 1, \cdots, n\}\rightarrow \mathbb{Z} \mid p(0)=0, p(n)=m, |p(i)-p(i+1)|=1\}$; i.e. the set of discrete paths. In other words, $(M_n)_{0,j} = \{\pm 1 \pm 2 \cdots \pm n \mid \text{there are exactly }j\text{ number of pluses}\}$ $(M_n)_{0,j}$ ""divides"" $(M_n)_{i,j}$ in a sense that $(M_n)_{i,j}$ can be expressed as a sumset of $(M_n)_{0,j}$ and some other set. For instance, for $n\geq 2$, $(M_n)_{1,1}=(M_n)_{0,1}+\{2n, 2n-2, \cdots, 6, 4, -2n\}$. Hopefully, we can find a simple combinatorial formula for $M_n$ which makes all of the observations above manifest. Added 9/5 : I just realized that the pattern becomes so much clearer once you divide every column by the first column. From this I was able to figure out that if we represent each entry of the form $\{r_1, \cdots, r_k\}$ by the polynomial $q^{r_1/2}+\cdots+q^{r_k/2}$, 
$$(M_n)_{i,j} = q^{-\frac{n(n+1)}{4}}\cdot\left(\text{coefficient of }x^iy^j\text{ in }\prod_{k=1}^{n}{(1+q^k x)}\prod_{k=1}^{i}{(1+q^{k-n-1}y)}\prod_{k=i+1}^{n}{(1+q^k y)}\right)$$
This is nice, but still not completely satisfactory, as it doesn't seem quite clear from this formula that $(M_n)_{i,j}=(M_n)_{j,i}$. Hence my question becomes : Is there a nice symmetric formula for this?","['pattern-recognition', 'puzzle', 'combinatorics', 'generating-functions']"
2905737,"What does this alphabet,$\mathbb C$, mean?","I see this representation in the paper,but i don't know what does this alphabet mean,neither the paper said. The sentence about this says: The energy-carrying information signal is denoted as $\mathbf s_B \in \mathbb C^{N_B} $with covariance matrix $\mathbf W_B = E[\mathbf s_B\mathbf s_B^H ]\in \mathbb C^{N_B \times N_B}$. Does anyone know that what do $\mathbb C^{N_B \times N_B}$ and $\mathbb C^{N_B} $ mean?","['matrices', 'notation']"
2905746,Can a relation be transitive when it is symmetric but not reflexive? [duplicate],"This question already has answers here : Example of a relation that is symmetric and transitive, but not reflexive [duplicate] (7 answers) Examples and Counterexamples of Relations which Satisfy Certain Properties (2 answers) Closed 5 years ago . Pretty much what the title asks. But here's some context: Suppose $X$ is finite and $R$ is a relation on $X$ that is not reflexive but it is symmetric. Also, suppose we can rule out $xRy$ and $yRz$ both occurring $\forall x,y,z\in X$ s.t. $x\neq y$ and $y\neq z$ and $x\neq z$. So we can rule out that $xRy$ and $yRz$ when $x,y,z$ are distinct . At this point, it seems that $R$ might be vacuously transitive. However, since $R$ is symmetric, $xRy$ and $yRx$ are both fine. But since $R$ is not reflexive, we cannot have $xRx$, which seems to be a violation of transitivity if $xRy$ and $yRx$. Is this a ""legitimate"" counter-example?","['elementary-set-theory', 'relations']"
2905747,Finding the minimum degree of a representation of an algebra,"I'm reading this paper, and in it the authors make the following claim (Eq. 27 in the paper): They first show that a certain subspace of $\mathbb{C}^N$ is invariant under a set of unitary operators $\{Z_i, Y_i\}$, $i=1,...,\alpha$, satisfying $[Z_i,Z_j]=0$ $[Y_i,Y_j]=0$ $[Y_i,Z_j]=0\quad$ provided $i\neq j$ $Y_iZ_i=e^{2\pi i p/q}Z_iY_i\qquad$  ($p/q$ is a fraction in lowest form) They go on to say the algebra generated by the $\{Z_i,Y_i\}$ can be represented in this subspace; in other words, by restricting the $Z_i,Y_i$ to act only on this subspace, we can get a lower-dimensional representation of the algebra. That makes sense. They then say that this representation must have degree at least $q^{\alpha}$, where $q$ is the denominator in the fourth bullet point. I don't understand how they reach this conclusion. How can we conclude from the relations between the generators that we cannot have a smaller complex representation than degree $q^\alpha$? Is there a method in general to find the minimal degree of a representation from some presentation of an algebra?","['group-theory', 'abstract-algebra', 'representation-theory']"
2905772,How to find $\lim_{x \to 0}\frac{1-\cos(2x)}{\sin^2{(3x)}}$ without L'Hopital's Rule.,"How would you find $\displaystyle\lim_{x \to 0}\frac{1-\cos(2x)}{\sin^2{(3x)}}$ without L'Hopital's Rule? The way the problem is set up, it makes me think I would try and use the fact that $\displaystyle\lim_{x \to 0}\frac{1-\cos(x)}{x}=0$ or $\displaystyle\lim_{x \to 0}\frac{\sin(x)}{x}=1$ So one idea I did was to multiply the top and bottom by $2x$ like so: $\displaystyle\lim_{x \to 0}\frac{1-\cos(2x)}{\sin^2{(3x)}}\cdot\frac{2x}{2x}$. Then I would let $\theta=2x$: $\displaystyle\lim_{\theta \to 0}\frac{1-\cos(\theta)}{\sin^2{(\frac{3}{2}\theta)}}\cdot\frac{\theta}{\theta}$ which would let me break it up: $\displaystyle\lim_{\theta \to 0}\frac{1-\cos(\theta)}{\theta}\cdot \frac{\theta}{\sin^2(\frac{3}{2}\theta)}$ So I was able to extract a trigonometric limit that is zero or at least see it. The second part needed more work. My immediate suspicion was maybe the whole limit will go to zero, but when checking with L'Hopital's Rule, I get $\frac{2}{9}$..... :/","['limits', 'calculus', 'limits-without-lhopital']"
2905800,Solution to one-dimensional Wave Equation with Method of Characteristics,"Consider the Problem. Let $\gamma$ be a parameterized curve in $\mathbb{R}^{2}$ by $\gamma: I \to \Omega$, where $I$ is an interval of $\mathbb{R}$ and $\Omega$ an open in $\mathbb{R}^{2}$. Let $a,b,c: \Omega \to \mathbb{R}$ be given functions. Determine a function $\varphi(x,y)$ solution of the equation
  $$a(x,y)\frac{\partial \varphi}{\partial x} + b(x,y)\frac{\partial \varphi}{\partial y} = c(x,y) \tag{Eq 1}$$
  where $\varphi(\gamma(t)) = \varphi_{0}(t)$ with $\varphi_{0}: I \to \mathbb{R}$ is a given function. Proof. (just a loosely idea) Fixed a point $\gamma_{0} = \gamma_{0}(s_{0}) = \gamma_{0}(x_{0},y_{0})$ of $\gamma$, consider the curve $\Gamma(t) = (x(t),y(t))$ passing through $\gamma_{0}$, that is, $\Gamma(0) = \gamma_{0}$. Define $z(t) = \varphi(x(t),y(t))$ where $\varphi$ is a solution of Eq 1. If $\Gamma$ is differentiable, by the Chain Rule,
$$\frac{dz}{dt} = \langle \Gamma'(t), \nabla\varphi(\Gamma(t)) \rangle = \frac{dx}{dt}\frac{d\varphi}{dx} + \frac{dy}{dt}\frac{d\varphi}{dy}.$$
Therefore, if $\Gamma$ satisfies the system of ODE
$$\begin{cases}
\frac{dx}{dt} = a(x,y),&x(0) = x_{0}\\
\frac{dy}{dt} = b(x,y),&y(0) = y_{0},
\end{cases}\tag{Sy 1}$$
we can the solution $\varphi$ solving
$$\frac{dz}{dt} = c(x,y),\quad z(0)=\varphi(s_{0}).$$
If we repeat the previous argument for all points $\gamma(s)$, $s \in I$, we obtain a family of curves on which the solution $\varphi$ can be determined. The solutions of the Sy 1 define a change of variables, that is, a function
$$f: \mathbb{R}^{2} \to \mathbb{R}^{2}$$
$$(t,s) \mapsto (x,y)$$
and the Inverse Function Theorem ensures a solution if $(a,b)$ is transversal to curve $\gamma$ where $(a,b)$ represents $(x,y) \mapsto (a(x,y),b(x,y))$. My objective is to apply the Method of Characteristics (the Problem ) to solve the one-dimensional linear Wave Equation that is given by $$\frac{\partial^{2} u}{\partial^{2} x} - c_{0}^{2}\frac{\partial^{2} u}{\partial^{2} y} = 0$$ and we write
$$\frac{\partial^{2} u}{\partial^{2} x} - c_{0}^{2}\frac{\partial^{2} u}{\partial^{2} y} = \left(\frac{\partial }{\partial x} + c_{0}\frac{\partial }{\partial y}\right)\left(\frac{\partial }{\partial x} - c_{0}\frac{\partial }{\partial y}\right)u = 0.$$
Let
$$\left(\frac{\partial }{\partial x} - c_{0}\frac{\partial }{\partial y}\right)u = v(x,y) = v.$$
Then it's enough to solve $$\underbrace{\frac{\partial u}{\partial x} - c_{0}\frac{\partial u}{\partial y} = v}_{(1)}\quad\text{and}\quad\underbrace{\frac{\partial v}{\partial x} + c_{0}\frac{\partial v}{\partial y} = 0}_{(2)}$$ I broke the Wave Equation in (1) and (2) because the Problem is only for order $1$, since the partial  derivatives are of order $1$. Then I think that solving the two equations, I'll find a solution for Wave Equation. This is the way that I found to use the Problem . Now, I want to use the Problem for solve (2) and so, use the solution for (2) to solve (1). I couldn't apply the above problem to get a solution to the equations (1) and (2). Hans Lundmark gave me a good reference, but in it, the author does the construction with some different details and my professor wants me to use the problem, exactly as it is, to get the solution. I wish someone could help me with this.","['partial-differential-equations', 'wave-equation', 'real-analysis']"
2905827,Do Maxwell's equations (generalized) apply to _every_ $k$-form on a pseudo-Riemannian manifold?,"Given a pseudo-Riemannian $n$-manifold and a $k$-form $F$ on the manifold, I will call its exterior derivative $J=dF$ the source of $F$ and the differential $K=dG$ the dual source of $F$, where $G=​{\star}F$ is the Hodge dual of $F$.  Since $d\circ d=0$, the conservation laws $dJ=0$ and $dK=0$ follow. (EDIT: This is an obvious generalization of Maxwell's equations to arbitrary $n$, $k$ and metric tensor.) Specialized to $n=4$ and $k=2$, and identifying $F$ as the electromagnetic 2-form (or rather, its dual), $J$ as the electric current 3-form and $K$ as the magnetic current 3-form, I have arrived at Maxwell's equations in general relativity as necessarily applying, including the associated conservation of electric and magnetic charge: $J=dF$ (the electric current 3-form is the source of the field) $K=d{\star}F$ (the magnetic current is the dual source of the field, though we observe that $K=0$) $dJ=0$ (conservation of electric charge) $dK=0$ (conservation of magnetic charge) using standard mathematics from minimal assumptions: We assume a classical setting, as for general relativity, but generalized to any number of dimensions $n$. We assume that the field of interest can be described as a $k$-form. My approach involves defining what the ""source"" and ""dual source"" of a field are, and then to show that these are what we call ""charge"", rather than the other way around.  Assuming that I have not made an error, the answer to the question in the title is yes (with a field's source being a $(k+1)$-form, and its dual source an $(n-k+1)$-form.  My question would then be: Why do we learn about electromagnetism as the consequence of innumerable observations (four different observational laws), rather than as the only possible form of the law aside from the non-existence of magnetic charge once we establish that the electromagnetic field is a $2$-form (which presumably follows from its transformation law established by measurement of the Lorentz force and special relativity)? I have not needed to invoke anything like gauge theory, potentials, Lagrangians, symmetry – only standard theorems of differential geometry and Hodge theory.","['hodge-theory', 'electromagnetism', 'differential-forms', 'differential-geometry']"
2905831,Why does A -> A or B,"Here's the problem. I have to prove that ((A or B') and (B' or C) -> ((B' or A) and (B' or C)) is always true no matter what A, B or C are. Which is easy, because I know (A -> A) is the same as (A' or A) and since the above statement is equal to (A -> A) (if you changed the long statements into A's) so you can change the above statement into (((A or B') and (B' or C)))' or ((B' or A) and (B' or C)) . And I know that (A' or A) is always true so that means that the above statement is always true. So why am I here? well, to get to ((A or B') and (B' or C) -> ((B' or A) and (B' or C)) the thing I have to prove is always true, I first have to get (A and (B' or C) -> ((B' or A) and (B' or C)) to be equivalent to that above statement. And the only way to make those to statement equivalent is if you add an B' and an or to the A. Which there just so happens to be a proof that states, A -> A or B. Long story short, can someone give me the proof that states why A -> A or B?","['elementary-set-theory', 'proof-verification']"
2905839,approximation to standard normal distribution,"I have a sequence of independent, but non-identically distributed Bernoulli random variables $X_i$'s taking on value $1$ with probability $p_{i1}\cdot p_{i2}$, where $i=1,\ldots,n$. Let $X=\sum_{i=1}^n X_i$. Using  Lyapunov central limit theorem, I have approximated the probability $\Pr[X \geq k]$ by standard normal distribution. However, for some values of $k$ the approximated value is not close to the exact value. I wanted to ask that why this happens?
Thanks in advance for your help.","['statistics', 'central-limit-theorem', 'normal-distribution']"
2905844,Beta distribution with parameters $\alpha = \beta \to 0$ is Bernoulli distribution,"In the article https://en.wikipedia.org/wiki/Beta_distribution#Symmetric_(α_=_β) it is said that a Beta distribution with parameters $\alpha = \beta \to 0$ has a Bernoulli distribution with probability $p=0.5$ at $0$ and $1$. Formally, does this mean that the sequence $\text{Beta}(1/n, 1/n)$ with $n \in \mathbb{N}$ converge to $\text{Bernoulli}(0.5)$ and what kind of convergence it is (distribution, a. e.)? How to prove this fact? Beta density function when $\alpha = \beta$ is: $$f_X(x)=\frac{x^{\alpha -1} (1-x)^{\beta -1}}{B(\alpha, \beta)} = \frac{\Gamma(2\alpha)\;x^{\alpha -1} (1-x)^{\alpha -1}}{\Gamma(\alpha)^2}$$ then I need to find $$\lim_{\alpha^+ \to 0}\frac{\Gamma(2\alpha)\;x^{\alpha -1} (1-x)^{\alpha -1}}{\Gamma(\alpha)^2}$$ where $\lim_{\alpha^+ \to 0}$ is the limit from the right since the parameters for the Beta distribution are real positive numbers. We know that $\lim_{\alpha^+ \to 0} \Gamma(\alpha) = +\infty$ and for every $x \in \mathbb{R}$ $x^{-1}$ and $(1-x)^{-1}$ is another real number, then we have a limit of the form $\infty / \infty$ and we can use L'Hopital, but I dont know how to use it with the $\Gamma$ function and how to conclude that the new function is a density function of Bernoulli distribution.","['gamma-function', 'beta-function', 'probability-distributions', 'probability']"
2905856,Showing the following set is a $\sigma$ algebra. Proof verification.,"Let $M$ be an infinite $\sigma$ algebra on X. Show that if $A \in M$, then the collection $M_A$={$B \cap A: B \in M$} is a $\sigma$ algebra on A. My thoughts/attempt (Really struggling) To show $M_A$ is a $\sigma$-algebra, I need to show that $M_A$ is closed under complements and countable unions. I'm really confused on how to show either but here is my attempt anyways. First I show finite unions: Let $B_1 \cap A$ $\in M_A$ and $B_2 \cap A$ $\in M_A$ (thus $A$, $B_1$ and $B_2 \in M$). $(B_1 \cap A)$ $\cup$  $(B_2 \cap A)$=($B_1 \cup B_2)$ $\cap$  $A$ $\in M_A$ $(B_1 \cup B_2 \in M$ as M is a $\sigma$ algebra). Thus we closed under finite union. Can do the same process to show for countable union. Second: I show closure under complements: Let $B_1 \cap A$ $\in M_A$. Need to show that $(B_1 \cap A)$$^{c}$ $\in$ $M_A$. I can write $(B_1 \cap A)$$^{c}$ = ($B_1 \cap A^C$) $\cup$ ($B_1^C \cap A$). $B_1 \cap A^C \in M_A$ as $A^c \in M$ as $M$ is a $\sigma$ algebra. $B_1^c \cap A \in M_A$ as $B_1^c \in M$ as $M$ is a $\sigma$ algebra.
Thus, $(B_1 \cap A)$$^{c}$ $\in M_A$ as it is the union of two things in $M_A$ and we previously showed that $M_A$ is closed under finite union. Is my proof correct? Or am I very off in my approach? Thank you.","['measure-theory', 'analysis', 'real-analysis', 'elementary-set-theory', 'general-topology']"
2905869,"Given $\sin(t) + \cos(t) = a$, derive an expression in '$a$' for $(\cos(t))^4 + (\sin(t))^4$","I received this question from a student's trigonometry review assignment. After spending an embarrassing amount of time on it, I consulted others and learned that nobody has been able to solve this problem for multiple semesters. I wonder if there is a typo in the statement or if I just haven't been rigorous enough?  Here is the question: $$\text{Given } \sin{t} + \cos{t} = a, \text{ find an equivalent expression for } \sin^4{t} + \cos^4{t} \text{ in terms of } a.$$ Has anybody seen this one before? I tried (among other things) this (which is an approximation of an earlier attempt): $(\sin{t} + \cos{t})^4$ and using whatever identities I could remember; $(\sin{t} + \cos{t})^4 = \sin^4{t} + \cos^4(t) + 4\sin{t}\cos^3{t} + 6\sin^2{t}\cos^2{t} + 4\sin^3{t}\cos{t}$ so then $\begin{align}
\sin^4{t} + \cos^4{t} &= (\sin{t} + \cos{t})^4 - 4\sin{t}\cos^3{t} - 6\sin^2{t}\cos^2{t} - 4\sin^3{t}\cos{t}\\
&= (\sin{t} + \cos{t})^4 - 2\sin{t}\cos{t}(2\cos^2{t} + 3\sin{t}\cos{t} + 2\sin^2{t})\\
&= (\sin{t} + \cos{t})^4 - 2\sin{t}\cos{t}(3\sin{t}\cos{t} + 4)\\
&= a^4 - 2\sin{t}\cos{t}(3\sin{t}\cos{t} + 4)
\end{align}$ Couldn't get further than this, felt like I was overthinking it.","['trigonometry', 'calculus', 'algebra-precalculus']"
2905875,condition for a matrix to be pseudo-hermitian,"As we know that a matrix $H$ is said to be Hermitian if $H=H^\dagger$, and if so, then all the eigenvalues are real. While a non-hermitian matrix $P$ is said to be pseudo-hermitian if $ \eta P \eta^{-1} =P^\dagger$, ($\eta$ is some constant metric), in this, eigenvalues may also be real. Could anyone please throw some light that - from where this condition of pseudo-hermiticity  $ \eta P \eta ^{-1}=P^\dagger$, comes??","['matrices', 'matrix-calculus', 'linear-algebra']"
2905895,Deciding which statements about matrix A is true where $A^3+A^2-3A+I=0$,"A is $2 \times 2$ matrix. $$A^3+A^2-3A+I=0$$ Decide which statements is true. $\quad$ A) 1 is eigenvalue of A. $\quad$ B) Det(A) is 1. $\quad$ C) $A^{-1}$ exists. $\quad$ D) If B is inverse of A, $B^3-3B^2+B+I=0$ . Choices are {A,B}, {A,C}, {C,D}, and {B,C,D}. What I've did so far is, A : If I multiply eigenvector v $_{(2 \times 1)}$ to given equation, It'll satisfy the equation if eigenvalue of A is 1. $\quad$ But I'm not sure if it's enough to say statement A is true. B: (?) C: $A(-A^2-A+3)=I$ , so it's true. D: $B=-A^2-A+3. $ $\quad B^3-3B^2+B+I=0=B(B^2-3B+1)+I\;$ , If I substitute B then $\quad =(-A^2-A+3)(A^4+2A^3-2A^2-3A+1)=-A^3-A^2+3A=I$ . $\quad$ So it's true. Have I done correctly? and How should I go for statements A and B?",['linear-algebra']
