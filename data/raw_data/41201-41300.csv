question_id,title,body,tags
406435,"Suppose that characteristic $F$ is $p$. If $K/F$ is separable then $K = F(K^{p})$ where $K^{p} = \{ x^{p} \, |\, x\in K\}$.","I am having difficulty finishing this problem.  So far I have this: Want to show $K \subset F(K^{p})$.  Since $K/F$ is separable then $K/F$ is algebraic.  In particular, $\alpha\in K$ is separable over $F$ hence separable over $F(\alpha^{p})$.  So $\alpha\in F(\alpha^{p})$ by an exercise already done.  But then I do not know how to conclude $\alpha\in F(K^{p})$. Any suggestions?  Or is my approach completely wrong?","['abstract-algebra', 'field-theory']"
406443,How to prove $\lvert \lVert x \rVert - \lVert y \rVert \rvert \overset{\heartsuit}{\leq} \lVert x-y \rVert$?,I'm trying to show that $\lvert \lVert x \rVert - \lVert y \rVert \rvert \overset{\heartsuit}{\leq} \lVert x-y \rVert$. A hint would be nice.,"['vector-spaces', 'normed-spaces', 'linear-algebra', 'inequality']"
406446,Cylinder-ray intersections equation,"I found an article involving infinite cylinder-ray intersections, and I don't know how they develop this equation: $$(q - p_a - (v_a, q - p_a)v_a)^2 - r^2 = 0$$ In the end of the first page I quote: Infinite cylinder along $y$-axis of radius $r$ has equation
  $x^2 + z^2 - r^2 = 0$.
  The equation for a more general
  cylinder of radius $r$ oriented along
  a line $p_a + v_at$: $(q - p_a - (v_a, q - p_a)v_a)^2 - r^2 = 0$
  where $q = (x,y,z)$ is a point on the cylinder.","['mathematical-physics', 'geometry', 'computational-mathematics']"
406450,Lattice ordered group,It seems the basic fact but I did not get the idea how to prove it. Let $G$ be a partially ordered group. Then $G$ is an $\ell$-group if and only if every pair of positive elements has least upper bound.,"['lattice-orders', 'group-theory']"
406460,How does the symmetric group act on tuples?,"Given a set $X$, I've seen people write the action of a permutation $ \sigma \in S_{n}$ on an $n$-tuple of elements of $X$ as
$$ \;\; \sigma (x_{1},...,x_{n})=(x_{\sigma^{-1}(1)},...,x_{\sigma^{-1}(n)}). \;\;$$ 
But this does not seem to me to give a left action of $S_{n}$ on $n$-tuples, since
$$\sigma(\tau(x_{1},...,x_{n}))=\sigma(x_{\tau^{-1}(1)},...,x_{\tau^{-1}(n)})=(x_{\sigma^{-1}\tau^{-1}(1)},...,x_{\sigma^{-1}\tau^{-1}(n)})$$
is not in general equal to
$$(\sigma \tau)(x_{1},...,x_{n})=(x_{(\sigma \tau)^{-1}(1)},...,x_{(\sigma \tau)^{-1}(n)})=(x_{\tau^{-1}\sigma^{-1}(1)},...,x_{\tau^{-1}\sigma^{-1}(n)}).$$ On the other hand, if we don't write out symbols, but think of a tuple as a map from 
the $n$-element set $\{1,...,n\}$ to $X$, then the first formula seems to make sense:
the $\sigma^{-1}$ comes from precomposing with the right action of $S_{n}$ on the set $\{1,...,n\}$ determined by the left action of $S_{n}$ on $\{1,...,n\}$, where we use the convention that functions act on the left of elements. So the question is whether the first formula defines a left or a right action on tuples. 
The symbols seem to suggest a right action while interpreting tuples as maps seems to suggest a left action. I think that in my formulas above I must be getting confused.","['group-theory', 'symmetric-groups', 'combinatorics']"
406475,What happens to small squares in Riemann mapping?,"I have a square $S$, and I want to convert it to the unit disc $D$. The Riemann mapping theorem says that I can to it with a conformal bijective map. But, any such mapping will cause some distortion. Specifically, if S contains small sub-squares, they will be mapped into sub-shapes of C that are not squares. The amount of distortion depends on the specific mapping selected, and also on the placement of the small square inside S (see, for example, this nice illustration : the distortion is minimal near the center, and maximal near the corners). The amount of distortion can be quantified in the following way: Let $s$ be a small sub-square contained in $S$. Let $d(s)$ be its image under the conformal mapping ($d(s)$ is contained in $D$). Let $m(d(s))$ be the maximum-area square that is contained in $d(s)$. Define: $\mathrm{distortion}(s) = \mathrm{area}(d(s)) / \mathrm{area}(m(d(s))) - 1$ So, if $s$ is mapped to a square, then $d(s)$ is a square, $m(d(s))=d(s)$, and $\mathrm{distortion}(s)=0$. I would like to know: What is the maximum distortion of a square of a certain size? What is the average distortion, over all squares of a certain size? If we divide $S$ into a k-by-k grid of $k^2$ sub-squares, what is the average distortion over all $k^2$ sub-squares?","['differential-geometry', 'conformal-geometry', 'rectangles', 'complex-analysis']"
406496,Last digits of a power of 2,"Prove that there exists a power of 2 such that the last 1000 digits in its decimal representation are all 1 and 2. One fact that I think can be used in this problem: if $2^{n}=\cdots dn$ where $d$ is the digit to the left of $n$, then $2^{dn}=\cdots dn$ (A concept that was used in MMO 1978). Furthermore I have a feeling that reducing $n$ to its binary or ternary base may be of some help. If one feels that the question is wrong then please mention why this is never possible. Thanks!","['decimal-expansion', 'contest-math', 'number-theory']"
406550,A beautiful limit involving primes and composites,"I observed the following limit empirically. Let $p_n$ be the $n$-th prime and $c_n$ be the $n$-th composite number then, $$
\lim_{n \to \infty}\frac{1}{n}\sum_{i=1}^{n}\frac{p_n c_n}{p_n c_n + p_i c_i} = \frac{\pi}{4}.
$$ I am looking for a proof.","['prime-numbers', 'limits', 'real-analysis', 'number-theory']"
406552,What does square brackets around a polynomial mean?,"EDIT: The first paragraph has been indicated as inaccurate, please see this answer . When we have something like $\mathbb{Z}_2[x]/(x^2 + x + 1)$, we
understand that this means the set of polynomials over indeterminate
$x$ where the coefficients are drawn from $\mathbb{Z}_2 = \{0,1\}$ with
degree less than $\text{deg} (x^2 + x + 1)$. So, in this set we can include polynomials like $x$, $x+1$ and 
even $1$, but not $x^2+1$ or $2x+1$. This is okay so far with me. But in some lecture note ( Rings and fields,
Sergei Silvestrov,
Spring term 2011, Lecture 5 ; page 3, Example 1
and subsequent ones) I find representations like,
$\mathbb{Z}_2[x]/(x^2 + x + 1) = \{[0], [1], [x], [x + 1]\}$. I fail to understand why the square brackets have been put around
the polynomials. Answer to this question says that this depends on the context and
may be same as parentheses or may mean braces to represent sets. Your help will be appreciated.","['discrete-mathematics', 'polynomials']"
406559,What is wrong with this fake proof that $\lim\limits_{n\rightarrow \infty}\sqrt[n]{n!} = 1$?,"$$\lim_{n\rightarrow \infty}\sqrt[n]{n!}=\lim_{n\rightarrow \infty}\sqrt[n]{1}*\sqrt[n]{2}\cdots\cdot\sqrt[n]{n}=1\cdot1\cdot\ldots\cdot1=1$$
I already know that this is incorrect but I am wondering why. It probably has something to do with the fact that multiplication in $n!$ is done infinite number of times.","['sequences-and-series', 'calculus', 'fake-proofs', 'limits']"
406568,Dual space of $H^1(\Omega)$,"I'm a bit confused, why do people not define $H^1(\Omega)^*$? Instead they only say that $H^{-1}(\Omega)$ is the dual of $H^1_0(\Omega).$ $H^1(\Omega)$ is a Hilbert space so it has a well-defined dual space. Can someone explain the issue with this?","['sobolev-spaces', 'functional-analysis']"
406576,if $\log_xa+\log_ya=4\log_{xy}a$ prove that $x=y$,"Let $x,y$ be numbers in the interval $(0,1)$ with the property that there exists a positive number $a\ne1$ such that 
$$\log_xa+\log_ya=4\log_{xy}a$$
I have used the property
$$\log _ba=\frac{\log a}{\log b}$$ and I have
$$\log_xa+\log_ya=\frac{\log a}{\log x}+\frac{\log a}{\log y}$$ $$\log a\left(\frac{\log x+\log y}{\log x \log y}\right)=4\frac{\log a}{\log(xy)} \implies (\log(xy))^2=4 \log x \log y$$ I don't see how this implies $x=y$","['logarithms', 'algebra-precalculus']"
406586,Characterizing multisets that are the sum of two permutations,"Suppose you have a permutation $\pi$ of $1 \ldots n$. The multiset $\{m_i\}$ = $\{ i + \pi(i) \}_i$ is easily seen to have the following properties: $\sum_j m_j = n(n+1)$. If the elements are ordered so $m_1 \leq m_2 \leq \ldots \leq m_n$, then $\sum_{j=1}^k m_j \geq k(k+1)$. For example, if the permutation $\pi$ is 5 2 3 1 4, the multiset is $\{4,5,6,6,9\}$ 1 2 3 4 5 
+ 5 2 3 1 4
= 6 4 6 5 9 Are these two properties sufficient? That is, for every multiset satisfying these two properties, is there a permutation $\pi$ that generates it? This is a question that I have thought about in the past, and I was reminded of it by this related cs.SE question. EDIT: and if you look at the linked cs.SE question, I give a link that shows the problem of determining whether a multiset is the sum of two permutations is NP-complete.",['combinatorics']
406616,Mean value theorem for essentially bounded functions,"I have the problem with the following: Let $f \in L^{\infty}(\mathbb{R}_+)$ is the mean value theorem in the following form: Let $0 \leq a < b < \infty$ , then $\int_{a}^{b} |f| \ d\mu = m(b-a)$ , for some $m \leq \mathrm{ess} \sup |f|$ . valid for $f$ ? We can see that $$\int_{a}^b |f| \ d\mu \leq \mathrm{ess} \sup |f|(b-a)$$ thus $f \in L^1([a,b])$ . Then $\mathrm{ess} \inf |f| = - \mathrm{ess} \sup |f|$ , so $$\mathrm{ess} \inf |f|(b-a) \leq \int_{a}^b |f| \ d\mu.$$ Thus the intermidiate value theorem applied to $|f|$ shows that there exists $x \in [a,b]$ such that $|f(x)| \in [\mathrm{ess} \inf |f|,\mathrm{ess} \sup|f| ]$ and $$ \int_{a}^b |f| \ d\mu = |f(x)| (b-a) .$$ Is it correct? How about the following: Let $0 \leq a < b < \infty$ ,  does it exist a complex number $m$ such that $\int_{a}^{b} f \ d\mu = m(b-a)$ , for some $|m| \leq \mathrm{ess} \sup |f|$ . I think for that we could use that $f= \mathrm{Re}f + i \mathrm{Im} f$ and then apply thesis from the first question (if it is true).
Thank you for any help.","['real-analysis', 'analysis']"
406635,Helly's Theorem for Biconvex Sets,"Helly's Theorem states the following. Suppose that $X_1,X_2,...,X_N$ are convex sets in $\mathbb{R}^d$, such that for any index-set $I$ with $|I| \leq h(d) := d+1$, we have $\bigcap_{i \in I} X_i \neq \varnothing$. Then $\bigcap_{i=1}^N X_i \neq \varnothing$. If instead $X_1,X_2,...,X_N$ are biconvex sets in $\mathbb{R}^d$, what is ""Helly's number"" $h(d)$ (or an upper bound of it)? Possible hint. Let $\mathcal{X} = \{X_1, ..., X_N\}$ such that the intersection of any its subfamily of size at most $k$ can be expressed as a disjoint union of $k$ closed convex sets. Then $h \leq k(d + 1)$. What is $k$ for biconvex sets? However, I found that the intersection of $2$ biconvex sets may be the union of an unbounded number of convex sets. So probably this hint is not useful. Comment. A set $S \subseteq \mathbb{R}^{n} \times \mathbb{R}^m = \mathbb{R}^d$ is biconvex if for all $y \in \mathbb{R}^m$ the set $S_y := \{ x \in \mathbb{R}^n \mid (x,y) \in S \}$ is convex, and for all $x \in \mathbb{R}^n$ the set $S_x := \{ y \in \mathbb{R}^m \mid (x,y) \in S \}$ is convex as well.","['geometry', 'algebraic-geometry', 'discrete-geometry']"
406641,Why is the integral $C^\infty$,"I am reading Differentiable manifolds from Warner. In order to prove that the dimension of the tangent space is the same as the dimension of the manifold, they use the following calculus lemma - If $g$ is of class $C^k$ ($k \geq 2$) on a convex open subset $U$ about $p$ in $\mathbb{R}^d$, then for each $q \in U$,
$g(q)\ =\ g(p) + \sum_{i=1}^d \frac{\partial g}{\partial r_i}|_p (r_i(q)-r_i(p))+\sum_{i,j}(r_i(q)-r_i(p))(r_j(q)-r_j(p))\int_0^1(1-t)\frac{\partial^2g}{\partial r_i\partial r_j}|_{(p+t(q-p))} dt.$ This is the Taylor expansion. It further says, if $g\in C^\infty$, then the integral as a function of $q$ is of class $C^\infty$. How is this? Do we have to use fundamental theorem of calculus or something like that?",['differential-geometry']
406652,How to test the convergence of the series $\sum_{n=1}^\infty n^{-1-1/n}$?,How to test the convergence of the series $\displaystyle\sum_{n=1}^\infty\frac{1}{n^{1+1/n}}?$ Help me. I'm clueless.,"['convergence-divergence', 'sequences-and-series', 'real-analysis']"
406655,Properties of an alternating bilinear form its coordinate matrix,"I found that I lack many basic knowledge about linear algebra, so read the wiki article about Bilinear Forms. Especially this Paragraph . I tried to proof of ""Every alternating form is skew-symmetric."" was quite easy. And I found a counter-example for the inverse if $char(F) = 2$. However, I am currently trying to find a proof for this: A bilinear form is alternating if and only if its coordinate matrix is skew-symmetric and the diagonal entries are all zero I looked at the following equations, to understand that. $\begin{pmatrix}v_1 & v_2\end{pmatrix} \cdot \begin{pmatrix}a & b \\ c & d\end{pmatrix} \cdot \begin{pmatrix}v_1 \\ v_2\end{pmatrix} = 0$ $\begin{pmatrix}v_1 & v_2\end{pmatrix} \cdot \begin{pmatrix}a & b \\ c & d\end{pmatrix} \cdot \begin{pmatrix}w_1 \\ w_2\end{pmatrix} = - \begin{pmatrix}w_1 & w_2\end{pmatrix} \cdot \begin{pmatrix}a & b \\ c & d\end{pmatrix} \cdot \begin{pmatrix}v_1 \\ v_2\end{pmatrix}$ And they are both solvable for $a=0, d=0, c=-b$, thus the matrix $\begin{pmatrix}0 & b \\ -b & 0\end{pmatrix}$. However, I am not any nearer to a proof. Can someone please point me into the right direction?","['matrices', 'linear-algebra', 'bilinear-form']"
406660,Find the rational points on $1 + 18 x + 81 x^2 + 44 x^3 = y^2$ with Sage,"I'm trying to use Sage on-line,but I meet some trouble with the code of it.
I want to find the rational points on an ellipse curve,such as $$1 + 18 x + 81 x^2 + 44 x^3 = y^2,\tag1$$
I know that $(x,y)=(0,1)(1,\pm12)(-\frac{1}{11},0)$ are on the curve,but I don't know how to find more,so I try to solve it with the math-software Sage. What should I input?Thanks in advance! Edit: Multiply by $44^2$,we can difine the ellipse curve by this way, E = EllipticCurve([0,81,0,44*18,44^2]) Then how to find the rational points on it?","['number-theory', 'sagemath', 'elliptic-curves', 'math-software', 'diophantine-equations']"
406667,continuous function on a set of lines through a point,"This came up in class once: Suppose we have a set $R \subset \mathbb{R}^2$ and $x \in
 \mathbb{R}^2$. If we define a function $A$ on all the
  lines $l$ going through $x$ such that $A(l)$ gives how much of the area
  of $R$ is ""above"" (or to the right left of) the line $l$, must
  $A$ be continuous? I do think it is continuous. If we take a horizontal line through $x$ and rotate it by $\theta$, we have a continuous function giving the area of the part of $R$ whose pre-image (under this rotation) is the part of $R$ initially under the horizontal line. However, I'm at loss how to rigorously show continuity using pre-image of open sets is an open set. In particular, how should an open set be defined on the set of lines through $x$, and thus allowing me to rework a proof. EDIT: What we mean by the ``above'': For a non-vertical line $l$ passing through $x$, we rotate $S$ clockwise until $l$ becomes horizontal. Then $A(l)$ gives the area of $R$ which is above this horizontal line (the image of $l$ after the rotation). If $l$ were vertical, we take the area to the left of $l$,","['continuity', 'real-analysis', 'analysis']"
406686,$f'(x)=f(x)$ and $f(0)=0$ implies that $f(x)=0$ formal proof,"How can I prove that if a function is such that $f'(x)=f(x)$ and also $f(0)=0$ then $f(x)=0$ for every $x$. I have an idea but it's too long, I want to know if there is a simple way to do it. Thanks!
Obviously in a formal way.",['real-analysis']
406694,Every function is the sum of an even function and an odd function in a unique way,"It is known that every function $f(x)$ defined on the interval $(-a,a)$ can be represented as the sum of an even function and an odd function. However How do you prove that this representation is unique? Thanks for your help.","['functions', 'real-analysis']"
406703,How to simplify $a^n - b^n$?,"How to simplify $a^n - b^n$? If it would be $(a+b)^n$, then I could use the binomial theorem, but it's a bit different, and I have no idea how to solve it. Thanks in advance.",['algebra-precalculus']
406714,Solving $B(n)=3B(\frac{n}{\log_{2}n}) +n$ using master theorem.,"First of all sorry if this has been posted before, I found lots of master theorem questions on the search but not one like this. I am familiar with master theorem but a little uncomfortable with substitutions and I can't seem to think of one for this. I tried to unravel the recursion a bit to find a hint and found $$ 3B(\frac{n}{\log_2^nn})+\frac{n}{\log_2^{n-1}n} $$ at depth $n$ (this could be incorrect though, I don't do that often.) Can somebody give me a hint of a substitution please? Thanks","['asymptotics', 'discrete-mathematics']"
406718,Is this series convergent? $\sum_{i=1}^{\infty} \frac{(\log n)^2}{n^2}$,"$\sum_{i=1}^{\infty} \frac{(\log n)^2}{n^2}$ I guess it is convergent, so I apply comparsion test for this. $\frac{log^2}{n^2} < \frac{n^2}{n^2} = 1$ So it is bounded by 1 and hence it is convergent. Can I do in this way? It kind of make sense, but I have never deal with constant in this case Usually we do bounding using another term depend on $n$?","['sequences-and-series', 'real-analysis']"
406750,Representing a real valued function as a sum of odd and even functions [duplicate],"This question already has answers here : How do I divide a function into even and odd sections? (3 answers) Closed 11 years ago . With $f(x)$ being a real valued function we can write it as a sum of an odd function $m(x)$ and an even function $n(x)$: 
      $f(x)=m(x)+n(x)$ Write an equation for $f(-x)$ in terms of $m(x)$ and $n(x)$: My attempt using the properties - even function if: $f(x)=f(-x)$ and odd function if: $-f(x)=f(-x)$ $f(x)=m(x)+n(x)  \implies f(-x)=m(-x)+n(-x) \implies f(-x)= -m(x)+n(x) \implies   f(-x)=n(x)-m(x)$ I think that is correct but then I need to find equations for both $m(x)$ and $n(x)$ in terms of $f(x)$ and $f(-x)$ so a suggestion on how to tackle that would be great.",['functions']
406758,"Find the time span of snow plow operation, given that its speed is inversely proportional to the height of the snow","One day snow began to fall before dawn and continued to fall at a constant rate. At midday a snowplot set out to clear a road. At 2pm it turned back, arriving to the starting point at 3pm. 
  If we suppose that the snowplow speed is inversely proportional to the height of the snow, at what time did it start snowing? At what time should the snowplot turn back in order to arrive to the starting point at 2pm? Let: $x(t)$ be the position of the snowplow at time $t$. $h(t)$ the height of the snow at time $t$ We denote $\Delta V $ the volume of snow removed by the snowplow in a time $\Delta t$ small enough to suppose that $h(t)$ constant in $\Delta x$. If L is the widht of the shovel of the snowplow we have: $\frac{\Delta V}{\Delta t} = h L \frac{\Delta x}{\Delta t}$ If we take  $\Delta t \rightarrow 0$: $\frac{dV}{dt} = h L \frac{dx}{dt}$ We know that $\frac{dV}{dt} = \alpha$ is constant, so we have $\frac{dx}{dt} = \frac{\alpha}{L h}$ As snow falls at a constant rate, we have $h(t) = c t$ then: $\frac{dx}{dt} = \frac{\alpha}{L c t}$ Calling $ A = \frac{\alpha}{L c}$ we have: $\frac{dx}{dt} = \frac{A}{t}$ and then: $x(t) = A Log(t) + C$ We have $x(T) = 0 \Longrightarrow x(t) = A Log(\frac{t}{T})$ This is clearly the position of the snowplow between 12pm and 2pm. I don't know how to continue from here because in the way back h(t) isn't as simple and depends on when the snowplow went over there the first time.",['ordinary-differential-equations']
406812,How Do You Expand and Simplify?,"How do you expand and simplify $(x + 2y)^2$ or $(7x + 5y)^2$. I don't understand, how would you multiply that by itself? I mean I can simplify and expand two brackets; e.g. $(x + 3) (x + 5)$ but I'm not sure how to do one bracket with a squared.",['algebra-precalculus']
406819,Find the limit of $(\sin \frac{1}{n} \cdot \sin \frac{2}{n} \cdot ... \cdot \sin 1)^{\frac{1}{n}}$,Could you tell me how to find $\lim_{n \rightarrow \infty} (\sin \frac{1}{n} \cdot \sin \frac{2}{n} \cdot ... \cdot \sin 1)^{\frac{1}{n}}$ ?,"['convergence-divergence', 'sequences-and-series']"
406820,Minimum probability of biased coin to satisfy particular condition,"Take $n$ pairs of integers $(x_i,y_i)$,  $1\leq i\leq n$, selected independently as follows: Toss a fair coin $X$ and a biased coin $Y$ with $$\Pr[Y=\text{heads}] = p \neq 1$$ If $(X=\text{heads})$, then $x_i=-1$, otherwise $x_i=1$ If $(Y=\text{heads})$, then $y_i=0$, otherwise $y_i=1$ Find $\min(p)$ such that $\Pr[\sum_{i=1}^{n}x_iy_i=0]\geq a$ for some given $a$. (Note: not a homework question)",['probability']
406825,probability of sample variance lying between values,"Let $X_1,\ldots,X_n$ be a random sample of size $n = 10$ from a population
which is normally distributed with mean $48$ and variance $36$ . What is the probability that the sample variance of such a sample
lies between $25$ and $60$ ?","['statistics', 'normal-distribution', 'sampling', 'probability']"
406832,How to Factorise these expressions?,I'm dong some Factorisation revision and I'm a bit stuck. I've never done this before and I'm stuck a bit. First of all I know how to do small factorisations and I know you have to find the Highest Common Factor and that it's the opposite of expanding but these expressions are scaring me a bit. e.g. how would you do this $x^2 + 8x + 16$ or this $9m^2 - 24mn + 16n^2$,['algebra-precalculus']
406847,How to find the integral of implicitly defined function?,"Let $a$ and $b$ be real numbers such that $ 0<a<b$. The decreasing continuous function
$y:[0,1] \to [0,1]$ is implicitly defined by the equation $y^a-y^b=x^a-x^b.$
Prove
$$\int_0^1 \frac {\ln (y)} x \, dx=- \frac {\pi^2} {3ab}.
$$","['multivariable-calculus', 'integration']"
406850,Minimal spectral radius of a primitive matrix,"Given the set of all primitive matrices of dimensions $m$ by $m$ that are non-negative and integer - which one is the matrix with the minimal spectral radius? Edit (according to the first comment): The matrix without the red '1' is an irreducible matrix with minimal (1) eigenvalue (in order to be irreducible it must have at least one '1' in each row). This matrix is not primitive, because of the locations of the '1's. In order to get a primitive matrix, we must add at least one '1'. The matrix below is indeed primitive with only one additional '1' (proof is needed...). Addition of more '1's will result in a larger spectral radius (for non negative matrices, $A \ge B$ implies $\rho(A) \ge \rho(B)$). The question is:
how do we know that this specific location of the red '1' will give the minimum?","['matrices', 'eigenvalues-eigenvectors', 'graph-theory']"
406855,Young's inequality for three variables,"Let $x, y, z \geqslant 0$ and let $p, q, r > 1$ be such that 
$$
\frac{1}{p} + \frac{1}{q} + \frac{1}{r} = 1.
$$
How can one show that under these hypotheses we have
$$
xyz \leqslant \frac{x^p}{p} + \frac{y^q}{q} + \frac{z^r}{r}
$$
with equality if and only if $x^p = y^q = z^r$, using twice the standard two-parameters Young's inequality which says that for all $x, y \geq 0$ and for all $p, q > 1$ for which $\frac{1}{p} + \frac{1}{q} = 1$ we have
$$
xy \leqslant \frac{x^p}{p} + \frac{y^q}{q}
$$
with equality if and only if $x^p = y^q$ ? I've tried to apply it twice directly, to multiply two inequalities and to add two inequalities, but in each case it gets quite messy and I can't get the desired result, even though I'm sure it should be quite simple.","['young-inequality', 'inequality', 'real-analysis']"
406864,Intersection of two lines in vector form,"Sorry about formatting, this is my first question here, and I have no idea how to do it properly. Assuming I have two lines $\displaystyle l_1 = \binom{x_1}{y_1} + a\binom{u_1}{v_1}$ $\displaystyle l_2 = \binom{x_2}{y_2} + b\binom{u_2}{v_2}$ Thus the intersection holds $\displaystyle \binom{x_1}{y_1} + a\binom{u_1}{v_1} = \binom{x_2}{y_2} + b\binom{u_2}{v_2}$ and have made sure those two lines will intercept (so $u_1,v_1$ and $u_2,v_2$ aren't parallel), how do I find the intersection point elegantly? Currently (I am using this in a C++ code), I need to do a lot of if() cases, since I cannot divide by 0, but in order to find the point, I need to substitute b in the formula I get $a = \dfrac{x_2-x_1+bu_2}{u_1}$ so here I have to check for $u_1\neq0$, and then substitute in the second line, where I then need to check for $v_2\neq0$, and this creates one hell a lot of code. Is there a better way, am I just missing something easy?",['geometry']
406873,$f:\mathbb{R}\to\mathbb{R}$ is continuous and $\int_{0}^{\infty} f(x)dx$ exists [duplicate],"This question already has answers here : $f:\mathbb R\rightarrow \mathbb R$ be a continuous function such that $\int_{0}^{\infty}f(x)dx$ exists. (4 answers) Closed 3 years ago . $f:\mathbb{R}\to\mathbb{R}$ is continuous and $\int_{0}^{\infty} f(x)dx$ exists  could any one tell me which of the following statements are correct? $1. \text{if } \lim_{x\to\infty} f(x) \text{ exists, then it is 0}$ $2.  \lim_{x\to\infty} f(x) \text{ must exists, and it is 0}$ $3. \text{ in case if f is non negative } \lim_{x\to\infty} f(x) \text{ exists, and it is 0}$ $4. \text{ in case if f is differentiable } \lim_{x\to\infty} f'(x) \text{ exists, and it is 0}$ I solved one problem in past which says: if $f$ is uniformly continuos and $\int_{0}^{\infty} f(x)dx$ exists then $\lim_{x\to\infty} f(x)=0$, so the condition in one says $f$ is uniformly continuous? and hence $1$ is true? well I have no idea about the other statements. will be pleased for your help.","['improper-integrals', 'continuity', 'real-analysis', 'limits']"
406884,A basic intuition on a probability problem,"Two players take turns shooting at a target, with each shot by player $i$ hitting the target with probability $p_i$, $i=1,2$. Shooting ends when two consecutive shots hit the target. Let $\mu_i$ denote the mean number of shots taken when player $i$ shoots first, $i=1,2$. Now, I have calculated that $$\mu_1 - \mu_2 = \frac{p_2 - p_1}{p_1+p_2-p_1p_2-2}=\frac{q_2-q_1}{1+q_1q_2}$$ where $q_i=1-p_i, i=1,2 $ i.e. if the player with higher winning probability starts the game then the mean number of shots taken is higher than the case when the the player with lower winning probability starts the game. I don't understand the intuition behind this.","['intuition', 'probability', 'conditional-probability']"
406894,Non-trivial solutions implies row of zeros?,"If there exist non trivial solutions, the row echelon matrix of homogenous augmented matrix A has a row of zeros. True or False? I'm not sure where to begin as to see why this would be true or false. I know that if there are a row of zeros it means that there are infinitely many solutions, but not sure how I can tell if that means there are non-trivial solutions though. Any help would be appreciated. Thanks in advance.","['matrices', 'linear-algebra']"
406918,Evaluating the series $\sum\limits_{n=1}^\infty \frac1{4n^2+2n}$,"How do we evaluate the following series: $$\sum_{n=1}^\infty \frac1{4n^2+2n}$$ I know that it converges by the comparison test. Wolfram Alpha gives the answer $1 - \ln(2)$, but I cannot see how to get it. The Taylor series of logarithm is nowhere near this series.","['sequences-and-series', 'calculus', 'real-analysis']"
406959,"Is $K=\{f\mid f\in \Pi_n , \|f\|\le 1\}$ equicontinuous or not?","I am trying find that below set is equicontinuous or not: $$K=\{f\mid f\in \Pi_n , \|f\|\le 1\}$$
$$\Pi_n=\{\text{polynomials of degree }\le n \text{ over } [a,b]\}$$
with norm :
$$||f||=\sup_{x\in \mathcal{D}(f)}|f(x)|$$
I proved that if we change the condition $f\in \Pi_n$ to $f\in \mathcal{C}$ it isn't equicontinuous with use of 
Weierstrass theorem: $$W_\varepsilon:\mathcal{C} \to \Pi \quad st \quad \|W_\epsilon(f)-f\|<\epsilon$$ So for every $0 < \varepsilon <1$ and $0<\delta$ we have the function $f_\delta(x)=W_\frac{1}{5}\left(\frac{4}{5}\sin(n\pi x)\right)$ where $\frac{1}{n}<\delta$ st equcontinuity doesn't hold: $$\left|0-\frac{1}{n}\right|<\delta\text{ but }\left|f_\delta(0)-f_\delta\left(\frac{1}{n}\right)\right|>\varepsilon$$ Which side of conjecture is true?","['examples-counterexamples', 'functional-analysis']"
406981,Difference between $P(Y \ge y)$ and $P(Y > y)$.,"In the book "" Probability: Theory and Examples "", there is this theorem: Lemma 2.2.8 If $Y \ge 0$ and $p > 0$ then $\mathbb E(Y^p) = \int_0^\infty py^{p-1} P (Y > y) \,  \mathrm dy$. The proof is by using Fubini's Theorem as follows \begin{align*}
\int_0^\infty py^{p-1} \mathbb P (Y > y)  \mathrm dy
& = \int_0^\infty \int_\Omega py^{p-1} 1_{(Y > y)} \, \mathrm dP \, \mathrm dy \\
& = \int_\Omega \int_0^\infty py^{p-1} 1_{(Y > y)} \, \mathrm dy \, \mathrm dP \\
& = \int_\Omega \int_0^Y py^{p-1} \mathrm dy \mathrm dP  \\
& = \int_\Omega Y^p \mathrm dP = \mathbb E Y^p \\
\end{align*} For me it seems totally fine to replace $Y > y$ with $Y \ge y$ in this proof, but the conclusion would be $\mathbb E(Y^p) = \int_0^\infty py^{p-1} P (Y \ge y) \,  \mathrm dy$. So if $Y$ can only take nonnegative integer values, we have $\mathbb E Y = \sum_{y \ge 0} P(Y \ge y)$, which is wrong. So why we can not replace $Y > y$ with $Y \ge y$?",['probability-theory']
406986,asymptotic growth of entire functions with positive taylor coefficients,"I have an entire function $g(x)$ with taylor coefficients that go to zero at a ridiculously fast rate and I am trying to bound it with another entire function $f$ with similar properties but with the added claim that $f(x) \sim x^{1-\epsilon}$ so that I can say $g(x) = o(x)$. I need the coefficients of $f$ to be positive though. 
For example I was thinking of a function like $$f(x) = \sum_{n=0}^{\infty} \frac{x^n}{n!^{n!}}$$ or something similar which must grow slowly.
I don't know how to prove this though, and was wondering if there was some technique or if another function would work better.","['real-analysis', 'analysis']"
406997,Question on Projective Dimensions,"$\require{AMScd}$I have a question regarding a claim in A first course of homological algebra by Northcott . I think it's very easy, since the author didn't provide a proof, and just kind of claimed it. It's on page 76. And it says: Let $0 \to A_1 \to P \to A \to 0$ is exact, and $P$ is projective, then $\mbox{Pd}(A) = \mbox{Pd}(A_1) + 1$, provided that $\mbox{Pd}(A) > 0$. Where $\mbox{Pd}(A)$ denotes the projective dimension of $A$, i.e if $A$ has a projective resolution $0 \to P_n \to P_{n-1} \to... \to P_1 \to P_0 \to A \to 0$, where all $P_i$'s are projective, and there's no shorter resolution. Then we'll write $\mbox{Pd}(A) = n$. Question Ok, so how can I prove it? I can get to the point that $\mbox{Pd}(A) \le \mbox{Pd}(A_1) + 1$, by assuming that $\mbox{Pd}(A_1) = n$, i.e there exists a projective resolution of $A_1$, which looks like this: $0 \to P_n \to P_{n-1} \to... \to P_1 \to P_0 \to A_1 \to 0$, since $P_0 \to A_1$ is epic, and $A_1 \to P$ is monic, I can combine the 2 mappings, and arrive the following resolution: $0 \to P_n \to P_{n-1} \to... \to P_1 \to P_0 \to P \to A \to 0$, which means that $\mbox{Pd}(A) \le n+1$. But I cannot prove that $\mbox{Pd}(A) = n+1$. Here's how I start, assume that $A$ has a resolution: $0 \to P_n \to P_{n-1} \to... \to P_1 \to P_0 \to A \to 0$, and I'll try to construct a resolution for $A'$ that lacks $P_0$, i.e, what I'm trying to prove is, if $\mbox{Pd}(A) < n+1$, then $\mbox{Pd}(A_1) < n$ (which is, of course not right, since we assume $\mbox{Pd}(A_1) = n$). So here's how it goes: $\begin{CD}@. @. @. 0 @>>> A_1 @>\chi>> P @>\sigma>> A @>>> 0\\
@. @. @. @. @. @. @| \\
0 @>>> P_n @>\chi_n>> P_{n-1} @>\chi_{n-1}>> ... @>\chi_2>> P_1 @>\chi_1>> P_0 @>\chi_0>> A @>0>> 0 \\
\end{CD}$ Since $\sigma$ is epic, and $P_0$ is projective, hence there exists: $\gamma_0: P_0 \to P$, such that the square commutes: $\begin{CD}@. @. @. 0 @>>> A_1 @>\chi>> P @>\sigma>> A @>>> 0\\
@. @. @. @. @. @A\gamma_0AA @| \\
0 @>>> P_n @>\chi_n>> P_{n-1} @>\chi_{n-1}>> ... @>\chi_2>> P_1 @>\chi_1>> P_0 @>\chi_0>> A @>0>> 0 \\
\end{CD}$ Since $\sigma (\gamma_0 \chi_1) = 0$, and the former row is exact, hence there exists $\gamma_1: P_1 \to A$, such that the square commutes. $\begin{CD}@. @. @. 0 @>>> A_1 @>\chi>> P @>\sigma>> A @>>> 0\\
@. @. @. @. @A\gamma_1AA @A\gamma_0AA @| \\
0 @>>> P_n @>\chi_n>> P_{n-1} @>\chi_{n-1}>> ... @>\chi_2>> P_1 @>\chi_1>> P_0 @>\chi_0>> A @>0>> 0 \\
\end{CD}$ One problem remains is that $\gamma_1$ is not epic, so, I cannot have the resolution: $0 \to P_n \to ... \to P_1 \to A_1 \to 0$. Can my proof be improved, or should I tackle it in a complete different way? Thank you guys very much in advance, And have a good day, Edit I still don't see how we can use Schanuel's lemma to tackle this problem, ok so I assume $\mbox{Pd}(A) < n + 1$, i.e, I'll have the resolution $\begin{CD} @. @. @. @. @. 0 \\ @. @. @. @. @. @VVV \\ 0 @>>> P_n @>>> P_{n - 1} @>>> ... @>>> P_0 @>>> A_1 @>>> 0 \\ @. @. @. @. @. @VVV \\ @. @. @. @. @. P \\ @. @. @. @. @. @VVV \\ 0 @>>> Q_n @>>> Q_{n - 1} @>>> ... @>>> Q_0 @>>> A @>>> 0 \\ @. @. @. @. @. @VVV \\ @. @. @. @. @. 0 \end{CD}$ How can I apply Schanuel's lemma to find a resolution for $A_1$ that has length no larger than $n - 1$?","['modules', 'homological-algebra', 'projective-module', 'abstract-algebra']"
407006,$5 \mid n^2 - m^2$ is an equivalence relation,"How can I show this is an equivalence relation: $$
n \operatorname{R} m \Longleftrightarrow n^2 - m^2 \textrm{ is divisible by } 5
$$ I know equivalence relations are symmetric, reflexive and transitive.  I'm just not sure how to use this knowledge to prove it.","['relations', 'equivalence-relations', 'elementary-set-theory']"
407028,Divisors in an abelian surface,"How to compute the Néron-Severi group of the abelian surface $Y = \mathbb{C}/\mathbb{Z}[i] \times \mathbb{C}/\mathbb{Z}[i]$. More generally, are there any result that compute the Néron-Severi group of product of curves? Suppose that surface $Y$ has a divisor class $[F]$ such that $[F]^{2} > 0$. Why is $[F]$ or $-[F]$ ample? Thanks.","['abelian-varieties', 'algebraic-geometry']"
407030,Prime number question,"Can somebody please give me a hint on how to start this question: Let $a$ and $n$ be two positive integers with $a,n ≥ 2$. Assume that $a^n−1$ is a prime number. Prove that $a = 2$ and $n$ is a prime number.","['prime-numbers', 'number-theory']"
407031,A (not necessarily continuous) function on a compact metric space attaining its maximum.,"I am studying for an exam and my study partners and I are having a dispute about my reasoning for $f$ being continuous by way of open and closed pullbacks (see below).  Please help me correct my thinking.  Here is the problem and my proposed solution: Let $(K, d)$ be a compact metric space, and let $f: K \rightarrow \mathbb{R}$ be a function satisfying that for each $\alpha \in \mathbb{R}$ the set {$x \in K: f(x) \ge \alpha$} is a closed subset of $K$.  Show that $f$ attains a maximum value on $K$. Proof: Notice that $A :=$ {$x \in K: f(x) \ge \alpha$} is precisely $f^{-1}[\alpha, \infty)$.  Since $[\alpha, \infty)$ is closed in $\mathbb{R}$ and $A$ is assumed to be closed in $K$, then it follows that $f$ is continuous on $A$.  On the other hand, $K-A = f^{-1}(-\infty, \alpha)$ is open in $K$ since $A$ is closed in $K$.  And since $(\alpha, \infty)$ is open in $\mathbb{R}$ and $K - A$ is open in $K$, then if follow that $f$ is continuous on $K - A$, hence $f$ is continuous on $K$.  Since $K$ is compact and $f$ is continuous, then $f(K)$ is compact in $\mathbb{R}$.  Compact sets in $\mathbb{R}$ are closed and bounded intervals.  Thus $\sup{f(K)} = \max{f(K)} = f(x_0)$ for some $x_0 \in K$.  Thus $f$ indeed attains its maximum value on $K$.  $\blacksquare$","['metric-spaces', 'compactness', 'real-analysis']"
407036,Zero of a complex polynomial satisfying one of three assertions.,"Let $n$ be a positive integer greater than $1$. Prove that if $x$ is a zero of  $ X^n+1+(-1)^n(X+1)^n$ then $|x|=1$ or $|x+1|=1$ or $|x+1|=|x|$. My initial thought was to study the cases $n=2,3,4$ (that confirm indeed the result) in order to establish a general approach, but I didn't succeed to do this.","['complex-analysis', 'polynomials']"
407039,Existence of Lipschitz reparametrization,"Suppose we are given a continuous path, $$\gamma:[0,1]\rightarrow (X,d)\text{,}$$ in a metric space $(X,d)$. When we deal with differentiable enough paths in Riemann manifolds we can give a parametrization that is $l$-Lipschitz, where $l$ is the length of the path. If have that $\gamma$ has finite length $l$ in the sense that
$$l=\sup\left\{\sum_{i=0}^{n-1}d(\gamma(t_i),\gamma(t_{i+1}))\,|\, n\in \mathbb{N}; \forall i< n,t_i\in[0,1]\text{ and }t_i<t_{i+1};t_0=0;t_n=1\right\}$$
is finite, can we guarantee the existence of a parametrization that makes the path $l$-Lipschitz? Note: The necessity for this result comes that I need this fact to prove that certain sequences of path in metric spaces have nice properties in order to obtain a limit path. And in this way proving that compact path metric spaces are geodesic metric spaces.","['parametric', 'metric-spaces', 'differential-geometry']"
407040,System of Pythagorean Quadratics,"I have a system of quadratics, obtained from three mechanical links, fixed at one end and free at the other. The intersection point of the three free ends is required. $AC=\sqrt{(A_x-C_x)^2+(A_y-C_y)^2+(A_z-C_z)^2}$ $BC=\sqrt{(B_x-C_x)^2+(B_y-C_y)^2+(B_z-C_z)^2}$ $FC=\sqrt{(F_x-C_x)^2+(F_y-C_y)^2+(F_z-C_z)^2}$ Where C_x,y,z are the unknowns. I am halfway through solving by substitution and it is really messy. Does anyone know of a slightly more elegant way of solving these? Any techniques I should research. I want to avoid doing it numerically if possible.
Many Thanks
James","['quadratics', 'algebra-precalculus']"
407070,Show that $f(z)=0$. [duplicate],This question already has answers here : on the boundary of analytic functions (3 answers) Closed 7 years ago . Suppose that $f:\mathbb{C}\rightarrow\mathbb{C}$ is analytic on the open unit disc and continuous on the closed unit disc. Assume that $f(z)=0$ on an arc of the circle $\{z\in\mathbb{C}:|z|=1\}$. Show that $f(z)=0$.,['complex-analysis']
407085,Tricky probability bound [duplicate],"This question already has answers here : Exercise 1.6.3 from Alon & Spencer's *The Probabilistic Method*: prove that $Pr[|X-Y| \leq 2] \leq 3 Pr[|X-Y| \leq 1]$ for i.i.d. real RVs $X$ and $Y$ (3 answers) Closed 4 years ago . Here's an easy-looking probability theory problem that so far has defied my attempts at solving it: Let $X$ and $Y$ be two real independent identically distributed random variables. Prove that $$
Pr(|X-Y| \le 2) \leq 3Pr(|X-Y| \le 1)
$$",['probability-theory']
407106,Legendre Symbol - Find Prime $p$ Which Divides A Polynomial,"I need to find a general form of a prime number $p$ which divides the polynomial $x^2-6$, i.e. $p$ such that $x^2 - 6\equiv 0\text{ (mod }p)$. By Legendre symbol, I actually need to find a prime p such as $\left(\frac{6}{p}\right) = 1$. I know that $\left(\frac{6}{p}\right) = \left(\frac{3}{p}\right)\left(\frac{2}{p}\right)$, so there are two options at the moment: Both $\left(\frac{3}{p}\right) = 1$ and $\left(\frac{2}{p}\right) = 1$. Both $\left(\frac{3}{p}\right) = -1$ and $\left(\frac{2}{p}\right) = -1$. I'd like to find out how could I find a general form of a prime $p$ which answers the two terms above? Thanks in advance","['prime-numbers', 'elementary-number-theory', 'number-theory']"
407112,How to solve $x\log(x) = 10^6$,I am trying to solve $$x\log(x) = 10^6$$ but can't find an elegant solution. Any ideas ?,"['calculus', 'discrete-mathematics']"
407123,How many paths are there in graph $K_n$?,We know In $K_4$ there is $(4*3)+(4*3*2)+(4*3*2*1)$ paths in graph. How many paths are there in graph $K_n$? I want a simple formula...,"['graph-theory', 'discrete-mathematics']"
407125,What is $\lim\limits_{n→∞}(\frac{n-x}{n+x})^{n^2}$?,"What is $$\lim_{n\rightarrow\infty}\left(\frac{n-x}{n+x}\right)^{n^2},$$ where $x$ is a real number. Mathematica tells me the limit is $0$ when I put an exact value for $x$ in (Mathematica is inconclusive if I don't substitute for $x$), but using $f(n)=(n-x)^{n^2}$ and $g(n)=(n+x)^{n^2}$, then $$\lim_{n\rightarrow\infty}f(n)=\lim_{n\rightarrow\infty}g(n)=\infty,$$ and by L'Hopital's rule we have
$$\lim_{n\rightarrow\infty}\frac{f(n)}{g(n)} = \lim_{n\rightarrow\infty}\frac{f'(n)}{g'(n)} = \lim_{n\rightarrow\infty}\frac{(s-z)^{-1+s^2} (s+z)^{1-s^2} (s+2 (s-z) \text{Log}[s-z])}{s+2 (s+z) \text{Log}[s+z]} = 1.$$ I'm not sure which to believe - Mathematica or L'Hopital.","['real-analysis', 'limits']"
407127,Adaptation to Banach–Mazur theorem,"I'm trying to prove the following: For every normed linear space $X$, there exists a isometric ismorphism of $X$ in $C(K)$, where $K$ is a compact space. I know from Banach–Mazur theorem that every separable Banach space satisfies the problem, so I tried to take a look at the demonstration of this theorem to have any idea how to solve this, but to prove that the function defined on the sketch of the proof of the theorem is in fact a isometry, we have to use Hahn-Banach, since the space $X$ is not necessary complete, I can't use this here and hence I'm stuck. Thanks for any help!","['normed-spaces', 'functional-analysis']"
407128,Simplifying $a(a-2) = b(b+2)$,"I reduced a number theory problem to finding all ordered pairs $(a,b)$ that satisfy the equation $a(a-2) = b(b+2)$ in a certain range. After thinking about this for a while, I figured that either $a = b + 2$, $a = -b$, $a = b = 0$ or $a = 2 \text{ and } b = -2$. It is easy to prove that these values for $a$ and $b$ will all satisfy the equation, but how would I solve this equation had I not come up with these solutions? And how do I know I did not miss one? In short, can this equation be solved (more) rigorously? Edit :
Silly of me I did not recognize that my last two solutions were already covered by my first two.","['elementary-number-theory', 'algebra-precalculus']"
407140,Construction of triangle,"I don't know how to prove or disprove the following problem:
How to construct triangle if elements $a$, $b$, $\beta-\gamma$ are given? 
Is it constructible (if not, how to prove it)? Any help is welcome.","['geometry', 'geometric-construction']"
407153,Choosing $a$ s.t. $\frac{a^k - 1}{a-1}$ is not a prime power,"Let us suppose that we are presented with a positive integer $k$ and asked to come up with a positive integer $a$ such that $\frac{a^k - 1}{a-1}$ is not a prime power, or just to prove in an elementary way that this can be done. How do we proceed? Of course, in practice this is essentially trivial. But is there a cannonical choice? For that matter, is there an elementary proof that shows that this is always possible? This came up in a discussion with another Brown mathie about interesting things to talk about in an elementary number theory class I will be teaching this summer. To be more specific, we were trying to prove the infinitude of primes in arithmetic progressions of the form $1, 1+p^n, 1+2p^n, ...$ using a naive a completely elementary approach (hopefully to be replicated by my students this summer), and this side topic came up.","['prime-numbers', 'elementary-number-theory', 'number-theory']"
407211,Spivak Calculus chapter 7 theorem 9,"I am working through Spivak Calculus chapter 7 theorem 9. There is one statement that I can't quite understand. The theorem states:
If $n$ is odd, then any equation
$$
\ x^n+a_{n-1}x^{n-1} +\cdots+a^0
$$
has a root. proof: we would like to prove that $f$ is sometimes positive and sometimes negative. The
intuitive idea is that for large $|x|$, the function is very much like $g(x) = x^n$ and,
since $n$ is odd, this function is positive for large positive $x$ and negative for large
negative $x$. A little algebra is all we need to make this intuitive idea work. $$
f(x) = x^n+a_{n-1}x^{n-1} +\cdots+a^0 = x^n \left(1+\frac{a_{n-1}}{x}+\cdots+\frac{a_0}{x^n}\right)
$$ Note that 
$$
\left|\frac{a_{n-1}}{x}+\frac{a_{n-2}}{x^2}+\cdots+\frac{a_0}{x^n} \right|\le \frac{|a_{n-1}|}{|x|}+\frac{|a_{n-2}|}{|x^2|}+\cdots+\frac{|a_{0}|}{|x^n|}
$$ Consequently if we choose $x$ satisfying
$$
|x|>1,2n|a_{n-1}|,\ldots,2n|a_0| \tag{*}
$$ I am not sure how he comes to $(*)$ Thanks in advance",['calculus']
407244,Not so easy optimization of variables?,"What is the maximum value of $x^2+y^2$, where $(x,y)$ are solutions to $2x^2+5xy+3y^2=2$ and $6x^2+8xy+4y^2=3$. (calculus is not allowed). I tried everything I could but whenever I got for example $or$ $x^2+y^2=f(y)$ or $f(x)$ the function $f$ would always be a concave up parabola, so I could not find a maximum for either variable. However, I also don't see how you could solve it if you leave both variables on one side. And by the way I know that you can solve for $x$ and $y$ using the quadratic formula and get $4$ different solutions but I am looking for a much more efficient way than that. This question came from a math competition from the Math Honor Society, Mu Alpha Theta.","['optimization', 'algebra-precalculus']"
407259,How do you prove that there are infinitely many primes of the form $5 + 6n$? [duplicate],This question already has answers here : Proving that there are infinitely many primes with remainder of 2 when divided by 3 (6 answers) Closed last year . There should be infinitely many primes of the form $5+6n$. How do you prove it? The same should be true for $7+6n$.,"['prime-numbers', 'number-theory']"
407272,Finding a direct basis for tangent space of piece with boundary of an oriented manifold.,"I have the following definition (from Hubbard's vector calculus book) for an oriented boundary of piece with boundary of an oriented manifold: Let $M$ be a $k$ dimensional manifold oriented by $\Omega$ and $P$ a piece with boundary of $M$. Let $x$ be a point of the smooth boundary $\partial^{ \ S}_MP$ and let $\vec{V}_{\text{out}}\in T_xM$ be an outward pointing bector. Then the function $\Omega^\partial : \mathcal{B}(T_x\partial P)\to\left\{+1,-1\right\}$ given by
  $$
\Omega_x^\partial(\vec{v}_1,...,\vec{v}_{k-1}) = \Omega_x(\vec{V}_{\text{out}},\vec{v}_1,...,\vec{v}_{k-1})
$$
  defines an orientation on the smooth boundary $\partial_M^{ \ S}P,$ where $\vec{v}_1,...,\vec{v}_{k-1}$ is an ordered basis of $T_x\partial_M^{ \ S}P$. I'm working on a problem that asks me to find a basis for the $T_x\partial P$ that is direct to a certain orientation (given by an elementary 3-form). My question is this: When I choose a basis for $T_x\partial P$, does this basis also need to lie in $T_xM$? Also, are there restrictions to how I should choose $\vec{V}_{\text{out}}$? In other words, does $\vec{V}_{\text{out}}$ need only lie in $T_xM$ and not in $T_x\partial P$?","['multivariable-calculus', 'manifolds', 'differential-geometry']"
407281,Proof of uniqueness of the bounded linear transformation extended in the Bounded Linear Transformation theorem,"B.L.T Theorem (from Reed/Simon): Suppose $T$ is a bounded linear transformation from a normed linear space $\langle V_1, \|\cdot\|\rangle$ to a complete normed linear space $\langle V_2, \|\cdot\|\rangle$.  Then $T$ can be uniquely extended to a bounded linear transformation (with the same bound), $\widetilde{T}$, from the completion of $V_1$ to $\langle V_2, \|\cdot\|\rangle$. The proof for $\widetilde{T}$ being bounded was given and very straightforward, and proving that it was linear was pretty simple as well.  I have been having issues with proving that $\widetilde{T}$ is unique, however, despite it probably being easy.  I tried supposing towards a contradiction and using that $V_1$ is dense in its completion, $\tilde{V_1}$, the extending transformations must agree on $V_1$, the extending transformations must both have the same bound as $T$, and since this implies that the extending transformations must have different bounds to be different themselves, this proves that $\widetilde{T}$ is unique. I don't think that this reasoning is right since I think that a transformation can act differently on some subset of $\tilde{V_1}\setminus{V_1}$ without changing the bound, and my only other idea was to use that both of these spaces are $T_1$, so if a sequence converges, it must converge to a unique point, and since $T$ is bounded and therefore continuous, and any extensions must be bounded and therefore continuous, we'll have sequences being mapped to their limits, and since these limits are unique, we will only yield one extension $\widetilde{T}$ that works for all of $\tilde{V_1}$.  Again, I think that this reasoning is missing something. Any insights, whether it be a nudge in the right direction or full proofs, are very welcome!  Thanks in advance.","['normed-spaces', 'functional-analysis']"
407286,Two questions about Euler's number $e$,"I am on derivatives at the moment and I just bumped into this number $e$, ""Euler's number"" . I am told that this number is special especially when I take the derivative of $e^x$ , because its slope of any point is 1. Also it is an irrational ($2.71828\ldots$) number that never ends, like $\pi$. So I have two questions, I can't understand What is so special about this fact that it's slope is always 1? Where do we humans use this number that is so useful, how did Mr Euler come up with this number? and how come this number is a constant? where can we find this number in nature?","['calculus', 'derivatives', 'constants']"
407304,Evaluating $\int_{0}^{1} \frac{ \ln x \ln (1-x)}{\sqrt{x} \sqrt{1-x}} dx$,"I have the following integral:
$$\int_{0}^{1} \frac{ \ln x \ln (1-x)}{\sqrt{x} \sqrt{1-x}} dx$$ I think I may be to evaluate this with beta and gamma functions but I am not quite sure how. Any help?","['calculus', 'integration']"
407307,Second pair of matching birthdays,"The ""birthday problem"" is well-known and well-studied.  There are many versions of it and many questions one might ask.  For example, ""how many people do we need in a room to obtain at least a 50% chance that some pair shares a birthday?""  (Answer: 23) Another is this: ""Given $M$ bins, what is the expected number of balls I must toss uniformly at random into bins before some bin will contain 2 balls?""  (Answer: $\sqrt{M \pi/2} +2/3$) Here is my question: what is the expected number of balls I must toss into $M$ bins to get two  collisions?  More precisely, how many expected balls must I toss to obtain the event ""ball lands in occupied bin"" twice? I need an answer for very large $M$, so solutions including summations are not helpful. Silly Observation: The birthday problem predicts we need about 25 US Presidents for them to share a birthday.  It actually took 28 presidents to happen (Harding and Polk were both born on Nov 2).  We see from the answers below that after about 37 US Presidents we should have a 2nd collision.  However Obama is the 43rd and it still hasn't happened (nor would it have happened if McCain had won or Romney had won; nor will it happen if H. Clinton wins in 2016).","['balls-in-bins', 'probability']"
407310,How can I find the surface area of a normal chicken egg?,"This morning, I had eggs for breakfast, and I was looking at the pieces of broken shells and thought ""What is the surface area of this egg?"" The problem is that I have no real idea about how to find the surface area. I have learned formulas for circles, and I know the equation for an ellipse; however, I don't know how to apply that. The only idea I can think of is to put an egg on a sheet of paper and trace it, and then measure the outline drawn, and then try to find an equation for that ellipse and rotate that about the $x$-axis.  Now, my problem is how I can find the equation of the ellipse from the graph, and will my tracing method really be the edge of the egg?  Also, can I use the standard surface area integral ? Will I have to use some techniques to solve the integral that are not covered in the AP BC Calculus ? There has to be a better method for finding the surface area. Please, help me understand how to find the surface area of an egg; i.e., how to use my mathematical knowledge for something other than passing exams.","['geometry', 'calculus']"
407313,If a series is absolutely converge then the series can be regroup with changing their order?,"I am just thinking about why this is true. Can I change it to Q1. If a series is convergent then the series can be regrouped without changing the order of terms. For example the sum of $(-1)^n$ is an alternating sequence and it is divergent, so I can't regroup them? Q2. Can I claim that a convergent, non-alternating series be absolutely convergent? As there is no difference after the term become absolute value, it should be still convergent after absolute those terms? Q3. What does conditionally convergent mean?
If a sequence is either convergent or absolutely convergent then it is conditionally convergent?","['sequences-and-series', 'real-analysis']"
407318,At $z=0$ the function $f(z)=\exp({z\over 1-\cos z})$ has,"At $z=0$ the  function $f(z)=\exp({z\over 1-\cos z})$ has $1$. A removable singualrity $2$. A pole $3$. An essential singularity $4$. Laurent series around $z=0$ has infinitely man positive and negative power of $z$ I see $\lim_{z\to\infty}f(z)=\infty\cup -\infty$ if you approach to $0$ from left and right side of the real line, so $f$ can not be bounded near $0$ so it has $f$ essential singularity?",['complex-analysis']
407326,Determinant of $4\times4$ Matrix,"I tried to solve for a $4 \times 4$ matrix, but I'm unsure if I did this properly, can anyone tell me if I did this correct? Or if there were any mistakes where at? Also, I know this is an inefficient method for finding the determinant, however I want to get practice with solving like so: $$A=
\begin{bmatrix}
2 & 4 & 0 & 1 \\
0 & 8 & 0 & 2 \\
0 & 3 & 0 & 5 \\
1 & 2 & 1 & 1
\end{bmatrix}
$$ $$\begin{align*}
\det(A)&=2
\begin{vmatrix}
8 & 0 & 2 \\
3 & 0 & 5 \\
2 & 1 & 1
\end{vmatrix}
-4
\begin{vmatrix}
0 & 0 & 2 \\
0 & 0 & 5 \\
1 & 1 & 1
\end{vmatrix}
+0
-1
\begin{vmatrix}
0 & 8 & 0 \\
0 & 3 & 0 \\
1 & 2 & 1
\end{vmatrix}\\[0.3in]
&=2\left(8
\begin{vmatrix}
0 & 5 \\
1 & 1
\end{vmatrix}-0+2
\begin{vmatrix}
3 & 0\\
2 & 1
\end{vmatrix}\right)\\[0.1in]
&\quad{}-4\left(0-0+2
\begin{vmatrix}
0 & 0\\
1 & 1
\end{vmatrix}\right)\\[0.1in]
&\quad{}+0\\[0.1in]
&\quad{}-1\left(0-8
\begin{vmatrix}
0 & 0\\
1 & 1
\end{vmatrix}+0\right)\\[0.3in]
&=2(8(0-5)-0+2(3-0))\\[0.1in]
&\quad{}-4(0-0+2(0))\\[0.1in]
&\quad{}+0\\[0.1in]
&\quad{}-1(0-8(0)+0)\\[0.3in]
&= 2(8(-5)-0+2(3))\\[0.3in]
&=2(-45+6)\\[0.3in]
&=2(-39)\\[0.3in]
&=-78
\end{align*}$$ Sorry for the long post, I tried to make the readability easy for everyone.","['matrices', 'linear-algebra', 'determinant']"
407330,"arrange k identical robots in 15 chairs, with limitations","I ran into this question, want to challenge you. in how many ways can you arrange a number of identical robots on $15$ chairs? limitations:
1) $2$ robots cannot sit next to each other.
2) each empty chair has at least one neighbour with a robot. what i got so far: i know that it has something to do with $\sum_{5}^{8}$ because the minimum amount of robots is $5$ and maximum is $8$. i'm not sure how to proceed from here though. Thank you very much in advance, Yaron","['discrete-mathematics', 'combinatorics']"
407337,Adriaan van Roomen's 45th degree equation in 1593,"Adriaan van Roomen proposed a 45th degree equation in 1593(see this book , picture reference as follows): $$
\begin{gathered}
f(x) = x^{45} - 45x^{43} + 945x^{41} - 12300x^{39} + 111150x^{37} - \color{red}{740459}x^{35} + 3764565x^{33} \\- 14945040x^{31} + 46955700x^{29} - 117679100x^{27} + 236030652x^{25} - 378658800x^{23} \\+ 483841800x^{21} - 488494125x^{19} + 384942375x^{17} - 232676280x^{15} + 105306075x^{13}
\\ - \color{red}{34512074}x^{11} + 7811375x^9 - 1138500x^7 + 95634x^5 - 3795x^3 +45x \\ = \sqrt{\frac{7}{4}-\frac{\sqrt{5}}{4}- \sqrt{\frac{15-3\sqrt{5}}{8}}} \approx 0.4158234.
\end{gathered}\tag{1}
$$
Another source said the right side is: $\sqrt{\frac{7}{4}-\frac{\sqrt{5}}{4}-\frac{5\sqrt{3}}{8}} $. ( EDIT : According to the answer, the coefficients in red are wrong.) Another mathematician Viète noticed that left side is nothing but the expansion of sine using
$$
\sin3\theta = 3\sin\theta - 4\sin^3\theta, 
$$
recursively, and the equation (1) reduced to:
$$
\sin(\alpha) = c, \quad \text{ for } \alpha \in (0,\frac{\pi}{2}) \tag{2}
$$
where $x = \sin(\alpha/45)$ 
( EDIT : According to the answer, this is not correct, should be  $x = 2\sin(\alpha/45)$). Now $2k\pi+\alpha$ satisfies (2) too, hence the solutions are:
$$
x = \sin\left(\frac{\alpha + 2k\pi}{45}\right), \quad \text{ for } k = 0,1,2,\ldots,44.
$$ 
Totally 45 roots in $(-1,1)$. However, when drawing the graph of $f(x)-c$, something does not look quite right: There are two other roots near roughly 3 and -3? Am I missing something here?","['trigonometry', 'math-history', 'roots', 'polynomials']"
407339,If $X^\ast $ is separable $\Longrightarrow$ $S_{X^\ast}$ is also separable,Let $X$ be a Banach space such that $X$* (Dual space of $X$) is separable How can we prove that $S_{X^\ast}$ (Unit sphere of $X$*) is also separable Any hints would be appreciated.,"['normed-spaces', 'metric-spaces', 'functional-analysis', 'banach-spaces']"
407343,How to solve this limit?,"I have a regression model: $y_i=\exp(a \sin(\frac{2 \pi i}{n}) + b \cos(\frac{2 \pi i} {n})+\varepsilon_i)$ where a, b are the regression parameters. Let ${\varepsilon}_i = {\varepsilon}_i(t)$ be independent identically distributed random processes. I want to evaluate an accuracy. Let $\hat{y} = \exp(\hat{a} \sin(\frac{2 \pi i}{n}) + \hat{b} \cos(\frac{2 \pi i} {n}))$ be the model with estimated parameters $\hat{a}, \hat{b}$. Then, $\hat{\varepsilon}_i = \ln{y_i} - \ln{\hat{y_i}}$ is a residual on ith point. $$Z_n(t)=\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]} \hat{\varepsilon_i}.$$ I need to find $\lim_{n\to\infty} Z_n(t)$ in distribution. I tried to proceed as follows: $$Z_n(t)=\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]} \hat{\varepsilon_i}=\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]}[\ln{y_i} - \ln{\hat{y_i}}]$$ $$Z_n(t)=\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]} [(a-\hat{a}) \sin(\frac{2 \pi i}{n}) + (b-\hat{b}) \cos(\frac{2 \pi i} {n})+\varepsilon_i]$$ $\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]} \varepsilon_i \to_{n\to\infty} N(0, 1)$. Here $N(0,1)$ is a standart normal distribution. $$Z_n(t) \to N(0,1) + \lim_{n\to\infty}\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]} [(a-\hat{a}) \sin(\frac{2 \pi i}{n}) + (b-\hat{b}) \cos(\frac{2 \pi i} {n})]$$ Now I think we can simplify the sum limit to something like $(a-\hat{a}) + (b-\hat{b})$ and can get $Z_n(t)\to N(a-\hat{a} + b-\hat{b}, 1)$. My question is how to get it and is it all correct with my calculations? Thanks in advance. EDIT: Well, now I have got a partial solution of this problem... First of all, we should get the OLS-estimators of $\hat{a}$ and $\hat{b}$. Let $\hat{u}_i = \ln(\hat{y}_i)$. Then, our model is $U = X \theta$, where $\theta=\begin{pmatrix} a \\ b \end{pmatrix}$,
$X = \begin{pmatrix} \sin(\frac{2 \pi 1}{n}) & \cos(\frac{2 \pi 1} {n}) \\ ... & ... \\ \sin(\frac{2 \pi n}{n}) & \cos(\frac{2 \pi n} {n})\end{pmatrix}$ An assessment can be found by using the formula: $\hat{\theta} = (X^T X)^{-1} X^T U $. After some calculations, $\hat{\theta} = \begin{pmatrix} 2 \overline{u_i \cos(\frac{2 \pi i}{n}}) \\ 2 \overline{u_i \sin(\frac{2 \pi i}{n}}) \end{pmatrix}$. So, $\hat{\varepsilon} = u - \hat{u} = -n\cdot \overline{U}$. Here $\overline{U}$ is a mean value of $U$, i.e. $\frac{1}{n}\Sigma_{i=1}^n u_i$. Now we want to describe a random process $Z_n(t)=\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]} \hat{\varepsilon_i}$. AFAIK, Ian B. MacNeill's theorem is just about it, but I can't yet understand it... Could you please help me to complete solution here?",['limits']
407347,Integral inequality (Divergence theorem),"I'm trying to prove the following inequality: $$2 \int_U |\nabla \phi|^2 dx \leq \int_U \phi^2 dx + \int_U |\Delta \phi|^2 dx$$ where $U \subset \mathbb{R}^n$ is bounded and open and $\phi \in C^\infty_c(U)$. I actually think I have managed to prove this just using one of Green's identities $$\int_UD \phi \cdot D \phi dx = -\int_U \phi \Delta \phi dx + \int_{\partial U}\frac{\partial \phi}{\partial \nu} \phi dx$$
(which comes from the divergence theorem), and then using the fact that $\phi = 0$ on $\partial U$. This then gives: $$\int_U |\nabla \phi|^2 dx = -\int_U \phi \Delta \phi dx \leq \int_U |\phi| |\Delta \phi| dx \leq \int_U \frac{\phi^2}{2}+\frac{|\Delta \phi|^2}{2}$$ where the last inequality comes from Cauchy's inequality $ab \leq a^2/2 + b^2/2$. Does this seem correct? The problem is that I have been given a hint which says to use the fact that $\nabla \cdot(\phi \nabla \phi) = |\nabla \phi|^2 + \phi \Delta \phi$ and I'm not sure how to use this hint. Would this just lead to an alternative proof of the inequality?",['multivariable-calculus']
407416,Show that H is a subset of the normalizer,My Abstract Algebra professer assigned us this homework problem which I'm assuming he created himself: Suppose $H$ is a subgroup of $G$. Show that $H$ is a subset of the normalizer of $H$. The normalizer of $H$ is defined as $N(H)=\{x \in G: x^{-1}Hx=H\}$. I've spent more time than I'd like trying just to understand the logic behind the statement and I end up confusing myself. Thanks in advance for any and all help.,"['group-theory', 'abstract-algebra']"
407417,Proving that free modules are flat (without appealing projective modules),"Suppose $R\neq 0$ is a commutative ring with $1$. Let $M$ be a free $R$-module. I would like to prove that $M$ is a flat $R$-module. Everywhere I have looked (mostly online) this is proved by first proving that every free module is projective, and then proving that every projective module is flat. Unfortunately, Atiyah & Macdonald's ""Introduction to Commutative Algebra"" (Chapter 2) does not discuss projective modules. But the result that every free module is flat comes very handy in the exercises. So my question is, Is it possible to prove that every free module is flat just by
  definitions and without appealing to projective modules? Thanks!","['modules', 'commutative-algebra', 'homological-algebra', 'abstract-algebra']"
407420,Evaluating $\int_{-1}^{1}\frac{\arctan{x}}{1+x}\ln{\left(\frac{1+x^2}{2}\right)}dx$,"This is a nice problem. I am trying to use nice methods to solve this integral, But I failed. $$\int_{-1}^{1}\dfrac{\arctan{x}}{1+x}\ln{\left(\dfrac{1+x^2}{2}\right)}dx, $$ where  $\arctan{x}=\tan^{-1}{x}$ mark: this integral is my favorite one. Thanks to whoever has nice methods. I have proved the following: $$\int_{-1}^{1}\dfrac{\arctan{x}}{1+x}\ln{\left(\dfrac{1+x^2}{2}\right)}dx=\sum_{n=1}^{\infty}\dfrac{2^{n-1}H^2_{n-1}}{nC_{2n}^{n}}=\dfrac{\pi^3}{96}$$ where $$C_{m}^{n}=\dfrac{m}{(m-n)!n!},H_{n}=1+\dfrac{1}{2}+\dfrac{1}{3}+\cdots+\dfrac{1}{n}$$ I also have got a few by-products 
$$\int_{-1}^{1}\dfrac{\arctan{x}}{1+x}\ln{\left(\dfrac{1+x^2}{2}\right)}dx=-I_{1}-2I_{2}$$ where $$I_{1}=\int_{0}^{1}\dfrac{\ln{(1-x^2)}}{1+x^2}\ln{\left(\dfrac{1+x^2}{2}\right)}dx=\dfrac{\pi}{4}\ln^2{2}+\dfrac{\pi^3}{32}-2K\times\ln{2}$$ and
$$I_{2}=\int_{0}^{1}\dfrac{x\arctan{x}}{1+x^2}\ln{(1-x^2)}dx=-\dfrac{\pi^3}{48}-\dfrac{\pi}{8}\ln^2{2}+K\times\ln{2}$$ and same methods,I have follow integral
$$\int_{0}^{1}\dfrac{\ln{(1-x^4)}\ln{x}}{1+x^2}dx=\dfrac{\pi^3}{16}-3K\times\ln{2}$$
where $ K $ denotes Catalan's Constant.",['integration']
407427,Showing an analytic function takes certain values exactly once,"Let $D$ be the open unit disk in the complex plane, and let $f(z)$ be a map from $D$ to $D$ with $f(0)=0$. Denoting $|f'(0)|=\delta$, we further require $\delta>0$. Fix $\eta>0$ with $\eta<\delta$. I have shown that for $z$ with $|z|<\eta$, we have $$|f(z)|\ge \left( \frac{\delta -\eta}{1+\eta\delta}\right)|z|.$$ I want to show that in the disk $|z|<\eta$, $f(z)$ takes on each value $w$ in the following disk once: $$|w|< \left( \frac{\delta -\eta}{1+\eta\delta}\right)\eta.$$ I thought about using Rouché's theorem, but did not see how to apply it. How does one show this? If it helps, I think it is possible the inequality I have shown is strict, but I will need to re-check some details. For the proof, apply the Schwarz-Pick lemma to $f(z)/z$ with the points $z$ and $0$ (so $|z|$ is the upper bound, and $f(z)/z$ at zero becomes $f'(0)$), then set $w$ equal to the left-hand side of the inequality, solve for $f(z)/z$ in terms of $w$, and use the triangle inequality to make the obvious bounds. I can provide more details if needed. There's also the possibility of a typo. The book I am working from is riddled with errors, and I believe I even spotted an error in the errata for this problem (the correct version, as far as I can tell, is the way I stated it above). If you suspect there is a mistake with the problem statement, please let me know. For reference, the version in the errata is as above, but with the constant in both inequalities replaced by $$\left( \frac{\delta -\eta}{1-\eta\delta}\right).$$ Note the sign change. To reiterate, I think the version stated above is correct, and that the errata is in error. This is an exercise on page $39$ of Garnett's Bounded Analytic Functions .",['complex-analysis']
407460,Pointwise convergence implies $L^{2}$ convergence,"If we have  a sequence of bounded functions $f_{n}$ converging almost everywhere to another bounded function $f$ in a finite measure space such that $$ |f_{n}(t)| \leq c$$ for some constant $c$. Then $$ \int |f_{n}(t) - f(t)|^{2} d\mu$$ goes to zero. Since we are in a finite measure space, the constants are integrable and hence DCT applies. However, it only gives the result that $$ \int f_{n}(t)d\mu \rightarrow \int f(t)d\mu$$ and
$$ \int |f_{n}(t) - f(t)| d\mu \rightarrow 0$$.
How do we prove the convergence in $L^{2}$ norm? Here is what I tried: $$ \int |f_{n}(t) - f(t)|^{2} d\mu = \int (f_{n}-f)(\overline{f_{n}}-\overline{f})$$
$$ = \int |f_{n}|^{2} - \int f\overline{f_{n}} - \int f_{n}\overline{f} + \int |f|^{2} $$
which goes to zero by DCT (as $f_{n}\overline{f} \rightarrow |f|^{2}$ and so on).
Am I right in all this or am I missing something? I am apprehensive because I intuitively feel that pointwise convergence should not imply $L^{2}$ convergence.",['measure-theory']
407484,Determining the kernel of a Vandermonde-like matrix,"The kernel of a Vandermonde matrix can be determined using this formula. The following type of matrix has a similar structure, and should also have a one-dimensional kernel. $$V= 
\begin{bmatrix} 
1 & 1 & 1 & \ldots & 1 \\
x_1 & x_2 & x_3 & \ldots & x_n \\
x_1^2 & x_2^2 & x_3^2 & \ldots & x_n^2 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
x_1^{m-1} & x_2^{m-1} & x_3^{m-1} & \ldots & x_n^{m-1}\\
y_1 & y_2 & y_3 & \ldots & y_n \\
y_1x_1 & y_2x_2 & y_3x_3 & \ldots & y_nx_n \\
y_1x_1^2 & y_2x_2^2 & y_3x_3^2 & \ldots & y_nx_n^2 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
y_1x_1^{m-1} & y_2x_2^{m-1} & y_3x_3^{m-1} & \ldots & y_nx_n^{m-1}\\
y_1^2x_1 & y_2^2x_2 & y_3^2x_3 & \ldots & y_n^2x_n\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
y_1^{m-1}x_1^{m-1} & y_2^{m-1}x_2^{m-1} & y_3^{m-1}x_3^{m-1} & \ldots & y_n^{m-1}x_n^{m-1}\\
\end{bmatrix} \in \mathbb{R}^{(n-1)\times n}$$ where $n = m^2+1$ and $(x_i, y_i) \neq (x_j, y_j)$ for $i \neq j$ ; i.e. there are $m$ groups of $m$ rows with all possible combinations of powers $y^ax^b$ and one more column than rows. Does a similar analytical form exist for it? Or, would additional constraints be required, like $x_i^ay_i^b \neq x_i^cy_i^d$ for $i \neq j$ ? ( Crossposted to MO )","['matrices', 'linear-algebra', 'determinant']"
407491,"A second order differential equation, $y''=y^{-3}$","The exercise is about to solve $y''=1/y^3$ for $y=y(t)$ with initial values $y(0)=1$ and $y'(0)=2$. My attempts: I wrote up it as a first order diff.equation with $2$ variables by introducing $x:=y'$, so that we are looking for (the $y$-coordinate of) the trajectory of the vector field $(1/y^3, x)$, I could draw some vectors, and could 'guess' how the solution will look like. Look for the solution in form $y=c\,t^\alpha$. I got that $\alpha=1/2$ and $c^4=-4$, that is $c=\pm1\pm i$. However, I guess, real function was asked. I guess I'm missing something I should know about solving such a differential equation.",['ordinary-differential-equations']
407499,Type of singularities of $\frac{z}{e^z-1}$,"I don't really understand how one can find the type of singularities for a given function. Say if 
$$f(z) = \frac{z}{e^z-1}$$
then I know that the singularities are at $z = 2n\pi i$ However, how do I find the type? If I try to write out Laurent series for this by using
$$e^z = 1 + z + z^2/2! + z^3/3! + \cdots $$, then I got a summation in the denominator, which I don't know how to rearrange as a Laurent series. Can anyone please give me a hint?","['laurent-series', 'complex-analysis']"
407501,Smallest projective subspace containing a degree $d$ curve,"Is it true that the smallest projective subspace containing a degree $d$ curve inside $\mathbb{P}^n$ has dimension at most $d$? If not, is there any bound on the dimension? Generalization to varieties? For $d=1$ this is obvious. I think for the case that the curve is an embedding of $\mathbb{P}^1$ this is also true: Suppose the embedding is given by $n$ degree $d$ homogeneous polynomials $f_0,\dots,f_n$. For each $0\leq i\leq d$, let $p_i=(c_{0,i},\dots,c_{n,i})$ where $c_{j,i}$ is the coefficient of $x^iy^{d-i}$ in $f_j$ (or we ignore $p_i$ if all $c_{j,i}$ are zero). Then the curve is contained in the projective subspace spanned by all $p_i$.","['algebraic-geometry', 'algebraic-curves']"
407517,The sine inequality $\frac2\pi x \le \sin x \le x$ for $0<x<\frac\pi2$ [duplicate],"This question already has answers here : Mean Value Theorem: $\frac{2}{\pi}<\frac{\sin x}{x}<1$ (3 answers) Closed 6 years ago . There is an exercise on $\sin x$. How could I see that for any $0<x< \frac \pi 2$, $\frac 2 \pi x \le \sin x\le x$? Thanks for your help.","['trigonometry', 'inequality', 'functions']"
407539,What is the difference between $\ell$-adic cohomology and cohomology with coefficient in $Z_\ell$?,"Let $X$ be a non-singular projective variety over $\mathbb{Q}$. Consider on the one hand $H^i_B(X(\mathbb{C}),\mathbb{Z}_\ell)$ the singular cohomology with value in $\mathbb{Z}_\ell$, and on the other hand $\varprojlim H^i_B(X(\mathbb{C}),\mathbb{Z}/\ell^n\mathbb{Z})$. Are these two groups equal ? If so why. Motivation : the comparison theorem between étale and singular cohomology states that
$$H^i_{ét}(X,\mathbb{Z}/\ell^n\mathbb{Z}) \simeq H^i_B(X(\mathbb{C}),\mathbb{Z}/\ell^n\mathbb{Z}),$$
hence there is an isomorphism of the $\ell$-adic cohomology
$$H^i(X,\mathbb{Z}_\ell):= \varprojlim H^i_{ét}(X,\mathbb{Z}/\ell^n\mathbb{Z}) \simeq\varprojlim H^i_B(X(\mathbb{C}),\mathbb{Z}/\ell^n\mathbb{Z}).$$
I was wondering why this implies that $H^i(X,\mathbb{Z}_\ell) \otimes_{\mathbb{Z}_\ell} \mathbb{C}$ is isomophic to $H_B^i(X(\mathbb{C}),\mathbb{C}) \cong H_B^i(X(\mathbb{C}),\mathbb{Z}) \otimes_\mathbb{Z} \mathbb{C} \cong H_{dR}^i(X(\mathbb{C}),\mathbb{C})$. I know that the $\ell$-adic cohomology is different from the étale cohomology of the constant sheaf $\mathbb{Z}_\ell$ (however I don't know why). I will be glad if moreover someone can point out a reference about this (I have looked at Milne's notes and book and Lei Fu's book, but they don't talk about this). Edit: I might rather ask this question instead. Where can I find a proof of the isomorphisms $H^i(X_{\bar{\mathbb{Q}}},\mathbb{Z}_\ell) \otimes \mathbb{C} \simeq H^i_{dR}(X(\mathbb{C}),\mathbb{C})$.","['homology-cohomology', 'sheaf-theory', 'algebraic-geometry', 'etale-cohomology']"
407545,Derivative map of the diagonal inclusion map on manifolds,"I was trying to work through a problem(#10 of $\S$1.2) in Guillemin and Pollack's book $\textit{differential topology. }$ The problem is given as follows. Let $f: X\longrightarrow  X\times X$ be the mapping $f(x)=(x,x)$.
  Check that $df_x(v)=(v,v)$. Here $X\subset \mathbf R^m$ is a manifold. My attempt so far has been: First we parametrise an open neighbourhood of $x\in X$ and $(x,x) \in X\times X$ locally by $\phi$ and $\phi \times \phi$ into open subsets $U\subset\mathbf R^m$ and $U\times U$ (we use $\phi(0)=x$ for simplicity). This gives the commuting diagram as follows: $$
\begin{array}[c]{ccc}
X\;\;&\stackrel{f}{\longrightarrow}&X\times X\\
\downarrow\scriptstyle{\phi}&&\downarrow\scriptstyle{\phi \times \phi}\\
U\;\;&\stackrel{h}{\longrightarrow}&U\times U
\end{array}
$$ $$
\begin{array}[c]{ccc}
T_x(X)&\stackrel{df_x}{\longrightarrow}&T_{(x,x)}(X\times X)\\
\downarrow\scriptstyle{d\phi_0}&&\downarrow\scriptstyle{d\phi_0 \times d\phi_0}\\
\mathbf R^m\;\;&\stackrel{dh_0}{\longrightarrow}&\mathbf R^m \times \mathbf R^m
\end{array}
$$ According to the definition (or the commuting diagram above), $df_x=(d\phi_0 \times d\phi_0) \circ dh_0 \circ d\phi_0$. However I have no idea how to proceed after that. If I want to calculate $df_x$, I have to know what $d\phi_0$ is first... But since we let $\phi(0)=x$, what would the derivative map of that be (since $\phi(0)=x$ just means we send a specific point $0$ to a specific point $x$, it doesn't tell us anything about the expression of this parameterisation)? Thanks everyone for the help!","['differential-geometry', 'general-topology', 'manifolds', 'real-analysis', 'differential-topology']"
407554,Computing the Frobenius normal form,"I was wondering whether someone could give me an example how one actually determines the Frobenius normal form of a given matrix. Further, it seems hard to find an example where the new basis is calculated so that a given matrix is in Frobenius normal form. I really tried to find an example where this is done, but most books just use this form as a theoretical example rather than actually calculating this form? A short summary how you would proceed to calculate the Frobenius normal form and the basis would be more than enough, too.","['matrices', 'linear-algebra', 'abstract-algebra']"
407601,Evaluating $\iint_s \vec F \cdot \hat n ds $ around the curved surface of cylinder cut by plane at $45^\circ $,"I need to calculate the surface integral of $F(x,y,z) = \hat i x +\hat j y + \hat k z$ on the curved part of surface $x^2+z^2 = 1, x+y=2, $ and $y$ goes from $1$ to $3$ as shown in following figure. How do I evaluate $\displaystyle \iint_S \vec F .\hat n ds$
 this surface? EDIT ::I couldn't do it via parametrization, I got the above figure which is incorrect. Using this formula $\iint_s \vec F \cdot \frac{\nabla \phi }{|\nabla \phi|}\sqrt{1 + (z_x)^2 + (z_y)^2} dx dy$ I got the following. Not sure if it's correct. 
$$\int_1^3 \;dy \int_{-1}^{2-y} \vec F(x, y , \sqrt{1-x^2})\cdot \frac{x \hat i + \sqrt{1-x^2}\hat k}{\sqrt{1-x^2}} dx  \\ + \int_1^3 \;dy \int_{-1}^{2-y} \vec F(x, y , -\sqrt{1-x^2})\cdot \frac{x \hat i - \sqrt{1-x^2}\hat k}{\sqrt{1-x^2}} dx$$",['multivariable-calculus']
407623,Horn and spindle tori,"I was trying to prove that the horn torus and the spindle torus are not manifolds by definition(locally diffeomorphic to some Euclidean space.). I have no idea how to do this, but I attempted it in the following way: I failed to show that a ""slice"" of manifold is a manifold itself. By a slice I mean, if the manifold $X\in \mathbf R^n$, then we set the coordinates $x_i=0$ for some $i$ where $0\leq i\leq n$. I feel this would work but I have no idea how to prove it. Then I look at the cross section for the tori I mentioned(this is equivalent to taking a slice.). For the horn torus, you have two circles touching each other. And the fore spindle torus, you have two circles intersecting each other. Since a circle is locally diffeomorphic to $\mathbf R^1$, the two circles better have to be diffeomorphic to $R^1$. Otherwise we get the result we want. I tried to proof that a neighbourhood around the point they touch(or intersect) cannot be locally diffeomorphic to $\mathbf R^1$. I tried to do it by contradiction. However I'm stuck on finding a contradiction... Any thoughts? P.S. Under the request of Sam Lisi, here are the definitions or horn and spindle tori: A torus is the set of points in $\mathbf R^3$ at a distance $b$ from the circle of radius a in the $xy$ plane. This is like you put a circle with radius $b$ in the $yz$ plane centred at $(a,0,0)$. Then you make it orbit around the origin and you get a torus. A horn torus is when $a=b$. If you take a cross-section, you'll find that it's two circles touching each other at one point. When $a<b$, it's called a spindle torus. The cross-section would look like two circles intersecting with each other at two pionts. There are pictures in this wikipedia article that might help you visualise the horn and spindle tori: http://en.wikipedia.org/wiki/Torus","['geometry', 'differential-geometry', 'general-topology', 'manifolds', 'real-analysis']"
407638,Rank of idempotent matrices,"Let $B_1, B_2, \dots, B_k$ be idempotent matrices, i.e., $B_i^2=B_i$. Can we prove that $$\mbox{rank}(I-B_1\cdots B_k)\leq \sum\limits_{i=1}^k \mathrm{rank}(I-B_i)$$ where $I$ is the identity matrix?","['projection-matrices', 'matrix-rank', 'inequality', 'linear-algebra', 'idempotents']"
407654,"If $\lim\limits_{x\rightarrow\infty} (f'(x)+f(x)) =L<\infty$, does $\lim\limits_{x\rightarrow\infty} f(x) $ exist?","I want to prove or disprove this problem:
If there exist $\lim\limits_{x\rightarrow \infty} (f'(x)+f(x))=L<\infty$ then $\lim\limits_{x\rightarrow\infty} f(x) =L$. When I assume problem below: If there exist $\lim\limits_{x\rightarrow\infty} (f'(x)+f(x)) =L<\infty$, There exists $\lim\limits_{x\rightarrow\infty} f(x)$? I can use mean-value theorem to show that. So my question is: If $\lim\limits_{x\rightarrow\infty} (f'(x)+f(x))=L<\infty$, does $\lim\limits_{x\rightarrow\infty} f(x)$ exist?","['calculus', 'limits']"
407658,Sum of Closed Operators,"If $A$ and $B$ are two closed operators on a Hilbert space (not defined everywhere), is their sum closed as well? I think not, but cannot construct a counterexample. Some posts on this site do address this but they are not complete. For example, Sum of Closed Operators Closable? gives two operators and shows that their sum is not cloasable, but does not show that the operators themselves are closed. There is another post saying the same thing Counterexample for ""the sum of closed operators is closable"" but the question is still unresolved. Could someone give a different, simpler counterexample?","['operator-theory', 'functional-analysis']"
407696,Solving $\;x+y+z =8 ; \;\;\sqrt{x^2+1}+\sqrt{y^2+4}+\sqrt{z^2+9}=10 $,"Solve the problem \begin{cases}x+y+z =8 \\ \\
\sqrt{x^2+1}+\sqrt{y^2+4}+\sqrt{z^2+9}=10 
\end{cases} with $(x,y,z) \in \mathbb R^3$ I have already solved it, but I'd like to see others creative solutions and before all, share this funny problem with the community.","['algebra-precalculus', 'recreational-mathematics']"
407710,Morphism of finite type between affine schemes is quasi-projective,"I want to prove that given $A \to B$ a ring homomorphism of finite type, then the induced morphism of schemes $X \to Y$ is quasi-projective. A morphism is quasi-projective if it factors into an open immersion followed by a projective morphism. My attempt is as follows: $B= A[X_0,\dots,X_n ] / I$, so there is a closed immersion $Proj B \to \mathbb{P}^n_{A}$; composing it with the projection $\mathbb{P}_A^n \to Spec A$, we have a projective morphism $Proj B \to SpecA$. I tried to find an open immersion $Spec B \to Proj B$ but I couldn't. Is this the right approach for this problem ?","['algebraic-geometry', 'schemes']"
407747,"What is the use of, and intuition behind, writing $\frac{d^2}{dx^2}$ for the second derivative?","Is it possible to take a second derivative without taking the first derivative before? Why do we multiply the $d$ and $dx$ operators? Like, does $\dfrac{d^2}{dx^2}$ really mean $\dfrac{d}{dx} \cdot \dfrac{d}{dx}$? What's the intuitive understanding about this? Can it be represented in a graph? Like... 'Little change squared in $y$ over little change squared in $x$'?","['notation', 'intuition', 'derivatives']"
407767,Product topology with finer/coarser comparison,"There is a question in Munkres' Topology which has me a little confused: Let X have topologies $\mathfrak{T}$, $\mathfrak{T'}$, and Y have a topologies $\mathfrak{U}$,$\mathfrak{U'}$. Show that if $\mathfrak{T} \subset \mathfrak{T'}$ and $\mathfrak{U} \subset \mathfrak{U'}$, then the product topology on X$\times$Y under $\mathfrak{T'},\mathfrak{U'}$ (I'll denote by $\mathfrak{T'}\ast\mathfrak{U'}$) is finer than the product topology under $\mathfrak{T},\mathfrak{U}$ (I'll denote by $\mathfrak{T}\ast\mathfrak{U}$). Is the converse true? So the first part was fine. The second I thought might be true and proved it, but I immediately found a counterexample. My question is where is the wrong step in the ""proof"" I got down. Let $\mathfrak{B},\mathfrak{B'}$ be bases for $\mathfrak{T},\mathfrak{T'}$, respectively, and $\mathfrak{C},\mathfrak{C'}$ for $\mathfrak{U},\mathfrak{U'}$, respectively. For $x\in B\in \mathfrak{B}$ and $y\in C\in \mathfrak{C}$, then $(x,y)\in B\times C$ which is a basis element for $\mathfrak{T}\ast\mathfrak{U}$. Since it's finer, there is a basis element $U\times V$ of $\mathfrak{T'}\ast\mathfrak{U'}$ such that $(x,y)\in U\times V\subset B\times C$ ($U$ open in X, $V$ open in Y under $\mathfrak{T'},\mathfrak{U'}$, resp). So $x\in U, y\in V$, and since they are open, there exist $B'\in \mathfrak{B'}, C'\in \mathfrak{C'}$ with $x\in B'\subset U$ and $y\in C'\subset V$. Since $U\subset B$ and $V\subset C$ then $x\in B'\subset B$, $y\in C'\subset C$. Then $\mathfrak{T'}$ is finer than $\mathfrak{T}$, and $\mathfrak{U'}$ is finer than $\mathfrak{U}$. As I said, I know something must be wrong. I know it's stupid, but I actually cannot find the wrong step here. Thanks in advance. EDIT: the counter-example I found was: X= {a,b,c}, Y={1,2,3}, $\mathfrak{T}$={X,{a},{a,c}}, $\mathfrak{T'}$={X,{a},{b},{a,b}}, $\mathfrak{U}$={Y,{1}}, $\mathfrak{U'}$={Y,{1},{2},{1,2}}. Of course, now I see this just confirms the contrapositive of the converse, so I was wrong about being wrong...",['general-topology']
