question_id,title,body,tags
1521436,Are there different ways to embed surface with nonvanishing curvature in a higher-dimensional Euclidean space?,"The two-dimensional Euclidean space $E^2$ can be embedded isometrically in many ways in $E^3$. Its First Fundamental form will always be $\delta_{\mu\nu}$, but the Second Fundamental form $II$ will depend on the embedding. If it is embedded as a plane, then $II$ vanishes, but if it's a wave or a cylinder (never mind the global topology), then $II$ and at least one principal curvature are non-zero. I find it difficult to imagine the same thing for spaces with non-vanishing intrinsic curvature. For example the sphere $S^2$, I can't imagine it being embedded in different ways in $\mathbb{R}^3$. Of course you can always move the sphere around, but I suspect any other operation (scaling it, skew it etc.) would affect the First Fundamental form. Is that true? Or is it possible to alter the Second Fundamental form of embedded surfaces without changing its First Fundamental form? If it's possible, are there any examples? EDIT: Intuitively spoken, a sheet of paper can be bent, because its curvature is zero. The bending will change $II$, but not $I$. But if you bend a sphere made of paper, I guess you will also bend its First Fundamental form. Right?","['metric-spaces', 'differential-geometry']"
1521466,The identity $\tan({\pi\over4}-{a\over2}) = \sec(a)-\tan(a)$,"Today I was solving an exercise and while checking the solution on WolframAlpha, the website used the following identity:
$$\tan \left({\pi\over4}-{\alpha\over2} \right) = \sec(\alpha)-\tan(\alpha)$$
Since I've never seen that formula, I tried to calculate it with the identities I know, especially using
$$\tan \left({\pi\over4}-{\alpha\over2} \right) = {(1-\tan({\alpha\over2}))\over(1+\tan({\alpha\over2}))},$$
but I couldn't make it work. Is this a known property of trigonometry, like for example $\cos({\pi\over2}-x)=\sin(x)$?",['trigonometry']
1521524,Difference between fractions at group level have different sign than difference between fractions in aggregate,"I have obtained a result (perhaps incorrectly; we shall find out) that appears paradoxical. Suppose I am interested in comparing fractions between 'groups' (not in the strict mathematical sense) $\alpha$ and $\beta$.  Within these groups, there are two subgroups (which are present in both $\alpha$ and $\beta$), denoted $y$ and $z$. Without loss of generality and for sake of argument, suppose that we're interested in United States baseball batting averages , $AVG$, and have data on hits $H$ and at-bats $AB$. Specifically, $(H_{\alpha,y}, AB_{\alpha,y}) = (34836, 268206) \Rightarrow AVG_{\alpha,y} = \frac{34836}{268206} = 0.129885237 \ldots$ $(H_{\alpha,z}, AB_{\alpha,z}) = (81311, 366970) \Rightarrow AVG_{\alpha,z} = \frac{81311}{366970} = 0.221573970 \ldots$ $(H_{\beta,y}, AB_{\beta,y}) = (33463, 253042) \Rightarrow AVG_{\beta,y} = \frac{33463}{253042} = 0.132242868 \ldots$ $(H_{\beta,z}, AB_{\beta,z}) = (69498, 312624) \Rightarrow AVG_{\beta,z} = \frac{69498}{312624} = 0.222305389 \ldots$ Note the differences in $AVG$ between groups with the same subgroup, $d_{y} = AVG_{\alpha,y} - AVG_{\beta,y} = -0.0024 \ldots$ $d_{z} = AVG_{\alpha,z} - AVG_{\beta,z} = -0.0007 \ldots$ However, the difference in $AVG$ between groups, without regard for subgroup has a sign that is not intuitive for me, $d_{y+z} = AVG_{\alpha, y+z} - AVG_{\beta, y+z} = \frac{(H_{\alpha,y} + H_{\alpha,z})}{(AB_{\alpha,y} + AB_{\alpha,z})} - \frac{(H_{\beta,y} + H_{\beta,z})}{(AB_{\beta,y} + AB_{\beta,z})} =  +0.0008 \ldots$ My expectation is that $d_{y+z} \in [\min{(d_y, d_z)}, \max{(d_y, d_z)}]$ Why is the difference in $AVG$ between groups not bounded by the subgroup-level differences?","['paradoxes', 'fractions', 'statistics']"
1521552,How do you square an ideal?,"For some of you, this question is going to seem extremely basic. I think I understand what an ideal is. Such as $\langle 2, 1 + \sqrt{-5} \rangle$, it consists of all numbers in this ring of the form $2a + (1 + \sqrt{-5})b$. But then what is $\langle 2, 1 + \sqrt{-5} \rangle^2$? My first thought was $\langle 4, -4 + 2 \sqrt{-5} \rangle$, but that seems wrong somehow. I also had something in the back of my mind saying $\langle 2 \rangle$, but I'm not sure about that one either. Then I thought about trying to figure out $\langle 2, 1 + \sqrt{-5} \rangle \langle 2, 1 + \sqrt{-5} \rangle$ when I realized I don't actually understand how to multiply ideals to begin with. I'm only using $\langle 2, 1 + \sqrt{-5} \rangle$ as an example (though that does draw in one question identified as similar that looks much more relevant than all the questions identified as ""Questions that may already have your answer""). In a principal ideal domain, would it be correct to think that $\langle a \rangle \langle b \rangle = \langle ab \rangle$? Any help would be much appreciated.","['abstract-algebra', 'ideals', 'ring-theory']"
1521574,Integral domain without prime elements which is not a field,"I was asked whether I knew a ring without prime elements which is not a field. The first thing I thought of was the Cartesian product of fields with component-wise addition and multiplication. But now I am looking for a ring which does not have any zero-divisors. I could not think of one. So does anyone know an integral domain (commutative ring without zero divisors) which is not a field and has no prime elements? My first idea was adjoining elements to a known ring such as $\mathbb{Z}[\alpha]$. But then the problem is that I always find some prime in $\mathbb{Z}$, which is also a prime in this new ring.","['abstract-algebra', 'integral-domain', 'ring-theory']"
1521603,"Real Analysis, Folland problem 2.13. Integration of Nonnegative functions [duplicate]","This question already has an answer here : Prove that $\int_{E}f =\lim \int_{E}f_{n}$ (1 answer) Closed 3 years ago . Suppose $\{f_n \}\subset L^{+}$, $f_n\rightarrow f$ pointwise, and $\int f = \lim\int f_n < \infty$. Then $\int_{E}f = \lim\int_{E}f_n$ for all $E\in M$. However, this need not be true if $\int f = \lim\int f_n = \infty$ Proof: Let $E\in M$, by Fatou's Lemma $$\int_{E}f = \int f 1_{E} = \int\lim_{n\rightarrow \infty}\inf f_n 1_{E} \leq \lim_{n\rightarrow \infty}\inf\int f_n 1_{E} = \lim_{n\rightarrow \infty} \inf\int_{E}f_n$$ Similarly, $$\int_{E^{c}}f\leq \lim_{n\rightarrow \infty}\inf\int_{E^{c}}f_n$$ Note, $f = \int f 1_{E} + f 1_{E^{c}}$ and $f_n = f_n 1_{E} + f_n 1_{E^c}$ for all $n\in\mathbb{N}$. This implies, $\int f_{E^c} = \int f - \int_{E} f$ and (for $n$ large) $\int_{E^c}f_n = \int f_n - \int_{E} f_n$. Therefore, $$\int f - \int_{E}f = \int_{E^c}f\leq \lim_{n\rightarrow \infty}\int_{E^c}f_n = \lim_{n\rightarrow\infty}\left(\int f_n - \int_{E} f_n\right) = \int f - \lim_{n\rightarrow \infty}\int_{E}f_n$$ Cancelling $\int f$ from both sides (which is allowed since $\int f = \lim\int f_n < \infty$)   we get $$\int_{E} f = \lim_{n\rightarrow \infty}\int_{E}f_n$$ I have not completed the second part yet, I just wanted to make sure this is correct, any suggestions is greatly appreciated especially for the second part.","['proof-verification', 'real-analysis', 'measure-theory']"
1521605,Geometrical interpretation of the Triple Product Rule,"Most of us who have done multivariable calculus are familiar with the rule
$$ \left( \frac{\partial x}{\partial y} \right)_{z} \left( \frac{\partial y}{\partial z} \right)_{x} \left( \frac{\partial z}{\partial x} \right)_{y} = -1, $$
particularly useful in thermodynamics. The normal way to prove this (as shown in either of these Wikipedia articles is by mucking about with the algebra of differentials and substituting, and there's nothing as geometrically unintuitive as pushing algebraic symbols around. The $-1$ is also famously counterintuitive to people seeing it for the first time. Therefore, is there a nice intuitive geometrical derivation or interpretation of this result? (And, indeed, its more-variabled generalisations?)","['partial-derivative', 'multivariable-calculus']"
1521718,Are there any situations in which L'Hopital's Rule WILL NOT work?,"Today was my first day learning L'Hopital's Rule, and I was wondering if there are any situations in which you cannot use this rule, with the exception of when a limit is determinable.","['limits-without-lhopital', 'calculus', 'examples-counterexamples']"
1521729,Convex hull in projective space,"Let $S \subseteq \mathbb R\mathbb P^n$ be a closed connected set that does not intersect every hyperplane. If I choose any affine chart containing $S$, I can consider its convex hull, and it seems reasonable that this operation should not depend on the particular chart. Is there some good way to see this? Thanks","['projective-geometry', 'convex-analysis', 'general-topology']"
1521759,"Please Explain a simple Formula, calculating time in-between a call queue","I have a simple algebra formula, proven to work. But I need help in understanding why it works. The Scenario: I work at a call center, and am trying to calculate the time free in-between calls. I have the 3 variables, provided by Live data: Staff Available (not on calls) Staff Busy (on calls) Average call length of 5 minutes So once I end a call, I go to the back of the line of available staff before I get the next call. This is the formula, tested to work: : (Staff Available / Staff Busy) * 5 minutes call length = Time in-between calls Example: 100 staff. 80 busy, 20 available. [20/80 * 5 = 1.25 minutes] Example: 100 staff. 50 busy, 50 available. [50/50 * 5 = 5 minutes] (Which is expected, as we are double staffed.) Example: 100 staff. 20 busy, 80 available. [80/20 * 5 = 20 minutes] Question - Why does this equation work? I must be taking shortcuts. Why do we divide Available/Busy instead of Available/Total? I'd greatly appreciate any explanation. Thank you very much.
-Brennan",['statistics']
1521776,"express prime as sum of squares, $p = a^2 + b^2$","Espress $2017$ as sum of two squares. attempt: by Fermat's Theorem on sums of squares, the prime $p = 2017$ is the sum of two squares $2017 = a^2 + b^2$ , $a,b \in \mathbb{Z}$, if and only if $p \equiv 1 mod 4$. And The irreducible elements in the Gaussian integers $\mathbb{Z[i]}$ are as follows
$(a + bi)(a - bi) $ for primes $p\in \mathbb{Z}$ with $p \equiv 1 mod 4$ (both of which have norm $p$). Then since $2017 \equiv 1 (mod 4)$ Then $2017 = a^2 + b^2$ . Notice that $\sqrt2017 $ is approximately $44.91$. So $a^2, b^2 $
will be between values $1,2^2,....,44^2$ . Then plugging different values from the above squares in  $2017 - a^2 = b^2$ we find $2017 - 44^2 = 81 = 9^2$ So $2017 = 44^2 + 9^2$. However, I found them using that approach.  But is there a way to find them without doing this approach? I dont' know how to use $p = a^2 + b^2 = (a + bi)(a - bi) $ for primes $p\in \mathbb{Z}$ with $p \equiv 1 mod 4$ (both of which have norm $p$). So $2017 = a^2 + b^2 = (a+ bi)(a - bi) $. I don't' know how I would proceed assuming I would not have found the values .
Any feedback or better approach would be appreciated it. 
Thank you!","['abstract-algebra', 'number-theory', 'sums-of-squares', 'ring-theory']"
1521824,Is there a proof that the sum of the trihedral angles of a tetrahedron is minimal when the latter is regular?,"Since the sum of the 6 dihedral angles is always 1 sphere more than the sum of the 4 trihedral angles, both sums are maximized or minimized at the same time. I showed that for all four extremal tetrahedra that i can think of, the sum is greater than for a regular tetrahedron. I tried to prove the theorem by calculus, using Lagrange multipliers, but the calculation is totally intractable. Is there a simple proof, or any proof? It would seem the minimal and maximal sums should occur for the regular case, or for one of the four extremal tetrahedra (two flattened and two elongated), but I can't prove that.",['trigonometry']
1521838,Prove by induction that $21 | 4^{n+1} + 5^{2n-1}$,The problem that I have is: Prove by induction that 21 divides 4 n+1 + 5 2n-1 So far I have: Base Case: $n = 1$ $4^{1+1} + 5^{2-1} = 4^2 + 5^1 = 16 + 5 = 21$ Inductive Step: Assume: $4^{k+1} + 5^{2k-1} = 3m$ $4^{(k+1)+1} + 5^{2(k+1)-1} = 3m$ $4^{k+2} + 5^{2k+1} = 3m$ I'm pretty sure I am far off on this one and not sure where to go. Thanks,"['discrete-mathematics', 'proof-writing']"
1521841,Using L'Hopital to solve $\lim_{x\to +\infty}\frac{\frac{-1}{x^2}}{\sin^2\left(\frac{2}{x}\right)}$,"Use L'Hopital to calculate $$\lim_{x\to
 +\infty}\frac{\frac{-1}{x^2}}{\sin^2\left(\frac{2}{x}\right)}$$ Right now this yields $\frac{0}{0}$ so let's go ahead and use L'Hopital: $$\lim_{x\to
 +\infty}\frac{\frac{2}{x^3}}{2\cdot \sin\left(\frac{2}{x}\right)\cdot\cos \left(\frac{2}{x}\right)\cdot\left(\frac{-2}{x^2}\right)}$$ This just won't do. Perhaps we should flip the numerator with the denominator instead: $$-\frac{\csc^2\left(\frac{2}{x}\right)}{x^{2}}$$ This yields -$\frac{\infty}{\infty}$, so we can go ahead and apply L'Hopital: $$-\frac{2\cdot-\csc\left(\frac{2}{x}\right)\cdot\cot\left(\frac{2}{x}\right)\cdot\frac{-2}{x^2}}{2x}$$ If I evaluate this I will get $\frac{0}{\infty}$ I have the feeling I'm not supposed to keep going this path, and there's a simpler solution (using L'Hopital). What can I do to solve this?","['calculus', 'limits']"
1521911,"Partition of $S = \{1,2,\dots, 3n\}$ in to three subsets $A, B, C$ such that $|A| = |B| = |C| = n$","Let $n$ be a positive integer and consider the set $S = \{1,2,\dots ,3n\}$. Show that, for every partition of $S$ into 3 subsets $A, B, C$ such that: i) $|A| = |B| = |C| = n$ (here $|X|$ denotes the number of elements of set $X$) ii) $A \cap B = B \cap C = C \cap A = \varnothing$ and $A \cup B \cup C = S$ there exist 3 elements $a \in A, b \in B, c \in C$ such that one of $a, b, c$ equals sum of the two other elements.","['set-partition', 'combinatorics']"
1521951,How can I calculate p(x=0 or y=0) when the variance is maximized?,"The random variables $X$ and $Y$ have join probability mass function $p(x,y)$ for $x\in\{0, 1\}$ and $y\in\{0,1,2\}$.   Suppose that $3p(1,1) = p(1,2)$ and $p(1,1)$ maximises the variance of $XY$.   Calculate the probability that $X$ or $Y$ is zero. Then I proceed to make the table; $P(0,0)+P(0,1)+P(0,2)+P(1,0)+P(1,1)+ 3P(1,2) = 1$ $P(X=0)+P(Y=0)-P(X=0 , Y=0)+ 4 P(1,1)=1$ Therefore, $P(X=0 \cup Y=0)=1-4 P(1,1)$ My question is how can I calculate the probability that X or Y is zero when I maximizes the variance of XY? I post the graph: Extra question: Why is no 0(cero) the correct answer? if x=0.13 maximize the variance? Thanks.","['statistics', 'probability', 'calculus']"
1521959,Help with proof regarding generalization of a function: $f(x)=2x+1-2^{\lfloor \log_2x\rfloor+1}$,"Consider the below function: $$f(x)=2x+1-2^{\lfloor \log_2x\rfloor+1}$$
Let $f^x(x)$ refer to the composition of $f(x)$, $x$ number of times. Now after having observed some patterns, I've hypothesized that : 
$$f^x(x)=2^y-1$$
Where: $x=2^{a_1}+2^{a_2}\cdot \cdot + 2^{a_y}$ $\{a_1,a_2\cdot\cdot\cdot\cdot a_y\}$ All elements in this set are distinct, and $\in \{Z^++\{0\}\}$ (i.e. belong to the set of non-negative numbers). $x \in Z^+$ How can I go about proving this?","['logarithms', 'proof-verification', 'ceiling-and-floor-functions', 'functions']"
1521989,"a question of theorem 3.13 in Real and Complex Analysis, Rudin","Here is the Theorem 3.13 in rudin's real and complex analysis book， Theorem 3.13 Let $S$ be the class of all complex,measureable,simple
  functions on $X$ such that \begin{equation} \mu(\{x:s(x)\neq
 0\})<\infty  \tag{1} \end{equation} If $1\leq p<\infty$,then $S$ is
  dense in $L^{p}(\mu)$. Proof : First,it is clear that $S\subset
L^{p}(\mu)$.Suppose $f\geq 0,f\in L^{p}(\mu)$,and let $\{s_{n}\}$ be
  as in Theorem 1.17.Since $0\leq s_{n}\le f$, we have $s_{n}\in
 L^{p}(\mu)$, hence $s_{n}\in S$,Since $|f-s_{n}|^{p}\leq f^{p}$,the
   dominated convergence theorem show that $\|f-s_n\|_{p}\to 0$ as $n\to
 \infty$.Thus $f$ is in the $L^{p}$-closure of $S$.The general case ($f$
   complex) follows from this. Here I have a question: Why we have $s_{n}\in L^{p}(\mu)$? in the proof of rudin,he claim that $s_{n}$ is defined as Theorem 1.17.but in Theorem 1.17,he first put $\delta_{n}=2^{-n}$.then to each positive integer $n$ and each real number $t$ corresponds a unique integer $k=k_{n}(t)$ that satisfie $k\delta_n\leq t<(k+1)\delta_{n}$.Define
$$\varphi_{n}(t)=\left\{
  \begin{array}{ll}
    k_{n}(t)\delta_{n}, & \hbox{if $0\leq t<n$;} \\
    n, & \hbox{if $n\leq t\leq \infty$.}
  \end{array}
\right.$$
then define $s_{n}=\varphi\circ f$. but I can't see this definitition can show that $s_{n}\in S$.in fact we need to show that
\begin{equation}
\mu(\{x:s_{n}(x)\neq 0\})<\infty  
\end{equation}
and I can't figure it out.Can anyone who have read this book help me? Thank you in advance.",['real-analysis']
1522044,Showing $[(fg)h]_n = [f(gh)_n]$ in the linear algebra $F^\infty$,"I am studying the proof of associativity of the multiplication in a linear algebra, from Hoffman and Kunze's Linear Algebra , 2nd edition. I am not understanding one of the steps taken to simplify a summation. The linear algebra $F^\infty$ is the vector space of sequences in $F$. The elements of $F^\infty$ are of the form $f = (f_0,f_1,f_2,\dots)$. The product of $f,g \in F^\infty$ is defined by $$(fg)_n=\sum_{i=0}^nf_ig_{n-i},\,n=0,1,2,...$$ We then have (on page 118): $$\begin{align} [(fg)h]_n &= \sum_{i=0}^n(fg)_ih_{n-i} \\
&=\sum_{i=0}^n \left(\sum_{j=0}^if_jg_{i-j}\right)h_{n-i}. \end{align}$$ 
What I do not understand are the following two steps: $$\begin{align} \phantom{[(fg)h]_n} &=\sum_{i=0}^n \sum_{j=0}^if_{i}g_{i-j}h_{n-i} \\ &=\sum_{j=0}^nf_j\sum_{i=0}^{n-j}g_ih_{n-i-j}, \end{align}$$ 
which gives the result
$$\phantom{[(fg)h]_n}=\sum_{j=0}^nf_j(gh)_{n-j} = [f(gh)]_n.$$ I can see the substitution of $f_i$ for $f_j$ after expanding the sum, but I do not understand the next step. Am I supposed to be able to see these steps intuitively (without expanding the sum)? What is going on in the substitution of $n$ for $n-j$?","['linear-algebra', 'algebra-precalculus']"
1522057,Problem in a proof of Hartshorne,"My issue is in Theorem 8.8 and Lemma 8.9 of Chapter II concerning Kähler differentials. I have managed to ""relax"" Lemma 8.9 as follows : Let $A$ be a noetherian local integral domain with residue field $k$ and quotient field $K$. If $M$ is a finitely generated $A$-module and if $r = \dim_k M \otimes_A k \le \dim_K M \otimes_A K$, then the inequality is an equality and $M$ is free of rank $r$. The proof is exactly the same as outlined in Hartshorne, since the inequality is sufficient to complete the proof (the surjectivity of $K^r \to M \otimes_A K$ with $\dim_K M \otimes_A K \ge r$ shows that it has to be injective and the dimensions need to be equal). My problem comes in the proof of Theorem 8.8 : Let $B$ be a local ring containing a field $k$ isomorphic to its residue field (I noticed afterwards that they more precisely mean that the map $k \to B \to B/\mathfrak m$ is a field isomorphism). Assume furthermore that $k$ is perfect, and that $B$ is the localization of a finitely generated $k$-algebra. Then $\Omega_{B/k}$ is a free $B$-module of rank equal to $\dim B$ if and only if $B$ is a regular local ring. It is assumed that $B$ is the localization of a finitely generated $k$-algebra, say $A$. Let us assume for simplicity that $A$ is a domain (since it is an admissible context in the hypotheses). The theorem claims that $\Omega_{B/k}$ is free of rank $\dim B$ if and only if $B$ is a regular local ring. The proof goes fine until they claim that $\dim B = \mathrm{tr.deg}(K/k)$ ; this equality is false in general since it doesn't hold when $B = A_{\mathfrak p}$ where $\mathfrak p \in \mathrm{Spec}(A)$ is not maximal, because 
$$
\dim(A_{\mathfrak p}) = \mathrm{ht}(\mathfrak p) < \dim A = \mathrm{tr.deg}(K/k)
$$ 
(that last equality is using the Theorem (I,1.8A) that Hartshorne quoted directly after) and $K = Q(A) = Q(A_{\mathfrak p})$. But we still have $\dim B \le \mathrm{tr.deg}(K/k)$. A mistake in a book, I could live with it and try to find the proof. But here's where I get confused. Using that inequality and the rest of the proof, we get
$$
\dim_k \Omega_{B/k} \otimes_B k = \dim_k \mathfrak m/\mathfrak m^2 = \dim B \le \mathrm{tr.deg}(K/k) = \dim_K \Omega_{K/k} = \dim_K \Omega_{B/k} \otimes_B K,
$$
from which it follows that $\dim_k \Omega_{B/k}\otimes_B k= \dim_K \Omega_{B/k} \otimes_B K$ by the Lemma, and in particular that $\dim B = \mathrm{tr.deg}(K/k)$. Which is now confusing since I know this to be generally false. Question : Where is the flaw in this argument?","['algebraic-geometry', 'commutative-algebra']"
1522063,Counter Example for Going Down Theorem,"Currently I'm reading  going down theorem from Atiyah and Macdonald's Commutative Algebra text. I'm trying to find a counterexample if the conditions of the theorem are not satisfied. There is a hint in my class notes about counterexample but I'm unable to complete this. Hint goes as follows: Let $R= \dfrac{\mathbb R[x,y,z]}{(y^2-x^2-x^3)}$. Let $S$ be the integral closure of $R$ in its quotient field. Show that going down theorem does not hold between $R$ and $S$. I've computed the ring $S$ and this turns out to be the ring $S=R[\frac {y}{x}]$. Any hints/ideas to ""guess"" the chain of prime ideals for which GD theorem does not hold.","['algebraic-geometry', 'commutative-algebra']"
1522082,Logical formula of definition of linearly dependent,"A subset $S$ of a vector space $V$ is said to be linearly dependent if there exist a finite number of distinct vectors $x_1, \ldots , x_n$ in $S$ and scalars $a_1 , \ldots ,a_n$ not all zero, such that
    $$ a_1 x_1 + \cdots + a_n x_n =0 $$ I want to translate this definition into logical formula and find the formula of linearly independence. Here is what I tried. $$ \exists [ x_1 ,\ldots , x_n \in S : x_i \not = x_j] \exists [a_k \in F : a_k \not = 0] : a_1 x_1+\cdots + a_n x_n = 0 $$ Now to obtain the definition of linearly independence, I deny the above. $$ \forall [x_1 ,\ldots , x_n \in S : x_i = x_j] \forall [a_k \in F : a_k = 0] : a_1 x_1+\cdots + a_n x_n \not =0 $$ This is obviously absurd. What is wrong and what is the exact translation?","['linear-algebra', 'predicate-logic']"
1522112,prove $f(x)=\sum_{k=1}^{+\infty}\frac{1}{n}\cos^{n}x\sin(nx)$ is $\mathcal{C}^{1}(\mathbb{R}-\pi\mathbb{Z})$,"let $f$ be a real valued function of a real variable defined by: $$f(x)=\sum_{k=1}^{+\infty}\frac{1}{n}\cos^{n}x\sin(nx)$$ Prove that $f(x)$ is $\displaystyle \mathcal{C}^{1}(\mathbb{R}-\pi\mathbb{Z})$ and calculate $\dfrac{df}{dx}$ My thoughts: at points of $\pi\mathbb{Z}$ we have $ \sin(nx)=0$ so there is no problem to set the  infinite series. On an interval $[a,b]$ included in $\mathbb{R}\setminus\pi\mathbb{Z}$ we can show that $|\cos x|\leq k$ for some constant $k<1$ which proves the existence of the sum and must be sufficient to apply the term by term derivation theorem series The union of all these intervals is $\mathbb{R}\setminus\pi\mathbb{Z}$","['sequences-and-series', 'calculus', 'real-analysis', 'derivatives']"
1522127,"Show that the sequence {$\sqrt{5},\sqrt{5+\sqrt{5}},\sqrt{5+\sqrt{5+\sqrt{5}}},....$} is convergent using monotone convergence theorem","QUESTION: Show that the sequence {$\sqrt{5},\sqrt{5+\sqrt{5}},\sqrt{5+\sqrt{5+\sqrt{5}}},\sqrt{5+\sqrt{5+\sqrt{5+\sqrt{5}}}},....$} is convergent and it converges to $\left(\frac{1+\sqrt{21}}{2}\right)$. MY ATTEMPT: The sequence takes the form of the recurrence $x_n=\sqrt{x_{n-1}+5}$. But neither can I show it to be monotonic increasing nor bounded. Once I have shown it to be convergent, I know how to find and show the limit. I have successfully done it too. But I cannot prove the convergence. ONLY HINTS required. P.S. Do not use Cauchy principle or any complicated test. I want the answer to be based on monotonicity and boundedness. For the problem belongs to that chapter only.","['sequences-and-series', 'convergence-divergence', 'real-analysis']"
1522131,Significance of having closed range for an operator?,One thing I think is of great importance is that it seems to correspond to possibilities similar to rank nullity theorem and further the fredholm alternative much as in finite dimension. Is this a correct observation and are there other direct significant consequences to having closed range or not having it? Some literature and comments regarding this would be greatly appreciated.,"['functional-analysis', 'soft-question']"
1522138,derivative of kronecker product,"Given $x \in \mathbb{R}^N$ and a function $$H = \sum_{i,j,k=1}^n\ J_{i,j,k}\ x_i x_j x_k$$ for a fixed $J \in \mathbb{R}^{n \times n \times n}$, I am trying to calculate the derivative $\frac{d H}{d x}$ and a little lost in the expressions. Is there is a clean formula to do so? In the quadratic case, if $J$ is symmetric, the derivative is simply $J x$, is the expression simpler if $J_{i,j,k}$ is made symmetric, e.g., by adding all the 6 permutations together? In general, if we have
$$
H = \sum_{i_1,i_2 \ldots i_p=1}^n\ J_{i_1,i_2 \ldots i_p}\ x_{i_1} \ldots x_{i_p}
$$
can we find a formula for the derivative?","['multilinear-algebra', 'kronecker-product', 'linear-algebra', 'multivariable-calculus']"
1522157,Are all instances of torsion special cases of the same concept?,"The concept of 'torsion' pervades mathematics. As far as I know the origin of the word is in algebraic topology where it was used to describe chains $\gamma$ which are not boundaries but such that $2\gamma$ are boundaries. Then there's torsion in general abelian groups, rings, and modules. There's torsion in differential geometry , and analytic torsion . Lastly, there's $\mathrm{Tor}$, the left derived functor of the tensor product which is defined at least in the case of modules. The lower dimensional $\mathrm{Tor}$ functors tell us about torsion. I don't understand what the higher ones do, but this bridge does exist. So the tensor product over of modules does poop out torsion from high above. In differential geometry, the torsion form is often identified with a section of $TM\otimes \Lambda ^2T^\ast M$, called the torsion tensor. So formally, the tensor product pops up here too. Unfortunately The definition of analytic torsion is beyond me entirely. To what extent can these concepts be unified, seen as special cases of each other, or obtained from abstract nonsense?","['big-picture', 'derived-functors', 'differential-geometry', 'algebraic-topology', 'intuition']"
1522172,Prove that such a function exists that mimics binomial theorem,"I would like to see if someone can show to me that a function exists such that:
$$
(f(x)-1)^k = \sum_{j=0}^k {k \choose j}(-1)^{k-j} g(j,k,x)
$$
Basically what im trying to say is im trying to find non-trivial solutions that are interesting. This means the following solutions are trivial:
\begin{align*}
g(j,k,x) =  f(x)^{j}
\\
g(j,k,x) = (-1)^k f(x)^{k-j}
\\
g(j,k,x) = 0^{k-j} (f(x)-1)^j
\\
g(j,k,x) = c(k,j)(f(x)\pm m(k,j))^{b(k,j)}
\end{align*}
Where $c(k,j),m(k,j),b(k,j)$ are some functions with the variables $j,k$ such that would be nonpathological.
 Any solutions that are similar to those listed above are considered trivial in this instance.Also we are dealing with real numbers here. I have already come up with my own solution to this problem but I would like to see if either someone can prove to my the uniqueness of my solution, or to prove that more than one of such functions exist. Here is how I derived my solution: It is known that:
$$
\left(f(x+z)-f(x)\right)^k = \sum_{n=k}^\infty B_{n,k}^f(x) \frac{z^n}{n!}
$$ This is shown in the following paper . Where $B_{n,k}^f(x)$ is the partial Bell Polynomial with respect to the function $f(x)$, this can also be viewed as:
\begin{align*}
B_{n,k}^f(x) = \sum_j \frac{n!}{j_1!j_2!\cdots j_{n-k+1}!} \prod_{m=1}^{n-k+1} \left(\frac{f^{(m)}(x)}{m!}\right)^{j_m}
\end{align*}
Where the sum $j$ runs through all the partitions that satisfy the following linear system:
\begin{align*}
n = \sum_{m=1}^{n-k+1} mj_m
\\
k = \sum_{m=1}^{n-k+1}j_m
\end{align*}
With that being said, it can be derived that:
\begin{align*}
B_{n,k}^{x^{-v}}(x) = \frac{n!}{k!} x^{-vk-n} \sum_{j=0}^k {k \choose j}(-1)^{k-j} {-vj \choose n}
\end{align*}
By implementing this into out definition we have:
\begin{align*}
\frac{\left((x+1)^{-v}-x^{-v}\right)}{k!} = \frac{x^{-vk}}{k!}\sum_{n=0}^\infty x^{-n-k} \sum_{j=0}^k {k \choose j}(-1)^{k-j} {-vj \choose n+k}
\\
= \frac{x^{-vk}}{k!} \sum_{j=0}^k {k \choose j}(-1)^{k-j} \left(\sum_{n=0}^{\infty} x^{-n-k} {-vj \choose n+k}\right)
\end{align*}
I am not going to show you proof by hand that this converges, I will let wolfram help you with that one:
\begin{align*}
\sum_{n=0}^{\infty} x^{-n} {-vj \choose n} = \left(\frac{1}{x}\right)^k {-vj \choose k} {}_2F_1\left(1,k+vj;k+1,-\frac{1}{x}\right)
\end{align*}
Therefore:
\begin{align*}
\left(\left(1+\frac{1}{x}\right)^{-v}-1\right)^k = x^{-k} \sum_{j=0}^k {k \choose j}(-1)^{j} {k+jv-1 \choose k} {}_2F_1\left(1,k+jv;k+1;\frac{-1}{x}\right)
\end{align*}
Notice that the monment when $j=0$, the resulting term is zero, therefore:
\begin{align*}
\left(\left(1+\frac{1}{x}\right)^{-v}-1\right)^k = x^{-k} \sum_{j=1}^k {k \choose j}(-1)^{j} {k+jv-1 \choose k} {}_2F_1\left(1,k+jv;k+1;\frac{-1}{x}\right)
\end{align*}
This is an example of a solution that is non-trivial in the sense that is defies intuition, note that $v > 0$ and $k >0$, convergence is absolute when $\lvert \frac{1}{x}\rvert < 1$. This part is for Morgan Rodgers. Lets assume that the solution i have come up with is considered a trivial solution under the pretenses i have set, then the following is true:
\begin{align*}
x^{-k} {-jv \choose k} {}_2F_1\left(1,k+jv;k+1;-\frac{1}{x}\right) = c(k,j) \left(f(x)+m(j,k)\right)^{b(j,k)}
\end{align*}
noting that in the following case, $f(x) = \left(1+\frac{1}{x}\right)^{-v}$ therefore we have $\frac{1}{x} = f(x)^{\frac{-1}{v}}-1$ therefore we have:
$$
{-jv \choose k}\left(f(x)^{-\frac{1}{v}}-1\right)^k {}_2F_1\left(1,k+jv;k+1;1-f(x)^{-\frac{1}{v}}\right) = \left(f(x)+m(j,k)\right)^{b(j,k)}
$$
If we set $k=3$ we find that:
$$
{-jv \choose 3}\left(f(x)^{-\frac{1}{v}}-1\right)^3 {}_2F_1\left(1,3+jv;3+1;1-f(x)^{-\frac{1}{v}}\right) = c(j,k)\left(f(x)+m(j,3)\right)^{b(j,3)} = \frac{{-jv \choose 3}}{\left(f(x)^{\frac{-1}{v}}-1\right)^{3}(jv+1)(jv+2)}\left(6f(x)^{\frac{2}{v}} \left(\frac{(jv)^2+2jv}{jv}\right) +3f(x)^{\frac{3}{v}} \left(\frac{2+2f(x)^j-(jv)^2-3jv}{jv}\right) - 3f(x)^{\frac{1}{v}}\left(jv+1\right)\right)
$$
When we simplify we have:
$$
c(j,k)\left(f(x)+m(j,3)\right)^{b(j,3)} = \frac{-jv}{6\left(f(x)^{\frac{-1}{v}}-1\right)^{3}}\left(6f(x)^{\frac{2}{v}} \left(\frac{(jv)^2+2jv}{jv}\right) +3f(x)^{\frac{3}{v}} \left(\frac{2+2f(x)^j-(jv)^2-3jv}{jv}\right) - 3f(x)^{\frac{1}{v}}\left(jv+1\right)\right) = \frac{jv}{\left(f(x)^{\frac{-1}{v}}-1\right)^{3}}\left(\frac{jv}{2}f(x)^{\frac{1}{v}}[jv+1]-f(x)^{\frac{2}{v}} [jv+2] +\frac{1}{2}f(x)^{\frac{3}{v}} [jv+3-\frac{2(1+f(x)^j)}{jv}]\right)
$$
Although i have not proven explicitly that such functions do not exists, the resultant of the terms do not even resemble anything like the binomial theorem especially considering that there is a lone $f(x)^j$ term in the middle of the convolution. Although this proof is not complete, it provides strong evidence that this solution is most likely either non-trivial by my definition or if such a solution is considered trivial, certainly it will pathological with respect to typical binomial theorem solutions.","['calculus', 'sequences-and-series', 'combinatorics', 'binomial-theorem', 'analysis']"
1522232,What are rings good for?,"It's not a homework question or something, but I was wondering: groups are useful in physics and can be applied to many symmetry holding problems, fields can be used to construct vector spaces over the field. But what use do general rings have? Is it to construct modules? Or more concretely, can someone give me an example of application of a ring. Thanks in advance. EDIT: Of course I know the trivial examples like $\mathbb{R}$, but since this is also a field it has its applications as a vector space or as a ground field for a vector space. What about $\mathbb{Z}[i]$ for example?","['abstract-algebra', 'ring-theory']"
1522240,Necessar and sufficient condition of similitude,Here is the problem : Give a necessar and sufficient over $M\in \mathcal{M}_n(\mathbb{C})$ (a complex matrix) such that there exists $P \in GL_n(\mathbb{C})$ and $N \in \mathcal{M}_n(\mathbb{R})$ satisfying : $$M=P^{-1}NP$$ I have barely no ideas how to start. The problem is difficult and has been proposed to a french oral examination (ENS) If you have the solution to this problem or hints please give them. Thank you.,"['linear-algebra', 'matrices']"
1522280,Union of conjugates of a subgroup of a finitely generated group.,"Is there a finitely generated group $G$, with a proper subgroup $H$, such that $$G=\bigcup_{g\in G}gHg^{-1}$$ I know that this is not the case for a finite $G$: Union of the conjugates of a proper subgroup I also found an example for an infinite $G$: https://mathoverflow.net/questions/34044/group-cannot-be-the-union-of-conjugates but (as far as I know) the example given, $GL_2(\mathbb{C})$ is not finitely generated. I've also seen that there are finitely generated groups with proper subgroups of infinite index (that are not abelian): give an example of finitely generated not abelian group which has subgroup of infinite index and not finitely generated. but the groups given are above my level of understanding, and thus I'm not sure if it helps me with my question. And for abelian groups this obviously fails as for all $g\in G$ we get $gHg^-1=H$ Unfortunately even with all of that, I wasn't able to figure out myself the answer to the question. Any help would be appreciated.","['abstract-algebra', 'group-theory']"
1522309,How to calculate this integral involving an exponential?,I would like to calculate the integral $$\int_{0}^{\infty} xe^{-x(y+1)}dy.$$ I think I get the first steps correct. First $$\int_{0}^{\infty} xe^{-x(y+1)}dy = x\int_{0}^{\infty} e^{-x(y+1)}dy.$$ I then select $u = -x(y+1)$ so $du = -xdy$ and $dy = \frac{du}{-x}$. Therefore $$x\int_{0}^{\infty} e^{-x(y+1)}dy = x\int_{0}^{\infty} \frac{e^{u}du}{-x} = -\int_{0}^{\infty} e^{u}du.$$ I don't get how to solve it from here. I tried $$-\int_{0}^{\infty} e^{u}du = -(e^{\infty} - e^{0}) = -(0 - 1) = -1$$ but my textbook says it should be $e^{-x}$. I guess it is the back-substitution step I don't understand. How to do this?,"['derivatives', 'calculus', 'definite-integrals', 'integration']"
1522321,Mean and variance on a metric space,"It is my first post, so please correct me if I am not following the rules/etiquette. Assume that we are given a space $\mathcal{S}$ composed by vectors $x\in\mathbb{R}^L$, constrained by
$$\sum x_i=1,\\
0\le x_i \le 1,$$ 
where $x_i$ is the $i$-th element of the vector $x$. We also assume to be able to measure the distance between the elements,
$$d(x,y)=\sqrt{x^TAy},$$ where $A$ is a given positive semi-definite matrix. If I stand correct, we cannot define a vector space because of the constraints on the vectors, i.e. the sum of two element in $\mathcal{S}$ does not belong to $\mathcal{S}$. I would like to compute quantities related to a set of elements belonging to $\mathcal{S}$, for example the mean and the variance. We define such set as $\{y^n\}_{n=1}^N\text{ where } y^n\in\mathcal{S}$. I thought of considering the Frechet mean/variance and solve the following optimization problem: $$ \min_{x \in \mathcal{S}} \sum_{n=1}^N d^2(x,y^n) = \min_{x\in\mathcal{S}} \sum_{n=1}^N x^TAy^n=\min_{x\in\mathcal{S}}x^TA\bar{y},$$
where $\bar{y} = \sum_{n=1}^N y^n$. This looks like an LP program that could be easily solved. The minimizer would be the Frechet mean and the argument would be the Frechet variance. It looks ok to me, but I am afraid that I have missed some details around that could not allow the solution of the optimization problem. 1) Is this the approach correct?
2) I could not find much information by searching around and I was wondering if there were some pointers around for similar works.","['means', 'metric-spaces', 'statistics']"
1522330,Taking away infinitely many elements infinitely many times [duplicate],"This question already has answers here : Mutually exclusive countable subsets of a countable set (6 answers) Closed 8 years ago . This is a somewhat hand wavy question but I'm not sure how to ask it more precisely. If we have a countably infinite sequence (or set), can we take away infinitely many elements from the sequence infinitely many times and still be left with infinitely many elements? Or is it the case that we must be left with 0 or finitely many elements?","['sequences-and-series', 'elementary-set-theory', 'cardinals']"
1522342,How to approach this question (multivariable calculus/limits)?,"The thing is I don't want any direct answer I just need a tip of a trick that I can apply to get the magic. I am not really interested in the answer, so please don't write the answer for the question but rather how to approach and think about the question. I'm given the following $$\lim_{(x,y)\to (2,-2)}\frac{\ln(x^4-y^4+1)}{x^2-y^2}.  $$ I obviously have tried to approach it from the $x$ axis by setting $y=0$ and so from the $y$ axis by setting $x =0$ but it didn't help out because when you set $x=0$ you end up with an undefined answer in $\mathbb R$. Mathematica tho says that the limit converges to $8$
Thank you in advance!","['multivariable-calculus', 'limits']"
1522357,"Solve $xy'' + y' + xy = 0, y(0)=1, y'(0)=0$","The initial value problem $xy'' + y' + xy = 0; y(0)=1, y'(0) = 0$ has_____ Options: a) a unique solution. b) no solution c) infinitely many solutions d) two linearly independent solutions (Is there any other way around to do it other than power series method?) Ans) I started with y(x) = $x^r$ but fails to continue.
Then I choose $$y(x) =\sum_{n=0}^{\infty} a_n x^n$$ $$y'(x) =\sum_{n=0}^{\infty} na_nx^{n-1}$$
$$y''(x) = \sum_{n-0}^{\infty} n(n-1)a_nx^{n-2}$$
Substitution leads to $$ \sum_{n=0}^{\infty} [n(n-1) + n]a_nx^{n-1} + \sum_{n=0}^{\infty} a_n x^{n+1}=0$$ Change of index leads to $$ \sum_{n=0}^{\infty} [n(n-1) + n]a_nx^{n-1} + \sum_{n=2}^{\infty} a_{n-2} x^{n-1}=0$$ Rewrite as $$ a_1 + \sum_{n=2}^{\infty} (n^2a_n + a_{n-2})x^{n-1}=0$$ which implies $a_1=0$. Recursive relation is $$n^2a_n = -a_{n-2}$$
Take $a_0 = a_0$ and $a_1 = a_1$ $a_2 = \displaystyle\frac{-a_0}{2^2}$  $a_3 = \displaystyle\frac{-a_1}{3^2}$ $a_4 = \displaystyle\frac{a_0}{2^2.4^2}$  $a_5 = \displaystyle\frac{a_1}{3^2.5^2}$ $a_6 = \displaystyle\frac{-a_0}{2^2.4^2.6^2}$  $a_7 = \displaystyle\frac{-a_1}{3^2.5^2.7^2}$ $$y(x) = a_0 + a_0\sum_{k=1}^{\infty} \frac{(-1)^kx^{2k}}{\Pi_{n=1}^{k} (2n)^2} + a_1\sum_{k=0}^{\infty} \frac{(-1)^kx^{2k+1}}{\Pi_{n=0}^{k} (2n+1)^2}$$ already we got $a_1 = 0$ hence solution will be
$$y(x) = a_0 + a_0\sum_{k=1}^{\infty} \frac{(-1)^kx^{2k}}{\Pi_{n=1}^{k} (2n)^2}$$
Given condition $y(0) = 1$ gives $a_0 = 1$ Final solution will be  $$y(x) = 1 + \sum_{k=1}^{\infty} \frac{(-1)^kx^{2k}}{\Pi_{n=1}^{k} (2n)^2}$$ and clearly $y'(0) = 0$ satisfies. So concludes a unique solution. Is there any other way to do it smartly?",['ordinary-differential-equations']
1522359,Is it correct to say that ($\color{red}{(} \limsup |W_k|/k\color{red}{)} \le 1) \supseteq \limsup \color{red}{(}|W_k|/k \le 1\color{red}{)}$?,"Let $W_0, W_1, W_2, \dots$ be random variables on a probability space
  $(\Omega, \mathscr{F}, \mathbb{P})$ where $$\sum_{k=0}^{\infty}P(|W_k|>k) <\infty$$ Prove that $$\limsup \frac{|W_k|}{k} \le 1 \ \text{a.s.} $$ I initially thought the conclusion meant $(**)$ when it really means $(*)$: $$P(\color{red}{(} \limsup |W_k|/k\color{red}{)} \le 1) = 1 \ \text{(*)}$$ $$P(\limsup \color{red}{(}|W_k|/k \le 1\color{red}{)}) = 1 \ \text{(**)}$$ What I tried: By the first Borel-Cantelli Lemma, we have $P(\limsup (|W_k| > k)) = 0$ $\to P(\limsup (|W_k|/k > 1)) = 0$ $\to P(\liminf (|W_k|/k > 1)) = 0$ $\to P([\liminf (|W_k|/k > 1)]^C) = 1$ $\to P(\limsup \color{red}{(}|W_k|/k \le 1\color{red}{)}) = 1$ $\to P(\color{red}{(} \limsup |W_k|/k\color{red}{)} \le 1) = 1$ QED assuming $(**)$ implies $(*)$. Is it really the case that $(**)$ implies $(*)$? Here is why I think such: $\forall \omega \in \Omega, \omega \in (\limsup \color{red}{(}|W_k|/k \le 1\color{red}{)})$ Then $\forall m \ge 1, \exists n \ge m$ s.t. $\omega \in (\frac{|W_n(\omega)|}{n} \le 1)$ $\to \omega \in (\limsup \frac{|W_n(\omega)|}{n} \le \limsup 1 = 1)$ Now if $(\color{red}{(} \limsup |W_k|/k\color{red}{)} \le 1) \supseteq \limsup \color{red}{(}|W_k|/k \le 1\color{red}{)}$, then it follows by monotonicity that $(**)$ implies $(*)$ QED. However $(*)$ does not imply $(**)$ because it does not hold that $(\color{red}{(} \limsup |W_k|/k\color{red}{)} \le 1) \subseteq \limsup \color{red}{(}|W_k|/k \le 1\color{red}{)}$. Counterexample given in comments below. I guess the intuitive explanation is that $\limsup \frac{|W_n(\omega)|}{n} \le \limsup 1$ --/--> $\frac{|W_n(\omega)|}{n} \le 1$ for similar reasons that $x_n \le y_n \to \lim x_n \le \lim y_n$, if those limits exist (1) but $\lim x_n \le \lim y_n$ --/--> $x_n \le y_n$ (2), which makes sense: (1) has an infinite number of statements which imply 3 statements (limsup, liminf, limsup=liminf) (2) has 3 statements that claims to imply an infinite number of statements Is that right? Edit based on what Daniel Fischer said: To prove $$\liminf \{\frac{W_k(\omega)}{k} \le 1\} \subseteq \{\limsup \frac{W_k(\omega)}{k} \le 1\}:$$ Suppose $$\omega \in \liminf \{\frac{W_k(\omega)}{k} \le 1\}$$ Then $\exists m \ge 1$ s.t. $\forall k \ge m$, $$\omega \in \{\frac{|W_k(\omega)|}{k} \le 1\}$$ Since $\frac{|W_k(\omega)|}{k} \le 1 \to \limsup \frac{|W_k(\omega)|}{k} \le \limsup 1 = 1$ , we have $$\to \omega \in \{\limsup \frac{|W_k(\omega)|}{k} \le 1\}$$","['probability-theory', 'borel-cantelli-lemmas', 'random-variables', 'sequences-and-series', 'limsup-and-liminf']"
1522380,Geometry question on triangle,Let ABC be an isosceles triangle with AB = AC and let Γ denote its circumcircle. A point D is on the arc AB of Γ not containing C and a point E is on the arc AC of Γ not containing B such that AD = CE.   how can I prove that BE is parallel to AD.?,['geometry']
1522383,Convergence in probability of normal distributions,"I need some help with the following. I am given that $X_n\sim\mathcal{N}\left(0,\frac{1}{n}\right)$ i.e. $X_n$ has a normal distribution with mean $0$ and variance $1/n$. I need to show that $X_n$ converges in probability to $0$. I started by doing $$\lim_{n\to\infty}P\left(\lvert X_n-0 \rvert\ge\epsilon\right)=\lim_{n\to\infty}P\left(X_n\ge\epsilon\right)$$ and I don't know where to go from here. I though that I could use the cdf of the normal but this doesn't seem correct.","['probability-theory', 'probability', 'convergence-divergence']"
1522389,How is implicit differentiation formally defined?,"I get that differentiation is an operation used on a function, so if a function is defined $x\mapsto x^2$, the derivative is 
$$
(x\mapsto x^2)'
   = x \mapsto \lim_{h\to 0} \frac{x^2+2xh+h^2-x^2}{h} = x\mapsto 2x.
$$ But how can you extend the definition $f' = \dfrac{f_h-f}{h}$ in such a way that it works with implicit functions/multifunctions? I know that it works, but I don't understand how it works for equations like $y^2 = 4-x^2$.","['implicit-differentiation', 'derivatives']"
1522397,How to tell is a matrix is a covariance matrix?,"How can we know that these matrices are valid covariance matrices?
$$
       C= \begin{pmatrix}     
        1 & -1 & 2 \\
        -1 & 2 & -1 \\
        2 & -1 &  1 \\
        \end{pmatrix}
\\
       C= \begin{pmatrix}     
        4 & -1 & 1 \\
        -1 & 4 & -1 \\
        1 & -1 &  4 \\
        \end{pmatrix}
$$ I know that $C_{xy}=C_{yx}$ (is symmetric) $C_{xx}= \mathrm{Var}[x] \ge 0$ (the diagonal entries are all positive) But I want to know, are there any other properties?","['covariance', 'probability', 'matrices']"
1522401,Simple? Math problem from a physics textbook,"Given 
$$\cos A ={ {xy-ab} \over {\sqrt{x^2-b^2}\sqrt{y^2-b^2}}},$$
where $x>0,y>0,x+y=a+b=const$. Here $x$ and $y$ are variables , $a$ and $b$ are constants. They say that the biggest possible $\cos A$ is when $x=y$ cause there is symmetry. Substitution $(x,y)$ to $(y,x)$ gives the same function. I have not studied two argument functions yet, so for me it is not that clear. Can you show me a simple way to understand it.Thanks.","['optimization', 'functions']"
1522472,A recursively defined sequence and a limit,"Fix real numbers $ a_0 $, $ a_1 $ and define, $$ a_{n+1} = a_n +  \Big(\frac{2}{n+1} \Big) a_{n-1} \space \space \forall \space n \ge 1 $$ Show that the sequence $ \Big\{ \dfrac{a_n}{n^2} \Big\}_{n=1}^{ \infty} $ is convergent and find its limit. This question is from an old Miklos Schweitzer competition (1958, problem 7, as pointed out by @user37238). I don't have any good ideas of approaching this question. Writing the recursion as $ (n+1) (a_{n+1}-a_n)=2a_{n-1} $ gives me the impression that it is a relation between the coefficients of a power series, obtained as a solution to some differential equation, although this line of thought doesn't lead me too far.","['recurrence-relations', 'sequences-and-series', 'limits']"
1522477,What is a Covering Space (intuitively) and is it related to the concept of open cover?,"I do know the definition of a covering space (e.g. Wikipedia: https://en.wikipedia.org/wiki/Covering_space ), but am a bit confused what exactly is it. Is it related to the concept of Cover in point set topology? ( https://en.wikipedia.org/wiki/Cover_(topology) ) My observation is that the covering space ($C$) is totally different from the original space ($X$), while for cover, the union of sets in the cover contain $X$. Any intuitive (non-rigorous is ok) explanation of covering space will be greatly appreciated. Thanks!","['algebraic-topology', 'general-topology']"
1522478,$h$-principle for Legendrian immersions,"It is ""folklore"" that continuous curves in contact 3 manifolds can be approximated by Legendrian curves and it seems that this follows from Gromov's $h$-principle for Legendrian immersions (in arbitrary dimensions). Since reading Gromov is a challenge (at least for me) I wonder if anybody has readable reference for this statement (preferably in any dimension) or at least for the case of approximating curves in 3 manifolds.","['h-principle', 'differential-geometry', 'ordinary-differential-equations', 'contact-topology']"
1522483,How to show that the quadratic equation has solutions with real part negative?,"If I have a quadratic equation $ax^2+bx+c=0$, where a,b and c are all strictly positive. Considering complex number solutions to the equation, how do I show that the real part of the solution is strictly negative? My solution uses the quadratic formula: $x = \dfrac{-b\pm\sqrt{b^2-4ac}}{2a}$ Suppose $b^2-4ac \leq 0$, then the real part of the solution is $-\frac{b}{2a}$ which is strictly negative. Suppose $b^2-4ac > 0$, then $\sqrt{b^2-4ac} < b$, so that the solution will be strictly negative. Is there a neater way to do this without using the quadratic formula and proving by cases?","['polynomials', 'algebra-precalculus']"
1522485,Finding the units in $M_n(R)$,"Let $R$ be a ring. Then I would like to know the units in $M_n(R)$. Here is my idea. The determinant is a homomorphism from $M_n(R)$ to $R$. If $A$ is a unit, then there is a $B$ such that $AB = I$. Then taking determinants one finds that $\det(A)$ is a unit in $R$. So therefore the units in $M_n(R)$ are those matrices $A$ where $\det(A)$ is a unit in $R$. Is this correct? Can this be done more explicitly?","['abstract-algebra', 'ring-theory', 'matrices']"
1522498,Deriving Iteration Formula for $y''(x)=(3x^2+x^6)y(x)$,"As stated in the question I want to derive a iteration formula for the second order differential equation. All I have been working with so far is the general Picard-iteration formula: $$y_{n+1}(x)=y_0+\int_{x_0}^xf(t,y_n(t))dt \space \space \space \text{} \space \space \space y_0(x)=y_0$$ I thought this was some sort of fixed algorithm that one can use to derive some a taylor-series. How can I derive my ""own"" algorithm for the differential equation mentioned above? Edit 1: I am not sure if this is correct but I thought about just integreating the differential equation twice: $$\begin{aligned}& y''(x)=(3x^2+x^6)y(x) \\ &y'(x)=\int (3x^2+x^6)y(x) dx+v_0 \\ &\boxed{y(x)=\color{red}{\int\int(3x^2+x^6)y(x)dxdx}+v_0x+y_0}\end{aligned}$$ I have no idea how to evaluate or simplify the red integral or if this is even the correct approach.",['ordinary-differential-equations']
1522522,Is a set contained in its Cartesian product with itself?,"Given the Cartesian product $X = A \times A$, is $A \subset X$?",['elementary-set-theory']
1522555,Verifying Divergence Theorem,"I have to verify Gauss' Divergence Theorem for $$\bar F(x,y,z)=xy^2 \hat i+yz^2\hat j+zx^2\hat k$$
for the region $$R:y^2+z^2\le x^2;\;\;0\le x\le4$$ Now, $\operatorname{div}\bar F=x^2+y^2+z^2$. I don't understand what type of parametrization will make calculating the triple integral easy. I tried $\bar r=x\hat i+x\cos\theta\hat j+x\sin\theta\hat k$ where $0\le \theta \le 2\pi$. But then, $\operatorname{div}\bar F =r^2$. Would then $$\iiint_R \operatorname{div}\bar F\;dV=\int_0^4\int_0^{2\pi}\int_0^{\sqrt2x}r^2\;rdr\;d\theta\;dx$$
be correct?","['vector-analysis', 'multivariable-calculus']"
1522560,Generalization of subsubsequence argument for stochastic convergence,"It is well known that a sequence of random variables $(X_n)_{n\in\mathbb N}$ converges in probability to some random variable $X$ if and only if every subsequence $(X_{n_k})_{k\in\mathbb N}$ has a further subsubsequence $(X_{n_{k_l}})_{l\in\mathbb N}$ which converges almost surely to $X$. I am struggling with the following generalization: Consider a sequence of random variables $(X_n)_{n\in\mathbb N}$ and another random variable $X$ with the following property: Any subsequence $(X_{n_k})_{k\in\mathbb N}$ satisfies
$$\liminf_{k\to\infty} X_{n_k} = X \quad\text{almost surely}.$$
Does $(X_n)_{n\in\mathbb N}$ converge in probability to $X$? I am struggling for the following reason: Since $\liminf$ is defined pathwise only, we cannot extract a (deterministic) subsequence which converges almost surely. Therefore, my assumption seems to be weaker in some sense. Are there any ideas for proving this statement or any counterexamples?","['probability-theory', 'probability', 'convergence-divergence']"
1522637,Show there exists a constant such that the inequality holds,"Prove there exists a  constant $c \in (0,1)$  ($c$ should depend on $n$) such that $$\displaystyle \sum_{i=1}^{n} x_i^3\cdot x_{i+1} \le c \sum_{i=1}^n x_i^4$$ holds for all  real numbers $x_i$ such that $\displaystyle \sum_{i=1}^nx_i=0$ and $x_{n+1}=x_1$.","['sequences-and-series', 'calculus']"
1522642,Does path-connected imply simple path-connected?,"Let $X$ be a path-connected topological space, i.e., for any two points $a,b\in X$ there is a continuous map $\gamma\colon[0,1]\to X$ such that $\gamma(0)=a$ and $\gamma(1)=b$. Note that beyond continuity little is required about $\gamma$. Is it always possible to make $\gamma$ a simple (=injective) curve? (Of course we consider only the case $a\ne b$). 
  If the answer depends on $X$, what are some mild sufficient conditions on $X$? If $X$ is sufficiently nice, e.g., an open subset of $\Bbb R^n$, the answer is ""yes"";  more generally, what one would call ""locally simplepath-connected"" suffices (the set of points reachable from $a\in X$ by a simple curve is both open and closed).
Also, for arbitrary $X$ it is clear that a single instance of self-crossing can be short-cut. But for infinitely many possible short-cuts the situation may become somewhat hairy, I'm afraid. EDIT: To avoid Mike Miller 's counterexample (indiscrete topology on $X$ with $|X|<|\Bbb R|$), let's at least assume that $X$ is Hausdorff. At any rate, $|X|\ge|\Bbb R|$ is a necessary condition - I am still looking for ""as mild as possible"" sufficient conditions ...","['connectedness', 'general-topology', 'path-connected']"
1522648,"recurrence relation, limits","Let $a_1\in(0,1)$ and $a_{n+1}=a_n(1-a_n^2)$ for $n\ge 1$. Prove that $\lim \sqrt{n}a_n = \frac{1}{\sqrt{2}}$ and find $\lim \left(\sqrt{n}a_n - \frac{1}{\sqrt{2}}\right)\frac{n}{\ln n}$. For the first part, I used identity $na_n^2=\frac{n}{\frac{1}{a_n^2}}$ and Stolz-Cesaro theorem. How to deal with the more advanced one? I don't see the use of Stolz-Cesaro here.","['sequences-and-series', 'limits', 'real-analysis']"
1522656,A is a positive definite matrix iff its leading principal minors are positive,"I am to prove that the a symmetric matrix $A$ is positive definite iff the leading principal minors of $A$ are positive. The forward implication is clear. Since the eigenvalues of a SPD matrix are positive and real, and $\det(A)$ is the product of eigenvalues, it must follow that these are positive too. However, can someone please help me with the backwards implication? I just don't know how to handle it.","['positive-definite', 'determinant', 'linear-algebra', 'matrices']"
1522671,Computation of a sum using Stirling's approximation and Watson's lemma,$$Ω=\sum_{n=0}^{N-\frac{E}{\epsilon}}  \frac{Ν!}{\left(\frac{N-n-\frac{E}{\epsilon}}{2}\right)!\left(\frac{N-n+\frac{E}{\epsilon}}{2}\right)!n!}$$ I am supposed to calculate the above sum using first Stirling's approximation and then using Watson's lemma integrate for $x=n/N$. The point is to find the natural logarithm of Ω and the above are hints.,"['approximation', 'entropy', 'functions']"
1522672,Why is the infinite product of this quotient of $\sin$'s equal to $\left(\frac{3}{\pi}\right)^{2}$[SOLVED],"I was intrigued by this answer the other day, but perhaps lack a little bit of the necessary background to understand a certain step. Namely, the fact that 
$$\prod_{n=1}^{+\infty} \left(\frac{\sin\left(\frac{\pi}{6\cdot 2^{n-1}}\right)}{2\sin\left(\frac{\pi}{6\cdot 2^n}\right)}\right)^2 = \left(\frac{3}{\pi}\right)^{2}$$
is used, and I cannot make sense of this at all. Jack D'Aurizio comments that "" the product of squares is the square of the product "" but I'm not entirely sure what this means. Can anyone explain it to me a little more in-depth? I'm not sure what to look up to understand this foreign concept. Edit : With a hint from Simon S I got the answer, so I have to thank him very much. I'm not sure what to do with my question now.","['calculus', 'infinite-product', 'trigonometry']"
1522709,Determine Fastest 3 horses out of 125 when only 5 racing track are given without using stopwatch?,"We have 125 horses, and we want to pick the fastest 3 horses out of those 125. In each race, only 5 horses can run at the same time because there are only 5 tracks. What is the minimum number of races required to find the 3 fastest horses without using a stopwatch? This question was easily solved when there were 25 horses but for 125 horses it is not solvable easily.Can anyone help me out with this complex problem?
This is the link of the question which is also not solved.","['algorithms', 'puzzle', 'discrete-mathematics']"
1522714,Why can a probability measure be defined over power set of countable sample space?,"Let $\Omega$ be the sample space, $p: \Omega \rightarrow [0,1]$, be any function satisfying $\sum_{w\in\Omega} p(w) = 1$. Then there is a valid probability triple $(\Omega, \mathcal{F},P)$, where $\mathcal{F}$ is the collection of all subsets of $\Omega$ (the power set), and for $A \in \mathcal{F}, P(A) = \sum_{w\in A} p(w)$. My question is why/how we know that, when $\Omega$ is finite or countable, a probability measure can be defined over ALL possible subsets (vs, say, the fact that a probability measure -- satisfying the standard axioms, of course -- cannot be defined over all subsets of [0,1]). Is there a proof of this somewhere (if so, please let me know where I can look), or could someone provide one?","['probability-theory', 'measure-theory']"
1522729,How do you evaluate $\lim_{n \to \infty}f_n(x)$ ? I can't seem to deal well with $f_n(x)$,"Let $$f_n:[0,\infty) \to R, f_n(x) = e^{-nx}\sin(nx), n \geq 0$$ How do you evaluate $$\lim_{n \to \infty}f_n(x)$$? I am having troubles getting the limit. Can somebody help me, please?","['limits', 'functions']"
1522732,Showing some set is not algebraic set,"I want to solve the Exercise 1.13 (c) in Fulton's Algebraic Curves Book: Show that the set $\{(\cos t,\sin t,t)\in\mathbb{A}^3(\mathbb R)\mid t\in\mathbb R\}$ is not an algebraic set. I have a solution. Can you check if it is accurate? Thanks in advance. My solution is the following: Suppose if possible the given set is algebraic. The line: $(y=0, x=1)$ intersects the given set (name as ) $V$ infinite times. Now $V=\cap V(F)$ where $F$ is a polynomial in $k[x,y,z]$. If $\forall F$, $V(F)$ contains all the points in the line: $(y=0, x=1)$ then $V$ will also contain all the points in the above mentioned line which is not true. So, $V\subset V(F)$, where $F$ is a polynomial whose zero set does not include all the points in the above mentioned line(may contain some point).We have chosen $F$ such a way that $F(1,0,z)$ cannot be a zero polynomial. So, $F(1,0,z)$ has atmost finite number of roots.But $V\subset V(F)$, so V intersects the above mentioned line  finite time, which is a contradiction","['algebraic-geometry', 'proof-verification']"
1522760,Express $\frac{1-x}{(x-1)^2+y^2}-\bigg(\frac{y}{(x-1)^2+y^2}\bigg)i$ in the form of $f(z)$,"I need to express $f(z)$ from the form $\color{blue}{u+vi}$ to the form $\color{blue}z$ for example if: $g(z)=\frac{1}{x+yi}$ so $ =g(z)=\frac{1}{z}$ $$f(z)=\underbrace {\frac{1-x}{(x-1)^2+y^2}}_{=u(x,y)}+\underbrace {\bigg(-\frac{y}{(x-1)^2+y^2}\bigg)}_{=v(x,y)}i$$ My try: $$f(z)=\frac{1-x-yi}{(x-1)^2+y^2}$$ $$=\frac{1-\overbrace{x-yi}^{=\bar z}}{\underbrace{x^2+y^2}_{=|z|^2}-2x+1^2}$$ $$-\frac{1-\bar z}{|z|^2-2x+1}$$ I'm stuck here","['complex-analysis', 'complex-numbers']"
1522790,"Stable subspace, unstable subspace, centre subspace: Of which kind of stability are we talking?","Let $x'(t)=Ax(t)$ be a linear ODE system. Then the span of the eigenvectors belonging to eigenvalues with negative real-part is called the stable subspace, the subspace spanned by the eigenvectors of eigenvalues of positive real-part is called unstable subspace and the subspace spanned by the eigenvectors belonging to eigenvalues with zero real-part is called centre subspace. What kind of stability is the stable subspace of? What kind of unstability (as negation of what kind of stability) is the unstable subspace of? Is the centre subspace stable or unstable? Are there situation in which it is stable (and which kind of stability then)?","['stability-in-odes', 'ordinary-differential-equations']"
1522796,Give bases for col(A) and null(A),I have A= $\begin{bmatrix}1&1&-3\\0&2&1\\1&-1&-4\end{bmatrix}$ I row reduce it to $\begin{bmatrix}1 &0& -3.5\\0&1&.5\\0&0&0\end{bmatrix}$ How do I find col(A) from the above info? Is it the pivot points correspond to the columns? So col(A) would be $\begin{bmatrix}1\\0\\1\end{bmatrix}$and $\begin{bmatrix}1\\2\\-1\end{bmatrix}$ and for null(A) I got $\begin{bmatrix}3.5\\-.5\\1\end{bmatrix}$,"['linear-algebra', 'matrices']"
1522832,Convergence of $\sum _{n=2}^{\infty }\:\frac{1}{n\ln\left(n\right)}$,"Is this convergent or not?
$$\sum _{n=2}^{\infty }\:\frac{1}{n\ln\left(n\right)}$$ I tried using the ratio test but the limit is giving me 1, which doesn't help me. I don't think I'm supposed to use the integral test, since we haven't studied it.","['sequences-and-series', 'convergence-divergence']"
1522926,Integral of $e^{ix^2}$,"How does one evaluate 
$$\int_{-\infty}^{\infty} e^{ix^2} dx$$ I know the trick how to evaluate $\int_{-\infty}^{\infty} e^{-x^2}dx$ but trying to apply it here I get a limit which does not converge: $I = \int_{-\infty}^{\infty} e^{ix^2}dx = \int_{-\infty}^{\infty} e^{iy^2}dy \\\implies I^2 = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{i(x^2+y^2)}dxdy = \int_{0}^{2\pi}\int_{0}^{\infty} re^{ir^2} = -\pi i (e^{i\infty}-e^0) $ and $e^{i\infty} $ is not defined. Are there any other methods? I am not interested in the result (WolframAlpha can do this for me), but rather the method.","['complex-analysis', 'complex-integration', 'integration']"
1522951,Example (for some $X$) of a nonclosed ideal in $\mathbb{C}(X)$?,"Let $X$ be a compact metric space and $\mathbb{C}(X)$ the algebra of continuous functions $f: X \to \mathbb{C}$, with pointwise operations. We equip $\mathbb{C}(X)$ with the maximum norm $N(f) := \max_{x \in X}|f(x)|$. An ideal $I \subset \mathbb{C}(X)$ is called a closed ideal if $I$ is a closed subset of $\mathbb{C}(X)$ viewed as a metric space. For any subset $Y \subset X$, the set $I_Y := \{f \in \mathbb{C}(X) : f(y) = 0 \text{ for all }y \in Y\}$ is a closed ideal in $\mathbb{C}(X)$. My question is, what is an example (for some $X$) of a nonclosed ideal in $\mathbb{C}(X)$?","['abstract-algebra', 'ideals', 'functional-analysis', 'commutative-algebra']"
1522977,For any ideal $I \subsetneq \mathbb{C}(X)$ the set $\{x \in X: f(x) = 0 \text{ for all }f \in I\}$ is not empty?,"See here for a related question by someone else. Let $X$ be a compact metric space and $\mathbb{C}(X)$ the algebra of continuous functions $f: X \to \mathbb{C}$, with pointwise operations. Does it follow that for any ideal $I \subsetneq \mathbb{C}(X)$ the set$$\{x \in X: f(x) = 0 \text{ for all }f \in I\}$$ is not empty?","['abstract-algebra', 'real-analysis', 'functional-analysis', 'commutative-algebra']"
1523033,Using Fatou's Lemma to Prove Monotone Convergence Theorem,"Monotone Convergence Theorem-
If $\{f_n\}$ is a sequence in $L^{+}$ such that $f_j\leq f_{j+1}$ for all $j$, and $f = \lim_{n\rightarrow \infty}f_n(=\sup_{n}f_n)$, then $\int f = \lim_{n\rightarrow\infty}\int f_n$ Fatou's Lemma - If $\{f_n\}$ is any sequence in $L^{+}$, then $$\int f = \int(\lim\inf f_n)\leq \lim\inf\int f_n$$ Let $\{f_n\}_{n\in\mathbb{N}}\subset L^{+}$, then by Fatou's Lemma $$\int f = \int (\lim_{n\rightarrow \infty} f_n) \leq \lim_{n\rightarrow \infty}\inf\int f_n$$ we know that $f = \lim_{n\rightarrow \infty}f_n (=\sup_{n}f_n)$, and that $f_n\leq f$ hence $\int f_n \leq \int f$ for all $n\in \mathbb{N}$, it is clear then that $\lim_{n\rightarrow \infty}\sup\int f_n \leq \int f$. Therefore, $$\lim_{n\rightarrow \infty}\sup\int f_n\leq \int f = \int \lim_{n\rightarrow \infty}\inf f_n \leq \lim_{n\rightarrow \infty}\inf\int f_n = \lim_{n\rightarrow \infty}\int f_n$$ so, $\int f = \lim_{n\rightarrow \infty}\int f_n$ Just want to make sure this is correct, any suggestions is greatly appreciated.","['proof-verification', 'real-analysis', 'measure-theory']"
1523039,"Prove that in every graph with at least two vertices, there are at least two vertices which have the same degree? [duplicate]","This question already has answers here : In any finite graph with at least two vertices, there must be two vertices with the same degree (4 answers) Closed 8 years ago . I've tried deriving a proof for this using the handshaking lemma, but that was unsuccessful. Could someone please explain to me how they would do this proof?","['graph-theory', 'discrete-mathematics']"
1523043,What's the best place to learn Quantum Homology,"I am a PhD student and I am trying to learn quantum homology. I already know some of the analysis, but I am struggling to really find a good readable reference which covers enough of the analysis to make it useful, but does not go into all details. So fx a good set of lecture notes. Of course there is McDuff and Salamon's opus, but I am aiming for something more accessible than that. 
Any ideas and input are much appreciated, thanks. :) Mads","['algebraic-geometry', 'reference-request', 'symplectic-geometry']"
1523054,"Suppose $\{X_n, n \ge1\}$ are iid and define $M_n=\bigvee^n_{i=1}X_i$","Suppose $\{X_n,n\geq 1\}$ are iid and define $M_n=\bigvee^n_{i=1}X_i. $ Check that $$P[M_n >x] \leq n P[X_1 >x].$$ Proof Note that $$[M_n >x] \subseteq \cup_{i=1}^{n}[X_i >x]$$ $$ \implies P[M_n >x] \leq P(\cup_{i=1}^{n}[X_i >x])\leq \sum_{i=1}^{n}P[X_i >x]=nP[X_1 >x]$$ If $E(X_1^p)< \infty$ then $\frac{M_n}{n^{\frac{1}{p}}}\rightarrow_{P} 0$ Proof Let $\epsilon >0$.
Observe:
$$P\left(\left|\frac{M_n}{n^{\frac{1}{p}}}\right| > \epsilon\right)=P(M_n >n^{\frac{1}{p}}\epsilon)\leq n P(X_1 > n^{\frac{1}{p}}\epsilon)\leq \frac{E\left(X_1^p 1_{X_1 > n^{1/p}\epsilon}\right)}{\epsilon^p} \rightarrow 0.$$
 3. If in addition to being iid, the sequence $\{X_n\}$ is non-negative, show
$$\frac{M_n}{n}\rightarrow^{P} 0\> \mbox{ iff } \> nP[X_1>n] \rightarrow 0 \> \mbox{ as }  \> n\rightarrow \infty.$$ I need help with part 3. But are my proofs for the previous two parts correct?","['probability-theory', 'probability', 'expectation', 'measure-theory']"
1523063,"If conditional expectation is a constant, can we infer the independency?","If $E[Y\mid X]= c$ where $c$ is the same constant for any $X=x$ (actually it's just $c = EY$), can we say that $X$, $Y$ are independent?","['probability-theory', 'conditional-expectation']"
1523069,"Trace-class, Hilbert Schmidt operators, $L^p(H)$: duality theorems","Let $H$ be a Hilbert space, separable if necessary, and let $tr$ be the usual trace on $L^1(H)$. It is classical theory that $K(H)^*=L^1(H)$, and $L^1(H)^*=B(H)$, via the canonical application $\langle T,S\rangle=tr(TS)$ for suitable $T$ and $S$. It is often mentioned that these duality results are analogous to the well-known $c_0^*=\ell_1$ and $\ell_1^*=\ell_\infty$. I think I more or less understand how this analogy works: The operator norm in $B(H)$ is the uniform norm when we restrict the maps to the unit ball, so $B(H)$ consists of the maps with finite ""uniform norm"", $K(H)$ consists of the elements which can be approximated by finite rank ones, and these are clear analogues of $\ell_\infty$ and $c_0$, respectively. I see more or less the analogy between $L^1(H)$ and $\ell_1$: identify the $i$-th element of a fixed basis for $H$ with the $i$-coordinate function in $\ell^1$, but I don't really see, precisely, how $\langle T\xi_i,\xi_i\rangle$ relates with $x_i$, for $T\in L^1(H)$ and $x=(x_i)\in\ell_1$. My question is what is the precise relation between these two duality results. More precisely, Question 1 : Can we obtain the duality results $c_0^*=\ell_1$ and $\ell_1^*=\ell_\infty$ from $K(H)^*=L^1(H)$ and $L^1(H)^*=B(H)$? What if we consider a measure/probability space $(X,\mu)$ instead of $\mathbb{N}$ with counting measure (although $c_0$ has no analogue in this case). More generaly, we might define $L^p(H)$ to consist of those $T\in B(H)$ for which the ""norm"" $\Vert T\Vert_p=(\Vert |T|^p\Vert_1)^{1/p}$ is finite. I'm not sure if this is indeed a complete norm, but if we assume so, it is natural to ask: Question 2 : Let $p$ and $q$ be (finite) Hölder conjugates. Are $L^p(H)$ and $L^q(H)$ duals of each other (with the usual application $\langle T,S\rangle=tr(TS)$), and does this indeed generalize the duality of $\ell^p$ and $\ell^q$? Moreover, is the map $p\mapsto\Vert T\Vert_p$ continuous in some sense and does $\Vert T\Vert_p$ converges to $\Vert T\Vert$ as $p\to \infty$ for $T\in \cap_p L^p(H)$? I'm interested in these questions to understand better how tracial von Neumann algebras/$II_1$ factors are related to probability spaces, and the importance of the Hilbert-Schmidt norm on such algebras.","['duality-theorems', 'von-neumann-algebras', 'functional-analysis']"
1523132,A non-abelian group of order $p^3$,"I am trying to show that for a non-abelian group of order $p^3$ for $p$ prime there exist a short exact sequence 
$$
1\rightarrow \mathbb{Z}_{p}\overset{j}{\rightarrow}G \overset{p}{\rightarrow}\mathbb{Z}_{p}\times \mathbb{Z}_{p}\rightarrow 1
$$ I have proved that the center of $G$ is $\mathbb{Z}_{p}$, which contains also the commutator of $G$. The center is a subgroup of $G$, so the first map $j$ is obviously an inclusion. Now I am stuck in the proof of the surjectivity of $p$ and that $\operatorname{Im}(j)=\operatorname{Ker}(p)$. Can anybody help me with this question? I would appreciate any comments and hints.","['abstract-algebra', 'group-theory']"
1523141,Show $\int_{s}^{\infty} f(x)dx = \mathcal{L} \{\frac{F(t)}{t}\}$ given $f(x) = \int_{0}^{\infty} e^{-xt}F(t)dt$,I'm trying to derive this to show that $$\int_{0}^{\infty} f(x)dx = \int_{0}^{\infty} \frac{F(t)}{t} dt$$ and use that to prove $$\int_{0}^{\infty} \frac{\sin t}{t} = \frac{\pi}{2}$$ How do I go about proving that $$\int_{s}^{\infty} f(x)dx = \mathcal{L} \{\frac{F(t)}{t}\}$$ given $$f(x) = \int_{0}^{\infty} e^{-xt}F(t)dt$$,"['definite-integrals', 'laplace-transform', 'ordinary-differential-equations', 'improper-integrals']"
1523143,Analogies between finite groups and Lie groups,"I believe there are some striking analogous facts between finite groups and Lie groups. One analogue almost too basic to mention is the appropriate notion of subobjects. In elementary group theory the correct notion of subobject is just a subset closed under the operations (identity, inverses and composition). A Lie group is also a topological space, and the corresponding notion of closure is being closed under the operation of taking limits - so closed in the usual sense of topology, thus the corresponding notion of Lie subgroup is closed subgroup. Here are some other notable analogies I can think of: A standard exercise in group theory asks us to prove that if $Z$ is central in $G$ then $G/Z$ cannot be cyclic (i.e. $\Bbb Z$ or $\Bbb Z/n\Bbb Z$). The analogue for Lie groups is that if a closed subgroup $Z$ is central then $G/Z$ cannot be of dimension $1$, so $\Bbb R$ or $\Bbb R/\Bbb Z$. Proof . The cyclic subgroups $C$ of $G/Z$ pull back to abelian subgroups of $G$, which commute elementwise pairwise and whose union is dense, making $G$ abelian. Fundamental Theorem of (appropriately ""finite"") AbGrps . For f.g. abelian groups, there is a surjection $\Bbb Z^n\to G$ for some $n$ making $G$ a quotient of $\Bbb Z^n$, which must be a direct product of quotients of $\Bbb Z$ (which are $\Bbb Z$ or $\Bbb Z/n\Bbb Z$). Similarly, for connected abelian Lie groups the exponential map $\exp:{\frak g}\to G$ should be a surjective group homomorphism so $G$ is a quotient of $\Bbb R^n$ (for some $n$) which must be a direct product of quotients of $\Bbb R$ (so $\Bbb R$ or $\Bbb R/\Bbb Z$). Orbit-Stabilizer : In finite group theory, if $G$ acts transitively on a set $X$ and there is a point-stabilizer $K$, then $|G|=|X|\cdot|K|$. In Lie theory, if $X$ is a homogeneous space with respect to a smooth action of $G$ and there is a point-stabilizer $K$, then $\dim G=\dim X+\dim K$. These are numerical versions of the OS Thm, a more structural flavor can be given with: Lagrange's Theorem . In elementary group theory, this essentially says $G/H$ is a partition of $G$, which in Lie theory we can express as $H\to G\to G/H$ being a fiber bundle. Any other good ones we can add to this list, big or small?","['lie-groups', 'group-theory', 'big-list']"
1523158,"A matrix has only $0$'s in its diagonal, all the other entries are $1$'s. What are the eigenvalues and eigenspaces of the matrix?","I think that I should to rewrite the matrix in a appropriate form, but I can't find it. For a $2\times2$ matrix I get the characteristics polynomial $x^2-1$ and the eigenvalues $-1,1$. For $3\times3$ matrix I get $-x^3+3x+2$ and the eigenvalues are $2,-1,-1$.","['linear-algebra', 'matrices']"
1523160,"Solving $\frac{1}{\pi}(x \tan x) = \frac{1}{8}$ for $x \in [0,\pi]$","How do I solve:
$$ 
\frac{1}{\pi}(x \tan x) = \frac{1}{8}
$$
for $x$ where $x$ is in $[0,\pi]$? The problem states: ""Determine if the graph of the curve $y = (1/\pi)(x\tan x)$ crosses
  the line $y = (1/8)$ at some point $x$ in $[0, \pi]$. Justify answer."" I have already plotted this and see that there is 2 intersection points at around ±0.588, my problem is that I cannot solve the function numerically thus unable to prove how i got to the answer. Any help is appreciated. ( Large version )","['calculus', 'trigonometry']"
1523203,Why are Radon measures that agree on closed balls equal?,"I have a task to prove that if two Radon measures $\mu$ and $\nu$ agree on closed balls in $\Bbb{R}^n$ then they are equal. It's given as a corollary of Vitali covering theorem, or to be precise, the corollary of this fact that for Radon measure $\mu$ for $\forall \epsilon>0$ we can find balls $\{B_i\}$, such that $\mu(A)\leq\sum_{i}\mu(B_i)\leq\mu(A)+\epsilon$. I understand how this fact above follows from Vitali covering theorem, but cannot get how to use it to prove that the measures are equal.",['measure-theory']
1523232,Geometric interpretation of Q in Lyapunov's equation,"Lyapunov's equation says: given any $Q > 0$ ($Q$ positive definite) there is $P > 0$ such that $A^T P + P A + Q = 0$ if and only if for $\frac{dx(t)}{dt}=A x(t)$ it is the case that the real part of each eigenvalue of $A$ is negative. Then, the ellipsoid $x^T P x \leq 1$ is an invariant of $\frac{dx(t)}{dt}=A x(t)$. The geometric interpretation of $P$ is such that the eigenvectors of $P$ form the principal axes of the ellipsoid and each eigenvalue is related to the length of the ellipsoid along the axis represented by the corresponding eigenvector. What is the geometric interpretation of $Q$ resp. how does the choice of $Q$ affect $P$?","['linear-control', 'semidefinite-programming', 'control-theory', 'linear-algebra', 'ordinary-differential-equations']"
1523247,Tricky set proof in Halmos Naive Set Theory,"Proposition: All sets obtained from successive applications of the axiom of pairing to the empty set are unique. My attempts on this so far are to start with the empty set and pair upwards to an arbitrary number of nested sets, but my professor slams this approach because we haven't yet constructed the idea of ""numbers"", so this proof is meaningless. Instead I'm supposed to ""start from above"" with ""any number(!!) of applications of pairing, and then start stripping away using extension."" What the hell is going on? How do I prove this?",['elementary-set-theory']
1523270,"Gluing together Riemann surfaces, don't see why $Z$ is union of two compact sets.","Consider Lemma 1.7 from page 60-61 of Miranda's Algebraic Curves and Riemann Surfaces . For a link to the book, see here . Lemma 1.7. With the above construction, $Z$ is a compact surface of genus $g$ . The meromorphic function $x$ on $X$ extends to a holomorphic map $\pi: Z \to \mathbb{C}_\infty$ , which has degree $2$ . The branch points of $\pi$ are the roots of $h$ (and the point $\infty$ if $h$ as odd degree). Proof. One checks readily that $Z$ is Hausdorff, and hence is a Riemann surface. $Z$ is compact, since it is the union of two compact sets $$\{(x, y) \in X : \|x\| \le 1\} \text{ and }\{(z, w) \in Y : \|z\| \le 1\}.$$ To me, I don't see why $Z$ is the union of the above two compact sets. Can anyone clarify? Thanks.","['algebraic-curves', 'algebraic-geometry', 'riemann-surfaces', 'several-complex-variables', 'complex-analysis']"
1523296,Developing Mathematic Intuition [duplicate],"This question already has answers here : How to Garner Mathematical Intuition (3 answers) Closed 9 months ago . I'm an engineering student, currently working my way through the fundamental mathematics courses. I've done reasonably well so far—mostly A's and a couple of B's in Algebra, Statistics, Pre-Calculus, and Calculus I (I'm currently struggling quite a bit in Calculus II; so only time (and sweat; no blood or tears yet) will tell if I can maintain my academic performance after this course. However, although my school is good and well-ranked among community colleges, it's still a community college. None of the courses go too in-depth on any of the topics we cover. It's all about teaching us techniques and methods for solving problems (not extraordinarily difficult problems, either). It's not that the instructors aren't good - many are quite good and certainly know their math. But there just isn't time to spend on any individual topics. We covered all of the integration techniques that are taught at this level (with the exception of improper integrals) in about 2 weeks, or 8 class meetings. In spite of this (or maybe because I've realized a lot of the responsibility for learning the rest falls on me), I've really developed an awe and a love for mathematics. Not enough too switch majors; I still have an overwhelming desire to build robots. ;) But I really want to master the subjects in mathematics I'm exposed to, to really learn them thoroughly and at a deep level—not only because the better I do that, the better an engineer I'll be (I hope), but also because I'm really blown away by how cool the math is. So, my question is, how can I develop more adept mathematic thinking and reasoning skills, better math intuition? None of my classes have been proof-based, yet. Would starting to learn how to build proofs help my intuitive skills to grow faster? For instance, I've been studying (and struggling with a lot ) infinite sequences and series, and how to represent functions as power, taylor, and maclaurin series. I have made some progress, but I'm advancing very slowly. When I look at a formula like: $$P_0(x) = \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n}}{2^{2n} n!^2}$$ or even a more simple one, like: $$\sum_{n=1}^{\infty} \frac{(-1)^n 3^{n-1}}{n!}$$ I have a great deal of trouble seeing past the jumble of variables and constants to the pattern they describe. I want to reach the point at which I can see the matrix!  ;)  (the movie type, not the spreadsheet type). That's a joke of course, but seriously, while a mathematician may look at a matrix and see a mathematical structure, I have to think very hard, and sometimes to sketch an actual structure, to see a matrix as anything more than a large table of numbers. If learning to prove theorems isn't the answer, (or the whole answer), what are some things you can try to help increase your capability to think mathematically / logically about concepts in calculus, and mathematics in general?","['intuition', 'calculus', 'education', 'soft-question']"
1523329,Cusps and Points of Inflection,Can cusps be considered points of inflection? I'm getting conflicting information but my thought process is that cusps cannot be points of inflection? Can points of inflection exist when there is a vertical tangent to the graph? Assume there is change in concavity and the function is continuous.,"['curves', 'graphing-functions', 'calculus', 'derivatives']"
1523362,"Value of the limit without (or with, but giving rigorous arguments) using the Taylor expansion of sin","I'm trying to evaluate the limit as $N\to \infty.$ $$\frac{  \left(\dfrac{\sin \frac{1}{N}} {\frac{1}{N}}\right)^{N}   -1 }{\frac{1}{N}}.$$ Note first that, using L'Hôpital, one can easily show that the numerator goes to $0.$ Using the Taylor series expansion for $sin$, the value of the actual limit seems to be  $-\frac{1}{6}$. But I'm not fully sure how to justify the infinite series in the numerator is $-\frac{1}{6N}+O(\frac{1}{N^2})$. You could either justify that, if I was right, or may be use some other method to tell me what the limit is? Many thanks in advance!","['taylor-expansion', 'calculus', 'limits']"
1523363,"Good way to state, in english, the negation of ""$f$ equals zero almost everywhere"".","Let $X = (X, \mathcal{E}, \mu)$ be a measure space and let $f$ be a measurable function on $X$. Consider the statement: $f$ equals zero almost everywhere Is there an concise, unambiguous way of stating the negation of this statement? One shouldn't really just say something like $f$ does not equal zero almost everywhere because this could be interpreted as meaning that the complement of the set $\{x : f(x) \neq 0\}$ has measure zero. The best I've been able to come up with are variations on It is not the case that $f$ equals zero almost everywhere but this seems a bit clunky to me. Any advice?","['logic', 'article-writing', 'proof-writing', 'measure-theory']"
1523365,How can you show that 2 (adjacency) matrices are isomorphic?,"Would you do it by showing that elementary matrix operations can be used to get from one matrix to the other? If not, how would you show that 2 adjacency matrices are isomorphic?","['graph-isomorphism', 'graph-theory', 'group-isomorphism', 'discrete-mathematics']"
1523367,Axiom of Double Induction?,"What would the set-theoretical axiom of induction look like for double induction * when stated in the mathematical language of first- or second-order logic? * References as to What Double Induction Is : To questions on this StackExchange: 'Double Induction' 'Good Examples of Double Induction' 'A case of double induction?' 'Divisibility Proof with Induction - Stuck on Induction Step' ( this answer , in particular…) To other sources: 'Proof method:  Multidimensional induction' 'Mathematical Induction' (PDF) (See §14.2.4, 'Appendix 2 — The Basic Schemes of Induction:  Induction for the Natural Numbers:  Double Induction (Weak Form)' on p. 15…) 'Mathematical induction:  variants and subtleties' (PDF) (See §3, which starts at the end of p. 2…) 'Different kinds of Mathematical Induction' (PDF) (See variant 11 at the end of p. 2…) 'Proof by Mathematical Induction'/'[The] Principle of Mathematical Induction' (PDF) (See the section on 'Double Induction' that starts at the end of p. 12…)","['elementary-set-theory', 'induction', 'logic']"
1523370,"Let $f$ and $g$ linear operators where $f$ and $g$ commute and $f$ has simple spectrumm, then there is $P$ a polynomial such thah $g=P(f)$.","Let $f : \mathbb{C}^{n}\rightarrow \mathbb{C}^{n}$ be a linear operator with  a simple spectrum, furthermore, let $g : \mathbb{C}^{n}\rightarrow \mathbb{C}^n
$ be a linear operator such that  $f$ and $g$ commute. Show that there is $P$ a polynomial such that $g=P(f)$. Remark: The spectrum is simple when the characteristic polynomial has not multiple roots.","['spectral-theory', 'linear-algebra', 'functional-analysis']"
1523378,Elementary proof that finite sums of square roots of primes is irrational,"It is relatively easy to show that if $p_1$, $p_2$ and $p_3$ are distinct primes then $\sqrt{p_1}+\sqrt{p_2}$ and $\sqrt{p_1}+\sqrt{p_2}+\sqrt{p_3}$ are irrational, but the only proof I can find that $\sqrt{p_1}+\sqrt{p_2}+...+\sqrt{p_n}$ is irrational for distinct primes $p_1$, $p_2$, ... , $p_n$ requires we consider finite field extensions of $\mathbb{Q}$. Is there an elementary proof that $\sqrt{p_1}+\sqrt{p_2}+...+\sqrt{p_n}$ is irrational exist? (By elementary, I mean only using arithmetic and the fact that $\sqrt{m}$ is irrational if $m$ is not a square number.) The cases $n=1$, $n=2$, $n=3$ can be found at in the MSE question sum of square root of primes 2 and I am hoping for a similar proof for larger $n$.","['alternative-proof', 'prime-numbers', 'number-theory', 'irrational-numbers']"
1523392,"Prove that the natural map $\alpha : \text{Hom}(X,\text{Spec} A) \rightarrow \text{Hom}(A,\Gamma(X,\mathcal{O}_X))$ is an isomorphism","This is question 2.4 in Hartshorne. Let $A$ be a ring and $(X,\mathcal{O}_X)$ a scheme. We have the associated map of sheaves $f^\#: \mathcal{O}_{\text{Spec } A} \rightarrow f_* \mathcal{O}_X$. Taking global sections  we obtain a homomorphism $A \rightarrow \Gamma(X,\mathcal{O}_X)$. Thus there is a natural map $\alpha : \text{Hom}(X,\text{Spec} A) \rightarrow \text{Hom}(A,\Gamma(X,\mathcal{O}_X))$. Show $\alpha$ is bijective. I figure we need to start off with the fact that we can cover $X$ with affine open $U_i$, and that a homomorphism $A \rightarrow \Gamma(X,\mathcal{O}_X)$ induces a morphism of schemes from each $U_i$ to $\text{Spec} A$ and some how glue them together. But I have no idea how to show that the induced morphisms agree on intersections. How does this work?",['algebraic-geometry']
1523416,What are differences and relationship between shannon entropy and fisher information?,"When I first got into information theory, information was measured or based on shannon entropy or in other words, most books I read before were talked about shannon entropy. Today someone told me there is another information called fisher information. I got confused a lot. I tried to google them. Here are links,
fisher information: https://en.wikipedia.org/wiki/Fisher_information and shannon entropy goes here https://en.wikipedia.org/wiki/Entropy_(information_theory) . What are differences and relationship between shannon entropy and fisher information? Why do two kinds of information exist there? Currently, my idea is that it seems fisher information is a statistical view while shannon entropy goes probability view. Any comments or anwsers are welcome. Thanks.","['probability-theory', 'entropy', 'information-theory', 'statistics']"
1523427,"Is there a countable cover of $\mathbb{R}^2$ by balls $B(x_n, n^{-1/2})$?","Is it possible to cover all of $\mathbb{R}^2$ using balls $\{ B(x_n,n^{-1/2})\}_{n=1}^\infty$ of decreasing radius $n^{-1/2}$? I know that if we chose e.g. radius $n^{-1}$ it could never work because $\sum \pi (n^{-1})^2 < \infty$. But in this case the balls cover an infinite amount of area, so it seems that it may be possible to construct. Edit: It can definitely be done with balls of radius $n^{-1/2+\epsilon}$ for any $\epsilon > 0$.","['real-numbers', 'real-analysis', 'measure-theory']"
1523461,What is the intuition behind differential forms? [duplicate],"This question already has answers here : What's the geometrical intuition behind differential forms? (2 answers) Closed 3 years ago . I am comfortable with the way physicists use differentials as elements of area/volume. I know the (algebraic) formal definition of differential forms, but it makes no intuitive sense, especially since it is not immediately compatible (to me) with the physicist POV. How do the two fit in?","['differential-geometry', 'differential-forms']"
1523473,Matrix Multiplication where $A\times B = B\times A$,"I am taking a CS class that involves OpenGL in C++ , but we are going through some trig and I was stumped by a question on our test involving matrices and multiplication of matrices. I'm going to be specific as possible but I can't remember the exact wording because it has been handed in so bear with me: ""Show me two $4\times4$ matrices ($M_1$ and $M_2$) where the rule $A\times B = B\times A$ is true for $M_1$ and $M_2$."" This teacher only discussed this rule briefly and certainly did not discuss cool exception to the rule. Should $M_2$ be the inverse of $M_2$ in order for this to be true? Or could $M_1$ be anything and $M_2$ be the Identity Matrix?","['trigonometry', 'matrices']"
1523482,Showing $|G|=90$ means $G$ is not simple by centraliser argument,"I want to show that a group $G$, with $|G|=90$ cannot be simple, specifically using a centraliser argument. The exercise gives a walkthrough really of what I am to do, but I am even then, still having trouble: Assume $G$ is simple. Let $P$ be a Sylow $3$-subggrup of $G$ and let $x\in P, x\ne 1$. Show that $C_G(x)=P$ and obtain a contradiction to conclude that $G$ is not simple. Now $n_3=1$ or $10$, and $P$ is of order $9=3^2$,(we know that $|H|=p$ or $p^2$, means that $H$ is abelian) which means that $P$ is abelian, hence $C_P(x)\supseteq P$, so this is where I am stuck. I understand that if $C_G(x)=P$ then I am done, since $C_G(x)\trianglelefteq G$, but if $P\subset G$ is abelian, is then $C_G(x)=P$? i.e. is it abelian with elements of $G\backslash P$ for some reason?","['abstract-algebra', 'sylow-theory', 'group-theory']"
1523490,Composition of flat morphisms,"Suppose I have nice enough schemes $X,Y,Z$ over a field $k$ (I can take $X,Y,Z$ to be smooth projective connected varieties and $k$ to be algebraically closed of characteristic 0 if needed) and morphisms of $k$-schemes $$X \xrightarrow{a} Y \xrightarrow{b} Z$$ 
such that their composition $b \circ a$ is flat and $a$ is flat. Can I conclude that $b$ is also flat?",['algebraic-geometry']
1523505,Prove a map of binary expansion is continuous,"Prove that the map: f: $\{0,1\}^\mathbb{N} \times \{0,1\}^\mathbb{N}$ $\to$ $[0,1] \times [0,1]$ is continuous. I know that the map can be written as ($m_1,m_2,m_3,m_4,...$) $\times$ ($n_1,n_2,n_3,n_4,...$) $\mapsto$ ($0.m_1m_2m_3m_4...,0.n_1n_2n_3n_4...$) as binary expansions, and f is surjective, but how do I show it is continuous?","['elementary-set-theory', 'real-analysis', 'functional-analysis', 'binary', 'general-topology']"
1523521,"Prove that $[0,1]$ isn't homeomorphic to $\mathbb{R}$","Prove that $[0,1]$ isn't homeomorphic to $\mathbb{R}$ My first thought is that there can not be a continuous bijection, $f$, from $[0,1]$ to $\mathbb{R}$ because a continuous function that maps $[0,1]\rightarrow \mathbb{R}$ must be bounded so there can't be a surjection. So that would then be the proof. Though I believe I am incorrect in my thinking because a hint for the assignment says to use the intermediate value theorem. That is ""Suppose $f:[a,b]\rightarrow\mathbb{R}$ is continuous. If $f(a)<\delta<f(b)$ or $f(b)<\delta<f(a)$ then $\delta=f(c)$ for some $c\in[a,b]$"". Why is my first thought wrong and how is the IVT useful?","['real-analysis', 'general-topology']"
