question_id,title,body,tags
4591271,Complex integral \[ \int_{0}^{+\infty}\frac{e^{\cos(x)}\cos\left(\sin\left(x\right)\right)}{a^{2}+x^{2}}dx. \],"I am trying to solve this real integral using complex integration with semi circle. To do this with semicircle I noticed that the function is even, so $\int_{-\infty}^{+\infty}f\left(x\right)dx=2\int_{0}^{+\infty}f\left(x\right)dx$ . $$
\int_{0}^{+\infty}\frac{e^{\cos(x)}\cos\left(\sin\left(x\right)\right)}{a^{2}+x^{2}}dx.
$$ I observe the function $\displaystyle f\left(z\right)=\frac{e^{e^{iz}}}{a^{2}+z^{2}}$ . Because I want to solve the integral using semi circle, only pole $z=ia$ is inside the circle. The residue in that pole is $\frac{e^{e^{-a}}}{i2a}$ and the value of integral is $$\int_{C_{R}}f\left(z\right)dz+\int_{-R}^{R}f\left(z\right)dz=2\pi i\cdot\frac{e^{e^{-a}}}{i2a}=\frac{\pi e^{e^{-a}}}{a}.$$ Now I want to solve the integral $\int_{C_{R}}f\left(z\right)dz$ : With the identity $z=Re^{i\theta}$ we have $dz=Rie^{i\theta}d\theta$ . Now: \begin{align*}
\left|\int_{C_{R}}f\left(z\right)dz\right| & \leq\int_{C_{R}}\left|f\left(z\right)\right|dz\\
 & \leq\int_{0}^{\pi}\left|\frac{e^{e^{i\cdot Re^{i\theta}}}}{a^{2}+R^{2}e^{i2\theta}}\cdot R\right|\cdot{\left|ie^{i\theta}\right|}\cdot d\theta\\
 & =\int_{0}^{\pi}\left|\frac{e^{e^{i\cdot Re^{i\theta}}}}{a^{2}+R^{2}e^{i2\theta}}\cdot R\right|\cdot d\theta.
\end{align*} I use $R^{2}=\left|R^{2}e^{i2\theta}\right|=\left|R^{2}e^{i2\theta}+a^{2}-a^{2}\right|\leq\left|R^{2}e^{i2\theta}+a^{2}\right|+\left|-a^{2}\right|$ thus $$\left|R^{2}e^{i2\theta}+a^{2}\right|\geq R^{2}-a^{2}\,\,\Rightarrow\,\,\frac{1}{\left|R^{2}e^{i2\theta}+a^{2}\right|}\leq\frac{1}{R^{2}-a^{2}}.$$ Now \begin{align*}
\left|\int_{C_{R}}f\left(z\right)dz\right| & \leq\int_{0}^{\pi}\frac{\left|e^{e^{i\cdot Re^{i\theta}}}\right|R}{R^{2}-a^{2}}\cdot d\theta\\
 & =\frac{R}{R^{2}-a^{2}}\int_{0}^{\pi}\left|e^{e^{i\cdot Re^{i\theta}}}\right|d\theta=\frac{R}{R^{2}-a^{2}}\int_{0}^{\pi}\left|e^{e^{iR\left(\cos\theta+i\sin\theta\right)}}\right|d\theta\\
 & =\frac{R}{R^{2}-a^{2}}\int_{0}^{\pi}\left|e^{e^{iR\cos\theta}\cdot e^{-R\sin\theta}}\right|d\theta
\end{align*} I don't know what to do with the last integral. Colud someone help me?","['complex-analysis', 'complex-integration']"
4591298,Doob's Optional Stopping Theorem to find probabilities of stopping times,"Suppose we have a simple random walk starting from $S_0=0$ , and $S_n=X_1+\dots+X_n$ such that $$\mathbb{P}(X_i=1)=p \hspace{1em}\mathbb{P}(X_i=0)=r \hspace{1em} \mathbb{P}(X_i=-1)=q$$ for positive $p,q$ and $r$ with $p+q+r=1$ and suppose $p \neq q$ . For any integer $x$ , let $T_x = \text{min}\{n \geq 1: S_n = x\}$ be the stopping time that is the first time the random walk hits $x$ . For $a<0<b$ , I want to use Doob's Optional Stopping Theorem to compute both $\mathbb{P}(T_a < T_b)$ and $\mathbb{P}(T_b < T_a)$ . That is, the probability that the random walk hits $a$ in less moves than it takes to hit $b$ , and vice-versa. My understanding of this Theorem is not the best and I think that the general idea is to find one of the conditions in the theorem to be satisfied, to then deduce that the expected value of the stopped process is equal to the initial value of our martingale. I am not sure where I would go from here so I would appreciate any hints please!","['conditional-expectation', 'expected-value', 'martingales', 'probability-theory', 'random-variables']"
4591304,Zero locus of polynomials in small field vanishing on a point in a larger field,"Let $L/K$ be a Galois field extension. Fix $n\geq 1$ and a point $\bf{x}$ in $L^n$ . Let $I$ be the ideal of $K[X_1,\dotsc ,X_n]$ consisting of all polynomials vanishing on $\bf{x}$ .
Let $X$ be the subset of $L^n$ consisting of the common solutions for all polynomials in $I$ .
Let $Y$ be the $\operatorname{Gal}(L/K)$ -orbit of ${\bf x}$ . Clearly $Y\subset X$ . Is $X=Y$ ? I want to know this because I have a polynomial $g\in K[X_1,\dotsc ,X_n]$ that does not vanish on $\bf{x}$ , but does vanish on some ${\bf y} \in L^n$ , and I want to prove that ${\bf y}$ is not a common solution for the polynomials in $I$ .","['field-theory', 'algebraic-geometry']"
4591328,Computing the De Rham Cohomology Group of Connected Sum $H_{\rm dR}^p(M_1 \# M_2)$,"I'm reading Lee's Introduction to Smooth Manifolds. I have a question about problem 17-7 . Problem 17-7 Let $M_1$ , $M_2$ be connected smooth manifolds of dimension $n\geq3$ , and let $M_1\# M_2$ denote their smooth connected
sum (Example 9.31). Prove that $H_{\rm dR}^p(M_1 \# M_2)\cong H_{\rm
dR}^p(M_1)\oplus H_{\rm dR}^p(M_2)$ for $0<p<n-1$ . Prove that the same
is true for $p=n-1$ if $M_1$ and $M_2$ are both compact and
orientable. [Hint: use Problems 9-12 and 17-6.] Problem 9-12 Suppose $M_1$ and $M_2$ are connected smooth $n$ -manifolds and $M_1\# M_2$ is their smooth connected sum (see
Example 9.31). Show that the smooth structure on $M_1\# M_2$ can be
chosen in such a way that there are open subsets $\widetilde{M_1},
\widetilde{M_2}\subseteq M_1\# M_2$ that are diffeomorphic to $M_1-\{p_1\}$ and $M_2-\{p_2\}$ , respectively, such that $\widetilde{M_1}\cup\widetilde{M_2}=M_1\# M_2$ and $\widetilde{M_1}\cap\widetilde{M_2}$ is diffeomorphic to $(-1,1)\times\mathbb{S}^{n-1}$ . Problem 17-6 Let $M$ be a connected smooth manifold of dimension $n\geq3$ . For any $x\in M$ and $0\leq p\leq n-2$ , prove that the map $H_{\rm dR}^p(M)\rightarrow H_{\rm dR}^p(M-\{x\})$ induced by
inclusion $M-\{x\}\hookrightarrow M$ is an isomorphism. Prove that the
same is true for $p=n-1$ if $M$ is compact and orientable. Here is my attempt about problem 17-7: According to problem 9-12, there are open subsets $\widetilde{M_1}, \widetilde{M_2}\subseteq M_1\# M_2$ , such that $\widetilde{M_1}$ is diffeomorphic to $M_1-\{p_1\}$ , $\widetilde{M_2}$ is diffeomorphic to $M_2-\{p_2\}$ , $\widetilde{M_1}\cup\widetilde{M_2}=M_1\# M_2$ and $\widetilde{M_1}\cap\widetilde{M_2}$ is diffeomorphic to $(-1,1)\times\mathbb{S}^{n-1}$ . Note that $(-1,1)\times\mathbb{S}^{n-1}$ and $\{0\}\times\mathbb{S}^{n-1}$ are homotopy equivalent, and $\{0\}\times\mathbb{S}^{n-1}$ is diffeomorphic to $\mathbb{S}^{n-1}$ . Therefore $H_{\rm dR}^p(\widetilde{M_1}\cap\widetilde{M_2})\cong H_{\rm dR}^p((-1,1)\times\mathbb{S}^{n-1})\cong H_{\rm dR}^p(\mathbb{S}^{n-1})=0$ for $0<p<n-1$ . According to Mayer–Vietoris Theorem, $$\cdots\rightarrow H_{\rm dR}^{p-1}(\widetilde{M_1}\cap\widetilde{M_2})\rightarrow H_{\rm dR}^p(M_1\# M_2)\rightarrow H_{\rm dR}^p(\widetilde{M_1})\oplus H_{\rm dR}^p(\widetilde{M_2})\rightarrow H_{\rm dR}^p(\widetilde{M_1}\cap\widetilde{M_2})\rightarrow\cdots$$ is exact. The groups on both ends are trivial when $1<p<n-1$ , which implies that $$H_{\rm dR}^p(M_1 \# M_2)\cong H_{\rm dR}^p(\widetilde{M_1})\oplus H_{\rm dR}^p(\widetilde{M_2})\cong H_{\rm dR}^p(M_1-\{p_1\})\oplus H_{\rm dR}^p(M_2-\{p_2\}).$$ According to problem 17-6, for $0\leq p\leq n-2$ , $$H_{\rm dR}^p(M_i-\{p_i\})\cong H_{\rm dR}^p(M_i),i=1,2.$$ Therefore $$H_{\rm dR}^p(M_1 \# M_2)\cong H_{\rm dR}^p(M_1)\oplus H_{\rm dR}^p(M_2)$$ for $1<p<n-1$ . My question is how to handle the cases $p=1$ and $p=n-1$ . Group on the left end of the exact sequence is isomorphic to $\mathbb{R}$ (not trivial) when $p=1$ . If $M_1$ and $M_2$ are both compact and orientable, according to problem 17-6, though $$H_{\rm dR}^{n-1}(M_i-\{p_i\})\cong H_{\rm dR}^{n-1}(M_i),i=1,2,$$ group on the right end of the exact sequence is isomorphic to $\mathbb{R}$ (not trivial). Both of the two cases will make it unclear to directly infer $$H_{\rm dR}^p(M_1 \# M_2)\cong H_{\rm dR}^p(\widetilde{M_1})\oplus H_{\rm dR}^p(\widetilde{M_2}).$$ Any help would be appreciated.","['de-rham-cohomology', 'smooth-manifolds', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
4591365,"Simple question on calculation of Fisher metric for $p(x,\theta)$","The Fisher information metric is defined as: $$g_{jk}(\theta)=
\int_X
 \frac{\partial \log p(x,\theta)}{\partial \theta_j}
 \frac{\partial \log p(x,\theta)}{\partial \theta_k}
 p(x,\theta) \, dx.$$ Say $p(x,\theta):= \exp \frac{\theta}{\log x},$ for $x\in(0,1).$ Then $\theta \in (0,\infty)$ gives us a coordinate on a $1-$ manifold equivalent to the positive real line $(0,\infty).$ I calculated (confirming with wolfram alpha) the integral to be a Bessel function: $$\frac{2K_1(2\sqrt{\theta})}{\sqrt{\theta}} $$ But this calculation was done without the normalization constant. Also I'm not sure how to write my result in the proper form i.e. as a metric. Am I on the right track here? What is the correct calculation and proper way to write the metric? I think I basically got the answer but looking for verification of the solution and additional feedback on how to write the metric. Edit: With the normalization constant included, I found the metric to be: $$ g(\theta)=\frac{K_0(2\sqrt{\theta})^3}{\theta K_1(2\sqrt{\theta})^2}-2K_0(2\sqrt{\theta})^2+\frac{1}{\theta}. $$ I think this is probably the correct form of the metric. Still unsure of the $jk$ subscripts and where they come into play in writing the metric properly.","['probability-distributions', 'information-geometry', 'solution-verification', 'fisher-information', 'probability-theory']"
4591387,"If $f(x+y)=f(x)u(y)+f(y)$ for some $f$ monotonic, find all such $u$ functions","Find all functions $u:R\rightarrow{R}$ for which there exists a
strictly monotonic function $f:R\rightarrow{R}$ such that $f(x+y)=f(x)u(y)+f(y)$ for all $x,y\in{\mathbb{R}}$ Let $P(x,y)$ be the assertion of f.e as usual. Let the strictly monotonic property be shown as $(\clubsuit)$ . Clearly $u(x) \neq 0$ and $f$ is injective. \begin{align*}
    &P(0,x):f(0)u(x)=0  \stackrel{(\clubsuit)}{\Rightarrow} f(0)=0 \\
    &P(x,0): f(x)=f(x)u(0) \stackrel{(\clubsuit)}{\Rightarrow} u(0)=1 \\ 
    &P(-x,x): f(-x)u(x)+f(x)=0 \Rightarrow  u(x) = \frac{ -f(x)}{f(-x)} \qquad &(\spadesuit) \\
    &P(x,y)-P(y,x)= f(x)\big(u(y)-1 \big)= f(y)\big( u(x)-1 \big)  \Rightarrow \frac{f(x)}{f(y)}= \frac{u(x)-1}{u(y)-1} &\qquad (1)
  \end{align*} Assume $u(a)=1$ for some $a \neq 0$ . By $(\clubsuit)$ if $x \neq 0$ in $(\spadesuit)$ we have $f(x) \neq f(-x)$ . if $-f(x)=f(-x)$ , or that is, $f$ is odd function, analyzing $P(-x,x)$ once more yields $$ f(x) \big( -u(x)+1\big)=0$$ Since $f$ is obviously not constant, we must have $u(x)=1$ which is a clear solution. Hence, for sake of finding another solutions, assume $u(a)=1$ only if $a=0$ . Observe $(1)$ carefully. $f(x)$ increases as same as $u(x)-1$ , or in other words $u'(x)= f'(x)$ i.e  if there exist another solution $u$ , then $f(x)=u(x)+c$ for some constant satisfishes the problem. In fact, $f'(x)=g'(x)$ is a very well known problem for  real numbers, which as the only solution (IFF) $f(x)=g(x)+c$ for some constant $c$ .
Then, $u(x)$ is also strictly monotonic function, which is obvious why. And, we can even find what our constant is, since $u(0)=1$ and $f(0)=0$ , $c=-1$ . Then we can rewrite our f.e as $$u(x+y)= u(x)u(y)$$ Which is a well known problem and the only solutions are $u(x)=e^{ax}$ for some parameter $a$ which indeed works, hence we are done. My problem: The problem about my solution is $f'(x)=g'(x)$ part.I am not sure but I think that $f,u$ being not continous or differentiable doesn't change anything ( still, $f,u$ is almost differentiable since they are monotonic, even if our claim is wrong, it must be true on every value except  set of measure $0$ ), the fact that $(1)$ is true for all $x$ must imply there are some $f$ such that $f(x)+1=u(x)$ , am I wrong? If I am wrong, is it possible that I can prove $f,u$ is continous and differentiable? Is it possible to fix my solution? Can someone help please? Thanks","['functions', 'real-analysis']"
4591473,The possible number of intersections of n lines,"Suppose there are $n$ lines on a plane with no $3$ lines concurrent, such that they are not all parallel. What are the possibilities for the number of intersections of these lines? I get that the minimum number is $n-1$ and the maximum number is $n(n-1)/2$ . Can we find a configuration of $n$ lines such that there are $x$ points of intersection, for any $x$ between $n-1$ and $n(n-1)/2$ ? This seems to be true when $n$ is $3$ and $4$ .","['discrete-geometry', 'combinatorics', 'geometry']"
4591514,"Every sequential space is compactly generated, and both categories of spaces are cartesian closed. Do the products coincide for sequential spaces?","The categories of sequential spaces and compactly generated spaces both use a finer product than the one from $\textbf{Top}$ in order to be cartesian closed, and in both cases the product is arguably more natural as well, especially in the sequential case. Every sequential space is (Hausdorff-) compactly generated. This can be seen either indirectly from it being a quotient of a metric space, or more explicitly by viewing sequential spaces as generated by maps from the compact (Hausdorff) space $\mathbb{N}\cup{\{\infty\}}$ that characterize convergent sequences. Does the product of sequential spaces coincide with the one from compactly generated spaces? I'm interested both in the case of finite products and infinite products / exponentials. Since every sequential space is compactly generated, abstract nonsense quickly shows that the sequential product topology is finer than or equal to the compactly generated product topology, but the other direction is harder. So it can be reduced to whether the (compact closed) product of sequential spaces is sequential.","['product-space', 'category-theory', 'general-topology', 'cartesian-closed-categories', 'compactness']"
4591531,Determining when $A(e_n) = \alpha_n\sum_{i=n}^{2n}e_i$ is a bounded linear function on $l^1(\mathbb{N})$,"Consider the space $S = l^1(\mathbb{N})$ over complex coefficients and define $A$ by $A(e_n) = \alpha_n\sum_{i=1}^{2n}e_n$ for the standard basis $\left(e_n\right)_{n\in\mathbb{N}}$ of $S$ . I am trying to determine when $A$ is a bounded linear function, but currently I am stuck at showing that $A$ 's codomain really is $l^1(\mathbb{N})$ , which in turn seems to be equivalent for $A$ to be continuous. I mean the following: $u \equiv \sum_{i=1}^\infty u_ie_i, u_i\in\mathbb{C}$ such that $\sum_{i=1}^\infty |u_i| < \infty$ . Now, $$A\left(u\right) = A\left(\sum_{i=1}^\infty u_ie_i\right)$$ and I would like to be able to conclude that $$A\left(u\right) = \sum_{i=1}^\infty A(u_ie_i) = \sum_{i=1}^\infty u_i\alpha_i\sum_{j=i}^{2i}e_i$$ But I don't really see an immediate way to conclude this. Being able to pull out the limit is equivalent to $$||\lim_{N\to\infty}\sum_{i=1}^NA(u_ie_i) - A(u)|| = \lim_{N\to\infty}||\sum_{i=1}^NA(u_ie_i) - A(u)|| = 0$$ but the problem is that we don't a priori know that $A(u)$ is, and hence concluding the convergence feels sketchy. How should I proceed with this proof?","['banach-spaces', 'lp-spaces', 'functional-analysis', 'linear-transformations']"
4591774,How to solve Volterra Integral Equation,"I need help solving an Integral Equation that my professor showed without teaching to us. I've tried emulating the textbooks example but I still can't crack it. $$ f(t) = \cos\, t + 4 \, e^{-2t} - \int \sin\, (t-\tau)f(\tau)\, dt $$ My work so far on the problem: \begin{align}
f(t) &= \cos\, t + 4 \, e^{-2t} - \int \sin\, (t-\tau)f(\tau)\, dt \\
\mathcal{L}\{f(t)\} &= \mathcal{L}\{\cos\, t\} + \mathcal{L}\{4 \, e^{-2t}\} - \mathcal{L}\left\{\int \sin\, (t-\tau)f(\tau)\, dt \right\} \\
F(s) &= \frac{s}{s^2 + 1} + \frac{4}{s+2} - F(s)\frac{1}{s^2 + 1} \\
F(s) &= \frac{s}{(s^2+2)} + \frac{4 s^2+1}{(s+2)(s^2+2)}
\end{align} I know that from here I'm supposed to take the inverse Laplace Transform to find $f(t)$ but everything I've tried on the second term (Partial fractions, expanding it out) either fails me or gives me a super ugly equation that I'm sure is false Any and all help is appreciated.","['integral-equations', 'ordinary-differential-equations']"
4591787,Does Pointwise Convergence imply L2 Convergence in L2?,"Its clear that convergence in $L^2$ does not necessarily imply pointwise convergence a.e but does this work the other way too? I considered the characteristic function $\chi_{[n, n+1]}$ which I guess pointwise converges to 0 in $L^2$ but not for $x \in (0,1)$ I think but I want the example for the other way around for something that converges pointwise but not in $L^2$ if the function in $L^2$ is bounded?","['hilbert-spaces', 'measure-theory', 'lp-spaces', 'real-analysis']"
4591889,Transform the region and compute the integral,"The solution for the integral $I=\int_{0}^{1}\int_{0}^{1}\frac{1}{1-x^2 y^2}dxdy$ is given by transformation $$
x=\frac{\sin u}{\cos v}\qquad \text{and}\qquad y=\frac{\sin v}{\cos u}
$$ and then integral becomes $I=\int\int_{E}dudv$ , where $E$ is the trinagle with vertices $(0,0)$ , $(\frac{\pi}{2},0)$ and $(0,\frac{\pi}{2})$ . I can not see how the square $\{(x,y):0\le x,y\le 1\}$ transform to the triangle $E$ under this transform? Since $x,y\geq 0$ , both sine and cosine should have same sign. It should also be $\sin u\le \cos v$ and $\sin v\le \cos u$ since $x,y\leq 1$ . For example let $A=\{(0,y):0\le y\le 1\}$ be one side of square. Under this transformation $0=x=\frac{\sin u}{\cos v}\rightarrow \sin u=0\rightarrow \cos u=1 \text{  or } \cos u=-1$ . In first case, $\cos u=1\rightarrow y=\sin v\rightarrow 0\le \sin v\le 1\rightarrow v\in [0,\frac{\pi}{2}]+2k\pi$ . In second case, $\cos u=-1\rightarrow y=-\sin v\rightarrow -1\le \sin v\le 0\rightarrow v\in [-\frac{\pi}{2},0]+2k\pi$ . I can not see the line segment $A$ transform which shape under this transformation?","['integration', 'multivariable-calculus', 'calculus', 'real-analysis']"
4591893,Representation of a martingale as a stochastic integral,"Let $\xi_{1},\xi_{2},\dots$ be an i.i.d. sequence that takes values $\pm 1$ with equal probabilities. Define the simple symmetric random walk with $X_{0}=0$ , and $X_{n}=\sum_{i=1}^{n}\xi_{i}$ . Define the filtration $\mathcal {F}_{n}=\sigma\left(X_{1},\dots, X_{n}\right)=\sigma(\xi_{1},\dots,\xi_{n})$ . Let $\mathcal {F}_{\infty}=\sigma\left(\cup_{n=1}^{\infty}\mathcal {F}_{n}\right)$ . Let $T\in \mathcal {F}_{\infty}$ be an integrable random variable. I want to show that there is a martingale sequence ( $M_{n},n\in \mathbb {N}$ ) such that $M_{n}\to T$ almost surely. Furthermore, I can always write $M_{n}$ as $$
M_{n}=\mathbb {E}(T)+\sum_{i=1}^{n}A_{i}\left(X_{i}-X_{i-1}\right),
$$ for some predictable sequence $(A_{i},i\in \mathbb {N})$ . Seeking for some hints on doing both.","['martingales', 'probability']"
4591921,Question about the PHP (The pigeonhole principle PHP),"A class of 32 students is organized in 33 teams. Every team consists of three students and there are no identical teams. Show that there are two teams with exactly one common student. The PHP as I know is like, there are 6 students and 5 groups, there must be a group that has two students according to the PHP.
How can solve the question by using PHP?","['pigeonhole-principle', 'functions', 'discrete-mathematics']"
4591985,Proving a statement by using its converse.,"Suppose we have proved $Q \Rightarrow P$ with the help of $P \Rightarrow Q.$ In order to avoid circularity, we haven't used $Q \Rightarrow P$ to prove $P \Rightarrow Q$ --- that is, the truth of $P \Rightarrow Q$ is established independently of $Q \implies P.$ Thus, we have in our system $P \Rightarrow Q$ (independently) and $Q \Rightarrow P$ (with the help of its converse). Together, we have $$P \Leftrightarrow Q$$ I suppose this should a fine proof technique, albeit a bit awkward at first glance. What are your thoughts ? Please do leave a comment/answer on the validity of such method. Any elementary examples ? Thanks for your time in advance !","['propositional-calculus', 'logic', 'discrete-mathematics']"
4592006,Calculting the infimum,"Consider the maps $u, \phi:\mathbb{R}^n \to \mathbb{R^n}$ . Let $\mathcal{F}$ be some class of functions and consider the quantity $$\inf_{u \in \mathcal{F}} \left( \frac{|u|^2}{2}+u\cdot\phi\right) .$$ Assuming that $-\phi \in \mathcal{F}$ , I guess the infimum will be attained at $u=-\phi$ as this is a parabola. Is this observation correct and can something be said if we consider $$\inf_{u \in \mathcal{F}} \left( \frac{|u|^{2n}}{2n}+u\cdot\phi\right) $$ instead?","['analysis', 'real-analysis']"
4592044,Solving $\min\int_{0}^{1} \left( x^2 + 2tx + tx\dot{x} + \dot{x}^2 \right) {\rm d} t$ with $x(0)=0$ and $x(1)=1$,"Solve $$\min\int_{0}^{1} \left( x^2 + 2tx + tx\dot{x} + \dot{x}^2 \right) {\rm d} t, \qquad x(0)=0, \quad x(1)=1$$ We need to solve the problem with the Euler Equation $$\frac{\partial F}{\partial x}-\frac{d}{dt}\left(\frac{\partial F}{\partial\dot{x}}\right)=0$$ We define $F$ as $$F(t,x,\dot{x})=x^2+2tx+tx\dot{x}+\dot{x}^2$$ Then subsequently we have: $$ \begin{aligned} \frac{\partial F}{\partial x} &= 2x+2t+t\dot{x} \\ \frac{\partial F}{\partial\dot{x}} &= tx+2\dot{x} \end{aligned} $$ When substituting this into the above mentioned equation: $\dfrac{\partial F}{\partial x}-\dfrac{d}{dt}\left(\dfrac{\partial F}{\partial\dot{x}}\right)=0$ , my final equation becomes $\ddot{x}-\frac{1}{2}x=t$ which is incorrect recording to the Student's Manual. The correct 'answer' should be $\ddot{x}-\frac{1}{2}x=\frac{1}{2}t$ . Could someone point out where I made a small/big mistake?","['ordinary-differential-equations', 'calculus-of-variations', 'euler-lagrange-equation', 'calculus', 'derivatives']"
4592102,"$A(z_1)$, $B(z_2)$ and $C(z_3)$ be vertices of ABC s/t $|z_1|=|z_2|=|z_3|=1$, $z_1+z_2\cos\alpha+z_3\sin\alpha=0$ then find $\bar z_2z_3+z_2\bar z_3$","If $A(z_1)$ , $B(z_2)$ and $C(z_3)$ be the vertices of a triangle ABC such that $|z_1|=|z_2|=|z_3|=1$ and there exist $\alpha\in(0,\frac\pi2)$ such that $z_1+z_2\cos\alpha+z_3\sin\alpha=0$ then find the value of $\bar z_2z_3+z_2\bar z_3$ and the maximum area of triangle $ABC$ . My Attempt: $A,B,C$ lie on the circle $|z|=1$ . Thus, circumcentre is origin. First I thought maybe the angle $\alpha$ is angle between two sides. But since that is not explicitly mentioned, I think maybe angle $\alpha$ could be just anything. Then I thought maybe $\cos\alpha$ , $\sin\alpha$ is a hint to apply inequalities. I tried but in vain. I tried squaring the given equation but couldn't finish. Edit: Thanks to Hari Shankar's answer below, I now know the answer to the first part. For area of triangle, I used determinant form and got $$z_2\bar{z_3}(\sin\alpha+\cos\alpha+1)$$ Is this correct? How to find maximum from this? The answer given is $1.21$ .","['contest-math', 'trigonometry', 'triangles', 'complex-numbers']"
4592129,Finding the probability of $P(40.5<Y<48.9|X>=68.6)$,"Two students go to a pizza place every week. Let $X$ and $Y$ be the weekly spend of each student at the pizza place. Assume that $X$ and $Y$ have a bivariate normal distribution with $\mu_X=60.6, \sigma_X=11.2, \mu_Y=46.8, \sigma_Y=8.4, \rho=0.94$ a) Find $P(40.5<Y<48.9|X\geq  68.6)$ EDIT: I think I have a solution. Can someone please verify? $$f_{XY}(x,y)=\frac{1}{2\pi(11.2)(8.4)\sqrt{1-(0.94)^2}}*\exp{-\frac{1}{2(1-(0.94)^2)}[(\frac{x-60.6}{11.2})^2+(\frac{y-46.8}{8.4})^2]-2(0.94)\frac{(x-60.6)(y-46.8)}{(11.2)(8.4)}}$$ $P(40.5<Y<48.9|X\geq  68.6)=\frac{P(40.5<Y<48.9, X\geq 68.6)}{P(X\geq 68.6)}$ $P(X\geq 68.6)=P(Z\geq 0.7143)=0.23752$ $$\int_{40.5}^{48.9}\int_{68.6}^{\infty}\frac{1}{2\pi(11.2)(8.4)\sqrt{1-(0.94)^2}}*\exp{-\frac{1}{2(1-(0.94)^2)}[(\frac{x-60.6}{11.2})^2+(\frac{y-46.8}{8.4})^2]-2(0.94)\frac{(x-60.6)(y-46.8)}{(11.2)(8.4)}} dx dy = 0.0051345$$ $P(40.5<Y<48.9|X\geq  68.6) = \frac{0.0051345}{0.23752}$ b) Find $P(40.5<Y<48.9)$ $=P(-0.75<Y<0.25)=0.3721$ c) Find $P(40.5<Y<48|X=68.6)$ $E(Y|X)=46.8+(0.94)(\frac{8.4}{11.2})(68.6-60.6)=52.44$ $Var(Y|X)=(8.4)^2-(0.94)^2(8.4)^2=8.21$ $P(-4.17<Z<-1.24)=0.1075$ d) What is the relationship between these probabilities? The relationship is that part (a) is smaller than the other two probabilities since you are dividing out a larger number. Also part(b) is univariate while part (a) and part (c) is bivariate. I am mostly just trying to see whether my solutions for parts (a) and part (d) are correct because those are the ones I am most confused about.","['conditional-probability', 'probability']"
4592155,Calculate the norm of the functional $f(x)=\displaystyle\int_{-1}^1tx(t)dt$,"Find the norm of the functional $f(x)=\displaystyle\int_{-1}^1tx(t)dt$ in the space $C^1[-1,1]$ , where the norm is given by $\|x\|=\max\limits_{t\in[-1,1]}|x(t)|+\max\limits_{t\in[-1,1]}|x'(t)|$ . The best upper bound I managed to get: integrating by parts, we get $$f(x)=\displaystyle\int_{-1}^1\left(\dfrac{1}{2}-\dfrac{t^2}{2}\right)x'(t)dt.$$ Adding this to the original functional and dividing by two, we get: $$f(x)=\displaystyle\int
_{-1}^1\left(\dfrac{t}{2}x(t)+\left(\dfrac{1}{4}-\dfrac{t^2}{4}\right)x'(t)\right)dt,$$ so $$|f(x)|\le\int_{-1}^1\left(\dfrac{|t|}{2}|x(t)|+\left|\dfrac{1}{4}-\dfrac{t^2}{4}\right||x'(t)|\right)dt\le\|x\|\cdot\int_{-1}^1\max\left\{\dfrac{|t|}{2},\left|\dfrac{1}{4}-\dfrac{t^2}{4}\right|\right\}dt=$$ $$=\frac{2\sqrt2-1}{3}\cdot\|x\|\approx0,609476\cdot\|x\|.$$ However, I can't find even numerical lower bound for $0,609476$ using $\dfrac{|f(x)|}{\|x\|}$ fractions. I have a feeling that received upper bound is overstated. I would be grateful for any comments and ideas, both on how to improve the upper bound, and how to get at least a numerical lower bound. Interestingly, the norm of the same functional under the condition that $\|x\|=\max\left\{\max\limits_{t\in[-1,1]}|x(t)|,\max\limits_{t\in[-1,1]}|x'(t)|\right\}$ is calculated trivially and is equal to $\dfrac{2}{3}$ . Thanks to Brifa's comment, we managed to improve the upper bound to $\|f\|\le0,4$ . For a while I thought that the norm is $\frac{1}{3}$ , but found the following example (see graph): The ""switch"" point is approximately $0.806$ , and $\dfrac{|f(x)|}{\|x\|}\approx0,35$ .","['functional-analysis', 'real-analysis']"
4592172,Closed form expression of $\int_{-\infty}^\infty \frac{e^{-x^2}}{1+ae^{bx}}dx$,"I am trying to find an expression for $$
I(a,b) = \int_{-\infty}^\infty \frac{e^{-x^2}}{1+ae^{bx}}dx
$$ where $a \ge 0, b$ real but so far without success. There are some easy special values like $I(0,b)=\sqrt{\pi}$ and $I(1,b)=\frac{\sqrt{\pi}}{2}$ . These following identities are not hard to prove by substituting $x\mapsto -x$ and using $\frac{1}{1+p}=1-\frac{1}{1+1/p}$ : \begin{align}
&(1) \hspace{1em} I(a,b) = I(a,-b) \\
&(2) \hspace{1em} I\left(a,b\right)+I\left(\frac{1}{a},-b\right)=\sqrt{\pi} \\
&(3) \hspace{1em} I(a,0)=\frac{\sqrt{\pi}}{1+a}
\end{align} $(1)$ and $(2)$ two imply $$
(4)\hspace{1em} I\left(a,b\right)+I\left(\frac{1}{a},b\right)=\sqrt{\pi}
$$ The asymptotic behaviour is $I(\infty,b)=0$ , $I(a,\infty)=\frac{\sqrt{\pi}}{2}$ . By experimenting I found that $$f(a,b) = \sqrt{\pi}\left(b^{2n} \log a + \frac{1}{1+a}\right)$$ is a solution to $(1)-(4)$ but obviously fails the limits.
I tried complex methods, series expansions, Feynman/Fubini tricks, all to no avail. My initial results made me confident enough to give it a go but now I have doubts there is even a solution. Context : The integral showed up in the calculation of the expected value of $\frac{A}{1+BX}$ where $X$ is log-normally distributed; or expected value of $\frac{A}{1+Be^X}$ for normally distributed $X$ . I am also interested in the case where the denominator is squared to find the variance. Additional : Graph of $I(a,b)$","['integration', 'expected-value', 'definite-integrals', 'probability']"
4592177,Need advice: what should be my next step for solving the derivative of $f(z)$ using the definition?,"What should be my next step for solving the derivative of $f(z)$ using the definition? $$f(z) = (2{z^2} + 1) \cdot ({z^3} - \sqrt {z}) $$ $$f'(z) = \mathop {\lim }\limits_{\vartriangle z \to 0} \frac{{f(z + \vartriangle z) - f(z)}}{{\vartriangle z}} \\= \mathop {\lim }\limits_{\vartriangle z \to 0} \frac{{[2{{(z + \vartriangle z)}^2} + 1] \cdot [{{(z + \vartriangle z)}^3} - \sqrt {(z + \vartriangle z)} ] - (2{z^2} + 1) \cdot ({z^3} - \sqrt {z}) }}{{\vartriangle z}}$$ At this point I don't know how to proceed: when I try to expand it,  I can't isolate the ∆z to simplify and avoid the division by zero. What am I missing here?","['limits', 'calculus', 'derivatives']"
4592306,Infinite products $f(x) = \prod_{n=1}^{\infty}(1-x^n)$ and $g(x) = \prod_{n=1}^{\infty}(1+x^n)$,"Consider the functions $
f(x) = \prod_{n=1}^{\infty}(1-x^n)
$ and $
g(x) = \prod_{n=1}^{\infty}(1+x^n)
$ $f(x)$ is defined for $x\in[-1,1]$ and $g(x)$ is defined for $x\in[-1,0]$ . I was wondering if there was a different/better way to express these functions, such as a closed-form expression/power series. Desmos screenshot ( $f(x)$ in red and $g(x)$ in blue) Edit: As pointed out by donaastor the Pentagonal Number Theorem can be used to express $f(x)$ as a power series. Edit 2: The Euler function $\phi(x)$ seems similar to $f(x)$ but with a different domain. For $\phi(x)$ the domain is $x\in(-\infty,-1)\cup(1,\infty)$ .","['special-functions', 'real-analysis', 'functions', 'polynomials', 'infinite-product']"
4592308,Write an equation for a sphere passing through a circle and tangent to a plane,"I'm trying to solve this task: '''Write an equation for a sphere passing through a circle $x^2 + y^2 = 11$ and tangent to a plane $x + y + z - 5 = 0$ .''' Center of the sphere should moves only in axis Z, so it has coordinates $(0, 0, \alpha)$ . I also found that $\alpha^2 = R^2 - 11$ , where R - radius of a sphere. I came to this system: $x_0^2 + y_0^2 + (z_0 - \sqrt(R^2 - 11))^2 = R^2$ $x_0 + y_0 + z_0 = 5$ where $x_0, y_0, z_0$ are coordinates of a touch point of a sphere with the plane. The correct answer is two spheres: $x^2 + y^2 + (z + 1)^2 = 12$ $x^2 + y^2 + (z + 4)^2 = 27$ I can't figure out how to find these two values of the parameter $\alpha$ . Could somebody please explain how to do it? Thanks in advance.","['spheres', 'geometry']"
4592316,What is the value of $f(1999)$?,"Let be a function $f \colon \Bbb R\to \Bbb R$ given from: $$f(x)=x^3+\sqrt{x^6+1}+\frac{1}{x^3-\sqrt{x^6+1}}$$ What is the value of $f(1999)$ ? To me, it immediately seemed strange that it was necessary to calculate $f(1999)$ . With a calculator everything would come easy. Since this question concerns high school students I have done the rationalization of $\frac{1}{x^3-\sqrt{x^6+1}}$ and I have found something very interesting. In fact $$\frac{1}{x^3-\sqrt{x^6+1}}=\frac{1}{x^3-\sqrt{x^6+1}}\cdot\frac{x^3+\sqrt{x^6+1}}{x^3+\sqrt{x^6+1}}=\frac{x^3+\sqrt{x^6+1}}{-1}$$ because $(x^3-\sqrt{x^6+1})(x^3+\sqrt{x^6+1})=-1$ Thus $$f(x)=x^3+\sqrt{x^6+1}-x^3-\sqrt{x^6+1}=0$$ and $f(1999)=0$ because $f(x)=0, \forall x\in\Bbb R$ . Is it correct?",['algebra-precalculus']
4592367,Help solving differential equation in closed form - Damped Harmonic Oscillator,"I am attempting to solve analytically, a differential equation of the form $$ -\alpha \frac{d^2y}{dx^2} + \beta y \frac{dy}{dx} + \gamma x^2 y = (\epsilon )y$$ Where, $\alpha $ , $ \beta$ , $\gamma$ and $ \epsilon$ are constants. The inclusion of the second term has thrown a spanner in the works. I attempted using Frobenius method but am unable to formulate a recursion relation. I have the solution of the equation with the second term excluded, and it yields a solution that depends on a Hermite polynomial. I am hoping the solution will incorporate that too. Let me know if you have any resources you can",['ordinary-differential-equations']
4592425,Double Integral Question: How are the limits set up?,"I am trying to find the area of the line y = x and below by the parabola $y = x^2-2x. $ I am using a double integral. However, for the integral, this is the correct answer: $$ \int_0^3\int_{x^2-2x}^xxdydx $$ I don't understand how the dy limits of integration are set up? Isn't it right function-left function? The graph on the right is the $x^2-2x$ . But in the limits of integration, this is reversed? In the picture the blue graph is $x^2-2x$ and the red is $y=x$","['multivariable-calculus', 'multiple-integral']"
4592445,Proofs question,"I have a question about set theory that and proofs that I was hoping you could help with. The goal is the prove or disprove that: $A\setminus (A \cap B) = A\setminus B$ So far I have: $A\setminus (A \cap B)$ is equivalent to $A \cap (A \cap B)^\mathsf{c}$ From De Morgan's law, we can derive that this is equivalent to: $A \cap A^\mathsf{c} \cup B^\mathsf{c}$ Or, if my logic is correct: $\varnothing \cup B^\mathsf{c}$ This is where I am getting stuck, as I'm not sure where to go from here. Thank you very much.",['discrete-mathematics']
4592584,Analytic continuation of the sum of the reciprocals of the $n$-bonacci sequences.,"In a previous question , I asked for an approximation of the sum of the reciprocals of the Tribonacci numbers. So I was wondering if there is a function for the sum of the reciprocals of the $n$ -bonacci numbers. This would look like this: $$\varkappa(n)=\sum_{k=1}^\infty\frac{1}{N_{n_k}}$$ Where $N_n$ gives the $n$ -bonacci sequence ( $n=1$ gives $1, 1, 1, ...$ , $n=2$ gives the fibonacci sequence, $n=3$ gives the tribonacci sequence, and so on). I graphed the first four points in this desmos graph . So what function could connect these points? And additionally, does $\varkappa$ have a lower bound? What is it? (I suspect it is $1.5$ or something near but I couldn't prove this.","['fibonacci-numbers', 'analytic-continuation', 'graphing-functions', 'functions', 'sequences-and-series']"
4592733,"Solution verification: Partials of $f(x,y) = y/x, x \neq 0$ at $(0,0)$","This is a question from Chapter 2 of Vector Calculus Study Guide Solutions Manual (Karen Pao, Frederick Soon) . Let $$f(x,y) =\begin{cases} y/x, & x\neq 0 \\ 0 & x = 0.  \end{cases}$$ at $(0,0)$ Compute $f_x(0,0)$ and $f_y(0,0)$ if they exist. I used the limit definition and I obtained: $$f_x(0,0) = \lim_{h \to 0} \dfrac{f(h,0) - f(0,0)}{h} = \lim_{h \to 0} \dfrac{0/h - 0}{h}= 0 $$ and $$f_y(0,0) = \lim_{h \to 0} \dfrac{f(0,h) - f(0,0)}{h} = \lim_{h \to 0} \dfrac{0-0}{h} = 0 .$$ However the given answer is that $f_x$ does not exist and $f_y = 0$ . Can someone please let me know how my calculation for $f_x(0,0)$ is wrong? Thank you!","['partial-derivative', 'limits', 'solution-verification', 'derivatives']"
4592739,Solve: $t\ddot{x}+\dot{x}=0$,"To solve the above mentioned equation I used a substitution $u=\dot{x}$ . Thus I now have $t\dot{u}+u=0 \rightarrow \dot{u} = -\frac{1}{t}u \rightarrow \int\frac{1}{u}du = -\int\frac{1}{t}dt \rightarrow ln(u) = -ln(t) + C$ . This is where I got stuck, can someone point out if something went wrong or help me continue.",['ordinary-differential-equations']
4592754,"If an initial value problem has a solution on $[0,a)$ for all $a>0$, will it have a solution on the whole $[0,\infty)$?","Consider the initial value problem on $[0,\infty)$ : $$x'(t)=f(t, x(t)) \qquad x(0)=0,\label{1}\tag{$*$}$$ where $f:(0,\infty)\times\mathbb R\to\mathbb R$ is a continuous function. Assume that for every $a>0$ , there exists a $C^1$ solution $x(t)$ to the initial value problem \eqref{1} on $[0,a)$ with $x\in C([0,a))\cap C^1((0,a))$ (the solution is not necessarily unique). Can we conclude that there is a solution to the initial value problem \eqref{1} on the whole $[0,\infty)$ ? This problem comes from one of my friends. Attempt. I think the answer is “no” . To find a counter-example, I tried to construct an $f$ such that the initial value problem $(*)$ has a solution $x(t)$ on $[0,T)$ with the property $$x(t)\sim \frac1{T-t}, \qquad t\sim T-$$ for all $T>0$ ; and such that $(*)$ doesn't have global solutions on the whole $[0,\infty)$ . However, the function $\frac1{T-t}$ doesn't take the value $0$ at $t=0$ , so I considered instead $$x(t)=\frac{\eta(t)}{T-t}, \qquad t\in[0,T)\tag{1}$$ for some good function $\eta(t)$ with $\eta(0)=0$ . Now the function $(1)$ satisfies $$x'(t)=\frac{\eta(t)}{(T-t)^2}+\frac{\eta'(t)}{T-t},\qquad t\in[0,T),$$ which impiles that $x$ is a solution to the following initial value problem on $[0, T)$ : $$x'(t)=\frac{1}{\eta(t)}(x(t))^2+\frac{\eta'(t)}{\eta(t)}x(t),\qquad x(0)=0. \tag{2}$$ However, the initial value problem $(2)$ has a trivial global solution $x\equiv0$ and many non-trivial global solutions $x(t)=\frac{\eta(t)}{C-t}$ on $t\in[0,\infty)$ for $C<0$ . Therefore, this method doesn't work . Any help would be appreciated! Note . This problem has been cross-posted in MO and has been answered by Saúl RM . Nevertheless, more examples or comments are welcome!","['initial-value-problems', 'ordinary-differential-equations', 'real-analysis']"
4592795,Surface Flux Integral Across a Circular Surface,"I need to calculate $$
\iint_S \textbf{J}\cdot\hat{\textbf{n}}\; dS
$$ Where S is the circular surface centered at the origin, area $A$ , in the $yz$ -plane, with unit normal having a negative $x$ -component. $$\textbf{J} = -\sigma\nabla\phi(\textbf{r}), \;\; \phi(\textbf{r}) = V(1-x/L)$$ What I'm struggling with here is getting a parametric representation of this surface for this integral in terms of u and v. I can very easily parametrise the curve of the boundary of this surface but obviously for this integral I need a representation of the surface.
I tried: $$
S:\; (u,v) \rightarrow \; \left(v, \sqrt\frac{A}{\pi}\cos(u), \sqrt\frac{A}{\pi}\sin(u)\right) \;\;
0\leq u\leq 2\pi, \; v = 0
$$ But then calculating the normal, $\textbf{v}_{u} \times \textbf{v}_{u}$ , I get a normal to the cylinder with this circle as it's end. I know I need to get $(-1, 0, 0)$ as my unit normal vector. Any help would be much appreciated!","['multivariable-calculus', 'surface-integrals', 'surfaces']"
4592812,About problem A4 2022 of Putnam,"I'm not passing the William Lowell Putnam competition (I live in France) but I'm still fascinated by some of the problems. This year the A4 problem challenged me and I wanna know your thoughts about it : Suppose $X_1, X_2, \dots$ real numbers between $0$ and $1$ that are chosen independently and uniformly at random. Let $S = \displaystyle\sum\limits_{i=1}^k \frac{X_i}{2^i}$ where $k$ is the least positive integer such that $X_k < X_{k+1}$ or $k=\infty$ if there is no such integer. Find the expected value of $S$ . 1 - Is this a classic problem that Putnam competitors know ? 2 - What's the idea to solve this ? Instictively I would say that : $$\mathbb{E}[S] = \mathbb{E}\left[ \displaystyle\sum\limits_{i=1}^k \frac{X_i}{2^i} \right] = \displaystyle\sum\limits_{i=1}^k \frac{\mathbb{E}[X_i]}{2^i} = \displaystyle\frac{\mathbb{E}[X_1]}{2} + \displaystyle\frac{\mathbb{E}[X_2]}{4} + \dots + \displaystyle\frac{\mathbb{E}[X_k]}{2^k}$$ And then calculate $\mathbb{E}[X_k]$ for all $k \in \mathbb{N}^*$ but maybe that's not the right idea and the solution is far more exotic.","['contest-math', 'probability']"
4592839,"How can one determine $e^{tA}$, where $A =\begin{bmatrix} 1 & 0 \\ 1 & 2 \end{bmatrix}$? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Let $A =\begin{bmatrix}
1 & 0 \\
1 & 2 
\end{bmatrix}$ be given. How can one compute the matrix function $e^{tA}$ ? Do you have to decompose $A$ , if so how?","['matrices', 'matrix-exponential', 'matrix-decomposition']"
4592922,Prove the existence of a fixed point for the sum of two mappings,"Let $X$ be a real valued Banach-space and $A \subseteq X$ which is bounded, closed and convex. Furthermore let $f,g,h : A \mapsto X$ be continuous with $f=g+h$ and $g(A)+h(A) \subseteq A$ . Lastly $g$ ist a contraction mapping and $h$ compact map. I'm supposed to show that $f$ has a fixed point $x^* \in A$ . Sadly I am getting nowhere with this. I know by Banach's theorem, that $g$ has a unique fixed point, and by Schauder's theorem, that $h$ has a non-unique fixed point. Obviously these fixed point need not coincide; even if they did it would do me no good. Furthermore I've been told as a hint to look at the map $id_A - g: A \mapsto X$ , where $id_A$ refers to the Identity. I'd appreciate a nudge in the right direction, since I'm getting nowhere with this.","['banach-spaces', 'functional-analysis', 'fixed-point-theorems']"
4592942,Line segments in the plane where each triple can be intersected by a common line,"I have the following problem at hand: We have a collection $\mathcal{C} = \{l_1,\dots, l_n\}$ of $n$ line segments in the plane where each segment is contained in a line through the origin. Assuming that each triple in $\mathcal{C}$ can be intersected by a single line, prove that all segments in $\mathcal{C}$ can be intersected by a single line. My attempt:
First we notice that if two segments are on the same line then all segments are on the same line otherwise we can construct a triple that would contradict the assumption so we WLOG assume that all segments are on different lines. Now we proceed by induction on $n$ . For $n=3$ everything is clear so we continue. Assume that it holds for $3,4,\dots, n-1$ . Then for every segment in $\mathcal{C}$ there is a line $p_i$ such that $p_i$ intersects all the segments except $l_i$ . Now we should have that $D_0(p_i)$ is an element of intersection of the dual sets of all segments except $l_i$ . Or in other words all the dual sets have a nonempty intersection. In the same time I know that dual of $l_i$ has a nonempty intersection with dual of any other two $l_j$ but I am not sure how to finish the proof. Alternate idea:
Let $X_i$ be the set of lines intersecting $l_i$ . Then by our assumption each three $X_i$ intersect we can apply Helly theorem. My problem with both ideas is that they do not use the fact that segments are on lines through the origin and this is definetly not true for line segments in general case. Notation: $D_0(l)$ is a point $a$ such that $\langle x,a \rangle = 1$ for every $x\in l$ .","['geometry', 'solution-verification', 'combinatorial-geometry', 'combinatorics', 'discrete-mathematics']"
4592945,Convergence in two $L^p$ spaces [duplicate],"This question already has an answer here : Intersection of Two $L^p$ Spaces (1 answer) Closed 1 year ago . How do you argue that for a sequence of functions $\{f_n\}$ , convergence in $L^{p_1}$ and $L^{p_2}$ implies convergence in $L^{p}$ for all $p \in [p_1,p_2]$ ? I know that we can pass to a $\mu$ -almost everywhere convergent subsequence $\{n_k\}$ so that $f_{n_k} \to f$ , and I know that by Fatou's lemma we have that $$\lVert f \rVert_p^p = \int |f^p|  \leq \liminf_{k \to \infty} \int |f_{n_k}^p|  \leq \liminf_{k \to \infty} \int |f_{n_k}^{p_1}|+ |f_{n_k}^{p_2}|,$$ where the latter term is finite, but I don't know how to conclude the argument. Is it the more gneeral version of the Dominated Convergence Theorem? I would greatly appreciate any help. Edit: I don't see how the linked post answers my question, and I would appreciate some more detail as to how this argument can be applied and if the approach I suggested is dead wrong or not.","['measure-theory', 'lp-spaces', 'real-analysis']"
4592965,"Maximum likelihoof estimator of $\mu$ if $X_1\sim N(\mu,1)$","Question: Let $X_1, . . . , X_n$ be a random sample with $X_1 \sim N(\mu, 1)$ where $\mu \geq 0$ . Derive the
maximum likelihood estimator of $\mu$ . I have some difficulties understanding the question. I already calculated the MLE of $\mu$ when $X_1,...,X_n$ are a random sample from $N(\mu,\sigma^2)$ . In that case the MLE of $\mu$ is $\hat{\mu} = \bar{X}$ . Is it the same case here? Or does the $\mu\geq 0$ changes anything? Or maybe the question states that $X_2,...,X_n\sim N(\mu, \sigma^2)$ ? I'm not that familiar with statistic jargon.","['statistics', 'maximum-likelihood']"
4592984,Krull dimension vs. Dimension in Zariski topology for a field that is not algebraically closed,"I'm new to algebraic geometry and struggle with the notion of the dimension of an affine variety. In Robin Hartshorne's book Algebraic Geometry the dimension is introduced using the Zariski topology: Definition: Let $X$ be a topological space, we define its dimension as the supremum over all integers $n$ s.t. there exists an ascending chain of closed irreducible components (w.r.t. the topology) $Z_0 \subset Z_1 \subset \dots \subset Z_n\subseteq X$ . The dimension of an affine variety is then defined to be the dimension as its topological space. Then he shows that for an affine variety $V \subseteq K^n$ the Krull dimension of the coordinate ring $K[x]/\mathcal{I}(V)$ (defined as the supremum of heights of prime ideals) equals the dimension of $V$ as defined above. Question 1: My understanding is that Hartshorne assumes the field $K$ to be algebraically closed throughout the book and I'm wondering whether this particular result changes when using e.g. $K=\mathbb{R}$ . My understanding from quickly going through section 3.3 in https://perso.univ-rennes1.fr/michel.coste/polyens/SAG.pdf is that this should indeed hold. Can anyone help me out or ideally refer to some literature about this? Question 2: I stumbled upon the supposed counter-example $\mathcal{V}(\sum_{i=1}^n x_i^2)\subseteq K^n$ , which I don't quite understand. My reasoning is as follows: Let $f=\sum_{i=1}^n x_i^2\subseteq \mathbb{R}[x_1,\dots,x_n]$ , then $V:=\mathcal{V}(f)\subseteq \mathbb{R}^n=\{(0,\dots,0)\}$ should have dimension 0 in the Zariski topology. The coordinate ring is given by $$ \mathbb{R}[x_1,\dots,x_n]/\mathcal{I}(V) = \{[f]:f\in \mathbb{R}[x_1,\dots,x_n]\} $$ with $g\in[f] \iff f-g\in \mathcal{I}(V)$ . Here we have that $h\in\mathcal{I}(V) \iff h(0,\dots,0)=0$ , which happens if the constant part of $h$ denoted by $a_0^h$ is zero (because $h(0,\dots,0)=a_0^h$ ). Thus, two polynomials are in the same equivalence class if they have the same constant part, i.e. $g\in[f] \iff a_0^g = a_0^f$ . Since $\forall f\in \mathbb{R}[x_1,\dots,x_n] : a^f_0\in\mathbb{R}$ we have $\mathbb{R}[x_1,\dots,x_n]/\mathcal{I}(V) =\mathbb{R}$ . If I'm not mistaken, the Krull dimension of $\mathbb{R}$ as a ring should also be 0. Is there anything that I have missed?","['affine-varieties', 'krull-dimension', 'zariski-topology', 'algebraic-geometry', 'commutative-algebra']"
4592985,"Evaluate $\int_0^1\arcsin^2(\frac{\sqrt{-x}}{2}) (\log^3 x) (\frac{8}{1+x}+\frac{1}{x}) \, dx$","Here is an interesting integral, which is equivalent to the title $$\tag{1}\int_0^1 \log ^2\left(\sqrt{\frac{x}{4}+1}-\sqrt{\frac{x}{4}}\right) (\log ^3x) \left(\frac{8}{1+x}+\frac{1}{x}\right) \, dx = \frac{5 \pi ^6}{1134}-\frac{22 \zeta (3)^2}{5}$$ Question: how to prove $(1)$ ? Using power expansion of $\log^2(\sqrt{x+1}-\sqrt{x})$ , we can derive an equivalent form of $(1)$ : $$\tag{2}\sum _{n=1}^{\infty } \frac{1}{n^2 \binom{2 n}{n}} \left(8 \sum _{j=1}^n \frac{(-1)^j}{j^4}+\frac{(-1)^n}{n^4}\right)=-\frac{22 \zeta (3)^2}{15}-\frac{97 \pi ^6}{34020}$$ Letting $\sqrt{\frac{x}{4}+1}-\sqrt{\frac{x}{4}}=\sqrt{u+1}$ gives $$\tag{3}\int_0^{\phi} \log^2 (1+u) \log^3\left(\frac{u^2}{1+u}\right)  \frac{(u+2)(9 u^2+u+1)}{u (u+1) (u^2+u+1)} du = \frac{10 \pi ^6}{567}-\frac{88 \zeta (3)^2}{5}$$ with $\phi = (\sqrt{5}+1)/2$ . But all these variations look equally difficult. Any idea is welcomed.","['integration', 'polylogarithm', 'definite-integrals', 'sequences-and-series']"
4592991,"Let $\{\phi_n\}$ be an orthonormal set in $L^2(\mu)$, $g\in L^2(\mu)$ such that $|\phi_n(x)|\le |g(x)|$ and $\sum a_n\phi_n(x)$ converges a.e.","Let $\mu$ be a  positive measure on measurable space $(X,\mathfrak{M})$ . Let $\{\phi_n\}$ be an orthonormal set in $L^2(\mu)$ and $g\in L^2(\mu)$ such that $$|\phi_n(x)|\le |g(x)|\text{ for a.e. }x\ \forall n$$ and $\sum\limits_{n=1}^\infty a_n\phi_n(x)$ converges a.e., then prove that $\lim a_n=0$ I define $f(x)=\sum\limits_{n=1}^\infty a_n\phi_n(x)$ . I want to prove that $f\in L^2(\mu)$ . If I canprove so, as $\{\phi_n\}$ is orthonormal set, $\lVert f\rVert_2=\sum|a_n|^2<\infty$ , therefore $\lim a_n=0$ . By Minkowski integral inequality, we have- $\lVert f\rVert_2$ $=\left[\int|\sum a_n\phi_n(x)|^2\ dx\right]^{1/2}$ $\le \sum\left[\int|a_n\phi_n(x)|^2 dx\right]^{1/2}$ $\le \sum|a_n|\left[\int|g(x)|^2\ dx\right]^{1/2}$ $=\sum |a_n|\lVert g\rVert_2$ But this says nothing. Can anyone suggest me a wayout to finish the proof? Thanks for your help in advance.","['measure-theory', 'analysis', 'hilbert-spaces', 'lp-spaces', 'functional-analysis']"
4593042,Confusion on the definition of the inverse function,"I am currently trying to prove the equivalence of two definitions of enumerability (one involving a surjective function, the other involving an injective one). In doing so, I wanted to consider the inverse of a function, but then it occurred to me that I can't just assume that any function has an inverse. I looked in my lecture notes and got the following definition Let $f: a \rightarrow b$ be $1$ - $1$ . Then the inverse function of $f$ is $f^{-1}: Ran(f) \rightarrow \text{Dom}(f)$ such that: $$\forall \ \ x \in \text{Ran}(f) \ \ \forall \ \ y \in \text{Dom}(f) (f^{-1} = y \leftrightarrow f(y) = x)$$ Where a function $1$ - $1$ function is a injective function. i.e.: $f: a \rightarrow b$ is injective ( $1$ - $1$ ) iff $$\forall \ \ x,y \in a (f(x) = f(y) \rightarrow x = y)$$ But everywhere else I am reading that only a bijective function has an inverse . Now, unless I am mistaken, being an injective function does not imply being a surjective function, so injective functions are not necessarily bijective. But in this case there is a palpable disagreement between these two definitions, and that affects my proof. So, I wonder: have my notes got it wrong, or have I made a mistake? Thank you for any help.","['elementary-set-theory', 'definition', 'functions']"
4593052,Show that $\sqrt{n}(\overline{X}_n^2 - \mu^2) \to 2\mu|\sigma Z$ in distribution,"(Full disclosure--this is a homework problem.) Let $(X_n)_{n=1}^{\infty}$ be a sequence of i.i.d. random variables with mean $\mu$ and variance $\sigma^2 < \infty$ . I want to show that if $\mu \neq 0$ then \begin{align*}
    \sqrt{n}(\overline{X}_n^2 - \mu^2) \to 2 \mu \sigma Z \quad \text{in distribution as } n \to \infty,
\end{align*} where $Z$ is a standard normal random variable. My attempt . We have \begin{align*}
    \frac{\sqrt{n}}{\sigma} (\overline{X}_n^2 - \mu^2) = \frac{\overline{X}_n^2 - \mu^2}{\sigma/\sqrt{n}} = \left(\frac{\overline{X}_n - \mu}{\sigma/\sqrt{n}} \right)\left(\overline{X}_n + \mu \right).
\end{align*} Now $\frac{\overline{X}_n - \mu}{\sigma/\sqrt{n}} \to Z$ in distribution as $n \to \infty$ by the Central Limit Theorem. And $\overline{X}_n \to \mu$ almost surely by the Strong Law of Large Numbers. Therefore, $\overline{X}_n \to 2\mu$ almost surely, and hence $\overline{X}_n \to 2\mu$ in probability. Then by Slutsky's Theorem, \begin{align*}
   \left(\frac{\overline{X}_n - \mu}{\sigma/\sqrt{n}} \right)\left(\overline{X}_n + \mu \right) \to 2\mu Z \quad \text{in distribution as } n \to \infty,
\end{align*} and therefore, \begin{align*}
    \sqrt{n}(\overline{X}_n^2 - \mu^2) \to 2\mu \sigma Z \quad \text{in distribution as } n \to \infty.
\end{align*} How do we get the absolute value sign? I'm not seeing my mistake...","['statistics', 'probability']"
4593116,Determine relative position of 3 large (equal) circles and 1 smaller circle within a minimum enclosing circle,"I want to visualize the position of $3$ large equal circles with radius $r_1$ and $1$ smaller circle with radius $r_2$ . The circles represent three wire conductor phases and one smaller neutral conductor. The enclosing circle represents the minimum wire bundle diameter.
I have tried solving the problem using trigonometric functions and algebra but so far unfortunately with no success. I would like to be able to determine the position of all $4$ circles from the enclosing circle center. This means that I am interested in dimensions: W, X, Y and Z . I am expecting that all dimensions can be written as a function of $r_1$ and $r_2$ . In the picture below I am interested mainly in cases 1 and 2, case 3 is optional.
Thanks in advance!","['euclidean-geometry', 'trigonometry', 'circles', 'packing-problem']"
4593205,Probability of partially sortedness of a perfectly shuffled array of unique numbers,"Sorry that I don't know any better way to express this question. Assuming that we have a perfectly shuffled array of unique numbers, we know that the array is in fact consisted of ascending or descending sorted subarrays. In average howmany such sorted subarrays consist this one particular perfectly shuffled array is the question. As a simple example, please consider an array of 20 integers [0...19] shuffled as [14,2,5,16,18,19,7,6,12,17,3,4,8,11,15,13,1,0,9,10] which is internally sorted in chunks as [[14,2],[5,6,18,19],[7,6],[12,17],[3,4,8,11,15],[13,1,0],[9,10]] so in total we obtain 7 subarrays. Now this observation just paves the way for a very efficient sorting algorithm once you merge them cleverly. Yet it's even better than that. I naively assumed that the number of subarrays in average should be like half the length of the initial array (50% sorted internally). But no, that's only the worst case which you very rarely get. It's like hitting 10101010 or 01010101 in 8 bits. So for 8 bits per se the chance of hitting worst case is $2\times\frac{1}{2^8} = \frac{1}{128}$ . Similarly hitting the best case where we end up with only one subarray (given the shuffled array is already sorted ascending or descending) should have the same chance (or is it?). As a result, in average we get better than 50% internal sorting but howmuch better? In my tests I have observed that in average a perfectly shuffled array of unique items turns out to be ~58.68% sorted internally. In other words a 1000 item array would yield in average ~410 sorted subarrays to merge. While my algorithm seems just fine, perhaps it misses some edge cases. I would appreciate helps to know if this 58.68% figure is inline with the math of it. Edit: To gain further insight, I have decided to add some more observations when repeatition is allowed . 2 choices in n length array i.e. (0,1) yields 75% (25 subarrays from 100 items array) 3 choices in n length array (0,1,2) yields 67.28% 4 choices in n length array yields 64.385% 13 choices yields %60.06 n choices in n length array yields 58.68% (same as no repetition)",['probability']
4593251,Help Understanding Explicit Automorphism Formulas for Semi-Direct Products,"I'm currently working through some material on semi-direct products, and I'm totally confused by some of the explicit examples given for semi-direct products, namely how one determines the automorphisms. For example, In classifying groups of order $30$ , done here specifically , the answer makes sense in that there are four possible automorphisms given by $\varphi(1) \in \{(0,0), (1,0), (0,2), (1,2)\}$ , but what are these automorphisms? For $\varphi(1) = (0,0)$ , I get this is a trivial map corresponding to simply the direct product. What about $\varphi(1) = (1,0)$ , what's an explicit formula for this automorphism?  Semi-direct products of this form arise from the action of conjugation of the right factor $K$ on the left factor $H$ (for a semi direct product $H \rtimes_{\varphi} K$ ).  In this case, we have a Sylow $2$ subgroup $P_2 \cong \mathbb{Z}_2$ , and a group $N \cong \mathbb{Z}_{15}$ of order $15$ . The theory states that if $\varphi: K \longrightarrow \text{Aut}(H)$ , then $\varphi(k)[(h)] = khk^{-1}$ .  So if $\varphi(1) = (1,0)$ , then wouldn't we simply have for $n \in \mathbb{Z}_{15}$ : $$\varphi(1)(n) = 1+n-1 = n?$$ This is the trivial automorphism however, which is in direct contradiction to the fact that $\varphi(1) = (1,0)$ isn't trivial. Essentially what I'm asking for is an explanation on how one deduces that (in the context of the linked post): ""if $\varphi(1)=(1,0)$ , then $P_2 \rtimes_{\varphi} N \cong D_{15}$ because _____, and if $\varphi(1)=(0,2)$ , then $P_2 \rtimes_{\varphi} N \cong \mathbb{Z}_5 \times S_3$ because ___..."" Could someone explain what's going on here?","['group-homomorphism', 'semidirect-product', 'group-theory', 'abstract-algebra']"
4593335,"If $c > 1$, show that $\sqrt[n]{c}$ approaches $1$ for large $n$.","I know there's a duplicate about this question, but I came up with this solution on my own so I need a verification. Using Bernoulli's inequality, we obtain $\forall \varepsilon > 0 $ , $\exists N > 0$ such that if $n > N$ , then $1 < 1 + n\varepsilon \le (1 + \varepsilon)^n$ .
However, as $n \to \infty$ , $1 + n \varepsilon \to \infty$ .
By squeeze theorem, we have $(1 + \varepsilon)^n \to \infty$ as well. Here comes the part I am not sure of. For large enough $n$ , we always could have a fixed $c$ , such that $(1 + \varepsilon)^n > c$ $\forall \varepsilon > 0$ . This yields $c^{\frac{1}{n}} - 1 < \varepsilon$ . Since $c > 1$ , we have $c^{\frac{1}{n}} > 1$ , so $\forall \varepsilon > 0$ , $c^{\frac{1}{n}} - 1 < \varepsilon$ gives the desired result. Is this proof lacking in any manners? Can we assume the bold sentence?
Any help is greatly appreciated.","['epsilon-delta', 'solution-verification', 'analysis', 'real-analysis']"
4593337,Regular polygon with diagonals: bounds on area of largest cell?,"Consider a regular $n$ -gon of side length $1$ with its diagonals. Here is an example with $n=11$ (from geogebra applet ). Excluding the centre cell when $n$ is odd, I've been trying to find bounds on the area of the largest cell, in terms of $n$ . It seems that, for every value of $n$ , the largest non-centre cell has roughly similar area as the outermost cell, whose area is $\frac{1}{4}\tan{\frac{\pi}{n}}\approx\frac{\pi}{4n}$ . That is, $(\text{area of largest non-centre cell})\approx\frac{\pi}{4n}$ . (This claim has strong numerical evidence .) My question is: In a regular $n$ -gon of side length $1$ with diagonals, what is the infimum and supremum of $n\times(\text{area of largest non-centre cell})$ ? (The number of cells is approximately $\frac{1}{24}n^4$ for large $n$ .)","['supremum-and-infimum', 'area', 'geometry', 'polygons']"
4593389,The set $\{f(X) \in \mathbb{R}[X] \hspace{1mm}|\hspace{3mm} f(n)\in \mathbb{Z} \hspace{3mm} \forall n \in \mathbb{Z}\}$ is countable.,"The set $\{f(X) \in \mathbb{R}[X] \hspace{1mm}|\hspace{3mm} f(n)\in \mathbb{Z}    \hspace{3mm} \forall n \in \mathbb{Z}\}$ is countable. Clearly, $\mathbb{Z}[X]$ is contained in the set, from this previously asked question(( {f(x) belongs to R[x] | f(n) belongs to Z for all n belongs to Z}uncountable or countable ? (TIFR GS 2022) ) we have that all $\mathbb{Z}$ linear combinations of $$\binom{X}{m} = \frac{X(X-1)\cdots(X-m+1)}{m!}$$ I am able to prove that if $f(X)\in\mathbb{Q}[X]$ then $f(X)$ will be of the above form. Now I am unable to prove what happens if at least one coefficient is irrational.Since sum of the coefficients is in $\mathbb{Z}$ we have at least two coefficients must be irrational. suppose, $f(X)=a_nX^n+a_{n-1}X^{n-1}+\cdot\cdot\cdot+a_0$ then since $f(\mathbb{Z}) \subset \mathbb{Z}$ , we have $a_0 \in\mathbb{Z}$ and $a_n+a_{n-1}+\cdot\cdot\cdot+a_0\in \mathbb{Z}$ $\implies$ $a_n+a_{n-1}+\cdot\cdot\cdot+a_1 \in \mathbb{Z}$","['elementary-set-theory', 'field-theory', 'polynomials', 'real-analysis']"
4593398,"$n$th derivative is non-negative for $n=0,1,2,...$, then Taylor series converge, and is exactly $f(x)$ everywhere","Try to prove, if $$f^{(n)}(x)\geq 0,\;\forall x\in \mathbb{R},\;\forall n=0,1,2,...$$ then the Taylor series at the point $0$ converges everywhere, and it's exactly $f(x)$ , i.e. $$f(x)=\sum_{n=0}^{+\infty}\frac{f^{(n)}(0)}{n!}x^n,\;\forall x\in \mathbb{R}.$$ I know it is sufficient, and necessary, to prove, $$
\lim\limits_{n\to+\infty}{R_n(f,x)}=0\iff\lim\limits_{n\to+\infty}{\frac{f^{(n+1)}(\xi)}{(n+1)!}x^n}=0$$ but I dont know any methods to estimate the value of the $n$ th derivative. Can you help me? Thanks! Update: Notice: The Taylor series converges everywhere (proved in another similar problem: Taylor series of Infinitely differentiable function with nonnegative derivatives ), but it's interesting that it is equal to $f(x)$ . It's possible to prove the formula where $x<0$ : consider $$\lim\limits_{n\to+\infty}{\left|\frac{f^{(n+1)}(\xi)}{(n+1)!}x^{n+1}\right|}\leq\lim\limits_{n\to+\infty}{\left|\frac{f^{(n+1)}(0)}{(n+1)!}\right||x|^{n+1}}=0.$$ ...And I have an idea ! For $x_0>0:$ consider $$g(x)=\sum_{k=0}^{n}{\frac{f^{(k)}(x_0)(x-x_0)^k}{k!}-f(x)}$$ and $g(2x_0)$ .","['calculus', 'analysis', 'real-analysis']"
4593452,Convergence of moments,"Suppose $\left(X_{n},n\in \mathbb {N}\right)$ is a sequence of random variables taking values in $[0,1]$ . Suppose that for every $k\in \mathbb {N}$ , $$
\lim_{n\to \infty}\mathbb {E}\left(X^{k}_{n}\right)=\int x^{k}\, d\mu,
$$ for some probability measure $\mu$ . Then show that $(X_{n})$ converges to $\mu$ weakly. I think this should be straightforward by the following theorem: where $\Rightarrow$ means weak convergence or convergence in distribution. I wonder what if I have the following instead: $$
\lim_{n\to \infty}\mathbb {E}\left(X^{k}_{n}\right)=\frac {a}{k+a},
$$ for some $a>0$ and every $k\in \mathbb {N}$ .","['weak-convergence', 'probability']"
4593481,"Kalman filter, intuitively","I am currently working my way through the Kalman filter equations. Be warned, I think do have a solid understanding of math, yet I am just an engineer. So first of all there is this excellent website that provides quite an easy to follow introduction to the matter. I get along with it really well but there is one caveat that really gives me headaches. Consider the multidimensional expression for the covariance extrapolation: $$ \mathbf P_{n+1,n} = \mathbf F \mathbf P_{n,n} \mathbf F^\top + \mathbf Q $$ Note the scheme $ \mathbf A \mathbf B \mathbf A^\top $ that occurs in other occasions again. How could one describe this intuitively ? For comparison, in $ \mathbf A \vec x = \vec b $ my mind intuitively grasps $\mathbf A$ as a recipe to make the components for $\vec b$ from the components of $\vec a$ on the lowest level. A little elevated from that I see $\mathbf A$ as a function/projection/... But I just cannot come up with something mnemonic for the above scheme, and I'd be thankful if someone could shine a light.","['matrices', 'linear-algebra', 'kalman-filter']"
4593483,Covariance of $X$ and $Y$ is 0,"Let $X$ be a random variable. Is it possible to construct a random variable $Y=g(X)$ , with $g$ strictly monotone, such that $\text{cov}(X,Y)=0$ ?","['correlation', 'statistics', 'covariance', 'probability']"
4593499,Degree of the twisted cubic curve in $\mathbb{P}^3$,"I want to compute the degree of the twisted cubic curve $$X = \{xw = yz, xz=y^2, yw = z^2\} \subset \mathbb{C}\mathbb{P}^3 \ni [x:y:z:w].$$ I already know that $\deg(X) = 3$ , however, I want to verify this claim by computing the degree of the intersection divisor $\operatorname{div}(x)$ . To this end, let $L: x= 0$ . Then, $$
X \cap L = \{yz = 0, y^2=0, yw = z^2\}
$$ contains only the point $p = [0:0:0:1]$ , since $y^2 = 0$ implies $y = 0$ and hence, $z = 0$ . If it is true that there is a unique intersection point, then, by definition, the degree of $X$ is the order of the meromorphic function $\tfrac{x}{w}$ at $p \in X$ . Why is this equal to 3? The order is $2$ around $y^2 = 0$ and likewise, around $z^2 = 0$ . I am not sure how one comes up with 3.","['riemann-surfaces', 'algebraic-geometry']"
4593527,Where exactly does this proof of Schroder Bernstein use the law of the excluded middle?,"I have finally worked my way through the entirety of this proof of the Schröder Bernstein Theorem but have now learned something new about it while browsing Stack Exchange - that one has to use the law of the excluded middle to prove Schröder Bernstein. Where exactly does this proof use the law of excluded middle? I can't see it - my guess would be that the author is using it somewhere around step (1), but I'm not quite sure. Also, since I don't really have a strong background in logic, it would be fantastic if someone could explain to me why the law of excluded middle is necessary for Schröder Bernstein. Thanks!","['elementary-set-theory', 'axioms', 'logic', 'set-theory']"
4593585,Proof of Folkman's lemma.,"In combinatorics,in particular,in Ramsey theory,there is a theorem called Folkman's theorem which is as follows: Given any positive integers $r,k$ there exists $f(r,k)\in \mathbb Z^+$ such that if $\{1,2,...,f(r,k)\}$ is $r$ -colored then there exist $1\leq a_1<a_2<...<a_k\leq f(r,k)$ such that $P(a_1,a_2,...,a_k)=\{\sum\limits_{i} c_ia_i: c_i=0,1$ for all $1\leq i\leq k$ and not all $0\}$ is monochromatic. I am looking for a proof of this theorem.Can someone please provide me a proof or give me a reference where I can find the proof?","['ramsey-theory', 'proof-writing', 'combinatorics', 'discrete-mathematics']"
4593604,Simple explanations of differences between topology and geometry,"I have google but still can't find a simple explanations of differences between topology and geometry . Most of answers on internet are not easy to understand for non-math major. Based on what I found (these might be incorrect): One of papers: roughly states the following:
Topology focuses on the ' 2D ' structure but Geometry focuses on the ' 3D ' structure and spatial relations . https://arxiv.org/pdf/2110.07728.pdf Can anyone kindly provide simple explanations and examples to NON-math major audiences? Thanks in advance!","['general-topology', 'motivation', 'geometry']"
4593680,Ratios of a triangle question [duplicate],This question already has answers here : Finding the ratios of the segments in a triangle (2 answers) Closed 1 year ago . Consider a triangle $ABC$ . Take a point $C′$ on the side $AB$ . Assume that $AC′:C′B=2$ . Let $O$ be the midpoint of the segment $CC′$ . Let $B′$ be the intersection point of the side $AC$ and the line $BO$ . Find $AB′:B′C$ and $BO:OB′$ . I am trying to solve this question and I am a bit stuck because the only additional information I could gain from this question was that $CO$ and $OC'$ will be split in the $1:1$ ratio but thats not really helping me solve the rest of the question. Is there something I am missing ?,"['euclidean-geometry', 'geometry']"
4593682,Question about Riemannian metrics,"I was reading a pdf about geodesics on Riemannian manifolds, and I've found this proposition $\textbf{Proposition :}$ Let $(M,g)$ be a Riemannian manifold. For every point, $p \in M$ , in normal coordinates at $p$ , $$g\left(\frac{\partial}{\partial x_i}, \frac{\partial}{\partial x_j}\right)_p = \delta_{ij} \quad \text{and} \quad \Gamma^k_{ij}(p)=0.$$ Before this proposition there is the following statement: The following proposition shows that Riemannian metrics do not admit any local invariants of order one. What does it mean that a Riemannian metric does not admit any local invariants of some order ?","['smooth-manifolds', 'differential-geometry']"
4593701,Water-pouring problem with $n$ jugs: closed form for total number of configurations.,"Consider the following water-pouring puzzle. Given $n$ initially empty jugs with maximum volumes $V_1, V_2,\dots ,V_n$ , and the following three actions on a jug: A jug is filled completely. A jug is emptied completely. The contents of a jug are poured into another jug until it is completely full, leaving any excess in the original jug. Is the closed form for the total number of possible configurations of $(V_{\rm jug\,1},V_{\rm jug\,2},\dots,V_{{\rm jug}\,n})$ given by $$\prod_{i=1}^{n}\left(\frac{V_i}{\gcd(V_1,\dots,V_n)}+1\right)-\prod_{i=1}^{n}\left(\frac{V_i}{\gcd(V_1,\dots,V_n)}-1\right)?$$ Example: Suppose we have two jugs with specified volumes, $V_1=4, V_2=3$ . Then the possible permutations of $(V_1, V_2)$ are $(0,0),(0,3),(3,0),(3,3),(4,0),(1,3),(1,0),(0,1),(4,1),(2,3)$ , $(2,0),(0,2),(4,2),(4,3)$ i.e. $14$ tuples, agreeing with $(4+1)(3+1)-(4-1)(3-1)=14$ , since $\gcd(4,3)=1$ . By considering the expansion of the monic polynomial $\prod\limits_{i=1}^n(\lambda-V_i)$ and substituting $\lambda=\pm1$ , we can also express this in terms of the elementary symmetric polynomial as the following $$2\sum_{k=1}^{n/2}\frac{e_{2k-1}(V_1,\dots ,V_n)}{\gcd(V_1,\dots ,V_n)^{2k-1}}\quad\text{for $n$ even and }2+2\sum_{k=1}^{(n-1)/2}\frac{e_{2k}(V_1,\dots ,V_n)}{\gcd(V_1,\dots ,V_n)^{2k}}\quad\text{for $n$ odd}, $$ for which I credit @TheSimpliFire for finding all the above general expressions. For example: $n=2$ (the $2$ jug case): the number of possible permutations for $2$ jugs of maximum volume $V_1,V_2$ is $2(V_1+V_2)/\gcd(V_1,V_2)$ ; or $n=3$ (the $3$ jug case): with maximum volumes $V_1,V_2,V_3$ , this has a total number of possible permutations of $2+2(V_1V_2+V_1V_3+V_2V_3)/\gcd(V_1,V_2,V_3)^2$ . However, one can simplify the problem by assuming WLOG that $V_1,V_2,\dots ,V_n$ are all coprime to each other and asking whether the closed form for the total number of possible combinations is given by $$\prod_{i=1}^{n} (V_i+1) - \prod_{i=1}^{n}(V_i-1)$$ since suppose for example that $\gcd(V_1,\dots ,V_n)=3$ , then we cannot make a jug have a volume that is not a multiple of $3$ , giving the same number of possible combinations as if we began with an initial step of scaling all the maximum volumes down by a factor of $3$ such that all the maximum volumes would now be coprime. Since writing this question, I have uploaded a Python script of mine, which is able to generate a table of the number of possible states for up to $4$ jugs, to GitHub and which was used to greatly assist in determining the formula in the question. I have linked it here in the hope of aiding anyone attempting to determine the number of states without the need for tedious by-hand computations. The code can likely be optimised massively, and I would be interested if anyone knows how to make it better. The context for this problem is due to a computer science coursework question of a friend of mine which asked to write a Java program using the DFS algorithm to find the number of possible states for the $V_1=8,V_2=5,V_3=3$ case, however, I wanted to think of the problem more mathematically and abstractly and out of my own interest tried to generalise it and find a general formula. Note that there are $160$ possible states with $V_1=8,V_2=5,V_3=3$ for which the above formulae agree.","['elementary-number-theory', 'combinatorics', 'discrete-mathematics']"
4593711,On a non split exact sequence,"Consider the following short exact sequence of groups $$
0 \longrightarrow \mathbb{Z} \longrightarrow \mathbb{C} \longrightarrow \mathbb{C}^{\ast} \longrightarrow 1
$$ where the last map is given by $z \mapsto \exp(2 i \pi z)$ . How can one show that this sequence is not split? It is easy to show that a continuous section cannot exist but here a section should just be a morphism. Any comment or answer would be highly appreciated! Thanks!","['group-theory', 'exact-sequence']"
4593781,Transformation of Variables as Marginalisation of Joint Distribution - Where am I going wrong?,"I am trying to derive some equivalent of the transformation of variables formula. That is, given a random variable $Y=g(X)$ , where $g$ is an invertible function, then the pdf of $Y$ , $f_Y(y)$ , is given by, $$
f_Y(y) = f_X(x)\left|\frac{dx}{dy}\right| \tag{1}.
$$ This result is readily derived by considering the relevant cdf and applying the chain rule. I am trying to find some alternate derivation via marginalisation. The marginal distribution, $f_Y(y)$ can also be found by, \begin{align}
f_Y(y) = &\int_\mathcal{X}f_{YX}(y,x)dx,\tag{2}\\
&=\int_\mathcal{X}f_{Y|X}(y|x)f_X(x)dx.\tag{3}
\end{align} Assuming that $g$ is invertible, I would have intuitively thought that $$
f_{Y|X}(y|x)=\delta(x-g^{-1}(y)),\tag{4}
$$ where $\delta$ is the Dirac delta function. As such, $$
f_Y(y)=\int_\mathcal{X}\delta(x-g^{-1}(y))f_X(x)dx=f_X(g^{-1}(y)).\tag{5}
$$ This is obviously incorrect. I believe I am making a mistake in (2), as the joint distribution is potentially not well defined? For context, I have a little knowledge of measure theory. Any help explaining where I made a mistake and how to correctly proceed would be greatly appreciated!","['conditional-probability', 'metric-spaces', 'probability-theory', 'probability', 'random-variables']"
4593825,A doubt in multivariable calculus,"Let $g:\mathbb{R}^3 \rightarrow \mathbb{R}^3$ be defined by $g(x,y,z)=(3y+4z,2x-3z,x+3y)$ and let $S= \{ (x,y,z) \in \mathbb{R}^3 : 0 \leq x \leq 1  ,0 \leq y \leq 1 , 0 \leq z\leq 1 \}.$ What is the image of the region $S$ under then mapping $g$ ? My attempt: Since $0 \leq y \leq 1 , 0 \leq z\leq 1$ and that the first coordinate of the mapping $g$ is $3y+4z,$ the first coordinate of the image of the region $S$ under $g$ will be from $0$ to $7.$ I got $0$ as lower limit because $min\{ 3y+4z : 0 \leq y \leq 1 , 0 \leq z\leq 1 \}=0$ and $max\{ 3y+4z : 0 \leq y \leq 1 , 0 \leq z\leq 1 \}=7.$ By doing this for second and third coordinate of the image of $S$ under $g,$ I got $g(S)$ as $$g(S)=\{ (x,y,z) \in \mathbb{R}^3 : 0 \leq x \leq 7  ,-3 \leq y \leq 2 , 0 \leq z\leq 4 \}.$$ Have I gone wrong somewhere?
I feel I have made a mistake because this was part of a bigger problem, and my answers are not matching.
Any ideas would be of great help. P.S. The original question is: Find $\alpha$ such that $$\iiint_{g(s)}(2x+y-2z)dxdydz=\alpha\iiint_S z dxdydz.$$","['multivariable-calculus', 'vector-analysis']"
4593868,"Birthday paradox - variance, parallelisation, simple proofs?","I am looking for an elementary proof of the fact that expected time for finding a colision with $n$ bins is $\sqrt{\frac{\pi n}{2}} + O(1)$ . The proof that I knows relies on the asymptotic expansion of the Ramanujan $Q$ -function. Additional information on the parallel algorithm (discussed below) is welcome too. Background information below Suppose we sample uniformly random elements from a set of cardinality $n$ , and save them in a table. We continue doing this process (each sampling is one step) until we get a collision. What is the expected number of steps until we find the first collision? This is a common problem, also known as the birthday paradox, since the solution is $O(\sqrt{n})$ which is rather unintuitive. Suppose we sample $k$ times. Then, the probability of having no collisions after $k$ steps is $$
\prod_{i=0}^{k-1}\left(\frac{n-i}{n}\right) = \prod_{i=0}^{k-1}\left(1-\frac{i}{n}\right) \leq \prod_{i=0}^{k-1}e^{\frac{-i}{n}} = e^{-\sum_{i=0}^{k-1}\frac{i}{n}}=e^{\frac{-k(k-1)}{2n}}\approx e^{-\frac{k^2}{n}},
$$ and so the probability of a collision with $k$ steps is $$
1 - \prod_{i=0}^{k-1}\left(\frac{n-i}{n}\right) \geq 1 - e^{-\frac{k^2}{n}}
$$ which is $O(1)$ when $k=\Theta(\sqrt{n})$ . The probability of no collision after $k$ steps can also be written as $\frac{n!}{(n-(k-1))! n^k}$ . Now consider the following question What is the expected number of steps until the first collision? I haven't seen an easy solution to this problem. This question and the corresponding Wikipedia page treat it, without a proof. Let $X$ is the random variable ""index of step of first collision"". Then $$
\mathbb{E}[X] = \sum_{k=1}^{\infty} \mathbb{P}[X \geq k] = 1 + \sum_{k=1}^n \frac{n!}{n^k (n-k)!},
$$ which is easy to prove from the above formula. Now comes the non-trivial part. The function $$
Q(n) = \sum_{k=1}^n \frac{n!}{n^k (n-k)!}
$$ is known as the Ramanujan $Q$ -function, and has the asymptotic expansion (in $\sqrt{n}$ ) $$
Q(n) = \sqrt{\frac{\pi n}{2}} - \frac{1}{3} + \frac{1}{12}\sqrt{\frac{\pi}{2n}} + O\left(\frac{1}{n}\right),
$$ and therefore the expected number of steps until first collision is $\sqrt{\frac{\pi n}{2}} + O(1)$ . Is there a simpler proof that the expected number of steps is $\Theta(\sqrt{n})$ ? What is the variance of $X$ ? Now suppose we parallelise, i.e., we run the same algorithm on $m$ different machines which do not communicate with each other. What is the expected run time until we find the first collision? The interesting thing here is that parallelisation with $m$ machines only gives a $\sqrt{m}$ improvement in the run-time. One can do a similar argument to show that after $k$ steps the probability of having no collisions is $$
\prod_{i=0}^{k-1}\left(\frac{n-i}{n}\right)^m = \cdots \approx e^{-\frac{k^2m}{2n}},
$$ so we will have an $O(1)$ probability of collision when $k^2m \approx n$ . Since the run-time is $k$ we have $k \geq \sqrt{\frac{n}{m}}$ , so increasing $m$ only gives a square-root improvement in the run-time $k$ . However, I read here that the expected number of steps until first collision is $\sqrt{\frac{\pi n}{2m}} + O(1)$ , which is a statement that I can't prove, so my final question is How do I prove that for the parallel algorithm the expected number of steps until first collision is $\sqrt{\frac{\pi n}{2m}} + O(1)$ ? But I'd also like to ask Is there a simple proof that this expected number of steps is $\Theta(\sqrt{\frac{n}{m}})$ ? What is the variance of the number of steps ? I feel like the parallelisation questions should be easily provable if one knowns the variance of $X$ .","['variance', 'birthday', 'collision-detection', 'expected-value', 'probability']"
4593935,Why is Stokes Theorem failing me?,"I am really sorry if it's a stupid question, but I can't seem to get what I am doing wrong. I have a final approaching and for the most part I feel like I understand Stoke's theorem (or at least why it's so convenient). But I am unable to find what's wrong with what I am doing. Let $T$ be the triangle of vertices $(2, 0, 0)$ , $(0, 2, 0)$ and $(0, 0, 2)$ .
Compute the flux (surface integral) of the vector field $F (x, y, z) = (z, z, z)$ accross $T$ . I have worked out the problems using segments, and got the expected answer of $4$ . I then tried to use Stokes theorem on it, but when I do that, I get zero.
From my understanding of stokes, it should apply here since: I have a vector field I have a surface and a boundary (plane with $x+y+z = 2$ , and the triangle $T$ ) The boundary is simple and closed. But when I work it out, it reduces to double integral over projection of T of $(-1,1,0)$ (Curl of $F$ ) and $(1,1,1)$ (Normal Vector). Which simply gives 0.","['multivariable-calculus', 'vector-analysis']"
4593943,Cosine series for Dirac Delta comb,"I am learning a bit about distributions and came across the following... In ""Theory of Distributions, a nontechnical introduction"" by Richards and Youn there is a formula with no explanation in the introduction. It states that as a distribution the following makes sense: $$\sum_{n=-\infty}^\infty \delta''(x-2\pi n)=\frac{-1}{\pi}\sum_{n=1}^\infty n^2\cos(nx) $$ Could someone please explain why the singularities don't pose an issue- how does one make sene of this summation? i am most curious about the singularity when the $n$ on the LHS=0. is this a known cosine representation of $\delta''$ ?
thanks","['distribution-theory', 'calculus', 'sequences-and-series', 'real-analysis']"
4593991,Solve $x\left(1+\sqrt{1-x^2}\right)=\sqrt{1-x^2}$,"I would like to solve the equation $x\left(1+\sqrt{1-x^2}\right)=\sqrt{1-x^2}$ analytically, without using Wolfram Alpha. I have tried several substitutions, including $x=\cos(t)$ , $\sqrt{1-x^2}=t$ , but they all fail and eventually result in a quartic polynomial which is not easily solved. Any suggestions would be helpful.","['radicals', 'substitution', 'algebra-precalculus', 'quartics', 'quadratics']"
4594013,How many distinct 5 letter arrangements can be made from ‘FLOYDADA’ with repetition allowed?,"I’m not sure if I am making a mistake or if the key is wrong but I’m working a problem on a mathematics competition practice test which states: How many distinct 5 letter arrangements can be made from ‘FLOYDADA’ with repetition allowed? I’m assuming that the repetition allowed means that the D and A can be used twice and not that any letter can be used multiple times.  To solve I separated into 3 cases, one where there are no repeated letters.  One where there is one repeated letter, and one where 2 are repeated.  Here is my solution: No repeats: $6C5 \cdot 5! = 720$ One repeat: $2C1 \cdot 5C4 \cdot \frac{5!}{2!} = 600$ Two repeats: $2C2 \cdot 4C3 \cdot \frac{5!}{2!2!} = 120$ Altogether, I get $1440$ but the key says the answer is $2040$ .  Am I doing something wrong?  I would really appreciate some help.  Thanks!","['permutations', 'combinatorics']"
4594033,Markov Chains: From Theory to Application,"A question that I have always wondered about is that how are the Transition Probabilities within a Markov Chain estimated in real-world applications? I tried to learn more about this online and found the following link ( https://hesim-dev.github.io/hesim/articles/mlogit.html ). Over here, the following equation and explanation is provided: Procedurally, I was trying to understand how this works. Suppose there is a dataset with 3 States (State A, State B, State C). Each row in this dataset is an individual medical patient and contains some information on their covariates (e.g. height, weight, blood pressure, etc.), the state they were in and the state they transitioned to (let's assume that transitions can only take place at fixed discrete times, e.g. the first of January) : patient_id initial_state final_state   height age   weight
1          1             A           B 147.9283  49 85.03746
2          2             B           B 147.6188  50 98.37848
3          3             A           C 146.0570  51 87.79418
4          4             C           A 147.0269  56 86.38467
5          5             C           A 158.2545  47 83.81863 Suppose we want to fit a DISCRETE TIME MARKOV CHAIN to this data and estimate the transition probabilities - My understanding is the following: Isolate a subset of all rows where the initial state was State = State A Fit a Multinomial Logistic Regression to this subset of rows - doing this will provide you with general equations to calculate the probability of anyone within the population transitioning to any of the 3 States based on their covariate vector. This will also tell you the effect of different covariates on the transition probability and if these effects were statistically significant Repeat these two steps from the other two states (i.e. isolate the subset where initial state = State B, etc.) and fit a Multinomial Logistic Regression. In the end, you will have a 3 x 3 transition matrix which equations (as provided above) that estimate the transition probabilities based on a given vector of covariates
Based on these transition probabilities, you can now perform standard calculations as is done with Markov Chains - for example, given an initial probability distribution vector, what is the probability that this Markov Chain will be State B after ""k"" iterations? I am not sure if in this transition matrix I described above, transition probabilities within a given row will sum to 1? Is my understanding of the above correct? Thanks!","['statistics', 'probability']"
4594053,If $G$ is nilpotent then so is $G/Z(G)$.,"The Problem : If $G$ is nilpotent then so is $G/Z(G)$ . My Background : Chapter 1-5 of Abstract Algebra $\mathit{3^{rd}}$ edition by Dummit and Foote. For any group $G$ define the following subgroups inductively: $Z_0(G)=1, Z_1(G)=Z(G)$ , and $Z_{i+1}(G)$ is the subgroup of $G$ containing $Z_i(G)$ such that $Z_{i+1}(G)/Z_i(G)=Z(G/Z_i(G))$ (i.e., $Z_{i+1}(G)$ is the complete preimage in $G$ of the center of $G/Z_i(G)$ under the natural projection). The chain of subgroups $$Z_0(G)\leq Z_1(G)\leq\dots$$ is called the upper central series of $G$ . A group is called nilpotent if $Z_c(G)=G$ for some $c\in\mathbb{Z}$ . My Attempt : Taking hint from this post , I tried to show that $Z_i(\overline{G})=Z_{i+1}(G)/Z(G)$ , where $\overline{G}=G/Z(G)$ . We proceed by induction on $i$ . Base Case : $i=1$ , then $Z_1(\overline{G})=Z(\overline{G})=Z(G/Z(G))=Z_2(G)/Z_1(G)=Z_2(G)/Z(G)$ . Induction Hypothesis : Suppose $Z_{i-1}(\overline{G})=Z_i(G)/Z(G)$ . Now, we have $$
Z_i(\overline{G})/Z_{i-1}(\overline{G})=Z(\overline{G}/Z_{i-1}(\overline{G}))=Z(G/Z(G)/[Z_i(G)/Z(G)]),\tag1
$$ where the first equality is by the definition of the upper central series of $\overline{G}$ and the last equality is by the induction hypothesis. By the Third Isomorphism Theorem , $G/Z(G)/[Z_i(G)/Z(G)]\cong G/Z_i(G)$ , hence $Z(G/Z(G)/[Z_i(G)/Z(G)])\cong Z(G/Z_i(G))$ , thus $(1)$ becomes $$
Z_i(\overline{G})/Z_{i-1}(\overline{G})\cong Z(G/Z_i(G))=Z_{i+1}(G)/Z_i(G),\tag2
$$ where the last equality is by the definition of the upper central series of $G$ . Screeching halt . I realize that the isomorphism I derived did not really help. But thinking of $Z_i(\overline{G})$ as the complete preimage in $\overline{G}$ of the center of $\overline{G}/Z_{i-1}(\overline{G})$ did not quite offer any insight on its relation with $Z_{i+1}(G)/Z(G)$ either, which is supposed to be an equality. Any help would be greatly appreciated.","['nilpotent-groups', 'group-theory', 'abstract-algebra']"
4594100,Variational Autencoders and the inequality $\mathbb{E}_{z∼q(z|x)}log (p_{model}(x | z)) − D_{KL}(q(z | x)||p_{model}(z)) ≤ log (p_{model}(x))$,"I am reading section $20.10.3$ of the book Deep Learning on Variational Autoencoders, where the authors write: To generate a sample from the model, the VAE ﬁrst draws a sample $z$ from the code distribution $p_{model}(z)$ . The sample is then run
through a differentiable generator network $g(z)$ . Finally, $x$ is
sampled from a distribution $p_{model}(x;g(z)) =p_{model}(x | z)$ .
During training, however, the approximate inference network (or
encoder) $q(z | x)$ is used to obtain $z$ , and $p_{model}(x | z)$ is
then viewed as a decoder network.The key insight behind variational
autoencoders is that they can be trained by maximizing the variational
lower bound $L(q)$ associated with data point $x:$ $$L(q) = \mathbb{E}_{z∼q(z|x)}log (p_{model}(z, x)) + H(q(z | x))$$ $$= \mathbb{E}_{z∼q(z|x)}log (p_{model}(x | z)) − D_{KL}(q(z |
> x)||p_{model}(z))$$ $$≤ log (p_{model}(x))$$ I'm not sure about the last inequality. I know that the $KL$ divergence is always positive, but I'm not sure why the expectation is not present. By definition: $\mathbb{E}_{z∼q(z|x)}log (p_{model}(x | z)) = \sum_{z \in Z} q(z|x) log (p_{model}(x | z))$ , but how does this relate to $log (p_{model}(x))$ ?","['machine-learning', 'statistics']"
4594180,Maximum/Minimum on sphere,"I've attempted a problem in Spivak's Calculus on Manifolds and I can't find a solution online.  This is question 2-27, which is as follows Define $g,h:\{x\in\mathbb{R}^2:|x|\le1\}\rightarrow\mathbb{R}^3$ by $$\begin{split}
g(x,y)&=(x,y,\sqrt{1-x^2-y^2})\\
h(x,y)&=(x,y,-\sqrt{1-x^2-y^2})
\end{split}$$ Show that the maximum of $f$ on $\{x\in\mathbb{R}^3:|x|=1\}$ is either the maximum of $f\circ g$ or the maximum of $f\circ h$ on $\{x\in\mathbb{R}^2:|x|\le1\}$ . My solution is as follws Assume that the maximum of $f$ occurs at some point $g(a)\in\mathbb{R}^3$ where $a\in\{x\in\mathbb{R}^2:|x|<1\}$ , and that $D_i\left(f\circ g\right)(a)$ exists.  Then by the chain rule $$D_i(f\circ g)(a)=D_if(g(a))\cdot D_ig(a)$$ since the maximum of $f$ appears at $g(a)$ , by theorem 2-6 $D_if(g(a))=0$ , and so $D_i(f\circ g)(a)=0$ .  This shows that the maximum of $f$ occurs at the maximum of $f\circ g$ . I have some issues with my solution, mainly that since the theorem only works in the interior so for points $|x|=1$ where $x\in\mathbb{R}^2$ the conclusion I've drawn doesn't hold on the boundary.  However, since it would seem that $f=g$ on the boundary it might not count? Also I'm not sure whether I've essentially done "" $D_if(b)=0$ so the maximum is at $b$ "", but I've tried to word it as similarly as the theorem to check that I haven't.","['multivariable-calculus', 'solution-verification']"
4594216,Generating fuction of the Bessel Function,"Consider the Bessel DEQ: $$x^2y''+xy'+(x^2-\alpha^2)y=0$$ Find a function $F$ such that $$F(x,t)=\sum_{\alpha=-\infty}^\infty J_{\alpha}(x)t^{\alpha}$$ I received a few instructions: a) first compute $\frac{\partial^n}{\partial x^n}F(x,t)$ and $\frac{\partial}{\partial t}(t\frac{\partial}{\partial t}F(x,t))$ b) Then exploiting the results of the first hint, write down the PDE that $F(x,t)$ has to satisfy. c) Then given the ansatz $F(x,t)=\exp(xT(t))$ find the two ODEs that $T(t)$ has to satisfy ( $T(t)=t^r$ ) d) Last, show that $T(t)=-T(1/t)$ which will fix all constants in the generating function which gives the generating function of the Bessel functions. I'm already running into problems with the first hint, I'm not sure how to take the $n$ th derivative of the $F(x,t)$ function and I would really appreciate some assistance! EDITs: Calculated the partial derivatives according to the helpful comments, but now stuck on the second part! $$\frac{\partial}{\partial x}J_{\alpha}(x)=\frac{1}{2}(J_{\alpha-1}(x)-J_{\alpha+1}(x))$$ $$\frac{\partial^2}{\partial x^2}J_{\alpha}(x)=\frac{1}{4}(J_{\alpha-2}(x)-2J_{\alpha}(x)+J_{\alpha+2}(x))$$ $$\frac{\partial}{\partial t}F(x,t)=\Sigma_{-\infty}^{\infty}J_{\alpha}(x)\alpha^2t^{\alpha-1}$$ Now for the second part, as @Gary mentioned in the comments I need to have $\sum\limits_{n =  - \infty }^\infty  {(x^2 J''_n (x) + xJ'_n (x) + (x^2  - n^2 )J_n (x))t^n } \;\; (=0)$ . But as the second hint suggests, i need to have a PDE but not sure where the time partial dertivative should be...Thanks 2nd Edit With the help of Gary i have : $$x^2\Sigma_{\alpha=-\infty}^{\infty}J_{\alpha}^2(x)t^{\alpha}+x\Sigma_{\alpha=-\infty}^{\infty}J_{\alpha}(x)t^{\alpha}+x^2\Sigma_{\alpha=-\infty}^{\infty}J_{\alpha}(x)t^{\alpha}-t^2\Sigma_{\alpha=-\infty}^{\infty}J_{\alpha}(x)t^{\alpha}$$ and the PDE follows: $$x^2F_{xx}+xF_x+x^2F-tF_t-t^2F_{tt}=0$$ Can someone give me a hint on how to find the two ordinary differential equations asked in c)?","['generating-functions', 'bessel-functions', 'ordinary-differential-equations', 'partial-differential-equations']"
4594250,How to prove this set has infinite measure,"Let $(a_k)$ be a sequence of real numbers and $A=\{a_k:k \in \mathbb{N}\}$ . Let $f: \mathbb{R} \rightarrow [0, \infty]$ , $f(x)=\sum_{k=1}^\infty $ $\frac{1}{4^k|x-a_k|}$ , if x $\notin A$ and $f(x)= \infty$ , if x $\in A$ . How can I prove that $m(\{x \in \mathbb{R} :f(x)<1\})= \infty$ , where m is the Lebesgue measure ? The first thing that I tried doing is assuming the sequence of functions $f_k(x)=$ $\frac{1}{4^k|x-a_k|}$ and find that $\int f_k dm= \infty $ Any ideas?","['measure-theory', 'lebesgue-integral']"
4594288,Can all the above sums be negative?,"So, I have an exercise for my school and I am a little lost. Every answer will be helpful! So about the exercise: Let $a,b,c,d,e,f,g,h$ be real numbers. Can all the following sums $$ac+bd\\ ae+fb\\ ag+bh\\ ce+df\\ cg+dh\\ eg+fh$$ be negative numbers?. I assumed that all of them are negative, I then assumed that every sum is the det of a matrix but it didn't help. Is there any way I can prove that it's not possible? Thank you all for your time!","['algebra-precalculus', 'linear-algebra']"
4594294,"I solved this interesting problem using Taylor series expansion, but would like to find a shorter way.","$f(x)=\frac{3x-1}{x^2-1}$ ; $f_1(x)= \frac{d}{dx}f(x)$ , $f_2(x)= \frac{d^2}{dx^2}f(x)$ ,
find $f_{50}(0)$ I was able to solve this question using the Taylor Series Expansion till 50 terms by differentiating it thrice and generalizing the pattern. I was unable to solve it by other shorter methods. The series expansion I got was:- $f(x)=1-3x+x^2-3x^3+x^4.....-3x^{49}+x^{50}$ $f_{50}(x)=50!$ $f_{50}(0)=50!$","['calculus', 'functions', 'derivatives']"
4594297,"In a circle, where should you draw a line segment of fixed length, to maximize the probability that it will be intersected by a random chord?","In a circle, a chord will be drawn by connecting two uniformly random points on the circle. Where in the circle should you draw a line segment of fixed length, to maximize the probability that it will be intersected by the chord? My attempts suggest (but do not prove), perhaps counter-intuitively, that the line segment should be drawn so that it is a chord of the circle. ATTEMPT $1$ Let the radius of the circle be $1$ , and let the length of the line segment be $2L$ , where $L<1$ . In deciding where to draw the line segment, the variables are: $p=$ distance between centre of circle and midpoint of line segment $\theta=$ acute angle between line segment, and line through centre of circle and midpoint of line segment. The diagram shows the line segment in red, and one of the randomly chosen points on the circle. For each value of angle $x$ , the probability that the line segment is intersected by the random chord, is $C/\pi$ . The overall probability is the average value of $C/\pi$ , as angle $x$ goes from $0$ to $2\pi$ . It takes a bit of work to express $C$ in terms of $L, p, \theta, x$ : $a=\sqrt{p^2+L^2-2pL\cos{(\pi-\theta)}}$ (cosine rule) $A=\arcsin{\left(\frac{L\sin{(\pi-\theta)}}{a}\right)}$ (sine rule) $b=\sqrt{1^2+a^2-2a\cos{(A+x)}}$ (cosine rule) $c=\sqrt{1^2+p^2-2p\cos{x}}$ (cosine rule) $\cos{B}=\frac{b^2+L^2-c^2}{2bL}=\frac{b^2+(2L)^2-d^2}{4bL}$ (cosine rule) $\implies d=\sqrt{2L^2-b^2+2c^2}$ $C=\arccos{\left(\frac{b^2+d^2-(2L)^2}{2bd}\right)}$ (cosine rule) $P=P(\text{line segment is intersected by chord})=\frac{1}{2\pi}\int_0^{2\pi}\frac{C}{\pi}dx$ Putting all of this into my computer, it seems that, for any value of $L<1$ , $P$ is maximized when the line segment is drawn as a chord of the circle. But trying to prove this analytically seems daunting. ATTEMPT $2$ Consider a disk of radius $r<1$ placed inside a unit circle. The distance between their centres is $t$ , and $P(t)$ is the probability that a random chord (chosen by connecting two uniformly random points on the unit circle) will intersect the disk. We will try to show that $P(t)$ is an increasing function in $t$ . Then in the original question, regard the line segment as a rigid row of many small disks. When the line segment (i.e. row of disks) is a chord of the unit circle, the $t$ -value of each small disk is maximized, so the probability that the line segment is intersected by a random chord is maximized. The diagram shows a disk, and one of the randomly chosen points on the circle. For each value of angle $x$ , the probability that the disk is intersected by the random chord, is $\alpha/\pi$ . The overall probability is the average value of $\alpha/\pi$ as angle $x$ goes from $0$ to $2\pi$ . $\alpha=2\arcsin{\left(\dfrac{r}{\sqrt{1+t^2-2t\cos{x}}}\right)}$ $P(t)=\dfrac{1}{\pi^2}\int_0^{2\pi}\arcsin{\left(\dfrac{r}{\sqrt{1+t^2-2t\cos{x}}}\right)}dx=\dfrac{2}{\pi^2}\int_0^{\pi}\arcsin{\left(\dfrac{r}{\sqrt{1+t^2-2t\cos{x}}}\right)}dx$ We can express the integral as a riemann sum, then differentiate with respect to $t$ , then express the new riemann sum as an integral. So we get: $\dfrac{dP(t)}{dt}=\dfrac{2r}{\pi^2}\int_{0}^{\pi}\dfrac{\cos{x}-t}{(1+t^2-2t\cos{x})\sqrt{1+t^2-2t\cos{x}-r^2}}dx$ Since $r$ will approach $0$ , I think we can ignore the $r^2$ in the denominator. So we just have to show that $\int_{0}^{\pi}\dfrac{\cos{x}-t}{(1+t^2-2t\cos{x})^{1.5}}dx>0$ for $0<t<1$ , but I do not know how to show this. I found a related question , but it has not helped me. ATTEMPT $3$ I tried to use the method in the answer of @Gribouillis below to get an expression for the probability that the line segment will be intersected by a random chord, but it does not seem to make the maximization problem any easier than my ""Attempt $1$ "". EXTRA INFORMATION $P$ seems to be minimized when the line segment's midpoint is at the centre of the circle. The ratio of the maximum to minimum probabilities seems to approach $\pi/2$ as $L\to0$ . Just for fun, I asked my high school students this question, and asked them to use their intuition, with the following choices: 24 students chose A. 18 students chose B. 10 students chose C. 6 students chose D.","['integration', 'circles', 'geometry', 'optimization', 'probability']"
4594349,Question about the definition of Markov kernel,"Let $(X,\mathcal A)$ and $(Y,\mathcal B)$ be measurable space. A ''Markov kernel'' with source $(X,\mathcal A)$ and target $(Y,\mathcal B)$ is a map $\kappa : \mathcal B \times X  \to [0,1]$ with the following properties: (1) For every (fixed) $B \in \mathcal B$ , the map $x \mapsto \kappa(B, x)$ is $\mathcal A$ -measurable. (2) For every (fixed) $x \in X$ , the map $B \mapsto \kappa(B, x)$ is a probability measure on $(Y, \mathcal B)$ . Can someone please explain what this definition is saying? I am not getting the point, please if someone explain with example that will be great help. Thanks.","['statistical-inference', 'information-geometry', 'information-theory', 'markov-process', 'probability-theory']"
4594407,Does the sequence of positive roots of $x^n+x^{n-1}+\cdots+x-1=0$ converge?,"For a positive integer $n$ , let $a_n$ denote the unique positive real root of the equation $$x^n+x^{n-1}+\cdots+x-1=0.$$ Then what can we say about the sequence $\{a_n\}$ ? We can observe that all $a_n<1$ so if $\lim a_n$ exists, let's say at $L$ , then $L \le 1$ . Also \begin{align} 
&&x^n+x^{n-1}+\cdots+x-1 &= 0 \\ 
\implies&& 
x^{n+1}-1 &= 2(x-1) \\ 
\implies&& x^{n+1} &= 2x-1 \\ 
\implies&& a_n^{n+1} &= 2a_n-1. 
\end{align} Taking limit both side we get $0=2L-1$ , i.e. $L=\frac12$ but how to show that the limit always exists?","['roots', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
4594437,help with Kernel and image of an endomorphism,"I am given the kernel and image of an endomorphism in $\Bbb R^3 $ , which are: $\ker f =\langle(1,1,0)\rangle$ $\text{Im}\, f = \langle(0, 1, -1), (2, 1, 2)\rangle$ and I have to find the matrix associated with the function. This is what I've done si far: $$ f(x,y,z) = ((0, 1, -1),(2,1,2),(a,b,c))*(x,y,z) = ((2y+az), (x+y+bz), (-x+2y+cz))$$ (I multiplied the two matrices) but then I obtain that $f(1,1,0) = (2,2,1)$ which cannot be since this is the base of the kernel linear subspace. Thanks to everyone in advance :)","['morphism', 'functions', 'linear-algebra']"
4594459,Why can't the antiderivative of 1/x be found using the limit of the power rule? [duplicate],"This question already has answers here : Demystify integration of $\int \frac{1}{x} \mathrm dx$ (11 answers) Closed 1 year ago . This post was edited and submitted for review 1 year ago and failed to reopen the post: Original close reason(s) were not resolved Which step does this go wrong? $$\frac{d}{dx} x^n = nx^{n-1}$$ $$∫ \frac{d}{dx} \left(x^{n} \right) dx = ∫ n x^{n-1} dx$$ $$x^n + c = n ∫ x^{n-1} dx$$ $$\frac{x^n}{n} + c = ∫ x^{n-1} dx, n≠0$$ $$\lim_{n→0} \left[ \frac{x^{n}}{n} + c = ∫ x^{n-1} dx \right]$$ However $$\lim_{n→0} \left[ \frac{x^{n}}{n}+c \right] DNE$$ Therefore $$ \lim_{n→0} \left[ ∫ x^{ n-1} dx \right] DNE$$ I understand that $∫ x^{-1} dx = \ln(x)$ , but why doesn't the limit work?","['integration', 'limits', 'calculus', 'improper-integrals']"
4594473,Prove that $\angle DIH=30^{\circ}$,"Let $ABCD$ a parallelogram with $\angle A>60^{\circ}$ . If $\triangle ACE, \triangle DEF, \triangle BEG$ are equilateral as below then show that $\triangle DHI$ is isoscele, where $FB\cap DG=\{H\}, FB\cap DC=\{I\}$ . I notice that $\triangle DEG\equiv \triangle BFE$ . Then $\angle HDE\equiv \angle HFE$ and $\angle HBE\equiv \angle HGE$ . So $HDFE, HBGE$ are inscriptible. Then $\angle DHI=120^{\circ}$ . Now I have to prove that $\angle DIH=30^{\circ}$ .","['contest-math', 'euclidean-geometry', 'geometry']"
4594588,Upper bound on ratio of probability sums,"Let us say I have some vector of probabilities $\vec{V}$ such that $\sum_i v_i = 1$ .  I am trying to find a sharp upper bound on $\frac{\sum_i v_i^3}{(\sum_i v_i^2)^2}$ .  I can currently prove via Cauchy-Schwarz a very bad upper bound - this ratio must be less than the number of entries in the vector.  But I have numerical evidence that leads me to believe that something like 1.5 should be a sharp upper bound, not dependent on the vector length.","['inequality', 'linear-algebra', 'probability']"
4594673,"Can $\mathbb{S}^2$ wind around $\mathbb{S}^2$ twice, just like you would wind $\mathbb{S}^1$ around $\mathbb{S}^1$ $n$ times","NOTE: This question is a duplicate of this and this . These questions are answered. I know this topic is about algebraic topology, but my knowledge there is rather weak. I essentially want to know if there exists a continuous function $\varphi:\mathbb{S}^2\rightarrow\mathbb{S}^2$ such that $(\forall x\in\mathbb{S}^2)|\varphi^{-1}(x)|=2$ ? Background thinking: If we replaced $\mathbb{S}^2$ with $\mathbb{S}^1$ and understood $\mathbb{S}^1$ as $R=\{z\in\mathbb{C}|1=|z|\}$ then $x\mapsto x^2$ would satisfy my condition. I wanted to do it with $\mathbb{S}^2$ too but it seemed that some point would be covered infinitely many times (here I had layering into longitutes in mind). Then I wanted to avoid that infinity and I reduced the problem to inverting $R$ to itself but with reversed orientation with bounded covering, or more precisely a function $f:\mathbb{S}^1\times[0,1]\rightarrow\mathbb{R}^2$ such that $f(\mathbb{S}^1\times\{0\})=R$ , $f(\mathbb{S}^1\times\{1\})=R$ but reversed and $|f^{-1}(x)|\le2$ for every $x$ . I found it and induced a related $\varphi$ but then I got two points which I covered only once. Then I remembered that I could simply twist the sphere just like $\mathbb{S}^1$ at all latitudes and leave the poles in place and I would have the exact same situation: Every point is covered twice except the poles which are covered only once. This made me thinking that I will always have such 2 points that I covered only once, either that or two infinitely covered points as with longitudes. How would I even go about proving it? EDIT: Comments gave some help towards formalizing my examples I gave in the discussion, so here is the clear definition of those 2 maps, after we identified $\mathbb{S}^2$ with $\mathbb{C}P^1$ : The longitudal one: $z\mapsto\frac{z}{|z|(1-|z|)}$ ; it covers the $\infty$ infinitely many times. The latitudal one: $z\mapsto z^2$ ; it covers $0$ and $\infty$ only once. EDIT 2: I have progressed a tiny little bit: If $X$ and $Y$ are metric spaces, $X$ being compact and $f:X\rightarrow Y$ is a continuous bijection then $f$ is a homeomorphism. Now I thought I could find some meaningful subset of $\mathbb{S}^2$ for which $\varphi$ restricted to it has this property, but I failed...","['general-topology', 'algebraic-topology']"
4594694,Are there any ODEs like these?,"I have been working on using automatic differentiation to compute the sensitivities of systems of ordinary differential equations of the form \begin{equation}
\frac{d\mathbf{u}(t)}{dt} = F(\mathbf{u}(t),t,\alpha),\qquad \mathbf{u}(0) = \mathbf{u}^0.
\end{equation} where $\alpha\in\mathbb{R}^M$ is a vetor of real valued parameters and $\mathbf{u}(t)\in\mathbb{R}^N$ . To showcase the advantages of the algorithm, I was looking for a real scenario where the vector valued function $F$ depends on a number of parameters $M$ much bigger than the number of equations in the system $N$ . By real, I mean some equation  with applicability in any scientific field. In the article ""Neural Ordinary Differential Equations"" by Ricky T. Q. Chen, they consider $F$ to be a neural network, where indeed $M \gg N$ , but I was looking for an alternative. What I have considered so far are PDEs discretized in space using finite differences, for example, the 1D heat equation with spatial dependent thermal diffusivity $\alpha(x)$ . However, if I consider the set of parameters $\alpha_i = \alpha(x_i)$ , where $x_i$ are the grid points, that would result in $M=N$ . I could artificially consider more parameters, like $\alpha_{i+\frac{1}{2}} = \alpha(x_{i+\frac{1}{2}})$ and force these on the function $F$ , but still I would get $M=2N$ , and I don't think it's usual for numerical methods to consider a finer grid for the parameters, rather the other way around  (thinking of FEM for example, where $\alpha(x)$ could possibly be represented by its mean value in the element). Another thing I have considered is if $F$ depends on some function which needs to be numerically evaluated, and thus would be approximated by a finite set of $M$ parameters ( for example, coefficientes for a given set of basis functions). Can you think of any real example like this where $M \gg N$ ? What I really really want is something like $M \approx N^2$ or at least $M \approx kN$ with $k \gg 1$ .","['numerical-methods', 'numerical-optimization', 'ordinary-differential-equations', 'partial-differential-equations']"
4594703,Criterion for local integrability of $1/f$ for $f$ a smooth function from $\mathbb R^2$ to $\mathbb C$,"Suppose I have a function $f=u+iv$ , $f:\mathbb{R}^2\to \mathbb{C}$ which is smooth, in the sense that $u$ and $v$ are smooth. I am wondering what some criteria are for $$
\int_{B_r(x_0,y_0)}\frac1 f\mathrm dA(x,y)
$$ to converge in some suitable sense, where $\mathrm dA(x,y)$ is the area measure. For example, if $f(x_0,y_0)=0$ but $f$ has no other zeros in the ball, is it enough to have $\nabla f(x_0,y_0)\ne 0$ ? Presumably, what I am asking is equivalent to asking if $$
\int_{B_r(x_0,y_0)}\frac {u} {u^2+v^2}\mathrm dA(x,y)
$$ and $$
\int_{B_r(x_0,y_0)}\frac {v} {u^2+v^2}\mathrm dA(x,y)
$$ converge separately.","['integration', 'measure-theory', 'real-analysis', 'complex-analysis', 'multivariable-calculus']"
4594731,"If $0 \leq A \leq B$ and $A$ and $B$ commute, then $A^n \leq B^n$","I am dealing with a problem from an old exam paper which might be simple but it turns out to be difficult for me to show. Here is the problem: Suppose that $0\leq A\leq B$ for self-adjoint elements $A,B$ in a $C^\ast$ -algebra. Then show that if $0\leq A\leq B$ and both $A$ and $B$ commute, then $A^n\leq B^n$ for every positive integer $n$ . More generally, show that if there are positive elements $C_j$ , $1\leq j\leq k$ with $0\leq A\leq C_1\leq C_2\leq \cdots \leq C_k\leq B$ so that any two neighbors in this list commute, then $A^n\leq B^n$ for every positive integer $n$ . I don't want the solution for this but maybe a sketch of strategy/sketch of proof so I can work the details out by my self. If you feel you would like to share a example of a solution then feel free. I just want to prepare for my exam in January. Thanks in advance.","['functional-analysis', 'operator-algebras']"
4594745,What did I do wrong in this simple related rates problem?,"The question is A street light is mounted at the top of a 15-ft-tall pole. A man  6 ft tall walks away from the pole with a speed of 5 ftys along a straight path. How fast is the tip of his shadow moving when he is 40 ft from the pole? I drew a diagram like this to model the situation: Where $x$ is the distance between the pole and the man, and $y$ is the length of the shadow of the man. I'm pretty sure the question is asking to find $\frac{dy}{dt}$ when $x=40$ ,  so I expressed $y$ in terms of $x$ . $$
\frac{15}{6}=\frac{x+y}{y} \Rightarrow y=\frac{2}{3}x
$$ I tried using the chain rule to find $\frac{dy}{dt}$ : $$
\frac{dy}{dt}=\frac{dy}{dx}\frac{dx}{dt}
$$ We're given that the man is walking at $5 \text{ ft/s}$ so $x$ is increasing at a rate of $5 \text{ft/s}$ , therefore, $\frac{dx}{dt}=5$ . To find $\frac{dy}{dx}$ we simply differentiate both sides of the equation with respect to $x$ . $$
y=\frac{2}{3}x \Rightarrow \frac{dy}{dx}=\frac{2}{3}
$$ So it follows that since $\frac{dx}{dt}=5$ and $\frac{dy}{dx}=\frac{2}{3}$ , $\frac{dy}{dt}$ must be $$
\frac{dy}{dt} = \frac{dy}{dx}\frac{dx}{dt} = 5\cdot \frac{2}{3} = \frac{10}{3} \text{ ft/s}
$$ Except that it isn't. The correct answer in the textbook I'm using states that it is in fact $\frac{25}{3} \text{ ft/s}$ . I'm just confused about where I went wrong! This was my entire process to get to the incorrect answer.","['related-rates', 'calculus', 'derivatives']"
4594762,Question inside $\int \tan^2 x dx$,"$$\int \tan^2 x dx$$ the question isn't behind on how to do it but rather on its solution , let me go through my solution quickly: If we do a substitution $u=\tan(x)$ then $du=\sec^2 x dx=(u^2+1)dx$ hence we get $$\int \frac{u^2}{1+u^2}du=\int\left(1-\frac{1}{1+u^2}\right)du=\tan x-\arctan(\tan x)+C$$ so no problem until here. The solution from the book or WolframAlpha is: $\tan x-x+C$ so it raised a question, since $\tan $ isn't injective and only has an inverse for $x\in(-\frac{\pi}{2},\frac{\pi}{2})$ , for which then $\arctan(\tan(x))=x$ is only true for $x\in(-\frac{\pi}{2},\frac{\pi}{2})$ . But then why if we do $\arctan(\tan x)=x$ in my solution we then get $$\tan x-x+C$$ which is the same as the correct solution the book or WolframAlpha shows, while we have only just considered the $x\in(-\frac{\pi}{2},\frac{\pi}{2})$ in my method? Isn't the indefinite integral considering $\forall x\in \mathbb{R}$ ? Why is this happening? Note: Not looking at all for an alternative solution or how to get there, rather why I get the correct solution by considering $x\in(-\frac{\pi}{2},\frac{\pi}{2})$ while an integral should be $\forall x\in \mathbb{R}$","['integration', 'indefinite-integrals', 'trigonometry', 'trigonometric-integrals']"
4594830,Behrend's construction on large 3-AP-free set,"Theorem (Behrend's construction) There exists a constant $C>0$ such that for every positive integer $N$ , there exists a $3$ -AP-free $A\subseteq[N]$ with $|A|\geq
 Ne^{-C\sqrt{\log N}}$ . Proof. Let $m$ and $d$ be two positive integers depending on $N$ to be
specified later. Consider the lattice points of $X=\{1,\dots, m\}^d$ that lie on a sphere of radius $\sqrt{L}$ : $$X_L:=\{(x_1,\dots,x_d)\in
 X: x_1^2+\dots+x_d^2=L\}.$$ Then $X=\bigsqcup\limits_{i=1}^{dm^2}
 X_i$ . So by pigeonhole principle, there exists an $L\in [dm^2]$ such
that $|X_L|\geq m^d/(dm^2)$ . Define the base $2m$ digital expansion $$\phi(x_1,\dots,x_d):=\sum_{i=1}^{d}x_i(2m)^{i-1}.$$ Then $\phi$ is
injective on $X$ . Furthermore, $x,y,z\in [m]^d$ satisfy $x+z=2y$ if
and only if $\phi(x)+\phi(z)=2\phi(y)$ . Since $X_L$ is a subset of a
sphere, it is $3$ -AP-free. Thus $\phi(X)\subseteq [(2m)]^d$ is a $3$ -AP-free set of size $\geq m^d/(dm^2)$ . We can optimize the
parameters and take $m=\lfloor e^{\sqrt{\log N}}/2\rfloor$ and $d=\lfloor \sqrt{\log N}\rfloor$ , thereby producing a $3$ -AP-free
subset of $[N]$ with of size $\geq Ne^{-C\sqrt{\log N}}$ , where $C$ is
some absolute constant. I understood the proof very well and I don't have any question regarding the proof. But I am little confused about paramaeters $m$ and $d$ . Can anyone explain in detail why do we choose these parameters $m$ and $d$ in such a way? Namely $m=\lfloor e^{\sqrt{\log N}}/2\rfloor$ and $d=\lfloor \sqrt{\log N}\rfloor$ . What is the idea beyond that choice? EDIT: I think that the statement of theorem has a small mistake. If you take $N=2$ , then $d=0$ but $d$ should be natural. I believe that this construction is true for sufficiently large $N$ . Am I right?","['ceiling-and-floor-functions', 'number-theory', 'additive-combinatorics', 'combinatorics', 'discrete-mathematics']"
4594841,Solve the radical equation for all reals: $x\left(1+\sqrt{1-x^2}\right)=\sqrt{1+x^2}$,Question: Solve the radical equation for all reals: $$x\left(1+\sqrt{1-x^2}\right)=\sqrt{1+x^2}$$ My approach: $$1+\sqrt {1-x^2}=\frac {\sqrt {1+x^2}}{x}\\1+2\sqrt {1-x^2}+1-x^2=\frac{1+x^2}{x^2}\\4(1-x^2)=\left(\frac{1+x^2}{x^2}+x^2-2\right)^2\\4x^4(1-x^2)=(x^4-x^2+1)^2$$ I don't know how can I proceed from here. Is there a way so that not using the complicated expansion of polynomials? I'm looking for methods that doesn't use $4$ or higher degree polynomial expansions.,"['algebra-precalculus', 'roots', 'radicals']"
4594893,"Intuitively, why aren't all points in a circle covered in an Apollonian gasket?","An Apollonian gasket is a figured formed by taking three mutually tangent circles, forming their Soddy circles , then recursively adding more circles by grabbing three circles and adding their Soddy circles. Here's an example: I've read online that not all points in the enclosing outer circle are covered by the circles that make up the Apollonian gasket, though the set of uncovered points has measure zero. While I trust that this is indeed the case, I'm having trouble articulating a coherent intuitive argument that explains why not all points are covered in the Apollonian gasket, but why all points would be covered in other limiting self-similar tilings. For example, consider the following way of tiling a square: subdivide the square into four congruent, smaller squares. Merge three of those squares into an L-shaped figure. Then recursively subdivide the remaining smaller square in the same way. That's shown here: This process will eventually cover every point in the original square. Is there an intuitive explanation for why Apollonian gaskets fail to cover every point in the outer circle while other sorts of tilings do eventually fill all of space?","['circles', 'geometry', 'recursion', 'fractals']"
4594931,Is this u-substitution valid?,"It is not necessary to take any example of any integral, so I'll just drop it: $$u=\sin x; \ \ \ du=\cos x dx$$ which my question is rather here : Is it possible to do: (squaring both sides) $$u^2=\sin^2(x)=1-\cos^2(x)$$ $$\cos(x)=\sqrt{1-u^2}$$ is it? I don't think it's true for the simple fact that $\sqrt{x^2}=|x|$ and not just $x$ but my question earlier today talked about how domain can be somewhat dealt with the constant $+C$ as shown in my question: My question about different domains on indefinite integrals So I'm very curious on whether it would be valid or not","['integration', 'calculus', 'trigonometric-integrals', 'indefinite-integrals', 'trigonometry']"
4595080,"$\{X_n,n>1\}$i.i.d,$S_n=X_1+\cdots+X_n$,$E(X_1) = 0$.Prove that $E|S_n|/n→0$","Suppose $\{X_n,n>1\}$ i.i.d, $S_n=X_1+\cdots+X_n$ , $E(X_1) = 0$ .Prove that $E|S_n|/n→0$ . Attempts: $\{X_n^+\}$ i.i.d and $\{X_n^-\}$ i.i.d,we have $E(|S_n|)$ = $E|\sum_i X_i^+-\sum_j X_j^-|$ and $E(\sum_i X_i^+)=E(\sum_j X_j^-)$ ,and from Strong Law of Large Numbers,we have $\sum_n X_n^+/n\to E(X_1^+)$ a.e and $\sum_n X_n^-/n\to E(X_1^-)$ a.e,but there are still gaps between the final result.","['law-of-large-numbers', 'probability-theory', 'probability']"
4595088,complex integration along real axis,"I am reading P. Coleman's many-body physics book . It gives a integral which is odd for me, $$
\int_{-\infty}^{\infty}\frac{{\rm d}x}{x - {\rm i}\alpha} =
{\rm i}\pi\operatorname{sgn}(\alpha),
\qquad \alpha \in \mathbb{R}.
$$ My question is how to calculate it. Sorry for my stupid question, at least it looks like.","['integration', 'complex-analysis', 'calculus']"
4595102,Tangency points of an ellipsoid with the coordinate planes,"Suppose you have an ellipsoid with known semi-axes, say $a,b,c$ .  The ellipsoid is then rotated so that the three axes of the ellipsoid are aligned with the three columns of a known rotation matrix $R$ .  Keeping this orientation, you place the ellipsoid on the $xy$ plane, in the first octant (where $ x \ge 0, y \ge 0, z \ge 0$ ).  Next, you drag the ellipsoid, while maintaining its spatial orientation, along the $xy$ plane, in a direction parallel to the $x$ axis, till it touches the $yz$ plane, then you drag it parallel to the $y$ axis, till it touches the $xz$ plane.  Now the ellipsoid is tangent to the $3$ coordinate planes.  The question is find the tangency points of the ellipsoid with the three coordinate planes . As for context, this question can be considered a extension of $2D$ ellipses in general orientation, to the $3D$ case involving an ellipsoid instead of an ellipse. My Progress: The algebraic equation of an ellipsoid is $ (r - r_0)^T Q (r - r_0) = 1 $ where $ r = [x , y, z ]^T $ and $ r_0 = [r_{0x}, r_{0y}, r_{0z}]^T $ is the center of the ellipsoid (unknown).  And $Q$ is known and is of the form $ Q = R D R^T $ with $ D = \text{diag}\bigg(\  \dfrac{1}{a^2},\  \dfrac{1}{b^2}, \ \dfrac{1}{c^2} \bigg) $ , and $R $ is a rotation matrix, whose columns (mutually orthogonal unit vectors) give the direction of the three axes of the ellipsoid. From tangency to the $xy$ plane, let $r_1$ be the tangency point, then $ (r_1 - r_0)^T Q (r_1 - r_0) = 1 $ and the gradient of the ellipsoid (which is a vector normal to its surface) points in a direction of $- \mathbf{k} $ (where $\mathbf{k} $ is the unit vector pointing in the positive $z$ -direction).  Hence, $ Q (r_1 - r_0) = -\alpha \mathbf{k} $ for some $\alpha \gt 0 $ Similar equations can written for $r_2$ and $r_3$ , the tangency points with the $xz$ plane and the $ y z $ plane respectively. Continuing with $r_1$ , and from the above equation, $ r_1 - r_0 = - \alpha Q^{-1} \mathbf{k} $ Plugging this into the equation of the ellipsoid, yields $ \alpha = \dfrac{1}{\sqrt{ \mathbf{k}^T Q^{-1} \mathbf{k} } } $ Everything on the right hand side is known, so $\alpha$ is now known. Recall that $r_1$ is the tangency point with $xy$ plane, so its $z$ -coordinate is $0$ .  Premultiplying the above equation for $r_1 - r_0$ by $ \mathbf{k}^T$ therefore, will give $ r_{0z} = \sqrt{ \mathbf{k}^T Q^{-1} \mathbf{k} } $ Similarly, we will get $ r_{0y} = \sqrt{ \mathbf{j}^T Q^{-1} \mathbf{j} } $ and $ r_{0x} = \sqrt{ \mathbf{i}^T Q^{-1} \mathbf{i} } $ where $\mathbf{i}$ and $\mathbf{j} $ are the unit vectors in the positive $ x $ and $y$ directions, respectively. Having found $r_0$ completely (the center of the ellipsoid).  Then now we can use the equations above for $(r_1 - r_0)$ , $(r_2 - r_0)$ , $(r_3 - r_0) $ to solve for $r_1, r_2, r_3$ .","['ellipsoids', 'geometry', 'multivariable-calculus', 'quadrics', 'linear-algebra']"
4595114,Does the Kronecker product preserve common irreducibility,"A set of $d\times d$ real or complex matrices is called “commonly irreducible” if those matrices do not jointly preserve a linear subspace with dimension strictly between $0$ and $d$ . I wanted to know whether the Kronecker product preserves this property. In other words, given two sets of commonly irreducible matrices $S$ and $Q$ , is the set $ 
S\otimes Q = \{P_1 \otimes P_2 \mid P_1 \in S, P_2\in Q\} 
$ commonly irreducible as well?","['matrices', 'linear-algebra', 'kronecker-product']"
4595120,How to decouple and solve these coupled differential equations,"While solving Euler's equation of motion for an asymmetric top, I cam across the three following equations : $$\dot{\omega_1}+\omega_2 \omega_3=0$$ $$\dot{\omega_2}-\omega_1 \omega_3=0$$ $$\dot{\omega_3}+\frac{\omega_2 \omega_1}{\mu^2}=0$$ The only information that I have been provided is that at time $t=0$ , $\omega=N\mu \hat{i}+N\hat{k}$ . So I know $\omega_1, \omega_2 $ and $\omega_3$ at time $0$ .
How can I decouple and solve the above set of equations ? I've already been given the solution as follows : $$\omega_1 =\frac{N\mu}{\cosh(Nt)}$$ $$\omega_2 ={N\mu}\tanh(Nt)$$ $$\omega_3 =\frac{N}{\cosh(Nt)}$$ How do I solve the equations myself ? I've solved linear coupled differential equations using matrices, however this one seems to be nonlinear because of the $\omega_i\omega_j$ terms.",['ordinary-differential-equations']
4595124,Functions of bounded variation on $\mathbb{R}^n$,"Given a function $f:[a,b]\rightarrow\mathbb{R}$ , we define the variation of $f$ on $[a,b]$ as follows: \begin{equation*}
V_a^b(f)=\sup\left\{\sum_{i=1}^{n}{|f(t_i)-f(t_{i-1})|}\middle|P=\{t_i\}_{i=0}^n\in\mathscr{P}([a,b])\right\}.
\end{equation*} where $\mathscr{P}([a,b])$ denotes the set of partitions of the interval $[a,b]$ . It's very easy to generalised this definition when the codomain is a metric space $(X,d)$ . It's enough to substitute $|f(t_i)-f(t_{i-1})|$ by $d(f(t_i),f(t_{i-1}))$ . We say that $f$ is a function of bounded variation if $V_a^b(f)<+\infty$ . I need to learn about functions of bounded variations defined on an open set of $\mathbb{R}^n$ . Given a function $f\in\mathcal{L}^1(U,\mathbb{R})$ , we define the variation of $f$ on $U$ as follows: \begin{equation*}
    V_U(f)=\sup\left\{\int_{U}{f(x)\cdot\text{div}(\varphi)(x)\,dx}\middle|\varphi\in\mathcal{C}_c^1(U,\mathbb{R}),|\varphi|\leq 1\text{ on $U$}\right\}.
\end{equation*} I don't understand why this is the natural definition. Could anyone explain how to get to this definition in a natural way?","['functions', 'analysis', 'bounded-variation']"
