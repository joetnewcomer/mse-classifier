question_id,title,body,tags
120405,"Compositional ""square roots""","Let $A$ be a set and $f\colon A\to A$ a function. The primary and general question is: What conditions are necessary so that there exists $g$ such that for each $x$ in $A$, $g(g(x))=f(x)$? This is a rather broad question, so a few more specific questions that result: Is there actually some primary literature or result for this problem? Or perhaps for some subproblem such as functions on the reals? Edit to clarify where the questions below arise from: Consider the case of $ f $ bijective. For this case, Qiaochu Yuan demonstrated here that $ f $ has a compositional square root exactly when the number of even (or infinite) cycles occuring in iteration of $ f $ is even or infinite. For infinite cycles: $ \{..., x_{-2}, x_{-1}, x_0, x_1, x_2, ...\} $ where $ x_{i+1} = f(x_i) $. If this already covers $ A $ and specifically, there is only one of these structures, there also is no compositional square root: Assume there was $ g $ with $ g(g)=f $, then $ g(x_0) = x_n $ for some $ 0 \neq n \in \mathbb{Z} $, and then $ g(x_1) = g(f(x_0)) = g(g(g(x_0))) = g(g(x_n)) = f(x_n) = x_{n+1} $. Inductively, then, $ g(x_i) = x_{n+i} $ for all $ i>0 $, but then $ g(g(x_{n}))=f(x_{n})=x_{n+1}=g(x_{n}) $ and that's a contradiction. An example of such a function is $ f: \mathbb{Z}\to\mathbb{Z} $, $f(x) = x+1 $. However, if A actually partitions into two components of the double-sided path form $ \{..., x_{-2}, x_{-1}, x_0, x_1, x_2, ...\} $ and $ \{..., y_{-2}, y_{-1}, y_0, y_1, y_2, ...\} $, then we can create $ g $ by defining it through $ g(x_i)=y_i $ and $ g(y_i)=x_{i+1} $. An example for this is $ f: \mathbb{Z}\to\mathbb{Z} $, $f(x) = x+2 $ - the components here are the even and odd numbers, and we create $ g $ by jumping between even and odd: $ g(x)=x+1 $. There was some discussion on another forum a while ago, which brought some partial results (such as: For $f\colon \mathbb{R}\to\mathbb{R}$ bijective and strictly monotonic such a $g$ exists); it seems that any such conditions are related to a kind of ""pairability"" of graphs generated by iterated application of $f$ (so that $g$ can ""jump"" between such graphs) and thus to graph theory, specifically infinite pseudo-trees. All literature on those seems to be algorithmically motivated and thus only for the (comparatively simple) finite case; i.e. finite A. Is there any literature on infinite pseudo-trees that might be relevant? Another aspect leads to set theoretic questions: The mentioned graphs are going to be equivalence classes under the equivalence relation $S$ on $A$ defined as 
$$xSy \iff\text{ There is }n \in \mathbb{N}\text{ with }f^n(x) = y\text{ or }f^n(y) = x.$$ 
These subsets are then, under iteration of $f$, the mentioned pseudo-trees, and the cardinalities of each preimage matter (since bijections are needed for said ""pairing"" of such trees). But even when acting on the reals, without CH it is not clear which cardinalities can be ""paired off"". And similarly, the cardinality of the height of such graphs seems relevant. Assuming CH for any relevant statement seems rather strong though, so the set-theoretic question is which other strengthenings of ZFC might be relevant here? An example to explain the above question: $ A = B \bigcup C \bigcup \{d\} \bigcup \{e\} $, $ B $ and $ C $ countable with fixed enumeration and $ f: A \to A $ with $ f(b_i) := c_i $,  $ f(c_i) := d $, $ f(d) := e $ and $ f(e):=d $ Claim: Then $ f $ does not have a compositional square root. Assume $ g $ with $ g(g)=f $. For arbitrary $ b \in B $ consider $ g(b) $. Cases: $ g(b) = d $ or $ g(b) = e $ lead to immediate contradictions due to $ g(g(b_i)) = f(b_i) $ being different for different i. Assume $ g(b) = c $ for some $ c \in C $. Then $ g(c) = f(b) \in C $ and thus $ d = f(g(c)) = g(f(c)) = g(d) $, but then $ g(g(d)) = d \neq f(d) $, contradiction. Thus, $ g(b) \in B $ for all b, but this is a contradiction as well, and so no such g can exist. But now consider two copies of our set: $ A^1 = B^1 \bigcup C^1 \bigcup \{d^1\} \bigcup \{e^1\} $ and $ A^2 = B^2 \bigcup C^2 \bigcup \{d^2\} \bigcup \{e^2\} $ and f defined on $ A = A^1 \bigcup A^2 $ as above for $ A^1 $ and $ A^2 $. Now $ g(g)=f $ exists: $ g(b^1_i) = b^2_i $, $ g(b^2_i) = d^1 $ , $ g(d^1) = d^2 $, $ g(d^2) = e^1 $, $ g(e^1) = e^2 $ and $ g(e^2) = d^1 $. Note that $ A^1 $ and $ A^2 $ are equivalence classes under S and that these satisfy the definition of ""pairability"" below. And now, finally, here is where cardinalities matter: This construction does not work if $ card(B^1) = card(C^1) \neq card(B^2) = card (C^2) $ as it assumes bijections. CH greatly simplifies which cardinalities there are to consider, but it is a rather strong assumption. The question, then, is which axioms in addition to ZFC help with simplifying the cardinalities to consider in scenarios like this, especially considering that the structures defined here can be seen as directed trees. Lastly, while it seems that the definition of ""pairable"" can be made rigorous like this: two such equivalence classes $R$ and $S$ are pairable for $f$ if and only if there exists functions $h\colon S\to T$ and $j\colon T\to S$ such that for all  $s \in S$, $j(h(s))=f(s)$ and for all $t \in T$,  $h(j(t))=f(t)$. But then it gets a bit murky: what we need is that every equivalence class can be paired with another one (or itself), but is stating that there exists a partition of the equivalence classes into groups of pairables equivalent to the existence of $g(g)=f$? It seems that ""$\Rightarrow$"" holds, but the other direction is less clear. Is it possible to have a case where $g$ can jump between more than two classes that would not be reducible to a ""pairable"" case? Perhaps even infinitely many, in which case the cardinality issues mentioned above come into play again?","['set-theory', 'functions']"
120407,Transforming the Laplace operator from Polar to Cartesian coordinates,"I'm trying to find the error in my logic here. Let's say we are given the Laplace operator in polar coordinates: $$ \frac{\partial^2}{\partial r^2} + \frac{1}{r}\frac{\partial}{\partial r} + \frac{1}{r^2}\frac{\partial}{\partial \theta^2} \tag{1} $$ and we're interested in transforming back to Cartesian coordinates.  We could first make use of the usual coordinate transformation, namely $$ u = r \sin(\theta), ~~v = r \cos(\theta), \tag{2}$$ to write the partial derivatives in polar coordinates as follows, $$ \frac{\partial}{\partial r} = \sin(\theta) \frac{\partial}{\partial u} + \cos(\theta)\frac{\partial}{\partial v}, \tag{3}$$ $$ \frac{\partial}{\partial \theta} = r \cos(\theta) \frac{\partial}{\partial u} -  r \sin(\theta)\frac{\partial}{\partial v}. \tag{4}$$ I think we should then rewrite all $\theta$ and $r$ in terms of $u$ and $v$, making use of $r^2 = u^2 + v^2$, so that in particular $$ \sin(\theta) = \frac{u}{\sqrt{u^2 + v^2}}, \tag{5}$$
$$ \cos(\theta) = \frac{v}{\sqrt{u^2 + v^2}} \tag{6}.$$ With this, (3) and (4) then become $$ \frac{\partial}{\partial r} = \frac{u}{\sqrt{u^2 + v^2}} \frac{\partial}{\partial u} + \frac{v}{\sqrt{u^2 + v^2}} \frac{\partial}{\partial v}, \tag{3*}$$ $$ \frac{\partial}{\partial \theta} = v \frac{\partial}{\partial u} -  u \frac{\partial}{\partial v}. \tag{4*}$$ Our next task is to then compute $\frac{\partial^2}{\partial r^2}$ and $\frac{\partial^2}{\partial \theta^2}$, which we can carefully do by multiplying the right-hand sides of (3*) and (4*), keeping in mind that the operators will act on the coefficients. However, executing this and adding the terms together yields nothing that looks remotely near the known form, namely $$\frac{\partial^2}{\partial u^2} + \frac{\partial^2}{\partial v^2}.$$  So I'm wondering, what am I missing here? In addition to this particular example, I'm interested in any general information you have about transforming partial derivatives from polar to Cartesian coordinates.","['multivariable-calculus', 'coordinate-systems', 'polar-coordinates', 'partial-differential-equations', 'differential-operators']"
120408,what are the applications of the isomorphic graphs?,"While studying data structures i was told my instructor that even i am given 3 hour/30 days/3 years to find out whether two graphs are isomorphic or not, it is  very very complex and even after spending this much amount of time i will not able to figure out clearly whether the given two graphs are isomorphic or not. It is NP-complete problem. So i got the curiosity that why i am studying it then. What could be the possible applications of such isomorphic graphs which can be solved ?","['np-complete', 'graph-theory', 'discrete-mathematics']"
120412,Verifying an inequality under certain conditions.,"Let $\mu$ be a positive measure on a measure space $\Omega$. How can I show that for all non-negative measurable $f$ $$\exp\left(\int_\Omega f \; d\mu \right) \leq \int_\Omega \exp(f) \; d\mu$$
the above inequality is valid if $\mu(\Omega) = 1$ and not valid if $\mu(\Omega) \neq 1.$ This is what I've been able to come up with. Since $\exp$ is convex, if $\mu(\Omega) =1$, then the inequality is just Jensen's inequality. Do I have to say more? How do I show that the inequality is not valid if $\mu(\Omega) \neq 1$.","['measure-theory', 'real-analysis']"
120424,An Exercise in Peter Petersen,"My question is an exercise in Peter Petersen ""Riemannian Geometry"" Chapter 5 #10 Let $N \subset M$ be a submanifold of Riemannian manifold $(M,g)$. (a) The distance from N to $x \in M$ is defined as $d(x,N) = \inf\{ d(x,y)\  |\ p \in N\}$.
    A unit speed curve $\sigma : [a,b] \to M$ with $\sigma(a) \in N,\sigma(a)$ and $l(\sigma) = d(x,N)$ is called a segment from $x$ to $N$. Show that $\sigma$ is also a segment from $N$ to any $\sigma(t),t<b$. Show that $\sigma'(a)$ is perpendicular to $N$. (b) Show that if $N$ is a closed subspace of $M$ and $(M,g)$ is complete, then any point in $M$ can be joined to $N$ by segments. (d) Show that $d(\dot \ ,N)$ is smooth on a neighborhood of $N$ and that the integral curves for its gradient are the geodesics that perpendicular to $N$. Please give me a answer as complete as possible,. Thank you very much!","['riemannian-geometry', 'differential-geometry']"
120429,Prove that $S_3$ and $S_4$ are solvable groups,"I wish to prove that $S_3$,$S_4$ (permutations on $3,4$ elements respectively) are solvable. I know that $D_6,D_{24}$ ($D_n$=Dihedral group of order $n$) are solvable and if I could prove that $S_3$ is isomorphic to $D_6$, $S_4$ is isomorphic to $D_{24}$ it will do the trick. With $S_3$ and $D_6$ I can do this 'the hard way' by defining the isomorphism, but with $S_4$ and $D_{24}$ proving that the function I define respects the multiplication of the group is too much work and doesn't seem like the 'right' way. Maybe it is sufficient to prove that S4 is solvable, since there is a $1-1$ homomorphism from $S_3 \to S_4$ ? I could use some help with this...","['symmetric-groups', 'finite-groups', 'group-theory', 'abstract-algebra']"
120430,Number of matrices with weakly increasing rows and columns,"I'm curious as to how many matrices there are of size $m \times n$ with elements of the set $\{1, \ldots , k\}$ such that each row and column is weakly increasing? The answer should be expressable as a determinant. I'm thinking that this could be solved by counting non-intersecting lattice paths somehow and using Lindström–Gessel–Viennot lemma, however I'm unsure of how to construct the matrix. Thanks!","['integer-lattices', 'determinant', 'combinatorics']"
120438,"Given the vertex angle and side lengths of an isosceles, find the base","I need to be able to do this programmatically, so I'll need to be able to convert an example into algebra, but for the sake of hopefully having it make more sense to me, let's say the two sides are 15 units long, and the angle of the vertex is 12 degrees. How would I go about determining the length of the base?","['geometry', 'triangles']"
120439,How to decide what is the probability distribution in a Monte-Carlo simulation?,"For a Monte-Carlo integration of $$\int_\Omega P(x)f(x)\ \text d x,$$ there seems to be no apriori distinction if $f$ or $P$ is the probability function. So does it matter if I consider $$P, f, P f, \text{ or 1 over all of } \Omega$$ to be the distribution function I draw the random points $x$ from, given that I use the complement function as object to plug these values in?","['integration', 'statistics', 'monte-carlo', 'numerical-methods', 'probability']"
120463,Prove the equation without solving for X,My niece asked me this - If $x=1/(5-x)$ prove $x^3 + \dfrac{1}{x^3}=110$ without solving for x. I said its not possible since solving for x itself gives me two roots for x (one being $\approx4.79$) and substituting for it in the second equation approx gives me 110. So proving algebraically without any assumptions is not possible. Is this right ?,['algebra-precalculus']
120469,Prove: $\int_0^{\frac{\pi}{2}}t(\frac{\sin nt}{\sin t})^4dt<\frac{\pi^2n^2}{4}$,"I have a question about integral. Prove: $$\int_0^{\frac{\pi}{2}}t\left(\dfrac{\sin(nt)}{\sin(t)}\right)^4dt<\dfrac{\pi^2n^2}{4}$$ I have tried several methods including $\sin(t)\geq\frac{2t}{\pi}$, but I can't work it out.",['calculus']
120476,Matrix over $\mathbb Z[X]$ which is not equivalent to a diagonal matrix,"How can I find $A=\begin{pmatrix} a & b \\ c & d \end{pmatrix}$ , where $a,b,c,d \in \mathbb Z[x]$ , such that there does not exist invertible matrices $B, C \in M_2(\mathbb Z[x])$ such that $B^{-1}AC$ is diagonal?","['matrices', 'ring-theory', 'linear-algebra']"
120489,"Given that $a+b\sqrt[3]{2} +c\sqrt[3]{4} =0$, where $a,b,c$ are integers. Show $a=b=c=0$","Given that $\displaystyle{a+b\sqrt[3]{2} +c\sqrt[3]{4} =0}$, where $a,b,c$ are integers. Show $a=b=c=0$ Do I use modular arithmetic?",['abstract-algebra']
120504,How do you pronounce (partial) derivatives?,"I am not an English speaker that is why I asked this question. In addition, I think english.stackexchange.com is not the proper place to ask this because (I am so sorry) I don't think most of them know mathematics deeply. How do you pronounce the following derivatives in English? $\frac{\textrm{d}y}{\textrm{d}x}$ $\frac{\textrm{d}^2y}{\textrm{d}x^2}$ $\frac{\partial y}{\partial x}$ $\frac{\partial^2 y}{\partial x^2}$ $\frac{\partial^3 y}{\partial x^2\partial z}$ Feel free to edit the tag if it has been chosen incorrectly. For example, is it correct if I pronounce $\frac{\textrm{d}y}{\textrm{d}x}$ as ""dee wai over dee eks""?","['notation', 'multivariable-calculus', 'pronunciation', 'derivatives']"
120536,How I can find the value of $abc$ using the given equations?,"If I have been given the value of 
$$\begin{align*}
a+b+c&= 1\\
a^2+b^2+c^2&=9\\
a^3+b^3+c^3 &= 1
\end{align*}$$ Using this I can get the value of 
$$ab+bc+ca$$ How i can find the value of $abc$ using the given equations? I just need a hint. I have tried by squaring the equations. But could not get it. Thanks in advance.",['algebra-precalculus']
120587,Proving that inversions are isometries with respect to the hyperbolic metric.,"I'd like to prove that the standard inversion $$(r,\theta)\mapsto\left(\frac{1}{r},\theta\right)$$ is an isometry with respect to the hyperbolic metric on the upper half-plane, and it would be nice to me to do the calculations using polar coordinates. First of all, I compute the length element in polar coordinates:
since $x=r\cos\theta$ and $y=r\sin\theta$ we get $dx=dr\cos\theta-r\sin\theta d\theta$ and $dy=dr\sin\theta+r\cos\theta d\theta$, so that $$\frac{dx^2+dy^2}{y^2}=\frac{dr^2+r^2d\theta^2}{r^2\sin^2\theta}.$$
Now, call $(u,v)=\left(1/r,\theta\right)$ and compute $du=-dr/r^2$ and $dv=d\theta$. Therefore, the new length element expressed in polar coordinate is 
$$\frac{du^2+dv^2}{v^2}=\left(\frac{dr^2}{r^4}+d\theta^2\right)\frac{1}{d\theta^2}=\frac{dr^2+r^4d\theta^2}{r^4d\theta^2},$$ which is not equal to the previous one. I guess that I'm wrong when I define $(u,v)$ directly in terms of polar coordinates, but I don't know how to fix it.
Could you help me with that? Thanks.","['multivariable-calculus', 'polar-coordinates', 'riemannian-geometry']"
120592,Three step proof of the divergence of $\sum_{p\in \mathbf{P}} \frac{1}{p}$,In my Number Theory skript it says: By showing that there are at most $(1+\frac{\log n}{\log 2})^{\pi(x)}$ numbers with $m\le n$ which are divisible only by prime numbers $p\le x$. By showing that there are at most $\sum_{x<p\le n} \frac{n}{p}$ numbers $m\le n$ which are divisible by at least one prime with $p\le x$. With 1. and 2. we can conclude that $\sum_{p\in \mathbf{P}} \frac{1}{p}$ is divergent by otherwise choosing x with $\sum_{p>x} \frac{1}{p} < \epsilon \le \frac{1}{2}$ and then $n\le (1+\frac{\log n}{\log 2})^{\pi (x)} + \epsilon n$ follows for all n. How can we show 1. and 2. ? And how does my professor conclude in 3?,['number-theory']
120608,Ring without distributive law?,"I recently came across a binary operation (in a very non-algebraic context - it's a way to organize a certain updating of log-likelihood-ratios) and was idly wondering whether it is any kind of reasonable algebraic object. The answer may well be no but it does satisfy some properties that look like those of a ring. Let $\boxplus \colon \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ be given by
$a \boxplus b = \log\left(\frac{1+e^{a+b}}{e^a+e^b}\right)$. It's a fun exercise to check the following properties: $a\boxplus b=b \boxplus a$ $a\boxplus b=0$ iff $a=0$ or $b=0$ $a \boxplus (-b) = (-a) \boxplus b= -(a \boxplus b)$ $a \boxplus \infty = a$, in the sense that $\displaystyle \lim_{x \to \infty} a\boxplus x=a$ $(a\boxplus b) \boxplus c=a \boxplus (b \boxplus c)$ So just looking at these properties I though maybe $(\mathbb{R},+,\boxplus)$ is a commutative ring without identity. But it doesn't satisfy the distributive law. Is there anything that can be said about such a structure? Edit: After a helpful comment from Bill I would like to point out which of these properties I find 'ringlike'. Property 3 is a statement that is true in a ring and doesn't make sense in just the semigroup $(\mathbb{R},\boxplus)$. Property 2 is the definition of an integral domain (if it was a ring). So it seems to me that having $-$ and $0$ puts me in the mind of connecting $\boxplus$ with $+$. But I don't know if this is necessary: is there some theory of semigroups with an extra unary negation operator? Maybe that's all I need.","['semigroups', 'abstract-algebra']"
120611,Are there any theories being developed which study structures with many operations and many distributive laws?,"""Algebraic structure"" will mean a set with some n-ary operations defined on it. This does not include vector spaces for example. During my study of algebra I have encountered mostly algebraic structures with one or two binary operations. I have not encountered any theory of structures with three or more. However, I do know examples of more operations being used in practice -- they just aren't included in the axioms, but defined in terms of the ""main"" operations. Those implicit operations together with the axiomatized ones may or may not be interconnected by distributive laws. It seems to me the most natural of possible laws connecting two binary operations since it is related to the concept of homomorphism. Say I have a structure $(S,\star,\circ).$ To say that $\circ$ left-distributes over $\star$ means that for any $x,y,z\in S$ $$x\circ (y\,\star\, z)=(x\circ y)\star (x\circ z).$$ This means no less that for any $x\in S$ the function $\phi_x : S\longrightarrow S$ defined by the formula $$\phi_x(s)=x\circ s$$ is an endomorphism of $(S,\star).$ I know axiomatized structures with two (or four if we want to count left  and right distributive properties separately) distributive laws between distinct operations, that is distributive lattices. This is the most a structure with two operations can have. However, I have noticed that there are interesting examples of structures with more (however ""implicit"") distributive laws. QUESTION Is there any, even obscure, theory being developed which studies algebraic structures with more than two binary operations interconnected by more than two distributive laws (counting the distribution of $\circ$ over $\star$ only once, even if both left and right distributive laws hold)? MOTIVATION I am asking this question because I find it curious that even though such structures may appear in nature, I have not encountered any study of them. I will give two examples that have occured to me. In both examples all laws will be two-sided. For ""$\circ$ distributes over $\star$"", I will write $$\circ\longrightarrow\star$$ A familiar example Let $\mathbb N$ denote the set $\{1,2,\ldots\}.$ Let $+$ and $\cdot$ have their usual meanings. We have $$\cdot\longrightarrow +$$ In $\mathbb N,$ we can define $\gcd$ and $\operatorname{lcm}$ to be the binary operations ascribing to a pair $(x,y)\in\mathbb N\times \mathbb N$ their positive greatest common divisor and positive least common multiple respectively. It is an exercise in elementary number theory to check that $$\begin{eqnarray}
\gcd\longrightarrow\operatorname{lcm}\\
\operatorname{lcm}\longrightarrow\gcd\\
\gcd\longrightarrow\gcd\\
\operatorname{lcm}\longrightarrow\operatorname{lcm}\\
\cdot\longrightarrow\gcd\\
\cdot\longrightarrow\operatorname{lcm}
\end{eqnarray}
$$ Together with $\cdot\longrightarrow +,$ this gives four binary operations and seven distributive laws. An exotic example Let $X$ be a set and $\mathscr B=2^{X\times X}$ be the set of all binary relations on $X.$ Since they are just elements of a power sets, binary relations can be unioned and intersected. Let's denote those operations in the standard way: $\cup$ and $\cap.$ It is known that $$\begin{eqnarray}
\cap\longrightarrow\cup\\
\cup\longrightarrow\cap
\end{eqnarray}
$$ For two binary relations $\rho,\sigma\in\mathscr B,$ we define $$
\rho\circ\sigma=\left\{(x,y)\in X\times X\,|\,(\exists z\in X)\; (x,z)\in \rho\wedge (z,y)\in\sigma\right\}.
$$ This is called the composition of binary relations. It is a well-known notion. Perhaps less known is the fact that $$\circ\longrightarrow \cup$$ I'll omit the easy proof. We can notice that there seems to be place for a ""dual"" operation. Let's define $$
\rho\star\sigma=\left\{(x,y)\in X\times X\,|\,(\forall z\in X)\; (x,z)\in \rho\vee (z,y)\in\sigma\right\}.
$$ I do not know the name of this operation, nor do I know if it has ever been studied. It is probably easy to believe that the complement function $$':\mathscr B\longrightarrow \mathscr B$$ provides an isomorphism $$(\mathscr B,\circ)\cong(\mathscr B,\star).$$ It may therefore be as easy to believe that $$\star\longrightarrow\cap$$ This gives us four binary operations on $\mathscr B$ interconnected by four distributive laws. (This seems to be an extremely strange structure. I do have a question about it, although I'm not sure if it's a good idea to ask it here, as it's probably as far from anybody's interest as can be.)","['soft-question', 'abstract-algebra']"
120612,Solving ODE with conditions,"I was reading some of my notes and I was not sure how the following works: $\frac{d^2y}{dx^2} + \frac{dy}{dx} = 0$ Solve the above with the condition $y(0)=0$ $\Rightarrow y(x) = A(1-e^{-x})$ with $A$ an arbitrary constant. I was just wondering how do you solve the above to get $y(x) = A(1-e^{-x})$? Because when I tried it, I got: $y=A + Be^{-x}$, and using the condition $y(0)=0$, I got $A+B =0 $.",['ordinary-differential-equations']
120630,Recurrence relations on a continuous domain,"While attempting to read Shannon's paper I came across the following (p. 3): suppose $N\colon \mathbb{R} \to \mathbb{R}$ is a function, which for some fixed (given) set of values $t_1, t_2, \dots, t_n$ satisfies
$$ N(t) = N(t-t_1) + N(t-t_2) + \dots + N(t-t_n). \quad \quad \quad (1)$$ Then, he says, ""according to a well-known result in finite differences"", $N(t)$ 
is asymptotic for large $t$ to $X_0^t$ where $X_0$ is the largest real solution of the characteristic equation
$$ X^{-t_1} + X^{-t_2} + \dots + X^{-t_n} = 1. \quad \quad \quad (2)$$ My question: what is this well-known theory, and where can I read about it? (If the proof is short enough to outline here in the space of a math.SE answer that would be great too, of course!) What sort of an equation is $(2)$, anyway? [Some fuzzy attempts follow.] I could not prove this, but I can non-rigorously find the result plausible: for instance, if we guess that $N(t)$ is of the form $X^t$ (or even $c X^t$) for some $X$, then plugging in $N(t) = c X^t$ into the equation $(1)$ gives 
$$ cX^{t} = cX^{t-t_1} + cX^{t-t_2} + \dots + cX^{t-t_n}$$
so dividing by $cX^t$ we get equation $(2)$,
$$ 1 = X^{-t_1} + X^{-t_2} + \dots + X^{-t_n}$$
And of course for any solution $X$ of the above equation and for any constant $c$, we can take $N(t) = cX^t$. Also linear combinations of solutions to $(1)$ are also solutions, so $N(t)$ could even be of the form 
$$N(t) = c_0X_0^t + c_1X_1^t + \dots$$
in which asymptotically only the largest solution $X_0$ matters. That is,
$$\lim_{t\to\infty}\frac{N(t)}{X_0^t} = \lim_{t\to\infty}\left(c_0 + \frac{c_1X_1^t}{X_0^t} + \dots \right)= c_0.$$ Now if were working with recurrence relations over the integers — $N\colon \mathbb{Z}\to\mathbb{R}$, say, with all the $t_i$ being integers, especially if they are integers $1$ to $n$ — then I guess we could further say that we have found an $n$-dimensional space of solutions (here $n$ being the number of solutions to equation $(2)$), and the solution space also is of dimension $n$ (it has $n$ ""degrees of freedom"" because the first $n$ values determine the sequence — all this needs to be made more rigorous and to consider more general $t_i$), and therefore these must be all the solutions. That would complete the ""proof"". But in this real-number case I'm completely clueless how one would prove a thing like this, and everything I wrote above may be entirely the wrong tack to pursue.","['recurrence-relations', 'functional-equations', 'reference-request', 'analysis', 'finite-differences']"
120636,An inductive construct of the Hausdorff reflection,"I have read that given any topological space $X$ you can construct a Hausdorff space $h(X)$ as a quotient of $X$ which is universal with respect to maps from $X$ to a Hausdorff space. This means there is a quotient map $q:X\to h(X)$ and if $f:X\to W$ is a map to a Hausdorff space, $W$, then there is a unique map $g:h(X)\to W$ such that $gq=f$. It seems you can construct the equivalence relation $\sim$ on $X$ so that $X/\sim$ by saying $x\sim y$ $\Leftrightarrow$ $f(x)=f(y)$ for every map $f:X\to W$ to Hausdorff $W$; this is essentially an application of a general adjoint theorem. I would like to know if it is possible to construct $h(X)$ using a transfinite process of taking quotients of $X$. It seems like the following should be a start: Let $X_0=X$. Say $x\sim_{0} y$ if $x$ and $y$ can't be separated by disjoint open sets in $X$. Let $\sim_{0}'$ be the transitive closure of this relation so you now have an equivalence relation. Now let $X_1=X_0/\sim_{0}'$. Since it seems $X_1$ is not necessarily Hausdorff (I am not even sure it is $T_1$) you can construct the equivalence relation $\sim_{1}'$ on $X_1$ and a quotient map $X\to X_2=X_{1}/\sim_{1}'$ by replacing $0,1$ with $1,2$. You should be able to continue this construction by transfinite induction to get a quotient $X_{\alpha}$ of $X$ for each ordinal $\alpha$. For instance, if $\alpha$ is a limit ordinal, define $x\sim_{\alpha}'y$ $\Leftrightarrow$ there is a $\lambda<\alpha$ such that $x\sim_{\lambda}'y$. Does this transfinite sequence of spaces stabilize to $h(X)$? If so, how does one show this? If this initial approach is off the mark, is there a more appropriate inductive approach which does stabilize to $h(X)$?","['general-topology', 'category-theory', 'separation-axioms']"
120639,"Convergence of $\int\int_{|x|\geq 1,|y|\geq 1} \frac{1}{|x|^\alpha+ |y|^\beta} \;dx\;dy$","For which values of $\alpha$ and $\beta$ does the following integral converge? \begin{equation}
\int\int_{|x|\geq 1, |y|\geq 1} \frac{1}{|x|^\alpha+ |y|^\beta}  \; dx \; dy, \quad \alpha,\beta>0.
\end{equation} Thank you!",['integration']
120640,Hadamard/Weierstrass product,"I want to construct an entire function with one zero at $0$, and zeros at negative natural numbers with degree $n$ (multiplicity $n$ at $z = -n$ for $n \geq 1$). Does $f(z) = z\displaystyle\prod_{n=1}^{\infty}(z+n)^{-n}$ work?",['complex-analysis']
120690,"What structures does ""geometry"" assume on the set under study?","The Wikipedia's article for geometry is somehow overwhelming. To make things clear, allow me to ask some questions: I wonder if ""geometry"" can be defined as the study of a metric
space (possibly with or without other structures)? Any thing more general than metric space (such as uniform spaces and topological spaces) is not in the scope of ""geometry""? Does ""geometry"" assume the set under study to have some algebraic
structure? Also there is algebraic geometries. Is the underlying set a topological vector space, normed space,
inner product space, or even Euclidean space? Since projective and affine spaces are pure algebraic concepts
without metrics, why are there ""projective geometry"" and ""affine geometry""? Thanks and regards!",['geometry']
120692,"Number of distinct $f(x_1,x_2,x_3,\ldots,x_n)$ under permutation of the input","$\alpha _n ^n-1=0$ $\alpha _n=e^{2 \pi i/n}$ $$f(x_1,x_2,x_3,\ldots,x_n)=(x_1+\alpha _n x_2+ \alpha _n ^2 x_3+\cdots+\alpha _n ^{n-1} x_n)^n$$ I have read in  Jim Brown's paper on page 5  that Lagrange showed If n=3  then
$f(x_1,x_2,x_3)$ Maximum can have 2 different results with  all permutations of $(x_1,x_2,x_3)$ If n=4  then
$f(x_1,x_2,x_3,x_4)$  Maximum can have 3 different results with all permutations of $(x_1,x_2,x_3,x_4)$ If n=5  then
$f(x_1,x_2,x_3,x_4,x_5)$ Maximum can have 6 different results with all permutations of $(x_1,x_2,x_3,x_4,x_5)$ but no proof how he did that result. According to the Paper, It was an important result for insolvability of  quintic via radicals. Thus I searched the paper of Lagrange (Lagrange's 1771 paper  reflections on the Algebraic theory of Equations ) in the internet but 
I could not find it. Jım Brown's paper does not mention the general solution for n. What is the  general formula of how many different values can have  $f(x_1,x_2,x_3,\ldots,x_n)$ with all permutations of $(x_1,x_2,x_3,\ldots,x_n)?$ Any idea to find the general formula for n? or if it is not possible for all n , at least to show a way how easily to proof for n=3,n=4 and n=5  (I tried to do that n=3 is relatively easy but need a lot calculation in classic approach as binomial expansion). Could you please help me how to approach the problem without using group theory? I need a proof in algebraic way. And also welcome all links that shows how Lagrange proved for n=3,n=4 and n=5. Note:I try to understand deeply how Abel and Ruffini showed the insolubility of  quintic via radicals. The problem is also related to my other question that shown that f is not symmetric function n>2.  and $f(x_1,x_2,x_3,\ldots,x_n)=f(x_n,x_1,x_2,\ldots,x_{n-1})=f(x_{n-1},x_n,x_1,\ldots,x_{n-2})=.....=f(x_2,x_3,x_4,\ldots,x_n,x_1)$ (totally $n$ permutation of f is equal each other)  it means at least n values are the same in total $n!$ all permutations of $(x_1,x_2,x_3,\ldots,x_n)$ . Thanks a lot for your answers and your advises. $UPDATE:$
I completed the Proof for $n=3$ 
I would like to share my way for $n=3$ All permutations for  $n=3$ are: $1)$-->$f(x_1,x_2,x_3)=(x_1+\alpha _3 x_2+ \alpha _3 ^2 x_3)^3$ $2)$-->$f(x_3,x_1,x_2)=(x_3+\alpha _3 x_1+ \alpha _3 ^2 x_2)^3=\alpha _3 ^3(x_3+\alpha _3 x_1+ \alpha _3 ^2 x_2)^3=(\alpha _3 x_3+\alpha _3 ^2 x_1+  x_2)^3$ $3)$-->$f(x_2,x_3,x_1)=(x_2+\alpha _3 x_3+ \alpha _3 ^2 x_1)^3=\alpha _3 ^3(x_2+\alpha _3 x_3+ \alpha _3 ^2 x_1)^3=(\alpha _3 x_2+ \alpha _3 ^2 x_3+x_1)^3$ $4)$-->$f(x_1,x_3,x_2)=(x_1+\alpha _3 x_3+ \alpha _3 ^2 x_2)^3$ $5)$-->$f(x_2,x_1,x_3)=(x_2+\alpha _3 x_1+ \alpha _3 ^2 x_3)^3=\alpha _3 ^3(x_2+\alpha _3 x_1+ \alpha _3 ^2 x_3)^3=(\alpha _3x_2+\alpha _3 ^2 x_1+ x_3)^3$ $6)$-->$f(x_3,x_2,x_1)=(x_3+\alpha _3 x_2+ \alpha _3 ^2 x_1)^3=\alpha _3 ^3 (x_3+\alpha _3 x_2+ \alpha _3 ^2 x_1)^3=(x_3\alpha _3+\alpha _3 ^2 x_2+  x_1)^3$ It can be easily seen that (Permutation 1 = Permutation 3) and (Permutation 2 = Permutation 3) Thus  (Permutation 1 = Permutation 2 =Permutation 3) (Permutation 4 = Permutation 6) and (Permutation 5 = Permutation 6) Thus  (Permutation 4 = Permutation 5 =Permutation 6) If so for n=3, the function can have 2 different result  {Permutation 1= $f(x_1,x_2,x_3)$ , Permutation 4 = $f(x_1,x_3,x_2)$) To test with inputs: $x_1=1, x_2=2 ,x_3=0$ we know very well that $1+\alpha _3+\alpha _3 ^2=0$ $\alpha _3=e^{2 \pi i/3}=-\frac{1}{2}+ i\frac{\sqrt{3}}{2}$ Permutation 1:
$f(x_1,x_2,x_3)=f(1,2,0)=(1+2\alpha _3)^3=1+6\alpha _3+12\alpha _3 ^2+8=-3-6\alpha _3=-3i\sqrt{3}$ Permutation 4:
$f(x_1,x_3,x_2)=f(1,0,2)=(1+2\alpha _3 ^2)^3=1+6\alpha _3^2+12\alpha _3 +8=3+6\alpha _3=3i\sqrt{3}$ As you see  in the example above .Permutation 1 and Permutation 4  cannot be the same always.","['symmetric-polynomials', 'functions', 'polynomials']"
120702,"Find the equation of an ellipse given its focus, directrix and eccentricity","Ellipse has a focus $(3;0)$, a directrix $x+y-1=0$ and an eccentricity of $1/2$.
Find its equation. I should probably use the fact that $r/d = e$, where $r$ is the distance from the focus to any point $M(x,y)$ of an ellipse. $d$ the distance from $M(x,y)$ to the directrix, and $e$ is the eccentricity. However my attempt failed.","['geometry', 'conic-sections']"
120704,How to prove a trigonometric identity $\tan(A)=\frac{\sin2A}{1+\cos 2A}$,"Show that
$$
\tan(A)=\frac{\sin2A}{1+\cos 2A}
$$ I've tried a few methods, and it stumped my teacher.",['trigonometry']
120708,How to determine the diagonalizability of these two matrices?,"I am trying to figure out how to determine the diagonalizability of the following two matrices. For the first matrix $$\left[\begin{matrix} 0 & 1 & 0\\0 & 0 & 1\\2 & -5 & 4\end{matrix}\right]$$ There are two distinct eigenvalues, $\lambda_1 = \lambda_2 = 1$ and $\lambda_3 = 2$. According to the theorem, If $A$ is an $n \times n$ matrix with $n$ distinct eigenvalues, then $A$ is diagonalizable. For the next one $3 \times 3$ matrix $$\left[\begin{matrix} -1 & 0 & 1\\3 & 0 & -3\\1 & 0 & -1\end{matrix}\right]$$ We also have two eigenvalues $\lambda_1 = \lambda_2 = 0$ and $\lambda_3 = -2$. For the first matrix, the algebraic multiplicity of the $\lambda_1$ is $2$ and the geometric multiplicity is $1$. So according to the theorem, this would not be diagonalizable since the geometric multiplicity is not equal to the algebraic multiplicity. For the second matrix, the algebraic multiplicity and the geometric multiplicity of both lambdas are equal, so this is diagonalizable according to my textbook. But there are still only two distinct eigenvalues in $3 \times 3$ matrix, so why is this diagonalizable if we are to accept the first theorem? Also, how to determine the geometric multiplicity of a matrix?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization']"
120711,What is the use of rationalizing the fraction? Can't the calculator still give an answer?,"Why do we need to rationalize the fraction, can't the calculator still give an answer?
And after that process, isn't the number still involved with an irrational number?",['algebra-precalculus']
120712,Proving equivalent form of the mean-value theorem for derivatives,"I'm trying to prove the following statement (from Apostol's Calculus I, p. 186, Exercise 6) Show that the mean-value formula can be expressed in the form
  $$f(x+h) = f(x) + hf'(x + \theta h) \qquad \text{where} \, 0 < \theta < 1. $$ As I'm not certain how standard the presentation of the mean-value formula is, Apostol gives it as
$$ f(b) - f(a) = f'(c)(b-a) $$
For $f$ continuous on $[a,b]$ and having a derivative at each point of the open interval $(a,b)$ where $c \in (a,b)$. This really does not strike me as a problem that should be too difficult, but for some reason I can't seem to make the right connections. First, I notice that $f(x+h) = f(x) + hf'(x + \theta h) \implies \frac{f(x+h) - f(x)}{h} = f'(x + \theta h)$ looks like we are getting something reminiscent of the derivative of $f$ at $x$ on the left.  It's not clear to me what I can or should do with that. The other thing that it occurs to me to do, is to write $x = \theta x + (1-\theta)x$ for $0 < \theta < 1$.  I'm not sure what to do with that precisely, but it seems like the sort of thing that one should do in this situation. I wish I had more of my own work to show, but I can't seem to make any legitimate progress.","['calculus', 'derivatives']"
120737,Finding infinite limit of hyperbolic trig functions,"I am trying to do random problems in my book and I do not know what to do for this one. I am suppose to find
the limit as x approaches infinity of $\tanh x$ I really do not know what to do I know the problem is $ \frac{(e^x - e^{-x})/2}{(e^x + e^{-x})/2}$ which I think can reduce to $ \frac{e^x - e^{-x}}{e^x + e^{-x}}$.",['calculus']
120747,Property of a sequence involving near-primes,"Let $p_k(m)^2:=$ the square of $m^{th}$ number containing k prime factors, including repetitions. Empirically for smallish numbers and as a conjecture, it appears that for every m and sufficiently large k, the formula $p_k(m)^2  = p_{2k}(n)$ returns the same value of n. Examples: $p_1(1)^2 = 4 = p_2(1)$, $p_2(1)^2 = 16 = p_4(1)$,... $p_1(5)^2 = 121 = p_2(40)$ $p_2(5)^2 = 196 = p_4(24)$ $p_3(5)^2 = 729 = p_6(23)$ $p_4(5)^2 = 2916 = p_8(23)$, ... For m = 1-7 the elements of the sequence are: 1, 3, 8, 12, 23, 26, 32,... The case m = 1 is easy enough. After that I'm not so sure. Thanks for any insights into this property. Edit: I think this generalizes a bit. For example, $p_i(a)p_j(b)...p_k(c) = p_{i+j+...+k}(d)$, in which i, j,..., k are increased by 1 stepwise so their difference remains constant, also returns a constant d. It also seems that the order of {a,b,...,c} does not change the value of d. For example, $p_1(2)p_2(3)p_4(5) = p_7(23)$ and using the conjecture, $p_3(2)p_4(3)p_6(5) = p_{13}(23)$ and if order unimportant$^*$, $p_3(5)p_4(2)p_6(3) = p_{13}(23)$ and again using the conjecture, $p_4(5)p_5(2)p_7(3) = p_{16}(23)$, verified computationally. And as an aside, again for ""sufficiently"" large subscripts, if we know that, e.g., $p_1(2)p_3(5)p_8(6) = p_{12}(41)$ we can augment subscripts pairwise to get $p_3(2)p_5(5)p_8(6) = p_{16}(41)$. $^*$ Changing the order of {a,b,...c} may change the size of subscripts needed for the property to obtain. The subscripts {1,2,4} and {2,3,5} do not work for the product in the last line, but {3,4,6} and higher appear to.","['prime-numbers', 'sequences-and-series', 'number-theory']"
120761,Which is the easiest way to prove $\sum \limits_{r=1}^n \binom{2n+1}{r} = 4^n-1$?,"Which is the easiest way to prove $\sum \limits_{r=1}^n
 \binom{2n+1}{r} = 4^n-1$? I encountered this problem while solving this question: A student is allowed to select at most $n$ books from a collection of
  $(2n+1)$ books. If the total number of books he can select is $63$,
  then what is the value of $n$? Evidently this boils to solving for $n$ in $\sum \limits_{r=1}^n
 \binom{2n+1}{r}  = 63$, I used Mathematica to derive that close form. I am wondering how could we get that without electronic aid? I would appreciate an algebraic or combinatorial approach.","['algebra-precalculus', 'combinatorics']"
120768,"Sufficient conditions to conclude that $\lim_{a \to 0^{+}} \int_{0}^{\infty} f(x) e^{-ax} \, dx = \int_{0}^{\infty} f(x) \, dx$","What are sufficient conditions to conclude that $$ \lim_{a \to 0^{+}} \int_{0}^{\infty} f(x) e^{-ax} \, dx = \int_{0}^{\infty} f(x) \, dx \ ?$$ For example, for $a>0$, $$ \int_{0}^{\infty} J_{0}(x) e^{-ax} \, dx = \frac{1}{\sqrt{1+a^{2}}} \, ,$$ where $J_{0}(x)$ is the Bessel function of the first kind of order zero. But I've seen it stated in a couple places without any justification that $$ \int_{0}^{\infty} J_{0}(x) \, dx = \lim_{a \to 0^{+}} \int_{0}^{\infty} J_{0}(x) e^{-ax} \, dx =  \lim_{a \to 0^{+}} \frac{1}{\sqrt{1+a^{2}}} =  1 .$$ EDIT : In user12014's answer, it is assumed that $ \int_{0}^{\infty} f(x) \, dx$ converges absolutely. But in the example above, $ \int_{0}^{\infty} J_{0}(x) \, dx$ does not converge absolutely. And there are other examples like $$ \int_{0}^{\infty} \frac{\sin x}{x} \, dx  = \lim_{a \to 0^{+}} \int_{0}^{\infty} \frac{\sin x}{x}e^{-ax} \, dx =  \lim_{a \to 0^{+}} \arctan \left(\frac{1}{a} \right) = \frac{\pi}{2} $$ and $$ \int_{0}^{\infty} \text{Ci}(x) \, dx = \lim_{a \to 0^{+}} \int_{0}^{\infty} \text{Ci}(x) e^{-ax} \, dx =  - \lim_{a \to 0^{+}} \frac{\log(1+a^{2})}{2a} =0 \, ,$$ where $\text{Ci}(x)$ is the cosine integral. SECOND EDIT : Combining Daniel Fischer's answer below with his answer to my follow-up question shows that if  $\int_{0}^{\infty} f(x) \, dx$ exists as an improper Riemann integral, then $$\lim_{a \to 0^{+}} \int_{0}^{\infty} f(x) e^{-ax} \, dx = \int_{0}^{\infty} f(x) \, dx.$$","['laplace-transform', 'integration', 'real-analysis', 'limits']"
120771,Nilpotent Matrix,"So I saw this problem: Is there an upper triangular matrix $A$ such that $A^n\neq 0$ but $A^{n+1}=0$? Prove or disprove. I said no, and my reasoning was that the matrix must have a zero diagonal since $(A^k)_{ii}=a_{ii}^k$. Then the matrix must be srtictly upper diagonal, and when we multiply it the diagonal of zeros starts ""moving up"" and eventually the matrix is zero and it follows that it must be in at most $n$ steps. Is there a neater way to do this? Like looking at characteristic polynomials or something? I was thinking that the minimal polynomial should be of the form $t^k$ and thus the characteristic polynomial ought to be $t^n$ as the minimal divides the characteristic, and by Cayley Hamilton we ought to have $A^n=0$, but I cannot see why the minimal polynomial must be $t^k$. If I was working over a complex (or alg closed field then yes since it could only have zero eigenvalues). Any thoughts? Thanks,","['matrices', 'linear-algebra']"
120782,Infinite product formula for a complex function,"Construct a function with zero at $z=0$ and zeros at $z=-n$ with multiplicities $n$. My answer is $$f(z) = z\prod_{n=1}^{\infty}\left[E_n\left(-\frac zn\right)\right]^n,$$
where $E_n(z)=(1-z)\exp\left(\sum_{k=1}^n\frac{z^k}k\right)$. Is that right? And does the product converge?",['complex-analysis']
120802,Algebraic Proof that $\sum\limits_{k=0}^m \binom{r}{k} \binom{m+n-r}{m-k} = \binom{m+n}{m}$,"How do I prove that: $$\sum\limits_{k=0}^m \binom{r}{k} \binom{m+n-r}{m-k} = \binom{m+n}{m} ~~~~~~~~~ (r <= m + n) $$ by using an algebraic identity? My Attempt: I know of the generating functions: 
$$ (1 + x)^n = \sum\limits_{k=0}^n \binom{r}{k} x^k ~~~~~  and ~~~~~~
 \frac {1}{(1 + x)^{n + 1}} = \sum\limits_{k=0}^{\infty} \binom{n + k}{k} x^k $$ and after a bit of trial and error I came up with the algebraic identity: $$(1 - x)^r  \frac {1}{(1 - x)^{n + r + 1}} = \frac {1}{(1 - x)^{n + 1}} ~~~~~~~~~~~~~~~~~ (a)$$ The right hand side of (a) clearly matches the 2nd generating function. So: $$ \frac {1}{(1 + x)^{n + 1}} = \sum\limits_{m=0}^{\infty} \binom{n + m}{m} x^m  $$ The left hand side of (a) can be expressed as product of summations: $$\begin{align*}
\displaystyle{ 
(1 - x)^r  \frac {1}{(1 - x)^{n + r + 1}} } 

&=\displaystyle{
\sum\limits_{m=0}^{\infty} \binom{r}{m} (-x)^m \sum\limits_{k=0}^{\infty} \binom{n + r + k}{k} x^k
} \\
&=\displaystyle{
\sum\limits_{m=0}^{\infty} \sum\limits_{k=0}^{m} \binom{r}{k} (-x)^k \binom{n + r + m - k}{m - k} x^{m-k} 
} \\
&=\displaystyle{
\sum\limits_{m=0}^{\infty} \sum\limits_{k=0}^{m} \binom{r}{k} \binom{n + r + m - k}{m - k} (-1)^k x^m
}
\end{align*}$$ But now I am stuck since I don't know how to make: $$ \binom{n + r + m - k}{m - k} (-1)^k ~~~~ equal ~~~~  \binom{m+n-r}{m-k} ~~~~~ ?????  $$ to complete the proof. Will my method work or should I be using another algebraic identity? I am quite new to combinatorics so please keep concepts understandable by a beginner. Thanks :) EDIT The answer It turns out that my equation wasn't taking me anywhere close to the answer - LOL! Here is the answer with the new algebraic identity: $$\begin{align*}

(1 + x)^r  (1 + x)^{m+n-r} &= (1 + x)^{m + n} \\


\sum\limits_{m=0}^r \binom{r}{m} x^m 
\sum\limits_{k=0}^{m + n-r} \binom{n+m-r}{k} x^k &=
\sum\limits_{m=0}^{n + m} \binom{n+m}{m} x^m
 \\

\sum\limits_{m=0}^{r + (m+n-r)} \sum\limits_{k=0}^m 
\binom{r}{m} \binom{n+m-r}{m-k} x^m &=
\sum\limits_{m=0}^{n + m} \binom{n+m}{m} x^m
 \\

\sum\limits_{m=0}^{m+n} \sum\limits_{k=0}^m 
\binom{r}{m} \binom{n+m-r}{m-k} x^m &=
\sum\limits_{m=0}^{n + m} \binom{n+m}{m} x^m
 \\


\end{align*}$$ Equating coefficients gives: $$\sum\limits_{k=0}^m \binom{r}{m} \binom{n+m-r}{m-k}  =
\binom{n+m}{m} ~~~~~~~~~~~~~~~~ QED!!! $$","['summation', 'binomial-coefficients', 'combinatorics']"
120810,Proof by Contradiction Problem Where do i start,"Prove the following: There are no rational number solutions to the equation
$x^3 +x+ 1$ = 0, i.e. no solution can be written as a ratio a/b where a, b ∈ N (you
can always consider a/b to be reduced to lowest terms). Hint: start your proof
as you would start a proof by contradiction, then multiply by $b^3$ to get rid of
the denominators. Then consider a case analysis of a and b based on even and
odd. Not sure where to start on this proof by contradiction do i work backwards? Any help would be awesome! Thanks!","['algebra-precalculus', 'proof-writing', 'discrete-mathematics']"
120846,Dilogarithm asymptotics for an exponential parameter.,"So this question is about this dilogarithm function. Assume the argument $z$ is real then I want to show the formula
$$\operatorname{Li}_2(e^{-z})=\frac{\pi^2}{6} + z\log z -z+O(z^2) $$ as $z$ approaches $0$ from positive value How can I show this?","['asymptotics', 'special-functions', 'analysis']"
120861,Homomorphism between cyclic groups,"I have some confusion in relation to the following question. Let $\langle x \rangle = G$, $\langle y \rangle = H$ be finite cyclic groups of order $n$ and $m$ respectively.
  Let $f : G \mapsto H$ be the mapping sending $x^i$ to $y^i$.
  Determine the conditions on $n$ and $m$ so that $f$ is going to be a homomorphism. In verifying the definition of a homomorphism I seem to be able to conclude that there is no restriction on $n$ and $m$ since the definition of a homomorphic mapping is satisfied: $$ f(x^i x^j) = f(x^{i+j}) = y^{i+j} = y^i y^j = f(x^i) f(x^j)  \tag{1} $$ But assuming $n < m$ we would get $$f(x^n) = f(e) = e \not = y^n$$ So clearly we have to have $m \mid n.$ My questions are Are there any other conditions on $n$ and $m$ besides $m \mid n.$ What exactly did I do wrong in $(1)$?","['cyclic-groups', 'finite-groups', 'group-theory']"
120867,Limit of sequence $x_n^n$,"Let $x_1=2$, $x_{n+1}=\sqrt{x_n+\frac{1}{n}}$ for all $n\geq 1$. Prove that $\lim\limits_{n\to\infty}x_n=1$ and evaluate $\lim\limits_{n\to\infty}x_n^n$.","['sequences-and-series', 'calculus', 'limits']"
120872,Group theoretical characterization of diagonal matrices,"Let $k$ be a field. Is there a group-theoretical characterization of the subgroup $D_n$ of diagonal matrices in $GL_n(k)$ ? For example, if $k = \mathbb{C}\;$ then $D_n$ is a maximal torus, but, of course, there are many of them.","['matrices', 'group-theory']"
120910,Trigonometry Min/Max Problem,"$f(x) = 2\sin x \hspace{10pt}(0 \leq x \leq \pi)$ $g(x) = -\sin x \hspace{10pt}(0 \leq x \leq \pi)$ Rectangle ABCD is enclosed between the above functions' graphs (its edges are parallel to the axes). How would I go about finding the maximum perimeter of ABCD? I'm really clueless about this, I don't even know how to begin. How am I supposed to represent the edges? Thanks","['trigonometry', 'calculus']"
120916,Map preserving indefinite scalar product must be linear,"Let $V$ be a finite dimensional real vector space and $\langle\cdot,\cdot \rangle$ be a positive definite scalar product in $V$. It is well know that if a map $T:V \to V$ preserves $\langle\cdot,\cdot \rangle$ (i.e. $\langle Tv,Tw \rangle = \langle v,w \rangle$ for all $v,w \in V$) then $T$ must be linear. One way of seing this is by computing $||x+y-z||^2 = ||x||^2 + ||y||^2 + ||z||^2 + 2\langle x,y\rangle - 2\langle x,z\rangle - 2\langle y,z\rangle$
$||Tx+Ty-Tz||^2 = ||Tx||^2 + ||Ty||^2 + ||Tz||^2 + 2\langle Tx,Ty\rangle - 2\langle Tx,Tz\rangle - 2\langle Ty,Tz\rangle$. Comparing both sides we get $||Tx+Ty-Tz|| = ||x+y-z||$ for all $x,y,z \in V$ and setting $z=x+y$ we obtain $T(x+y)=Tx + Ty$. By continuity its not hard to show that $T(\lambda x) = \lambda Tx$ for $\lambda \in \mathbb R, x \in V$. Now let $g:V \times V \to \mathbb R$ be an indefinite scalar product (that is, a non-degenrate symmetric bilinear form) and $T:V \to V$  a map preserving $g$, i.e., $g (Tv,Tw)  = g(v,w)$ for all $v,w \in V$. Is it true that $T$ must be linear in this case? The above computation no longer aplies, since vectors with zero norm need not to be zero anymore. The only thing one can say is that $T(x+y)-Tx - Ty$ is light-like for every $x,y \in V$. I think I managed to write a proof using the fact that $T$ is $C^1$ and surjective. Does anyone know if these conditions are necessary? I wasn't able to find any counter example.","['quadratic-forms', 'linear-algebra', 'inner-products']"
120917,Where am I going wrong when removing brackets from (s+1)(s+5)(s-3),"One of my text book exercises is to remove the brackets from this expression: $ (s+1)(s+5)(s-3) $ The I've tried a number of times and the result I keep getting is: $ s^3 -3s^2 -13s -15 $ However, my textbook says the answer is: $ s^2 -3s^2 -13s -15 $ I keep getting the same answer, but I'm reluctant to think the text book is wrong. Here's how I come to that result: $ (s+1)(s+5)(s-3)$ $ = s((s+1)(s+5)) - 3((s+1)(s+5))$ $ = s(s(s+1) + 5(s+1)) - 3(s(s+1) + 5(s+1))$ $ = s(s^2 + s + 5s + 5) - 3(s^2 + s + 5s + 5)$ $ = s^3 + s^2 + 5s^2 + 5s - 3s^2 -3s -15s -15$ Then re-arrange that to: $ = s^3 +6s^2 - 3s^2  + 5s - 3s -15s -15$ And my final answer (Updated to reflect jorikis answer below): $ s^3 +3s^2 -13s -15 $ Any advice greatly appreciated.",['algebra-precalculus']
120921,Why does cubic equation have 3 roots but not 9?,"In here: http://en.wikipedia.org/wiki/Cubic_function#Cardano.27s_method Cardano's method says that for $ax^3+bx^2+cx+d=0$ , $x=y-\dfrac{b}{3a}$ , and $y=u+v$ . We have found the values of $u^3$ and $v^3$ , but both $u^3$ and $v^3$ should have $3$ roots respectively (including complex ones), so $u^3+v^3$ should have $3\times3=9$ roots?! So why are there only $3$ roots for $u+v$ ?? Edit: How do we prove that there are only 3 sets of values of $u$ and $v$ that satisfy $y=u+v$ and $3uv+p=0$ ?",['algebra-precalculus']
120941,Simple Least Squares Regression?,"I have a vector X of 50 real numbers and a vector Y of 50 real numbers. I want to model them as y = ax + b How do I determine a and b such that it minimizes the square of the error to this training set? That is given X = (x1,x2,...,x50)
Y = (y1,y2,...,y50) What is the closed form for a = ???
b = ??? See also: https://codereview.stackexchange.com/questions/10122/c-correlation-leastsquarescoefs","['statistics', 'regression', 'linear-algebra', 'machine-learning']"
120953,Integral of an absolute value function,How do I find the definite integral of an absolute value function? For instance: $f(x) = |-2x^3 + 24x|$ from $x=1$ to $x=4$,"['absolute-value', 'calculus', 'integration']"
120961,What is the meaning of $\vec A \cdot \nabla$?,"Looking at the application of divergence in Cartesian coordinates in Wikipedia I was wondering about the meaning of $\vec A \cdot \nabla$? This dot product is found at the vector cross product identity:
$\nabla \times (\mathbf{A} \times \mathbf{B}) = \mathbf{A} (\nabla \cdot \mathbf{B}) - \mathbf{B} (\nabla \cdot \mathbf{A}) + (\mathbf{B} \cdot \nabla) \mathbf{A} - (\mathbf{A} \cdot \nabla) \mathbf{B}$",['multivariable-calculus']
120969,Ideals in Commutative Banach Algebras,"Suppose that $A$ is a natural Banach function algebra on $K$, a compact Hausdorff space.  So $A$ is realised as an algebra of continuous functions on $K$, is a Banach algebra for some norm (necessarily dominating the supremum norm) and each character on $A$ is given by evaluation at a point of $K$. If $F\subseteq K$ is closed, then $$ I(F)=\{f\in A : f(k)=0 \ (k\in F) \}$$ is a closed ideal in $A$.  If e.g. $A=C(K)$ then every closed ideal is of this form. What's a simple example of an $A$ where not every closed ideal is of this form? If I look in Bonsall+Duncan, I find that the Disc Algebra is an example.  But quite a bit of theory is needed to show this.  I'd like an easy example which I can explain to students.  For bonus marks: Can we find an $A$ which is conjugate closed?","['banach-algebras', 'functional-analysis']"
120970,Bayesian inference on partitioned multivariate Gaussian,"My question is on Bayesian inference of partitioned multivariate Gaussian. To make things easier, suppose there is a 2-dimensional Guassian, $$
X_1 \sim N(\mu_1, \sigma^2_1) \\
X_2 \sim N(\mu_2, \sigma^2_2)
$$ with covariance $\sigma_{1,2}$. Suppose we know $\sigma_{i,j}$, $\sigma^2_1$ and $\sigma^2_2$; don't know $\mu_1$, $\mu_2$ but have priors for them as, $$
\mu_1 \sim N(\theta_1, \delta^2_1) \\
\mu_2 \sim N(\theta_2, \delta^2_2)
$$ Now we have an observation $x_1$ for $X_1$. By Bayesian inference we get, $$
\theta'_1 | x_1 = \frac{\delta^2_1 x_1 + \sigma^2_1 \theta_1}{\delta^2_1 + \sigma^2_1} \\
\delta'^2_1 | x_1 = \frac{\delta^2_1 \sigma^2_1}{\delta^2_1 + \sigma^2_1}
$$ and by partitioned Gaussian we have, $$
X_2 | X_1 \sim N \left(\mu_2 + \frac{\sigma_{1,2}}{\sigma^2_1}(x_1 - \mu_1), \sigma^2_2 - \frac{\sigma^2_{1, 2}}{\sigma^2_1} \right)
$$ Finally my question is, how to update the correlated r.v using Bayesian inference,
$$
p(\mu_2 | x_1) = \frac{p(x_1 | \mu_2) p(\mu_2)}{p(x_1)}
$$
since I don't know how to deal with $p(x_1 | \mu_2)$. Or maybe there's other ways around to get it? Hope you get the idea of what I'm trying to do. Thanks!","['probability-theory', 'probability-distributions', 'probability']"
120972,"Is the ""limit function"" a continuous function?",Let $f:\mathbb{R}\to\mathbb{R}$ be a function such that for all $x_0\in\mathbb{R}$ we have $\lim\limits_{x\to x_0}f(x)=g(x_0)\in \mathbb{R}$. Is $g$ a continuous function?,"['calculus', 'functions']"
120981,Evaluating $\int_{0}^{1} \sqrt{1+x^2} \text{ d}x$,I'm learning integral. Here is my homework: $$\int_0^1 \sqrt{1+x^2}\;dx$$ I think this problem solve by change $x$ to other variable. Can you tell me how please. (just direction how to solve) thanks :),"['definite-integrals', 'calculus', 'integration']"
121020,"How does one describe the terms of the sequence defined by $a_{1}=1, a_{2}=2,a_{n+1}=a_{n}-a_{n-1},$ if $n\gt 2?$","I am self-learning Discrete Mathematics and I am supposed to solve the following Exercise. (Translated from Portuguese). The sequence $(a_{n})$ is defined by $a_{1}=1, a_{2}=2,a_{n+1}=a_{n}-a_{n-1},$ if $n\gt 2$. Prove that $a_{n+6}=a_{n}$ for all natural numbers $n.$ Describe all terms of this sequence. I think I didn't understand the last sentence, because I thought that $a_{n+6}=a_{n}$ is the description of all terms of this sequence. But as you can see, I am wrong! Here is the sequence $(1,2,1,-1,-2,-1,1,2,1,-1,-2,-1,\cdots).$  Should I find a formula for the terms of the sequence?","['induction', 'discrete-mathematics']"
121029,Calculating a Residue at one of the poles of $\frac{1}{1+z^n}$,"Is there an easy way to calculate the residue at one of the poles of a rational expression of the form $\frac{1}{1+z^n}$?  I end up having to add a bunch of polar-form complex numbers in the denominator which I have no idea how to do except to convert them to rectangular coordinates, which becomes really messy.  I feel like there ought to be simple pattern/rule for such a canonical expression.",['complex-analysis']
121035,"Same eigenvalues, different eigenvectors","I'm interested in the case of a specific matrix having different eigenvectors corresponding to two identical eigenvalues. The method I use for spectral decomposition returns different eigenvectors, even though the eigenvalue is the same. Is this possible, and if so, what this tells about the matrix?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
121036,"Find $y$ which satisfies: $y'=y^a$, $y(a)=a-2$, for $a \in \mathbb{N}$","I'd love your help with finding the function $y$ which satisfies: $y'=y^a$, $y(a)=a-2$, for $a \in \mathbb{N}$ This is what I did: $$\begin{align*}
\int \frac{y'}{y^a}dx&=\int 1dx\\
\frac{y^{-a+1}}{-a+1}+c_1&=x+c_2\\
y^{-a+1}&=(x+C)(-a+1),
\end{align*}$$
where $C=c_2-c_1$, and for $a=1$ there's no solution. So I get 
$$y=\left(\frac{1}{(x+c)(1-a)}\right)^{a-1},$$ 
finding $c$ is not pleasant. I assume that something is wrong, Am I suppose leave $y$ in the way that I find it after finding $c$? Thanks a lot!",['ordinary-differential-equations']
121052,Convergence/Divergence of series with terms $2^{n}\left ( \frac{n}{n+1} \right )^{n^{2}} $ and $\sin (n)\sin \frac{x}{n}$,"Help me please with these 2 questions: 1.Does it converge or diverge? :
$$ \sum_{n=2}^{\infty }2^{n}\left ( \frac{n}{n+1} \right )^{n^{2}} $$ 2.Check out absolute and conditional convergence of:  $x>0 $ $$ \sum_{n=1}^{\infty }\sin (n)\sin \frac{x}{n} $$ Thanks a lot!","['sequences-and-series', 'calculus']"
121063,"How does one find a formula for the recurrence relation $a_{1}=1,a_{2}=3, a_{n+2}=a_{n+1}+a_{n}?$","I am self-studying Discrete Mathematics, and I have two exercises to solve. Find a formula for the following recurrence relation: (translated from Portuguese) a) $a_{1}=3,a_{2}=5, a_{n+1}=3a_{n}-2a_{n-1}.$ b) $a_{1}=1,a_{2}=3, a_{n+2}=a_{n+1}+a_{n}.$ Let me show how I solved the first one. Note that $a_{1}=3,a_{2}=5, a_{3}=9,a_{4}=17$, then $a_{n}=2^{n}+1$ for all $n\in \mathbb{N}$ (I proved it by induction on $n$), but the last one I was not able to solve. I would appreciate your help.","['recurrence-relations', 'discrete-mathematics']"
121075,Showing $\sum_{n=-\infty}^\infty \frac{1}{(z+n)^2}=\frac{\pi^2}{\sin^2(\pi z)}$,"I'm doing a homework problem, and so far I've proved
$$\sum_{n=-\infty}^\infty \frac{1}{(z+n)^k}=\frac{(-2\pi i)^k}{(k-1)!}\sum_{m=1}^\infty m^{k-1}e^{2\pi imz}$$
for $k$ an integer $\geq 2$ and $\text{Im}(z)>0$. The next part of the problem asks me to put $k=2$ and show 
$$\sum_{n=-\infty}^\infty \frac{1}{(z+n)^2}=\frac{\pi^2}{\sin^2(\pi z)}$$ for $\text{Im}(z)>0$, but after nearly an hour of bashing I still see it-- I've tried product expansions of sine, using Euler's formula (which equates to showing $\frac{e^{2\pi ir}+e^{-2\pi ir}-2}{16}=-4\pi^2 \sum_{m=1}^\infty me^{2\pi imz}$). Could anybody point out a way to get the above equality from the one I derived? Apologies in advance if I'm just ditzy and it's really obvious. Also, the next question asks if the above formula is true if $z$ is any complex number that is not an integer, but I'm not really sure I understand what it's asking. Why wouldn't it be true?","['fourier-analysis', 'complex-analysis']"
121081,Neither Artinian nor Noetherian rings,"I read a message about the ring of complex entire functions, that is neither Artinian nor Noetherian (see here ). Can you show me other examples of rings that are neither Artinian nor Noetherian?","['ring-theory', 'abstract-algebra']"
121088,Eigenvector of magic square,"I'm trying to show: A ""magic square"" $A$ is a matrix $n\times n$ with slots $1,2,\cdots, n^2$ such that the sum of  the elements of each row (and column) is the same . Prove that $\frac{n(n^2+1)}{2}$ is a eigenvalue of the matrix $A$. I was trying to make a proof with a proposition: ""$\beta$ is  a eigenvalue of $A$ if and only if $\det(A-\beta I_n)=0$"", I is the matrix idetity $n\times n$. But I can not do it. Thanks for your help.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors', 'magic-square']"
121099,Limit using Poisson distribution [duplicate],This question already has answers here : Evaluating $\lim\limits_{n\to\infty} e^{-n} \sum\limits_{k=0}^{n} \frac{n^k}{k!}$ (9 answers) Closed 10 years ago . Show using the Poisson distribution that $$\lim_{n \to +\infty} e^{-n} \sum_{k=1}^{n}\frac{n^k}{k!} = \frac {1}{2}$$,"['probability-distributions', 'real-analysis']"
121100,"Does the total variation of a function bound its numerical integration error, much like its first derivative?","When estimating the convergence of a Riemann sum to its integral, or equivalently the error in numerical integration, the commonly used bound is by upper bounding it's first derivative (see, for example the section on wikipedia). Specifically, the inequality $\left| \int_a^b f(x)\,dx - (b - a) f(a) \right| \leq {(b - a)^2 \over 2} \sup_{a \leq x \leq b} \left| f'(x) \right|$ implies that the error $E_{n}$ of an $n$ term Riemann sum is bounded by $E_n \leq \frac{1}{2n}\sup_{0 \leq x \leq 1} \left| f'(x) \right|$ However, this is seemingly poor if the maximum of the first derivative is very peaked.  A smooth step function, for example, would have Riemann sum converging faster than the bound suggests: consider the separately the subsets of the domain where the first derivative is small and large, and add the errors together.  In the former subset you may use a smaller maximum for the derivative. My question is about improving this to use the total variation of the function $f$ instead of the derivative.  Intuitively if the function has large derivative --- but only briefly, like the step function --- then this should be evident from its total variation.  Is such a bound possible?","['bounded-variation', 'integration', 'numerical-methods']"
121105,Which functor does the projective space represent?,"I hope this question isn't too silly. It is certainly fundamental, so the answer is likely contained at least implicitly in most sources out there, but I haven't seen it done this way (that is, in this particular functorial manner) in a way which is overt enough for me to catch on. I am familiar with the classical $Proj$-construction for a graded ring, so that's not quite what I'm looking for. Let $k$ be a ring. Let's call a covariant functor of sets on some category of $k$-algebras an algebraic functor (over $k$). The affine $I$-space over $k$ is the algebraic functor $\mathbb{A}^I:(k−alg)→(set)$ which takes a $k$-algebra $R$ to the set $\mathbb{R}^I$ of $I$-tuples of elements of $R$. This functor is (co)representable by the ring $k[T_i],i∈I$, so $\mathbb{A}^I$ is (represented by) an affine scheme. I want the projective space over $k$ in terms of an algebraic functor over $k$. I'm thinking something like $R↦\{\mathbb{R}^{I+∞}/\mathbb{G}_m(R)\}$ (where $\mathbb{G}_m(R)$ is the multiplicative group of $R$), or as a functor sending $R$ to some set of modules of rank $1$. One should then be able to show that it has a cover by four copies of the affine $I$-space over $k$. Alternatively, it would likely make sense to consider it a functor on the category of graded $k$-algebras.",['algebraic-geometry']
121116,Structure Sheaf on Scheme,"Given a scheme $X$, its structure sheaf on the elements of the cover by affines is pretty easy to define, say $\mathcal{O}_X(Spec(A))=A$.  But how is it then defined on the union of two of these sets? In general, it is clear how to define open subfunctors of affines, and the definition of an open subfunctor of $X$ is completely understandable, but what does the ""union"" of two open subfunctors look like? Thanks!",['algebraic-geometry']
121122,Why do direct limits preserve exactness?,"I've heard that taking direct limits is an exact functor in the category of modules, and I'm trying to figure out why, as I couldn't find a proof. Suppose you have homomorphisms $\varphi_i: K_i\to N_i$ and $\psi_i: N_i\to M_i$ for $(K_i,h^i_j)$ , $(N_i,g^i_j)$ , and $(M_i,f^i_j)$ directed systems of modules such that $0\to K_i\to N_i\to M_i\to 0$ is exact for every $i$ . Why is $0\to\varinjlim K_i\to\varinjlim N_i\to\varinjlim M_i\to 0$ also exact? So I let $\varphi:\varinjlim K_i\to\varinjlim N_i$ and $\psi:\varinjlim N_i\to\varinjlim M_i$ be the natural homomorphisms. Take $x\in\ker\psi$ . Then $x=g^i(x_i)$ for some $x_i\in N_i$ . Then $0=\psi(g^i(x_i))=f^i(\psi_i(x_i))$ . I know there exists some $j\geq i$ such that $f^i_j(\psi_i(x_i))=0$ in $M_j$ . But $f^i_j\circ\psi_i=\psi_j\circ g^i_j$ , so $g^i_j(x_i)\in\ker\psi_j=\text{im}(\varphi_j)$ . Then $g^i_j(x_i)=\varphi_j(y_j)$ for some $y_j\in K_j$ , so $$
x=g^i(x_i)=g^j(g^i_j(x_i))=g^j(\varphi_j(y_j))=\varphi(h^j(y_j))
$$ and so $\ker\psi\subseteq\text{im}\varphi$ . Conversely, suppose $x\in\text{im}\varphi$ . Then $x=\varphi(y)$ for some $y=h^i(y_i)$ and $y_i\in K_i$ . So $x=\varphi(h^i(y_i))=g^i(\varphi_i(y_i))$ . Thus $$
\psi(x)=\psi(g^i(\varphi_i(y_i)))=f^i(\psi_i(\varphi_i(y_i)))=0
$$ since $\psi_i\circ\varphi_i=0$ . Then $\ker\psi=\text{im}\varphi$ . (Please let me know if I've written nonsense, too many maps can cause me to get lost!) What is bugging me is, is $\varphi$ injective and $\psi$ surjective to see that the short exact sequence is in fact exact? Is there some obvious fact I'm missing? If possible, is there an explanation in the same vein as the above (i.e. using the maps and manipulating the elements without relying on more general facts from category theory? I'm not too knowledgeable about the latter.) Thanks.","['modules', 'abstract-algebra']"
121139,Derivative is a constant,"A function $f:U\subset\mathbb{R}^n \rightarrow \mathbb{R}^m$, $U$ open, is differentiable in $p \in U$ if there exists a linear transformation $T:\mathbb{R}^n \rightarrow \mathbb{R}^m$ such that $f(p+v)=f(p)+T(v)+R(v)$, where $R(v)$ satisfies $lim _{v\rightarrow 0}\dfrac{R(v)}{|v|}=0$, for all $v\in\mathbb{R}^n$ with $p+v\in U$. That said, if $f$ as above is differentiable and $f'(x)=T$, $\forall x\in\mathbb{R}^n$. I need to show that there is an $a\in\mathbb{R}^n$ such that $f(x)=Tx+a$. The problem I'm having is, how do I show that $R(v)=0$ for all $v$?",['analysis']
121167,Is $\mathbb{Q^+}$ countably infinite?,"Is $\mathbb{Q^+}$ countably infinite? I am trying to construct a bijection $\mathbb{Z^+}\rightarrow \mathbb{Q^+}$, i think it is obvious to find a injection from $\mathbb{Z^+}\rightarrow \mathbb{Q^+}$, but i cannot prove for that injection, there exist a surjection between $\mathbb{Q}$ and $\mathbb{Z}$. Is there any way to prove it ? Edited: What about is there a bijection between $\mathbb{Z^+}\times \mathbb{Z^+}\rightarrow \mathbb{Q^+}$",['elementary-set-theory']
121170,Traveling between integers- powers of 2,"Moderator Note: At the time that this question was posted, it was from an ongoing contest. The relevant deadline has now passed. Consider the integers. We can only travel directly between two integers with a difference whose absolute value is a power of 2 and every time we do this it is called a step. The distance $d$ between two integers is the minimum number of steps required to get from one to the other. Note however that we can travel backwards. For instance $d(2,17)$ is 2: $2+16=18 \rightarrow 18-1=17$. How can we prove that for any integer n, we will always have some $d(a,b)=n$ where$b>a$? If we are only able to take forward steps I know that  the number of 1s in the binary representation of $b-a$ would be $d(a,b)$. However, we are able to take steps leftward on the number line...",['number-theory']
121175,Is it possible to simplify these iterated functions?,"I'll apologise in advance, as I'm a programmer and my math is a bit rusty, so please bear with me. Let's say I have a linear function: $$f(x) = mx + c$$ But the result of the function is clamped between $0$ and $1$. I'm not sure of the mathematical term for this, but basically it's setting a minimum and maximum on the result. If it's less than $0$ it becomes $0$, and if it's greater than $1$ it becomes $1$. $$f(x) = clamp(mx + c)$$ I have a certain number ($n$) of these functions, each with different values for $m$ and $c$, and I apply them in order like so: $$ f_n(f_\ldots(f_2(f_1(x)))) $$ Is it possible to simplify these $n$ functions into a single function that will give the same result? For instance (this is obviously wrong, but it gives you an idea of what I'm after): $$ g(x) = clamp \left( \left( \sum m \right)x + \sum c \right) $$ If it's not possible due to the clamping, would it be possible if each function was just a simple linear function?",['functions']
121203,Find the number of values of k?,"Find the number of values of k  for which the expression $$y=  (x^2 -  x(k-2) + k^2)(x^2 +  kx + (2k-1))$$ is a perfect square . I tried by assuming the product as $$ t^2 $$
And then assuming two separate equations as t Is this the right approach. How i can achieve this? Thanks in advance.",['algebra-precalculus']
121225,"If $B\subset A$ and the exist a injection $A\rightarrow B$, then $A$ and $B$ has same cardinality?","By using the definition, the cardinality is the same iff there is a bijection. So if $B\subset A$ and the exist a injection $f\colon A\rightarrow B$, then $A$ and $B$ has same cardinality.",['elementary-set-theory']
121236,"If $A$ is a subset of $B$, then the closure of $A$ is contained in the closure of $B$.","I'm trying to prove something here which isn't necessarily hard, but I believe it to be somewhat tricky.  I've looked online for the proofs, but some of them don't seem 'strong' enough for me or that convincing.  For example, they use the argument that since $A\subset \overline{B} $ , then $ \overline{A} \subset \overline{B} $ .  That, or they use slightly altered definitions.  These are the definitions that I'm using: Definition #1: The closure of $A$ is defined as the intersection of all closed sets containing A. Definition #2: We say that a point x is a limit point of $A$ if every neighborhood of $x$ intersects $A$ in some point other than $x$ itself. Theorem 1: $ \overline{A} = A \cup A' $ , where $A'$ = the set of all limit points of $A$ . Theorem 2: A point $x \in \overline{A} $ iff every neighborhood of $x$ intersects $A$ . Prove: If $ A \subset B,$ then $ \overline{A} \subset \overline{B} $ Proof : Let $ \overline{B} = \bigcap F $ where each $F$ is a closed set containing $B$ .  By hypothesis, $ A \subset B $ ; hence, it follows that for each $F \in \overline{B} $ , $ A \subset F \subset \overline{B} $ .  Now that we have proven that $ A \subset \overline{B} $ , we show $A'$ is also contained in $\overline{B} $ . Let $ x \in A' $ .  By definition, every neighborhood of x intersects A at some point other than $x$ itself.  Since $ A \subset B $ , every neighborhood of $x$ also intersects $B$ at some other point other than $x$ itself.  Then, $ x \in B \subset \overline{B} $ . Hence, $ A \cup A' \subset \overline{B}$ .  But, $ A \cup A' = \overline{A}$ .  Hence, $ \overline{A} \subset \overline{B}.$ Is this proof correct? Be brutally honest, please.  Critique as much as possible.",['general-topology']
121255,Factor groups and isomorphisms,"I've somewhat recently been going back through one of my brother's old textbooks reviewing group theory.  I'm up to a chapter called Factor-Group Computations and Simple Groups.  The problems at the end seem to have me stumped and I want to make sure I'm understanding enough before I proceed. There are a dozen problems asking to classify a given group according to the fundamental theorem of finitely generated abelian groups.  These are a couple that stumped me. The first is $(Z_4\times Z_4\times Z_8)/\langle (1,2,4)\rangle$.  I can see that $Z_4\times Z_4\times Z_8$ has order $128$ while $\langle(1,2,4)\rangle$ has order $4$, so the factor group in question has order $32$.  It also appears there are elements of up to order $8$, which I think should narrow it down to $Z_4\times Z_8$ or $Z_2\times Z_2\times Z_8$, but how do I tell which? This next one I have no clue how to proceed with.  $(Z\times Z)/\langle(1,2)\rangle$  How would I go about solving this one?","['self-learning', 'group-theory']"
121275,Find the value of $(-1)^{1/3}$.,"Evaluate $(-1)^{\frac{1}{3}}$. I've tried to answer it by letting it be $x$ so that $x^3+1=0$.
But by this way, I'll get $3$ roots, how do I get the actual answer of $(-1)^{\frac{1}{3}}$??",['algebra-precalculus']
121290,Accumulation point in a pre-image,"Let $f:U\subset\mathbb{R}^m\rightarrow\mathbb{R}^n$ be a differentiable function over an open set $U$. If there is a $b\in\mathbb{R}^n$ such that the set $f^{-1}(b)$ has an accumulation point $a\in\mathbb{R}^n$, then $(Df)_a:\mathbb{R}^m\rightarrow\mathbb{R}^n$ is not injective.","['multivariable-calculus', 'analysis']"
121294,Fundamental groups,"I need to calculate the fundamental groups of the following spaces: $X_1 = \{ (x, y, z) \in \mathbb{R}^3 | x \neq 0\} $ $X_2 = \mathbb{R}^3 \backslash \{ (x, y, z)  | x = 0, y = 0, 0 \leq z \leq 1 \}$ $X_3 = \mathbb{R}^3 \backslash \{ (x, y, z)  | x= 0, 0 \leq y \leq 1 \} $ I'm not sure at all how one would calculate these.  I think that $X_1$ is still a convex space, so the fundamental group might be {1} but I'm really not at all sure...I need to calculate the fundamental groups of the following spaces:","['general-topology', 'algebraic-topology']"
121296,What formula describes $(a+b)^n$,"I know that $(a+b)^2=a^2+b^2+2ab$ and $(a+b)^3=a^3+b^3+3ab(a+b)$ So i think it must be a formula that describes $(a+b)^n$, but I don't know what.",['algebra-precalculus']
121305,What is the probability that a solitaire game be winnable?,"By ""solitaire"", let us mean Klondike solitaire of the form ""Draw 3 cards, Re-Deal infinite"". What is the probability that a solitaire game be winnable? Or equivalently, what is the number of solvable games ? When I came up with the question, it seemed a pretty reasonable thing to ask, and I thought ""surely it must have been answered"". I have no probability formation (save for an introductory undergraduate-level course), but anyway I started thinking on how could the problem be tackled. Immediately my interest shifted from the answer to the above question, to the methods involved in answering it. I couldn't even begin to figure out how would one go solving this problem! How does one even begin to find the number of solvable games? In the same wikipedia link, it is stated that For a ""standard"" game of Klondike (of the form: Draw 3, Re-Deal Infinite, Win 52) the number of solvable games (assuming all cards are known) is between 82-91.5%. The number of unplayable games is 0.25% and the number of games that cannot be won is between 8.5-18%. The reference for the thresholds is this paper by Ronald Bjarnason, Prasad Tadepalli and Alan Fern. It came as a surprise to me that the answer is not really known, and that there are only estimates. I tried reading the paper, but I'm too far away from those lines of thinking to understand what they're talking about. There seems to be some programming going around, but what is the big idea behind their approach to the question? I would like to end this question with a couple of lines from the paper (emphasis by me): Klondike Solitaire has become an almost ubiquitous computer application, available to hundreds of millions of users worldwide on all major operating systems, yet theoreticians have struggled with this game, referring to the
  inability to calculate the odds of winning a randomly dealt game as “ one of the embarrassments of applied mathematics ” (Yan et al., 2005).","['card-games', 'recreational-mathematics', 'probability']"
121309,Find the sum of  p+q?,If p = last non zero digit of 96! and q = Remainder(121212....300 times / 99 ) Find the value of p+q . Thanks in advance. How we can get the value from such big numbers? Any link that gives more knowledge about this type of questions will be very much thankful.,['number-theory']
121326,Unique critical point does not imply global maximum/global minimum,"I have actually two questions here, but both are very much related so I decided to put them both in this question. From Wikipedia I found the following example of a function that has a single critical point which is a local minimum, yet is not a global minimum: $f(x,y) = x^2 + y^2(1-x)^3$ Indeed the only critical point of $f$ is $(0,0)$ which is a local minimum by the second derivative test, but $(0,0)$ is not a global minimum because $f(4,1) = -11$. Why does this happen? I am having some trouble understanding intuitively why this is true. I know that $f$ has a minimum when restricted to a compact set. Then $(0,0)$ is a global minimum in any closed ball around $(0,0)$, and thus $(0,0)$ is a global maximum?? This is not true because $f$ is a counterexample, but I can't quite understand why. In some cases we do have a global minimum at the unique critical point. For example, let $f(x,y) = (-x+y)^2 + (x-1)^2 + (x+y-1)^2$ Then $f$ has a unique critical point at $(\frac{2}{3}, \frac{1}{6})$. The critical point turns out to be a global minimum (according to the graph and wolframalpha). How would I prove that the point is a global minimum? And how does one in general decide that a function has a global minimum/maximum on a critical point when the function is defined on all of $\mathbb{R}^2$, particularly when there is only one critical point?",['multivariable-calculus']
121327,Classical theorem of Riesz and Banach,"I am new to functional analysis. So maybe I am missing something obvious. I was reading this paper A closure problem related to the Riemann zeta function -- by Arne Beurling http://ukpmc.ac.uk/ukpmc/ncbi/articles/PMC528084/pdf/pnas00720-0060.pdf Just before equation (5) he mentions, If $C$ is not dense in $L^p$, we know by a classical theorem of F. Riesz and Banach that the dual space $L^q$ must contain a nontrivial element $g$ which is orthogonal to $C$ in the sense that $$ 0 = \int_0^1 g(x)f(x) dx \quad \quad f\in C$$ I could not see any references in the paper either where the classical theorem is mentioned, and so I do not have the slightest clue about that. Can someone please point to some references or the name of the theorem?","['functional-analysis', 'reference-request']"
121332,Construct / find the simplest function based on data,"Let's say I have these 7 natural numbers (all between 0 and 255): 255, 23, 45, 32, 87, 52, 146 How can I find a function F(x) that, once computed, gives me back these values, ie. f(1) = 255
f(2) = 23
f(3) = 45
... I'm not sure what's the technical term to use for this, but I trust you got the picture. All the above numbers are just examples (except that all values are natural number between 0 and 255). All I really care is that I need the simplest possible function that can give me such result, be it polynomial, quartic or any other type of function. I used to be good at math, but it's been at least 10 years since I took my last math class, so please bear with me! Thanks in advance!","['interpolation', 'regression', 'functions']"
121337,Generalization Limit,"I would like to know if it is possible generalize this result and how it could be shown: we know that $$
\begin{align}
\lim_{x\to 0^+}x^x & =1;\\  \\
\lim_{x\to 0^+}x^{x^x} & =0;\\  \\
\lim_{x\to 0^+}x^{x^{x^x}}& =1
\end{align}
$$ $$\begin{matrix}\displaystyle\lim_{x\to 0^+} \overbrace{x^{x^{x^{x^{\cdots^{x}}}}}}^{n\text{ times}} \end{matrix}\quad$$ I would like to conclude that if $n$ equal limit is $1,$ and if $n$ is odd limit is $0$
is it Possible? tanks in advances","['calculus', 'real-analysis', 'limits']"
121347,If two elements belong to the same conjugacy class then they have the same order,"How do we show that if two elements belong to the same conjugacy class then they have the same order? i.e. if G is a group and for $$ g,h\in G$$ $$\exists x\in G$$ s.t. $$h=xgx^{-1}$$ Thanks for any help","['group-theory', 'abstract-algebra']"
121365,Show that the representation $\mathbb Z\ni a\mapsto\begin{pmatrix}1& a\\0&1\end{pmatrix}$ is not completely reducible,"Let $\rho : \mathbb Z \to \mathrm{GL}_2(\mathbb C)$ be the representation defined by $\rho(1) = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$ . I'd like to show that $\rho$ is not completely reducible. I have one preliminary question (which is probably a silly one) - for what vector space $V$ is $\mathrm{GL}(V) \cong \mathrm{GL}_2(\mathbb C)$ ? Firstly, I noted that $\rho(1)$ has an eigenvector, so the representation is not irreducible. So if it were completely reducible, it would have to break up as a direct sum of two $1$ -dimensional sub representations. But a 1-dimensional subrep is given by an eigenvector - but $\rho$ only has one eigenvalue, which has a $1$ -dimensional eigenspace. So this can't happen. Is this reasoning OK? Once I've shown that the representation isn't irreducible, the problem is equivalent to showing that $\rho(1)$ cannot be diagonalised (which I've done by showing that the sum of the dimensions of the eigenspaces is $1$ , not 2). Depending on the answer to question (1), I could have reduced (excuse the pun) the amount of work by considering Jordan Normal Form ( $\rho(1)$ is in JNF but isn't diagonal, so isn't diagonalisable).","['matrices', 'finite-groups', 'representation-theory']"
121369,"Show that $\operatorname{ran}(I-T)$ is dense in $\ell^2$, $T$ is a right shift operator","$T$ is a right shift operator from $\ell^2 \to \ell^2$, $(\alpha_1, \alpha_2,\ldots)\mapsto (0,\alpha_1,\alpha_2,\ldots)$. I want to show that $\operatorname{ran}(I-T)$ is dense in $\ell^2$. 
Could anyone help me or give me a hint please?",['functional-analysis']
121372,Picard group of generic fibre,"Let $C$ be an irreducible curve over a field $k$ and let $X$ be a $k$-variety equipped with a morphism $f: X \to C$. Let $X_{k(C)} \to k(C)$ be the generic fibre of this morphism. Under which ""reasonable"" conditions on $X$, $C$ and/or $f$ (smoothness, properness and so on) will the natural sequence $$\text{Pic}\,C \to \text{Pic}\,X \to \text{Pic}\,X_{k(C)} \to 0$$ be exact? For example, does this hold if $X$, $C$ and $f$ are smooth and proper?",['algebraic-geometry']
121381,"Is $(p,\epsilon,p)$ a path of an automaton?","$A$ is an alphabet. An automaton over $A$ can be defined
as a set $A_0 = (Q, E, I, T),$ where $Q$ is the set
of states, $E \subseteq Q \times A \times Q$ is the
set of edges or transition, $I, T \subseteq Q$ of
initial and terminal states. A path in the automaton
$A_0$ is a sequence $(p_0, a_1, p_1),$ $(p_1, a_2, p_2),$
$\ldots,$ $(p_{n-1}, a_n, p_n)$ of consecutive edges.
A state $p$ is accessible if there is a path starting in an
initial state and ending in $p.$ It is coaccessible if
there is a path starting in $p$ and ending in a terminal
state. An automaton is trim if every state is accessible
and coacessible. The information above is from M. Lothaire's Algebraic
Combinatorics on Words. My question is whether
$(p, \epsilon, p)$ is a path or not. Since $\epsilon$ is
not in $A,$ my answer is no. If so, is an inital state
accessible? According to the definition above, we may
say it will be if there is a path starting from an initial state and ending in the initial state.Then if an automaton is trim,the initial states should be accessible.But in the following statement about the trim automaton,I don't think the $trim$ ensures the initial state(s) is(are) necessary accessible. So actually I want to make sure whether the $(p,\epsilon,p)$ is a path. Second, if the initial states are only coaccessible(i.e there may be no path starting from an initial state and ending in the initial states), is the automaton still trim? These two questions make me too confused to continue to read the following content in the book. Thanks in advance. If I applied $\epsilon$ into the definition.Then the example of the first returns to state 1 below shouldn't be $X=${$b,ab$} but $X=${$\epsilon,b,ab$} which contradicts with the former $X=${$b,ab$} given in the book.The set of $first$ $returns$ to a state $q$ is the set of labels(or words) of paths from $q$ to $q$ which do not pass another time through $q$. On the other hand,without the $\epsilon$ path,I don't know how to ensure the automaton in the following proof is trim,since the initial state doesn't seem to be accessible without the $\epsilon$ path. All the information above is shown in http://x.co/iEVa , from Page 12 to Page 14. Thank you for reading this long post.","['automata', 'abstract-algebra', 'combinatorics']"
121386,How to prove $\lim_{n\rightarrow \infty} {a^n \over n!}=0$ [duplicate],"This question already has answers here : Closed 12 years ago . Possible Duplicate: How to prove that $\lim\limits_{n \to \infty} \frac{k^n}{n!} = 0$ As the topics, how to prove $\lim\limits_{n\rightarrow \infty} \dfrac{a^n}{n!}=0$ $\forall a  \in \mathbb{R^+}$","['real-analysis', 'limits']"
121387,Calculating Gröbner basis for Sudoku,"I'm trying to write a program that solves sudokus using a Gröbner basis.
I introduced 81 variables $x_1$ to $x_{81}$, this is a linearisation of the sudoku board. The space of valid sudokus is defined by: for $i=1,\ldots,81$ : $F_i = (x_i - 1)(x_i - 2)\cdots(x_i - 9)$
    This represents the fact that all squares have integer values between 1 and 9. for all $x_i$ and $x_j$ which are not equal but in the same row, column or block:
    $G_{ij} = (F_i - F_j)/(x_i - x_j)$
    This represents that the variables $x_i$ and $x_j$ can not be equal. All these $F_i$ and $G_{ij}$ together define the space of valid sudokus. This consists of 891 polynomials. Now to solve a sudoku we can add the clues to the space, so by example if the clue of a sudoku is the first square is a 5, then we add $(x_1 - 5)$ to the space.
If we now take the Gröbner basis of this space we can directly see the solution for it. I understand what I am doing this far. But I have trouble finding a computable manner for finding the Gröbner bases.
I have successfully done everything for $4\times4$ sudokus (or so-called shidokus). But Maple nor Singular are giving me a result for the Gröbner basis of the $9\times9$ sudoku space.
You can see the commands I gave to Maple here (First I define the 891 polynomials, then I ask for a basis of it)
I read papers saying it's feasible although inefficient to do what I strive for but I don't see how to find the solution, as they don't include many implementation details. Can anyone point me to a direction, making this problem easier for Maple or other software?","['groebner-basis', 'sudoku', 'puzzle', 'abstract-algebra', 'recreational-mathematics']"
121388,"How should I understand ""$A$ unless $B$""?","The statement is from the book Linear Integral Equations by Rainer Kress: A compact operator cannot have a bounded inverse unless its range has finite dimension. Here are my questions : How should I understand the word ""unless"" here? If one interprets the statement in the form of $A\rightarrow B$, what should be $A$ and $B$ respectively? Is there an operator such that its range has finite dimension, but it does not have a bounded inverse? There can be two cases for ""not have a bounded inverse"": It does not have inverse. It has an inverse but the inverse is unbounded. For the first question, I think  we can translate a statement of the form ""$P$ unless $Q$""
as ""if not $Q$, then $P$"", or: $\neg Q → P$, according to this note for formal logic. But I'm not sure if the ""unless"" above is the same as the one here, which is the very reason why I ask the second question.","['logic', 'functional-analysis']"
121398,Order of the entire function: $\prod\limits_{n=1}^{\infty} \left(1-\frac{z}{n^k}\right)$,Please how to find order of $$ f_k(z) = \prod\limits_{n=1}^{\infty} \left(1-\frac{z}{n^k}\right) .$$ Let $M(r) = \max \{|f_k(z)|:|z| = r\}.$ Then order of $f_k(z)$ is defined as : $$\lambda = \limsup_{r\to \infty} \frac{\log \log M(r)}{\log r}.$$ Can someone help to solve this. Thank you.,['complex-analysis']
121403,Motivation behind the definition of a Manifold.,"A manifold $M$ of dimension n is a topological space with the following properties: a) $M$ is Hausdorff b)$M$ is locally Euclidean of dimension n c) $M$ has a countable basis of open sets. Why is the first property necessary? I do not have much experience with Hausdorff spaces, hence I am not able to see the importance of that condition, probably it is something obvious. Since I come from a physics background, I can understand the importance of the second property. But what is the strong mathematical motivation to study such special subset of a topological space? Also the second property can be alternatively stated as: each point $p\in M$ has a neighborhood $U$ which is homeomorphic to an open subset $U$ of $\mathbb R^n$. Why should it be homeomorphic to an open set and not closed? Similarly in the third property,why a countable basis of open sets?","['manifolds', 'differential-geometry']"
121407,How to compute $\sum\limits_{k=0}^n (-1)^k{2n-k\choose k}$?,"I got stuck at the computation of the sum 
$$
\sum\limits_{k=0}^n (-1)^k{2n-k\choose k}.
$$
I think there is no purely combinatorial proof here since the sum can achieve negative values. Could you give me solution, it seems to involve generating functions.","['binomial-coefficients', 'combinatorics']"
121422,Bounded analytic function on a punctured region,"Let $G$ be a region in $\mathbb{C}$ and $a\in G$. Suppose $f:G-\{a\}\to\mathbb{C}$ is an injective analytic function such that $f(G-\{a\})=\Omega$ is bounded. Show that $f(a)\in\partial\Omega$. I know a couple things. Since $f$ is injective it's non-constant, so by the open mapping theorem we know $\Omega$ is open. Also, $f$ is obviously bounded in a neighborhood of $a$, so $z=a$ is a removable singularity. Then we can define $f$ so that it's analytic at $a$, possibly losing injectivity in the process. But I'm stuck here. Any help is appreciated",['complex-analysis']
121424,Picard group and cohomology,"It's an easy but boring exercise (Hartshorne Ex. III.4.5 or Liu 5.2.7) that the group $Pic(X)$ of isomorphism classes of invertible sheaves on a ringed topological space (well, maybe we can restrict to schemes) is isomorphic to $H^{1}(X, \mathcal{O}_X^{*})$, where $\mathcal{O}_X^{*}$ denotes the sheaf whose sections over an open set $U$ are the units in the ring $\mathcal{O}_X(U)$. The proof that I know (that uses the hint given by Hartshorne) uses heavily Cech cohomology: basically the idea is that given an invertible sheaf $\mathcal{L}$ and an affine open covering $\mathcal{U}=(U_i)$ on which $\mathcal{L}$ is free, we can construct an element in $\check{C}^1(\mathcal{U},\mathcal{O}_X^{*})$ using the restriction of the local isomorphism to the intersections $U_i\cap U_j$. The cocycle condition on triple intersection implies that we have a well defined element in $\check{H}^{1}(X, \mathcal{O}_X^{*})$. Then one proves that the map is an isomorphism of groups. My question is the following: this approach is not very enlightening. Is there a more intrinsic proof of the isomorphism between $Pic(X)$ and $H^{1}(X, \mathcal{O}_X^{*})$, without Cech cohomology?","['homology-cohomology', 'algebraic-geometry']"
121431,"Let K/F be a finite extension, given a polynomial in K[x] find another so that their product is in F[x]","Let $K$ be a finite extension of a field $F$, and let $f(x)$ be in $K[x]$. Prove that there is a nonzero polynomial $g(x)$ in $K[x]$ such that $f(x)g(x)$ is in $F[x]$. Should I do this by induction on the degree of $f(x)$? Obviously if $n=0$, then $g(x)=1/f(x)$ Let $f(x) = a_nx^n+...a_1x+a_0$       then I know that there exists a h(x) so that $(f(x)-a_nx^n)h(x)$      is in $F[x]$. I want now to find a $g(x)=h(x)+i(x)$ so that $f(x)g(x)$ is in $F[x]$. Thus I need to find an $i(x)$ so that $a_nx^nh(x)+i(x)f(x)$ is in $F[x]$. I feel like this is wrong because I have no control over the degrees of $h(x)$. Any suggestions?","['galois-theory', 'field-theory', 'abstract-algebra', 'polynomials']"
