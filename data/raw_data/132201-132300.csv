question_id,title,body,tags
2070496,Does $2X\neq X$ imply $2Y\neq Y$?,"Let $n$ be a sufficiently large integer and $X\subseteq \mathbf{Z}_n$ a finite set of residues modulo $n$ such that $0\in X$ and $2X \neq X$. Fix also $y \in 2X\setminus X$ and define $Y=\{x \in X:y\notin X+x\}$. Question . Is it true that $2Y \neq Y$? Ps. Here $A+B:=\{a+b:a \in A,b \in B\}$, $2A:=A+A$, and $A+b:=A+\{b\}$.","['number-theory', 'sumset', 'additive-combinatorics']"
2070497,How is a ring a $\mathbb{Z}$-algebra?,"If all of the below is true, does it follow directly from the definitions and the canonical way every abelian group can be considered a $\mathbb{Z}$ -module? EDIT: In the ""definitions"" below, it is necessary to include left and right distributivity; the two distributive axioms do not follow from the other structures. (For commutative rings technically one of the two will always follow from the other so it is only necessary to assume one -- for non-commutative rings one needs to assume both.) Question: If we define: ring : abelian group under $+$ , semigroup under $\times$ , ring with identity : abelian group under $+$ , monoid under $\times$ , commutative ring : abelian group under $+$ , commutative semigroup under $\times$ , commutative ring with identity : abelian group under $+$ , commutative monoid under $\times$ , then are the following equivalences true? (Yes/no will suffice for an answer.) $R$ ring $\iff$ $R$ associative $\mathbb{Z}$ -algebra $R$ ring with identity $\iff$ $R$ unital, associative $\mathbb{Z}$ -algebra $R$ commutative ring $\iff$ $R$ commutative, associative $\mathbb{Z}$ -algebra $R$ commutative ring with identity $\iff$ $R$ unital, commutative, associative $\mathbb{Z}$ -algebra In particular, no ring is a non-associative $\mathbb{Z}$ -algebra?","['terminology', 'abstract-algebra', 'definition']"
2070504,discrete mathematics notes possibilities problem,"In how many ways you can put two similar notes in a bag, so on each
  one there is a password  of 6 letters (abc..z). I am having trouble with it. My way of thinking:
On the first note: 26^6
On the second note: it is the same thing, but i may have doubles, so i need to divide it by 2. so i am thinking of (26^6)/2 so the answer is 26^6 + (26^6)/2 But i don't think I am right.",['discrete-mathematics']
2070505,Expectation of $W(t)e^{\lambda aW(t)}$ [duplicate],"This question already has answers here : What is $\mathbb{E}[W(s)\mathrm{e}^{W(S)}]$ where W(S) is a standard Brownian Motion? (3 answers) Closed 7 years ago . Let $\{W_t\}_{t \in \mathbb{R}_+}$ be a Wiener process. I wish to calculate $E \left[ W(t)e^{\lambda W(t)} \right], \lambda \in \mathbb{R}$. Solution attempt First define a function given by $f(t,x) = xe^{\lambda x}$. Then, \begin{align} 
\frac{\partial f(t,x)}{\partial x} &= e^{\lambda x} +  \lambda x e^{\lambda x}, \\ \frac{ \partial^2 f(t,x)}{\partial x^2} &= 2\lambda e^{\lambda x} + \lambda^2 xe^{\lambda x}.\end{align} So by Ito's formula, the differential of $f(t,W(t))$ is given by \begin{align} df =  \left[  \lambda e^{\lambda W} + \frac{\lambda^2}{2}We^{\lambda W} \right]dt + \left[ e^{\lambda W} + \lambda We^{\lambda W} \right] dW\end{align} Or, rather, $$f_s = f_0 + \int_0^s \left[  \lambda e^{\lambda W} + \frac{\lambda^2}{2}We^{\lambda W} \right]dt + \int_0^s \left[ e^{\lambda W} + \lambda We^{\lambda W} \right] dW.$$
Then we take expectations, so that the stochastic integral disappears, then we define $m(t) = E f_s = E f(s,W(s))$, and then we take the derivatives. We then get $$m'(t) = E \lambda e^{\lambda W} + \frac{\lambda^2}{2}m(t).$$
The first term on the left hand side equals $\lambda e^{t\frac{\lambda^2}{2}}$. At this point, as I am not good with differential equations, I am kind of stuck. And, either way, I would like to have my solution so far checked out as well, as I am unsure whether the stochastic integral really does disappear when I take expectations. What condition needs to be satisfied for this to hold, and how do I check whether it does?","['stochastic-processes', 'probability-theory', 'measure-theory', 'stochastic-integrals', 'stochastic-calculus']"
2070516,Prove the series $\sum_{n=1}^{\infty}{x^{n-1}\over(n-1)!}\cdot{e^{-xn}-1\over e^{xn}-1}=-e^{-x(1-e^{-x})}$,"Show that, $$\sum_{n=1}^{\infty}{x^{n-1}\over(n-1)!}\cdot{e^{-xn}-1\over e^{xn}-1}=-e^{-x(1-e^{-x})}$$ My try: We know $$\sum_{n=1}^{\infty}{x^{n-1}\over (n-1)!}=e^x$$ $$\sum_{n=1}^{\infty}{e^{-xn}-1\over e^{xn}-1}={1\over 1-e^{x}}$$ How do I use these two formulae to arrive  at the top formula? Let and try letting $x=\ln{y}$ then we have $$\sum_{n=1}^{\infty}{\ln{y}^{n-1}\over (n-1)!}\cdot{y^{-n}-1\over y^n-1}=-y^{{1\over y}-1}$$ Still can't see anything obvious step to do next, any hints?",['sequences-and-series']
2070536,Probability of Royal Flush dealt in 25-card hand,"This question arose from the game ""Poker Shuffle"", where one can rearrange 25 cards in a 5x5 grid to maximize the net score of the resulting 10 poker hands. As the royal flush is the highest-scoring hand, I am wondering what the probability of being dealt a royal flush is. I don't even know where to begin in attacking such a problemâ€”my only insight is that it will likely have to take the form of an inclusion-exclusion formula due to the possibility of multiple royal flushes in one ""hand"". Thanks in advance for any thought any of you put into this!","['statistics', 'poker', 'probability']"
2070559,Solutions for diophantine equation $3^a+n=2^b$,"A few days ago I asked for a solution for $3^a+1=2^b$ where $a\in \Bbb N$ and $b\in \Bbb N$ ( Solutions for diophantine equation $3^a+1=2^b$ ), and I got two good answers for this question. But there I also asked for the more general case $p_1^a+n=p_2^b$ where $p_1$ and $p_2$ are prime and $a,b,n\in \Bbb N$. But this part of my question was ignored, so here I explicitly asks for ways to find solutions for this general case. I still am interested in any $p_1$ and $p_2$, but im very focused on $p_1=3$ and $p_2=2$ because this is part of another problem that I try to solve. I explicitly am interested in solutions for this equations: $3^a+5=2^b$ $3^a+7=2^b$ $3^a+13=2^b$ $3^a+23=2^b$ ... where $a\in \Bbb N$ and $b\in \Bbb N$ and where $b>2$ ($2^b>4$) Supposed that $b>2$, you can show, that in $2^b-3^a \equiv n \pmod{24}$ $n$ only can be $5,7,13$ or $23$ Proof: $2^b \pmod{8}$ is always $0$ when $b>2$; $3^a \pmod{8}$ is always $1$ or $3$; so $2^b-3^a \pmod{8}$ only can be $5$ or $7$. $3^a \pmod{3}$ is always $0$ when $a>0$, but $2^b \pmod{3}$ is never $0$, it's either $1$ or $2$, so also $2^b-3^a \pmod{3}$ only can be $1$ or $2$. Together this leads to the fact, that $2^b-3^a \pmod{24}$ only can be $5,7,13$ or $23$ when $b>2$. qed Knowing this, I was searching for differences $n$ of powers of $2$ minus powers of $3$ having $n<100$ and I found this: $n=5$: $3^1+5=2^3 \rightarrow 3+5=8$ $3^3+5=2^5 \rightarrow 27+5=32$ $n=7$: $3^2+7=2^4 \rightarrow 9+7=16$ $n=13$: $3^1+13=2^4 \rightarrow 3+13=16$ $3^5+13=2^8 \rightarrow 243+13=265$ $n=23$: $3^2+23=2^5 \rightarrow 9+23=32$ $n=29 = 5+24$: $3^1+29=2^5 \rightarrow 3+29=32$ $n=31 = 7+24$: no solution found $n=37 = 13+24$: $3^3+37=2^6 \rightarrow 27+37=64$ $n=47 = 23+24$: $3^4+47=2^7 \rightarrow 81+47=128$ $n=53 = 5+2*24$: no solution found $n=55 = 7+2*24$: $3^2+55=2^6 \rightarrow 9+55=64$ $n=61 = 13+2*24$: $3^1+61=2^6 \rightarrow 3+61=64$ $n=71,77,79,85,95$: no solutions found By comparing any power of 2 up to a certain limit (which was $b \le 2^{10}$) with the biggest powers of 3 being smaller than that power of 2, I found out, that all other differences of powers of 2 minus powers of 3 are bigger than 100 for all $b \le 2^{10}$ which means $2^b \le 1.8 \times 10^{307}$. So the solutions for $n<100$ listed above are the only existing solutions with $3^a$ and $2^b$ having less than 307 decimal digits. And this makes me believe, that there also are no solutions for $n=5,7,13,23,31,...,95$ when $3^a$ and $2^b$ are bigger than $10^{307}$. And I also think, that there is no difference, that appears infinitely often. I even think, that the maximum number that a given difference can appear is 2. But I have no idea how to prove this. Can you help? Are there any other solutions for low values of $n$? If yes: How can you find them? If no: How can you prove that? How many values for $a$ (or $b$) can share the same difference $n$?","['number-theory', 'diophantine-equations']"
2070597,"Baumslag-Solitar group BS(1,2) is torsion-free","I need to show that the Baumslag-Solitar group $BS(1,2)$ is torsion-free. It is given by $$BS(1,2) = \langle a,t \mid tat^{-1} = a^2\rangle$$ I have taken a non-trivial element $g$ from $BS(1,2)$ . I assumed it is of finite order and tried to play with it, in order to get contradiction, with no luck.","['torsion-groups', 'group-theory', 'geometric-group-theory']"
2070599,"Suppose $X_n\sim \text{Poisson}(n)$. Show that $\sqrt{X_n}-\sqrt{n}\overset{d}\to \mathcal N(0,1/4)$.","Suppose $X_n\sim \text{Poisson}(n)$. Show that $\sqrt{X_n}-\sqrt{n}\overset{d}\to \mathcal N(0,1/4)$. I already know that $(X_n-n)/\sqrt{n}\overset{d}\to \mathcal N(0,1)$. How to do next to go through the proof?","['probability-theory', 'probability-distributions', 'statistics', 'probability', 'measure-theory']"
2070607,Is there a deep connection between the urn problem and the sum of powers?,"The urn problem goes as follows: How many ways are there to place $k$ indistinguishable balls into $n$ distinguishable urns? The solution to the problem being $\binom {n+k-1}{k}$ (or $\binom {n+k-1}{n-1}$). Upon typing the solution into Wolfram Alpha I found the following table: This table, I noticed, exhibits uncanny similarity to the table of the sums of powers $\sum _{i=1}^{n}i^{k}$: My question is, is this similarity accidental, or is there a more deep connection between the two? And if there is such connection, is there an intuitive way of deriving the first from the second and vise versa?","['combinatorics', 'summation', 'binomial-coefficients']"
2070706,proving that $f(x) = x^s$ is holder continuous with holder exponent s,"I want to show that $$f: \mathbb{R}_{\geq0} \to \mathbb{R}_{\geq0}$$ $$x \mapsto x^s$$ is Holder continuous with Holder exponent $s \in \mathbb{R}$, where $0<s \leq 1$. So what I want to show is that $\exists \hspace{2 mm} C \in \mathbb{R}_{\geq0}$ sucht that for all $ x,y \in \mathbb{R}_{\geq0},  $ $$|x^s -y^s| \leq C|x-y|^s$$ and therefore, assuming, wlog $\hspace{1mm} x>y$
$$(x^s -y^s) \leq C(x-y)^s.$$ I thought about Bernoulli's inequality but couldn't make that work.I thought about the binomial theorem, but didn't know how to handle the fact that $s \in \mathbb{R}$.","['continuity', 'holder-spaces', 'analysis']"
2070723,Homeomorphism between half sphere and whole sphere,"Let $\sim$ be the equivalence relation on the sphere $$S^2=\left\{(x,y,z)\in \mathbb R^3 \mid x^2+y^2+z^2=1 \right\}$$ given by $$(x,y,z)\sim (-x,-y,z).$$ Prove that $S^2/{\sim}$ with quotient topology is homeomorphic to $S^2$. My plan is to construct a quotient map from $S^2$ to itself which is constant for the equivalent class in $S^2$, then the induced map from $S^2/{\sim}$ to $S^2$ is homeomorphism. But I don't know how to construct it. Or is there some other approach?",['general-topology']
2070734,Convolution that is equiprobable to a uniform distribution,"I am struggling with finding two distributions a and b on the non
non-negative integers (both not concentrated at 0, this is t trivial)
such that the convolution of a and b is the equiprobable  distribution on the set 0,1,...n-1 n is not a prime number. Any ideas?","['probability-theory', 'probability', 'convolution']"
2070810,Matrices with Three Nonzero Diagonals,"How would you compute eigenvectors of matrices of the form \begin{equation}
\nonumber
M =
\left( \begin{array}{cccccccccc}
 a_1 & 0  & b_1&&&&&&&\\ 
0 &  a_2 & 0& b_2&&&&&&& \\
c_1 &0 &  a_3 & 0&b_3&&&&&& \\
&c_2&0&  a_4& 0&&&&&& \\
&&c_3&0& &&&&&& \\
&&&&&\ddots\\ \\
&&&&&& a_{n-3} & 0 & b_{n-3}& \\
&&&&& &0 & a_{n-2} & 0 & b_{n-2}\\
&&&&&&c_{n-3}& 0& a_{n-1}&  0\\
&&&&&&&c_{n-2}& 0 &a_n \\
\end{array} \right) 
\end{equation} where all the omitted entries are null? The particular matrix I'm dealing with has the additional property of being stochastic, so the principal eigenvalue is one.","['eigenvalues-eigenvectors', 'stochastic-processes', 'matrix-equations', 'matrices', 'linear-algebra']"
2070832,Find all pairs of positive integers that add up to $667$ and their $\frac{\text{lcm}}{\text{gcd}} =120$,"Let $x,y$ be positive integers, $x<y$, and $x+y=667$. Given that $\dfrac{\text{lcm}(x,y)}{\text{gcd}(x,y)}=120,$ find all such pairs $(x,y)$. The only way I can think of solving this is trying all possibilities where one number is odd and the other even, and testing them all. Using this, I found one solution, $(115,552)$, but I'm wondering if there is a more efficient way to do this problem.","['algebra-precalculus', 'least-common-multiple', 'gcd-and-lcm', 'elementary-number-theory']"
2070842,how to prove $f(x) = x^s$ Lipschitz continuous,"I want to show that for $0<s \leq 1$ $$f: \mathbb{R}_{\geq0} \to \mathbb{R}_{\geq0}$$ $$x \mapsto x^s$$ is Lipschitz continuous on$[a,\infty)$ for any $a \in\mathbb{R}_{>0}$, and that it is NOT on $[0,\infty)$. So what I want to show is that $\exists \hspace{2 mm} C \in \mathbb{R}_{\geq0}$ sucht that for all $ x,y \in [a,\infty)  $ $$|x^s -y^s| \leq C|x-y|$$ and therefore, assuming, wlog $\hspace{1mm} x>y$
$$(x^s -y^s) \leq C(x-y),$$ And that $\nexists $ such $C$ on $[0,\infty).$ Once again my (lack of ) mastery with inequalities is not yet sufficient for this type of problem. Can someone lead the way (without recourse to concave/convex functions)?","['continuity', 'lipschitz-functions', 'analysis']"
2070843,"Discrete formula for ""special angle"" outputs? Why is pi/5 not one of them?","One might (arbitrarily) define the ""special angles"" as angles $\theta \in [0, \pi)$ such that $\sin(\theta) = \left. \left\{ \frac{\sqrt{k}}{2} \right\} \right|_0^4 = \left\{0, \frac{1}{2}, \frac{\sqrt{2}}{2}, \frac{\sqrt{3}}{2}, 1 \right\}$ which are of course the angles $\theta' = \left\{ 0, \frac{\pi}{6}, \frac{\pi}{4}, \frac{\pi}{3}, \frac{\pi}{2} \right\}$. (Using prime to denote ""special"", i.e. belonging to this set.) I'm wondering if there is a pattern underlying the relationship between the ""special angles"" and the sine-outputs; i.e. if a discrete index-based formula could be constructed that yields the value of sine for any of the special angles. I'm thinking of the form $f(\theta ') = \sin(\theta ')$ which I only expect to be valid for the special angles $\theta '$ but might possibly be a very simple function (if we are lucky). I believe constructing such a formula requires writing the sequence $ \left\{ \infty, 6, 
4, 3, 2 \right\} \left(=\frac{\pi}{\left\{ \theta' \right\} }\right)$ in standard nth-term form $ \left\{ a_n \right\}$ with a discrete index $n$, but I'm not seeing the pattern. If $\frac{\pi}{5}$ were a special angle, the pattern would nearly trivial. This makes me wonder how such an apparently ""unnatural"" pattern (which excludes 5) arises in the natural mathematics of trigonometry. Any insight into constructing $f(\theta ')$ or explaining the anomalous nature of $\frac{\pi}{5}$ would be appreciated.","['trigonometry', 'sequences-and-series']"
2070851,Prove the series $(-1)^k\sum_{n=0}^{\infty}{2^{n+1}(2k-1)!!\over {2n\choose n}(2n+1)(2n+3)\cdots(2n+2k+1)}=\pi-4(...)$,"Show that for $k\ge1$, \begin{align}
\\&\quad(-1)^k\sum_{n=0}^{\infty}{2^{n+1}(2k-1)!!\over {2n\choose n}(2n+1)(2n+3)\cdots(2n+2k+1)}\\[10pt]&=\pi-4\left(1-{1\over 3}+{1\over 5}-{1\over 7}+\cdots+{1\over 2k-1}\right).
\end{align} I try: The RHS sort of the favourite Leibniz $\pi$ series. We could split into composite of fractions ${A\over 2n+1}+{B\over 2n+3}+{C\over 2n+5}+\cdots$ and then take the sum individually. We know that $\sum_{n=0}^{\infty}{2^{n+1}\over {2n\choose n}(2n+1)}=\pi$ $\sum_{n=0}^{\infty}{2^{n+1}\over {2n\choose n}(2n+3)}=3\pi-8$ We don't know the general of $\sum_{n=0}^{\infty}{2^{n+1}\over {2n\choose n}(2n+2k+1)}=F(k)$ I am sure what to do next? Help please!","['power-series', 'real-analysis', 'sequences-and-series']"
2070887,Is this theorem wrong?,"From Schaum's Outline of Theory and Problems of Discrete Mathematics Theorem 2.3: Let $R$ be a relation on a set $A$. Then: $(i)$ $R\cup \Delta A$ is the reï¬‚exive closure of $R$. $(ii)$ $R \cup R^{âˆ’1}$ is the symmetric closure of $R$. I think the theorem is wrong. For $(i)$, consider set $A = \{1, 2, 3,\}$ $R = \{(1,2) (2,1)\}$
The Reflexive closure of $R$ is $R \cup \{(1,1), (2,2)\}$ and not $R \cup \{(1,1) (2,2) (3,3)\}$
I do agree though, that the Reflexive closure of $R$ reflexive$(R) \subseteq R \cup \Delta A$ For $(ii)$ same argument $R \cup R^{-1}$ is $\Bbb U$ for the example above, $R$ is it's own Symmetric closure. And of course $R \in \Bbb U$. Am I right? Are the two theorems indeed wrong, or am I missing something.","['relations', 'discrete-mathematics']"
2070904,clarification on showing $\lim_{x\to 0} \frac{1}{x}$ does not exist.,"So this has to do with the classic problem from here: Proof that the limit of $\frac{1}{x}$ as $x$ approaches $0$ does not exist I only had a question of how to properly come up with the needed contradiction. So if I am proving this via contradiction I would be assuming that $\left|\frac{1}{x} - l\right| < \epsilon$,  my issue is, how do I arrive at the necessary $x$ because to obtain that $$\left|\frac{1}{x}\right| > \epsilon + |l|$$ I would need to use the condition $$\left|\frac{1}{x} - l\right| > \epsilon$$ Doing the manipulations with my current assumption is not leading me to the right answer. What would I have to do to arrive at the contradiction?","['real-analysis', 'calculus', 'limits']"
2070913,Prove that $f(x)=0$ has no repeated roots,"$$\text{If } f(x)=\frac{x^n}{n!}+\frac{x^{n-1}}{(n-1)!}+\cdots+x+ 1\text{, then show that } f(x) = 0\\ \text{ has no repeated roots.}$$ I tried solving this question and I think I have come up with a proper answer. I need some verification. My solution/attempt First, we need to prove a theorem. Theorem 1 : If a polynomial function $f(x)$ has a repeated root (say $a$, i.e. $f(a)=0$), then $f'(a)=0$. Proof: We assume that $f(x)$ has a degree of $n \geq 2$.
Since $a$ is a factor of $f(x)$, we can write: $$f(x)=(x-a)^m\cdot h(x) \tag{1}$$
where $2  \leq m \leq n$ and $h(x)$ is a polynomial of degree $n - m$ On differentiating $(1)$, we can write
$$f'(x) = m(x-a)^{m-1}\cdot h(x) + (x-a)^m\cdot h'(x)\tag{2}$$ Plugging in $x=a$, we obtain
$$f'(a)=0+0=0$$ Thus, theorem 1 is true. Now, from the question, we have
$$f(x)=\frac{x^n}{n!}+\frac{x^{n-1}}{(n-1)!}+\cdots+x+ 1\tag{3}$$
If we plug in $x=0$, we get $f(0)=1 \neq 0$, for any value of $n$. Hence $0$ is not a root of $f(x)$. On differentiating, we get
$$f'(x)=\frac{x^{n-1}}{(n-1)!}+\frac{x^{n-2}}{(n-2)!}+\cdots+x+ 1\tag{4}$$ From $(3)$ and $(4)$, we obtain
$$f(x)-f'(x)=\frac{x^n}{n!} \tag{5}$$ Suppose $c \neq 0$ is a root of $f(x)$. Then from $(4)$, we have $$f'(c)=-\frac{c^n}{n!} \neq 0\tag{6}$$ Hence, from theorem 1 , we can conclude that $f(x)=0$ has no repeated roots. Q.E.D. My question I am a beginner in this field. Did I do all the steps correctly? Are there any points I need to take care of?","['derivatives', 'calculus', 'proof-verification']"
2070918,"""Proving"" the definition of the Laplace Transform from two properties","Suppose $\mathbf{G}$ is the class of piecewise continuous functions of exponential order from $[0,\infty)$ to $\mathbb{R}$ and that $\mathcal{L}$ is a transformation defined on elements of $\mathbf{G}$ satisfying the two properties For $f,\,g\in\mathbf{G}$ and $c_1,\,c_2\in\mathbb{R}, \mathcal{L}\{c_1f+c_2g\}=c_1\mathcal{L}\{f\}+c_2\mathcal{L}\{g\}$ $\mathcal{L}\{f^\prime(t)\}=s\mathcal{L}\{f(t)\}-f(0)$ Certain elementary properties of Laplace transforms follow easily from these two. For example: $\mathcal{L}\{0\}$ follows from property ($1$). Let $f(t)=1$ for $t\ge0$. Then $\mathcal{L}\{1^\prime\}=s\mathcal{L}\{1\}-1$ therefore
$$\mathcal{L}\{1\}=\dfrac{1}{s}\tag{1}$$ Using this as a basis step and using property ($2$) to prove that for $n\ge0$
$$ \mathcal{L}\{t^{n+1}\}=\dfrac{s}{n+1}\mathcal{L}\{t^n\}$$
one may establish the inductive step showing that
$$\mathcal{L}\{t^n\}=\dfrac{n!}{s^{n+1}}\tag{2}$$ Also, since property ($2$) gives $\mathcal{L}\{\left(e^{at}\right)^\prime\}=s\mathcal{L}\{e^{at}\}-e^0$ we immediately get $$ \mathcal{L}\{e^{at}\}=\dfrac{1}{s-a}\tag{3}$$ From $\mathcal{L}\{(\cos t)^\prime\}=s\mathcal{L}\{\cos t\}-1$ we obtain $$ \mathcal{L}\{\sin t\}+s\mathcal{L}\{\cos t\}=1$$ and from $\mathcal{L}\{(\sin t)^\prime\}=s\mathcal{L}\{\sin t\}-0$ we obtain $$ s\mathcal{L}\{\sin t\}-\mathcal{L}\{\cos t\}=0$$ And the solution of these two gives $$ \mathcal{L}\{\sin at\}=\dfrac{a}{s^2+a^2}\tag{4}$$ $$ \mathcal{L}\{\cos at\}=\dfrac{s}{s^2+a^2}\tag{5}$$ When I first adopted this approach decades ago in a differential equations class I believed I also had a proof that $$ \mathcal{L}\{f(t)\}=\int_0^\infty e^{-st}f(t)\,dt$$ but I cannot recall it so I may have been mistaken. Can anyone prove this usual definition of the Laplace transform from the two properties at the top of this post?","['ordinary-differential-equations', 'laplace-transform']"
2070957,Gradient of cross product,"Consider $\mathbb{R}^3 \times \mathbb{R}^3$ with standard coordinates $(q_1, q_2, q_3, p_1, p_2, p_3)$. For a fixed $v \in \mathbb{R}^3$, consider the function $f : \mathbb{R}^3 \times \mathbb{R}^3 \to \mathbb{R}$ given by $$f(q, p) = \langle v, q \times p \rangle$$ Writing everything out, it's easy to show that $\nabla f = (- v \times p, v \times q)$. Is there an easier way to see this, that doesn't involve writing out the coordinate-wise formulas for cross product and inner product?","['multivariable-calculus', 'cross-product', 'differential-geometry']"
2070973,Why is $\tan x$ not a continuous function?,"My textbook defines a continuous function as follows: The function $f(x)$ is continuous if, for all $a$ in its domain, $\lim_{x\rightarrow a} f(x)$ exists and is equal to $f(a)$. However, when I apply this to $f(x)=\tan x$, it seems to show that $\tan x$ is continuous, because: For all $a$ in the domain of $\tan x$ (i.e. all real numbers except $\frac{(2k+1)\pi}{2}, n\in \mathbb{Z}$), we have that $\lim_{x\rightarrow a} \tan x$ exists and is equal to $\tan a$ (this can be easily seen from the graph of $\tan x$). So it appears that $\tan x$ is continuous. However, I already know that it isn't continuous at $x=\frac{(2k+1)\pi}{2}$. Does this still mean it is continuous because it agrees with the definition? I'm not sure what to do now. Edit: Really what I meant to ask is the following: Here is the wording in the book: ""A function $f(x)$ is continuous at $x=a$, if $f(a)$ is defined and $\lim_{x\rightarrow a} f(x)$ is defined and is equal to $f(a)$. A continuous function satisfies this condition for all values of $a$ in its domain, so the graph of a continuous function is unbroken."" Question: I'm assuming the book's definition is wrong? Because the last sentence seems to imply that $\tan x$ is continuous (which is what had me confused).","['continuity', 'trigonometry', 'functions']"
2070983,How to differentiate $G(y)=\ln\left[\frac{(2y+1)^5}{(y^2+1)^{1/2}}\right]$?,"I'm trying to differentiate the following expression. My work is below. I'm not sure where my algebra has gone awry. Really appreciate any help. The problem is from James Stewart's Calculus Early Transcendentals 7th Ed. Page 223, Exercise 13. Please differentiate:
$G(y)=\ln\left[\frac{(2y+1)^5}{(y^2+1)^{1/2}}\right]$ My answer: First, differentiate the natural log.
$$G'(y)=\left(\frac{1}{\frac{(2y+1)^5}{(y^2+1)^{1/2}}}\right)\left(\frac{(2y+1)^5}{(y^2+1)^{1/2}}\right)'$$ The second term is a quotient rule.
$$G'(y)=\left(\frac{1}{\frac{(2y+1)^5}{(y^2+1)^{1/2}}}\right)\left(\frac{(2y+1)^5{'}(y^2+1)^{1/2}-(y^2+1)^{1/2}{'}(2y+1)^5}{(y^2+1)^{1/2*2}}\right)$$ Now differentiate the various terms, and recognize chain rule.
$$(2y+1)^5{'}=(5)(2y+1)^4(2)$$
$$(y^2+1)^{1/2}{'}=(\frac{1}{2})(y^2+1)^{-1/2}(2y)$$
$$G'(y)=\left(\frac{1}{\frac{(2y+1)^5}{(y^2+1)^{1/2}}}\right)\left(\frac{(10)(2y+1)^4(y^2+1)^{1/2}-(y)(2y+1)^5}{(y^2+1)^{1}}\right)$$ Simplify the expression. First, I move $(y^2+1)^{1/2}$ below the denominator.
$$G'(y)=\left(\frac{1}{\frac{(2y+1)^5}{(y^2+1)^{1/2}}}\right)\left(\frac{(10)(2y+1)^4-(y)(2y+1)^5}{(y^2+1)^{-1/2}(y^2+1)^{1}}\right)$$ Combine the denominator terms.
$$G'(y)=\left(\frac{1}{\frac{(2y+1)^5}{(y^2+1)^{1/2}}}\right)\left(\frac{(10)(2y+1)^4-(y)(2y+1)^5}{(y^2+1)^{1/2}}\right)$$ Multiply the first term in the expression (the complex fraction term) by the reciprocal of the denominator to eliminate the complex fraction.
$$G'(y)=\left((1)*\frac{(y^2+1)^{1/2}}{(2y+1)^5}\right)\left(\frac{(10)(2y+1)^4-(y)(2y+1)^5}{(y^2+1)^{1/2}}\right)$$ Multiply both of these terms. Note that $(y^2+1)^{1/2}$ is in both the numerator and denominator and therefore cancel.
$$G'(y)=\left(\frac{(y^2+1)^{1/2}[(10)(2y+1)^4-(y)(2y+1)^5]}{(2y+1)^5(y^2+1)^{1/2}}\right)$$
$$G'(y)=\left(\frac{(10)(2y+1)^4-(y)(2y+1)^5}{(2y+1)^5}\right)$$ Now to cancel the $(2y+1)^x$ terms like so:
$$G'(y)=\left(\frac{(10)(2y+1)^4}{(2y+1)^5}\right)-\left(\frac{(y)(2y+1)^5}{(2y+1)^5}\right)$$
$$G'(y)=[(10)(2y+1)^{4-5}]-[(y)(2y+1)^{5-5}]$$
$$G'(y)=[(10)(2y+1)^{-1}]-[(y)(2y+1)^{0}]$$
$$G'(y)=[(10)(2y+1)^{-1}]-(y)$$ Exponent property on $(2y-1)^{-1}$
$$G'(y)=\frac{(10-y)}{(2y+1)^{1}}$$
$$G'(y)=\frac{(10-y)}{(2y+1)}$$ The textbook answer is:
$$G'(y)=\frac{10}{2y+1}-\frac{y}{y^2+1}$$ It looks like I'm missing a term. But I'm not sure how. Thanks for your help.","['derivatives', 'calculus']"
2070991,Is $\sum\limits_{n=1}^{\infty}\frac{1}{n^k+1}=\frac{1}{2} $ for $k \to \infty$?,"This series  : $\displaystyle\sum_{n=1}^{\infty}\frac{1}{n^k+1}$ is convergent for every $k>1$ , it's seems that it has a closed form for every $k >1$ , some calculations here in wolfram alpha show to me that the sum approach to $\frac{1}{2}$ for large $k$ , My question here is : Question: Does $\displaystyle\sum_{n=1}^{\infty}\frac{1}{n^k+1}\to\frac{1}{2} $ for $k \to \infty$ ?","['sequences-and-series', 'convergence-divergence']"
2071094,Unique representation of a vector,"In a book I am reading the author states without proof that in an $n$-dimensional vector space $X$, the representation of any $x$ as a linear combination of a given basis $e_{1},e_{2},...,e_{n}$ is unique. How to proof that?","['linear-algebra', 'vectors', 'vector-spaces']"
2071154,Higher-order partial derivatives for functions defined $\mathbb{R}^n \rightarrow \mathbb{R}^m$.,"So far I've only seen examples of high-order (ie: second, third) partial derivatives of functions $f:\mathbb{R^n} \to \mathbb{R}$, but not any with $f:\mathbb{R^n} \to \mathbb{R^m}$. Is there a reason for this? Is this because the definition of a partial derivative is a function $f: \mathbb{R^n} \to \mathbb{R}$? Merry Christmas!",['multivariable-calculus']
2071158,If $a^4+b^4+c^4=3$ so $\sum\limits_{cyc}\frac{1}{3-2ab}\leq3$,"Let $a$ , $b$ and $c$ be non-negative numbers such that $a^4+b^4+c^4=3$ . Prove that: $$\frac{1}{3-2ab}+\frac{1}{3-2ac}+\frac{1}{3-2bc}\leq3$$ I tried SOS, C-S and more, but without success. By brute force methods with computer we can get that this inequality is true, but I am looking for an human proof, which we can release during a competition. Thank you!","['algebra-precalculus', 'contest-math', 'inequality']"
2071159,Struggling to understand epsilon-delta,"The definition of a limit is: $\lim_{x\to a}f(x)=L$ if for every $\epsilon > 0$ there is a $\delta > 0$ so that whenever $0 < \lvert x - a \rvert < \delta$ we have $\lvert f(x) - L \rvert < \epsilon$ Now it seems pretty intuitive. But I am hung up on a few problems: Many pictures show something like this: epsilon-delta This seems intuitive at first and it demonstrates that $\lvert x - a \rvert$ and $\lvert f(x) - L \rvert < \epsilon$ are not necessarily the same (as the graph can be deceptive, especially if $f(x)$ is a straight line) as when you are projecting from $L$ to the graph down to $a$, $\lvert x - a \rvert$ and $\lvert f(x) - L \rvert$ will be different. The problem in my understanding became apparent when I saw a similar graph in a textbook where the projected lines were not $\lvert f(x) - L \rvert$, but projection for aesthetic purposes and that it was bounded by $\lvert f(x) - L \rvert$. I then realized I don't get it geometrically at all (Google ""mooculus"", page 20). I don't understand what the ""verification"" in the proof is. It seems to be a tautology. For example take $\lvert f(x) - L \rvert < \epsilon \Longrightarrow \lvert (3x - 1) - 2 \rvert < \epsilon$. You will eventually get to $\lvert x - 1 \rvert < \epsilon/3$. Then the proof is ""completed"" by showing that $\lvert x - a \rvert < \delta \Longrightarrow \lvert x - 1 \rvert < \epsilon/3 \Longrightarrow \lvert f(x) - L \rvert < \epsilon$. But $\delta$ is taken to be $\epsilon/3$. It seems to be the equivalent of demonstrating that $x + 1 = 2$ by plugging in $-3$ into it. The proof starts by either assuming the limit exists or doesn't exist. In fact, I've found many textbooks or teachers to take this approach: Think of it as a game. You give me an $\epsilon > 0$ and I can give you a $\delta > 0 $... But they typically omit a glaring part: that if this doesn't hold, the limit doesn't exist. Also glaringly missing, I haven't seen an example of a proof that isn't simply ""proving"" or ""showing"" the premise that is already assumed! How would you for example use the epsilon-delta definition to show a limit doesn't exist if you don't already know in advance it is the case? To extend on number 3, I'm aware that you must choose an $\epsilon$ and that if you prove it for one, you prove it for all. However, the catch is in cases where $\lvert x - a \rvert$ needs to be restricted (i.e, $\lvert x - a \rvert < 1$) $1$ is typically chosen but this does not work in all cases! I have no intuition on how to choose an $\epsilon$ let alone know if I'm simply doing something wrong or if the limit does not exist. That is, proving the negative seems more difficult. Can someone explain it in a different way? I've resorted to many different .edu sources, free online textbooks and even questions on this site and the pedagogy doesn't seem to be reaching me.","['calculus', 'limits']"
2071174,A fair coin is tossed repeatedly and independently until two consecutive heads or two consecutive tails appear? [duplicate],"This question already has an answer here : Tossing a fair coin until two consecutive tosses are the same (1 answer) Closed 4 years ago . A fair coin is tossed repeatedly and independently until two consecutive heads or two consecutive tails appear . What is the probability of the number of tosses ? I tried it as : For success, either we end up getting HH,THH,HTHH,............ TT,HTT,THTT,............. Then, Add up both the successes. Am I right with my understanding ?","['combinations', 'combinatorics', 'probability']"
2071186,Discrepancy in differentiating: $y=\tan^{-1}\left(\frac{\sqrt{1+\sin{x}}+\sqrt{1-\sin{x}}}{\sqrt{1+\sin{x}}-\sqrt{1-\sin{x}}}\right)$,"$$y=\tan^{-1}\left(\frac{\sqrt{1+\sin{x}}+\sqrt{1-\sin{x}}}{\sqrt{1+\sin{x}}-\sqrt{1-\sin{x}}}\right)$$
$$\sqrt{1+\sin{x}}=\sin{\frac{x}{2}}+\cos{\frac{x}{2}}$$
$$\sqrt{1-\sin{x}}=\sin{\frac{x}{2}}-\cos{\frac{x}{2}}$$
$$y=\tan^{-1}\left(\tan{\frac{x}{2}}\right)$$Differentiating, we get: $$\frac{dy}{dx}=\frac{1}{2}$$
But taking: $$\sqrt{1-\sin{x}}=\cos{\frac{x}{2}}-\sin{\frac{x}{2}}$$
$$y=\tan^{-1}\left(\cot{\frac{x}{2}}\right)$$
Differentiating, we get: $$\frac{dy}{dx}=\frac{-1}{2}$$
I figured that I needed to use absolute value for the simplification of $\sqrt{1-\sin{x}}$, i.e. $\sqrt{1-\sin{x}}=\left|\sin{\frac{x}{2}}-\cos{\frac{x}{2}}\right|$. Subsequently, I put the below functions into Wolfram Alpha's input box to differentiate: $$y_1=\tan^{-1}\left(\frac{\sqrt{1+\sin{x}}+\sqrt{1-\sin{x}}}{\sqrt{1+\sin{x}}-\sqrt{1-\sin{x}}}\right)$$
And
$$y_2=\tan^{-1}\left(\frac{\left|\sin{\frac{x}{2}}+\cos{\frac{x}{2}}\right|+\left|\sin{\frac{x}{2}}-\cos{\frac{x}{2}}\right|}{\left|\sin{\frac{x}{2}}+\cos{\frac{x}{2}}\right|-\left|\sin{\frac{x}{2}}-\cos{\frac{x}{2}}\right|}\right)$$
The answers should ideally match, but they dont. $\frac{dy_1}{dx}=\frac{-1}{2}\neq\frac{dy_2}{dx}$ Why don't they?","['derivatives', 'trigonometry', 'calculus', 'proof-verification', 'absolute-value']"
2071195,How do I solve this problem based on induction?,"Let $a_0 = 0, a_1 = 1$ and $ a_k = a_{k-1} + a_{k-2}$ for $n \ge 3$. Let $b_0 = 2, b_1 = 1$ and $b_n = a_{n+2} - a_{n-2}$. Prove by induction that $a_{2n} = a_nb_n$. For $n=1$, $a_2 = 1$ and $a_1b_1=1$. For $n=2$, $a_4 = 3$ and $a_2b_2=3$. Let's assume it to be true for $n=k-1$ and $n=k$ Then $a_{2k-2} = a_{k-1}b_{k-1}$
and $a_{2k} = a_{k}b_{k}$ $=> a_{2k-2} + a_{2k-1} = a_{k}b_{k}$ $=> a_{k-1}b_{k-1} + a_{2k-1} = a_{k}b_{k}$ $=> a_{2k-1} = a_{k}b_{k}-a_{k-1}b_{k-1}  $ $=> 2a_{2k} + a_{2k-1} = 3a_{k}b_{k}-a_{k-1}b_{k-1}  $ $=> 2a_{2k} + a_{2k-1} = 3a_{k}b_{k}-a_{k-1}b_{k-1}  $ $=> a_{2k} + a_{2k+1} = 3(a_{k+1}-a_{k-1})(a_{k+2}-a_{k-2})-a_{k-1}(a_{k+1}-a_{k-3})  $ $=> a_{2k+2} = 3(a_{k+1}-a_{k-1})(a_{k+3}-a_{k+1}-a_{k-1}+a_{k-3})-a_{k-1}a_{k+1}+a_{k-1}a_{k-3}  $
$=> a_{2k+2} =2a_{k+1}a_{k+3} - 3(a_{k+1})^2 +3a_{k+1}a_{k-3} - 3a_{k-1}a_{k+3} + 3(a_{k-1})^2 - 2a_{k-1}a_{k-1} +a_{k+1}a_{k+3} - a_{k+1}a_{k-1}$
$=> a_{2k+2} =2a_{k+1}a_{k+3} - 3(a_{k+1})^2 +3a_{k+1}a_{k-3} - 3a_{k-1}a_{k+3} + 3(a_{k-1})^2 - 2a_{k-1}a_{k-1} +a_{k+1}(a_{k+3} - a_{k-1})$
$=> a_{2k+2} =2a_{k+1}a_{k+3} - 3(a_{k+1})^2 +3a_{k+1}a_{k-3} - 3a_{k-1}a_{k+3} + 3(a_{k-1})^2 - 2a_{k-1}a_{k-1} +a_{k+1}b_{k+1}$ I have to prove here that $$2a_{k+1}a_{k+3} - 3(a_{k+1})^2 +3a_{k+1}a_{k-3} - 3a_{k-1}a_{k+3} + 3(a_{k-1})^2 - 2a_{k-1}a_{k-1} = 0$$
I hope there is an easier method than this.","['algebra-precalculus', 'induction']"
2071226,"Spivak Calculus, Chapter 9, Exercise 22(b)","Assuming $f : \mathbb R \to \mathbb R$ is differentiable, how can one prove that
$$
f'(x) = \lim_{h,k \to 0^+} \frac{f(x+h)-f(x-k)}{h+k},
$$
an alternate expression to the usual limit definition of the derivative $f'(x)=\lim_{h \to 0^+} \frac{f(x+h)-f(x)}h$? I figured the problem out for the special case of $k=h$: add and subtract $f(x)$ in the numerator of the expression. An $\epsilon$-$\delta$ requirement was not required to prove that $\frac{f(x+h)-f(x-h)}{2h} = \frac{f(x+h)-f(x)}h$. This was part (a) of Exercise 9.22. I also thought about sending $k \to 0^+$ first to simplify the function, and since $f$ is differentiable (therefore continuous), sending $k \to 0^+$ would reduce the original expression down to the familiar $\lim_{h \to 0^+} \frac{f(x+h)-f(x)}{h}$. But then again we can't send $h \to 0^+$ and $k \to 0^+$ separately.","['derivatives', 'limits']"
2071228,When is a function of a product the product of functions?,"I am trying to figure out what are all functions, $f, g$ that satisfy the following: $f(ab) = g(a)g(b) \forall a,b$ In other words, a function of a product of arguments is always the product of a (possibly different) function of both of the arguments separately? For example, I know that $f(x) = kx^n, g(x) = \sqrt{k}x^n$ for all positive $k$ and real $n$ holds true. Is there a way to find other such functions, or prove that these are the only such functions? (As pointed out, the functions should also be continuous and R $\rightarrow$ R.","['functions', 'functional-equations']"
2071236,Intuition for connectedness of positive definite matrices,"I have read, even on this website, about connectedness of positive definite matrices. The proof is given in the form of convex equation. I wonder how to understand it intutively. e.g., as an analogy, a parabola is convex and continuous, so, its connectedness is obvious. How to understand positive definite matrices on this line. At first thought, I felt, in the vecor space, the next placed matrix might not be positive definite, as we need all leading principal minors to be positive. I felt, maybe this condition would be satisfied only after certain distance in the vector space. So, I felt these matrices would be disconnected. But of course I was wrong. I wonder how to build intuition for connectedness along these lines. Thanks.","['matrices', 'positive-definite']"
2071246,"How many rounds are required in a ""Swiss tournament sorting algorithm""?","You're organizing a Swiss-style tournament with N players of a game. The game is a two-player game, and it results in one winner and one loser. The players are totally ordered by skill, and whenever two players play against each other, the more skilled player always wins. In each tournament round, each player can play only one game. Going into the tournament, nothing is known about the relative skill levels of the players. The pairings for each round are not decided until the previous round has finished, so you can use the results from previous rounds when you're deciding how to pair the players up. You are not required to follow any traditional pairing rules. Your goal is to completely determine the ranking of all $N$ players. What is $Swiss(N)$, the number of rounds required in the worst case? Results for small $N$: If $N$ is $0$ or $1$, the number of rounds required is $0$. For $N = 2$, the number of rounds required is $1$. For $N = 3$, it can be seen that $2$ rounds are not sufficient. If you use only $2$ rounds, then there must be at least two players who only play one game each. If these players both win their games, then their relative skill level is unknown. However, $3$ rounds are sufficient, because this is enough to play out all possible pairings (a round-robin tournament). So the number of rounds required is $3$. For $N = 4$, $3$ rounds are necessary (because they are necessary for $N = 3$) and sufficient (because this is enough for a round-robin tournament). Some sub-questions: We can come up with a logarithmic lower bound for $Swiss(N)$ using information theory. The complete ranking of all $N$ players contains $\log_2(N!)$ bits of information, but each tournament round only gives you $\lfloor N/2 \rfloor$ bits of information, so at least $\log_2(N!) / \lfloor N/2 \rfloor$ rounds are required. Is there a better lower bound? We can come up with a linear upper bound for $Swiss(N)$ by simply pairing every player against every other player (a round-robin tournament). This gives us an upper bound of $N$ for odd $N$, and $N - 1$ for even $N$. Is there a better upper bound? In particular, is there an algorithm which uses $o(N)$ rounds? What is the asymptotic behavior of $Swiss(N)$? Is it logarithmic, linear, or something in between?","['sorting', 'algorithms', 'discrete-mathematics']"
2071253,To prove the inequality:- $\frac{4^m}{2\sqrt{m}}\le\binom{2m}{m}\le\frac{4^m}{\sqrt{2m+1}}$,"Problem Statment:- Prove:-$$\dfrac{4^m}{2\sqrt{m}}\le\binom{2m}{m}\le\dfrac{4^m}{\sqrt{2m+1}}$$ My Attempt:- We start with $\binom{2m}{m}$ (well that was obvious), to get $$\binom{2m}{m}=\dfrac{2^m(2m-1)!!}{m!}$$ Now, since $2^m\cdot(2m-1)!!\lt2^m\cdot2^m\cdot m!\implies \dfrac{2^m\cdot(2m-1)!!}{m!}\lt 4^m$ $$\therefore \binom{2m}{m}=\dfrac{2^m(2m-1)!!}{m!}\lt4^m$$ Also, $$2^m\cdot(2m-1)!!\gt2^m\cdot(2m-2)!!\implies 2^m(2m-1)!!\gt2^m\cdot2^{m-1}\cdot(m-1)!\\
\implies \dfrac{2^m\cdot(2m-1)!!}{m!}\gt\dfrac{4^m}{2m}$$ So, all I got to was $$\dfrac{4^m}{2m}\lt\binom{2m}{m}\lt4^m$$ So, if anyone can suggest me some modifications to my proof to arrive at the final result, or just post a whole different non-induction based proof.","['algebra-precalculus', 'inequality', 'binomial-coefficients']"
2071280,Functions That Also Satisfy Binomial Distribution Recurrence,"Consider a function $f(k,n,p)$ that satisfies the following recurrence: $$
f(k,n,p) = p\cdot f(k-1, n-1, p) + (1-p) \cdot f(k, n-1, p)\,.
$$ This is of course the probability of $k$ successes from $n$ trials, easily satisfied by: $$f(k,n,p) = {n \choose k}p^k(1-p)^{n-k}\,,$$ but I was wondering: Are there other examples of functions that satisfy this recurrence relation, for $k,n \in \mathbb{N}$ and $p \in \mathbb{R^+}$?","['recurrence-relations', 'discrete-mathematics']"
2071282,Closed linear span in a Hilbert space defined as the sums of unconditionally convergent series,"Let $H$ be a Hilbert space over complex numbers and let $U \subset H$. Then, the closed linear span of $U$ is defined as $$T = \left\{ \sum_{u \in U} c_n u \mid  c_n \text{ complex } , \sum_{u \in U} c_n u  \text{ converges unconditionally} \right\}.$$ But, how do I see that this set is closed?","['functional-analysis', 'hilbert-spaces']"
2071290,"Prob. 5, Chap. 3, in Baby Rudin, 3rd ed: $\lim\sup_{n\to\infty}\left(a_n+b_n\right)\leq\lim\sup_{n\to\infty}a_n+\lim\sup_{n\to\infty}b_n$.","Here's Prob. 5, Chap. 3 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: For any two real sequences $\left\{ a_n \right\}$ , $\left\{ b_n \right\}$ , prove that $$\lim\sup_{n\to\infty}\left( a_n + b_n \right) \leq \lim\sup_{n\to\infty} a_n + \lim\sup_{n\to\infty} b_n,$$ provided the sum on the right is not of the form $\infty-\infty$ . My effort: Let us put $$ c_n \colon= a_n + b_n \tag{Definition A} $$ for all $n \in \mathbb{N}$ , and let's put $$ 
\begin{align} 
a^* & \colon= \lim\sup_{n\to\infty} a_n, \\
b^* & \colon= \lim\sup_{n\to\infty} b_n, \\
c^* & \colon= \lim\sup_{n\to\infty} c_n. 
\end{align}
\tag{Definitions B} $$ We need to show that $$ c^* \leq a^* + b^*. \tag{0} $$ So let's suppose that $$ c^* \not\leq a^* + b^*. $$ Then we have $$ c^* > a^* + b^*, $$ and so $$ c^*- b^* > a^*,$$ and let's take a real number $x$ such that $$ c^*- b^* > x > a^*. \tag{0} $$ Then as $ x > a^*$ , so by Theorem 3.17 (b) in Baby Rudin, we can find a natural number $N_1$ such that $$ x \geq a_n \tag{1}$$ for all $n > N_1$ . Now from (0) above, as $$ c^*-x > b^*, $$ so we can find a real number $y$ such that $$c^*-x > y > b^*. \tag{2} $$ Then as $y > b^*$ , so again by Theorem 3.17 (b) in Baby Rudin, we can find a natural number $N_2$ such that $$ y \geq b_n \tag{3}$$ for all $n>N_2$ . Now from (2) above we can conclude that $$ c^* > x+y, \tag{4} $$ and hence from (1) and (3) we also have $$ x+y \geq a_n + b_n = c_n, $$ for all $n > \max \left\{\ N_1, N_2 \ \right\}$ , which in turn implies that $$ x+y \geq \lim\sup_{n \to \infty} c_n, $$ that is [Please Refer to (Definition A) and (Definitions B) above.], $$ x+y \geq c^*, $$ which, in view of (4) above, gives rise to a contradiction to our choice of $c^*$ as the limit superior of the sequence $\left\{c_n\right\}$ . Is this proof correct? If not, then where is it deficient?","['real-analysis', 'limsup-and-liminf', 'sequences-and-series', 'analysis']"
2071308,What is the dimension of a topology?,"We know about the manifold dimensions of topological objects. I have a query, as every vector space has Hamel basis and the dimension of a vector space is defined as the number of elements in the basis set. My question is why this definition of dimension is not used to defined the basis of a topological space. Is that the reason, a topological space may not have same number of elements in its every basis?","['general-topology', 'linear-algebra', 'vector-spaces']"
2071380,A variant of the Knight's tour problem,"The knight's tour problem is a famous problem in chess and computer science which asks the following question: can we move the knight on an $n \ \times \ n$  chessboard such that it visits every square exactly once? The answer is yes iff $n\geq5$. Additionally, there are algorithms which can solve it in $O(n^2)$ time. I have two variations of it discussed below. Fix an $n \ \times \ n$ chessboard. In this variant, instead of one knight we have $m$ knights. These knights take turns moving (ie., one knight moves, after that another one, once all of them have moved, the first one moves again and the cycle repeats itself). What is the largest $m$ such that for some initial starting position, each knight can tour the board (as described in the first paragraph) without threatening any other knight? In other words, there is no ""instance"" of the chessboard in which one knight may capture another knight (e.g., knight $A$ cannot be, for example, two cells right and one cell up from another knight $B$ when it is his turn). Note that there is obviously an upper bound on $m$ (e.g., $n^2$). In essence, I'm looking for a function $f:\mathbb{N}_{\geq 5} \to \mathbb{N}$ which assigns to each $n$ the largest possible $m$. My second variation is exactly the same, except now there are no restrictions on the turns. After one knight moves, any knight can move, including the one that has moved prior.  I do not know how these variations relate to each other; perhaps they are equivalent. Also, as a function of $n$ what would the time complexity of an optimal algorithm be? Can either of these variants be solved in polynomial time?","['chessboard', 'graph-theory', 'knight-tours', 'discrete-mathematics']"
2071402,Complex integrals,"Let $f$ be an analytic function, $C^1$ on an open and connected set $U$. for all $z\in U$ : $|f(z)-1|<1$. Prove that $\int_\gamma \frac{f'(z)}{f(z)}dz =0$ for every closed curve (closed curve- $\gamma(a)=\gamma(b)$) in $U$. Any hints ?","['complex-analysis', 'integration', 'complex-numbers', 'complex-integration']"
2071403,Minimal specification of isometry in terms of norm preservation,"Let $V,W$ be $n$-dimensional (real) inner product spaces, and let $T:V \to W$ be a linear map. Let $v_1,...,v_n$ be a basis of $V$. It is easy to see that if $|T(v)|_W=|v|_V$ for every $v \in \{v_1,...,v_n,v_1+v_2,v_1+v_3,...,v_{n-1}+v_n\}$, then $T$ is an isometry (a proof is provided below). In other words, after choosing wisely $k(n):=\frac{n(n+1)}{2}$ vectors, it is enough to verify $T$ preserves the norms of these special vectors, in order to conclude it's an isometry. Question: Is there no way to choose less than $k(n)$ vectors, in such a way that every linear map which preserves their norms is an isometry? I believe we cannot choose less vectors. I have some ""convincing evidence"" for the cases $n=1,2,3$ (see below), but I am not sure how to give a rigorous argument. Note that a ""wise choice"" of vectors does not have to be of the form of some vectors, and linear combinations of them (I do feel this it the most efficient method, but I don't see how to prove this). Even if we prove that this is the case, than we need to show we cannot do better than to work with only orthonormal bases. The partial ""evidence"": $n=1: k=1$. Obvious $n=2: k=3$. Take $V=W=\mathbb{R}^n$ with its standard inner product. Then, $T(e_1)=e_1, T(e_2)=\frac{e_1+e_2}{\sqrt 2}$ is a counter example. $n=3: k=6$. Then any matrix of the form $$ \begin{pmatrix} c & s & x \\  -s & c & y  \\  0 & 0 & z \\\end{pmatrix} $$ where $c^2+s^2=1,x^2+y^2+z^2=1, sx+cy=0$ preserves the norms $e_1,e_2,e_1+e_2,e_3,e_2+e_3$ but it's an isometry only if $|z|=1,x=y=0$. Proof that $k(n)=\frac{n(n+1)}{2}$ vectors are enough: Noting that $$ \langle u,v \rangle = \frac{1}{2}(|u+v|^2 - |u|^2 - |v|^2) ,$$ we obtain $$ \langle Tv_i,Tv_j \rangle = \frac{1}{2}(|Tv_i+Tv_j|^2 - |Tv_i|^2 - |Tv_j|^2) = \frac{1}{2}(|T(v_i+v_j)|^2 - |v_i|^2 - |v_j|^2)  $$
$$ = \frac{1}{2}(|v_i+v_j|^2 - |v_i|^2 - |v_j|^2) = \langle v_i,v_j \rangle,$$ thus $T$ is an isometry.","['isometry', 'combinatorics', 'symmetry', 'linear-algebra']"
2071420,Mathematical meaning of Residual Sum of Squares,"I know what is Residual Sum of Squares (RSS) . The problem is I don't understand why it works. For example, you have a set of descreet values, and you want to describe them with linear, quadratic or any other function. To choose the best function you calculate the RSS (sum of variances) instead sum of deviances. Why to choose the most precise function you have to sum variances (instead of deviances)? How is that prooved?","['statistics', 'standard-deviation', 'variance']"
2071421,How to use the 'modulus' operator?,"This is a problem from BdMO $2012$ Dhaka region Question Paper : The product of a number with itself is called its square. For example,
  $2$ multiplied by $2$ is $4$, so $4$ is the square of $2$. If you take a square
  number and multiply it with itself, what will be the largest possible
  remainder if the product is divided by $10$? I came up with this: $$x^4 \mod {10}$$ I know that the modulus (%) operator calculates the remainder of a division. And that it can be used to see, suppose, whether $N$ is a multiple of $M$ or not. Nothing more than that. I am much familiar with mod because of my programming experience with mid-level languages like C and C++. It was not until later that I came to know that modulus is used in mathematics as well. Now, how to use  the 'modulus' operator? How can I use this to go further into solving this problem?","['number-theory', 'contest-math', 'elementary-number-theory']"
2071458,Prove that $\lim_{x \to 1}(2x+3)=5$,"Please check my proof There exist $\delta $ such that $0<x<\delta \rightarrow |2x+3-5|=|2x-2|<\epsilon $ $$0<x<\delta \rightarrow 2|x-1|< \epsilon $$
$$ \rightarrow |x-1|<\frac{\epsilon }{2}$$ Choose $\delta =\frac{\epsilon }{2}$ For x is real number and 
$$0<x<\delta \rightarrow |2x+3-5|=|5-5|=0<\delta =\frac{\epsilon }{2}$$ Then limit is 5","['epsilon-delta', 'real-analysis', 'proof-verification', 'limits']"
2071473,Deriving a Series Representation of the Bessel Function of the First Kind,"I've tried to use an integral representation of the Bessel Function of the First Kind $J_n(x)$ to derive a series representation of the function. My end result is pretty close to the answer that it should be, but it still doesn't quite match up and I'd be grateful for anybody that could point out where my work is faulty. \begin{align}
J_n(x) &= \frac{1}{2\pi}\int_0^{2\pi}\cos(n\phi-x\sin\phi)d\phi\\
&= \frac{1}{2\pi}\int_0^{2\pi}e^{in\phi}e^{-ix\sin\phi}d\phi \tag1\\ 
&=\frac{1}{2\pi}\int_0^{2\pi}\left(e^{i\phi}\right)^ne^{-\frac{x}{2}\left(e^{i\phi}-e^{-i\phi}\right)}d\phi \tag2
\end{align} In $(1)$, I used the fact that the integral of the imaginary part of the integrand over a complete period is equal to $0$. A parametrisation of the unit circle in the complex plane is given by $z=e^{i\phi}, \phi\in(0,2\pi)$, thus $e^{-i\phi}=\left(e^{i\phi}\right)^{-1}=\frac{1}{z}$. Letting $\Gamma$ be the unit circle in the complex plane traversed in the counterclockwise direction, $(2)$ becomes \begin{align}
J_n(x) &= \frac{1}{2\pi}\oint_\Gamma z^n e^{-\frac{x}{2}z}e^{\frac{x}{2z}}dz\\
&= \frac{1}{2\pi}\oint_\Gamma z^n e^{-\frac{x}{2}z}\sum_{k=0}^{\infty}\frac{x^k}{2^kk!z^k}dz \tag3\\
&=\frac{1}{2\pi}\sum_{k=0}^{\infty}\frac{x^k}{2^kk!}\oint_\Gamma z^{n-k}e^{-\frac{x}{2}z}dz\\
&=i\sum_{k=0}^{\infty}\frac{x^k}{2^kk!}\text{Res}\left(z^{n-k}e^{-\frac{x}{2}z},0\right) \tag4
\end{align} $(3)$ uses the Laurent series expansion of $e^{\frac{1}{z}}$ and $(4)$ uses the Residue Theorem. Let $m:=k-n\rightarrow k=m+n$. Then the Residue of $z^{-m}e^{-\frac{x}{2}z}$ is only non-zero for strictly positive values of $m$ and $(4)$ becomes \begin{align}
J_n(x) &= i\sum_{m=1}^{\infty}\frac{x^{m+n}}{2^{m+n}(m+n)!}\text{Res}\left(z^{-m}e^{-\frac{x}{2}z},0\right)\\
&= i\sum_{m=1}^{\infty}\frac{x^{m+n}}{2^{m+n}(m+n)!}\frac{1}{(m-1)!}\lim\limits_{z\rightarrow 0}\frac{d^{m-1}}{dz^{m-1}}e^{-\frac{x}{2}z}\\
&=i\sum_{m=1}^{\infty}\frac{x^{m+n}}{2^{m+n}(m+n)!}\frac{(-1)^{m-1}}{(m-1)!}\frac{x^{m-1}}{2^{m-1}} \tag5
\end{align} Letting $r=m-1\rightarrow m=r+1$, $(5)$ becomes \begin{align}
J_n(x)=i\sum_{r=0}^{\infty}(-1)^r\frac{x^{2r+n+1}}{2^{2r+n+1}r!(r+n+1)!}\tag6
\end{align} However, comparing this with the known series representation, the $i$ shouldn't be there (obviously, since $J_n(x)$ is a real function) and the sum (disregarding the $i$ for a moment) is actually a series representation of $J_{n+1}(x)$, not $J_n(x)$. I've been over this calculation a few times, and I simply cannot find where I made a mistake that either allowed the $i$ to creep in there or that raised the order of the Bessel Function that I am trying to find a series representation for. Any ideas?","['bessel-functions', 'complex-analysis', 'integration', 'contour-integration', 'power-series']"
2071495,$\binom{28}{2}+\binom{28}{5}+\binom{28}{8}+\cdots+\binom{28}{26}=\frac{2}{3}(2^{27}+1)$,"I need to prove the following identity 
$$
\binom{28}{2}+\binom{28}{5}+\binom{28}{8}+\cdots+\binom{28}{26} = \frac{2}{3}(2^{27}+1).
$$ I have tried to use the fact that $\binom{m}{n}=\binom{m}{m-n}$ but it doesn't help.","['combinatorics', 'recreational-mathematics', 'summation', 'sequences-and-series', 'discrete-mathematics']"
2071503,Are these two statements true or false?,"For all sets $A,B$ we have that $|A \cup B|=|A|+|B|$ whereby $|\text{ }|$ stands for the cardinality. For all vectors $v,w \in \mathbb{R}^{2}$ the vector $u= \left \langle v,w \right \rangle v \text{ } - \left\|v\right\|^{2}w$ is located vertically at $v$ whereby $\left \langle , \right \rangle$ stands for the euclidean scalar product on $\mathbb{R}^{2}$ I think first statement is false because to be true it must be subtracted by $|A \cap B|$ and also this must be finite and it's not stated in the task either. Or that is wrong? I still say it's false because this is unclear from task still. About the other statement I have no idea at all : /","['cardinals', 'linear-algebra', 'elementary-set-theory', 'proof-explanation']"
2071506,Minimal Covariance of random variables.,"Now I was wondering if you have some bernulli random variables $X_1, X_2, X_3,\dots X_n$. That distribute on some $1/2$ probability (Ber($1/2$)), and all of their Cov are equal, meaning that $\text{Cov}(X_i,X_j) = \text{Cov}(X_k, X_l)$ for every $i \neq j$, and every $k \neq l$. I'm asked to find how small can the $\text{Cov}(X_i,X_j)$  be for $i \neq j$, give an example for when the minimum is received. So I started by saying that $\text{Cov}(X_i,X_j) = \mathbb E [X_iX_j] - \mathbb E[X_i]\cdot \mathbb E[X_j] = \mathbb E[X_iX_j] - 0.25$ Basically I need now to minimize $\mathbb E [X_iX_j] $ but I'm having a hard time understanding what is exactly multiplying bernulli random variables. Thanks for any help!","['probability-theory', 'probability', 'covariance']"
2071513,"If you didn't already know that $e^x$ is a fixed point of the derivative operator, could you still show that some fixed point would have to exist?","Let's suppose you independently discovered the operator $\frac{d}{dx}$ and know only its basic properties (say, the fact it's a linear operator, how it works on polynomials, etc.) If you didn't know that $e^x$ was a fixed-point of this operator, would there be any way to (possibly nonconstructively) show that such a fixed point would have to exist? I'm curious because just given the definition of a derivative it doesn't at all seem obvious to me that there would be some function that's it's own derivative. Note that I'm not asking for a proof that $\frac{d}{dx} e^x = e^x$, but rather a line of reasoning providing some intuition for why $\frac{d}{dx}$ has a fixed-point at all. (And let's exclude the trivial fixed point of 0, since that follows purely from linearity rather than any special properties of derivatives).","['fixed-point-theorems', 'calculus']"
2071532,The sum of $\binom{n}{0}\binom{3n}{2n}-\binom{n}{1}\binom{3n-3}{2n-3}+\binom{n}{2}\binom{3n-6}{2n-6}+\cdots \cdots $,"The sum of $$\binom{n}{0}\binom{3n}{2n}-\binom{n}{1}\binom{3n-3}{2n-3}+\binom{n}{2}\binom{3n-6}{2n-6}-\cdots \cdots \cdots $$ $\bf{My\; Try::}$ We can write above sum  as $$\sum^{n}_{k=0}(-1)^k\binom{n}{k}\binom{3n-3k}{2n-3k} = \sum^{n}_{k=0}(-1)^k\binom{n}{k}\binom{3n-3k}{n}$$ So sum $$ = \sum^{n}_{k=0}(-1)^k\cdot \frac{n!}{k!\cdot (n-k)!}\times \frac{(3n-2k)!}{n!\cdot (2n-3k)!} = \sum^{n}_{k=0}(-1)^k \cdot \frac{(3n-2k)!}{k!\cdot (n-k)!\cdot (2n-3k)!}$$ Now i did not understand how can i solve it after that , help Required, Thanks",['combinatorics']
2071544,Application of Baire theorem for proof of non-existence of a universal function,"I have encountered the following problem: Prove that there is no continuous function $U\colon [\,0;1\,]\times[\,0;1\,]\rightarrow\mathbb{R}$, so that
$\forall f\in C[\,0;1\,], \quad|f(x)|\leq1\quad \forall x\in[\,0;1\,]$, exists such $y_f\in[\,0;1\,]$ so that $f(x)\equiv U(x,y_f)$. It ia advised to use Baire theorem, but I have no idea which sets to choose. What sould I try?","['functional-analysis', 'functions']"
2071563,Machine Learning: Good book for learning Probability and statistics? [duplicate],"This question already has answers here : What is the best book to learn probability? (13 answers) Closed 9 months ago . I am trying to learn machine learning and looking for a good book to understand probability and statistics from machine learning point of view and for the sake of understanding probability. Though I have studied probability in the past, I am still having a hard time to solve homework questions from Stats 110 course by Blitzstein. I think I am missing many concepts in probability theory as I didn't pay attention in my Probability classes.
So what I need is a good book which can kind of introduce me and at the same time refresh some concept left in my brain and provides good intuitive explanation of questions and their answers and provide very good but few important exercises to understand all the concepts. I checked many questions like this and I am still in a dilemma which book to consider: 1 - An Introduction to Probability Theory and Its Applications - William Feller 2 - A First Course in Probability - Sheldon Ross Which one would you recommend? Or if you have other good book in mind please do let me know.","['machine-learning', 'probability-theory', 'probability', 'statistics']"
2071574,"If $\sum a_n$ converges and $\left\{b_n\right\}$ is monotonic and bounded, then $\sum a_n b_n$ converges.","Here's Prob. 8, Chap. 3 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $\sum a_n$ converges, and if $\left\{ b_n \right\}$ is monotonic and bounded, prove that $\sum a_n b_n$ converges. My effort: Let $b = \lim_{n \to \infty} b_n$ . Let's first suppose that $\left\{ b_n \right\}$ is monotonically decreasing. Then $b_n \geq b$ for all $n$ . So we have $$ b_0 - b \geq b_1 - b \geq \cdots \geq 0.$$ Moreover the sequence of the partial sums of $\sum a_n$ is bounded and $\lim_{n \to \infty} \left( b_n - b \right) = 0$ . So by Theorem 3.42 in Rudin, we can conclude that $\sum a_n (b_n - b)$ converges. And since $a_n b_n = a_n (b_n - b) + a_n b$ and since $\sum a_n$ converges, therefore $\sum a_n b_n$ converges by Theorems 3.47 in Rudin. Now let's suppose that $\left\{ b_n \right\}$ is monotonically increasing. Then $\left\{ -b_n \right\}$ is monotonically decreasing and so $\sum a_n \left(-b_n\right)$ converges. Therefore by Theorem 3.47 in Rudin $\sum a_n b_n$ converges as well. Is my proof correct? If not, then where is it wanting?","['real-analysis', 'sequences-and-series', 'calculus', 'convergence-divergence', 'analysis']"
2071590,Series $e^1+e^2+\:...\:+e^n$,"Is there a way to calculate this not term-by term ? $$E_n =e^1+e^2+...+e^n$$ I tried developping : $$E_n = \sum^\infty_{k=0}\frac{1^k}{k!}+\sum^\infty_{k=0}\frac{2^k}{k!}+...+\sum^\infty_{k=0}\frac{n^k}{k!}$$ It can be shortened to : $$E_n = \sum^\infty_{k=0}\frac{1^k+2^k+...+n^k}{k!}$$ $$E_n = \sum^\infty_{k=0}\frac{\sum^n_{i=1}i^k}{k!}$$ Now, I don't know if there's a way to continue. I searched around the internet and stumbled across the Faulhaber formula. Does this can help ? Edit : ok i'm stupid I forgot about the geometric series :(","['exponential-function', 'sequences-and-series']"
2071600,Distribution of $Y(t)=B(t)$ when $\max\limits_{0\le s \le t}B(s)<a$ and $Y(t)=a$ otherwise,"If $B(t),t\ge0$ is a standard Brownian motion with initial condition $B(0)=0$ and $a>0$, let $Y(t)=B(t)$ when $\max\limits_{0\le s \le t}B(s)<a$ and $Y(t)=a$ otherwise. 
  Find the distribution function of $Y(t)$. Let $M(t)=maxB(s)$. Here I am confused if the question is asking me to find $F_Y(t)=P\{Y(t)\le y\}$ or $F_Y(t)=P\{Y(t)=B(t)|M(t)<a\}$ or  $F_Y(t)=P\{M(t)<a|Y(t)=B(t)\}$ or something else related to Bayes equation.","['stochastic-processes', 'probability-theory', 'brownian-motion', 'probability-distributions']"
2071625,Proving that a MLE is an asymptotically unbiased estimator,"Let $X_1,\ldots,X_n$ be a sample in a space with PDF $f_X(x; \theta) = \frac{3}{\theta^3}x^2 I(0\le x \le \theta)$ then caclulate the MLE for $\theta$ and prove that it is an asymptotically unbiased estimator. So far, I managed to calculate $$ \theta_m (\mathrm{MLE}) = \max(X_i),$$ but proving that it is an asymptotically unbiased estimator isn't working out. I've tried integrating $\max(X_i)$, but the integral ends up not working out for me. Should I still use an integral for proving that $\operatorname{E}(\theta_m) = \theta$ or not? And if so, how should I integrate/calculate it? Thanks.",['statistics']
2071644,prime sequence prove that $p_n\ne 5$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question i was preparing for olympiad then I got this question Let $$<p_1,p_2,,,,p_n,,,>$$ be a sequence of primes defined by$p_1=2$ and for $n\ge 1$,$p_{n+1}$ is the largest prime factor of $p_1p_2...p_n+1$.Prove that $p_n\ne 5$ for any $n$. I put $p_2=3,p_3=7$ I think sequence is increasing","['contest-math', 'prime-numbers', 'sequences-and-series']"
2071653,Functions with the property $\frac{d}{dx}(f(x)\cdot f(x))=f(2x)$,"I've noticed the derivative of $\sin^2 x$, $$\frac{d}{dx}\sin^2(x)=\sin 2x$$ and the same applies to $\sinh^2 x$, that is 
$$\frac{d}{dx}\sinh^2(x)=\sinh(2x)$$ I was wondering about the other functions of that property. (I posted a similar question about a week ago but I had an incorrect premise.) So these functions must satisfy $$(f^2(x))'=f(2x)\rightarrow 2f'(x)f(x)=f(2x)$$ The only way I could think of solving these was the power series method which is something I've never been good with. $$f(x)=\sum_{n=0}^{\infty} C_nx^n,$$ $$f'(x)=\sum_{n=0}^{\infty}(n+1)C_{n+1}x^n$$ plugging this $$2\sum_{n=0}^{\infty}(n+1)C_nC_{n+1}x^{2n}=\sum_{n=0}^{\infty}C_n(2x)^n.$$ and now I don't know what to do with that $x^{2n}$ term. Could someone help me find the general form of $f(x)$?","['ordinary-differential-equations', 'trigonometric-series', 'calculus']"
2071683,"For $x$ being irrational, why is $\lim_{m\to\infty}\lim_{n\to\infty}[\cos(n!\pi x)]^{2m}=0$?","Let $x \in \mathbb{R} \setminus \mathbb{Q}$. What is the value of $$\lim_{m \to \infty} \lim_{n \to \infty} \left[ \cos(n!\pi x) \right]^{2m}, \qquad (m,n \in \mathbb{N})$$ The answer given is $0$. I don't understand why it is so.","['real-analysis', 'trigonometry', 'limits']"
2071684,Understanding projective scheme as representable functor,"I cannot understand the notes that my teachers use for class. Theorem: let $S$ be a scheme and $E$ a quasicoherent $O_S$-module. The functor $$Sch_S^o\rightarrow Set$$ $$(\phi :X\rightarrow S)\mapsto \{[\phi ^*E\rightarrow L]\}$$ is representable by an $S$-scheme $\mathbb{P}(E)\rightarrow S$.
To elaborate on the meaning on the RHS of $\mapsto$ an $S$-scheme $X$ is mapped to the set of maps from $\phi ^*E$ to $L$, where $L$ is invertible, modulo isomorphisms i.e. two maps $a$ and $b$ are same iff $\exists s:L_1\rightarrow L_2$ such that $b=sa$. For a ring $A$, we denote $\mathbb{P}^n _A :=\mathbb{P} ((A^{(n+1)})^*)$. Firstly how is this seen as a quasicoherent sheaf on $\mathrm{Spec}(A)$
As an example they take $\mathbb{P} ^n _\mathbb{C}(\mathbb{C}) _{\mathbb{C}}$ which I don't understand. Firstly, can anyone guess the meaning of the last $\mathbb{C}$? I have given my guess below. The one in the brackets is talking about $\mathbb{C}$ points of the scheme. Apparently, the following sequence of equalities hold $\mathbb{P}^n _\mathbb{C}(\mathbb{C})_\mathbb{C}=\{\mathrm{Spec}(\mathbb{C})\mathrm{-morphisms.of. }\mathrm{Spec}(\mathbb{C})\mathrm{to.}\mathbb{P}^n _\mathbb{C}\}$ $=\{\mathrm{invertible.quotients.of.rank.1.of.}(\mathbb{C}^{n+1})^*\}=\{\mathrm{subspaces.of.rank.1.of.}\mathbb{C}^{n+1}\}$ which is indeed the projective space.
Can anyoune explain meaning of the first and second $=$? I think the first is just definition.
What would be obtained if we would take $\mathbb{P} ^n _\mathbb{C} (\mathbb{C})$ ? How actually to work with this (in my view) cumbersome definition? How is  $\mathrm{Proj}$ related to this?","['schemes', 'projective-schemes', 'algebraic-geometry']"
2071691,$\mathcal{L}^1$-integrability with restriction to a set $E$,"Let $E \in \Sigma$. We look at the measurable space $(E,\Sigma_E)$, where $\Sigma_E = \{E \cap F: F \in \Sigma\}$, which is a $\sigma$-algebra. We consider integration to this space. Let $f$ be a measurable function and denote by $f_E$ its restriction to $E$. Now, I want to prove by standard machinery that
\begin{align}
f_E \in \mathcal{L}^1(E,\Sigma_E,\mu_E) \iff \mathbb{1}_E\ f \in \mathcal{L}^1(S,\Sigma,\mu),
\end{align}
in which case the identity $\mu_E(f_E) = \mu(\mathbb{1}_E\ f):=\int_E f\ d\mu$ holds. Now, I am wondering if we can use the fact that $f_E = f\ \mathbb{1}_E$ in the elaboration of the standard machinery. If this is the case, the results follows easily. Otherwise, I do not know how to prove the above identity. Any suggestions?","['probability-theory', 'integration', 'measure-theory']"
2071697,Prove this bizarre integral:$\int_{0}^{\infty}{\sin(x^{\pi\over4})\over x[\cos(x^{\pi\over 4})+\cosh(x^{\pi \over 4})]}dx=1$,"Integrate this bizarre integral, $$I=\int_{0}^{\infty}{\sin(x^{\pi\over4})\over x[\cos(x^{\pi\over 4})+\cosh(x^{\pi \over 4})]}dx=1$$ I try: Let generalise the integral and try and to determine the closed form, $$\int_{0}^{\infty}{\sin(x^n)\over x[\cos(x^n)+\cosh(x^n)]}=F(n)$$ $$\cos{x}=\sum_{n=0}^{\infty}{(-1)^{2n}x^{2n}\over (2n)!}$$ $$\cosh{x}=\sum_{n=0}^{\infty}{x^{2n}\over (2n)!}$$ $$\cos{x}+\cosh{x}=2\left[{1+{x^4\over4!}+{x^8\over8!}+\cdots}\right]$$ Can't go any further, so I used  Wolfram integrator and try to figure out the closed form and I got $F(n)={\pi\over 4n}$.","['integration', 'definite-integrals', 'closed-form']"
2071721,Integration of $ \frac{1}{x}$ from First Principles,"I'm interested in finding the area under $f(x)=\frac{1}{x}$ without resorting to the first part of the Fundamental Theorem of Calculus. This has been my attempt so far, not sure how to continue as the harmonic series diverges. Integration of 1/x","['integration', 'calculus']"
2071722,"how to prove that $u\in W^{1,p}(\Omega)$ and $u_k\to u$ weakly in $W^{1,p}(\Omega)$?","Let $\Omega \subset \mathbb{R}^n$, $p>1$. Let $(u_k)_{k\in\mathbb{N}}\subset W^{1,p}(\Omega)$, $u\in L^p(\Omega)$ such that (i)if $p<\infty$: $u_k\to u$ weakly in $L^p(\Omega)$, (ii)if $p=\infty$ $u_k\to u$ weak* in $L^{\infty}(\Omega)$. And let $$\ 	\sup_k \| 	\nabla u_k\|_{L^p}<\infty$$ Prove that $u\in W^{1,p}(\Omega)$ and 1)if $p<\infty$: $u_k\to u$ weakly in $W^{1,p}(\Omega)$ 2)if $p=\infty$: $u_k\to u$ weak* in $W^{1,\infty}(\Omega)$. Sorry, I don't know how to prove it. Especially, I don't know how to conclude from (i) that $u$ has a weak derivative in $L^p$, thus I appreciate any hints how to do it.","['functional-analysis', 'weak-convergence', 'weak-derivatives', 'lp-spaces', 'sobolev-spaces']"
2071750,Prove that the order in $\mathbb{Z}_{mn}^*$ is this,"Let $o_{n}(a)$ be the order of $a$ in the group $\mathbb{Z}_n^*$.
Suppose that $a,m,n$ are relative primes, prove that $o_{mn}(a)=lcm(o_m(a),o_n(a))$. I did the following: Suppose that $d=o_{mn}(a)$, then $a^d\equiv1$ $mod$ $mn$.
Then $a^d\equiv1$ $mod$ $n$ and $a^d\equiv1$ $mod$ $m$. So $o_m(a),o_n(a)|d$, so $lcm(o_m(a),o_n(a))|d$. But I think that this is incorrect and I dont know to prove the equiality. Thanks for all ;)","['finite-groups', 'abstract-algebra', 'group-theory']"
2071756,"Show if $f(x+y)=f(x)+f(y)$ for all $x,y$ and $f$ is Lebesgue measurable, then $f$ is continuous. [duplicate]","This question already has an answer here : Let $g:\mathbb{R}\to\mathbb{R}$ be a measurable function such that $g(x+y) =g(x)+g(y).$ Then $g(x) = g(1)x$ . [closed] (1 answer) Closed 7 years ago . I'm trying to solve this problem $(5.8)$ from Bass' Real Analysis for Graduate Students: Show that if $f:\Bbb{R}\to\Bbb{R}$ is Lebesgue measurable and $$f(x+y)=f(x)+f(y)$$ for all $x,y\in\Bbb{R}$, then $f$ is continuous. The first thing I note is that it suffices to show that $f$ is continuous at $0$, since if this is the case then for any $\epsilon>0$ and $x_0\in\Bbb R$, there exists $\delta>0$ such that $|x|<\delta$ implies $|f(x)|<\epsilon$; in particular, replacing $x$ with $x-x_0$, we see $|x-x_0|<\delta$ implies $|f(x)-f(x_0)|=|f(x-x_0)|<\epsilon$. Now, what I need to show then is that $f^{-1}(-\epsilon,\epsilon)$ contains some interval at the origin. I know that this set is Lebesgue measurable because $(-\epsilon,\epsilon)$ is Borel measurable, and I know it contains $0$ since $f(0)=0$, but I'm not sure how to show it contains $(-\delta,\delta)$ for some $\delta$. I haven't used the fact that the set is Lebesgue measurable, so obviously theres some way I can use that but I'm not seeing it. Any suggestions? edit: because this has been tagged as a possible duplicate, I should point out that I'd prefer to figure out if the solution I am working on will work, rather than just copy a completely different solution outlined by some other user.","['real-analysis', 'measure-theory']"
2071775,Intersection with object on ellipsoid path,"In my game I need to calculate the time when a ship arrives at the planet. What is known is ship's starting position and velocity (which is constant), and planetâ€™s position at a given time (it follows an elliptic path). To be more specific: Regarding the ship: $x_0, y_0$ - ship's initial position $v$ - ship's constant speed Regarding the planet: The planetâ€™s position is given by $$\begin{align}x(t)&=a\cos{\omega_0t}\\y(t)&=b\sin{\omega_0t}\end{align}$$ with $a\ge b\gt0$ and $\omega_0\gt0$. I.e., the planetâ€™s path follows an ellipse in standard position with a phase of $0$. Now, what I need to know, is at what point does the two positions intersect (ship's position and planet's position). So I am either looking for the intersection time $t_i$, or an intersection point $p_i$, or angle $\alpha$ at which the ships should be fired, or distance $d_i$ of intersection point from starting point $x_0, y_0$. Any of these 4 things should do (figuring one from the other is trivial, of course). I am looking for the closest such intersection point of course (the smallest such $d_i$ or $t_i$ of all possible ones). In other words, I need to know at what angle should I send the ship from $x_0, y_0$ so that it will arrive at the planet in smallest possible time. UPDATE 29.12.2016 I am pausing my work on this problem, since I've already spent 5 days on it and am really tired of it. I've tried all kinds of approaches, but the code is buggy and this problem is much more complex than I first thought. I will finish it at some point in the future, but right now there are other aspects of the game I have to implement. So I'd like to help everyone who's contributed, I'll upvote/accept your answers once I get to finish the implementation (I'll present my final algorithm at that point and post it for anyone to use it). Thanks again to everybody for now! (P.S., if interested, this is how the thing works when it works: https://youtu.be/KjQCOkWVIvg )",['trigonometry']
2071798,How to calculate expected value for piecewise constant distribution function?,"The distribution function of a discrete random variable X is given $F_X(x)=\begin{cases} 0, &x<1\\ \frac{5}{13},& 1\leq x< 2  \\ \frac{10}{13}, & 2\leq x<3 \\ \frac{11}{13}, & 3\leq x<4 \\ 1, & 4\leq x   \end{cases} $ $A=(X=2)\cup  (X=4)$ Calculate: $P(A)$ and $E(X)$ I was thinking to solve $P(A)$ with formula: $P(a)=\begin{pmatrix} n \\ a  \end{pmatrix} p^a (1-p)^{n-a} $, but I dont $p$ and $n$. Which formula I should use?","['statistics', 'probability']"
2071817,Prove $a_{n}=\frac{- (\ln n)^2}{n + \ln n} \rightarrow 0$,"Prove $$a_{n}=\frac{- (\ln n)^2}{n + \ln n} \rightarrow 0$$ So I know the fact that $\lim_{n \rightarrow+\infty}\frac{log_{a}n}{n^{\epsilon}}=0$ (($\epsilon>1$, $a>0, a\neq0$)). Is it useful?","['sequences-and-series', 'calculus', 'limits']"
2071837,Distances inducing the same topology but with different balls,"I'm looking for two different metrics $d,d'$ on the same underlying space $X$, such that 1) $d$ and $d'$ induce the same topology on $X$; 2) there exists $x \in X$ such that small balls for $(X,d)$ and for $(X,d')$ with center $x$ are not homeomorphic. This might be a trivial question, but I can't find an easy example. EDIT : I'm mostly interested in the case $X = \mathbb{R}^n$ and $d$ is the Euclidean metric.","['general-topology', 'metric-spaces']"
2071841,Projective equivalence: Linear subspaces under the action of $PGL_n$,"A pair of ordered collections linear subspaces $\Lambda_1, \ldots, \Lambda_k$ and $\Lambda'_1, \ldots, \Lambda'_k$ of $\mathbb{P}^n$ are called projectively equivalent if there exists a regular automorphism $\phi : \mathbb{P}^n \to \mathbb{P}^n$ (equivalently, a member of $PGL_{n+1}$) such that $\phi(\Lambda_i) = \Lambda'_i$ for each $i$. It is well-known that two ordered sets of $n+2$ points in $\mathbb{P}^n$ in general position are projectively equivalent, and that any two ordered sets of three pairwise disjoint lines in $\mathbb{P}^3$ are projectively equivalent. Likewise, any two pairs of a hyperplane in $\mathbb{P}^n$ and a point outside of it are projectively equivalent. Three pairwise disjoint lines satisfies the condition that no two of them lies in a 2-plane, which is a condition on their relative position similar to points in general position. I have two questions: 1) In what sense does collections of linear subspaces lie in general position? Is it simply that any subset of them span a linear subspace of maximal dimension? 2) What kind of ""classifying theorems"" are there regarding which pairs of collections of ordered linear subspaces in $\mathbb{P}^n$ (in general position, or other types of conditions) are projectively equivalent? I'm looking for a sharp relation between the number $k$, the individual dimensions of each $\Lambda_i$, and the strictness of the condition of their relative position under which two such collections are projectively equivalent. Perhaps such a theorem is best formulated in terms of Grassmannians.","['linear-algebra', 'algebraic-geometry', 'geometry']"
2071882,Factorize $x^4+16x-12$ over reals,"Factorize $x^4+16x-12$ over reals. The factor is $x^4+16x-12=(x^2-2x+6)(x^2+2x-2)$ It can be factorized again but I am stuck in this step.If we want to add and then subtract we have a lot of thing to add and subtract.Another idea that I saw in books  is writing as this: $x^4+16x-12=(x^2+ax+b)(x^2+a'x+b')$ and then find $a,b,b',a'$ but there are two problems I can't find these here and we can say maybe it can factorized into one degree $3$ and one degree $1$ polynomial. Isn't there a nice way to factor this?","['algebra-precalculus', 'factoring']"
2071893,Is this an allowed step for working with infinite sequences?,"I just started learning about infinite sequences, which are of course very interesting. Out of curiosity, I tried doing a proof that: $${1,-1,1,-1 ...} = 0$$ This was pretty easy to do, if I could make a certain step. Now, this step makes a lot of intuitive sense to me, but I would like to know whether this is actually mathematically justifiable. Does it work in every scenario? The step is: $$ \lim_{n \to \infty} (-1)^n =   \lim_{n \to \infty} (-1)^{2n} + \lim_{n \to \infty} (-1)^{2n+1} $$ Or more generally: $$ \lim_{n \to \infty} (a)^n =   \lim_{n \to\infty} (a)^{2n} + \lim_{n \to\infty} (a)^{2n+1} $$ The proof from there on isn't very difficult, so this is kind of the breaking point of my argument. Some insights on the matter would be very much appreciated!","['infinity', 'sequences-and-series', 'limits']"
2071916,Solving a system of linear differential equations with repeated eigen values,"I have this problem where to solve the system,
$$x'=4x+y-z$$
$$y'=2x+5y-2z$$
$$z'=x+y+2z$$
using a linear algebraic solution. I have found the eigen values of the 
$$\begin{bmatrix}
        4 & 1 & -1 \\
        2 & 5 & -2 \\
        1 & 1 & 2 \\
        \end{bmatrix}$$
as 3,3 and 5. When evaluating the corresponding eigen vectors for 3, the following occurs.
$$(A-3I)x=0$$
$$\begin{bmatrix}
        1 & 1 & -1 \\
        2 & 2 & -2 \\
        1 & 1 & -1 \\
        \end{bmatrix}
\begin{bmatrix} x_1\\ x_2\\ x_3 \end{bmatrix} = 0$$
We can say ok and $x_3=x_1+x_2$ and then a set of eigen vectors which are not multiples of each other are formed. As the next step, I have to find $\rho$ such that $(A-3I)\rho = \eta_{\lambda =3}$. There I'm getting nowhere because of the ambiguity of $\eta_{\lambda=3}$. I am new to this eigen things. Am I doing something terribly wrong or what?","['multivariable-calculus', 'eigenvalues-eigenvectors', 'ordinary-differential-equations', 'linear-algebra']"
2071920,The sign function is a homomorphism,"We define an inversion of a permutation $\sigma\in S_k$ to be a pair $(\sigma(i), \sigma(j))$ such that $i<j$ but $\sigma(i)> \sigma(j)$. The sign of $\sigma$, written $\text{sgn}(\sigma)$, is defined by \begin{align*}
\text{sgn}(\sigma) = (-1)^{\# \text{ of inversions in }\sigma}  = 
\begin{cases}
+1 &\text{ if the number of inversions in $\sigma$ is even}\\
-1 &\text{ if the number of inversions in $\sigma$ is odd}
\end{cases}.
\end{align*} I want to prove that: $\text{sgn}(\sigma \tau)= (\text{sgn }\sigma)(\text{sgn }\tau)$ for any two permutations $\sigma$ and $\tau$, using the above definition. 
I tired many times but i failed. If I got some equation relating the number of inversions of $\sigma$, $\tau$ and the composite $\sigma\tau$, I had done. I need your help please.","['permutations', 'abstract-algebra']"
2071929,"What is the difference between ""Predicate"" and ""Function""? [duplicate]","This question already has answers here : What is the difference between a predicate and function? (3 answers) Predicate vs function [duplicate] (3 answers) Closed 5 years ago . Functions are defined as particular kinds of relations and relations themselves relie on the concept of Cartesian Product.
It can be proved from ZF the Cartesian Product of two sets exists but this requires using either the Axiom of Separation or the Axiom of Replacement.
In both of them we make use of the concept of predicate, so it should be a difference to avoid falling in circularity. Am I right? Am I missing something? Thanks in advance :)","['predicate-logic', 'set-theory', 'logic', 'functions']"
2071936,Are there more even or odd groups?,"While I was thinking about solvable groups and how the Feit-Thompson theorem states that every finite group of odd order is solvable, I wondered how strong this result really is or how many groups it covers. I know that there isn't an explicit formula for the number of isomorphism classes of groups with order less or equal to some given integer $n$, but the following question came to my mind: Let $G(n)$ denote the number of isomorphism classes of groups with order less or equal to a given integer $n$ and let denote $S(n)$ the number of isomorphism classes of groups with odd order less or equal to $n$. Question: What is known about:
$$
\lim_{n\to\infty}\frac{S(n)}{G(n)} 
$$
Does this limit even exist? If yes, what is it? If not, what about the $\limsup$ and $\liminf$?","['finite-groups', 'solvable-groups', 'group-theory']"
2071964,Left/Right Eigenvectors,"Let $M$ be a nonsymmetric matrix; suppose the columns of matrix $A$ are the right eigenvectors of $M$ and the rows of matrix $B$ are the left eigenvectors of $M$. In one of the answers to a question on left and right eigenvectors it was claimed that $AB=I$. Is that true, and how would you prove it?","['eigenvalues-eigenvectors', 'matrices', 'determinant', 'inverse', 'linear-algebra']"
2071977,Power of a Nonsymmetric Matrix,"I have a matrix $M$ and a vector $x$. I would like to compute the vector $y= M^n x$. I found the eigenvalues $Î»_k$ of $M$ and the left and right eigenvectors, properly normalized. If M was symmetric, I would now project $x$ on each eigenvector, multiply by $Î»_k^n$, and then recombine them. That would give me $y$. But $M$ is not symmetric, Is it any useful that I have the left and right eigenvectors? Would you know how to compute $y$? EDIT (in answer to Florian below): the matrix whose rows are the right eigenvectors of $M$ is too hard to invert, so my question is whether one can to bypass the inversion by using the fact that one knows both sets of eigenvectors?","['eigenvalues-eigenvectors', 'matrices', 'determinant', 'inverse', 'linear-algebra']"
2072017,Brauer Group of $\mathbb{Q}_2$,"I have read that the Brauer Group of any local field is $\mathbb{Q}/\mathbb{Z}$, and I want to see this for $\mathbb{Q}_2$. I'm confused because it appears there are $3$ division algebras of dimension $4$, namely $\mathbb{Q}_2(u, v)/(u^2=a, v^2=b, uv=-vu)$ for $a, b$ are any pair among $2, 3, 5$. What am I missing?","['abstract-algebra', 'class-field-theory', 'field-theory']"
2072019,Measurability of Voronoi Partitions,"Let $(X, d)$ be a complete separable metric space, and let $Y$ be a finite subset of $X$. The associated Voronoi diagram is the collection of sets $\{R_y\}_{y \in Y}$ where $$R_y :=\bigl\{x \in X : d(x, y) = \min\limits_{y' \in Y}\,d(x, y')\bigr\}\,.$$ Question: Does there exist a collection $\{S_y\}_{y \in Y}$ of disjoint Borel-measurable subsets of $X$ satisfying $\bigcup_{y \in Y}S_y = X$ and $S_y \subseteq R_y$ for all $y \in Y$?","['computational-geometry', 'metric-spaces', 'measure-theory']"
2072022,Splitting of Poisson processes and Bernoulli trials,"When a Poisson process is split according to the results of independent Bernoulli trials, two Poisson processes are obtained. It is trivial to prove that these two processes are independent. I was examining the converse. If a Poisson process is split in two Poisson processes, is it necessary that the split takes place according to the results of Bernoulli trials? I was able to prove this on condition that the two processes are independent. Is it possible to prove this without the previous condition?","['poisson-process', 'probability']"
2072026,Find integral $\int\frac{\arcsin(x)}{x^{2}}dt$,"Find integral $$\int\frac{\arcsin(x)}{x^{2}}dx$$
what I've done: $$\int\frac{\arcsin(x)}{x^{2}}dx=-\int\arcsin(x)d(\frac{1}{x})=-\frac{\arcsin(x)}{x}+\int\frac{dx}{x\sqrt{1-x^{2}}}$$ I got stuck with that","['integration', 'calculus']"
2072072,"$x^y+y^x>1$ for all $(x, y)\in \mathbb{R_+^2}$ [duplicate]","This question already has an answer here : Exponential teaser [closed] (1 answer) Closed 7 years ago . Prove that $x^y+y^x>1$ for all $(x, y)\in \mathbb{R_+^2}$.",['inequality']
2072076,"Is this proof of limit of $f(x,y)$ correct?","The limit is this: $$\lim\limits_{(x,y)\to(0,0)}\dfrac{\log\left(\dfrac{1}{\sqrt{x^2+y^2}}\right)}{\dfrac{1}{\sqrt{x^2+y^2}}}.$$ I think I did the proof well. Using that
$\lim\limits_{z\to\infty}\dfrac{\log(z)}{z}=0$
$,\lim\limits_{t\to 0^+}\dfrac{1}{t}=\infty$
$,\lim\limits_{(x,y)\to(0,0)}\sqrt{x^2+y^2}=0,$
$ \sqrt{x^2+y^2}>0 \space$ and the characterization of limits with sequences, we obtain that $\lim\limits_{(x,y)\to(0,0)}\dfrac{\log\left(\dfrac{1}{\sqrt{x^2+y^2}}\right)}{\dfrac{1}{\sqrt{x^2+y^2}}}=0$.","['multivariable-calculus', 'limits-without-lhopital', 'limits']"
2072087,"Oppenheim's Inequality for triangles, American Mathematical Monthly problems","I did a proof for the inequality below, and I would like know if anyone also has a trigonometric proof for this inequality.If you have a trigonometric demonstration, please post your solution. This problem appeared in the American Mathematical Monthly magazine in 1965, the inequality was proposed in that form by Sir Alexander Oppenheim: Let $x,y,z$ positive real  numbers and $\Delta ABC$ a triangle . $\displaystyle [ABC]$ denotes the triangle area  and $\displaystyle a,b,c$ the sides of the triangle. The inequality below is true: $$a^2x+b^2y+c^2z\geq 4[ABC]\sqrt{xy+xz+yz}$$ Various inequalities can be deduced through this inequality, for example, WeitzenbÃ¶ck's inequality, Neuberg-Pedoe inequality, Hadwiger-Finsler inequality, and so on. I'll post my solution right below.
$$Proof$$ Let $\alpha,\beta,\gamma$  denote the opposite angles to the sides $a, b, c$, respectively. $R$ is the circumradius of $\Delta ABC$. Observe that:
    $$a^2x+b^2y+c^2z\geq 4[ABC]\sqrt{xy+xz+yz}$$
  $$a^2x+b^2y+c^2z\geq \frac{abc}{R}\sqrt{xy+xz+yz}$$
    $$\frac{aRx}{bc}+\frac{bRy}{ac}+\frac{cRz}{ab}\geq \sqrt{xy+xz+yz}$$
    $$\frac{1}{2}\left(\frac{4aR^2x}{2Rbc}+\frac{4bR^2y}{2Rac}+\frac{4cR^2z}{2Rab}\right)\geq \sqrt{xy+xz+yz}$$
         $$x\frac{\sin\alpha }{\sin\beta \sin \gamma}+y\frac{\sin\beta }{\sin\alpha \sin \gamma}+z\frac{\sin\gamma }{\sin\alpha \sin \beta}\geq 2\sqrt{xy+xz+yz}$$ $$x\frac{\sin(\pi-\alpha) }{\sin\beta \sin \gamma}+y\frac{\sin(\pi-\beta )}{\sin\alpha \sin \gamma}+z\frac{\sin(\pi-\gamma )}{\sin\alpha \sin \beta}\geq 2\sqrt{xy+xz+yz}$$ $$x\frac{\sin(\alpha+\beta+\gamma-\alpha) }{\sin\beta \sin \gamma}+y\frac{\sin(\alpha+\beta+\gamma-\beta )}{\sin\alpha \sin \gamma}+z\frac{\sin(\alpha+\beta+\gamma-\gamma )}{\sin\alpha \sin \beta}\geq 2\sqrt{xy+xz+yz}$$ $$x\frac{\sin(\beta+\gamma) }{\sin\beta \sin \gamma}+y\frac{\sin(\alpha+\gamma )}{\sin\alpha \sin \gamma}+z\frac{\sin(\alpha+\beta )}{\sin\alpha \sin \beta}\geq 2\sqrt{xy+xz+yz}$$ $$x\frac{(\sin\beta \cos\gamma+\sin\gamma \cos \beta) }{\sin\beta \sin \gamma}+y\frac{(\sin\alpha \cos\gamma+\sin\gamma \cos \alpha) }{\sin\alpha \sin \gamma}+z\frac{(\sin\alpha \cos\beta+\sin\beta \cos \alpha) }{\sin\alpha \sin \beta}\geq 2\sqrt{xy+xz+yz}$$ \begin{equation}
 (\cot\beta+\cot\gamma)x+(\cot\alpha+\cot\gamma)y+(\cot\alpha+\cot\beta)z\geq 2\sqrt{xy+xz+yz} \tag{1}
\end{equation} Since inequality is homogeneous in the variables $x,y,z$, do it $\displaystyle xy+xz+yz=1$ and take the substitution $\displaystyle x=\cot\alpha',y=\cot\beta',z=\cot\gamma'$, we have que $\displaystyle \alpha',\beta',\gamma'$ are angles of a triangle, and our inequality will be equivalent to the inequality below: \begin{equation}
  (\cot\beta+\cot\gamma)\cot\alpha'+(\cot\alpha+\cot\gamma)\cot\beta'+(\cot\alpha+\cot\beta)\cot\gamma'\geq 2 \tag{2}
\end{equation}
Suppose without loss of generality that (the reverse case is analogous) : \begin{equation}
 \cot\alpha \geq  \cot \alpha' \tag{3}
\end{equation}
\begin{equation}
 \cot\beta \geq \cot \beta' \tag{4}
\end{equation}
\begin{equation}
 \cot\gamma'\geq \cot \gamma \tag{5}
\end{equation}
Because these variables are angles of a triangle, we can not have $\cot \alpha \geq \cot \alpha' , \cot\beta \geq \cot \beta', \cot \gamma\geq \cot\gamma'$.In fact, this can not occur, since it supposes without loss of generality that $\displaystyle \alpha'\geq\alpha$ and  $\displaystyle \beta'\geq\beta$(as the cotangent is decreasing, this implies that $\displaystyle  \cot\alpha \geq \cot \alpha' $  and $\displaystyle \cot\beta \geq \cot \beta'$), summing up these first two inequalities we have: $\\ \\ \displaystyle \alpha'+\beta'\geq \alpha+\beta \Rightarrow \cot(\alpha+\beta)\geq \cot(\alpha'+\beta')\Rightarrow -\cot(\pi-\alpha+\beta)\geq- \cot(\pi-\alpha'+\beta') \Rightarrow -\cot(\alpha+\beta+\gamma-(\alpha+\beta))\geq- \cot(\alpha'+\beta'+\gamma'-(\alpha'+\beta')) \Rightarrow -\cot(\gamma)\geq- \cot(\gamma')  \Rightarrow \cot(\gamma')\geq \cot(\gamma)\\ \\$ Now set the $\displaystyle f_1(\alpha,\beta,\gamma,\alpha',\beta',\gamma'):\mathbb{R}^6\rightarrow \mathbb{R}$ and $\displaystyle f_2(\alpha,\beta,\gamma,\alpha',\beta',\gamma'):\mathbb{R}^6\rightarrow \mathbb{R}$ such that: \begin{equation*}
 f_1(\alpha,\beta,\gamma,\alpha',\beta',\gamma')=
\end{equation*}
\begin{equation}
 (\cot\beta+\cot\gamma)(\cot\alpha'-\cot\alpha)+(\cot\alpha+\cot\gamma)(\cot\beta'-\cot\beta)+(\cot\alpha+\cot\beta)(\cot\gamma'-\cot\gamma) \tag{6}
\end{equation} \begin{equation*}
 f_2(\alpha,\beta,\gamma,\alpha',\beta',\gamma')=
\end{equation*}
\begin{equation}
 (\cot\beta'+\cot\gamma')(\cot\alpha-\cot\alpha')+(\cot\alpha'+\cot\gamma')(\cot\beta-\cot\beta')+(\cot\alpha'+\cot\beta')(\cot\gamma-\cot\gamma') \tag{7}
\end{equation} Note now that by inequalities (3), (4) and (5) it follows that: \begin{equation}
0 \geq  \cot\alpha'-\cot\alpha \tag{8}
\end{equation} \begin{equation}
 0 \geq \cot \beta' -\cot\beta \tag{9}
\end{equation} \begin{equation}
 \cot\gamma'-\cot\gamma \geq 0 \tag{10}
\end{equation}
We know that  $\displaystyle \alpha',\beta',\gamma'$ are angles of a triangle, so there exists $\displaystyle a',b',c'$ such that $\displaystyle a'^2=b'^2+c'^2-2b'c'\cos\alpha',b'^2=a'^2+c'^2-2a'c'\cos\beta',c'^2=a'^2+b'^2-2a'b'\cos\gamma'$.Let $\displaystyle R'$ the circumradius of the triangle of sides $\displaystyle a',b',c'$. Let $\displaystyle k_{\alpha',\beta',\gamma'}:=\frac{a'}{b'c'}+\frac{b'}{a'c'}+\frac{c'}{a'b'}$, therefore: \begin{equation}
 \frac{a'}{b'c'}+\frac{b'}{a'c'}+\frac{c'}{a'b'}=k_{\alpha',\beta',\gamma'} \tag{11}
\end{equation} Where $\displaystyle k_{\alpha',\beta',\gamma'}$ is a real variable of any kind.And since our original inequality is homogeneous in the variables a, b, c, suppose without loss of generality that the equality below occurs: \begin{equation}
 \frac{a}{bc}+\frac{b}{ac}+\frac{c}{ab}=k_{\alpha',\beta',\gamma'} \tag{12}
\end{equation} For each real value of fixed $\displaystyle k_{\alpha',\beta',\gamma'}$.Since x, y, z do not depend of the circumradius R ', suppose that $\displaystyle R'\geq R$.Take the inequality (3) and consider the development (applying the law of cosines and law of sines):
$\\ \displaystyle \cot\alpha \geq  \cot\alpha' \Rightarrow  \frac{(b^2+c^2-a^2)R}{abc} \geq \frac{(b'^2+c'^2-a'^2)R'}{a'b'c'} \Rightarrow  \left(\frac{a}{bc}+\frac{b}{ac}+\frac{c}{ab}-2\frac{a}{bc}\right)R\geq \left(\frac{a'}{b'c'}+\frac{b'}{a'c'}+\frac{c'}{a'b'}-2\frac{a'}{b'c'}\right)R' \Rightarrow  Rk_{\alpha',\beta',\gamma'}-2\frac{aR}{bc}\geq Rk_{\alpha',\beta',\gamma'}-2\frac{a'R'}{b'c'} \Rightarrow \frac{a'R'}{b'c'}\geq \frac{aR}{bc} \Rightarrow $ \begin{equation}
 \frac{a'R'}{b'c'}\geq \frac{aR}{bc} \tag{13}
\end{equation} Applying the same rationale for inequality (4), we conclude:
\begin{equation}
 \frac{b'R'}{a'c'}\geq \frac{bR}{ac} \tag{14}
\end{equation} Suppose by contradiction that it occurs: \begin{equation}
   \cot\alpha+\cot\gamma>  \cot\alpha'+\cot\gamma' \tag{15}
  \end{equation} See that: $\\ \displaystyle \cot\alpha+\cot\gamma>  \cot\alpha'+\cot\gamma' \Rightarrow \frac{(b^2+c^2-a^2)R}{abc}+\frac{(a^2+b^2-c^2)R}{abc}> \frac{(b'^2+c'^2-a'^2)R'}{a'b'c'}+\frac{(a'^2+b'^2-c'^2)R'}{a'b'c'} \Rightarrow \frac{bR}{ac}>\frac{b'R'}{a'c'} \\$ This contradicts the inequality (14). On the other hand, suppose by contradiction that it occurs: \begin{equation}
   \cot\beta+\cot\gamma>  \cot\beta'+\cot\gamma' \tag{16}
  \end{equation} See that: $\\ \displaystyle \cot\beta+\cot\gamma>  \cot\beta'+\cot\gamma' \Rightarrow \frac{(a^2+c^2-b^2)R}{abc}+\frac{(a^2+b^2-c^2)R}{abc}> \frac{(a'^2+c'^2-b'^2)R'}{a'b'c'}+\frac{(a'^2+b'^2-c'^2)R'}{a'b'c'} \Rightarrow \frac{aR}{bc}>\frac{a'R'}{b'c'} \\$ This contradicts the inequality (13).Therefore: \begin{equation}
   \cot\alpha+\cot\gamma \leq \cot\alpha'+\cot\gamma' \tag{17}
  \end{equation} \begin{equation}
   \cot\beta+\cot\gamma \leq  \cot\beta'+\cot\gamma' \tag{18}
  \end{equation} Multiplying (17) by $\displaystyle \cot\beta'-\cot\beta$ and  multiplying (18) by $\displaystyle \cot\alpha'-\cot\alpha$, note that these inequalities will reverse, since we are multiplying by non-positive quantities, we will have, respectively: \begin{equation}
   (\cot\alpha+\cot\gamma) (\cot\beta'-\cot\beta)\geq (\cot\alpha'+\cot\gamma')(\cot\beta'-\cot\beta) \tag{19}
  \end{equation} \begin{equation}
   (\cot\beta+\cot\gamma) (\cot\alpha'-\cot\alpha)\geq  (\cot\beta'+\cot\gamma')(\cot\alpha'-\cot\alpha) \tag{20}
  \end{equation} On the other hand of inequalities (3) and (4) we know that:
  \begin{equation}
   \cot\alpha+\cot\beta \geq \cot\alpha'+\cot\beta' \tag{21}
  \end{equation}
  Multiplying the above inequality by $\displaystyle \cot\gamma'-\cot\gamma$, that by the inequality (10) we know to be greater than or equal to zero, we will have: \begin{equation}
   (\cot\alpha+\cot\beta) (\cot\gamma'-\cot\gamma)\geq (\cot\alpha'+\cot\beta')(\cot\gamma'-\cot\gamma) \tag{22}
  \end{equation}
Adding (19), (20) and (22), we will have: \begin{equation*}
   f_1(\alpha,\beta,\gamma,\alpha',\beta',\gamma')\geq
  \end{equation*}
  \begin{equation}
   (\cot\alpha'+\cot\gamma')(\cot\beta'-\cot\beta)+(\cot\beta'+\cot\gamma')(\cot\alpha'-\cot\alpha)+(\cot\alpha'+\cot\beta')(\cot\gamma'-\cot\gamma) \tag{23}
  \end{equation} Adding the LHS of (23) with the LHS of (7) and the RHS of (23) with the RHS of (7), the terms will cancel and we will have: \begin{equation}
   f_1(\alpha,\beta,\gamma,\alpha',\beta',\gamma')+f_2(\alpha,\beta,\gamma,\alpha',\beta',\gamma')\geq 0
  \end{equation}
And this implies, finally, that: \begin{equation}
 (\cot\beta+\cot\gamma)\cot\alpha'+(\cot\alpha+\cot\gamma)\cot\beta'+(\cot\alpha+\cot\beta)\cot\gamma'\geq 2
\end{equation}
That is precisely the inequality (2), which is equivalent to the desired inequality.Thus, the inequality yields.","['inequality', 'trigonometry', 'geometric-inequalities', 'geometry', 'contest-math']"
2072097,Vector space of differential forms on a curve,"Why is the space of differential forms on a curve (defined on the fraction field of the coordinate ring) a one-dimensional vector space over the function field of the curve?
(I am uncomfortable with most elements of field theory such as transcendence basis or separability so please avoid using them in the answer)
Also, what is an analogous statement of this in differential geometry?","['algebraic-curves', 'elliptic-curves', 'differential-forms', 'algebraic-geometry']"
2072103,Integration of Power Series. When is the Function in $L_1$?,"Suppose we are given a function $f$ in terms of its power series
\begin{align}
f(x)= \sum_{n+0}^\infty a_n x^n.
\end{align}
We assume that we know all $a_n$'s and $f$ has infinite radius of convergense. Can we say based on $a_n$ if the function is integrable or not? That is if 
\begin{align}
\int_{\mathbb{R}} |f(x)| dx<\infty.
\end{align} For example, if $a_n \ge 0$ then we have that
\begin{align}
f(x) \ge a_0+a_1 x,\  x >0 ,
\end{align}
and the function is not integrable. The case that I am interested is when $a_n$'s have alternating sign. 
  Specifically, can we determine if the following $f(x)$ is integrable
  \begin{align} f(x)= \sum_{k=0}^\infty \frac{\cos( \frac{\pi}{2}k)\
 (k+1)^{\frac{k+1}{6}}}{k!} x^k. \end{align} Here is the plot of $f(x)$ where the series was computed up to $N=600$. where blue curve is $f(x)$ and red curve is $sinc(\pi/2 x)= \frac{\sin(\pi/2 x)}{\pi/2 x}$. It seems that $f(x)$ has a tail that decays faster than $sinc(\pi/2 x)$. Moreover, next we give a plot of $f(x) \cdot x$  and  $f(x) \cdot x^2$ where blue curve is $f(x) \cdot x$ and red curve is  $f(x) \cdot x^2$. This seems to indicate that $f(x)$ decreases faster than $\frac{1}{x^2}$ which would imply that $f(x)$ is integrable. Therefore, it would also be interesting to show that 
\begin{align}
\lim_{x \to \infty} f(x)=0,
\end{align}
which at this moment I do not know how to do. Thanks you. **Edit: **  Please see a possible solution via Mellin transform.","['real-analysis', 'sequences-and-series', 'lp-spaces', 'integration', 'power-series']"
2072107,integration - bartle theorem 5.3 - absolute integrability,"Theorem 5.3: A measurable function $f$ belongs to  $L$ if and only if $|f| $ belongs to $L$. Definitions: $L = L(X,\mathbf{X}, \mu)$ of integrable functions consists of all real-valued $\mathbf{X}$-measurable functions $f$ defined on $X$, such that both the positive and negative parts $f^{+}, f^{-}$ of $f$ have finite integrals with respect to $\mu$. A function $f: X \rightarrow \mathbb{R}$ is $\mathbf{X}$-measurable if $f^{-1}((\alpha, \infty)) $ are measurable sets for all real $\alpha$. If $f: X \rightarrow \mathbb{R}^{+}$ is $\mathbf{X}$-measurable we define the integral with respect to $\mu$ to be the extended real number 
$$ \int f \, d \mu = \sup \int \phi \, d \mu ,$$ 
where supremum is taken over all simple functions $\phi: X \rightarrow \mathbb{R}$ such that $ 0 \le \phi \le f $. (The integral of simple function is then the usual one with $\mu$.) What I don't see: $|f| \in L \Rightarrow f \in L$ We have $|f|^{+} = f^{+} + f^{-} , |f|^{-} = 0$ have finite integrals. But this does even not imply that $f^{+}$ and $f^{-}$ are $\mathbf{X}$- measurable. What am I missing? EDIT: As given in the counter example by the comments, I believe the theorem should be reformulated as, Let $f$ be a measurable function, then $f \in L \Leftrightarrow |f| \in L$ Is this correct?","['proof-verification', 'indefinite-integrals', 'integration', 'measure-theory', 'proof-explanation']"
2072113,Four Fundamental subspaces,"Given an $m \times n$ matrix, where $m$ is the number of rows and $n$ is the number of columns. Four Fundamental Subspaces The row space is $C(A^t)$, a subspace of $\mathbb{R}^n$. The column space is $C(A)$, a subspace of $\mathbb{R}^m$. The nullspace is $N(A)$, a subspace of $\mathbb{R}^n$. The left nullspace is $N(A^t)$, a subspace of $\mathbb{R}^m$. This is our new space. If the column space is the space that is spanned by the column vectors, why is it that it is a subspace of $\mathbb{R}^m$ and not $\mathbb{R}^n$, since the dimension of the columns should be $n$ instead? Or am I missing something fundamental here?",['linear-algebra']
2072123,Looking for an elementary explanation for the existence of spinors,"(Dear mods, I have edited the question. The previous version was probably too confused.) I can convince myself that spinors exist in 2D using complex numbers. The elementary example is a (complex number) $f (\theta) = \exp(i \theta /2)$. When the argument is increased by $2\pi$, it flips sign ($\exp(i (\theta + 2\pi) /2) $ = $-\exp(i \theta /2)$), and requires another rotation of $2\pi$ to bring it back to the original value, and it is therefore a spinor. I want to understand why we can deduce, hopefully in a similarly easy way,  that spinors can exist even in a three (odd-dimensional) space. Thanks","['spin-geometry', 'differential-geometry', 'lie-groups']"
2072137,Well Ordering and Isomorphism: Possible Error in Textbook Proof?,"My textbook claims to prove the statement that for $2$ well-ordered sets, $X$ and $Y$, either $X$ and $Y$ are isomorphic or one of the sets is isomorphic to an initial segment of the other set (i.e. an upper bounded subset of the other set). The proof starts off: ""Let $X$ be well-ordered, and let $f:X \Rightarrow X$ be a monotonic map, i.e., $$Z_1<Z_2 \Rightarrow f(Z_1)<f(Z_2).$$  Then for all $Z \in X$ we have $f(Z)\geq Z$..."" 1) Isn't this the definition of strictly increasing, not monotonic? 2) Take $X = [0,5]$ and $f(x) = 0.5x$. This is a strictly increasing map of $X$ to itself--note that the least element is preserved--for which $f(Z) \leq Z$. Who's confused: me or the author?",['functions']
2072139,What exactly is the Fourier transform?,"I'm familiar with the slogan ""the Fourier transform decomposes signals into frequencies"", but I've still somehow managed to get by without really understanding what the Fourier transform is or why it is useful. My understanding is that it is a sort of change of basis on the space of functions. Is this accurate? What exactly is the basis we are changing to, and which basis are we changing from? I'm also familiar with the characteristic function from probability theory, or the Fourier transform of the pdf of a random variable, and the fact that it determines the distribution of a random variable. Other than making a couple proofs a bit nicer, what exactly is the significance of the characteristic function?","['probability-theory', 'fourier-analysis']"
2072147,How to prove that if $2^n-1=a\cdot b$ then $a+b\neq 2^m$,"I didn't find any solutions for the following Diophantine equation: for $n=2k-1$, and $a,b\in \{2,3,4,...\}$, and $m\in \mathbb{N}$
$$if\ \ 2^n-1=a\cdot b,\ \ then\ \ a+b\neq 2^m$$ I would like to prove that there aren't any solutions, but I have no idea how.","['number-theory', 'diophantine-equations']"
2072163,extension of line bundles,"Let $X/k$ be a normal variety and $U \subseteq X$ open with closed complement of codimension $\geq 2$. Let $\mathscr{L}/U$ be a line bundle. Is there a unique extension of $\mathscr{L}$ to a line bundle on $X$, i.e. is $\mathrm{Pic}(X) \to \mathrm{Pic}(U)$ an isomorphism? If $X$ is locally factorial, this is true by the isomorphism $\mathrm{Pic}(X) = \mathrm{Cl}(X)$ and the corresponding statement for the Weil divisor class group. For global sections, this holds by [EGA IV${}_2$], p. 115, ThÃ©orÃ¨me 5.10.5 since a normal variety is (S2).","['reference-request', 'algebraic-geometry']"
2072189,Proving the uniqueness property of Lebesgue measure,"I'm having trouble in showing the following statement: The Lebesgue measure is the only map $E \to m(E)$ from the class of measurable sets to $[0,+\infty]$, which satisfies the following properties: $(i)$ Empty set property : $m(\phi)=0$ $(ii)$ Countable additivity : For disjoint measurable sets $E_1,E_2,\ldots$ ; $$m\bigg(\bigcup_{n=1}^{\infty}E_n\bigg)=\sum_{n=1}^{\infty}m(E_n)$$
$(iii)$ Translation invariance : For any measurable set $E \subset \mathbb{R}^d$ and for any $x \in \mathbb{R}^d$, $E+x := \{y+x|y \in E\}$ is measurable and $m(E+x)=m(E)$. $(iv)$ Normalization : $m([0,1]^d)=1$, i.e. measure of the unit hypercube is $1$. My approach: To go via Lebesgue outer measure . We already know that the only mapping from the class of all subsets of $\mathbb{R}^d$ (which obviously contains the class of all measurable sets in $\mathbb{R}^d$) to $[0,+\infty]$, satisfying $(i)$, a weaker version of $(ii)$ (only subadditivity ) and $(iv)$, is the Lebesgue outer measure $m^*(\cdot)$. Then the job left is to show that with strict additivity and translation invariance , the Lebesgue outer measure $m^*(\cdot)$ is upgraded to the Lebesgue measure $m(\cdot)$, which I cannot complete. Any help is greatly appreciated!","['real-analysis', 'lebesgue-measure', 'measure-theory']"
