question_id,title,body,tags
3784879,A lower bound on the largest eigenvalue of a symmetric matrix,"I am trying to prove the following: Let $A$ be an $n \times n$ real symmetric matrix with eigenvalues $\lambda_1 \geq \cdots \geq \lambda_n$ ; the sum all entries in $A$ is $s$ . Prove that $\lambda_1\geq\frac{s}{n}$ . I have tried the following: because of symmetry, we have \begin{equation}
n\lambda_1^2 \geq \lambda_1^2 + \cdots + \lambda_n^2 = \mbox{Trace}(M^2)=\sum_{i,j}(M_{i,j})^2 \geq \frac{s^2}{n^2},
\end{equation} by Cauchy-Schwarz inequality. Therefore, \begin{equation}
\lambda_1^2\geq\frac{s^2}{n^3} \Longrightarrow \lambda_1\geq\frac{s}{n\sqrt{n}}.
\end{equation} This is the best I can get, I appreciate any corrections and hints to the result $s/n$ .","['matrices', 'linear-algebra', 'symmetric-matrices', 'eigenvalues-eigenvectors']"
3784889,Minimizing $x^2+y^2+z^2$ subject to $xy -z + 1 = 0$ via Lagrange multipliers,"$$\begin{array}{ll} \text{minimize} & f(x,y,z) := x^2 + y^2 + z^2\\ \text{subject to} & g(x,y,z) := xy - z + 1 = 0\end{array}$$ I tried the Lagrange multipliers method and the system resulted from has no solution. So I posted it to see if the question is wrong by itself or I'm missing something. So I made the Lagrangian equation $L(x,y,z,λ)=x^2 + y^2 + z^2 + λ(xy -z+1)$ and then $θL/θx = 2x + λy =0$ $θL/θy = 2y + λx =0$ $θL/θz = 2z - λ =0$ $θL/θλ = xy -z +1 =0 $ The obvious solution for that system is x=0 , y=0 , z=1 and λ=2 But solving it in an online solver for nonlinear systems of equation the answer I get is that it's unsolvable. So my question is: What I'm doing wrong","['optimization', 'multivariable-calculus', 'qcqp', 'lagrange-multiplier']"
3784907,couple of questions in probability,"I'm trying to solve some questions and I'm not sure if I'm solving it correctly. Question 1: "" A boy participating in a game and throwing normal dice,
if the dice shows the number 6 the game is over.
Else, the boy need to wait the number of minutes the dice shows. For example: if the dice shows 4 the boy need to wait 4 minutes and then continue playing. What is the Expected value the boy will wait? My answer: $x$ is a random variable that gets the values ( $1, \ldots, 6$ ) with the same probability, so the expected value should be $$2.5 = \frac{1}{6}(1+2+3+4+5) + \frac{1}{6} \cdot 0$$ Question 2: The number of earthquakes in a year at  a country is  a Poisson variable with $2 = \lambda$ .  Given that in certain year the the number of earthquakes was $2$ , what is the probability that the $2$ earthquakes accord at the first $3$ months of the year? My answer: I am not sure that my way is correct it thought that I divide $\lambda$ by $4$ ,then $\lambda$ is the number of earthquakes in a $1/4$ of a year
and then I get that the answer is $0.0758$ , but I am not sure this is the right way. Thanks for the help!","['expected-value', 'probability']"
3784953,Is there a closed form without MZV for $ \sum _{k=1}^{\infty }\frac{H_k}{k^6\:2^k}$?,While evaluating the weight $7$ integral $\displaystyle \int_0^1\frac{\ln^3\left(1-x\right)\ln^3\left(1+x\right)}{1+x}\:dx$ I managed to prove that $$\int_0^1\frac{\ln^3\left(1-x\right)\ln^3\left(1+x\right)}{1+x}\:dx=\frac{3465}{32}\zeta \left(7\right)-\frac{423}{8}\ln \left(2\right)\zeta \left(6\right)-\frac{81}{2}\zeta \left(3\right)\zeta \left(4\right)-36\sum _{k=1}^{\infty }\frac{H_k}{k^6\:2^k}$$ $$+72\operatorname{Li}_7\left(\frac{1}{2}\right)+36\ln \left(2\right)\operatorname{Li}_6\left(\frac{1}{2}\right)-54\zeta \left(2\right)\zeta \left(5\right)-36\ln ^2\left(2\right)\zeta \left(2\right)\zeta \left(3\right)+90\ln ^2\left(2\right)\zeta \left(5\right)$$ $$-21\ln ^3\left(2\right)\zeta \left(4\right)+\frac{21}{2}\ln ^4\left(2\right)\zeta \left(3\right)+36\ln \left(2\right)\zeta ^2\left(3\right)-\frac{21}{10}\ln ^5\left(2\right)\zeta \left(2\right)+\frac{5}{28}\ln ^7\left(2\right)$$ While using the Mathemathica package written by Pisco found here to calculate that integral I saw that the result had MZV which is an indicator that the sum might not have a nice closed form but im quite stubborn and i'd like to know if this sum can be computed without these.,"['integration', 'definite-integrals', 'harmonic-numbers', 'polylogarithm', 'sequences-and-series']"
3784964,"If $f(x)=\frac{e^{\tan x} -e^x+\ln (\sec x +\tan x)-x}{\tan x -x}$ is a continuous function at $x=0$, find $f(0)$","Using L’Hospital will provide the answer, the process is long and tedious, so I generally like to avoid it in such questions. But I am not able to find an alternative. Can I get a hint for this?","['limits', 'functions', 'limits-without-lhopital', 'continuity']"
3785018,Intuitive Proof of the Multivariable Chain Rule,"Is there any intuitive or more ""dumbed-down"" proof of the formula $$f_u = \sum_{j=1}^n f_{x_j} \frac{\partial x_j}{\partial u}$$ Because I don't seem to be able to understand where it comes from.","['multivariable-calculus', 'calculus', 'chain-rule']"
3785074,"$\frac{1}{503}$, $\frac{4}{524}$, $\frac{9}{581}$,...","The following question was posed on a national selection competition: Find the greatest number in the following sequence: $\frac{1}{503}$ , $\frac{4}{524}$ , $\frac{9}{581}$ , $\frac{16}{692}$ ... It took me a long time before I could solve this question, as I could not identify the motif displayed in the above sequence. Afterwards I realized that the motif is expressed by the function $f(x)=\frac{x^2}{500+3x^3}$ . Once I realized that then, I solved the question as follows: As $f(x)=\frac{x^2}{500+3x^3}$ , then $f'(x)=\frac{2x(500+3x^3)-x^2*9x^2}{(500+3x^3)^2}$ $f'(x)=\frac{1000x-3x^4}{(500+3x^3)^2}$ $f'(x)=\frac{x(1000-3x^3)}{(500+3x^3)^2}$ So all we have to do now is find its maximum. $f'(x)=0$ , for $x=\frac{10}{\sqrt[3]{3}}$ and $x=0$ , once we analyze the function we find that for $x=\frac{10}{\sqrt[3]{3}}$ we have the maximum value of the function. However since that is a non integral number, we check for $x=6$ and $x=7$ , which are the two closest integers to $x=\frac{10}{\sqrt[3]{3}}$ . We have that $f(7)>f(6)$ and hence its maximum is reached for $x=7$ and $f(x)=\frac{49}{1529}$ My question is how do I recognize this motif, as that was my biggest hurdle for this question?","['contest-math', 'maxima-minima', 'calculus', 'functions']"
3785107,Can the inverse of the Riemann zeta function in $s > 1$ be expressed as a series?,"In this post, we are interested in the Rimenann zeta function $\zeta(s)$ in $s > 1$ only where it is strictly decreasing rather than $s$ in the entire complex plane. We have the Stieltjes series expansion of the Riemann Zeta function. I inverted the first few terms of this series using series reversion and showed that if $s > 1$ and $\zeta(s) = a$ , then, $$
s = \zeta^{-1}(a) = 1 + \frac{1}{a - \gamma_0} - \frac{\gamma_1}{1!(a - \gamma_0)^2} + \frac{\gamma_2}{2!(a - \gamma_0)^3} - \frac{\gamma_3 - 12\gamma_1}{3!(a - \gamma_0)^4} + \mathcal O(a^{-5})
$$ It seems that $\zeta^{-1}(a)$ can be expressed in the form $$
1 + \sum_{n=0}^{\infty} (-1)^n\frac{f(\gamma_1, \gamma_2, \ldots, \gamma_n)}{n!(a - \gamma_0)^{n+1}}
$$ where $f(\gamma_1, \gamma_2, \ldots, \gamma_n)$ is some polynomial function of the Stieltjes constants $\gamma_n$ . Question : I am looking for a closed or a recurrence formula for $f$ ? Also any reference in this series in literature? Note : Posted in MO since there was no answer in MSE","['number-theory', 'analytic-number-theory', 'sequences-and-series', 'riemann-zeta', 'prime-numbers']"
3785131,Proving closure of set,"My attempt: $\overline{B} \subseteq \ell^\infty$ follows from the fact that $B \subseteq \ell^\infty$ and $\ell^\infty$ is closed. For $\ell^\infty \subseteq  \overline{B}$ , we must show every sequence in $\ell^\infty$ can be represented as a limit point of a sequence in $B$ . Thus, consider $x = (x_1, x_2, ...) \in \ell^\infty$ . If $x$ has a constant subsequence, we are done, so consider when $x$ does not. Let $d_i = (x_1, x_2, ..., x_i, \sup\{x_{i+1}, x_{i+2}, ...\}, \sup\{x_{i+1}, x_{i+2}, ...\}, ...\}$ . Thus, $d_i$ is an element of $B$ as it has a constant subsequence. Now, consider the case where $x$ does not reach its supremum on any of its elements. Thus, $x$ must have an infinitely increasing or decreasing tail whose elements converge to the supremum. Thus, by nature of the supremum, we can find a $N \in \mathbb{N}$ such that when $n > N$ , $||x - d_n|| < \epsilon$ . Thus, we need only consider when $x$ has no infinitely decreasing/increasing tails, so reaches its supremum on all tails. Formally, $||x||_\infty$ is reached by some $x_i$ on each tail of $x$ . Thus, since $x$ has no constant subsequence, $||x||_\infty$ can only be repeated a finite number of times in the sequence. Thus, we can form a decreasing chain of maximum elements in $||x||_\infty$ . Thus, $||x - d_i||$ form an eventually decreasing sequence as $i \rightarrow \infty$ . However, at this point I am having trouble proving that $||x - d_i||$ becomes $\epsilon-$ close to each other and can only really state that they form a decreasing chain. For the second question, i.e. showing what $int(B)$ is, I believe $int(B) = \emptyset$ . Suppose for a contradiction $B_r(x) \subseteq B$ for some $x \in B, r > 0$ . However, consider the sequence $y$ which replaces each constant subsequence $x_{n_k}$ of $x$ with the subsequence $x_{n_k} + r/2^k$ and is the same otherwise. Thus, $||x - y|| = r/2$ so $y \in B_r(x)$ but $y \not \in B$ , so this open ball cannot be contained in $B$ . Contradiction. At this point, I would really appreciate any feedback on whether or not I am on the right track for both of these proofs (and if the latter looks complete or not). Thank you!",['discrete-mathematics']
3785182,"If $a^2+a b+b^2=40$ and $a^2-\sqrt{a b}+b=5$, then find $a^2+\sqrt{a b}+b$","I was given this problem to solve with elementary methods (High School level). Knowing that $$\begin{align}
a^2+a b+b^2 &=40 \\
a^2-\sqrt{a b}+b &=\phantom{0}5
\end{align}$$ find $$a^2+\sqrt{a b}+b$$ I tried to look for $\sqrt{a b}$ , since the requested quantity is $$a^2+\sqrt{a b}+b=(a^2-\sqrt{a b}+b)+2\sqrt{ab}=5 + 2\sqrt{ab}$$ So I set $$x=a^2;\;y=\sqrt{ab};\;b=z$$ and the system became $$x+y^2+z^2=40;\;x-y-z=5$$ subtracting the two equations I got $$z^2-z+y^2+y-35=0$$ which has one real solution when the discriminant is zero. That is $1-4(y^2+y-35)=0$ and then $y=\frac{1}{2} \left(-1\pm\sqrt{142}\right)$ and finally $$a^2+\sqrt{a b}+b=4\pm\sqrt{142}$$ I know that there are other solutions because I've found them with Wolfram Mathematica , but I couldn't find them with elementary methods. Any help will be appreciated.",['algebra-precalculus']
3785197,Relationship between the ideal of an effective Cartier divisor and its invertible sheaf,"Let $X$ be a scheme. We'll assume it's noetherian to avoid any pathologies. Let $D$ be an effective Cartier divisor on $X$ . I am having trouble understanding how to go between the language of invertible sheaves, ideal sheaves, and effective Cartier divisors. I want to be able to go between the following two ideas: An effective Cartier divisor as a pair $(\mathcal{L}, s)$ where $\mathcal{L}$ is an invertible sheaf and $s$ is a regular section (i.e a section $s \in \Gamma(X, \mathcal{L})$ whose corresponding morphism $\mathcal{O}_{X} \longrightarrow \mathcal{L}$ is injective). An effective divisor as a sheaf of ideals ${I}_{D} \subseteq \mathcal{O}_{X}$ which is locally generated by a single non-zerodivisor. I know these two sheaves should be inverses of eachother. In particular, beginning with an ideal sheaf $\mathscr{I}_{D}$ as in (2), considering the dual $\mathscr{Hom}_{\mathcal{O}_{X}}(\mathscr{I}_{D}, \mathcal{O}_{X})$ , we have an obvious choice for a regular section which is just given by the inclusion $\mathscr{I}_{D} \hookrightarrow \mathcal{O}_{X}$ . However, I am not as comfortable going from (1) to (2). Given an invertible sheaf and regular section $(\mathcal{L}, s)$ , I want to define a sheaf of ideals $\mathscr{Hom}_{\mathcal{O}_{X}}(\mathcal{L}, \mathcal{O}_{X})$ . The problem is I don't see any obvious way to realise this as a subsheaf of $\mathcal{O}_{X}$ . The obvious choice is to define a morphism $\mathscr{Hom}_{\mathcal{O}_{X}}(\mathcal{L}, \mathcal{O}_{X}) \rightarrow \mathcal{O}_{X}$ by evaluation at the section $s$ of $\mathcal{L}$ but I see no reason that morphism should be injective. Is there any way to see this easily so I can translate between the two ideas?","['divisors-algebraic-geometry', 'algebraic-geometry', 'sheaf-theory', 'schemes', 'line-bundles']"
3785242,Find the number of solutions for this trig-algebraic equation: $x^2 -x \sin x - \cos x =0$,"The number of points in $(-\infty, \infty)$ for which $x^2 -x \sin x - \cos x =0$ , are? The first thought that came to my my mind was that the equation above is looking like a algebraic quadratic equation, and those equations have at most two different solutions. But this equation is the mixture of algebraic and trigonometric functions. Having some experience with a similar problem I proceeded like this $$
x^2 - x \sin x = \cos x \\
x(x- \sin x ) = \cos x$$ $$
-1 \leq \cos x \leq 1 \\
-1 \leq x(x-\sin x ) \leq 1 $$ $$x(x-\sin x ) = -1 \tag{1}$$ $$x(x-\sin x ) = 1  \tag{2}
$$ $$
x-1 \leq x - \sin x  \leq x+1 \tag{i}$$ $$x-1 \leq -\frac{1}{x} \leq x+1 ~~~~~~~~~~\text{from equation (1)} \\
x-1 = \frac{-1}{x}  ~~~~~~~~~~~~; ~~~~~~~~~~~~~~ x+1 = \frac{-1}{x} \\
x^2 - x +1 =0 ~~~~~~~~~~~~; ~~~~~~~~~~~~~~ x^2 + x +1 = 0$$ Above two equations have no real solutions. From equation (2) and inequality (i), we have $$
x-1 = \frac{1}{x} ~~~~~~~~~~~ ; ~~~~~~~~~~~ x+1 = \frac{1}{x} \\
x^2 - x -1 = 0 ~~~~~~~~~~~ ; ~~~~~~~~~~~ x^2 + x -1 = 0$$ So, above two equations have two different solutions each, so in total we have four distinct values of $x$ . But this answer of mine is not correct, drawing the graph from desmos shows there are just two intersections of $x-axis$ . I'm in need of an explanation of why my solution is giving a wrong answer, and I have two more questions: I felt suspicious when I equated $x(x-\sin x) = \{1, -1\}$ , because what the inequality says is that $x(x-\sin x)$ lies between $-1$ and $1$ , so I could very well equated $x(x-\sin x)$ to any number in between $[-1 , 1]$ but I did not. Could I equate it with any number in between? The solutions of the last two quadratic equations differ from the other's pair only in signs, so it seems to me that there is some minor issue but cannot find it. Is there a specific reason that I'm off to actual answer just due to the consideration  of signs?","['trigonometry', 'inequality']"
3785317,Find $\lim\limits_{x \to 0} \frac{x^2}{\sin^2(x)}$,"I think I should use L'Hospital's rule twice, to end up with $$\lim_{x \to 0} \dfrac{x^2}{\sin^2(x)}=\lim_{x \to 0} \dfrac{2}{2(\cos^2(x)-\sin^2(x))} = 1$$ Right?","['limits', 'calculus']"
3785334,"Non Planar graph doesn't seem to contain neither $K_{3,3}$, nor $K_5$","I am given the following graph G: and asked to determine if its planar or not, and if it is not, give a $K_{3, 3}$ or $K_{5}$ that is a subgraph of G I have determined G  is not planar. I have done this by using the circle chord method. According to the theorem of Kuratowski: A graph is planar if and only if it does not contain a subgraph that
is a K5 or K3,3 configuration. I have spent hours trying to find the subgraph $k_{3, 3}$ or $k_{5}$ and cant find neither. Do these subgraphs really not exist? And if thats the case, then G must be planar, but my circle chord method has repeatedly shown that it is not planar. I have therefore seemingly hit a paradox. Can anyone help me resolve this?","['graph-theory', 'discrete-mathematics', 'planar-graphs']"
3785416,Moving Coins (find winning strategy),"I have question about exercise Coin Slide on page 149 I found in
""Thinking Mathematically"" by Mason, Burton and Stacey. Here it
is: I was able to solve it successfully for $3-3$ case (original problem)
and $4-4$ as well  by try and error ""strategy"". But I'm looking for a conceptional strategy which can be extended to 5-5, 6-6, ...n-n cases.
Let denote by $B$ the big circle, by $S$ the small circe,
and by $-$ the gap of length of big circle and $.$ the gap of length of small circle. Then the possible solutions for $3-3$ and $4-4$ are: Problem: I solved these two cases more less by try and error
and unfortunatelly I ain't recognized a general ""winning strategy""
which can be successfully extended to $n-n$ case. The reason why
I think that there exist such general winning strategy to attack every $n-n$ case is author's hint in the exercise: Remember Leapfrogs. The Leapfrog exercise on p 52 is: And the important thing is that elaboration of simpler cases of this
exercise reveals a strategy that can easily applied to general cases: That is the straight forward Leapfrog strategy is that after every move the colours in the row should alternate.
The point is that since the author gives this Leapfrogs
exercise as hint for my original Coin Slide problem, I think that
there should be also exist a general winning strategy to attack it
independent of $n$ in the $n-n$ case. Does somebody see how this
exercise is related to Leapfrogs and what is the successful
strategy in this game?","['recreational-mathematics', 'logic', 'discrete-mathematics']"
3785420,Understanding Difference Between Cauchy-Goursat and Related Theorem,"I am reading Brown and Churchill's Introductory complex analysis book, which states the Cauchy-Goursat theorem as follows:
If a function $f$ is analytic at all points interior to and on a simple closed contour $C$ , then $$ \int_{C} f(z) dz =0 $$ I understand this result and its proof just fine. However, there is a second theorem which claims the following:
If a function $f$ is analytic throughout a simply connected domain $D$ , then $$ \int_{C} f(z) dz =0 $$ for every closed contour $C$ lying in $D$ . So in this second theorem we don't $C$ to be simple due to the fact that $D$ is simply connected. However, if $f$ is analytic at all points interior to and on a closed contour $C$ , isn't the interior of the closed contour $C$ a simply-connected domain by default? To be more precise, if I rewrite the first theorem as: If a function $f$ is analytic at all points interior to and on a closed contour $C$ , then $$ \int_{C} f(z) dz =0 $$ is this not true? I don't see why we require $C$ to be simple, because even if $C$ intersects itself, we can just treat our non-simple closed contour as a union of simple closed contours, and those integrals are all $0$ .","['complex-analysis', 'complex-integration', 'cauchy-integral-formula']"
3785477,Intuition of the Surface Integral of a Real-Valued Function,"I'm having trouble understanding the idea of a surface integral of a real valued function $f$ . I've read some of the other answers here on Stack Exchange, but they seem to be focused on the surface integral of a vector field. If I have a surface $S$ , and I'm taking the surface integral $\int_S f dA$ , my understanding is that I'm finding the ""volume"" underneath the the surface S, analogous to how a line integral of a real valued function is finding the area underneath the graph of $z=f(x,y)$ when we traverse through a curve $C$ on the $x,y$ plane. But I am having trouble visualizing this. It seems like I can't visualize in a 3-d plot, since $f(x,y)$ would take 3 dimensions to plot, but the surface also lives in 3-d space, so I'm not sure what it means geometrically to take the surface integral of a real valued function. Does anyone have any analogies to help withe intuition here? Analogies: Line Integral over scalar function: Walking along a path in the x-y plane, graphing f(x,y) on the z-axis. The total area underneath the ""fence"" carved out by the the path, and f(x,y). Line Integral over vector field: Walking along a path in the x-y plane, and being pushed around by a mysterious force at each point. The total amount of ""work"" exerted on me as I walk along the curve. Surface Integral over vector field: Placing a parachute (surface) in a region with lots of turbulence, such that the force acting on the parachute at each point is different. Questions: What's the relevant one for the surface integrals over a real valued function, similar to the ones I've provided above?
What's the geometric intuition? (i.e. what volume am I calculating)?","['multivariable-calculus', 'surface-integrals', 'intuition']"
3785509,"""Line integrals"" vs ""integration on manifolds"" in Lee's Introduction to Smooth Manifolds","I'm getting very muddled by the notion(s) of integration used in Lee's book. At first I thought the 4th chapter on the integral of 1-forms was separated from the main treatment of integration in chapter 10 simply because the author wished to present an application early on in the text to stop things getting too terse, but after closely looking at the two definitions I'm starting to get paranoid that these integrals are subtly different. In chapter 4 our integral requires a curve $\gamma : [a,b] \rightarrow M$ and a $1$ -form to be integrated $\omega$ that eats vectors from $T_{\gamma(t)}[a,b]$ . Our ""abstract manifold"" is really just the interval $[a,b]$ . What confuses me is we have this freedom to choose the manifold $M$ which $\gamma$ maps into. We define $\displaystyle \int_\gamma \omega:=\int_a^b\gamma^*\omega$ Compare this to chapter 10 where we are integrating in an $n$ manifold $N$ , and we integrate the $n$ -form $\omega \in T^* N$ by taking the pullback of an inverse chart map $\varphi^{-1}$ . Here we have $\displaystyle \int_U \omega:=\int_{\varphi(U)}{(\varphi^{-1})}^*\omega$ At first glance it looks obvious, the curve is just like the inverse chart map for a 1-D manifold, job is done. However, if I want to do the ""chapter 10 integrals"" in 1-D then $N = [a,b]$ my chart maps $\varphi : U\subset N \rightarrow \varphi(U) \subset \mathbb{R}$ , in particular, note how since $N$ is a 1-d manifold, my chart can only map to subsets of the real line by definition of being a smooth manifold of dimension 1. There is no freedom like in chapter 4 to get exotic curves in $\mathbb{R}^2, \mathbb{R}^3$ or any $M$ . What's going on here? Is there some freedoms that you can get in 1-D that don't exist in higher dimensions? I certainly remember my vector calculus courses letting me do line integrals in any number of dimensions I wanted, Lee's example 4.18 certainly shows a nice curve in $\mathbb{R}^3$ , while the chapter 10.22 example has a 2-form integrated on sphere which is calculated by pulling back with a map $F:D \subset \mathbb{R}^2 \rightarrow \mathbb{S}^2$ , like we are trapped in the $\mathbb{R}^n$ of the same dimension as the manifold! Even the name line integral begins to confuse me, are integrals of differential forms meant to generalise integrals from calculus such as $\hspace{15mm}\displaystyle \int {\bf{v}} \cdot d{\bf{r}} \hspace{10mm}\text{or}\hspace{10mm} \int f({\bf{r}})ds \hspace{10mm}\text{or}\hspace{10mm} \int f(x)dV$ ? Chapter 4 makes it sound like differential forms give us $n$ -dimensional analogues on curved spaces to integrals like the left, while chapter 10 sounds more like an n-dimensional curved analogue of the right most integral, but I thought integrating functions was done by the Riemannian volume form? And that still leaves me wondering how to generalise the middle to get a ""curved scalar hypersurface integral"", of the kind you might see on the RHS of an $n$ -dimensional divergence theorem, say. I think this needs a Riemannian metric, but am unsure how it could be used, since problem 4.3 rules out the existence of any differential form $\omega$ such that $\omega ""="" ds$ generalises the concept of a line element. I would appreciate any assistance in clearing up this huge error in my understanding.","['multivariable-calculus', 'vector-analysis', 'manifolds', 'differential-forms', 'differential-geometry']"
3785537,Algebraic quaternion construction,"Does there exist an algebraic construction of quaternion algebra such that the algebraic properties (such as associativity and distributivity) are immediately obvious?
I am looking for something similar to how the complex numbers are constructed in Galois theory, as an extension of reals: $$\mathbb{C} \cong \mathbb{R}[x]/(x^2+1)$$ Apart from the algebraic properties, which are crystal clear in this construction, we also obtain that complex conjugation is an isomorphism without any explicit calculations.
Is this possible for quaternions?","['abstract-algebra', 'quaternions']"
3785561,How to quickly solve a series of exponential equations.,"When $2^x - 2^{-x} = 4$ , then $2^{2x} + 2^{-2x} =\hbox{ ?}$ and $2^{3x} - 2^{-3x} =\hbox{ ?}$ I have a doubt with this one. I assume I should use some kind of formula to solve it because if I solve the first equation I get that $x = \log_2(1 + \sqrt5)$ and then I just solve the other two equations and get that the result of the second one is $18$ and the last one is $76$ . The thing is that this exercise is supposed to be done quickly but this method takes a lot of time to calculate. Is there another way to solve it that I'm missing?",['algebra-precalculus']
3785562,Random walks and finiteness of a sum,"Let $S_n = X_1 + \dots + X_n$ , $S_0 = 0$ , denote a random walk, where the i.i.d. increments satisfy $EX = 0$ and $EX^2 \in (0,\infty)$ . I want to show that for any $x \geq 0$ $$ \sum_{n = 0}^\infty P(0 \leq S_n \leq x, \min_{0 \leq k \leq n} S_k \geq 0) < \infty.$$ It seems that it is not enough to just consider $\min_k S_k$ , as it is well-known that $P( \min_{0 \leq k \leq n} S_k \geq 0 ) $ behaves like $n^{-1/2}$ . Any ideas? Note that in our case $\liminf_n S_n = - \infty$ and $\limsup_n S_n = + \infty$ .","['stochastic-processes', 'probability-theory', 'random-walk']"
3785564,"Does my set function equal the Lebesgue Measure on subsets of $[0,1]$?","If $f:[0,1]\to\mathbb{R}$ where $S\subseteq [0,1]$ , and we define the following $I=[0,1]$ $\left(I_k\right)_{k=1}^{m}$ are $m$ open sub-intervals of $I$ $\ell(I)=1$ is the length of $I$ $\ell(I_k)=c\in\mathbb{R}^{+}$ is the length of $I_k$ for $k=1,...,m$ $$\Omega(S\cap I_k)=\begin{cases} 
0 & S\cap I_k \ \text{is countable}\\
1 & S\cap I_k \ \text{is uncountable} \\ 
\end{cases}$$ $\mu^{*}(c,S)$ is the outer set function defined as \begin{align*}
& \mu^{*}(c,S)=  \inf\limits_{m\in\mathbb{N}}\left\{ \sum\limits_{k=1}^{m}c\ \Omega\left(S\cap I_{k}\right): S\subseteq\bigcup\limits_{k=1}^{m}I_{k}\right\} 
\end{align*} The inner set function $\mu_{*}(c,S)$ is $\mu_{*}(c,S)=\mu^{*}(c,[0,1])-\mu^{*}(c, [0,1]\setminus S)$ And $\lim\limits_{c\to 0}\mu(c,S)$ is defined when $$\lim_{c\to 0}\mu(c,S)=\lim_{c\to 0}\mu^{*}(c,S)=\lim_{c\to 0}\mu_{*}(c,S)$$ Then Is $\lim\limits_{c\to 0}\mu(c,S)$ equivelant to the Lebesgue Measure of $S$ ?","['measure-theory', 'lebesgue-measure', 'probability-theory', 'outer-measure']"
3785577,On the equivalence of several definitions of the torus,"Usually the topological object which goes under the name of the torus is defined as the product $$T=S^1 \times S^1 $$ of two circles. I call this Definition (1) Another definition I have seen is the following, Definition (2A) We draw a rectangle, label each pair of parallel sides with letters e.g. $a,$ $b,$ and make sure each couple of parallel sides is oriented in the same direction. Then we quotient the rectangle so that the parallel sides are identified. What we get is again the torus. Another version of this last one is the following Definition (2B) We consider in $\mathbb{R}^2$ the unit square $[0,1]\times[0,1]$ and we quotient it by the relation that identifies $$(x,y)\sim (x',y') \iff (x=0,x'=1,y=y') \lor (y=0,y'=1,x=x')\lor (x=x',y=y') $$ Yet another one, Definition (3A) is We consider $\mathbb{R}^2$ , its group of homeomorphisms $H=\text{homeo}(\mathbb{R^2})$ and the subgroup $G\subset H$ generated by the traslations $a:(x,y)\mapsto (x+1,y)$ $ \ $ $b:(x,y) \mapsto (x,y+1).$ Then we define the Torus as the group $$T=\mathbb{R}^2/G$$ with the quotient topology. Related to this is Definition (3B) We define the Torus as the quotient $$T= \mathbb{R}^2/\mathbb{Z}^2.$$ The main question is the following I want to understand, in a detailed and rigorous way, how to go back and forth from each one of these definitions to the others. A few more specific questions related to the main one: In the second definition we just consider a ""rectangle"" as a topological object, but how is it defined? Do we have to see it as some $[a,b] \times [c,d]$ embedded in $\mathbb{R}^2$ ? Can I define it without embedding it in $\mathbb{R}^2?$ Still thinking about the second definition, I understand, at an intuitive level, that we have to consider the parallel sides of the rectangle ""oriented in the same direction"", because otherwise, with different orientations, we would get a completely different object such as a sphere or the Klein bottle or a Mobius strip. How is this choice of orientation, that gives us the torus, encoded in the other definitions? In particular why we do not need to talk about the orientation of the sides in definition 2B? Looking at definition 3A, I see that the translations $a,b$ morally correspond to the two couples of parallel sides in the previous definitions, and the important thing is that they commmute i.e. $ab=ba$ , which morally says that the parallel sides have the same orientation. The commutativity of $a,b$ implies immediately that $G$ is isomorphic as a group to $\mathbb{Z}^2,$ but I'm not sure this implies immediately that $\mathbb{R}^2/G \simeq \mathbb{R}^2/\mathbb{Z}^2$ without having to say anything else.","['surfaces', 'geometric-topology', 'general-topology', 'differential-topology', 'algebraic-topology']"
3785584,Iterative algorithm for $\pi$?,"Let $a_0=3$ and $a_n=a_{n-1}+\sin a_{n-1}$ . Then $$\pi =\lim_{n\to\infty}a_n.$$ I encountered this algorithm a long time ago and don't remember where. It converges very quickly, which I found fascinating (digits agreeing with $\pi$ are in green): $$\begin{align}a_1&\approx\color{green}{3.141}12,\\
a_2&\approx\color{green}{3.1415926535}722,\\
a_3&\approx \color{green}{3.14159265358979323846264338327950}19.\end{align}$$ Why does it compute $\pi$ ? And why is the convergence so fast?","['pi', 'sequences-and-series', 'real-analysis']"
3785586,Lee's Smooth Manifolds Problem 1-8 - Angle function is a smooth coordinate chart.,"The following is Problem 1-8 in Lee's Introduction to Smooth Manifolds , 2nd Edition: By identifying $\mathbb R^2$ with $\mathbb C$ , we can think of the unit circle $\mathbb S^1$ as a subset of the complex plane. An angle function on a subset $U \subset \mathbb S^1$ is a continuous function $\theta: U \to \mathbb R$ such that $e^{i \theta(z)} = z$ . Show that there exists an angle function $\theta$ on an open subset $U$ of $\mathbb S^1$ if and only if $U \neq \mathbb S^1$ For any such angle function, show that $(U, \theta)$ is a smooth coordinate chart for $\mathbb S^1$ with its standard smooth structure. My question regards the last part: showing that $(U, \theta)$ is a coordinate chart compatible with the standard smooth structure of $\mathbb S^1$ . I have no clue where to start. Any hints will be the most appreciated.","['manifolds', 'smooth-manifolds', 'differential-geometry']"
3785589,Computing $\sum_{k=1}^{\infty}\frac{1}{k}\int_{\pi k}^{\infty}\frac{\sin(x)}{x}dx$,"I was asked to evaluate the following sum: $$\sum_{k=1}^{\infty}\frac{1}{k}\int_{\pi k}^{\infty}\frac{\sin(x)}{x}dx$$ I'm trying to use $$\int_{0}^{\infty} \frac{\sin(x)}{x}dx=\frac{\pi}{2}$$ However, it doesn't seem to work. Any help is greatly appreciated.","['sequences-and-series', 'real-analysis']"
3785612,Is the mapping from non-negative functions to their finitely additive integrals semi-continuous?,"This is a follow up to a question I asked at MO , which I think ended up being too easy for that site. Let $(X, \mathcal X)$ be a measurable space. Say that a net $(\mu_\alpha)$ of finitely additive probability measures converges to a finitely additive probability measure $\mu$ if and only if $\mu_\alpha(A) \to \mu(A)$ for all $A \in \mathcal X$ . If $f$ is an extended-real-valued simple $\mathcal X$ -measurable function of the form $f = \sum_{j=1}^n a_j 1_{A_j}$ , then the integral of $f$ with respect to a finitely additive probability measure is defined in the usual way: $$\int fd\mu = \sum_{j=1}^n a_j \mu(A_j).$$ If $f: X \to [0,\infty]$ is non-negative, then define $$\int f d\mu = \sup\Big\{ \int gd \mu: g \ \text{simple}, \ 0 \leq g \leq f \Big\}.$$ Question. Is it the case that if $\mu_\alpha \to \mu$ , then $\liminf_\alpha\int f d\mu_\alpha \geq \int f d\mu$ for all non-negative $\mathcal X$ -measurable $f: X \to [0,\infty]$ ? In the previous question, I asked whether $\mu_\alpha \to \mu$ implies $\int f d\mu_\alpha \to \int f d\mu$ , and this was shown to be false by a simple example in which $\int f d\mu_\alpha = 1$ for all $\alpha$ and $\int f d\mu = 0$ , which is consistent with the answer to the present question being affirmative.","['measure-theory', 'probability-theory']"
3785655,Can we claim that $\int\limits_{X} f(x)d\mu=\int\limits_{X_1} f(x)d\mu$?,"Let $(X,\mathfrak{M},\mu)$ be a measure space. Let $X_1\in\mathfrak{M}$ with $\mu(X\setminus X_1)=0$ and $f:X_1\to [0,+\infty]$ be a measurable function.  Can I conclude that $$\int\limits_{X} f(x)d\mu=\int\limits_{X_1} f(x)d\mu$$ The integral on the RHS makes sense but on the LHS does not because function $f(x)$ is defined on $X_1\subset X$ . Even $\mu(X\setminus X_1)=0$ I don't think that this notation is rigorous. Would be grateful to read your comments, please!","['measure-theory', 'lebesgue-integral', 'real-analysis']"
3785679,Continuity of multivalued function,"Every multivalued function $A \rightrightarrows B$ can be thought as a function $A → \mathcal{P}(B)$ , where $\mathcal{P}$ represents power set. Let me have an ""analytic"" definition of continuity of such functions. A function $f : \mathbb{R} → \mathcal{P}(\mathbb{R})$ is continuous iff, for every $x \in \mathbb{R}$ and $\epsilon > 0$ , there exists $\delta > 0$ such that: $$
\forall y \in (x - \delta, x + \delta) \quad f(y) \subset \bigcup_{u \in f(x)} (u - \epsilon, u + \epsilon)
$$ This is basically an extension of the epsilon-delta definition. An example of a continuous multivalued function is what I call ""continuous signum function"": $$
f(x) = \begin{cases} \{-1\} & \text{if } x < 0 \\ \{-1,0,1\} & \text{if } x = 0 \\ \{1\} & \text{if } x > 0 \end{cases}
$$ Another example is what I call ""interpolated reciprocal function"": $$
f(x) = \begin{cases} \{x^{-1}\} & \text{if } x ≠ 0 \\ \mathbb{R} & \text{if } x = 0 \end{cases}
$$ Here's the question. Is there a topology on $\mathcal{P}(\mathbb{R})$ so the analytic definition and the topological definition will coincide?","['general-topology', 'real-analysis']"
3785703,"Number of solutions of the equation: $\tan x=\cos2x$ in $[-π, π]$","I know that this question can be solved by using graphs, but in our examinations, we are not allowed to use calculators, or any digital devices. I have a doubt that how to check whether the graphs will intersect or not at the points which are marked by red colour. Can anyone suggest any simple method which is quick and easy which saves our time in the examination. Any suggestion from your side will be appreciated.",['trigonometry']
3785743,"For any integer $n$, $a_n$ and $b_n$ are two real numbers and function....[CONT]","For any integer $n$ , $a_n$ and $b_n$ are two real numbers and function $$f(x)=\begin{cases} a_n + \sin \pi x & x\in [2n, 2n+1] \\\ b_n +\cos \pi x & x\in (2n-1,2n) \end{cases}$$ If $f(x)$ is continuous prove that $a_{n-1} - b_n=-1$ At $x=2n$ , $$f(2n) =a_n$$ And $$\lim_{x \to 2n^-} b_n+\cos \pi x$$ $$=\lim _{h\to 0^+} b_n +\cos \pi (2n-h)$$ $$=\lim_{h\to 0^+} b_n +\cos \pi h$$ $$=b_n+1$$ So $$a_n-b_n=1$$ But I am not able to get $a_{n-1}$ in any expression. How should I do that? Edit For the part I am not clear: Why has the RHL been chosen as the second function at $x=2n+1$ and why does $b_n$ change to $b_{n+1}$ ?","['limits', 'continuity']"
3785745,How does ESPN calculate the probability of a team winning a game?,ESPN has a probability winning percentage in which what team has more of a probability of winning the game. Like team A is up a certain amount of points and time remaining they’ll have a certain percentage chance of winning the game. How does ESPN calculate that?,"['statistics', 'probability']"
3785764,Difficulty Understanding Rudin's Example 2.10(b),"I was studying Rudin's Principles of Mathematical Analysis , Chapter 2 Example 2.10(b) and I came across these two examples which I didn't understand and did not get a clear idea of what they meant: Let $A$ be the set of a real number $x$ such that $x\in\mathbb{R}_{(0,1]}$ . $\forall x\in A$ , Let $E_{x}$ be the set of real numbers $y$ such that $y\in\mathbb{R}_{(0,x)}$ . Then $$
\bigcup_{x\in A}E_{x} = E_{1}\qquad \text{and}\qquad \bigcap_{x\in A}E_{x} = \emptyset.
$$ I am having a hard time understanding them and I would be glad if there is a clear proof for these two results.","['elementary-set-theory', 'general-topology', 'proof-writing', 'real-analysis']"
3785765,"How to show $k[x]$ is not isomorphic to $k[x,y]/(xy+x^3+y^3)$?","Is it true that $k[x]$ is not isomorphic to $k[x,y]/(xy+x^3+y^3)$ as a ring, where $k$ is any algebraic closed field? If this is true, is there a simple proof, where by simple, I mean the proof is elementary as in first year abstract algebra that does not involve technique like localization? I ask this because the graph of $xy+x^3+y^3$ contains a singular point.","['algebraic-geometry', 'ring-theory', 'abstract-algebra', 'polynomials', 'commutative-algebra']"
3785791,"If $A$ is a normal matrix such that $A^{247} = A^{246}$, how to prove that $A$ is an orthogonal projection?","Let $A$ be a normal matrix and $A^{247} = A^{246}$ . Then I want to prove: $A^2 = A$ . $A$ is Hermitian. I'm not sure on how to prove this. If $A$ is normal, then $A$ is unitarily diagonalizable. Then $A^{247}=A^{246}$ implies $U^\ast A^{247}U = U^\ast  A^{246}U$ . Then if $D = U^\ast AU$ , can I say that $D^{247}=D^{246}$ ? I don't know how to continue or if my thinking was correct. Please help.","['matrices', 'hermitian-matrices', 'linear-algebra']"
3785799,The value of the following product is?,"Evaluate the following product: $$\newcommand{\T}[1]{\frac{\sin\frac{\theta}{#1}}{\tan^2\frac{\theta}{#1}\tan\frac{2\theta}{#1} + \tan\frac{\theta}{#1}}} \\
P(\theta) = \T{2} \times \T{2^2} \times \T{2^3} \times .... \infty$$ For $\theta = \frac \pi 4$ Simplified, $P(\theta)$ is $$P(\theta) = \lim_{n \to \infty}\prod_{r=1}^n T(\theta,r)= \lim_{n \to \infty}\prod_{r=1}^n\T{2^r}$$ The denominator can be simplified as follows: $$D = \tan\frac{\theta}{2^r}\left( \tan\frac{\theta}{2^r}\tan\frac{\theta}{2^{r-1}} + 1\right) \\
= \tan\frac{\theta}{2^{r-1}} - \tan\frac{\theta}{2^{r}}$$ After this, $P(\theta)$ becomes $$P(\theta) = \lim_{n \to \infty}\prod_{r=1}^n \frac{\sin\frac{\theta}{2^r}}{\tan\frac{\theta}{2^{r-1}}- \tan\frac{\theta}{2^r}}$$ One more detail I found out is that $\lim_{n \to \infty} T(\theta,n) = 1$ , but I couldn't proceed further from here. Any hints/solutions are appreciated. EDIT : After the hints in the comments, $T(\theta, r)$ resolves to $\cos \frac \theta {2^{r-1}} \cos \frac \theta {2^r}$ as follows (assuming $\frac \theta {2^r} = t$ ) $$\begin{gather}
T(\theta, n) = \frac{\sin t}{\tan^2t\tan 2t + \tan t} \\
= \frac{\cos t}{\tan t \tan 2t + 1} \\
= \frac{\cos t(1-\tan^2t)}{1+\tan^2t} \\
= \cos t \cos 2t \\
= \cos \frac \theta {2^{r-1}} \cos \frac \theta {2^r}
\end{gather}$$ Now, $$P(\theta) = \lim_{n \to \infty} \frac{ \left( \cos\theta\cos\frac\theta2... \cos \frac{\theta}{2^n} \right)^2 }{\cos\theta} = \frac{\sin^2\theta}{2^{2n}\sin^2 \frac \theta {2^n}\cos \theta} = \frac{\sin^2 \theta}{\theta^2 \cos \theta}$$ Therefore, $$\boxed{P(\pi/4) = \frac{8\sqrt2}{\pi^2}}$$ However, the answer mentioned in the textbook is $\frac{2}{\pi}$ . Where am I going wrong? (I think there's a silly mistake somewhere here; just not able to find it :(","['limits', 'trigonometry', 'limits-without-lhopital', 'sequences-and-series']"
3785822,Find all values of $a$ for which the maximum value of $f(x)=\frac{ax-1}{x^4-x^2+1}$equals $1$.,Find all values of $a$ for which the maximum value of $$f(x)=\frac{ax-1}{x^4-x^2+1}$$ equals $1$ . I equated $f(x)$ equal to $1$ to obtain a polynomial $x^4-x^2-ax+2=0$ . Now this must have at least one repeated root.But I could not get any further. Is there a calculus way to do it,"['maxima-minima', 'calculus', 'derivatives', 'algebra-precalculus']"
3785846,Reference request for the Elimination Properties of Resultants,"Let $f,g$ be polynomials in $k[y_1,\dots,y_n][x]$ over a field $k$ . Assume that at least one of $f$ and $g$ is of positive degree in $x$ . Denote by $\operatorname{res}_x(f,g)$ the resultant of $f$ and $g$ with respect to $x$ . On the Wikipedia page for Resultants, under Elimination properties it is stated that if $R$ is a ring of polynomials, $f,g\in R[x]$ at least one of positive degree in $x$ and $I=(f,g)$ , then $I\cap R$ is a principal ideal of $R$ , generated by some $r\in R$ $\operatorname{res}_x(f,g)$ is in the principal ideal $(r)$ of $R$ There exists a positive integer $k$ such that $r^k$ is in the principal ideal $(\operatorname{res}_x(f,g))$ of $R$ . I am kindly asking for a reference on all three claims. I was not able to find any mention of these results in the refereces listed on the Wikipedia page (Gelfand Kapranov Zelevinski; Cox Little O'Shea; Macaulay; Salmon).","['reference-request', 'algebraic-geometry', 'abstract-algebra', 'resultant', 'commutative-algebra']"
3785849,Applications of the martingale convergence theorem,"I am trying to solve the following question and the hint makes no sense to me as to why its even necessary to begin with: We have that $\sup_n E(M_n^+) = \sup_n E(M_n) \leq E(\sup_n M_n) < \infty$ so that we may apply the martingale convergence theorem to state there exists a limit $M_\infty$ which is in $L^1$ .  Now it is obvious that $M_n \leq \sup_n M_n \in L^1$ by assumption, so that dominated convergence yields the result.  Is this wrong?  Am I missing something huge? Even if the question is much easier than the hint makes it out to be, I want to know the motivation and how to actually use it.  I can follow steps (a) - (d) easily enough, but I cannot conclude using the result in (d).  What were the people who made the question looking at trying to do?  How do you conclude using (d)?","['martingales', 'convergence-divergence', 'probability-theory']"
3785854,Contribution of Time-Dependent Variable to Change in Function,"Given a function $$C(x(t),y(t))=x*y$$ and discrete data for variables $x(t),y(t)$ at the points $t_0,t_1$ , what is the contribution (total or percentage) of the change in variable $x$ to the change $\Delta C$ $$\Delta C = C(x(t_0),y(t_0))-C(x(t_1),y(t_1))$$ in the function $C$ ? Background: Going through a paper recently I got stuck on the approach that the authors were using. I had not come across this before, so maybe there is an elegant way to explain this. In a 2018 paper on solar photovoltaics (P.9 Main Body, P.1 in Supplementary Material), the authors have a cost function $C$ which describes the cost associated with manufacturing one unit. It depends on manufacturing variables $x,y$ , which change over time (e.g. price of silicon, price of chemicals, etc.) $$
C(x(t),y(t))
$$ They want to determine the contribution of a single variable $x$ to the total change of the cost function between two points in time $\Delta C (t_0, t_1)$ . Variables are known only at discrete points in time ( $t_0,t_1$ ). They start by writing out the differential of the cost function $C$ as $$
dC (x(t), y(t)) = \frac{ \partial C }{ \partial x } \frac{ \text{d} x }{ \text{d} t} \text{d} t + \frac{ \partial C }{ \partial y } \frac{ \text{d} y }{ \text{d} t} \text{d} t
$$ where the contribution of the change in variable x over time $t_0 < t < t_1$ is then $$
\Delta C_x = \int_{t=t_0}^{t_1} \frac{ \partial C }{ \partial x } \frac{ \text{d} x }{ \text{d} t} \text{d} t
$$ Here they say If it were possible to observe the (...) variables x in continuous time, (...) [this equation] would provide all that is needed to compute the contribution of each variable x. Using logarithmic differentiation , they go on to rewrite the expression as $$
\Delta C_x = \int_{t=t_0}^{t_1} C(t) \frac{ \partial \ln C }{ \partial x } \frac{ \text{d} x }{ \text{d} t} \text{d} t
$$ and then for $C(t)$ assume a constant $C(t) \approx \tilde{C} $ which is ultimately chosen to be $\tilde{C} = \frac{ \Delta \tilde{C} }{ \Delta \ln \tilde{C} }$ , such that $\Delta C_x + \Delta C_y = \Delta C$ . Questions: Even if the time dependence of variables was known (eg. daily data on the price of silicon, etc.), then integrating would not yield what the authors are actually looking for. They are interested in the contribution of single variables to the total change in $C$ (eg. what percentage of total manufacturing cost reductions are due to decrease in silicon price). But integrating using $$\Delta C_x = \int_{t=t_0}^{t_1} \frac{ \partial C }{ \partial x } \frac{ \text{d} x }{ \text{d} t} \text{d} t $$ is dependent on the path of curves $x(t),y(t)$ . This would yield different results for different time dependency of variables. A variable $x(t)$ would yield a different $\Delta C_x$ than a variable $x'(t)$ , which is not what the authors seek to describe.","['statistics', 'differential', 'analysis', 'real-analysis', 'economics']"
3785874,"In the Cholesky decomposition, the argument of the square root is always positive if the matrix is real and positive definite. Why?",Computing the Cholesky decomposition for an $n \times n$ matrix $A$ you need to evaluate $$l_{jj} = \sqrt{a_{jj}-\sum_{k=1}^{j-1} l^2_{jk}}$$ The argument of the square root is always positive if $A$ is real and positive definite.  Why is that the case?,"['cholesky-decomposition', 'matrices', 'numerical-linear-algebra', 'positive-definite', 'matrix-decomposition']"
3786042,Flint Hills series $\sum_{n=1}^{\infty}\frac{1}{n^3\sin^2 n}$,"The well known Flint Hills series , $$\sum_{n=1}^{\infty}\frac{1}{n^3\sin^2 n}$$ Since I'm aware   of the fact that the series convergence issue is still unknown or falls under  unsolved problem however, making  check on WA ,  the infinite sum has been approximated $\sim 4.80$ . Similarly, I came here where the proposer, Tobi Ope claims that $$\sum_{n=1}^{\infty}\frac{1}{n^3\sin^2 n} =3\zeta(5)-\frac{2}{\pi^2}\zeta(2)\zeta(3)-2\sum_{k=1}^{\infty}\frac{\cot k}{k^4}\tag{1}$$ which doesn't meet the approximation done by Wolfram alpha. Whether the series converges or not which is still a big unsolved issue so I believe the equality $(1)$ done is incorrect? Similarly, what is wrong with Wolfram alpha? Is it possible to approximate or find the possible closed form for any such infinite series whose convergence is unknown? Thank you","['summation', 'sequences-and-series', 'real-analysis']"
3786087,Does the pushforward of a smooth map on a manifold coincide with the derivative in a tangent direction of any local extension?,"Let $d\in\mathbb N$ , $k\in\{1,\ldots,d\}$ , $M$ be a $k$ -dimensional embedded $C^1$ -submanifold of $\mathbb R^d$ with boundary, $x\in M$ , $$T_x\:M:=\{\gamma'(0):\gamma\text{ is a }C^1\text{-curve on }M\text{ through }x\}$$ denote the tangent space of $M$ at $x$ , $E$ be a $\mathbb R$ -Banach space and $f:M\to E$ be $C^1$ -differentiable at $x$ , i.e. $$\left.f\right|_{O\:\cap\:M}=\left.\tilde f\right|_{O\:\cap\:M}\tag1$$ for some $\tilde f\in C^1(O,E)$ for some $\mathbb R^d$ -open neighborhood $O$ of $x$ . If I got it right, the problem with defining the derivative of $f$ at $x$ to be ${\rm D}\tilde f(x)$ is that this definition would not be well-defined, since the operator ${\rm D}\tilde f(x)$ does depend on the choice of $\tilde f$ . Now, it's easy to see that $C^1$ -differentiability of $f$ at $x$ is equivalent to the $C^1$ -differentiability of $f\circ\phi^{-1}$ for some $k$ -dimensional chart $\phi$ of $M$ around $x$ (i.e. $\phi$ is a $C^1$ -diffeomorphism from an $M$ -open subset $\Omega$ onto an open subst of $\mathbb H^k:=\mathbb R^{k-1}\times[0,\infty)$ ). This fact can be used to show that if $v\in T_x\:M$ and $\gamma$ is any $C^1$ -curve on $M$ through $x$ with $\gamma'(0)=v$ , then $${\rm D}_xf(v):=(f\circ\gamma)'(0)\tag2$$ is well-defined, i.e. independent of the choice of $\gamma$ . (The crucial thing is that the derivative on a half space, like the interval on which $\gamma$ is defined or the open subset $\phi(\Omega)$ of $\mathbb H^k$ on which $\phi^{-1}$ is defined, does not depend on the choice of a differentiable local extension: Differentiability at the boundary of a function on a half space ) Now I've got two questions: Can we show that, if $v\in T_x\:M$ as above, then $${\rm D}_xf(v)={\rm D}\tilde f(x)v\tag3,$$ where ${\rm D}\tilde f(x)$ is the Fréchet derivative of $\tilde f$ at $x$ and hence ${\rm D}\tilde f(x)v$ is the ordinary directional derivative of $\tilde f$ at $x$ in direction $v$ ? If $x$ is in the topological interior $\operatorname{Int}M$ of $M$ , can we conclude that $$T_x\:M=\mathbb R^d\tag4$$ and that $(3)$ holds for all $v\in\mathbb R^d$ ? Regarding 2.: Since $O\cap\operatorname{Int}M$ is open, $$B_\varepsilon(x)\subseteq O\cap M\tag5$$ for some $\varepsilon>0$ and hence it should follow from $(1)$ that $f$ is actually continuously Fréchet differentiable at $x$ and $${\rm D}f(x)={\rm D}\tilde f(x)\tag6.$$ Moreover, if $v\in\mathbb R^d\setminus\{0\}$ , then $$\gamma(t):=x+tv\in B_\varepsilon(x)\;\;\;\text{for }t\in I:=\left(-\frac\varepsilon{\left\|v\right\|},\frac\varepsilon{\left\|v\right\|}\right).$$ By construction, $\gamma\in C^1(I,M)$ with $\gamma(0)=x$ and $\gamma'(0)=v$ and $$(f\circ\gamma)'(0)={\rm D}f(\gamma(0))\gamma'(0)={\rm D}f(x)v\tag7$$ by the chain rule. This should yield both $(2)$ and $(3)$ . 1: If $E_i$ is a $\mathbb R$ -Banach space, $\Omega_1\subseteq E_1$ and $g:\Omega_1\to E_2$ , then $f$ is called $C^1$ -differentiable at $x_1\in\Omega_1$ if $$\left.g\right|_{O_1\:\cap\:\Omega_1}=\left.\tilde g\right|_{O_1\:\cap\:\Omega_1}$$ for some $\tilde g\in C^1(O_1,E_2)$ for some $E_1$ -open neighborhood $O_1$ of $x_1$ . $f$ is called $C^1$ -differentiable if it is $C^1$ -differentiable at $x_1$ for all $x_1\in\Omega_1$ , which is equivalent to $$g=\left.h\right|_{O_1}$$ for some $h\in C^1(O_1,E_2)$ for some $E_1$ -open neighborhood $O_1$ of $\Omega_1$ . 2: $\gamma$ is called $C^1$ -curve on $M$ through $x$ if $\gamma:I\to M$ , where $I$ is a nontrivial interval with $0\in I$ , is $C^1$ -differentiable with $\gamma(0)=x$ .","['pushforward', 'smooth-manifolds', 'solution-verification', 'differential-topology', 'differential-geometry']"
3786090,Equivalence relation criteria,"I'm  studying set theory/equivalence relation. For an equivalence relation, $3$ conditions are to be met: $1)$ reflexivity $2)$ symmetry $3)$ transitivity Does the following set and the relation on it fulfill these criteria, even though $(2)$ and $(3)$ are missing? They are not disproven, they're just absent. $\{0,1,2\}$ $R=\{(0, 0), (1, 1),(2, 2)\}$ thanks
ralph",['elementary-set-theory']
3786107,Two methods are giving two different answers to the this differential equation : $\frac{dy}{dx}=\frac{1}{2} \frac{d(\sin ^{-1}(f(x))}{dx}$,"$f(x)=\left(\sin \left(\tan ^{-1} x\right)+\sin \left(\cot ^{-1} x\right)\right)^{2}-1,\ |x|>1$ If $\displaystyle\frac{\mathrm{d} y}{\mathrm{d} x}=\frac{1}{2} \frac{\mathrm{d}}{\mathrm{d} x}\left(\sin ^{-1}(f(x))\right)$ and $y(\sqrt{3})=\frac{\pi}{6}$ , then $y(-\sqrt{3})$ is equal to : Options: $1. \quad-\frac{\pi}{6}\\
2. \qquad \frac{2 \pi}{3}\\
3. \qquad \frac{5 \pi}{6}\\
4. \qquad\frac{\pi}{3}$ Now I am getting two Answers in two methods. Can anyone tell me which method is wrong and why? Method - 1 $f(x) = [\sin(\tan^{-1}x) + \sin(\cot^{-1}x) ]^2 -1 $ . Let $\tan^{-1}x = \theta $ . So $f(x) = [\sin(\theta) + \sin(\frac{\pi}{2} - \theta) ]^2 -1 = \sin 2\theta$ . So $f(x) = \sin 2\theta = \frac{2\tan \theta}{1+ \tan^2 \theta} = \frac{2x}{1+x^2}\tag 1$ . Now $\frac{d}{dx} \sin^{-1} \frac{2x}{1+x^2} = \frac{2(1-x^2)}{\sqrt{(1-x^2)^2}(1+x^2)} = \frac{2(1-x^2)}{(1+x^2)(x^2 -1)}$ [Since $|x| > 1$ ]. Now $\frac{dy}{dx} = \frac{1}{2}\frac{d(\sin^{-1}f(x)}{dx}$ . So $\frac{dy}{dx} = \frac{-1}{1+x^2}$ . So $y= - \tan^{-1} x + C$ . Now as $y(\sqrt 3) = \frac{\pi}{6}$ , $C = \frac{\pi }{2}$ . So $y = -\tan^{-1} x + \frac{\pi}{2}$ . So $ \displaystyle   y(-\sqrt 3) =  \frac{\pi}{3} + \frac{\pi}{2} = \frac{5\pi}{6}$ Method -2 - $\displaystyle\frac{\mathrm{d} y}{\mathrm{d} x}=\frac{1}{2} \frac{\mathrm{d}}{\mathrm{d} x}\left(\sin ^{-1}(f(x))\right)$ . So $y = \frac{\sin^{-1}f(x)}{2 } + C$ . Now $f(x) =  \sin (2\tan^{-1} x)$ . SO $\displaystyle y = \frac{\sin^{-1}(\sin (2\tan^{-1} x))}{2 } + C$ . Now as $y(\sqrt 3) = \frac{\pi}{6}$ , $C = 0$ . So $\displaystyle y = \frac{\sin^{-1}(\sin (2\tan^{-1} x))}{2 } $ . So $ \displaystyle y(-\sqrt 3) = \frac{-\pi}{6}$ I am really confused. Why I am getting two answers? Can anyone please help me out?","['solution-verification', 'derivatives', 'ordinary-differential-equations']"
3786140,Given $\triangle ABC$ can we construct point $O$ such that $AO\times BC=BO\times AC=CO\times AB$?,"Given a $\triangle ABC$ , is it possible to construct, with compass and straightedge, a point $O$ such that $$AO\cdot BC=BO\cdot AC=CO\cdot AB$$ Does that point exist?","['geometric-construction', 'euclidean-geometry', 'geometry', 'plane-geometry']"
3786158,"Are there ""tri-commutative"" structures for which: $AB \neq BA$, $BC \neq CB$, yet $ABC = BAC = ACB$?","Groups can be Abelian or non-Abelian, however I'm curious of the space between these two where there are either weaker forms of commutativity or special properties some elements have which endows the group with quasi-Abelian characteristics. Having a name for this could be handy for universal algebra. Preferably a structure with associativity and where all products of three elements are commutative w.r.t. their neighbors (the elements to the right and left of them in the composition/sequence of the triple-product, so we don’t necessarily have $ABC = CAB = CBA$ , but we can interchange $A$ with $B$ and $B$ with $C$ while preserving the outcome). There may be some non-Abelian groups which have specific subsets with this property, or even subgroups where all members have this property. I'm under the impression there are theorems which hold for all groups which may be easy to prove for Abelian groups and hard to prove for non-Abelian groups. My intuition is that commutativity gives you a lot (at least when bundled with closure and associativity) and it might be tricky to prove something right at the edge of ""things which are true for non-Abelian groups"" if you could use commutativity as a shortcut. I'm not sure how well-founded this intuition, but clearly there are things which are common to all groups and things which are different between Abelian and non-Abelian groups; it's just whether one can use assumptions of commutativity instead of taking a longer path without it. Binary relations are stronger than $n$ -ary relations as the binary relations can imply the $n$ -ary relations. For instance sets resulting from a Cayley-Dickson construction have power-associativity, which is much weaker than binary associativity because it only applies to $x^n$ rather than for products of distinct elements with $n$ -many repetitions, or even that all products of $n$ -many terms are associative but products of $(n-1)$ -many terms are not necessarily so. To me power-associativity resembles a form of idempotency, however clearly in these algebras $x^n \neq x$ . There may be some very interesting structures only possible with $n$ -ary relations rather than binary relations, however my understanding is that if an algebra is pairwise commutative and tri-wise associative then it's also tri-wise commutative (we can permute $ABC$ however we like and preserve the result. Note: this doesn't imply commutativity between $AC$ if we have commutativity of $\{A,B\}$ and $\{B,C\}$ , only that all triple-products of $\{A,B,C\}$ are the same). Rings require commutativity of addition and fields require commutativity of both addition and multiplication, so clearly commutativity is important. There are non-commutative rings however addition is still commutative in these cases, so it would be interesting to see if there are many results for quasi-rings where addition is almost commutative, or how much commutative ring theory breaks down if we weaken the multiplicative commutativity. I'm aware of (Tropical) semi-rings however the condition which is weakened pertains to inverses and not to commutativity. While it isn't exactly weakening commutativity of fields there's Quantum Stochastic Calculus , it captures the spirit of the question which is to explore what happens when we weaken commutativity conditions. A semi-group is too weak since: We have closure We have extra structure via our ""not quite commutativity"" properties -Therefore I’m wondering if there are names for traits such as “Triad commutativity” or “Triad associativity”. Triad commutativity could be described as ""A symmetric (w.r.t. it's arguments) trinary function which can be decomposed into (not necessarily symmetric) binary functions such that under composition these binary functions yield a symmetric trinary function"". Triad associativity is analogous to the term power-associativity. There is $n$ -ary associativity , so we could just have a set equipped with a binary operation and impose $n$ -ary associativity on sufficiently long compositions of this binary operation. Power-associativity feels special compared to sets where associativity is arbitrarily restricted to a weaker form since it holds for Octonions, Sedenions, etc. This resembles the concept of the center of a group , since we could look for a subgroup with ""tri-commutativity"" within a non-Abelian group. It might be messier to ask for a subset (not necessarily a subgroup, as we may lack closure) of a group where any two elements are ""tri-commutative"" with the rest of the group (or even more restrictive, to find the elements that are tri-commutative with any other two elements of the group, perhaps requiring these special elements to be the middle term in our triple product though this may not be necessary). The question in the title is thus whether there are (named and hopefully interesting) structures for which $g(x,y,z) = g(y,x,z) = g(x,z,y)$ and $f(x,y) \neq f(y,x)$ , $f(y,z) \neq f(z,y)$ [where $g(a,b,c) = f(a,f(b,c)) = f(f(a,b),c)$ ]. I'm not sure whether this implies $g(x,y,z) = g(z,x,y) = g(z,y,x)$ , or if many non-commutative structures have this for the special case where for some $x$ , $z$ we have $f(x,z) = f(z,x)$ . Associativity can be represented as $f(f(x, y), z) = f(x, f(y, z))$ . Are there any names for things like these, such as “pseudo-Abelian"", ""sub-Abelian"" or ""hypo-Abelian”? Sub-Abelian might be a bad name since it could instead refer to Abelian subgroups of non-Abelian groups, such as how for any group $G$ and $g\in G$ , then $⟨g⟩=\{g^n:n\in Z\}$ is Abelian. The subgroups generated this way seem very simple, but for some groups (e.g. Quaternions for $\{±1\}$ ) there might be much fancier ones to construct. Could we call the ability to carry out this construction ""power sub-Abelian""?","['associativity', 'noncommutative-algebra', 'products', 'binary-operations', 'group-theory']"
3786184,Deforming the torus without a point to $S^1 \lor S^1$,"Let $T$ be the topological torus, given by taking a rectangle with parallel sides oriented in the same direction, and glueing together each pair of parallel sides along the given direction. Take a point $P\in T$ and remove it, I want to show that $S^1 \lor S^1$ is
a deformation retract of $T-\{P\}$ which is given by glueing two
circles at some point. Intuitively: The point $P$ divides the inner area of the rectangle in four parts, those points lying above $P$ or below $P$ , and left and right of $P$ . These parts will be the triangles delimited by the segments connecting the point $P$ to the vertices. Now I can define a retraction collapsing each point of the rectangle to the corresponding point on the base of the triangle it belongs to.
This is not well defined for points lying on the segments connecting $P$ to the vertices, but each choice will give some retraction, and the retraction will be homotopic to the identity because the rectangle is a convex set ( edit : I think this point is wrong, because if I remove a point I lose the convexity). So we have produced a retraction of the rectangle onto its sides. Passing to the quotient which identifies the parallel sides, this composition is still a retraction if we quotient also the rectangle by the same relation making it a torus. We conclude by observing that quotienting the boundary of the rectangle by the above relation gives a space homeomorphic to two circles glued at a point, and we are done. Can we make this argument more rigorous? What I am not satisfied about my argument is that it does not make
clear why it is important to remove a point from the torus. What is a
rigorous way to make clear that removing a point is necessary to make
this argument work? My main question is: Where precisely in the argument above am I using the fact that I removed the point $P$ ?","['proof-writing', 'geometric-topology', 'general-topology', 'differential-topology', 'algebraic-topology']"
3786194,"On a variety, every line bundle $L$ is isomorphic to $\mathcal{O}(D)$ for some Cartier divisor $D$?","I'm trying to read a part of the proof of Lemma 2.2 in Fulton's Intersection theory . He wants to prove that every line bundle $L$ on a variety (integral, of finite type over $k$ ) $X$ is of the form $\mathcal{O}(D)$ form some Cartier divisor $D$ . Let $g_{\alpha \beta}$ be transition functions for $L$ for some affine open covering $\{U_\alpha\}$ of $X$ . Fix one index $\alpha_0$ and define $f_\alpha = g_{\alpha \alpha_0}$ . Then $f_\alpha / f_\beta = g_{\alpha \beta}$ , so the data $(U_\alpha, f_\alpha)$ define a Cartier divisor $D$ with $\mathcal{O}(D) \cong L$ . By definition, the $f_\alpha$ should belong to $\mathcal{K}(U_\alpha)$ , however $g_{\alpha \alpha_0}$ is an isomorphism $\mathcal{O}(U_\alpha \cap U_{\alpha_0}) \to \mathcal{O}(U_\alpha \cap U_{\alpha_0})$ , so it is not defined on the whole of $U_\alpha$ . So in a sense, $(U_\alpha \cap U_{\alpha_0}, f_\alpha)_\alpha$ defines a Cartier divisor on $U_{\alpha_0}$ , not on $X$ ? What is happening here? Also, does he use here that $X$ is a variety ?","['divisors-algebraic-geometry', 'algebraic-geometry', 'intersection-theory', 'line-bundles']"
3786205,Prove a recursive relation,"Suppose one has the following recursive relation : $$
a_{n+1} = a_n + \dfrac{1}{a_n}
$$ Where: $$a_n > 0$$ Is there any way to find a closed form formula for something like this? I tried looking at the local factors by looking at how it develops, but it seems very chaotic. I found an inverse formula for this relation: $$
a_{n-1} = \dfrac{a_n+ \sqrt{{a_n}^2 - 4}}{2}
$$ A bit odd is the fact that the inverse function is undefined ( in the reals) for $a_n < 2$ even though the regular function is defined for values lower than 2. Also, since the original function is increasing and monotonic, we know that the inverse is decreasing and monotonic - which means that for very large values of $a_0$ we always expect that the limit as n goes to infinity to be undefined, as it is guaranteed to go lower than 2 at some point in the sequence. Thats all I got basically, Would love to hear if you have any ideas or know anything about these types of recursive relations, because I noticed  for example: $$
a_{n+1} = a_n + \dfrac{1}{2a_n}
$$ has a very similar inverse: $$
a_{n-1} = \dfrac{a_n+ \sqrt{{a_n}^2 - 2}}{2}
$$","['inverse-function', 'recurrence-relations', 'sequences-and-series']"
3786212,"Prove that limit of function does not exist, if and only if sequence $f(s_n)$ is not convergent.","I don't understand the hint (in bold) to prove this problem. Problem : Let $f: D \rightarrow  \mathbb{R}$ and let $c$ be an accumulation point of D. Then the following are equivalent: (a) $f$ does not have a limit at $c$ . (b) There exists a sequence $(s_n)$ in $D$ with each $s_n \ne c$ such that $(s_n)$ converges to $c$ , but $f((s_n))$ is not convergent in $\mathbb{R}$ . Hint : To prove that $(a) \Rightarrow (b)$ , suppose that (b) is false. Let $s_n$ be a sequence with $s_n \rightarrow c$ . Before we can use Theorem 20.8, we must show that given any sequence $(t_n)$ in $D$ with $t_n \Rightarrow c$ , we have limit $f(t_n)=L$ . We only know from the negation of (b) that $(f(t_n))$ is convergent. To see that $\lim f(t_n) = L$ , consider the sequence $(u_n) = (s_1, t_1, s_2, t_2...)$ and note that $(f(s_n))$ and $(f(t_n))$ are both subsequences of $(f(u_n))$ . My question: The hint wants for any sequence $(t_n)$ . But, how does $(u_n)$ help showing $(t_n)$ is any sequence? What do I need $(u_n)$ for? The proof is here under Theorem 2.","['sequence-of-function', 'limits', 'proof-writing', 'real-analysis']"
3786217,Proving: $\lim_{x\to\infty}\frac{f(x)f''(x)}{(f'(x))^2}=\frac{1}{2-c}$,"let f be a function three times differentiable on $\mathbb{R}_{+}^{*}$ such that $f(x)> 0, f'(x)>0$ and $f''(x)>0$ for $x>0$ prove that if: $$\lim_{x\to\infty}\frac{f'(x)f'''(x)}{(f''(x))^2}=c ,\quad  c\neq 1$$ So: $$\lim_{x\to\infty}\frac{f(x)f''(x)}{(f'(x))^2}=\frac{1}{2-c}$$ using the hospital rule we get: $(1)\quad $ $$\lim_{x\to\infty}\frac{f'(x)f'''(x)}{(f''(x))^2}=\lim_{x\to\infty}\left(1-\frac{f'(x)}{xf''(x)}\right)=c$$ proof $(1)$ : $$\lim_{x\to\infty}\left(1-\frac{f'(x)}{xf''(x)}\right)=\lim_{x\to\infty}\frac{\left(x-\frac{f'(x)}{f''(x)}\right)}{x'}=\lim_{x\to\infty}\frac{f'(x)f'''(x)}{(f''(x))^2}=c$$ So we find : $$
\lim_{x\to \infty}\frac{f'(x)}{xf''(x)}=1-c$$ This is what I found now, how can I use this result in finding the required or is there any other way to prove","['calculus', 'derivatives', 'real-analysis']"
3786220,Almost every square matrix satisfies Cayley-Hamilton Theorem,"I was watching Steve Brunton's lecture and he pointed out that Cayley Hamilton theorem is not true for every single square matrix, but it's true for almost every of them: Someone pointed out to me that this might not actually be true for every single square matrix $A$ . So, almost every matrix $A$ satisfies its own characteristic equation. I don't want to get into the edge cases where this is not true. You can look this up in a linear algebra book and find out if this is true everywhere, but basically this is true for most matrices, OK? I think it might actually be true for every matrix... Could you clarify what is the matrix which does not satisfy the theorem?","['linear-algebra', 'control-theory']"
3786221,The volume of a parallelepiped $p_2$ spanned by the face diagonals of another parallelepiped $p_1$ is twice the volume of $p_1$.,"I would like to purely geometrically (without referring to the dot and cross vector product) prove the following: The volume of a parallelepiped $p_2$ spanned by the face diagonals of another parallelepiped $p_1$ is twice the volume of the $p_1$ , i. e. $V_{p_2}=2V_{p_1}$ . The statement follows easily from the definition: Let $\vec a,\vec b,\vec c$ be vectors of the sides with the same origin in a vertex of the parallelepiped $p_1$ $$\begin{aligned}V_{p_2}&=\left(\vec b+\vec c\right)\cdot\left(\left(\vec a+\vec c\right)\times\left(\vec a+\vec b\right)\right)\\&=\left(\vec b+\vec c\right)\cdot\left(\underbrace{\vec a\times\vec a}_{=0}+\vec a\times\vec b+\vec c\times\vec a+\vec c\times\vec b\right)\\&=\underbrace{\vec b\cdot\left(\vec a\times\vec b\right)}_{=0}+\vec b\left(\vec c\times\vec a\right)+\underbrace{\vec b\cdot\left(\vec c\times\vec b\right)}_{=0}+\vec c\cdot\left(\vec a\times\vec b\right)+\underbrace{\vec c\cdot\left(\vec c\times\vec a\right)}_{=0}+\underbrace{\vec c\cdot\left(\vec c\times\vec b\right)}_{=0} \end{aligned}$$ Obviously $\vec c\cdot\left(\vec a\times\vec b\right)=\vec c\cdot\left(\left(\vec a+\vec c\right)\times\left(\vec a+\vec b\right)\right)$ , so the determinant won't be changed by adding these
rows, i. e., $$\begin{vmatrix}c_1&c_2&c_3\\a_1&a_2&a_3\\b_1&b_2&b_3\end{vmatrix}=\begin{vmatrix}c_1&c_2&c_3\\a_1+c_1&a_2+c_2&a_3+c_3\\a_1+b_1&a_2+b_2&a_3+b_3\end{vmatrix}.$$ This means the volume will remain the same as long as the new parallelepiped is spanned by at least one former vector side. We can interpret it like this:
Let $ABCDEFGH$ be an arbitrary parallelepiped and let $\begin{aligned}\vec a&=\overrightarrow{AB}\\\vec b&=\overrightarrow{BC}=\overrightarrow{AD}\\\vec c&=\overrightarrow{AE}\\\vec a+\vec b&=\overrightarrow{AC}\\\vec b+\vec c&=\overrightarrow{ AH}\end{aligned}$ . Let $I\in CD$ s. t. $\overrightarrow{CI}=\vec a$ then the area of the parallelogram $ABIC$ spanned by the vectors $\vec a$ and $\vec a+\vec b$ equals the area of the parallelogram $ABCD$ spanned by the vectors $\vec a,\vec b$ . Next, let $J, K$ be points s .t. $\overrightarrow{AB}=\overrightarrow{IJ}=\overrightarrow{CK}$ . Then the parallelepipeds $ABCDEFGH$ and $ABICHGJK$ have equal heights and bases, and hence, equal volumes. Let points $L,M,N$ be s. t. $\overrightarrow{NL}=\overrightarrow{AH}=\overrightarrow{BG}$ . Then $ACKH\cong BIJG\cong FNLM$ . Picture: However, I don't know how to continue proving the volume of $AFMHCNLK$ is twice the volume of $ABICHGJK$ . May I ask for advice on solving this task? Thank you in advance!","['euclidean-geometry', 'vectors', 'geometry', 'volume']"
3786231,On Complex Improper Integrals and Rectifiable Paths,"I would like to know if we may generalize both definite and improper complex integral as follows: I tried to write as Conway does in his book. Let's define a path in $U \cup \partial{U}$ , $\Gamma$ , as follows $$\Gamma: [0,1] \to U \cup \partial{U} \\ u \mapsto \Gamma(u) = p(z) + u \cdot [q(z) - p(z)]$$ Let $g: U \to \mathbb{C}$ and $p,q: D \to U \cup \partial{U}$ both analytic on their open and connected domains, and $G: U \cup \partial{U} \to \mathbb{C}$ , primitive of $g$ . Then $I$ converges for every $z \in D$ . $$ I: D \to \mathbb{C}  \\z \mapsto I(z) = \int_\Gamma g(s)\,ds = G(q(z)) - G(p(z)), \\ \Gamma: [0,1] \to U \cup \partial{U} \\ u \mapsto \Gamma(u) = p(z) + u \cdot [q(z) - p(z)]\\ \, \\$$ The main question: Is this how complex improper integrals are defined? I.e., we define a path which one of its end point is at the boundary of the integrand's domain? Is there anything missing? Most books emphazise solving complex improper integrals via residues, but that's not what I want now. Does $\Gamma$ need to be rectifiable? I guess it is already As far as I know, we have the following possibilities: P1 $I(z)$ will be a definite integral, if $z \in D$ such that $p(z), q(z) \in U$ . P2 On the other hand, $I(z)$ will be an improper integral, if $ z \in D$ such that $ p(z) \lor q(z) \in \partial{U}$ . Would this be correct? From Conway's Graduate book, we have Theorem 1.18 (which I'm using as an example): Let $G$ be open in $\mathbb{C}$ and let $\gamma$ be a rectifiable path in $G$ with initial and end points $\alpha$ and $\beta$ resceptively. If $f: G \to \mathbb{C}$ is a continuous function with a primitive $F:G \to \mathbb{C}$ , then $$\int_\gamma f = F(\beta)-F(\alpha)$$ (Recall that $F$ is a primitive of $f$ when $F'=f$ .) I have read the whole chapter, but no detailed explanation was found. Thus, any other references would be appreciated. Thanks EDIT It's been a while since the bounty started and no answers. I'm starting to think that my question is ""wrong"" somehow. Please comment anything that may be corrected. Thanks again EDIT 2 If $g$ is analytic on its open domain $U$ , shouldn't it be also at $\partial{U}$ ? If so, we may use the Theorem 1.18. Right?","['integration', 'complex-analysis', 'contour-integration', 'solution-verification', 'complex-integration']"
3786234,Complex eigenvalues of a matrix in conjugate pairs (or not),"I have learnt that in a matrix, if there are complex eigenvalues, they should come as conjugate pairs. Also, I know that, in a diagonal matrix, eigenvalues are the diagonal elements. So how about the following matrix? $$\begin{pmatrix}
i & 0\\ 
 0& 2
\end{pmatrix}$$ Shouldn't the eigenvalues be $i$ and $2$ , where it doesn't have a conjugate pair?! I appreciate your help to clarify my mistake.","['eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'algebra-precalculus', 'complex-numbers']"
3786312,Show that $f_n\to f$ in $L^p$ if and only if $f^p_n \to f^p$ in $L^1.$,"Let $f_n$ be a sequence of nonnegative measurable functions in $L^p(\mathbb R)$ for some $1<p<\infty.$ Show that $f_n\to f$ in $L^p$ if and only if $f^p_n \to f^p$ in $L^1.$ I showed that if $f,f_n\in L_p$ and $f_n \to f$ pointwise a.e., then $||f−f_n||_p \to 0$ iff $||f_n||_p \to||f||_p$ . But I found that $||f_n||_p \to||f||_p$ was different from $f^p_n \to f^p$ in $L^1.$ So, it didn't help with my problem. I got stuck on this problem.","['measure-theory', 'lp-spaces', 'lebesgue-integral', 'real-analysis']"
3786317,"CGMO 2020: Prove that $X, P, Q, Y$ are concyclic.","In the quadrilateral $ABCD$ , $AB=AD$ , $CB=CD$ , $\angle ABC =90^\circ$ . $E$ , $F$ are on $AB$ , > $AD$ and $P$ , $Q$ are on $EF$ ( $P$ is between $E, Q$ ), satisfy $\frac{AE}{EP}=\frac{AF}{FQ}$ . $X, Y$ are on $CP, CQ$ that satisfy $BX \perp CP, DY \perp CQ$ . Prove that $X, P, Q, Y$ are concyclic. My Progress: Couldn't proceed much . I noted that $ABCD$ is cyclic quad with diameter $AC$ .  I feel to use POP on C , so it is enough to show that $CX\cdot CP= CY\cdot CQ$ . But I am not sure about how to use "" $\frac{AE}{EP}=\frac{AF}{FQ}$ "" criteria . Please post hints rather than solution. It really helps me a lot. Thanks in advance.","['contest-math', 'euclidean-geometry', 'geometry', 'power-of-the-point', 'geometric-transformation']"
3786341,How do you prove $\pi =\sqrt{12}\sum_{n\ge 0}\frac{(-1)^n}{3^n(2n+1)}$?,"In the book Pi: A Source Book I found the following: Extract the square root of twelve times the diameter squared. This is the first term. Dividing the first term repeatedly by 3, obtain other terms: the second after one division by 3, the third after more division and so on. Divide the terms in order by the odd integers $1,\,3,\,5,\,\ldots$ ; add the odd-order terms to, and subtract the even order terms from, the preceding. The result is the circumference. That is equivalent to $$\pi =\sqrt{12}\sum_{n\ge 0}\frac{(-1)^n}{3^n(2n+1)}.$$ The formula is due to an Indian mathematician Madhava of Sangamagrama . The proof of this formula should be in the treatise Yuktibhāṣā written in c. 1530 by an Indian astronomer Jyesthadeva , which I don't have access to. I've been trying to find a proof of the formula elsewhere but with no success. Maybe this could be proved from $$\arctan x=\sum_{n\ge 0}\frac{(-1)^n x^{2n+1}}{2n+1}$$ which is mentioned in Yuktibhāṣā as well, but I don't see how could that be done.","['euclidean-geometry', 'pi', 'sequences-and-series', 'real-analysis']"
3786350,Conjecture about the transitive closure of a relation $\mathcal R $ over a finite set $A$,"Given a binary relation $\mathcal R$ over a set $A$ ,then the transitive closure of $\mathcal R$ over $A$ is the smallest transitive relation on $A$ containing $\mathcal R$ , it's indeed the intersection of all transitive relations over $A$ that are a superset of $\mathcal R$ . The transitive closure of $\mathcal R$ is denoted by $\mathcal R^{+}$ and has the following explicit formula: $$\mathcal R^{+}=\bigcup_{n=1}^{\infty} \mathcal R^n$$ Where $\mathcal R^1=\mathcal R$ , and $\mathcal R^{n+1}=\mathcal R^n ∘ \mathcal R^1\tag{$n \in \mathbb N^+$}$ The first time I faced the formula I thought that it is not practically applicable,since it's needed to take the union of infinitely many relations (with infinity many distinct relations). However after working with some sets I realized that it's not generally true,for example let $\mathcal R$ be a set over $A=\left\{1,2,3\right\}$ defined as $$\mathcal R:=\left\{(3,2),(2,3)\right\}$$ Then one can prove by induction that $$\mathcal R^{2n}=\left\{(3,3),(2,2)\right\}=\text{id}_A \setminus \left\{(1,1)\right\}\;,\;\mathcal R^{2n-1}=\left\{(3,2),(2,3)\right\}=\mathcal R\tag{$n \in \mathbb N^{+}$}$$ So the transitive closure of $\mathcal R$ over $A$ is given by: $$\mathcal R^{+}=\bigcup_{n=1}^{\infty} \mathcal R^n=\left[\bigcup_{n=1}^{\infty} \mathcal R^{2n-1}\right]\cup \left[\bigcup_{n=1}^{\infty} \mathcal R^{2n}\right]  $$ $$=\mathcal R \; \cup \text{id}_A \setminus \left\{(1,1)\right\}=\left\{(3,2),(2,3),(3,3),(2,2)\right\}$$ I computed the transitive closure of $\mathcal R$ over a few sets,and all of them had such a pattern,neither of them needed the union of an infinite number of infinity many distinct relation to be taken. The questions are : Is this true that to determine the transitive closure of a relation $\mathcal R $ over a finite set $A$ , we always just need to take the union of infinite number of finitely many distinct relations on $A$ ? If no,then give a counterexample, if yes then please prove it.","['elementary-set-theory', 'relations', 'examples-counterexamples', 'function-and-relation-composition']"
3786378,Proof of the Iteration Theorem,"The author give a definition of the set $K$ used in the proof and four steps to do the proof. The question is stated as: A Proof about the Iteration Theorem. Consider a set $W$ , an element $c$ , and a singulary operation $g$ on $W$ . Let $(P,S,1)$ be a Peano system. We must show that there is a unique function $F:P\rightarrow W$ such that $F(1)=c$ and $F(S(x)=g(F(x))$ for all $x$ in $P$ . Let $K = \{ H:H \subseteq P\times W \land (1,c) \in H \land (\forall x)(\forall w)((x,w) \in H \Rightarrow (S(x), g(w)) \in H\}$ . Observe that $P \times W \in K$ . Let $F = \bigcap_{H \in K}H$ Prove: a. $F \in K$ b. $F:P \rightarrow W$ . (Hint: Let $B = \{x : (\exists!w)(w \in W \land (x,w) \in F) \}$ . By mathematical induction prove B=P.) c. $F(1) = c$ . d. $F(S(x)) = g(F(x))$ for all $x$ in $P$ . My atempt: a) By definition of $F$ we have that $(\forall x)(\forall y)((x,y) \in F \Rightarrow (\forall H)(H \in K \Rightarrow (x,y) \in H))$ , Thus $(\forall H)(H \in K \Rightarrow F \subseteq H)$ and as $(\forall H)(H \in K \Rightarrow H \subseteq P \times W)$ we have also $F \subseteq P \times W$ As for all $H$ in $K$ we have $(1,c) \in H$ , then $ (1,c) \in \bigcap_{H \in K}H$ and therefore $(1,c) \in F$ . Let $(x,y)$ be some arbitrary element of $F$ , the by defintition it belongs to every $H$ in $K$ , thus by definition of $K$ , $(S(x), g(y))$ belong to every $H$ in $K$ and therefore belongs to $F$ also, as $(x,y)$ is arbitrary we got $(\forall x)(\forall y)((x,y) \in F \Rightarrow (S(x), g(y)) \in F)$ . Thus we have that $F$ met all conditions to belong to $K$ and therefore $F \in K$ . b) (Using the set $B$ defined in the hint above) First let some $N \in K$ where $(\forall y)((1,y) \in N \Rightarrow y=c)$ , from this we have that $(\forall y)((1,y) \in \bigcap_{H \in K}H) \Rightarrow y=c$ , thus by definition of $F$ we have that $(\exists !w)(w \in W \land (1,w) \in F)$ , namely $w=c$ , therefore $1 \in B$ . Now assume $x \in B$ thus $(\exists !w)(w \in W \land (x,w) \in F)$ , by definition of $F$ we have that $(x,w)$ belong to every $H$ in $K$ , thus by definition of $K$ we got that $(S(x), g(w))$ belong to every $H$ in $K$ and therefore $(S(x), g(w)) \in F$ . Let some $M$ in $K$ be defined with the following restriction $(\forall y)((S(x),y) \in M \Rightarrow y = g(w))$ , thus $(\forall y)((S(x),y) \in \bigcap_{H \in K}H \Rightarrow y=g(w))$ , from this and by definition of $F$ we have that $(\exists !u)(u \in W \land (S(x), u) \in F)$ , namely $u=g(w)$ , therefore we have shown that $x \in B \Rightarrow S(x) \in B$ , thus by Mathematical induction $B=P$ . c) We have $(1,y) \in F$ , then $(1,y)$ belong to every $H$ in $K$ . Let some $O$ in $K$ where $(\forall y)((1,y) \in O \Rightarrow y=c)$ , then $(\forall y)((1,y) \in \bigcap_{H \in K}H \Rightarrow y=c)$ , thus by definition of $F$ we have that $F(1) = c$ . d) Let $A = \{x : x \in P \land F(S(x)) = g(F(x))\}$ . First $(1,c) \in F \Rightarrow (\forall H)(H \in K \Rightarrow (1,c) \in H)$ , thus $(S(1), g(c))$ belong to every $H$ in $K$ and from this $F(S(1)) = g(F(1)) = g(c)$ , thus $1 \in A$ . Now assume $x \in A$ , thus $x \in P \land F(S(x)) = g(F(x))$ , as $(S(x), g(F(x)) \in F$ , then $(S(x), g(F(x))$ belong to every $H$ in $K$ , therefore (S(S(x)), g(g(F(x)))) belong to every $H$ in $K$ thus we got that $F(S(S(x))) = (g(F(S(x)))$ , thus $S(x) \in A$ . We have shown that $x \in A \Rightarrow S(x) \in A$ , then by Mathematical induction A=P.","['elementary-set-theory', 'solution-verification']"
3786380,"Finding a closed form for $\sum_{n=1}^{\infty}\frac{1}{F(n)F(n+2k)}$, where $F(n)$ is the $n$-th Fibonacci number","So, I want to find the closed value for the sequence $$\sum_{n=1}^{\infty} \frac{1}{F(n)F(n+2k)}$$ Here by $F(n)$ I mean the $n$ -th term of Fibonacci Sequence. I got motivated for this by the YouTube video ""Using partial fractions to evaluate two Fibonacci reciprocal sums"" by Prof. Michael Penn. So in the end of video he says for finding this for the general case : k. So i tried it and here is what i did:- Let $$\frac{1}{F(n)F(n+2k)} = \frac{A}{F(n)} + \frac{B}{F(n+2k)}$$ so by rearranging we can say that: $AF(n+2k) + BF(n) = 1$ Now let $C_0 = F(n+2k)$ then $C_1 = F(n+2k-1) + F(n+2k-2)$ so $C_2 = 2F(n+2k-2) + F(n+2k-3)$ then by induction we can prove that if i repeat this process j times then $C_j = F(j+1)F(n+2k-j) + F(j)F(n+2k-j-1)$ So now let $j = 2k - 1$ F
then $C_{2k-1} = F(2k)F(n+1) + F(2k-1)F(n)$ now putting this in to the Eq, of A and B then $AF(2k)F(n+1) + AF(2k-1)F(n) + BF(n) = 1$ now by putting $F(n+1) = F(n+2) - F(n)$ then I will get $AF(2k)F(n+2) + (B+AF(2K-1) - AF(2k))F(n = 1)$ there can be infinite values of A and B satisfying this relation so if I let $A = \frac{1}{F(2k)F(n+2)}$ becaues the first term in the Eq.  will become 1 then $B = \frac{F(2k-2)}{F(2k)F(n+2)}$ By putting this values in the summation I will get $\sum_{n=1}^{\infty}\frac{1}{F(2k)F(n)F(n+2)} + \frac{F(2k-2)}{F(n+2)F(n+2k)}$ now in the video It is showned that $\sum_{n=1}^{\infty}\frac{1}{F(n)F(n+2)} = 1$ so I am not going to derive it. Now putting it we get $\frac{1 + F(2k-2)\sum_{n=1}^{\infty}\frac{1}{F(n+2)F(n+2k)}}{F(2k)}$ $\frac{1 + F(2k-2)\sum_{n=3}^{\infty}\frac{1}{F(n)F(n+2(k-1))}}{F(2k)}$ in this I have changed the limit. Now by adding and subtracting some termes I get $Sum(k) = \frac{1 + F(2k-2)\sum_{n=1}^{\infty}\frac{1}{F(n)F(n+2(k-1))} - \frac{F(2k-2)}{F(2k-1)} - \frac{F(2k-2)}{F(2k)}}{F(2k)}$ Now finnaly I ended up with this $Sum(k) = \frac{1 + F(2k-2)Sum(k-1) - \frac{F(2k-2)}{F(2k-1)} - \frac{F(2k-2)}{F(2k)}}{F(2k)}$ But this is not a direct formulae. Can someone help?","['fibonacci-numbers', 'sequences-and-series']"
3786398,Exterior differentiation under pullback question,"In Loring Tu's book, An Introduction to manifolds , he uses the following proposition to prove that the pullback of a smooth $k$ form is a smooth $k$ form: Proposition :
Let $F:N\rightarrow M$ be a smooth map. If $\omega$ is a smooth k-form then $$\mathrm dF^{*}\omega=F^{*}\mathrm d\omega.$$ However, my question is: Isn't the exterior derivative, $\mathrm d$ is a map $\Omega^{k}(M)\rightarrow \Omega^{k+1}(M)$ (space of smooth $k$ , $k+1$ forms), so isn't he assuming that $F^{*}\omega$ is smooth?","['pullback', 'differential-forms', 'differential-geometry']"
3786432,Maps between Equivalence Relations and Partitions,"NOTE: [I’m not (yet) interested in proving that there is a bijection] ~ not a duplicate Consider the following definition. Definition: Let $A$ be a non-empty set. Let $\varepsilon(A)$ denote the set of all equivalence relations on $A$ , and let $\mathcal{T}_{A}$ be the set of all partitions of $A$ . Define a map $\Phi:\varepsilon(A) \to \mathcal{T}_{A}$ as follows. If $\sim$ is an equivalence relation on $A$ , let $\Phi(\sim)$ be the quotient set $A/\sim$ . Define the map $\Psi: \mathcal{T}_{A} \to \varepsilon(A)$ as follows. If $\mathcal{D}$ is a partition of $A$ , let $\Psi(\mathcal{D})$ be the relation on $A$ given by $x \Psi(\mathcal{D}) y$ if and only if ther is some $P \in \mathcal{D}$ such that $x, y \in P$ , for all $x, y \in A$ . Then, consider the following lemma. Lemma: Let $A$ be a non-empty set. The maps $\Phi$ and $\Psi$ in the above definition are well-defined. I want to prove this result. Although I don’t know what I should do in order to show the well-definition of these maps. The problem: Generally speaking, let $A, B$ be non-empty sets and let $f: A \to B$ . To show that $f$ is well-defined we need to show: that $x = y$ implies $f(x) = f(y)$ for all $x, y \in A$ OR that $f(x) \in B$ for all $x \in A$ . And how do I apply the correct strategy in the case of the maps $\Phi$ and $\Psi$ ? Thank you so much in advance!","['set-partition', 'proof-explanation', 'equivalence-relations', 'functions', 'elementary-set-theory']"
3786441,Matrix differential of a trace with Hadamard product,"I'm encountering difficulties taking the differential of the following matrix expression with respect to $S$ : $\text{logdet}(S) + \text{Tr}[C(D\odot((AS^{-1/2}B)(AS^{-1/2}B)^{T}))]$ $C$ and $D$ are symmetric and $S$ is diagonal so I mean taking the element-wise inverse of the element-wise square-root by the notation $S^{-1/2}$ . From Matrix CookBook, I know that the first term leads to $\text{Tr}(S^{-1}dS)$ and I know that I can apply the differential of the expression inside the trace term but I'm struggle with the computation of the differential because of the quadratic form coupled with the Hadamard product. I have tried to rewrite the expression by means of Hadamard and Frobenius products (that are commutative)...without success. Then, my goal is to find the ""roots"" of the derivative with respect to $S$ . Given the form of the expression, my intuition is that I will obtain a fixed-point expression (in the sense that it is not possible to obtain a closed-form expression in the form $\hat{S}=$ something that doesn't depend on $S$ ), but it's not a problem, I will solve it numerically. Can you help me ? Thank you in advance.","['differential', 'hadamard-product', 'matrices', 'matrix-calculus', 'derivatives']"
3786470,What's wrong with manipulating this algebraic equation? and why does a manipulated system of equations have a different solution than the original?,"I'll give an example for my first question: $x^2 + x + 1 = 0$ Clearly $x = 0$ and $x = 1$ aren't solutions, so first we can safely divide by $x$ : $x + 1 + 1/x = 0$ By subtracting $1/x$ from both sides we get: $x + 1 = -1/x$ By plugging the value $x + 1$ back we get: $x^2 - 1/x = 0$ Multiplying by $x$ and adding $1$ to both sides: $x^3 = 1$ Which $x = 1$ is clearly a solution to, unlike the original equation. I have a problem with this, all manipulations did not include dividing by zero or any non-defined operations, all what was done is expressing $x$ by a different way so why does it change the final solution? When we divided the equation by $x$ the solution shouldn't change which means the first equation holds so why isn't the second one compatible with the first? On another note, I have a different question that is slightly related to this one, an example for this one is: We have the system of equations $S$ which is: $x + y + z = 1 \quad (L1)$ $x + y - z = 1/2 \quad (L2)$ $x - y + z = -4 \quad (L3)$ Then we transform this system into $S'$ by manipulating equations together: $2x + 2y = 3 \quad (L1 + L2)$ $2y - 2z = 6 \quad (L2 - L3)$ $2x + 2z = -3 \quad (L1 + L3)$ The solution for $S$ (which is $(-3/2, 5/2, 0)$ ) also doesn't satisfy $S'$ , which is counterintuitive to me because that's the kind of transformations we're taught then why does it change the solution? For both questions are those like general phenomenons or are they just special cases?
What are the names of topics concerned with the act of transforming equations like the examples above?","['systems-of-equations', 'linear-algebra', 'polynomials', 'linear-transformations', 'algebraic-equations']"
3786495,Convergence of $\sum_{n=1}^\infty 2^n\sin\frac{1}{3^nz}$,"The problem is:
prove $\sum_{n=1}^\infty 2^n\sin\frac{1}{3^nz}$ converges absolutely for all $z\neq 0$ , but does not converge uniformly near $z=0$ . Proof :
for all $z\neq 0$ $$\left|2^n\sin\frac{1}{3^nz}-2^n\frac{1}{3^nz}\right|
= \left|
2^n\left(
    -\frac{(\frac{1}{3^nz})^3}{3!}+\frac{(\frac{1}{3^nz})^5}{5!}\dots
\right)
\right|, \ \ \ \ (1)$$ $\exists~ N$ such that when n>N, $\frac{1/z}{3^n}<1$ , so that (1) is less than $$\left|
2^n\left(
    \frac{|\frac{1}{3^nz}|^3}{3!}+\frac{|\frac{1}{3^nz}|^5}{5!}\dots
\right)
\right|
<\left|
\frac{2^n}{3^nz}\left(
    \frac{|\frac{1}{3^nz}|^2}{1-|\frac{1}{3^nz}|^2}
\right)
\right|
<\frac{2^n}{3^n}\left|
\frac{1}{z}\left(
    \frac{|\frac{1}{3^nz}|^2}{1-|\frac{1}{3^nz}|^2}
\right)
\right|,
$$ $\forall~ \epsilon, \exists~ N_1>-\log(\epsilon^{1/2} z^{3/2})$ , such that when $n>N_2=\max\{N, N_1\}$ , $|\frac{1}{z}||\frac{1}{3^nz}|^2<\epsilon$ , and so (1) is less $\frac{2^n}{3^n}\epsilon$ . Therefore, we have $N_2(\epsilon)$ satisfying that $\forall~ p,$ $$\left|\sum_{n=N_2}^{N_2+p} 2^n\sin\frac{1}{3^nz}-\sum_{n=N_2}^{N_2+p} 2^n\frac{1}{3^nz}\right|
<\sum_{n=N_2}^{N_2+p} \left|2^n\sin\frac{1}{3^nz}-2^n\frac{1}{3^nz}\right|\ \ \ \ (2)\\
<\sum_{n=N_2}^{N_2+p}\frac{2^n}{3^n}\epsilon
\leq 2\epsilon,
$$ and so $\sum_{n=1}^\infty 2^n\sin\frac{1}{3^nz}$ converges absolutely. (A step seems to be missing. One should, instead of $\sum_{n=N_2}^{N_2+p}\frac{2^n}{3^n}$ , use something like 2/z (plus a constant), which is the limit of the former.) $\blacksquare$ A possibly trivial question is whether it is proper to prove this way: given n sufficiently large, $u_n<f(n)\epsilon$ (different from that $u_n<\epsilon$ , or that $u_n/f(n)<\epsilon$ and so $u_n<f(n)\epsilon$ ). There are other questions that I may post somewhere else. The following is a proof that the series doesn't converge uniformly. It's unnecessarily for answering my questions , but I put it here for completeness of proof. Proof : However large $N_2$ , we can find $n_0$ > $N_2$ and $z=\frac{2}{\pi 3^{n_0}}$ such that $2^{n_0}\sin\frac{1}{3^{n_0}z}=2^{n_0}$ , and so (say the limit function is $f(z)=\frac{2}{z}+C$ , where $C$ is a constant; I suddenly realize C seems also to be a function of z, that could cause some issues), $$\left|\sum_{n=1}^{\infty} 2^n\sin\frac{1}{3^nz}-f(z)\right|
>\left|\sum_{n=N_2}^{N_2+p} 2^n\sin\frac{1}{3^nz}-\sum_{n=N_2}^{N_2+p} 2^n\frac{1}{3^nz}-C\right|>|2^{n_0}-\frac{2}{z}-C|>\epsilon.$$","['complex-analysis', 'absolute-convergence']"
3786510,Is this “limit” of a sequence of $L^2$ functions in $L^2$?,"Suppose we have a sequence $\{f_n\}$ in $L^2([0,1])$ and a Lebesgue measurable $f$ such that $$\int_E f_ndx\rightarrow\int_E fdx$$ as $n\rightarrow\infty$ for every Lebesgue measurable subset $E\subseteq[0,1]$ . If $\sup_n\|f_n\|_2<\infty$ , then do we necessarily have $f\in L^2([0,1])$ ? I’m not seeing how to show this, one way or the other. I tried constructing a counterexample along the lines of $f_n(x)=x^{\frac{1}{n}-\frac{1}{2}}$ , since then $f(x)=x^{-1/2}$ isn’t square-integrable, but this doesn’t satisfy either condition. I’m inclined to think that we do have $f\in L^2([0,1])$ , since it doesn’t seem like you can make $\int_{[0,1]}|f|^2dx$ blow up without either of $\int_{[0,1]}|f_n|^2dx$ or $\int f dx$ blowing up, but I don’t see a way to formalize or justify this instinct.","['lebesgue-measure', 'functional-analysis', 'real-analysis']"
3786543,Evaluate $\tan\frac{\pi}{7}\tan\frac{2\pi}{7}\tan\frac{3\pi}{7}=\sqrt 7$ [duplicate],"This question already has answers here : Show that $\tan{(\pi/7)} \tan{(2\pi/7)}\tan{(3\pi/7)}=\sqrt{7}$ (2 answers) Closed 3 years ago . I'm trying to show that $$\tan\frac{\pi}{7}\tan\frac{2\pi}{7}\tan\frac{3\pi}{7}=\sqrt 7$$ My attempt:
Since $\tan x=\frac{\sin x}{\cos x} $ so immediately the denominator is recognized as $$\prod_{k=1}^{3}\cos\frac{k\pi}{7}=\frac{1}{2^3}=\frac{1}{8}$$ as known since elementary classes. To tackle with sine product I tried as $\sin(x)=\cos\left(\frac{\pi}{2}-x\right)$ . But I fail with this ideas. How do  I deal with $\sin x$ product?","['trigonometry', 'geometry']"
3786546,Describe the graph of $f^{-1}$ when $f$ is decreasing and always negative - dispute with solution from Spivak.,"The question comes from Calculus by Spivak - Ch 12 - 2) iv) . It asks us to describe the graph of $f^{-1}$ when $f$ is decreasing and always negative. I interpret the inverse function as decreasing and not defining it for $x \geq 0$ . But using the tools I have at my disposal, mainly the diagonal line test and drawings, I would get a function  that is increasing but still defined only on for $x < 0$ . This is also what the solution manual says: here is a screenshot of its solution. I really don't agree with the drawing of the inverse function. I agree with how the original function, $f$ , is drawn. Based on the tools this is correct, but it feels wrong......What may I be missing to reconcile this somewhat simple issue?","['calculus', 'functions', 'inverse-function', 'real-analysis']"
3786557,Two different definitions of limits.,"There are two definitions of limits that I know of. Definition $(1)$ : Let $X$ be  subset of $\mathbf{R}^n$ , and $x_0$ a point in $\overline{X}$ . A function $f\colon X\rightarrow\mathbf{R}^m$ has the limit $a$ at $x_0$ if for all $\varepsilon>0$ , there exists $\delta>0$ such that for all $x\in X$ , we have $$ |x-x_0|<\delta \implies |f(x)-a|<\varepsilon.$$ Definition $(2)$ is the exact same, except with $$ 0<|x-x_0|<\delta \implies |f(x)-a|<\varepsilon.$$ I am not exactly sure how to reconcile these two definitions. With the first definition, limits work well with composition. It also has the interesting property, that if you take a limit of a function like $\lim_{x\rightarrow 0}\text{sgn}(x)$ , the limit does not exist since $\text{sgn}(0)=0$ , as of course $|0-0|<\delta$ for any $\delta>0$ . Thus, it seems like for $x_0\in X$ , the limit in $(1)$ exists at $x_0$ if and only if $f$ is continuous at $x_0$ (is this correct?). In $(2)$ however, the limit exists at a jump discontinuity since we are disregarding the point $x_0$ , and there are plenty of examples where the limit exists at $x$ while the function $f$ is not continuous at $x$ . The $\text{sgn}$ function mentioned previously fits the bill. Another interesting distinction that I thought of is when taking limits of both sides of an equation. Suppose that the domains of both $f$ and $g$ is $X$ and $x_0\in\overline{X}\setminus X$ . Then under definitions $(1)$ and $(2)$ , if $f(x)=g(x)$ for all $x$ in a neighborhood of $x_0$ , we have $$\lim_{x\rightarrow x_0}f(x)=\lim_{x\rightarrow x_0}g(x),$$ as $x_0\not\in X$ , so the distinction between the two definitions does not show up. Of course, this is provided the limit exists. However, if we do the same example with $x_0\in X$ , where $f(x)=g(x)$ for all $x$ in a neighborhood of $x_0$ excluding $x_0$ , under definition $(2)$ we again have the same result, but under definition $(1)$ a jump discontinuity at $x_0$ could imply that $$\lim_{x\rightarrow x_0}f(x)\neq\lim_{x\rightarrow x_0}g(x).$$ This seems somewhat significant. Is there a way to fit these together, or in general, what is going on? And how does the distinction get ""erased""? I don't think many books dwell on this at all.","['limits', 'real-analysis']"
3786586,How to solve double absolute value inequality?,"This question comes from Spivak Calculus Chapter 1. How can we algebraically solve $|x − 1|+|x − 2| > 1?$ I know that if we 2 absolute values and no constants, we can square both sides, but I'm pretty sure this is not the case here. My attempt was to split this into different sections: $|x − 1|+|x − 2| > 1 \rightarrow |x − 1| > 1 - |x − 2|$ .
So we would have: $x − 1 > 1 - |x − 2|$ $x − 1< -1 +| x − 2|$ Then we can split this into 4 equations more equations based on the absolute value on $(x-2)$ . However, after doing this, I obtained conflicting solutions and unsolvable expressions (i.e $2<-2$ ). That being said, how would I go about algebraically solving this inequality?
Thanks!","['algebra-precalculus', 'absolute-value', 'inequality']"
3786619,Defining localization of modules through universal property,"On page 33 of Vakil's book on Algebraic Geometry, he shows how one can define the localization of modules purely in terms of universal property and later shows that a specific definition satisfies the property. Basically he says that if $M$ is an $A$ -module and $S$ a multiplicative subset of $A$ , define a map $\phi:M\rightarrow S^{-1}M$ as being initial among $A$ -module maps $M\rightarrow N$ such that all elements of $S$ are invertible in $N$ i.e. $s\times\cdot: N\rightarrow N$ is an isomorphism for all $s$ . So far, so good. However, he then proceeds to make two assertions which confuse me: (i) The definition determines $\phi:M\rightarrow S^{-1}M$ up to unique isomorphism. (ii) By definition, $S^{-1}M$ can be extended to a $S^{-1}A$ -module. I tried to prove the first one by first assuming that there's another map $\psi:M\rightarrow B$ that also satisfies the universal property. I was tempted to then say, by the universal property defined, that there is a unique map from $f:B\rightarrow S^{-1}M$ such that $\phi = f\circ \psi$ . However, I have no way of knowing that elements of S are invertible in $S^{-1}M$ . Do note that we haven't explicitly defined $S^{-1}M$ yet and so we can't conclude that elements in $S$ are invertible in $S^{-1}M$ The second assertion is related to the first one. How does one conclude from the definition that $S^{-1}M$ can be extended to form a $S^{-1}A$ -module? I tried using the invertibility of $S$ in $N$ or choosing a specific $N$ but invertibility in $N$ says nothing about invertibility in $S^{-1}M$ and choosing a specific $N$ (localized module $M$ according to the actual definition for instance) seems like cheating since we're supposed to get the assertion purely via universal property. EDIT: So it seems like I didn't fully understand what ""initial"" meant. I didn't count $\phi:M\rightarrow S^{-1}M$ to be in the class of maps that have elements of $S$ to be invertible in the range. The problem is pretty straightforward after realizing that.",['algebraic-geometry']
3786639,Why is error term in the definition of derivative for $\mathbb{R}^n \to \mathbb{R}^m$ of order $o(h)$?,"The following intuition is usually supplied for a definition of derivative. We would like to approximate function $f(x)$ near some point $x_0$ with a linear map and we would like to show that as we get closer to $x_0$ , this approximation becomes good (for certain notion of good). The following equation is true in general (by definition of error term $\varepsilon(h)$ ). $$ f(x_0+h) - f(x_0) = L(h) + \varepsilon(h) $$ Now, as $L(h)$ and $\varepsilon$ are vectors of $\mathbb{R}^m$ , to compare them it is reasonable to use magnitude. So, ""intuitively"", we would like magnitude of relative error to become small as we approach $x_0$ . In other words, we would like: $$ \lim \limits_{h \to 0} R(h) = \lim \limits_{h \to 0}  \left( \frac{|\varepsilon(h)|}{|L(h)|} \right) = 0. $$ Question 1 : This expression does not make sense if $L(h) = 0$ which could certainly be a derivative for some function. How can in that case we say that function $0$ approximates function close to $x_0$ if relative error is undefined? Do we use some different criterion? Do we ignore this case? Now, if we assume limit is defined and exists, we can use that $|L(h)| \leq M|h|$ as it is a linear map $\mathbb{R}^n \to \mathbb{R}^m$ . Then, we get the following. $$ 0 = \lim \limits_{h \to 0} R(h)  \geq \frac{1}{M} \lim \limits_{h \to 0} \left( \frac{|\varepsilon(h)|}{|h|} \right)$$ From here it seems to be if we assume (?) limit exists and assume it is zero by out intuition on what good approximation is, we can show that it necessarily means that error is of order $o(h)$ . Question 2 : Is this reasoning valid? Question 3 : The other direction -- if error term is $o(h)$ -- does not seem to imply that relative error tends to $0$ . So, could it be that derivative is defined, but its relative error compared to non linear term does not vanish? How to interpret this with mindset of derivative being the best linear approximation? Appreciate your thoughts and comments.","['multivariable-calculus', 'definition', 'derivatives']"
3786668,Closed form of $\int\limits_0^{2\pi} \prod\limits_{j=1}^n \cos(jx)dx$ and combinatorial link,"I have been trying to find a closed form for this integral: $$I_n = \int\limits_0^{2\pi} \prod_{j=1}^n \cos(jx)dx$$ The first values are: $I_1=I_2=0,I_3=\frac{\pi}{2}, I_4=\frac{\pi}{4}, I_5=I_6=0, I_7=\frac{\pi}{8}, I_8=\frac{7\pi}{64}$ I am not able to see here a clean pattern except that for $n=4k+1,4k+2$ the integral should be zero. If someone could give me a hint I would appreciate it. EDIT As suggested by Winther in the comments, the problem can be viewed from a combinatorial standpoint. Looking at the complex exponential representation one gets $2^n$ integrals of the form $\int_0^{2\pi}e^{iNx}dx$ , which is only nonzero, if $N=0$ . The integral evaluates to $\frac{M\pi}{2^{n-1}}$ , where $M$ is the number of nonzero integrals. So one needs to find $M$ , which is the number of binary numbers $b$ for which holds that $$\sum_{k=1}^n (2b_k-1)k = 0$$ where $b_k$ is the k-th digit of $b$ .
With this, it is easy to see if for some $b$ it holds, it will also hold for $\overline{b}$ (each digit is inverted).","['integration', 'trigonometry', 'combinatorics', 'products']"
3786686,Abel's summation formula and approximating an integral of Jacobi theta functions,"As my previous post remains unanswered, I thought I would post a more complete form of the problem in case it would be more practical to work on/ solve. I am trying to compute the following integral \begin{align}
I_{\pm\pm} (x,y) = \int_0^1 dz \int_0^\infty ds \, \frac{e^{-\alpha/s}}{s} \,\partial_s \left[ \theta_3 \left( \frac{\pi(x \pm z)}{2},e^{-s} \right) \,\theta_3 \left( \frac{\pi(y \pm z)}{2},e^{-s} \right) \right]
\end{align} where $x,y \in (0,1)$ and $\alpha > 0$ . To give a bit of context, I have come across this in a setting of heat propagation with Neumann boundary conditions and it does not seem to be something that can be calculated analytically. I have indeed looked at the numerical results for various values of $x$ and $y$ , but I still like to have an expression that approximates $I(x,y)$ (or various expression for different regimes). Any suggestions are therefore appreciated. My own (hopeless) attempt is based on approximating the $\theta_3$ functions first, for exmaple using Abel's summation formula following this post . Let's define \begin{align}
f (z,s) = \sum_{n=0}^\infty e^{-n^2 \pi^2 s} \cos (n \pi z)
\end{align} which in terms of Jacobi theta function could be represented as $
f (z,s) = ( \theta_3 ( \pi z/2 , e^{-\pi^2 s}) + 1 )/2.
$ To use Abel's formula, we can define $ \phi (h) = e^{-h^2 \pi^2 s} \cos(h \pi z) $ and write \begin{align}
\sum_{n=0}^\infty \phi(n) = \lim_{h \to \infty} \left( \lfloor h+1 \rfloor \phi(h) \right) - \int_0^\infty \lfloor h+1 \rfloor \phi'(h) \, dh
\end{align} The first term on the r.h.s vanishes because of the exponential factor. We have \begin{align}
\phi'(h) = -\pi e^{-h^2 \pi^2 s} \left( 2\pi h s \cos(h\pi z) + z \sin(h\pi z) \right)
\end{align} and $\lfloor h+1 \rfloor = h+1 -\{h+1\} = 1 + h - \{h\}$ . Moreover, we can use the integral results \begin{align}
\int_0^\infty dh \, 2 h \pi^2 s \, e^{-h^2 \pi^2 s} \, h \, \cos(h \pi z) &= 1 - \frac{z}{\sqrt{s}} D_+(\frac{z}{2\sqrt{s}}) \\
%
\int_0^\infty dh \,  2 h^2 \pi^2 s \, e^{-h^2 \pi^2 s} \, \cos(h \pi z) &=  \frac{ e^{-\frac{z^2}{4s}}}{\sqrt{4\pi s}} \left( 1 - \frac{z^2}{2 s} \right) \\
%
\int_0^\infty dh \, z \pi \, e^{-h^2 \pi^2 s}  \, \sin(h \pi z) &=  \frac{z}{\sqrt{s}} D_+(\frac{z}{2\sqrt{s}})\\
%
\int_0^\infty dh \,  z \pi h \, e^{-h^2 \pi^2 s} \, \sin(h \pi z) &=  \frac{ e^{-\frac{z^2}{4s}}}{\sqrt{4\pi s}} \, \frac{z^2}{2 s}
\end{align} where $D_+(z)$ is the Dawson function . Then we'll have (ps: possibly easier to get this just using integrations by part...) \begin{align}
\sum_{n=0}^\infty \phi(n) = 1 + \frac{e^{-\frac{z^2}{4s}}}{\sqrt{4\pi s}} + \int_0^\infty \{h\} \phi'(h) dh
\end{align} I have no idea how to estimate/find bounds of the remaining integral. In case that integral is something that could be neglected (I doubt), then we can just take the rest of the r.h.s and plug it into the expression for $I$ , compute the $s$ derivative and perform the integrations (which I think will give some error functions etc). Update We can work out the following bounds: as $ 0 \leq \{h\} <1 $ we have \begin{align}
 \int_0^\infty \{h\} \phi'(h) \, dh &\leq \int_0^\infty \{h\} |\phi'(h)| \, dh \\
& \leq \int_0^\infty |\phi'(h)| dh \\
&\leq \int_0^\infty \pi e^{-h^2 \pi^2 s}\sqrt{4\pi^2h^2s^2+z^2} dh\\
&= \frac{ z^2 e^{\frac{z^2}{8s}}}{8s} \left( K_0(\frac{z^2}{8s}) + K_1 (\frac{z^2}{8s} ) \right),
\end{align} where $K_\alpha$ is the modified Bessel function of the second kind . I am not sure if this bound is actually useful.","['theta-functions', 'approximation', 'sequences-and-series']"
3786696,variation of Vitali in $\mathbb{R}^2$,"Let $B(x, r)$ , be a two dimensional open disc with center $x \in \mathbb{R}^2$ and radius $r > 0$ . Consider a set $E \subset \mathbb{R}^2$ such that $E \subset \bigcup_{i = 1}^n B(x_i, r_i)$ and $\sum_{i = 1}^n r_i \leq 1$ Show that then there exists a collection of non-overlapping open discs $B(y_i, R_i)$ such that $E \subset \bigcup_{i = 1}^n B(y_i, R_i)$ and $\sum_{i = 1}^n R_i \leq 1$ Clearly if the balls $B(x_i, r_i)$ are non overlapping then we are done. If not then at least $2$ balls must overlap. First attempt Let $B(x_1,r_1)$ be the ball whose radius is the smallest such that $B(x_1,r_1)$ overlaps with at least one other ball. Pick $B(x_j,r_j)$ whose radius is maximal among all the balls overlapping with $B(x_1,r_1)$ , and note that the sums of the radius of the balls in $B(x_{j},r_{j} + r_1) \bigcup_{i = 2, i \neq j}^n B(x_i,r_i)$ is the same as the sum of the radius of the balls in $\bigcup_{i = 1}^n B(x_i, r_i)$ . The problem here is that $B(x_1,r_1)$ is not necessarily contained in $B(x_j,r_j + r_1)$ . Second attempt Let $B(x_1,r_1)$ be the ball whose radius is the smallest such that $B(x_1,r_1)$ overlaps with at least one other ball. Pick $B(x_j,r_j)$ whose radius is maximal among all the balls overlapping with $B(x_1,r_1)$ , and note that $B(x_1, r_1)$ is contained in $B(x_j, r_j + 2r_1)$ . The problem here is that the sums of the radius of the balls in $B(x_{j},r_{j} + 2r_1) \bigcup_{i = 2, i \neq j}^n B(x_i,r_i)$ is Not the same as the sum of the radius of the balls in $\bigcup_{i = 1}^n B(x_i, r_i)$ and therefore not necessarily less then or equal to $1$ . Since I want to cover $E$ and have radius that sum up to be less then or equal to $1$ , I know I want to maximize surface area while keeping the sum of the radi as small as possible so it dose feel logical to get rid of smaller circles while expanding upon larger once. Any help would be appreciated. Edit So after thinking about it a little longer I realized it might be impossible to create the disjoint collection of balls $B(y_i,R_i)$ such that each $y_i = x_i$ and I might have to move the discs. For example if $E$ was the union of $2$ balls of each radius $1/2$ , one centered at the origin and one centered at $(7/8,0)$ then there is no way to just expand one ball without moving it so that its radius remains $1$ but it covers the other ball. So now I am thinking start with letting $B(x_1,r_1)$ be the ball whose radius is the smallest such that $B(x_1,r_1)$ overlaps with at least one other ball. Pick $B(x_j,r_j)$ whose radius is maximal among all the balls overlapping with $B(x_1,r_1)$ , Consider the ball $B( ?,r_1 + r_j)$ so I need to find a suitable value for $?$ . I am temped to average the two centers $x_1, x_j$ but I know it should be closer to $x_j$ , but then that makes this really complicated.","['measure-theory', 'real-analysis']"
3786702,"Finding Bayes estimator for $\theta$ of Unif$(0,\theta)$","Finding Bayes estimator for $\theta$ of Unif $(0,\theta)$ Let $Y = \max{X_i}$ where $(X_1,\ldots,X_n)$ is a random sample from Unif $(0,\theta)$ . $Y$ is sufficient for $\theta$ . Find the Bayes estimator $w(Y)$ for $\theta$ based on $Y$ using the loss function $L(\theta,a) = \lvert a- \theta\rvert$ The prior density of $\theta$ is $\displaystyle \pi(\theta) = \frac{2}{\theta^3}1_{(1 < \theta < \infty)}$ I am pretty unfamiliar with Bayesian inference. From what I understand the posterior is given by $\displaystyle p(\theta \mid \underline{x}) = \frac{L(\theta \mid \underline x)\pi(\theta)}{\int L(\theta \mid \underline x)\pi(\theta) \, d\theta }\, ;  $ where $$
L(\theta \mid \underline{x})\pi(\theta) = \frac{1}{\theta^n}1_{(0 \le \min(x_i))}1_{(y \le \theta)}\frac{2}{\theta^3}1_{(1<\theta<\infty)}
$$ Aside from this I am not sure how I set this up to solve or where I use the loss function or how I base it off $Y$ .","['self-learning', 'statistics', 'statistical-inference', 'bayesian', 'order-statistics']"
3786722,Positive self dual Fourier eigenfuctions,"Some of the ""basic"" Fourier eigenfuctions encountered in real and complex analysis are $$ e^{-\pi x^2}, \quad \frac{1}{\cosh{\pi x}} $$ What are some other (smooth) functions fixed under the Fourier transform which have different decay rates than the above two functions? (1) For $f\in{\mathcal{S}(\mathbb{R})}$ , define $$\widehat{f}(\xi) = \int_{\mathbb{R}} f(x) e^{-2 \pi i x \xi} dx $$ Some background: It is possible to generate additional Fourier eigenfuctions by means of various iterative processes. A classical example are the Hermite functions , which may be given by $$h_k(x) = (-1)^k e^{x^2/2} \frac{d^k}{dx^k} \left( e^{-x^2} \right) $$ $h_0(\sqrt{2 \pi} x) = e^{-\pi x^2}$ and it turns out that $H_k(x) = h_k(\sqrt{2 \pi} x) $ are Fourier eigenfuctions with eigenvalue $(-i)^k$ . So, if $\widehat{f} = f$ , then it is possible to write $$ f \sim \sum_m a_{4m}H_{4m} $$ It appears difficult to find the/any correct combination of Hermite functions which create a +1 function satisfying (1). Alternatively, if we set $ \gamma(x) $ equal to $e^{-\pi x^2} $ or $ \mathrm{sech}\pi x$ from above, then for $a > 1$ , the functions $\gamma(x)$ , $a\gamma(ax) + \gamma(x/a) $ , and $a\gamma(ax) + \gamma(x/a) - (1+a)\gamma(x) $ are +1 eigenfunctions due directly to the dilation property $ f(\delta x) \longrightarrow 1/ \delta \widehat{f}(\xi / \delta) $ . The following plot shows these three functions with $ \gamma(x) = \mathrm{sech} \pi x $ and $a=1.5$ . Although $a \gamma(ax) + \gamma(x/a)$ is another eigenfunction, it's not fundamentally different from $\gamma(x)$ . One ""problem"" (at least in my case) with $ a\gamma(ax) + \gamma(x/a) - (1+a)\gamma(x)$ (in green) is that it oscillates. The higher order Hermite functions also oscillate before decaying rapidly. $\mathrm{sech} \pi x$ decays like $e^{-|x|}$ while the Hermite functions, which take the form $e^{-x^2/2} P(x)$ (P a ploynomial) decay like $e^{-|x|^2}$ . I'm wondering if there are other +1 (self-dual) functions which decay at different rates . In a sense, I'm trying to reverse an iteration or generating relation, and one possible avenue may be to consider functions of the form $$ \int_1^\infty (a\gamma(ax) + \gamma(x/a) - (1+a)\gamma(x)) d \sigma (a) $$ where $\sigma$ is a (positive?) measure on $(1,\infty)$ . However, it appears that pinning down any particular such measures which make the integral converge is difficult.","['complex-analysis', 'harmonic-analysis', 'real-analysis']"
3786738,Intersection of $(n-1)$-rectifiable set with hyperplane,"Suppose $E$ is a $(n-1)$ -rectifiable set in $\mathbb{R}^n$ , which means $E$ is a set that has Hausdorff dimension $n-1$ and $H^{n-1} (E) < \infty$ and there is a countable family of Lipschitz maps $f_i: \mathbb{R}^{n-1} \rightarrow \mathbb{R}^n$ such that the union of their image covers $H^{n-1} $ - almost all $E$ . Given a hyper-hyperplane $H_0$ and consider the collection of planes $C = \{H_0+v: v \bot H_0 \}$ . Is for $L^{n-2}$ - a.e. hyperplanes $H \in C$ , $H\cap E$ necessarily a rectifiable curve? I think this intuitionally makes sense to me but I'm not sure if it is a valid statement.","['measure-theory', 'lipschitz-functions', 'geometric-measure-theory', 'riemannian-geometry']"
3786751,INTUITION: Orthonormal columns implies orthonormal rows [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 3 years ago . Improve this question I was asked by a student and was stumped upon the very intuition of this statement. I searched on this site and found many proofs but little intuition. For example: Column Vectors orthogonal implies Row Vectors also orthogonal? and Intuition behind row vectors of orthonormal matrix being an orthonormal basis I am open to abstract linear algebra ideas but I don't think they help bring in much intuition. I am hoping for either a geometric intuition , a quick algebraic manipulation on vector components, or an intuitive explanation of the key step in the proof: $A^TA = AA^T = I$ . Edit: many comments went for the last option of the three. However, it was probably the hardest to gain any intuition with this option. I would personally prefer answers exploring the first two options, or something really really special about this last option.","['matrices', 'linear-algebra', 'vector-spaces', 'intuition']"
3786754,Connection Between Bézout's Identity and Linear Algebra,"Today I looked at Bézout's Identity and I became reminded a bit of linear algebra. Let me explain. Bézout's Identity says that for coprime integers $a, b$ , there exists integers $x,y$ such that $ax+by=1$ . So in essence $a$ and $b$ can generate any integer $n$ through the linear combination $a(nx)+b(ny)=n$ . I thought this reminiscent of the notion of linear independence and a spanning set, in that coprime integers can be thought of as independent, and two coprime integers $\{a,b\}$ can be said to span the integers. It seems to me that this bit of number theory generalizes to some algebraic structure. Is my intuition off? My knowledge of abstract algebra is to the extent of elementary group theory.","['number-theory', 'abstract-algebra', 'linear-algebra']"
3786757,How can I prove that $p(x)=x^4+x+1$ doesn't have real roots?,"Let $p(x)=x^4+x+1$ be a polynomial in $\mathbb{R}[x]$ . How can I prove that $p$ doesn't have real roots? My attempt: From calculus, I know that $$\lim_{x \to \pm\infty} p(x) = \infty\,.$$ Then, if it has real roots, then we have two or four real roots. I know that if $$\gcd \left( p(x), p'(x) \right) = 1\,,$$ then the roots is simple. Is there another hint?","['roots', 'calculus', 'polynomials', 'algebra-precalculus', 'quartics']"
3786770,$\bigcup^n_{i=1} \mathcal{P} (E_i) \subseteq \mathcal{P} (\bigcup^n_{i=1}E_i)$,"Given some sets $E_1,...,E_n$ , prove that $$\bigcup^n_{i=1} \mathcal{P} (E_i) \subseteq \mathcal{P} \left(\bigcup^n_{i=1}E_i\right)$$ I don't know if my proof is valid: Given an arbitrary $x \in \bigcup^n_{i=1} \mathcal{P} (E_i) \Rightarrow \exists E_j :x \subseteq E_j \Rightarrow x \subseteq \bigcup^n_{i=1}E_i \Rightarrow x \in \mathcal{P}(\bigcup_{i=1}^nE_i)$","['elementary-set-theory', 'solution-verification']"
3786857,Minimal polynomial of $\alpha + \beta$ over $\mathbb{Q}$,"Let $\alpha,\beta \in \mathbb{C}$ such that $\alpha^3+\alpha+1=0$ and $\beta^2+\beta-3=0$ . Find the minimal polynomial of $\alpha+\beta$ over $\mathbb{Q}$ . I was trying the usual trick for this kind of problems, that is Let $\gamma = \alpha+\beta$ , and therefore \begin{align*}
(\gamma - \beta)^3 &= \alpha^3 = -\alpha - 1 \\
\gamma^3 - 3\gamma^2\beta+3\gamma\beta^2-\beta^3 &= -\alpha - 1
\end{align*} and using $\beta^2+\beta-3=0$ , i get $\gamma^3 - 3\gamma^2\beta-3\gamma\beta + 9\gamma+3-4\beta = -\alpha - 1$ . I think the minimal polynomial of $\alpha+\beta$ has degree $6$ , then i was trying raise to the $2^{nd}$ power the last equation, but i don't get nothing to obtain the minimal polynomial of $\alpha + \beta$ .","['irreducible-polynomials', 'ring-theory', 'minimal-polynomials', 'abstract-algebra']"
3786898,Is every extreme probability on a countably generated space a point mass?,"An extreme probability $P$ on the measurable space $(\Omega, \mathcal A)$ is such that $P(A) \in \{0,1\}$ for every $A \in \mathcal A$ . $P$ is a point mass if there is some $\omega \in \Omega$ such that $P(A) = 1_A(\omega)$ for every $A \in \mathcal A$ . Clearly every point mass is extreme. Earlier today I was wondering whether every extreme probability is a point mass. I think I convinced myself that this is not the case by considering $\mathcal A =$ the countable/co-countable sigma-algebra over some uncountable set $\Omega$ . Then, $P(A)=1$ for every co-countable $A$ defines an extreme probability that cannot be a point mass. I am now wondering if the result holds when $\mathcal A$ is countably generated. I think the answer is yes, but would appreciate verification of this proof. Let $\mathcal A$ be countably generated by $A_1,A_2,...$ . Consider sets of the form $\bigcap_{i=1}^\infty B_i$ , where each $B_i =$ either $A_i$ or $A_i^c$ . These sets are disjoint, in $\mathcal A$ , and they are atoms of $\mathcal A$ in the sense that $\mathcal A$ does not contain any strict subsets of these sets. If $P$ is extreme, then it assigns exactly one of these atoms, call it $A^*$ , probability $1$ . In particular, $A^*$ is that atom where $B_i = A_i$ iff $P(A_i)=1$ . Then, for any $\omega \in A^*$ , $P(A) = 1_A(\omega)$ for all $A \in \mathcal A$ .","['measure-theory', 'solution-verification', 'probability-theory']"
3786905,"Let $A, B$ be skew-symmetric matrices such that $AB = -BA$. Show that $AB = 0$","Let $A, B$ be skew-symmetric matrices such that $AB = -BA$ . Show that $AB = 0$ . I know that $AB$ is skew-symmetric,because $$(AB)^t=B^tA^t=BA=-AB$$ but I don't know how show that $AB=0$ .","['matrices', 'skew-symmetric-matrices', 'linear-algebra']"
3786978,$\int_ {0}^{\infty} \frac{(e^{3x}-e^x)dx}{x(e^x+1)(e^{3x}+1)}$ [duplicate],"This question already has answers here : Finding $\int_{0}^{\infty} \frac{e^{3x}-e^x}{x(e^{3x}+1)(e^x+1)} dx$ (2 answers) Evaluate the integral: $\int_{0}^{\infty}\frac{\mathrm{e}^{3x}-\mathrm{e}^{x}}{x\left(\mathrm{e}^{3x}+1\right)\left(\mathrm{e}^{x}+1\right)} \, dx$ [duplicate] (2 answers) Closed 3 years ago . $$\int_ {0}^{\infty} \frac{(e^{3x}-e^x) \ \mathrm dx}{x(e^x+1)(e^{3x}+1)}$$ I tried converting it to $$\int_ {0}^{\infty} \frac{\big((e^{3x}+1)-(e^x+1)\big) \ \mathrm dx}{x(e^x+1)(e^{3x}+1)}$$ integral-calculator.com says no antiderivative found. I would like to see how it is solved by Feynman's Trick.","['integration', 'calculus', 'improper-integrals']"
3786988,Solving equation $\tan(5π\cos\alpha) = \cot(5π\sin\alpha)$,"$$\tan(5π\cos \alpha) = \cot(5π\sin 
\alpha)$$ I did that $\tan(5π\cos\alpha) = \tan\left[\frac π2-5π\sin\alpha\right]$ And then used the solution of Trigonometric Equation $\tan(\theta)=\tan(\beta)$ Which is $\theta = nπ + \beta$ , $n$ is an integer. But the basic condition of using the above result is that $\beta$ lies between $\left(-\frac π2,\frac π2\right)$ And so gives $\sin \alpha $ lies between $\left(0,\frac 15\right)$ What is wrong with this? PS correct answer comes by using my method..",['trigonometry']
3787057,"What is the minimum number of integers chosen from $S = \{1,2,3,4,5,6,7,8,9\}$ so that there are always three of them whose sum is $15$?","Edit: Comments showed me the answer is 7. How can I prove that using pigeonhole? I know this is a pigeonhole principle question and that $6$ integers is enough to guarantee the condition, but I'm having trouble figuring out the appropriate pigeons/pigeonholes. Here are a few thoughts, though I don't know how useful they are: There are $8$ possible triplets from $\{1,2,...,9\}$ that sum to $15$ : $\{1,5,9\},
   \{1,6,8\}, \{2,4,9\}, \{2,5,8\}, \{2,6,7\}, \{3,4,8\}, \{3,5,7\}, \{4,5,6\}$ . From $6$ integers there are $20$ possible triplets, and from $S$ the minimum triplet sum is $1+2+3=6$ , and the maximum is $7+8+9=24$ , giving us a range of $19$ possible sums. In order for a triplet to sum to $15$ , the sum of its smallest integers
must be at least $6$ , and the sum of its largest integers must be at
most $14$ . I think I'm missing some crucial observation.","['pigeonhole-principle', 'discrete-mathematics']"
3787059,Evaluating $\cos24^\circ-\cos84^\circ-\cos12^\circ+\sin42^\circ$,"How to evaluate: $$\cos24^\circ-\cos84^\circ-\cos12^\circ+\sin42^\circ$$ Can somebody help me handle it?
I have no idea what to do. This is my attempt: $$\cos24^\circ-\cos(60^\circ+24^\circ)-\cos12^\circ+\sin (12^\circ+30^\circ)$$",['trigonometry']
3787080,A binary relation $\mathcal R$ over a set $A$ is transitive if and only if $\mathcal R$ is equal to its transitive closure $\mathcal R^{+}$.,"Given a binary relation $\mathcal R$ over a set $A$ ,then the $\mathsf {Transitive \;Closure}$ of $\mathcal R$ over $A$ is the smallest transitive relation on $A$ containing $\mathcal R$ , it's indeed the intersection of all transitive relations over $A$ that are a superset of $\mathcal R$ . The transitive closure of $\mathcal R$ is denoted by $\mathcal R^{+}$ and has the following explicit formula: $$\mathcal R^{+}=\bigcup_{n=1}^{\infty} \mathcal R^n$$ Where $\mathcal R^1=\mathcal R$ , and $\mathcal R^{n+1}=\mathcal R^n ∘ \mathcal R^1\tag{$n \in \mathbb N^+$}$ Theorem: A binary relation $\mathcal R$ over a set $A$ is transitive if and only if $\mathcal R$ is equal to its transitive closure $\mathcal R^{+}$ . $\Longrightarrow$ Assume $\mathcal R$ is transitive,then by the definition of transitive closure $\mathcal R \subseteq \mathcal R^{+}$ ,it's left to show that $\mathcal R^+ \subseteq \mathcal R$ .For the sake of contradiction assume exists $a,b$ in $A$ such that $(a,b) \in \mathcal R^+ \setminus \mathcal R$ ,then there are two cases two consider: $\exists \;c \in A:(b,c) \in \mathcal R$ Such $c$ for which $(b,c) \in \mathcal R$ does not exist. If the first case happens then since $\mathcal R \subseteq \mathcal R^{+}$ and $\mathcal R^{+}$ is transitive follows $(a,c) \in  \mathcal R^{+}$ ,if such ordered pair does exist,then $\mathcal R^+=\mathcal R \cup \mathcal F$ ,where $\mathcal F$ is a set containing $(a,b),(a,c)$ .Define a transitive relation $\mathcal R^{'}:=\mathcal R$ (the transitivity of $\mathcal R^{'}$ follows from the transitivity of $\mathcal R$ ),clearly $\mathcal R^+ \not \subseteq \mathcal R^{'}$ .Contradicts the definition of transitive closure as the smallest. If the ordered pair $(a,c)$ does not exist in $\mathcal R^+$ ,then it contradicts the transitivity of the transitive closure. If the second case happens then $\mathcal R^+=\mathcal R \cup \mathcal F'$ ,where $\mathcal F'$ is a set containing $(a,b)$ .Define a transitive relation $\mathcal R^{'}:=\mathcal R$ (the transitivity of $\mathcal R^{'}$ follows from the transitivity of $\mathcal R$ ),clearly $R^+ \not \subseteq \mathcal R^{'}$ .Contradicts the definition of transitive closure as the smallest. Implies $\mathcal R^+ \subseteq \mathcal R$ , also since $\mathcal R \subseteq \mathcal R^+$ follows $$\mathcal R^+=\mathcal R$$ $\Longleftarrow$ If $\mathcal R=\mathcal R^+$ ,then the definition of transitive closure implies $\mathcal R^+$ is transitive and from the equality we conclude that $\mathcal R$ is also transitive. $\;\blacksquare$ The theorem is indeed based on my conjecture and I could not find any kind of proof about the theorem.It would be appreciated if someone check the proof.","['elementary-set-theory', 'solution-verification', 'relations', 'function-and-relation-composition']"
3787081,How to imagine/prove that all of the following pictures are 2-torus?,"How to imagine/prove that all of the following pictures are a portion of a torus? The obvious way is  counting number of genus, but it isn't easy for me to count it. any suggestion? (image source: MO-Renato G. Bettiol and Cassidy Curtis website )","['general-topology', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
3787095,"Limit of ${ \lim_{(x,y)\to(0,0)} {(\left| x \right| + \left| y \right|) \ln{(x^2 + y^4)} }}$","This question comes from Hubbard's textbook ""Vector Calculus, Linear Algebra, and Differential Forms: A Unified Approach"" (5th edition, exercise 1.5.16b) I wrote a $\epsilon -\delta$ proof to show ${ \lim_{(x,y)\to(0,0)} {(\left| x \right| + \left| y \right|) \ln{(x^2 + y^4)} }} = 0$ , but wanted to ask to check if what I wrote was correct: Suppose we have $\epsilon > 0$ . First, it can be shown (with L'Hoptial for example) that $\lim_{x\to 0} |x| \ln(x^k) = 0$ for even positive integers $k$ . Thus there exists $\delta_{1;k} > 0$ such that $|x| < \delta_{1;k}$ implies $\left| |x| \ln(x^k) \right| = \left| k |x| \ln(x) \right| < \frac{k\epsilon}{6}$ . Therefore: $|x| \ln(x) > - \frac{\epsilon}{6}$ . Secondly, clearly we have $\lim_{x \to 0} {|x| \ln(x^2 + c)} = \lim_{y \to 0} {|y| \ln(y^4 + c)} = 0$ for any $c > 0$ . So there exists $\delta_{2; c}, \delta_{3;c} > 0$ such that: $|x| < \delta_{2;c} \implies \left| |x| \ln(x^2 + c) \right| < \frac{\epsilon}{2}$ . $|y| < \delta_{3;c} \implies \left| |y| \ln(y^4 + c) \right| < \frac{\epsilon}{2}$ . Let $\delta = \min(\epsilon, \delta_{1;2}, \delta_{1;4}, \delta_{2;\epsilon}, \delta_{3;\epsilon})$ . So if $\sqrt{x^2 + y^2} < \delta$ , note that this implies $|x| < \delta$ and $|y| < \delta$ . Then: $$\begin{align} (|x| + |y|) \ln(x^2 + y^4) &= |x| \ln(x^2 + y^4) + |y| \ln(x^2 + y^4) \\ &\leq |x| \ln(x^2 + \epsilon^4) + |y| \ln(\epsilon^2 + y^4) \\ &< \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon \end{align}$$ As $x^2 + y^4 < \epsilon^2 + y^4$ (as $|x| < \delta \leq \epsilon$ ) and $\ln(x)$ is a strictly increasing implies $\ln(x^2 + y^4) < \ln(\epsilon^2 + y^4)$ . Since $|y| \geq 0$ then $|y| \ln(x^2 + y^4) \leq |y| \ln(\epsilon^2 + y^4)$ . Moreover: $$\begin{align} (|x| + |y|) \ln(x^2 + y^4) &= |x| \ln(x^2 + y^4) + |y| \ln(x^2 + y^4)
\\ &\geq |x| \ln(x^2) + |y| \ln(y^4) \\ &= 2|x|\ln(x) + 4|y| \ln(y) \\ &> -\frac{2\epsilon}{6} - \frac{4\epsilon}{6} = -\epsilon \end{align}$$ Putting these together we have $\left| (|x| + |y|) \ln(x^2 + y^4) \right| < \epsilon$ . Therefore ${ \lim_{(x,y)\to(0,0)} {(\left| x \right| + \left| y \right|) \ln{(x^2 + y^4)} }} = 0$ .","['limits', 'multivariable-calculus', 'solution-verification']"
3787117,"If $T_t$ is a diffeomorphism, show $\frac\partial{\partial t}{\rm D}T_t(x)={\rm D}\left(\frac\partial{\partial t}T_t\right)(x)$","Let $d\in\mathbb N$ , $U\subseteq\mathbb R^d$ be open, $\tau>0$ and $T_t$ be a $C^1$ -diffeomorphism from $U$ onto $U$ for $t\in[0,\tau)$ with $T_0=\operatorname{id}_U$ . Which assumption do we need to impose in order to conclude that $$[0,\tau)\ni t\mapsto{\rm D}T_t(x)\tag1$$ is $C^1$ -differentiable at $0$ and $$\left.\frac\partial{\partial t}{\rm D}T_t(x)\right|_{t=0}={\rm D}\left(\left.\frac\partial{\partial t}T_t\right|_{t=0}\right)(x)\tag2$$ for all $x\in U$ ? It clearly holds if $$[0,\tau)\times U\ni(t,x)\mapsto T_t(x)\tag3$$ is $C^2$ -differentiable at $(0,x)$ for all $x\in U$ by Schwarz' theorem .","['frechet-derivative', 'derivatives', 'real-analysis']"
3787127,"What is the intuition behind the definition of the action of a group on a set? Why do we call a homomorphism $G\to\mathcal S(X)$ a ""representation""?","Let $G$ be a group acting on a set $X$ .
For all $ g\in G $ , we consider the application \begin{align*} \varphi_{g}: &~ X \longrightarrow X \\ &~ x \longrightarrow g.x
\end{align*} It is clear that $\varphi_{gh}=\varphi_g\circ\varphi_h$ , $\forall~ g,h \in G,$ $\varphi_e=\text{Id}_X$ ( $e$ the neutral element of $G$ ), and $\varphi_g \circ \varphi_{g^{-1}}=\varphi_{g^{-1}} \circ \varphi_g $ , so $\varphi_g$ is bijective, for all $g\in G$ . i.e: $\varphi \in \mathcal S(X)$ , $\forall~ g\in G$ , and the application: \begin{align*}\Phi :&~ G \longrightarrow  \mathcal S(X) \\ &~g\longrightarrow \varphi_g\end{align*} is a group homomorphism that we call a representation of $G$ in $\mathcal S(X).$ I didn't understand what this definition is for, I'm looking for the intuition or the idea behind it.
Why do we call the application $\Phi$ by this name: ""representation"", it is only a homomorphism?
if anyone has any ideas or comments that he can add, I will be very grateful.","['group-theory', 'group-actions']"
3787135,Can octonions be represented by infinite matrices?,"It is sometimes possible to multiply matrices of countably-infinite dimension. (Matrix multiplication is defined in the usual way, with rows and columns multiplied termwise and summed.) However, it turns out the associative property fails in general for infinite matrices, due to conditional convergence of infinite series. Meanwhile, the octonions $\mathbb{O}$ are a unital nonassociative $8$ -dimensional algebra that cannot be represented by $n\times n$ matrices (else they would associate). So it seems natural to ask, is is possible to represent $\mathbb{O}$ by infinite matrices? I suppose one plan would be to take a finite-dimensional representation of the quaternions $\mathbb{H}$ , ""copy and paste"" it into infinite matrices, and then find an infinite matrix for $\ell\in\mathbb{O}$ that squares to $-I$ and satisfies the rules of the Cayley-Dickson construction, but I don't see a way to do this. (I suppose one could also generalize this question to arbitrary nonassociative algebras.)","['representation-theory', 'octonions', 'matrices', 'abstract-algebra', 'nonassociative-algebras']"
3787167,Linear operators well-defined by matrices in $l^2$ are bounded,"Let $\{a_{jk}\}$ be  an infinite matrix such that corresponding mapping $$A:(x_i) \mapsto (\sum_{j=1}^\infty a_{ij}x_j)$$ is well defined linear operator $A:l^2\to l^2$ .
I need  help with showing that this operator will be bounded. I guess it means that i need to check if a unit sphere maps to something bounded, so i need to manage to get some inequality on coefficients of matrix that will allow to write a straight sequence of inequalities and get desired bound. But I don't understand how to get bound from operator being well defined.","['matrices', 'functional-analysis']"
3787188,Show that $\mu (X)< \infty$,"Let $(X,A,\mu)$ be a measure space and $f:X\rightarrow ]0, \infty[$ summable such that $\frac{1}{f}$ is also summable. Show that $\mu (X)< \infty$ What I did was applying cauchy-schwarz inequality on $f$ and $\frac{1}{f}$ , $\mu (X)^2=\left ( \int_{X}^{}\left | f \right |\left | \frac{1}{f} \right |d\mu \right )^2\leq \left ( \int_{X}^{}|f|^2d\mu \right )\left ( \int_{X}^{} \frac{1}{|f|^2}d\mu\right )$ How can I continue ? I was thinking of considering cases for $f$ when it is bounded and when not, but I don't know if it's useful","['measure-theory', 'cauchy-schwarz-inequality']"
3787209,Show that $\sum\frac{(-1)^{n+1}} {{n}^r} \sum\frac{(-1)^{n+1}} {{n}^s} $ by Abel's rule forms a series that doesn't converge when r+s=1.,"It is a similar problem to that in Show that the series $\frac{1} {\sqrt{1}} -\frac{1} {\sqrt{2}} +\frac{1} {\sqrt{3}} +\dots$ converges, and its square (formed by Abel's rule) doesn't. . It may provide some hint to the latter. Show that $\frac{1} {{1}^r} -\frac{1} {{2}^r} +\frac{1} {{3}^r} +\dots$ and $\frac{1} {{1}^s} -\frac{1} {{2}^s} +\frac{1} {{3}^s} +\dots$ , where 0 < r < 1, being multiplied by Abel's rule, forms a series (say $\sum \nu_n$ ) that doesn't converge, when r+s=1. Abel's rule: given $\sum a_n, \sum b_n$ , $\sum_{n=0} ^\infty c_n=\sum_{n=0} ^\infty [\sum_{i=0} ^n a_{n-i}b_i]$ is the infinite series gotten from multiplication of two series. Initial steps are similar to those in the post, $(\frac{1} {{1}^r} -\frac{1} {{2}^r} +\frac{1} {{3}^r} +\dots)(\frac{1} {{1}^s} -\frac{1} {{2}^s} +\frac{1} {{3}^s} +\dots)\\
=\frac{1} {{1}^r}\frac{1} {{1}^s}+\dots
+[(-\frac{1} {{1}^r} \frac{1} {{(2k)}^s}+\frac{1} {{1}^r} \frac{1} {{(2k+1)}^s}
-\frac{1} {{2}^r} \frac{1} {{(2k-1)}^s}+\frac{1} {{2}^r} \frac{1} {{(2k)}^s}+\dots
-\frac{1} {{k}^r}\frac{1} {{(k+1)}^s}+\frac{1} {{k}^r}\frac{1} {{(k+2)}^s}
-\frac{1} {{(k+1)}^r}\frac{1} {{k}^s}+\frac{1} {{(k+2)}^r}\frac{1} {{k}^s}
\dots-\frac{1} {{(2k)}^r}\frac{1} {{1}^s}+ \frac{1} {{(2k+1)}^r}\frac{1} {{1}^s})
 +\frac{1} {{(k+1)}^{r+s}}]+\dots,$ where $\sum_{m=1}^{2k}|(-\frac{1} {m^r} \frac{1} {(2k+1-m)^s}+\frac{1} {m^r} \frac{1} {(2k+2-m)^s})|
=\sum_{m=1}^{2k}\frac{1} {m^r} \frac{1} {(2k+1-m)^s}(1-\frac{1} {(1+\frac{1}{2k+1-m})^s})\\
=\sum_{m=1}^{2k}\frac{1} {m^r} \frac{1} {(2k+1-m)^s}(s\frac{1}{2k+1-m}+O(\frac{1}{(2k+1-m)^2}))
=\sum_{m=1}^{2k}\frac{1} {m^r} \frac{s} {(2k+1-m)^{s+1}},$ for $1-(1+x)^{-s}=-\frac{(-s)}{1!}x-\frac{(-s)(-s-1)}{2!}x^2+\dots.$ We can not easily use $\frac{1}{\sqrt{ab}}>\frac{1}{a+b}$ here, instead we use Taylor expansion. It seems the above sum approximates $\sum_{m=1}^{2k}\frac{1} {k^r} \frac{1} {(k)^{s+1}}\approx \frac{k}{k^{r+s+1}}=\frac{1}{k},$ and so the series (say $\sum \psi_n$ ) it forms diverges as well. But here we are gonna show that the serives diverges more than $\sum \frac{1}{k+1}$ , which we can't yet show in the above post. Let s go near 1-0 (i.e. r+s-0), then $\sum \psi_n$ goes near $\sum_{m=1}^{2k}\frac{1} {m^0} \frac{r+s} {(2k+1-m)^{r+s+1}}
=\sum_{m=1}^{2k}\frac{1} {(2k+1-m)^{2}}=\frac{1}{(2k)^2}+\frac{1}{(2k-1)^2}+\dots+\frac{1}{1^2}>\frac{2}{k+1}$ ( A note for myself: for calculation of this sum and the sum in the above post, see Formula for $\frac{1}{(n)^2}+\frac{1}{(n-1)^2}+\dots+\frac{1}{1^2}$. . According to results there, the left side tends to $\frac{\pi^2}{6}$ which is obviously larger than the right side that tends to 0. So $\sum \nu_n\approx \sum_{k=0}^\infty \frac{\pi^2}{6}$ , it doesn't oscillates between two values but oscillates to infinity.) when k $\geq$ 3 (i.e. $\frac{2}{k+1}\leq \frac{1}{2^2}$ ). Therefore when s is near 0, $|\sum \nu_n|>\sum\frac{2}{k+1}-\sum\frac{1}{k+1}$ , which diverges. My question is how to, in general, prove that it the series $\sum \nu_n$ diverges?","['complex-analysis', 'absolute-convergence']"
