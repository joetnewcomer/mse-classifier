question_id,title,body,tags
110760,"$\wedge,\cap$ and $\vee,\cup$ between Logic and Set Theory always interchangeable?","In "" $\wedge,\cap,\times$ and $\vee,\cup,+$ are always interchangeable? "" 
It has been shown that arithmetic shouldn't be included. So the new modified question is: The analogy of $\wedge,\cap$ and $\vee,\cup$ are seem to be in Logic and Set Theory are seem to be so similar that one might might be forgiven to think they are the same thing but with different notation. My question is are $\wedge,\cap$ the same thing and just different symbols are being used depending on the framework? same question regarding $\vee,\cup$ What about infinite cases? does this type of intuition break down between Logic and Set theory?","['logic', 'intuition', 'elementary-set-theory', 'soft-question']"
110797,How do I show that this metric space is not convex?,"Denote $X$, the space of all sequences $\in$ $\mathbb R$. I have a metric $$d(x,y):=\sum_{n=1}^\infty 2^{-n}\frac{| x_n-y_n|}{1+| x_n-y_n|}$$ and $(X,d)$ is a metric space. How would I show that the closed (metric) ball $B[0,\frac{1}{3}]$ with centre $0$ and radius $1/3$ is not convex? I tried working from the definition that a set $A$ in a vector space $X$ is convex if for all $x,y \in A$, and all $t$ in the interval $[0,1]$, $(1 − t ) x + t y \in A,$ but I cant seem to show its not convex. I was looking up on this, and I read somewhere it might have something to do with summable sequences like $x=(1/2,1/2,1/2,...)?$ Im not sure though, I might be wrong.","['metric-spaces', 'analysis']"
110804,A contest problem about multiple roots,"if a is real, what is the only real number that could be a mutiple root of $x^3 +ax+1$=0 No one in my class know how to do it, so i have to ask it here.",['algebra-precalculus']
110809,"If $a_M$ is not locally nilpotent, why does there exist $x \in M$ and a prime ideal $\mathfrak{p}$ such that $(Ax)_\mathfrak{p} \neq 0$?","Let $A$ be a commutative ring. I have a short question about the small result (Proposition 2.5 of Lang's Algebra , pg. 418, third edition) that if $M$ is an $A$ module, and $a\in A$ , then $a_M$ defined by $x\mapsto ax$ for $x\in M$ , (i.e. the left-multiplication map) is locally nilpotent if and only if $a$ lies in every prime ideal $\mathfrak{p}$ such that $M_\mathfrak{p}\neq 0$ . Here $M_\mathfrak{p}$ is the localization of $M$ by $A\setminus \mathfrak{p}$ . In the converse statement, if $a_M$ is not locally nilpotent, then there is $x\in M$ such that $a^nx\neq 0$ for all $n\geq 0$ . Let $S=\{1,a,a^2,\dots\}$ be a multiplicative set, so I know there exists a prime $\mathfrak{p}$ maximal among ideals not intersecting $S$ . Why does it follow that $(Ax)_\mathfrak{p}\neq 0$ , so that $M_\mathfrak{p}\neq 0$ ? Lang states this in one line, so I feel it must be obvious.","['nilpotence', 'maximal-and-prime-ideals', 'abstract-algebra', 'modules', 'commutative-algebra']"
110812,"Show a $\sigma$-algebra contains the Borel sets : with $(a,\infty)$ or $(-\infty,b)$?","For a certain $\sigma$-algebra $A$ on the real line, I would like to show that it contains the Borel sets. I can show that $A$ contains the left and right half-line $(a,\infty)$ and $(-\infty,b)$ for any real numbers $a$ and $b$. My question is : can I infer that $A$ contains the Borel sets by only prooving that it contains the left half-line or is it mandatory to show that $A$ contains both half-line?
I'm not clear on how the Borel sets are generated from half-line and open intervals.","['measure-theory', 'real-analysis', 'analysis']"
110845,Maximal submodule in a finitely generated module over a ring,"Let $R$ be a unital ring, $M$ is a finitely generated $R$-module. My question is to prove that there exist a maximal submodule in $M$. However I have no strategy to prove that except using the idea of Zorn lemma. Can any body help me to solve this problem? Also, please give a counter example for the case that if $M$ is not finitely generated. Thank for reading. I beg your pardon for my poor English","['modules', 'ring-theory', 'abstract-algebra']"
110856,"unit circle, derive number for any degree, cosinus and sinus","$\sin(90°)= \sin(\frac{1}{2}\pi)= 0$ $\cos(90°)= \cos(\frac{1}{2}\pi)= 1$ $\sin(60°)= \sin(\frac{1}{3}\pi)=\frac{\sqrt{3}}{2}$ $\cos(60°)= \cos(\frac{1}{3}\pi)=\frac{1}{2} $ $\sin(45°)= \sin(\frac{1}{4}\pi)=\frac{\sqrt{2}}{2}$ $\cos(45°)= \cos(\frac{1}{4}\pi)=\frac{\sqrt{2}}{2}$ $\sin(30°)= \sin(\frac{1}{6}\pi)=\frac{1}{2}$ $\cos(30°)= \cos(\frac{1}{6}\pi)=\frac{\sqrt{3}}{2}$ An heres my question (just for the purpose of curiosity):
What number (not with decimals, i want numbers like for those above, square roots and fractions allowed) would $x$ and $y$ be: $\sin(1°)= \sin(\frac{1}{180}\pi)=\ x$ $\cos(1°)= \cos(\frac{1}{180}\pi)=\ y$ And how would I generally derive ANY degree, lets say $\sin(3°)$ or wathever.",['trigonometry']
110858,The area problem!,"We have to find area of the quadrilateral formed by joining the point of intersection of the four quarter circles that are drawn from each vertex in a unit square. $\hspace{4cm}$ The challenge is only to use planer geometry (not even coordinate or calculus), I was wondering how could we do this? PS: This is actually an extension of this problem.","['geometry', 'self-learning', 'recreational-mathematics', 'intuition']"
110867,Why is the real function $x^{1/3}$ continuous?,I want to prove that the function $f(x) := x^3$ for all real $x$ defines a homeomorphism from $\mathbb{R}$ to $\mathbb{R}$. But I am finding it difficult to prove that the inverse map is continuous!,"['general-topology', 'real-analysis']"
110868,"How can I solve $8n^2 = 64n\,\log_2(n)$","I currently try to analyze the runtime behaviour of several algorithms.
However, I want to know for which integral values $n$ the first algorithm is better ($f(n)$ is smaller) and for which the second one is better. The two algorithms: $$f(n) = 8n^2$$ $$g(n) = 64n\,\log_2(n)$$ I started by equaling the two to $8n^2 = 64n\,\log_2(n)$. The problem is that $n$ is both outside and inside a $\log_2$, and I don't know how to resolve this.","['logarithms', 'integration', 'algorithms']"
110869,Combining Taylor expansions,"How do you taylor expand the function $F(x)={x\over \ln(x+1)}$ using standard results? (I know that WA offers the answer, but I want to know how to get it myself.) I know that $\ln(x+1)=x-{x^2\over 2}+{x^3\over 3}+…$ But I don't know how to take the reciprocal. In general, given a function $g(x)$ with a known Taylor series, how might I find $(g(x))^n$, for some $n\in \mathbb Q$? Also, how might I evaluate expressions like $\ln(1+g(x))$ where I know the Taylor expansion of $g(x)$ (and $\ln x$). How do I combine them? Thank you.","['taylor-expansion', 'analysis']"
110876,What is an intuitive meaning of $E(\overline { X } )$ and $Var(\overline { X } )$?,"Let $X$ be a random variable distributed over, for example say, the Binomial Distribution. Then $P(X)$ is the probability of getting $x$ successful trials in $n$ total trials. So I saw a notation that represents the mean of random variables that made me I feel sceptical about my understanding of all the notations I have known. So here's my understanding of the notations: When it says the expectation of $X$, $E(X)$, does it mean over a long
  run, $E(X)$ is the likely number of successful trials? In other words,
  the expected value of $X$ is the expected number of successful trials
  we would expect in a long run? When it says the variance of $X$, $Var(X)$, does it mean how spread
  out the probability of successful trials are? Like how far apart the
  probability between the successful trials are? Now, here's the confusing part. I see a notation like this: $\overline { X } =\frac { 1 }{ n } \sum _{ i=1 }^{ n }{ { X }_{ i } }  $ and this is called the mean of all the random variables. But it doesn't seem to make sense to me. $X$ is the random variable and carries the value that is the number of successful trials. The average of $X$ is like the average number of successful trials? Does it then mean $\overline { X } =E(X)$? Then, there is also the expectation of the mean of all the random variables, $E(\overline { X } )$. So does this represent the average of the average of all the random variables, which means $E(\overline { X } )=E(E(X))$? But at this point, I couldn't understand what it means intuitively. What does it mean here to say the average of the average of all random variables? Similarly, $Var(\overline { X } )$ is also a confusing term to me. Since $\overline { X } $ is just the average value, what spread does it have? What is the intuitive meaning of this $\overline { X } $ mean of all random variables $X$ and what does this add on to the meaning of $E(\overline { X } )$ and $Var(\overline { X } )$?","['statistics', 'probability']"
110880,Find the EigenValues and EigenVectors of the matrix associated with quadratic form,Problem: Find the EigenValues and EigenVectors of the matrix associated with quadratic forms $2x^2+6y^2+2z^2+8xz$. I know how to convert a set of polynomial equations to a matrix but I have no clue how to convert this single quadratic equation with 3 variables into a matrix. Google tells me to use Cayley-Hamilton theorem... how?,"['matrices', 'quadratic-forms']"
110884,"`""Variation of Constant""` -method to solve linear DYs?","My school instructs to use some method called ""variation of constant"" (first page here ) to solve linear DY more in my earlier question here . I think I solved the problem without the method just by the integrating factor -multiplication. Could someone explain why I get the same solution without using the method to the problem in my earlier question here ? In the foreign course book, the method is very fast covered on the page 636. It mixes up the integrating-factor -method and the variation -method making it for me very hard reading, cannot actually understand a word from it. Then in a rush-minute, it mentions some basic rule ""basic rule from analysis"" -- and well I think it has a mistake on page 637 with minus but well perhaps there is something better in English to explain the variation -method. I am not sure whether it is the variation-of-parameters -method or something else, I cannot yet understand why this method is actually needed in my earlier question (I got the solution by integrating-factor -method -- or so I think, I may be wrong. Please, correct me if I am wrong.). Perhaps Related Differential equation with a constant in it Substitution $x=\sinh(\theta)$ and $y=\cosh(\theta)$ to $(1+x^{2})y'-2xy=(1+x^{2})^{2}$?",['ordinary-differential-equations']
110891,Definition of the Brownian motion,"The way I understood the definition of a Brownian motion $B_t$ in $\mathbb R$ is that it consists of two parts: We first define the finite-dimensional distributions
$$
\nu_{t_1,\dots,t_n}(A_1,\dots,A_n)
$$
Since they satisfy two properties of Kolmogorov extension theorem there is a probability space $(\Omega,\mathscr F,\mathsf P)$ and a stochastic process $X$ such that 
$$
\nu_{t_1,\dots,t_n}(A_1,\dots,A_n) = \mathsf P\{X_{t_1}\in A_1,\dots,X_{t_n}\in A_n\}.
$$ Second, we claim that trajectories of this process have to be continuous with probability $1$. Such claim can be satisfied since by Kolmogorov continuity theorem there is a continuous version $Y$ of a process $X$ with such finite-dimensional distributions $\nu$. What is not clear is the following: in the first step we have already defined the stochastic process $X$. What do we do in the second step? We still stay in the same probability space (since we have to define what does the version mean). So does it mean that finite-dimensional distributions of $Y$ are different from that of $X$? If they are not different, does it mean that $X=Y$ (considering them as measurable function from $\Omega$ to $\mathbb R$) or it means that Kolmogorov extension theorem does not provide the uniqueness?","['probability-theory', 'stochastic-processes', 'brownian-motion']"
110896,Radius of convergence composite function,"If the Taylor series of 2 complex functions $f,g$ have radii of convergence $r_f, r_g$ respectively, does it follow that the radius of convergence of their composition has radius of convergence equal to $\min\{r_f, r_g\}$?","['power-series', 'complex-analysis', 'analysis']"
110903,"Geometrical interpretation of $I(X_1\cap X_2)\neq I(X_1)+I(X_2)$, $X_i$ algebraic sets in $\mathbb{A}^n$","Edit: I should point out that I'm working over an algebraically closed field $k$. Let $X_1,X_2\subset\mathbb{A}^n$ be affine algebraic sets. Show that $I(X_1\cap X_2)=\sqrt{I(X_1)+I(X_2)}$. Show by example that taking the radical here is necessary. Can you see geometrically what it means if $I(X_1\cap X_2)\neq I(X_1)+I(X_2)$? I showed this using that for ideals $\mathfrak{a}_1,\mathfrak{a}_2$ in a ring, $\sqrt{\mathfrak{a}_1+\mathfrak{a}_2}=\sqrt{\sqrt{\mathfrak{a}_1}+\sqrt{\mathfrak{a}_2}}$. As for an example, I took $X_1=V(x)$, $X_2=V(x+y^2)$ in $\mathbb{A}^2$. Then $$I(X_1)+I(X_2)=\langle x\rangle+\langle x+y^2\rangle=\langle x,y^2\rangle,$$
which is not radical. But I don't know exactly how to interpret that geometrically. The intersection of those two varieties is just the origin. I would have supposed that it has something to do with the origin occurring ""more than once"" (read from the ideal $\langle x,y^2\rangle$), but I'm not sure why that is the case. Is it because the variety $X_1$ is a tangent to $X_2$ at the origin, and thus it counts as a ""double"" intersection? Then I don't understand it, because the point $0$ occurs only once in $X_1$, so why twice in the intersection? And what is the difference - geometrically - between $\langle x,y^2\rangle$ and $\langle x^2,y\rangle$? Thank you very much for helping me understand these things!","['commutative-algebra', 'algebraic-geometry']"
110907,Bijection between Regular maps and Algebraic Closed sets,"Can anyone tell me where I can find the proof of the following result: Prove the one-to-one bijection between the set of regular maps between $2$ algebraically closed sets $X$ and $Y$, and the set of $k$-algebra homomorphisms between $k[Y]$ and $k[X]$ Any reference or a complete proof would be appreciated.",['algebraic-geometry']
110913,"Evaluating the definite integral $\int_{-\infty}^{+\infty} \mathrm{e}^{-x^2}x^n\,\mathrm{d}x$","I recognize that the $\int_0^\infty \mathrm{e}^{-x}x^n\,\mathrm{d}x = \Gamma(n+1)$ and $\int_{-\infty}^{+\infty} \mathrm{e}^{-x^2}\,\mathrm{d}x = \sqrt{\pi}$. I am having difficulty, however with $\int_{-\infty}^{+\infty} \mathrm{e}^{-x^2}x^n\,\mathrm{d}x$. By the substitution $u=x^2$, this can be equivalently expressed as $\frac{1}{2} \int_{-\infty}^{+\infty} \mathrm{e}^{-u}u^{\frac{n-1}{2}}\,\mathrm{d}u$. This integral is similar to the first one listed (which equates to the $\Gamma$ function), except that its domain spans $\mathbb{R}$ like the second integral (which equates to $\sqrt{\pi}$). Any pointers on how to evaluate this integral would be helpful.","['improper-integrals', 'calculus', 'integration']"
110921,Area of a trapezoid from given the two bases and diagonals,"Find the area of trapezoid with bases $7$ cm and $20$ cm and diagonals $13$ cm and $5\sqrt{10} $ cm. My approach: Assuming that the bases of the trapezoid are the parallel sides, the solution I can think of is a bit ugly, Find the  other two non-parallel sides of the trapezoid by using this formula. Find the height using this $$ h= \frac{\sqrt{(-a+b+c+d)(a-b+c+d)(a-b+c-d)(a-b-c+d)}}{2(b-a)}$$ Now, we can use $\frac12 \times$ sum of the parallel sides $\times$ height. But, this is really messy and I am not sure if this is correct or feasible without electronic aid, so I was just wondering how else we could solve this problem?",['geometry']
110926,How can I find a conformal map from one domain onto another?,"I have the following problem: Problem. Find a conformal mapping from $\{|z-8|<16\}\setminus\{|z-3|<9\}$ onto $\{t<|z|<1\}$. Find $t$. I know a conformal mapping is one whose derivative doesn't vanish, but nothing more. How should I be thinking about this problem? Is there a standard way of solving it? And what does ""find $t$"" mean? Is it only possible to find the conformal mapping with one particular value of $t$ and not with other values?","['conformal-geometry', 'complex-analysis']"
110939,"Evaluating the definite integral $\int_0^\infty x \mathrm{e}^{-\frac{(x-a)^2}{b}}\,\mathrm{d}x$","To evaluate $\int_0^\infty x \mathrm{e}^{-\frac{(x-a)^2}{b}}\,\mathrm{d}x$, I have applied the substitution $u=\frac{(x-a)^2}{b}$, $x-a=(ub)^{1/2}$, and $\frac{\mathrm{d}u}{\mathrm{d}x}=\frac{2(x-a)}{b}$. I would first like to ask if $x-a$ should actually equal $\pm (ub)^{1/2}$ (i.e., is it valid to ignore the minus sign, and why?). Applying this substitution,
\begin{align}
I &=\int_0^\infty x \mathrm{e}^{-\frac{(x-a)^2}{b}}\, \mathrm{d}x \\
&=\int_0^\infty x \mathrm{e}^{-u} \frac{b\,\mathrm{d}u}{2(x-a)} \\
&=\int_{\frac{a^2}{b}}^\infty \left((ub)^{1/2} + a\right)\cdot{}\mathrm{e}^{-u} \frac{b\,\mathrm{d}u}{2(ub)^{1/2}} \\
&=\frac{b}{2}\int_{\frac{a^2}{b}}^\infty \mathrm{e}^{-u}\,\mathrm{d}u + ab^{-1/2}\mathrm{e}^{-u} u^{-1/2}\,\mathrm{d}u \\
&=\frac{b}{2}\int_{\frac{a^2}{b}}^\infty \mathrm{e}^{-u}\,\mathrm{d}u + \frac{a\sqrt{b}}{2}\int_{\frac{a^2}{b}}^\infty \mathrm{e}^{-u} u^{-1/2}\,\mathrm{d}u,
\end{align}
I find that I am unable to evaluate the second term because the domain is from a non-zero constant to $+\infty$. Were the domain $[0,+\infty)$, the second integral would simply be a $\Gamma$ function. Seeing that the substitution I have attempted has not worked, could someone please propose an alternate route to evaluating this definite integral? Thank you.","['calculus', 'integration']"
110958,Taylor-like expansion for multivariable functions,"Is there any analogue for taylor series for multivariable functions? In other words, can we rewrite any function as a sum of algebraic terms? For example, $x^y$. Can it be written of the form $\sum C_{m,n}x^my^n$, where $C_{m,n}$ is some constant pertaining to the particular m,n (most probably in terms of $\frac{\partial^m}{\partial x^m}x^y$ etc). Is there a generalization for more then two variables? Another example would be $\frac{x+y}{x^2+y^2}$. I suspect that it can be derived by using partial differentials and mashing together the taylor expansions of $f(x,constant)$ and $f(constant,y)$, but I can't manage to do it.",['multivariable-calculus']
110963,Proving $A: l_2 \to l_2$ is a bounded operator,"Let us consider the following linear operator acting on $l_2$:
$$ A(x_1,x_2,x_3,\ldots) ~\colon=~ \left(x_1,\frac{x_1+x_2}{2},\frac{x_1+x_2+x_3}{3},\ldots\right) $$ I need to show that $A$ is a bounded operator, that is $||Ax|| \leq C~||x||$ for some constant $C$ and all $x \in l_2$. In other words, I need to prove the inequality
$$ \sum_{n=1}^{\infty}{\left( \frac{x_1+\ldots+x_n}{n} \right)^2} \leq C 
\sum_{n=1}^{\infty}{x_n^2} $$ I tried to use the fact that
$$ \frac{x_1+\ldots+x_n}{n} \leq \sqrt{\frac{x_1^2+\ldots+x_n^2}{n}} $$
but it doesn't work because in that case we get
$$ \sum_{n=1}^{\infty}{\left( \frac{x_1+\ldots+x_n}{n} \right)^2} \leq \sum_{n=1}^{\infty}~{\frac{x_1^2+\ldots+x_n^2}{n}} = \sum_{n=1}^{\infty}{\left( \frac{1}{n} + \frac{1}{n+1} + \cdots \right)x_n^2} $$
and coefficients of $x_n^2$ diverge. -- Thank you.","['operator-theory', 'functional-analysis']"
110965,Prove that every commutative infinite rng $R$ has an infinite subrng $S$ s.t $S\neq R$,"Prove that every infinite commutative rng $R$ has an infinite subrng $S$ such that $R\neq S$. (Where the rng is not defined to have the identity as a member). Any help or hints of how to go about doing this would be great thanks, I thought I could use elements of infinite order in $\langle R,+\rangle$ but then I'm not sure that there is necessarily elements of infinite order in an infinite group. Thanks for any help.","['ring-theory', 'rngs', 'abstract-algebra']"
110966,"If $\omega$ is an $n$-form on a compact $n$-manifold $M$ without boundary, then $\omega $ is exact if and only if $\int\limits_{M}\omega=0$","If $\omega$ is an $n$-form on a compact $n$-dimensional manifold $M$ without boundary, then $\omega $ is exact if and only if $\int\limits_{M}{\omega }=0$. Maybe there are two ways - use de Rham theory, and another way is to prove this directly.I don't know both. Help!","['differential-forms', 'differential-geometry']"
110970,Extremum in $f:R^2\to R$ via partial differential?,"In my math lectures, I learnt that an extremum of a function $f:\mathbb R^2\to \mathbb R$ requires $\mathrm{grad}(f)=0$. So if $f$ was $f(x_1, x_2)$ that means $(∂f/∂x_1, ∂f/∂x_2) \cdot (x_1, x_2)^{\top}=0$. (Sorry for my poor Tex skills, am working to improve those). No I came across the following in an economics lecture and I can't figure out if what they do is correct: To find an extremum in $\pi=p*f(x_1, x_2) - w_1x_1 - w_2x_2$ they claimed it was sufficient to find a point satisfying $\frac{∂\pi}{∂x_1}=0$ and $\frac{∂\pi}{∂x_2}=0$ In contrast, the  normal gradient approach would yield
$\frac{∂\pi}{∂x_1}x_1 + \frac{∂\pi}{∂x_2}x_2=0$ as the precondition for an extremum. It's pretty clear that $\frac{∂\pi}{∂x_1}=0$ and $\frac{∂\pi}{∂x_2}=0$ implies $\frac{∂\pi}{∂x_1}x_1 + \frac{∂\pi}{∂x_2}x_2=0$, but not the other way round. Because of that, I'd say that the approach used in the economics lecture might not find all interesting points for an extremum. Is that correct? Or is there anything I have overlooked?",['multivariable-calculus']
110972,What is the difference between $\mathrm{E}[Y|X = x]$ and $\mathrm{E}[Y|X]$ and between $\mathrm{Var}(Y|X = x)$ and $\mathrm{Var}(Y|X)$?,"I am slightly confused about the different between $\mathrm{E}[Y|X = x]$ and $\mathrm{E}[Y|X]$ and similarly for Variance. It seems to me the first should be a scalar, because we first pick a specific $X = x$ and then get the expected value of $Y$ within that set whereas the second one is a random variable that depends on the random variable $X$. Is that correct? Any definition using the probabilities $\mathrm{P}(X)$, $\mathrm{P}(Y)$, $\mathrm{P}(Y|X)$ and $\mathrm{P}(Y, X)$ is appreciated.","['statistics', 'probability', 'definition']"
110981,Initial guesses for complex Newton method,"For the iterative method $$z_{n+1} = z_n - \frac{f(z_n)}{f'(z_n)}\hspace{2cm}(*)$$ where $f:U \to \mathbb{C}$ and $U$ is an open subset of $\mathbb{C}$. I know that for stationary points ($f'(z)=0$) and points which enter a cycle, the method will not converge. But are there some sufficient features of an initial guess $z_0 \in U$ so the method converges? I don't want to write that (*) ""will converge with luck"". Thanks for your help!",['complex-analysis']
110987,Define a function given some constraints,"I try to define a two-variable function $f(x,y) \ge 0$ with both $x,y \in [0,1]$ which satisfies the following constraints: $f$ is increasing in both $x$ and $y$ if $x=0$ then $f(x,y)=0$ if $y=0$ then $f(x,y)=0$ only if $x=0$ Any suggestion?","['algebra-precalculus', 'functions']"
111003,Reference request on vector-valued integration,"I'm looking for some nice, neat text which discusses the Bochner and Pettis approaches to integration of vector-valued functions. I'm not interested in the most general case, so the less technical the text the better. To be precise, the level of generality I'm interested in is integration of functions defined on some measure space $(X,\mathcal{M},\mu)$ taking values in some Banach space $V$, w.r.t. the measure $\mu$.","['measure-theory', 'functional-analysis', 'integration']"
111011,"two shapes in a $2n\times 2n$ grid sheet, can we pick third one?","Can anyone help me with this problem? It just popped to my mind!!! we have a $2n\times 2n$ grid sheet and a connected shape $L$ consisting of $2n-1$ grid squares. we've cut two copies of $L$ out of the sheet. Is it always possible to cut a third copy of $L$? I think the answer is yes, but I couldn't solve it. any Ideas?",['combinatorics']
111017,confused about order of operations,"Ok, I'm confused on which is the next step in solving this equation: $$0 = (6.0\text{ kg}) (-3.0\frac{\text{m}}{\text{sec}}) + (78\text{ kg})  v$$ I'm supposed to solve for $v$, however, I'm not sure what the next step is. The solution guide to my problem states that I'm supposed to end up with this: $$v = \frac{-18\frac{\text{m}}{\text{sec}}}{78} = 0.23\frac{\text{m}}{\text{sec}}$$ but how does the $78$ stay positive or the $18$ stay negative. Shouldn't it either be $\dfrac{-18\frac{\text{m}}{\text{sec}}}{-78}$ or $\dfrac{18\frac{\text{m}}{\text{sec}}}{78}$?",['algebra-precalculus']
111021,How to find all rational numbers satisfy this equation?,"Find all rational number $a,b,c$ satisfy: $$a+b+c=abc$$ I try to change this in different forms like $(ab-1)c = a+b$, $(ac-1)b = a+c$, $(cb-1)a = b+c$ etc but it won't help...","['trigonometry', 'algebra-precalculus', 'diophantine-equations', 'contest-math']"
111026,Integrating factor and differential 1-forms,"I am working on the following exercise: The function $f$ is called an integrating factor for the 1-form $\omega$ if $f({\bf x}) \neq 0$ for all $\bf x$ and $d(f\omega) = 0$. If the 1-form $\omega$ has an integrating factor, show that $\omega \wedge d\omega = 0$. I am stuck here... I got $$d(f\omega) = df \wedge \omega + f \wedge d\omega = df \wedge \omega + f\ d\omega = 0$$ but that doesn't seem to get me anywhere. I also tried expanding this further (using the definition of $df$), but this gets quite ugly soon and didn't help either. The same goes for $\omega \wedge d\omega$.","['multivariable-calculus', 'differential-forms']"
111074,Independence of sample mean and variance through covariance,"I have seen the text book derivation where the independence is established through factoring the joint distribution. But has anyone tried to prove that the covariance is zero?. Let $Z_{i}$ come from a standard normal distribution. Let $X = \bar{Z}$ be the sample mean and let $$Y = \sum_i\frac{1}{n-1} (Z_{i} -\bar{Z})^2$$ be the sample variance. Prove that $\operatorname{Cov}(X,Y) = 0$ . I am getting terms like E($\bar{Z}^3$) in the expansion which is making it very cumbersome to handle...","['statistics', 'probability-theory']"
111081,Quotient map from $\mathbb R^2$ to $\mathbb R$ with cofinite topology,"Let $X = \mathbb R$ under the cofinite topology. Is there a quotient map $q : \mathbb R^2 \rightarrow X$? Intuitively, this seems like it should be false, since $\mathbb R^2$ has ""too many"" open sets. However, I am not sure how to prove it. Any ideas?",['general-topology']
111089,Covers without automorphisms,"Let $X\to \mathbf{P}^1$ be a branched cover of the complex projective line, where $X$ is a compact connected Riemann surface. Let $G=\mathrm{Aut}(Y/\mathbf{P}^1)$. Question 1. Could somebody provide some examples of branched covers $X\to \mathbf{P}^1$ for which $G= (0)$? One could construct a compact connected Riemann surface without automorphisms to this end.","['riemann-surfaces', 'algebraic-geometry', 'algebraic-curves']"
111098,(Not) Surprising Result on Natural Numbers as Sum of $k$-Almost Primes,"I started with the following idea: Let $P_k$ be the infinte set of all $k$-almost primes. The counting function for $k$-almost primes less than $x$, is 
$\displaystyle \pi_k(x)\sim\frac{x}{\log x}\frac{(\log \log x)^{k-1}}{(k-1)!}$, where the error goes like $\displaystyle O\left(\frac{x(\log \log x)^{k-2}}{\log x} \right)$. See answer to this MO question . The union of all $P_k$ is $\mathbb{N}$ (except $1$), so the counting functions sum up to $\displaystyle \sum_k \pi_k (x)=x$. I wanted to look at the error term and see how large the discrepancy, between the sum of primes counting functions and natural number couting function (haha!) is. But to my surprise I found the following: With the series expansion of $\displaystyle e^t=\sum_{m=0}^\infty \frac{t^m}{m!}$, where $t=\log \log x$, we get:
$$
\log x = e^{\log \log x}=\sum_{m=1}^\infty\frac{(\log \log x)^{m-1}}{(m-1)!}.
$$
Here I used a Taylor series with derivatives w.r.t. to $\log \log x$. We use $\displaystyle \frac{d^me^{\log \log x}}{d(\log \log x)^m}=1$ .
Now summing up all $\pi_k(x)$, we have 
$$
x=\sum_k \pi_k(x) \sim \frac{x}{\log x} \sum_{k=1}^\infty\frac{(\log \log x)^{k-1}}{(k-1)!} 
=x.
$$ Ok, it's correct, but why? Do the error terms all cancel or is this just a artifact of the bad approximation of the counting function $\pi_k(x)$?","['prime-numbers', 'asymptotics', 'number-theory', 'combinatorics']"
111106,Showing that a sequence converges (in metric space),"In $(\ell ^\infty,{\Vert .\Vert_\infty)}$, how would I show that $x_n=\left(\frac{n+1}{n},\frac{n+2}{2n},\frac{n+3}{3n}, ...\right)$ converges and how would I find the limit? I tried using the fact that the uniform norm ${\Vert .\Vert_\infty}= \text{sup}|X_n|$ and the definition of convergence is that given $\epsilon > 0$, there exists $N \in \mathbb N$ such that $d(x_n,x)< \epsilon$ for all $n>N$, but I cant seem to show it converges. How would I show it converges and find the limit?","['metric-spaces', 'sequences-and-series', 'analysis']"
111118,"Example of height $n$ ideal with $I/I^2$ (locally) $n$-generated, but $I$ is not.","For $R$, a commutative noetherian ring of dimension $d$, I'm looking for an example where $I \subset R$ is an ideal of height $n \lt d$ such that $I/I^2$ is generated by $n$ elements (locally $n$-generated is also fine), however, $I$ itself is not. Moreover, it would be greatly helpful if your response could address the geometric intuition of the example as well.","['commutative-algebra', 'algebraic-geometry', 'examples-counterexamples']"
111123,"Given a list of $2^n$ nonzero vectors in $GF(2^n)$, do some $2^{n-1}$ of them sum to 0?","Let $G=(\mathbb{Z/2Z})^n$ written additively, $n>1$. (you can think of it as $\mathbb{F}_{2^n}$ but I didn't find that useful... yet) Let $v_i$ be nonzero elements of $G$ for $i \in \{1 \dots 2^n \}$ so that $\sum_{i=1}^{2^n} v_i=0$. Is it possible to find a subset $J \subset \{1 \dots 2^n \}$, $|J|=2^{n-1}$ so that $$\sum_{j \in J} v_j=0?$$
Note that the $v_i$s are not assumed to be distinct. Most theorems I've seen of this kind are rendered trivial by the characteristic $2$, although I might have missed something. My first idea would be to apply Alon's Combinatorial Nullstellensatz but I can't find a suitable polynomial. I don't know if this can be solved at all (I suspect this statement is true), but it has a feel of being somewhat well-studied already. Any ideas? (I'm also considering asking this on MathOverflow, do you think it's appropriate?) UPDATE 1, based on some comments: Computer checking by joriki shows the claim for $n=3$ and suggests it for $n=4,5$. For small cases, pairing also works, see Jyrki's comment below. The role of the condition $\sum_i v_i=0$ is unclear; I think in the case $\sum_i v_i \neq 0$ it's actually easier to find a suitable $J$ because then $J$ and its complement don't have the same sums. Thank you all for your help.","['arithmetic-combinatorics', 'finite-fields', 'additive-combinatorics', 'algebraic-combinatorics', 'combinatorics']"
111124,How do I determine if a matrix is contained in another matrix?,"Is there a clever way of determining if one matrix is contained within another larger matrix? Iterating over the larger matrix to check each item until potential matches show up is straightforward but gets slow for large matrices. Example, a smaller matrix: $$\begin{pmatrix}4&3&2\\2&3&4\end{pmatrix}$$ which is ""within"" (I'm probably not using the right terminology) this larger matrix: $$\begin{pmatrix}1&2&3&4&5\\5&\color{red}4&\color{red}3&\color{red}2&1\\1&\color{red}2&\color{red}3&\color{red}4&5\end{pmatrix}$$ It feels like a problem that could have a smart mathematics trick for determining if this is the case - is there one?","['matrices', 'algorithms', 'decision-problems']"
111164,In which ordered fields does absolute convergence imply convergence?,"In the process of touching up some notes on infinite series, I came across the following ""result"": Theorem: For an ordered field $(F,<)$ , the following are equivalent: (i) Every Cauchy sequence in $F$ is convergent. (ii) Absolutely convergent series converge: $\sum_n |a_n|$ converges in $F$ $\implies$ $\sum_n a_n$ converges in $F$ . But at present only the proof of (i) $\implies$ (ii) is included, and unfortunately I can no longer remember what I had in mind for the converse direction.  After thinking it over for a bit, I wonder if I was confusing it with this result: Proposition: In a normed abelian group $(A,+,|\cdot|)$ , the following are equivalent: (i) Every Cauchy sequence is convergent. (ii) Absolutely convergent series converge: $\sum_n |a_n|$ converges in $\mathbb{R}$ $\implies$ $\sum_n a_n$ converges in $A$ . For instance, one can use a telescoping sum argument, as is done in the case of normed linear spaces over $\mathbb{R}$ in (VIII) of this note . But the desired result is not a special case of this, because by definition the norm on a normed abelian group takes values in $\mathbb{R}^{\geq 0}$ , whereas the absolute value on an ordered field $F$ takes values in $F^{\geq 0}$ . I can show (ii) $\implies$ (i) of the Theorem for ordered subfields of $\mathbb{R}$ .  Namely, every real number $\alpha$ admits a signed binary expansion $\alpha = \sum_{n = N_0}^{\infty} \frac{\epsilon_n}{2^n}$ , with $N_0 \in \mathbb{Z}$ and $\epsilon_n \in \{ \pm 1\}$ , and the associated ""absolute series"" is $\sum_{n=N_0}^{\infty} \frac{1}{2^n} = 2^{1-N_0}$ . Because an ordered field is isomorphic to an ordered subfield of $\mathbb{R}$ iff it is Archimedean, this actually proves (ii) $\implies$ (i) for Archimedean ordered fields.  But on the one hand I would prefer a proof of this that does not use the (nontrivial) result of the previous sentence, and on the other hand...what about non-Archimedean ordered fields? Added : The article based on this question and answer has at last appeared: Clark, Pete L.; Diepeveen, Niels J.;
Absolute Convergence in Ordered Fields.
Amer. Math. Monthly 121 (2014), no. 10, 909–916. If you are a member of the MAA, you will be frustrated if you try to access it directly: the issue is currently advertised on their website but the articles are not actually available to members.  The article is available on JSTOR and through MathSciNet.  Anyway, here is an isomorphic copy .  Thanks again to Niels Diepeveen!","['ordered-fields', 'sequences-and-series', 'real-analysis']"
111181,What is the continuous distribution version of multinomial distribution?,"I am trying to model a distribution, on the number of occurrences of an event in a 24 hour time span. Right now, I discretize the 24 hour time span into hourly intervals, and each hour is taken as a categorical outcome, and I count the number of occurrences of the event in each hour (outcome). Hence, this problem is modeled as a multinomial distribution. As time is a continuous variable, is there a continuous version of multinomial distribution? That is, I can count the number of occurrence of the event in the continuous outcome(time).","['statistics', 'probability-distributions', 'multinomial-distribution']"
111185,Using the Lambert W to express a solution of a differential equation.,I solved a differential equation some time ago and I need to solve for $y$. How can we solve for $y$ using the Lambert W function? $$C_1+x = e^y+Cy$$,"['ordinary-differential-equations', 'special-functions']"
111192,How do I write a group action in terms of the action of a subgroup?,"Suppose $H < K < G$ are finite groups and $G$ acts primitively by (right) multiplication on the set $\Gamma = G/K$ of (right) cosets of $K$ in $G$, and $K$ acts primitively on the set $\Delta = K/H$ of cosets of $H$ in $K$.  Let's assume that $H$ is core-free in $K$ and $K$ is core-free in $G$, so these are, indeed, permutation groups. I suspect there is a natural way to write the action of $G$ on the set $\Omega = G/H$ in terms of (a composition of) the actions $G$ on $\Gamma$ and $K$ on $\Delta$.  In other words, I think the action of $G$ on $\Omega$ could be viewed as (1) permuting the cosets of $H$ which lie within a single $K$-coset, followed by (2) permuting the cosets of $K$.  So it seems there is a wreath product underlying this, but I'm having trouble writing it down. Here's what I have so far.  Perhaps someone who knows more group theory can tell me what's right or wrong with it: For each $g\in G$, we have the action $g : Hy \mapsto Hyg$.  Suppose $g = kx$, for some $k \in K$ and some $x \in G$, where $x$ is a $K$-coset representative.  Then the action of $g$ ""factors through"" the action of $k$ as follows: $$g: Hy \mapsto Hyk \mapsto Hykx$$ Now, since we assumed these are permutation groups, we have $K\hookrightarrow Sym(\Delta)$ and $G\hookrightarrow Sym(\Gamma)$.  Let $\mathcal K$ and $\mathcal G$ be the images of $K$ and $G$ under these embeddings. Is the action of $G$ on the set $\Omega=G/H$ somehow related to the wreath product $$\mathcal K^{\Gamma} \rtimes \mathcal G \; ?$$ (Incidentally, I'm fairly certain I don't need the primitivity assumptions, but in my application I happen to know that $H$ is maximal in $K$ and $K$ is maximal in $G$.) Update: Professor Holt's answer below is perfectly clear, but I recently came across this nice article by Cheryl Praeger describing in detail exactly what I had in mind.","['finite-groups', 'group-theory']"
111200,How does big-Oh notation work?,"I am reading this document . In this article after defining strong derivative Knuth goes on to calculate derivative of $x^n$ . There he uses definition of strong derivative to expand $(x+\epsilon)^{n+1}$ as follows, $(x+\epsilon)^{n+1} = (x+\epsilon)[x^n + d_n(x)\epsilon + \mathcal{O}(\epsilon^2)]$ when I expand the right side  I get $x^{n+1} + (x d_n(x) +  \ x^n)\epsilon + \color{red}{x \ \mathcal{O}(\epsilon^2)  + \epsilon^2 \ d_n(x) + \epsilon \ \mathcal{O}(\epsilon^2)}$ But in Knuths calculations the red part is just $\mathcal{O}(\epsilon^2)$ . Question is how? I don't know how to work this out in detail.","['asymptotics', 'calculus']"
111213,Linear Algebra: Find a matrix A such that T(x) is Ax for each x,"I am having difficulty solve this problem in my homework: (In my notation, $[x;y]$ represents a matrix of 2 rows, 1 column) Let $\mathbf{x}=[x_1;x_2]$, $v_1$=[−3;5] and $v_2=[7;−2]$ and let $T\colon\mathbb{R}^2\to\mathbb{R}^2$  be a linear transformation that maps $\mathbf{x}$ into $x_1v_1+x_2v_2$. Find a matrix $A$ such that $T(\mathbf{x})$ is $A\mathbf{x}$ for each $\mathbf{x}$. I am pretty clueless. So I assume that I start off with the following: $x_1v_1 + x_2v_2 = x_1[−3;5] + x_2[7;−2]$ But I do not know what to do from here, or if this is even the correct start!","['matrices', 'linear-algebra']"
111221,Hellinger metrics of Weibull probability distributions,"The Hellinger distance between two probability distributions $f(x)$ and $g(x)$ is given by:
$$H(f,g)=1-\int_\Omega(dx\sqrt{f(x)g(x)})$$
My question is: if $f(x)$ and $g(x)$ are two Weibull distributions defined as $$f(x;\lambda_1,k_1) =  \begin{cases}\frac{k_1}{\lambda_1}\left(\frac{x}{\lambda_1}\right)^{k_1-1}e^{-(x/\lambda_1)^{k_1}} & x\geq0 ,\\0 & x<0 ,\end{cases}$$ and $$f(x;\lambda_2,k_2) =  \begin{cases}\frac{k_2}{\lambda_2}\left(\frac{x}{\lambda_2}\right)^{k_2-1}e^{-(x/\lambda_2)^{k_2}} & x\geq0 ,\\0 & x<0 ,\end{cases}$$ is it possible to derive a metrics tensor in the space of probability as a function of the parameters $\lambda$ and $k$ using the Hellinger distance? Thanks","['probability-distributions', 'differential-geometry']"
111222,Expected value of function of random walk,"I am trying to calculate $\lim_{n \to \infty} {E[e^{i \theta \frac{S_n}{n}}]}$. Where $\theta \in \mathbb{R}$, and $S_n$ is simple random walk.
I could simplify it to $\lim_{n \to \infty}E[\cos(\theta \frac{S_n}{n})]$, but I don't know what to do next.. Can you help me? The hint in the book says that I should use Taylor expansion of $\ln(\cos(x))$ around $x=0$, but I don't see how it can be applied here.","['statistics', 'random-walk', 'probability']"
111224,Definition of asymptote,I understand that the asymptote to a curve is a straight line such that the distance between the curve and the line tends to zero as they tend to infinity. However many books also say that an asymptote is a straight line which meets the curve at two coincident points at infinity. My doubts are: I can't understand the second definition. What does meeting the curve at infinity mean? What does two coincident points at infinity mean? How are these two definitions equivalent? I will be grateful if someone clarifies my doubts. Thanks,"['algebraic-geometry', 'calculus']"
111253,How convergence relates to equivalence of norms,"Let $X$ be a normed linear space with two norms $||\cdot||_1$ and $||\cdot||_2$.
Prove or disprove that this statements are equivalent: $||\cdot||_1$ and $||\cdot||_2$ are equivalent, $\{x_n\}$ converges in $||\cdot||_1$ iff $\{x_n\}$ converges in $||\cdot||_2\;\; $   $\forall \{x_n\}\in X$ The first implies the second trivially because of equivalence of topologies. But it implies something more: sequence converges to the same element in both norms. But I don't have such condition. Is this a necessary condition to converge to the same in both norms, or this is just a consequent of ""if and only if"" in the statement of convergence, or there is a example disproving the equivalence?","['normed-spaces', 'real-analysis']"
111265,What does  $\sin^{2k}\theta+\cos^{2k}\theta=$?,"What is the sum $\sin^{2k}\theta+\cos^{2k}\theta$ equal to? Besides Mathematical Induction,more solutions are desired.",['trigonometry']
111275,Analytic continuation of an integral involving the Mittag-Leffler function,"I have posted this question on MO, and I didn't get an answer. We have the following integral: $$I(s)=\int_{0}^{\infty} \frac{s}{2x}\left(E_{s/2}((\pi x)^{s/2})-1\right)\omega(x)dx -\lim_{z \to 1 }\zeta(z)$$ where $E_{\alpha}(z)$ is the Mittag-Leffler function, $\omega(x)=\dfrac{\theta(ix)-1}{2}$ , $\theta(x)$ is the Jacobi theta function, and $\zeta(s)$ is the Riemann zeta function. The integral behaves well for $Re(s)>1$ . I am trying to extend the domain of the integral to the whole complex plane except for some points. I have tried the identities: $\omega(x^{-1})=-\frac{1}{2}+\frac{1}{2}x^{1/2}+x^{1/2}\omega(x)$ , and $E_{\alpha}(z^{-1})=1-E_{-\alpha}(z)$ , to split the integration at $1$ , and replace $x$ by $x^{-1}$ in the interval $[0,1]$ , and I had hoped to get a factor that would cancel out with the divergent $\zeta(z)$ , and get the Euler–Mascheroni constant or something. But it seems I am lost!!! Hence, the post.","['analytic-continuation', 'special-functions', 'complex-analysis']"
111281,Are minimax and maximin condition interchangable?,"I've came across a classic problem in my field where $$\min_h \max_{\delta,\omega} |h^TP{(\delta,\omega)}-R_d(\delta,\omega)|$$
where $h$ ( a set of coefficients), $\omega$, and $\delta$ are independent variables. My question here is that whether or not this 'minimax' problem can be rewritten/reformulated as a 'maximin' problem? Such that
$$ \max_{\delta,\omega} \min_h |h^TP{(\delta,\omega)}-R_d(\delta,\omega)|$$ Will that make sense? Sorry if this is a silly question, but I tried googling it and wiki it. Doesn't really answer this general question! Thank you in advance!",['functions']
111287,Application of continuity of measure,"I need help with this problem: Let $\mathcal{M}$ be a sigma algebra of subsets of $X$ and the set function $\mu:\mathcal{M}\to [0,\infty)$ be finitely additive. Prove that $\mu$ is a measure if and only when $\{A_k\}_{k=1}^\infty$ is a descending sequence of sets in $\mathcal{M}$, then $$ \mu\left(\bigcap_{k=1}^\infty A_k\right) = \lim_{k\to \infty} \mu(A_k).$$ Below is my attempt: Clearly one direction follows from continuity of measure. Now suppose that the display equation above is true. Let $E=\cup_{k=1}^\infty E_k$, where the $E_k$ are disjoint. Set $A_k=E\setminus \cup_{k=1}^n E_k$. Then $A_k \supseteq A_{k+1}$. Also, $\cap_{k=1}^\infty A_k = \emptyset$. $\mu$ is also finitely additive, so $$ \mu(A_k)= \mu(E)- \sum_{n=1}^k \mu(E_n).$$ 
So $$\begin{align*}

\mu(E) & = \lim_{k\to \infty} \mu(A_k)+\lim_{k\to \infty} \sum_{n=1}^k \mu(E_n)\\
&        = \mu \left(\cap_{k=1}^\infty A_k\right) + \sum_{k=1}^\infty \mu(E_k)\\
& = \sum_{k=1}^\infty \mu(E_k)

\end{align*}
$$ Please, does this look right?",['measure-theory']
111288,Why $\arccos(\frac{1}{3})$ is an irrational number?,I was reading the following question . It is a very nice question with a very nice answer! I would like to know why $\arccos(\frac{1}{3})$ is an irrational number.,"['trigonometry', 'number-theory']"
111293,Probability Term for something that defies the odds.,"I'm not a mathematician; I just wandered over here from Writers SE and am hoping you guys can help. I'm writing a novel in which the theme is characters beating the odds. (It's a future dystopia, the government is super-powerful, etc. - the characters aren't doing any actual math). I'm looking for a term for a character who beats the odds. (I'm looking for gambling terms AND probability terms). So, going in, probability said that x was very likely to happen. But then y happened, instead.  Is there a term for y, after the event is over? (look at me trying to be all math-y - hopefully someone can edit this to be more coherent). In gambling terms, I'm looking for a word that would mean the dark horse or underdog AFTER he's won. Maybe math doesn't personalize things so much, and there just isn't a term, but if there is one, I'd love to know about it! Thanks!","['terminology', 'probability']"
111314,Choose a random number between $0$ and $1$ and record its value. Keep doing it until the sum of the numbers exceeds $1$. How many tries do we need?,Choose a random number between $0$ and $1$ and record its value. Do this again and add the second number to the first number. Keep doing this until the sum of the numbers exceeds $1$. What's the expected value of the number of random numbers needed to accomplish this?,"['probability-theory', 'probability', 'expectation']"
111319,A tricky double integral,"What is $$\int_0^1 \int_0^1 \frac{ dx \; dy}{1+xy+x^2y^2} ? $$ Can you do one of the integrals and turn it into a single integral?
I get lost in a sea of inverse tangents.",['integration']
111328,Eigenvalues of a parameter dependent matrix,"Given the parameter dependent matrix $A=\begin{pmatrix} 0 & I\\ A_1 & kA_2\end{pmatrix}$, with $A_1, A_2\in \mathbb{R}^{n\times n}$ and $k\in\mathbb{R}>0$, is there a way to display the eigenvalues of $A$ as a function of $A_1, A_2, k$? Or to give an estimation of the eigenvalues' location? What can be said about the corresponding eigenvectors of $A$ dependent on $k$? We can suppose that $A_1,A_2$ have full rank and all eigenvalues are negative or have a negative real part. They may have repeated eigenvalues. Clearly if $\lambda(A)$ is an eigenvalue, for $k=0$ we have $\lambda(A)=\pm \sqrt{\lambda(-A_1)}$, and for $k\rightarrow \infty$ we have $n$ eigenvalues $\lambda(A)=0$ and $n$ eigenvalues $\lambda(A)=\lambda(kA_2)$. Furthermore we have $k\mathrm{tr}(B)=\sum_{i=1}^{2n} \lambda_i(A)$. Assuming that $\mathrm{tr}(B)<0$ this would mean that at least some eigenvalues of $A$ have negative real parts of magnitude growing with $k$. I'm interested in what happens for $0\ll k\ll\infty$. My intuition is that with growing $k$ the eigenvalues of $A$ will move into the left half-plane, before tending to $-\infty$ (as all eigenvalues of $A_2$ have negative real parts) or $0$ but so far I haven't been able to find a suitable proof method for this.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
111332,source of proof for a characterization of normal distribution,"I want to know the proof of the following statement about normal distribution: If the sample mean and sample variance are independent for a population, then the distribution of the population is normal. I cannot find the proof in any probability/statistics textbook that I have. Please help.","['statistics', 'reference-request', 'probability']"
111342,Measure problems.,"Suppose that $A \subset [0, 1]$ is a measurable set. Prove that the set $B = \cos A := \{\cos x: x \in A\}$ is measurable and $\mu(B) \leq 0.85\mu(A)$, where $\mu$ is the Lebesgue measure.","['measure-theory', 'real-analysis']"
111354,"Polar coordinates, line integrals, and the Beltrami Identity","Imagine you are walking along the xy-plane. There is a landmark at the origin of the plane which distorts time at every point on the plane, such that the distortion is a function of the distance between that point and the origin. What this essentially means is that, if you're trying to get from point A to point B, you'll want to stay a reasonable distance away from the origin, because the cost of moving a given unit of distance is higher the closer you get to the origin. What I'm trying to do is, given two points A and B, find the path from A to B which will minimize the cost of travel: the ""subjective distance"". The first step, then, is to construct an equation to represent the cost for a given path; the second step is to find a path that minimizes that cost, using the Calculus of Variations. The equation for temporal distortion is given as $\tau = C r^{-n} + 1$, where r is the distance from the origin. If we define the path as $r(\theta)$, then the cost of travelling from A to B should be $\int_{r(\theta)} C r^{-n} + 1 ds$, or $\int_{\alpha}^{\beta} \left( C \left[r(\theta)\right]^{-n} + 1 \right) \sqrt { \left[ r(\theta) \right]^2 + \left[ r'(\theta) \right]^2 } d\theta $. Now, in order to find a $r(\theta)$ such that the integral is minimized, I'm supposed to use the Beltrami Identity (because there's no $\theta$ term in the integral!), which is given as $$
L - r' \frac{\partial L}{\partial r'} = const
$$ where L is the integrand above. So I find the partial derivative: $$
\begin{align}
&\frac{\partial}{\partial r'} \left[ \left( C r^{-n} + 1 \right) \sqrt{ r^2 + r'^2 } \right]\\
&{} = \left( C r^{-n} + 1 \right) \frac{\partial}{\partial r'} \sqrt{ r^2 + r'^2 }\\
&{} = \left( C r^{-n} + 1 \right) \cdot \frac{1}{2}  \left( r^2 + r'^2 \right)^{-\frac{1}{2}} \cdot 2r'\\
&{} = \frac{r' \cdot \left( C r^{-n} + 1 \right)}{\sqrt{r^2 + r'^2}}
\end{align}
$$ And I substitute into the Beltrami Identity: $$
L - r' \frac{\partial L}{\partial r'} = const\\
\left[ \left( C r^{-n} + 1 \right) \sqrt{ r^2 + r'^2 } \right] - r' \frac{r' \cdot \left( C r^{-n} + 1 \right)}{\sqrt{r^2 + r'^2}} = const\\
\frac{\left( C r^{-n} + 1 \right) \left( r^2 + r'^2 \right)}{\sqrt{r^2 + r'^2}} - \frac{\left( C r^{-n} + 1 \right) r'^2}{\sqrt{r^2 + r'^2}} = const\\
\frac{\left( C r^{-n} + 1 \right) r^2}{\sqrt{r^2 + r'^2}} = const
$$ This is the differential equation I must solve to find the optimum path. However, this cannot be the correct differential equation. We can observe that $r = const$ is a solution to this differential equation, but if we let C equal 0, then the distortion disappears... which would imply that the shortest distance between two points on a normal plane is a circular arc between them. Which is incorrect. I can't find a point in my equation where I assumed $C \neq 0$, so I must have done something wrong algebraically... or conceptually. Can anyone spot my error?","['polar-coordinates', 'ordinary-differential-equations', 'calculus-of-variations']"
111357,Is there an $O(n^2)$ test to determine if an $n \times n$ Boolean matrix $B$ has an inverse?,"D.E. Rutherford shows that if a Boolean matrix $B$ has an inverse, then $B^{-1}= B^T$, or $BB^T=B^TB=I$. I have two related questions: The only invertible Boolean matrices I can find are permutation matrices. Are there others? Is there an $O(n^2)$ test to determine if an $n \times n$ Boolean
        matrix $B$ has an inverse? Note : The $O(n^2)$ Matlab function I gave here is wrong. UPDATE : I have posted a new $O(n^2)$ Matlab invertibility test here .","['matrices', 'boolean-algebra']"
111368,What is a geometric explanation of complex integration in plain English?,"Im trying to get my head around complex integration/complex line integrals. Real integration can be thought of as the area under a curve or the opposite of differentiation. Thinking of it geometrically as the area under a curve or the volume under a surface in 3 dimension is very intuitive. So is there a geometric way of thinking about complex integration? Or should I just be viewing it as process that reverses differentiation? Or has integration other meanings in complex analysis? Here is an example. Could someone explain this to me?
Here's the definition of the integral along a curve gamma in $\mathbb{C}$ , parameterized by $w(t):[a, b] \to \mathbb{C}$ . $$\int_\gamma f(z) \mathrm{d}z = \int_a^b f[w(t)] w'(t) \mathrm{d}t$$ So I have that: $\gamma$ is the unit circle with anti-clockwise orientation parameterized by $w:[0, 2\pi] \to \mathbb{C}$ . $w(t) = e^{it} = \cos(t) + i \sin(t)$ . So if we use the definition of the integral, $$\int_\gamma f(z) \mathrm{d}z = \int^b_a f[w(t)]w'(t) \mathrm{d}t$$ and work this out, it comes to $$\int^{2\pi}_0 i \mathrm{d}t = 2{\pi}i$$ So what does this $2{\pi}i$ represent? Does it mean anything geometrically, like if a regular integral works out to be 10 that means the area under the curve between 2 points is 10...","['integration', 'complex-analysis']"
111374,"Solution to Locomotive Problem (Mosteller, Fifty Challenging Problems in Probability)","My question concerns the solution Professor Mosteller gives for the Locomotive Problem in his book, Fifty Challenging Problems in Probability. The problem is as follows: A railroad numbers its locomotives in order 1, 2, ..., N. One day you see a locomotive and its number is 60. Guess how many locomotives the company has. Mosteller's solution uses the ""symmetry principle"". That is, if you select a point at random on a line, on average the point you select will be halfway between the two ends. Based on this, Mosteller argues that the best guess for the number of locomotives is 119 (locomotive #60, plus an equal number on either ""side"" of 60 gives 59 + 59 + 1 = 119. While I feel a bit nervous about challenging the judgment of a mathematician of Mosteller's stature, his answer doesn't seem right to me. I've picked a locomotive at random and it happens to be number 60. Given this datum, what number of locomotives has the maximum likelihood? It seems to me that the best answer (if you have to choose a single value) is that there are 60 locomotives. If there are 60 locomotives, then the probability of my selecting the 60th locomotive at random is 1/60. Every other total number of locomotives gives a lower probability for selecting #60. For example, if there are 70 locomotives, I have only a 1/70 probability of selecting #60 (and similarly, the probability is 1/n for any n >= 60). Thus, while it's not particularly likely that there are exactly 60 locomotives, this conclusion is more likely than any other. Have I missed something, or is my analysis correct?","['recreational-mathematics', 'probability']"
111377,Self-avoiding walk on $\mathbb{Z}$,"How many sequences $a_1,a_2,a_3,\dotsc$, satisfy: i) $a_1=0$ ii) ($a_{n+1}=a_n-n$ or $a_{n+1}=a_n+n$) iii) $a_i\neq a_j$ for $i\neq j$ iiii) $\mathbb{Z}=\{a_i\}_{i>0}$ Are the two alternating sequences the only solutions? $a_1,a_2,a_3,..=0,1,-1,2,-2,3,-3,4,...$ or $a_1,a_2,a_3,..=0,-1,1,-2,2,-3,...$ Is there a sequence satsifying i),iii),iiii) and ii) ($a_{n+1}=a_n-n^2$ or $a_{n+1}=a_n+n^2$) ?","['graph-theory', 'recurrence-relations', 'sequences-and-series', 'number-theory']"
111379,Does every sequence that satifies this requirement converge? [duplicate],"This question already has answers here : Closed 12 years ago . Possible Duplicate: Why doesn't $d(x_n,x_{n+1})\rightarrow 0$ as $n\rightarrow\infty$ imply ${x_n}$ is Cauchy? If we have a sequence $(x_n)_n$ that satisfies this rule: $\forall \varepsilon > 0, \exists n_0 : \forall n > n_0 : |x_{n+1} - x_n| < \epsilon$ Does is always converge?  The criterion is less strict than that of a Cauchy sequence, but I fail to find an example of a sequence which doesn't converge.","['convergence-divergence', 'sequences-and-series']"
111391,Fractional differential equation,"Does someone know how to solve this fractional differential equation? $$a\frac{d^2}{dx^2}u(x)+b\frac{d^\frac{1}{k}}{dx^\frac{1}{k}}u(x)+cu(x)=0$$ assuming $(a,b,c) =const$ and $k$ a parameter? Thanks in advance","['fractional-differential-equations', 'ordinary-differential-equations', 'fractional-calculus']"
111406,Regularity of the distance function,"Let $\Omega \subseteq \mathbb{R}^N$ be open and bounded, and set: $$d(x):=\text{dist} (x,\partial \Omega) =\inf_{y\in \partial \Omega} |x-y|\;.$$
I would appreciate if somebody could verify my proof. I tried to show that it is Lipschitz continuous: Let $\forall x,y \in \Omega$, and WLOG assume that $d(x)\geq d(y)$. Let $\forall \varepsilon >0$. By definition of infimum, $\exists z \in \partial \Omega$ such that $d(y)+\varepsilon > |y-z|$ and $|x-z| \geq d(x)$. Putting everything together, we obtain that $0 \leq d(x)-d(y) \leq |x-z| - |y-z| +\varepsilon \leq |x-y| + \varepsilon $. Since $\varepsilon$ was arbitrary, done.",['analysis']
111420,Are there ideals in $M_n(P)$ that are not of the form $M_n(I)$?,"Let $P$ be a noncommutative ring with units, $M_n(P)$ be a ring of matrices with coefficients in $P$. Is there an two-sided ideal $J$ in $M_n(P)$ that is not of the form $J=M_n(I)$, where $I$ is some two-sided ideal in $P$? Thanks.","['ring-theory', 'abstract-algebra']"
111443,Construct a function that is differentiable only on the rationals.,"In the spirit of having handy counter examples, is it possible to construct a function that is differentiable on all of the rational numbers and nowhere else? Similarly, if a function is holomorphic at a point, must it always be holomorphic in an open neighborhood around that point? Constructive proofs and counterexamples will be given priority.","['complex-analysis', 'calculus', 'real-analysis', 'analysis']"
111486,Solving the equation $- y^2 - x^2 - xy = 0$,"Ok, this is really easy and it's getting me crazy because I forgot nearly everything I knew about maths! I've been trying to solve this equation and I can't seem to find a way out.
I need to find out when the following equation is valid: $$\frac{1}{x} - \frac{1}{y} = \frac{1}{x-y}$$ Well, $x \not= 0$, $y \not= 0$, and $x \not= y$ but that's not enough I suppose. The first thing I did was passing everything to the left side:
$$\frac{x-y}{x} - \frac{x-y}{y} - 1 = 0$$ Removing the fraction:
$$xy - y² - x² + xy - xy = 0xy$$ But then I get stuck..
$$- y² - x² + xy = 0$$ How can I know when the above function is valid?",['algebra-precalculus']
111489,The union of growing circles is not homeomorphic to wedge sum of circles,"Let $X$ be the subspace of $\mathbb{R}^2$ that is the union of the circles $C_n$ of radius $n$ and center $(n,0)$ for $n \in \mathbb{N}$.  Show that $X$ and $\bigvee_\infty S^1$ are homotopy equivalent, but not homeomorphic. It is unclear to me why the ""obvious"" maps between the two spaces do not give a homeomorphism.   It seems like the difference would have to be a result of the behavior of the map near the origin.  Could I get a hint?","['general-topology', 'algebraic-topology']"
111508,Proof that $\binom{n}{\smash{0}}^2+\binom{n}{1}^2+\cdots+\binom{n}{n}^2=\binom{\smash{2}n}{n}$ using a counting argument,Prove the following by way of a counting argument: $$\binom{n}{0}^2+\binom{n}{1}^2+\cdots+\binom{n}{n}^2=\binom{2n}{n}$$,"['binomial-coefficients', 'combinatorics']"
111518,Counting (and enumerating) pairs,Suppose you have 4 tickets to a gala event to be distributed to your friends. Let's say you have 6 male friends and 7 female friends. Each ticket requires a male-female couple. How do you count in how many ways you can match your friends to the 4 tickets? How would you enumerate all possible combinations?,"['discrete-mathematics', 'combinatorics']"
111522,What's wrong with this solution of Tarski's circle-squaring problem?,"Tarski's circle-squaring problem asks whether it is possible to cut up a circle into a finite number of pieces and reassemble it into a square of the same area. Note that this is different from the problem of squaring the circle (which is about compass and straightedge constructions) and also from the Banach-Tarski paradox (which involves a change of volume). Laczkovich showed in 1990 that the answer was yes, and proved the existence of a solution with about $10^{50}$ pieces. The proof was nonconstructive. What is the error in the following proof that the answer to the problem is no, contrary to Laczkovich? At a given stage in the process of cutting and reassembly, let $u$ be the total length of all convex edges that are circular arcs whose radius equals the radius $r$ of the original circle, and similarly let $v$ be the total length of all such concave edges. Initially, $u=2\pi r$ and $v=0$. At the end of the construction, we would have to have $u=v=0$. But $u-v$ is conserved by both cutting and reassembly, so this is impossible. [EDIT] Edited to remove mistake about Banach-Tarski.",['geometry']
111526,Why are these groups solvable?,"Let $G$ be the group presented by
$$G=\bigl\langle x,y,z\;\bigm|\; xy=yx,\;zxz^{-1}=x^my^n,\;zyz^{-1}=x^py^q\bigr\rangle.$$ I would like to prove that for every $m,n,p,q$, $G$ is solvable. What is my idea: call $d=mq-np$. First suppose that $d\neq0$. I think that in this case $G\cong (\mathbb{Z}[\frac{1}{d}])^2\rtimes_\varphi\mathbb{Z}$ (that is solvable), where $\varphi(1)(1,0)=(m,n)$ and $\varphi(1)(0,1)=(p,q)$; but I have some problems to find $\alpha,\beta$ such that $\alpha^d=x$ and $\beta^d=y$, could you help me? Now suppose $d=0$, if $m=n=p=q=0$ then $G=\mathbb{Z}$ and it's solvable. Otherwise suppose $m\neq0$, I don't know how to continue now, I proved that in this case $x^p=y^m$ I don't know if this helps, any idea? EDIT: in the second case we can also suppose $p\neq0$, in fact if $p=0$ then $q=0$ and so $zyz^{-1}=1$ that implies $y=1$ and so $G$ becomes $\langle x,z|zxz^{-1}=x^m\rangle$ that is isomorphic to $\mathbb{Z}[\frac{1}{m}]\rtimes\mathbb{Z}$ that is solvable. EDIT EDIT: now suppose $n=0$ then $q=0$ so the group becomes $\langle x,y,z|xy=yx,zxz^{-1}=x^m,zyz^{-1}=x^p\rangle$ but I proved that $x^p=y^m$ and so in this case $G\cong(\mathbb{Z}[\frac{1}{m}])^2\rtimes\mathbb{Z}$ that is solvable, so we can assume $n,m,p,q\neq0$ (I really don't know if this is useful).",['group-theory']
111541,$n^{th}$ roots and $e$.,"In Churchill's book on complex variables, the $n^{th}$ root of $e$ is defined to be $e^{1/n}$. A comment is made that in this respect $e$ is treated differently than the $n^{th}$ roots of other complex numbers (in the sense that there are typically n roots of the nth root of a number in complex analysis rather than just one as in the case of $e$). I am curious why $e$ is treated so differently. Is there an obvious reason/motivation why? Edit: The section from Churchill is, As anticipated earlier, we define here the exponential function $e^z$ by writing
$$ e^z = e^xe^{iy}\ \ \ \ \ \ (z = x + iy)\ \ \ \ \ \ \ \ \ (1)$$
where Euler's formula
$$ e^{iy} = \cos y + i\sin y$$
is used and $y$ is to be taken in radians. We see from this definition that $e^z$ reduces to the usual exponential function in calculus when $y=0$; and, following the convention used in calculus, we often write $\exp z$ for $e^z$. Note that since the positive $n$th root $\sqrt[n]{e}$ of $e$ is assigned to $e^x$ when $x = 1/n$ ($n = 2,3,\ldots$), expression (1) tells us that the complex exponential function $e^z$ is also $\sqrt[n]{e}$ when $z = 1/n$ ($n = 2,3,\ldots$). This is an exception to the convention that would ordinarily require us to interpret $e^{1/n}$ as the set of $n$th roots of $e$.","['complex-numbers', 'complex-analysis']"
111551,Product Chain Power Rule： Either it's the book or I am wrong.,"The problem:　$x^3\sqrt{2x+4}$ $f(x):= x^3$,  $g(x):= \sqrt{2x+4}$ $(f\times g)' = f^{\prime}g+fg^{\prime}$  thus it should be $3x^2\sqrt{2x+4} + (x^3)[\frac{1}{2}(2x+4)^{\frac{-1}{2}}(2)]$ which is $3x^2\sqrt{2x+4}+\frac{x^3}{\sqrt{2x+4}}$ The book gives: $3x^2\sqrt{2x+4}+\frac{x^3}{2\sqrt{2x+4}}$ I'm correct? I always get worry when my answers don't match the book. $\frac{d}{dx}[f\times g(h(x))] = f^{\prime} \times g(h(x))+ f\times g^{\prime}(h(x))h^{\prime}$ right?",['derivatives']
111559,"Class equation of subgroup of $SL(4,\mathbb{F}_2)$","Can you point me toward a computation-light derivation of the class equation of the subgroup of $SL(4,\mathbb{F}_2)$ consisting of upper-triangular matrices with 1's on the main diagonal? The question is motivated by a homework problem from a class in representation theory. The homework problem is to describe all the irreducible complex representations of the above group. I began by doing pages of calculations in the group, trying to find the conjugacy classes by hand, so I would know how many representations to expect and might be able to play with the numbers to find their dimensions. I turned away from this because it was getting too computationally heavy for me to feel safe I wasn't making any small mistakes. I've started looking for representations in an ad-hoc way, but I don't know how many to expect or what dimensions, so this is also pretty hairy. Also, I'd like to organize my results using the character table, but for this again I would need to know the group's decomposition into conjugacy classes. Besides specific computations, here is what I have figured out about the group: it is a semidirect product of $D_4$ and $C_2\times C_2\times C_2$. This is because it acts on vectors in $\mathbb{F}_2^4$ with last component 0. The action coincides with the action of the upper left $3\times 3$ block on $\mathbb{F}_2^3$, so this gives us a homomorphism onto the subgroup of $SL(3,\mathbb{F}_2)$ of upper-triangular matrices with 1's on the main diagonal. This latter group is isomorphic to $D_4$, and the restriction of the homomorphism to the subgroup of the original group which is 0 off the diagonal in the last column is an isomorphism. The kernel is the subgroup of matrices that are zero off the main diagonal away from the last column, and this is isomorphic to $C_2\times C_2\times C_2$. However I am unsure how to use this information to find the class equation.","['finite-groups', 'representation-theory', 'group-theory']"
111570,What is this formula?,"There is some formula that I can't precisely remember for polynomials, which goes something like $x^n-1 = (x-1)(\text{a lot of stuff})$. It could be more general, like $x^n - k$, or maybe it is just for the same powers, so $x^m - k^m$, but I think it's not just for the same powers. Does anyone know what I am talking about? I realize this is vague.",['algebra-precalculus']
111576,Localization at a prime ideal is a reduced ring,"Here is the question that I came up with, which I am having trouble proving or disproving: Let $A$ be a ring (commutative). Let $p \in Spec(A)$ such that $A_p$ is reduced. Then there exists an open neighborhood of $U \subset Spec(A)$ containing $p$ such that $\forall q \in U$, $A_q$ is reduced. Here is some background to my question: I am basically trying to prove that if the stalks at all closed points of a quasicompact scheme are reduced rings, then the scheme is reduced. Since the closure of every point of a quasicompact scheme contains a closed point of that scheme, proving the above commutative algebra statement (if it is true) will yield a proof of this statement about reducedness of quasicompact schemes. If the statement in bold is true, then I guess the neighborhood $Spec(A)-V(A-p)$ should suffice (this is just a guess), but I am running into some problems trying to use this neighborhood to show that the localization at every point of $Spec(A)-V(A-p)$ gives me a reduced ring. So there might be some other neighborhood of $p$ that I am missing, or the statement in bold is not true. Either way, some help would be appreciated (if the statement in bold is true, then I would appreciate hints and not complete answers).","['commutative-algebra', 'algebraic-geometry', 'abstract-algebra']"
111577,About the second fundamental form,"Let $U\subset\mathbb R^3$ be an open set, and $f:U\to \mathbb R$ be a smooth function. Suppose that the level set $S=f^{-1}(\{0\})$ is non-empty, and that at each $p\in S,$ the gradient $\overrightarrow \nabla f(p)$ is not the zero vector. Then $S$ is a smooth two-dimensional surface in $U$, and $p\mapsto \overrightarrow \eta(p)=\frac{1}{||\overrightarrow \nabla f(p)||}\overrightarrow \nabla f(p)$ defines a smooth unit-length normal vector field along $S$. At each $x\in U,$ write $H(f)_{(x)}$ for the $3\times 3$ Hessian matrix specified by $$(H(f)_{(x)})_{ij}=\frac{\partial^2f}{\partial x_i\partial x_j}(x).$$ Show that , at each $p\in S$, the second fundamental form $II_p: T_p(s)\times T_p(s)\to \mathbb R$ is the symmetric bilinear map 
  $$II_p(\overrightarrow v,\overrightarrow w)=\frac{-1}{||\overrightarrow \nabla f(p)||}\overrightarrow v\cdot H(f)_{(p)}\overrightarrow w,$$for all $\overrightarrow v ,\overrightarrow w \in T_p(s)$. (Here, we view the tagent space $T_p(S)$ as the two-dimensional subspace $(span\{ {\overrightarrow \eta(p)}\})^{\bot}$ of $\mathbb R^3$. Edit: Actually my question is why the second fundamental form under the usual definition can be written in this way. Definition : The quadratic form $II_p$, defined in $T_p(S)$ by $II_p(v)=-<d  N_p(v),v>$ is called the second fundamental form of $S$ at $p$, where $dN_p:T_p(S)\to T_p(S)$ is the differential of the Gauss map. Hopefully, I express this problem explicitly. I was just wondering how to prove this statement. I took a diffrential geometry class last semester, and when I organized my notes this morning, I found this statement, but there was no proof... Looking forward to an understandable explaination. Thanks in advance. Edit 2 :Furthermore, show that , at each point $p\in S$, the expression
  $$\phi_p(z)=det\pmatrix{-H(f)_{(p)}-zI_{3\times 3} & \overrightarrow \nabla f(p)\\\ \pm \overrightarrow \nabla f(p)& 0}$$
  (the underlying matrix here is $4\times 4$) defines a second-degree polynomial whose roots $\lambda_1$ and $\lambda_2$ are $||\overrightarrow \nabla f(p)||k_1$ and $||\overrightarrow \nabla f(p)||k_2$, where $k_1$ and $k_2$ are the principal curvatures of $S$ at $p$. Also , if a non-zero vector $\pmatrix {\overrightarrow v \\c}$ lies in the kernel of the $4\times 4$ matrix $$\pmatrix{-H(f)_{(p)}-\lambda_jI_{3\times 3} & \overrightarrow \nabla f(p)\\\ \pm \overrightarrow \nabla f(p)& 0},$$
  then $\vec v$ is a non-zero element of $T_p(S)$ and lies in the ""principal direction"" corresponding to $K_j$.",['differential-geometry']
111610,"If the product of two non-zero square matrices is zero, then both factors must be singular.","In the textbook Contemporary Linear Algebra by Anton and Busby, there was a small question in section 3.2 page 101 concerning this. It asks if $A$ and $B$ are two non-zero square matrices such that $AB=0$, then $A$ and $B$ must both be singular. Why is this so? I can prove that if $A$ is non-singular then $B=I_nB=A^{-1}AB=0$, implying $B$ must be the zero matrix which is a contradiction. Similarly if $B$ is non-singular, then $A$ must be the zero matrix. Hence, both must be singular. But this doesn't really answer why, it just shows a contradiction for any case and hence must be the negation of our supposition that at least one is non-singular. I would like to know the essence and inherent property as to why they must be both singular (and why can't it be the case that only one is singular?) and what is the motivation for such a conclusion?","['matrices', 'linear-algebra']"
111619,Further explanation on proof that associated primes are precisely those belonging to primary modules in reduced decomposition of $0$.,"Consider the following theorem in Chapter X Noetherian Rings and Modules from Lang's Algebra (page 423, third edition): Theorem 3.5. Let $A$ and $M \neq 0$ be Noetherian. The associated primes of $M$ are precisely the primes which belong to the primary submodules in a reduced primary decomposition of $0$ in $M$ . In particular, the set of associated primes of $M$ is finite. Proof: Let $0=Q_1\cap\cdots\cap Q_r$ be a reduced primary decomposition of $0$ in $M$ . There is an injective homomorphism $$
M\to\bigoplus_{i=1}^r M/Q_i.
$$ Then every associated prime of $M$ belongs to some $Q_i$ . (1) Conversely, let $N=Q_2\cap\cdots\cap Q_r$ . Then $N\neq 0$ because the decomposition is reduced. Then $$
N=N/(N\cap Q_1)\approx (N+Q_1)/Q_1\subset M/Q_1.
$$ Hence $N$ is isomorphic to a submodule of $M/Q_1$ , and consequently has an associated prime which can be none other than the prime $p_1$ belonging to $Q_1$ . (2) I have two questions about this proof I hope somebody can clarify. How does one conclude from the injective homomorphism that every associated prime of $M$ belongs to some $Q_i$ ? I'm aware that a submodule $Q$ of $M$ is primary iff $M/Q$ has exactly one associated prime $p$ , in which case $p$ belongs to $Q$ . I also know that for a submodule $N$ of $M$ , an associated prime of $M$ is associated with $N$ or with $M/N$ , but don't know how to tie it together. Why does $N$ being isomorphic to a submodule of $M/Q_1$ implies that $p_1$ is its associated prime?","['maximal-and-prime-ideals', 'abstract-algebra', 'noetherian', 'modules', 'commutative-algebra']"
111658,Differential form is closed if the integral over a curve is rational number.,"The following problem comes from do Carmo's book Differential Forms and Applications , Chapter 2, Exercise 4: Let $\omega$ be a differentiable 1-from defined on an open subset $U\subset \mathbb{R}^n$. Assume that for each closed differential curve $C$ in $C$, $\int_C \omega$ is a rational number. Prove that $\omega$ is closed. Can anyone give some hints? My idea was that we could try to show the integral vanishes if the curve lies in $N_\epsilon (p)$ (i.e. a small neighborhood around $p$) for any $p$ in $U$ and use the Poincare Lemma, but failed.",['differential-geometry']
111678,Is there a null sequence that is in not in $\ell_p$ for any $p<\infty$?,"Is 
$$\bigcup_{p<\infty}\ell_p=c_0 ?$$ At least one inclusion obvious: every $p$-summable sequence converges to zero.","['lp-spaces', 'sequences-and-series', 'functional-analysis', 'banach-spaces']"
111682,Periodic composition of non-periodic functions,Could you help me with following problem? I need to find two non-periodic functions $f$ and $g$ where their composition $f(g)$ will be periodic. Note that constant function is periodic too. Thanks for help!,"['functions', 'real-analysis', 'function-and-relation-composition']"
111692,Partial Derivatives and Linear Map,"Proposition: Suppose that $T : R^n \to R^m$
is the linear transformation defined by $T(x) = Mx$ for some m × n matrix $M$. Then $DT(x) = M$ for all points $x \in R^n$ where $D$ is the partial derivative matrix. (Jacobian?) Question: I don't understand what is being said. $T(x)$ is a linear transformation on $x$. How does the partial derivative of $T(x)$ lead to the transformation matrix. Neither do I have an algebraic intuition nor a geometric one. Further,
How is the total derivative of $g(x,y,z)$ equal to $ Dg(x,y,z) \begin{pmatrix}
x \\
y\\
z \end{pmatrix}$? This is stated without proof. There is a chance, I made a wrong interpretation so I am pasting the portion of the text where it appears. Is it that the change in $x$ in all dimensions of the output of $g(x,y,z)$ multiplied by $x$ and similar for y and z gives a total derivative. I don't seem to understand.I know the total derivative is a derivative taking into account that other variables are not constant during differentiation by one variable.","['multivariable-calculus', 'linear-algebra']"
111700,Probability of global epidemic,"Consider $\mathbb{Z}^2$ as a graph, where each node has four neighbours. 4 signals are emitted from $(0,0)$ in each of four directions (1 per direction) . A node that receives one signal (or more) at a timestep will re-emit it along the 4 edges to its four neighbours at the next time step. A node that did not receive a signal at the previous timestep will not emit a signal irrespective of whether it earlier received a signal. There is a $50\%$ chance that a signal will be lost when travelling along a single edge between two neighbouring nodes.
A node that receives more then 1 signal acts the same as if it received only 1.
The emitting of a signal in each of the 4 directions are independent events. What is the probability that the signal will sometime arrive at $(10^5,10^5)$? Research: Simulations show: Yes. ~90% What is the probability that the signal will sometime arrive at $(x,y)$ if a signal traveling along an edge dies with probability $0<p<1$? What is the least p, for which the probability that N initial random live cells die out approaches 0, as N approaches infinity? Experiment shows p close to 0.2872 . In $\mathbb{Z}^1$, $p_{min}=0.6445...$, how to calculate this? In $\mathbb{Z}^3$, $p_{min}=0.1775...$, how to calculate this?","['stochastic-processes', 'network-flow', 'graph-theory', 'probability-distributions', 'probability']"
111714,The dual of a Fréchet space.,"Let $\mathcal{F}$ be a Fréchet space (locally convex, Hausdorff, metrizable, with a family of seminorms $\{\|~\|_n\}$). I've read that the dual $\mathcal{F}^*$ is never a Fréchet space, unless $\mathcal{F}$ is actually a Banach space. I'd like to know in which ways the dual can fail to be Fréchet in general; for example, is it always incomplete as a metric space? Or is it always non-metrizable? What are the real issues here? If there's a relatively easy proof of this fact (that $\mathcal{F}^*$ is not Fréchet unless $\mathcal{F}$ is Banach), I would appreciate it as well. Thanks. [EDIT: The reference cited by Dirk contains a Theorem whose proof is inaccessible to me, so I'd upvote/accept an answer that at least sketches such a proof, or provides another way of settling the question. I'd also be interested in any further explanations regarding the statement that "" (...) a LCTVS cannot be a (non-trivial) projective limit and an inductive limit of countably infinite families of Banach spaces at the same time. "", made by Andrew Stacey in that link, which is related to my question.]","['topological-vector-spaces', 'locally-convex-spaces', 'functional-analysis']"
111721,Weak convergence of a triangular array of Bernoulli-RV's,"I am looking at the series $$X_{1,1},$$$$X_{2,1}, X_{2,2}$$ $$X_{3,1},X_{3,2},X_{3,3}$$ $$\dots$$ of independent r.v's with $p_n:=P(X_{n,k}=1)=n^{-\frac{1}{4}}$ and  $q_n:=P(X_{n,k}=0)=1-n^{-\frac{1}{4}}$. So they are Bernoulli-distributed. I would like to know if$$S_n:=\frac{\sum_{k\leq n}(X_{n,k}-E(X_{n,k}))}{Var(\sum_{k\leq n}X_{n,k}) }$$ converges weakly, for $n\rightarrow \infty$. One can observe that for every $n$ the sums  $\sum_{k\leq n}X_{n,k} (=:Y_n)$ are $B(n,p_n)$ distributed. One gets $E(X_{n,k})=p_n$, $E(Y_n)=np_n$, $Var(Y_n)=np_nq_n$. So it is $$S_n=\frac{Y_n-np_n}{np_nq_n }$$. The standard CLT can't be applied because the $Y_n$ have different winning-probabilities $p_n$. Also $Y_n$ does not converge to a Poisson-distributed r.v. because $$np_n=n^{\frac{3}{4}}$$ is not constant. In which way can I apply the CLT?","['probability-theory', 'probability-distributions']"
111727,Smallest angle among two lines in $n \times n$ grid,"Given an $n \times n$ grid, what is the minimum angle between any two distinct lines, each going through some grid point $p$ and at least one other grid point? My guess is the minimum is attained by the line through $(0,0), (n,1)$ and the line through $(0,0), (n,0)$ but how can I show that this is the optimal line pair?","['geometry', 'integer-lattices']"
111730,relation between p-sylow subgroups,"let $G$ be a group of order divisible by $p,q$.
for any $p$-sylow subgroup $P$ and $q$-sylow subgroup $Q$.
I know the intersection $P\cap Q = \{e\}$ because the orders are different.
what about two $p$-sylow subgroups, can I say their intersection is only $e$? thanks.
benny.",['group-theory']
111740,Support of a Coherent Sheaf and Noetherianity,"Exercise 5.6 b) of Chapter II of Hartshorne's Algebraic Geometry asks to prove that if $A$ is a Noetherian ring and $M$ a finitely generated $A$-module then $Supp(\tilde{M})=V(Ann(M))$. Where $\tilde{M}$ is the sheaf on the scheme $Spec(A)$ associated to the module $M$. I have ""proved"" the statement without using the Noetherianity condition on $A$ (or without noticing I'm using it!). So I guess I'm missing something. I would like some help in understanding where my error lies! Here follows my ""proof"" of the statement: First inclusion $V(Ann(M))\subseteq Supp(\tilde{M})$ Let $p\in V(Ann(M))$, so $Ann(M)\subseteq p$. By contradictions I suppose $M_p=0$, in specific for each $m \in M$ there exists $a\in A - p$ such that $am=0$. Let $\{m_i\}$ be a finite set of generators for $M$ and $\{a_i\} \subseteq A$ such that $a_im_i=0$ for each $i$.
On one hand $\prod a_i\in A-p$ because $p$ is a prime ideal, but on the other hand $(\prod a_i)m_j=0$ for each $j$, so $\prod a_i\in \bigcap Ann(m_i) = Ann(M) \subseteq p$. Contradiction. Second inclusion $Supp(\tilde{M}) \subseteq V(Ann(M))$ Let $p \in Supp(\tilde{M})$, then $M_p\neq 0$ and there exists $m\in M$ such that it does not exist $a \in A - p$ such that $am=0$. Then $Ann(M)\bigcap (A-p)=\emptyset$ so $Ann(M)\subseteq p$, in specific $p\in V(Ann(M))$. So, what's wrong? Thank you for your help!","['commutative-algebra', 'algebraic-geometry']"
111743,About the $p$ summable sequences,How to prove that $\ell^p \setminus \cup_{1\leq q <p}\ell^q$ is not empty? Here $\ell^p=L^p(\mathbb{N})$.,"['functional-analysis', 'real-analysis']"
111744,Solve $\lim_{x \rightarrow 0} (a^x + b^x - c^x)^\frac{1}{x}$,"I need to solve the limit
$$\lim_{x \rightarrow 0} (a^x + b^x - c^x)^\frac{1}{x}$$
when $a,b,c \gt 0$.
I'm looking for ways to avoid $\frac{1}{x}$ power.","['calculus', 'limits']"
