question_id,title,body,tags
2879957,In Search of a map $\phi:\mathbf{C}\to\mathbf{C}$ satisfying additivity.,"I am trying without much luck to come up with an example of a function $\phi:\mathbf{C}\to\mathbf{C}$ such that $\phi(x+y) = \phi(x)+\phi(y),\forall x,y\in\mathbf{C}$ and yet for some $a,\alpha\in\mathbf{C}$ we have $\phi(a\alpha)\neq a\phi(\alpha)$. Any hints on how to get me started?",['linear-algebra']
2880002,Analytic continuation of several complex variables,"Let $f(w_1,\ldots,w_n;z)$ be a holomorphic function of $n+1$ variables.
For every fixed $w_1\ldots w_n$, let $g(w_1,\ldots,w_n;z)$ be an analytic continuation of $f$ as a holomorphic function of $z$.
Of course, $g$ is holomorphic in $z$.
Now, is $g$ holomorphic in $w_1,\ldots, w_n,z$ as well? In the concrete, I am interested in the following situation.
Let $E(a_1,\ldots,a_n;z)$ be an ODE, where $z$ is a complex variable and $a_1,\ldots,a_n$ are complex parameters.
Assume that $E(a_1,\ldots,a_n;z)$ has a local solution $u(a_1,\ldots,a_n;z)$ which is holomorphic in $a_1,\ldots,a_n,z$, and  $v(a_1,\ldots,a_n;z)$ is an analytic continuation of $u$.
Is $v$ holomorphic in $a_1,\ldots,a_n,z$?","['complex-analysis', 'several-complex-variables', 'analytic-continuation', 'ordinary-differential-equations']"
2880025,Why is 3 a bad constant in the Vitali covering lemma (infinite version)?,The Hardy-Littlewood maximal function inequality can be proved using the Vitali covering lemma (infinte version) with constant k=5. Actually it can be shown that the  constant in the lemma just needs to be k>3. But I cannot find a counter example why 3 is not enough for the infinite case (it is enough in the finite version of the lemma). Has anyone seen such a counter example?,"['measure-theory', 'geometric-measure-theory', 'real-analysis']"
2880029,"Prove that if $\langle df_x(v),v\rangle>0$, then $f$ is injective","Suppose $f:\mathbb R^n\to \mathbb R^n$ is a $\mathcal C^1$ map such that $\langle df_x(v),v\rangle>0$ for all $x\in \mathbb R^n$ and $v\in R^n\setminus \{0\}$. Prove that $f$ is injective. Hint: For $x\ne 0, $ consider $g:\mathbb R\to \mathbb R^n$ given by $g(t)=f(tx)$. Find $g'(t)$ and show that $f(x)\ne f(0)$. Let $x=(x_1,\dots,x_n)$. To compute $g'(t)=dg_t$, note that $g$ is the composition of $h:\mathbb R\to \mathbb R^n$ given by $t\mapsto tx$ and $f$. By the chain rule, $dg_t=df_{tx}\cdot dh_t$ as matrices. Now $dh_t=[x_1,\dots,x_n]^t,df_{tx}=[D_if_j]$, where $f=(f_1,\dots,f_n)$, so $$dg_t=[D_1f_1(tx)x_1+\dots+D_1f_n(tx)x_n,\dots, D_nf_1(tx)x_1+\dots +D_nf_n(tx)x_n]^t.$$ Injectivity means that $g(0)=f(x)\ne 0$ if $x\ne 0$.  What does it have to do with injectivity? How to use the given inequality?","['multivariable-calculus', 'calculus', 'derivatives', 'real-analysis']"
2880037,If $p_A(x) = x^4 (x+3)^2 (x-4)$ then $A$ is diagonalizable iff $\operatorname{Rank}(A) + \operatorname{Rank}(-3I-A) = 8$,"Given the Characteristic polynomial of a matrix $A$ is
  $$
  p(x) = x^4 (x+3)^2 (x-4),
$$
  show that $A$ is diagonalizable if and only if
  $$
  \operatorname{Rank}(A) + \operatorname{Rank}(-3I-A) = 8.
$$ Given: $A$ is diagonalizable Prove: $\operatorname{Rank}(A) + \operatorname{Rank}(-3I-A) = 8$ (Help is needed in the other way around)
The geometric multiplicity equals the algebraic multiplicity, hence the diagonal matrix $D$ will look like
$$
    D
  = \begin{pmatrix}
      -3  &     &   &   &   &   &   \\
          & -3  &   &   &   &   &   \\
          &     & 4 &   &   &   &   \\
          &     &   & 0 &   &   &   \\
          &     &   &   & 0 &   &   \\
          &     &   &   &   & 0 &   \\
          &     &   &   &   &   & 0 
    \end{pmatrix}
$$
Since $D$ and $A$ are similar matrices, their rank must be the same:
$$
 \operatorname{Rank}(A) = \operatorname{Rank}(D) = 3.
$$
Also
$$
    -3I - A
  = \begin{pmatrix}
      0 &   &     &   &     &     &     \\
        & 0 &     &   &     &     &     \\
        &   & -7  &   &     &     &     \\
        &   &     & 0 &     &     &     \\
        &   &     &   & -3  &     &     \\
        &   &     &   &     & -3  &     \\
        &   &     &   &     &     & -3 
    \end{pmatrix}
$$
and therefore
$$
    \operatorname{Rank}(A) + \operatorname{Rank}(-3I-A)
  = 3 + 5
  = 8.
$$
Perfect.
(Is it okay to place $D$ instead of $A$ in $\operatorname{Rank}(-3I-A)$? The rank will be the same, is it not?) Given: $\operatorname{Rank}(A) + \operatorname{Rank}(-3I-A) = 8$ Prove: $A$ is diagonalizable No clue really…
I was thinking since geometric multiplicity is less or equal to algebraic multiplicity, I was thinking of finding all possible $D$'s and show that the one that makes the statement $\operatorname{Rank}(A) + \operatorname{Rank}(-3I-A) = 8$ hold makes $A$ diagonal. Any hint is appreciated.","['matrices', 'diagonalization', 'linear-algebra']"
2880075,"How to prove the real value function $f$ is convex on $[a,d]$ if $f$ is convex on both $[a,c]$ and $[b,d]$? ($a<b<c<d$).","How to prove the real value function $f$ is convex on $[a,d]$ if $f$ is convex on both $[a,c]$ and $[b,d]$? ($a<b<c<d$). I try to show that: for arbitrary $x\in(a,b), z\in(c,d)$, and for all $y\in (x,z)$ the inequality $\dfrac{f(y)-f(x)}{y-x}\leqslant \dfrac{f(z)-f(y)}{z-y}$ holds. Then $f$ is convex on $[a,d]$. But it seems to need some inequality tricks, I can't finish it.",['calculus']
2880140,Partial derivative of $e^{x^2 + 2y^2 + 3z^2}$,"Find $f_y$ of $e^{x^2 + 2y^2 + 3z^2}$ How do I do this ? I am trying to follow the formula of - $\frac{d}{dx} e^{ax +b} = ae^{ax+b} $ Since I am differentiating the independent variable of y, Why isn’t the partial derivative $ (f_y)$ of $e^{x^2 + 2y^2 + 3z^2} = 2e^{x^2 + 2y^2 + 3z^2} \frac{\partial}{\partial y} (x^2 + 2y^2 + 3z^2) $ ? In fact the workings is - $e^{x^2 + 2y^2 + 3z^2} \frac{\partial}{\partial y} (x^2 + 2y^2 + 3z^2) $ Why isn’t the ‘2’ infront if $e$ ?","['partial-derivative', 'calculus', 'derivatives']"
2880175,"Find $T\left(\frac{\partial}{\partial x^k}, dx^l\right)$ for a tensor of type ${1}\choose{1}$ on $T_p(\mathbb{R}^n)$","So say I have the tensor of type ${1}\choose{1}$, with $T \in T_p(\mathbb{R}^n) \otimes T_p^*(\mathbb{R}^n)$ where $$T = T^{a}_{b} \frac{\partial}{\partial x^a} \otimes dx^b$$ summed over all $a, b$. Problem: Find $T\left(\frac{\partial}{\partial x^k}, dx^l\right)$ Wouldn't $T$ take as an input (when identified with it's corresponding multilinear map) in its first argument a one-form and in it's second argument a tangent vector? Said more concretely since we have $$T_p(\mathbb{R}^n) \otimes T_p^*(\mathbb{R}^n) \cong L\left(T_p^*(\mathbb{R}^n) \ ,\  T_p(\mathbb{R}^n); \mathbb{R}\right)$$ we can identify $T$ with a multilinear map $$T(\omega, X_p) = \left(T^{a}_b  \frac{\partial}{\partial x^a} \otimes dx^b\right)(\omega, X_p) = T^a_b \left(\frac{\partial}{\partial x^a}(\omega) \cdot {dx}^b(X_p)\right)$$ (Also note that strictly $\frac{\partial}{\partial x^a}$ doesn't take as inputs one-forms but we can identify it with an element of the bidual $T_p^{**}(\mathbb{R}^n)$ which does so we can do this without harm. But the problem states that we need to find $T\left(\frac{\partial}{\partial x^k}, dx^l\right)$ , shouldn't it instead be to find $T\left(dx^l, \frac{\partial}{\partial x^k}\right)$?","['tensors', 'tensor-products', 'differential-geometry']"
2880190,Clarification of Qualitative Behaviour of BVP Solutions Example,"I have the following example from my PDE textbook ( Essential Partial Differential Equations by Griffiths, Dold, and Silvester): I don't understand how the authors found the solution $u(x)$. The exercise (5.2) they reference is as follows: Verify that (5.6) and (5.8) are solutions to the BVP in Example 5.1. But how does verifying this find us $u(x)$? Why does $|u(\frac{1}{2})| \to \infty$ as $b \to -(2n - 1)^2 \pi^2$ indicate that the BVP is not well posed for these values of $b$? For the graph (figure 5.1), in a), the authors state that, when $\epsilon > 0$, the solution is negative throughout the interval $0 < x < 1$. But we can see from the graph (figure 5.1) that, when $\epsilon = 1 > 0$, the solution is not always negative throughout the interval $0 < x < 1$. Specifically, the solution becomes positive for $b = 100$ and $b = -100$. I would greatly appreciate it if someone could please take the time to clarify the author's work here.","['boundary-value-problem', 'ordinary-differential-equations', 'partial-differential-equations']"
2880207,Choosing a boundary for integration,"I have the following differential equation 
$$
\frac{d}{dx}\left(\mu e^{cx}f(x)\right) = -\mu\left(\frac{a xe^{-cx}}{a x+x-1}\right)
$$
that I am trying to integrate to find $f(x)$ with the boundary constraint that $f(1) =1$. The integrating factor $\mu$ is given by 
$$
\mu = e^{\frac{a c}{1+a}x}\left((a+1)x-1\right)^{\frac{1+a+ac}{(1+a)^2}}
$$
If we integrate this with a lower limit of $x$ then 
$$
\mu e^{ct}f(t)\big|_x = -\int_x  \mu\left(\frac{a te^{-ct}}{a t+t-1}\right)dt
$$
However, what would be a sensible upper limit so that I can find $f(x) = ?$. The integrating factor has a zero at $x=1/(a+1)$. Would it make sense to integrate up to this boundary?","['integration', 'boundary-value-problem', 'ordinary-differential-equations']"
2880213,Isoperimetric inequality for non-spherical multivariate Gaussian,"Disclaimer: Sorry in advance, if the question is not very reasonable.  Recently (like a few days ago...), I've started studying isoperimetric inequalities, and my thoughts on the subject are rather cloudy with lots of black holes. Setup So, the classical Gaussian Isoperimetric inequality states that [GIPI] if $d\gamma_d(x):= (2\pi)^{-d/2}\exp(-\|x\|_2^2/2)dx$ is the Gaussian measure on $\mathbb R^d$ and $B$ is a Borel subset of $\mathbb R^d$, then
  $$
\gamma_d(B_\epsilon) \ge \Phi(\Phi^{-1}(\gamma_d(B)) + \epsilon),\;\forall \epsilon > 0
$$ where: $B_\epsilon := \{x \in \mathbb R^d | \|x-A\|_2^2 \le \epsilon\}$ is $\epsilon$-neighborhood/blowup of $B$ the function $\Phi(z): = \int_{-\infty}^zd\gamma_1(z)$ defines the CDF of the 1D standard Gaussian and $\Phi^{-1}$ is its inverse. A corollary to this inequality (see Otto et Villani 2000, for example) says that [GIPI Corollary] if $\epsilon \ge \sqrt{2\log(1/\gamma_d(B))}$, then
  $$
\gamma_d(B_\epsilon) \ge 1 - \exp(-\frac{1}{2}(\epsilon-\sqrt{2\log(1/\gamma_d(B))})^2).
$$ I'm interested in ""simple"" generalizations of [GIPI] and [GIPI Corollary] . Question 1 I know there are works (e.g Otto et Villani 2000, Bobkov 1999, etc.) which generalize this to more general log-concave distributions $d\mu = e^{-V}dx$ (coupled with even more general curved manifolds with lower bounded Ricci curvature), but it's hard to get one's hand on precise formulae (e.g involving the modulus of strong-convexity of the potential $V$, etc.). As a concrete example, suppose $V$ is twice continuously differentiable with $\operatorname{Hess}(V) \succeq C \operatorname{I}_d$,
what analogues of [GIPI] and [GIPI Corollary] do we get ? Question 2 In particular, what analogues of [GIPI] and [GIPI Corollary] do we get when we instead consider an non-spherical Gaussian distribution $d\gamma_{\mu,\Sigma} := (2\pi\operatorname{det}(\Sigma))^{-d/2}\exp(-\frac{1}{2}x^T\Sigma^{-1}x)$, with mean $\mu$ and covariance matrix $\Sigma$ (positive definite) ? Observation It turns out that explicit answers to both questions can be derived from Corollary 3.2 Bobkov et al. For example, we have a multi-variate Gaussian version of [GIPI Corollary] by simply replacing the 2 appearing in the square-root and the fraction (i.e $\frac{1}{2}$) with $2$ $\times$ the smallest eigenvalue of covariance matrix $\Sigma$. I can writeup a self-contained answer, but I don't know if this is a good idea since I asked the question in the first place.","['concentration-of-measure', 'reference-request', 'upper-lower-bounds', 'probability-theory', 'differential-geometry']"
2880235,Irreducible 2-Brauer characters of $S_5$,"Beginning with the ordinary character table of the symmetric group $S_5$, one immediately gets the following Brauer characters in characteristic two: $\begin{array}{c|c|c|c}
S_5 & ()  & (12345) & (123) \\
\hline
\beta_1 & 1 & 1 & 1\\
\beta_2 & 4 & -1 & 1 \\
\beta_3 & 5 &  0 & -1 \\
\end{array}$ It is easy to see that both $\beta_1$ and $\beta_2$ are irreducible. Moreover, the third (and last) irreducible 2-Brauer character must be given either by $\beta_3$ or by $\beta_3 - \beta_1$. Indeed, by a computer search, I found a representation $S_5 \to \mathrm{GL}(4,2)$ affording the Brauer character $\beta_3 - \beta_1$. So $\beta_3$ is not irreducible. Is it possible to come to the same conclusion by character theoretic arguments? I actually found a purely character theoretic solution to my question, which requires some work though. I am still very interested in seeing other solutions! Starting with the character table of the alternating group $A_5$ it is possible to derive the table of its irreducible 2-Brauer characters in a straightforward way (this is carried out here ). $\begin{array}{c|c|c|c|c}
A_5 & ()  & (12345) & (13524) & (123) \\
\hline
\gamma_1 & 1 & 1 & 1 & 1\\
\gamma_2 & 2 & \varphi-1 & \psi -1 & -1 \\
\gamma_3 & 2 & \psi-1 & \varphi-1 & -1\\
\gamma_4 & 4 & -1 & -1 & 1\\
\end{array}$ ($\varphi$ is the golden ratio, and $\psi$ is its algebraic conjugate) At this point, it is easy to see that $\beta_3-\beta_1 = (\gamma_2)^{S_5}$ is induced by a Brauer character of $A_5$, and so is a Brauer character as well.","['characters', 'representation-theory', 'finite-groups', 'abstract-algebra', 'group-theory']"
2880274,Gamma distribution family and sufficient statistic,"Let X $=X_1,...X_n$ be a random sample iid from the probability density function: $$ f(x;\theta)=\frac{\Gamma(\theta)\sin(\pi\theta)\theta^{1-\theta}}{\pi}e^{-\theta x}x^{-\theta}$$ $x>0, 0<\theta < 1$ Is the distribution a member of the exponential family of probability
  distributions? we know that the exponential family member distribution have the following form: $$ f_x(x;\theta)=c(\theta)g(x)\exp \Big\{\sum_{j=1}^{l}Q_j(\theta)T_j(x) \Big\}$$ SOLUTION : $c(\theta)=\frac{\Gamma(\theta)\sin(\pi\theta)\theta^{1-\theta}}{\pi}$ $h(x)=1$ $\nu(\theta)=-\theta$ $T(x)=x + \log(x)$ However i am not sure how these were obtained ? I am aware that  $e^{-\theta x}x^{-\theta} = e^{-\theta(x+\log(x))}$. Then $T(x)$ contains all expression containing x and $c(\theta)$ everything containing $\theta$. What is the purpose of $h(x)$ and $\nu(\theta)?$ I am also interested in finding the sufficient statistic for parameter $\theta$. Now the joint density of $X_1,...,X_n$ is $$ f(x;\theta)= c(\theta)^n e^{-\theta \sum (x_1 + \log x_i)}$$ which comes from the fisher factorization. Therefore $\sum (x_1 + \log x_i)$ is sufficient statistic for $\theta$.","['statistics', 'parameter-estimation', 'exponential-distribution']"
2880279,Is modus tollens applicable when a premise contains additional information,"I have something similar to the below: \begin{align}
&1. \ p \to q \\
&2. \ \lnot q \land \lnot s \\ 
&\text{c} \colon \, \lnot p
\end{align} Is this still equivalent to modus tollens since the 2nd premise can only be true if $\lnot q$ is true, and will be false if not? I am not really sure how to treat the $s$ variable seeing as it is not included in the conclusion.","['propositional-calculus', 'logic', 'discrete-mathematics']"
2880283,"Given a projection $p$, find the largest subset $G(p) \subset Map(S,S)$ that is a group under composition","I want to show, that given a set $S$ and a projection $p: S \rightarrow S$ (i. e. $p = p \circ p$), there exists a largest subset $G(p) \subset Map(S, S)$, which contains $p$ and is closed under composition and $(G(p), \circ)$ is a group. I have no idea, how this $G(p)$ looks like - $p$ is of course the identity, but how to define $G(p)$? Any idea?","['group-theory', 'abstract-algebra']"
2880361,Why are nonzero eigenvalues of a compact operator poles of its resolvent?,"Let $X$ be a complex Banach space and $T$ a compact operator on $X$. I read on Wikipedia that nonzero elements of the spectrum of $T$ are poles of its resolvent. It says ""by functional calculus"", but I don't see how. I'm looking for a reference or a sketch of the proof. At the moment I have no idea how it goes. By reading Yosida's ""Functional Analysis"" I know that an eigenvalue is a pole when its residue has finite rank (VIII.8, Theorem 4). But I don't see why / know if residues have finite rank when $T$ is compact. I can show that those isolated singularities are poles when $T$ is normal and trace class, using the spectral theorem and by investigating its Laurent-coefficients. But I don't want to assume that $T$ is normal.","['compact-operators', 'operator-theory', 'meromorphic-functions', 'functional-analysis', 'spectral-theory']"
2880377,Find the limit of $x(x + 1 - \sin(\frac{1}{1+x})^{-1})$ as $x \rightarrow \infty$,"As the title states, I need to find the limit for $x\left(x + 1 - \frac{1}{\sin(\frac{1}{1+x})}\right)$ as $x \rightarrow \infty$, as part of a larger proof I am working on. I believe the answer is 0. I think that to start, I can show that $\frac{1}{\sin(\frac{1}{1+x})} \rightarrow x + 1$ for large x. By looking at the series expansion for Sin, it's clear that Sin approximates to $\frac{1}{1+x}$ for large x, as the higher-power entries in the series $\frac{1}{1+x}^3 + \frac{1}{1+x}^5 + ...$ would disappear faster, but would it be sufficient to state this? Is there not a more rigorous way of showing this to be true? If my approach is entirely wrong, or there is a more elegant way of reaching the answer, please share.","['analysis', 'real-analysis', 'functions', 'taylor-expansion', 'limits']"
2880379,"The solutions of $(y')^2=P(y)$ with $\deg P \in \{3,4\}$.","Here is some background on my question. The Weierstraß function (as well as its translates) $\wp(x)$ solves the implicit first-order ODE
$$(y')^2=4y^3-g_2y-g_3.$$
Differentiating gives the second-order explicit ODE
$$y''=12y^2-g_2, $$
in which $g_3$ makes no appearance. $\wp$ can be used to express the solution of any first-order equation of the form
$$(y')^2=a_3 y^3+a_2 y^2+a_1 y+a_0, $$
or equivalently, any second-order equation of the form
$$y''=b_2 y^2+b_1 y+b_0. $$
In both cases, all it takes is a simple change of variables to remove one of the powers, and get the leading coefficient right. My question is about differential equations whose RHS is one degree larger, that is
$$(y')^2=a_4 y^4+a_3y^3+a_2y^2+a_1 y+a_0 , \tag{1}$$
or
$$y''=b_3 y^3+b_2y^2+b_1 y+b_0. \tag{2} $$ In some special cases, such as
$$y''=2k^2 y^3-(1+k^2) y, $$
and 
$$y''=-2y^3+(2-k^2)y,$$
the Jacobi elliptic functions $\operatorname{sn},\operatorname{dn}$ play a role in the solution. Here is my question: For which values of the coefficients in $(1)$ and $(2)$ can the ODEs be solved explicitly in terms of special functions? What are the solutions in those cases? Thank you!","['elliptic-functions', 'closed-form', 'special-functions', 'ordinary-differential-equations']"
2880413,"Solve: $\cos x\, dy=y(\sin x-y)\,dx$","Solve: $\cos x \,dy=y(\sin x-y)\,dx$ I have done the solution the following way. Is my answer correct or wrong? The answer given in the book: $\sec x =y(\tan x +c)$",['ordinary-differential-equations']
2880420,"How to evaluate $\int_S(x^4+y^4+z^4) \, dS$ over surface of the unit sphere.","Question. Let $S$ denote the unit sphere in $\mathbb{R}^3$. Evaluate: $$\int_S (x^4+y^4+z^4) \, dS$$ My Solution. First I parametrize $S$ by $$r(u,v)=(\cos v \cos u, \cos v \sin u, \sin v)$$ $0\le u \le 2 \pi;~-\frac{\pi}{2}\le v \le \frac{\pi}{2}$ Let $~f(x,y,z)=x^4+y^4+z^4$. Here $~|\frac{\partial r}{\partial u} \times \frac{\partial r}{\partial v}|=|\cos v|$ Then $\displaystyle \int_S(x^4+y^4+z^4)\,dS = \int_{-\pi/2}^{\pi/2} \int_0^{2\pi} f[r(u,v)] \left|\frac{\partial r}{\partial u} \times \frac{\partial r}{\partial v}\right|~du~dv$ Thus I try to calculate this integral directly using the definition of surface integral. But I had so much calculations in this way. Does this particular problem can be solved using any theorems e.g-Gauss' Divergence (By writing $f$ as $F\cdot n$ for some vector field $F$?
Thank you.","['multivariable-calculus', 'surface-integrals']"
2880425,Solve $(3x^2y^4 +2xy)dx + (2x^3y^3 - x^2)dy=0$,Solve $(3x^2y^4 +2xy)dx + (2x^3y^3 - x^2)dy$ This is not one of the standard forms and neither is it an exact form. How do I go about doing this question?,['ordinary-differential-equations']
2880444,Affine algebraic varieties associated with arbitrary sets,"I was pondering on affine varieties and ideals. And a curious (and possibly irrelevant) question came to mind, when we talk about for a variety $V\subset \mathbb A^n$, $I(V) =\{ f\in \mathbb C[x_1,...,x_n] \mid f(x) =0 \ \forall x\in V\} $. Now this definition isn't restricted to varieties alone. We can consider $V$ to be an arbitrary set, just like how $V(I)$ can actually be defined on a set of polynomials, other than ideals. So my question is, for an arbitrary set $S\subset \mathbb A^n$, what is the relation between $S$ and $V(I(S))$? A similar question about coordinate ring can be asked. For an arbitrary set $S$, by considering the restriction of polynomials to $S$, we can see that $\mathbb C[S] \cong \mathbb C[x_1,...,x_n]/I(S)$. Would it make sense to talk about Hilbert's basis theorem /Nullstellensatz on this ""coordinate ring""? I suspect that these questions are irrelevant because we don't really care about arbitrary sets as they aren't as geometric. But I'm curious about it. Thanks in advance.","['affine-varieties', 'algebraic-geometry', 'ideals']"
2880477,How can I prove that $f:A_{n+1}\times \prod_{i\in I_n}A_i\to\prod_{i\in I_{n+1}}A_i$ is bijective?,"The cartesian product of a family $(A_i\mid i\in I)$ is defined as $$\prod_{i\in I}A_i=\{f:I\to\bigcup A_i\mid f(i)\in A_i\}$$ I'm trying to prove the below theorem. While I'm able to construct a desired mapping and intuitively found that it is a bijection, I'm unable to prove that this mapping is actually bijective in a formal proof. I also found that it's easier to use the definition of cartesian product as n-array, but I would like to use the definition of cartesian product as a function. Let $I_{n+1} = \{i\in\mathbb N\mid 1\leq i\leq n+1\}$ and $(A_i\mid i\in I_{n+1})$ be a family of finite sets. Then $|A_{n+1}\times\prod_{i\in I_n}A_i|=|\prod_{i\in I_{n+1}}A_i|$ where $|X|$ is the cardinality of $X$. First, we generate a family of indexed sets $(B_i\mid i\in I_2)$ as follows: $B_1=A_{n+1}$ and $B_2=\prod_{i\in I_n}A_i$. Then we construct a mapping $f:B_1\times B_2\to\prod_{i\in I_{n+1}}A_i$ such that $\{(0,b_1),(1,b_2)\}\mapsto \{(n+1,b_1)\cup b_2\}$ where $b_1\in B_1=A_{n+1},b_2\in B_2=\prod_{i\in I_n}A_i$. How can I proceed to prove that $f$ is bijective?",['elementary-set-theory']
2880478,A question on equivalence of Sobolev norm,"Define $\|.\|_{T^k(\Omega)}$ as  $$ \|f\|_{T^k(\Omega)} =     \|f\|_{L^2(\Omega)} + \|(\sum\limits_{i=1}^d(\frac{\partial^{k}f}{\partial x_i^{k}})^2)^{\frac{1}{2}}\|_{L^2(\Omega)} $$ Is this norm equivalent to Sobolev norm, $\|.\|_{W^{k,2}(\Omega)}$, under two cases $\Omega$ is a bounded open subset of $\mathbb{R}^d$ $\Omega = \mathbb{R}^d$ PS : I read that it is true, for the case $d = 1$, but I have not able to find anything for $d\ge2$","['sobolev-spaces', 'functional-analysis', 'real-analysis']"
2880490,"Virtual vector bundles, Grothendieck group, and a pair of bundles","I hope to understand the following expression better: $$w_3(V_{SO(3)}\otimes (TM-5)) = w_1^3(TM)+  w_3(V_{SO(3)}).$$ Here $V_{SO(3)}$ is the vector bundle of $SO(3)$. The $TM$ means the tangent bundle of a manifold $M$. The $TM-5$ means the construction of the tangent bundle and the virtual vector bundle. The $V_1-V_2+V_3$ is an example of an operation on the vector bundles $V_1,V_2,V_3$ in the Grothendieck group. One way to define it, is as the isomorphism classes of ``virtual'' vector bundles, that is a pair of bundles $(V,V')$ modulo relation $(V\oplus W,V'\oplus W)\sim (V,V')$. The operation $+$ then descends from $\oplus$ so that $-(V,V')=(V',V)$. Then $V_i\equiv (V_i,0)$. Question 1: Under what constraints of vector bundles, we can obtain the $w_3(V_{SO(3)}\otimes (TM-5))$ as the $w_1^3(TM)$ $+$ $w_3(V_{SO(3)})$? Question 2: The $TM-5$ is a $0$-dimensional virtual vector bundle? Question 3: How do we interpret the operations: (1) the $\otimes$, (2) the $+$, (3) the $\oplus$, (4) the $-$, operations, precisely as the above? Any useful references are welcome.","['algebraic-topology', 'vector-bundles', 'differential-topology', 'characteristic-classes', 'differential-geometry']"
2880492,"Number theory, possibly mathematical induction to use","For $n, r \in \mathbb{N}$ denote $S_r(n)$ sum $1^r + 2^r + ... + n^r$. Verify that for all $n, r$ :
$$(n+1)^{r+1} -(n+1) = \binom{r+1}{1}S_r(n) + \binom{r+1}{2}S_{r-1}(n)+ \cdots +\binom{r+1}{r}S_1(n) $$
To prove this for r, I fixed n and tried induction on r. However my calculations get messy and I can't find a way to do this. Also I have used Pascal's identity for each binomial which resulted in having $(n+1)^{r+2} - (n+1)$ LHS and $(n+1)^{r+1} -(n+1) $ RHS with some coefficients and constants. I'm asking for a clue. I think this task is related to mathematical induction as it is in the same module.","['number-theory', 'induction', 'elementary-number-theory']"
2880505,Applications of Sard's theorem?,"I'm writting an essay for University about differential topology. In particular, I'm studying the applications of Sard's theorem on differential topology. I have included the proof of Whitney's embedding theorem and some results about transversality (stability and genericity), but I would like to study one more application. Could anyone give me any suggestion? Thank you","['measure-theory', 'differential-topology', 'smooth-manifolds']"
2880518,Product of odd evil numbers over the product of odd odious numbers,"im working on a problem where im confronted with this fraction $\frac{3 \cdot 5\cdot9\cdot15\cdot17\cdot23\cdot27\cdot29 ...}{1\cdot7\cdot11\cdot13\cdot19\cdot21\cdot25\cdot31 ...}$ . 
I found that the product in the numerator is the product over all odd evil numbers and the product in the denominator is the product over all odd odious numbers. Im especially interested in the sequence $\frac{3}{1}=3$ , $\frac{3\cdot5}{1\cdot7}=\frac{15}{7}$ , $\frac{3\cdot5\cdot9\cdot15}{1\cdot7\cdot11\cdot13}=\frac{2025}{1001}$ , ... where $2^n$ factors are put together. Its easy to see that this series converges monotonically decreasing to 2 but i have no idea how to proof that. Can somebody help me ? Note: an odious number is a nonnegative integer which has an odd number of $1$s in its binary expansion. An evil number is one which has an even number of $1$s. Edit: 
Because some people are a bit suspicious about odious and evil numbers i want to give an alternative formulation of my problem. Consider the following recursive function: $f(n)=(-1)\cdot f(l)$ where $n=2^k+l$ and $l>0$ is the unique representation of n as the maximal power of 2 with a remainder $0\le l \lt 2^k$  and $f(1)=(-1)^k$ where $n=2^k$ which is defined on non negative odd integers. Im looking for the value of the product $\lim_{N->\infty}\prod_{n=1}^{2^N}{(2\cdot n-1)^{-f(n)}}$ . 
This is the way i came to this fraction given above. Edit2: I think i found another representation without a proof that these are equal $\frac{3 \cdot 5\cdot9\cdot15\cdot17\cdot23\cdot27\cdot29 ...}{1\cdot7\cdot11\cdot13\cdot19\cdot21\cdot25\cdot31 ...}=(1+\frac{2}{1})\cdot(1-\frac{2}{7})\cdot(1-\frac{2}{11})\cdot(1+\frac{2}{13})\cdot(1-\frac{2}{19})\cdot(1+\frac{2}{21})\cdot(1+\frac{2}{25})\cdot(1-\frac{2}{31})...$ It seems to me that the rule of building this product is $\prod_{n=1}^{\infty}{(1+\frac{(-1)^{m_n}\cdot2}{o_n})}$ where $m_n$ is the n-th element of the Thue–Morse sequence and $o_n$ is the n-th element of the odd odious number sequence. Maybe this looks familiar to somebody",['number-theory']
2880544,Application of chain rule for partial derivatives,"A rectangular metal block above has length y com and a square cross section of side x cm. When the metal block is heated, the area of cross-section A and the length of the metal block increase at a constant rate of $0.3 cm/s$ and $0.2 cm/s$ respectively. Find the rate in which the volume of the metal block, V is increasing when $A= 4cm^2$ and $y=5 cm$ Here’s my workings - $\frac{dV}{dt} = \frac{\partial V}{\partial A} \cdot \frac{dA}{dt} +  \frac{\partial V}{\partial L} \cdot \frac{dL}{dt} $ $ V = y \cdot x^2 $ $A = x^2$ $A= x^2 = 4$ $x=2$ $ \frac{\partial V}{\partial A} = 2xy $ $\frac{\partial V}{\partial L} = x^2 $ Therefore $\frac{dV}{dt} = 2(2)(5) \cdot (0.3) + (2)^2 \cdot (0.2) $ However, my answer is wrong and my mistake was in $ 2(2)(5) \cdot (0.3) $ Why is this so ? Thanks..","['partial-derivative', 'calculus', 'derivatives']"
2880573,"If a sum of $L$ positive integers grows like $L^d$, how does the summand grow?","Suppose that $(a_N)_{N \in \mathbb{N}}$ is a sequence of (strictly) positive integers which satisfies the following property, namely there exists $C \in (0, \infty)$ and  an integer $d \in \mathbb{N}$, $d > 1$, such that for any $L \in \mathbb{N}$,
$$
\sum\limits_{N=0}^{L} a_N \leq C \, \,  L^d.
$$
Does this imply that there exists $C^{\prime} \in (0, \infty)$ such that the following inequality is fulfilled for any $N$?
$$
a_N \leq C^{\prime} N^{d-1}
$$
Of course an obvious bound is $a_N \leq C^{\prime}N^{d}$, but it seems to me that it can be improved.","['power-series', 'limits', 'sequences-and-series', 'real-analysis']"
2880591,"Is it true that $\forall N\in\mathbb{N},\exists N<n\in\mathbb{N},P(n)\equiv\exists A\in\mathscr{P}_{\aleph_0}(\mathbb{N}),\forall n\in A,P(n)$?","I have some questions: Is the following proposition is true? And if it is true, Is my proof is correct? And another thing, I am not fully able to intuitively comprehend why (4) implies (3) , I.e. Why if for some predicate $P(n)$ it is true that $\forall A\in\mathscr{P}_{\aleph_0}(\mathbb{N}),\exists n\in A, P(n)$, Then it must be the case that $\exists N\in\mathbb{N},\forall N<n\in\mathbb{N}, P(n)$ ? In words it means that, If we know that for any infinite subset of the natural numbers we take, There exists an element in the subset for which $P(n)$ is satisfied, Then it must be the case that there is some $N$ from which all of the $n$’s that are greater than this $N$ will satisfy $P(n)$ ? Let $P:\mathbb{N}\to\{True,False\}$ be some predicate, Prove that the following two statements are logically equivalent: (1) $\forall N\in\mathbb{N},\exists N<n\in\mathbb{N}, P(n)$ (2) $\exists A\in\mathscr{P}_{\aleph_0}(\mathbb{N}),\forall n\in A, P(n)$ And use this logical equivalence to prove that the following two statements are logically equivalent too: (3) $\exists N\in\mathbb{N},\forall N<n\in\mathbb{N}, P(n)$ (4) $\forall A\in\mathscr{P}_{\aleph_0}(\mathbb{N}),\exists n\in A, P(n)$ Note: $\mathscr{P}_{\aleph_0}(\mathbb{N})$ denotes the set of all subsets of $\mathbb{N}$ that have cardinallty of $\aleph_0$, I.e. $\mathscr{P}_{\aleph_0}(\mathbb{N}):=\{A\in\mathscr{P}(\mathbb{N})| |A|=\aleph_0\}$. Proof that (1) implies (2) : Let’s define $A:=\{n\in\mathbb{N}|P(n)\}$, It is clear that $A\in\mathscr{P}(\mathbb{N})$ and that $\forall n\in A, P(n)$, Therefore it is just left for us to show that $|A|=\aleph_0$ to accomplish the proof: Suppose by contradiction that $|A|\neq \aleph_0$, Then we get that it must be the case that $A$ is finite, And thus $|A|\in\mathbb{N}\cup\{0\}$, Now there are two cases: $|A|\in\{0\}$ or $|A|\in\mathbb{N}$ If $|A|\in\{0\}$, We conclude that $|A|=0$, And thus $A=\emptyset$, Now because of (1) , We get that in particular for $N:=1$ we have $\exists N\lt n\in\mathbb{N}, P(n)$, And thus $n\in\mathbb{N}$ is natural number for which the statement $P(n)$ is satisfied, And we conclude by the way we defined the set $A$ that $n\in A$ which contradicts the fact that $A=\emptyset$. If $|A|\in\mathbb{N}$, Then $A$ is a non-empty finite set of natural numbers, And thus its maximum $\max(A)$ exists and is an element in $A$, Now let’s define $N:=\max(A)$, Then we get that $N\in A$, And thus $N\in \mathbb{N}$, And we can conclude by (1) that $\exists N<n\in\mathbb{N},P(n)$, And thus by the way we defined the set $A$, We conclude that $n\in A$, Which implies that it must be the case that $n\leq \max(A)$, But since $N=\max(A)$, We conclude that $n\leq N$, But this contradicts the fact that $N<n$. Thus we see that wether $|A|\in\{0\}$ or $|A|\in\mathbb{N}$, We always reach a contradiction, And thus it must be the case that $|A|=\aleph_0$, And we conclude that $A\in\mathscr{P}_{\aleph_0}(\mathbb{N})$ as was to be shown. Proof that (2) implies (1) : Suppose by contradiction that it is not true that $\forall N\in\mathbb{N},\exists N<n\in\mathbb{N}, P(n)$, I.e. suppose that (#) $\exists N\in\mathbb{N},\forall N<n\in\mathbb{N}, \lnot P(n)$, We’ll show that (##) $\forall n\in A, n\leq N$: Suppose by contradiction that $\exists n\in A, n>N$, Then since $A\subseteq\mathbb{N}$ (Because  $A\in\mathscr{P}_{\aleph_0}(\mathbb{N})$), We conclude that $N<n\in\mathbb{N}$, And thus by (#) we conclude that $\lnot P(n)$, I.e. $P(n)$ is not satisfied, Also, Since $n\in A$, We get by (2) that it is also the case that $P(n)$ is satisfied, Therefore we got that $P(n)$ is not satisfied and $P(n)$ is satisfied at the same time, And thus we’ve reached a contradiction and it must be the case that $\forall n\in A, n\leq N$ as was to be shown. Now let’s define $B:=\{i\in\mathbb{N}|i\leq N\}$, We’ll show that $A\subseteq B$: Let $n\in A$, Then by the fact that $A\subseteq \mathbb{N}$, We get that $n\in\mathbb{N}$, Also by (##) we get that $n\leq N$, And thus we can conclude that $n\in B$ as was to be shown. Now since $|B|=N\in\mathbb{N}$, We conclude that $B$ is finite, And since $A\subseteq B$, We conclude that it must be the case that $A$ is also finite, But this contradicts the fact that $|A|=\aleph_0$ (as  $A\in\mathscr{P}_{\aleph_0}(\mathbb{N})$)) and we’ve reached a contradiction. Therefore it must be the case that $\forall N\in\mathbb{N},\exists N<n\in\mathbb{N}, P(n)$ as was to be shown. Proof that (3) and (4) are logically equivalent: Let’s define a new predicate $Q(n)\equiv \lnot P(n)$, Then by what we’ve just shown, We conclude that: $$\forall N\in\mathbb{N},\exists N<n\in\mathbb{N}, Q(n)\equiv\exists A\in\mathscr{P}_{\aleph_0}(\mathbb{N}),\forall n\in A, Q(n)$$ But this implies that: $$\lnot\forall N\in\mathbb{N},\exists N<n\in\mathbb{N}, Q(n)\equiv\lnot\exists A\in\mathscr{P}_{\aleph_0}(\mathbb{N}),\forall n\in A, Q(n)$$ Now since: $$\lnot\forall N\in\mathbb{N},\exists N<n\in\mathbb{N}, Q(n)\equiv \\\equiv\exists N\in\mathbb{N},\forall N<n\in\mathbb{N}, \lnot Q(n)\equiv\\\equiv\exists N\in\mathbb{N},\forall N<n\in\mathbb{N}, \lnot\lnot P(n)\equiv\\\equiv\exists N\in\mathbb{N},\forall N<n\in\mathbb{N}, P(n)$$ And since: $$\lnot\exists A\in\mathscr{P}_{\aleph_0}(\mathbb{N}),\forall n\in A, Q(n)\equiv\\\equiv\forall A\in\mathscr{P}_{\aleph_0}(\mathbb{N}),\exists n\in A, \lnot Q(n)\equiv\\\equiv \forall A\in\mathscr{P}_{\aleph_0}(\mathbb{N}),\exists n\in A,\lnot\lnot P(n)\equiv\\\equiv \forall A\in\mathscr{P}_{\aleph_0}(\mathbb{N}),\exists n\in A, P(n)$$ We conclude that: $$\exists N\in\mathbb{N},\forall N<n\in\mathbb{N}, P(n)\equiv\forall A\in\mathscr{P}_{\aleph_0}(\mathbb{N}),\exists n\in A, P(n)$$ as was to be shown. Note: One example of using this proposition is in the context of limits of sequence: Let $\{a_n\}_{n=1}^\infty$ be some sequence of real numbers, Inorder to show that $\lim_\limits{n\to\infty}a_n\neq L$, We have to show that $\exists\epsilon\in (0,\infty),\forall N\in\mathbb{N}, \exists N<n\in\mathbb{N}, a_n\notin (L-\epsilon, L+\epsilon)$ (Here $P(n)\equiv a_n\notin (L-\epsilon,L+\epsilon)$) By the proposition we get that this is equivalent to show that: $\exists\epsilon\in (0,\infty),\exists A\in\mathscr{P}_{\aleph_0}(\mathbb{N}),\forall n\in A, a_n\notin (L-\epsilon,L+\epsilon)$ I.e. It is enough to show that $a_n\notin (L-\epsilon,L+\epsilon)$ for an infinite number of $n$’s. Also inorder to show that $\lim_\limits{n\to\infty}a_n= L$, We have to show that $\forall\epsilon\in (0,\infty),\exists N\in\mathbb{N}, \forall N<n\in\mathbb{N}, a_n\in (L-\epsilon, L+\epsilon)$ (Here $P(n)\equiv a_n\in (L-\epsilon,L+\epsilon)$) By the proposition we get that this is equivalent to show that: $\forall\epsilon\in (0,\infty), \forall A\in\mathscr{P}_{\aleph_0}(\mathbb{N}),\exists n\in A, a_n\in (L-\epsilon,L+\epsilon)$","['proof-verification', 'logic', 'real-analysis', 'sequences-and-series', 'elementary-set-theory']"
2880624,Condition number of $AA^T$ when $A$ is polynomial Vandermonde,"Suppose I'm doing polynomial regression of degree $m$ $$p(x, \mathbf{w}) = w_0 + w_1x + \dotsb + w_mx^m$$ given training data $(x_1, t_1), \dotsc, (x_N, t_N)$. Suppose I'm using the loss function $$L(\mathbf{w}) = \frac12 \sum_{j=1}^N \big( p(x_j, \mathbf{w}) - t_j \big)^2$$ The Hessian of the loss function is $AA^T$, where $A_{ij} = {x_{(i)}}^{j-1}$. What can be said about the eigenvalues of the positive definite matrix $AA^T$, as we vary which $x_{i}$ we choose? Specifically its condition number?","['condition-number', 'statistics', 'regression', 'matrices', 'linear-algebra']"
2880678,How to show that $C^\infty_0$ is not dense in $L^p_{weak} (\mathbb{R}^n)$?,"Let $C^\infty_0$ denote the  smooth, compactly supported functions on $\mathbb{R}^n$ . Let $L^p_{\mathrm{weak}}(\mathbb{R}^n)$ denote the space of all functions $f:\mathbb{R}^n \rightarrow \mathbb{R}$ which satisfy $$
 \Big| \big\{x \in \mathbb{R}^n : |f(x)| > \lambda \big\} \Big| \lesssim \lambda^{-p}
$$ for all $\lambda > 0$ . This space when endowed with the quasi-norm $$
\Vert f\Vert_{L^p_{\mathrm{weak}}}^* : = \sup_{\lambda > 0} \lambda \Big| \big\{x \in \mathbb{R}^n : |f(x)| > \lambda \big\} \Big|^{1/p}
$$ is a quasi-Banach space. How do you show that $C^\infty_0$ is not dense in $L^p_{\mathrm{weak}} (\mathbb{R}^n)$ for $1< p < \infty$ ? This question was inspired by a real analysis qualifying problem, and I know that considering the function $f(x) = |x|^{-n/p}$ is important.","['general-topology', 'weak-lp-spaces', 'functional-analysis', 'real-analysis']"
2880761,The Lie algebra version of $1 \to SU(N) \to U(N) \to U(1) \to 1$,"For the unitary group $U(N)$ and the special unitary group $SU(N)$, we have the exact sequence
$$
1 \to SU(N) \to U(N) \to U(1) \to 1.
$$
How does this behave at the level of algebras? I suppose we still get an exact sequence of Lie algebras? In particular, I would be very interested in understanding this sequence in terms of the Gell-Mann matrix https://en.wikipedia.org/wiki/Gell-Mann_matrices presentation of $\frak{su}_3$: https://en.wikipedia.org/wiki/Special_unitary_group#Lie_algebra","['matrices', 'representation-theory', 'lie-algebras', 'lie-groups']"
2880765,Show that $f(x) = x^TQx$ is convex using the inequality $f(y+\alpha(x-y))-\alpha f(x)-(1-a)f(y)\le 0$,"Suppose that $f(x) = x^TQx$, where $Q$ is $n\times n$ symmetric
  positive semidefinite matrix. Show that $f(x)$ is convex on the domain
  $\mathbb{R}^n$. Hint: it may be convenient to prove the following
  equivalent inequality: $$f(y+\alpha(x-y))-\alpha f(x)-(1-a)f(y)\le 0$$
  for all $\alpha\in[0,1]$ UPDATE : I redid the calculations: I did and tried to prove it's $\le0$ by the following: $$(y^T + ax^T-ay^T)(Qy+aQx-aQy) - ax^TQx-(1-a)y^TQy = \\y^TQy + ay^TQx-ay^TQy+ax^TQy+a^2x^TQx-a^2x^TQy -ay^TQy -a^2y^TQx +a^2y^TQy- ax^TQx -(1-a)y^TQy = \\(1-2\alpha+\alpha^2-1+a)y^tQy + (\alpha^2-\alpha) x^tQx+(\alpha+\alpha-\alpha^2-\alpha^2)y^tQx =\\ (-2\alpha+\alpha^2+a)y^tQy+(\alpha^2-\alpha)x^tQx+(2\alpha + 2\alpha^2)y^tQx$$ But I still don't see why this should be less than $0$. We have that $y^tQy$ and $x^tQx$ are positive semidefinite matrices. However I don't know how to deal with the term $y^tQx$","['matrices', 'convex-analysis', 'positive-semidefinite', 'quadratic-forms']"
2880830,Finitely generated R-module is a field iff R is a field? [duplicate],"This question already has answers here : Proof that an integral domain that is a finite-dimensional $F$-vector space is in fact a field (4 answers) Closed 5 years ago . Not sure how to do this one. If $S$ is a field, then I was considering that $\exists r_1,\ldots, r_n\in R$ s.t. $1 = r_1s_1+\cdots+r_ns_n$ so for $r = rr_1s_1+\cdots+rr_ns_n$ . Maybe that is somehow useful for taking inverses of elements. The assumption that $S$ is an integral domain is necessary because otherwise we could have $S = \mathbb{Z}_p[x]/f(x)$ where $f(x)$ is not irreducible. This is still a finitely generated $\mathbb{Z}_p$ -module, but its not a field. Any hints or solutions would be much appreciated. I feel like this isn't that hard and I'm missing something simple Source: https://dornsife.usc.edu/assets/sites/363/docs/F17_510ab.pdf","['ring-theory', 'abstract-algebra', 'commutative-algebra']"
2880846,Integral of $\log(1-x^t)$ with respect to $t$,"I would need some help to work with the following integral: $$f(x) = \int_2^\infty \log (1-x^t) dt ,\ \ \ \ \ \ \ \ \  |x|<1$$ I would like to get a closed form or something similar (which seems to be impossible), but any other type of exact equivalent expression to work with would be great. Making a change of variables seems not to help much. I also tried to evaluate it as a complex integral, but the path of integration $[2, \infty)$ is not the easiest to work with. Any idea will be welcomed.","['integration', 'improper-integrals', 'definite-integrals']"
2880848,Proof that cross product is orthogonal,"I'm trying to prove that (u x v) is orthogonal to both u and v. Is it a sufficient proof to simply demonstrate that the dot product of u and (u x v) is equal to zero because due to the properties of the cross product, the previous expression is equivalent to the dot product of (u x u) and v. Since the cross product of u with itself is obviously 0, we can see that u is orthogonal to (u x v). I would repeat this same process for v. Is that a sufficient proof?","['orthogonality', 'cross-product', 'proof-verification', 'linear-algebra']"
2880863,Does $\sum_{n=1}^{\infty}\frac{1}{2^n} + \frac{3}{n}$ converge or diverge?,"Does this series converge or diverge? If it converges, determine its limit.
$$\sum_{n=1}^{\infty}\frac{1}{2^n} + \frac{3}{n}$$ So far I said that $\frac{1}{2^n}$ is a geomotric series that converges, and $\frac{3}{n}$ diverges since its the harmonic series (I think), but I don't know where to go from that! (sorry I'm a beginner)","['convergence-divergence', 'sequences-and-series']"
2880886,Spectral theorem and strong convergence,"Question:Let $H$ be a Hilbert space, and $T$ be a self-adjoint operator. If $||T||\leq 1$ and $(Tx,x)\geq 0$ for all $x\in H$, then $T^n$ strongly convergent. My idea : By spectral theorem and $||T||\leq 1$, $Tx=\int _{-1}^{1} \lambda dE_\lambda x$. I guess $T^n$ strongly convergent to $0$. But I can't compute $T^nx$.","['hilbert-spaces', 'operator-theory', 'functional-analysis']"
2880902,Determining whether the given statements is true or not?,"$Problem$ Statement 1) $(A\subset C 
    ~\land~ B\subset D
) ~\to~ (A\times B)\subset (C\times D)$ Statement 2) $(A\times B)\subset (C\times D)~\to~(A\subset C 
    ~\land~  B\subset D
)$ Statement 3) Statement 2 if sets A and B are not empty It is easy to see that statement 1 is true . I am confused about statement 2 and statement 3. 
Any suggestion will be appreciated.","['elementary-set-theory', 'logic']"
2880905,"Find all prime ideals of $\mathbb{F}_3[x,y]/(y^2−x^3+x)$ whose intersection with $\mathbb{F}_3[x]$ is equal to $(x^2+x+2)$","Find all prime ideals of $\mathbb{F}_3[x,y]/(y^2−x^3+x)$ whose intersection with $\mathbb{F}_3[x]$ is equal to $(x^2+x+2)$. I'm a completely beginner in commutative algebra and try to learn it in advance. I have actually no idea about how to solve this question. My thought till now: The prime ideals of $D:=\mathbb{F}_3[x,y]/(y^2−x^3+x)$ are the canonical images of the prime ideals containing $(X^2+Y^2−1)$. $(y^2-x^3+x)$ and $(x^2+x+2)$ are both irreducible in $\mathbb{F}_3[x,y]$; $D$ is integrally closed. Please give me some hints! I'm really stuck on this problem and lost on the right directions! Thanks very much!","['algebraic-geometry', 'abstract-algebra', 'linear-algebra', 'commutative-algebra']"
2880946,Can nested limits be flattened into a single limit?,"Is it true that $$\lim_{x \to c}{\Bigl(\lim_{y \to x}{g(y)}\Bigr)}=L \implies \lim_{y \to c}{g(y)}=L$$ (assuming $\lim_{y \to x}{g(y)}$ exists for all $x$) The question crossed my mind when doing a problem to do with derivatives. Specifically, the problem of whether $$\lim_{x \to c}{f^\prime(x)} = L \implies f^\prime(c) = L$$ since expanding $f^\prime$ using the definition of a derivative yields something a lot like the expression this question asks about: $$\lim_{x \to c}{\Bigl(\lim_{y \to x}{\frac{f(y)-f(x)}{y-x}}\Bigr)}=L \implies \lim_{y \to c}{\frac{f(y)-f(c)}{y-c}}=L$$ (assuming that inner limit always exists, i.e. assuming that $f$ is differentiable for all $x$)","['limits', 'derivatives']"
2880948,Explicit computations for derived functors,"Let $F$ be a left exact functor from the category of sheaves of abelian groups to the category of abelian groups, $\mathscr{F}$ a sheaf of abelian groups on a topological space $X$. Since injective resolutions always exist, and acyclic ones are sent to acyclic ones, we may define the right derived functor $RF$ by $RF(\mathscr{F}):=F(I^\cdot)$ for any injective resolutions $\mathscr{F}\to I^\cdot$. However, though they exist, an injective resolution is usually messy, so we often do not use it for computations. An example is we use $\check{C}$ech resolution to compute for $F=(f:X\to\{pt\})_*$; note that $R^iF(\mathscr{F})=H^i(X,\mathscr{F})$. More precisely, we pick a good open cover $\mathcal{U}$ for $X$, and then we have $$H^i(X,\mathscr{F}) = \check{H}^{\,i}(\mathcal{U},\mathscr{F}) \mbox{ [Harshorne A.G. ex.III.4.11].}$$ For good enough spaces, we can pick such a good cover and compute the right-hand side precisely. Questions: In general is there a way to compute a derived functor first by resolving by a Cech complex with a good cover? If we cannot expect my first question to be true, is it at least possible for some specific functors? When $F=(f:X\to Y)_*$, we have a clearer description: $R^iF(\mathscr{F})$ is the sheaf that associates to the presheaf 
$$V\mapsto H^i(f^{^1}(V), \mathscr{F}|_{f^{-1}(V)}) \mbox{ [Harshorne A.G. III.8.1].}.$$ 
Good! this makes things more explicit! I notice that this result can be obtained by my first question (if it is true). However, Hartshorne uses a quite complicated proof that refers to other concepts such as ""effacable"", ""universal $\delta$-functors"". I also found that if I prove it directly by definition, it will be a mess (Homology sheaf is a quotient by the image sheaf.. so you have to take two sheafifications!). Is there a plain explanation? For the third question, is it possible to get an explicit result just for $RF(\mathscr{F})$ but not $R^iF(\mathscr{F})$? I would like to know since $RF(\mathscr{F})$ contains more information.","['algebraic-topology', 'algebraic-geometry', 'sheaf-theory', 'homology-cohomology', 'derived-functors']"
2880950,Relation between sizes of chains and antichains in a poset,"The questions is to, Show that every partially ordered set with $n$ elements either contains a chains of size greater than $c$ or an anti-chain of size at least $n/c$. I only know definitions of chains and anti-chains but couldn't proceed with just that much. 
I came across Dilworth's theorem so I was attempting induction using Dilworth but am unable to proceed further. I know that if for a given poset $P$, let the size of the maximal chain is $r$, then I can partition $P$ into $r$ anti-chains but how should I take thiss further ?","['graph-theory', 'order-theory', 'discrete-mathematics']"
2880972,Evaluate $\lim_{x \to \infty} [(x+2)\tan^{-1}(x+2) -x \tan^{-1}x]$,$\underset{x \to \infty}{\lim} [(x+2)\tan^{-1}(x+2) -x \tan^{-1}x]=?$ My Try :$[(x+2)\tan^{-1}(x+2) -x \tan^{-1}x] = x \tan^{-1} \frac {2}{1+2x+x^2} + 2. \tan^{-1}(x+2)$ $\underset{x \to \infty}{\lim} x \tan^{-1} \frac {2}{1+2x+x^2} =0$ [By manipulating L'hospital] and $\underset{x \to \infty}{\lim}2. \tan^{-1}(x+2) = \pi$ so $\underset{x \to \infty}{\lim} [(x+2)\tan^{-1}(x+2) -x \tan^{-1}x]= \pi$ Can anyone please  correct me If I have gone wrong anywhere?,"['limits', 'calculus', 'real-analysis']"
2880979,"Show that $y=x^{k}$ with $gcd(k,n)=1$ is a generator of $G$. [duplicate]","This question already has answers here : How to find a generator of a cyclic group? (9 answers) Closed 5 years ago . Could someone please verify whether my solution is okay? Let $G$ be a finite cyclic group with $|G|=n$ and generator $x$. If $y=x^{k}$ and $gcd(k,n)=1$, then show that $y$ is a generator of $G$. Let $y=x^{k}$ with $gcd(k,n)=1$. Then $|\langle y\rangle|=|y|=\frac{n}{gcd(k,n)}=\frac{n}{1}=n$. Then $G=\langle y\rangle$.","['group-theory', 'proof-verification', 'cyclic-groups']"
2881035,"What is ""$\ldots$"" called in English when it comes to counting rule for compound events?","Hi! How do you read this theory in English, mainly GenAm/RP? Theorem one point two. If an operation consists of k steps, of which the first can be done in n-subscript-one ways, for each of these the second step can be done in n-subscript-two ways, for each of the first two the third step can be done in n-subscript-three ways, and so forth, then the whole operation can be done in n-subscript-one times n-subscript-two ellipsis times n-subscript-k ways. Is "" $\ldots$ "" informally called ""all the way down to"" and formally ""and so forth""? In the second case, like ""r = 0,1,2,...,n"". How is this sentance said? r equals zero, one, two, and so on to n. r equals zero, one, two, and so forth to n. r equals zero, one, two, and so on n equals n. r equals zero, one, two, and so forth n equals n. Are they correct? What is your familiar way to say them?","['statistics', 'combinatorics', 'probability']"
2881072,How can I learn about the Monster group?,"There are several questions about the Monster group on this site, but none really answer the question in the title. While reading about groups in a first year algebra course, I was told about the classification of finite simple groups and the existence of the Monster group, but further research on my part lead me straight into a brick wall. Any answers I could find were either far, far beyond my current knowledge, or had so little detail, I learned nothing at all. I actually feel quite frustrated with this. I suspect this is all for a couple reasons. The Monster group is very complicated and difficult to understand, in the realm of research level mathematics. Additionally, it's not really well-understood, but this makes it an exciting object to study as a budding mathematician. This leads to the question in the title; what is the necessary background and motivation needed to start studying the monster group? Also, what is the necessary background needed to study related topics like the moonshine conjectures and the $j$-function? Keep in mind, I'm a second year undergraduate, though a little beyond the standard second year undergraduate program. I've studied some abstract algebra, to the level of Dummit & Foote on groups, rings and fields. I also have studied general topology and some very basic algebraic topology if that's of any use, and I'm diving into number theory right now. Hopefully this helps answers understand what ""level"" I'm at. As a further note, it is obviously too early for me to know what I will be studying as a research mathematician, but I think it's unlikely that I'll become a finite group theorist. In any case, I hope that doesn't mean I'm doomed to never understand the Monster, moonshine, and the $j$-function.","['finite-groups', 'group-theory', 'soft-question']"
2881090,A group with an infinite cyclic normal subgroup that has a finite cyclic quotient is abelian,"Let $G$ be a group with a normal subgroup $N$ such that $N$ is isomorphic to $\mathbb{Z}.$ Also suppose that $G/N$ is isomorphic to $\mathbb{Z}/n\mathbb{Z}$ for some integer $n \geq 2.$ I need to show that if $n$ is odd, then $G$ is abelian. The following is my attempt: Because $N$ is infinite cyclic, we have that $N = \langle t \rangle$ for some element $t \in G.$ As $G/N$ is finite cyclic, we have that $G/N = \langle sN \rangle$ for some $s \in G,$ where $s^n \in N.$ Then $G$ is generated by $\{s,t\},$ so to show $G$ is abelian, it suffices to show that the two generators commute. Consider the element $st \in sN.$ As $N$ is normal, $st \in Ns,$ so $st = t^is$ for some integer $i$. However, I don't know how to proceed any further. In particular, I'm not sure how I am supposed to use the fact that $n$ is odd. The only difference between odd-order cyclic groups and even-order cyclic groups I can think of is that in an odd-order cyclic group, every nontrivial element has an inverse different from itself (which doesn't seem very useful here). Any help would be much appreciated!","['group-theory', 'normal-subgroups', 'abelian-groups']"
2881097,"Topologically, is there a definition of differentiability that is dependent on the underlying topology, similar to continuity?","I'm studying Analysis on Manifolds by Munkres, and at page 199, it is given that Let $S$ be a subset of $\mathbb{R}^k$; let $f: S \to \mathbb{R}^n$. We
  say that $f$ is of class $C^r$, on $S$ if $f$ may be extended to a
  function $g: U \to \mathbb{R}^n$ that is of class $C^r$ on an open set
  $U$ of $\mathbb{R}^k$ containing $S$. It is clear from this definition that, even if we were working on a subspace $M$ of $\mathbb{R}^n$ (or on the set $M$ with different topology other subspace topology), we still consider the opens sets of $M$ as a subset of $\mathbb{R}^n$, and show the differentiability according to that. However, for example, if we were to show the continuity of a function $f : \mathbb{R}^k \to M$, we would consider open sets of $M$, as open sets of $M$, i.e not $\mathbb{R}^n$. In this sense, the continuity of a function is depends on the topology of the domain & codomain of that function, whereas the differentiability does not, as far as I have seen. So my question is that, is there any definition of differentiability that is dependent on the underlying topologies of domain & codomain ? Clarification For example, normally, for $f: A \to \mathbb{R}^m$, the concept differentiability is defined for $x \in Int(A)$, but the very definition of interior needs the definition of what we mean by an open set , which is dependent on the underlying topology, so say (trivially) $A = \{1,2\}$, then with the discrete topology both $1,2 \in Int(A)$, but can we define differentiability in this space ? Or let say, $A = (0,1]$ as a subspace of $[0,1]$ with the standard topology (subspace topology inherited from $\mathbb{R}$).Now if we consider our bigger space as $[0,1]$, then we should be able to define differentiability at $x = 1$ because $(0,1]$ is open in $[0,1]$, hence $1\in Int[0,1]$.","['real-analysis', 'continuity', 'general-topology', 'differential-topology', 'derivatives']"
2881099,"prove $x+y=a, xy=b$ uniquely determine $x,y$","To be precise, there are two solutions ($a,b$ swapped) or none. But if there is a pair of solutions, there is only the one pair. EDIT Because I'm reviewing algebra 2 (see algebra-precalculus tag), I'd ideally like a proof at that level (though it mightn't be possible). This arises in factoring simple quadratics and factoring by grouping. e.g. [using different variables] $(x+a)(x+b) = x^2+(a+b)x + ab$ If we find two other numbers $p,q$ with the same sum $p+q=a+b$ and same product $pq=ab$, we can write $x^2+(p+q)x + pq = (x+p)(x+q)$ This is true, but do we also know that $p=a$ and $q=b$? Could there be some other $p,q$ with the same sum and product? Certainly, just one of sum or product does not uniquely determine $p,q$. e.g. for $p+q=a+b=4$, we could have $a=2, b=2$ and $p=1, q=3$. For $pq=ab=12$, we could have $a=3, b=4$ and $p=2, q=6$. How do we know that using both contraints always gives one solution? In other words, does $p+q=a+b$ and $pq=ab$ have a unique solution, and why? My reasoning is easiest with real numbers. $x+y=a => y=-x+a$ gives a falling diagonal line, with slope $-1$, y-intercept $a$, and x-intercept $a$. $xy=b$, for $b>0$, gives the typical curve in the $+,+$ quadrant and in the $-,-$ quadrant. The curve is symmetrical across the rising diagonal $y=x$ (and also across $y=-x$). The line might go between the two curves. It might just touch one of them, or intersect twice. The single intersection can only occur at $y=x$. Here, $2x=a, x^2=b$, $x=a/2$, $(a/2)^2=b, a^2/4=b, a^2=4b$, and therefore where $a=\pm 2\sqrt{b}$. If $a$ is between these values, there is no solution; and if $\mod{a}>2\sqrt{b}$, there are two solutions. Because of symetry, these are $x,y$ swapped. i.e. if $(x,y)$ is one solution, $(y,x)$ is the other. $xy$, for $b<0$, gives similar curves but in the $-,+$ and $+,-$ quadrants. The line always intersects both curves. Because of symmetry, the intersections are at $(x,y)$ and $(y,x)$. It's also possible to show this by manipulating the constraints into a quadratic, and then solving that (e.g. with the quadratic formula). But since the theorem of this question is used to solve quadratics, it seems begging the question to use quadratics to solve it.","['algebra-precalculus', 'proof-verification', 'proof-writing']"
2881107,How to find out whether a matrix to the $4$th power is the identity matrix?,"Last semester, I was following a Linear Algebra course and in the exam of that course, the following question was asked: Of the following 5 matrices A, how many of them satisfy $A^4 = I$ ?
$$\begin{bmatrix}1&0\\0&-1\end{bmatrix}, \begin{bmatrix}\frac{\sqrt2}{2}&\frac{\sqrt2}{2}\\-\frac{\sqrt2}{2}&\frac{\sqrt2}{2}\end{bmatrix}, \begin{bmatrix}\frac{\sqrt2}{2}&\frac{\sqrt2}{2}\\-\frac{\sqrt2}{2}&-\frac{\sqrt2}{2}\end{bmatrix}, \begin{bmatrix}\frac{1}{2}&-\frac{\sqrt3}{2}\\\frac{\sqrt3}{2}&\frac{1}{2}\end{bmatrix}, \begin{bmatrix}0&-1\\1&0\end{bmatrix} $$ I answered this question by doing all the calculations to the power of 4, and as you would expect, this took way too much time but I eventually figured out the correct answer which is 2 of them, which are the following matrices: $$\begin{bmatrix}1&0\\0&-1\end{bmatrix} and \begin{bmatrix}0&-1\\1&0\end{bmatrix} $$ I'm taking the exam again and was wondering if there's a different and easier way to solve such a question?","['matrices', 'linear-algebra']"
2881136,Is the convergence of $\dot{x}=2A(t)x$ faster than that of $\dot{x}=A(t)x$?,"Let $x \in \mathbb{R}^{n}$ and $A(t) \in \mathbb{R}^{n\times n}$. If $\dot{x}=A(t)x$ and $\dot{x}=cA(t)x$ with $c>1$ are exponentially stable. Is the convergence rate of $x$ to zero of $\dot{x}=cA(t)x$ faster than that of $\dot{x}=A(t)x$? Here is my initial thought: For linear time-invariant system, the fact $\dot{x}=A(t)x$ is exponentially stable implies $A(t)=A$ is Hurwitz. It is clear that the $i$th eigenvalue $\lambda_{i}(cA)=c\lambda_{i}(A)$, $i,\ldots,n$, thus, the convergence for $\dot{x}=cAx$ with $c>1$ is faster than that of $\dot{x}=Ax$. For a linear time-varying system. Let us consider the extreme case where $c=0$, $\dot{x}=cA(t)x=0$, which implies it is no longer exponentially stable. For $c>1$, can we have the same conclusion for exponentially stable linear time-varying systems, i.e., can we conclude that the convergence is faster when $c>1$? Update 1:
For a scalar time-varying system, i.e., $x\in\mathbb{R}$. We can actually prove this conjecture. In fact, from the uniqueness of the equilibrium point ($x=0$ is the only solution that renders $\dot{x}=0$), the solution of $\dot{x}=ca(t)x$ is a monotone function either strictly increasing or strictly decreasing, depending on its initial condition $x(0)$. Thus, for $\dot{x}=ca(t)x$ where $c>1$, the absolute value of the derivative is larger than that of $\dot{x}=a(t)x$, while for both cases the sign of $\dot{x}(t)$ remains unchanged for all $t\ge0$. Thus, $\dot{x}=ca(t)x$ does converge faster to $x=0$ than $\dot{x}=a(t)x$.","['stability-in-odes', 'linear-algebra', 'ordinary-differential-equations', 'dynamical-systems']"
2881146,Sum of Geometric Series Formula [duplicate],"This question already has answers here : Prove that 1/2 + 1/4 + 1/8 ....... = 1 [duplicate] (3 answers) Closed 5 years ago . I just need the formula for the sum of geometric series when each element in the series has the value $1/2^{j+1}$, where $j = 0, 1, 2, \ldots, n$. Please help. Someone told me it is: $$S = 2 - \frac{1}{2^n}$$ I am not sure if its right because he has given me no proof and I couldn't prove it when I calculate it manually. Say for example: $$S = 1/2 + 1/4 + 1/8 = .875$$ But when using the formula given above, with $n=3$ (since there are $3$ elements): $$S = 2 - 1/8 = 1.875$$ The answers are not the same. Please enlighten me with this issue.","['summation', 'geometric-progressions', 'sequences-and-series']"
2881147,"Which $f,g \in \mathbb{C}[x,y]$ satisfy $\mathbb{C}(fy,gy)=\mathbb{C}(x,y)$?","Let $f=f(x,y), g=g(x,y) \in \mathbb{C}[x,y]$. Which $f,g$ satisfy $\mathbb{C}(fy,gy)=\mathbb{C}(x,y)$? Examples: (1) $f=2x$, $g=1-2x$. It is easy to see that
$\mathbb{C}(2xy,(1-2x)y)=\mathbb{C}(2xy,y-2xy)=\mathbb{C}(2xy,y)=\mathbb{C}(x,y)$. (2) $f=x$, $g=xy$. Attempts to generalize the two examples: (1) $af+bg \in \mathbb{C}[x]-\{0\}$, for some $a,b \in \mathbb{C}$, similarly to example (1) . But if $f=x^2$ and $g=x^4$, then this does not work since $\mathbb{C}(x^2y,x^4y)=\mathbb{C}(x^2,y)$. So perhaps we should require $af+bg \in \mathbb{C}-\{0\}$? This will imply that $\mathbb{C}(fy,gy) \ni afy+bgy=y(af+bg)=\lambda y$, and then $f,g \in \mathbb{C}(fy,gy)$, which still does not guarantee that $x \in \mathbb{C}(fy,gy)$. (2) The following seems to be a false claim: $f$ and $g$ themselves already generate $\mathbb{C}$, namely: $\mathbb{C}(f,g)=\mathbb{C}(x,y)$; although in example (2) it works, but in other cases it may not work. Edit: After receiving a comment that the resultant may be relevant, I add the following links: Theorem 2.1 and this question . If I am not wrong, the criterion in the second link implies that, in our case:
If $\gcd(f,g)=1$, considering $f,g \in (\mathbb{C}(x))[y]$, then $\mathbb{C}(x)(fy,gy)=(\mathbb{C}(x))(y)=\mathbb{C}(x,y)$. Thank you very much!","['field-theory', 'algebraic-geometry', 'polynomials']"
2881168,"On the sets of sums $\sum\limits_{n=1}^\infty\frac{a_n}{n^s}$ with $(a_n)$ periodic and integer valued, for different values of $s$ natural number","For every positive integer $s$, let $A_s$ denote the set of the sums of the converging series $\sum\limits_{n=1}^\infty\frac{a_n}{n^s}$ for every periodic sequence of integers $(a_n)$. Then each $A_s$ is a countable dense subset of the real numbers, and an additive group. The set $A_1$ is in fact a vector space with scalars drawn from the rationals. I suspect $A_s$ should contain no non-zero rationals (counterexamples are welcome!) but a proof of this would imply that Catalan's number is irrational so attacking that directly should be avoided... Question Can anything interesting be said about the intersections of these sets? For example, is it the case that $A_s\cap A_t=\{0\}$ for every $s\ne t$? This question comes from my own musings and it may be open. I suppose this is a risk one always has when asking questions that flirt with the zeta function. Some Notes: $\zeta(s)\in A_s$, $\eta(s) \in A_s$,
$\ln(\mathbb{Q})\subset A_1$. Generalizations that may be worthy of follow up: 1) Is this just the case for positive real numbers $s\neq t$? This has now been answered below. This is not the case. 2) If we define $A_s$ with Gaussian integers do we get the same results? Edit 1 (an effort to spruce this question up): Some Motivations + some cool values This question didn't get the excitement I expected so I will now add some crazy values! Here are a couple of values from Dirichlet series in $A_1$, $A_3$, $A_5$, $A_7$. We can compute specific values in $A_s$ but when we manage to get exact forms of values in these sets (it seems) invariably this is because of their relationship to Dirichlet Series. $$f(s,\vec{a})= \sum_{n=1}^\infty{\frac{a_n}{n^s}} $$ Then 
$$
\begin{array}{c|c|c|c|c|c}
f(s,\vec{a}) & \vec{a}=(1,-1) 
& \vec{a}=(1,0,-1,0) 
& \vec{a}=(1,1,0,-1,-1,0) 
& \vec{a}=(1,0,1,0,-1,0,-1,0) \\
\hline
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
s=1 
& \ln(2) 
& \frac{\pi}{4}          
& \frac{2  \pi}{3\sqrt{3}} 
& \frac{\pi}{2\sqrt{2}}  \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
s=3    
& \frac{3}{4}\zeta(3)       
& \frac{\pi^3}{32} 
& \frac{5  \pi^3}{81\sqrt{3}}   
& \frac{3\pi^3}{64\sqrt{2}} \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
s=5    
& \frac{15}{16}\zeta(5)       
& \frac{5 \pi^5}{1536}   
& \frac{17 \pi^5}{2916\sqrt{3}} 
& \frac{19 \pi^5}{4096 \sqrt{2}} \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
s=7    
& \frac{63}{64}\zeta(7)       
& \frac{61\pi^7}{184320} 
& \frac{91 \pi^7}{157464\sqrt{3}} 
& \frac{307 \pi^7}{655360\sqrt{2}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{array}$$ Column(1) Column(2) Column(3) Column(4) And more So here are just some specific elements in $A_s$ to get a feeling for these sets.","['vector-spaces', 'real-analysis', 'dirichlet-series', 'sequences-and-series', 'zeta-functions']"
2881197,Associative laws with negation,"Is it possible to simplify a statement like the one below with the associative law despite the negation? I can't seem to find a law that outlines this. The associative law is the closest I could find. \begin{align}
\ (\lnot p \land \lnot q) \ \lor \ q 
\end{align} If the above is possible, is it also possible if only one variable in the brackets were to be negated like the below? \begin{align}
\ (p \land \lnot s) \ \lor \ s 
\end{align} Thanks.","['propositional-calculus', 'logic', 'discrete-mathematics']"
2881242,What is the group-like structure on $x^2+y^2+z^2-2xyz=1$?,"(Background: this is inspired by Chebyshev polynomials and expanding a function as a Chebyshev series.) Solving for $ z $ gives
$$
z=xy \pm \sqrt{(1-x^2)(1-y^2)},
$$
where $-1\leq x,y \leq 1$. Now choose the positive square root and call it $ x\oplus y$.
(Alternatively, we can define $x\oplus y=\cos(\arccos(x)+\arccos(y))$.) We can check commutativity, identity ($1$), inverses ($-x$). Associativity is might be a bit harder, but I reckon it works. The defining equation is a cubic, hence, it reminds me of the group structure of an elliptic curve, but an elliptic curve has two variables and the group law is defined by intersection with a line, while this equation has three variable, one is the ""sum"" of the other two. Question Has anyone seen a structure like this before?","['algebraic-geometry', 'chebyshev-polynomials', 'elliptic-curves', 'reference-request']"
2881277,Improper integrals over the reals and surreal numbers,"Is it possible to assign improper integrals over the reals a surreal value in a consistent way? Are there any papers available on this? Note that I am not inquiring about formalizing integration over the surreal themselves, which I realize is still a somewhat open problem. Rather, I would like to identify an integral with a number $z$ in the surreals, e.g. $\int_{\mathbb{R}^+} x^2 dx = z, \; z\in \text{Surreals}$ with the ultimate purpose of comparing and performing algebra on the values of ""classically"" divergent integrals.","['surreal-numbers', 'logic', 'analysis', 'field-theory', 'combinatorial-game-theory']"
2881354,Solving $(x^2 + 1) dy + 4 xy dx = x dx$ using separable variable method and integrating factor method,"Solving $(x^2 + 1) dy + 4 xy dx = x dx$ using separable variable method and integrating factor method By integrating factor method - I put it into the form of $ \frac{dy}{dx} + \frac{4x}{x^2 + 1} \cdot y = \frac{x}{x^2 + 1} $ My integrating factor is $I = (x^2 + 1)^2 $ By formula, $ y (x^2 +1)^2 = \int{ (x^3 + x)} dx $ And thus my general solution is $ y = \frac{x^4 + 2x^2 + C}{4(x^2 + 1)^2} $ By separable method is where I got problems with... I put it into the form of $\int{ (\frac{x}{x^2 + 1} )} dx = \int{ (\frac{1}{(1-4y)}} dy $ Getting $\frac{1}{2} \ln (x^2 + 1) + C = \frac{1}{4} \ln (1-4y) $ $2 \ln (x^2 +1) + C = \ln (1-4y) $ $1-4y = e^{\ln (x^2 + 1)^2} + e^{C} = (x^2 +1)^2 + C $ $ y = \frac{ (x^2+1)^2 + C - 1}{4} $ Why is my general solution from separable method very different from the integrating factor method ? I suspect that I have went wrong in the separable method but cannot identify it ...","['calculus', 'derivatives', 'ordinary-differential-equations']"
2881378,Solve: $(x+1)^3y''+3(x+1)^2y'+(x+1)y=6\log(x+1)$,Solve: $(x+1)^3y''+3(x+1)^2y'+(x+1)y=6\log(x+1)$ Is my solution correct: Answer given in the book : $y(x+1)=c_1+c_2\log(x+1)+\log3(x+1)$ For my later reference: link wolfram alpha,['ordinary-differential-equations']
2881384,Series solution to $y''+(\cos t) y=0$,"Series solution to $\mathbf{y''+(cost)y=0}\;$ is$$\sum_{n=0}^{\infty}a_nt^n$$Find $a_2/a_4$ choices are -6, -4, 4, 6 I found $a_{k+2}=-\frac{\cos t}{(k+1)(k+2)}a_k$ So, $a_4=-\frac{\cos t}{3\cdot4}a_2 \;\to\;a2/a4=\frac{-12}{\cos t}$ I'm not sure how to pick answer from this. Is information given in this problem enough to solve it?",['ordinary-differential-equations']
2881403,Why does $A_5$ have $\binom{5}{4}$ Sylow 2-subgroups?,"Let $Syl_p(G)$ be the number of Sylow $p$-subgroups in a group $G$. Why does $|Syl_2(A_5)|=\binom{5}{4}$? Is this true in general, i.e., does $|Syl_2(A_n)|=\binom{n}{2^\alpha}$ where $2^\alpha$ is the maximal power of $2$ dividing $n!/2$? Looking at the case of $A_5$, I know that Sylow subgroups are all conjugate. Taking $P=\langle (1234) \rangle$, it's easy to see that $P$ is a Sylow 2-subgroup, so $|Syl_2(A_5)|$ should be the size of the orbit of $P$ under the action of $A_5$ on its subgroups given by conjugation. Why does this orbit have size $\binom{5}{4}$? What can be said in general?","['abstract-algebra', 'combinatorics', 'sylow-theory', 'symmetric-groups', 'group-theory']"
2881463,Different results after changing the order of integration with constant limits (Failure of Fubini's theorem),"I have the following question $I_{1}=\int _{0}^{1}\int _{0}^{1}\ \frac{(x-y)}{(x+y)^{3}}\ dy\,dx$ Evaulating the above I get $I_{1}=0.5$ Now if I switch the order of integration $I_{2}=\int _{0}^{1}\int _{0}^{1}\ \frac{(x-y)}{(x+y)^{3}}\ dx\,dy$ I get $I_{2}=-0.5$ Why is the value negative after changing the order? Shouldn't the result be same for constant limits (as the region of space is the same for both the integrals) ?","['integration', 'order-of-integration', 'multivariable-calculus']"
2881483,Fourier transformation and eigenvalues,"Let $a,b>0$, $F$ be Fourier transformation and $\chi$ be indicator function. Suppose $T:L^2(\mathbb{R}) \to L^2(\mathbb{R})$ $Tf(y):=F^{-1}(F(f\chi _{[-a,a]})\chi _{[-b,b]})$ and $\{c_n\}_{n=1}^{\infty}$ are all eigenvalues except $0$.
Then prove $\sum_{n=1}^{\infty} |c_n|^2 \leq 4ab$ My idea : I proved $T$ is compact. Let $P_n$ be a projection to eigenspace of $c_n$. By spectral theorem, $Tf=\sum _{n=1}^{\infty}c_n P_nf$,  so $||Tf||^2=\sum |c_n|^2||P_nf||^2$. But I can't prove $\sum_{n=1}^{\infty} |c_n|^2 \leq 4ab$","['hilbert-spaces', 'operator-theory', 'fourier-transform', 'functional-analysis']"
2881486,"Given that $\lim\limits_{n \to \infty}\left(1+\frac{1}{n}\right)^n=e$, show that $1+\frac{1}{1!}+\frac{1}{2!}+\cdots=e$.","Problem Given that $\lim\limits_{n \to \infty}\left(1+\dfrac{1}{n} \right)^n = e$, show that $1+\dfrac{1}{1!}+\dfrac{1}{2!}+\cdots=e$. Proof By binomial theorem, we have \begin{align*}\left(1+\dfrac{1}{n}\right)^n&=\sum_{k=0}^{k=n}\binom{n}{k}1^k \left(\frac{1}{n}\right)^{n-k}\\
&=\sum_{k=0}^{k=n}\frac{1}{k!}\left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right)\cdots \left(1-\frac{k-1}{n}\right).
\end{align*} Thus, on one hand, $$\left(1+\dfrac{1}{n}\right)^n \leq \sum_{k=0}^{k=n}\frac{1}{k!}.\tag1$$ Take the limits as $n \to \infty$ on the both sides of $(1)$. We have $$e=\lim_{n \to \infty}\left(1+\dfrac{1}{n}\right)^n\leq \varliminf_{n \to \infty} \sum_{k=0}^{k=n}\frac{1}{k!}.\tag2$$ On the other hand, take a positive integer $m$ such that $m<n$ and fix it. We have $$\left(1+\dfrac{1}{n}\right)^n \geq \sum_{k=0}^{k=m}\frac{1}{k!}\left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right)\cdots \left(1-\frac{k-1}{n}\right).\tag3$$ Likewise,take the limits as $n \to \infty$ on the both sides of $(3)$. We have $$e=\lim_{n \to \infty}\left(1+\dfrac{1}{n}\right)^n\geq \sum_{k=0}^{k=m}\frac{1}{k!}.\tag4$$ Take the limits as $m \to \infty$ on the both sides of $(4)$. We have $$e \geq \varlimsup_{m \to \infty}\sum_{k=0}^{k=m}\frac{1}{k!}.\tag 5$$ Combine $(2)$ and $(5)$. It follows that $$\lim_{n \to \infty}\sum_{k=0}^{k=n}\frac{1}{k!}=e.$$ Please correct me If I'm faulty. Hope to see other solutions.","['limsup-and-liminf', 'limits', 'proof-verification', 'analysis']"
2881522,Show that problem is well defined for each time,"We have the Cauchy problem of the equation $u_t+xu_x=xu, x \in \mathbb{R}, 0<t<\infty$ with some given smooth ($C^1$) function $g$ as initial value. I want to check if the problem is well defined for each time. We know that a problem is well defined if the solution exists, is unique and depends continuously on the data of the problem. I have computed that the solution of the problem is $u(x,t)=g(xe^{-t}) e^{x(1-e^{-t})}$. So we have that the problem is well-defined if the function $u$ that we found is the unique solution of the problem and if $u$ depends continuously on the data of the problem, right? How can we deduce that there is no other solution except from $u$ ? Is it implied that $u$ depends continuously on the data of the problem, since it contains $g$ ?","['cauchy-problem', 'ordinary-differential-equations', 'partial-differential-equations']"
2881533,"Rigorous version of ""spatial infinity is a point and not a sphere""","This is a question on differential geometry/topology of Lorentzian manifolds. It is motivated by Physics, but since I want a mathematicaly rigorous formulation I think the correct place to ask is here. So let us state the problem: Let Minkowski spacetime $(\mathbb{R}^4,\eta)$ be given with metric tensor $(\eta_{\mu\nu}) = \operatorname{diag}(-1,1,1,1)$ . We want to talk about infinity as a place . So we want a bigger spacetime $(M,g)$ properly containing Minkowski spacetime and such that the boundary of Minkowski spacetime on this bigger manifold can be seen as the ""points at infinity"". It turns out that one can discover that the right way to do this and have a correct picture of infinity is to require that the embedding of Minkowski spacetime on the bigger manifold be conformal. So far so good. So what we do in practice is: Introduce null spherical coordinates $u =t-r$ and $v = t+r$ with ranges $-\infty < u,v < +\infty$ and $u\leq v$ so that the metric tensor becomes $$\eta=-dudv+\frac{1}{4}(v-u)^2(d\theta^2+\sin^2\theta d\phi^2).$$ Compactify along these by defining $u = \tan U$ and $v = \tan V$ with ranges $-\pi/2 < U,V < \pi/2$ and $U\leq V$ so that the metric in these coordinates becomes $$\eta=\dfrac{1}{\cos^2 U\cos^2 V}(-dUdV+\frac{1}{4}\sin^2(U-V)(d\theta^2+\sin^2\theta d\phi^2))$$ Up to this point the coordinates $(U,V,\theta,\phi)$ with the ranges described in (2) together with the above metric, is just an awkward reparametrization of Minowski spacetime. What we want to do is to precisely describe the points $(U,V,\theta,\phi)$ with $U = \pm \pi/2$ or $V = \pm \pi/2$ . These comprise the desired boundary . It is also clear that if we find a bigger manifold $(N,g)$ with the metric in parenthesis, Minkowski spacetime can be conformally mapped to one open region of it. We now turn to the description of the bigger manifold. The obvious thing to do is to allow the coordinates $U,V$ to extend further . In other words, the naive idea would be: *well, define $$g = -dUdV+\frac{1}{4}\sin^2(U-V)(d\theta^2+\sin^2\theta d\phi^2)$$ and then allow $U,V$ to run from $-\infty$ to $\infty$ together with the usual $(\theta,\phi)\in S^2$ to describe $(N,g)$ . Here comes the trouble. Proceeding naively like this, it seems to be no reason that $(-\pi/2,\pi/2,\theta,\phi)$ is not a sphere. It turns out that Roger Penrose states in an old work from 1964 that ""because $\sin(U-V)=0$ on the metric in these coordinates, they all represent the same point. The coordinate system $(U,V,\theta,\phi)$ looses injectivity if we go to $U = -V = -\pi/2$ . He says this is exactly the same as the $r = 0$ in polar coordinates in the plane. In other words: the metric places a constraint that the bigger manifold is such that the points with coordinates $(-\pi/2,\pi/2,\theta,\phi)$ are all the same point. Now, I really have thought hard about that, but I fail to grasp the reasoning in (5). I mean, we in that case we do have the manifold $\mathbb{R}^2$ explicit and we see that the coordinate chart we propose geometricaly is not injective if we include the origin. Here we don't have the manifold. All we have are coordinates and a metric and from this we want to find a manifold with this metric and with these coordinates containing properly the piece we already have. The fact that the $S^2$ part of the metric vanishes there seems to imply the extension is so that points which naively are $S^2$ are actually the same point. Now why is that? How to understand this? And more importantly: how to state all of this discussion rigorously (I realize the talk about ""to describe the bigger manifold just enlarge the ranges of coordiantes $U,V$ "" is not rigorous)?","['riemannian-geometry', 'coordinate-systems', 'smooth-manifolds', 'differential-topology', 'differential-geometry']"
2881543,Edit: Show that $f$ is differentiable on $\Bbb{R}^n$ and compute $f'$,"Good day all! Edit: I'm currently doing a personal study on differentiation on $\Bbb{R}^n$ but I have this challenging problem. Although, some answers have been provided on how to show that show that $f$ is differentiable on $\Bbb{R}^n$ but I would further like to compute $f'$ on $\Bbb{R}^n$. There is this function
$$f:\Bbb{R}^n\to \Bbb{R}$$ 
$$x\mapsto f(x)=\frac{1}{2}\langle x,u(x)\rangle+\langle x,b\rangle$$
where $u:\Bbb{R}^n\to\Bbb{R}^n$ is linear and symmetric $:$ $(\forall\;x,y\in \Bbb{R}^n,\langle x,u(y)\rangle=\langle u(x), y \rangle)$ and $b\in \Bbb{R}^n.$ Honestly, I am just coming across this kind of function. I want to know what name it's called. How do I show that $f$ is differentiable on $\Bbb{R}^n$ and how do I compute $f'$? Thanks for your help!","['multivariable-calculus', 'calculus', 'normed-spaces', 'derivatives']"
2881546,Finding the sub-algebras (up to conjugation) of $\frak{sl}_2(\mathbb{C})$,"I am trying to find the connected subgroups of the Lie Group $SL_2(\mathbb{C})$ up to conjugation. My thought was to find the sub-algebras of the Lie Algebra $\frak{sl}_2(\mathbb{C})$, and then exponentiate. For the one dimensional sub algebras, it should just be determined by the Jordan form. I am a little stuck with the 2-dimensional sub algebras. I think that the answer should just be 
$$\left\{\begin{pmatrix}a & b\\0 & -a \end{pmatrix}: a,b \in \mathbb{C}\right\} $$ but i'm not sure how to prove it. Any advice would be appreciated.","['abstract-algebra', 'linear-algebra', 'lie-algebras', 'lie-groups']"
2881563,Characterization of maximal monotone operators,"I found the following theorem: Given $H$ Hilbert space and a monotone operator $A\colon H\rightarrow H$ , then A is maximal monotone if and only if $\operatorname{Range}(A+I)=H$ . Note that: $A$ monotone (multivalued) means that $\forall u,v \in H$ and $\forall f\in Au, g \in Av$ , then $(u-v,f-g) \geq 0$ . Moreover a monotone operator is said to be maximal in the sense of inclusion of graphs (i.e. the graph of A has no proper monotone extension). Where can I find a detailed proof of this fact? Thank you in advance, I'm really clueless.","['hilbert-spaces', 'monotone-operator-theory', 'functional-analysis']"
2881581,"If $\Omega\subset\mathbb{R}^n$ is convex and $f$ is differentiable, then $f(x)-f(y)\geq f'(y)(x-y),\;\forall \;x,y\in \Omega$","Let $\Omega$ be a convex set in $\Bbb{R}^n$. We say that that $f:\Omega\to \Bbb{R}$ is convex if $$f(tx+(1-t)y)\leq tf(x)+(1-t)f(y),\;\forall\;0\leq t\leq 1,\;\&\;\forall\;x,y\in \Omega.$$ I want to show that if $f$ is convex and  differentiable on $\Omega,$ then $$f(x)-f(y)\geq f'(y)(x-y),\;\forall \;x,y\in \Omega.$$ I'm thinking of using Partial derivatives but don't know how to go about it. Please, can anyone help out?","['convex-optimization', 'multivariable-calculus', 'calculus', 'derivatives', 'convex-analysis']"
2881657,How to prove that $\sqrt[3]{20+14\sqrt{2}}+\sqrt[3]{20-14\sqrt{2}}=4$?,"Using the Cardano formula , one can show that $\sqrt[3]{20+14\sqrt{2}}+\sqrt[3]{20-14\sqrt{2}}$ is a real root of the depressed cubic
$f(x)=x^3-6x-40$. Actually, one can show by the calculating the determinant that this is the only real root. On the other hand, by the rational root theorem, one can see that the possible rational root must be a factor of 40 and one can check that $f(4)=0$. Therefore, by uniqueness of the real root, one must have
$$
\sqrt[3]{20+14\sqrt{2}}+\sqrt[3]{20-14\sqrt{2}}=4\tag{1}
$$ I had the observation above when I solved the cubic equation $x^3-6x-40=0$. My question is as follows: without referring to the unique real root of the cubic, can we show (1) directly? [An attempt.]
When taking the cube on both sides of (1) and simplifying further, I ended up with (1) again.","['cubics', 'algebra-precalculus', 'nested-radicals']"
2881669,Conditional Expectation of Bernoulli R.V.,"Let $X_1, X_2,\ldots, X_n$ be iid bernoulli r.v. with parameter $p$. Let $S=X_1+\cdots+X_n$ and $Y=X_1X_2$. Compute $\mathbb{E}(Y\mid S)$. I know that $\mathbb{E}(X_1\mid S) = S/n$. So If I could split $\mathbb{E}(Y\mid S)$ into two then I would be done. But I don't think that is allowed. How to proceed?","['conditional-expectation', 'probability-theory', 'probability']"
2881700,Pure states on subalgebras of $\mathcal{B}(\mathcal{H})$ in finite dimensions.,"I consider only finite-dimensional Hilbert spaces. We know that pure states on $\mathcal{B}(\mathcal{H})$ are exactly the vector states or in terms on density matrices, the rank one projections. My question is now, is there something similar to determine purity of states on subalgebras of $\mathcal{B}(\mathcal{H})$?","['von-neumann-algebras', 'c-star-algebras', 'operator-algebras', 'functional-analysis', 'quantum-information']"
2881717,Find all functions satisfying certain requirements,"The requirements are: $f(x, y) = f(y, x)$ $f(x, x) = x$ $f(x, y) = f(x, x + y)$ f: $\mathbb{N}^2 \rightarrow \mathbb{N}$ I think $\gcd(x, y)$ works, but haven't found any other solutions nor have I been able to prove gcd is the sole function that works.","['gcd-and-lcm', 'functions', 'integers']"
2881725,"Find $x$, given: $x^2 + \frac{9x^2}{(x+3)^2} = 16$","Here's an equation: $$x^2 + \frac{9x^2}{(x+3)^2} = 16$$ First, I subtracted 16 from both sides and factored $x^2$ so I would get a quadratic equation, but with no success. Also, I can see that the equation can be rewritten as: $$(x+4)(x-4) + \left(\frac{3x}{x+3}\right)^2 = 0$$
But I can't see how can I use that information. What should I do?","['algebra-precalculus', 'polynomials']"
2881745,Proof that every polygon has at least 2 ears,"I had this question asked a few weeks ago and gave an argument that involved finding an ear and clipping it, professor said it was not quite the correct answer and that I lacked some insights. How would you prove this? I'm not allowed to use the dual graph of a triangulation, because given that any polygon has at least 2 ears, then we can triangulate the polygon by ear clipping. So how can I prove that every polygon has at least 2 ears?","['geometry', 'discrete-mathematics', 'computational-geometry']"
2881753,Irrep. of SU(2) and Laplace Eigenspaces,"In order to calculate the Dirac spectrum on Berger's sphere $(S^3,g_t)$, I came across irreps of SU(2) (see Hitchin p. 30). Apperently, Hitchin restricts the Dirac operator to the eigenspaces of the Laplacian relative to the standard metric because they commute. That's clear. Now he says, that they are given by irreps of SU(2), ie. homogeneous polynomials in two complex variables. And that's the part I do not understand. The eigenspaces of the corresponding Laplacian on $S^3$ are given by harmonic homogeneous polynomials all with respect to the Euclidean $\mathbb{R}^4$, aren't they? So why does he talk about just homogeneous polynomials without the harmonic restriction? I already consulted Bröcker and Hall and cannot find anything helpful. Obviously, I don't understand the connection between Laplace eigenspaces and irreps and definitely miss something. Maybe it's a silly question. But unfortunately, I don't get the point. So I would appreciate some help. Thank you! :) EDIT: I worked up a little and come to the conclusion that the Peter-Weyl theorem is the key point here. It seems that the functions given by the matrix coefficents \begin{align}
g \mapsto f_{v,w}(g) = \left< \pi_k(g)v,w\right>
\end{align} on the irreps $\pi_k$ of SU(2), namely homogeneous polynomials in two complex variables of degree $k$, must give the spherical harmonics I'm seeking for. Can anybody confirm this?","['harmonic-analysis', 'representation-theory', 'spherical-harmonics', 'lie-groups', 'differential-geometry']"
2881755,Solving non-linear ODEs,"I am trying to solve a differential equation of the form: \begin{equation}x^2y''+2xy'+x^2e^{ay}=0\end{equation} This arises from calculating the electric potential of ions following the Boltzmann distribution attracted to a spherically symmetric electrode for those who are curious. Anyway, seeing as this is very similar to #33 from here , I made the substitutions $z=x^2e^{ay}$ and $w=xy'$. I then found \begin{equation}dz=2xe^{ay}dx+ax^2e^{ay}y'dx=(2+w)z\frac{dx}{x}\end{equation} \begin{equation}dw=y'dx+xy''dx\end{equation} I therefore got \begin{equation}\frac{dw}{dz}=w'=\frac{x^2y''+xy'}{(2+aw)z}\end{equation} Substituting this and the definition of $z$ into the original ODE gives \begin{equation}z(2+aw)w'+w+z=0\end{equation} I thought this would be easier to solve, as it is a 1st order ODE. However, it is non-linear and not separable so far as I can see. In the original from the link there is no 2 on the $y'$ term, which makes the transformed ODE separable. Can anyone confirm that what I did with the substitution was correct? Also does anyone have any other suggestions for solving either the original or transformed ODE analytically?","['ordinary-differential-equations', 'statistical-mechanics']"
2881765,"Can anything be proven about this complex variant of the Collatz problem, or is it just as intractable?","Given a Gaussian integer $z = a + bi$, where $a, b \in \mathbb{Z}$, $i = \sqrt{-1}$, iterate the function $$f(z) = \frac{z}{1 + i}$$ if $z$ has even Gaussian norm (that is, both $a$ and $b$ are odd, or they're both even), otherwise $f(z) = 3z + i$. I conjecture that iterating this function eventually leads, if not to $1$, to one of the other Gaussian units ($-1, i, -i$). For example, starting with $z = 14$, we get $$7 - 7i, -7i, -22i, -11 - 11i, -11, -33 + i, -16 + 17i, \ldots$$ (this is wrong, see edit below) I have tried a few different values of $z$ with small norm, some purely real, with pencil and paper, haven't gotten far. Also I have tried it in Mathematica, but either I've made some mistakes in my programming that crash the program, or there really are a lot of values that escape to some infinity. Surely someone else has studied this variant? If so, have they been able to determine anything (like finding a periodic orbit that doesn't include any units)? EDIT: I made a mistake. Mathematica did save my notebook file at some point before it crashed, and I could have gotten the correct sequence from there instead of having to recalculate it anew. It should go like this: $$7 - 7i, -7i, -20i, 10 + 10i, 10, 5 - 5i, -5i, -14i, -7 - 7i, -7, -20, \ldots$$ Thanks to Mr. Cortek for pointing this out.","['collatz-conjecture', 'number-theory', 'complex-numbers']"
2881768,Showing $E(S^2\mid \bar X)=\bar X$ for i.i.d Poisson random variables $X_i$,"Let $X_1,X_2,\ldots,X_n$ be i.i.d $\text{P}(\lambda)$ random variables where $\lambda(>0)$ is unknown. Define $$\bar X=\frac{1}{n}\sum_{i=1}^n X_i\qquad,\qquad S^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar X)^2$$ as the sample mean and sample variance respectively. Since $\sum_{i=1}^n X_i$ and hence $\bar X$ is a complete sufficient statistic for $\lambda$ such that $E(\bar X)=\lambda$, $\bar X$ is the uniformly minimum variance unbiased estimator (UMVUE) of $\lambda$ by the Lehmann-Scheffe theorem.
Again, $E(S^2)=\lambda$, so that $E(S^2\mid \bar X)$ is also the UMVUE of $\lambda$. As UMVUE is unique whenever it exists, it must be that $$E(S^2\mid \bar X)=\bar X$$ The question is: How can I directly show that $E(S^2\mid \bar X)=\bar X$ ? I don't see how to proceed from \begin{align}
E(S^2\mid \bar X=t)&=E\left[\frac{1}{n-1}\sum_{i=1}^n(X_i-t)^2\mid \bar X=t\right]
\\&=E\left[\frac{1}{n-1}\left(\sum_{i=1}^nX_i^2-nt^2\right)\mid \bar X=t\right]
\\&=E\left[\frac{1}{n-1}\sum_{i=1}^nX_i^2\mid \bar X=t\right]-E\left[\frac{nt^2}{n-1}\mid \bar X=t\right]
\\&=\frac{1}{n-1}\sum_{i=1}^nE\left(X_i^2\mid \bar X=t\right)-\frac{n}{n-1}E(\bar X^2\mid \bar X=t)\tag{1}
\end{align} Any hint would be great. As correctly pointed out by Mike Earnest, the conditional distribution of $(X_1,X_2,\cdots,X_n)\mid \bar X$ is multinomial. That is, for a natural number $k$, $$P\left(X_1=x_1,\cdots,X_n=x_n\mid \bar X=\frac{k}{n}\right)=\frac{k!}{x_1!\,x_2!\cdots x_n!}\left(\frac{1}{n}\right)^{x_1}\left(\frac{1}{n}\right)^{x_2}\cdots\left(\frac{1}{n}\right)^{x_n}\mathbf1_{x_i\in A}$$ , where $$A=\left\{(x_1,\cdots,x_n)\in\{0,1,\cdots,k\}: \sum_{i=1}^nx_i=k\right\}$$ From this , we have for each $i$, $$V\left(X_i\mid \bar X=t\right)=t\left(1-\frac{1}{n}\right)\qquad,\qquad E\left(X_i\mid \bar X=t\right)=t$$ And for all $i\ne j$, $$E\left(X_iX_j\mid \bar X=t\right)=t\left(t-\frac{1}{n}\right)$$ So, \begin{align}
E\left(X_i^2\mid \bar X=t\right)&=V\left(X_i\mid \bar X=t\right)+\left[E\left(X_i\mid \bar X=t\right)\right]^2
\\&=\frac{t}{n}(n+nt-1)
\end{align} Also, as expected, \begin{align}
E\left(\bar X^2\mid \bar X=t\right)&=E\left[\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^nX_iX_j\mid \bar X=t\right]
\\&=\frac{1}{n}E\left(X_1^2\mid \bar X=t\right)+\frac{1}{n^2}\sum_{i\ne j}E\left[X_iX_j\mid \bar X=t\right]
\\&=\frac{1}{n}\cdot\frac{t}{n}(n-1+nt)+\frac{2}{n^2}\binom{n}{2}t\left(t-\frac{1}{n}\right)
\\&=t^2
\end{align} So from $(1)$ I finally get, \begin{align}
E(S^2\mid \bar X=t)&=\frac{n}{n-1}\cdot\frac{t}{n}(n+nt-1)-\frac{n}{n-1}\cdot t^2
\\&=t
\end{align} Hence proved. (Thanks to Mike Earnest in particular.)","['statistics', 'poisson-distribution', 'probability-distributions', 'conditional-expectation', 'probability']"
2881800,"$u$-substitution failure in finding $f'(x)$ where $ f(x) = \int_x^0 \frac{\cos(xt)}{t}\, dt$","I'm practicing for the GRE exam, and came across the following question: If
$$
f(x) = \int_x^0 \frac{\cos(xt)}{t}\, dt,
$$
find $f'(x)$. The answer given is $\frac{1}{x}(1 - 2\cos(x^2))$, and I see how they get that answer. What I'm wondering is why the following $u$-substitution gives the wrong answer (or perhaps I'm making a mistake somewhere): If we set $u = xt$, then the integral transforms to 
$$
f(x) = \int_{x^2}^0 \frac{\cos(u)}{u}\, du,
$$
which means that $f'(x) = -(\cos(x^2)/x^2)(2x) = -\cos(x^2)/x$, which misses the $1/x$ term which the answer key says we should have. I cannot see where I am making a mistake in the $u$-sub; is it invalid in this case? Any help is much appreciated.","['integration', 'calculus', 'substitution']"
2881842,Number of poles of an algebraic function,"I don't know much about complex analysis, so I'm not sure if this has an obvious answer or not. I am reading a paper and got stuck on a parenthetical comment. $w(z)$ is a meromorphic function on some simply connected open subset $U\subset \mathbb{C}$, satisfying a polynomial equation in two variables $P(z,w)=0$ over the field of rational functions $\mathbb{C}(t)$. Once this conclusion is reached, the author states in parentheses, ""so it has only finitely many poles"". I'm not sure why this is true, and I'm having trouble proving it.",['complex-analysis']
2881850,"Counting, arranging 8 ppl","I should be able to solve this Problem in my sleep but i cant figure out why my solution is not right - i even have thought about the possibility that the textbook solution is wrong but I guess that would be pretty arrogant. So here is the Question. In how many ways can 8 people be seated in a row if there are 4 men and 4 women and no 2 men or 2 women should sit next to each other. My solution and thoughts are as follows: There are 4! ways to arrange the women and 4! ways to arrange the men, and the row can start with either a men oder a women so times two 4!*4!*2=1152
But the textbook solution shows 
5! *4!=2880 and I have no idea how they come up with that and what is wrong with my solution. I would be very grateful for every hint that could help me understand.",['combinatorics']
2881864,Why is normal curvature well defined?,"I am reading Do Carmo's Differential Geometry and am confused by the definition of normal curvature (page 143). Here is the definition: DEFINITION 3. Let $C$ be  a regular curve in $S$ passing through $p \in S, k$ the curvature of $C$ at $p$, and $\cos \theta = \langle n, N \rangle$, where $n$ is the normal vector to $C$ and $N$ is the normal vector to $S$ at $p$. The number $k_n = k \cos \theta$ is then called the normal curvature of $C \subset S$ at $p$ What is confusing is that curves with the same normal vector can have different curvatures for example the curves 
$$\alpha(t) = (t, t^2, 0), \beta(t) = (t, 2t^2, 0)$$
 on the xy-plane have the same tangent vectors at $p = (0,0,0)$ but different curvatures: $k_\alpha(0) = 2, k_\beta(0) = 4$. Why does this definition not depend on the choice of $C$? It seems to me that this is just assumed when Meusnier's Proposition is proven in the following page.","['geometry', 'differential-geometry']"
2881871,If $N/M$ is a normal subgroup of $G/M$ then $N$ is a normal in $G$,Is it true that if $N/M$ is a normal subgroup of $G/M$ then $N$ is a normal subgroup of $G$ itself? I would think that not necessarily because I would expect that under conjugation we could get a subgroup of $G$ isomorphic to $N$ but maybe not $N$ it self but a subgroup identical from $N$ except its elements be differing from $N$ by  elements of $M$. However I think the correspondence theorem says that $N$ is normal in $G$ so I am a bit confused is it normal after all?,"['normal-subgroups', 'group-theory', 'abstract-algebra']"
2881909,Line integral and differentiability of $\Vert f \Vert^{p}$ where $f(x) = A(x)$,"Problem. Let $f: \mathbb{R}^{n} \to \mathbb{R}^{n}$ , $n \geq 2$ , the function defined by $f(x) = Ax$ where $A$ is a matrix $n \times n$ . For each of the statements below, show if is true or give a counterexample if is false. (a) If $g(x) = \Vert Ax \Vert^{p}$ , with $p>1$ , then $g$ is differentiable in $\mathbb{R}^{n}$ (here $\Vert\;\Vert$ denote the euclidean norm in $\mathbb{R}^{n}$ ). (b) If $\det A = 1$ , then $\int_{\gamma}f\mathrm{d} \gamma$ not depends of the path $\gamma$ that connects the points $x_{0}$ e $x_{1}$ of $\mathbb{R}^{n}$ . (b) In my book, $\displaystyle \int_{\gamma}f\mathrm{d}\gamma$ is defined over a curve $\gamma: [a,b] \to \mathbb{R}^{n}$ of class $C^{1}$ and I proved that If $\Omega$ is connected open in $\mathbb{R}^{n}$ and $f: \Omega \to \mathbb{R}$ is a function of class $C^{1}$ , then $f'$ is a conservative field, that is, $$\int_{\gamma_{1}}f\mathrm{d}\gamma = \int_{\gamma_{2}}f\mathrm{d}\gamma \tag{*}\label{*}$$ for any differentiable curves that connect $x$ to $y$ for every $x,y \in \Omega$ . So, since $f$ is linear, $f$ satisfies the hypothesis of (*). Moreover, $f' \equiv f$ , then $f$ is conservative. But, I don't know if in this question, the definition of $\int_{\gamma}f\mathrm{d}\gamma$ is equal of my book. Moreover, if I'm right, the hypothesis $\det A = 1$ seems unnecessary. It made me think I'm totally wrong and try another way. So I tried to apply a linear change in the variables, but I think I couldn't use it correctly (a) I know that If $f:\mathbb{R}^{n} \to \mathbb{R}$ is given by $f(x) = \Vert x \Vert_{p}^{p}$ where $\Vert\;\Vert_{p} = (|x_{1}|^{p} + ... + |x_{n}|^{p})^{\frac{1}{p}}$ , then is differentiable and If $g:\mathbb{R}^{n} \to \mathbb{R}$ is differentiable, then $F(x) = g(Ax)$ (where $A$ is a $n \times n$ matrix) is differentiable and $F'(x) = A^{T}g'(Ax)$ . These two results make me think that (a) is true, because (a) would be like a generalization of them. But I couldn't prove. Can someone help me?","['integration', 'derivatives', 'real-analysis']"
2881914,Double series convergent to $2\zeta(4)$?,"Using a computer I found the double sum $$S(n)= \sum_{j=1}^n\sum_{k=1}^n \frac{j^2 + jk + k^2}{j^2(j+k)^2k^2}$$
has values $$S(10) \quad\quad= 1.881427206538142 \\ S(1000) \quad= 2.161366028875634 \\S(100000) =  2.164613524212465\\$$ As a guess I compared with fractions $\pi^p/q$ where $p,q$ are positive integers and it appears $$\lim_{n \to \infty} S(n) = \frac{\pi^4}{45} =  2\zeta(4) \approx 2.164646467422276 $$ I'd be interested in seeing a proof if true.","['zeta-functions', 'sequences-and-series']"
2881967,Logic - Member of an inductive set that is not a member of the set of theorems in a formal system?,"First question on this site. Hope to ask/answer many more in the future. I'm currently self-studying An Introduction to Mathematical Logic by Richard E. Hodel and came across an interesting exercise. This is how it is introduced: Let F be a formal system, let FOR be the set of formulas of F, and let THM be the set of theorems of F. A subset I of FOR is said to be inductive if it satisfies these two conditions: (a) Every axiom of the formal system F is in I ; (b) If A 1 , ... , A n / B is a rule of inference of F, and each of the hypotheses A 1 , ... , A n are in I , then B is also in I . The exercise leads one to prove that THM is a subset of any arbitrary inductive set I , and that THM is the smallest inductive subset of FOR, which I already worked out successfully. The exercise does not imply, however, that THM = I . So my question is then, what would be an example of a member of an inductive set that is not a member of THM? And, more generally, when does a formula fail to be a theorem, but succeed in being a member of I ? I'm still on chapter 1 of the book, so maybe my question will be answered down the line, but I am still curious and think that this would help me understand the definitions of a formal system. So any help is appreciated. Thanks","['elementary-set-theory', 'formal-systems', 'induction', 'logic']"
2881968,Birationality of affine line implies birationality?,"Let $A,B$ be noetherian regular domains of dimension one, such that $\mathbb{A}^1_A$ is birational to $\mathbb{A}^1_B$. Is $\text{Spec}(A)$ birational to $\text{Spec}(B)$?","['algebraic-geometry', 'commutative-algebra']"
2882033,Domain of square root of a fraction with variables in the denominator and numerator,"I've been playing around with function
$$f(x)=\sqrt{\frac{x+2}{x+1}}$$
I tried to find its domain, and I did so with finding the interval in which this applies
$${({x\geq-2}\wedge{x\gt-1})}\vee{({x\leq-2}\wedge{x\lt-1})}$$
which in turn gave this domain
$$D(f)\in{(-\infty;-2>\cup{(-1;\infty)}}$$
(I am using standard Czech notation which differs from anglosaxon, in intervals we use <,> instead of [,]) Now all is well and fine, however, I thought to myself: ""I should be able to apply this identity $\sqrt{\frac{a}{b}}=\frac{\sqrt{a}}{\sqrt{b}}$ and the domain should not change."" Now when I solved for the domain of $f(x)=\frac{\sqrt{x+2}}{\sqrt{x+1}}$, the domain turned out to be only
$$D(f)\in(-1;\infty)$$
which I know is wrong. Upon further investigation I was able to get the other interval with $f(x)=\frac{\sqrt{-(x+2)}}{\sqrt{-(x+1)}}$ and get
$$D(f)\in{(-\infty;-2>}$$
Now comes the question, had I not known the first way which i referenced of finding the domain and only used the identity with distributed square roots and then not checked the result. How would I have known that that was not the complete solution  and that I should have also used the equation with negative polynomials under the square roots. Is there some rule that I am missing or something? For graphs of the functions see https://www.desmos.com/calculator/jk14w994q3","['functions', 'real-analysis']"
2882059,"Let $X$ be a K3 surface, show that $H_1(X,\mathbb Z)=0$","Let $X$ be a(n algebraic) K3 surface, i.e., $X$ is a smooth algebraic surface with trivial canonical bundle and $H^1(X,\mathcal{O})=0$. This assumption directly implies that $H^1(X,\mathbb C)=0$, so $H_1(X,\mathbb C)=0$ by Poincaré duality. In particular $H_1(X,\mathbb Z)$ is a torsion group. Next, to show that $H_1(X,\mathbb Z)$ is actually $0$, the book says otherwise there is a nontrivial torsion element in fundamental group which allows us to pass to a finite covering $p:\tilde{X}\to X$, but $\tilde{X}$ is still a K3 surface, and considering any K3 surface has Euler characteristic 24, so $p$ has to be an identity map, which is a contradiction. Here is my question: First homology group is the Abelianzation of fundamental group $H_1=\pi_1/[\pi_1,\pi_1]$, but why a nontrivial torsion element in $H_1$ has a nontrivial torsion representative in $\pi_1$?","['algebraic-geometry', 'surfaces', 'k3-surfaces']"
2882098,Find the derived subgroup of $A_4$,"Find the derived subgroup of $A_4$. Since it is $A_4$, for a permutation $\sigma$ to be in $A_4$, $\sigma$ must have a cycle structure of $2$ cycles. Therefore, $\sigma=(ab)(cd)$. The commutator of such elements, would be obviously another permutation of $2$-cycle (of length $2$). Does it mean that $A'_4=A_4$?","['permutations', 'group-theory', 'symmetric-groups']"
2882103,Computing the Inverse of a two dimensional map?,"In order to find the inverse of the function $y = x^3$ where $y = f(x) = x^3$ we need $x = f^{-1}(y)$, which we compute it as $x = y^{\frac{1}{3}}$ so the inverse function. But how do I calculate the inverse map of the following map? $x \mapsto Ax +By + C$
and $y \mapsto Dx$ ?, where $A,B,C,D$ are real numbers. I was trying to visualize this in terms of matrices, $\begin{bmatrix}x \\ y\end{bmatrix} \mapsto \begin{bmatrix} Ax + By + C \\ Dx\end{bmatrix}$, may that open up some new insights? How can we guarantee the existence of the inverse for this two dimensional map?","['functions', 'inverse-function']"
2882155,how many words of $4$ consonants and $3$ vowels can be made from $12$ consonants and $4$ vowels? if all the letters are different?,"my effort for this question is I am selecting $4$ consonants from $12$ available consonants and $3$ from $4$ available vowels. After selecting $4$ consonants and $3$ vowels now I have $7$ letters in my hand now I am permuting them all with $7!$.
  So the total number of words can be made is ${12\choose4}{4\choose3}7!$. But the answer is ${12\choose4}{4\choose3}$. I would appreciate if anyone advises on this question.","['permutations', 'combinations', 'discrete-mathematics']"
2882174,Sylvester's criteria and Negative definite Matrices.,"A)Sylvester's criterion states that a Hermitian matrix M is positive-definite if and only if all leading principal minors are positive. AA) a Hermitian matrix M is negative-definite if and only if all leading principal minors are negative. B)a Hermitian matrix M is positive-semidefinite if and only if all principal minors of M are nonnegative. BB) a Hermitian matrix M is negative-semidefinite if and only if all principal minors of M are nonpositive. Now My question is the following::
1) Can AA) be deduced from A) ? Or vice versa.. 2) Can BB ) be deduced from B) ?  Or vice vera... My Thoughts: I think they can be as if $A$ is positive definite or positive semi definite then $-A$ will be negative definite or negative semi definite.
So AA)[BB) ] can be deduced from  A)[BB) ]. Edit I wanted to ask if AA)[BB) ] can be deduced from A) [B) ]???? Basically I wanted to know if Sylvester's law is useful to determine whether a Matrix is negative-semidefinite or negative definite?? I am sorry..PLease edit your answer accordingly.. Can anyone please correct me if I went wrong anywhere?? Thank You.","['matrices', 'linear-algebra', 'quadratic-forms']"
2882177,Algebraic trick to map $|z|<2$,Suppose that we want to find the image of the region $|z|<1$ under the mapping $w=\frac z{z+1}$ . Since $z=\frac{-w}{w-1}$ (and assuming $w= u+iv$ ) we should have $\left|\frac w{w-1}\right|=\left|\frac{u(u-1)+v^2-iv}{(u-1)^2+v^2}\right|<1$ or equivalently $$u^2(u-1)^2+v^4+2v^2u(u-1)+v^2<(u-1)^4+v^4+2v^2(u-1)^2.$$ Now we can use the following algebraic trick to write $$(u-1+1)^2(u-1)^2+2v^2(u-1+1)(u-1)+v^2<(u-1)^4+2v^2(u-1)^2$$ which implies that $$(u-1)^2+2(u-1)(u-1)^2+2v^2(u-1)+v^2<0$$ or $$((u-1)^2+v^2))(2u-1)<0.$$ Thus the image would be $2u-1<0$ . Now I wonder if there is some similar algebraic trick for $|z|<2$ . In this case we have $$u^2(u-1)^2+v^4+2v^2u(u-1)+v^2<4(u-1)^4+4v^4+8v^2(u-1)^2$$ I've tried similar method but couldn't arrive to something useful. Could anyone help me in this case? Thanks!,"['complex-analysis', 'complex-geometry', 'conformal-geometry', 'complex-numbers']"
2882220,"How to calculate (x,y) position of number in square","I am looking for a way to determine the X & Y position of a number to draw following square: 1   2   5   10  17
3   4   6   11
7   8   9   12
13  14  15  16 What kind of algorithm / formula can I use ? I have tried rounding the square root of the number to determine the X position. To be clear, here is what I want to achieve, numbers with their corresponding X and Y position: #  X  Y
1: 1  1
2: 2  1
3: 1  2
4: 2  2
5: 3  1
6: 3  2","['geometry', 'sequences-and-series']"
