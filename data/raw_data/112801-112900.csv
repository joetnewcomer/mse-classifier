question_id,title,body,tags
1637414,What is the Picard group of a variety of type G/P with $G$ a semi simple algebraic group and $P$ a parabolic subgroup?,Let $G$ be a simple algebraic group and let $P$ be a parabolic subgroup of $G$ . It follows $X:=G/P$ is a smooth projective variety - the flag variety of $G$ corresponding to $P$ . Is it true that the following holds: Pic( $X$ ) has rank $1$ iff $P$ is a maximal parabolic subgroup. Why? Where do i find a reference?,"['algebraic-geometry', 'homogeneous-spaces', 'algebraic-groups', 'lie-algebras', 'lie-groups']"
1637415,Determining the distance of a point from a line segment given a starting and ending point,"I've found a lot of answers on how to find the distance from a point to a line, but not so much from a point to a line segment. I am given the $x$ and $y$ coordinates of the start point and end point of a line segment.  I am also given another point.  I want to find if that point is a certain distance away from anywhere on the line.  For instance, if I'm given the line segment with the starting coordinates $(0, 0)$ and $(10, 1)$, is the point $(3, 4)$ less than $20$ units away from the line segment at the closest point? In the example below, I would want to be able to theoretically tell that any point in the yellow box is within range of the line. UPDATE: Right now I am calculating the distance of the point to the line and making sure that the point is within the threshold that I want, however, my calculation continues the line indefinitely.  How would I cap it at the threshold past the end points?",['geometry']
1637459,Probability Brownian motion is positive at two points,"Let $0<s<t$ and $(B_r)_r$ is Brownian motion. Does anybody know what $P(B_s>0,B_t>0)$ is? I think I remember it was some $arctan$-law but I don't know the exact form. So I do not need a proof, stating the result would be completely sufficient, but I could not find it using google. Thank you for your help.","['stochastic-processes', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
1637463,How can I show that $X$ and $Y$ are independent and find the distribution of $Y$?,"$X_1,X_2,\dots,X_n$ is an i.i.d. sequence of standard Gaussian random variables. \begin{align}X&=\frac{1}{n}(X_1+X_2+\dots+X_n) \\[0.2cm] Y&=(X_1-X)^2+(X_2-X)^2+\dots+(X_n-X)^2\end{align} How can I show that $X$ and $Y$ are independent? How can I find the distribution of $Y$? Can we use the following method to show that $X$ and $Y$ are independent?
$$Cov(X,Y)=0$$ Or is there any other proper way?","['independence', 'probability-theory', 'probability-distributions']"
1637464,"Find unit vector given Roll, Pitch and Yaw","Is it possible to find the unit vector with: Roll € [-90 (banked to right), 90 (banked to left)] , Pitch € [-90 (all the way down), 90 (all the way up)] Yaw € [0, 360 (N)] I calculated it without the Roll and it is \begin{pmatrix}
cos(Pitch)  sin(Yaw)\\ 
cos(Yaw)  cos(Pitch)\\ 
sin(Pitch)
\end{pmatrix}. How should it be with the Roll rotation and how can I get to this result? My coordinate system is with +z up , +x right and +y forward Many thanks!","['rotations', 'matrices', 'trigonometry', 'geometry', 'vectors']"
1637478,Find the derivative of the function $ y= x|\cos{\frac{\pi}{x}}|$,Function is defined as it follows : $x \neq 0$ and $f(0)=0$ My work is: $\frac{d}{dx}(x|\cos{\frac{\pi}{x}}|)$ = $|\cos{\frac{\pi}{x}}|$ + $x(\frac{d}{dx}|\cos{\frac{\pi}{x}}|)$ = $|cos{\frac{\pi}{x}}|$ + $\frac{\pi}{x}(\operatorname{sgn}(\cos{\frac{\pi}{x}}))(\sin{\frac{\pi}{x}})$ = $\operatorname{sgn}(\cos{\frac{\pi}{x}})[(\cos{\frac{\pi}{x}}) + \frac{\pi}{x}(\sin{\frac{\pi}{x}})]$ Should I consider any points on the domain and find the derivative of the function on those points? Thank you,['derivatives']
1637487,What are the different ways of performing Triangular matrix-vector multiplication?,"Suppose we have $$\left[\begin{array}{cccc} x_1 & 0 & 0 & 0 \\ x_2 & x_1 & 0 & 0 \\ x_3 & x_2 & x_1 & 0 \\ x_4 & x_3 & x_2 & x_1 \end{array}\right] \times \left[\begin{array}{c} y_1 \\ y_2 \\ y_3 \\ y_4 \end{array}\right]$$
Where $x_i, y_i \in \mathbb{Z}$ and in the range of $[70,2^{31}-1]$ for $i=1,\dots,4$. In this context we call each $x_i, y_i$ as single precision and $x_iy_j$ as double precision for $j=1,\dots,4$. We have two options of computing this matrix-vector product Single Lower Triangular matrix-vector product Partition into $2\times2$ sub-matrices that results into two Triangular matrix and a Topelitz matrix The number of operations for each will be $10$M(single precision)+$6$A(double precision) $9$M(single precision)+$3$A(single precision)+$6$A(double precision) What can be other ways of performing this matrix-vector multiplication? Such that the number of operations change like the above two techniques. Is there a way of reducing the number of double precision addition?","['matrix-equations', 'matrices', 'discrete-optimization', 'linear-algebra', 'discrete-mathematics']"
1637489,Difference between a limit and accumulation point? [duplicate],"This question already has answers here : Difference Between Limit Point and Accumulation Point? (8 answers) Closed 1 year ago . What is the exact difference between a limit point and an accumulation point? An accumulation point of a set is a point, every neighborhood of which has infinitely many points of the set. Alternatively, it has a sequence of DISTINCT terms converging to it? Whereas a limit point simply has a sequence which converges to it? i.e. something like $(1)^n$ which is a constant sequence. Is this the right idea? As much detail and intuition as possible would be greatly appreciated.","['real-analysis', 'soft-question']"
1637494,"If $f\in C[0,1]$ and $A\subset[0,1]$ is finite, can $f$ be approximated uniformly by polynomials that coincide with $f$ on $A$?","Let $f\colon[0,1]\to\mathbb{R}$ be continuous and $A$ a finite subset of $[0,1]$. Given $\epsilon>0$ is there a polynomial $p$ such that
$$
|f(x)-p(x)|\le\epsilon\quad\forall x\in[0,1]\quad\text{and}\quad p(a)=f(a)\quad\forall a\in A?
$$ This question arised while I was writing notes on Weierstrass's approximation theorem. By considering the interpolating polynomial of $f$ on $A$, we may asume without loss of generality that $f(a)=0$ for all $a\in A$. There are three situations in which I know the answer to be in the positive. $A$ is a singleton, say $A=\{a\}$. There is a polynomial $q$ such that $|f(x)-q(x)|\le\epsilon/2$ for all $x\in[0,1]$. Since $f(0)=0$ we have $|q(a)|\le\epsilon/2$. Take $p(x)=q(x)-q(a)$. $A=\{0,1\}$. There is a polynomial $q$ such that $|f(x)-q(x)|\le\epsilon/2$ for all $x\in[0,1]$. Then $|q(0)|,|q(1)|\le\epsilon/2$. Take $p(x)=q(x)-q(0)(1-x)-q(1)\,x$. $f$ is differentiable at all $a\in A$ (with he usual understanding that if $a=0$ or $1$ we mean one-sided differentiability.) Let $P_A=\prod_{a\in A}(x-a)$ and
$$
g(x)=\begin{cases}f(x)/P_A(x) & \text{if }x\notin A,\\f'(x)\Bigl(\prod\limits_{a\in A,a\ne x}(x-a)\Bigr)^{-1} & \text{if }x\in A.\end{cases}
$$
The $g$ is continuous on $[0,1]$. Let $M$ be a bound of $P_A$ on $[0,1]$. There exists a polynomial $q$ such that $|g(x)-q(x)|\le\epsilon/M$ for all $x\in[0,1]$. Let $p(x)=P_A(x)\,q(x)$. Do you know the answer for the general case?","['approximation-theory', 'real-analysis']"
1637511,Finding the square root of $6-4\sqrt{2}$,"I found this standupmaths video on YouTube about the A4 paper puzzle. I really liked the puzzle and managed to get the answer by using a calculator. However, the answer (which I won't spoil), led me to think that the equation to solve it might simplify - which it does. In the middle of the simplification, I got this expression: $$\sqrt{6-4\sqrt{2}}$$ which for other reasons I suspected to be equal to: $$\ 2-\sqrt{2}$$ I tried squaring the above and, sure enough, it does give: $$6-4\sqrt{2}$$ My question is, how would I have been able to find the square root of $$6-4\sqrt{2}$$ if I hadn't been able to guess at it? Is there a standard technique? I've tried looking on the web but don't really even know what to search for.","['algebra-precalculus', 'radicals']"
1637518,What does $f|_A$ mean? [duplicate],"This question already has answers here : What does this notation mean? F|U (4 answers) Closed 8 years ago . If $f$ a is a function and $A$ is a set, what could the notation $$f|_A$$ mean? Is it perhaps ""restricted to set $A$""?","['elementary-set-theory', 'notation', 'functions']"
1637521,Why are some solutions excluded if we simply multiply the denominator in an fraction inequality?,"I have the next inequality for which I have to find the solutions : $$\frac{2x-5}{3x-1}\geq 1, \text{ where } x\in\mathbb{R}$$ I know I have to subtract $1$ and then I have to analyse the sign for the grade 1 function for both numerator and denominator and then see for which intervals they have different signs... Why the approach of multiplying on both sides with $3x-1$ is incorrect?","['inequality', 'functions']"
1637559,Examples of non-Euclidean domains which have a universal side divisor,"Let $R$ be a ring. A nonzero nonunit $u \in R$ is called a universal side divisor if for every $x \in R$ there is some $z \in R$ such that $u$ divides $x - z$ in $R$ where $z$ is either zero or a unit, i.e., there is a type of division algorithm. This concept is used to demonstrate examples which are P.I.D. but not Euclidean. The existence of universal side divisors is a weakening of the Euclidean condition. I seek examples which are non-Euclidean domains which have universal side divisors.","['abstract-algebra', 'ring-theory', 'examples-counterexamples', 'algebraic-number-theory']"
1637577,"$G$ be a group of order $pn$ , where $p$ is a prime and $p>n$ , then is it true that any subgroup of order $p$ is normal in $G$?","Let $G$ be a group of order $pn$ , where $p$ is a prime and $p>n$  , then  is it true that any subgroup of order $p$ is normal in $G$  ? ( I know that any subgroup of index smallest prime dividing order of the group would be normal , but this thing is far away from it . Please help . Thanks in advance )","['finite-groups', 'group-actions', 'normal-subgroups', 'group-theory']"
1637621,Image of a family of circles under $w = 1/z$,"Given the family of circles $x^{2}+y^{2} = ax$, where $a \in \mathbb{R}$, I need to find the image under the transformation $w = 1/z$. I was given the hint to rewrite the equation first in terms of $z$, $\overline{z}$, and then plug in $z = 1/w$. However, I am having difficulty doing this. I completed the square in $x^{2}+y^{2}=ax$ to obtain $\left(x - \frac{a}{2} \right)^{2} + y^{2} = \left(\frac{a}{2} \right)^{2}$. Then, given that $\displaystyle x = Re(z) = \frac{z+\overline{z}}{2}$ and $\displaystyle y = Im(z)= \frac{-(z-\overline{z})}{2i}$, I made those substitutaions and my equation became $\left( \frac{z+\overline{z}-a}{2}\right)^{2} - \left(\frac{\overline{z}-z}{2} \right)^{2} = \left(\frac{a}{2} \right)^{2}$. Then, sbustituting in $z = \frac{1}{w}$, this became $\displaystyle \frac{\left(\frac{1}{w} + \frac{1}{\overline{w}} - a \right)^{2}}{4} - \frac{\left(\frac{1}{\overline{w}} - \frac{1}{w} \right)^{2}}{4} = \frac{a^{2}}{4}$. Beyond this, my algebra gets very wonky. Could someone please tell me what my final result should be? Knowing that would allow me to work backwards and then apply these methods to other problems (of which I have many to do!). Thanks.","['complex-analysis', 'complex-numbers', 'transformation']"
1637667,Show that $ \lim\limits_{n\to\infty}\frac{1}{n}\sum\limits_{k=0}^{n-1}e^{ik^2}=0$,"TL;DR : The question is how do I show that $\displaystyle \lim_{n\to\infty}\frac{1}{n}\sum_{k=0}^{n-1}e^{ik^2}=0$ ? More generaly the question would be : given an increasing sequence of integers $(u_k)$ and an irrational number $\alpha$, how do I tell if  $\displaystyle \lim_{n\to\infty}\frac{1}{n}\sum_{k=0}^{n-1}e^{2i\pi \alpha u_k}=0$ ? I'm not asking for a criterium for completely general sequences, an answer for sequences like $u_k=k^2$, $v_k=k!$ or $w_k=p(k)$ with $p\in \mathbf Z [X]$ would already be awesome. A little explanation about this question : In Real and Complex Analysis by Rudin there is the folowing exercise : Let $f$ be a continuous, complex valued, $1$-periodic function and $\alpha$ an irrational number. Show that 
$\displaystyle \lim_{n\to\infty}\frac{1}{n}\sum_{k=0}^{n-1}f(\alpha k)=\int_0^1f(x)\mathrm d x$. (We say that $(\alpha k)_k$ is uniformly distributed in $\mathbf R / \mathbf Z$) With the hint given by Rudin the proof is pretty straightforward : First one show that this is true for every $f_j=\exp(2i\pi j\cdot)$ with $j\in \mathbf{Z} $. Then using density of trigonometric polynomials in $(C^0_1(\mathbf{R}),\|\cdot\|_\infty)$ and the fact that the $0$-th Fourier coefficient of $f$ is it's integral over a period, one can conclude using a $3\varepsilon$ argument. This proof is possible because one can compute explicitly the sums $$\displaystyle \frac{1}{n}\sum_{k=0}^{n-1}e^{2i\pi j \alpha k}=\frac{1}{n}\cdot\frac{1-e^{2i\pi j\alpha n}}{1-e^{2i\pi j\alpha}}\longrightarrow 0 \text{ when }n\to\infty \text{ and }j\in \mathbf{Z}^*.$$ Now using a different approach (with dynamical systems and ergodic theorems) Tao show in his blog that $(\alpha k^2)_k  $ is uniformly distributed in $\mathbf R / \mathbf Z$ (corollary 2 in this blog ). I'd like to prove this result using the methods of the exercice of Rudin, but this reduce to show that $$\displaystyle \frac{1}{n}\sum_{k=0}^{n-1}e^{2i\pi j \alpha k^2}\longrightarrow 0 \text{ when }n\to\infty \text{ and }j\in \mathbf{Z}^*.$$ Hence my question. P.S. When i ask wolfram alpha to compute $\sum_{k\geq0}e^{ik^2}$ it answer me with some particular value of the Jacobi-theta function. Of course the serie is not convergent but maybe it's some kind of resummation technique or analytic continuation. I'm not familiar with these things but it might be interesting to look in that direction.","['equidistribution', 'real-analysis', 'exponential-sum', 'limits']"
1637668,Calculate the vector surface integral,"Let $V=\{(x,y,z)\in \mathbb{R}:\frac{1}{4}\le x^2+y^2+z^2\le1\}$ and $\vec{F}=\frac{x\hat{i}+y\hat{j}+z\hat{k}}{(x^2+y^2+z^2)^2}$ for $(x,y,z)\in V$. Let $\hat{n}$ denote the outward unit normal vector to the boundary of $V$ and $S$ denote the part $\{(x,y,z)\in \mathbb{R}^3:x^2+y^2+z^2=\frac{1}{4}\}$ of the boundary $V$. Then find $\int\int_S\vec{F}.\hat{n}dS$.
Here is what I did.
$$\int\int_S\vec{F}.\hat{n}dS$$
$$=\int\int_S\frac{x\hat{i}+y\hat{j}+z\hat{k}}{(x^2+y^2+z^2)^2}.(2x\hat{i}+2y\hat{j}+2z\hat{k})dxdy$$
$$\int\int_S2(x^2+y^2+z^2)^{-1}dxdy$$
$$\int\int_S\frac{2}{1/4}dxdy=8\int\int_Sdxdy=8\pi$$
where $\int\int_Sdxdy$ is the surface area of a sphere of radius $1/2$. Please advise on my solution.","['multivariable-calculus', 'proof-verification', 'vector-analysis']"
1637681,How many numbers of $10$ digits that have at least $5$ different digits are there?,"In principle I resolved it as if the first number could be zero, to the end eliminate those that start with zero. The numbers that can use $4$ certain figures (for example, $1$, $2$, $3$ and $4$) are $4^{10}$. The numbers that can use any $4$ digits are ${10\choose 4}\cdot 4^{10}$ I'm saying ""they can use,"" which does not mean that use; however this is very advantageous for this problem, for those who ""can use"" includes four digits using $4$ digits, which use $3$ to $2$ and using those who only use one. So answering the question of the problem, the answer is: ""All ten-digit numbers except those who can only use four digits""
\begin{align}
&= 10^{10} - {10\choose 4} \cdot 4^{10}\\
&= 10^{10} - 210 \cdot 4^{10}
\end{align}
There is no reason to believe that the figures have some asymmetric distribution, so it is obvious that for all these numbers, the tenth start with zero. Since starting with zero are not exactly ten-digit numbers, we discard it. The solution is:
\begin{align}
\tfrac 9{10} (10^{10} - 210 \cdot 4^{10})&= 9(10^9 - 21 \cdot 4^{10})\\
&= 8,801,819,136
\end{align}
But I'm not sure this reasoning is correct.","['permutations', 'combinatorics', 'factorial', 'integer-partitions']"
1637689,"Is this limit finite, or infinite?","Is
$$\lim_{x\uparrow 1}\left(\frac{1-x}{x}\max\{nx^n|n\in\mathbb{N}\} \right)$$
infinite, or finite? ($\mathbb{N}$ is the set of the natural numbers).
According to Mathematica, it looks like converging to between 0.3 and 0.4... Thanks for your help!","['optimization', 'sequences-and-series', 'calculus', 'limits']"
1637708,"True or false? ""sum of an m-strongly convex and a convex function is m-strongly convex""","I would like to know if the following conjecture is true or false? If $f(x) = g(x) + h(x)$ where $g$ is m-strongly convex and $h$ is convex, then $f$ is m-strongly convex. NOTE: For a non-differentiable function $F$, m-strongly convexity 
means $F(y) \geq F(x) + g^T(y - x) + \frac{m}{2} ||y - x||^2, \forall x, y$ where $g \in \partial F(x)$ is a subgradient of $F$ at $x$. If $F$ is differentiable, m-strongly convexity can also be defined as $\nabla^2 F(x) \succeq m I, \forall x$ where $I$ is the identity matrix. You can see the wikipedia page , this blog post by Sébastien Bubeck, or these lecture notes from the Algorithms course at Cornell University for more details on strong convexity.","['optimization', 'functions', 'convex-analysis']"
1637719,"Given $3$ types of coins, how many ways can one select $20$ coins so that no coin is selected more than $8$ times.","So I was given this question. Given $3$ types of coins, how many ways can one select $20$ coins so that no
coin is selected more than $8$ times. First I make $x_1 + x_2 + x_3 = 20$ Then $ 0 \leq x_i \leq 8$ Then we use the Inclusion exclusion principle Let $A_i$ be the non-negative integer solutions to $x_1 + x_2 + x_3 = 20$ $x_i \geq 9$. Then use inclusion exclusion formula to find $N(A_1 \bigcap A_2 \bigcap A_3)$ What i don't get is how to apply the inclusion exclusion formula. I know the process to get to it but not how to apply it.","['inclusion-exclusion', 'discrete-mathematics']"
1637729,Prove that all automorphisms of the line $\Bbb A^1$ are of the form $f(x) = ax + b$ with $a\neq 0$.,An isomorphism $f : X → X$ of a closed set $X$ to itself is called an automorphism. Prove that all automorphisms of the line $\Bbb A^1$ are of the form $f(x) = ax + b$ with $a\neq 0$. I think I can take the co-ordinate ring and again closed subsets are finite here.So do I use these efficiently?,['algebraic-geometry']
1637732,Nested Quantification of exactly one.,"Suppose my domain is ""All students in the class"" and P(x, y):= x has emailed y. So, how do i define: Every student has emailed exactly one student. Exactly one student has emailed every one. A proper explanation would be really helpful.","['predicate-logic', 'quantifiers', 'discrete-mathematics']"
1637740,Prove that $\dim(U+W) + \dim(U\cap W) = \dim U + \dim W$,"Let $V$ be a vector space over a field $k$ and let $U$ , $W$ be finite-dimensional subspaces of $V$ . Prove that $\dim(U+W) + \dim(U\cap W) = \dim U + \dim W$ I'm given that to begin this problem I can find the bases: $\{v_1,\dots,v_p\}$ for $U\cap W$ $\{v_1,\dots,v_p, u_1,\dots,u_q\}$ for $U$ and $\{v_1,\dots,v_p, w_1,\dots,w_r\}$ for $W$ and then I just need to show that $\{v_1,\dots,v_p, u_1,\dots,u_q, w_1,\dots,w_r\}$ is a basis for $U+W$ . My question is: how does one go about showing that it is a basis for $U+W$ and then use that to prove the above question? Side note: This question has already been asked here: Given two subspaces $U,W$ of vector space $V$, how to show that $\dim(U)+\dim(W)=\dim(U+W)+\dim(U\cap W)$ However, the first answer given does not apply to solving it the way I want to with finding the bases. The second answer simply gives me what I already knew to start with. Thus, I am asking this question again since I'm asking how to solve it a particular way instead of just any general hints towards solving it.","['proof-writing', 'linear-algebra', 'vector-spaces']"
1637748,Limit in number theory,"I was given the following thing to prove: $$\lim_{n \to \infty} {d(n) \over n} = 0$$
where $d(n)$ is the number of divisors of n. I'm so sure how to approach this question. One way I thought of is to use the UFT to turn the expression to: $$\lim_{n \to \infty} {\prod (x_i + 1) \over \prod p_i^{x_i}}$$ And then to use L'Hôpital's rule for each $x_i$, so I get something like this: $$\lim_{n \to \infty} {1 \over \ln (\sum p_i) \prod p_i^{x_i}}$$ That equals zero. Is this a good approach? Is there a different way to solve this?","['number-theory', 'limits']"
1637792,Does continuous extension of a function and its densely defined derivative imply everywhere differentiability?,"Let $V \subset \mathbb R^n$ be a closed set, and let $U \subset V$ be open as a subset of $\mathbb R^n$ and dense in $V$. Let $f:V \to \mathbb R$ and $G: V \to \mathbb R^n$ be continuous, with $G = \nabla f$ on $U$. Does it follow that $G = \nabla f$ on the interior of $V$? To put it another way, is $(f,G)$ a Whitney field? The obvious thing to do is the following: let $x \in V \setminus U$. We want to show that $f(x+h) - f(x) - \langle G(x),h \rangle = o(|h|).$ Choose $x_\epsilon$ arbitrarily close to $x$ in $U$, and write $$f(x+h) - f(x) - \langle G(x),h \rangle = f(x+h) - f(x_\epsilon) - \langle G(x_\epsilon),h-x_\epsilon \rangle + f(x_\epsilon) - f(x) $$ $$+ \langle G(x_\epsilon) - G(x), h \rangle - \langle G(x_\epsilon), x - x_\epsilon \rangle.$$ We control the first three terms using the differentiability of $f$ at $x_\epsilon$, the next two using the continuity of $f$ and the fact taht we can take $|x - x_\epsilon|$ as small as we like, and similarly for the last two. Unfortunately, we choose $x_\epsilon$ depending on $h$, and we don't know that $f$ is uniformly differentiable on $U$, so this doesn't quite work.","['derivatives', 'analysis']"
1637831,What is an integral differential form and how do we recognize it as such?,"I am reading about embedding theorems of various types of manifolds (Kodaira's embedding theorem being one of them), and one condition that repeats in all of them is that the manifolds should be endowed with an integral 2-form. What does this mean? Let me take a chance at guessing an answer: a $p$-form is integral if its pairing against any $p$-chain is an integer. Alternatively, the $p$-form should be in the image of the embedding of $H^p _{simplicial} (M, \Bbb Z)$ into $H^p _{deRham} (M, \Bbb R)$ (would anything change if I complexified the manifold and used $\Bbb C$ instead of $\Bbb R$?) Is my guessing correct? Checking whether a form is integral seems very complicated using the definition. Are there more humane sufficient conditions implying this integrality test (I'm mostly interested in symplectic forms)? EDIT: Let me try to resuscitate this, because I am still confused. So far, I have seen three ways of defining the integrality of the cohomology class $[\omega]$ of a closed $2$-form $\omega$: by requiring $[\omega]$ to belong to the image of $H^2 (M, \Bbb Z)$ in $H^2 (M, \Bbb R)$; this is the most concise but also the most abstract and difficult to work with version; by requiring that $\int _S \omega \in \Bbb Z$ for all closed oriented surfaces $S$ embedded in $M$; the most technical of them all: for any cover $M$ with (contractible? is this necessary?) open sets $U_i$ on which $\omega = \textrm d \theta_i$; on $U_i \cap U_j$ we have $\textrm (\theta_i - \theta_j) = \textrm d \textrm d \omega = 0$, so $\theta_i - \theta_j = \textrm d f_{ij}$; note that on $U_i \cap U_j \cap U_k$ we have $\textrm d (f_{ij} + f_{jk} + f_{ki}) = \theta_i - \theta_j + \theta_j - \theta_k + \theta_k - \theta_i = 0$, so $f_{ij} + f_{jk} + f_{ki} = c_{ijk} \in \Bbb R$; $[\omega]$ is said to be integral if and only if $c_{ijk} \in \Bbb Z \ \forall i,j,k$. My problem is that I fail to understand whether the three of them are equivalent, and if so - why. For instance, (2) is almost an immediate corollary of (1), but does (2) imply (1)? Why may I test integrality by using just embedded smooth oriented surfaces, instead of general cycles from $Z_2 (M, \Bbb Z)$? Number (3)'s connection with the others baffles me.","['homology-cohomology', 'differential-topology', 'smooth-manifolds', 'differential-forms', 'differential-geometry']"
1637836,Estimating the sum $\sum_{y \in \Bbb{Z}^d} (|y|+1)^{-\alpha}(|x-y|+1)^{-\beta}$ as $|x| \to \infty$,"I would like to know a rather precise asymptotics of the sum $$ S(x) = S_{\alpha,\beta}(x) :=  \sum_{y \in \Bbb{Z}^d} \frac{1}{(|y| + 1)^{\alpha}(|x-y| + 1)^{\beta}}$$ as $|x| \to \infty$. Here, $\alpha, \beta > 0$. I suspect that $$ S(x) = \Theta_{\alpha,\beta}( |x|^{-(\alpha+\beta-d)} ) $$ for each fixed $\alpha, \beta$ with $\alpha+\beta > d$. However, I would like to know about this asymptotics in a better precision, such as an expansion of the form $$ S(x) = \frac{a_0}{|x|^{\alpha+\beta-d}} + \frac{a_1}{|x|^{\alpha+\beta+1-d}} + \cdots $$ as $|x| \to \infty$, with constants $a_0, a_1, \cdots$ depending only on $d$, $\alpha$ and $\beta$. Even a keyword would be appreciated. Remark. It is easy to see that for some constants $c, C > 0$ depending only on the dimension $d$, we have $$ c^{\alpha+\beta} I(x) \leq S(x) \leq C^{\alpha+\beta} I(x), $$ where $I$ is defined as $$ I(x) = I_{\alpha,\beta}(x) := \int_{\Bbb{R}^d} \frac{1}{(|y|^2 + 1)^{\alpha/2}(|x-y|^2 + 1)^{\beta/2}} \, dy $$ This one is easier to deal with, and in fact we have $$ I(x) = \frac{2\pi^{d/2}\Gamma(\frac{\alpha+\beta-d}{2})}{\Gamma(\frac{\alpha}{2})\Gamma(\frac{\beta}{2})} \int_{0}^{\pi/2} \frac{\cos^{\alpha-1}\theta\sin^{\beta-1}\theta}{(|x|^2 \cos^2\theta\sin^2\theta + 1)^{(\alpha+\beta-d)/2}} \, d\theta. $$ This might be used to give a good guess on the leading coefficient of the expansion, but I am not confident about this. Or, recognizing $S(x)$ as a convolution might be helpful in conjunction with Fourier analysis technique, but I am even less confident about this.","['real-analysis', 'asymptotics', 'analysis']"
1637855,Find the two values of $k$ for which $2x^3-9x^2+12x-k$ has a double real root.,"Find the two values of $k$ for which $2x^3-9x^2+12x-k$ has a double
  real root. I've found one method which is to equate $$2x^3-9x^2+12x-k=2(x-r)^2(x-c)$$ Expanding and equating coefficients I get the system of equations :
\begin{array}
\space 2(c+2r) &=9 \\
2r(2c+r) &=12 \\
-2r^2c&=k \\
\end{array} Solving this I've found the solutions $k=4,5$ However, I would like to know if there's some easier solution than the
  one I've already found, or  in general, if there's any other solution to the
  problem.","['algebra-precalculus', 'cubics', 'roots', 'polynomials']"
1637907,sum of open balls in normed space,"Let $X$ be a normed nonempty space and $x \in X$ . We define the set: $$B(x,r)=\{y \in X:\|y-x\|<r\}$$ ; I have to show that: $$B(x+x',r+r')=B(x,r)+B(x',r')$$ . I proved an inclusion like that: let $a \in B(x,r)+B(x',r')$ ; so $a=a_{1}+a_{2}$ , with $a_{1}\in B(x,r)$ and $a_{2}$ in the other ball; we have that $$\|x-a_1\|<r, \|x'-a_2\|<r'$$ ; suming up, we obtain: $\|x+x'-a_1-a_2\|\le \|x-a_1\|+\|x-a_2\|<r+r'$ ; so $\|x+x'-a\|<r+r'$ and we obtain that $a \in B(x+x',r+r')$ ; for the reverse inclusion, I dont see a solution at all; let $a \in B(x+x',r+r'$ ; so we have that $\|x+x'-a\|<r+r'$ ; I have to find $a_1,a_2 $ , such that $a=a_1+a_2$ and $a_1\in B(x,r)$ and $a_2$ in the other ball. Is a useful idea to use that these balls are convex?",['functional-analysis']
1637910,How does Hartshorne's definition of group schemes encode the law for the neutral element?,"Hartshorne's Algebraic Geometry says A scheme $X$ with a morphism to another scheme $S$ is a group scheme over $S$ if there is a section $e\colon\;S\to X$ (the identity) and a morphism $\rho\colon\;X\to X$ over $S$ (the inverse) and a morphism $\mu\colon\;X\times X\to X$ over $S$ (the group operation) such that (1) the composition $\mu\circ(\operatorname{id}\times\rho)\colon\;X\to X$ is equal to the projection $X\to S$ followed by $e$, and (2) the two morphisms $\mu\circ(\mu\times\operatorname{id})$ and $\mu\circ(\operatorname{id}\times\mu)$ from $X\times X\times X\to X$ are the same. Clearly those two demands formalize that $\rho$ is a right-inverse and $\mu$ is associative. However, I miss some statement concerning the (right-)neutrality of $e$: I would expect something like The morphism $\mu\circ(\operatorname{id}\times e)\circ(X\overset\sim\to X\times_S S)$ is the identity. Does this somehow already follow from the cited definition?","['algebraic-geometry', 'schemes', 'group-schemes', 'category-theory', 'definition']"
1637912,Can we take a logarithm of an infinite product?,"Suppose we have an infinite product $S = \prod_{n=1}^{\infty} a_n$ of positive real numbers. Then is it always the case that
$$
\log(S) = \sum_{n=1}^{\infty} \log a_n ?
$$ I am sure this is the case, but I wanted to make sure. 
Thank you!","['real-analysis', 'calculus']"
1637951,Why is $s$ used for arc length?,"Why is $s$ used for arc length? I looked around online, but I can't find a definite answer. Thank you!","['math-history', 'notation', 'trigonometry', 'soft-question', 'differential-geometry']"
1637977,"Salem Numbers, roots on the unit circle","There are algebraic integers which are not roots of unity , for example consider the irreducible polynomial $ P(x)= x^4-2x^3-2x+1 $. A computer software can show that this polynomial has two real roots outside the unit circle(one greater than one and the other less than one) and two roots on the unit circle. However I don't know how to prove this rigorously that there are two roots on the unit circle. Usually when I wanted to prove some root of a polynomial is on the unit circle , I'd multiply that by some other polynomial to get something of the form $ x^n - 1 $ and it's obvious that every root of such expression has norm one, however , in this case this is not possible since none of roots of $ P(x) =0 $ are roots of unity. Actually there is a whole lot of examples, called Salem numbers . It's an algebraic integer $\lambda > 1$ such that all of its Galois conjugates are on the unit circle except $\frac{1}{\lambda}$. The polynomial given above was an example of a minimal polynomial of a Salem number. Does anyone have any idea how can I prove this, i.e. roots are on the unit circle except two of them?(I'm looking for a method that can be applied to more than just one example, hopefully lots of Salem numbers)","['number-theory', 'abstract-algebra']"
1637980,Help with understanding a proof of compact surface having an elliptic point,"In my studies of differential geometry from do Carmo's book, I have come across a very nice claim which states that a regular compact surface has an elliptic point that is a point with positive Gaussian curvature
I have read the proof and it said that looking at the normal sections at a point where the surface and a sphere are tangent, we see that the normal curvatures at this point of the surface is greater than those for the sphere known to be positive. I have no idea how they reached this conclusion as I do not see any obvious relation between normal sections of the sphere and surface aside from the surface and sphere being tangent, and I certainly would appreciate the help explaining it. I have attached the proof here highlighting the conclusion I do not know how to deduce, I thank all helpers","['curvature', 'differential-geometry', 'surfaces']"
1637986,Clarke's generalized gradient formula computed on functions defined on open sets,"In the book [1], Clarke et al. define the generalized gradient for a Lipschitz function $f:\mathbb{R}^n\to\mathbb{R}$ as follows. 8.1. Theorem (Generalized Gradient Formula). Let $x\in\mathbb{R}^n$, and let $f:\mathbb{R}^n\to\mathbb{R}$ be Lipschitz near $x$. Let $\Omega$ be any subset of zero measure in $\mathbb{R}^n$, and let $\Omega_f$ be the set of points in $\mathbb{R}^n$ at which $f$ fails to be differentiable. Then,
  \begin{equation*}
 \partial f(x):=co\{\lim\nabla f(x_i):x_i\to x, x_i\notin\Omega,x_i\notin\Omega_f\}
\end{equation*} In other words, the generalized gradient of $f$ at $x$ is the convex hull whose elements are the limiting points of the gradient of $f$ computed at the elements of sequences converging to $x$. Moreover, these elements of sequences do not belong to any set of measure zero nor to the set of points where $f$ fails to be differentiable. Here follows my question. If $f$ is defined on any open subset $S$ of $\mathbb{R}^n$, i.e., $f:S\to\mathbb{R}$, does the above formula hold as below Let $x\in S$, and $\Omega(S)$ be any subset of $S$ with measure zero with respect to $\mathbb{R}^n$, and let $\Omega_f(S)$ be the set of points in $S$ at which $f$ fails to be differentiable. Then,
\begin{equation*}
 \partial f(x):=co\{\lim\nabla f(x_i):x_i\to x, x_i\notin\Omega(S),x_i\notin\Omega_f(S)\} ?
\end{equation*} My answer is no, because $S$ is open. Consequently, $S$ does not contains the limit $\lim\nabla f(x_i)$. Thus, $\partial f(x)$ may not be defined in $S$. Is this reasoning correct? References [1] Clarke et al, ""Nonsmooth Analysis and Control Theory"", Springer 1998","['derivatives', 'real-analysis', 'control-theory', 'functional-analysis', 'non-smooth-analysis']"
1637989,How to numerically test a limsup? (Example : numerical simulation of the law of iterated logarithm),"I have a random walk $S_n$ (the increments are Bernoulli $\pm 1$ with probability $1/2$ each). I'd like to test numerically the Law of iterated logarithm : $$\limsup_{n \rightarrow \infty} \underbrace{\frac{S_n}{\sqrt{2 n (\log \log n)}}}_{Y_n} = 1, \qquad \rm{a.s.}$$ My attemps have failed (see this question ) since, when you do a numerical simulation, you can never evaluate this quantity that would be required for the $\limsup$ evaluation (because the computer memory is not infinite...): $$Z_k=\sup_{\ell \geq k}Y_{\ell}$$ but only: $$Y_{k,n}=\max_{k\leq \ell \leq n}Y_\ell$$ Question: how can you do a simulation that showcases that the $\limsup$ is $1$ ? (and have a plot showing a convergence to 1, in the contrary of this failed attempt ). Sidenote: in my case, the increments are not exactly independent, but close to it. I'd like to numerically test if a law-of-iterated-logarithm-like result holds. But for now, I would already be more than happy if I could get a numerical evidence of the standard law in the standard case where increments are independent. Sidenote2: code for failed attempt: import numpy as np
import matplotlib.pyplot as plt
N = 10*1000*1000
B = 2 * np.random.binomial(1, 0.5, N) - 1       # N independent +1/-1 each of them with probability 1/2
B = np.cumsum(B)                                # random walk
plt.plot(B); plt.show()
C = B / np.sqrt(2 * np.arange(N) * np.log(np.log(np.arange(N))))
M = np.maximum.accumulate(C[::-1])[::-1]        # limsup, see http://stackoverflow.com/questions/35149843/running-max-limsup-in-numpy-what-optimization
plt.plot(M); plt.show()","['simulation', 'random-walk', 'probability-theory', 'probability', 'random-variables']"
1637997,"Show that the vector field $\vec F=(xf(u),xg(u))$ is not conservative","I'm trying to prove that the vector field  $\vec F=(xf(u),xg(u))$ with $u=xy$ is not conservative. I suppose that there is a function $\phi$ so that $\nabla \phi= \vec F$. So I need to satisfy that: $$\frac {\partial \phi}{\partial x}= yf(u)$$ $$\frac {\partial \phi}{\partial y}= xg(u)$$ Calculating the mixed partial for each I get that: $$\frac {\partial^2 \phi}{\partial x \partial y}= f(u)+y \frac{\partial f(u)}{\partial y}$$ $$\frac {\partial^2 \phi}{\partial y \partial x}= g(u)+x \frac{\partial g(u)}{\partial x}$$ Since mixed partials are equal I have that: $$f(u)+y \frac{\partial f(u)}{\partial y}=g(u)+x \frac{\partial g(u)}{\partial x}$$ and $\frac {\partial f(u)}{\partial y}=\frac {df}{du}\frac {\partial u}{\partial y}= x\frac{df}{du}$, applying this to both sides I end up with: $$f(u)+u \frac{df}{du}=g(u)+u \frac{dg}{du}$$
This is where I'm stuck, I'm not quite sure how to get a contradiction out of this or how I can show that this statement cannot hold. Should I have taken a different approach to this problem?","['multivariable-calculus', 'vector-fields', 'vector-analysis']"
1638044,An obvious pattern to $i\uparrow\uparrow n$ that is eluding us all?,"Start with $i=\sqrt{-1}$. This will be $a_1$. $a_2$ will be $i^i$. $a_3$ will be $i^{i^{i}}$. $\vdots$ etc. In Knuth up-arrow notation : $$a_n=i\uparrow\uparrow n$$ And, amazingly, you can evaluate $\lim_{n\to\infty}a_n=\lim_{n\to\infty}i\uparrow\uparrow n=e^{-W(-\ln(i))}\approx0.4383+0.3606i$. You can check this, it does indeed converge to this value. In fact, I decided to make a graph of $a_n$ to show that it converges. (y axis is imaginary part, x axis is real part.) And, to little astonishment, I quickly noticed that there is an apparent pattern to the graph. Commonly, we define $x\uparrow\uparrow0=1$, which I have included in the graph. So the pattern seems very obvious.  It follows a curved path that converges onto the point that was given above. And, if you connect the dots, starting with the first point (given on the left as the first point) and trace a nice line to the second, third, and so fourth numbers, you will find an interesting spiral.  I thought that at first, this spiral was writable as an equation, but apparently, there are a few implications. You will notice that the blue dots are way closer to the converging point and that the red and black dots are a little closer.  So whatever equation you can come up with should account that $a_{3n}$ is closest to the number you are trying to converge to. I want (so desperately) to see if anyone can come up with an equation that allows the computation of $a_{0.5}$ that satisfies $$i^{a_{0.5}}=a_{1.5}$$a well known identity you can find on the Wikipedia . At first glance of the graph I went on to think that perhaps, just perhaps, I (or you) could find a formula that allows us to define $i\uparrow\uparrow 0.5$. If you are familiar with De'Moivres formula, it is a formula that allows us to perform compute $$\sqrt{i}$$ with relative ease.  It was derived when De'Moivre noticed an interesting pattern to $(a+bi)^n$.  He proceeded to write his formula concerning the distance from zero and the angle from the positive real axis. So I must tell you that I wish for the same to occur with $i\uparrow\uparrow n$.  Perhaps the answer lies in using a different coordinate system.  Perhaps the answer lies in calculating the distance one of the points on one of the lines (black, red, or blue) is from the converging spot and the adding in the angle at which the next point changes. My progress on determining such a formula has gone nowhere.  The most I can say is that $a_n$ is probably not chaotic and does indeed converge in a way that is most certainly not random.","['algebra-precalculus', 'tetration', 'power-towers']"
1638073,Showing that a function is uniformly continuous but not Lipschitz,"If $g(x):= \sqrt x $ for $x \in [0,1]$, show that there does not exist a constant $K$ such that $|g(x)| \leq K|x|$ $ \forall x \in [0,1]$ Conclude that the uniformly continuous function $g$ is not a Lipschitz function on interval $[0,1]$. Necessary definitions: Let $A \subseteq \Bbb R$. A function $f: A \to \Bbb R$  is uniformly continuous when:
Given $\epsilon > 0$ and $u \in A$ there is a $\delta(\epsilon, u) > 0$ such that $ \forall x \in A$ and $|x - u| < \delta(\epsilon,u)$ $\implies$ $|f(x) - f(u)| < \epsilon$ A function $f$ is considered Lipschitz if $ \exists$ a constant $K > 0$ such that $ \forall x,u \in A$ $|f(x) - f(u)| \leq K|x-u|$. Here is the beginning of my proof, I am having some difficulty showing that such a constant does not exist. Intuitively it makes sense however showing this geometrically evades me. Proof (attempt): Suppose $g(x): = \sqrt x$ for $x \ in [0,1]$
Assume $g(x)$ is Lipschitz. $g(x)$ Lipschitz $\implies$ $\exists$ constant $K > 0$ such that $|f(x) - f(u)| \leq K|x-u|$ $\forall x,u \in [0,1]$. Evaluating geometrically: $\frac{|f(x) - f(u)|}{ |x-u|}$ = $\frac{ \sqrt x - 1}{|x-u|}$ $ \leq K$ I was hoping to assume the function is Lipschitz and encounter a contradiction however this is where I'm stuck. Can anyone nudge me in the right direction?","['uniform-continuity', 'real-analysis', 'lipschitz-functions']"
1638079,lower bound on volume of balls,"It is well known that a lower bound on Ricci curvature gives an upper bound on the volume of balls. What are conditions that gives a lower bound on the volume of balls? It is reasonable to think that an upper bound on curvature would give some kind of lower bound on the volume of balls, is this not true? If not, what are some counter-examples? Here are two results I found in Schoen and Yau's book on Differential Geometry: 1) Given an upper bound on sectional curvature $K_M \leq b^2$, if the metric at $x \in M$ in normal coordinates has the form $ds^2 = dr^2 + r^2g_{ij}(r,\theta)d\theta^id\theta^j$, then $Vol(B_x(R)) \geq C(n,b,R)$ (this constant is independant of $x$). This is on page 11. I don't really understand the condition on the metric, I guess this says that the metric is ""comparable to Euclidean metric"" somehow? However, they don't say what conditions should hold on those $g_{ij}$'s. Obviously, one could just write 
$$dr^2 + \tilde{g}_{ij}(r,\theta)d\theta^id\theta^j = dr^2 + r^2(r^{-2}\tilde{g}_{ij}(r,\theta))d\theta^id\theta^j$$
and take $g_{ij} = r^{-2}\tilde{g}_{ij}$ but then the condition on the metric is vacuous... On page 29, they have another result: 2) Given a lower bound on Ricci, $Ric \geq -(n-1)K$ and a point $p \in M$, we have a lower bound $Vol(B_x(1)) \geq e^{-C\rho(x)}$ for $C = C(n,K,Vol(B_p(1)))$ and $\rho(x) = d(x,p)$. This is interesting but it is far from uniform in $x$. Do you know of some other results of this kind?","['riemannian-geometry', 'differential-geometry']"
1638098,Hillary Clinton's Iowa Caucus Coin Toss Wins and Bayesian Inference,"In yesterday's Iowa Caucus, Hillary Clinton beat Bernie Sanders in six out of six tied counties by a coin-toss *. I believe we would have heard the uproar about it by now if this was somehow rigged in her favor, but I wanted to calculate the odds of this happening, assuming she really was that lucky, and assuming she rigged various numbers of the tosses. * As many people have pointed out already, this turned out to be a selective data set - Sander's won just about as many coin tosses as Mrs. Clinton did. Read on if you still care about the problem. At first I calculated the odds using simple rules for the probabilities of independent events: $$ P(6\text{H})=6*P(\text{H})=\left( \frac{1}{2} \right)^{6}= \frac{1}{64} \approx 1.56\% $$ i.e. naively, there was a 1.56% chance it was fair . But I vaguely remembered from reading about Bayesian inference that we can make a more educated statement about whether or not this was fair using Bayes' Theorem, and assuming various numbers of the coin tosses were rigged. I tried it out myself, and here's what I came up with, but I'm fairly positive I worked this out incorrectly, so here's hoping you wonderful people can help. Here's my shot at it: Example assuming it was fair (0% chance it was rigged): $$ P(6\text{H}) = \underbrace{P(6\text{H}|\text{fair})}_{1/64}\underbrace{P(\text{fair})}_{1} + \underbrace{P(6\text{H}|\text{not fair})}_{1}\underbrace{P(\text{not fair})}_{0} = \frac{1}{64} $$ and by Bayes Theorem: $$ P(\text{fair}|6\text{H}) = \frac{P(6\text{H}|\text{fair})P(\text{fair})}{P(6\text{H})} = \frac{(1/64)(1)}{(1/64)}=1 $$ (obviously). Assuming $n$ of the tosses were rigged: $$ P(6\text{H}) = \underbrace{P(6\text{H}|\text{fair})}_{\left(\frac{1}{2}\right)^{6}}\underbrace{P(\text{fair})}_{1-\frac{n}{6}} + \underbrace{P(6\text{H}|\text{not fair})}_{1}\underbrace{P(\text{not fair})}_{\frac{n}{6}} = \frac{6-n}{384} + \frac{n}{6} = \frac{63n+6}{384} $$ and by Bayes' Theorem: $$ P(\text{fair}|6\text{H}) = \frac{P(6\text{H}|\text{fair})P(\text{fair})}{P(6\text{H})} = \frac{\left(\frac{1}{64}\right)\left(\frac{6-n}{6}\right)}{\left(\frac{63n+6}{384}\right)}=\frac{6-n}{63n+6} $$ Here's a plot of the probabilities that the coin tosses were fair given an assumption of $n$ unfair coins: Questions: I'm pretty sure some of my assumptions for probabilities were off in various parts of this - if so, where did I go wrong? On the off chance I carried this out correctly, what can be made of these results? For example, is it most probable that there were 0, 1, or 2 coin tosses that were unfair, as making the assumption that there were $n<3$ unfair coins gives a probability $P(\text{fair}|6\text{H})$ greater than the $1/64$ chance it was fair? EDIT: @Eric Wofsey Informed me that I was calculating the wrong probability. What I really wanted to calculate was $P(0|6H)$, the probability of 0 coins being rigged, considering an outcome of 6 heads. What I learned (I'm new to Bayesian inference) is that it all depends upon your prior guess as to the probability that n of the coins were rigged. As he pointed out: $$ P(0|6H) = \frac{P(0)}{\sum_{i=0}^{6}2^iP(i)} $$ where $$ P(n) = {6 \choose n}p^n(1-p)^{6-n} $$ and $p$ is the prior probability that each coin toss was rigged. Here's what $P(0|6H)$ looks like fully expanded (assuming the prior $p$ is the same for each $P(n)$): As I learned, the prior probability is arbitrarily chosen, and represents your belief/guess as to the likelihood that the coins were rigged. I was interested in looking at what the distribution of $P(0|6H)$ looked like for values of $p$ from 0 to 1 (0 meaning you believe there's no possibility the coins were rigged, 1 meaning you're certain the coins were rigged). Here's the plot: I may be going way off the reservation here, but if this graph represents values of $P(0|6H)$ for prior probabilities of having rigged coins, wouldn't the integral of this from $p=0$ to $1$ represent the total probability of 0 rigged coins, considering an outcome of 6 heads, with each prior $p$ weighted equally? Whether or not I'm abusing the maths, the integral evaluates to: $$ \int_{0}^{1} P(0|6H)(p) \ dp = 0.0822\dots $$ Note: I'm thinking in retrospect that the prior $p$ should probably be different for every $P(n)$ and assuming they're the same for each $P(n)$ is likely problematic, but I thought I'd share my process anyway. EDIT 2 : On further thought, it seems like what I really want to compute is the integral: $$\int{\int{\int{\int{\int{\int{\int P(0|6H) \ d p_0 \ d p_1 \ d p_2 \ d p_3 \ d p_4 \ d p_5 \ d p_6}}}}}}$$ where $$ P(0|6H) = \dfrac{(1-p_0)^6}{(1-p_0)^6 + 12p_1(1-p_1)^5 + 60p_2^2(1-p_2)^4 + 160p_3^3(1-p_3)^3 + 240p_4^4(1-p_4)^2 + 192p_5^5(1-p_5) + 64p_6^6} $$ and $$ p_0 + p_1 + p_2 + p_3 + p_4 + p_5 + p_6 = 1 $$ and $p_n$ is the prior probability that $n$ coins are rigged. I have absolutely no idea how one would go about even thinking about evaluating this integral - it seems as though there are a range of values for the integral anyway, depending on the choices of $p_n$. It seems it is definitely possible given specific choices for $p_n$ and maybe even a distribution for the $p_n$s, dependent on n, such that the distribution is still normalized, like a weighted decaying distribution or something (gets less likely as n increases that that number of coins was rigged). Happy Tuesday","['bayesian', 'recreational-mathematics', 'probability', 'soft-question']"
1638125,Quotient space of the reals by the rationals,"Let $\mathbb{R}/{\sim}$ be the quotient space given by the equivalence relation $a \sim b$ if $a$ and $b$ are rational. I am trying to understand general properties of the quotient topology and this example seems worth fleshing out in full. It's also a very strange example to me so I'd appreciate feedback on what I've figured out so far. In order to figure out what the topology on $\mathbb{R}/{\sim}$ looks like we need to examine where the surjection $\pi: \mathbb{R} \to \mathbb{R}/{\sim}$ sends open sets in $\mathbb{R}$. Now any open interval $U \subset \mathbb{R}$ contains both irrational and rational points; the rationals all get sent to the same point $q$ while the irrationals get sent to separate points. So an open set in $\mathbb{R}/{\sim}$ is similar to an open set in the irrationals as a subspace of the reals (with the caveat that all open sets in $\mathbb{R}/{\sim}$ share the rational point $q$). Is this space connected? I believe so as I can't think of a proper separation. As Alex notes below this is not correct: My professor also mentioned this space is an example where a compact subset, namely the irrationals, is not closed. As for compactness I think it is for this reason: the rationals are dense in $\mathbb{R}$, so if we put an open neighborhood around each rational then we will cover $\mathbb{R}$. Similarly, if we put an open neighborhood around the rational point $q \in \mathbb{R}/{\sim}$, then this single neighborhood will contain all irrational points and thus be a finite cover of $\mathbb{R}/{\sim}$ . Are there any other significant properties of this space I should know about? In particular, is it homeomorphic to anything notable? Does it serve as a useful counterexample for any other important properties? And does this particular topology have a name?","['general-topology', 'quotient-spaces']"
1638132,Showing that a function is injective,"I am trying to show that the following function is injective in some neighborhood of $(0, 0)$: $f:\mathbb R^2 \rightarrow \mathbb R^2$ given by $$f(x, y)=(\sin(x^3)\cosh(y), \cos(x^3)\sinh(y))$$ I tried to use the inverse function theorem but $df(0, 0)$ is not invertible.","['multivariable-calculus', 'inverse-function-theorem']"
1638134,Aren't Legendre's conjecture and Andrica's conjecture same?,"If Legendre's conjecture is true, couldn't we easily obtain $\sqrt{p_{n+1}}-\sqrt{p_{n}}<1$ where $p_{n}$ is the $n$th prime? $$p_{n+1}<(\lfloor \sqrt{p_{n}} \rfloor + 1)^{2}<( \sqrt{p_{n}}+ 1)^{2}$$
$$\sqrt{p_{n+1}}<1+\sqrt{p_{n}}$$
$$\sqrt{p_{n+1}}-\sqrt{p_{n}}<1$$ which is Andrica's conjecture.","['number-theory', 'prime-gaps', 'prime-numbers']"
1638150,Construct differential equation given the phase portrait (non-linear pendulum),"I am curious to know how to recover the differential equation that goes with a phase portrait.  I have seen the following posts but the first one was a $y'$ (and easy enough to ""do in my head"") and the second one lacks the figure. As an example, I solved (in Mathematica ) the non-linear pendulum equation and plotted the phase portrait as below: $$y''(t)+\frac{y'(t)}{q}+\sin \left(y(t)\right)=g \cos (\omega t)$$ Where $q$ is damping, $g$ is the forcing term while $\omega$ is the frequency of forcing.  The phase portrait ($y'$ vs $y$) is plotted: So, now if I were given this phase portrait, how do I derive the autonomous differential equation for it? I ask because I do not know the ""language"" to search for online. The reason I posted this question here instead of mathematica.SE is because this is more of a Mathematics question and not so much a Mathematica question. My Mathematica code, in case interested: g = 1/10; q = 8; \[Omega] = -0.04; TMax = 40;
pSolNL = NDSolveValue[{y''[t] + (1/q) y'[t] + Sin[y[t]] == 
    g  Cos[\[Omega]  t], y[0] == 0, y'[0] == 0}, y, {t, 0, TMax}];

ParametricPlot[{pSolNL[\[Tau]] /. \[Tau] -> t, 
  D[pSolNL[\[Tau]], \[Tau]] /. \[Tau] -> t}, {t, 0, TMax}, 
 AxesLabel -> {""y(t)"", ""y'(t)""}]","['stability-in-odes', 'chaos-theory', 'ordinary-differential-equations', 'mathematical-modeling']"
1638195,$2x(1-x)$ is not onto?,"How come $4x(1-x)$ is onto in $[0,1]$ but $2x(1-x)$ is not? Isn't it true that for any $y$ in the range interval, there exist two $x$ such that $f(x)=y$?","['algebra-precalculus', 'calculus', 'functions']"
1638251,Why doesn't $\int_{-1}^{1}\frac{dx}{x} = \ln|x|\biggr\rvert_{-1}^{1} = 0$?,"$1/x$ is an odd function, so it makes sense to me intuitively that the area would be $0$, and similarly I would expect that $\int_{-1}^{2}\frac{dx}{x} = \ln(2)$. Proof Wiki seems to confirm my intuition, but with the exception of functions that don't have a primitive (i.e. integral?), which I guess this one doesn't, because of the discontinuity at $x=0$. Nonetheless, it seems to me that the area under $1/x$ must be $0$ because: $$\int_{-1}^{1}\frac{dx}{x} = \lim_{a\to0} \left[ \int_{-1}^{a}{x^{-1} + \int_{a}^{1}{x^{-1}}} \right] = 0$$ I just can't shake the intuitive feeling that the area is $0$. Bonus points if you can explain why it is not $0$ in an intuitive way.","['integration', 'definite-integrals', 'area', 'limits']"
1638300,Integral of $-4\sin(2t - (pi/2)) $ weird behavior on wolfram alpha,"I'm confused by what Wolfram Alpha is doing with my function:
$$-4\sin{(2t - (\pi/2))}$$
on why the it gets replaced by 
$$4\cos{(2t)}$$. Is it equal? Link: See behavior here","['indefinite-integrals', 'trigonometry', 'wolfram-alpha']"
1638303,Many point compactification,"If $X$ is a noncompact LCH space (locally compact, Hausdorff) then its one point compactification is $X^*=X\cup \{\infty\}$ with topology $\mathcal{T^*}$ given by $U \in \mathcal{T^*}$ iff either a) $U \subset X$ is open, or b) if $\infty \in U$ then $U^c \subset X$ is compact What whould be the definition and the topology for 2 point compactification, or 4 point compactification. Like in $\mathbb{R^2}$ where there are 4 infinities: $\pm \infty$ on $x$ axis and $\pm \infty$ on $y$ axis?","['general-topology', 'compactness']"
1638315,"If $(u,v)$ is a point on $4x^2+a^2y^2=4a^2$,where $4<a^2<8$,that is farthest from $(0,-2)$ then $u+v$ is equal to?","If $(u,v)$ is a point on $4x^2+a^2y^2=4a^2$,where $4<a^2<8$,that is farthest from $(0,-2)$ then $u+v$ is equal to? My Approach: I took a parametric point $(t,4-4t^2/a^2)$.And then tried to find the minima of the distance.But that is too lengthy method.Any other suggestions?",['derivatives']
1638321,Advantage of Lebesgue sigma-algebra over Borel?,"What it says on the tin. Using the Borel $\sigma$-algebra on the reals instead of the Lebesgue $\sigma$-algebra has the advantage that it allows a broader class of measures, many of which are quite natural: For example the ""uniform"" measure on the Cantor set is defined on the Borel $\sigma$-algebra, but cannot be defined on the Lebesgue algebra. So why don't we just use the Borel $\sigma$-algebra for everything? What advantage does the Lebesgue $\sigma$-algebra have? I mean, it has more measurable sets, but sets that are Lebesgue-measurable but not Borel-measurable (or for that matter, sets that are not Borel-measurable, period) are extremely pathological, not explicitly constructible, and (as far as I can tell) never show up naturally. And it's complete, but I have no idea what makes that a useful property.",['real-analysis']
1638330,What is the span of an infinite set?,"If a subset $W$ of a vector space $V$ is a subspace of $V$, I want to show that $\operatorname{span}(W)=W$. But is it possible to define $\operatorname{span}(W)$? $W$ can either be a finite or infinite set and according to what I know $\operatorname{span}(W)$ can only be defined on a finite set $W$. Is there any problem with my argument?","['linear-algebra', 'definition']"
1638349,"Prove that if $\sum_{n=1}^ \infty na_n$ converges, then $\sum_{n=1}^ \infty a_n$ converges.","Prove that if $\displaystyle \sum_{n=1}^ \infty na_n$ converges, then $\displaystyle\sum_{n=1}^ \infty a_n$ converges. No, $a_n$ are not necessarily positive numbers. I've been trying summation by parts.","['real-analysis', 'sequences-and-series', 'analysis']"
1638384,Obtaining quadratic equation using Least Squares Method,"This question is most likely extremely trivial, but I'm having some difficulty obtaining the least squares equation from the following data points: {{1.08, 0}, {1.07, 0.0659232}, {0.97, 0.1695168}, {0.77, 0.188352}, {0.84, 0.0847584}} In particular, I'm trying to obtain a quadratic equation using least squares, so I was wondering if someone could show me the method they used to obtain it. I have based my working (and obtained the data above) from the following link: http://www.maths.manchester.ac.uk/~pjohnson/resources/math60082/lecture-monte-carlo-ls.pdf and I have tried to obtain the least squares equation by using the method as given on slide 14. My working is given as follows: Firstly, since I am trying to find a quadratic least squares form, I must solve three equations and attempt to find the coefficients $a_{0}, a_{1}, a_{2}$. Using the equations on slide 14 given in the link above, I then plug in the data points I was given into the equations, which are now given below: (0 + .07*.94176 + .18*.94176 +.20*.94176 + .09*.94176) = 5*$a_{0}$ + $a_{1}$*(1.08 + 1.07 + .97 + .77 + .84)+$a_{2}$*(1.08^2 + 1.07^2 + .97^2 + .77^2 + .84^2) (0*1.08 + .07*.94176*1.07 + .18*.94176*.97 +.20*.94176* .77 + .09*.94176*.84) = $a_{0}$*(1.08 + 1.07 + .97 + .77 + .84)+$a_{1}$*(1.08^2 + 1.07^2 +.97^2 + .77^2 + .84^2)+$a_{2}$*(1.08^3 + 1.07^3 + .97^3 + .77^3 + .84^3) (.07*.94176*1.07^2+.18*.94176*.97^2+.20*.94176*.77^2+.09*.94176*.84^2) = $a_{0}$*(1.08^2 + 1.07^2 + .97^2 + .77^2 + .84^2)+$a_{1}$*(1.08^3 + 1.07^3 + .97^3 + .77^3+.84^3)+$a_{2}$*(1.08^4+1.07^4+.97^4 + .77^4 + .84^4) However, putting this into Wolfram Alpha gives coefficient values -1.13685, 3.12955, and -1.89201, which are incorrect, since the quadratic equation should be -1.81357x^2 + 2.9834x - 1.06998. If it's not too much trouble could someone please show me how to obtain the quadratic equation using least squares, or at least show me what I've done wrong? Thanks in advance","['statistics', 'probability', 'least-squares', 'data-analysis']"
1638422,Definition of Sigma Algebra,"I was wondering, why are we not allowed to take arbitrary unions (likewise intersections) in the definition of a sigma algebra?; I am looking for a more or less intuitive reason. It seems to me that most of the motivation in defining sigma algebras lies in Measure Theory. So, does allowing arbitrary unions/intersections somehow screw-up the theory? Also, is there a mathematical structure which is similar to a sigma algebra, for which arbitrary unions/intersections is specified? EDIT: By arbitrary, I mean to include families of subsets which are not necessarily index-able by a countable set.","['general-topology', 'real-analysis', 'measure-theory']"
1638443,"$f$ convex strictly decreasing function , is $f'(x+\delta)-f'(x)$ convex","Assume you have a strictly decreasing convex differentiable function $f(x)$, $x \in \Bbb R^+$, I am wondering if the increment of the first derivative is also convex; i.e.,
$$g(x) = f'(x+\delta) - f'(x)$$  where  $\delta$ is any positive number. What I concluded : I can say that $f'(x)$ is a strictly increasing function, also since $f(x)$ is strictly decreasing, $f'(x)$ is always negative, meaning that it increases and approaches zero as $x \to \infty $, now I can visualize $f'$ as concave and the difference : $f'(x+\delta)-f'(x)$ to be decreasing but not sure how to show its convexity (if it is).","['derivatives', 'real-analysis', 'convex-analysis', 'calculus', 'functions']"
1638477,Does differentiability imply having bounded variation on some subinterval?,"Suppose that $f:(a,b)\to\mathbb{R}$ is a differentiable function. Does it follow that $f$ has bounded variation on some subinterval $[c,d]\subset (a,b)$? Details and ideas Being differentiable means only that $f'(x)$ exists for all $x\in (a,b)$. Continuity of $f'$ is not assumed. $f$ need not be of bounded variation on every subinterval $[c,d]\subset (a,b)$. For example, $f(x)=x^2 \sin x^{-2}$ is differentiable on $\mathbb{R}$ but has infinite variation on any interval containing $0$. One can add several copies of $f$ as above to create several points where variation blows up. But trying to add infinitely many of them, e.g., $\sum c_n f(x-q_n)$ with $q_n$ running over a dense set, appears likely to destroy differentiability somewhere. A post by Dave L. Renfro gives a list of bad properties that the derivative of a differentiable function may have, but I didn't find anything inconsistent with being BV on some subinterval. The question is motivated by this problem .","['derivatives', 'real-analysis', 'bounded-variation']"
1638479,"Finding the Distribution of Y given $X_1 + X_2$ where X, Y ~ Poisson $\Lambda$","So, because this is honestly homework for a course, I'm primarily looking for a hint from where I've gotten so far. The question is very quick. $X, Y$ are independently distributed Poisson $(\lambda)$, find the distribution of $$Y^* = Y\mid X_1 + X_2 = t$$ So, the logic I've gotten to so far, but not really further is the following:
$x_1 + x_2 \sim$ Poisson$(\lambda + \lambda)$. And that's it. Our professor gave this as a hint but I'm not sure how it was reached. $$\frac{P(X_1 = x_1, x_2 = x_1)}{P(x_1+x_2=t)}=\frac{P(X_1=x_1,x_2=x_1)}{P(x_1=x_1)P(x_2=t-x_1)}$$ And I very may well have just written the hints down incorrectly. Please advise.","['probability-theory', 'probability', 'probability-distributions']"
1638496,Prove an annulus is homeomorphic to a cylinder,"Let $A \subset \mathbb{R}^{2}$ be the annulus $A = \{(x,y) \in \mathbb{R}^{2} \colon 1 \leq x^{2} + y^{2} \leq 4 \}$. Prove that $A$ is homeomorphic to $S^{1} \times I$, where $I = [0,1]$ is the unit interval. Define $f \colon S^{1} \times I \rightarrow A$, and $g \colon A \rightarrow S^{1} \times I$ by the following maps. $$f(x,y,t) = ((t+1)x,(t+1)y)$$ and $$g(u,v) =\left(\frac{u}{\sqrt{u^{2}+v^{2}}},\frac{v}{\sqrt{u^{2}+v^{2}}},\sqrt{u^{2}+v^{2}}-1\right).$$ $f$ and $g$ are clearly well-defined.  Moreover, they and are continuous as their components are continuous.  Lastly, we must show that $f \circ g = I_{A}$ and $g \circ f = I_{S^{1} \times I}$, where $I$ is the identity function. \begin{equation*}
\begin{split}
f(g(u,v)) &=
f\bigg(\frac{u}{\sqrt{u^{2}+v^{2}}},\frac{v}{\sqrt{u^{2}+v^{2}}},\sqrt{u^{2}+v^{2}}-1\bigg) \\ 
&= \bigg( (\sqrt{u^{2}+v^{2}}-1 + 1)\frac{u}{\sqrt{u^{2}+v^{2}}},(\sqrt{u^{2}+v^{2}}-1 + 1)\frac{v}{\sqrt{u^{2}+v^{2}}}  \bigg) \\
&= (u,v).
\end{split}
\end{equation*}
\begin{equation*}
\begin{split}
g(f(x,y,t)) 
&= g((t+1)x,(t+1)y) \\
&= \bigg(\frac{(t+1)x}{\sqrt{((t+1)x)^{2} + ((t+1)y)^{2}}},\frac{(t+1)y}{\sqrt{((t+1)x)^{2} + ((t+1)y)^{2}}}, \\
&\qquad  \sqrt{((t+1)x)^{2} + ((t+1)y)^{2}} -1\bigg) \\
&= \bigg( \frac{(t+1)x}{\sqrt{(t+1)^{2}(x^{2}+y^{2})}}, \frac{(t+1)y}{\sqrt{(t+1)^{2}(x^{2}+y^{2})}}, \sqrt{(t+1)^{2}(x^{2}+y^{2})} -1\bigg) \\
&= \bigg(\frac{x}{\sqrt{x^{2}+y^{2}}}, \frac{y}{\sqrt{x^{2}+y^{2}}}, (t+1)\sqrt{x^{2}+y^{2}}-1\bigg) \\
&= (x,y,t).
\end{split}
\end{equation*} I might look stupid after this, but I can't see why the last component on the second to last line is not working out.  Unless, my maps weren't defined correct?  Can someone see the problem? Using the comments below, since $(x,y) \in S^{1}$, its norm $\sqrt{x^{2}+y^{2}} =1$.  Thus, the last line in the chain of equalities in the above equation holds.","['general-topology', 'proof-verification']"
1638503,Why was poisson distribution introduced?,"I am studying probabilites and the notion of poisson random variable was introduced in the class. But it seems to me that the introduction of poisson random variable is to provide a easy approximation of the binomial random variable conditioned that n is large and p is small. Besides, the preconditions in the poisson distribution that events are independent of each other seem to come from the fact that binomial random variable is composed by many independent bernouli variables. So I wonder if originally, poisson distribution was invented to model binomial distribution or was it invented to solve a particular kind of problem","['statistics', 'binomial-distribution', 'poisson-distribution']"
1638513,"Prob. 9, Sec. 19 in Munkres' TOPOLOGY, 2nd edition: Equivalence of the choice axiom and non-emptyness of Cartesian product","The Axiom of Choice is as follows: Given a collection $\mathcal{A}$ of disjoint non-empty sets, there exists a set $C$ consisting of exactly one element from each element of $\mathcal{A}$; that is, a set $C$ such that $C$ is contained in the union of the elements of $\mathcal{A}$, and for each $A \in \mathcal{A}$, the set $C \cap A$ contains a single element. Now here is Prob. 9, Sec. 19 in the book Topology by James R. Munkres, 2nd edition: Show that the choice axiom is equivalent to the statement that for any indexed family $\{ A_\alpha \}_{\alpha \in J}$ of non-empty sets, with $J \neq \emptyset$, the Cartesian product $$\prod_{\alpha \in J} A_\alpha$$ is not empty. My effort: Suppose the choice axiom holds. Let $\{A_\alpha\}_{\alpha \in J}$ be an indexed family of non-empty sets, with the index set $J \neq \emptyset$. For each $\alpha \in J$, let's define the set $A^\prime_\alpha$ as follows: 
$$A^\prime_\alpha \colon = \left\{ \ (\alpha, a ) \ \colon \ a \in A_\alpha \ \right\}.$$
Now let $\mathcal{A}$ be the indexed family $\{A^\prime_\alpha\}_{\alpha \in J}$. Since $J \neq \emptyset$ and since each set $A_\alpha$ is non-empty, therefore each set $A^\prime_\alpha \neq \emptyset$. Moreover, for any $\alpha, \beta \in J$, where $\alpha \neq \beta$, we also have $$A^\prime_\alpha \cap A^\prime_\beta = \emptyset.$$
Thus, $\mathcal{A}$ is a collection of disjoint non-empty sets. So, by the choice axiom, there exists a set $C$ such that $C$ consists of exactly one element from each set in $\mathcal{A}$. That is, for each $\alpha \in J$, the set $C$ consists of exactly one ordered pair $(\alpha, a)$ such that $a \in A_\alpha$. Then the set of all these ordered pairs in $C$ constitutes an element of the Cartesian product $\prod_{\alpha \in J} A_\alpha$. Am I right? Conversely, suppose that, for any indexed family $\{A_\alpha\}_{\alpha \in J}$ of non-empty sets, with the index set $J \neq \emptyset$,  the Cartesian product $$\prod_{\alpha \in J} A_\alpha$$ is not empty. Let $\mathcal{A}$ be a collection of disjoint non-empty sets. Then, assuming that the collection $\mathcal{A}$ is non-empty, the Cartesian product $$\prod_{A \in \mathcal{A}} A$$ is not empty. Let $$\mathcal{A} \colon= \left\{ \ X_i \ \colon \ i \in I \ \right\},$$
where the index set $I$ is non-empty and, for each $i \in I$, the set $X_i \neq \emptyset$. Then $$\prod_{A \in \mathcal{A}} A = \prod_{i \in I} X_i.$$
Let $x$ be an element of this Cartesian product. Then, by definition,$x \colon I \to \cup_{i \in I} X_i$ (i.e.,  $x$ is a function with domain $I$ and images in the union $\cap_{i \in I} X_i$) such that $x(i) \in X_i$ for each $i \in I$. Let $C$ be the set $$\left\{ \ x(i) \ \colon \ i \in I \ \right\}.$$ 
Since for each $i, j \in I$ with $i \neq j$, we have $X_i \cap X_j = \emptyset$, therefore the set $C$ consists of exactly one element from each set in the collection $\mathcal{A}$. Is the above proof correct?","['axiom-of-choice', 'elementary-set-theory', 'proof-verification']"
1638544,Derivation of the variation of parameters in Second-Order Differential Eq.,"In Second-Order ODEs ,There is a problem which I haven't solved. Method of Variation of Parameters; In derivation of the method , there is a part which is following.
$$k'[2Pu'_1 + Qu_1] + k''[Pu_1]=0 \\
k'=\frac{e^{-\int [Q/P]dx}}{u_1^2}$$
But I find  $$k=\frac{e^{-\int [Q/P]dx}}{u_1^2}$$
I could't understand how they find k' like that.İs it typo or my result is wrong ? Here is my attempt to solve; $$[2Pu'_1 + Qu_1]\frac{dk}{dx}=-[Pu_1]\frac{d(\frac{dk}{dx})}{dx} \\
\int [2Pu'_1 + Qu_1]dk=\int [Pu_1]d(\frac{dk}{dx}) \\
[2Pu'_1 + Qu_1]k=[Pu_1]\frac{dk}{dx} \\
k=\frac{e^{-\int [Q/P]dx}}{u_1^2}$$",['ordinary-differential-equations']
1638546,rayleigh quotient of eigenvalue problem (sturm liouville theory and partial differential equations),"I am reading ""A First Course in Partial Differential Equations with Complex Variables and Transform Methods"" (Weinberger, p. 168). if we have the eigenvalue problem
$$ (pu')'- qu + \lambda \rho u = 0 $$
$$ u(0) = 0 $$
$$ p(1)u'(1) + au(1) = 0, a \ge 0 $$ we find that the eigenvalues are defined by the minimum principles $$ \lambda_k = \min_{\phi \in S_k} \frac{\int_{0}^1 (p\phi'^2 + q\phi^2) dx + a\phi(1)^2} {\int_{0}^1 \rho \phi^2 dx} $$ where $ S_k $ is the set of all continuous and piecewise continuously differentiable functions satisfying $$ \phi(0) = 0 $$
$$ \int_{0}^1 \rho \phi u_j dx = 0 $$
for $ j = 1, ..., (k-1) $ Note that the eigenvalues are arranged in increasing order. Could you please explain how the eigenvalues $ \lambda_k $ of the problem are given by minima of the aforementioned Rayleigh quotient.","['eigenvalues-eigenvectors', 'sturm-liouville', 'partial-differential-equations', 'eigenfunctions', 'ordinary-differential-equations']"
1638640,Showing $\int_{1}^{0}\frac{\ln(1-x)}{x}dx=\frac{\pi ^{2}}{6}$,Is there way to show $$\int_{1}^{0}\frac{\ln(1-x)}{x}dx=\frac{\pi ^{2}}{6}$$ without using the Riemann zeta function?,"['integration', 'calculus']"
1638744,Norris exercise: Showing $P_0[\text{no return to}\ 0]=6/\pi^2$,"Consider exercise 1.3.4 of Norris' Markov Chains. The question is as follows: Let $\{X_n\}_{n\geq 0}$ be a Markov Chain with state space $S=\{0,1,2,\dots\}$. Suppose the transition probabilities are given by $$p_{01}=1,\,\,\,p_{i,i+1}+p_{i,i-1}=1, \,\,\,p_{i,i+1}=\dfrac{(i+1)^2}{i^2}p_{i,i-1}$$ for $i\geq1$. Show that $P[X_n\geq1\space\forall n\geq1\mid X_0=0]=\dfrac{6}{\pi^2}$ I defined $h_k=P[X_n\neq 0\space\forall n\geq1\mid X_0=k]$ for all $k\geq0$. Note first that $h_0=h_1$ so it is enough to compute $h_1$. I have deduced that for any $k\geq1$, $$h_k=h_1S_k \quad \text{ where }\quad  S_k=\sum_{i=1}^k\dfrac{1}{i^2}$$ I will be done if I can show that $\lim_{k\to\infty}h_k=1$. (And in fact, this will be the case). However, I cannot show it. Intuitively it is clear, but I cannot really show it rigorously. I noted that $h_k$ are increasing in $k$, so the limit $\lim_{k\to\infty}h_k$ exists. But why should this limit be $1$?","['markov-chains', 'probability-theory']"
1638786,quadratic simultaneous equation,"Solve simultaneously: $$ 12x^2-4xy+11y^2=64 $$ $$ 16x^2-9xy+11y^2=78$$ I understand that it can be solved using the quadratic formula by rearranging the equation in $ax^2+bx+c=0 $ form
$$ (12)x^2-(4y)x+(11y^2-64)=0$$
Can this be solved using any other method (substitution or elimination?).",['algebra-precalculus']
1638790,Finding the expected value in the given problem.,"It is given that a monkey types on a 26-letter keyboard with all the keys as lowercase English alphabets. Each letter is chosen independently and uniformly at random. If the monkey types 1,000,000 letters, what is the expected number of times the sequence ""proof"" appears? Here is the suggested solution: Let the random variable $X_i = 1$ if the word ""proof"" appears at the index $i$ else $X_i = 0$ . 
Let $n = 1,000,000$ Hence, the expected value of number of appearances of the word is: \begin{align}
&E\left[ \sum\limits _{i=1}^{n-4}X_i \right] & & \text{Eqn 1} \\
&= \sum\limits _{i=1}^{n-4}E[X_i] & & \text{Eqn 2}
\end{align} Now $E[X_i] = 26^{-5}$ . Hence expected number of appearances = $(n-4)\cdot26^{-5}$ (Note that the upper limit is $n-4$ because the word proof is of length $5$ . Hence it can at most start at the index $n-4$ to finish the index $n$ ) Now here is my doubt. We know that if $X_i = 1$ , then the following few random variables like $X_{i+1}$ , $X_{i+2}$ etc, can't be $1$ because you cannot have a word proof starting at index $i$ and then another word proof starting at index i+1. Where in this proof have we imposed that restriction?","['probability-theory', 'expected-value', 'random-variables']"
1638796,"How many DFA's exist with two states over the input alphabet $\{0,1\}$?","How many DFA's exist with two states over the input alphabet $\{0,1\}$? My attempt : Input set is given. So, we have 3 parts of DFA which we can change: Start state Transition Function Final state Start state can be chosen as any one among 2 in 2 ways. Transition function is from $Q \times Z$ to $Q$, where $Q$ is the set of states and $Z$ is the alphabet. $|Q| = 2$, $|Z| = 2$. So, number of possible transition functions $= 2^{2 \times 2} = 2^{4}$ Final state can be any subset of the set of states including the empty set. With $2$ states, we can have $2^2 = 4$ possible sub states. Thus total number of DFAs possible : $=2\times2^4\times4=128$. Where total 40 DFA's are accepting empty language. Can you explain in formal way, with a formula, please?","['regular-language', 'formal-languages', 'permutations', 'combinatorics', 'automata']"
1638838,Subsets of the reals when the Continuum Hypothesis is assumed false,"If one assumes that the continuum hypothesis is false then there are subsets of the reals of intermediate cardinality, uncountable but smaller than the continuum. What can be said about the necessary 
properties of such a set e.g. topological properties other than that it cannot be closed?","['descriptive-set-theory', 'general-topology', 'set-theory']"
1638845,Joining the Midpoints of the Sides of a Quadrilateral,"$ABCD$ is a quadrilateral. $P$, $Q$ and $R$ are the midpoints of $AB$, $BC$ and $CD$ respectively. If $PQ = 3$, $QR = 4$ and $PR = 5$; find the area of $ABCD$. Since, $5^2 = 3^2+4^2$, So, $\angle PQR = 90^o$ I can't Find a way to solve this. Note : This is a problem from BDMO $2010$ National.","['contest-math', 'trigonometry', 'geometry']"
1638863,Evaluating $\lim_{x\to1}{\frac{\sqrt{x^2+3}-2}{\sqrt{x^2+8}-3}}$ without L'Hospital's Theorem,"I've been trying to evaluate$$\lim_{x\to1}{\frac{\sqrt{x^2+3}-2}{\sqrt{x^2+8}-3}}$$ I tried: (a) Rationizing the numerator -> Error (b) Rationizing the denominator -> Error (c) Factoring out $x$ -> Error Finally, I used L'Hospital's Rule and got the answer $3/2$, but that is not what I am supposed to do, for not a single word about this Theorem was mentioned during my lectures. Is there any other way to solve this limit without this Theorem?","['functions', 'limits-without-lhopital', 'calculus', 'limits']"
1638890,Cellular homology of the real projective space $\mathbb R P^n$,"I've been able to calculate the cellular homology of $\mathbb R P^2$ but I'm struggling to do the same for higher dimensions. My problem is that I don't exactly see how one get to the result $d_i: C_{i}(\mathbb R P^n) \to C_{i-1}(\mathbb R P^n)$ has degree $1+(-1)^i$. I looked this up in Hatcher but our lecture followed Bredon where in chapter 14 they explain it but I do not understand it. In Hatcher they used the local degree, which I know, but I don't understand how the computation works. I'm interested in seeing how one does the step by step calculation with the local degree as I feel I'm completely lost here. Edit: To be more precise: 
$\mathbb R P^n$ can be identified by $B^n / \tilde{}$ where we identify antipodal points on the boundary. We can also view the real projective space as $S^n/ \tilde{}_{antipodal}$. With these description one sees that we can construct $\mathbb R P^n$ as a CW complex by taking one $\sigma^i$ i-cell from dimension $0$ to $n$. For the attaching maps we do the following:
$$ f_{\partial \sigma ^1}: \partial B^1_{\sigma^1} \approx \{0,1\} \to K^{(0)}=\{\sigma ^0\}$$ 
and for the dimension $1 \leq i \leq n$: $$f_{\partial \sigma ^i}: \partial B_{\sigma^i}^i \approx S^{i-1} \to K^{(i-1)}$$ where we define this map by antipodal identification. On the chain complex level we get $C_i(\mathbb R P^n)=\mathbb Z \sigma ^i \cong \mathbb Z$. So we have the sequence: $$0 \stackrel{0}\to \mathbb Z \sigma ^n \stackrel{d_n}\to \mathbb Z \sigma ^{n-1} \stackrel{d_{n-1}}\to \mathbb Z \sigma ^{n-2} \stackrel{d_{n-2}}\to ...\stackrel{d_1}\to \mathbb Z \sigma ^1 \stackrel{0}\to 0$$ We can then compute the cellular homology $H_*(\mathbb R P ^n)=H_*(C_{\cdot}(\mathbb R P^n,d))$ by computing $ker(d_i)/im(d_{i+1})$ where the boundary/differential formula of $d_i$ is: $$ d_i: \mathbb Z \sigma^i: \to \mathbb Z \sigma^{i-1}$$
defined uniquely by $d_i(\sigma^i)=[\sigma^{i-1}:\sigma^{i}]\sigma^{i-1} \in \mathbb Z \sigma^{i-1}$ where the square brackets denote the incidence number, i.e. $$[\sigma^{i-1}:\sigma^{i}]=deg(p_{\sigma^{i-1}} \circ f_{\partial \sigma^i})$$. And here comes the point where I'm stuck. How do I exactly compute the $d_i$'s?","['algebraic-topology', 'general-topology', 'homological-algebra']"
1638893,How does Green's theorem apply here?,"Let $D$ be the region delimited by 
$$\partial D:
\begin{cases}
C_1: x^2 + y^2 = 5^2\\
C_2:(x-2)^2+y^2= 1\\
C_3:(x+2)^2+y^2 = 1\\
C_4: x^2+(y-2)^2= 1\\
C_5: x^2+(y+2)^2= 1
\end{cases}
$$
I've sketched this and the region looks like this (the inside of the big circle, intersection the outside of the small circles should be shaded): So the boundary of the region is not a closed curve... but I'm asked to verify Green's theorem for this region anyway. I don't understand how the theorem applies, could someone explain this? E: From the comments it looks like I should do $$\iint_D (Q_x-P_y)dA=\oint_{C_1} Fds-\sum_{i\ge 2} \oint_{C_i}Fds$$ Is this correct? I don't see how this follows from Green's theorem's statement (using wikipedia for reference).","['multivariable-calculus', 'integration', 'greens-theorem']"
1638894,Sketch the set of points satysfing an inequality $|z+1|+|z-1|\leq2$,"The inequality is 
$$|z-1|+|z+1|\leq2$$ I used a triangle inequality to show that 
Since triangle inequality states:
$$|z+w|\leq|z|+|w|$$
Then
$$|z-1+z+1|\leq|z-1|+|z+1|\leq2$$
So $$|2z|\leq2$$
From this point I expanded out
$$\sqrt{(2x)^2+(2y)^2}\leq2$$
Or $$4x^2+4y^2\leq4$$
So I end up with a disk centred at origin, radius 1. However my ""answer"" at the back of the page says ""one point"" (it doesnt give step by step solution)Thanks for the help","['complex-analysis', 'inequality']"
1638911,Modified Laplace's method,"In the application of Laplace method (or steepest descent) it is often assumed that the dependence on the factor N, on which we are expanding the integral, is only in the argument of the exponential. What if we have an expression to expand for large $N$ of the type: $\int_{-\infty}^{+\infty} f(N,t)  e^{Ng(t) dt}$ where the function $f$ is only mildly depending on $N$. Would the method change? Are there examples of functions of this form?","['integration', 'asymptotics']"
1638912,Is $C_0(\mathbb{R})$ a Banach space?,"Let $C(\mathbb{R})$ be a Banach space of continuous real-valued functions defined on $\mathbb{R}$ , with supremum norm, and let $C_0(\mathbb R)$ be the subspace of functions vanishing at infinity. Is $C_0(\mathbb{R})$ a Banach space? I try to see it using: $f\in C_0(\mathbb{R})$ iff for any $\epsilon>0$ there exists $K>0$ such that $|f|<\epsilon$ whenever $|x|>K$ . But I think it is not Banach. Please I need a counter-example or a proof.","['functional-analysis', 'banach-spaces', 'uniform-continuity']"
1639045,Proving that $X^2- [X]$ is a local martingale given that $X$ is a cadlag locally square-integrable martingale,"Suppose that $X$ is a cadlag locally square-integrable martingale.  Let $[X]$ denote the quadratic variation of $X$. My textbook claims, by Ito's formula that $$ X^2 _t = X^2_0 + [X]_t + 2 
\int_0^t X_{s^{-}} 
\,d X_s, $$ which shows that $X^2 - [X]$ is indeed a local martingale.  Let $
\Delta Y_t = Y_t - Y_{t^{-}}.$ But the version of Ito's formula for cadlag semimartingales tells me that $$ X^2 _t = X^2_0 + [X]_t + 2 
\int_0^t X_{s^{-}} 
\,dX_s + \sum_{s \leq t }   \bigg[ \Delta (X^2_s) - 2 (X_{s^{-}}) \Delta X_s - (\Delta X_s)^2 \bigg] . $$ I am quite unfamiliar with this, as my course so far has just focused on continuous processes. Any ideas?","['stochastic-processes', 'probability-theory', 'quadratic-variation', 'martingales', 'stochastic-calculus']"
1639049,"Is $(-\infty, 0)$ the same size as $(0, \infty)$?","A differential equations problem asked about the largest interval on which the solution was defined. The solution was defined except for $t=0$, which made me wonder whether the intervals $(-\infty, 0)$ and $(0, \infty)$ are the same size. If these intervals are the same size, does that mean that $\mathbb{R}$ can be divided in half?","['real-numbers', 'ordinary-differential-equations', 'elementary-set-theory']"
1639050,Derivative of matrix logarithm with respect to matrix,I saw in this post that $\frac{d}{dt}\text{logm}(Z(t)) = \frac{dZ(t)}{dt}(Z(t))^{-1}$ Is this true to say: $\frac{d}{{dU}}{\mathop{\rm logm}\nolimits} (A) = {A^{ - 1}}\frac{d}{{dU}}A$ where U is an m by n matrix and A is an m by m matrix which is a function of U ? EDIT: A is not Symmetric and positive definite.,"['matrices', 'logarithms', 'matrix-calculus', 'derivatives']"
1639052,Drawing a circle tangential to 3 circles (internally to one of them),"The two small circles (in black) are equal in radius, and tangential to the large circle. They also touch each other at the center of the large circle. Now, I want to construct a circle (in orange) which is tangential to the inner two, and the larger circle. How can I construct it?","['circles', 'geometric-construction', 'geometry']"
1639081,"$\sin2(x) - \tan(x) = 0$ , solve for $-180\le x\le 180$","I have been unable to solve the following question, If $$\sin(2x) - \tan(x) = 0$$ Find $x$ , $-\pi\le x\le \pi$ So far my workings have been
Use following identity: $$\sin(2x) = 2\sin(x)\cos(x)\\2\sin(x)\cos(x) - \tan(x) = 0\\2\sin(x)\cos(x) - \frac{\sin(x)}{\cos(x)} = 0\\
2\frac{\sin(x)\cos(x)}{1} - \frac{\sin(x)}{\cos(x)} = 0$$ Then cross multiply to give : $$-\sin x+((2\cos(x)\sin(x))\cos(x))/\cos(x) = 0$$ $$-\sin x+(2\cos^2(x)\sin(x))/ \cos(x) = 0$$ However, I have been unable to get any further. If someone could help me find a solution to this question it would be very much appreciated.Thank you.","['algebra-precalculus', 'trigonometry']"
1639127,If $\lim_{n \rightarrow \infty} a_n=L$ then $\lim_{n \rightarrow \infty} f(a_n)=f(L)$?,"If we have for example $a_n=1+\sqrt{a_{n-1}}$ and $\lim_{n \rightarrow \infty} a_n=L$ then can I say that $ L=1+\sqrt{L}$? If it's so, what's the proof?","['recurrence-relations', 'sequences-and-series']"
1639163,Complicated surface integral/line integral.,"Problem Compute the integrals $$I=\iint_\Sigma \nabla\times\mathbf F\cdot d\,\bf\Sigma$$ And $$J=\oint_{\partial\Sigma}\mathbf F\cdot d\bf r$$ For $F=(x^2y,3x^3z,yz^3)$, and $$\Sigma:\begin{cases}x^2+y^2=1\\-1\le z\le 1 \end{cases}$$ I don't quite understand which kind of curve could be the boundary of such a cylinder, so I was not able to calculate the line integral, how can I do it? For the surface integral, I parametrized the surface::
\begin{align}\Sigma_1(r,t)&=(\cos t, \sin t,r)\\
\end{align} So $I=\iint_{\Sigma_i}\nabla\times F\, \mathrm{d}\bf\Sigma_i$. I just like to get a verification on the first one. We have $\nabla\times F= (z^3-3x^3,0,x^2(9z-1))$. $\Sigma_{1r}\times\Sigma_{1t}=(-\cos t,-\sin t,0)$, and $\nabla\times F(\Sigma_1(r,t))=(r^3-3\cos^3t,0,\cos^2t(9r-1))$, then $$
I_1=\iint_D (\nabla\times F)\cdot dS= \int_0^{2\pi}\int_{-1}^1 (-r^3\cos t-3\cos ^4t)drdt
$$ Then I just calculate that. Is my procedure correct so far? I know Stokes' theorem says that $I=J$, but we're asked to calculate these two to just 'verify' it. What would $\oint_{\partial \Sigma} F\cdot \,dr$ be here?  Do I have to pick different orientations for each of the circles on the top and bottom of the cylinder?","['multivariable-calculus', 'stokes-theorem', 'integration']"
1639190,Computation of an iterated integral,"I want to prove $$\int\limits_{-\infty}^\infty\int\limits_{-\infty}^\infty\frac{\sin(x^2+y^2)}{x^2+y^2}dxdy=\frac{\pi^2}{2}.$$ Since the function $(x,y)\mapsto\sin(x^2+y^2)/(x^2+y^2)$ is not integrable, I can't use the Theorem of Change of Variable. So, I'm trying to use residue formulae for some suitable holomorphic function to compute the inner integral, but I can't continue. Can someone suggest me a hint to solve this problem? Addendum: I may be wrong, but I suspect Theorem of Change of Variable (TCV) is not the answer. The reason is the following: the number $\pi^2/2$ is gotten if we apply polar coordinates, but TCV guarantees that if we apply any other change of variable we can get the same number, $\pi^2/2$. If this function were integrable, this invariance property would be guaranteed, but it is not the case. Thus we may have strange solutions to this integral.","['complex-analysis', 'real-analysis', 'contour-integration']"
1639212,Is $k+p$ prime infinitely many times?,"I have the following conjecture: Let $k\in\mathbb{N}$ be even. Now $k+p$ is prime for infinitely many primes $p$. I couldn't find anything on this topic, but I'm sure this has been thought of before. I tried to solve this using Dirichlet's theorem on arithmetic progressions and the Green–Tao theorem , but no luck with those. Is this question equivalent to an existing open problem? If not, how can I prove this (I prefer hints, but I appreciate full answers, too)? - Edit - As has been pointed out in the comments, this is not a duplicate. I'm asking for infinitely many primes $p$ such that $p+k$ is prime, not only one.","['number-theory', 'prime-numbers', 'elementary-number-theory']"
1639220,Compact operator in Hilbert spaces reach the maximum in the sphere.,"I found the following question in my textbook: (QUESTION) Let $\mathcal{H}$ a Hilbert space and $T: \mathcal{H} \rightarrow \mathcal{H}$ a compact operator. Show that exists $x \neq 0$ in $\mathcal{H}$, such that $\|Tx\| = \|T\|\|x\|$ A few days ago, I found a similar question in another textbook, but I had $f: \mathcal{H} \rightarrow \mathbb{R}$ bounded. This one, I can use Hanh-Banach theorem to show that exists $x \in \mathcal{H}$, such that $f(x) = \|f\| \|x\|$. Thanks.","['functional-analysis', 'compact-operators', 'hilbert-spaces']"
1639253,Coherent sheaves on $\mathbb{P}^1$,Let $F$ be a coherent sheaf on $\mathbb{P}^1$. How to show that there exists a unique exact sequence of the form $$0\to\mathcal{O}_{\mathbb{P}^1}(-1)^{\oplus a}\to\mathcal{O}_{\mathbb{P}^1}^{\oplus b}\to F\to\mathcal{O}_{\mathbb{P}^1}(-1)^{\oplus c}\to\mathcal{O}_{\mathbb{P}^1}^{\oplus d}\to 0?$$,"['coherent-sheaves', 'algebraic-geometry']"
1639275,"What is the number of ordered triplets $(x, y, z)$ such that the LCM of $x, y$ and $z$ is ...","What is the number of ordered triplets $(x, y, z)$ such that the LCM of $x, y$ and $z$ is $2^33^3$ where $x, y,z\in \Bbb N$? What I tried : At least one of $x, y$ and $z$ should have factor $2^3$ and at least one should have factor $3^3$. I then tried to figure out the possible combinations but couldn't get the correct answer.","['permutations', 'combinatorics', 'combinations']"
1639309,$f_n \geq 0$ and $\int f_n = 1$ implies $\limsup_n \left( f_n(x) \right)^{\frac{1}{n}} \leq 1$ for a.e. $x$,"I am studying for a qualifying exam and am having difficulty with this problem: Let $\left( X, \mathcal{M}, \mu \right)$ be a measure space and assume $f_n \geq 0$ such that $\int f_n = 1$ for all $n$.  Show that $$\limsup_n \left( f_n(x) \right)^{\frac{1}{n}} \leq 1 \text{ for a.e. } x. \;\;\;\;\;\;\;\;\;\; (*)$$ Attempts: (a)  Notice that if $x$ does not satisfy $(*)$, then by the Root Test $\sum f_n(x) = \infty$, hence if $x$ does not satisfy $(*)$ on a set of positive measure $E$, then $\sum \int_E f_n = \int_E \sum f_n = \infty$.  However, this does not seem to contradict the hypothesis. (b)  If $\limsup_n \left( f(x) \right)^{1/n} > 1$, then $\limsup_n f_n(x) = \infty$.  Hence, if $x$ does not satisfy $(*)$ on a set of positive measure $E$, then $\infty = \int_E \limsup f_n \geq \limsup \int_E f_n$.  Again, this does not seem to contradict the hypothesis. Thanks in advance for your help.","['real-analysis', 'measure-theory']"
1639320,"What is ""white noise"" and how is it related to the Brownian motion?","In the Chapter 1.2 of Stochastic Partial Differential Equations: An Introduction by Wei Liu and Michael Röckner, the authors introduce stochastic partial differential equations by considering equations of the form $$\frac{{\rm d}X_t}{{\rm d}t}=F\left(t,X_t,\dot B_t\right)$$ where $\left(\dot B_t\right)_{t\ge 0}$ is a ""white noise in time"" (whatever that means) with values in a separable Hilbert space $U$. $\left(\dot B_t\right)_{t\ge 0}$ is said to be the ""generalized time-derivative of a $U$-valued Brownian motion $(B_t)_{t\ge 0}$. Question: What exactly do the authors mean? What is a ""white noise in time"" and why (and in which sense) is it the ""generalized time-derivative"" of a Brownian motion? You can skip the following, if you know the answer to these questions. I will present what I've found out so far: I've searched the terms ""white noise"" and ""distributional derivative of Brownian motion"" on the internet and found few and inconsistent definitions. Definition 1 : In the book An Introduction to Computational Stochastic PDEs the authors do the following: Let $(\phi_n)_{n\in\mathbb N}$ be an orthonormal basis of $L^2([0,1])$, e.g. $\phi_n(t):=\sqrt 2\sin(n\pi t)$. Then $$W_t:=\lim_{n\to\infty}\sum_{i=1}^n\phi_i(t)\xi_i\;\;\;\text{for }t\in [0,1]\;,$$ where the $\xi_i$ are independent and standard normally distributed random variables on a probability space $(\Omega,\mathcal A,\operatorname P)$, is a stochastic process on $(\Omega,\mathcal A,\operatorname P)$ with $\operatorname E[W_t]=0$ and $$\operatorname E[W_sW_t]=\delta(s-t)\;\;\;\text{for all }s,t\in [0,1]$$ where $\delta$ denotes the Dirac delta function . They call $(W_t)_{t\in [0,1]}$ white noise . This definition seems to depend on the explicit choice of the orthnormal basis $(\phi_n)_{n\in\mathbb N}$ and I don't see the connection to a ""derivative"" of a Brownian motion (needless to say that I don't see how this would generalize to a cylindrical Brownian motion). However, maybe it has something to do with the following: Let $(B_t)_{t\ge 0}$ be a real-valued Brownian motion on $(\Omega,\mathcal A,\operatorname P)$. Then the Karhunen–Loève theorem yields $$B_t=\lim_{n\to\infty}\sum_{i=1}^n\sqrt{\zeta_i}\phi_i(t)\xi_i\;\;\;\text{for all }t\in [0,T]$$ in $L^2(\operatorname P)$ and uniformly in $t$, where $(\phi_n)_{n\in\mathbb N}$ is an orthonormal basis of $L^2([0,1])$ and $(\xi_n)_{n\in\mathbb N}$ is a sequence of indepedent standard normally distributed random variables on $(\Omega,\mathcal A,\operatorname P)$. In particular, $$\zeta_i=\frac 4{(2i-1)^2\pi^2}$$ and $$\phi_i(t)=\sqrt 2\sin\frac t{\sqrt{\zeta_i}}\;.$$ The authors state, that we can formally consider the derivative of $B$ as being the process $$\dot B_t=\lim_{n\to\infty}\sum_{i=1}^n\phi_i(t)\xi_i\;.$$ I have no idea why. Nevertheless, we may notice the following: Let $${\rm D}^{(\Delta t)}_t:=\frac{B_{t+\Delta t}-B_t}{\Delta t}\;\;\;\text{for }t\ge 0$$ for some $\Delta t>0$. Then $\left({\rm D}^{(\Delta t)}_t\right)$ is a stochastic process on $(\Omega,\mathcal A,\operatorname P)$ with $$\operatorname E\left[{\rm D}^{(\Delta t)}_t\right]=0\;\;\;\text{for all }t\ge 0$$ and $$\operatorname{Cov}\left[{\rm D}^{(\Delta t)}_s,{\rm D}^{(\Delta t)}_t\right]=\left.\begin{cases}\displaystyle\frac{\Delta t-|s-t|}{\Delta t^2}&\text{, if }|s-t|\le \Delta t\\0&\text{, if }|s-t|\ge \Delta t\end{cases}\right\}=:\eta^{(\Delta t)}(s-t)\;\;\;\text{for all }s,t\ge 0\;.$$ Since $$\int\eta^{(\Delta t)}(x)\;{\rm d}x=\int_{-\Delta t}^{\Delta t}\eta^{(\Delta t)}(x)\;{\rm d}x=1$$ we obtain $$\eta^{(\Delta t)}(x)\stackrel{\Delta t\to 0}\to\delta(x)\;,$$ but I have no idea how this is related to white noise. Definition 2 : In Stochastic Differential Equations with Applications to Physics and Engineering , Modeling, Simulation, and Optimization of Integrated Circuits and Generalized Functions - Vol 4: Applications of Harmonic Analysis they take a real-valued Brownian motion $(B_t)_{t\ge 0}$ on $(\Omega,\mathcal A,\operatorname P)$ and define $$\langle W,\phi\rangle:=\int\phi(t)B_t\;{\rm d}\lambda\;\;\;\text{for }\phi\in\mathcal D:=C_c^\infty([0,\infty))\;.$$ Let $\mathcal D'$ be the dual space of $\mathcal D$. We can show that $W$ is a $\mathcal D'$-valued Gaussian random variable on $(\Omega,\mathcal A,\operatorname P)$, i.e. $$\left(\langle W,\phi_1\rangle,\ldots,\langle W,\phi_n\rangle\right)\text{ is }n\text{-dimensionally normally distributed}$$ for all linearly independent $\phi_1,\ldots,\phi_n\in\mathcal D$, with expectation $$\operatorname E[W](\phi):=\operatorname E\left[\langle W,\phi\rangle\right]=0\;\;\;\text{for all }\phi\in\mathcal D$$ and covariance $$\rho[W](\phi,\psi):=\operatorname E\left[\langle W,\phi\rangle\langle W,\psi\rangle\right]=\int\int\min(s,t)\phi(s)\psi(t)\;{\rm d}\lambda(s)\;{\rm d}\lambda(t)\;\;\;\text{for all }\phi,\psi\in\mathcal D\;.$$ Moreover, the derivative $$\langle W',\phi\rangle:=-\langle W,\phi\rangle\;\;\;\text{for }\phi\in\mathcal D\tag 1$$ is again a $\mathcal D'$-valued Gaussian random variable on $(\Omega,\mathcal A,\operatorname P)$ with expectation $$\operatorname E[W'](\phi)=0\;\;\;\text{for all }\phi\in\mathcal D\tag 2$$ and covariance
\begin{equation}
\begin{split}
\varrho[W'](\phi,\psi)&=\int\int\min(s,t)\phi'(s)\psi'(t)\;{\rm d}\lambda(s)\;{\rm d}\lambda(t)\\
&=\int\int\delta(t-s)\phi(s)\psi(t)\;{\rm d}\lambda(t)\;{\rm d}\lambda(s)
\end{split}
\end{equation}
for all $\phi,\psi\in\mathcal D$. Now they call a generalized Gaussian stochastic process with expectation and covariance given by $(1)$ and $(2)$ a Gaussian white noise . Thus, the generalized derivative $W'$ of the generalized Brownian motion $W$ is a Gaussian white noise. Again, I don't know how I need to generalize this to the case of a cylindrical Brownian motion. Moreover, this definition seems to be less naturally to me and I don't think that this is the notion Liu and Röckner had in mind. Definition 3 : In some lecture notes, I've seen the following the definition: Let $W$ be a centered Gaussian process, indexed by test functions $\phi\in C^\infty([0,\infty]\times\mathbb R^d)$ whose covariance is given by $$\operatorname E\left[W_\phi W_\psi\right]=\int_0^\infty{\rm d}t\int_{\mathbb R^d}{\rm d}x\int_{\mathbb R^d}{\rm d}y\phi(t,x)\psi(t,x)\delta(x-y)\tag 3$$ or $$\operatorname E\left[W_\phi W_\psi\right]=\int_0^\infty{\rm d}t\int_{\mathbb R^d}{\rm d}x\phi(t,x)\psi(t,x)\tag 4\;.$$ Then $W$ is called ""white noise in time and colored noise in space"" in the case $(3)$ and ""white noise, both in time and space"" in the case $(4)$. They simply state that $\delta$ is some ""reasonable"" kernel which might blow up to inifinity at $0$. I suppose this is related to Definition 2. Again, I don't know how I need to generalize this to the case of a cylindrical Brownian moton. Definition 4 : This definition is very sloppy in its notation: Let Let $(W_t)_t$ be a centered Gaussian process with covariance $\operatorname E[W_sW_t]=\delta(s-t)$ where $\delta$ denotes the Dirac delta function. Then, in a [lecture note] I've found (Example 3.56), they state that $$B_t:=\int_0^tW_s\;{\rm d}B_s\tag 5\;\;\;\text{for }t\ge 0$$ is a real-valued Brownian motion. I haven't verified that result. Is it correct? Whatever the case is, if this is the reason, why white noise is considered to be the derivative of a Brownian motion, we should be able that every Brownian motion as a representation of the form $(5)$. Can this be shown? The same questions as above remain. Definition 5 : Let $(B_t)_{t\ge 0}$ be a real-valued Brownian motion on $(\Omega,\mathcal A,\operatorname P)$ and define $$\langle W,\varphi\rangle:=\int_0^\infty\varphi(s)\;{\rm d}B_s\;\;\;\text{for }\phi\in\mathcal D:=C_c^\infty((0,\infty))\;.$$ Then $$\langle W',\varphi\rangle:=\int_0^\infty\varphi'(s)\;{\rm d}B_s\;\;\;\text{for }\phi\in\mathcal D$$ is considered to be the generalized derivative of the generalized Brownian motion $W$. The same questions as above remain. Conclusion : I've found different notions of ""white noise"" and ""generalized derivative"" of a Brownian motion, but I don't know in which sense they are consistent and which of them Liu and Röckner meant. So, I would be very happy if someone could give a rigorous definition of these terms in the case of a cylindrical Brownian motion or at least in the case of a Hilbert space valued Brownian motion.","['stochastic-processes', 'probability-theory', 'functional-analysis', 'stochastic-analysis', 'brownian-motion']"
1639335,Show $A=\limsup_\limits{n\to\infty}a_n$.,"Let $\{a_n\}$ be a sequence of real numbers bounded from above, $A\in \Bbb R$. Given any $\epsilon>0$, a)$\exists n_0 \in \Bbb N$ such that $ a_n<A+\epsilon$ for all $n\ge n_0$. b)$\exists k\ge n_0$ such that $a_k>A-\epsilon$. If the sequence satisfies the above two properties, show that $A=\limsup_\limits{n\to\infty}a_n$. I know the definition of the limit superior as:$\limsup a_n = \inf_{\forall m} \sup_{n \ge m} a_n$. Also, if ($a_n$) is a real sequence bounded from above. Let $S :=$ {$t \in \Bbb R:$ $t$ is the limit of a convergent subsequence of ($a_n$) }. Then $A = sup S$. I've proved the opposite direction (i.e. Given $A=\limsup_{n\to\infty}a_n$, then it has the following two properties), but stuck on trying to prove the two properties imply A. Could someone provide a precise proof of this please? Thanks.","['real-analysis', 'sequences-and-series', 'analysis']"
1639384,Find all functions so that $f\left(\frac{x}{f(y)}\right) = \frac{x}{f(x\sqrt{y})}$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question I have to find all functions so that $$ f\left(\frac{x}{f(y)}\right) = \frac{x}{f(x\sqrt{y})} $$ I have no idea how to solve this one. Any help would be appreciated!",['functions']
1639448,Which non-abelian finite groups have the property that every subgroup is normal?,"If $G$ is an abelian group, every subgroup $H$ of $G$ is normal. I searched for non-abelian finite groups $G$ , such that every subgroup is normal and
GAP showed only the groups $G'\times Q_8$ , where $Q_8$ is the quaternion-group
of order $8$ and $G'$ is an abelian group with the additional property that $C_4$ is not a subgroup of $G'$. The largest group I found with GAP was $C_{15}\times Q_8$. Is it true that every non-abelian finite group $G$ with the property that every subgroup of $G$ is normal, is isomorphic to $G'\times Q_8$ with an abelian 
  group $G'$ without $C_4$ as a subgroup ?","['finite-groups', 'normal-subgroups', 'group-theory']"
1639456,"For which values of $a,b$ is the matrix invertible?","I am trying to figure out the below question: 15 . For which values of the constants $a$ and $b$ is the matrix
  $$A = \left[\begin{array}{cc} a & -b \\ b & a \end{array}\right]$$
  invertible? What is the inverse in this case? See Exercise 13. My understanding is that a matrix is invertible when the determinant is not zero. In this case, when $a^2 - b^2 = 0$ the matrix is not invertible. Thus, for any values $a,b$ such that $a^2$ does not equal $b^2$, the matrix is invertible. However, the solutions in the back of the book state that the matrix is invertible if $a$ does not equal zero or if $b$ does not equal zero. Can someone explain this to me?","['matrices', 'linear-algebra', 'inverse']"
1639461,"On groups with presentations $ \langle a,b,c\mid a^2=b^2=c^2=(ab)^p=(bc)^q=(ca)^r=(abc)^s=1\rangle $...","$$
\langle a,b,c\mid a^2=b^2=c^2=(ab)^p=(bc)^q=(ca)^r=1\rangle =\Delta(p,q,r)
$$ This is a presentation of a triangle group $\Delta(p,q,r)$ , a special kind of Coxeter group . EDIT In fact, these are called extended triangle groups, by G. Jones and D. Singerman in Maps, hypermaps and triangle groups ... What about the following presentation: $$
\langle a,b,c\mid a^2=b^2=c^2=(ab)^p=(bc)^q=(ca)^r=(abc)^s=1\rangle 
$$ Do these groups have a name and where are they treated? The presentation in question are motivated by this and that ... ANOTHER EDIT if $p=q=r$ is prime and $s=1$ this is called triangular Fuchsian group here ...","['reference-request', 'group-theory', 'group-presentation', 'coxeter-groups']"
1639462,How to prove these two sets are identical?,"This is more a question of the methadology one should use to solve these type of questions: Say there is a set $V \subseteq X \subseteq Y$ and $U \subseteq Y$ such that 
$$X \setminus V = U \cap X $$ Prove that $$ V = X\cap (Y \setminus U)$$ The easiest approach I found to prove these solutions is to simply draw a venn diagram that fits all the properties and then the equivalence becomes obvious but I don't think that is rigorous enough. The other approach I know of is to do something like this: $$x\in V \implies (x\in X) \wedge (x\notin U\cap X ) \implies (x\notin U\cap V) \implies x\notin U\implies x\in Y\setminus U\implies V\subseteq X\cap (Y\setminus U)$$ $$x\in X\cap (Y\setminus U)\implies (x\in X)\wedge(x\notin U)\implies x\notin X\setminus V
\implies x\in V\implies X\cap (Y\setminus U)\subseteq V$$ However I don't like this approach because it looks so messy. Long ago I took a class on Digital Logic and we had all sorts of rules in which we could open up statements in a systematic way. De Morgan's Laws in that case did not depend on whether or not one set was a subset of another. Is there some kind of similiar methadology that one could use in this case to solve such problems in a systematic way?","['boolean-algebra', 'proof-writing', 'alternative-proof', 'elementary-set-theory']"
1639463,"How can I check whether the group $[16,13]$ in GAP with $3$ generators can be generated by $2$ elements?","The group $[16,13]$ in GAP has structure $(C4\times C2):C2$ and is generated by the permutations $(1234)(5678)$ , $(15)(26)(37)(48)$ and $(57)(68)$ . The group $[16,3]$ in contrast with the same structure is a $2$-generated group : $(12)(34)$ and $(23)(5678)$ is a possible set of generators. How can I prove that the group $[16,13]$ cannot be generated by $2$ elements ?","['finite-groups', 'permutations', 'group-theory', 'gap']"
1639485,How many values does $\sqrt{\sqrt{i}}$ have?,"Wolfram says, there are only two roots, but $\sqrt{i}$ already gives two roots. So if we express them in Cartesian form we can take square roots of them separately and end up with four roots. $$\sqrt{i}=e^{\frac{i\pi}{4}}=\frac{1}{\sqrt2}+i\frac{1}{\sqrt2}$$
But also
$$\sqrt{i}=e^{\frac{-3i\pi}{4}}=-\frac{1}{\sqrt2}-i\frac{1}{\sqrt2}$$
Then take square roots of each of those and you end up with 
$$\sqrt{\sqrt{i}}=e^{\frac{i\pi}{8}},e^{\frac{5i\pi}{4}},e^{\frac{-3i\pi}{4}},e^{\frac{-7i\pi}{4}}$$ It works, doesn't it? Raise each of those to the power of 4 and you get $i$. What am I missing here?","['radicals', 'complex-analysis', 'wolfram-alpha']"
1639501,Suppose that $f(0)=f(2\pi)$. Show that there exists an x such that $f(x)=f(x+\pi)$.,"I am supposed to show that there exists an $x$ in the interval $[0,\pi]$ such that $f(x)=f(x+\pi)$ by considering another function $g:[0,\pi] \to \mathbb{R}$ defined by $g(x)=f(x)-f(x+\pi)$. Should I use a concrete example by considering a trig function?","['continuity', 'real-analysis', 'functions']"
