question_id,title,body,tags
3099005,Find all metrics on a set $X$ consisting of two points. Consisting of one point.,"First, this is an exercise in the first section of Kreyszig's introductory functional analysis text. In this section he has already given several examples of metric spaces, including: $l^\infty$ , $C[a,b]$ , and an example of a discrete metric space. In this section he has stated that we can interpret things like infinite but bounded sequences (for $l^\infty$ ) as points, or continuous functions on closed intervals (for $C[a,b]$ ) as points. So, I think I should interpret this question as all possible metrics for any type of abstract set $X$ , which only has two things--points--in it. Attempt: No matter what the set $X$ considered is, as long as the metric $d$ defined on $X$ is maps $(x,y) \in X \times X$ to the nonnegative real numbers, (not including $+ \infty$ ), maps zero to zero, and is a non-affine function of $(x-y)$ it will suffice as a metric. ... ...all of these assumptions I'm making will just build to the definition of a metric it seems like. Maybe the author means a traditional set of points, i.e., finite tuples? I think I'm missing the spirit of the question.","['metric-spaces', 'functional-analysis', 'real-analysis']"
3099031,Applications of Ramanujan's Master Theorem,"Ramanujan's Master Theorem is really neat. Unfortunately, however I have only used it once before, and I want to use it more. I would like a list of integrals to which I may apply this beautiful theorem. The Theorem: (Taken from Wikipedia ) If $f(x)$ is a complex valued function with a series representation in the form $$f(x)=\sum_{n\geq0}\frac{\phi(n)}{n!}(-x)^n$$ Then $$\int_0^\infty x^{s-1}f(x)\mathrm dx=\Gamma(s)\phi(-s)$$ Where $\Gamma(s)$ is the Gamma function. Cheers!","['integration', 'calculus', 'definite-integrals']"
3099048,Intuition: Dual Space is always Banach,"Theorem. Let $X$ be a normed space and $Y$ be a Banach space. Then the set of continuous linear maps $L(X,Y)$ is a Banach space (with the operator norm). From the well-known theorem above, we get an immediate consequence: Dual spaces $X^*$ are always Banach spaces. Why should that be true intuitively? (I am not looking for a proof.) I'd like to think that somehow, the space $L(X,Y)$ plays more on $Y$ than on $X$ , so it inherites being Banach from $Y$ . Though, I can't quite make out a clear intuition for all this.","['banach-spaces', 'functional-analysis', 'dual-spaces']"
3099066,Solving $f'(x)=f(x+1)$,"I was wondering if it was possible to find functions $f$ such that $$
f'(x)=f(x+1)
$$ for all $x \in \mathbb{R}$ . The only thing i've found so far is that it implies $$
f^{\left(n\right)}\left(x\right)=f\left(x+n\right)
$$ is there any way to solve this ?","['functions', 'ordinary-differential-equations']"
3099093,A Probabilistic Drinking Problem,"This question was asked by a fellow MSE user in chat. Motivational credits to @ Quintec . Question: Bob goes to a bar and drinks a drink. On drink $n$ , Bob has a $1-\left(\frac12\right)^n$ probability of getting drunk. If he doesn't get drunk that drink, then he will add $n$ drinks to the amount of drinks he drinks. Even if he gets drunk before finishing his ""queue"" of drinks, he will still finish the rest of the drinks. He will stop drinking after that. What is his expected number of drinks? Attempt: We have the following relationships: 1

drunk -> 1 (probability 1/2)

not drunk -> 1 2 (adds on one drink - probability 1/2)

1 2

drunk -> 1 2 (probability 3/4)

not drunk -> 1 2 3 4 (adds on two drinks - probability 1/4)

1 2 3 4

drunk -> 1 2 3 4 (probability 7/8)

not drunk -> 1 2 3 4 5 6 7 (adds on three drinks - probability 1/8)

1 2 3 4 5 6 7 and so on Therefore the expectation is given by $$\frac12\times1+\frac12\left[\frac34\times2+\frac14\left[\frac78\times4+\frac18\left[\cdots\right]\right]\right]=\sum_{i=1}^\infty\left[\frac1{2^{i(i-1)/2}}\times\frac{2^i-1}{2^i}\times\left(1+\frac{i(i-1)}2\right)\right]$$ on noticing that $1,2,4,7,\cdots$ are the sequence of one plus the triangular numbers, and this can be written as $$\sum_{i=1}^\infty \frac{2^i-1}{2^{(i^2+i+2)/2}}\left(i^2-i+2\right)\approx1.800$$ Unfortunately, W|A does not return a closed form . So is there any methods to find such a form, if any exists?","['expected-value', 'closed-form', 'probability', 'sequences-and-series']"
3099115,Find the probability that among seven persons no two were born on the same day of the week,"I Was initially thinking P(born on Monday) = 1/7 and P(Not Born on Monday) = 6/7 and then 1/7 * 6/7 = 6/42, but I don't know if that's the correct approach? In addition what is P(at least 2 were born on same day) 
and P(two were born on a Saturday and 2 born on Tuesday) ?","['statistics', 'probability']"
3099146,Difference between Lebesgue Sigma Algebra and Borel Sigma Algebra,"I have read that probability measure cannot be defined on set of all subsets of unit interval namely (0,1]. Proof uses construction of the Vitali set etc. Specifically, it is well known that probability measure with the following properties does not exist on power set of (0,1]. Translational invariance If $0 \leq a \leq b \leq 1$ , then $\mathbb{P}((a,b]) = b-a$ This is actually Lebesgue measure on (0,1]. This might be a dumb question but I cannot find an answer to it. The domain of probability measure is a Borel Sigma algebra while that of a Lebesgue measure is a Lebesgue Sigma algebra. It is well known that Lebesgue sigma algebra has cardinality of $2^{\mathbb{R}}$ while that of Borel sigma algebra is $2^{\mathbb{N}}$ . So even though Lebesgue measure on (0,1] $ \textbf{IS}$ a probability measure satisfying 1. and 2. above, its domain has a cardinality strictly greater than that of Borel Sigma Algebra. Where am I going wrong? Even though Lebesgue measure on (0,1] is in fact a uniform probability measure, why is there a difference in the domains? If not, please clarify.","['measure-theory', 'lebesgue-measure', 'probability-theory', 'borel-measures']"
3099170,Showing that the Cayley Transform is an involution.,"For a finite dimensional vector space V the Cayley Transform is a function $T: Gl(V) \to End(V)$ such that $T(f) = (I-f)(I+f)^{-1}$ . I am asked to show that the Cayley Transformation is an involution, which means that $T(T(f)) = f$ . I tried to do this by evaluating the function and I get that $T(T(f)) = (I-(I-f)(1+f)^{-1})(I+(I-f)(I+f)^{-1})^{-1}$ , I don't really know what to do from here.","['linear-algebra', 'differential-geometry']"
3099189,Find the derivative of $f(\beta) = (\vec{y} -X\beta)^T(\vec{y} -X\beta)$ using the product rule,"So I want to differentiate $f(\beta) = (\vec{y} -X\beta)^T(\vec{y} -X\beta)$ using the product rule. Here: $\vec{y}$ is an $n \times 1$ vector $X$ is an $ n \times p$ matrix $\beta$ is a $p \times 1$ vector In particular, say I just want to expand the original expression like this: $$f(\beta) = (\vec{y} -X\beta)^T(\vec{y} -X\beta) = (\vec{y}^T -\beta^TX^T)(\vec{y} -X\beta).$$ Then, I want to just apply the product rule $$\frac{\mathrm{d}f(\beta)}{\mathrm{d}\beta} =-X^T(\vec{y} -X\beta) - (\vec{y}^T -\beta^TX^T)X = $$ $$ = -X^T\vec{y} + X^TX\beta-\vec{y}^TX+\beta^TX^TX =$$ $$= -X^T\vec{y}+X^TX\beta-(X^T\vec{y})^T + (X^TX\beta)^T.$$ But this doesn't work because the first two terms have dimensions $p \times 1$ , while the last two have dimensions $1 \times p$ . I know I should be able to combine the terms, so what exactly goes wrong in a derivation like this. Note that I don't want to expand the original expression further and then take the derivative; I've seen it done that way, but I really want to figure out why this doesn't work, i.e. what rule I'm missing. Also, I know I could just use the chain rule and the fact that $$\frac{\mathrm{d}}{\mathrm{d}x} x^T x = 2x^T,$$ but I still want to figure out why the product rule doesn't work in the naive way I wanted to do it above. edit: Hmm, is it because the derivative is a function that acts on a vector in this case? So that if we call that vector $\vec{z}$ , we would get $$f'(\beta) \vec{z} = -(X\vec{z})^T(\vec{y} -X\beta) - (\vec{y}^T -\beta^TX^T)(X \vec{z}) = $$ $$ = -\vec{z}^TX^T\vec{y}+\vec{z}^TX^TX\beta-\vec{y}^TX\vec{z} + \beta^TX^TX\vec{z}.$$ But then because these are scalar quantities, we have $$-\vec{z}^TX^T\vec{y} = (-\vec{z}^TX^T\vec{y})^T = -\vec{y}^TX\vec{z} \text{, and}$$ $$ \vec{z}X^TX\beta = (\vec{z}^TX^TX\beta)^T = \beta^TX^TX\vec{z}.$$ So what I wrote above would be perfectly correct, it's just that I could simplify it further this way by taking into account what the derivative is and how it acts?","['derivatives', 'matrix-calculus', 'linear-algebra', 'least-squares']"
3099199,If $L\mid K$ is a finite extension of fields then K is perfect iff L is perfect,"Problem: Let $L\mid K$ be a finite extension of fields. Then $K$ is perfect $\iff$ $L$ is perfect. The implication $\implies$ is quite easy and it has already been discussed here . I'm interested in the implication $\impliedby$ because of the following application: Suppose $\text{char}(k)=p>0$ . Then any finitely generated field extension $K\mid k$ that is perfect has $\text{tr.deg}_k(K)=0$ . For the proof of this recall that a purely trascendental extension is never perfect as $t_1$ is never a $p$ -power in the field $k(t_1,\dots,t_n)$ . Then if $K$ is a finite extension of a purely trascendental extension it wouldn't be perfect as well. That can be restated in a neat way in the language of algebraic geometry: If $\text{char}(k)>0$ then any $k$ -variety with perfect function field must have dimension $0$ . Also it would be interesting to see if the equivalence in the problem is true if we change finite extension by algebraic extension . Again the implication $\implies$ is not so hard and has been discussed here .","['field-theory', 'algebraic-geometry', 'abstract-algebra', 'extension-field']"
3099219,Derivative of the Cayley Transform.,"Given $V$ a finite dimensional vector space, let $R=\{f \in  \operatorname{End}(V) : I+f \text{ is invertible}\}.$ We define the Cayley Transform $T$ such that $T: R \to  \operatorname{End}(V)$ and $T(f) = (1-f)(1+f)^{-1}$ . I am asked to show that the Cayley Transform is differentiable and to find its derivative. I've already showed that the Cayley Transform is an involution but I don't know how to use this to show that its differentiable. I am trying to use the definition of differentiable but all I get is that $T(f+h) = (I-(f+h))(I+(f+h))^{-1}$ and I don't know what to do next","['derivatives', 'differential-geometry']"
3099224,Using the chain rule to guess the derivative of $\frac{1}{x}$,"So I learned that you can use the product rule to guess what the derivative of $\frac{1}{x}$ should be, you just use the fact that $(\frac{1}{x}\cdot x) = 1$ and differentiate both sides and solve for $(\frac{1}{x})'$ .
My teacher said that there is a similar trick for guessing the derivative of $(\frac{1}{x})$ using the chain rule, I tried using the fact that $\frac{1}{x}\circ\frac{1}{x}= x$ but I am stuck.","['calculus', 'derivatives']"
3099288,Need help to evaluate $I_n=\int_{0}^{\infty}e^{-x}\sin(n\ln(x))dx$,"For $n\in\mathbb{N}$ , I'm trying to find a closed form for the following integrals : $$I_n=\int_{0}^{\infty}e^{-x}\sin(n\ln(x))\text{d}x$$ My real objective is to evaluate $\sum\limits_{n=1}^{\infty}\frac{I_n}{n}$ , and since interchanging the sum and the integral didn't lead anywhere,  I suppose that finding a closed form expression for $I_n$ is the way to go, but I'm lost as of how to proceed... Maybe the residue theorem/contour integration could help, but I'm not familiar with complex analysis so I haven't tried it - feel free to use it though. Any suggestion ?","['integration', 'calculus', 'sequences-and-series', 'real-analysis']"
3099326,Why is the norm $1$ of matrix $A$ is equal to the maximum sum of column,"first, I know that there exists a similar question to mine which is in here , and it is actually very well explained. However, there is just one part that I do not understand. That is the conclusion. First, how did they conclude that norm of Matrix $A$ is less than the maximum column sum of matrix $A$ ? Then how did they conclude that it did not the inequality turned into equality?","['numerical-linear-algebra', 'normed-spaces', 'linear-algebra', 'functional-analysis']"
3099355,Generalized Poincaré Lemma,"I'm reading the proof of an improved version of Poincaré's Lemma on Ana Cannas da Silva's Lectures on Symplectic Geometry , page 40. I am terribly confused. Here's the setup: $U_0$ is a tubular neighborhood of the zero section of the normal bundle $NX$ of a submanifold $X\subseteq M$ . She defines $\rho_t\colon U_0 \to U_0$ by $\rho_t(x,v) = (x,tv)$ , for $0 \leq t \leq 1$ , and then we have a homotopy operator $$Q\omega = \int_0^1 \rho_t^*(\iota_{v_t}\omega)\,{\rm d}t,$$ where $v_t$ is, at the point $\rho_t(x,v)$ , the vector tangent to the curve $\rho_s(x,v)$ at $s=t$ . This doesn't look correct to me at $t=0$ . If $t \neq 0$ , then $\rho_t$ is a diffeomorphism with $\rho_t^{-1} = \rho_{1/t}$ , since we have $v_t(x,v) = (x, v/t)$ , and this does not have any extension for $t=0$ . Indeed, $\rho_0(x,v) = (x,0)$ is just a submersion. But she says that if $x \in X$ , then $v_t(x,0) = 0$ because $\rho_t(x,0) = (x,0)$ is a constant curve, which I agree. But $v_0$ doesn't seem to be defined away from $X$ , since we would have in general $$v_t(x,v) = \frac{{\rm d}}{{\rm d}s}\bigg|_{s=t} \rho_s\rho_{t}^{-1}(x,v),$$ and so $\rho_t$ needs to have an inverse. And even if you consider the above integral as an improper integral, why should it converge? So: How can the proof go on if $v_t$ is not continuous? How to make sense of this integral, since what happens near $0$ is exactly what we're interested in?","['vector-fields', 'symplectic-geometry', 'differential-forms', 'differential-geometry']"
3099363,Concentration inequalities for stochastic processes,"I understand this is a rather vague question, but I am curious if there exist any generalization of, e.g. Hoeffding's inequality or Chernoff's inequality to the case of stochastic processes? I am in particular interested in upper bounding the following term $$P(\sup_{t\in T}\vert X(t)-EX(t) \vert \ge c)$$ where $t$ is a continuous (or discrete, if necessary) parameter where $T \subset \mathbb{R}$ , $EX(t)$ is the expectation of the random variable $X(t)$ , and $X$ and $T$ can be arbitrarily nice, for example, perhaps for Gaussian processes and $T$ compact we can say something about the above term?","['statistics', 'probability-distributions', 'reference-request', 'probability-theory', 'probability']"
3099407,Show that $\int_{-\infty}^{\infty} \frac{\cos (\theta) e^{\theta y}}{2 \cosh(\pi y/2)} y dy=\tan (\theta)$.,"I'm not a hardcore mathematician. I have exhausted all integration trick I learned in calculus including change of variables, integration by parts. That cosh on denominator is just very much in the way. How one should approach an integral like this.","['integration', 'calculus', 'definite-integrals', 'analysis']"
3099430,Derivative of $\mathbf{XX}^T$ with respect to $\mathbf{X}$,"Problem $$\nabla_{\mathbf{X}}\mathbf{XX}^T$$ What I Have Done I checked matrix cookbook , but there is no luck. So I tried to derive it from scratch. I have $$(\mathbf{XX}^T)_{kl}= \mathbf{X}_k^T\mathbf{X}_l=\sum_{q=1}^n \mathbf{X}_{kl}\mathbf{X}_{ql}$$ where $\mathbf{X}_i$ is the $i$ -th column of $\mathbf{X}$ . However, when I tried to get $\nabla_{\mathbf{X}_{ij}} (\mathbf{XX}^T)_{kl}$ , I am lost in the indices $i,j,k$ and $l$ and did not how to resolve this issue. Could anyone help me, thank you in advance.","['matrices', 'matrix-calculus', 'derivatives']"
3099432,Evaluating $\frac{2013^3-2\cdot 2013^2\cdot 2014+3\cdot 2013\cdot 2014^2-2014^3+1}{2013\cdot 2014}$,"What is the value of $$\frac{2013^3-2\cdot 2013^2\cdot 2014+3\cdot 2013\cdot 2014^2-2014^3+1}{2013\cdot 2014}?$$ What I have tried: $$\implies\frac{2013^2(2013-2\cdot2014)+2014^2(3\cdot 2013-2014)+1}{2013\cdot 2014}$$ $$\implies\frac{2013^2(-2015)+2014^2(4025)+1}{2013\cdot 2014}$$ I'm not sure what to do next... Help is appreciated! Furthermore, if you are nice, could you also help me on this problem ( Transferring bases of numbers. ) too? Thanks! Max0815",['algebra-precalculus']
3099436,Matrix Multiplication on Riemannian Manifolds,"I am having a hard time understanding the concept of matrix (and / or vector) multiplication on a Riemannian Manifold $(M, g)$ . On $\mathbb R^n $ we can multiply a matrix for a vector in the usual way. How do I translate that on $M$ ? The naive way would be to just do the multiplication on the local coordinates, but this entirely disregards the metric, which seems wrong. Is the matrix multiplication something that lives on $T_vM$ ? Intuitively yes, but why?","['matrices', 'riemannian-geometry']"
3099476,"I integrated $\int_0^1 \sqrt{x-x^2} dx$ after a $u$-substitution and trig substitution, but I'm not sure what to do with the limits",I'm a bit lost on the technique in this problem and could use some insight. When do we have to change the  limits? $$\int_0^1 \sqrt{x-x^2} dx$$ completing the square gives me: $$\int \sqrt{-x^2 +x - \frac{1}{4} + \frac{1}{4} } dx$$ $$\int \sqrt{-(x-\frac{1}{2})^2 + \frac{1}{4}} dx$$ $$ \int \sqrt{-u^2 + \frac{1}{4}} du$$ $$\int \sqrt{\frac{1}{4} - u^2} du$$ ' Is trig sub the best way to go? if $u = \frac{1}{2} \sin \theta$ then $du = \frac{1}{2} \cos \theta d \theta$ $$ = \int \sqrt{\frac{1}{4} - \frac{1}{4} \sin^2\theta} \frac{1}{2} cos \theta d \theta$$ $$ \int \frac{1}{2} cos \theta \frac{1}{2} cos \theta d \theta$$ $$\frac{1}{4} \int cos^2 \theta d \theta$$ using half angle identity I eventually get: $$ \frac{1}{4} \int \frac{1}{2} ( 1 + cos(2 \theta)) d \theta$$ $$\frac{1}{8} \int (1 + cos(2 \theta) d \theta$$ $$ \frac{1}{8} ( \theta + \frac{1}{2} sin 2 \theta)$$ Where do I go from here? I'm a bit lost as to what to do with the u sub and how to get back to x from here.,"['integration', 'trigonometry']"
3099486,"Let $G$ be a group and let $N,H$ be subgroups of G. Suppose that $N\cap H=$ {$I$}. Let $h,h'\in H.$ Prove $Nh=Nh'$ iff $h=h'.$","Let $G$ be a group and let $N,H$ be subgroups of G. Suppose that $N\cap H=$ { $I$ }. Let $h,h'\in H.$ Prove $Nh=Nh'$ iff $h=h'.$ I am trying to teach myself a little abstract algebra and came across this problem. I know this probably seems trivial, but I would just like some verification/advice on the simple proof I came up with for this. I am just trying to make sure I am not making any invalid arguments. Thank you. $(\Rightarrow)$ Suppose $Nh=Nh'$ . Then $N=N(h'h^{-1})$ $\Rightarrow$ $h'h^{-1}\in N$ , but $N\cap H$ = { $I$ } $\Rightarrow$ $h'h^{-1}=I$ $\Rightarrow$ $h'=h.$ $(\Leftarrow)$ Suppose $h'=h$ . Then $h'h^{-1}=I$ $\Rightarrow$ $N(h'h^{-1})=N(I)=N$ $\Rightarrow$ $Nh=Nh'.$","['group-theory', 'abstract-algebra', 'proof-verification']"
3099497,"Show that for any prime $p$ and any integer $m$, $m^p + (p − 1)! m$ is divisible by $p$.","I'm currently working on the following exercise: Show that for every prime number $p$ and every integer $m$ , the number $m^p + (p − 1)! m$ is divisible by $p$ . What I'm doing is the following: $$m(m^{p-1}+(p-1)!)\equiv 0\mod p$$ $$m((1)+(-1))\equiv 0\mod p$$ $$m(0)\equiv 0 \mod p$$ $$0 \equiv 0 \mod p.$$ Is a valid proof? Am I missing something? Any hint or help will be really appreciated.","['number-theory', 'modular-arithmetic']"
3099517,Is there any geometric explanation of Koebe's $1/4$ theorem?,"Koebe's $1/4$ theorem claims that if $f:\mathbb{D}\to \mathbb{C}$ is an injective holomorphic function defined on a unit disk such that $f(0) =0$ and $f'(0)= 1$ , then the image $f(\mathbb{D})$ of $\mathbb{D}$ contains an open disk of radius $1/4$ centered at the origin. There are some known proofs of the theorem, but all of them seems to be nonintuitive. 
For example, one proof uses Bieberbach's coefficient theorem. Is there any possible geometric explanation or even proof of Koebe's theorem? Thanks in advance.",['complex-analysis']
3099534,How can a relation be both not symmetric and not antisymetric?,"I am aware that relations can be both symmetric and antisymmetric, or either one of the two. However, I am still a little bit confused as to why they can not be both (i.e. not symmetric and not antisymmetric)?",['discrete-mathematics']
3099544,"$A,B,C,D$ on a circle. $\widehat{BAC}=\widehat{BDC}$","Points $A,B,C,D$ belong to a circle. What's a rigorous yet simple proof that $\widehat{BAC}=\widehat{BDC}$ ? Does this property have a name? I get that in the above figure: summing angles, $\widehat{BOA}+\widehat{AOC}=\widehat{BOC}=\widehat{BOD}+\widehat{DOC}$ sum of the angles of isosceles triangle $AOB$ is $\Pi$ , thus $\widehat{BOA}=\Pi-2\widehat{BAO}\quad$ and similarly $\widehat{AOC}=\Pi-2\widehat{OAC}\quad$ $\widehat{BOD}=\Pi-2\widehat{BDO}\quad$ $\widehat{DOC}=\Pi-2\widehat{ODC}\quad$ replacing then simplifying, we get $\widehat{BAO}+\widehat{OAC}=\widehat{BDO}+\widehat{ODC}\quad$ thus $\widehat{BAC}=\widehat{BDC}\quad$ Q.E.D. However this reasoning seems dependent on the order of points on the circle, and perhaps other hypothesis.",['geometry']
3099590,Nested sequence of compact subsets covering an open set in $\mathbb{C}$,"Let $U$ be an open set in $\mathbb{C}$ . I would like to prove the following result: There exists a sequence of compact sets $\{K_n\}$ with the following properties: Each $K_n$ is a subset of $U$ . $K_n \subset \mathrm{int}(K_{n+1}) \ \forall n$ , where $\mathrm{int}()$ denotes interior. $\bigcup_{n} K_n = U$ . each bounded component of the complement of $K_n$ meets the complement of $U$ (This result is used without proof in Theorem 1.4.3 of ""Several Complex Variables with Connections to Algebraic Geometry and Lie Groups"" by Joseph L. Taylor.) Construction of  a sequence with the first 3 properties has already been answered here . How do I ensure that the fourth property is also satisfied?","['complex-analysis', 'general-topology', 'metric-spaces', 'compactness']"
3099636,cycles in gradient descent on non-euclidean manifold?,"In a euclidean space, we cannot have gradient descent of a function $f$ and still have cycles. i.e. if $x(t)$ is the path traced by the dynamical system given by $\dot x(t)=-\nabla_x f(x)$ , for some function $f$ , then we cannot have $x(t_0)=x(t_1)\neq x(s)$ for $t_0>s>t_1$ . My question is whether there is an example of an (possibly exotic) space where this doesn't hold. Intuitively I would say it should always hold, but I might not be imagining a weird space where it doesn't.",['differential-geometry']
3099639,Computing sum of lengths of legs of a right triangle,"Consider a right triangle $\triangle ABC$ , where the right angle is $\angle A\hat CB$ , as in the picture below. Let $\alpha=\angle A\hat BC$ . Problem: To determine the sum of the lengths of the legs of $\triangle ABC$ as a function of $\alpha$ and of the radius $r$ of the incircle of $\triangle ABC$ . Actually, the problem is not just this. If it was just this, then my answer would be $$r\left(2+\cot\left(\frac\alpha2\right)+\cot\left(\frac\pi4-\frac\alpha2\right)\right).\tag1$$ However, this is a problem from a textbook on plane Geometry and Trigonometry and the given answer is $$r\left(2+\frac{1+\sin(\alpha)+\cos(\alpha)}{\sin(\alpha)\cos(\alpha)}\right).\tag2$$ Of course, it is easy to show that $(1)\iff(2)$ , but my guess is that whoever made this exercise meant to find $(2)$ directly from the picture. Any idea about how to do that?","['euclidean-geometry', 'trigonometry', 'triangles']"
3099641,Analytically determine if $f(x) = f'(x)$ is possible?,"I was taking a test and two true/false type questions were asked. In one of them, I had to say if there is a function $f(x)$ such that $f(x) = f'(x)$ . Of course, $e^x$ is such a function and almost everyone who has taken a calculus course knows this fact well. In the other question, I had to determine if $f(x) = -f'(x)$ was possible. I was completely stumped at this one. I had never before encountered a function with such property nor did I know how to approach this problem analytically as I am just a high school student. My question is: is there an analytical way to determine if such a function exists? By analytical, I mean no guessing allowed and just giving an example won't be enough. Is this possible? If not, can you give an example of a function with the above property?","['ordinary-differential-equations', 'calculus', 'functions', 'derivatives', 'exponential-function']"
3099670,Why these $2$ methods give the exact same answer for sum of squares?,"Consider $n = 8$ , the sum of squares from $1$ through $8$ is: $1 \times 1 + 2 \times 2 + 3 
\times 3 + 4 \times 4 + 5 \times 5 + 6 \times 6 + 7 \times 7 + 8 \times 8 = 204$ . Also, equal to $1 \times 8 + 3 \times 7 + 5 \times 6 + 7 \times 5 + 9 \times 4 + 11 \times 3 + 13 \times 2 + 15 \times 1 = 204$ . The second one logic is that I start with $1$ , then I increment by $2$ each time and subtract $1$ from the second one, until I reach $1$ . For $n = 2$ . $1 \times 1 + 2 \times 2 = 4 = 1 \times 2 + 3 \times 1$ . For $n = 3$ . $1 \times 1 + 2 \times 2 + 3 \times 3 = 1 \times 3 + 3 \times 2 + 5 \times 1$ The question is, why is it supposed to be the equal the one above? I tried it with a lot of values for $n$ ?",['number-theory']
3099721,Determinant of a symmetric matrix with entries on diagonals,"I am interested in the calculation of the determinant of the $N\times N$ symmetric matrix \begin{equation*}
\mathbf B = \left(\begin{array}{*{20}c}
2	&	& -1&	&-1& &\\
	& 2	&	& -1& 	& \ddots&    \\
 -1& & \ddots&  &\ddots & & -1 \\
 & -1& & & &-1&  \\
 -1& & \ddots&  &2 &&-1 \\
 &\ddots & &-1  & &2& \\
  & &-1 &  &-1 & &2   \\
\end{array}\right)
\end{equation*} where the first $-1$ on the left in the first line from left to right is at position n, and the second $-1$ at position $m>n$ . The blank parts of the matrix are implicitly filled with zeros. Are there results or properties for such class of matrices that would allow to compute the determinant for any dimension $N$ , without its explicit calculation (that could be accomplished e.g. via Leibniz formula for determinants)?","['matrices', 'determinant', 'linear-algebra', 'symmetric-matrices']"
3099727,"In a right angled $\triangle ABC$, $DE$ and $DF$ are perpendicular to $AB$ and $BC$ respectively. What is the probability of $DE\cdot DF>3$?","In a right angled $\triangle ABC$ , $\angle B = 90^\circ$ , $\angle C = 15^\circ$ and $|AC| = 7.\;$ Let a point $D$ ( Random Point ) be taken on $AC$ and then perpendicular lines $DE$ and $DF$ are drawn on $AB$ and $AC$ respectively. What is the probability of $DE\cdot DF >3?$ Attempt : By trigonometry, I got the length of other two side from the hypotenuse $AC:$ $AB$ $\approx 1.8117$ $BC$ $\approx 6.7614$ . And than, I got the equation that $DE\cdot DF = (6.7614 - DE)\cdot AE\;$ (from the similarity of both $\triangle AED$ and $\triangle DFC$ ) Again, from right angled $\triangle AED$ , $\dfrac{AE}{DE} =  \tan 15^{\circ}\quad \implies \quad AE = DE\cdot \tan 15^\circ$ Here, I got stuck. I couldn't find a way out to proceed and skip that situation. I became lost and was unable to complete that process. Any kind of help or clue will be greatly helpful for me to step forward.","['contest-math', 'triangles', 'geometry', 'probability']"
3099729,Asking about $\sum_{n=2}^{\infty}\frac{(-1)^n}{n}\left[\frac{4372}{(2n-1)^7(n-1)}+\frac{n^2+n+4372}{(2n+1)^7(n+1)}\right]$,$$\sum_{n=2}^{\infty}\frac{(-1)^n}{n}\left[\frac{4372}{(2n-1)^7(n-1)}+\frac{n^2+n+4372}{(2n+1)^7(n+1)}\right]=\frac{61}{184320}\pi^7\tag1$$ Step 1: $$\sum_{n=2}^{\infty}\frac{(-1)^n}{n}\left[\frac{4372}{(2n-1)^7(n-1)}+\frac{4372}{(2n+1)^7(n+1)}\right]+\sum_{n=2}^{\infty}\frac{(-1)^n}{n}\left[\frac{n^2+n}{(2n+1)^7(n+1)}\right]\tag2$$ $$\sum_{n=2}^{\infty}\frac{(-1)^n}{n}\left[\frac{4372}{(2n-1)^7(n-1)}+\frac{4372}{(2n+1)^7(n+1)}\right]+\sum_{n=2}^{\infty}(-1)^n\left[\frac{1}{(2n+1)^7}\right]\tag3$$ Recall $$\sum_{n=0}^{\infty}(-1)^n\frac{1}{(2n+1)^7}=\frac{61}{184320}\pi^7$$ It looks like that this sum $$\sum_{n=2}^{\infty}\frac{(-1)^n}{n}\left[\frac{4372}{(2n-1)^7(n-1)}+\frac{4372}{(2n+1)^7(n+1)}\right]$$ is a rational number I am not able to show that sum $(1)=\frac{61}{184320}\pi^7$ Any help.Thank you!,['sequences-and-series']
3099801,Evaluating $\int_{-\infty}^0 \log(\frac{1}{2}\operatorname{erfc}(x))\mathrm dx$,I am looking to evaluate $$\int_{-\infty}^0 \log\left(\frac{1}{2}\operatorname{erfc}(x)\right)\mathrm dx = -0.337~668~477...$$ Both Maple and Mathematica have failed to give a closed-form expression but indicate the value is around $−0.337~668~477...$ which doesn't appear to be close to some well known quantity.,"['integration', 'calculus', 'definite-integrals', 'error-function']"
3099836,Equation in the complex field $(z+2)^6=z^6$,"I'm wondering why if $z$ is a solution of the equation $$
         (z+2)^6=z^6
$$ then we must have $\Re(z)=-1$ . I've tried to take the real part of both sides, noticing that $$
 \Re((z+2)^6)=\Re([(z+2)^3]^2)=|(z+2)^3|^2
$$ but it doesn't seem to work. Thank you in advance.","['complex-analysis', 'complex-numbers']"
3099842,Cosine of a Linear System,"Let $\mathbf{A}$ be a $m\times n$ matrix, $\mathbf{y}$ and $\mathbf{b}$ are vectors of dimension $n\times 1$ . All of them have real entries. I am interested in solving the system of equations $$\mathbf{y} = \cos(\mathbf{Ax}+\mathbf{b})$$ where cosine is applied element wise on the vector $\mathbf{Ax}+\mathbf{b}$ . If cosine is not there, the solution is straightforward and I know this. The problem is that $\arccos(\mathbf{y})$ is one-to-many mapping. Thus for each $\mathbf{y}$ among that, we need to consider whether a solution exists or not. How can I solve this problem? I tried the following and got stuck after a while. Let $\mathbf{z} = \arccos(\mathbf{y})$ . Since $\cos(2d\pi+x)=\cos(x)$ for any integer $d$ , it follows that $$z_i = 2d_i\pi + r_i$$ for every positive integer $d_i$ and $r_i\in(0,2\pi)$ . Thus, the system of equations (after rearrangement in vector form) becomes $$2\pi\mathbf{d}+\mathbf{r}-\mathbf{b} = \mathbf{Ax}$$ Thus, we are now interested in solving a system of equations to find the integer vector $\mathbf{d}$ and $\mathbf{x}$ such that above equation is satisfied. Is this correct? Or intuitively, is there a vector in the column space of $\mathbf{A}$ such that it can be expressed as the LHS above for some integer vector $\mathbf{d}$ ? Are all this steps correct?","['matrices', 'linear-algebra']"
3099881,Area using $\iint dxdy$,"I know about finding a volume under a curve using $\iint f(x,y) dxdy$ . But what $\iint dxdy$ describe? Should I consider $f = 1$ ? I thought if I need to find an area, not the volume shouldn't the $\text{Area} = \iint 0 dx dy$ . So that the $z$ -axis is always zero? But of course, the area will also be zero. I'm super confused about this concept. Why should I care to find an Area under region? Isn't that volume is something we should consider and not just the area?","['integration', 'multivariable-calculus']"
3099889,Intuition behind Riemannian-metric,"What is a Riemannian metric? I have just started reading 'Riemannian Geometry' using primarily do Carmo's text and I've got the idea that to introduce a notion of distance to be able to talk about length of curves on smooth manifolds, angles between curves we introduce something called a 'Riemannian metric'.
It's definition on the other hand, seems very unintuitive(Carmo's definition doesn't involve anything tensor related). I guess it involves an inner product in it because we want to measure length of tangent vectors (but why?) and what better way to measure it than using inner product... To be able to proceed further into the theory and make sure I can make sense of the things, I think it's important I understand the definition very well but after reading it again and again, I'm not sure if I 'get' it. I would love it if someone could give me an explanation for it. Also, if we want to give a metric structure (to be able to talk about all things distance related) then why not define a metric (the topological one, satisfying positivity, symmetry, triangle inequality) rather than this? Or maybe I'm confused and Riemannian metric as we've defined is actually that metric(topological one) and together with it, our smooth manifolds becomes a metric space? If that's the case, wouldn't the theory of Riemannian Geometry become somewhat easy as we already know a lot about metric spaces?(I seriously think, that that is not the case) Thanks a lot for reading and answering/commenting (on) my post!","['intuition', 'riemannian-geometry', 'differential-geometry']"
3099901,Proof by induction: $\sum_{i=1}^{2^n} \frac{1}{2i-1} > \frac{n+3}{4}$ [SOLVED],"I need help to proof the following inequality by induction $$P(n):\quad\sum_{i=1}^{2^n} \frac{1}{2i-1} > \frac{n+3}{4}$$ Here's what I did so far.. Please tell me where I'm wrong or how to continue First I check $P(1)$ is true,
Then I asume $P(n)$ is true for all Natural numbers $n\leq k$ and want to see if it implies the following statement: $$P(k+1): \quad\sum_{i=1}^{2^{k+1}} \frac{1}{2i-1} > \frac{(k+1)+3}{4} $$ So I begin like this $$\sum_{i=1}^{2^{k+1}} \frac{1}{2i-1} = \sum_{i=1}^{2^{k}} \frac{1}{2i-1} + \sum_{i=2^k+1}^{2^{k+1}} \frac{1}{2i-1}$$ Then by induction hypothesis $$\sum_{i=1}^{2^{k}} \frac{1}{2i-1} + \sum_{i=2^k+1}^{2^{k+1}} \frac{1}{2i-1}
>\frac{k+3}{4}+\sum_{i=2^k+1}^{2^{k+1}} \frac{1}{2i-1}$$ Here is where I'm stuck. I think that I should prove that $$\sum_{i=2^k+1}^{2^{k+1}} \frac{1}{2i-1} > \frac{1}{4}$$ That way, I could state that $p(k+1)$ is valid $\forall k \in N$ . Is this the right way to solve it? How should I continue?","['proof-verification', 'discrete-mathematics']"
3099925,changing the order of Integration weirdly,"So far what I understood is if your limits are some constant, you can find the volume under a function f(x,y) is $\begin{equation}
\int_{c}^{d} \int_{a}^{b}f(x,y) \,dx\,dy
\end{equation}$ and it is equal to $\begin{equation}
\int_{a}^{b} \int_{c}^{d}f(x,y) \,dy\,dx
\end{equation}$ And if you have a variable limit its not always equal. But I came across a problem which looks like this: $\begin{equation}
\int_{0}^{1} \int_{x}^{x+2}f(x,y) \,dx\,dy
\end{equation}$ And here I am completely lost. I thought the question doesn't make any sense. As you can see the x limit is from x to x+2. why the x limit is not in y? How can I even draw the limit of x and x+2 in x-axis? But in textbook they simply solved it by changing the order of Integration. $\begin{equation}
\int_{0}^{1} \int_{x}^{x+2}f(x,y) \,dx\,dy
\end{equation}$ = $\begin{equation}
\int_{0}^{1} \int_{x}^{x+2}f(x,y) \,dy\,dx
\end{equation}$ And now the question looks normal. But can we just change the dx and dy anytime we want? I am completely lost here. Please help me! Here is the actual question and the answer in a handwritten note that my teacher sent me.","['integration', 'multivariable-calculus']"
3099949,Assumptions on test statistics to guarantee uniform asymptotic level,"I'm trying to formalize the fundamental ideas behind test statistics in the context of hypothesis testing. In particular, I want to define different concepts of test statistics leading to different desireable qualities of tests. I'm having trouble formalizing the idea of a ""uniform test statistic"" yielding a test of uniformly asymptotic level (defined below) and was hoping for some input. Setup Let $(\mathcal{X}, \mathbb{E})$ be a measure space and let $\mathcal{P}$ be a set of probabilty measures on the space. Let furthmore $\mathcal{P}_0 \subseteq \mathcal{P}$ . We imagine getting a sample of size $n$ sampled independently, i.e. some $x \in \mathcal{X}^n$ and want to determine whether any of the measures in $\mathcal{P}_0$ are acceptable distributions for the observation. For each $n \in \mathbb{N}$ , we define a test of size $n$ as a partition of $\mathcal{X}^n$ into an acceptance region $\mathcal{A}_n$ and a critical region $\mathcal{A}_n^c$ . This partition is also expressed through the critical function $\psi_n: \mathcal{X}^n \to \{0,1\}$ given by $$
\psi_n(x) = \begin{cases}
0 & \text{if $x \in \mathcal{A}_n$}\\
1 & \text{if $x \in \mathcal{A}_n^c$}
\end{cases}
$$ A sequence of tests is either $(\mathcal{A}_n)_{n \in \mathbb{N}}$ or equivalently $(\psi_n)_{n \in \mathbb{N}}$ . For a given level $\alpha \in (0,1)$ a sequence of tests is said to have pointwise asymptotic level if $$
\sup_{\nu \in \mathcal{P}_0} \limsup_{n \to \infty} P_\nu(\psi_n =1) \leq \alpha
$$ and uniformly asymptotic level if $$
\limsup_{n \to \infty} \sup_{\nu \in \mathcal{P}_0}  P_\nu(\psi_n =1) \leq \alpha
$$ where $P_\nu$ denotes the probability assuming that $X$ has distribution $\nu$ . Pointwise asymptotic test statistic Let $(T_n)_{n \in \mathbb{N}}$ be a sequence of functions $T_n: \mathcal{X}^n \to \mathbb{R}$ . If $T_n(X^n)$ converges in distribution to the same distribution for all $\nu \in \mathcal{P_0}$ , we say that $(T_n)_{n \in \mathbb{N}}$ is a pointwise asymptotic test statistic wrt. $\mathcal{P}_0$ . Letting $V$ be a random variable with the same distribution as the limit of $T_n(X)$ for any $\nu \in \mathcal{P}_0$ , if $\mathcal{B}$ is a closed set such that $P(V \in \mathcal{B}) \leq \alpha$ , we say that the sequence of tests $$
\psi_n(x) = \begin{cases}
1 & \text{if $T_n(x) \in \mathcal{B}$}\\
0 & \text{otherwise}
\end{cases}
$$ is based on the pointwise asymptotic test statistic $(T_n)_{n \in \mathbb{N}}$ . Note that it is quite easy to show that this test has pointwise asymptotic level by an application of the Portmanteau theorem (for the second to last inequality) since $$
\limsup_{n \to \infty} P_\nu(\psi_n = 1) = \limsup_{n \to \infty} P_\nu(T_n(X^n) \in \mathcal{B}) \leq P(V \in \mathcal{B}) \leq \alpha
$$ and then taking supremums. Uniform asymptotic test statistic What extra assumptions are required to prove uniform asymptotic level of a test constructed as above? We would need to ensure that the $$
\sup_{\nu \in \mathcal{P}_0} P_\nu(\psi_n=1) = \sup_{\nu \in \mathcal{P}_0} P_\nu(T_n(X^n) \in \mathcal{B})
$$ are convergent to $P(V \in \mathcal{B})$ . What is a sufficient condition for this to occur? Hope someone can help! EDIT: I'm thinking that some conditions are required on $\mathcal{P}_0$ , since in the case of $\mathcal{P}$ being all distributions on $\mathbb{R}$ with finite variance and $\mathcal{P}_0$ being all distributions with mean zero, one can show that the usual $t$ -test has pointwise asymptotic level but not uniform asymptotic level.","['statistics', 'hypothesis-testing']"
3099970,Does $\sum_{i=1}^n a_i^j=\sum_{i=1}^n b_i^j$ for infinitely many $j$ imply $\{a_i\}=\{b_i\}$?,"Suppose $a_1,...,a_n,b_1,...,b_n$ are real numbers with $\sum_{i=1}^na_i=\sum_{i=1}^n b_i,\sum_{i=1}^n a_i^2=\sum_{i=1}^n b_i^2$ and for infinitely many $j\geq3$ , $\sum_{i=1}^n a_i^j=\sum_{i=1}^n b_i^j$ . Does this imply $\{a_1,...,a_n\}=\{b_1,...,b_n\}$ ? The answer is positive if I can find infinitely many even integers $j$ such that $\sum_{i=1}^n a_i^j=\sum_{i=1}^n b_i^j$ . But if I don't have this, still is the result true? Intuitively it seems to be.","['real-numbers', 'real-analysis']"
3100000,Calculation of $\lim\limits_{n\to \infty }\frac {(2n)!}{(2^n(n)!)^2} $,I've had a hard time computing the limit $\lim\limits_{n\to \infty }\frac {(2n)!}{(2^n(n)!)^2} $ either by bounding it or by simplifying it. I would appreciate some help. (P.S. I came across this limit while using the ratio test to calculate the radius of convergence of the solution of $y''+xy'+2y=0 $ around $x_0=0$ in the form of power series.),"['ordinary-differential-equations', 'real-analysis', 'calculus', 'power-series', 'limits']"
3100022,Maximal subgroup of Wreathed 2-groups,"Definition A $2$ -group $S$ is called wreathed if it is isomorphic to $(C\times C)\rtimes \langle i \rangle$ where $C$ is a cyclic group of order $2^n$ and $i$ is an involution with action $(a,b)^i=(b,a)$ for all $(a,b)\in C\times C$ . I wonder about the structure of maximal subgroups of $S$ . Clearly, one of them is $C\times C$ and it is abelian. I think the rest of the maximal subgroups is not abelian when $n\geq 2$ . (When $n=1$ we have a dihedral group of order $8$ .) However, if $M$ is a maximal subgroup of $S$ then $M$ has an abelian subgroup of index $2$ . Any remark and reference is welcome.","['group-theory', 'finite-groups', 'p-groups', 'reference-request']"
3100050,Proof that composition of two permutations is again a permutation.,"Permutations are symmetries of a (not necessarily finite) set $X$ , often denoted as Sym(T). That is, a permutation $p: X\to X$ is a bijective map from a set $X$ to itself. I wish to prove the following and wish to check my approach: The composition of two permutations is again a permutation. Approach: Proof 1 Given two permutations $p_1$ and $p_2$ we know that these are bijective , therefore by definition of a bijection: $$\forall y \in X \quad  \exists ! x\in X: p(x)=y$$ In English : Every element in the codomain, corresponds to a unique element in the domain. It thus uniquely pairs elements in the codomain to elements in the domain. We now consider the composition of these two permutations and show it is bijection from $X$ to itself. To prove bijectivity one can prove that there exists one unique element in the domain, for every element in the codomain. Consider an arbitrary $z\in X$ , we then know since $p_1$ is permutation (bijection), so there is some unique $y$ , such that $$ p_1(y)=z$$ Now since $y\in X$ , by there must exist a unique element $x$ , such that we can write: $$ p_2 (x)=y $$ We conclude that: $$ p_1(p_2(x))=p_1(y)=z$$ Since for every element $z$ in the codomain $X$ , there exists a unique element $x$ in the domain, we have a bijective map. This means that the composition $p_1 \circ p_2$ is again a permutation on $X$ . $\square$ Proof 2: Canonical approach:
Let $f$ and $g$ be bijective maps from a set $X$ to itself. We will prove that the composition $f \circ g$ is bijective. Injectivity: (each element that is reached is reached once) Consider arbitrary $a, b \in X$ such that: $$ f(g(a))=f(g(b))$$ by injectivity of $f$ we know that $g(a)=g(b)$ . Now by injectivity of $g$ we have that $a=b$ , hence the composition $f \circ g$ is injective. surjectivity: (each element is reached)
We have to prove that for every element $b \in X$ , there exists some element $a$ sucht that $f(g(a))=b$ . Indeed we have bijective maps so the inverse map is a well-defined bijective map for each of these maps $f, g$ . Consider the element $a=g^{-1}( f^{-1}(b))$ and observe: $$ f(g(a))=f(g(g^{-1}( f^{-1}(b))))=f(f^{-1}(b))=b$$ By the associativity property of maps (and the fact that composition of a map with its inverse yields the identity). Injectivity and surjectivity both hold therefore the composition is bijective. (every element is reached exactly once) Proof 3 Alternatively, by the pigeonhole principle we have that for finite sets of the same cardinality a map is surjective if and only if it is injective. Consider arbitrary $a, b \in X$ such that: $$ f(g(a))=f(g(b))$$ by injectivity of $f$ we know that $g(a)=g(b)$ . Now by injectivity of $g$ we have that $a=b$ , hence the composition $f \circ g$ is injective. Now we also know the composition is surjective and therefore bijective.","['permutations', 'functions', 'proof-verification']"
3100081,Rank/determinant of the $n\times n$ matrix $((a_{ij}))$ where $a_{ij}=(i+j-1)^2$,"I am trying to find the rank and determinant of the following $n\times n$ matrix : $$A=\begin{bmatrix}1^2&2^2&3^2&\cdots&n^2
   \\ 2^2&3^2&4^2&\cdots&(n+1)^2
   \\\vdots&\vdots&\vdots&\ddots&\vdots\\n^2&(n+1)^2&(n+2)^2&\cdots&(2n-1)^2
   \end{bmatrix}$$ I verified that the determinant of $A$ vanishes for $n> 3$ , implying that $A$ has full rank for $n\le 3$ . And the rank seems to remain $3$ for $n>3$ . But I could not provide a rigorous proof. Is there a way to deduce the rank/determinant without reducing $A$ to echelon form or any other shortcut? Or how can I easily find the number of linearly independent rows/columns? Any hints would be great. Related question: Determinant of the matrix with $a_{i,j} = (i+j)^2$ .","['matrix-rank', 'determinant', 'linear-algebra']"
3100096,Can any finite group of order $n$ be embedded in $SL_n(\mathbb{Z})$?,"If $G$ is a finite group of order $n$ , it can be embedded in $S_n$ by Cayley's theorem and therefore in $GL_n(\mathbb{Z})$ and $SL_{n+1}(\mathbb{Z})$ . Can $G$ be embedded in $SL_n(\mathbb{Z})$ ?","['general-linear-group', 'group-theory', 'finite-groups']"
3100102,Let $G$ be group with order $p^n$; does there then exist a sequence of normal subgroups?,"I would like to show the following statement: Let $p$ be a prime. Let $G$ be group with order $p^n$ . Let $H$ be a normal in $G$ with order $p^k$ . Then prove $H$ has subgroups $K$ such that $K$ has order $1,p,p^2,\ldots,p^k$ and $K$ is normal in $G.$ I was trying to prove this by induction on $k$ . When $k=0$ or $1$ , this is clear, since $H=\{e\}$ or $H\le Z(G)$ .
Suppose this is true for $k-1$ .  Let $H$ be a normal subgroup of $G$ with order $p^k$ , then by Sylow theorem, $H$ has (in fact normal) subgroups of order $1,p,\ldots,p^{k-1}$ . However how can I show they are normal in $G$ ? Thanks for any hints or helps!","['group-theory', 'p-groups']"
3100146,Definition of a geometric simplicial complex,"For $a_0,...,a_k$ affinely independent points in $\mathbb{R}^N$ for $N\ge k$ we define a $k$ -simplex $\sigma$ to be $\sigma=\{\sum_{i=1}^kt_ia_i|t_0+...+t_k=1, t_i\ge 0\}$ . A simplicial complex $K$ is then a finite collection of simplices in some $\mathbb{R}^n$ such that a) $K$ contains all faces of a simplex $\sigma\in K$ and b) the intersection of two simplices is a face of each of them. I have two questions: 1) Why do we need the condition a)? What confuses me is that, in particular taking geometric realization, one can not distinguish e.g. between the standard simplex and the union of all its faces. Is there a historical reason for this formalism? 2) What if we defined $\sigma=\{\sum_{i=1}^kt_ia_i|t_0+...t_k=1, t_i > 0\}$ , a somewhat open analogue of the above definition, and take simplicial complexes of open simplices? This seems to be more natural to me considering the geometric realization (we don't take point-sets twice), and the notion of the interior and closure of a simplex is much more natural. However, this approach seems to be quite rare in the literature. Is there a good reason to consider closed simplices and do both theories coincide?","['geometric-topology', 'general-topology', 'combinatorics', 'algebraic-topology']"
3100148,Why $\frac{1}{2} \arctan(x) = \arctan(x-\sqrt{x^2+1}) + \frac{\pi}{4}$?,"Since $$\dfrac{d}{dx} \left( \dfrac{1}{2} \arctan(x) \right) = \dfrac{d}{dx} \left( \arctan(x-\sqrt{x^2+1}) \right) $$ then the format of their graphs are the same, but separated by a constant, which is $\dfrac{\pi}{4}$ . That is, $$\dfrac{1}{2} \arctan(x) = \arctan(x-\sqrt{x^2+1}) + \dfrac{\pi}{4} $$ My question is: why is this constant and what procedure is done to discover it.","['calculus', 'trigonometry']"
3100168,determining if sequence has upper bound,"I am somewhat stuck in my calculations when determining if sequence has an upper bound. The sequence $$x_n = \frac{1}{n+1}+\frac{1}{n+2}+..+\frac{1}{2n-1}+\frac{1}{2n}$$ Is equal to $$\frac{1}{n}(\frac{1}{1+\frac{1}{n}}+\frac{1}{1+\frac{2}{n}}+..+\frac{1}{1+\frac{n}{n}})$$ And so I notice that all the denominators are greater than 1, which means that all terms in the parentheses are less than 1. But how can I determine further if there is an upper bound?","['real-analysis', 'harmonic-numbers', 'cauchy-schwarz-inequality', 'sequences-and-series', 'inequality']"
3100174,How to prove that at least two vertices have the same degree in any graph? [duplicate],"This question already has answers here : If $n$ is a natural number $\ge 2$ how do I prove that any graph with $n$ vertices has at least two vertices of the same degree? (2 answers) ""In a party people shake hands with one another"" (4 answers) Closed 5 years ago . I want to prove that at least two vertices have the same degree in any graph (with 2 or more vertices). I do have a few graphs in mind that prove this statement correct, but how would I go about proving it (or disproving it) for ALL graphs?","['graph-theory', 'combinatorics', 'discrete-mathematics']"
3100189,"Show that each number of $100!+1,100!+2,100!+3,...,100!+100$ is composite","I'm working in the following Wilson's theorem excercise: Show that each number of $100!+1,100!+2,100!+3,...,100!+100$ is composite number. I'm starting from: $$100! \equiv -1 \pmod {101}$$ My thoughts about this is that I may be able to add the required number each time to that congruence, so for the first case I would have: $$100! +1\equiv (-1)(+1) \pmod {101}$$ $$100! +1\equiv 0 \pmod {101}$$ Showing that $101k=100!+1$ so $100!+1$ is a composite number and I would basically do the same for each number. Is that correct? Am I missing something? Any hint, help or correction will be really appreciated.","['number-theory', 'modular-arithmetic']"
3100200,Minimize $152207x-81103y$ over the positive integers.,"Minimize the expression $152207x-81103y$ over the positive integers,
  given $x,y\in\mathbb{Z}.$ So the book takes me through modular arithmetic and how to find $\text{gcd}(a,b)$ in order to solve diophantine equations. Then this question pops up in the same chapter. I know how calculate using modular arithmetic, I know how to find $\text{gcd}(a,b)$ and solve diophantine equations but I don't know how to bunch them up together in order to solve this. How should I think?","['modular-arithmetic', 'divisibility', 'elementary-number-theory', 'discrete-mathematics', 'linear-diophantine-equations']"
3100264,Solving $y'' - 2xy' - 2y = 0$ using power series.,"$$(1) : y'' - 2xy'
 - 2y = 0 $$ Determine the power series solutions to the equation (1). Let $y$ be a a solution to the equation $(1)$ , defined on $\mathbb{R}$ and from $C^2$ , prove that for all $x \in \mathbb{R} $ $$ \left( e^{x^2} \left( e^{-x^2} y(x) \right)' \right)' = 0 $$ Deduce the set of solutions of the equation (1). $y$ as a power series is: $y = \sum_{n = 0}^{+ \infty} a_n x^n$ Pluggins it in the equation $(1)$ , we get: $$ \sum_{n = 2}^{+ \infty} \left[ (n+1)(n+2)a_{n+2} - 2na_n - 2a_n \right]x^{n}  
 - 2a_0 - 2a_1x = 0 $$ We get from this the relation: $$ a_{n+2} = \frac{2}{n} a_n$$ I do not know how to proceed to get the full solution as a power series. as I don't know how to answer question $2$ and $3$ .","['power-series', 'sequences-and-series', 'ordinary-differential-equations', 'real-analysis']"
3100310,"How to show that, in this case, all the Sylow $p$-subgroups of $G$ are abelian.","Let $G$ be a finite, simple group of order $n$ . Let $p$ be a prime divisor of $|G|$ and suppose that the number of conjugacy classes of $G$ is $> \frac{n}{p^2}$ . Then all the Sylow $p$ -subgroups of $G$ are abelian. Clearly we may assume that $G$ is not abelian. Moreover we may assume that the power of $p$ which divides the order of $G$ is $>2$ , because a group of order $p^2$ is abelian. By our assumption and by simplicity of $G$ , we have $Z(G)=\{1\}$ . I would like to apply in some way the Theorem of Burnside (Theorem 3.8 of Isaacs' Character theory of finite groups), but i don't know how to do it. Do you have any hints?","['characters', 'representation-theory', 'abstract-algebra', 'sylow-theory', 'group-theory']"
3100323,Flatness criterion for a finite type algebra based on the Jacobian,"In order to show that two definitions of smoothness in algebraic geometry coincide I would like to see a direct proof of the following fact: Let $R$ be a commutative ring and $f_1,\dots,f_k\in R[x_1,\dots,x_n]$ polynomials such that the Jacobian $J=(\partial f_{j}/\partial x_i)_{i,j}$ has total rank in every residue field $k(P)$ for $P\in\text{Spec}(R)$ . Then the following $R$ -algebra is flat $$A=R[x_1,\dots,x_n]/(f_1,\dots,f_k).$$ The idea is that $\text{Spec }A\rightarrow \text{Spec }R$ should be a smooth morphism under this hypothesis and smooth morphisms are flat by definition. Edit: The two definitions that I am trying to compare are the following (Gortz and Wedhorne Algebraic Geometry I) A morphism $f:X\rightarrow Y$ of schemes is smooth of relative dimension $d$ if $\forall x\in X$ there exists an open neighborhoods $U$ of $x$ such that $f(U)\subseteq V$ , $V=\text{Spec}(R)$ and an open immersion $$j:U\rightarrow R[T_1,\dots,T_n]/(f_1,\dots, f_{n-d})$$ of $R$ -schemes for suitable $n$ and $f_i$ such that the Jacobian matrix $$J_{f_1,\dots,f_{n-d}(x)}=\left ( \frac{\partial f_i}{\partial T_j}(x)\right )\in M_{(n-d)\times n}(\kappa(x))$$ has rank $n-d$ . (Qing Liu, Hartshorne, Wikipedia) A morphism $f:X\rightarrow Y$ is called smooth if it is locally of finite presentation, flat and for each $y\in Y$ the fiber $X_y$ is a smooth $\kappa(y)$ -variety. Even though this definitions are stated in full generality I think it can be wise to restrict them to the locally noetherian case (otherwise we can't speak about regular points for example). Maybe later the locally noetherian case implies the finite presentation case by some base change trick. The implication 2. $\implies$ 1. is a bit hard but is based in the fact that if $f:X\rightarrow Y$ is a smooth morphism then it is locally a complete intersection, that is, locally in $x$ we can decompose $f$ as a regular immersion $X\rightarrow \mathbb{A}^{n+1}_Y$ followed by a projection $\mathbb{A}^{n+1}_Y\rightarrow Y$ (the sketch is in Liu Remark 3.19 and the references therein). Now for 1. $\implies$ 2., it is clear that 1. implies locally of finite presentation and smooth fibers. The problem was the flatness part and that's the reason of the question.","['algebraic-geometry', 'commutative-algebra']"
3100337,Probability distribution to maximize the expected distance between two points,"Let $M$ be some metric space. We will also say that $M$ is a measurable space . How do you find a probability measure $P$ on $M$ that maximizes the expected distance between two points? (That is, $P$ maximizes $E[d(X,Y)]$ , where $X$ and $Y$ are independent random variables with distribution $P$ .) If $M$ is a discrete metric, then $P$ will the uniform distribution, for example. Note that we can think of this as a game, and $P$ as a mixed strategy . You would have two players, which are both trying to maximize the distance. This is not a purely game-theoretical situation though, since we need to require the players to pick the same strategy. There is something called superrationality that might apply, but it is not well studied though. Maybe techniques similar to those used in game theory could be used though.","['metric-spaces', 'expected-value', 'optimization', 'probability-theory', 'probability']"
3100338,Eigenvalues of a matrix whose square is zero,"Let $A$ be a nonzero $3 \times 3$ matrix such that $A^2=0$ . Then what is the number of non-zero eigenvalues of the matrix?
I am unable to figure out the eigenvalues of the above matrix. P.S.:  how would the answer change if it were given that $A^3=0$ ?","['eigenvalues-eigenvectors', 'matrices', 'nilpotence', 'linear-algebra', 'matrix-equations']"
3100367,"Prove that if a graph $G$ has a Hamilton path, then for every $S\subseteq V(G)$, the number of components of $G-S$ is at most $|S|+1$.","Prove that if a graph $G$ has a Hamilton path, then for every $S\subseteq V(G)$ , the number of components of $G-S$ is at most $|S|+1$ . My solution (rough and incorrect): Consider a Hamilton path $P$ in $G$ . $P$ has to visit each of the components of $G-S$ . There is no direct path between two components so the path $P$ has to go to $S$ when it leaves a component. So the number of components in $G-S$ is at most $|S|$ . This is obviously incorrect. What am I missing here? How would it be $|S|+1$ and not $|S|$ . Also how can I show that the arrival vertices are distinct? Surely if you prove it this way, you could have just, for example, one vertex in $S$ which has an edge connecting to all of the components.","['connectedness', 'graph-theory', 'hamiltonian-path', 'combinatorics', 'discrete-mathematics']"
3100371,Prove that $\mathfrak{U} \setminus (\mathfrak{U} \times \mathfrak{U})$ is not a set,"Let $\mathfrak{U}$ denote the universe we are working in and $\mathfrak{U} \times \mathfrak{U}$ is the class of all ordered pairs $z = (x, y)$ where $x, y \in \mathfrak{U}$ . I know that neither $\mathfrak{U}$ nor $\mathfrak{U} \times \mathfrak{U}$ are sets. However, I am not sure how to go about showing that $\mathfrak{U} \setminus (\mathfrak{U} \times \mathfrak{U})$ is not either. The definition of a set I am using is the following: A class $X$ is called a set if there exists a class $Y$ such that $X \in Y$ , i.e. $X$ is a set $\iff (\exists Y) X \in Y$ . So, for this problem, I need to show that there does not / cannot exist a class $Y$ with $\mathfrak{U} \setminus (\mathfrak{U} \times \mathfrak{U})$ as a member. I figured that I should try and assume that such a class $Y$ exists and then (hopefully) reach a contradiction, but I am not sure how to proceed. Edit: Part of my issue is that I am having difficulty conceptualizing the class $\mathfrak{U} \setminus (\mathfrak{U} \times \mathfrak{U})$ . I understand that it means ""the universe without ordered pairs""... so does that mean it is a subclass of $\mathfrak{U}$ ?",['elementary-set-theory']
3100406,Commensurability of Subgroups,"Let $G$ be some group, and let $S_1$ and $S_2$ be some subgroups. Then $S_1$ and $S_2$ are said to be commensurable iff $|S_1 : S_1 \cap S_2|$ and $|S_2 : S_1 \cap S_2|$ are finite. I am trying to show that this is an equivalence relation on the collection of all subgroups of $G$ . Reflexivity and symmetry are trivial to verify; however, transitivity has been giving me trouble for a few days now. I found this , but I don't understand the following excerpt from the linked page: $$[S_1:S_1∩ S_3] \le [S_1:S_1∩S_2∩S_3]$$ $$ = [S_1:S_1 ∩ S_2][S_1 ∩ S_2:S_1 ∩ S_2∩S_3]$$ $$\le [S_1:S_1∩S_2][S_2:S_2∩S_3]$$ I cannot figure out how to get the third line from the second line. Why is $$[S_1 ∩ S_2:S_1 ∩ S_2∩S_3] \le [S_2:S_2∩S_3]?$$","['equivalence-relations', 'group-theory']"
3100440,Combinations & Probability - Sport Club Teams Probability - Is my solution correct?,"I'm revising permutations, combinations and probability for an upcoming exam and would really appreciate if someone could take a look at my procedure to solve this problem and let me know if it's correct. I have no answers to refer to. Hence, I have no way of knowing if I'm solving this correctly. Thank you in advance for your time! It is much much appreciated! A basketball club consist of 5 girls and 11 boys. For the next game 5 persons are randomly chosen to take the following positions: point guard, shooting guard, small forward, power forward and center. Specify a probability space for the experiment and calculate the probabilities of the following events: A: All players are girls B: All players are boys C: Small forward is the only girl D: Small forward and point guard are girls So we start with specifying the sample space. $B=[1,2,3...16]$ $1,2,3,4,5$ - girls; $6,7,8,9,10,11,12,13,14,15,16$ - boys $\Omega=[c1...c5] ⊂B$ , where ci≠cj for i≠j; $n=16$ $k=5$ A: All players are girls $\binom{5}{5}:\binom{16}{5}=1:4368=0.00022$ B: All players are boys $\binom{11}{5}:\binom{16}{5}=462:4368=0.1057$ C: Small forward is the only girl $\binom{5}{1}*\binom{11}{4}:\binom{16}{5}=(5*330):4368=0.377$ D: Small forward and point guard are girls $\binom{5}{2}*\binom{14}{3}:\binom{16}{5}=3640:4368 = 0.83$","['permutations', 'statistics', 'combinations', 'combinatorics', 'probability']"
3100468,square root n limit,$$(x_{n})_{n\geq 2}\ \ x_{n}=\sqrt[n]{1+\sum_{k=2}^{n}(k-1)(k-1)!} $$ $$\lim_{n\rightarrow \infty }\frac{x_{n}}{n}=?$$ I'm trying to solve the sum I but I don't know how to rewrite it.Can you give me some hints? edit: The sum $\sum_{k=2}^{n}(k-1)(k-1)!$ should be $(n! - 1)$ so $x_{n}=\sqrt[n]{n!}$ .The limit would be $\lim_{n\rightarrow \infty } \sqrt[n]{\frac{n!}{n^{n}}}$ .If I use cauchy d'alembert criterion and make some simplifications I would get $e^{-1}$ (the right answer)My problem is that sum.How I obtain $n! - 1$ ?,"['limits', 'calculus']"
3100501,A sum of Fibonacci numbers,Let $F_n$ be the Fibonacci numbers. I would like to prove this really messy identity: $$\sum_{n=0}^{\infty}(-1)^n(n+1)^2\frac{5n^2F_{k}+(F_{k+3}+3F_{k+2})n+2F_{k+2}+F_{k-1}}{{2n+2\choose 1}{2n+2 \choose 2}{2n \choose n}}=F_{k}\tag1$$ which simplifies to: $$\sum_{n=0}^{\infty}(-1)^n\frac{5n^2F_{k}+(F_{k+3}+3F_{k+2})n+2F_{k+2}+F_{k-1}}{{2n \choose n}(4n+2)}=F_{k}$$ How can I evaluate the following sums to show this identity (1) is correct? $$\sum_{n=0}^{\infty}(-1)^n\frac{n^2}{{2n \choose n}(2n+1)}=A$$ $$\sum_{n=0}^{\infty}(-1)^n\frac{n}{{2n \choose n}(2n+1)}=B$$ $$\sum_{n=0}^{\infty}(-1)^n\frac{1}{{2n \choose n}(2n+1)}=C$$,"['fibonacci-numbers', 'sequences-and-series']"
3100507,How can I solve this problem : $2^{x} \equiv{2070442609 \cdots 226509} \pmod {6561}$,"I want to solve this discrete logarithm problem with Pohlig–Hellman algorithm: $$2^{x} \equiv{ 
2070442609353644988500364779751625112994538364565830646055667805\\
1945605813418120257083690993568845753318608515495923060805120997\\
3428789429908548559535354422962118802026940584074383162419987316\\
4251257235243687584403222687359220263252625476372842589113471426\\
1782004893903634453733275871450024554309850603821543260259554681\\
9788249500416881166827892874757890895573842787278113899169213463\\
6207754656894365789382736647587424234413487070250150001802765877\\
5362018623752370739226509} \pmod {6561}$$ In fact, I don't know how to do it, because the numbers look terrible. For to solve this discrete logarithm I've read a few examples in PDF books. But, I couldn't apply this problem. It doesn't seem like the problem I can solve on one's own. I hope you can help. Thank you very much!","['discrete-logarithms', 'modular-arithmetic', 'discrete-mathematics', 'algorithms']"
3100513,Zeroes of $\sin(z)+2\sin(8z)$,"Clearly the function $f(x)=\sin z+2\sin8z$ has many zeroes on the real line. Does it have any off the real line? I thought of inspecting its real and imaginary parts separately: $$f(x+iy) = (\sin x\cosh y+2\sin8x\cosh8y)+i(\cos x\sinh y+2\cos8x\sinh8y)$$ However, I didn't find this to be very helpful. In the case of just a single sine I'd have $$\sin(x+iy)=(\sin x\cosh y)+i(\cos x\sinh y)$$ and I could show that all zeroes are on the real line by observing that cosh is always positive (as a real function) so I must have $x=\pi k$ , so for the imaginary part $0=\cos \pi k\sinh y=\sinh y$ implies $y=0$ . However, it's not so simple for the case of $f(z)$ above. Can anyone help me solve this?","['complex-analysis', 'roots']"
3100543,How many times would you have to roll a single die on average to reach a sum of at least 30?,"I am primarily asking for what this kind of problem would be called. Say I have one six-sided die. How many times on average would I have to roll it in order to have a sum of at least 30? Doing some googling, the closest I have found was a geometric distribution. But I am unsure if that is strictly for pass/fail scenarios (such as flipping a coin or drawing a name out of a hat). The closest I can get is that it will take no more than 30 tries (at a $1/6^30$ chance) and no less than 5 tries (at a $1/6^5$ chance). My problem arises when I think about the number of possible ways to get 6 tries. It would be easy if each die had 2 outcomes, but each one has 6, and many combinations can accomplish this. I don't even know where to begin figuring out how many combinations exist outside of counting (which would take hours). EDIT: So I found this Help with the Probabilty of Rolling Two Ten-Sided Dice Multiple Times Until 100 is Reached This is very much what this problem is, but it doesn't mention what the process is called. Which in the end is what my main question is.","['dice', 'probability']"
3100561,Poisson as a limit of the Binomial Characteristic Function,"We are given $X_n\sim B(n,p_n)$ where $np_n\rightarrow\lambda$ , and $\lambda>0$ . The goal is to prove $X_n$ converges in distribution to Poisson( $\lambda$ ) by use of characteristic functions. My issue is proving this with $p_n$ being arbitrary as it is.  Here are my current steps: $\phi_{X_n}(t)=(1-p_n+p_ne^{it})^n=(1+p_ne^{it}-p_n)^n=(1+\frac{np_n(e^{it}-1)}{n})^n=e^{n\ln(1+\frac{np_n(e^{it}-1)}{n})}=e^\frac{\ln(1+\frac{np_n(e^{it}-1)}{n})}{1/n}$ Then evaluating the limit means examining the exponent: $\displaystyle\lim_{n\rightarrow\infty}\frac{\ln(1+\frac{np_n(e^{it}-1)}{n})}{1/n}=\frac{\ln(1+\frac{\lambda(e^{it}-1)}{\displaystyle\lim_{n\rightarrow\infty} n})}{\displaystyle\lim_{n\rightarrow\infty}1/n}=\frac{\ln(1)}{0}=\frac{0}{0}$ . At this point I look to applying L'Hoptials rule, but I need to tweak something because when doing a u-substitution of $u = 1+\frac{np_n(e^{it}-1)}{n}$ I'm not sure how to evaluate the derivative to get the result. I would imagine it is just zero, treating $p_n$ as a constant, but then this falls apart.  As for treating the derivative with $p_n$ , I don't see the trick.  My first thought was the difference quotient manually, but that did not lead anywhere obvious. The result should be $\displaystyle\lim_{n\rightarrow\infty}\phi_{X_n}(t)=e^{\lambda(e^{it}-1)}$ , which is the $Poisson(\lambda)$ characteristic function. Any feedback is much appreciated.","['characteristic-functions', 'probability-distributions', 'fourier-transform', 'real-analysis', 'probability']"
3100567,"If an integer $n$ has the form $3k+1$, then $n$ does not have the form $9l+5$","I am trying to do the following proof by contradiction and need verification if the proof is correct: If an integer $n$ has the form $3k+1$ , then $n$ does not have the form $9l+5$ This is my proof: Let $n$ be an integer of the form $3k+1$ where $k$ is some integer. We want to prove that $n$ does not have the form $9l+5$ where $l$ is some integer. Suppose, on the contrary, $n$ does have the form $9l+5$ It follows that : $3k +1 = 9l +5$ $3k -9l = 5-1$ $3(k-3l) = 4$ $k - 3l = \frac{4}{3}$ Since $k -3l$ is an integer by the closure properties of integers $k-3l$ can't equal $\frac{4}{3}$ , which is a rational number. Therefore the assumption that $n$ does have the form $9l+5$ is false and the original statement holds. I am new to proofs. Is this proof correct? Is it stylistically correct as well?","['proof-writing', 'proof-verification', 'discrete-mathematics']"
3100656,The Center Manifold of an ODE,"Let $f:\mathbb{R}^n\to\mathbb{R}^n$ be a vector field given by $f_i(x)=\sum_{k=1}^n \sin(x_i-x_k), \forall x\in \mathbb{R}^n $ . Now consider the ODE $\dot x=f(x)$ . We observe that the Jacobian of $f(x)$ is singular (each row sums to zero). The intuition behind this singularity is that the vector field is invariant under shift, i.e., if $f(x)=f(x+a\mathbf{1}^n)$ , where $\mathbf{1}^n$ is a vector of all ones and $a$ is any constant. What can we say about the center manifold of this ODE? Is it true that the center manifold in this case is unique and is equal to the one-dimensional subspace (i.e., the null space of the Jacobian)? Another question is that how can I handle this kind of singularity? I am asking because I cannot apply many theorems (like the inverse function theorem) because of this singularity. Thank you, in advance, for your comments.","['manifolds', 'control-theory', 'ordinary-differential-equations', 'differential-geometry']"
3100700,On the solvability of the Dirichlet Problem $\Delta u =f$ for $f$ locally Holder continuous and $L^p$ for $p>n/2$,"It's well known that if $\Omega$ is a bounded set and $f$ is locally Holder continuous on $\Omega$ and bounded, then $u = \int_{\Omega} \Gamma(x-y)f(y) \ dy$ is a classical solution to $\Delta u=f$ , where $\Gamma$ here is the fundamental solution of the Laplacian. I have read the proof of this in Gilbarg and Trudinger. There is a problem in this book to prove that we can replace the assumption $f \in L^\infty(\Omega)$ with the assumption $f \in L^p(\Omega)$ for $p>n/2$ . In the proof of the bounded case, the assumption of boundedness only comes in (as far as I can tell) in showing that $$\int_{B_\epsilon(x)} |\nabla \Gamma(x-y)| |f(y)| \to 0$$ as $\epsilon \to 0$ . If we assume $f \in L^p(\Omega)$ with $p>n$ , this can be shown with Holder's inequality. But just $p>n/2$ is giving me trouble. I'm wondering if this is a mistake, because another book I've glanced through (Analysis, by Lieb and Loss) seems to state that for $p>n$ , $u$ is indeed a classical $C^2$ solution, and all they say for $n/2 \le p <n$ is that $u$ is Holder continuous. So, is the result true? Or is it just a mistake by Gilbarg and Trudinger.","['elliptic-equations', 'analysis', 'real-analysis', 'laplacian', 'partial-differential-equations']"
3100720,Understanding what ij mean in a Double Riemann Sum (Double Integral),"I am having trouble understanding what the ( $x^*_{ij} $ , $y^*_{ij}$ ) in this diagram (circled in blue) is explaining. What I do know is that $i$ is the iteration of the $x$ Riemann Sum and the $j$ is the iteration for the $y$ Riemann Sum. What I am having an issue with is understanding why $x$ needs the $j$ and $y$ needs the $i$ . Couldn't the $P$ simply be represented as ( $x^*_i$ , $y^*_j$ )?","['integration', 'riemann-sum', 'multivariable-calculus', 'definite-integrals']"
3100784,A difficulty in understanding the proof of Riemann Lebesgue lemma.,The proof is given below: My questions are: 1- In the second line from below how do we get $2/|\lambda|$ (in the second term) from the line before  it. 2- What is the lemma trying to say?,"['fourier-analysis', 'analysis', 'real-analysis', 'calculus', 'fourier-series']"
3100804,Does equal sizes of two sets imply they have the same cardinality in general?,"This may seem like a silly question (and it certainly seems that way asking it) but I want to make sure my thought process is correct. If two sets have the same cardinality then we can define a function $f: A \to B$ that is one-to-one and onto. Cardinality refers to the size of the sets. Suppose instead I have two sets I know are the same size. Fixing an example: $A: \{1, 2, 3\}$ $B: \{4, 5, 6\}$ Each has 3 elements. Since they have the same size, does this imply I can find a one-to-one and onto function $f: A \to B$ ? I would guess so. If I had to pin point my confusion - I think it comes from defining the function. In this case, I think I can define a a function $f = a + 3$ , that maps exactly one element of $a \in A$ to every $b \in B$ , and every $b \in B$ has some $a \in A$ such that $f(a) = b$ . Is this true in general?","['elementary-set-theory', 'real-analysis']"
3100811,Max product of $20$ numbers with mean $12$ if $n$-th number is $\frac{n}2$ or $2n$?,"A list contains $20$ numbers. For each positive integer $n$ , from $1$ to $20$ , the $n$ th number in the list is either $\frac{1}{2}n$ or $2n$ . If the mean of the number in the list is exactly $12$ and the product of the numbers is $P$ , what is the greatest possible value of $\frac{P}{20!}$ ? I have tried some examples, but they bring me nowhere. I then tried creating equations: I can call the sum of the numbers that you are halving by $a$ and the sum of the numbers that you are doubling by $b$ . However, after trying repeatedly, I don't see any way to create an equation. Can I have a hint? Also, if you are nice, may you please help me on this question( $N$'s base-5 and base-6 representations, treated as base-10, yield sum $S$. For which $N$ are $S$'s rightmost two digits the same as $2N$'s? )? Thanks! Max0815","['contest-math', 'inequality', 'combinatorics']"
3100828,Calculate the circle that touches three other circles,"Given three circles on a cartesian grid (with centres and radii known), how would you calculate the centre of the circle that touches those three? The three known circles may have any radius length, and may touch or cross each other (but not have overlapping centres), but the calculated circle must lie externally to the three known circles. See this online graphing tool for an example of how this looks: https://www.desmos.com/calculator/lf1q90ymrh Note: Imagine we have the first 3 circles as given (two red ones, plus a black one). The question is: how can we mathematically deduce the formula of the fourth circle - the purple one - that just touches the first three? In this example I added the purple circle by trial-and-error, and it is only approximate. There is one answer against this question already. It might be correct but I don't understand how to start with 3 concrete circles - like in the link above - and then work out the fourth. I think I need someone to take that linked example, use the numbers there, and explain how to perform the maths to calculate the fourth.","['triangulation', 'geometry', 'intuition']"
3100829,Too much long line,"There is a so-called long line in topology, which is a topological space with a base set $[0,1]\times \mathbb{R}$ with order topology given by lexicographic order: $(x_{1}, y_{1}) < (x_{2}, y_{2})$ if and only if $x_{1} < x_{2}$ or $x_{1} = x_{2}$ and $y_{1} < y_{2}$ . Here are some properties of long line. (Actually, definition of long line in the link is more general than the definition above completely different , and we will use the above definition.) Now let $L_{1}$ be the above long line. Define long long line $L_{2}$ as a topological space with a base set $[0, 1]\times L_{1}$ with order topology is given by lexicographic order as before. We can embed $L_{1}$ in $L_{2}$ as $L_{1} \simeq \{1/2\}\times L_{1}\hookrightarrow L_{2}$ . My question is the following: (1) Are $L_{1}$ and $L_{2}$ are homeomorphic? (2) If not, we can also define a $long^{3}$ line $L_{3}$ in similar way, and even $L_{n}$ for any $n\geq 1$ . What is a direct limit $$
L_{\omega} = \lim_{\to}L_{n}?
$$ Is there any interesting property of $L_{\omega}$ ? (3) If we proceed more, then we can also define $$L_{\omega + 1}, L_{\omega +2}, \dots, L_{2\omega}, \dots, L_{\omega^{2}}, \dots, L_{\omega^{\omega}}, \dots, L_{\epsilon_{0}}, \dots$$ for any given ordinals $\alpha$ . Are they all different? I think the most important question is (1). Thanks in advance. Since the definition of my long line is different from the original one, here's the new version of questions for original definition. First, we have the classical (original?) long ray $R_{1}$ , which is a set $\omega_{1} \times [0, 1)$ with an order topology via lexicographic order. 
Now we can define a long line $L_{1}$ by gluing two long rays $R_{1}$ with respect to their endpoints. 
To define $L_{2}$ , we define $R_{2}$ by $\omega_{1} \times R_{1}$ with an order topology (lexicographic order again) and glue two copies. By continuing this process, we can define $L_{\alpha}$ for any given ordinal $\alpha$ (I hope).",['general-topology']
3100831,Domain of $g(x)=\frac{1}{1-\tan x}$,"What is the domain of $g(x)=\frac{1}{1-\tan x}$ I tried it and got this. But I'm not really sure if it is right. Is that gonna be like this ? $(\mathbb{R}, \frac{\pi}{4})$","['trigonometry', 'functions']"
3100871,"Conjecture: in an ellipse with major axis AB, the projection I of one of the foci on any tangent is such that AIB is right angle","I think I have found out some property of an ellipse. Define the following: The point $P$ belongs to the ellipse. The tangent line $\ell$ goes through $P$ . The point $F$ is one of the foci . The line $m$ , through $F$ , is perpendicular to the line $\ell$ at point $I$ . $A$ and $B$ are the vertices of the ellipse (that is, the endpoints of the major axis). In this case, The $\angle AIB$ is always $\pi/2$ . (I think.....) But I didn't prove yet... Is it true? Could anybody give me some advice? Thanks in advance. Edit1 For jmerry Edit2 For jmerry Really thanks, makes me fun!",['geometry']
3100913,The fields involved in algebraic geometry / polynomial rings,"I am a beginner in algebraic geometry  and I have a doubt over the rings/fields involved in algebraic varieties. Consider this definition: An aﬃne algebraic variety over the ﬁeld $\mathbb{F}^n$ is the zero set $V (p_1,...,p_k) ⊂ \mathbb{F}^n$ of a ﬁnite number of polynomials $p_1, ..., p_k \in \mathbb{F}[x_1,...,x_n]$ I realise there are types of varieties other than affine ,but even looking ahead in Wikipedia here my question is this: In algebraic geometry, are the polynomial variables (hence the zeros) always required to live in (a cartesian product of) the field of the coefficients ? For example, do we also study cases where, say, the coefficient ring is $\mathbb{Z_4}$ and the variables (hence the zeros) live in the field $\mathbb{C}$ or vice-versa ?","['ring-theory', 'algebraic-geometry']"
3101047,Surface area of a sphere sliced by two orthogonal planes,"I'm a programmer doing a graphics effect, and I've arrived at a math problem that my old dumb brain can't quite figure out the best way to solve. Here's the problem: take a sphere and slice off a bit of it with a plane. You get a sphere cap and sliced sphere. Now, take another plane that is exactly orthogonal to the slicing plane and slice off another sphere cap. What is the surface area of the sliced sphere left over? If the two sphere caps don't intersect, then the problem is simple: just subtract the surface area of the two sphere caps from the surface area of the sphere, and you're done. However, if the two planes are close enough to the center that the two sphere caps intersect, then there's a sliver of the sphere that gets sliced off twice, and you have to add it back. How do you do that? In case anyone is curious about the actual thing I'm trying to do: I want to bake ambient occlusion into a texture that I'm going to use on a voxel mesh. The idea with AO is that you take a point on a surface, and a hemisphere centered around the surface normal, and you check how much of that surface area is occluded by objects. In a voxel world, that essentially reduces to the problem above, since the only thing that could occlude is neighboring voxels. I've googled this and the stuff I find are some variation of this method which uses vertex coloring (essentially) and looks quite bad. I want to do it properly.",['geometry']
3101063,"Proof by induction: $a_n > n + \frac{1}{3} \space \forall n \in N,\space n\geq 4$","How do I prove this by induction? Let $(a_n)_{n\in N}$ be the sequence defined by: $$a_1=1,\space a_2=\frac{3}{2},\space a_{n+2}=a_{n+1}+\frac{2n+1}{n+2}a_n \space \space (n\in N)$$ Prove that $a_n > n + \frac{1}{3} \space \forall n \in N,\space n\geq 4$ Here's what I did, is it ok? 1) I define $$P(n): a_n > n + \frac{1}{3} \space \forall n \in N,\space n\geq4\\$$ 2) Since the base case is true, $$a_1=1, \ a_2=\frac{3}{2},\ a_3=\frac{5}{2},\ a_4=5 $$ $$P(4):a_4 = 5 > 4+\frac{1}{3}$$ 3) I assume $P(n)$ (already defined) and $$P(n+1):a_{n+1}>n+1+\frac{1}{3} = n + \frac{4}{3}$$ are true, and want to prove that it implies $$P(n+2):a_{n+2}>n+2+\frac{1}{3}=n+\frac{7}{3}$$ is true. 4)So since: by the second induction hypothesis, $$a_{n+2}=a_{n+1}+\frac{2n+1}{n+2}a_n > n+\frac{4}{3}+\frac{2n+1}{n+2}a_n\ $$ and by the first induction hypothesis, $$n+\frac{4}{3}+\frac{2n+1}{n+2}a_n > n+\frac{4}{3}+\frac{2n+1}{n+2}(n + \frac{1}{3}) = n+\frac{4}{3} + \dfrac{2n^2+n+\dfrac{2n}{3}+\dfrac{1}{3}}{n+2} $$ 5)Now as I know that every term is positive I can remove some terms and assure that: $$n+\frac{4}{3} + \dfrac{2n^2+n+\dfrac{2n}{3}+\dfrac{1}{3}}{n+2} > n+\frac{4}{3}+\frac{2n+1}{n+2}(n + \frac{1}{3}) = n+\frac{4}{3} + \dfrac{2n^2}{n+2}$$ So the only step remaining would be making sure that: $$\dfrac{4}{3}+\dfrac{2n^2}{n+2} > \dfrac{7}{3}\ $$ then $$ \dfrac{2n^2}{n+2} > 1 which \ is \ true \ \forall n \in N, n \geq 4$$ Therefore $P(n+2)$ is true, $P(n+1)$ is true and also $P(n)$ is true.","['proof-verification', 'recurrence-relations', 'discrete-mathematics']"
3101080,How can we find this limit $\lim_{n\to\infty \\x\to\infty}f^n(x)$?,"Let $f:\mathbb{R}\rightarrow \mathbb{R}$ , $f(x)=\frac{ax+b}{cx+d}$ and $a,b,c,d>0$ then $f^1(x)=f(x), f^2(x)=f(f(x)), f^3(x)=f(f(f(x)))$ and $f^n(x)={f(f(f\cdots f(x)\cdots )))}$ , where $ f^n(x)$ is the $ n $ composition of the function $f(x).$ It's required finding this limit: $$\lim_{n\to\infty \\x\to\infty}f^n(x)$$ It is possible to find $f^n(x)$ for finite number $n$ . For example, I found $f^2(x) = \frac{a^2 x + a b + b c x + b d}{a c x + b c + c d x + d^2}$ . I can't do more than that. What is the math level of this problem? In fact, I wanted to solve. But I could not. Thank you very much!","['contest-math', 'calculus', 'functions', 'limits', 'algebra-precalculus']"
3101158,How to show $\sum_{i=1}^{\lceil L/2\rceil}\frac{\binom{2i-2}{i-1}}{i}2^{-2i}=1/2-2^{-L-1}\binom{L}{(L-1)/2}$?,"By numerical evaluation, it seems that the following identity holds. For any odd positive integer $L$ , $$\sum_{i=1}^{\lceil L/2\rceil}\frac{\binom{2i-2}{i-1}}{i}2^{-2i}=1/2-2^{-L-1}\binom{L}{(L-1)/2}.$$ I was trying to expand everything out but didn't have a good luck. I guess there should be a more clever way using some binomial identity.","['binomial-coefficients', 'combinatorics']"
3101195,A Functional equation in one variable: $ 2f(x) + 3f\left(\frac{2x+29}{x-2}\right) = 100x+80$,Let $ f: \mathbb R\setminus\{2\} \rightarrow \mathbb R$ be a function satisfying the following functional equation: $$ 2f(x) + 3f\left(\frac{2x+29}{x-2}\right)  = 100x+80$$ Find $f(x)$ . I tried observing the behaviour of the function at $ x\rightarrow 2$ and $ x\rightarrow \infty $ but still could not get a good conclusion. How do I approach the problem?,"['functional-equations', 'algebra-precalculus']"
3101264,Distribution at First Time a Sum Reaches a Threshold,"Consider the following problem.
Roll a die many times, and stop when the total exceeds $M$ , for some prescribed threshold $M$ .
Call this time $\tau$ , and call the running score after $n$ rolls $X_n$ . What is the distribution of $X_\tau$ ? Of course $X_\tau \in [M,M+5]$ . However, I can get little beyond this.
Moreover, for small $M$ the distribution should be very sensitive, and I doubt have a nice form. However, if I define $X'_\tau = X_\tau - M$ , then it seems to me that $X'_\tau$ should have a limiting distribution at $M \to \infty$ . Extra comments. $\quad$ Note that there are various related questions here on maths.SE, but these (as far as I have seen) are about determining $\tau$ , in particular its expectation, $E(\tau)$ . Also, one can solve for $E(\tau)$ directly. If I write $k_M$ for $E(\tau)$ with threshold $M$ , then $$ \textstyle k_M = \tfrac16 \sum_{j=1}^6 k_{M-j} + 1, $$ and writing $\ell_M = k_M - k_{M-1}$ we get $$ \textstyle \ell_M = \tfrac16 \sum_{j=1}^6 \ell_{M-j}. $$ In principle, by trying the solution $\ell_r = r^\lambda$ and solving $$ 6 \lambda^6 - \lambda^5 - \lambda^4 - \lambda^3 - \lambda^2 - \lambda - 1 = 0, $$ (see this WolframAlpha computation ), and calculating some initial conditions by hand, then one can find $k_M = E(\tau)$ .
Note that this would involve finding six initial conditions and solving a set of six simultaneous equations. This is only for the $\ell$ -s; one then needs to convert this into the $k$ -s.
This does not sound like fun to me! =P By a simple martingale argument--- $(X_n - \tfrac72n)_{n\ge0}$ is a martingale, and $\tau$ is a deterministically bounded stopping time---from this one immediately gets $E(X_\tau) = \tfrac27 E(\tau)$ (for any $M$ ).","['probability-distributions', 'martingales', 'recreational-mathematics', 'probability-theory', 'probability']"
3101351,"Show that quadratics $f, g\in\mathbb {C}[x, y]$ have at most four common zeroes, unless they have a non-constant factor in common.","Here is a problem from Algebra , Artin: Suppose $f, g\in\mathbb {C}[x, y]$ are quadratic. Prove that $f$ and $g$ have at most four common zeroes, unless they have a non-constant factor in common. The book gives the general fact that the number of comnon zeros of such $\mathit f, \mathit g$ (no need to be quadratic) is bounded by the Bezout bound and also gives the proof of the finiteness of the number of common zeroes. So with this fact the above problem is obviously solved. I am trying solving the problem without Bezout's Theorem . Are there some special properties of quadratic polynomials help to solve the problem?","['algebraic-geometry', 'abstract-algebra', 'polynomials']"
3101364,Understanding sheaves on a $2$-element set,"I'm working through the Geometry of Schemes and wanted some clarification for an exercise. Exercise I-5 considers a two-element set $X=\{0,1\}$ with the discrete topology and asks the reader to find the relations between the objects of a sheaf (of abelian groups) on $X$ . If we let $\mathcal{F}$ be the sheaf, then it's clear that $\mathcal{F}(\emptyset)$ is the trivial group and that we have a commutative diagram of restrictions from $\mathcal{F}(\{0\})$ to $\mathcal{F}(\emptyset)$ , from $\mathcal{F}(\{1\})$ to $\mathcal{F}(\emptyset)$ , from $\mathcal{F}(\{0,1\})$ to $\mathcal{F}(\{0\})$ , and from $\mathcal{F}(\{0,1\})$ to $\mathcal{F}(\emptyset)$ . By the sheaf axiom, it seems like for any $s\in\mathcal{F}(\{0\})$ and $t\in\mathcal{F}({\{1\}})$ , $s$ and $t$ restricted to the intersection, $\emptyset$ , must be the same, so there is some unique section in $\mathcal{F}(\{0,1\})$ that restricts to $s$ and $t$ . What does this say about $\mathcal{F}(\{0,1\})$ ? I'm guessing it's related to the fiber product, but I'm not particularly well-versed in category theory. Also, how does this generalize to sheaves over different categories?","['algebraic-geometry', 'category-theory', 'sheaf-theory']"
3101423,$\epsilon$ fattening of an open set needn't be open,"This was a question on a test. Let $(X,d)$ be a metric space and $\epsilon \geq 0$ . Define the $\epsilon$ -fattening of a set $S$ as $$S_\epsilon:=\{ x \in X; \exists s \in S: d(s,x) \leq \epsilon \}$$ As the title says, but I can't find a counterexample. It seems to me that if $S$ doesn't contain its boundary then the ""fattened"" set shouldn't either by the definition. Any help would be appreciated.","['general-topology', 'metric-spaces', 'hausdorff-distance']"
3101429,Exercise in algebra involving binary operation,"I started doing algebra exercises before the course starts, and I'm befuddled with following problem, where it is asked to show if the operation is associative and commutative and whether it has an identity element and inverse elements. Let set $X\neq\emptyset$ and $\mathcal{P}(X)$ denote all its subsets. Let $*$ be binary operation on $\mathcal{P}(X)$ which is defined with following equation: $A*B = A\cup B$ . It is commutative: $A\cup B = B \cup A$ And also associative for $C \in \mathcal{P}(X)$ : $A\cup (B\cup C ) = (A\cup B)\cup C$ Identity element $e = \emptyset$ , for every set holds $\emptyset$ . So, $A \cup e = \emptyset^{^*}$ Now I'm not sure about inverse element such that $A \cup B = e$ Is it possible in advanced algebra ? I thought of something like $B:= e \setminus A$ *edit: Identity element equation should be $A \cup e = A$","['elementary-set-theory', 'abstract-algebra']"
3101495,Conditional Probabilities - Are the Events Independent - Confused About part of this problem,"No answers to refer to, so would be really greatful if someone can take a look at this problem and explain me a bit where am I going wrong or if I've done something right to begin with, haha. A fair coin is tossed tree times. Specific the probability space for the experiment and consider the following events:
A: The result of the third toss is heads
B: the coin shows heads exactly two times Compute the conditional probabilities P(A given B) and P(B given A). Are A and B independent? So our sample space is: ${HHH}, {TTT}, {HHT}, {HTT}, {THH}, {TTH}, {HTH}, {THT}$ Hence, $P(A): 0.5$ $P(B): 0.375$ We can see that event A appears exactly 2 times when event B happens, hence, A is independent of B as the conditional probability of A given B is still equal to the probability of event A= 0.5 However, I'm a bit confused for the second part in finding the conditional probability of B given A. B is appearing also twice while A is taking place, but that is different than the probability of event B alone, so is it true that B is dependent on A? 0.66 versus the P(B) = 0.375. If so, how do we compute the conditional probability? Any help would be much much aprpeciated!","['conditional-probability', 'statistics', 'probability']"
3101504,Relative Spec (the structure map),"Given a scheme $S$ and a quasi coherent sheaf $\mathcal{F}$ of $\mathcal{O}_S$ algebras, we want to define a scheme $X = \mathrm{Spec}(\mathcal{F})$ over $S$ . To do so, we define it in three stages: as a set, as a topological space, and then give a structure sheaf. As a set, we define $\mathrm{Spec}(\mathcal{F})$ to be the set of prime ideal sheaves in $\mathcal{F}$ , that is, quasi coherent sheaves $\mathcal{P}$ of ideals such that for every $U \subset S$ open, the ideal $\mathcal{P}(U)$ is either prime or the unit ideal. As a topological set, given $U \subset S$ and $\sigma \in \mathcal{F}(U)$ , the sets $$V_{U,\sigma} = \{ \text{prime ideal sheaves $\mathcal{P} \subset \mathcal{F}$ such that $\sigma \notin \mathcal{P}(U)$}\}$$ for a topology. Finally we set $\mathcal{O}_X(V_{U,\sigma}) = \mathcal{F}(U)[\sigma^{-1}]$ for the structure sheaf. The problem arises while defining the morphism $f \colon X \to S$ . In Eisenbud-Harris The Geometry of Schemes , it states that as a map of topological spaces, $f \colon X \to S$ is given by sending a prime ideal sheaf $\mathcal{P} \subset \mathcal{F}$ to its inverse image under the morphism of sheaves $\mathcal{O}_S \to \mathcal{F}$ , and the pullback map $$f^{\#} \colon \mathcal{O}_S(U) \to f_* \mathcal{O}_X$$ is given by the structure map $\mathcal{O}_S \to \mathcal{F}$ on $U$ . What is exactly meant by ""the inverse image in $\mathcal{O}_s \to \mathcal{F}$ ""? How does that give us a continuous map on the underlying topological spaces of $X$ and $S$ ?","['quasicoherent-sheaves', 'modules', 'algebraic-geometry', 'abstract-algebra', 'sheaf-theory']"
3101548,Why is it necessary to split the definite integral of a piecewise function into a sum,"The second fundamental theorem of calculus (Newton-Leibniz) tells us that: If $f$ is a real-valued function on a closed interval $[a, b]$ and $F$ is an antiderivative of $f$ in $[a,b]$ s.t. $F'(x)=f(x)$ , then $$\int_{a}^{b} f(t) dt = F(b)-F(a)$$ Say we have a real-valued piecewise continuous function $f(x)$ defined on $[a,b]$ which is made up of intermediate functions $f_1, f_2, ..., f_n$ for $x \in [x_1, x_2) \cup [x_2, x_3) \cup ...\cup[x_n, x_{n+1}]$ Using the second fundamental theorem of calculus, one may find an appropriate $F(x)$ s.t. $F'(x)=f(x)$ , like $$ F(x) = \begin{cases}
       \int f_1(x) dx + C_1 &\quad\text{if} \ x \in [x_1, x_2) \\
       \int f_2(x) dx + C_2 &\quad\text{if} \ x \in [x_2, x_3) \\
... \\
       \int f_n(x) dx + C_n &\quad\text{if} \ x \in [x_n, x_{n+1}] \\ 
     \end{cases} $$ My question is the following: why is it that when we evaluate $\int_a^b f(t) dt$ , we must split it into the sum of $$\int_a^{x_i} f(t) dt + \int_{x_i}^{x_{i+1}} f(t) dt + \ ...\ +\int_{x_{i+k}}^{b} f(t) dt $$ instead of simply evaluating $F(b) - F(a)$ by selecting the appropriate antiderivatives from the piecewise function? Example: $$ f(x) = \begin{cases}
       x+1 &\quad\text{if} \ x \in [0, 2) \\
       1 &\quad\text{if} \ x \geq 2 \\
     \end{cases} $$ An antiderivative of $f$ would be $$ F(x) = \begin{cases}
       \frac{x^2}{2}+x + 2 &\quad\text{if} \ x \in [0, 2) \\
       x+7 &\quad\text{if} \ x \geq 2\\ 
     \end{cases} $$ Note the constants of integration $C_1=2$ and $C_2=7$ Clearly, computing $F(3)-F(1)$ does not give the area under $f(x)$ from $x=1$ to $x=3$ . Even though intuitively it makes sense to break the definite integral into a sum, I am unable to come up with a straightforward reason as to why $F(3)-F(1)$ wouldn't give the correct result as long as I respected the conditions of the Newton-Leibniz axiom (maybe I didn't, but please let me know why ). EDIT: For a discontinuous function $f(x)$ on an interval $[a,b]$ with finitely many discontinuities, why do we need its antiderivative $F(x)$ to be continuous in order to apply $F(b)$ - $F(a)$ ? No matter if it's continuous or not, $F(x)$ still won't be differentiable at $f$ 's discontinuities, and its derivative would still be $f$ . However, if it's not continuous, $F(b)$ - $F(a)$ would no longer give the area under the curve. Why? EDIT 2: Lebesgue Integration asserts that that an antiderivative $F$ differentiable almost everywhere must be absolutely continuous for $F(b)$ - $F(a)$ (FTC 2) to work. So case closed.","['integration', 'calculus', 'derivatives', 'riemann-integration', 'piecewise-continuity']"
3101560,Generating the $n^{th}$ full binary tree over $N$ labeled leaves,"I am looking for an algorithm to incrementally generate distinct full binary trees over $N$ unique leaves. That is, I want an algorithm that can generate the $n^{th}$ distinct tree without generating all the $n-1$ trees before. Pre-generating all trees is practically impossible above a certain N . A full binary tree with its $N$ leaves labeled is equivalent to a binary grouping of $N$ unique elements grouped into pairs. There are $C_{N-1}$ different full binary trees or groupings of N leaves, where $C_n$ is the $n^{th}$ Catalan number. For $N = 4$ , there are $C_3 = 5$ trees. These are, with internal nodes labeled $5..7$ and with the equivalent grouping: For $N = 5$ : (1 (2 (3 (4 5))))
(1 (2 ((3 4) 5)))
(1 ((2 3) (4 5)))
(1 ((2 (3 4)) 5))
(1 (((2 3) 4) 5))
((1 2) (3 (4 5)))
((1 2) ((3 4) 5))
((1 (2 3)) (4 5))
((1 (2 (3 4))) 5)
((1 ((2 3) 4)) 5)
(((1 2) 3) (4 5))
(((1 2) (3 4)) 5)
(((1 (2 3)) 4) 5)
((((1 2) 3) 4) 5) I see three ways to solve this problem (ultimately they are equivalent): There is a simple algorithm that can directly generate the next distinct (non-isomorphic) tree incrementally. There is a bijective encoding from tree $T_i$ to sequence $S_i$ such that generating $S_{i+1}$ (decoding into tree $T_{i+1}$ ) is easily doable. In the ideal case, there is a simple bijection of the $C_{N-1}$ trees to a continuous interval of $C_{N-1}$ natural numbers (preferentially $(1..C_{N-1})$ ) so that generating the $i^{th}$ tree is as easy as to decode it from the integer $i$ . There are many algorithms to encode a binary tree to a bijectively unique sequence (e.g. to a Prüfer sequence) but the issue is then how to generate the next sequence that can be decoded to the next tree without many failed sequences that do not encode a valid tree of the above description and do not encode a tree that was already visited.","['graph-theory', 'trees', 'combinatorics', 'planar-graphs']"
3101578,Does the Time Evolution Operator Satisfy the Schrodinger Equation?,"I would just like to confirm my solution to the following question. I'm a bit hesitant on my solution because of a specific step. I would just like confirmation if that step, which I will point out, is mathematically legal. The question I'm working on is: Show that the time evolution operator, given by the Dyson series, \begin{equation}
    \mathcal{U}(t,0) 
        = 
            1 + \sum_{n=1}^\infty \bigg ( \dfrac{-i}{\hbar} \bigg )^n \int_0^t dt_1 \int_0^{t_1} dt_2 \dots \int_0^t dt_{n-1} H(t_1) H(t_2) \dots H(t_n)
\end{equation} satisfies Schrodinger's equation \begin{equation}
    i\hbar \dfrac{\partial}{\partial t} \mathcal{U}(t,0) = H\mathcal{U}(t,0).
    \label{SE}
\end{equation} For this problem, I will evaluate the left-hand side of the Schrodinger equation to show that it is equivalent to the right-hand side. Firstly, we have that the Dyson series can be rewritten as \begin{equation}
\mathcal{U}(t,0) 
= 
1 + \sum_{n=1}^\infty \bigg ( \dfrac{-i}{\hbar} \bigg )^n \int_0^t dt_1 \int_0^{t_1} dt_2 \dots \int_0^t dt_{n-1} H(t_1) H(t_2) \dots H(t_n)
=
T\big \{ \exp{\big ( \dfrac{-i}{\hbar} \int_0^t dt' H(t') \big )} \}
\nonumber
\end{equation} The following steps is where my question is. The part I'm referring too is when I take the time-derivative with respect to $t'$ .
Using this, we have that \begin{align}
i\hbar \dfrac{\partial}{\partial t} \mathcal{U}(t,0)
&=
i\hbar \, \partial_{t'} \bigg [ T\big \{ \exp{\big ( \dfrac{-i}{\hbar} \int_0^t dt' H(t') \big )} \} \bigg ]
\nonumber
\\[.5em]
&=
i\hbar \bigg [  \partial_{t'}  \big ( \dfrac{-i}{\hbar} \int_0^t dt' H(t') \big )  \bigg ] \bigg [ T\big \{ \exp{\big ( \dfrac{-i}{\hbar} \int_0^t dt' H(t') \big )} \} \bigg ]
\nonumber
\\[.5em]
&=
-i\hbar \dfrac{i}{\hbar} \partial_{t'}\int_0^t dt' H(t') \mathcal{U}(t,0) 
\nonumber
\\[.5em]
&=
H \mathcal{U}(t,0) 
\end{align} Therefore, $i\hbar \dfrac{\partial}{\partial t} \mathcal{U}(t,0) = H\mathcal{U}(t,0)$ and $\mathcal{U}(t,0)$ satisfies the Schrodinger equation. I'm essentially performing the following differential: \begin{equation}
\partial_x e^{f(x)} = f'(x) e^{f(x)}
\end{equation} Since the time-evolution operator is defined as the sum, I'm not sure how legal it is to take that derivative in the way that I did. If this is incorrect, I would appreciate an alternative method of working this problem. Any guidance would be appreciated, thank you!","['quantum-mechanics', 'physics', 'derivatives', 'mathematical-physics']"
3101581,"$L^{q,\infty}(\mu) \subset L^p(\mu)$ with $1 \leq p<q <\infty$ and $\mu$ finite measure?","Let $\mu$ be a finite measure on a measurable space $(X,\Sigma)$ . I want to prove that exists $C > 0$ so that $$ \| f\|_p \leq C \| f \|_{q,\infty}$$ when $1 \leq p < q < \infty$ , where $$ \| f \|_{q,\infty} = \sup\limits_{\lambda > 0}\lambda \mu(\{x \in X: |f(x)| > \lambda \} )^{1/q}. $$ I think the key of the proof relies in the fact that $\mu$ is a finite measure, because it's not difficult to show a counterexample when you consider the Lebesgue measure on $\mathbb{R}$ . My best aproach consists of putting $$ \int_X |f|^p d\mu = \int_0^\infty \mu\{ |f| > t\} p t^{p-1} dt = \int_0^\lambda\mu\{ |f| > t\} p t^{p-1} dt  + \int_\lambda^\infty \mu\{ |f| > t\} p t^{p-1} dt,  $$ where $\lambda > 0$ . For the first integral it's easy to see that it's $\leq \mu(X) \frac{\lambda^p}{p} = C_1 \lambda^p$ , but I'm stuck with the second integral and I don't know how to continue. Any idea of how to continue the proof? Do yo know any different approach to the solution? Thank you very much.","['measure-theory', 'lp-spaces', 'weak-lp-spaces', 'real-analysis']"
3101595,"$a_n=(-1)^{n-1}, \; s_n=\sum_{i=1}^{n}a_i$ then find $ \lim_{n\to \infty}\frac{s_1+s_2+\dots s_n}{n}$","$a_n=(-1)^{n-1}, \;  s_n=\sum_{i=1}^{n}a_i$ then find $ \lim_{n\to
> \infty}\frac{s_1+s_2+\dots s_n}{n}$ $$s_k=1,\; \text{if k is odd and } s_k=0  \text{ if k is even}  $$ Cauchy's theorem for a sequence $(x_n) $ in $R$ , we have $\lim\frac{x_1+x_2+\dots x_n}{n}=\lim x_n$ How do I make use of this theorem here when $(s_n)$ is oscillating between $0$ and $1$ ?","['limits', 'limits-without-lhopital', 'cesaro-summable', 'sequences-and-series']"
3101603,Prove that spaces $C^k(\mathbb{R}^n)$ and $C^\infty(\mathbb{R}^n)$ are infinite dimensional.,"Prove that spaces $C^k(\mathbb{R}^n)$ and $C^\infty(\mathbb{R}^n)$ are infinite dimensional. So in order to prove that they're infinite dimensional spaces, I need to form a linearly independent sequence which is infinite. Would I simply pick polynomials? (i.e. $1,x,x^2,\dots$ as my basis?)","['vector-spaces', 'functional-analysis', 'analysis']"
