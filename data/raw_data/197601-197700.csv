question_id,title,body,tags
3814485,Inexistence or limit that does not exist,"We suppose that have this limit: $$\lim _{x\to +\infty }\frac{(x-1)^{\sqrt x}}{x-2}$$ Are there theorems in Mathematical Analysis, corollaries that use successions, particular strategies, that help me to demonstrate that a limit exists or does not exist? Related question : Limits that do not exist: search of general techniques","['alternative-proof', 'limits', 'calculus', 'proof-explanation']"
3814502,Is it possible to prove that a group is the only group of its order given some properties?,"I came across the following question: Show that $(a, b:a^3 = 1, b^2= 1, ba=a^2b)$ gives a group of order $6$ . Show that it is non abelian. Is it the only non abelian group of order $6$ up to isomorphism? I managed to prove everything except the last statement. How could anyone prove the group is the only non-abelian group of a particular order, without knowing all the groups of that order from some worksheet? Does this generalize as a problem, or did our instructor just want us to learn the groups of order $6$ ? Also, what does ""up to isomorphism"" mean?","['group-presentation', 'group-theory', 'abstract-algebra']"
3814505,"In an inner product space that is not complete, is there a closed orthonormal system that is also complete?","Let $V$ be an inner product space and $(e_n)_{n=1}^{\infty}$ be an orthonormal system. We call it complete if $\left \langle v,e_n \right \rangle=0$ for all $n$ implies $v=0$ ; and closed if $v=\sum_{n=1}^{\infty}\left \langle v,e_n \right \rangle e_n$ for every $v\in V$ . My question is this: is there a closed orthonormal system that is also complete in an inner product space that is not complete (not a hilbert space)?","['hilbert-spaces', 'functional-analysis']"
3814506,Finding solution of $y' = \frac{(y^2 - 4yt + 6t^2)}{t^2}$ with initial condition $y(2) = 4$,"Concerning the ordinary differential equation $$y' = \frac{(y^2 - 4yt + 6t^2)}{t^2}$$ with initial condition $y(2) = 4$ . My text book gives the solution as $y = 2t$ , and so does MATLAB. However, I only see the following : \begin{align}
v &= \frac{y}{t}\\
y' &= v^2 - 4v + 6\\
tv' + v &= v^2 - 4v + 6\\
tv'&= v^2 - 5v + 6\\
\end{align} \begin{align}
\frac{dv}{(v-3)(v-2)} &= \frac{dt}{t}\\
\frac{dv}{(v-3)} - \frac{dv}{(v-2)} &= \frac{dt}{t}\\
\ln(|v-3|) - \ln(|v-2|) &= \ln(|t|) + c\\
\frac{v-3}{v-2} = te^{c}\\
\end{align} which then reduces to $$v = \frac{3 - 2te^{c}}{1-te^{c}}$$ and $$y = \frac{t(3 - 2te^{c})}{1-te^{c}}$$ There must be at least some logic to this, because several online calculators (for example WolframAlpha ) give the same result. But nevertheless it seems wrong, as the initial condition implies $4 - 8e^{c} = 6 - 8e^{c}$ ? Can someone please explain to me how to arrive at $y=2t$ , and where I would have gone wrong?","['initial-value-problems', 'ordinary-differential-equations']"
3814519,Diagonals dividing an hexagon into regions of the same area.,"Say a diagonal of a convex polygon with an even number of vertices is a line segment connecting opposite vertices. I've found the following problem in an old exercise list a teacher gave me: If each of the three diagonals of a convex hexagon divides it into regions of the same area, show that the three diagonals meet at the same point. In the image below I marked the areas determined by the diagonals. My first impulse was to try to show that $P = 0$ . For this I set up the following system: $$\begin{cases}
T/2 &= A+B+Y\\
T/2 &= B+C+Z\\
T/2 &= C+A+X\\
T/2 &= X+Y+A+P\\
T/2 &= Y+Z+B+P\\
T/2 &= Z+X+C+P\\
\end{cases}$$ Where $T = A+B+C+X+Y+Z+P$ is the hexagon's total area. The system may be simplified to $$\begin{cases}
A &= Z+P\\
B &= X+P\\
C &= Y+P
\end{cases}$$ but I don't see how to proceed from that point. Also, I wonder if the same statement is true by replacing the hexagon with a $2n-$ agon. Of course, in this case, writing a system with the areas would become unfeasible, so a smarter approach would be needed. Update: My approach was deemed to failure, as pointed out by a comment, since three segments dividing a polygon into regions of the same area are not required to concur. The fact that some of the regions are triangles seems to be important. This is a negative indicative for the more general question. Moreover, I've just now found out the general problem had already been posted in the Math Stack Exchange here , though it have not been properly answered. It does provide, however, a much pretty proof for the hexagonal case.","['contest-math', 'area', 'geometry']"
3814531,Multiset equivalence,"Let $a,b,c,d,e,f,g,h$ be natural numbers such that the multisets $\{a,b,c,d,a+b,c+d\}$ and $\{e,f,g,h,e+f,g+h\}$ are the same. Can we say that $\{a,b\}=\{e,f\}$ or $\{g,h\}$ ? and similarly $\{c,d\}=\{e,f\}$ or $\{g,h\}$ ? I have tried with case by case. Looks like it is correct. Is there an elegant proof for this ?","['multisets', 'combinatorics', 'discrete-mathematics']"
3814554,A generalizaton of nilpotent groups,"‎‎‎‎‎‎‎Let ‎ $‎G‎$ ‎be a‎ ‎group ‎and ‎‎ $‎‎\alpha\in Aut(G)$ ‎be a‎ ‎fixed ‎automorphism ‎of ‎‎ $‎G‎$ ‎. An ‎ $‎‎\alpha$ -commutator ‎of ‎elements ‎‎ $‎‎x, y\in G$ ‎is ‎‎ $‎‎[x, y]_{\alpha}= x^{-1}y^{-1}xy^{\alpha}$ . ‎The ‎‎ $‎‎\alpha$ -center ‎subgroup ‎of ‎‎ $‎G‎$ ‎, denoted by ‎ $‎‎Z^{\alpha}(G)$ ‎is ‎defined ‎as ‎‎ $‎‎Z^{\alpha}(G)= \{x\in G : [y, x]_{\alpha}= 1, ‎\forall ‎y\in ‎G‎ \}‎‎$ . ‎ ‎If ‎ $‎‎N$ ‎is a‎ ‎normal ‎subgroup ‎of ‎‎ $‎G‎$ ‎which ‎is ‎invariant ‎under ‎‎ $‎‎\alpha$ ‎and‎ ‎‎ $\bar{\alpha}‎$ is an automorphism of quotient group‎‎‎‎‎ ‎‎‎‎‎ $‎G/N$ ‎such that  ‎send ‎an ‎element ‎‎ $‎‎gN$ ‎to ‎‎ $‎‎g^{\alpha}N$ ‎, then ‎the ‎following normal ‎series‎ $$
‎\{ ‎1\}= ‎G_{0}‎\unlhd ‎G_{1}‎\unlhd ‎\dots‎ ‎\unlhd ‎G_{n}= ‎G‎,
‎‎$$ ‎‎is called a central ‎ $‎‎\alpha$ ‎-series whenever ‎ $‎‎G_{i}^{\alpha}= G_{i}$ ‎and ‎‎ $‎‎G_{i+1}/G_{i}‎\leq Z^{\bar{\alpha}}(G/G_{i})‎$ ‎, for ‎ $‎‎0‎\leq i‎\leq n-1‎‎$ ‎.‎‎ ‎An ‎‎ $‎‎\alpha$ -nilpotent ‎group ‎is a‎ ‎group ‎which ‎possesses  ‎at ‎least a‎ ‎central ‎‎ $‎‎‎\alpha‎$ ‎-series. It is to see that f $\alpha$ is an inner automorphism of a nilpotent group $G$ , then $G$ is an $\alpha$ -nilpotent group. My Question is:
Is there any non-inner automorphim $\alpha$ of a finite non-abelian $p$ -group $P$ , such that $P$ is $\alpha$ -nilpotent?","['automorphism-group', 'nilpotent-groups', 'abstract-algebra', 'p-groups', 'group-theory']"
3814573,How to Prove Something is a Contradiction only by Logical Equivalencies,"Having the proposition: $$
(\lnot p \rightarrow q) \land (\lnot p \rightarrow \lnot q)\land(p \rightarrow q) \land (p \rightarrow \lnot q)
$$ I want to prove this to be contradiction. So far I have that: $$
p \rightarrow q \equiv \lnot p \lor q
$$ But now I am stuck because I want to proceed with the Distributive Law, but don't know how to apply it in my new situation: $$
(p \lor q) \land (p \lor \lnot q) \land (\lnot p \lor q) \land (\lnot p \lor \lnot q)
$$","['propositional-calculus', 'logic', 'discrete-mathematics']"
3814704,Searching open source (possibly) to perform multivariate limit,"I'm searching same open source software to perform this kind of limit (without restricting and executing the limit to a variable): $$
\lim_{(x, y)\to(0, 0)}\frac{x^3y}{x^6+y^2}
$$ I've seen sage and maxima, but i don't know if they can help me...
Then, if not this, can they perform double and triple integrals?","['integration', 'multivariable-calculus', 'multiple-integral', 'derivatives']"
3814739,Intersection of collection of sets when given one is a subset of the other...,"I am trying to prove a proposition in Topology, however, in order to do so...it appears I need the following to be true. Hopefully, someone can validate this along with my proof. Thanks! Is the following true: Let $\{I_{\alpha}\}_{\alpha \in A}$ and $\{E_{\beta}\}_{\beta \in B}$ be two arbitrary collection of sets. If $\{I_{\alpha}\}_{\alpha \in A} \subseteq \{E_{\beta}\}_{\beta \in B}$ , then the following inclusion holds: $$\bigcap_{\beta\in B} E_{\beta} \subseteq \bigcap_{\alpha \in A} I_{\alpha}.$$ Quite naturally, I can think of examples where this holds. For example, take the following two collection of sets: $$I = \{\{a,b\}, \{a,b,c\}, \{a,b,c,d\}\} \implies \bigcap I = \{a,b\}.$$ $$E = \{\{a\},\{a,b\}, \{a,b,c\}, \{a,b,c,d\}, \} \implies \bigcap E = \{a\}.$$ And so we have that $I \subseteq E$ and $\bigcap E \subseteq \bigcap I$ . (My attempted) Proof: Let $x \in \bigcap_{\beta\in B} E_{\beta}$ , then for every $\beta \in B$ we have $x \in E_{\beta}$ . This is where I get stuck. I know (probably) that at this step I need to use the fact that $\{I_{\alpha}\}_{\alpha \in A} \subseteq \{E_{\beta}\}_{\beta \in B}$ , however, I am not sure how to properly word this... maybe something like...for each $\alpha \in A$ , there exists a $\beta \in B : 
I_{\alpha} = E_{\beta}$ . Therefore, for each $\alpha \in A$ we have that $x \in I_{\alpha}$ . This implies that $x \in \bigcap_{\alpha \in A} I_{\alpha}$ . Hence, the following inclusion holds: $$\bigcap_{\beta\in B} E_{\beta} \subseteq \bigcap_{\alpha \in A} I_{\alpha}.$$ Any help would be great, thanks!","['elementary-set-theory', 'general-topology']"
3814769,Munkres Thm 19.5 and Axiom of Choice,"Theorem 19.5 in Munkres Let $\{X_\alpha\}$ be an indexed family of spaces; let $A_\alpha  \subset X_ \alpha$ for each $\alpha$ . If $\prod X_{\alpha}$ is given either the product or the box topology, then $$\prod \bar{A}_{\alpha} = \overline{\prod A_{\alpha}}$$ Proof. Let $x=(x_\alpha)$ be a point of $\prod \bar{A}_{\alpha}$ ; we show that $x \in \overline{\prod A_{\alpha}}$ . Let $U=\prod U_{\alpha}$ be a basis element for either the box or product topology that contains $x$ . Since $x_\alpha \in \bar{A}_{\alpha}$ , we can choose a point $y_\alpha \in U_\alpha \cap A_\alpha$ for each $\alpha$ . Then $y=(y_\alpha)$ belongs to both $U$ and $\prod A_{\alpha}$ . Since $U$ is arbitrary, it follows that $x$ belongs to the closure of $\prod A_{\alpha}$ . I think that this part of the proof requires the Axiom of Choice because it says ""choose a point $y_\alpha \in U_\alpha \cap A_\alpha$ for each $\alpha$ "". In addition, I know that ""The Cartesian product of any nonempty family of nonempty sets is nonempty"" is equivalent to the AC. So since we know $U_\alpha \cap A_\alpha$ is nonempty for each $\alpha$ , we need AC to show that $U \cap \prod A_{\alpha} = \prod {U_{\alpha} \cap A_{\alpha}}$ is nonempty, proving that $x$ belongs to the closure of $\prod A_{\alpha}$ . Is there any way to prove this part of the theorem without using AC?","['elementary-set-theory', 'general-topology', 'axiom-of-choice', 'set-theory']"
3814813,The set of continuous functions vanishing at $c$ is not the principal ideal generated by $x-c$,"Let $R = \mathcal C([0,1], \mathbb R)$ and $M_c = \{f \in R \mid  f(c) = 0\}$ .
I would like to show that: a. $M_c \neq M_b$ if $b \neq c$ ; b. $M_c$ is not the principal ideal generated by $x-c$ ; c. $M_c$ is not finitely generated. For a. I think its clear that if $f(x)=x-c$ then $ f \in M_c $ and $f \in M_b$ iff $b=c$ . b. is trickier. I'm thinking about something like $f=|x-c|$ because then if $f=g(x-c)$ then $g=|x-c|/(x-c)$ and then $g \notin R$ . This doesn't feel good enough. How can I make this precise or am I even on the right track? EDIT; Suppose $c \neq 1$ . Let $f(x)=(x-c)^{1/2}\chi_{x > c}$ . Clearly $f \in M_c$ . Suppose $(x-c)$ divides $f$ Then $f= g(x-c)$ for some $g \in \mathcal{R}$ . I want to go for $g \notin \mathcal{R}$ but I'm pretty sure I cannot say $g=\frac{f}{x-c}$ because $x-c$ is not a unit...so I'm stuck here. EDIT2; Ok I think I have it.  Suppose $x>c$ . If we calculate: $$|g(x)-g(c)|=|\frac{f(x)}{x-c}-g(c)|\geq|x-c|^{-1/2}-|g(c)| \geq 1 - |g(c)|$$ So $g$ has no right limit as $x \rightarrow c$ and therefore $g \notin R$","['ring-theory', 'abstract-algebra']"
3814830,Counterexample in A.M.-G.M. inequality.,"TL;DR: Why does A.M.-G.M. inequality not give desired result for $x^4+\frac{1}{x^2}$ , in the form $x^4+\frac{1}{x^2}\ge 2\sqrt{x^2}$ but $2^{\sin x} + 2^{\cos x}$ does? My confusion about the application of this inequality keeps on increasing. First, the things I believe are true: For the equality to hold, each term must be equal . To get the actual absolute extrema, there should be a constant value on other side. (Because otherwise it will keep changing?) And that's why, this inequality does not work for $x^4+\frac{1}{x^2}$ in the form $$x^4+\frac{1}{x^2}\ge 2\sqrt{x^2}$$ Though, both terms are equal at $x=1$ but since there's not a constant value on RHS, we don't get desired result. Here's the question I encountered in my yesterday's exam: Find the minimum value of $2^{\sin x} + 2^{\cos x}$ I found that in this case there is not going to be a constant term in any case. Hence, A.M.-G.M. inequality is not useful here. So I just left question assuming it is out of my league. However, later my classmate showed me that the minimum value will be $2^{1-\frac{1}{\sqrt{2}}}$ using A.M.-G.M. inequality. $2^{\sin x}+2^{\cos x}\ge 2\cdot 2^{\frac12({\sin x +\cos x})}$ For minima, $\sin x +\cos x =-\sqrt2$ I pointed out that there should be a constant value. He then showed me the graph of the function on desmos and the minimum value is actually the absolute minima! My question is, Why does A.M.-G.M. inequality not give desired result for $x^4+\frac{1}{x^2}$ , in the form $x^4+\frac{1}{x^2}\ge 2\sqrt{x^2}$ but $2^{\sin x} + 2^{\cos x}$ does? Is this application of A.M.-G.M. inequality mere pure coincidence? (I'm not asking the condition for quality to hold). If yes, then how do we know which ones are suitable and which ones are not? If not, then how do we find the absolute extrema? And, I was thinking it has something to do with minimum of $\sqrt{x^2}$ not being in domain of $x^4+\frac{1}{x^2}$ . So I checked for $(x^2+x+1)^2+\frac{1}{(x^2+x+1}$ which verifies that my suspicion was wrong.","['maxima-minima', 'algebra-precalculus', 'a.m.-g.m.-inequality']"
3814861,Conditions for compactness of operator,"Let $A$ be an bounded operator on a Hilbert space with ONB $\{e_n\}_n$ . I am looking for precise conditions on $\langle e_n, A e_m \rangle$ to guarantee that $A$ is compact (i.e. the limit of finite rank operators). It is known that $A$ is trace-class if $$ \sum_{m} \sum_n|\langle e_n, A e_m \rangle| < \infty $$ and the trace class operators are within the compacts. However, what if $A$ is not trace class? If $A$ were diagonal we know the sufficient and necessary condition is that $$\langle e_n, A e_n \rangle\to 0 \qquad( n\to\infty)\,.$$ However, what if $A$ is not diagonal? Is there a criterion in terms of the decay of $\langle e_n, A e_m \rangle$ ?","['integral-operators', 'compact-operators', 'functional-analysis']"
3814880,Picard groups and localization,"Would like some help in solving an exercise in the K-book (Weibel). It has to do with the surjectivity of the base extension map $Pic(R)\rightarrow Pic(S^{-1} R)$ of Picard groups in a specific situation. Suppose $R$ is a (commutative) Krull domain (see https://en.wikipedia.org/wiki/Krull_ring ) and $S$ a multiplicatively closed subset of non-zero elements. Further assume that every height one prime ideal $\mathfrak{p}$ of $R$ which intersects $S$ is an invertible ideal. Then the base extension map $Pic(R)\rightarrow Pic(S^{-1} R)$ is onto. This is part (c) of exercise 3.8 in chapter 1 of the K book. If $R$ is regular, this is immediate via the class group, however, there are mild singularities in the context in which I would like to apply it. (EDIT) Incomplete solution: Let $I$ be a divisorial fractional ideal of $R$ . This means that it corresponds to a Weil divisor, or more precisely, $I=(R:(R:I))$ , see p. 11 of http://alpha.math.uga.edu/~pete/classgroup.pdf for further details. Assume that $I$ is invertible as an ideal of $S^{-1} R$ and thus the equivalence class $[I]\in Pic(S^{-1}R)$ . Let $J$ be an inverse of $I$ as an ideal in $S^{-1} R$ . Thus the $R$ -ideal $IJ$ equals $S^{-1}R$ on inverting $S$ . Every height one prime ideal containing $IJ$ must intersect $S$ and thus is invertible. Enumerate these ideals by $\mathfrak{p}_1,\dots, \mathfrak{p}_m$ . Let $n_i$ be the order to which a uniformizer in $R_{\mathfrak{p}_i}$ divides $(IJ)R_{\mathfrak{p}_i}$ and $M:=\prod_i \mathfrak{p}_i^{n_i}$ . It would seem that $IJ=M$ on the nose, and, if this is the case, $I(JM^{-1})=R$ . As a result, $I$ would be invertible as an $R$ -ideal. We would thus conclude that $[I]\in Pic(R)$ , and the exercise. However, in order to deduce that $IJ=M$ , I need to show that $IJ$ is divisorial. This is probably the hardest part, and this is where I am stuck.","['algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
3814909,Solving the following Diophantine equation: $m^2=n^5-5$,"I need to find all integer solutions to $m^2=n^5-5$ , yet I can't seem to find a good strategy. Here what I attempted: Taking mod 5, we see that $n^5-5\equiv n\pmod{5}$ using Fermat's little theorem. We see that $m^2\equiv 0, 1, 4\pmod{5}$ for any $m$ , so $n\equiv 0, 1, 4\pmod{5}$ . We also see by mod 4, $n^5-5\equiv n-1\pmod{4}$ , so and since $m^2\equiv 0, 1\pmod{4}$ , we see that $n\equiv 1, 2\pmod{4}$ . However, I don't even know where to go from here. I'm also relatively new to diophantine equations, so I don't even know if I am on the right path. Can anyone please help or give me a hint?","['number-theory', 'diophantine-equations']"
3814952,How do I show that a finite group $G$ of order $n$ is cyclic if there is at most one subgroup of order $d$ for each $d\mid n$?,"This particular question was asked in masters exam for which I am preparing and I could not solve it. Question: (a) Prove that if $G$ is a finite group of order $n$ such that for integer $d>0$ , $d\mid n$ , there is no more than one subgroup of $G$ of order $d$ , then $G$ must be cyclic . (b) Using (a) prove that multiplicative group of units in any finite field is cyclic. For (a), I thought that as $n\mid n$ and there is only one subgroup of $G$ of order $n$ and order of a subgroup is order of element so, there exists an element $a$ such that $|a|=n$ . But same argument can be used if statement says that there are more than one subgroup of order $d$ for each $d \mid n$ . So, what mistake I am making? and kindly tell right approach. For (b), the number of elements of in group is $p^{n} -p^{n-1}$ . I don't know how can I show that there exists an element equal to order of the group.","['group-theory', 'abstract-algebra', 'cyclic-groups']"
3815021,Radius of the circumscribed circle of an isosceles triangle,"An isosceles triangle $ABC$ is given $(AC=BC).$ The perimeter of $\triangle ABC$ is $2p$ , and the base angle is $\alpha.$ Find the radius of the circumscribed circle $R$ . $$R=\frac{p}{2\sin\alpha(1+\cos\alpha)}$$ Let $CD=2R.$ The triangle $BCD$ is a right triangle and we have $\angle BAC=\angle ABC=\angle BDC=\alpha.$ I am not sure how to approach the problem. It's really hard for me to solve problems like this. Can you give me a hint and some thoughts on the problem?","['triangles', 'trigonometry', 'geometry']"
3815040,$\int_{\Bbb{T}} e_n(\lambda) |\varphi(\lambda)|^2 = 0$ for all $n \neq 0$ implies $|\varphi|^2$ is constant almost surely,"Consider the circle group $\Bbb{T}\subseteq \Bbb{C}$ with its Haar measure $d \lambda$ . I have the following situation: $\varphi \in L^2(\Bbb{T})$ has norm $1$ , i.e. $$\Vert \varphi\Vert_2^2 = \int_\Bbb{T} |\varphi|^2 d \lambda=1$$ Put $e_n(\lambda) = \lambda^n, n \in \Bbb{Z}$ . We have the following situation $$n \neq 0 \implies \int_\Bbb{T} e_n |\varphi|^2 d \lambda = 0$$ Can I deduce that $|\varphi|^2$ is constant almost surely? Attempt: I tried to show that $|\varphi|^2 \in L^2(\Bbb{T})$ , so that $$|\varphi|^ 2 = \sum_{n \in \Bbb{Z}}\langle |\varphi|^2, e_n\rangle e_n= \langle |\varphi|^2, e_0\rangle e_0$$ by Plancherel's theorem. However, I don't succeed in showing that $|\varphi|^2 \in L^2(\Bbb{T})$","['measure-theory', 'fourier-analysis', 'harmonic-analysis', 'lp-spaces', 'haar-measure']"
3815065,Fourier Legendre expansion of Beta kernel $x^a (1-x)^b$,"Preliminaries. I have difficulty computing FL expansion of Beta kernel $f_{a,b}(x)=x^a (1-x)^b$ where $4a, 4b \in \mathbb{Z}$ . Here are two important examples: $a=s-1,b=0: x^{s-1}=\sum_{n=0}^\infty \frac{(-1)^n(3/2)_n(1-s)_n}{s (1/2)_n(1+s)_n} P_n(2x-1)$ $a=b=s-1: (x(1-x))^{s-1}=B(s,s)\sum_{n=0}^\infty  \frac{(5/4)_n(1-s)_n(1/2)_n}{(1/4)_n(1/2+s)_n(1)_n} P_{2n}(2x-1)$ When $4s\in \mathbb Z$ these can be simplified further (see this article by M. Cantarini and J. D'Aurizio for examples and applications). By using these formulas as well as reflection and multiplication by $x$ , all FL expansions of $f_{a,b}(x)$ where at least one of $a,b,a-b\in \mathbb Z$ are computable. Problem. How can we calculate the FL expansion where none of $a,b,a-b$ is integral? For instance I have no idea how to calculate the expansion of $\sqrt[4]{\frac x{1-x}}$ or $\frac{1}{\sqrt{x}\sqrt[4]{1-x}}$ . Thoughts. By repeated IBP, whenever $f$ has no polymonial singularities at $0,1$ : $$I_n=\int_0^1 f(x) P_n(2x-1) dx=\frac 1{n!} \int_0^1 f^{(n)}(x) x^n (1-x)^n dx$$ Taking $f(x)=x^a (1-x)^b$ yields $$I_n=\frac{1}{n!}\sum _{k=0}^n (-1)^{n-k} (a-k+1)_k \binom{n}{k} (b-(n-k)+1)_{n-k} B(a-k+n+1,b+k+1)$$ Here $(a)_k$ denotes Pochhammer symbol. In previous $2$ cases, this finite hypergeometric sum is evaluable via Dixon identity/residue calculus, but not for the general case. Background. This problem arises from evaluation of $\int_0^1 x^a (1-x)^b \text{Li}_n(x) \, dx$ . See this post for basic examples. More are given in this article . Update. When $a+b\in\mathbb Z$ , one may evaluate $\int_0^1 x^a (1-x)^b \text{Li}_n(x) \, dx$ by Beta derivatives directly, which circumvents calculation of FL expansion of rational terms like $x^k \sqrt[4]{\frac x{1-x}}$ , etc.","['integration', 'legendre-polynomials', 'closed-form', 'sequences-and-series', 'hypergeometric-function']"
3815078,Extension of the Schur product theorem to operators,"Given two $n\times n$ matrices $A$ and $B$ , define their Hadamard product $A\circ B$ as the element-wise product, i.e. $$(A\circ B)_{ij} = A_{ij}B_{ij}\,.$$ A well known result is the Schur product theorem, stating that if both $A$ and $B$ are non-negative defined matrices, then $A\circ B$ too is non-negative. Is it possible to somehow extend this result to integral operators on a Hilbert space? For instance let us consider two real functions $a,b:[0,1]^2\to \mathbb{R}$ . Let's assume that they're both continuous and symmetric (i.e. $a(x,y) = a(y,x)$ and the same for $b$ ). Then we can define two compact self-adjoint integral operators $A$ and $B$ on $L^2([0,1])$ , by $$A\phi(x) = \int_0^1 a(x,y)\phi(y)dy\,;\qquad B\phi(x) = \int_0^1 b(x,y)\phi(y)dy \,.$$ Let $A\circ B$ be the integral operator given by $$(A\circ B)\phi(x) = \int_0^1 a(x,y)b(x,y)\phi(y)dy\,.$$ Assume that $A$ and $B$ are non-negative, i.e. for all $\phi$ $\langle A\phi,\phi\rangle\geq 0$ and $\langle B\phi,\phi\rangle\geq 0$ . Can we state that $A\circ B$ is non-negative? At least, is it the case when $A$ and $B$ commute, so that they have a common orthonormal basis?","['positive-semidefinite', 'hadamard-product', 'integral-operators', 'hilbert-spaces', 'functional-analysis']"
3815137,"Integral Calculus, Infinitesimal","To integrate $y=f(x)$ from $a$ to $b$ we break the function into small rectangles of width $dx$ . So the $n$ -th rectangle will be at a distance of $n\,dx$ from $a$ on the $x$ -axis. Let there be $t$ rectangles between $a$ and $b$ .
Therefore $b=a+t\,dx$ . But $t\,dx$ will be very small for any positive Integer $t$ due to the properties of Infinitesimals. So how will it ever reach $b$ ? Do I have any misconceptions regarding calculus and infinitesimals?","['integration', 'area', 'definite-integrals', 'calculus', 'infinitesimals']"
3815174,Is there a way to solve the equation $\sin x = x\ln x$ analytically?,"Is there a way to solve the equation $\sin x = x\ln x$ numerically or analytically? The only way I have been able to solve this is using a graphic calculator like Desmos, but is there another way to solve this?","['trigonometry', 'logarithms', 'roots', 'numerical-methods']"
3815210,Evaluating two integrals involving $\tan^{-1}\left(\frac{\sqrt{x(1-x)}}{x+\frac12}\right)$,"I want to show $$I:=\int_0^{1}\tan^{-1}\left(\frac{\sqrt{x(1-x)}}{x+\frac12}\right)dx=\frac{\pi}{8}$$ and $$J:=\int_0^{1}\frac{1}{1-x}\tan^{-1}\left(\frac{\sqrt{x(1-x)}}{x+\frac12}\right)dx=\pi\log \frac{3}{2}$$ My work: Let us try to do the first one. Note that by making a change of variable $x\mapsto 1-x$ we have $$I=\int_0^1 \tan^{-1}\left(\frac{\sqrt{x(1-x)}}{\frac32-x}\right)dx$$ Adding the two expression of $I$ and doing some simple algebra lead us to $$2I=\int_0^1 \tan^{-1}\left(\frac{8}{3}\sqrt{x(1-x)}\right)dx$$ We can obtain similar expression for $2J$ as well. But I am not sure how to deal with this integral. The fractor $\frac{3}{8}$ looks really odd here. However, if you plug it in wolframalpha they indeed give you the desired result. I also tried substituting $x=\sin^2\theta$ or $x=\cos^2\theta$ . The expression didn't simplify. Perhaps there is some clever way to do it.","['integration', 'definite-integrals', 'real-analysis']"
3815212,How to perform double contour integrals?,"Let's say I have an integral of the form $$\oint\oint \frac{dz_1}{2\pi i} \frac{dz_2}{2\pi i} \frac{1}{(z_1 + n_1)(z_2 + n_2)(z_1 + z_2 + n)}$$ where $n_1,n_2,n\in\mathbb{Z}$ and both the contours are of the form which go from $c-i\infty$ to $c+i\infty$ in both cases for $c\in(0,1)$ and the contour is closed to the left at infinity. How would one evaluate the integral? What bothers me is that when $n=n_1+n_2$ , we end up getting a double pole, while in other cases there seems to be a single pole. How do we take care of this?","['complex-analysis', 'complex-integration']"
3815228,Does it make sense to relax the definition of convergent sequence?,"The usual definition of a convergent sequence is: A sequence $\{a_n\}_{n=1}^\infty$ converges to $c \iff \forall \epsilon \gt 0$ there exists an $N_\epsilon$ such that $|a_n - c| \le \epsilon$ for all $n \ge N_\epsilon$ The intuitive idea is that a sequence converges to $c$ , if after a certain point all $a_n$ stay within a given distance $\epsilon$ of $c$ . Now consider the following sequence: $$
\{a_n\}_{n=1}^\infty = \begin{cases}
 1 \quad \text{if } n = 2^k \text{ for some } k \in \mathbb{N} \\
 \frac{1}{n} \quad \text{otherwise}
\end{cases}$$ This sequence obviously doesn't converge to 0 according to the usual definition because there's always a large $N$ for which $a_N = 1$ . However, these $N$ s are very sparse: There's only one for each doubling of $a_n$ . In particular, as $n \to \infty$ , these outliers become ""infinitely sparse"". If "" $n = \infty$ "", then we'd need to wait for ""another $\infty$ "" elements until convergence is violated again. (I took this idea from slowly varying functions where, intuitively speaking, a function is slowly varying if it ""converges at $\infty$ "". Intuitively, $ln(x)$ is slowly varying because as $x \to \infty$ it takes larger and larger $x$ to produce a relevant increase in $ln(x)$ .) Has there been research using a relaxed definition of convergence, such that the above (and similar) sequences $\{a_n\}_{n=1}^\infty$ are convergent in this sense? If yes, are there interesting insights from this?","['convergence-divergence', 'sequences-and-series']"
3815246,Solving $\sqrt{1-x}=2x^2-1+2x\sqrt{1-x^2}$,"I have to solve this irrational equation on $\mathbb{R}$ : $$ \sqrt{1-x}=2x^2-1+2x\sqrt{1-x^2}$$ I tried to do a substitution with $u=1-x$ but the only things I manage to reach is the following equation by squaring and using $(a-b)(a+b)=a^2 -b^2$ : $$ (\sqrt{1-x}-2x\sqrt{1-x^2})^2 = (2x^2 -1)^2$$ $$\implies 1 - x + 4 x^2 - 4 x^4 - 4x \sqrt{1 - x}  \sqrt{1 - x^2} = 4x^4 - 4x^2 +1$$ $$ \implies -4x\sqrt{(1-x)(1+x)(1-x)} = 8x^4-8x^2$$ $$ \implies 4(1-x)\sqrt{1+x} = 8x^4 -8x^2$$ $$ \implies  (1-x)\sqrt{1+x} =  2x^2 (1-x^2)$$ I don't manage to go forward. The only thing I know is that the solution (if there is one) is in [-1;1]. Could you help me, please ?","['algebra-precalculus', 'radical-equations']"
3815298,"If $\tan x + \tan y = 4$ and $\cos x + \cos y = 1/5$, find $\tan(x+y)$.","If $\tan x + \tan y = 4$ and $\cos x + \cos y = 1/5$ , find $\tan(x+y)$ . Well, from the first condition, we get $$\tan x + \tan y = \frac{\sin(x+y)}{\cos x \cos y}=4 \implies \sin(x+y)=4\cos x \cos y$$ Then, $$\tan(x+y)=\frac{\tan x + \tan y}{1-\tan x \tan y}=\frac{4}{1-\frac{\sin x \cos y}{\cos x \cos y}}=\frac{4}{1-\frac{4\sin x \cos y}{\sin(x+y)}}$$ But in this way, I couldn't get use of the second condition. Actually, I squared the first condition and replaced $\tan^2$ with $\sec^2-1$ . Then, squared the second condition to find something useful about $\sec^2$ 's. Yet, nothing that works. Therefore, started substituting $\cos x = a$ and $\cos y = b$ to get $$
\begin{cases}
\cfrac{\pm\sqrt{1-a^2}}{a} + \cfrac{\pm\sqrt{1-b^2}}{b} = 4 \\
a + b = \cfrac{1}{5}
\end{cases}
$$ Nonetheless, this system doesn't seem to be as easy as this problem might be given as a multiple-choice problem (2-3 minutes for solving). Any help is appreciated.","['trigonometry', 'systems-of-equations']"
3815314,Converse of inverse function theorem,"Consider the following statement of the inverse function theorem: Let $f : W \to \mathbb{R}^n$ be a $C^1$ function from an open subset $W \subseteq \mathbb R^n$ and let $p \in W$ . $f$ has a local $C^1$ inverse if $Df(p)$ is invertible. This is effectively what one sees in Rudin's Principals of Mathematical Analysis . The question is: can the ""if"" can be replaced by ""iff""? It seems like the answer is obviously ""yes"": just apply the chain rule to $f \circ g = 1$ and $g \circ f = 1$ . Despite this, I can't find a statement that has the inverse function theorem in the ""iff"" form, so I feel like I must be making some kind of dumb mistake. For example this question has someone saying it only holds for ""low dimensions"" and someone else saying that it suffices to require the inverse to be surjective, which doesn't make sense to me given the straightforward proof I gave above. What am I missing?","['multivariable-calculus', 'derivatives', 'inverse-function-theorem', 'real-analysis']"
3815494,Solving a nonlinear (vector or multi-variable) ODE,"I am interested in solving the following differential equation: $$\frac{d\mathbf{v}}{dt}=A\mathbf{w}, \qquad \mathbf{v}=\left[x,y\right]^T, \mathbf{w}=[x^2,y^2,xy]^T, A\in\mathbb{R}^{2\times 3}$$ Equivalently, for $a_{ij}\in\mathbb{R}, \quad\forall i,j$ $$\frac{dx}{dt} = a_{11}x^2+a_{12}y^2+a_{13}xy$$ $$\frac{dy}{dt} = a_{21}x^2+a_{22}y^2+a_{23}xy$$ I have noticed that $$\mathbf{vv^T}=\left[\begin{array}{cc}x^2&xy\\xy&y^2\end{array}\right]$$ I'm looking for analytical solution to this problem.","['differential', 'ordinary-differential-equations']"
3815556,Prove $\det(A+B)=\det(B)$ implies that $A = 0$,"first time here.
I tried my best to translate the question : Show that the only matrix $A$ of size $n\times n$ that verify $\det(A+B)=\det(B)$ , for any $n\times n$ matrix $B$ , is the null matrix (the matrix full of zero, here of size n). I thought about using the formula $\det(A)$ that uses matrices of size $(n-1)$ : image But can't find to make it work.
Any ideas ? I don't need a full proof, only the start of it so I can try myself because i'm stuck","['matrices', 'determinant', 'linear-algebra']"
3815616,How is analysis beautiful? -- confusion from an algebraist [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 3 years ago . Improve this question As a math major about to go into grad school, I find the algebra-side of mathematics beautiful and inspiring -- I like to explore the hidden structure of things. I also find the geometry/topology interesting as they bring intuition to something we can visualize. Yet I can't feel the beauty of analysis but only its difficulty to visualize and its complicated ad-hoc techniques for manipulating the epsilons. I couldn't find satisfactory answers online. Most math educators seem to like algebra (or more easy-to-explain thing in general). It just seems hard to tell the big picture of the analysis. On others sites reddit and quora , I have mostly seen evidence for why people love algebra, but little evidence for why people love analysis. (possible exceptions may be Riemannian geometry/complex manifolds but I know very little about the details) For what it's worth, I am also physics/string theory inclined and know that much of theoretical physics that's hard to experimentally verify is driven by ""mathematical appeal"". I have not studied much analysis beyond measure theory. I would like to invite experts in analysis or people who have had any inspiring analysis courses to share their excitement. (You are also welcome to share why you hated analysis if you really want to. I just watched 3B1B's monster group video and felt more excited about algebra) More specifically, can you share results from analysis, which provide a deeper understanding of the underlying structures analysts work with? I am trying to get a sense of the beauty of analysis, and struggling to do so because it all seems very ad-hoc. Sorry if this question seems too opinion-based. But I believe the answer is illuminating to many rising math students. Technically, this post belongs to ""Constructive subjective questions"" so it should be reopened. If you also think so, you can vote for ""reopen"" below.","['riemannian-geometry', 'analysis', 'learning', 'soft-question', 'mathematical-physics']"
3815659,"Bijection from $A$ to $S\setminus A$, where $A$ is countably infinite","Original question: Suppose $S$ is an infinite set such that $|N|\leq|S|$ show there exists a countable infinite set $A$ , where $A$ is a subset of $S$ , and there is a bijection between $S\setminus A$ and $S$ . I know the same question is here but it doesn't conclude and it doesn't go down the path I end up taking. My work, I have proven there is a countably infinite subset of S. The proof I used is the same as proof 2 in this wiki proof guide . This is where things get shaky. To begin, I think the way I constructed set $A$ makes my proof impossible. I think there are two cases to consider. first $S$ is infinite, but not countable. Let $S = T\bigcup A$ , therefore $S\setminus A=T$ So, I'm am just showing a bijection form $T$ to $S$ . I know $T$ is a subset $S$ and I know that $T$ is infinite and uncountable from $A\bigcup T$ . I don't know how to progress here. Case 2, If $S$ is countable infinite then the way I constructed $A$ would mean that $S\setminus A = \{\emptyset\}$ as $A$ would be $S$ . P.S errors were pointed out to me, I would like to amend some statements above without obfuscating what was originally said. First, I meant to say proof 1 in the wiki article. I wanted to use the image of injective mapping $\phi:N\rightarrow S$ as my term for A",['elementary-set-theory']
3815671,Arguments on translation of affine space.,"Let $k$ be an algebraically closed field. I have found in some algebraic geometry books arguments such as ""by translation we may suppose that every maximal ideal $\mathcal{m} = \langle x_1-a_1, \ldots, x_n-a_n \rangle$ is of the form $\mathcal{m} = \langle x_1, \ldots, x_n \rangle$ "". My question is, what are all the transformations that we usually can use over $\mathbb{A}^n$ to simplify arguments? What does it mean formally to have such an invariance? Is there any book where this arguments are treated?",['algebraic-geometry']
3815674,Sets by Inclusion Clarification,"\begin{align}
A &= \lbrace 70, 210, 280\rbrace\\
B &= \mathbb{Z}\\
C &= \lbrace n\in\mathbb{N}\,|\, n=7 m \,\mbox{for some} \, m\in\mathbb{Q} \rbrace\\
D &= \lbrace n\in\mathbb{N}\,|\, n=35 m \,\mbox{for some} \, m\in\mathbb{N} \rbrace \\
E &= \lbrace n\in\mathbb{N}\,|\, n=7 m \,\mbox{for some} \, m\in\mathbb{N} \rbrace
\end{align} $$ Letter ⊆ Letter⊆Letter ⊆Letter ⊆Letter $$ I'm starting discrete mathematics and encountered this problem where we are tasked with ordering the set by inclusion. I understand the premise, but I think that my reasoning is wrong. Upon initial glance, I would sort it as $ A⊆ D⊆E⊆B⊆C$ , as A only has the three elements, the elements of D can be made by E, but then things start to get confusing with B and C. B stipulates all integers, which I understand, but C would be able to ""create"" elements that are not integers by virtue of the m $\in \Bbb{Q}$ allowing the use of all rational numbers, as well as all of the integers that could exist in $\Bbb{N}$ . However, would this even matter as we are adding them to set $n\in\Bbb{N}$ , which is only natural numbers? Any help would be appreciated. I've also included a picture for clarification. A screenshot of the problem","['elementary-set-theory', 'inclusion-exclusion', 'discrete-mathematics']"
3815702,"Find the probability distribution of the distance, of a random point in a unit square, from some diagonal","I already did some work on this, and would like to know, if I am not wrong. Basically I have a unit square (sides $1\times1$ ). In that square I pick a point, in a random way. Next I will define random variable $X$ , whose value is defined as ""the minimal distance of the randomly picked point, from some diagonal"". I measured  the distance between the point and the diagonal, as the length of a line between the two, with the line being perpendicular to the said diagonal. Next I interpreted the minimal distance as this: I have two diagonals in the square, from which I can measure the distance. So I simply pick the one, who is lesser than the other. With these assumptions, I get following image where the blue area plus pink area (let's name it $A$ ) is the set of all points, for which $X < d$ , where $d$ is chosen minimal distance. This area I calculated as follows: Area of the ""pink strip"" can be stated as ""the area of the triangle with red lining"" minus ""the are of the green triangle"". That I calculated as $\frac{1}{8}-\frac{1}{16}(1-2\sqrt{2}d)^2$ . Next, the area $A$ is eight time the pink strip, so $A = 1 - \frac{1}{2}(1-2\sqrt{2}d)^2 $ . Now, since area of the unit square is one, than with usage of the geometric probability, the probability distribution is given as $P(X \leq d) = 1 - \frac{1}{2}(1-2\sqrt{2}d)^2$ . What I would like to know is whether I am correct or not? And if not, where did I make a mistake? Thanks for help.","['probability-distributions', 'probability-theory', 'probability']"
3815709,Dice game - deciding whether to re-roll or not,"I am working on the following problem from a book: A casino has a dice game. You can roll as many times as you want. For each roll you get paid $M$ dollars where $M$ is the number of dots on the roll as long as you do not roll a 6. The payment for each roll is additive. However, if you roll a 6, the game terminates and you lose your accumulated profit thus far. How much are you willing to spend on this game? I am looking at the solution provided by the book, and I am confused. The solution is posted below. The part I am confused about is examining the threshhold for $n$ at which $$
5/6 \cdot n + 2.5 > n
$$ Equality in the above expression holds when $n = 15$ . I understand how they determined this solution, but it is not clear to me why this is the most optimal threshhold because the equation $5/6 * n + 2.5$ is derived from assuming you can only roll 1 more time. So if we have $n = \$16$ , the solution is telling us that we shouldn't re-roll because the expectation of the profit of an additional roll is less than the current profit. But this assumes that we can only roll 1 additional time. Shouldn't be consider the cases of rolling more than 1 time if we have $n = \$16$ already?","['gambling', 'expected-value', 'markov-process', 'dice', 'probability']"
3815764,A regular space has an infinite family composed by disjoint open sets.,"As the title says, I need to prove thath if $X$ is an infinite $T_3$ (here $T_3$ is $T_1$ + regularity) topological space then there exist $\mathcal{F}=\{U_n\mid n\in\mathbb{N} \}$ such that for all $n\in\mathbb{N}$ , the set $U_n$ is open and if $n\neq m$ then $U_n\cap U_n=\emptyset$ . My attempt: First, take two different points $x_1,x_2\in X$ . By Hausdorfness of $X$ , there exist $V_1, V_2$ a disjoint open sets such that $x_1\in V_1$ and $x_2\in V_2$ . Take $x_3\in X\setminus\{x_1,x_2 \}$ . Then, by the regularity of $X$ , there exist $V_3$ and $V_4$ a disjoint open sets such that $x_3\in V_3$ and $\{x_1,x_2 \}\subseteq V_4$ . Then take $U_1=V_4\cap V_1$ , $U_2=V_4\cap V_2$ and $U_3=V_3$ . Therefore $x_1\in U_1$ , $x_2\in U_2$ and $x_3\in U_3$ and moreover, $U_1\cap U_2=\emptyset$ , $U_1\cap U_3=\emptyset$ and $U_2\cap U_3=\emptyset$ and all of them are open sets. This step is like the basis of the induction. Now, suppose that we have constructed $U_1,U_2,\dots,U_n$ a family of mutually disjoint non-empty open sets. Following the later construction, we can take $x_i\in U_i$ for $i\in\{1,\dots,n \}$ . For $x_{n+1}\in X\setminus\{x_1,\dots,x_n \}$ , by regularity, there exist $W_1$ and $W_2$ disjoint open sets such that $x_{n+1}\in W_1$ and $\{x_1,\dots,x_n \}\subseteq W_2$ . But from here I'm stuck. What can I do? Any suggestion? Thanks.","['general-topology', 'separation-axioms']"
3815819,Clarification of a proof of $RP^3\cong SO(3)$,"I have some questions about the proof of $RP^3\cong SO(3)$ by Peter Franek on MSE: Each rotation in $\Bbb R^3$ is characterized by an ""oriented axis"" $v\in S^2$ and an angle $\varphi\in [0,\pi]$ and the only relations are $(v,\pi)=(-v,\pi)$ and $(v,0)=(w,0)$ for each $v,w\in S^2$ . If you represent $\Bbb RP^3$ as a 3-ball of diameter $\pi$ with identified antipodal points $v\cdot \pi=-v\cdot \pi$ for each $v\in S^2$ , then the map $SO(3)\to \Bbb RP^3$ just maps $(v,\varphi)$ to $[v\cdot \varphi]$ .
The angle $\varphi\,\,\mathrm{mod}\,2\pi$ depends continuously on the rotation and the axis $v$ depends continuously on the rotation whenever $\varphi\neq 0$ . I don't understand how we regard $[v\cdot \varphi]$ as an element of $\Bbb RP^3$ . $v\in S^2$ only has three coordinates but an element in $\Bbb RP^3$ should be like $[x,y,z,w]$ right? If his $[v\cdot \varphi]$ really means $[v, \varphi]$ then $[v,0] \ne [v',0]$ . Can some clarify the definition of his map for me? Source https://math.stackexchange.com/a/1688183/185631","['general-topology', 'lie-groups']"
3815824,Find all the intermediate fields of the splitting field of $x^4 - 2$ over $\mathbb{Q}$,"Okay, so I mostly worked this out, and I even created lattice diagrams as shown below. But I have a specific question about finding intermediate fields, which I will ask shortly. Let $\alpha = \sqrt[4]{2}$ and $\omega = e^{\frac{\pi}{4}i} = i$ .  Then $L = \mathbb{Q}(\alpha, i)$ is the splitting field of $x^4 -2$ over $\mathbb{Q}$ . Also, the Galois group $\Gamma_\mathbb{Q}(x^4 - 2) = D_8$ acts on the roots $\alpha, \alpha i, -\alpha,$ and $-\alpha i$ , and is generated by rotation $\sigma$ and reflection $\tau$ , where $\sigma(i) = i, \sigma(\alpha) = \alpha i$ and $\tau(\alpha) = \alpha, \tau(i) = -i$ . To find the intermediate fields between $L$ and $\mathbb{Q}$ , find the subgroups of $D_8$ instead with the idea that finding subgroups is easier and better understood than finding intermediate fields. Then from the subgroups, use the Galois correspondence to get all the intermediate fields. There are 10 subgroups of $D_8$ which must correspond to 10 intermediate fields.  Well, I pieced together 8 obvious candidates for intermediate fields, and in the end, I had to look up the other 2 which were $\mathbb{Q}(\alpha(1 + i))$ and $\mathbb{Q}(\alpha(1 - i))$ . Those two seemed strange until I realized that $\sqrt{8\alpha^2 i} = \alpha(1 + i)$ . Finally, I was able to check fixed fields to verify the exact correspondence, and come up with the diagrams. Question: Is there a systematic approach to finding and connecting up the corresponding intermediate fields once all the subgroups are known? I'm guessing, in general and maybe in this example with $D_8$ , there isn't a good, canonical way to anticipate and construct the field extensions?  The structure of groups and subgroups, as stated earlier, is easier and better understood than the structure of field extensions. Maybe this makes sense because the groups are finite and have only one operation, and fields are often infinite and have two operations. and UPDATE: In this lecture , Richard Borcherds clearly describes how to obtain the two nonobvious intermediate fields from the subgroups. Specifically, add the roots $\alpha$ and $\alpha i$ to fix by reflection one way, and then add the roots $\alpha$ and $-\alpha i$ to fix by reflection the other way.","['field-theory', 'galois-theory', 'abstract-algebra', 'splitting-field']"
3815852,what does the average degree of edges mean in graph?,"Hi I am a beginner in graph theory, discrete math, and network analysis. I am reading a paragraph about Friendship Paradox (the mean number of friends of friends is always greater than the mean number of friends of individuals) in a notebook. In order to proof this argument, it defines average degree of vertices (see below, the first formula), and average degree of edges (see below, the second formula). $$d_V(v)=\frac{1}{\deg(v)}\sum_{w\in N(v)}\deg(w)$$ $$d_{E}({v,w})=\frac{1}{2}(\deg(v)+\deg(w))$$ Theorem:
(1) $$<deg> \leq  <d_V>$$ (2) $$<deg> \leq  <d_E>$$ I can understand what average degree of vertices mean, which is the average of the number of friends that your friends have . We can proof the friendship paradox by proving the Mean of average degree of vertices is greater than average degree in the graph (as (1) of the above theorem). But what does average degree of edges mean in this context, and what (2) are trying to say? Thank you.","['graph-theory', 'network', 'discrete-mathematics']"
3815853,Difference in $\sin \theta ^2$ and $\sin^2\theta$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question What is difference between $\sin \theta ^2$ and $\sin ^2\theta$ ? What is the meaning of $\sin \theta\ \times$ $\sin \theta$ ?","['notation', 'trigonometry']"
3815870,Implicit Differentiation of $x+y= \arctan(y)$?,"I'm trying to verify that $x+y = \arctan(y)$ satisfies this differential equation: $1+(y^2)+(y^2)y'= 0$ . To do so, I tried to differentiate $x+y = \arctan(y)$ to get $y'$ , but only got so far: $$1 = \left(\frac{1}{1+y^2}y' -1 \right) y'.$$ Now I'm not sure how I can isolate $y'$ , or whether I am even taking the right first steps to solving the problem. Any hints/help about what direction I should take?","['implicit-differentiation', 'ordinary-differential-equations']"
3815892,Want help with proving a calculus theory,"Let $f(x)$ have a second derivative on the closed interval $[-2,2]$ . If $\left| f(x) \right| \le 1$ and $\frac{1}{2}  (f^{\prime}(0))^2+f(0)^3>\frac{3}{2}  $ when $-2\le x\le2$ , now I need to prove that there must be a point $x_{0}$ on the interval $(-2,2)$ such that $f^{\prime \prime}\left(x_{0}\right)+3f\left(x_{0}\right)^2=0$ . (Series[1/2 (f'[x])^2 + f[x]^3, {x, 0, 1}]) // FullSimplify The above method does not reveal the nature of the problem and solve it cleverly. I want to use a more generic and heuristic method to verify the conclusion of this abstract function problem. What can I do to solve this problem? The source of this problem (张宇高等数学18讲):","['calculus', 'derivatives']"
3815923,Alternative proof of isomorphism between linearly ordered sets,"The following theorem theorem is proved in Hrbacek and Jech's Introduction to Set Theory: Theorem: Let $(A, \prec)$ be a nonempty linearly ordered set with the properties: (a) For every $p \in A$ , there is $q \in A$ such that $q \succ p$ . (b) Every nonempty subset of a has a least element (in the order $\prec$ ). (c) Every nonempty subset of $A$ that has an upper bound    has a greatest element (in the order $\prec$ ). Then $(A, \prec)$ is isomorphic to $(\mathbb{N}, <)$ . We need to show there is a biyection from $\mathbb{N}$ to $A$ that preserves order. My questions ( below ), specifically, are about the part where they prove their function is surjective. So i'm gonna sketch the first part of the proof and then will reproduce verbatim the argument I am referring to: Sketch of proof: A sequence $f: \mathbb{N} \to A$ is constructed recusrively as follows: (i) $f_0 = a =$ the least element of $A$ (it exists by (b)) (ii) $f_{n+1} =$ the least element of $A$ greater than $f_n$ . (it exists by (a) and (b)) Clearly $f_n \prec f_{n+1}$ for every $n \in \mathbb{N}$ . Then $f_m \prec f_n$ whenever $m < n$ (by induction) and $f$ is one to one. We now prove the range of $f$ is $A$ . Their argument is this: If not, $A - \operatorname{ran}{f} \neq \emptyset$ ; let $p$ be the
least element of $A - \operatorname{ran}{f}$ . The set $B = \{q \in A: q \prec p \}$ has an upper bound $p$ , and is nonempty (otherwise, $p$ would be the least element of $A$ , but then $p = f_0$ ). Let $q$ be the
greatest element of $B$ (it exists by (c)). Since $q \prec p$ , we have $q = f_m$ for some $m \in \mathbb{N}$ . However, it is now easily seen
that $p$ is the least element of $A$ greater than $q$ . Therefore, $p = f_{m+1}$ by the recursive condition (ii). Consequently, $p \in \operatorname{ran}{f}$ , a contradiction. Question 1: is the boldfaced line the only part where they use the fact $p$ is the least element of $A - \operatorname{ran}{f}$ ? It seems to me it is. Question 2: Is the following alternative ""proof"" of this fact correct? Proof: Take $p \in A$ (notice I do not assume $p$ is a least element) and define $B = \{f_n \in A: f_n \prec p\}$ . If $p = a = f_0$ then we are done, so suppose $a = f_0 \prec p$ ; hence $B \neq \emptyset$ since $f_0 \in B$ . Also, $B$ has $p$ as an upper bound so it has a greatest element $f_n$ . If $f_{n+1} \prec p$ , then $f_{n+1} \in B$ and $f_n \prec f_{n+1}$ , which is impossible since $f_n$ is the greatest element of $B$ . If $p \prec f_{n+1}$ , then $f_n \prec p \prec f_{n+1}$ and $f_{n+1}$ is not the least element of $A$ greater than $f_n$ . We conclude then that $f_{n+1} = p$ (we are using the fact $\prec$ is a linear order here) and $p \in \operatorname{ran}{f}$ .","['elementary-set-theory', 'solution-verification']"
3815946,"Determining $\forall x \exists y \varphi (x,y) \to \forall y \exists x \varphi (x,y)$ is valid","I have the following formula, in an unspecified language: $\forall x \exists y \varphi (x,y) \to \forall y \exists x \varphi (x,y)$ where $\varphi(x,y)$ is a formula containing free variables $x$ and $y$ , presumably in that order. So I want to show whether or not this is a valid formula, ie I want to show whether or not $\vDash \forall x \exists y \varphi (x,y) \to \forall y \exists x \varphi (x,y)$ which is the case iff $\mathcal{S}, v \vDash \forall x \exists y \varphi (x,y) \to \forall y \exists x \varphi (x,y)$ , for every structure $\mathcal{S}$ and for every variable assignment $v$ iff $\mathcal{S}, v \nvDash \forall x \exists y \varphi (x,y)$ or $\mathcal{S}, v \vDash \forall y \exists x \varphi (x,y)$ , or both, for every structure $\mathcal{S}$ and for every variable assignment $v$ etc But trying to follow through with this process systematically by describing all the variants of variable assignments and such leaves me with a lot of moving parts but not really much clarity. My instinct is that the formula is not valid, and to show it as follows: Consider $\mathcal{N}$ , the structure of natural numbers, and let $\varphi(x,y) \equiv x<y$ Then indeed for every $x$ there is $y$ such that $x<y$ , but it is not the case that for every $y$ there is $x$ such that $x<y$ , as the natural numbers have a minimum value for which this does not hold. So the formula isn't true in at least one structure. Does this suffice? Or is there a neater, more formal method I should be using? And what about a formula that is valid, where I wouldn't be able to hunt for a counter-example? Any help appreciated, thank you","['predicate-logic', 'first-order-logic', 'model-theory', 'logic', 'discrete-mathematics']"
3815973,Finding the number of ways of arranging $2n$ white and black balls each such that no $n$ consecutive white balls are together,The question is: Find number of ways of arranging $2n$ white and $2n$ black balls such that no $n$ consecutive white balls are together. What I did was to arrange the black balls and number the $2n+1$ gaps between them as $x_i$ where $1\le i\le 2n+1$ and now use the relation: $$\sum_{i=1}^{2n+1}x_i=2n$$ where $0\le x_i\le n-1$ and $x_i$ denotes number of white balls in the $i^{th}$ gap. This yields the solution for the number of ways as the coeff. of $x^{2n}$ in $(1+x+x^2+...+x^{n-1})^{2n+1}$ This is where I'm having a problem. How do I calculate this coefficient? Any help or alternate methods would be appreciated.,"['permutations', 'binomial-coefficients', 'combinatorics']"
3815976,Are scalings of disjoint balls in $\mathbb{R}^{n}\setminus\{0\}$ still disjoint?,"Suppose $x,y\in S^{n-1}$ are distinct points. If I put $\epsilon = \frac{1}{2}\min\{\left\Vert x-y\right\Vert,\left\Vert x+y\right\Vert\}$ and form the balls $B(x,\epsilon),B(y,\epsilon)\subset \mathbb{R}^n\setminus\{0\}$ . Are the scalings of these ball still disjoint? I.e for $\lambda,\beta\in \mathbb{R}^+$ does $B(\beta\cdot y,\beta \cdot \epsilon)\cap B(\lambda \cdot x,\lambda \cdot \epsilon)=\emptyset$ hold for any choice of $\lambda,\beta \in \mathbb{R}^+$ ? My intuition tells me that the union of the scalings of such a ball is a cone in $\mathbb{R}^n\setminus\{0\}$ , and that the two cones created by the balls should be disjoint. However I haven't been able to show that it's true.","['general-topology', 'metric-spaces', 'real-analysis']"
3816042,An application of the Casey's theorem.,"Let $AB$ and $CD$ be two chords of a circle $\Phi$ , which intersects in the point $E$ .
Circles $\Phi_1$ and $\Phi_2$ are placed inside $\Phi$ such that $\Phi_1$ touches to segments $AE$ and $DE$ and to the circle $\Phi$ and $\Phi_2$ touches to segments $BE$ and $CE$ and to the circle $\Phi$ .
Let $l$ be a common external tangent to $\Phi_1$ and to $\Phi_2$ such that $l$ intersects segments $AE$ and $CE$ . Prove that $l||AC.$ I solved this problem $25$ years ago by using the Casey's theorem, but I forgot, how I made it. I am looking for a proof by the Casey's theorem only. This problem also $4.7.29$ from a book of A.Akopyan ""Geometry in Figures"". We can make here the following things. We can use  Casey for  ""quadrilaterals"" $A\Phi_2D\Phi_1$ , $C\Phi_2D\Phi_1$ or even for $AC\Phi_2\Phi_1$ ,
but I don't see how it may help. About the Casey's theorem see here: https://en.wikipedia.org/wiki/Casey%27s_theorem Thank you!","['contest-math', 'euclidean-geometry', 'circles', 'geometry']"
3816150,Spectral Theorem for unbounded self adjoint operators,"So I've been working on the Spectral Theorem for self adjoint unbounded Operators with the Book by Rudin and got to a problem:
Let $(X,\mathcal{A})$ be a measure space, $H$ a complex Hilbert space and $P:\mathcal{A}\rightarrow B(H)$ a resolution of the identity. Then, to every measurable function $f:X\rightarrow\mathbb{C}$ there exists a densely defined operator $\Psi(f)$ in $H$ , with domain $D(\Psi(f)) = \{x\in H: \int_{X} \vert f(\lambda) \vert^{2} \ d\langle P(\lambda)x,x\rangle < \infty\}$ , which is characterized by $$\langle \Psi(f)x,y\rangle = \int_{X} f(\lambda) \ d\langle P(\lambda)x,y\rangle$$ for all $x\in D(\Psi(f))$ and $y\in H$ . My problem is the following theorem: In the above situation, if $D(\Psi(f)) = H$ then $f$ is essentially bounded. $\textbf{Proof:}$ Since $\Psi(f)$ is a closed operator, the closed graph theorem implies $\Psi(f)\in B(H)$ . If $f_{n} = f\chi_{A_{n}}$ for $A_{n} = \{x\in X : \vert f(x)\vert \leq n\}$ and $n\in\mathbb{N}$ , then it follows $$\Vert f_{n}\Vert_{\infty} = \Vert \Psi(f_{n})\Vert = \Vert \Psi(f)\Psi(\chi_{A_{n}}) \Vert \leq\Vert \Psi(f) \Vert,$$ since $\Vert\Psi(\chi_{A_{n}})\Vert = \Vert \chi_{A_{n}} \Vert_{\infty}\leq 1$ . Thus $\Vert f \Vert_{\infty} \leq \Vert\Psi(f)\Vert$ and $f$ is essentially bounded. I don't know how we can conclude that $\Vert f \Vert_{\infty} \leq \Vert \Psi(f)\Vert$ , since $f_{n}\rightarrow f$ only pointwise. I also know that $\Vert \Psi(f_{n}) \Vert$ converges to $\Vert\Psi(f)\Vert$ but I can't see how that could help me. I would appreciate any help.","['spectral-theory', 'functional-analysis']"
3816204,Meaning of a characteristic curve (introductory PDE),"What is the meaning of a characteristic curve when solving PDE? For example, in solving $u_x + yu_y = 0 $ , we get $dy/dx = y/1$ and so solving this ODE, we obtain $y=Ce^x $ . Here, what does $y=Ce^x $ really mean? Does it mean that $x$ and $y$ have to be in the relation $y=Ce^x $ ? Drawing the characteristic curves $y=Ce^x $ , how does it help us determine the solution $u$ ?","['ordinary-differential-equations', 'partial-differential-equations']"
3816211,"prove that $g_\theta(t)$ is increasing on $[1,\infty)$.","Here's an aggravating problem.  Fix some constant $\theta\in(0,1)$ .  I have a function $$g_\theta(t)=t^\theta\left[(t+1)^{1-\theta}-\left(t+\frac{1}{2}\right)^{1-\theta}\right]$$ which, when plotted in wolfram for various values of $\theta$ , is clearly increasing on $[1,\infty)$ .  But, I need to prove this rigorously. The obvious thing to do is to try to show that $g'_\theta(t)>0$ on $[1,\infty)$ .  Unfortunately, we have this mess for the derivative: $$g'_\theta(t)
=(\theta t^{\theta-1}+t^\theta)(t+1)^{-\theta}
-(\theta t^{\theta-1}/2+t^\theta)\left(t+\frac{1}{2}\right)^{-\theta}$$ Maybe there's some special function I can use to make this easier.  Or some convexity trick I'm not seeing.  Idk.  What do you guys think? Thanks!","['derivatives', 'monotone-functions', 'real-analysis']"
3816256,"Check the range is the same: $\sum\limits_{\ell=0}^k\sum\limits_{j=\ell}^m=\sum\limits_{j=0}^m\sum\limits_{\ell=0}^{\min(j,k)}$?","Given that $m\ge k$ , why the range of $$\sum_{\ell=0}^k\sum_{j=\ell}^m$$ is the same as $$\sum_{j=0}^m\sum_{\ell=0}^{\min(j,k)}$$ Instead of write down all to check, is there any systematical way? I thought the latter would be $\displaystyle\sum\limits_{j=0}^m\sum\limits_{\ell=0}^{j[j\le k]+(-1)[j>k]}$ , but it seems like this is wrong, can anyone help me point out way? ( $\textrm{if P is True, }[P]=1, \textrm{otherwise }[P]=0$ .)","['algebra-precalculus', 'summation']"
3816283,Existence of a limit of a function given that the derivative at the point exists,"Suppose $f(x)$ is continuous in a neighbourhood of $a$ , and $f'(a)$ exists. Does the limit $$\lim_{x \rightarrow a} \frac{f(x) - f(2a -x)}{2(x-a)}$$ always exist?","['limits', 'derivatives']"
3816328,Slope-stability: subsheaves vs. subbundles,"Recall that the slope of a holomorphic vector bundle $\mathcal{E}$ over a smooth projective variety (or rather a compact Kähler manifold) $X$ is defined as $\mu(\mathcal{E}) :=\frac{\operatorname{deg}(\mathcal{E})}{\operatorname{rk} \mathcal{E}}$ where $\operatorname{deg}(\mathcal{E})$ is defined as $c_1(\mathcal{E}) \cdot \omega^{n-2}$ for your favourite ample  (or even Kähler) class $\omega$ . The bundle is called stable if for every subsheaf $\mathcal{F} \subset \mathcal{E}$ , one has $\mu(\mathcal{F}) <\mu( \mathcal{E})$ . These are my questions: Why do we require that $\mathcal{E}$ has no subsheaves with greater slope, rather then no subbundles? Is the difference sufficient and what is the motivation for this choice? More precisely, 1a) is there an explicit example of a non-stable vector bundle, which has no subbundles with bigger slope? 1b) is there a reasonable moduli space for holomorphic vector bundles having no destabilising subbundles? If yes, how far is it from the moduli space of stable vector bundles? 2)The definition of slope a priori depends on the choice of Kähler form. How much does the moduli space of stable vector bundles depends on this choice? I am also interested in the analogues of the questions 1a) and 1b) for the Higgs bundles. In this case we require that  a Higgs bundle $(\mathcal{E}, \theta)$ has no Higgs subsheaves(?) $(\mathcal{F}, \theta|_{\mathcal{F}})$ with bigger slope. UPD: -cross-list with https://mathoverflow.net/questions/371022/on-definition-of-stable-vector-higgs-bundle?noredirect=1#comment937997_371022 -In 1a) one can ask the same for semi-stability; -On a curve every destabilizing subsheaf is contained in a subbundle, hence there is no difference indeed; -For Gieseker-stability the answer for 2) is the theory of wall-crossings, but a have never seen a version of it for slope-stability (and for Higgs bundles as well). Is it exist?","['holomorphic-bundles', 'kahler-manifolds', 'complex-geometry', 'vector-bundles', 'algebraic-geometry']"
3816389,Inequality for divisor sigma $\sigma_{\nu}(n)$ function.,"The divisor sigma function is defined as $$
\sigma_{\nu}(n):=\sum_{d|n}d^{\nu}\textrm{, }n\in\textbf{N}
$$ It is known that under Riemann's hypothesis it holds the following inequality due to Ramanujan and Robin see here $$
\sigma_1(n)<e^{\gamma}n\log\log n\textrm{, }n>5040\tag 1
$$ ( $\gamma$ is Euler-constant, $0.5772156649...$ )The opposite is also true: (1) implies Riemann's hypothesis. My question is: Are there similar inequalities in the literature for $\sigma_{\nu}(n)$ , for other values of $\nu=2,3,\ldots$ ? With the word similar I mean inequalities of the form $$
\sigma_{\nu}(n)<C_{\nu}n^{\nu}\log\log n\textrm{, }\forall n>n_0,
$$ where $C_{\nu}$ some known constants (I need the values of these constants also). My starting point is equation $$
9d^2(k^2+4d)=n^2\textrm{, }(n,k,d)\in\textbf{Z}^3.\tag 2
$$ When $n$ is around $\sigma_1(n)\approx e^{\gamma}n\log\log n$ i.e. when $n$ is such that $0,85<\frac{\sigma(n)}{e^{\gamma}n\log\log n}<1$ , then we have a ''jump'' in the number of solutions $L=L(n)$ of (2) from (say) $L$ to $L+6$ . For example $n=10080$ , $L=18$ ; $n=19440$ , $L=24$ ; $n=55440$ , $L=30$ ; $n=443520$ , $L=36$ ; $\ldots$ . Equation (2) is equivalent (have the same number of solutions) to $$
x^3+y^3+z^3=n\textrm{, }x+y+z=0
$$ For the case $$
x^3+y^3+z^3=n\textrm{, }x+y+z=t,
$$ we have the general equation $$
\left(\frac{n-t^3}{3d}+2t\right)^2-4d=k^2,
$$ which I suspect to have solutions $n$ around $\sigma_{\nu}(n)\approx C_{\nu}n^{\nu}\log\log n$ , $\nu=\nu(t)$ .","['number-theory', 'riemann-hypothesis', 'analytic-number-theory', 'divisor-counting-function', 'inequality']"
3816400,Locally constant group schemes,"Let $k$ be a field of characteristic $p$ , then the group scheme $\mu_\ell$ , if $p$ does not divide $\ell$ , is étale-locally isomorphic to $\mathbb{Z}/\ell\mathbb{Z}$ . I have two quick questions which I don't feel very certain. Is $\mu_p$ fppf locally isomorphic to $\mathbb{Z}/p\mathbb{Z} $ ? Also, is $\alpha_p $ fppf locally constant?","['group-schemes', 'algebraic-geometry']"
3816401,Rational functions on elliptic curves,"Recall that an elliptic curve over a field $k$ i.e a proper smooth connected curve of genus $1$ equipped with a distinguished $k$ -rational point, I'll be really grateful for any help in understanding the following part of our course Let $(E,0)$ be an elliptic curve, using Riemann-Roch we construct an isomorphism into $\operatorname{Proj}\,k[X,Y,Z]/Y^2Z+a_1XYZ+a_3YZ^2-X^3-a_2X^2Z-a_4XZ^2-a_6Z^3$ that can be written informally as $P\rightarrow [x(P):y(P):1(P)]$ , where $x$ and $y$ are rational functions such that $v_0(x)=-2$ and $v_0(y)=-3$ . Why does $0$ map to the infinity point $O=[0:1:0]$ ? According to Hartshorne it is because both $x$ and $y$ have poles in $0$ but I can't see why.","['algebraic-geometry', 'schemes', 'elliptic-curves']"
3816455,Evaluating double sum $\sum_{k = 1}^\infty \left( \frac{(-1)^{k - 1}}{k} \sum_{n = 0}^\infty \frac{1}{k \cdot 2^n + 5}\right)$,"Find $$\sum_{k = 1}^\infty \left( \frac{(-1)^{k - 1}}{k} \sum_{n = 0}^\infty \frac{1}{k \cdot 2^n + 5}\right)$$ So far, I've gotten that the sum of the left is equal to $\log(2),$ meaning we have to evaluate $\displaystyle \sum_{n=0}^{\infty} \frac{\log(2)}{k\cdot2^n+5},$ but I don't know how to proceed. I don't think it's geometric or we can use Partial Fraction Decomposition on it.","['calculus', 'algebra-precalculus', 'summation']"
3816498,"How should I state the null and alternative hypotheses, when the alternative speaks in favour of the null one?","The question is as follows: More than 50% of all people usually drink coffee before breakfast. To
check this claim 100 people were chosen randomly and 60 of them
declared to have a coffee before breakfast. Clearly the sample taken confirms that the null hypothesis is correct. Moreover, it is said alternative hypothesis never contains <=, >= or = operators, so even if I set H 0 : p > 0.5, I shouldn't do H a : p <= 0.5, which would contradict the sample taken. How should I deal with this problem? Or could that be an error in the question?","['statistics', 'hypothesis-testing']"
3816502,I am stuck in this geometry problem,"Coordinates of point $P$ are $(4,5)$ . If A and B are variable points on the straight lines $y=x$ and $y=2x$ . Then the minimum value of ${ \left( PA+PB+AB \right)  }^{ 2 }$ I tried it as follow PA will be minimum when when A is the foot of perpendicular from P to $y=x$ and similarly B is the foot of perpendicular from P to $y=2x$ . So I simply find all the distances by finding the coordinates of $A$ and $B$ .
But I am not confident in my approach as the $AB$ might not be minimum in this case .","['geometry', 'applications']"
3816534,"If $f:A→\Bbb R^n$ is differentiable at $a$ then there exist $δ>0$ such that $\Biggl|\frac{f(a+tu)-f(a)-B\cdot tu}{|t|}\Biggl|<ε$ for any $t\in(-δ,δ)$","Definition Let $A\subset\Bbb R^m$ and let $f:A\rightarrow\Bbb R^n$ a function and we suppose that $A$ contains a neighborhood of $a$ . So given $u\in\Bbb R^m$ with $u\neq 0$ we define the directional derivative of $f$ at $a$ with respect to the vector $u$ the quantity $$
f'(a;u):=\lim_{t\rightarrow 0}\frac{f(a+tu)-f(a)}t
$$ provided the limit exists. Definition Let $A\subset\Bbb R^m$ and let $f:A\rightarrow\Bbb R^n$ a function and we suppose $A$ contains a neighborhood of $a$ . So we say that $f$ is differentiable at $a$ if there is a $n$ by $m$ matrix $B$ such that $$
\frac{f(a+h)-f(a)-B\cdot h}{|h|}\rightarrow0\,\,\,\text{as}\,\,\,h\rightarrow0
$$ The matrix $B$ , which is unique, is called the derivative of $f$ at $a$ ; t is denoted $Df(a)$ . Theorem Let $A\subset\Bbb R^m$ and let $f:A\rightarrow\Bbb R^n$ a function. So if $f$ is differentiable at $a$ then all the directionalderivatives of $f$ at $a$ exists and $$
f'(a;u)=Df(a)\cdot u
$$ Proof . See the theorem $5.1$ of the text Analysis on Manifolds by James Munkres. So clearly with the previous definition if $f:A\rightarrow\Bbb R^n$ is derivable at the point $a$ in the direction $u\in\Bbb R^n$ then for any $\epsilon>0$ there exist $\delta_{\epsilon,u}$ such that $$
\Biggl|\frac{f(a+tu)-f(a)}t\Biggl|<\epsilon
$$ for any $t\in(-\delta_{\epsilon,u},\delta_{\epsilon,u})$ and so for the completness theorem given $\epsilon>0$ the quantity $$
\delta_\epsilon:=\inf\{\delta_{\epsilon,u}\in\Bbb R^n: u\,\text{is a direction of}\,\Bbb R^n\}
$$ is well defined and it is non negative. So I ask if in the case where $f$ is differentiable at $a$ necessarly it must be $\delta_\epsilon>0$ for any $\epsilon>0$ So could someone help me, please?","['real-analysis', 'multivariable-calculus', 'calculus', 'derivatives', 'differential-geometry']"
3816551,GAP function to convert a subgroup lattice to a simple graph?,"I've been using XGAP to view the subgroup lattices of various groups, and I am interested in their properties as simple graphs. I need a program that, given a group G, can output the subgroup lattice of G as a simple graph; that is, as a set V of vertices and a set E of edges. Does such a function exist? I have looked into GRAPE and DIGRAPH to no avail.","['graph-theory', 'gap', 'group-theory']"
3816562,Show this quad is cyclic,"Doesn't seem hard but it got me stuck: $I$ is the incenter of $\triangle ABC$ $D$ the contact point of the incircle with $BC$ $M,M'$ are the intersection of the circumcircle of $\triangle ABC$ with the perpendicular bisector of $BC$ , $M'$ on the arc $BAC$ $E = AD \cap (ABC)$ $F,F' = M'E \cap (BCI)$ Show that $AIEF'$ lies on a circle. I saw this problem in a blog. We know that quadrilateral $BFCF'$ is harmonic because $BM'$ and $CM'$ are both tangent lines. Therefore line $F'F$ is symedian of $\triangle BF'C$ . The blog said that this, along with the fact that $\angle DIM = \angle DEF$ implies the quad $AIEF'$ is cyclic and I don't see this implication (even tho I do see that $FF'$ is symedian and the angle equality). If you guys can come up with any other ideas that will be cool too. A couple facts: $E$ is midpoint of $FF'$ the bisector of $\angle BF'C$ meets both: $(BCI)$ and $MM'$ at the same point.","['euclidean-geometry', 'projective-geometry', 'quadrilateral', 'circles', 'geometry']"
3816579,Number of matrices with determinant value $0$,"A $3 \times 3$ matrix is formed using the elements from the set $\{-1,0,1\}$ . How many matrices will have determinant value $0$ . Let matrix is \begin{bmatrix}
p & q & r\\
x & y & z \\
a & b &c
\end{bmatrix} So total matrices formed will be $3^9$ and determinant is given by $\Delta=pyc+rxb+qza-rya-qxc-pzb$ Making some combinations I deduced that determinant value will go from $-4$ to $4$ but what approach should I follow to get number of determinants having value $0$ ?","['permutations', 'combinations', 'determinant', 'matrices', 'combinatorics']"
3816586,"Does the inverse of a unimodular matrix with entries in $\{-1,0,1\}$ again have entries in $\{-1,0,1\}$?","A matrix $U \in \mathbb Z^{n \times n}$ with integer entries is called unimodular if its determinant is $+1$ or $-1$ . The inverse of a unimodular matrix is again unimodular, since its entries as calculated by Cramer's rule are integers divided by $+1$ or $-1$ . Question: If $U\in \mathbb Z^{n \times n}$ is a unimodular matrix that has only entries in $\{-1,0,1\}$ , will its inverse $U^{-1}$ again have entries in $\{-1,0,1\}$ ?","['linear-programming', 'matrices', 'linear-algebra', 'integer-programming', 'discrete-mathematics']"
3816600,"Prove that if $(x,y)∈X$, then for some $λ\in [0,1],\space x=λa_1+(1−λ)a_2,\space y=λb_1+(1−λ)b_2$.","Let $X⊂R^2$ be a set satisfying the following properties: (i) If $(x_1,y_1)$ and $(x_2,y_2)$ are any two distinct elements in X, then either, $x_1>x_2$ and $y_1>y_2$ or, $x_1<x_2$ and $y_1<y_2$ (ii) There are two elements $(a_1,b_1)$ and $(a_2,b_2)$ in $X$ such that for any $(x,y)\in X$ , $a_1≤x\leq a_2$ and $b_1≤y≤b_2$ (iii) If $(x_1,y_1)$ and $(x_2,y_2)$ are two elements of $X$ , then for all $λ∈[0,1],\space (λx_1+(1−λ)x_2,λy_1+(1−λ)y_2)∈X$ Show that if $(x,y)∈X$ , then for some $λ\in [0,1],\space x=λa_1+(1−λ)a_2,\space y=λb_1+(1−λ)b_2$ This question is the same as the one I am asking here but I think that my approach is different from the one given there and I want to verify my approach. My Approach: From observation $(i)$ , it can be said that $x_1-x_2$ and $y_1-y_2$ will always be of same sign. Hence the slope of line passing through points $(x_1,y_1)$ and $(x_2,y_2)$ will be positive. From observation $(ii)$ , which says that if $(x,y)\in X$ , then $a_1\leq x\leq a_2$ and $b_1\leq y\leq b_2$ , it can be inferred that the subset $S$ denotes a rectangle in $R^2$ with vertices $(a_i,b_i),\space i,j=1$ or $2$ . (suppose this) From observation $(iii)$ , the point can be written as $(\lambda(x_1-x_2)+x_2,\space \lambda(y_1-y_2)+y_2)$ . It can be observed that this point satisfies the equation $y-y_2=\dfrac{y_1-y_2}{x_1-x_2}(x-x_2)$ . Thus any such point will lie on the line joining $(x_1,y_1)$ and $(x_2,y_2)$ . Also since $\lambda\in [0,1],\space x_2\leq (\lambda(x_1-x_2)+x_2)\leq x_1$ and $y_2\leq (\lambda(y_1-y_2)+y_2)\leq y_1$ (assuming $x_1>x_2$ and $y_1>y_2$ ). Thus this point will always lie on line segment joining the points $(x_1,y_1)$ and $(x_2,y_2)$ . Using these observations, we can say that $(\lambda(a_1-a_2)+a_2,\lambda(b_1-b_2)+b_2)$ or $(λa_1+(1−λ)a_2,λb_1+(1−λ)b_2)$ will lie on line segment joining $(a_1,b_1)$ and $(a_2,b_2)$ . Since the given line segment lies completely in $X$ , there must be some point $(x,y)\in X$ which corresponds to the given point. After writing this complete solution, I have now observed that I have done the opposite of what was asked, but I feel that this approach of connecting this question to coordinate geometry must yield something good. So please offer suggestions to improve this solution but using the same approach. THANKS","['elementary-set-theory', 'coordinate-systems', 'solution-verification']"
3816714,Prove that the area of a triangle is $\frac12 |a \times b| = \frac12 |b \times c| = \frac12 |c \times a|$,"I have a triangle $ABC$ where $\overrightarrow{AB} = \vec c, \overrightarrow{BC} = \vec a, \overrightarrow{CA} = \vec b$ and $\angle (\vec b, \vec c) = \alpha, \angle (\vec c, \vec a) = \beta, \angle (\vec a, \vec b) = \gamma$ I have to prove that the area of this triangle is given by $ \frac 12 |a \times b| = \frac 12 |b \times c| = \frac 12 |c \times a|$ I'm not sure where to begin but I don't think the following can be considered as a proof. $\frac 12 |a \times b| = |a||b| \sin \gamma = |b||c| \sin \alpha = \frac 12 |b \times c|$","['triangles', 'vectors', 'geometry']"
3816749,What are lattices called that are isomorphic to the open sets of a topology? [duplicate],"This question already has answers here : When a lattice is a lattice of open sets of some topological space? (2 answers) Closed 3 years ago . What are the bounded lattices $\mathcal{L}=(Q,\lor,\land,\bot,\top)$ called that are isomorphic to topological spaces? I.e. those such that there exists a topological space $(X,\tau)$ and a bijection $f:Q\to\tau$ such that $f(\top)=X$ and $f(\bot)=\emptyset$ with $f(\bigvee_{i\in I}x_i)=\bigcup_{i\in I}f(x_i)$ and $f(a\land b)=f(a)\cap f(b)$ ? It would seem the study of these lattices is essentially just the study of point-set topology, thus I imagine they must have a name? What are they called?","['order-theory', 'general-topology', 'lattice-orders']"
3816777,Why are topologies typically defined with open sets rather then closed sets?,"In general one can define a topological space either in terms of its open sets or in terms of its closed sets, however it seems that depending on the context the ""closed sets"" might make more intuitive sense, for example suppose that some class of geometric structures is closed under isomorphisms with respect to arbitrary intersections and finite unions. Now if we look at these as the open sets of a topology on some specific sets by taking complements it may be that this isomorphism between the structures no longer works as the complements are relative with respect to the points, thus it would make more sense to study these as closed sets rather then complementing them and looking at them as open sets. Of course though it is simpler to adopt one of the two representations when defining a bunch of associated terminology otherwise you'll end up having to do this twice, with roughly the same definition only you're running around complementing every set in the corresponding definitions. Is that what this is just an arbitrary convention to save space i.e. people just use a single notion to save time?","['general-topology', 'soft-question', 'terminology']"
3816788,How many solutions are there to $x_1 + x_2 + x_3 + x_4 = 30$ s.t. $x_1 + x_2 \le 20$ and $x_3 \ge 7$?,"Here is the original question from the book: How many ways are there to distribute $30$ green balls to $4$ persons if Alice and Eve together get no more than $20$ and Lucky gets at least $7$ ? I rewrote the problem as the title of this post, i.e. how many solutions are there to $x_1 + x_2 + x_3 + x_4 = 30$ such that $x_1 + x_2 \le 20$ (Alice and Eve together get no more than $20$ ) and $x_3 \ge 7$ (Lucky gets at least $7$ ). First, I ""eliminated"" the second restriction. Give Lucky $7$ balls and distribute the remaining $23$ . Thus, we have $$
x_1 + x_2 + (x_3 + 7) + x_4 = 30 \\
x_1 + x_2 + x_3 + x_4 = 23
$$ Then, since the other restriction left me with doubts, I counted the number of solutions to the equation above, which is $23+3 \choose 3$ , and tried counting the solutions that violate $x_1 + x_2 \le 20$ to subtract from $23+3 \choose 3$ . Let $k = x_1 + x_2 \le 20$ . The solutions that violate this are those where $k \ge 21$ . Hence $$
(k + 21) + x_3 + x_4 = 23 \\
k + x_3 + x_4 = 2
$$ which has $2+2 \choose 2$ solutions. Therefore, my solutions was ${23+3 \choose 3}-{2+2 \choose 2} = 2600 - 6 = 2594$ . However, the book provides the following solution: Suppose Alice and Eve together get $k$ balls. This can be done in $k + 1$ ways. That leaves $30 - k$ balls for the other $2$ persons, but Lucky must get at least $7$ of these. So, there are $30 - k - 7$ additional balls to distribute to Lucky and the fourth person. This can be done in $(30 - k - 7) + 1$ ways. Hence, our answer is $\sum_{k=0}^{20}(k+1)(24-k)$ . That gives $2464$ . Forgive me if I didn't get some ""obvious"" detail, but what the heck does that summation mean? I was getting everything until it appeared, although I do see what the summands are. Could you point out where is the mistake in my solution? If you think the answer provided is simpler, please do explain. Thank you very much for any clarifications!","['combinatorics', 'discrete-mathematics']"
3816797,"Real spectrum for an ""almost self-adjoint"" Fredholm mapping","Consider the Sobolev space on the torus $\mathbb{T}=\mathbb{R}/2\pi \mathbb{Z}$ given by $H^m(\mathbb{T})$ is the $L^2$ -functions such that the Fourier coefficients satisfy the decay rate $\sum_{n\in\mathbb{Z}} |\hat{f}(n)|^2(1+n^2)^m<\infty$ (this is the square of the norm). Suppose $A: H^m(\mathbb{T})\rightarrow L^2(\mathbb{T})$ is bounded and linear, and there exists $B:L^2(\mathbb{T})\rightarrow H^m(\mathbb{T})$ such that $AB-I=E$ and $BA-I=F$ where $I$ is the identity and $E,F$ are compact. Suppose also that $(Af,g) = (f,Ag)$ whenever $f,g\in H^m(\mathbb{T})$ , where $(,)$ is the $L^2$ inner product. The goal is to show that $A-\lambda I: H^m(\mathbb{T})\rightarrow L^2(\mathbb{T})$ is invertible when $\lambda\notin \mathbb{R}$ . As a hint, it is given that $E^*$ maps $L^2$ into $H^m$ and $(Bf,g)=(f,Bg)$ for all $f,g\in L^2(\mathbb{T})$ . I was able to show that $(Ef,g)=(f,Fg)$ whenever $g\in H^m$ and $f\in L^2$ but I wasn't sure how to make use of this. I'm not sure how to mimic the proof of the spectral theorem for compact operators into this case.","['sobolev-spaces', 'fourier-analysis', 'functional-analysis']"
3816822,Smallest positive integer solution to $\tan{19x} = \frac{\cos{96} + \sin{96}}{\cos{96} - \sin{96}}$,"Find the smallest positive integer solution to $\tan{19x°} = \frac{\cos{96°} + \sin{96°}}{\cos{96°} - \sin{96°}}.$ The solution states to use $\sin(\theta) = \cos(90-\theta)$ and simplify the fraction to $-\cot{51}$ , then use some number theory to finish it off. My approach: We can use difference of squares on the RHS. \begin{align}
  &\frac{(\cos{96°} + \sin{96°})(\cos{96°} - \sin{96°})}{(\cos{96°} - \sin{96°})^2} = \\
  &\qquad\frac{\cos^2{96°}-\sin^2{96°}}{\cos^2{96°}+\sin^2{96°}-2\cos{96°}\sin{96°}} = \frac{\cos{192°}}{1-\sin{192°}}.
\end{align} However, finding the value for this is hard. I did note the resemblance of the half-angle tangent formula. It states that for any angle $\theta$ , $$\tan{\frac{\theta}{2}} = \frac{\sin{\theta}}{1+\cos{\theta}} = \frac{1-\cos{\theta}}{\sin{\theta}}.$$ My question is, can $\frac{\cos{192°}}{1-\sin{192°}}$ be used in any way to relate to the half-angle tangent formula? An added bonus is that we want to find $\tan{19x}$ , and having a tangent formula only helps. However, I was unable to find a relation. Problem from 1996 AIME Problem 10. The official solution is linked here .","['contest-math', 'algebra-precalculus', 'trigonometry']"
3816844,"Why the ""self-referential number"" function eventually fixes every point","Given an 8-digit decimal number $N$ , output a new 8-digit number $f(N)$ whose first digit is the number of zeroes in $N$ , the second the number of ones, ..., the seventh the number of sixes, and the eight the number of distinct digits of $N$ . The MoMath posted a puzzle that boils down to ""find the (unique) fixed point of $f$ "", and the solution given was to start with an arbitrary seed number $N$ and apply $f$ until one finds the fixed point. They comment on why there's no reason a priori this would work, and admit they're not sure why this works. Here are my related questions: Is there a way to see that $f$ has a unique fixed point? Is there a way to see that applying $f$ starting from any arbitrary seed $N$ , you get to the fixed point and don't get caught in a cycle when applying $f$ ? They remark that no matter what seed you pick, $f$ finds its fixed point relatively quickly (say within $10$ applications of $f$ ). Does anyone have a reason for why one should find the fixed point so soon? I don't have a good sense for how to bound how quickly this happens.","['puzzle', 'combinatorics']"
3816973,An easy Real Analysis Problem,"I found this Real Analysis problem from the MTRP book published by ISI for class 11. I . Suppose the function $f:[a,b]\to \Bbb R$ is differentiable on $(a,b)$ , where $b-a\geq 4$ . Prove that there is $x_0\in (a,b)$ such that $$f'(x_0)<1+\big(f(x_0)\big)^2.$$ My ideas of solving : When I first saw the problem I just tracked it from reverse and got idea that this problem involves $\arctan(x)$ . So I considered a function $g(x) = \arctan(f(x))$ . This get's the problem to be transformed to showing that for some real number $c$ we have $g(c) < c$ . For this I considered another function $h(x) = g(x) - x$ . But at this point Iam struck and have no idea where to go about.","['continuity', 'derivatives', 'real-analysis']"
3816974,How to transform quadratic function's output range to be negative for its negative input domain?,"The function $f(x) = x^2$ normally gives a range of output values that are all non-negative, $\mathbb{R}_{\geq 0}$ . How can the function be transformed so that when $x<0$ , the output $f(x)$ values are also negative, without resorting to a piece-wise function? In other words, flip the left hand side of the function across the x-axis, without altering the right hand side.","['algebra-precalculus', 'functions']"
3816978,Integral involving composition of Trig. and Inverse Trig. functions.,"For a minor class i am taking this year, I found the following integral in a problem set, and where i had no luck in evaluating it: $$\int \cos(2\cot^{-1}\sqrt{\frac{(1-x)}{(1+x)}})dx$$ . I proceeded as follows: ->first, let $x=\cos(2\theta)$ $\implies$ $dx=-2\sin2\theta d\theta$ so the integral becomes: $$\int \cos(2\cot^{-1}\sqrt{\frac{(1-\cos(2\theta))}{(1+\cos(2\theta))}}).-2\sin2\theta d\theta =\int \cos(2\cot^{-1}\sqrt{\frac{(\sin^2(\theta))}{(\cos^2\theta)}}).-2\sin2\theta d\theta\\=\int \cos(2\cot^{-1}(\tan\theta).-2\sin2\theta d\theta=\int \cos(\frac{2}{\theta}).-2\sin2\theta d\theta$$ After this i am stuck. How do i proceed? I cant seem to find any mistakes with my substitution and the succeeding lines. Where did i make a mistake(if any) though? Can i still use this substitution?. I tried looking this up for some guidance and in this online integral solver i found, https://www.integral-calculator.com/ , they used some other method to get to the right answer which i didn't quite understand(refer to image). I wanted to keep using a substitution method if possible.","['calculus', 'trigonometry']"
3817079,"What are the known examples of closed, connected and simply connected manifolds of positive Euler characteristic?","What are the known examples of closed, connected and simply connected manifolds of positive Euler characteristic in dimension $n$ ? Is there any complete list? I think they are rare. The examples that I know are $\Bbb S^{2n}$ , $\mathbb{CP}^n$ , $\mathbb{HP}^n$ and $\mathbb{CaP}^2$ .","['manifolds', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
3817106,Which solution is correct for $\tan x+\tan2x+\tan3x=\tan x\tan2x\tan3x$?,"Equation to solve is: $$\tan x+\tan2x+\tan3x=\tan x\tan2x\tan3x$$ $\text {Solution 1:}$ $$\tan x+\tan 2x=\tan x \tan 2x\tan 3x-\tan 3x$$ $$\tan x+\tan 2x=\tan 3x(\tan x \tan 2x-1)$$ $$\tan x+\tan 2x=-\tan 3x(1-\tan x \tan 2x)$$ $$\dfrac{\tan x+\tan 2x} {(1-\tan x\tan 2x)}= -\tan 3x$$ $$\tan 3x=-\tan 3x$$ $$2\tan 3x=0$$ $$\tan 3x=0$$ $$3x=n\pi, \text {where n is an integer}$$ $$x=\frac{n\pi}{3}$$ $\text {Solution 2:}$ $$\tan x+\tan 2x=\tan x\tan 2x \tan 3x-\tan 3x$$ $$\tan x+\tan 2x=\tan 3x(\tan x \tan 2x-1)$$ $$\tan x+\tan 2x=-\tan 3x(1-\tan x \tan 2x)$$ $$ \frac{\tan x+\tan 2x} {(1-\tan x \tan 2x)}= -\tan 3x$$ $$\tan 3x=-\tan 3x$$ $$\tan 3x=\tan(-3x)$$ $$3x=n\pi + (-3x)$$ $$6x=n\pi, \text  {where n is an integer}$$ $$x=\frac{n\pi}{6}$$ Also, $\dfrac{n\pi}{3} \subset \dfrac{n\pi}{6}, \text {i.e. Solution 1 is contained in Solution 2.}$ But, the extra solution provided $\text {Solution 2}$ (which are not present in $\text {Solution 1}$ ) are of no use, as the original equation is undefined at those point, for eg, $\frac{\pi}{6}$ and $\tan$ function is not defined at $\frac{3\pi}{6}$ . So, $\text {Solution 1}$ can give all the solutions for the equation. My book has $\text {Solution 2}$ as the answer. Which is the correct answer?","['algebra-precalculus', 'trigonometry']"
3817198,An interesting conjugate-type fraction,"I was playing around with numbers and I appear to have discovered a very fascinating fraction: For all $n>0$ , $$\cfrac{\bigg(1-\cfrac 1{3n+2}\bigg)\bigg(1-\cfrac 1{6n+1}\bigg)}{\bigg(1-\cfrac 1{3n+1}\bigg)\bigg(1-\cfrac 1{6n+5}\bigg)}=\cfrac{\bigg(1+\cfrac 1{3n+2}\bigg)\bigg(1+\cfrac 1{6n+1}\bigg)}{\bigg(1+\cfrac 1{3n+1}\bigg)\bigg(1+\cfrac 1{6n+5}\bigg)}$$ How did I derive this? I was looking at my answer here and decided to play around with those mixed fractions (by playing around, I mean multiplying some, dividing others, merely because I was bored). It was greatly to my surprise that I found this (by sheer accident too, due to my interest in conjugates). Anybody know how to derive this rigorously, and if other such fractions exist? Thanks.","['number-theory', 'elementary-number-theory', 'fractions', 'algebra-precalculus', 'rational-functions']"
3817256,Compute the number of sequence permutations,"I would like to compute the no. possible permutations for a sub-set of objects. Consider the set of objects: $$
X = \{X_1,X_2,X_3,....,X_N\}
$$ Question: What is the number of ways that I can pick a sub-sequence from $X$ of length $M \leq N$ , such that: Each object, $X_i$ , may only appear once in the sub-sequence Order matters, i.e. $S_1 = \{X_1, X_2, X_3\}$ is not the same as $S_2 = \{X_2, X_1, X_3\}$ - i.e. these two examples count as 2 Rotational symmetry, i.e. $S_1 = \{X_1, X_2, X_3\}$ is the same as $S_2 = \{X_2, X_3, X_1\}$ - i.e. these two examples count as 1. The reason for this is that $S_2$ appears as a sub-sequence of $S_1$ if $S_1$ is repeated $\{X_1, \mathbf{X_2, X_3\} \{X_1}, X_2, X_3\}$ . I have been able to come up with a formula for the problem accounting for 1) and 2). The number of combinations is the ways to choose M items from N items set. Without repetition and with order, the formula is: $\frac{N!}{(N-M)!}$ . However, I do not know how to extend the formula to account for 3).","['permutations', 'combinatorics']"
3817374,A Family of Limits Leading to an Interesting Function,"A while back I got very interested in limits of the form $$
\lim_{n\to\infty} (2A)^n \left (A-\underbrace{\sqrt{a+\sqrt{a+\ldots\sqrt{a+z}}}}_{n\textrm{ radicals}}  \right )=f_a^{-1}(z)
$$ Where $A$ is the positive solution of $A^2=a+A$ . As notated, the limit can be used to define a function with some rather interesting properties. I explore some of them here . In particular, the function becomes trigonometric in the case $a=2$ , and related to the golden ratio in the case $a=1$ . These values also correspond to notable properties of the function. In particular, the roots of the function have some very interesting behavior that seems to become fractal. Does anyone know if this limit has been studied before? It seems to be related to the Mandelbrot set, inasmuch as the iterated radical is the inverse operation of the Mandelbrot iteration, though I haven't looked into this connection very much. Any information about other research on this subject or related fields of study would be greatly appreciated.","['limits', 'analytic-continuation', 'fractals', 'reference-request']"
3817376,Argument in Lemma 3.2.4 of Karatzas & Shreve - approximating bounded progressively measurable process by a continuous process,"The following is part of an argument for Lemma 3.2.4 from Karatzas and Shreve's Brownian Motion and Stochastic Calculus. Let $X$ be a bounded, progressively measurable, $\mathscr{F}_t$ -adapted process.
For each fixed $T>0$ , we wish to approximate $X$ by by a bounded continuous process. Consider the continuous, progressively measurable processes $$F_t(\omega) = \int_0^{t \wedge T} X_s(\omega)ds; \; \tilde{X}_t^{(m)} = m [F_t(\omega)-F_{(t-(1/m))^+}(\omega)]; \; m \ge 1,$$ for $t \ge 0, \omega \in \Omega$ . From a result earlier in the proof, there exists for each $m \ge 1,$ a sequence of simple processes $\{\tilde{X}^{(m,n)}\}_{n=1}^\infty$ such that $\lim_{n\to \infty} E\int_0^T |\tilde{X}_t^{(m,n)} - \tilde{X}_t^{(m)}|^2 dt = 0.$ Let us consider the $\mathscr{B}([0,T]) \otimes \mathscr{F}_T$ -measurable product set $$A = \{(t,\omega) \in [0,T] \times \Omega; \; \lim_{m \to \infty} \tilde{X}_t^m (\omega) \neq X_t(\omega)\}.$$ For each $\omega \in \Omega$ , the cross section $A_\omega = \{t \in [0,T];(t, \omega) \in A\}$ is $\mathscr{B}([0,T])$ -measurable and, according to the fundamental theorem of calculus, has measure zero. The bounded convergence theorem now gives $\lim_{m\to \infty} E\int_0^T |\tilde{X}_t^{(m)} - X_t|^2 dt = 0$ . Question: In the definition of $\tilde{X}_t^{(m)}$ above, why do we consider $F_{(t-(1/m))^+}$ instead of $F_{(t-(1/m))}$ ? Since $F$ is continuous, aren't they the same? Next, by the fundamental theorem of calculus, doesn't $A_\omega$ equal the whole $[0,T]$ ? Finally, to apply the bounded convergence theorem, we need $\tilde{X}^{(m)}$ to be a bounded sequence. How do we ensure this from the definition? I have been struggling to understand these points for some time and I would greatly appreciate some help solving these questions.","['measure-theory', 'analysis', 'real-analysis', 'calculus', 'probability-theory']"
3817380,"Two monotone functions have the same derivative when both differentiable, and the same discontinuities. Must they differ by a constant?","Suppose $f: [0,1]\to [0,1]$ and $g: [0,1]\to [0,1]$ both satisfy the following: (1) They are both weakly increasing. (2) They are discontinuous at the same set of points. Moreover, if $f$ and $g$ are discontinuous at $x$ , then $$\lim_{z\rightarrow x^{+}}f(z)-\lim_{z\rightarrow x^{-}}f(z)=\lim_{z\rightarrow x^{+}}g(z)-\lim_{z\rightarrow x^{-}}g(z)$$ (3) If $f$ and $g$ are both differentiable at $x$ , then $f'(x)=g'(x)$ . Is it true that for any $x$ and $z$ where $f$ and $g$ are continuous, $f(x)-g(x)=f(z)-g(z)$ ?","['measure-theory', 'ordinary-differential-equations', 'real-analysis']"
3817439,What is the difference between total variation and arc length?,"Let $f:[a,b] \rightarrow \mathbb R$ be $C^1$ . Is the length of $\{f(x): x \in [a,b] \}$ and the total variation of $f$ the same thing ? The definition are extremely similar to each others: The total variation of a real-valued (or more generally complex-valued) function $f$ , defined on an interval $[a, b] \subset \mathbb{R}$ is the quantity $$
V_{b}^{a}(f)=\sup _{\mathcal{P}} \sum_{i=0}^{n_{P}-1}\left|f\left(x_{i+1}\right)-f\left(x_{i}\right)\right|
$$ where the supremum runs over the set of all partitions $\mathcal{P}=\left\{P=\left\{x_{0}, \ldots, x_{n_{P}}\right\} \mid P\right.$ is a partition of $\left.[a, b]\right\}$ of the given interval. Let $f:[a, b] \rightarrow \mathbb{R}^{n}$ be a continuously differentiable function. The length of the curve defined by $f$ can be defined as the limit of the sum of line segment lengths for a regular partition of $[a, b]$ as the number of segments approaches infinity. This means $$
L(f)=\lim _{N \rightarrow \infty} \sum_{i=1}^{N}\left|f\left(t_{i}\right)-f\left(t_{i-1}\right)\right|
$$ where $t_{i}=a+i(b-a) / N=a+i \Delta t$ for $i=0,1, \ldots, N .$ This defintiton is equivalent to the standard definition of arc length as an integral: $$
\lim _{N \rightarrow \infty} \sum_{i=1}^{N}\left|f\left(t_{i}\right)-f\left(t_{i-1}\right)\right|=\lim _{N \rightarrow \infty} \sum_{i=1}^{N}\left|\frac{f\left(t_{i}\right)-f\left(t_{i-1}\right)}{\Delta t}\right| \Delta t=\int_{a}^{b}\left|f^{\prime}(t)\right| d t
$$",['real-analysis']
3817461,"Given a coupling $\pi(\mu,\nu)$, show that $E_\mu f- E_\nu f= E_\pi [f(X) - f(Y)]$","In the lecture notes by for High-Dimensional Probability by Handel, the following is affirmed: Let $\mu$ and $\nu$ be probability measures, then $$\mathcal C(\mu,\nu) = \{ \text{Law} (X,Y) : X\sim \mu, Y\sim \nu \}
$$ Therefore, any $\pi \in \mathcal C (\mu,\nu)$ is called a coupling of $\mu$ and $\nu$ . Hence, the author claims that $$E_\mu f- E_\nu f= E_\pi [f(X) - f(Y)]$$ my question is how to prove the claim above. At first it seemed easy, but I’m getting confused on how to prove this rigorously. I imagine that there is some kind of abuse of notation, since, for example, $E_\mu f = \int_\mathbb R f(w) d\mu$ and $E_\pi f(X) = \int_{\mathbb R^2} f(X((w,z)))d\pi$ . But since $X:\Omega \rightarrow \mathbb R$ , then $X((w,z))$ is ill defined.","['measure-theory', 'coupling', 'probability-theory', 'marginal-probability']"
3817477,"If $\mathbb E_{\mathbb P} \vert f(X,Y)\vert <\infty$, is also $\mathbb E_{\mathbb P_1\times \mathbb P_2} \vert f(X,Y)\vert <\infty$?","Let $(E_1,\mathcal E_1)$ and $(E_2,\mathcal E_2)$ be two measurable spaces. Let $(E=E_1\times E_2,\mathcal E=\mathcal E_1\times \mathcal E_2)$ . Let $\mathbb P$ a valid probability distribution on $(E,\mathcal E)$ . Define the marginal distributions $\mathbb P_1(A)=\mathbb P (A\times E_2)$ and $\mathbb P_2(A)=\mathbb P (E_1\times A)$ , and finally denote the product distribution by $\mathbb P_1\times \mathbb P_2$ . Finally assume that we know \begin{align*} \mathbb E_{\mathbb P} \vert f(X,Y)\vert <\infty.\end{align*} My question is if it also holds that \begin{align*} \mathbb E_{\mathbb P_1\times \mathbb P_2} \vert f(X,Y)\vert <\infty?\end{align*} In particular, if it is false I hope to see a counterexample and I'd also be interested to know if it is true in the case $f(X,Y)=g(X)h(Y)$ for some functions $g$ and $h$ .","['integration', 'measure-theory', 'probability-distributions', 'real-analysis', 'probability-theory']"
3817593,What is an eigenmatrix and what is its use?,"I stumbled upon a paper using the term eigenmatrix which I never heard of before. Sadly I found little to none literature to it. Even my books at home don't know the term. I believe that an eigenmatrix $E$ of some $(n\times n)$ matrix $A$ with eigenvalues $\lambda_1,\ldots,\lambda_n$ is of form: $E=\begin{align}\left( \begin{array}{rrrr}
\lambda_1 & 0 & 0 \\
0 & \ddots & 0  \\
0 & 0 & \lambda_n 
\end{array}\right)\end{align} $ where each $\lambda_i$ is represented as often as its multiplicity. If my definition up to this point is correct, my questions are: What is this matrix used for? How does it look like if $A$ doesn't have a full set of (real)
eigenvalues? Do eigenvectors play any role, if so what? (I ask this primarily because the paper I read states that "" $E$ is an eigenmatrix of $A$ corresponding to an eigenvector."") How can one calculate an eigenmatrix of some square matrix $A$ ? If my definition wasn't correct, feel free to tell me what an eigenmatrix is.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3817607,Asymptotic cut in half of a $d$-dimensional ball,"Denote by $\Delta_{d-1}=\{\mathbf{x}\in\mathbb{R}^d:x_1+x_2+\cdots+x_d=1\}$ the $d-1$ simplex and define $R=\{\mathbf{x}\in\mathbb{R}^d: d^3\|\mathbf{x}\|^2_2 - \|1/\mathbf{x}\|_1 \geq 0\}$ where $1/\mathbf{x} = (1/x_1, 1/x_2,...,1/x_d)$ . Is it possible to show that $$\lim_{\varepsilon \to 0} \frac{\textrm{Vol}[\Delta_{d-1} \cap R\cap B_{\mathbb{R}^d}(\mathbf{1}/d, \varepsilon)]}{\textrm{Vol}[\Delta_{d-1} \cap R^c\cap B_{\mathbb{R}^d}(\mathbf{1}/d, \varepsilon)]} = 1 $$ i.e. that, restricted to the hyperplane $\Delta_{d-1}$ , the region $R$ asymptotically splits the ball centered at $\mathbf{1}/d = (1/d, 1/d,..., 1/d)$ in two equal halves ?","['limits', 'volume', 'differential-geometry']"
3817652,Eigenvalue bound for quadratic maximization with linear constraint,"This builds on my earlier questions here and here . Let $B$ be a symmetric positive definite matrix in $\mathbb{R}^{k\times k}$ and consider the problem $$\begin{array}{ll} \text{maximize} & x^\top B x\\ \text{subject to} & \|x\|=1 \\ & b^\top x = a\end{array}$$ where $b$ is an arbitrary unit vector and $a > 0$ is a small positive number. Let $$\lambda_1 > \lambda_2 \geq \cdots \geq \lambda_k > 0$$ be the eigenvalues of $B$ with corresponding eigenvectors $z_1,...,z_k$ . I conjecture that the optimal value of the problem is bounded below by $a^2 \lambda_1 + \left(1-a^2\right)\lambda_2$ , at least if $a$ is small enough. To motivate this conjecture, let us consider two special cases. First, suppose that $a= 0$ . Then, as was explained to me in one of my previous posts, the optimal value is between $\lambda_1$ and $\lambda_2$ by the Courant-Fischer theorem. Thus, $\lambda_2$ is a lower bound, and it also coincides with my conjectured lower bound in this special case. Second, let $a > 0$ but suppose that $b = z_i$ for some $i = 1,...,k$ . Any feasible $x$ can be written as $$x = ab + \sqrt{1-a^2} \cdot \hat{b}$$ where $\hat{b}\perp b$ . If $b = z_1$ , I can take $\hat{b} = z_2$ , and if $b = z_i$ for $i \neq 1$ , I can take $\hat{b} = z_1$ . Either way, the objective value of $x$ is bounded below by $a^2 \lambda_1 + \left(1-a^2\right)\lambda_2$ as long as $a$ is small enough (note that this requires $\lambda_1 > \lambda_2$ ). The difficulty is showing that it holds in the case where $b$ is not one of the eigenvectors of $B$ (perhaps with additional restrictions on how large $a$ can be). My intuition is that, if $b$ is not required to be orthogonal to $x$ , but only ""almost"" orthogonal (meaning that $a$ may be required to be sufficiently small), you should be able to go a bit further in the direction of the principal eigenvector than in the case where $a = 0$ . Here is the most up-to-date work on this problem. In the answer below, it was found that the optimal value $v$ of the problem is a generalized eigenvalue of the system $$PBx = vPx,$$ which in turn was derived from the system $$PBPy + aPBb = v Py.$$ Any pair $\left(y,v\right)$ that solves these equations then leads to a feasible $x = ab+Py$ , with $v$ being the objective value. We can write $$\left(vI - PB\right)Py = aPBb.$$ Note that, for any $v$ that is not an eigenvalue of $PB$ , the matrix $vI-PB$ is invertible, whence $$Py = a\left(vI-PB\right)^{-1}PBb.$$ The normalization $x^\top x = 1$ then becomes $y^\top P y = 1-a^2$ , leading to the equation $$\frac{1-a^2}{a^2} = b^\top BP\left(vI-PB\right)^{-2} PBb.$$ The largest root of this equation is the optimal value of the problem. Perhaps, as suggested, it can be found numerically.","['qcqp', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'positive-definite']"
3817653,How fast are you moving on the earth at a given latitude?,"The radius of the earth is about $3959\,\mathrm{mi}$ , so the earth is rotating at about $$\frac{1}{24\,\mathrm{hours}}\times 2\pi\times 3959 \,\mathrm{mi} \approx 1036\,\mathrm{mph}\,$$ and so someone standing on the equator is moving that fast too. But suppose you're standing on the earth at a latitude to $\theta^{\,\circ}$ . How fast are you moving then?","['algebra-precalculus', 'spherical-coordinates', 'trigonometry']"
3817674,Converting integration dV in spherical coordinates for volume but not for surface?,"When calculating the volume of a spherical solid, i.e. a triple integral over angles and radius, the standard $dx\,dy\,dz$ gets converted into $f(x,y,z)r^2\sin\Phi \,d\Phi \,d\Theta \,dr$ . However, it seems that when we calculate a spherical surface integral, that is not the case, and we instead just have $f(x,y,z)\left|\frac{\delta r}{\delta \Phi}\times\frac{\delta r}{\delta \Theta}\right|\,d\Phi \,d\Theta$ . Why is that? I'm just confused about when I should ""convert"" when parametrizing a surface and when not.","['integration', 'multivariable-calculus', 'spherical-coordinates', 'multiple-integral']"
3817703,Mixed directional derivative of second order and the Hessian matrix,"First order directional derivative in 2 dimensional $xy$ -coorinate system is $$D_{\bf{u}}=u_1\frac{\partial}{\partial x}+u_2\frac{\partial}{\partial y}$$ I thought to go further and analyse second order directional derivatives.
The formula for general second order mixed directional derivative is $$ D^2_{\bf{u}\bf{v}}=D_{\bf{v}}\left[u_1\frac{\partial}{\partial x}+u_2\frac{\partial}{\partial y}\right]=u_1D_{\bf{v}}\frac{\partial}{\partial x}+u_2D_{\bf{v}}\frac{\partial}{\partial y}=$$ $$=u_1(v_1\frac{\partial^2}{\partial x^2}+v_2\frac{\partial^2}{\partial y\partial x})+u_2(v_1\frac{\partial^2}{\partial x\partial y}+v_2\frac{\partial^2}{\partial y^2})=$$ $$=u_1v_1\frac{\partial^2}{\partial x^2}+u_1v_2\frac{\partial^2}{\partial y\partial x}+u_2v_1\frac{\partial^2}{\partial x\partial y}+u_2v_2\frac{\partial^2}{\partial y^2}$$ I see the formula we get has definitely something to do with the following two matrices: $$U=\left[\matrix{u_1v_1 && u_1v_2\\ u_2v_1 && u_2v_2}\right]$$ $$H=\left[\matrix{\frac{\partial^2}{\partial x^2} && \frac{\partial^2}{\partial x\partial y} \\ \frac{\partial^2}{\partial y\partial x} && \frac{\partial^2}{\partial y^2}}\right]$$ Where the second of them is known as the Hessian operator in the 2-dimensional $xy$ -plane. Are here any matrix algebra masters, who will find how to express $D_{\bf{u}\bf{v}}$ in terms of $U$ and $H$ ? If we had defined an operation like $$\left[\matrix{a_{11} && a_{12} \\ a_{21} && a_{22}}\right]* \left[\matrix{b_{11} && b_{12} \\ b_{21} && b_{22}}\right]=a_{11}b_{11}+a_{12}b_{12}+a_{21}b_{21}+a_{22}b_{22}$$ $$A*B=\sum_{i, j} A_{ij}B_{ij},$$ then the formula for mixed directional 2nd order derivative would look like $$D_{\bf{u}\bf{v}}=U^T*H$$ Any ideas how to express the $*$ operation using standard operations on matrices?","['partial-derivative', 'multivariable-calculus', 'hessian-matrix']"
3817731,Proof that $\mathbb{P}^1$ is not affine.,"I am confused by the last line in the proof that $\mathbb{P}^1$ is not affine, as presented in Ravi Vakil's algebraic geometry notes. First, he computes the ring of global sections. It turns out that $\Gamma(\mathbb{P}^1,\mathcal{O}_{\mathbb{P}^1})=k$ . This is all fine. What confuses me is the next line. He says: ""If $\mathbb{P}^1$ were affine, then it would be $\operatorname{Spec}\Gamma(\mathbb{P}^1,\mathcal{O}_{\mathbb{P}^1})=\operatorname{Spec}k$ , i.e., one point. But it isn't -- it has lots of point."" I don't know what to make of that sentence. In the second equality, isn't he just taking $\operatorname{Spec}$ of both sides of the first equality? If so, what does this have anything to do with assuming $\mathbb{P}^1$ is affine? Further, why do we know $\operatorname{Spec}\Gamma(\mathbb{P}^1,\mathcal{O}_{\mathbb{P}^1})$ should have ""lots of points""? In short, what am I missing here? Edit: The question linked does not answer my question. I am asking about a specific line in this proof given by Vakil. The linked questions only asks why $\mathbb{P}^1$ is not affine in general. Further, none of the given answers there address my concern. The chosen answer uses dimension theory, which is clearly not what Vakil had in mind since dimension theory hasn't been discussed yet","['affine-schemes', 'algebraic-geometry', 'schemes', 'commutative-algebra']"
3817747,System of generators for $\Bbb Z_9\times \Bbb Z_{18}$.,"Is $\{(4,3),(3,5)\}$ a system of generators for $\Bbb Z_9\times \Bbb Z_{18}$ ? I tried to generate the elements using the straightforward method $(4,3), (8,6), (3, 9)$ ... but it takes too long. Can I use a faster method?","['finitely-generated', 'group-theory', 'finite-groups', 'discrete-mathematics']"
3817760,How can you show that the center of mass of a triangle lies on the medians?,"In geometry class, it is usually first shown that the medians of a triangle intersect at a single point.  Then is is explained that this point is called the centroid and that it is the balance point and center of mass of the triangle.  Why is that the case? This is the best explanation I could think of.  I hope someone can come up with something better. Choose one of the sides of the triangle. Construct a thin rectangle with one side coinciding with the side of the triangle and extending into it.  The center of mass of this rectangle is near the midpoint of the side of the triangle.  Continue constructing thin rectangles, with each one on top of the previous one and having having the lower side meet the two other sides of the triangle. In each case the centroid of the rectangle is near a point on the median.  Making the rectangles thinner, in the limit all the centroids are on the median, and therefore the center of mass of the triangle must lie on the median. This follows because the center of mass of the combination of two regions lies on the segment joining the centroids of the two regions.",['geometry']
3817786,Verification that the Borel $\sigma$-algebra on $\mathbb{R}$ is not atomic.,"Let $X$ be a set, and let $\mathcal{A} = (A_n)_{n=1}^{\infty}$ be a sequence of disjoint, nonempty
subsets whose union is $X$ . Then the set $\mathcal{M}$ of
all finite or countable unions of elements of $\mathcal{A}$ together with $\emptyset$ is a $\sigma$ -algebra. A $\sigma$ -algebra of this form is called atomic. Then the Borel $\sigma$ -algebra $\mathcal{B}_{\mathbb{R}}$ on $\mathbb{R}$ is not atomic. $\text{Proof.}$ Suppose for sake of contradiction that $\mathcal{B}_{\mathbb{R}}$ is the collection of all finite or countable unions of sets in $\mathcal{A} = (A_n)_{n=1}^{\infty} $ , where the $A_i$ are mutually disjoint, non-empty subsets of $\mathbb{R}$ whose union is $\mathbb{R}$ . In particular it follows from this that each set in $\mathcal{A}$ is itself a Borel-set in $\mathbb{R}$ . Now let $\mathcal{U}= \left\{\left\{p\right\}:0<p<1\right\}$ . Then each singleton in $\mathcal{U}$ is a Borel-set, being closed with respect to the standard topology on $\mathbb{R}$ . Hence, for each $p\in (0,1)$ , it follows that $\left\{p \right\}$ is a union of a finite or countable sub-collection of the $A_i$ . But since the $A_i$ are each nonempty, $\left\{p\right\}$ cannot be a union of more than one $A_i$ , since otherwise $\left\{p\right\}$ would have more than one element. Thus, for each $p\in (0,1)$ , we can injectively associate a set $A_i\in\mathcal{A}$ with $\left\{p\right\} = A_i$ . But this is absurd because $\mathcal{U}$ is uncountable and $\mathcal{A}$ is countable. I'd appreciate if anyone here could check for the accuracy of the above proof. Thanks.","['measure-theory', 'real-analysis']"
3817805,"Is there a numeral system for real numbers that is always unique, but still has the usual convenient properties?","For each integer $b\ge 2,$ we know that representations of real numbers are usually unique in the base- $b$ positional notation . The only time that uniqueness fails is if the form ends in a tail of $0$ 's or a tail of $(b-1)$ 's, in which case it is easy to convert between these dual representations. However, the fact that multiple representations are ever possible forces the mathematician to be additionally careful in writing some proofs. For example, in the standard application of Cantor's diagonal argument to show that the continuum is uncountable, one has to be careful to mention that we  are constructing the rows using only terminating forms when there are dual representations and that the (anti-)diagonal element constructed is not somehow a dual form of one of the those terminating forms. Question: Can a numeral system be constructed which represents all real numbers uniquely and only real numbers while still admitting some or all of the following convenient properties of the ordinary positional notation, and perhaps additional nice properties of its own: Being exponentially more efficient than unary , meaning the number of distinct integers represented by at most a certain number of digits is something like the number of distinct symbols in the system to the power of the number of digits. Admitting convenient pen-and-paper and computer algorithms for performing the arithmetic operations of addition, subtraction, multiplication, division and exponentiation, at least when integers or rationals are involved. Allowing for the existence of some convenient divisibility rules of integers, though not necessarily the same ones as those admitted by base- $b.$ Having predictable (eg. periodic/cyclic) patterns in the representations of some large classes of real numbers, like the rationals. If these properties are not possible to fulfill, I would still be interested in a system where there is uniqueness at the cost of losing these features. References to non-standard numeral systems that aim for such a goal (or perhaps other goals of convenience) would be appreciated.","['number-theory', 'number-systems', 'analysis', 'decimal-expansion']"
3817830,Question on Fundamental Theorem of Calculus,"In my answer to this question ( When we evaluate an indefinite integral of one variable, what area does this yield? ) I wrote the following: Now, if we divide both sides by $h$ we obtain the following: $$\lim_{h\to0}\frac{F(x+h)-F(x)}{h}=\lim_{h\to0}\frac{f(x+h)+f(x)}{2}$$ but we can see that the expression on the left hand side is the definition of the derivative, $F'(x)$ , for $F(x)$ , our area accumulator function. So we can write $$F'(x)=\lim_{h\to0}\frac{f(x+h)+f(x)}{2}=\frac{f(x)+f(x)}{2}=\frac{2f(x)}{2}=f(x)$$ My question is, on the right hand side of the equality we basically say that $$\lim_{h\to0}f(x+h)=f(x)$$ However, why don't we say the same also on the left hand side; ie why don't we write (instead of $F'(x)$ ) $$\lim_{h\to0}\frac{F(x+h)-F(x)}{h}=\lim_{h\to0}\frac{F(x)-F(x)}{h}=\lim_{h\to0}\frac{0}{h}=\infty$$ ie why doesn't the left hand side become meaningless if we are willing to write on the right hand side $\lim_{h\to0}f(x+h)=f(x)$ ? Thank you for your help. If my derivation of the Fundamental Theorem of calculus is mistaken please tell me and help me correct it :)","['integration', 'fake-proofs', 'calculus', 'limits', 'derivatives']"
3817842,"Suppose $[a],[b],[c]\in\Bbb Z_n$, and are residue classes which satisfy $[a][b]=[1]$ and $[a][c]=[1]$. Prove $[b]=[c]$.","I have a partial solution to this problem, but I am now stuck and unsure where to go to. I don't want a solution to the problem, just guidance on where to go, or if what I'm doing is wrong. Proof: As $[a][b]=[1]$ , we have $[ab]=[1]$ . Thus $ab\equiv 1\pmod n$ , and similarly as $[a][c]=[1]$ , we have $[ac]=[1]$ . Hence we obtain $ac\equiv 1 \pmod n$ . From this we see that $n\mid ab-1$ and $n\mid ac-1$ . By definition of division, we have that $ab-1 =nt$ , and $ac-1 =np$ where $t,p\in\Bbb Z$ . Thus, $ac-np=1=ab-nt$ . Thus, by factoring, we obtain $a(c-b)=n(p-t)$ where $p-t\in\Bbb Z$ . Thus we have that $n\mid a(c-b)$ . At this point I'm stuck, and I feel as if the problem is close to being done/nearly done. Any guidance is appreciated, thank you.","['elementary-number-theory', 'abstract-algebra', 'solution-verification', 'modular-arithmetic']"
