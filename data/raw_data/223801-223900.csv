question_id,title,body,tags
4604150,Are there hints of the exceptional nature of $S_6$ in its representation theory?,$S_6$ is the unique symmetric group to have non-trivial outer automorphisms. It also has a triple cover. Is it possible to see any hints of this exceptional behavior in its representations?,"['group-theory', 'abstract-algebra', 'representation-theory']"
4604157,"Problem 12.7 in ""A Probabilistic Theory of Pattern Recognition""","I'm trying to understand the Problem 12.7 in the book A Probabilistic Theory of Pattern Recognition . This is part of the proof of Theorem 12.7. For the sake of clarity, I'll introduce a few notations before presenting the problem: let $Z_1, Z_2, \ldots, Z_n, Z_1', Z_2', \ldots, Z_n'$ be iid random variables; for any measurable set $A$ in some class of sets $\mathcal{A}$ , \begin{aligned}
\nu(A) & = \mathbf{P}\left\{Z \in A\right\}, \\
\nu_n(A) & = \frac{1}{n} \sum_{i = 1}^{n} I_{\left\{Z_i \in A\right\}}, \\
\nu_n'(A) & = \frac{1}{n} \sum_{i = 1}^{n} I_{\left\{Z_i' \in A\right\}}.
\end{aligned} The problem, as in the book, is presented below. PROBLEM 12.7. Prove that $$
\mathbf{P}\left\{\sup _{A: v_n(A)=0}\left|v_n(A)-v(A)\right|>\epsilon\right\} \leq 2 \mathbf{P}\left\{\sup _{A: v_n(A)=0}\left|v_n(A)-v_n^{\prime}(A)\right|>\frac{\epsilon}{2}\right\}
$$ holds if $n \epsilon>2$ . This inequality is needed to complete the proof of Theorem 12.7. HINT: Proceed as in the proof of Theorem 12.5. Introduce $A^*$ with $v_n\left(A^*\right)=0$ and justify the validity of the steps of the following chain of inequalities: $$
\begin{aligned}
\mathbf{P} & \left\{\sup _{A: v_n(A)=0}\left|v_n(A)-v_n^{\prime}(A)\right|>\epsilon / 2\right\} \\
& \geq \mathbf{E}\left\{I_{\left\{v\left(A^*\right)>\epsilon\right\}} \mathbf{P}\left\{v_n^{\prime}\left(A^*\right) \geq \frac{\epsilon}{2} \mid Z_1, \ldots, Z_n\right\}\right\} \\
& \geq \mathbf{P}\left\{B(n, \epsilon)>\frac{n \epsilon}{2}\right\} \mathbf{P}\left\{\left|v_n\left(A^*\right)-v\left(A^*\right)\right|>\epsilon\right\},
\end{aligned}
$$ where $B(n, \epsilon)$ is a binomial random variable with parameters $n$ and $\epsilon$ . Finish the proof by showing that the probability on the right-hand side is greater than or equal to $1 / 2$ if $n \epsilon>2$ . (Under the slightly more restrictive condition $n \epsilon>8$ , this follows from Chebyshev's inequality.) Based on the hint above, and the proof of Theorem 12.4 (which is related to Theorem 12.5), I started my solution: Let $A^* \in \mathcal{A}$ be a set for which $$\nu\left(A^*\right)>\varepsilon \text{ and } \nu_n\left(A^*\right)=0.$$ If such a set does not exist, let $A^*$ be a fixed set that guarantees $\nu_n\left(A^*\right)=0$ . Then, \begin{aligned}
\mathbf{P}&\left(\sup _{A: \nu_n(A)= 0} \left|\nu_n(A)-\nu_n^{\prime}(A)\right|>\varepsilon / 2\right) \\
& \geq \mathbf{P}\left(\left|\nu_n\left(A^*\right)-\nu_n^{\prime}\left(A^*\right)\right|>\varepsilon / 2\right)=\mathbf{P}\left(\nu_n^{\prime}(A^*) >\varepsilon / 2\right) \\
& \geq \mathbf{P}\left(\nu\left(A^*\right)>\varepsilon,\nu_n^{\prime}\left(A^*\right)>\varepsilon / 2\right) \\
& =E\left\{I_{\left\{v\left(A^{*}\right)>\varepsilon\right\}} \mathbf{P}\left[v_n^{\prime}\left(A^*\right)>\varepsilon/2 \mid Z_1, \ldots, Z_n\right]\right\} \\
& \stackrel{(* *)}{=} \mathbf{P}\left[\nu_n^{\prime}\left(A^*\right)>\varepsilon / 2\right] E\left[I_{\left\{\nu\left(A^*\right)>\varepsilon\right\}}\right] \\
& =\mathbf{P}\left(\sum_{i=1}^n I_{\left\{Z_i^{\prime} \in A^*\right\}}> n \varepsilon / 2\right) \mathbf{P}\left(\nu\left(A^*\right)>\varepsilon\right) \\
&\equiv \mathbf{P}\left(B(n, \nu(A^*))> n \varepsilon / 2\right) \mathbf{P}\left(\left|\nu_n(A^*) - \nu\left(A^*\right)\right|>\varepsilon\right),
\end{aligned} where in $(* *)$ I used the fact that $Z_i$ s and $Z_i'$ s are independent. Am I doing something wrong? More specifically, my questions are: I don't know how they came to a $B(n, \epsilon)$ in the last inequality of the HINT. Looking at other proofs in the book (e.g. Theorem 12.4), I think it is necessary to show that, if $n\varepsilon > 2$ , then $\mathbf{P}\left\{B(n, \epsilon)>\frac{n \epsilon}{2}\right\} \geq \frac{1}{2}$ . The result makes sense, but don't know to show this. Furthermore, I don't know how to use Chebyshev's inequality, since it provides upper bounds for this kind of probability and we need a lower bound. Finally, based on my solution, I think that I can do $\mathbf{P}\left(B(n, \nu(A^*))> n \varepsilon / 2\right) \geq \mathbf{P}\left(B(n, \varepsilon)> n \varepsilon / 2\right)$ , but I don't know how to show this either.","['statistics', 'probability', 'inequality']"
4604199,Help for generic logisitic population growth with variable harvesting problem,"I am attempting to work through a common problem entailing variable harvesting of a population growing logistically. I am struggling to determine all the properties for stable harvesting. I will detail my work to show where I need help The given ODE is, $\frac{dN}{dt}=RN(1-\frac{N}{K})-HN$ Where, $N$ is the population $R$ is the instinsic growth rate of the population $K$ is the carrying capacity $H$ is relative portion of the population to be harvested The goal is to determine properties for $H$ that allow for equilibrium of the population. The given ODE has no method for finding a general solution as it's nonlinear. Instead, I choose a more pratical method by setting $K=1$ to represent the maximum relative carry capacity. This would in turn mean that any choices for $R$ and the resulting $N$ would be in decimal form to be fractional pieces of $K=1$ . This provides, $\frac{dN}{dt}=RN(1-\frac{N}{1})-HN=\frac{dN}{dt}=RN(1-N)-HN$ $\space$ Next I determine equilibrium $N_e$ through finding solutions to $\frac{dN_e}{dt}=0$ $\frac{dN_e}{dt}=RN_e(1-N_e)-HN_e=RN_e-RN_e^2-HN_e=N_e(R-RN_e-H)=0$ . Two solutions exist, $N_{e_1}=0$ and $N_{e_2}=\frac{R-H}{R}=1-\frac{H}{R}$ . An equilibrium point for a population must be greater than zero, otherwise the population is tending towards extinction. Thus, $N_{e_1}=0$ is an invalid solution and we must have $N_{e_2}=1-\frac{H}{R}>0$ . By solving inequality of $N_{e_2}$ for $H$ , we discover $H<R$ . Which is a logical property. If the relative portion of the population harvested $H$ was greater than the growth rate $R$ , the population would reach extinction. $\space$ The next step is to determine the maximum harvest which sustains $N_{e_2}$ . To do so, we employ the second derivative test. $\frac{d^2N}{dt^2}=\frac{d}{dt}[RN_{e_2}(1-N_{e_2})-HN_{e_2}]=R(1-2N_{e_2})-H=R[1-2(1-\frac{H}{R})]-H$ By the properties of the second derivative test, for a local maximum to exist, we must have $\frac{d^2N}{dt^2}<0$ . Thus, $R[1-2(1-\frac{H}{R})]-H<0$ $\space$ $\space$ This is where I fail to continue the problem. When solving the above inequality for $H$ I always find $H<R$ , which is the exact same property discovered earlier in the solution. There should be a unique maximum for $H$ . I'm unsure of where I made error in my logic or what other methods to employ. Help would be greatly appreciated. Thank you.","['calculus', 'mathematical-modeling', 'ordinary-differential-equations']"
4604210,On fundamental units and generalizing $\left(\frac{3+\sqrt{13}}2\right)^{3/2} =\frac{\tan(t)\tan(3t)\tan(4t)}{\sqrt[4]{13}}$,"From this previous post , given $t = 2\pi/p$ for the appropriate prime $p=4m+1$ , we find closed-forms only for $x,y$ , $$\begin{align}
x &= \sum_{k=1}^m \sin^2(a_k\, t) = \frac{p+\sqrt{p}}8 \quad\quad\\
y &= \sum_{k=1}^m \cos^2(a_k\, t) = \frac{(p-2)-\sqrt{p}}8 \quad\quad\\
z &= \sum_{k=1}^m \tan^2(a_k\, t) = \; ??\quad\quad
\end{align}$$ using the same finite sequence of integer $a_k.$ To find the closed-form of the tangent version is a bit trickier, though I know $z$ is also a root of a quadratic. But the tangent product using the same $a_k$ for certain $p=4m+1$ was slightly easier, $$\begin{align}
\quad\quad\left(\frac{1+\sqrt{5}}2\right)^{3/2} &=\frac{\tan(t)}{\sqrt[4]{5}}\\ 
\quad\quad\left(\frac{3+\sqrt{13}}2\right)^{3/2} &=\frac{\tan(t)\tan(3t)\tan(4t)}{\sqrt[4]{13}}\\
\quad\quad\left(\frac{5+\sqrt{29}}2\right)^{3/2} &=\frac{\tan(t)\tan(4t)\tan(5t)\tan(6t)\tan(7t)\tan(9t)\tan(13t)}{\sqrt[4]{29}}\\
\end{align}$$ and so on for other primes. Question : Given prime $\color{blue}{p=n^2+4}=4m+1,$ with $t = 2\pi/p.$ From the previous post , define the finite set of integer $a_k$ that satisfies, $$x = \sum_{k=1}^m \sin^2(a_k\, t) = \frac{p+\sqrt{p}}8\quad\quad$$ Using the very same $a_k$ , is it true that, $$\quad w = \frac1{\sqrt[4]{p}}\,\prod_{k=1}^m \tan(a_k\, t)  = \left(\frac{\color{blue}n+\sqrt{p}}2\right)^{3/2} = \big(U_p\big)^{3/2}$$ where $U_p$ is a fundamental unit ?","['algebraic-number-theory', 'trigonometry', 'closed-form', 'radicals']"
4604242,Coloring every point of the plane with 4 colors,"Problem. If every point in the plane ( $\mathbb{R}^2$ ) is colored either red, yellow, green, or blue, show that some two points are a distance of either $1$ or $\sqrt{3}$ apart and have the same color. Attempt. By placing the $5$ points on the vertices of a regular pentagon with sidelength 1, we can use the pigeonhole principle to see that two of the vertices must have the same color. Thus, there must be two points of the same color which are either distance $1$ apart or $\phi=\frac{1+\sqrt{5}}{2}$ apart. This is because the length of the diagonal of a regular pentagon is the golden ratio $\phi$ . But I cannot get $\sqrt{3}$ as one of the two possible distances. I tried working with a regular hexagon but could not succeed there either. Source. I saw this on a problem set for a Putnam exam preparation (it is Problem 3 in that sheet). Bonus question(s). What is special about $\sqrt{3}$ ? In other words, if $\alpha > 0$ is any other real number with $\alpha\neq 1$ , can we show that there must exist two points of the same color that are either $1$ apart or $\alpha$ apart? If not, is there a characterization of such values of $\alpha$ ? Of course, the hypothesis in the bonus question is the same as before, namely each point of the plane is colored by one of the $4$ distinct colors. Thank you for your help!","['contest-math', 'discrete-geometry', 'pigeonhole-principle', 'geometry', 'problem-solving']"
4604319,Find the area of Quadrilateral $ABCD$. A puzzle for 10th graders.,"As the title suggests, the problem in this post was meant to be a puzzle for 10th graders, so claims the person who posted this on a language exchange platform: The problem is as follows: Given a Quadrilateral $ABCD$ with internal point $P$ , where $AP=1$ , $BP=2$ and $CP=3$ , and unknown sides $k$ and $2k$ , compute the area of this Quadrilateral. I first tried to inscribe this quadrilateral into a square but that approach did not turn out successful, I was thinking if there are any other ways to solve it, perhaps via setting up a coordinate system, or via a trigonometric method. I will share my own successful approach below as an answer!","['contest-math', 'euclidean-geometry', 'trigonometry', 'geometry']"
4604345,The pre-image sigma algebra of the greatest integer function [x].,"The first exercise in my Measure and Integration book is to find the pre-image sigma algebra of various functions, namely $f(x)=x^3$ , $x^2$ and $[x]$ . You can correct me if I'm wrong but I'm fairly sure that for $x^3$ we have the power set for $\mathbb{R}$ and for $x^2$ we have the power set for $\mathbb{R^+}$ , however for $[x]$ I'm quite unsure. I don't ever really work on the greatest integer function however it's inverse must simply be for $f(x)=c$ the group all the real numbers greater than $c$ but less than $c+1$ . How could I translate this into the preimage sigma algebra? My best guess on how to class these up would be the set $\left\{x+\frac{1}{n+\epsilon}: x\in\mathbb{R}\right\}, n\in\mathbb{N}$ however I'm very apprehensive that this could be wrong as I'm fairly new to measure theory.","['measure-theory', 'inverse-function', 'algebras']"
4604367,Selberg Class- Ramanujan Conjecture,"The wikipedia for Selberg class L-functions ( https://en.wikipedia.org/wiki/Selberg_class )
states 4 conditions: Analyticity, Ramanujan conjecture, Functional equation, Euler product. I would like to get a better intuitive understanding of the Ramanujan conjecture and why it is included as a condition. What properties of l-functions that violate RC but abide by the other 3 preclude RH?","['analytic-number-theory', 'number-theory', 'l-functions']"
4604453,Definition of Polish space: why homeomorphic?,"While glancing over measure theory books I noticed a discrepancy in the definition of a Polish space: given a topological space $(X,\mathcal T)$ , some authors use Definition A : $X$ is a Polish space when $X$ is separable, metrizable by a distance $d$ , and $(X,d)$ is complete. Others use Definition B : $X$ is a Polish space when $X$ is homeomorphic to a complete separable metric space. A clearly implies B (identity is a homeomorphism). If $(Y,d)$ is a complete separable metric space and $f:X\to Y$ is a homeomorphism, then $D:(x_1,x_2)\mapsto d(f(x_1),f(x_2))$ is a distance on $X$ , and $(X,D)$ is isometric to $(Y,d)$ hence complete, and the topology on $X$ is metrized by $D$ . Why is Definition B preferred in some references ?","['general-topology', 'metric-spaces', 'polish-spaces']"
4604469,Show that $\det(AB-BA) × \det(AC-CA) \geq 0$ if $A^2 = -BC$,"We have $A,B,C$ three $n×n$ matrices with real entries. We know that $$
  A^2 = -BC
$$ and we want to show that $$
  \det(AB-BA) × \det(AC-CA) \geq 0 \,.
$$ We can easily show that for $n=2k$ we have $\det(B) × \det(C) \geq 0$ , and for $n = 2k + 1$ we have $\det(B) × \det(C) \leq0$ . Then for $n = 2k$ we can expand $\det(AB-BA)$ like $$
  2 × \det(A) × \det(B) - c_1 + c_2 - c_3 + c_4 - \dotsb +c_{n-2} - c_{n-1}
$$ and expand $\det(AC-CA)$ like $$
  2 × \det(A) × \det(C) - d_1 + d_2 - d_3 + d_4 - \dotsb + d_{n-2} - d_{n-1} \,.
$$ For $n = 2k + 1$ , we can expand $\det(AB - BA)$ like $$
  c_1 - c_2 + c_3 - c_4 + \dotsb + c_{n-2} - c_{n-1}
$$ and $\det(AC - CA)$ like $$
  d_1 - d_2 + d_3 - d_4 + \dotsb + d_{n-2} - d_{n-1} \,.
$$ If we multiply these two expanded forms in every case $n = 2k$ and $n = 2k+1$ , then we get an ugly answer and I don’t know we can check the sign like that. I am now stuck. Maybe we need to use eigenvalues? Please someone help me solve this problem.","['determinant', 'eigenvalues-eigenvectors', 'matrices', 'characteristic-polynomial', 'matrix-decomposition']"
4604497,Conditional Second Moments of Multivariate Normal Variable on Binary Vectors,"Suppose we observe a binary table $Y \in \mathbb R^{N \times G}$ , corresponding to $N$ observations of $G$ dimensional binary vectors $Y_1, \cdots, Y_n$ . We imagine each vector $Y_i$ is generated from an unobserved multivariate normal vector $w_i \in \mathbb R^{k}$ through the following process: $W_i \sim N(0, I_k)$ , where $I_k$ is the k-dimensional identity matrix. $U_i = BW_i$ , where $B \in \mathbb R^{G \times K}$ is known. So $U_i$ is $G$ dimensional $Y_i \sim \text{Bernoulli}(\sigma(U_i))$ , where $\sigma(x)= \frac{1}{1+e^{-x}}$ is the sigmoid(logistic) function applying to $U_i$ component-wise. Now denote $W= [W_1^T, \cdots, W_n^{T}] \in \mathbb R^{N \times K}$ , and I am interested in computing the conditional second moments of $W$ on Y, e.g. : $$\mathbb E[W^{T}W |Y]$$ Here is my current approach: To proceed the computation, I think it would be easier to write $$\mathbb E[W^{T}W |Y] = E[W|Y]^{T} E[W|Y] + \text{Cov}(W|Y)$$ To compute $E[W|Y]$ ,  we just need to run a series of penalized logistic regressions to compute its posterior mean, as $B$ is known. However, I find it difficult to compute the conditional covariance of $W_i$ on $Y_i$ . If $Y_i$ is also normal, $W_i$ and $Y_i$ would be jointly Gaussian, and we can compute the conditional variance using the conditional variance formula for Gaussian. Is there an easy way to compute this entity when $Y_i$ is discrete? Thank you so much and happy holidays!","['statistics', 'covariance', 'conditional-probability', 'bayesian', 'machine-learning']"
4604513,No Nash-Kuiper theorem for $\mathcal C^2$ isometries,"Reading about the Nash-Kuiper theorem, I found the following statement in Y. Eliashberg and N. Mishachev's book Introduction to the $h$ -Principle : Is there a regular homotopy $f_t:S^2\rightarrow\mathbb R^3$ which begins with the inclusion $f_0$ of the unit sphere and ends with an isometric immersion $f_1$ into the ball of radius $\frac{1}{2}$ ? Here the word 'isometric' means preserving length of all curves . The answer is, of course, negative if $f_1$ is required to be $\mathcal C^2$ -smooth. Indeed, in this case the Gaussian curvature of the metric on $S^2$ should be at least $\geq 4$ somewhere. I am puzzled about the last affirmation. More precisely, suppose we have a regular surface immersed inside a ball of radius $1/2$ . Why should its Gaussian curvature be at least 4 at some point? This is intuitively clear, but I am not familiar with results about isometric immersions, so I don't know how one should stablish such a conclusion.","['riemann-surfaces', 'surfaces', 'riemannian-geometry', 'curvature', 'differential-geometry']"
4604523,Why are we solving this integral in this specific fashion?,I am solving the following problem: $\int \frac{\text{d}x}{(x^2*\sqrt{1+x^2})}$ I set $x = \tan(\theta)$ $\frac{\text{d}x}{\text{d}\theta} = \sec^2(\theta)$ $\int \frac{\sec^2(\theta)}{\tan^2(\theta)*\sqrt{1+\tan^2(\theta)}}*\text{d}x$ $\int \frac{\sec^2(\theta)}{\tan^2(\theta)*\sec(\theta)} *\text{d}\theta$ After some simplifying I get: $\int \frac{\cos(\theta)}{\sin^2(\theta)} * \text{d}\theta$ $u = \sin(\theta)$ $\frac{\text{d}u}{\text{d}\theta} = \cos(\theta)$ $\int \frac{\text{d}u}{u^2}$ $\frac{-1}{\sin(\theta)} + c$ It is here where I get confused If you use the first triangle: $\sin(x) = \frac{x}{\sqrt{x^2+1}}$ If you use the second triangle: $\sin(x) = \frac{1}{\sqrt{x^2+1}}$ Could I use either one to find my answer in terms of x? My theory is that we use the first triangle because $x = \tan(\theta)$ and using the first triangle we get $x = \frac{x}{1}$ compared to $x = \frac{1}{x}$ This makes $x$ true for all real numbers in the first one while $x$ can only be equal to 1 in the second equation. Is this right or no? Sorry for making this question so long.,"['integration', 'calculus', 'derivatives', 'trigonometry']"
4604563,Continuity of Convolution on $C_c( G)$,"I’m reading through an introductory operator algebra paper currently, and I’m a little lost on how one proves continuity of the convolution on $C_c(G)$ for any locally compact topological group $G$ . The paper says “For $f,g \in C_c(G)$ and $t \in G$ , define: $$ (f \ast g)(t) = \int_{G} f(s)g(s^{-1}t) d\mu(s).$$ Then $f \ast g$ is continuous by the Dominated convergence theorem.” I follow this reasoning for $G$ first countable, since then for $(t_k)_{k=1}^{\infty}$ converging to $t$ , we can define $h_k(s) = f(s)g(s^{-1}t_k)$ and apply the DCT (maybe I’m not doing this correctly?).  But for $G$ not first countable, I don’t see how you could apply DCT, since convergent nets of measurable functions need not even be measurable. I’d be very appreciative if someone could point out what I’m missing (I feel like this may need a uniform convergence argument).  Thank you.","['operator-algebras', 'operator-theory', 'convolution', 'analysis', 'functional-analysis']"
4604570,Peculiar subspace for a measure on infinite-dimensional separable Banach,"Let $E$ be an infinite-dimensional separable Banach space, and $P$ be a Borel probability measure on $E$ . There exists $F$ verifying the following conditions: $F$ is in the Borel $\sigma$ -algebra of $E$ , $F$ is a vector subspace of $E$ , $F\subsetneqq E$ , $P(F)=1$ . I'm looking for a proof of this fact, which is Exercise 7 p.82 in this book : As Eric Wofsey points out, $F$ is not necessarily unique, so I've removed this part of the question. Here's the theorem mentioned in the hint: Since $E$ is Polish, $P$ is a Radon measure, hence there exists a separable Borel set $A$ with $P(A)=1$ . However this doesn't help much. Since $E$ is separable, there is an increasing sequence of finite-dimensional subspaces $G_n$ such that $\bigcup_n G_n$ is dense in $E$ , but that doesn't seem to help either.","['banach-spaces', 'probability-theory', 'functional-analysis', 'measure-theory']"
4604583,Teaching Abstract Algebra for High School Teachers,"I am teaching a class in the Spring for abstract algebra for high school teachers. The goals of this class is to give future high school teachers a deeper understanding of many of the patterns and concepts in geometry, introductory algebra, and trigonometry. What concepts would you consider necessary for students to learn? Also, what topics in high school/middle school math could be explained using abstract algebra? I have a general course outline in place listed below. First couple of weeks- Review sets, functions, relations, binary operations, and common proof techniques Next 6-8 weeks - Focus on groups. We will talk about the definition of groups, subgroups, cyclic groups, Cayley diagrams, multiplication tables, Direct Products, quotient groups, LaGrange's Theorem, cosets isomorphisms, homohorphisms, and plenty of examples of groups in geometry, trig, and algebra (such as rotations of a circle, plane isometries, symmetries of regular polygons, symmetry groups of the conics, modular arithmetic, even and odd functions, ect.) These concepts are not listed in the order they will be presented. The last 4-6 weeks - Rings and fields. We will talk about the definitions of rings, fields, subrings, modules, ideals, Fermat's little theorem, and focus a fair bit on rings of polynomials. We will also talk about the differences between various number systems and compare them to less common number systems (naturals, integers, rationals, reals, complex, dual, and split numbers, and Gaussian integers). If we have time, we will wrap up with splitting fields and field extensions. Are there any important concepts that are missing?","['geometry', 'abstract-algebra', 'education', 'trigonometry', 'algebra-precalculus']"
4604645,Max. likelihood and sufficient statistic of exponential distribution.,"Consider the following probability function of a random variable $Y$ : $$
f(y \mid \theta)=e^{-(y-\theta)},\quad y\ge\theta
$$ and $0$ otherwise. We take a random sample $(Y_1,Y_2,...,Y_k)$ and want to find a sufficient statistic and a maximum likelihood estimator for $\theta$ . Now, the likelihood is given by $$
L\left(y_1, y_2, \ldots, y_k \mid \theta\right)=\prod_{i=1}^k e^{-\left(y_i-\theta\right)}=\exp \left(-\sum_{i=1}^k y_i+k \theta\right)
$$ Obviously, this is maximized when $\theta$ is maximized. Since the density function is nonzero only when $y\ge\theta$ , my first intuition is that the MLE for $\theta$ is $\min(y_1,y_2,...,y_k)$ , although I am not sure that it is correct. For the sufficient statistic, I believe we can choose $S=-\sum_{i=1}^k Y_i$ , in which case the likelihood function can be written as the product of $g(s, \theta)=e^{s+k \theta}$ and $h(y_1,y_2,...,y_k)=1$ , and a theorem then tells us that $S$ is a sufficient statistic. Can someone tell me if I have made a mistake or misunderstood something?","['statistics', 'sufficient-statistics', 'solution-verification', 'maximum-likelihood', 'probability']"
4604651,Equilibrium point of a function and its basin of attraction,"I'm very lost with the following problem: Consider $f=(f_1,f_2,f_3)\in\mathcal{C}(\mathbb{R}^3,\mathbb{R}^3)$ such that $f(0,0,0)=(0,0,0)$ and \begin{equation} x_1f_1+x_2f_2+x_3f_3<0 \tag{*}
\end{equation} for all $(x_1,x_2,x_2)\in\mathbb{R}^3\backslash\{ (0,0,0)\}$ . Prove that the $(0,0,0)$ is the unique equilibrium point of $f$ , and its basin of attraction is all $\mathbb{R}^3$ . I have started my course of dynamical systems recently and I find this problem in my practice list of exercises. My problem is that I don't understand very well what does it means that and equilibrium point of a function. I find in my notes the definition of equilibrium point, but for a differential equation $x'=f(x)$ . It means to say that I need to find the equilibria of the system $$
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 
\end{bmatrix}'=\begin{bmatrix}
f_1 \\
f_2 \\
f_3 
\end{bmatrix} \ ?
$$ If so, how does the condition $(*)$ help me? And finally, how it is obtained his basin of attraction from this? I appreciate your help a lot.","['basins-of-attraction', 'stability-in-odes', 'ordinary-differential-equations', 'dynamical-systems']"
4604655,"What is the ""polar coordinate function""?","I'm reading Tapp's Differential Geometry of Curves and Surfaces, I have this problem: $\quad\color{green}{\text{Exercise 1.10.}}$ Prove that the arc length, $L$ , of the graph of the polar coordinate function $r(\theta)$ , $\theta \in [a, b]$ , is What is the ""polar coordinate function""? I tried a few guesses but nothing I thought seemed to make sense nor would give me the arclength function above. Using the definition of arclength, I tried to ""reverse engineer"" it and I got: $$\left(\int_{a}^{\theta} r(x)\, dx,\, r(\theta) \right)$$ This seems to be the only function that would make sense here but I have no idea what $r(\theta)$ would be. This is — perhaps — the least worst thing I could think of.","['integration', 'multivariable-calculus', 'calculus', 'polar-coordinates', 'differential-geometry']"
4604662,Existance of Linearly Independent generalized eigenvectors,"Is it true that all $n\times n$ matrices $A$ admit $n$ generalized eigenvectors? Are they all linearly independent? Intuitively I know it's true over complex numbers but I could not find a rigorous proof. I am doing linear systems of differential equations and I want to prove that for the system $x'=Ax$ it is always possible to find $n$ linearly independent solutions of the form: $$
x=\sum_{i=1}^n \xi_ie^{\lambda t}t^{n-i}
$$ Where $\xi_i$ is a generalized evigenvector of rank $i$ with associated eigenvalue $\lambda$ . I need to ensure that if $\lambda$ is a repeated eigenvalue of multiplicity $m$ then $m$ linearly independent solutions are associated with it. For insufficient eigenvalues, existence of higher rank generalized eigenvectors guarantees extra solutions because of the following property: $$
A\xi_i=\xi_{i-1}+\lambda\xi_i \quad\text{which is analogous to}\quad [t^ne^{\lambda t}]'=t^{n-1}e^{\lambda t}+\lambda t^ne^{\lambda t}
$$ Geometrically, if the generalized eigenvectors are linearly independent, then we can express every point in terms of generalized eigenvectors. This explains why the solution is is a degenerate node that curves towards the eigendirection.","['linear-algebra', 'ordinary-differential-equations']"
4604665,Confused about answer to Spivak's calculus in prologue,"I have attached the question and answer to this question from Spivak's Calculus . The proposition we are supposed to prove: $$
x^n - y^n = (x - y)\, 
(x^{n-1} + x^{n-2}y + \dots + xy^{n-2} + y^{n-1})
$$ The proof of this proposition: \begin{align}
(x - y) \, 
(x^{n-1} + x^{n-2}y &+ \dots + xy^{n-2} + y^{n-1}) \\[2pt] 
&= x \, (x^{n-1} + x^{n-2}y + \dots + xy^{n-2} + y^{n-1}) \\ 
&\qquad{} - y \, (x^{n-1} + x^{n-2}y + \dots + xy^{n-2} + y^{n-1}) \\[2pt]
&= \bigl[ x^n + x^{n-1}y + \dots + x^2y^{n-2} + xy^{n-1} \bigr] \\ 
&\qquad{} - \bigl[ x^{n-1}y + x^{n-2}y^2 + \dots + xy^{n-1} + y^n  \bigr] \\[2pt]
&= x^n - y^n
\end{align} I do not understand how $x^2\,y^{n-2}$ in the third to last line cancels with $x^{n-2}\,y^2$ in the second to last line (after the negative sign is distributed to $x^{n-2}\,y^2$ ). Also, I do not understand how what is within the ellipses is treated. do we conceive of what is in the ellipses as multiplied by $x$ and $-y$ when $x$ and $-y$ are distributed to the expression with which $(x-y)$ is multiplied originally? If so, how can the ellipses cancel with each other as they are multiplied by different variables which we don't know are equal?
thanks, let me know if my question lacks clarity and I will explain my questions better/further.","['proof-explanation', 'algebra-precalculus', 'polynomials']"
4604675,Show generalized spherical coordinate are surjective.,"Define $G: \mathbb{R}^n \rightarrow \mathbb{R}^n$ by $G\left(r, \phi_1, \ldots, \phi_{n-2}, \theta\right)=\left(x_1, \ldots, x_n\right)$ where $$
\begin{aligned}
& x_1=r \cos \phi_1, \quad x_2=r \sin \phi_1 \cos \phi_2, \quad x_3=r \sin \phi_1 \sin \phi_2 \cos \phi_3, \ldots \\
& x_{n-1}=r \sin \phi_1 \cdots \sin \phi_{n-2} \cos \phi_1, \quad x_n=r \sin \phi_1 \cdots \sin \phi_{n-2} \sin \theta
\end{aligned}
$$ Show $G$ maps $\mathbb{R}^n$ onto $\mathbb{R}^n$ . Here is my attempt. We will show by induction, that the map is surjective. Base Case when n=2, fix $(x,y)\in \mathbb{R}^2$ . Then choose $r=\sqrt{x^2+y^2}$ . Now, we must find a $\theta$ such that $(x,y)=(r\cos(\theta),r\sin(\theta))$ . Thus, $$\frac{y}{x}=\tan(\theta)$$ . Using the $\operatorname{atan2}$ function, we can define the inverse over any point on the plane excluding the origin.
We have $$\operatorname{atan}2(y, x)= \begin{cases}\arctan \left(\frac{y}{x}\right) & \text { if } x>0, \\ \arctan \left(\frac{y}{x}\right)+\pi & \text { if } x<0 \text { and } y \geq 0, \\ \arctan \left(\frac{y}{x}\right)-\pi & \text { if } x<0 \text { and } y<0, \\ +\frac{\pi}{2} & \text { if } x=0 \text { and } y>0, \\ -\frac{\pi}{2} & \text { if } x=0 \text { and } y<0, \\ \text { undefined } & \text { if } x=0 \text { and } y=0 .\end{cases}$$ Thus, choosing $\theta=\operatorname{atan}2(y, x)$ , we are done. Induction Step Fix $(x_1,\dots,x_n,x_{n+1})\in \mathbb{R}^n$ . We would like to appeal to the induction step. Choose $r,\varphi_1,\dots,\varphi_{n-2},\theta$ by the induction hypothesis. How can I then select the proper final angle to complete the induction step? How can I choose this theta to complete the induction step? Finally, on wikipeida, the following function was obtained for the inverse but I am not sure how they came up with this.
We may define a coordinate system in an $n$ -dimensional Euclidean space which is analogous to the spherical coordinate system defined for 3 -dimensional Euclidean space, in which the coordinates consist of a radial coordinate $r$ , and $n-1$ angular coordinates $\varphi_1, \varphi_2, \ldots \varphi_{n-1}$ , where the angles $\varphi_1, \varphi_2, \ldots \varphi_{n-2}$ range over $[0, \pi]$ radians (or over $[0,180]$ degrees) and $\varphi_{n-1}$ ranges over $\left[0,2 \pi\right.$ ) radians (or over $[0,360)$ degrees). If $x_i$ are the Cartesian coordinates, then we may compute $x_1, \ldots x_n$ from $r, \varphi_1, \ldots \varphi_{n-1}$ with: $[4]$ $$
\begin{aligned}
x_1 & =r \cos \left(\varphi_1\right) \\
x_2 & =r \sin \left(\varphi_1\right) \cos \left(\varphi_2\right) \\
x_3 & =r \sin \left(\varphi_1\right) \sin \left(\varphi_2\right) \cos \left(\varphi_3\right) \\
& \vdots \\
x_{n-1} & =r \sin \left(\varphi_1\right) \cdots \sin \left(\varphi_{n-2}\right) \cos \left(\varphi_{n-1}\right) \\
x_n & =r \sin \left(\varphi_1\right) \cdots \sin \left(\varphi_{n-2}\right) \sin \left(\varphi_{n-1}\right) .
\end{aligned}
$$ Except in the special cases described below, the inverse transformation is unique: $$
\begin{aligned}
& r=\sqrt{{x_n}^2+x_{n-1}{ }^2+\cdots+x_2{ }^2+x_1{ }^2} \\
& \varphi_1=\operatorname{arccot} \frac{x_1}{\sqrt{x_n{ }^2+x_{n-1}{ }^2+\cdots+x_2^2}} \quad=\arccos \frac{x_1}{\sqrt{x_n{ }^2+x_{n-1}{ }^2+\cdots+x_1^2}} \\
& \varphi_2=\operatorname{arccot} \frac{x_2}{\sqrt{x_n{ }^2+x_{n-1}{ }^2+\cdots+x_3^2}} \quad=\arccos \frac{x_2}{\sqrt{x_n{ }^2+x_{n-1}{ }^2+\cdots+x_2{ }^2}} \\
& \varphi_{n-2}=\operatorname{arccot} \frac{x_{n-2}}{\sqrt{x_n{ }^2+x_{n-1}{ }^2}} \quad \quad=\arccos \frac{x_{n-2}}{\sqrt{x_n{ }^2+x_{n-1}{ }^2+x_{n-2}{ }^2}} \\
& \varphi_{n-1}=2 \operatorname{arccot} \frac{x_{n-1}+\sqrt{x_n^2+x_{n-1}^2}}{x_n} \quad= \begin{cases}\arccos \frac{x_{n-1}}{\sqrt{x_n^2+x_{n-1}{ }^2}} & x_n \geq 0, \\
2 \pi-\arccos \frac{x_{n-1}}{\sqrt{x_n^2+x_{n-1}{ }^2}} & x_n<0 .\end{cases} \\
&
\end{aligned}
$$ where if $x_k \neq 0$ for some $k$ but all of $x_{k+1}, \ldots x_n$ are zero then $\varphi_k=0$ when $x_k>0$ , and $\varphi_k=\pi$ (180 degrees) when $x_k<0$ . Was this contstruction also obtained inductively? and why do they not use tan but rather cot?","['physics', 'calculus', 'measure-theory', 'analysis']"
4604684,"Continuous bijection from $(0,1]$ to $(0,1)$ on rational numbers.","Recently, I have been dealing with generalizing notions using the real numbers by using the rationals instead. A function $f\colon\Bbb Q\to\Bbb Q$ is rational-continuous at a rational number $\alpha$ iff for any $\varepsilon > 0,$ there exists a corresponding $\delta$ based on $\alpha$ and $\varepsilon$ such that if $|\alpha-x| \leq \delta, |f(\alpha)-f(x)| \leq \varepsilon.$ Is there a ""rational-continuous"" bijection from the rationals in $(0,1]$ to the rationals in $(0,1)?$","['general-topology', 'analysis', 'rational-numbers']"
4604719,Centralizer of a Subgroup is a Subgroup,"If $H$ is a subgroup of $G$ , then Gallian defines the centralizer $C(H)$ of $H$ as the set $$C(H) = \{g \in G : \text{$g h = h g$ for all $h \in H $}\} \,.$$ It’s easy to show that $C(H)$ is a subgroup of $G$ : Clearly $e h = h e = h$ for all $h \in H$ , so $C(H)$ is nonempty. Then if $g_0 \in C(H)$ and $g_1 \in C(H)$ , then $$(g_0 g_1) h = g_0 (g_1 h) = g_0 (h g_1) = (g_0 h) g_1 = h(g_0 g_1)$$ for all $h\in H$ , so $g_0g_1\in C(H)$ . Similarly, $$g_0 \in C(H) \iff g_0 h = h g_0 \iff g_0^{-1} (g_0 h) g_0^{-1} = g_0^{-1} (h g_0) g_0^{-1} \iff h g_0^{-1} = g_0^{-1} h$$ for all $h\in H$ , so we have $g_0^{-1}\in C(H)$ . Thus $C(H)$ is a subgroup of $G$ . I have two concerns about this proof. First, it is the exact same proof as for showing $C(h)$ is a subgroup of $G$ for any fixed $h \in G$ , except for adding the two “for all $h \in H$ ” clauses after each closure argument. Is anything else needed? Second, where does the condition that $H$ is a subgroup come into play? It seems one could take $H$ to be any subset of the group, not necessarily a subgroup. Thanks in advance!","['proof-explanation', 'group-theory', 'abstract-algebra']"
4604804,Calculating $\frac{1.01^5}{1.01^5-1}$ without calculator with good accuracy?,"Question (may SKIP reading this): A computer is sold either for $19200$ cash or for $4800$ cash down payment together with five equal monthly installments. If the rate of interest charged is $12\%$ per annum, then the amount of each installment (nearest to a rupee) is? Options: A) $2965\qquad$ B) $2990\qquad$ C) $3016\qquad$ D) $2896\qquad$ E) $2880$ Boiling down to calculating: $$\frac{144(1.01)^5}{1.01^5-1}$$ Where if I were to approximate $1.01^5\sim1+0.01(5)=1.05$ , returns $3024$ as answer thus tempting one to select option 'C' which is wrong! The answer option is Option A (which is itself a bit too off from the more accurate value of $2967$ but makes sense anyways as other options are too far apart from this value). So, without using a calculator , how to calculate the above expression with greater accuracy? Note: Just acknowledging the fact that I received a lot of great answers but sadly could accept only $1$ which turned out to be a very difficult task. Finally, unable to select on my own, I went with the one that the community selected (most upvoted).","['contest-math', 'elementary-functions', 'algebra-precalculus']"
4604869,An equivalent norm of $\|\cdot\|_\infty$,"Let $X$ be a compact topological space and $(E, |\cdot|)$ a Banach space. Let $\mathcal C$ be the space of all continuous functions from $X$ to $E$ . Let $\|\cdot\|_\infty$ be the supremum norm on $\mathcal C$ . Then $(\mathcal C, \|\cdot\|_\infty)$ is a Banach space . We fix a continuous map $g:X \to (0, \infty)$ and define a new norm $[\cdot]$ on $\mathcal C$ by $$
[f] := \sup_{x\in X} g(x) |f(x)| \quad \forall f \in \mathcal C.
$$ Because $X$ is compact and $g$ continuous, there are $c_1, c_2 >0$ such that $c_1 \le |g(x)| \le c_2$ for all $x \in X$ . As such, $$
c_1 \|\cdot\|_\infty \le [\cdot] \le c_2 \|\cdot\|_\infty.
$$ It follows that $[\cdot]$ is equivalent to $\|\cdot\|_\infty$ . Hence $(\mathcal C, [\cdot])$ is a Banach space. Could you confirm if my above understanding is correct?","['banach-spaces', 'functional-analysis']"
4604904,Show that $(x+1)^{p}x^{1-p}-(x+1)^{1-p}x^{p}$ is strictly increasing,"Let $1/2<p< 1$ . I am asked to show that $$f(x)=(x+1)^{p}x^{1-p}-(x+1)^{1-p}x^{p}$$ is strictly increasing for $x\geq 0$ and to compute $\lim_{x\to\infty} f(x)$ . I first computed the derivative, but I don't see why it must be positive: $$\frac{d f(x)}{d x}=p(x+1)^{p-1}x^{1-p}+(1-p)(x+1)^px^{-p}-(1-p)(x+1)^{-p}x^p -p(x+1)^{1-p}x^{p-1}$$ Any ideas?
Thanks a lot for your help. Case $p=3/4$ . Then $$f(x)=(x+1)^{1/4}x^{1/4}(\sqrt{x+1}-\sqrt{x})=\frac{(x+1)^{1/4}x^{1/4}}{\sqrt{x+1}+\sqrt{x}}$$ It suffices to show that $\ln f(x)$ is strictly increasing. We have $$\ln f(x) =\frac{1}{4} \ln(x+1)+\frac{1}{4} \ln(x)-\ln(\sqrt{x+1}+\sqrt{x})$$ Taking derivative w.r.t. $x$ we get $$\frac{d \ln f(x) }{d x}=\frac{1}{4}\frac{1}{x+1}+\frac{1}{4}\frac{1}{x}-\frac{1}{\sqrt{x+1}+\sqrt{x}}(\frac{1}{2}\frac{1}{\sqrt{x+1}}+\frac{1}{2}\frac{1}{\sqrt{x}})$$ Hence $\frac{d \ln f(x) }{d x}>0$ is equivalent to $$(\frac{1}{4}\frac{1}{x+1}+\frac{1}{4}\frac{1}{x})(\sqrt{x+1}+\sqrt{x})>\frac{1}{2}\frac{1}{\sqrt{x+1}}+\frac{1}{2}\frac{1}{\sqrt{x}}$$ which is equivalent to $$\frac{1}{4}\frac{\sqrt{x}}{x+1}+\frac{1}{4}\frac{\sqrt{x+1}}{x}>\frac{1}{4}\frac{1}{\sqrt{x+1}}+\frac{1}{4}\frac{1}{\sqrt{x}}$$ or $$\frac{\sqrt{x+1}-\sqrt{x}}{x}>\frac{\sqrt{x+1}-\sqrt{x}}{x+1}$$ which holds. Hence $f(x)$ is strictly increasing.","['functions', 'monotone-functions', 'real-analysis']"
4604908,find the number of ways to distribute six handouts,"Fifteen freshmen are sitting in a circle around a table, but the course assistant (who remains standing) has made only six copies of today's handout. No freshman should get more than one handout, and any freshmen who doesn't get one should be able to read a neighbor's. If the freshmen are distinguishable but the handouts are not, how many ways are there to distribute the six handouts subject to the above conditions? I think the answer is 750, and my reasoning is shown below. We consider the sequences of 6 gap sizes between consecutive students. By the problem conditions, each sequence must consist only of the numbers 0,1,2 and sum to 9. The only possibilities up to ordering are $1,1,1,2,2,2$ and $2,2,2,2,0,1,2$ . Distinct sequences are not cyclic shifts of each other because they involve at least two different numbers. Each sequence corresponds to 15 possibilities, which are the cyclic shifts of the sequences (since the freshmen are distinct while the handouts are not and cyclic shifts are distinct). So we have $\dfrac{6!}{3!3!} + \dfrac{6! }{4!} = 50$ sequences, giving $50\cdot 15 = 750$ possibilities. However, it seems I've overcounted possibilities due to cyclic shifts.","['contest-math', 'solution-verification', 'combinatorics', 'discrete-mathematics']"
4604957,Infinite-dimensional Banach has proper dense subspace,"Show that every (separable) infinite-dimensional Banach space $X$ contains a proper vector subspace $Y$ with $cl (Y)$ = $X$ This is Exercise 1 (f) p.245 of this book . Assuming $X$ is separable, there is an increasing sequence of finite-dimensional subspaces $G_n$ such that $G:=\bigcup_n G_n$ is dense in $E$ . The vector subspace $span(G)$ is dense as well. Since $X$ has infinite dimension, each $G_n$ is closed and has empty interior. By Baire's theorem, $G$ has empty interior. However this does not imply that $span(G)\neq X$ . Can this line of thought lead to a solution ? Completely different solutions are fine. I'm also interested in a proof for the case where $X$ is not separable.","['banach-spaces', 'general-topology', 'functional-analysis']"
4604969,Show that the $D_{3h}$ prism is given by the direct product $D_{3h}=D_3 \otimes C_s$,"I have some questions regarding the notation and solution to the following problem involving the direct product: The diagram below shows the symmetry operations of an equilateral right triangular prism. The elements of $D_3$ are the identity $E$ , $(\mathrm{a})$ the rotations $C_3$ and ${C_3}^2$ , and $(\mathrm{b})$ the vertical reflection planes $\sigma_{v,1}$ , $\sigma_{v,2}$ , and $\sigma_{v,3}$ , and those of $C_s$ are the identity $E$ and $(\mathrm{c})$ a horizontal reflection plane $\sigma_h$ midway between the two horizontal faces of the prism. Show that the group $D_{3h}$ prism is given by the direct product $$D_{3h}=D_3 \otimes C_s$$ where $C_s=\{E, \sigma_h\}$ My first question is regarding the notation, according to this article on Wikipedia for the direct product of two groups, the correct notation is actually $D_{3h}=D_3 \oplus C_s$ and not $D_{3h}=D_3 \otimes C_s$ . This is confusing me as I thought the ' $\oplus$ ' was the notation reserved for the direct sum . Why then the inconsistency for the notation? Looking across the literature, ""Contemporary Abstract Algebra"" by Joseph A Gallian, 10th edition, page 172 uses $\oplus$ , yet ""Group Theory - Application to the Physics of Condensed Matter"", by Dresselhaus, 2008, page 101 uses $\otimes$ as does ""Group Theory in Physics"" by Wu-Ki Tung, 1985, page 25. Yet other books such as ""Applications of Group Theory in Quantum Mechanics"" by Petrashen and Trifonov, published in 2009 on page 49 use a regular "" $\times$ "". Due to this I'm very confused which notation to use. On to the question itself, using the definition from my notes: DEFINITION $1.2.$ A direct product of two groups $H$ and $K$ , denoted as $H \otimes K$ , is deﬁned as the set of all distinct ordered pairs $(h, k)$ , where $h$ is an element of $H$ and $k$ is an element of $K$ , with the binary composition law $$(h_1,k_1)(h_2,k_2) \equiv (h_1h_2,k_1k_2),$$ determined by the composition rules within $H$ and $K$ . This direct product, also known as the external product , satisﬁes the group axioms. Applying the procedure outlined above to the 2 groups I find that, $$D_{3h}=D_3\otimes C_s =\left(E, C_3, {C_3}^2, \sigma_{v,1}, \sigma_{v,2}, \sigma_{v,3}\right)\left(E, \sigma_h\right)$$ $$=\left(E, C_3, {C_3}^2, \sigma_{v,1}, \sigma_{v,2}, \sigma_{v,3}, \sigma_h, C_3\sigma_h,{C_3}^2\sigma_h,\sigma_{v,1}\sigma_h,\sigma_{v,2}\sigma_h,\sigma_{v,3}\sigma_h\right)\tag{1}$$ The problem with this equation is that I don't know what the elements of $D_{3h}$ are, so I followed the prescription laid out by my notes but how does this prove that $D_{3h}=D_3 \otimes C_s$ ? Here is the author's solution: The group $D_3$ of the top and bottom faces has elements $$D_3=\{E, C_3, {C_3}^2, \sigma_{v,1}, \sigma_{v,2}, \sigma_{v,3}\}$$ The prism has an additional symmetry operation, a horizontal reflection plane $\sigma_h$ , resulting from the equivalence of the top and bottom faces. The associated group is $C_s = \{E, \sigma_h\}$ . Since $\sigma_h$ operates on vertical coordinates, and the elements of $D_3$ operate on horizontal coordinates, the elements of $C_s$ and $D_3$ commute. Hence, the group of the prism, which is
denoted as $D_{3h}$ , is $$D_{3h}=D_3 \otimes C_s.$$ Again, this solution doesn't really seem to 'prove' that $D_{3h}=D_3 \otimes C_s$ , to me the author has just given a handwaving argument and I'm left confused. So to summarize, is there a way to be more formal about this proof (like what I was trying to do in equation $(1)$ ), ie. utilizing the recipe given, that $(h_1,k_1)(h_2,k_2)=(h_1h_2,k_1k_2)$ ?","['direct-product', 'dihedral-groups', 'abstract-algebra', 'intuition', 'group-theory']"
4604971,Role of verification theorems in stochastic optimal control?,"I am currently working on the optimal control of certain classes of stochastic processes and I have difficulties understanding the roles of verification theorems. My problem is the following: I am not sure to understand whether this is purely a problem that arises from the use of partial differential equations for which we may need to consider viscosity solutions or whether this is something related to the connection between the optimal control problem and its solution expressed in terms of the value function which is a solution to the Hamilton-Jacobi-Bellman (HJB) equation, which is a PDE in many instances of those problems. Would not be the Bellman optimality principle ensures that the optimal control can be computed from the value function itself a solution of the HJB equation? I am also wondering whether this a specificity of stochastic optimal control problems because I am not sure to have seen verification theorems in the deterministic setting. Thanks and feel free to comment to ask for more details.","['optimal-control', 'stochastic-processes', 'probability-theory', 'partial-differential-equations']"
4604978,"Continuous bijection from $(0,1]$ to $(0,1)$ on rational numbers with a continuous inverse.","A function $f\colon\Bbb Q\to\Bbb Q$ is rational-continuous at a rational number $\alpha$ iff for any $\varepsilon > 0,$ there exists a corresponding $\delta$ based on $\alpha$ and $\varepsilon$ such that if $|\alpha-x| \leq \delta, |f(\alpha)-f(x)| \leq \varepsilon.$ Is there a ""rational-continuous"" bijection with a ""rational-continuous"" inverse from the rationals in $(0,1]$ to the rationals in $(0,1)?$ The answer to my original question has an inverse which is not  ""rational-continuous"", but I think a nicer solution exists.","['general-topology', 'analysis', 'rational-numbers']"
4605015,Comparison between $a^{b!}$ and $(a^b)!$,"I've recently discovered two conclusions that may be true. However I can't prove it. The statement is as follows : Suppose $a$ , $b$ are positive integers and $a\geq b \geq 2$ , then I found this conclusion $a^{b!}<(a^b)!$ might be true. On the other hand, for any $a\geq 2$ , there must exist an integer $b_0>a$ ( $b_0$ is related to $a$ ) such that for any $b\geq b_0$ we have $a^{b!}>(a^b)!$ . I have verified the above conclusion in some cases such as $2^{100!}>2^{100}!$ , $99^{280!}>99^{280}!$ , $100^{100!}<100^{100}!$ , $200^{100!}<200^{100}!$ . For $a=2$ , we can verify that $2^{5!}>2^{5}!$ is true and $b_0=5$ is the smallest one.","['inequality', 'factorial', 'analysis']"
4605038,Derive the Lemma of Christoffel symbol,"Derive the Lemma of Christoffel symbol, $$\frac{\partial^2x^\rho}{\partial\bar x^\mu\partial\bar x^\nu}=\Gamma_{\mu\nu}^\gamma\frac{\partial x^\rho}{\partial\bar x^\gamma}-\frac{\partial x^\alpha}{\partial\bar x^\mu}\frac{\partial x^\beta}{\partial\bar x^\nu}\Gamma_{\alpha\beta}^\rho$$ We know that, $$\bar g_{ij}=\frac{\partial x^\alpha}{\partial \bar x^i}\frac{\partial x^\beta}{\partial\bar x^j}g_{\alpha\beta}\tag 1$$ Now, the process my book followed, differentiate $(1)$ with respect to different co-ordinate $\bar x^i$ , interchange several variable and use the definition of Christoffel symbol of first kind and multiplying conjugate metric $\bar g^{rk}$ . The complete computation was so tedious and it took 5 pages. Now, I can't just memorize that and copy in my exam sheet. Is there any other approach where things were flowing a sequence or I can easily memorize or understand those steps intuitively. Any solution or external redirect will be appreciated. Thanks in advance.","['special-relativity', 'tensors', 'general-relativity', 'differential-geometry']"
4605052,Somewhat confused about a proof of surjective implies right inverse,"I have a question about the Proof $1$ in the link below. There are a a few related questions on MSE. My question has to do with how they chose the selector function. At first I thought $g$ was a selector, but that wouldn't make sense. I think they did it implicitly. I'll try to fill in the details. Please, see if that makes sense. Since $f$ is surjective, exists $x \in S$ s.t. $f(x) = y$ meaning $|f^{-1}(y)| \ge 1.$ But for a conclusion $f \circ g = I_T$ we need $|f^{-1}(y)| = 1.$ By Axiom of Choice, there's a selector $h$ on $S$ s.t. $h(f^{-1}(y)) = x \in f^{-1}(y)$ .  Thus for every $y$ there is some designated $x_y \in S.$ Define $g: T \to S$ as $g(y) = x_y$ . Now suppose $y_1 = y_2$ . Then $f^{-1}(y_1)  = f^{-1}(y_2)$ meaning $g(f^{-1}(y_1))  = g(f^{-1}(y_2))$ and so $g$ is a well-defined function. Thus $f \circ g(y) = f(g(y)) = f(x_y) = y$ and so $f \circ g(y) = I_T.$ https://proofwiki.org/wiki/Surjection_iff_Right_Inverse","['elementary-set-theory', 'functions', 'discrete-mathematics']"
4605055,"Integral: $\int_{1}^{\infty} \operatorname{arctanh} \left(k\,\sqrt{\frac{x^2-1}{x^2}} \right)\, e^{-\alpha \, x \,} dx$","Is there any chance to find a closed form for these integrals? $$I_1(k,\alpha)=\int_{1}^{\infty} \operatorname{arctanh}  \left(k\,\sqrt{\frac{x^2-1}{x^2}} \right)\, e^{-\alpha \, x \,} dx$$ $$I_2(k,\alpha)=\int_{1}^{\infty} x^2\, \operatorname{arctanh}  \left(k\,\sqrt{\frac{x^2-1}{x^2}} \right)\, e^{-\alpha \, x} \, dx$$ $\operatorname{arctanh} (z)=\tanh^{-1}(z)$ is the inverse hyperbolic tangent. $k$ and $\alpha$ are real constants. These two integrals appear in part of my master's work in plasma physics. Obviously, solutions are calculated numerically. But the symbolic results allow me to discuss the solutions of an algebraic equation. Surely there's a chance that someone might be able to calculate them.","['integration', 'definite-integrals', 'hyperbolic-functions', 'closed-form', 'hypergeometric-function']"
4605103,How many coin flips would it take to have a 90% chance of flipping 3 heads in a row?,"If you were to flip a fair coin independently over and over, hoping to get 3 heads in a row, how many coin flips would it take for you to have a 90+% chance of having succeeded? The way I've been thinking about this problem is as a Markov Chain with states: 0, 1, 2, and 3 heads in a row, where 3 is the absorptive state. The transition matrix T is: $$T=\begin{bmatrix}0.5&0.5&0&0\\0.5&0&0.5&0\\0.5&0&0&0.5\\0&0&0&1\end{bmatrix}$$ So roughly, what I'd like to be able to do is solve for n in the following equation  (let $k_1, k_2$ , and $k_3$ be arbitrary constants): $$\begin{bmatrix}1&0&0&0\end{bmatrix}*T^n =\begin{bmatrix}k_1&k_2&k_3&0.9\end{bmatrix}$$ Through trial and error on the calculator, I can figure out that [1 0 0 0] $*T^{30}\approx$ [0.050 0.027 0.014 0.908]. However, I'd like to be able to do this in a systematic way that could be applied to other Markov Chains, but I'm not sure how. Thanks for any help!","['markov-chains', 'probability']"
4605119,Find the closest point to the origin of $\{\alpha x + \beta y + \gamma z = c\}\cap\{x+y+z=1\}.$,"On the line given by this intersection: $$\{\alpha x + \beta y + \gamma z = c\}\cap\{x+y+z=1\}.$$ We need to find the closest point to the origin. My attempt:
Let's look at $f(x,y,z)=x^2 + y^2 + z^2$ , this is the euclidean norm squared. Now, setting the constraints $g_1(x,y,z)=x+y+z-1$ and $g_2(x,y,z)=\alpha x + \beta y + \gamma z - c$ .
We can denote that the intersection is a compact set since it's closed and blocked. Hence, by Weierstrass theorem we can guarantee that $f$ has minimum and maximum in the intersection above. Note that $f$ , $g_1$ and $g_2$ are $C^1$ functions. In addition, $\nabla g_1=(1,1,1)$ and $\nabla g_2=(\alpha,\beta,\gamma)$ are linearly dependent if and only if $\alpha = \beta = \gamma$ . If they're equal we have two cases: One is $\alpha=c$ , in this case the intersection isn't empty iff $\alpha=c=1$ and then we'll get that $(x,y,z)=(\frac{1}{3}, \frac{1}{3},\frac{1}{3})$ . The other is that $\alpha \ne c$ and then the intersect is empty and there isn't a point on which we can say it is the closest to the origin since there're no points at all.
Under the assumption that at least one of then is different from the rest, we can use Lagrange multiplier theorem and obtain the following: $\nabla f=\lambda_1\nabla$$g_1+\lambda_2\nabla$$g_2$ . Denote this equations system: \begin{cases}2x=\lambda_1+\alpha\lambda_2 \\ 2y=\lambda_1+\beta\lambda_2 \\ 2z=\lambda_1+\gamma\lambda_2 \\ x+y+z-1=0 \\ \alpha x + \beta y + \gamma z - c=0\end{cases} .
I got stuck here, thanks in advanced!","['multivariable-calculus', 'calculus', 'lagrange-multiplier']"
4605120,Decompose $\mathrm{erf}(x-y)$ into pairs of separable functions,"What would be the most accurate way to decompose $\mathrm{erf}(x-y)$ into pairs of separable functions?
Or in another word, we would like to find $f_i(x)$ and $g_i(y)$ such that with $$
\mathrm{erf}(x-y) \approx \sum_{i=1}^N f_i(x)g_i(y),
$$ the approximation error decreases if we increase $N$ . What I have done I have considered using Taylor series of $\mathrm{erf}$ around $0$ , which is $$
\mathrm{erf}(x - y) = \frac{2}{\sqrt{\pi}}\sum_{n=0}^\infty \frac{(-1)^n (x-y)^{2n+1}}{n!(2n+1)},
$$ where $(x - y)^n$ can be expanded and be decomposed into $\sum_i f_i(x) g_i(y)$ .
However, this naive Taylor series approach requires a lot of decomposed terms to get a reasonable accuracy, especially for large $|x - y|$ .
Also, for each Taylor series term (i.e., for each value of $n$ ), it requires $n + 1$ decomposition terms (e.g., $(x - y)^2$ can be decomposed into 3 terms: $x^2 - 2xy + y^2$ ).
Therefore, including 10 Taylor series terms, for example, will actually produce 110 decomposition terms. I'm looking for some decomposition that can converge relatively quickly within a few terms. For example, $\exp(-(x-y)^2 / 2)$ , can be decomposed into: $$
\exp(-(x-y)^2 / 2) = \sum_{n=0}^\infty \frac{1}{n!} \left(x^n e^{-x^2/2}\right) \left(y^n e^{-y^2/2}\right).
$$ The expression above can be obtained by expanding the square inside exp, and taking the Taylor series for $e^{xy}$ .
In this exponential case, 10 terms is enough to get error less than $10^{-3}$ for reasonable values of $x$ and $y$ .
However, I can't find the similar decomposition terms for $\mathrm{erf}$ .
Any helps are appreciated!","['error-function', 'functions', 'exponential-function', 'sequences-and-series']"
4605136,Why can the Orthogonal group be split up in this way?,"In my groups course at university, we’ve spent a while on the orthogonal group, leading up the the conclusion that $$
  \mathrm{O}_n
  =
  \mathrm{SO}_n
  \mathbin{\dot{\cup}}
  \begin{pmatrix}
    -1 &   &        &   \\
       & 1 &        &   \\
       &   & \ddots &   \\
       &   &        & 1
  \end{pmatrix}
  \mathrm{SO}_n \,.
$$ The proof given was just “because cosets partition”. I just want to know: Is this true because of a specific choice of matrix or will any matrix in $\mathrm{O}_n \setminus \mathrm{SO}_n$ do? How do we know there arent any other cosets? (I.e., that this expression covers everything in $\mathrm{O}_n$ ).","['orthogonal-matrices', 'group-theory', 'linear-groups']"
4605179,Fitting a dampening coefficient and computing a Fourier transform at the same time,"I have a brief mono audio sample (a recording of a single note played on a guitar) that looks like this: As you can see in the image, the signal is losing intensity over time. My goal is to quantify the rate of decay of this note, i.e. how quickly it is losing intensity. I can do this heuristically, e.g. by taking the ratio between the first and last peak, but this approach is not very robust because it only uses two of the sample points and is thus highly sensitive to random noise. If possible, I'd like to ""divide out"" the oscillation component of the signal (probably using a Fourier transform) and then fit an exponential decay equation to the result. This approach would make use of the entire audio sample, and therefore be less sensitive to error in an individual sample point. However, I do not know if this approach is viable, and I don't understand the details of how to carry this out. My attempt We know from differential equations that the position of a damped harmonic oscillator is given by (after scaling and translation) $$x(t) = \exp(-\lambda t) \sin( \omega t),$$ where $\lambda$ is a damping coefficient and $\omega$ is the frequency. So, one idea is to fit the equation above to my input data (e.g. using least squares). But a guitar string doesn't oscillate at just one frequency $\omega$ —my input signal is really the composite of several frequencies, which depend on the physical properties of the string and guitar body. To get frequency components from time series data, we normally use a (discrete, in this case) Fourier transform. I'm stuck because it seems like I need to fit a damped harmonic oscillator model and perform a Fourier transform simultaneously. I know how to do these tasks individually, but not at the same time. My questions Is there a standard way to estimate the decay coefficient from time series data like this? Can it done programmatically? (That is, in a way that doesn't require manually labeling peaks, specifying the fundamental frequency, etc.) I have seen references online to something called RT60 estimation, used to quantify how much sound decays in a room, but this seems like a somewhat different problem: There, the problem is to integrate impulse response data sampled from many different places in the room, whereas I have just a single time series and no notion of place within the room.","['fourier-series', 'ordinary-differential-equations', 'algorithms']"
4605182,Expected value of repeatedly betting on a coin flip?,"Edit - I just stumbled across this YouTube video that explained this phenomenon very well so thought I'd include it here for future readers Consider the following game: You start with $1000 Flip a fair coin If it's heads gain 21% , if it's tails lose 20% You can play as many times as you want My question is: How would you calculate the expected amount of money you have after N games? How many times should you play? If I consider playing a single round, the expected value would be: $$
\begin{aligned}
E[1] &= 1000 \cdot ( 0.5 \cdot 1.21  + 0.5 \cdot 0.8 ) \\
&=1005
\end{aligned}
$$ It seems to me like the rational decision would be to play the game, because you expect to end up with more money. And then after your first coin toss, the same reasoning should apply for the decision about whether to play a second time, and so on... So then it seems like you should play as many times as possible and that your amount of money should approach infinity as N does. However when I simulate this in Python I am finding that the final amount of money always tends toward zero with enough flips. How can a game where each round has a positive expected return end up giving a negative return when played many times? I read about the Kelly Criterion and think it might apply here. Calculating the geometric growth rate: $$
\begin{aligned}
r &= (1 + f \cdot b)^p \cdot (1 - f \cdot f )^{1-p} \\
&= (1 + 1 \cdot 0.21)^{0.5} \cdot (1 - 1 \cdot 0.2 )^{0.5} \\
&= 0.983870 
\end{aligned}
$$ This seems to indicate that I should not play, because the geometric growth rate is below 1. But how do I reconcile that with my reasoning above and the fact that the expected gain of playing any individual round is positive?","['gambling', 'probability']"
4605188,"Is there another simpler method to evaluate the integral $\int_0^{2 \pi} \frac{1}{1+\cos \theta \cos x} d x , \textrm{ where } \theta \in (0, \pi)?$","Using ‘rationalization’, we can split the integral into two manageable integrals as: $\displaystyle \begin{aligned}\int_0^{2 \pi} \frac{1}{1+\cos \theta \cos x} d x = & \int_0^{2 \pi} \frac{1-\cos \theta \cos x}{1-\cos ^2 \theta \cos ^2 x} d x \\= & \int_0^{2 \pi} \frac{d x}{1-\cos ^2 \theta \cos ^2 x}-\cos \theta \int_0^{2 \pi} \frac{\cos x}{1-\cos ^2 \theta \cos ^2 x} d x \\= & 4 \int_0^{\frac{\pi}{2}} \frac{\sec ^2 x}{\sec ^2 x-\cos ^2 \theta} d x+\int_0^{2 \pi} \frac{d(\cos \theta \sin x)}{\sin ^2 \theta+\cos ^2 \theta \sin ^2x} \\= & 4 \int_0^{\frac{\pi}{2}} \frac{d\left(\tan x\right)}{\sin ^2 \theta+\tan ^2 x}+\frac{1}{\sin \theta}\left[\tan ^{-1}\left(\frac{\cos \theta \sin x}{\sin \theta}\right)\right]_0^{2 \pi} \\= & \frac{4}{\sin \theta}\left[\tan ^{-1}\left(\frac{\tan x}{\sin \theta}\right)\right]_0^{\frac{\pi}{2}} \\= & \frac{4}{\sin \theta} \cdot \frac{\pi}{2} \\= & \frac{2 \pi}{\sin \theta}\end{aligned}\tag*{} $ Is there another simpler method to evaluate the integral? Your comments and alternative methods are highly appreciated.","['integration', 'calculus', 'definite-integrals', 'trigonometry']"
4605236,$ \text{sec}^2x + 3\text{cosec}^2x =8 $ Solving this trigonometric equation.,"I have found two different methods of solving this trigonometric equation : $$ \text{sec}^2x + 3\text{cosec}^2x =8 
$$ But these methods give different answers. Solution 1 $$ \text{sec}^2x + 3\text{cosec}^2x =8 $$ $$\implies \frac{1}{\text{cos}^2x} + \frac{3}{\text{sin}^2x} =8 
$$ $$\implies 3{\text{cos}^2x} + {\text{sin}^2x} =8(\text{cos}^2x ×\text{sin}^2x)$$ $$\implies 2{\text{cos}^2x} + 1 =8(\text{cos}^2x ×(1-\text{cos}^2x))$$ $$\implies 8{\text{cos}^4x}  -6\text{cos}^2x +1 =0$$ $$\implies (2\text{cos}^2x-1)(4\text{cos}^2x-1)=0
$$ $$ \implies x= 2nπ±\frac{π}{4},2nπ±(π-\frac{π}{4}),2nπ±\frac{π}{3},2nπ±(π-\frac{π}{3}),
$$ Solution 2 $$ \text{sec}^2x + 3\text{cosec}^2x =8 $$ $$ \implies 1+\text{tan}^2x + 3 + 3\text{cot}^2x =8 $$ $$\implies {\text{tan}^2x} + \frac{3}{\text{tan}^2x} =4 
$$ $$ \implies \text{tan}^4x -4\text{tan}^2x + 3 =0 $$ $$ \implies (\text{tan}^2x -1)(\text{tan}^2x-3) =0 $$ $$ \implies x=nπ±\frac{π}{3},nπ±\frac{π}{4} $$ Now my question is that which one should I reject and why ?",['trigonometry']
4605240,Paint a cube by rolling it (Puzzle Algorithm),"I stumbled across this game in Simon Tatham's puzzle app. It's called cube. The description according to the game is: You have a grid of 16 squares, six of which are blue; on one square rests a cube. Your move is to use the arrow keys to roll the cube through 90 degrees so that it moves to an adjacent square. If you roll the cube on to a blue square, the blue square is picked up on one face of the cube; if you roll a blue face of the cube on to a non-blue square, the blueness is put down again. (In general, whenever you roll the cube, the two faces that come into contact swap colours.) Your job is to get all six blue squares on to the six faces of the cube at the same time. Attached is a link to a screenshot of the game : The Puzzle is available via JavaScript , hence it can be played online. I would like to ask the Math exchange community if there is a known algorithm for solving such a problem as I haven't found anything online.","['algorithmic-game-theory', 'group-theory', 'puzzle', 'algorithms']"
4605246,Frullani like Trig integral,$$\int_{0}^{\infty}\frac{\left|\cos\left ( x-\frac{\pi}{4} \right ) \right|- \left|\cos\left ( x+\frac{\pi}{4} \right ) \right| }{x}dx=\sqrt{2}\ln(1+\sqrt{2})$$ The above integral seems to look like a frullani type integral and has a closed form in terms of natural log. I tried to indefinitely integrate it. But the closed form are in terms $\text{Si}(x)$ and $\text{Ci}(x)$ . I would highly  appreciate if there's any method or unique  substitution  to evaluate  this Integral.,"['integration', 'improper-integrals', 'calculus', 'definite-integrals']"
4605284,"Most efficient way to find the ""solution intervals"" / ""case conditions""? (problem regarding a function with absolute value notation):","I am trying to write the function $f(x) = |x^2-1|+|x|-1$ without the notation for the absolute value. ""||"" It makes logical sense to consider four cases, because each term in the absolute value notation can either be positive or negative. Therefore, we get: Case 1 (positive, positive): $f(x) = x^2-1+x-1 = (x-1)(x+2)$ Case 2 (positive, negative): $f(x) = x^2-1-x-1 = (x+1)(x-2)$ Case 3 (negative, positive): $f(x) = -x^2+1+x-1 = -x(x-1)$ Case 4 (negative, negative): $f(x) = -x^2+1-x-1 = -x(x+1)$ To use the piecewise function notation, I need the intervals for which each term is correct. $$
  f(x) = \cases{        term       & $condition\ $ \cr
                 term       & $condition\ $ \cr
                        term       & $condition\ $ \cr
term       & $condition\ $ \cr}
$$ What is the most efficient way to find the case conditions (in this particular example, and generally speaking)? (I have tried putting the term of each case equal to zero to find the roots. However, doing this, I get $5$ roots. Our expression is of order $3$ , therefore something has to be wrong/I overlooked something while trying this.) I am thankful for any input/ideas! Thank you!","['absolute-value', 'calculus', 'functions', 'algebra-precalculus', 'piecewise-continuity']"
4605339,"Dodecahedron, angle between edge and face.",In an effort to build a dodecahedron frame in Fusion360 I need to know some of the angles. Looking around I found out that the angle between an edge and a face on a regular dodecahedron is $121.7^\circ$ but I couldn't find the mathematical formula nor the way to calculate this angle. The formula is needed so the exact angle can be used so the simulation is precise. Can anyone help?,"['euclidean-geometry', 'polyhedra', '3d', 'angle', 'geometry']"
4605352,Fastest way to converge to mean,"Given a sequence of i.i.d random variables $\{X_k\}_{k=1}^N$ , from the central limit theorem, we know that $$
\left(\frac{1}{N}\sum_{k=1}^N X_k \right)- \mathbb{E}(X_1) = \mathcal{o}_p\left(1\right)
$$ Is there any way to obtain an estimator for mean with a faster rate of convergence in probability?","['statistics', 'central-limit-theorem', 'probability-theory', 'probability']"
4605367,Chain rule for matrix derivatives & multiplication of matrix with rank-3-tensor,"While studying, I noticed that in some lectures notes of mine we hand-wavingly use the chain rule formula to design a gradient descent algorithm with respect to some matrix $W \in \mathbb{R}^{m \times n}$ (i.e. the variable we want to minimize is a $m \times n$ matrix). Curious about how one would define the jacobian with respect to a matrix, I stumbled upon this question and how to represent rank-3-tensors as stacked matrices. With this, we define the Jacobian of a function $f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}^{l}$ with respect to the variable $W$ as $$\nabla_W f = \left({\frac{\partial f_i}{\partial w_{jk}}}\right)_{ijk} = \left(\left(\frac{\partial f_i}{\partial w_{j k}}\right)_{jk}\right)_{i}$$ Visually, this Jacobian consists of $l$ stacked $m \times n$ matrices each containing the individual $W$ derivatives (but fixing the output components $f_i$ ). With this we can try to prove the chain rule for matrix derivatives. Let $f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}^{l}$ and $g: \mathbb{R}^l \rightarrow \mathbb{R}^s$ . The chain rule should be $$J_W(g \circ f) = J(g)(f) \cdot J_W(f)$$ On the one hand, treating a matrix in this multiplication as a ""scalar"" and elementwise-multiplying it with the rank-3-tensor (seems natural in a module-theoretic sense) would consequently yield $$\frac{\partial g_i}{\partial w_{jk}}(f(W)) = (\nabla g_j)(f(W)) \cdot \left(\frac{\partial f_i}{\partial w_{ak}}(W)\right)_{a} = \sum\limits_{a = 1}^{l} \partial_a g_j(f(W)) \frac{\partial f_i}{\partial w_{ak}}(W)$$ On the other hand, employing the usual chain rule and some mildly sketchy argumentation, we have $$\frac{\partial}{\partial w_{jk}} (g_i(f(W))) = \frac{\partial}{\partial w_{jk}} (g_i(f_1(W),\ldots,f_l(W))) = \sum\limits_{a = 1}^{l} (\partial_a g_i(f(W)))\frac{\partial f_a}{\partial w_{jk}}(W) = (\nabla g_i)(f(W)) \cdot \left(\frac{\partial f_a}{\partial w_{jk}}(W)\right)_a$$ for arbitrary $i,j,k$ . This means our Jacobian now looks like $$\nabla_W (g \circ f) = \left(\left((\nabla g_i)(f(W)) \cdot \left(\frac{\partial f_a}{\partial w_{jk}}(W)\right)_a\right)_{jk}\right)_i = \left(\left(\nabla g_i)(f(W)) \cdot \left(\frac{\partial f_a}{\partial w_{jk}}(W)\right)_a\right)_{jk}\right)_{i}$$ By writing this out in the stacked matrix format, it seems that this calculation only differs up to a swap of indices. This is where I am stuck. If I take $s = 1$ (which corresponds to the case examined in my notes), the definition seems to fit, but my derivation of the chain rule seems to be wrong. It also might be that my understanding of tensor multiplication is not correct, such that both definition and calculation could be correct and just me not realizing it. EDIT: Reading through the (concerningly sparse) material on the net about this, I might have found out that both formulations are equivalent under the premise that matrix-tensor-multiplications (generalized by tensor contractions) are the right way to go about this. Visually, employing the stacked matrix representation again, a multiplication of a rank-3-tensor with a matrix from the left would look like this: To obtain the value at index $ijk$ , take the $i$ -th row vector of $J(g)(f)$ and multiply it by the column vector gained by taking the $(j,k)$ -th index of each submatrix of $D_W(f)$ in order (!). This should make both arguments coincide (we especially get scalar-matrix-multiplication back for $l = s = 1$ ) and it seems natural for some reason. Is my reasoning about tensor multiplication correct? Summarizing, my questions now are: Is my derivation of the chain rule up to the last point correct? If not, where exactly am I going wrong? How does the tensor multiplication work in the presumably correct statement of the chain rule above? With this I might be able to reverse engineer my error(s). Thank you for reading through this barrage of indices and matrices!","['multivariable-calculus', 'tensor-products', 'chain-rule']"
4605409,Minimize $x^2+y^2+z^2+w^2+2w(x+z)$ given certain constraints,"I am trying to figure out how to show that the minimum of $f(x,y,z,w)=x^2+y^2+z^2+w^2+2w(x+z),$ given that $x+y+z+w=1,$$ $$x+y\geq 0.7,$ and $x\geq 0, y\geq 0, z \geq 0, \text{ and } w\geq 0,$ is 0.335 and that it is achieved when $x=y=0.35,$ $z=0.3,$ and $w=0.$ Note that the partial derivatives of $f$ are $$\frac{\partial f}{\partial x}=2x+2w,$$ $$\frac{\partial f}{\partial y}=2y,$$ $$\frac{\partial f}{\partial z}=2z+2w,$$ and $$\frac{\partial f}{\partial w}=2x+2z+2w.$$ All of these are positive when the variables $x, y, z, w$ are all positive, meaning that this function is increasing with respect to all of it's variables when they are positive. I think I can safely say that the minimum must occur when $x+y=0.7$ , which means that $z+w=0.3$ . Then $y=0.7-x$ and $w=0.3-z$ . Substituting these into $f$ gives us $$g(x,z):=f(x,0.7-x,z,0.3-z)=2x^2-0.8x-2xz+0.58.$$ I believe that minimizing $f$ with the original constraints is equivalent to minimizing $g$ assuming that $0\leq x\leq 1$ and $0\leq z\leq 0.3.$ This, in fact, does lead to the right answer, but I want to make sure I haven't made a mistake. Thank you in advance.","['convex-optimization', 'maxima-minima', 'multivariable-calculus', 'calculus', 'optimization']"
4605420,How to calculate the Picard group of the following variety,"I am puzzled by the Picard group of $X=\text{Proj }k[x_0,x_1,x_2,x_3,x_4]/(x_0x_1-x_2x_3)$ .
I can calculate its Weil's divisor class group ( $\mathbb{Z}\oplus \mathbb{Z}$ ) because it is the projective cone of $\text{Proj }k[x_0,x_1,x_2,x_3]/(x_0x_1-x_2x_3)$ . And the relevant propositions have been analyzed in the second chapter of GTM 52. However, due to the singularity at $[0,0,0,0,1]$ , I cannot conclude that Picard group is the Weil's class group. I once tried to use Cech's cohomology to calculate $H^1(X,\mathcal{O}_X^*)$ . But I don't think that I can handle the difficulty in concrete calculation.",['algebraic-geometry']
4605440,Why Lebesgue measure? Why Borel σ-algebra?,"Is any measure on any σ-algebra inside the power set of $\mathbb{R}^d$ a formal definition (or generalisation) of ""volume"" in $\mathbb{R}^d$ ? What's so special about Lebesgue measure that we choose it as the standard way to assign measure to subsets of $\mathbb{R}^d$ ? What's so special about Borel σ algebra? Why not other σ-algebra? Is there a measure on the Borel σ algebra of $\mathbb{R}^d$ such that $\gamma ((a,b])$ may not be $b-a$ ? For question 2, I guess Lebesgue measure is chosen as the standard way because it's the unique measure on the Borel σ algebra of $\mathbb{R}^d$ such that $\gamma ((a,b])=b-a$ . But I'm not sure if that's the reason, I'm not even sure if the important bit is the ""Borel σ algebra"" or "" $\gamma ((a,b])=b-a$ "". Any help will be appreciated!","['measure-theory', 'lebesgue-measure', 'analysis', 'real-analysis', 'borel-sets']"
4605447,Is induction strictly required to prove finite AC?,"Consider the following proof of the (finite) AC in ZF. Assume we are given a finite, non-empty collection $\mathcal{F}$ of non-empty sets. Hence $\mathcal{F}= \{A_i : 1 \leq i \leq n\}$ for some positive integer $n$ . Since the sets are non-empty, we have that $\exists x_1 \exists x_2 \cdots \exists x_n \ \forall i \in \{1,2,\cdots ,n\} \  x_i \in A_i$ . Hence by existential instantiation $y_i \in A_i$ (for some new symbol $y_i$ ). Define the choice function $A_i \mapsto y_i$ . I have seen variants of this proof for instance on MathOverflow . Some time ago I recall seeing a comment here by Asaf Karagila (which I cannot cite off the top of my head) which states that the above argument is in fact incorrect for some technical reason related to non-standard integers and some issue related to Skolem's paradox, and that the ""correct"" proof of finite AC in ZF requires induction. I would ask for a detailed explanation of the problem with the above proof, which I still don't quite understand. I would also like to know how induction ""saves the day"" so to speak: isn't the proof in the first paragraph just the inductive proof ""unrolled""?","['elementary-set-theory', 'axiom-of-choice']"
4605454,"How to find the domain of $\,\csc^{-1}\!\!\left(\frac{x+1}{x}\right)\;?$","How to find the domain of $$\csc^{-1}\left(\frac{x+1}{x}\right)$$ My attempt : the cosecant function lies in $\,\Bbb R\setminus(-1,1)\;$ , so first checking the range of $\;\dfrac{x+1}{x}\;$ , we find that it covers all reals but $1$ . Thus the domain of the function must be $\,\Bbb R\setminus(-1,1)$ . But this is wrong. Why is this so ? Source : JEE Mains ,2021, august attempt","['trigonometry', 'inverse-function']"
4605475,Formally defining and understanding the summation over a finite set,"Let $S$ be a finite, nonempty set. Then there exists a positive integer $n$ and a bijective map $\phi: [n]\rightarrow S$ , where $[n]:=[1, n]\cap \mathbb{Z}$ . Let $(Y, +)$ be an abelian group, and $f: S\rightarrow Y$ . It seems that the summation over a set is defined to be $$ \sum_{x\in S} f(x):=\sum_{i=1}^n  f(\phi(i)),$$ even though Wikipedia gives it as an identity (as if it would be a consequence of some more elementary properties; see the fourth identity in the Wikipedia page). It turns out that this is well-defined as the value does not depend on the choice of the bijection $\phi$ . So we have a rigorous definition for the sum. However, it is also important to have an intuitive understanding of it. We want $\Sigma_{x\in S} f(x)$ to mean that $f$ is evaluated at every element of $S$ precisely once, and those evaluations are added together; no other terms are added. What would be the easiest way to verify that the formal definition corresponds to intuition? Summations over a set are done all the time even without a thought, but I had never encountered the formal definition before. The definition seems to be somewhat complicated in the sense that it involves bijections, but we are dealing with something which you would expect to be very elementary.","['elementary-set-theory', 'summation', 'definition', 'intuition']"
4605488,How can I show by using Hahn Banach separation theorem?,"The convex hull of a set of points S in n dimensions is the intersection of all convex sets containing $S$ . For $N$ points $p_1, ..., p_N$ , the convex hull $C$ is then given by the expression $$ C = \left\{ \sum_{j=1}^ N \lambda _j p_j : \lambda _j \geq 0 \quad \text{for all} \, j \, \text{and} \, \sum_{j=1}^ N \lambda _j =1  \right\}. $$ Let $F$ be a real Banach space, $\mu$ be a Borel probability measure on a compact Hausdorff space $X$ , and let $ f : X \rightarrow F$ be a continuous mapping. Let linear continuous functionals $ \phi _1 , \ldots ,\phi_n \in F'$ and $$ \nu_j = \int _{X}\phi_j \circ f d\mu \quad \textrm{for} \; j = 1,\ldots,n.$$ Let $ T \in L({F; \mathbb{R}^n})$ be defined by $$ Ty := (\phi_1(y),\ldots\phi_n(y)) \quad \textrm{for every} \; y \in F. $$ How can I  show that  by using the Hahn-Banach separation theorem, $$ (\nu_1,\ldots \nu_n) \in co(T\circ f(X)) = T(co(f(X))), $$ where $co(B)$ denotes the convex hull of the set B ?","['complex-analysis', 'banach-spaces', 'multilinear-algebra', 'functional-analysis']"
4605507,A question concerning linear algebra,"(Multiple options could be correct!) Q. Let $A$ be a $4\times 4$ matrix over $\mathbb R$ such that $\rho(A)=2$ , and $A^3=A^2\ne O$ . Suppose that $A$ is not diagonalizable.
Then (a) One of the Jordan blocks of the Jordan Canonical form of $A$ is $\pmatrix{ 0 & 1 \\ 0 & 0}.$ (b) $A^2=A\ne O.$ (c) $\exists$ a vector $v$ such that $A^2v=O.$ (d) The characteristic polynomial of $A$ is $x^4-x^3.$ Here, $\rho(A)$ means the rank of $A,$ and $O$ denotes the zero matrix of order $4$ . My attempt: If $f(x)=x^3-x^2=x^2(x-1),$ then $f(A)=O.$ So, the minimal polynomial of $A, m(x)$ (say), must divide $f(x).$ Since $A$ is not diagonalizable, the only possible choice for $m(x)$ is $x^2(x-1).$ This imples that there is a Jordan block of size $2$ in the Jordan Canonical form of $A$ corresponding to the eigen value $0$ of $A$ . Therefore, option (a) is correct. If $A^2=A\ne O$ were true, that would mean that the matrix $A$ is idempotent, hence diagonalizable. However, it is not so. Thus, we see that option (b) is false. (right?) $A^2v=O$ represents a homogeneous system of linear equations. As such, it is always consistent. So, option (c) is correct! (right?) Since $m(x)\mid c(x),$ the characteristic polynomial of $A.$ There are two possible choices for $c(x)$ , viz., $x^3(x-1)$ and $x^2(x-1)^2.$ In addition, the geometric multiplicity of the eigen value $0, \gamma(0)$ (say), equals the nullity of $A-0I_4,$ that is: $\gamma(0)=\eta(A-0I_4)=\eta(A)=4-\rho(A)=4-2=2.$ But, $A$ is not diagonalizable. Therefore, the algebraic multiplicity of the eigen value $0$ , $\alpha(0)$ (say), must not equal $\gamma(0).$ Now, we find that among the two feasible choices for $c(x),$ only the former one, i.e, $c(x)=x^3(x-1)=x^4-x^3,$ satisfies this condition. Because $\alpha(0)=3\ne 2=\gamma(0)$ in this case. Hence, option (d) is correct as well. (right?)","['matrices', 'linear-algebra', 'linear-transformations']"
4605510,prove that $13/42\sum_{x=0}^{29} (x+1) \dfrac{{29\choose x}}{{41\choose x}} = 43/14$,"Prove that $13/42\sum_{x=0}^{29} (x+1) \dfrac{{29\choose x}}{{41\choose x}} = 43/14$ . If I were to guess a generalization, I'd say that for positive integers $m,n,$ with $m\leq n-1$ we have $\sum_{x=0}^m (x+1) \dfrac{{m\choose x}}{{n-1\choose x}} \dfrac{n-m}{n} = \dfrac{n+1}{n-m+1}.$ The latter sum can be described in probabilistic terms as the expected value of a certain random variable X, described as follows. Suppose we have a deck of $m$ red cards and $n-m>0$ blue cards and all cards are distinct. We shuffle the deck randomly so that each permutation is equally likely to occur. We then select the top card and remove it until we get a blue card. Let X be the number of cards we removed from the top. The expected value calculation follows directly from the definition; the probability of drawing $x+1$ cards is $m(m-1)\cdots (m-x+1) (n-m) (n-x-1)!/n!,$ as there are $m(m-1)\cdots (m-x+1) $ ways to draw the first x red cards, $n-m$ ways to select the $(x+1)$ th blue card, and $(n-x-1)!$ ways to arrange the remaining cards, and there are of course n! total permutations. Using the definition of expected value, we can easily obtain the required sum. However evaluating the sum is another issue in itself, and I'm unsure of any good ways to make progress. It seems intuitive that the expected value would be $\dfrac{m}{n-m+1} + 1,$ but I'm not sure how to formally justify this. I'm not sure if there are any useful binomial identities for proving the stated equality. I know some tricks involving binomial coefficients that involve integration and differentiation, but they don't seem useful for this problem (e.g. one can evaluate $\sum_{x=0}^n {n\choose x} \dfrac{1}{x+1}$ easily using integration and the resulting sum would be $\dfrac{2^{n+1}-1}{n+1}$ ).","['contest-math', 'summation', 'binomial-coefficients', 'discrete-mathematics', 'probability']"
4605535,Gradient flows: is this a typo in a discretization scheme?,"I'm reading below theorem from this lecture note. Theorem 4.5. Let $H$ be a Hilbert space over $\mathbb{R}$ . If $\varphi: H \rightarrow \mathbb{R}$ is differentiable and convex, then for every $u \in H$ there exists a unique $y:[0, \infty) \rightarrow H$ such that $$
\begin{aligned}
& y^{\prime}(t)=-\nabla \varphi(y(t)), \quad t \geq 0, \\
& y(0)=u.
\end{aligned}
$$ Idea of proof. Fix $h>0$ . Discretize the differential equation with Euler's implicit scheme, $$
\frac{y((n+1) h)-y(n h)}{h}=-\nabla \varphi(\color{red}{y(n h)}) .
$$ Then, with $y_n=y(n h)$ , $$
y_{n+1}+h \nabla \varphi\left(\color{red}{y_{n+1}}\right)=y_n,
$$ so that $$
y_{n+1}=(I+h \nabla \varphi)^{-1}\left(y_n\right) .
$$ One of the difficulties is to show that the function $x \mapsto x+h \nabla \varphi(x)$ is invertible. Then $$
J_h(x):=(I+h \nabla \varphi)^{-1}(x), \quad x \in H,
$$ is called the resolvent associated to $\nabla \varphi$ . We obtain $$
y_n=J_h^n(u),
$$ and this is hoped to be a good approximation for $y(n h)$ . The next steps are to show that $$
y(t):=\lim _{k \rightarrow \infty} J_{t / k}^n(u)
$$ exists and that the function $y$ thus defined is the unique solution. My understanding It seems to me $$
\frac{y((n+1) h)-y(n h)}{h}=-\nabla \varphi(\color{red}{y(n h)})
$$ implies $y_{n+1}+h \nabla \varphi\left(\color{red}{y_{n}}\right)=y_n$ , and not $y_{n+1}+h \nabla \varphi\left(\color{red}{y_{n+1}}\right)=y_n$ as written in the lecture note. Then it should be $y_{n}=(I-h \nabla \varphi)^{-1}\left(y_{n+1}\right)$ . Could you confirm if my understanding is correct, or I miss something else?","['hilbert-spaces', 'gradient-flows', 'derivatives', 'ordinary-differential-equations']"
4605559,Inductive defintion for real numbers,"I was learning about inductive definitions and an example was given for the natural numbers : \[  0 \in N \wedge n \in N\implies n+1 \in N  \] 


I found a definition for all integers and all rational numbers (please correct me if I'm wrong) : \[  0 \in Z \wedge n \in Z\implies n+1 \in Z \wedge n-1 \in Z  \] 
\[ and\]
\[ 0 \in Q \wedge n \in Q \implies n+1\in Q \wedge n-1 \in Q \wedge n \ne 0 \implies \forall x \in Q : \dfrac{x}{n} \in Q \]


I could however not come up with any inductive definition for all real numbers or all imaginary numbers. Does anyone have knowledge of such inductive definition? Thanks in advance.","['algebra-precalculus', 'logic', 'induction']"
4605579,How do we know that nullspace and row space of a matrix are orthogonal complements?,"I'm following along Ch. 4 entitled ""Orthogonality"" in Gilbert Strang's Introduction to Linear Algebra . Here are some of the initial results in that chapter Definition: two subspaces of a vector space are orthogonal if every vector in the first subspace is perpendicular to every vector in the second subspace Every vector $x$ in the nullspace of a matrix $A$ is perpendicular to every row of $A$ . Similarly, every vector $y$ in the nullspace of $A^T$ is perpendicular to every column of $A$ . Definition: The orthogonal complement of a subspace $V$ contains every vector that is perpendicular to $V$ By this definition, the nullspace is the orthogonal complement of the
row space. Every $x$ that is perpendicular to the rows satisfies $Ax=0$ . My question is about this last quote. How do we know that all vectors not in the nullspace are in the row space? Every vector $x$ that satisfies $Ax=0$ is in the nullspace of $A$ , and is perpendicular to every row of $A$ . Thus, $x$ is also perpendicular to linear combinations of rows of $A$ and is thus perpendicular to every vector in the row space. If $A$ is $m$ by $n$ with rank $r$ , the nullspace has dimension $n-r$ and the row space has dimension $r$ . Vectors in the nullspace and in the row space are in $\mathbb{R}^n$ . How do we know that not only are these two subspaces orthogonal, they are also orthogonal complements because there is no vector in $\mathbb{R}^n$ that is perpendicular to the vectors in the nullspace that is not in the row space, and there is no vector perpendicular to the vectors in the row space that is not in the null space?","['orthogonality', 'linear-algebra', 'vector-spaces']"
4605602,"In the game of Repeat-a-Number, who wins?","I devised a game recently. There is a string of numbers, and each player extends the string by appending a number to the end based on the current last number of the string. The string starts as the single number $1.$ If the last number of the string is $x,$ the player can append $2x+1,2x+5,$ or $\lfloor \frac{x}{3} \rfloor.$ The goal is to repeat a number already in the string. Here's an example game: $1, 7, 2, 9, 19,$ 43 $, 14, 33, 11,$ 3 $, 1.$ The first player was trapped. The italicized number is the trapped player's mistake, the moves after that by the trapped player before the bolded number are forced moves (by forced moves I mean if the trapped player plays anything else, the other player wins on the next turn), and the bolded number is basically the player trapped resigning. In the general case, is it guaranteed that somebody wins with perfect play, or would two perfect players battle forever? Edit: Trap #2: $1, 7, 2, 9, 19, 39, 13, 31, 10, 25, 55,$ 111 $, 37, 79, 26,$ 8 $, 2.$ This is also winning for the second player. Trap #3: $1, 7, 2, 9, 19, 39, 13, 31, 10, 25, 55, 18, 37, 79, 26, 53, 107,$ 35 $, 11,$ 3 $,1.$ I'm pretty sure this trap using $33, 34,$ or $35$ can manifest itself in many situations. This is also winning for the second player. I have been analyzing a different opening and found Trap #4: $1, 7, 15,$ 35 , $11, 27, 9, 19, 6,$ 2 , $9$ . Here's Trap #5 in yet ANOTHER opening: $1, 7, 19,$ 6 , $2,$ 0 , $0$ .","['game-theory', 'number-theory', 'infinite-games', 'combinatorial-game-theory']"
4605619,"Show that $f(X_{1},...,X_{n})\in\mathcal G$","Suppose $X_{1},\cdots,X_{n}$ are independent random variables from $(\Omega,\mathcal{F})$ to $(\mathbb{R},\mathcal{B}(\mathbb{R}))$ ; $f:\mathbb R^{n}\mapsto \mathbb R$ and $f\in \mathcal{B}(\mathbb{R}^{n})/\mathcal{B}(\mathbb{R})$ . Define $\mathcal{F_{i}}:=\sigma(X_{i})$ where $\sigma(X_{i})=X_{i}^{-1}(\mathcal{B}(\mathbb{R})),i=1,...,n$ and $\mathcal{G}:=\sigma(\cup_{i=1}^{n}\mathcal{F_{i}}).$ Show that $f(X_{1},...,X_{n})\in\mathcal G$ . For any $S\in\mathcal B(\mathbb R)$ , since $f$ is a Borel measurable function, $f^{-1}(S)\in\mathcal B(\mathbb R^n)$ obviously .But I don't understand that  why we have $\{\omega\in\Omega:(X_{1}(\omega),...,X_{n}(\omega))\in f^{-1}(S)\} \in \mathcal G ?$","['measure-theory', 'probability-theory', 'random-variables']"
4605651,Why does solving this integral using trigonometric substitution lead to the wrong answer?,"I was solving the integral $$\int \frac{\sqrt{x^2 - 16}}{x} \, dx,$$ and I admittedly attempted to solve it blindly using trigonometric substitution: $$\begin{align}
&\int \frac{\sqrt{x^2 - 16}}{x} \, dx, \quad \text{let } x = 4\sec\theta \implies dx = 4\sec\theta\tan\theta \, d\theta \quad(\text{meaning} \ \ \theta=\sec^{-1}\frac{x}{4})\\[0.5em]
&\int \frac{\sqrt{16\sec^2\theta - 16}}{4\sec\theta} \cdot 4\sec\theta\tan\theta = \int 4\tan^2\theta \, d\theta = \int 4(\sec^2\theta - 1) \, d\theta = 4\tan\theta - 4\theta + c \\[0.5em]
&4\tan\theta - 4\theta + c \ \ \text{becomes} \ \ \sqrt{x^2-16} - 4\sec^{-1}\Bigl(\frac{x}{4}\Bigr) + c \ \ \text{so} \ \ \fbox{$\int \frac{\sqrt{x^2 - 16}}{x} \, dx = \sqrt{x^2-16} - 4\sec^{-1}\Bigl(\frac{x}{4}\Bigr) + c$}
\end{align}$$ So $\sqrt{x^2-16} - 4\sec^{-1}\Bigl(\frac{x}{4}\Bigr) + c$ is the family of antiderivates of $\frac{\sqrt{x^2 - 16}}{x}$ , so if $$F(x) = \sqrt{x^2-16} - 4\sec^{-1}\Bigl(\frac{x}{4}\Bigr),$$ then $$F'(x) = \frac{\sqrt{x^2 - 16}}{x}$$ , right? However, as the graph below shows, that's not the case. For negative values, the two functions $F'$ and $f$ do not match at all. When setting $$F(x) = \sqrt{x^2 - 16} - 4\tan^{-1}\biggl(\frac{\sqrt{x^2 - 16}}{4}\biggr),$$ then the two functions do seem to agree. When I tried to ask an instructor, I was told that we always represent our solutions in terms of either inverse tangent or inverse sine, but not inverse secant. The reason I confidently chose to represent theta as inverse secant is that that's how we always did it, so when I compared my answers to the ones provided, I was extremely confused. I fear that it might be an issue with the domain of the inverse secant, but I'm not exactly sure how.","['integration', 'indefinite-integrals', 'calculus']"
4605653,Uniqueness of high-dimensional derivative [Zorich's book],"I was reading the definition of the high-dimensional derivative from Zorich's book and I'd like to ask a question about the uniqueness of high-dimensional derivative. Definition 1. A function $f:E\to \mathbb{R}^n$ defined on a set $E\subset \mathbb{R}^m$ is differentiable at the point $x\in E$ ,
which is a limit of $E$ , if $$f(x+h)-f(x)=L(x)h+\alpha(x;h), \quad \quad \quad(1)$$ where $L(x):\mathbb{R}^{m}\to \mathbb{R}^n$ is a function that is linear in $h$ and $\alpha(x;h)=o(h)$ as $h\to 0, x+h\in E$ . The linear function $L(x):\mathbb{R}^m\to \mathbb{R}^n$ in $(1)$ is
called the differential of the function $f:E\to
 \mathbb{R}^n$ at the point $x\in E$ . Claim. If $E\subset \mathbb{R}^m$ and $f:E\to\mathbb{R}^n$ is differentiable at the point $x\in E$ , which is a limit point of $E$ , then the differential of $f$ at the point $x\in E$ is unique. Proof. Let's assume there are $A,B:\mathbb{R}^m\to \mathbb{R}^n$ linear mappings such that $$f(x+h)-f(x)=Ah+\alpha(x;h) \quad \text{and} \quad f(x+h)-f(x)=Bh+\beta(x;h),$$ where $\alpha(x;h)=o(h)$ and $\beta(x;h)=o(h)$ as $h\to 0, x+h\in E$ . Remark. Here the base is $h\to 0, x+h\in E$ . The elements of this base are $B_{\delta}:=\{h\in \mathbb{R}^m: 0\leq \lVert h\rVert<\delta, x+h\in E\}$ for $\delta>0$ . Indeed, it satisfies to the definition of base. Let $C:=A-B$ , then $C:\mathbb{R}^m\to \mathbb{R}^n$ is a linear mapping also. Then it is not difficult to show that $$\lim \limits_{\substack{h\to 0 \\ x+h\in E}} \frac{\lVert Ch\rVert}{\lVert h\rVert}=0.$$ Let $y$ be a nonzero vector in $\mathbb{R}^m$ . If we can show that $\lim \limits_{\substack{\mathbb{R}\ni t\to 0 \\ x+ty\in E}} \frac{\lVert C(ty)\rVert}{\lVert ty\rVert}=0,$ then we are done. Indeed, in that case it follows that $\lVert Cy\rVert=0$ for all $y\neq 0$ , which implies that $Cy=0$ for all $y$ and hence $Ay=By$ . Therefore, linear mappings $A$ and $B$ are equal. Question. How to prove that if $\lim \limits_{\substack{h\to 0 \\ x+h\in E}} \frac{\lVert Ch\rVert}{\lVert h\rVert}=0$ , then $\lim \limits_{\substack{\mathbb{R}\ni t\to 0 \\ x+ty\in E}} \frac{\lVert C(ty)\rVert}{\lVert ty\rVert}=0$ ?? It is certainly true if $x$ is an interior point of $E$ but it does not look correct to me because we claim that $x$ is the limit point of $E$ . Let me try to explain why do I think so. Let $\varepsilon>0$ be given. Take $\delta_0=\frac{\delta}{\lVert y\rVert}>0$ , where $\delta>0$ comes from the first limit. Then for any $t\in \mathbb{R}$ such that $0<|t|<\delta_0,\ x+ty\in E$ we'll get that $\frac{\lVert C(ty)\rVert}{\lVert ty\rVert}<\varepsilon$ . It seems that we are done but how do we know that such $t$ exists?
I mean how do we know that $\{t\in \mathbb{R}: 0<|t|<\delta_0, x+ty\in E\}\neq \varnothing$ ? Thank you so much for your help!","['multivariable-calculus', 'linear-algebra', 'vector-spaces', 'real-analysis']"
4605669,"$SL(n,\mathbb{C})\rightarrow GL(2n,\mathbb{R})$ reduction of frame bundle","When we study the frame bundle of a n-dimensional Riemannian manifold $M$ we start with a principal $GL(n,\mathbb{R})$ bundle over $M$ . There are a series of topological obstructions to reducing the structure. If the bundle is orientable we have a reduction $$GL^{+}(n,\mathbb{R})\rightarrow GL(n,\mathbb{R}) $$ If the manifold admits a metric (always since we're talking Riemannian), then we get: $$O(n,\mathbb{R})\rightarrow GL(n,\mathbb{R})$$ We call it a reduction to the orthonormal frame bundle. Suppose we look at instead: $$SL(n,\mathbb{R})\rightarrow GL(n,\mathbb{R})$$ Then we have a volume form on our manifold. I'm familiar with the idea, what I'm wondering about is a reduction of the form: $$SL(n,\mathbb{C})\rightarrow SL(2n,\mathbb{R})$$ for a manifold of even dimension. What does that correspond to? What are the topological obstructions to it's existence? Or we might consider instead: $$GL(n,\mathbb{C})\rightarrow GL(2n,\mathbb{R})$$ is this just the existence of a complex structure, or is there more, what kind of frames does this reduction of the frame bundle correspond to?","['principal-bundles', 'algebraic-topology', 'differential-geometry']"
4605679,Finding a Digit of One Root of $x\sqrt{8}+\frac{1}{x\sqrt{8}}-\sqrt{8}$ Knowing The Digit of The Other Root,"Consider $f(x)=x\sqrt{8}+\frac{1}{x\sqrt{8}}-\sqrt{8}$ . The function has 2 real roots, say, $x_1$ and $x_2$ . If the $1994$ th digit in the decimal expansion of $x_1$ is $6$ , what is the $1994$ th digit in the decimal expansion of $x_2$ ? This is question 1 of the 34 th Swedish Olympiad (1994). Either by explicitly finding the roots, or by Vieta's relations, we get that $x_1+x_2=1$ . Now, if we write $1$ as $0.999\cdots$ , we see that the corresponding digits in the decimal place add up to $9$ . That is, the $m$ th digit of $x_1$ plus the $m$ th digit of $x_2=9$ , for $m\geq2$ . This means that the $1994$ th digit of $x_2$ is $3$ . Is this correct? For some reason, I can't find the answer/solution anywhere. Also, if we have $a,b\in\mathbb{R}$ , and $(a+b)\in\mathbb{Z}$ , then what can be said about the decimal expansion of $a$ and $b$ ? For example, $\pi=3.1415\cdots$ , and $1-\pi=-2.1415\cdots$ . We can see that the digits in the decimal expansion of $\pi$ and $1-\pi$ are equal (except for the first digit, of course). Can this be generalised? Edit: I thought it would be helpful to include a screenshot that demonstrates that corresponding digits in the decimal expansions of $x_1$ and $x_2$ add up to $9$ :","['contest-math', 'algebra-precalculus', 'decimal-expansion']"
4605683,What does a function of its own arc length look like?,"Introduction What does a function of its own arc length look like? A strange question for sure, but first let me elaborate:
Imagine a function that starts at the point $\left( 0, 0 \right)$ . If we now assume that the two closest (from the left and right) points also have $y = 0$ , then we can take the closest point of the function with a value which has the distance between the two points. If we now want to have the point after this, it has the value of the length of the previous curve (from $0$ ). This goes on and on, so we can say that the $y$ -value of a point of the function is the length of that function from $0$ to the point infinitely close to the point. Now you might ask yourself why you should look for something. I don't have a plan but it looks like fun and I couldn't find anything online about it. My Thoughts $f\left( n \right) \in \mathbb{R}$ and $2 < n \in \mathbb{N}$ Since I don't see an obvious solution, I would first try to find such a function for $f\left( n \right) \in \mathbb{R}$ and $n \in \mathbb{N}$ . Since the starting point is $\left( 0, 0 \right)$ aka $f\left( 0 \right) = 0$ , we can already take $\left( 0, 0 \right)$ as a point of the function. The next point would be at $n = 1$ , which due to the function having no length (which has length $0$ ) also gets the function value $0$ aka $f\left( 1 \right) = 0$ , so we get $\left( 1, 0 \right)$ . The next point would be at $n = 2$ , which by virtue of the function having length as the distance between the two previous two points also has the function value as that length. With this we'll get $\left( 3, 1 \right)$ aka $f\left( 3 \right) = 1$ . For the next points we do the same, only that instead of just calculating the newly added length, we also add it to the existing one, which gives us a recursiv formula: $$
\begin{align*}
f\left( n \right) &= f\left( n - 1 \right) + \sqrt{\left( \left( n - 1 \right) - \left( n - 2 \right) \right)^{2} + \left( f\left( n - 1 \right) - f\left( n - 2 \right) \right)^{2}}\\
f\left( n \right) &= f\left( n - 1 \right) + \sqrt{\left( n - n - 1 + 2 \right)^{2} + \left( f\left( n - 1 \right) - f\left( n - 2 \right) \right)^{2}}\\
f\left( n \right) &= f\left( n - 1 \right) + \sqrt{\left( 1 \right)^{2} + \left( f\left( n - 1 \right) - f\left( n - 2 \right) \right)^{2}}\\
f\left( n \right) &= f\left( n - 1 \right) + \sqrt{1 + \left( f\left( n - 1 \right) - f\left( n - 2 \right) \right)^{2}}\\
\\
f\left( n \right) &= f\left( n - 1 \right) + \sqrt{1 + \left( f\left( n - 1 \right) - f\left( n - 2 \right) \right)^{2}} \tag{1.}\\
\end{align*}
$$ with the graph (from $0$ to $4$ ): $f\left( x \right) \in \mathbb{R}$ , $\Delta x \in \mathbb{Q}$ and $x - \Delta x > 2$ where $\Delta x$ is the distance between $x$ -values ​​of the two closest points Since the principle worked well for $f\left( n \right) \in \mathbb{R}$ and $n \in \mathbb{N}$ I would simply want to apply it to $\lim_{{\Delta x} \to {0}^{+}} \Delta x, \Delta x \in \mathbb{Q}$ for decreasing distances between $x$ -values ​​of the two closest points. With some work, the logic behind $\left( 1. \right)$ and the help of some more vector addition I found the generalized recursive formula: $$
\begin{align*}
f\left( x \right) &= f\left( x - \Delta x \right) + \sqrt{\left( \Delta x \right)^{2} + \left( \Delta f \right)^{2}}\\
f\left( x \right) &= f\left( x - \Delta x \right) + \sqrt{\left( \left(  x - 1 \cdot \Delta x \right) - \left(  x - 2 \cdot \Delta x \right) \right)^{2} + \left( f\left( x - 1 \cdot \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}}\\
f\left( x \right) &= f\left( x - \Delta x \right) + \sqrt{\left( \left(  x - \Delta x \right) - \left(  x - 2 \cdot \Delta x \right) \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}}\\
f\left( x \right) &= f\left( x - \Delta x \right) + \sqrt{\left( x - x - \Delta x + 2 \cdot \Delta x \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}}\\
f\left( x \right) &= f\left( x - \Delta x \right) + \sqrt{\left( \Delta x \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}}\\
\\
f\left( x \right) &= f\left( x - \Delta x \right) + \sqrt{\left( \Delta x \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}} \tag{2.}\\
\end{align*}
$$ with the graph: (from $x = 0$ to $x = 4$ , from $y = 0$ to $y = 8$ and $\Delta x \in \left\{ {\color{Red} 1}, {\color{Blue} 0} {\color{Blue} .} {\color{Blue} 5}, {\color{Purple} 0} {\color{Purple}.} {\color{Purple} 2} {\color{Purple} 5}, {\color{Black} 0} {\color{Black} .} {\color{Black} 1} {\color{Black} 2} {\color{Black} 5} \right\})$ : It seems to me that if we let $\Delta x \to 0^{+}$ , $f\left( x \right)$ with $x \to 1^{+}$ itself goes to $+\infty$ , which I think is kinda cool as I wasn't expecting it. $y\left( x \right) \in \mathbb{R}$ , $x \in \mathbb{R}$ , $\lim_{{\Delta x} \to {0}^{+}} \Delta x$ and $\Delta x \in \mathbb{R}$ where $\Delta x$ is the distance between $x$ -values ​​of the two closest points If we now extend the formula from $\left( 2. \right)$ by the fact that $\Delta x$ should approach $0$ , we get the recursive formula: $$
\begin{align*}
y\left( x \right) &:= \lim_{{\Delta x} \to {0^{+}}} f\left( x \right) = \lim_{{\Delta x} \to {0^{+}}} \left[ f\left( x - \Delta x \right) + \sqrt{\left( \Delta x \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}} \right]\\
y\left( x \right) &\equiv \lim_{{\Delta x} \to {0^{+}}} \left[ f\left( x - \Delta x \right) + \sqrt{\left( \Delta x \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}} \right]\\
\\
y\left( x \right) &\equiv \lim_{{\Delta x} \to {0^{+}}} \left[ f\left( x - \Delta x \right) + \sqrt{\left( \Delta x \right)^{2} + \left( f\left( x - \Delta x \right) - f\left( x - 2 \cdot \Delta x \right) \right)^{2}} \right] \tag{3.}\\
\end{align*}
$$ Now comes the challenge... Finding an explicit function formula. I know that we can calculate the length of a function $f$ in $\left[ a, b \right]$ using ""the arc length"" formula $\operatorname{arc length}\left( a, b, \frac{\operatorname{d}y}{\operatorname{d}x} \right) = \int_{a}^{b} \sqrt{1 + \left( \frac{\operatorname{d}y}{\operatorname{d}x} \right)^{2}} \operatorname{d}x$ if $y$ is continuously differentiable in $x \in \left[ a, b \right]$ but I don't know how to deal with it here. I am grateful for every help, correction and suggestion.","['arc-length', 'analysis', 'real-analysis', 'calculus', 'infinitesimals']"
4605697,Example of functions where L'Hospital fails with limit at finite value due to nonexistence of limits of derivatives but the actual limit exists.,"I've seen that L'Hospital's Rule fails for functions such as $$\lim_{x \to \infty} \frac{x}{x+\sin(x)}$$ since the limit of the derivatives oscillates. But all the examples I've seen so far have been limits at infinity. Are there any examples of functions where the actual limit exists, the limit is evaluated at a finite value but the limit of the derivatives is not equal to the actual limit? I've been thinking about the functions $f(x)=1/x+\sin(1/x)$ and $g(x)=1/x$ and taking the limit as $x\to 0$ but clearly these functions are not differentiable at an interval containing zero so it doesn't work.","['limits', 'calculus', 'real-analysis']"
4605744,Rotation matrix from given axis,"Let $T\colon\mathbb{R^3}\to\mathbb{R^3}$ denotes linear transformation which rotates by $\frac{\pi}{3}$ counter-clockwise along the vector $u=(1,1,1)$ . If $T(0,1,0)=(a,b,c)$ , Find $3a^2+b^2+c^2$ . My Attempt Consider a plane which contains a point $(0,1,0)$ and uses $u$ as normal vector. Then I got : $x+y+z=1$ . Since $T$ is rotation, $T(0,1,0)=(a,b,c)$ must be on the same plane $x+y+z=1$ . This means $a+b+c=1$ . Again, $T$ preserves norm of vector : $1=|(0,1,0)|=|T(a,b,c)|=a^2+b^2+c^2$ . So, I got two equations : $$a+b+c=1 \\ a^2+b^2+c^2=1$$ I want to Find $a, b, c$ without finding exact form of $T$ but I need one more equation about $a,b,c$ to solve this system. Is there any property of rotation which can give me one more equation about $a,b,c$ ?","['linear-algebra', 'rotations']"
4605775,An example of $f$ and $a$ such that $f$ is Gâteaux but not Fréchet differentiable at $a$,"I'm reading this lecture note about differentiability. Let $(X, |\cdot|_X)$ and $(Y, |\cdot|_Y)$ be normed spaces. Let $A$ be an open subset of $X$ and $f: A \to Y$ . The directional derivative $f^{\prime}(a) (v)$ of $f$ at $a \in A$ along direction $v \in X$ is the limit (if exists) $$
f^{\prime}(a)(v) :=\lim _{t \rightarrow 0} \frac{f(a+t v)-f(a)}{t}.
$$ We shall say that $f$ is: Gâteaux differentiable at $a$ if there exists $x^{*} \in X^{*}$ such that $f^{\prime}(a)(v)=x^{*}(v)$ for each $v \in X$ (that is, $f^{\prime}(a)$ is everywhere defined, real-valued, linear and continuous); Then $x^{*}$ is called the Gâteaux differential (or derivative ) of $f$ at $a$ , and is denoted by $d f(a)$ . Fréchet differentiable at $a$ if there exists $x^{*} \in X^{*}$ such that $$
\lim _{h \to 0} \frac{f(a+h)-f(a)-x^{*}(h)}{|h|_X}=0 .
$$ Then $x^{*}$ is called the Fréchet differential (or derivative ) of $f$ at $a$ , and is denoted by $\partial f(a)$ . Could you provide an example of $f$ and $a$ such that $f$ is Gâteaux but not Fréchet differentiable at $a$ ? I only find an example in which $f^{\prime}(a)$ exists and is continuous, but not linear. For example, consider the real-valued function $F$ of two real variables defined by $$
F(x, y)= \begin{cases}\frac{x^3}{x^2+y^2} & \text { if }(x, y) \neq(0,0), \\ 0 & \text { if }(x, y)=(0,0).\end{cases}
$$ Then $$
d F(0,0) (a, b)=\left\{\begin{array}{ll}
\frac{F(\tau a, \tau b)-0}{\tau} & \text{if } (a, b) \neq(0,0), \\
0 & \text{if } (a, b)=(0,0).
\end{array}= \begin{cases}\frac{a^3}{a^2+b^2} & \text{if }  (a, b) \neq(0,0), \\
0 &\text{if } (a, b)=(0,0).\end{cases}\right.
$$","['frechet-derivative', 'gateaux-derivative', 'derivatives', 'examples-counterexamples']"
4605781,Finding an alternative nicer method to evaluate the summation,If $a_{n+1}=a_{n}^2+3a_n+1$ and $a_1=\frac13$ then find the value of $$\frac{1}{a_1+2}+\frac{1}{a_2+2}+\frac{1}{a_3+2}+\cdots+\frac{1}{a_{11}+2}+\frac{1}{a_{12}+1}$$ I can easily do this by calculating each $a_i$ separately then doing the calculation but that will be a tiresome task. Is there any other nicer way $?$ Any help is greatly appreciated.,"['alternative-proof', 'sequences-and-series']"
4605811,"If $\delta \colon R[x_1, \dotsc, x_r] \to M$ is an $R$-derivation, then $\delta P = \sum_{i = 1}^r \frac{\partial P}{\partial x_i} dx_i$","Let $\phi \colon R \rightarrow S$ be a ring homomorphism and $M$ an $S$ -module. An $R$ -derivation $\delta \colon S \rightarrow M$ is a $R$ -module homomorphism such that: $\phi(R) \subset \ker(\delta)$ , (Leibniz’ rule:) $\delta(s s') = s \delta s' + s' \delta s$ for all $s, s' \in S$ . Then, according to the exercises on my lecture notes, if $\delta \colon R[x_1, \dotsc, x_r] \rightarrow M$ is an $R$ -derivation, then $\delta P = \sum_{i = 1}^r \frac{\partial P}{\partial x_i} dx_i$ . I’m not totally sure what they mean by $dx_i$ here, since I think they don’t have defined the operator $d$ as far as I know. I guess it should be just the same as $\delta x_i$ ? I’d like to prove this. I thought of doing it by induction: for the case $r = 1$ , it is fairly easy to see using the above properties that $\delta P = \frac{\partial P}{\partial x} \delta x$ . Then, if we write a multivariable polynomial as $\sum_{j = 0}^k a_j x^j_{n + 1}$ , where $a_j \in R[x_1, \dotsc, x_n]$ , we may assume that the result holds for $n$ and try to prove it for $n + 1$ : $$
  \delta P
  = \delta\Biggl( \sum_{j = 0}^k a_j x^j_{n+1} \Biggr)
  = \sum_{j = 0}^k \Bigl( a_j \delta(x_{n+1}^j) + \delta(a_j) x_{n+1}^j \Bigr)
$$ … where we may use that $\delta(x_{n+1}^j) = j x_{n+1}^{j-1} \delta(x_{n+1})$ and that $\delta(a_j) = \sum_{i = 1}^n \frac{\partial{a_j}}{\partial{x_i}}dx_i$ (because of the induction hypothesis), but I don’t really know where to go from there. I guess it’s a matter of manipulating expressions with polynomials in several variables, but I’m also not entirely sure what the exercise expects me to prove (as I don’t know what “ $dx_i$ ” is).","['commutative-algebra', 'modules', 'abstract-algebra', 'derivatives', 'differential-algebra']"
4605837,Olympiad geometry: Prove equal segments,"$A$ , $B$ , $C$ , $D$ , $E$ and $F$ are six concyclic points. $AC$ , $BD$ and $EF$ are concurrent at $G$ . Line $EF$ intersects $\odot(ABG)$ and $\odot(CDG)$ at $I$ and $J$ respectively. Show that $IE=FJ$ . My idea was the trigonometry version of Ptolemy theorem. Let $\angle AGE=\angle CGF=\beta$ and $\angle BGE=\angle DGF=\alpha$ . I got $$\begin{aligned}GI=\frac{AG\sin\alpha+BG\sin\beta}{\sin(\alpha+\beta)};\\GJ=\frac{CG\sin\alpha+DG\sin\beta}{\sin(\alpha+\beta)}.\end{aligned}$$ $EI=FJ$ is equivalent to $GI-GJ=GE-GF$ . So we need only prove that $$GE-GF=\frac{AG\sin\alpha+BG\sin\beta-CG\sin\alpha-DG\sin\beta}{\sin(\alpha+\beta)}.$$ This is not any much easier, although it becomes independent of $I$ and $J$ .","['contest-math', 'geometry']"
4605859,Alternative ways to evaluate a limit of this sequence,"Find the limit $$\lim\limits_{n\to \infty} \sin \left( (2 + \sqrt 3 )^n\pi\right)$$ for $n \in \mathbb N$ . I know that this question has been asked earlier here. However, I cannot convince myself about the answer given. I understood that $(2 + \sqrt 3 )^n$ approaches an even integer, but are there any other ways to show that this is true without using its conjugate? (like adding or subtracting integral multiples of $\pi$ ) I tried doing that but couldn't show that the resulting sequence converges. Can anyone help me out with this? Thanks in advance.","['limits', 'sequences-and-series']"
4605872,Proof of Bolzano Weierstrass Theorem in $\mathbb{R}^n$,"I would like to show the Bolzano-Weierstrass in $\mathbb{R}^n$ , I have seen this theorem in $\mathbb{R}$ and I know it can be shown by induction, something I will try now. Theorem : Every bounded sequence in $\mathbb{R}^n$ has a convergent subsequence in $\mathbb{R}^n$ Proof : By induction : we know that it is true for $n=1$ , we assume this holds for $p=k$ and we want to show this holds for $p=k+1$ . Consider $(x_n)$ a bounded sequence in $\mathbb{R}^{k+1}$ . For all $n\in\mathbb{N}$ we can write $x_n = (a_n, x_{n}^{k+1})$ where $a_n = (x_{n}^{1}, ..., x_{n}^{k})\in\mathbb{R}^{k}$ since $\mathbb{R}^{k+1}$ is isomorphic to $\mathbb{R}^{k}\times\mathbb{R}$ so this rewritting of $x_n$ , even if not exactly the same ""element"" as the initial $x_n$ , can be treated as the same. Clearly, for all $n\in\mathbb{N}, \exists M>0 : \lvert x_{n}^{k+1}\rvert\leq\lVert x_n\rVert_{2}\leq M$ and $\lVert a_n\rVert_{2}\leq\lVert x_n\rVert_{2}\leq M$ . Using the induction hypothesis, we know that $(a_n)$ has a convergent subsquence $(a_{n_j})$ , we denote $x\in\mathbb{R}^{k}$ its limit. Now, the sequence $(x_{n}^{k+1})$ has also a convergent subsequence in $\mathbb{R}$ by Bolzano-Weierstrass with limit $x^{k+1}\in\mathbb{R}$ . This shows that $x_n$ admits a convergent subsequence $(a_{n_j}, x_{n_j}^{k+1})$ whose limit is $(x,x^{k+1}) = (x^1, ..., x^{k+1})\in\mathbb{R}^{k+1}$ so the Bolzano-Weierstrass is true for $p=k+1$ , which concludes the proof. This seems correct to you? EDIT : My proof is false, we need to take another subsequence to be sure to have a vector with coordinate that have the same index ! Thanks to FShrike, Vercassivelaunos and egreg, you will find three very clear answers to this problem below.","['analysis', 'real-analysis', 'calculus', 'sequences-and-series', 'general-topology']"
4605925,Open Sets of Convergence Space Induce a Topology (and vice versa),"It's Christmas holidays and I have some time to read about filter convergence. But I am not sure about the following exercise: Let $X$ be a space equipped with a convergence (i.e. a relation $\xi\subseteq X\times\mathfrak F$ satisfying some conditions, c.f. https://en.wikipedia.org/wiki/Convergence_space#Definition_of_(pre)convergence_spaces ; here $\mathfrak F$ denotes the set of filters over $X$ ), and let $\mathcal T$ be a set of subsets of $X$ . I want to show that $\mathcal T$ is a topology over $X$ if and only if $\mathcal T = \big\{O\subseteq X : \text{for each $\mathcal F\in\mathfrak F$ it holds that $O\in\mathcal F$ whenever $\lim\mathcal F\cap O\neq\emptyset$}\big\}$ . I think I figured out the "" $\Leftarrow$ ""-direction: Let $\mathcal F\in\mathfrak F$ be such that $\lim\mathcal F\cap\emptyset \neq \emptyset$ . Thus, it's vacuously true that $\emptyset\in\mathcal T$ . Let $\mathcal F\in\mathfrak F$ such that $\lim\mathcal F\cap X\neq\emptyset$ . It follows from the definition of a filter that $X\in\mathcal F$ . Thus, $X\in\mathcal T$ . Let $A,B\in\mathcal T$ , and let $\mathcal F\in\mathfrak F$ such that $\lim\mathcal F\cap (A\cap B)\neq\emptyset$ . Since $A\cap B\subseteq A$ it follows that $\lim\mathcal F\cap A\neq\emptyset$ . By hypothesis, $A\in\mathcal T$ . Hence it follows from the definition of $\mathcal T$ that $A\in\mathcal F$ . Similarly, since $A\cap B\subseteq B$ it follows that $\lim\mathcal F\cap B\neq\emptyset$ . By hypothesis, $B\in\mathcal T$ . Hence it follows from the definition of $\mathcal T$ that $B\in\mathcal F$ . Consequently, $A\cap B\in\mathcal F$ by definition of a filter. Thus, $A\cap B\in\mathcal T$ . Let $\mathcal A\subseteq\mathcal T$ . Let $\mathcal F\in\mathfrak F$ such that $\lim\mathcal F\cap \left(\bigcup\mathcal A\right)\neq\emptyset$ . So there exists some $A\in\mathcal A$ such that $\lim\mathcal F\cap A\neq\emptyset$ . By assumption, $A\in\mathcal T$ . Hence it follows from the definition of $\mathcal T$ that $A\in\mathcal F$ . Since $A\subseteq\bigcup\mathcal A$ , it follows from the definition of a filter that $\bigcup\mathcal A\in\mathcal F$ . Thus, $\bigcup\mathcal A\in\mathcal T$ . That is, $\mathcal T$ is a topology over $X$ . Is my proof fine? I followed the steps discussed in this question and added some details: Topology of a convergence space I have some problems with the "" $\Rightarrow$ ""-direction: For each $\mathcal F\in\mathfrak F$ , put $$\lim\mathcal F:=  \big\{x\in X : \text{for each $N\in\mathcal N(x)$ there exists some $F\in\mathcal F$ with $F\subseteq N$. }\big\},$$ where $\mathcal N(x)$ denotes the neighborhood filter of $x\in X$ , i.e. $$\mathcal N(x) := \big\{N\subseteq X : \text{there exists some $U\in\mathcal T$ with $x\in U$ and $U\subseteq N$}\big\}.$$ "" $\subseteq$ "": Let $O\in\mathcal T$ and consider some $F\in\mathfrak F$ with $\lim\mathcal F\cap O\neq\emptyset$ . Hence there is some $x\in O$ with $x\in\lim\mathcal F$ . So for each $N\in\mathcal N(x)$ there is some $F\in\mathcal F$ such that $F\subseteq N$ . In particular, there is some $F\in\mathcal F$ such that $F\subseteq O$ , as $O\in\mathcal N(x)$ since $O\in\mathcal T$ by hypothesis as well as $x\in O$ and $O\subseteq O$ . Thus, it follows from the definition of a filter that $O\in\mathcal F$ . "" $\supseteq$ "": Let $O\subseteq X$ and $\mathcal F\in\mathfrak F$ such that $O\in\mathcal F$ whenever $\lim\mathcal F\cap O\neq\emptyset$ . Let $x\in O$ and consider $\mathcal N(x)$ . Note that $\mathcal N(x)\in\mathfrak F$ and $x\in\lim\mathcal N(x)$ , so $\lim\mathcal N(x)\cap O\neq\emptyset$ . Hence it follows from the hypothesis that $O\in\mathcal N(x)$ . So there exists some $U_x\in\mathcal T$ such that $x\in U_x$ and $U_x\subseteq O$ . Define $\mathcal A :=\{ U_x : x\in O\}$ . Clearly, $\mathcal A\subseteq\mathcal T$ , so $\bigcup\mathcal A\in\mathcal T$ by definition of a topology. Since $$O = \bigcup_{x\in O}\{x\} \subseteq \bigcup_{x\in O}U_x \subseteq O$$ it follows that $\bigcup\mathcal A = O$ . Hence $O\in\mathcal T$ . That is, $\mathcal T = \big\{O\subseteq X : \text{for each $\mathcal F\in\mathfrak F$ it holds that $O\in\mathcal F$ whenever $\lim\mathcal F\cap O\neq\emptyset$}\big\}$ . I am not sure about this direction. Especially in the second part, I only showed it for a particular choice of $\mathcal F\in\mathfrak F$ (namely $\mathcal N(x)$ ). But I don't know how to prove it for a general choice of $\mathcal F$ ... To be honest, I am not even sure whether the proof of the "" $\Rightarrow$ ""-direction is correct at all. Are the other steps ok, though? Is there anything else that needs to be corrected? Edit 1: Thinking again about this, the "" $\Rightarrow$ ""-direction cannot be true. If it was true, then there would be a topology for each type of filter convergence. However, there are types of filter convergence (i.e. almost sure convergence) that cannot be described by a topology. So the right side needs to be modified. But how? Edit 2: The exercise may has to be modified too in order to overcome the issue mentioned in Edit 1.","['filters', 'general-topology', 'solution-verification']"
4605971,What Laws of Mathematics Best Explain How Surveys Work?,"Suppose I have some probability distribution - as an example, I choose the Normal Distribution with some very large variance. Suppose I assume that the distribution of the number of calories eaten a day follows such a Normal Distribution in a country (population = 1000000 people) with a very large population. Now, let's say that I can only ask a very small percentage (50 people) of these people how many calories they consume every day, and I am interested in estimating the true number of calories the average person eats in one day within the population. Here is some R code to simulate this: # lets assume that the true average is 2000 calories, but this is unknown
set.seed(123)
population_calories = rnorm(1000000, 2000, 1000) Suppose a researcher randomly selects 50 people from this country and asks how many calories   they eat and takes the average: mean(sample(population_calories , 50, replace=FALSE))
[1] 1988.098 As we see, this number is very close to the actual average. Now, if we repeat this for 100 researchers: my_list = list()

for (i in 1:100)

{
sample_i = mean(sample(population_calories , 50, replace=FALSE))
my_list[[i]] = data.frame(i,sample_i)
} Looking at the distribution of these results: m = do.call(rbind.data.frame, my_list)
plot(density(m $sample_i))
mean(m$ sample_i)
[1] 1999.711 We see that the average estimates from a very small sample (0.005% of the population) from a  population with considerably large variance comes very close to the true value! Furthermore, I have heard that even when the underlying distribution is not a Normal Distribution, the above phenomena would still repeat. I was wondering - what principles of mathematics best explain this above phenomena? Is this more an application of the ""Central Limit Theorem"" ( https://en.wikipedia.org/wiki/Central_limit_theorem ) or ""Weak Law of Large Numbers"" ( https://en.wikipedia.org/wiki/Law_of_large_numbers )? Thanks!","['statistics', 'probability']"
4606002,Likelihood ratio test.,"Consider a random sample from a distribution with density function $$
f(y)=\frac{1}{2 \lambda} e^{-\frac{y}{2 \lambda}}
$$ when $\lambda,y>0$ . I want to construct a likelihood ratio test to determine when we can discard $H_0 :\lambda = 1$ in favor of $H_1 :\lambda \ne 1$ , with significance level $\alpha=0.01$ .
I obtain the following likelihood: $$
L(\theta)=\prod_i^n \frac{1}{2 \lambda} e^{-\frac{1}{2 \lambda} y_i}=\frac{1}{2^n \lambda^n} e^{-\frac{1}{2 \lambda} \sum_{i=1}^n y_i}=\frac{1}{2^n \lambda^n} e^{-\frac{n \bar{y}}{2 \lambda}}
$$ and the MLE for $\lambda$ : $$
\hat{\lambda}=\frac{1}{\bar{y}}
$$ The likelihood ratio is then: $$
\Lambda=\frac{L\left(\lambda_0=1\right)}{L\left(\hat{\lambda}=\bar{y}^{-1}\right)}=\frac{\bar{y}^n}{2^n} e^{-\frac{n \bar{y}}{2}+n}
$$ We discard $H_0$ if $\Lambda\le c$ for some $c$ determined by the signifinance level. My book states that we can use that $-2 \log \Lambda \stackrel{D}{\rightarrow} \chi^2(1)$ under the null hypothesis $H_0 :\lambda=\lambda_0$ . Hence, I assume that we can discard $H_0$ if $$
-2 \log\Lambda\ge \chi^2_{.01}
$$ where $\chi^2_{.01}$ is the $.01$ -quantile of the chi-squared distribution with 1 degree of freedom: $\chi^2_{.01}=1.57\times10^{-4}$ . Is this the correct procedure, or have I misunderstood something?","['statistics', 'probability-distributions', 'solution-verification', 'probability', 'random-variables']"
4606035,Equation for a Sphere with Increasing Volume,"The Problem: A sphere with radius $r$ increases in volume at a rate proportional to the surface area of the sphere at that time, with proportionality constant $k$ . Write a function for the sphere’s volume at any time $t$ . My Attempt: We can write an ODE to solve for $r(t)$ , and then plug that into $V=\frac{4}{3}\pi r^3$ . Since the rate of change of the radius is equal to the surface area $4\pi r^2$ times $k$ , we get: $$\frac{dr}{dt}=4\pi kr^2$$ I separated the variables and got: $$r(t)=-\frac{1}{4\pi kt+C}$$ Let alone the volume function, this radius function makes no sense. Help please!","['spheres', 'volume', 'ordinary-differential-equations', 'geometry', 'calculus']"
4606046,How to find a convex function such that $F$ is $C^2$?,"Given the function $F(x)=\begin{cases} 
      1 & x\leq 1; \\
      f(x) & 1\leq x\leq 3; \\
      2x^2 & 3\leq x.\end{cases}$ I would like to know if there is a general method to find $f$ convex such that $F$ is $C^2$ . I directly tried to interpolate with $f$ a polynomial of degree 5 such that $f(1)=1,f'(1)=f''(1)=0$ and $f(3)=18,f'(3)=12,f''(3)=4$ . The problem is that $f''(x)\not\geq 0,\forall 1\leq x\leq 3$ .","['interpolation', 'real-analysis', 'calculus', 'functional-analysis', 'numerical-methods']"
4606065,Counter example about convolution of symmetric functions on locally compact groups,"This question is motivated on what I think is an error in a result (Lemma 1.6.5) given in Dietmar, A. and Echterhoff, S., Principles of Harmonic Analysis, 2nd edition, Springer, pp.24 Suppose $G$ is a locally compact topological group and let $\lambda$ be a (left) Haar measure. One statement of the  aforementioned Lemma is that if $\phi$ and $\psi$ are $L_1(G,\lambda)$ symmetric functions ( $\phi(x^{-1})=\phi(x)$ and $\psi(x^{-1})=\psi(x)$ for all $x\in G$ ), then $\phi*\psi$ is also symmetric. I think this is not necessarily true, unless $G$ is also Abelian, or $\phi=\psi$ . A simple deduction one can make about $\phi*\psi$ is that \begin{align}
\phi*\psi(x)&:=\int_G \phi(y)\psi(y^{-1}x)\,\lambda(dy)\\
&=\int_G\phi(xy)\psi(y^{-1})\,\lambda(dy)\\
&=\int_G\phi(y^{-1})\psi(yx)\Delta(y^{-1})\,\lambda(dy)\\
&=\int_G\phi(xy^{-1})\psi(y)\Delta(y^{-1})\,\lambda(dy)
\end{align} where $\Delta$ is the modular function of $G$ . From this, and the symmetry of $\phi$ and $\psi$ one can establish that $$\phi*\psi(x^{-1})=\int_G\phi(x^{-1}y)\psi(y^{-1})\,\lambda(dy)=\int_G\phi(y^{-1}x)\psi(y)\,\lambda(dy)=\psi*\phi(x)$$ I seems to me that this is the best one can say about the between relation $\phi*\psi(x)$ and $\phi*\psi(x^{-1})$ under the general assumptions above. Of course, if $G$ were in addition commutative, then $\phi*\psi=\psi*\phi$ and thus, $\phi*\psi$ would be symmetric. The problem is to prove or disprove (via a counter example) whether it is indeed the case that $\phi*\psi$ is symmetric under the general assumptions on $G$ .","['harmonic-analysis', 'analysis']"
4606069,Connection between compactly supported smooth functions on a bounded domain and sobolev spaces,"I am interested in finding a direct yes or no answer for the following: In general, on a bounded domain $\Omega$ in $R^n$ with say $C^1$ boundary, can we that say any function $f$ $\in$ $C_c^{\infty}$ ( $\Omega$ ), where $C_c^{\infty}$ ( $\Omega$ ) is the space of all compactly supported smooth functions defined on $\Omega$ , must also belong to a sobolev space $W^{k,p}(\Omega)$ , for $1 \le p < \infty$ ? I appreciate anyone's help, and enlightenment. If not can someone please give an example? Thanks,
Sandy","['smooth-functions', 'analysis', 'real-analysis', 'sobolev-spaces', 'derivatives']"
4606092,Evaluating a double integral,"I was trying to evaluate the following integral $$\int_{x=0}^{\infty}\int_{y=0}^{\infty}\frac{y \ln y \ln x}{(x^2+ y^2)( 1+y^2)} dy dx.$$ I have a guess that the value of this integral is $\frac{\pi^4}{8}$ . But I am unable to prove it.
Could someone please help me in evaluating this integral? Or, can we show the following identity holds without much calculation? $$\int_{x=0}^{\infty}\int_{y=0}^{\infty}\frac{2y \ln y \ln x}{(x^2+ y^2)( 1+y^2)} dy dx=   \int_{x=0}^{\infty}\int_{y=0}^{\infty}\frac{y (\ln y )^2}{(x^2+ y^2)( 1+y^2)} dy dx.$$ Any help or hint would be appreciated. Thanks in advance.","['integration', 'indefinite-integrals', 'improper-integrals']"
4606097,Wrong expected value of sum of Poisson process wait times [duplicate],"This question already has an answer here : Expected value of the total waiting time of all passengers catching a train. (1 answer) Closed 1 year ago . Assuming a Poisson process $N_t$ and denoting wait times $S_k$ (i.e. times until the $k$ -th jump), I want to find the expected value of their sum: $$ \mathrm{E}\left[\sum_{k=1}^{N_t} S_k\right]\;. $$ It is an exercise in O. Calin's An Informal Introduction to Stochastic Calculus with Applications and I actually know how to do it. For instance using the integrated Poisson process $$ U_t = \int_0^t N_s \,\mathrm{d}s $$ and the result $$ U_t = tN_t - \sum_{k=1}^{N_t} S_k\;. $$ Taking the expected value of both sides and using its linearity gives $$ \frac12 \lambda t^2 = \lambda t^2 - \mathrm{E}\left[\sum_{k=1}^{N_t} S_k\right], $$ so the answer is $\frac12\lambda t^2$ . The problem is that I can calculate the expected value using a different straighforward method, getting a different (apparently wrong) answer. And the question is why. In the second method we note that $N_t=n$ for different $n$ are disjoint events. So we can calculate the conditional expectations $$ \mathrm{E}\left[\sum_{k=1}^{N_t} S_k \middle| N_t = n\right] $$ and sum over $n$ with the corresponding proabilities: $$ \mathrm{E}\left[\sum_{k=1}^{N_t} S_k\right] = \sum_{n=0}^\infty \mathrm{Pr}(N_t=n)\times\mathrm{E}\left[\sum_{k=1}^{N_t} S_k \middle| N_t = n\right]\;. $$ Using $$ S_1 + S_2 + S_3 + \dots + S_n = nT_1 + (n-1)T_2 + \dots + 2T_{n-1} + T_n $$ and independence of inter-arrival times $T_k$ we get for the conditional expectation $$ \mathrm{E}\left[\sum_{k=1}^{N_t} S_k \middle| N_t = n\right] = \frac{n(n+1)}{2\lambda}\;. $$ This is in fact the result of another exercise (3.11.4). The probability is just Poisson $$ \mathrm{Pr}(N_t=n) = \mathrm{e}^{-\lambda t} \frac{(\lambda t)^n}{n!}\;. $$ Putting it together: $$ \mathrm{E}\left[\sum_{k=1}^{N_t} S_k\right] = \sum_{n=0}^\infty \mathrm{e}^{-\lambda t} \frac{(\lambda t)^n}{n!} \times \frac{n(n+1)}{2\lambda} = \mathrm{e}^{-\lambda t} \frac t2 \sum_{n=0}^\infty \frac{(\lambda t)^n}{n!} (n+2) = \frac12\lambda t^2 + t\;. $$ So there is an extra $t$ . Now I am stuck and can't find neither an error in the calculation, nor a reason why the summation over $n$ could be wrong.","['expected-value', 'stochastic-processes', 'poisson-process', 'probability']"
4606104,Finding radius of convergence on a Banach space,"Let $(\phi_m)$ be the sequence of coordinate functionals on $\ell^p$ where $1\leq p <\infty.$ Then the power series $\sum_{m=0}^\infty (\phi_m(x))^m$ is absolutely convergent for any $x\in\ell^p$ but the radius of convergence is $1$ . Here, the definition of power series is : A power series from the Banach space $X$ to $\mathbb C$ is a series of the form $\sum_{m=0}^\infty P_m(x-a)$ where $(P_m)$ is a sequence of polynomials from $X^m$ to $\mathbb C$ . Radius of convergence for this series is defined as $R=\frac{1}{\limsup_{m \to\infty}\|P_m\|^{1/m}}$ by the Cauchy-Hadamard formula. I also have the following lemma : Let $X$ be a Banach space and $\phi \in X^*$ with $\|\phi\|=1$ . Then $P:=\phi^m$ is a polynomial from $X^m$ to $\mathbb C$ with $\|P\|=1$ . Absolute convergence is ok. I am trying to see that it is a power series (please note that it is not a classical power series, I added my definition above) and also state its radius of convergence. My thoughts : Since coordinate functionals are linear and continuous, we have $(\phi_m)\subset {\ell^p}^*$ . Also, I showed $\|\phi_m\|=1$ for each $m$ . Then by the previous lemma each $P_m:=\phi_m^m$ is a polynomial from ${(\ell^p)}^m$ to $\mathbb C$ and $\|P_m\|=1$ . Hence, given series is a power series and by the Cauchy-Hadamard formula, its radius of convergence is $R=\frac{1}{\limsup_{m \to\infty}\|P_m\|^{1/m}}=\frac{1}{\limsup_{m \to\infty}1^{1/m}}=1$ . I am not sure about whether my thoughts are true. I appreciate any correction or suggestion. Thank you and merry xmas Edit : Definition of a polynomial from $X$ to $\mathbb C$ : A mapping $P:X\to\mathbb C$ is a polynomial if there exists a multi-linear map from $X^m$ to $\mathbb C$ such that $P(x)=Ax^m$ for every $x\in X$ . ( $x^m$ is a notation here, it means that $x^m=(x,x,\dots, x)$ -m times $x$ )","['banach-spaces', 'complex-analysis', 'solution-verification', 'functional-analysis', 'power-series']"
4606130,How do you prove ${n \choose k}$ is maximum when $k$ is $ \lceil \tfrac n2 \rceil$ or $ \lfloor \tfrac n2\rfloor $?,"How do you prove $n \choose k$ is maximum when $k$ is $\lceil n/2 \rceil$ or $\lfloor n/2 \rfloor$ ? This link provides a proof of sorts but it is not satisfying. From what I understand, it focuses on product pairings present in $k! (n-k)!$ term which are of the form $i \times (i-1)$ . Since these are minimized when $i=n/2$ , we get the result. But what about the reasoning for the rest of the terms?","['optimization', 'inequality', 'binomial-coefficients', 'combinatorics']"
4606218,Is this result correct?,"I have the following property of summation $e^{x}=\sum_{k=0}^{∞}\frac{x^{k}}{k!}$ , then $$\sum _{k=61} ^{\infty} \frac{e^{-66}\left(66\right)^{k}}{k!} = \frac{1}{e^{66}} \sum_{k=61} ^{\infty} \frac{\left(66\right)^{k}}{k!}=\frac{e^{66}}{e^{66}}=1$$ The correct answer is supposed to be $0.747$ but I have not found a structure or a recursive method to know where the result comes from. Thank you very much for anyone who can give me a good indication.","['statistics', 'summation', 'analysis', 'discrete-mathematics', 'probability']"
4606269,Optimal Strategies for Coin-Flipping Games,"My nephew thought of the following problem over the holidays: I have 100 coins, half these coins are currently face up heads, the other half are currently face up tails. Currently, my score is 0. Each coin has a 0.5 probability of landing on heads and a 0.5 probability of landing on tails Round 1: I randomly generate an integer ""n"" between 0 and 100, select ""n"" coins and flip them - naturally, some change sides, some don't. the coins that weren't selected stay as they are. After Round 1 - each coins that is now facing the opposite side gives me 1 point. And each coin that is facing the same way takes away 1 point. Record the total number of points. Round 2: I now again generate another integer ""n""  and flip these ""n"" coins. For coins that were not previously selected but were now selected
if they land on the other side, I get 1 point but if they land on the same side, I lose 1 point.
And all coins that were not selected in Round 2 automatically deduct 1 point each. Record total points from Round 2 and add to total points from Round 1. Repeat for many rounds Now, my nephew's game lead me to the following questions: After Round 2, what is my expected score? How many Rounds do I need to play to get some expected score of ""x""? On average, what is the optimal number of rounds I should play to give me the highest possible score (before my score starts to decrease and decrease)? My feeling is that initially it it makes sense to play this game for a few rounds regardless - but then, based on your score, there is probably an probabilistic optimal number of rounds to stop playing this game. I think that maybe you can use a Markov Chain in where states are either the ""score of the game"" (e.g. probability of transitioning to -5 points given you are at -1 point) and then estimate the ""time to absorption"" for different questions based on simulations or solving this analytically - but I am not sure. Any ideas?",['probability']
4606302,"Formula for the $n$-th term of the sequence $1, 2, 4, 6, 12, 16, 24, 30, 60, 72, 96, 112, \ldots$, where $f_n := \frac{1}{n} (f_{2n} - f_n)$","I'm struggling with this sequence. $$1, 2, 4, 6, 12, 16, 24, 30, 60, 72, 96, 112, \ldots$$ Where, $f_n := \frac{1}{n} (f_{2n} - f_n)$ You can also work it out for negative powers of 2, $$f_\frac{1}{2} = \frac{2}{3}$$ $$f_\frac{1}{4} = \frac{8}{15}$$ $$f_{2^{-n}} = \prod_{k=1}^{n} (\frac{2^k}{2^k +1})$$ I wanted to know if it possible to find a formula for $f_n$ , and to generalize it to all real numbers? Can generating functions help here? (I don't know much about them.) (And sorry if I had made some mistake)","['real-numbers', 'generating-functions', 'functions', 'sequences-and-series']"
4606303,Decomposition of a bounded Hilbert space operator,"I am trying to prove the following homework problem:
Let $B$ be a positive operator on a Hilbert space $H$ with $\Vert B\Vert=1$ and $B$ is invertible. Try to prove that for each $\,T\in \mathfrak{B}(H,H)$ , there exists an $S\in\mathfrak B(H,H)\,$ such that $\,T=\frac{1}{2}(BS+SB)\,$ . I have tried to apply the polar decomposition of $T$ but seemingly in vain. Actually, I am wondering how the condition $\Vert B\Vert=1$ is used. Can somebody give me some hints on this problem?","['hilbert-spaces', 'operator-theory', 'functional-analysis']"
4606324,Maximum length of a trail in a hypercube,"A trail is an alternating sequence of vertices and edges but edges cannot be repeated. Given a hypercube $Q_{n}$ what will be the maximum length of the trail? My attempt: For base case $Q_{2}$ we will have a maximum trail length of $4$ . I came up with this recurrence relation based on the intuition that $Q_{3}$ is formed from connection $2$ $Q_{2}$ hypercubes, this relation happens to be working fine up to $Q_{4}$ (beyond which is really hard to draw) $ T(n) = 2*T(n-1) + 1 $ $T(2) = 4$ Solving it we get $T(n) = 2^{n} + 2^{n-2} - 1$ Is this recurrence relation correct, if yes please provide a reason. If it's wrong any other alternate approach or idea that you have please share.","['graph-theory', 'discrete-mathematics']"
4606358,How to solve the ordinary differential Equations? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question \begin{equation}
\begin{aligned}
\frac{\mathrm{d} x_1}{\mathrm{d} t} & =-x_1+x_2\\
\frac{\mathrm{d} x_2}{\mathrm{d} t} & =x_1\cos t -x_2
\end{aligned}
\label{eq:3.6}
\end{equation} At the beginning, I prove the zero solution is stable, then I used Maltab ode45 function test more than 10 different initial points, they all convergenced to zero, so I guess the zero solution asymptotically stable, But I can't prove it. So I want to know how to solve the ODEs.","['ordinary-differential-equations', 'dynamical-systems']"
4606462,"If $X$ is normed space, $V \subset X$ is closed, and $W \subset X$ is finite-dimensional with $V \cap W = \{0\}$, then $\pi(V) \subset X/W$ is closed","Let $X$ be a normed space, let $V \subset X$ be a closed subspace, and let $W \subset X$ be a finite-dimensional subspace with $V \cap W = \{0\}$ . I would like to show that $\pi(V) \subset X/W$ is closed, where $\pi : X \to X/W$ is the quotient map. I was able to show with a proof by contradiction that there is some $C > 0$ such that $$\forall x \in V, \forall y \in W : \|x \| + \|y\| \leq C \|x - y\|. $$ But I don't see how to use the above norm estimate to show that $\pi(V)$ is closed. The estimate above implies that $$\forall x \in V : \|\pi(x)\|_{X/W} \geq \frac{1}{C}\|x\|, $$ and with the additional fact that $\pi$ is bounded, we have $$\forall x \in V: \frac{1}{C}\|x\| \leq \|\pi(x)\|_{X/W} \leq \|x\|, $$ but I cannot conclude that $\pi(V)$ is closed without knowing that $V$ is complete, which a priori is not.",['functional-analysis']
4606471,"If $\lim_{x\to \infty} f^{(k)}(x)=\theta$ (irrational), then $(f(n))$ is uniformly distributed modulo $1$","Let $f\in \mathcal C^{k+1}$ , $x\ge 1$ and let for some integer $k\ge 1$ , we have $$\lim_{x\to \infty} f^{(k)}(x)=\theta$$ for irrational $\theta$ . Then, prove that $(f(n))$ is uniformly distributed modulo $1$ . To do this, I think, I need to prove the following proposition first- Let $(x_n)$ be a sequence of real numbers with the property that for some integer $k\ge 1$ , $$\lim_{n\to \infty} \Delta^k x_n=\theta$$ where $\theta$ is irrational. Then, prove that $(x_n)$ is uniformly distributed modulo $1$ . I do know that if $$\lim_{n\to \infty} \Delta x_n=x_n-x_{n-1}=\theta$$ for irrational $\theta$ , then $(x_n)$ is ud mod $1$ . I have also used this (and IVT) to show that if $f\in \mathcal C^2$ , $x\ge 1$ and if $$\lim_{x\to \infty} f^\prime (x)=\theta$$ for irrational $\theta$ , then $(f(n))$ is ud mod $1$ . To prove the proposition, I think, I need to use the stated result and use Induction by noting that if $h$ is a positive integer, then $$\Delta^k\left(f(n+h)-f(n)\right)=\sum_{j=0}^{h-1}\Delta^{k+1}f(n+j)$$ I feel like I may also have to use the fact that if $(x_n)$ is a sequence such that $(x_{n+h}-x_n)$ is ud mod $1$ for all positive integers $h$ , then $(x_n)$ is ud mod $1$ . However, I couldn't proceed any further.","['ergodic-theory', 'equidistribution', 'analysis', 'analytic-number-theory', 'sequences-and-series']"
4606477,"Kalman Filtering the Vasicek Model, are there different Kalman Filters for the same application? [closed]","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed last year . Improve this question While studying parameter estimation in affine term structure models, I stumbeled across two papers. Affine Term-Structure Models: Theory and Implementation by David Bolder (2001) https://www.researchgate.net/publication/4744752_Affine_Term-Structure_Models_Theory_and_Implementation Calibration of the Vasicek Model of Interest rates Using Bicriteria Optimization by Jasurkova and Stehlikova (2020) http://www.iam.fmph.uniba.sk/amuc/ojs/index.php/algoritmy/article/view/1585 In both papers, a Kalman filter is implemented for the Vasicek model. In Bolder's work, for a 3-factor Vasicek model, and in Jasurkova and Stehlikova's work, a 1-factor Vasicek model. However, in my opinion, both methods should be independent of the number of factors. But upon closer inspection, there are a few differences, in addition to some different writing styles. When both present the necessary steps for implementing the Kalman filter, there are some discrepancies: -In the first and second step, everythink agrees in both papers.
When moving to the third step, however, things differ: Bolder writes in equation (74) the following: $var[y_{t_i} \vert \mathcal{F}_{t_{i}}] = (I-K_{t_i}H)var[y_{t_i} \vert \mathcal{F}_{t_{i-1}}]$ whereas Jasurkova and Stehlikova state that: (page 214) $Var[r_{t_i} \vert \mathcal{F}_{t_{i}}] = (I-K_{t_i}H)Var[R_{t_i} \vert \mathcal{F}_{t_{i-1}}]$ the small $y$ in Bolder's work corresponds to the small $r$ , which are in both cases the short rates. And for the bond rates Bolder writes $z_{t_i}$ corresponding to $R_{t_i}$ in the work from Jasurkova and Stehlikova. Proceeding to step 4, when the state vector/variable is updated things continue to differ: Bolder writes: $\mathbb{E}[y_{t_{i+1}} \vert \mathcal{F}_{t_{i}}] = C + F\mathbb{E}[y_{t_{i}} \vert \mathcal{F}_{t_{i}}]$ In  the other paper it is written that: $\mathbb{E}[r_{t_{i+1}} \vert \mathcal{F}_{t_{i+1}}] = C + F\mathbb{E}[r_{t_{i}} \vert \mathcal{F}_{t_{i}}]$ and for the conditional variance: $var[y_{t_{i+1}} \vert \mathcal{F}_{t_{i}}] = var[y_{t_{i}} \vert \mathcal{F}_{t_{i-1}}]-Fvar[y_{t_{i}} \vert \mathcal{F}_{t_{i}}]F^T + Q$ $Var[r_{t_{i+1}} \vert \mathcal{F}_{t_{i}}] = FVar[r_{t_{i}} \vert \mathcal{F}_{t_{i}}]F^T + Q$ Additionally, the construction of the Likelihood function differs, beside the fact that Bolder uses the log-likelihood function: Log-likelihood function by Bolder (equation (77)): $l(\theta) = - \frac{nNln(2\pi)}{2}-\frac{1}{2} \sum_{i=1}^N [ln(det(var[y_{t_{i}} \vert \mathcal{F}_{t_{i-1}}])) + \zeta_{t_i}^{T}(var[y_{t_{i}} \vert \mathcal{F}_{t_{i-1}}]^{-1}\zeta_{t_i}]$ The Likelihood function by Jasurkova and Stehlikova: $\mathcal{L}(\kappa,\theta,\sigma,\lambda) = \prod_{i=1}^n \frac{1}{(2 \pi )^{\frac{m}{2}} \vert Var[R_{t_i} \vert \mathcal{F}_{t_{i-1}}] \vert ^{\frac{1}{2}} } e^{-\frac{1}{2} \zeta_{t_i}^{T}Var[R_{t_i} \vert \mathcal{F}_{t_{i-1}}]^{-1}\zeta_{t_i}}$ In total, these differences should result in a completely different likelihood function (beside the log) and therefore also in a different output of the Kalman filter. My question now is whether there is a mistake in one of the papers?
Or are there different Kalman filters that can explain these differences?","['statistics', 'kalman-filter', 'stochastic-processes', 'stochastic-programming', 'maximum-likelihood']"
4606490,Regular Borel measures,"I'm trying to solve the following problem and got stuck. Let $\mu$ and $\nu$ be two regular Borel measures on $\mathbb{R}^n$ such that $\mu = \nu$ on $\mathcal{B}(\mathbb{R}^n)$ . Then $\mu = \nu$ on $\mathcal{P}(\mathbb{R}^n)$ . My idea was the following: Let $F\subset\mathbb{R}^n$ , then by definition there exists $E\in\mathcal{B}(\mathbb{R}^n)$ such that $F\subset E$ and $$\mu(F) =\mu(E) = \nu(E) = \nu(H)$$ for some $H\subset\mathbb{R}^n$ and $H\subset E$ However, I don't know how to show that $F=H$ .
Any idea on how to show that $H = F$ ? Thank you for the help.","['borel-sets', 'measure-theory', 'borel-measures']"
4606523,Weak convergence of symmetries measures,"Consider a sequence of probability measure $\mu_1, \cdots, \mu_N$ with support on non-negative real numbers, $\mathbb{R}_{\geq 0}$ . Now, let for each $1 \leq i \leq N$ , $\hat{\mu}_i$ be the symmetrized measure of $\mu_i$ , which is defined as: $$
\hat{\mu} = \mu \, * \, \mu^\#
$$ with $\mu^\#(B) = \mu(-B)$ for each $B \in \mathfrak{B}(\mathbb{R})$ . Are the following two arguments equivalent? $$
 \mu_i \to \mu \hspace{10 pt} \text{weakly}  \Longleftrightarrow \hat{\mu}_i \to \hat{\mu} \hspace{10 pt} \text{weakly} 
$$","['measure-theory', 'weak-convergence', 'probability-distributions', 'probability-theory', 'probability']"
4606530,Determinant of $A^T A$ where $A$ is a block lower triangular matrix,"Is there a trick or simple way to compute $$\text{det}(A^T A)$$ where $A \in  \mathbb{R}^{m \times n}$ , $m \neq n$ , is a block lower triangular matrix? An example of such a matrix would be $$
\begin{pmatrix}
\mathbf{A} & \mathbf{0} & \mathbf{0} \\
\mathbf{B} & \mathbf{C} & \mathbf{0} \\
\mathbf{D} & \mathbf{E} & \mathbf{F} \\
\end{pmatrix},
$$ where all bold face sub-matrices are, for example, $3 \times 2$ . I know that if $A$ is a square matrix with square blocks, it is just the product of the determinants of the blocks on the diagonal. However, I'm interested in the case when the blocks are non-square. Note: If it helps, we may assume that the matrix $A$ has full column rank. In addition to the determinant, is there a simple way to find the inverse, i.e., $(A^T A)^{-1}$ ?","['matrices', 'linear-algebra']"
