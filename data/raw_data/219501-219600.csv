question_id,title,body,tags
4489619,Quantum probability vs. Kolmogorov's probability,"It has been argued that quantum probability requires a different treatment from other random events, and a different formalization is needed than that provided by Kolmogorov's axioms. And as a physicist by training I have read such statements dozens of times. However, I have never seen a simple and concrete example showing that some result of quantum mechanics cannot be reduced to the usal treatment random events over a measure space $(\Omega, \mathcal{F},\mathbb{P})$ . Could someone provide a reasonable example of the probability used in quantum mechanics not being reducible to Kolmogorov's axioms? Disclaimer : I am asking the question here and not in a physics space, because I would like to see a rigorous demonstration of the fact, with the formality and rigor proper to mathematics, so as not to leave room for the ambiguity typical of discussions in physics.","['quantum-mechanics', 'probability-theory']"
4489675,Infinitesimally small time intervals,"When saying that in a small time interval $dt$ , the velocity has changed by $d\vec v$ , and so the acceleration $\vec a$ is $d\vec v/dt$ , are we not assuming that $\vec a$ is constant in that small interval $dt$ , otherwise considering a change in acceleration $d\vec a$ , the expression should have been $\vec a = \frac{d\vec v}{dt} - \frac{d\vec a}{2}$ (Again assuming rate of change of acceleration is constant). According to that argument, I can say that $\vec v$ is also constant in that time interval and so $\vec a = \vec 0$ . Can someone point out where exactly I have gone wrong. Also this was just an example, my question is general.","['calculus', 'kinematics', 'derivatives']"
4489682,Find all analytic bijections $f: \mathbb{C} \to \mathbb{C}$. Justify that there are no other analytic bijections besides those you found.,"This is a question from a previous complex analysis qualifying exam. I'm working problems to study for my own upcoming qual. I'd like to know if my solution below is correct and complete or not. Thanks! Problem: Find all analytic bijections $f: \mathbb{C} \to \mathbb{C}$ . Justify that there are no other analytic bijections besides those you found. Attempted Solution: (1) Since we're looking for analytic functions on the whole complex plane, we are considering only functions which are entire. (2) A constant function is neither injective nor surjective, so clearly it is not a bijection. By Liouville's Theorem, any bounded entire function is constant. Thus, such a bijection must have a singularity at infinity since it is analytic everywhere else. (3) The singularity at infinity must be either a pole or an essential singularity. If there is an essential singularity at infinity, then by the Great Picard theorem, in any neighborhood of infinity, the function attains every value in $C$ (with at most one exception), so these functions would be ""infinity-to-1"". Thus, not bijections. (4) Finally, we are left with entire functions that have a pole at infinity. Any such function is a polynomial (since it can be written as a power series), but polynomials of degree $n$ are $n$ -to-1 functions. Thus, the only functions that are 1-to-1 are 1st degree polynomials or linear functions of the form $f(z) = \alpha z + \beta$ with $\alpha \neq 0$ . Update to include suggestion by @Greg Martin: (5) We have shown that if an entire function is a bijection, then it is linear. It remains only to show that if a function is linear, then it is an analytic bijection. We know that any polynomial is analytic. We have that a function $f(z)=\alpha z + \beta, \; \alpha\neq 0$ is injective if $f(z)=f(w) \implies z=w$ . This is straightforward as $$ \alpha z + \beta = \alpha w + \beta \implies z = w $$ by simply subtracting off $\beta$ and dividing both sides by $\alpha \neq 0$ . Then we have that $f$ is surjective if, for every $w \in \mathbb{C}$ , there exists a $z \in \mathbb{C}$ such that $f(z)=w$ . One can see that $$\alpha z + \beta = w \quad \implies z = \frac{w-\beta}{\alpha}, \; \alpha\neq 0. $$ Thus, for any $w$ there exists such a $z$ , and $f(z)$ is an analytic bijection.","['complex-analysis', 'solution-verification', 'analytic-functions']"
4489686,"Trace of $2021×2021$ symmetric matrix where each row is a permutation of $\{1,2,\cdots,2021\}$?","Suppose that $M$ is a $2021\times 2021$ symmetric matrix such that the entries of each row is set by rearranging $\{1,2,\cdots,2021\}$ . Then what is the trace of $M$ ? We know that ${\rm tr}(M)$ is the sum of all eigenvalues of $M$ . I can see that $\frac{2021\cdot 2022}{2}$ is an eigenvalue of $M$ since $$M(1,1,\cdots,1)^{\rm T}=(1+\cdots+2021)\cdot(1,1,\cdots,1)^{\rm T} .$$ But after that I’m stuck. Any help is appreciated.","['permutations', 'trace', 'linear-algebra', 'combinatorics', 'symmetric-matrices']"
4489696,Global sections of $p$-torsion of elliptic curve,"Suppose that $R$ is a ring and $E$ is an elliptic curve over $R$ . (That is, we have an elliptic curve $E$ which is a group scheme over the affine scheme $\text{Spec }R$ .) Then for any prime $p$ , we have the multiplication by $p$ -map $E \to E$ , which has group-scheme theoretic kernel $E[p]$ : $$0 \to E[p] \to E \to E.$$ I'd like to emphasize that I'm looking at $E[p]$ as a group scheme over $R$ , as opposed to just an abelian group. My question is : what are the global sections of the scheme $E[p]$ ? That is, if $\mathcal{O}$ denotes the structure sheaf of the scheme $E[p]$ , then what is $H^0(E[p], \mathcal{O})$ ? What I've tried : $E[p]$ is a closed subscheme of $\mathbf{P}^2_{R}$ , and so it is of the form $\text{Proj}(R[x,y,z]/I)$ for some homogenous ideal $I$ . Here is where I'm stuck: if $R$ was an algebraically closed field, then the global sections would be the constants. But I'm not sure what the global sections would be for general rings $R$ . Any clarification / help would be appreciated. Thanks!","['algebraic-geometry', 'schemes', 'elliptic-curves']"
4489700,Can one prove a connection between the order of the polynomials in a differential equation and the roots of the characteristic equation?,"I am teaching myself how to solve differential equations and have now gone through a section on non-homogeneous second order linear equations. I have the following problem to solve, which I have been stuck on for a few days: Assume that the differential equation $ y''+ay'+by=p(x)e^{kx}$ has a solution $q(x)e^{kx}$ , where both $p(x)$ and $q(x)$ are polynomials.
Prove the following: If k is not a root of the characteristic equation, then the degree of q equals the degree of p If k is a simple root of the characteristic equation, then the degree of q is one higher than the degree of p If k is a double root of the characteristic equation, then the degree of q is two higher than the degree of p I have tried substituding y for a guessed solution of the form $y=ze^{kx}$ where z is a polynomial and expanded the differential equation, but this has brought me nowhere. I honestly have no idea how to go forward with this. Can anyone think of a strategy?",['ordinary-differential-equations']
4489716,Will squaring both sides of the ode change its type?,"Given the ode \begin{equation}
y^{\prime}=\sqrt{1+x+y}\tag{1}
\end{equation} The question is, is this a D'Alembert's ode or not? As is known, D'Alembert's has the form Wikipedia \begin{equation}
y=xf\left(  p\right)  +g\left(  p\right)  \tag{2}
\end{equation} Where $p\equiv\frac{dy}{dx}=y^{\prime}$ . As it stands (1) is not of the form
(2). But after squaring both sides of (1) the ode becomes \begin{align}
\left(  y^{\prime}\right)  ^{2}  & =1+x+y\nonumber\\
y  & =-x-1+\left(  y^{\prime}\right)  ^{2}\tag{3}\\
& =xf\left(  p\right)  +g\left(  p\right)  \nonumber
\end{align} Where \begin{align*}
f\left(  p\right)    & =-1\\
g\left(  p\right)    & =-1+p^{2}%
\end{align*} Now ode (3) is D'Alembert's. But this is after squaring both sides of (1). So the question is, is it mathematically correct to say (1) is D'Alembert's ode
or not? Maple agrees and says that (1) is D'Alembert's ode. So it must have squared both sides. restart;
ode:=diff(y(x),x)=sqrt(1+x+y(x));
DEtools:-odeadvisor(ode); gives [[_homogeneous, `class C`], _dAlembert] But some might not agree with Maple here. I am not sure if one is allowed to do that, or if one must only apply the
form comparison on the original ode without squaring it. Hence my question. I am well aware that squaring the ode and solving the squared ode will introduce extraneous solutions to the original ode , and one must therefore verify that each solution generated satisfies the original ode, else removed as  extraneous. But that is not my question. My question is on the type of the original ode itself. In particular , I am not sure now if squaring both sides of the ode will generate an ode with all of its solutions that fail to verify the original ode. I do not have such an example myself of such case, but this does not mean such case does not exist. In the example given in this question at the top, squaring the ode generates two solutions, one of which is extraneous and can be discarded, but the second one does verify the original ode. On a side note, without squaring the above ode and solving it as d'Alembert type, I now have no idea how to solve the original ode as is, as I do not know what type it is, and hence do not know what method to apply to it. Maple says the above ode is also of type _homogeneous, class C . But I looked at Maple's website , and read the description, but did not follow it and still have no idea still what it means as there is no actual reference outside Maple that says what _homogeneous, class C ode type is.",['ordinary-differential-equations']
4489747,Why do I need Fubini to evaluate these integrals of products of functions?,"I am currently reading a book on applied mathematics, and the current chapter is proving some statements related to probability theory. I'm stuck with one step in a derivation, because the author claims to use Fubini('s theorem), but I do not see why he would need it. Concretely, consider continuous random variables $X, Y$ on $\mathcal{X}$ with probability densities $p(x), q(y)$ , respectively, as well as a function $\phi:\mathcal{X}\rightarrow\mathbb{R}^N$ . The steps in question are $$\mathbb{E}_p[\phi(X)^T]\mathbb{E}_p[\phi(X)]=\int\phi(x)^Tp(x)dx\int\phi(x')p(x')dx'=\int\int\phi(x)^T\phi(x')p(x)p(x')dxdx'$$ and $$\mathbb{E}_p[\phi(X)^T]\mathbb{E}_q[\phi(Y)]=\int\phi(x)^Tp(x)dx\int\phi(y)q(y)dy=\int\phi(x)^T\phi(y)p(x)q(y)dxdy$$ I can not figure out why there would be any need to use Fubini in either step. What's more, I think that the order of integration wouldn't matter either way, since according to my understanding I can just do (e.g. in the second case): $$\int\phi(x)^Tp(x)dx\int\phi(y)q(y)dy=\int\left(\int\phi(y)q(y)dy\right)\phi(x)^Tp(x)dx=\int\left(\int\phi(y)q(y)\phi(x)^Tp(x)dy\right)dx$$ where in the first step I used that the integral in parentheses exists and hence has a constant value, and in the second step that $\phi(x)^Tp(x)$ is constant with respect to $y$ . Analogously, $$\int\phi(x)^Tp(x)dx\int\phi(y)q(y)dy=\int\left(\int\phi(x)^Tp(x)dx\right)\phi(y)q(y)dy=\int\left(\int\phi(y)q(y)\phi(x)^Tp(x)dx\right)dy$$ Which means the order of integration does not matter. (I understand that this would be different if the integrand were e.g. a function $h(x,y)$ for which I wouldn't just have $h(x,y)=f(x)g(y)$ for some appropriate $f,g$ , and then I would need Fubini. Does anybody see why the author would write that one needs Fubini?","['integration', 'measure-theory', 'real-analysis', 'fubini-tonelli-theorems', 'probability']"
4489782,$ A\sin\alpha + B\cos\alpha=C$ solution confusion,The solution of equation: $$ A\sin\alpha + B\cos\alpha=C$$ according to python is: $$\alpha=2\arctan\left[\frac{A \pm \sqrt{A^2+B^2-C^2}}{B+C}\right]$$ My question is why two answers or in otherwords in which situations we choose the postive and which the negative ? Can we get rid of the plus minus sign to outside of the function inside the $\arctan$ to outside?,"['calculus', 'trigonometry']"
4489808,Does $z$ depend on $x$ in the proof of Theorem 6.20 in Folland?,"The theorem states: Let $K$ be a Lebesgue measurable function on $(0, \infty) \times (0, \infty)$ such that $K(\lambda x, \lambda y) = \lambda^{-1}K(x,y)$ for all $\lambda > 0$ and $\int_0^\infty |K(x,1)|x^{-1/p}\,dx = C < \infty$ for some $p \in [1, \infty]$ , and let $q$ be the conjugate exponent to $p$ . For $f \in L^p$ and $g \in L^q$ , let $$Tf(y) = \int_0^\infty K(x,y)f(x)\,dx, \quad Sg(x) = \int_0^\infty K(x,y)g(y)\,dy.$$ Then $Tf$ and $Sg$ are defind a.e., and $\|Tf\|_p \leq C\|f\|_p$ and $\|Sg\|_q \leq C\|g\|_q$ . The proof proceeds as follows: Setting $z = x/y$ , we have $$\int_0^\infty |K(x,y) f(x)|\,dx = \int_0^\infty |K(yz,y) f(yz)|y\,dz = \int_0^\infty |K(z,1) f_z(y)|\,dz$$ where $f_z(u) = f(yz)$ ; moreover $$\|f_z\|_p = \Bigg[\int_0^\infty |f(yz)|^p\,dy\Bigg]^{1/p} = \Bigg[\int_0^\infty |f(x)|^pz^{-1}\,dx\Bigg]^{1/p} = z^{-1/p}\|f\|_p.$$ The doubt I have is rather elementary. In the very last equality, how can we pull out $z$ if $z$ itself depends on $x$ ? The proof then proceeds: Therefore, by the Minkowski inequality for integrals $Tf$ exists a.e. and $$\|Tf\|_p \leq \int_0^\infty |K(z,1)\|f_z\|_p\,dz = \|f\|_p \int_0^\infty |K(z,1)|z^{-1/p}\,dz = C\|f\|_p.$$ How does the Minkowski inequality for integrals imply the existence of $Tf$ ? I understand the rest of the proof so I have omitted it.","['integration', 'lp-spaces', 'lebesgue-integral', 'real-analysis']"
4489810,Finding $\alpha$ such that $\;\tan\alpha=\frac{8\sqrt3\cos^240^\circ+8\sqrt3\cos20^\circ-2\sqrt3}{(8\cos^240^\circ-1)(8\cos20^\circ-1)-3}$,I have to find $\alpha$ (in degrees) so it is satisfied $$\tan{\alpha}=\frac{8\sqrt{3}\cos^{2}40^{\circ}+8\sqrt{3}\cos20^{\circ}-2\sqrt{3}}{\left(8\cos^{2}40^{\circ}-1\right)\left(8\cos20^{\circ}-1\right)-3}$$ I tried to write $\cos40^{\circ}=2\cos^{2}20^{\circ}-1$ . ALso i tried $\cos^{2}40^{\circ}=\frac{1+\cos80^{\circ}}{2}=\frac{1+\sin10^{\circ}}{2}$ and to simplify the expression but unsuccessfully.,['trigonometry']
4489811,check the set of point of Continuity of the function [duplicate],"This question already has answers here : Explanation of Proof for (Dis)continuity of Thomae's Function (2 answers) Closed 1 year ago . I know that this kind of problem is asked on this site earlier but I put this question here to clear my doubt related to my conceptual confusion about these kinds of functions. Consider the function $$
f(x)= \begin{cases} 1/q : x\in p/q \cap [0,1]; p,q \in \mathbb{N}, (p,q)=1 \\ 0    :~~~~~~~~~~~~~~~~~ else \end{cases}
$$ Then How to check the continuity of $f(x) $ in the given domain I know how to check the continuity of function by sequential criteria like the Dirichlet function. But in this function, how do select how sequences of a rational and irrational number?? I have studied about continuity of these kinds of functions which are defined by rationals and irrationals; I think there are a maximum of four possible types of sequences to check about continuity of function in the given domain that is we choose the sequence of rationals that converges to irrationals or rational and
the sequence of irrationals that converges to irrationals or rational
and these four types of sequences satisfy the sequential criteria of continuity then we say the function is continuous or not?? Please correct me if I am not correct and please give some hints to solve the above question. Thanks!","['continuity', 'functions', 'sequences-and-series']"
4489842,"Is there a function $f:[0,\infty) \to \{-1, 1\}$ such that $\int_0^\infty{f(x)\,dx}$ is well defined?","I suspect it is impossible to find such an $f$ , but I can see one way it might be possible to as well. If it is possible to construct two sets $A$ and $B$ which partition the non-negative reals such that $A \cap [0,a)$ and $B \cap [0,a)$ both have measure $a/2$ for any positive real $a$ , then take $f^{-1}(1) = A$ , $f^{-1}(-1) = B$ . We would have $\int_0^a{f(x)\,dx}=0$ for all $a$ and $\lim_{a \to \infty}\int_0^a{f(x)\,dx}=0$ .","['riemann-integration', 'measure-theory', 'improper-integrals', 'real-analysis']"
4489851,Two affine group schemes over $k$ with isomorphic group for each $k$-algebra $S$ but not isomorphic as affine group schemes,"I'm looking for two affine group schemes $G_1$ and $G_2$ (i.e. functors from (affine group schemes over $k$ ) to (Group) ) such that for every $k$ -algebra $S$ , $G_1(S)\cong G_2(S)$ , but $G_1$ and $G_2$ are not isomorphic as affine group schemes (i.e. as functors). The following is the guideline of the example given to me. In the following $k=\mathbb{F}_p$ . Consider $\varphi:\operatorname{Spec}k[x^{\pm 1}] = \mathbb G_m\to \operatorname{Spec}k[x^{\pm 1}] = \mathbb G_m$ corresponding to the map $x\mapsto x^p$ and define $\mu_p = \operatorname{Spec}\left(k[x^{\pm 1}]/(x^p-1)\right) = \ker\varphi$ . Consider $F_p:\operatorname{Spec}k[x] = \mathbb G_a\to \operatorname{Spec}k[x] = \mathbb G_a$ corresponding to the map $x\mapsto x^p$ and define $\alpha_p = \operatorname{Spec}\left(k[x]/(x^p)\right) = \ker F_p$ . I know how to proof that $\mu_p$ is not isomorphic to $\alpha_p$ as affine group schemes. Now for all $k$ -algebra $S$ we have $\mu_p(S) = \{s\in S \ | \ s^p = 1\}$ and $\alpha_p(S) = \{s\in S \ | \ s^p=0\}$ . Then we have a bijection: $$
\mu_p(S) \longleftrightarrow \alpha_p(S), \qquad s\longleftrightarrow s-1
$$ Also $\alpha_p(S)$ and $\mu_p(S)$ are abelian, and each non trivial element of each group has order $p$ . I'm stuck here. I know that the informations stated above are sufficient to imply that $\alpha_p(S)\cong \mu_p(S)$ if the two groups are finitely generated, but I don't know how to conclude (and if it is possible) in the not finitely generated case. Any hint or ideas? Thank you in advance.","['group-schemes', 'affine-schemes', 'algebraic-geometry', 'abelian-groups', 'schemes']"
4489856,Divisors on $\mathrm{Spec}(\mathbb{Z})$ and normed-modules,"Let $D$ a Weil divisor on $\mathrm{Spec} (\mathbb{Z})$ , so we can write : $$ D = \sum_{p} \mathrm{ord}_p(D) [p]$$ with coefficients $\mathrm{ord}_p(D) \in \mathbb{Z}$ . We define : $$ H^0(\mathrm{Spec}(\mathbb{Z}),\mathcal{O}_{\mathrm{Spec}(\mathbb{Z})}(D))= \lbrace f \in \mathbb{Q}^{*} \; : \; (f)+D \geq 0 \rbrace \cup \lbrace 0 \rbrace$$ where $(f)$ is the principal divisor associated with the rational number $f$ . $H^0(\mathrm{Spec}(\mathbb{Z}),\mathcal{O}_{\mathrm{Spec}(\mathbb{Z})}(D))$ is a free $\mathbb{Z}$ -module of rank $1$ . For any $n \in \mathbb{N}$ , $H^0(\mathrm{Spec}(\mathbb{Z}),\mathcal{O}_{\mathrm{Spec}(\mathbb{Z})}(nD))$ is also a free module of rank $1$ ? Let $\|.\|$ be a norm on $H^0(\mathrm{Spec}(\mathbb{Z}),\mathcal{O}_{\mathrm{Spec}(\mathbb{Z})}(D)) \otimes_{\mathbb{Z}} \mathbb{R}$ . We denote $B(D,\|.\|)$ the set : $$ B(D,\|.\|) = \lbrace f \in H^0(\mathrm{Spec}(\mathbb{Z}),\mathcal{O}_{\mathrm{Spec}(\mathbb{Z})}(D)) \otimes_{\mathbb{Z}} \mathbb{R} \; : \; \|f\| \leq 1\rbrace$$ the unit ball for the norm. I want to compute or etablish an explicit formula for $\log \mathrm{vol} (B(D,\|.\|))$ But I don't have any strategy, I just know that I have to put a Haar measure on $\mathbb{R}$ , can I choose the Lebesgue measure ? And then, what's the formula for $\mathrm{vol} (B(D,\|.\|))$ ? If you have a reference for this kind of theory I would like (article or book).","['algebraic-number-theory', 'algebraic-geometry', 'arithmetic-geometry']"
4489861,A question about $GL_n(\mathbb{Z})$ and $GL_n(\mathbb{F}_p)$,"Let $p\ge 3$ be a prime number. $G$ is a subgroup of $GL_n(\mathbb{Z})$ and $|G|<\infty$ . Let $\sigma: GL_n(\mathbb{Z})\to GL_n(\mathbb{F}_p)$ be the natural map. Prove that $\sigma|_G$ is injective. Suppose $\exists A,B \in G,A\ne B$ , s.t. $\sigma(A)=\sigma(B)$ i.e. $A=B \pmod{ p}$ . Since $|G|<\infty$ , $\exists m,k\in\mathbb{Z}^+$ , $A^m=B^k=I$ . In linear algebra we know $A=C\ {\rm diag}(\zeta_1,\dots,\zeta_n)\ C^{-1}$ where $\zeta_i^m=1,C\in GL_n(\mathbb{C})$ . But I don't know if $m=k$ . Taking trace and norm can't solve this problem. I think $A=B \pmod{p} $ is not easy to use. Any ideas?","['matrices', 'group-theory', 'abstract-algebra', 'linear-algebra']"
4489867,Expected value of vitamins taken of the prefered taste,"I saw the following problem here : Ex 1.5.7. You and your spouse each take two gummy vitamins every day. You share a single bottle of 60 vitamins, 30 of one flavor and 30 of another. You each prefer a different flavor, but it seems childish to fish out two of each type (but not to take gummy vitamins). So you just take the first four that fall out and then divide them up according to your preferences. For example, if there are two of each flavor, you and your spouse get the vitamins you prefer, but if three of your preferred flavor come out, you get two of the ones you like and your spouse gets one of each. Of course, you start a new bottle every 15 days. On average, over a 15 day period, how many of the vitamins you take are the flavor you prefer? (From fivethirtyeight.com .) Here's how I tackled it. Instead of considering 60 vitamins in a bottle and getting 4 of what's left, we'll suppose the vitamins are placed in something like a mini vending machine, which is clicked once a day and spits the 4 closest-to-the-edge gummy vitamins. In this way we can represent the vitamins as a string, which consists of 30 letters 'a' (for my spouse's preferred flavor) and 30 letters 'b' (for my preferred flavor). For example, if the string is $abaabaabbaaabbbaaaabbbabbbaaabaabaabbabababaaabbabbabbaabbbb$ , we can make it clear which vitamins are taken every day: $$abaa\ baab\ baaa\ bbba\ aaab\ bbab\ bbaa\ abaa\ baab\ baba\ baba\ aabb\ abba\ bbaa\ bbbb$$ What will be important, is not the string itself, rather the 15 quadruplets, it consists of, and more specifically the number of each. Obviously, $bbaa$ is the same as $baba$ , so each quadruplet can be presented as one of these five multisets: $$M_1=\{4.a\};\ M_2=\{3.a,1.b\};\ M_3=\{2.a,2.b\};\ M_4=\{1.a,3.b\};\ M_5=\{4.b\}$$ Let $x_i$ be the number of occurrences of $M_i(i=1,2,3,4,5)$ in the string. The quadruplets of the example above can be written more compactly as a multiset of multisets: $S=\{0.M_1, 4.M_2, 8.M_3, 2.M_4, 1.M_5\}$ . For a general string of 'a'-s and 'b'-s: $$S=\{x_1.M_1, x_2.M_2, x_3.M_3, x_4.M_4, x_5.M_5\}$$ Two equations can be written right away: $$x_1+x_2+x_3+x_4+x_5=15\ \ \ \ \ (1)$$ $$x_1(4.a)+x_2(3.a+b)+x_3(2.a+2.b)+x_4(a+3.b)+x_5(4.a)=30a+30b$$ The former one shows that every bottle (string) is used up in 15 days, and the latter - for these 15 days all 30 vitamins of each kind are taken. The latter equation can be split in two by equating the coefficients of $a$ and $b$ : $$4x_1+3x_2+2x_3+x_4=30\ \ \ \ \ (2)$$ $$x_2+2x_3+3x_4+4x_5=30\ \ \ \ \ (3)$$ For every occurrence of $M_1$ I'll take two 'a'-s (not my preferred flavor), for each $M_2$ - one of each kind and for all of the rest cases I'll take two of my favorite 'b'-s. This means that I'll take $N=2x_1+x_2$ of the ones I don't like and $L=x_2+2x_3+2x_4+2x_5$ of my preferred flavor. Considering $(1)$ , we have $N+L=30$ , which means that counting $N$ is enough to solve our problem. Now let $\overline{x_i}$ be the average value of $x_i$ after (infinitely) many 15-day periods, so it can be thought of as the expected value of $x_i$ . Equations $(1)$ , $(2)$ and $(3)$ can be rewritten in terms of the average values $\overline{x_i}$ (the proof is trivial): $$\overline{x_1}+\overline{x_2}+\overline{x_3}+\overline{x_4}+\overline{x_5}=15\ \ \ \ \ (4)$$ $$4\overline{x_1}+3\overline{x_2}+2\overline{x_3}+\overline{x_4}=30\ \ \ \ \ (5)$$ $$\overline{x_2}+2\overline{x_3}+3\overline{x_4}+4\overline{x_5}=30\ \ \ \ \ (6)$$ Similarly for the expected values of $N$ and $L$ : $$\overline{N}=2\overline{x_1}+\overline{x_2}$$ $$\overline{L}=\overline{x_2}+2\overline{x_3}+2\overline{x_4}+2\overline{x_5}$$ We'll evaluate $\overline{N}$ . From now on, my solution seems too elaborate and brute-forced. I think there's a more clever one, though I have no idea what it is. Here's how I continue. I consider every multiset of 15 quadruplets in terms of its $x_1$ and $x_2$ and count the number of such multisets across all possible 60-elemnet strings of 'a'-s and 'b'-s. For example, for $x_1=4$ and $x_2=2$ , we have 4 quadruplets with 4 'a'-s, 2 quadruplets with 3 'a'-s and 9 quadruplets with no more than 2 'a'-s. First, we have to arrange $4.M_1$ and $2.M_2$ in 15 empty slots. The number of ways it can be done is: $$\text{ExternalOrderings}(4,2)=\binom{15}{4\ \ \ 2\ \ \ 9}=\frac{15!}{4!2!9!}=75075$$ All $M_1$ sets have only one internal ordering (as they consist only of 'a'-s), but for every $M_2$ set its elements have $\binom{4}{1}=4$ permutations and adds a multiplier of $4$ . In our case $x_2=2$ , so we can write: $$\text{InternalOrderings}(4,2)=4^2=16$$ Now we are left with $\{8.a, 28.b\}$ for $15-2-4=9$ empty quadruplets. All of them are of type $M_3$ , $M_4$ or $M_5$ , therefore, in each of them, there should be no more than 2 'a'-s. We now first count any placements of the remaining 'a'-s and then with the Inclusion-Exclusion Principle (IEP) the number of invalid ones will be found. First, we will place the remaining 8 'a'-s in any of the $9*4=36$ empty spots. This can be done in: $$\text{RemainigAll}(4, 2)=\binom{36}{8}=\frac{36!}{8!28!}=30260340$$ ways. However, this number counts many invalid cases, where one or more of the quadruplets are of type $M_1$ or $M_2$ . There are a few cases of these (only problematic quadruplets are characterized): $$(1*M_1)\rightarrow A \text{ of these}$$ $$(1*M_2)\rightarrow B \text{ of these}$$ $$(1*M_1, 1*M_2)\rightarrow C \text{ of these}$$ $$(2*M_1)\rightarrow D \text{ of these}$$ $$(2*M_2)\rightarrow E \text{ of these}$$ Now, not all of these can be evaluated with ease, but here the power of IEP shines bright. We can with ease find the number of cases with at least $1.M_1$ , the number of cases with at least $1.M_2$ , etc. We do this by first finding external orderings, then internal orderings and at last letting what's left of the 'a'-s free. We calculate: $$N_1=N(\text{at least }1*M_1)=\binom{9}{1}*4^0*\binom{32}{4}=9.1.35960=323640$$ $$N_2=N(\text{at least }1*M_2)=\binom{9}{1}*4^1*\binom{32}{5}=9.4.201,376=7249536$$ $$N_3=N(\text{at least }1*M_1,1*M_2)=\binom{9}{1\ \ \ 1\ \ \ 7}*4^1*\binom{28}{1}=72.4.28=8064$$ $$N_4=N(\text{at least }2*M_1)=\binom{9}{2}*4^0*\binom{28}{0}=36.1.1=36$$ $$N_5=N(\text{at least }2*M_2)=\binom{9}{2}*4^2*\binom{28}{2}=36.16.378=217728$$ I will show why IEP is applicible here. $N_1$ counts much more than the members of $A$ alone. It counts the members of $A$ , the members of $C$ and the members of $D$ twice , so we can write $N_1=A+C+2D$ . By similar reasoning we find $N_2=B+C+2E$ and finally the trivial $N_3=C$ , $N_4=D$ and $N_5=E$ . It's easy to convince ourselves that the number of invalid cases is: $$\text{Invalid}(4, 2)=A+B+C+D+E=N_1+N_2-N_3-N_4-N_5=7347348$$ This is a direct application of IEP with two variables (in our case the number of $M_1$ -s and $M_2$ -s). The sign in front of $N_i$ depends on the parity of the sum # $M_1+$ # $M_2$ - positive for odd sum and negative for even sum. At last, we find for this particular case $(x_1=4, x_2=2)$ that the number of such 60-element strings are: $$N(x_1=4, x_2=2)=\text{ExternalOrderings}(4,2)*\text{InternalOrderings}(4,2)* 
 \left(\text{RemainigAll}(4, 2)-\text{Invalid}(4, 2) \right)=75075*16(30260340-7347348)=27523085990400$$ What we now do with this huge number, is to multiply it by $(2x_1+x_2)=10$ , as this gives the number of not-preferred vitamins of mine I'll take, if all these strings appear at some point in our random vending machine. Let's have a number $\text{NotLiked}$ , which counts all such not-liked vitamins of mine across all possible scenarios for the pair $(x_1, x_2)$ . The contribution in the case $(x_1=4, x_2=2)$ to $\text{NotLiked}$ is $275230859904000$ . When we run across all possible pairs $(x_1, x_2)$ and $\text{NotLiked}$ is evaluated, it will be divided by the total number of strings, that may appear in our vending machine. They are $\text{All}=\binom{60}{30}=118264581564861424$ . Finally we'll have: $$\overline{N}=\frac{\text{NotLiked}}{\text{All}}$$ It's evident how tedious this work is and it's almost impossible to be done by hand. So I wrote a program in C++ for me. Here's the code: #include <iostream>
#include <cmath>
using namespace std;

int d;

long long Power (int a, int n) //evaluates a^n
{
    long long ans=1;
    for (int i=0; i<n; i++)
        ans*=a;
    return ans;
}

long long Binomial (int n, int k)
{
    long double temp=1;
    if (k >(n-k))
        k=n-k;
    int j=k;
    for (int i=n; i>n-k; i--)
    {
        temp*=i;
        temp/=j;
        j--;
    }
    long long fl=floor (temp), ce=ceil(temp);
    if (temp-floor(temp)<ceil(temp)-temp) //temp is closer to its floor value
        return fl;
    else
        return ce;
}

long long Trinomial (int n, int k1, int k2)
{
    int k3=n-k1-k2;
    long double temp=1;
    int j=n;
    for (int i=1; i<=k1; i++)
    {
        temp*=j;
        temp/=i;
        j--;
    }
    for (int i=1; i<=k2; i++)
    {
        temp*=j;
        temp/=i;
        j--;
    }
    for (int i=1; i<=k3; i++)
    {
        temp*=j;
        temp/=i;
        j--;
    }
    long long fl=floor (temp), ce=ceil(temp);
    if (temp-floor(temp)<ceil(temp)-temp)
        return fl;
    else
        return ce;
}

long long IEP (int p, int rem_a) //p - number of free 4-tuplets
{
    long long ans=0;
    for (int i=0; i<=rem_a/3; i++) // i shows at least how many {3.a, 1.b} are there
    {
        for (int j=0; j<=(rem_a-3*i)/4; j++) // j shows at least how many {4.a} are there
        {
            if (i+j==0) //i=j=0 => this includes all cases (valid and invalid)
                continue;
            long long ext_ord = Trinomial(p, i, j);
            long long int_ord = Power (4, i);
            long long rem_ord = Binomial (4*(p-i-j), rem_a-3*i-4*j); //the rest of the 'a'-s can be placed anywhere, the IEP will deal with the excessive counting
            short int sign=1; // positive, if i+j is odd, and negative otherwise
            if ((i+j)%2==0)
                sign=-1;
            ans+=ext_ord*int_ord*rem_ord*sign;
        }
    }
    return ans;
}

long long Number_of_Orderings (int x1, int x2)
{
    long long ext_ord = Trinomial(d, x1, x2); //external orderings of x1 * {4.a}, x2 * {3.a, 1.b} and d-x1-x2 with at most 2.a
    long long int_ord = Power (4, x2); //internal orderings of all {3.a, 1.b}
    int rem_a=2*d-(4*x1+3*x2); // Number of remaining (unassigned) 'a'-s
    //Checking special cases (which are easily dealt with):
    if (rem_a==0)
        return ext_ord*int_ord;
    else
        if (rem_a <= 2) //they are 1 or 2
        {
            int rem_ord=Binomial(4*(d-x1-x2), rem_a); //all remaining 'a'-s can be placed in any of the free spots left
            return ext_ord*int_ord*rem_ord;
        }
    if (rem_a == 2*d) //when x1=x2=0 => all 4-tuplets are {2.a, 2.b}
    {
        long long rem_ord=Power (6, d); // the numbering of orderings of the remaining 'a'-s ; 6 = Binomial (4, 2)
        return ext_ord*int_ord*rem_ord;
    }
    if (rem_a == 2*d-3) //when x1=0 x2=1 => one 4-tuplet is {1.a, 3.b} and all but these two are {2.a, 2.b}
    {
        long long rem_ord=(d-1)*4; //d-1 -> number of unassigned 4-tuplets; 4 -> internal orderings for {1.a, 3.b}
        rem_ord = Power (6, d-2);
        return ext_ord*int_ord*rem_ord;
    }
    long long all = Binomial(4*(d-x1-x2), rem_a); //all possible placements of the remaining 'a'-s (including cases, when in some of the remaining 4-tuplets there are 3 or 4 'a'-s)
    //Now whit the Inclusion-Exclusion Principle (IEP) the number of invalid cases will be found.
    long long invalid = IEP (d-x1-x2, rem_a);
    long long rem_ord=all-invalid;
    return ext_ord*int_ord*rem_ord;
}

int main ()
{
    cin>>d;
    long long not_liked=0;
    long long all=Binomial(4*d, 2*d);
    for (int x1=floor(d/2); x1>=0; x1--)
    {
        int rem=2*d-4*x1;
        for (int x2=rem/3; x2>=0; x2--)
        {
            long long ord=Number_of_Orderings(x1, x2);
            not_liked+=ord*(2*x1+x2);
        }
    }
    long double ans=(long double) not_liked;
    ans/=all;
    double perc=(ans*100)/(2*d);
    cout<<""On average you take ""<<ans<<"" of the not-liked, which is ""<<perc<<"" % of all.""<<endl;
    cout<<""On average you take ""<<30-ans<<"" of the liked, which is ""<<100.0-perc<<"" % of all.""<<endl;
    return 0;
} The program deals with the more general case, where a bottle of vitamins is used up in d days. In our case, the input is d=15 . The corresponding output is: On average you take 5.43228 of the not-liked, which is 18.1076 % of all.
On average you take 24.5677 of the liked, which is 81.8924 % of all. In conclusion, $\overline{N}\approx5.43$ , which means that $\overline{L}\approx24.57$ and this is our final answer. As I said, this solution relies too much on brute force and I personally don't like it. Any suggestions for what I can improve are welcomed. If you cannot come up with something better, please upvote, so that more people can see this problem.","['binomial-coefficients', 'combinatorics', 'probability-theory', 'probability']"
4489898,What are some other other examples similar to completing the square where a derived value is added and taken away again to create a useful form?,"After 18 months of studying an advanced junior high school mathematics course, I'm doing a review of the previous 6 months, starting with solving difficult quadratics that are not easily factored, for example: $$x^2+6x+2=0$$ This could be processed via the quadratic equation but the course I'm working through asks me to use the complete the square method.  I can do it, and I appreciate the geometric illustration of what is happening. But it's so powerful and elegant, I can't help but wonder where else this method of adding something into an expression only to take it away in another is employed in mathematics.  And is there a name for the general case of this kind of operation?",['algebra-precalculus']
4489915,Finding a simpler form for a sum involving harmonic numbers and binomial coefficients,"I was trying to simplify the expression $\displaystyle \sum_{j=k}^n j(j-1) {n \brack j} {j \brace k}$ for $0 \leq k \leq n$ , where ${n \brack j}$ and ${j \brace k}$ denote the Stirling number of the first and second kind respectively. I ended up with the following equation, where $H_n$ denotes the n-th harmonic number: $$2 \cdot \Biggl[ \frac{n!}{(k-2)!} \displaystyle \sum_{j=1}^{n-k+1} H_j \frac{(-1)^{j+1}}{j+1} \binom{n+1}{k+j} + \frac{n!}{(k-1)!} \displaystyle \sum_{j=1}^{n-k} H_j \frac{(-1)^{j+1}}{j+1} \binom{n}{k+j} \Biggr] $$ Next up, I wanted to simplify the sums. After shifting the index by $k$ , $$ \displaystyle \sum_{j=k+1}^{n}  (-1)^{j-k+1} \binom{n}{j} \frac{H_{j-k}}{j-k+1} $$ it looks a lot like the following equation, where $H_n^{(2)} = \sum_{j=1}^n \frac{1}{j^2}$ : $$\sum_{j=k+1}^n(-1)^{j-1}{n\choose j} \frac{H_j}{j}=H_n^{(2)} - H_k^{(2)}.$$ which can be proven according to the second answer in A finite sum involving the binomial coefficients and the harmonic numbers . But of course, some terms are shifted, while the binomial coefficient stays the same. I tried solving it similarly to the equation above using the binomila formula, but wasn't able to find a solution due to the shifted terms. At this point I'm not sure whether there is an easier form, but I thought someone might recognize it from somewehere or know a way to prove it. I will keep on trying to simplify it and update the question if I find a way. The reason the expression is interesting is that it can be used to calculate the Variance of the Lah-distribution, which is defined in Definition 1.1 in https://arxiv.org/abs/2105.11365 . Although the authors do calculate a formula for the second moment in Corollary 3.6, I have used a different method to arrive at the above mentioned formula and am therefore interested whether you can find a nicer form for the second moment than Corollary 3.6 gives. I have tested my formula numerically and it is correct, so simplifying it might be worthwhile.","['harmonic-numbers', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics', 'sequences-and-series']"
4489927,About Group Isomorphism Kernel,"It's clear to me that a group homomorphism kernel measures how much the homomorphism is close to be an isomorphism, i.e. if this kernel contains more than the identity, the homomorphism cannot be injective, and so cannot lead to any isomorphism at all. This sounds to me a ""necessary"" condition, so far so good. I was wondering whether this is a ""sufficient"" condition as well, i.e. whether to check a kernel being made by the identity only it's ENOUGH to claim the homomorphism is actually a isomorphism. Please, give some counter examples in negative case.
Tx in advance.","['group-theory', 'group-isomorphism']"
4489953,Can this property of certain pythagorean triples in relation to their inner circle be generalized for other values of $n$?,"This question was raised in comments of Is the $(3,4,5)$ triangle the only rectangular triangle with this property? and I was suggested to ask it as a separate one. First some notation, let's write $(a,b,c)$ where $a<=b<c$ for a pythagorean triple, and let's write $(x,y,z)$ where $x<=y<z$ for the distances from the vertices to the center of the inner circle of the corresponding right triangle. ( $XYZ$ should be lowercase but geogebra did not allow these labels) The answer to above question proved (partially left to reader) the property: $x * y = z$ if and only if $c - b = 1$ In comments was asked if a similar property would exist for $c - b = 2$ and it was confirmed (and proof was left to reader) that: $x * y = 2 * z$ if and only if $c - b = 2$ Somewhat natural question then was raised (by me) if one can generalize for other values of $n >= 1$ , that is: for which $n$ (perhaps all) holds: $x * y = n * z$ if and only if $c - b = n$ ? Thanks to @heropup for the suggestion (and the answers for $n=1,2$ ) update A simple computer programmed enumeration seems to confirm equivalence. At least for all $(a,b,c)$ with maximum $c <= 10000$ . Note that it is not known to me (but perhaps it is known to others) if all $c - b$ cover all $n >= 1$ . So asking for all $n$ is a bit ambiguous since some $n$ might never occur. An alternative, perhaps better, question rephrase is: Prove equality $$x * y = (c - b) * z$$ for all pythagorean triples. update see also my answer below for non pythagorean triples","['triangles', 'pythagorean-triples', 'geometry']"
4489964,How to solve the equation in algebraic number theory?,"First step: When $p\equiv 1 \pmod{ 3}$ , prove that there exists a pair $(a,b)$ of integers such that $4p=a^2+27b^2$ , $a\equiv 1 \pmod{ 3}$ and a is unique (the proof of the first step) . Second step: Prove that $x^3+y^3=1$ has $p-2+a$ solutions in $\mathbb{F}_p$ . For the second step, we know that in $\mathbb{F}_p^*$ , $\ \exists g\ $ s.t. $\mathbb{F}_p^*=\{g,g^2,\dots,g^{p-1}(=1)\}$ . Hence $\{x^3|x\in\mathbb{F}_p^*\}=\{g^3,g^6,\dots,g^{p-1}\}$ is the unique subgroup of index 3 in $\mathbb{F}_p^*$ . But I don't know the relation between the solution of the equation and the decomposition $4p=a^2+27b^2$ . Any ideas?","['algebraic-number-theory', 'number-theory', 'finite-fields', 'primitive-roots', 'diophantine-equations']"
4489966,Recommendations on Intermediate Level Probability/Applied Statistics Book,"So I'm an Internal Medicine Resident with an interest in mathematics and I have a BS in physics and MS in math. Lately I've been getting more into the statistical interpretation of diagnostic test, signs, and symptoms and the application to clinical reasoning. I would like to have a better fundamental understanding of how it affects clinical reasoning. I understand a little of probability through the lens of quantum mechanics, however I haven't really studied higher level statistics as I mostly focused on geometry/topology. So essentially my question is what would be some good intermediate level resources/books on probability theory? I think ideally it would be something with an applied bent as well, and maybe some exposure to decision theory. Thanks!","['statistics', 'decision-theory', 'book-recommendation', 'applications', 'probability-theory']"
4490006,Computing the Galois group of the splitting field of $X^{q+1} + X + T$ over the function field $\mathbb{F}_q(T)$,"Let $k = \mathbb{F}_q$ , $A=k[T]$ and $K=k(T)$ . Let $f(X) := X^{q+1} + X + T \in K[X]$ . Let $L$ be the splitting field of $f$ over $K$ . (In other words, fixing an algebraic closure $K^{\mathrm{alg}}$ of $K$ , let $L$ be the field generated by all the roots of $f$ in $K^{\mathrm{alg}}$ over $K$ ). My question : How to compute the Galois group of $L/K$ ? We explain the motivation for the above question in the remaining part of this post. But it seems that the question itself can be well-presented without the motivation below. Motivation : The problem arises when I'm learning Drinfeld modules. Let $\phi: A \rightarrow \operatorname{End}(\mathbb{G}_a)$ be a Drinfeld module over $K$ of rank 2, defined by $$
\phi_T := T + \tau + \tau^2.
$$ Then the absolute Galois group $\operatorname{Gal}_K$ acts on the $T$ -torsion points $\phi[T]$ of $\phi$ , which induces a Galois representation $$
\pi_T: \operatorname{Gal}_K \rightarrow \operatorname{GL}_2(A/(T)) = \operatorname{GL}_2(\mathbb{F}_q).
$$ In paper here , the authors found that in general $\pi_T$ may not be surjective. My ultimate goal is to check that in this case, $\pi_T$ is indeed surjective. Then by definition of $T$ -torsion point, we have an injection of groups $$
\overline{\pi_T}: \operatorname{Gal}(K(\phi[T])/K) \hookrightarrow \operatorname{GL}_2(\mathbb{F}_q),
$$ and $K(\phi[T])$ is merely the splitting field of $\Phi_T(X)$ over $K$ , where $\Phi_T$ is the ""untwisted version"" of $\phi_T$ : $$
\Phi_T (X) = TX + X^{q} + X^{q^2} = X(T+ X^{q-1} + (X^{q-1})^{q+1}).
$$ So we break the extension $K(\phi[T])/K$ into the splitting field $L$ of $f(Y) := T + Y + Y^{q+1} \in K[Y]$ and an extension adjointing some "" $(q-1)$ -th"" root. So to get my desired surjectivity of $\pi_T$ , by counting $\#(\operatorname{GL}_2(\mathbb{F}_q))$ , I'm hoping that at least we know the order of $L/K$ is $\geq q(q-1)(q+1)$ . But I got stuck here on how to calculating the Galois group, or at least the order of $L/K$ . Something more : It is more illustrating if one comes with a rank two Drinfeld module with nonsurjective $\pi_T$ . I was told to try $\psi_T := T + \tau + T^q \tau^2$ , for which $\pi_T$ has image $\operatorname{SL}_2(\mathbb{F}_q)$ . This is even more chanllenging for me, as merely by counting may not be suffice to settle down the subgroup structure of $\operatorname{GL}_2(\mathbb{F}_q)$ . Sorry for such a long post and thank you all for help! EDIT on July 13 after Jyrki Lahtonen's hint :
It seems that Gauss' lemma really works!  To show $f(X) \in K[X]=k(T)[X]$ is irreducible, suppose otherwise it is reducible in $k(T)[X]$ , then by Gauss' lemma, it is reducible in $k[T][X]=k[T,X]$ . Since $\deg_{T}(f)=1$ , it factors as $$
f(X,T)=f_1(X,T) f_2(X), 
$$ where $f_1(X,T) = T + g_1(X) \in k[X][T]$ and $g_1(X),f_2(X) \in k[X]$ .
But now compare the coefficient of $T$ in $f \in k[X,T]$ , we see that $f_2(X)=1$ , showing that $f$ is irreducible in $k[X][T]$ and hence irreducible in $k(T)[X]$ .","['field-theory', 'galois-theory', 'abstract-algebra', 'algebraic-number-theory']"
4490040,How do I find the rotation matrix between 2 normalized vectors coming from the origin,"In my project, I have two vectors a normal vector $(0,0,1)$ (this is up in the program I'm using), and another normalized vector $(x,y,z)$ . Suppose I rotate $(0,0,1)$ to a new $(a,b,c)$ which is also a normalized vector from the origin, how do I find the position of the new $(x,y,z)$ which rotated along with it. What I think might work is finding the rotation matrix between $(0,0,1)$ and $(a,b,c)$ and applying that rotation matrix to $(x,y,z)$ , but I don't know how to find the rotation matrix between them. Below is an example, the red is $(0,0,1)$ in case 1 and $(a,b,c)$ in the rest of them, and blue is $(x,y,z)$ and $(x', y', z')$ (or what I think $(x',y',z')$ is). Also if it is indeed the rotation matrix between $(0,0,1)$ and $(a,b,c)$ that is being applied to $(x,y,z)$ I think it should be the simplest rotation matrix from $(0,0,1)$ to $(a,b,c)$ , because if I'm not wrong there should be infinitely many rotation matrices between those two. The motion of the rotation also matters, as I want $(0,0,1)$ to move to $(a,b,c)$ along a plane, as demonstrated below. The drawn orange vector on the right is perpendicular to this plane.","['matrices', 'geometry', 'rotations']"
4490083,What's a good rigorous Arithmetic book? Specifically one on which the classic arithmetic algorithms are rigorously proven?,"I’ve been looking for some time for a book that rigorously builds up arithmetic from basic axioms, like the Peano axioms for example, and ends up showing why our classic algorithms for additions, subtraction, multiplication and division, the ones learned in primary school, work. So far the only arithmetic books I’ve been able to find limit themselves to stating the steps of the algorithms (for addition, first align the numbers such that their rightmost digits concur, then add up each digit individually and carry if necessary), or they don’t even mention the algorithms such as this book, which is really nice and shows properties of arithmetic operations such as commutative law, but doesn’t show the correctness of the algorithms typically used to perform such operations. How can we know that the algorithms produce the desired result? I’m looking for a book that answers this question in a rigorous way. Here is an example of what I mean. Thanks in advance!","['elementary-number-theory', 'logic', 'book-recommendation', 'discrete-mathematics']"
4490088,Three hyperbolas related to a triangle concur at two points,"I found this interesting fact (which I believe is non-trivial): Let $\triangle ABC$ be a triangle. Let $\mathcal{H}_A$ be the hyperbola with foci at $B$ and $C$ , passing through $A$ . Define $\mathcal{H}_B$ and $\mathcal{H}_C$ similarly. Then $\mathcal{H}_A,\mathcal{H}_B,\mathcal{H}_C$ meet at exactly two points unless $\triangle ABC$ is equilateral, in which they would meet at one point (which is the orthocenter=incenter=circumcenter in that case). I'm not very well-versed with conics, so I'm not sure how I would go about proving this. I tried using the basic definition of a hyperbola, and you get a system of three distance equations, which are quite tricky to solve. I then supposed that $X$ is some point such that $|XA-XB|=|a-b|, |XB-XC|=|b-c|$ (the intersection of two of the hyperbolas), and then tossing this on the complex plane, setting $X$ to be the origin. This didn't go so well. I don't believe any bashing approaches work, I think this problem requires some sort of projective transformation. Also, it's quite a simple conjecture, so it's likely well known, but I haven't been able to find it on the internet. All help is appreciated.","['conjectures', 'conic-sections', 'geometry']"
4490127,Show the Dimension of a Finite Vector Space is Even,"Let $V$ be a finite-dimensional vector space over a field $\mathbb{F}$ of characteristic $\ne 2$ . Let $X,Y: V \rightarrow V$ be linear transformations such that $X^2=Y^2=\rm Id$ , where $\rm Id$ is the identity transformation. Suppose that $XY=-YX$ . Show that $\dim V$ is even, and that there is a basis of $V$ in which the matrices of $X$ and $Y$ are $$\begin{bmatrix} 0 & I_n \\ I_n & 0\end{bmatrix} \quad \& \quad \begin{bmatrix} I_n & 0 \\ 0 & -I_n\end{bmatrix}$$ respectively, where $I_n$ is the identity matrix and $\dim V=2n$ . Things I know: I've showed that both $X$ and $Y$ are diagonalizable with eigenvalues $\lambda= \pm 1$ . I have also thought the since $X,Y$ are diagonalizable, we may use the Jordan canonical form. But from here, I'm not sure how to proceed. Any help will be appreciated!","['diagonalization', 'linear-algebra', 'vector-spaces', 'linear-transformations']"
4490138,Multivariable Piecewise function with interval defined by the variables,"I am wondering how you analyse and take the partial derivatives of a multivariable piecewise functions where the intervals are defined by the variables Something like f(x,y)= $\left\{ \begin{array}
             (f(x) &   if  & x>h(y) \\
             \\ g(x) &  if & x\leq h(y)\\
             \end{array}
   \right.$ How would you take the partial derivative of f(x,y) in y?",['multivariable-calculus']
4490145,"Does there exist two functions $f, g\in C^1(I)$ for which $W(f, g) (x) >0$ for some $x$ and $W(f, g) (x) <0$ for some $x$?","$f, g\in C^1(I) $ where $I$ is an open interval and $f, g$ both are real valued. Let $W(f,g)(x) =\begin{vmatrix}f(x) &g(x) \\f'(x)&g'(x)\end{vmatrix}$ denote the Wronskian of $f, g$ at $x\in I$ $\textbf{Question}$ Does there exists such $f, g$ for which $W(f, g) (x) >0$ for some $x$ and $W(f, g) (x) <0$ for some $x$ ? $W(f, g) (x) \neq 0$ for some $x\in I$ implies $\{f, g\}$ linearly independent. If two functions are solutions of a differential equation $y""+p(x) y'+q(x) y=0$ on $I$ where $p, q\in C(I) $ then by Abel's identity we have $$W(f, g) (x) =W(f, g) (x_o) e^{-\int_{x_0}^{x} p(t) dt}$$ Then $W(f, g) (x_0) \neq 0$ for some $x_0\in I$ implies $W(f, g) \neq 0$ on $I$ Moreover $W(f,g)$ different from zero with the same sign at every point ${\displaystyle x} \in {\displaystyle I}$ Hence we have to find two functions $f, g$ with the properties: $f, g$ must have to be linearly independent. $f, g\in C^1(I) $ $f, g$ can't be the solution of $2$ nd order homogenous linear ODE. $W(f, g) $ attains both positive and negative values on $I$ . Let $0\in I$ be an open interval and $f, g\in C^1(I) $ defined by $f(x) =x^2$ and $g(x) =x|x| $ Then $f,g$ satisfy $1, 2,3 $ but not $4$ as $W(f, g) (x) =0$ on $I$ .","['ordinary-differential-equations', 'independence', 'applications', 'analysis', 'wronskian']"
4490179,Pure states in the proof of the Gelfand-Naimark theorem,"I'm working through the original proof of the Gelfand-Naimark theorem (every abstract C* algebra is isometrically star-isomorphic to a C* subalgebra of the bounded linear operators on a Hilbert space), and I'm a bit confused on pure states. Subsequent proofs require the linear functionals used (for the GNS constructed representations for the final direct sum) to be pure states, but I'm struggling to find which part of Gelfand and Naimark's proof implies this. So my question is - what in the original proof implies the linear functionals used are pure states? I'm very fresh on pure states so I have probably missed something pretty obvious.","['c-star-algebras', 'banach-algebras', 'functional-analysis']"
4490197,Prove $\pi$-$\lambda$ theorem from Monotone Class Theorem,"I am trying to solve this exercise from Billingsley's Probability and Measure Exercise 3.12 Deduce the $\pi-\lambda$ theorem from the monotone class theorem by showing directly that, if a $\lambda$ -system $\mathscr{L}$ contains a $\pi$ -system $\mathscr{P}$ , then $\mathscr{L}$ also contains the field generated by $\mathscr{P}$ . Here is what I have come up with: The monotone class theorem applies to an (algebra/field). Therefore, we need some field to apply it to and not just a $\pi$ -system. The natural choice would be the field (not $\sigma$ -field) generated by $\mathscr{P}$ . We also need some monotone class. We claim that $\mathscr{L}$ is a monotone class. Using the alternate axiom $(\lambda'_2)$ which says that $A,B \in \mathscr{L}$ and $A\subset B$ imply $B\setminus A = \mathscr{L}$ , we can easily show that $\mathscr{L}$ is closed under monotone unions and intersections. To see this, fix $A_1,A_2,\dots,\in \mathscr{L}$ where $A_n \uparrow A$ . Then define the sets $B_1=A_1$ $B_2=A_2\setminus B_1$ , $B_3=A_3\setminus B_1 \cup B_2$ and so on which are elements of $\mathscr{L}$ by $(\lambda_2')$ . Then $B_1\cup B_2 \cup \dots$ are all disjoint so by $(\lambda_3)$ which says that $A_1,A_2,\dots, \in \mathscr{L}$ and $A_n\cap A_m=\emptyset$ for $m\neq n$ imply $\bigcup_n \in \mathscr{L}$ , we conclude that $\mathscr{L}$ is closed under increasing unions. The proof for decreasing unions is similar. Now to use the monotone class theorem, we finally need to show that the field generated by $\mathscr{P}$ is a subset of $\mathscr{L}$ . This holds by minimality of the field generated by $\mathscr{P}$ because $\mathscr{L}$ is a field. To see this, it is closed under complements as it is a $\lambda$ -system, it contains $\Omega$ as it is a $\lambda$ -system, but I am not sure how to show $A\cup B\in \mathscr{L}$ . What can I do from here to show this fact? Is it even true?","['measure-theory', 'solution-verification', 'probability-theory', 'probability']"
4490199,Probability of $X+Y>Z$ for i.i.d. random variables [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question $X,Y,Z$ are i.i.d. $\operatorname{U}(0,1)$ random variables. What is the probability that $X+Y>Z$ ? Not sure how to solve this problem, does this has anything to do with volume of simplex?","['calculus', 'statistics', 'problem-solving', 'probability']"
4490209,Generalizing Lami's theorem,"In statics, Lami's theorem is an equation relating the magnitudes of three coplanar, concurrent and non-collinear vectors, which keeps an object in static equilibrium, with the angles directly opposite to the corresponding vectors. According to the theorem $$\frac{F_1}{\sin{\alpha}}=\frac{F_2}{\sin{\beta}}=\frac{F_3}{\sin{\gamma}}.\tag{1}$$ where $F_1$ , $F_2$ and $F_3$ are the magnitudes of the three coplanar, concurrent and non-collinear vectors which keep the object in static equilibrium, and $\alpha$ , $\beta$ and $\gamma$ are the angles directly opposite to the vectors (see Figure 1). Lami's theorem is applied in static analysis of mechanical and structural systems. The theorem is named after Bernard Lamy. Its proof is essentially based on the law of sines. On the Internet there are hundreds of static equilibrium problems where they apply Lami's theorem to a three-force system, see for instance Dubey - Engineering Mechanics: Statics and Dynamics, section 3.10 . Although Dubey's book is recent (2013), there is not a single equilibrium problem based on a four-force system. Coincidentally, the author of this note has come across questions on the Internet questioning the possibility of applying Lami's theorem for more than three forces. In this note we give a generalization of Lami's theorem for four forces . Theorem 1 (Generalization) . If four coplanar, concurrent and non-collinear forces act upon an object, and the object remains in static equilibrium, then $$AD\sin{\alpha'}+BC\sin{\gamma'}=AB\sin{\beta'}+CD\sin{\delta'}.\tag{2}$$ where $A$ , $B$ , $C$ and $D$ are the magnitudes of the four vectors and $\alpha'$ , $\beta'$ , $\gamma'$ and $\delta'$ are the angles between them (see Figure 2). Proof . Consider the quadrilateral formed by the four vectors in such a manner that the head of one touches the tail of another (see Figure 3) and denote $\Delta$ its area. If $\alpha$ , $\beta$ , $\gamma$ and $\delta$ are the interior angles of the quadrilateral, then its area can be written as $$\Delta=\frac12AD\sin{\alpha}+\frac12BC\sin{\gamma}=\frac12AB\sin{\beta}+\frac12CD\sin{\delta}\tag{3}$$ and as $\sin{\alpha'}=\sin{(\pi-\alpha)}=\sin{\alpha}$ , and similarly for $\beta'$ , $\gamma'$ and $\delta'$ , the relation in $(2)$ follows. $\square$ Theorem 1 is a generalization in the sense that if one of the vectors vanishes, the relation we obtain is that of Lami's theorem. Indeed, for instance suppose $C=0$ , then the relation $(2)$ reduces to $$D\sin{\alpha'}=B\sin{\beta'},\tag{4}$$ which is Lami's theorem. Remark . A generalization of Lami's theorem is given by H.Shekhar . However, this generalization is different since it only considers cyclic polygons with an odd number of sides. Question: Can Theorem 1 generalize to higher dimensions?","['physics', 'trigonometry', 'geometry', 'reference-request']"
4490217,Show that a continuous function zero for all rationals is zero everywhere. [duplicate],"This question already has answers here : Prove $f(x) = 0$ for all rational numbers implies $f(x)=0$ for all reals. (6 answers) Closed 1 year ago . Let $f: \mathbb{R} \to \mathbb{R}$ be continuous on $\mathbb{R}$ and that $f(r) = 0 \ \forall r \in \mathbb{Q}$ . Prove that $f(x) = 0 \ \forall x \in \mathbb{R}$ . I believe I have a proof of this, but I am hoping to confirm it because I found myself using the argument in other similar problems. I have: $\forall x \in \mathbb{R}, \forall \epsilon > 0, \exists \delta > 0$ s.t. whenever $y \in B_{\delta}(x), f(y) \in B_{\epsilon}(f(x))$ Since $\mathbb{Q}$ is dense in $\mathbb{R}$ , every ball around a rational contains irrationals. So, whenever $|r - y| < \delta, |f(r) - f(y)| = |f(y)| < \epsilon$ So, $f(y) = 0$ and $y$ is arbitrary. Take the union of all of the balls center at the rationals. This is open since the union of open balls is open. Since the rationals are dense, this union is equal to all of $\mathbb{R}$ . So, this holds $\forall y$ . Is this a possible argument? Thanks in advance!","['limits', 'solution-verification', 'continuity', 'real-analysis']"
4490297,Where does the constant term $+1$ go when simplifying this integral in eq (2)?,"I've been looking at this breakdown that deals with taking the gradient of an expectation w.r.t the distribution the expectation is sampling from. On equation 2 is where I get stuck. Where does the $+1$ on the RHS of eq (2) go? I was/am expecting it to be: $\int{ (\log p_\theta(y) - \log q_\phi(x) +1 )  \nabla_{\phi}  q_\phi(x)  dz }$ . I've my answer and should add that both $p(x,z)$ and $q_\phi (z|x)$ are probabilities. By integrating over one (to get 1), then taking the gradient (to get 0), my $+1$ dissapears. $$\begin{align} 
\nabla_{\phi} E_{q_\phi(z|x)} \left [ \log p(x, z) - \log q_\phi(z|x) \right] &= \nabla_{\phi} \int{\log p(x, z)  q_\phi(z|x) dz} -\nabla_{\phi} \int{q_\phi(z|x) \log q_\phi(z|x) dz } \tag{1}\\
&=\int{\log p(x, z) \nabla_{\phi} q_\phi(z|x) dz} - \int{(\log q_\phi(z|x) +1) \nabla_{\phi} 
q_\phi(z|x)  dz } \tag{2}\\
&= \int{ (\log p(x, z) - \log q_\phi(z|x) )  \nabla_{\phi}  q_\phi(z|x)  dz } \tag{3}
\end{align}$$","['integration', 'expected-value', 'derivatives', 'density-function']"
4490366,N-length string loop cut N times: piece length as N $\to \infty$?,"Suppose I take a loop of string of length N units, and mark N points on it at independant and uniformly random (not necessarily integral) points along its length. (So for example suppose I have a 3 inch string:  I might make marks at 0.425 inches, 0.924 inches and 2.4155 inches clockwise from some arbitrary origin point on the loop.) Then lets say I cut the string loop at each of the N points.  I will be left with N pieces of string.  The average length of these pieces of string will be 1 unit (as their total length is N units, and there are N of them: $N/N = 1$ ). As N approaches infinity, what is the distribution of the length of these pieces of string? I think the answer is a probability density function with center of mass at 1, and total integral 1. What is this function?  Does it have a name?","['statistics', 'probability-distributions', 'calculus', 'probability-theory', 'probability']"
4490377,Mixed Hodge Structure on the first cohomology of surfaces.,"I want to know whether any of the following questions are true or not. If they are not true I will appreciate a counter-example: If $X$ is a smooth complex non-simply connected projective surface, with a normal crossing divisor $D$ on $X$ then $H^1(X, \mathbb{Q})\rightarrow H^1(X-D, \mathbb{Q})$ is an isomorphism. The mixed Hodge structure on the first rational cohomology of smooth complex quasi-projective surfaces is always pure.","['hodge-theory', 'algebraic-geometry']"
4490412,Does every closed subset in a scheme correspond to a unique closed subscheme? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Could anyone give an example of a closed subset $Y$ in a scheme $X$ ,
such that there is no closed immersion from a scheme into $X$ whose ""image of underlying points"" is $Y$ ? Also, could anyone give an example $Y$ that admits more than one ""realization"" as a closed subscheme of $X$ ?","['algebraic-geometry', 'schemes']"
4490455,Why is the Fourier transform of $\tan(t)$ not $\sqrt{\frac{\pi}{2}} \csc(\frac{\pi \omega}{2})$,"$\DeclareMathOperator{\sech}{sech}$ $\DeclareMathOperator{\csch}{csch}$ I was curious why Wolfram Alpha does not give a Fourier transform for $\tan(t)$ when it does provide one for $\tanh(t)$ . According to Wolfram Alpha $\mathcal{F}[\tanh(t)] = i\sqrt{\frac{\pi}{2}} \csch(\frac{\pi \omega}{2}) $ . This is all well and good, however using the fact that $\mathcal{F}[g(a t)] = \frac{1}{|a|}\hat{g}(\frac{t}{a})$ , we could easily show that $\mathcal{F}[\tanh(t)] = \sqrt{\frac{\pi}{2}} \csc(\frac{\pi \omega}{2})$ . Note that, $-i \tanh(i t) = \tan(t)$ and $i \csch(i \omega) =\csc(\omega)$ . Therefore, setting $a = i$ the following formula holds. $-i\sqrt{\frac{\pi}{2 |a|}} \csch(\frac{\pi \omega}{2 a}) =-i\sqrt{\frac{\pi}{2|i|}} \csch(\frac{\pi \omega}{2 i}) =-\sqrt{\frac{\pi}{2}}\csc( \frac{-\pi \omega}{2}) = \csc(\frac{\pi \omega}{2}) $ This question implies that this sort of manipulation is allowed. Have I voilated some underlying integrability assumption where this is not a valid operation? Or does Wolfram Alpha have some oversight. As context I was working a derivation of the fact $\sech(\pi t)$ is its own Fourier transform, and the method works for $\tan(t)$ as well. I was a bit surprised that Wolfram Alpha does not have $\tan(t)$ , leading me to question my method.","['trigonometry', 'fourier-analysis', 'hyperbolic-functions']"
4490472,"Does there exists two differentiable functions $f, g$ on $I$ such that $W(f, g) (x) >0$ on $A$ and $W(f, g) <0$ on $I\setminus A$?","Let $I=(0, 1) $ and $A=\mathcal{C}\cap (0, 1) $ where $\mathcal{C}$ denote Cantor set. $\color{red}{Question}$ : Does there exists two differentiable functions $f, g$ on $I$ such that $W(f, g) (x) >0$ on $A$ and $W(f, g) <0$ on $I\setminus A$ ? Where $W(f,g)(x) =\begin{vmatrix}f(x) &g(x) \\f'(x)&g'(x)\end{vmatrix}$ denote the Wronskian of $f, g$ at $x\in I$ Previous Question : Does there exists two functions $f, g\in C^1(I)$ for which $W(f, g) (x) >0$ for some $x$ and $W(f, g) (x) <0$ for some $x$? Let me summarize $W(f, g) (x) \neq 0$ for some $x\in I$ implies $\{f, g\}$ linearly independent. If two functions are solutions of a differential equation $y""+p(x) y'+q(x) y=0$ on $I$ where $p, q\in C(I) $ then by Abel's identity we have $$W(f, g) (x) =W(f, g) (x_o) e^{-\int_{x_0}^{x} p(t) dt}$$ Then $W(f, g) (x_0) \neq 0$ for some $x_0\in I$ implies $W(f, g) \neq 0$ on $I$ Moreover $W(f,g)$ different from zero with the same sign at every point ${\displaystyle x} \in {\displaystyle I}$ If $f, g \in C^1(I) $ then $w(x) =W(f, g) (x) =f(x) g'(x) -f'(x) g(x) $ is a continuous map on $I$ . Then $S=\{x\in I : W(f, g) (x) >0\}$ is an open set. Further if $W(f, g) $ attains both positive and negative values then by Darboux property $W(f, g) (x) =0$ for some $x\in I$ . Hence if $f, g$ are solution of the same differential equation and $f, g$ has continuous derivative. Then $ W(f, g) (x) =0$ on $I$ . Hence to get two functions $f, g$ with the mentioned properties, we need to consider the following : $f, g$ can't be the solution of  a Linear Homogenous Differential Equation. $f, g$ can't be continuously differentiable on $I$ . $f, g$ can't be linearly dependent on $I$ . Does there exists two differentiable functions $f, g$ on $I$ such that $W(f, g) (x) >0$ on $A$ and $W(f, g) <0$ on $I\setminus A$ ?","['determinant', 'ordinary-differential-equations', 'applications', 'analysis', 'wronskian']"
4490542,What is the probability distribution for manhattan distance on a finite grid?,"Suppose I have a finite grid of size NxN. (Example is in two dimensions). I want to know whether there exists a closed-form formula (or approximation) for the probability distribution of the Manhattan distance between two randomly selected points, using the manhattan metric $$||a,b||_M = |x_a - x_b| + |y_a - y_b|$$ . So given two random points $a$ and $b$ , what is $P(||a,b||_M = d)$ for any value $0 \leq d < 2N$ ? By using a PRNG, and randomly trying out a billion points or so, the distribution looks like a parabola with a long tail. This is with $N = 256$ : It's possible to manually enumerate the points... (given that $N > 3$ ). $$d=0: \frac{N^2}{N^4} = \frac{1}{N^2}$$ $$d=2N-1: \frac{2}{N^4}$$ $$d=2N-2: \frac{8}{N^4}$$ $$d=2N-3: \frac{12 + 8}{N^4}$$ For $d=2N-1$ The points are in opposite corners. For $d=2N-2$ observe that one point has to be in a corner, and the other point then has two possible locations. For $d=2N-3$ there's a case with two pairs of points in opposite corners (four options, two symmetries), and one similar to previous case but with three options (three options, four symmetries). This quickly balloons in possibilities though. I wonder if there's a better way of approaching this.","['geometry', 'probability']"
4490594,Do all holomorphic functions on $\{z^2 + w^2 = 1\}$ extend to $\mathbb{C}^2$?,"Let $$X = \{(z, w) \in \mathbb{C}^2 : z^2 + w^2 = 1\}.$$ Is there a holomorphic function $f : X \to \mathbb{C}$ which do not extend to a holomorphic function on all of $\mathbb{C}^2$ ? I tried to find an example of the form $f(z, w) = \frac{1}{g(z, w)}$ , where $g(z, w)$ is a holomorphic function which has zeros, but none that lie in $X$ . But I couldn't find any.","['complex-analysis', 'complex-geometry', 'algebraic-geometry', 'several-complex-variables']"
4490596,How do I compute the density of the random variable $\frac{U}{\sqrt R}$?,"Let $R$ be a positive random variable with density $$g(r)=\frac{1}{\sqrt{ \pi r }}e^{-r}~~~~,~~~~r>0$$ Let $U$ be uniformly distributed on $[0,1]$ and independent of $R$ . Let me define $X=\frac{U}{\sqrt R}$ . We need to compute the density of $X$ . My idea was the following. Let $f$ be a measurable bounded function. Then consider $$\Bbb{E}(f(X))=\Bbb{E}\left(f\left(\frac{U}{\sqrt R}\right)\right)=\int_{\Bbb{R}^2}f\left(\frac{u}{\sqrt r}\right)P_{(U,R)}(du~dr)\stackrel{independent}{=}\int_0^\infty \int_0^1 f\left(\frac{u}{\sqrt r}\right)g(r)~~du~dr$$ Now let me substitute $x=\frac{u}{\sqrt r}$ Then I get $$\Bbb{E}(f(X))=\frac{1}{\sqrt \pi}\int_0^\infty \int_0^{\frac{1}{\sqrt r}} f(x)e^{-r}~~dx~dr=\frac{1}{\sqrt \pi}\int_0^\infty \int_0^{\frac{1}{x^2}} f(x)e^{-r}~~dr~dx=\int_0^\infty f(x)h(x)~dx$$ where $h(x)=\int_0^{\frac{1}{x^2}} \frac{1}{\sqrt \pi}e^{-r}~~dr$ for $x>0$ and $h(x)=0$ else.  Then I can explixitly compute $h(x)$ and get $$h(x)=\left(\frac{1}{\sqrt \pi}-\frac{e^{-\frac{1}{x^2}}}{\sqrt \pi}\right)\Bbb{1}_{\{x>0\}}(x)$$ Then this $h$ is our density.
Now I wanted to ask if this is correct like this or if I did something wrong. Thanks for your help.","['integration', 'solution-verification', 'probability-theory', 'probability', 'density-function']"
4490619,Is there an intuitive reason why $\int_{-\infty}^{\infty} x dx$ doesn't exist but $\lim\limits_{N\to \infty} \int_{-N}^N xdx$ exists?,"Is there some intuitive reason why $\int_{-\infty}^{\infty} x dx$ doesn't exist but $\lim\limits_{N\to \infty} \int_{-N}^N xdx$ does? It would seem they represent the same thing (area beneath $f$ everywhere), but apparently not. Consider the following problem from Ch. 14 ""The Fundamental Theorem of Calculus"" from Spivak's Calculus 27 (b) The improper integral $\int_{-\infty}^a f$ is defined in the obvious
way, as $\lim\limits_{N\to -\infty} \int_N^a f$ . But another kind of
improper integral $\int_{-\infty}^{\infty} f$ is defined in a
nonobvious way: it is $\int_0^{\infty} f + \int_{-\infty}^0 f$ ,
provided these improper integrals both exist. (b) Explain why $\int_{-\infty}^{\infty} x dx$ does not exist. (But
notice that $\lim\limits_{N\to \infty} \int_{-N}^N xdx$ does exist) My calculations are as follows $$\int_{-N}^N xdx = \left . \frac{x^2}{2} \right |_{-N}^N=\frac{N^2-(-N)^2}{2}=0$$ On the other hand $$\int_{-\infty}^{\infty} xdx=\int_{-\infty}^0xdx+\int_0^{\infty}xdx$$ $$=\lim\limits_{N \to -\infty} \int_{N}^0 xdx+\lim\limits_{N\to\infty}\int_0^{N} xdx$$ $$=\lim\limits_{N\to -\infty} \left . \frac{x^2}{2}\right |_N^0+\lim\limits_{N\to\infty} \left . \frac{x^2}{2} \right |_0^N$$ $$\lim\limits_{N\to -\infty} \frac{-N^2}{2}+\lim\limits_{N\to \infty} \frac{N^2}{2}$$ Neither of these limits exists. This seems like a weird result. Furthermore, if we had defined $\int_{-\infty}^a f$ as $\lim\limits_{N\to \infty} \int_{-N}^a f$ then in the calculation above we would have $$\int_{-\infty}^{\infty} xdx=\int_{-\infty}^0xdx+\int_0^{\infty}xdx$$ $$=\lim\limits_{N \to \infty} \int_{-N}^0 xdx+\lim\limits_{N\to\infty}\int_0^{N} xdx$$ $$=\lim\limits_{N \to \infty} \left ( -\frac{(-N)^2}{2}+\frac{N^2}{2} \right )$$ $$=\lim\limits_{N \to \infty} 0$$ $$=0$$","['integration', 'limits', 'calculus', 'derivatives']"
4490648,Do Wronskians have the intermediate value property?,"I wonder if the following is true: Conjecture: Let $I \subset \Bbb R$ be an open interval and $f, g: I \to \Bbb R$ be differentiable functions. Then the Wronskian $$
W(f,g) =\begin{vmatrix}f &g \\f' & g'\end{vmatrix} = f g' - f'g
$$ is a Darboux function. A Darboux function is a real-valued function $f$ which has the “intermediate value property”: for any two values $a$ and $b$ in the domain of $f$ , and any $y$ between $f(a)$ and $f(b)$ , there is some $c$ between $a$ and $b$ with $f(c) = y$ . Motivation and thoughts: In Does there exists two differentiable functions $f, g$ on $I$ such that $W(f, g) (x) >0$ on $A$ and $W(f, g) <0$ on $I\setminus A$? it what proved that If the Wronskian takes both positive and negative values on an interval, then it must be zero somewhere. and my conjecture would be a natural generalization. However, I do not yet see how the case of an arbitrary intermediate value $y$ can be reduced to the special case of $y = 0$ as the intermediate value. The conjecture is (trivially) true if both $f$ and $g$ are continuously differentiable, since then $W(f, g)$ is continuous. So the interesting case is that $f$ and $g$ are just assumed to be differentiable. Derivatives have the Darboux property, that covers the case that $f$ or $g$ is constant, e.g. $W(1, g) = g'$ . Sums and products of Darboux functions are not necessarily Darboux functions (see for example The sum of Darboux is a Darboux function? ). So even if all terms in $f g' - f'g$ have the intermediate value property, there is no immediate way to conclude the conjecture. A (failed) proof attempt: Assume that $w = W(f, g)$ does not take a value $y \in \Bbb R$ , and consider the sets $$
A = \{ x \in I \mid w(x) > y \} \, , \, B = \{ x \in I \mid w(x) < y \} \, .
$$ If we can show that both $A$ and $B$ are open then one of them must be empty (since $I$ is connected), and we are done. If $f(x_0) \ne 0$ then we can define $h(x) = y \int_{x_0}^x f(t)^{-2} dt$ in a neighborhood of $x_0$ , and $$
 W(f, g) -y = f^2 \left( \frac gf - h\right)'
$$ shows that $W(f, g) -y$ does not change its sign near $x_0$ , so that $x_0$ is an interior point of $A$ or of $B$ . A similar argument works if $g(x_0) \ne 0$ . However, other than in my previous answer , one can not exclude the case $f(x_0) = g(x_0) = 0$ . That is where my I am stuck in my current proof attempt.","['determinant', 'wronskian', 'real-analysis']"
4490649,How to verify this group presentation is contradictory,"I'm studying group characters and representation on Hill's book, for fun self study. One of the exercises ask to verify the following presentation contains contradictory relations: $$\langle x, y : x^3 = y^3 = 1, yx = x^2y \rangle$$ The first concern I have is the book is claiming ""a group of order 9 would presumably result"", and it's not fully clear how it can estimate that. The second is how to find a solution. I mean, the book also suggests that $x^3=1$ implies $x^2\neq1$ and $x\neq1$ , so I thinking what really means to establish a contradiction. If I was able to confirm the expected order is 9, I could try to build some table and verify the generated elements would be more than that value, but it would be eventually too mechanic and not sure if feasible neither. For example I could update the last relation twofold, using the first two ones like: $$yxy^2=x^2\qquad\text{or}\qquad xyx=y$$ The last one puzzles me a bit from a ""logical point of view"", since I cannot ""see"" how $y$ could stay invariant when wrapped around two $x_s$ . Have you any hints about how to check that? I mean, an I forced to enumerate some possible group elements and search a contradiction among them, or could I found a more elegant solution finding a contradiction playing with relations only? Thanks in advance","['combinatorial-group-theory', 'group-presentation', 'group-theory']"
4490685,How to find points on a curve whose tangent line pass through a specified point?,"Now we have a compact closed simple smooth curve in $\Bbb R^2$ and the curve's bounding box is known. But the curve is given implicitly by a signed distance function $f(x, y)$ . Note that we don't have a global formula for $f(x, y)$ , but we can sample $f(x, y)$ at arbitrary points. definition of signed distance function can be found here The question is: Given a point $P$ , how can we find all the points on the curve whose tangent line pass through $P$ . Can you give me an algorithm for this process? Here is an illustration. Points on the red curve whose tangent line pass through $P$ are $A, B, C, D$ .","['differential-geometry', 'plane-curves', 'partial-differential-equations', 'numerical-methods', 'computer-science']"
4490688,Find summation of the series (PDE approach),"The summation is given as: $$
\sum _{j=1}^{\infty } \frac{a^j e^{-\frac{b}{j}}}{j!}
$$ where $a>1$ and $b>1$ This question actually arises when you try to find the sum over the product of two functions: a poisson distribution $$
\frac{a^j} {j!} e^{-a}
$$ a j-times convolved gaussian function $$
\underbrace{e^{-px^2}\circledast e^{-px^2}\circledast\cdots\circledast e^{-px^2} }_{\text{j times}}= e^{-\frac{p}{j}x^2}=e^{-\frac{b}{j}}
$$ Can it be solved as follows: $$
y=\sum _{j=1}^{\infty } \frac{a^j e^{-\frac{b}{j}}}{j!}
$$ $$
\frac{\partial y}{\partial b}=\sum _{j=1}^{\infty } \frac{a^j e^{-\frac{b}{j}}\cdot\frac{-1}{j}}{j!}
$$ $$
\frac{\partial}{\partial a}(\frac{\partial y}{\partial b})=\sum _{j=1}^{\infty } \frac{j\cdot a^{j-1} e^{-\frac{b}{j}}\cdot\frac{-1}{j}}{j!}
$$ $$
\frac{\partial}{\partial a}(\frac{\partial y}{\partial b})=-\sum _{j=1}^{\infty } \frac{a^{j-1} e^{-\frac{b}{j}}}{j!}
$$ $$
\frac{\partial}{\partial a}(\frac{\partial y}{\partial b})=-\sum _{j=1}^{\infty } \frac{a^{j-1} e^{-\frac{b}{j}}}{j!} \cdot \frac{a}{a}
$$ $$
\frac{\partial}{\partial a}(\frac{\partial y}{\partial b})=-\frac{1}{a}\sum _{j=1}^{\infty } \frac{a^{j} e^{-\frac{b}{j}}}{j!}
$$ $$
\frac{\partial}{\partial a}(\frac{\partial y}{\partial b})=\frac{-y}{a}
$$ How should I solve this PDE?","['summation', 'partial-differential-equations', 'exponential-function', 'sequences-and-series']"
4490714,"Show that $\beta(f,g)=\int_0^1 f(x)g'(x)dx$ is nondegenerate.","We have the bilinear form $\beta:V \times V \to \mathbb{R}, (f,g) \mapsto \int_0^1 f(x)g'(x)dx$ with $V=\{f\in C^1[0,1] \mid f(0)=f(1)=0 \}$ . Show that $\beta$ is nondegenerate. My attempt: For $0\neq f \in V$ we have $\beta(f,F)=\int_0^1 f(x)^2dx > 0$ where $F(x)=\int_0^x f(t)dt$ . But $F(1)$ is not always $0$ . So i tried something like $G(x)=F(x)-F(1)x$ or $G(x)=F(x)(1-x)$ but i am not able to show $\beta(f,G)\neq 0$ .","['integration', 'bilinear-form', 'derivatives', 'analysis']"
4490715,"Solving the pde $f_x(x,y)+f_y(x,y)=0$ given $f(x,0)=\sin x$","Suppose, $f:\mathbb{R}^2\to\mathbb{R}$ is a differentiable function such that, $f_x(x,y)+f_y(x,y)=0$ for all $(x,y)\in \mathbb{R}^2.$ If $f(x,0)=\sin x$ then, find $f(0,y).$ A quick look reveals that $f(x,y)=\sin(x-y) $ is a solution, so $f(0,y)=-\sin y,$ but I got this only by inspection. I am not sure whether this is the only solution to the above pde, I am new to this area, so please help me.","['ordinary-differential-equations', 'partial-differential-equations']"
4490722,Dimensions in group theory,"Remark . My previous post about it has been linked to a post that apparently solves my problem -- yet, said post does not satisfy me. I haven't learned the techniques used in my class. As a physics student, I am currently learning about group theory. I have trouble understanding how to find the dimension of a (sub)group. Let us consider the orthogonal group $O(n)$ . Before talking about its dimension, one needs to show that $O(n)$ is indeed a group. We assume throughout that $O(n)\subset GL(n,\mathbb{R})$ . A matrix is said to be orthogonal whenever it preserves the Euclidian norm - that is if \begin{equation}
x^Ty = \left(Mx\right)^T\left(My\right) = x^TM^TMx
\end{equation} This equation holds true if and only if $M^TM = Id$ . Closure . Let $A,B\in O(n)$ . Let $C = AB$ . \begin{equation}
C^TC = (AB)^T(AB) = B^TA^TAB = B^TB = Id.
\end{equation} Identity . One easily sees that $Id^T = Id$ , such that $Id^T Id = Id$ . Inverse . Because $O(n)\subset GL(n,\mathbb{R})$ , every $F\in O(n)\subset GL(n,\mathbb{R})$ , is non-singular. Therefore $F^{-1}$ exists. Let us show that $F^{-1}\in O(n)$ . \begin{equation}
(F^{-1})^TF^{-1} = (F^T)^{-1}F^{-1} = (F^TF)^{-1} = Id.
\end{equation} Associativity . Matrix multiplication being associative, any two matrices that happen to be a members of $O(n)$ will still be associative. We now know for sure that $O(n)$ is a group. In fact, we even know that it is a subgroup of $GL(n,\mathbb{R})$ . Now, how does one work out the dimension of that group? Here is how I tried to solve that problem. Let $K\in O(n)$ . For $n=2$ , I find 3 conditions on the matrix elements of $K$ . For $n=3$ , I find 6 such conditions. We know that for a general matrix in $GL(n,\mathbb{R})$ , the dimension is $n^2$ . Hence, the dimension of $O(2)$ should be 2^2-3 = 1. Equivalently, for $O(3)$ one finds that the dimensions should be $9-6=3$ . How does one find out from this that the dimension of $O(n)$ follows $\frac{n(n-1)}{2}$ ?","['matrices', 'group-theory', 'abstract-algebra']"
4490750,Do all holomorphic functions on a smooth affine variety $X \subset \mathbb{C}^n$ extend to $\mathbb{C}^n$?,"Let $X$ be a non-singular complex affine variety in $\mathbb{C}^n$ and let $$f : X \to \mathbb{C}$$ be a holomorphic function.
Does $f$ extend to a holomorphic function $f : \mathbb{C}^n \to \mathbb{C}$ ? This question is inspired by this one, where the answer is shown to be yes in the special case where $$ X = \{(z, w) \in \mathbb{C}^2 : z^2 + w^2 = 1\}.$$","['complex-analysis', 'complex-geometry', 'algebraic-geometry', 'several-complex-variables']"
4490761,Let $H$ be a maximal subgroup of a finite group $G$ such that $|G:H|=4$. Then there exists $K\leq H$ such that $|H:K|=3$,"Let $H$ be a maximal subgroup of a finite group $G$ such that $|G:H|=4$ . Then there exists $K\leq H$ such that $|H:K|=3$ . My attempt: Since maximal subgroups of nilpotent groups have prime index, so $G$ is not nilpotent. (In particular, $G$ is neither Abelian nor a $p$ -group.) Also, as $H\leq N(H)\leq G$ and $H$ is maximal, so $N(H)=G$ or $N(H)=H$ . I also know that $G/\bigcap_{g\in G} H^g$ is isomorphic to a subgroup of $S_4$ . I haven no idea how to proceed and am really lost. Any hints are appreciated.","['maximal-subgroup', 'group-theory', 'abstract-algebra', 'finite-groups']"
4490765,Overestimates and underestimates in Inclusion-Exclusion principle,"In the celebrated Inclusion-Exclusion principle, $$|\cup A_i|=\sum _i |A_i|-\sum _{i<j}  |A_i \cap A_j|+\sum _{i<j<k}|A_i \cap A_j \cap A_k|-\dots +(-1)^{n+1}|\cap A_i|$$ if we take only $m \leq n$ items of the right side, then we would get an overestimate if $m$ is odd, and we obtain an underestimate if $m$ is even, for instance, $$|\cup A_i|\leq \sum _i |A_i|$$ and $$|\cup A_i|\geq \sum _i |A_i|-\sum _{i<j}  |A_i \cap A_j|.$$ I want an ""elementary"" proof for this fact which does not use probability or any measure theory. I tried induction, for the second inequality but could not get anything. I would be thankful for the answer or any leading comment in this connection.
Thanks in advance!","['inclusion-exclusion', 'combinatorics']"
4490778,A particular function of $L^1$,"Let $\mu$ be a sigma finite positive measure on $(X,\mathcal{A})$ . then exists $w\in L^1(\mu)$ such that $0< w(x) < 1$ for all $x\in X$ . Since $\mu$ is a sigma finite measure we have that $$X=\bigcup_{n=1}^\infty E_n\quad \mu(E_n)<\infty.$$ We define $$w_n(x)=\frac{1}{2^n(1+\mu(E_n))}\quad\text{if}\;x\in E_n$$ zero otherwise. Define $$w(x):=\sum_{n=1}^\infty w_n(x).$$ I can't find a way to show that $$\int_X w\;d\mu <\infty$$ could someone give me a suggestion? Why $0<w<1$ ?","['measure-theory', 'functional-analysis']"
4490782,"Why is $(3, 1+\sqrt{-26})^3=( 1+\sqrt{-26})$ in $\mathbb Z[\sqrt{-26}]$?","$a = 3, b = 1+\sqrt{-26}$ then $(a,b)^3=(a^3,b^3,a^2b,ab^2)$ each generator except $a^3$ has a $b$ factor and $\bar b b=27$ , so $""\subseteq""$ . Now the question is how to obtain $b$ using these generators. Is there a general formula, which states in which case this is possible or even is  there an algorithm. I made it very complicated I think; $a^3 = 27,\quad$ $b^3 = -77+23\sqrt{-26},\quad$ $a^2b = 9+9\sqrt{-26},\quad$ $ab^2 = -75+6\sqrt{-26}$ $3*a^3+ab^2=81+(-75+6\sqrt{-26})=6+6\sqrt{-26}$ $a^2b-ans = 3+3\sqrt{-26}$ are these correct so far ?","['number-theory', 'abstract-algebra', 'ideals']"
4490785,PDE in S.-T. Yau College Student Mathematics Contests 2019 [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question This problem is from S.-T. Yau College Student Mathematics Contests 2019.
I don't have enough pre-knowledge, but I want to learn. Let $\Omega \subset \mathbb{R}^2$ be a bounded domain with smooth
boundary. Prove that, for all $p>1$ and $1\leq q<\infty$ , for all $f\in L^p(\Omega)$ , there exists a unique $u\in H^1_0(\Omega)$ , such
that $$\Delta u = |u|^{q-1} u+f  \  \text{in} \ \Omega.$$","['functional-analysis', 'partial-differential-equations']"
4490822,Find subset of integral satisfying $n$ equality constraints,"Let $f_1, \dots f_n \in \mathcal{C}^0(I,\mathbb{R}_{>0})$ be $n$ continuous and strictly positive functions from $I$ to $\mathbb{R}_{>0}$ with $I$ a closed interval of $\mathbb{R}$ . Let $g\in \mathcal{C}^0(I,\mathbb{R}_{>1})$ be a continuous function from $I$ to $\mathbb{R}_{>1}$ (for all $t\in I$ , $g(t)>1$ ). Does there always exists a common subset (not necessarily an interval) $J\subseteq I$ such that: $$
\forall k,\, \int_J f_k(t)g(t)dt = \int_I f_k(t)dt
$$ Intuitively it seems to hold, but I have a hard time figuring out a proof. Is this a known result ?","['integration', 'measure-theory', 'lebesgue-measure', 'real-analysis', 'measurable-functions']"
4490849,Convex subring is local,"Let $K$ be an ordered field, $R\subseteq K$ a convex subring - that is, for all $a \in K$ , $a \in R$ if $x \leq a \leq y$ for some $x,y \in R$ . Define $I = \{ x \in R: x^{-1} \notin R\}$ . It is clear that if $I$ is an ideal, then $R$ is local. However, I am having some trouble to understand why it is a convex ideal of $R$ . I know that, if $a>1$ and $a \in R$ , then $0 < a^{-1}< 1$ , which implies that $a \notin I$ . I am also unsure of the hypothesis one should assume about $K$ (for example, $K$ may be real closed - or even a model of the theory of the reals as an ordered field). Can someone help me? Edit: For $a \in R$ , assume that $|a|\geq 1/n$ for some $n \in \mathbb{N}$ . If $a>0$ , then $0<a^{-1}\leq n$ which implies $a \notin I$ . If $a<0$ , $-n \leq a^{-1} < 0$ - from which $a^{-1} \notin I$ . Thus, $I \subseteq \{a : |a|<1/n \mbox{ for all } n \}$ .","['abstract-algebra', 'commutative-algebra', 'ordered-fields']"
4490856,Using Laplace transform solve the following differential equation : $ y'+y = 2 + \delta(t-4)$,"Using Laplace transform solve the following differential equation : $$ y'+y = 2 + \delta(t-4)$$ , $$y(0)=0$$ Express the solution $y(t)$ as a piecewise function about $t=4$ and tell us what happens to the graph of it at $t=4$ . My try : Taking Laplace transform on the both sides : $$ \big( s \cdot Y(s) - y(0) \big) + Y(s) = \frac{2}{s} + e^{-4s} \\
 \implies Y(s) = \frac{2}{s(s+1)} + \frac{e^{-4s}}{(s+1)} \\
\implies Y(s) = 2\Big(\frac{1}{s}-\frac{1}{s+1}\Big) + \frac{e^{-4s}}{(s+1)}
$$ Now I'm stuck here because I dont know how to take the inverse Laplace transform of $\frac{e^{-4s}}{(s+1)}$ . I haven't ever dealt with something involving exponential function. Can I get some help here please?","['laplace-transform', 'ordinary-differential-equations']"
4490912,Is the Fisher-Information even continuous in a regular statistical model?,"Definition (Regular Model [1, p. 203]). A standard statistical model $\big( X, \mathcal F, (\mathbb P_{\vartheta})_{\vartheta \in \Theta}\big)$ , where $\Theta \subset \mathbb R$ is an open interval, $X \ne \emptyset$ , $\mathcal F$ is a $\sigma$ -algebra on $ X$ and $\mathbb P_{\vartheta}$ is a probability measure on $( X, \mathcal F)$ for every $\vartheta \in \Theta$ such that there exists a measure $\mu_0$ with $\mathbb P_{\vartheta} \ll \mu_0$ ( absolute continuity ) for all $\vartheta \in \Theta$ is regular , if the likelihood function $$\rho \colon X \times \Theta \to \mathbb{R}, \qquad (x, \vartheta) \mapsto \frac{\text{d} \mathbb P_{\vartheta}}{\text{d} \mu_0}(x)$$ ( Radon-Nikodym derivative ) is positive and continuously differentiable (almost everyhwere) with respect to $\vartheta$ . Then the score $$
U_{\vartheta}(x)
:= \frac{\partial}{\partial \vartheta} \ln\big(\rho(x, \vartheta)\big)
= \frac{\frac{\partial}{\partial \vartheta} \rho(x, \vartheta)}{ \rho(x, \vartheta)}
$$ is well defined. For a regular model, we also require that the Fisher information of the model $I(\vartheta) := \mathbb{V}_{\vartheta}[U_{\vartheta}]$ , where $V_{\vartheta}$ is the variance with respect to $\mathbb P_{\vartheta}$ , is always an element of $(0, \infty)$ and that \begin{equation}
\int_{ X} \frac{\partial}{\partial \vartheta} \rho(x, \vartheta) \, \text{d}{\mu_0(x)}
\overset{!}{=} \frac{\partial}{\partial \vartheta} \underbrace{\int_{ X} \rho(x, \vartheta) \, \text{d}{\mu_0(x)}}_{= 1}
= 0
\end{equation} holds, where we require that the left side is well defined, that is, that $\frac{\partial}{\partial \vartheta} \rho(\cdot, \vartheta)$ is integrable with respect to $\mu_0$ . It is not difficult to show that $\mathbb{E}_{\vartheta}[U_{\vartheta}^2] = 0$ and thus that $I$ is lower continuous due to Fatou's Lemma ). My question: Is $I$ even continuous? Remark. The proof of the Cramér-Rao inequality one shows that if that the inequality is an equality, then $I$ is continuous, but the requirements of that theorem are, among others, that the estimator is regular and that the expected value of the estimator with respect to $\mathbb P_{\vartheta}$ is continuously differentiable with respect to $\vartheta$ with pointwise non-vanishing derivative. 1 Georgii, Hans-Otto , Stochastics. Introduction to probability and statistics. Translated by Marcel Ortgiese, Ellen Baake and the author. , de Gruyter Textbook. Berlin: de Gruyter (ISBN 978-3-11-019145-5/pbk; 978-3-11-020676-0/ebook). ix, 370 p. (2008). ZBL1270.62005 .","['statistics', 'variance', 'parameter-estimation', 'continuity', 'fisher-information']"
4490983,Why $\Bbb{R}$ and $\Bbb{R}^2$ are not homeomorphic.,"Can I argue that if there exists a homeomorphism $$f: \Bbb{R} \rightarrow \Bbb{R}^2$$ Then subtracting a point should preserve connectedness by continuity of $f$ , but then $\Bbb{R}$ minus the origin is disconnected while $\Bbb{R}^2$ minus the origin Is still connected. Is this a good enough argument? As connectedness is a topological property. Which I can prove.",['general-topology']
4490993,Confusions when differentiating a Fourier Transform and using the Kramers-Kronig relations,"Let $x(t)$ be a real-valued squared-integrable signal that have a beginning and an ending time (so, is of finite duration). Then I will have that its Fourier Transform: $$X(w) = U(w)+iV(w)$$ with $\{U(w),\ V(w)\}\in\mathbb{R}$ is such that: $X(w)$ is analytic $U(-w) = U(w)$ $V(-w) = -V(w)$ Kramers–Kronig relations : $U(w) = \displaystyle{\frac{\pi}{2}\,\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}d\xi}$ and $V(w) = \displaystyle{-\frac{\pi}{2}\,\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{U(\xi)}{\xi-w}d\xi}$ Now, since differentiating the Fourier Transform goes at follow: $\displaystyle{\frac{\partial X(w)}{\partial w}=iw X(w)} \equiv iwU(w)-wV(w)\tag{Prop. 1}$ $$\Rightarrow \frac{\partial X(w)}{\partial w}\; \overset{\text{linearity of}\,\frac{\partial}{\partial w}}{=}\; \frac{\partial U(w)}{\partial w}+i\frac{\partial V(w)}{\partial w} \overset{\text{Prop. 1}}{\equiv} iwU(w)-wV(w)$$ so by pairing the real and imaginary parts: $\frac{\partial U(w)}{\partial w} = -wV(w) \tag{Eq. 1}$ $\frac{\partial V(w)}{\partial w} = wU(w) \tag{Eq. 2}$ Then, for example for the first one, I will have: $$V(w) = -\frac{1}{w}\frac{\partial U(w)}{\partial w} = -\frac{1}{w}\frac{\partial}{\partial w}\left(\frac{\pi}{2}\,\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}d\xi\right) \tag{Eq. 3}$$ but I don't know how to differentiate $\frac{\partial}{\partial w}\left(\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}d\xi\right)$ : if I use Wolfram-Alpha it says that: $$\frac{\partial}{\partial w}\left(\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}d\xi\right)= \int\limits_{-\infty}^{\infty} \frac{V(\xi)}{(\xi-w)^2}d\xi$$ bit I am not sure if its right because the Principal Value implies is a complex integral. How to differentiate $\frac{\partial}{\partial w}\left(\frac{\pi}{2}\,\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}d\xi\right)$ ? Does $\text{Eq. 3}$ set an equation to find a restricted form for $V(w)$ as a function of $w$ ? or it just will end in something like $V(w)=V(w)$ ? It is possible to solve this equation for $V(w)$ ? Does the equations $\text{Eq. 1}$ and $\text{Eq. 2}$ behave as similar restrictions as the Cauchy–Riemann equations ? As example, taking the second derivative of $X(w)$ : $$\frac{\partial^2 X(w)}{\partial w^2} = (iw)^2X(w) = -w^2X(w) = -w^2U(w)-iw^2V(w) \tag{Eq. 4}$$ But if I use $\text{Eq. 1}$ and $\text{Eq. 2}$ , by differentiating them I will have: $$
\frac{\partial^2 U(w)}{\partial w^2} = \frac{\partial}{\partial w}\left(\frac{\partial U(w)}{\partial w}\right) \overset{\text{Eq. 1}}{=}  \frac{\partial}{\partial w}\left(-wV(w)\right) =-V(w)-w\frac{\partial V(w)}{\partial w}  \overset{\text{Eq. 2}}{=} -w^2U(w)-V(w) \tag{Eq. 5} $$ $$\frac{\partial^2 V(w)}{\partial w^2} = \frac{\partial}{\partial w}\left(\frac{\partial V(w)}{\partial w}\right) \overset{\text{Eq. 2}}{=} \frac{\partial}{\partial w}\left(\; wU(w)\right) = \; U(w)+w \frac{\partial U(w)}{\partial w} \overset{\text{Eq. 1}}{=} -w^2V(w) +U(w) \tag{Eq. 6}$$ Now since, $$\begin{array}{r c l}
\displaystyle{\frac{\partial^2 X(w)}{\partial w^2}} & = & \displaystyle{\frac{\partial^2 U(w)}{\partial w^2}+i\frac{\partial^2 V(w)}{\partial w^2}} \\
& \overset{\text{Eq. 5 & Eq. 6}}{=} & -w^2U(w)-V(w)+i\left(-w^2V(w) +U(w)\right) \\
& = & -w^2U(w)-iw^2V(w)+i\left(U(w) +iV(w)\right) \\
& = & -w^2X(w)+iX(w) = (i-w^2)X(w) \\
& \neq & \text{Eq. 4}
\end{array}$$ Since I found a contradiction, please explain where and why I am making the mistake. Added later___________________ I found in this answer that: $$\frac{\partial}{\partial w}\left(\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}d\xi\right)= \text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{V(\xi)-V(w)}{(\xi-w)^2}d\xi$$ so it somehow solves point $(1)$ , but I still don't know if and how to use it to make a differential equation for $V(w)$ , so it still missing an answer  for point $(2)$ . About the mentioned differential equation, I believe that the matching of $\text{Eq. 1}$ and $\text{Eq. 2}$ is an illegal operation, which leads to the contradiction, but I still don't know why and I would like to know were I am making a conceptual mistake... my intuition tells me is related with something about non-uniqueness of the complex derivative when Cauchy-Riemann Equations aren't hold: I believe this is the case of the Fourier Transform where only the imaginary axis is considered. As example, following $\text{Eq. 1}$ and $\text{Eq. 2}$ I can make the following differential equations: $$\begin{array}{l}
V''-\frac{V'}{w}+w^2V=0\\
U''-\frac{U'}{w}+w^2U=0\\
\Rightarrow y''-\frac{y'}{w}+w^2y=0 \Rightarrow y(w) = c_1\cos\left(\frac{w^2}{2}\right)+c_2\sin\left(\frac{w^2}{2}\right)
\end{array}$$ where not just imply that $U \equiv V$ which is false, but also the solution is an even function which don't fulfill the properties of $V(w)$ which is odd - an also is a fixed solution when the transform can have multiple values. The problem I think is that the transform $X(w)$ which is related to any function (so it could take many forms), cannot be considered as a function in their differentiation property: $$\frac{\partial X(w)}{\partial w} = iw X(w) \overset{\text{as function}}{\Rightarrow} X'(w)-iwX(w)=0 \Rightarrow X(w)=X(0)e^{i\frac{w^2}{2}}=\int\limits_{-\infty}^{\infty}x(t)dt\ e^{i\frac{w^2}{2}}$$ which is a fixed functions differently from the transform, so it is a conceptual mistake, but I would like to know why it don't work: Does it imply is not possible to make a differential equation for $V(w)$ ? As example using the Kramer-Kronig relation an $\text{Eq. 2}$ : $$V(w) = \displaystyle{-\frac{\pi}{2}\,\text{P.V.}\!\!\!\int\limits_{-\infty}^{\infty} \frac{1}{\xi(\xi-w)}\cdot\frac{\partial V(\xi)}{\partial \xi}\,d\xi}$$ Is this conceptually wrong? Does having a differential equation fix the solution so is not applicable in the transforms framework? My guess is $\frac{\partial X(w)}{\partial w} \neq \frac{\partial U(w)}{\partial w}+i\frac{\partial V(w)}{\partial w}$ but I don't know why is not plausible, since the derivative is a linear operator.","['fourier-analysis', 'fourier-transform', 'distribution-theory', 'complex-analysis', 'real-analysis']"
4491000,About divergence of a vector field,"Let $f:\mathbb{R}^n\to \mathbb{R}^n$ . Divergence of $f$ is defined as $$\operatorname{div}(f)=\sum_{i=1}^n \frac{\partial f_i}{\partial x_i}$$ I am reading a paper where it says that the divergence of a vector field defined by the odes $$\frac{dq_i}{dt}=\frac{\partial H}{\partial p_i},\frac{dp_i}{dt}=-\frac{\partial H}{\partial q_i}$$ for $i=1,\ldots, n$ where $H:\mathbb{R}^n\times \mathbb{R}^n\to \mathbb{R}$ is given by $$\sum_{i=1}^n \frac{\partial}{\partial q_i}\frac{dq_i}{dt}+\frac{\partial}{\partial p_i}\frac{dp_i}{dt}$$ and I don't understand how that follows from definition of divergence of a function and the odes. Any suggestions?","['vector-fields', 'multivariable-calculus', 'ordinary-differential-equations']"
4491031,A closed form for integral $\int_0^\infty \frac{\ln(1-e^{-\pi x})}{1+x^2} dx$,"$$\int_0^\infty \frac{\ln(1-e^{-\pi x})}{1+x^2} dx$$ let $u=e^{-\pi x}$ , the integral goes to: $$\pi \int_0^1 \frac{1}{u}\cdot\frac{\ln(1-u)}{\pi^2+\ln^2(u)}du$$ This looks a little like Gregory's coefficient, but not the same, because the integral is from $0$ to $1$ , instead of from $0$ to $\infty$ . How to do next?","['integration', 'definite-integrals']"
4491049,"For Riemannian manifolds, what does it mean to express a vector using the dual basis?","I've read most of Lee's Smooth Manifolds and also his Intro to Riemannian Manifolds . In Lee's books, for a manifold $M$ , he defines a tangent vector $v \in T_p M$ as a derivation at a point $p$ , i.e., a linear map that maps real-valued functions $f \in C^\infty (M)$ to a real number: $v: C^\infty (M) \rightarrow \mathbb R$ . And I'm not sure if it's necessarily has to be of this form, but he seems to identify them earlier by the fact that a tangent vector $v$ yields a map $D_v|_p: C^\infty (M) \rightarrow \mathbb R$ , the directional  derivative of a function $f$ . He also defines tangent covectors as linear functionals on the tangent space at a point $p$ , i.e., if the tangent space is a vector space $V$ , then the covector space $V^*$ is the set of linear maps $\omega: V \rightarrow \mathbb R$ , and the basis $E_j$ and dual basis $\epsilon_i$ of the vector/covector spaces are orthogonal in the usual way: $\langle  \epsilon_i , E_j \rangle = \epsilon_i (E_j) = \delta^i_j$ . That all makes sense to me, but I'm conceptually confused by other things I saw. E.g., in An Elementary Introduction to Information Geometry (p5) by Frank Nielsen, the author says: and: I read this to mean that a given vector $v$ can be expressed using either its primal basis ( $e_i$ ) or its dual basis ( $e^i$ ), in Einstein notation: $$v = v^i e_i = v_i e^i$$ What I don't understand is how $v$ can be expressed with either basis, given that the basis of the primal space are vectors, while the basis of the dual space are covectors! the way I read it in the Lee books were that vectors and covectors are different entities, which do different things. My best guess is that it somehow involves using the isomorphism the metric provides between a basis and its dual, e.g., $e^i = g^{ij} e_j$ , but I don't see how that gets around the fact that $v$ still must be a vector in the end. How can a vector be expressed using both the primal or dual bases? edit: and with regards to the last picture, I'm even more confused: my understanding (and what he says) is that the metric $g$ is a mapping $V \times V \rightarrow \mathbb R$ , and that defines an isomorphism that basically allows us to translate between vectors and covectors, i.e., $g: V \times V \rightarrow \mathbb R \Rightarrow g: V \rightarrow (V \rightarrow \mathbb R) \Rightarrow g: V \rightarrow V^\ast$ . And that's what he appears to be doing in eq's 7 and 8, but it's translating between $e_i$ and $e^{\ast j}$ , which seem to be both vectors...","['dual-spaces', 'riemannian-geometry', 'differential-geometry']"
4491055,Calculating odds of Trading Cards [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . The community reviewed whether to reopen this question 1 year ago and left it closed: Original close reason(s) were not resolved Improve this question I'm trying to calculate the odds of a box of trading cards containing less than $100$ unique cards.
From what I can gather, there's $11$ common cards per pack of $159$ unique common cards, $1$ item card of $13$ , $1$ rare of $79$ , one card that may be rare or higher (very rare - $1$ in $12$ packs, out of $31$ v.rare cards, or super rare - $1$ in $96$ packs, out of $6$ s.rare cards) and one special edition that can be of any of the common cards. There are $16$ packs to a box. I've not done statistics much since high school some $15$ years ago so I'm a little overwhelmed as to where to start. Thanks!",['statistics']
4491081,Why we cant take average of slope for non differential points,Why can't we use the formula of $$\lim_{h\to 0} \frac{f(x+h)-f(x-h)}{2h} $$ it gives the same result for continuous functions like $x^2$ and for discontinuous function it gives average of the slope on either side. It makes sense too as seen in the graph of velocity of ball as it touches the ground it should have $0$ velocity. Also as derivative is the approx of the values around that point it makes sense that at the corner of a discontinous function the derivative is the average of the slope on either side. so for $|x|$ the slope on either side of $0$ is $-1$ and $+1$ so the average should be $0$,"['limits', 'calculus', 'solution-verification', 'derivatives']"
4491090,"Why is $\lim\limits_{x\to a}\frac{f'(\alpha_x)}{g'(\alpha_x)}=\lim\limits_{x\to a}\frac{f'(x)}{g'(x)}$ when $\alpha_x\in (a,x)$ by Cauchy-Schwarz MVT?","Let $f$ and $g$ be continuous an differentiable on $[a,x]$ , and let $f(a)=g(a)=0$ , and assume that $g'$ and $g$ are $\neq 0$ on $(a,x]$ . Apply the Cauchy-Schwarz Mean Value Theorem to $f$ and $g$ on $[a,x]$ . Then, there is a number $\alpha_x$ in $(a,x)$ such that $$[f(x)-f(a)]g'(\alpha_x)=[g(x)-g(a)]f'(\alpha_x)$$ $$\frac{f(x)}{g(x)}=\frac{f'(\alpha_x)}{g'(\alpha_x)}\tag{1}$$ $\alpha_x$ approaches $a$ as $x$ approaches $a$ , ie $\alpha_x-a|<|x-a|$ so $0<|x-a|<\delta \implies 0<|\alpha_x-a|<\delta$ . Now assume that $$\lim_{x\to a} \frac{f'(x)}{g'(x)}$$ exists. If we take the limit of $(1)$ as $x\to a$ , apparently we can write the following $$\lim\limits_{x\to a}\frac{f(x)}{g(x)}=\lim\limits_{x\to a}\frac{f'(\alpha_x)}{g'(\alpha_x)}=\lim\limits_{x\to a} \frac{f'(x)}{g'(x)}\tag{2}$$ I've thought about this way too many hours and I can't convince myself in terms of the $\epsilon$ and $\delta$ definition of limit that we can justify the second equality in $(2)$ . After all, $\lim\limits_{x\to a}\frac{f'(\alpha_x)}{g'(\alpha_x)}$ means $$\forall \epsilon>0\ \exists \delta>0\ \forall x, 0<|x-a|<\delta \implies \left | \frac{f'(\alpha_x)}{g'(\alpha_x)}-l \right| <\epsilon$$ Now, this also means that $$\forall \epsilon>0\ \exists \delta>0\ \forall x, 0<|\alpha_x-a|<|x-a|<\delta \implies \left | \frac{f'(\alpha_x)}{g'(\alpha_x)}-l \right| <\epsilon$$ But even if we write $$\forall \epsilon>0\ \exists \delta>0\ \forall x, 0<|\alpha_x-a|<\delta \implies \left | \frac{f'(\alpha_x)}{g'(\alpha_x)}-l \right| <\epsilon$$ this still doesn't fit into the cookie-cutter definition of limit. I would expect to be able to write $\forall \alpha_x$ , but it wouldn't be true I believe. I would really like to understand how and why we can say $$\lim\limits_{x\to a}\frac{f'(\alpha_x)}{g'(\alpha_x)}=\lim\limits_{x\to a} \frac{f'(x)}{g'(x)}$$ By the way, for context on where this comes from, it is part of the proof of L'Hôpital's Rule (as present in Spivak's Calculus )","['proof-explanation', 'limits-without-lhopital', 'calculus', 'limits', 'derivatives']"
4491098,Number of Triangles in this figure,"How many triangles are in this figure? I suspect there are $45$ , but I would like someone to confirm or correct me. I arrived at $45$ after manually counting them. If anyone has a formula by which this number can be obtained, please share.","['combinatorics', 'triangles']"
4491148,Distribution of a random variable that is produced by another uniformly distributed random variable,"Assume a simple two-step lottery: the reward (denoted by $Y$ ) is a random variable and realized as follows: First: random variable $X$ is drawn from uniform distribution by interval $(a,c)$ . Second: After the realization of $X$ , a player determines reward ( $Y$ ) stochastically: with probability $\alpha$ : reward is $bX$ ( $b$ is constant) with probability $1-\alpha$ : reward is $X$ Now, what is the distribution of the random reward $Y$ ? If the solution is weighted average on both states: i.e. $Y=\alpha bX+(1-\alpha)X=(\alpha b+1-\alpha )X$ then we can say $Y$ is a linear rescaling transformation of $X$ and so $Y$ has the same distribution as $X$ has. But I am suspicious that $Y$ really is a weighted average of $X$ and $bX$","['probability-distributions', 'uniform-distribution', 'probability-theory', 'probability']"
4491204,Centroid coordinate for region,"This is the region from which I want to calculate the centroid: $\{(x, y) \in \mathbb{R}^2 : 0 < 2x < y < 3-x^2\}$ . I calculated the area for the region and it is $A = \frac{5}{3}$ . Now, for the $x$ coordinate of the centroid I'm trying to calculate: $$\bar{x} = \frac{S_x}{A} = \frac{\int_0^1\int_{2x}^{3-x^2}ydydx}{\int_0^2\int_0^{\frac{1}{2}y}dxdy + \int_2^3\int_0^\sqrt{3-y}dxdy} = \frac{\frac{1}{2}\int_0^1\frac{y^2}{2}\biggr\rvert_{2x}^{3-x^2}dx}{\frac{5}{3}} = \frac{21/6}{5/3} = \frac{21}{15}$$ But the solutions give $\frac{7}{20}$ . I'm not understanding what am I doing wrong. Is it in my logic by constructing the $\bar{x}$ integrals? Or is it my integral calculations? I'm being around this problem for a while now.","['area', 'centroid', 'multivariable-calculus', 'calculus', 'functions']"
4491205,How much do we have to do this action to make this triangle equilateral?,"How much do we have to do this action to make this triangle equilateral: $P(A): $ Move $A$ to the meet of the perpendicular bisector of $\overline{BC}$ and parallel line of $\overline{BC}$ which includes $A$ . So, I tried some: Original: $P(A)$ : $P(B)$ : $P(C)$ : And so on... Can we make this triangle equilateral with these actions? If we can, how much do I have to do these actions( $P(A), P(B), P(C), P(A_1), P(B_1), P(C_1), \cdots$ ) to make $\triangle A_{\square}B_{\square}C_{\square}$ equilateral?","['triangles', 'vectors', 'geometry']"
4491230,Property of positive definite matrix: Why is this True?,"I have left maths world a few years ago and doing Machine learning nowadays. But In one of the paper I am reading, the author just writes (well slightly different wording/ notation) $H$ is positive definite so $H^{-1}v$ = arg min $_t \{ t^\top H t - v^\top t \}$ and quite honestly I have no idea why this is the case. Help would be nice thanks!","['matrices', 'linear-algebra', 'positive-definite']"
4491237,Why is the localization of a commutative Noetherian ring still Noetherian?,"This is an unproven proposition I've come across in multiple places. Suppose $A$ is a commutative Noetherian ring, and $S$ a multiplicative subset of $A$. Then $S^{-1}A$ is Noetherian. Why is this? I thought about taking some chain of submodules
$$
S^{-1}M_1\subset S^{-1}M_2\subset\cdots
$$
and pulling back to a chain 
$$
M_1\subset M_2\subset\cdots
$$
of submodules of $A$ which must eventually stablize. Is there more to it than this? I kind of wary of assuming all submodules of $S^{-1}A$ have form $S^{-1}M$ for $M\leq A$.","['noetherian', 'localization', 'modules', 'ring-theory', 'commutative-algebra']"
4491252,Intersection points of two exponential decay functions,"I have not found much online regarding this question. It's by no means my area of research, but is a questioning I came across when writing a program.
If I have two exponential decay functions defined on the same interval $[0,1]$ , how many intersection points can they have on this interval? The two functions can be defined in the form (with $a,b,c,a',b',c' > 0$ ): \begin{align}
f(x) = a + b\exp(-c x), && g(x) = a' + b'\exp(-c' x).
\end{align} Assuming $f(x)$ and $g(x)$ are not equal, I seem to either find $0$ , $1$ or $2$ intersection points. Would you have suggestions on how to approach this problem?
Thank you!","['functions', 'exponential-function', 'analysis']"
4491269,Is the functor $\operatorname{Spec}(R) \mapsto \operatorname{GL}_n(R) / R^*$ not a Zariski sheaf?,We call Zariski sheaf a contravariant functor $F: (\operatorname{Aff \ Sch}/k) \to (\operatorname{Grp})$ such that $F_X: (\operatorname{Op}(X) \cap (\operatorname{Aff \ Sch}/k)) \to (\operatorname{Grp})$ is a sheaf for each affine scheme $X$ (here $\operatorname{Op}(X)$ is the category of open subsets of $X$ ) Fix $n>0$ and consider the functor $F: (\operatorname{Aff \ Sch}/k) \to (\operatorname{Grp})$ such that $F(\operatorname{Spec}R) = \operatorname{GL}_n(R) / \operatorname{id_n}\cdot R^*$ . I want to show that $F$ is not a Zariski sheaf. I know that in general quotient of sheaves are not sheaves. In this case it seems to be easier to find a proof of the fact that $F$ is not a Zariski sheaf (since we ask for a quotient sheaf for each affine scheme) but I can't find a way. Any hint or ideas? Thank you in advance,"['algebraic-geometry', 'functors', 'schemes', 'sheaf-theory']"
4491289,Prove that the sum of cosines of the two smallest angles in a triangle is $\geq 1$,"Claim: If $\alpha, \beta$ are the two smallest angles in a given triangle, then $\cos(\alpha)+\cos(\beta)\ge1$ . Is there a short/illuminating proof of this, perhaps purely geometrical? I was able to prove this, but in way I feel to be a little tortuous. I'll outline my proof. If $\gamma$ is the largest angle (not necessarily unique) and $M=180°  -\gamma$ , then I want to minimize the function $f(x)=\cos(x)+\cos(M-x)$ and prove that its minimal value $\ge1$ . The lower bounds on $x$ are $x>0$ and $x \ge 180°-2\gamma$ , the latter to ensure $\gamma \ge M-x = 180°-\gamma-x$ . By solving $f'(x)=0$ , we see that the only extremum value is at $x=M/2$ , but this is a maximum (by looking at the second derivative or a simple example). Therefore $f(x)$ attains its minimum on the boundary, and by assuming $x$ to be the smaller angle of the two, we can take it to be either $0$ in the limit or $180°-2\gamma$ . In the former case, $\gamma>90°, M<90°$ , and $f(x)$ tends to $f(0)=1+\cos M >1$ , so $f(x)\ge 1$ .
In the latter case, we have the angles $x,\gamma,\gamma$ , so $\gamma=90°-\frac{x}{2}$ and the minimal value of $f(x)$ is $$\cos(x)+\cos(90°-\frac{x}{2})=\cos(x)+\sin(\frac{x}{2})=1-2\sin^2(\frac{x}{2})+\sin(\frac{x}{2}))$$ $$=1+\sin{\frac{x}{2}}(1-2\sin\frac{x}{2})$$ and since $x$ as the smallest angle $\le 60°$ , we have $\sin(\frac{x}{2})\le\frac{1}{2}$ , and the minimal value of $f(x)$ greater or equal to $1$ as required.","['euclidean-geometry', 'trigonometry', 'geometry', 'inequality']"
4491299,Could the product of a skew-symmetric matrix and an invertible matrix be nilpotent?,"Suppose that $A$ is a $d\times d$ skew-symmetric matrix, $B$ is a $d\times d$ invertible matrix and $AB$ is a nilpotent matrix. The unique example I could find is $A=O$ , the null matrix. My question: Does there exist another pair of matrices $A,B$ such that $A$ is skew-symmetric, $B$ is invertible and $AB$ is nilpotent? I guess $A=O$ is the only possibility. Since $AB$ is nilpotent, its Jordan canonical form is \begin{bmatrix} 
   S_1 & 0 & \ldots & 0 \\ 
   0 & S_2 & \ldots & 0 \\
   \vdots & \vdots & \ddots & \vdots \\
   0 & 0 & \ldots & S_r 
\end{bmatrix} where each of the blocks $ S_{1},S_{2},\dots ,S_{r}$ is a shift matrix (possibly of different sizes). And the column transformations can not change this matrix to be skew-symmetric.","['matrices', 'linear-algebra', 'linear-transformations', 'matrix-decomposition']"
4491333,Does a pointwise convergent sequence of functions converges locally uniformly almost everywhere?,"Does a pointwise convergent sequence of functions $f_n:\mathbb R\to \mathbb R$ converges locally uniformly almost everywhere? Here locally uniform convergence at $x$ means $x$ has a open neighborhood on which the restriction of $f_n$ converges uniformly (as opposed to the restriction of $f_n$ to every bounded set converges uniformly). The motivation of this question is the statement of Egorov's Theorem in Terry Tao's Introduction to Measure Theory : Let ${f_n: {\bf R}^d \rightarrow {\bf C}}$ be a sequence of measurable functions that converge pointwise almost everywhere to another function ${f: {\bf R}^d \rightarrow {\bf C}}$ , and let ${\epsilon > 0}$ . Then there exists a Lebesgue measurable set ${A}$ of measure at most ${\epsilon}$ , such that ${f_n}$ converges locally uniformly to ${f}$ outside of ${A}$ . The author comments one cannot pick $A$ to have measure zero if one uses the definition of local uniform convergence in terms of bounded subset. But I think it is more natural to use the definition in terms of open neighborhood and I wonder if in that case the statement can be upgraded. Note that the moving bump counterexample doesn't work (since it converges locally uniformly everywhere) as well as the function $f_n=\frac{1}{nx}$ with $f_n(0)=0$ since you can just minus the point zero.","['measure-theory', 'lebesgue-measure', 'convergence-divergence']"
4491341,Limit Evaluation using Taylor Polynomials.,"Consider the following limit problem: $$\lim \limits _{x\to 0}\frac{e^x-1}{x}.$$ It's known that $$e^x=\lim \limits _{n\to \infty}\sum \limits _{r=0}^n\frac{x^r}{r!}\qquad \forall x\in N_0,$$ where $N_0$ is a deleted neighbourhood of $0$ . So the original limit gets converted into the following: $$\lim \limits _{x\to 0}\frac{\lim \limits _{n\to \infty}\sum \limits _{r=0}^n\frac{x^r}{r!}-1}{x},$$ which simplifies to: \begin{align*}\lim \limits _{x\to 0}\frac{\lim \limits _{n\to \infty}\left (1+\sum \limits _{r=1}^n\frac{x^r}{r!}\right )-1}{x} & =\lim \limits _{x\to 0}\frac{\lim \limits _{n\to \infty}\sum \limits _{r=1}^n\frac{x^r}{r!}}{x} \\
& =\lim \limits _{x\to 0}\left (\lim \limits _{n\to \infty}1+\sum \limits _{r=2}^n\frac{x^{r-1}}{r!}\right ) \\
& =1.
\end{align*} My only problem with the above method is how can we say that the last limit will for sure be $1$ ? Precisely, how are we so sure that the last summation( $\displaystyle \sum \limits _{r=2}^n\frac{x^{r-1}}{r!}$ ) will be $0$ when both limits (of $n$ and of $x$ ) are applied to it? I'm still in high school so I don't know how to evaluate limits using epsilon-delta definition. Is there any other way(other than using  epsilon-delta definition) to convince myself that the last summation indeed is $0$ in the limit? PS: I know other more efficient ways to approach the above limit problem, however I have a similar type of problem in every limit I evaluate using taylor expansions. So I'm using this limit as a placeholder for my general problem.","['limits-without-lhopital', 'calculus', 'taylor-expansion', 'limits', 'algebra-precalculus']"
4491359,Is there only one way to divide an equilateral triangle into congruent fourths?,"Suppose we wish to divide an equilateral triangle into fourths, such that each piece is congruent. (Let's also require connectedness.) One way to do this is to connect the medians, forming one inverted triangle in the center and three at the corners. Is this the only way? Are there any other ways to divide an equilateral triangle into congruent fourths?","['dissection', 'triangles', 'geometry']"
4491432,How do I evaluate the following limit?,"I understand that I need to somehow use that $PR=AP=AQ$ as the point $A \to P$ . But beyond that, I am unable to use that information to find $OB$ . This problem is from the textbook ""Calculus with analytic geometry"" by G. Simmons. The problem is the $27$ th problem in section $12.2$ where the L'Hospitals rule is introduced.","['analytic-geometry', 'limits', 'calculus', 'circles']"
4491460,Why can't matrix commute?,"Sorry if this question sounds silly, and it probably is. I can prove easily that matrix multiplication is noncommutative; however, look at this 'fake' proof: $$AB=e^{\ln(A)}e^{\ln(B)}=e^{\ln(A)+\ln(B)}$$ $\ln(A)$ and $\ln(B)$ are also matrices, and matrix addition is comutative. So: $$e^{\ln(A)+\ln(B)}=e^{\ln(B)+\ln(A)}=e^{\ln(B)}e^{\ln(A)}=BA$$ What's the problem?","['exponentiation', 'logarithms', 'fake-proofs', 'matrices', 'linear-algebra']"
4491482,Conditional Variance of PDF,"A random vector $(X,Y)$ has a continuous distribution with a density function $$f(x,y)=\begin{cases}c⋅x & \text{when }0 ≤ x ≤ 2, \max\{0,1−x\} ≤ y ≤2−x\\ 0& \text{otherwise}\end{cases}$$ where $c > 0$ is a constant. Find variance of a $Y$ conditioned on $X = 1.5$ , $Var(Y |X = 1.5)$ . I found $c = \frac 67$ with the given integral. Now I want to ask how can I find variance? I found variance as -87/3136. Is it possible ? Here is my attempt Thank you","['statistics', 'probability']"
4491521,Let $H$ be a normal subgroup of a finite group $G$ such that $|H|=4$. Then $G$ contains a normal subgroup of order $2$ or a subgroup of index $3$.,"Let $H$ be a normal subgroup of a finite group $G$ such that $|H|=4$ . Then $G$ contains a normal subgroup of order $2$ or a subgroup of index $3$ . My attempt: By Lagrange's theorem, $4\mid |G|$ . In particular, $G$ has a subgroup of order $2$ . If $G$ is Abelian, this subgroup is normal. So, assume that $G$ is not Abelian. Thus $|G|\geq 8$ .
My idea was to embed $G$ into a larger group $K$ such that $G$ is a maximal subgroup of $K$ and $|K:G|=4$ . Then $G$ would have a subgroup of index $3$ using: Let $H$ be a maximal subgroup of a finite group $G$ such that $|G:H|=4$ . Then there exists $K\leq H$ such that $|H:K|=3$ . Suppose that $|G|=4n\ (n\geq 2)$ . Then we need $|K|= 16n$ . I tried using induction, but couldn't succeed.","['normal-subgroups', 'group-theory', 'abstract-algebra', 'finite-groups']"
4491555,Solving $y'' + 2y' + 2y = 2\delta' + 2\delta$ without Laplace transform,Im trying to solve the following differential equation: $$y'' + 2y' + 2y = 2\delta' + 2\delta$$ I did this by first setting $ y(t) = z(t)\theta(t)$ and finding the causal solution to the problem. From this i got the following solution: $$y(t) = 2e^{-t}\cos(t)\theta(t) $$ However this is only the solution to the homogenous equation and when it comes to finding a particular solution im stuck. I'm somewhat aware that it can be solved with Laplace transform but since i haven't studied it yet i can't use it.,"['step-function', 'dirac-delta', 'ordinary-differential-equations']"
4491586,Ordinary generating function - undo transformation without complex arithmetic,"Ordinary generating function is very similar in concept to the $Z$ transform. For linear recurrences with constant coefficients we get rational functions. Suppose that we are limited to real partial fractions. We have rational function $$\frac{P(x)}{Q(x)}.$$ We have following cases: Denominator has distinct linear factors (we do not consider complex numbers). Here it is easy to calculate $n$ th derivative or simply get geometric series. Denominator has distinct irreducible quadratic factors.
Here we know that \begin{align*}\operatorname{OGF}\left (\sin \left (\frac{\pi}{2}n\right )\right ) & =\frac{x}{1+x^2}, \\
\operatorname{OGF}\left (\cos \left (\frac{\pi}{2}n\right )\right ) & =\frac{1}{1+x^2}.
\end{align*} But in this case we have following fractions: $$\frac{Ax+B}{x^2+px+q}.$$ Can we conclude something if we complete the square in the denominator? Is there shift property like in Laplace transform? Denominator has repeated factors (linear of irreducible quadratics). Here we have that $$\operatorname{OGF}(f*g)=F(x)G(x),$$ but convolution is given by sum instead of an integral $$(f*g)(n)=\sum \limits _{k=0}^nf(k)g(n-k).$$ Maybe this question help me. What is $\operatorname{OGF}(a^n\cos (\omega n+\theta ))$ or $\operatorname{OGF}(a^n\sin (\omega n+\theta ))$ ? I think that one of these is enough. Is it necessary to play with complex numbers to get this $\operatorname{OGF}$ ? If I don't make mistake in my calculations $$\operatorname{OGF}(a^n\cos (\omega n+\theta ))=\frac{\cos (\theta )-a\cos (\omega -\theta )x}{1-2a\cos (\omega )x+a^2x^2}.$$","['partial-fractions', 'discrete-mathematics', 'generating-functions']"
4491587,"According to the figure, what is the area of the triangle $BCD$ of type $x$?","$Q:$ In a circle $\omega \ $ with a center $A$ , $E\in \omega \ $ is the tangent point, $[BA]\bot[AC] \ $ , $\angle ABD= \angle DCB \ $ , $\angle DBC= \angle DCA \ $ , $D\in\omega \ $ , $|EC|=x \ $ What is the area of the triangle $BCD$ of type $x$ ? $\text{Try on me:}$ I drew $[AD]$ and named it $\angle BAD=\theta$ and $|BD|=a, |DC|=b$ . I also knew that $\alpha+ \beta =45^{\circ}$ . I first found $\frac{\sin{\beta}}{\sin{\alpha}}=\frac{b}{a}$ by applying the sine theorem in the triangles $\triangle ABD$ and $\triangle DCA$ . Then $\tan{\theta}=\frac{a^2}{b^2}$ came out of the trigonometric ceva theorem. I have drawn a right triangle between the point $D$ and the right part of $\triangle DCA$ by selecting the constant of the ratio $k$ . By trying a little, I got the following two equivalences, $r$ is the half-diameter of the circle: $$2r^2+2x^2=\frac{1}{2k^2},$$ $$a^4+b^4+\sqrt{2}ab=\frac{3}{4k^2}.$$ I tried to make a few more calculations and make observations by choosing $r=1$ , but I couldn't get anything. Can you help? Any help will be greatly appreciated.","['euclidean-geometry', 'geometry']"
4491593,Different approaches to Jordan Canonical Form,"I know two different proofs of the existence of JCF. Let $V$ be a finite-dimensional vector space over base field $\mathbb{C}$ and $\alpha \in \mathsf{End}_{\mathbb{C}}(V)$ . Given transformation $\alpha$ we can define the action of polynomial $f(z) \in \mathbb{C}[z]$ on $v \in V$ as $f(z)v := f(\alpha)(v)$ . This turns $V$ into a $\mathbb{C}[z]$ -module. Since $\mathbb{C}[z]$ is a PID, $V \cong \oplus_{i,j} \frac{\mathbb{C}[z]}{(p_i(z)^{r_{ij}})}$ , where $p_i(z) \in \mathbb{C}[z]$ are monic and irreducible and $r_{ij} > 0$ (polynomials $p_i(z)^{r_{ij}}$ are called 'elementary divisors'). Question 1 . Why after going from $V$ to $\oplus_{i,j} \frac{\mathbb{C}[z]}{(p_i(z)^{r_{ij}})}$ via this isomorphism, operator $\alpha \colon V \to V$ still acts via multiplication by $t$ ? UPD (Answer to Q1) . If $\pi\colon V \to \oplus_{i,j} \mathbb{C}[z] / (\cdots)$ is iso, then $$(\pi \circ \alpha \circ \pi^{-1})(v) = \pi(\alpha(\pi^{-1}(v))) = \pi(t \pi^{-1}(v)) = t \pi(\pi^{-1}(v)) = t v$$ by definition of $\alpha$ and linearity of $\pi$ . (Sorry, it was obvious). Hence it is easy to see that characteristic polynomial $\chi_\alpha(z)$ is equal to $\prod_{i,j} p_i(z)^{r_{ij}}$ , and $p_i(z) = z - \lambda_i$ . Moreover, $\alpha$ acts on each summand $U := \frac{\mathbb{C}[z]}{(z - \lambda)^r}$ with multiplication by Jordan cell $J_{\lambda, r}$ if we choose $$(z - \lambda)^{r-1}, (z-\lambda)^{r-2}, \ldots, (z - \lambda)^0 = 1$$ for basis of $U$ . The second approach is as follows. We can prove that $V$ is the direct sum of its root subspaces $V = \oplus_{i = 1}^s V^{\lambda_i}(\alpha)$ . We can prove that for each nilpotent operator $\beta := \alpha - \lambda \cdot I$ the space $V^\lambda(\alpha)$ is the direct sum of some $\beta$ -cyclic subspaces and on each of them $\alpha$ acts with multiplication by some Jordan cell $J_{\lambda,-}$ . If the first approach was chosen, then I don't understand how to obtain Jordan basis in terms of initial basis of $V$ ; using second approach it is an easy process, but the first approach seems more natural to me. Question 2 . How could we construct the Jordan basis of $V$ or, more specifically, how could we construct the isomorphism between $U := \frac{\mathbb{C}[z]}{((z - \lambda_i)^{r_{ij}})}$ and $r_{ij}$ -dimensional cyclic subspace of $V_{\lambda_i}(\alpha)$ ? Is the corresponding subspace actually $\pi^{-1}(U)$ ? Thank you in advance for any help!","['jordan-normal-form', 'linear-algebra', 'principal-ideal-domains', 'linear-transformations']"
4491604,Are there theorems about extending a monoid without inverses to a group by reflecting the elements? [duplicate],"This question already has answers here : From monoids to groups (2 answers) Closed 1 year ago . What I have in mind is something like the following: the natural numbers with addition form a monoid. You can imagine constructing the integers by taking the naturals, adding a set constructed by ""reflecting"" all of the non-zero natural numbers about $0$ , and extending addition. What I mean by ""reflecting"" is that for each $i\in N$ the reflected element $-i$ is such that $i+j=k\iff -i+-j=-k$ , and that $i+-i=0$ . My intuition is that this should be generally possible for monoids that don't have any inverses.","['monoid', 'group-theory', 'abstract-algebra', 'semigroups']"
4491615,Given graph of the function $f(x)=x^3+ax^2+bx+c$ What is the $x$ value at the local minimum point?,"This is a problem from a timed exam, so I prefer approaches that lead to answer quickly. Graph of the function $f(x)=x^3+ax^2+bx+c$ is as follow. What is the $x$ value at the local minimum point? $1)\frac12\qquad\qquad2)2\qquad\qquad3)\frac32\qquad\qquad4)3$ Here is my approach, Suppose we have minima at $x_0$ . We have, $$f(0)=4\Rightarrow c=4$$ $$f'(x)=3x^2+2ax+b\qquad\text{Since $f'(0)=0\rightarrow b=0$}$$ Hence $f'(x)=x(3x+2a)$ , So $x_0=-\frac{2a}3$ . We have $f(x)=x^3+ax^2+4$ . $$f(-\frac{2a}3)=0\Rightarrow -\frac{8a^3}{27}+\frac{4a^3}9+4=0\Rightarrow a=-3$$ Finally $x_0=\frac{-2a}3=2$ . Although one can get the answer with this approach in one or two minutes, I'm looking for quicker ways to solve the problem. Actually, first time I tried to solve the problem, I noticed that we can write $f(x)=(x+m)(x-n)^2$ where $m,n\in \mathbb{R}^+$ . According to graph of function and noting that $f(0)=mn^2=4$ , I can guess it the function is $f(x)=(x+1)(x-2)^2$ . But assuming I'm in the exam, at this point should I quickly mark the answer and go to the next question or this approach is unreliable and I got the correct answer with a bit of luck?!","['maxima-minima', 'calculus', 'functions']"
4491620,Is there a way to find/closely estimate the value that the Mean Value Theorem promises?,"For background, the Mean Value Theorem says that if $f\in C^1([a,b])$ , then there exists $c\in(a,b)$ such that $$f'(c) = \frac{f(b) - f(a)}{b-a}.$$ My question is, how does $c$ vary as a function of properties of $f$ , $a$ , and $b$ ? For some $f$ , we have an easy closed form for $c$ .  Take $f(x) = x^3$ , $a = 0$ , and $b$ an arbitrary $x$ (wlog, assume $x > 0$ ), for example.  Then we have the equation $$\frac{x^3}{x} = 3c^2,$$ so that $c = \frac{x}{\sqrt{3}}$ . But what if the function is more complicated?
Obviously we could just solve numerically, but is there a theorem that can tell us anything interesting about $c$ ?  For example, a theorem that tells us when $c$ is a polynomial function of $x$ ?  Or an algebraic function?  Or how close to $x / 2$ we can expect $c$ to be?  How good an estimate $c = x/2$ is?  How higher derivatives affect whether $c$ is closer to one or the other endpoint?  Etc. Running list of insights We may assume $a = 0$ and $f(0) = 0$ without loss of generality because derivatives are well-defined up to addition by a constant and phase shifts do not affect the underlying problem.","['mean-value-theorem', 'calculus', 'derivatives', 'taylor-expansion']"
4491630,How would one go about summing the factorials?,"I wish to sum the following series: $$
\sum_{r=1}^nr!
$$ My initial thought was to first convert this in terms of the gamma function, and sum all of the integrals that this made like so: $$
\sum_{r=1}^nr! = \sum_{r=1}^n\Gamma(r+1)=\sum_{r=1}^n\int^\infty_0x^re^{-x}\,dx
$$ No matter how hard I try, I struggle to find a solution yet Wolfram Alpha managed to find the answer to be $(-1)^{n+1}\Gamma(n+2)!(-n-2)-!(-1)-1$ where $!n$ is the sub-factorial function. I feel like this is a pretty complex problem, but could someone try  and help me understand this? Or at least give me a better angle to take this problem from as using the gamma function may not be the best way to solve this.","['integration', 'gamma-function', 'factorial', 'sequences-and-series']"
4491633,"Spivak's Calculus, Ch. 14, Problem 29a: If $f$ is continuous on $[0,1]$, compute $\lim\limits_{x\to 0^+} x\int_x^1 \frac{f(t)}{t}dt$.","The following problem is from Chapter 14 ""The Fundamental Theorem of Calculus"" from Spivak's Calculus (a) If $f$ is continuous on $[0,1]$ , compute $\lim\limits_{x\to 0^+} x\int_x^1 \frac{f(t)}{t}dt$ . I am aware that someone asked a question about this problem but the question I have is not about the use of L'Hôpital's Rule or about a solution involving supremums or logarithms. I am interested in a proof along the lines of the solution manual, which uses knowledge present in the book only up to this chapter. Here is my solution which provides many intermediate steps that the solution manual doesn't. I'd like to know if they are correct. We can rewrite the limit as $$\lim\limits_{x\to 0^+} \frac{\int_x^1 \frac{f(t)}{t}dt}{\frac{1}{x}}$$ Let $$f(x)=\int_x^1 t^{-1}dt$$ $$g(x)=x^{-1}$$ It can be shown that $$\lim\limits_{x\to 0^+} f(x)=\lim\limits_{x\to 0^+} g(x)=\infty$$ $$\lim\limits_{x\to 0^+} x^{-1}=\infty$$ Also, $$\lim\limits_{x\to 0^+} \frac{f'(x)}{g'(x)}= \lim\limits_{x\to 0^+} \frac{-\frac{1}{x}}{-\frac{1}{x^2}}=\lim\limits_{x\to 0^+} x=0$$ Therefore, by L'Hôpital's Rule we can infer that $$\lim\limits_{x\to 0^+} x \int_x^1 t^{-1} dt=\lim\limits_{x\to 0^+} \frac{f(x)}{g(x)}=\lim\limits_{x\to 0^+} \frac{f'(x)}{g'(x)}=0\tag{1}$$ But we want to compute $\lim\limits_{x\to 0^+} x\int_x^1 \frac{f(t)}{t}dt$ . Since $f$ is continuous on $[0,1]$ , it is bounded on that interval. Therefore, there is some $M>0$ such that for $x\in [0,1]$ we have $|f(x)|\leq M$ . Therefore $$-M \leq f(x)\leq M$$ $$-\frac{M}{x}\leq \frac{f(x)}{x}\leq \frac{M}{x}$$ $$-\int_x^1 \frac{M}{t}dt\leq \int_x^1\frac{f(t)}{t}dt\leq \int_x^1\frac{M}{t}dt$$ Multiply by $x$ (and remember that $x\in [0,1]$ $$-x\int_x^1 \frac{M}{t}dt\leq x\int_x^1\frac{f(t)}{t}dt\leq x\int_x^1\frac{M}{t}dt$$ And take the limit $$0=\lim\limits_{x\to 0^+}\left [-x\int_x^1 \frac{M}{t}dt\right ]\leq \lim\limits_{x\to 0^+}x\int_x^1\frac{f(t)}{t}dt\leq \lim\limits_{x\to 0^+} x\int_x^1\frac{M}{t}dt=0$$ Hence we have $$\lim\limits_{x\to 0^+}x\int_x^1\frac{f(t)}{t}dt=0$$ $$\blacksquare$$ The solution manual has something very similar, but as usual is very terse and skips many intermediate steps. I'd like to know if my solution above is correct at every step.","['integration', 'calculus', 'solution-verification', 'limits', 'derivatives']"
4491637,"$ f(x) = f(x + 1), \, \forall x \in \mathbb{R} \land \displaystyle \lim_{x \to \infty} f(x) = L \Rightarrow f(x) = L, \forall x \in \mathbb{R}$","I want to prove that $ f $ is continuous $\,  \land f(x) = f(x + 1), \, \forall x \in \mathbb{R} \land \displaystyle \lim_{x \to \infty} f(x) = L \Rightarrow f(x) = L, \forall x \in \mathbb{R}$ , is my proof valid? Assume $ \exists x \in \mathbb{R} $ such that $ f\left(x\right) \neq L $ . Choose $ \epsilon = \left|f\left(x\right) - L\right| \neq 0 $ , because the absolute value function is a metric. Therefore, $ \exists N \gt 0 : \forall z \gt N, \left|f\left(z\right) - L\right| \lt \epsilon $ . Choose $ z = x + \left|\lceil Nx \rceil\right| \gt N \Rightarrow \left|f\left(z\right) - L\right| =  \left|f\left(x\right) - L \right| \lt \epsilon$ - contradiction.","['limits', 'calculus', 'real-analysis']"
4491676,Show that $\mathbb{Q}(\sqrt[3]{2})$ does not have non trivial automorphisms,"Since I started the abstract algebra course, I haven't had exercises associated with the existence or non-existence of automorphisms, and I haven’t found exercises similar to this either so I don't know how to proceed. I would greatly appreciate your help.","['field-theory', 'group-theory', 'abstract-algebra', 'rational-functions']"
4491700,Diameter and frontier in metric spaces,"It is well known that in normed vector spaces, ${\rm diam}({\rm Fr}(A)) = {\rm diam}(A)$ . What I'm looking for is a counterexample to this equality in a metric space not deriving from a norm. With the discrete distance ( ${\rm d}(x,y)=0$ if $x=y$ , else ${\rm d}(x,y)=1$ ), ${\rm Fr}(A)$ is empty for any $A$ , so you could say ${\rm diam}=-\infty$ , but it's just a trick. I'm looking for a real counterexample where ${\rm diam}({\rm Fr}(A)) \ne {\rm diam}(A)$ . If you can provide some help, I would be grateful. \bye","['general-topology', 'metric-spaces']"
4491701,Finding a smooth curve $\alpha: \Bbb R\to \Bbb R^n$ such that $\overline{\alpha(\Bbb R)} = \Bbb R^n$,"I was reviewing my notes from a Differential Geometry course I took last year, and I started thinking of the following problem $\color{blue}{^1}$ . Let $n \ge 2$ . Does there exist a smooth curve $\alpha: \Bbb R\to \Bbb R^n$ satisfying $\overline{\alpha(\Bbb R)} = \Bbb R^n$ , i.e., the trajectory of the curve is dense in $\Bbb R^n$ ? It is not immediately clear that such a curve exists in the first place, but an explicit construction shall take that problem off our hands. Some Thoughts: Since $\Bbb R^n$ is separable, let us choose a countable dense subset $S:= \{x_k\}_{k\in \Bbb Z}$ of $\Bbb R^n$ . For every $k\in \Bbb Z$ , define $\alpha$ on the closed interval $[k,k+1]$ as the segment joining $x_k$ to $x_{k+1}$ . Then, the trajectory of $\alpha$ is dense in $\Bbb R^n$ , but the curve obtained is not necessarily smooth. Perhaps there is some way to modify the $\alpha$ so obtained, to ""smoothen"" it around the corners? Is the problem easier to solve when $n = 2$ ? Maybe we can take $n = 2$ as our guiding light to generalize to higher dimensions. I'd appreciate any help! Thanks a lot. P.S. By a ""curve"", I really mean a regular curve, i.e., $\alpha'(t) \ne 0$ for all $t\in \Bbb R$ , but please feel free to drop this assumption if that is really needed. Furthermore, it would be good if we can prevent the curve from intersecting itself. Smooth means $\mathcal C^\infty$ . $\color{blue}{1.}$ The source of this problem is my imagination , so I cannot comment on the difficulty and/or solvability of the same. It doesn't seem out of the scope of a first course in differential geometry, though.","['curves', 'differential-geometry', 'real-analysis']"
