question_id,title,body,tags
1355016,Gradient steepest direction and normal to surface?,"From this Maths SE question , I now understand the gradient to be the directional derivative that returns the steepest slope at a point. However, reading my textbooks, they all say that the gradient is normal to the tangent plane. In the above SE question, the gradient sounds like it should be parallel to a direction in the $xy$ plane, not actually up on the surface. My question is therefore do I have a misunderstanding of the tangent plane, gradient or directional derivative? In addition, I question how the gradient always passes through the origin or the $xy$ plane? If for instance in the graphic, the gradient had been another direction (say horizontal), then it wouldn't pass through the origin! In addition, the gradient of other points in the graphic would satisfy this example. Many thanks!","['vectors', 'multivariable-calculus']"
1355019,What's wrong with the argument $\sin^2{x}+\cos^2{x}=0$,"I will just quote an argument given in the book Analysis 1 by Terence Tao. Start with the expression $\lim_{x→\infty}\sin(x)$ , make the change of variable $x=y+\pi$ and recall that $\sin(y+\pi)=-\sin(y)$ to obtain
  $$\lim_{x→\infty}\sin(x)=\lim_{y+\pi→\infty}\sin(y+\pi)=\lim_{y→\infty}-\sin(y)=-\lim_{y→\infty}\sin(y)$$ Since $\lim_{x→\infty}\sin(x)=\lim_{y→\infty}\sin(y)$ we thus have$$\lim_{x→\infty}\sin(x)=-\lim_{x→\infty}\sin(x)$$
  and hence$$\lim_{x→\infty}\sin(x)=0$$
  If we then make the change of variable $x=\pi/2+z$ and recall that $\sin(\pi/2+z)=\cos(z)$ we conclude that $$\lim_{x→\infty}\cos(x)=0$$ Squaring both of these limits and adding we see that$$\lim_{x→\infty}(\sin^2(x)+\cos^2(x))=0^2+0^2=0$$
  On the other hand we have $\sin^2(x)+\cos^2(x)=1$ Although I have read the relevant portion from the book, I am still not able to understand the fault in the above argument.","['sequences-and-series', 'real-analysis']"
1355037,Banach spaces containing copies of $\ell^1$,Why is it important that a Banach spaces $X$ contains (or not) copies of the space $\ell^1$? I always hear talk about it but I don't know its importance. Could someone explain this?,"['motivation', 'banach-spaces', 'functional-analysis']"
1355044,Two variable perturbation analysis of differential equations,"I have following set of equations, $\frac{dy(t)}{dt}=k z(t) - 3 k y(t) - y(t)^2 + \epsilon_1 (M-z(t))^2$ $\epsilon_2 \frac{dz(t)}{dt}=Mz(t) - z(t) y(t) - 2 \epsilon_2 y(t) + 2 \epsilon_1 \epsilon_2 (M-z(t))^2$ From above, I can't even solve the unperturbed problem if the perturbation is around only one of the $\epsilon_1$ or $\epsilon_2$ . Now my question is: Is it possible to do perturbation theory for two small parameters and not just one? One more thing to note is: the equations are singular for $\epsilon_2$ but regular for $\epsilon_1$ . Any suggestion, reference or help on how to go about solving these equations will be of great help.","['nonlinear-system', 'ordinary-differential-equations', 'perturbation-theory']"
1355061,Is there a function that doesn't have a limit at infinity but its derivative does?,"Is there a function that is differentiable in $\mathbb{R}$, its derivative converges to $0$ as $x\to \infty$ but the function does not converge as $x \to \infty$, neither to a finite limit nor to an infinite limit?
If not, how can you prove that? Thank you Gal","['calculus', 'limits', 'derivatives']"
1355080,"Prove that $\sin x - x\cos x = 0$ has only one solution in $ [-\frac{\pi}{2}, \frac{\pi}{2}]$","I have to prove that $\sin x - x\cos x = 0$ has only one solution in $\left[-\frac{\pi}{2}, \frac{\pi}{2}\right]$. While it seems obvious that one solution might be $x=0$, I don't know how to do a formal proof.","['roots', 'calculus', 'trigonometry']"
1355081,why is representing rotations through quaternions more compact and quicker than using matrices??,"According to the wikipedia page on Quaternions: The representations of rotations by quaternions are more compact and quicker to compute than the representations by matrices. However, I have to admit, I don't fully understand what quaternions are and why they are useful. I have tried to read the article, but I don't understand why defining such a system is useful. It appears to a define a four dimensional space in which 3 components are imaginary and one is real. Is this attempting to describe spacetime? Regardless, I was hoping someone here could show how to represent a rotation using both quaternions and matrices and compare the two for me.","['quaternions', 'rotations', 'matrices']"
1355082,Weak convergence and integrals,"Assume
$$u_k\rightharpoonup u,\quad v_k\rightharpoonup v\quad\text{in}\quad L^1(0,T;Y)\tag{1}$$
and
$$\int_0^T u_k(t)\varphi'(t)\ dt=-\int_0^T v_k(t)\varphi(t)\ dt\tag{2}$$
for some $\varphi\in C_0^\infty(0,T)$.
""Passing to the limit"" we get
$$\int_0^T u(t)\varphi'(t)\ dt=-\int_0^T v(t)\varphi(t)\ dt.\tag{3}$$ My question is: How to justify this passage to the limit? Remark: this passage to the limit is a step of the proof that ""generalized derivatives are compatible with weak limits"" in the following sense:
$$u_k\rightharpoonup u\text{ in }L^p(0,T;Y)\quad \text{and}\quad u_k'\rightharpoonup v\text{ in }L^q(0,T;Y)\qquad \Rightarrow \qquad u_t=v,$$
where $1\leq p,q<\infty$ (page 419 of Zeidler's book ). Writing $u_k'=v_k$, $(2)$ is obtained from the definition of generalized derivative. And $(1)$ is obtained from the continuous embedding $$L^s(0,T;Y)\subseteq L^r(0,T;Y),\quad 1\leq r\leq s\leq\infty.$$
The author says that $(3)$ follows from the proposition below, but I didn't understand it. So, I'd like an explanation. Thanks.","['measure-theory', 'distribution-theory', 'functional-analysis', 'general-topology', 'partial-differential-equations']"
1355088,Is the absolute value of a P.D.S. matrix P.D.S.,"Suppose that $A$ is a positive definite symmetric matrix (P.D.S.). Now consider the matrix $|A|$, the matrix arrived at by taking the absolute value of all the entries of $A$. Is $|A|$ also P.D.S.? I have been trying to construct a counter example, but I can't seem to get one. Can someone proved a proof or counterexample?","['linear-algebra', 'matrices']"
1355091,Curvature flow for convex planes curves,"Tentative translation of the original question. I've read several articles on the curvature flow for convex plane curves (the curve remains convex during evolution, and eventually shrinks to a point). I found this result interesting but couldn't relate it to a real life phenomenon. Does anyone know of a real phenomenon which could shed some light on this result? Original question. Bonsoir mes amis, j'ai lu plusieurs articles sur le flot de courbure pour le cas des courbes convexes planes (pendant l’évolution la courbe reste convexe et tend vers un point à la limite), je l'ai trouvé très intéressant mais je n'ai pas pu appliquer un exemple dans notre vie pratique ou un certain phénomène déroulant dans la nature qui explique ce flot et cette évolution. Est-ce que quelqu'un a une idée d'un certain exemple ou phénomène réel qui explique ce résultat ??","['euclidean-geometry', 'geometry', 'riemannian-geometry', 'differential-geometry']"
1355097,Infinite Integral of Trigonometric Functions,"I am interested in finding the Integral: $I = \int\limits_{0}^{\infty} \sin x \,dx$. Clearly going the conventional way $I = -\cos (\infty) + \cos(0)$ will not lead to a definite answer. However I have thought of the problem in a different way. 
First, we already know from Fourier Transform that $ \int\limits_{-\infty}^{\infty}\exp \left(i \omega t\right) \, dt = 2 \pi \delta\left(\omega \right) $. Thus , upon setting $\omega = 1$, we have $ \int\limits_{-\infty}^{\infty}\cos \left(t\right) \, dt = 0$. But knowing that $\cos(t)$ is even in $t$ thus, $ 0 = \int\limits_{-\infty}^{\infty}\cos \left(t\right) \, dt = 2 \int\limits_{0}^{\infty}\cos \left(t\right) \, dt \Rightarrow \int\limits_{0}^{\infty}\cos \left(t\right) \, dt = 0$. Now consider the transformation $u = t + \frac{\pi}{2}$, we thus have $0 = \int\limits_{\frac{\pi}{2}}^{\infty}\cos \left(u - \frac{\pi}{2}\right) \, du = \int\limits_{\frac{\pi}{2}}^{\infty}\sin \left(u \right) \, du = \int\limits_{\frac{\pi}{2}}^{0}\sin \left(u \right) \, du + \int\limits_{0}^{\infty}\sin \left(u \right) \, du = \int\limits_{0}^{\infty}\sin \left(u \right) \, du -1 \Rightarrow \int\limits_{0}^{\infty}\sin \left(u \right) \, du = 1$. This solution is even sustained by the Laplace Transform results where we have $\mathcal{L} \lbrace \sin\left( t\right)\rbrace = \dfrac{1}{s^2 + 1}$. But $\mathcal{L} \lbrace \sin\left( t\right)\rbrace = \int\limits_{0}^{\infty}\exp \left(-st \right) \sin \left(t \right) \, dt $ Thus $\int\limits_{0}^{\infty}\sin \left(t \right) \, dt = \lim\limits_{s \rightarrow 0} \int\limits_{0}^{\infty}\exp \left(-st \right) \sin \left(t \right) \, dt = \lim\limits_{s \rightarrow 0}\dfrac{1}{s^2 + 1} = 1$.
Hence $\int\limits_{0}^{\infty}\sin \left(t \right) \, dt = 1$. I am here getting exactly the same result for the integral by computing it in two different methods. Is my work correct? I definitely know that $\sin$ is not Reimann-integrable over the interval $[0,\infty)$. Is there a special name for this integral, or its evaluation in this method? Thanks very much for your suggestions!","['real-analysis', 'integration']"
1355101,A summation involving multinomial coefficient,"We need to find out $$\sum {\binom{N}{a_1,a_2,a_3...a_B} a_1^{\alpha}a_2^{\alpha}...a_C^{\alpha} }$$ $$a_1+a_2...a_B=N, \alpha>0 ,0\lt C \le B$$ All are nonnegative integers. We need to sum for all $a_i$. $N$,$B$ and $C$ are constants. I was wondering if a closed form or a recurrence exists. I tried to solve it and failed miserably. All suggestions are welcome.","['summation', 'multinomial-coefficients', 'combinatorics', 'recurrence-relations']"
1355105,"Uniqueness of a configuration of $7$ points in $\Bbb R^2$ such that, given any $3$, $2$ of them are $1$ unit apart","This question from earlier today asks (paraphrasing here): Is there a configuration of $7$ points in the Euclidean plane such that, given any $3$ of the $7$ points, at least $2$ of them are $1$ unit apart? One such configuration, given in this answer , is the set of blue points in this diagram; the red line segments all have unit length: This is an embedding of the Moser Spindle graph. This raises a natural question: Is this the unique such configuration up to Euclidean motions?","['euclidean-geometry', 'plane-geometry', 'combinatorial-geometry', 'geometry', 'combinatorics']"
1355113,"If $f$ is continuous and piecewise $C^1$ and $f'$ is bounded a.e., is $f$ Lipschitz?","If $f$ is continuous and piecewise $C^1$ on $\mathbb{R}$ (only a finite number of pieces) and $f'$ is bounded a.e., is $f$ globally Lipschitz? So $f$ is only not differentiable in a finite number of places. If $f$ is absolutely continuous then it is Lipschitz but I don't see it.","['lipschitz-functions', 'continuity', 'functions']"
1355133,Why is the probability multiplied by $\binom{n}{k}$,"A while ago I asked a question about probability here Why is binomial probability used here? I get that you can find how many ways of choosing the $6$ correct out of $10$ questions. But why do we multiply by $\binom{n}{k}$? I thought it is simple casework that: $$P(\text{total probability}) = P(\text{Q 1->6 right and 7-10 wrong} + P(\text{Q 1->5 right, 6 wrong, 7 right, 8-10 wrong}) + ...$$ What is the idea?","['elementary-set-theory', 'algebra-precalculus', 'statistics', 'combinatorics', 'probability']"
1355160,Computing an Ito Integral using the Definition,"Let $B_t$ be a brownian motion adapted to $\mathcal F_t$. For general $\mathcal F_t$-adapted processes $X_t$ the Ito-integral could be defined as
$$
 \int_0^t X_s dB_s = \lim_{n\to \infty} \int_0^t X_s^{(n)} dB_s
$$
where the $X_t^{(n)}$ are simple processes approximating $X_t$ such that for each $0 = t_0 < t_1 < \ldots < t_k = t$ as the constant value on the interval $[t_i, t_{i+1})$ the left end point is choosen, i.e. $X_t^{(n)}(t) = X_{t_i}$ for $t_i \le t < t_{i+1}$ and such that
$$
 \lim_{n\to \infty} E\int_0^T | X_t^{(n)} - X_t |^2 dt = 0.
$$
Now I am looking for examples where this definition is used to compute the integral (and not Ito's Lemma). The only one I find in the literature are
$$
 \int_0^t B_s dB_s \quad \mbox{ and } \quad \int_0^t B_s^s dB_s
$$
For example, for the first we can choose
$$
 \int_0^1 B_s dB_s = \lim_n \sum_{i=0}^{n-1} B_{i/n} (B_{(i+1)/n} - B_{i/n})
$$
and by $\sum_{i=0}^{n-1} B_{i/n} (B_{(i+1)/n} - B_{i/n}) = \frac{1}{2} B_1^2 - \frac{1}{2} \sum_{i=0}^{n-1} (B_{(i+1)/n} - B_{i/n})^2$ we have
$$
  \int_0^t B_s dB_s. = \frac{1}{2} B_1^2 - \frac{1}{2}
$$
where the last term approaches $1$ as this is the quadratic variation of the Brownian motion. Another computation using the definition with the law of large numbers could be found on the german wikipedia . Now I am looking for other examples where the definition is used, do you know any? For example for $\exp(B_t)$, by using Ito's Lemme we find
$$
 \int_0^t \exp(B_s) dB_s = \exp(B_t) - \frac{1}{2}\int_0^t B_s ds
$$
but I do not know how to derive this result using just the definition (i.e. finding an approximating series of simple processes).","['probability-theory', 'stochastic-calculus', 'stochastic-integrals', 'stochastic-processes', 'probability']"
1355171,trigonometric finite series equals to polynomial function [duplicate],"This question already has answers here : Proving $\sum\limits_{l=1}^n \sum\limits _{k=1}^{n-1}\tan \frac {lk\pi }{2n+1}\tan \frac {l(k+1)\pi }{2n+1}=0$ (4 answers) Closed 8 years ago . I am interest to prove the equation below :
$$
\sum_{k=1}^m  \tan^2\left(\frac{k\pi}{2m+1}\right) =  m(2m+1) 
$$
you can understand better the first member of the equation here: WolframAlpha (mark the whole url with your mouse because i dont know why the link isn't blue at all)
sorry but i am not familiar writing equations here .
i hope to understand. 
so what's the problem in formula i cannot eliminate the trigonometric functions to prove this series is a polynomial .
any idea how to manipulate the formula ?","['sums-of-squares', 'trigonometric-series', 'algebra-precalculus', 'trigonometry']"
1355181,Independence of Poisson random variables coming from Poisson sampling,"Context: Let $x \in \mathbb{R}^n$ be the unknown probability vector of a finite discrete distribution $X$. We are able to sample $X$ and we want to learn $x$. Poissonization: Each observation belongs to the $i^\text{th}$ category with probability $x_i$, thus for a sample of size $m \in \mathbb{N}$, the sum of the $i^\text{th}$ category follows a binomial distribution $B(x_i\ ,\ m)$. These binomial random variables are not independent since their sum is $m$ ($X$ is a distribution). However, I found a trick online : if you sum over a $M \sim \mathrm{Poisson}(m)$ sample size rather than $m$, the sum $M_i$ of the $i^\text{th}$ category is no longer binomial but follows $\mathrm{Poisson}(m \times x_i)$ law. Furthermore, these $n$ Poisson random variables are independent! I try to prove this. My approach I proved that $M_i \sim \mathrm{Poisson}(m \times x_i)$ for any $i$ :
$$ \sum_{j=0}^\infty \left(\ P[\mathrm{Poisson}(m)=j] \times P[B(p,j)=k]\ \right) = P[\mathrm{Poisson}(p\times m)=k]$$
$\iff$
$$ \sum_{j=k}^{\infty} \left(\ P[\mathrm{Poisson}(m)=j] \times P[B(p,j)=k]\ \right) = P[\mathrm{Poisson}(p\times m)=k]$$
$\iff$
$$ \sum_{j=k}^{\infty} \left(\frac{e^{-m} m^j}{j!} \times \frac{j!\ p^k (1-p)^{j-k}}{k!\ (j-k)!} \right) = \frac{e^{-pm} (p m)^k}{k!}$$
$\iff$
$$e^{-m} \sum_{j=k}^\infty \left(\frac{m^j (1-p)^{j-k}}{(j-k)!} \right) = e^{-pm} m^k$$
$\iff$
$$e^{-m} \sum_{j'=0}^\infty \left(\frac{m^{j'+k} (1-p)^{j'}}{j'!} \right) = e^{-pm} m^k$$
$\iff$
$$e^{-m} e^{m(1-p)} m^k = e^{-pm}m^k $$
$\square$ Now, how can I prove that all those $n$ random variables are independent ? I found this paper which states that even if $X_1$, $X_2$ and $X_1 + X_2$ are $\mathrm{Poisson}$, $X_1$ and $X_2$ don't have to be independent. But here we are not in the general case since we have $\sum\limits_{i=1}^n x_i = 1$.","['binomial-distribution', 'probability', 'poisson-distribution', 'sampling']"
1355253,Find sum of a finite nested radical $ \sqrt{2012+\sqrt{2012+\sqrt{2012+\cdots}}} $ 2012 times,$$ \sqrt{2012+\sqrt{2012+\sqrt{2012+\cdots}}} $$ This is not an infinite series but is limited to the 2012th term . How can this expression be simplified?,"['sequences-and-series', 'nested-radicals']"
1355261,Brownian motion and associated martingales,"Under the Wiener measure $\Bbb{W}$ the process $x(t)$ is a brownian motion. This means that $\Bbb{E}[{x(t)-x(s)\mid \mathcal{F}_s}]=0$. Let $P$ be a measure in $C([0,\infty),\Bbb{R}^d)$ such that for every $\theta \in \Bbb{R}^d$ $$X_\theta(t):=\exp\bigg\{ \langle\theta,x(t)\rangle - \frac{1}{2} t |\theta|^2\bigg\}$$ is a martingale. Then $P = \Bbb{W}$ is the Wiener measure. Let $\sigma$ be a simmetric matrix: Assume now we define $$\tilde{P}(x(t_1)\in A_1, \ldots x(t_k) \in A_k) := \Bbb{W}(\sigma(x(t_1)-x) \in A_1,\ldots ,\sigma(x(t_k)-x) \in A_k)$$ Let $a = \sigma \sigma^T$ Is it true that $$Y_\theta(t):=\exp\bigg\{ \langle\theta,x(t)\rangle - \frac{d}{2} t \langle\theta,a\theta \rangle^2\bigg\} $$ Is a martingale? Attempt: $\langle\theta,\sigma(x_t-x)\rangle = \sum_{i,j}\theta_i \sigma_{i,j}(x(t)-x)_j$ We could use Ito calculus to conclude that \begin{align*}
\exp\{\langle\theta,\sigma(x_t-x\rangle)\} &= \exp\{\langle\theta,\sigma(x_0-x)\rangle\} + \sum_{j=1}^d\int_0^t  \partial_i\langle\theta\sigma(x(s)-x)\rangle \exp\{\langle\theta,\sigma(x_s-x)\rangle\}\, dx_i(s) \\
&+ \sum_{j = 1}^d \frac{1}{2}\int_0^t \partial_{i}\exp\{\langle\theta,\sigma(x_s-x)\rangle\} \partial_i \exp\{\langle\theta,\sigma(x_s-x)\rangle\} \, ds 
\end{align*}
Since $\partial_j \sum_{i,j}\theta_i \sigma_{i,j}(x(t)-x)_j = \sum_{i}\theta_i \sigma_{i,j}$
\begin{align*}
\exp\{\langle\theta,\sigma(x_t-x)\rangle\} &= \exp\{\langle\theta,\sigma(x_0-x)\rangle\} + \sum_{j=1}^d\int_0^t  \sum_{i}\theta_i \sigma_{i,j}\exp\{\langle\theta,\sigma(x_s-x)\rangle\}\, dx_i(s) \\
&+ \sum_{j = 1}^d \frac{1}{2}\int_0^t (\sum_{i}\theta_i \sigma_{i,j})(\sum_{i}\theta_i \sigma_{i,j})\exp\{\langle\theta,\sigma(x_s-x)\rangle\} \partial_i \exp\{\langle\theta,\sigma(x_s-x)\rangle\} \, ds 
\end{align*}
Now note that $(\sum_{i}\theta_i \sigma_{i,j})(\sum_{i}\theta_i \sigma_{i,j}) = \sum_{i,j}\theta_i a_{i,j}\theta_j= \langle \theta, a\theta \rangle$ So we conclude \begin{align*}
\exp\{\langle\theta,\sigma(x_t-x)\rangle\} &= \exp\{\langle\theta,\sigma(x_0-x)\rangle\} + \sum_{j=1}^d\int_0^t  \sum_{i}\theta_i \sigma_{i,j}\exp\{\langle\theta,\sigma(x_s-x)\rangle\}\, dx_i(s) \\
&+  \frac{d}{2}\int_0^t \langle \theta, a\theta \rangle \exp\{\langle\theta,\sigma(x_s-x)\rangle\} \, ds 
\end{align*} Now it suffices to note that once $\phi(t)$ is a continuous bounded (on intervals) martingale and $\psi(t)$ is a continuous function of bounded variation such that $\Bbb{E}[\|\psi \|_[0,t]]<\infty$ (the total variation on the interval $[0,t]$ has finite expectation) then 
$$\phi(t)\psi(t)- \int_0^t \phi(s)\,d\psi(s) $$ is also a martingale. Take \begin{align*}\phi(t)&=\exp\{\langle\theta,\sigma(x_t)-x\rangle\} -   \frac{d}{2}\int_0^t \langle \theta, a\theta \rangle \exp\{\langle\theta,\sigma(x_s)-x\rangle\} \, ds \\
\psi(t) &= \exp{-\frac{d}{2} t \langle \theta, a\theta \rangle }
 \end{align*} So in a sense, the result is already proved. Question: is there a different approach to this question? I was thinking something along the following lines: $$\tilde{P}(x(t_1)\in A_1, \ldots x(t_k) \in A_k) := \Bbb{W}(\sigma(x(t_1)-x) \in A_1,\ldots ,\sigma(x(t_k)-x) \in A_k)$$ Implies that 
$$\Bbb{E}_{\tilde{P}}[f(x_t)] = \Bbb{E}_{\Bbb{W}}[f(\sigma(x_t-x))]  $$ then the condition 
$$\Bbb{E}_{\Bbb{W}}\bigg[\exp\bigg\{ \langle\theta,x(t)\rangle - \frac{d}{2} t |\theta|^2\bigg\}\bigg]=1$$ Should imply that 
$$\Bbb{E}_{\tilde{P}}\bigg[\exp\bigg\{ \langle\theta,\sigma(x(t)-x)\rangle - \frac{1}{2} t \langle\theta, a\theta\rangle\bigg\}\bigg]=1$$
But I don't see it","['martingales', 'probability', 'stochastic-processes']"
1355278,Simplifying Chain of Conditional Variances given a Markov Chain,"$\newcommand{\Var}{\operatorname{Var}}$Suppose $X,Y,W$ form a Markov chain $X \to Y \to W$.
Can we simplify the following expression?
\begin{align*}
E [ \Var ( \Var (X\mid Y) \mid W)]
\end{align*}
Because we have a Markov chain, that is $p(x\mid y)=p(x\mid y,w)$, we have that
\begin{align*}
E[X\mid Y]&=E[X\mid Y,W]\\
\Var(X\mid Y)&=\Var(X\mid Y,W)\\
\end{align*} If we were to replace $\Var$ with expected value we can use towering property of expected value that is
\begin{align*}
E \left[ E \left(E [X\mid Y] \mid W\right)\right]&=E \left[ E \left(E [X\mid Y,W] \mid W\right)\right]\\
&=E[E[X\mid W]]=E[X]
\end{align*} Can we do something similar for $E \left[ \Var \left( \Var (X\mid Y) \mid W\right)\right]$? One thing we can show is 
\begin{align*}
E \left[ \Var \left( \Var (X\mid Y) \mid W\right)\right]=E \left[ \Var \left( \Var (X\mid Y,W) \mid W\right)\right]
\end{align*}
but I am not sure if this helps at all. Would be grateful for any ideas. Thank you very much","['conditional-expectation', 'probability', 'markov-chains']"
1355280,Transition probability between urn,"$N$ black balls and $N$ white balls are placed in two urns so that
  each urn contains $N$ balls. At each step one ball is selected at
  random from each urn and the two balls interchange. The state of the
  system is the number of white balls in the first urn. What I think Let $X$ the number of white balls in the urn $(1)$ then we want $$P_{jk}=P(X_{n+1}=k|X_n=j)$$
then we have three possibilities for $k$ $$(i)\;k=j+1$$ $$(ii)\;k=j$$ $$(iii)\;k=j-1$$
First we know if $(1)$ have $j$ white balls then $(2)$ have $N-j$ white balls. Let $A$ the event take a white ball in $(1)$ and $A'$ the event take black ball in $(1)$ then $$P(A)=\frac{j}{N}\space ,\space P(A')=\frac{N-j}{N}$$
analogous let $B$ the event take white ball in $(2)$ and $B'$ the event take a black ball in $(2)$ so $$P(B)=\frac{N-j}{N}\space,\space P(B')=\frac{j}{N}$$ Then we have for $i)$ $$P(X_{n+1}=k|X_n=j)=P(A')*P(B)=\left(\frac{N-j}{N}\right)^2$$
$ii)$ $$P(X_{n+1}=k|X_n=j)=P(A)P(B)+P(A')P(B')=\left(\frac{j}{N}\right)\left(\frac{N-j}{N}\right)+\left(\frac{N-j}{N}\right)\left(\frac{j}{N}\right)=2\left(\frac{j}{N}\right)\left(\frac{N-j}{N}\right)$$
$iii)$ $$P(X_{n+1}=k|X_n=j)=P(A)P(B')=\frac{j}{N}\frac{j}{N}=\left(\frac{j}{N}\right)^2$$ Is that right? There is an easier way to do?","['solution-verification', 'probability', 'stochastic-processes']"
1355300,Calculating the Lie algebra representation of the regular representation on subspace of functions on $\mathbb R$.,"Let $G = \mathbb R$ and let $\pi$ be the regular representation of $G$ on $L^2(\mathbb R)$, that is, $\pi(g)(f)(x) = f(x-g)$ for $g \in G$. Let $V = \{f \in \mathcal C_c^\infty | supp f \subseteq [0,1]\}$.
I am a bit confused at how to calculate the corresponding representation of the Lie algebra $\mathfrak g$ of $G$.
That should be for $X \in \mathfrak g$: $(Xf)(y) = \frac{d}{dt}|_{t=0} (\pi(exp(tX))f))(y) = [f'(y-e^{tX}) \cdot (-e^{tX} X)] _{t=0} = -f'(y-1) X$. 
But if this were correct, it wouldnt make sense, since now the support has moved and the above $V$ is supposed to be invariant under $\mathfrak g$. Where do I go wrong?","['lie-groups', 'lie-algebras', 'representation-theory', 'derivatives']"
1355311,"Is there a continuous $f(x,y)$ which is not of the form $f(x,y) = g_1(x) h_1(y) + \dots + g_n(x) h_n(y)$","For a continuous function $f : [0,1]^2 \to \mathbb{R}$, let us say $f$ is a sum of products (SOP) if there exist an integer $n > 0$ and continuous functions $g_1, \dots, g_n, h_1, \dots, h_n : [0,1] \to \mathbb{R}$ such that $$f(x,y) = g_1(x) h_1(y) + \dots + g_n(x) h_n(y)$$
for all $x,y \in [0,1]$. How can I show there exists a continuous $f$ which is not an SOP? I think this should be easy, but for some reason I don't see how to proceed. Note that the Stone-Weierstrass theorem says that every continuous function on $[0,1]^2$ is a uniform limit of SOPs.  I want to see that you can't drop the limit. Is the same true if we replace $[0,1]$ by an arbitrary infinite compact Hausdorff space?","['real-analysis', 'functional-analysis']"
1355320,Simplify expression involving real or imaginary part of complex rational function,Basically I want simplify the following so that the real or imaginary operator do not appear: $$\Im \prod_{i=1}^{N-1} \left(  z-x_i\right)^{l_i}$$ or $$\Re \prod_{i=1}^{N-1} \left(  z-x_i\right)^{l_i}$$ where $z = x+iy$ is complex and $x_i$ is real. The $l_i$ can be any real number. Is such a form readily obtained?,"['complex-analysis', 'complex-numbers']"
1355374,Showing a function is invertible,I came across this problem and not sure how to prove it. Show that if $ f\circ f \circ g\circ g \circ f\circ f $ is invertible then $ g $ is invertible. I'm not sure if it's correct to say that  $f$ is invertible because it's the most left function and then show that $g$ is invertible.,"['elementary-set-theory', 'functions']"
1355382,If $n$ is an integer then $n^2$ is the same as $0$ or $1\pmod 4$? [duplicate],"This question already has answers here : The square of an integer is congruent to 0 or 1 mod 4 (6 answers) Closed 8 years ago . I have been stuck on this problem for awhile. How would i go about solving it, an explanation would be helpful as well. Show that if $n$ is an integer then $n^2 \equiv 0$ or $1 \pmod 4$?",['discrete-mathematics']
1355394,"Show that $f^*\omega = \det(df) \, dx_1\wedge\cdots\wedge dx_n$","Let $f:\Bbb{R}^n\to \Bbb{R}^n$ be a differentiable map given my $f(x_1,\ldots,x_n)=(y_1,\ldots,y_n)$, and let $$\omega=dy_1\wedge\cdots\wedge dy_n.$$ Show that $$f^*\omega = \det(df) \, dx_1\wedge\cdots\wedge dx_n.$$ We can simplify the left hand side in the following way
$$\begin{align}
f^*\omega
&= f^*(dy_1)\wedge\cdots\wedge f^*dy_n
\\&=d(f^*y_1)\wedge\cdots\wedge d(f^*y_n)
\\&=d(y_1\circ f)\wedge\cdots\wedge d(y_n\circ f)
\end{align}$$ I'm honestly not sure how to proceed from there. The composition $y_i\circ f$ means first apply $f$ and then to $i$-th component function of $f$, right? I tried using the chain rule but it got confusing.","['differential-topology', 'differential-geometry', 'differential-forms']"
1355398,"Cantor-Bendixson rank of a closed countable subset of the reals, and scattered sets","I am reading the notes in the following link, and I am a bit unclear about the connection between scattered sets, countable sets, and sets for which the Cantor-Bendixson derivative is eventually the empty set. http://www.cs.man.ac.uk/~hsimmons/DOCUMENTS/PAPERSandNOTES/CB-examples.pdf On page 3, the author says: A closed set X (of the reals) is scattered if $X^{\alpha}=\emptyset$, where $\alpha$ is the Cantor-Bendixson rank of $X$. From what I could find, scattered means that any proper subset contains isolated points. However, for a countable subset of the reals, the Cantor-Bendixson-derivatives eventually vanish. As well, if I understand the definition of scattered correctly, a converging sequence together with its limit would be a closed, non-scattered set (the set with only the limit point in it will not have isolated points), but the CB-derivatives still eventually vanish. It seems to me that a closed set of the reals has vanishing CB-derivatives if and only if it is countable. Am I misreading  the definition of scattered ?","['descriptive-set-theory', 'ordinals', 'general-topology']"
1355400,Generalized Bessel Equation?,"This seems like a long shot, but is there any closed form solution to 
$$y''x^{2r} - {\frac{r}{2}}y'x^r - cyx^{\frac{5r}{2}} = 0 ?$$ Here, we can take $x>0,r>0$ if it helps. This sort of looks like some kind of generalized Bessel equation... but not quite?","['bessel-functions', 'ordinary-differential-equations']"
1355444,Proof Verification: Converse of Intermediate Value Theorem,"A function $f$ is increasing on $A$ if $f(x)\leq f(y)$ for all $x<y$ in $A$. Show that the Intermediate Value Theorem does have a converse if we assume $f$ is increasing on $[a,b]$. The converse of IVT along with the additional hypothesis would look like this: Lef $f:[a,b] \rightarrow \mathbb{R}$ be an increasing function on $[a,b]$ which satisfies: if $L$ is a real number such that $f(a)<L<f(b)$ (or $f(a)>L>f(b)$), then there exists a point $c\in (a,b)$ where $f(c)=L.$ If the preceding conditions are met, then $f$ is continuous on $[a,b]$. The proof (if it's correct) I'm going to show is only for the $c \in (a,b)$, but I think for end points ($a$ or $b$), the method should be similar. We want to show that given $c \in (a,b)$, for all $\epsilon>0$, there exists a $\delta>0$ such that $|x-c|<\delta \implies |f(x)-f(c)|<\epsilon$. Since $f$ is increasing, we know that $f(a)\leq f(c)$. If $f(c)-\epsilon<f(a)$, then set $x_1=a$, if $f(c)-\epsilon\geq f(a)$, then we know by hypothesis that a $x_1<c$ exists such that $f(x_1)=f(c)-\epsilon.$ In either case, we have for $x \in (x_1,c]$ $$ f(c)-\epsilon \leq f(x_1) \leq f(x) \leq f(c) .$$ In a similar way, since we know that $f(c)\leq f(b)$, we can deduce that a $x_2>c$ exists such that, for $x \in [c,x_2)$ $$f(c)\leq f(x)\leq f(x_2)\leq f(c)+\epsilon .$$ Pick $\delta = \min \{c-x_1,x_2-c \}$, we then have $$|x-c|<\delta \implies |f(x)-f(c)|<\epsilon .$$ Is this correct? In the proof that I saw, they originally used $\frac{\epsilon}{2}$, instead of just $\epsilon$, but I think this one works just as well, what do you guys think?","['analysis', 'proof-verification']"
1355460,A weaker form of Lebesgue's differentiation theorem in $\Bbb R ^n$,"If $f : \Bbb R ^n \to \Bbb C$ is locally-integrable then Lebesgue's differentiation theorem says that $$\lim \limits _{r \to 0} \frac 1 {\lambda \big( B(x, r) \big)} \int \limits _{B(x, r)} f \Bbb d \lambda = f(x) \tag{*}$$ almost everywhere. What happens if I want to study the set of those points $x$ where $\lim \limits _{r \to 0} \frac 1 {\lambda \big( B(x, r) \big)} \int \limits _{B(x, r)} f \Bbb d \lambda$ simply exists, without having to be equal to $f(x)$? Is it equal to the set of the points where $(*)$ holds, or can it be larger? Edit: The above question has been answered below, but I would like to complement it with the following closely related one: are there functions $f$ and points $x$ in which the above limit does not exist? I would like to understand whether in general this limit exists everywhere or only almost everywhere.","['lebesgue-measure', 'lebesgue-integral', 'measure-theory']"
1355467,"If $n = 4k + 1$, does $4$ divide $n^2 -1$?",How would I show that $4$ divides $n^2 -1$ if $n = 4k+1$? Is there more than one way to solve this?,"['discrete-mathematics', 'elementary-number-theory']"
1355472,How can I define a topology on the empty set?,"We know that, indiscrete topology is the smallest topology. It has $2$ elements (they are the empty set and whole set). Suppose the given set is the empty set, then how can I define a topology on that set? Is it possible?",['general-topology']
1355483,"Given matrices $A$ and $B$, how can I find a scalar $s$ that makes $A + s B$ rank-$1$?","Given $3 \times 3$ matrices $A$ and $B$ , how can I find a scalar $s$ that makes the matrix $A + s B$ rank- $1$ ? Is there a method using singular value decomposition or eigenvalues? Thanks!","['rank-1-matrices', 'matrices', 'linear-algebra', 'matrix-rank', 'svd']"
1355511,Probability that no two consecutive heads occur?,"A fair coin is tossed $10$ times. What is the probability that no two
  consecutive tosses are heads? Possibilities are (dont mind the number of terms): $H TTTTTTH$, $HTHTHTHTHTHTHT$. But except for those, let $y(n)$ be the number of sequences that start with $T$ $T _$, there are two options, $T$ and $H$ so, $y(n) = y(n - 1) + x(n-1) = y(n - 1) + y(n - 2)$ Let $x(n)$ be the number of sequences that start with $H$, $H _$ the next option is only $T$ hence, $x(n) = y(n - 1)$ The total number of sequences $F(n)$ is: $$F(n) = y(n) + x(n) = 2y(n - 1) + y(n - 2)$$ We are after $F(10)$, $$F(10) = 2y(9) + y(8)$$ $$F(3) = 2y(2) + y(1) = 1 + 4 = 5$$ $$F(4) = 2y(3) + y(2) = 6 + 2 = 8$$ But I'm not quite sure where this will take me though.","['contest-math', 'algebra-precalculus', 'sequences-and-series', 'combinatorics', 'probability']"
1355521,Difference of Ordered Uniform Random Variables,"Let $X_1, X_2,..., X_n$ be $n$ random variables distributed uniform(0,1) and $X_{(1)},X_{(2)},..., X_{(n)}$ be the ordered statistics of $X_1,...,X_n$ such that: $X_{(1)} < X_{(2)} < ... < X_{(n)}$ $X_{(1)} = min(X_1,...,X_n)$ $X_{(n)} = max(X_1,...,X_n)$ I know that these variables are distributed: $X_{(i)} \sim Beta(i, n+1-i)$ I am looking to find the distribution of the difference of consecutive ordered statistics: $Y_{i+1,i} = X_{(i+1)} - X_{(i)}$ in order to calculate the total probability: $p = P(Y_{2,1} < d_{2,1} \cap Y_{3,2} < d_{3,2} \cap ... \cap Y_{n,n-1} < d_{n,n-1})$ Where $d_{i+1,i}$ are some given distances This proof, Difference of order statistics in a sample of uniform random variables , suggests that the distribution of $Y_{i+1,i}$ is $Y_{i+1,i} \sim Beta(1,n)$ This suggests that the events in the probability, $p$, above are independent... is this true?","['order-statistics', 'probability-distributions', 'statistics', 'beta-function', 'probability']"
1355563,Linear Recurrence In Faster Time,"I am trying to solve this linear recurrence using matrix exponentiation:- $$f(n) = 2f(n-1) - f(n-2) + c,$$ where $c$ is a constant. What I have come up with is this - Let the matrix $M$ be $$
\begin{bmatrix}
2 & -1 & 1\\ 1 & 0 & 0\\ 0 & 0 & 1
\end{bmatrix}.
$$ Now, we have $$M^{n-4}\cdot\begin{bmatrix}f(4)\\f(3)\\c\end{bmatrix} = \begin{bmatrix}f(n)\\f(n-1)\\c\end{bmatrix}. $$ I have used this to solve the recurrence.
However, I was wondering: since there is a constant in the recurrence, is there any way to solve it using a $2\times 2$ matrix $M$ instead of the $3\times 3$ one which I have used? Any suggestions would be valued, thank you.","['recurrence-relations', 'contest-math', 'exponentiation', 'matrices']"
1355593,Is it possible to express $\left(\begin{array}{cc}a & -a \\ a-1 & 1-a \\ \end{array} \right)$ as a certain product of two matrices?,"Is it possible to express
$$\left(
          \begin{array}{cc}
            a & -a \\
            a-1 & 1-a \\
          \end{array}
        \right), \ \ \ \ a\in\mathbb R$$
as a certain product of two matrices? Namely,
$$\left(
          \begin{array}{cc}
            a & -a \\
            a-1 & 1-a \\
          \end{array}
        \right)=PQ$$
where $P$, $Q$ are two matrices with entries not all $1$. I tried to do some product but without success. Any suggestions please?","['linear-algebra', 'matrices']"
1355608,Bounded and closed but not compact in rational numbers,"I'm sorry if topic repeated. I solved this problem. And I want to know is my solution true? Regard $\mathbb{Q}$, the set of all rational numbers, as a metric space, with $d(p,q)=|p-q|$. Let $E=\{p\in \mathbb{Q}: 2<p^2<3\}$. Show that $E$ is closed and bounded in $\mathbb{Q}$, but $E$ is not compact. Is $E$ open in $\mathbb{Q}$? Proof: Boundness: It easy to check that $E$ is bounded. Because $E\subset B_2(0)=\{z\in \mathbb{Q}:d(z,0)<2\}$ - open ball in $\mathbb{Q}$. Closed: Also $E$ is closed because $E^c=\{p\in \mathbb{Q}: p^2<2\quad  \text{or} \quad p^2>3 \}$ is open set. Because if $z\in E^c$ then $z\in \mathbb{Q}\cap \{z: z^2<2\}$ or $z\in \mathbb{Q}\cap \{z: z^2>3\}$. And for both cases $\exists n\in \mathbb{N}$ s.t. $(z+1/n)^2<2$ or $(z-1/n)^2>3$ because both inequalities are solvable for large $n$. Taking $n$ so large we got $N_{1/n}(z)\subset E^c$ where $N_{1/n}(z)=\{q\in \mathbb{Q}: d(q,z)<1/n\}$. So $E^c$ is open in $\mathbb{Q}$. Therefore $E$ is closed in $\mathbb{Q}$. Compactness: We'll prove that $E$ is not compact in $\mathbb{Q}$. It means that exists some open cover of $E$ which contains no finite subcover. Let $$G_n=\{\mathbb{Q}\cap [-\sqrt{3}, -\sqrt{2}-\frac{1}{n}]\}\cup \{\mathbb{Q} \cap [\sqrt{2}, \sqrt{3}-\frac{1}{n}]\}$$ for $n\geqslant 4$. It's easy to verify that $\{G_n\}$ is an open of $E$. But it contains no finite subcover. Therefore, $E$ is not compact in $\mathbb{Q}$. Openess: Set $E$ is open. If $z\in E$ then $z\in \mathbb{Q}$ and $2<z^2<3$. Then exists $n$ so large s.t. $2<(z-1/n)^2<(z+1/n)^2<3$. Hence $N_{1/n}(z)\subset E$.","['real-analysis', 'general-topology']"
1355623,$\{∅\} ∈ \{∅\}$ is this right or wrong?,I am very confused about whether $\{∅\} ∈ \{∅\}$ or not. I thought it's right because in curly brackets the phy is also a member. Can anyone help me understand this?,"['elementary-set-theory', 'notation']"
1355696,"Proof of Liouville's formula , details and confusions. [Matrices, determinants..]","So I've got the homogeneous linear equation: $$x^{(n)}+a_1(t)x^{(n-1)}+...+a_{n-1}(t)x'+a_n(t)x=0.$$ where $a_1(t)...a_n(t)$ are real continuous on intervals. This is what my textbook states: If $x_1(t)...x_n(t)$ are solutions to the homogeneous equation and the Wronski matrix being $$W(t)= \begin{vmatrix} x_1 & x_2 & ... & x_n \\ x_1' & x_2' & ... & x_n' \\...&...&...& ...\\x_1^{(n-1)} & x_2^{(n-1)} & ... & x_n^{(n-1)}  \end{vmatrix} $$ Now we are trying to get Liouville's formula. $$W(t)=W(t_0)\exp({-\int_{t_0}^{t}a_1(c)dc})$$ Starts of by stating this (which I need clarification as to why,the yellow is unclear to me) Differentiating $W(t)$ we have: $$W'(t)=W_1'(t)+...+W_n'(t).$$ Where
 $W_i(t)$ is the determinant that is formed when from $W(t)$ this
 $i^{th}$ row is differentiated, if we notice: $W_i'(t)=0,  i=1,2,3...,n-1$ We get $$W'(t)=W_n'(t)$$","['calculus', 'matrices', 'linear-algebra', 'ordinary-differential-equations', 'derivatives']"
1355705,What does it mean to convolve matrices of finite dimension?,"If one is given two matrices $I$ and $K$ what does the notation: $$ I * K $$ mean rigorously/precisely? I do know the definition of convolution: $$ s[i, j] = (I * K)[i, j] = \sum_m \sum_n I[m,n] K[i - m, j - n]$$ However, I have some issues/doubts that I can't resolve between the two. First is that the definition of the convolution is in terms of functions rather than matrices or vectors. So I am having some difficulty understanding what $ I * K $ should mean if I am given two matrices alone. This seems to be common in convolutional neural networks for example and its confusing me. I am trying to come up with an example to make this concrete and not confusing. For example, if I had:
$$
I=
  \begin{bmatrix}
    1 & 2 & 3 & 4 \\
    5 & 6 & 7 & 8 \\
    9 & 10 & 11 & 12 \\
  \end{bmatrix}
$$ and I had $$
K=
  \begin{bmatrix}
    1 & 2  \\
    5 & 6  
  \end{bmatrix}
$$ what would a convolution between the two mean? What type of mathematical object should it return? Would it be using the usual equation for convolution as I defined above with $s[i,j]$? Some of the issue I have with this is that the negative sign in the convolution is suppose to reflect around the y-axis, but with finite structures I don't think that makes sense. There is also a translation and the indices are discrete and I am not sure what translating means in this context. Just making cyclic shifts doesn't make sense to me as translation. All of those make sense with functions (and infinite dimensional vectors) to me, but I am not sure it makes sense to me in this context. Actually, after some though, I think one of the things that bother me the most about convolutions with finite vectors (that is NOT an issue with inner products) is this translation thing. The thing is that for example, for finite vectors, translating doesn't make sense to me because they are not indexed on a real line but with inner products, this issue never arises because we only compute products of corresponding elements! Also, if anyone can make explicit what the result would be of $ I * K$ with the specific example I provided, it would be very helpful. I suspect that the result would a matrix, but if one can show what the matrix is (and possible a few of its entries explicitly computed, it would be tremendously useful). For reference, here is where I saw this type of operations/notation being used: http://people.idsia.ch/~masci/papers/2011_icann.pdf and the section that uses this type of notation:","['machine-learning', 'convolution', 'functional-analysis', 'functions']"
1355710,"If you know the slope of a line and the angle between them, can you find the slope of the second line?","The two lines intersect at (1,4) and the slope of the first line is 3/5. The second line makes a 45 degree angle with the first clockwise so that the second lines slope must be less but I don't know how to find it","['geometry', 'algebra-precalculus']"
1355720,What is the name for a function whose codomain and domain are equal?,"What do we call a function whose domain and co-domain are the same set? Edit:
While i expressed my question in terms of functions, domains and codomains, i was actually interested in the most abstract mathematical formulation for similar structures. Thus i accepted as the correct answer endomorphism that, being at the level of category theory, answers rather the question: What do we call a morphism from a mathematical object to itself? I could find this out just after getting some answers","['abstract-algebra', 'terminology', 'category-theory', 'functions']"
1355722,When is the union of $\sigma$-algebras atomless?,"Suppose that we are given a probability space $(\Omega, \mathcal{F}, \mathsf P)$ and an increasing sequence of $$\mathcal{F}_1\subset \ldots \subset\mathcal{F}_n\subset \mathcal{F}_{n+1} \subset \ldots \subset \mathcal{F}$$
of $\sigma$-algebras. Assume that $\mathsf P|_{\mathcal{F}_n}$ is atomless for each $n$. Is so $\mathsf P$ restricted to the $\sigma$-algebra generated by the union of $\mathcal{F}_n$ ($n\in \mathbb{N}$)?","['probability-theory', 'measure-theory']"
1355730,Proof that $\Bbb R/\Bbb Z$ is isomorphic to $S^1$,"I just learned the first isomorphism theorem for groups and now I need to prove that: $$\Bbb R/\Bbb Z \cong S^1$$ In other words, the quotient group of $\Bbb Z$ in $\Bbb R$ is isomorphic to the $S^1$ (the group of all permutations of $1$ elements, I guess). I'll write what I understand about the first isomorphism theorem for groups: If we have $\phi: G\to G'$ as an homomorphsim, then the quotient group $$G/\ker(G)\cong G'$$ and the isomorphism if given by $\phi'$ which I forgot exactly how it's given. So, if I want to prove that $$\Bbb R/\Bbb Z  \cong S^1$$ using the first isomorphism theorem, I would need to somehow get $\Bbb Z$ to be the kernel of a homomorphism between $\Bbb R$ and $S_1$? Because the exercise does not give any information about which is the homomorphism between the two sets, so I'm thinking about creating one. Am I doing the rigth thing?","['abstract-algebra', 'group-theory']"
1355760,"Every closed subspace of ${\scr C}^0[a,b]$ of continuously differentiable functions must have finite dimension.","If $F \subset {\scr C}^1[a,b] \subset {\scr C}^0[a,b]$, then $\dim F < +\infty,$ where $F$ is a closed subspace (in $ {\scr C}^0[a,b]$). I found this answer , which is very good and solves the problem, but how can we prove the assertion without using Ascoli-Arzelà in the end? We didn't cover equicontinuity, etc. Is there a more elementary proof of the result?","['analysis', 'banach-spaces', 'functional-analysis']"
1355769,Find the Laplace inverse of the following.,"$$
\frac{2s+5}{s^2+6s+34}
$$ I am stuck on this part: Wolfram has the step by step showing that you simply split up the original fraction into $$
\frac{2s}{s^2+6s+34} + \frac{5}{s^2+6s+34}
$$ and then it solves it. But that doesn't help. Could someone please help me understand how to do this problem?","['laplace-transform', 'ordinary-differential-equations']"
1355776,Does Euler's $\phi$ function have the same value in arbitrarily large subsets of $\mathbb{N}$?,"As my most recent question still does not have any answers and it appears to be a difficult problem, I propose the following problem (that seems easier), but which I still could not manage to solve: Is it true that for every $k \in \mathbb{N}$ there exists distinct natural numbers $x_1, \cdots, x_k$ such that $\phi(x_1)=\phi(x_2)=\cdots=\phi(x_k)$, where $\phi$ is the Euler's totient function? Any ideas? Thank you!","['totient-function', 'number-theory', 'elementary-number-theory', 'combinatorics']"
1355837,"Find an equivalent of this function,","a) $f$ continuous on $[0,1]$ such that $f(x)>0$.  Find an equivalent of $$h(\epsilon) = \int_0^1 \frac {f(x)}{x^2 + \epsilon^2}dx$$ when $\epsilon$ goes to zero and when $\epsilon$ goes to infinity. b) Assume $f$ is differentiable at $0$, that $f(0) = 0$ and $f'(0) \ne 0$. Find an equivalent of $h(\epsilon)$, when $\epsilon$ goes to zero. Edit:  I think I am satisfied with my answer to part(a).  So, if anyone can help with part(b), that would be great :-) Thanks,","['derivatives', 'limits', 'real-analysis', 'integration']"
1355849,Size of Totally Ordered Set with Countable Predecessors,"Assume Choice. Let $S$ be a set, and $\trianglelefteq$ be a total order on $S$. If for all $s \in S$, the set $\{t:t\trianglelefteq s\}$ is countable, what are the possible cardinalities of $S$? Obviously, any cardinality $\le \omega_1$ has such a total ordering.","['elementary-set-theory', 'order-theory']"
1355865,Is this a Morera´s Theorem Application?,"Let $G \subset \mathbb C$ be a domain and $f: G \to \mathbb C$ a continuous function such that for any closed and rectifiable path $\gamma \subset G$, 
$$
\left| \oint_\gamma f(z)dz \right|\leq \left( L(\gamma)\right)^3.
$$
Where $L(\gamma)$ denote the arc length of the path $\gamma$, that is $L(\gamma)=\int_\gamma |dz|$. Prove that $f$ is holomorphic in $G$. My Ideas: This looks like a good candidate to use the powerful result of Morera´s Theorem, which tells us that is enough to prove that $\oint_\gamma f(z)dz$ vanishes for any closed path $\gamma \subset G$. However I don't seem to put together a good argument and I am starting to believe this may be prove in another way. Why 3? Why use the $3$ in ""$\leq \left( L(\gamma)\right)^3$""? I guess I would know that if I could prove it but do the assertion still holds if we replace $3$ by $2$, $5$, $8$ or any other whole number? Any help on how proving this or hints will be very appreciated.","['contour-integration', 'analyticity', 'complex-analysis']"
1355886,"Prove that if Diagonal is open in Product Topology, then the original topology is discrete","I found the following exercise in Introduction to Metric and Topological Spaces by Sutherland (Chapter 10 Question 20). Prove that the topology on a space X is discrete iff the diagonal $\Delta=\{ (x,x) \mid x\in X\}$ is open in the topological product $X \times X$. I believe I could prove in the $implies$ direction. It is the converse that got me stuck. So,I would want to prove that if the diagonal is open in the topological product then the topology on $X$ must be discrete. I was thinking that I could achieve the result if I could show that every singleton $\{ x\}$ is open in $X$, but I couldn't think of a way to establish this. I tried considering $X-\{ x\}$ and try to show that it's closed but  I couldn't continue to anything fruitful. Also, I think I could use projection maps but I am stumped as well. Any help/hint is appreciated. Thanks in advance.",['general-topology']
1355895,How many triangles exist whose angles are rational and side lengths are roots to quadratic equations?,"By ""rational angles"" I mean a rational degree measure (equivalently, angle a rational multiple of $\pi$). Obviously similar triangles should be counted once. Off the top of my head we have: 30-60-90, equilateral triangles, 45-45-90, and 36-36-72.","['geometry', 'trigonometry']"
1355921,problem of a positive definite matrix,"Let $H$ be a positive definite matrix and $I$ the identity matrix. If $k_1,k_2>0$, can we conclude that $k_1H-k_2I$ is positive definite if $k_1\gg k_2$?","['positive-characteristic', 'matrix-equations', 'linear-algebra', 'matrices']"
1355959,Minimal free resolution of the twisted cubic,"This is exercise 13.15 in Harris' book ""A First Course..."". Let $X$ be the twisted cubic with ideal $I(X) = (XZ-Y^2,YW-Z^2,XZ-YW).$ Let $S(X)$ denote the homogeneous coordinate ring of $X$ and $S$ the homogeneous coordinate ring $k[X,Y,Z,W]$ of $\mathbb{P}^3$ (this is Harris' notation). The goal is to compute the Hilbert polynomial of the twisted cubic by producing a minimal free resolution of $S(X)$ . I know the polynomial is supposed to be $h_X(m) = 3m+1$ , so I know I am constructing the wrong resolution. Here is what I have: The polynomials generating the ideal of $X$ are all degree 2. We thus have an exact sequence of graded $S$ -modules $$ S(-2)^3 \to S \to S(X) \to 0$$ where $S(d)$ denotes the twist of $S$ by the integer $d$ . We know the kernel of the first map is generated by the relations that $XZ-Y^2$ , $YW-Z^2$ , $XW-YZ$ satisfy. I think I am having trouble producing these relations. For convenience put $$F_1 = XZ-Y^2 \qquad F_2=YW-Z^2 \qquad F_3 = XZ-YW.$$ The only relations I can think of are the tautological ones $F_iF_j = F_jF_i$ , i.e. the matrices $A=(F_2,-F_1,0)$ , $B=(F_3,0,-F_1)$ , $C=(0,F_3,-F2)$ , thought of as maps $S^3 \to S^3$ . Following Harris, this would give the exact sequence $$ S(-4)^3 \to S(-2)^3 \to S \to S(X) \to 0.$$ Then the matrices $A$ , $B$ , and $C$ satisfy the relation $-F_3A + F_2B = F_1C$ , i.e. the matrix $(-F_3,F_2,F_1)$ . This is the only relation I can think of, so I guess the kernel is a free module, so we have the exact sequence $$0 \to S(-6) \to S(-4)^3 \to S(-2)^3 \to S \to S(X) \to 0.$$ By p. 170 in Harris we should have $$h_X(m) = {m+3 \choose 3} - 3 {m+3-2 \choose 3} + 3 {m+3-4 \choose 3} - {m+3-6 \choose 3} = 8.$$ This is obviously wrong, so my free resolution is wrong.","['abstract-algebra', 'algebraic-geometry', 'graded-modules', 'commutative-algebra']"
1355963,Sequence of polynomials converging to a discontinuous function in $\mathbb{C}$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Is there a sequence of polynomials $P_n $ such that $\displaystyle\lim_{n\rightarrow\infty} P_n (z) $ exists everywhere in $\mathbb{C}$ and equals to $1$ if $\text{Im} (z)>0$, $0$ if $\text{Im} (z) = 0$ and $-1$ if $\text{Im} (z) < 0$? Intuitively, I guess such polynomial doesn't exist, but I have no idea how to prove or disprove it..","['analysis', 'polynomials', 'complex-analysis']"
1355970,a vector calculus problem when coping with problem 9 chapter 5 evans,"Hi I was trying to understand the solution given by Ray Yang in the post Question $5.9$ - Evans PDE $2$nd edition It gets to the sort of things I am quite bad at...
When I get to the point
$$\int_{\Omega}|Du|^p\,dx=-\sum_{i=1}^n\int_{\Omega}u\left[(\frac{\partial^2 u}{\partial x_i^2}|Du|^{p-2})+\frac{\partial}{\partial x_i}(u)\frac{\partial}{\partial x_i}|Du|^{p-2}\right]dx,$$
I deal with the claim,
$$ \frac{\partial}{\partial x_i}|Du|^{p-2}=(p-2)|Du|^{p-4}\sum_j\frac{\partial^2 u}{\partial x_i\partial x_j}\frac{\partial u}{\partial x_j},$$ I feel I start making mistakes, what I did was…I have
$$ \frac{\partial}{\partial x_i}|Du|^{p-2}=(p-2)|Du|^{p-3}\frac{\partial}{\partial x_i}(|Du|)$$,
Treat $Du$ as y, I have
$$(p-2)|Du|^{p-3}\frac{Du}{|Du|}\frac{\partial}{\partial x_i}(Du)$$, What I could have done wrong? Next, putting things together to get the second term, could anyone show me how to get from
$$ -\sum_{i=1}^n\int_{\Omega} u\frac{\partial u}{\partial x_i}\frac{\partial}{\partial x_i}|Du|^{p-2} dx$$
to 
$$-\int_{\Omega}(p-2)(\nabla u^T\cdot D^2 u\nabla u)|Du|^{p-4}?$$, for this term, what I have was
$$=-\sum_{i=1}^n\int_{\Omega} u\frac{\partial u}{\partial x_i}(p-2)|Du|^{p-4} Du\frac{\partial}{\partial x_i}(Du).$$ I think my $\frac{\partial u}{\partial x_i}$ corresponds to $\nabla u^T$, $\frac{\partial}{\partial x_i}(Du)$ corresponds to $D^2 u$, $Du$ corresponds $\nabla u$ in the solution posted. Could anyone help?","['partial-differential-equations', 'sobolev-spaces', 'multivariable-calculus', 'integration']"
1355973,Inequality involving number of binary Lyndon words of length $n$ and $n+1$,"Let $f(n)$ be the number of binary Lyndon words of length $n$ .  This sequence is given by OEIS entry A001037 . Is it true that $2f(n) \ge f(n+1)$ for all positive $n$ ? I have found a general formula to calculate $f(n)$ : $$
  f(n) = \frac{1}{n} \sum_{d \mid n} \mu(n/d) 2^d.
$$ Symmetry also allows us to rewrite it another way: $$
  f(n) = \frac{1}{n} \sum_{d \mid n} \mu(d) 2^{n/d}.
$$ Here $\mu(x)$ is the Möbius function , and $n$ is a positive integer (the case $n=0$ can be handled separately). It is possible to substitute these identities into the inequality above and perform some simplifications, but it does not seem to go anywhere. Any help is appreciated. EDIT: My original formula was incorrect, I forgot the factor of $1/n$ in front of the summation.","['sequences-and-series', 'combinatorics', 'necklace-and-bracelets']"
1355974,For which values does this series converge? [duplicate],"This question already has answers here : Convergence of double sum $\sum_{m, n}\frac{1}{m^p + n^k}$ (4 answers) Closed 8 months ago . p and k are real numbers.  For which values of p and k does the following double series converge $$\sum_{n,m=1}^\infty \frac{1}{n^p + m^k}$$ I am trying to find a better (and quicker) way to solve this problem. I'm trying to use RRL's hints (see below) to prove boundedness of the partial sums. Edit:  I was able to figure out the solution with the integral test method and think that I will move on to new problems now.  But if anyone would like to post a solution that doesn't use the integral test and p-tests, that would be interesting to see - thanks in advance :-) Thanks,","['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence', 'multivariable-calculus']"
1355975,Closed form expression for products,How can I find a closed form expression for products of the following form $$\prod_{k=1}^n (ak^2+bk+c)\space \text{?}$$,"['number-theory', 'products']"
1355976,Cancellation problem: $R\not\cong S$ but $R[t]\cong S[t]$ (Danielewski surfaces),"I would like to understand why the two rings
$$
R={\mathbb{C}[x,y,z]}/{(xy - (1 - z^2))} \\ S=\mathbb{C}[x,y,z]/{(x^2y - (1 - z^2))}
$$
are not isomorphic, but $R[t]\cong S[t]$. This example is discussed in this document by Hochster. He references a paper by Danielewski, but it is a preprint from 1989, so I could not locate it anywhere. In fact, this example was also given by Timothy Wagner in this MSE thread , but no proof was supplied. Finally, this MSE thread seems to refer to this problem (though possibly with different formulation), but unfortunately it hasn't even received a single comment in the last 3.5 years. Can someone either find a precise reference (preferably online, or at least published in some book/journal) or supply a proof for this claim?","['ring-theory', 'commutative-algebra', 'abstract-algebra', 'algebraic-geometry', 'reference-request']"
1355981,Statistical Significance,"I'm attempting to compute statistical significance for two data sets. I have the mean and the number of data points, but I don't think I can compute std deviation, because I don't have the individual data points. Under previous teacher, 1,072 tests given with average score of 27.7 correct. New teacher, 179 tests given with average score of 28.6. Seems self-evident to me that combination of small uptick and tiny sample size means that conclusion ""new teacher responsible for improvement"" is untenable, but I'd like to be able to say that the improvement is not statistically significant. All help appreciated.",['statistics']
1355996,"Exercise 1 on page 10 in Naive Set Theory, following Axiom of Pairing","In Section 3 ""Axiom of Pairing"" in Naive Set Theory on page 10 Halmos proposes the following argument. ""Consider the sets $ \emptyset , \lbrace \emptyset \rbrace , \lbrace \lbrace \emptyset \rbrace \rbrace , \lbrace \lbrace \lbrace \emptyset \rbrace \rbrace \rbrace , $ etc.; consider the pairs such as $ \lbrace \emptyset , \lbrace \emptyset \rbrace \rbrace , $ formed by any two of them; Consider the pairs formed any two such pairs, or else the mixed pairs formed by any singleton and any pair; and proceed so on ad infinitum. Exercise Are all the sets obtained in this way distinct from one another? I really didn't get what he asks. Does he ask question about the first sequence only, then about the second sequence of pairs and so on or does he ask about any possible mix of sets of empty sets? Can anyone elucidate this exercise for me with suggested justification or hint how to show that they are distinct?",['elementary-set-theory']
1356001,What is the significance of the integral of the Hessian determinant?,The integral of a function over some region measures the total value of the function in that region: $$T(u)=\int u\thinspace\mathrm{d}V$$ The integral of the squared norm of the gradient of the function measures its variability over some region and is known as the Dirichlet energy : $$E(u)=\int \lvert\nabla u\rvert^2\thinspace\mathrm{d}V$$ Is there a geometric significance to the integral of the determinant of the Hessian matrix (the square matrix of second-order partial derivatives) of a function over some region? $$\int\det H(u)\thinspace\mathrm{d}V$$ What other integral functionals of a similar form provide interesting information about the behavior of a function over some region?,"['functional-analysis', 'differential-operators', 'functional-calculus', 'calculus-of-variations', 'multivariable-calculus']"
1356016,"Intuition of joint density of min(X,Y) and max(X,Y)","The problem is to find the joint density of $U = min(X,Y)$ and $V=max(X,Y)$ when both are exponential random variables. The solution to it is: I can finish it after the first step but I don't understand the intuition behind $$P(U \le u, V \le v) = P(V\le v)-P(U>u, V\le v) $$ I understand that it's because $P(A\cap B) = P(B) -  P(A^c\cap B)$ but what would make you think that it's a good idea to start off by rewriting it like this?","['probability', 'probability-distributions']"
1356027,Permutations without repetitions (exclude repeated permutations) [duplicate],"This question already has answers here : Permutations of a string with duplicate characters (3 answers) Closed 8 years ago . The formula to calculate all permutations without repetitions of the set {1,2,3} is $\dfrac{n!}{(n-r)!}$ But how to calculate it if the set (or rather array in programming terms) includes repeated numbers {1,2,2,3,3} so that you don't add up same permutations?","['combinatorics', 'permutations']"
1356045,"Show that if n+1 integers are choosen from set $\{1,2,3,...,2n\}$ ,then there are always two which differ by 1","Considering n=5 i have 
$\{1,2,3,...,10\}$ .Making pairs such as $\{1,2\}$ ,$\{2,3\}$ ... total of $9$ pairs  which are my holes and $6$ numbers are to be choosen which are pigeons .So one hole must have two pigeons ,i am done .Is this correct ? In general if we have $2n$ numbers and then we have $2n-1$ holes and $n+1$ pigeons ,But for $n=3$ onwards it is valid ? There  is something wrong i think Thanks","['discrete-mathematics', 'combinatorics']"
1356066,Global structure of the Gromov-Hausdorff space,"EDIT: now crossposted at mathoverflow ( https://mathoverflow.net/questions/212364/on-the-global-structure-of-the-gromov-hausdorff-metric-space ) This is a purely idle question, which emerged during a conversation with a friend about what is (not) known about the space of compact metric spaces. Background. If $A, B$ are compact subsets of a metric space $M$ , the Hausdorff distance between $A$ and $B$ is the worst worst best distance between their points: $$d_H(A, B)=\max\{\sup\{\inf\{d(a, b): b\in B\}: a\in A\}, \sup\{\inf\{d(a, b): a\in A\}: b\in B\}\}.$$ For two compact metric spcaes $X, Y$ , their Gromov-Hausdorff distance is the infimum (in fact, minimum) over all isometric embeddings of $X, Y$ into $Z$ via $f, g$ of $d_H(f(X), g(Y))$ . The Gromov-Hausdorff space $\mathcal{GH}$ is then the ""set"" of isometry classes of compact metric spaces, with the metric $d_{GH}$ .) Question. How homogeneous is $\mathcal{GH}$ ? For example: while distinct points in $\mathcal{GH}$ are in fact distinguishable in a broad sense, it's not clear that distinct points can always be distinguished just by the metric structure of $\mathcal{GH}$ , so it's reasonable to ask: Are there any nontrivial isometries of $\mathcal{GH}$ ? There are of course lots of related questions one can ask (e.g., homeomorphisms instead of isometries); I'm happy for any information about the homogeneity of $\mathcal{GH}$ . Please feel free to add tags; I couldn't think what to label this question.","['metric-spaces', 'general-topology']"
1356069,Two different definitions of a Liouville measure,"Ok, I'm currently confused because of two different definitions for the Liouville measure associated to a smooth manifold $M$ of dimension $n$. These are: a) The measure $\mu$ on the cotangent bundle $T^*M$ induced by the volume form $(d\alpha)^n$ where $\alpha$ is the canonical 1-form on $T^*M$ and $\omega:=d\alpha$ makes $T^*M$ a symplectic manifold. This $\mu$ is invariant under any Hamiltonian flow (that is, any flow that integrates a vector field $X_H$ defined for some smooth $H:T^*M\to \mathbb{R}$ by $\theta(X_H)=\omega(\theta,dH)$ for all $\theta\in T^*(T^*M)$ ). b) The measure $\nu$ that may be defined, once $M$ is endowed with a Riemannian metric $g$, on $SM$, the unit tangent bundle. This measure is given locally by the product of the Riemannian volume on $M$ (i.e. $\text{det}(g_{ij})dx_1\wedge\dots\wedge dx_n$) and the usual Lebesgue measure on the unit sphere. This $\nu$ is invariant under the geodesic flow on $SM$. Now, what is, if any, the relationship between these measures?
I imagine that once $M$ is endowed with $g$ we can identify $T^*M$ with $TM$ ($q,p\to q,v$ iff $p(v')=g(v,v')$ for al $v'\in T_qM$). Will then the restriction of $\mu$ to the unit cotangent bundle (assuming this restriction to a submanifold of smaller dimension is well defined) correspond to $\nu$? It's the only reason I could imagine for giving the same name to two different measures defined on two different spaces ($T^*M$ and $SM$, respectively), but I could not find such a statement on any reference book.","['symplectic-geometry', 'differential-geometry', 'riemannian-geometry']"
1356075,Point inside a triangle,"Let $\triangle ABC$ be an acute triangle and let $P$ be a point inside of it. $PD$, $PE$ and $PF$ are respectively parallel to sides $AC$, $BC$ and $AB$. Prove that $$\frac{|BE|}{|AB|}+\frac{|CD|}{|BC|}+\frac{|AF|}{|AC|}=1$$ Could someone give me a hint? I already extended $PD$, $PE$ and $PF$ and wrote down all the similar triangles and the ratio's, but that doesn't seem to help..",['geometry']
1356077,"When solving $\tan(3x) = \cot(4x)$, how to formulate the answer?","when I solve the following equation: $\tan(3x) = \cot(4x)$ I get the following solution: $x = \frac{\pi}{14} + \frac{\pi n}{7}, n \in \mathbb{Z}$ But as x must be $\neq \frac{\pi}{6} + \frac{\pi k}{3}$ and $\neq \frac{\pi k}{4}$, with $k \in \mathbb{Z}$, there are values for n for which $x = \frac{\pi}{14} + \frac{\pi n}{7}, n \in \mathbb{Z}$ gives wrong solutions, e.g.: for
$n = 3, x = \frac{\pi}{14} + \frac{3\pi}{7} = \frac{7\pi}{14} = \frac{\pi}{2}$ Which is not a solution because $\tan(3(\frac{\pi}{2}))$ is not defined (cosine is 0) So how do I merge the solution $x = \frac{\pi}{14} + \frac{\pi n}{7}, n \in \mathbb{Z}$ with the domain? What is the algorithm? Or, can I just write: $x \in \{\frac{\pi}{14} + \frac{\pi n}{7}: n \in \mathbb{Z}\} \setminus \{\frac{\pi}{6} + \frac{\pi k}{3} : k \in \mathbb{Z} \} \setminus \{\frac{\pi k}{4} :k \in \mathbb{Z}\}$ Which is a bit ugly, I think. What is the right way to do that? Thanks for the attention!","['algebra-precalculus', 'trigonometry']"
1356089,Interchanging order of summation mechanically,"How can I interchange order of summation mechanically, without thinking? For instance, I had to interchange the sums below (assume $i$ is a constant where $i\gt 0$). $$\sum_{n\ge 1}\sum_{i\lt k \lt n} a_{nk}$$ I wrote down a matrix, and read it by columns instead of rows, arrived at: $$\sum_{k\gt i}\sum_{n\gt k} a_{nk}$$ Which I think it's correct. But there must be a way to do it mechanically, without writing down the matrix. By mechanically, I mean something like what happens with this sum, where I can make the steps without thinking: $$\sum_{n\gt k}\frac{1}{n} z^n = \sum_{n-k\gt k-k}\frac{1}{n} z^n $$
$$m=n-k\implies n=m+k$$
$$\sum_{n-k\gt 0}\frac{1}{n} z^n = \sum_{m\gt 0}\frac{1}{m+k} z^{m+k}$$ No thinking required here, just algebra. Concrete Mathematics calls my example a ""rocky road"" double summation and suggests to use Iverson notation, but I haven't been able to apply it to my case.","['summation', 'discrete-mathematics']"
1356094,Spectral theorem for compact and self-adjoint operators,"I am looking at the proof of this theorem which states that if $H$ is a separable Hilbert space and $A:H\rightarrow H$ a compact self-adjoint operator, then there exists a sequence of real eigenvalues $(\lambda_n)$ converging to $0$, and an orthonormal basis $\{v_n\}$ of eigenvectors with $Av_n=\lambda_nv_n$ for all $n\geq1$. After he found that $V=\langle e_1,...\rangle$ and that $A\mid_{V^{\perp}}=0$, he uses the separability condition to choose an orthonormal basis for $V^{\perp}$ ""(which might be zero, in which case the theorem is already proved, or might be finite dimensional)"" and conclude saying that listing the orthonormal basis for $V^{\perp}$ with the one constructed of $V$ Proves the theorem. I think that it should be fine till now. The point is that as a remark he states that separability is used for making use of orthonormal basis. How would be the idea for the conclusion without the separability condition?","['spectral-theory', 'hilbert-spaces', 'functional-analysis']"
1356095,Functions that are their own inverse.,"What are the functions that are their own inverse? (thus functions where $ f(f(x)) = x $ for a large domain) I always thought there were only 4: $f(x) = x , f(x) = -x , f(x) = \frac {1}{x} $ and $ f(x) = \frac {-1}{x} $ Later I heard about a fifth one $$f(x) = \ln\left(\frac {e^x+1}{e^x-1}\right) $$ (for $x > 0$) This made me wonder are there more? What are conditions that apply to all these functions to get more, etc.","['inverse', 'algebra-precalculus', 'functions']"
1356097,Mapping sphere surface to a vector space such that distances are preserved?,"I have a unit radius sphere (say in 3D) centered on the origin. Thus the shortest distance between two points on the sphere is the geodesic. Is there a transformation (linear or non-linear) on the points on the sphere to a higher-dimensional space (or lower if it exists), such that the distance between the points on the sphere is preserved in the transformed space as well. To make it more rigorous: let $x_1$ and $x_2$ be the two points on the sphere. Let $y_1=T(x_1)$ and $y_2=T(x_2)$ be the transformed points. Let $d_s(x_1,x_2)$ be the shortest distance between $x_1$ and $x_2$ on the sphere's surface. Is there any T(.) such that $$d_s(x_1,x_2)=||y_1-y_2||_2^2$$ (the Euclidean distance) in the transformed space holds. Any reference to relevant research or literature is welcome as well. Any engineering approaches are welcome too.","['matrices', 'functional-analysis', 'geometry', 'differential-geometry', 'linear-algebra']"
1356104,History of inner products and texts on it?,"Where does the inner product originate from, was it defined in term of the dual or was it defined from just two copies of the space? I.e $(*,*) : V \times V \rightarrow  scalar $ or $(*,*) : V \times V^{*} \rightarrow  scalar $ and where can one find texts regarding this?","['math-history', 'inner-products', 'functional-analysis']"
1356109,Solution to system of linear ODE's,"Let $\Delta_n$ be  the closed unit simplex in $\mathbb R^n$. For any $a,b \in \Delta_n$, define the differential equation:
$$
a'(u) = b-a(u) \quad\quad\quad a(0) = a
$$ How does one go about solving this system of ODEs? Solving the case where $n=1$ is simple, does this carry over to the $n$ dimensional case? I've looked at using eigen values but is there a simpler way when dealing with a large choice of $n$ ? Any hints appreciated Attempt for the Matrix Differential Equation:
$$
\frac{dx}{dt} = Ax + f(t), \quad\quad\quad x(0) = x_0
$$
where $A$ is a $n\times n$ matrix, $x\in \mathbb R^n$ the solution is given by: $$
x(t) = e^{tA} \int_0^te^{-sA}f(s)ds + e^{tA}x_0
$$ Using the fact that for a diagonal matrix : $D=\text{diag}(a_1,...,a_n)$, $e^D = \text{diag}(e^{a_1},....e^{a_n})$ then for my problem: $$
\frac{da}{dt} = \text{diag}(-1,...-1)a(u) + b
$$
then
\begin{align*}
a(t) &= \text{diag}(e^{-t},...,e^{-t}) \int_0^t \text{diag}(e^{-s},...,e^{-s})~ b~ ds + \text{diag}(e^{-t},...,e^{-t}) a\\
&=\text{diag}(e^{-t},...,e^{-t})\text{diag}(1-e^{-t},...1-e^{-t}) b +\text{diag}(e^{-t},...e^{-t}) a\\
&=\text{diag}(e^{-t}-e^{-2t},...e^{-t}-e^{-2t}) b + \text{diag}(e^{-t},...e^{-t}) a\\
\end{align*}
$$
   =\begin{pmatrix}
        e^{-t}(a_1(0)+b_1) -b_1e^{-2t}   \\
          \vdots\\
           \vdots \\
         e^{-t}(a_n(0)+b_n) -b_ne^{-2t} \\
        \end{pmatrix}
$$ Is this correct?","['systems-of-equations', 'multivariable-calculus', 'ordinary-differential-equations', 'matrix-calculus']"
1356124,Differential equation for finding closest point on surface.,Inspired by this question I got to think about a more general case. Say I have any discretized surface and want to find closest point from each point outside of surface to the surface. Say that I can estimate a local tensor for the local tangent space and normal space. Can I create a differential equation to encode with a vector field which optimally points out trajectories to go from each point and end up on a point on the surface of the object which would be the closest point?,"['optimization', 'differential-geometry', 'numerical-linear-algebra', 'ordinary-differential-equations']"
1356195,Special Case of Composite mersenne number mod p,"We want to investigate if a composite mersenne number $p|2^{qb}-1$ where $p\nmid2^q-1$ ,$q,p$ are primes, $p=1+6qb,\ qb\equiv1(mod64) $ and $b$ is an odd number. In general for 
$$\begin{align*}
x^n\equiv1&\pmod p\\\
\end{align*}$$ The distinct solutions are
$$
g^{j(p-1)/d},\ \text{for}\ j=0,1,2\ldots,d-1.
$$
where $d=\gcd(n,p-1)$ and there are $d$ solutions. Applying this to our special case $x=2^q,n=b \  $we get the number of solutions as $d=\gcd(n,p-1)=gcd(b,6qb)=b$ and the solutions are 
$$
g^{j6q},\ \text{for}\ j=1,2\ldots,b-1.
$$
Finally we get $2^q\equiv\ g^{j6q}(mod \ 1+6qb)\\$ This is how far we got can anybody help?
Note: We know If $p|2^{qb}-1$ then $p^2|2^{qb}-1$, so $p^2|(2^{qb}-1)/(2^{q}-1)$","['number-theory', 'elementary-number-theory']"
1356198,Convergence of series of nested sequence,"Let $a_n =\underbrace{\sin \left ( \sin \left ( \sin \cdots (\sin x) \cdots \right ) \right )}_{n \; \rm {times}}, \; \; x \in (0, \pi/2)$. Examine if the series: $$S=\sum_{n=1}^{\infty} a_n$$ converges. I don't know how to tackle this.It seems very difficult to me!","['sequences-and-series', 'real-analysis']"
1356226,"Prove that $x\ge n(n-1)(n-3)/8$, where $x$ is the number of $4$-cycles in a graph on $n$ vertices with at least $\frac12\binom{n}{2}$ edges.","Prove that $x\ge \dfrac{n(n-1)(n-3)}8$ , where $x$ is the number of $4$ -cycles in a graph on $n$ vertices with at least $\displaystyle\frac12\,\binom{n}{2}$ edges. If there are no $4$ -cycles in a graph the edges in $G$ , then the number of edges $e(G)$ of $G$ satisfies $$e(G)\le\frac n4(\sqrt{4n-3}+1)\,.$$ But if $e(G)\ge\displaystyle\frac12 {n \choose 2}$ , we have to show that the number of $4$ -cycles $x$ satisfies $$x \ge \dfrac{n(n-1)(n-3)}8\,.$$ I know that we have to find the difference of edges also the number of cycles of length $4$ is $${n \choose 4}\cdot \frac{3!}2$$ (where $\displaystyle n \choose 4$ is to account for the selection of $4$ vertices and $3!$ for ways of selecting and division by $2$ to delete reverse cycles). But am unable to incorporate the condition on edges and find my result.","['extremal-graph-theory', 'discrete-mathematics', 'graph-theory', 'combinatorics', 'ramsey-theory']"
1356235,How to use the factor theorem on $a(b^2-c^2) + b(c^2-a^2) + c(a^2-b^2)$?,"I know the factor theorem i.e, Let $P(x)$ be a polynomial of degree greater than or equal to $1$ and $a$ be a real number such that $P(a) = 0$, then $(x-a)$ is a factor of $P(x)$. I have an question in my textbook which is - Using Factor theorem , show that $a-b,b-c$ and $c-a$ are the factors of $$a(b^2 -c^2)+b(c^2-a^2)+c(a^2-b^2).$$ I can not see any polynomial over here . How can I solve the problem ? Hints are welcome","['polynomials', 'factoring', 'algebra-precalculus']"
1356236,What is the solution to the equation $9^x - 6^x - 2\cdot 4^x = 0 $?,"I want to solve: $$9^x - 6^x - 2\cdot 4^x = 0 $$ I was able to get to the equation below by substituting $a$ for $3^x$ and $b$ for $2^x$: $$ a^2 - ab - 2b^2 = 0 $$ And then I tried \begin{align}x &= \log_3{a} \\
x &= \log_2{b} \\
\log_3{a} &=\log_2{b}\end{align}
But I don't know what to do after this point. Any help is appreciated.","['roots', 'exponentiation', 'algebra-precalculus', 'logarithms']"
1356295,A question on Green's functions & integral operators,"I'm fairly new to the concept of Green's functions, but from what I understand so far, they are a powerful tool for solving PDEs with boundary conditions. Given a differential equation (in operator form)
$$\mathcal{L}u(x)=f(x)$$ (where $\mathcal{L}$ is a differential operator) it is possible to find a solution for $u(x)$ of the form
$$u(x)=\int G(x,\xi)f(\xi)d\xi$$
I'm struggling to understand the motivations given for the integral operator form of the solution given above. Why is it possible to represent it like this? I understand that the motivation partially comes from the fact that $\mathcal{L}$ will have an inverse (assuming that $\mathcal{L}$ is non-singular ), and as it is a differential operator this implies that the inverse $\mathcal{L}^{-1}$ will be an integral operator . From the differential equation above we have $$u(x)=\mathcal{L}^{-1}f(x)$$ What is the motivation for introducing a so-called Green's function $G(x,\xi)$ such that the integral operator is of the form given above? Is it just that $G(x,\xi)$ is a generator of the operator? Also, is the reason why we end up with the defining equation for a Green's function $$\mathcal{L}G(x,\xi)=\delta (x-\xi)$$ simply because $\mathcal{L}\cdot\mathcal{L}^{-1}=\mathbf{1}$ and the fact that we can formally write (as $\mathcal{L}$ is linear ) $$\mathcal{L}u(x)=\mathcal{L}\left(\int G(x,\xi)f(\xi)d\xi\right)=\int \left(\mathcal{L}G(x,\xi)\right)f(\xi)d\xi=f(x)\quad\Rightarrow\quad\mathcal{L}G(x,\xi)=\delta (x-\xi)$$","['greens-function', 'linear-algebra', 'ordinary-differential-equations']"
1356335,Show that the equation $y^2 = x^3 + 3$ has infinitely many rational solutions in $x$ and $y$.,"Show that the equation $y^2 = x^3 + 3$ has infinitely many rational solutions
  in $x$ and $y$. I'm really not sure how to go about this question. I've been using trial and error and have not got very far.","['number-theory', 'diophantine-equations', 'elementary-number-theory']"
1356361,Why binomial distribution doesn't count permutations?,"Why in Binomial distribution the formula starts with $n\choose k$ and not with something like $k!\over n!$? Isn't the order important? Or, it is important but due to the independence of the event?",['probability']
1356388,Sum of Permutations of n objects taken r at a time ( r=1 to n ) where objects may be groups of same entities,"Given n objects where n 1 objects are the same ,along with another group of n 2 objects of same element etc.. such that Σn i = n (i=1 to k). Assuming there are k groups of similar objects eg: in {1,1,2,2,2,3,4} Thus n 1 =2, n 2 =3, n 3 =1, n 4 =1 and n=7
for two 1's, three 2's, one 3 and one 4 thus k=4 groups Here Permutations of taking r elements at a time from n when r=n is 
$\frac{n!}{n1!.n2!.n3!...nk!}$ What about the permutations where r < n (i.e. r=1 to r=n-1) ? Is there a direct formula? Is it possible to find the sum of all numbers that can be formed with these elements when the objects are numbers (using the above concept)? (Similar to the formula when r=n Total sum of all numbers that can be formed = (n-1)!.(sum of all n digits).(1111...n times) )","['combinatorics', 'permutations']"
1356393,Sequence involving floor function - limit and bounds,"My first time here, so please excuse any breaches of etiquette. For a given  $p \in \mathbb N$  and irrational $\alpha$, let $\varepsilon_n=\alpha n-\lfloor \alpha n \rfloor,$ and $S_n=\frac{1}{n+1}\sum_{i=0}^{n} p^{-\varepsilon_i}$ (where $n=0 , 1, ...$). I suspect that $\lim_{n \to \infty} S_n=\frac{p-1}{p \ln p}$, but have no idea how to prove (or disprove) it. [Edit: Many thanks to Daniel Fischer for explaining this]. I'd like to find some well-behaved bounding functions for the sequence $\{ S_n \}$, i.e., relatively simple continuous functions $f$ and $g$ such that $f(n) \le S_n \le g(n)$: I think $f(x)=\frac{p-1}{p \ln p}$ and $g(x)=f(x)+\frac {1}{x+1}$ will do, but I would like some suggestions for proving/disproving this (or functions that are a tighter fit). Can anyone please help with part 2, or point me towards some basic resources that might shed light on it?","['sequences-and-series', 'calculus', 'elementary-number-theory']"
1356403,About the definition of regular surface in do Carmo,"According to do Carmo, the definition of regular surface requires us to check $X^{-1}$ to be continuous (where $X$ is a local parametrization). But doesn't it infer from other conditions (as shown in his Prop. 4 of Sec 2-2, see photo attached here !!! )? Although he stated regular surface has to be known in advance (prop. 4), actually it seems that he doesn't use it in the proof. He simply uses $X$ to be differentiable and regular and inverse function theorem to conclude $X^{-1}$ is continuous.",['differential-geometry']
1356444,Functional Derivative ${\delta q_a(t)}/{\delta q_b(t')}$,"$\newcommand{\fdv}[2]{\frac{\delta #1}{\delta #2}}$
$\newcommand{\dv}[2]{\frac{\mathrm{d} #1}{\mathrm{d}#2}}$
$\newcommand{\pdv}[2]{\frac{\partial #1}{\partial #2}}$ I'm from a physics background and I've always known the definition to be related to the Euler-Lagrange Equations i.e. $$\fdv {L(q,\dot q)} {q(t)} = \pdv{L}{q} - \dv{}{t} \pdv{L}{\dot q} \; ,$$ where $\dot q$ denotes the derivative of the function $q$ with respect to $t$. However with this definition I cannot prove a fact that I've seen in a lecture video about Quantum Field Theory, which is $$\fdv{q_a(t)}{q_b(t')} = \delta_{ab} \delta(t-t') \; , \tag 1$$ where $\delta(t-t')$ is the Dirac delta function/distribution and $\delta_{ab}$ is the Kroneker delta. I don't know how I should make sense out of this equation let alone prove it because $q_a(t)$ is not a functional but a function . I'd appreciate if someone can explain what is meant by the equation (1) and give a proof thereof.","['functional-calculus', 'physics', 'functional-analysis']"
1356472,Turning infinite sum into integral,"Could someone explain how one can replace infinite sum with integral? Examples (or you may use your own, it doesn't matter as I want to understand principles): $$\frac{1}{n} \sum_{i=1}^{n} \sin \frac{i-1}{n} \pi$$ $$\frac{1}{n} \sum_{i=1}^{n} \sqrt{1 + \frac{i}{n}}$$ I see that $\frac{1}{n}$ denote partition, so my problem is that I don't know how to figure out the function, hence, the integrand.","['sequences-and-series', 'calculus', 'real-analysis', 'integration']"
1356499,Maximum is attained on the boundary,"Let $f_1, ... , f_n$ be holomorphic on a bounded domain $\Omega$, continuous on $\overline{\Omega}$.  Let $g = |f_1| + \cdots + |f_n|$.  The problem I couldn't get is to show that the maximum of $g$ is attained on the boundary of $\Omega$.  This should be some obvious application of the fact that the maximum of each $f_i$ occurs on the boundary, but I'm just not seeing it.",['complex-analysis']
1356523,What are the restrictions on using substitution in integration?,"What are the restrictions on using substitution in integration? For example, integration in this case may be done by the substitution $u=\tan x$ , as seen in the following $\textit{Mathematica}$ notebook Integration done by substitution $u=\tan {x\over 2}$ . The source function is a continuous positive function over $\mathbb {R}$ which implied by necessity that its antiderivative is continuous and increasing over $\mathbb {R}$ (which contradicts the graph of its analytical antiderivative that has jumb discontinuities). When I sent this question to Dr.Math, he replied - thanks to him - that the antiderivative of the function $ f(x)={1\over {1+{\sin x}^2}} $ is: $$ {\tan^{-1} (\sqrt 2 \tan x) \over \sqrt 2}  + \lfloor \frac x \pi-\frac 12 \rfloor * \frac{\pi}{\sqrt 2} + C $$ Which matches the ""True"" antiderivative of the function after redefining the integral at $( {\pi \over 2} + n.\pi) $ . 
The second term has a derivative of $0$ over $ \Bbb R $ . This kind of ""weird"" functions makes me wonder about the ""Restrictions"" on using substitution when evaluating integrals, how to know that the function I work on needs refinements, and how to construct this ""True"" antiderivative ?","['substitution', 'continuity', 'integration']"
1356574,Ratio of $\frac{\sin x_1 }{\sin x_2}$ where $f(x_1)=f(x_2)$ for a trigonometric sum $f(x)$,"If $f(x) = \cos(x+a_1)+\frac12\cos(x+a_2)+\frac14\cos(x+a_3)+\cdots+\frac1{2^{n-1}}\cos(x+a_n)$, where $a_1, a_2, ... a_n$ are some constants and $f(x_1)-f(x_2)=0$, where $x_2 \neq m\pi$, find
$$\frac{\sin x_1}{\sin x_2}+\frac{\left|\sin x_1\right|}{\sin x_2}+\frac{\sin x_1}{\left|\sin x_2\right|}+\left|\frac{\sin x_1}{\sin x_2}\right|$$ A friend gave me this one and I tried using
$$0=f(x_1)-f(x_2) = 2\sin\left(\frac{x_2-x_1}2 \right)\left[\sin\left(\frac{x_1+x_2}2+a_1 \right)+\frac12\sin\left(\frac{x_1+x_2}2+a_2 \right) +\dots\right]$$ which gives one possibility $x_2=x_1+2k\pi$ so the desired value can be $0$ or $4$.  However I also see a possibility of $a_i=-\frac{x_1+x_2}2+2k_i\pi$ which leads to a lot of possibilities for the value.  Am I doing this incorrectly or is there something missing in the question (my friend swears no)?","['contest-math', 'trigonometry']"
1356584,construct a path in $\Bbb{R}^n$ with specific derivatives,"If we want a curve $\gamma:(-\epsilon,\epsilon)\to \Bbb{R}^n$ to have  $\gamma(0) = x$, and $\gamma'(0) = b$. It suffices to take $\gamma(u)= x + u \cdot b$. At the same time, I have a function on $\mathbb{R}^n$( $f: \Bbb{R}^n \to \Bbb{R}$). I would like to consider a path $\gamma$ such that $f\circ\gamma(t)$ satisfies $$f\circ\gamma(0) = f(x) \qquad (f\circ\gamma)'(0) = f'(x)\cdot b \qquad (f\circ\gamma)''(0) = \sum_{ij} a_{ij} (\partial_i \partial_j f(x))$$ Is this possible? Note: it is not the case that $a_{ij} = c_i c_j$  for some $c \in \Bbb{R}^n$","['real-analysis', 'ordinary-differential-equations']"
1356586,A second order linear ordinary differential equation,"I am trying to solve the following differential equation: \begin{equation}
(1-x^{2})y''+\{\alpha x+1 \}y'+\{\beta^{2}x^{2}-\beta{x}+\gamma\}y=0
\end{equation} where $y=y(x)$. $y'$ and $y''$ are the first and second derivatives respectively. $\alpha$, $\beta$ and $\gamma$ are constants. There are various second order differential equations that are very similar looking to this equation. For instance Mathieu equation, Legendre equation and generalized Hypergeometric equation, but not quite the same. I am guessing that a change of variable can transform the equation into a known form. However, no success yet!",['ordinary-differential-equations']
1356598,Derivative of a definite integral issue,"$g:\mathbb{(0,1]}\to \mathbb{R}$ We have the function $$g\left(x\right)=\int _x^1\left(\frac{\sin\left(t\right)}{t}dt\right)\:$$ Show that the function is strictly decreasing. So I thought that I'd differentiate the function and prove that the derivative is $\lt0$. I found on-line that that the derivative of this function is:
$$\frac{d}{dx} \int _x^1\left(\frac{\sin(t)}{t}\right) \, dt=-\frac{\sin(x)}{x}$$ Why is that exactly? I thought that $\frac{d}{dx} \int _a^b (f(t)) \, dt = f(t)\text{ from }a\text{ to }b$.","['derivatives', 'integration']"
1356619,Maximum condition for second order differential operators,"Let $A$ be a second order differential operator such that $$Af(x) = \sum_{ij} a_{ij}(x) \big(\partial_i \partial_j f(x)\big) + \sum_j b_j(x) \partial_j f(x) $$ Assume that  $x \in B(0,r)\Rightarrow Af(x)>0$. How do I see that $f$ cannot attain a maximum inside of $B(0,r)$? Here $f : \Bbb{R}^n \to \Bbb{R}$ is $C^2$ and for every $x\in \Bbb{R}^n$$$\sum_{ij}x_ia_{ij}x_j \geq 0$$","['ordinary-differential-equations', 'partial-differential-equations']"
