question_id,title,body,tags
2395468,Find the integer part,"Sorry I failed to type it. The integer part of $$\sqrt{2\sqrt[3]{3\sqrt[4]{4\cdots \sqrt[2011]{2011}}}}$$
  is... My Atempt: As $2011<2^{11}$ I tried to write the expression between two power of $2$ , but things are quickly becoming very ugly. I think there may another neat way. Can anyone help?","['algebra-precalculus', 'contest-math', 'sequences-and-series']"
2395470,Why will an implication be true when the hypothesis is false? [duplicate],"This question already has answers here : In classical logic, why is $(p\Rightarrow q)$ True if both $p$ and $q$ are False? (25 answers) Closed 6 years ago . When $p$ and $q$ are propositions such that ""if $p$ then $q$"" (which is an implication) is false only in the condition when $p$ (which is hypothesis) is true but $q$ (which is conclusion) is false. But I couldn't understand why will the implication be true when the hypothesis itself is false. Shouldn't the implication be false when the initial assumption or hypothesis is false ?","['logic', 'discrete-mathematics']"
2395471,"Is $L^{p}(I,X)\cong L^{p}(I) \widehat{\otimes_{\pi}}X$?","It is well known that
$$L^{1}(I,X) \cong L^{1}(I) \widehat{\otimes_{\pi}}X$$
For any compact interval (I,m) with the Lebesgue measure and any Banach space X.
Is it still true when $1<p<\infty$
that
$$L^{p}(I,X) \cong L^{p}(I) \widehat{\otimes_{\pi}}X$$ If not, can we embed one of them in the other ?","['functional-analysis', 'tensor-products', 'tensors', 'banach-spaces']"
2395485,How often are prime powers next to factorials.,"Consider:$$7! = 7\cdot 6\cdot 5\cdot 4 \cdot 3 \cdot 2\cdot 1 = 5040$$
We also have:
$$71^2 = 5041$$ How often does it happen that a factorial is right next to ($\pm 1$) a prime power (exponent $>1$)? Since factorials grow so quickly we can consider how many percent of $n$ s.t. $\exists p,k:$ $$n! = p^k\pm1, \cases{n\in \mathbb Z\\k\in \{2,3,\cdots\}\ \\p \text{ prime}}$$ Own work: So far checked $n<10$ by hand: $$\begin{align*}5^2 &= 4!+1\\11^2&=5!+1\\71^2&=7!+1\end{align*}$$ I suppose some language with ""big-int"" types would be very helpful as $n!$ grows very fast.","['number-theory', 'factorial', 'prime-numbers']"
2395529,Statement of the Perron–Frobenius theorem,"Let $A=(a_{{ij}})$ be an $n\times n$ positive matrix: $ a_{{ij}}>0$ for $1\leq i,j\leq n$.
On Wikipedia, the statement of Perron–Frobenius theorem indicates, among others, the following claims: There exists an eigenvector $v = (v_1,\dots,v_n)$ of $A$ with eigenvalue $r$ such that all components of $v$ are positive: $A v = r v$, $v_i > 0$ for $1 \leq i \leq n$. (Respectively, there exists a positive left eigenvector $w : w^T A = r w^T$, $w_i > 0$.) $\lim _{{k\rightarrow \infty }}A^{k}/r^{k}=vw^{T}$, where the left and right eigenvectors for A are normalized so that $w^Tv = 1$. I'm trying to prove the second claim, but so far I have not gone well. Any suggestion, please? Thanks in advance.","['matrices', 'eigenvalues-eigenvectors', 'positive-matrices', 'linear-algebra']"
2395583,About $\int_0^1 f_n(x)dx$ when $f_n$ is $f:x\mapsto2x(1-x)$ composed $n$ times with itself,"The following question is taken from here exercise $4:$ Let $f(x) = 2x(1-x),x\in\mathbb{R}.$ Define 
  $$f_n = f \circ f \circ ... \circ f (n \text{ times}), f_n(x)=f(f(...f(x)...)).$$
  (a) Find $$\lim_{n\rightarrow\infty} \int_0^1 f_n(x)dx.$$
  (b) Compute the integral $$\int_0^1 f_n(x)dx.$$ I use Wolfram Alpha to obtain the answer $\frac{1}{2}$ for (a) and $\frac{2^{n-1}}{1+2^n}$ for (b). However, I have no idea on how to get close to answer. For (a), I try to evaluate the composition directly. However, I have trouble evaluating when $n=3.$ I think we need to interchange the limit and integral but even after that I have no idea.","['contest-math', 'real-analysis', 'integration', 'limits']"
2395661,"Square root of the determinant of AB+I where A, B are skew-symmetric","Imagine I have two skew-symmetric square matrices $A$, $B$. (So $A^\intercal = -A$, etc.) Now I am interested in the square root of the determinant of $AB+I$, where $I$ is the identity matrix, $$ x = \sqrt{ \det \left( AB + I \right) } $$ As quick inspection for small matrices suggests that this $x$ is a polynomial of the elements of $A$ and $B$, for example for $3 \times 3$ matrices we find $$ x =  1 - a_{12} b_{12} - a_{13} b_{13} - a_{23} b_{23} $$ and I checked this analytically for matrices up to $6 \times 6$. It reminds me of the pfaffian of a skew-symmetric matrix, which is also a 'square root of a determinant' but nonetheless a polynomial in the matrix elements. Now my questions are: Does anyone know a proof that $x$ is a polynomial in the elements of $A$ and $B$, and if so, what is that polynomial? Does anyone know an efficient (so not $O(n!)$) algorithm to compute $x$?","['matrices', 'radicals', 'determinant']"
2395699,"values of $x^y=y^x$ for $x \neq y$, hopefully with straightforward explanation","As I understand it, many people thought I am asking in a question with exactly this title about pairs of (x,y) such that $x^y=y^x$, or that I am looking for a function $f(x)$, such that $x^{f(x)}=(f(x))^x$, but I am not. I am asking for another function $g(x)=x^{f(x)}$, and where is it concave, convex, its minima, maxima, asymptotes etc. And more importantly, there are two such functions f(x) with the above property and their corresponding functions g(x). Because one $f(x)$ is simply $x$, so for $y=f(x)=x$ we get $g(x)=x^x$, but I am interested in the other function $g(x)$ that is yielded by the rather complicated function $f(x)$. From the first time I asked the question, I already manage through guesses and observation on individual values come up with these possible properties: So, with more of a thinking and wolframalpha, I found out (hopefuly this time correctly) that the function is defined only on the interval $(1,\infty)$, goes to infinity for $x$ going from the right to $1$ , it is decreasing  in a convex fashion all the way to $x=e$, where is global minimum $e^e$, and it is increasing towards infinity in rather a quite slow fashion, with a possible asymptote $x=y$, to which it is approaching from the left (from the above), which would indicate it is concave somewhere after $x=e$. Can anybody confirm or correct that? Is anyone able to help me with this, now that I hope we understand each other? EDIT: The asymptote part is incorrect, as proved by Ennar. Convexity and concavity are still debatable.","['exponential-function', 'functions']"
2395729,How are inequalities spread over multiple lines to be read?,"Here's a basic question I've been avoiding. One often sees strings of equalities like these: \begin{split}
A & = B\\
  & = C\\
\end{split} Which can be read as either $$A = B\\
B = C
$$ or $$
A = B\\
A = C.
$$ But what about 
\begin{split}
A & < B\\
  & < C\,?\\
\end{split} In this case $$
A < B\\
B < C
$$ and $$
A < B\\
A < C
$$ say two entirely different things. So what's the standard interpretation?","['algebra-precalculus', 'inequality']"
2395737,Understanding a proof of the vanishing of the Weyl tensor for a diagonal Riemannian metric,"I'm currently working through this paper from Duke Math J., Volume 51, Number 2 (1984), 243-260 (unfortunately couldn't find a publicly visible link); more specifically, the part about the obstructions to the diagonalizability of Riemannian metrics in $n \geq 4$ dimensions, but there are a few parts where I just can't wrap my head around what's happening. On page 258, they start by taking an orthonormal coframe $\{\omega^i = f^i d x^i : i = 1, ..., n\}$ altogether with it's dual frame $\{e_i\}$ . Their goal is to show that the Weyl-tensor $W$ vanishes, so that one can apply the Weyl-Schouten theorem to get that the manifold is already conformally flat. Using some calculations with the connection form $\omega_j^i$ and Cartan's structure equations which I can for the most part follow, they arive at the result that $$ d(\omega^i \wedge \omega^j \wedge \omega_j^i) = \omega^i \wedge \omega^j \wedge \Omega_j^i = 0 \text{ for } i \neq j$$ where $\Omega_j^i$ is the curvature $2$ -form. Now from this, Deturck and Yang make the jump to the sectional curvature $R$ and then on to the Weyl Tensor $W$ . Namely, they argue the following: Thus $\omega^i \wedge \omega^j \wedge \Omega_j^i = 0$ for all $i \neq j$ . This will manifest itself as an integrability condition, as we shal see. Rewriting this in terms of the sectional curvature $R$ , we must have $$R(e_i, e_j, e_k, e_l) = 0 \text{ if $i, j, k, l$ are distinct} $$ where $\{e_i\}$ is the dual frame to $\{\omega^i\}$ . How do they make this jump? How can we come from the equation $\omega^i \wedge \omega^j \wedge \Omega_j^i = 0$ where we have only two distinct components $i, j$ to the equation with four distinct components $i, j, k, l$ for which the curvature $R$ vanishes? What exactly do we do when ""rewriting"" this equation, as they only call it? It might be due to my limited experience with these things, but I fail to see the exact relation between the first equation and the vanishing of the curvature tensor for these four components. Right thereafter, they continue to carry over this equation about the Weyl tensor, namely Since $\{e_i\}$ is an orthonormale frame, none of these components of $R$ enter into the Ricci tensor. We conclude that $$W(e_i, e_j, e_k, e_l) = 0 \text{ if $i, j, k, l$ are distinct}$$ where $W$ is the Weyl tensor. This seems less obscure to me, but I don't understand it entirely either. My first question would be, how do we get from the $\{e_i\}$ being orthonormal to not entering into the Ricci tensor? Is this some property of the Ricci-tensor that I'm missing? And my 2nd question here would be: I know that the Riemannian curvature tensor decomposes into the ""trace""-part (Ricci-Tensor) and the ""traceless""-part (Weyl-Tensor). Is this the argument Deturck and Yang use here to conclude the $W(...) = 0$ -equation? That if the components don't enter into the Ricci-tensor, they can only be in the only other part the curvature tensor consists of, namely the Weyl-tensor, so if one of them is $= 0$ , the other one is too? Am I understanding this argument correctly? (Even if I probably didn't express it too well...) From there on, they continue with some calculations that I can roughly follow to show that $W \equiv 0$ , but I just can't wrap my head around about this passage. I haven't worked that much with Weyl- and Curvature tensors before, so maybe some stuff I have trouble with has a very easy explanation that I'm just missing out on. Any help would be greatly appreciated.","['tensors', 'riemannian-geometry', 'differential-geometry', 'curvature']"
2395747,"If $y^3 + 3a^2x + x^3 = 0,$ then prove that $y'' + \frac{2a^2x^2}{y^5} = 0$",I am comfortable with second derivatives but I am just unable to set all the variables up in such a format that I get the latter (the part which needs to be proven). A hint would be a lot of help.,"['derivatives', 'ordinary-differential-equations', 'calculus']"
2395749,Triangle and Circle maximization problem,"So I was playing around GeoGebra and found this thing out, I don't know if this problem has a name or something. Triangle ABC is inscribed inside a circle, from point D which is located inside the circle, we draw 3 perpendicular lines to each side of the triangle, what is the maximum area of the triangle whose vertices are the intersections of the perpendicular lines and the sides of the triangle? (maximum area of triangle EFG, the red triangle in the picture) Using Geogebra I found out that this area is always maximal when point D is located at the center of the circle, or in other words, when the perpendiculars divide the sides into 2 equal segments. If someone could provide a proof/explain why, I would be grateful. See the diagram below:","['circles', 'optimization', 'calculus', 'triangles', 'geometry']"
2395771,"To any given positive integer $m$ there exists a positive integer $x,$ such that $x^3+x+m^2$ is a square","Let $m$ be a positive integer. Show that
there exists positive integer $x$ such that
$x^3+x+m^2$ is a square. This problem I can't have any idea,How to prove it?","['number-theory', 'elliptic-curves', 'diophantine-equations']"
2395821,Odds of ellipse from five random points,"It's known that five points determine a conic section .  Five random points can go right into the $6\times6$ matrix, and then the $A x^2 + B xy + C y^2$ part can be looked at.  If $B^2-4AC<0$, it's an ellipse.  Five random points will almost never produce circles or parabolas, so the results will be ellipses and hyperbolas.  What are the odds of an ellipse? In a random run of 100000 trials, I got 27974 ellipses. ""It's less than $e/10$,"" seems like a solid answer.  Anyone have anything more specific? EDIT:  As Oscar points out, I should have said ""It's more than $e/10$.""  In my trial, real-values points were randomly picked from a unit square. Square Triangle Picking methods might be applicable. EDIT2: Aretino points out that odds of a convex pentagon are $49/144≈0.34$. So how can points making a convex pentagon give a non-ellipse?  Here's a picture. With the red points fixed, the black points are outside of the convex hull yet still yield a non-ellipse. EDIT3: That spray of points above goes back to Newton, Philosophiae naturalis
principia mathematica, 1687, where he solved the 4 point parabola ( another version ). If a point is between one of the two parabolas and the degenerate lines, then it gives a hyperbola.","['conic-sections', 'linear-algebra', 'geometry']"
2395848,How do I show that $V_n$ is as required?,"Question : If $V_n=\frac{d^n}{dx^n}(x^n \log x)$, show that $V_n=nV_{n-1}+(n-1)!$ Hence show that $$V_n=n! (\log x + 1 + \frac{1}{2}+\frac{1}{3}+\dot{} \dot{} \dot{}+\frac{1}{n})$$ What I have managed to do so far: I have found that $V_{n+1}=\frac{n!}{x}$ but I cannot use it further to answer the questions. PS : Here $V_n$ is $n^{th}$ derivative of $V$ Can someone kindly guide me on how to pursue further in this problem?",['derivatives']
2395852,What will be the $n$th derivative of $y = e^{ax} \sin(ax+b)$?,"$$y = e^{ax} \sin(ax+b)$$ I tried finding a pattern but no luck till the 7th derivative, and I have seen some people solving it with Euler's method but I doubt my familiarity with it. It'd be great if someone could shed some light on a more straight-forward, general and intuitive solution to the problem. Where a and b are constants.","['derivatives', 'real-analysis', 'calculus']"
2395853,Gate 2011-Find $p $ such that given series is convergent.,"Let $x=(x_1,x_2,\ldots) \in l^4$, $x\ne 0$. For which of the following values of $p$ the series $\sum\limits_{i=1}^\infty x_iy_i$ converges for every $y=(y_1,y_2,\ldots) \in l^p$. (A) $1$;
  (B) $2$;
  (C) $3$;
  (D) $4$ I have used holders inequality. I got $\frac{1}{p}=1-\frac{1}{4}$. But this option is not in the list. Please help me. Thank you in advance.","['real-analysis', 'functional-analysis', 'lp-spaces', 'convergence-divergence', 'sequences-and-series']"
2395882,Prove that the graph of a measurable function is measurable.,"How can I prove that the graph of a measurable function is measurable. I recall that the graph of $f:\mathbb R\longrightarrow \mathbb R$ $$\Gamma(f)=\{(x,f(x))\mid x\in \mathbb R\}.$$ Attempt 1) if $f=1_{[a,b]}$ then $\Gamma(f)=[a,b]\times \{1\}\cup[a,b]^c\times \{0\}$, and thus $\Gamma(f)$ is measurable. 2) If $f$ is a step function, same thing. 3) If $f\geq 0$, then there are step function $f_n$ s.t. $f_n\nearrow f$. Now, I would like to have $\Gamma(f)=\bigcup_{n\in\mathbb N}\Gamma(f_n)$, but unfortunately it doesn't look to be the case. Any idea ? 4) Same for $f$ measurable. I have that $f=f^+-f^-$, but I don't think that $\Gamma(f)=\Gamma(f^+)\cup \Gamma(f^-)$. Any idea ?","['lebesgue-measure', 'measure-theory']"
2395920,Cardinality of field completion,"Let $K$ be the completion of a field $k$ equipped with a norm $\|\cdot\|$. What can we say about it's cardinality, $|K|$? By Cantor's completion process we can treat $K$ as a quotient of $c \le k^\infty$ (space of convergent sequences) by $c_0$, space of null sequences. So definitely $|K| \le |k|^{\aleph_0}$. Is the inequality with flipped sign also true? Sometimes yes ($k = \mathbb R$). Would an assumption that $k = K$ help?","['complete-spaces', 'elementary-set-theory']"
2395926,How to solve this question on harmonic functions?,"How can I determine $a$ so that the given function is harmonic, and find its harmonic conjugate? $$u = e^{\pi x}\cos(av)$$ Where $v$ is itself a real valued function of x,y. Is there any other method than using Laplace Equation and taking double derivative and solving the equation as it tends to become too complicated?","['complex-analysis', 'harmonic-functions']"
2395941,On thinking about independent events,"The definition of indepedent event is the following, $P(A \cap B) = P(A)P(B)$. Via the conditional dependence definition this makes some kind of sense. Is there another way to think about this equality? I been staring at it for a while but I cant seem to get any intuition except via the conditional. Any ideas?","['probability-theory', 'soft-question']"
2395955,Evaluating the limit $\lim_{x\to1}\left(\frac{1}{1-x}-\frac{3}{1-x^3}\right)$,"In trying to evaluate the following limit:
$$\lim_{x\to1}\left(\frac{1}{1-x}-\frac{3}{1-x^3}\right)$$ I am getting the indefinite form of:
$$\frac{1}{\mbox{undefined}}-\frac{3}{\mbox{undefined}}$$
What would be the best solution to evaluating this limit?",['limits']
2395967,Dominated a.e. convergence implies almost uniform convergence,"Let $(f_n)$ be a sequence of measurable functions that converges almost everywhere to a measurable function $f$. Assume that there is an integrable function $g$ such that $|f_n|\leq g$ for all $n$ almost everywhere. Show that $(f_n)$ converges almost uniformly to $f$. Now I don't know of any other sufficient condition for almost uniform convergence other that Ergorov's theorem. However, to apply it I need my space to be of finite measure, and somehow the existence of a dominating integrable function $g$ has to play a role.
 But I wouldn't know where to start, so I'd like some hint on how to start working on the problem. Thank you in advance!","['almost-everywhere', 'real-analysis', 'measure-theory', 'uniform-convergence']"
2396015,Sum of three dice is eleven,"the sample space of one toss of three dice is:
$\Omega = \left \{ (1,1,1), ..., (6,6,6) \right \}$ so there are $6^3 = 216$ possible outcomes. What is the probability to obtain an outcome where the sum of its three components is equal to 11? I've considered the possible value can assume dice without a particular position and then I have considered the permutations to include every position: $(6,4,1), 3! = 6 \\ (6,3,2), 3! = 6 \\ (5,5,1), \frac{3!}{2!} = 3 \\ (5,4,2), 3! = 6 \\ (5,3,3), \frac{3!}{2!} = 3 \\ (4,4,3), \frac{3!}{2!} = 3$ so I've summed up obtaining $27$ and the probability would be $\frac{27}{216}$ Now, consider if I have to do this same passages for sums from 3 to 18, it is very exhausting. So, my question is: Is there any ""faster"" way to do that?","['combinatorics', 'probability', 'dice']"
2396018,Why do the Torus and the Klein Bottle have the same Euler characteristic and aren't homeomorphic?,Isn't the Euler Characteristic a topological invariant?,"['algebraic-topology', 'differential-geometry']"
2396022,How do I evaluate this limit?,"I'm trying to evaluate this limit: $$\lim_{r\to 10}\int_0^r\sqrt\frac{{400-3x^2}}{400-4x^2}\ dx$$ at $x = 9.99999999999$, the value is $12.1105532$, so I feel like it is approaching some value around this. Does that mean there is a limit ? How do I evaluate that limit?","['integration', 'calculus', 'limits']"
2396093,which one is bigger $100^n+99^n$ or $101^n$,"Suppose $n \in \mathbb{N} , n>1000$ now how can we prove :which one is bigger $$100^n+99^n \text{ or  }  101^n \text{ ? }$$ 
I tried to use $\log$ but get nothing . Then I tried for binomial expansion...but  I get stuck on this . can someone help me ? thanks in advance.","['algebra-precalculus', 'number-theory', 'calculus', 'elementary-number-theory']"
2396141,A property of area functional,"Involving the Plateau's problem, we have a property about the functional area which is: $$\int_{\Omega}\sqrt{1+\mid Du \mid^{2}}dx=\sup\lbrace \int_{\Omega}(g_{n+1} + u\, \text{div} g)dx; g\in C_{C}^{1}(\Omega;\mathbb R^{n+1}), \mid \mid g \mid \mid \leq 1 \rbrace$$ where $u\in W^{1,1}(\Omega)$, $\Omega$ is a domain in $\mathbb R^{n}$. Please, could you show me more precisely how to prove the above equation from the fact that: $$\int_{\Omega^{'}}\mid Dv \mid dx=\sup\lbrace \int_{\Omega^{'}}(v\, \text{div} g)dx; g\in C_{C}^{1}(\Omega^{'};R^{n+1}), \mid \mid g \mid \mid \leq 1 \rbrace$$ where $\Omega^{'}$ is a domain in $\mathbb R^{n+1}$ and $v\in W^{1,1}(\Omega^{'})$.","['geometric-measure-theory', 'calculus-of-variations', 'minimal-surfaces', 'differential-geometry']"
2396287,How do I calculate $\frac{\ln(2)}{\ln(3)}\frac{\ln(4)}{\ln(5)}...\frac{\ln(2n)}{\ln(2n+1)}$?,"The product has only positive factors so it has zero as lower bound. Also the product is decreasing as all its factors are less than one. In conclusion the series must have a limit. I also compute the first 150 values of the product and I got around 0.297. I believe that the product converges very, very, slowly to zero, but I can't prove it.","['real-analysis', 'infinite-product', 'limits', 'logarithms', 'sequences-and-series']"
2396349,How to prove a property is preserved under isomorphism?,"I'm following along an MIT discrete maths course. One problem is as follows: Determine which among the four graphs pictured in the Figures are isomorphic. If two of these graphs are isomorphic, describe an isomorphism between them. If they are not, give a property that is preserved under isomorphism such that one graph has the property, but the other does not. For at least one of the properties you choose, prove that it is indeed preserved under isomorphism (you only need prove one of them). ( source - MIT open courseware 6-042j, assignment 4) So, let's just consider graph G1 and G2. From examination I would say there is one additional edge in G2 that's not present in G1, so no isomorphism exists. The problem says: If they are not [isomorphic], give a property that is preserved under isomorphism such that one graph has the property, but the other does not. For at least one of the properties you choose, prove that it is indeed preserved under isomorphism […] How to proof that?
And one more question, how do you usually go about finding isomorphims between graphs? Write down all the edges from one graph and see if they are present in the other?","['graph-theory', 'discrete-mathematics']"
2396380,Continuity points of the function $f$ defined by $f(p/q)=\sqrt{(1+p^2)/(1+q^2)}$ and $f(x)=x$ if $x$ is irrational or zero,"Let $f:[-1,1]\rightarrow\mathbb{R}$ defined by $f(x)=x$ for every $x\in[-1,1]\cap \mathbb{Q}^{c})\cup\{0\}$ and $f(x)=\sqrt{\frac{1+p^{2}}{1+q^{2}}}$ for every $x=\frac{p}{q} \in[-1,1]$. Then, $f$ is continuous on $((0,1)\cap\mathbb{Q}^{c})\cup\{0,1\}$. How to find the continuity and discontinuity points? Is there any theorem to check the continuity and continuous point for this type of function. Proving the continuity using $\epsilon-\delta$ definition like the proof of Thomae function is lengthy. Please help me to deduce the conclusion with less time.","['continuity', 'real-analysis']"
2396410,"$ \ f: \mathbb{N} \times \mathbb{N} \to \mathbb{R}$ via $ \ f(a,b) = a + b. \sqrt{11}$","Question: Let $ \ f: \mathbb{N} \times \mathbb{N} \to \mathbb{R}$ via $ \ f(a,b) = a + b. \sqrt{11}$ Is $ \ f$ an injection? Is it a surjection? My attempt: It is injective. $ f(a,b) = f(c,d) \implies a + b. \sqrt{11} = c + d. \sqrt{11} \implies (a-c) + \sqrt{11}(b-d) = 0 \implies a -c = 0$ and $ \ \sqrt{11}(b-d) = 0 \implies a = c$ and $ \ b = d$. It is not surjective. Notice that $ \ f(a,b) \neq 1\  \forall \ a,b \in \mathbb{N}$. That is $ \ a+ b.\sqrt{11} \neq 1\  \forall \ a,b \in \mathbb{N}$. So $ \ 1 \notin $ image(f). Hence not surjective. Is my approach and reasoning correct?","['elementary-set-theory', 'functions', 'proof-verification']"
2396448,Proof that the sum of all degrees is equal to twice the number of edges,"We want to proof $2|E| = \sum \limits_{v \in V} deg(v)$ for a simple graph (no loops). For our proof we assume $n$ to be the number of edges in a simple graph $G(E, V)$. We proceed our proof by induction. Base case P(0), no edges exist, so all nodes in $G$ have degree 0. Therefore we find that $2n = 2 * 0 = \sum deg(v) = 0$ Inductive step, assuming P(n) is true, we need to show that P(n + 1) is also true, that is: $2(n + 1) = \sum \limits_{v \in V} deg(v)$ In a graph $G$ with number of edges $n + 1$. If we remove one edge at random $G$, we get a subgraph $G'(E',V')$ for which we can assume P(n): $2n = \sum \limits_{v \in V'} deg(v)$ $G$ is equal to the subgraph $G'$ plus one edge. As every edge contributes $2$ to the total number of degrees (as every edge connects two vertices) we can say for $G$: $2n + 2 = 2(n + 1) = \sum \limits_{v \in V'} deg(v)$ Which proofs P(n + 1). Does the above proof make sense? I had a look at some other questions , but couldn't find a fully written proof by induction for the sum of all degrees in a graph.","['graph-theory', 'proof-verification', 'discrete-mathematics']"
2396515,Span of a subset of a vector space is the smallest subspace containing that set,"To Prove: If  $S=[{v_1,v_2,...,v_k}]$ is a subset of vector space $V$. Then $span(S)$ is the smallest subspace of $V$ containing set $S$. I know that $L[S]$ is a subspace of $V$. 
But in most arguments for proving the $L[S]$ is the smallest subspace containing $S$ , I find that if $W$ is another subspace of $V$ containing $S$ then, proving $S \subset W$ means $S$ is the smallest. I couldn't understand that if $S \subset W$ proves that $L[S]$ is the smallest containing $S$. Please elaborate.",['linear-algebra']
2396556,Are diffeomorphic smooth manifolds truly equivalent?,"It seems to be an often repeated, ""folklore-ish"" statement, that diffeomorphism is an equivalence relation on smooth manifolds, and two smooth manifolds that are diffeomorphic are indistinguishable in terms of their smooth atlases. There is a strange counter example in Lee's Introduction to Smooth Manifolds though, let us define two smooth manifolds modelled on the real line. Let $\mathcal A$ be a smooth maximal atlas on $\mathbb{R}$ that is generated by the global chart $\varphi:\mathbb{R}\rightarrow\mathbb{R}$, $\varphi(x)=x$, and let $\bar{\mathcal A}$ be the maximal smooth atlas on $\mathbb{R}$ generated by the global chart $\bar{\varphi}(x)=x^3$. The transition function $\varphi\circ\bar{\varphi}^{-1}$ is not smooth, so these two smooth structures are incompatible. However the map $F:(\mathbb{R},\mathcal A)\rightarrow(\mathbb{R},\bar{\mathcal{A}})$ given by $F(x)=x^{1/3}$ is a diffeo, because $$ (\bar{\varphi}\circ F\circ \varphi^{-1})(x)=x, $$ and this map is smooth. So the smooth manifolds $(\mathbb{R},\mathcal A)$ and $(\mathbb{R},\bar{\mathcal{A}})$ are diffeomorphic. Yet the two manifolds have incompatible, thus, different smooth structures. Question: I guess I don't have a clear question, I am just somewhat confused. Because this is a counterexample , it seems to prove that the statement ""two diffeomorphic manifolds cannot be told apart by their smooth structures"" is wrong. However how wrong it is? Can we consider the two manifolds given in this example equivalent? Is there any practical difference between the two? Is differential geometry the same on them?","['category-theory', 'smooth-manifolds', 'differential-geometry', 'differential-topology']"
2396570,Finding flaw in bogus k-coloring proof,"I'm following along an MIT discrete maths course, one of the assignments states the problem of finding the flaw in a proof for $k$ -coloring (the course only considers simple graphs without loops): False Claim. Let $G$ be a (simple) graph with maximum degree at most $k$ . If $G$ also has a vertex of degree less than $k$ , then $G$ is $k$ -colorable. a) Give a counterexample to the False Claim when $k = 2$ . Maybe I think a bit too simple about the problem, but it says ""… If $G$ also has a vertex of degree less than $k$ …"" , that means that $G$ only needs to have at least one node with degree less than the maximum degree $k$ . So to answer a) I thought of this graph: The maximum degree here is 2 and it also has one node that has degree less than 2, but you still need $k+1$ colors to color it, so the claim can't be true in this case. Next, they also provide a bogus proof for the claim: Identify the exact sentence where the proof goes wrong. Induction hypothesis: P(n) is defined to be: Let $G$ be a graph with $n$ vertices and maximum degree at most $k$ . If $G$ also has a vertex of degree less than $k$ , then $G$ is $k$ -colorable. Base case: ( $n=1$ ) $G$ has only one vertex and so is 1-colorable. So P (1) holds. Inductive step: We may assume $P(n)$ . To prove $P(n + 1)$ , let $G_{n+1}$ be a graph with n + 1 vertices and maximum degree at most k. Also, suppose $G_{n+1}$ has a vertex, $v$ , of degree less than k. We need only prove that $G_{n+1}$ is $k$ -colorable. To do this, first remove the vertex v to produce a graph, $G_n$ , with n vertices. Removing v reduces the degree of all vertices adjacent to v by 1. So in $G_n$ , each of these vertices has degree less than k. Also the maximum degree of $G_n$ remains at most k. So $G_n$ satisfies the conditions of the induction hypothesis $P(n)$ . We conclude that $G_n$ is k-colorable. Now a k-coloring of Gn gives a coloring of all the vertices of $G_{n+1}$ , except for v. Since v has degree less than k, there will be fewer than k colors assigned to the nodes adjacent to v. So among the k possible colors, there will be a color not used to color these adjacent nodes, and this color can be assigned to v to form a k-coloring of $G_{n+1}$ . We need to identify where the proof goes wrong. This one I find a bit harder, as the proof is relatively long. For me, the base case seems off. It reads: Base case: (n=1) G has only one vertex and so is 1-colorable. So P (1) holds. It's true that you can color a one node graph with one color. But the claim talks about a graph that has degree at most k (which is 0 in a one node graph) that also has a vertex with a degree less than k (which would be -1 or less?), but the single node in that graph has degree k (which is 0 and equals the maximum degree of that graph which is also 0). Am I on the right track or is the proof going wrong somewhere else?","['graph-theory', 'fake-proofs', 'coloring', 'discrete-mathematics']"
2396577,Prove that $F(x) = \sum_{n=1}^\infty f\left(\frac{x}{n}\right)$ is smooth,"Let $f$ be a smooth function, for which $f(0)=0=f'(0)$. Prove that for $x\in\mathbb  R$ and $$F(x) = \sum_{n=1}^\infty f\left(\frac{x}{n}\right),$$ $F(x) \in \mathbb R$ and $F$ is smooth. I wanted to prove first that $F$ is differentiable. $F(0)$ is convergent. I don't really have a clue about the rest of the task.","['derivatives', 'sequences-and-series', 'convergence-divergence']"
2396582,Does $f(a)=1$ and $f'(x)=g\big(f(x)+x\big)f(x)$ imply that $f$ has no roots? (Proof verification),"Let $f: I \to \Bbb R$ be differentiable, where $I$ is an open interval in $\Bbb R$. Let $a \in I$. Let $g: \Bbb R \to \Bbb R$ be continuous. If $f(a)=1$ and $f'(x) = g\big(f(x)+x\big) f(x)$, then prove that $f(x)=0$ does not have any solutions in $I$. Is the following correct? Let's say we have a $t$ that belongs in $I$ so that $f(t)=0$, then: $$\lim_{x\to t}f'(x) = \lim_{x \to t} g\big(f(x)+x\big)f(x) \tag1$$ We know that $\displaystyle \lim_{x \to t} g\big(f(x)+x\big) = b$ where $b \in \Bbb R$, so $(1)$ becomes $\frac{\mathrm dy}{\mathrm dx} = b \ \mathrm dy$. Then, $b=\frac1{\mathrm dx}$, which is false because $b$ cannot tend to $\infty$. Can I cancel out $\mathrm dy$?","['proof-verification', 'real-analysis', 'calculus', 'functions']"
2396603,Math for statistics,I suddenly got interested in machine learning so i began looking for where to start. After some research i found a book about statistical learning which has to be a good place to begin. I'm about to take my second year in civil engineering and i don't know any matrix algebra. I found out that matrix algebra is a part of linear algebra. I searched for some videos and books on linear algebra and found quite a few of them. I don't know what source to pick so I would appreciate if someone would point me to a good source to learn linear algebra which would help me with mentioned book about statistical learning.,"['reference-request', 'statistics', 'self-learning', 'linear-algebra']"
2396640,"Prove or disprove : If $a\equiv b$ mod $m$, when $a,b,m\in \mathbb{Z}$ , then $a^3\equiv b^3$ mod $m^2$.","Prove or disprove : If $a\equiv b$ mod $m$, when $a,b,m\in \mathbb{Z}$ , then 
$a^3\equiv b^3$ mod $m^2$ My attempt : since  $a\equiv b$ mod $m$ then $a=b+mk$ for some $k$ Now $a^3=m^3k^3+b^3+3m^2k^2b+3mkb^2$ $a^3-b^3=m^3k^3+3m^2k^2b+3mkb^2$ from here I stuck","['algebra-precalculus', 'number-theory', 'elementary-number-theory']"
2396660,"Why can we say ""Let $(x_n)$ be a Cauchy sequence in $X$""?","I am very new to analysis. To show a space $X$ is complete, they always begin with, such as, ""Let $(x_n)$ be a Cauchy sequence in $X$"". But I am confused that why we can must find a Cauchy sequence in $X$, without knowing any property of $X$. In other words, there must exist Cauchy sequence in any space $X$, why? I know Cauchy sequences are those whose elements become closer and closer in terms of a norm, as the label goes larger and larger. While I think nothing guarantees such a sequence exists in arbitrary space. For example from a textbook: (Please pay attention to my remarks) To show a linear subspace $M$ of a Banach space $(X, \|\cdot\|)$ is complete if and only if it is closed. Complete $\Rightarrow$ Closed: Let $x \in \overline{M}$, then there is a sequence $(x_n)$ in $M$ such that $\|x_n - x\| \to 0$ as $n \to \infty$. (Remark: Why is there such a sequence? Why does it guarantee that we can find such a sequence?) Since $(x_n)$ converges, it is Cauchy. (Remark: This is true if $M$ is a normed linear space. A subspace of a normed space must be normed with the same norm?) Completeness of $M$ guarantees the existence of an element $y \in M$ such that $\|x_n - y\| \to 0$ as $n \to \infty$. By uniqueness of limits, $x = y$. Hence $x \in M$ and consequently $M$ is closed. (Remark: Which one is the definition of closed set: Its complement set is open, or, the limit points of all its sequences are contained in it? If the later, $(x_n)$ is ""any"" sequence rather than ""a"" sequence in step 1, and consequently $x$ here is every limit point?) Complete $\Leftarrow$ Closed: Let $(x_n)$ be a Cauchy sequence in $M$. (Remark: Why does such a Cauchy sequence exist and can be found? I mean, who can guarantee this? Indeed, here it means ""a"" Cauchy sequence or ""any"" Cauchy sequence?) Then $(x_n)$ is a Cauchy sequence in $X$. Since $X$ is Banach and thus complete, there is an element $x \in X$ such that $\|x_n - x\| \to 0$ as $n \to \infty$. But then $x \in M$ since $M$ is closed. (Remark: Is this by definition of closed set I mentioned?) Hence $M$ is complete. (Remark: It seems not ""a"" but ""every"" Cauchy sequence in step 1?) As I said, I am very new to analysis. I am really hoping someone can help me to review and answer my remarks carefully. Thank you in advance for your patience.","['real-analysis', 'cauchy-sequences', 'complete-spaces', 'metric-spaces', 'analysis']"
2396679,Closed form for a series of functions,"Let $f_1:\mathbb{R}\to \mathbb{R}$ be a locally integrable function (that is $f_1\in L^1_{loc}(\mathbb{R})$). Let us define $f_{n+1}:=\int_0^x f_n(t)\,dt$ for all $n\ge1$. We consider the series $S(x)=\sum_{n=1}^{+\infty} f_n(x)$. The problem asks: prove that $S$ pointwise converges for all $x\in\mathbb{R}$ and find a closed form for the sum. For the part about the pointwise convegence I think one can consider: $|f_n(x)|=|\int_0^x\int_0^{t_1}...\int_0^{t_{n-2}} f_1(t_{n-1})\,dt_{n-1}\,dt_{n-2}\,...\,dt_1   |\le||f_1||_{L^1(0,x)}\frac{x^{n-2}}{(n-2)!}=C\frac{x^{n-2}}{(n-2)!}$ with $C\ge 0$ depending on $x$. By Stirling approximation $\frac{x^{n}}{(n)!}$ is asymptotic to $\frac{1}{\sqrt{2\pi n}}\big(\frac{xe}{n}\big)^n$ which gives a converging series. But now what about the closed form for the series? How can we proceed? I am not even sure about what one means by ""closed form"" in this case. Thank you all!","['real-analysis', 'calculus', 'closed-form', 'sequences-and-series', 'analysis']"
2396682,Lebesgue's original definition of a measurable set?,"There are many ways to construct the Lebesgue measure, and different definitions are available for a subset $A\subset\mathbb{R}$ to be 'measurable'. I believe the most common definition is to use Caratheodory extension theorem involving the concept of an outer-measure; Royden's book uses this definition. Another way to construct the Lebesgue measure is to first define a positive linear functional on $C_c(\mathbb{R})$ using the Riemann integration, and then applying the Riesz representation theorem on locally compact Hausdorff spaces; Rudin's book uses this approach, but I personally think that this is using a sledgehammer to crack a nut. Yet another approach uses both the concepts of an outer-measure and an inner-measure. We first define a subset with a finite outer-measure to be 'measurable' if its outer-measure equals its inner-measure. Then for a general subset, we take intersections with 'measurable' subsets and define things the way they're supposed to be. This approach is taken in Frank Jones' book. All these various ways make me wonder what Lebesgue's original construction was. How did he construct the measure? What was he definition of a 'measurable' subset of $\mathbb{R}$? I don't speak French, so it is very hard for me to go through his original dissertations, and unfortunately it seems that many modern books lack this historic information. So, what was Lebesgue's definition?","['lebesgue-measure', 'math-history', 'lebesgue-integral', 'measure-theory']"
2396690,Is there any positive function $f$ such that $f(x)f(y)\leq |x-y|$ for every $x$ rational and $y$ irrational? [duplicate],"This question already has answers here : Is there a positive function $f$ on real line such that $f(x)f(y)\le|x-y|, \forall x\in \mathbb Q , \forall y \in \mathbb R \setminus \mathbb Q$? (2 answers) Closed 6 years ago . We are given a function $f$ from reals to positive reals (not including $0)$ satisfying 
  $$f(x)f(y)\leq |x-y|$$
  for every rational number $x$ and irrational number $y$. Does this function exist? If $f$ is continuous, it is easy to show that $f$ must be a constant zero function (So it does not exist, in this case). Otherwise, for an irrational number $y$ it can be shown that for any sequence $x_n$ of rationals converging to $y$, $f(x_n)$ converges to zero and the same is true for rational number $x$ and the sequence of irrationals $y_n$. Does this argument give us any information about $f$, and does it say whether $f$ exists at all?","['analysis', 'functional-equations']"
2396716,Flipping odd number array by two numbers in series.,"Suppose an array of length $n$(odd number) which is composed with only $0$. We start to ‘flip’( or ‘change’) numbers with following rules. [1, 1] → [0, 0] [0, 0] → [1, 1] [1, 0] or [0, 1] : do not change. Change elements sequentially and rotationally(when you reach at the end of the array), and repeat until the array becomes its initial state. For example, if the length of the array is 3, 0   [0 0 0] #start
1   [1 1 0] #first and second element is [0, 0], by rule 2, it became [1, 1]
2   [1 1 0] #third and first element = [0, 1], by rule 3, just pass.
3   [1 1 0] #second and third = [1, 0], pass
4   [0 0 0] #first and second = [1, 1], by rule 1, change it to [0, 0]. After 4 steps, the array repeats its configuration, so, the period is $4$. Another example, $n = 5$ 0   [0 0 0 0 0]
1   [1 1 0 0 0]
2   [1 1 1 1 0]
3   [1 1 1 1 0]
4   [1 0 0 1 0]
5   [1 0 0 1 0]
6   [1 0 0 1 0]
7   [1 0 0 1 0]
8   [1 0 0 1 0]
9   [1 1 1 1 0]
10  [1 1 1 1 0]
11  [0 0 1 1 0]
12  [0 0 0 0 0] The period is $12$. And when $n=7$, 0   [0 0 0 0 0 0 0]
1   [1 1 0 0 0 0 0]
2   [1 1 1 1 0 0 0]
3   [1 1 1 1 1 1 0]
4   [1 1 1 1 1 1 0]
5   [1 0 0 1 1 1 0]
6   [1 0 0 0 0 1 0]
7   [1 0 0 0 0 1 0]
8   [1 0 0 0 0 1 0]
9   [1 0 1 1 0 1 0]
10  [1 0 1 1 0 1 0]
11  [1 0 1 1 0 1 0]
12  [1 0 1 1 0 1 0]
13  [1 0 1 1 0 1 0]
14  [1 0 1 1 0 1 0]
15  [1 0 1 1 0 1 0]
16  [1 0 0 0 0 1 0]
17  [1 0 0 0 0 1 0]
18  [1 0 0 0 0 1 0]
19  [1 1 1 0 0 1 0]
20  [1 1 1 1 1 1 0]
21  [1 1 1 1 1 1 0]
22  [0 0 1 1 1 1 0]
23  [0 0 0 0 1 1 0]
24  [0 0 0 0 0 0 0] The period is $24$. It looks like the series of periods follows A046092 (0, 4, 12, 24, 40, 60, 84, 112, 144, 180, 220, 264,… ) How can I prove this? (If you want to look into more longer cases, refer here ) (Any modification of my English will be appreciated. English is not my mother tongue.)","['permutations', 'puzzle', 'sequences-and-series']"
2396772,Evaluate the determinant of a Hessenberg matrix,"The following question is taken from here exercise $1$: Question Evaluate the determinant: 
  \begin{vmatrix}
a_0 & a_1 & a_2 & \dots & a_n \\
-y_1 & x_1 & 0 & \dots & 0 \\
0 & -y_2 & x_2 & \dots & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & \dots & x_n
\end{vmatrix} Since this is a competition question, I suppose the normal determinant formula will not work. Indeed, tedious and messy calculations are inevitable if one just expand the determinant. I observe that the matrix has $-y_i$ at super diagonal for all $1\leq i \leq n.$
However, I do not think this helps. Any hint would be appreciated.","['matrices', 'contest-math', 'linear-algebra', 'determinant']"
2396805,Intuitive picture for topological and fractal dimensions of a fractal,"The usual one-liner definition given for what a fractal dimension represents, is along the lines of: a measure of the change in complexity of the fractal pattern at different scales of measurement. On the other hand, for the topological dimension of a fractal, it is often described as a union of open covers/sets that contain the topological space in which the fractal lies. These two definitions are both very vague and confusing to me, likely because I sadly lack a good understanding of topology in general, but I was hoping maybe someone can elucidate the meaning behind these two quantities with simple examples of fractals.","['general-topology', 'fractals']"
2396817,Why can $z^z$ take any value when $|z| \rightarrow 0$?,"In the real numbers the expression $x^x$ converges to 1 coming from ""any"" direction (which here means from the positive side or from the negative side). Now I feel to remember that this does not hold in the complex plane, because there the expression $z^z$ should be able to take any value (when $|z|$ goes to 0) depending on the direction/angle you're coming from. Now I tried to make sense of that, but couldn't complete the picture. I will demonstrate how far I got.
Let $z = r \cdot e^{i \phi}$ then $z^z = (r \cdot e^{i \phi})^{r \cdot e^{i \phi}} = (r)^ {r \cdot e^{i \phi}} \cdot (e^{i \phi})^{r \cdot e^{i \phi}} = (r)^ {r \cdot e^{i \phi}} \cdot e^{i \phi \cdot (r \cdot e^{i \phi})}$. With this parametrization, when we look at $|z| \rightarrow 0$, this just means $r \rightarrow 0$. My intuition (and Wolframalpha) would now say that the term with $r$ as the base would approach 1 as $r$ approaches 0. But now Wolframalpha also says that the second term with $e$ as the base also approaches 1 as $r$ approaches 0, which would mean the combined limit is 1, not depending on $\phi$. This would then mean that $z^z$ does indeed also approach 1 in the complex plane from whereever we are coming (as the limit is independent of $\phi$). Now, where is the flaw? Does $z^z \rightarrow 1$ as $|z| \rightarrow 0$ for all $z \in \mathbb{C}$? Is there a flaw in my calculations? (either in an intermediate step or the reasoning at the end)","['complex-numbers', 'limits']"
2396823,Question about numerical range of a linear operator,"I am trying to solve the following question, but I did not reach to any answer, I would be so glad if anyone could help me on that. Let $E$ be a Hilbert space over $\mathbb{K}=\mathbb{R}$ or $\mathbb{C}$, with inner product $\langle\cdot\;| \;\cdot\rangle$ and the norm $\|\cdot\|$. Let $T\in \mathcal{L}(E)$ be  an operator. It is well known that the numerical range of $T$
$$W(T)=\{\langle Tx\;|\;x\rangle:\;x \in E,\;\;\|x\|=1\},$$
is convex. If $\lambda=\langle T x_1\; |\;x_1\rangle\in W(T)$, and $\mu=\langle T y_1\; |\;y_1\rangle\in W(T)$, where $\|x_1\|=\|y_1\|=1$. Why for any point $\eta$ on the line segment joining $\lambda$ and $\mu$, there exist complex numbers $\alpha$ and $\beta$ such that $\|\alpha x_1+\beta y_1\|=1$ and $\langle T (\alpha x_1+\beta y_1)\; |\;\alpha x_1+\beta y_1\rangle=\eta.$ ?? I try as follows: Let $M$ be a subspace spanned by $x_1$ and $y_1$ and $P_{M}$ be a projection of $E$ onto ${M}$. Consider $S=P_{M}TP_{M}$. We deduce that $\langle S x_1\; |\;x_n\rangle=\langle T x_1\; |\;x_1\rangle$ and $\langle S y_1\; |\;y_1\rangle=\langle T y_1\; |\;y_1\rangle$. Hence, $\langle T x_1\; |\;x_1\rangle,\langle T y_1\; |\;y_1\rangle\in W(S)=\{\langle Sx\;|\;x\rangle=\langle Tx\;|\;x\rangle:\;x \in M,\;\;\|x\|=1\},$ which is convex. As a consequence, any point $\eta$ on the line segment joining $\lambda$ and $\mu$ belongs to $W(S)$. So, there exist $z\in M$ such that $\|z\|=1$ and $\langle Tz\; |\;z\rangle=\eta$. This implies that there exist complex numbers $\alpha$ and $\beta$ such that $\|\alpha x_1+\beta y_1\|=1$ and $\langle T (\alpha x_1+\beta y_1)\; |\;\alpha x_1+\beta y_1\rangle=\eta.$ But why
$$W(S)=\{\langle Sx\;|\;x\rangle=\langle Tx\;|\;x\rangle:\;x \in M,\;\;\|x\|=1\}?$$ Thank you everyone !!","['functional-analysis', 'operator-theory', 'hilbert-spaces']"
2396826,Find a $f$ function such that$f'(x)\geq 0$ but not continuous,"I just started reading continuity, differentiability etc. So I was thinking of an example of following type: Let $f: [a,b]\rightarrow \mathbb{R}$ , where $f(x)$ is monotonically increasing continuous function and differentiable on $(a,b)$ or simply $f'(x)\geq 0$ for $x\in (a,b)$ . can we find such function for which $f'(x)$ is not continuous? I could not find any, whatever function I take $f'(x)$ is becoming continuous. Is there such function even exists!","['real-analysis', 'calculus', 'analysis']"
2396869,Describing the image and nullspace of a linear map,"Let $V$ be the space of all continuous functions on $[a,b$]. Define the map $T$ according to
$$T[f](x)=\int_a^b f(t) \sin(x-t) \mathrm{d} t. $$
I want describe the image and kernel (nullspace) of this map. 
In order to do this, I've first noticed that
$$T[f](x)=\left(-\int_a^b f(t) \sin(t) \mathrm{d} t \right) \cos(x)+\left( \int_a^b f(t) \cos(t) \right) \sin(x) .$$
This means that the image $T[V]$ is a subspace of the space spanned by $\{ \cos(x),\sin(x)\}$. The kernel consists of all functions $f$ such that
$$\int_a^b f(t) \sin(t) \mathrm{d}t=\int_a^b f(t) \cos(t) \mathrm{d} t=0. $$
I'd like some help on showing that $T[V]=\operatorname{span} \{\cos(x),\sin(x)\}$, and perhaps getting a clearer characterization of the kernel. Notice that if $[a,b]=[-\pi,\pi]$ we can use the orthogonality of $\sin$ and $\cos$, but I'm interested in a general interval. Thanks!","['linear-algebra', 'linear-transformations']"
2396899,"I am a physicist with a difficult equation (quadratic exponential) I am curious about. No luck with a lit review, sympy or Mathematica.","I have an equation I have been playing with (the variable is $x$ and the constants are positive and real): $$( r_1 - x )^2 \frac{  a  r_3  e^{x / r_2} + b  r_2  e^{x / r_3} }{\left( a  e^{ x / r_2} + b  e^{ x / r_3} \right)^2} = k.$$ Certain approximations can lead to an equation that looked better to me. It has the form $e^{2 x} +ae^x = bx +c $ where the symbols no longer have the same meaning. Without the $e^{2x}$ term I can find the solution easily with Lambert's W and it might be good enough for my purposes. Does anyone have experience with equations like these? Do you know of a resource that might show lots of examples of the W function in action? Or, do you have a reason why an analytical solution here is not reasonable and I should just settle for the numerical approach? What branch of advanced algebra would govern these kinds of problems? Any tips on keywords for my research? Like I said I'm a physicist and don't necessarily know the math search terms to use.","['problem-solving', 'transcendental-equations', 'functions']"
2396942,How to prove that $\mathrm{var}(X-E(X|Y)) \leq \mathrm{var}(X)$?,"I tried to solve this exercise but got stuck: Assume we have the random variables $X$ and $Y$ where $E(X) = 0$. How can we prove the following inequality $\operatorname{Var}  (X-E(X|Y)) \leq\operatorname{Var}(X)$? I tried to write out the rhs: $\operatorname{Var}(X-E(X|Y)) = \operatorname{Var}(X) + \operatorname{Var}(E(X|Y)) - 2\operatorname{Cov}(X,E(X|Y))$
Since $E(X) = 0$, we have $\operatorname{Var}(E(X|Y)) = E(E(X|Y)^2)$ and $\operatorname{Cov}(X,E(X|Y)) = E(X*E(X|Y))$. Thus we have $\operatorname{Var}(X) + E(E(X|Y)^2) - 2E(XE(X|Y)) \leq \operatorname{Var}(X)$ giving
$E(E(X|Y)^2) \leq 2E(XE(X|Y))$ But that did not get me anywhere as you can see, does anybody have any tips? OR do you think there might be a typo in the exercise?","['probability-theory', 'conditional-expectation', 'covariance']"
2396957,Sufficient condition for function to be monotonic increasing,"Suppose $X\sim \operatorname{Bin}(n,\theta)$ and define the function
$$
\lambda(x)=\frac{P(X=x,\theta>\theta_0)}{P(X=x,\theta\leq\theta_0)}
$$
for some constant $0<\theta_0<1$. Suppose also a prior distribution on the parameters, $\Theta\sim G$. I'm trying to find a condition on $\theta_0$ such that $\lambda(x)$ is monotonically increasing in $x$. \begin{align}
\nabla_x\lambda(x)&=\nabla_x\frac{\int_{\theta_0}^1 s^x(1-s)^{n-x}dG(s)}{\int_{0}^{\theta_0} t^x(1-t)^{n-x}dG(t)}\\
&=\nabla_x\frac{\int_{\theta_0}^1 s^x(1-s)^{n-x}dG(s)}{\int_{0}^{\theta_0} t^x(1-t)^{n-x}dG(t)}\\
&=\frac{\int_{0}^{\theta_0} t^x(1-t)^{n-x}dG(t)\int_{\theta_0}^1 s^x(1-s)^{n-x}\log\left(\frac{s}{1-s}\right)dG(s)-\int_{\theta_0}^1 s^x(1-s)^{n-x}dG(s)\int_{0}^{\theta_0} t^x(1-t)^{n-x}\log\left(\frac{t}{1-t}\right)dG(t)}{[\int_{0}^{\theta_0} t^x(1-t)^{n-x}dG(t)]^2}\\
\end{align} I can bring the differential operator inside the integral by dominated convergence, since the integrands are bounded by 1. Furthermore, the denominator is always greater than zero, so we can consider only the numerator. \begin{align}
\text{numerator}(\nabla_x\lambda(x))=&\int_{0}^{\theta_0} \int_{\theta_0}^1 s^x(1-s)^{n-x}\log\left(\frac{s}{1-s}\right)t^x(1-t)^{n-x}dG(s)dG(t)-\int_{0}^{\theta_0}\int_{\theta_0}^1 s^x(1-s)^{n-x} t^x(1-t)^{n-x}\log\left(\frac{t}{1-t}\right)dG(s)dG(t)\\
=&\int_{0}^{\theta_0}\int_{\theta_0}^1 s^x(1-s)^{n-x} t^x(1-t)^{n-x}\left[\log\left(\frac{s}{1-s}\right)-\log\left(\frac{t}{1-t}\right)\right]dG(s)dG(t)\\
=&\int_{0}^{\theta_0}\int_{\theta_0}^1 s^x(1-s)^{n-x} t^x(1-t)^{n-x}\log\frac{s/(1-s)}{t/(1-t)}dG(s)dG(t)
\end{align} This is where I got stuck. Even when I imposed a uniform distribution on $G$ (i.e. $dG(s)=ds$, $dG(t)=dt$), I had issues deriving a condition on $\theta_0$ such that $\text{numerator}(\nabla_x\lambda(x))>0$. I tried to use integration by parts for this specific case, but to no avail. Am I missing something fundamental in my derivation? EDIT Per @MANMAID's suggestion, I went through the algebra for $\lambda(x+1)-\lambda(x)$. For clear presentation, I set $G$ to be the uniform distribution. \begin{align}
\lambda(x+1)-\lambda(x)&=\frac{P(X=x+1,\theta>\theta_0)}{P(X=x+1,\theta\leq\theta_0)}-\frac{P(X=x,\theta>\theta_0)}{P(X=x,\theta\leq\theta_0)}\\
&=\frac{\int_{\theta_0}^1 s^{x+1}(1-s)^{n-x-1}ds}{\int_{0}^{\theta_0} t^{x+1}(1-t)^{n-x-1}dt}-\frac{\int_{\theta_0}^1 s^{x}(1-s)^{n-x}ds}{\int_{0}^{\theta_0} t^{x}(1-t)^{n-x}dt}\\
&=\frac{\int_{\theta_0}^1 s^{x+1}(1-s)^{n-x-1}ds\int_{0}^{\theta_0} t^{x}(1-t)^{n-x}dt-\int_{\theta_0}^1 s^{x}(1-s)^{n-x}ds\int_{0}^{\theta_0} t^{x+1}(1-t)^{n-x-1}dt}{\int_{0}^{\theta_0} t^{x+1}(1-t)^{n-x-1}dt\int_{0}^{\theta_0} t^{x}(1-t)^{n-x}dt}
\end{align} Again, considering only the numerator, as the denominator is positive: \begin{align}
\text{numerator}(\lambda(x+1)-\lambda(x))=&\left(\frac{\Gamma(x+2)\Gamma(n-x)}{\Gamma(n+2)}-B_{\theta_0}(x+2,n-x) \right)B_{\theta_0}(x+1,n-x+1) - \left(\frac{\Gamma(x+1)\Gamma(n-x+1)}{\Gamma(n+2)}-B_{\theta_0}(x+1,n-x+1) \right)B_{\theta_0}(x+2,n-x) \\
=&\frac{\Gamma(x+2)\Gamma(n-x)}{\Gamma(n+2)}B_{\theta_0}(x+1,n-x+1)-\frac{\Gamma(x+1)\Gamma(n-x+1)}{\Gamma(n+2)}B_{\theta_0}(x+2,n-x)\\
=&\frac{\Gamma(x+1)\Gamma(n-x)}{\Gamma(n+2)}\left\{(x+1)B_{\theta_0}(x+1,n-x+1)-(n-x)B_{\theta_0}(x+2,n-x) \right\}
\end{align} where $\Gamma(y)=(y-1)!$ for positive integer $y$ and $B_{\theta_0}(q,r)=\int_0^{\theta_0}u^{q-1}(1-u)^{r-1}du$. The last expression above is positive $iff$ the expression in $\{\cdot\}$ is positive. However, is there a way to simplify that condition? I've tried plugging in various values for $x$ and $n$, and it seems that the condition I need takes the form of $0<\theta_0<\frac{n-x-1}{n+2}$. However, this condition on $\theta_0$ depends on $x$, which is not really something that is desired.","['real-analysis', 'monotone-functions', 'statistics', 'probability-theory']"
2396973,Analogs of Cayley-Hamilton theorem for Pfaffian,"The Pfaffian $\text{pf}$ is defined for a skew-symmetric matrix which is also a polynomial of matrix coefficients. One property for Pfaffian is that $\operatorname {pf} (A)^{2}=\det(A)$ holds for every skew-symmetric matrix A . As for determinants we have Cayley-Hamilton theorem, here is my question:
\begin{align}
w &= \begin{pmatrix} -A \cdot a & -I \\ I & B \cdot b \end{pmatrix} \\
\operatorname{Pf}(w) &= \sum\limits_{i=0}^{\left\lfloor n/2 \right\rfloor} P_i \cdot a^i b^i \\
A \cdot B &= C
\end{align} Where $A,B \in M_n(\Bbb R)$ are skew-symmetric, and $a,b \in \mathbb R$.
$\text{Pf}(w)$ is expressed as polynomial of $ab$ ($P_i \in \mathbb R$ are coefficients). Then can we show the following vanishing property?
$$
\sum\limits_{i=0}^{\left\lfloor n/2 \right\rfloor} P_i \cdot C^{\left\lceil n/2\right\rceil - i} = 0
$$ My idea is that we can show the square of $P_iC^i$ is zero by Cayley-Hamilton theorem, but how to proceed? Last but not least, can we proof something more general about analogs of Cayley-Hamilton theorem for Pfaffian?","['abstract-algebra', 'pfaffian', 'linear-algebra', 'determinant']"
2396988,Rolling a dice 10 times and getting exactly r same roll.,"I am interested in the probability of rolling a six-sided dice 10 times and getting exactly r same roll. I wrote a simple brute-force Monte Carlo simulation program to get the probability, and I also calculated the probability using binomial distribution mass function. My Monte Carlo simulation flows like this:
-> roll a dice 10 times (an example roll: 2,1,3,4,3,6,5,3,3,2)
-> count the number of same rolls (for the example above, 2:2, 3:4)
-> extract the highest number of same rolls (for the example above, extract 4)
-> record in a list accumulating the counts (for the example above, the key ""4"" will increment by 1 since it's 1 case where the roll result in 4 of the same) 
-> repeat 100,000 times I used binomial mass function like this:
for example, getting exactly 8 of the same 6*10C8*(1/6)^8*(5/6)^2 = 0.0111% The Monte Carlo result and the binomial function result converge when the r is larger than 4, but diverge when r <= 4 . See below list of results: M.C. Result: {2: 6.7621, 3: 52.9124, 4: 31.0536, 5: 7.8071, 6: 1.3046, 7: 0.1483, 8: 0.0111, 9: 0.00046, 10: 1e-05} % Binomial Result: {2: 174.426, 3: 93.0272, 4: 32.5595, 5: 7.8143, 6: 1.3024, 7: 0.1488, 8: 0.0111, 9: 0.00050, 10: 9.922e-06} % I'm just wondering why it works up until 3 or 4...","['probability-theory', 'probability', 'probability-distributions']"
2397063,$\lim_{n\to\infty} \frac{\text {No. of elements of order 2 in }S_n}{\text {No. of elements of order 2 in} A_n}=?$,If $\alpha \in S_n$ be an element of order $2$ then $\alpha$ is product of disjoint cycles of lenght $2$.Moreover if $\alpha \in A_n$ then the number of two cycles is even.From this the no. of elements of order $2$ in $S_n$ and $A_n$ can be calculated.If $a_n$ and $b_n$ be the no. of elements of order $2$ in $S_n$ and $A_n$ respectively then it can be shown that $$a_n=\sum_{m=1}^{\lfloor n/2 \rfloor} \frac {n!}{2^m(n-2m)!}$$ and $$b_n=\sum_{m=1}^{\lfloor n/4 \rfloor} \frac {n!}{2^{2m}(n-4m)!}.$$ Now my question is what is $$\lim_{n\to\infty} \frac{a_n}{b_n}?$$ I have taken the values $n$ from $1$ to $20$ which suggest that this limit is $2$ but can't approach this analytically. Please help so solve this.,"['limits', 'sequences-and-series', 'combinatorics', 'group-theory', 'symmetric-groups']"
2397067,Improper integral of complex exponential,"I do not understand why the following holds $$\int_{0}^{+\infty}e^{i \alpha x }dx=-\frac{1}{i \alpha }   \,\,\,\,\, ,\,\,\,\, \alpha \in \mathbb{R}$$ I can see that $$\int_{0}^{+\infty}e^{i \alpha x }dx=\lim_{c \to  +\infty}\frac{1}{i \alpha }e^{i \alpha x}|_{0}^{c}=\lim_{c \to  +\infty}\frac{1}{i \alpha }e^{i \alpha c}-\frac{1}{i \alpha }$$
But I do not see how $$\lim_{c \to  +\infty}\frac{1}{i \alpha }e^{i \alpha c}=0$$","['complex-numbers', 'limits', 'complex-analysis', 'improper-integrals', 'integration']"
2397081,"Let $\{K_n \}_{n \in \mathbb{N}}$ be a collection of sets, where $K_n = [n, \infty)$. Why is $\bigcap_{n \in \mathbb{N}} K_{n}$ empty?","This question is related to Exercise 2.15 in Baby Rudin, in which the reader is instructed to ""[s]how that Theorem 2.36 and its Corollary become false (in $R^1$, for example) if the word 'compact' is replaced by 'closed' or by 'bounded.'"" The relevant theorem and corollary are as follows: Theorem 2.36: If $\{K_{α}\}$ is a collection of compact subsets of a metric space $X$ such that the intersection of every finite subcollection of $\{K_{α}\}$ is nonempty, then   $\{K_{α}\}$ is nonempty. Corollary: If $\{K_{n}\}$ is a sequence of nonempty compact sets such that $K_{n}$ contains $K_{n+1},  (n = 1, 2, 3, ...)$, then   $K_n$ is not empty. My Question: I realize this question and similar ones have been asked elsewhere on MSE, but I've yet to see a straightforward (to me, at least) answer to my question specifically. Usually, for the ""closed"" part of the exercise, the following collection of closed sets, $\{K_n \}_{n \in \mathbb{N}}$ is used: $$K_n = [n, \infty), n \in \mathbb{N}.$$ For, the intersection of every finite subcollection of ${K_n}$ is clearly nonempty. Indeed, $$ \bigcap_{i=1}^{m} K_{n_{i}} = [N, \infty) $$ where $N = \max\{n_1, n_2, ..., n_m\}$. Then, the assertion is made that $$ \bigcap_{n=1}^{\infty} K_n = \emptyset. $$ However, I don't see how the same argument can't simply be used, i.e., that, for example, $N \in  \bigcap_{n=1}^{\infty} K_n$ (or maybe, say, $n+1$). Intuitively, this certainly makes sense, I'd say. For, given the archimedean property, it seems that as $n$ approaches infinity, we can always find an $l$ in each increasingly smaller set such that $n < l < \infty$; and so $l \in  \bigcap_{n=1}^{\infty} K_n$. What is it exactly that I'm misunderstanding or overlooking here?","['general-topology', 'real-analysis', 'elementary-set-theory']"
2397108,Showing cardinality of all infinite sequences of natural numbers is the same as the continuum.,"So I'm trying to show that these two sets have the same cardinality, i.e. there is a possible bijection between the two. I'm trying to use the Cantor-Schroeder-Bernstein theorem as I can't explicitly think of a bijection that will work. For this I need to find an injective map each way. I can find an injective mapping between $R$ and the set of infinite sequences of natural numbers. E.g., for each real number $x$, I can associate it with the sequence $S$, where the first element is $2^{n}$ (where $n$ is the greatest integer less than or equal to $x$) if x is nonnegative, and $3^{n}$ if x is negative. Then the rest of the sequence can be the decimal expansion of x in single digits. E.g. the sequence associated with pi through the function would be $(2^{3}, 1,4, etc)$. This is injective, so the first half of Cantor-Schroeder-Bernstein is satisfied. If I can find an injective function going the other way then I am done. I first thought of something involving turning the digits of the sequence into a real number, but that failed to be injective as my function would not be able to differentiate between the sequences $(1,0,1,0,1,0,...)$ and $(10,10,10,...)$. Any hints or help? I'm trying to figure it out without resorting to proofs based on cardinal arithmetic or related to the power set of N.","['cardinals', 'elementary-set-theory']"
2397151,Prove there are singularities,"For all $n\in\mathbb{N}$ let $\displaystyle P_{n}(z)=\sum_{k=0}^{n}(-1)^k\frac{z^{2k+1}}{(2k+1)!}$ Prove: there is $n_0\in \mathbb{N}$ such that for all $n>n_0$ $P_{n}(z)$ has 7 zeros including multiplicity in in the circle $B(0,10)$ If we take $n\to\infty$ the function will be $\sin z$ or looking at some terms we get $\displaystyle z-\frac{z^3}{3!}+\frac{z^5}{5!}-\frac{z^7}{7!}$ which has one zero (?)",['complex-analysis']
2397182,Point in triangle with maximum distance from vertices,"Given a triangle with vertices $V_1$, $V_2$ and $V_3$, what is the point inside the triangle that is farthest away from any vertex (i.e. the point which maximizes the minimum distance to each vertex). My first thought is that it is the circumcenter for acute triangles and the midpoint of the longest edge for obtuse triangles, but I can't seem to prove this.","['trigonometry', 'geometry']"
2397228,Limit of $\int_0^1\frac{f(hx)}{x^2+1}dx$ when $h\to0$,"Let $f\in \mathcal{C}^0\big([0,1],\mathbb{R}\big)$ and, for every $h\in(0,1]$ , $$I(h)=\int_0^1\dfrac{f(hx)}{x^2+1}dx$$ For $\varepsilon >0$ , show there exists $\eta>0$ such that for every $h\in(0,\eta)$ , $$\left|I(h)-f(0)\frac{\pi}{4}\right|\leq \varepsilon$$ Since $\dfrac{\pi}{4} = \displaystyle\int_0^1\dfrac{1}{x^2+1}dx$ $$\left |\int_0^1\dfrac{f(hx)}{x^2+1}dx -\int_0^1\dfrac{f(0)}{x^2+1}dx \right|\leq\varepsilon$$ And now, I have got no idea how to solve this problem. I think, I should show that : $$|h|\leq \eta \implies \left|I(h)-f(0)\frac{\pi}{4}\right|\leq \varepsilon$$ but I'm not sure.","['continuity', 'real-analysis', 'integration']"
2397229,Expected value of $\max\limits_{1\leq i\leq m}X_i-\min\limits_{1\leq i\leq m} X_i$ for balls and bins problem,"Recently I came across the following problem which has occupied me for a few weeks. This is just out of curiosity, and also I can't find any research paper on this particular question . Problem : Given $N$ balls, each ball is placed in a box chosen uniformally at random among  $m $  numbered boxes, Let $X_i $ denote the random variable counting the number of balls in box $i$. Define $Y=\max\limits_{1\leq i\leq m}X_i-\min\limits_{1\leq i\leq m} X_i$.
  What is the expected value and the variance of the variable $Y$? I am looking for any reference or research paper providing an answer (approximations) to the problem when $\mathbf{N}$ is large and $m$ is constant.","['reference-request', 'probability-theory', 'balls-in-bins', 'probability-distributions', 'poisson-distribution']"
2397236,Automorphisms of punctured plane,"It is well known that set of automorphisms (bijective, conformal self maps) of $\Bbb{C}_{\infty}$ is $\dfrac{az+b}{cz+d},\,\,\,ad-bc\not=0$ (Möbius transformations), $\Bbb{C}$ is $az+b,\,\,\, a\not=0$ (scaling+rotating+translating), $\Bbb{H}$ is $\dfrac{az+b}{cz+d},\,\,\,ad-bc\not=0$ with $a,b,c,d\in\Bbb{R}$, $\Bbb{D}$ is $e^{i\theta}\dfrac{z-a}{1-\bar{a}z},\,\,\,|a|\lt1,\,\,\,\theta\in\Bbb{R}$, $\text{Ann}(0,r_1,r_2)$ is $e^{i\theta}z,\,\,\,\theta\in\Bbb{R}.$ Here I am trying to figure-out the automorphisms of punctured plane $\Bbb{C}\setminus\{0\}.$ My guess is it is the collection of functions of the form $az+\dfrac{b}{z},$ where $a,b$ are any two complex numbers with $|a|^2+|b|^2\not=0.$ Is it correct? If it is so, how can I prove this rigorously?","['mobius-transformation', 'complex-numbers', 'complex-analysis', 'geometry', 'conformal-geometry']"
2397249,The Coordinate Ring: Its Etymology,"Well, to quote from Wolfram MathWorld directly, Given an affine variety $V$ in the $n$-dimensional affine space $K^n$, where $K$ is an algebraically closed field, the coordinate ring of $V$ is the quotient ring $K[V] = K[x_1 , \dots , x_n] / I(V)$. My question is simply this, why is it called the ""coordinate ring""? In what sense does this ring give or can be said to define coordinates on the affine variety?","['affine-geometry', 'ring-theory', 'field-theory', 'algebraic-geometry']"
2397277,Proving that the pdf of reciprocal of random variable has zero density at the origin.,"Given a real-valued, continuous random variable $Y$ with an infinitely differentiable pdf $f_{Y}(y)$, define the reciprocal random variable
  $$
Z=\frac{1}{Y}.
$$ Using the univariate transformation formula it can be shown that the pdf of $Z, f_{Z}$, is
  $$
f_{Z}(z) = \frac{f_{Y}(z^{-1})}{z^{2}}.
$$ Under the above stated conditions, is $f_{Z}(0)=0$ generally true? My Approatch Intuitively, the only value $f_{Z}(0)$ could seemingly take on is $0$. So I attempted an initial start at a ""proof"" below... $$
f_{Z}(0)
  = \lim_{z\to 0} f_{Z}(z)
  = \lim_{z\to 0} \frac{f_{Y}(z^{-1})}{z^{2}}
$$ Let $t=1/z$, then $$
f_{Z}(0) = \lim_{t\to\pm\infty}t^{2} f_{Y}(t)
$$ Since $f_{Y}$ is a pdf, it must be a positive function with a finite integral meaning that $\lim_{t\to\pm\infty} f_{Y}(t)=0$. It can then be observed that,
$$
\lim_{t\to\pm\infty}t^{2} f_{Y}(t)=\infty\times 0,
$$
which is an indeterminate form. Therefore we rewrite the limit and apply L'Hospital's rule
$$
f_{Z}(0)
=\lim_{t\to\pm\infty}\frac{t^{2}}{f_{Y}^{-1}(t)}
=\lim_{t\to\pm\infty}-\frac{2t}{\frac{f_{Y}'(t)}{f_Y^2(t)}}
=\lim_{t\to\pm\infty}-\frac{2t f_Y^2(t)}{f_{Y}'(t)}
$$ Not sure where to go from here...","['probability-theory', 'probability-distributions', 'indeterminate-forms', 'limits']"
2397314,Trigonometry limit's proof: $\lim_{x\to0}\frac{\sin(x)+\sin(2x)+\cdots+\sin(kx)}{x}=\frac{k(k+1)}{2}$,"How to prove that $$\lim_{x\to0}\frac{\sin(x)+\sin(2x)+\cdots+\sin(kx)}{x}=\frac{k(k+1)}{2}$$
I tried to split up the fraction and multiple-divide every new fraction with its $x$ factor but didn't work out.
ex: $$\lim_{x\to 0}\frac{\sin(2x)}{x} = \lim_{x\to 0}\frac{\sin(2x)\cdot 2}{2\cdot x}=2$$","['summation', 'trigonometry', 'limits-without-lhopital', 'limits']"
2397366,"Let $H \leq G$ be a subgroup with finite index $n$. Show for every $g \in Z(G)$, $g^n \in H$.","Let $H \leq G$ be a subgroup with finite index $n$ . Show for every $g \in Z(G)$ , $g^n \in H$ . I am given a hint: Consider $C = \langle g \rangle$ and show the left multiplication action of $C$ on $G/H$ has orbits of size $|C/(C\cap H)|$ . Proof of hint: Let $g \in Z(G)$ and $C = \langle g \rangle$ act on $G/H$ by left multiplication.  For any $xH \in G/H$ , $(g^kx)H = x(g^kH)$ which shows $g^k \in \text{Stab}_C(xH)$ if and only if $g^k \in H$ .  By the orbit stabilizer theorem, $|\text{Orb}_C(xH)| = [C:\text{Stab}_C(xH)] = |C/(C\cap H)|$ . $\square$ I am having trouble proving the result from here.  I have not yet used the assumption that $H$ has finite index in $G$ which seems essential for finishing this proof.  Any help would be appreciated.","['abstract-algebra', 'group-theory']"
2397368,How can I prove that $(x+1)^x \leq x^{x+1}$ for all $x > 2$?,"I was playing around with this inequality for natural numbers, and it appears that for $x=1$ and $x=2$, the inequality is false, but for all other numbers, it seems to be true. I verified this graphically, but I'm still not 100% sure that it's true, i.e. if there's some very large number for which the inequality is false. Any ideas how to prove (or disprove + possibly find the counterexample) this?","['proof-writing', 'functions']"
2397426,About the second fundamental form,"Let $U\subset\mathbb R^3$ be an open set, and $f:U\to \mathbb R$ be a smooth function. Suppose that the level set $S=f^{-1}(\{0\})$ is non-empty, and that at each $p\in S,$ the gradient $\overrightarrow \nabla f(p)$ is not the zero vector. Then $S$ is a smooth two-dimensional surface in $U$, and $p\mapsto \overrightarrow \eta(p)=\frac{1}{||\overrightarrow \nabla f(p)||}\overrightarrow \nabla f(p)$ defines a smooth unit-length normal vector field along $S$. At each $x\in U,$ write $H(f)_{(x)}$ for the $3\times 3$ Hessian matrix specified by $$(H(f)_{(x)})_{ij}=\frac{\partial^2f}{\partial x_i\partial x_j}(x).$$ Show that , at each $p\in S$, the second fundamental form $II_p: T_p(s)\times T_p(s)\to \mathbb R$ is the symmetric bilinear map 
  $$II_p(\overrightarrow v,\overrightarrow w)=\frac{-1}{||\overrightarrow \nabla f(p)||}\overrightarrow v\cdot H(f)_{(p)}\overrightarrow w,$$for all $\overrightarrow v ,\overrightarrow w \in T_p(s)$. (Here, we view the tagent space $T_p(S)$ as the two-dimensional subspace $(span\{ {\overrightarrow \eta(p)}\})^{\bot}$ of $\mathbb R^3$. Edit: Actually my question is why the second fundamental form under the usual definition can be written in this way. Definition : The quadratic form $II_p$, defined in $T_p(S)$ by $II_p(v)=-<d  N_p(v),v>$ is called the second fundamental form of $S$ at $p$, where $dN_p:T_p(S)\to T_p(S)$ is the differential of the Gauss map. Hopefully, I express this problem explicitly. I was just wondering how to prove this statement. I took a diffrential geometry class last semester, and when I organized my notes this morning, I found this statement, but there was no proof... Looking forward to an understandable explaination. Thanks in advance. Edit 2 :Furthermore, show that , at each point $p\in S$, the expression
  $$\phi_p(z)=det\pmatrix{-H(f)_{(p)}-zI_{3\times 3} & \overrightarrow \nabla f(p)\\\ \pm \overrightarrow \nabla f(p)& 0}$$
  (the underlying matrix here is $4\times 4$) defines a second-degree polynomial whose roots $\lambda_1$ and $\lambda_2$ are $||\overrightarrow \nabla f(p)||k_1$ and $||\overrightarrow \nabla f(p)||k_2$, where $k_1$ and $k_2$ are the principal curvatures of $S$ at $p$. Also , if a non-zero vector $\pmatrix {\overrightarrow v \\c}$ lies in the kernel of the $4\times 4$ matrix $$\pmatrix{-H(f)_{(p)}-\lambda_jI_{3\times 3} & \overrightarrow \nabla f(p)\\\ \pm \overrightarrow \nabla f(p)& 0},$$
  then $\vec v$ is a non-zero element of $T_p(S)$ and lies in the ""principal direction"" corresponding to $K_j$.",['differential-geometry']
2397442,Inequality between Frobenius and nuclear norm,"Let $M$ be a square matrix, $\|\cdot\|_*$ be the nuclear (trace) norm, and $\|\cdot\|_F$ be the Frobenius norm. The following inequality holds between the norms: $$\|M\|^2_* \leq \text{rank}(M) \|M\|^2_F.$$ This is pretty easy to show by using the definitions of the norms in terms of the singular values $\sigma_i$, since $\|M\|_* = \sum_i \sigma_i$ and $\|M\|_F = \sqrt{\sum_i \sigma_i^2}$ and the result follows by Cauchy-Schwarz. However, out of curiosity I have been trying to prove this using the definitions of the norms as $\|M\|_* = \text{trace} (\sqrt{M^* M})$ and $\|M\|_F = \sqrt{\text{trace}(M^* M)}$. Can the above inequality be shown using these definitions and without invoking the singular values explicitly?","['alternative-proof', 'nuclear-norm', 'matrices', 'matrix-norms', 'linear-algebra']"
2397458,"Asymptotics of $\operatorname{agm}(1,x)$ when $x\to\infty$","I am interested in the asymptotics of $\newcommand{agm}{\operatorname{agm}}\agm(1,x)$ as $x\to\infty$ , where $\agm$ is the arithmetic geometric mean . I know that $$\sqrt x\le\agm(1,x)\le\frac{1+x}2$$ Thus, $\agm(1,x)\in\mathcal O(x)$ . I then noticed that $$\agm(1,x)=\agm\left(\sqrt x,\frac{1+x}2\right)=\sqrt x\agm\left(1,\frac{\sqrt x+\frac1{\sqrt x}}2\right)$$ For large $x$ , I imagine that we have $$\agm\left(1,\frac{\sqrt x+\frac1{\sqrt x}}2\right)\sim_\infty\agm\left(1,\frac{\sqrt x}2\right)$$ And if $\agm(1,x)\sim_\infty\alpha x^\epsilon$ , then $$x^\epsilon=x^{\frac12(\epsilon+1)}\\\epsilon=\frac12(\epsilon+1)\\\epsilon=1$$ Is this correct? And if so, how do I calculate $$\alpha=\lim_{x\to\infty}\frac{\agm(1,x)}x\ ?$$ It certainly appears to be the case that $\alpha<1$ , though I cannot conclude much more than that. Perhaps one might find the integral form to be useful: $$\agm(1,x)=\frac\pi{2I(1-x^2)}$$ where $$I(x)=\int_0^{\pi/2}\frac{dt}{\sqrt{1-x\sin^2(t)}}=\int_0^1\frac{dt}{\sqrt{(1-t^2)(1-xt^2)}}$$","['elliptic-integrals', 'asymptotics', 'sequences-and-series', 'analysis']"
2397460,Why is this symbolic derivative zero?,"I'm trying to compute the derivative of this function with respect to $\theta$ (i.e., $\frac{dv}{d\theta}$): $$
v(t) = m_2 a_1 a_2 \dot{\phi}(t)\dot{\theta}(t) \cos(\theta(t)-\phi(t))
$$ I thought the answer was: $$
\frac{dv}{d\theta} = -m_2 a_1 a_2 \dot{\phi}(t)\dot{\theta}(t) \sin(\theta(t)-\phi(t))
$$ But Matlab yields zero using this script: clear variables
clc
syms t a1 a2 m2 th(t) p(t) 
f = m2*a1*a2*diff(p,t)*diff(th,t)*cos(th-p);
v = sym('th');
dvdth = subs(diff(subs(f,th,v),v),v,th) Is there any problem with the preceding script?","['derivatives', 'matlab', 'symbolic-computation']"
2397468,The sum of odd powered complex numbers equals zero implies they cancel each other in pairs,"Show that if a set of complex numbers $z_1,z_2,\ldots,z_n$ satisfy 
$$z_1^l+z_2^l+\cdots+z_n^l=0$$ for every odd $l$, then for any $z_i$ we can always find some $z_j$ such that $z_i+z_j=0$. The question has been answered here for real numbers but not for complex numbers","['algebra-precalculus', 'induction', 'complex-numbers']"
2397476,The benefit of LU decomposition over explicitly computing the inverse,"I'm going to teach a linear algebra course in the fall, and I want to motivate the topic of matrix factorizations such as the LU decomposition. A natural question one can ask is, why care about this when one already knows how to compute $A^{-1}$ and can use it to solve $Ax=b$ just by matrix-vector multiplication? The standard answer is that is that in practice one should (almost) never actually invert a matrix , because you will incur less roundoff error by backsubstituting a factorization like $LUx = b$ than by performing the multiplication $x=A^{-1}b$. However, I recently came across the paper "" How accurate is inv(A)*b ? "" by Druinsky and Toledo (2012) where they argue that the received wisdom is misleading, and a solution $x_{\text{inv}}=Vx$ obtained using an approximate inverse $V\approx A^{-1}$ is typically just as close to the true $x$ as a solution $x_{\text{LU}}$ obtained by backsubstitution. So I am no longer as sure about numerical accuracy as a motivation for the LU decomposition as I used to be. I did a few numerical experiments with random ill-conditioned matrices in Matlab, based on Druinsky and Toledo's code in Sec. 6 of their paper. It seems like the forward errors $\|x_{\text{inv}}-x\|$ and $\|x_{\text{LU}}-x\|$ were indeed usually quite similar, but the backward error $\|Ax_{\text{inv}}-b\|$ could be much bigger than $\|Ax_{\text{LU}}-b\|$. I also worked out a tiny example by hand:
$$\begin{align}
A &= \begin{bmatrix}1.00&2.01 \\ 1.01 & 2.03\end{bmatrix} \\
&= \underbrace{\begin{bmatrix}1.00 & 2.01 \\ 0 & -1.00\times10^{-4}\end{bmatrix}}_{L}\underbrace{\begin{bmatrix}1 & 0 \\ 1.01 & 1\end{bmatrix}}_{U} \\
&= {\underbrace{\begin{bmatrix}-2.03\times10^4 & 2.01\times10^4 \\ 1.01\times10^4 & -1.00\times10^4 \end{bmatrix}}_{A^{-1}}}^{-1},
\\
b &= \begin{bmatrix}1.01 \\ 1.02\end{bmatrix}.
\end{align}$$
The exact solution is $x=[-1, 1]^T$. Computing $A^{-1}b$ with three decimal digits of precision in the intermediate calculations yields the result $x_{\text{inv}} = [0, 0]^T$ due to catastrophic cancellation. The same precision in LU backsubstitution gives $x_{\text{LU}} = [1.01, 0]^T$. Both solutions clearly differ from the true solution $x$ by an error on the order of $10^0$, but $Ax_{\text{LU}}$ matches $b$ to numerical precision while $Ax_{\text{inv}}$ is totally off. My questions are: Is the explicit $2\times2$ example above representative of what happens in practice, or is it a meaningless example computed with too little precision that just coincidentally happens to match the other numerical experiments? Are the numerical observations generally true? To make this precise, let $A=LU$ and define $V = U^{-1}*L^{-1}$, $x_{\text{inv}} = V*b$, and $x_{\text{LU}}= U^{-1}*(L^{-1}*b)$, where $*$ denotes multiplication with numerical roundoff error. Is it usually the case that $\|Ax_{\text{inv}}-b\|>\|Ax_{\text{LU}}-b\|$? What are some examples of practical applications where forward error is more important than backward error, and vice versa?","['numerical-linear-algebra', 'matrices', 'matrix-decomposition', 'numerical-methods', 'linear-algebra']"
2397534,Putnam Pigeonhole question,"Let $X = \{x_1, x_2, \ldots, x_m\}$ be a set of $m$ positive integers, all less than or equal to $n$, and let $Y = \{y_1, y_2, \ldots, y_n\}$ be a set of $n$ positive integers, all less than or equal to $m$. Prove that there is a nonempty subset of $X$ and a nonempty subset of $Y$ with the same sum. The ''informal'' solution given was: Imagine that the elements of $X$ are white stones, with weights $x_1,x_2,\ldots,x_m$, and that the elements of $Y$ are black stones, with weights $y_1,y_2,\ldots,y_n$. We also imagine a large balance-type scale, with two pans balancing across a fulcrum. Our goal is to put some number of white stones in the left pan and some number of black stones in the right pan, so that the two sides balance. We use what is essentially a “greedy” algorithm: start by placing a white stone in the left pan, and then at each step, add a stone to whichever side is lighter, and keep doing this until we run out of stones. Either at some intermediate point the scales will be in balance (in which case we’re done), or we’ll run out of stones. But the bounds on the weights of the individual stones mean that the scale can never be out-of-balance by more than $n$ on the left side or by more than $m-1$ on the right side. (It can never be out of balance by $m$ on the right, since we started with a white stone in the left pan.) Since there are $m+n$ total stones and only $m+n-1$ different ways that the scale can be out of balance, we may conclude by the Pigeonhole Principle that if we run out of stones, the scale must have been out of balance by the same amount at two different times. But this means that all the stones that we added between these two times must balance evenly, thus solving the problem. How does the scale balancing twice evenly solve the problem?","['pigeonhole-principle', 'combinatorics', 'contest-math']"
2397547,"Prove that the interval $ \ [0,2)$ and $ \ [5,6) \cup [7,8)$ have the same cardinality","Question: Prove that the interval $[0,2)$ and $[5,6) \cup [7,8)$ have the same cardinality by constructing a bijection between the two sets. You can add a graph to support your argument. I tried graphing a function with domain $[0,2)$  and target space $[5,6) \cup [7,8)$ but I can't come up with an explicit formula. Will the function be linear?","['elementary-set-theory', 'cardinals', 'functions', 'graphing-functions']"
2397564,"Prove that if $ \ A\cup B \subseteq C \cup D,\ A \cap B =$ ∅ $\land \ C \subseteq A \implies B \subseteq D.$","Question: Prove that if $ \ A\cup B \subseteq C \cup D,\  A \cap B =$ ∅ $\land \ C \subseteq A \implies B \subseteq D$. My attempt: Let $ \ x\in B \implies x \in A \cup B \implies x \in C \cup D \because A\cup B \subseteq C \cup D$. Now, $ x \in C \lor x\in D$. If $\  x \in C \implies x \in  A \because C \subseteq A$. But that's not possible $\because x \notin A \cap B$, in particular $ x \notin A$. So we must have $ x \in D$. I found this proof a little challenging. Not quite sure if this is the correct way to prove it. Is my logic correct?","['elementary-set-theory', 'proof-verification']"
2397605,Sum of vector subspaces,"I am trying to self study linear algebra and am stuck on a problem. It comes from Axler's Linear Algebra done right example 1.38 and I don't understand the solution that I could find online. Suppose that $U=\{(x,x,y,y)\in F^4 : x,y \in F\}$ and $W=\{(x,x,x,y) \in F^4 : x,y \in F\}$, show that $U+W=\{(x,x,y,z) \in F^4 : x,y,z \in F\}$. Its stated that the sum of vector subspaces is the set of all possible sums of the elements but how does $x+y=y$ and $y+y=z$",['linear-algebra']
2397654,"If $(M,d)$ has an uncountable discrete subset then $M$ is not separable?","I was thinking a bit about the proof of the fact that $\ell^{\infty}$ is not separable. And from the proof I saw, which uses a subspace which is discrete and uncountable, I thought I can prove it for any space which has the property. Let's take $M$ a metric space with a discrete subspace $X$. Let $D$ be a dense subset of $M$. For every $x\in X$, we consider the ball $B_x=B_{\frac{1}{3}}(x)$. Then since $D$ is dense, for every such $x$, $B_x\cap D\neq \varnothing$, so let's take an arbitrary fixed element $a(x)\in B_x\cap D$ for each $x$. Since $x\neq y\Rightarrow B_x\cap B_y=\varnothing$ (because $d(x,y)=1$), so $(B_x\cap D)\cap (B_y\cap D)=\varnothing\Rightarrow a(x)\neq a(y)$, so $x\mapsto a(x)$ is injective, thus $D$ is uncountable and $M$ is not separable. Is that right? Edit: Now I see that this can be solved using this since the subset $D$ would not be separable.","['real-analysis', 'metric-spaces', 'separable-spaces']"
2397664,Prove or disprove $ \ (A \times A) - (B \times B) = (A-B) \times (A-B)$,"Question: Let $ A,B$ be sets. Prove or disprove: $ \ (A \times A) - (B \times B) = (A-B) \times (A-B)$ My attempt: Let $ \ (x,y) \in (A \times A) - (B \times B) \implies (x,y) \in (A \times A)$ and $ \ (x,y) \notin (B \times B) \implies x \in A $ and $ \ x\notin B$ and $ \ y \in A$ and $ \ y \notin B \implies (x,y) \in (A-B) \times (A-B)$ Let $ \ (x,y) \in (A-B) \times (A-B) \implies x \in A$ and $ x \notin B$ and $ \ y \in A$ and $ \ y \notin B \implies (x,y) \in (A \times A)$ and $ \ (x,y) \notin (B \times B) \implies (x,y) \in (A \times A) - (B \times B)$. Is this approach correct?","['elementary-set-theory', 'proof-verification']"
2397676,What is the sum (in base 10) of all the natural numbers less than 64 which have exactly three ones in their base 2 representation?,"What is the sum (in base 10) of all the natural numbers less than 64 which have exactly three ones in their base 2 representation? The numbers will have at most $6$ digits in their binary representation and let ﬁrst digits can be $0$ , hence all numbers can be made six digit numbers. The number of numbers with exactly $3$ ones $= 6C3= 6·5·4 6 = 20$. Each number has $3$ ones and $3$ zeros. So totally out of $20·6$ digits $20·3$ one’s are there. So there are exactly $20*3/6$ = $10$ one's in each place. I don't know how I can proceed for here!","['combinatorics', 'contest-math', 'binary']"
2397677,"$f\left(\frac{2z}{1+z^2}\right)=\left(1+z^2\right)f(z)$, solve $f$.","Problem Given $f$ is smooth and defined on $(-1,0)\cup (0,1)$ , $\lim_{z\to 0}f(z)$ exists and it satisfies $$f\left(\frac{2z}{1+z^2}\right)=\left(1+z^2\right)f(z).\tag {*}$$ Solve $f$ . First attempt I tried to extend $f$ to all real number arguments by substituting $z=\frac{1}{t}$ , $$f\left(\frac{2t}{1+t^2}\right)=\left(1+\frac1{t^2}\right)f\left(\frac1t\right).\tag {1.1}$$ Therefore, $\forall z\in(-\infty,0)\cup (0,\infty)$ , $$f(z)=\frac1{z^2}f\left(\frac1z\right).\tag {1.2}$$ Let $g(z)=zf(z)$ , $$g(z)=g\left(\frac1z\right).\tag {1.3}$$ How can this be continued? Second attempt I notice $$\tanh2x=\frac{2\tanh x}{1+\tanh^2x}.\tag {2.1}$$ Let $g(z)=zf(z)$ again, then $$g\left(\frac{2z}{1+z^2}\right)=2g(z).\tag {2.2}$$ Put $z=\tanh x$ , $$g(\tanh2x)=2g(\tanh x).\tag {2.3}$$ Write $h(x)=g(\tanh x)$ , $$h(2x)=2h(x).\tag {2.4}$$ $$\therefore h(x)=cx, \forall c\in \mathbb R.$$ $\therefore g(x)$ is the inverse of $\tanh\dfrac xc$ . $$\therefore f(z)=\frac{c \text{ arctanh } z}{z}, \forall c\in \mathbb R.\tag {**}$$ As putting $\tanh x$ is not the general method, I want to see other ways of solving the problem. Third attempt $$f\left(\frac{2z}{1+z^2}\right)=\left(1+z^2\right)f(z).$$ Let $h(z)=zf(z)$ , $$h\left(\frac{2z}{1+z^2}\right)=2h(z).\tag {3.1}$$ Assume the power series $$h(z)=c_0+\sum_{k=1}^\infty c_k z^k.\tag {3.2}$$ I try to find the coefficients of $h$ . Then $$h\left(\frac{2z}{1+z^2}\right)=c_0+\sum_{k=1}^\infty c_k \left(\frac{2z}{1+z^2}\right)^k.\tag {3.3}$$ $$h\left(\frac{2z}{1+z^2}\right)=c_0+\sum_{k=1}^\infty c_k 2^k z^k \sum_{n=0}^\infty \binom{n+k-1}{n} \left(-z^2\right)^n$$ $$h\left(\frac{2z}{1+z^2}\right)=c_0+\sum_{k=1}^\infty \sum_{n=0}^\infty (-1)^n 2^k c_k \binom{n+k-1}{n} z^{2n+k}.\tag {3.4}$$ Let $m=2n+k$ , since $n\in\left[0,\lfloor{m-1\rfloor}/2\right]$ , $$h\left(\frac{2z}{1+z^2}\right)=c_0+\sum_{m=1}^\infty \sum_{n=0}^{\lfloor{m-1\rfloor}/2} (-1)^n 2^{m-2n} c_{m-2n} \binom{m-n-1}{n} z^m.\tag {3.5}$$ Claim 1: $$\forall k\in \mathbb N,c_{2k}=0.$$ The statement can be proved by mathematical induction. Compare constant term on both sides in $(3.1)$ , $c_0=2c_0\Rightarrow c_0=0.$ From $(3.5)$ , since $c_{2k+2}$ is derived from the linear combination of $c_0,c_2,c_4,\cdots,c_{2k}$ , it follows that $c_{2k+2}=0$ . Claim 1 is established. Claim 2: $$\forall k\in \mathbb N,c_{2k+1}=\frac{c_1}{2k+1}.$$ How can Claim 2 be proven? Fourth attempt I think of the following equation and try to solve it: $$g\left(\frac{x+y}{1+xy}\right)=g(x)+g(y).\tag {***}$$ We can arrive $(2.2)$ by setting $x=y=z$ . Let $W(x,y)=\frac{x+y}{1+xy}$ , $R(u,v)=u+v$ . Rewrite $(***)$ as $$g\left(W(x,y)\right)=R\left(g(x),g(y)\right).\tag {4.1}$$ Could $g$ in $(4.1)$ be solved through partial derivatives ? Apart from above attempts, any other thoughts? Thanks.","['functions', 'functional-equations']"
2397702,Why $f_x(x)=\frac1{3x^{2/3}}$ is not continuous at $x=0$?,"Im reading a book of differential equations and at some point it says that the function defined by $f_x(x)=\frac1{3x^{2/3}}$ is not continuous at $x=0$, but $x=0$ is not a point of the domain of $f_x$ because the function is not defined at it. Then it is correct to say that if a function is not defined at some point then it is not continuous at this point either? Background: the function comes from the initial value problem defined by $$x'=x^{1/3}=:f(x),\quad x(0)=0\tag1$$ From here the book used the partial derivative of $f$ respect to $x$, that is $$f_x(x):=\partial_x f(x)=\frac1{3x^{2/3}}$$ and the fact that $f_x$ is not continuous at $x=0$ is used to show that, from some theorem about uniqueness of solutions, that the solutions to $(1)$ could be not unique, in this case it is shown that there are infinite solutions.","['continuity', 'ordinary-differential-equations']"
2397733,Is there an analytic solution for the equation $\log_{2}{x}+\log_{3}{x}+\log_{4}{x}=1$?,"I am looking for a close form solution for below equation. $$\log_{2}{x}+\log_{3}{x}+\log_{4}{x}=1.$$ I solve it by graphing, but I don't know is there a way to find $x$ analytically  ?","['logarithms', 'exponential-function', 'calculus', 'proof-verification', 'algebra-precalculus']"
2397738,Two Polynomials,"I have troubling to prove this problem without any calculus Tools. It's an Olympiad Problem for High school students which says : Is there a pair of real numbers $ (a,b) $
Such that the two polynomials $ P(x) = (x-a)^3+(x-b)^2+x $ $ , $ $ Q = (x-b)^3+(x-a)^2+x $ have all their roots in real numbers? Can you help me? ( There's a simple solution that I came up with using derivative. But I need some simple proof which doesn't use calculus. )","['polynomials', 'roots', 'functions', 'algebra-precalculus', 'contest-math']"
2397750,Orthogonal projection of point onto line not through origin,"What is the orthogonal projection of the point $(8,3)$ onto the line $y = -2x - 3$? Using basic analytical methods, I get the point $(-4/5,-7/5)$, but I want to obtain this point by seeing what happens to the standard basis vectors. But I always end up with the matrix
$$
\begin{bmatrix}-1&-\frac{8}{5}\\-1&\frac{1}{5}\end{bmatrix},
$$
and
$$
\begin{bmatrix}-1&-\frac{8}{5}\\-1&\frac{1}{5}\end{bmatrix}\begin{bmatrix}8\\3\end{bmatrix}
=\begin{bmatrix}-\frac{64}{5}\\-\frac{37}{5}\end{bmatrix}
\neq\begin{bmatrix}-\frac{4}{5}\\-\frac{7}{5}\end{bmatrix}.
$$
Can anyone tell me where I'm going wrong?","['projection', 'linear-algebra']"
2397758,$(x_{n})$ is a real sequence with $x_{n}\ne0$. Assume $\lim\frac{x_{n+1}}{x_{n}}=\ell$. Show $|\ell|<1\implies\lim{x_{n}}=0$.,"The book's (A Problem Book in Real Analysis by Aksoy and Khamsi) solution goes like this: $\lim\frac{x_{n+1}}{x_{n}}=\ell\implies\lim\left|\frac{x_{n+1}}{x_{n}}\right|=|\ell|$. So given $\varepsilon=\frac{1-|\ell|}{2}$, there exists $N_{0}\ge1$ such that for any $n\ge N_{0}$ we have $$\left| \frac{|x_{n+1}|}{x_{n}}-|\ell|\right|<\varepsilon\\\implies|\ell|-\varepsilon<\frac{|x_{n+1}|}{|x_{n}|}<|\ell|+\varepsilon$$
Then by definition of $\varepsilon$ we have $$\frac{|x_{n+1}|}{|x_{n}|}<\frac{|\ell|+1}{2}<1 \ \ \ \ \ \ \ (*)$$
For any $n\ge N_{0}$ $$|x_{n+1}|<\left(\frac{|\ell|+1}{2}\right)^{n-N_{0}+1}|x_{N_{0}}| \ \ \ \ \ \ \ (**)$$ Then $$\lim\left(\frac{|\ell|+1}{2}\right)^{n-N_{0}+1}=0\implies\lim|x_{n}|=0\implies\lim x_{n}=0$$ What I don't get: 1) How did they come up with $\varepsilon=\frac{1-|\ell|}{2}$? By $(*)$ it looks like they wanted something that when added to $|\ell|$ is $<1$, but I don't know why. 2) By multiplying $(*)$ by $|x_{n}|$ I see you can get something that looks like $(**)$, but how do you know $\left(\frac{|\ell|+1}{2}\right)^{n-N_{0}+1}|x_{N_{0}}|$ won't be smaller than $|x_{n+1}|$? How they did they know to raise $\frac{|\ell|+1}{2}$ to the power $n-N_{0}+1$? 3) Why does $0\le |x_{n+1}|<\text{sequence that converges to 0}$ allow us to conclude $\lim|x_{n}|=0$? How did they even know $\lim|{x_{n}}|$ exists? Thanks in advance for any help.","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'limits']"
2397768,Lax-Milgram theorem on Banach space,"Lax-Milgram theorem states that If $B(,)$ is a symmetric,strictly positive and bounded bilinear form on Hilbert space $V$, then for any continuous functional $l$, there exists $u\in V$ s.t.
$B(u,v)=l(v)$. I am wondering if this result can be extended to the case of Banach space,i.e. $B$ is defined on a Banach space $V$. By the condition of strictly positive and boundededness, we know that the topology of the Banach space is the same as the topology defined by bilinear form $B$, then we can use Riesz reprensentation theorem to prove it.",['functional-analysis']
2397782,Evaluate $\lim_{x \to \infty}{\frac{1}{2^x-5^x+3^x}}$. Why is my solution wrong?,"Evaluate $$\lim_{x \to\infty}{\frac{1}{2^x-5^x+3^x}}. $$ My attempt: $$\lim\limits_{x \rightarrow \infty}{\frac{1}{2^x-5^x+3^x}}
= \lim\limits_{x \rightarrow \infty}{\frac{1}{2^x} \cdot\frac{1}{1-\frac{5^x}{2^x}+\frac{3^x}{2^x}}} \\= \lim\limits_{x \rightarrow \infty}{\frac{1}{2^x} \cdot\frac{1}{\frac{1}{5^x}-\frac{5^x}{10^x}+\frac{3^x}{10^x}}\cdot\frac{1}{5^x}} = 0 
\cdot\infty \cdot 0 = 0.$$ Someone told me that it is a wrong way to show the limit. Can anyone explain why?","['real-analysis', 'indeterminate-forms', 'limits']"
2397794,How do I prove or disprove this matrix identity: $ABA=0$ and $B$ is invertible implies $A^2=0$?,"Let us have two $n \times n$ matrices $A$ and $B$ with real entries. Either prove, or disprove by providing a counterexample, that if $ABA=0$ and $B$ is invertible, then $A^2=0$. My attempt: I could not find a counterexample so I assume the statement is true and then try to prove it. I first assume that AB=0, then $ABB^{-1}=0$ so $A=0$, which then gives the required result. A similar result is given by assuming $BA=0$. Next I assume $AB\neq0$,i.e. $AB=C$ where $C$ is some $n\times n$ matrix such that $CA=0$. However, I am stuck at this point because I believe I can show that the matrices $A$ and $C$ are non-invertible, and I'm afraid I don't really know how to proceed. I would really appreciate it if someone could give me a hint on how to prove or disprove this statement, or point out a flaw in my attempt.","['matrices', 'abstract-algebra', 'examples-counterexamples', 'nilpotence']"
2397817,"2nd derivative test , Maximum point","A study shows that a human body reaction R to a dosage D of a certain drug is given by $$ R = D^2(\frac{k}{2} - \frac{D}{3}) $$ Show that he rate of change in the reaction $R$ with respect to the dosage $D$ is maximum if $D = K/2 $ My first derivative is $R' = KD - D^2 $, Dosage for maximum reaction = $k$ My second derivative is $R''= K - 2D $ $R(\frac{k}{2}) = k - 2(\frac{k}{2}) = 0 $ But 0 is neither negative or positive , how do I show that it's a maximum (negative) ?","['derivatives', 'calculus']"
2397901,"Showing that if $x$ is a limit point of $A$ in a $T_1$ space, then every neighborhoood of $x$ contains infinitely many points of $A$?","Let $X$ be a space satisfying the $T_1$ axiom, and let $A$ be a subset of $X$. Then the point $x$ is a limit point of $A$ if and only if every neighborhood of $x$ contains infinitely many points of $A$. Here is my attempt at one part of this proof. Suppose $x$ is a limit point of $A$. This means that for if we take some neighborhood $U$ of $x$, then $U$ intersects $A$ at at least one point other than $x$ itself. Let $y_1 \in U\cap A$, with $y_1\neq x$. Is $\{y_1\} \cup \{u\} = U?$ No, because it is union of closed sets and $U$ is open. So take another point $y_2$ and check if $\{y_1\} \cup \{y_2\} \cup \{u\} = U?$. Again, no, because it is a union of closed sets. Ultimately, this means we need to take an infinite number of closed point sets in order that their union equals $U$. Is this argument correct?","['general-topology', 'separation-axioms', 'proof-verification']"
2397944,How to evaluate $\binom{2}{2}\binom{10}{3} + \binom{3}{2}\binom{9}{3} + \binom{4}{2}\binom{8}{3} + \ldots + \binom{9}{2}\binom{3}{3}$,"I really can't understand how to approach exercise 41. I tried by comparing the 5th term of binomial expansion and something like that, but getting 12C5, I think the answer should be 13C6. How to approach?","['combinatorics', 'binomial-theorem', 'binomial-coefficients']"
2397965,Rational Classes (Conjugacy Classes),"Quick one. If I'm not mistaken, the columns of a character table represent the conjugacy class associated with a group. When we talk of the ""rational classes"" of the group, is this related to the entries of the columns of the character table being rational ($\in \mathbb{Q}$)? The reason why I ask this is because this document here: ( http://www.math.colostate.edu/~hulpke/lectures/666/hw10.pdf ) says that the conjugacy classes 2A, 4A, 11A (in ATLAS notation) are rational classes (see q40). But if one is to look at the character table of M11 (simply using GAP) we see that the columns of 11A are not rational? Perhaps I am mistaken. Thank you for your help.","['abstract-algebra', 'characters', 'group-theory', 'representation-theory', 'gap']"
2397980,Prove $\sum_{n=1}^\infty\frac{(-1)^n}{\sqrt{n}}\arctan(\frac{x}{\sqrt{n}})$ converges and defines a continuously differentiable function,"Prove $\sum_{n=1}^\infty\frac{(-1)^n}{\sqrt{n}}\arctan\big(\frac{x}{\sqrt{n}}\big)$ converges for all $x,$ and defines a continuously differentiable function on $\mathbb{R}.$ By Leibniz test, the series converges for all $x.$ For $x=0,$ we get series of $0$'s, and the series converges. Let us show the series of the derivatives converges uniformly: $$\bigg(\sum_{n=1}^\infty\frac{(-1)^n}{\sqrt{n}}\arctan\bigg(\frac{x}{\sqrt{n}}\bigg)\bigg)'=\sum_{n=1}^\infty \frac{(-1)^n}{\sqrt{n}\sqrt{n}\big(1+\frac{x^2}{n}\big)}=\sum_{n=1}^\infty \frac{(-1)^n}{x^2+n}$$ $\sum(-1)^n$ is bounded uniformly and $\frac{1}{x^2+n}$ decreasing to $0,$ so by Dirichlet's test the series of derivatives converges uniformly. By term-by-term differention theorem we conclude the original series is continuously differentiable. Is that correct?","['derivatives', 'real-analysis', 'sequences-and-series', 'uniform-convergence']"
2398015,"Show that $[1/2, 1] \not\in \mathscr{T}_{1Y}$.","Background Let $X$ be the set of real numbers, and let $Y = (0,1]$.  If $\mathscr{T}_1$ is the standard topology on $\mathbb{R}$, and $\mathscr{T}_2$ is the lower limit topology on $\mathbb{R}$, then $Y$ as a subspace of $(\mathbb{R}, \mathscr{T}_1)$ has as topology
$$\mathscr{T}_{1Y}=\left\{(0,1] \cap \mathscr{O} \text{ }| \text{ } \mathscr{O} \in \mathscr{T}_1 \right\},$$
while $Y$ as a subspace of $(\mathbb{R}, \mathscr{T}_2)$ has as topology
$$\mathscr{T}_{2Y}=\left\{(0,1] \cap U \text{ }| \text{ } U \in \mathscr{T}_2 \right\}.$$ Note that
$$[1/2, 1] = (0, 1] \cap [1/2, 2) \in \mathscr{T}_{2Y}$$
since $Y=(0,1]$ and $[1/2, 2) \in \mathscr{T}_2$. Problem Statement Show that $[1/2, 1] \not\in \mathscr{T}_{1Y}$. Attempt In $\mathscr{T}_{1Y}$:
$$(0,1] \cap \mathscr{O} = (0,1] \cap \left(\bigcup_{i \in I}(a_i,b_i)\right) = \bigcup_{i \in I}\bigg((0,1] \cap (a_i,b_i)\bigg)$$ But we have the following cases to consider:
$$(0,1] \cap (a_i,b_i) =
\begin{cases}
(0, 1] \text{  if  } a_1 \leq 0 \land 1 < b_1 \\
(0, b_i) \text{  if  } a_i \leq 0 \land b_i < 1 \\
(a_i, b_i) \text{  if  } 0 \leq a_i < b_i < 1 \\
(a_i, 1] \text{  if  } 0 \leq a_i \land b_i > 1 \\
\end{cases}
$$
Consequently, for each $i \in I$,
$$[1/2, 1] \not\in (0,1] \cap (a_i,b_i).$$ Added August 18 2017: Should the last line be
$$[1/2, 1] \neq (0,1] \cap (a_i,b_i)?$$
Thus,
$$[1/2, 1] \notin \bigcup_{i \in I}\bigg((0,1] \cap (a_i,b_i)\bigg) = \mathscr{T}_{1Y}.$$ Question Is this proof correct ?  Is it also adequate ?","['general-topology', 'proof-verification']"
2398017,Is $\sum_{k=1}^n \cos \sqrt k= o(n)$ as $n\to \infty$?,"In the solution @Jack D'Aurizio: gave the estimate $$\sum_{k=1}^{n} \cos k^2 = \mathcal{O}(\sqrt{n}\log n)$$ and mentioned the Weyl's inequalities $$\sum_{k=1}^n \cos( f(k) ) = \mathcal{O}(F(n))$$ if $f$ is polynomial function. I wonder if one can give some nontrivial estimates for other kind of functions, for instance $$\sum_{k=1}^n \cos \sqrt{k} = \mathcal{O}(F(n))$$ for some $F(n) = \mathcal{o}(n)$ .","['real-analysis', 'asymptotics']"
2398022,"Vanishing of Tor on $\mathbb{C}[x,y]$","Let $I\unlhd \mathbb{C}[x,y]$. Then is it possible that 
$$Tor_1^{\mathbb{C}[x,y]}\left(\frac{\mathbb{C}[x,y]}{(x)},\frac{\mathbb{C}[x,y]}{I}\right)=0$$
for all $I$? I'm really not too sure what to do here, I thought I'd just see if anyone had any intuition?","['abstract-algebra', 'ring-theory', 'homological-algebra']"
