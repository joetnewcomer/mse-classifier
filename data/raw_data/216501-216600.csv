question_id,title,body,tags
4402839,"Proving a bounded function is integrable on $[a,b]$","I need to prove that Let $f:[a,b] \to \mathbf{R}$ be a bounded map and let $f$ be an integrable map on the interval $[c,b]$ for all $c \in (a,b).$ Then, $f$ is integrable on $[a,b].$ My attempt: By hypotesis, $f:[a,b] \to \mathbf{R}$ is bounded, so there exists $0 < K \in \mathbf{R}$ such that $|f(x)| \leq K$ , for all $x \in [a,b].$ Let $\varepsilon>0,$ and take $c \in (a,b)$ such that $K \cdot (c-a) \leq \frac{\varepsilon}{4}$ . By hypotesis, $f$ is integrable on $[c,b],$ then, there exists a partition $\left\{t_1, \dots, t_n \right\} \subset [a,b]$ such that $\sum_{i=2}^{n} \omega_i(t_i-t_{i-1}) < \frac{\varepsilon}{2}$ . Let's make $t_{1}=a$ , then we get  a partition $\left\{t_0, t_1, \dots, t_n \right\}$ of $[a,b]$ . Now I need to prove that $\omega_0 \leq 2K$ , for me to get that $f$ is integrable on $[a,b]$ . Am I following the correct idea? I don't know how to procceed. Any ideas would be appreciated!","['integration', 'definite-integrals', 'analysis', 'real-analysis', 'calculus']"
4402845,Representations and central extensions of $A_5$,"I'm taking a course on representation theory this semester, and the professor assumed we had some knowledge in central extensions. Unfortunately, I'm not very familiar with the topic, and I'm having some trouble really grasping it. I'm not entirely sure what the use of such an object is, or what the relationship between the center and the central extension is. On our homework, we have a few questions related to finding irreducible representations of the central extension of the alternating group of order $5$ , $A_5$ . I've asked the professor about how to find this central extension and its representations, and he's explained it to me, but I don't fully understand what exactly he is doing. Effectively, he says the central extension $\tilde A$ is a short exact sequence of the form $$ 1 \rightarrow \mathbb{Z}_{2} \rightarrow \tilde A \rightarrow A_5 \rightarrow 1 $$ which is in some way related to the central extension of $\mathbb{Z}_{2} \times \mathbb{Z}_{2}$ as $Q_{8}$ , the quternion group in the short exact sequence: $$ 1 \rightarrow \mathbb{Z}_{2} \rightarrow Q_{8} \rightarrow \mathbb{Z}_{2} \times \mathbb{Z}_{2} \rightarrow 1 $$ I'm not sure how these two short exact sequences are related. Can anyone explain what the relationship between these two central extensions is? Further, my professor claims that since $A_5$ is embedded in $PGL_{2}(\mathbb{C})$ the two dimensional projective linear group, we have an embedding of $\tilde A$ in $PG_{2}(\mathbb{C})$ , the general linear group, and through this embedding we can determine two irreducible representations. I really don't see how this works at all. How can we find this second embedding, and how can we derive these two irreducible representations from it?","['representation-theory', 'group-extensions', 'exact-sequence', 'abstract-algebra', 'group-theory']"
4402860,Finding all a to have atleast one real root,"Find all $a \in \Bbb R$ for which $$(1 + a) \left(\frac{x^2}{1 + x^2}\right)^2 - 3a\left(\frac{x^2}{1 + x^2}\right) + 4a = 0$$ has at least one real root. For this problem, I tried substituting $y = x^2 / 1 + x^2$ and then arrived at the quadratic $$(1+a)y^2 - 3ay + 4a = 0$$ then the discriminant $$\Delta = 9a^2 - 4(1+a)(4a) = 9a^2 - 16a - 16 a^2 = -7a^2 - 16a$$ Now setting the discriminant $\geq 0$ since we need real roots gives $$-\frac{16}{7}\le\:a\le\:0$$ . However, I am confident this is an incorrect way since this considers the quadratic and not the original equation (??). I do not have access to the solution/answers so I am a bit lost on this.
So what is the correct approach for this? Any hints will suffice.",['algebra-precalculus']
4402892,Is it worth learning measure theory and Lebesgue integration using the Daniell scheme and Shilov's book for a first timer self-studying?,"I'm an undergraduate with somewhat little analysis background (one undergraduate course a year ago and a graduate complex analysis course this semester) who wishes to self-study advanced real analysis (measure theory, Lebesgue integration, etc.). This is most immediately so that I can take a Fourier analysis course next semester (textbook: Fourier Analysis by Duoandikoetxea, if anyone is familar), but I also just because I hope to get a deep and meaningful understanding of the topic for its own purposes. I've been reading Integral, Measure, and Derivative: a Unified Approach by Shilov and was wondering if anyone could comment on its ultimate utility as a first introduction to the topic. After a few days of reading, I've finished chapter 3 ( n -space Lebesgue integration) and feel I understand everything presented in the book well enough, but any time I look at an outside source I become completely lost. The Daniell scheme definitions rarely match with what nearly everyone else uses, and while I'm sure Shilov will eventually prove them equivalent, I can't help but feel like I'm making everything harder for myself by using this book. For example, when I think of a measurable function on a certain set, I think of a function which is the almost-everywhere limit of a sequence of some of the elementary functions which define integration on that set, but when I Google it, I'm hit with some measure theory-based definition that is completely new to me. Is the generality of the Daniell scheme worth being this (temporarily) isolated or should I look to switch to another text and only later come back to Shilov? I own but have not read Kolmogorov's Elements of the Theory of Functions and Functional Analysis , if anyone has an opinion to share on that. I was also thinking of potentially buying Stein's Princeton Lectures in Analysis edition. Any other suggestions/reviews would be appreciated, too.","['self-learning', 'measure-theory', 'lebesgue-integral', 'analysis', 'real-analysis']"
4402915,Conjugacy class of maximum order in $S_n$,Apparently it is well known that the conjugacy class of maximum order is the one with one cycle of length $n-1$ and one element fixed. Which is somewhat intuitive when you already know the answer since we want $1^{c_1}c_1!\cdot2^{c_2}c_2!\cdot\ldots\cdot n^{c_n}c_n!$ to be minimal. However I wasn't able to find a formal proof anywhere on the internet. This question was asked here once and wasn't answered. I was thinking that one probably could make an injective and non surjective mapping from all other conjugacy classes but I'm pretty sure that is not an elementary approach and I have no idea how I would even begin to do that. I'd rather have some kind of combinatorial proof if possible. Edit: I know how to find order of the specific conjugacy class in $S_n$ . What I want to know is how would someone deduce that the one of the highest order is the conjugacy class consisting of a cycle of length $n-1$ and one fixed element.,"['permutations', 'group-theory', 'symmetric-groups', 'combinatorics']"
4402955,Functions satisfying $f(\lambda x)\leq\lambda f(x)$ for some $\lambda$,"Fix some $\lambda>1$ . I am interested in the class of functions $f:\mathbb{R}\to\mathbb{R}$ which satisfy $f(\lambda x)\leq\lambda f(x)$ for all $x\in\mathbb{R}$ . Does this class depend on $\lambda$ ? Or is it the case that if $f$ has this property for one $\lambda$ , then it has it for all? I was trying to start with $\lambda=2$ and $\lambda=4$ . It is straightforward to show that any $f$ with this property for $\lambda=2$ also has this property for $\lambda=4$ . But I was unable to prove the converse which somehow makes me believe that the converse is just not true. But I was also unable to find a counter example.","['functions', 'real-analysis']"
4402966,Can this set be not separable ? Can it be not compact?,"Let $(\mathcal X,\Sigma_X)$ be a measure space, let $\mathcal S_X=\{ q : (\mathcal X, \Sigma_X, q)\text{ is a probability space}\}$ , i.e. the simplex associated to $(\mathcal X,\Sigma_X)$ . We equip $\mathcal S_X$ with the weakest topology such that for all bounded measurable $f:\mathcal X\to\mathbb R$ , the function $p\to p(f)\triangleq\int_{\mathcal X}f ~dp$ is continuous. This is a bounded, closed convex subset of a Hausdorff locally convex vector space. I believe that in general, this set is non separable and non compact, however I could not find an example, I think that we need $\Sigma_X$ to be big enough of an infinite set, having $\mathcal X$ infinite is not enough. I have tried few example, including $\mathcal X=[0,1]$ and $\Sigma_X$ is the Borel $\sigma$ -algebra on $[0,1]$ , but I think this might actually be both compact and separable, I am not sure on how to show that either. Any input would be very useful.","['separable-spaces', 'topological-vector-spaces', 'simplex', 'probability-theory', 'compactness']"
4403034,Finding the inverse of a piecewise function,"I am struggling with finding the inverse of a piecewise function, namely: $f(x) = \begin{cases}
          x - 1, &  0 \le x < 1\\
           2 - x,& 1 < x \le 2 \\
          \end{cases} $ For the first case $\\$ $f: y = x - 1$ when $0 \le x < 1 \rightarrow f^{-1}: x = y + 1$ when $-1 \le x < 0$ But I find this because I only have to subtract 1 from the endpoints of the inequality. I don't have a clear intuition how to do this with $2 - x$","['functions', 'inverse']"
4403048,Simplification in Proof of Countable Union of Countable Sets is Countable,"In the book Analysis I by Amann and Escher, Chapter 1, Proposition 6.8 the following statement is given: 6.8 Proposition A countable union of countable sets is countable Proof For each $n\in\mathbb{N}$ , let $X_{n}$ be a countable set. By Proposition 6.7, we can assume that the $X_{n}$ are countably infinite and pairwise disjoint. Thus we have $X_{n}=\{x_{n,k}:k\in\mathbb{N}\}$ with $x_{n,k}\neq x_{n,j}$ for $k\neq j$ , that is $x_{n,k}$ is the image of $k\in\mathbb{N}$ under a bijection from $\mathbb{N}$ to $X_{n}$ . Now we order the elements of $X=\bigcup_{n=0}^{\infty}X_{n}$ as indicated by the arrows in the 'infinite matrix' below. This induces a bijection from $X$ to $\mathbb{N}$ . We leave to the reader the task of defining this bijection explicitly. $\blacksquare$ Here is Proposition 6.7: 6.7 Proposition Any subset of a countable set is countable. The part of Proposition 6.8 that I am unsure about is that it can be assumed that the $X_{n}$ are countably infinite and pairwise disjoint. Suppose you have the sets $X_{n}$ . Then you can recursively define $Y_{0}=X_{0}$ and $Y_{n+1}=X_{n+1}\setminus\left(\bigcup_{k=0}^{n}X_{k}\right)$ . Then $\bigcup_{n=0}^{\infty}X_{n}=\bigcup_{n=0}^{\infty}Y_{n}$ and the collection of $Y_{n}$ is pairwise disjoint. In particular, each $Y_{n}$ is countable from Proposition 6.7 as it is a subset of a countable set. However, I cannot see why each $X_{n}$ can also be assumed to be countably infinite. This is also stopping me from understanding why there must also be a countably infinite number of sets which restricts the case where there are a finite number. Is there supposed to be a simple way to either rule out these cases or to transform them into the above form?",['elementary-set-theory']
4403054,Tangent bundle of a tensor product bundle,"Let $E\to M$ and $F\to M$ be vector bundles. The structure of their tangents $TE$ and $TF$ is well known. In particular, connectors map $K_E: TE \to E\times_M E$ and $K_F: TF \to F\times_M F$ induce isomorphisms $TE \simeq E \times_M (E\oplus TM)$ and $TF \simeq F \times_M (F\oplus TM)$ (I am using here a fibered product notation, rather than the equivalent pullbacks). Consider now the vector bundle $E\otimes F\to M$ . Its tangent can be characterized in the same way as the tangent bundle of every vector bundle. My question is whether there exists a canonical isomorphism of $T(E\otimes F)$ involving the tangent bundles $TE$ and $TF$ . I would suspect a positive answer, which in particular looks like a ""Leibniz rule"". Also, what is the connector $K_{E\otimes F}$ induced by $K_E$ and $K_F$ ? After all, covariant derivative of tensor products satisfy a Leibniz rule. The same question can be asked regarding the vector bundle $\operatorname{Hom}(E,F)$ .","['connections', 'differential-geometry']"
4403078,Maximum number of spanning cycles with no common edge in a complete graph,"A new class has just been started, and there are $n$ people in this class that do not know each other. At each session they sit around a round table. They decided to change their adjacent persons in each session, so that all people come to know each other as soon as possible. What is the minimum number of sessions after which all people know each other? To solve the above problem, I associated a graph to each session where the nodes are the persons and the neighbours to each person are connected to that person via an edge. So the degree of each node is $2$ . Indeed, each session is a cycle which includes all nodes (persons) of the graph. So the question is how many spanning cycles exist in a complete graph that do not have any common edge? A common edge is not allowed since it represents a repeated neighbor for some person. A complete graph with $n$ nodes has $\frac{n(n-1)}{2}$ edges and each cycle is going to include $n$ of these edges. So, I think if we are lucky and can use all of the edges to build such cycles then at most we can make $\frac{n-1}{2}$ of these cycles. $1$ . Is this argument valid? $2$ . Can we use all of the edges to make such cycles?","['graph-theory', 'combinatorics', 'discrete-mathematics']"
4403086,Volume of a solid enclosed by the hyperboloid and cone,"Compute the volume of the solid inside the cone $2(x^2+y^2)=z^2$ and hyperboloid $x^2+y^2=z^2-a^2.$ Source Demidovich, task 2024 My attempt: Because of the symmetry with respect to the $xy$ plane, we can compute the volume above the plane and multiply it by $2$ . Intersection of the cone and hyperboloid : $$\begin{cases}x^2+y^2=\frac{z^2}2\\x^2+y^2=z^2-a^2\end{cases}\implies x^2+y^2=\frac{z^2}2=a^2$$ is the circle of the radius $a$ . I used the cylindrical coordinates $$\begin{aligned}x=r\cos\varphi\\y=r\sin\varphi\\z=z, r\in[0,a]\\\varphi\in[0,2\pi]\\z\in\left(\sqrt 2r,\sqrt{r^2+a^2}\right)\end{aligned}$$ so the Jacobian equals $r$ and volume is $\begin{aligned}2\int_0^{2\pi}\int_0^a\int_{\sqrt 2r}^{\sqrt{r^2+a^2}}r\mathrm dz\mathrm dr\mathrm d\varphi&=4\pi\int_0^a\left(\sqrt{r^2+a^2}-\sqrt 2r\right)\mathrm dr\\&=4\pi\int_0^a\left(r\sqrt{r^2+a^2}-\sqrt 2r^2\right)\mathrm dr\\&=4\pi\left(\frac{(r^2+a^2)^{3/2}}3-\sqrt 2\frac{r^3}3\right)\Big|_0^a\\&=4\pi\left(\frac{2\sqrt 2 a^3}3-\sqrt 2\frac{a^3}3\right)\\&=\frac43a^3\pi,\end{aligned}$ but the solution is $\frac43a^3\pi(\sqrt 2-1),$ and I can't find my mistake. Can anybody see it?","['integration', 'multivariable-calculus']"
4403097,Relation of derivative of $\operatorname{sinc}(z)$ and derivative of $\cos$.,"From this paper, in Eq. (4), it is asserted that (LHS being (3) and RHS being (4)) $$
(-1)^{n} z^{n} \frac{d^{n} \operatorname{sinc}(z)}{(z d z)^{n}}
=
(-1)^{n+1} z^{n+1} \frac{d^{n+1} \cos (z)}{(z d z)^{n+1}}
$$ with $\operatorname{sinc}(z) = \sin(z)/z$ and $n \in \mathbb{N}_0$ and $z\in \mathbb{C}$ . The notation employed in the cited paper is a bit ambiguous but in other literature (or here on SE ), as far as I see it, $$\frac{d^n}{(zdz)^n} f(z)
:= \left(\frac{1}{z} \frac{d}{d z}\right)^{n} 
:= \underbrace{
\left(\frac{1}{z} \frac{d}{d z}\right) 
\left(\frac{1}{z} \frac{d}{d z}\right)
\cdots
\left(\frac{1}{z} \frac{d}{d z}\right)}_{\text{n times}}
f(z)
$$ I can't verify this though. When I substitute $n=1$ to test, I get for the LHS: $-\frac{\operatorname{cos}(z)}{z}+\frac{\operatorname{sin}(z)}{z^{2}}$ but for the RHS $z \left(\frac{\sin (z)}{z^2}-\frac{\cos (z)}{z}\right)$ . Hence it feels like in the equation above, the $z^{n+1}$ on the RHS should actually be just $z$ . Is this an error in the paper or am I missing something?","['calculus', 'derivatives', 'trigonometry']"
4403132,Contraction Mapping and Fixed Point with two different distance metrics,"I have been looking at the fixed point theorems that use the contraction-mapping and all seem to use the same distance metric for the input and output spaces. If we have a differentiable mapping $f: X \rightarrow X$ . I was wondering if we could use different distance metrics for the input $X$ and output $X$ , say $d'$ and $d$ . If we have $d'(f(x),f(y))\le k d(x,y)$ with $0<k<1$ , would we still have a fixed point for $f$ ? Will it be unique?","['contraction-operator', 'fixed-points', 'functional-analysis', 'real-analysis']"
4403186,A random vector satisfying invariance under rotation and belonging to the unit sphere must be uniform distribution on the unit sphere?,"If a random vector $u \in\mathbb{R}^n$ has $\|u\|_2=1$ , and for any orthogonal matrix $Q$ , it has $Qu \overset{d}{=}u$ , does this imply $u \sim Unif(S^{n-1})$ ?","['measure-theory', 'statistics', 'probability-distributions', 'normal-distribution', 'probability']"
4403228,Two false equalities,"I have to prove that the following equalities are not valid. First. Let $a,b\in\mathbb{R}$ and $n\in\mathbb{N}$ , $$[a,b)=\bigsqcup_{k=1}^n (a_k,b_k],$$ where $-\infty<a_k<b_k<+\infty$ for all $k\in\mathbb{N}$ . Proof. If this were true there would be only one $k_0\in\{1,\dots, n\}$ such that $a\in (a_{k_0}, b_{k_0}]$ and therefore $a>a_{k_0}$ . On the other hand, $(a_{k_0}, b_{k_0}]\subset [a, b)$ , but $a_{k_0}\notin[a,b)$ since $a_{k_0}<a.$ Thanks to the suggestions I conclude that: $\sup{[a,b)}=b$ and $\sup{\sqcup_{k=1}^n (a_k,b_k]}=b_n$ (We can assume without affecting the generality that $b_1< b_2<\dots< b_n$ ), then $b_n=b$ and therefore $b\in \sqcup_{k=1}^n (a_k,b_k]$ , but $b\notin [a,b)$ . Second. Let $a,b\in\mathbb{R}$ and $n\in\mathbb{N}$ , $$(a,b)=\bigsqcup_{k=1}^n(a_k,b_k],$$ where $-\infty<a_k<b_k<+\infty$ for all $k\in\mathbb{N}$ . Proof. If this were true there would be only one $k_0\in\{1,\dots, n\}$ such that $b\in (a_{k_0}, b_{k_0}]$ , then $b\le b_{k_0}$ . On the other hand we have tha $(a_{k_0}, b_{k_0}]\subset (a,b)$ , but $b_{k_0}\notin (a.b)$ . since $b_{k_0}\ge b$ . Are they formally correct? Thanks!","['elementary-set-theory', 'solution-verification']"
4403242,Question on conditional expectation $E[(U_{t \wedge (s\vee \tau)} - U_{t\wedge \tau})X_{t \wedge \tau}|\mathscr{F}_s]=(U_s-U_{s\wedge \tau})X_\tau?$,"I have a question regarding conditional expectations from this post: https://almostsuremath.com/2010/05/03/girsanov-transformations/ In the final equation in the proof below, I can't see why this follows from the conditional expectation properties. So here, $U$ is a nonnegative martingale, and $X$ is a cadlag adapted process. We are trying to show that $(UX)^\tau$ is a martingale if and only if $UX^\tau$ is a martingale, and so define $M$ to be the difference and show that $M$ is a martingale. Using the tower property and optional sampling for $U$ we are able to obtain $E[M_t|\mathscr{F}_s] = E[(U_{t \wedge (s\vee \tau)} - U_{t\wedge \tau})X_{t \wedge \tau}|\mathscr{F}_s]$ . However, from this, how do we get $$E[(U_{t \wedge (s\vee \tau)} - U_{t\wedge \tau})X_{t \wedge \tau}|\mathscr{F}_s]=(U_s-U_{s\wedge \tau})X_\tau?$$ I cannot think of any properties to derive this. I would greatly appreciate any help.","['conditional-expectation', 'stochastic-processes', 'martingales', 'probability-theory', 'probability']"
4403253,What percentage of a population that has survived one standard deviation will survive two?,"This is a problem I had in school that I think I remember how to solve except my son does not agree with me. Given a population with a life expectancy that fits a normal distribution, mean of 75 and a standard deviation of 5.  What percentage of the people that live to age 80 will live to age 85? As soon as I'll figure out how to produce suitable graphics to answer the question but I'll post it now in case anybody can come up with a good explanation.","['statistics', 'probability']"
4403259,Average value of trigonometric functions,"Suppose I have to find out $\langle \sin^2\theta\rangle$ and $\langle\cos^2\theta\rangle$ . What I do normally would be : $$\langle\sin^2\theta\rangle=\frac{1}{2\pi}\int_{0}^{2\pi}\sin^2\theta d\theta=\frac{1}{2}$$ $$\langle\cos^2\theta\rangle=\frac{1}{2\pi}\int_{0}^{2\pi}\cos^2\theta d\theta=\frac{1}{2}$$ However, in a question they have asked us to do the same thing, but mentioned that $\theta$ is the polar angle and $\phi$ is the azimuthal angle. Is the answer going to be the same in this case. I know that the limits would change from $0\rightarrow \pi$ or from $\frac{-\pi}{2}\rightarrow \frac{\pi}{2}$ , and I'd divide by $\pi$ instead of $2\pi$ . However, the answer should be the same, and equal to $1/2$ . However, some sources suggest that since the word 'polar' angle us used for $\theta$ , apart from this change in limits, we must also replace $d\theta$ by $\sin\theta d\theta$ , and instead of dividing by $\pi$ , we should do the following : $$\langle\sin^2\theta\rangle=\frac{\int_{0}^{\pi}\sin^3\theta d\theta}{\int_{0}^{\pi}\sin\theta d\theta}=\frac{2}{3}$$ $$\langle\cos^2\theta\rangle=\frac{\int_{0}^{\pi}\cos^2\theta\sin\theta d\theta}{\int_{0}^{\pi}\sin\theta d\theta}=\frac{1}{3}$$ This second method doesn't make any sense to me, and I don't see why we need to this, instead of what we did in the first case. Any help in understanding why this happens, would be highly appreciated. I don't even know if this second method is valid at all. Some people say that we do this second thing, since we are solving in spherical coordinates, as the word 'polar angle' is used, but I don't see why this extra term would show up, and why would the expectation value of these functions change depending on cartesian or polar coordinates.","['integration', 'trigonometry', 'expected-value', 'trigonometric-integrals']"
4403277,"Is it possible to partition $[a,b]$ into countably many ""identical"" sets which are translation from each other?","Given $[0,1]$ , is it possible to find one set $A_1\subset[0,1]$ and $A_i = A_1 + a_i, i\in\mathbb{N}, a_i\in\mathbb{R}$ such that $\{A_i\}$ is a countable partition of $[0,1]$ . By $A_1 + a_i$ , it means translating $A_1$ by the value of $a_i$ , e.g. $[0, 0.5] + 0.25\triangleq [0.25, 0.75]$ . Edit: It turns out with constraint of modulo one, this $A_1$ could be the non-measurable Vitali set. And $[0,1] = \cup_{q\in Q\cap[0,1]}(V_1 + q)$ .",['probability']
4403306,How can one find an identity for $\sum_{k=0}^{n-1}\tan^2(\frac{k\pi y}{n}) $?,"Using the function $$
f(z)=\frac{n/z}{z^n-1}\left(\frac{z-1}{z+1}\right)^2\tag{1}
$$ and residue theorem $$
\sum_{k=0}^{n-1}\tan^2\left(\frac{k\pi}{n}\right)=n^2-n\tag{2}
$$ can be found according to this post: Prove that $\sum\limits_{k=0}^{n-1}\dfrac{1}{\cos^2\frac{\pi k}{n}}=n^2$ for odd $n$ So I was wondering if it is possible to use the residue method to find an identity for $$
\sum_{k=0}^{n-1}\tan^2\left(\frac{k\pi y}{n}\right)\tag{3} 
$$ $0\leq y \leq 1$ and how? or alternative method if not.","['summation', 'complex-analysis', 'contour-integration', 'residue-calculus', 'trigonometry']"
4403314,"Given any symmetric matrix $A$, how to find a diagonal matrix $B$ such that $B \succeq A$?","Given any symmetric (not necessarily PSD) matrix $A$ , how can we find a good diagonal matrix $B$ such that $A \preceq B$ ? We want each of the diagonal elements of $B$ to be as small as possible. One possible such matrix I can think of is $\mathrm{diag}(\|A_i\|_1)$ , but I don't think it is tight enough. Can we find a better matrix than this? Also, I hope that $B$ depends on the $\ell_2$ norms of $A_i$ instead of the $\ell_1$ norms, or the eigenstructure of the matrix $A$ . It would be very useful if there are solutions that only needs eigenvalues or singular values, so it is possible to approximate it in almost linear time to the dimension. The background for this problem is ""coordinate-wise smoothness"" in convex and non-convex optimization. The global smoothness of a function can be defined as $\sup\|\nabla^2f(x)\|_2$ , that is to find a constant $L$ such that $-LI \preceq \nabla^2 f(x) \preceq LI$ . I'm looking for a similar way to find the coordinate-wise smoothness efficiently.","['positive-semidefinite', 'spectral-norm', 'convex-optimization', 'matrices', 'linear-algebra']"
4403325,Line equation tangent to convex level curve,"Suppose we are given a differentiable function $f(x,y)$ such that $\forall$ $t \in \mathbb{R}$ , $f = t$ yields strictly convex level curves. If we are given a line equation $L:y = mx + c$ such that it always intersects two points of a level curve or is tangent to it or does not intersect at all, will it mean that a level curve tangent to $y = mx + c$ always exists? This is required for a different (convex optimization) problem that I am trying to solve. I am not sure if this holds, but intuitively it feels like that, although I don't have a solid intuition to solve this. Using the Mean Value Theorem, one can say that a tangent with slope $m$ exists that will intersect a given level curve. But I can't extend this to prove that the line $y = mx + c$ itself will be tangent to some level curve. Update: The problem is almost solved (thanks to @copper.hat), but there's a case that I can rule out. Please see my last comment below this post.","['convex-optimization', 'curves', 'economics', 'multivariable-calculus', 'calculus']"
4403337,Count Diophantine solutions to $x^2+xy+y^2≤r^2$. How to obtain geometric series? Eisenstein integer- Triangular lattice,"I am trying to understand how it is obtained the right part: $$ \sum_{x,y \in Z } q^{x^2 + xy + y^2} = 1 + 6 \sum_{k\geq 0} (\frac{q^{3k+1}}{1 - q^{3k+1} } - \frac{q^{3k+2}}{1 - q^{3k+2} }).$$ I want to understand the math behind the right part.
I found this expression in different articles but without an easy explanation. This is one https://www.mat.univie.ac.at/~slc/wpapers/s42hirsch.pdf . Why is $ 3k+1$ and $3k+2$ ?. And what it means geometrically? In this case it is a triangular lattice but which points represents $ 3k+1$ and  and which points represents $3k+2$ ?
If someone can explain to me like for dummies. This is useful for finding triangular lattice points Oeis A308685 : $$N(r) = 1 + 6 \sum_{k=0}^{\infty} \lfloor \frac{r^2}{3k+1}\rfloor - \lfloor \frac{r^2}{3k+2}\rfloor. $$ I found that $N(r)$ expression is coming from geometric series: $$\frac{x}{1-x} = x + x^2 + x^3+ \dots$$ but I don't get it. Any help?","['number-theory', 'geometry']"
4403338,The probability that no runs of k consecutive heads OR tails will occur in n coin tosses,"I am trying to find the probability that no runs of k consecutive heads or tails will occur in n coin tosses. After doing some reading, I found this that shows that the probability that no k consecutive tails will occur in n tosses is given by $F^k_{n+2}/2^n$ where $F^k_l$ is a  Fibonacci k-step number. Now this works if I am trying to see the probability that no k consecutive tails or no k consecutive heads occurs. But if I want to know the probability that no k consecutive tails or heads occurs, the problem becomes more complicated, because there are many coin flip combinations that include both k consecutive heads and k consecutive tails at the same time. If I could determine the number of combinations that included both k consecutive heads and k consecutive tails, I could then use it to find the number of combinations that have either k consecutive heads or k consecutive tails and sum all three of those to find the probability that no runs of k consecutive heads or tails will occur in n coin tosses. Running this with R, I found that the number of combinations where there were both k consecutive heads and k consecutive heads that occurred was as follows: k n count 3 5 0 3 6 2 3 7 8 3 8 26 3 9 74 3 10 194 3 11 482 3 12 1152 k n count 4 7 0 4 8 2 4 9 8 4 10 26 4 11 76 4 12 206 4 13 530 4 14 1314 k n count 5 9 0 5 10 2 5 11 8 5 12 26 5 13 76 5 14 208 5 15 542 5 16 1362 There seems to be a pattern that starts 2 later for each increased value of k, and has similar values that then stabilize with higher values of k. This seems to me to be related to a Fibonacci k-step number, but I can't see the pattern here. My question is this. Is there a simpler way to calculate the probability that no runs of k consecutive heads or tails will occur in n coin tosses. And if not, is there a way to calculate the number of combinations with k consecutive heads and tails, following the pattern here.","['statistics', 'fibonacci-numbers', 'probability']"
4403353,How to prove this index equality problem with group actions?,"Let $H$ be a $p$ -subgroup of a finite group $G$ . Prove that: $[N_G(H):H] \equiv [G:H] (\text{mod}  \; p)$ . Hint: Consider the appropriate action of $H$ on the set of all cosets of $H$ in $G$ . I literally have no idea what to do. I suppose the action that is meant is the natural one; $H \times (G / H )\to G/H$ , defined as: $a \cdot b H = ab H$ . I guess we could somehow use the class formula: $|G / H| = |Z.| + \sum_{gH \notin Z}[H:H_{gH}]$ . I have no idea what to do with the normaliser: $N_G(H) = \{ a \in G; aHa^{-1}=H \}$ or how to proceed with the problem.","['group-actions', 'group-theory', 'abstract-algebra', 'finite-groups']"
4403400,Order of variables of integration in double integral,"This is a question from the actuarial practice set: An insurance company insures a large number of drivers. Let X be the random variable
representing the company’s losses under collision insurance, and let Y represent the
company’s losses under liability insurance. X $$f(x,y )=
\begin{cases}
 \frac{2x + 2 + y}{4}&(x,y) \in [0,1] \times [0,2]\\
      0\      &\text{otherwise}
\end{cases}$$ Calculate the probability that the total company loss is at least 1. The solution suggests to integrate with respect to y first: $$ \int_0^1 \int_{1-x}^2 \frac{2x + 2 + y}{4} dy \, dx   $$ I thought this is more natural, yet it gives the wrong answer: $$ \int_0^2 \int_{1-y}^1 \frac{2x + 2 + y}{4} dx \, dy  $$ Why does the latter not work?","['integration', 'calculus', 'definite-integrals', 'probability']"
4403402,Does the set of 2x2 complex matrices form a field,"Let $C$ be the set of all the matrices of the form $$ C = \{\begin{pmatrix}
z & -w \\ w 
 & z
\end{pmatrix} \; | \; z,\ w \in \mathbb{C}\}. $$ My question would be if the $C$ forms a field with an addition ( $+$ ) and matrix multiplication ( $\times$ )? If not, why not? I went through all of the field axioms and couldn´t find the issue but it doesn´t seem right.",['matrices']
4403404,Alternate proof of Sylow's First theorem.,"I am trying to prove first Sylow Theorem using the Lemma: if $G$ is a finite group such that has a Sylow $p$ -subgroup and $H\subset G$ , then $H$ has a Sylow $p$ -subgroup. The way I want to go about proving first Sylow Theorem using lemma is to notice that any finite group $G$ injects into $S_{|G|}$ . Given the Lemma, it suffices to show that $S_{|G|}$ has a Sylow $p$ -subgroup. If $p^k$ is the highest power of $p$ such that $p^k| |G|$ , then I can easily see that $\langle(123\cdots p^k)\rangle\subset S_{|G|}$ is a subgroup with order $p^k$ . But this doesn't finish the proof as $|G|!$ may be divisible by $p^{k+1}$ , which then means this subgroup is not a Sylow $p$ -subgroup of $S_{|G|}$ . Would there be hints about what steps I should make in this way of proving the first Sylow Theorem?","['symmetric-groups', 'group-theory', 'abstract-algebra', 'sylow-theory']"
4403433,"Find sufficient and complete statistic: Uniform distribution, $\theta \geq 1$","Take a random sample $X_1,X_2, \dots X_n$ from the distribution f $(x;\theta)=\frac{1}{\theta}$ for $0 \leq x \leq \theta$ , where $\theta \geq 1$ (pay attention!). I need to find an efficient estimator (in terms of MSE). I've managed to check if $\max(1, X_{(n)})$ is sufficient and complete. But I can't transform it to get an unbiased estimator. So, my questions are: Could you suggest a suitable transformation? If not, could you find another sufficient and complete estimator?","['statistics', 'uniform-distribution']"
4403443,Probability of rolling $n$ dice that are each are greater than or equal to $x$ with a given pool of possible positive modifiers applied on each dice,"I am currently analyzing a tabletop games probabilities. Successful rolls in the game are determined by rolling $n$ number of dice and counting the number of dice that are greater than or equal to a certain value (some pre-determined threshold $x$ ; for instance, values could be between 1 through 6 from a six sided die). For example, suppose my threshold $x=3$ ; if I roll $10$ six sided die, what is the probability that $4$ dice would land on a $3$ or greater? Additionally, the game allows for a positive pool of modifiers to be applied on the dice after they are rolled, such that it can bring them over the determined threshold $x$ and have it count as a success. For example, suppose I roll three six sided die. Let $n_i$ represent the value of the die, and let $n_1=2$ , $n_2=3$ , and $n_3=6$ . Suppose I have a threshold of $x=4$ have a pool of $+3$ points to apply on any one of those die. I would be able to distribute all those points on both $n_1$ and $n_2$ such that I now have three successes instead of only one (where now $n_1=2+2=4$ , $n_2=3+1=4$ , and $n_3=6$ .) How are the probabilities of rolling $n$ dice that are each are greater than or equal to $x$ affected by the number of points in the modifier pool? I have attempted to develop a formula specifically to answer the last question above so that I can analyze the distribution of a certain number of dice with different thresholds. I started with thinking of it as a binomial distribution, however, I am uncertain how it is affected by the modifiers. EDIT :
After almost a year since asking the question I think I have something close to a solution, but not exact. From my analysis, the probability computation for a combination of the number of dice, modifiers, successes required, and dice sides is incredibly complex to generalize. Here is a dropbox link of my analysis (very informal), where I attempted to find a distribution I've labeled $tdw(n,f,d,s)$ (to answer probability statements such as $Pr(X=x_{2}|n=7,f=4,d=4,s=3)$ , where $x_{2}$ denotes getting 2 successes - here I defined the threshold to be $d$ , the number of sides of the dice to be $s$ , and the total number of available modifiers to be $f$ ) which in effect is a modified binomial distribution with extra parameters. I have no understanding of how to create a discrete distribution like this formally, however. The framework I attempted to create used two 3 dimensional matrixes of different sizes depending on the value of $f$ and $n$ . These would track the probabilities of a certain combination of $n$ , $f$ , $d$ , and $s$ . This idea, however, is completely made up and may have no basis in solving this kind of problem, however, for the $i=0$ case, the method always correctly solves the probability when compared to the probability I calculated manually.","['statistics', 'probability-distributions', 'binomial-distribution', 'dice', 'probability']"
4403456,Is the product of two invertible symmetric matrices always diagonalizable?,"I have two symmetric matrices $A$ and $B$ , which are both invertible. Their eigenvalues are obviously real, but not necessarily positive. I know that if one matrix were positive definite, we could use ( Product of two symmetric matrices is similar to a symmetric matrix ) to show that $AB$ is similar to a symmetric matrix (with real eigenvalues). In my cases, the eigenvalues of $AB$ are generally complex, but can one adjust the proof to show that $AB$ is always diagonalizable?","['diagonalization', 'linear-algebra']"
4403459,The unit disk with pseudo hyperbolic distance is complete space,"The pseudo-hyperbolic distance on the unit disk $D$ is defined as: $$\rho(z,w)=\left|\dfrac{z-w}{1-\bar wz}\right|.$$ I need to prove that $(D,\rho(z,w))$ is complete space. I know that a metric space is called complete if every Cauchy sequence converges, but I really don't know how to prove that this space is complete, could you give me some idea?","['complex-analysis', 'complete-spaces', 'metric-spaces']"
4403481,Question about the product of some consecutive integers being factorials. [duplicate],This question already has answers here : Four fractions of certain factorials give another factorial (1 answer) $n! =$ the product of consecutive integers. [duplicate] (1 answer) Closed 2 years ago . A nice little curio. 1 = 1! 2( 3 ) = 6 = 3! 4(5)( 6 ) = 120 = 5! 7(8)(9)( 10 ) = 5040 = 7! Are there any other examples of products of 'some' consecutive integers equalling factorials?  Is there a proof that there is not?  To be clear I am asking if the pattern above continues for all the triangular numbers (in bold)?  It does not appear to hold for the next few examples so I conjecture that the pattern only holds for the triangular numbers above.  Is this conjecture known or proven/disproven?,"['factorial', 'elementary-number-theory', 'natural-numbers', 'discrete-mathematics', 'arithmetic']"
4403483,Prove that x $\in y$ iff $\{x\}\subseteq y$,"My goal is to prove that $x\in y$ iff $\{x\}\subseteq y$ . Here's my sketch of a proof: We have $x\in y$ , and by definition $A\subseteq B := \forall a$ ( $a\in A \implies a \in B$ ). We may infer $\forall$$z$ ( $z = x$ $\implies$ $x \in y$ ) and $\forall$ z ( $z = x$ $\iff$ $z \in {x}$ ), then $\forall$$z$ ( $z \in \{ x\}$ $\implies$ $z \in y$ ). And i stopped here. I'm pretty sure that there is a simpler way of writing this proof (and by simpler, i mean doing this demonstration without invoking first order logic formalizations of set theoretic definitions)","['elementary-set-theory', 'proof-writing', 'solution-verification', 'alternative-proof']"
4403512,"Is there any closed form for $\int_{0}^{1} \frac{\ln ^nx}{\sqrt{1-x^{2}}}\,d x$?","In my post , I found that $$\int_{0}^{1} \frac{\ln x}{\sqrt{1-x^{2}}} d x = -\frac{\pi}{2} \ln 2 .$$ Then I try to generalize the result to the integral by the same technique. $$
J_{n}:=\int_{0}^{1} \frac{\ln ^nx}{\sqrt{1-x^{2}}} d x
$$ Letting $x=\cos \theta $ converting $I_n$ into \begin{aligned}
J_{n} &=\int_{0}^{\frac{\pi}{2} } \frac{\ln^n (\cos \theta)}{\sin \theta} \sin \theta d \theta \\
&=\int_{0}^{\frac{\pi}{2}} \ln ^{n}(\cos \theta) d \theta \end{aligned} By my post , $$\begin{aligned}
(-2)^{n}J_n&= 2 \ln 2(-2)^{n-1} J_{n-1} + (n-1) !\sum_{k=0}^{n-2} \frac{2^{n-k}-2}{k_ !} \zeta(n-k) (-2)^{k} J_k \\J_n&= -\ln 2 J_{n-1} + (n-1) !\sum_{k=0}^{n-2} \frac{(-1)^{n-k}}{k !} \left(1-\frac{1}{2^{n-k-1}} \right)\zeta(n-k)  J_k
\end{aligned}
$$ which is a reduction formula for $J_n.$ For example, $$
\begin{aligned}
\int_{0}^{1} \frac{\ln ^2x}{\sqrt{1-x^{2}}} d x&=-\ln 2 J_{1}+\frac{1}{2} \zeta(2) J_{0} \\
&=-\ln 2\left(-\frac{\pi}{2} \ln 2\right)+\frac{1}{2} \zeta(2) J_{0} \\
&=\frac{\pi \ln ^{2} 2}{2}+\frac{\pi^{3}}{24},
\end{aligned}
$$ which is checked by Wolframalpha. My Question Is there any closed form for the integral?  Your comments and methods are warmly welcome.","['integration', 'calculus', 'definite-integrals', 'trigonometry']"
4403551,Coloring a Generalized Latin Square,"Suppose we have an $n \times n$ array, and there is a decomposition $\mathcal{A}$ of its coordinates $a_{i,j}$ into sets $A_m$ as follows: If $a_{i,j} \in A_m$ , then $a_{j,i} \in A_m$ .  So they're symmetrical pieces of the array (across the diagonal). If $a_{i,j}, a_{i,k} \in A_m$ and $j \neq k$ then $a_{j,k} \in A_m$ .  Similarly, if $a_{j,i}, a_{k,i} \in A_m$ and $j \neq k$ then $a_{j,k} \in A_m$ .  So the relations are transitive, in a sense. If a diagonal coordinate $a_{i,i}$ is contained in $A_m$ , then $A_m = \lbrace a_{i,i} \rbrace$ is a singleton. The question is this: Thinking of the $A_m$ 's as chunks of the $n \times n$ array, can we apply a color to each $A_m$ so that the array satisfies a 'Latin Square' condition using at most $n$ colors, i.e. within any row or column no color appears more than once unless they belong to the same $A_m$ ?  So, every entry in any fixed $A_M$ will all have the same color, and we color the square 'up to $\mathcal{A}$ .' This is a restatement of a known hypergraph problem (after using some observations I made) that doesn't have much associated theory to attack it with. Here are a couple of examples for $n=8$ : In the first example we have $A_1 = \lbrace a_{1,2}, a_{2,1}, a_{1,3}, a_{3,1}, a_{2,3}, a_{3,2} \rbrace$ , $A_2 = \lbrace a_{1,4}, a_{4,1}, a_{1,5}, a_{5,1}, a_{4,5}, a_{5,4} \rbrace$ , $A_3 = \lbrace a_{6,7}, a_{7,6}, a_{6,8}, a_{8,6}, a_{7,8}, a_{8,7} \rbrace$ and the rest are pairs $\lbrace a_{i,j}, a_{j,i} \rbrace$ or diagonal singletons.  An $8$ -coloring is shown. In the second example, we have $A_1 = \lbrace \text{non-diagonal entries with indices from 1-5} \rbrace$ $A_2 = \lbrace a_{1,6}, a_{6,1}, a_{1,7}, a_{7,1}, a_{1,8}, a_{8,1}, a_{6,7}, a_{7,6}, a_{6,8}, a_{8,6}, a_{7,8}, a_{8,7} \rbrace$ and the rest are pairs $\lbrace a_{i,j}, a_{j,i} \rbrace$ or diagonal singletons.  A $7$ -coloring is shown. It starts getting much more complex once you get up into double-digit values of $n$ , the way the 'chunks' can be enmeshed with each other. Also, please double-check that the colorings I gave are actually right.  I screwed it up a jillion times haha","['combinatorial-designs', 'coloring', 'graph-theory', 'matroids', 'combinatorics']"
4403570,A lower bound of $\sum_{i=1}^n a_i \sum_{i=1}^n \frac{1}{a_i}$,"For fixed $n \ge 2$ , find the maximum of real number $t$ such that $$
\sum_{i=1}^n a_i \sum_{i=1}^n \frac{1}{a_i} \ge n^2 + t \cdot \frac{\displaystyle\sum_{1\le i<j \le n} (a_i - a_j)^2 }{\left( \displaystyle\sum_{i=1}^n a_i\right)^2}
$$ holds for arbitrary positive numbers $a_i\quad(i=1,2,\dots,n)$ . This problem comes from a training math camp for CMO competitors in 2021. The standard solution seems to be a little unkind to high school students as Lagrange multiplier
method is somehow involved. I wonder if here is any solution without it. Following is my attempt. By Lagrange identity, it's clear that $$
\sum_{i=1}^n a_i \sum_{i=1}^n \frac{1}{a_i}=n^2 + \sum_{1\le i<j \le n}\left(\sqrt{\frac{a_i}{a_j}}-\sqrt{\frac{a_j}{a_i}}\right)^2=n^2 + \sum_{1\le i<j \le n} \frac{(a_i - a_j)^2}{a_ia_j}
$$ Then we only need to find the maximum of $t$ such that $$
\sum_{1\le i<j \le n} \frac{(a_i - a_j)^2}{a_ia_j} \ge t \cdot \frac{\displaystyle\sum_{1\le i<j \le n} (a_i - a_j)^2 }{\left( \displaystyle\sum_{i=1}^n a_i\right)^2}
$$ However, by Cauchy-Schwarz we have $$
\sum_{1\le i<j \le n} \frac{(a_i - a_j)^2}{a_ia_j} \ge \frac{\left(\displaystyle\sum_{1\le i<j \le n} |a_i - a_j|\right)^2}{\displaystyle\sum_{1\le i<j \le n} a_ia_j}
$$ which means nothing.","['contest-math', 'lagrange-multiplier', 'multivariable-calculus', 'cauchy-schwarz-inequality', 'inequality']"
4403581,Sets of Unambiguous Measure,"Here is a fictionalised history of measure on $[0,1]$ . We started with intervals and assigned them lengths. We noticed the properties of these lengths regarding disjoint unions and complements, and, wanting these to hold for arbitrary sets, came to the Borel measure $\mu$ . We then asked if there was a natural way to extend this measure. The Lebesgue measure was natural because its sets $S$ , in some sense, had to have the measure $\lambda(S)$ . They are all disjoint unions of Borel sets $B$ and null sets $N$ , which in any possible extension of the Borel measure, had to be assigned measure $\mu(B)$ . My question is, can we go any further in this fashion? Formally, let $(\Omega,\mathcal{F},\mu)$ be a measure space and $(\Omega,\mathcal{F}_0,\mu_0)$ its completion. Call a measure space $(\Omega,\mathcal{F}',\mu')$ such that $\mathcal{F}\subseteq\mathcal{F}'$ and $\mu'|_\mathcal{F}=\mu$ an extension of $(\Omega,\mathcal{F},\mu)$ . Suppose that for the set $S\subseteq\Omega$ there exists an extension $(\Omega,\mathcal{F}',\mu')$ of $(\Omega,\mathcal{F},\mu)$ such that $S\in\mathcal{F}'$ and for any two such extensions $(\Omega,\mathcal{F}',\mu')$ and $(\Omega,\mathcal{F}'',\mu'')$ , $\mu'(S)=\mu''(S)$ . Then is it necessarily the case that $S\in\mathcal{F}_0$ ? If not, is it at least true for $\mathbb{R}$ as the Borel or Lebesgue measure space?",['measure-theory']
4403658,What are the chances of rolling 2 dice 43 times without rolling a 7?,"Forgive my very long explanation.  I play craps and I attempt to make my throws less random. I track my throws and collect data from sets of 900 throws.  Obviously, a random thrower is likely to throw around 150 sevens every 900 rolls.  My last set of 900 rolls I rolled 113 sevens or a seven every 7.96 rolls.  The 900 rolls prior to that I rolled 123 sevens or a seven every 7.32 rolls. Obviously. the math will not work out to be the odds of rolling 43 times in a single turn as a player may throw a seven on a come out roll after making the point. I am asking specifically about 43 rolls with no sevens as I recently went to the casino for the first time since the COVID restrictions began.  My first turn I threw 43 times with my first seven on my 44th throw.
I hope you made it through my long explanation.  Thank you.","['statistics', 'dice', 'probability']"
4403663,"Ten dwarfs are seated around a table. At the beginning, one of them has ten gold coins. Every minute,","Ten dwarves are seated around a table. At the beginning, one of them has ten gold coins. Every minute, the dwarves check to see if one of them has more than one piece. If yes then exactly one of these gives one coin to each of its two neighbours. Is it possible to arrive at the situation where each dwarf has exactly one piece? I think one thing is conserved and that is the fact that at each step there's at least  one dwarf that has an even number of coins , and so the situation is impossible, I still don't know how to prove it",['combinatorics']
4403684,Intuition on independence of two events.,"The following question is taken from a textbook: Stanley H. Chan: Introduction to Probability for Data Science, 2021. (pp. 87-88) Please note that this is not a question on how to calculate independence, I am fully aware of this. Instead, the intuition is what I am looking for in the below confusing question. Consider the experiment of throwing a die twice. One should be clear from the context that the outcomes are in the form of a tuple $(\textbf{dice_1}, \textbf{dice_2})$ and the sample space is: $$
S = \left\{(1, 1), (1, 2), \ldots, (6, 6)\right\}
$$ Define the three events below: $$
A = \{\textbf{1st dice is 3}\} \quad B = \{\textbf{sum of two die is 7}\} \quad C = \{\textbf{sum of two die is 8}\}
$$ We want to find out if events $A$ and $B$ are independent? How about $A$ and $C$ ? Now the answer is easy to get if you just use the formula and show that if $P(A \cap B) = P(A)P(B)$ and in a similar vein for event $A$ and $C$ . However, I want to understand it more intuitively as my intuition failed me immediately, when I saw the question I thought that both should have similar answer since they are asked similarly. We focus on the independence of $A$ and $C$ first. The author said that intuitively, given that event $C$ has happened, will this affect the probability of $A$ happening? I assume that this means we do have to know the probability of event $A$ without $C$ first. We can enumerate and see that event $A$ has the following set representation: $$
A = \{(3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6)\}
$$ which amounts to $P(A) = \frac{6}{36} = \frac{1}{6}$ . Now if $C$ happened, we know that the two rolls have a sum of $8$ , and we cannot construct a sum of $8$ with a roll of $1$ . To me, I immediately know that event $A$ cannot have the outcome that has a $1$ in the second roll, and thus the outcomes should only be limited to $5$ instead of $6$ and hence dependence is established. However, I believe somewhere my intuition is flawed, the author mentioned that: If you like a more intuitive argument, you can imagine that C has happened, i.e., the sum is 8. Then the probability for the first die to be 1 is 0 because there is no way to construct 8 when the first die is 1. As a result, we have eliminated one choice for the first die, leaving only five options. Therefore, since C has influenced the probability of A, they are dependent. I think I cannot understand why the author mentioned about ""first die"" when in event $A$ , the first die is already a $3$ . If we follow this line of logic, does this mean we do not actually need to know how event $A$ is defined? Is my interpretation wrong?",['probability']
4403738,Weakly shrinking functions on compact spaces,"I have been dealing with this problem for some days without any progress: ""Let $f:K \rightarrow K$ be a weakly shrinking function defined on a compact metric space $K$ (i.e.: $\forall x,y \in K, x \neq y \implies d(f(x), f(y)) \le d(x,y)$ ); then, prove that $f$ is surjective if and only if $\forall x,y \in K, x \neq y \implies d(f(x), f(y)) = d(x,y)$ (that is, the equality always holds in the definition of weakly shrinking function)"" My Attempt So far I have only been able to prove that $f$ is uniformly continuous. Also, in trying to show the direction (surjectivity $\implies$ equality) I had the idea to separate the points on which the equality holds from those on which the strict inequality holds into two sets A and B respectively; then, I thought of reaching a contradiction in the two possible cases: B finite (in which case I should use finiteness somehow) and B infinite (in which case I would probably use the existence of a limit point by the characterization of compact sets). However I haven't made any progress in this direction yet. Any hint on how to proceed or any comment on my idea are highly appreciated as always!","['metric-spaces', 'real-analysis', 'calculus', 'general-topology', 'compactness']"
4403779,Possible error in published proof of Cantor-Schroeder-Bernstein,"I don't understand the proof of Cantor-Schroeder-Bernstein in https://www.maa.org/sites/default/files/pdf/news_old/monthly544-553.pdf (Section 2, Bernstein's Equivalence Theorem, Page 546.) I seem to have found a counterexample to the assertion that ""all sets $X^*$ are normal"". Let $X = \emptyset$ . Then the normality of $X^*$ is equivalent to $f(Q)\subset Q$ . But this can't be true because $Q$ is a subset of the complement of $A''$ and yet $f$ maps to $A''$ .  Am I missing something or is the proof just wrong?  Thank you.","['elementary-set-theory', 'set-theory']"
4403804,why substitution works in general,"Consider the equations $x^2 +y^2=1$ and $y=x^2-1$ . If $y=x^2-1$ is substituted into $x^2 +y^2=1$ to obtain $x^2 +(x^2-1)^2=1$ , then the $x$ values that result from solving this equation will be the $x$ values that produce equal $y$ values in the original equations $y=x^2-1$ and $x^2 +y^2=1$ . I don't understand why this is true on conceptual a level, but rather just accept it as a process. I understand that if I am trying to solve the system $y=x+1$ and $y=2x-1$ , I can set these equations equal to each other and $2x-1=x+1 \implies$ that the $y$ values are the same when $x=2$ . This works because $x+1=2x-1 \iff  x+2=2x \iff x=2$ . I don't understand, however, why this sort of substitution works in general. Is there a proof that this technique works for higher order polynomials and more complicated equations? Please not only answer the questions posed above but also help me identify additional things that I may not understand.",['algebra-precalculus']
4403806,Clarification needed on (one of) the definition(s) of differentiable manifold,"This is taken from page 3 of Olver’s Application of Lie Groups to Differential Equations : I can’t figure out a proof for the highlighted statement. More precisely, what I need to prove is that the collection $$\bigcup_\alpha \left\{ \chi_\alpha^{-1}(W) \colon W \text{ is an open subset of } V_\alpha \right\}$$ satisfies the second condition for a basis (the following definition is taken from Munkres’ Topology ): If $X$ is a set, a basis for a topology on $X$ is a collection $\mathcal B$ of subsets of $X$ (called basis elements) such that For each $x \in X$ , there is at least one basis element $B$ containing $x$ . If $x$ belongs to the intersection of two basis elements $B_1$ and $B_2$ , then there is a basis element $B_3$ containing $x$ such that $B_3 \subset B_1 \cap B_2$ . I thought it would be enough to show that, for any 4-tuple $(\alpha,W_1,\beta,W_2)$ such that $W_1$ is an open subset of $V_\alpha$ and $W_2$ is an open subset of $V_\beta$ , having set $B_1 =\chi_\alpha^{-1}(W_1)$ and $B_2 = \chi_\beta^{-1}(W_2)$ , the set $\chi_\alpha(B_1 \cap B_2)$ is open. In order to do so, I noticed that $$\chi_\alpha(B_1 \cap B_2) = W_1 \cap \left(\chi_\alpha \circ \chi_\beta^{-1}\right) \! \left(W_2 \cap \chi_\beta\!\left(U_\alpha \cap U_\beta\right)\right)$$ My problem is, how do I know that $\chi_\beta\!\left(U_\alpha \cap U_\beta\right)$ is open? This is also necessary for condition (b) of definition 1.1 to make sense, which makes me think that maybe it is an implicit assumption, but I’d like to be sure about that. Thanks for the help.","['manifolds', 'general-topology', 'smooth-manifolds', 'differential-geometry']"
4403865,Is there any variation known to the sum of two squares theorem?,"Originally posed by Fermat and subsequently generalized as sum of two squares theorem , we can see the following statement. An integer greater than one can be written as a sum of two squares if and only if its prime decomposition contains no factor $p^k$ , where prime $p\equiv 3 \pmod {4}$ and $k$ is odd . My question is simple.
Is there any variation known to this theorem? Such as, when we refer the theorem above as $1 \pmod {4}$ version, I would like to know whether there are any $1 \pmod {6}$ version, $1 \pmod {8}$ version, $1 \pmod {12}$ version...and so on. The Diophantine equation won't have to be necessarily similar with the two square version.
Such as, someone might find some property of prime decomposition regarding some modular restriction with a Diophantine equation higher that the degree 2. I've tried to make the question simple, but I'm not sure whether they could have been conveyed to the readers. If the points were not clear, please let me clarify them with further comments. Thanks. Edit : My question was posed to ask for some variation in regard of modular restriction of prime decomposition. For example let's think about some formula A which is a Diophantine polynomial. $$ A = n $$ when $A = a^2 + b^2$ , it is sum of two squares theorem, which I refer as $1 \pmod {4}$ version. My main question is, whether there is any $A$ that makes $n$ on the right hand side being factorized only with $1 \pmod {6}$ numbers, or $1 \pmod {8}$ numbers, or $1 \pmod {12}$ numbers. If there is any, we may refer them as $1 \pmod {6}$ version, $1 \pmod {8}$ version, $1 \pmod {12}$ version. I see that already some of users are sharing their examples which I appreciate. Thank you for your interests on my question.","['prime-factorization', 'sums-of-squares', 'number-theory', 'diophantine-equations', 'faq']"
4403950,Finding a confidence interval for a binomial proportion without knowing the mean or variance?,"I'm just learning statistics and I've been given an interesting problem to solve that I'm unsure how to approach. I've dealt with various tests (t-test, chisq test, confidence intervals etc.) but I'm unsure how to apply it to this problem. Given 20,000 products, we take a random sample of 320 products and find that 59 of them are faulty. Identify with 95% confidence the confidence interval of the ratio of faulty products. Since this is all the information we have, I don't know the mean or variance since this was a single trial, or do we assume mean to be 59 and variance 0?.. I've never dealt with this kind of problem before, and I believe I may be overthinking it.","['statistics', 'confidence-interval']"
4403959,"Showing that max of uniform laws on $[0,\theta]$ is sufficient statistic with definition","Let $X_1, \cdots, X_n$ be i.i.d. $Unif(0,\theta)$ and $T = \max\{X_1,X_2,···,X_n\}$ . Show that T is a sufficient statistic using the definition. So I need to show that for $t>0$ , $\Bbb P(X_1 \leq x_1, \cdots, X_n \leq x_n \lvert T \leq t)$ does not depend on $\theta$ . Here are my computations : $$\Bbb P(X_1 \leq x_1, \cdots, X_n \leq x_n \lvert T \leq t)=\frac{\Bbb P(X_1 \leq x_1, \cdots, X_n \leq x_n , T \leq t)}{\Bbb P( T \leq t)}$$ $$=\frac{\Bbb P(X_1 \leq x_1, \cdots, X_n \leq x_n , T \leq t)}{\Bbb P( T \leq t)}=\frac{ \Bbb P(X_1 \leq \min(x_1,t), \cdots, X_n \leq \min(x_n,t))}{\Bbb P(X_1 \leq t, \cdots, X_n \leq t)}$$ $$=\frac{ \Bbb P(X_1 \leq \min(x_1,t), \cdots, X_n \leq \min(x_n,t))}{\Pi_{i=1}^n \Bbb P(X_i \leq t)}=\frac{\Pi_{i=1}^n \int_{-\infty}^{\min(x_i,t)}\Bbb 1_{[0,\theta]}dx}{\bigg(\int_{-\infty}^{t}\Bbb 1_{[0,\theta]}dx\bigg)^n}$$ It is very technical but we can finally write it as $$\Bbb P(X_1 \leq x_1, \cdots, X_n \leq x_n \lvert T \leq t)=\frac{\Pi_{i=1}^n \min(x_i,t,\theta)}{(\min(t,\theta))^n}$$ Let us suppose for example that $t\geq \theta$ and all $x_i$ are also bigger than $\theta$ then $\Bbb P(X_1 \leq x_1, \cdots, X_n \leq x_n \lvert T \leq t)=\frac{\theta^n}{\theta^n}=1$ which does not depend on $\theta$ . The big issue is starting to be clear : if $t \geq \theta$ and (at least) one of $x_i$ is smaller than $\theta$ then we have that $\Bbb P(X_1 \leq x_1, \cdots, X_n \leq x_n \lvert T \leq t)=\frac{x_i \theta^{n-1}}{\theta^n}=\frac{x_i}{\theta}$ that depends on $\theta$ ! If maths are never wrong, then I am but where ? I was told there must be a mistake in my computations but they seem ok. Can anyone see what's not right ? (I know I can use the equivalent factorisation but I really want to do it by definition).","['statistics', 'sufficient-statistics', 'uniform-distribution', 'solution-verification', 'probability']"
4404009,Extrema of a surface $z=f(x;y)$ when $det(H)=0$,"I'm given the following problem: $\text{Examine}\ z=f(x;y)=x^4+y^4+18xy-9x^2-9y^2+1\text{ for extrema and saddle points.}$ It is trivial to find $\nabla f=(4x^3+18y-18x; 4y^3+18x-18y)$ and the critical points are $P_0(0,0), P_1(3,-3)\text{ and }P_2(-3,3)$ . Also the determinant of the Hessian matrix is easily found (details not included): $$\text{det}(H)=72(2x^2y^2-3x^2-3y^2)$$ Evaluating $\text{det}(H)$ for $P_1$ and $P_2$ gives positive values so there are extrema in these points. Furthermore $f''_{xx}>0$ so these are both minima. For $P_0$ the determinant is $0$ . In our class it was said that in these cases ""further investigation is needed"", but noone provided me with any information about this ""further investigation"". In internet I also found nothing. I plotted the graph of $z=x^4+y^4+18xy-9x^2-9y^2+1$ in GeoGebra and it turned out that in $P_0$ there is a saddle point. But I want to find out how can one analytically determine what to further do when $\text{det}(H)=0$ When in a single-variable calculus $f'(x)=f''(x)=0$ , I keep differentiating $f$ until I get a non-zero derivative. And if the first non-zero derivative is of odd order (i.e. $f^{(3)}, f^{(5)}$ and so on), I know the function has an inflection point; when it is of even order, there is an extremum, whose kind depends on the sign of this non-zero derivative. And so I'm stuck with this problem. I'm not explicitly asked to find what is there in $P_0(0,0)$ , but I want to learn how to tackle such problems. Any help will be appreciated. Thanks in advance! $\textbf{Edit:}$ The answer by @Robert Z is very helpful, but I find it a bit of a guess-and-check method. And if at $P_0(0,0)$ there was an extremum, it wouldn't work out because we can't check all lines, passing through $P_0$ . Any suggestions for the case where there would be an extremum?","['maxima-minima', 'multivariable-calculus', 'calculus', 'optimization', 'hessian-matrix']"
4404019,Show that the inverse norm of Brownian motion in $R^3$ is uniformly integrable,"Let $B$ be an $(\mathcal{F})_t$ Brownian motion with values in $\mathbb{R}^3$ , with $B_0 = x \in \mathbb{R^3}\backslash\{0\}$ . Let $M_t := \lVert B_t \rVert ^{-1}$ . Show that $M_t$ is uniformly integrable by showing that for all $\gamma \in (0, 3)$ , there exists a constant $C_{\gamma}$ such that for all $t \geq 0$ , $\mathbb{E}[M_t^{\gamma}] \leq C_{\gamma} \min(1, t^{-\gamma/2})$ The purpose of the problem is to show that there exists a local martingale which is uniformly integrable but is not a martingale. So far I've shown that $M_t$ is a local martingale, by applying Ito's formula on $f(B_t)$ , where $f(x, y, z) = (x^2+y^2+z^2)^{-\frac{1}{2}}$ . The integral in the quadratic variation ends up being 0. Edit: I believe it has to do with using the Burkholder-Davis-Gundy inequality, but I can't figure out how.","['stochastic-processes', 'brownian-motion', 'probability-theory']"
4404042,What property is being used to relate $\arctan(x)$ and $\ln(1 + x^2)$ in these complex integrals?,"I was looking at a couple of definite integrals on this site which had a combination of $\arctan(x)$ and $\ln(1+x^2)$ in the numerator, and I notices that in the solutions to these integrals there was a step relating these function using complex analysis which I didn't understand. For example, in this answer it is stated that $$
\int _0^{\infty }\frac{\arctan ^2\left(x\right)\ln ^2\left(1+x^2\right)}{x\left(1+x^2\right)}\mathrm{d}x = -\frac{2}{3}\Re \left\{\int _0^{\infty }\frac{\ln ^4\left(\frac{i}{i+x}\right)}{x\left(1+x^2\right)}\mathrm{d}x\right\}+\frac{2}{3}\int _0^{\infty }\frac{\arctan ^4\left(x\right)}{x\left(1+x^2\right)}\mathrm{d}x+\frac{1}{24}\int _0^{\infty }\frac{\ln ^4\left(1+x^2\right)}{x\left(1+x^2\right)}\mathrm{d}x
$$ where the product of $\arctan ^2\left(x\right)\ln ^2\left(1+x^2\right)$ has been ""split up"" into integrals involving only logarithms or arctangent, plus the real part of another complex logarithm. It is not obvious to me how this step takes place. Another example is in this answer where it is stated that $$
\int_0^1 \frac{\Re\left\{\left(2\arctan(x)i-\ln\left(\frac{(1-x)^2}{1+x^2}\right)
\right)^n\right\}}{x}dx = -2^n \int_0^1 \frac{\Re\{(\log(1-x)-\log(1+ix))^n\}}{x}dx
$$ Where now the expression managed to get rid of the $\arctan(x)$ part completely. Could somebody tell me what complex numbers property is being used in these integral manipulations to relate arctangent and logarithms? Thank you very much!","['integration', 'definite-integrals', 'logarithms', 'complex-analysis', 'complex-integration']"
4404052,"Why is the Traveling Salesperson Problem ""Difficult""?","The Traveling Salesperson Problem is originally a mathematics/computer science optimization problem in which the goal is to determine a path to take between a group of cities such that you return to the starting city after visiting each city exactly once and the total distance (longitude/latitude) traveled is minimized. For $n$ cities, there are $(n-1)!/2$ unique paths - and we can see that as $n$ increases, the number of paths to consider becomes enormous in size. For even a small number of cities (e.g. 15 cities), modern computers are unable to solve this problem using ""brute force"" (i.e. calculate all possible routes and return the shortest route) - as a result, sophisticated optimization algorithms and approximate methods are used to tackle this problem in real life. I was trying to explain this problem to my friend, and I couldn't think of an example which shows why the Travelling Salesperson Problem is difficult! Off the top of my head, I tried to give an example where someone is required to find the shortest route between Boston, Chicago and Los Angeles - but then I realized that the shortest path in this case is pretty obvious! (i.e. Move in the general East to West direction). Real world applications of the Travelling Salesperson Problem tend to have an additional layer of complexity as they generally have a ""cost"" associated between pairs of cities - and this cost doesn't have to be symmetric. For example, buses might be scheduled more frequently to go from a small city to a big city, but scheduled less frequently to return from the big city to the small city - thus, we might be able to associate a ""cost"" with each direction. Or even a simpler example, you might have to drive ""uphill"" to go from City A to City B, but drive ""downhill"" to go from City B to City A - thus there is likely a greater cost to go from City A to City B. Many times, these ""costs"" are not fully known and have to be approximated with some statistical model. However, all this can become a bit complicated to explain to someone who isn't familiar with all these terms. But I am still looking for an example to explain to my friend - can someone please help me think of an obvious and simple example of the Travelling Salesperson Problem where it becomes evidently clear that the choice of the shortest path is not obvious? Every simple example I try to think of tends to be very obvious (e.g. Manhattan, Newark, Nashville) - I don't want to overwhelm my friend with an example of 1000 cities across the USA : just something simple with 4-5 cities in which it is not immediately clear (and perhaps even counterintuitive) which path should be taken? I tried to show an example using the R programming language in which there are 10 (random) points on a grid - starting from the lowest point, the path taken involves choosing the nearest point from each current point: library(ggplot2)

set.seed(123)

x_cor = rnorm(5,100,100)
y_cor = rnorm(5,100,100)


my_data = data.frame(x_cor,y_cor)

      x_cor     y_cor
1  43.95244 271.50650
2  76.98225 146.09162
3 255.87083 -26.50612
4 107.05084  31.31471
5 112.92877  55.43380


ggplot(my_data, aes(x=x_cor, y=y_cor)) + geom_point() + ggtitle(""Travelling Salesperson Example"") But even in this example, the shortest path looks ""obvious"" (imagine you are required to start this problem from the bottom most right point): I tried with more points: set.seed(123)

x_cor = rnorm(20,100,100)
y_cor = rnorm(20,100,100)


my_data = data.frame(x_cor,y_cor)

ggplot(my_data, aes(x = x_cor, y = y_cor)) +
    geom_path() +
    geom_point(size = 2) But my friend still argues that the ""find the nearest point from the current point and repeat"" (imagine you are required to start this problem from the bottom most right point): How do I convince my friend that what he is doing corresponds to a ""Greedy Search"" that is only returning a ""local minimum"" and it's very likely that a shorter path exists? (not even the ""shortest path"" - just a ""shorter path"" than the ""Greedy Search"") I tried to illustrate this example by linking him to the Wikipedia Page on Greedy Search that shows why Greedy Search can often miss the true minimum : https://en.wikipedia.org/wiki/Greedy_algorithm#/media/File:Greedy-search-path-example.gif Could someone help me think of an example to show my friend in which choosing the immediate nearest point from where you are, does not result in the total shortest path? (e.g. some example that appears counterintuitive, i.e. if you choose a path always based on the nearest point from your current position, you can clearly see that this is not the optimal path) Is there a mathematical proof that shows that the ""Greedy Search"" algorithm in Travelling Salesperson has the possibility of sometimes missing the true optimal path? Thanks!","['graph-theory', 'optimization', 'combinatorics', 'discrete-mathematics']"
4404174,Quickly calculate the probability of 12 dice having sum less than 30,"I try to quickly answer the question of whether one should play a game: roll 12 fair dice and sum up the face values; if the sum is less than 30, win 10 dollars, otherwise lose 1 dollar. Let $S_{12}=\sum\limits_{i=1}^{12} x_i$ be the sum where $x_i$ is the value for the ith die. I think if the expected gain is greater than 0, we should play the game, and the gain is $$E = 10 \times P(S_{12}<30) - 1\times [1-P(S_{12}< 30)] >0 ?$$ So I tried to quickly calculate or approximate $P(S_{12}<30)$ . My way is through CLT, $$\frac{S_{12} - 12E[x_i]}{\sqrt{12 var(x_i)} } \xrightarrow{d} N(0,1)$$ $$\Rightarrow P(S_{12} < 30) \approx p(z < \frac{30-42}{\sqrt{35}}) = \Phi \left( \frac{30-42}{\sqrt{35}} \right)  $$ But this approach still seems to require a certain amount of computation. Especially, it requires evaluating the normal cdf $\Phi(z)$ . So I was wondering if there is a better/quicker way without referring to computers/calculators. Thanks in advance.","['statistics', 'approximation', 'combinatorics', 'combinatorial-game-theory', 'probability']"
4404209,"How to calculate median, variance and correlation on two-dimensional random variable. [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Two random variables, X and Y, have the joint density function: $$f(x, y) = \begin{cases} 2 & 0 < x \le y < 1 \\ 0 & ioc\end{cases}$$ Calculate the correlation coefficient between X and Y. I am pretty much stuck because y is an upper limit for x, and x is a bottom limit for y; so calculating medians and such is proving too hard for me. I'd appreciate it if anyone could lend me a hand and teach me how to solve this. If it helps save some time, the marginal equations are $f_x (x) = 2 - 2x$ and $f_y (y) = 2y$ .","['variance', 'median', 'correlation', 'multivariable-calculus', 'multivariate-statistical-analysis']"
4404220,Can we construct a group with exactly $k$ Sylow-Subgroups?,"Inspired by the answers given by these three questions ( here , here , and here ), what is the general solution for constructing a group with a specific number of Sylow subgroups? That is, given a prime $p$ and a positive integer $n\equiv1\pmod p$ , is it always possible to construct a group $G$ with exactly $n$ subgroups of order $p$ ? For example, is it possible to construct a group with exactly $15$ subgroups of order $7$ , or exactly $35$ subgroups of order $17$ ? From Hölder's Theorem (Theorem 19 here ), if there is at least one prime $q \mid n$ such that $q \neq 1\pmod p$ , then we can conclude that $g=|G|$ cannot be squarefree. At the same time, it is not a necessary condition that every prime $q\mid n$ is congruent to $1\pmod p$ . In our example with $n=15$ and $p=7$ , we know that $g$ cannot be square-free. From the Sylow Theorems, $np \mid g$ , so $g$ is a multiple of $105$ . However, $g$ is at least $315$ because of the requirement that $g$ must not be square-free. That is as far as I can get with constructing such groups. Edit: Derek's answer suggests that the answer to my original question is false in general. That is, it is not always possible to construct a group with $n$ subgroups of order $p$ , even when $n$ is restricted to $1\pmod p$ .
How would one prove this? Secondly, under what conditions would there exist a group with exactly $n$ $p$ -Sylow subgroups?","['group-theory', 'abstract-algebra', 'finite-groups', 'sylow-theory']"
4404228,How many solutions has $x_1 + 2x_2 + 2x_3 = 200$ with non-negative integers?,"Problem: Let $x_1$ , $x_2$ and $x_3$ be integers such that $x_1 \geq 0$ , $x_2 \geq 0$ and $x_3 \geq 0$ . How many solutions does the following equation have: $$ x_1 + 2x_2 + 2x_3 = 200 $$ Answer: Let $c$ be the number of solutions of this equation. For the case where $x_1$ is odd, the equation has no solutions. Now I consider the smallest case of $x_1$ where $x_1 = 0$ . I now
have the following equation: $$2x_2 + 2x_3 = 200 $$ or $$ x_2 + x_3 = 100 $$ This equation has $101$ solutions.  Now I consider the case where $x_0 = 2$ . I now
have the following equation: $$2 + 2x_2 + 2x_3 = 200 $$ or $$ x_2 + x_3 = 99 $$ This equation has $100$ solutions.  Now I consider the case where $x_0 = 4$ . I now
have the following equation: $$4 + 2x_2 + 2x_3 = 200 $$ or $$ x_2 + x_3 = 98 $$ This equation has $99$ solutions. Now I consider the case where $x_0 = 200$ . I now
have the following equation: $$100 + 2x_2 + 2x_3 = 200 $$ or $$ x_2 + x_3 = 0 $$ This equation only has one solution. \begin{align*}
c &= \sum_{i = 0}^{100} i+1 = \sum_{i = 1}^{100} i + \sum_{i = 0}^{100} 1 \\
\sum_{i = 1}^{100} i &= \dfrac{ 100(101) }{2} = 50(101) \\
\sum_{i = 0}^{100} 1 &= 101 \\
c &= 50(101) + 101 \\
c &= 5151
\end{align*} Is my solution correct?","['solution-verification', 'combinatorics']"
4404243,Proving Geometric definition of divergence of vector field as given by Tristan Needham,"In page-479 of Visual Complex Analysis, Tirstan Needham derives the flux of a vector field in Geometric form: Here $z$ is the point to which the shaded region $R$ will ultimately be collapsed in order to find the divergence there, $S$ and $P$ are the streamline and orthogonal trajectory through $z$ , and $s$ and $p$ are arclength along $S$ and $P$ , the direction of increasing $p$ being choosen to make a positive right angle with $X$ $$ \nabla \cdot X = \partial_s |X| + \kappa_p |X|$$ The $\partial_S$ is a derivative along streamlines of the vector field $X$ and $\kappa_S$ is the curvature of the streamline  at the point we are taking divergence at. In the derivation of the formula using inifinitesimals, the following identity is used: $$ \delta ( dp)= \kappa_p ds dp= \kappa_p dA$$ I am trying to derive it. My attempt : $$ dp = r_p d \theta$$ Where $r_p$ is radius of osculating circle at $p$ of the orthogonal trajectory of $S$ and $d \theta$ is the angle in common with circle circle and arclength of the orthogonal trajectory. Assuming that the $d \theta$ doesn't change from the labelled dp to the red marked edge , we have: $$ \delta dp = dr_p d \theta \tag{1}$$ But we can consider the shaded region as a differential area of a circle: $$ dA= d(r^2) \frac{ d \theta}{2} = r_p dr_p d \theta$$ Rerranging $$ \frac{dA}{r_p} = \kappa_p dA = dr_p d \theta$$ .  Plugging that into (1) we have: $$ \delta (dp) = \kappa_p dA$$ Does my derivation look like what was intended?","['curvature', 'differential-forms', 'differential-geometry']"
4404245,Solid enclosed by the paraboloid $\frac{y^2}{b^2}+\frac{z^2}{c^2}=2\frac{x}a$ and the plane $x=a.$,"Compute the volume of the solid enclosed by the paraboloid $$\frac{y^2}{b^2}+\frac{z^2}{c^2}=2\frac{x}a$$ and the plane $x=a.$ My attempt: I considered slices of the solid parallel with the plane $x=a$ and summed up their areas. $$\begin{aligned}y&=br\sqrt{\frac2a}\sin\varphi\\z&=cr\sqrt{\frac2a}\cos\varphi,\\\varphi&\in[0,2\pi],\\r&\in[0,\sqrt x],\\x&\in[0,a]\\\text{Jacobian 
: } J_\psi&={2bc r}a\\\int_0^a\int_0^{2\pi}\int_0^{\sqrt x}\frac{2bcr}adrd\varphi dx&=\frac{2bc}a2\pi\int_0^a\frac{x}2dx\\&=\frac{2bc}a2\pi\frac12\int_0^axdx\\&=\frac{2bc}a\pi\frac{a^2}2\\&=abc\pi\end{aligned}$$ Is this right and if so, is there any other efficient method?","['integration', 'multivariable-calculus', 'solution-verification']"
4404269,$f$ is uniformly continuous iff $ \|\tau_y f - f \|_u \rightarrow 0 $ as $y \to 0$ (Proof verification),"I'm trying to prove the following elementary fact mentioned on page 238 of Folland's Real Analysis : A function $f$ is called uniformly continuous if $\|\tau_y f - f \|_u \rightarrow 0$ as $y \to 0$ . (The reader should pause to check that this is equivalent to the usual $\epsilon$ - $\delta$ definition of uniform continuity.) I am taking Folland's advice and trying to show the equivalence of these two definitions of uniform continuity. First, some notation and definitions: If $f$ is a function on $\mathbb{R}^d$ and $y \in \mathbb{R}^d$ , then $\tau_y f(x) := f(x-y)$ . $\|f \|_u := \sup\left\{|f(x)|: x \in \mathbb{R}^d \right\}$ . ( $\|\cdot \|_u$ is called the uniform norm .) $e_1 := (1,0,\ldots,0) \in \mathbb{R}^d$ My working $\epsilon$ - $\delta$ definition of uniform continuity: A function $f: \mathbb{R}^d \to \mathbb{C}$ is uniformly continuous if, for each $\epsilon > 0$ , there exists some $\delta > 0$ such that $$x,y \in \mathbb{R}^d \text{ and } |x-y| < \delta \implies |f(x) - f(y)| < \epsilon.$$ My proof : [ Edit 3/21/22: This proof is wrong! See updated proof below. ] $\implies$ direction : Suppose $f$ is uniformly continuous in the $\epsilon$ - $\delta$ sense. Then we may pick a sequence of positive real numbers $\{\delta_n\}_{n=1}^{\infty}$ such that $$ x,y \in \mathbb{R}^d \text{ and } |x-y| \leq \delta_n \implies |f(x) - f(y)| < \frac{1}{n}$$ for each $n \in \mathbb{N}$ . Then $|f(x - \delta_n e_1) - f(x)| < \frac{1}{n}$ for all $x \in \mathbb{R}^d$ , for each $n \in \mathbb{N}$ (since $|(x - \delta_n e_1) - x| = |\delta_n e_1| = \delta_n \leq \delta_n$ ), and so $\sup\{|f(x-\delta_n e_1) - f(x)|: x \in \mathbb{R}^d \} \leq 1/n$ for each $n \in \mathbb{N}$ . In other words, $\|\tau_{\delta_n e_1}f - f \|_u \leq 1/n$ for all $n$ . Then since $\{\delta_n e_1 \}_{n=1}^{\infty}$ is a sequence in $\mathbb{R}^d$ and $\delta_n e_1 \rightarrow 0$ as $n \to \infty$ (with respect to $| \cdot |$ , the standard metric on $\mathbb{R}^d$ ), we have \begin{align*}
   \lim_{y \to 0} \|\tau_{y}f - f \|_u &= \lim_{n \to \infty} \|\tau_{\delta_n e_1} f - f \|_u \leq \lim_{n \to \infty} \frac{1}{n} = 0.
\end{align*} Thus, $\lim_{y \to 0} \|\tau_{y}f - f \|_u  = 0$ . (Just a note that $y \in \mathbb{R^d}$ here, so $y \to 0$ really means $y \to (0,\ldots,0)$ .) $\impliedby$ direction : Suppose $\|\tau_y f - f \|_u \to 0$ as $y \to 0$ . Then for each $\epsilon > 0$ , there exists a $\delta > 0$ such that \begin{align*}
    & y \in \mathbb{R}^d \text{ and } |y| < \delta \implies \|\tau_y f - f \|_u < \epsilon \\[4pt]
\implies \quad & y \in \mathbb{R}^d \text{ and } |y| < \delta \implies \sup \left\{|f(x-y) - f(x)|: x \in \mathbb{R}^d \right\} < \epsilon \\[4pt]
\implies \quad & y \in \mathbb{R}^d \text{ and } |y| < \delta \implies |f(x-y) - f(x)| < \epsilon \text{ for all } x \in \mathbb{R}^d \\[4pt]
\implies \quad & x,y \in \mathbb{R}^d \text{ and } |x - y| < \delta \implies |f(x) - f(y)| < \epsilon,
\end{align*} and so $f$ is uniformly continuous in the $\epsilon$ - $\delta$ sense. $\qquad \square$ Does this proof look ok? Any feedback or suggestions for improvement are welcomed. Update 3/21/22 : Thanks to @Mason for pointing out the issue with my "" $\implies$ "" direction proof. I've attempted to write a corrected proof: Corrected "" $\implies$ "" proof: Suppose $f$ is uniformly continuous in the $\epsilon$ - $\delta$ sense. Then for each $\epsilon > 0$ , there exists a $\delta > 0$ such that \begin{align*}
    & x,y \in \mathbb{R}^d \;\text{ and }\; |x-y| < \delta \implies |f(x)-f(y)| < \epsilon \\[3pt]
 \implies \quad & x,y \in \mathbb{R}^d \;\text{ and }\; |(x-y) - x| < \delta \implies |f(x-y) - f(y)| < \epsilon \\[3pt]
   \implies \quad & x,y \in \mathbb{R}^d \;\text{ and }\;  |y| < \delta \implies |f(x-y) - f(x)| < \epsilon \\[3pt]
    \implies \quad & y \in \mathbb{R}^d \;\text{ and }\;  |y| < \delta \implies |f(x-y) - f(x)| < \epsilon \quad \text{for all } x \in \mathbb{R}^d \\[3pt]
   \implies \quad & y \in \mathbb{R}^d \; \text{ and }\; |y| < \delta \implies \sup_{x \in \mathbb{R}^d} |f(x-y) - f(x)| \leq \epsilon \\[3pt]
    \implies \quad & y \in \mathbb{R}^d \; \text{ and }\; |y| < \delta \implies \|\tau_y f - f \|_u \leq \epsilon.
\end{align*} Does this look okay now?","['solution-verification', 'uniform-continuity', 'analysis', 'real-analysis']"
4404272,Fisher Information and Cramér-Rao lower bound problem,"Suppose $X_1,...,X_n$ are random samples from $N(\mu, \sigma^2)$ , where both $\mu$ and $\sigma \gt 0$ are unknown, and let $\theta = \sigma^p$ for some $p \gt 0$ . I want to find the Fisher Information of $\theta$ , and the Cramér-Rao lower bound for the variance of any unbiased
estimator for $\theta$ . I am kind of confused of this assumption $\theta = \sigma^p$ , and I don't quite know how to deal with it. Any help is welcome","['statistical-inference', 'statistics', 'parameter-estimation', 'sampling']"
4404294,Alternative strategy to choose the largest random number,"I want to discuss a strategy to determine what is the largest number of a random sample. Suppose that we generate a random sample of $n$ distinct numbers from $0$ to $1$ according to a uniform distribution. The sample generated will be labeled as $I = \{d_1,...,d_n\}$ where $0\leq d_1 < d_2 < ... < d_n\leq 1$ (I have sorted the sample for simplicity). Let us randomly shuffle the sample. This would give us a particular permutation of $I$ that I will label as $$
S = \{z_1,z_2,...,z_n\}
$$ Each of the elements $z_i$ is shown one by one to a person who has to decide whether $z_i$ is the maximum of the whole sample. If this person chooses $z_i$ as the maximum and they are right, they win, otherwise, they lose. If this person doesn't choose $z_i$ , then they need to make a decision about $z_{i+1}$ . They can accept $z_{i+1}$ as the maximum, and check if they win, or they refuse $z_{i+1}$ and check $z_{i+2}$ and so on. I want to test the following strategy to make a decision. Notice that I am not sure if the following strategy is optimal in any sense (and I don't care at this point). Assume that we refuse the first $k-1$ elements. If the $k$ -th element $z_k$ is greater than all the others drawn so far, we consider $z_k$ as a candidate for the maximum. If $z_k$ is a candidate, the probability that the next $n-k$ elements are smaller than $z_k$ is $z_k^{n-k}$ . I decide the element $z_k$ is the maximum if $z_k^{n-k}\geq r$ for some cut-off $r$ that maximizes the probability of winning the game under such strategy. Ultimately, I want then to find the value of such cut-off $r$ .
Let me define the following events $$
A_k : z_k = d_n \\
B_k: z_k^{n-k}\geq r\, \cap\,z_k > z_i \forall i = 1,..,k-1 
$$ I think that the probability of winning under these conditions is $$
P(r) = \sum_{k=1}^{n}p(B_k)p(A_k|B_k)\,.
$$ Question: Is the expression for $P(r)$ right? By means of the Bayes theorem, $P(r)$ takes the form $$
P(r) =\sum_{k=1}^{n}p(A_k)p(B_k|A_k)\,.
$$ Now, $$p(A_k)= 1/n$$ and $P(B_k|A_k)$ is simply the probability that $d_n^{n-k}\geq r$ , since $d_n > z_i \forall i = 1,..,k-1 $ is always true, therefore $$
P(B_k|A_k) = (1-r^\frac{1}{n-k})
$$ and therefore I get $$
P(r) = \sum_{k=1}^{n}\frac{1}{n}(1-r^\frac{1}{n-k})
$$ However, this cannot be correct. For $r=0$ , this last equation says that $P(0)=1$ , which is really wrong. I have the feeling I have messed around with the formulae and fundamental concepts of probability. Can anyone help me? EDIT I have implemented this strategy numerically on Mathematica for $n=100$ . Look at the following code checkStrategy[r_, n_] := Module[{win = 0, loss = 0, list, max, k, i},
  For[i = 1, i <= 1000, i++,
   list = RandomSample[Range[10 n], n]/(10 n) // N;
   max = Max[list];
   For[k = 1, k <= n, k++,
    If[list[[k]]^(n - k) >= r && list[[k]] > Max[list[[1 ;; k - 1]]], 
     If[list[[k]] == max, win = win + 1; k = n + 1, k = n + 1];]
    ];
   ];
  loss = 1000 - win;
  Return[{r, win/(win + loss), loss/(win + loss)} // N]]

SeedRandom[42];
points = Table[checkStrategy[s, 100], {s, 0, 0.99, 0.01}];
points[[All, 1 ;; 2]] // ListPlot which returns the following plot As you can see, the probability has a maximum for values of $r$ between 0.4 and 0.6. Notice that this is Problem 48 of the book ""Fifty Challenging Problems in Probability with Solutions"". In this book, the authors give a strategy whose probability of success is 0.58 at large $n$ (not so different from what I have found numerically).","['conditional-probability', 'probability-theory', 'probability']"
4404307,How to show that $f(x)=|x^2-3x+2|$ is not differentiable at $x=1$,"Consider the function $$f(x)=|x^2-3x+2|$$ over the interval $[0,3]$ . Intuitively I know that $f$ is not differentiable at $x=1$ but when I calculate the limits $$\lim_{x\rightarrow 1^+} \frac{f(1+h)-f(1)}{h}\;\;\text{and}\;\;\lim_{x\rightarrow 1^-} \frac{f(1+h)-f(1)}{h},$$ I find they are equal and so by definition the limit $$\lim_{x\rightarrow 1} \frac{f(1+h)-f(1)}{h}$$ exists and the function is differentiable at $x=1$ . Have I made an error in my logic?","['calculus', 'derivatives']"
4404325,Karatzas & Shreve 2.6: Show that the hitting time $H_{\Gamma}$ is an optional time.,"Consider a stochastic process $X$ with right-continuous paths, which is adapted to a filtration $\{ \mathscr{F}_t \}$ . Consider subset $\Gamma \in \mathscr{B}(\mathbb{R}^d)$ of the state space of the process, and define the hitting time as follows: \begin{align*}
  H_{\Gamma} &: \Omega \to \mathbb{R} \cup \{\infty\} \\
  H_{\Gamma}(\omega) &= \inf \{t \ge 0; X_t(\omega) \in \Gamma \} \\
\end{align*} Where the infimum of the empty set is infinity. If the set $\Gamma$ is open, show that $H_{\Gamma}$ is an optional time, such that $\{ H_{\Gamma} < t \} \in \mathscr{F}_t$ . The solution I have given below doesn't use the fact that $\Gamma$ is open or that the sample paths to $X$ are right-continuous. Is there a mistake or omission? Consider the set: \begin{align*}
  \bigcup\limits_{s \in \mathbb{Q}: 0 \le s < t} \{ X_s \in \Gamma \} \\
\end{align*} This is the subset of $\Omega$ where for any $\omega$ in this subset there is some $s \in [0,t)$ such that $X_s(\omega) \in \Gamma$ . Therefore $H_{\Gamma}(\omega) < t$ or $\omega \in \{ H_{\Gamma} < t \}$ . This gives us: \begin{align*}
  \bigcup\limits_{s \in \mathbb{Q}: 0 \le s < t} \{ X_s \in \Gamma \} \subset \{ H_{\Gamma} < t \} \\
\end{align*} Now, for the converse direction, consider any $\omega \in \{ H_{\Gamma} < t \}$ such that $H_{\Gamma}(\omega) < t$ . That means that for some $s < t$ , we have $X_s(\omega) \in \Gamma$ or $\omega \in \{ X_s \in \Gamma \}$ . This gives us: \begin{align*}
  \{ H_{\Gamma} < t \} \subset \bigcup\limits_{s \in \mathbb{Q}: 0 \le s < t} \{ X_s \in \Gamma \} \\
\end{align*} Combining the two directions we have: \begin{align*}
  \{ H_{\Gamma} < t \} = \bigcup\limits_{s \in \mathbb{Q}: 0 \le s < t} \{ X_s \in \Gamma \} \\
\end{align*} Since the expression on the right is a countable union of $\mathscr{F}_t$ measurable elements, the expression on the right is $\mathscr{F}_t$ measurable and therefore so is the equal expression on the left. Then we conclude that $H_{\Gamma}$ is an optional time.","['stochastic-processes', 'probability-theory', 'filtrations']"
4404370,What is the apparent contradiction in this integral?,"I was solving this exercise, however it is proposed that there is a possible contradiction in the exercise but I cannot determine what it is. The idea is to find the integral of $\int{\frac{1}{\sin(x)\cos(x)}dx}$ For this purpose, the following is expressed $\int{\frac{1}{\sin(x)\cos(x)}dx}=\int{\frac{\cot(x)}{\cos^2(x)}dx}=\int{\cot(x)\tan'(x)dx}=\cot(x)\tan(x)-\int{\tan(x)\cot'(x)dx}=1+\int{\frac{\tan(x)}{\sin^2(x)}dx}=1+\int{\frac{1}{\sin(x)\cos(x)}dx}$ Where does the failure occur? thank you for your help.","['integration', 'indefinite-integrals', 'calculus']"
4404379,What is a tuple?,"In theory of computation, DFA's, NFA's, etc. are represented as a ""tuple"". Probability spaces are tuples. I am confused on what the notion of a tuple is and how it differs from a set?","['definition', 'discrete-mathematics', 'elementary-set-theory', 'probability-theory', 'computer-science']"
4404416,Find a limit as product of cos,"$$\lim\limits_{n\to\infty}\cos\frac{1}{n\sqrt{n}}\cos\frac{2}{n\sqrt{n}}\cdots\cos\frac{n}{n\sqrt{n}}$$ It is not hard to prove that the limit exists, but is that possible to calculate the limit? Suggestions are welcome!","['limits', 'calculus']"
4404436,Number of ways to arrange $n$ integers from $1$ to $n$ such that $k-1$ or $k-2$ is to the left of $k$,"How can I find the number of ways to arrange $n$ integers in a line satisfying the following conditions: I. The first number in the line is $1$ . II. $\forall k = 2..n $ in the line, there must exist at least the number $k-1$ or $k-2$ to the left of it. For example with $n=5$ , the sequence $(1,3,5,4,2)$ satisfies the above conditions, whereas $(1,2,5,3,4)$ does not. My attempt: We define $a_n$ the number of arrangements satisfy the above conditions. A bit of brute-force yields $a_1 =1, a_2=1, a_3=2, a_4=4, a_5=10$ . I noticed that $a_{n+2} = a_{n+1} + na_n$ , but was unable to prove it. I let $a_n = b_n + c_n$ where $b_n$ is the number of arrangements such that $n-2$ is to the left of $n$ and $n-1$ , $c_n$ is the number of arrangements such that $n-1$ is to the left of $n$ and $n-2$ (it is obvious that $n$ cannot be to the left of both $n-1$ and $n-2$ ). But it was too hard to find the recurrence relations of the two sequence since I thought I had to find the exact position of $n-1$ and $n$ in every arrangement in order to find the number ways to put $n+1$ in so I'm clueless now.","['combinatorics', 'recurrence-relations']"
4404447,"How to find all the integer pairs $(x,y)$ satisfying the equation $x^{17}+6 x^{14}+2 x^{5}=y !+2 $","We are going to solve the equation $$x^{17}+6 x^{14}+2 x^{5}=y !+2 \tag*{(*)}$$ by the  Fermat Little Theorem: $$x^3\equiv x \quad  \pmod 3 \quad \forall x\in Z.$$ Assume that $y \geqslant 3$ . Applying the  Fermat Little Theorem to $(*)$ in modulo $3$ yields $$x^{5} \equiv x^3\cdot  x ^2\equiv x^3 \equiv x$$ $$x^{17} \equiv\left(x^{5}\right)^{3} \cdot x^2  \equiv x^{3}\cdot x^2\equiv x\\y !=0 $$ Expressing the equation $(*)$ in modulo $3$ yields $$ x+0+ 2x\equiv 2 \quad(\bmod 3) \\ 0\equiv 2 \quad(\bmod 3) $$ which is a contradiction. $$\boxed{ \textrm{There is no such integer pairs for }y\geqslant 3.}$$ Now let’s check for the integer pairs $(x,y)$ for $y=0,1,2$ . When $y=2$ , $$0 \equiv x+0+2x\equiv 2!+2 \equiv 1   \pmod 3,  $$ which is a contradiction. $\therefore$ there is no such integer solution. When $y=0,1$ , $(*)$ becomes $$f(x)=x^{17}+6 x^{14}+2 x^{5}-3=0$$ because $f(-1)=0$ but $f(1) \neq 0$ and $f(\pm 3) \neq 0$ $\therefore x=-1$ is the ONLY integer solution. Now we can conclude that the integer pairs $(x, y)$ satisfying $(*)$ are only $(-1,0)$ and $(-1,1)$ . My Question Is there simpler solutions?","['number-theory', 'solution-verification']"
4404508,Evaluate $\int \frac{1}{1+x^{12}} d x$ using three partial fractions,"For this question, we need to resolve integrands into partial fractions. The process is rather tedious and time consuming. Therefore I want to minimise the number steps as least as possible. I shall resolve the integral of $$\frac{1}{1+x^{12}}$$ into three partial fractions . If you are interested in it you may try yourself, just read the footnotes or trust me. \begin{aligned}\int \frac{1}{1+x^{12}} d x&=\frac{1}{3}  \left[\int \frac{d x}{1+x^{4}}-\int \frac{x^{4}-2}{1-x^{4}+x^{8}} d x\right]\\&=\frac{1}{3}\left[\underbrace{\int \frac{d x}{1+x^{4}}}_{I_1}-\frac{1}{2} \underbrace{  \int \frac{\sqrt{3} x^{2}-2}{x^{4}-\sqrt{3} x^{2}+1} dx }_{I_2} \displaystyle +\frac{1}{2} \underbrace{\int \frac{\sqrt{3} x^{2}+2}{x^{4}+\sqrt{3} x^{2}+1} d x}_{I_3}\right] \end{aligned} $$I_1=\displaystyle \int \frac{d x}{1+x^{4}}=\frac{1}{4 \sqrt{2}}\left[2 \tan ^{-1}\left(\frac{x^{2}-1}{2 x}\right)+\ln \left|\frac{x^{2}+\sqrt{2} x+1}{x^{2}-\sqrt{2} x+1}\right|\right]+c_1$$ $$ I_{2}  =\int \frac{\sqrt{3} x^{2}-2}{x^{4}-\sqrt{3} x^{2}+1} d x  =\int \frac{\sqrt{3}-\frac{2}{x^{2}}}{x^{2}+\frac{1}{x^{2}}-\sqrt{3}} d x$$ $\displaystyle \text {Let } \sqrt{3}-\frac{2}{x^{2}}=A\left(1-\frac{1}{x^{2}}\right)+B\left(1+\frac{1}{x^{2}}\right). $ $\text {Then } A+B=\sqrt{3} \cdots(1) \text { and }-A+B=-2 \cdots(2) $ $(1)-(2) \text { gives } \displaystyle A=\frac{\sqrt{3}+2}{2} \\$ $(1)+(2) \text { gives } \displaystyle B=\frac{\sqrt{3}-2}{2}$ \begin{aligned}I_{2} &=\frac{\sqrt{3}+2}{2} \int \frac{1-\frac{1}{x^{2}}}{x^{2}+\frac{1}{x^{2}}-\sqrt{3}} dx +\frac{\sqrt{3}-2}{2} \int \frac{1+\frac{1}{x^{2}}}{x^{2}+\frac{1}{x^{2}}-\sqrt{3}} d x \\
&=\frac{\sqrt{3}+2}{2} \int \frac{d\left(x+\frac{1}{x}\right)}{\left(x+\frac{1}{x}\right)^{2}-(2+\sqrt{3})} +\frac{\sqrt{3}-2}{2} \int \frac{d\left(x-\frac{1}{x}\right)}{\left(x-\frac{1}{x}\right)^{2}+(2-\sqrt{3})} \\
& =\frac{\sqrt{3}+2}{4 \sqrt{2+\sqrt{3}}} \ln \left|\frac{x+\frac{1}{x}-\sqrt{2+\sqrt{3}}}{x+\frac{1}{x}+\sqrt{2+\sqrt{3}}}\right|+\frac{\sqrt{3}-2}{2 \sqrt{2-\sqrt{3}}} \tan ^{-1} \frac{x-\frac{1}{x}}{\sqrt{2-\sqrt{3}}} +c_2\\
&=\frac{\sqrt{3}+1}{4 \sqrt{2}} \ln \left| \frac{\sqrt{2} x^{2}-(\sqrt{3}+1) x+\sqrt{2}}{\sqrt{2} x^{2}+(\sqrt{3}+1) x+\sqrt{2}}\right|\quad -\frac{\sqrt{3}-1}{2 \sqrt{2}} \tan ^{-1}\left(\frac{\sqrt{2} \left(x^{2}-1\right)}{(\sqrt{3}-1) x}\right)+c_2 \end{aligned} Similarly, $$\quad I_{3}=-\frac{\sqrt{3}-1}{4 \sqrt{2}} \ln \left|\frac{\sqrt{2} x^{2}-(\sqrt{3}-1) x+\sqrt{2}}{\sqrt{2} x^{2}+(\sqrt{3}-1) x+\sqrt{2}} \right|+\frac{\sqrt{3}+1}{2 \sqrt{2}} \tan ^{-1}\left(\frac{\sqrt{2}\left(x^{2}-1\right)}{(\sqrt{3}+1) x}\right)+c_3$$ We can now conclude that \begin{aligned} I &=\frac{1}{3}\left(I_{1}-\frac{1}{2} I_{2}+\frac{1}{2} I_{3}\right)\\ \displaystyle &=\frac{1}{24 \sqrt{2}} \Big[2 \ln \left|\frac{x^{2}+\sqrt{2} x+1}{x^{2}-\sqrt{2} x+1}\right|+4 \tan ^{-1}\left(\frac{x^{2}-1}{2 x}\right)\\
\displaystyle &\quad +(\sqrt{3}+1) \ln \left|\frac{\sqrt{2} x^{2}+(\sqrt{3}+1) x+\sqrt{2}}{\sqrt{2} x^{2}-(\sqrt{3}+1) x+\sqrt{2}}\right|+2(\sqrt{3}-1) \tan ^{-1}\left(\frac{\sqrt{2}\left(x^{2}-1\right)}{(\sqrt{3}-1) x}\right)\\
\displaystyle &\quad +(\sqrt{3}-1) \ln \left|\frac{\sqrt{2} x^{2}+(\sqrt{3}-1) x+\sqrt{2}}{\sqrt{2} x^{2}-(\sqrt{3}-1) x+\sqrt{2}}\right|+2(\sqrt{3}+1) \tan ^{-1}\left(\frac{\sqrt{2}\left(x^{2}-1\right)}{(\sqrt{3}+1) x}\right)\Big]+C \end{aligned} Is there other any simpler method? For completeness and checking, I also evaluate $\displaystyle \quad \int_0^1 \frac{1}{1+x^{12}}dx$ $\displaystyle =\frac{1}{24 \sqrt{2}}\left[2 \ln \left(\frac{2+\sqrt{2}}{2-\sqrt{2}}\right)+(1+\sqrt{3}) \ln \left(\frac{2 \sqrt{2}+\sqrt{3}+1}{2 \sqrt{2}-\sqrt{3}-1}\right)\right.\\$ $\displaystyle \left.+(\sqrt{3}-1) \ln \left(\frac{2 \sqrt{2}+\sqrt{3}-1}{2 \sqrt{2}-\sqrt{3}+1}\right)+2(1+\sqrt{3}) \pi\right] \\$ $ \doteq 0.94747$ (checked by Wolframalpha )","['integration', 'indefinite-integrals', 'calculus']"
4404516,Differential equations with a derivative in the exponent,I would like to know if there is a method to find solutions to ODEs where the derivative is in the exponent. For example: $y^{y'}=y''$ Thanks!,['ordinary-differential-equations']
4404610,How can I convert an if..else if..else statement to a single formula,"I have this program where I made a variable x, and I want to assign it a value based on this formula (where c is a positive integer): $$
x = f(c) = 
\begin{cases}
c & c<100\\100 & 100<=c<200\\c-100 & c>=200
\end{cases}
$$ At the moment I have this in some code using an if statement, but I'd like to have this using a single assignment with just a formula and no if statements. int x;
  if (c < 100) 
    x = c;
  else if (c < 200)
    x = 100;
  else
    x = c-100; I've heard that if statements like this can be converted into some formula, but I'm not sure where to start, any help would be appreciated!
In the end, I'll hopefully have just one statement: int x = f(c);","['recreational-mathematics', 'discrete-mathematics']"
4404665,Is this a conjugate prior?,"I have a parameter $\theta = (\theta_1, \theta_2)^\top\in\mathbb{R}^2$ with a multivariate normal prior $p(\theta) = \mathcal{N}(\theta\mid 0, I)$ . The likelihood is a univariate normal distribution $\mathcal{N}(y \mid \theta_1 + \theta_2, \sigma^2)$ . Is the posterior a normal distribution as well? I know in general the product of normals is not normal. But I was wondering if in this case it is. Not sure how to check it.","['statistics', 'probability-distributions', 'bayesian', 'probability-theory', 'probability']"
4404782,Algorithm for expressing a homogeneous polynomial with integer coefficients nonnegatively (if possible) in terms of specific dependent binomials,"I have specific homogeneous polynomials expressed in terms of binomials of the form $$x_i+y_j$$ for $i,j\in\mathbb N$ with integer coefficients that I conjecture can have nonnegative coefficients when expressed in terms of these binomials. I have an algorithm for generating the polynomials, but the specific expressions the algorithm generates often do not have nonnegative coefficients; in fact if the algorithm did express them visibly with nonnegative coefficients, provably in all cases, I'd likely win a Fields medal. I'm looking for an algorithm for expressing these polynomials with nonnegative coefficients (if possible) in terms of these binomials, starting with an expression that can have arbitrary, possibly negative, coefficients. I suspect my algorithm generates expressions where this can be achieved with cancellation by inspection, but I have no evidence for this. I think this is probably something where a general algorithm, starting with an arbitrary polynomial with integer coefficients in terms of products of binomials of the form $x_i+y_j$ and ending with an expression with nonnegative coefficients (if possible) would be preferable instead of me telling you what the specific polynomials are, as I don't think it would help to know them. For those ""in the know,"" or those who want to look it up, they are the result of applying a skew divided difference operator to a double Schubert polynomial, i.e. there are permutations $u,v,w$ such that the polynomial is $$\partial_u^w(\mathfrak{S}_v(x;-y))$$ Skew divided difference operators (with different conventions from mine, though for this purpose it is irrelevant) are defined in this paper where the symmetric group acts trivially on the $y$ variables, and double Schubert polynomials are defined here , though I assure you the references will likely not help. As requested, an example polynomial where my algorithm generates a negative coefficient is $$ -(y_{1}+ z_{1}) (y_{1} + z_{2}) + (y_{1} + z_{1}) (y_{4} + z_{1}) + (y_{1} + z_{2}) (y_{1} + y_{3} + z_{1} + z_{2}) + (y_{3} + z_{2}) (y_{4} + z_{1})$$ It should not be difficult to eyeball the cancellation that makes this representable positively.","['algebraic-geometry', 'abstract-algebra', 'polynomials', 'schubert-calculus']"
4404789,"Trouble with ""only if""","This is from pg. 45 of Discrete Mathematics with Applications by Epp: I'm having trouble understanding the last sentence. If we say that $p$ is John breaking the world's record and $q$ is John running the mile in under four minutes, doesn't $q \Longrightarrow p$ say that if John runs the mile under four minutes, he will break the world record? It seems like she meant to say that ""His time could be over four minutes and still break the record."" regarding the case where $p$ is true and $q$ is false.",['logic']
4404832,Optimal strategy for getting the larger envelope of two envelopes,"I encountered a problem similar to the two envelope paradox: Two envelopes with amounts $x$ and $2x$ , where $x$ is uniformly distributed in $[0,100]$ . Randomly shuffle the two envelopes, and you pick one and observe the amount. You can choose to switch or stay. What is the optimal strategy to get the envelope with the larger amount? An intuitive strategy is to switch if the envelope has less than 100. But I am not sure how to show this strategy in fact maximizes the probability of getting the envelope with the larger amount. Here is what I tried: \begin{align*}
P(\text{getting large envelop}) &= P(\text{getting large envelop } | \text{ observed <100})P(\text{ observed <100})\\ 
&+ P(\text{getting large envelop } | \text{ observed >100})P(\text{ observed >100})\\
&= P(\text{getting large envelop} | \text{ observed <100}) \cdot \frac{3}{4} + 1 \cdot \frac{1}{4} \\
\end{align*} but how can I show that the strategy maximizes $P(\text{getting large envelop} | \text{ observed <100})$ ? Thanks.","['game-theory', 'statistics', 'probability']"
4404836,"Generalization of central limit theorem to sums of the form $\sum_{k=1}^n\frac{1}{X_k^m}$, $m=1,2,\dots$?","Consider this lesser-known fact: If $X\sim\mathcal N(\mu,\sigma^2)$ then $$
\frac{\frac{1}{n}\sum_{k=1}^n\frac{1}{X_k}-\mathsf E_\mathcal PX^{-1}}{\pi f_X(0)}\overset{d}{\to}\operatorname{Cauchy}(0,1),
$$ where $\operatorname{Cauchy}(0,1)$ is the standard Cauchy distribution and $$
\mathsf E_\mathcal PX^{-1}=\mathcal P\int_{-\infty}^\infty\frac{f_X(x)}{x}\,\mathrm dx
$$ is the Cauchy principal value integral for the expected value of $1/X$ . This result bears a striking resemblance to the central limit theorem and indeed may be considered part of the generalized central limit theorem . Because suitably normalized sums of $1/X$ converge to the Cauchy distribution we say that $1/X$ lies within the domain of attraction of the Cauchy law. Question: Are there analogous results for sums containing $1/X^m$ with $m\in\Bbb N$ ? In other words, what stable distributions do sums of $1/X^m$ converge to and what are the associated location and scaling constants? Thoughts: For the case $n=2$ , I believe $1/X^2$ lies within the domain  of attraction of the Levy distribution so that we have potentially something of the form $$
\frac{a\sum_{k=1}^n\frac{1}{X_k^2}-b}{c}\overset{d}{\to}\operatorname{Levy}(0,1),
$$ where $\operatorname{Levy}(0,1)$ is the standard Levy distribution. But how to find the constants $a$ , $b$ , and $c$ is outside my current understanding of the subject.","['probability-limit-theorems', 'central-limit-theorem', 'probability-theory', 'cauchy-principal-value']"
4404919,$\lim_{n\rightarrow \infty} n^{2} \sum_{k=1}^{n}\frac{1}{(n^2+k^2)(\sqrt{n^2+k^2}+n)}$.,"Find $$\lim_{n\rightarrow \infty} n^{2}\sum_{k=1}^{n}\frac{1}{(n^2+k^2)(\sqrt{n^2+k^2}+n)}.$$ My approach: \begin{align*}
I& =\lim_{n\rightarrow \infty} n^{2}\sum_{k=1}^{n}\frac{1}{(n^2+k^2)(\sqrt{n^2+k^2}+n)}=\lim_{n\rightarrow \infty}\frac{1}{n}\sum_{k=1}^{n}\frac{1}{(1+(\frac{k}{n})^2)(1+\sqrt{1+(\frac{k}{n})^2})}\\ & =\int_{0}^{1} \frac{1}{(1+x^2)(1+\sqrt{1+x^2})}dx \Longrightarrow 
\end{align*} \begin{align*}I & =\int_{0}^{1} \left(\frac{1}{x^2+1}-\frac{1}{\sqrt{x^2+1}}+\frac{1}{1+\sqrt{x^2+1}}\right)dx\\ & =\arctan(1)-\ln(1+\sqrt{2})+\int_{0}^{1}\frac{1}{1+\sqrt{x^2+1}}dx\end{align*} and at this point I am stuck. Any help, please?","['integration', 'limits', 'definite-integrals', 'sequences-and-series']"
4404930,Equivalence of group representations under base change,"Let $G$ be a group and $\rho_{1},\rho_{2}:G\rightarrow \operatorname{GL}(V)$ be two representations of $G$ over a field $k$ . Let $K/k$ be a field extension. Suppose that $\rho_{1}\otimes_{k}K$ is equivalent to $\rho_{2}\otimes_{k}K$ . Is it true that $\rho_{1}$ and $\rho_{2}$ are equivalent? I know the answer is positive if $k$ is an infinite field. What if the case when $k$ is a finite field? Here is a proof of the case when $k$ is an infinite field: Consider the matrix representations. By assumption, there is an $A\in \operatorname{GL}_{n}\left(K\right)$ such that $\rho_{1}\left(g\right)A=A\rho_{2}\left(g\right)$ for all $g\in G$ . Then, write $$
A=e_{1}A_{1}+\cdots +e_{m}A_{m}
$$ for some $A_{i}\in M_{n}\left(k\right)$ and $\left\{e_{i}\right\}$ is $k$ -linear independent subset of $K$ . By the linearly independence of $e_{i}$ , we have $\rho_{1}\left(g\right)A_{i}=A_{i}\rho_{2}\left(g\right)$ for all $g\in G$ and $i$ . Then, consider the polynomial $$
f\left(x_{1},\ldots,x_{m}\right)=\det\left(x_{1}A_{1}+\cdots +x_{m}A_{m}\right)\in k\left[x_{1},\ldots,x_{m}\right].
$$ Since $k$ is infinite, we may choose some non-zero $\left(a_{1},\ldots,a_{m}\right)\in k^{m}$ such that $f\left(a_{1},\ldots,a_{m}\right)\neq 0$ . Then, set $$
B=a_{1}A_{1}+\cdots +a_{m}A_{m}\in\operatorname{GL}\left(k\right).
$$ Then, $\rho_{1}\left(g\right)B=B\rho_{2}\left(g\right)$ for all $g\in G$ and thus $\rho_{1}$ is equivalent to $\rho_{2}$ . Thanks in advanced.","['finite-fields', 'group-theory', 'abstract-algebra', 'representation-theory']"
4404961,"Ratio between circumference and ""radius"" of a polygon","Given some polygon $P$ in two-dimensional Euclidean space, I want to define the radius of $P$ as the average of the radii of the smallest outer circle and the largest inner circle. An outer circle has all points of $P$ inside of it (touching points allowed) and conversely, an inner circle has them outside of it. Note that their centers do not have to coincide. I want to show (as I suspect) that $2\pi$ is a lower bound for the perimeter of the polygon $P$ divided by the radius of $P$ , i.e. that circles are the shape with the smallest value of "" $\pi$ "". My idea: If I fix the smallest outer and largest inner circle of $P$ , I know they can be described by using their touching points. Keeping those fixed and changing everything else, it's easy to see how to minimize the circumference: You simply connect the points defining the inner circle directly (or, whenever they would have to traverse the inner circle, along that circle) with those defining the outer circle.
Now, for the last step, it suffices to show that in both cases 1) the outer circle is fixed and 2) the inner circle is fixed: the minimum ratio (perimeter divided by radius) occurs when inner and outer circles coincide. This is where I am struggling. Any hints are appreciated!","['euclidean-geometry', 'circles', 'geometry', 'polygons', 'pi']"
4404974,Can we approximate the most likely event's probability if we know the distribution's entropy?,"Suppose that we know that a distribution $p$ over small $n$ elements has entropy $H(p)=0.01$ . I intentionally chose a small number. Is there some kind of inequality or aproximation to the probability $p_i$ for $i = \arg\max_j p_j$ ? What I came up with so far is the following inequality: $H(p) = \sum _j -p_j \log(p_j) \geq -p_i \log(p_i)$ . This gives us in principle an inequality for $p_i$ , because if we assume that the most likely event has probability larger than $0.5$ (a consequence of the entropy being sufficiently small for a given $n$ ), then this inequality $H(p) \geq -p_i \log(p_i)$ should give us a bound on $p_i$ in terms of the entropy. I'm not sure how to solve that inequality for $p_i$ though, or if it's even possible. Maybe an approximation would help as well, that works for small entropy. Is there a bound or approximation on the maximum probability $p_i$ in terms of the entropy, that works for sufficiently small entropy?","['entropy', 'approximation', 'information-theory', 'upper-lower-bounds', 'probability']"
4404991,Generalization of the law of tangents for a cyclic quadrilateral,"The law of tangents is a statement about the relationship between the tangents of two angles of a triangle and the lengths of the opposing sides. Let $a$ , $b$ , and $c$ be the lengths of the three sides of a triangle, and $\alpha$ , $\beta$ and $\gamma$ be the angles opposite those three respective sides. The law of tangents states that $$\frac{\tan\frac12(\alpha-\beta)}{\tan\frac12(\alpha+\beta)}=\frac{a-b}{a+b}\tag{1}$$ The law of tangent can be used in any case where two sides and the included angle, or two angles and a side, are known. Although Viète gave us the modern version of the law of tangents, it was Fincke who stated the law of tangents for the first time and also demonstrated its application by solving a triangle when two sides and the included angle are given (see Wu - The Story of Mollweide and Some Trigonometric Identities ) A proof of the law of tangent is provided by Wikipedia (see here ). Generalization . Let $a$ , $b$ , $c$ and $d$ be the sides of a cyclic convex quadrilateral. Let $\angle{DAB}=\alpha$ and $\angle{ABC}=\beta$ , then the following identity holds $$\frac{\tan\frac12(\alpha-\beta)}{\tan\frac12(\alpha+\beta)}=\frac{(a-c)(b-d)}{(a+c)(b+d)}\tag{2}$$ Proof . Using the sum-to-product formulas we can rewrite the left-hand side of $(2)$ as follows $$\frac{\tan\frac12(\alpha-\beta)}{\tan\frac12(\alpha+\beta)}=\frac{\sin\frac12(\alpha-\beta)\cos\frac12(\alpha+\beta)}{\cos\frac12(\alpha-\beta)\sin\frac12(\alpha+\beta)}=\frac{\sin{\alpha}-\sin{\beta}}{\sin{\alpha}+\sin{\beta}}.$$ The area of a cyclic quadrilateral can be expressed as $\Delta=\frac12(ad+bc)\sin{\alpha}$ (see $(12)$ at Killing three birds with one stone ) and similarly for the other angles. Then substituting, simplifying and factorizing we have $$\begin{align*}\frac{\tan\frac12(\alpha-\beta)}{\tan\frac12(\alpha+\beta)}&=\frac{\frac{2\Delta}{ad+bc}-\frac{2\Delta}{ab+cd}}{\frac{2\Delta}{ad+bc}+\frac{2\Delta}{ab+cd}}=\frac{ab-ad+cd-bc}{ab+ad+cd+bc}=\frac{(a-c)(b-d)}{(a+c)(b+d)}\end{align*}.$$ $\square$ The formula $(2)$ reduces to the law of tangent for a triangle when $c=0$ . A related result can be found at A generalization of Mollweide's formula (rather Newton's) . Crossposted at MO . Question: Is this generalization known? A historical correction : Fincke was not the first to publish the law of tangents. It was actually Ibn Muadh who first described it in the 11th century. See the discussion at HSMSE .","['euclidean-geometry', 'trigonometry', 'reference-request']"
4405033,Understanding some concepts about Monodromy,"I'm reading Simpson's paper ""Higgs Bundles and Local Systems"". There he defines for a local system $V$ of vector spaces on a compact Kahler manifold $M$ we have a representation of fundamental group $\pi_1(M,x) \to GL(V_x)$ . Define the monodromy group $M(V,x)$ to be the Zariski closure of the image of $\pi_1(M,x)$ . He goes on to say that there's an alternative description of monodromy group.For each $T^{a,b}V = V^{\otimes a} \otimes V^{\otimes b}$ identify the set of sub local systems $W \subset T^{a,b}V$ . Then $M(V,x)$ is the subgroup of $GL(V_x)$ of all $g$ such that $g(W_x) \subset W_x$ for all sub local systems $W$ . Here are my questions : How does the alternative description relate to the usual one? How exactly does $g$ act on $W_x$ ? He also says that if $V$ is semisimple local system (which is the same as the fundamental group representation being semisimple) then $M(V,x)$ is a reductive group. Why is that so?","['monodromy', 'representation-theory', 'local-systems', 'smooth-manifolds', 'differential-geometry']"
4405051,When is this linear combination of Normals exactly zero?,"Setup : Suppose I have a mean-zero multivariate normal $X = (X_1, X_2, X_3, X_4)$ , where each component has strictly positive but possibly unequal variance. $(X_1, X_3)$ is independent of $(X_2, X_4)$ . $X_1$ and $X_3$ are dependent, and $X_2$ and $X_4$ are dependent. Consider $a = (a_1,a_2,a_3,a_4)$ where the $a_i$ are either $1$ or $-1$ for each $i =1,\dots,4$ . Question : When is $ a^TX  = 0$ with positive probability? Suppose $(a_1,a_2)\neq -(a_3,a_4)$ to rule out the trivial case where $X_1=X_3$ and $X_2 = X_4$ . Are strictly positive variances somehow enough to make this a probability zero event? Why I am asking: The variance of $X$ is $$\Sigma = \begin{pmatrix} \sigma_1^2 &0 & \sigma_{13} & 0\\ 0 &\sigma_2^2 &0 & \sigma_{24}\\ \sigma_{13} &0 & \sigma^2_{3} & 0\\ 0 &\sigma_{24} &0 & \sigma^2_{4} \end{pmatrix}$$ This looks a bit strange, which makes me believe that there is a better characterization of $P(a^TX \neq 0) =1$ than "" $a^T \Sigma a$ has to be positive,'' especially because the choice of $a$ is restricted.","['statistics', 'probability-theory', 'probability', 'normal-distribution']"
4405052,Quick way of drawing Hasse diagrams of posets,"When drawing a Hasse diagram, I have seen that you can draw a bigraph for the poset and remove the reflexive and transitive edges of the poset. However, doing this for a poset with many elements can get tedious (by hand). Is there a more efficient way of drawing a Hasse diagram?","['order-theory', 'discrete-mathematics']"
4405099,Integrate $\int_0^{\infty} \frac{\sin^2 x}{\cosh x\>+\>\cos x}\frac{dx}x $,"It is known that (see for example ) \begin{align}
&\int_0^{\infty} \frac{\sin x}{\cosh x+\cos x}\frac{dx}x =\frac\pi4\\
&\int_0^{\infty} \frac{\sin^3 x}{\cosh x+\cos x}\frac{dx}x =\frac\pi8
\end{align} I am wondering if the similar integral below $$\int_0^{\infty} \frac{\sin^2 x}{\cosh x+\cos x}\frac{dx}x $$ can be evaluated to a simple close-form as well. I have tried the same approaches for the known integrals above and they do not lead to any results. Alternatively, I have also manipulated the integral and expressed it in the equivalent form $$\int_0^\infty e^{-x}\cos x\tanh x \>\frac{dx}x
$$ which, though appearing simpler, is not any easier.","['integration', 'trigonometric-integrals']"
4405126,Is this a valid proof for the area of a circle?,"My teacher challenged my class to prove that the area is $$A=\pi r^2.$$ We recently learned about Riemann sums, so I thought it would be possible to apply them to them to deriving the formula for the area of the circle.  I know similar proofs exist, but this is one that I genuinely came up with on my own and I am wondering if it is valid. Please tell me if there is anything invalid about this proof or how it can be improved. Imagine splitting a circle into an infinite number of isosceles triangles, where two legs extend from a vertex at the center of the circle to the edge of the circle.  The central angle that each triangle makes can be represented as $\frac{2\pi}{n}$ , where $n$ is the number of triangles in the circle. The area of a triangle is $A=\frac{1}{2}ab\sin{C}$ .  Since the legs of each of the triangles extends from the center of the circle to the edge, that means that $a=b=r$ , the radius of the circle.  Therefore, the summation of every triangle in the circle as the number of triangles approaches infinity can be represented as: $\lim_{ n\to\infty} \sum_{i=1}^{n} \frac{1}{2}r^2\sin(\frac{2\pi}{n})$ which can be rewritten as: $\lim_{ n\to\infty} \frac{1}{2}r^2n\sin(\frac{2\pi}{n}) = 2\pi(\frac{1}{2})r^2 = \pi r^2$","['integration', 'definite-integrals', 'circles', 'geometry', 'calculus']"
4405138,"How to prove $\int_0^1 \frac{\arctan^2(x)\ln\left(\frac{x}{(1-x)^2}\right)}x \, \mathrm{d}x=G^2$?","A while back I made a post asking for examples of integrals which evaluated to famous irrational constants (or constants that were very likely irrational but yet unproven to be). The top answer in said post was by Quanto, who posted this equation: $$\int_0^1 \frac{\arctan^2(x)\ln\left(\frac{x}{(1-x)^2}\right)}x \, \mathrm{d}x=G^2 $$ where $G$ is Catalan's constant . I decided to try and prove said equation. These were my attempts: Attempt 1: The integral can be split into a linear combination of the $2$ integrals $I_1 = \int_0^1 \frac{\arctan^2(x)\ln\left(x\right)}x \, \mathrm{d}x$ and $I_2 = \int_0^1 \frac{\arctan^2(x)\ln\left(1-x\right)}x \, \mathrm{d}x$ such that the original integral $I = I_1 - 2I_2$ . The first integral was evaluated in this answer to be $$
\int_0^1 \frac{\arctan^2(x)\ln\left(x\right)}x \, \mathrm{d}x = \operatorname{Li}_4 \left (\frac{1}{2} \right ) - \frac{151 \pi^4}{11520} + \frac{7}{8} \zeta (3) \ln(2) - \frac{\pi^2}{24} \ln^2(2) + \frac{1}{24} \ln^4(2)
$$ Following the same steps used to evaluate $I_1$ for $I_2$ we get \begin{align*}
\int_{0}^{1}\frac{\arctan^2(x)\ln\left(1-x\right)}x \, \mathrm{d}x & =-\frac{\pi^4}{96} + 2\int_{0}^{1}\frac{\arctan(x)\, \mathrm{Li}_2(x)}{x^2+1} \, \mathrm{d}x\\
& = -\frac{\pi^4}{96} - 2\int_0^1 \int_0^1\frac{\arctan(x) \ln(1-xy)}{(x^2+1)y}\, \mathrm{d}y \, \mathrm{d}x
\end{align*} which I couldn't find a way to continue evaluating even exploiting the change of the order of integration. Attempt 2: Trying the substitution $u = \frac{1-x}{1+x}$ gives \begin{align*}
\int_0^1 \frac{\arctan^2(x)\ln\left(\frac{x}{(1-x)^2}\right)}x  \, \mathrm{d}x & = \int_{0}^{1} \left( \frac{\pi}{4} - \arctan(u) \right)^2 \ln\left(\frac{1-u^2}{4u^2} \right)\frac{2}{1-u^2} \, \mathrm{d}u
\end{align*} This allows us to split the integral into several other integrals, however, several of the resulting integrals seemed to me equally hard to evaluate compared to the original. So I felt this approach was more akin to cutting off one hydra head, just to have two more take its place. Attempt 3: I noticed that given real numbers $a, l$ then $\Re\{l^3 - (l + ia)^3\} = 3a^2l$ . This meant that taking $a = \arctan(x)$ and $l = \ln\left(\frac{x}{(1-x)^2} \right)$ we could get: $$
\int_0^1 \frac{\arctan^2(x)\ln\left(\frac{x}{(1-x)^2}\right)}x \, \mathrm{d}x  = \frac{1}{3}\Re \left\{ \int_{0}^{1}\left[ \ln^3\left(\frac{x}{(1-x)^2} \right)- \frac{1}{2^3} \ln^3\left( \frac{(i-x)x^2}{(i+x)(1-x)^4}\right)\right] \frac{\mathrm{d}x}{x}\right\}
$$ which I had the hopes of being able to split up, but unfortunately the integrals don't converge separately. Since the final result is so concise, I have hopes that there's a clever way to evaluate the integral which avoids going into all the polylogarithm-territory where similar integral evaluations end up going to. I suspect (with no evidence, though) that the integral can be cleverly split up into a separable double integral, where each of the individual integrals would evaluate to $G$ by themselves (and hence their product resulting in $G^2$ ). This would kind of be like doing the evaluation of the Gaussian integral in reverse, starting at the single integral $I = 2\pi \int_{0}^{\infty}e^{-x^2} x \, \mathrm{d}x$ and then splitting it into $\left(\int_{\mathbb{R}}e^{-x^2} \, \mathrm{d}x\right)\left(\int_{\mathbb{R}}e^{-y^2} \, \mathrm{d}y\right)$ , except that in this case the resulting product of integrals would be a product of known integral representations of Catalan's constant. But this is only conjecture as I haven't been able to spot a clever way to do this. Does anyone have any ideas on how to evaluate this integral? Either by continuing/improving on my attempts or trying something else entirely, everything is welcome. Thank you very much!","['integration', 'definite-integrals', 'calculus', 'closed-form', 'catalans-constant']"
4405160,"A ""smooth"" version of this function?","Let $(L,M,H)\in\mathbb{R}^3$ s.t. $L<M<H$ . I am seeking a function $\varphi:\mathbb{R}\to[0,1]$ that satisfies the following: $\varphi(M)=1$ and $\varphi(x)=0$ $\forall x\in(-\infty,L]\cup[H,\infty)$ . $\varphi'(x)\geq0$ $\forall x\in[L,M) $ and $\varphi'(x)\leq0$ $\forall x\in[M,R] $ Continuously-differentiable Something that achieves the first two criteria -- but not the third -- is the ""triangle function"" $f(x;L,M,H)$ , given by \begin{equation}
f(x;L,M,H) := \begin{cases}
0, & \text{if } x<L \\ 
\frac{x-L}{M-L}, & \text{if } x \in[L,M)\\
1-\frac{x-M}{H-M}, & \text{if }  x\in[M,R]\\
0 , & \text{if } x>R 
\end{cases}
\end{equation} Is there a continuously-differentiable function that achieves all three criteria? Reason for question: I am tinkering with a model. I need a unimodal function with a range [0,1], whose bounded support ( $[L,R]$ ) and mode $M$ can be flexibly manipulated. The ""triangle function"" $f$ I defined above acheives this, but its ""kink"" is making life difficult.","['recreational-mathematics', 'functions', 'probability-theory']"
4405174,Why this Taylor expansion of $ \dfrac{1}{1+\cos x}$ is wrong?,"I want to expand the function $$ \dfrac{1}{1+\cos x} $$ at $x = 0$ The way I did this is first regard $\cos x $ as a whole and denote it by $y$ The expansion will become $$ \dfrac{1}{1+y}=1-y+y^2+o(y^2)$$ and then return $y=\cos x$ back and I want to expand each $\cos x$ at $ x=0$ Since $$ \cos x = 1-\dfrac{x^2}{2} + o(x^2) $$ $ o(y^2) = o(\big(1-\dfrac{x^2}{2} + o(x^2)\big)^2)=o(x^2)$ and the result is $$ \dfrac{1}{1+\cos x}=1-1+\dfrac{x^2}{2}+1-x^2+o(x^2)=1-\dfrac{x^2}{2}+o(x^2)$$ but this is wrong Thus, my question is why this is wrong. Also by the same way, I can correctly get the expansion of $ \tan(\tan x)$ and $\sin(\sin x)$ For example, $ \tan(\tan x)$ . I want to expand this at $ x=0$ I regard the inner $\tan x$ as $y$ , so the expansion will be $$ \tan(y)=y+\dfrac{1}{3}y^3+o(y^3)$$ Then by some calculations, I can get $ o(y^3)= o(x^3),\ \ y=\tan x=x+\dfrac{1}{3}x^3+o(x^3), \ \ y^3=\tan^3 x=(x+o(x))^3 =x^3+o(x^3)$ Then the result is $$ x+\dfrac{2}{3}x^3+o(x^3)$$ which is right","['taylor-expansion', 'analysis']"
4405249,Finding the solution of: $5^m -m^n=mn$,"I want to find all solutions to: $5^m -m^n=mn \tag*{}$ Where $(m,n) \in \mathbb{N}$ I rearranged and got: $5^m= m^n + mn \tag{1}$ Taking $\bmod 5$ on both sides I got: $5|m \text{ or } 5|m^{n-1}+n\tag*{}$ I am having a problem with how to proceed from here. Any help would be appreciated. Thanks. EDIT $1$ : I tried to find the parity of $(m,n)$ . Taking $\bmod 2$ in $(1)$ . We get: $1 = m^n + mn \bmod 2 \tag*{}$ Note that if $m$ is even, it doesn't satisfy the equation above, and if $m$ is odd and $n$ is odd too doesn't satisfy so $\boxed{\text{m is odd and n is even}}$","['number-theory', 'discrete-mathematics', 'modular-arithmetic', 'elementary-number-theory']"
4405267,'Non-algebraic Calabi-Yau' threefolds,"By a Calabi-Yau threefold I mean a simply connected compact Kahler threefold with trivial canonical bundle. By an algebraic compact complex manifold, I mean one that admits a closed immersion into a complex projective space. What are examples of non-algebraic Calabi-Yau threefolds? Or is there none? What I know: if the Calabi-Yau threefold $ M $ has $ h^{2,0} (M) = 0 $ , then it is algebraic. (Algebraicity holds for any compact Kahler manifold with the vanishing condition, btw). So any such example must have nonzero $ h^{2,0} $ . I don't know the Ricci flow viewpoint to this subject, so I'm probably missing something but I'm eager to learn.","['manifolds', 'algebraic-geometry', 'complex-geometry', 'kahler-manifolds']"
4405308,How to take limits of 'almost Riemann' sums like $\lim_{n \to \infty} \sum_{k=0}^n \frac{1}{n} \cos (a \pi k \log(n)/n)$,"How can I solve limits of sums that are 'almost' Riemann, but can't be written in the typical form (i.e, $\lim_{n \to \infty} \sum_{k=0}^n \frac{1}{n} f(k/n)$ which we can rewrite as an integral $\int_0^1 f(x) dx$ )? For example, I'd like to turn $\lim_{n \to \infty} \sum_{k=0}^n \frac{1}{n} \cos (a \pi k \log(n)/n)$ into an integral. More generally, is there a closed-form way to convert sums like $\lim_{n \to \infty} \sum_{k=0}^n \frac{1}{n} f(k h(n)/n)$ into integrals, when $h(n) = o(n)$ ? Clearly such sums don't always converge - if $h(n) = \log(n)$ , and $f(kh(n)/n) = k\log(n)/n$ , the limit is unbounded. What conditions on $f, h$ might we require for convergence?","['integration', 'riemann-sum', 'calculus', 'limits', 'riemann-integration']"
4405361,Contraction of a tensor over all indexes?,"In Chapter 2.3 of this book , let $F \in T^k_l(V)$ and $\{\omega^i\}_{i=1}^l \in V^*, \{X_i\}_{i=1}^k$ , then it is clear that $$
F \otimes \omega^1 \otimes \cdots \otimes w^l \otimes X_1 \otimes \cdots \otimes X_k \in T^{2k}_{2l}(V)
$$ so the contraction of this tensor over all indexes is $$
\text{tr}(F \otimes \omega^1 \otimes \cdots \otimes w^l \otimes X_1 \otimes \cdots \otimes X_k) = F(\omega^1,\dotsc,\omega^l,X_1\dotsc,X_k)
$$ ...which I don't understand at all. I know that $\text{tr}$ can be viewed as an operation that reduce the rank of the tensor by 2 via $$  
  \text{tr}(F)(\omega^1,\dotsc,\omega^l,X_1,\dotsc,X_k) = \text{tr}(F(\omega^1,\dotsc,\omega^l, \star\ ,X_1,\dotsc,X_k, \star\ ))
  $$ but if we want to sum through all indexes we need to have $l = k $ , so how is it possible to take the contraction over all indexes on a general tensor?","['contraction-operator', 'tensor-products', 'differential-geometry']"
4405384,"What does it mean for a topological space to be ""normal"" (T4)?","I'm working through Topology Through Inquiry, and while I grok the meaning of T1 and T2 spaces (they roughly quantify the ability of a topology to separate the elements in the space), what the meaning of T3 and T4 spaces is less clear. I'm not really asking for a definition (that's plenty clear to me), but rather an intuition for why being able to ""separate"" closed sets makes a topology ""more able to separable"" beyond ""points are the smallest possible closed sets"".",['general-topology']
4405388,Does any symmetric surface look like a surface of revolution? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let $(M^2,g,\boldsymbol{\xi})$ be a two-dimensional Riemannian manifold endowed with a Riemannian metric $g$ (strictly positive signature) and a Killing field $\boldsymbol{\xi}$ . Will there be an isometric embedding into $\mathbb{R}^3$ such that it is symmetric under rotations about some straight line ?","['riemann-surfaces', 'surfaces', 'riemannian-geometry', 'differential-geometry']"
4405394,Determine affine linear f $\in C^1(R)$ such that the differential form is exact,"Determine affine linear function $f$ such that the differential form $\omega$ is exact, where $$
\omega (x,y) = (6x^2y+2y^3+2yf(x^2+y^2))dx + (3y^2-2xf(x^2+y^2))dy.
$$ I started the exercise but at some point I can't continue.
I calculated the partial derivatives and imposed them to be equal. $$
\begin{split}
\frac{\partial}{\partial y}F_1  & = 6x^2+6y^2+2f(x^2+y^2)+4y^2f'(x^2+y^2) \\
& = -2f(x^2+y^2)-4x^2f'(x^2+y^2) = \frac{\partial}{\partial x}F_2
\end{split}
$$ Doing the math I found $$
6(x^2+y^2)+4f(x^2+y^2)+4(x^2+y^2)f'(x^2+y^2)= 0
$$ The problem is in solving this differential equation.
I have placed $x^2+y^2 = s$ so $$
6(s)+4f(s)+4(s)f'(s)= 0
$$ can someone help me?","['differential-forms', 'ordinary-differential-equations', 'real-analysis']"
4405397,"Prove that $\sup_{(a, b)} f’_{-} (x) \geq 0 \geq \inf_{(a, b)} f’_{-}(x) $","Function $f$ is continuous on $[a, b]$ and has a $f’_{-}$ on $(a, b)$ . $f(a) = f(b)$ , then show that $$\sup_{(a, b)} f’_{-}(x) \geq 0 \geq \inf_{(a, b)} f’_{-}(x) $$ Here’s my attempt.
Consider 2 cases: $f$ is constant, then the statement is obvious. $f$ is not a constant. Since $f$ is continuous, there is a minimum $m$ and maximum $M$ on $[a, b]$ . Then $m = f(x_1)$ , $M = f(x_2)$ . If both $x$ s are in $(a, b)$ , the statement is proven, because $f’_{-}(x_1) \leq 0, f’_{-}(x_2) \geq 0$ .
I don’t know what to do if only one of the $x$ s lies in $(a, b)$ . Do you have any ideas?","['proof-writing', 'derivatives']"
4405410,Is there a connection between exact differentials and law of total probability?,"The law of total probability is in general: $$P(A)=\sum_n P(A\mid B_n)P(B_n)$$ You can express a differential of a function in the following sum: $$\text{d}f = \sum_{i=1}^n \frac{df(x_1,x_2,...,x_i)}{dx_i} \,dx_i$$ These two expressions look quite similar, and is about summing terms that are in some sense 'disjoint'. Like a decomposition of of an 'object' (I don't know how I would call it) in its disjoint terms. It's almost like $x_i$ is like a partition similar to $B_n$ . Is there a direct relation? Or is there actual no similarity at all? I hope there is some insight to be gained.","['derivatives', 'probability']"
4405444,Bounding spectral radius of special matrix (extension),"Let $A$ be an $n \times n$ matrix with all nonnegative entries and row sums strictly
less than one, let $V$ be an $n \times n$ nonnegative diagonal matrix satisfying $V \leq I$ (entrywise), let $B\equiv\left(I-AV\right)^{-1}$ and $B^{*}\equiv\left(I-A\right)^{-1}$ , let $X$ be a vector in the $n$ -dimensional simplex (i.e., $x_j \geq 0,\sum_j^n x_j=1$ ), let $D_1$ and $D_2$ be two strictly positive diagonal $n \times n$ matrices, and finally let $$\tilde{M}\equiv\left(\mathrm{diag}\left\{ B^{T}X\right\} \right)^{-1}B^{T}\left[V\mathrm{diag}\left\{ X\right\} +\left(I-V\right)\mathrm{diag}\left\{ B^{T}X\right\} \right]D_{1}BD_2\mathrm{diag}\left(\iota-A\iota\right).$$ I want to show that the spectral radius of $\tilde{M}$ is less than or equal to one, $\rho(\tilde{M})\leq 1$ , provided that the following condition holds $$\tag{1} D_{1} B^{*} D_{2}  \left(I-A\right) \iota\leq\iota.$$ It is useful to note the connection between this question and two questions that have already been solved. First, in the special case with $D_2 = I$ , the problem above simplifies to showing that $\rho(M)\leq 1,$ where $$M\equiv\left(\mathrm{diag}\left\{ B^{T}X\right\} \right)^{-1}B^{T}\left[V\mathrm{diag}\left\{ X\right\} +\left(I-V\right)\mathrm{diag}\left\{ B^{T}X\right\} \right] B\mathrm{diag}\left(\iota-A\iota\right).$$ This is question Bounding spectral radius of special matrix which has already been solved, see https://math.stackexchange.com/a/4402778/165163 . Second, in the special case in which $D_1 = \eta I$ and $D_2 = \gamma \mathrm{diag}(e_i)$ , with $\eta,\gamma$ nonnegative constants and $e_i$ the vector with zeros everywhere except for a 1 in position $i$ , then $$\rho(\tilde{M}) = \tilde{M}_{ii}=\frac{\sum_{l}\left(v_{l}x_{l}+\left(1-v_{l}\right)\sum_{k}b_{kl}x_{k}\right)b_{li}^{2}\eta\left(1-s_{i}\right)\gamma_{i}}{\sum_{r}b_{ri}x_{r}}.$$ Since both the numerator and numerator are linear in $x$ , it is enough to consider this expression at corners, $X=e_j$ . Thus, we need to show that $$\tilde{M}_{ii}^{(j)}=v_{j}b_{ji}\eta\left(1-s_{i}\right)\gamma_{i}+\frac{\sum_{l}\left(1-v_{l}\right)b_{jl}b_{li}^{2}\eta\left(1-s_{i}\right)\gamma_{i}}{b_{ji}}\leq1.$$ Given our assumptions on $D_{1}$ and $D_{2}$ , condition (1) collapses to $\eta b_{ji}^{*}\left(1-s_{i}\right)\gamma_{i}\leq$ for all $j$ . Since $b_{ii}^{*}\geq b_{ji}^{*},\forall j,$ then we would need to show that $$v_{j}b_{ji}^{2}+\sum_{l}\left(1-v_{l}\right)b_{jl}b_{li}^{2}\leq b_{ji} b_{ii}^{*},$$ as formulated in this question Inequality involving matrix inverse elements , which has already been solved (see here ). To some extent, the challenge now is to somehow combine the ideas in the solutions to these two special cases to solve the more general problem postulated here. Finally, as in the case with $D_2 = I$ discussed in Bounding spectral radius of special matrix , two simple cases are illustrative. First, if $V = I$ then condition (1) implies that $\tilde{M}\iota \leq \iota$ and so $\rho(\tilde{M}) \leq 1$ . Second, if $A$ is diagonal then $\tilde{M}$ would be diagonal and so we would just need to show that each diagonal element is lower than one. But each of diagonal element of $\tilde{M}$ would be of the form $$d_1d_2\left(v+\frac{1-v}{1-av}\right)\frac{1-a}{1-av},$$ while (1) implies that $d_1d_2\leq 1$ , so it is enough to show that $$\left(v+\frac{1-v}{1-av}\right)\frac{1-a}{1-av}\leq 1.$$ Simple algebra shows this to be true.","['matrices', 'spectral-radius', 'linear-algebra', 'upper-lower-bounds', 'inequality']"
