question_id,title,body,tags
4072833,Unitary Operators are Connected in a $C^*$-algebra,"Let $A$ be a unital $C^*$ - algebra. Let $U = \{ u \in A : u^*u=uu^*=1\}$ be the unitary group of $A$ . Let $U'= \{ e^{ia_1}e^{ia_2} \cdots e^{ia_n} : a_k = a^*_k \in A, \text{for } 1\leq k \leq n \}$ . Show that $U'$ is the connected component of the identity in $U$ . If $A$ is commutative, show that $U' = \{ e^{ia} : a = a ^* \in A\}$ . I thought of using the theorems below from Banach Algebra Techniques in Operator Theory by Douglas. Following the proof of 2.14, if $f = e^{ia} \in A$ . Then $f\in U$ . Consider $\phi: [0,1 ] \rightarrow e^(A)$ defined by $\phi(\lambda) = e^{i \lambda a}$ , I don't see why $f \in U'$ , and $e^A$ is contained in $U'$ ... I also showed that if $\| u-1\| <2 $ , then we have $-1 \ne spec(u) $ and we can write $u = e^{ia}$ for some self-adjoint $a \in A$ Any help or suggestions will be appreciated. Thank you!","['c-star-algebras', 'operator-theory', 'functional-analysis', 'connectedness']"
4072878,Why the order of elements of my group $\mathbb{Z}\left[\sqrt 7\right]$ under multiplication modulo prime $p$ often divide $p^2-1$,"Consider the group $G$ with elements of the form $a + b \sqrt 7$ ( $a, b \in \mathbb{Z}$ ) under multiplication mod $p$ where $p$ is prime. I noticed the order of $1 + \sqrt 7$ in my group is usually of the form $\frac{p^2 - 1}{d}$ where $d$ is some positive integer divisor. For example, operating under mod $p := 5$ : $5^2 - 1 = 24$ and $\mathrm{ord}\left(1 + \sqrt 7\right) = 12 = 24/2$ . I plotted the order of $1 + \sqrt 7$ under different values of $p$ and here are the results: You can easily see the patterns emerge (i.e. broken parabolas in the plot): $p^2 - 1$ $(p^2 - 1)/2$ etc. Why does this happen? Is there a reliable way to predict when this happens and even what the divisor would be? This could potentially provide an efficient way to calculate the order of my group element.","['number-theory', 'group-theory']"
4072881,Are Rational Power of e is transcendental?,It is well known to all of us that the rational powers of $e$ are irrational numbers. Many of the proofs proving this use a similar approach as proving $e$ irrational using Niven's Polynomials. Is it true that rational powers of $e$ are also transcendental numbers using proofs similar(as proved by Hermite) for proving $e$ transcendental? How to measure the irrationality measure of those rational powers of $e$ ?,"['irrational-numbers', 'irrationality-measure', 'analysis', 'transcendental-numbers', 'rational-numbers']"
4072893,Ordinary Power series,"How do I find an  exponential generating function for $$\{p(n)/n!\}_{1}^{\infty}$$ where $$ p \in \mathbb{Z}^{+}[x] $$ I see a method to find an opsgf for the same sequence by applying $x \frac{d}{dx}$ operator to $e^x$ but I am not able to figure out a good way for egf. Help would be appreciated.
Thank You.","['generating-functions', 'sequences-and-series']"
4072940,First passage time in Brownian motion,"Let $X_t,\, t\geqslant 0,$ be a Brownian motion and consider the stopping times $T_a := \inf \{t \mid X_t = a\}$ . Find the probability $\mathbb P\{T_{2}< T_{-1} < T_{3}\}$ , for instance. So we have two events $\{T_2 < T_{-1}\}$ and $\{T_{-1}< T_3\}$ . Separately, the probabilities are clear. My intuition says that we can multiply the probabilites for the initial problem, but I'm not really satisfied with this intuitive mumbo-jumbo. What if there are some funny cases, when it doesn't work.. Yet, I have no idea how to formally explain this.","['stochastic-processes', 'brownian-motion', 'probability-theory']"
4072951,Usual conditions - filtration,"$(\Omega, \mathcal{F}, P)$ - probability space. What is the definition of filtration whch satisfy usual conditions? I know that it must be right-continuous and $\mathcal{N}\subset \mathcal{F_0}$ , but what is the form of $\mathcal{N}$ ? Is it $\mathcal{N}=\{X\subset \Omega:X\subset Y\in\mathcal{F}, P(Y)=0\}$ or $\mathcal{N}=\{A\in \mathcal{F}:P(A)=0\}$ ?","['measure-theory', 'probability-theory', 'probability']"
4072968,Function to Check the Parity of an Integer,"Is there any function, f(x), that outputs whether a given integer is odd or even? For example, if x were 1237, the function would output 1; and if it were 80, it would output 2. I know this is quite easy to accomplish in programming but when it comes to paper and pen math, I am having difficulties.","['functions', 'parity']"
4073159,Showing that a divergence free vector field is curl of a vector potential,"(This must have been asked somewhere before but for some reason I could not find an answer) In physics one often meets the criterion that a vector field has zero divergence, e.g. $$\nabla\cdot\mathbf{v}= 0 \quad (1)$$ Based on this one often claim that the vector field can be written as the curl of another field, $$\mathbf{v}=\nabla\times\mathbf{A} \quad (2)$$ since $\nabla\cdot (\nabla \times \mathbf{f})= 0$ for any (smooth) $\mathbf{f}$ . But the implication here is only obvious from (2) -> (1) and not the other way around. I tried the Helmholtz decomposition theorem where the vector field can be decomposed into curl-free and divergence-free components $$\mathbf{v} = \nabla \phi + \nabla\times \mathbf{A}$$ Taking the divergence of this and combining with (1) gives $$\nabla\cdot\mathbf{v} = \nabla \cdot\nabla \phi = 0$$ But I can't see how this would imply $\nabla \phi = 0$ which I guess would thus imply (2). Am I missing something or completely on the wrong track?",['multivariable-calculus']
4073161,one substitution in Chern's intrinsic proof of Gauss-Bonnet-Chern theorem,"In Chern's proof for Gauss-Bonnet-Chern theorem, he claims that $$
\varepsilon_{i}u_{i_1}u_j\Omega_{ji_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}}=P_k+2(p-k-1)\Sigma_k
$$ where $$
P_k=\varepsilon_{i}u_{i_1}^2\Omega_{i_1i_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}}
$$ and $$
\Sigma_k=\varepsilon_{i}u_{i_1}u_{i_3}\Omega_{i_3i_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}}
$$ By direct computations: $j=i_1$ : we get $P_k$ . $j=i_3,\dots,i_{2p-2k}$ : we get $\Sigma_k$ . My question is: why $$
\sum_{j=i_{2p-2k+1}}^{2p}\varepsilon_{i}u_{i_1}u_j\Omega_{ji_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}}=0?
$$ It seems that $$
\varepsilon_{i}u_{i_1}u_{j}\Omega_{ji_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}}\\=\varepsilon_{i}u_{i_1}u_{i_{2p-2k+1}}\Omega_{i_{2p-2k+1}i_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}}
$$ for all $j\in\{2p-2k+2,\dots,2p\}$ , then why $$
\varepsilon_{i}u_{i_1}u_{i_{2p-2k+1}}\Omega_{i_{2p-2k+1}i_2}\theta_{i_3}\cdots\theta_{i_{2p-2k}}\Omega_{i_{2p-2k+1}i_{2p-2k+2}}\cdots\Omega_{i_{2p-1}i_{2p}}=0?
$$ Any help would be appreciated. The link for Chern's original proof is: https://www.maths.ed.ac.uk/~v1ranick/papers/chern7.pdf","['curvature', 'riemannian-geometry', 'differential-geometry']"
4073195,Probability of Type 1 Error when using $X_{min}$ as a test static,"Let $X_1,X_2, ..., X_{15}$ be a random sample from the exponential distribution with $\lambda > 0 $ . To test $H_0 : \lambda = 1/5$ versus $H_A : \lambda < 1/5$ use $X_{min}$ as a test statistic. If $X_{min} \geq 1$ reject the null hypothesis. My problem is then to compute the probability of a type 1 error. I know that I have to calculate $$\begin{align*}
  P(\text{Type 1 Error}) & = P(\text{Reject} \ H_0 \ | \ H_0 \ \text{True}) \\
  & = P(X_{min} \geq 1 \ | \ \lambda = 1/5) \\
  & = 
\end{align*}
$$ However, I am not sure how to proceed now. As a hint in my book, I have to look an exercise where the PDF for $x_{min}$ is found. Do I have to find the pdf for $X_{min}$ now? I know that the PDF for $x_{min}$ is $$
f_{min}(x) = n(1 - F(x))^{n-1}f(x)
$$ Do I have to use this? Furthermore, is there any way to calculate this probability with in r studio? All help is appreciated. TIA.","['statistics', 'probability']"
4073226,"If $f : X \to Y$ is a homeomorphism and $\rho(x,y)= d'(f(x),f(y))$ defines a metric in $X$. Show that $d$ and $\rho$ are equivalent.","Let $(X,d)$ and $(Y,d')$ be metric spaces and $f : X \to Y$ a homeomorphism. Show that $\rho(x,y)= d'(f(x),f(y))$ defines a metric $\rho$ in $X$ and that $d$ and $\rho$ are equivalent. To show that $\rho(x,y)$ is a metric I have $$\rho(x,y)=d'(f(x),f(y)) \leqslant d'(f(x),f(y)) + d'(f(y),f(z)) = \rho(x,y)+\rho(y,z) \\ \rho(x,y)=d'(f(x),f(y))=d'(f(y),f(x)) = \rho(y,x) \\  \rho(x,y)=0 \iff d'(f(x),f(y))=0 \implies f(x)=f(y) \underset{f \text{ homeo}}\iff x=y$$ thus $\rho$ is a metric in $X$ . To show that $\rho$ and $d$ are equivalent I'm  a bit lost. I have three metrics here and I'm not sure where I should even use $d$ since $\rho$ depends on $d'$ not $d$ . In essence I would need to show that $\operatorname{id} :(X,d) \to (Y,d') \to  (X,\rho)$ is
homeomorphic? The fact that $f$ is homeomorphic implies that there exists and continuous inverse $f^{-1}:(Y,d') \to (X,d)$ , but this also says nothing about $(X,\rho)$ . $\textbf{If}$ I would have that $f^{-1}:(Y,d') \to (X,\rho)$ , then picking $y_1,y_2 \in Y$ I would have that $$\rho(f^{-1}(y_1), f^{-1}(y_2))=d'(f(f^{-1}(y_1)),f(f^{-1}(y_2)) = d'(y_1,y_2)$$ which would imply that $f^{-1}$ is a bijective isometry from $(Y,d') \to(X,\rho)$ and thus homeomorphism. Furhtermore I would have that $f^{-1} \circ f :(X,d) \to(X,\rho)$ would be a homeo since it's a composition of two homeos. So $\operatorname{id} :(X,d) \to  (X,\rho)$ is homeomorphic and $d$ and $\rho$ are equivalent. I'm very confused about the fact that I have three metrics. Any clarification would be welcome.","['isometry', 'general-topology', 'solution-verification', 'metric-spaces']"
4073315,Confusion about definition of faithful group action.,"Here are some excerpts from my lecture note: An action from a group $G$ to a set $X$ is a homomorphism $\alpha: G \to \text{Sym}(X)$ , where $\text{Sym}(X)$ is the group of bijections $X \to X$ . Equivalently, an action $\alpha$ of $G$ on $X$ can be viewed as a map $G \times X \to X$ , which we usually write as $(g,x) \mapsto g \cdot x := \alpha_g(x) \in X$ . An action $\alpha$ is called faithful if it is injective. The action of left translation of a group $G$ on itself is defined as $L_g(h) = gh$ , for some fixed $g \in G$ . This action is faithful. Am I correct to assume that the statement that $L_g$ is faithful is vacuously true, since we are restricting the domain of the action to just one element $g$ ? Or am I missing something?","['group-actions', 'group-theory', 'definition']"
4073321,Showing Lebesgue integral over set of measure zero is in fact zero zero.,"Question: Let $E\subset\mathbb{R}$ be a Lebesgue measurable set for which $\operatorname{m}(E)=0$ , where $\operatorname{m}$ denotes the Lebesgue measure. I'm trying to show that this implies $$\int_{E}e^{-x^2/2}dx=0.\tag{1}$$ (Attempted) Solution: Let $f:=e^{-x^2/2}$ . Since $0 < f \leq 1$ over $\mathbb{R}$ , and thus over any $E\subset\mathbb{R}$ , it follows $$0\leq\int_{E}e^{-x^2/2}dx\leq\int_{E}1dx = 1\cdot\operatorname{m}(E) =0.$$ That is, $$0\leq\int_{E}e^{-x^2/2}dx\leq0.$$ Therefore, indeed we have that $(1)$ holds. My question is, is this solution correct? If not, can it be repaired ? And if not, any help would be appreciated!","['measure-theory', 'real-analysis']"
4073417,Symbol for very small variable,"Is there any representation to state that a variable is close to (not equal) zero? Let me give you an example. Consider the function $u(x)=\alpha (e^{i\omega \delta t}-1) f(x)$ I am interested in the function $u(x)$ when $\delta t$ is very small. For this case, it should be easy to see that $u(x)|_{\text{small }\delta t} \approx i \alpha \omega \delta t f(x)$ Is there any ""nice"" notation to represent such an equation (without having to write small)? I thought that I could use the limit notation for that [for example, $\lim_{\delta t \to 0} u(t)$ ], but then I realized that if $\delta t$ goes to zero, then $u(x)=0$ . Therefore, it is not what I need. I found this link in the same forum, but it did not help.","['notation', 'limits', 'functions']"
4073437,Japanese theorem for cyclic quadrilaterals Proof Inversion,"Two weeks ago our professor taught us the Japanese theorem for cyclic quadrilaterals . It states that the inscribed centres of the four triangles formed by two sides and a diagonal of a cyclic quadrilateral always form a rectangle. After the proof he mentioned that he was convinced that the reverse also holds, that is, that whenever the incircle centres form a rectangle the outer quadrilateral is a cyclic quadrilateral, but he never managed to prove it. After finding absolutely nothing on the internet about the inversion of the Japanese theorem, I set out to prove it myself. The big problem is that the construction of the rectangle is not unique, many different chordal quadrilaterals have the same rectangle of incircle centres. Since there is thus no construction rule in the opposite direction, all my elementary geometric attempts have failed. So I came up with the idea of a purely arithmetic proof for the Japanese theorem. In the hope that, unlike the geometric proof, it works in both directions. Without limiting generality, I have given the four vertices of the chord quadrilateral the following coordinates: $A (0, 0), B (1, 0), C(x_c, y_c), D(x_d, y_d)$ . This is possible because by moving and scaling in the coordinate system alone, any quadrilateral can be brought into this shape. The simple values for A and B simplify the calculation enormously. After much effort I have found formulas to represent all vectors between the incircle centres only dependent from $x_c, y_c, x_d, y_d$ : $$\overrightarrow{M_1M_2} =    \left(\begin{array}{c}      \frac{\sqrt{x_c^2 + y_c^2} + x_c}{\sqrt{1 + x_c^2 - 2x_c + y_c^2} + \sqrt{x_c^2 + y_c^2} + 1} - \frac{\sqrt{x_d^2 + y_d^2}+ x_d}{1 + \sqrt{1 + x_d^2 - 2x_d + y_d^2} + \sqrt{x_d^2 + y_d^2}}\\      \frac{y_c}{\sqrt{1 + x_c^2 - 2x_c + y_c^2} + \sqrt{x_c^2 + y_c^2} + 1} - \frac{y_d}{1 + \sqrt{1 + x_d^2 - 2x_d + y_d^2} + \sqrt{x_d^2 + y_d^2}}    \end{array}\right)$$ $$\overrightarrow{M_2M_3} =     \left(\begin{array}{c}      \frac{\sqrt{x_c^2 + y_c^2 + x_d^2 + y_d^2 - 2x_cx_d - 2y_cy_d} + \sqrt{1 + x_d^2 - 2x_d + y_d^2} \cdot x_c + \sqrt{1 + x_c^2 - 2x_c + y_c^2} \cdot x_d}{\sqrt{x_c^2 + y_c^2 + x_d^2 + y_d^2 - 2x_cx_d - 2y_cy_d} + \sqrt{1 + x_d^2 - 2x_d + y_d^2} + \sqrt{1 + x_c^2 - 2x_c + y_c^2}} - \frac{\sqrt{x_c^2 + y_c^2} + x_c}{\sqrt{1 + x_c^2 - 2x_c + y_c^2} + \sqrt{x_c^2 + y_c^2} + 1}\\      \frac{\sqrt{1 + x_d^2 - 2x_d + y_d^2} \cdot y_c + \sqrt{1 + x_c^2 - 2x_c + y_c^2} \cdot y_d}{\sqrt{x_c^2 + y_c^2 + x_d^2 + y_d^2 - 2x_cx_d - 2y_cy_d} + \sqrt{1 + x_d^2 - 2x_d + y_d^2} + \sqrt{1 + x_c^2 - 2x_c + y_c^2}} - \frac{y_c}{\sqrt{1 + x_c^2 - 2x_c + y_c^2} + \sqrt{x_c^2 + y_c^2} + 1}    \end{array}\right)$$ $$\overrightarrow{M_3M_4} =    \left(\begin{array}{c}       \frac{\sqrt{x_d^2 + y_d^2} \cdot x_c + \sqrt{x_c^2 + y_c^2} \cdot x_d}{\sqrt{x_c^2 + y_c^2 + x_d^2 + y_d^2 - 2x_cx_d - 2y_cy_d} + \sqrt{x_d^2 + y_d^2} + \sqrt{x_c^2 + y_c^2}} - \frac{\sqrt{x_c^2 + y_c^2 + x_d^2 + y_d^2 - 2x_cx_d - 2y_cy_d} + \sqrt{1 + x_d^2 - 2x_d + y_d^2} \cdot x_c + \sqrt{1 + x_c^2 - 2x_c + y_c^2} \cdot x_d}{\sqrt{x_c^2 + y_c^2 + x_d^2 + y_d^2 - 2x_cx_d - 2y_cy_d} + \sqrt{1 + x_d^2 - 2x_d + y_d^2} + \sqrt{1 + x_c^2 - 2x_c + y_c^2}}\\       \frac{\sqrt{x_d^2 + y_d^2} \cdot y_c + \sqrt{x_c^2 + y_c^2} \cdot y_d}{\sqrt{x_c^2 + y_c^2 + x_d^2 + y_d^2 - 2x_cx_d - 2y_cy_d} + \sqrt{x_d^2 + y_d^2} + \sqrt{x_c^2 + y_c^2}} - \frac{\sqrt{1 + x_d^2 - 2x_d + y_d^2} \cdot y_c + \sqrt{1 + x_c^2 - 2x_c + y_c^2} \cdot y_d}{\sqrt{x_c^2 + y_c^2 + x_d^2 + y_d^2 - 2x_cx_d - 2y_cy_d} + \sqrt{1 + x_d^2 - 2x_d + y_d^2} + \sqrt{1 + x_c^2 - 2x_c + y_c^2}}     \end{array}\right)$$ $$\overrightarrow{M_4M_1} =     \left(\begin{array}{c}       \frac{\sqrt{x_d^2 + y_d^2}+ x_d}{1 + \sqrt{1 + x_d^2 - 2x_d + y_d^2} + \sqrt{x_d^2 + y_d^2}} - \frac{\sqrt{x_d^2 + y_d^2} \cdot x_c + \sqrt{x_c^2 + y_c^2} \cdot x_d}{\sqrt{x_c^2 + y_c^2 + x_d^2 + y_d^2 - 2x_cx_d - 2y_cy_d} + \sqrt{x_d^2 + y_d^2} + \sqrt{x_c^2 + y_c^2}}\\        \frac{y_d}{1 + \sqrt{1 + x_d^2 - 2x_d + y_d^2} + \sqrt{x_d^2 + y_d^2}} - \frac{\sqrt{x_d^2 + y_d^2} \cdot y_c + \sqrt{x_c^2 + y_c^2} \cdot y_d}{\sqrt{x_c^2 + y_c^2 + x_d^2 + y_d^2 - 2x_cx_d - 2y_cy_d} + \sqrt{x_d^2 + y_d^2} + \sqrt{x_c^2 + y_c^2}}    \end{array}\right)$$ As I feared, the formulae that result from the geometric construction are so complicated that I could not work with them even with the help of Mathematica and Matlab. So I discarded the arithmetic approach. After many more failed attempts, I wanted to rule out the possibility that a counterexample exists and that the reverse of the Japanese theorem is not true in the first place. I have written a programme in C++ which tries out millions and millions of constellations and indeed all points where the centres of the incircles form a rectangle always lie on a circle. But I've run out of ideas how I could still attempt the proof. And apparently no one has ever cared about the validity of the reversion, I've only found one old StackExchange post . But it is very sparse and the counterexample presented in the only answer is invalid because the vertices of the rectangle do not coincide with the inscribed centres.","['quadrilateral', 'euclidean-geometry', 'geometry']"
4073445,"Calculate $\int_{-\infty}^{\infty}\frac{\cos^2(x)}{x^2}e^{-ikx}dx$ for $k\in[0,2]$","After some alegbra, we find that the integral equals: $$\frac{1}{4}\int_{-\infty}^{\infty}\frac{2e^{-ikx}+e^{i(2-k)x}+e^{-i(2+k)x}}{x^2}dx$$ . Now since $k\in[0,2]$ , for contour integration, we consider the LHP for the first and third term in the integrand and then the UHP for the second term. If we call $C_{\epsilon,U}$ the semi-circle of radius $\epsilon$ in the UHP, $C_{\epsilon,L}$ the semi-circle of radius $\epsilon$ in the LHP, $C_{R,U}$ the semi-circle of radius $R$ in the UHP, $C_{R,L}$ the semi-circle of radius $R$ in the UHP, with closed contours $\Gamma_{U}$ and $\Gamma_{L}$ , we have: $$\int_{-\infty}^{\infty}=\lim_{R\to\infty}\lim_{\epsilon\to 0}\Bigg(\int_{\Gamma_U}-\int_{C_{\epsilon,U}} -\int_{C_{R,U}}\Bigg) \text{ for the second term}$$ $$\int_{-\infty}^{\infty}=\lim_{R\to\infty}\lim_{\epsilon\to 0}\Bigg(-\int_{\Gamma_L}-\int_{C_{\epsilon,L}} -\int_{C_{R,L}}\Bigg) \text{ for the remaining terms}$$ (Note the negative sign in line 2 due to negative orientation of $\Gamma_L$ ) No poles lie inside the closed $\Gamma_U$ and $\Gamma_L$ so the first integral on the RHS of each line is zero, and the last integral on the RHS each line $\to 0$ as $R\to \infty$ . So, for the top line, we're left with: $$ \frac{1}{4}\int_{-\infty}^{\infty}\frac{e^{i(2-k)x}}{x^2}dx=-\frac{1}{4}\lim_{\epsilon\to 0}\int_{C_{\epsilon,U}}\frac{e^{i(2-k)z}}{z^2}dz=-\frac{1}{4}\lim_{\epsilon\to 0}\int_{\pi}^{0}\frac{e^{i(2-k)\epsilon e^{i\theta}}}{\epsilon^2e^{2i\theta}}i\epsilon e^{i\theta}d\theta$$ . Expanding $e^{i(2-k)\epsilon e^{i\theta}}=1+i(2-k)\epsilon e^{i\theta}+...$ (can ignore higher order terms as they tend to $0$ as $\epsilon \to 0$ ). So we get (for the second term in the original integrand): $$\frac{1}{4}\lim_{\epsilon\to 0}\int_{0}^{\pi}\frac{1+i(2-k)\epsilon e^{i\theta}}{\epsilon^2e^{2i\theta}}i\epsilon e^{i\theta}d\theta=\frac{1}{4}\lim_{\epsilon\to 0}\int_{0}^{\pi}\frac{ie^{-i\theta}}{\epsilon}d\theta-\frac{1}{4}\int_{0}^{\pi}(2-k)d\theta=\frac{1}{4}\lim_{\epsilon\to 0}\int_{0}^{\pi}\frac{ie^{-i\theta}}{\epsilon}d\theta-\frac{1}{4}(2-k)\pi$$ But surely this is divergent? Have I gone wrong somewhere with this? When I consider the remaining terms (the 1st and 3rd terms in the original integrand, using the LHP), using the same method I get: $$=-\frac{3}{4}\lim_{\epsilon\to 0}\int_{-\pi}^{0}\frac{ie^{-i\theta}}{\epsilon}d\theta-\frac{1}{4}(2k+2)\pi$$ which again is divergent. So summing these two results would mean that the original integral in my question is divergent (assuming my method is correct - which I don't think it is)?","['integration', 'definite-integrals', 'improper-integrals', 'complex-analysis', 'contour-integration']"
4073453,Taylor series for $\sqrt[3]{x+1}$,"I'm trying to find the Taylor series for $\sqrt[3]{x}$ , but since the n-th derivative of the function $$f(x)=\sqrt[3]{x}$$ not definded at $x=0$ , I've swithched to $f(x)=\sqrt[3]{x+1}$ , This is my Attempt: $$f(x)=\sum_{n=0}^\infty\frac{f^{(n)}(0)x^n}{n!}$$ $$\sqrt[3]{x+1}=1+\frac{x}{3}-\frac{2x^2}{9\cdot2!}+\frac{10x^3}{27\cdot 3!}-\frac{80x^4}{81\cdot4!}+...$$ but when i plug $x=125 \iff \sqrt[3]{x+1}=\sqrt[3]{126}$ i get something way bigger than the acttual value which is about $5.01...$ , you can check it here . But I can't find the mistake in the equation.","['calculus', 'taylor-expansion']"
4073460,Reference algorithm/formula for the distribution of the median of random variables?,"The distribution of the mean of two random variables can be calculated using a convolution. I have a collection of $n$ independent random variables each with PDFs that are simple functions on $[0,1]$ . I would like to know the exact distribution of the median of these variables. I understand there is a central limit theorem for the distribution of the sample median for i.i.d variables, but I don't have that assumption here. I also see that there's a way to get a formula for discrete random variables. Is there a reference for continuous random variables?","['statistics', 'median', 'reference-request', 'order-statistics', 'random-variables']"
4073471,Technical Question: smoothness of $C^p$ $(2\le p<\infty)$ charts in Frobenius Theorem in Lang's Fundamentals of Differential Geometry (1999),"This is a technical question: Lang's Fundamentals of Differential Geometry (1999) has a proof of Frobenius' Theorem for $C^p$ ( $2\le p<\infty$ ) on (possibly, infinite dimensional Banach) manifolds. To do this, he produces a chart, $\phi:U_0\times V_0 \to U\times V$ on pg. 159, where $\phi(x,y)=(x,\alpha(x,y)),$ and $\alpha(x,y)$ is essentially a component of a flow of a time-dependent vector field, which on a $C^p$ manifold is only $C^{p-1}$ . Thus, a priori, $\phi$ is only $C^{p-1}$ and so could not be a chart for the $C^p$ manifold $X$ . Can one actually show $\phi$ is $C^p$ ? A similar issue arises in the ""straightening out"" proposition, 2.13 pg. 95, where the candidate chart is again obtained from a flow. Marsden, Ratiu, Abraham's Manifolds, Tensor Analysis and Applications has similar proofs of the two theorems and also do not address the degree of smoothness of the charts produced. Lang alludes to this loss of differentiability on pg. 158, about the morphism $f$ and again in the global Frobenius theorem, pg. 164.) Again $p\ne\infty$ -- other proofs in other books work exclusively with $C^\infty$ (and finite dimensional) manifolds. In more details: On pg. 158, in the course of proving that an involutive distribution, $E$ (i.e. a tangent subbundle of $TX$ ) on a $C^p$ ( $p\ge 2$ ) Banach manifold $X$ is integrable, he reduces the problem to one of local charts: $$
0\to U\times V\times\mathbb{E}\overset{\bar f}\to U\times V\times\mathbb{E}\times\mathbb{F}
$$ where $\mathbb{E}$ and $\mathbb{F}$ are the model (Banach) spaces of the fibers of $TX$ , and $\bar f(x,y,\mathbb{e})= (x,y,\mathbb{e},f(x,y)\mathbb{e})$ , and $f:U\times V\to L(\mathbb{E},\mathbb{F})$ is $C^{p-1}$ . On pg. 159, he further reduces the problem to solving the PDE (or essentially the ""classical Frobenius theorem, as mentioned by Warner in Foundations of Diff Manifolds....) $$
D_1\alpha(x,y)=f(x,\alpha(x,y))
$$ He solves this using proposition 2.1 on pg. 160, taking $\alpha(x,y)=\beta(1,x,y)$ . Here $\beta$ is the solution of $$
D_1\beta(t,x,y) = f(tx,\beta(tx,y))\cdot x
$$ He then takes a chart for manifold to be $\phi(x,y)=(x,\alpha(x,y))$ , where $\phi:U_0\times V_0\to U\times V$ . From proposition 2.1 it seems that $\beta$ is a priori only $C^{p-1}$ , and therefore likewise, $\alpha$ , and $\phi$ . Thus the supposed ""submanifold"" of $X$ given by $U_0\times y_0$ is only $C^{p-1}$ .","['partial-differential-equations', 'global-analysis', 'differential-geometry']"
4073502,Properties of integers squareness function,"Consider the map $$
\begin{array}{rccc}
s: &\mathbb{N}_{\geq1} & \longrightarrow & \left(0,1\right]\cap\mathbb{Q}\\
   & n & \longmapsto & \frac{\max\{d|n:\ d\ \leq\ \sqrt{n}\}}{\min\{d|n:\ d\ \geq\ \sqrt{n}\}}
\end{array}
$$ which gives the ""squareness"" of $n$ . The two following properties should be clear: $n$ is a square number $\Leftrightarrow$ $s(n)=1$ $n$ is a prime number $\Leftrightarrow$ $s(n)=\frac{1}{n}$ One nice pattern to display the integer numbers so that the end of each row is a square number is the following: $$
\begin{array}{cccccc}
1 \\
2 & 3 & 4 \\
5 & 6 & 7 & 8 & 9 \\
10 & 11 & 12 & 13 & 14 & 15 & 16 \\
\cdots
\end{array}
$$ If we write the values of $s$ applied to this pattern we get: $$
\begin{array}{cccccc}
1 \\
\color{red}{1/2} & \color{blue}{1/3} & 1 \\
1/5 & \color{red}{2/3} & 1/7 & \color{blue}{1/2} & 1 \\
2/5 & 1/11 & \color{red}{3/4} & 1/13 & 2/7 & \color{blue}{3/5} & 1 \\
\cdots
\end{array}
$$ The diagonal highlighted in red (formed by taking the $(k-1)$ -th value in each row) is given by the relation $s(n)=\frac{k-1}{k}$ , i.e. $n=k(k-1)$ . These are the oblong numbers . We can see that after the 1 corresponding to perfect squares, on each row the oblong number attains the maximum value of $s$ (in the end ""oblong"" can be understood as ""quasi-square""). The diagonal highlighted in blue (formed by taking the number before the last in each row) is given by the relation $s(n)=\frac{k-1}{k+1}$ , i.e. $n=(k-1)(k+1)=k^2-1$ , as we already knew. These are the 3rd-most largest values for $s$ in each row. (very open-ended) QUESTION: What other nice properties, derived from the pattern or not, does the squareness function $s$ have?","['integers', 'sequences-and-series']"
4073559,Threshold probability of the 3-uniform hypertriangle,"I am stuck on a random hypergraph problem (I am encountering random hypergraphs for the very first time). Let $G_{3}(n, p)$ be a binomial 3-uniform hypergraph. Find a threshold probability for containing a 3-uniform hypertriangle. What I have tried: I saw a similar problem for the existence of triangles in $G(n, d/n)$ . I am trying the same technique here for the hypergraphs. So according to my calculations I am getting $E[x]=\binom{n}{6}p^{3}$ , where $x$ is the number of hypergraphs. Then for the variance part, I get here 5 cases: when the number of common edges are : (i) up to 2, in which case its contribution is $E^{2}[x]$ , (ii) 3, the contribution is $\binom{n}{9}p^{6}$ , (iii) 4, the contribution is $\binom{n}{8}p^{5}$ , (iv) 5, the contribution is $\binom{n}{7}p^{4}$ and (v) 6, the contribution is $\binom{n}{6}p^{3}$ . After this, I do not know how to proceed and somehow, I feel that all this is completely wrong and something entirely different needs to be done for hypergraphs. Any suggestions would be helpful. Thanks in advance. $\textbf{Edit}$ : A 3-uniform hypergraph is a pair $(V, E)$ , where $V$ is a set of its vertices, and $E\subset\binom{V}{3}$ is a set of its edges. And a 3-uniform hypertriangle is a 3-uniform hypergraph with $|V | = 6$ with three edges such that every pair of edges share 1 vertex, and all three edges
do not have a common vertex. $G_{3}(n, p)$ is a binomial 3-uniform hypergraph such that $V = \{1, . . . , n\}$ and every hyperedge is chosen independently with probability $p$ . And I got my ideas from a similar calculation for graphs on page 6 of this link : https://www.cs.cmu.edu/~avrim/598/chap4only.pdf","['random-graphs', 'graph-theory', 'hypergraphs', 'probability-theory', 'probability']"
4073628,"Questions about SVD, Singular Value Decomposition","I am not a mathematician, so I need to understand what SVD does and WHY more than how it works exactly from the math perspective. (I understand at least what is the decomposition though). This guy on youtube gave the only human explanation of SVD saying, that the U matrix maps ""user to concept correlation"" Sigma matrix defines the strength of each concept, and V maps ""movie to concept correlation"" given that initial matrix M has users in the rows, and movie (ratings) in the columns. He also mentioned two concept specifically ""sci fi"" and ""romance"" movies. See the picture below. My questions are: How SVD knows the number of concepts. He as human mentioned two - sci fi, and romance, but in reality in resulting matrices are 3 concepts. (for example matrix U - that one with blue titles - has 3 columns not 2). How SVD knows what is the concept after all. I mean, what If i shuffle the columns randomly how SVD then knows what is sci fi, what is romance. I mean, I suppose there is no rule, group the concepts together in the column order. What if scifi movie is the first and last one? and not first 3 columns in the initial matrix M? What is the practical usage of either U, Sigma or V matrices? (Except that you can multiply them to get the initial matrix M) Is there also any other possible human explanation of SVD than the guy up provided, or it is the only one possible function? Matrices of correlations.","['matrices', 'linear-algebra', 'svd', 'matrix-decomposition']"
4073633,Comparing decay of integrable and non integrable function,"If we have two positive, decreasing functions $f,g:[a,\infty)\to\mathbb{R}$ such that $$\int_{a}^{\infty}f(x)\text{d}x<\infty\;\;\;\text{and}\;\;\;\int_{a}^{\infty}g(x)\text{d}x=\infty$$ can we infer that there exists a function $\widehat{f}$ with $\widehat{f}(x)=f(x)$ for almost all $x$ , such that we have $\lim_{x\to\infty}\frac{\widehat{f}(x)}{g(x)}=0$ ? Generally $\lim_{x\to\infty}\frac{f(x)}{g(x)}=0$ does not hold, a counterexample was provided here . But in this counterexample one can also choose a null set $A$ , so that for $\widehat{f}:= f\mathbb{1}_{A^C}$ we have $\lim_{x\to\infty}\frac{\widehat{f}(x)}{g(x)}=0$ .","['integration', 'limits', 'real-analysis']"
4073671,A binary strings combinatorics problem,"Suppose, there is a primary set with 2 binary strings: {0000,0011} . I want to know the number of elements in a set that has all the elements that are an HD of 2,4 away from each of the elements in the primary set. For 0011, there are $(4C_2+4C_3+4C_4)$ = 11 elements that are HD of 2,3,4 away. However, only 8 of the strings as shown below satisfy both these strings: 0000,0011 . {0101, 0110, 1001, 1010, 1100, 1110, 1101, 1111}","['permutations', 'combinations', 'binary', 'combinatorics', 'elementary-set-theory']"
4073674,Schouten theorem and doubts in the calculation of the tensor derivative,"I am working on the following theorem Theorem (J. A. Schouten) For $n\geq 4$ the metric $g$ is conformally flat if and only if $W=0$ . For $n=3$ , the metric $g$ is conformally flat if and only if the relation $$ (\nabla_X C) (Y, Z) = (\nabla_Y C) (X, Z)$$ holds for all $X, Y, Z$ . Here $C$ and $W$ denote the Schouten tensor and the Weyl tensor with $R=C\bullet g+W$ . I am following Theorem 8.31, pg. 352 in Differential Geometry: Curves - Surfaces - Manifolds by Wolfgang Kühnel. For the demonstration he uses the following derivation: $$d^\nabla A(X, Y, Z):= (\nabla_X A)(Y,Z) - (\nabla_Y A)(X, Z)$$ for an arbitrary symmetric (0,2)-tensor. So he concludes, for a scalar function $\varphi$ : \begin{eqnarray*}
d^\nabla \nabla^{2} \varphi (X, Y, Z) &=& \left<R(X, Y)\text{grad}\varphi, Z \right>\\
d^\nabla\left( \nabla\varphi\cdot\nabla \varphi \right)(X, Y, Z) &=& (Y\varphi)\nabla^{2}\varphi(X, Z) - (X\varphi)\nabla^{2}\varphi(Y, Z)\\
d^\nabla\left(\frac{1}{2}\vert\vert \text{grad}\varphi\vert\vert\cdot g\right)(X,Y,Z) &=& \nabla^{2}\varphi (X,\text{grad}\varphi)\left<Y,Z\right>-\nabla^{2}\varphi(Y,\text{grad}\varphi)\left<X,Z\right>
\end{eqnarray*} where $R$ is the curvature tensor. Similarly, for a one-form $\alpha=d\varphi$ he concludes: \begin{eqnarray*}
 d^\nabla\alpha(X, Y, Z) &=& - \alpha(R(X, Y)Z)\\
d^\nabla(\alpha\cdot\alpha)(X,Y,Z) &=& d\alpha(X,Y)\alpha(Z)+\alpha(Y)\nabla\alpha(X,Z)-\alpha(X)\nabla\alpha(Y,Z)\\
d^\nabla\left(\frac{1}{2}\vert\vert \alpha\vert\vert^{2}\cdot g\right)(X,Y,Z) &=& \left<\nabla_X\alpha, \alpha\right>\left<Y,Z\right> - \left<\nabla_Y\alpha, \alpha\right>\left<X,Z\right>
\end{eqnarray*} I am having difficulties in obtaining these six equalities above. I tried several times. For each of the identities above, the desired term appears but still contains some other terms that have not been canceled. I looked for other references that may present the proof of this theorem but I haven't found it. For the first equality, using the definition of $d^\nabla$ in $d^\nabla\nabla^{2}\varphi(X,Y,Z)$ , I got $$d^\nabla\nabla^{2}\varphi(X,Y,Z) = \left<R(X,Y)\text{grad}\varphi, Z\right>+\left<\nabla_{[X,Y]}\text{grad}\varphi, Z\right>+\left<\nabla_{Y}\text{grad}\varphi, \nabla_X Z\right>-\left<\nabla_{X}\text{grad}\varphi,\nabla_Y Z\right> $$ And for the second: $$ d^\nabla\left( \nabla\varphi\cdot\nabla \varphi \right)(X, Y, Z) = (Y\varphi)\nabla^{2}\varphi(X, Z) - (X\varphi)\nabla^{2}\varphi(Y, Z)+Z(\varphi)\left<\text{grad}\varphi, [X,Y] \right> +  Y(\varphi)\left<\text{grad}\varphi, \nabla_X Z\right>-X(\varphi)\left<\text{grad}\varphi, \nabla_Y Z \right>$$","['riemannian-geometry', 'tensors', 'curvature', 'tensor-decomposition', 'differential-geometry']"
4073696,Is there a smooth homotopy lifting theorem?,"Given a smooth map $p\colon E \to B$ of smooth manifolds with the continuous homotopy lifting property, does $p$ satisfy the smooth homotopy lifting property?","['fibration', 'homotopy-theory', 'differential-geometry']"
4073718,"If $c_1,\dots, c_h$ are the orders of the centralisers of elements of distinct conjugacy classes of a finite group, then $1/c_1+\dots+1/c_h=1$","This is part of Exercise 1.6.8 of Robinson's ""A Course in the Theory of Groups (Second Edition)"" . The Details: The centraliser of an element $x$ in a group $G$ is defined as $$C_G(x)=\{g\in G\mid xg=gx\}.$$ The conjugacy class of $x$ is defined as $$Cl(x)=\{gxg^{-1}\mid g\in G\}.$$ The previous part of the exercise in question is as follows. Lemma: Let $G$ be a finite group. The elements of the same conjugacy class have conjugate centralisers. Proof: Let $x,y\in Cl(a)$ for some $a\in G$ . Then there exist $g,h\in G$ such that $x=gag^{-1}$ and $y=hah^{-1}$ . Note that $gh=f$ for $f\in G$ , solved for $g$ , has the unique solution $g=fh^{-1}$ . We have $$\begin{align}
C_G(x)&=\{k\in G\mid kx=xk\}\\
&=\{k\in G\mid kgag^{-1}=gag^{-1}k\}\\
&=\{k\in G\mid (kg)a(kg)^{-1}=gag^{-1}\}\\
&=\{k\in G\mid (kfh^{-1})a(kfh^{-1})^{-1}=(fh^{-1})a(fh^{-1})^{-1}\}\\
&=\{k\in G\mid (f^{-1}kf)h^{-1}ah(f^{-1}kf)^{-1}=h^{-1}ah\}\\
&=\{b=f^{-1}kf\in G\mid byb^{-1}=y\}\\
&=f\{k\in G\mid (fkf^{-1})y=y(fkf^{-1})\}f^{-1}\\
&=f\{K\in G\mid Ky=yK\}f^{-1}\\
&=fC_G(y)f^{-1},
\end{align}$$ because $fkf^{-1}$ runs through $G$ just as an arbitrary $K\in G$ does. $\square$ (I'm sorry about the garbled proof. I'm trying to piece together my understanding without recourse to external sources.) The Question If $c_1,\dots, c_h$ are the orders of the centralisers of elements of distinct conjugacy classes of a finite group, then $1/c_1+\dots+1/c_h=1$ . Thoughts: This might require a combinatorial approach (in the numerical sense), since it involves counting, but of reciprocals; that's where I'm struggling: I can't use anything beyond the number of conjugacy classes being $$\frac{1}{|G|}\sum_{\pi\in G}|{\rm Fix}(\pi)|,$$ by Exercise 1.6.2 , where ${\rm Fix}(\pi)$ is the number of fixed points of the action of conjugacy on $\pi$ . (I'm a little unsure of this. Have I got it right?) I know that conjugation partitions the underlying set of $G$ . This is mentioned earlier in the book. Please help :)","['finite-groups', 'group-theory', 'group-actions', 'combinatorics']"
4073720,Calculate the Limit of Double Sum,"Compute \begin{equation}
L=\lim _{n \rightarrow \infty}\frac{1}{n} \sum_{a=1}^n \sum_{b=1}^n \frac{a}{a^2+b^2 }.
\end{equation} My attempt :
Define \begin{equation} f(n,m)= \frac{1}{n} \sum_{a=1}^n \frac{1}{m}\sum_{b=1}^m \frac{\frac{a}{n}}{(\frac{a}{n})^2+(\frac{b}{m})^2 } = \frac{1}{m}\sum_{b=1}^m \frac{1}{n} \sum_{a=1}^n \frac{\frac{a}{n}}{(\frac{a}{n})^2+(\frac{b}{m})^2 }
\end{equation} Now for any $\epsilon >0$ there exists some $B>0$ such that for any $n,m \in \mathbb{N}$ and $n,m\geq B$ : \begin{equation}
|f(n,m)-f(n,n)|<\epsilon
\end{equation} Thus, we have \begin{equation}
L=\lim _{n \rightarrow \infty} f(n,n)=\lim _{m \rightarrow \infty} \lim _{n \rightarrow \infty} f(n,m)= \lim _{m \rightarrow \infty} \frac{1}{m}\sum_{b=1}^m \int_{0}^{1}\frac{x}{x^2+(\frac{b}{m})^2}dx=\\ \frac{1}{2}\lim _{m \rightarrow \infty} \frac{1}{m}\sum_{b=1}^m \ln\frac{1+(\frac{b}{m})^2}{(\frac{b}{m})^2}=\frac{1}{2}\int_{0}^{1}\ln\frac{1+x^2}{x^2}dx=\frac{2\ln2 +\pi}{4}
\end{equation}","['limits', 'riemann-sum']"
4073759,Calculating the geometric realization of a non-representable functor,"Background Let $\mathcal{F} : \textbf{CRing} \to \textbf{Set}$ be a functor and denote by $\textbf{P}_\mathcal{F}$ the category of points of $\mathcal{F}$ whose objects are pairs $(R , \rho)$ where $R$ is a ring and $\rho : h^R \to \mathcal{F}$ is a morphism (here $h^R := \operatorname{Hom}_{\textbf{CRing}}(R , -) \in [\textbf{CRing} , \textbf{Set}]$ is the Yoneda embedding $h^-$ applied to $R$ ), and whose morphisms $(R , \rho) \to (S , \sigma)$ are ring homomorphisms $\varphi: R \to S$ such that $\rho \circ h^\varphi = \sigma$ . Define the diagram $D_\mathcal{F} : \textbf{P}_\mathcal{F}^\text{op} \to \textbf{LRS}$ to be the composition of the forgetful functor $\textbf{P}_\mathcal{F}^\text{op} \to \textbf{CRing}^\text{op}$ and the spec functor $\operatorname{Spec} : \textbf{CRing}^{\text{op}} \to \textbf{LRS}$ . Explicitly it sends $(R , \rho) \mapsto \operatorname{Spec}(R)$ . In Demazure & Gabriel's Introduction to Algebraic Geometry and Algebraic Groups , the geometric realization of the functor $\mathcal{F}$ is defined by $| \mathcal{F} | := \operatorname{colim} D_\mathcal{F}$ . In this book it is proved that the geometric realization functor $| - | : [\textbf{CRing} , \textbf{Set}] \to \textbf{LRS}$ is left adjoint to the functor $\textbf{LRS} \to [\textbf{CRing} , \textbf{Set}]$ which sends $X \mapsto \operatorname{Hom}_{\textbf{LRS}}(\operatorname{Spec}(-) , X)$ . In particular if $\mathcal{F}$ is representable by some scheme $Y$ , i.e. $\mathcal{F} \cong \operatorname{Hom}_{\textbf{LRS}}(\operatorname{Spec}(-) , Y)$ , then I think that $|\mathcal{F}| \cong Y$ . Question My question is how does one go about computing the geometric realization of a functor which is not representable by schemes? For example what is the geometric realization of the functor $R \mapsto R^{\oplus \mathbb{N}}$ ? (This is basically the only non-representable functor I know.) Small colimits in $\textbf{Set}$ are easy to construct (take the disjoint union and mod out by some equivalence relation), but the colimit in question here is certainly not small. Of course Demazure & Gabriel use Grothendieck universes throughout their book, so then $\textbf{P}_\mathcal{F}$ is a small category, however I'm not sure how to find a suitable Grothendieck universe to explicitly calculate this colimit for my example.","['representable-functor', 'category-theory', 'algebraic-geometry', 'yoneda-lemma', 'commutative-algebra']"
4073801,Specific Example for the Existence of Partial Derivatives,"Suppose $\sigma$ has bounded first and second derivatives and refer to the following picture.  Why do we know that $u$ even has a partial derivative with respect to $y$ ?  As far as I can understand, (2.37) is justified by fixing y and applying Picard Lindelof, so I'm confused (this is on p. 296 of Karatzas and Shreve, for reference)","['calculus', 'derivatives', 'ordinary-differential-equations', 'partial-differential-equations']"
4073834,checking that a group is $A_n$ or $S_n.$,"Suppose I have two (random) permutations of $[n].$ We know (Dixon, Babai) that they almost surely generate $A_n$ or $S_n$ (if both even, then $A_n,$ otherwise $S_n.$ ) But the question is: what is the best (theoretical or practical) algorithm to check that the generated subgroup really is one of those big groups?","['group-theory', 'computational-complexity', 'computer-science']"
4073922,Is there a theory that describes eigenspaces of matrix functions near degeneracies?,"Let $X$ be a smooth manifold and let $h$ be a smooth map from $X$ to hermitian $N \times N$ matrices. Under favorable circumstances (one sufficient condition: all eigenvalues of $h(x)$ are simple for every $x \in X$ ) this datum determines a number of vector bundles over $X$ whose fibers are eigenspaces of $h(x)$ . These bundles are all subbundles of a fixed trivial bundle with fiber $\mathbb C^N$ , whose direct sum is the whole trivial bundle. More generally one could consider the situation in which the above is true generically but fails on some locus $Y \subset X$ (e.g. the subset of $X$ on which $h(x)$ does not have simple spectrum). One nice example is $X= \mathbb R^3$ , $h(x) = \begin{bmatrix} x_3 & x_1 - \mathrm{i} x_2 \\ x_1 + \mathrm{i} x_2 & -x_3 \end{bmatrix}$ for which the ""bad locus"" is $\{ 0 \}$ . I would like to know if there is some general theory which describes the behaviour of eigenspaces at ""bad points"". I suspect that this problem should have interesting local and global aspects; I am interested in both.","['eigenvalues-eigenvectors', 'vector-bundles', 'algebraic-geometry', 'linear-algebra', 'algebraic-topology']"
4073931,Commutant built from the Unilateral Shift,"Let $T: \ell^2 \rightarrow \ell^2$ be the unilateral shift, and let $\mathcal{T}= \{ I, T, T^* \}$ . Show that $\mathcal{T}' = \{\lambda I_{\ell^2}  : \lambda \in \mathbb{C} \}$ and $\mathcal{T}'' = B(\ell^2)$ . Attempt: Since $T$ is the unilateral shift, we have that $Te_n = e_{n+1}$ . Also, by definition, we have $\mathcal{T}' = \{ S \in B(\ell^2) : ST =TS, \forall T \in \mathcal{T}\}$ . Let's look at the each element in $T$ : We know that the identity $I$ always has the property $IS=SI$ for $S \in B(\ell^2) $ . Also, let $S \in B(\ell^2)$ . Then we have $STe_n= Se_{n+1} =e_{n+1 }S= Te_n S = TSe_{n}$ . For $T^*$ , I know that $T^*e_n= e_{n-1}$ , for $n>1$ . So don't we get a similar result as above, i.e $ST^*e_n =T^* S e_n$ ? So I'm a bit confused on how to get the results we want. I feel like I got the definitions of $\mathcal{T}'$ and $\mathcal{T}''$ mixed up somewhere... Thank you for your help!","['von-neumann-algebras', 'operator-theory', 'functional-analysis']"
4073936,Pollution of 3 lakes with differential equations,"Consider three lakes of equal volumes connected to each other with a flow. An accident results in the spillage of 300 000 kg of chemicals in Lake 1. Calculate the quantity of this chemical present, after the accident, in each of the lakes, assuming that it remains in one of the three lakes.
Then the problem gives me this suggestion: Suggestion: construct the illustrated compartmental model in which $l_i(t)$ indicates the quantity of
pollutant in lake $i$ at time $t$ , $V$ is the volume of each of the lakes and $r$ is the flow rate between 2 lakes and deduce the equations \begin{align}
    \frac{dl_1(t)}{dt}&= -r\frac{l_1}{V}+r\frac{l_3}{V}\nonumber\\
    \frac{dl_2(t)}{dt}&=-r\frac{l_2}{V}+r\frac{l_1}{V}\nonumber\\
    \frac{dl_3(t)}{dt}&=-r\frac{l_3}{V}+r\frac{l_2}{V}\nonumber
\end{align} with the initial condition $l_1 (0) = 300000$ kg, $l_2 (0) = 0$ and $l_3 (0) = 0$ . Indicate how to bring these three equations to the system of two equations that need to be resolved: \begin{align}
    \frac{dl_1(t)}{dt}&= -r\frac{l_1}{V}+r\frac{300 000-l_1-l_2}{V}\nonumber\\
    \frac{dl_2(t)}{dt}&=-r\frac{l_2}{V}+r\frac{l_1}{V}\nonumber
\end{align} I was wondering how can I transform the 3 differential equations into the 2 differential equations above? Thank you!","['calculus', 'linear-algebra', 'ordinary-differential-equations']"
4074008,"Finding a set of $n-1$ languages, such that everyone speaks at least one language in the set","For any integer $n$ , the following fact can be proven to be true: Given $n^n+1$ people, where each person speaks a distinct set of $n$ languages, such that any two of these people speak at least one language in common, there exists a set $T$ of $n-1$ languages such that everyone speaks at least one language in $T$ . I know how to prove that fact, and included a proof at the end, but my question is, what is the smallest number you can replace $n^n+1$ by and have that fact still be true ? Letting $t_n$ be this smallest number, then $t_2=4$ , for example. The fact that $t_2>3$ follows from this counterexample with three people: $$
\{\{\text{English},\text{Español}\},\{\text{English},\text{Français}\},\{\text{Español},\text{Français}\}\}
$$ Furthermore, it is easy to show that for any four sets of two languages whose intersections are pairwise nonempty, there will exist a single language spoken by all. What I know so far is that $$
\binom{2n-1}{n}< t_n \le  n^n+1
$$ with the lower bound realized by the collection of $n$ -elements subsets of a $(2n-1)$ -element set. I wonder which one of these is asymptotically correct, or if the truth is somewhere between? The proof of $t_n\le n^n+1$ is far from obvious. You can prove the following by induction on $k$ , for each $k\in \{0,1,\dots,n\}$ : Let $S$ be a set of people who each speak $n$ languages, such that every pair of people in $S$ have a language in common, and there is no set $T$ of $n-1$ languages for which everyone in $S$ speaks at least one language in $T$ . Furthermore, let $R$ be a subset of $S$ such that the intersection of all language sets of the people in $R$ has size at least $n-k$ . Then $|R|\le n^k$ . In particular, $S$ itself satisfies this with $k=n$ , so $|S|\le n^n$ , implying any $n^n+1$ sets will have a set $T$ of $n-1$ languages which works, so $t_n\le n^n+1$ . Proof: The base case $k=0$ is obvious. For $k\ge 1$ , let $R$ be a subset for which the intersection $I$ of all language sets of all people in $R$ has size at least $n-k$ . We can assume $|I|\le n-1$ (else $|R|=1$ ), so there must be some person $p\in S$ who speaks no languages in $I$ . Let $p$ speak the languages $l_1,\dots,l_n$ . For each $i\in \{1,\dots,n\}$ , let $R_i$ be the set of people in $R$ who speak $l_i$ . Then the intersection of all the language sets for all people in $R_i$ has size at least $n-k+1$ , since it includes $I\cup \{l_i\}$ . By induction, we have $|R_i|\le n^{k-1}$ , so $$|R|\le |R_1|+\dots+|R_n|\le n\cdot n^{k-1}=n^k.$$","['combinatorics', 'extremal-combinatorics', 'ramsey-theory']"
4074038,Find all functions $f: \mathbb{R} \to \mathbb{R}$ such that $f(f(x+y)) = f(x)+f(y)$,"The problem is to find the set of all functions $f: \mathbb{R} \to \mathbb{R}$ such that for all $x,y \in \mathbb{R}$ we have $f(f(x+y)) = f(x)+f(y)$ . We first notice that $f(x)=x+c$ is a solution. With some work it turns out that it is enough to find solutions with $f(0)=0$ and from that to find all solutions, and moreover, it is not that hard to show that in this case of $f(0)=0$ the functional equation is equivilent to the two conditions: $$(1): f(x+y)=f(x)+f(y) \\ (2) :f(f(x))=f(x) $$ I was expecting that the only solutions will be $f(x) = x, f=0$ but with a bit more thought I think the set of solutions is bigger. I would like to know if I'm right. Here's what I thought: take a basis $\{e_\alpha\}_{\alpha \in I}$ of $\mathbb{R}$ as a vector space over $\mathbb{Q}$ . Consider a division of this basis to disctint pairs $(e_\alpha, e_\beta)$ [each element of the basis appears in one and exactly one of those pairs). Define $f$ on the basis elements as $f(e_\alpha) = e_\beta, f(e_\beta)=e_\beta$ . Thus we defined $f$ on the basis elements. Now, we simply continue $f$ linearly, to define it on all $\mathbb{R}$ : $f(\sum a_i e_i) = \sum a_i f(e_i)$ . This makes sure $(1)$ holds and $f$ is additive. By definition, it is trivial that $(2)$ also works if $x$ is a basis element,and therefore, because $f$ is additive, it also holds for any real number (just write it as a linear combination of the basis elements and do the algebra). Therefore I think this is a construction which shows that there are plenty of pathological solutions. Am I right? Is there a nice way to characterize all solutions?","['contest-math', 'functions', 'linear-algebra', 'vector-spaces']"
4074042,Von Neumann Algebra built from the bilateral shift,"Let $T: \ell^2(\mathbb{Z}) \rightarrow \ell^2(\mathbb{Z})$ be the bilateral shift, and let $S= \{ I, T, T^* \}$ . Why do we have $S'= S'' = \{ W\in B(\ell^2(\mathbb{Z})) : \langle e_n, We_k \rangle = \langle e_{n-k}, We_0 \rangle\} $ ? I know that if we think of it in terms of matrices then this is $w_{{n+j},{k+j}} = w_{n,k}$ . But I still don't really see what's going on here. I wonder if there is way to see this with out the use of matrices. Thank you in advance!","['von-neumann-algebras', 'functional-analysis', 'operator-algebras']"
4074052,Prove $\pi=\lim_{n\to\infty}2^{4n}\frac{\Gamma ^4(n+3/4)}{\Gamma ^2(2n+1)}$,"How could it be proved that $$\pi=\lim_{n\to\infty}2^{4n}\frac{\Gamma ^4(n+3/4)}{\Gamma ^2(2n+1)}?$$ What I tried Let $$L=\lim_{n\to\infty}2^{4n}\frac{\Gamma ^4(n+3/4)}{\Gamma ^2(2n+1)}.$$ Unwinding $\Gamma (n+3/4)$ into a product gives $$\Gamma \left(n+\frac{3}{4}\right)=\Gamma\left(\frac{3}{4}\right)\prod_{k=0}^{n-1}\left(k+\frac{3}{4}\right).$$ Then $$\lim_{n\to\infty}\frac{(2n)!}{4^n}\prod_{k=0}^{n-1}\frac{16}{(3+4k)^2}=\frac{\Gamma ^2(3/4)}{\sqrt{L}}.$$ Since $$\frac{(2n)!}{4^n}\prod_{k=0}^{n-1}\frac{16}{(3+4k)^2}=\prod_{k=1}^n \frac{4k(4k-2)}{(4k-1)^2}$$ for all $n\in\mathbb{N}$ , it follows that $$\prod_{k=1}^\infty \frac{4k(4k-2)}{(4k-1)^2}=\frac{\Gamma ^2(3/4)}{\sqrt{L}}.$$ But note that this actually gives an interesting Wallis-like product: $$\frac{2\cdot 4\cdot 6\cdot 8\cdot 10\cdot 12\cdots}{3\cdot 3\cdot 7\cdot 7\cdot 11\cdot 11\cdots}=\frac{\Gamma ^2(3/4)}{\sqrt{L}}.$$ I'm stuck at the Wallis-like product, though.","['gamma-function', 'limits', 'pi', 'real-analysis']"
4074079,Looking for alternative ways to solve $3^{2^x}=2^{3^x}$,"If $\log2=m$ and $ \log3=n$ , What is the answer of $\large3^{2^x}=\large2^{3^x}$ ? $1)\log
 mn\quad\quad\quad\quad\quad\quad2)\frac{m-n}{\log m-\log
 n}\quad\quad\quad\quad\quad\quad3)\frac{\log \frac
 mn}{m-n}\quad\quad\quad\quad\quad\quad4)\frac{m+n}{\log mn}$ Here is my approach: To solve the equation I took logarithm in base $3$ of both sides of the equation: $$2^x=\log_32^{3^x}=3^x\log_32=3^x\times\frac{m}{n}$$ So we have $(\frac23)^x=\frac mn$ . by taking logarithm in base $\frac23$ we have: $$x=\log_{\large\frac23}(\frac mn)=\frac{\log(\frac mn)}{\log(\frac23)}=\frac{\log \frac mn}{m-n}$$ My questin: Is there other approach to solve this problem?","['algebra-precalculus', 'logarithms']"
4074086,Intuition behind Group Action,"I am going over the group action and unfortunately I am not sure if I understand it very well. So for this reason, I have a few questions that possibly could help me to understand this concept better. A group action is defined as: Group $G$ acts on a set $X$ if there is a homomorphism $\sigma:G \rightarrow S_X$ . I am aware group action has other definitions but for me this one is easier to understand. Now $S_X$ is the group of permutations of $X$ . It is not crystal clear what is meant by this to me. Is $S_X$ the same thing as $S_{|X|}$ ? When we talk about $S_n$ , I have a clear mental picture; $n$ is an integer and I think of all the permutations with $n$ elements. But in $S_X$ , $X$ is not an integer but a set. So suppose $X=\{1,2,3\}$ . Is $S_X$ the same thing as $S_3$ in this case? The other part of my question is about what the definition implies: it says ""... if there is a homomorphism"". Is it possible that homomorphism does not exist? Is the homomorphism unique? Or is it possible to have more than one homomorphism? Once we specify a group and a set, how do we find such homomorphism(s)? Is there a requirement for the size of the group and the set to make this definition work, for instance $|G| \geq |X|$ ? Please let me know if you would like me to clarify my question. Any help with making a better intuition is appreciated!","['group-theory', 'group-actions', 'intuition']"
4074137,Hypothesis testing - When to subtract one during type I and type II testing?,"I'm currently studying statistics and I'm reviewing notes I took during a class last week. However, there's something I'm confused about.
My professor subtracted 1 from the ""successes"" and I can't seem to figure out why.
Essentially, the scenario was like this: Someone flipped a coin 16 times. $$H_0:p=0.5\text{ v.s. }H_a:p=0.55$$ Test 1: Reject $p = 0.50$ if 10 or more heads are observed out of 16. $\text{Pr}(X \ge  10 \text{ when } p= 0.5)$ where X is a binomial with $n = 16$ and $p = 0.50$ . This is the part I don't understand. When using R, he did the following: 1 - pbinom(9, 16, 0.5) Where did the 9 come from? Why did he subtract 1 from the initial 10 tosses? If I was in a scenario where I had: $$\text{Pr}(X \le 15 \text{ when } p = 0.50)$$ Would I still subtract 1? Would the R solution be pbinom(15, 16, 0.50) or pbinom(14, 16, 0.50) ? How do we decide when to subtract 1 like this? ( Sorry if my formatting is bad! I've never posted math equations like this to these forums and I can't seem to figure it out)","['statistics', 'hypothesis-testing']"
4074144,Evaluating $\lim_{n \to \infty}\prod_{r=1}^{n}[1+(r/n)^2]^{1/r}$ by expressing it as a definite integral,"I have been asked to find the limit of the following series by expressing it as definite integral: If $na=1$ always and $n$ tends to infinity, find the limiting value $$\lim_{n \to \infty}\left(\prod_{r=1}^{n}[1+(ra)^2]^{1/r}\right)$$ The answer given is $\exp\left(\dfrac{\pi^2}{24}\right)$ . I took the log of both sides and set up my integral as $$\int_{0}^{1}\log(1+x^2)dx$$ But my answer came out wrong. Thanks in advance","['limits', 'calculus', 'definite-integrals']"
4074145,Is the following a Bijective Function?,In a bijective function is it necessary that all the elements of the domain correspond to a value in the range? Like for example can the following be a bijective function - If not then what type of function is it?,['functions']
4074170,Is this intuitive by graph ? Or its needs rigorous proof,"A double differentiable function $f : \mathbb{R} \to \mathbb{R}$ satisfy $f(x)f^{\prime \prime}(x) \ne 0 \ \forall x \in \mathbb{R}$ . Can we conclude $f(x)f^{\prime \prime}(x) >0$ for all $\mathbb{R}$ , even though continuity of $f''(x)$ is not given. I thought of making a contradiction by graph supposing $f(x)>0$ and $f''(x) <0$ , I thought graph would need to cut x axis hence after that $f(x)<0$ ( contradiction) but I don't have a good calculus based proof on this fact. As continuity of $f''(x)$ not given is also a problem for all $\mathbb{R}$","['calculus', 'derivatives']"
4074196,Number of distinct associated classes of the ring $\mathbb{Z}_n[i]$.,"Let $x, y$ be two elements of a commutative ring $R$ .
Then we say $x$ and $y$ are associate, denoted $x ∼ y$ , if and only if $x = uy$ for a unit $u$ in $R$ .
Then the equivalence class, $[x] = \{z \in R| z ∼ x\}$ is the associate class of $x$ . My question is: How many different associated classes exist in the ring of Gaussian integers modulo $n$ ?, $\mathbb{Z}_n[i]$ . I expect an answer in terms of $n$ or using the prime factorization of $n$ in $\mathbb{Z}[i]$ or in $\mathbb{Z}$ . I have seen the post about $\mathbb{Z}_n$ . But, here unlike in $\mathbb{Z}_n$ , the number of associated classes is not equal to the number of principal ideals. $\mathbb{Z}_4[i]$ is an example for this, which has 9 principal ideals but has only 5 associate classes. Working out with examples for $n$ up to 12, I have noticed that,
If $n=2^ap_1^{k_1}p_2^{k_2}\cdots p_t^{k_t}q_1^{m_1}q_2^{m_2} \cdots q_s^{m_s}$ be the prime factorization of $n$ in $\mathbb{Z}$ such that $p_i\equiv 1 
\mod 4$ and $q_i \equiv  3 \mod 4$ .
Then the number of conjugate classes is equal to $(2a+1)(K_1+1)^2(k_2+1)^2 \cdots (k_t+1)^2(m_1+1)(m_2+1)\cdots (m_s+1)$ .
But I didn't found a way to prove this.","['gaussian-integers', 'equivalence-relations', 'ring-theory', 'abstract-algebra', 'combinatorics']"
4074213,Existence of derivative at a point.,"Assume that f : (a, b) → R is differentiable on (a, b) except possibly at c ∈ (a, b). Assume that lim $_{x→c}$ f '(x) exists. Prove that f '(c) exists and f ' is continuous at c. I am getting a counter example for this statement.
Consider the function f(x) defined on (-1,1) by f(x) = x , x ∈ (-1,0) U (0,1) 5 , x = 0 Here f '(x) is 1 for all x ∈ (-1,0) U (0,1).Thus lim $_{x→0}$ f '(x) exists. But we know f(x) is not continuous at 0. Thus , f'(0) doesn't exist. I don't know where I have made a wrong assumption in the example. Please help.","['limits', 'functions', 'derivatives', 'real-analysis']"
4074247,Show that $a$ and $b$ have no greatest common divisor in $\mathbb{Z}[\sqrt-5]$,"Show that in the ring $R=\mathbb{Z}[\sqrt{-5}]$ , $a=3\cdot 7 \cdot(1+2\sqrt{-5})$ and $b=(1+2\sqrt{-5})\cdot 7\cdot(1+2\sqrt{-5})$ have no greatest common divisor. Since $N(a)=3^3\cdot 7^3$ and $N(b)=3^2\cdot 7^4$ so it is clear that any common divisor of $a$ and $b$ must have norm dividing $3^2\cdot 7^3$ . Now we will get many possibilities. Am I on right track or there is some other smart way to solve this problem?","['ring-theory', 'abstract-algebra']"
4074255,Zero probability condition: Thinking about '$P(H)=\int_{b \in \mathbb R}P(H|B=b)f_B(b)$',"Let $B$ be a continuous random variable. Let $H$ be an event. Am I right to think it does not necessarily make sense to say ' $P(H)=\int_{b \in \mathbb R}P(H|B=b)f_B(b)$ '? My guess: Well based on Wiki , I think yes, i.e. we cannot do this for just any $H$ because $P(H|B=b)$ need not be defined. Furthermore, even if we somehow define $P(H|B=b)$ , I think we'll still have to think about defining the integral $\int_{b \in \mathbb R}P(H|B=b)f_B(b)$ , depending on the definition of $P(H|B=b)$ . Is ' $P(H|B=b)$ ' well-defined if $H=\{Y \in U\}$ for any continuous random variable $Y$ s.t. $Y$ and $B$ have a continuous joint pdf and for any interval $U$ ? (I forgot if any 2 continuous random variables necessarily have a well-defined joint pdf. Also, I'm trying not think of $U$ as an arbitrary Borel set.) With the same conditions as in (2) and assuming the answer to (2) is affirmative, does it necessarily make sense to say $P(H)=\int_{b \in \mathbb R}P(H|B=b)f_B(b)$ , and is such equation correct? My guess: I believe it makes sense and then is correct: Pretend $U=(1,7)$ . Then: $LHS = P(H)=\int_1^7 f_Y(y) dy$ . $RHS= \int_{b \in \mathbb R}P(Y \in (1,7)|B=b)f_B(b) db = \int_{b \in \mathbb R} \int_1^7 f_{Y|B=b}(y) f_B(b) dy db = \int_{b \in \mathbb R} \int_1^7 f_{Y,B}(y,b) dy db$ Then pretend I know what Fubini's theorem is to get $=  \int_1^7 \int_{b \in \mathbb R} f_{Y,B}(y,b)  db dy$ $=  \int_1^7 f_{Y}(y) dy$ Re Fubini's theorem, is there a way to argue this at an elementary probability level?","['conditional-probability', 'calculus', 'probability-distributions', 'probability']"
4074265,Derivative involving inverse matrix,"Let $\alpha_i\in\mathbb{R}$ , $x_i\in\mathbb{R}^d$ for all $i\in[k]$ , with $k \geq d$ . I am looking for this derivative $$
\frac{\partial}{\partial\alpha_i} \left(\sum_{i=1}^k\alpha_ix_ix_i^T\right)^{-1} = \frac{\partial}{\partial\alpha_i} \left(X\Lambda X^\top\right)^{-1},
$$ where we define $X: \text{col(X)}= \{x_i\}_{i\in[k]}$ , $\Lambda = \text{diag}(\alpha)$ , and we assume $\left(X\Lambda X^\top\right)$ is invertible.","['matrices', 'matrix-calculus', 'inverse', 'partial-derivative', 'derivatives']"
4074318,"Let $f_n\to f$ pointwise on a set $E$. If $|f_n(x)-f(x)|\le a_n \ \forall x\in E$ with $a_n\to 0$, then the convergence is uniform","Let $f_n$ and $f$ be a real valued functions defined on a set $E$ . If $f_n\to f$ pointwise on $E$ and there is a real-valued sequence $a_n$ such that $a_n\to 0$ and $|f_n(x)-f(x)|\le a_n \ \forall x \in E$ . Show that $f_n\to f$ uniformly. I would like to know if my proof holds, please. I also have a question on the statement in the end. Thank you in advance for help! First, as $f_n\to f$ pointwise, then we have $\forall x \in E:\forall \epsilon>0 \ \exists N \ \forall n>N:$ $|f_n(x)-f(x)|<\epsilon$ . Then, as $a_n\to 0$ as $n\to\infty$ , we have: $\forall \epsilon>0 \ \exists N' \ \forall n\ge N':$ $|a_n|<\epsilon$ Moreover, $|f_n(x)-f(x)|\le a_n \ \forall x \in E$ . So, $\forall \epsilon>0 \ \forall n\ge n_0:=\max(N,N') \ \forall x:$ $|f_n(x)-f(x)|\le|a_n|<\epsilon$ . So, $f_n\to f$ uniformly. I'm not sure that I have to use $\max(N,N')$ . I think that using only $N'$ the inequality might work, as the pointwise convergence is needed here to ensure simply that the limit of $f_n$ exists and as $f_n$ is bounded by $a_n$ independently of $n$ . Is it correct?","['sequence-of-function', 'proof-writing', 'real-analysis', 'functions', 'uniform-convergence']"
4074430,Möbius transformation mapping $\mathbb{D}$ into itself.,"I want to show that a Möbius transformation $f(z) = \frac{az + b}{cz + d}$ for $a,b,c,d \in \mathbb{C}$ maps the unit disk $\mathbb{D}$ into itself if and only if the coefficients satisfie \begin{equation*}\label{ineq}
|\overline{b}d - \overline{a}c| + |ad - bc| \leq |d|^2 - |c|^2.
\end{equation*} So far I have shown that if a Möbius transformation maps $\mathbb{D}$ into itself it satisfies the inequality. Now my idea for the implication $\Leftarrow$ : I consider the inequality above and want to show that for $|z| \leq 1 \implies$ $|f(z)| \leq 1$ .
Instead of showing $|f(z)| \leq 1 $ I defined for $|d| > |c|$ \begin{equation*}
 v = \frac{b \overline{d} - a\overline{c}}{|d|^2 - |c|^2}, \; R = \frac{|ad - bc|}{|d|^2 - |c|^2}
\end{equation*} and want to show $|f(z) - v|^2 \leq R^2$ since by the provided inequality
from $|f(z) - v|^2 \leq R^2$ follows $f(z) \in K_R(v) \subseteq \mathbb{D}$ .
For the case $|d| = |c|$ , $f$ wouldn't be a möbius transformation. Probelem: Now I have trouble showing the inequality above. I tried using the provided inequality
show that \begin{equation*}
|f(z) - v|^2 = |f(z)|^2 - 2 \Re(f(z)\overline{v}) + |v|^2 \leq |f(z)|^2 - 2 \Re(f(z)\overline{v}) + 1 - 2R + R^2
\end{equation*} but from then on I stuck by estimating \begin{align*}
    |f(z)|^2 &= f(z) \overline{f(z)} = \frac{|a|^2 |z|^2 + 2\Re(az \overline{b}) + |b|^2}{|c|^2 |z|^2 + 2\Re(cz \overline{d}) + |d|^2} \\
    |v|^2 &= \frac{b \overline{d} - a\overline{c}}{|d|^2 - |c|^2}\frac{\overline{b} d - \overline{a}c}{|d|^2 - |c|^2} = \frac{|b|^2 |d|^2 - 2 \Re(b \overline{d} \overline{a}c) + |a|^2|c|^2}{(|d|^2 - |c|^2)^2} \\
    &2 \Re(f(z)v) = 2 \frac{1}{|d|^2 - |c|^2}\Re  \left( \frac{ab\overline{d}z - a^2 \overline{c}z  - ab\overline{c} + b^2 \overline{d}}{cz + d}\right)
\end{align*} or showing that \begin{align*}
|f(z)|^2 - 2 \Re(f(z)\overline{v}) + 1 - 2R &\leq 0 \\
\end{align*} I made some estimates based on the fact that $|z| \leq 1$ but most of them lead nowhere.
I would appreciate it if someone can give me a hint or can tell that the idea doesn't work.","['complex-analysis', 'solution-verification', 'mobius-transformation']"
4074444,Continuous-time analogon to show that $\{\{t \geq 0: X_{t}=x\}\; \text{ is unbounded}\}$ is a $0-1$ event for a Markov process,"Is there a continuous-time analogon to show that $\{\{t \geq 0: X_{t}=x\}\; \text{ is unbounded set}\}$ is a $0-1$ event for a Markov process $(X_{t})_{t\geq 0}$ on a discrete state space $E$ My attempt: assume that $P_{x}(\{\{t \geq 0: X_{t}=x\}\; \text{ is unbounded set}\})<1$ Then with positive probability, there exists $A$ with $P_{x}(A)>0$ such that $\forall \omega \in A$ we have $\exists T(\omega)>0$ such that for all $t > T(\omega)$ , $X_{t}(\omega)\neq x$ , I want to introduce some kind of stopping time, I will choose: $R_{x}:=\inf\{t \geq 0: X_{t}=x\}$ . $P_{x}(\{\{t \geq 0: X_{t}=x\}\; \text{ is unbounded set}\})=\mathbb E_{x}[1_{\{t \geq 0: X_{t+R_{x}}=x\}\; \text{ is unbounded set}}1_{\{R_{x}<\infty\}}]=\mathbb E_{x}[\mathbb E_{x}[1_{\{t \geq 0: X_{t+R_{x}}=x\}\; \text{ is unbounded set}}\lvert \mathcal{F}_{R_{x}}]1_{\{R_{x}<\infty\}}]=\mathbb E_{x}[P_{x}(1_{\{t \geq 0: X_{t}=x+R_{x}\}\; \text{ is unbounded set}}\lvert \mathcal{F}_{R_{x}})1_{\{R_{x}<\infty\}}]=\mathbb E_{x}[P_{X_{R_{x}}}(1_{\{t \geq 0: X_{t}=x\}\; \text{ is unbounded set}})1_{\{R_{x}<\infty\}}]=P_{x}(\{\{t \geq 0: X_{t}=x\}\; \text{ is unbounded set}\})P_{x}(R_{x}<\infty)$ Could this fact help me at all? I guess I can assume cadlag sample paths so that we may use the strong Markov property.","['stochastic-processes', 'markov-process', 'probability-theory', 'stochastic-calculus', 'random-variables']"
4074475,Asymptotic location of the turning points of $\ddot x = -x-{\dot x}^3$,"Fix a solution $x(t)$ of the following ODE: $$\ddot x=-x-\dot {x}^3.$$ Does the energy $E(t) := x^2 + \dot x^2 $ decrease to zero as $t \to \infty$ ? Denote by $x_1,x_2,\ldots $ the positive values of $x(t)$ for which $\dot x(t)=0$ , ordered by time. Prove that the following limit exists: $$\lim_{n \to \infty} \frac{x_n-x_{n+1}}{x_n^3}.$$ Some Remarks To prove 1 it is enough to prove that $\dot x \to 0$ , since then $x\to c$ and the ODE forces $c=0$ . If we had $\dot x \not \to 0$ then $\dot x$ would have been bounded away from $0$ ""a lot"", so $\dot E=-\dot x ^4$ would decrease at a rate bounded from below, hence tend to zero. This is not yet a proof because $\dot x$ might only have spikes away from zero. Perhaps this will make its second derivative large and lead to a contradiction with the ODE? In the linearized equation the sequence $(x_n)$ is constant, and for $t\gg 0$ we expect the system to be very close to its linearization. The limit quantifies this. I cross-posted from the physics site.","['ordinary-differential-equations', 'perturbation-theory']"
4074508,"Does ""zero dimensional domains are fields"" require the Boolean Prime Ideal theorem?","Consider the following simple lemma: Lemma 1. Let $R$ be a commutative domain, then $R$ is a field iff $\dim(R)=0$ . N.B. By ""commutative domain"" I mean ''nonzero commutative ring with 1 such that $xy=0$ implies either $x=0$ or $y=0$ '' and ""dimension"" means ""Krull dimension"". $(\!\!\implies\!\!)$ is unconditionally true. The proof I know for $(\!\!\impliedby\!\!)$ involves the existence of maximal ideals and thus Zorn's lemma. This initally led me to wonder: Wrong question: is the axiom of choice (in the form that every proper ideal is contained in a maximal ideal) required to prove Lemma 1 ? Is Lemma 1 even equivalent to the axiom of choice? After searching the internet for clues I stumbled on this MathOverflow question on the existence of prime ideals and godelian's answer . The following doesn't require any choice principle: Lemma 2. Let $R$ be a nonzero commutative ring with $1$ , then $R$ is a field iff its only ideals are $(0)$ and $(1)$ . So to prove that zero dimensional domains are fields it is enough to prove that their only ideals are $(0)$ and $(1)$ . If there were some proper ideal $(0)<I<(1)$ then, by the Boolean Prime Ideal (BPI) theorem, the quotient ring $R/I$ would have a prime ideal which pulls back to a prime $(0)<I\leq \mathfrak{p}$ in $R$ . Thus $\dim(R)\geq 1$ which is a contradiction. Better question: is BPI required to prove Lemma 1 ? Is Lemma 1 strictly weaker than the BPI? One can slightly broaden the scope of the question by dropping the domain condition and asking Slightly broader question. Does ""prime ideals in zero dimensional rings are maximal"" require the BPI? Is it possibly even equivalent to the BPI? Or is it weaker? One can ask a broader question still without mentioning dimensions: Alternative phrasing. What choice principle is required to prove that prime ideals which are maximal among prime ideals are maximal among all proper ideals.","['ring-theory', 'axiom-of-choice', 'abstract-algebra', 'maximal-and-prime-ideals']"
4074548,Show that $f''(\xi)≥8$,"Let $f:[0,1]\rightarrow\mathbb{R}$ be a $\mathcal{C}^{2}([0,1])$ function such that $f(0)=f(1)=1$ and such that the minimum of $f$ is negative. Prove that there must exist a point $\xi\in [0,1]$ such that $f''(\xi)≥8$ Maybe I can use the fact that there exists maximum and minimum in $[0,1]$ so there are two critical points. But I don't know how to proceed. Can someone give me any advice? Thanks before!","['maxima-minima', 'derivatives', 'real-analysis']"
4074570,Percentage calculation: What's the difference between using 1.08875 and 0.08875?,"Use case is for tax calculation. Tax is at 8.875%. Using $100 tax-inclusive amount, I get different tax amounts using these formulas: First: $$\mathrm{taxAmount} = \$100\times 0.08875 = \$8.875$$ Second: $$\mathrm{taxAmount} = \$100 - (\frac{\$100}{1.08875}) = \$8.15$$ I find the first formula is the correct one and straightforward but a colleague suggests to use the second. What's the difference and which one is correct?","['algebra-precalculus', 'percentages']"
4074575,"The Etale Fundamental Group of $\mathbb{A}_{\mathbb{F}_q}$ and $\mathbb{G}_{m,\mathbb{F}_q}$","I am learning the etale fundamental group of a scheme. And I am hugely confused by the etale fundamental group of the additive  and multiplicative group scheme $\mathbb{G}_{a}$ and $\mathbb{G}_{m}$ over a finite field $\mathbb{F}_q$ . I know that for an algebraic variety $X$ over a general field $k$ , its etale fundamental group suits the following exact sequence $1 \rightarrow \pi_1(X_{k^s}) \rightarrow  \pi_1(X) \rightarrow Gal(k^s/k) \rightarrow 1$ . So what I want can be captured more or less by $\pi_1(\mathbb{G}_{a,\bar{F_q}})$ and $\pi_1(\mathbb{G}_{m,\bar{F_q}})$ . But the reduced question seems complicated too, because of the characteristic is not $0$ . When characteristic $0$ everything is known and can be found, and the result is that $\pi_1(\mathbb{G}_{a,\bar{k}})=1 $ and $\pi_1(\mathbb{G}_{m,\bar{k}})=\mathbb{\widehat{Z}}$ , by Riemann-Hurwitz formula or something else. But in characteristic $p$ , for $\mathbb{G}_{m}$ , at least you don't have the $\mathbb{Z}_p$ part of $\mathbb{\widehat{Z}}$ , for the multiplication by $p$ map $\mathbb{G}_m \rightarrow \mathbb{G}_m$ is not unraminfied. And further, the etale fundamental group of $\mathbb{G}_{m}$ seems to be larger than $\prod_{l\neq p}{\mathbb{Z}_l}$ , for some kind of higher ramification theory which I don't know. I also heard the existence of Artin-Schreier extension, but totally don't understand. Can someone give me any tips on that? Or any information about the fundamental group of $\mathbb{G}_{a}$ and $\mathbb{G}_{m}$ over finite field $\mathbb{F}_q$ ?","['etale-cohomology', 'finite-fields', 'fundamental-groups', 'algebraic-geometry', 'arithmetic-geometry']"
4074630,Improper Riemann Integral equal to Lebesgue Integral,"Let $f: [a,b] \to [0,\infty)$ and $f$ is Riemann Integrable on every subinterval $[a + \epsilon,b]$ for $\epsilon > 0$ . Suppose that the improper Riemann integral exists. That is $$I = \lim_{\epsilon \to 0} \int_{a + \epsilon}^{b} f(x) dx < \infty$$ exists. Prove that $f$ is Lebesgue integrable on $[a,b]$ and that $\int_{[a,b]} f(x) dx = I$ . I found another posting of this problem written in the same way, but it used dominated convergence theorem, which I have not covered. The others I saw were written slightly differently and/or weren't making too much sense. Also, maybe I'm not seeing something clearly, but here's something I thought: The improper integral I is finite (it exists). If the improper integral exists, isn't it equal to the ""proper"" integral? That would mean the ""normal"" proof of showing that every Riemann Integrable function is Lebesgue integrable would apply. However, I have a feeling this is not the way to prove it or else this question wouldn't be asked. Thanks for the help!","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'real-analysis', 'riemann-integration']"
4074651,"Which is the smallest parameter of the polynomial of the smallest degree such that it has $\frac{\sqrt{3}}{2},\frac{\sqrt{2}}{3}$ as roots?","Which is the smallest parameter of the polynomial of the smallest degree such that it has $\frac{\sqrt{3}}{2},\frac{\sqrt{2}}{3}$ as roots and has only integral parameters? Since it has two different roots the it must be of degree at least $2$ . If it is of degree $2$ , then it is of the form $(x-\frac{\sqrt{3}}{2})(x-\frac{\sqrt{2}}{3})$ , but this type of polynomial has parameters which are not integers. If it is of degree $4$ then we have the polynomial $(x-\frac{\sqrt{3}}{2})(x+\frac{\sqrt{3}}{2})(x-\frac{\sqrt{2}}{3})(x+\frac{\sqrt{2}}{3})$ . It remains to examine the case where it is of degree $3$ . This is where I got stuck. I don't know how to examine that case. Could you please explain to me how to solve this question?","['contest-math', 'algebra-precalculus', 'problem-solving', 'polynomials']"
4074756,Some (maybe) basic estimate of expected values involving brownian motion,"I have the following estimation in a paper I'm reading about Brownian Motion on a Torus: $$
1+\theta\ \mathbb E_x[\tau_A] + \sum_{i=2}^\infty \left(\theta\sup_{x\in\mathbb T_2}\mathbb E_x[\tau_A]\right)^i \leq \exp\left(\theta\mathbb E_x[\tau_A] + 2\theta^2 \sup_{x\in\mathbb T_2}\mathbb E_x[\tau_A]^2\right)
$$ where $\mathbb T_2$ is a Torus, $A\subset\mathbb T_2$ closed, $x\in\mathbb T_2$ and $0<\theta<\frac 1 2 \sup_{x\in\mathbb T_2}\mathbb E_x[\tau_A]^{-1}$ . Ideas: To prove this,  I have already tried the Taylor series of $e^x$ , the geometric sum, the Jensen inequality and a few other things, however nothing worked so far. I'd be really glad if you could give me some thoughts about what else to do.","['stochastic-processes', 'brownian-motion', 'probability-theory', 'probability']"
4074867,Beautiful monster: Catalan's constant and the Digamma function,"The problem I have been trying for a while now to show that this monster $$\begin{align}
&\int_0^{\pi/4}\tan(x)\sum_{n=1}^{\infty}(-1)^{n-1}\left(\psi\left(\frac{n}{2}\right)-\psi\left(\frac{n+1}{2}\right)+\frac{1}{n}\right)\sin(2nx)\,\mathrm{d}x \\
+&\int_0^{\pi/4}\cot(x)\sum_{n=1}^{\infty}\left(\psi\left(\frac{n+1}{2}\right)-\psi\left(\frac{n}{2}\right)-\frac{1}{n}\right)\sin(2nx)\,\mathrm{d}x,
\end{align}
$$ where $\psi$ denotes the Digamma function ,
is equal to the beautiful Catalan's constant $$G :=\sum_{n=0}^{\infty} \frac{(-1)^{n}}{(2n+1)^2}.$$ I think it's a great problem; going from such a big expression to a nice, simple constant. We can see that these integrals are very similar, the main difference is probably the $\tan(\cdot)$ and the $\cot(\cdot)$ . I think that a crutial step would be to find a Fourier series. Small thank you note The user @Quanto helps to higher the quality of this site by posting now and then very interesting integrals. I want to start doing the same. Thank you for inspiring me.","['integration', 'definite-integrals', 'digamma-function', 'fourier-series', 'catalans-constant']"
4075009,Is there a Kubota-Leopoldt $p$-adic zeta function implementation in SageMath?,Exactly as in the title. I am learning the $\mathbb{Z}_p^\times$ measure-theoretic construction of $p$ -adic $L$ -functions and was wondering if there was an `easy' way produce some example computations with these in the specific case of the interpolated $p$ -adic Riemann zeta function. Any code for this or suggestions for how one might algorithmically implement this would also be very welcomed! Thank you in advance!,"['l-functions', 'measure-theory', 'p-adic-number-theory', 'sagemath']"
4075052,Finding the value of the following double integral,"The following question appeared in the American Mathematical Monthly (AMM), problem 12247, Vol.128, April 2021. For positive real constants $a$ , $b$ , and $c$ , prove $$\int_0^{\pi} \int_0^{\infty} \frac{a}{\pi(x^2+a^2)^{3/2}} \frac{x}{\sqrt{x^2+b^2+c^2-2cx\cos \theta}}~dx~d\theta=\frac{1}{\sqrt{(a+b)^2+c^2}}.$$ I tried to use the substitution $x=a\tan\theta$ . But it does not help a lot. Please guide. Any answer will be highly appreciated. Solutions posted in MSE are based on Fourier transform methods which sometimes are beyond the scope of undergraduate courses.
I would like to see if solutions based on more traditional Calculus methods can be found.","['integration', 'multiple-integral', 'real-analysis']"
4075118,Finding the inverse of this spatial transform,"I have a 2D spatial transform that intuitively is 1:1 (bijective), but I am having a hard time coming up with the formula for the inverse. Given a point $(x,y)$ , the forward transform is like so: $
y' = f(x,y) = y + A_y \cdot sin(f_y\cdot x + P_y)\\
x' = g(x,y) = x + A_x \cdot sin(f_x\cdot y + P_x)$ Where $A$ , $f$ , and $P$ are constants representing amplitude, frequency, and phase, respectively. They can be different for the $x$ and $y$ components. Note, importantly, $y'$ is a function of both $x$ and $y$ , and the same with $x'$ . Visually, if you start with space like this: And apply the transform only for one of $x$ or $y$ , you might get this: Or if you have nonzero contribution in both components, you might get: So, just eyeballing it, and generally considering cases where $A\ne0, f\ne0$ , it seems like this transform should be invertible. But so far, I've failed to come up with a good formula for it. Some basic progress I've made, trying to ""solve for x"" (the x and y cases are essentially the same): We can rearrange the $y' = \dots$ formula to: $y = y' - A_y\cdot sin(f_y\cdot x + P_y)$ Or, likewise, we can rearrange the $x' = \dots$ formula to get $y$ $y = \frac{sin^{-1}(\frac{x' - x}{A_x}) - P_x}{f_x}$ And the obvious thing to attempt is to set these equal, and solve for $x$ : $y' - A_y\cdot sin(f_y\cdot x + P_y) = \frac{sin^{-1}(\frac{x' - x}{A_x}) - P_x}{f_x}$ Okay, great... Now how to turn that into $x = \dots$ ? Or maybe there's some superior approach I'm not thinking of? Even just ignoring $A$ , $f$ , and $P$ , it boils down to: $y' - sin(x) = sin^{-1}(x' - x)$ which doesn't seem clear how to solve, to me.","['trigonometry', 'functions', 'transformation', 'inverse-function']"
4075152,"On an identity between fractions, related to a determinant by K. Rohn","The following formula for a determinant is attributed to Karl Rohn (e.g. in Bruijn, de, N. G. (1955). On some multiple integrals involving determinants. Journal of the Indian Mathematical Society. New Series, 19, 133-151. ): $$ 
\det \left(\left(\frac{x_i-x_j}{x_i+x_j} \right)_{i, j=1, \ldots, n}\right) = \prod_{1 \le i < j \le n}\left(\frac{x_i-x_j}{x_i+x_j} \right)^2 \, .
$$ In a recent answer I provided a proof of that formula via induction. A crucial role in the induction step played the following identity: $$ \tag{1}
\frac{a-b}{a+b}\cdot\frac{c-d}{c+d} + \frac{a-c}{a+c}\cdot\frac{d-b}{d+b} + \frac{a-d}{a+d}\cdot\frac{b-c}{b+c}\\ =
\frac{a-b}{a+b}\cdot\frac{a-c}{a+c}\cdot\frac{a-d}{a+d}\cdot\frac{b-c}{b+c}\cdot\frac{b-d}{b+d}\cdot\frac{c-d}{c+d} \, ,
$$ which came a bit surprising. $(1)$ holds for all real (or complex) numbers as long as it is well-defined, i.e. no denominator is zero. I verified it with a computer algebra system (Maxima). One could also argue as follows: For fixed $b, c, d$ are both sides of $(1)$ rational functions in $a$ , of degree at most $3$ . These rational functions both have zeros at $a=b, c, d$ and poles at $a=-b, -c, -d$ . It follows that these rational functions differ only by a constant factor. Therefore it suffices to show that $(1)$ holds for $a \to \infty$ , i.e. that $$ \tag{2}
\frac{c-d}{c+d} + \frac{d-b}{d+b} + \frac{b-c}{b+c} =
\frac{b-c}{b+c}\cdot\frac{b-d}{b+d}\cdot\frac{c-d}{c+d}\, .
$$ And that is a known identity, see for example Show the identity $\frac{a-b}{a+b}+\frac{b-c}{b+c}+\frac{c-a}{c+a}=-\frac{a-b}{a+b}\cdot\frac{b-c}{b+c}\cdot\frac{c-a}{c+a}$ , where one can find some elegant proofs of $(2)$ . One could also repeat the argument: For fixed $c, d$ are both sides of $(2)$ rational functions in $b$ , of degree at most $2$ . These rational functions both have zeros at $b=c, d$ and poles at $b=-c, -d$ . It follows that these rational functions differ only by a constant factor. Therefore it suffices to show that $(2)$ holds for $b \to \infty$ , i.e. that $$ \tag{3}
 \frac{c-d}{c+d}  = \frac{c-d}{c+d} \, ,
$$ and that is obviously true. So $(1)$ implies $(2)$ and that implies $(3)$ . On the other hand, $(3)$ can be used to prove $(2)$ , and that can be used to prove $(1)$ . What I am asking for is: Other proofs of the initial identity $(1)$ . If possible, some deeper insight how equations $(1)$ , $(2)$ , and $(3)$ are related. If possible, further generalizations: What would the corresponding identity be for five numbers (if there is one)?","['fractions', 'algebra-precalculus', 'determinant']"
4075168,Convergence of $\lim_{n \to \infty}\sum_{i=1}^{n}\frac{a_i}{i+n}$ [duplicate],"This question already has answers here : Suppose a series $a_n$ is greater than 0 for all positive integer n, and that $\sum \frac {a_n} n$ converges, then does the following also converge? (2 answers) Closed 3 years ago . If $\sum_{n=1}^{\infty}\frac{a_n}{n}=a$ converges to a finite number with $a_n \geq 0$ for all $n \geq 1$ does this also imply that $$\lim_{n \to \infty}\sum_{i=1}^{n}\frac{a_i}{i+n}=0$$ I tried using $\lim_{n \to \infty}\sum_{i=1}^{n}\frac{a_i}{n}$ as a bound so that I can use comparison test but I can't prove that it converges either. How would I go about proving this statement?","['integration', 'calculus', 'infinity', 'sequences-and-series', 'limits']"
4075230,Union of countable sequence of countable sets,"I realize this question has been asked and answered multiple times, but I'm still not understanding the reasoning. For reference, I've read this , this , and this . Theorem 2.12. Let {𝐸𝑛}, 𝑛=1,2,…, be a sequence of countable sets, and put $$S = \bigcup\limits_{n=1}^{\infty}E_n$$ Then 𝑆 is countable. Proof. Let every set 𝐸𝑛 be arranged in a sequence {𝑋𝑛𝑘},
𝑘=1,2,3,…, and consider the infinite array in which the elements of 𝐸𝑛 form the 𝑛th row. The array contains
all elements of 𝑆. As indicated by the arrows, these elements can be arranged in a
sequence 𝑥11;𝑥21,𝑥12;𝑥31,𝑥22,𝑥13;𝑥41,𝑥32,𝑥23,𝑥14;…(*) If any two of the sets 𝐸𝑛 have elements in common, these will appear
more than once in (∗). Hence there is a subset 𝑇 of the set of all
positive integers such that 𝑆∼𝑇, which shows that 𝑆 is at most
countable. Since 𝐸1⊂𝑆, and 𝐸1 is infinite, 𝑆 is infinite, and thus
countable. ◼ I understand if there's an injection $F: S\rightarrow \mathbb{N}$ , S is countable. I don't understand how utilizing the diagonals leads to such an injection. Since there's 1 element in the first diagonal, 2 in the second, ..., n in the n-th, (n - 1) in the (n + 1)-th, etc., how is it possible to say that since there are n elements at most in any diagonal the set is countable?","['elementary-set-theory', 'real-analysis']"
4075258,Arithmetic on infinite formal prime factorizations,"By the fundamental theorem of arithmetic, we can identify a natural number $x$ with the sequence $(a_n)$ of exponents in its prime factorization $x=\prod_np_n^{a_n}$ , where $p_n$ is the $n$ th prime. A sequence obtained in this way will of course have only finitely many nonzero elements. However, multiplication/ $\gcd$ / $\operatorname{lcm}$ operations on natural numbers correspond respectively to pointwise addition/ $\min$ / $\max$ of these sequences, and these operations make sense on arbitrary sequences. So I wonder: How much arithmetic can we do with these sequences, thinking of them as formal infinite products of prime powers? Specifically: Is there a semiring structure (without $0$ ) on $R=(\Bbb Z_{\ge 0})^\Bbb N$ where $\times$ is pointwise addition and $+$ coincides with normal addition on finite natural numbers (represented as described above)? If so, I'm also interested in whether we can get other familiar properties. Does $\gcd(a,a+b)=\gcd(a,b)$ hold (where $\gcd$ = pointwise $\min$ )? Is $<$ , defined by $a<b\iff\exists c.a+c=b$ , a total order? If we include $0$ and negatives (e.g. by considering equivalence classes of formal differences), does Bézout's lemma hold? Can we in fact get a (non-standard) model of arithmetic? Are these the hyperintegers or something? Edit: Commenters have pointed out that the answer to the last bullet is no, since this structure lacks large primes and includes elements with no largest prime divisor (and both of these properties are inconsistent with the axioms of arithmetic). The latter issue prevents this structure from even being a sub-semiring of a model of arithmetic.","['number-theory', 'ring-theory', 'nonstandard-models']"
4075263,Comparing the formulas of trigonometric and hyperbolic functions,"I just tried to memorize hyperbolic formulas, and realized that there are lots of similarity between formulas for trigonometric and hyperbolic functions. for example: $$\cos^2x+\sin^2x=1\quad\quad\quad\quad\quad\quad\quad1+\tan^2x=\sec^2x$$ $$\cosh^2x-\sinh^2x=1\quad\quad\quad\quad\quad\quad\quad1-\tanh^2x=\text{sech}^2 x$$ And so on. When I look at it closer I see they are almost the same formulas but the sign comes before a $\sinh$ or $\tanh$ are different than the sign of $\sin$ , $\tan$ in trig functions. To justify that, I considered the fact that parametric points $(\cos\theta,\sin\theta)$ placed on the circle $x^2+y^2=1$ and points $(\cosh \theta,\sinh \theta)$ are placed on hyperbola $x^2-y^2=1$ , so we see the different signs comes before $y^2$ and because $\sin$ and $\sinh$ represents the vertical distance, these signs are different in all formulas. Is my justification right? Is there better way to justify this different in the formulas?","['calculus', 'trigonometry', 'hyperbolic-functions']"
4075267,Concentration of the $\ell_2$ error of the empirical distribution,"Let $X$ be a random variable that takes values only in the set $\{1,2, \dots, m\}$ such that $\Pr(X = i) = p_i$ for all $i = 1,2, \dots, m$ . Let $S = \{X_1, X_2, \dots, X_n\}$ be a set of $n$ i.i.d. realisations of $X$ . We can then construct the empirical distribution $\{\hat{p}_i\}_{i = 1}^m$ using the samples from $S$ , where $\hat{p}_i = \frac{1}{n} |\{ j : X_j = i\}|$ . I am interested in the concentration inequalities for the random variable $Z := Y - \mathbb{E}[Y]$ where $Y = \| \hat{p} - p \|^2_2$ , the square of the $\ell_2$ norm of the difference between the true and the empirical distribution. I have found results in the literature on the concentration of the $\| \hat{p} - p\|_1$ , $D(\hat{p} \| p)$ and $\sup_{i} |\hat{p}_i - p_i |$ , where $D(p\|q)$ is the KL divergence between $p$ and $q$ . However, quite surprisingly, I haven't come across any result on the concentration of the $\ell_2$ norm. While it is possible to directly use the results for other norm for my case, it turns out that the resulting bounds are not tight enough for my case. Any leads or references will be appreciated. Thanks!","['concentration-of-measure', 'probability', 'random-variables']"
4075312,How to show $Var(T_n)=\infty$?,"This question is from George Casella Statistical Inference textbook question 10.5. For the mean $\bar{X_n}$ of n iid normal observations with $EX=\mu$ and $VarX=\sigma^2$ , if we take $T_n=\sqrt{n}/\bar{X_n}$ then show $Var(T_n)=\infty$ . The solution is : I don't quite understand this solution. Please see my process is correct or not. First, I need to know the pdf of $\bar{X_n}$ , which is $n(\mu, \sigma^2/n)$ . Hence, $$E(T_n^2)=\int_{-\infty}^{\infty}\frac{n}{x^2}*pdf=\int_{-\infty}^{\infty}\frac{n}{x^2}\frac{\sqrt{n}}{\sqrt{2\pi\sigma^2}}exp(-\frac{n(x-\mu)^2}{2\sigma^2})$$ . Is my above step correct?","['integration', 'statistics', 'normal-distribution', 'probability']"
4075364,"Question on limiting variance in example 10.1.8 (Casella, Statistical Inference, 2nd edition)","This is from George Casella textbook statistical inference (page 470) Example 10.1.8. Limiting variances. It says for the mean $\bar{X_n}$ of n iid normal observations with $EX=\mu$ and $VarX=\sigma^2$ , if we take $T_n=\bar{X_n}$ , then $lim\sqrt{n}Var\bar{X_n}=\sigma^2$ is the limiting variance of $T_n$ . I feel very confused. I think $\bar{X_n}$ is $n(\mu, \sigma^2/n)$ . Then $lim\sqrt{n}*Var\bar{X_n}=lim\sqrt{n}*\sigma^2/n=\sigma^2lim\frac{\sqrt{n}}{n}=0$ . The limit is 0, not $\sigma^2$ . Please point my mistake.","['statistical-inference', 'statistics', 'probability']"
4075407,"How do you obtain Variance of the ""method of moment estimator"" for the beta distribution using delta method?","I'm trying to solve a question where I have to find $V(\hat{\alpha})$ using the delta-method where $V$ is notation for variance and $\hat{\alpha}$ is the method of moment estimator for a beta distribution. To be more specific, I write down the question below: Suppose $X_1, ..., X_n$ be a random sample from the following distribution, $$f(x)=\alpha x^{\alpha -1}, 0<x<1\text{ and }\alpha>0.$$ Let $\hat{\alpha}$ be the method of moment estimator. Obtain the $V(\hat{\alpha})$ using the delta-method. I know that this function is just Beta distribution where $X\sim\mathrm{Beta}(\alpha,1)$ . So the method of moment $E(X^k)=\frac{\Gamma(\alpha +1)\Gamma(\alpha +k)}{\Gamma(\alpha)\Gamma(\alpha +k+1)}$ , where $\Gamma(\cdot)$ is a Gamma function. But how do I get the variance of the method of moment using delta method? I am not sure how the usage of the delta method will get me the variance. Thank you.","['gamma-function', 'statistics', 'variance', 'delta-method']"
4075411,"Given $\triangle ABC$ with $AB=BC$ and $\angle ABC=80^\circ$, and $O$ such that $\angle OAC=30^\circ$ and $\angle OCA=10^\circ$, find $\angle BOC$.","I encountered the following problem in a math problem book, in a section, entitled Geometry . In an $\Delta ABC$ with $AB=BC$ and $\angle ABC = 80^\circ$ , the point $O$ is chosen so that $\angle OAC = 30^\circ$ and $\angle OCA = 10^\circ$ . Find $\angle BOC$ . I struggled repeatedly trying to solve this using some reasonable selection of methods, and failed utterly. I can construct a solution using analytic geometry, but that is not a fair choice of methods for this section. From simple geometry, I keep writing down systems of equations using various triangles in the picture, and always keep missing one constraint, as it seems there are infinite amount of solutions to the system, which I know is wrong since the analytic geometry approach yields a unique solution. I am happy to add my infinite solution approach using the angle constraints, if you find it helpful. Please let me know if that's the case. I would like to solve this using methods from geometry, without using more advanced techniques. Thank you very much, I really appreciate your help.","['triangles', 'angle', 'geometry']"
4075438,Doob-like equality with conditional expectation,"Suppose $X_t$ is a positive, continuous martingale w.r.t. a filtration $\mathcal{F}_t$ such that $X_t \to 0$ a.s. when $t \to \infty$ . Given $X^* = \sup_{t \in \mathbb{R}^+} X_t$ , I want to prove that, for a given $x \in \mathbb{R}^+$ \begin{equation*}
\mathbb{P}(X* \geq x \mid \mathcal{F}_0) = 1 \land \frac{X_0}{x}
\end{equation*} Where $a \land b = \min{(a,b)}$ This is very similar to the Doob martingale inequality , and, to get rid of the conditional expectation, I have tried to work with the sequence $I_{B} X_n$ with $B \in \mathcal{F}_0$ and prove it like Rosenthal;s A First Look at Rigorous Probability Theory proof of Doob's inequality (Theorem 14.3.1): Let $A_k = \{ X_k \geq x , X_i < x \;, \; \forall \; 0< i <j\}$ . Note that $A_i \cap A_j = \emptyset$ . So $$ A = \bigcup_{k=0}^\infty A_k = \{ X* \geq x\}$$ Then \begin{split}
x \; \mathbb{P}(A\mid B) &= x \sum_{k=0}^\infty \mathbb{E}[{I_A I_B}] \\
&=  \sum_{k=0}^\infty \mathbb{E}[x {I_A I_B}] \\
\end{split} But I keep stuck there. ¿Is this the right path?","['stochastic-processes', 'probability-theory', 'martingales']"
4075454,Lipschitz monotone functions have inverse a.e.?,"Suppose we have some $f:[0,1] \to \mathbb{R}$ , such that $f$ is Lipschitz continuous and monotone, with $\mathrm{range}(f) = [0,1]$ . What can we say about the size of the set $I = \{t\in [0,1]: f^{-1}(t) \text{ exists and is unique}\}$ ? I suspect that it is possible to say that $\lambda(I) = 1$ , but I'm not even sure how to show that the set is nonempty. Note that the Cantor staircase is NOT Lipschitz, so it is not a valid counterexample to showing that the set can be empty. One fact that I tried playing around with the fact that absolutely continuous + monotone implies that $f'$ exists a.e. with $f(1) - f(0) = \int_0^1 f'(t) dt$ implying that $f'(t) >0$ in a set that has positive measure. But I'm not sure how to continue.","['measure-theory', 'real-analysis']"
4075514,"Proving a function is an automorphism, but not an inner automorphism","I am solving a homework problem from Benedict Gross's abstract algebra class, the lectures for which are available online. Since there aren't solutions available, I was hoping someone here could look over and critique my solutions. The problem is: Define $f: \mathrm{GL}_n (\mathbb{R}) \to \mathrm{GL}_n (\mathbb{R})$ by $f(A) = \left(A^t\right)^{-1}$ . Show that $f$ is an automorphism, but not an inner automorphism for $n \geq 1$ . Here is my attempt. We first show that $f$ is a homomorphism. Given $A,B \in \mathrm{GL}_n (\mathbb{R})$ , we have that \begin{align*}
f(AB) = \left[\left(AB\right)^t\right]^{-1} = \left(B^t A^t\right)^{-1} = \left(A^t\right)^{-1} \left(B^t\right)^{-1} = f(A) f(B),
\end{align*} as desired. Second, given $A,B \in \mathrm{GL}_n (\mathbb{R})$ for which $f(A) = f(B)$ , we have $\left(A^t\right)^{-1} = \left(B^t\right)^{-1}$ , so $A^t = B^t$ since inverses are unique. Transposing both sides, we obtain $A = B$ , so $f$ is injective. Finally, given $A \in \mathrm{GL}_n (\mathbb{R})$ , consider $B = \left(A^{-1}\right)^t$ . Since $A$ is invertible, $A^{-1}$ exists and is itself invertible, and since $\det \left(A^{-1}\right) = \det\left(A^{-1}\right)^t$ , we have $\left(A^{-1}\right)^t \in \mathrm{GL}_n (\mathbb{R})$ . We then have \begin{align*}
f\left(B\right) = \left(B^t\right)^{-1} = \left[\left(\left(A^{-1}\right)^t\right)^t\right]^{-1} = \left(A^{-1}\right)^{-1} = A,
\end{align*} so $f$ is surjective and hence bijective, so $f$ is an automorphism of $\mathrm{GL}_n (\mathbb{R})$ . Now suppose for the sake of contradiction that for every $n \geq 1$ , $f$ is an inner automorphism, so given such an $n$ , there exists a $B \in \mathrm{GL}_n (\mathbb{R})$ such that $f(A) = BAB^{-1}$ . But let $\lambda \neq 0,1$ and consider $A = \lambda I$ . Since $\lambda \neq 0$ , $A \in \mathrm{GL}_n (\mathbb{R})$ . Furthermore, $\lambda I \in Z\left(\mathrm{GL}_n (\mathbb{R})\right) \subset \mathrm{GL}_n (\mathbb{R})$ , we have \begin{align*}
BAB^{-1} = A\left(BB^{-1}\right) = AI = A,
\end{align*} but $f(A) = A^{-1}$ , though $A^{-1} \neq A$ since $\frac{1}{\lambda} I \neq I$ for any $\lambda \neq 1$ , so we have reached a contradiction. The way that I derived $A = \lambda I$ is that I was looking for a matrix, generally, for which $A \neq A^{-1}$ , i.e., $A^2 \neq I$ , and this was the cleanest way to get a counterexample for every natural number $n$ . If there are any other counterexamples, I'd be interested in seeing them.","['group-theory', 'solution-verification']"
4075522,Showing a set function is a premeasure,"Definition: Let $S$ be a collection of subsets of a set $X$ and $\mu\colon S\to[0,\infty]$ a set function. Then $\mu$ is called a premeasure provided $\mu$ is both finitely additive and countably monotone and, if $\emptyset$ belongs to $S$ , then $\mu(\emptyset) = 0$ . Question: Consider the collection $S = \{\emptyset, [0, 1], [0, 3], [2, 3]\}$ of subsets of $\mathbb{R}$ and define $\mu(\emptyset) = 0$ , $\mu([0, 1]) = 1$ , $\mu([0, 3]) = 1$ , $\mu([2, 3]) = 1$ . Show that $\mu\colon S\to[0,\infty]$ is a premeasure. Firstly, we'll show finite additivity. Notice, $\{\emptyset,[0, 1]\}$ , $\{\emptyset,[0, 3]\}$ , $\{\emptyset,[2, 3]\}$ , $\{\emptyset\}$ , $\{[0, 1]\}$ , $\{[0, 3]\}$ , $\{[2, 3]\}$ are all the finite collections of disjoint sets in $S$ whose unions of each induvial collection if back in $S$ . Further, notice for each induvial collection, $$\mu(\bigcup\{\emptyset,[0,1]\}) = \mu([0,1]) = 1  = 0 + 1 = \mu(\emptyset) + \mu(\{0,1\})$$ $$\mu(\bigcup\{\emptyset,[0,3]\}) = \mu([0,3]) = 1  = 0 + 1 = \mu(\emptyset) + \mu(\{0,3\})$$ $$\mu(\bigcup\{\emptyset,[2,3]\}) = \mu([2,3]) = 1  = 0 + 1 = \mu(\emptyset) + \mu(\{2,3\})$$ $$\mu(\bigcup\{\emptyset\}) = \mu(\emptyset) = 0  = \mu(\emptyset)$$ $$\mu(\bigcup\{[0,1]\}) = \mu([0,1]) = 1  = \mu([0,1])$$ $$\mu(\bigcup\{[0,3]\}) = \mu([0,3]) = 1  = \mu([0,3])$$ $$\mu(\bigcup\{[2,3]\}) = \mu([2,3]) = 1  = \mu([2,3]).$$ Therefore, the set function $\mu$ is finitely additive. Now, we'll show countably monotone. For $\emptyset$ any cover will have a sum of measure equal to $0$ or greater than $0$ . For $[0,1]$ any cover will have a sum of measure equal to $1$ or greater than $1$ . For $[0,3]$ any cover will have a sum of measure equal to $1$ or greater than $1$ . For $[2,3]$ any cover will have a sum of measure equal to $1$ or greater than $1$ . So, indeed whenever a set $E\in S$ is covered by a countable collection $\{E_k\}_{k=1}^{\infty}$ of sets in $S$ , then the measure of $E$ is less than the measure of the cover. Therefore, the set function $\mu$ is countably monotone. As $\emptyset$ belongs to $S$ , and $\mu(\emptyset)=0$ by construction, we have that $\mu$ is a premeasure. My question is, is the above correct?","['measure-theory', 'real-analysis']"
4075523,Existence of continued fraction $\sqrt{n}$ with any period $k$,"In this paper it is conjectured that for any positive integer $k$ there are infinitely many primes $p$ with the continued fraction expansion of $\sqrt{p}$ having length $k$ (Conjecture 5.1, https://web.williams.edu/Mathematics/sjmiller/public_html/mathlab/public_html/jr02fall/Periodicity/alexajp.pdf ) Do we have the weaker result that for any positive integer $k$ there exists at least one prime $p$ with the continued fraction expansion of $\sqrt{p}$ having length $k$ ? Or perhaps relax the restriction of prime $p$ to any positive integer $n$ ?","['number-theory', 'continued-fractions', 'open-problem']"
4075536,"Set $S$ of integers $\ge0$ such that $\{1,2,\cdots,n\}$ partitions into $A, B$ with $\sum_\limits{a\in A}a^k=\sum_\limits{b\in B}b^k$ for all $k\in S$","Let $s_n$ be the largest size of a set $S$ of integers $\ge0$ st there exist two subsets $A,B\subseteq \{1,2,...,n\}$ that satisfy the conditions (1) $A\cap B=\emptyset$ , (2) $A\cup B=\{1,2,...,n\}$ , (3) $\sum_\limits{a\in A}a^k=\sum_\limits{b\in B}b^k$ for all $k\in S$ . What is $s_n$ ?  What is an example for $S,A,B$ in the case $|S|=s_n$ ? This question is inspired by a question my teacher asked me.  The original question was to show that $\{1,2,...,2^m\}$ can be partitioned into two sets $A,B$ of equal size st $\sum_\limits{a\in A}a^k=\sum_\limits{b\in B}b^k$ for $k=0,1,...,(m-1).\quad$ I solved that problem (by induction on m) and wonder about what happens if I replace $2^m$ by other integers $n$ . First $s_n=0$ for all $n\equiv 1\pmod{4}$ because the number of odd numbers in $\{1,2,...,n\}$ is odd.  Then $s_n=1$ for all $n\equiv 2\pmod{4}$ using the same reason and only $S=\{0\}$ works.  For $n=2^mt\space$ where $t$ is odd and $m\ge 2$ , I know from the question my teacher asked me that $s_n\ge m$ because $S=\{0,1,...,m-1\}$ is an example.  I'm not sure whether $s_n=m$ in these cases.  Please help me solve the case $n=2^mt\space$ where $t$ is odd and $m\ge 2$ .","['algebra-precalculus', 'set-partition', 'summation', 'recreational-mathematics']"
4075551,Progress on a conjecture of Burnside...,"Given a group $G $ , the set of automorphisms of $G $ also forms a group, $\rm {Aut}(G) $ ,with composition as the operation (recall that an automorphism of a group is a bijective endomorphism) . An inner automorphism is one determined by conjugation by some element $g\in G $ .  That's we have the automorphism $i_g $ given by $i_g (h)=ghg^{-1}\,\forall h $ . I learned from a comment by @LeeMosher on this site that Burnside once conjectured that any class-preserving automorphism is inner.  Does anyone know about any progress on this? Of course the converse is trivial. (Btw, the reference here is to conjugacy classes.  Of course the class equation gives the sizes of these classes.  For example,  in the case of an abelian group,  the class equation consists in all ones.) For reference,  this would have been early in the $20$ -th century.","['automorphism-group', 'mapping-class-group', 'conjectures', 'abstract-algebra', 'group-theory']"
4075585,The set of Involutary Matrices is a smooth regular submanifold?,"Let $M_n(\mathbb R)=M_{n\times n}(\mathbb R)$ be the vector space of $n\times n$ matrices with entries in $\mathbb R$ .
Let $S=\big\{ A\in M_2(\mathbb R)\,\big|\, A^2=I,\, A\ne \pm I\big\}$ . Show that $S$ is a smooth regular submanifold of $M_2(\mathbb R)$ . I am using the Regular Value Theorem stated in this question: Regular Value Theorem Using Implicit Function Theorem in Calculus. I tried to construct a map $f: M_2(\mathbb{R}) \to M_2(\mathbb{R})$ $$f : A \mapsto A^2 - I$$ The above mapping can be expressed as: $$f(a, b, c, d) = (a^2+b c - 1, a b+b d, a c+c d, b c+d^2 - 1)$$ I can calculate the derivative to be $$Df(a, b, c, d) = \left(
    \begin{array}{cccc}
    2 a & c & b & 0 \\
    b & a+d & 0 & b \\
    c & 0 & a+d & c \\
    0 & c & b & 2 d \\
    \end{array}
    \right)$$ However, there are some element in $S$ that is not a regular point. For example, $B = \begin{pmatrix} 0 &1 \\ 1 & 0 \end{pmatrix}$ with $f(B) = 0$ where the $Df(B) = \left(
\begin{array}{cccc}
 0 & 1 & 1 & 0 \\
 1 & 0 & 0 & 1 \\
 1 & 0 & 0 & 1 \\
 0 & 1 & 1 & 0 \\
\end{array}
\right)$ where the rank is 2. It seems that the Regular Value Theorem cannot be used to conclude that $S$ is a smooth regular submanifold. Is there a another way to do it? Any thoughts are appreciated!","['submanifold', 'smooth-manifolds', 'differential-geometry']"
4075605,Differentiating $x^{x^{x^{...}}}$,How do I differentiate $$ x^{x^{x^{...}}}$$ with respect to $x$ ? (Note that $x$ is raised infinitely many times.) My attempt: Let $y = x^{x^{x^{...}}}$ . Taking logarithm of both sides we get $\ln y = y \ln x$ and let $f = y \ln x - \ln y$ . Now $$\frac{dy}{dx} = -\frac{\frac{\partial f}{\partial x}}{\frac{\partial f}{\partial y}} = \frac{y^2}{x(1 - y \ln x)}$$ Is this approach correct? If not how do I proceed ? Help would be appreciated. Thanks.,"['calculus', 'derivatives', 'real-analysis']"
4075628,Prove $(AB)^{k} = A^{k}B^{k}$,"Apologies if this question was posed already. I need to prove that $(AB)^{k} = A^{k}B^{k}$ holds if $AB=BA$ . After trying it myself, I looked at the solution and found it quite strange. ""Use induction: $(AB)^{k} = A^{k}B^{k}$ is true for $k=1$ since since $AB=BA$ . Assume $(AB)^{k-1}=(BA)^{k-1}$ and prove it true for k: $$ (AB)^{k} = (AB)^{k-1} AB $$ $$  = A^{k-1}B^{k-1} AB $$ $$  = A^{k-1}A B^{k-1} B $$ $$  = A^{k} B^{k} $$ "" I don't understand how this really proves anything. We had to assume $(AB)^{k-1}=(BA)^{k-1}$ , how does if follow then that $(AB)^{k} = A^{k}B^{k}$ holds?","['matrices', 'exponentiation', 'linear-algebra']"
4075644,Finding all triples of numbers with the following divisibility,"Could somebody tell me how to tackle this problem? I don’t know where to start with it. The problem was part of the Math Olympiad in Kazhakstan (or Kyrgistan, I keep forgetting…) this February, but there are no solutions online. Find all triples $(x, y, z)$ of positive integers for which $$x|(y + 1)$$ $$y|(z + 1)$$ $$z|(x + 1)$$ holds.","['number-theory', 'divisibility', 'modular-arithmetic', 'elementary-number-theory']"
4075648,"For positive integers $x$, $y$, and positive real $k$, if $\frac{x+1}{y+k}=\frac{y}{x}$, then $k\geq1$","I saw this problem in the prep book for the USAMO in the Algebra section, but I don’t know how to tackle it. As the problems before were all solvable by factoring/Cauchy-Schwarz, I have tried finding a way to factor/rearrange this into a C.S.-form , but I have made no progress at all. As I’m getting quite frustrated with this problem I would appreciate all help! Let $x$ and $y$ be positive integers and $k$ be a positive real number for which $$\frac{x + 1}{y + k} = \frac{y}{x}$$ is satisfied. Prove that $k\geq1$ holds. My thoughts: After rearranging we arrive at $(x+1)x$ which must at least be 2. And $y(y+k)$ where $y$ is at least $1$ . So we have $$x^2+x=y^2+yk$$ and $$\frac{x^2+x-y^2}{y}=k$$ This obviously holds for $x>y$ , but for $y<x$ I am running into difficulties.","['algebra-precalculus', 'quadratics', 'a.m.-g.m.-inequality', 'inequality']"
4075680,Vector function going to zero faster than the norm of its variable uniformly in time,"I need to find some conditions on a function $h:\mathbb{R}^+\times \mathbb{R}^n\rightarrow\mathbb{R}^n$ to ensure that it goes, in norm, to zero faster than $\|z\|$ uniformly in time, i.e. $$ \lim_{\|z\|\to 0}\sup_{t\geq 0}\frac{\|h(t,z)\|}{\|z\|}=0.$$ I know that for any fixed time $t\in\mathbb{R}^+$ the limit goes to zero, but in general this convergence is not uniform in time. My function $h$ has even the property that $h(t,0)=0$ for any $t\in\mathbb{R}^+$ . Therefore the first property I thought for $h$ in order to satisfy this condition is that it is so that $\|h(t,x)\|\leq \|x\|^p$ where $p>1$ , which however does not give me relevant information about it. Do you suggest other possible conditions to ensure this kind of behaviour? If it might help, I need this condition in order to define the linearization of a non-autonomous dynamical system, where $h(t,z)$ is the nonlinear part of my ODE.","['limits', 'uniform-convergence', 'ordinary-differential-equations', 'dynamical-systems']"
4075693,What is another more direct proof of this exercise?,"The exercise states that "" If we know that $(A \cup B)-(A \cap B)=(A \cup C)-(A \cap C)$ , then $B=C$ "". At my first sight, I don't know how to do that. However, when I see another exercise in the later of the book, I find a way to work out. That exercise states that we can definite $+$ and $.$ , such that, $A+B:=(A \cup B)-(A \cap B), A.B=A \cap B$ . Then it let me prove that : $1.A+B=B+A$ $2.A+\emptyset=A$ $3.A+A=\emptyset$ $4.A.A=A$ $5.A+(B+C)=(A+B)+C$ These are very easy to prove. And from these property, I start my proof of the former exercise. I use property 3 to prove the former. Because now I can translate the $(A \cup B)-(A \cap B)=(A \cup C)-(A \cap C)$ to $A+B=A+C$ . By using property 3, $A+A+B=A+A+C$ , using property 5, I can conclude that $(A+A)+B=(A+A)+C$ , so by using property 3, $B=C$ . Though I work out the exercise finally, I think that there must be some simpler and more direct proof, since this exercise is situated before than those five properties. Can anyone help with a more direct and more intuitive proof of this?",['elementary-set-theory']
4075738,Differentiating a column with respect to a matrix,"Let $\mathbf{X} = [\mathbf{x}_1 | ... | \mathbf{x}_n]$ be a $m \times n$ matrix.
I would like to differentiate $\mathbf{x}_i = \mathbf{X} \mathbf{e}_i$ (where $\mathbf{e}_i \in \mathbb{R}^{n \times 1}$ is the unit vectors with $1$ on the $i$ th place and $0$ 's in the rest) with respect to $\mathbf{X}$ . Then $$
d\mathbf{x}_i = d(\mathbf{X}\mathbf{e}_i) = (\mathbf{X} + d\mathbf{X})\mathbf{e}_i - \mathbf{X}\mathbf{e}_i = (d\mathbf{X})\mathbf{e}_i
$$ and therefore $$
\frac{d\mathbf{x}_i}{d\mathbf{X}} = \mathbf{e}_i \in \mathbb{R}^{n \times 1}
$$ However, I suspect that is not consistent dimension-wise. For example: $f(\mathbf{X}) = \mathbf{a} \mathbf{x}_i$ where $\mathbf{a} \in \mathbb{R}^{1 \times m}$ then simply using the result above $$
\frac{d f(\mathbf{X})}{d\mathbf{X}} = \frac{d(\mathbf{a}\mathbf{x}_i)}{d\mathbf{X}} = \mathbf{a} \mathbf{e}_i \implies \mbox{Dimensions mismatch!}
$$ since $\mathbf{a} \in \mathbb{R}^{1 \times m}$ and $\mathbf{e}_i \in \mathbb{R}^{n \times 1}$ . How to fix this issue? An idea is to put a pseudo identity matrix $$
\frac{d\mathbf{x}_i}{d\mathbf{X}} = \mathbf{I}_{m \times n} \mathbf{e}_i \in \mathbb{R}^{n \times 1}
$$ such that $\mathbf{X} = \mathbf{X} \circ \mathbf{I}_{m \times n}$ with Hadamard product. But is this the right way to go?","['matrices', 'matrix-calculus', 'derivatives', 'tensors']"
4075808,"Degree, Angle, Period and maps onto the unit circle","This question is a follow-up from this question , whose accepted answer was to go and read up Allen Hatcher's book on Algebraic Topology, Chapter 1. I read it, but it only helps up to a point. The question is: suppose that $\Omega=D_2\setminus \overline{D_1}=\{(x_1,x_2) : 1< x_1^2+x_2^2 <4\}$ , and let $\phi$ be a smooth map from $\Omega$ to $S^1=\{(x_1,x_2) : 1= x_1^2+x_2^2\}$ . Can I write $$
\phi = \left(\cos(\theta),\sin(\theta)\right),
$$ for some function $\theta\in C^\infty(\Omega;\mathbb{R}/(2\pi k\mathbb{Z}))$ , for some $k$ possibly depending on $\phi$ ? And if not, what can I write? I am doing a bit of cargo cult mathematics here, because I think this is what I read means, but am certainly not confident and I don't master the machinery. If $\phi$ was a map from $S^1$ to $S^1$ , then I think this is what it would be, and $k$ would be the Brouwer degree of $\phi$ . And since $\Omega$ is $2$ dimensional but homotopic to the circle, it should be the same...but is that true? Going further, I can understand functions $ C^\infty(\Omega;\mathbb{R}/(2\pi\mathbb{Z}))$ . So is it the case that, in fact, degree has nothing to do with it and the answer is simply $$
\theta\in C^\infty(\Omega;\mathbb{R}/(2\pi \mathbb{Z}))
$$ (and Algebraic Topology says something simple, with no $k$ index subgroup involved..)?","['complex-analysis', 'algebraic-topology', 'analysis', 'partial-differential-equations']"
4075853,What does LSQR stand for,"One of the most popular and efficient iterative methods to solve large sparse systems of equations in the least squares sense is LSQR. It is related to CGLS (Conjugate Gradient Least Squares) in that it has the same iterates (mathematically, not numerically). But what does its name mean? is it L east S quares with QR factorisation? If so, why is it called that, does it provide Q and R? I suppose not because Q would not be sparse. L east SQ ua R es? L east SQ uares R egression? Something else? I have looked at the three references over at the SciPy documentation , but none of them seem to explain the naming. In the MATLAB documentation , the call it ""the Least Squares Method"", even stating it as ""... the least squares (LSQR) algorithm..."", suggesting that my second option is correct, but with no source.","['sparse-matrices', 'linear-algebra', 'terminology']"
4075864,Mumford representation of points on the Jacobian of (real) hyperelliptic curves,"I'm having trouble understanding why do we have a bijection between points on Jac(C) and divisors in Mumford representation, where C is a hyperelliptic curve. So I found here https://en.wikipedia.org/wiki/Imaginary_hyperelliptic_curve#Reduced_divisors_and_their_Mumford_representation that if we have an imaginary hyperelliptic curve there is a 1-1 correspondence between reduced divisors and divisors in Mumford representation. But what happens if we have a real hyperelliptic curve? I found here https://en.wikipedia.org/wiki/Real_hyperelliptic_curve under the ""Transformation from real hyperelliptic curve to imaginary hyperelliptic curve"" section that if we have a $K-$ rational point on a real hyperelliptic curve that we have a birational equivalence to an imaginary hyperelliptic curve. But what does that mean for us? That if we have a real hyperelliptic curve (with a $K-$ rational point) we can birationally transform it to an imaginary hyperelliptic curve and then go through the same reasoning? It the existence of a $K-$ rational point really enough or am I misunderstanding something? What happens if a real hyperelliptic curve has no $K-$ rational points? I am under the impression that we also have a bijection between points on Jac(C) and divisors in Mumford representation but I don't know why. Does my reasoning make sense?","['algebraic-curves', 'algebraic-number-theory', 'abelian-varieties', 'algebraic-geometry', 'abstract-algebra']"
4075886,Composition $f^n$ is a contraction $\implies f$ possesses a fix point.,"Definition. $x \in \mathbb{R}^n$ is called a fixpoint of the function $f:\mathbb{R}^n \to \mathbb{R}^n$ , if $f(x)=x$ . Notation. $f^n(x) := \underbrace{(f \circ f \circ \cdots \circ f)}_{\text{$n$ times}}(x)$ Definition. A function $f:\mathbb{R}^n \to \mathbb{R}^m$ is called a contraction, if there exists a positive constant $L<1$ such that for all $x,y \in \mathbb{R}^n$ the inequality $$\|f(x)-f(y)\|_{\mathbb{R}^m} \leq L \cdot \|x-y\|_{\mathbb{R}^n} $$ holds. Let $A \subseteq \mathbb{R}$ be a closed subset and $f : A \to A$ . Assume, there exists an $n \in \mathbb{N}$ , such that the $n$ -fold composition $f^n:A \to A$ is a contraction. Can it be shown, that there exists a fixpoint $x^* \in A$ for $f$ ?","['functions', 'fixed-point-theorems', 'real-analysis']"
4075906,First variation and Gâteaux derivatives,"I have two questions about the relation between Gâteaux and functional derivatives. In calculus of variation textbooks, the first variation is usually defined as follows. Let $V$ be a 'function space' and $f: V \to \mathbb{R}$ a functional. Then, the first variation of $f$ is defined by: \begin{eqnarray}
\delta f|_{y}(\eta) \equiv \frac{d}{dt}\bigg{|}_{t = 0}f(y+t\eta) := \lim_{t\to 0}\frac{f(y+t\eta)-f(y)}{t} \tag{1}\label{1}
\end{eqnarray} The above definition (\ref{1}) is just the usual definition of a Gâteaux derivative of a function $f$ . As far as I know, Gâteaux derivatives are defined, in its most generic way, on locally convex spaces . Question 1: If the Gâteaux derivative is defined on locally convex spaces and if the first derivative is defined as the Gâteaux derivative of a functional $f$ , why do most books not assume $V$ to be a locally convex space and, instead, use an imprecise ""function space""? At least for most purposes, aren't locally convex spaces enough to cover all possible function spaces used in this subject? For the second question, the physicist's notion of a functional derivative is very close to the notion of a first variation defined above. In the calculus of variations, we are usually aiming to study functionals which have the form of an integral, so what physicists call functional derivative is the kernel of the integral obtained after evaluating the first derivative (in the physics literature, this kernel is defined as taken the derivative in the direction of a Dirac delta distribution). Question 2: Assume that the function $f$ has an integral form. Is it correct to define the functional derivative as the function (if it exists) $\delta f/\delta y$ satisfying: \begin{eqnarray}
\frac{d}{dt}\bigg{|}_{t = 0}f(y+t\eta) = \int \frac{\delta f}{\delta y}(x)\eta(x) dx \tag{2}\label{2}
\end{eqnarray} or is there a better definition?","['variational-analysis', 'calculus-of-variations', 'gateaux-derivative', 'derivatives', 'mathematical-physics']"
4075907,Does my set have empty interior?,"I am trying to figure out if my set has empty interior or not. It is defined as follows. Let $(q_n)_{n \in \mathbb{N}} = \mathbb{Q}^2$ , we define $U = \bigcup_{n \in \mathbb{N}} B_{\frac{1}{2^n}}(q_n) \subset \mathbb{R}^2$ and $A = U^C$ . Now we fix $x \in \mathbb{R}\setminus \mathbb{Q}$ and define $C = A \cap (\{x\} \times \mathbb{R})$ . Then we can view $C$ as subset of $\mathbb{R}$ and I wonder if $C$ has any interior points in $\mathbb{R}$ . I mean I can always find a point $q_n \in U$ which is arbitrary close to $C$ in $\mathbb{R}^2$ . But the radius of its ball $\frac{1}{2^n}$ could be even smaller. I can't say if in every neighbourhood of a $y \in C$ is an intersection with a ball $B_{\frac{1}{2^n}}(q_n)$ . Does anyone have ideas how to tackle that?",['general-topology']
4075979,General solution for sound propagation in a semi-infinite pipe,"I need to find the velocity potential $\Phi$ defined by $\vec{u}=\nabla\Phi$ in the domain $D=\{(x,y,z) : x^2+y^2\leq R^2, z\geq0\}$ . We are considering sound propagation so $\Phi$ satisfies the wave equation $\partial_{tt}\Phi=c^2\Delta\Phi$ . I am told that I can separate variables into $\Phi(x,y,z,t)=R(r)e^{in\theta}\psi(z,t)$ . Here is what I have done so far If I put this into the wave equation, after a few lines of algebra I end up with $R''+\frac{1}{r}R'+R(\frac{\partial_{zz}\psi}{\psi}-\frac{n^2}{r^2}-\frac{1}{c^2}\frac{\partial_{tt}\psi}{\psi})=0 \implies \frac{1}{R}\left(R''+\frac{1}{r}R'\right)-\frac{n^2}{r^2}=-\left(\frac{\partial_{zz}\psi}{\psi}-\frac{1}{c^2}\frac{\partial_{tt}\psi}{\psi}\right)$ The left side is dependent on $r$ , the right side is dependent on $z,t$ , so I can set both sides to be equal to a constant $C$ . I can deal with the right side $\frac{\partial_{zz}\psi}{\psi}-\frac{1}{c^2}\frac{\partial_{tt}\psi}{\psi}=C$ by separation of variables once again, say $\psi(z,t)=A(z)B(t)$ . I can show $\frac{A''(z)}{A(z)}-\frac{1}{c^2}\frac{B''(t)}{B(t)}=C \implies \frac{A''(z)}{A(z)}=\frac{1}{c^2}\frac{B''(t)}{B(t)}+C$ . Again I can play the same game, the left side is dependent on $z$ , the right side on $t$ , so both sides are equal to a constant, call it $k_z$ . Both are simple second order ODEs to solve, $A(z)=C_1e^{-ik_zz}+C_2e^{ik_zz}$ $B(t)=C_3e^{-i\omega t}+C_4e^{i\omega t}$ Where $\omega, k_z$ and the $C_i$ are constants. Here is where I get a bit lost For this semi infinite pipe the boundary conditions would be (I think) $u_z=0$ at $z=0$ , $u_r=0$ for $r=R$ . I also think there might be a pressure condition at the open end at $\infty$ , say $p=0$ at $z\rightarrow\infty$ , but I am not sure. The $u_z$ condition tells me that the $z$ constants have to be equal, that is $C_1=C_2=D$ . I am not sure how to eliminate any of the other constants here though, since the initial conditions are confusing me. I am also confused about the R solution If I sub $\psi$ back into the equation $R''+\frac{1}{r}R'+R(\frac{\partial_{zz}\psi}{\psi}-\frac{n^2}{r^2}-\frac{1}{c^2}\frac{\partial_{tt}\psi}{\psi})=0$ which I derived at the start, I get $R''+\frac{1}{r}R'+R\left(-k_z^2-\frac{n^2}{r^2}+\frac{\omega^2}{c^2}\right)=0$ which looks sort of like a Bessel ODE, but I am not sure what the correct transformation is to get it into the Bessel form. I think from here I could get the general solution, but I am confused about (1) the initial/boundary conditions and (2) the Bessel ODE here.","['ordinary-differential-equations', 'bessel-functions', 'partial-differential-equations', 'mathematical-physics', 'fluid-dynamics']"
4076074,"Synthetic proof that, given points $A,B,C,D$ in space, $\angle BAC \leq \angle BAD+\angle DAC$?","Consider the following statement: Given points $A,B,C,D$ in space, $\angle BAC \leq \angle BAD+\angle DAC$ . This seems obvious enough -- if you're rotating a beam that moves through space at a fixed angular speed, the fastest way to go from pointing at $B$ to at $C$ is directly, as opposed to through some other point $D$ . This is also equivalent to the triangle inequality on a sphere (where the distance between two points on a sphere is defined to be the length of the arc), if we specify $AB=AC=AD=1$ . However, my attempts to find a simple geometric proof have been elusive. It's certainly possible to prove the spherical triangle inequality via the spherical law of cosines , analogously to the proof of the (Euclidean) triangle inequality using the standard law of cosines, but proving this law seems to require assigning coordinates somehow. The calculations aren't that bad, but they seem considerably hairier than such a simple statement should require. Is there a strictly geometric (synthetic) proof of this result?","['alternative-proof', 'spherical-trigonometry', 'geometry']"
4076154,How do angles of two linked rotatable rods change in dependence on their corresponding lengths (of their ratio $\frac{r_1}{r_2}$)?,"Suppose a system of four rods with known dimensions $r_1, r_2, a, l$ is given (see picture). The black rod between the points $A_1$ and $A_2$ is fixed in space, whereas the linkages at points $A_1, A_2, B_1, B_2$ can rotate.
How can the change of the fist angle $\alpha_1$ be described in terms of $\alpha_2$ ? Is it possible to set up an ordinary differential equation? How would a plot of $\alpha_1$ and $\alpha_2$ look like? I have animated the movement in GeoGebra, but I'm still interested in the maths behind this: This is particularly interesting in regards of a mechanism I am currently designing, I'd be thankful for any advice!","['classical-mechanics', 'trigonometry', 'geometry']"
4076181,Help solving Boundary Condition Ordinary Differential Equation with absolute value.,"Good day everybody.
I'm a physics student enrolled in a scientific computation course and my teacher sent us a bunch of ODEs to solve both analitically and numerically. I haven't really studied boundary value problems so maybe I need to be pointed to a resource for that but right now I'm but having trouble with this one in particular: $$u'' + 9u = 36(5- |x|)^2$$ with the boundary conditions $u(-5) = 0$ and $u(5) = 0$ . My attempt at solving it goes like this: I expand and separate the equation in its two domains $$u""+ 9u = 36x^2 + 360x + 900, \ x < 0$$ $$u""+ 9u = 36x^2 -  360x + 900, \ x \ge 0$$ Now I have two equations so I believe I need two homogeneous solutions, two particular solutions and four conditions. Therefore I solve for to the homogeneous part: $$U_h(x) = A\cos(3x) + B\sin(3x), \ x<0$$ $$U_h(x) = C\cos(3x) + D\sin(3x), \ x\ge 0$$ I resort to undetermined coefficients looking for particular solutions: $$U_p(x) = 4x^2+40x + 892/9, \ x < 0$$ $$U_p(x) = 4x^2-40x + 892/9, \ x \ge 0$$ I add $U_h$ and $U_p$ to make two general solutions: $$U(x) = A\cos(3x) + B\sin(3x) + 4x^2+40x + \frac{892}{9}, \ x<0 $$ $$U(x) = C\cos(3x) + D\sin(3x) + 4x^2-40x + \frac{892}{9}, \ x\ge0 $$ This is where I notice I have only two boundary conditions, not four. So, there is that. Should I merge both solutions back togheter into one? or maybe two conditions are enough and that's what I'm not seeing. Anyway, I hope you guys can help.","['boundary-value-problem', 'absolute-value', 'ordinary-differential-equations']"
4076237,What Does the Antiderivative of This Multivariable Function Mean?,"In this video, the instructor uses Undetermined Coefficients (or at least what would be called Undetermined Coefficients in the ODE world) to find a particular solution to the PDE $U_t + cU_x = f(x + ct)$ , where $c$ is a parameter present in the original Wave Equation. The guess used is $U = aF(x + ct)$ , where $a$ is the constant to be determined, and $F$ is said to be the antiderivative of $f$ .  However, I don't recall from MV Calc any single notion of an antiderivative of a multivariable function, only antiderivatives taken with respect to one of the function's inputs.  The closest thing I can think of would be a potential function, where one begins with a vector field and pieces together a single function whose gradient is that vector field, but I don't think that matches what's going on here. Later in the problem, he seems to take $\frac{\partial F}{\partial x}$ and $\frac{\partial F}{\partial t}$ separately, in both cases yielding $f$ , but it seems to me that baring certain exceptional cases, it is not possible to find a function whose partial with respect to each of its variables yield a single, given function.  What is going on here?","['integration', 'multivariable-calculus', 'wave-equation', 'partial-differential-equations', 'partial-derivative']"
