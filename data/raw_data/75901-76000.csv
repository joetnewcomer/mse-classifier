question_id,title,body,tags
949991,Is a multivariable function continuous iff it is continuous with respect to each variable?,"I am very uncertain when it comes to understanding the continuity of multivariable functions. If we have, for example, a function $f: \mathbb{R}^{4} \to \mathbb{R}$ , and we denote the four variables $x,y,z,w$ , are the following statements equivalent? i) $f$ is continuous ii) $f|_{x}, f|_{y}, f|_{z}, f|_{w}$ are each continuous Here, $f|_{x}$ stands for the function attained by fixing the variables $w,y,z$ .  I hope this question makes sense.","['multivariable-calculus', 'continuity', 'real-analysis']"
949997,When Is A Quot Scheme Reduced?,"For my research, I would like to know whether a certain Quot scheme is reduced. Reading the thread How To Tell Whether A Scheme Is Reduced From Its Functor , I was disappointed to find that there's no easy way to read this information off the functor in general. Instead, how about this: are there standard situations in which it is known and proven that a Quot scheme is reduced? If $E$ is a vector bundle on a $k$-scheme $X$, and $Quot_{E/X}^{P,L}$ is the Quot scheme of quotients $Q$ of $E$ such that $Q$ has Hilbert polynomial $P$ with the respect to the line bundle $L$, then what are some restrictions on the data {$E,X,P,L$} that would allow one to conclude reducedness? For instance, reducedness holds for the Grassmannian: $X=Spec(k)$, $E=O_{k}^r$, $L=O_k$, $P=d<r$, although obviously I would like something a bit less narrow. Perhaps a better question of more general interest: what are some stronger, sufficient conditions from which reducedness follows? These may be easier to check on the functor level.",['algebraic-geometry']
949998,Calculus integral evaluation using substitution,"I have to find this integral: Evaluate the integral using an appropriate substitution $$\int\dfrac{8e^x+7e^{-x}}{8e^x-7e^{-x}}\mathrm dx.$$ I've tried my solution $\ln\Big[15\cdot \sinh(x) + \cosh(x)\Big]$, however, it is wrong. What do I have to do?","['logarithms', 'calculus', 'integration', 'derivatives']"
950045,$2\cos(x)+x=0$? Advanced trig. question.,"Title says it all:
$$2\cos(\theta)+(\theta)=0$$
the interval should be between $0$ to $2\pi$. Been trying to figure this out for quite a while, still no luck.
I'm trying to find if the solution exists or not.",['trigonometry']
950065,Analyzing the series $\sum_{n \geq 2} \frac{1}{n^p \ln^qn}$.,"Consider the series $$\sum_{n \geq 2} \frac{1}{n^p \ln^qn}$$ Prove that: The series converges if $p > 1$ (and any $q$ ), or if $p = 1$ and $q > 1$ . The series diverges if $p < 1$ (and any $q$ ), or if $p = 1$ and $q \leq 1$ . Immediately I know that we must have $p > 0$ , otherwise the general term won't go to zero. The ratio test fails, since if we call $x_n = \frac{1}{n^p \ln^qn}$ , we get that: $$\frac{x_{n+1}}{x_n} = \frac{n^p \ln^q n}{(n+1)^p \ln^q(n+1)} = \left(\frac{n}{n+1}\right)^p \left(\frac{\ln n}{\ln(n+1)}\right)^q \to 1 \cdot 1 = 1$$ Since $n^{p/n} = e^{(p \ln n)/n}$ and $\ln^{q/n}n = e^{(q \ln \ln n)/n}$ , the root test also fails: $$\sqrt[n]{|x_n|} = \sqrt[n]{\frac{1}{n^p \ln^qn}} = \frac{1}{n^{p/n} \ln^{q/n}n} \to \frac{1}{1 \cdot 1} = 1$$ so, no good. I hardly think that the integral test will help here. I thought the idea was to get some condition on $p$ and $q$ by using the above tests. Then, I'm left with comparing it with $1/n^2$ or something like it, but I'm a little lost about how to go about it. Can someone give me a hand? Thanks.","['sequences-and-series', 'real-analysis']"
950082,Difficult differential equation,"So, someone challenged me to solve a differential equation, and this would be unorthodox, but MSE I need your help. It turned out impossible to solve by the methods I know. I would prefer hints over solutions. $$\frac{\mathrm{d}^2 y}{\mathrm{d}x^2}+\frac{\cos x}{\sin x} \frac{\mathrm{d} y}{\mathrm{d}x} +k(k+1)y=0$$",['ordinary-differential-equations']
950097,"Limit of $\frac{x^2-y^2}{x^2+y^2}\sin(x-3y)$ when $(x,y) \to (0,0)$","Show that $$\lim_{(x,y)\to (0,0)}\frac{x^2-y^2}{x^2+y^2}\sin(x-3y)$$ Does not exists I've tried the traditional patches, but I always find zero as answer. Any hint? Thanks in advance!","['multivariable-calculus', 'limits']"
950115,How can we show that $ \sum_{n=1}^{\infty} \frac{n}{2^n} = 2 $? [duplicate],"This question already has answers here : How can I evaluate $\sum_{n=0}^\infty(n+1)x^n$? (24 answers) Closed 9 years ago . How can we prove the following? $$ \sum_{n=1}^{\infty} \frac{n}{2^n} = 2 $$ It would be great to see multiple ways, or hints, about how this can be proven. I know this is a power series $\sum_{n=0}^\infty c_n \, (x-a)^n$ where $c_n = n$ and $x = \frac{1}{2}$ and $a = 0$, but otherwise I am stuck.","['power-series', 'sequences-and-series', 'calculus', 'analysis', 'limits']"
950141,Number of Fibonacci series that contain a certain integer,"In my question, I consider general Fibonacci sequences (sequences satisfying the recurrence relation $F_{n+2}=F_{n+1}+F_n$ independent of their starting value). Given two arbitrary different integers, the second being greater than the first, one can reverse the above equation to determine the lowest possible starting values of a Fibonacci series containing those two numbers. Let's call them elementary tuple of a certain Fibonacci series. Then a certain Fibonacci series starting with an elementary tuple ( elementary Fibonacci series ) is uniquely characterized by two different integers. Now, can we calculate or estimate the number of elementary Fibonacci series that contain a certain integer $n$ where $n$ is not in the elementary tuple (otherwise the number would be infinite)? Is the question easier if we consider all Fibonacci series and not only the elementary ones? PS: I tagged it under combinatorics since I expect the solution to come from there. Naturally, I don't know , so please delete it if appropriate.","['fibonacci-numbers', 'sequences-and-series', 'combinatorics']"
950166,Problem on matrices : $\dim E\leq n^2-(n-r)^2-1$,"I have the following problem : Let $E$ be a subspace of $M_n(\mathbb{R})$ that contains no invertible matrix. Let $r=\max\{rank(M)\mid M\in E\}$ Show that $\dim E\leq n^2-(n-r)^2-1$ I don't know how to do this. We have obviously $\dim E=n^2$, and I guess that he $(n-r)^2$ is due to the rank of the space with invertible matrixes, but I'm kind of lost here.","['matrices', 'linear-algebra']"
950168,Examples where the product of reciprocal derivatives isn't one?,"I'm trying to understand why it isn't a good idea to treat derivatives like fractions. Could someone give me an example of a function $y$ such that $$\frac{dy}{dx} \cdot \frac{dx}{dy} \not = 1$$ This inspired my question, so I would appreciate it if someone could address it: Conside a function
 $PV = kT$ for some constant $k$. Then $\dfrac{\partial P}{\partial V}\dfrac{\partial T}{\partial P}\dfrac{\partial V}{\partial T} = -1$ instead of $1$ as you would have expected if it's valid to treat derivatives ""intuitively"" as fractions.",['calculus']
950189,Geometry and land,"The word ""geometry"" in Greek means ""measurement of Earth/land"". This may imply that geometry was originally invented in order to solve problems related to land. Are there historical accounts of concrete problems related to land, either from ancient Greece or elsewhere, whose solution required the development of geometric concepts or theorems?","['geometry', 'math-history']"
950194,Determining Measurability given property of symmetric difference,"I came across this statement and I am not sure why it is true. If $\mu$ is sub-additive and $\mu(E\bigtriangleup F)$ is in the null set (where $E \bigtriangleup F$ denotes the symmetric difference of $E$ and $F$), and $F$ is $\mu$-measurable, then $ E$ is necessarily $\mu$-measurable as well. Any ideas on how the proof would go would be appreciated.",['measure-theory']
950273,Are any (non-empty) Euclidean open sets dense in the Zariski topology?,"It's well known and easy to show that every Zariski open set is dense in the Zariski topology. However I search the web and didn't find an answer to my question, which I believe is true. My argument is not strict but only intuitive: given an Euclidean open set $U\subset \mathbb{A}^n$, a point $p\in \mathbb{A}^n \setminus U$, and any Zariski open set $\mathbb{A}^n\setminus V$ containing $p$. $V$ is a sub-variety, having no interior point( neither in the Euclidean topology nor the Zariski topology), so $V$ cannot cover an Euclidean open set like $U$, which implies $(\mathbb{A}^n\setminus V) \cap U\ne\emptyset$. Q.E.D. It there any problem of the above argument? How can I prove that a sub-variety doesn't have interior points in both topology?","['general-topology', 'algebraic-geometry']"
950282,Indefinite integral with partial fractions,"$$\int{ -5x^3-2x^2+32\over x^4-4x^3 } dx $$ How should I solve this indefinite integral using partial fractions?
I have already checked the online calculators but the answer they give me is incorrect whenever I check it. I have used partial fractions of type ${A\over x}+{B\over x^2} + {C\over x^3} + {D\over (x-4)}.$ The answer I get when I solve the problem is ${2(x-2) \over x^2 }-5\ln(x-4) + C.$ However, the answer shows as incorrect when I input it.","['calculus', 'algebra-precalculus', 'indefinite-integrals', 'partial-fractions']"
950286,Is the identification between symmetric tensors and homogeneous polynomials useful?,"The general question: Given an $n$-dimensional vector space $V$ over a field $k$, there exists an identification 
$$\mathrm{Sym}^d(V) \sim k[x_1, \dots, x_n]_d$$
between the space of symmetric order $d$ tensors on $V$ and the space of order $d$ homogeneous polynomials in $n$ indeterminates over $k$. I am wondering whether this identification is ever of much use in the study of tensors and tensor fields. In particular, I am interested in ways in which this identification might simplify problems in differential geometry, but uses in other fields would be interesting also. A potential example of what I'm asking about: This question is inspired by the following observation I made, which I would also like to confirm is valid: Let $\omega \in \mathrm{Sym}^1(V)$ be some unknown. Suppose we have a map
$$\phi \colon \mathrm{Sym}^1(V) \longrightarrow k$$
along with a few known elements $\eta, \theta, \nu_1, \nu_2 \in \mathrm{Sym}^1(V)$. Additionally, we have the following system of equations:
$$
\begin{align}
\phi(\omega) \cdot \eta + \phi(\eta) \cdot \omega &= \nu_1 \\
\phi(\omega) \cdot \theta + \phi(\theta) \cdot \omega &= \nu_2 
\end{align}
$$
Our goal is to solve for $\omega$. In light of the above, we can identify $\mathrm{Sym}^1(V)$ with $k[x_1, \dots, x_n]_1 \subset k[x_1, \dots, x_n]$, form the field of fractions $k(x_1, \dots, x_n)$ and solve the above system for $\omega$ using linear algebra. Doing so, we will arrive at an expression of the form:
$$
\left(\phi(\theta)\eta - \phi(\eta)\theta\right)\omega = \theta \cdot \nu_1 + \eta \cdot \nu_2
$$
where all the elements are now considered to be in $k(x_1, \dots, x_n)$, so all the products make sense. Dividing, we can obtain an expression for $\omega$ expressed entirely in terms of objects we know. We can now evaluate this expression on the appropriate vectors to obtain an expression for $\omega$ in terms of a basis for $\mathrm{Sym}^1(V)$.","['differential-geometry', 'abstract-algebra', 'polynomials', 'tensors', 'soft-question']"
950290,Help on the relationship of a basis and a dual basis,"If $B_1 = \{v_1,\ldots,v_n\}$ and $B_2 = \{v_1',\ldots,v_n'\}$ are bases for a vector space $V$ , and $D_1= \{\delta v_1,\ldots, \delta v_n\}$ and $D_2 = \{\delta v_1',\ldots, \delta v_n'\}$ are the corresponding dual bases of $V^*$ prove that, if $P$ is the change-of-basis matrix from $B_1$ to $B_2$ , then $(P^{-1})^T$ is the change-of-basis from $D_1$ to $D_2$ (this is a theorem from Schaum's outline of theory and problems of linear algebra) My knowledge of linear algebra is not great, but so far I have that \begin{align}
\text{For any }v \in V, [v]_{B_1}&=P[v]_{B_2}\\
\text{so } (\alpha_1,\ldots,\alpha_n)(v_1,\ldots,v_n)^T&=P(\beta_1,\ldots,\beta_n)(v_1',\ldots,v_n')^T
\end{align} for some $\alpha_1,\ldots,\alpha_n,\beta_1,\ldots,\beta_n \in F$ (where $F$ is a field) How do I use the fact that $D_1$ , $D_2$ are dual basis to further this proof? And also, where does the transpose of $P^{-1}$ come in? Can I use that $D_1 = \delta B_1$ and $D_2 = \delta B_2$ (where I am guessing $\delta$ is the Kronecker delta?) Any hints or suggestions are greatly appreciated, I know that my knowledge is quite limited with regard to this, links to text or sources that could explain a problem like this would be great.",['linear-algebra']
950313,Question on subgroups of reductive groups,"A linear algebraic group $G$ over some field $k$, which I assume being of characteristic 0, is reductive if $R_u(G^0_{\overline{k}})$ is trivial, where $R_u$ denotes the unipotent radical, $G^0$ is the connected component of the identity, and $G_{\overline{k}}= G \times_k \overline{k}$.
My question: Is any subgroup of a reductive group reductive? My thoughts: let $H \leq G$. Then $H^0 \leq G^0$. If $R_u(H^0_{\overline{k}})\leq R_u(G^0_{\overline{k}})$, then I'm done, but I don't know if this holds.","['algebraic-groups', 'group-theory']"
950387,Set of limit points of topologist's sine curve $S$,"Let $S=\{(x,\sin(1/x)):x \in (0,1]\}$ be the topologist's sine curve. Find the limit points $\lim S$ of $S$. I claim $\lim S = S \cup \{(0,y):y \in [-1,1]\}$. But, how do you show that any of these points is such? Certainly there is some nice lemma for this? All I can think to do is revert to analysis by showing that, for $s=(x,\sin(1/x)) \in S$, and any open ball $B=B(z,\epsilon)$ which contains $s$, there is a point $s'=(x',\sin(1/x')) \in S$ with $s' \in B$, but I'm having trouble finding an $x'$...",['general-topology']
950390,Matrix question: implication of $\frac{1}{n}X'X\to M$,"Suppose $K$ is fixed and consider a matrix $X$ that is $n\times K$ and has full column rank. Assume that we know
$$
\frac{1}{n}X'X\to M\text{ as } n\to\infty.\tag{i}
$$
That is, as $n$ becomes larger, we add more rows to $X$ in such a way that $X$ still has full column rank and (i) holds. It is given that $M$ is a fixed positive definite matrix with all of its eigenvalues in $(b,B)$ with some $0<b<B$. Now, let $X_i$ be the $K\times 1$ vector such that $X'_i$ is the $i$-th row of $X$. Does it follow that $$ \lim_{n\to\infty}\frac{1}{n}\max_{1\leq i\leq n}|X_i|^2=0?\tag{ii} $$ I tried looking at
$$
\frac{1}{n}X'X=\frac{1}{n}\begin{pmatrix}X_1&\cdots& X_n\end{pmatrix}\begin{pmatrix}X'_1\\\cdots\\X'_n\end{pmatrix}=\frac{1}{n}\sum_{i=1}^n X_iX'_i\to M.\tag{iii}
$$
It seems intuitive that each summand in (iii) has to be ""small"" and perhaps that will eventually leads me to (ii) but I can't produce a decent argument. Can someone help please? Edit : I also looked at
$$
\text{Tr}\left(\frac{1}{n}X'X\right)=\frac{1}{n}\sum_i\text{Tr}(X_iX_i')=\frac{1}{n}\sum_i |X_i|^2\to\text{Tr}(M)\tag{iv}
$$
but I reached a deadend here too.","['economics', 'matrices', 'linear-algebra', 'limits']"
950398,"Evaluate $\int_{0}^{\frac {\pi}{3}}x\log(2\sin\frac {x}{2})\,dx$","Prove that $$\int_0^{\pi/3}x\log \left(2 \sin\frac {x}{2}\right)\,dx = \frac {2\zeta(3)}{3}-\frac {\pi^2}{9}\log (2\pi)+\frac {2\pi ^2}{3}\log \left|\frac {\Gamma_2 \left(\frac {5}{6}\right)}{\Gamma_2 \left(\frac {7}{6}\right)}\right|$$ ( Editor's note, July 2019 : According to Adamchik's paper "" On the Barnes Function "" (p. 4), the notation $\Gamma_n(z)$ follows Vigneras and Vardi and is the reciprocal of the multiple gamma function $G_n$ , $$\Gamma_n(z) = \frac{1}{G_n(z)}$$ with the special case, $$\Gamma_2(z) = \frac{1}{G_2(z)} = \frac{1}{G(z)}$$ where $G(z) = G_2(z)$ is the double Gamma function aka Barnes G function.","['improper-integrals', 'calculus', 'integration', 'special-functions', 'definite-integrals']"
950414,Show $|\sin(y)y - \sin(x)x| \leq C|y - x|$ for some $C > 0$,"Show $|\sin(y)y - \sin(x)x| \leq C|y - x|$ for some $C > 0$. This is one of the steps in a bigger problem I'm trying to solve, and while it first appeared it would be entirely straightforward, I got stuck on this part. It seems to me this is true and that such a constant $C>0$ exists, but I see no way of showing it. In fact, I figured it'd be $C = 2$, but again, I'm missing the argument. It this constant does not exist, then I'll have to rethink the problem entirely, so I'd be grateful for any help.","['trigonometry', 'inequality', 'lipschitz-functions', 'analysis']"
950418,Is it true that $\left\lfloor\sum_{s=1}^n\operatorname{Li}_s\left(\frac 1k \right)\right\rfloor\stackrel{?}{=}\left\lfloor\frac nk \right\rfloor$,"While studying polylogarithms I observed the following. Let $n>0$ and $k>1$ be integers. Is the following statement true? $$\left\lfloor \sum_{s=1}^n \operatorname{Li}_s\left( \frac{1}{k} \right) \right\rfloor \stackrel{?}{=} \left\lfloor \frac{n}{k} \right\rfloor $$ If it is, then how could we prove it? If not, give a counterexample.","['polylogarithm', 'summation', 'calculus', 'ceiling-and-floor-functions']"
950462,Integrate by partial fraction decomposition,"$$\int\frac{5x^2+9x+16}{(x+1)(x^2+2x+5)}dx$$
Here's what I have so far...
$$\frac{5x^2+9x+16}{(x+1)(x^2+2x+5)} = \frac{\mathrm A}{x+1}+\frac{\mathrm Bx+\mathrm C}{x^2+2x+5}\\$$
$$5x^2 + 9x + 16 = \mathrm A(x^2+2x+5) + (\mathrm Bx+\mathrm C)(x+1)=\\$$
$$\mathrm A(x^2+2x+5) + \mathrm B(x^2+x)+\mathrm C(x+1)=\\$$
$$(\mathrm A+\mathrm B)x^2 + (2\mathrm A + \mathrm B + \mathrm C)x + (5\mathrm A+\mathrm C)\\$$
$$\mathrm A=-3,\;\mathrm B=8,\;\mathrm C = 31$$
$$$$
$$\int\frac{5x^2+9x+16}{(x+1)(x^2+2x+5)}dx = \int\bigg(-\frac{3}{x+1}+\frac{8x+31}{x^2+2x+5}\bigg)dx\Rightarrow$$
$$\int-\frac{3}{x+1}dx +\int\frac{8}{x^2+2x+5}dx+\int\frac{31}{x^2+2x+5}dx $$ Hopefully I've got it correct until this point (if not, someone point it out please!). I can do the first integration by moving the -3 out and using $u=x+1$ to get 
$$-3 \ln(x+1)$$
but I'm stuck on the next two.","['calculus', 'integration']"
950496,Am I going about this the right way?,"Show that for any $α ∈ R$, there exist infinitely many rational numbers $\frac{m}{n}$
with $|α − \frac{m}{n^2}| < \frac{1}{n}$. So we know that $-1≤\frac{1}{n}≤1$ which implies $\frac{1}{n^2}≤1$. Case $1$: if $m=n$ then $\frac{m}{n^2} = \frac{1}{n}$ so obviously we get $|α − \frac{m}{n^2}| < \frac{1}{n}$. Case 2: if $m<
n$ that implies $m<
n^2$ which implies $\frac{m}{n^2}<1$ and if $n<
n^2$ then $\frac{m}{n^2}<\frac{1}{n}$ so again $|α − \frac{m}{n^2}
| < \frac{1}{n}$ makes sense. I'm having trouble seeing how $m > n$ would come up with the same conclusion. (Am I going about this proof the right way?)",['analysis']
950540,Cauchy sequences. Show that $(x_n)$ is Cauchy.,"Let $(x_n)$ and $(y_n)$ be sequences such that $\lim y_n = 0$. Suppose that for all $k \in  \Bbb N$ and all $m ≥ k$ we have
$|x_m − x_k| ≤ y_k$.
Show that $(x_n)$ is Cauchy. I need a little guidance on how to approach the problem.  As I see this is the same definition of Cauchy sequences.   But I do not see how to connect everything in a logic sequence in order to have a rigorous proof. My attempt of reasoning I started first defining the $\lim$ of $y_n$. For every $\varepsilon>0$ exists $N$ s.t. $n>N$   $|y_n|<\varepsilon$ for all $n>N$
Then I see that all terms of $y_n$ get smaller and smaller as $n$ gets larger. 
So distance between $x_m$ and $x_k$ gets smaller as the terms get bigger. But one thing that puts me off is that 
$m ≥ k$ and $| x_m − x_k| ≤ y_k$ why are they $\leq$? Thanks for help in advance'","['cauchy-sequences', 'real-analysis']"
950603,composition sum of functions/sum of composition of functions,"I know it sounds really dumb, but is it true that $(f_1+f_2)\circ g=f_1\circ g+f_2\circ g$? I know it must be really elementary, but I don't recall seeing this being proved (or defined) explicitly.","['linear-algebra', 'functions']"
950610,Normal Distribution and Iterated Logarithm,"Let $X_n$ be independent $N(0, \sigma^2)$-distributed random variables with partial sum $S_n := \sum_{k=1}^n X_k$, $n \geq 1$. Then I read the following results. $$
\sum_{k = 1}^n \mathbb P (S_n > \epsilon\sqrt{n\log\log n}) \sim \sum_{k = 1}^n \frac{\sigma}{\epsilon\sqrt{\log\log n}} (\log n)^{-\frac{\epsilon^2}{2\sigma^2}} = \infty,
$$ for any $\epsilon > 0$. I do not know how these results are derived. To be specific, How to get the first approximation, please? How to know the second series does not converge, please? I tried to apply ratio test, but I got the limit is 1 which is not helpful. Could anyone help me, please? Thank you! Update of the first question: By the answer provided below, one has $$\mathbb P(S_n > \epsilon\sqrt{n\log\log n}) = \mathbb P\left(Z > \frac{\epsilon}{\sigma} \sqrt{\log\log n}\right) \leq \frac{1}{\sqrt{2\pi}}\frac{\sigma}{\epsilon\sqrt{\log\log n}} (\log n)^{-\frac{\epsilon^2}{2\sigma^2}}.$$ Then how to get $$\lim_{n \to \infty} \frac{\mathbb P\left(Z > \frac{\epsilon}{\sigma} \sqrt{\log\log n}\right)}{\frac{\sigma}{\epsilon\sqrt{\log\log n}} (\log n)^{-\frac{\epsilon^2}{2\sigma^2}}} = 1,$$ please? Thanks.","['self-learning', 'probability-theory', 'real-analysis', 'normal-distribution', 'probability']"
950616,Why does elementary row operation on combination of Identity matrix and invertible matrix yields inverse matrix?,"Why does elementary row operation on combination of Identity matrix and invertible matrix yields inverse matrix? $$A = \begin{pmatrix}
1 & 3\\ 
4 & 2
\end{pmatrix}$$ if we combine $A$ with $I_{2\times 2}$ in which $I$ is an identity matrix, and do elementary row operation on it to get identity matrix on the other side, the opposite side(block) yields out the inverse of the matrix. The question is, why is it true?","['matrices', 'linear-algebra', 'block-matrices']"
950642,"Show that $\gcd(a,b)>1$","Given are three natural numbers $a$, $b$ and $c$, for which $$\frac1a+\frac1b=\frac1c,$$ show that $\gcd(a,b)>1$. Could you someone provide a hint? I already tried algebraic manipulation, but I just can't find a way to prove it...","['algebra-precalculus', 'divisibility', 'number-theory', 'elementary-number-theory', 'integers']"
950651,"Finding the general solution to system of linear equations: $y' = 2y,y''=4y-y'$","Question: I want to find the general solution to the following system:
  $\begin{pmatrix} \dot{y}_1 \\ \dot{y}_2 \end{pmatrix} = \begin{pmatrix} 2&0\\4&-1 \end{pmatrix}\begin{pmatrix}y_1\\y_2\end{pmatrix}$ I find working in questions makes them look cluttered and noone helps. They are hidden below. I believe I am meant to find the eigenvalues and eigenvectors here. Since we have a zero on the opposite diagonal, we know that the eigenvalues are $2$ and $-1$, and the eigenvectors corresponding to these turned out to be $(3,4)^T$ and $(0,\gamma)^T$. cont. I am fairly sure I am meant to use the form $\vec{y}= \alpha \vec{x}^{(1)} e^{\lambda_1 t} + \beta \vec{x}^{(2)} e^{\lambda_2 t}$ Attempt solution Hence it should equal $\alpha \begin{pmatrix} 3 \\ 4 \end{pmatrix} e^{2t}+  \beta \begin{pmatrix} 0 \\ \gamma \end{pmatrix} e^{-t} $, but I am not sure this is right.","['ordinary-differential-equations', 'real-analysis']"
950653,Show that $\prod (1- P(A_n))=0$ iff $\sum P(A_n) = \infty$,Let $A_n$ be independent events with $P(A_n) \neq 1$. Show that $\prod_{n=1}^{\infty} (1- P(A_n))=0$ iff $\sum P(A_n) = \infty$ It kind of looks obvious but I really have no idea how to prove it. Can someone give me help?,"['probability-theory', 'infinite-product', 'probability']"
950671,"In what sense $\alpha \times \alpha$ is the initial segment generated by $(0,\alpha)$ in $Ord \times Ord$?","This is from Jech's book on set theory: We define a well ordering of the class $Ord \times Ord$ of ordinal pairs.
  Under this well ordering, each $\alpha \times \alpha$ is an initial segment of $Ord^2$. Moreover, the well ordered class $Ord^2$ is isomorphic to the class $Ord$ and we have a one-to-one function $\Gamma$ of $Ord^2$ onto $Ord$: We define \begin{align}(\alpha,\beta) < (\gamma,\delta)  \iff&  
\max\{ \alpha, \beta \} < \max\{\gamma,\delta\},\\ &\text{or }
\max\{ \alpha, \beta \} < \max\{\gamma,\delta\}\text{ and }\alpha < \beta,\\& \text{or }\max\{ \alpha, \beta \} = \max\{\gamma,\delta\}\text{ and }\alpha = \beta\text{ and }\beta < \delta.\end{align} The relation $<$ defined above, is a linear ordering of the class $Ord \times Ord$. Morover, if $X \subset Ord \times Ord$ is nonempty, then $X$ has a least element. Also, for each $\alpha$, $\alpha \times \alpha$ is the initial segment given by $(0,\alpha)$. I am trying to understand, how come $Ord \times Ord$ is isomorphic to $Ord$. How come the relation is one-to-one? For example: Applying the above relation, If $\alpha \times \alpha$ is the initial segment given by $(0,\alpha)$,what will the initial segment given by $(\alpha,\alpha)$ represent in $Ord^2$?","['ordinals', 'elementary-set-theory']"
950682,"Let $f$ be a holomorphic in $D(0,1)$, with Re$\,f(z) >0$ and $f(0)=1.$ Then $\lvert\, f'(0)\rvert\leq 2$","Let $f:D(0,1) \to \mathbb{C}$ be a holomorphic function, such that $$
\mathrm{Re} \,f(z) >0\quad \text{and}\quad f(0)=1.
$$ How to prove $\lvert\, f'(0)\rvert\leq 2 \ ?$ This is now a self-answered question.","['absolute-value', 'inequality', 'derivatives', 'complex-analysis']"
950685,Smooth function on a closed set.,"Evans book on PDE's defines for a given open subset $U$ of $\mathbb{R}^{n}$, $C^{k}(\overline{U})=\lbrace u:U\rightarrow \mathbb{R}^{n}$, such that $D^{\alpha}u$ exists and is uniformly continuous on bounded subsets of $U$, for all mulitindexes $\alpha$, with $|\alpha|\leq k\rbrace$ Alternatively one could define, $C^{k}(\overline{U})=\lbrace u:\overline{U}\rightarrow \mathbb{R}^{n},$ such that there exists an open subset $V$ of $\mathbb{R}^{n}$ containing $\overline{U}$, and an extension of $u$ to $V$ that has continous partial derivatives up to order $k$ in $V\rbrace$ Are this definitions equivalent? Is this trivial?",['multivariable-calculus']
950689,Lagrange multiplier for more than one constraints.,"How to minimize $x^TAx$ over the set $D=(x\geq 0, x^TBx=1$ and $(I-A^\dagger A)x=0$), where $A$ is copositive matrix of order $n-1$ and $B$ is strictly copositive matrix of order $n$. If I drop the last constraint from set $D$ then using Lagrange multiplier I am able to minimize $x^TAx$ over set $D$, but if I have third constraint then how to proceed?
Set $D$ can be rewritten as $D=( x\in \mathbb{R}^n, x^TBx=1, h_{i}(x)=0, 1\leq i\leq n$) where $h_{i}(x)=\langle (I-A^\dagger A)e^{i},x \rangle=0$","['numerical-linear-algebra', 'multivariable-calculus', 'linear-algebra', 'copositivity']"
950696,OLS standard error that corrects for autocorrelation but not heteroskedasticity,"Question: By mapping the OLS regression into the GMM framework, write the formula for the standard error of the OLS regression coefficients that corrects for autocorrelation but not heteroskedasticity. Furthermore, show that in this case, the conventional standard errors are OK if the $x$'s are uncorrelated over time, even if the errors $\varepsilon$ are correlated over time. Attempt: So the general model is $y_t = \beta' x_t + \varepsilon_t$. OLS picks parameters $\beta$ to minimize the variance of the residual:
$$\min_{\beta} E_T[(y_t-\beta' x_t)^2] $$
where the notation $E_t(\cdot) = \frac{1}{T} \sum_{t=1}^T( \cdot )$ denotes the sample mean. We find $\widehat{\beta}$ from the first-order condition, which states that:
$$g_T(\beta) = E_T[x_t(y_t - x_t' \beta)] =0$$
In the GMM context, here, the number of moments equals the number of parameters. Thus, we set the sample moments exactly to zero and solve for the estimate analytically:
$$\widehat{\beta} = [E_T(x_tx_t')]^{-1} E_T(x_t y_t)$$
Using the known result from GMM theory that 
$$Var(\widehat{b}) = \frac{1}{T} (ad)^{-1} aSa^{\prime} (ad)^{-1 \prime}$$
where in this case $a = I$ (the identity matrix), $d = -E[x_t x_t']$, and $S = \sum_{j=-\infty}^{\infty} E[f(x_t, b), f(x_{t-j}, b)']$ with $f(x_t, \beta) = x_t(y_t - x_t'\beta) = x_t \varepsilon_t$. So the general formula for the standard error of OLS is
$$Var(\widehat{\beta}) = \frac{1}{T}E(x_t x_t')^{-1} \left[\sum_{j=-\infty}^{\infty} E(\varepsilon_t x_t x_{t-j}' \varepsilon_{t-j})\right]E(x_t x_t')^{-1}$$ Now I know from the OLS assumptions: (i) No autocorrelation: $E(\varepsilon_t \mid x_t, x_{t-1}, \cdots, \varepsilon_{t-1}, \varepsilon_{t-2}, \cdots) =0$ (ii) No heteroskedasticity: $E(\varepsilon_t^2 \mid x_t, x_{t-1}, \cdots, \varepsilon_{t-1}, \cdots) = constant = \sigma_{\varepsilon}^2$ What would the OLS standard error become if I correct for autocorrelation but not heteroskedasticity? Also how do I show that the conventional standard errors are OK if the $x$'s are uncorrelated over time, even if the errors $\varepsilon$ are correlated over time?","['statistics', 'regression', 'correlation', 'least-squares']"
950712,How to compute $\int_{-\infty}^\infty\exp\left(-\frac{(x^2-13x-1)^2}{611x^2}\right)\ dx$,"$$\int_{-\infty}^\infty\exp\left(-\frac{(x^2-13x-1)^2}{611x^2}\right)\ dx$$ WolframAlpha gives a numerical answer of $43.8122$, which appears to be $\sqrt{611\pi}$. And playing with that, it seems that replacing $611$ with $a$ just gives $\sqrt{a\pi}$. My trouble is that the stuff in the exponential always seems to be just a big mess, and I haven't been able to get it into a form I can understand or deal with. I would greatly appreciate seeing a method for solving this integral.","['improper-integrals', 'closed-form', 'calculus', 'integration', 'definite-integrals']"
950763,Does $\frac{1}{n}\sum_{i=1}^n|x_i|\to L<\infty$ imply $\frac{1}{n}\max_{1\leq i\leq n}|x_i|=0$?,"To simplify notation, let us assume that $\{x_n\}_{n\geq 1}$ is a sequence of nonnegative real numbers. Does
$$
\frac{1}{n}\sum_{i=1}^nx_i\to L
$$
for some finite $L$ imply
$$
\frac{1}{n}\max_{1\leq i\leq n}x_i\to 0?
$$
I have been thinking about this question for the past few hours because of here but I can't come up with anything. If you can help me, please feel free to head there to resolve that post as well.","['sequences-and-series', 'real-analysis', 'limits']"
950772,how to solve equation with cos,"I have this equation $\cos2x +5 \cos x + 3=0$. To solve it I rewrite $\cos2x$ to $2 \cos^{2} x- 1$ and set $\cos = t$. I get the following equation $2t^2 - 1 +5t +3 = 0$ with that and then divide the equation with two $t^2 +\frac{5}{2} t +1 = 0$. I solve this equation and get two $t$, $t_1 = -2$ and $t_2 = - \frac {1}{2}$. $t_2$ is the valid because $t$ can't be larger than 1. From here on I don't know how to use $t$ to solve this equation $\cos2x +5 \cos x + 3=0$. Can anyone explain what to do next and how to solve this equation? Thanks!!",['trigonometry']
950804,How to transfrom my equation to $Y=KX^2$,"In general ,
$$\vec{C}(u)=\vec{a_0}+\vec{a_1} u+\vec{a_2} u^2$$ is a parabolic arc between the points $\vec{a_0}$ and $\vec{a_0} + \vec{a_1} + \vec{a_2}$. So I'd like to prove it by myself: My trial as below: $\vec{a_i}=(x_i,y_i)^T$ $\Rightarrow$ $$x=x_0+x_1 u+ x_2 u^2 \qquad (1)$$
$$y=y_0+y_1 u+ y_2 u^2 \qquad (2)$$ Obviously, (1) and (2) are the equations about $u,u^2$ So I can denote $u,u^2$ by $x,y$ $$u=p_1 x+q_1y+r_1$$
$$u^2=p_2 x+q_2y+r_2$$ $\Rightarrow$ $$p_2 x+q_2y+r_2=(p_1 x+q_1y+r_1)^2$$ Unfortunately,I didn't know what transformation I need to apply to $x,y$ in the following steps. Can someone help me?",['linear-algebra']
950815,Probability that $xy = yx$ for random elements in a finite group [duplicate],"This question already has an answer here : Prove: if $a,b\in G$ commute with probability $>5/8$, then $G$ is abelian (1 answer) Closed 5 years ago . let $G$ a finite group, not abelian. I don't know if a short proof of this fact exists : $$\mathbb{P}(xy = yx) \leq 5/8$$
$x,y$ are randomly picked. Edit : If possible, i want to know if there is a shorter proof than this one : (in french sorry)","['group-theory', 'finite-groups', 'probability']"
950821,How do I solve $x=\log^e{(x+1)}$ analytically?,"How do I solve the following, analytically? $$x=\log^e{(x+1)}$$ It looks like it should be simple, but whether I take the $e$ th root of each side or take the $\log$ of each side (ending up with a $\log\log$), I get stuck—and both approaches seem naïve, anyhow. It appears to have two solutions: In case it raises some eyebrows, I only stumbled across the formula while blindly tinkering with reward functions for a productivity gamification system. In short, I needed a function $g(x)$ that behaves very similarly to $f(x)=x$ below a certain constant, e.g. 27 (which I can scale arbitrarily), then tapers off and flatlines. $g(x)=\log^e{(x+1)}$ happened to work perfectly, but of course I wouldn't assert the formula to be meaningful in any natural sense.","['algebra-precalculus', 'systems-of-equations', 'problem-solving']"
950835,Epimorphisms from a free group onto a free group,"Let $f:F_n\to F_m$ be an epimorphism ($n\geq m$). Then it is true that there is a basis $X=X_1\sqcup X_2$ in $F_n$ such that $f$ maps $\langle X_1\rangle$ isomorphically onto $F_m$, and maps $X_2$ to identity. One can probably deduce this statement from the Grushko-Neumann theorem, but somehow I cannot get an elegant proof of it. I can see that by the Grushko-Neumann we can assume that $m=1$, and hence our epimorphism factors through the abelianization. And then an ugly linear algebra appears... Any suggestions how to make the proof slick?","['free-groups', 'group-theory']"
950869,How to multiply and reduce ideals in quadratic number ring.,"I am studying quadratic number rings and I have a problem with multiplying and reducing ideals, for example: Let $w=\sqrt{-14}$. Let $a=(5+w,2+w)$, $b=(4+w,2-w)$ be ideals in $\mathbb Z[w]$. Now, allegedly, the product of ideals $a$ and $b$ in $\mathbb Z[w]$ is $(6,3w)$. Please explain clearly, how to get to $(6,3w)$.","['algebraic-number-theory', 'abstract-algebra']"
950900,How to solve the trigonometric equation $\cos17x=20\cos x$?,How to solve the following trigonometric equation? $$\cos17x=20\cos x$$ I'm really awful in trigonometry. I tried division of both sides by $20$. Thanks.,['trigonometry']
950903,Are smooth varieties locally isomorphic to the affine space?,"A smooth $n$-dimensional manifold is locally isomorphic to $\mathbb{R}^n$. I am wondering if the analogous statement for smooth algebraic varieties is also true. Let $X$ be an $n$-dimensional connected smooth variety, and $p \in X$ an arbitrary point. Is there an open subset $U \subseteq X$ such that $p \in U$ and $U \cong \mathbb{A}^n$? Thank you in advance!","['algebraic-geometry', 'abstract-algebra']"
950906,$f$ is continuous $ \iff $ $f^{-1}$ is continuous?,"Is the following true? Let $A, B \subseteq \Bbb R$ and let $f : A \to B$ be a bijective map.
  Then $\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;f$ is continuous on $A$  $
 \iff $ $f^{-1}$ is continuous on $B$ It seems like such an obvious result but I can't seem to prove it. At least is the result true if we restrict $A$ and $B$ to be intervals? My Attempt at a Proof: $\implies $: Let $b \in B$. I was going nowhere after considering an arbitrary sequence $ (x_n) $ in $B$ which converges to $b$. So I instead assumed that $V$ was any neighbourhood around $f^{-1}(b)$. Now suppose there is no neighbourhood $U$ of $b$ such that $ x \in U \implies f^{-1}(x) \in V $. But for every neighbourhood $U'$ of $b$ there is a neighbourhood $V'$ of $f^{-1}(b)$ such that $ f^{-1}(x) \in V' \implies x \in U' $. This is by considering $b = f(f^{-1}(b))$ and since $f$ is continuous. If one of these $V'$s is a subset of $V$ then we are done. But suppose not. I cannot proceed further. I considered arbitrarily small neighbourhoods $U'_n = \{ x \ | \ |x - b| \lt \frac 1 n \}$ but still got nowhere. Any help is appreciated.","['inverse', 'continuity', 'real-analysis']"
950961,"How to proof the following function is always constant which satisfies $f\left( x \right) + a\int_{x - 1}^x {f\left( t \right)\,dt} $?","Suppose that $f(x)$ is a bounded continuous function on $\mathbb{R}$,and that there exists a positive number $a$ such that
$$f\left( x \right) + a\int_{x - 1}^x {f\left( t \right)\,dt} $$
is constant. Can anybody show that  $f$ is necessarily constant ?","['calculus', 'integration', 'functions']"
950963,Integral difficulties (attempt included),"I am having difficulties with the following integral. I began working on it and thought I had obtained the answer, but when I went to graph it I received an integral of 1. I obtained the same answer when I went to double check it on WolframAlpha. Here is my working. Would anyone be able to please push me in the right direction? Let u = 1 + ln(t); du = dt/t The integral then becomes: ∫du/u^2 = -1/u = -1/(1 + |ln(t)) from e^-1 to e =-1/2 - (-1/2) = 0","['definite-integrals', 'algebra-precalculus', 'calculus', 'integration']"
951000,Percentages of percentages,"I do not even know how to title this question, or the name of the thing i'm looking for, but here's an example: there's N participants we'll take just three into account, but i need a formula for N participants A has x% to win B, B has y% of winning C .. x% and y% are known since A hasn't competed with C .. i want to guess what % of A winning against C would be, based on their percentages against B. there could be N participants between A and C, not just 1 as in the example how would i calculate this?
Thanks for your time!","['statistics', 'percentages', 'probability']"
951050,Birthday Problem: Big Numbers and Distribution of the Number of Samples involved in Collisions,"A lot of questions about the birthday problem can be found here, but none seems to address my problem: Background I am thinking of a hash-type data structure design which accepts a certain number of collisions to occur. Collisions shall be detected and handled in a second data structure with substantially lower collision probabilities. The number I'm interested in is the number of datasets which go into the second data structure. Question I do not care if there are 4 'children' having birthday at the same 'day' or 2 pairs of children where each pair shares a certain birthday. Both would be counted as 4 children being involved in collisions. Also I do not care about exact results, approximations would be fine. My Question is: Given n persons and m days possible for birthdays. How to calculate the probability of k=2,3,4,5,... persons being involved in collisions? Clarification Apparently, the main problem is that I need to handle pretty big values. My dimensions are about: ""Lets say a year had 100,000 days (alternatively 1,000,000 days). Then think of a class of 50.000 kids. Whats the probability of everyone has  a unique birthday. Whats the probability of 1-20, 20-50, 50-100 kids not having a unique birthday?"" As I said, results must not be perfectly exact.","['statistics', 'probability-distributions']"
951123,compute euler characteristic for pull back line bundle of finite map,"i want to know if there is any formula to compute the euler characteristic for pull back line bundle of finite morphism, and the same question for blow up one point?  Espesically for algebraic surface.",['algebraic-geometry']
951136,Find the sum to n terms of the series,"Find the sum to n terms of the series $$\frac {\sin x}{\cos x+\cos2x} + \frac {\sin2x}{\cos x+\cos4x} + 
\frac {\sin3x}{\cos x + \cos6x} +\dotsb $$ How can I solve this? Here is what I did for the first term: $$\frac {\sin\bigl(\frac{3x}{2} - \frac x2\bigr)}
  {2\cos\bigl(\frac{x}{2}\bigr)\cos\bigl(\frac {3x}{2}\bigr)}$$ After opening $N^r$ by $\sin(A-B) = \sin A\cos B - \cos A\sin B$ we get $$ \frac 12\left(\tan\frac {3x}{2} - \tan\frac {x}{2}\right)$$ But this doesn't work out for the remaining terms. Am I missing something here or is this method completely wrong? Any hints? Help please! edit : Please give a hint. I want to do it on my own :)","['trigonometry', 'sequences-and-series']"
951138,"If $f_1,...,f_{n+1}\in\mathbb{C}[x_1,...,x_n]$, is there a polynomial in the coefficients which vanishes iff the $f_i$ have a common root?","My question is as in the title: Suppose $f_1,...,f_{n+1}\in \mathbb{C}[x_1,....,x_n]$.  Is there polynomial $g$ (or a system of polynomials) with variables given by the coefficients of the $f_i$ with the property that $g$ vanishes iff there is a common root of the $f_i$? For my particular application, I don't need the explicit form of $g$ - I simply need its existence. What Google has told me is the following:  There is something similar to what I want called the Macaulay resultant $R(f_1,...,f_{n+1})$ (where one must first homogenize the $f_i$).  Then the Macaluay resultant vanishes iff the $f_i$ have a common root in $\mathbb{C}P^n$. What I'm asking is something slightly stronger, I want a polynomial whose vanishing is necessary and sufficient for the $f_i$ to have a common root in $\mathbb{C}$, that is, I disallow points at infinity in $\mathbb{C}P^n$. Further, Googling tells me that in the case that $n=1$, there is something called the resultant which does exactly what I want. If I nest resultants (thinking of $\mathbb{C}[x_1,...,x_n]$ as $\mathbb{C}[x_1,...,x_{n-1}][x_n]$), does that work?  I'm thinking of something like $Res_{x_{n-1}}(Res_{x_n}(f_1,f_2),Res_{x_n}(f_1,f_3))$ where the subscript indicates which thing we view as the variable. In case these questions are too broad, here's the specific example I'm interested in.    I have 8 variables and 9 polynomials.  Writing $\mathbb{C}[a_1,a_2,a_3,a_4,b_1,b_2,b_3,c]$, 8 of the polynomials have the form $$f_i(\vec{a},\vec{b},c) = g_i(\vec{a}) + \left(\sum a_i^2\right)( h_i(\vec{a}) + j_i(\vec{b}))$$ where $g_i$, $h_i$, and $j_i$ are all linear functions (with trivial constant coefficient).  The $9$th equation is $$f_9(\vec{a},\vec{b},c) = \left(\sum a_i^2\right)c -1.$$ If one homogenizes by inserting the variable $z$, then one find the point $[\vec{a}:\vec{b}:c:z] = [\vec{0}: \vec{b}:c:0]$ is a simultaneous solution for all $\vec{b}$ and $c$.  In particular, I think the Macaulay resultant vanishes in this situation. Thanks in advance!","['ring-theory', 'algebraic-geometry']"
951159,"How to integrate $\int_C{\frac{\sin\pi z}{(z^2-1)^2}}dz$, where $C: |z-1|=1$ using Cauchy's formula?","How can evaluate $$\int_C{\frac{\sin\pi z}{(z^2-1)^2}}dz$$, where $$C: |z-1|=1$$ by using Cauchy's formula. I have to use Cauchy's formula.
Cauchy's formula $$f(z_0)=\frac{1}{2\pi i}\oint_L\frac{f(z)dz}{z-z_0}$$ requires me to have denominator in form of $(z-z_0)^n$. I am confused about how to get denominator to fit formula.","['complex-integration', 'complex-analysis', 'contour-integration']"
951171,Convergence of $\sum \frac{(2n)!}{n!n!}\frac{1}{4^n}$,"Does the series
$$\sum \frac{(2n)!}{n!n!}\frac{1}{4^n}$$ converges? My attempt: Since the ratio test is inconclusive, my idea is to use the Stirling Approximation for n! $$\frac{(2n)!}{n!n!4^n} \sim (\frac{1}{4^n} \frac{\sqrt{4\pi n}(\frac{2n}{e})^{2n}}{\sqrt{2 n \pi} \sqrt{2n \pi} (\frac{n}{e})^{2n}} =\frac{(2)^{2n}}{4^n \sqrt{n \pi}}$$
The series of the secomd term diverges. It is correct to conclude thatthe series diverges? Another ideas are welcome! Thanks","['sequences-and-series', 'convergence-divergence', 'calculus', 'analysis']"
951177,Inverting a Characteristic Function for half-cubic Student's T entailing a Modified Bessel of 2nd kind,"The Characteristic function of the Student's T with $\alpha$ degrees of freedom, 
$C(t)=\frac{2^{1-\frac{\alpha }{2}} \alpha ^{\alpha /4} \left| t\right| ^{\alpha /2}
   K_{\frac{\alpha }{2}}\left(\sqrt{\alpha } \left| t\right| \right)}{\Gamma
   \left(\frac{\alpha }{2}\right)}$ entails a modified Bessel function of the second kind
$K_{\alpha/2}\left(\sqrt{\alpha } \left| t\right| \right)$. To invert the Fourier to get the probability density of the $n$-summed variable when $\alpha$ is not an integer poses problem as the equation below seems integrable otherwise. Of particular interest is the distribution for $\alpha= 3/2$ (""halfcubic""). With $n$ an integer ( $n >2$):
  $$f_n(x)= \left(\frac{3^{3/8}}{\sqrt[8]{2} \,\Gamma \left(\frac{3}{4}\right)}\right)^n \int_{-\infty }^{\infty } e^{-i\, t x}  \left| t\right| ^{\frac{3 n}{4}} K_{\frac{3}{4}}\left(\sqrt{\frac{3}{2}} \left| t\right| \right)^n \, dt$$
   I tried all manner of expansions and reexpressions of the Bessel into other functions (Hypergeometric, Gamma) to no avail. One good news is that $n=2$ works on Mathematica because the Wolfram library has the square of a Bessel function. It would be great to get the solution for at least $n=3$.","['integration', 'probability-theory', 'analysis', 'probability-distributions', 'complex-analysis']"
951180,"Fractional Sobolev space $H^{1/2}(-\pi,\pi)$","Let $H^{1/2}(-\pi,\pi)$ be the space of $L^2$ functions whose Fourier series coefficients $\{c_n\}_n$ satisfy the summability constraint $\sum_n |n| |c_n|^2 < \infty$. Are functions in $H^{1/2}(-\pi,\pi)$ continuous (in the sense that each admits a continuous representant)?","['sobolev-spaces', 'fourier-series', 'sequences-and-series', 'functional-analysis']"
951184,Inverse of matrix sum of identity and outer product,"So before we begin, I already know the answer. I'm just having difficulty figuring out the steps for finding it. Given $u,v \in \mathbb{R}^{n}$, I want to show that $$(I+uv^{T})^{-1}= I - \frac{uv^{T}}{1+v^{T}u}$$ I know from Inverse of the sum of matrices that this is the answer, and since both O(u)=O(v)=n that $Tr(uv^{T}) = v^{T}u$. It's just a matter of getting from point A to point B.",['linear-algebra']
951197,Finding the norm of this upper triangular matrix,"I have a matrix $A=\begin{pmatrix} a & b\\ 0 & a\end{pmatrix}\in M_2(\mathbb{C})$. Given that $|a|<1$ and $|b|\leq 1-|a|^2$, I am supposed to show that $\|A\|\leq 1$ (operator norm). I can't get a clear answer out of computing the operator norm as $\|A\|=\displaystyle \sup_{|x|=1}|Ax|$, and the same holds for trying to find eigenvalues of $A^*A$ (or the numerical range of $A^*A$. Since the matrix is not normal, I can't use the spectral theorem. At best, all my approaches do no better than just taking $\|A\|\leq \|aI\|+\left\|\begin{pmatrix} 0 & b\\ 0&0\end{pmatrix}\right\|\leq 1-|a|^2+a$. Is there anything else I can try here? I should say that this is part of a larger problem in Paulsen, which asks you to show that $\|A\|\leq 1$ iff $|b|\leq 1-|a|^2$. I have already proved that, for $f(z)=\displaystyle\frac{z-a}{1-\overline{a}z}$, and polynomials $p_n(z)=(z-a)\displaystyle\sum_{k=0}^n (\overline{a}z)^k$, which converge to $f$ in $A(\mathbb{D})$ that $p_n(A)=\begin{pmatrix} p_n(a)&bp'_n(a)\\ 0&p_n(a)\end{pmatrix}=\begin{pmatrix} 0&b\sum_{k=0}^n|a|^k\\0&0\end{pmatrix}$ converges in norm to $\begin{pmatrix} f(a)&bf'(a)\\0&f(a)\end{pmatrix}=
\begin{pmatrix} 0&\frac{b}{1-|a|^2}\\0&0\end{pmatrix}$. However, this (plus Von Neumann's Inequality) is only sufficient to prove $\|A\|\leq 1\Rightarrow |b|\leq 1-|a|^2$. This is a homework problem, so I'd prefer to have just some guidance.","['operator-theory', 'matrices', 'operator-algebras', 'linear-algebra', 'functional-analysis']"
951216,How to create a function based on the characteristics?,"I wonder how to create a function based on the characteristics.
suppose I have function $f$ and $g$ like this: $f(x,g(x,y,z)) = y$ $\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,,$
    $g(x,f(x,z),a) = z$ With $x,y,z,a$ are the parameters. Are there ""algorithm"" to ""generate"" arbitrary function which appropriate to that functions ( f and g )?$\,\,\,$
or I should use brute forces / try and error to do that? what mathematics branch to study that?",['functions']
951219,Is my proof by contradiction about the empty set correct?,"I am trying to learn about proofs and one of the exercice in my book (Maths ABC) is about proof by contradiction. I think I understand the concept but I would like to have a feedback on the following proof about whether or not it is correct. Let $A$ be the following proposition: 
  $$[(S\cup P = P) \land (S\cap P = \emptyset)]\rightarrow(S = \emptyset)$$
  Prove by contradiction that $A$ is true. My proof: Let assume that $[(S\cup P = P) \land (S\cap P )= \emptyset)]\rightarrow\;S\neq \emptyset$ is true. $$(S\cup P) = \{x\mid x\in S \lor x\in P\}\land P\{x\mid x\in \emptyset\lor x\in P\}$$
hence, if $(S\cup P=P)$ we can see by symmetry that $S$ has to be equal to $\emptyset$ which is absurd and contradict our assumption. $[(S\cup P = P)\land(S\cap P = \emptyset)]$ cannot be true if one of the two statement is false thus it is enough for us to assert that $S=\emptyset$ QEA","['elementary-set-theory', 'proof-verification']"
951233,Algorithm to answer questions on dominated input,"Consider a setting where we see inputs one-by-one, with each input being an $n$-tuple $(a_1,a_2,...,a_n)$, where each $a_i\in\{0,1\}$. For each new input we see, we have to answer two questions: 1) Have we seen this input before? 2) Is the input dominated by some input we've seen before? (An input $A=(a_1,\ldots,a_n)$ is said to be dominated by an input $B=(b_1,\ldots,b_n)$ if $a_i\leq b_i$ for all $i$.) How fast can we answer these two questions for each input? For question 1), we can use a hash table. We check whether the input has been seen before in $O(1)$ time, and if not, insert the input into the hash table in $O(1)$ time. To accommodate question 2), the trivial way is to compare the new input with each one of the previous inputs, which would take $O(k)$ time, where $k$ is the number of elements in the hash table. This could be up to $O(2^n)$ time. Is there a way to reduce this to something polynomial in $n$?","['discrete-mathematics', 'algorithms']"
951252,"Number of ways to choose a pair $\{a,b\}$ of distinct numbers from the set $\{1,2,...,50\}$","Find the number of ways to choose a pair $\{a,b\}$ of distinct numbers from the set $\{1,2,...,50\}$ such that i) $|a-b| = 5$; ii) $|a-b| \leq 5 $ My thoughts: For (i)
For every 6 consecutive numbers, there's a pair. But I can't do $C_{6}^{50}$ as it's not every 6 number combinations I want. Then answer given is (i)45 (ii)235","['combinations', 'combinatorics']"
951256,help with strange Double Integral: $\iint_E {x\sin(y) \over y}\ \rm{dx\ dy}$,"i'm having trouble with this double integral:
$$
\iint_E {x\sin(y) \over y}\ \rm{dx\ dy},\ \ \ \ 
E=\Big\{(x,y) \in \mathbb{R^2} \mid 0<y\le x\ \ \  \land\ \ \  x^2+y^2 \le \pi y\Big\}
$$
i've tried using polar coordinates, but after i made the domain normal i realized that the integrand got a lot more complicated..
then i've tried another transform: $u=y/x, v=y$; with even worse results.
i'm looking mainly for a tip on how to tackle this,
also i'd like to know the reasoning behind an eventual tip... thanks in advance!","['multivariable-calculus', 'calculus', 'integration']"
951293,"If A is positive definite, can we prove that x>0 and Ax>0 always have a feasible solution?","I'm new here and couldn't find a similar question, so pardon me if it's already asked elsewhere. The question is literally simple: Suppose A is a positive definite matrix , could it be generally proved that the set $S=\left \{ x\in R^{n} |x>0,Ax>0 \right \}$ is nonempty? The inequalities are strict and by $x>0$, I mean $x_{i}>0$ for $i=1,2,...,n$. Honestly speaking, I haven't had any mentionable progress on the analytical side, except trying different types of decompostions (Cholesky, Spectral, etc ...) to no avail. so I tried solving this linear program for different choices of $A\succ 0$ using Matlab so that I could find a counterexample, without success. $$min w=\mathbf{1}^Ty+\mathbf{1}^Tz$$
$$s.t. \left\{\begin{matrix}x=\mathbf{1}+y\\Ax=\mathbf{1}+z \\ x,y,z\geq 0\end{matrix}\right.$$ where $\mathbf{1}=(1,1,...,1)^{T}$ I'd really appreciate if you would guide me on how could it be proven or negated.",['linear-algebra']
951339,belong to and subset in the set,"My question is I can't understand the difference between belong and subset . Set theory: difference between belong/contained and includes/subset? I've read this already but I didn't get it yet... I hope you tell me how did they answer this question in this way, I am really confused. Why they didn't put belong to ?? And why belong to is wrong?? I don't know where should I put subset or belong to...thank you.",['elementary-set-theory']
951341,Are the plane and the line parallel?,"Problem In the picture above are given the coordinates of the points $O(0,0,0)$, $A(6,0,0)$, $C(0,12,0)$, $D(0,0,5)$, $K(0,6,5)$, $L(6,12,4)$, $M(6,8,0)$, $N(0,8,0)$. It seems as if line $KL$ is parallel to plane $DEM$. Show if they are indeed parallel or not. My attack I set up an equation for plane $DEM$: $5y+8z=40$. Then, I set up a vector equation for line $KL$: $\begin{pmatrix}x\\y\\z\end{pmatrix}=\begin{pmatrix}0\\6\\5\end{pmatrix}+\lambda\begin{pmatrix}6\\6\\-1\end{pmatrix}$. I plugged in $y=6+6\lambda$ and $z=5-\lambda$ into the equation for plane $DEM$, and ended up with $\lambda=-\frac{15}{11}$. This means they intersect at a point with $\lambda=-\frac{15}{11}$. But , my book says they are parallel. Am I wrong or is my book wrong? Thanks for the help. @Minibill, how do you explain the visible intersection in this plot of the line and the plane?",['multivariable-calculus']
951343,Combining four distinct objects with repetition,"With four different objects $k = \{obj1, obj2, obj3, obj4\}$ . How many combinations are there if I were to copy $20$ freely-chosen objects. I could for example have $20\times obj1$ if I wanted. However, permutations are not to be counted. Probable solution I'm thinking I need to use one of the following ( theorems?) : $$\binom{n+k-1}{k}\quad or\quad \binom{n+k-1}{n}$$ However, there are two problems with this. I wouldn't know which one, and I can't find any understandable explanation. The latter I withdrew from the theorem from stars and bars method. What's the difference betweeen the formulas? And should I even use one of these formulas for my problem? The first yields $8855$ combinations, and the second yields $1771$ combinations. Further question Wherever I look, people use the formulas interchangeably $^{[1]}$ , however; they're not equal. Take for example: $$\binom{9+3-1}{3} \not= \binom{9+3-1}{9}$$ How come? $^{[1]}$ http://mathworld.wolfram.com/Multichoose.html http://en.wikipedia.org/wiki/Stars_and_bars_%28combinatorics%29#Theorem_two_2","['discrete-mathematics', 'combinatorics']"
951394,Second order linear ODE question,"I am working on this equation: $ x^4y''+2x^3y'+y=0$ and i need a little help. Should I substitute y=exp(integrate(u)dx), and transform given equation into Riccati's: $ u'=-u^2-2u/x-1/x^4 $ (but i dont know know how to solve this either) or is there any other way? Thanks for tips and help",['ordinary-differential-equations']
951418,Examples of measures that induce certain inclusions in the Lp spaces.,"I apologize for the terribly worded title, but I didn't know how else to title this questions (which comes from Rudin's Real & Complex Analysis chapter 3 questions). The question says: For some measures, the relation $r<s$ implies $L^r(\mu)\subset L^s(\mu)$; for others, the inclusion is reversed; and there are some for which $L^r(\mu)$ does not contain $L^s(\mu)$ if $r\neq s$. Give examples of these situations, and find conditions on $\mu$ under which these situations will occur. I am having an extremely difficult time grasping any/everything having to do with measures and Lp spaces, so this question is mind-crippling. I don't even know where/how to begin thinking about such a question. Any input/help/criticism is greatly appreciated.","['measure-theory', 'lp-spaces', 'real-analysis']"
951449,Frobenius norm of a matrix [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I know that Frobenius norm of a matrix A is equal to the square root of the trace of (A*conjugate transpose(A)). But how do I prove it mathematically?","['matrices', 'normed-spaces']"
951464,Trigonometric equation $\sin(60^\circ-2X)\sin(5X)=\sin(8X)\sin(3X)$,"A trigonometric equation is to be solved, the solution ($X=10^\circ$) is very clear but I need a proper method $$\sin(60^\circ-2X)\sin(5X)=\sin(8X)\sin(3X).$$","['trigonometry', 'algebra-precalculus']"
951498,Why is $E(u)=0$ when an intercept is included in OLS Estimation?,"I am reading Wooldridge's graduate econometrics text. There he states that when estimating the equation $y=\mathbf{x\beta}+u$ by OLS, if an intercept (constant term) is included in your $\mathbf{x}$ vector, so that $\mathbf{x}=(1,x_1,...,x_k)$, where $y$ and the other $x_1,...,x_k$ are random variables, then we have automatically that $E(u)=0$. I  am trying to see why. Later on, when the textbook introduces OLS, one of the assumptions is that $E(\mathbf{x}^{\top}u)=\mathbf{0}$. Now note that if $\mathbf{x}$ contains an intercept, then this statement implies that $E(u)=0$. However I believe this assumption is not needed to know that when an intercept is included, then $E(u)=0$, rather we have $E(u)=0$ ""for free"" as the textbook says. I found a similar question (with name not representative of its question/answer) located here: Why the expected value of the error when doing regression by OLS is 0? which says the constant effectively ""absorbs"" $E(u)$ to make $E(u)=0$. How does this work, in theory? My questions: How do we know that an intercept ""absorbs"" $E(u)$ exactly -- no more, no less? Does this rely on the assumption $E(\mathbf{x}^{\top}u)=\mathbf{0}$? Thanks! Edit My econometrics instructor notes that if we include an intercept, so that \begin{equation}y=\beta_0+\mathbf{x\beta}+u,\end{equation}
where $E(u)=\alpha\ne0$, then we can always rewrite the first equation as
\begin{align*}
y&=(\alpha+\beta_0)+\mathbf{x\beta}+(u-\alpha) \\
&=(\alpha+\beta_0)+\mathbf{x\beta}+\tilde{u},
\end{align*}
where $\tilde{u}=u-\alpha$ and $E(\tilde{u})=0$. However this still doesn't explain how we get $E(u)$, so that we can ""include"" it in our intercept. (We can only estimate $E(u)$, but how do we do that?) I know that if $x_0=1$ and all the other $x_1,...,x_k$ equal $0$ then what really happens is $\hat{\beta}=(x^{\top}x)^{-1}x^{\top}y=y$... Am I getting there?","['statistics', 'regression', 'least-squares', 'expectation']"
951501,Exponential equations with variables on both sides,I have the following: $$8^{3x+4} = 5^{4x-2}$$ How would I solve this? I tried this: $$(3x+4)\log 8 = (4x-2)\log 5$$ but have no idea where to go from there. Thank you!,"['exponentiation', 'algebra-precalculus']"
951512,Show independence of number of heads and tails,"I am independently studying Larry Wasserman's ""All of Statistics"" Chapter 2 exercise 11 is this: Let $N \sim \mathrm{Poisson}(\lambda)$ and suppose we toss a coin $N$ times. Let $X$ and $Y$ be the number of heads and tails. Show that $X$ and $Y$ are independent. First, I don't understand what the significance of the Poisson distribution of $N$ is? Secondly, I am guessing that I'm supposed to show: $$f_{X,Y}(x,y) = f_X(x)f_Y(y)$$ I know that: $f_X(x) = {n \choose x} p^x(1-p)^{n-x}$ because the number of heads would follow a binomial distribution I don't understand how to express $f_Y(y)$ or $f_{X,Y}(x,y)$","['statistics', 'probability-distributions']"
951516,Going-up and going-down theorems: motivation,"I am reading about the going-up and going-down theorems in Atiyah & Macdonald's commutative algebra book. I'm wondering if anyone could give me some basic facts/examples to help me understand why these two theorems are so important. Neither of these theorems are actually used in Atiyah & Macdonald, and yet I don't find the statements to be so obviously remarkable that they should deserve a special name.","['commutative-algebra', 'algebraic-geometry']"
951522,Trig sum: $\tan ^21^\circ+\tan ^22^\circ+\cdots+\tan^2 89^\circ = \text{?}$,"As the title suggests, I'm trying to find the sum $$\tan^21^\circ+\tan^2 2^\circ+\cdots+\tan^2 89^\circ$$ I'm looking for a solution that doesn't involve complex numbers, or any other advanced branch in maths. The solution can involve techniques such as induction, telecoping, etc, but preferably only ideas from precalculus, e.g. trig identities, polynomials, etc. EDIT: I know that the sum is a rational number.","['trigonometry', 'summation', 'algebra-precalculus']"
951562,Integral of greatest integer function divided by an exponential,"If $\lfloor x \rfloor$ denotes the greatest integer not exceeding $x$, then find $\displaystyle\int_{0}^{\infty} \displaystyle \frac{\lfloor x \rfloor}{e^{x}} dx$. The correct answer is supposed to be $\frac{1}{e-1}$. In order to wrap my head around this problem, I wanted to see if there was a pattern for what was going on for each of the integrals $\displaystyle \int_{n}^{n+1}\displaystyle \frac{\lfloor x \rfloor}{e^{x}}dx$. Each one is equal to $\displaystyle \frac{n(e-1)}{e^{n+1}}$. I figured then that a good way to solve this problem, then, was to sum up the infinite series, $\displaystyle \sum_{n=0}^{\infty}\frac{n(e-1)}{e^{n+1}}$. Now, I believe that this series must converge, since the trapezoids formed by the x-axis and each $\displaystyle \frac{n}{e^{x}}$ become infinitesimally small as $n$ gets large. However, it's not a geometric series, so I am not sure how to find what it converges to, which, from the answer to the problem, must be $\frac{1}{e-1}$. Am I going about this in an unnecessarily complicated way, and is there a simpler method to evaluate this integral? If not, what is the piece that I'm missing? Thanks :)","['definite-integrals', 'improper-integrals', 'integration']"
951596,How do I find the PMF of X when X is the number of flips of a fair coin that are required to observe the same face on consecutive flips?,"How do I find the PMF of $X$ when $X$ equals number of flips of a fair coin that are required to observe the same face on consecutive flips? The hint was to draw some sort of a tree diagram, but I'm not quite sure as to how it helps. I know that $P(X=x)=\sum(f(x))$, but I don't even know where to start for this particular question. Any help would be greatly appreciated! Thank you!","['functions', 'probability-theory', 'probability', 'statistical-inference', 'combinatorics']"
951620,Beta/Dirichlet question,"A generalization of the beta distribution is the Dirichlet distribution. In its bi-variate version, (X,Y) have pdf $f(x,y) = Cx^{a-1}y^{b-1}(1-x-y)^{c-1}, 0<x<1, 0<y<1, 0<y<1-x<1$ , where $a>0,b>0, c> 0 $ are constants. (a)Show that $C = \frac{\Gamma(a)\Gamma(b)\Gamma(c)}{\Gamma(a+b+c)}$ (b) Show that, marginally, both X and Y are beta. (c) Find the conditional distribution of $Y|X=x$ and show that $Y/(1-x)$ is beta(b,c). (d) Show that $E(XY) =\frac{ab}{(a+b+c+1)(a+b+c)}$ , and find their covariance. Attempt at (a): $\int_0^1\int_0^{1-x} x^{a-1}y^{b-1}(1-x-y)^{c-1}dx dy$ $\int_0^1x^{a-1}\int_0^{1-x} y^{b-1}(1-x-y)^{c-1}dx dy$ Let $u=\frac{y}{(1-x)}$ $\int_0^1x^{a-1}(1-x)^{b+c-1}dx\int_0^1 u^{b-1}(1-u)^{c-1}du$ $\int_0^1x^{a-1}(1-x)^{b+c-1}dx*\frac{\Gamma(b)\Gamma(c)}{\Gamma(b+c)}$ $\frac{\Gamma(b)\Gamma(c)}{\Gamma(b+c)}\int_0^1x^{a-1}(1-x)^{b+c-1}dx$ $\frac{\Gamma(b)\Gamma(c)}{\Gamma(b+c)}\frac{\Gamma(a)\Gamma(b+c)}{\Gamma(a+b+c)}$ $\frac{\Gamma(a)\Gamma(b)\Gamma(c)}{\Gamma(a+b+c)}$ Not sure how to correctly do parts b-d","['conditional-probability', 'statistics', 'probability-theory', 'probability-distributions', 'probability']"
951621,Integral ${\large\int}_0^1\left(-\frac{\operatorname{li} x}x\right)^adx$,"Let $\operatorname{li} x$ denote the logarithmic integral $$\operatorname{li} x=\int_0^x\frac{dt}{\ln t}.$$
Consider the following parameterized integral:
$$I(a)=\int_0^1\left(-\frac{\operatorname{li} x}x\right)^adx.$$ Can we find a closed form for this integral? We can find some special values of this integral:
$$I(0)=1,\,\,I(1)=1,\,\,I(2)=\frac{\pi^2}6,\,\,I(3)\stackrel?=\frac{7\zeta(3)}2$$
The last value was suggested by numeric computations, and I do not yet have a proof for it. Can we prove the conjectured value of $I(3)$? One could expect that $I(4)$ might be a simple rational (or at least algebraic) multiple of $\pi^4$ but I could not find such a form. Can we find closed forms for $I(4),I(5)$ and other small integer arguments?","['closed-form', 'calculus', 'special-functions', 'definite-integrals', 'logarithms']"
951622,Why isn't $f(x)=\sqrt{2-x}$ reflected across the y-axis?,"If I try to graph this function, it does not appear to reflect across the y-axis when it comes time to do the reflection. Rather, it is reflected around the point where the function begins on the graph. Here is what I tried:
$$\sqrt{2-x}=\sqrt{-(x-2)}$$
This makes it easier to graph the transformations: Root function ($f(x)=\sqrt{x}$): With the right horizontal shift ($f(x)=\sqrt{x-2}$): With a horizontal reflection ($f(x)=\sqrt{-(x-2)}$): This is the part I'm confused about. Why doesn't it reflect across the y-axis? I would expect the final graph to look like this:","['algebra-precalculus', 'reflection', 'graphing-functions', 'functions', 'transformation']"
951647,Ways to find the order of an element in a group,"Is there a better way of finding the order of an element in a group other than circling until the identity is reached? Is there or CAN there be a better general ways of finding orders of elements?
(if no, please, explain why there can't be any ways) An example I have is a multiplicative group of elements $Z_{20}$ under modulo 20. Do I have to circle over every element to find orders?","['finite-groups', 'group-theory', 'abstract-algebra']"
951672,"If $xH=yH$, then $xy^{-1} \in H$.","Use a counterexample to disprove the following statements: If $xH=yH$, then $xy^{-1} \in H$. If $xy^{-1} \in H$, then $xH=yH$. I was thinking for the first statement:
$$xH=yH \rightarrow y^{-1}x \in H$$
But we do know if H is commutative. Is this correct? Would it be the same for the second statement.",['group-theory']
951687,Method of characteristic for second order pde,Can I use the method of characteristic to solve second order pdes? For instance I canconsider the equation $$u_t+u_x=u_{xx}$$,"['partial-differential-equations', 'real-analysis', 'analysis']"
951719,Problems in elementary number theory and methods from physics,"I was wondering if there are intuitive ""physical"" arguments to solve problems from number theory (elementary number theory in particular, but also advanced topics). To make an example, a proof of some equivalences about primes that uses concepts like energy, forces, or something of that kind. Can you suggest some references on this subject (or make some examples yourself)?","['big-picture', 'physics', 'number-theory', 'elementary-number-theory', 'soft-question']"
951726,Why is the spectrum usually defined for operators between Banach spaces?,"The spectrum of a linear operator $L: \mathcal{D}(L) \rightarrow \mathcal{X} $
is generally defined for $\mathcal{X}$ a Banach space (as seen for example wikipedia on link above, or spectral decomposition on wikipedia , or this question or this answer ). Why is this? Why don't we define the spectrum more generally for operators between normed spaces? Where do we need the completeness?","['operator-theory', 'spectral-theory', 'functional-analysis']"
951870,Exact definition of a circular helix,"I know that a cylindrical helix is a curve $\alpha: I \to \Bbb R^3$, such that exists a constant vector $\bf u$ which makes a constant angle with the tangent vector $\bf T$ to the curve, at every point, that is, $\langle {\bf T}(s), {\bf u}\rangle = \cos \theta$, for all $s \in I$ (we can suppose that $\alpha$ is parametrized by arc-length). Then, there this a caracterization of cylindrical helices: a curve $\alpha$ with non-zero curvature is a cylindrical helix if and only if $\tau/\kappa$ is constant. Notice that $\kappa$ and $\tau$ need not be constant, only their ratio, though. Now, it is stated: A curve $\alpha$ is a circular helix if and only if both $\kappa$
  and $\tau$ are constants. How am I supposed to prove this? Every book seems to think that it is so obvious what a circular helix is, but no one gives a straight definition for it. I am at a loss about what to do. I might as well define a circular helix being a curve with this property. If someone could tell me an exact definition for it, or give me a reference, I am thankful. Note: I found this question , but I don't think it was intended for the proof of this affirmation to become as complicated as in the answer there.","['differential-geometry', 'definition']"
951912,How to find LU factors of a matrix when diagonals are changed,"Say I have $A=LU$ already factored into lower and upper triangular matrices $L$ and $U$. Now I want to work on the eigenvalue problem $A-\lambda I=A'=L'U'$ where prime indicates new matrices. Given $A$, $L$, $U$, and $\lambda$, is there a way to get $L'$ and $U'$ without refactoring $A'$ from the beginning? Thanks!","['matrices', 'linear-algebra']"
951927,How to solve Sturm-Liouville problems $y''-2y'+(\lambda+1)y = 0$?,"I'm currently having a class at university that discusses Sturm Liouville Problems. We have to solve a few problems one of which I can't seem to solve. We use Advanced Engineering Mathematics by Erwin Kreyszig. The problem is a Sturm-Liouville problem:
$$y''-2y'+(\lambda+1)y=0\ ,$$ with boundary conditions $y(0)=0$ and $y(1)=0$. The problem says the following:
Find the eigenvalues and eigenfunctions. Verify orthogonality. Start by writing the ODE in the form of $$[p(x) y']' + [q(x) + \lambda r(x)]y=0$$ using problem 6. Problem 6 states the following:
Show that $$y''+f y'+(g+\lambda h)y=0$$ takes the form
$$[p(x) y']' + [q(x) + \lambda r(x)]y=0$$
if you set
$$\begin{align}
p&=e^{\int f\ dx}\ ,\cr
q&=p g\ ,\cr
r&=p h\ .
\end{align}$$
Why would you do such a transformation? Now the problems are that I can't seem to understand why such a transformation is useful, and I can't seem to find the solutions of the differential equation. According to problem 6 I find the following:
$$\begin{align}
f&=-2\cr
g&=1\cr
h&=1
\end{align}$$
so:
$$
\begin{align}
p(x) &= e^{-2x}\cr
q(x) &= e^{-2x}\cr
r(x) &= e^{-2x}\ .
\end{align}
$$
Thus I get
$$[e^{x}y']' + [1+\lambda]e^{-2x}y = 0$$ where $y(0)=0$ and $y(1)=0$. But I can't seem to get any further with this.","['ordinary-differential-equations', 'sturm-liouville']"
951937,Is there a relationship between the clique of a graph and colouring of a graph?,Can one say that the minimum number of colours required to colour a graph (such that across any edge the two vertices have distinct colours) is lower bounded by the size of the maximum clique in the graph? Is there anything more stronger known along these lines?,"['graph-theory', 'computational-complexity', 'combinatorics']"
952006,"Number of permutations of thet set $\{1,2,...,n\}$ in which $k$ is never followed immediately by $k+1$","For $n \in \mathbb N$, let $C_n$ denote the number of permutations of the set $\{1,2,...,n\}$ in which k is never followed immediately by $k+1$ for each $k=1,2,...,n-1$ i) Find $C_n$ ii) Show that $C_n = D_n + D_{n-1}$ for each $n \in \mathbb N$ I know that the number of r-combinations of a set which contains no consecutive integers is given by $\binom{n-r+1}{r}$ but I'm not sure how to proceed from there. Answer given for (i) $C_n = \sum_{i=0}^{n-1}(-1)^{i}\binom{n-1}{i}(n-i)!$",['combinatorics']
952025,Absolute Max/Min of a function of two variables on a set?,"How do you find the absolute maximum/minimum values of the function $f(x,y) = x^2 + y^2 - 8y + 16$ on the given set R where $R = {(x,y): x^2 + y^2 ≤ 25}$ I know the absolute maximum is 81 and minimum is 0. How exactly does this work? I have seen something about converting the inequality in the set into an equality and then plugging it back into the equation. Every way I do this seems to be wrong and my book skips way too many steps to help. Thanks.",['multivariable-calculus']
952137,"$f(AB)=f(A)f(B)$, show that $f$ is or injective or zero","Let $f\in\mathcal{L}(\mathcal{M}_n(\mathbb{R}))$ such that: $\forall(A,B)\in\mathcal{M}_n(\mathbb{R}),f(AB)=f(A)f(B)$ How can I show that $f$ is or injective or the null function ? What I have tried so far haven't worked at all. Notations : $\mathcal{L}(E)$ : set of the endomorphisms $E\rightarrow E$","['matrices', 'linear-algebra', 'functions']"
952147,Find the number of prime ideals (CSIR 2014),"Let $p,q$ be distinct primes. Then (1) $\dfrac{\mathbb{Z}}{p^2q}$ has exactly 3 distinct ideals. (2) $\dfrac{\mathbb{Z}}{p^2q}$ has exactly 3 distinct prime ideals. (3) $\dfrac{\mathbb{Z}}{p^2q}$ has exactly 2 distinct prime ideals. (4) $\dfrac{\mathbb{Z}}{p^2q}$ has unique maximal ideal. Generally, the ideals of $\mathbb{Z}_n$ are of the form $<d>$, where $d|n$. But how to describe all prime maximal ideals?","['ring-theory', 'ideals', 'abstract-algebra']"
952174,Finding the eigenvalues of a given Markov matrix,"Let $$A = \begin{pmatrix}
0.6 & 0.1 & 0.1\\ 
0.1 & 0.8 & 0.2\\
0.3 & 0.1 & 0.7 
\end{pmatrix}$$ I want to find the eigenvalues of this matrix. Because this is a markov matrix, I know that $\lambda_1 = 1$. Furthermore, I know that the remaining eigenvalues $\lambda_2$ and $\lambda_3$ should be less than one (their magnitudes should be less than one, at least) because all the entries in $A$ are positive. I found the characteristic polynomial, but it is an ugly polynomial. Is there a better/easier way to determine the remaining two eigenvalues? Maybe by using the information I already have that $\lambda_1 = 1$?","['linear-algebra', 'eigenvalues-eigenvectors']"
952181,How to find the derivative of $(3x-1)^2(2x+3)^2$,"I used the power rule and the chain rule and ended up with this:
$$y'= (3x-1)^2 \times 2(2x+3) \times 2 + (2x+3)^2 \times 2(3x-1)\times 3$$ The next step, which I do not understand how it is combined or created is this: $$y'= 2(3x-1)(2x+3)\left[2\cdot(3x-1)+3(2x+3)\right]$$
Where did the exponents go? What is combined? How is it combined? The Final Answer should be this, according to my teacher: 
$$y'= 2(3x-1)(2x+3)(12x+18)$$","['calculus', 'derivatives']"
952185,Find angle x in the picture [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question This question is in the form of a picture: How long did it take you to find out?","['geometry', 'soft-question']"
