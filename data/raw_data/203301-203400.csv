question_id,title,body,tags
4014192,Show that the equation has a periodic orbit,"I've been learning differential equations and need help with this exercise: $$
x^{\prime \prime}+\left(5 x^{4}-9 x^{2}\right) x^{\prime}+x^{5}=0
$$ I've tried to write it as a system: $$
\begin{array}{l}
x^{\prime}=y \\
y^{\prime}=\left(9 x^{2}-5x^4 \right) y-x^{5}
\end{array}
$$ any help would be appreciated",['ordinary-differential-equations']
4014207,Show $\log\left(\frac{x-y}{x}\right)-2\sqrt{\frac{x-y}{x}}<\log\left(\frac{x+y}{x}\right)-2\sqrt{\frac{x+y}{x}}$ if $x\geq 5$ and $1\leq y\leq x-2$,"Assume that all logarithms are natural. Let $x$ and $y$ be integers that satisfy $x \geq 5$ and $1 \leq y \leq x-2$ . I am trying to show that $$\log\left( \frac{x-y}{x}\right)-2\sqrt{\frac{x-y}{x}} < \log\left( \frac{x+y}{x}\right)-2\sqrt{\frac{x+y}{x}}.$$ This is equivalent to showing that $$\log\left( \frac{x-y}{x+y}\right)<2\left(\sqrt{\frac{x-y}{x}}-\sqrt{\frac{x+y}{x}}\right).$$ My first inclination was to apply the well known inequality, $\log(z)\leq 2(\sqrt{z}-1)$ if $z>0$ , to the left side by letting $z=\frac{x-y}{x+y}$ , but this did not help me. I also tried double induction on $(x,y)$ (since $x$ and $y$ are integers) but I could not finish it. I appreciate any help, thank you.","['inequality', 'logarithms', 'multivariable-calculus', 'calculus', 'algebra-precalculus']"
4014229,Proof of Picard's theorem,"Let $U$ be an open set in $\mathbb{R}^2, (a,b)\in U,$ and let $F: U\to \mathbb{R}$ satisfy a Lipschitz condition on $U.$ Then there exists $\delta > 0 $ so that the differential equation $\frac{dy}{dx} = F(x,y)$ has a unique solution $y=f(x)$ with $f(a) = b,$ defined for all $x\in [a-\delta, a+\delta].$ Below is part of a proof of this theorem. Observe that $y=f(x)$ is a solution to the differential equation $\dfrac{dy}{dx} = F(x,y)$ with $f(a) = b$ iff $f(x)$ satisfies the equation $f(x) = b + \int_a^x F(t, f(t)) dt$ for all $x\in [a-\delta, a+\delta].$ Let $\ell$ be a Lipschitz constant for $F.$ Choose $r > 0 $ so that $\overline{B}((a,b),r)\subseteq U$ and let $k=\max_{(x,y)\in \overline{B}((a,b),r)} |F(x,y)|.$ Choose $\delta >0$ with $\delta < \frac{1}{\ell}$ small enough so that the rectangle $R=[a-\delta, a+\delta]\times [b-k\delta, b+k\delta]\subseteq B((a,b), r).$ I have two main questions: I do not know how to show that if $f(x)$ is a solution to the given differential equation with $f(a) = b,$ then the graph of $f$ must be contained in the rectangle $R$ . Here, $\operatorname{Graph}(f) := \{(x,f(x)) : x\in [a-\delta, a+\delta]\}.$ I was thinking of using the Mean Value Theorem and considering when $x\in [a-\delta, a), x=a, x=[a, a+\delta).$ Suppose $x\in [a-\delta, a+\delta].$ If $x=a, (x,f(x))\in R.$ Otherwise, if $x \in [a-\delta, a), \exists c \in (x,a)$ so that $f(a) - f(x) = \delta f'(c) = \delta F(c,f(c)).$ If I could show that $(c,f(c)) \in \overline{B}((a,b),r)$ (that is, $\lVert(c,f(c)) - (a,b)\rVert\leq r$ ), then the definition of $k$ gives the result. And the case where $x\in (a, a+\delta]$ should be similar. I'm also not sure how to verify that the set $X = \{f \in \overline{C}[a-\delta, a+\delta] \mid\operatorname{Graph}(f)\subseteq R\}$ is a closed subspace of the metric space $C[a-\delta, a+\delta]$ under the supremum metric. I know one way is to show that every sequence in $X$ converges in $X,$ but I'd have to show that the limit's graph is contained in $R,$ which doesn't seem that easy. I only request justification for the two claims above. I know the rest of the proof can be completed using the fact that $X$ is a closed subset of the complete metric space $C[a-\delta, a+\delta].$ Then we can define $G(f)(x) = \int_a^x F(t, f(t)) dt + b,$ has the property that $G(X)\subseteq X$ and that $c=\ell \delta < 1$ is a contraction constant for $G.$ Thus by the Banach fixed point theorem, $G$ has a unique fixed point, which is the unique solution  to the differential equation.","['calculus', 'derivatives', 'ordinary-differential-equations', 'real-analysis']"
4014230,Conditional Expectation of X with respect to a $\sigma$-algebra $\mathcal{G}$,"I read this definition from my lecture handout: If X is a random variable on a countable probability space $(\Omega,\mathcal{F},\mathbb{P})$ , then the conditional expectation $\mathbb{E}_\mathbb{P}(X|\mathcal{G})$ of X with respect to $\mathcal{G}$ is defined as a random variable which satisfies, for every $\omega\in A_i$ , $$\mathbb{E}_\mathbb{P}(X|\mathcal{G})(\omega)=\sum_{i\in I}\frac{\mathbb{E}_\mathbb{P}(X\mathbb{1}_{A_i})\mathbb{1}_{A_i}}{\mathbb{P}(A_i)}\quad\quad \forall A_i\in \mathcal{P}$$ $\mathcal{P}$ is a countable partition that generates $\mathcal{G}$ . And I have the following questions: Some of the other materials give this definition as a conditional expectation of X with respect to the whole $\sigma$ -field $\mathcal{G}$ , which without indicating the small omega, i.e.: $$\mathbb{E}_\mathbb{P}(X|\mathcal{G})=\sum_{i\in I}\frac{\mathbb{E}_\mathbb{P}(X\mathbb{1}_{A_i})\mathbb{1}_{A_i}}{\mathbb{P}(A_i)}.$$ I personally support this definition, because $\mathbb{E}_\mathbb{P}(X|\mathcal{G})(\omega)$ seems to be a partial condition and it should be equivalent to $\mathbb{E}_\mathbb{P}(X|\mathcal{G})\mathbb{1}_{A_i}=\mathbb{E}_\mathbb{P}(X|A_{i})$ . So which one is the correct definition? Why we have two indicator functions over there? As per my logic, $\mathbb{E}_\mathbb{P}(X|\mathcal{G})=\sum_{i\in I}\mathbb{E}_\mathbb{P}(X|A_i)=\sum_{i\in I}\frac{\mathbb{E}_\mathbb{P}(X\mathbb{1}_{A_i})}{\mathbb{P}(A_i)}.$ I don't know where is the other $\mathbb{1}_{A_i}$ comes from.","['conditional-probability', 'measure-theory', 'conditional-expectation', 'filtrations']"
4014262,Tournament bracket for a 4-player game and 13 players in total,"We have a tournament in which 13 players participate, and matches are played in groups of 4 players (i.e. it's a 4-player game). There are 13 rounds in total, each player skips one round, and the other 12 are divided into three matches of 4 competitors. So, for example, if we denote players by 1-13:
in round 1, player 1 skips that round, 2-3-4-5 is one match, 6-7-8-9 the other one, and 10-11-12-13 the last one. In the end, every player would play in 12 (out of 13) rounds. Is it possible to make a bracket so that every player plays every opponent exactly three times in total across the duration of 13 rounds? (In the example above, in Round 1, player 2 played once against players 3, 4 and 5, and zero times against all of the others.)","['graph-theory', 'combinatorial-designs', 'combinatorics']"
4014319,"What's the word for being able to ""pull something out of a function""?","If I have something like $(5x + 7x) = x(5 + 7),$ I've ""factored"" out the $x$ . With linear functions you can say $f(a \cdot x) = a \cdot f(x)$ ""by linearity"", you've (for lack of better phrasing) ""pulled out the multiplication"". Or for preimages of sets, the preimage of union = union of preimage $f^{-1}(\cup_i A_i) = \cup_i f^{-1}(A_i)$ . Is there a general term for saying some operation inside a function works in a similar way? Something like $$f(A \odot B) = f(A) \odot f(B)$$ where $\odot$ is some general operator. I guess we could say ""linearity in $\odot$ "", is that standard?","['functions', 'terminology']"
4014387,Grouping vertices of a graph,"Suppose $G$ is a graph with $4n$ vertices. Prove that one can partition the vertices into two groups with $2n$ vertices, such that that there is an even amount of edges between the two groups. How can one approach this problem? Induction doesn't seem to work and it is just so strange.","['graph-theory', 'combinatorics', 'discrete-mathematics', 'contest-math']"
4014390,Limit of Sequence??,"The sequence: $-2, 0, -1, 0, -1/2, 0, -1/4, 0, -1/8, 0, -1/16, 0\ldots$ Heyy. So I was a little confused about this question. How would this have an existing limit if it keeps alternating between a value and $0$ ? I know if you have a sequence of $1, -1, 1, -1\ldots$ that would not have a limit. So why is the sequence above have a limit? Thank you in advance for any help!","['limits', 'calculus', 'sequences-and-series']"
4014418,Calculation for Standard Deviation Given a Gaussian Distribution,"Quick background: I work as a Software Engineer in the Embedded Systems space, and my primary job function has me working with signal processing algorithms. I feel pretty good when it comes to statistics, but this one comment left in a codebase by a former employee has me stumped. The calculation is supposed to be for the standard deviation of a signal to noise difference, and is described as such: sigma = pow(1-normdist(n), m). I haven't been able to place the equation to anything standard deviation related, at least from my knowledge. The 1-normdist(n) part remind me of calculating tail probabilities, but even then, taking a power of a probability doesn't make much sense to me. Any help and advice is welcomed!","['statistics', 'signal-processing', 'standard-deviation']"
4014437,"Closed subspace consisting of continuous functions in $L^{2}([0,1])$ is finite-dimensional","I wish to show that any closed subspace $M$ of $L^{2}[0,1]$ consisting of continuous real-valued functions is finite-dimensional (which amazes me quite a bit, I must admit). My approach proceeds as follows: Step 0. Note that $L^{2}[0,1]$ is a separable Hilbert space with the usual scalar product on $L^2$ . Moreover, since $M$ is closed, it is separable Hilbert, too, with inner product of $L^2([0,1])$ restricted to $M$ . Step 1. Let $I: (M, \| \cdot \|_{\infty}) \to (M, \| \cdot \|_{2})$ be the identity operator on $M$ . Since $I$ is bijective, by the open mapping theorem, $I^{-1}$ exists in the space of bounded linear operators on $M$ . Hence, for any $f \in M$ , we have $$ \| f\|_{\infty} = \| I^{-1} (f)\|_{\infty} \leq C \|f \|_{2} \quad \forall f \in M,$$ where $C := \| I^{-1} \|.$ Step 2. Now, let $t \in [0,1]$ and consider the linear functional $\ell_{t}: M \to \mathbb{R}, f \mapsto f(t).$ As $M$ is Hilbert, we may apply Riesz representation theorem to deduce that $\ell_{t} = \langle \cdot, g_t \rangle =: \ell_{g_t}$ for some $g_t \in M$ and $\|\ell_{g_t}\| = \| g_t\|_{2}$ . Moreover, $$ f(t) = l_t (f) =  \langle f, g_t \rangle \quad \forall t \in [0,1].$$ Using step 1, we may further conclude that $\| g_t \|_{2} \leq C$ . Step 3. By seperability of $M$ , we may suppose that $S = \{h_{n}: n \in \mathbb{N}\}$ is a orthonormal basis of $M$ . Then, Parseval's identity yields $$
\infty > \|g_t\|_{2} = \sum_{h \in S} | \langle h, g_t \rangle |^{2} = \sum_{h \in S} |h(t)|^{2}
$$ I am stuck in the last line. I want to argue that this sum has to be finite. Can we bound $|h(t)|$ from below? I guess, this suffices to conclude, right? Does this argument work?",['functional-analysis']
4014448,Is a quotient set the same as a partition?,So I've just started trying to teach myself some topology and im really confused on what a partition is and how it is in any way related to an equivalence relation. My main confusion however is that of the concept of a quotient set I've read that a quotient set is the set of all equivalence classes of a set namely if $S$ is some set with an equivalence relation $R$ Then is the quotient set $S/R=\{[a]  |  a \in S\} $ My question is how is this quotient set the same as a partition of a set?,"['elementary-set-theory', 'set-partition']"
4014452,"What is the maximal interval for the IVP $x'(t) = x(t)(1-x(t))$, with $ x(0) = x_{0}$?","I am new to ODEs and I'm stuck on this problem. I will write down my attempt.  The IVP in question is: \begin{cases}
x'(t) = x(t)(1-x(t)), \\
x(0) = x_{0}.
\end{cases} I applied separation of variables. I defined the function $f: X \mapsto \mathbb{R}$ , $X := X_{1} \times X_{2},$ \begin{align}
f(t,x) := g(t)\cdot h(x) = x(t)(1-x(t))
\end{align} such that $g: X_{1} \mapsto \mathbb{R}$ and $h: X_{2}\mapsto \mathbb{R}$ , with $g(t) := 1$ and $h(x) := x(1-x), X_{1} := \mathbb{R}$ and $X_{2} := \mathbb{R}$ .  Then I defined the solution $x : I \mapsto \mathbb{R}$ , defined on an interval $I \subset \mathbb{R}$ such that $0 = t_{0} \in I$ too. Firstly, I checked when $h(x) = 0$ : \begin{align}
h(x_{0}) = 0 \implies x_{0} = 0 \text{ or } x_{0} = 1.
\end{align} Hence, \begin{align}
\text{for } x_{0} = 0 \implies x(t) = 0; \\
\text{for } x_{0} = 1 \implies x(t) = 1.
\end{align} Afterwards, I solved the equation given that $h(\xi) \neq 0$ ( $\implies x_{0} \neq 0$ and $x_{0} \neq 1$ ), $$
\int_{x_{0}}^{x}\frac{1}{h(\xi)} d\xi = \int_{t_{0}}^{t} g(\eta) d\eta,
$$ i.e. \begin{align*}
\int_{x_{0}}^{x}\frac{1}{\xi(1-\xi)} d\xi = \int_{0}^{t}  d\eta
&\implies (\ln\lvert\xi\rvert - \ln\lvert 1-\xi\rvert)\big\rvert_{x_{0}}^{x} = t \\
&\implies \ln\Big\lvert\frac{\xi}{1-\xi}\Big\rvert\Bigg\rvert_{x_{0}}^{x} = t \\
&\implies \ln\Big\lvert\frac{x}{1-x}\Big\rvert - \ln\Big\lvert\frac{x_{0}}{1-x_{0}}\Big\rvert = t \\
&\implies \ln\Big\lvert\frac{x(1-x_{0})}{x_{0}(1-x)}\Big\rvert = t  \\ &\implies \Big\lvert\frac{x(1-x_{0})}{x_{0}(1-x)}\Big\rvert = e^{t}.
\end{align*} I distinguished two cases: When $h(x_{0}) > 0 \implies h(x) > 0$ , so \begin{align*}
&\frac{x(1-x_{0})}{x_{0}(1-x)} = e^{t} 
\implies &x(t) = \frac{e^{t}x_{0}}{1-x_{0}(1-e^{t})}.
\end{align*} Here is where I'm having issues with finding the maximal interval. Since, $h(x_{0}) > 0$ then \begin{align*}
x_{0}(1-x_{0})>0 \implies
\end{align*} \begin{cases}
x_{0}>0 \\
1-x_{0}>0
\end{cases} $$ \textbf{or} $$ \begin{cases}
x_{0}<0 \\
1-x_{0}<0
\end{cases} which results in $$ 0 < x_{0} < 1 .$$ Additionally, $$x_{0} \neq \frac{1}{1-e^{t}}.$$ However, in this case, $x(t)$ exists for $t \in \mathbb{R} \setminus \{\ln(1- \frac{1}{x_{0}})\}$ , since $$ 1-x_{0}(1-e^{t}) \neq 0 \implies t \neq \ln\Big(1- \frac{1}{x_{0}}\Big) $$ which is a problem because the bounds on $x_{0}$ make it so that $\ln(1- \frac{1}{x_{0}})$ can't exist.
So, I'm wondering what I did wrong and it's confusing me a lot. For $h(x_{0})<0$ I got the same $x(t)$ but the bounds on $x_{0}$ are $$ x_{0} < 0 \cup x_{0} > 1 $$ which in this case works with $t \in \mathbb{R} \setminus \{\ln(1- \frac{1}{x_{0}})\}$ .
What confuses me even more, is that by differentiating $x(t)$ you get the ODE in the IVP, so it's presumably right (?).
I'm not sure if I get the wrong interval because I used the wrong approach, or I messed up the signs, or something else I'm not noticing. Any help/suggestions would be appreciated.","['integration', 'ordinary-differential-equations', 'initial-value-problems', 'analysis', 'calculus']"
4014487,Are these two definitions of base for a topology equivalent?,"I have some experience with metric spaces, but I never took a dedicated Topology class. I recently encountered the term ""base for a topology"", and looking on Wikipedia , there seem to be two definitions. The very first line of the article says: [Definition 1]: In mathematics, a base or basis for the topology $\tau$ of a topological space $(X, \tau)$ is a family $B$ of open subsets of $X$ such that every open set is equal to a union of some sub-family of $B.$ However, the first thing in the ""Definition and basic properties"" section is: [Definition 2] A base is a collection $B$ of open subsets of $X$ satisfying the following properties: The base elements cover $X$ . Let $B_1$ , $B_2$ be base elements and let $I$ be their intersection. Then for each $x$ in $I$ , there is a base element $B_3$ containing $x$ such that $B_3$ is subset of $I$ . I believe I have proved these definitions to be equivalent. But I want to make sure I haven't made any hidden assumptions, because I know that unlike metric spaces, General Topology can be very unintuitive for the outsider. Def 1 $\implies$ Def 2: Suppose $B$ is a base according to definition 1. Take any two base elements $B_1, B_2$ . Their intersection $I$ is an open set, so it is a union of base elements. If $x$ is in $I$ , then it is in the union of those base elements, so it is in at least one of them. Therefore $x$ is in a base element which is contained in the intersection. Def 2 $\implies$ Def 1: Suppose $B = \{B_i|i \in I\}$ is a base according to definition $2$ . Let $U$ be an open set of $X$ . The set $$S = \bigcup_{i \in I}U \cap B_i.$$ is a union of oepn sets, and we claim that $S = B$ . It is obvious that $S \subseteq B$ . To see the reverse, let $x$ be any element of $B$ . Since the base covers $X$ , there is a base element $b$ such that $x \in b$ . Therefore $x \in U \cap b$ , so $x \in S$ .",['general-topology']
4014510,Conditions for the existence of solutions to a peculiar Diophantine equation,"If $a$ and $b$ are even natural numbers, how can I determine when the equation $$a+kb =2^n$$ has at least one solution where $k$ and $n$ are natural numbers? It's necessary for $a$ and $b$ to not have any common prime factors other than $2$ . Is that sufficient as well? Any direction pointing would be much appreciated!","['number-theory', 'diophantine-equations']"
4014517,In how many ways can we enumerate from 1 to 20 the sides of a icosahedral,In how many ways can we enumerate from 1 to 20 the sides of a icosahedral? Consider the sides indistinguishable. I thought about starting off by fixing 1 to one of the sides and then choosing the number of its opposite side but I can't go much further than that,"['permutations', 'combinatorics']"
4014574,Evaluate integral with no elementary antiderivative $\int_{0}^{\infty} \frac{\ln x}{x^2+x+1} dx $,"This is a followup from my previous question here . I was asked to determine if the following integral converges or diverges. If the integral converges, I was asked to determine the value. $$\int_{0}^{\infty} \frac{\ln(x)}{x^2+x+1} dx $$ . With help from the question I posted above, I managed to figure out my comparison (Thanks!). I definitely know this converges now. How do I find the value though? As far as I can see, this integral has no elementary antiderivative. This is a standard intermediate calculus class which does do not calculus of residues or anything complicated like that so I'm not how to proceed there. Wolfram alpha says that $$\int_{0}^{\infty} \frac{\ln(x)}{x^2+x+1} dx = 0$$ and that makes a lot of sense! However, I can't find a way to justify what the value should be rigirously. Can someone help out here?","['integration', 'calculus', 'convergence-divergence']"
4014588,Line graph $L(G)$ is bipartite if and only if the graph is isomorphic to path or even cycle.,Let $G$ be a connected graph. Line Graph $L(G)$ is bipartite if and only if the graph is isomorphic to path or even cycle. I have shown that if the graph is a path or even cycle then the line graph is bipartite. I am having difficulty in proving the other part.,"['graph-theory', 'combinatorics', 'discrete-mathematics']"
4014594,When does a system of polynomial equations have infinitely many solutions?,"It seems intuitively correct to say that a system of polynomial equations has finitely many solutions if there are as many equations as there are variables in the system. However, how would you prove that a system of polynomial equations with fewer equations than variables has infinitely many solutions (corresponding to the ""under-determined"" case in linear systems)? How would you characterize when a subset of equations ""coincide"" (you can combine several to get another, like linear dependence in linear systems)? For context, I'm in the middle of an abstract algebra course, and we've just finished Chapter 7 of Dummit & Foote (though this question isn't yet directly related to the course, that's just my background knowledge).","['systems-of-equations', 'algebraic-geometry', 'abstract-algebra', 'polynomials', 'computer-algebra-systems']"
4014622,blowup in finite time of $\frac{dx}{dt}=1+x^{10}$,"I feel like this is extremely silly question. It is a question from Strogatz' Nonlinear Dynamics and Chaos
I want to show that $\frac{dx}{dt}=1+x^{10}$ blows up in finite based on the fact that $\frac{dx}{dt}=1+x^2$ blows up in finite time. Well clearly, $1+x^2 < 1+x^{10}$ but then I realized that this is only true for $x > 1$ (in absolute value) So what if $x<1$ ?
Is there a nice and elegant way to do this problem without explicitly solving for a solution?",['ordinary-differential-equations']
4014623,Help integrating the contour integral $\oint_C \frac{e^{\frac{1}{z}}}{z(1-qz)}dz$ around the unit circle,"Consider the integral $$\oint_C \frac{e^{\frac{1}{z}}}{z(1-qz)}dz$$ where C is the anti-clockwise oriented unit circle and q is a complex constant. I have no clue how to integrate this. I tried using the residue theorem but it just doesn't come out correctly. Any help would be appreciated. The reason I am stuck is because it seems like there are an infinite amount of poles at z =0, so am very confused","['integration', 'complex-analysis', 'contour-integration', 'exponential-function']"
4014624,Applying the Normal Equations to solve the Linear Regression Problems.,"I am new to machine learning and I am currently studying the gradient descent method and its application for linear regression. An iterative method known as gradient descent is finding the linear function: $$
J(\theta)=\underset{\theta}{\operatorname{argmin}}\frac{1}{2}\sum_{i=1}^{n}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)
\tag1$$ However, I came to notice of an explicit non-iterative scheme in $\text{Andrew Ng's}$ lecture notes right here: https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf for which he concluded in page $11$ that the gradient descent is now equivalent to finding the vector $\theta$ where: $$
\theta=(X^{T}X)^{-1}X^{T}y
\tag2$$ Terminology: $\theta$ are the parameters. $n$ are number of training examples. $x$ are input features and $y$ are output variables. $(x^{(i)},y^{(i)})$ is the $i^{th}$ training example. $X\in\mathbb{R}^{m\times n}$ is the design matrix where $\mathbf{X}=\begin{bmatrix}
--(x^{(1)})^{T}-- \\
--(x^{(2)})^{T}-- \\
\vdots\\
--(x^{(n)})^{T}--
\end{bmatrix} $ I have two questions. $(1)$ : Say I have $(1,2)$ , $(2,1.5)$ , and $(3,2.5)$ I wish someone can demonstrate this procedure to find $\theta$ by solving $\theta=(X^{T}X)^{-1}X^{T}y$ $(2)$ : I would really hope if a Python,MATLAB, or C $++$ algorithm for this procedure can be provided.","['linear-regression', 'gradient-descent', 'linear-algebra', 'algorithms']"
4014631,Hypothesis Testing Biased vs Unbiased Coin with One Flip [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question You're given a coin to flip and you only can flip it once. The null hypothesis is that it is unbiased. The alternate hypothesis is that it is twice as likely to land heads than it is tails. Create a powerful test with 90% confidence and calculate the power. If you can help me with that question, it would be much appreciated. Thank you!","['binomial-distribution', 'statistics', 'normal-distribution', 'hypothesis-testing']"
4014697,"Does $(\sqrt{2x^{2}})(\sqrt{6x}) $ simplify to $2x\sqrt{3x}$, or to $2|x|\sqrt{3x}$?","I am having trouble simplifying these expressions. $$(\sqrt{2x^{2}})(\sqrt{6x}) $$ The textbook states that for n is even $$|a| = \sqrt[n]{a^n}$$ The answer key says that the answer is $$ 2x\sqrt{3x}$$ But when I use the rules stated in the textbook, I get $$ 2|x|\sqrt{3x}$$ because $|a| = \sqrt[n]{a^n}$ . This has happened a couple of times where I simplify to an absolute value variable where the answer key says it's just a variable. Where am I missing a step?",['algebra-precalculus']
4014713,Does nuclear norm decrease as some elements in a matrix are set zero?,Let $M$ be a generic nonzero real matrix and $M_{0}$ is constructed by replacing some elements in $M$ with zero. Let $||\cdot||_{*}$ denotes the nuclear norm operator. Is it true that $||M||_{*}\geq ||M_{0}||_{*}$ regardless of the replacement rule?,"['matrices', 'normed-spaces', 'linear-algebra', 'nuclear-norm']"
4014782,Why should we care about sequence in real analysis?,"I'm self-learning Real analysis through the book Real Analysis by Terence Tao. There is a concept of sequence which I don't fully understand its intuition and motivation by constructing one. It is not quite natural for me. I see that the concept of sequence arise for the first time when constructing real number using rational number. And then it come back again and again when dealing with continuity, derivative,... My question is:
When first think about sequence (of object in general) I can think of many useless sequences which behave arbitrarily.
For example, the sequence $1, 3, -1, 2, 2, -1...$ doesn't mean anything (i choose these number randomly). It seems that the number of useless sequences is much more than that of useful sequence. So, does the concept of sequence naturally arise in real analysis ? Why is the concept of sequence extensively while there are way more useless sequences than the useful ones ? Thank you very much for your help!","['sequences-and-series', 'real-analysis']"
4014795,Limit of fourier series,"I want to compute the limit $f(x)$ of $$
f(n,x)=\sum_{k=1}^n\frac{1}{k^2+1}\sin(kx)
$$ and didn't succeed. I tried with the means used to compute the >>easy<< limits of trigonometric series like those listed in (e.g.) Bronstein, however, the above series is not in Bronstein. I plotted the graph with sagemath (see my post on ask.sagemath.org: https://ask.sagemath.org/question/55596/limit-of-fourier-series/ ). At least this suggests that f(x) exists and is not a polynomial. Is it possible to compute the above limit and if so, how?","['limits', 'fourier-series']"
4014818,"A matrix equation $Ax=0$ has infinite solutions, does $A^Tx = 0$ have infinite solutions?","I'm wondering whether a system with a transpose of a matrix has the same type of solution that the original matrix system has. If an equation $Ax=0$ equation has a unique solution, would a system with $A$ transpose instead of $A$ also have a unique solution? And what about with no solution, and infinite solutions?","['matrices', 'transpose', 'linear-algebra']"
4014828,compute $\int_\gamma F\cdot dr$ using Stokes,"Evaluate $$ \int_\gamma F\cdot dr$$ where $ F=(ye^x,e^x+x^3,z^5)$ and $ \gamma\ $ is the intersection between $x^2+y^2=1 $ and $z=2xy$ oriented in such a way that the orthogonal projection on the $xy$ plane is oriented counterclockwise. Here is how I have tried to solve this problem:
I used Stokes and first calculated the curl of F which is $(0,0,3x^2)$ and then  doted this with unit normal but it become zero, however the answer should not become zero. What am I doing wrong ?
Any suggestion would be great , Thanks","['integration', 'multivariable-calculus', 'line-integrals', 'stokes-theorem']"
4014834,"Convergence of Cauchy sequences in $(\ell^1, \|\cdot\|_\infty)$","Do Cauchy sequences in $(\ell^1, \|\cdot\|_\infty)$ converge in $\ell^1$ ? I feel that the answer is No , but I am not able to find a counterexample. If $x^{(n)}$ is a sequence in $\ell^1$ , then we have $$\sum_{i=1}^\infty |x^{(n)}_i| < \infty$$ for every $n\in\mathbb N$ . Also if $x^{(n)}$ is Cauchy w.r.t the sup-norm, then $$\forall\epsilon>0\exists N\in\mathbb N\forall m,n>N (\|x^{(m)} - x^{(n)}\|_\infty < \epsilon)$$ That is the definition, and that is all I can see. Any ideas?","['cauchy-sequences', 'analysis', 'real-analysis']"
4014877,Do antilinear maps have adjoints?,"Let $T: H \to H$ be a continuous antilinear map on a Hilbert space $H$ , that is $T(\alpha \xi + \eta)= \overline{\alpha} T \xi + T \eta$ for $\xi, \eta \in H$ and $\alpha \in \mathbb{C}$ . Does there exist an antilinear map $T^*: H \to H$ such that $$\langle T^* \xi, \eta\rangle = \langle \xi, T\eta\rangle$$ for all $\xi, \eta \in H$ (or a variation on this). I could try to mimique the ""usual"" proof for bounded linear maps, but maybe there is a smart way to avoid this kind of work by reducing to this case?","['hilbert-spaces', 'functional-analysis']"
4014931,Sufficient conditions for Hamilton paths,"I have a conjecture about the Hamiltonian path, expressed as follows. Is it correct? For a connected graph $G$ , if every vertex of $G$ is an end-vertex of some longest path of $G$ , then $G$ has a Hamilton path.","['graph-theory', 'hamiltonicity', 'discrete-mathematics']"
4014949,Counterexample on two identical random variables with Poisson sum,"Raikov's theorem states that if $X_{1}$ and $X_{2}$ are two independent random variables, such that $X_{1}+X_{2}$ has a Poisson distribution, then these two variables also have Poisson distribution. Is the same true when $X_{1}$ and $X_{2}$ are not necessarily independent, but identically distributed and have a symmetric joint probability distribution, i.e., $\mathbb{P}(X_{1} = i, X_{2} = j) = \mathbb{P}(X_{1} = j, X_{2} = i)$ for all $i,j$ ?","['statistics', 'poisson-distribution', 'probability-theory', 'probability', 'random-variables']"
4014952,What is the sample space of the best of seven series?,"Original question: In a soccer game, two teams are playing against each other in a best of seven series. The game ends when one team wins four games and each game corresponds to a win for the team. What is the sample space of the best of seven series? Answer: $2(1+4+10+20)=70$ My question is whether my answer or the answer above is correct. Even though I think my answer makes more sense I also think it is wrong because my answer is very different from the given answer. My method is to use the following formula and add the total number of ways to win then multiply the sum by $2$ to include the total number of ways to lose: If the number of games $= 4$ : $$\frac{n!}{n_1!n_2!...n_k!}$$ If the number of games $> 4$ : $$\frac{n!}{n_1!n_2!...n_k!}-1$$ where $n$ is the total number of letters and $n_1,n_2,...,n_k$ are possible duplicate letters. $W$ = {win} $L$ = {lost} Team wins $4$ games out of $7$ : (1 way) $$WWWW$$ Team wins $5$ games out of $7$ : (4 ways) $$LWWWW,WLWWW,WWLWW,WWWLW$$ $WWWWL$ (removed since this is the same as $WWWW$ ) $$\frac{5!}{4!}-1=4$$ Team wins $6$ games out of $7$ : (14 ways) $$\frac{6!}{4!2!}-1=14$$ Team wins $7$ games out of $7$ : (34 ways) $$\frac{7!}{4!3!}-1=34$$ So the sample space (total number of ways of winning/losing) is $$S=2(1+4+14+34)=106$$","['discrete-mathematics', 'combinatorics', 'probability', 'permutations']"
4014974,Understanding L1 and L2 norms,"I am not a mathematics student but somehow have to know about L1 and L2 norms. I am looking for some appropriate sources to learn these things and know they work and what are their differences. I am asking this question since there are lots of stuff on this matter in the internet and I am looking for a simple applicable one to understand this. To add more, I want to understand why/how/when these norms are used in optimization problems? And maybe, how these cost functions are solved.","['optimization', 'least-squares', 'linear-algebra']"
4014987,Understanding $(x)=(y)$ in rings that aren't domains,"Suppose $R$ is a commutative ring with $1$ . I would like to get a better understanding of the equivalence relation on $R$ $$x\sim y\iff(x)=(y)\iff\exists u,v\in R,~x=uy~\text{ and }~y=vx$$ where $(x)$ is the principal ideal generated by $x$ . Let us also write $$x\approx y\iff \exists u\in R^\times,~x=uy$$ Clearly $x\approx y\implies x\sim y$ holds in any ring. Question 1. Is there a nice description of $\sim$ equivalence classes as unions of $\approx$ equivalence classes ? When $R$ is a domain these two equivalence relations are identical. But some non domains also satisfy this property, for instance all rings $\Bbb{Z}/n\Bbb{Z}$ have this property, too. Below we prove moregenerally that rings of the form $R/\mathfrak{a}$ for $R$ PID have this property. This raises the question: Question 2. Can one characterize the rings $R$ in which the relations $\sim$ and $\approx$ are identical? One can make some simple observations, for instance $\sim$ equivalence classes agree with $\approx$ equivalence classes when $x$ is $0$ , a unit or irreducible. If $x$ is cancellable in the sense that $xu=0\implies u=0$ then the equivalence classes agree, too. If $x\sim y$ and $x$ is nilpotent, then $y$ is nilpotent and $x$ and $y$ have the same nilpotency index, but I'm not sure they are necessarily $\approx$ -equivalent. Question 3. For what types of elements of $R$ do the $\sim$ and $\approx$ equivalence classes agree ? When $R$ is a domain one has that $x$ is irreducible if and only if $(x)$ is maximal in the set of proper principal ideals of $R$ . In a general ring ""irreducible implies maximal"" holds. Question 4. What can be said about $x$ if $(x)$ is maximal in the set of principal ideals $<(1)$ ? Finally, in order to build intuition, I would like to see some examples of non domains where the relations $\approx$ and $\sim$ agree and examples where they disagree. Question 5. What are some good examples of non domains where the relations $\approx$ and $\sim$ agree? Where they disagree? Generalizing the example $\Bbb{Z}/n\Bbb{Z}$ of rings where $\approx~=~\sim$ The fact that the $\sim$ and $\approx$ define the same equivalence relation in the rings $\Bbb{Z}/n\Bbb{Z}$ can be generalized: Lemma. Let $R$ be a PID and $\mathfrak{a}<(1)$ a proper ideal of $R$ . Then $\sim$ and $\approx$ are identical in the quotient ring $R/\mathfrak{a}$ . I'm sure there is a more straightforward proof of this, but here goes. Proof. Since $R$ is a domain there is nothing to prove for $\mathfrak{a}=(0)$ . Suppose $(0)<\mathfrak{a}<(1)$ and let $x$ be a generator: $\mathfrak{a}=(x)$ . Since PIDs are UFDs we can factor $x$ as a product of irreducibles $x=\prod_{i=1}^np_i^{m_i}$ , and so $\mathfrak{a}=(x)=\prod_{i=1}^n\big(p_i^{m_i}\big)$ . Since irreducibles are primes in UFDs and nonzero prime ideals are maximal in PIDs, the nonzero prime ideals $(p_i)=\sqrt{\big(p_i^{m_i}\big)}$ are maximal, pairwise distinct and therefore pairwise coprime. It follows that the ideals $(p_i^{m_i})$ are pairwise coprime. The chinese remainder theorem yields $$R/\mathfrak{a}\simeq\prod_{i=1}^nR/\big(p_i^{m_i}\big)$$ Now for any family of rings $(R_i)_{i\in I}$ we have $$\Big(\prod_{i\in I}R_i\Big)^\times=\prod_{i\in I}R_i^\times$$ and for any rings $R_1,\dots,R_n$ the ideals of the product ring are products of ideals: $$
\begin{array}{ccc}
\displaystyle
\left\{
\text{ideals of }
\displaystyle\prod_{i=1}^n R_i
\right\}
&=&
\displaystyle
\left\{
\displaystyle\prod_{i=1}^n \mathfrak{a}_i
\text{ for ideals }\mathfrak{a}_i\subset R_i
\right\}
\\
\style{display: inline-block; transform: rotate(-90deg)}{\subseteq}
&&
\style{display: inline-block; transform: rotate(-90deg)}{\subseteq}
\\
\displaystyle
\left\{
\text{principal ideals of }
\displaystyle\prod_{i=1}^n R_i
\right\}
&=&
\displaystyle
\left\{
\displaystyle\prod_{i=1}^n \mathfrak{a}_i
\text{ for principal ideals }\mathfrak{a}_i\subset R_i
\right\}
\end{array}$$ These two elementary propositions show that it is enough to prove that $\sim$ and $\approx$ are equal for quotient rings of the form $R/(p^n)$ for primes $p$ and positive $n$ . Thus, let $p$ be a prime of $R$ and $n\geq 1$ a positive integer. The ring $R/(p^n)$ is local and artinian (because noetherian and zero dimensional) and all its ideals are principal. The proof of Theorem 8.5 in Atiyah-MacDonald shows that all its only ideals are the $(0)<\mathfrak{m}^{n-1}<\cdots<\mathfrak{m}<(1)$ for $k=0,\dots,n$ where $\mathfrak{m}=(p)/(p^n)$ . Now suppose $(x)=(y)$ . Then for some $k$ , $(x)=\mathfrak{m}^k$ and so there exists $u$ with $x=up^k$ . Since $\mathfrak{m}$ is the only prime ideal in $R/(p^n)$ we have $\sqrt{(0)}=\mathfrak{m}$ and $R/(p^n)=\Big(R/(p^n)\Big)^\times\sqcup \mathfrak{m}$ . Therefore, if $u$ were noninvertible it would belong to $\mathfrak{m}$ and so $x\in(p^{k+1})$ whence $(x)\subseteq\mathfrak{m}^{k+1}<\mathfrak{m}^k$ which is a contradiction. Therefore $u$ is invertible. Similarly there exists $v$ invertible with $y=vp^k$ and so $x$ and $y$ are associates. The paper referenced by @HansLundmark actually contains the preceding and generalizes it: $\sim$ and $\approx$ agree for all quasilocal rings, i.e. rings with only finitely many maximal ideals: Since the maximal ideals of $R/\mathfrak{a}$ are precisely the $(p_i)/\mathfrak{a}$ , $i=1,\dots,n$ , quotients of PIDs are quasilocal and thus have the property that $\sim$ and $\approx$ are identical.","['ring-theory', 'abstract-algebra', 'ideals']"
4015015,Show for the centroid $M$ of a triangle $ABC$ that $\vec{OM}=\frac13(\vec{OA}+\vec{OB}+\vec{OC})$,Show for the centroid $M$ of a triangle $ABC$ that $$\vec{OM}=\dfrac13\left(\vec{OA}+\vec{OB}+\vec{OC}\right)$$ where $O$ is an arbitrary point. I haven't studied position vectors and am very new to vectors so I would like a simple solution. Any help would be appreciated. I don't even know how to start. Maybe we can use the fact that the centroid divides each median in a ratio of $2:1$ ? Thank you!,"['euclidean-geometry', 'vectors', 'centroid', 'geometry', 'triangles']"
4015018,Help understanding the proof: $\forall k \in \mathbb N [(a_1+...+a_k)+a_{k+1}=a_1+...+a_{k+1}]$,"In Spivak's Calculus , one is asked to prove that $(a_1+...+a_k)+a_{k+1}=a_1+...+a_{k+1}$ I will write his proof (from the solution manual) verbatim, but then I have some pedantic questions at the end, which are the primary purpose of this post. The only extra piece of information Spivak provides is: $\text{P1}$ . $a+(b+c) = (a+b)+c \quad$ Associative Law for Addition and the following definition: $a_1+a_2+a_3+...+a_{n-2}+a_{n-1}+a_n:= a_1+(a_2+(a_3+...+(a_{n-2}+(a_{n-1}+(a_n)))...)$ However, it seems to me that he should have also mentioned that his bracket symbols $[$ and $]$ are presumably meant to represent different layers of the parentheses symbols $($ and $)$ . Base Case : $a_1+a_2 = a_1 + a_2 $ Assume : $(a_1+...+a_k) + a_{k+1}=a_1+...+a_k+a_{k+1}$ Prove the $k+1$ case: $(a_1+...+a_k + a_{k+1})+a_{k+2}=a_1+...+a_k+a_{k+1}+a_{k+2}$ \begin{align}
\text{ If the equation holds for } k \text{ then } \\
(a_1+...+a_k + a_{k+1})+a_{k+2}&=[(a_1+...+a_k)+a_{k+1}]+a_{k+2} \\
& \quad \quad \text{ by P1} \\
&=(a_1+...+a_k)+(a_{k+1}+a_{k+2}) \\
&\quad \quad \text{since the equation holds for } k \\
&=a_1+...+a_k+(a_{k+1}+a_{k+2}) \\
&\quad \quad \text{by the definition of } a_1+...+a_{k+2}\\
&=a_1+...+a_{k+2} \\
\end{align} First Question I do not understand Spivak's base case. Shouldn't his base case start as $(a_1)+a_2$ ? It seems to me the only way of going beyond this sentence is to first make the definition that $(x):=x$ . However, in making this definition, doesn't the base case already solve everything? Because any finite expression of $k$ terms can be lumped together, called $x$ , and then you'd have $(x)+a_{k+1}=x+a_{k+1}$ ...which is the whole point of this proof. Edit: A work around for this seems to be to dictate that this proof only operate on $k\geq 2$ . Because if so: We could say $(a_1+a_2) +a_3=a_1+a_2+a_3$ because by definition, $a_1+a_2+a_3=a_1+(a_2+a_3)$ and then we can invoke the associative property to PROVE . $(a_1+a_2) +a_3=a_1+a_2+a_3$ . Second Question The offending transition that has confused me is when Spivak writes: \begin{align}
&=(a_1+...+a_k)+(a_{k+1}+a_{k+2}) \\
&\quad \quad \text{since the equation holds for } k \\
&=a_1+...+a_k+(a_{k+1}+a_{k+2}) \\
\end{align} Now, I imagine what Spivak is trying to convey is that we can think of $a_{k+1}+a_{k+2}$ as being a single term...say $\alpha$ ...and that our assumption in our Assume step can be reframed as $(a_1+...+a_k) + \alpha=a_1+...+a_k+\alpha$ . From what I've learned in set theory, $a$ is actually a function with a domain of some subset in $\mathbb N$ and, in this context, a range that is a subset of $\mathbb R$ . Given this, I do not see why at all our initial assumption case of $(a_1+...+a_k) + a_{k+1}=a_1+...+a_k+a_{k+1}$ can be applied to $(a_1+...+a_k)+(a_{k+1}+a_{k+2})$ . In general, the quantity $(a_{k+1}+a_{k+2}) \neq a_{k+1}$ . So how exactly is this step actually being applied? Edit : The only thing I can think to do here is invoke ordinals ; I will explain. Originally, the formal definition of the proof I thought was: $\forall k \in \mathbb N [(a_1+...+a_k)+a_{k+1}=a_1+...+a_{k+1}]$ . However, to get around the issue of $(a_{k+1}+a_{k+2}) \neq a_{k+1}$ , it seems like I should instead write the proof as: $\forall k \in \mathbb N \Big [ \forall a \Big( [\text{func}(a) \land \text{dom}(a) \in \omega \land \text{ran}(a) \subseteq \mathbb R] \rightarrow (a_1+...+a_k)+a_{k+1}=a_1+...+a_{k+1} \Big) \Big]$ In doing this, when I arrive at my assumption case, I assume for some $k$ that: $\forall a \Big( [\text{func}(a) \land \text{dom}(a) \in \omega \land \text{ran}(a) \subseteq \mathbb R] \rightarrow (a_1+...+a_k)+a_{k+1}=a_1+...+a_{k+1} \Big)$ WITH THIS, when we arrive at the situation of: \begin{align}
&=(a_1+...+a_k)+(a_{k+1}+a_{k+2}) \\
&\quad \quad \text{since the equation holds for } k \\
&=a_1+...+a_k+(a_{k+1}+a_{k+2}) \\
\end{align} we can use our updated formula to tackle this. Specifically, because $\forall a \Big( [\text{func}(a) \land \text{dom}(a) \in \omega \land \text{ran}(a) \subseteq \mathbb R] \rightarrow (a_1+...+a_k)+a_{k+1}=a_1+...+a_{k+1} \Big)$ , there definitely exists one function, call it $a'$ , that has the first $k$ terms in common with $a$ but, as its $k+1$ term, has $a_{k+1} + a_{k+2}$ . That is to say: $a'_{k+1}=a_{k+1} + a_{k+2}$ . Then we can absolutely apply our assumption to produce: $a_1+...+a_k+(a_{k+1}+a_{k+2})$","['elementary-set-theory', 'proof-explanation', 'induction', 'logic']"
4015041,Is this proposition True? $[(∀x)A(x) → (∀x)B(x)] → [(∀x)(A(x) → B(x))]$,"Is this proposition True? $[(∀x)A(x) → (∀x)B(x)] → [(∀x)(A(x) → B(x)]$ I cannot think if there is a difference between the first implication and the last one. Therefore, I think the proposition is true, although I got someone saying that the proposition is false, again, for the proposition to be false, there must be a difference between $(∀x)A(x) → (∀x)B(x)$ and $(∀x)(A(x) → B(x))$ and I just can't think in one. This is one of the solution that I tried, but now I think that this is wrong, because if $(∀x)A(x)$ is false in the first implication, then so It's in the last one.","['propositional-calculus', 'logic', 'discrete-mathematics']"
4015084,"Show that $f(x,y) = \frac{x^2y^2}{x^2+y^2}$ is (totally) differentiable","I want to prove that $f(x,y) = \frac{x^2y^2}{x^2+y^2} , (f(0,0) = 0)$ is (totally) differentiable at $ \begin{pmatrix}  0 \\ 0 \end{pmatrix}$ . I want to use the criterion that a function which is continuously partially differentiable is differentiable. So I compute $\frac{\partial f}{\partial x}=\frac{2xy^4}{(x^2+y^2)^2}$ (The partial derivative $\frac{\partial f}{\partial y}$ works analogously). Going back to the definition of partial deriviative with respect to the $x$ direction, we see that $\frac{\partial f}{\partial x}(0,0)=0$ . So it remains to show that $\lim_{(x,y) \to (0,0)} \frac{\partial f}{\partial x}(x,y) = 0.$ I use the $\epsilon-\delta$ criterion: Let $\epsilon >0 $ . Let $\delta := \frac{1}{2} \epsilon$ . So let $d((x,y),(0,0)) = \sqrt{x^2+y^2}< \delta$ , say $\sqrt{x^2+y^2} = \delta'<\delta.$ This implies $|x|\leq\delta'<\delta, |y|\leq\delta'<\delta$ . Therefore: $|\frac{\partial f}{\partial x}(x,y)| \leq \frac{2(\delta')^5}{(\delta')^4} = 2\delta'<2\delta=\epsilon$ , as desired. Is this analysis correct?","['partial-derivative', 'multivariable-calculus', 'derivatives', 'rational-functions']"
4015103,How to prove the variance of this function of a normal random variable is decreasing?,"Partial answers found here and here . Suppose $\phi(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$ and $\Phi(x)=\int_{-\infty}^x\phi(t)dt=\frac{1}2 \text{Erfc}[-\frac{x}{\sqrt{2}}]$ where Erfc is the complementary error function. These are the density and distribution function of a standard normal random variable. Define the function $g(\mu)=\int_{-\infty}^{\infty}\Phi(x) \phi(x-\mu)dx$ . $g(\mu)$ is the expected value of $\Phi(X)$ when $X$ has a normal distribution with mean $\mu$ and variance 1. Differentiate under the integral to observe that $$g'(\mu)=\int_{-\infty}^{\infty}\Phi(x)(x-\mu) \phi(x-\mu)dx=\frac{\phi\left(\frac{\mu}{\sqrt{2}}\right)}{\sqrt{2}}$$ Since $g'(\mu)$ is always positive, $g(\mu)$ is an increasing function. Furthermore, define the function $h(\mu)=\int_{-\infty}^{\infty}(\Phi(x)-g(\mu))^2 \phi(x-\mu)dx$ . $h(\mu)$ is the variance of $\Phi(X)$ when $X$ has a normal distribution with mean $\mu$ and variance 1. I can shown that: $h(0)=\frac{1}{12}$ $h'(0)=0$ $h''(0)=-\frac{1}{2 \pi}+\frac{1}{2 \sqrt{3}\pi} \approx -0.06727$ $\lim_{\mu\rightarrow \infty}h(\mu)=0$ $h(\mu)=h(-\mu)$ so that $h$ is an even function. I can draw $h(x)$ for $x \in [0,4]$ I can also find all the terms of the Taylor series for $h(\mu)$ at $\mu=0$ . But, I cannot show the derivative is never $0$ for $\mu \ne 0$ .
Is it possible to prove that $h(\mu)$ is decreasing for $\mu>0$ ?","['integration', 'normal-distribution', 'calculus', 'derivatives', 'probability']"
4015168,How to proceed $\int \frac{dx}{(x-2) \left(1+\sqrt{7 x-10-x^2}\right)} $,$$I=\int \frac{dx}{(x-2) \left(1+\sqrt{7 x-10-x^2}\right)} $$ I have taken the second term of denominator (in square root) as $(x - 2)t$ . But cannot go further. Please suggest how to proceed or any alternate method to solve it.,"['integration', 'indefinite-integrals', 'calculus', 'substitution']"
4015169,Continuous and residual spectrum of a multiplication operator on $\ell^p$ in the non-Hilbert case $p\ne2$,"Let $a\in\ell^\infty$ , $p\in[1,\infty)$ and $$T:\ell^p\to\ell^p\;,\;\;\;x\mapsto ax.$$ It's easy to show that $\sigma_p(T)=\{a_n:n\in\mathbb N\}$ . How can we determine $\sigma_c(T)$ and $\sigma_r(T)$ ? And is there a nice characteriation of $\sigma_p(T')$ ? It really easy to show that $\mathbb C\setminus\overline{\sigma_p(T)}\subseteq\rho(T)$ . But beyond that, I'm only able to answer the question in the case $p=2$ . In that case $T$ is self-adjoint and hence $\sigma_r(T)=\emptyset$ from which we can conclude that $\sigma_c(T)=\overline{\sigma_p(T)}\setminus\sigma_p(T)$ .","['operator-theory', 'spectral-theory', 'functional-analysis']"
4015178,Why does the compression criterion require homotopy relative to the boundary?,"The Compression Criterion states the following: Let $(X,A,x)$ be a nested space triple and $$f: (D^n, \partial D^n, z) \longrightarrow (X,A,x)$$ a continuous map of space triples. Then the following are equivalent: $f$ is triple homotopic to $const_x$ . $f$ is triple homotopic, relative $\partial D^n$ , to a map with image in A. The direction 2. $\Rightarrow$ 1. works by concatenating the given homotopy with one that contracts $D^n$ to $z$ . The direction 1. $\Rightarrow$ 2. is the interesting one: Here one can picture a cylinder standing upright with a horizontal slice corresponding to the homotopy at a point in time. The bottom face is $f$ , the top face is $const_x$ . The boundary maps to $A$ . By blowing the bottom face into the cylinder as if it was made from rubber while fixing $\partial D^2 \times \{0\}$ , and doing this all the way until the former bottom faces is contained in the cylinder's boundary again, we can find a homotopy from $f$ to a map with image in $A$ , relative to the boundary. A practically identical proof should show that: $f$ is triple homotopic to a map with image in $A$ . implies 2. The picture is identical except that the top face maps into $A$ , instead of being $const_x$ .
In particular a stronger version of the compression criterion would be that the following are equivalent: $f$ is triple homotopic to $const_x$ . $f$ is triple homotopic, relative $\partial D^n$ , to a map with image in A. $f$ is triple homotopic to a map with image in $A$ . My question is the following: How come I have never seen this version of the compression criterion before, especially since for me, personally, this really helps with understanding relative homotopy? Or is this result not correct? In this case I must really be overlooking something. Any help is greatly appreciated.
Note that this question is a duplicate of Compression Criterion for $\pi_n(X,A,x_0)$. Why do we need homotopies $\text{rel} \ S^{n-1}$? , but since this is a 5 year old question with no answer and it seems like quite a relevant question, I am okay with possibly having this question deleted to get an answer to the original one.","['general-topology', 'homotopy-theory', 'algebraic-topology']"
4015221,Permutation involving 4 cups and 4 saucers.,"I study maths as a hobby.  I have come across this problem: On a shelf there are 4 saucers of different colours and 4 matching cups. In how many ways can the cups be arranged on the saucer so that no cup is on a matching saucer? I start off by saying the first cup can be placed on any of 3 saucers.  For the second cup, there are 3 choices, unless the first cup was placed on the second cup’s matching saucer, in which case there are only 2 choices.  That gives 8 outcomes so far.  But the answer in the book is 9.  So I know my method is wrong. I have seen similar problems posted on here but the solutions were too complex for me.",['probability']
4015266,Got $e^{-x}=\cos(ix)$ from Euler's formula. Where is my mistake?,"When I was messing around with Euler's formula, I came across this: $$e^{ix}=\cos(x)+\sin(x)i$$ Then, let $x$ be an imaginary value, $ix$ , so then: $$e^{i(ix)}=\cos(ix)+\sin(ix)i$$ which we can simplify to $$e^{-x}=\cos(ix)+\sin(ix)i,$$ but since $e^{-x}$ is a real value for all real inputs, then $\sin(ix)i$ must be equal to $0$ . so that means that $$e^{-x}=\cos(ix)$$ This doesn't seem right, so could someone please point out where I made a mistake?","['algebra-precalculus', 'exponential-function', 'trigonometry', 'complex-numbers']"
4015293,Second derivative near a minimum positive?,"If I have a smooth real valued non-constant function $f$ with isolated minimum at $x_M$ does that mean that close to $x_M$ its second derivative must be positive? I.e. must there exist $a > 0$ so that $$
\begin{align}
f''(x_M + \varepsilon) > 0 \\
f''(x_M - \varepsilon) > 0
\end{align}
$$ for all $\varepsilon \in (0, a]?$ If not can someone give an example?","['optimization', 'maxima-minima', 'derivatives', 'real-analysis']"
4015375,"For any conditionally convergent series $\sum _{n=1}^\infty a_n,\ \exists\ k\geq 2\ $ such that the subseries $\sum _{n=1}^\infty a_{nk}$ converges.","A subseries of the series $\displaystyle\sum _{n=1}^\infty a_n$ is defined to be a series of the form $\displaystyle\sum _{k=1}^\infty a_{n_k}$ , for $n_k \subseteq \Bbb N$ . Prove or disprove: For any conditionally convergent series $\displaystyle\sum _{n=1}^\infty a_n,\ \exists\ k\geq 2\ $ such that the subseries $\displaystyle\sum _{n=1}^\infty a_{kn}$ converges. Certainly, every conditionally convergent series has some subseries that converge (decreasing alternating terms), and some subseries that diverge (the subseries of all of the positive terms; also the subseries of all of the negative terms). $$$$ My original ""answer"" which I now doubt is a counter-example to the proposition: Hint: Find a subset $\ A \subset \mathbb{N}\ $ with the property that, for each $\ n \in \mathbb{N},\ $ the set $ \{kn\}_{k \in \mathbb{N} }$ has finitely many elements in common with $A.$ Answer: The prime numbers is the standard example of such a subset of $\mathbb{N}$ with the property in the hint. This point is to make the 2nd, 3rd, 5th, 7th, ... members of our sequence have one sign, and every other member of our sequence have another sign. For example, $$ a_1 =  \frac{1}{1},\ a_2 =  \frac{-1}{4},\ a_3 =  \frac{-1}{4},\ a_4 =  \frac{1}{3},\ a_5 =  \frac{-1}{4},\ a_6 =  \frac{1}{5},\ a_7 =  \frac{-1}{6},\ a_8 = a_9 = a_{10} =  \frac{1}{3} \times \frac{1}{7},\ a_{11} = \frac{-1}{8},\ ... $$ This is simply the alternating harmonic series split up in a way that (I thought) answers the question. That was my answer, but now I'm not sure the series above is a valid counter-example. I constructed it so that every subseries of the form $\displaystyle\sum _{n=1}^\infty a_{kn}$ contains finitely many negative numbers. And then I'm not sure what my logic was. It was either that "" every subseries of a monotonic divergent series diverges "", but now I realise that this is not what the link says, and is also not true. Or my reasoning was that, for each $k \in \mathbb{N},\ $ every subseries $\displaystyle\sum _{n=1}^\infty \frac{1}{kn}$ of the harmonic series, diverges, which is true, but the subseries of my series defined above do not match these subseries of the harmonic series. So the problem remains open...","['real-analysis', 'conditional-convergence', 'absolute-convergence', 'sequences-and-series', 'convergence-divergence']"
4015438,Quotients of finite C-star algebras: are they finite?,"As I was trying to answer this question, I thought of the following: If $A$ is a unital, finite $C^*$ -algebra and $I$ is an ideal in $A$ , then is $A/I$ a finite $C^*$ -algebra? Recall that a unital $C^*$ -algebra is called finite when the implication $$p\sim_{M-vN}1_A\implies p=1_A$$ holds, i.e. every isometry in $A$ is a unitary. Besides the main question, I realized something: I do not know what $C^*$ -algebras are finite or not. For example, I know that abelian $C^*$ -algebras are stably finite, as it is (briefly) discussed in this post, and finite dimensional $C^*$ -algebras are (stably) finite too, but that's all I know. So a bonus question is: what are some interesting examples of finite $C^*$ -algebras besides abelian and finite dimensional ones? Note that in the answer of the linked question I am using a particular feature of the ideal $\sum_nA_n$ in $\prod_nA_n$ , something that we cannot do in the general case.","['c-star-algebras', 'functional-analysis', 'operator-algebras']"
4015463,"What is the probability that a random walk on $\mathbb{Z}^2$ will hit $(1,0)$ before $(2,0)$?","Suppose we have a 2-dimensional simple random walk: we start at $(0,0)$ , and at every step, we add a random unit vector in one of the four cardinal directions selected independently and uniformly. It is well-known that this procedure will with probability $1$ hit every element of $\mathbb{Z}^2$ infinitely often. Thus, it makes sense to ask about the probability that such a walk will hit $(1,0)$ before $(2,0)$ . Running some Monte Carlo simulations, it looks like the walk first lands on $(1,0)$ something like $70\%$ of the time, but I don't have much confidence about the accuracy of these simulations since I cannot actually run them all to completion and have to either throw out the unfinished trials or make a guess as to how they will conclude. Some more precise simulations show that the walk first lands on $(1,0)$ with probability at least $0.607$ and on $(2,0)$ with probability at least $0.153$ . Is there a known exact value for this probability, and how can it be computed in general for any two points on the square lattice? I'm also interested in a general formula for the probability of encountering $n$ points in a given order.","['random-walk', 'probability']"
4015486,Prove that the minimum of a functional doesn't exist,"Prove that there is no smooth solution ho the minimization problem: $$\mathcal{L} (u)= \int_{0}^1 e^{-u'}+u^2 dx$$ Where the admissible space is $X =\{ u \in \mathcal{c}^2 [0,1] | u(0)=0,  u(1)=1
       \} $ UPDATE: GOAL: I am trying to define a sequence of functions $u_n(x)$ in $X$ s.t their integrals $\mathcal{L}(u_n) \to 0$ What I have done so far: Define $u_n(x)$ in the following manner: $$u_n (x)= \begin{cases}
0, \quad 0 \leq x\leq 1-\frac{1}{n} \\[2ex]
ax^2 +bx+ c, \quad 1-\frac{1}{n}<x \leq 1 
\end{cases}$$ where $n=1,2,3,..$ . Require that $u_n$ satisfy: \begin{align}
 u_n(1-\frac{1}{n})=0\\
 u_n(1)=1\\
 u_n'(1-\frac{1}{n})=0\\
\end{align} We then have: \begin{align}
a+b+c=1\\
a(1-\frac{1}{n})^2+b(1-\frac{1}{n})+c=0\\
2a(1-\frac{1}{n})+b=0 
\end{align} Find $a,b,c$ then: $$u_n (x)= \begin{cases}
0, \quad 0 \leq x\leq 1-\frac{1}{n} \\[2ex]
n^2x^2 -2n(n-1)x+ (n-1)^2, \quad 1-\frac{1}{n}<x \leq 1 
\end{cases}$$ \begin{align}
\int_0^1 e^{-u_n(x)'} + u_n^2(x) dx &= \int_0^{1-\frac{1}{n}} e^{0} + 0 dx +  \int_{1-\frac{1}{n}}^1 e^{-(2n^2 x -2n(n-1))} + (n^2x^2 -2n(n-1)x+ (n-1)^2)^2 dx\\& = 1- \dfrac{1}{n}+ \dfrac{\mathrm{e}^{-2n}\left(\left(2n+5\right)\mathrm{e}^{2n}-5\right)}{10n^2}
\end{align} $$\dfrac{\mathrm{e}^{-2n}\left(\left(2n+5\right)\mathrm{e}^{2n}-5\right)}{10n^2} \to 0$$ and $$1-\dfrac{1}{n} \to 1$$","['variational-analysis', 'calculus-of-variations', 'euler-lagrange-equation', 'real-analysis', 'functional-analysis']"
4015504,Determinant Identity: Elegant Solution Please.,"Can anyone provide a proof of this identity: $$\begin{vmatrix}
(b+c)^2 &a^2&a^2\\
b^2&(a+c)^2&b^2\\
c^2&c^2&(a+b)^2\\
\end{vmatrix}=2abc(a+b+c)^3 ?$$ Ideally I would like a series of row column operations resulting in the formula.
No brute force calculations. Note that letting $a=0$ we get two rows equal. So $abc$ is a factor. But I have unable to produce this by a simple series of operations, I mean to get one row or column all of whose entries are divisible by $a$ . Getting divisibility by $(a+b+c)^2$ is also not hard just subtract one column from the other two. After that however I have been unable to make any further progress. Hopefully someone here is smarter than I.","['determinant', 'linear-algebra']"
4015538,$\cot 30^\circ = \cot 20^\circ - \operatorname{cosec} 80^\circ$ (?) [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Prove that, $\cot 30^\circ = \cot 20^\circ - \operatorname{cosec} 80^\circ$ I could not do that by trying heart and soul. Please solve it.",['trigonometry']
4015569,Proving that the Laguerre polynomials do indeed solve the differential equation,"I am trying to show that the Laguerre differential equation, given in my homework problem as $xL''_n(x)+(1−x)L'_n(x)+ nL_n(x) = 0$ , is indeed solved by the Laguerre polynomials in their closed sum form: $L_n(x)=\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!} x^k$ I've tried substituting the sum into the equation but I haven't been able to resolve it: $xL''_n(x)+(1−x)L'_n(x)+ nL_n(x)$ $ = x \sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!}k(k-1) x^{k-2} + (1-x)\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!}k x^{k-1} +n\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!} x^k$ $=\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!} [xk(k-1)x^{k-2}+(1-x)kx^{k-1}+nx^k]$ $=\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!} [(k^2-k)x^{k-1}+kx^{k-1}-kx^k+nx^k]$ $=\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!} [k^2 x^{k-1}+(n-k)x^k]$ $=\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k!} [x^{k-1}(k^2+(n-k)x)]$ What am I doing wrong? How would I show that this equals zero? Or is this the completely wrong approach? I've seen various similar questions here but they always either work with a different version of the differential equation, or with recurrence relations - which I am hesitant to use, since my homework problem has not introduced the recurrence properties of the Laguerre polynomials so I'd have to derive and prove them before being allowed to use it in my proof. Beyond the closed sum form I am also given the Rodrigues form of the polynomials: ${\displaystyle L_{n}(x)={\frac {\mathrm {e} ^{x}}{n!}}{\frac {\mathrm {d} ^{n}}{\mathrm {d} x^{n}}}{\bigg (}x^{n}\mathrm {e} ^{-x}{\bigg )}}$ But substituting those into the differential equation only leads me to terms with $L_{n+1}$ and $L_{n+2}$ . Can anybody tell me how to approach this? Is there any way to prove that the polynomials in either of these two forms satisfy the above differential equation? Or is there no way around the recurrence relations and shall I prove these first, even though they were mentioned neither in my lecture nor the homework problem?","['laguerre-polynomials', 'ordinary-differential-equations', 'real-analysis', 'polynomials', 'quantum-mechanics']"
4015573,$\bar{\partial}$ operator of a function,"Let $\bar{\partial}$ be the Cauchy–Riemann operator that is for $z=x+i y$ , $$\bar{\partial} = \frac{1}{2i}\left( i\partial_x - \partial_y\right).$$ Now assume that $z=u+ve^{ia}$ , where $u$ and $v$ are real variables, $a\in (0,1)$ a real constant, and we want to calculate $\bar{\partial} f(z)$ in terms of $\partial_u$ and $\partial_v$ . My question is that the following claim correct? $$\bar{\partial} f(z)=\frac{1}{2 i \sin a} \left( e^{i a} \partial_u f-\partial_v f\right).$$ In my attempt, I used the chain rule. I found the same first term on r.h.s, but I got a different term for the second. Edit: Here is my calculation: Write $z=u+ve^{ia}=(u+v\cos a)+i (v\sin a)=u'+iv'$ . Then $$\bar{\partial} f(z)=\frac{1}{2 i} \left(i \partial_{u'} f-\partial_{v'} f\right).$$ By the chain rule, we have $\partial_{u'} f=\partial_{u} f + \frac{1}{\cos a} \partial_{v} f$ and $\partial_{v'} f=-\cot a \;\partial_{u} f + \frac{1}{\sin a} \partial_{v} f$ . When replacing we get $$\bar{\partial} f(z)=\frac{1}{2 i \sin a} \left( e^{i a} \partial_u f- (i \tan a -1)\partial_v f\right).$$","['complex-analysis', 'coordinate-systems', 'derivatives', 'chain-rule']"
4015584,Difficulty with question about countable product of copies of $\mathbb{N}$,"I had a lot of trouble trying to figure this problem out. Can someone explain how to properly do this? Problem:Assume that $\mathbb{N}$ has the usual order. Let $\mathbb{N}^{\omega}$ denote the Cartesian product of a countable number of copies of the space $\mathbb{N}$ . It can be endowed with the dictionary order in a natural way. Show that $\mathbb{N}^{\omega}$ with the dictionary order topology is uncountable, is not well ordered, and any set that does not have a least element does have a limit point. Edit:I realized I showed a misunderstanding for the first part of the problem since for some reason I assumed each $s_{i,j} \in \{1,...,9\}$ instead of $\mathbb{N}$ . Attmempt:Suppose that $\{S_1,S_2,...\}$ is an enumeration of the elements of $\mathbb{N}^{\omega}$ .Where $f:\mathbb{N} \rightarrow \mathbb{N}^{\omega}$ given by $f(n)=S_n$ , is bijective.We will produce an element in $\mathbb{N}^{\omega}$ that is not on this list. Then there is no surjection from $\mathbb{N}$ onto $\mathbb{N}^{\omega}$ , and our assumption that $f$ is bijective is false.
A list of elements in the enumeration is given by: $$S_1=(s_{1,1},s_{1,2},...)$$ $$S_2=(s_{2,1},s_{2,2},...)$$ $$\vdots$$ $$S_n=(s_{n,1},s_{n,2},...)$$ $$\vdots$$ Where $s_{i,j} \in \mathbb{N}$ for all $i,j \in \mathbb{N}$ .
Consider the element $A=(a_1,a_2,...) \in \mathbb{N}^{\omega}$ such that $a_k=7$ if $s_{k,k} \in \{1,2,3,4,5,6\}$ , and $a_k=3$ if $s_{k,k} \in \{7,8,9\}$ . $f$ is bijective so $A \in \mathbb{N}^{\omega}$ must appear on the list. However, the $k$ 'th component of $a_k$ of $A$ and $s_{k,k}$ of $S_k$ differ by between $4$ and $7$ , so $A$ does not occur on this list and $\mathbb{N}^{\omega}$ is uncountable.
We will construct a set of elements that has no least element.  Let $S=\{S_1,S_2,...\}$ be the infinite set with elements given by: $$S_1=(1,2,3,4,5,6,7,...)$$ $$S_2=(1,1,2,3,4,5,6,...)$$ $$S_3=(1,1,1,2,3,4,5,...)$$ $$S_4=(1,1,1,1,2,3,4,...)$$ $$S_5=(1,1,1,1,1,2,3,...)$$ $$\vdots$$ Then the succesor of every element in the set is less than it's predecessor in the lexicographical ordering. Since there is no smallest element in this nonempty subset of $\mathbb{N}^{\omega}$ , $\mathbb{N}^{\omega}$ is not well ordered. Let $S$ be any subset of elements in $\mathbb{N}^{\omega}$ that does not have a limit point. Note for the part below $(\downarrow$ ).I ran out of time, and had to turn in but knew it was wrong. I felt like it was wrong, because for a decreasing sequence of elements (in the dictionary order) must have reach a point where all the components are identical, up to a point where, the the components become eventually constant, to a point. However I had difficulty expressing this mathematically. Any subset $S$ of $\mathbb{N}^{\omega}$ without a minimum contains a sequence of elements of the form $(a_1,a_2,a_3,...)$ , where $a_i=a_j$ for $i \neq j$ holds for all terms after some index $t$ for all indices $n$ with $1\leq t \leq n<k$ , and $a_k>a_n$ . Consider the element in $\mathbb{N}^{\omega}$ having each entry the element $a_n \in \mathbb{N}$ after $a_t$ . Then any open set in $\mathbb{N}^{\omega}$ containing this element, contains elements in $S$ . Since $S$ has no minimum there is an element $(a_1,a_2,...) \in S$ where $a_{t+1}=\dots=a_n$ for each natural number $n$ , so that any open interval containing the element with all entries $a_n$ must contain an element of this form.","['elementary-set-theory', 'order-theory', 'general-topology']"
4015598,"What is the higher dimensional analogy of $x^3, x^4$, etc.?","We know that a paraboloid, $x_1^2 + x_2^2 = x^T x$ is the generalization of a quadratic $x^2$ . Is there some function similar to $x^Tx$ that can be used to represent cubic, quartic, quintic...polynomials? I was thinking of $x x^Tx$ for $x^3$ but obviously that doesn't work. Or perhaps $x_1^3 + x_2^3$ is the natural generalization that you simply cannot be put into a dot product between two vectors.","['graphing-functions', 'multivariable-calculus', 'calculus', 'linear-algebra', 'vector-analysis']"
4015660,Suppose $g(u) = \int_{2}^{u} \sqrt{1 + t^3} \ dt$. Find the value of $(g^{-1})'(0)$ if it exists.,"I want some verification to see if I am doing this question right. Here is the question, and my attempt: Suppose $g(u) = \int_{2}^{u} \sqrt{1 + t^3} \ dt$ . Find the value of $(g^{-1})'(0)$ if it exists. So first I found the derivative of $g(u)$ , which was $$g'(u) = \sqrt{1 + u^3}$$ Then I let $v = g'(u)$ . The domain of $g'$ is $u\in[-1, \infty)$ and the range of $g'$ is $v\in[0,\infty)$ . Now solve for $u$ , so \begin{align*}
v &= \sqrt{1 + u^3} \\
v^2 &= 1 + u^3 \\
u^3 &= v^2 - 1 \\
u &= \sqrt[3]{v^2 - 1}
\end{align*} So the inverse is $u = \sqrt[3]{v^2 - 1}$ , or $(g^{-1})'(v) = \sqrt[3]{v^2 - 1}$ . The domain of $(g^{-1})'$ is $v\in[0,\infty)$ , and the range for $(g^{-1})'$ is $u\in[-1,\infty)$ . Now find $(g^{-1})'(0)$ , so sub $v = 0$ . \begin{align*}
(g^{-1})'(v) &= \sqrt[3]{v^2 - 1} \\
(g^{-1})'(0) &= \sqrt[3]{0^2 - 1} \\
(g^{-1})'(0) &= \sqrt[3]{-1} \\
(g^{-1})'(0) &= -1 \\
\end{align*} Since the value $-1$ is within the range, this value exists. Therefore, the value of $(g^-1)'(0)$ is $-1$ . This is my attempt, not sure if it is correct...Would like some suggestions or advice, if it is off somewhere.","['integration', 'calculus', 'inverse-function', 'derivatives']"
4015666,"Proof that a Markov matrix, $M$, has$ |\det(M)|=1$ iff $M$ is a permutation matrix","I've read in a number of places (eg here ) that if a matrix $M$ is Markov (aka stochastic), then $|\det(M)|\leq 1$ , with equality $|\det(M)|=1\iff$ $M$ is a permutation matrix. It is fairly easy to show the first part of this result, based on the eigenvalues of $M$ being of magnitude 1 or less , but I'm struggling with the second part. If anyone could provide a proof (or reference) of this result, I would be much obliged.","['matrices', 'determinant', 'linear-algebra']"
4015741,Solve $(x+1)^{63}+(x+1)^{62}(x-1)+\cdots+(x-1)^{63}=0$,"I want to find the solutions of $(x+1)^{63}+(x+1)^{62}(x-1)+\cdots+(x-1)^{63}=0$ . It is not hard to see $x=0$ is a root of the equation. but I don't know how to solve this equation in general. I can see terms of the equation looks very similar to binomial expansion of $[(x+1)+(x-1)]^{63}$ except the coefficient of each term is $1$ rather than $63\choose k $ (for $k=0,1,\cdots,63$ ). is it possible to use binomial theorem to solve the equation? (or other approaches)",['algebra-precalculus']
4015743,What is the Z-transformation of following discrete time signal of this?,"I am learning how $Z$ -Transforms work, but I have no encountered a situation in which the bound does not account for any signal. Take for example the following discrete time signal: \begin{cases}
x(-2)=4\\
x(-1)=0\\
x(0)=2\\
x(1)=4\\
x(2)=6\\
x(3)=8\\
x(4)=-2\\
\end{cases} I need to find the $Z$ -transform for $x(n)=0$ for $n$ from $-\infty$ to $-3$ and also for $n$ from $5$ to $\infty$ . My answer for this would be $0$ for both cases. Is this correct?","['z-transform', 'discrete-mathematics']"
4015777,Why does calculating probabilities intuitively result in differences from Bayesian calculations?,"As an example, say you're asked to solve for the following problem: You have three bags, labeled A, B, and C. Bag A contains two red marbles and three blue marbles. Bag B contains five red marbles and one blue marble. Bag C contains three red marbles only. Suppose a bag is randomly selected (again, with equal probability), but you do not know which it is. You randomly draw a marble and observe that it is blue. What is the probability that the bag you selected this marble from is A? I understand that you can calculate P(A|blue) using Baye's Theorem which would ask me to solve for P(blue|A) * P(A) / P(blue) which comes out to be 0.78. However, if I were to be asked this question outside the context of a class on bayesian statistics, I would think that the probability would be 0.75 or 3/4 since there are 4 blue marbles between the three bags. Since there is an equal chance of drawing from either bag, couldn't we disregard the probability of drawing from a given bag and focus on the distribution of blue marbles between bags? What is the error in my thinking here?","['statistics', 'bayesian']"
4015786,For discrete group $G$ and $H\leq G$. Show that $G$ also satisfies the Folner condition if $H$ satisfies it and $[G:H]<\infty$. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question A finitely generated group $G=\langle S \rangle$ is said to have the Folner condition if $\forall \varepsilon>0$ , there exists a finite subset $F\subset G$ such that $$\#((S\cup S^{-1})F\setminus F)<\varepsilon\# F,$$ where $\#S$ is the cardinality of a set $S$ . Or equivalently, $\forall\varepsilon>0$ , for any finite $T\subset G$ , there exists a finite $F\subset G$ such that $$\#(TF\setminus F)<\varepsilon\#F.$$ Let $G$ be a discrete group and let H be a subgroup of $G$ . Suppose that the index $[G:H]$ is finite and that $H$ satisfies the Folner condition. Prove that $G$ also satisfies the Folner condition.","['amenability', 'geometric-functional-analysis', 'functional-analysis', 'geometric-group-theory', 'group-theory']"
4015793,What happens with the convex hull of $6$ random points on a sphere?,"Given a collection of points on the sphere, we can consider their spherical convex hull: add all points on the shortest path between two points in the set, repeat until the resulting set does not expand. (For specificity, say that the shortest path between two diametrically opposite points is the whole sphere, but it doesn't matter for our purposes.) I am interested in the distribution of outcomes from taking the convex hull of $n$ random points on the sphere. When $n=3$ , the result is a triangle with probability 1. When $n=4$ , the result is a triangle with probability $\frac12$ , a quadrilateral with probability $\frac38$ , and the whole sphere with probability $\frac18$ . When $n=5$ , the result is a triangle with probability $\frac5{16}$ , a quadrilateral with probability $\frac5{16}$ , a pentagon with probability $\frac1{16}$ , and the whole sphere with probability $\frac5{16}$ . The above results can be derived given the moments of a random triangle's area and some cute counting arguments; see this question and answer for an instance of the sort of reasoning that produces these conclusions. Happy to elaborate on the proofs of these results in the comments if desired. However, the cute geometric arguments for $n\le 5$ start falling apart at $n=6$ . The probability that the convex hull of $6$ points will be a triangle is $\frac{15}{32}-\frac{15\ln(2)}{4\pi^2}\approx0.2054$ ; again, see the linked thread above for an explanation of why this is (basically, the issue is that the third moment of the area of a random spherical triangle gets messy in a way that earlier moments do not). On the other hand, the odds that the convex hull of $6$ random points is the entire sphere is exactly $\frac12$ ; see this answer for a citation and general formula. What are the chances that the convex hull is a quadrilateral, pentagon, or hexagon? Given the above results, it seems possible that one of these three probabilities may yet be rational (and thus possibly amenable to a natural geometric argument), even though of course not all three can be. Empirically, in a Monte Carlo simulation with $100000$ trials, there were $20369$ triangles, $22127$ quadrilaterals, $6880$ pentagons, $591$ hexagons, and $50033$ spheres.","['spherical-trigonometry', 'spherical-geometry', 'geometry', 'probability']"
4015810,How does Taleb's Kappa metric relate fat-tailed distributions to Gaussians?,"In Chapter 8 of Nassim Nicholas Taleb's Statistical Consequences of Fat Tails , Taleb defines a metric $\kappa$ . The relevant definitions are as follows: $$\mathbb{M}(n)=\mathbb{E}\left(\left\lvert\sum_{i=1}^n X_i-\mathbb{E}\sum_{i=1}^n X_i\right\rvert\right)$$ $$\kappa_{{n_0},n}=2-\frac{log(n)-log(n_0)}{log\left(\frac{\mathbb{M}(n)}{\mathbb{M}(n_0)}\right)}$$ where the $X_i$ are i.i.d. variates from some distribution with a mean. Thus $\mathbb{M}$ is the MAD for $n$ summands and $\kappa$ is a measure of the ""rate of convergence"" of $\mathbb{M}$ . Solving the $\kappa$ definition for $\mathbb{M}(n)/\mathbb{M}(n_0)$ yields: $$\frac{\mathbb{M}(n)}{\mathbb{M}{n_0}} = \left(\frac{n}{n_0}\right)^{\frac{1}{2-\kappa_{n_0,n}}}$$ Now, let $G$ be a Guassian distribution and $V$ an arbitrary distribution with a mean such that $\mathbb{M_g}(1)=\mathbb{M_v}(1)$ . For a given $n_g > 0$ , Taleb defines the corresponding $n_v$ to be the smallest value such that: $$\mathbb{E}\left(\left\lvert\sum_{i=1}^{n_v}\frac{X_{v,i}-m_v}{n_v}\right\rvert\right) \le \mathbb{E}\left(\left\lvert\sum_{i=1}^{n_g}\frac{X_{g,i}-m_g}{n_g}\right\rvert\right)$$ i.e., the smallest value where the MAD of the summands is no greater than $n_g$ summands of the Gaussian. Taleb then claims that: $$n_v=n_g^{-\frac{1}{\kappa_{1,n_g}-1}}$$ I don't follow that claim. Beginning with the inequality, I reason as follows: $$\frac{1}{n_v}\mathbb{M_v}(n_v) <= \frac{1}{n_g}\mathbb{M_g}(n_g) = \frac{1}{n_g}\sqrt{n_g}\mathbb{M_g}(1)=\frac{1}{\sqrt{n_g}}\mathbb{M_v}(1)$$ because the distributions have the same scale. So: $$\frac{1}{n_v}\frac{\mathbb{M_v}(n_v)}{\mathbb{M_v}(1)}\le\frac{1}{\sqrt{n_g}}$$ $$\frac{1}{n_v}n_v^\frac{1}{2-\kappa_{1,n_v}}\le\frac{1}{\sqrt{n_g}}$$ I can solve this inequality for $n_v$ , but it does not yield the value given by Taleb. What am I missing?","['distribution-tails', 'probability-theory']"
4015813,What kind of distribution should I use to this question?,"From every 200 students in High School, 20 students failed. If we were to pick 40 students from the class. Calculate: (a) the probability of getting 8 failed students.
(b) the expected value for the failed students out of 40 students. Should I use binomial distribution or hypergeometric?","['statistical-inference', 'statistics', 'binomial-distribution', 'probability']"
4015822,Connection between independence number and complete induced subgraphs.,"Question : I'm trying to prove that if the independence number of a graph $G=(V,E)$ is $n$ , then $V$ may be partitioned into $n$ sets $S_1, S_2, \dots, S_n$ such that $\bigcup\limits_{i}S_i=V$ and for each $S_i$ , the induced subgraph $G[S_i]$ is complete. My attempt at a constructive proof : Let $X$ be a maximum independent set in $G$ with $|X|=n$ . For each $x_i \in X$ , let $S_i$ be the set of vertices of the largest complete subgraph of $G[N(x_i) \cup \{x_i\}]$ . From here, I wanted to show that for each $v \in V \setminus X$ , $v$ must be in at least one such $S_i$ but I'm not sure how to go about this.","['graph-theory', 'discrete-mathematics']"
4015853,find $f$ such $f(f(x)+y)=2x+f(f(f(y))-x)$,"Find all function $f:R\to R$ ,such for any real number $x,y$ have $$f(f(x)+y)=2x+f(f(f(y))-x)$$ I have prove this one result: $$f(f(0))=f(0)$$ proof:let $x=y=0$ ,then we have $$f(f(0))=f(f(f(0)))$$ and take $x=f(f(0)),y=0$ ,we have $$f(f(f(f(0))))=2f(f(0))+f(0)$$ so we have $$f(f(0))=-f(0)$$ use this links Ivan Loh methods to prove two reslut: $f(x)$ is also injective proof: For convenience, let $P(x, y)$ represent the equation $f(f(x)+y)=2x+f(f(f(y))-x)$ . $P(x, -f(x))$ gives $f(f(-f(f(x)))-x)=f(0)-2x$ . Thus $f(x)$ is surjective. Suppose $f(a)=f(b)$ for some $a, b$ . Then $P(x, a)$ and $P(x, b)$ give $$f(f(x)+a)=2x+f(f(f(a))-x)=2x+f(f(f(b))-x)=f(f(x)+b)$$ Since $f(x)$ is surjective, $f(x+a)=f(x+b) \, \forall x \in \mathbb{R}$ . Now using the above equation and $P(a, y), P(b, y)$ give \begin{align}
2a=f(f(a)+y)-f(f(f(y))-a) & =f(f(a)+y)-f(f(f(y))-(a+b)+b) \\
&=f(f(a)+y)-f(f(f(y))-(a+b)+a) \\
& =f(f(b)+y)-f(f(f(y))-b) \\
& =2b
\end{align} Thus $f(x)$ is also injective. Finally, $P(0, y)$ gives $f(y+f(0))=f(f(f(y)))$ , so since $f(x)$ is injective, $$f(f(y))=y+f(0) \, \forall y \in \mathbb{R}$$ . then I can't  continue ,and I think this answer is $f(x)=x$ .so How to prove it.and this problem is  from   my teacher to give me exercise .Thanks for you help!
Now when post this post,This site automatically displays a prompt for a similar question to me(But it's different problem) : links",['functions']
4015863,Looking for different approach to find $\left(x_1+\frac1{x_1}\right)\left(x_2+\frac1{x_2}\right)\left(x_3+\frac1{x_3}\right)$,"If $x_1,x_2,x_3$ be roots of $x^3+3x+5=0$ , find the value of $$\left(x_1+\frac1{x_1}\right)\left(x_2+\frac1{x_2}\right)\left(x_3+\frac1{x_3}\right)$$ To solve this problem I expanded the expression and used Vieta's formula and got the answer $-\frac{29}5$ . but I wonder is there any other approach to solve this problem? I tried this way as an alternative ansewr: Consider $x_1,x_2,x_3$ be roots polynomial $P(x)=x^3+3x+5$ . then $\frac1{x_1},\frac1{x_2},\frac1{x_3}$ are roots of $P(\frac1x):$ $$P(\frac1x)=0\quad \rightarrow \quad P(\frac1x)=(\frac1x)^3+\frac3x+5=0\rightarrow 5x^3+3x^2+1=0$$ But I don't know whether it helps or not.","['contest-math', 'algebra-precalculus', 'roots', 'polynomials']"
4015885,Prove every subset of $\mathbb{N}$ is either finite or countable,"Prove every infinite subset of $\mathbb{N}$ is either finite or countable. I think I have defined the function correctly, however I am having trouble showing the function is injective and surjective. Suppose $A \subset \mathbb{N}$ is infinite. Define a function $f:\mathbb{N} \rightarrow A$ . Let $f(1)=\text{min}\{n \in \mathbb{N}| n \in A \}$ . Assume $f(n)$ has been defined for all $k<n$ . Choose $f(n)=\text{min}\{n \in \mathbb{N}|n \in A-\{f(1),...,f(n-1)\}\}$ . Then $f$ is a bijective function from $\mathbb{N}$ to $A$ . $f$ is one to one since if $m \neq n$ and $m<n$ then $f(n) \in \{A-\{f(1),...,f(n-1)\}\}$ however $f(m) \in \{f(1),...,f(n-1)\}$ . $f$ is onto since if $a \in A$ , $a=f(n)\in \text{min}\{n \in \mathbb{N}|n \in A-\{f(1),...,f(n-1)\}\}$ after inductively taking out the minimum element. I would like to know how I could better show that the defined function is one to one and onto. Also, do I need to even worry about the finite case? I know to prove a statement of the form $p \implies (q \ \vee r)$ you can prove $(p \ \wedge \sim  q) \implies r$ So I felt I could disregard the finite case by using this logical equivalence.",['elementary-set-theory']
4015904,Sum of squared probability distribution $\sum_c p_c^2$ is a measure of uncertainty?,"Can the sum of squared probabilities, i.e. $\sum_c p_c^2$ ,
be considered to be a measure of uncertainty? If so, does it have a mathematical name or theory? The form is similar to Shannon's entropy, $-\sum_c p_c \log(p_c)$ . It satisfies $\frac{1}{C} \leq \sum_c p_c^2 \leq 1$ , partly according to: Is there some general lower bound for sum of squared probabilities? , which shows that the lower bound is satisfied when $\vec{p}=\frac{1}{C}(1,\cdots,1)$ . $\sum_c p_c^2$ never exceeds 1, since $\sum_c p_c=1$ and $p_c^2 \leq p_c$ ( $\because 0 \leq p_c \leq 1$ ). An obvious case for the upper bound is $\vec{p}=(0,\cdots,0,1,0,\cdots,0)$ . Taking them together, it seems like that it can be considered to be a measure of uncertainty. It is appreciated if anyone could provide any names/theories. Best,","['statistics', 'entropy', 'probability', 'information-theory']"
4015947,How to show two properties about the Cantor Set,"Define $C_0=[0,1]$ and for $n\in\mathbb{N}$ , define $$C_n=C_{n-1}\setminus\bigg(\bigcup_{k=0}^{3^{n-1}-1}\bigg(\frac{1+3k}{3^n},\frac{2+3k}{3^n}\bigg)\bigg) $$ Then the Cantor set is defined as $$C=\bigcap_{n\in\mathbb{N}}C_n$$ Things I need to show: (1) Each $C_n$ is the disjoint union of $2^n$ closed sub-intervals of length $3^{-n}$ and that the endpoints of each $C_n$ are in $C$ . (2) For any distinct $x,y\in C$ , there exists non-empty, disjoint, open sets (open in in $C$ ) $A,B\subset C$ such that $A\cup B=C$ with $x\in A$ and $y\in B$ . For (1) , using induction seemed to me to be the best way to go about proving (1) , since I am unsure how to go about proving it for an arbitrary $n\in\mathbb{N}$ . However, using the inductive hypothesis with this definition turned out to be unwieldy and awkward, and did not yield any results with the methods I tried. Proof outline for (2): Let $x,y\in C$ and WLOG, let $x<y$ . Choose $N\in\mathbb{N}$ such that $$3^{N-1}(y-x)>2$$ Now $$3^{N-1}x<3^{N-1}x+2<3^{N-1}y $$ so there is an integer $k\in\mathbb{N}$ such that $$3^{N-1}x<k<k+1<y3^{N-1}$$ Then $$x<\frac{3k}{3^{N}}<\frac{3(k+1)}{3^{N}}<y $$ and finally, we have $$x<\frac{3k}{3^{N}}<\frac{1+3k}{3^N}<\frac{2+3k}{3^N}<\frac{3(k+1)}{3^N}<y $$ The idea here was to find one of the ""deleted"" intervals which separated $x$ and $y$ . If we set $A=(-1,z)$ and $B=(z,2)$ for some $z$ between $\frac{1+3k}{3^N}$ and $\frac{2+3k}{3^N}$ , then $A\cap C$ and $B\cap C$ will be open in $C$ , they will be disjoint, and their union will equal $C$ . Questions: How should I proceed in proving (1) ? Is this proof for (2) sufficient?","['cantor-set', 'real-analysis', 'alternative-proof', 'solution-verification', 'general-topology']"
4015989,"If an ODE has only periodic solutions and one equilibrium point, is that equilibrium point Lyapunov stable?","Consider the non-linear IVP $$\dot x=f(x),$$ $$x(0)=x_0.$$ where $f$ is locally Lipschitz and $$f(0)=0$$ and $$f(x)\ne 0\ \forall x\ne 0.$$ If all solutions to this IVP for various initial conditions are periodic does that mean that the origin is Lyapunov stable, i.e., $$\forall \varepsilon>0, \ \exists\delta>0, \ |x(0)|<\delta\implies|x(t)|<\varepsilon \ \forall t\ge 0.$$ My professor said yes in class and he ""justified"" this by drawing periodic orbits that are circular. I know that periodic solutions are bounded, but what if a solution close starts very close to the origin, moves far away from the origin, then returns to the same point? Is there an example of such a system or is my professor right? I was thinking of a solution like $$x(t)=\begin{cases}\frac{x(0)}{|x(0)|}\sin(t)+x(0)(1-\sin(t))&x\ne 0\\0&x=0\end{cases}.$$ $x(t)$ oscillates between $x(0)$ and a point with norm 1, so no matter how small $\delta$ is, the solution will never satisfy the definition of Lyapunov stability with $\varepsilon=\frac{1}{2}$ . The origin is an equilibrium point and the only equilibrium point and $x(t)$ satisfies the initial condition. $x(t)$ is differentiable, $2\pi$ periodic and satisfies the time-invariance property. I know that $x(t)$ does not work because it is not continuous with respect to the initial condition and $f$ is locally Lipschitz but I'm wondering if there is a way to alter $x(t)$ to make it a solution to an autonomous ODE and still have the same properties.","['stability-in-odes', 'periodic-functions', 'ordinary-differential-equations']"
4016011,Show circle through feet of two altitudes and midpoint of third side passes through center of another circle,"This is a question that was asked on the site a few days back and before an answer, it was deleted by the OP. While I was able to solve the problem, I feel there is a more elegant way. Question: In $\triangle ABC$ , $BF \perp AC$ and $AG \perp BC$ . Also $E$ is the midpoint of $AB$ . Show that the circumcircle of $\triangle EFG$ passes through the circumcenter of $\triangle CFG$ . My solution: Let $N$ be the intersection of the perpendicular bisector of $FG$ and the circumcircle of $\triangle EFG$ . I assumed circumradius of $\triangle ABC = R$ . Then I showed circumradius of $\triangle EFG = \frac{R}{2}$ . Using similarity of $\triangle CGF$ and $\triangle CAB$ , I showed $MN = k^2R$ where $FG:AB = CF:BC = CG:AC = k:1$ so circumradius of $\triangle CFG = kR$ . Finally I showed $FN = NG = kR$ which proves $N$ is the circumcenter of $\triangle CFG$ . Would like to see other solutions.","['euclidean-geometry', 'triangles', 'circles', 'geometry']"
4016035,Solve the Integral in the following set,"I have the Set $D=\{ (x,y) \in\mathbb{R}: |x|\le2,|x|\le y \le \sqrt{4-x^2}\}$ and I have to calculate $\int_{D}(x^2y+xy^2) dydx$ . So I cannot explain how hard it is for me to find the borders of the integral. I've tried to understand it but It's very hard for meand every time I think i have understood it, i do it wrong in another excercise. I have an exam tomorrow and of course I can't do wonders with understanding them. We often do them in the lectures that we turn them somehow to polar coordinates or so. However, my question is: is there any way, that I can read it from the definition of the set D for example, with x and y(without having to turn them in some other coordinates) and what suggestions could you give me in order to be able to do that? For example in this specific excercise, i thought that since $|x|\le2$ then $-2\le x\le 2 $ . Then I just took $y$ between $2$ (since the absolute value of $x$ can't be more than $2$ , and $\sqrt{4-x^2}$ .
So I calculated $\int_{-2}^{2}\int_{2}^{\sqrt{4-x^2}}x^2y$ . My answer is not the same as the answer in the book, so I got completely lost. I would be very thankful for some help. Annalisa","['integration', 'multivariable-calculus', 'multiple-integral']"
4016059,PCA for image analysis - Eigendecomposition VS SVD,"I have learned about two different ways of doing PCA, one using Eigendecomposition and the other (IMO more intuitive) using SVD. Although both are ""just"" a change of basis, I struggle to see how one can use Eigendecomposition for dimensionality reduction in the context of object recognition. Situation: Given $n$ images $f_i$ , each written as an $m$ vector (training set) and centered around $0$ , we want to find a smaller representation using Eigenfaces (i.e. using principal components of a face images). Using SVD: Write the images as columns into a matrix $M := [f_1, f_2, ..., f_n]\in \mathbb{R}^{m \times n}$ . Apply (economical) SVD to get $U \in \mathbb{R}^{m \times n}, \Sigma \in \mathbb{R}^{n \times n}, V^T \in \mathbb{R}^{n \times n}$ . Note that $U$ holds orthonormal vectors in columns (our principal components), while $\Sigma \cdot V^T $ holds our weights for each such vector. To reduce dimension, we just truncate the matrices: keep first $k$ columns of $U$ , the upper left $k \times k$ submatrix of $\Sigma$ and the first $k$ rows of $V^T$ . Call them $U_k, \Sigma_k, V^T_k$ respectively and $W_k := \Sigma_k \cdot V^T_k$ . For any ""new"" input image $g$ centered around $0$ , to check how similar it is with any stored image $f_i$ , we can just compute $||(W_k)_{:,i} - U_k^T \cdot g||_2$ . Hence, we only now need to do $\mathcal{O}(n \cdot k)$ operations rather than $\mathcal{O}(n \cdot m)$ to compare $g$ with our dataset $M$ . This approach makes sense, as I can really see where the $W_k$ come from. I also find it intuitive how to reconstruct an (approximation of an) image given its weight vector $w$ , as all we need to do is multiply it from the left with $U_k$ . If we want to do an alternative approach using Eigendecomposition, I don't see how the above goal is achieved. Based on what I know, I would compute PCA using Eigendecomposition the following way: Same as SVD Do autocorrelation $C_M := \frac{1}{n} M M^T$ Diagonalize $C_M$ , get $\Phi, \Lambda$ out, where $\Phi$ holds the Eigenvectors in columns, $\Lambda$ is diagonal matrix of eigenvalues $\lambda_i$ . Problem: The issue I am having is that the PCA using Eigendecomp. is performed on the autocorrelation matrix and not on the original data. This indirection step makes me lose track of: How to compare similarity of a new image $g$ with the existing ones, similar to step (4) of SVD-based PCA. How to go back from PCA to reconstructed images. For that, I reckon we would need to reverse Autocorrelation, which I am not sure how to do. So far, all ressources I checked out only use Eigendecomposition as a didactic tool and then quickly switch over to the SVD based approach. They include but are not limited to: Math Stackexchange Lecture notes A Tutorial on Principal Component Analysis","['statistics', 'svd', 'eigenvalues-eigenvectors', 'linear-algebra', 'matrix-decomposition']"
4016106,"Let ABCD be a tetrahedron of volume 1 and M,N,P,Q,R,S on AB,BC,CD,DA,AC,BD s.t. MP,NQ,RS are concurrent. Then the volume of MNRSPQ is less than 1/2.","So far, I have denoted $$\frac{MA}{MB}=a,\frac{BN}{NC}=b,\frac{CP}{PD}=c,\frac{DQ}{QA}=d,\frac{AR}{RC}=e,\frac{BS}{SD}=f$$ and then I have computed the volumes of the ""small"" tetrahedra $$V_1=V_{AMRQ}=\frac{ae}{(a+1)(e+1)(d+1)};$$ $$V_2=V_{BMSN}=\frac{bf}{(b+1)(f+1)(a+1)};$$ $$V_3=V_{CRNP}=\frac{c}{(c+1)(e+1)(b+1)};$$ $$V_4=V_{DPQS}=\frac{d}{(d+1)(c+1)(f+1)}.$$ Also, by Menelaus theorem for tetrahedra, one can deduce that $$abcd=1;~~~ af=ce; ~~~b=fde.$$ It remains to show that $$V_1+V_2+V_3+V_4\geq \frac{1}{2}.$$ The problem has a well known analogue in 2-D: for a triangle $ABC$ of area 1 and $AM, BN, CP$ concurrent cevians, the area of the triangle $MNP$ is less than 1/4. Later edit/update: I have managed to rewrite $V_k$ as follows: $$V_1={\frac {a}{ \left( a+1 \right)  \left( d+1 \right)  \left( cd+1
 \right) }};$$ $$V_2={\frac {b}{ \left( a+1 \right)  \left( b+1 \right)  \left( ad+1
 \right) }};$$ $$V_3= {\frac {c}{ \left( c+1 \right)  \left( b+1 \right)  \left( ab+1
 \right) }};$$ $$V_4= {\frac {d}{ \left( c+1 \right)  \left( d+1 \right)  \left( bc+1
 \right) }},$$ Plus, we still have $abcd=1$ .","['inequality', 'geometry', 'volume']"
4016136,Derivative of matrix multiplication's norm for linear regression,"I am trying to solve the derivative for the following function $f(θ)=0.5∥Xθ−y∥_2^2$ where X is a big(1000x2) matrix, θ is a 2x1 vector and y is a 1000x1 vector. I have so far realised that f can be reduced to 0.5(Xθ−y)(Xθ−y) and I tried applying both the chain rule and the product rule to come to the same result both times. The result being $f'(θ)= (Xθ−y)*\dfrac{df}{dθ}(Xθ−y)$ I've tried to do a lot of research on how to continue past this point and frankly from what I've seen I might have done everything wrong from the start. Could anyone give me any pointers?","['machine-learning', 'derivatives', 'linear-algebra', 'linear-regression']"
4016154,When is floor function homogeneous?,"I am trying to figure out, when is $\lfloor ax \rfloor = a\lfloor x \rfloor$ for $x,a \in \mathbb{R}$ , where $\lfloor x \rfloor$ is floor function, and I'm completely stuck. Is there a general rule, for which pairs of $a$ and $x$ is the equation above true? It's obvious that $a \in \mathbb{Q}$ but I couldn't get any further than that. Clearly, for fixed $a={b\over c}$ we have at least one $x$ , namely $x=c$ and every multiple of $c$ . But how to  find all such $a$ and $x$ ?","['ceiling-and-floor-functions', 'functions']"
4016176,compute $\iint_Y F.N\ dS $ with Gauss,"The question is: $$ \iint_Y F.N\ dS \quad Y=(x-z)^2+(y-z)^2=1+z^2,\ \  0\leq z\leq 1 \quad N=\text{pointing outward}
$$ $$ F=(y,x,1+x^2z)$$ Here is how I have tried to solve it: I tried to solve it with Gauss theorem like this, $\gamma=\text{upper lid}$ , $\sigma=\text{lower lid} $ $$ \iint_{Y+\sigma+\gamma}F.N\ dS=\iiint_K \text{div}f\  dA \iff \iint_Y F.N\ dS=\iiint_k-\iint_\sigma-\iint_\gamma$$ $$ \iint_\sigma=-\pi, \iint_\gamma=2+\pi$$ But I have trouble to integrate this triple integral $$ \iiint_kx^2 dxdydz$$ I don't know if I have been  calculating right so far, and how should I proceed? Any suggestion would be great, thanks","['integration', 'multivariable-calculus', 'line-integrals', 'surface-integrals']"
4016192,"General formula for $\int^1_0 x^\alpha \log(1-x)\operatorname{Li}_2 (x)\, \mathrm dx$","Consider a following definite integral $$I(\alpha)  = \int^1_0 x^\alpha \log(1-x)\operatorname{Li}_2 (x)\, \mathrm dx$$ Mathematica is able to provide a result for many $\alpha$ integers. See table here: $$\begin{array}{c}
 I(0)=-2 \zeta (3)+3-\frac{\pi ^2}{6} \\
 I(1)=-\zeta (3)+\frac{25}{16}-\frac{\pi ^2}{8} \\
 I(2)=-\frac{2 \zeta (3)}{3}+\frac{227}{216}-\frac{11 \pi ^2}{108} \\
 I(3)=-\frac{\zeta (3)}{2}+\frac{5443}{6912}-\frac{25 \pi ^2}{288} \\
 I(4)=-\frac{2 \zeta (3)}{5}+\frac{677309}{1080000}-\frac{137 \pi ^2}{1800} \\
 I(5)=-\frac{\zeta (3)}{3}+\frac{673333}{1296000}-\frac{49 \pi ^2}{720} \\
 I(6)=-\frac{2 \zeta (3)}{7}+\frac{229495199}{518616000}-\frac{121 \pi ^2}{1960} \\
 I(7)=-\frac{\zeta (3)}{4}+\frac{1824315697}{4741632000}-\frac{761 \pi ^2}{13440} \\
 I(8)=-\frac{2 \zeta (3)}{9}+\frac{16317321883}{48009024000}-\frac{7129 \pi ^2}{136080} \\
 I(9)=-\frac{\zeta (3)}{5}+\frac{1946452513}{6401203200}-\frac{7381 \pi ^2}{151200} \\
 I(10)=-\frac{2 \zeta (3)}{11}+\frac{64401732356723}{234300040128000}-\frac{83711 \pi ^2}{1829520} \\
\end{array}$$ Is there a closed form expression for $f,g,h$ in $I(\alpha) = f(\alpha) + g(\alpha)\pi^2 + h(\alpha)\zeta(3)$ ?","['integration', 'definite-integrals', 'special-functions', 'mathematica']"
4016241,Proving that a function has primitives,"Let f a differentiable function such that $\lim_{x\to\infty}\frac{f(x)}{x}$ = $\lim_{x\to-\infty}\frac{f(x)}{x}=0$ .Prove that the function $g:\Bbb R\to\Bbb R$ admits primitives when g is: $g(x)=f'(1/x)$ , for any $x\neq 0$ and $g(0)=0$ I tried to integrate the function g and to make the substitution $x:=1/x$ but it did not helped me out.","['integration', 'limits', 'functions', 'real-analysis']"
4016294,Smallest possible area for triangle,"I'm solving the following question: The two legs of a right triangle lie along the positive x and y axes.
The hypotenuse is tangent to the ellipse $2x^2 + y^2 = 1$ . What is the
smallest possible area for such a triangle ? This is my attempt at solving the problem: Let $a$ be the length of the x axis and $b$ be the length in it's y axis if the triangle. So it's area is $1/2 a.b$ Let's find the tangent of the ellipse: $2x^2 + y^2 = 1$ $4x + 2y \dfrac{dy}{dx} = 0$ $\dfrac{dy}{dx} = \dfrac{-2x}{y}$ Since the two points on the triangle are $(0,b)$ and $(a,0)$ , we can
find the slope of the line: $m = \dfrac{b-0}{0-a} = \dfrac{-a}{b}$ So the equation of hypotonuse is $y = \dfrac{-a}{b}x + c$ Since $(0,b)$ is one of the point in the line, we can substitute it in
the above line to find that $b = c$ . Solving it with the other
co-ordinate, we can find the following fact $b = a$ . So now area is $1/2 a.b = 1/2 a^2$ Now I used the first derivative test to find that the function attains
it's local minima at $0$ . So the smallest possible area for such a
triangle is zero. I have missed lots of steps in my above solution for brevity,
but I can expand if needed. My question: is zero
the right answer ?  Unfortunately, the book I'm using doesn't provide
the answer to this question.","['optimization', 'calculus', 'derivatives']"
4016367,"Why does this pattern work: $1 \cdot{1} = 1, 11 \cdot{11} = 121, 111 \cdot{111} = 12321\ldots$","I have recently learned about pattern that goes like this: $$\begin{align}
1^2 &= 1\\
11^2 &= 121\\
111^2 &= 12,321\\
1,111^2 &= 1,234,321\\
11,111^2 &= 123,454,321.
\end{align}$$ It is a very cool pattern, but after a bit it stops: $$1,111,111,111^2 = 1,234,567,900,987,654,321$$ My main question: Why does this pattern work? A side question that's less important: Is there an algebraic equation to describe this pattern?","['number-theory', 'arithmetic', 'sequences-and-series']"
4016511,Intuition behind picking group actions and Sylow,"A common strategy in group theory for proving results/solving problems is to find a clever group action. You take the group you are interested in (or perhaps a subgroup), and find some special set that your group can act on, usually by left multiplication or conjugation. Somehow studying this action makes the result you want clearer to see. The most obvious example of this is with proving the Sylow theorems. To prove the First Sylow theorem (that a $p$ -subgroup exists) you can let $G$ act by left multiplication on all subsets of $G$ that have size $p^n$ . To prove the Second Sylow theorem (that Sylow $p$ -subgroups are conjugate) you let $Q$ be any $p$ -subgroup, $P$ a Sylow $p$ -subgroup, and let $Q$ act on $G/P$ by left multiplication. To prove the Third Sylow theorem (that the number of Sylow $p$ -subgroups is $1\pmod p$ and divides $|G|$ ) you can consider both $G$ and some Sylow $p$ -subgroup $P$ acting on $\operatorname{Syl}_p(G)$ by conjugation. My question: is there any intuitive or natural way to come up with these group actions? To me, all three proofs seem magical -- if someone had told me which group action to consider, I could probably have completed the proofs myself, but I would never have come up with the appropriate action myself. More generally, are there any ways to ""see"" which group action might help for solving a specific problem?","['sylow-theory', 'algebraic-combinatorics', 'group-theory', 'group-actions', 'soft-question']"
4016560,Expand $\frac{\Gamma\left(\frac{x}{2}\right)}{\Gamma\left(\frac{x-1}{2}\right)}$ at $x=\infty$,"I used Wolfram Alpha for this problem and it gives me a ""Puiseux expansion"": $$\sqrt{x}-\frac{3 \sqrt{\frac{1}{x}}}{8}-\frac{7}{128}\left(\frac{1}{x}\right)^{3 / 2}-\frac{9\left(\frac{1}{x}\right)^{5 / 2}}{1024}+O\left(\left(\frac{1}{x}\right)^{3}\right)$$ which is exactly what I need. My only question is how one would be able to obtain this expansion manually.","['laurent-series', 'gamma-function', 'calculus', 'taylor-expansion', 'sequences-and-series']"
4016579,"Drake, Seven Axioms of the Algebra of Events","On page 3 of Drake Fundamentals of Applied Probability he lists The Seven Axioms of the Algebra of Events. $1. A \cup B = B \cup A \\
2. A \cup (B \cup C) = (A \cup B) \cup C \\
3. A\cap( B \cup C) = A \cap B \cup A \cap C \\
4. (A')' = A \\
5. (A\cap B)' = A' \cup B'\\
6. A\cap A' = \phi \\
7.A \cap U = A  $ I have two questions. First while Drake states that his choice is not unique is there a reason that he selected these seven and is seven the minimum possible number of axioms with which the Algebra of events can be expressed. Second he further states that any other relation in the algebra of events can be proved directly from these seven axioms with no additional information and in fact in the chapter problem section asks the student to do so. Does the author mean that all the other set relations in the Algebra of Events can be proved by direct manipulation of the seven axioms with out resorting to the method of $x \in A$ For example, proofs of $A \cup A' = U$ are given in this posting Proof of union of a set and its complement is equivalent to a universe but these proofs use the method of $x \in A$ and not direct manipulation of Axiomatic statements as required by Drake. How would I prove $A \cup A' = U$ or $A \cup A = A$ using only direct manipulation of Drake's Seven Axioms of the Algebra of Events?","['elementary-set-theory', 'measure-theory', 'logic', 'probability']"
4016585,"Given $n+1$ points, bound the product of the distances from one of them","We have $n+1$ real numbers $x_1,\cdots,x_{n+1}$ such that $-1\leq x_i\leq 1$ for all $1\leq i\leq n+1$ . I am wondering whether the following fact is true: There exists some $j$ such that $\prod_{\substack{i=1\\i\neq j}}^{n+1}\left |x_j-x_i\right |\leq \frac{n+1}{2^{n-1}}$ . Some base steps are easy. Assume that this holds for $n-1$ . Applying this inductive hypothesis to $\left\{x_i:i\in\mathbb{N}_{\leq n+1}\right\}\setminus\left\{x_j\right\}$ for each $j\in\mathbb{N}_{\leq n+1}$ , we conclude that for each $j\in\mathbb{N}_{\leq n+1}$ there exists $h\left(j\right)\in\mathbb{N}_{\leq n+1}\setminus\left\{j\right\}$ such that $\prod_{\substack{i=1\\i\neq j\\i\neq h\left(j\right)}}^{n+1}\left |x_{h\left(j\right)}-x_i\right |\leq\frac{n}{2^{n-2}}$ . Now, if for some $j\in\mathbb{N}_{\leq n+1}$ we have $\left |x_{h\left(j\right)}-x_j\right |\leq \frac{1}{2}$ , then $\prod_{\substack{i=1\\i\neq h\left(j\right)}}^{n+1}\left |x_{h\left(j\right)}-x_i\right |\leq \frac{1}{2}\cdot\frac{n}{2^{n-2}}\leq \frac{1+\frac{1}{n}}{2}\frac{n}{2^{n-2}}=\frac{n+1}{2^{n-1}}$ , which gives what we want. But what about the case in which $\left |x_{h\left(j\right)}-x_j\right |>\frac{1}{2}$ for all $j\in\mathbb{N}_{\leq n+1}$ ? Is there any other way?","['real-numbers', 'optimization', 'upper-lower-bounds', 'real-analysis']"
4016604,"Eliminating $x$, $y$, $z$ from $\frac{x^2-xy-xz}{a}=\frac{y^2-yx-yz}{b}=\frac{z^2-zx-zy}{c}$ and $ax+by+cz=0$","Here is a Math Tripos problem I cannot solve. Eliminate $x$ , $y$ , $z$ from the equations $$\frac{x^2-xy-xz}{a}=\frac{y^2-yx-yz}{b}=\frac{z^2-zx-zy}{c}$$ and $$ax+by+cz=0$$ I dont know if there is a general (practical) method for such problem. I think you just manipulate the equations to eliminate $x,y,z$ but have been unable to find the path in this question. The answer is $a^3+b^3+c^3=a^2(b+c)+b^2(a+c)+c^2(a+b)$ if that yields any insight.","['contest-math', 'algebra-precalculus']"
4016714,Divisibility by 7 Proof by Induction,"Prove by Induction that $$7|4^{2^{n}}+2^{2^{n}}+1,\; \forall n\in \mathbb{N}$$ Base case: $$
\begin{aligned}
7&|4^{2^{1}}+2^{2^{1}}+1,\\
7&|7\cdot 3
\end{aligned}$$ Which is true. Now, having $n=k$ , we assume that: $$7|4^{2^{k}}+2^{2^{k}}+1,\;\; \forall k\in \mathbb{N}$$ We have to prove that for $n=k+1$ that, $$7|4^{2^{k+1}}+2^{2^{k+1}}+1,\;\; \forall k\in \mathbb{N}$$ We know that if $a$ is divisible by 7 then $b$ is divisible by 7 iff $b-a$ is divisible by 7. Then, $$
\begin{aligned}
b-a &= 4^{2^{k+1}}+2^{2^{k+1}}+1 - (4^{2^{k}}+2^{2^{k}}+1)\\
&= 4^{2^{k+1}}+2^{2^{k+1}} - 4^{2^{k}}-2^{2^{k}}\\
&= 4^{2\cdot  2^{k}}+2^{2\cdot  2^{k}} - 4^{2^{k}}-2^{2^{k}}
\end{aligned}
$$ I get stuck here, please help me.","['induction', 'discrete-mathematics']"
4016729,Determining when $(\sin(\theta) - \cos(\theta))(2+\sin(\theta) \cos(\theta)) \leq 2$,"Question: $(\sin(\theta)-\cos(\theta))(2+\sin(\theta)\cos(\theta)) \leq 2 \qquad \text{ LHS} \\ $ Answer key: $\implies  \frac{1}{2}(\cos(\theta)-\sin(\theta)+2\sin(\theta)\cos(\theta)+3)(\sin(\theta)-\cos(\theta)-1) \leq 0 \qquad \text{ RHS} $ I verified RHS = LHS. However, to get to the RHS part, they have factored it somehow and I can't figure out an intuitive way to do so. If anybody has a more intuitive solution (does not have to be the same as what is given here), please provide it. From this step onwards we can easily solve the question because: $\implies  (\sqrt{2}\sin\left(\frac{\pi}{4} - \theta\right)+\sin(2 \theta)+3)(\sin(\theta)-\cos(\theta)-1) \leq 0\\ $ $ \implies  (\sin(\theta)-\cos(\theta)-1) \leq0 \because  (3 + \sqrt{2}\sin\left(\frac{\pi}{4} - \theta\right)+\sin(2 \theta)) >0 \text{ }\forall \theta \in \mathbb{R} \\ $ Doubt : I have no idea how the went from the LHS part to the RHS part. Is there an intuitive way to solve $(\sin(\theta)-\cos(\theta))(2+\sin(\theta)\cos(\theta)) \leq 2$ ? In this solution they have simply said: $$(\sin(\theta)-\cos(\theta))(2+\sin(\theta)\cos(\theta)) -2 \leq 0 \\ \implies 
\frac{1}{2}(\cos(\theta)-\sin(\theta)+2\sin(\theta)\cos(\theta)+3)(\sin(\theta)-\cos(\theta)-1) \leq 0 \\$$ Please provide a logical way to solve it. Thanks in advance. I'm looking for a no calculator solution where each step is motivated. And prove one factor is always positive or negative and use the other factor to find the region would be appreciated. Although, if there is another method, (as long as its intuitive) that works too","['algebra-precalculus', 'trigonometry', 'inequality']"
4016777,Can non-reduced fibers appear over a subset of codimension $\geq 2$?,"Suppose $f: X \to Y$ is a proper morphism of complex manifolds such that all fibers of $f$ are connected and of constant dimension $n = \dim X - \dim Y$ . Also suppose that $Z \subset Y$ is an analytic subset of codimension $\operatorname{codim}(Z, Y) \geq 2$ , such that $f$ is smooth over $f^{-1}(Y \setminus Z)$ . Question: Is it true that $f$ has reduced fibers?","['complex-geometry', 'algebraic-geometry']"
4016803,Is there a reverse of Baker-Campbell-Hausdorff formula?,"The BCH formula says that if $\exp(A)\exp(B) = \exp(C)$ where $C$ is a series in nested commutators of $A$ and $B$ , $$C = A + B + \frac{1}{2}[A,B] + \frac{1}{12}\Big(\big[A,[A,B]\big] + \big[B,[B,A]\big]\Big) + \cdots.$$ I was wondering if there was a way to read the equation backwards, where if you start with one exponential, $$\exp(A+B)$$ then is it it possible to get some expression with two (or more?) exponentials, for example $$\exp(A+B) = \exp(A)\exp(B)\exp(D).$$ I imagine that the extra $\exp(D)$ could very well be in front instead in the back, or it could be in the middle, or perhaps there are 3 extra factors that go in all 3 places. If there is, is there a specific formula for the extra factors in terms of $A$ and $B$ ?","['quantum-mechanics', 'group-theory']"
4016840,"Proof verification: $su(2)$ is not isomorphic to $sl(2,R)$","I’m trying to prove Lie algebra $sl(2,R)$ is not isomorphic to $su(2)$ by showing that all Lie algebra isomorphism will have determinant $0$ , hence a contradiction. However, my computation gives a ridiculous result that said such isomorphism always exist. Please help me checking what’s wrong here. I take the usual basis of $sl(2,R)$ such that $$[X_1,X_2]=X_2$$ $$[X_1,X_3]=-X_3$$ $$[X_2,X_3]=2X_1$$ And the usual basis of $su(2)$ such that $$[Y_i,Y_j]=\epsilon_{ijk}Y_k$$ Now suppose $sl(2,R)$ is isomorphic to $su(2)$ , then there must exist a set of basis in $sl(2,R)$ , which can be written as $$X’_1=a_1X_1+b_1X_2+c_1X_3$$ $$X’_2=a_2X_1+b_2X_2+c_2X_3$$ $$X’_3=a_3X_1+b_3X_2+c_3X_3$$ satisfies $$[X’_i,X’_j]=\epsilon_{ijk}X’_k$$ We then have $$[a_1X_1+b_1X_2+c_1X_3,a_2X_1+b_2X_2+c_2X_3]\\=a_1b_2[X_1,X_2]+a_1c_2[X_1,X_3]+b_1a_2[X_2,X_1]+b_1c_2[X_2,X_3]+c_1a_2[X_3,X_1]+c_1b_2[X_3,X_2]\\=a_1b_2X_2+a_1c_2(-X_3)+b_1a_2(-X_2)+b_1c_2(2X_1)+c_1a_2(X_3)+c_1b_2(-2X_1)\\=(2b_1c_2-2c_1b_2)X_1+(a_1b_2-b_1a_2)X_2+(c_1a_2-a_1c_2)X_3$$ Which implies $$a_3=2b_1c_2-2b_2c_1$$ $$b_3=a_1b_2-a_2b_1$$ $$c_3=a_2c_1-a_1c_2$$ The other $6$ conditions are obtained just by permutation $(1,2,3)$ . By above relationships, I finally get $$det(a_i,b_j,c_k)=\frac{1}{2}a_1^2+2b_1c_1= \frac{1}{2}a_2^2+2b_2c_2= \frac{1}{2}a_3^2+2b_3c_3\\=\frac{1}{2}a_1^2+\frac{1}{2}a_2^2+\frac{1}{2}a_3^2=b_1c_1+b_2c_2+b_3c_3 $$ by all possible cofactors expansions. This result seems not going to give $det(a_i,b_j,c_k)=0$ . I’m not sure what is going wrong here. I have checked my calculations more than $10$ times to make sure nothing goes wrong. Please help me, really thanks!","['matrices', 'lie-algebras', 'lie-groups']"
4016870,On the generalized Leibniz rule,"problem definition I have to evaluate in $z=0$ the $n$ -th derivative with respect $z$ of the product $f(z)\cdot z^k$ , where $f(\cdot)$ is a generic smooth function and $k$ is a given integer. I will use the short notation $F^{(n)}(z)$ to denote the $n$ -th derivative of the generic function $F(z)$ , so what I want is to compute for any $n\in\mathbb{N}$ \begin{equation}
[(f(z)\cdot z^k)^{(n)}]_{z=0}\triangleq \frac{\text{d}^n\left[f(z)\cdot z^k\right]}{\text{d}z^n}\Bigg|_{z=0}
\tag{1}
\end{equation} my attempt For the generalized Leibniz rule , holds for any $n$ \begin{equation}(f(z)\cdot z^k)^{(n)}=\sum_{i=0}^n \binom{n}{i}\cdot f^{(n-i)}(z) \cdot \left(z^k\right)^{(i)}\end{equation} so the problem consist into compute the $i$ -th derivative of the power $z^k$ . If I'm not wrong, \begin{equation}
\left(z^k\right)^{(i)}=\begin{cases}
\frac{k!}{(k-i)!}z^{k-i} & \text{if } i\leq k\\
0 & \text{otherwise}
\end{cases}
\end{equation} This formula says that the index $i$ of the previous summation cannot exceed the value $k$ . Anyway, $k$ is an external parameter and can be greater than $n$ : in this case the summation stops at the value $n$ ,
otherwise the summation stops at value $k$ .
So I would write \begin{equation}\begin{aligned}(f(z)\cdot z^k)^{(n)}&=\sum_{i=0}^{\text{min}(n,k)} \binom{n}{i}\cdot f^{(n-i)}(z) \cdot  \frac{k!}{(k-i)!} z^{k-i}\\
&=k!\cdot\sum_{i=0}^{\text{min}(n,k)} \binom{n}{i}\cdot \frac{1}{(k-i)!}\cdot f^{(n-i)}(z) \cdot z^{k-i}\\
\end{aligned}\end{equation} Now comes the problems. By setting $z=0$ turns out \begin{equation}\begin{aligned}
%
[(f(z)\cdot z^k)^{(n)}]_{z=0}
&=k!\cdot\sum_{i=0}^{\text{min}(n,k)} \binom{n}{i}\cdot \frac{1}{(k-i)!}\cdot f^{(n-i)}(0) \cdot 0^{k-i}\\
\end{aligned}\end{equation} from my prospective this expression is quite tricky because of the term $0^{k-i}$ . I'm tempted to write \begin{equation}
0^{k-i}=\begin{cases}1 & \text{if } i=k\\
0 & \text{otherwise}
\end{cases}
\end{equation} and consequently simplify the last summation as \begin{equation}\begin{aligned}
%
[(f(z)\cdot z^k)^{(n)}]_{z=0}
&=k!\cdot\sum_{i=0}^{\text{min}(n,k)} \binom{n}{i}\cdot \frac{1}{(k-i)!}\cdot f^{(n-i)}(0) \cdot 0^{k-i}\\
&=\begin{cases}
k!\cdot\binom{n}{k}\cdot \frac{1}{(k-k)!}\cdot f^{(n-k)}(0) \cdot 0^{k-k} & \text{if } n\geq k \\
0 & \text{otherwise}
\end{cases}\\
&=\begin{cases}
\frac{n!}{(n-k)!}\cdot f^{(n-k)}(0) & \text{if } n\geq k \\
0 & \text{otherwise}
\end{cases}\\
\end{aligned}\end{equation} question I don't have a precise question about my problem. Essentially I'm doubtful about my derivation because of the undefined power $0^0$ .","['summation', 'factorial', 'calculus', 'binomial-coefficients', 'derivatives']"
4016888,Uncountable chain of negligible sets,"Let $(E,\mathcal{A},m)$ be a (non empty) measure space, with $m$ a complete measure, it's easy to prove that an uncountable union of negligible sets does not need to be negligible or even measurable. But, now, let $(I,<)$ be a totally ordered set ( $I\neq \emptyset$ and I not countable) and $(A_i)_{i\in I}$ be an increasing family of negligible sets (for $m$ ) ( ie $\forall i\in I \ \ m(A_i)=0$ and $\forall (i,j)\in I² \ \ i<j \implies A_i \subset A_j$ ). Is $A=\bigcup_{i\in I} A_i$ measurable and $m(A)=0$ ?
Thanks.",['measure-theory']
4016920,Solving a puzzle: Graph where each node has degree 3,"I am trying to solve this puzzle: Problem:
The  Park is covered by a network of hiking paths. Goal: Find an algrithm by which you'll find (in finite time) the crossing at which the restaurant is located. Rules: The paths form a finite connected graph in which each crossing (that is, each crossing of paths) is of degree 3. The crossings are indistinguishable one from another: it is not possible to know which crossing one has arrived at, to identify the paths meeting there, or to recognize whether one has been there before. Strict rule against leaving markings in the park. It is not ok to mark crossings or path in any way. If you enter a crossing by one path you must leave it by a different path. You have initially arrived at some node by one of its incident paths. My questions: For rule 1: I cannot find any graph where every node has a degree of 3. At least one node has degree 2. 2nd question: If I cannot mark the paths where I have been, what can I do to find a sufficient solution? Just go right, right right and one time left?","['puzzle', 'discrete-mathematics']"
4016926,Find the height of the following rectangle,"We have a rectangle ABCD, and a point P on the diagonal AC. From P we see BC at an angle $\alpha$ . Knowing $\alpha$ , AP and AB, find BC. Here's a probably unnecessary illustration of the problem, with known data in red: I am lost with this, certainly, extremely simple problem. I might use the cosine law to define BC as a function of $\alpha$ , PC and PB, but I find no way of finding those latter. I thought of finding PB constructing a parallelogram including sides PB and AP, having BC as diagonal, but I'm stuck there since I, in my poor knowledge on solving parallelograms, I would need the other diagonal of the parallelogram to find PB. On the other hand, I looked where could I transpose $\alpha$ as to express its trigonometric functions using known information, but didn't found anything useful. Sorry for coming to you with high-school problems (I feel ridiculous to be asking this after three semesters of real analysis, one of complex, and one of linear algebra). I'd appreciate, in addition to a method of resolution, ressources that might help me get a comprehensive approach to planar, high-school like geometry.","['rectangles', 'trigonometry', 'geometry']"
4016933,Forming a group from power set pairs,"Question I want to form a group out of disjoint pairs taken from the power set $\mathcal{P}(X)$ over a finite set $X$ . An element of the group $G$ is a pair $u=(u_1,u_2) \in \mathcal{P} \times \mathcal{P}$ such that $u_1 \cap u_2 = \emptyset$ . Are there any ways to form a group on these disjoint pairs of elements? Attempt I thought to use the symmetric difference $a \Delta b = (a \cup b) \backslash (a \cap b)$ as done for the regular power set group , but for both pairs. Take $u,v \in G$ and the group operation gives $w$ defined as: \begin{eqnarray}
w = u \circ v &=& (u_1 \Delta v_1,\;u_2 \Delta v_2 \backslash [(u_1 \Delta v_1) \cap (u_2 \Delta v_2)])\\
&=&(u_1 \Delta v_1,\;(u_2 \Delta v_2) \backslash (u_1 \Delta v_1)).
\end{eqnarray} The term in the square brackets ensures that the new pair is disjoint: $w_1 \cap w_2 = \emptyset$ . This is closed in $G$ since $G$ contains every pair of disjoint sets. To form a group: (a) this has an identity $(\emptyset, \emptyset)$ , (b) each element is its own inverse (an involution), (c) I'm unclear about associativity. I tried showing that it's associative by expanding both $a \circ (b \circ c)$ and $(a \circ b) \circ c$ , but I'm running into some trouble. So far, I have: \begin{eqnarray}
a \circ (b \circ c) &=& a \circ (b_1 \Delta c_1,\; b_2 \Delta c_2 \backslash [(b_1 \Delta c_1) \cap ( b_2 \Delta c_2)])\\
&=& (a_1 \Delta b_1 \Delta c_1,\; a_2 \Delta (b_2 \Delta c_2 \backslash  [(b_1 \Delta c_1) \cap (b_2 \Delta c_2)])\\
\end{eqnarray} and \begin{eqnarray}
(a \circ b) \circ c &=& (a_1 \Delta b_1,\; a_2 \Delta b_2 \backslash [(a_1 \Delta b_1) \cap ( a_2 \Delta b_2)]) \circ c\\
&=& (a_1 \Delta b_1 \Delta c_1,\; (a_2 \Delta b_2 \backslash [(a_1 \Delta b_1) \cap ( a_2 \Delta b_2)])\Delta c_2)\\
\end{eqnarray} The left parts of the pairs match, but the right parts need some work. So we want to check whether the right parts are equal, i.e. that: $$
a_2 \Delta (b_2 \Delta c_2 \backslash  [(b_1 \Delta c_1) \cap (b_2 \Delta c_2)] = (a_2 \Delta b_2 \backslash [(a_1 \Delta b_1) \cap ( a_2 \Delta b_2)])\Delta c_2
$$ I'm getting stuck here just because this expands (in $\Delta$ ) to a lot of terms. Perhaps there's a smart to check this?","['elementary-set-theory', 'group-theory']"
4016945,When is the augmentation ideal of $\mathbb{Z}[G]$ principal?,"Let $G$ be a finite group and let $$I_G = \left\{\sum_{g \in G} a_gg \in \mathbb{Z}[G] : \sum_{g \in G}a_g = 0\right\}$$ be the augmentation ideal of the group ring $\mathbb{Z}[G]$ . If $G$ is cyclic, then $I_G$ is a principal ideal generated by $g - 1$ for $g$ a generator of $G$ . Is it true that if $I_G$ is principal, then $G$ must be cyclic? I know that $I_G$ is generated by $g - 1$ for $g \in G$ . If we assume that $x = \sum_{g \in G}a_gg \in I_G$ is a generator of $I_G$ , then for each $g_0 \in G$ there is some $\sum_{g \in G}b_gg$ such that $$\sum_{G \in G}a_gg \cdot \sum_{g \in G}b_gg = g_0 - 1.$$ This gives us a system of linear equations over $\mathbb{Z}$ which should have a solution if and only if the resulting matrix has rank $\vert G \vert - 1$ . The columns of this matrix are all permutations of each other, but I'm not sure how to connect the rank of the matrix to whether or not $G$ is cyclic (if this is even true).","['ring-theory', 'group-theory', 'abstract-algebra']"
4016969,"Solutions to $A^N+B^N=C^N \pm 1,\,$ for $N \geq 4$?","Is there a solution to $A^N+B^N=C^N \pm 1$ where $A,B,C,N\in\Bbb{N}$ , such that $A,B,C > 1,N \geq 4$ and $gcd(A,B)=gcd(B,C)=gcd(A,C)=1$ ? This question was inspired by this one: $A^X+B^Y=C^Z\pm 1$ Beal's conjecture ""almost"" solutions A related question was asked here before. Noam Elkies has published a paper on this subject. The table on page 15 of the paper and his extended table here have some interesting results. If I understood his work correctly, there are no solutions for $4 \leq N \leq 20$ and $C < 2^{23}=8,388,608$ . I also found a MathWorld article on 4th order diophantine equations, but it doesn't cover equations of this form. I also checked that the OEIS has no sequences for such Fermat near misses.","['number-theory', 'perfect-powers', 'diophantine-equations']"
