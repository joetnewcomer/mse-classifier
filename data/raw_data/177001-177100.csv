question_id,title,body,tags
3186473,"Interpolation, identity for the derivative","Let $f\in C^{n+1}([a,b])$ , $x_0,\dotso, x_n\in[a,b]$ pairwise different and $p\in \mathbb{P}_n$ the interpolation polynomial (of degree n) with $f(x_j)=p(x_j)$ for all $0\leq j\leq n$ Show that for every $j$ there exists $\xi_j\in [a,b]$ with $f'(x_j)-p'(x_j)=\prod_{i\neq j} (x_i-x_j)\cdot\frac{f^{(n+1)}(\xi_j)}{(n+1)!}$ I struggle to find the idea for a proof.
The only thing I got so far is, that I want to define a 'help-function', which might look like this: $h(x)=f(x)-p(x)-\frac{f'(x_j)-p'(x_j)}{\prod_{i\neq j}(x_j-x_i)}\cdot\prod_{i=0}^n (x-x_i)$ But this does not do it yet. And I doubt, that it is a correct approach anyways.
I wanted to give a function $h$ with at least $n+2$ roots. The function above has at least $n+1$ roots $x_0,\dotso, x_n$ . I also thought about using some mean-value-theorem. Of the differential calculus or even for integrals, but never thought it through. Can you give me a hint for this problem? I would like to solve it alone, but I am stuck here.
Is the idea with a 'help-function' correct? Thanks in advance.","['interpolation', 'derivatives']"
3186480,"If the empty set is a subset of every set, why write ... $\cup \{∅\}$?","I met the notation $ S=\{(a,b] ; a,b\in \mathbb R,a<b\}\cup\{\emptyset\} $ I know $S$ is a family of subsets ,a set of intervals, and from set theory $\emptyset$ is a subsets of every set then why in the notation : $ S=\{(a,b] ; a,b\in \mathbb R,a<b\}\cup\{\emptyset\} $ appear $\color{red}{\cup\{\emptyset\}}$ ?","['elementary-set-theory', 'measure-theory', 'notation']"
3186541,Number of legal bracket expressions of length n,"I am currently working on calculating the number of legal bracket expressions of length n, that is expressions like ()(()) But not like ()())( These are to be counted as an analogy for another problem. That is, imagine a chessboard of size n, with n being uneven, and a bishop standing in position (1,1). The bishop may only ever move one square either to the top and right, or to the bottom and right. With these moves, he must reach the square (n,1). I went through possibly anything that I could think of. Drawing the paths made some interesting looking pyramids and I spent a bunch of time just writing all possible bracket expressions. I even already know what the answer will be, as it seems like I am looking at several problems from which one may derive the so called Catalan numbers. The problem is, that I can't think of a way to count the amount of expressions. I have seen the formula C(2n,n) - C(2n,n+1) being used. I understand that C(2n,n) calculates all possible sequences of n left and n right brackets, legal or not, but I do not understand, why C(2n,n+1) amounts to all expression amongst those, that are not legal. As much fun as I had with this problem, I belief I am in a spot where I just can't progress. I don't know how to look at the second binomial coefficient as something that really relates to this. I do not understand where the n+1 comes from. Any tips would be much appreciated.",['discrete-mathematics']
3186561,"Picking $6$ numbers from $\{1, \ldots, 49\}$, what is the chance that the difference between at least $2$ of them is $= 1$?","You choose $6$ different natural numbers from $\{1, \ldots, 49\}$ . What is the probability that at least $2$ of these numbers have a difference equal to $1$ ? E.g. $1, 2, 10, 20, 30, 31$ - you'd have $2$ pairs $(1,2), (30,31)$ with the difference $= 1$ . I tried solving it by taking the inverse probability $P(\neg A)$ < No $2$ numbers have a difference of $1$ >, however, I encounter some difficulties with some special cases. For the first number, you'd have $49$ options. For the second number however, you'd have $46$ options (removing the previous number, and the $2$ surrounding it - which would give a diff. of $1$ ). However if the first number was $1$ or $49$ , you'd have $47$ options for the your second number, as there could only be $1$ number that would give you a diff of $1$ ( $2/48$ ). It only gets more complicated if two numbers are $x$ and $x+2$ , because the next number cannot be $\{x-1,x+1,x+3\}$ . Thank you.","['combinatorics', 'probability']"
3186562,"Difference between ""generating set"" and free product?","Let $G$ and $H$ be free groups and $g \in G$ . Is there a difference between $\langle H, G\rangle$ and the free product $H*G$ ?. In particular is $\langle H ,g \rangle = H * \langle g \rangle$ ?","['group-theory', 'free-groups', 'free-product']"
3186595,Finding the angle between a line and a plane,"Given that the equation of the line is: $$
\mbox{P:}\quad
\left\{\begin{array}{rcrcrcr} 
3x & - & y & + & z & = & 6
\\
x & + & 2y & + & z & = &-3
\end{array}\right.
$$ $$
\mbox{and the plane is A:}\quad 
x + 2y + z = 5.
$$ I believe you need to find the vector and use it to find the angle between the vector of the line and the normal vector of the plane. I tried finding two points for the first equation but couldn't move further from there.","['trigonometry', 'vectors', 'geometry']"
3186814,Is there a way to solve $x\left(\frac{e^x+1}{e^x-1}\right)=4$ for x besides just plugging numbers in?,"This comes into play in the equation for the shift in Cosmic Microwave Background (CMB) photon frequency due to inverse Compton scattering: $\frac{\Delta T}{T_{CMB}} = y \left( x\left(\frac{e^x+1}{e^x-1}\right)-4 \right)$ Where $y$ is essentially the integral of electron pressure, and $x$ is a scaled frequency. To find the null frequency, where the CMB temperature doesn't change, you need to solve for $\Delta T = 0$ , and therefore you get what's in the title: $x\left(\frac{e^x+1}{e^x-1}\right)=4$ I know by plotting the left hand side and then just plugging in numbers that the answer is $x = 3.830016097$ but I was wondering if there was any other way to solve for x.","['algebra-precalculus', 'mathematical-astronomy']"
3186878,Convert a general second order linear PDE into a weak form for the finite element method.,"Problem I want to convert the general second order linear PDE problem \begin{align}
\begin{cases} 
a(x,y)\frac{\partial^2 u}{\partial x^2}+b(x,y) \frac{\partial^2 u}{\partial y^2} +c(x,y)\frac{\partial^2 u}{\partial x \partial y}\\+d(x,y)\frac{\partial u}{\partial x}+e(x,y)\frac{\partial u}{\partial y}+f(x,y)u=g(x,y) & \text{in } R \text{ PDE} \\
u=u^* & \text{on } S_1 \text{ Dirchlet boundary condition} \\
\dfrac{\partial u}{\partial n}=q^* & \text{on } S_2 \text{ Neumann boundary condition} \\
\dfrac{\partial u}{\partial n}=r^*_1-r^*_2 u & \text{on } S_3 \text{ Robin boundary condition} \\
\end{cases} 
\end{align} into a weak form suitable for the finite element method. That is into the weak bilinear form $B(u,v)=L(v)$ where $B$ is bilinear, symmetric and positive definite functional and $L$ is a linear functional. Work thus far I know to how convert the following \begin{align}
\begin{cases} 
\dfrac{\partial^2 u}{\partial x^2}+\dfrac{\partial^2 u}{\partial y^2}+u=g(x,y) & \text{in } R \text{ PDE} \\
u=u^* & \text{on } S_1 \text{ Dirchlet boundary condition} \\
\dfrac{\partial u}{\partial n}=q^* & \text{on } S_2 \text{ Neumann boundary condition} \\
\dfrac{\partial u}{\partial n}=r^*_1-r^*_2 u & \text{on } S_3 \text{ Robin boundary condition} \\
\end{cases} 
\end{align} into the weak bilinear form $B(u,v)=L(v)$ where $B$ is bilinear, symmetric and positive definite and $L$ is linear. The steps are as follows (note that $v$ is our test function) \begin{align}
\int \int_{R} \left(\frac{\partial^2 u}{\partial x^2}+\frac{\partial^2 u}{\partial y^2}+u \right) v  \ dA &= \int \int_{R} g(x,y)  v  \ dA 
\end{align} Using the identity \begin{align}
\int \int_{R} v \nabla^2 u\ dA &= \int_{S} v \frac{\partial u}{\partial n}\ ds-\int\int_{R} \nabla u \cdot \nabla v\ dA
\end{align} We get \begin{align}
\int \int_R -\nabla u \cdot \nabla v +uv  \ dA  &= \int \int_R g  v  \ dA - \int_{S} v \frac{\partial u}{\partial n}\ ds \\
\int \int_R -\nabla u \cdot \nabla v  +uv  \ dA  &= \int \int_R g  v  \ dA - \int_{S_1} v \frac{\partial u}{\partial n}\ ds- \int_{S_2} v \frac{\partial u}{\partial n}\ ds - \int_{S_3} v \frac{\partial u}{\partial n}\ ds \\
\int \int_R -\nabla u \cdot \nabla v  +uv  \ dA  &= \int \int_R g  v  \ dA - \int_{S_2} v q^* \ ds - \int_{S_3} v (r^*_1-r^*_2 u) \ ds \\
\int \int_R -\nabla u \cdot \nabla v  +uv  \ dA  &= \int \int_R g  v  \ dA - \int_{S_2} v q^* \ ds - \int_{S_3} v r^*_1\ ds  +\int_{S_3} r^*_2 uv \ ds \\
\int \int_R -\nabla u \cdot \nabla v  +uv  \ dA +\int_{S_3} r^*_2 uv \ ds  &= \int \int_R g  v  \ dA - \int_{S_2} v q^* \ ds - \int_{S_3} v r^*_1\ ds  \\
B(u,v)&=L(v)
\end{align} Where I am having trouble I do not know what to do with the terms $$c(x,y)\frac{\partial^2 u}{\partial x \partial y}+d(x,y)\frac{\partial u}{\partial x}+e(x,y)\frac{\partial u}{\partial y}$$ as using the divergence theorem/integration by parts used in the work thus far section leaves terms that are not symmetric and therefore does not not satisfy the requirements for $B(u,v)$ . The other problem are the terms $$a(x,y)\frac{\partial^2 u}{\partial x^2}+b(x,y) \frac{\partial^2 u}{\partial y^2}$$ the identity that I used in the work thus far section does not work (I am probably wrong on this part). I could really use some guidance on both of these problems. Notes This question is part of a much larger problem in which I have to use the finite element method. Once the problem is in a weak form in which the finite element/galerkin method can be applied I know what to do. From what I know the symmetry of $B(u,v)$ is essential. If there is some other weak form that works with the finite element (that is suitable for a numerical solution), that would be an acceptable answer to my problem. I have been following ""Finite Elements: A Gentle Introduction"" I could not find anything in the book that answered the problem. If you have any references that covers my problem that would be great (so far I have found nothing). Have also posted my question here , to increase interest in my problem If you have any questions feel free to ask. Notation $n$ is the vector normal to the boundary surface. $u(x,y)$ is the solution to the given PDE or ODE. $v(x,y)$ is a test function. $\int \int_{R} * \ dA$ is an integral over region $R$ . $\int_{S} * ds$ is a surface integral over $S$ . $u^*, q^*, r^*_1, r^*_2, r^*_3$ are either constants or functions used to define the boundary conditions. The surface (S) boundary conditions can be divided into Dirchlet, Neumann, and Robin boundary conditions. That is $S=S_1\cup S_2 \cup S_3$ .","['multivariable-calculus', 'finite-element-method', 'functional-analysis', 'partial-differential-equations', 'numerical-methods']"
3186914,Summation Formula for Tangent/Secant Numbers,"I came across the following expressions: $$\begin{align} \widehat{S}_{2n} &:= \sum_{1 \leq k_1<\cdots<k_n \leq 2n} \prod_{\ell=1}^n (k_\ell-2\ell)^2, \\
\widehat{T}_{2n+1}&:=\sum_{1 \leq k_1 <\cdots <k_n \leq 2n} \prod_{\ell=1}^n (k_\ell-(2\ell+1))(k_\ell-2\ell) \end{align} $$ and I suspect that they equal the secant $S_{2n}$ and tangent $T_{2n+1}$ numbers , respectively. The secant and tangent numbers may be defined using Taylor series: $$\begin{align} \sec x &= \sum_{n=0}^\infty \frac{S_{2n}}{(2n)!} x^{2n}, \\ \tan x &= \sum_{n=0}^\infty  \frac{T_{2n+1}}{(2n+1)!} x^{2n+1} .\end{align}  $$ I have verified that $\widehat{S}_{2n}= S_{2n}$ and that $\widehat{T}_{2n+1}=T_{2n+1}$ for $n \leq 10$ , but I couldn't find a proof for the general case. I should also mention that the transformation $k_i \mapsto m_i+i$ , which maps strictly increasing sequences to non-decreasing sequences produces the nicer-looking formulas: $$\begin{align} \widehat{S}_{2n} &:= \sum_{0 \leq m_1 \leq \cdots \leq m_n \leq n} \prod_{\ell=1}^n (m_\ell-\ell)^2, \\
\widehat{T}_{2n+1}&:=\sum_{0 \leq m_1 \leq \cdots  \leq m_n \leq n} \prod_{\ell=1}^n (m_\ell-(\ell+1))(m_\ell-\ell). \end{align} $$ In search of a proof, I have tried generalizing this pattern. For example, the numbers $$\widehat{S}^{(N)}_{2n}:= \sum_{0 \leq m_1 \leq \cdots \leq m_n \leq n} \prod_{\ell=1}^n (\ell-m_\ell)^N, $$ appear to be the Taylor coefficients of the function $$f_N(x) = \frac{1}{1-\frac{1^N x}{1-\frac{2^N x}{1-\frac{3^N x}{1-\dots}}}}, $$ for any natural number $N$ . That is, it seems that $$f_N(x) = \sum_{n=0}^\infty \widehat{S}^{(N)}_{2n} x^n. $$ That made me think that a proof could be obtained using a ""continued-fraction-to-power-series"" formula. Unfortunately, I do not know of such a formula. I would appreciate help in confirming or denying the equalities $\widehat{S}_{2n}= S_{2n}$ and $\widehat{T}_{2n+1}=T_{2n+1}$ for all $n$ . Also, a proof (or disproof) that $\widehat{S}^{(N)}_{2n}$ are indeed related to continued fractions as above would be great. Thanks!","['calculus', 'trigonometry', 'taylor-expansion', 'generating-functions', 'continued-fractions']"
3186915,Why does this iterative way of solving an equation work?,"I was solving some semiconductor physics problem and in order to get the temperature I got this nasty equation: $$ T = \dfrac{7020}{\dfrac{3}{2}\ln(T)+12}.$$ It seems that I can solve this kind of equation by simply guessing a solution for $T$ and then substituting that answer back into the equation and then again substituting the new answer back into equation and so on until I am satisfied by the precision of result. Somehow this method works. Concretely for my example, my first guess was $T=1$ and I got this sequence of numbers $(585.0, 325.6419704169386, 339.4797907885183, 338.4580701961562, 338.53186591337385,338.52652733834424, ...)$ and they really seem to solve equation better and better. Questions. 1) What is an intuitive way to see why this method works? 2) How can I show rigorously that this method actually converges to a solution of the equation? 3) An obvious generalization for which the method might work seems to be: $$ x = \dfrac{a}{b\ln(x)+c}. $$ For which $a,b,c$ will this method work? Is this equation a special case of some natural generalization of this equation? What are some similar equations which I can solve by the method described? 4) When will the sequence of numbers in the iteration process converge in  finitely many steps to an exact solution to the equation? Does that case exist? Is a solution to: $$ x = \dfrac{a}{b\ln(x)+c} $$ irrational for every $a,b,c$ ? Is it transcendental? If not, for which $a,b,c$ will that be the case? Thank you for any help.","['irrational-numbers', 'sequences-and-series', 'recurrence-relations', 'real-analysis']"
3187019,Write a bivector as the exterior product of two vectors,"The Wikipedia article https://en.wikipedia.org/wiki/Bivector#Simple_bivectors states that ""A bivector that can be written as the exterior product of two vectors is simple. In two and three dimensions all bivectors are simple."" This implies that if we have a bivector in 3D given by $$B :=\alpha(e_1\wedge e_2) + \beta(e_2\wedge e_3) + \gamma(e_3\wedge e_1)$$ then there are the two vectors $v, w$ such that $B = v \wedge w$ . I am wondering how we can construct such $v, w$ from $\alpha, \beta, \gamma$ ? I understand that there is not a unique choice of $v$ and $w$ however is there a 'nicest' choice? One that is the most symmetric? I attempted the following manipulation on the definition of $B$ : $$\begin{eqnarray}B &=&\alpha(e_1\wedge e_2) + \beta(e_2\wedge e_3) + \gamma(e_3\wedge e_1) \\ &=&\alpha(e_1\wedge e_2) - \beta(e_3\wedge e_2) - \gamma(e_1\wedge e_3) \\ &=&  (\alpha e_1 - \beta e_3)\wedge e_2 - \left(\frac{\gamma}{\alpha}\right)(\alpha e_1 \wedge e_3) +  \frac{\beta\gamma}{\alpha}(e_3\wedge e_3)\\ &=& (\alpha e_1 - \beta e_3)\wedge e_2 - (\alpha e_1) \wedge\left(\frac{\gamma}{\alpha}e_3\right) +  (\beta e_3)\wedge\left(\frac{\gamma}{\alpha} e_3\right) \\ &=& (\alpha e_1 - \beta e_3)\wedge e_2 - (\alpha e_1 - \beta e_3) \wedge\left(\frac{\gamma}{\alpha}e_3\right) \\ &=& (\alpha e_1 - \beta e_3)\wedge \left(e_2 - \frac{\gamma}{\alpha}e_3\right)
\end{eqnarray}$$ This shows that the decomposition of a 3D bivector into the wedge product of two vectors is possible (and thus that every 3D bivector is simple) however it is not a very satisfying end result in that it is not symmetric and doesn't offer any insight into the nature of the decomposition. For example, a nicer decomposition would be one of the form $$(ae_1 + be_2 + ce_3)\wedge (a'e_1 + b'e_2 + c'e_3)$$ in which there is some symmetry in the coefficients $a, b, c, a', b', c'$ .","['outer-product', 'linear-algebra', 'geometric-algebras', 'exterior-algebra', 'clifford-algebras']"
3187062,Number of different necklaces with $2$ black and $6$ white beads,"This is not a home work question, I'm preparing for an entrance test. The number of different necklaces you can form with $2$ black and $6$ white beads is? My approach: We can place the white beads in the necklace in $1$ way because they all are white. Then the black beads can again be placed anywhere in $1$ way, once a black bead is placed it now acts like a reference, and I can place the last black bead either next to the first black, or with a gap of $1,2,3$ white beads, giving me $4$ combinations as, wbbwwwww,  wbwbwwww,  wbwwbwww, wbwwwbww. Is there any other better approach for this? 
I tried to follow this way, How many different necklaces can be formed with $6$ white and $5$ red beads? but I am getting fractional values, $$\frac{7!}{6! \cdot 2! \cdot 2}$$ Why is it happening this way? Can't we use this formula logic in all cases?","['permutations', 'combinatorics', 'necklace-and-bracelets', 'discrete-mathematics']"
3187102,"Necklace is made out of ten different red, twenty different orange and thirty same yellow pearls.","A necklace is made out of $10$ different red, $20$ different orange and $30$ same yellow pearls. If we know that no two red pearls are next to each other, what is the probability that between each couple of red pearls there is at least $1$ orange and from $2$ to $5$ yellow pearls? I got stuck when I tried to count the set $A$ = {no two red pearls are next to each other }. My idea was to fix one red and then from all possible circular permutations subtract the permutations where there is one red pearl right next that fixed one, or where some other two reds are together, then analogously with three reds together, etc. It seems to me that this way I would count a lot more than I should, but I cannot think of any other way.","['combinatorics', 'probability']"
3187136,Alternate complex binomial series sum,"Calculation of $\displaystyle \sum^{2n-1}_{r=1}(-1)^{r-1}\cdot r\cdot \frac{1}{\binom{2n}{r}}$ is My Try: Using $$\int^{1}_{0}x^m(1-x)^ndx = \frac{1}{(m+n+1)}\cdot \frac{1}{\binom{m+n}{n}}$$ So $\displaystyle \int^{1}x^{2n-r}(1-x)^r=\frac{1}{2n}\cdot \frac{1}{\binom{2n}{r}}$ Sum convert into $\displaystyle 2n\sum^{2n-1}_{r=1}(-1)^{r-1}r\int^{1}_{0}x^{2n-r}(1-x)^rdx$ $\displaystyle \Longrightarrow 2n \int^{1}_{0}x^{2n}\sum^{2n-1}_{r=1}(-1)^{r-1}\cdot r \cdot \bigg(1-\frac{1}{x}\bigg)^rdx$ Could some help me to solve it , Thanks",['sequences-and-series']
3187142,"If $\tan 9\theta = 3/4$, then find the value of $3\csc 3\theta - 4\sec 3\theta$.","If $\tan9\theta=\dfrac{3}{4}$ , where $0<\theta<\dfrac{\pi}{18}$ , then find the value of $3\csc 3\theta - 4\sec 3\theta$ . My approach:- $$\begin{align*} \tan9\theta &=\frac{3}{4} \\[6pt] \implies \theta & = \frac{37^{\circ}}{3} \end{align*}$$ By using this, we get value of $(3\csc3\theta - 4\sec3\theta) =9.95$ by using calculator. I want know if there's any way to solve this problem without calculator.","['trigonometry', 'inverse-function']"
3187147,Uniqueness proof : $a = a'$ so $a$ is unique. Is the proof absolutely rigorous?,"My question deals with uniqueness proofs. For example the proof of the uniqueness of the empty set, or the proof of the uniqueness of the identity element in a group. These proofs are convincing of course, but are they absolutely rigorous? Is it possible to consider the following objection: when I prove that empty-set- 1 and ( hypothetical ) empty-set-2 are in fact equal, I prove that the total number of empty sets is not equal to 2, but is this the same as showing that the total number of empty sets is equal to 1? How to make totally explicit the logic that is behind uniqueness proofs? Is this logic questionable?","['elementary-set-theory', 'abstract-algebra']"
3187198,What is the domain of the function $f(x)=\sqrt[3]{x^3-x}$?,"Let $f$ be: $f(x) = \sqrt[3]{x^3 -x}$ , an exercise book asked for the domain of definition. Isn't it over $\mathbb R$ . The book solution stated $Df = [-1,0] \cup [1, +\infty[$ I don t get it. Can you explain?",['calculus']
3187228,Either locally convex or concave?,"$z=f(x,y).$ $f$ is differentiable (updated). Define upper contour set $S=\{(x,y)\in\mathbb R^2|f(x,y)\geq c\}$ where $c$ is a constant. Let $C=\{U_\alpha:\alpha\in A\}$ be an index family of measure non-zero sets $U_\alpha$ , and $C$ covers $S$ : $$S\subseteq\bigcup_{\alpha\in A} U_\alpha.$$ Claim : there exists a cover $C$ such that for any $\alpha$ , either $U_\alpha\cap S$ is a convex set or $U_\alpha\setminus S$ is a convex set. How to prove this claim? My sketch: 1) Since $f$ is almost everywhere differentiable, the boundary set $bd(S)$ consists of curves that are almost everywhere smooth. 2) We can find a cover $C$ for $bd(S)$ such that each $U_{\alpha}\cap bd(S)$ is the graph of an almost everywhere differentiable function. 3) Then we can further find a refinement cover $C'=\{U_{\alpha'}:\alpha'\in A'\}$ for each set $U_{\alpha}\cap bd(S)$ , such that each $U_{\alpha'}\cap bd(S)$ is either a convex function or a concave function. Is this sound? I am sure that this can be proved much more elegantly without involving functions, though.","['analysis', 'real-analysis', 'functions', 'general-topology', 'derivatives']"
3187240,Covariant derivative of determinant of the metric tensor,"Let $(M,g)$ be a Riemannian manifold and $g$ the Riemannian metric in coordinates $g=g_{\alpha \beta}dx^{\alpha} \otimes dx^{\beta}$ , where $x^{i}$ are local coordinates on $M$ . Denote by $g^{\alpha \beta}$ the inverse components of the inverse metric $g^{-1}$ . Let $\nabla$ be the Levi-Civita connection of the metric $g$ . Consider, locally, the function $\det((g_{\alpha \beta})_{\alpha \beta})$ . It is known that $\nabla \det((g_{\alpha \beta})_{\alpha \beta}) = 0$ by using normal coordinates etc... I would like to show this fact without using normal coordinates. Just by computation. Here is what I have so far: $$\nabla \det((g_{\alpha \beta})_{\alpha \beta}) =  \left [ g^{\gamma \delta} \partial_{\delta} \det((g_{\alpha \beta})_{\alpha \beta}) \right ] \partial_{\gamma} = \left [ \det((g_{\alpha \beta})_{\alpha \beta}) g^{\gamma \delta} g^{\beta \alpha} \partial_{\delta} g_{\alpha \beta}\right ] \partial_{\gamma}.$$ Here: the first equality sign follows from the definition of the gradient of a function and the second equality sign is the derivative of the determinant. Question: How do I continue from here without using normal coordinates? Or are there any mistakes? If yes, where and which?","['riemannian-geometry', 'differential-geometry']"
3187321,"Guessing delta value, in epsilon-delta proof","So as I've solved various problems regarding epsilon-delta proof, I have faced several questions where I had to kind of implement this process: So here's one of them: $\lim_{x\rightarrow 1} (x^3+x+1) = 3 $ So I started off saying, Let $\epsilon$ > 0 be given. We want to find $\delta$ > 0 s.t. if 0 < $|x-1|$ < $\delta$ , then $|x^3+x+2| < \epsilon$ . $|x^3+x+2| = |(x-1)(x-1)(x+2)|$ ..... (a) (Here comes the part! ) Let $\delta < 1, $ ... (b) $ |x-1| < 1$ $-1 < x-1 < 1$ $1 < x + 2 < 3$ $x + 2 < 3$ Apply to the part (a), $<(x-1)\times 1 \times 3  < \delta \times 3
 = \epsilon$ $\delta = \epsilon/3$ $\delta = min(1, \epsilon/3)$ . and the proof follows... So my question is what is the standard of choosing n in part (b) Let $\delta < n$ ? Sometimes they choose $\frac{1}{2}$ or $\frac{1}{4}$ for n, and sometimes 1.
For this case, my answer sheet specifically chose $\frac{1}{2}$ for n and I have no idea why.","['limits', 'calculus', 'epsilon-delta']"
3187409,"Is Fractional Calculus an important research topic ""in pure mathematics"" today?","Being a potential graduate student, I would like to know if fractional calculus is an actively developing research topic in the area of pure mathematics today.","['research', 'soft-question', 'fractional-calculus', 'analysis']"
3187410,Convergence in the weak-star sense of measures and $ \int_{\Omega} \sqrt{1+u_k^2} \to \int_{\Omega} \sqrt {1+u_k^2}$ gives convergence in $L^1$.,"I have a question, which is exercise 1.20 of the following book: Functions of Bounded Variation and Free Discontinuity Problems. Let assume $\Omega$ is a bounded subset of $\mathbb{R}^n$ and $u_k, u \in L^1(\Omega)$ and $u_k$ converge to $u$ in the weak star sense of measures as follow: $$ \int_{\Omega} u_k \phi \to  \int_{\Omega} u \phi \qquad \forall \phi \in C_c^{\infty}(\Omega).$$ Also assume that $$ \int_{\Omega} \sqrt{1+u_k^2} \to  \int_{\Omega} \sqrt{1+u^2}.$$ Then we want to show that we have strong convergence in $L^1(\Omega)$ too. There is a hint which says first show that $$\sqrt{1+u_k^2}+\sqrt{1+u^2}-2\sqrt{1+\left(\frac{u+u_k}{2}\right)^2} 
\to 0 $$ in $L^1(\Omega)$ . This is okay but I don't know how to use this last one to conclude the result.","['analysis', 'real-analysis', 'lp-spaces', 'functional-analysis', 'partial-differential-equations']"
3187460,Can one simplify $\arctan(a\tan(x))$?,"We know that $\arctan(\tan(x))=x$ when $x$ lies between $-\pi/2$ and $+\pi/2$ ; but do you know a way to transform the expression $\arctan(a\tan(x))$ , where $a$ is a real number between $0$ and $1$ ? I thought $a$ could be transformed with trigonometric functions, such as $a=\sin(\alpha)\cos(x)$ , but $\arctan(\sin(\alpha)\sin(x))$ does not remind me anything. Maybe there is no further possible transformation?",['trigonometry']
3187497,Evaluate $\lim_{t\to1^-}(1-t)\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}$,$\lim_{t\to1^-}(1-t)\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}$ My approach $\frac{t^r}{1+t^r}=t^r-t^{2r}+t^{3r}-\cdots$ $\implies \sum_{r=1}^{\infty}\frac{t^r}{1+t^r}=\frac{t}{1-t}-\frac{t^2}{1-t^2}+\frac{t^3}{1-t^3}-\cdots$ $\implies(1-t)\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}=t-\frac{t^2}{1+t}+\frac{t^3}{1+t+t^2}-\cdots$ $\implies \lim_{t\to1^-}(1-t)\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}=1-\frac12+\frac13-\frac14+\cdots=\ln 2$ Is this correct? Any other more rigorous approach ? Solution provided by problem poser: $\lim_{t\to1^-}(1-t)\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}=\lim_{t\to1^-}-\ln t\sum_{r=1}^{\infty}\frac{1}{1+e^{-r\ln t}}$ $=\lim_{n\to\infty}\frac1n\sum_{r=1}^{\infty}\frac{1}{1+e^{\frac rn}}$ $=\int_{0}^{1}\frac{1}{1+e^x}dx=\ln\frac {2e}{1+e}$ This answer seems to be incorrect as $\frac{t^r}{2}<\frac{t^r}{1+t^r}<t^r\implies \frac{t}{2(1-t)}\le\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}\le\frac{t}{1-t}\implies\frac12\le\lim_{t\to1^-}(1-t)\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}\le 1.$ But $\ln\frac{2e}{1+e}\approx 0.38$,['limits']
3187498,does $f(x)$ defined by $f(x)=f(x^2)$ must be a constant?,"I proved that if $f(x)$ is continuous then it must be a constant. 
But I can't disprove or prove it for non-continuous function.","['functions', 'real-analysis']"
3187533,A constraint that implies convexity,"Let $f:\mathbb{R} \to \mathbb{R} $ be a function such that $\forall x<y, \exists z\in(x, y) $ with $(y-x) f(z) \le (y-z) f(x) +(z-x) f(y) $ . a) Give an example of a non-convex function $f$ which has this property. b) Prove that a continuous function $f$ which has this property is convex. For a) it is obvious that we must search for a discontinuous $f$ with this property, but I can't find one. For b), I tried to assume that $f$ is not convex, which means that $\exists u<v$ and $a \in [0,1]$ such that $f(au+(1-a)v)>af(u)+(1-a)f(v)$ ,but here I am stuck.","['contest-math', 'convexity-inequality', 'functional-analysis', 'real-analysis']"
3187539,Why is Brownian Motion so Big in the Theory of Stochastic Differential Equations?,"I am reading some introductory material on stochastic differential equations at the moment. In almost all cases, the equations which are presented are of the form $$ dX_t = \mu(t,X_t) dt + \sigma(t, X_t) dB_t, \quad (\dagger) $$ where $B_t$ usually is a standard Brownian motion. I am aware, that it is possible to generalize the above equaton by substituting $B_t$ with some semimartingale $H_t$ ; however, it seems that these cases are almost never studied in practise. In almost all books about applications of stochastic differential equations, the case where $H_t = B_t$ dominates completely. Why is this the case? Are stochastic differential equations based on Brownian motion really that general? How is it possible, that these types of equations satisfy the need of so many practicioners? Are there any Theorems stating, that it is possible to model almost all stochastic processes of interest with equations like $(\dagger)$ ? I do not see how almot exclusively studying the special case of Brownian-motion based stochastic differential equations is not a massive loss of generality in the theory.","['stochastic-processes', 'stochastic-differential-equations', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
3187576,Limit for $e$ and $\frac{1}{e}$,"My question concerns the derivation of this: $$e^r = \lim_{n \rightarrow \infty} \left(1 + \frac{r}{n}\right)^n \ \ ...(1).$$ One of the definitions of $e$ is as follows: $$e = \lim_{n \rightarrow \infty} \left(1+\frac{1}{n}\right)^n.$$ Then, textbooks usually derive equation (1) in the following manner: \begin{align}
\lim_{n \rightarrow \infty} \left(1 + \frac{r}{n}\right)^n &= \lim_{u \rightarrow \infty} \left(1 + \frac{1}{u}\right)^{ru} \ \ \text{where} \ u = \frac{n}{r}\\
&= \lim_{u \rightarrow \infty} \left(\left(1 + \frac{1}{u}\right)^u\right)^r \\
&= \left(\lim_{u \rightarrow \infty} \left(1 + \frac{1}{u}\right)^u\right)^r \\
&= e^r.
\end{align} This argument is fine if $r > 0$ since $u \rightarrow \infty$ as $n \rightarrow \infty$ , but when $r < 0$ , $u \rightarrow - \infty$ as $n \rightarrow \infty$ . How can I extend the proof for (1) where $r$ is any real number? When $r = 0$ , $\lim_{n \rightarrow \infty} (1+0/n)^n = 1$ (Although, I should be careful about evaluating limits that look like $``1^{\infty}""$ .) Here's my attempt so far for the case where $r < 0$ : \begin{align}
\lim_{n \rightarrow \infty} \left(1 + \frac{r}{n}\right)^n &= \lim_{u \rightarrow \infty} \left(1 - \frac{1}{u}\right)^{-ru} \ \text{where} \ u = -\frac{n}{r} \\
&= \left(\lim_{u \rightarrow \infty} \left(1 - \frac{1}{u}\right)^u\right)^{-r}.
\end{align} My question boils down to how to show the following limit from the definition above for $e$ $$\lim_{n \rightarrow \infty} \left(1 - \frac{1}{n}\right)^n = \frac{1}{e}.$$ Thanks.","['limits', 'calculus']"
3187621,Trying to prove that $\pi$ is irrational using Legendre Polynomials.,"Unfortunately, numerical data sugggest that is not possible to show that $\pi$ is irrational with the polynomials below. I've to search for another polynomial... But I've little faith that such polynomial exists. In the end there is a ""explanation"" why one of the polynomials defined below don't work. We gonna do something very similar to what Beukers did to prove that $\zeta (2)$ is irrational. Sorry if is there any mistakes below. Here, $\pi = 4-\frac{4}{3}+\frac{4}{5}+\cdots$ We have the series: $\frac{1}{1+x^2} = 1-x^2+x^4-\cdots$ \begin{align}
I_n = 4\int_0^1\frac{x^{2n}}{1+x^2}\:dx &=
4\int_0^1 x^{2n} \sum_{k=0}^{\infty}(-1)^{k}x^{2k} \: dx\\
&=4\int_0^1 \sum_{k=0}^{\infty}(-1)^{k}x^{2k+2n} \: dx\\
&=4\sum_{k=0}^{\infty}(-1)^{k} \frac{1}{2k+2n+1}
\end{align} This shows that that for $n=0$ we have $I_0 = \pi$ . For $n>0$ we have a piece of $\pi$ (hehe). So we choose a polynomial with integers coefficients with all even powers: \begin{align}
I_n = \frac{a_n\pi +b_n}{d_{2n}}
\end{align} where $a_n,b_n$ are integers and $d_n$ is the less commom multiple of all consecutives natural numbers up to $2n$ . If it's possible to show that $d_{2n}I_n = a_n+b_n\pi \to 0$ as $n$ grows larger, that proves that $\pi$ is irrational. The shifted legendre polynomial is defined as below: $P_n(x) = \frac{1}{n!} \frac{d^n}{dx^n}(x^2-x)^n$ . The first $5$ are: They are nice because it's easy to integrate $n$ times from $0$ to $1$ , it makes the integral go to $0$ very fast and it's easy to find a upper bound for it.  I first learned about them in irrationality proofs made by Beukers, where he uses these polynomials to show that $\zeta(2)$ and even $\zeta(3)$ are irrational. He also proves that $\pi$ is irrational but uses a different approach to what we are trying here. The problem here is that these polynomial have odd power too, and that is not allowed here. So I wanted something like this: $2x^2-1$ $ 6x^4-6x^2+1$ $ 20x^6-30x^4+12x^2-1$ I also created a polynomial of the form: $\frac{1}{(2n)!}\frac{d^{2n}}{dx^{2n}}(x^4-x^2)^n$ , that satifies the property of having only even powers and integer coefficients, but this is terrible because it make the integral goes to infinity rather than $0$ . Probably because the coefficients are very large since we must differentiate some many times. I don't think we necessarily need to use a Legendre type polynomial. Maybe a polynomial with these properties mentioned to show that $\pi$ is irrational doesn't even exist. Any help will be appreciated. Below are defined polynomials, $Q_n(x)$ and $P_n(x)$ with degrees $(2n)$ and $(4n)$ respectively. $$Q_n(x) = \frac{d^{n}}{dx^{n}} \frac{(x-x^3)^{n}}{n!}$$ $Q_1(x) = 1-3x^2$ $Q_2(x) = 1 - 12 x^2 + 15 x^4$ $Q_3(x) = 1 - 30 x^2 + 105 x^4 - 84 x^6$ . Degree of $Q_n(x) = 2n$ . Using the identity, $$\int_a^b f^{(n)}(x)g(x) \:dx = (-1)^n\int_a^b f(x)g(x)^{(n)} \:dx $$ if $\left. f(x)g(x) \right|_a^b=0$ : $$\int_0^1 Q_n(x)/(1+x^2) = \frac{a_n \pi+b_n}{d_{2n}}.$$ $$ \int_0^1 Q_n(x)/(1+x^2) = (-1)^n \int_0^1 \frac{(x-x^3)^{n}}{n!}
\frac{d^{n}}{dx^{n}} \frac{1}{1+x^2} $$ Another polynomial: $$P_n(x) = \frac{d^{2n-1}}{dx^{2n-1}} \frac{(x-x^3)^{2n+1}}{(2n+1)(2n-1)!} $$ $P_1(x) = x^2 - 5 x^4 + 7 x^6 - 3 x^8$ $P_2(x) = 2 x^2 - 35 x^4 + 168 x^6 - 330 x^8 + 286 x^{10} - 91 x^{12}$ $P_3(x) = 3 x^2 - 126 x^4 + 1386 x^6 - 6435 x^8 + 15015 x^{10} -
18564 x^{12} + 11628 x^{14} - 2907 x^{16} $ $$\int_0^1 P_1(x)/(1+x^2) = 4\pi - \frac{88}{7} \approx 0.005057$$ $$\int_0^1 P_2(x)/(1+x^2) =  228\pi -\frac{70912}{99} \approx -0.000296$$ $$\int_0^1 P_3(x)/(1+x^2) = 3(4672\pi -\frac{6678272}{455} ) \approx  0.0000046480$$ Degree of $ P_n(x) = 4n+4$ : $$\int_0^1 P_n(x)/(1+x^2) = \frac{a_n\pi+b_n}{d_{4n+4}} $$ $$ \int_0^1 P_n(x)/(1+x^2) = (-1)^n \int_0^1 \frac{(x^3-x)^{2n+1}}{(2n+1)(2n-1)!}
\frac{d^{2n-1}}{dx^{2n-1}}\frac{1}{1+x^2}.  $$ Finding the $n$ th derivative of $1/(1+x^2)$ \begin{align}
&\frac{1}{1+x^2} = \frac{i}{2(x+i)} - \frac{i}{2(x-i)}\\
&\frac{d^n}{dx^n} \frac{1}{1+x^2} = (-1)^n\frac{n!\:i}{2(x+i)^{n+1}}
- (-1)^n \frac{n!\:i}{2(x-i)^{n+1}}\\
\end{align} Chosing $Q_n(x)$ , which has a degree $2n$ : \begin{align}
I_n &=\int_0^1 Q_n(x)/(1+x^2)\\
&= (-1)^n \int_0^1 \frac{(x-x^3)^{n}}{n!} \frac{d^{n}}{dx^{n}} \frac{1}{1+x^2}\\
&=  (-1)^n \int_0^1 \frac{(x-x^3)^{n}}{n!}\left(
(-1)^n\frac{n!\:i}{2(x+i)^{n+1}}- (-1)^n \frac{n!\:i}{2(x-i)^{n+1}} \right)
\end{align} Multiplying by $i$ and taking the absolute value: \begin{align}
|iI_n| = |I_n|
 &=\left| (-1)^n \int_0^1 \frac{(x-x^3)^{n}}{n!}\left(
(-1)^{n+1}\frac{n!}{2(x+i)^{n+1}} - (-1)^{n+1} \frac{n!}{2(x-i)^{n+1}} \right) \right|\\
&=\int_0^1 \frac{(x-x^3)^{n}}{n!}\left(
\frac{n!}{2(x+i)^{n+1}} - \frac{n!}{2(x-i)^{n+1}} \right) \\
&=\int_0^1 {(x-x^3)^{n}}\left(
\frac{1}{2(x+i)^{n+1}} - \frac{1}{2(x-i)^{n+1}} \right). \\
\end{align} Removing the $i$ above: \begin{align}
\frac{1}{2(x+i)^{n+1}} - \frac{1}{2(x-i)^{n+1}} =\frac{(x-i)^{n+1} -(x+i)^{n+1}}{2(1+x^2)}
\end{align} writing in polar form $x+i = |z|e^{i\theta}:$ \begin{align}
\frac{(x-i)^{n+1} -(x+i)^{n+1}}{2(1+x^2)} &= \frac{(e^{-i\theta})^{n+1} - (e^{i\theta})^{n+1}} {|z|^{n+1}}\\
&=\frac{\cosh[ (n+1) \theta ] } {|z|^{n+1}}\\
&=\frac{\cosh[ (n+1) \arctan 1/x ] } {(\sqrt{1+x^2})^{n+1}}\: \text{because $x>0$}.
\end{align} \begin{align}
|I_n| &= \int_0^1 {(x-x^3)^{n}} \frac{\cosh[ (n+1) \arctan 1/x ] } {(\sqrt{1+x^2})^{n+1}}\\
&= \int_0^1 \frac{(x-x^3)^{n}} {(\sqrt{1+x^2})^{n+1}} \frac{1}{2} 
\left( \frac{1}{e^{(n+1)\arctan 1/x}} + e^{(n+1)\arctan 1/x} \right)\\
&= \int_0^1 \left( \frac{(x-x^3)}{\sqrt{1+x^2}} \right)^n \frac{1}{\sqrt{1+x^2}}
\left( \frac{1} { {(e^{\arctan 1/x})}^n  } \frac{1}{e^{\arctan 1/x}} + {(e^{\arctan 1/x})}^n e^{\arctan 1/x} \right)  \\
&= \int_0^1 \left( \frac{x-x^3}{e^{\arctan 1/x}\sqrt{1+x^2}} \right)^n 
\frac{1}{e^{\arctan 1/x}\sqrt{1+x^2}} + 
\left( \frac{(x-x^3)(e^{\arctan 1/x})}{\sqrt{1+x^2}} \right)^n \frac{e^{\arctan 1/x}}{\sqrt{1+x^2}}
\end{align} The maximum of $$\left( \frac{(x-x^3)(e^{\arctan 1/x})}{\sqrt{1+x^2}} \right)^n$$ is $\approx 1.03^n$ (thanks wolfram) which implies that $Q_n(x)$ can't be used for proving the irrationality of $\pi$ . $P_n(x)$ goes to zero and we can do all this work again, but what will happen is that maximum of $P_n(x)$ multiplied by the $LCM$ of ${1,2,...,4n}$ will be greater than $1$ , because of the degree $4n$ . I'll try to show that, since we have all the tools. Also, I believe that polynomials of the form $$\frac{d^{an+c_1}}{dx^{an+c_1}} \frac{P(x)^{bn+c_2}}{(an+c_1)!}$$ where both $a,b$ and $c's$ are constants, is that $c's$ are useless, it's only matters the degree of the polynomial and addding or subtracting constants doesn't change that. Choosing $P_n(x)$ , which has degree $4n+4$ : \begin{align}
J_n &= \int_0^1 P_n(x)/(1+x^2) \\
&= (-1)^n \int_0^1 \frac{(x-x^3)^{2n+1}}{(2n+1)(2n-1)!}
\frac{d^{2n-1}}{dx^{2n-1}}\frac{1}{1+x^2} \\
&= (-1)^n \int_0^1 \frac{(x-x^3)^{2n+1}}{(2n+1)(2n-1)!} \left(
(-1)^{2n-1}\frac{(2n-1)!\:i}{2(x+i)^{2n}}
- (-1)^{2n-1} \frac{(2n-1)!\:i}{2(x-i)^{2n}} \right)\\
&= (-1)^n \int_0^1 \frac{(x-x^3)^{2n+1}}{(2n+1)(2n-1)!} \left(
-\frac{(2n-1)!\:i}{2(x+i)^{2n}}
+ \frac{(2n-1)!\:i}{2(x-i)^{2n}} \right)
\end{align} Doing the same as before: \begin{align}
|J_n| &= \int_0^1 \frac{(x-x^3)^{2n+1}}{(2n+1)(2n-1)!} \left(
\frac{(2n-1)!}{2(x+i)^{2n}} - \frac{(2n-1)!}{2(x-i)^{2n}} \right)\\
&=\int_0^1 \frac{ (x-x^3)^{2n+1} }{ 2n+1 } \left(
\frac{1}{2(x+i)^{2n}}
- \frac{1}{2(x-i)^{2n}} \right)\\
&=\int_0^1 \frac{ (x-x^3)^{2n+1} }{ 2n+1 } \frac{\cosh(2n\arctan 1/x)}{(\sqrt{1+x^2})^{2n}}\\
&=\int_0^1 \frac{ (x-x^3)^{2n+1} }{ (2n+1)(\sqrt{1+x^2})^{2n} } 
\frac{1}{2} \left( \frac{1}{e^{(2n)\arctan 1/x}} + e^{(2n)\arctan 1/x} \right)\\
&=\int_0^1 \frac{ (x-x^3)^{2n+1}\sqrt{1+x^2} } { (2n+1)(\sqrt{1+x^2})^{2n+1} }
\frac{1}{2} \left( \frac{ e^{\arctan 1/x} }{ e^{(2n+1)\arctan 1/x} } +
\frac{ e^{(2n+1)\arctan 1/x} }{ e^{\arctan 1/x} } \right)\\
&= 1/2 \int_0^1 \frac{ e^{\arctan 1/x} \sqrt{1+x^2} }{2n+1}
\left( \frac{x-x^3} { e^{\arctan 1/x} \sqrt{1+x^2}  } \right)^{2n+1} + 
\frac{ \sqrt{1+x^2} } {(2n+1)e^{\arctan 1/x}} 
\left( \frac{ (x-x^3)e^{\arctan 1/x} } {\sqrt{1+x^2}} \right)^{2n+1}
\end{align} I was wrong about why this polynomial would fail, i thought it was bacause of the $LCM(1,2,...,4n)$ would be greater than $1$ when multiplied with the maximum of the integral, but what happened is that we have the exactly same function that appeared using $Q_n(x)$ . I think there is no polynomial with such properties that can be used to prove that $\pi \notin \mathbb{Q}$ using this approach. If anyone have any polynomial in mind... Well, I think that's all folks.","['number-theory', 'irrational-numbers', 'legendre-polynomials']"
3187641,Representation of Generalized Quadratic Form of Random Variables,"I am interested in finding/understanding a ""good"" $L^2$ -orthogonal (i.e. uncorrelated) decomposition of a matrix-valued quadratic form. The setup is as follows: Given a random matrix $X$ (tall, of size $n\times N$ ), with mean zero rows $X_i$ consider $$Q(X) = X'AX.$$ The covariance structure of $X$ can be encoded in a block matrix $\Sigma>0$ given as $$
\Sigma = \begin{bmatrix}
\Sigma_{11} & \Sigma_{12} & \dots& \Sigma_{1N}\\
\Sigma_{12} & \Sigma_{22} & \dots&\\
\vdots & \ddots& \ddots
\end{bmatrix}.
$$ The blocks are of size $n$ corresponding to the length of the vectors $X_i$ . Ideally, I would like to represent $Q$ as $$
Q(X) = X'AX = \sum U'_i\Lambda_iU_i
$$ Where the covariance structure of the $U_i$ is at least block-diagonal: $$
\Lambda = \begin{bmatrix}
\Lambda_1 & 0 & 0&\dots\\
0 & \Lambda_2 & 0 &\dots\\
0 & 0 & \ddots & \ddots
\end{bmatrix}.
$$ Is this possible? Also, and if so, I am interested in the relationship between the singular values of the blocks of $\Sigma$ and $\Lambda$ . I suspect something like this to be acheivable given that we can do it ordinary quadratic forms: sum of squares of dependent gaussian random variables I guess maybe one could use a vectorized version of the argument presented there, but I am not entirely sure how to procede?","['matrices', 'statistics', 'probability', 'quadratic-forms']"
3187652,"Exercise 4.8 from Hrbacek's ""Introduction to set theory""","4.8 Let $(A,<)$ be linearly ordered. Define $\prec$ on $\text{Seq}(A)$ by: $\big\langle a_0,...,a_{m-1}\big\rangle \prec \big\langle
 b_0,...,b_{n-1} \big\rangle$ if and only if there is $k<n$ such that $a_i=b_i$ for all $i<k$ and either $a_k<b_k$ or $a_k$ is undefined
  (i.e., $k=m<n$ ). Prove that $\prec$ is a linear ordering. If $(A,<)$ is
  well-ordered, $(\text{Seq}(A),\prec)$ is also well-ordered. $\text{Seq}(A)$ is the set of all finite sequences. I don't understand why $\prec$ is well-ordering, when $(A,<)$ is well-ordered.
Let $A$ be the set of natural numbers. 
Then what is the least element of this subset $\{\big\langle 0,0,..., \underbrace{1}_{i} \big\rangle;i \in N\}$ of $(\text{Seq}(A),\prec)$ ?",['elementary-set-theory']
3187678,Minimal example of Simpson's paradox,"Let's say that a finite probability space $(\Omega,\mathscr P(\Omega),P)$ has Simpson's property if you can find events $A,B,C\in\mathscr P(\Omega)$ such that $P(C) \in (0,1)$ . $A$ and $B$ are positively correlated: $P(A\cap B) > P(A)P(B)$ . $A$ and $B$ are negatively correlated conditionally to both $C$ and $\overline C$ : $$P(A\cap B\mid C) < P(A\mid C) P(B\mid C) \text{ and } P(A\cap B\mid\overline C) < P(A\mid\overline C)  P(B\mid\overline C).$$ One way to state Simpson's paradox is that there are probability spaces with Simpson's property. The cat-vs-human example given in this nice video , for instance, boils down to this: (The four small points each have a probability of $1/10$ , and the two big ones weigh $3/10$ each). My (very naïve and probably not very interesting) question is to know if it is possible to find a smaller example (with fewer points) and, if so, to find a provably minimal example.","['conditional-probability', 'statistics', 'paradoxes', 'probability']"
3187784,How is the continuous input probabalistic generative model derived from single class model?,"So for the single valued model we have: $p(C_1|\textbf{x}) = \frac{p(\textbf{x}|C_1)p(C_1)}{p(\textbf{x}|C_1)p(C_1)+p(\textbf{x}|C_2)p(C_2)}$ If we rearrange the terms, we can write this as a sigmoid function: $\frac{1}{1+exp(-a)}=\sigma(a)$ ...(4.57) where $a=ln \frac{p(\textbf{x}|C_1)p(C_1)}{p(\textbf{x}|C_2)p(C_2)}$ ...(4.58) Then we moved onto the continuous input case where we assumed $p(C_k|\textbf{x})$ was gaussian: $p(\textbf{x}|C_k) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp(-\frac{1}{2}(\textbf{x}-\mu_k)^T\Sigma^{-1}(\textbf{x}-\mu_k))$ ...(4.64) Then I became confused when the text said, using 4.57 and 4.58 we have: $p(C_1|\textbf{x}) = \sigma(\textbf{w}^T\textbf{x}+w_0)$ where: $\textbf{w} = \Sigma ^{-1}(\mu_1-\mu_2)$ $w_0=-\frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1+\frac{1}{2}\mu_2^T\Sigma^{-1}\mu_2+ln\frac{p(C_1)}{p(C_2)}$ Is it saying that if I plug everything in the sigmoid I will recover from it $p(C_1|\textbf{x}) = \frac{p(\textbf{x}|C_1)p(C_1)}{p(\textbf{x}|C_1)p(C_1)+p(\textbf{x}|C_2)p(C_2)}$ but $p(x|C_1)$ and $p(x|C_2)$ are the normal distributions like in 4.64? Why can't we just use the Bayes theorem as is? Why do we have create a sigmoid function out of seemingly no where?","['machine-learning', 'statistics', 'bayesian']"
3187850,Is my evaluation of this limit correct/sufficient?,"$g(x) = \begin{cases} \frac{1}{b} \text{ if } x^2 = \frac{a}{b} \in \mathbb{Q} \text{ in lowest terms}\\ 0 \text{ if } x^2 \notin \mathbb{Q} \end{cases}$ Evaluate $\displaystyle{\lim_{x \to 0} g(x)}$ Here's what I've done to evaluate the limit. Is this correct/sufficient to show that the limit as $x$ approaches $0$ is $0$ ? $\displaystyle{\lim_{x \to 0} 0 = 0}$ $\displaystyle{\lim_{x \to 0} x^2 = 0}$ $\forall x \in \mathbb{Q} \text{, } 0<x<1 \text{, } 0 < \frac{1}{b} \leq \frac{a}{b} = x^2$ $\Rightarrow$ by the Squeeze Theorem, the limit of $g(x)$ as $x \rightarrow 0^+$ is $0$ Edit: Fixed formatting","['limits', 'proof-verification']"
3187852,"Determine whether f is a function, an injection, a surjection","Let $P=\{p(x)$ | $p(x)$ is a polynomial of degree $n$ , $n \in \Bbb Z^+\cup\{0\} $ with coefficients in $\Bbb R \}$ . Define $f : P\rightarrow P$ where $f(p(x)) =p'(x)$ , the derivative of $p(x)$ . Determine whether $f$ is a function, an injection, a surjection, a bijection. Now I have the solutions, and I understand that it is a function because each polynomial has a unique derivative. And it is not an injection as the antiderivative of a given polynomial is not unique. However, I do not understand the book's solution for determining whether it is a surjection, nor am I able to come up with one myself. In all honesty, I think I am having trouble understanding the mapping from P to P. The solution states that it is a surjection. Why is this so?","['elementary-set-theory', 'calculus', 'functions', 'derivatives']"
3187856,Find $n \times n$ matrices $A$ such that $\det A = 0$ and $\text{rank}(AB) = \text{rank}(BA)$ for any $n \times n$ matrix $B$,"Find all complex-valued $n \times n$ matrices $A$ such that $\det A = 0$ and $\text{rank}(AB) = \text{rank}(BA)$ for any $n \times n$ complex-valued $B$ . I believe that $A = 0$ is the only answer. I have been able to prove that, if $A$ is of rank $r$ , then any $r$ lines and any $r$ columns are linearly independent. To see this, note that since $A$ is of rank $r$ , then A has $r$ linearly independent columns; say that the indexes of these columns are $i_1, i_2, ..., i_r$ . Then by making B equal to a matrix that has 1 in positions $(i_k,i_k)$ and 0 elsewhere, $AB$ basically ""selects"" $r$ independent columns from $A$ having all other columns equal to 0, so $\text{rank} AB = r$ . Now, $BA$ selects rows $i_1,i_2,..,i_r$ from $A$ . If $A$ were to have $r$ rows that were not linearly independent, then there would be an inversible matrix $M$ which would place these rows in positions $i_1,i_2,...,i_r$ . Then $$\text{rank} (BA) = \text{rank} (BAM) < r,$$ which would contradict our hypothesis. Thus any $r$ rows of $A$ are linearly independent. Running the same argument in reverse, we get that any $r$ lines of $A$ are linearly independent. Any ideas about how to proceed?",['linear-algebra']
3187964,Size of conjugacy class in subgroup compared to size of conjugacy class in group,"Given: $\bullet$ A finite group $G$ , an index 2 subgroup $H$ , an element $a \in H$ $\bullet$ $[a]_H$ and $[a]_G$ are the conjugacy class in $H$ of $a$ and the conjugacy class in $G$ of $a$ , respectively To prove: $[a]_H = [a]_G$ or $[a]_H$ is half the size of $[a]_G$ , depending on whether or not the centralizer $Z_G(a)$ is contained in $H$ . Attempt : $H$ being of index 2 means that $H$ is normal, which means that $H \cdot Z_G(a) $ is a subgroup of $G$ . By the Second Isomorphism Theorem, $(H \cdot Z_G(a)) \, / \,H \cong Z_G(a) \, / \, (H \cap Z_G(a))$ . If $Z_G(a)$ is contained in $H$ , then $H \cap Z_G(a) = Z_G(a)$ and the group on the right-hand side of the isomorphism is trivial, and therefore the group on the left-hand side is trivial as well, and so $|H \cdot Z_G(a))| = |H|$ . Where do I go from here? Can I say that $H$ must be equal to $H \cdot Z_G(a))$ ? Even if I could, I don't know how to continue.","['group-theory', 'abstract-algebra', 'finite-groups']"
3187965,How much should I pay for a chance to win 100$?,"There are $4$ closed doors, with $100\$$ behind one of them. You can pay $X$ to open a door. If the money is there, you can keep it. If not you can pay another $X$ to open the next door, and so on. What is the most I can pay and still win on average? I think that it's $25\$$ . Since the money can be behind any door, you have as much of a chance of getting it on the first door as any other door. Since there are $4$ doors, and equal chance of being behind any door, $100 / 4$ = $25$ .",['probability']
3188012,"Book-recommendations ""Profinite Groups""","I started studying Profinite Groups a few weeks ago. I'm using the book ""Profinite Groups"" by Wilson as a basis, but the book is not clear enough sometimes (probably because I'm studying it for the first time). Classify a book as good or bad is subjective, so I would just like book-recommendations for me to analyze.","['group-theory', 'profinite-groups', 'book-recommendation']"
3188026,Looking for a different type of Linear Algebra book,"Are there any good linear algebra books with lots of (mathematical, preferably algebraic or geometric-flavored) applications? E.g. I'm not so interested in the typical engineering-style applications or even really analysis-style (not that I would be upset by interesting ones) applications since I feel like those are very commonly covered many books, but if it contained computing homology or graph theory or combinatorics or etc... that would be awesome! I'm comparing against things like Axler, Hoffman/Kunze, Strang, Friedberg/Insel/Spence, which all seem to have very same-y treatments of linear algebra with no super exciting exercises to keep young math students excited!","['soft-question', 'linear-algebra', 'book-recommendation', 'reference-request']"
3188139,$ax+by=x^2+y^2\implies a=x$ and $b=y$,"I currently edit curriculum for high school geometry and I came across a mistake in one of their diagrams. After doing some work, I boiled down their mistake to an assumption that if $ax+by=x^2+y^2$ for $a,b,x,y>0$ , then $a$ is not necessarily equal to $x$ and $b$ is not necessarily equal to $y$ . However, after analyzing the corresponding graph of this equation (and using some common sense), I am quite confident that we must have $a=x$ and $b=y$ . Does anyone have any suggestions on how I could show this algebraically? Thank you.",['algebra-precalculus']
3188156,Weak Convergence in Lp space,"I'm trying to solve the following problem. Let $f_0 \in L^p(\mathbb{R})$ and let $f_n(x)=f_0(x+n)$ . Show that $f_n$ converges weakly to zero in $L^p(\mathbb{R})$ . I know that if $(x_n)$ is a sequence in $X$ , then we have the following. $\forall f \in X^*, f(x_n)$ converges to $f(x)$ iff $(x_n)$ converges to $x$ in the weak topology. I know the dual space of $L^p$ is $L^q$ where $\frac{1}{p}+\frac{1}{q}=1$ . I've been playing around with some of the well-known properties of $L^p$ spaces but haven't made much progress.","['convergence-divergence', 'lp-spaces', 'functional-analysis', 'weak-convergence']"
3188181,How $\operatorname{cl}(A) \cap \operatorname{cl}(B) = \operatorname{cl}(A \cap B) \implies$ $X$ is the discrete space?,"I know that if $A,B \subset X$ then $\operatorname{cl}(A \cap B) \subset \operatorname{cl}(A) \cap \operatorname{cl}(B)$ for every topological space $X$ , but how I use that $ \operatorname{cl}(A) \cap \operatorname{cl}(B) \subset \operatorname{cl}(A \cap B) $ to arrives that $X$ is the discrete topological space?",['general-topology']
3188293,A map is injective if it is nonzero at the generic point?,"Proposition IV 2.1 in Hartshorne's states that if $f: X\to Y$ is a finite separable morphism of curves. Then $f^{*}\Omega_Y\to \Omega_X$ is injective. And he proves this by saying that it will be sufficient to show the map is nonzero at the generic point, since both $f^{*}\Omega_Y$ and $\Omega_X$ are invertible sheaves on $X$ . I don't know why this proof can lead to the conclusion of injective. Could anyone explain? Thanks!","['curves', 'algebraic-geometry']"
3188304,When $f'(x)$ is Weierstrass function?,"$y=f(x)$ is almost everywhere differentiable. Given: $$f'(0)=0,$$ $$f(0)=0,$$ $$f(x)\geq0,$$ $$f(1)=f(-1)=1.$$ Claim: there exists $\epsilon$ such that $f$ is locally convex on interval $(-\epsilon, \epsilon).$ I think this claim is wrong. I am thinking about a counterexample similar to $f'(x)=x\sin(1/x)$ for $x\neq 0$ and $f'(0)=0$ . Will this work? Another possible example is when $f'(x)$ is a Weierstrass function. Then for any $\epsilon$ , the subderivative of function $f'(x)$ cannot be always greater than or equal to zero for $x\in (-\epsilon,\epsilon)$ .","['analysis', 'real-analysis', 'continuity', 'calculus', 'functions']"
3188320,How to determinate the the number of crossing points?,"This question is an extension of the question: how-to-determine-the-convergence-the-start-and-the-finish-points . One can apply the next algoritm and obtaine the 1-2-3 grid pattern. On square grid paper start in the middle. Draw a line 1-unit long. Turn a right angle clockwise. Draw a line 2-unit long. Turn a right angle clockwise. Draw a line 3-unit long. Repeat steps 1-6 four times. Аfter the step 6 you return to the start point (the red square #1 ). This rule is correct for any a-b-c grid pattern, where $a, b, c \in \mathbb{N}$ . 
On the figure you can see the five crossing points (four green and one blue) for the '1-2-3' pattern. The red line marks the movement by steps 1-6 from the origin. The order of $a$ , $b$ , and $c$ in the triplet is irrelevant and a desired closed-path motion can be obtained by combining translation and rotation for the original closed-path motion through matrix multiplications. Question. Is there a rule by which it can be determined the number of self-crossing points (nodes), k ,  for the a-b-c grid pattern? I am looking for a function f(a,b,c)=k . My Attempt based on Graph Theory. Below on figure you can see four patterns: 5-2-2 , 3-2-4 , 3-2-2 and 3-3-3 pattern. Start and finish points denoted by the square 1 . For the 5-2-2 pattern you can see four crossing points, for the 3-2-4 pattern -- eight crossing points, for the 3-2-2 pattern -- 12 crossing points, for the 3-3-3 pattern -- self-crossing points are absent. In the last case we can say that path was repeated in four points (squares with numbers #1, #4, #7, #10). When forming a triplet $(a, b, c)$ it is necessary to consider at least three  cases: a) all numbers are equal to one another $a = b = c$ , b) all numbers are  unique $a \neq b \neq c$ , and c) a triplet has the two repeating elements, for example, $a = b$ . In the last case, there are three possible relations between $a$ and $b$ should be emphasized: a) $a = 2\cdot b$ , b) $a> 2\cdot b$ , and c) $a < 2\cdot b$ . We have found the seven different cases but can defined f() for five cases only: $f(a,b,c)=
\left\{
\begin{array}{ll}
0, & a=b=c, \\
8, & a<b<c \quad \text{and} \quad c < a+b, \\
?, & a<b<c \quad \text{and} \quad c=a+b, \\
?, & a<b<c \quad \text{and} \quad a+b<c, \\
12, & a=b<c \quad \text{and} \quad c<a+a, \\
5, & a=b<c \quad \text{and} \quad c=a+a, \\
4, & a=b<c \quad \text{and} \quad a+a<c.
\end{array}
\right.$ As one can see a resulted graph is connected, its vertices can have odd or even degrees (2, 3 and 4). On the figure the vertecies with degree equals to 2 is colored by red, the vertecies with degree equals to 3 is colored by green and  the degree of blue vertecies is 4 . Based on examples above we can suggest that the number of self-crossing point k equals to the number of vertices that have degree more than 2 . My Attempt based on Complex Analisys. Follow to the Chris Culter answer , we work in the complex plane. Let $z=e^{i\alpha}$ . Then the first part of closed-path motion (red lines on the figures above) is: $$az^0+bz+cz^2 \tag{A}$$ and $\alpha=-\frac{\pi}{2}$ . We return to the start point #1 after $N=4$ steps.  We write equations for second, third and fourth segments by analogy: $$az^3+bz^4+cz^5, \tag{B}$$ $$az^6+bz^7+cz^8, \tag{C}$$ $$az^{9}+bz^{10}+cz^{11}. \tag{D}$$ The full path equation is $$(az^0+bz+cz^2)+(az^3+bz^4+cz^5)+
(az^6+bz^7+cz^8)+ (az^{9}+bz^{10}+cz^{11})= 0. \tag{1}$$ Then $z=e^{i\alpha}=cos(\alpha)+i \cdot sin(\alpha)=cos(-\frac{\pi}{2})+i \cdot sin(-\frac{\pi}{2}). \tag{2}$ Substitute the expression (2) in the formula (1) and collect similar terms: $$a - i \cdot b - c + i \cdot a + b - i \cdot c -
a + i \cdot b + c - i \cdot a - b + i \cdot c = 0.$$ Let us take the fist pair of relations (A) $az^0+bz+cz^2$ , (B) $az^3+bz^4+cz^5$ and the 3-2-4 pattern ( $a=3$ , $b=2$ , $c=4$ ). Using the Wolfram alpha the equation $3+2 \cdot z+4\cdot z^2=3 \cdot z^3+2 \cdot z^4+4 \cdot z^5$ was solved. We found one real root $z=1$ and four complex solutions. Refs Marc Lackenby (2016) Elementary Knot Theory","['puzzle', 'geometry', 'complex-analysis', 'planar-graphs', 'knot-theory']"
3188325,"In how many ways can 7 women, 10 men sit at table such that no woman sits besides another?","We have $7$ women and $10$ men; they sit at a table. I've been trying to solve how many ways can they sit excluding the case of women sitting next to each other. My reasoning was the following: I will have to alternate woman and men to certain extent so that no woman will sit next to another woman. The possible alternations are the following cases: $a)$ One of $7$ woman takes the first sit; one of $10$ men take the second; one of $6$ remaining women take the third; one of $9$ remaining men take the fourth, etc. So I have $10*7*9*6*8*5*7*4*6*3*5*2*4*1=10!7!$ possibilties. $b)$ One of $10$ men takes the first place; one of $7$ woman the second; etc. Again, $10!7!$ possibilities, but the order has changed. In either case, I have $3$ men remaining. Let's call $A$ and $A'$ the selections of alternated men and woman that I described on points $a)$ and $b)$ ,  and $S$ the selection of the three remaining men. The first man of $S$ is any of the three remaining; the second man of $S$ is any of the two remaining; the last one is the one that's neither of the previous two. So I have for $S$ $3*2*1=3!$ cases. So my posibilities are that $A$ is the case or $A'$ is the case, and $P$ . Which leaves me with $(10!7!+10!7!)*3!$ posibilities. Is my reasoning okay? With this type of problems it troubles me how hard it is to actually test or check if one's answer is right. (Excuse any error on my english, it's not my native language.)","['combinatorics', 'discrete-mathematics', 'factorial']"
3188430,Does $\sum_{k=1}^n|\cot \sqrt2\pi k|$ tends to $An\ln n$ as $n\to\infty$?,"Question: How can we prove that $$L(n)=\sum_{k=1}^n\left|\cot \sqrt2\pi k\right|=\Theta(n\log n)$$ as $n\to\infty$ ? Furthermore, if $\sqrt2$ is replaced with a quadratic irrational number, does it still holds? Numerical experiment . By plotting $$\frac1{n\ln n}\sum_{k=1}^n\left|\cot \sqrt2\pi k\right|,$$ we can find that it approximately tends to $0.6$ . (The following graph is added after an edit) Failed attempt of the upper bound . $$L(n)<\sum_{k=1}^nCk=C\frac{n(n+1)}2$$ for some $C$ . It can be easily deduced due to the irrationality measure $2$ of $\sqrt2$ . Failed attempt of the lower bound . Asymptotically, half of the summand is greater than $1$ due to the irrationality of $\sqrt2$ . Therefore, $L(n)>Dn$ for some $D$ when $n$ is large enough.","['diophantine-approximation', 'number-theory', 'irrationality-measure', 'asymptotics', 'fractional-part']"
3188460,What is the 'smaller' side of an angle called?,"Every angle has two sides, a big one and a small one. For example, a 90 degree angle also has a 270 degree angle on the other side. Is there a technical name within mathematics for the big and small sides of an angle? Thanks!","['geometry', 'terminology']"
3188467,Is it true that the eigenvalues of $A + B$ are the sum of some eigenvalue of $A$ and some eigenvalue of $B$?,"Is it true that the eigenvalues of $A + B$ are the sum of some eigenvalue of $A$ and some eigenvalue of $B$ ? I'm taking a linear algebra class, and I recently learned about eigenvalues. I think that this claim is true, but it is not a theorem that I can find anywhere. I've tried it for many examples, and I just wanted someone to confirm. Thanks","['linear-algebra', 'linear-transformations']"
3188471,Two boys pick a subset of $40$ toys that they like. They can pick the same ones. What is the probability that they picked three same toys or more?,"Two boys pick a subset of $40$ toys that they like. They can pick the same ones. What is the probability that they picked three same toys or more?
My answer would be $$\frac{ \sum_{ i =3}^{40} \binom{40}{i} 3^{40-i}} { 2^{40} 2^{40}}.$$ Is that right? I would first pick the same toys that they picked, then for each of the remaining toys, I would either give to the first boy, second boy or nobody.",['probability']
3188476,What does vanishing of higher direct images of the structure sheaf tell us?,"Let $f:X\to Y$ be a morphism of schemes over $k$ . I am wondering about, what geometric consequences $R^qf_*O_X=0$ for $q\geq k$ does have. I saw vanishing of higher direct images used in some proofs of coherence of structure sheaves and also for the computation of cohomology if one in addition assumes that $f_*O_X=O_Y$ . Also there seem to be a lot of papers out there discussing cases in which the higher direct images vanish for $q\geq 1$ . What are natural situations in which one wants vanishing of higher direct images and what does it tell us (geometrically)? I know this is kind of vague but I was unable to figure it out myself. Thanks in advance!","['algebraic-geometry', 'abstract-algebra']"
3188500,$f$ is $\mathbb{R}$ - differentiable iff $Re(f)$ and $Im(f)$ are $\mathbb{R}$ - differentiable,"I just read that a sufficient condition for a function $f:A \rightarrow \mathbb{C},f(z) = u(z)+ i v(z)$ to be holomorphic is: $A$ open. f is $\mathbb{R}$ - differentiable in $A$ . The Cauchy Riemann equations hold in $A$ . In the book i' m reading $\mathbb{R}$ - differentiability is defined as: $f: D \rightarrow \mathbb{C}$ is $\mathbb{R}$ - differentiable in $z_0$ if there exists an $\mathbb{R}$ - linear map $T$ such that $$\lim_{z \rightarrow z_0}\frac{f(z)-f(z_0)-T(z-z_0)}{| z-z_0 |}=0$$ Is this equivalent to saying that $Re(z)$ and $Im(z)$ (which are real valued) are differentiable in $A$ ?","['complex-analysis', 'cauchy-riemann-equations']"
3188517,"Stirling numbers of second kind, but no two adjacent numbers in same part.","Update: The problem has been solved. @Phicar and I individually give two transformation from $h\rightarrow S$ and $S\rightarrow h$ , and they are inverse of each other. Any other explanation or bijective is still welcomed! We know that the number of ways to put $n$ distinct balls (indexed $1,2,\ldots,n$ ) into $m$ non-empty non-distinct boxes ( $m\leq n$ ) is the Stirling number of second type $S(n,m)$ We have the formula $S(n,m)=S(n-1,m-1)+mS(n-1,m)$ as well as the initial value $S(n,n)=S(n,1)=1$ Now we add the restriction that the adjacent balls should not be put into the same box（here we define $1$ and $n$ is non-adjacent）,and the number of ways is $h(n,m)$ Similarly, we have $h(n,m)=h(n-1,m-1)+(m-1)h(n-1,m)$ and $h(n,n)=1,h(n,2)=1​$ . The only thing change here is the coefficient of the second term. In fact, we can easily get the result that $h(n,m)=S(n-1,m-1)$ But I cannot figure out a more intuitive explanation or a bijective to show this equivalent relationship. Here I provides some basic example $h(4,3)=S(3,2)=3​$ ， $\{13|2|4\},\{14|2|3\},\{1|24|3\}​$ and $\{12|3\},\{13|2\},\{1|23\}​$ $h(5,3)=S(4,2)=7$ , $\{135|2|4\},\{13|25|4\},\{14|25|3\},\{14|2|35\},\{15|24|3\},\{1|24|35\},\{13|24|5\}$ and $\{124|3\},\{12|34\},\{134|2\},\{13|24\},\{14|23\},\{1|234\},\{123|4\}$","['combinatorics', 'stirling-numbers', 'recurrence-relations']"
3188675,What is wrong with this false proof of $\pi=0$?,"Consider the integral $$I=\int_{-1}^{1}\frac{1}{x^2+1}\mathrm{d}x$$ Now, from the standard integral results we know, $$\int\frac{1}{x^2+1}\mathrm{d}x=\arctan(x)+c$$ So, $$\int_{-1}^{1}\frac{1}{x^2+1}\mathrm{d}x=\arctan(1)-\arctan(-1)=\frac{\pi}{4}-(-\frac{\pi}{4})=\frac{\pi}{2}$$ Now, if you are bored and just doing random things you might evaluate this integral in a roundabout way by the substitution $u=\dfrac{1}{x}$ . This gives the bound for the integrals as $u=\dfrac{1}{-1}=-1$ to $u=\dfrac{1}{1}=1$ and the differential becomes $\mathrm{d}{x}=\dfrac{-1}{u^2}\mathrm{d}{u}$ . So, the integral becomes $$\int_{-1}^{1}\frac{1}{\frac{1}{u^2}+1}\frac{-1}{u^2}\mathrm{d}u = -\int_{-1}^{1}\frac{1}{u^2+1}\mathrm{d}u=-I$$ . Now, $$I=-I  \implies 2I=0\implies \pi=0$$ . So, there is your false proof. I think that the problem in the proof comes when we do the substitution $x=\dfrac{1}{u}$ . But, I dunno what it is. Does it have to do something with the continuity of the substitution, or is it something else.","['integration', 'fake-proofs']"
3188689,"$f$ is monotonically increasing, $0 \le f \le 1$ and $\int_0^1 (f(x) - x) dx = 0$ then $\int_0^1|f(x)-x|dx \le \frac{1}{2}$.","$f(x)$ is monotonically increasing in $[0,1]$ , $0 \le f \le 1$ and $\int_0^1 (f(x) - x) \mathrm{d}x = 0$ . Prove that $\int_0^1|f(x)-x|\mathrm{d}x \le \frac{1}{2}$ . It's easy if $f(x) \ge x$ in $[0,1]$ . And even in $[a,b]$ we have $\int_a^b |f(x)-x|\mathrm{d}x \le \frac{(b-a)^2}{2}$ . But the zero points of $f(x) - x$ may be infinitely many. This is where difficulty exists.",['analysis']
3188708,Topological Algebraic Independence of power series,"Let $p$ be a prime number, let $x$ be a variable, and consider two power series over the ring $\mathbb{Z}_p$ of $p$ -adic integers: $a(x):=\underset{n\geq 1}{\sum}{\frac{p^n}{n!}x^n}=px+\frac{p^2}{2}x^2+\frac{p^3}{6}x^3+\cdots$ $b(x):=\underset{n\geq 1}{\sum}{\frac{p^n}{n!}x^{2n}}=px^2+\frac{p^2}{2}x^4+\frac{p^3}{6}x^6+\cdots$ My question is, can we find a power series $0\neq f(u,v)\in\mathbb{Z}_p[[u,v]]$ with coefficients in $\mathbb{Z}_p$ , such that $f(a,b)=0$ , or in other words are $a(x)$ and $b(x)$ topologically algebraically independent (TAI) over $\mathbb{Z}_p$ ? It is not true that $a$ and $b$ are TAI over $\mathbb{Q}_p$ , which we can observe simply by choosing a sequence of polynomials with rational coefficients which remove successively higher and higher powers of $x$ . For example: $0=a(x)^2-pb(x)-pa(x)b(x)-\frac{(7p-12)p}{12}b(x)^2+\cdots$ In fact, we can apply the same argument to say that any distinct pair of univariate power series over $\mathbb{Q}_p$ are not TAI over $\mathbb{Q}_p$ . Unfortunately, I can think of no way of ensuring that the coefficients of this power series lie in $\mathbb{Z}_p$ , or even that the series can be scaled by a power of $p$ so that they will. If anyone has any ideas or suggestions, I'd be very interested to hear them. Thanks.","['number-theory', 'p-adic-number-theory', 'power-series', 'convergence-divergence', 'prime-numbers']"
3188744,Collisions during random insertion unto given domain,"Here's a satisfying one. Assume a list of maximum length L which starts out empty. For the first element we pick a random number within the domain [0 ... L ), we check if the list already contains it (it obviously doesn't when the list is empty), and if it's not already present then we populate the first empty element with that random number. If the number is present in the list (which will happen during later iterations) then we count a collision, we choose another random number, we check it again, and we try to populate the same position in the list. We iterate until the list is complete, counting collisions along the way. Counting the number of collisions for each element in the list, and graphing that against the list's occupation level, we end up with the following graph: The x axis represents the list's occupation degree, and the y axis represents the average number of collisions for each attempt to choose a collision-free number. Note that the y axis is logarithmic – an exponential trend line would be represented as a straight line. I find a lot of satisfying things in this graph: the shape of the curve, the symmetry of the y range (10E-3 ... 10E+3), and the fact that the sweet spot where there's one collision per number selection is at roughly 33%. Having said that, everything here is based on my empirical attempts to work out how this works – I averaged multiple iterations of computer-generated simulations to get to this graph. I have tested it with both [0...1000) and [1...10000), and the results were basically identical. I would appreciate a confirmation that my empirical results can be validated using a formal mathematical approach, rather than my naïve computer simulations.",['statistics']
3188816,How to prove that the $n$-sphere $S^n$ is path connected for $n\geq 1$?,"How would I prove that the $n$ -sphere $S^n$ is path connected for $n\geq 1$ ? I have seen this question. I do not really understand this section: For b) The application $\varphi$ from $\mathbb{R}^n \setminus \{ 0 \}$ to $S^{n-1}$ defined by $x \to x/\Vert x \Vert_2$ is continuous. If $x,y$ are on the sphere and not on a diameter, the path $\varphi([x,y])$ is continuous and join $x$ to $y$ on the sphere. If $x$ , $y$ are on a diameter, $\varphi([x,z][z,y])$ is a solution. What is $\varphi$ ? What is $x/||x||_2$ ? This is my proof attempt: Proof: $\newcommand{\R}{\mathbb{R}}$ We want to show that the $n$ -sphere is path connected for $n\geq 1$ . Let $S^n$ be an $n$ -sphere where $n\geq 1$ . Then $S^n\subseteq\R^{n+1}$ . We know $\R^{n+1}$ is path connected, as is every open and every closed ball in $\R^n$ . Let $f:\R^{n+1}\to S^n$ be a function ???. (What would this be?) Then by Theorem 6.29, $f(\R^{n+1}) = S^n$ is a path connected subspace of $S^n$ Because $S^n$ is path connected in the subspace topology that $S^n$ inherits from $S^n$ , we have that $S^n$ is path connected in $S^n$ . In other words, the $n$ -sphere $S^n$ is path connected for $n\geq 1$ . $\square$ Thank you for any help you can provide. EDIT: Removed textbook specific definition and theorem references.","['proof-explanation', 'general-topology', 'path-connected', 'real-analysis']"
3188817,How to apply continuous functional calculus,"This is the statement I am using. Theorem 2.17, pg 34: Suppose that $A$ is a unital $C^*$ algebra and that $a$ is a normal element in $A$ , then there is a $*$ -isometric isomorphism $$C(\sigma(a)) \rightarrow C^*(\{1_A, a \}) \subseteq A$$ such that $id \mapsto a$ , where $id$ is the identity function on $\Bbb C$ . I want apply this result to a bounded operator $T$ on Hilbert space $H$ . If $T \in B(H)$ is normal, and $\sigma(a)$ is discrete, then the ""spike"" functions, $$\delta_\lambda(x):=
\begin{cases}
1 \text{ if } \lambda =x \\ 
0 \text{ otherwise } 
\end{cases}
$$ Maps to the projection map $$\delta_\lambda(T)= P_\lambda$$ the projection map on to the eigenspaces. This seems intuitively true. But I don't know how to prove it. The problem may not  be well defined too, since we do not know that the eigenspaces are in fact closed - let us suppose this to be the case.","['functional-calculus', 'operator-algebras', 'operator-theory', 'hilbert-spaces', 'functional-analysis']"
3188843,Is it $Z(\operatorname{Aut}(G)) \cap \operatorname{Inn}(G) \cong H/Z(G)$ for some $H \le G$?,"Could you check if this proof is correct, please? (I'm not even sure about the result itself, whence the title.) Proposition . Let $G$ be a group. Then: $$Z(\operatorname{Aut}(G)) \cap \operatorname{Inn}(G) \cong H/Z(G)$$ where $H=\lbrace a \in G \mid \sigma(a) \in Z(G)a, \forall \sigma \in \operatorname{Aut}(G) \rbrace$ . Proof . Let $\varphi: G \rightarrow \operatorname{Aut}(G)$ be the homomorphism induced by conjugacy, namely $\varphi_a(g):=a^{-1}ga$ . We get: \begin{alignat}{1}
\varphi_a \in Z(\operatorname{Aut}(G)) &\Leftrightarrow \varphi_a\sigma=\sigma\varphi_a, \forall \sigma \in \operatorname{Aut}(G) \\
&\Leftrightarrow (\varphi_a\sigma)(b)=(\sigma\varphi_a)(b), \forall b \in G, \forall \sigma \in \operatorname{Aut}(G) \\
&\Leftrightarrow \varphi_a(\sigma(b))=\sigma(\varphi_a(b)), \forall b \in G, \forall \sigma \in \operatorname{Aut}(G) \\
&\Leftrightarrow \varphi_a(\sigma(b))=\sigma(a^{-1}ba), \forall b \in G, \forall \sigma \in \operatorname{Aut}(G) \\
&\Leftrightarrow \varphi_a(\sigma(b))=\sigma(a^{-1})\sigma(b)\sigma(a), \forall b \in G, \forall \sigma \in \operatorname{Aut}(G) \\
&\Leftrightarrow \varphi_a(\sigma(b))=\sigma(a)^{-1}\sigma(b)\sigma(a), \forall b \in G, \forall \sigma \in \operatorname{Aut}(G) \\
&\Leftrightarrow \varphi_a(\sigma(b))=\varphi_{\sigma(a)}(\sigma(b)), \forall b \in G, \forall \sigma \in \operatorname{Aut}(G) \\
&\Leftrightarrow (\varphi_a\sigma)(b)=(\varphi_{\sigma(a)}\sigma)(b), \forall b \in G, \forall \sigma \in \operatorname{Aut}(G) \\
&\Leftrightarrow \varphi_a\sigma=\varphi_{\sigma(a)}\sigma, \forall \sigma \in \operatorname{Aut}(G) \\
&\Leftrightarrow \varphi_a=\varphi_{\sigma(a)}, \forall \sigma \in \operatorname{Aut}(G) \\
&\Leftrightarrow \sigma(a) \in (\operatorname{ker}\varphi)a, \forall \sigma \in \operatorname{Aut}(G) \\
&\Leftrightarrow a \in H \\
\end{alignat} where $H:= \lbrace a \in G \mid \sigma(a) \in Z(G)a, \forall \sigma \in \operatorname{Aut}(G) \rbrace $ . Thence, $H=\varphi^{\leftarrow}\lbrace \operatorname{Inn}(G) \cap Z(\operatorname{Aut}(G)) \rbrace$ and, by the Correspondence Theorem: $H \le G$ , $H \supseteq Z(G)$ , $H/Z(G) \cong \operatorname{Inn}(G) \cap Z(\operatorname{Aut}(G))$ . $\Box$ EDIT: Corollary $Z(\operatorname{Aut}(G)) \cap \operatorname{Inn}(G) = \lbrace \iota  \rbrace \Leftrightarrow H=Z(G)$ : this holds if $G$ is abelian (trivially, being then $\operatorname{Inn}(G)=\lbrace \iota \rbrace$ ). Are there nonabelian $G$ s such that $H=Z(G)$ ? If $G$ is centerless $(Z(G)=\lbrace e \rbrace)$ , then: $$Z(\operatorname{Aut}(G)) \cap \operatorname{Inn}(G) \cong \lbrace a \in G \mid \sigma(a)=a, \forall \sigma \in \operatorname{Aut}(G) \rbrace = \bigcap_{\sigma \in \operatorname{Aut}(G)}\operatorname{Fix}(\sigma)$$ where $\operatorname{Fix}(\sigma):=\lbrace g \in G \mid \sigma(g)=g \rbrace$ .","['group-theory', 'abstract-algebra', 'proof-verification']"
3188850,Alternate inner products on Euclidean space?,"After reading about inner products as a generalization of the dot product, I was hoping to be able to prove that the dot product is in some sense the unique inner product in Euclidean space (e.g., up to constant scaling). But it seems that there are a whole bunch of alternative inner products in $\mathbb{R}^2$ with nonzero cross-terms between basis vectors, for example, $\langle (a, b)^\intercal, (x, y)^\intercal \rangle = ax + by + 0.5(ay + bx)$ . Unless I've made a mistake, this satisfies symmetry, linearity, and positive-definiteness. Is there a sense in which the dot product is the canonical inner product on Euclidean space? Or do we just pick it because the implied norm matches our notion of distance?","['inner-products', 'linear-algebra']"
3188866,"Is $\operatorname{SL}(n,\mathbb{R})/\operatorname{SL}(n, \mathbb{Z})$ a Hausdorff space?","The special linear group $\operatorname{SL}(n,\mathbb{R})$ of degree $n$ over $\mathbb{R}$ is the set of $n \times n$ matrices with determinant $1$ , with the group operations of ordinary matrix multiplication and matrix inversion. We denote by $\operatorname{SL}(n,\mathbb{Z})$ the group of $n \times n$ matrices  with integer entries and determinant equals 1. Note that $\operatorname{SL}(n,\mathbb{Z})$ is a discrete subgroup of $\operatorname{SL}(n,\mathbb{R})$ . Let $G$ to be a topological group and $H$ to be a subgroup of $G$ . We will say that a regular Borel measure $\mu$ on the quotient $G/H$ is a left invariant Haar measure if for all Borel sets $E \subseteq G/H$ and all $g \in G$ we have $\mu(gE) = \mu(E)$ . If $G$ is a locally compact Hausdorff group and $\Gamma$ is a discrete subgroup such that $G/H$ carries a finite left G-invariant Harr measure, then we say that $\Gamma$ is a $\textbf{lattice}$ in $G$ . We have the following results: $\operatorname{SL}(n,\mathbb{Z})$ is a lattice in $\operatorname{SL}(n,\mathbb{R})$ . Moreover, the quotiont $\operatorname{SL}(n,\mathbb{R})/\operatorname{SL}(n,\mathbb{Z})$ is not compact. (Mahler Criterion) For a sequnece $(g_{m})_{m\in \mathbb{N}}$ of $\operatorname{SL}(n,\mathbb{R})$ ,  the sequence $(\pi (g_{m}))_{m\in \mathbb{N}}$ of $\operatorname{SL}(n,\mathbb{R})/\operatorname{SL}(n, \mathbb{Z})$ does not have a convergent subsequence if, and only if, there exists a sequence $v_{m} \in \mathbb{Z}^{n}$ with $v_{m} \neq 0$ such that $g_{m}(v_{m})$ tends to $0$ . where $\pi\colon\operatorname{SL}(n,\mathbb{R}) \to  \operatorname{SL}(n,\mathbb{R})/\operatorname{SL}(n,\mathbb{Z})$ is the natural projection. The second result suggest to me that $\operatorname{SL}(n,\mathbb{R})/\operatorname{SL}(n, \mathbb{Z})$ is a Hausdorff space, but I can't find any reference. So, I do not know if it is true. Thanks","['lie-algebras', 'riemannian-geometry', 'lattices-in-lie-groups', 'lie-groups', 'differential-geometry']"
3188896,Heuristic on Sobolev and BV functions,"Let $f: \Omega \subset \mathbb{R}^N \to \mathbb{R}^M$ be a Sobolev or BV vector field. A heuristic that I've heard frequently is the following: $f$ is almost Lipschitz on a large ""good"" set but there is a small ""bad"" set where $Df$ is very large. What theorems make this heuristic rigorous?","['reference-request', 'real-analysis', 'sobolev-spaces', 'functional-analysis', 'soft-question']"
3188929,"Prove that $[\mathbf{Q}(\sqrt{1+i},\sqrt{2}):\mathbf{Q}]=8$.","I am trying to calculate the Galois group of the polynomial $f=X^4-2X^2+2$ . $f$ is Eisenstein with $p=2$ , so irreducible over $\mathbf{Q}$ . I calculated the zeros to be $\alpha_1=\sqrt{1+i},\alpha_2=\sqrt{1-i},\alpha_3=-\alpha_1$ and $\alpha_4=-\alpha_2$ . Let $\Omega_f=\mathbf{Q}(\alpha_1,\alpha_2,\alpha_3,\alpha_4)=\mathbf{Q}(\alpha_1,\alpha_2)$ be a splitting field of $f$ over $\mathbf{Q}$ . Since $\alpha_1\alpha_2=\sqrt{1+i}\sqrt{1-i}=\sqrt{2}$ , we have $\Omega_f=\mathbf{Q}(\sqrt{1+i},\sqrt{2})$ . So if we can prove that $[\Omega_f:\mathbf{Q}]=8$ , then we have $\#\operatorname{Gal} (f)=8$ and for $\operatorname{Gal}(f)\subset S_4$ , we must have that it is isomorphic to the dihedral group $D_4$ . How do I go about proving $[\mathbf{Q}(\sqrt{1+i},\sqrt{2})]=8$ ?","['galois-theory', 'abstract-algebra', 'splitting-field']"
3188931,Question about linear algebraic groups split vs isotropic,"I am reading notes on linear algebraic groups and I'm getting confused with some definitions and I would appreciate any clarification. They define $G$ to be split if there exists a maximal torus $T$ of $G$ that is split. Then later on they say: If there is no split torus contained in $G$ then $G$ is said to be anisotropic. Otherwise $G$ is said to be isotropic. If $G$ is isotropic then there exists a maximal torus $T$ contained in $G$ unique up to conjugation. 1) With this definition, to me it looks like split and isotropic mean the same thing... What am I missing here? 2) When $G$ is split we have the decomposition of the Lie algebra of $G$ as $$
\mathfrak{g} = \mathfrak{t} \oplus \oplus_{\alpha \in \Phi(G,T)} \mathfrak{g}_{\alpha}
$$ where $\mathfrak{t}$ is the Lie algebra of $T$ (and the rest with usual notation of roots of $T$ in $G$ with root spaces). But when isotropic we have $$
\mathfrak{g} = \mathfrak{m} \oplus \oplus_{\alpha \in \Phi(G,T)} \mathfrak{g}_{\alpha}
$$ where $\mathfrak{m}$ is the $0$ eigenspace. 
If someone could also explain (or provide me with some idea) me where this difference is coming from, I would greatly appreciate it. Thank you. PS Further clarification regarding the second question: when $G$ is split we have that the Lie algebra of $G$ has the decomposition $$
\mathfrak{g} = \mathfrak{t} \oplus \oplus_{\alpha \in \Phi(G,T)} \mathfrak{g}_{\alpha}
$$ where $\mathfrak{t}$ is the Lie algebra of $T$ and each $\mathfrak{g}_{\alpha}$ is $1$ dimensional. However in the situation between split and anisotropic, my understanding is that we don't have exactly the same situation. In the above notation we have $\mathfrak{m}$ is not (necessarily?) the Lie algebra of $T$ and each $\mathfrak{g}_{\alpha}$ is not (necessarily?) $1$ dimensional anymore. I guess I was hoping I could get some idea on why this happens to be the case... (Even though maybe the only difference is that when there is a maximal split torus the situation is ""nice"" and not as nice otherwise)","['definition', 'algebraic-geometry', 'lie-algebras', 'algebraic-groups']"
3188940,Model of ordered plane which is neither isomorphic to $\mathbb{R}^2$ nor to Klein model,"Let $B_{\mathbb{R}}\subset\mathbb{R}\times\mathbb{R}\times\mathbb{R}$ be standard (strict) betweenness relation on $\mathbb{R}$ i.e. $$B_{\mathbb{R}}(abc):\iff\left(a<b<c \vee c<b<a\right)$$ My definition of ordered plane is as follows: $P$ is a set (plane), $\mathcal{L}\subset 2^P$ is a family of lines and $B\subset P\times P\times P$ is a ternary betweenness relation. We say that $(P,\mathcal{L},B)$ is an ordered plane whenever $(P,\mathcal{L})$ is a model of Hilbert's incidence axioms. $B(abc)$ implies that $a,b,c$ are collinear. For any line $L$ : $(L,B|_{L\times L\times L})$ is isomorphic to $(\mathbb{R},B_{\mathbb{R}})$ . Pasch's axioms holds. I know two standard models of this axioms i.e. $\mathbb{R}^2$ and Klein model and I'm looking for a model which is not isomorphic to any of these two. Basically these axioms are a variant of Hilbert's neutral geometry axioms excluding congruence relations. Actually I think it's a bit more because every line is isomorphic to $\mathbb{R}$ and maybe there are models of Hilbert's axioms without congruence (but with continuity) in which at least one line is not isomorphic to $\mathbb{R}$ (This is something I am asking as well). It is known that neutral geometry has exactly two models up to isomorphism but excluding congruence should make it have more models. Note that these axioms involve (Dedekind) continuity axiom and as a result there are no countable models such as ""rational"" plane.","['model-theory', 'axiomatic-geometry', 'geometry']"
3188966,Number of real solutions to $x^7 + 2x^5 + 3x^3 + 4x = 2018$,"Find the number of real solutions of $x^7 + 2x^5 + 3x^3 + 4x = 2018$ ? What is the general approach to solving this kind of questions? I am interested in the thought process. Few of my thoughts after seeing this question:
since $x$ has all odd powers so, it can not have any negative solution. 
2018 is semiprime; not much progress here. We can sketch the curve but graphing a seven order polynomial is difficult.","['algebra-precalculus', 'polynomials']"
3188991,The Laplacian operator is invariant to $SL_2(\mathbb{R})$,"I am reading Iwaniec's book on the spectral analysis of automorphic forms, where I bumped into the following statement in p.20 section 1.6. Given a function $f:\mathbb{H}\longrightarrow \mathbb{C}$ , having continuous second derivatives, and some $g\in SL_2(\mathbb{R})$ , then $\Delta(f(gz)) = (\Delta f)(gz)$ . Note that the Laplacian operator we use on the hyperbolic plane $\mathbb{H}$ , is $$\Delta = y^2(\dfrac{\partial^2}{\partial x^2} + \dfrac{\partial^2}{\partial y^2})$$ My idea is that in order to prove such a statement, it is enough to prove it over the generators of $SL_2(\mathbb{R})$ , i.e., it is enough to check that the identity holds for $g_t(z) = z + t$ for $t\in\mathbb{R}$ , and $g^*(z) = -\dfrac{1}{z}$ . I found it easy proving the statement for $g=g_t$ , however for $g^*$ I didn't understand why it is true that: $$\Delta(f(-\dfrac{1}{z})) = (\Delta f)(-\dfrac{1}{z})$$ Writing down the formulas for the Laplacian, we get that the LHS is equal to: $$y^2(\dfrac{\partial^2 (f\circ\dfrac{-1}{z})}{\partial x^2} + \dfrac{\partial^2 (f\circ\dfrac{-1}{z})}{\partial y^2})$$ Whereas the RHS equals: $$\Im(-\dfrac{1}{z})^2(\dfrac{\partial^2 f}{\partial x^2}(\dfrac{-1}{z}) + \dfrac{\partial^2 f}{\partial y^2}(\dfrac{-1}{z}))$$ I could continue with the computation, however, simply analyzing the coefficient of $$\dfrac{\partial^2 f}{\partial x^2}(\dfrac{-1}{z})$$ on both sides seems to show me that I am on the wrong path. For the LHS: $$y^2\cdot\dfrac{\partial^2 f}{\partial x^2}(-\dfrac{1}{z})\dfrac{\partial^2 (-\dfrac{1}{z})}{\partial x^2}$$ For the RHS I obtain: $$\Im(-\dfrac{1}{z})^2\dfrac{\partial^2 f}{\partial x^2}(\dfrac{-1}{z})$$ Hence the identity could potentially be true if one has the equality: $$\Im(-\dfrac{1}{z})^2 = y^2\dfrac{\partial^2 (-\dfrac{1}{z})}{\partial x^2}$$ However, the RHS is $\dfrac{-2y^2}{z^3}$ , whereas the LHS is: $\dfrac{y^2}{(x^2+y^2)^2}$ , we cannot expect equality in the general case, so I am confused.","['complex-analysis', 'automorphic-forms', 'laplace-transform']"
3189011,Proof - Number of pairs in a cartesian product set,"I am attempting to prove that $${n+1 \choose 2}+{n \choose 2} = n^2$$ in a combinatorial way. I start by using a set $N=\{1,2,...,n\}$ and stating that $\vert N\times N\vert=n^2$ . Then I want to partition the set of the cartesian product into two subsets which contain (I) The pairs $(s,t)$ where $s<t$ (II) The pairs $(s,t)$ where $s>t$ or $s=t$ There is a solution to the problem proving (II) by introducing a new set $N\cup\{x\}$ where $x$ is a new symbol. I find this way a little confusing and was wondering if the following way is correct. For simplicity, I use the set $N=\{1,2,3\}$ but this applies for any set $N$ defined above. In the image below, the blue lines depict partition (I) and it shows clearly that you can only form this partition having ${n \choose 2}$ elements. The red lines show how to create (II). Since an element can choose itself or another less than it, then there are ${n \choose 2} + n = {n+1 \choose 2}$ elements in (II). Is this reasoning using the image correct, and is there additional information that can be added to this proof that show how the respective partitions have those many elements?","['combinatorics', 'discrete-mathematics']"
3189041,Is the Unitary Group of a Hilbert Space a Lie group?,"Let $H$ be an infinite-dimensional complex Hilbert space.  Then the set of unitary operators on $H$ forms a group, known as the unitary group or Hilbert group.  My question is, is this group a Lie group?  That is, is there a standard Lie group structure for this group? I ask because in quantum mechanics, at least subgroups of this group seem to be treated as Lie groups. EDIT: The second page of this paper off-handedly mentions ""the Frechet Lie group $U(H)$ consisting of all unitary operators on H, equipped with the strong operator topology"".  What that means is that the Lie group structure on $U(H)$ with the strong operator topology is a Frechet manifold, i.e. a manifold which is locally isomorphic to a Frechet space rather than a finite-dimensional Euclidean space.  But does anyone know the details of this Frechet manifold structure?","['representation-theory', 'hilbert-spaces', 'functional-analysis', 'quantum-mechanics', 'lie-groups']"
3189067,"Show that $(\binom{p^2}{p} -p ) $ is divisible by $p^5$, for every prime number $p, p\ge 5$","Show that $(\binom{p^2}{p} -p ) $ is divisible by $p^5$ , for every prime number $p, p\ge 5$ . I have a combinatorics problem, and this is what it reduces to. I am not quite sure how to link the fifth power in divisiblity. Edit: I have shown this is equivalent to $$(p-1)!\cdot(\sum_{i=1}^{p-1} \frac{1}{i}) \equiv 0 \pmod  {p^2}$$","['divisibility', 'number-theory', 'elementary-number-theory', 'combinatorics', 'prime-numbers']"
3189082,Derivative of an integral function with variable bound that appears in integrand $y(t) = \int_{t_0}^t \sin(t-s)g(s)ds$,"I have a function: $$y(t) = \int_{t_0}^t \sin(t-s)g(s)ds$$ I want to calculate $y',y''$ .
I tried to use fundamental theorem of Calculus but I don't know how to apply it here. because the bound of integration is present in the function itself and the integrand doesn't have a constant form for different t. How should I use Fundamental theorem of Calculus in this situtaion?","['integration', 'calculus', 'derivatives']"
3189132,Is there a formal definition for $f(x)$ ~ g(x)?,"I was looking to see if curved asymptotes were possible and came across an answer that referred to an end behavior of a function as being $f(x)$ ~ $x^2$ .  I'm assuming this either means the end behavior of a function or a generalization of what a function does given a set of large x values, but I don't want to simply assume something I don't know.  Is there a formal definition for what $f(x)$ ~ $g(x)$ is?","['definition', 'functions', 'asymptotics']"
3189208,Find a pattern/function between p and m,"I have this set of values and I am trying to find a pattern or function that links p and m. If I'm given the value of p, is there a formula that can generate m? p   m
1   1
2   1
3   1
4   2
5   2
6   2
7   2
8   2
9   2
10  3
11  3
12  3
13  3
14  3
15  3
16  3
17  3
18  3
19  3
20  4
21  4
22  4
23  4
24  4
25  4
26  4
27  4
28  4
29  4
30  4
31  4
32  4
33  4
34  4
35  5
36  5
37  5
38  5
39  5
40  5
41  5
42  5
43  5
44  5
45  5
46  5
47  5
48  5
49  5
50  5
51  5
52  5
53  5
54  5
55  5
56  6
57  6
58  6
59  6
60  6
61  6
62  6
63  6
64  6
65  6
66  6
67  6
68  6
69  6
70  6
71  6
72  6
73  6
74  6
75  6
76  6
77  6
78  6
79  6
80  6
81  6
82  6
83  6
84  7
85  7
86  7
87  7
88  7
89  7
90  7
91  7
92  7
93  7
94  7
95  7
96  7
97  7
98  7
99  7
100 7
101 7
102 7
103 7
104 7
105 7
106 7
107 7
108 7
109 7
110 7
111 7
112 7
113 7
114 7
115 7
116 7
117 7
118 7
119 7 Here's what I've observed. There are 3 numbers (1-3) that produce 1, 6 numbers(4 - 9) that produce 2, 10 numbers (10 - 19), that produce 3, 15 numbers (20 - 34) that produce 4, 21 numbers (35-55) that produce 5 and so on so forth. So basically, the size of each group of numbers is increasing by triangular numbers n(n + 1) / 2. I've been thinking about it for a few hours but I can't formulate a formula that will calculate m if I'm provided with the value of p. For more context and background to this problem, I am studying tetrahedral numbers and the value p is just any randomly selected positive integer, and the value of m is the length of a list of tetrahedral numbers ( https://www.geeksforgeeks.org/tetrahedral-numbers/ ) smaller than p. For example, when p = 5, the list of tetrahedral numbers smaller than 5 is {1, 4} and the size of that list, m = 2. When p = 40, the list = {1, 4, 10, 20, 35} and the size of that list, m = 5.",['functions']
3189300,Moving particles on graph,"Consider the complete graph $K_n$ , and suppose we put $k$ different particles in each vertex, so there are $kn$ particles in total. We say the $k$ particles in a vertex are paired with each other. Now we do the following game: At the step $1$ we must move every particle from its current vertex to another one, with the following rules in mind: No particle can stay in its position. No particle can stay paired with another one. There can not be more than $k$ particles in each vertex at the end of the step. So, after the step $1$ every two particles $p,q$ which were paired before (shared the same vertex before the step 1) must go to different vertices. Then, we end up with a different configuration of the $kn$ particles in the $n$ vertices. So, if we have just done the step $i$ , we proceed with the step $i+1$ as follows:
We must move every particle from its current vertex to another one, with the following rules No particle can stay in its position nor to a vertex it's been before. No particle can be paired with any particle it's been paired before with. There can not be more than $k$ particles in each vertex at the end of the step. We continue until we cannot proceed at some step $s$ ; then the game ends. We win if we can make every particle reach every vertex, and we lose otherwise. So, my questions are: Is there any way to build some algorithm to know if some game with $k$ particles in each vertex is winnable? In particular, is there any way to find the value of $k$ such that a game with $k$ particles in each vertex is winnable, but a game with $k+1$ particles is not? Is there a more or less known (solved or not) problem or theorem which could help getting a solution for this one (In case a solution of this problem is not so elementary)? For example, with $1$ particle in each vertex the game is clearly winnable (Just go through a Hamiltonian cycle with all the particles), and with $n$ particles the game is not (There will be two vertices which must stay paired or one vertex which doesn't move at the step $1$ ). I'm particularly interested in the case where $n=6$ , but when I thought about it I wanted to generalize it. Of course, instead of being $K_n$ we can start with an arbitrary Hamiltonian graph $G$ , modifying the number of particles each vertex has initially having the particles only move from a vertex to some of its neighbors, but it can get more complex. The game came from a problem my father (who is a P.E. teacher at a school) asked me about. He has $36$ students and wanted to make 6 groups, place 6 students in each group to discuss some topic, and then each student must go to another group to discuss another topic. But he didn't want the students to stay in the same group or repeat partners. Of course, this is not possible, but I can try giving him an alternative where instead of being groups of 6 people, they are, for example groups of $3$ pairs of people.","['graph-theory', 'combinatorics']"
3189307,Inverse of $\frac{\sin(x)}{x}$,"How would one find the inverse of the function $y=\frac{\sin(x)}{x}$ ? Here are my steps: $y=\frac{\sin(x)}{x}$ , $x=\frac{\sin(y)}{y}$ , $xy=\sin(y)$ , $\arcsin(xy)=y$ ,
After that step, I can’t find a way to isolate $y$ .","['trigonometry', 'functions', 'inverse-function']"
3189329,I need to find the potential function of a vector field.,"I was given F = (y+z) i + (x+z) j + (x+y) k . I found said field to be conservative, and I integrated the x partial derivative and got f(x,y,z) = xy + xz + g(y,z). The thing is that I am trying to find g(y,z), and I ended up with something that was expressed in terms of x, y and z (I got x+z-xy-xz). I don't know what to do with this information not that I arrived at something expressed in all three variables.","['integration', 'vector-fields', 'multivariable-calculus']"
3189376,Proof involving the spectral radius and the Jordan canonical form,"Let $A$ be a square matrix. Show that if $$\lim_{n \to \infty} A^{n} = 0$$ then $\rho(A) < 1$ , where $\rho(A)$ denotes the spectral radius of $A$ . Hint: Use the Jordan canonical form. I am self-studying and have been working through a few linear algebra exercises. I'm struggling a bit in applying the hint to this problem — I don't know where to start. Any help appreciated.","['matrices', 'jordan-normal-form', 'spectral-radius', 'linear-algebra']"
3189384,"Mass and center of mass of lamina: $B={(x,y);x^2+y^2\le1,0\le y}$? I'm close to the answer but I'm missing something.","I'd like some help here because I can't get the right answer. The lamina we are working with is defined by: $B={(x,y);x^2+y^2\le1,0\le y}$ . Also the density function is ""proportional to the distance of the point (x,y) to the x-axis"" I think here is where my mistake lies, for me density function is: $p(x,y)=y$ Acording to the exercise center of mass should be: $(Xc=0,Yc=\frac{3pi}{32})$ So I did this: To find mass M I converted to polar coordinates. $x=rcos(\theta),y=rsin(\theta)$ $M=\int\int_Bp(x,y)dA => M=\int_0^{\pi}\int_0^1rsin(\theta)rdrd\theta=2/3$ . Now with the mass we can find $Xc$ and $Yc$ . It's easy to note that $Xc=0$ with symmetry $Yc = \frac{1}{M}\int\int_Byp(x,y)da = > Yc=\frac{3}{2}\int_0^\pi\int_0^1rsin(\theta)rsin(\theta)rdrd\theta=\frac{3\pi}{16}$ But Yc should be $\frac{3\pi}{32}$ , so I'm close but I'm doing something wrong.",['multivariable-calculus']
3189406,Determinant is linear as a function of each of the rows of the matrix.,Today I heard in a lecture (some video on YouTube) that the determinant is linear as a function of each of the rows of the matrix. I am not able to understand the above statement. I know that determinant is a special function which assign to each $x$ in $\mathbb K^{n \times n}$ a scalar. This is the intuitive idea. And this map is not linear as well. One way to see this is to consider the fact that determinant of $cA$ is $c^n\det(A)$ Can someone please explain what did the person mean by saying that the determinant is linear as a function of each of the rows of matrix?,"['matrices', 'determinant', 'linear-algebra']"
3189411,Minimizing lengths of cevians in an isosceles right triangle,"Consider isosceles right triangle $ABC$ with $BC$ as the hypotenuse and $AB=AC=6$ . $E$ is on $BC$ and $F$ is on $AB$ such that $AE+EF+FC$ is minimized. Compute $EF$ . My thought process: I reflected triangle $ABC$ across $BC$ to get a square $ABA'C$ . Then, I messed around with the placement of $F$ on $AB$ , to see what results I could produce, with $E$ always being the midpoint. May someone help me on this?","['contest-math', 'euclidean-geometry', 'geometry', 'maxima-minima', 'optimization']"
3189419,Generalizing the solution to an ODE,"Is there a way to solve the following ODE for general integral values of $m$ \begin{align}
\frac{\partial A(x)}{ \partial x} = -A(x)^m + \frac1x  \label{rec}\tag{1}
\end{align} I have some ways to approach this problem for a special case of $m=2$ . For this case, if we substitute $A(x) = \frac{u^\prime(x)}{u(x)}$ , we would get a differential equation of the form, $$u^{\prime\prime}(x) = \frac{u(x)}x$$ And, it is possible to write a solution for this equation in terms of Bessel functions. But I don't know how to generalize this for higher $m$ . Any help would be appreciated!","['ordinary-differential-equations', 'bessel-functions']"
3189424,Trying to prove one set in terms of another using identity laws,"Prove: $A\ - (A\cap \ B ) = A - B$ My work thus far $$
\begin{aligned}
&\quad A\ - (A\cap \ B) \\
&= (A-A)\ \cap\ (A-B) \text{(using the distributive law)} \\
&= A-B\ \text{(since A-A is just {} (empty set) and the intersection with (A-B) would be A-B)}
\end{aligned}
$$","['elementary-set-theory', 'discrete-mathematics']"
3189483,Choose 2 good batteries out of 8 (4 bad 4 good) [duplicate],"This question already has answers here : We have $n$ charged and $n$ uncharged batteries and a radio which needs two charged batteries to work. (4 answers) Closed 5 years ago . A pack of 8 batteries is given, with 4 good batteries and 4 bad batteries. We need to take 2 good batteries for our device to work properly. How to calculate what is the smallest number of steps in which we are sure, that we have 2 good batteries?
What is the best approach to take I we want to be left with two good batteries in our hand?
Only way we can check if a pair is good or bad is to put it in the device. It either works (2 good batteries) or doesn’t work (1 good 1 bad or 2 bad) EDIT:
I've managed so far to find a solution, that we can do this with only 7 checks (where a check is putting two batteries to the device). Divide 8 batteries into 4 pairs Check each pair (4 checks total) If none worked, all pairs are of type (0,1) or (1,0) Take any two pairs, cross-check them (we already know the result of one possibility, so we're left with 3 other possibilities to check, i.e. 3 checks, 7 checks total) But I am still wondering, If this can be done with 6 checks only? Can we apply graph theory here somehow?","['permutations', 'optimization', 'combinations', 'combinatorics']"
3189490,Will these geometric means always converge to $1/e$?,"Let $p_n$ be the $n$ -th prime and $F_n$ be the $n$ -th Fibonacci number. We have $$
\lim_{n \to \infty}\frac{(p_1 p_2 \ldots p_n)^{1/n}}{p_n} 
= \lim_{n \to \infty}\frac{\{\log(F_3)\log(F_4)\ldots \log(F_n)\}^{1/n}}{\log(F_n)}
= \frac{1}{e}
$$ The first limit was proved by Sandor and Verroken using the prime number theorem and the Chebyshev function. The second limit was proved by Farhadian and Jakimjuk using the Binet's formula for Fibonacci numbers and Stirling's approximation for factorial. Although these two results were proved using different ingredients, their structure is exactly similar which led me to investigate if there is a stronger phenomenon governing such results. My analysis led me to the following. Claim : If $a_n= n^{1+o(1)}$ is increasing then, $ \lim_{n \to \infty}\dfrac{(a_1 a_2\ldots a_n)^{1/n}}{a_n} = \dfrac{1}{e}.$ I believe that I have a proof for this using Weyl's Equidistribution Theorem. I am looking for a simpler or a more elementary proof of this. Also have I got the conditions right for this claim to hold?","['number-theory', 'analysis', 'real-analysis', 'complex-analysis', 'limits']"
3189492,Singletons in the $\sigma$-algebra generated by clopen sets of a Stone space,"Let $A$ be a Boolean algebra and let $Ult(A)$ be its Stone space, that is, the set of all ultrafilters on $A$ . It is well known that $C=\{\{u\in Ult(A)\!:a\in u\}\!:a\in A\}$ is an algebra of sets isomorphic to $A$ and the elements of $C$ are exactly the clopen sets of the topology on $Ult(A)$ generated by $C$ . Questions: Let $B$ be the $\sigma$ -algebra of sets generated by $C$ . Does $\{\{u\}\!:u\in Ult(A)\}\subseteq B$ necessarily hold true? If not, is there any standard name for the ultrafilters $u$ satisfying $\{u\}\in B$ ? Is there any characterization of Boolean algebras $A$ for which $\{\{u\}\!:u\in Ult(A)\}\subseteq B$ ? My attempt: A candidate counterexample is the Boolean algebra $A=\mathcal{P}(\omega)$ . Then $Ult(A)$ is $\beta\omega$ , the Stone-Čech compactification of the integers. It is known that $\beta\omega$ is not first countable, see here (statement 7A). More precisely, if $u\in\beta\omega\setminus\omega$ then $u$ does not have a countable local base, hence $\{u\}$ is not a countable intersection of clopen sets. However, it is not clear to me if this implies $\{u\}\notin B$ .","['filters', 'boolean-algebra', 'general-topology', 'measure-theory']"
3189517,Find $\limsup _{n\to\infty} \bigl(\frac{2\cdot5\cdot8\cdot\cdots\cdot(3n-4)}{3^nn!}\bigr)^{1/n}$,"Find $\limsup_{n\to\infty}(\frac{2\cdot5\cdot8\cdot\cdots\cdot(3n-4)}{3^nn!})^{1/n}$ I've tried multiplying the nominator and the denominator by what is lacking for there to be $3n!$ in the nominator, but that landed me in an algebraic mess. I've also tried to see if we can extract something like $const^n$ from $2\cdot5\cdot8\cdot\cdots\cdot(3n-4)$ , and nothing came out except for a very thin sight of $2^{n/2}$ (I think) that I don't know how to use. I think we are supposed to use Stirling formula somewhere in the exercise. Thank you.","['limits', 'supremum-and-infimum', 'real-analysis']"
3189535,What is the operational way of discovering scale invariance of differential equations?,"Context The answer here by @Keenan Pepper gives an instance for what it means for an algebraic or trigonometric formula to be scale invariant . For quick reference, I quote his answer here but with a slightly changed notation. He essentially says that The Euclidean distance formula $a^2 + b^2 = c^2$ is scale invariant since $$(\lambda a)^2 + (\lambda b)^2 = \lambda^2(a^2+b^2) = (\lambda c)^2.$$ In other words, the formula retains its form when all the variables are scaled by a constant multiplicative factor $\lambda$ . Question What would it mean for a differential equation to be (not to be) scale invariant? When I say differential equations, I have very simple physics equations in mind such as $$\frac{d^2x}{dt^2}+\omega_0^2x=0,~ (\text{Undamped oscillation})\\
\frac{d^2x}{dt^2}+\gamma\frac{dx}{dt}+\omega_0^2x=0,~(\text{Damped oscillation})\\
\frac{d^2x}{dt^2}+\gamma\frac{dx}{dt}+\omega_0^2x=F_0\cos\omega t~ (\text{Damped, forced oscillation})$$ where $\omega_0,\gamma,\omega,F_0$ are all constants and the variables are $x$ and $t$ . For instance, let us scale $x\to \lambda x$ , and $t\to \lambda t$ so that the LHS becomes $$\frac{d^2(\lambda x)}{d(\lambda t)^2}+\omega_0^2(\lambda x)=\frac{1}{\lambda}\frac{d^2x}{dt^2}+\lambda\omega_0^2x\neq0$$ unless $\lambda=\pm 1$ . So what's the conclusion? The first equation not scale invariant?","['ordinary-differential-equations', 'invariance', 'physics', 'symmetry', 'mathematical-physics']"
3189545,Exponential distribution MLE with lifetime and frequency table,"\begin{array}{c|c}
\hline \text{Lifetime (months)} & \text{Observed frequency} \\ 
\hline 0-2 & 50 \\ 
\hline 2-4 & 35 \\ 
\hline 4-6 & 25 \\
\hline 6-8 & 15 \\
\hline 8-10 & 5 \\
\hline >10 & 10 \\
\hline\end{array} The above is the lifetime and frequency table of $140$ light bulbs. I would like to calculate the MLE of $\lambda $ . I plan to use this formula: $$\lambda = \frac{n}{x_1+x_2 + \ldots + x_n}$$ , where $x_1$ means the lifetime of first bulb but I only know the lifetime of the first bulb is between $0$ to $2$ months so how can I solve this question? Thank you in advance.","['statistics', 'probability-distributions', 'exponential-distribution', 'maximum-likelihood']"
3189621,Find a composite solution to the following problem,"I'm trying to solve the following exercise: Find a composite solution to the following problem: $$
\epsilon y'' + y(y' + 3) = 0 \text{ for }0<x<1, \text{ where }y(0) = 1, \,y(1) = 1
$$ where $\epsilon <<1.$ What I've tried: I've just learned about matched asymptotic expansions and I'm pretty sure that my teacher wants me to use those to solve this exercise. The procedure works as follows: Find the outer solution . This solution is often found by assuming that the solution can be expanded in powers of $\epsilon$ . Because this solution often has only one arbitrary constant it will not be able to satisfy both the boundary value conditions. Find the inner solution . Assume that there exists a boundary layer at one of the boundaries. Introduce a boundary-layer coordinate and use an expansion for the boundary-layer solution . Matching. Since the inner (corresponding to the boundary-layer) and outer expansions are approximations of the same function we expect them to be the same in the region between the inner and outer layers. Composite expansion. Use the inner and outer solution to find a solution that will work over the entire interval. This can be done by adding the two solutions and subtracting corresponding to the region where they are equal. I found the outer solution by assuming that the solution $y$ can expanded in powers of $\epsilon$ as follows: $$
y \sim y_0(x) + \epsilon y_1(x) + \ldots \tag{1}
$$ If we substitute $(1)$ into the problem equation we get $$
\epsilon(y''_0 + \epsilon y''_1 + \ldots) + (y_0 + \epsilon y_1 + \ldots)(y'_0 + y'_1 + \ldots + 3) = 0
$$ We can find $y_0$ by looking at the order one terms: $\mathcal{O}(1):$ $$
y_0(y'_0 + 3) = 0
$$ so that $y_0$ or $y_0 = c_1 - 3x$ . This is the first moment where I don't exactly know how to proceed: I would have expected one solution. To find the inner solution or boundary-layer solution we assume that there is a boundary layer at $x = 0$ and introduce a boundary layer coordinate: $$
\bar{x} = \dfrac{x}{\epsilon^\alpha}
$$ From this change of variables and the chain rule, we have that $$
\dfrac{d}{dx} = \dfrac{d\bar{x}}{dx}\dfrac{d}{d\bar{x}} = \dfrac{1}{\epsilon^\alpha}\dfrac{d}{d\bar{x}}
$$ If we now let $Y(\bar{x})$ denote the solution when using this boundary-layer coordinate, the problem equation transforms to $$
\epsilon^{1 - 2\alpha}\dfrac{d^2}{d\bar{x}^2}(Y_0 + \epsilon Y_1 + \ldots) + (Y_0 + \epsilon Y_1 + \ldots)\dfrac{1}{\epsilon^\alpha}\dfrac{d}{d\bar{x}}(Y_0 + \epsilon Y_1 + \ldots) + 3(Y_0 + \epsilon Y_1 + \ldots)
$$ In order to determine $Y_0$ , I want to balance the terms in this equation. I tried balancing the first and second term so that the third term becomes higher order. To balance the first and second term we need $1 - 2\alpha = -\alpha$ so that $\alpha = 1$ . In this case the first to terms are $\mathcal{O}(\frac{1}{\epsilon})$ and the third term is $\mathcal{O}(1)$ so that the requirement that the third term is higher order is satisfied. If $\alpha = 1$ we find: $\mathcal{O}(\frac{1}{\epsilon})$ : \begin{align}
Y''_0 + Y_0Y'_0 = 0\\
Y_0(0) = 1
\end{align} If I enter this equation in wolfram I get a pretty long solution and I don't think it's supposed to be like that. It feels as if I'm doing something wrong. Question: I am doing this correctly? What should I be doing differently? I don't know how to proceed from here since I can't find a reasonable solution for $Y_0$ .","['power-series', 'approximation', 'ordinary-differential-equations']"
3189765,Weak version of the Banach-Tarski Paradox for Circles,"It is well known that the Banach-Tarski paradox does not translate to circles, i.e. a circle cannot be decomposed into finitely many pieces which can be rearranged to form two circles of equal circumference. However what I'd like to know is can a circle be decomposed into finitely many pieces that can be rearranged to form two circles whose difference in circumference is arbitrarily small? Thank you.","['measure-theory', 'geometry', 'real-analysis']"
3189813,When do a holomorphic function's zeroes occur in conjugate pairs?,"I have the following proof that a holomorphic function's zeroes occur in conjugate pairs when its derivatives evaluated at $0$ lie on a line through $0$ : Let $f:\mathbb{C}\mapsto\mathbb{C}$ be holomorphic, then $$f(z)=\sum\limits_{r=0}^{\infty}\frac{f^{(r)}(0)}{r!}z^r.$$ Writing $z\equiv\cos(\arg(z))+i\sin(\arg(z))$ , and invoking De Moivre's theorem, we get $$\Re(f(z))=\sum\limits_{r=0}^{\infty}\frac{|z|^r}{r!}(\Re(f^{(r)}(0))\cos(r\cdot\arg(z))-\Im(f^{(r)}(0))\sin(r\cdot\arg(z))),$$ $$\Im(f(z))=\sum\limits_{r=0}^{\infty}\frac{|z|^r}{r!}(\Im(f^{(r)}(0))\cos(r\cdot\arg(z))+\Re(f^{(r)}(0))\sin(r\cdot\arg(z))).$$ Suppose $f^{(r)}(0)\in\mathbb{R}$ for all $r\in\mathbb{N}_0$ , then $\Im(f^{(r)}(0))=0$ for all $r$ , so $$\Re(f(z))=\sum\limits_{r=0}^{\infty}\frac{|z|^r}{r!}f^{(r)}(0)\cos(r\cdot\arg(z)),$$ $$\Im(f(z))=\sum\limits_{r=0}^{\infty}\frac{|z|^r}{r!}f^{(r)}(0)\sin(r\cdot\arg(z)).$$ Therefore, $\Re(f(z))$ is even with respect to $\arg(z)$ , and $\Im(f(z))$ is odd with respect to $\arg(z)$ , so $f(\overline{z})=\overline{f(z)}$ for all $z\in\mathbb{C}$ . Therefore, if $f(z)=0$ , $f(\overline{z})=0$ . Now let $f$ be such that its derivatives evaluated at $0$ lie on a line through $0$ such that $\arg(z)=\theta$ for all non-zero $f^{(r)}(0)$ , then $g(z)=e^{-i\theta}f(z)$ is such that $g^{(r)}(0)\in\mathbb{R}$ for all $r$ since $g^{(r)}(0)=e^{-i\theta}f^{(r)}(0)$ . Therefore, the zeroes of $g$ occur in conjugate pairs, and $f$ is simply a rotation of $g$ about $0$ , so the zeroes of $f$ occur in conjugate pairs. My question is, are there known results about curves $\gamma$ in $\mathbb{C}$ other than lines through $0$ such that any holomorphic $f$ such that $f^{(r)}(0)\in\gamma$ for all $r\in\mathbb{N}_0$ is such that, if $f(z)=0$ , $f(\overline{z})=0$ . I've looked around online and all I can find is the conjugate pair theorem, which is a special case of what I have already proven. A method I have tried of extending the class of curves that have this property is to note that, given some curve $\gamma$ and any sequence of points $(a_n)_{n\in\mathbb{N}}$ such that $a_n\in\gamma$ for all $n\in\mathbb{N}$ , if there exists some holomorphic $g$ such that $g^{(r)}(0)\in\mathbb{R}$ for all $r$ and some holomorphic $T$ such that $T(0)=0$ and $T(z)\neq 0$ for all non-zero $z\in\mathbb{C}$ , such that $(T\circ g)^{(r)}(0)=a_r$ for all $r$ , then any function whose derivatives evaluated at $0$ lie on $\gamma$ is such that it zeroes occur in conjugate pairs. I won't write out the whole proof, but I then used the combinatorial form of Faa De Bruno's formula to show that one has sufficient freedom in the choosing of the derivatives of $T$ and $g$ to conclude that there exists some $T$ and $g$ such that $(T\circ g)^{(r)}(0)=a_r$ for all $r\in\mathbb{N}$ , and, moreover, that there is even more freedom in the choosing than that; I have come close to proving that you can get $T(0)=0$ , but not that $T(z)\neq 0$ for all non-zero $z$ (in fact, this is definitely not true for all $\gamma$ , and I don't see how I will be able to distinguish between those curves for which it is true and those for which it isn't, even if I do make progress on determining what curves one has ""more freedom"" for), and was wondering if there are known results related to this.",['complex-analysis']
3189817,Find $\angle BDA$,A circle with radius $AB$ and center at $A$ is constructed. $D$ is on the circle $CD$ is the angle bisector of $\angle BCA$ . $E$ is on the circle so that $DE$ is the angle bisector of $\angle BDA$ . Find $\angle BDA$ if $BC \cong CE$ . My work I have done so far is $$a = \angle ADE$$ $$EA \cong AD$$ $$\angle AED \cong \angle ADE$$ $$\angle AED = a$$ $$\angle BDE = a$$ $$\frown BDE = 2a$$ $$\angle EAB = 2a$$ and I'm stuck at this step and I can't continue.,"['euclidean-geometry', 'geometry']"
3189823,Determining the power of permutation matrix of order $N\times N$ to get identity matrix.,"This particular 6x6 permutation matrix is P $$ P = \begin{pmatrix} 
           0 &  0 &  0 &  0 &  0 &  1 \\
           0 &  0 &  1 &  0 &  0 &  0 \\
           1 &  0 &  0 &  0 &  0 &  0 \\
           0 &  0 &  0 &  0 &  1 &  0 \\
           0 &  0 &  0 &  1 &  0 &  0 \\
           0 &  1 &  0 &  0 &  0 &  0 \\
\end{pmatrix}$$ the least power of $P$ that gives identity is $8$ . However, lets consider $P_2$ $$
P_2 = \begin{pmatrix}
         0 &  0 &  0 &  1 &  0 &  0 \\
         0 &  0 &  1 &  0 &  0 &  0 \\
         1 &  0 &  0 &  0 &  0 &  0 \\
         0 &  0 &  0 &  0 &  1 &  0 \\
         0 &  0 &  0 &  0 &  0 &  1 \\
         0 &  1 &  0 &  0 &  0 &  0 
\end{pmatrix}
$$ the least power of $P_2$ that gives identity is $6$ . How to identify the least power of a permutation matrix for which it turns into identity. 
2.If a number is given, say $n$ , how to construct a permutation matrix such that the minimum power the matrix has to be raised to get identity is that particular number $n$ ?","['matrices', 'permutations']"
3189857,Long anti-arithmetic permutation,"A permutation is a sequence $(a_1, \ldots, a_n)$ in which each number $1, \ldots, n$ appears precisely once. We call a sequence anti-arithmetic if there are no non-trivial arithmetic subsequences in it; that is, if there are no $i < j < k$ such that $(a_i, a_j, a_k)$ is an arithmetic sequence. An example of an anti-arithmetic sequence of length 6 is $$
(3, 5, 4, 6, 1, 2).
$$ Intuitively it seems ""hard"" to me for a long sequence to be anti-arithmetic. For example, suppose that you have built about half the sequence so far; then adding any element $a$ near $n/2$ requires that the elements of $1, \ldots, n$ you have used so far mirror (almost) exactly around $a$ , and there are lots of ways for this to go wrong. In particular, I do not know how to create anti-arithmetic sequences of arbitrary length. Are there anti-arithmetic sequences of arbitrarily high length? How can I construct those? I am asking this question because of Kattis problem Antiarithmetic? ; googling the word ""antiarithmetic"" gets me only references to this recreational programming problem. I am not looking for a solution to the problem, only for some more intuition about antiarithmetic sequences. Edit: Some programming strongly suggests that such anti-arithmetic sequences continue to appear for higher $n$ . The following (unoptimized, but a fair bit faster than brute force) script shows anti-arithmetic sequences of length up to 40 very quickly, and that there are about 74904 such sequences of length 15. def extend_aas(length, partial_sequence=[]):
    results = []
    for i in range(length):
        if i in partial_sequence:
            continue
        for j in partial_sequence:
            if 0 <= i + i - j < length and (i + i - j not in partial_sequence):
                break
        else:
            yield partial_sequence + [i]

def get_aases(length, partial=[]):
    for extended in extend_aas(length, partial):
        if len(extended) == length:
            yield extended
        else:
            for result in get_aases(length, extended):
                yield result

for n in range(1, 41):
    print(n, next(get_aases(n)))

for n in range(1, 16):
    print(n, len(list(get_aases(n)))) However, this still does not give me intuition for why this might be the case.","['recreational-mathematics', 'combinatorics']"
3189888,Difference between Bernoulli random variables,"Given are $n$ independent Bernoulli random variables with parameters $p_1,\dots,p_n$ . We want to split them into two parts so as to minimize the expectation $\mathbb{E}[|X-Y|]$ , where $X$ is the sum of the first part and $Y$ the sum of the second part. What is the best way to split so that this expectation is minimized? A reasonable guess is that it is always to make the sums of the parameters $p_i$ 's in the two parts as close as possible. Are there examples where this is not optimal?","['independence', 'probability-theory', 'probability']"
3189914,Addition of continuous functions over topological spaces is continuous.,"Just wanted to verify if the following proof works:
Suppose $f:X\rightarrow \mathbb{R}$ $g:X\rightarrow \mathbb{R}$ are continuous. Want to show that: $h=f+g$ is continuous. We are going to prove the statement using the localised definition of continuity: https://www.emathzone.com/tutorials/general-topology/continuity-in-topological-spaces.html Let $x\in X$ and $U$ is open in $\mathbb{R}$ s.t $h(x)\in U$ . Hence, $\exists \epsilon>0$ s.t $(h(x)-\epsilon,h(x)+\epsilon)\subset U$ . Since $f$ and $g$ are continuous at $x$ and $$f(x)\in B(f(x),\epsilon/2)$$ and $$g(x)\in B(g(x),\epsilon/2)$$ which are open sets in $\mathbb{R}$ , this implies the $\exists V_1, V_2$ open in $X$ s.t $x\in V_1\cap V_2$ and $$f(V_1)\subset B(f(x),\epsilon/2) $$ and $$g(V_2)\subset B(g(x),\epsilon/2)$$ Hence, $\forall z\in V_1\cap V_2$ $$|h(z)-h(x)|\leq|f(z)-f(x)|+|g(x)-g(z)|< \epsilon$$ i.e $h(V_1\cap V_2)\subset(h(x)-\epsilon,h(x)+\epsilon)\subset U$ . Since $V_1\cap V_2$ is open in $X$ and $x\in V_1\cap V_2$ , this implies $h$ is continuous at $x$ .","['continuity', 'general-topology', 'proof-verification']"
3189924,Correspondence between a countably infinite set A and the set of positive integers,"I'm currently taking a course on logic & computability and they're using as a manual the famous ""Logic and computability"" by Boolos, Burgess and Jeffrey. The last week I've been trying to solve this problem presented in chapter 1 but I'm having a really hard time trying to understand the question. The problem is: 1.4 A set A has n elements, where n is a positive integer, if it is equinumerous with the set of positive integers up to n, so that its
  elements can be listed as a1, a2, ... , an. A nonempty set A is finite
  if it has n elements for some positive integer n. Show that any
  enumerable set is either finite or equinumerous with the set of all
  positive integers. (In other words, given an enumeration, which is to
  say a function from the set of positive integers onto a set A, show
  that if A is not finite, then there is a correspondence, which is to
  say a one-to-one, total function, from the set of positive integers
  onto A.) So, my understanding is that I have to prove $$
p \lor q
$$ I was thinking that, as a first step, I could suppose that $$
\lnot p\to q
$$ That is, assume that A is not finite and show that a correspondence between the set of positive integers and the set A follows. Now, I know that if A is an enumerable set, then there exists a function $$
f:\mathbb Z^+ \to A
$$ Such that that function is surjective. What I should prove now is that there exists a function: $$
g:\mathbb Z^+ \to A
$$ Such that that function is one-to-one and total. But I'm not sure how to proceed, at first I thought that I could workaround the inverse function, but that's a no-go. I'm unable to see how to construct an injective, total function from the set of positive integers to A and show the correspondence. I think my misunderstanding is that I don't understand how the infinity of the set A is playing a role in this proof. Could you please provide me with some keys in order to construct this proof?","['elementary-set-theory', 'logic', 'computability']"
3189979,Using Hotelling's T-statistic to find an elliptic confidence set,"The problem: We have samples of sizes ${n_1} = 25,{n_2} = 15,{n_3} = 30$ drawn independently from $N\left( {{\mu _i},{\sigma ^2}} \right),i = 1,2,3$ (normal distributions with same variance). We have ${\overline x _1} = 10.5,{\overline x _2} = 14,{\overline x _1} = 12,s_1^2 = 2.5,s_2^2 = 3,s_3^2 = 2.7$ (unbiased variance estimators). Find 95% elliptic confidence set for $\left( {{z_1},{z_2}} \right) = \left( {{\mu _1} + {\mu _2} - 2{\mu _3},{\mu _1} - {\mu _2}} \right)$ by using Hotelling's distribution. My attempt: We have $$\left( {{X_{1i}},{X_{2j}},{X_{3k}}} \right)\sim N\left( {\left( {\begin{array}{*{20}{c}}
  {{\mu _1}} \\ 
  {{\mu _2}} \\ 
  {{\mu _3}} 
\end{array}} \right),\left[ {\begin{array}{*{20}{c}}
  {{\sigma ^2}}&0&0 \\ 
  0&{{\sigma ^2}}&0 \\ 
  0&0&{{\sigma ^2}} 
\end{array}} \right]} \right) \Rightarrow \left( {{{\overline X }_1},{{\overline X }_2},{{\overline X }_3}} \right)\sim N\left( {\underbrace {\left( {\begin{array}{*{20}{c}}
  {{\mu _1}} \\ 
  {{\mu _2}} \\ 
  {{\mu _3}} 
\end{array}} \right)}_\mu ,\underbrace {\left[ {\begin{array}{*{20}{c}}
  {{\sigma ^2}/{n_1}}&0&0 \\ 
  0&{{\sigma ^2}/{n_2}}&0 \\ 
  0&0&{{\sigma ^2}/{n_3}} 
\end{array}} \right]}_\Sigma } \right)$$ So $$\left( {{Z_1},{Z_2}} \right) = \left( {{{\overline X }_1} + {{\overline X }_2} - 2{{\overline X }_3},{{\overline X }_1} - {{\overline X }_2}} \right) = \underbrace {\left( {\begin{array}{*{20}{c}}
  1&1&{ - 2} \\ 
  1&{ - 1}&0 
\end{array}} \right)}_B\left( {\begin{array}{*{20}{c}}
  {{{\overline X }_1}} \\ 
  {{{\overline X }_2}} \\ 
  {{{\overline X }_3}} 
\end{array}} \right)\sim N\left( {B\mu ,B\Sigma B'} \right)$$ and $$\left( {{Z_1},{Z_2}} \right)\sim N\left( {\left( {\begin{array}{*{20}{c}}
  {{\mu _1} + {\mu _2} - 2{\mu _3}} \\ 
  {{\mu _1} - {\mu _2}} 
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
  {{\sigma ^2}\left( {\frac{1}{{{n_1}}} + \frac{1}{{{n_2}}} + \frac{4}{{{n_3}}}} \right)}&{{\sigma ^2}\left( {\frac{1}{{{n_1}}} - \frac{1}{{{n_2}}}} \right)} \\ 
  {{\sigma ^2}\left( {\frac{1}{{{n_1}}} - \frac{1}{{{n_2}}}} \right)}&{{\sigma ^2}\left( {\frac{1}{{{n_1}}} + \frac{1}{{{n_2}}}} \right)} 
\end{array}} \right)} \right)$$ The only estimator for ${{\sigma ^2}}$ that comes to mind is the pooled variance ${\widehat \sigma ^2} = s_p^2 = \frac{{\sum\limits_{i = 1}^3 {\left( {{n_i} - 1} \right)s_i^2} }}{{\sum\limits_{i = 1}^3 {\left( {{n_i} - 1} \right)} }}$ , but I don't see how to get from there to Hotelling's distribution. I'm assuming that $S = s_p^2\left( {\begin{array}{*{20}{c}}
  {\frac{1}{{{n_1}}} + \frac{1}{{{n_2}}} + \frac{4}{{{n_3}}}}&{\frac{1}{{{n_1}}} - \frac{1}{{{n_2}}}} \\ 
  {\frac{1}{{{n_1}}} - \frac{1}{{{n_2}}}}&{\frac{1}{{{n_1}}} + \frac{1}{{{n_2}}}} 
\end{array}} \right)$ does not follow a Wishart distribution. How would I find the required elliptic confidence set for the linear combination of expectations with unknown (and common) variance from those independent normally distributed samples? EDIT: We also know that $\sum\limits_{i = 1}^3 {\left( {{n_i} - 1} \right)\frac{{S_i^2}}{{{\sigma ^2}}}} \sim {\chi ^2}\left( {\sum\limits_{i = 1}^3 {\left( {{n_i} - 1} \right)} } \right)$ , which further makes me suspect that Hotelling's distribution will not play a role in the solution. Still, I don't see which test statistic to employ (I'm guessing ${\left( {\left( {{z_1},{z_2}} \right) - B\mu } \right)^\prime }{S^{ - 1}}\left( {\left( {{z_1},{z_2}} \right) - B\mu } \right)$ ) and which distribution would it follow. EDIT2: Also see here","['statistics', 'confidence-interval', 'normal-distribution']"
