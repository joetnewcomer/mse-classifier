question_id,title,body,tags
3811870,How do I prove injective property of $(x + y)^2 + y: \mathbb{N}×\mathbb{N} \to \mathbb{N}$,"Given this function: $(x + y)^2 + y$ , how do I go about proving it's injective property of mapping $\mathbb{N}×\mathbb{N} \to \mathbb{N}$ ? Surjection is not required. My current attempts include proving by negation: assume $(x_1,y_1) \ne (x_2,y_2)$ yet $(x_1 + y_1)^2 + y_1 = (x_2 + y_2)^2 + y_2$ , then attempt to arrive at a contradiction. I wasn't able to find a technique that would help me reach that goal. Geometrically, I can think of the square value to be a growing line but must have a length of certain values (square values). The addition of $y$ must not overwhelm the line to the next ""border"" of square values. Thus no other value of $y$ would provide the same total length. While $x$ is bound to stretch the line between square values only. My math jargon isn't refined, but that it is how I think of this question.","['elementary-set-theory', 'functions', 'natural-numbers']"
3811883,Reciprocal of a Liouville number is also a Liouville number,"Prove that the reciprocal of a Liouville number is also a Liouville number I am using the definition of a Liouville number given in the book Transcendental Numbers by M. Ram Murty . A screenshot of the definition is attached Proof: Let, if possible, $\frac{1}{\beta}$ is not a Liouville number where $\beta$ is a Liouville number . Then, $\exists$ a non-negative real number $v_0$ such that for any non-zero rational number $\frac{p}{q}$ such that $\gcd(p,q)=1$ and $q\neq0$ , we have, \begin{gather*}
\left|\frac{1}{\beta}-\frac{p}{q}\right|\geq\frac{1}{|q|^{v_0}}\tag{1}
\end{gather*} Since $\beta$ is a \textit{Liouville number}, for any $v\in\mathbb{R}_{\geq0}$ , there exists a rational $\frac{p_v}{q_v}$ such that $\gcd(p_v,q_v)=1$ , $q_v>0$ and, \begin{gather*}
    0<\left|\beta-\frac{p_v}{q_v}\right|<\frac{1}{q_v^v}\tag{2}
\end{gather*} For any positive integer $m$ , there are only finite number of rationals in reduced form with denominator $m$ which satisfy the Liouville's condition. Therefore, it's clear that $q_v\rightarrow\infty$ as $v\rightarrow\infty$ . Also, since $|\beta|$ is a finite positive quantity, all but at most a finite number of $\frac{p_v}{q_v}=0$ . Therefore we can extract a strictly increasing sequence of non-negative reals $\{v_n\}_{n\geq1}$ such that $\frac{p_{v_n}}{q_{v_n}}\neq0$ . Therefore, combining $(1)$ and $(2)$ we get, for any $n\geq1$ the follwing facts, \begin{gather*}
0<\left|\beta-\frac{p_{v_n}}{q_{v_n}}\right|<\frac{1}{q_{v_n}^{v_n}}\\
\implies \left|\frac{q_{v_n}}{\beta p_{v_n}}\right|\frac{1}{q_{v_n}^{v_n}}>\left|\frac{1}{\beta}-\frac{q_{v_n}}{p_{v_n}}\right|\geq\frac{1}{\left|p_{v_n}\right|^{v_0}}\\
\implies\left|\frac{q_{v_n}}{\beta p_{v_n}}\right|\frac{\left|q_{v_n}\right|^{v_0}}{q_{v_n}^{v_n}}\geq1\tag{3}
\end{gather*} Since \begin{gather*}
0<\left|\beta-\frac{p_{v_n}}{q_{v_n}}\right|<\frac{1}{q_{v_n}^{v_n}}
\end{gather*} and \begin{gather*}
\lim_{n\rightarrow\infty}\frac{1}{q_{v_n}^{v_n}}=0
\end{gather*} we have, \begin{gather*}
    \lim_{n\rightarrow\infty}\left|\frac{p_{v_n}}{q_{v_n}}\right|=|\beta|
\end{gather*} Therefore we have, \begin{gather*}
\lim_{n\rightarrow\infty}\left|\frac{q_{v_n}}{\beta p_{v_n}}\right|\frac{\left|q_{v_n}\right|^{v_0}}{q_{v_n}^{v_n}}=\lim_{n\rightarrow\infty}\left|\frac{q_{v_n}}{\beta p_{v_n}}\right|\left|\frac{q_{v_n}^{v_0}}{q_{v_n}^{v_0}}\right|\frac{1}{q_{v_n}^{v_n-v_0}}\\
=\left|\frac{q_{v_n}}{\beta p_{v_n}}\right|\lim_{n\rightarrow\infty}\left|\frac{p_{v_n}}{q_{v_n}}\right|^{v_0}\cdot\lim_{n\rightarrow\infty}\frac{q_{v_n}^{v_0}}{q_{v_n}^{v_n}}=\left|\frac{q_{v_n}}{\beta p_{v_n}}\right|\cdot|\beta|^{v_0}\cdot0=0
\end{gather*} This contradicts inequality $(3)$ . Hence we conclude that $\frac{1}{\beta}$ is also a Liouville number . Can someone verify this solution? Please point out if there is any flaw in the proof and then I will try to fix that. Any help will be appreciated. Thanks in anticipation!","['diophantine-approximation', 'number-theory', 'liouville-numbers', 'solution-verification', 'transcendental-numbers']"
3811916,Bijective Mapping Between $\mathbb{R}$ and $\mathbb{R}\setminus\{0\}$,"I am looking to show that these two sets have the same cardinality. I know that since $\mathbb{R}$ is infinite, $\mathbb{R}\setminus\{0\}$ must also be infinite as we have just taken a finite number of elements out (really just the one). Nevertheless, I want to formalize this by finding a bijective mapping between the two and just can't seem to figure out what it would be. Are there any simple examples?","['elementary-set-theory', 'real-analysis']"
3811986,"Show that $\lim_{(x, y) \to (0, 0)} \frac{y + \sin x}{x + \sin y}$ does not exist.","Show that the limit $$\lim_{(x, y) \to (0, 0)} \dfrac{y + \sin x}{x + \sin y}$$ does not exist. I tried using two-path test but all of them gave the same value $1$ . I tried using paths $y = 0, y = kx, y = \sin x$ but all of them give limit $1$ . Since this is the only method taught as of now, I would like to know how I can use two-path test to show this.","['limits', 'multivariable-calculus']"
3811991,Absolutely convergent doubles sums in Banach spaces,"Let $X$ a Banach space and $x_{ij} : \mathbb{N} \times \mathbb{N} \to X$ . Suppose $\sum_i \sum_j \|x_{ij}\| < \infty$ . Then it is easy to show that the following series are convergent: $$\sum_j x_{ij}, \sum_i x_{ij}, \sum_i \sum_j x_{ij}, \sum_j \sum_i x_{ij}.$$ Then I want to know if: $$\sum_i \sum_j x_{ij} = \sum_j \sum_i x_{ij}.$$ This would be a direct corollary of a Fubini-Tonelli type theorem for Bochner integrals, but I can't find a reference on that. So if someone could point me to a reference to such a theorem for Bochner integrals, that'd be ideal (or let me know if/why such a theorem doesn't exist). But also a direct proof of the above would work.","['banach-spaces', 'lebesgue-integral', 'functional-analysis', 'real-analysis']"
3811995,Cauchy's MVT-Lagrange's MVT-Rolle's theorem independence,"In many textbooks, the former two have been proved with the help of Rolle's theorem. However my teacher(and many sites as well) say that Rolle's theorem is a special case of LMVT and Cauchy's is a generalization. Can we prove Cauchy's MVT and LMVT without using Rolle's theorem? If not, should we admit that these are just applications of Rolle's theorem and hence yield no extra result? $\mathcal{Remark}$ I found an analogy which could be useful: Suppose a car is travelling at an average speed of 40 miles/hr. In the course of the travel, it has to, at some point, travel at exactly 40 miles an hour. This is exactly what LMVT has to say.","['calculus', 'derivatives']"
3812087,Why do we define compactness the way we do?,"Let $(M,d)$ be a metric space. A set $A \subset M$ is said to be compact if every open cover of $A$ has a finite subcover. Why do we use this definition, rather than the other ""definition"" which holds in $\mathbb{R}^n$ , that is, a set is compact if it is closed and bounded? It is a more intuitive definition, and it is hard for me to think of compact sets being separate from ""merely"" closed and bounded sets (probably because I can only imagine Euclidian spaces). Is it simply because being closed and bounded (together) is not a topological property?","['metric-spaces', 'definition', 'general-topology', 'soft-question', 'compactness']"
3812107,Polynomial matrix equality and controllability,"Let $A\in\mathbb{R}^{n \times n}$ , $B\in\mathbb{R}^{n\times m}$ and $I$ be the $n\times n$ identity matrix. Show that for any polynomial $n$ -vector $X_0(s)$ with elements of degree $n-1$ or less, we can always find polynomial vectors $X(s)$ and $U(s)$ such that $(sI-A)X(s)-BU(s)=X_0(s)$ , if and only if $$\text{rank}[sI-A\quad B]=n\quad\forall s\in\mathbb{C}$$ i.e., $\{A,B\}$ is controllable. Thanks for any hint.","['matrix-rank', 'control-theory', 'matrices', 'linear-algebra', 'polynomials']"
3812211,Why does having $X_0 = 1$ mean that the hyperplane includes the origin?,"I was just reading this question on stats.stackexchange, because I had the same question about why having $X_0 = 1$ means that the hyperplane includes the origin, and why it is an affine set cutting the $Y$ -axis at the point $(0, \hat{\beta}_0$ ) if the constant is not included in $X$ . However, I don't think the answer actually explains this; rather, it seems like it just restates it in a verbose way. And judging by the mathematics involved, I think that it would be a more appropriate question for the minds at math.stackexchange. So I am looking for a clear explanation of why this is the case; that is, why does having $X_0 = 1$ mean that the hyperplane includes the origin, and why it is an affine set cutting the $Y$ -axis at the point $(0, \hat{\beta}_0$ ) if the constant is not included in $X$ ? The textbook section is 2.3.1 Linear Models and Least Squares from here . The relevant parts are all at the beginning of section 2.3.1. EDIT: The part that I'm interested in is Often it is convenient to include the constant variable $1$ in $X$ , ... and In the $(p + 1)$ -dimensional input-output space, $(x, \hat{Y})$ represents a hyperplane. If the constant is included in $X$ , then the hyperplane includes the origin and is a subspace; if not, it is an affine set cutting the $Y$ -axis at the point $(0, \hat{\beta}_0)$ .","['linear-algebra', 'geometry', 'affine-geometry']"
3812260,Dynamical system and level curves [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Is it possible to design a function $f(x, z) = 0$ from a dynamical system $\dot{x} = a(x)$ such that the trace of trajectories are the level curves of $f$ , i.e. $f(x, c) = 0$ , $c \in \mathbb{R}$ ?","['multivariable-calculus', 'dynamical-systems', 'ordinary-differential-equations', 'differential-geometry']"
3812300,Weird looking ODE solution: How to verify this is indeed a solution?,"I will try to express my question using an example. Consider this homogeneous ODE: $y' = \frac{y-x}{x+y }$ Its solution is: $\boxed{\frac12 \log\left( \frac{y^2(x)}{x^2} +1\right) - \log(x) + \arctan\left( \frac{y(x)}{x}\right) = c} \quad  c\in \mathbb{R}$ As far as I am understanding solving a differential equation means finding a $y(x)$ that satisfies the differential equation. But this particular solution, cannot be expressed in terms of $y(x)$ (at least I can't) . If the previous argument is true how could one verify the solution, given the fact that there is no actual $y$ to plug into the ODE? If the previous argument is false, how could the solution be expressed in terms of $y$ ? P.S: A previous question I've asked is How do $\arctan$ and $\ln$ relate? . Given the fact that the solution only contains those two functions, I suspect that it may be helpful to note this down. Could there be some complex analysis involved?","['soft-question', 'ordinary-differential-equations']"
3812315,What is the universal property of the prime spectrum of a commutative rig?,"Let $A$ be a commutative rig, i.e. a commutative monoid equipped with a unital associative commutative bilinear multiplication and let $L$ be a distributive lattice. For the purposes of this question, say an oplax morphism $\lambda : A \to L$ is a map with the following properties: $\lambda (0)$ is the bottom element of $L$ and $\lambda (a + b) \le \lambda (a) \vee \lambda (b)$ . $\lambda (1)$ is the top element of $L$ and $\lambda (a b) = \lambda (a) \wedge \lambda (b)$ . The dual concept was defined by Joyal in [1975, Les théorèmes de Chevalley-Tarski et remarques sur l'algèbre constructive ] under the name notion of zeros .
Wraith [1979, Generic Galois theory of local rings ] calls an oplax morphisms (as above) a support notion . Notice that if $A$ itself is a distributive lattice considered as a rig (i.e. $\vee$ as addition and $\wedge$ as multiplication), then oplax morphisms $A \to L$ are the same as lattice homomorphisms $A \to L$ . This is essentially because multiplicative maps of lattices are automatically monotone, and monotone maps of join semilattices are automatically lax additive. Let $A_D$ be the initial distributive lattice equipped with an oplax morphism $D : A \to A_D$ , and say a finite set $\{ b_1, \ldots, b_n \} \subseteq A$ covers an element $a \in A$ if $D (a) \le D (b_1) \vee \cdots \vee D (b_n)$ . Proposition. $\{ b_1, \ldots, b_n \}$ covers $a$ if and only if  there exist $c_1, \ldots, c_m$ in $A$ and a positive integer $m$ such that $b_1 c_1 + \cdots + b_n c_n = a^m$ . (The ""if"" direction is an easy manipulation of the relations defining $A_D$ , but the ""only if"" direction seems to require an explicit construction of $A_D$ .) Once we have the above proposition, it is straightforward to check that $A_D$ is isomorphic to the lattice of quasicompact open subsets of the usual prime spectrum of $A$ when $A$ is a ring, with $D (a)$ identified with the standard open subset $\{ \mathfrak{p} \in \operatorname{Spec} A : a \notin \mathfrak{p} \}$ . On the other hand, if $A$ is a distributive lattice, then $D : A \to A_D$ is an isomorphism. This suggests: Question 1. Is there some reasonable (say, locally finitely presentable) concrete category $\mathcal{C}$ with the following properties? The category of commutative rigs is a (non-full) subcategory of $\mathcal{C}$ . The morphisms in $\mathcal{C}$ from a commutative rig to a distributive lattice are the oplax morphisms. The morphisms in $\mathcal{C}$ between distributive lattices are lattice homomorphisms. The (full!) subcategory of distributive lattices is a reflective subcategory in $\mathcal{C}$ . Put another way, I am wondering what an oplax morphism between commutative rigs should be. Secondly: Question 2. Is there some principled, a priori reason why we should be considering oplax morphisms in the first place? A post hoc reason for considering oplax morphisms instead of rig homomorphisms is simply that rig homomorphisms give the ""wrong"" answer for rings. Indeed, the category of commutative rigs and rig homomorphisms already has the category of distributive lattices as a reflective full subcategory, but the reflector sends every ring of positive characteristic to the trivial lattice. It is also unclear to me why we ""only"" need to relax additivity – why not also relax multiplicativity to, say, $\lambda (a) \wedge \lambda (b) \le \lambda (a b)$ ? It would be nice if this turns out to be the decategorification of some reasonable notion of functor between rig categories.","['category-theory', 'algebraic-geometry', 'lattice-orders', 'locales', 'commutative-algebra']"
3812343,Cramér–Rao bound on estimating the parameters of an impulse,"Given a noisy discrete-time complex signal that is the sum of an impulse at some time, $t_0$ , (with amplitude, $a_0 e^{i \phi_0}$ ) and additive white Gaussian noise, what is the Cramér–Rao lower bound on the variance of an unbiased estimator of $t_0, a_0, \phi_0$ ? If I have a discrete-time signal of $N$ samples (let $N$ be even for simplicity), $z_n$ , as described above, if you took the discrete Fourier transform, you would get: $$ Z_n = a_0 \exp\left(\frac{-2\pi i t_0 n}{N} + i\phi_0\right) + \mathcal{CN}(0, 2 N \sigma^2)$$ where $t_0$ is the time of the impulse in the time-domain (and the parameter to be estimated), $a_0$ is some complex amplitude of this impulse, $n = -\frac{N}{2}, ... \frac{N}{2} - 1$ , and $i$ is the imaginary unit. Here I have assumed a sampling frequency of $1$ without loss of generality. The additive complex Gaussian noise, $\mathcal{CN}(0, 2 N \sigma^2)$ , is a complex random variable where both the real and imaginary parts follow a $\mathcal{N}(0, N \sigma^2)$ distribution each. The factor of $N$ in the variance of the additive noise accounts for the normalization factor in the inverse Discrete Fourier Transform, ensuring a constant noise variance in the time-domain. $a_0 > 0, t_0 \in [0, N], \phi_0 \in [-\pi, \pi)$ are real parameters that describe the impulse in the time-domain. Intuitively, it seems to me that if we take the discrete-time Fourier transform $$ f(t) = \frac{1}{N} \sum_{n} Z_n \exp\left(\frac{2 \pi i t n}{N}\right) $$ then an unbiased estimator of $t_0$ is $$\hat{t} = \underset{t}{\operatorname{argmax}} \left|f(t) \right| $$ and $a_0$ and $\phi_0$ can also be similarly estimated via $f(\hat{t}) = \hat{a} e^{i \hat{\phi}}$ . I have a hunch that this should be a maximum-likelihood estimator and should achieve the Cramér–Rao lower bound. To determine the Cramér–Rao lower bounds, we need to derive the likelihood function. Let, $$ p_n = a \cos\left(-2\pi t \frac{n}{N} + \phi\right) \\
q_n = a \sin\left(-2\pi t \frac{n}{N} + \phi\right)$$ With $Z_n = X_n + i Y_n$ , we have $$X_n = a_0 \cos\left(-2\pi t_0 \frac{n}{N} + \phi_0\right) + \mathcal{N}(0, N\sigma^2) \\
Y_n = a_0 \sin\left(-2\pi t_0 \frac{n}{N} + \phi_0\right) + \mathcal{N}(0, N\sigma^2)$$ Then, the likelihood function is $$ \mathcal{L}(\boldsymbol{Z}) = \left(\frac{1}{2 \pi N \sigma^2}\right)^N \exp\left[-\frac{1}{2 N \sigma^2} \sum_n\left((X_n - p_n)^2 + (Y_n - q_n)^2\right)\right]$$ Now, I must derive a $3 \times 3$ Fisher Information matrix for three unknown parameters, $a_0, t_0, \phi_0$ , using this likelihood function and invert it to get the lower bound on the variance of an unbiased estimator for the impulse's parameters. This is where I am stuck. I have no idea how to proceed in this case.","['statistics', 'log-likelihood', 'estimation', 'fisher-information', 'signal-processing']"
3812369,Uneven Sine Curve,"I am not a mathematician, so apologies for my lack of knowledge/terminology. For the day/night cycle in a video game, I'd like to use a sine wave to determine the lighting for the scene. (Where $X$ is the number of seconds elapsed) For this graph, I'd like the maximum point to be $1$ and the minimum point to be $0$ . I'd like the full day/night to take $15$ minutes, or $900$ seconds. (So I believe that would be a ""period"" of $900$ , though I'm not sure) But this is where things get too complex for me. I'd like the night's duration to be half the day. In other words, the duration of the rise from $0.5$ to $1$ be twice the duration of the dip from $0.5$ to $0$ . Any help with this problem would be super appreciated by me and my team! EDIT: I would like it to start at 1","['trigonometry', 'calculus', 'algebra-precalculus']"
3812401,"Local strictly henselian $\mathbb{Q}$-algebras (i.e. ""points in étale topology"")","In the étale topology, we have an equivalence of categories between the category of fiber functors on the (small) étale site $Ét(\text{Spec}(S))$ and the category of local strictly henselian $S$ -algebras (see for instance the wonderful paper https://arxiv.org/pdf/1407.5782.pdf ). The relevance of the fiber functors is that it is sensible to think of them as ""Points"" in the étale topology. However, I admit that I don't really have a feel for the local strictly henselian $S$ -algebras for any given ring $S$ and would like to see some more examples. For instance, if $S=\mathbb{Q}$ , then for each $p$ , the algebra $\mathbb{Q}_p^{un}$ is strictly henselian. Are these all the local strictly henselian $\mathbb{Q}$ -algebras?","['etale-cohomology', 'hensels-lemma', 'algebraic-geometry', 'topos-theory', 'commutative-algebra']"
3812435,Reality check on Schwarzschild computations,"TL;DR: I have something Ricci-flat which is not turning out to be scalar-flat and this is absurd. Consider $P_I\times_r \Bbb S^2$ , where $P_I  =\{(t,r) \in \Bbb R^2 \mid r>2m\}$ , with $m>0$ , and the Lorentzian metric $g^{P_I} =-h(r)\,{\rm d}t^2+h(r)^{-1}\,{\rm d}r^2$ , where $h(r) = 1-2mr^{-1}$ . The metric on $P_I\times_r \Bbb S^2$ is $$g=-h(r)\,{\rm d}t^2+h(r)^{-1}\,{\rm d}r^2 + r^2\,{\rm d}\Omega^2,$$ where ${\rm d}\Omega^2$ is the standard round metric on $\Bbb S^2$ . I have computed that the Gaussian curvature of the half-plane $P_I$ is given by $K^{P_I}(t,r) = -h''(r)/2 = 2mr^{-3}$ . Hence $${\rm Ric}^{P_I} = -\frac{h''(r)}{2} g^{P_I} \quad \mbox{and}\quad {\rm s}^{P_I} = -h''(r) = 4mr^{-3},$$ where ${\rm s}$ stands for scalar curvature. It's not hard to check that $\nabla r = h(r)\partial_r$ and ${\rm Hess}\,r = (h'(r)/2) g^{P_I}$ . Great. Now in O'Neill's Semi-Riemannian Geometry book we have that for an arbitrary $B\times_\phi F$ , the formulas hold: ${\rm Ric}(X,Y) = {\rm Ric}^B(X,Y) - \dfrac{(\dim F)}{\phi}\,{\rm Hess}\,\phi$ for horizontal $X,Y$ ; ${\rm Ric}(X,Y) = 0$ for horizontal $X$ and vertical $Y$ . ${\rm Ric}(V,W) = {\rm Ric}^F(V,W) - g(V,W) \phi^\#$ for vertical $V$ , $W$ , where $$\phi^\# = \frac{\triangle \phi}{\phi} + (\dim F-1)\frac{g(\nabla\phi,\nabla\phi)}{\phi^2}.$$ This is corollary 43 in page 211. Now, the Schwarzschild black hole is supposed to be Ricci-flat, right? Sure, take $\phi = r$ , so $\triangle r = h'(r)$ and thus $r^\# = r^{-2}$ . For horizontal vectors we have $$-\frac{h''(r)}{2}g^{P_I}_{ij} - \frac{2}{r} \frac{h'(r)}{2}g^{P_I}_{ij} = 0$$ for all $i,j \in \{t,r\}$ , by plugging in $h(r) = 1-2mr^{-1}$ . Similarly, $r^\# = r^{-2}$ cancels the warping factor in ${\rm d}\Omega^2$ and since the Gaussian curvature of $\Bbb S^2$ is $1$ , ${\rm Ric}$ vanishes on pairs of vertical vectors as well. Awesome, right? Now look at exercise 13a) in page 214: $$ {\rm s} = {\rm s}^B + \frac{{\rm s}^F}{\phi^2} - 2\dim F\frac{\triangle \phi}{\phi} - \dim F(\dim F-1)\frac{g(\nabla \phi, \nabla \phi)}{\phi^2}.$$ We have $\dim F = 2$ , so: $${\rm s} = -h''(r) + r^{-2} - 4r^{-1} h'(r) - 2r^{-2}h(r).$$ But for $h(r) = 1-2mr^{-1}$ , we have $h'(r) = 2mr^{-2}$ and $h''(r) = -4mr^{-3}$ , leading to: $$4mr^{-3} + r^{-2}-8mr^{-3}-2r^{-2} + 4mr^{-3} = {\color{red}{-r^{-2} \neq 0}}.$$ What gives?","['semi-riemannian-geometry', 'general-relativity', 'riemannian-geometry', 'differential-geometry']"
3812541,There are $1000$ people in a hall. One person had their hand painted. Every minute everyone shake their hand with someone else.,"There are $1000$ people in a hall. One person had their hand painted. Every minute everyone shake their hand with someone else.
How much time is needed to paint all the hands? What is the best scenario? What is the worst scenario?
Scenarios are asking for max and min time to complete this task Here is what I was thinking:
Assuming we start from one person and time $0$ $\frac{1}{1000}$ -one minute $\rightarrow$ $\frac{2}{1000}$ -one minute $\rightarrow$ $\frac{4}{1000}$ -one minute $\rightarrow$ $\frac{8}{1000}$ Seems like pattern here is that the number of handshakes will double with every minute, so I would just need to find how long it takes to get to $\frac{500}{1000}$ $2n = 500 \implies n = 250$ times? Feels very wrong and definitely don't know how to approach.",['combinatorics']
3812556,how can i find $ \frac{1}{2\pi}\left ( \frac{\pi^{3}}{1!3}-\frac{\pi^{5}}{3!5}+\frac{\pi^{7}}{5!7}-... \right ) $,"Consider a sequence $ s_{n} = \frac{1}{2\pi}\left ( \frac{\pi^{3}}{1!3}-\frac{\pi^{5}}{3!5}+\frac{\pi^{7}}{5!7}-...+\frac{\left ( -1 \right )^{n-1}\pi^{2n+1}}{\left ( 2n-1 \right ) ! \left ( 2n+1 \right )}  \right ) $ How can I attack this to find $ \lim_{n \to \infty}s_{n} \ $ ? As the series $ \lim_{n \to \infty}s_{n} \ $ converges absolutely, so by simple manipulation I arrived to this : $ \lim_{n \to \infty}s_{n} = \frac{1}{\pi}\left \{ \left ( \pi \right ) + \left ( \pi - \frac{\pi^{3}}{3!} \right ) + \left ( \pi - \frac{\pi^{3}}{3!} + \frac {\pi^{5}}{5!} \right )+...\right \} $ What to do next? Anyone please?","['sequences-and-series', 'cauchy-sequences', 'analysis', 'real-analysis']"
3812576,Epsilon-delta proof for $\lim_{x \rightarrow a} \frac{x^n - a^n}{x -a}$,"I'm having trouble finding the right way to approach this limit. $$ \left| \frac{x^n - a^n}{x-a} - na^{n-1} \right| < \varepsilon, \text{ given } |x-a| < \delta $$ I've tried rewriting $\frac{x^n - a^n}{x-a}$ as $\sum_{k=0}^n x^ka^{n-k}$ , but that made it hard to reintroduce a $\delta$ into the inequality. I've also tried assuming $\delta < 1$ and rewritting the numerator as $(a+1)^n - a^n$ , but similarly ran into problems with a disappearing delta. Can I please have a nudge in the right direction?","['limits', 'real-analysis']"
3812644,Correct function notation (domain/range) for $f(x)=\sqrt{x+5}$?,"I have $f(x)=\sqrt{x+5}$ and I want to write this in a proper function notation. I tried the notations $$
f:\mathbb R \rightarrow \mathbb R, \quad f(x)=\sqrt{x+5} 
\tag 1
$$ and $$
f:\mathbb R \rightarrow \mathbb R, \quad 
x\mapsto \sqrt{x+5}
\tag 2
$$ But my problem: Isn't it wrong to denote the domain with $\mathbb R$ , because we have the condition $x\geq -5$ ? If so, what is the correct notation? Thanks!","['elementary-set-theory', 'algebra-precalculus', 'functions', 'notation']"
3812652,Differentiability in zero,"Consider the function given by, $$f(x)=\begin{cases} 
      x^2+kx+m & -\frac{\pi}{2}<x< 0 \\
      \tan(x)+\cos(x) & 0\leq x< \frac{\pi}{2}
   \end{cases}$$ I now want to determine $k$ and $m$ such that $f$ is differentiable in $x=0$ . Furthermore, I want to determine $k$ such that the line $k=y-36x$ is perpendicular to the following curve given by, $$y=\frac{1}{|x-7|}$$ For the first task I concluded $m=1$ and $k\in \mathbb{R}$ . Is this correct, for the second task, I have not made any progress","['calculus', 'derivatives']"
3812658,"generators of $SL(2,\mathbf{Q}_p)$","I want to ask how to prove the normal subgroup generated by matrices $(1,𝑒;0,1)$ and $(1,0;𝑒,1)$ for $𝑒$ small is still the whole of $SL(2,\mathbf{Q}_p)$ ? This is mentioned in the answer of this question . Thanks!","['algebraic-number-theory', 'p-adic-number-theory', 'matrices', 'abstract-algebra', 'group-theory']"
3812659,How much are $\sum_n X_n$ and $\sum_n X_n X_{n+1}$ independent in a random walk?,"I was thinking about a problem in combinatorics and came up with an idea of estimating the joint probability distribution of multiple quantities related to a Markov chain. Let's say $(X_n)_{1 \leq n \leq N}$ is a Markov chain with a finite state space $S$ . The quantities I am interested in have the form $$\sum_{1 \leq n_1 < n_2 < \cdots < n_k \leq N} f(X_{n_1}, X_{n_2}, \cdots, X_{n_k}),$$ for some function $f : S^k \rightarrow \{ +1, 0, -1 \}$ . These are very complicated sums, so I would like to start by considering a simplified model. Let $(X_n)_{1 \leq n \leq N}$ be i.i.d. random variables which take values $\pm 1$ with probability $1/2$ each. Consider the two quantities $$S_N = \sum_{1 \leq n \leq N} X_n \quad \text{and} \quad T_N = \sum_{1 \leq n \leq N-1} X_n X_{n+1}.$$ Of course, they are not independent; $S_N = \pm N$ forces $T_N = N-1$ and vice versa. But for moderate values of $S_N$ and $T_N$ (say, of magnitude $O(\sqrt N)$ ), I expect them to be nearly independent in the sense that $$\mathbb{P}(S_N = s, T_N = t) \approx \mathbb{P}(S_N = s) \cdot \mathbb{P}(T_N = t) \quad (s, t = O(\sqrt N)).$$ How much are $S_N$ and $T_N$ independent in the limit $N \rightarrow \infty$ ? Would it be possible to give a bound for $$\left| \frac{\mathbb{P}(S_N = s, T_N = t)}{\mathbb{P}(S_N = s) \cdot \mathbb{P}(T_N = t)} - 1 \right|$$ or the mutual information of $S_N$ and $T_N$ ? (Or any other measures of dependency?)","['random-walk', 'markov-chains', 'probability-theory', 'probability', 'random-variables']"
3812678,What is the rank of a vector?,"From linear algebra we know that the rank of a matrix is the maximal number of linearly independent columns or rows in a matrix. So, for a matrix , the rank can be determined by simple row reduction, determinant, etc. However, I am wondering how the concept of a rank applies to a single vector, i.e., $\mathbf{v} = [a, \ b, \ c]^{\top}$ . My intuition suggests that the rank must be equal to 1, but I'm not even sure if it is defined for a vector. Can anyone help shed some light on this issue? Thanks in advance.","['matrices', 'matrix-rank', 'linear-algebra', 'vectors']"
3812684,Is Steiner ellipse defining a unique triangle?,"Steiner ellipse is a unique ellipse that touches the triangle at its vertices and whose centre is the triangle's centroid. Similar is the case for the Steiner inellipse . However, this one seems to be
uniquely linked to the Steiner ellipse, i.e. it carries no additional info. So my question is whether these Steiner ellipses define a unique triangle?
It seems to me that I still have an extra degree of freedom to fix the triangle.
What is this degree of freedom once I have fixed the Steiner ellipse? In other words, once I have a Steiner ellipse how can I parametrised all
triangles associated with it?","['euclidean-geometry', 'conic-sections', 'geometry']"
3812705,Can a non-zero matrix have a zero characteristic polynomial?,"Are there some field $\mathbb{F}$ , some $n \in \{1,2,\dots\}$ , and some non-zero $n \times n$ matrix $A$ over $\mathbb{F}$ , whose characteristic polynomial $p_A(t)$ is identically $0$ ? The same question was asked here in the past, and the answer explained that such a $p_A(t)$ was impossible, because a characteristic polynomial of an $n\times n$ matrix had degree $n$ . But this answer is unsatisfactory, because in some cases an identically zero polynomial has a positive degree: take for instance the polynomial $p(t) = t^5 + 4t$ in the field $\mathbb{Z}/5\mathbb{Z}$ of the integers modulo $5$ .","['matrices', 'field-theory', 'linear-algebra', 'characteristic-polynomial']"
3812729,In how many ways can a team of $4$ boys and $4$ girls be selected out of $6$ boys and $5$ girls if a particular boy and girl refuse to work together?,"In how many ways can a team of $4$ boys and $4$ girls be selected out of $6$ boys and $5$ girls if a particular boy and a particular girl refuse to work together. My attempt: Case 1: That particular boy is not considered. So, 4 boys to be selected out of 5 boys. 4 girls to be selected out of 5 girls. So, $\dbinom{5}{4} \cdot \dbinom{5}{4}=25$ Case 2: That particular girl is not considered. So, 4 boys to be selected out of 6 boys. And 4 girls to be selected out of 4 girls. So, $\dbinom{6}{4}\cdot\dbinom{4}{4}=15$ Case 3: That particular boy and girl are not considered. So, 4 boys out of 5 and 4 girls out of 4. So, $\dbinom{5}{4}\cdot\dbinom{4}{4}=5$ So, total cases: $45$ . Answer is given as $35$ . What's my mistake?","['combinations', 'combinatorics']"
3812757,Integrating $\int \frac{3x^4+2x^2+1}{\sqrt{x^4+x^2+1}} \mathrm{d}x$,"Evaluate the indefinite integral: $$\int \frac{3x^4+2x^2+1}{\sqrt{x^4+x^2+1}} \mathrm{d}x$$ I multiplied up and down by $x$ and substituted $x^6+x^4+x^2=t$ so that $2x(3x^4+2x^2+1)\mathrm{d}x=\mathrm{d}t$ and therefore the integral converts to $\displaystyle \int \frac{1}{2\sqrt{t}}\mathrm{d}t=\sqrt{t}+C$ . But the above approach was motivated by seeing the answer first. I couldn't solve it before that. What should be the more natural approach? Edit: To some, the above method may seem natural.Then can they provide an alternate method?","['integration', 'indefinite-integrals', 'calculus']"
3812764,Solutions to the under-damped Harmonic Oscillator equation ??,"For the damped harmonic oscillator equation $$\frac{d^2x}{dt^2}+\frac{c}{m}\frac{dx}{dt}+\frac{k}{m}x=0$$ we get that the general solution is $$x(t)=Ae^{-\gamma t}e^{i\omega_d t}+Be^{-\gamma t}e^{-i\omega_d t}$$ where $\gamma = \frac{c}{2m}$ and $ \omega_d=\sqrt{\omega^2-\gamma ^2}$ . Using Euler's equation, we can expand this as follows: $$Ae^{-\gamma t}(\cos(\omega _dt)+i\sin(\omega_d t))+Be^{-\gamma t}(\cos(\omega _dt)-i\sin(\omega_d t))$$ $$\Rightarrow e^{-\gamma t}(A+B)\cos(\omega_d t) +e^{-\gamma t}(Ai-Bi)\sin(\omega_d t)$$ But now we are dealing with a physical problem so we only examine the real part which is $e^{-\gamma t}(A+B)\cos(\omega_d t)$ . But this does not have any phase difference. Yet textbooks always make the claim that the real part of the solution is $$e^{-\gamma t}(C)\cos(\omega_d t+\phi)$$ where $\phi$ is some arbitrary initial phase. But where does that initial phase come from if the real part of the solution does not have a phase change in it? I understand that $A$ and $B$ themselves need not be real however I do not understand how this fact could ever lead to a non zero initial phase in the real part of the solution. This issue has bothered me for quite some time now so any help would be immensely appreciated!","['complex-analysis', 'mathematical-physics', 'complex-numbers', 'ordinary-differential-equations']"
3812769,"Trouble when to use $C(n+k-1,k)$ vs $C(n + (k-1),k-1)$","In my lecture notes I had the exact same problem: we have 6 different apples and 4 identical oranges to distribute them into 5 distinct boxes and I need to place a question: If the boxes are distinct , I am not sure if we should take into consideration when we  distribute the same oranges , that $(b1,b2,b3,b4,b5)=(1,0,1,2,0,0) \neq (0,1,1,2,0,0)$ (?).  In both cases the partitions are identical but they are different in terms of boxes. And I am confused whether I should use ${n+k-1} \choose {k}$ (where n stands for boxes and k for objects) or ${n + k-1} \choose {k-1}$ . I am particularly confused since my teacher gave us this question ( We have $n$ identical objects, and we want to distribute them to $3$ different children $A$, $B$, $C$. ) .
I can't see to be able to recognize anymore, in any problem that we distribute identical objects, which formula is needed As for the second question : I get the method mentioned  by summing up all the inclusive facts but what is wrong with $\frac{10!}{4! (2!)^5}$ and if the boxes are the same ( so we dont consider $(b1,b2,b3,b4,b5)=(1,0,1,2,0,0) \neq (0,1,1,2,0,0)$ ) shouldn't we also divide by $5! \rightarrow \frac{10!}{4! (2!)^5* 5!}$","['combinatorics', 'discrete-mathematics']"
3812783,Prove that there are integral solutions to $\frac{1}{x_1^2}+\frac{1}{x_2^2}+\frac{1}{x_3^2}+\cdots+\frac{1}{x_n^2}=1$ if $n \ge 6$,"We need to prove that the equation $$\frac{1}{x_1^2}+\frac{1}{x_2^2}+\frac{1}{x_3^2}+\cdots+\frac{1}{x_n^2}=1$$ has integral solutions for all $n\ge 6$ The best i could think of was inverses of atleast $6$ perfect squares add up to $1$ What kind of approach should we have to solve these kinds of problems? I find one of these problems that i can't solve, what should i try then?
All Hints are welcome and full length solutions discouraged","['contest-math', 'algebra-precalculus']"
3812814,Explaining Directional derivatives,"I'm trying to understand the concept of the directional derivative, from the perspective of my multivariable calculus textbook. I've typed out a summary of the explanation, with the questions I couldn't answer in boldface. Any intuitive answers, geometrical answers, physical answers are welcome. Formal, rigorous answers are also welcome. Partial explanations (answering only one of the questions etc) are also very welcome! Consider the problem of calculating the rate of change of $\phi$ in some particular direction. For an infinitesimal vector displacement $d \mathbf{r},$ forming its scalar product with $\nabla \phi$ we obtain $$
\begin{aligned}
\nabla \phi \cdot d \mathbf{r} &=\left(\mathbf{i} \frac{\partial \phi}{\partial x}+\mathbf{j} \frac{\partial \phi}{\partial y}+\mathbf{k} \frac{\partial \phi}{\partial z}\right) \cdot(\mathbf{i} d x+\mathbf{j} d y+\mathbf{k} d x) \\
&=\frac{\partial \phi}{\partial x} d x+\frac{\partial \phi}{\partial y} d y+\frac{\partial \phi}{\partial z} d z \\
&=d \phi
\end{aligned}
$$ which is the infinitesimal change in $\phi$ in going from position $\mathbf{r}$ to $\mathbf{r}+d \mathbf{r} .$ In particular, if $\mathbf{r}$ depends on some parameter $u$ such that $\mathbf{r}(u)$ defines a space curve then the total derivative of $\phi$ with respect to $u$ along the curve is simply $$
\frac{d \phi}{d u}=\nabla \phi \cdot \frac{d \mathbf{r}}{d u}.
$$ Question 1: How did we get this? Should I just divide both sides of $\nabla \phi \cdot d \mathbf{r} = d\phi$ by $du$ ? I don't even know if that's a valid operation. In the particular case where the parameter $u$ is the arc length $s$ along the curve, the total derivative of $\phi$ with respect to $s$ along the curve is given by $$
\frac{d \phi}{d s}=\nabla \phi \cdot \hat{\mathbf{t}}
$$ where $\hat{\mathbf{t}}$ is the unit tangent to the curve at the given point. Question 2: Then why isn't $\frac{d \phi}{d s} = 0$ ? Surely $\nabla \phi$ is perpendicular/tangent to the surface of $\phi$ , so it will be perpendicular to $\hat{\mathbf{t}}$ ! In general, the rate of change of $\phi$ with respect to the distance $s$ in a particular direction a is given by $$
\frac{d \phi}{d s}=\nabla \phi \cdot \hat{\mathbf{a}}
$$ (Question 3: (most burning question) I have no idea how to obtain/understand, the above result/why the above result holds. Also, am I to think $\nabla \phi \cdot \hat{\mathbf{a}} = \nabla \phi \cdot \hat{\mathbf{t}}?$ ) and is called the directional derivative. Since $\hat{\mathbf{a}}$ is a unit vector we have $$
\frac{d \phi}{d s}=|\nabla \phi| \cos \theta
$$ where $\theta$ is the angle between $\hat{\mathbf{a}}$ and $\nabla \phi$ . Clearly $\nabla \phi$ lies in the direction of the fastest increase in $\phi$ and $|\nabla \phi|$ is the largest possible value of $d \phi / d s$ . Question 4: I get that the largest possible value of $d \phi / d s$ is when $\theta = 0$ , which is the direction of $\nabla \phi$ , but why does largest $\frac{d \phi}{d s}$ imply direction of fastest increase of $\phi$ ?","['vectors', 'vector-fields', 'real-analysis', 'multivariable-calculus', 'vector-analysis']"
3812841,Prove $\sum_{m=i}^{n}2^{n-m}\binom{m}{i}=\binom{n+1}{i+1}+\ldots+\binom{n+1}{n+1}=\sum_{m=i}^{n}\binom{n+1}{m+1}$ without induction,"I would like to prove the following: $$\sum_{m=i}^{n}2^{n-m}\binom{m}{i} = \binom{n+1}{i+1}+\binom{n+1}{i+2}+\ldots+\binom{n+1}{n+1} = \sum_{m=i}^{n}\binom{n+1}{m+1}$$ which is quite easy to prove by induction. I'm looking for an algebraic or combinatoric approach. This shouldn't be very complicated from a combinatoric point of view, since the right hand side is the number of different subsets of the set $\{1, 2,  \ldots, n+1\}$ with at least $i+1$ elements. Also on the left hand side $2$ appears, which arises when one counts the total number of subsets. But I struggle to find a method to count this number to make it look like the left hand side; what do you suggest?","['summation', 'combinatorial-proofs', 'alternative-proof', 'binomial-coefficients', 'combinatorics']"
3812876,Concentration Inequality with Random Coefficients,"Suppose $X_1,X_2,\ldots$ are independent and take values in $[0,1]$ . For any coefficients $C_1,C_2, \ldots \in [0,1]$ we have the concentration inequality $$P \left( \Big| \,\sum_{i=1}^N C_iX_i - \sum_{i=1}^N C_i \mathbb E[X_i]\,\Big|> t\right) \le \exp \left(-\frac{t^2}{2\sum_{i=1}^N C_i^2}\right) \le \exp \left(-\frac{t^2}{2N}\right).$$ In particular take $t = \frac{1}{2}\sum_{i=1}^N C_i \mathbb E[X_i]$ to get for example $$P \left(  \sum_{i=1}^N C_iX_i \ge \frac{1}{2}\sum_{i=1}^N C_i \mathbb E[X_i]\right) \le \exp \left(-\frac{\big(\sum_{i=1}^N C_i \mathbb E[X_i]\big)^2}{8N}\right).$$ Provided $C_i,\mathbb E[X_i]$ do not go to zero too quickly the RHS has order $e^{-cN}$ for some $c >0$ . Thus we get a  high probability bound for the sum being too far from its expectation. Now suppose instead of $C_1,C_2,\ldots$ we use random variables $Y_1,Y_2,\ldots$ as coefficients. Under some independence or martingale assumptions it is possible to bound the chance $\sum_{i=1}^N Y_i X_i $ is far from it's expectation. If $X_i,Y_i$ are independent the expectation is $\sum_{i=1}^N \mathbb E[Y_i] \mathbb E[X_i]$ . What I am interested in is subtler. I want to bound the chance $\sum_{i=1}^N Y_i X_i $ is far from $\sum_{i=1}^N   \mathbb E[Y_i] X_i$ . I don't know how to go about this, or what kind of assumptions are needed. Does anyone have any hints? Some things I am happy to assume are that $X_1,X_2,\ldots$ are independent and each $X_i,Y_i$ are independent. I am not happy to assume $Y_1,Y_2,\ldots$ are independent or that $Y_i$ is independent of $X_1,\ldots, X_{i-1}$ . Context: This comes up in the analysis of an algorithm. Each $X_i$ is a prediction for the state of the world on day $i$ and each $Y_i$ is part of the action taken on day $i$ . Hence $Y_i$ is dependent on $X_i$ directly. Since the actions change slowly $Y_i$ is not independent of the past actions. Hence it is also dependent on $X_{i-1},\ldots, X_1$ . The $X_i$ in $X_i Y_i$ measures the performance of the algorithm.","['martingales', 'concentration-of-measure', 'probability']"
3812904,"Prove that, if $y''+w(x)y=0,$ then $y$ has infinitely many zeroes","I have to prove, that for $w(x)\geq 1$ every solution for  differential equation $y''+w(x)y=0$ has infinity many zeros. My idea is, that I first take $w(x)=1$ , which has 2 linearly independent solutions $y=A\sin x+B\cos y$ Booth $\sin x$ and $\cos x$ have infinity many zeros, which is true for any of their linear combinations. So I have proven that for $w=1$ . For $w(x) > 1$ I can use Sturm–Picone comparison theorem , so between every 2 zeros of solution for $w=1$ , there is at least one solution of every solution for $w>1$ , therefore it is at least the same amount minus one of them as for $w=1$ . Does this proof make any sense, or what are its limits/problems? Is there any other way to prove that?","['roots', 'ordinary-differential-equations']"
3812907,Genericity of embeddings of closed manifolds,"Let $M$ be a closed manifold, and let $f:M\to\mathbb{R}^n$ be a continuous map. Then, assuming that $n$ is sufficiently large, is it true that $f$ is generically an embedding in a certain sense? Here's one attempt to make this question more precise (and please point out if there are better ways to do so): Question: Is it true that for any $\epsilon>0$ , there exists a continuous embedding $f':M\to\mathbb{R}^n$ such $\|f-f'\|_\infty<\epsilon$ ? Any references for a related discussion would also be great.","['manifolds', 'differential-topology', 'differential-geometry']"
3812958,How to understand an example in Krattenthaler's “Advanced Determinant Calculus”?,"In C. Krattenthaler's 1999 paper entitled “Advanced Determinant Calculus”, an example is given on p. 8 to illustrate Lemma 3 (p. 7). I don't quite understand this example, and I'm trying to wrap my head around it. The author illustrates the aforementioned lemma by means of the computation of $$\det_{1 \leq i,j \leq n} \bigg{(} {a+b \choose a-i+j} \bigg{)} .$$ To make everything a bit more palpable, let's take $a=2, b=3,$ and $n=3$ . The first step of the 'recipe' described right above the example is to “take as many factors out of rows and/or columns of your determinant, so that all denominators are cleared”. In this case, we do so by multiplying the matrix with the product \begin{align*} \prod_{i=1}^{n} \frac{(a+b)!}{(a-i+n)!(b+i-1)!} &= \prod_{i=1}^{n} \frac{(2+3)!}{(2-i+3)(3+i-1)!} \\
&= \prod_{i=1}^{3} \frac{5!}{(5-i)!(2+i)!} \\
&= \frac{5!}{4!3!}\cdot\frac{5!}{3!4!}\cdot\frac{5!}{2!5!}. \end{align*} So far, so good. The trouble starts with what happens inside the new matrix that is multiplied by this product. I don't understand what happens inside this matrix, of which the determinant can be calculated. Upon trying to understand it, I assumed that every term from the product corresponds to the respective rows of the matrix. So $\frac{5!}{4!3!}$ belongs to the first row, $ \frac{5!}{3!4!}$ belongs to the second one, and $ \frac{5!}{2!5!}$ corresponds to the third and last one. In order for the equality to hold, we must divide the rows inside the new matrix by their respective terms in the product. According to my calculations (which may or may not be right, please correct me if the latter is the case), the matrix becomes: $$ A = 
\begin{pmatrix} 4\cdot 3 & 3\cdot4 & 3\cdot 2 \\
3\cdot2 & 3\cdot 4 & 4\cdot3 \\
2\cdot1 & 2\cdot5 & 5\cdot4 \end{pmatrix} .$$ Now, I don't understand how this relates to the author's description of the new matrix $$B = \big{(} (a-i+n)(a-i+n-1) \dots (a-i+j+1) \cdot (b+i-j+1)(b+i-j+2) \dots (b+i-1) \big{)} ,$$ and how it arises after the product is introduced like we did above. In other words, my overall question would be: why is matrix $A$ equal to matrix $B$ ? Or perhaps: why are their determinants equal? I'm surprised this should be the case, because I computed the entries that correspond to the different terms in the product inside matrix $B$ and the term-wise multiplications of the resulting matrices don't seem to be equal to matrix $A$ . To illustrate, let's consider some terms. For term $(a-i+n)$ , we have the matrix $b_{a_{n}} := \begin{pmatrix} 4 & 4 & 4 \\
3 & 3 & 3 \\
2 & 2 & 2 \end{pmatrix}  $ . For term $(a - i + n -1)$ , there is the matrix $b_{a_{n-1}} := \begin{pmatrix} 3 & 3 & 3 \\
2 & 2 & 2 \\
1 & 1 & 1  \end{pmatrix} $ . For term $(a-i+j+1)$ , the corresponding matrix is $b_{a_{j+1}} := \begin{pmatrix} 3 & 4 & 5 \\
2 & 3 & 4 \\
1 & 2 & 3 \end{pmatrix} $ For term $(i+b-j+1)$ , we find the matrix $b_{b_{j-1}} :=\begin{pmatrix} 4 & 3 & 2 \\
5 & 4 & 3 \\
6 & 5 & 4 \end{pmatrix} $ Etc. It seems to me that the element-wise multiplication of these matrices does not equal matrix $B$ . Questions Have I made a mistake in my calculations somewhere? Or am I making other errors? If so, please let me know. Is the author indeed performing row-wise multiplications of the matrix in question? If so, are they multiplied by the terms that I indicated? Or do different rows correspond to other term-wise multiplications? Why does the element-wise multiplication of the matrices $b_{a_{n}} \dots b_{b_{1}} $ not equal matrix $A$ according to my computations? How -- if at all -- should I interpret the products inside matrix $B$ differently? Could you please explain why matrix $\det(A)$ is equal to matrix $\det(B)$ ? Or why are their determinants equal? Both in the general case, and in the specific case of my toy example?","['matrices', 'proof-explanation', 'determinant', 'linear-algebra']"
3813006,Residue Theorem Integral of $\frac{1}{\sinh(x)-1}$,"I need help with the integral: $$\int_{-\infty}^\infty\frac{x}{\sinh(x)-1}dx,$$ and I (unfortunately) have to use contour integration techniques. I know how to do the integral $$\int_{-\infty}^\infty\frac{1}{\sinh(x)}dx,$$ so using a similar strategy, I tried integrating $$f(z):=\frac{z}{\sinh(z)-1}$$ around a box of width $2R$ and height $\pi $ centered at the origin. However, $\frac{1}{\sinh(x)-1}$ has poles at $z_n=\ln(2\pm\sqrt{2})+2\pi i n,$ for $n\in\mathbb{N}$ , and so we must make $\epsilon$ bumps around $\log(2\pm\sqrt{2})$ . However, around these bumps, $$\int_{C_\epsilon}f(z)dz=\int_0^\pi\frac{2\log(1\pm\sqrt{2})+2\epsilon e^{-i\theta}}{\log(1\pm \sqrt{2})[e^{\epsilon e^{i\theta}}-e^{-\epsilon e^{-\theta}}]-1}\cdot -i\epsilon e^{-i\theta}d\theta$$ Does this simplify? I'm not sure how to tackle this. I also thought about making a substitution $x\mapsto \ln(x)$ at the very beginning, but the integration bounds that I get confuse me (I get from $-\infty +i\pi$ to $\infty$ ).","['complex-analysis', 'contour-integration']"
3813038,Relationship between two Galois Theorems.,"The classical Galois theorem states that For $K\subset L$ a finite Galois extension of fields, any intermediate extension $K\subset M\subset L$ satisfies $\mathrm{Fix}(\mathrm{Hom}_M(L,L))\subset M$ and any subgroups $H$ of $\mathrm{Hom}_K(L,L)$ satisfies $\mathrm{Hom}_{\mathrm{Fix}(H)}(L,L)\subset H$ , where $\mathrm{Hom}_M$ means the $M$ -algebra morphisms. Another version of the theorem, which in the book Galois Theories , chapter 2, is a generalization according to the authors: First define the category $\mathrm{Split}_K(L)$ to be the $K$ -algebra $A$ with the property that $A$ is finite over $K$ and the minimal polynomial of any $a\in A$ splits to linear factors in $L$ . As usual let $K\subset L$ be a finite Galois extension. The book states that The contravariant functor $F: \mathrm{Split}_K(L)\to \mathrm{Hom}_K(L,L)\mathrm{-Sets}$ defines by $F(A)=\mathrm{Hom}_K(A,L)$ induces a categorical equivalence. But how does this equivalence contains the classical version of Galois theorem? Let $G= \mathrm{Hom}_K(L,L)$ . Taking $H$ a subgroup of $G$ then the quotient $G/H$ as $G\mathrm{-Set}$ is isomorphic to some $\mathrm{Hom}_K(A,L)$ so we have a surjective homomorphism from $\mathrm{Hom}_K(L,L)$ to $\mathrm{Hom}_K(A,L)$ which by the categorical equivalence corresponds exactly to one mono arrow $A\to L$ . But does it mean that I need to show every mono is injective in the category $\mathrm{Split}_K(L)$ ? I have attempted to prove this using varies results in integral extensions but can not obtain anything. It seems to be trivial but I am lacking the background of categorical language. Any help would be appreciated.","['galois-theory', 'galois-extensions', 'abstract-algebra', 'category-theory']"
3813064,Can we derive results from infinite sequences of integration by parts?,"I remember from my school days some old trick with integrals where if $\sin$ or $\cos$ were involved, we could sometimes apply the partial integration theorem once or twice and express the first integral in terms of itself in a way which let us solve it. An example: $$I=\int\sin(x)^2dx = x -\int \cos(x)^2dx = x - \sin(x)\cos(x) - \int \sin(x)^2dx$$ Now rearranging gives $$2I = x-\sin(x)\cos(x)$$ Which we can verify. Now to the question, can this be applied in a more general setting? For example if we do not after a finite set of steps end up with a closed form expression, could we use a series of integrals which might arise from iterating the integration-by-parts indefinitely? Are there any particular problems solvable this way which aren't solvable otherwise?","['integration', 'calculus', 'soft-question', 'real-analysis']"
3813075,How to show that $ C := A^3-3A^2+2A = 0$?,"Let $$A = \begin{pmatrix} -1 & 1 & 2 \\ 0 & 2 & 0 \\ -1 & 1 & 2  \end{pmatrix}$$ Let $C:= A^3-3A^2+2A $ .
Show that $C=0$ . I know that $A$ is diagonalisable with $\operatorname{spec}(A)=\{0,1,2\}$ .
I have no clue how to approach that problem. Any advice?","['matrices', 'spectral-theory', 'diagonalization']"
3813101,Difficulty in understanding Feller test for explosions for SDE. Any other source?,"I was focusing on Feller test for explosions for a SDE like this $$dX_t=\mu(X_t)\cdot dt + \sigma^2(X_t)\cdot dW_t$$ Particularly, I was focusing on Karatzas, Shreve and attention is on exit time of the process $\{X_t\}$ from an interval on the real line $U=(k,l)$ . Starting from $$\tau_{n}=\inf\Big\{t\geq0: \displaystyle{\int}_0^t \sigma^2(X_s)ds\geq n\Big\}; \hspace{0.5cm}n=1,2,...$$ $$T_{u,v}=\inf\{t\geq0: X_t\notin(u,v)\};\hspace{0.5cm}k<u<v<l$$ , after a series of passagges I am not going to mention here, letting both $n$ and $t$ go to $\infty$ I am told that $$\mathbb{E}\{T_{u,v}\}<\infty$$ that is "" $X$ exits from every compact subinterval of $(k,l)$ in finite expected time "". I cannot understand why a conclusion on the exit of the process from every compact subinterval of $(k,l)$ can be drawn. What I mean is: if I let $t$ go to $\infty$ , then $T_{u,v}=\inf\{t\geq0: X_t\notin(u,v)\}=\infty$ , which - as far as I know - means that there is not a finite stopping time in which the process $\{X_t\}$ at least touches the bounds of the interval $U$ . However, then the two probabilities $$\mathbb{P}[X_{T_{u,v}}=u]$$ $$\mathbb{P}[X_{T_{u,v}}=v]$$ are defined and I am told that they add up to $1$ , making me guess that either $X_{T_{u,v}}=u$ or $X_{T_{u,v}}=v$ , contradicting my above conclusion that the process $X_t$ does never touch (at least) the bounds $u$ and/or $v$ . What's wrong with my reasoning? Could you possibly clear my mind up and/or suggest some other sources which could help me clear my mind up?","['stochastic-processes', 'stochastic-differential-equations', 'intuition', 'probability-theory', 'stochastic-calculus']"
3813107,"Solve $x^5 - 1 = y^2$ for integer values of $(x,y)$ .","Solve $x^5 - 1 = y^2$ for integer values of $(x,y)$ . What I Tried :- I see that $x^5 = y^2 + 1$ , from here I can  conclude that $x$ has to be positive , because if $x \leq 0$ , then $x^5 \leq 0$ , but $y^2 + 1 > 0$ . Also I thought that maybe $(x^\frac{5}{2} + 1)(x^\frac{5}{2} - 1) = 1$ would do the trick [which would have implied that both the terms are $(1,1)$ or $(-1,-1)$ ], but that dosen't necessarily mean that $x^\frac{5}{2}$ is an integer . Then I see that :- $x^5 - 1 = y^2$ $\rightarrow (x-1)(x^4 + x^3 + x^2 + x + 1) = y^2$ From here the only idea is that $x \neq 2$ , because if it is then it can't be a perfect square (it's a foolish idea though) . But I tried playing this problem with many ways , but I am stuck at the same place . Can anyone help ? Note :- It's given that answer is only $(1,0)$ , but how is it coming?","['linear-algebra', 'problem-solving', 'complex-numbers']"
3813133,Ampleness of a line bundle on the generic fibre,"Let $f:X\rightarrow S$ be a morphism of regular schemes and $\mathscr{L}$ a line bundle over $X$ . Suppose that $S$ is irreducible and let $\eta$ be its generic point. Assume that $\mathscr{L}\otimes\kappa(\eta)$ is an ample line bundle on $X_{\eta}$ . I have two questions: Does there exist an open subset $U$ of $S$ such that $\mathscr{L}\otimes U$ is ample on $f^{-1}(U)$ ? Does there exist an open subset $U$ of $S$ such that $\mathscr{L}\otimes\kappa(s)$ is ample on $X_s$ for all $s\in U$ ? If necesary, one can add some more assumptions on $X$ , $S$ (noetherianity...) and $f$ (properness...).","['algebraic-geometry', 'schemes', 'line-bundles']"
3813143,Radius of semicircle inscribing $5$ congruent circles of radius $r$,"There are $5$ congruent circles with radius $r$ inscribed in a semicircle as shown in the below diagram. What is the radius of the semicircle? My attempt: If the min distance between circle $C_2$ and $C_3$ is $2x$ , $C_2C_3 = 2 (r + x)$ $C_2C_5 = C_3C_5 = 2r$ Perpendicular from $C_5$ to the line $C_2C_3$ = $r+h$ then $(r+h)^2 + (r+x)^2 = (2r)^2$ If I get one more equation in $x, h, r$ , I can find $x$ and $h$ in terms of $r$ . If we extend $C_1C_4$ and $C_3C_5$ , they will meet at point $M$ , with $MC_2$ perpendicular to line $C_1C_2$ . While I know $C_1C_2 = 2(r+x)$ , I am not sure I can express $C_2M$ in terms of $r$ and $h$ . I also tried using trigonometry but could not find the radius of the bigger circle. I would appreciate any directional help or solution.","['trigonometry', 'geometry']"
3813160,Is there some analytical result for Wright-Fisher SDE?,"Are there already known analytical results (e.g. Laplace transform) about the Wright-Fisher SDE $$dY=(A-(A+B)Y)dt+C\sqrt{Y(1-Y)}dW$$ with $A$ , $B$ and $C$ parameters? If so, could you please quote some reference?","['stochastic-processes', 'stochastic-differential-equations', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
3813161,Sum of products of multiplicative characters of $\mathbb{F}_q$,"This is problem 5.30 from the book ""Introduction to Finite Fields"" by Lidl and Niederreiter. Let $\lambda_1, \lambda_2, \lambda_3$ be nontrivial multiplicative characters of $\mathbb{F}_q$ and let $a_1\not=a_2\in\mathbb{F}_q$ . Prove that $$
\sum_{b\in\mathbb{F}_q} \:\left | \: \sum_{c\in\mathbb{F}_q} \lambda_1(c+a_1)\lambda_2(c+a_2)\lambda_3(c+b) \:\right| ^2  = \begin{cases}{q^2 - 3q  \:\:\:\:\:\:\:\:\:\:\text{ if $\lambda_1\lambda_2$ nontrivial,} \\
q^2 - 2q - 1 \:\:\:\text{ if $\lambda_1\lambda_2$ trivial.} }
\end{cases}
$$ I can reduce the above formula to $$
q^2-2q - \sum_{c\in\mathbb{F}_q} \sum_{d\in\mathbb{F}_q} \lambda_1(c+a_1)\lambda_2(c+a_2)\overline{\lambda_1(d+a_1)\lambda_2(d+a_2)}.
$$ If $\lambda_1\lambda_2$ is trivial, then the sum is equal to $(-1)^2$ so we obtain the wanted result.
However if $\lambda_1\lambda_2$ is nontrivial, we have , we can simplify the double sum above to $$
\left | \: \sum_{c\in\mathbb{F}_q} \lambda_1(c+a_1)\overline{\lambda_2^{-1}(c+a_2)} \: \right | ^2_.
$$ This is awfully similar to the product of two multiplicative characters which states $$
\left | \:\sum_{c\in\mathbb{F}_q} \lambda(c)\overline{\psi(c)} \: \right | ^2 = q-1 \quad \text{ if $\lambda \psi^{-1}$ nontirvial.}
$$ This would indeed lead to the wanted result where $\lambda=\lambda_1$ and $\psi=\lambda_2^{-1}$ , however there is an additive factor of $a_1$ (resp. $a_2$ ) in the argument of $\lambda_1$ (resp. $\lambda_2^{-1}$ ) which doesn't allow me to use the above formula.","['field-theory', 'group-theory', 'characters']"
3813195,Showing that $x^{\frac{1}{x}}-1-\frac{\ln(x)}{x}<\frac{1}{x\ln(x)}$ for all $x>1$,"When trying to come up with an asymptotic formula for the partial sums $\sum_{n\leq x}(\sqrt[n]{n}-1)$ , I came across the following inequality which seems to be to holding for all $x>1$ : \begin{equation}\tag{$*$}
x^{\frac{1}{x}}-1-\frac{\ln(x)}{x}<\frac{1}{x\ln(x)}.
\label{eq:special}
\end{equation} I am unable to prove this, but numerical evidences suggest that it is correct. I have only been able to prove a weaker inequality for all $x>1$ $$x^{\frac{1}{x}}-1-\frac{\ln(x)}{x}<\frac{1}{ex}.$$ This is obtained by rewriting $x^{\frac{1}{x}}-1-\frac{\ln(x)}{x}=\frac{1}{x}\int_{1}^{x}\frac{t^{1/x}-1}{t}dt$ and noticing that for all $x>1$ and $t>0$ , we have (by computing the derivative of $f(t)=\frac{t^{1/x}-1}{t}$ and solving $f'(t)=0$ ), $$\frac{t^{1/x}-1}{t}\leq\frac{(x-1)^{x-1}}{x^x}=\frac{1}{x-1}\left(1-\frac{1}{x}\right)^{x}\leq\frac{1}{(x-1)e}.$$ Something that also comes to mind when dealing with \eqref{eq:special} is to write \begin{equation}
x^{\frac{1}{x}}-1-\frac{\ln(x)}{x}=\exp{\left(\frac{\ln(x)}{x}\right)}-1-\frac{\ln(x)}{x}=\sum_{k=2}^{+\infty}\frac{1}{k!}\frac{\ln^{k}(x)}{x^{k}}
\end{equation} and try to bound the infinite sum by $\frac{1}{x\ln(x)}$ . This however didn't help me prove the inequality. Does anyone have any hints on how to prove \eqref{eq:special}.","['inequality', 'logarithms', 'real-analysis']"
3813203,Confusion in understanding epsilon delta definition of limit with a discontinuous function,"To understand the definition (from Wikipedia) I have taken a function $$
f(x) = \begin{cases} 5 \quad &\text{if $x \le 2,$} \\ 5 \quad &\text{if $x=3$,} \\ 5 \quad &\text{if $x\ge 4$,} \end{cases}
$$ I also see that $\forall \epsilon \gt 0, \exists \delta=2 \gt 0, \forall x \in D, 0 \lt |x-3| \lt \delta \implies |f(x) - 5| \lt \epsilon$ where $D = \mathbb{R} \setminus \{(2,3) \cup (3,4)\}$ With this, it seems the limit for $f(x)$ exists at $x=3$ even tough there is a big gap around $x=3$ . Wondering if I'm misinterpreting the definition! Is there any bound on the gap in the neighborhood of $x=c$ , that I'm not able to gather from the definition of limit.","['limits', 'epsilon-delta', 'real-analysis']"
3813224,Inconsistent solution to differential equation,"$Q.$ Let f(x) be a continuous function satisfying the following differential equation- $$f(x)=(1+x^2)(1+\int_0^x\frac{f^2(t)dt}{1+t^2})$$ $$\text{Find  f(1)}$$ My Work-
1)Putting $x=0$ in the equation we get $f(0)=1$ 2)Dividing by $1+x^2$ and differentiating w.r.t. $x$ we get- $$(\frac{y}{1+x^2})'=\frac{y^2}{1+x^2}\qquad\text{∴ y=f(x)}$$ Simplifying- $$y'(1+x^2)-2xy=y^2(1+x^2)$$ so either $f(x)=0$ or $$\frac{-dy(1+x^2)}{y^2}+\frac{2xdx}{y}=(-1-x^2)dx$$ $$\frac{d}{dx}(\frac{1+x^2}{y})=\frac{d}{dx}(-x-\frac{x^3}{3})$$ $$\frac{1+x^2}{y}=-x-\frac{x^3}{3}+c$$ Using $y(0)=1$ we get $c=1$ and hence $$f(1)=-6$$ My issue- looking at the question, no matter the value of f(x), the R.H.S. of the equation must always be positive (squared) however the answer is coming to be negative. Am I missing something? Or is there any error in the question? Or is $f(x)=0$ the only acceptable solution?",['ordinary-differential-equations']
3813230,"Is $\forall x \forall y (x \neq y \rightarrow (P(x,y) \leftrightarrow \neg P(y,x)))$ valid?","I often came forward questions like these ( true or false) in my past papers. They are supposed to be answered quickly , not by calculations by ""trying to make sense by intuition if they provide an obvious fact ( so they are true) or if we suspect that they're not, we may come up easily with a counterexample. But since there no official solutions outhere, I can't rely on me , to be sure whether I assumed something right . So here is my approach: If it is valid , then it's negation must be always false $\rightarrow \neg[\forall x \forall y (x \neq y \rightarrow (P(x,y) \leftrightarrow \neg P(y,x))) ] \equiv \exists x \exists y(x \neq y \land \neg(P(x,y) \leftrightarrow \neg P(y,x)) $ In order this to be always false : assuming ** $x \neq y$ is always false , so we are in a universe with
only one element , but this is
not neccessary hence we move to the next case (2) (there are many cases that the statement is true) $\neg(P(x,y) \leftrightarrow \neg P(y,x))$ has to be always false $\rightarrow (P(x,y) \leftrightarrow \neg P(y,x))$ has to be always
true which I don't see why if for example P(x,y) is     defined as :
""x is connected to y in a digraph"" in a graph So I came to conculsion, that there are cases in order the negation of the statement to hold true . Hence , it is not valid . Let me know if you agree or if you have another way of thinking of it","['relations', 'logic', 'discrete-mathematics']"
3813287,"It's an indefinite integral $\int\frac{x^2(1- \ln(x))}{(\ln(x))^4-x^4}\,dx.$","$$\int\frac{x^2(1- \ln(x))}{(\ln(x))^4-x^4}\,dx.$$ I try to solve it by factoring, but got stuck in the problem. Please help anyone.","['integration', 'logarithms']"
3813291,Necessary and sufficient condition for convergence of series,"I'm solving this exercise in Klenke's book: Let $X_1,X_2, \dots $ be i.i.d. nonnegative random variables. By virtue of the Borel-Cantelli lemma, show that for every $c \in(0,1)$ , $$\sum_{n=1}^\infty  e^{X_n} c^n \begin{cases}
< \infty \textrm{ a.s.} & \textrm{if } \mathbb E[X_1] < \infty; \\
= \infty \textrm{ a.s.} & \textrm{if } \mathbb E[X_1] = \infty
\end{cases}$$ There are different ways to prove the statement using the Borel-Cantelli lemma (here's a thread with different answers: link )
However I wanted to try a different approach. I defined $S_k := \sum_{n=1}^k  e^{X_n} c^n $ which given the nonnegativity of its elements converges from below to $S:= \sum_{n=1}^\infty  e^{X_n} c^n $ . We can prove using the 0-1 Law that $S=a$ almost surely where $ a \in [-\infty, \infty]$ is a constant. And now applying the monotone convergence theorem and taking the expectations delivers: $$
a=\mathbb{E}[S]=\sum_{n=1}^\infty  \mathbb{E}[e^{X_n}] c^n =\mathbb{E}[e^{X_1}]  \sum_{n=1}^\infty  c^n 
$$ Which means that $a$ is finite iff $\mathbb{E}[e^{X_1}] < \infty$ . This however is not equivalent to the statement in the exercise. Does anyone see where the argument fails?","['borel-cantelli-lemmas', 'convergence-divergence', 'probability-theory']"
3813333,Vakil 9.3 F: Fibers over Blow-Up Map,"This in in regards to Vakil 9.3 F. I am trying to compute the fiber over any closed point $p$ of $\mathbb P^1_k$ of the map $$g: \operatorname{Bl}_{(0, 0)} \mathbb A^2_k \to \mathbb P^1_k$$ The definition we are using for the blowup is $$\operatorname{Bl}_{(0, 0)} \mathbb A^2_k := \mathbb A^2_k \times_k \mathbb P^1_k$$ modulo the relation $xv = yu$ , where $x, y$ are the coordinates of $\mathbb A^2_k$ and $u, v$ are the coordinates of $\mathbb P^1_k$ . By definition, I am trying to compute $$g^{-1}(p) := \operatorname{Bl}_{(0, 0)} \mathbb A^2_k \times_{\mathbb P^1_k} \operatorname{Spec} \kappa(p)$$ where $\kappa(p)$ is the residue field of $p$ . Writing this out on the affine open $u \neq 0$ , for example, we get $$g^{-1}(p) = (\mathbb A^2_k \times_k \mathbb A^{2, v/u}_k) / (xv - yu) \times_{\mathbb A^{2, v/u}_k} k$$ or $$\operatorname{Spec} k[x, y] \otimes_k k[v/u]/(xv - yu) \otimes k$$ I'd like to say, using the given relation on the four variables, that this is $$\operatorname{Spec} k[x, y, y/x]$$ but that seems off to me. Where have I gone wrong?","['algebraic-geometry', 'blowup']"
3813341,Alternative notation for intersection and union [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 3 years ago . Improve this question I was helping a friend recently with some math homework when I came across the following notations for intersection and union respectively: $$ ç $$ $$ È $$ So I am more familiar with what I would think is the standard notation for these operations ( $\cup$ , $\cap$ ) and have never seen this before. Does anyone recognize/regularly use these notations, I'm just curious! Thanks in advance!","['elementary-set-theory', 'notation']"
3813403,Embedding in the $d+1$ Euclidean space,"I have heard of the Whitney embedding theorem where any $d$ -dimensional manifold can be embedded in $\mathbf{R}^{2d}$ . My question is whether the necessity of going up to $2d$ rather than $d+1$ is only due to the global properties of the manifold (e.g. avoiding self intersections) or not? Here is a more precise statement of what I am looking for: Take a $d$ -dimensional manifold $\mathcal{M}$ equipped with a metric $g$ . For any point $p\in M$ is it possible to choose a neighborhood $U$ containing $p$ , and a map $\phi:U\rightarrow \mathbf{R}^{d+1}$ , where $\mathbf{R}^{d+1}$ is equipped with a flat Euclidean metric $\eta$ , and we have the pullback $\phi^* \eta=g$ (restricted to $U$ of course)?","['manifolds', 'differential-geometry']"
3813429,"Find the volume formula of a simplex proving that $\int_0^1\int_0^{1-x_n}...\int_0^{1-(x_n+...+x_2)}1\,\,\,dx_1...dx_n=\frac 1{n!}$","Definition If $x_0,...,x_n$ are $(n+1)$ affinely independent point of $\Bbb R^n$ (which means that the vectors $(x_1-x_0),...,(x_n-x_0)$ are linearly independent) then simplex determined by them is the set $$
S:=\Biggl\{x\in\Bbb R^n: x=\alpha_1v_1+...+\alpha_nv_n, \sum_{i=1}^n\alpha_i\le1\,\,\,\text{and}\,\,\,\alpha_i\ge0\,\,\,\text{for all}\,i\Biggl\}
$$ where $v_i:=(x_i-x_0)$ for each $i>0$ . So with the previous definition I try to show that the volume of a symplex $S$ is given by the formula $$
v(S)=\Big|\frac{1}{n!}\det\big[(x_1-x_0),...,(x_n-x_0)\big]\Big|
$$ for each $n\in\Bbb N$ . First of all we observe that any simplex is the intersection of two parallelopides (see here for details) so that any simplex is rectifiable (indeed any parallelopiped is rectifiable and the intersection of rectifiable sets is rectifiable too) and thus any continuous function is there integrable. Now if $x_0,...,x_n$ are $(n+1)$ point affinely independent we define the transformation $h:\Bbb R^n\rightarrow\Bbb R^n$ through the condition $$
h(x):=A\cdot x+x_0
$$ where $A$ is the matrix whose $j$ -th column is the vectors $(x_j-x_0)$ for each $j=1,...,n$ . So now we observe that the transformation $h$ carries the simplex $$
E:=\{x\in\Bbb R^n:x_1+...+x_n\le 1\,\,\,\text{and}\,\,\, x_i\ge 0\,\,\,\text{for all}\,i\}
$$ onto the simplex $S$ generated by the points $x_0,...,x_n$ . So if we prove that $$
v(E):=\frac 1{n!}
$$ for all $n\in\Bbb N$ then by the change variable theorem (it is easy to verify that $h$ is a diffeomorphism) $$
v(S)=\int_S 1=\int_E|\det A|=\frac 1{n!}|\det A|
$$ for each $n\in\Bbb N$ . So let's start to prove by induction that $$
v(E)=\frac 1{n!}
$$ for each $n\in\Bbb N$ . So if $n=1$ then $E=[0,1]$ and so clearly the formula trivially holds. So we suppose that the formula holds for $(n-1)$ and we prove that it holds for $n$ . So we have to prove that $$
\int_E1=\frac 1{n!}
$$ and to do this we will use Fubini's formula. So I have to prove that $$
\int_0^1\int_0^{1-x_n}...\int_0^{1-(x_n+...+x_2)}1\,\,\,dx_1...dx_{n-1}dx_n=\frac 1{n!}
$$ but unfortunately I don't be able to prove it. For sake of completeness I point out that it seems that here there is a similar solution to that I gave (see the answer of the professor Blatter) but I don't fully understand it. In particular the solution I linked says that if we define $$
E_\xi:=\{x\in\Bbb R^n:x_1+...+x_{n-1}\le1,\,\,\,\text{and}\,\,\,x_1,...,x_{n-1}\ge 0\,\,\,\text{and}\,\,\,x_n=\xi\}
$$ for any $\xi\in[0,1]$ then $E=\bigcup_{\xi\in[0,1]}E_\xi$ and so if we observe that the projection of $E_\xi$ is a $(n-1)$ dimensional simplex then $\int_E 1=\int_0^1(1-x_n)^{n-1}v(E_\xi)\,dx_n=\frac 1 n(1-x_n)^nv(E_\xi)=\frac 1{n!}$ that complete the proof but I don't understand how to prove effectively last equality. So I ask to prove the last equality and then I ask to prove that $h[E]=S$ too. So could someone help me, please?","['volume', 'simplex', 'multivariable-calculus', 'multiple-integral', 'differential-geometry']"
3813436,Hard geometry problem,"This is geometry problem in my textbook for contests, but without a solution. Also my professor can't solve it, so it is quite a challenge: The triangle $ABC$ is given. The points $D$ and $E$ on the line $AB$ are such that $AD = AC$ and $BE = BC$ , with the arrangement $D-A-B-E$ . The circumscribed circles around triangles $DCB$ and $ECA$ intersect at the point $X \neq C$ , and the circumscribed circles around triangles $DEC$ and $ABC$ intersect at $Y \neq C$ . If $DY + EY = 2XY$ is true, determine the $\measuredangle ACB.$ So far, I chased the angles and tried with trigonometry, but soon it becomes too complicated. Textbook suggests that this problem is as hard as third on olympiad. I know fundamental trigonometric theorems(law of sin, law of cosine, etc) and I can understand a lot of solutions for other problems with similar difficulty, so feel free to use whatever you need to solve this. Can someone help? Thanks in advance :)","['contest-math', 'trigonometry', 'geometry']"
3813437,How to solve a differential equation (with an indeterminate form at a point) using Runge-Kutta method?,"I am trying to plot the integral curves corresponding to the following differential equation: where 'u' and 'a' are respectively the velocity of fluid and sound speed respectively. The sonic point (r_s) of the flow is defined as the point where the flow velocity equals the sound speed obtained by setting both the numerator and denominator to zero. In the plot below, the sonic point is the point where the two transonic curves intersect. Now I need to plot the Mach number (M=u/a) as a function of the radial coordinate to obtain the following plot: I tried the following: I tried to solve the differential equation using Runge-Kutta method. Since du/dr is 0/0 form at the sonic point (r_s), we need to use the L'Hospital's rule. But I could not understand how to deal with this. Can anyone please suggest me how to proceed?","['integration', 'ordinary-differential-equations', 'calculus', 'runge-kutta-methods', 'numerical-methods']"
3813442,Definite integral $\int_{0}^{1}\left(\frac{x^{2}-1}{x^{2}+1}\right)\ln\left(\operatorname{arctanh}x\right)dx$,"Prove the following closed form: $$\int_{0}^{1}\left(\frac{x^{2}-1}{x^{2}+1}\right)\ln\left(\operatorname{arctanh}x\right)dx=\ln\pi-\gamma-\left(2-\frac{\pi}{2}\right)\ln2-\pi\ln\left(\frac{\Gamma\left(\frac{3}{4}\right)}{\Gamma\left(\frac{1}{4}\right)}\sqrt{2\pi}\right).$$ I had discovered this while attempting to prove a previous problem when I attempted to integrate both sides of $$\frac{d}{dx}(x^2-1)\arctan(x)\ln(\operatorname{arctanh}(x)) = 2x\arctan(x)\ln(\operatorname{arctanh}(x))+\left(\frac{x^2-1}{x^2+1}\right)\ln(\operatorname{arctanh}(x))-\frac{\arctan(x)}{\operatorname{arctanh}(x)}.$$ That post was nine months ago and I didn't keep a systematic record of my process. I'm sure I could figure it out if I dug through my old work, but now I'm mostly curious to see how others in the community would attack this.","['integration', 'improper-integrals', 'closed-form', 'hyperbolic-functions']"
3813446,Integer solutions to $ 2 (r^2 - r) = t^2 - t $,"I'm trying to find integer positive solutions to the equation: $$
2 (r^2 - r) = t^2 - t
$$ So far I've been giving ""test"" values to t, say $t = 20$ , and then solving the quadratic equation with substituted $t$ . If the resulting value of $r$ is a natural number, then I have a solution. If not, I just try with a different number. I have been able to get some solutions with this method (such as $t = 21, r =15$ and $t = 120, r =85$ ) but it's very repetitive and tedious for larger values. Is there any smarter way to get integer solutions for this equation?","['discrete-mathematics', 'diophantine-equations']"
3813529,A canonical construction proving $\left|HK\right| = \left|H\right|\left|K\right| / \left|H \cap K\right|$?,"Let $H$ and $K$ be subgroups of a finite group $G$ .  A standard
result is the formula $$\left|HK\right| =
\frac{\left|H\right|\left|K\right|}{\left|H \cap K\right|}$$ for
the cardinality of the set $HK$ . The proofs of this that I've seen always involve some arbitrary
choice, such as the choice of a transversal for $\{hK \colon 
h \in H\}$ . Is there a way to show that these quantities are equal without
making such an arbitrary choice?  For example, can they be shown
to be the respective dimensions of a pair of canonically
isomorphic modules?","['universal-property', 'group-theory', 'combinatorics']"
3813554,Can a polynomial have an isolated local minimum at a transcendental point?,"Let $f:\mathbb{R}^n \to \mathbb{R}$ be a polynomial with coefficients in $\mathbb{Q}$ . Is it possible for there to be a point $\textbf{a} \in \mathbb{R}^n$ with all transcendental coordinates such that $\textbf{a}$ is an isolated local minimum of f? By isolated I mean that there exists a neighborhood of $\textbf{a}$ such that for every $\textbf{b}$ in the neighborhood, either $f(\textbf{a}) < f(\textbf{b})$ or $\textbf{a} = \textbf{b}$ . In particular, I am thinking about the case when $f$ is non-negative and $f(\textbf{a}) = 0$ . It seems to me like such a local minimum shouldn't be possible. For $n = 1$ it is not possible. I am pretty sure it is not possible for $n=2$ . For example, if $f = (x - 2y)^2$ , then $f$ has a minimum at $(2\pi,\pi)$ , but it is not isolated. I can not figure out how to prove this for $n>2$ and I can't find any counterexamples. I have tried various ideas from calculus and for the case when $f(\textbf{a}) = 0$ , I have tried to make arguments about the dimension of the variety not being zero. I'm not very familiar with Algebraic Geometry though so maybe this idea doesn't work. Any thoughts would be appreciated.","['multivariable-calculus', 'algebraic-geometry']"
3813564,Supremum of $\sin(x) + \sin(\varphi x)$,"Let $\varphi$ be the golden ratio. What is the supremum of $f(x)=\sin(x) + \sin(\varphi x)$ ? My idea was that we can consider the sequence of functions $f_n(x) = \sin(x) + \sin(\frac{F_{n+1}}{F_n} x)$ for the Fibonacci numbers $F_n$ , which converges to $f(x)$ . The sequence $\{x_n\}_{n \geq 1}=F_n \pi + a_n$ satisfies $f_n(x_n) = \sin(a_n) + \sin(F_n a_n)$ , so that if we choose the correct $a_n$ , then $f_n(x_n) =2$ $\forall n$ ; which changes the problem to one with an integer instead of $\varphi$ , but I'm not sure where to proceed from here.","['maxima-minima', 'trigonometry', 'real-analysis']"
3813600,About the filtering equation (Kushner-Stratonovich),"Let $$dX_t = \mu(X_t,t) dt + \sigma dB_t$$ $$dY_t = h(X_t,Y_t,t) dt + \eta dW_t$$ where $B$ and $W$ are independent standard Brownian motions, $\eta, \sigma$ are positive real numbers. What equation does the law/density of $E[X_t | \sigma(Y_{0 \rightarrow t})]$ satisfy ? This is supposed to be what is called the Kushner-Stratonovich equation, or filtering equation, right ? Depending on where I look, I get different equations, it is very confusing. I found this paper: https://arxiv.org/pdf/1407.6043.pdf But it seems to treat a more general setting and I cannot make sense of it in the above case that interests me. A nice (modern) reference for the above case would be very welcome. The older papers of the ""fathers"" of filtering seem to not consider the case where the drifts depend on time.","['conditional-expectation', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
3813668,Real Analysis objective question.,I don’t know exactly how to solve it but my attempt is as follows $$f’(x)=\sin^4(2g(x))$$ So function is differentiable as composition of two differentiable function is differentiablity . Now it has infinitely many zeroes because of sin(x) has infinity many zero . Please suggest. Thank you .,"['derivatives', 'real-analysis']"
3813683,How to define derivatives in Wasserstein space,"Let $M$ be a Polish space equipped with a metric $d$ . Let $p\geq 1$ . The $p^{th}$ Wasserstein distance between $\mu,\nu \in \mathcal P_p(M)$ (the space of Borel measures on $M$ with finite $p$ moments) is $$
W_{p}(\mu, \nu):=\left(\inf _{\pi \in \Pi(\mu, \nu)} \int_{M \times M} d(x, y)^{p} \mathrm{d} \pi(x, y)\right)^{1 / p}.$$ $(P_p(M), W_{p})$ is a metric space called the $p^{th}$ Wasserstein space. How do we define a derivative of a functional $$F: (P_p(M), W_{p}) \rightarrow \mathbb R ?$$ The Wasserstein space is not a normed vector space, so the Fréchet derivative does not make sense. A particular functional I am interested in is $F(\mu)=W_{p}(\mu, \delta_0)$ . People do study gradient flows in Wasserstein spaces so a rigorous definition must exist. Is this related to metric derivatives ?","['measure-theory', 'metric-spaces', 'functional-analysis', 'derivatives', 'probability-theory']"
3813717,A hint on finding all solutions of $S = \frac{a}{a+b+d} + \frac{b}{a+b+c} + \frac{c}{b+c+d} + \frac{d}{a+c+d}$?,"I've been working at this for awhile but I haven't been able to figure out the right approach. The question is to find all values of $S = \frac{a}{a+b+d} + \frac{b}{a+b+c} + \frac{c}{b+c+d} + \frac{d}{a+c+d}$ for $a,b,c,d > 0$ . Anyone have a hint (no solutions, please) at a more principled way to approach the problem? I've been just throwing stuff at the wall and trying to find something that sticks. What I've tried so far: Plugging in some quick values. Putting $a=b=c=d=1$ yields $S = 4/3$ . Taking $a=c=1, b=d=2$ yields $7/5$ so $4/3$ is not the only answer. Note: if $(a,b,c,d)$ yields $k$ then $(sa,sb,sc,sd)$ yields $k$ for any $s > 0$ . If I fix $a=b=c=1$ then $S = 4/3$ regardless of the value of $d$ . If I fix $a=c, b=d$ , I can write $b = ka$ , which gives $\frac{2}{2k+1} + \frac{2k}{k+2} = S$ . This is a quadratic in $k$ , with positive discriminant for any value of $S$ . But $k$ is not necessarily positive, so I can't claim that all values of $S$ are valid. Getting some quick bounds: $S > \frac{a}{a+b+c+d} + \frac{b}{a+b+c+d} + \frac{c}{a+b+c+d} + \frac{d}{a+b+c+d} = 1$ , and $S < a/a + b/b + c/c + d/d = 4$ , so I know $1 < S < 4$ . The thought is to either try and tighten these bounds somehow (not sure how to approach this) or figure out how to represent an arbitrary $x$ in this range with some choice of $a,b,c,d$ . Some substitutions: Imposing the constraint $a+b+c+d = 1$ or $u = a+b+d, v = a+b+c, w = b+c+d, x = a+c+d$ come to mind, both attempts at removing the sums from the denominator. Respectively those give $S = \frac{a}{1-c} + \frac{b}{1-d} + \frac{c}{1-a} + \frac{d}{1-b}$ and $S = \frac{u+v+w-2x}{3u} + \frac{u+v+w-2x}{3v} + \frac{v+x+w-2u}{3w} + \frac{u+w+x-2v}{3x}$ . Another way of looking at things: $\frac{a}{a+b+d} + \frac{b}{a+b+c} + \frac{c}{b+c+d} + \frac{d}{a+c+d}$ is continuous in $(a,b,c,d)$ . And I think we can get arbitrarily close to 1 by setting $a$ very large, and then $c$ very small relative to $b + d$ . So 1 is probably the $\inf$ of $S$ . If we do the opposite and set $a$ large and $c$ large, we can get close to 2. My suspicion is that $2$ is the $\sup$ but I'm not sure how to prove it.","['contest-math', 'inequality', 'cauchy-schwarz-inequality', 'optimization', 'algebra-precalculus']"
3813767,Prove that $\lim\limits_{t \to \infty} x(t)$ exists and is an integer.,"Let $f : \Bbb R \longrightarrow \Bbb R$ be a $\operatorname {C}^{\infty}$ -function such that $f(x) = 0$ if and only if $x \in \Bbb Z.$ Suppose the function $x : \Bbb R \longrightarrow \Bbb R$ satisfies $x'(t) = f(x(t)),$ for all $t \in \Bbb R.$ If $\Bbb Z \cap \{x(t)\ |\ t \in \Bbb R \} = \varnothing,$ then show that $\lim\limits_{t \to \infty}  x(t)$ exists and is an integer. What I have seen is that $x$ is also a $\operatorname {C}^{\infty}$ - function and since image of $x$ doesn't contain any integer so by IVP it follows that $$\{x(t)\ |\ t \in \Bbb R \} \subseteq (a,b)$$ where $a, b \in \Bbb R$ with $b-a \leq 1$ such that $(a, b) \cap \Bbb Z = \varnothing.$ In other words the image of $x$ is strictly lying between two consecutive integers. Hence $x$ is bounded and also by the definition of $x'$ and $f$ it follows that $x'(t) \neq 0,$ for all $t \in \Bbb R.$ Now how do I proceed? Any help will be highly appreciated. Thanks in advance.","['limits', 'proof-writing', 'ordinary-differential-equations', 'real-analysis']"
3813768,Convergence of all mixed moments implies convergence in distribution?,"Let $\mathbf X=(X_1,\dotsc,X_n)$ be a vector of random variables. For $\mathbf a=(a_1,\dotsc,a_n)$ , a vector of non-negative integers, let $\mathbf X^{\mathbf a}$ denote the monomial $X_1^{a_1}\dotsb X_n^{a_n}$ . Assume that $\mathbf E(\mathbf X^{\mathbf a})<\infty$ for every vector $\mathbf a$ of non-negative integers. Assume further, that the joint probability distribution of $\mathbf X$ is completely determined by the set of mixed moments $\mathbf E(\mathbf X^{\mathbf a})$ as $\mathbf a$ runs over all vectors of non-negative integers. Now suppose $\{\mathbf X^{(k)}\}$ is a sequence of vectors of random variables, so that $\mathbf X^{(k)}=(X^{(k)}_1,\dotsc, X^{(k)}_n)$ for each $k$ . Suppose that, for every vector $\mathbf a$ of non-negative integers, $$
\lim_{k\to \infty} \mathbf E(\mathbf X^{(k)\mathbf a})=\mathbf E(\mathbf X^{\mathbf a}).
$$ I believe it should follow that $\mathbf X^{(k)}$ converges to $\mathbf X$ in distribution. This would be a multivariate analogue of the method of moments in probability theory. Is it true? Is there a good reference to cite for it?","['probability-limit-theorems', 'probability-theory']"
3813827,Clarification on pigeonhole principle for case of choosing $k$ elements from a set such that $2$ elements from the subset sum to a particular number,"I have $20$ cards, each labelled $1$ to $20$ , and I would like to find the minimum number of cards I have to select such that there will always be $2$ cards in the subset of cards selected, that add up to $21$ . The method I used to solve this question (which I am not sure of) is as follows: First I list down the possible unique pairs of cards that add up to $21$ , which are $\{1, 20\}, \{2, 19\}, \{3, 18\}, \{4, 17\}, \{5, 16\}, \{6, 15\}, \{7, 14\}, \{8, 13\}, \{9, 12\} ,\{10, 11\}$ and there are $10$ unique pairs. Then I applied the pigeonhole principle which states that for $kn+1$ pigeons, to be distributed into $n$ holes, there must be at least $k+1$ pigeons in each hole. In this case, I let the pairs be the ""holes"" and the cards be the ""pigeons"", and solved for $k$ to get $k=$${19}\over{10}$ , and therefore, at least $3$ cards must be selected for there to always have $2$ cards that have a sum of $21$ . I am not sure if my solution is correct, and if I am using the pigeon hole principle correctly, and I think the selection of what to use for the ""holes"" is wrong. Can someone explain what is the right way to go about selecting the ""holes"" for this problem? If I am correct, I am not sure why I am correct and why the selection of the ""holes"" as the pairs is correct. Can someone explain to me?","['pigeonhole-principle', 'solution-verification', 'combinatorics']"
3813832,Evaluating $\int_{-1/2}^{1} \cos^{-1} \frac{1-x^2}{1+x^2} dx$,Let us do the integration by parts $$I=\int_{-1/2}^{1} ~\cos^{-1} \frac{1-x^2}{1+x^2}. 1~ dx =\left.x \cos^{-1} \frac{1-x^2}{1+x^2}\right|_{-1/2}^{1}-\int_{-1/2}^{1} \frac{2x}{1+x^2} dx$$ $$=\frac{\pi}{2}+\frac{1}{2}\cos^{-1} \frac{3}{5}-\left .\ln(1+x^2)\right|_{-1/2}^{1}=\frac{\pi}{2}+\frac{1}{2}\cos^{-1} \frac{3}{5}-\ln\frac{8}{5}.$$ The question is whether something is amiss here and whether the answer is right. EDit The correct answer is $$I=\frac{\pi}{2}+\frac{1}{2}\cos^{-1} \frac{3}{5}-\ln\frac{5}{2}.$$,"['integration', 'definite-integrals', 'derivatives']"
3813844,Differential in terms of $T_pX \simeq \left(\mathfrak{m}_p/ \mathfrak{m}_p^2\right)^\ast$,"I am studying different definitions of the tangent space to a manifold $X$ in a point. When we identify $T_pX \simeq \left(\mathfrak{m}_p/ \mathfrak{m}_p^2\right)^\ast$ , how can we express the differential $dF_p$ of a differentiable map $F:(X, p) \to (Y, q)$ ? The ""standard"" definition, seeing $T_pX$ as a space of derivations of $\mathcal{C}^\infty_X(p)$ , would be $dF_p(\delta)=\delta F^\ast_p$ for every $\delta \in T_pX$ . My idea is that if we call $\alpha_q=\alpha: T_qY \to  \left(\mathfrak{m}_q/ \mathfrak{m}_q^2\right)^\ast$ and $\beta_p=\beta: \left(\mathfrak{m}_p/ \mathfrak{m}_p^2\right)^\ast \to T_pX$ the two isomorphism defined by $$\alpha(\delta)(f+ \mathfrak{m}_q^2)=\delta(f), \quad \beta(\omega)(g)=\omega(g-g(p)+\mathfrak{m}_p^2) \quad \forall \delta \in T_qY, f \in \mathfrak {m}_q, g \in \mathcal{C}^\infty_X(p),\omega \in \left(\mathfrak{m}_p/ \mathfrak{m}_p^2\right)^\ast, $$ then the differential $ \left(\mathfrak{m}_p/ \mathfrak{m}_p^2\right)^\ast \to \left(\mathfrak{m}_q/ \mathfrak{m}_q^2\right)^\ast$ should be $\alpha \circ dF_p \circ \beta$ , that is, the bottom row of the following diagram: $\require{AMScd}$ \begin{CD}
{T_pX} @>{dF_p}>> T_qY\\
@A{\beta_p}AA @VV{\alpha_q}V\\
\left(\mathfrak{m}_p/ \mathfrak{m}_p^2\right)^\ast @>{??}>> \left(\mathfrak{m}_q/ \mathfrak{m}_q^2\right)^\ast
\end{CD} However I find difficulties finding an explicit expression.","['tangent-spaces', 'differential-geometry']"
3813882,Find the ideal corresponding to an affine variety,"Suppose $X \subseteq \mathbb{A}^2$ is defined by the equations $f: x^2 + y^2 =1$ and $g: x= 1$ . Find the ideal $I_X$ of all the regular functions that vanish on $X$ . Is it true that $I_X= (f,g)$ ? My attempt : The only common solution of the system composed by $f=0$ and $g=0$ is the point $(1,0)$ and $I_X = \left\{ F \in k[x,y] : F(1,0) =0 \right\}$ . So $I_X \neq (f,g)$ , because $x^2 - 1 \notin (f,g)$ . Is it correct? Is there a more explicit expression of the ideal $I_X$ ?",['algebraic-geometry']
3813911,Prove that a parabolic problem is well posed (Lax-Milgram lemma appilication),"I need a check on the following problem, which is an application of the Lax-Milgram lemma. $$
\begin{cases}
\dfrac{\partial u}{\partial t} -\dfrac{\partial }{\partial x} \Bigl( \alpha \dfrac{\partial u}{\partial x}\Bigr) - \beta u =0 \\
u(x,0)=u_0(x), & x \in [0,1] \\
u = \eta, & x=0,\; t>0 \\
\alpha \dfrac{\partial u}{\partial x} + \gamma u =0, & x=1,\; t>0
\end{cases}
$$ where $\alpha(x), u_0(x)$ are given functions, $\beta >0$ and $\eta,\gamma \in \mathbb{R}$ . Question. Prove existence and uniqueness of the weak solution, giving some suitable assumptions on $\alpha(x)$ , $\gamma, \eta$ . Here's my attempt: As test functions, I choose $v \in V=H_{\Gamma_d}^1$ , where $\Gamma_d={0}$ ,i.e. I use tes functions which are $0$ where I have the Dirichlet data. By multiplying, with standard arguments one obtains the following bilinear form $$a(u,v)=\gamma u(1)v(1) + \int_0^1 \alpha u' v' dx - \beta \int_0^1 u v dx$$ To show existence and uniqueness, I need to show $a(v,v) + \lambda \geq \alpha \|v\|_V^2$ (weakly coercive) $a(u,v) \leq M \|u\|_V \|v\|_V$ Weakly coercive :
Assume $0<\alpha_0 < \alpha(x) < \alpha_1$ $$a(v,v) + \beta \|v\|_V^2 \geq \frac{\alpha_0}{1+C_p^2} \|v\|_V^2 + \gamma v(1)^2$$ where $C_p$ is the Poincarè constant. If $\gamma >0$ , then: $$a(v,v) + \beta \|v\|_V^2 \geq \frac{\alpha_0}{1+C_p^2} \|v\|_V^2$$ i.e. $a(\cdot, \cdot)$ is weakly coercive. Continuity : $$|a(u,v)| \leq \alpha_1 \|u\|_V \|v\|_V + \beta \|u\|_V \|v\|_V + \gamma u(1)v(1)$$ Now, since I want a bound with the $H^1$ norm, I note that $$|u(1)| \leq \int_0^1 |u'(s)|ds + \eta$$ and therefore $$|u(1)| \leq \|u\|_V + \eta$$ Also, $v(1) = \int_0^1 v'(s)ds$ implies $|v(1)| \leq \|v\|_V$ Hence, $$|a(u,v)| \leq (\alpha_1 + \beta) \|u\|_V \|v\|_V+\gamma \|u\|_V \|v\|_V + \gamma \eta \|v\|_V $$ Now, if $\eta <0$ , then $$|a(u,v)| \leq (\alpha_1+\beta+\gamma)\|u\|_V \|v\|_V$$","['solution-verification', 'sobolev-spaces', 'functional-analysis', 'partial-differential-equations', 'weak-derivatives']"
3813927,"Set of branch points isn't discrete, but branch points are isolated?","I refer to Chapter II.4 of Rick Miranda - Algebraic curves and Riemann surfaces , which I understand says that the branch points of a nonconstant holomorphic map $F: X \to Y$ between Riemann surfaces $X$ and $Y$ , both of which are not necessarily compact, form a discrete subset of range $Y$ of the map $F$ . (Not sure if 'connected' is part of the definition of Riemann surfaces in other textbooks, but it is here.) Question 1 : Do I understand correctly, and is this indeed true? What else I understand: $A$ is a discrete subspace/subset (I just assume everything here is the usual subspace topology) of $B$ if and only if every singleton subset of $A$ is open in $A$ if and only if every point of $A$ is isolated. This overflow question ( Basic question about branch points on Riemann surfaces ) seems to suggest this is not true but is true if $F$ is proper. However, I'm not sure the definitions of Riemann surface are the same (maybe some answers use definitions where Riemann surfaces are not necessarily connected or something). 2.1. I know $F$ is 'discrete' (meaning that its fibres are discrete; here, i refer to a different text ): see definition before Lemma 3.1, on p. 7), by Proposition II.3.12, and open, by open mapping theorem, which is Proposition II.3.8, but I'm not sure these help show $F$ is proper or are otherwise helpful. Ostensibly 'discrete' maps don't map discrete subsets to discrete subsets. This stackexchange question ( Are branch points always isolated? ) seems to suggest this is true. I think this is true if $F$ is injective or at least something like locally injective. (Please don't make me type up all the stuff I did for this part.) However based on the question in (3) and based on the '3.2. Branch points' on p. 7 in text by Armin Rainer in (2.1) , which I think uses the term 'branch points' to mean the same thing as what Miranda means by 'ramification points' (and thus is different from Miranda's 'branch points'), it seems that no neighbourhood $U$ of a ramification point $p \in U \subseteq X$ of $F$ is such that the restriction $F|_U$ is injective. Kinda related: If $F$ has a ramification point, then is $F$ necessarily not injective? This page ( rigtriv: Hurwitz’s Theorem ) says Now, the ramification and branch points must form a discrete set I don't think the page assumes anything like $F$ is proper. The Armin Rainer text in (2.1) actually seems to assume proper based on the '3.7. Proper holomorphic maps' part, but said part refers to 'Lemma 3.17', so I think proper can be replaced with closed. Maybe $F$ is closed or something. Kinda related (see Question 2 below): When closed and continuous maps discrete to discrete S. K. Donaldson - Riemann Surfaces says If $F$ is proper then the image $\Delta = F(R)$ is discrete in $Y$ . This seems to have all the same assumptions as with Miranda. Ostensibly, either Miranda is wrong or there's some higher level machinery that allows us to omit $F$ proper. Question 2 : To possibly generalise this, what are the minimum requirements on $F, X$ and $Y$ to make $F$ map discrete subsets to discrete subsets? I mean, I'm not sure we really need 'holomorphic' here. I have a feeling this applies perhaps to maps that are just open/closed, continuous and discrete and spaces that are just Hausdorff and locally compact or something. (again) Kinda related: When closed and continuous maps discrete to discrete","['riemann-surfaces', 'branch-points', 'complex-geometry', 'complex-analysis', 'complex-manifolds']"
3813953,Intuitive explanation for a constant answer in a Bayes theorem question,"Question (previously asked here ) You know there are 3 boys and an unknown number of girls in a nursery at a hospital. Then a woman gives birth a baby, but you do not know its gender, and it is placed in the nursery. Then a nurse comes in a picks up a baby and it is a boy. Given that the nurse picks up a boy, what is the probability that the woman gave birth to a boy? Assume that - in this question's universe - the unconditional probabiilty that any newly born baby is a boy or a girl is exactly half. Short solution Let number of girls be $k$ . Event A is the newborn is a boy, Event B is that nurse picks up a boy. So, we are asked $P(A|B)$ . $$P(A|B) = \frac{P(B|A)P(A)}{P(B)} = \frac{\frac 4{k+4}\frac 12}{\frac 4{k+4}\frac 12 + \frac 3{k+4}\frac 12} = \frac 47$$ My question Why is the probability constant? I would have expected the probability to change with respect to the number of girls. More specifically, I would have expected the probability to increase as the value of $k$ increases, and decrease if $k$ was less. Why so? Because we are already given the claim that we have selected a boy. If we have infinite girls, then the newborn has to almost surely be a boy to help support that observed claim. Because initially there are only three boys, the more help they could get in supporting the claim, the better. Of course, this is not a very rigorous argument, but the point here is that in many such questions there is a natural expectation for the probability to vary with the variable. And it does do in many, say for example the generalized monty hall problem . I do know that technically the $k$ does not matter because it gets cancelled out in the denominator, but intuitively that is not a very helpful explanation. Can anyone give an intuitive explanation for why the probability answer in this question is a constant?","['bayes-theorem', 'probability']"
3814093,"If $R$ is a total order over set $A$, then all subsets of $A$ can be sorted","I'm working on the proof of this theorem: Let A be a set and let $\leq_A$ be a partial order over 𝐴. We say that a sequence $x_1,...,x_n$ is sorted if $x_1 \leq_A x_2 \leq_A ... \leq_A x_n$ . Prove that any subset of $n$ elements of $A$ can be sorted iff $A$ is a total order. Since this is bidirectional, I need to prove both directions of implication. I've actually already proved the first direction (I asked a question about this) - if any subset of $A$ can be sorted, then $\leq_A$ is total. Now I'm trying to prove the second direction, namely if $\leq_A$ is total, then any subset of $A$ can be sorted. I tried different techniques (direct, contradiction, contrapositive) and the best results I achieved was when I tried to prove by induction, but I can't complete the proof. Here is it: Assume that $P(n)$ is true iff any set $S \subseteq A$ with $n$ elements can be sorted. Then $P(0)$ is true, since all empty sets are sorted. So assuming that for any $n \in \mathbb{N}$ if $P(n)$ is true, then we prove that $P(n + 1)$ is also true. Consider any set $S \subseteq A$ with exactly $n$ elements and consider element $y \in A$ which is also $y \not\in S$ . Since $\leq_A$ is total order we have that $y$ is either the least element in $S \cup \{y\}$ , or the greatest element in $S \cup \{y\}$ , or there are some $x_l \in S$ , where $x_l \leq_A y$ , and there are some $x_g \in S$ , where $y \leq_A x_g$ . In the first and the second cases we are done, because $S \cup \{y\}$ is sorted... I don't actually know how to prove that the set is also sorted in the third case. Moreover I'm not even sure that my assumptions about the first and the second cases are correct. Could someone please provide any hints and suggestions for where to go next? Or if you know another way (simplier/without induction) to prove this, please give me a hint.","['elementary-set-theory', 'order-theory', 'solution-verification', 'alternative-proof']"
3814107,What is the packing number of the unit cube?,"The $\varepsilon$ - packing number of the unit cube $[0,1]^d$ with respect to the infinity norm is the biggest number of $\varepsilon$ -strictly-separated points, i.e., the biggest cardinality of a set of points $E \subset [0,1]^d$ such that each two distinct $x,y\in E$ satisfy $\|x-y\|_\infty > \varepsilon$ . It seems geometrically intuitive to me that this is at most $\left( \bigl\lfloor \frac{1}{\varepsilon}\bigr\rfloor + 1 \right)^d$ , where $\lfloor x \rfloor$ is the biggest integer lower or equal to $x$ (I am very naively taking a uniform grid of $(\varepsilon + \varepsilon')$ -spaced points, for a small $\varepsilon'$ ). Is this true? If so, how is it proven? I can only found bounds like $(2/\varepsilon + 1)^d$ that upper bound the packing number with the related notion of covering number and proceed to find a covering.","['statistics', 'geometry', 'packing-problem']"
3814110,Calculating $n$-th power of a matrix,"I was doing an exercise of a past exam in which one of the things I had to do was calculating the $n$ th power of a Jordan matrix $$J=\begin{pmatrix}
2 & 1 & 0 \\
0 & 2 & 1 \\
0 & 0 & 2
\end{pmatrix}.$$ I started calculating until the 5th power but I couldn't guess the expression for the $a_{1,2}$ , $a_{1,3}$ and $a_{2,3}$ components. When I looked it up in an online calculator it showed that: $$J^n=\begin{pmatrix}
2^n & \frac{2^n·n}{2} & \frac{2^n·(n^2-n)}{8} \\
0 & 2^n & \frac{2^n·n}{2} \\
0 & 0 & 2^n
\end{pmatrix}.$$ My question is: when the relationships are as difficult as these　(especially the $a_{1,3}$ component), are there any tricks for figuring out the $n$ th power component? Because these kind of relationships are difficult to think in the middle of an exam.",['linear-algebra']
3814133,Ramanujan series Type $\sum _{k=1}^{\infty } \frac{\sinh (2 \pi k)}{2 \sqrt{2} \pi ^9 k^{11} (1-\cosh (2 \pi k))}$,"In Ramanujan lost notebook I see series like $$\sum _{k=1}^{\infty } \frac{343 \sinh \left(\sqrt{2} \pi  k\right)}{32 \pi ^{11} k^{13} \left(\cos \left(\sqrt{2} \pi  k\right)-\cosh \left(\sqrt{2} \pi  k\right)\right)}=\frac{721 (-1)^{3/4} \pi ^2}{277992000}-\frac{721 \sqrt[4]{-1} \pi ^2}{277992000}$$ Mathematica verifies its correctness numerically, but other series cannot be dealt with Mathematica, for example: $$\sum _{k=1}^{\infty } \frac{\sinh (2 \pi  k)}{2 \sqrt{2} \pi ^9 k^{11} (1-\cosh (2 \pi  k))}=-\frac{1453 \pi ^2}{851350500 \sqrt{2}}$$ How can we prove this identity? Thank for helping.","['closed-form', 'sequences-and-series']"
3814199,Which (non-trace class) operators have a well-defined trace (if any)?,"On a Hilbert space $\mathcal{H}$ , it is well known that trace class operators have a finite trace. However, there are operators which are not trace class but have a finite trace, e.g. the unilateral shift . Then, consider a (bounded) operator $A$ such that $$
c=\sum_{n=1}^{\infty}\langle x_n,A x_n\rangle<\infty,
$$ for some orthonormal basis $\{x_1,x_2,\ldots\}$ . I'm not sure whether this is enough for the trace of $A$ to be defined in a basis independent way. Namely, $\mathrm{tr}(A)=\sum_{n=1}^{\infty}\langle y_n,A y_n\rangle=c$ for any orthonormal basis $\{y_1,y_2,\ldots\}$ . In the classics von Neumann's book on Mathematical Foundations of Quantum Mecanics, Sec. II.11, he commented this is true (in fact, he did not consider trace class operators) using essentially the following argument: $$
c=\sum_{n=1}^{\infty}\langle x_n,A x_n\rangle=\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}\langle x_n,y_m\rangle \langle y_m, A x_n\rangle=\sum_{m=1}^{\infty}\sum_{n=1}^{\infty}\langle y_m, A x_n\rangle\langle x_n,y_m\rangle=\sum_{m=1}^{\infty}\langle y_m,A y_m\rangle.
$$ However, it is not clear to me that the double sum can be switched. Using that $|\langle u,y_m \rangle \langle y_m, v\rangle|\leq \frac12 (|\langle u,y_m \rangle|^2+|\langle v,y_m \rangle|^2)$ it is obtained that $$
\sum_{m=1}^{\infty}\langle x_n,y_n\rangle \langle y_n, A x_n\rangle
$$ is absolutely convergent. Is this enough for the summation switch? Thank you in advance. EDIT : As Robert Israel points, $\sum_{n=1}^{\infty}\langle x_n,A x_n\rangle$ must be absolutely convergent. Is it enough?","['operator-theory', 'trace', 'functional-analysis']"
3814200,"$f$ is continuous on $[0,\infty)$, differentiable $(0,\infty)$, $f(0)\geq0$ and $f'(x)-f(x)\geq0$. Prove that $f(x)\geq0$","This question was asked in my real analysis quiz and I was unable to solve it. It has 2 parts and unfortunately, I couldn't solve both. Question:(a) Suppose $f$ is continuous on $[0,\infty) $ , differentiable on $(0,\infty)$ and $f(0)\geq0$ . Suppose $f'(x) \geq f(x)$ for all $x \in (0,\infty)$ . Show that $f(x)\geq 0$ for all $x \in (0,\infty)$ . (b) Let $f$ and $g$ are continuous functions on $[0,1]$ satisfying $f(x)\geq g(x)$ for every $0\leq x \leq 1$ , and if $\int_0^{1} f(x) dx =\int_0^{1} g(x) dx $ then show that $f=g$ . Attempt: In (a) I thought of integrating $f'(x)\geq f(x)$ from $0$ to $x$ with variable t to get $f(x) \geq \int_{0}^{x} f(t) dt  +f(0)$ . But I dont know how to proceed from here. (b) Continuous functions are integrable so $\int_0^{1} f(t) dt \geq \int_0^{1} g(t) dt  > I$ tried by thinking of teaking sets $A =\{x : f(x)>g(x)\}$ and $B =\{ x: f(x)<g(x) \}$ but could not  proceed . Kindly Help !!","['derivatives', 'real-analysis']"
3814215,PDF and CDF of the ratio of the max to min of an iid random sample: a quick check of the calculation!,"My question can be thought as a direct continuation of this important question and its first answer (but see this question also), which gives us the joint distribution of the max and min of an iid random sample. In my question, I just wanted to make sure if this formula for the ratio  of maximum to minimum of an iid random sample is correct, given that first answer to the abovementioned question, which is indeed correct. Proposition: Let $\{N_1 \dots N_n\}$ be iid random sample generated by a positive random variable $N > 0$ with CDF $F_N$ and PDF $f_N.$ Let $U_n, L_n$ denote the maximum and minimum of the random sample. Let us denote the PDF of their joint distribution by $f_{U_n, L_n}$ and  that of the ratio by $f_{\frac{U_n}{L_n}}. $ Then we have the following expression of the PDF of the ratio: $$  f_{\frac{U_n}{L_n}}(z) =  n(n-1)\int_{s=0}^{\infty}sf_N(s)f_N(sz)(F_N(sz)-F_N(s))^{n-2}, z \in [1, \infty) $$ Note that: $\frac{U_n}{L_n} \ge 1.$ So let $ z \ge  1.$ Then: \begin{align*}
	\\& f_{\frac{U_n}{L_n}}(z)
	\\& = \frac{d}{dz} P[\frac{U_n}{L_n} \le z]
	\\&= \frac{d}{dz} P[{U_n} \le z L_n]
	\\&= \frac{d}{dz} \int_{ \{ t \le z s\} } f_{L_n, U_n}(s,t) ds dt
	\\&= \frac{d}{dz} \int_{s=0}^{\infty} \left[  \int_{t=0}^{zs} f_{L_n, U_n}(s,t) ds  \right]  dt
	\\&=  \int_{s=0}^{\infty} \frac{d}{dz} \left[  \int_{t=0}^{zs} f_{L_n, U_n}(s,t)  ds \right]  dt
	\\&=  \int_{s=0}^{\infty} s  f_{L_n, U_n}(s,sz) ds
	\\&= n(n-1) \int_{s=0}^{\infty} s f_N(s)f_N(sz)(F_N(sz)-F_N(s))^{n-2} ds
    \\& \text{ (The next step is only for Uniform distributions) }
    \\&= n(n-1) \int_{s=0}^{1/z} s f_N(s)f_N(sz)(F_N(sz)-F_N(s))^{n-2} ds (\text{ because when } sz > 1, f_N(sz)=0. )
	\end{align*} Note that in the second to last line of the above, proof, I indeed used the general ( not for uniform distribution) expression for $f_{L_n, U_n}$ provided by the first answer to the question I mentioned in the very first line. In the last line, we specialized for uniform distributions. EDIT: After a few first comments, I'm including a check for $n=2, N \sim \mathcal{U}(0,1)$ below: Plug in $n=2, $ and assume $N \sim U(0,1),$ then the last line of my calculation becomes $2 \int_{0}^{1/z}sds= 1/z^2,$ which is the PDF of the inverse uniform distribution . Now note that, in this question , they treat the distribution of $L_2/U_2$ and obtain it as a uniform distribution $\mathcal{U}(0,1),$ implying that at least in this simple case, my calculations are correct. Indeed a similar calculation also shows that when $X_i \sim_{i.i.d.} \mathcal{U}(0,1), U_n/L_n$ has PDF $f_{U_n/L_n}(z)= \frac{n-1}{z^n}, z \geq 1.$","['probability-distributions', 'probability', 'density-function']"
3814219,Prove that these $3$ functions are constant given a relation is satisfied by divisors of them,"This particular question was asked in masters exam for which I am preparing and I am not able to solve it . Question : suppose $f,g,h$ are functions from the set of positive real numbers into itself  satisfying $f(x)g(y)=h((x^2 +y^2)^{1/2})$ for all $x,y \in(0,\infty)$ . Then show that the three functions $\frac{f(x)}{g(x)} ,\frac{g(x)}{h(x)}$ and $\frac{h(x)}{f(x)}$ are all constant . Attempt : I first tried by putting $y=0$ only to realize that $0$ is not in domain of $g(y)$ . I am unable to think about any other approach . I am sorry but cant give any other thing in attempt . Kindly tell how should I approach this question .","['analysis', 'real-analysis']"
3814233,Volume of $(x^2 y^2) + (x^2 z^2) + (y^2 z^2) = 1$,"I've long been intrigued by this surface which tightly hugs each axis, extending to infinity but with finite volume. But integrating this formula is beyond my powers. Any suggestions on how to do this, or demonstrations of the full answer? I suspect spherical coordinates might help, but that seems to make the formulas even more complex and beyond my capacity. An upper bound on volume: Consider the surface near the x-axis only, for x > 1. The cross-section around the axis should be within the circle $y^2 + z^2 = 1/x^2$ [Gabriel's Horn, as David K helpfully pointed out below], and yet near it, since the dropped term, $y^2 z^2 / x^2$ , is << $1/x^2$ . This circle has area $\pi/x^4$ ; integrating from 1 to infinity gives an upper bound of volume around all axes of $6\pi$ plus the area within 1 unit of the origin (max. 8 there, of course), for a total <27. The graph of the cross-section still looks nearly circular at x=1, so I'm expecting a more exact answer might be in the 24-26 range. Slight progress: the smallest cube enclosed by the surface has vertices at x=y=z = $1/\sqrt[4]3$ , so its volume is $8/\sqrt[4]{27}$ . The six arms fully separate at axis values >1. In between these values one would have to integrate a complex cross section of squares with rounded corners--the square sides separating the parts closest to each axis to get a partial volume, then multiplying by 6 just as for each arm segment to get a total volume. But both this and the infinite part of the arm requires the anti-differential of $\sqrt{((1-x^2y^2)/(x^2+y^2))}$ . Latest update: I found that wolfram alpha has a double integral calculator widget; it doesn't show an anti-derivative for the above function of z, but presumably did a numerical integration and delivered the volume for the curve (x: 0-infinity; y: 0-1/x) of 3.24099; multiplying by 8 gives 25.928, in the ballpark of my earlier estimate. It would still be interesting to know if it has some more exact formulation (even if this involves square roots or other complex terms).","['multivariable-calculus', 'spherical-coordinates']"
3814249,How to show that Airy Function in Integral form obeys $y''=xy$?,"I wanna show that having Airy function defined as: $$ \mathrm {Ai} (x)={\dfrac {1}{\pi }}\int _{0}^{\infty }\cos \left({\dfrac {t^{3}}{3}}+xt\right)dt $$ Solves equation: $$y''=xy.$$ Edit: After clearing out that $k^3$ is not $x^3$ , which i have misread:
I am stuck at the integral: $$  \int_0^\infty (k^2+x)\cos(kx+k^3) $$ which is clearly not converging but some how wiki says it right solution?","['improper-integrals', 'ordinary-differential-equations', 'airy-functions']"
3814256,Limit of a fraction at infinity when all terms go to zero,"Suppose $Y = \sum_{j = 1}^{(n-1)!} \prod_{i = 1}^{n-1} y_{ij}$ and $Z = \sum_{j = 1}^{(n-1)!} \prod_{i = 1}^{n-1} z_{ij}$ . Also suppose $0 < x < 1$ , $-1 < y_{ij} < 1$ , and $-1 < z_{ij} < 1$ for all $i$ and $j$ , and $x \geq |y_{ij}|$ and $x \geq |z_{ij}|$ for all $i$ and $j$ , but it is such that each product has at least one $|y_{ij}| < x$ (and $|z_{ij}| < x$ ). I believe the following limit is true. $
\lim_{n \rightarrow \infty}\frac{x^{n-1} + Y}{x^{n-1} + Y + Z} = 1
$ Loosely speaking, I believe the reason is because all other terms converge to zero faster than $x^{n-1}$ , so we are left with $
\lim_{n \rightarrow \infty}\frac{x^{n-1} + Y}{x^{n-1} + Y + Z} = \frac{x^{n-1} + 0}{x^{n-1} + 0 + 0} = 1
$ . How would one formally prove this? Edit: Suppose, instead, all the $y_{ij}$ and $z_{ij}$ are weakly positive and $Y$ and $Z$ are such that $Y = \sum_{j = 1}^{n!}(-1)^j \prod_{i = 1}^{n} y_{ij}$ and $Z = \sum_{j = 1}^{n!} (-1)^j \prod_{i = 1}^{n} z_{ij}$ . Then is the above limit equal to $1$ ? If so, how could one show this?","['limits', 'calculus', 'convergence-divergence', 'real-analysis']"
3814259,Can this monstrous expression be simplified?,"$\sqrt{\left( r_d \cos\left(\frac{-4(C - X) \csc⁡(2α)}{Z}\right) + r_p \left(\frac{2C \tan⁡(α) - 4 (C - X) \csc⁡(2α))}{Z}\right) \sin\left(\frac{-4 (C - X) csc⁡(2α)}{Z}\right) - m X \tan⁡(α) \sin\left(\frac{-4 (C - X) \csc⁡(2α)}{Z}\right)\right)^2 + \left( r_d \sin\left(\frac{4(C - X) \csc⁡(2α)}{Z}\right) - r_p \left(\frac{2C \tan⁡(α) - 4 (C - X) \csc⁡(2α))}{Z}\right) \cos\left(\frac{-4 (C - X) csc⁡(2α)}{Z}\right) + m X \tan⁡(α) \cos\left(\frac{-4 (C - X) \csc⁡(2α)}{Z}\right)\right)^2}$ $r_d = r_p + m X - m C$ $r_p = \frac {m Z}{2}$ $m$ is positive $α$ is between $0$ and $\frac π 4$ $Z$ is a positive integer $X$ is between -1 and +1 $C$ is between 1 and 1.5 I've been staring at this until I'm cross-eyed, but I can't find any way to break it down. It's not for a class, so I don't have any resources to call upon. Context: I'm trying to find the radius of a point on the curve defined by the parametric expressions $$x = r_d \cos(γ) + r_p \left(\frac{2C \tan(α)} Z + γ\right) \sin(γ) - m X \tan(α) \sin(γ),$$ $$y = r_d \sin(γ) - r_p \left(\frac{2C \tan(α)} Z + γ\right) \cos(γ) + m X \tan(α) \cos(γ)$$ Specifically, the point at $γ = \frac{-4(C - X) \csc⁡(2α)}{Z}$ . My instinct for solving that was to plug in the value and use the Pythagorean theorem, which created the expression that is the subject of this question. If there's a better way to find this radius, I would love to try it out. Update: Looking to simplify the base expressions, I can expand the instances of $r_d$ and $r_p$ and then factor out the $m$ from all three terms, and I can factor out the $\sin$ and $\cos$ from the last two terms, but I can't see how to use that to any advantage...","['algebra-precalculus', 'trigonometry']"
3814283,Special case ($3\times 3$ and $4\times 4$) of USAMO 1998 problem #$4$,"This is a special case ( $3\times 3$ and $4\times 4$ ) of USAMO 1998 problem  #4. A $98\times 98$ chess board has the squares colored alternately black and white in the usual way. A move consists of selecting a rectangular subset of the squares (with boundary parallel to the sides of the board) and switching their colors. What is the smallest number of moves required to make all the squares black? I am trying to obtain optimal moves for $3\times 3$ and $4\times 4$ cases. (I couldn't understand the answers of linked post) I read somewhere that the optimal moves for first one is 2 and for second case is 4. But I tried almost all ways without reaching these numbers. For example for $3\times 3$ one possible solution is as follows: $$\begin{bmatrix}
\color{red}{1} & 0&1\\
\color{red}{0}&1&0\\
\color{red}{1} & 0&1
\end{bmatrix}\implies \begin{bmatrix}
0 & 0&\color{red}{1}\\
1&1&\color{red}{0}\\
0 & 0&\color{red}{1}
\end{bmatrix}\implies \begin{bmatrix}
0 & 0&0\\
\color{red}{1}&\color{red}{1}&\color{red}{1}\\
0 & 0&0
\end{bmatrix}\implies \begin{bmatrix}
0 & 0&0\\
0&0&0\\
0 & 0&0
\end{bmatrix}$$ which is 3 moves and not 2. What is the best way with 2 moves? and the $4\times 4$ case one possible solution is as follows which is 5 moves: $$\begin{bmatrix}
\color{red}{1} & 0&1&0\\
\color{red}{0}&1&0&1\\
\color{red}{1} & 0&1&0\\
\color{red}{0}&1&0&1
\end{bmatrix}\implies \begin{bmatrix}
0 & 0&\color{red}{1}&0\\
1&1&\color{red}{0}&1\\
0 & 0&\color{red}{1}&0\\
1&1&\color{red}{0}&1
\end{bmatrix}\implies \begin{bmatrix}
0 & 0&0&\color{red}{1}\\
1&1&1&\color{red}{0}\\
0 & 0&0&\color{red}{1}\\
1&1&1&\color{red}{0}
\end{bmatrix}\implies \begin{bmatrix}
0 & 0&0&0\\
\color{blue}{1}&\color{blue}{1}&\color{blue}{1}&\color{blue}{1}\\
0 & 0&0&0\\
\color{red}{1}&\color{red}{1}&\color{red}{1}&\color{red}{1}
\end{bmatrix}\stackrel{2 rows=2times}{\implies}\begin{bmatrix}
0 & 0&0&0\\
0&0&0&0\\
0 & 0&0&0\\
0 & 0&0&0
\end{bmatrix}$$","['contest-math', 'combinatorics', 'discrete-mathematics']"
3814294,Is it true that $|ab| = |a||b|$ in every group?,"I want to show that the order of $ab$ is the same as the product of orders of $a$ and $b.$ So at title, $|ab| = |a||b|.$ Here $|a|$ means the order of an element $a$ in a group $G$ . So $a^n = e$ if the order of $a$ is n, and $e$ is an identity of a group. I think it's false. Let $G$ be a group of $\Bbb Z$ mod $4$ under addition modulo $n$ . The $G = \{0,1,2,3\} .$ Let $a = 2$ and $b = 3$ .
Then the order of $a$ is $2,$ and that of $b$ is $4$ . However the order of $ab$ is $4$ . So $|ab|$ is not equal to $|a||b|.$ Is this correct thought?","['group-theory', 'abstract-algebra']"
3814314,Possible interpretation for points of non equilibrium in a 2D ODE system where only one of the components of the vector field is $=0$.,"I have a 2D ODE system $$
x'=F(x),
$$ where $x=(x_1,x_2)$ , i.e., $$
\begin{cases}x_1'(t)=F_1(x)\\
x_2'(t)=F_2(x);\end{cases}
$$ A component of the vector field $F$ I have vanishes at a point $x=(x_1,x_2)$ , i.e. $F_2(x)=0$ . If $F_1(x)=0$ , then $x$ would be an equilibrium point. However, I only have $F_2(x)=0$ : can I still get some substantial information about the behavior of the solution $$
x(t)=\big(x_1(t), x_2(t)\big)
$$ around such points of $F(x)$ ? Many thanks.","['vector-fields', 'ordinary-differential-equations', 'dynamical-systems']"
3814361,"When packing disks into a square, is it best to be greedy?","The problem is to pack $n$ non-overlapping disks (not necessarily of the same size) of greatest total area  into a unit square. The case $n=1$ is obvious: just place a disk of radius $\frac12$ centrally in the square, to occupy an area $\frac14\pi\approx78.54\%$ of the square. For $n=2$ , we can do no better than follow up the previous case and add a disk (of radius $\frac32-\surd2$ ) in a corner to touch the existing disk and two sides of the square, thus altogether covering an area $(\frac92-3\surd2)\pi\approx80.85\%$ of the square. Similarly, for $n=3,4,5$ , we can do no better than place further disks of that size in the remaining corners of the square. For $n=6,...,13$ , it seems optimal to build on the case $n=5$ and place further disks in the eight regions bounded by a side of the square, the disk of radius $\frac12$ , and a disk of radius $\frac32-\surd2$ . Clearly, we can keep going like this, packing the next disk into whatever remaining space will accommodate the largest disk—the greedy method. But the question arises: Is there an $n$ for which the packing of maximal total area is not the greedy packing described above?","['packing-problem', 'geometry', 'algorithms']"
3814366,"This finite sum involving roots of unity is bounded, but why?","For $N \in \mathbb{N}$ , let $\omega := \exp(\frac{2 \pi}{N} \sqrt{-1} )$ . For $k,l \in \{1, \dots, N \}$ , define $$d_{k,l}^{(N)} := \begin{cases}
	\frac{\sin\left(\frac{2\pi}{N}(k-1)\right)}{\sin\left(\frac{2\pi}{N}(k-1)\right)^2 + \sin\left(\frac{2\pi}{N}(l-1)\right)^2}&\mbox{ if } \sin\left(\frac{2\pi}{N}(k-1)\right) \neq 0\\
	0 & \mbox{ otherwise }
	\end{cases}$$ For $i,j,m,n \in \{1, \dots, N\}$ , let $$a_{i,j,m,n}^{(N)} :=\frac{1}{N^2}  \sum_{l=1}^N \sum_{k=1}^N (\omega^{i-j})^{k-1} (\omega^{m-n})^{l-1} d^{(N)}_{k,l} $$ Numerical experiments very strongly suggest that there exists $C>0$ independent of $N$ ( $C \approx 0.25$ ) such that $\forall N$ , $$
 \max_{1 \le i,j,m,n \le N} | a^{(N)}_{i,j,m,n} | \le C
$$ How can one formally show that this is true?","['trigonometry', 'roots-of-unity', 'complex-numbers', 'upper-lower-bounds']"
3814397,Two players alternate coloring squares in $n\times m$ rectangular board,"I'm interested in the following game: Given a rectangular board consisting of $n\times m$ unit squares, two players alternate coloring squares composed of uncolored unit squares. The player that is unable to color a square loses the game. What is known: If $n=m$ , then the first player wins by coloring the entire board. If $n=1$ , then the first player can win unless $m\equiv 0\pmod{2}$ , where the second player wins. Therefore, from now on we assume $1\lt n\lt m$ . If $n,m$ are of equal parity (both odd or both even), then the first player can color a $n\times n$ square in the center of the board which leaves us with two equal separated boards. By symmetry, the first player proceeds to mirror the moves of the second player across those two boards, to win. If $n$ is odd and $m$ is even, then the first player can color a $(n-1)\times (n-1)$ square in the center of the board which leaves us with two equal separated boards and a $1\times(n-1)$ strip next to the colored square.  By symmetry, the first player proceeds to mirror the moves of the second player across those two boards and across the two halves of the strip, to win. This leaves us with the case when $n$ is even and $m$ is odd. If $n=2$ , then the first player can win unless $m\equiv 1\pmod{12}$ , where the second player wins. This was observed by TonyK in their answer to ""Game with coloring squares in rectangular board"" where $a=n,b=m$ . But when $n\gt 2$ , this is apparently a hard problem. Does this game have a name and had anyone attempted to solve it in the past? What strategies can be used to try to solve the remaining case? I did not find anything relevant other than the linked answer above, hence I am asking this question. Note that the linked question is not a duplicate  since the author decided to ignore the ""(even, odd) case"" by accepting a partial answer.","['game-theory', 'combinatorial-game-theory', 'combinatorics', 'reference-request']"
3814431,Count paths of length $2n$ that don't hit the diagonal after starting,"Consider a path on the coordinate plane that can only move up or right. Prove that the number of such paths of length $2n$ that don't ever touch (previously read: ""cross"") the diagonal is $\binom{2n}n$ by a bijection. Attempt: Tried to implement a Catalan-style proof by considering the first point it crosses the diagonal. Reflecting across the diagonal doesn't do anything, since the endpoint of a path is not necessarily $(n,n)$ . I have no other ideas to attempt... Help! :)","['combinatorics', 'discrete-mathematics']"
3814432,Proving an extension of the conjugate root theorem,"In our analysis class, we just proved the complex conjugate theorem - that for any polynomial with real coefficients and a root $z$ , $\overline{z}$ is also a root of that polynomial. I searched online to try to find a generalization of this fact, and ended up piecing together something like the following: Let $L/K$ be a field extension, let $p\in K[x]$ and $z\in L$ such that $p(z)=0$ . If $\sigma\colon L\rightarrow L$ is a ring homomorphism such that $\sigma$ fixes the elements of $K$ , then $\sigma(z)$ is a root of $p$ . This would certainly be nice if true, but coming from an intro to analysis class I don't have the right tools to prove it and can't find a proof online. It doesn't stem easily from the (purely algebraic) proof of the complex conjugate theorem we were shown, either. How can I prove this statement?","['field-theory', 'group-theory', 'abstract-algebra']"
3814447,Solving $2^x = \sqrt{3^x}+1$ using logarithm.,"I have problem solving the equation $$2^x = \sqrt{3^x} +1$$ for $x$ using logarithm. I know the only answer is $2$ which can be proven using graphs or derivatives,etc or by dividing the two sides by $2^x$ which gives the sum of $\sin 60°$ to the power of $x$ and $\cos 60°$ to the power of $x$ equal to $1$ , concluding $x=2$ . I'm looking for a way to solve it using logarithm which is not easy because of the "" $1$ "" in one side of the equation.","['algebra-precalculus', 'logarithms']"
3814451,Map function to apply to each item of tuple,"Is there a mathematical function or notation for applying a function to each item of a tuple which returns a new tuple with items that correspond with the 'mapped' items of the original? e.g. if $f(x) = x^2$ and $T = (1, 2, 3)$ , it could be something like $map(T, f)$ which would return $(1, 4, 9)$ .","['notation', 'functions']"
3814483,Can we define $\ln(x)$ as a solution of $\frac{d}{dx}(f(x))=\frac{1}{x}$?,Can we define $\ln(x)$ as the solution of the differential equation $$\frac{d}{dx}(f(x))=\frac{1}{x}$$ such that $\ln(1)=0$ ?,"['ordinary-differential-equations', 'logarithms']"
