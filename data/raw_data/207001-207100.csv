question_id,title,body,tags
4137154,Unbalance in cake-cutting,"In cake-cutting, we cut a round cake into $m$ pieces by $n$ cuts. If we seek maximum $m$ pieces with $n$ cuts, we may observe that the area unbalance increases greatly with $m$ . Could we define a mathematical relationship among the unbalance, pieces and cuts? Let's describe it precisely in math notions. Given a unit disk with total area $\pi$ , for a configuration $c$ of $n$ straight lines, $c$ cuts the disk into $m$ pieces, and the area of each piece is $A_i$ . Define the portion of each piece by $p_i = \frac{A_i}{\pi}$ The area unbalance may be described by entropy: $$S_2 = - \sum_{i=1}^{m}p_i \ln p_i$$ The arc length of the outer pieces is also unbalanced, a similar entropy $S_1$ can be introduced: $$S_1 = - \sum_jq_j \ln q_j$$ where $q_j = \frac{L_j}{2 \pi}$ is the portion of the segments of the circumference. Could we describe the relationship among $S_1$ , $S_2$ , $m$ and $n$ ? And given $m$ and $n$ , which cut $c$ leads to the least unbalance $S_1$ and $S_2$ ?","['optimization', 'geometry', 'probability']"
4137168,$\phi(\pi)$ and other irrationals (Euler's totient function),"Over the natural numbers, Euler's totient function $\phi(n)$ has the nice property that $\phi(n^m)=n^{m-1}\phi(n)$ . I've found that this can naively extend the totient function over the rationals via: $$\phi(b)=\phi\left(\left(\frac{1}{b}\right)^{-1}\right)=\left(\frac{1}{b}\right)^{-2}\phi\left(\frac{1}{b}\right)=b^2\phi\left(\frac{1}{b}\right)$$ $$\implies\phi\left(\frac{1}{b}\right)=\frac{\phi(b)}{b^2}$$ Thus, with another property being that $\gcd(a,b)=1\implies \phi(ab)=\phi(a)\phi(b)$ , then under the assumption that $\gcd(a,b)=1\implies\gcd\left(a,\frac{1}{b}\right)=1$ , we can define $$\phi\left(\frac{a}{b}\right):=\frac{\phi(a)\phi(b)}{b^2}$$ Note that this still preserves consistency over the natural numbers: $$\phi\left(\frac{a}{1}\right):=\frac{\phi(a)\phi(1)}{1^2}=\phi(a)$$ With this, I was immediately curious as to if a sequence of rational numbers $q_n$ converged to an irrational, would $\phi(q_n)$ also converge, and if so, to what? As an initial test, I used the sequence $\pi_n=\sum_{k=0}^n\frac{4(-1)^k}{2k+1}$ . Which, as you know, converges to $\pi$ . I also tested the sequence $e_n=\sum_{k=0}^n\frac{1}{k!}$ (which converges to $e$ , respectively). To my surprise, I found that with this definition of $\phi$ , it seemed that both $\pi_n$ and $e_n$ converge. The graph of $\phi(e_n)$ is in blue, and $\phi(\pi_n)$ in red. Curiously, (perhaps due to the closeness of $\pi\approx e$ ), they both seemingly approach a value of about $0.4$ . That being said, my computer and I only had the patience to calculate the first $40$ -ish terms, so I would very much like to know what the long-term behavior of the graph is. Any insight would be very much appreciated. Addendum: May 13th With the feedback I've gathered in the comments, I've done some more analysis, which may be interesting to some of you. Specifically, it does not appear that the totient of all converging rational sequences converge. For example, with suggestions from Conifold, I tested the sequence of rationals defined by the continued fraction for the golden ratio. Which can be simplified to $\varphi_n=\frac{F_{n+1}}{F_n}$ , where $F_n$ is the $n$ 'th Fibonacci number. It seemed evident from computational analysis that $\phi(\varphi_n)$ did not converge, however it seemed evident that $\limsup_{n\to\infty}\phi(\varphi_n)=\varphi=\frac{1+\sqrt{5}}{2}.$ . The average and $\liminf$ also seemed to converge, however the $\limsup$ seemed to give more canonical results; on other sequences as well. For the case of $\sqrt{2}$ , it may be defined by $\phi(\sqrt{2})=2^{\frac{1}{2}-1}\phi(2)=\frac{\sqrt{2}}{2}$ , which seemed to be exactly what the $\limsup$ converged to when taking the continued fraction for $\sqrt{2}$ . Lastly, and perhaps most curious, if we define $e_n:=\left(\frac{n+1}{n}\right)^n$ (as $\lim_{n\to\infty}e_n=e$ ), then, assuming my math is correct, we can deduce $$L=\limsup{\phi(e_n)}=\limsup_{n\to\infty}{\phi\left(\left(\frac{n+1}{n}\right)^n\right)}=\limsup\left(\frac{n+1}{n}\right)^{n-1}\frac{\phi(n)\phi(n+1)}{n^2}$$ $$\implies \ln L = \limsup(n-1)\ln\left(\frac{n+1}{n}\right)+\ln\left(\frac{\phi(n)}{n}\right)+\ln\left(\frac{\phi(n+1)}{n}\right)$$ $$=1+\ln 1+\ln 1$$ $$\implies L=e$$ However in my computational analyses, It seemed that $\limsup \phi(e_n)\approx \frac{e}{2}$ I'm not sure what can be taken from this, however I do find these results interesting, so perhaps you will too.","['irrational-numbers', 'totient-function', 'real-analysis']"
4137464,"A question on Williams' ""Probability with Martingales"" Theorem 12.2 (b).","Theorem 12.2 (b) of Williams' ""Probability with Martingales"" states that the convergence of a series of zero-mean independent and bounded random variables $X_k$ (say bounded by $K$ ) implies the convergence of the series of their variances. That is, if $\sum_{k} X_k$ converges then so does $\sum_{k} \mathbb{E} X_k^2$ . This is of course plausible but there is one step in the proof that I do not quite understand. Here is the except from the book. Here, $M_n = \sum_{k=1}^n X_k$ , $A_n = \sum_{k=1}^n \mathbb{E} X_k^2$ and $N_n = M_n^2 - A_n$ . My question has to do with the use of the stopping time $T$ . This is a valid stopping time of course but then Williams derives the inequality $(**)$ assuming that $T$ is finite only to say later that $T$ is inifnite for some $c$ with positive probability. I find the reasoning here very confusing so I would be grateful if someone could give a little more detail. Also, how is it that we have $|M_n^T| \leq K + c$ for every $n$ ? Thank you.","['martingales', 'measure-theory', 'probability-theory', 'probability']"
4137482,Bounded monotone convergence for functions,"Let $f:[a,b]\rightarrow\mathbb{R}$ be a bounded, increasing function. Prove that $\;\lim\limits_{x\to b^-} f(x)=\sup\limits_{x\in\left[a,b\right[}f(x)\;.$ This looks a lot like the principle of bounded monotone convergence for sequences, but instead for limits at a point as opposed to limits at infinity. I tried using the sequential characterisation of limits but got nowhere (as we must take a limit to infinity)","['calculus', 'functions', 'analysis', 'real-analysis']"
4137494,How to calculate the limit $\lim_{n\to\infty}\left(\frac{1}{\sqrt{n^2+n+1}}+\frac{1}{\sqrt{n^2+n+2}}+\cdots+\frac{1}{\sqrt{n^2+n+n}}\right)^n$,"$$I=\lim_{n\to\infty}\left(\frac{1}{\sqrt{n^2+n+1}}+\frac{1}{\sqrt{n^2+n+2}}+\cdots+\frac{1}{\sqrt{n^2+n+n}}\right)^n$$ I tried $$\frac{n}{\sqrt{n^2+n+n}}\leq\frac{1}{\sqrt{n^2+n+1}}+\frac{1}{\sqrt{n^2+n+2}}+\cdots+\frac{1}{\sqrt{n^2+n+n}}\leq\frac{n}{\sqrt{n^2+n+1}}$$ But $$\lim_{n\to\infty}\left(\frac{n}{\sqrt{n^2+n+n}}\right)^n=\lim_{n\to\infty}\left(1+\frac2n\right)^{\displaystyle-\frac{n}{2}}=e^{\displaystyle \lim_{n\to\infty}-\frac n2\ln(1+\frac2n)}=\frac1e$$ And in the same way,i got $$\lim_{n\to\infty}\left(\frac{n}{\sqrt{n^2+n+1}}\right)^n=\lim_{n\to\infty}\left(1+\frac1n+\frac{1}{n^2}\right)^{\displaystyle-\frac{n}{2}}=e^{\displaystyle \lim_{n\to\infty}-\frac n2\ln(1+\frac1n+\frac{1}{n^2})}=\frac{1}{\sqrt e}$$ So i only got $$\frac1e\leq I\leq\frac{1}{\sqrt e}$$ Could someone help me get the value of $I$ . Thanks!","['limits', 'analysis', 'real-analysis']"
4137517,Searching radioactive balls,"There are $n$ balls, and $m$ of them are radioactive. You can test any set of balls and find out whether there is at least one radioactive ball in this set (but it is impossible to know how many of them are radioactive). What is the minimal possible number of tests that can guarantee finding of all radioactive balls? It is clearly $\geq \lceil \log_2(C_n^m) \rceil$ by pigeonhole principle. Indeed, there are $C_n^m$ possible combinations of radioactive balls but a sequence of $k$ binary tests can only have $2^k$ distinct outcomes. That bound is tight for degenerate cases $m = 0$ and $m = n$ (we do not need to test anything in that case) and for case $m = 1$ (use  binary search here). However, it not tight in general. For example, the minimal possible number of tests for $m = n - 1$ is $n-1$ . That is because in that case we can only test individual balls (otherwise the test will return true no matter what) and there are $n$ of them. Therefore we need something better.","['extremal-combinatorics', 'information-theory', 'combinatorics', 'algorithms', 'optimization']"
4137533,"For $(1 + x + x^2)^6$, find the term which has $x^6$ in it.","For $(1 + x + x^2)^6$ , find the term which has $x^6$ in it. I tried to use Newton's binomial formula as: $$
(1 + x + x^2)^6 = \sum_{k = 0}^{6}\left( \binom{6}{k}(1 + x)^{n-k} x^{2k}\right)
$$ and that's all I can think of, other then just to compute it.","['newton-series', 'binomial-theorem', 'combinatorics']"
4137552,How can we calculate the following limit?,"I came across this problem and spent a lot of time on this but couldn't figure it out. $$\lim_{x\to\infty}\left({\frac{2 \arctan(x)}{\pi}}\right)^x$$ Using a general approach for $1^\infty$ type, I could come to the following expression: $$ e^\left(\lim_{x\to\infty}{x\frac{2 \arctan(x)}{\pi}-1}\right) $$ But I can't get further from this. I was able to observe that this is a ( $\infty*0$ )form so maybe we could transform the whole parenthesis with the $\arctan$ in the denominator so that we could get a $\frac{\infty}{\infty}$ form which could enable us to use L'Hospital's rule here but I that didn't yield a good result. Is there any better way to do this?","['limits', 'calculus']"
4137571,"Proving that $a^2x^2+(b^2+a^2-c^2)x+b^2=0$ has no real roots if $a+b>c$ and $|a-b|<c$ for real $a$, $b$, $c$","How is this answer derived, and is there any other way to prove it? Prove that $$a^2x^2+ (b^2 + a^2 - c^2)x + b^2 = 0$$ does not have real roots if $a+b>c$ and $|a-b|<c$ . $a,b,c \in \mathbb{R}$ . Solution I found online: The discriminant is given as: $$D = (b^2 + a^2 - c^2)x - 4a^2b^2$$ $$(a^2 + b^2)^2 - 2c^2(a^2 + b^2) + c^4 - 4a^2b^2$$ Let $c^2= t$ . You get : $$t^2 - 2t(a^2 +b^2) + (a^2 + b^2)^2$$ has to be less than zero since $D < 0$ means nonreal roots. Then, you get $$(a-b)^2 < t < (a+b)^2.$$ Hence, you get the condition satisfied. My questions regarding this proof : It is right to say that $a,b,c$ have to be some particular numbers only. Only one type. By here, what I mean to say is that let $a=5$ , $b =6$ or $c=7$ or another set of numbers , $a=6$ , $b=7$ , $c=4$ . Now , according to the condition, it may or may not be possible that one of the conditional value satisfies the condition since $a , b$ and $c$ can have infinite values, right? There can be infinite quadratic equations but not all will satisfy this condition of our question. If not, can we find out whatever those numbers value can be and which are those numbers which satisfy the condition and how many types of this condition is possible. I think it is impossible since that can be infinite. I don’t think also that it is true that there will be only two conditions. If I know the values of each variable , I can create a few more conditions. Here , I mean to say that if I have a question with all of its values in numbers except $x$ . Like $a, b, c$ has a numerical value. You don’t need any other value. Making condition like $a+b>c$ . Who knows ? I can also create another condition like $a*b*c>0$ . Right ? Then, our answer will not be the same if this condition was given as an alternative to some other one. How did you know that in the end you would get this result only ? This answer is like a luck. I am not satisfied with this way of deriving. I am looking for alternate solutions of the above question or id you could help me to understand this proof as a right one.","['proof-explanation', 'roots', 'alternative-proof', 'algebra-precalculus', 'quadratics']"
4137578,Two generators of random points in a disk,"I have two ways to generate points in a disk: The first is: $
a, b \sim U[0, 1]. 
$ Point is generating the next way: $(a \cos(2\pi b), a \sin(2 \pi b))$ . The second is: $a, b \sim U[-1, 1]$ and point generating only if $a^2 + b^2 \le 1$ . Suppose I've given points already distributed. I want to derive by what method (first or second) it was produced.
By the way graph of this two method is: It's easy to see, that for the first method points are more 'squeezed' to zero than for the second method. My approach: is very clumsy. I'm finding probability that points are more concentrated to the center in first method, i.e. I'm finding $\mathbb{P}(-0.2 < x < 0.2)$ for each method. I'm doing it using Cumulative Distribution Function estimation: $$ \tilde F(t)=\frac{1}{n + 1} \left(\frac{1}{2}  + \sum_{i: x_i < t} 1  \right) $$ For the first it is near the $0.4$ , while for the second it is more near the $0.25.$ Any ideas how to solve it more mathematical way?","['statistical-inference', 'statistics', 'hypothesis-testing']"
4137753,How would one go about solving the problem below efficiently?,"If $$\sum_{k=1}^{5}\left(-1\right)^{\left(k+1\right)}\left(ka^{2}-k^{2}a\right)=P\cdot a^{2}+Q\cdot a$$ compute P and Q. I managed to solve it (albeit slowly) by solving for each term from k=1 to k=5 and adding them up, but that seems rather inefficient. I imagine there has to be a better way to solve this problem, and if so, could someone explain it?","['algebra-precalculus', 'sequences-and-series']"
4137787,Substring of a bit string probability [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Given a fixed bit string $A$ of length $k$ and a randomly generated bit string $B$ of length $n > k$ , meaning each bit of $B$ has probability $1/2$ to be  zero or one respectively, how can one determine the probability that $A$ is a substring of $B$ , meaning that the sequence of bits of $A$ appears somewhere in $B$ (contiguously)? Unfortunately, I don't really have any approaches to this problem...","['probability', 'bit-strings']"
4137803,Relation via derivative of two angle-based probabilities for $n$ points on a circle.,"Suppose we sample $n$ points uniformly (and independently) on the unit circle in the sense that the probability that a point lies within some circular arc is proportional to its length. For this question, we'll choose the convention to measure angles from 0 to 1 (rather than from 0 to $2\pi$ when we use radians) to make the formulae come out nicer. Then if we are given a fixed circular arc subtending an angle $0 \leq \alpha\leq \frac{1}{2}$ (the bounds chosen due to technicalities from reflex angles), we know that the probability that all $n$ points lie in this arc is $\alpha^n$ . It is also a known result/elementary exercise that the probability that $n$ such points lie within some arc which subtends an angle of $\alpha$ is $n\alpha^{n-1}$ . Is there some probabilistic reason that these formulae are related by differentiation with respect to $\alpha$ ?","['derivatives', 'geometric-probability', 'probability']"
4137835,"To prove : $\lim_{n \to +\infty} a_{n} $ diverges, where $a_{n}= (-1)^n$ from $\varepsilon$ definition","How to prove : $\lim_{n \to +\infty} a_{n}$ diverges, where $a_{n}= (-1)^n$ from $\varepsilon$ definition. My way : From definition : $a = \lim_{n \to +\infty} a_{n} = \forall \varepsilon>0 \exists N(\varepsilon) \in \mathbb{N} : \forall n \geq N(\varepsilon) \implies \mid a_{n} -a \mid < \varepsilon$ ---(1) Taking the negation of (1) statement we get : $a \neq  \lim_{n \to +\infty} a_{n} = \exists \varepsilon>0 \forall N(\varepsilon) \in \mathbb{N} \exists n \geq N(\varepsilon) : \mid a_{n} -a \mid \geq \varepsilon$ ---(2) We've to prove (2). We've to divide the statement (2) into two parts : (i) $\forall a \geq 0 \exists \varepsilon>0 \forall N(\varepsilon) \in \mathbb{N} \exists n = 2N+1\geq N(\varepsilon) : \mid a_{n} -a \mid \geq \varepsilon$ ---2(a) (ii) $\forall a < 0 \exists \varepsilon>0 \forall N(\varepsilon) \in \mathbb{N} \exists n = 2N\geq N(\varepsilon) : \mid a_{n} -a \mid \geq \varepsilon$ ---2(b) From 2(a) and 2(b) we get : $\forall a \mathbb{R} \exists \varepsilon>0 \forall N(\varepsilon) \in \mathbb{N} \exists n \geq N(\varepsilon) : \mid a_{n} -a \mid \geq \varepsilon$ ---(3) But, I'm confused to choose the proper $\varepsilon$ so that my work would be true. I'm stuck here. Please help me. My previously asked the same question got downvoted due to lack of enough data, so I deleted the previous post and asked again with my detailed work.","['epsilon-delta', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
4137885,Basic Algebraic Geometry (Silverman Elliptic Curves problem 2.3),"Let $\phi:C_1 \rightarrow C_2$ be a morphism of smooth projective curves over an algebraically closed field. One wants to show that $$[K(C_1):\phi^*K(C_2)] = \sum_{P \in \phi^{-1}(Q)}ord_P (\phi^*t_Q)$$ where $t_Q$ is a uniformizer at $Q$ . I started off by looking at the integral closure $B$ of the DVR $\phi^*K[C_2]_Q = A$ . Then $B$ is a Dedekind domain and it is easy to check the field of fractions is $K(C_1)$ . Thus if $M_Q$ is the unique maximal ideal of $\phi^*K[C_2]_Q$ , and $M_Q$ factors in $B$ as $r_1^{e_1}\cdots r_n^{e_n}$ , then $[K(C_1):\phi^*K(C_2)] = \sum_{i=1}^n e_i$ (since $B/r_i = A/M_q = K$ ). It is easy to check that a subset of these maximal ideal factors includes all ideals of the form $M_PK[C_1]_P \cap B$ for $P \in \phi^{-1}(Q)$ and also that any two distinct elements of $\phi^{-1}(Q)$ yield different maximal ideals in $B$ . Furthermore if $r_i = M_PK[C_1]_P \cap B$ , then one can show using induction, localization, and short exact sequences that $e_i = ord_P(\phi^*t_Q)$ . Thus if all maximal ideals of $B$ are of the form $M_pK[C_1]_P \cap B$ , the theorem is proven. How do you show all maximal ideals of $B$ are indeed of that form? I can't seem to figure it out and looking at similar questions there don't appear to be answers that directly address this problem.","['algebraic-geometry', 'abstract-algebra']"
4137907,"Finding all functions $f: \mathbb{Z^{+}}\to \mathbb{Z^{+}}$ such that $f(a)| (f(b)+a-b)$ for all $a,b\in \mathbb{Z^{+}}$",I managed to find the two trivial solutions - that is $f(n)=n$ and $f(n)=1$ . I am not sure if there are any more. How to go about finding the full set of solutions and proving it is complete? Thank you for your help,"['functions', 'natural-numbers', 'divisibility']"
4137918,Relating Norm and Measure for doing calculus.,"In $\mathbb{R}$ , integration and differentiation are complementary to each other but as we make our definition more abstract the differ, as to define integration we need a measure space and to define derivative we need vector space with norm. These requirements are disjoint and may not always occurs together (Note once we defined each of the entities mentioned above, we can talk of anti integral or anti derivative resp as the inverse process). But I was wondering if given an measure space, can we define derivative by constructing a norm based on the measure specified (for integral) such that the anti-integral and derivative are compatible. Some comment/remark/insight will be helpful","['lebesgue-integral', 'measure-theory', 'normed-spaces', 'derivatives']"
4137928,Why does infinite sum of finite summands converge in this proof $\sum 1/p$ converge?,"In this proof from  the university of Warwick ( pdf ) there is a step which assumes the infinite sum of finite summands converges. Why is this? I can see why each summand $\sum_p1/p^s$ for $s \geq 2$ converges, but not why an infinite sum of these.","['number-theory', 'convergence-divergence', 'prime-numbers', 'sequences-and-series']"
4137964,Find the minimum of $\frac{a^3+2020}{b}+\frac{b^3+2020}{a}$,"Problem says: Let $a,b>0$ and $2(a^2+b^2)-(a+b)=2ab$ Find the minimum of $$\frac{a^3+2020}{b}+\frac{b^3+2020}{a}$$ The things I have done: $$\begin{align}\frac{2ab+(a+b)}{2}≥2ab \end{align}$$ $$\begin{align} &\implies 2ab+(a+b)≥4ab \\
&\implies a+b≥2ab \\
&\implies \frac {ab}{a+b}≤\frac 12\end{align}$$ $$\begin{align}&2(a^2+b^2)-(a+b)=2ab, ~ab>0\end{align}$$ $$\begin{align}&\implies 2\left((a+b)^2-2ab\right)-(a+b)-2ab=0\\
&\implies 2(a+b)^2-4ab-(a+b)-2ab=0 \\
&\implies 2(a+b)^2-(a+b)-6ab=0 \\
&\implies \frac 13(a+b)-\frac{ab}{a+b}-\frac 16=0\\
&\implies \frac{ab}{a+b}=\frac 13(a+b)-\frac 16≤\frac 12\\
&\implies a+b≤2\\
&\implies 2≥a+b≥2ab \\
&\implies 0<ab≤1 \end{align}$$ $$\begin{cases}\frac {a^3}{b}+\frac{b^3}{a} ≥2ab \\ 2020 \left(\frac 1a+\frac 1b \right)≥\frac{4040}{\sqrt{ab}}\end{cases}$$ $$\begin{align}\implies &\frac{a^3+2020}{b}+\frac{b^3+2020}{a}\\
&≥2ab+\frac{4040}{\sqrt{ab}} \end{align}$$ Let $n:=ab$ ,  then we have $$f(n)=2n^2+\frac{4040}{n},~ 0<n≤1$$ $$\begin{align}f(n)-4042&=2n^2+\frac{4040}{n}-4042\\
&=\frac{2n^3-4042n+4040}{n}\\
&=\frac{2(n-1)(n^2+n-2020)}{n}\\
&≥0 \end{align}$$ $$\begin{align} &\implies f(n)-4042≥0 \\
&\implies f(n)≥4042,~ 0<n≤1\end{align}$$ $$\begin{align}\frac{a^3+2020}{b}+\frac{b^3+2020}{a}\\ ≥2ab+\frac{4040}{\sqrt{ab}}≥4042\end{align}$$ $$\min\left\{\frac{a^3+2020}{b}+\frac{b^3+2020}{a},~{\large{\mid}} 2(a^2+b^2)-(a+b)=2ab ∧ ~a>0∧b>0\right\}=4042 ~ \text{at}~ ab=1$$ $$\begin{cases}ab=1 \\ a+b=2 \end{cases} \implies a=b=1$$ $$\min\left\{\frac{a^3+2020}{b}+\frac{b^3+2020}{a},~{\large{\mid}} 2(a^2+b^2)-(a+b)=2ab ∧ ~a>0∧b>0\right\}=4042 ~ \text{at}~ a=b=1.$$ Cyclicity/symmetry  argument (?) Let, $$f(a,b)=\frac{a^3+2020}{b}+\frac{b^3+2020}{a}$$ Then suppose that, $f(a,b)$ gets its minimum value at the point $a=m$ . Substitution $a\longmapsto b$ shows that, $f(a,b)$ also gets its minimum value at the point $b=m$ . This means, we have $a=b.$ I see that $$2(a^2+b^2)-(a+b)=2ab$$ is also simmetric/cyclic. We get, $$\begin{align}2(a^2+b^2)-(a+b)=2ab \end{align}$$ $$\begin{align}&\implies 4a^2-2a-2a^2=0 \\
&\implies a=b=1\end{align}$$ $$\begin{align}\min \left\{f(a,b) \mid 2(a^2+b^2)-(a+b)=2ab ∧ ~a>0∧b>0\right\}=4042.\end{align}$$ Question: What are the points that are not rigorously correct (I refer to both proofs) in the things I do? Please, don't post the correct solution. Thank you for reviewing.","['contest-math', 'inequality', 'proof-writing', 'maxima-minima', 'algebra-precalculus']"
4137974,How is this a function? - Analysis.,"Let $X = \{1, 2, 3\}, Y = \{4, 5, 6\}$ . Define $F \subseteq X \times Y$ as $F = \{(1, 4),(2, 5),(3, 5)\}$ .
Then $F$ is a function. I simply do not see how this could be a function, as there is nothing that it is mapping to, if anyone can explain how this is a function, that would be lovely.","['elementary-set-theory', 'functions']"
4137989,"If $X\sim U(0,1)$ then $Y=-\frac{1}{\lambda }\ln(1-X)\sim Exp(\lambda )$","If $X\sim U(0,1)$ then $Y=-\frac{1}{\lambda }\ln(1-X)\sim Exp(\lambda )$ Here is my solution : If $X\sim U(0,1)$ then $f(x)=1$ where $0\leq x\leq 1$ . $0<x\Rightarrow -x\leq 0\Rightarrow 1-x\leq 1\Rightarrow \ln (1-x)\leq 0\Rightarrow 0<-\frac{1}{\lambda }\ln (1-x)\Rightarrow y>0$ $F(y)=P(Y\leq y)=P(-\frac{1}{\lambda }\ln (1-X)\leq y)=P(X\leq 1-e^{-\lambda y})=\int_{0}^{1-e^{-\lambda y}}dx=1-e^{-\lambda y}$ From here, $$\begin{cases}
 f(y)=\frac{dF(y)}{dy}=\lambda e^{-\lambda y}& \text{ if } y>0 \\ 
 0& \text{otherwise }
\end{cases}$$ But I am confused the interval of uniform distribution. Should I take the interval $0<x<1$ or $0\leq x\leq 1$ ? If I take $0\leq x\leq 1$ then $y\geq 0$ but from definiton y can not be $0$ when $f(y)=\lambda e^{-\lambda y}$ . Any help will be appreciated.","['statistics', 'uniform-distribution', 'probability-distributions', 'exponential-distribution', 'probability-theory']"
4138002,Homeomorphic Spaces in Topology,"Let $X$ and $Y$ be two topological spaces.
Let $f : X \to Y$ and $g: Y \to X$ , such that $f$ and $g$ are surjective; $f$ and $g$ are continous. Does this imply that $X$ and $Y$ are homeomorphic? It seems similar to Bernstein's theorem in set theory, and many of the topological properties like compactness, connectedness, etc are getting preserved. Any help would be appreciated.",['general-topology']
4138004,How many terms are there containing the term $xyk^2$ in the expansion of $(2x-y+t+3z+4k)^8$,"How many terms are there containing the term $xyk^2$ in the expansion of $(2x-y+t+3z+4k)^8$ such as $xyk^2t^2z^2$ or $xyk^2t^4z^0$ or $xyk^2z^3t$ etc. I made up this question and calculated it , but i do not know whether my solution is correct or not. $\color{blue}{Solution:}$ $\color{red}{1-)}$ I turned it out to be binomial expression such that $\color{green}{a=}2x-y+4k$ and $\color{green}{b=}t+3z$ , so I will work over $(a+b)^8$ . $\color{red}{2-)}$ I thought that i should look for $xyk^2$ in $a$ ,so i said that the exponential of $a$ must be $4$ , because $1+1+2=4$ which are the sum of the exponents of $xyk^2$ . $\color{red}{3-)}$ Hence, the exponent of $b$ must be $4$ ,as well. Because the sum of the exponents of $a$ and $b$ must be equal to $8$ . $\color{red}{4-)}$ Therefore, the terms which contains $xyk^2$ in the expansion of $(2x-y+t+3z+4k)^8$ must be equal the numbers of terms in the expansion of $b$ ,i.e, $(3z+t)^4$ . It is equal to $5$ So , my answer is $5$ . I checked it with second solution. However , i am not sure about the truth of the first way. Is my first way true? Thanks for all contributions.. $\color{blue}{Solution 2:}$ Because of the fact that the sum of the exponents must be equal to $8$ , i should find the number of partition of $4$ in two part so as to find the number of combinations of the exponents of $t$ and $z$ . Then, $t=4,z=0$ or $t=3,z=1$ or $t=2,z=2$ or $t=1,z=3$ or $t=0,z=4$","['solution-verification', 'combinatorics', 'multinomial-coefficients', 'discrete-mathematics', 'binomial-theorem']"
4138021,Can isomorphic groups act on a topological space in different ways?,"The question arises from the fact that each topological manifold $X$ is homeomorphic to its universal cover $X_0$ quotiented by the action of the fundamental group $\pi_1(X)$ .
It is natural to ask wether two spaces with the same universal covering and with isomorphic fundamental group are homeomorphic.
The answer is affirmative in the case of compact surfaces.
In general we ask wether two isomorphic groups $G$ and $G'$ can act properly discontinously and freely on a simply connected space $X$ in two different ways i.e. the corresponding quotient spaces $X/G$ and $X/G'$ are not homeomorphic.","['general-topology', 'group-actions', 'covering-spaces']"
4138031,Prove that $\overline{A\cup B} = \overline{A} \cup \overline{B} $ where $\overline{A}$ is the closure of $A$,"I am self-learning Real Analysis from the text Understanding Analysis by Stephen Abbott. I would like to ask, if this constitutes a valid proof of the fact the closure of the union of two sets equals the union of the respective closures? $$\overline{A \cup B} = \overline{A} \cup \overline{B}$$ Proof. Suppose $\displaystyle x\in \overline{A\cup B} .$ Let $\displaystyle L$ be the set of limit points $\displaystyle A\cup B$ . Then, either $\displaystyle x\in A\cup B$ or $\displaystyle x\in L$ . If $\displaystyle x\in A\cup B$ , then $\displaystyle x\in A$ or $\displaystyle x\in B$ or $\displaystyle x$ belongs to both. So, $\displaystyle x\in \overline{A} \ \cup \overline{B}$ . If $\displaystyle x\in L$ , then $\displaystyle x$ is a limit point of atleast one of the sets, $\displaystyle A$ or $\displaystyle B$ . ( $\star$ ) Thus, $\displaystyle x\in \overline{A} \cup \overline{B}$ . Altogether, $\displaystyle \overline{A\cup B} \subseteq \overline{A} \cup \overline{B}$ . In the opposite direction, suppose $\displaystyle x\in \overline{A} \cup \overline{B}$ . Then, $\displaystyle x\in \overline{A}$ or $\displaystyle x\in \overline{B}$ or $\displaystyle x$ belongs to both. Since $\displaystyle A\subseteq A\cup B$ , $\displaystyle \overline{A} \subseteq \overline{A\cup B}$ . Similarly, $\displaystyle \overline{B} \ \subseteq \overline{A\cup B}$ . Consequently, $\displaystyle x\in \overline{A\cup B}$ . Edit: $\star$ If $x$ is a limit point of $A \cup B$ , then for all $\epsilon>0$ , by definition, the open interval $(x-\epsilon,x+\epsilon)$ intersects $A \cup B$ in some point other than $x$ . Consequently, $V_\epsilon(x)$ intersects atleast one of $A$ , $B$ in some point other than $x$ . Thus, $x$ is limit point of atleast one of $A$ , $B$ .","['general-topology', 'solution-verification', 'real-analysis']"
4138035,Limit involving prime counting function,"Recently I came across this problem: $$\lim_{n\rightarrow \infty} \frac{n}{\pi(n)} -H_{n} $$ Where $\pi(n)$ is prime counting function which counts number of primes $\le n$ . & $$H_{n}=\sum_{k=1}^{n} \frac{1}{k}$$ I would attempt the problem as follows.
By using prime number theorem which states: $$\lim_{n \rightarrow \infty} \frac{\pi(n)}{\frac{n}{\ln(n)}}=1 $$ So will subtitute $\pi(n)$ with $\frac{n}{\ln(n)}$ as both are asymptotically equal , in limit will get: $$\lim_{n\rightarrow \infty} \frac{n}{\pi(n)} -H_{n}= \lim_{n\rightarrow \infty} \frac{n}{\frac{n}{\ln(n)}} -H_{n}= \lim_{n\rightarrow \infty} \ln(n) -H_{n}=-\gamma$$ Where $\gamma$ is euler macheroni constant.
However, when I input this limit in wolfram alpha I get: $$\lim_{n\rightarrow \infty} \frac{n}{\pi(n)} -H_{n}=-1-\gamma$$ https://www.wolframalpha.com/input/?i=limit+calculator&assumption=%7B%22F%22%2C+%22Limit%22%2C+%22limitfunction%22%7D+-%3E%22x%2Fpi%28x%29-H_x%22&assumption=%7B%22F%22%2C+%22Limit%22%2C+%22limit%22%7D+-%3E%22infinity%22 So how is this the case? Is there an asymptotic equation/series expansion of the prime counting function that can help prove that this is the case?","['limits', 'number-theory', 'real-analysis']"
4138062,"What is essential non-isolated singularity, of $f(z) = \sin\left(\frac{1}{\cos(\frac{1}{z})}\right)$.","Let a function $f$ be defined as $f(z) = \sin\left(\dfrac{1}{\cos\left(\frac{1}{z}\right)}\right)$ . I need to check what type of singularity it has at $z = 0$ ? I found it is non-isolated as it is a limit point of the set of singularities. Now in the answer key, It is written that it is essential and non-isolated. Now this confuses me as I have read that essential singularity is a type of isolated singularity. I even googled and saw the same. Can anyone throw some light on the meaning of essential non-isolated singularity? It would be a great help.","['complex-analysis', 'definition', 'singularity']"
4138096,Abelian and nonabelian groups with the same composition factors.,"I think I've answered the question in writing out my thoughts, but I'll share this nonetheless, not as a solution-verification question, but as an examples-counterexamples question. This is Exercise 3.1.2 of Robinson's ""A Course in the Theory of Groups (Second Edition)"" . According to this search , it is new to MSE. The Details: Since definitions vary, on page 15, ibid. , paraphrased, it states that A subgroup $N$ of $G$ is normal in $G$ if one of the following equivalent statements is satisfied: (i) $xN=Nx$ for all $x\in G$ . (ii) $x^{-1}Nx=N$ for all $x\in G$ . (iii) $x^{-1}nx\in N$ for all $x\in G, n\in N$ . On page 28, ibid. , A right operator group is a triple $(G, \Omega, \alpha)$ consisting of a group $G$ , a set $\Omega$ called the operator domain and a function $\alpha:G\times \Omega\to G$ such that $g\mapsto (g,\omega)\alpha$ is an endomorphism of $G$ for each $\omega\in\Omega$ . We shall write $g^\omega$ for $(g,\omega)\alpha$ and speak of the $\Omega$ -group if the function $\alpha$ is understood. On page 63, ibid. , Let $G$ be an operator group with operator domain $\Omega$ . An $\Omega$ -series (of finite length) in $G$ is a finite sequence of $\Omega$ -subgroups including $1$ and $G$ such that each member of the sequence is a normal subgroup of its successor: thus a series can be written $$1=G_0\lhd G_1\lhd\dots\lhd G_l=G.$$ The [. . .] quotient groups $G_{i+1}/G_i$ are the factors of the series. On page 64, ibid. , If $\mathbf{S}$ and $\mathbf{T}$ are $\Omega$ -series of [an $\Omega$ -group] $G$ , call $\mathbf{S}$ a refinement of $\mathbf{T}$ if every term of $\mathbf{T}$ is also a term of $\mathbf{S}$ . If there is at least one term of $\mathbf{S}$ which is not a term of $\mathbf{T}$ , then $\mathbf{S}$ is a proper refinement of $\mathbf{T}$ . On page 65, ibid. , An $\Omega$ -series which has no proper refinements is called an $\Omega$ -composition series . [. . .] If $\Omega$ is empty, we speak of a composition series . The Question: Give an example of an abelian group and a nonabelian group with the same composition factors. Thoughts: Something tells me that the abelian group $A$ and the nonabelian group $G$ - if both finite - must have the same order. This is just a guess but it has guided my exploration of potential groups. Looking at an extreme case, consider the smallest nonabelian group $G=S_3$ . It has order $3!=6$ . Thus compare $G$ to the only other group of order six, namely $A=\Bbb Z_6$ . We have $$1\lhd \Bbb Z_3\lhd G\tag{1}$$ and $$1\lhd \Bbb Z_3\lhd A.\tag{2}$$ So far, so good; indeed: $$G/\Bbb Z_3\cong \Bbb Z_2\cong A/\Bbb Z_3$$ and in both series $\Bbb Z_3/1\cong \Bbb Z_3$ . But something tells me this isn't right. Is it really so simple? I think I have made a stupid mistake somewhere. Have I even got the quotients right? Besides, I'd like a more systematic approach to the exercise than my half-baked heuristics above, please.","['group-theory', 'normal-subgroups', 'examples-counterexamples']"
4138100,PDF for sum of dependent random variables,"When the variables $X, Y$ are independent, then the PDF of $Z = X + Y$ can be computed using convolutions: $$
f_Z(z) = \int_{-\infty}^{\infty} f_X(x)f_Y(z - x) dx
$$ When the variables are dependent, apparently you can use $$
f_Z(z) = \int_{-\infty}^{\infty} f_{XY}(x, z - x) dx
$$ I am wondering where the expression came from for the dependent case? It looks very similar to the independent case except you can't separate the joint distribution into marginals.","['convolution', 'probability']"
4138193,Possible mistake in article proving every Riemannian manifold admits a conformal complete metric,"In ""The existence of complete Riemannian metrics."" , by Nomizu, Katsumi, and Hideki Ozeki, the authors prove that every Riemannian manifold admits a complete metric which is also conformal to the original one. The proof starts like this: Isn't the statement ""if $r(x) = \infty$ at some point $x$ , $M$ is compact"" plainly false? A counter example would be $\mathbb{R}^n$ with the standard metric, where $r(x) =  \infty$ for every point but $\mathbb{R}^n$ is not compact. I tried to use this argument in an exam and my professor mentioned it was wrong because of this, but I want to make sure I'm not misunderstanding anything.","['general-topology', 'differential-geometry', 'riemannian-geometry', 'real-analysis']"
4138207,Find the center of the circle given two tangent lines and one point of tangency,"I'm attempting to find the center of the purple circle (and/or the radius) given the following information: A point of tangency and the slope of the line (orange line) A point on a line that is tangent to the circle and the slope of the line (green line) I cannot solve this trigonometrically because it is possible for the tangent lines to be at any angle to each other (i.e. including parallel). Edit: just to clarify, the diagram below is not intended to show the exact values, just to illustrate the information that is available (i.e. one tangent line with a point definitely on the circle, and another tangent line with a known point not on the circle). The tangent lines could be intersecting or parallel. My intuition says I can use the equation of a circle and plug in the fixed tangency point and the equation of the other line to get a set of two equivalent circle equations, but I seem to be stuck there, and this is a bit out of the limit of my skillset (apologies if I'm making basic errors here): $$(6 - h)^2 + (4 - k)^2 = r^2 = (1x - h)^2 + (0y - k)^2$$ Which seems to me to be equivalent to: $$(6 - h)^2 + (4 - k)^2 = (1x - h)^2 + (0y - k)^2$$ But I'm not totally sure where to go from there.","['algebra-precalculus', 'circles', 'geometry', 'tangent-line']"
4138213,Approximation by rectangles in metric spaces and Radon-Nikodym derivative,"I'm reading a paper about metric measure spaces but I have some doubts on how those two things can hold: let $(X,d)$ be a Polish space and $\mathfrak{m}$ a non-negative Radon measure on $X$ . Let $C\subset D\subset X\times X$ with $C$ open subset, $\pi$ a probability measure on $X\times X$ such that $\pi(D)=1$ and assume that $\mathrm{supp}\,\mathfrak{m}=X$ . Then is it true that there exist countably many Borel sets $A_i,B_i\subset X$ such that $$\pi\bigg(\bigcup_{i\in\mathbb{N}}A_i\times B_i\bigg)=1,\qquad\bigcup_{i\in\mathbb{N}}A_i\times B_i\subset D?$$ I know that the set $C$ is open and that I can approximate it with countably many balls, but this works for $C$ and not for $D$ . Moreover, the fact that the union of the rectangles has full measure follows from regularity of $\pi$ ? Assume $\mathfrak{m}$ to be locally finite. Then the author says that from the Lindelof property and separability there exists a Borel probability measure on $\mathfrak{h}$ such that $$\mathfrak{m}\ll\mathfrak{h}\leq C\mathfrak{m}\qquad\mathrm{for }\,C>0$$ and that $\frac{\mathrm{d}\mathfrak{h}}{\mathrm{d}\mathfrak{m}}$ is locally bounded from below by a positive constant. Do you have any hints on how to construct the measure $\mathfrak{h}$ ? (the paper is about Optimal Transport in Lorentzian lenght spaces)","['measure-theory', 'analysis', 'real-analysis', 'probability-theory', 'radon-nikodym']"
4138256,When do we use common logarithms and when do we use natural logarithms,"Currently, in my math class, we are learning about logarithms. I understand that the common logarithm has a base of 10 and the natural has a base of e. But, when do we use them? For example the equation $7^{x-2} = 30$ in the lesson, you solve by rewriting the equation in logarithmic form $\log_7 30 = x-2$ .               The,n apply the change of base formula, and use a calculator to evaluate. $$\frac{\ln30}{\ln7}$$ now this is where I get confused. Why do use natural logarithms here? Why don't we use common logarithms? Am I missing something simple? Any help is greatly appreciated.","['algebra-precalculus', 'calculator', 'logarithms']"
4138261,A question about change of variables in an ODE or in general,"I have a question about differentiation in this question here: differential equation Cauchy-Euler I understand that it uses product rule to go from the 2nd line to the 3rd line (where the arrow point from and pointing to in the picture). But I was not sure how to get the second term in the 3rd line (the one being circled). I think I understand how to get the first term, but not able to figure out how to get the second term. I guess the second term is $\frac{1}{x} * \frac{d}{dx}(\frac{dy}{dt})$ , so how is it being manipulated to become that term inside the circle there? Are we allowed to just add say: $dp$ in nominator, and $dp$ in denominator however we wish? Say if I have a term like this: $\frac{dx}{dz}$ , can I just add $dp$ to that fraction anyway I wish as long as bottom and top ""cancel outs""? like: $\frac{dx}{dz}=\frac{dx}{dp}\frac{dp}{dz} $ ? Is it what is being done in that part inside the circle? (is it actually valid to do this if this is actually being done?)","['calculus', 'ordinary-differential-equations']"
4138292,Why are $\frac{1}{\cos \theta}+\tan{\theta}$ and $\frac{1}{\cos\theta}-\tan{\theta}$ always reciprocals (besides simply multiplying to get $1$)?,"Can anybody help me understand why these terms are always reciprocals? (theta <= 45°) $$ x =  \frac{1}{\cos \theta} + \tan{\theta} $$ $$ \frac{1}{x} =  \frac{1}{\cos \theta} - \tan{\theta} $$ I understand that if we multiply them, they equal $1$ (because of the equation for a circle). $$\begin{align}
1 &= (\frac{1}{\cos \theta} + \tan{θ})(\frac{1}{\cos \theta} - \tan{θ}) \\[4pt]
1 &= \frac{1}{(\cos{\theta})^{2}} - \frac{\tan{\theta}}{\cos{\theta}} +  \frac{\tan{\theta}}{\cos{\theta}} - (\tan{\theta})^2 \\[4pt]
1 &= \frac{1}{(\cos{\theta})^2} - (\tan{\theta})^2 \\[4pt]
(\cos{\theta})^2 &= 1 - (\cos{\theta})^2(\tan{\theta})^2 \\[4pt]
(\cos{\theta})^2 &= 1 - (\sin{\theta})^2 
\end{align}$$ But I am looking for a deeper understanding? Regards",['trigonometry']
4138323,Verify if the boundaries of the following double integrals are correct:,"I would appreciate if anyone can verify if I did the boundary right, I am trying to practice only this thing, I don't want to evaluate the integrals, just to learn how to do the boundary right. $\left(1\right)$ $$\int \:\int _D\:\sqrt{x^2-y^2}dxdy\:$$ $$\text{where }D \text{ is bounded by the sides of the triangle } OAB\: \text{ with }O\left(0,0\right),\:\:A\left(1,-1\right),\:B\left(1,1\right)$$ Here I found out that $y=-x \text{ and }y=\:-1$ from doing OA and AB hence my integral becomes bounded by: $$\int _0^1\:\left(\int _{-x}^{-1}\:\:\sqrt{x^2-y^2}dy\:\right)dx$$ $\left(2\right)$ $$\int \:\int _D\:\:xy \: dxdy$$ $$\text{where } D \text{ is bounded by } y=x^2,\:y=2x+3$$ For this one I did the following: $$\int _{-1}^{0}\:\:\left(\int _{x^2}^{2x+3}xy \: dy\right)\:dx$$ because i took $-1\le x\le 0$ $(3)$ $$\int \:\int _D \arcsin\sqrt{x+y}\:dxdy$$ $$\text{ where } D\text{ is bounded by } x+y=0,\:x+y=1,\:x=0,\:x=1$$ For this one I took: $0\le x\le 1$ and $y=-x\:,\:y=\:1-x$ so my integral will have the following boundary: $$ \int _0^1\:\left(\int _{-x}^{1-x} \arcsin\sqrt{x+y}\:dy\right)dx $$ I hope I didn't do any typos, I am a bit tired and also still not used to the typing format yet.","['multivariable-calculus', 'calculus', 'solution-verification', 'multiple-integral']"
4138346,Using definite integral to find the limit.,"I would like someone to verify this exercise for me. Please. Find the following limit: $\lim\limits_{n \to \infty}\left(\dfrac{1}{n+1}+\dfrac{1}{n+2}+...+\dfrac{1}{3n}\right)$ $=\lim\limits_{n \to \infty}\left(\dfrac{1}{n+1}+\dfrac{1}{n+2}+...+\dfrac{1}{n+2n}\right)$ $=\lim\limits_{n \to \infty}\sum\limits_{k=1}^{2n} \dfrac{1}{n+k}$ $=\lim\limits_{n \to \infty}\sum\limits_{k=1}^{2n} \dfrac{1}{n\left(1+\frac{k}{n}\right)}$ $=\lim\limits_{n \to \infty}\sum\limits_{k=1}^{2n}\left(\dfrac{1}{1+\frac{k}{n}}\cdot\dfrac{1}{n}\right)$ $=\displaystyle\int_{1+0}^{1+2} \frac{1}{x} \,dx$ $=\displaystyle\int_{1}^{3} \frac{1}{x} \,dx$ $=\big[\ln|x|\big] _{1}^3$ $=\ln|3|-\ln|1|$ $=\ln(3)-\ln(1)$ $=\ln(3)$","['definite-integrals', 'riemann-sum', 'calculus', 'solution-verification', 'limits']"
4138356,"Would these 2 infinite sets be equal, if so why?","My friends and I were asking the following question:
If Minecraft worlds were to be infinite, does that mean that every Minecraft world is identical? My friends and I are adding this constraint to say they are ""identical"": $(x_1,y_1,z_1)$ in world1 doesn’t necessarily need to be equal to $(x_2,y_2,z_2)$ in world2 for the worlds to be identical These Minecraft world have a fixed amount of possible blocks that can occupy one space block, let this fixed amount be $x$ . There is a height limit from $0$ to $255$ in the $z$ -axis, but there would be no limit on the $x$ and $y$ axes. So if we were to focus only on the $z$ axis and one of the other axes, we would have $255$ infinite sequence of numbers, one set on top of another. I was thinking the following: If we grab 2 series from point 2 above, let's say $s_1$ and $s_2$ . Since they are infinite, any ""subsequence"" (so a small portion of the sequence) in $s_1$ is bound to happen in $s_2$ . Is this true? If so, why? I was thinking that if 1 is true, then by extension, all the 255 infinite sequences would be equal, and this further extends to all sequences in the 3d world. Is this true? If so, why? Finally, do 1 and 2 here imply that every sequence mentioned above is identical? If so, why? Does 3 imply that the worlds would be identical? If so, why?","['combinatorics', 'infinity', 'sequences-and-series']"
4138466,"How to draw a graph f(x,y) in (x,y) plane","Is there possible way to  draw function $\vec{f}: R^2 \to R^2 $ such as like $\vec{f}(x,y) = \sin x+ \sin y, y+\sin x\ $ and other $\vec{f}(x,y)$ in $xy$ -corordinate instead of $xyz$ plane. Is it okay to do it on some online graphers like symbolab and desmos? Thank a lot!","['visualization', 'functions', 'vectors']"
4138619,Let $|G| = 20$ and $G$ has only two elements of order $4$. Then $G$ is cyclic,"Let $|G| = 20$ and $G$ has only two elements of order $4$ . Then $ G$ is cyclic. I was trying to prove this assertion and I was given some hints also. But firstly I don't know about sylow theorems yet. Definition: Let $G$ be a group and $|G|=p^n.q$ , where $(p,q)=1$ . Then every subgroup of order $p^n$ is called sylow $p$ -subgroup. Theorem (Sylow's III):
If $N_p$ is the number of sylow $p$ - subgroup of a finite group $G$ ( $|G|= p^n.q$ , where $(p,q)=1$ ). Then $N_p = 1+kp$ , where $k>=0$ and $N_p$ divide $q$ . $( N_p\mid q)$ . Theorem: If there exists only one sylow $p$ -subgroup for each prime $p$ , which divides $|G|$ . Then $G$ is direct product of those sylow $p$ -subgroup. My Attempt : Using these theorems I got to the point to prove that there's only one sylow $5$ -subgroup and it is of order $5$ . Again, for sylow $2$ -subgroups : $$ N_2 = (1+2k)\mid 5$$ Therefore, $k = 0$ or, $2$ If $k=2$ , then there's only $5$ subgroups of order 4 say $H_1,H_2,H_3,H_4,H_5$ .
Now, given that there's only two elements of order 4.
Let, $|a|=4=|b|$ Then, $\langle a\rangle=\langle b\rangle $ otherwise resulting two more distinct elements $a^3,b^3$ of order 4. Now here I came to a dead end. I know that if I can prove that $k=2$ results in contradiction then my proof is done. But I can't find any ways from here. Any suggestions?","['cyclic-groups', 'finite-groups', 'abstract-algebra', 'sylow-theory', 'group-theory']"
4138647,Proof verification for simple derivative property,"Given a function $f:(a, b)\to \mathbb{R},\space f\in C^1$ , show that $f'(x_0) =  c > 0, \space x_0\in (a,b)$ implies that there exists some $x_1\in (x_0-\delta,x_0)$ s.t. $f(x_1) < f(x_0)$ and some $x_2\in (x_0, x_0 +  \delta)$ s.t. $f(x_2) > f(x_0)$ . Proof: We have that $\lim\limits_{x\to x_0}\dfrac{f(x)-f(x_0)}{x-x_0} = c$ meaning for $\epsilon= c$ there exists some $\delta > 0$ s.t. for all $x\in (x_0 -\delta, x_0 + \delta)$ we have: $-\epsilon (x-x_0)  < f(x)-f(x_0)-c(x-x_0) < \epsilon(x-x_0)$ $\Longleftrightarrow -\epsilon (x-x_0)  + f(x_0) + c(x-x_0) < f(x) < \epsilon (x-x_0) + f(x_0) + c(x-x_0) $ So when choosing $x_1\in (x_0-\delta, x_0):$ $f(x) < f(x_0)$ and for $x_2\in(x_0, x_0 + \delta): f(x_0) < f(x)$ because $c >0$ . Is this proof correct? What would I have to  adjust when dealing with $f'(x_0) = \infty$ ?","['solution-verification', 'derivatives', 'real-analysis']"
4138692,Upper bound on the largest eigenvalue of a simple graph,"Let $X$ be a simple graph with $n$ vertices and $e$ edges and let $\lambda$ be an eigenvalue of $X$ . Show that $$ |\lambda| \leq \sqrt\frac{2e(n-1)}{n}$$ which is equivalent to $$n\lambda^{2} \leq 2e(n-1)$$ I've been given an hint to consider the equality $\mathrm{tr}(A^{2}) = 2e$ , where $A$ is the adjacency matrix of $X$ , which I've already proved, and to use Cauchy-Schwarz inequality from Calculus. I tried to, but I can't figure out which values I'm supposed to consider in that inequality to prove the previous equivalent form. I also know that $\mathrm{tr}(A^2)$ is the sum of the squares of all eigenvalues of $A$ , so that it suffices to show that inequality for the largest module of an eigenvalue. I would appreciate if someone helps me here.","['eigenvalues-eigenvectors', 'graph-theory', 'adjacency-matrix', 'matrices', 'combinatorics']"
4138742,Help me find error in ODE for sensitivity analysis of parameters of Lotka-Voltera equation,"I have a Lotka-Voltera model on which i want to perform parameter estimation by calculating the gradients of the parameters using an extended ODE system. I know there are different methods for doing that but i choose to ignore those. is a Lotka-Voltera model For it derived the Forward sensivity equations using: $$\dot{s}_{i,j}  = \frac{df_j}{dy_i} s_{i,j} + \frac{df_j}{p_i} \text{ where } \dot{y}=f(y,p_1, \ ..., p_I ,t), \ i \in \{1,...,I\}, y \in \mathbb{R}^J, j \in \{1,...,J\}   $$ So $s_{i,j}$ would be the sensitivity of the $j$ -th component of solution on parameter $i$ . This gave me the following extension of the system of differential equations: I can solve the Lotka-Voltera equations for intital conditions using: function  lotka_voltera(u,p,t)
    N₁, N₂ = u
    ϵ₁, ϵ₂, γ₁, γ₂ = p # p_1 ... p_4
    return  [N₁*ϵ₁ - γ₁*N₁*N₂, -N₂*ϵ₂ + γ₂*N₁*N₂]
end

u₀ = [1.,1.];
p = [3.,1.,0.5,0.4];
t =(0., 3.);

prob = ODEProblem(lotka_voltera, u₀, t, p)
solution = solve(prob);
plot(solution) and get a plot like this: However when implemented the extension of the system of equations in Julia: function lotka_sensitivity(du, u, p, t)
    du[:,1] = lotka_voltera(u[:,1],p,t)
    N₁, N₂ = u[:,1] # ; s₁ₚ₁ s₂ₚ₁; s₁ₚ₂ s₂ₚ₂; s₁ₚ₃ s₂ₚ₃; s₁ₚ₄ s₂ₚ₄
    p₁, p₂, p₃, p₄ = p
    
    J = [(p₁-p₂*N₂) (-p₂*N₁); (p₄*N₁) (p₄*N₂-p₃)]
    du[:,2] = (J*u[:,2]) .+ [N₁, 0.]
    du[:,3] = (J*u[:,3]) .+ [-N₂*N₁, 0.]
    du[:,4] = (J*u[:,4]) .+ [0., -N₂]
    du[:,5] = (J*u[:,5]) .+ [0., N₂*N₁]
end 

u₀ₚ = hcat(u₀, zeros(2,4)) # the 2nd to fith coulouns are for the sensitivities. Those are 0 as the inital conditions u_0 are fixed and independent of p
prob_sensitivity = ODEProblem(lotka_sensitivity, u₀ₚ, t, p)

solution_sensitivity = solve(prob_sensitivity);
plot(solution_sensitivity; vars=[(1),(2),(3),#=(4),=#(5),#=(6),=#(7),#=(8),(9),(10)=#]) Then the solutions i commented out from plotting grow exponentially with time. This shouldn't be in my understanding. What did i do wrong? Possible sources of error are: Did i use the right formula for the sensitivity i want? Is my extended system correct? Is it correct that i let the sensitivities start at 0? Did i implement the ODE correctly? The complete code in a Pluto notebook which also is a standalone file for reference and also includes what dependencies are necessary . In vorschlag3 i try to calculate the integrated square error in the first component of solution compared to one with different parameters and attempt to do a gradient descent to minimize it. This also fails.","['numerical-methods', 'parameter-estimation', 'ordinary-differential-equations', 'real-analysis']"
4138752,A Law of Large Numbers for Conditional Expectations,"Let $(\Omega,\mathcal F,P)$ be a probability space, and suppose that we are given, for each $\gamma \in[0,1]$ , an iid sequence of real integrable random variables $\{X_n(\gamma)\}_{n=1}^\infty$ . Let $Y$ be a random variable taking values in $[0,1]$ which is independent from $X_n(\gamma)$ for all $n,\gamma$ . How can I show that $$\frac{1}{n}\sum_{k=1}^n X_k(Y)\to E[X_n(Y)| \sigma(Y)] \,\,\text{ as } \,\,n\to \infty$$ in probability? EDIT: Assume the following additional condition: for all $\delta>0$ we have $$\sup_{\gamma\in[0,1]} P\bigg(\bigg|\frac{1}{n}\sum_{k=1}^n \Big(X_k(\gamma)-E[X_k(\gamma)] \Big) \bigg|>\delta\bigg)\to 0 \,\,\text{ as } \,\,n\to \infty.$$","['measure-theory', 'ergodic-theory', 'law-of-large-numbers', 'probability-theory', 'probability']"
4138782,$P(X_{(n-k_n)}>X_1\mid X_1>u_n)=0$?,"Let $X_1,X_2,\dots$ be continuous random variables with full support (I need the result when they follow AR time series $X_i=\alpha X_{i-1}+\varepsilon_i$ for iid epsilons. But if you will consider iid case, it may also help with the time series case) with their stationary distribution function $F$ . Notation $X_{n; (k)}$ represents the $k-$ th order statistic, i.e. $X_{n; (1)}=\min_{i\leq n} X_i$ . Let $k_n\in\mathbb{N}$ fulfill $$k_n\to\infty, \frac{k_n}{n}\to 0 \text{, for } n\to\infty.$$ Let $$u_n=quantile(X_1, 1-\frac{k_n}{n}), \text{ i.e. } P(X_1>u_n)=\frac{k_n}{n}.$$ Is the following true $$\frac{n}{k_n}P(X_{n; (n-k_n)}>X_1>u_n)\overset{n\to\infty}{\to}0?$$ Maybe a helpful observation is that this is equal to $$
\frac{n}{k_n}P(X_{n; (n-k_n)}>X_1>u_n)=P(X_{n; (n-k_n)}>X_1\mid X_1>u_n)=1-P(\hat{F}(X_1)>1-\frac{k_n}{n}\mid F(X_1)>1-\frac{k_n}{n}).
$$","['conditional-probability', 'large-deviation-theory', 'empirical-processes', 'order-statistics', 'probability']"
4138804,What might be the (essentially) easiest possible proof of Cayley-Hamilton theorem?,"Today, I learnt that there is a surprisingly easy proof of the Cayley-Hamilton theorem:
First we assume $F$ is algebraically closed WLOG, then we treat matrices as points in $F^{n^2}$ where $n$ is the dimension of the vector space $V$ . Now, note that the condition that a matrix is annihilated by its characteristic polynomial is a zariski-closed condition, so we only need to verify this theorem for diagonal matrixes with distinct diagonal entries, which is transparent. I'm deeply impressed by this proof, not only because it's short, but also because this idea seems unbelievably natural and straight-forward once one adopted some of the most basic ideas in Algebraic Geometry. I'm looking forward to some background explanation or similarly easy-and-natural proof of the Cayley-Hamilton theorem.","['matrices', 'algebraic-geometry', 'linear-algebra', 'commutative-algebra']"
4138850,"Understanding the ""Découpage de Lévy"" (or Levy decomposition)","I am looking for someone to help me understand what exactly the ""Découpage de Lévy"" (or Levy decomposition) of random variables is and why it works. The book I am reading is ""The Central Limit Theorem for real and banach valued random variables"" by A. Araujo and E. Giné. In my opinion they are not doing a great job of explaining it. If anyone know a book that explains it in greater detail then I am very interested in hearing so! First I am not even sure I understand what they mean by a ""small variable"" and ""usually zero"". Is is it small as in bounded by a certain value? And usually zero as in zero on a set of full measure? Also what is meant by means of randomization? Then some notation is introduced as well as a lemma and proof which does at least make sense to me. They proceed to introduce $\eta$ to make the $V$ -term and $U$ -term independent. I assume this is so the sum is a convolution. What I don't understand is the last part of page 52. It is written as if it follows from just looking at those previous results that normal approximates the $U$ -term and Poisson the $V$ -term. However that realisation is hard for the see. Is it instead meant to be a foreshadowing of Theorem 3.2 on the next page in which we see exactly that these terms are in fact well-approximated? To make sure, is a random variable $X$ small in probability if $P(X>\delta)$ is small for an appropiate $\delta>0$ ? Finally is there any intuition as to why it makes sense that sums of independent random variables are a convolution of normal and poisson? I have attached the relevant to pages from their book.",['probability-theory']
4138868,What are the integer solutions to $\cos(\sqrt{n^2-1}) = \frac{1}{n}$?,"$$\cos(\sqrt{n^2-1}) = \frac{1}{n}$$ I was wondering if there existed integer solutions to to this equation apart from n=1. I've thought that there are probably no more solutions, because RHS is rational and I believe that LHS is not rational in general. I've found theorems about the output of the cosine when the argument is rational, but not for when it is irrational. (Niven's theorem).
How could this be solved?","['trigonometry', 'irrational-numbers', 'diophantine-equations']"
4138871,Summation over $a+b+c=5$,"Let $a,b,c$ be positive integers. Compute $$\sum_{a+b+c=5} (6-a)(6-b)(6-c).$$ The first thing I notice is symmetry, so that I can permute $3!=6$ ways, but i'm not really sure how that works with the condition $a+b+c=10.$ The other method is to fix $a$ , but that is reall time-consuming and unfeasible if say $a+b+c=20.$ Is there a clever method to evaluate this sum? I would like to have a generalized method, please.","['summation', 'discrete-mathematics', 'generating-functions']"
4138898,What is it to solve an equation forward?,"I'm reading a book in Monetary Economics and I don't understand a step. I have this expression: $$ \dfrac{\lambda_{t}}{P_{t}} = \beta \left( \dfrac{\lambda_{t+1} + \mu_{t+1}}{P_{t+1}} \right) $$ And then it says ""solving this equation forward implies that"": $$ \dfrac{\lambda_{t}}{P_{t}} = \sum_{i=1}^{\infty} \beta^{i} \left( \dfrac{\mu_{t+i}}{P_{t+i}} \right) $$ I dont't know what they're doing. What is it to solve an equation forward?","['ordinary-differential-equations', 'finance', 'recurrence-relations', 'economics', 'problem-solving']"
4138922,Why do $\sum_{i=0}^3 (-2^i)$ and $\sum_{i=0}^3 ((-2)^i)$ give different values?,"On my TI-84, I have noticed something weird.
I have: $$\sum_{i=0}^3 (-2^i) = -15$$ and $$\sum_{i=0}^3 ((-2)^i) = -5$$ Does anybody know why these sum to a different number? Correct me if I am wrong but isn't $-2^i$ the same as $(-2)^i$ ?","['notation', 'algebra-precalculus', 'summation']"
4138927,Markov chain with exponentially decreasing transition probabilities,"I'm currently trying to figure out the steady-state vector of a particular infinite-state Markov chain. The chain has (countably) infinitely many states $S_0, S_1, \dots, S_i, \dots$ , such that for all $i$ : $$
\begin{align}
 \mathbb{P}(S_i \to S_i) &= 0 \\
 \mathbb{P}(S_i \to S_{i+1}) &= \frac{1}{2^i} \\
 \mathbb{P}(S_i \to S_{i-1}) &= 1-\frac{1}{2^i}.
\end{align}
$$ So far, all I can tell that if $\mathbf{p}$ is the steady-state vector for this chain, then $\lim_{i\to\infty} \mathbf{p}_i = 0.$ I know that for a finite-state Markov chain, the rows of $P^\infty$ are equal to the steady-state vector — I'm assuming that this holds for a Markov chain with infinitely many states as well? For Markov chains with a small number of states (i.e. two or three), I can usually find $P^n$ by hand with diagonalization, but in this case I feel like there should be an easier way. How should I approach problems like this, and how would I find $\mathbf{p}$ in this case?","['stochastic-processes', 'markov-chains', 'probability']"
4138946,Proving a statement using the information about function's derivatives.,"Function $u(x,y)$ is called a harmonic function if it has partial derivatives from the first degree ( $f'_x , f'_y)$ and second degree ( $f''_{xx}, f''_{xy}, f''_{yx}, f''_{yy}$ ) that are continuous, and for every $(x,y)$ this equation holds: $\frac{\partial ^2u}{\partial x^2}+ \frac{\partial ^2u}{\partial y^2} =0$ Prove: if $u(x,y)$ is a harmonic function and $f(u)\in C^2, (u\ne Constant)$ (I'm not sure of the notation but from the question I can understand it means that partial derivatives are continuous up to second degree), and the function $z(x,y)=f(u(x,y))$ is harmonic function, then $f(u)=Au + B$ . Before adding my work and attempt, I have to say I haven't really understood what $A,B$ are in what I needed to prove, I don't know if it's a mistake in the question or just me not understanding, but I decided to look at them as constants because it's the only thing I thought of that they could be. My attempt : All what I have tried is to take derivatives and find conclusions and try to use the information that I got about harmonifc functions: Updated $z_x=f'(u)u_x$ $z_y=f'(u)u_y$ $z_{xx}=f''(u)u_x^2 + f'(u)u_{xx}$ $z_{yy}=f''(u)u_y^2 + f'(u)u_{yy}$ Now given that $z$ is harmonic: $z_{xx}+z_{yy} = 0$ $f''(u)u_x^2 + f'(u)u_{xx} + f''(u)u_y^2 + f'(u)u_{yy}=f'(u)[u_{xx}+u_{yy}]+f''(u)[u_{x}^2+u_y^2]$ And since $u$ is harmonic we get $=f''(u)[u_x^2+u_y^2]=0$ I'm not sure how to continue since I can't see any problem in $f''(u)=0$ or $u_x^2+u_y^2=0$ . BUT I know one thing, if I get that $f''(u)=0$ that means $f'(u)=c$ where $c$ is a constant, which means $f(u)=Au + B$ . Update 2 if $u_x^2 + u_y^2 = 0$ , then $ u_x=0 and u_y=0$ , and that means $u=Const$ , which contradicts the information that $u\ne Const$ . The idea / approach I have in my mind is to find $f'(u)=A$ (some constant), and then I will have that $f(u)=Au+B$ . Any help is really appreciated, thanks in advance.","['partial-derivative', 'multivariable-calculus', 'derivatives']"
4138973,Problem with normal derivative,"I have a problem with normal derivative of given function $h(x)=e^{-ar^{2}}-e^{-ar^{2}_{0}}$ and showing that $L(h)>0$ , where $L$ is elliptic operator. $L=\sum_{i,k}a_{ik}\frac{\partial^2u}{\partial x_i \partial x_k}+\sum_{i}b_{i}(x) \frac{\partial u}{\partial x_i} \ (a_{ik},  b_i - \text{continuous})$ I found in the book that the normal derivative of $h$ is greater than $0$ , but I don't know how to show that. I did compute a gradient which is equal $-2ae^{-ar^{2}}\vec{x}$ , so the normal derivative is $-2ae^{ar^{2}}\vec{x} \cdot \vec{n}$ and as I see $\vec{x} \cdot \vec{n}$ should be negative but I don't know why. $L(h)=(\sum_{i,k}4a^2a_{ik}x_ix_k-\sum_{i}2ab_{i} x_i)e^{-ar^{2}}$ $L(h)$ should be greater than $0$ , but why?","['analysis', 'partial-differential-equations']"
4138976,Solving a second-order matrix differential equation - periodic solutions,"Let $\frac{d^2}{dt^2}x=\begin{pmatrix}1 &1 \\ 0 &a\end{pmatrix}x$ . For which $a \in {\mathbb{R}}$ there exist periodic solutions? I think only for $a<0$ there can be periodic solutions because then we get non-real eigenvalues. So we have $$\frac{d^2}{dt^2}x_2=ax_2. \tag{1}\label{eq1}$$ $P(\lambda)=\lambda^2-a=0$ so $\lambda_1=-\sqrt{-a}i$ , $\lambda_2=\sqrt{-a}i$ . Each function $e^{-\sqrt{-a}ti}$ , $e^{\sqrt{-a}ti}$ is a solution to (\ref{eq1}) so their linear combination is also a solution to (\ref{eq1}). So we have $$x_2(t)=E_1\cos(\sqrt{-a}t)+E_2\sin(\sqrt{-a}t).$$ We put $x_2$ to $$\frac{d^2}{dt^2}x_1=x_1+x_2, \tag{2}\label{eq2}$$ and we have $\frac{d^2}{dt^2}x_1-x_1=E_1\cos(\sqrt{-a}t)+E_2\sin(\sqrt{-a}t)$ . We look for a solution in a form: $C\cos(\sqrt{-a}t)+D\sin(\sqrt{-a}t)$ . By putting it to the (\ref{eq2}) we get that $C = E_1/(a-1)$ , $D = E_2/(a-1)$ . And the solution of homogeneous equation of (\ref{eq2}) is $D_1e^t+D_2e^{-t}$ . So we have $$x_1(t)=\frac{E_1}{a-1}\cos(\sqrt{-a}t)+\frac{E_2}{a-1}\sin(\sqrt{-a}t) + D_1e^t+D_2e^{-t}.$$ So the periodic solutions exist when $a<0$ . and I think whenever $D_1$ or $D_2$ $=0$ there are such solutions. Is it a good approach to this problem?",['ordinary-differential-equations']
4138989,A question involving Lindeberg-Levy CLT,"Suppose $X_k$ are i.i.d. random variables with $E(X_1)=0$ and $0< E(X_1^2)<\infty$ . Define $$S_n=\sum_{k=1}^nX_k$$ I want to show that $\liminf_{n\rightarrow \infty}S_n=-\infty$ and $\limsup_{n\rightarrow \infty}S_n=\infty$ almost surely.
Now I know I am supposed to use Lindeberg-Levy CLT, which states that $$\frac{1}{\sqrt{n}}\sum_{k=1}^n X_i\xrightarrow{D}N(0,E(X_1^2))$$ in this specific case. However, I am having trouble using the convergence in distribution to show that $$P(\omega\in\Omega:\liminf_{n\rightarrow \infty}S_n=-\infty)=1$$ A hint would be appreciated.","['central-limit-theorem', 'weak-convergence', 'stochastic-analysis', 'stochastic-processes', 'probability-theory']"
4139047,"Let $U$ and $V$ be unitary matrices and $A$ a positive definite matrix , for which $AU = VA$. Show that...","a) Show that $UA  = AV$ . b) Show that $VA^2  = A^2V$ . c) Show that $VA  = AV$ . d) Show that $U = V$ . I did a, b and d, but I'm having trouble with c. I would appreciate some help. a) $AU  = VA => (AU)^*  = (VA)^* => U^*A^* = A^*V^*$ , since A is positive definite $A = A^*$ , so $U^*A = AV^* => UU^*AV = UAV^*V$ , since $U$ and $V$ are unitary $ UU^* = I$ and $V^*V = I$ so $AV = UA$ . b) $AU  = VA => AUA = VAA$ , since $UA = AV => AAV = VAA => A^2V = VA^2$ . d) $AU  = VA$ and $VA  = AV$ so $AU = AV$ . Since A is positive definite it has an inverse, so $A^{-1}AU = A^{-1}AV => U = V$ .","['matrices', 'unitary-matrices', 'linear-algebra', 'positive-definite']"
4139077,Cardinality of a simple set of integer tuples,"Let $k, m\in\mathbb{N}$ be two integers with $m\geq k$ (agree that $0\notin\mathbb{N}$ ). Consider the set $$A_{m|k}^\neq:=\{(i_1, \ldots, i_k) \in \mathbb{N}^{\times k} \mid  \ 
 i_1,\ldots,i_k \ \text{ pairwise distinct } \ \text{ s.t. } \ \ i_1+\ldots+i_k=m\}$$ of $k$ -tuples with pairwise distinct integer entries whose $\ell_1$ -norm is $m$ . Question: Do you know a formula for the cardinality of $a_{m|k}$ of $A_{m|k}^\neq$ and whether these numbers have a name? Remark: The cardinality of the superset $A_{m|k}:=\{(i_1,\ldots,i_k)\in\mathbb{N}\mid i_1+\ldots + i_k = m\}$ is $\binom{m-1}{k-1}$ , and I'm interested in estimates concerning the quotients $q_{m|k}:=\frac{|A_{m|k}^\neq|}{|A_{m|k}|}$ as $m\rightarrow\infty$ .","['graph-theory', 'elementary-number-theory', 'extremal-combinatorics', 'combinatorics', 'discrete-mathematics']"
4139078,Finding extrema of a function of multiple variables,"I had to find extrema of the following function: \begin{align} z=x^3y^2(2-x-y)\end{align} I identified first order partial derivatives, which are: \begin{align} z_x^{'}=y^2(3x^2(2-x-y)-x^3) 
\\\\
z_y^{'}=x^3(2y(2-x-y)-y^2)\end{align} I then equated the two expressions to $0$ to identifiy critical points for further analysis (points which would be checked for extrema, in my case, by the means of cunstructing and evaluating a Hessian matrix).
The solutions for the system of equations I contrived were $y=0; x=0; x=1, y=2/3$ .
However, as I was solving the system and going through all the possibilities, I got some roots that were special cases of the $y=0, x\in{\Bbb{R}}$ and $x=0, y\in{\Bbb{R}}$ solutions. Here is an example: \begin{cases}
&3x^2(2-x-y)-x^3=0 \\
&x^3(2y(2-x-y)-y^2)=0 \\ \end{cases} \begin{align}x_1=0,y_1=2\\\\x_2=3/2, y_2=0\\\\x_3=1, y_3=2/3 \end{align} Are those points ( $x_1=0,y_1=2; x_2=3/2, y_2=0$ ) critical? If not, then what's the gist here, in what way are those points special? Thank you!","['maxima-minima', 'multivariable-calculus']"
4139094,Automorphisms of $\mathbb{R^*}$ as a group,"I was just thinking randomly about groups and cardinality of sets then I thought of the problem What will be the cardinality of the group of all automorphisms of the multiplicative group $\mathbb{R^*}$ ? My Attempt: At first I thought that there's only the identity automorphism and I also got some knowledge about it from few questions over here but then I realised that the questions here was about Ring automorphisms mainly.
Then I thought if $f(x) = x^{2n+1}$ where $n \in \mathbb{N}$ . Then $f$ is automorphism. So I could say that $Aut(\mathbb{R^*})$ is at least countably Infinite but then I couldn't go any further. Can anyone give me some ideas about it?
I'd prefer if it involves simple group theory approaches.
It's okay if it's not being possible to do so. Edit : I was also looking for functions $f$ satisfying $f(xy)=f(x)f(y)$ . But as I can't say anything about $f$ rather than it's bijection and preserves group structure, I could not find any form of $f$ .","['automorphism-group', 'group-theory', 'abstract-algebra', 'real-analysis']"
4139103,Understanding Young's Convolution Inequality and its relation to Convex Bodies,"On Pg. 34 of this reference , I encountered Young's Convolution Inequality . The author states the inequality and manipulates it into various forms. I write this post to better understand the manipulations of Young's Convolution Inequality into different forms, and what the author is trying to achieve by doing so. I shall reproduce the text from the reference below (as quotes), and ask my questions inline. It would be greatly appreciated even if you can help answer some of the following questions, if not all. Thank you! If $f$ and $g:\mathbb R\to \mathbb R$ are bounded, integrable functions, the convolution $f * g$ of $f$ and $g$ is defined by $$f*g(x) = \int_\mathbb R f(y) g(x-y) dy$$ Young's Convolution Inequality: If $f \in L_p$ , $g\in L_q$ , and $$\frac 1p + \frac1q  = 1 + \frac1s$$ then $$\|f * g\|_s\le \|f\|_p \|g\|_q$$ Once we have Young’s inequality, we can give a meaning to convolutions of functions that are not both
integrable and bounded, provided that they lie in the correct $L_p$ spaces. Q1. What is meant by ""correct"" $L^p$ spaces here? Young’s inequality holds for convolution on any locally compact group, for example the circle. On compact groups it is sharp: there is equality for constant functions. But on $\mathbb R$ , where constant functions are not integrable, the inequality can be improved (for most values of $p$ and $q$ ). Q2. What is meant by compact groups and locally compact groups ? Wikipedia says:  (i) a compact (topological) group is a topological group whose topology is compact and (ii) a locally compact group is a topological group G for which the underlying topology is locally compact and Hausdorff. However, I am able to see which groups we are talking about in this context. It was shown by Beckner [1975] and Brascamp and Lieb [1976a] that the correct constant in Young’s inequality is attained if $f$ and $g$ are appropriate Gaussian densities: that is, for some positive $a$ and $b$ , $f(t) = e^{−at^2}$ and $g(t) = e^{−bt^2}$ . (The appropriate choices of $a$ and $b$ and the value of the best constant for each $p$ and $q$ will not be stated here. Later we shall see that they can be avoided.) Q3. What is meant by the correct constant in Young's inequality? First of all, where is the constant? Is it present in some other form of the inequality, that has not been stated here? Pretty confused about this. How are convolutions related to convex bodies? To answer this question we need to rewrite Young’s inequality slightly. If $1/r+1/s = 1$ , the $L_s$ norm $\|f ∗g\|_s$ can be realized as $$\int_\mathbb R (f*g)(x)h(x)$$ for some function with $\|h\|_r = 1$ . Q4. How was the inequality rewritten into the above format? It'd be great if someone could provide details because I'm unable to see how and what just happened. So, the inequality says that if $1/p + 1/q + 1/r = 2$ , then $$\int \int f(y) g(x-y) h(x) dy dx\le \|f\|_p \|g\|_q \|h\|_r$$ We may rewrite the inequality again with $h(-x)$ in place of $h(x)$ , since this doesn't affect $\|h\|_r$ . $$\int \int f(y) g(x-y) h(-x) dy dx\le \|f\|_p \|g\|_q \|h\|_r$$ Q5. What is the point of replacing $x$ by $-x$ ? What are we gaining? This can be written in a yet more symmetric form, with the help of the map $\phi: \mathbb R^2 \to \mathbb R^3$ such that $\phi: (x,y) \to (y,x-y,-x) := (u,v,w)$ . The range of $\phi$ is the subspace $H$ of $\mathbb R^3$ , given by $$H = \{(u,v,w): u+v+w = 0\}$$ Besides the factor coming from the Jacobian, the integral can be written as $$\int_H f(u) g(v) h(w)$$ where the integral is with respect to the two-dimensional measure on the subspace $H$ . Q6. What are we gaining by rewriting the integral in the form (except that it looks nicer)? Also, does the Jacobian contribute to the constant factor that is being talked about in several places? Q7. Which measure on $H$ are we talking about, explicitly? So Young’s inequality and its sharp forms estimate the integral of a product function on $\mathbb R^3$ over a subspace. What is the simplest product function? If $f, g$ , and $h$ are each the characteristic function of the interval $[−1, 1]$ , the function $F$ given by $$F(u,v,w) = f(u)g(v)h(w)$$ is the characteristic function of the cube $[−1, 1]^3 ⊂ \mathbb R^3$ . The integral of $F$ over a subspace of $\mathbb R^3$ is thus the area of a slice of the cube: the area of a certain convex body. So there is some hope that we might use a convolution inequality to estimate volumes. Q8. What is the sharp form of Young's inequality that the author talks about here? How is it sharp ?","['measure-theory', 'harmonic-analysis', 'convex-geometry', 'analysis', 'convex-analysis']"
4139141,Existence and Uniqueness theorem as it applies to finding an explicit solution,"If the conditions of the theorem are met for some ordinary differential equation, then we are guaranteed that a solution exists. However, I don't fully understand what it means for a solution to exist. If we can show that a solution exists, does that mean that it can be found explicitly using known methods? Or, are there some differential equations, that we know exist because of the theorem, but for which we can not find a general solution, and are thus forced to use numerical methods for an approximation?",['ordinary-differential-equations']
4139205,Rigorous justification for this conditional probability inequality?,"Let $(\Omega,\mathcal F,P)$ be a probability space. Consider a collection of bounded real random variables $X(\gamma)$ , for $\gamma\in[0,1]$ , defined on this probability space. Let $(\gamma_i)_{i=1}^\infty$ be a sequence of iid random variables taking values in $[0,1]$ . The family $\{\gamma_i : i \in \mathbb{N}\}$ is assumed to be independent of the family $\{X(\gamma) : \gamma \in [0,1]\}$ . Consider the following inequality for a given $\delta>0$ : $$\sup_{i\in\mathbb{N}}P\bigg(\Big|X(\gamma_i)-E[X(\gamma_i)|\sigma(\gamma_i)]\Big|>\delta \bigg | \sigma(\gamma_i) \bigg)\leq \sup_{\gamma \in [0,1]}P\bigg(\Big|X(\gamma)-E[X(\gamma)]\Big|>\delta \bigg) .$$ It seems to me that this inequality is true, since once we condition on $\gamma_i$ , the $i$ th probability on the left must appear on the right as well. But how to show it rigorously? Any help on this is very appreciated. EDIT: In fact I would be happy if someone could just give a rigorous proof of the statement $$E[X(\gamma_i)|\sigma(\gamma_i)(\omega)=E[X(\gamma_i(\omega)) ,\quad \omega\in\Omega.$$","['measure-theory', 'conditional-probability', 'conditional-expectation', 'probability-theory', 'probability']"
4139261,Discrepancy of answers between differing computations of $E[e^{W_s}e^{W_t}]$ ($W_t$ being the Wiener process),"I was looking at another thread , and the following two distinct solutions to $E[e^{W_s}e^{W_t}]$ (assume that $W_0 = 0$ and $t>s$ ) were given, with both giving identical answers (I have slightly re-written the notation in the second solution for the reader's sake): 1. by direct computation I find: $$
\begin{array}
\mathbb{E}[e^{W_t}e^{W_s}] &= \mathbb{E}[e^{W_t - W_s} e^{2W_s}] \\
 &= \mathbb{E}[e^{W_t - W_s}]\mathbb{E}[e^{2W_s}] \\
 &= e^{\frac12(t-s)} e^{2s} \\
 &= e^{\frac12 t + \frac32 s}.
\end{array}
$$ Given the facts that for $X_t = e^{W_t}$ , $(a)$ $e^{-\frac t2}X_t$ is a martingale and $(b)$ For any constant $\lambda \in\mathbb R$ , the process $Y_t= e^{\lambda W_t - \frac12 \lambda^2t}$ is a martingale, we have \begin{align}
E[e^{W_s}e^{W_t}] &= e^{\frac{t}{2}}E[e^{W_s}e^{W_t - \frac{t}{2}}] \\
&= e^{\frac{t}{2}}E[e^{W_s}E[e^{W_t - \frac{t}{2}}|\mathcal{F}_s]]
\quad \text{$W_s$ is $\mathcal{F}_s$-mesurable and thanks to a)}\\ 
&= e^{\frac{t}{2}}E[e^{2W_s - \frac{s}{2}}] \\
&= e^{\frac{t+3s}{2}}E[e^{2W_s - 2s}] \quad \text{thanks to b) with $\lambda=2$}\\
&= e^{\frac{t+3s}{2}}\\
\end{align} On the other hand, I took quite a different approach, getting a different answer (note that I write $\exp(x)$ instead of $e^x$ ): Let $Z \sim N(0,1)$ . Then its moment generating function is, \begin{align*}
E[\exp{(\lambda Z)}] &= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty} \exp\left({\lambda z})\exp({-z^2/2}\right)dz \\
&= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty}\exp\left({(-z^2 + 2\lambda z)/{2}}\right)dz \\
&= \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{\infty}\exp\left({{-(z + \lambda)^2}/{2} + {\lambda^2}/{2}}\right)dz \\
&= \exp\left({\frac{\lambda^2}{2}}\right)\int_{-\infty}^{\infty}\frac{\exp\left({{-(z + \lambda)^2}/2}\right)}{\sqrt{2 \pi}}dz .
\end{align*} Since the function inside the last integral is precisely the pdf of $N(\lambda , 1)$ , its integral equals 1. Therefore we have, $$E[\exp{(\lambda Z)}] = \exp\left({\frac{\lambda^2}{2}}\right).$$ Now, \begin{align*}
E[\exp\left({W_s}\right)\exp\left({W_t}\right)] &= E[\exp\left({W_s + W_t}\right)] \\
&= E[\exp\left({(\sqrt{s} + \sqrt{t})Z}\right)] \text{ since $W_t \sim N(0,t)$}\\
&= \exp\left({\frac{(\sqrt{s} + \sqrt{t})^2}{2}}\right) \\
&= \exp\left( \frac{s + t + 2\sqrt{st}}{2} \right). \\
\end{align*} Since $t > s$ , we can write $t = a s$ for some $a > 1$ . Hence, \begin{align*}
E[\exp\left({W_s}\right)\exp\left({W_t}\right)] &= \exp\left( \frac{s + t + 2\sqrt{s(as)}}{2} \right) = \exp\left( \frac{s + t + 2s\sqrt{a}}{2} \right) \\
&> \exp\left( \frac{s + t + 2s \cdot \sqrt{1}}{2} \right) = \exp\left( \frac{t + 3s}{2} \right),
\end{align*} and hence the discrepancy is clear (with the two answers being equal only if $s = t$ ). Naturally, my question is where my mistake is. Thanks!","['brownian-motion', 'probability-theory', 'probability', 'stochastic-calculus']"
4139278,"Find the curvatures intrinsic, mean and extrinsic of a half-plan in hyperbolic metric","I'm trying to solve this exercise: Let $\mathbb{H}^3=\{(x,y,z)\in \mathbb{R}^3: z>0\}$ with a hyperbolic metric $g_{ij} = \dfrac{\delta_{ij}}{z^2}$ . Use the mobile referential method to find the curvatures (intrinsic, mean and extrinsic) of a open half-plan whose boundary it's in the plan $\{z=0\}$ and whose normal (pointing up) forms an angle $\theta$ with a vertical direction (pointing up). Well, I used the referential: $$\begin{cases} 
t_1= z(1,0,0) \\
t_2 = z(0,-\cos(\theta), \sin (\theta)) \\
t_3 = z(0, \sin (\theta), \cos (\theta)) \end{cases}$$ And I get: $$ w_1=\dfrac{dx}{z}, \quad w_2= -\dfrac{\cos(\theta)}{z}dy+\dfrac{\sin(\theta)}{z}dz,$$ $$ w_3= \dfrac{\sin(\theta)}{z}dy+\dfrac{\cos(\theta)}{z}dz, \quad w_{12} = \dfrac{\sin(\theta)}{z}dx$$ But when I do $dw_{12} = -k w_1 \wedge w_2$ it doesn't work PS: $w_1 \wedge w_2$ denotes the exterior product.","['riemannian-geometry', 'curvature', 'hyperbolic-geometry', 'differential-forms', 'differential-geometry']"
4139312,Why does $G$ have to be a finite group? (Sylow theory question),"I am trying to show that if $G$ is a finite group, and $P$ is a normal Sylow $p$ -sugroup of $G$ ,  then $P$ is characteristic in $G$ . I think this is pretty straightforward: Since $P$ is a normal Sylow $p$ -subgroup of $G$ , it must be the unique Sylow $p$ -subgroup of $G$ .  So, if $\sigma\in \operatorname{Aut}(G)$ , we have $\sigma(P)=P$ , since automorphisms of $G$ permute Sylow $p$ -subgroups of $G$ .  Thus, $P$ is characteristic in $G$ .  My question is (assuming I didn't miss a detail in the proof), where does the finiteness of $G$ come into play?","['group-theory', 'abstract-algebra', 'finite-groups', 'sylow-theory']"
4139313,How to write a limit in terms of finite summation,"I managed to find $$\int\limits_0^\infty \frac{\ln^{2a}(x)\ln(1+x)}{\sqrt{x}(1+x)}\mathrm{d}x=-\pi\lim_{m\to \frac12 }\frac{d^{2a}}{d m^{2a}} \frac{\psi(1-m) + \gamma}{\sin(m\pi)}.$$ To show this relation, we follow the same approach here : Reduce $n$ by $m$ in the beta function: $$\int_0^\infty\frac{x^{m-1}}{(1+x)^{m+n}}\mathrm{d}x=\operatorname{B}(m,n)=\frac{\Gamma(m)\Gamma(n)}{\Gamma(m+n)},$$ we have $$\int_0^\infty\frac{x^{m-1}}{(1+x)^{n}}\mathrm{d}x=\frac{\Gamma(m)\Gamma(n-m)}{\Gamma(n)}.$$ Differentiate both sides $2a$ times with respect to $m$ and once with respect to $n$ , \begin{gather*}
\frac{\partial^{2a}}{\partial m^{2a}} \frac{\partial}{\partial n}  \frac{\Gamma(m)\Gamma(n-m)}{\Gamma(n)}=\frac{\partial^{2a}}{\partial m^{2a}} \frac{\partial}{\partial n}\int\limits_0^\infty \frac{x^{m-1}}{(1+x)^n}\mathrm{d}x\\
\{\text{use differentiation under the integral sign theorem}\}\\
=\int\limits_0^\infty \frac{\partial^{2a}}{\partial m^{2a}} \frac{\partial}{\partial n}\frac{x^{m-1}}{(1+x)^n}\mathrm{d}x\\
=-\int\limits_0^\infty \frac{\ln^{2a}(x)\ln(1+x)x^{m-1}}{(1+x)^n}\mathrm{d}x.
\end{gather*} Now take the limit on both sides letting $m\to 1/2$ and $n\to1$ , \begin{gather*}
-\int\limits_0^\infty \frac{\ln^{2a}(x)\ln(1+x)}{\sqrt{x}(1+x)}\mathrm{d}x=\lim_{\substack{m\to 1/2 \\ n \to 1}}\frac{\partial^{2a}}{\partial m^{2a}} \frac{\partial}{\partial n} \frac{\Gamma(m)\Gamma(n-m)}{\Gamma(n)}\\
=\lim_{\substack{m\to 1/2 \\ n \to 1}}\frac{\partial^{2a}}{\partial m^{2a}}\Gamma(m)\left( \frac{\partial}{\partial n} \frac{\Gamma(n-m)}{\Gamma(n)}\right)\\
=\lim_{\substack{m\to 1/2 \\ n \to 1}}\frac{\partial^{2a}}{\partial m^{2a}} \Gamma(m)\left(\frac{\Gamma(n-m)[{\psi}(n-m) -\psi(n)]}{\Gamma(n)}\right)\\
\{\text{evaluate the limit when $n\to 1$ and use $\psi(1)=-\gamma$}\}\\
=\lim_{m\to 1/2 }\frac{\partial^{2a}}{\partial m^{2a}} \Gamma(m)\Gamma(1-m)[\psi(1-m) + \gamma]\\
\left\{\text{use  $\Gamma(m)\Gamma(1-m)=\frac{\pi}{\sin(m\pi )}$}\right\}\\
\left\{\text{and write $\frac{\partial}{\partial m}$ as $\frac{d}{dm}$, since we have one variable left}\right\}\\
=\pi\lim_{m\to \frac12 }\frac{d^{2a}}{d m^{2a}} \frac{\psi(1-m) + \gamma}{\sin(m\pi)}.
\end{gather*} The question here is can we write the limit in terms of finite summation?","['integration', 'digamma-function', 'calculus', 'limits', 'derivatives']"
4139342,"How to prove $\lim_{h\rightarrow 0} \frac{\exp\left\{\int_t ^{t+h}A(s)\,\mathrm{d}s\right\}-I_n}{h}=A(t)$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $t\mapsto A(t)\in C(\mathbb{R},M_n(\mathbb{R}))$ , it seems ""obvious"" that we should have: \begin{equation}
\lim_{h\rightarrow 0} \frac{\exp\left\{\int_t
^{t+h}A(s)\,\mathrm{d}s\right\}-I_n}{h}=A(t)\end{equation} However, I'm having a hard time finding rigorous proof of this result. I don't even know where to start?","['limits', 'matrix-calculus', 'derivatives', 'matrix-exponential']"
4139374,How to prove injectivity and surjectivity of the map $\phi:S^{1}\times S^{2} \to SO(3)$,"This is the exercise (6.14) of Greenberg-Harper Algebraic Topology book. I already showed continuity but I have problems for shows injectivity and surjectivity. For injectivity, if I assume $R_\theta \circ A(x)=R_{\theta^{\prime}} \circ A(x^\prime)$ (with $-\pi\leq \theta <\pi$ ) then $R_\theta \cdot A(x)=R_{\theta^{\prime}} \cdot  A(x^\prime)$ (as matrix). But I dont Know how to conclude that $\theta=\theta^{\prime}$ and $x=x^\prime$ . Other form to show its, is defining the inverse of map. I think to define $\psi: SO(3)\to S^{1}\times S^{2}$ by $T \mapsto (T\cdot (A(T(e_1))^{-1}, T(e_1))$ (Here $T(e_1)$ denotes first column of $T$ ) and see if $T\cdot (A(T(e_1))^{-1}$ is the form $$T\cdot (A(T(e_1))^{-1}=\begin{bmatrix} 1 & 0 & 0 \\ 0 & \cos \theta &-\sin \theta\\ 0& \sin \theta 
&\cos \theta \end{bmatrix}$$ with finality to identify $T\cdot (A(T(e_1))^{-1}$ with an angle. But, I dont know if a matrix of $T\cdot (A(T(e_1))^{-1}$ has this form. I tried to prove its. Is easy see that $A((T(e_1))^{-1} \cdot T$ has a form described above. But what I want is that $T\cdot (A(T(e_1))^{-1}$ but  to have the form described above. I also tried to do the same by taking $T(e_1)$ as the first row instead of the first column but still can't see that it has the required form.","['vector-fields', 'general-topology', 'algebraic-topology']"
4139399,Local maximum uniqueness of the logarithm of the Student-t distribution,"The negative logarithm of the Student-t distribution partial density function is $$f(\nu,x) := -\ln\Gamma\left(\frac{\nu+1}{2}\right)
            +\ln\Gamma\left(\frac{\nu}{2}\right)
            +\frac{1}{2}\ln(\pi\nu)
            +\frac{\nu+1}{2}\ln\left(1+\frac{x^2}\nu\right)$$ How would one prove or disprove there is only one local minimum with respect to $\nu>0$ for any given $x$ ? Numerical computation seems to suggest $f(\nu,x)$ strictly decreases with $\nu\in(0,\infty)$ for $x\in[0,1.5]$ , and $f(\nu,x)$ is convex in $(0,a)$ and concave in $(a,\infty)$ for some $a>0$ for $x\in (b,\infty)$ for some $b\ge 1.5$ . To facilitate the solution, I post the first and second partial derivative of $f$ as follows. \begin{align}2\frac{\partial f}{\partial \nu}=\frac{1-x^2}{\nu+x^2}+\ln\Big(1+\frac{x^2}\nu\Big)-\int_0^\infty \frac{e^{-\frac \nu2t}}{1+e^{-\frac t2}}\,dt,
\end{align} $$4\frac{\partial^2 f}{\partial \nu^2}=-2\frac{\nu+x^4}{\nu(\nu+x^2)^2}+\int_0^\infty \frac{te^{-\frac \nu2t}}{1+e^{-\frac t2}}\,dt$$","['special-functions', 'real-analysis', 'gamma-function', 'maxima-minima', 'inequality']"
4139400,Showing transitivity of a relation for a set,"I have to check if the following set is an equivalence relation: Set: $A=\mathbb{R}$ Relation: $x \sim y$ if $x=ay$ for some $a \in \mathbb{Q}\backslash \{0\}$ I know I have to use the reflexive, symmetric and transitive properties here. It's reflexive since for $x \sim x$ , we have $x=ax$ which is true if $a=1$ .
It's symmetric because for all $x,y \in A$ and $a,b \in \mathbb{Q}$ , we have: $x=ay$ , $y=bx$ which implies that $y=(ba)y$ . Since $0$ is not included in $\mathbb{Q}$ , this is always possible since we will never have a division by $0$ error or some kind of statement which does not make sense. How do I show transitivity though? I believe this means I have to show that if $x=ay$ and $y=bz$ then $x=cz$ for $a,b,c\in\mathbb{Q}$ . Is my understanding right? How would I proceed from here?","['elementary-set-theory', 'equivalence-relations', 'relations', 'discrete-mathematics']"
4139440,Determining if the following sets are an equivalence relation,"I am trying to determine if the following sets are equivalence relations: Set: $B=\mathbb{R}$ Relation: $x \sim y$ if $|x-y|\leq 1$ Reflexive property: For $x \sim x$ , we have $|x-x|\leq 1=|0|=0\leq1$ so this is true. Symmetry property: We need to ensure that if $x \sim y$ then $y \sim x$ . This is also true since we have $|y-x|=|-(x-y)|=|x-y|$ and we know that $|x-y|\leq1$ so symmetry is satisfied. Transitive property: I always struggle with this part. I think what I need to show is that if $|x-y|\leq1$ and $|y-z|\leq1$ , then $|x-z|\leq1$ . If my assumption is right, how would I go about doing this? My idea was to write $|x-z|\leq1$ as $|(x-y)+(y-z)||\leq1$ . Since $(x-y)$ and $(y-z)$ could both potentially equal something that adds to a value larger than 1, transitivity is NOT satisfied so this is not an equivalence relation. However, I am not sure if this is the right approach... Set: $B=M_n(\mathbb{R})$ , the set of $n \times n$ matrices with real entries, where $n$ is some fixed positive integer. Relation: $X \sim Y$ if $Tr(X-Y) \in\mathbb{Z}$ Reflexive property: For $x \sim x$ we have $Tr(X-X)=0\in \mathbb{Z}$ so the reflexive property is satisfied. Symmetry property: We need to ensure that if $x \sim y$ then $y \sim x$ . We have $Tr(Y-X)=-Tr(X-Y)$ which is still in $\mathbb{Z}$ so symmetry is satisfied Transitivity: Again, this is what I struggle with. I believe that here, we need to check that if $Tr(X-Y)\in \mathbb{Z}$ and $Tr(Y-Z)\in \mathbb{Z}$ , then we need to show that $Tr(X-Z)\in \mathbb{Z}$ . My idea was to write $Tr(X-Z)$ as $Tr(X)-Tr(Z)$ . However, I am not sure if this implies that $Tr(X)\in \mathbb{Z}$ and $Tr(Z)\in \mathbb{Z}$ which implies transitivity. Any guidance on this would be very much appreciated! Also, is my work on the other properties right?","['elementary-set-theory', 'equivalence-relations', 'relations', 'discrete-mathematics']"
4139459,Finding the global minimum of the multivariable function using only algebraic tools,"Problem says: Find the global minimum of $$\begin{align}f(x,y): &= x^2 + y^2 + \alpha xy + x + 2y\end{align}$$ where, $\alpha\in\mathbb R$ . The things I have done: Let, $$f(x,y):=x^2+x(\alpha y+1)+2y+y^2$$ Algebraic tool I will use along the way: $$ax^2+bx+c=a(x-m)^2+n$$ $$m=-\frac{b}{2a},~n=-\frac{\Delta}{4a}$$ In this case, we have $$\begin{align}m:&=-\frac{\alpha y+1}{2} \\
 n:&=\frac 14 \left(y^2(4-\alpha ^2) + 2y(4-
\alpha)-1\right)&\end{align}$$ Then, $$\begin{align}f(x,y):=\left(x+\frac{\alpha y+1}{2}\right)^2+\frac 14 \left(y^2(4-\alpha ^2) + 2y(4-\alpha)-1\right)\end{align}$$ If $\alpha =±2$ , putting $$x=-\frac{\alpha y+1}{2}$$ we get $$f(x,y):=\frac{y(4-\alpha)}{2}-\frac 14$$ Since $4-\alpha>0$ for $\alpha=±2$ , we observe that the lower bound of $\frac{y(4-\alpha)}{2}-\frac 14$ doesn't exist (applying $y\to -\infty$ ). If $|\alpha|>2$ , then applying the same method, we have $$\begin{align}\frac 14 \left(y^2(4-\alpha ^2) + 2y(4-\alpha)-1\right)=y^2\left(1-\frac{\alpha ^2}{4}\right)+y\left(2-\frac{\alpha}{2}\right)-\frac 14=\left(1-\frac{\alpha^2}{4}\right)\left(y+\frac{4-\alpha}{4-\alpha ^2}\right)^2+\frac{2 \alpha - 5}{4 - \alpha^2}\end{align}$$ So, we obtain $$\begin{align}f(x,y):=\left(x+\frac{\alpha y+1}{2}\right)^2+\left(1-\frac{\alpha^2}{4}\right)\left(y+\frac{4-\alpha}{4-\alpha ^2}\right)^2+\frac{2 \alpha - 5}{4 - \alpha^2}\end{align}$$ Likewise, if $|\alpha|>2$ then putting $$x=-\frac{\alpha y+1}{2}$$ we get, $$\begin{align}f(x,y):=\left(1-\frac{\alpha^2}{4}\right)\left(y+\frac{4-\alpha}{4-\alpha ^2}\right)^2+\frac{2 \alpha - 5}{4 - \alpha^2}\end{align}$$ Observing that, $$1-\frac{\alpha^2}{4}<0$$ where $|\alpha|>2$ . This means, if $|\alpha|>2$ , then the lower bound of $$\begin{align}\left(1-\frac{\alpha^2}{4}\right)\left(y+\frac{4-\alpha}{4-\alpha ^2}\right)^2+\frac{2 \alpha - 5}{4 - \alpha^2}\end{align}$$ doesn't exist. (applying $y\to +\infty$ ) Finally, if $|\alpha|<2\iff -2<\alpha<2$ , then $1-\frac{\alpha^2}{4}>0$ and putting $$x=-\frac{\alpha y+1}{2}, ~~y=\frac{\alpha-4}{4- \alpha^2}$$ in the original function $$\begin{align}f(x,y):=\left(x+\frac{\alpha y+1}{2}\right)^2+\left(1-\frac{\alpha^2}{4}\right)\left(y+\frac{4-\alpha}{4-\alpha ^2}\right)^2+\frac{2 \alpha - 5}{4 - \alpha^2}\end{align}$$ we conclude $$\begin{align}\min \left\{x^2 + y^2 + \alpha xy + x + 2y {\large{\mid}} -2<\alpha<2\right\}=\frac{2\alpha-5}{4-\alpha^2} ~\text {at}~ x=\frac{2-2\alpha}{\alpha^2-4} ~\text{and}~ y=\frac{\alpha-4}{4- \alpha^2}. \end{align}$$ Questions: Is the algebraic method I use rigorous enough?  Are there still non-rigorous points in my steps? What I'm doing is just finding the minimum of the given function.  How can I show that the minimum I found is a global minimum? Thank you for reviewing.","['optimization', 'algebra-precalculus', 'proof-writing', 'maxima-minima']"
4139462,Exponential series representation,"It is well known that the exponential function can be represented as follows: $$e^x=\sum_{n=0}^\infty\frac{x^n}{n!}$$ However, the right hand side is not defined at $x=0$ due to $n=0$ . Is it wrong? should we use  this one: $$e^x=1+\sum_{n=1}^\infty\frac{x^n}{n!}\text{?}$$","['power-series', 'functions', 'sequences-and-series']"
4139480,If $T$ is self-adjoint then $||T^n|| = ||T||^n$,"Let $T$ be a bounded linear operator on a Hilbert space $H$ . If $T$ is self-adjoint then $||T^n|| = ||T||^n$ . It is easy to see that $||T||^n$ is an upper bound. Indeed, there exists a $C>0$ such that, $||T^nx|| \leq C||x||$ . Then $||T^n|| \leq ||T||^n$ . To prove the other direction, \begin{align*}||T||^2  = \sup_{||x|| =1}\{||Tx||^2\} & = \sup_{||x|| =1}\{\langle Tx,Tx\rangle\}= \sup_{||x|| =1}\{\langle x,TTx\rangle\} \\ & \leq \sup_{||x|| =1}\{||x||^2||T^2||\} = ||T^2||.\end{align*} But for the case $n = 3$ (the induction step), it seems that trick I've used doesn't work. Since \begin{align*}
||T||^3 = \sup_{||x|| =1}\{||Tx||^3\} & = \sup_{||x|| =1}\{\sqrt{\langle Tx,Tx\rangle}^3\}= \sup_{||x|| =1}\{\sqrt{\langle x,TTx\rangle}^3\} \\ & \leq \sup_{||x|| =1}\{\sqrt{||x||^2||T^2||}^3\} = ||T^2||^{\frac{3}{2}}.
\end{align*} How to show the induction step? I read some comments using the spectral theorem but I have not learnt it yet. Is there another proof using just properties of being self-adjoint?","['linear-algebra', 'functional-analysis', 'real-analysis']"
4139547,Alternating series summation,"I'm curious if the following series can be summed up $$\sum_{k=1}^{\infty} \frac{(-1)^{k+1}}{k+2j+1}$$ I tried to apply the typical alternating series summation for series of this form through the Taylor series of the $\log(1+x)$ , but that $2j+1$ term really messes it up.","['power-series', 'calculus', 'sequences-and-series', 'real-analysis']"
4139578,Circle determined by $4.89=90-\tan^{-1}(\frac{31.1-x}y)-\tan^{-1}(\frac{y}{36.9-x})$ & $4.89=90-\tan^{-1}(\frac{x-36.9}y)-\tan^{-1}(\frac{y}{x-31.1})$,"When graphed, these two equations appear form each half of a circle. $$
4.89=90-\tan^{-1}\left(\frac{31.1-x}{y}\right)-\tan^{-1}\left(\frac{y}{36.9-x}\right)
$$ $$
4.89=90-\tan^{-1}\left(\frac{x-36.9}{y}\right)-\tan^{-1}\left(\frac{y}{x-31.1}\right)
$$ I am trying to derive the equation for the circle they form, like $\left(x-h\right)^{2}-\left(y-k\right)^{2}=r^{2}$ , but have struggling thus far. Through trial and error on Desmos, I managed to determine the following equation: $$
\left(x-34\right)^{2}+\left(y-33.8761\right)^{2} = 34^{2}\
$$ Its graph appears very similar (within a few d.p.) to that of the two equations above but what I am really interested in is how one would go about deriving such an equation. I understand that there are rather complex distributivity rules for trigonometric  functions, and am wondering what exactly I would need to do to transpose them.","['trigonometry', 'circles']"
4139629,A Fibonacci conjecture: $\log_{F_{n+1}}{F_n}<\log_{F_{n+2}}{F_{n+1}}$,"Given the Fibonacci sequence $F_n$ , that is $$F_1=F_2=1,F_{n+2}=F_{n+1}+F_n.$$ I find that $$
\log_21<\log_32<\log_53<\log_85<\cdots,
$$ i.e. we have $$
\log_{F_{n+1}}{F_n}<\log_{F_{n+2}}{F_{n+1}}, \quad \text{for} \; n\geqslant1\tag{1}
$$ If $n=2k$ , $(1)$ is easy to get, but if $n=2k+1$ , I have no idea to prove it. What's more, if we let $F_1=a,F_2=b$ be two positive integers, then $(1)$ still exstis when $n$ is greater than some integer $m$ . I do not konw whether my conjecture is true.","['number-theory', 'inequality', 'fibonacci-numbers']"
4139666,"Why are topological spaces defined in terms of open sets, and not in terms of connected open sets?","I'm starting to learn about point-set topology, and I find the definition of topological spaces and open sets to be very weird. Since we care about continuity in topology, it's odd that topological spaces are defined based on open sets, which can be disconnected. It makes more sense that they should be built on connected sets. Below is an alternative formulation I made of topological spaces: A topological space is a set $X$ of points, and the set $\tau$ of all
""bunches"" in $X$ . All bunches are subsets of $X$ . A topological space
must have the following properties: The empty set and $X$ are both bunches. Given a non-empty collection of bunches, if their intersection is non-empty , then their union is also a bunch. Given two bunches, their intersection is also a bunch. My definition of bunch is intended to be equivalent to connected open
sets. Open sets can be then defined as the union of disjoint bunches. Is my alternative formulation equivalent to the usual formulation? If yes, why is this formulation not used? If no, then what's the difference? Note: I'm told that my formulation seems related to bases , although I'm not sure what the exact relationship is. Note 2: My initial intuition was to define continuous functions as functions that sends bunches to bunches, but that doesn't quite work .",['general-topology']
4139733,Is there a way to find UMVUE without just guessing?,"We have $(X_1,...,X_n)$ random sample of distribution $N(m,1)$ . I need to find UMVUE of $g(m)=e^m$ . The natural guess was that the unbiased estimator which I need to find may be something like $e^T$ , where $T=\frac{1}{n} \sum\limits_{i=1}^n X_i$ . So I calculated $$\mathbb{E}[e^{T}]=exp \left[m+\frac{1}{2n}\right]$$ therefore the unbiased estimator that I was looking for is $g(T)=e^{T-\frac{1}{2n}}.$ My question is: is there a way to get this result in any other way than just guessing?
I considered solving: $$\mathbb{E}g(T)=e^m$$ so I would get $$\int\limits_0^\infty g(t) \frac{1}{\sqrt{\frac{2 \pi}{n}}}e^{-\frac{(t-m)^2}{\frac{2}{n}}} dt=e^m$$ but that doesn't seem to get me anywhere. What do you think?","['statistics', 'parameter-estimation', 'normal-distribution', 'estimation']"
4139778,How can open spheres be shapes like squares?,"I'm reading through Schaum's General Topology and I came across the section on equivalent metrics. So, the author introduces the idea that metrics $d_1$ and $d_2$ are equivalent when the $d-$ open spheres of $d_1$ and $d_2$ induce the same topology on a space $X$ . Then, as an example, the author goes on to show that the three metrics below all serve as a base for the natural topology on $\mathbb{R}^2$ . So, I can clearly understand how we can use these shapes as a base for the natural topology on $\mathbb{R}^2$ . What I don't understand is how $S_{d_1}(p,\delta)$ and $S_{d_2}(p,\delta)$ are open ""spheres"". Geometrically speaking $S_{d_1}(p,\delta)$ and $S_{d_2}(p,\delta)$ are squares. Since they are squares, they are not ""spheres"" in the sense that they are sets centered at some point $p$ which contain all points within $\delta$ of $p$ . Like I said, squares are perfectly fine as a base for the natural topology on $\mathbb{R}^2$ but can we really call $S_{d_1}(p,\delta)$ and $S_{d_2}(p,\delta)$ open spheres ? In a square centered at $p$ , won't some points be farther from $p$ than others, thus violating our traditional notion of open sphere?","['general-topology', 'metric-spaces']"
4139863,"$\mathcal{S}$, smallest $\sigma$-algebra on $\mathbb{R}$ containing $\{(r,r+1):r\in\mathbb{Q}\}$, is the set of Borel subsets of $\mathbb{R}$ [duplicate]","This question already has answers here : Show that $\mathcal{J}=\{[a, a+1) \mid a \in \mathbb{R}\}$ generates the Borel set of $\mathbb{R}$ (3 answers) Closed 3 years ago . I have proved the following statement and I would like to know if my proof is correct, thank you. "" $\mathcal{S}$ , smallest $\sigma$ -algebra on $\mathbb{R}$ containing $\{(r,r+1):r\in\mathbb{Q}\}$ , is the set of Borel subsets of $\mathbb{R}$ "". DEF. (Borel set): The  smallest $\sigma$ -algebra on $\mathbb{R}$ containing  all  open  subsets  of $\mathbb{R}$ , $\mathcal{B}$ , is called the collection of Borel subsets of $\mathbb{R}$ . An element of this $\sigma$ -algebra is called a Borel set. LEMMA (1): Let $x\in\mathbb{R}$ : then there exists both a decreasing sequence and an increasing sequence of rational numbers converging to $x$ . LEMMA (2): Every open subset of $\mathbb{R}$ can be written as a countable union of disjoint open intervals. Let $(a, b), a,b\in\mathbb{R}, a<b$ be an open subset of $\mathbb{R}$ . Then by LEMMA (1) there exist a decresing sequence of rational numbers $(a_i)$ such that $a_i\overset{i\to +\infty}{\to} a$ and an increasing sequence of rational numbers $(b_i)$ such that $b_i\overset{i\to +\infty}{\to}b-1$ . Now, since $\mathcal{S}$ is a $\sigma$ -algebra we have that: $[1]\ \mathbb{R}\setminus (a_i,a_i+1)=(-\infty,a_i]\cup [a_i+1,+\infty)\in\mathcal{S}$ ; $[2]\ \mathbb{R}\setminus (b_i,b_i+1)=(-\infty,b_i]\cup [b_i+1,+\infty)\in\mathcal{S}$ ; $[3]\ \bigcup_{i=1}^{\infty}\mathbb{R}\setminus (a_i,a_i+1)=(-\infty,a]\cup [a+1,+\infty)\in\mathcal{S}$ ; $[4]\ \bigcup_{i=1}^{\infty}\mathbb{R}\setminus (b_i,b_i+1)=(-\infty,b-1]\cup [b,+\infty)\in\mathcal{S}$ ; $[5]\ (-\infty, b-1]=\bigcup_{k=1}^{\infty} (-k,b-1]\in\mathcal{S}$ ; $[6]\ [a+1,+\infty)=\bigcup_{k=1}^{\infty} [a+1, k)\in\mathcal{S}$ . From $[4]$ , $[5]$ , using the fact that $\sigma$ -algebras are closed under differences of sets we get $(-\infty,b-1]\cup [b,+\infty)\setminus (-\infty,b-1]=[b,+\infty)\in\mathcal{S}$ and similarly from $[3]$ and $[6]$ that $(-\infty,a]\cup [a+1,+\infty)\setminus [a+1,+\infty)=(-\infty, a]\in\mathcal{S}$ so it is also $(-\infty,a]\cup [b,+\infty)\in\mathcal{S}$ hence $(a,b)\in\mathcal{S}$ . By LEMMA (2) and the fact that $\sigma$ -algebra are closed under unions we have thus proved that the $\sigma$ -algebra $\mathcal{S}$ contains every open subset of $\mathbb{R}$ but since by definition $\mathcal{B}$ is the smallest such $\sigma$ -algebra it must be $\mathcal{B}\subset\mathcal{S}$ . Similarly, since $(r,r+1)\in\mathcal{B}$ for every $r\in\mathbb{Q}$ (they are open sets in $\mathbb{R}$ ) and by definition $\mathcal{S}$ is the smallest $\sigma$ -algebra containing these intervals, it must also be $\mathcal{S}\subset\mathcal{B}$ thus we can conclude that $\mathcal{S}=\mathcal{B}$ , as desired. EDIT : This question is not a duplicate of Show that $\mathcal{J}=\{[a, a+1) \mid a \in \mathbb{R}\}$ generates the Borel set of $\mathbb{R}$ Here the intervals are open intervals with rational end-points, not semi-open intervals with real end-points. This difference leads to different answers. The two questions are related but not duplicate.","['borel-sets', 'measure-theory', 'solution-verification', 'real-analysis']"
4139878,"Continuity equation, distributional derivative = usual derivative?","We say that a family of pairs measures/vector fields $(\rho_t,v_t)$ with $v_t\in L^1(\rho_t;\mathbb{R}^d)$ and $\int_0^T\|v_t\|_{L^1(\rho_t)}dt=\int_0^T\int_{\Omega}|v_t|d\rho_tdt<+\infty$ solves the continuity equation on (0,T) in the distributional sense if
for any bounded and Lipschitz test function $\phi\in C_c^1((0,T\times\overline{\Omega})$ , we have $$\int_0^T\int_{\Omega}(\partial_t\phi)d\rho_tdt+\int_0^T\int_{\Omega}\nabla\phi\cdot v_t d\rho_tdt=0$$ We can also define a weak solution of the continuity equation through
the following condition: we say that $(\rho_t,v_t)$ solves the
continuity equation in the weak sense if for any test function $\psi\in C_c^1(\overline{\Omega})$ , the function $t\mapsto\int\psi d\rho_t$ is absolutely continuous in t and, for a.e. $t$ , we have $$\frac{d}{dt}\int_{\Omega}\psi d\rho_t=\int_{\Omega}\nabla\psi\cdot v_t d\rho_t$$ Now apparently the two notions of solutions are equivalent, i.e. every weak solution is a distributional solution and every distributional solution is a weak solution. If I consider a distributional solution $(\rho_t,v_t)$ and I take the particular ""separated"" test functions $\phi(t,x)=a(t)\psi(x)$ then I get $$\int_0^T a'(t)\int_{\Omega}\psi(x)d\rho_t(x)dt=-\int_0^1 a(t)\int_{\Omega}\nabla\psi(x)\cdot v_t d\rho_t(x)dt$$ which means that the distributional derivative of $t\mapsto\int_{\Omega}\psi(x)d\rho_t(x)$ is $t\mapsto\int_{\Omega}\nabla\psi(x)\cdot v_t d\rho_t(x)$ . But then I don't understand why the actual derivative (not just the distributional derivative) of $t\mapsto\int_{\Omega}\psi(x)d\rho_t(x)$ is $t\mapsto\int_{\Omega}\nabla\psi(x)\cdot v_t d\rho_t(x)$ .","['measure-theory', 'distribution-theory', 'partial-differential-equations']"
4139918,How to solve this homogeneous partial differential equation,"I'm trying to solve this second order differential equation, but it seems my solution isn’t accurate, since i could not find the correct $\sigma$ in $$ \ddot \sigma - p e^\sigma - q e^{2\sigma} =0\qquad\qquad(1)$$ or $$\frac{d^2 \sigma}{dt^2} - p e^\sigma - q e^{2\sigma} =0$$ where $p$ and $q$ are constants. So any help is appreciated. Here's what I have tried: Let $\sigma = \log ~ r$ , then: $ \dot \sigma= \frac{\dot r}{ r}$ , and $\ddot\sigma= \frac{\ddot r}{ r} -  \frac{\dot r^2}{r^2}$ . Sub in (1) $$ \frac{\ddot r}{ r} -  \frac{\dot r^2}{r^2} - r^2 q - r p  =0\qquad\qquad(2)$$ Now to solve (2), will I use something like $ r = e^{\lambda t}$ again? Then (2) becomes: $$ \lambda^2 - \lambda^2 -  e^{\lambda t} q - p =0 \qquad\qquad(3) $$ therefore $\lambda = \frac{1}{t}~ \log~ \frac{p}{q} $ , or $ r = \frac{p}{q} $ and $ \sigma = \log \frac{p}{q} $ . This solution can not be, cause it means $ \dot \sigma = \ddot \sigma =0!!! $ Have I missed something?? Thanks.",['ordinary-differential-equations']
4139966,Expected value of a product of function and stochastic integral that depend on a solution of SDE,"Let $X_t$ be the solution of the SDE $$ dX_t=f(t,X_t)dt+\sigma(X_t)dW_t.$$ I need to check if $$ \mathbb{E}\biggl[(\sigma(X_t)+\sigma(X_a))\int_a^t\sigma'(X_s)\sigma(X_s)dW_s\biggr]=0.$$ If this is not the case, it is enough for me to prove that it is equal to $\mathcal{O}(t-a)$ if possible. Thank you in advance!","['stochastic-integrals', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
4139971,The Probabilistic Method: proving existence of functions satisfying certain property,"Let $\mathcal{X}$ be a subset of $\mathbb{R}^n$ and let $\mathcal{F}$ be a space of functions: $$\mathcal{F} = \{f | f: \mathcal{X} \to \mathbb{R} \}$$ Let $P_\mathcal{X}$ be a probability distribution over $\mathcal{X}$ and let $P_\mathcal{F}$ be a probability distribution over $\mathcal{F}$ . Consider $$\mathbb{P} \left( F(X^n) > a \right)$$ where $F$ is a random function picked according to $P_\mathcal{F}$ and $X^n$ is a random sequence picked according to $P_\mathcal{X}$ . Now suppose I showed that $$\mathbb{P} \left( F(X^n) > a \right) \leq \frac{1}{n}$$ This implies (for $n \geq 2$ ) that there exists a deterministic function $f$ and a deterministic $x^n \in \mathcal{X}$ such that $f(x^n) \leq a$ . But I want to make the following statement: for sufficiently large $n$ , there exists a deterministic function $f$ such that with high probability, $f(X^n) \leq a$ , where $X^n$ is random. I call this partial derandomization. Can I make this statement from what I have? If not, what other ways I could explore? PS: $\mathcal{X}$ is compact.","['probabilistic-method', 'random', 'functions', 'probability']"
4140042,How many ways are there to distribute $15$ different letters into $5$ post boxes if:,"How many ways are there to distribute $15$ different letters into $5$ post boxes if: a-) Post boxes are different and each will have $3$ letters. b-) Post boxes are identical and each will have $3$ letters. c-) Post boxes are identical and they will have $3,3,2,2,5$ letters. d-) Post boxes are identical and they will have $3,1,4,2,5$ letters. I want you to check my thinking way. Is it true or not ? My solution : a-) It is easy we can find it by saying choose $3$ elements for the first box , choose $3$ elements for the second box so on. Then , the answer is $C(15,3) \times C(12,3) \times C(9,3) \times C(6,3) \times C(3,3) $ . b-) Lets show the cluster of letters which have the same number of letter with red balls. Then we have $5$ identical red balls representing cluster of $3$ letters. To solve question easily , i assume that post boxes are different. Then , we can disperse $5$ identical objects to $5$ different post boxes with each boxes have one red ball by $1$ way. So , we can write $1 \times C(15,3) \times C(12,3) \times C(9,3) \times C(6,3) \times C(3,3) $ . However , the post boxes were identical , so i should multiply it with $1/5!$ Answer : $\frac{1}{5!} \times 1 \times C(15,3) \times C(12,3) \times C(9,3) \times C(6,3) \times C(3,3)$ c-)Lets firstly assume that the post boxes are different .Moreover , show the cluster of $3$ letters with red balls and   the cluster of $2$ letters with blue balls and  the cluster of $5$ letters with green balls. Now ,we have $2$ red balls , $2$ blue balls , $1$ green balls to distribute $5$ different boxes. We can do it $\frac {5!}{2! \times 2! \times 1!}$ different ways. Then , $\frac {5!}{2! \times 2! \times 1!} \times C(15,3) \times C(12,3) \times C(9,2) \times C(7,2) \times C(5,5)$ However , in original , the boxes were identical but we assumed them as different to solve easily. Then , we should multiply with $\frac{1}{5!}$ to make them identitical $\color{blue}{again}$ . So, the real answer is $\frac{1}{5!} \times \frac {5!}{2! \times 2! \times 1!} \times C(15,3) \times C(12,3) \times C(9,2) \times C(7,2) \times C(5,5)$ d-)Lets firstly assume that the post boxes are different. Moreover , because of all clusters have different number of letters , lets show them $5$ different colors. Then , we can disperse $5$ different objects to $5$ different boxes by $5!$ ways. Hence ,we can write $5! \times C(15,3) \times C(12,1) \times C(11,4) \times C(7,2) \times C(5,5)$ . However , the post boxes were identical ,so i should multiply with $\frac{1}{5!}$ to make them identitical $\color{blue}{again}$ . Then , the answer is $C(15,3) \times C(12,1) \times C(11,4) \times C(7,2) \times C(5,5)$ . Is my thinking $\color{red}{WAY}$ correct ? Can i convert diffrent object  into identical objects like i did in my solutions. Thank you..","['permutations', 'combinations', 'combinatorics', 'balls-in-bins', 'discrete-mathematics']"
4140047,Potentially Ambiguous Stochastic Calculus Question,"In the Lecture Notes for my Stochastic Calculus module, my lecturer provides the following question as an essential exercise in preparation for the exam. It reads as follows: Let $(W_t)_{t≥0}$ be a one-dimensional Wiener process. In the solution that you
found in the previous exercise, replace $G_t$ by $W_t$ , that is, set $X_t = xe^{at + bW_t}$ . Write an equation for X. Now, the previous exercise asked us to find a solution to the equation: $$d X_t = aX_t\, dt + bX_t \, dG_t, \,\,\,\,\,\,\,\,\,\,\,\, X_0 = x,$$ where $G \in C^1$ , i.e. $G$ is a continuously differentiable function and $G(0) = 0$ , and $a,b \in \mathbb{R}$ . My solution, as suggested in the original exercise was $$X_t = x e^{at + bG_t}$$ I'm not too sure what the process would be to answer the original question. It seems fairly ambiguous - am I essentially just reverse-engineering the 'previous exercise' and trying to find some sort of differential equation? If so, how would I do this?","['stochastic-processes', 'brownian-motion', 'stochastic-calculus', 'ordinary-differential-equations']"
4140051,Find the cardinality of a set of convex subsets,"Find the cardinality of the set of all convex subsets of $\mathbb{R}^2.$ Is it true that the cardinality of the set in $1$ is at most that of the real numbers? If not, then the following only gives a rough lower bound on that cardinality. A convex subset $C$ of $\mathbb{R}^2$ is one that satisfies that for all $a,b \in C, t\in[0,1], ta + (1-t)b\in C.$ One would need to construct a distinct convex subset of $\mathbb{R}^2$ for each positive real number, and intuitively this can be done by taking a convex subset and rescaling it for each positive real number (if $A$ is convex and $k >0,$ then $kA$ is convex because for all $a,b \in kA, a = kx, b= ky$ for some $x,y\in C$ so for any $t\in [0,1], ta + (1-t)b = t(kx) + (1-t)(ky) = k(tx + (1-t)y) \in C$ . Then wouldn't this describe an injective function from $\mathbb{R}^+$ to the set of convex subsets? And if so, the cardinality of the set of convex subsets is at least $|\mathbb{R}|$ . Since the cardinality is also at most $|\mathbb{R}|$ , it is equal to $|\mathbb{R}|.$","['elementary-set-theory', 'cardinals', 'convex-analysis', 'real-analysis']"
4140079,Is the quotient map on a locally compact topological group closed?,"Let $G$ be a locally compact group, $H$ a closed subgroup, $G/H$ the space of left cosets with the quotient topology, and $q:G\rightarrow G/H$ the projection map.  Also, let $C(G,\mathbb{C})$ be the space of continuous functions from $G$ to $\mathbb{C}$ .  The author of a book I am reading is attempting to define a subspace of $C(G,\mathbb{C})$ using (among others) the condition "" $q($ supp $(f))$ is compact"".  Here is my problem: unless $q$ is a closed map I do not see how this condition can respect linearity (i.e., if $q($ supp $(f))$ and $q($ supp $(g))$ are compact then so is $q($ supp $(f+g))$ for $f,g$ in this subspace).  If $G$ is simply a topological group and $H$ is compact, then it is true that $q$ is closed.  But in particular for locally compact groups can we say that $q$ is closed?  My belief is that this is not true, but I could be wrong.  Since the other conditions imposed on this ""subspace"" don't seem to help the matter, this would appear to be an error in the text.","['topological-groups', 'functional-analysis', 'analysis']"
4140099,How to determine the number of double cosets from table of marks,I remember reading a while back that a lot of information about double cosets of a group can be extracted from the group's table of marks. I can't recall the source. The group is an arbitrary finite group. I'm also mainly interested in the number of double cosets of the same subgroup; so I would expect these to somehow correspond to the diagonal entries of the table of marks. Does anyone know the exact correspondence or a reference where this is discussed. (How to do this in GAP is even better).,"['gap', 'group-theory', 'finite-groups']"
4140138,"Knowing $\exp(i\pi) = -1$, can we find out the value of $\pi$?","We know $\exp(i\pi) = -1$ or $e^{i\pi} = -1$ . Can we solve for this infinite polynomial in $\pi$ to obtain the value of $\pi$ ? We already know the value of $e$ from two methods: $e = \exp(1) = 1 + \frac{1}{2!} + \frac{1}{3!} + \cdots$ or from: $ e = \lim_{n\to\infty}\left(1+\frac{1}{n}\right)^{n} $ So we should in theory be able to solve for the other unknown $\pi$ . We can also try: $\log(-1) = i\pi$ But it is weird that: $\log(1 + (-2)) = (-2)-\frac{(-2)^2}{2}+\frac{(-2)^3}{3}-\frac{(-2)^4}{4} + \cdots$ doesn't look like $i\pi$ , whatever else it could be made in to.","['exponentiation', 'algebra-precalculus', 'pi']"
4140145,Limit of a (rather general) recursive sequence,"So I am struggling with the following problem: Let $F$ be a vector (sub)space of recursive sequences satisfying $x_{i+2} = bx_{i+1} + ax_{i}$ in field $\mathbb{K}$ , with a vector space endomorphism $L: F \to F, (x_{i})_{i} \to (x_{i+1})_i $ . Show that if $a>0$ and $b \neq 0$ , $\lim_{i \to \infty} \frac{x_{i+1}}{x_{i}}$ exists and is equal to one of the eigenvalues of the endomorphism $L$ . I have shown that $F$ is indeed a vector subspace of $\mathbb{K}^\mathbb{N}$ and that the function $L$ is and an endomorphism of this vector subspace. I have proven that the $\dim F = 2$ and that we have a ""natural"" basis of two sequences $f_0 = (1, 0, ...)$ and $f_1 = (0, 1, ...)$ . The endomorphism $L$ can then be described by a matrix: \begin{equation*} A= 
\begin{pmatrix}
0 & 1 \\
a & b
\end{pmatrix}
\end{equation*} in this natural basis. I have then found two eigenvalues, $e_1 = \frac{b + \sqrt{b + 4a}}{2}$ and $e_2 = \frac{b - \sqrt{b + 4a}}{2}$ . The related eigenvectors are then \begin{equation*} v_{1,2} = 
\begin{pmatrix}
2 \\
2e_{1,2} 
\end{pmatrix}
\end{equation*} We also see that: \begin{equation*}
\begin{pmatrix}
x_i \\
x_{i+1} 
\end{pmatrix} = A^i
\begin{pmatrix}
x_0 \\
x_1 
\end{pmatrix} 
\end{equation*} Since $A$ is diagonalizable, we can write: \begin{equation*}
\begin{pmatrix}
x_i \\
x_{i+1} 
\end{pmatrix} = S D^i S^{-1}
\begin{pmatrix}
x_0 \\
x_1 
\end{pmatrix} 
\end{equation*} with \begin{equation*}
S =
\begin{pmatrix}
2 & 2\\
2e_1 & 2e_2 
\end{pmatrix}, \; D = 
\begin{pmatrix}
e_1 & 0\\
0 & e_2 
\end{pmatrix}, \; S^{-1} = \frac{-1}{4\sqrt{b^2 + 4a}}\begin{pmatrix}
2e_2 & 2e_1\\
2 & 2 
\end{pmatrix}
\end{equation*} We can then find the explicit formula for $x_i$ , we get: \begin{equation}
x_i = \frac{-1}{4\sqrt{b^2 + 4a}}((4e_1^i e_2 + 4e_2^i)x_0 + (4e_1^{i+1} + 4e_2^{i})x_1)
\end{equation} And so \begin{equation*}
\lim_{i \to \infty} \frac{x_{i+1}}{x_i} = \lim_{i \to \infty} \frac{(4e_1^{i+1} e_2 + 4e_2^{i+1})x_0 + (4e_1^{i+2} + 4e_2^{i+1})x_1)}{(4e_1^i e_2 + 4e_2^i)x_0 + (4e_1^{i+1} + 4e_2^{i})x_1)}
\end{equation*} And that is where I am stuck, if I could prove that this limit exists, I could simply use the fact that \begin{equation*}
\theta_{i+1} = \frac{x_{i+1}}{x_{i}} = \frac{bx_i + ax_{i-1}}{x_i} = b + a \frac{x_{i-1}}{x_i} = b + \frac{a}{\theta_{i-1}}
\end{equation*} and then $\theta = \lim_{i \to \infty}\frac{x_{i+1}}{x_{i}}$ satisfies \begin{equation*}
\theta = b + \frac{a}{\theta}
\end{equation*} and so \begin{equation*}
\theta = \frac{b \pm \sqrt{b^2 + 4a}}{2}
\end{equation*} which is equal to the eigenvalues of $L$ . But how do I prove that this limit exists in the first place? I tried to evaluate the explicit formula that I got, but I didn't find a way to do it. Could you please help?","['calculus', 'linear-algebra', 'recursion', 'sequences-and-series']"
4140148,"Prove $X_n=2^n\textbf{1}_{(0,2^{-n}]}$ is a martingale","Let $((0,1], \text{Borel}_{(0,1]}, \mu)$ be a measure space, where $\mu$ is the Lebesgue measure. I want to prove that $\{X_n\}_{n\geq 0}$ is a martingale with respect to the filtration $\mathcal{F}_n=\sigma(X_1,\ldots,X_n)$ , where $X_n=2^n\textbf{1}_{(0,2^{-n}]}$ . I only need help to prove the third property of martingales, that is, $\mathbb{E}(X_n\mid\mathcal{F}_{n-1})=X_{n-1}$ $\forall n\geq1$ . So far this is what I've tried: \begin{align}
\mathbb{E}(X_n\mid\mathcal{F}_{n-1})&=
\mathbb{E}(2^n\textbf{1}_{(0,2^{-n}]}\mid\mathcal{F}_{n-1})
\\&=2\mathbb{E}(2^{n-1}\textbf{1}_{(0,2^{-n}]}\mid\mathcal{F}_{n-1})
\\&=2\mathbb{E}[2^{n-1}(\textbf{1}_{(0,2^{-n+1}]}-\textbf{1}_{(2^{-n},2^{-n+1}]})\mid\mathcal{F}_{n-1}]
\\&=2X_{n-1}-\mathbb{E}[2^n\textbf{1}_{(2^{-n},2^{-n+1}]}\mid\mathcal{F}_{n-1}]
\end{align} If I were working with normal expectantions instead of conditional ones, I could write: $$\mathbb{E}[2^n\textbf{1}_{(2^{-n},2^{-n+1}]}\mid\mathcal{F}_{n-1}]=\mathbb{E}[2^n\textbf{1}_{(0,2^{-n}]}\mid\mathcal{F}_{n-1}]$$ because $(0, 2^{-n}]$ and $(2^{-n},2^{-n+1}]$ have the same probability under Lebesgue measure, and then my problem would be solved because I would get: $$\mathbb{E}(X_n\mid\mathcal{F}_{n-1})=2X_{n-1}-\mathbb{E}(X_n\mid\mathcal{F}_{n-1}) \Longrightarrow \mathbb{E}(X_n\mid\mathcal{F}_{n-1})=X_{n-1}$$ However I don't think I can use this technique here, because: $$\exists A\in \mathcal{F}_{n-1} \quad \mbox{so that} \quad \int_{A}\textbf{1}_{(0,2^{-n}]}d\mu \neq \int_{A}\textbf{1}_{(2^{-n}, 2^{-n+1}]}d\mu$$ And therefore the conditional expectations are not the same. Can someone please help me?","['conditional-expectation', 'martingales', 'lebesgue-measure', 'probability-theory']"
4140150,Find solution: $\frac {d^4x}{dt^4}=x$,"Find solution: $\frac {d^4x}{dt^4}=x \tag{1}\label{eq1}$ For which $x_0 = (x(0),x'(0),x''(0),x^{(3)}(0))$ the solution is: limited on $(0,\infty)$ limited on $(-\infty,0)$ periodic so we have $P(\lambda)=\lambda^4-1$ $$\lambda_1=1,\lambda_2=-1,\lambda_3=i,\lambda_4=-i$$ each funtion $e^{it},e^{-it}$ is  a solution to \eqref{eq1} so their linear combination also is a solution to \eqref{eq1} So we have: $$x(t)=A_1e^t+A_2e^{-t}+A_3\cos{t}+A_4\sin{t}$$ $x'(t)=A_1e^t-A_2e^{-t}-A_3\cos{t}+A_4\sin{t}$ $x''(t)=A_1e^t+A_2e^{-t}-A_3\cos{t}-A_4\sin{t}$ $x^{(3)}(t)=A_1e^t-A_2e^{-t}+A_3\cos{t}-A_4\sin{t}$ So: $$x_0=(A_1+A_2+A_3, A_1-A_2+A_4, A_1+A_2-A_3, A_1-A_2-A_4)$$ Thus the solution is limited on $(0,+\infty) \Leftrightarrow A_1=0$ because then there is no $e^t$ and $\lim_{t \to \infty} e^{-t} = 0$ the solution is limited on $(-\infty,0) \Leftrightarrow A_2=0$ , as $\lim_{t \to -\infty} e^{t} = 0$ , $\lim_{t \to 0^-} e^{t} = 1$ the solution is periodic: on $(0,\infty) \Leftrightarrow A_1=0$ on $(-\infty,0)\Leftrightarrow A_2=0$ on $\mathbb{R} \Leftrightarrow A_1=A_2=0$ It is a good solution? If there are some kind of mistakes can I get hints what should I correct, thanks.","['solution-verification', 'ordinary-differential-equations']"
4140240,"Convergence in quadratic mean, addition of random variables","We defined that $X_n\rightarrow X$ , i.e, that $X_n$ converges in quadratic mean to $X$ , if $\mathbb{E}[(X_n-X)^2]\rightarrow0$ as $n\rightarrow\infty$ . I'm trying to prove the following statement: $$X_n\rightarrow X, Y_n\rightarrow Y \Rightarrow X_n+Y_n\rightarrow X+Y$$ I did the following: \begin{split}
\mathbb{E}[(X_n+Y_n-(X+Y))^2] &= \mathbb{E}[(X_n-X+Y_n-Y)^2] \\
&= \mathbb{E}[(X_n-X)^2]+2\mathbb{E}[(X_n-X)(Y_n-Y)]+\mathbb{E}[(Y_n-Y)^2] \\
\end{split} I know that the first and third terms go to $0$ as $n\rightarrow \infty$ , but what happens to the term in the middle? How do I show that it goes to $0$ as $n\rightarrow \infty$ ?","['statistics', 'convergence-divergence', 'order-statistics']"
4140266,Infinite fraction's derivative [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question $f(x)=x+\dfrac{1}{x+\dfrac{1}{x+\dfrac{1}{x+\ldots}}}$ $f'\left(\dfrac{3}{2}\right)=?$ I tried to make equation like $y^2=xy+1$ but I can't made it clearly.","['continued-fractions', 'calculus', 'derivatives']"
4140275,Let $a=\sqrt{2}+\sqrt[3]{3}+\sqrt[5]{5} \in \mathbb{R}$. Show that $a$ is algebraic over $\mathbb{Q}$ and that $|\mathbb{Q}(a):\mathbb{Q}|\le 30$.,"I know that if $a$ is algebraic over $\mathbb{Q}$ then there exists minimal polynomial $f(x)\in Z[x]$ such that $a$ is the root of that polynomial. I am trying to find that polynomial which I know will be of degree $30$ so that $|\mathbb{Q}(a):\mathbb{Q}|\le 30$ . I have looked at other examples on this forum like when $a=\sqrt{3}+\sqrt{5}$ or $a=\sqrt[3]{2}$ or $a=\sqrt[3]{3}+\sqrt[3]{9}$ and looked at their methods of finding the required polynomial but each method is different and the given $a=\sqrt{2}+\sqrt[3]{3}+\sqrt[5]{5}$ , it is difficult to find its polynomial using any their methods. If I try solving it following way $(a-\sqrt{2})^2=(\sqrt[3]{3}+\sqrt[5]{5})^2 \Rightarrow a^2+2-2a\sqrt{2}=\sqrt[3]{9}+\sqrt[5]{25}-2\sqrt[3]{3}\sqrt[5]{5}$ and then go on from there, then I get a messy equation which can't be solved to get a polynomial $f$ in $a$ for which $f(a)=0$ . Any help would be appreciated.","['algebraic-number-theory', 'abstract-algebra']"
4140317,Prove that all elements of sequence $a_{n}=\frac{ \left(1+\sqrt{n^4-n^2+1}\right)^{n} + \left(1-\sqrt{n^4-n^2+1}\right)^{n}}{2^{n}}$ are integers.,"$\textbf{PROBLEM}$ : Prove or disprove that all elements of sequence $a_{n}=\frac{ \left(1+\sqrt{n^4-n^2+1}\right)^{n} + \left(1-\sqrt{n^4-n^2+1}\right)^{n}}{2^{n}}$ are integers. $\textbf{MY THOUGHTS}$ : First thing to note is that $${\forall n \in \mathbb{N}~~~~~ \exists k \in \mathbb{Z}:~~~~~  \left( n^4-n^2 \right) = 4k}$$ Secondly, according to Binomial theorem, $$ a_{n}=\frac{\sum\limits_{i=0}^{\lfloor \frac n2 \rfloor} {n \choose 2i} \cdot \left(n^4-n^2+1 \right)^i}{2^{n-1}}$$ I've tried to prove statement by induction on $n$ , using facts above, but didn't manage to do it. I also thought about using the fact that $$\begin{cases} \left(1+\sqrt{n^4-n^2+1}\right) + \left(1-\sqrt{n^4-n^2+1}\right) = 2\\ \left(1+\sqrt{n^4-n^2+1}\right) \cdot \left(1-\sqrt{n^4-n^2+1}\right)= n^2 - n^4\end{cases}  $$ So, we could think about $\left(1+\sqrt{n^4-n^2+1}\right)$ and $\left(1-\sqrt{n^4-n^2+1}\right)$ as roots of $x^2 -2x + n^2 -n^4$ .
In that case we can try to solve equivalent problem: Prove that $ \frac{x_1^n+x_2^n}{2^n}$ is integer if $x_1$ and $x_2$ are roots of $x^2 -2x + n^2 -n^4$ .","['elementary-number-theory', 'integers', 'sequences-and-series']"
4140320,taylor series for inverse of error function,"I was asked the following question: Determine the Taylor Series degree 3 around $0$ of the inverse function of $erf(x)$ . I took the first derivative of the function $erf'(x) = \frac{2}{\sqrt{\pi}} e^{-x^2}$ . When $erf(x)=0 \implies x=0$ , thus I would have that $erf'(0)=\frac{2}{\sqrt{\pi}}$ . The derivative of the inverse function is given by the $\frac{1}{erf'(0)}=\frac{\sqrt{\pi}}{2}$ and the first term of the Taylor Series is $\frac{\sqrt{\pi}}{2} x$ . How would I proceed to get the 2nd and 3rd one. For some reason, I got a diferent 3rd derivative than what appears in the solution, and I can't understand why, specifically, why a $\pi^{\frac{3}{2}}$ appears in the 3rd derivative.","['calculus', 'inverse-function', 'derivatives']"
