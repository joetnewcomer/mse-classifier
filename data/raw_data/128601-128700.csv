question_id,title,body,tags
1977100,Prove that a finite group of complex numbers are the $n$th roots of unity,"Prove that if we have a finite group of complex numbers they are the $n$th roots of unity for some finite integer, $n$. I have seen some clunky ways of doing this but is there a very short simple solution?","['finite-groups', 'group-theory', 'complex-numbers']"
1977139,Is it possible to derive the equation for the arithmetic mean?,"As I understand it, the arithmetic mean is a measure of central tendency , i.e. it is a value that quantifies the location of the centre of a distribution of data points (the point about which the data tends to cluster). My question is, is it possible to derive the formula for the arithmetic mean, $\bar{x}$ of a discrete set of data: $$\bar{x}=\frac{1}{N}\sum_{i=1}^{N}x_{i}$$ where $N$ is the total number of data points and $\lbrace x_{i}\rbrace$ are the data points. I have attempted a derivation, but I'm unsure whether it is valid: Suppose one has a finite , discrete set of $N$ data points $\lbrace x_{i}\rbrace_{i=1,\ldots N}$. Assuming this set has a central value (i.e. a mean value), then by definition, the sum of positive deviations should be equal to the sum of negative deviations from this central value (where by positive deviation , we mean that a given data point $x_{i}$ is deviated from the central value $\bar{x}$ by an amount $x_{i}-\bar{x}$ and by negative deviation , that a given data point is deviated from the central value by an amount $\bar{x}-x_{i}$). Now, if we rearrange the set of data points into two subsets, one containing all points each of whose value is less than the central value, i.e. $x_{1},x_{2},\ldots,x_{i}<\bar{x}$, and the other containing all points each of whose value is greater than, or equal to the central value, i.e. $x_{i+1},x_{i+2},\ldots,x_{N}\geq\bar{x}$. It follows that, $$\left(x_{N}-\bar{x}\right)+\left(x_{N-1}-\bar{x}\right)+\cdots +\left(x_{i+1}-\bar{x}\right)-\left(\bar{x}-x_{i}\right)-\left(\bar{x}-x_{i-1}\right)-\cdots -\left(\bar{x}-x_{1}\right)=0$$ Which, upon rearranging terms, gives $$x_{1}+x_{2}+\cdots +x_{i-1}+x_{i}+x_{i+1}+\cdots +x_{N-1}+x_{N}-N\bar{x}=\sum_{i=1}^{N}x_{i}-N\bar{x}=0\\ \Rightarrow\qquad\bar{x}=\frac{1}{N}\sum_{i=1}^{N}x_{i}\;\;.$$","['intuition', 'means', 'statistics']"
1977170,Sum of reciprocals of the perfect powers,"What is the sum of all of the numbers which can be written as the reciprocal of an integer that is also a power greater than one, excluding the powers of one ?. I'm asking about the sum of numbers which can be written as powers. $$
\mbox{This sum would start off as}\quad
\frac{1}{4} + \frac{1}{8} + \frac{1}{9} + \frac{1}{16} + \frac{1}{25} + \cdots
$$ Note that I didn't count $1/16$ twice, as we are only adding numbers which can be written in that form . -I can see how this alternative sum would have probably been easier to handle, since we can see that it equates to the sum $\zeta\left(s\right) - 1$ for all $s$ larger than one.
 - I know that the first sum must converge, because I have worked out that the sum of numbers which can be written in the form $1/\left(a^{b} - 1\right)$ converges to $1$, where
$a,b > 1$. Is there any way of evaluating the first sum ?. Also, does the second sum converge as well and how can we evaluate it ?. I have made a program $\left(~\texttt{python}~\right)$ which carries out partial sums for these sorts of series. I'll attach the source code if anyone wants it.","['number-theory', 'riemann-zeta', 'sequences-and-series', 'analysis']"
1977226,Extension of scalars functor essentially surjective.,Let $f:A\rightarrow B$ be a morphism of rings. When is every $B$-module $N$ of the form $M\otimes_A B$ for some $A$-module $M$?. What are sufficient and necessary conditions on $f$? I know for instance that if $f$ is $A\rightarrow S^{-1}A$ then this is true (take $M$ to be $N$ regarded as an $A$-module).,"['abstract-algebra', 'modules', 'tensor-products', 'commutative-algebra']"
1977248,Reference for: smooth projective curve minus a point is affine,"Does anybody know a textbook or journal reference which contains the proof of the ""well-known"" fact that a  smooth projective curve minus a point (or even finitely many points) is affine? The proof appears here on math.SE multiple times, in Ravi Vakil's notes (but not in his book, at far as I can tell) and other discussion forums. I need something citable. The case of elliptic curves would suffice.","['algebraic-curves', 'reference-request', 'elliptic-curves', 'algebraic-geometry']"
1977250,Proving the result $294!<100^{300}<295!$,"Proving $294!<100^{300}<295!$ $\bf{My\; Try::}$ I have used Stirling Approximation $$n!\approx \left(\frac{n}{e}\right)^n\cdot \sqrt{2\pi n}$$ Put $n=294$ and $n=295$, $$294!\approx \left(\frac{294}{e}\right)^{294}\cdot \sqrt{2\pi \cdot 294}$$ and $$295!\approx \left(\frac{295}{e}\right)^{295}\cdot \sqrt{2\pi \cdot 295}$$ Now i did not understand hoe can i solve it, Help required, Thanks","['algebra-precalculus', 'number-comparison']"
1977284,Why is $ Var(X)= E(Var(X|Y))+Var(E(X|Y)) $?,I see this identity but it does not make sense to me. $ Var(X)= EVar(X|Y)+Var(E(X|Y)) $ Is this something that should intuitively make sense? Is it saying the variance of $X$ is equal to the expected variance of $X$ given $Y$ plus the variance of $Y$ ? I say the second part because since $E(X|Y)$ is a function with Y as variable $Var(E(X|Y))$ would capture the variance of $Y$ .,['probability']
1977299,"Given, $x=\cos \theta +i \sin \theta$, then find $x-x^{-1}$?","Given, $x=\cos \theta +i \sin \theta$, then find $x-x^{-1}$? My attempt: It's a Euler formula as :
$$x=\cos \theta +i \sin \theta=e^{i\theta}$$ And, also $$\cos \theta -i \sin \theta=e^{-i\theta}$$ I have tried it to solve as :
(there is two different solution possible!) $\color{blue}{\text{First solution}}$ $x-x^{-1}=x-\cfrac{1}{x}=\cfrac{x^2-1}{x}=\cfrac{e^{2i\theta}-1}{e^{i\theta}}$ But, there is confusion to get the correct value (ambiguity): $\color{red}{\text{Second solution}}$ $x-x^{-1}=\cos \theta +i \sin \theta -\cos \theta +i \sin \theta=2i\sin\theta$ Can you explain it, please?","['algebra-precalculus', 'trigonometry']"
1977318,Exponential inequality(Proof) [duplicate],"This question already has answers here : Proving the exponential inequality: $x^y+y^x\gt1$ (3 answers) Closed 7 years ago . Given $\forall x,y>0$ Prove that $x^y+y^x\ge1$ I have tried weighted inequalities and Jensen's but unfortunately ended up no where. Please help me. (I know this is a basic inequality).","['inequality', 'exponential-function', 'analysis']"
1977346,What do we call a metric that doesn't satisfy triangle inequality?,"According to wikipedia a metric is a function $d(x,y)$ that satisfies non-negativity identity of indiscernibles symmetry triangle inequality What should we call $d(x,y)$ if the triangle inequality (and probably identity of indiscernibles) doesn't hold?
For example $d(x,y)=x^2+y^2-3xy$ ?","['general-topology', 'metric-spaces']"
1977360,Expected value from a game which gives random numbers from 1 to 100 with reducing payoff,"I bumped into a probability problem, proposed by an app. Here it is! ""In front of you is an infinitely-lived machine that proposes amounts of money, which you can either accept or reject. If you accept, the machine hands over the proposed amount, but shuts down and will never give you anything else. If you reject, it'll show you a new proposal next period. 
Each period's proposal is an independent draw from a uniform distribution on [0, 100]. The time between periods is long — several months, say — and you are impatient: a dollar next period is worth only 0.9 to you today; similarly, a dollar two periods from now is worth 0.9*0.9 = 0.81 today, et cetera. 
If your strategy were to always accept, you'd expect to make 50 dollars, i.e. the mean of the first draw. If instead you decided to accept any first proposal above 50, and — in case you reject the first — any second proposal whatsoever, your expected discounted payoff would be 60 dollars. But you can do better! 
If you follow the strategy that maximizes your expected discounted payoff, what is the threshold above which you should accept the machine's first proposal?"" Now, I solved others similar problems, but this time the payoff gets reducing by 10% every time I reject the amount and go on with the game. If you could enlighten me, I'd appreciate it.","['combinatorics', 'probability']"
1977507,Is matrix with entries $a_{ij} = x_ix_j$ positive semidefinite?,I wonder if I can proof that the square matrix with entries $a_{ij} = x_ix_j$ for a given vector $x$ is positive semidefinite. I hope it is because this matrix is somehow related to this question where I asked if mean square error is convex function in linear regression. I actually calculated the Hessian matrix and obtained the one I give you (multiplied by 2). The problem is that I don't know how to proof that it is positive semidefinite so that I can show that mean square error is convex. Please give a proof or a counterexample.,"['hessian-matrix', 'real-analysis', 'matrices', 'convex-analysis', 'linear-algebra']"
1977511,Can tangent and cotangent spaces be distinguished?,"Let $M$ be a Riemannian manifold, and let $p \in M$. I know that the tangent space at $p$, $T_pM$, is isomorphic as a vector space to the cotangent space at $p$, $(T_pM)^*$. Thus, in some sense, as vector spaces , they are indistinguishable. Since $M$ is a Riemannian manifold, $T_pM$ has an inner product, the Riemannian metric, and thus is not just a vector space, but also a Hilbert space. Thus elements $v \in T_pM$ have notions of angle and length, and can be thought of as ""geometric vectors"", i.e. arrows pointing from some origin. Do elements $w \in (T_pM)^*$ also have notions of length and angle, allowing them to be thought of as geometric vectors? Or is $(T_pM)^*$ just a metrizable topological vector space, allowing one to think of $w \in (T_pM)^*$ as ""points"", objects for which there is a meaningful notion of distance and a ""Cartesian grid"", as well as notions of addition and scalar multiplication, but not of length or angle. Overarching question: do the tangent space at $p$, $T_pM$, and the cotangent space at $p$, $(T_pM)^*$, have any different structures defined on them which are not defined on their counterpart? And for those structures which are defined on both sets, for which are they isomorphic? Is the cotangent space also a Hilbert space? If so, are the tangent and cotangent spaces isomorphic as Hilbert spaces ? Is the cotangent space also a Banach space? If so, are they isomorphic as Banach spaces ? Are they both metric spaces? If so, are they isomorphic as metric spaces ?","['riemannian-geometry', 'differential-geometry', 'soft-question', 'geometry']"
1977519,"Series of functions: convergence interval type vs. monotony of the function and calculation of $\mathrm{sup}_{x \in [r,\infty)}|f_n(x)|$","I have two doubts regarding the normal convergence of a function series. Consider a real functions series $f_n(x): A \subset \mathbb{R} \to  \mathbb{R}$ of wich I want to study the normal convergence .
$$\sum_{n \geq 0} f_n(x)$$ Suppose that I find that the series
$$\sum_{n \geq 0} |f_n(x)|$$
converges pointwise in a open interval $I=(a,+ \infty)$. Since normal convergence is stronger than pointwise convergence of $\sum_{n \geq 0} |f_n(x)|$, only $x \in I$ should be considered. Besides that, proving that series converges in a open interval (as $I$ is) would imply that it converges normally also in the corresponding closed interval, that is $I'=[a,\infty)$, but this would also imply that the series $\sum_{n \geq 0} |f_n(x)|$ converges pointwise in $x=a$ which is not true. Therefore I will look in ""smaller"" intervals inside $I$, so for $x \in J_r=[r,\infty)$ with $r>a$. The normal convergence requires to find some constants $M_n$ such that
$$\mathrm{sup}_{x \in [r,\infty)}  |f_n(x)|<M_n \,\,\,\, \mathrm{and} \,\,\,\, \sum_{n \geq 0} M_n \,\,\, \mathrm{converges}$$ That's the situation, now I will explain my doubts. I noticed some link between the type of the convergence interval $I$ (pointwise convergence of $\sum_{n \geq 0} f_n(x)$, in general) and the monotony of the function $f_n(x)$, that is If the function  $f_n(x)$ is monotonically increasing in $I$, then $I$ is of
the type $I=(-\infty,a)$, $a \in \mathbb{R}$ If the function  $f_n(x)$ is monotonically decreasing in $I$, then $I$ is of the type $I=(a,+\infty)$, $a \in \mathbb{R}$ If the function $f_n(x)$ is neither increasing or decreasing (for istance it has a maximum in $I$) then $I$ is of the type $(a,b)$, $a,b \in
\mathbb{R}$ I'm totally not sure about this statement, is there any link between the monotony of $f_n(x)$ and the type of interval in which $\sum_{n \geq 0} f_n(x)$ converges (in the general case, pointwise)? My second doubt is linked to the previous one. Suppose for istance to be in case 2. (as the situation described at the beginning).
If I want to prove the normal convergence, I must consider $\mathrm{sup}_{x \in [r,\infty)}|f_n(x)|$. Which of the following two is correct? $\mathrm{sup}_{x \in [r,\infty)}|f_n(x)|= |f_n(r)|$ $\mathrm{sup}_{x \in [r,\infty)} |f_n(x)| \leq |f_n(r)|$ The second one includes the first one, but the two are not the same thing. Putting a $=$ means to take $|f_n(r)|$ as the smallest of the numbers such that 
$|f_n(x)| \leq w \,\,\,\, \forall x \in [r,\infty)$, but is this always true?
Would it be a good idea to put a $\leq$ sign, is such way to be sure that the relation holds true?","['monotone-functions', 'limsup-and-liminf', 'sequences-and-series', 'functions', 'convergence-divergence']"
1977520,"$P(x)+P'''(x)\geq P'(x) + P''(x)$ then $P(x)>0 \, \forall x\in \mathbb R $","Let $P(x)$ be a polynomial function of real coefficients with the following property:
$$P(x)+P'''(x)\geq P'(x) + P''(x) $$
then, $$P(x)\geq0 \quad \forall x\in \mathbb R $$ I've tried writing the polynomial in its expanded form and cancelling terms, without any real success. Thanks for your help.","['functional-inequalities', 'functions', 'functional-equations']"
1977524,Does two variables have same correlation before and after applying copula?,"I was reading a blog about copula. The post starts with two variables $X$, $Y$ sampling from a multivariate Gaussian with some kind of positive correlation, let's say $\rho$. Then applying copula (basically doing two times inverse cumulative function trick), we have new variables $X'=F^{-1}(\Phi(X))$ and $Y'=G^{-1}(\Phi(X))$, where $\Phi(x)$ is normal cumulative distribution function. While I understand $X'$ and $Y'$ then have marginal distribution $F(x)$ and $G(x)$, my question is does $corr(X', Y')$ equal $\rho$? My guess is not necessarily, then does any method help correct variant correlation in case we want to maintain same correlation after applying copula?","['probability-theory', 'probability', 'statistics']"
1977577,Lebesgue integrable implies measurable,"if a function is lebesgue integrable, does it imply that it is measurable?
(without any other assumption) The reason why I ask this is because royden, in his book, kind of imply about a measurable function when assuming the function to be lebesgue integrable",['measure-theory']
1977588,Why are closed intervals used for continuity and open intervals for differentiability?,"In books like Calculus (Larson), in the theorems'definitions like Rolle's theorem, when they talk about continuity, they use closed intervals [a,b]. But when they talk about differentiability they use open brackets (a,b). Why are closed intervals used for continuity and open intervals  for differentiability? Why can't you say ""differentiable on the closed interval [a,b]""? Rolle's Theorem definition","['derivatives', 'general-topology', 'continuity', 'functions']"
1977686,A curious case of $1729$,"Ramanujan's Taxicab number 1729 is famous for being the smallest positive integer which can be written as the sum of two positive cubes in two different ways. On a different note, I observed that $12^3 + 1^3 = 1729$ $12^2 + 1^2 = 1\cdot7\cdot2\cdot9 + 1 + 7 + 2 + 9$ $12^1 + 1^1 = -1+7-2+9$ Question 1 : Is there any other $n$ number with the property that if $n = a^3 + b^3$ for some positive $a$ and $b$ then $$
a^2 + b^2 = \text{Products of the digits of $n$} \ + \ \text{Sum of the digits of $n$} 
$$ Question 2 : While searching for a solution of Question 1, the program run by Peter has found only two solution, $(6,11)$ and $(1,12)$ for $1 \le a,b \le 20000$. Looks like there are no more solution. Can this be proven or disproven?","['problem-solving', 'number-theory', 'puzzle', 'recreational-mathematics', 'elementary-number-theory']"
1977697,Dobiński's formula,A derivation of Dobiński's formula is reproduced below. Why does the upper limit on the first sum change from $n$ to $\infty$ ? (See the red-boxed portion.),['combinatorics']
1977800,Matrices over noncommutative rings,"I am wondering whether matrices over noncommutative rings have gone undergone a systematic study, particularly noncommutative group rings? I would appreciate sources, if any are available. Thanks!","['matrices', 'reference-request', 'noncommutative-algebra', 'reference-works']"
1977803,Current induced by a locally integrable differential form: I don't understand why it is not trivially $0$,"Today this question captured my attention, hence I want to generalize it. Let $X$ be a complex manifold of dimension $M$ and let $\omega $ be a $(n-p,n-q)$-differential form such that in each chart it is represented by locally integrable functions. A standard example of current on $X$ (see for example De Rham - Differential manifolds, Chap. III example 2) is the following: $$[\omega]:\alpha \mapsto\int_X \alpha\wedge\omega$$
where $\alpha$ is a $C^\infty$ $(p,q)$-form and $\alpha\wedge\omega$ is a locally integrable $(n,n)$-form. Who ensures that $\alpha\wedge\omega$ is integrable? Usually the integral is defined for smooth differential forms on oriented manifolds (see for example Lee's book). About this point I'm quite sure the answer will be: ""integration can be extended to locally integrable forms"", I just wanted  to check. Consider $n=1$, $p=0$, $q=0$ and let'es examine the example of the question linked above:
$$\omega=\partial\bar\partial\log|f|$$
where $f$ is a meromorphic function on the Riemann surface $X$. Then the current $[\omega]$ is $$[\omega]:g \mapsto\int_X g\omega$$ for any $C^\infty$ function $g$. Here the problem: note that $\omega$ is $0$ almost everywhere, in particular $\omega$ is supported in the set of zeroes and poles of $f$ i.e. in a finite set! Why is the integral $\int_X g\omega$ different from $0$? It seems almost obvious to deduce that the integral of a differential form supported in a finite set is $0$ because of the properties of the Riemann integral in $\mathbb R^n$. I've been thinking to this fact all the day but without any solution.","['riemann-surfaces', 'complex-manifolds', 'differential-forms', 'integration', 'differential-geometry']"
1977812,"Is always possible say that $\langle df(\frac{\partial\varphi}{\partial u})\wedge df(\frac{\partial\varphi}{\partial v}) ,N_2\rangle>0$","Let $S_2$ be an orientable regular surface and $f:S_1\to S_2$ be a differentiable map which is a local diffeomorphism at every $p\in S_1$. Prove that $S_1$ is orientable. My attemp: Suppose that $S_2$ is oriented by $N_2$ (since all orientable surface has a vector field of unit vectors N, we can assume that $S_2$ is oriented by $N_2$), and let $\{\varphi_{\lambda}:U_\lambda\to S_1\}_{\lambda\in\Gamma}$ a family of coordinate neighborhood of $S_1$. Since $f$ is a local diffeomorphism, then $df_p$ is injective, but in particular $T_{f(p)}(S_2)=df_p(T_p S_1)$, and if is necessary we interchange $u$ and $v$, we can assume $$\left\langle df_p\left(\dfrac{\partial\varphi_{\lambda}}{\partial u}\right)\wedge df_p\left(\dfrac{\partial\varphi_{\lambda}}{\partial v}\right) ,N_2\right\rangle_{f(p)}>0\qquad (*)$$ Now, such $\{\varphi_{\lambda}\}_{\lambda\in\Gamma}$ is an orientation of $S_1$. Suppose that $\varphi_{\alpha},\varphi_{\beta}\in\{\varphi_{\lambda}\}_{\lambda\in\Gamma}$ and such that $\varphi_{\alpha}(r,s)\cap\varphi_{\alpha}(u,v)\neq0$. Now, for the change of cordinates we have $$(\varphi^{-1}_{\alpha}\circ\varphi_{\beta})(r,s)=(u(r,s),v(r,s))\implies \varphi_{\beta}(r,s)=\varphi_{\alpha}((u(r,s),v(r,s)))$$
Then, $$\dfrac{\partial\varphi_{\beta}}{\partial r}=\dfrac{\partial\varphi_{\alpha}}{\partial u}\dfrac{\partial u}{\partial r}+\dfrac{\partial\varphi_{\alpha}}{\partial v}\dfrac{\partial v}{\partial r}$$ $$\dfrac{\partial\varphi_{\beta}}{\partial s}=\dfrac{\partial\varphi_{\alpha}}{\partial u}\dfrac{\partial u}{\partial s}+\dfrac{\partial\varphi_{\alpha}}{\partial v}\dfrac{\partial v}{\partial s}$$ So, $df\left(\dfrac{\partial\varphi_{\beta}}{\partial r}\right)\wedge df\left(\dfrac{\partial\varphi_{\beta}}{\partial s}\right)=\dfrac{\partial(u,v)}{\partial(r,s)}df\left(\dfrac{\partial\varphi_{\alpha}}{\partial u}\right)\wedge df\left(\dfrac{\partial\varphi_{\alpha}}{\partial v}\right)$. Therefore, $$0<\left\langle df\left(\dfrac{\partial\varphi_{\beta}}{\partial r}\right)\wedge df\left(\dfrac{\partial\varphi_{\beta}}{\partial s}\right) ,N_2\right\rangle=\dfrac{\partial(u,v)}{\partial(r,s)}\left\langle df\left(\dfrac{\partial\varphi_{\alpha}}{\partial u}\right)\wedge df\left(\dfrac{\partial\varphi_{\alpha}}{\partial v}\right),N_2\right\rangle$$
Which implies that $\dfrac{\partial(u,v)}{\partial(r,s)}>0$ and a regular surface is say ""orientable"" if is possible cover it with a family of coordinates neighborhood such that if a point belongs two differents coordinates family then, the change of coordinates has positive jacobian in such point. The solution of this problem looks fine to me, but I have the little problem that is, if is always possible say that $\left\langle df_p\left(\dfrac{\partial\varphi_{\lambda}}{\partial u}\right)\wedge df_p\left(\dfrac{\partial\varphi_{\lambda}}{\partial v}\right) ,N_2\right\rangle_{f(p)}>0$. My Attemp: If I have a basis for $T_{p}(S_1)$, $\{\varphi_u,\varphi_v\}$, and $df_p:T_p(S_1)\to T_{f(p)}(S_2)$ is a injective linear map, then $\{df_p(\varphi_u),df_p(\varphi_v)\}$ is linearly independent in $T_{f(p)}(S_2)$, i.e., a basis for $T_{f(p)}(S_2)$. Now, we can define a positive basis if $$\left\langle df_p(\varphi_u)\wedge df_p(\varphi_v), N_2 \right\rangle>0$$ Because a orientation $N_2$ on $S_2$ induces an orientation on each tangent space $T_{f(p)}(S_2)$. This is right?. Thanks!","['differential-geometry', 'proof-verification']"
1977824,Why does my textbook not define complex square and square root functions at the origin?,"For example, the square function is defined as $w=f(z)=z^2=r^2e^{i2\theta}$, where r>0 and $-\pi<\theta\leq \pi$.  Why is the origin always excluded?  Also, why do we use the branch cut on the negative real axis for the square root function when $f$ is also one-to-one on the the larger $D=\{re^{i\theta}:\text{$r>0 \space and -\pi/2<\theta\leq \pi/2$}\}$?",['complex-analysis']
1977859,Astonishing: the sum of two infinite products of nested radicals equal to $ \pi $.,"Recently, I found the following handwritten expression in an old math book of my family. Probably it belonged to my great grandfather Boris, who had a P.D. in mathematics. $ \pi = \frac{4\sqrt{5}}{5}.\frac{2}{\sqrt{2 + \frac{4}{\sqrt{5}}}}.\frac{2}{\sqrt{2 + \sqrt{2 +\frac{4}{\sqrt{5}}}}}.\frac{2}{\sqrt{2 + \sqrt{2 + \sqrt{2 +\frac{4}{\sqrt{5}}}}}}... +\frac{2\sqrt{10}}{5}.\frac{2}{\sqrt{2 + \frac{6}{\sqrt{10}}}}.\frac{2}{\sqrt{2 + \sqrt{2 +\frac{6}{\sqrt{10}}}}}.\frac{2}{\sqrt{2 + \sqrt{2 + \sqrt{2 +\frac{6}{\sqrt{10}}}}}}... $ I found this identity extremely interesting, and, in fact, I had never seen it. It's similar, but different, from Vieta's formula for $ \pi $ How to prove this identity?","['real-analysis', 'infinite-product', 'trigonometry', 'calculus', 'pi']"
1977903,"If $a,b$ are linearly independent functions on an interval $I$, are they linearly independent on any interval $J$ contained in $I$?","Let's say $~a,~ b~$ are linearly independent functions on an interval $~I~$ . Are they linearly independent on any interval $~J~$ contained in $~I~$ ? If so, how do I prove it? Let's say $~a,~ b~$ are instead linearly dependent functions on an interval $~I~$ . Are they linearly dependent on any interval $~J~$ contained in $~I~$ ? If so, how do I prove it? I have a feeling I'm supposed to use the Wronskian determinant for these but I'm not sure how to apply it.","['wronskian', 'ordinary-differential-equations']"
1977926,Which of the following sets of matrices are dense in the set of square $n \times n$ square matrices over $\mathbb{C}$?,Practicing for the GRE I found this question and I was wondering if anyone had any general tips to approach this type of questions or any literature I could review to approach them. Which of the following sets are dense in the set of square $n \times n$ square matrices over $\mathbb{C}$? I) Invertible matrices II) Unitary Matrices III) Symmetric Matrices IV) Diagonalizable Matrices.,"['matrices', 'linear-algebra', 'gre-exam']"
1977972,Escaping from a circle of fat lions.,"You are surrounded, by X fat lions equally spaced around a circle of radius 200 meters in an open field.  While making your escape plan you note several things: they are slow, they can only travel at one tenth of your speed, they are stupid, 
they can only move directly at your current position, and they can’t cooperate with one another. If any lion gets within 1 meter of you, you will be eaten. What is the maximum value of X for which you have a strategy to escape from them?","['puzzle', 'recreational-mathematics', 'differential-games', 'calculus']"
1977986,How would I show that $f(A \cup B) = f(A) \cup f(B)$?,If A and B are a subset of C. I know it's true by showing examples but how is it proved?,"['functions', 'discrete-mathematics']"
1978054,What is the measure of angle F?,"This is an isosceles triangle with base angles being the only part filled in. I have trouble understanding that if the base angles are not like terms, then how can I solve my equation (which is angle measure D equals angle measure E)? Help? This is my triangle. Also, I checked in the back of my math book and it told me the correct answers were either 130 degrees or 172 degrees, but I still don't understand how they came about these answers.","['triangles', 'geometry']"
1978063,What does it mean $2^\mathbb{N}$? [duplicate],"This question already has answers here : Strange set notation (a set as a power of 2)? (4 answers) Closed 7 years ago . I have an exercise that involves this set: $2^\mathbb{N}$, but I don't know what set it is. I know that the elements of the set are functions whose domain is $\mathbb{N}$, but no idea which type of functions they are. Thanks in advance.","['notation', 'elementary-set-theory']"
1978107,"Is {{∅}} ⊂ {{∅},{∅}} true","Is {{∅}} ⊂ {{∅},{∅}} true or false. I can't decide if this question is true or false. It seems to be false as the sets would be equal? is that correct since an proper subset isn't equal. the ⊂ in this means proper subset, the answer is false thanks to the below response.",['discrete-mathematics']
1978142,$\sum _{n=1}^{\infty} \frac 1 {n^2} =\frac {\pi ^2}{6}$ and $ S_i =\sum _{n=1}^{\infty} \frac{i} {(36n^2-1)^i}$ . Find $S_1 + S_2 $,I know to find sum of series using method of difference. I tried sum of write the term as (6n-1)(6n+1). i don't know how to proceed further.,['sequences-and-series']
1978171,Is this a valid way to prove this modified harmonic series diverges?,"I am trying to find a way to prove that $$\dfrac 11 + \dfrac 12 + \dfrac 13 + \dfrac 14  + \cdots \color{red}{-} \dfrac 18 + \cdots$$ where the pattern repeats every $8$ terms. Knowing about the Riemann Series Theorem , I am a little hesitant about manipulating conditionally convergent series at all. Granted that the harmonic series diverges, is the following a valid way to prove my series diverges? $$\dfrac 11 + \dfrac 12 + \dfrac 13 + \cdots + \dfrac 17 - \dfrac 18 + \cdots = \sum_{n=1}^{\infty} \dfrac 1n - 2 \cdot \dfrac 18\sum_{n=1}^{\infty} \dfrac 1n $$ $$=\dfrac 34 \sum_{n=1}^{\infty} \dfrac 1n$$ Since the harmonic series diverges, so does $\dfrac 34$ times it.","['real-analysis', 'divergent-series', 'sequences-and-series', 'calculus']"
1978283,Flipping Summation of Kronecker Products,"Question Suppose $\mathbf A$ is an $n\times n$ matrix, and that $\mathbf B_i$ is an $m\times m$ matrix, for all $i\in\{1,\dots, n\}$. Is it possible to find $n\times n$ matrices $\mathbf U$ and $\mathbf V$, and $m\times m$ matrices $\mathbf X_i$, for all $i\in\{1,\dots, n\}$, such that $$\left[\sum_{i}(\mathbf E_{ii}^n\otimes \mathbf B_{i})\right](\mathbf A\otimes \mathbf I_{m})\left[\sum_{i}(\mathbf E_{ii}^n\otimes \mathbf B_{i})\right]=(\mathbf U \otimes \mathbf I_{m})\left[\sum_{i}(\mathbf E_{ii}^n\otimes \mathbf X_{i})\right](\mathbf V\otimes \mathbf I_{m}), $$ where $\{\mathbf E_{ij}^n\}$ are the $n\times n$ matrix units? Perhaps this is not possible under these general conditions, so basically I'm trying to find the minimal set of assumptions (i.e. necessary and sufficient) on the $\{\mathbf B_i\}$ matrices that would allow such a transformation. Obs$_1$: The answers to this question can be helpful to generalize Assumption 2 bellow in the step that leads to equation $(2.2)$. Obs$_2$: If a complete answer is not possible, I would be happy with an answer that simply relaxes one of the assumptions bellow. $$$$
$$$$ How far I've been able to go, i.e. strong assumptions that work: Assumption 1 Suppose that $$ \mathbf B_i \mathbf B_j = d_{ij} \mathbf D. \tag 1$$ To see how it works in this case, notice that $$\left[\sum_{i}(\mathbf E_{ii}^n\otimes \mathbf B_{i})\right](\mathbf A\otimes \mathbf I_{m})\left[\sum_{i}(\mathbf E_{ii}^n\otimes \mathbf B_{i})\right]= \sum_{i,j}(\mathbf E_{ii}^n\mathbf A\mathbf E_{jj}^n\otimes \mathbf B_{i}\mathbf B_{j}), $$ and that under $(1)$, $$\sum_{i,j}(\mathbf E_{ii}^n\mathbf A\mathbf E_{jj}^n\otimes \mathbf B_{i}\mathbf B_{j})=\sum_{i,j}(a_{ij}d_{ij}\mathbf E_{ij}^n\otimes \mathbf D), $$ so it is enough to set $$ \mathbf U = \sum_{i,j}a_{ij}d_{ij}\mathbf E_{ij}^n, \quad \mathbf X_i = \mathbf D, \quad \text{and} \quad \mathbf V = \mathbf I_n.$$ Assumption 2 For all $i\in\{1,\dots, n\}$, let $$	\mathbf B_{i} = \operatorname{diag}(b_{i1},...,b_{im}),\tag{2.1}$$ and notice that \begin{align}
\left[\sum_{i=1}^n(\mathbf E_{ii}^n\otimes \mathbf B_{i})\right](\mathbf A\otimes I_{m})\left[\sum_{j=1}^n(\mathbf E_{jj}^n \otimes \mathbf B_{j})\right]
&=\sum_{i=1}^n\sum_{j=1}^n(\mathbf E_{ii}^n \mathbf A \mathbf E_{jj}^n\otimes \mathbf B_{i}\mathbf B_{j})\\[1.5ex]
&=\sum_{i=1}^n\sum_{j=1}^n(a_{ij}\mathbf E_{ij}^n \otimes\sum_{k=1}^{m}b_{ik}b_{jk}\mathbf E_{kk}^{m})\\[1.5ex]
&=\sum_{i=1}^n\sum_{j=1}^n\sum_{k=1}^{m}(a_{ij}b_{ik}b_{jk}\mathbf E_{ij}^n\otimes \mathbf E_{kk}^{m})\\[1.5ex]
&=\sum_{k=1}^{m}\left(\left(\sum_{i=1}^n\sum_{j=1}^n a_{ij}b_{ik} b_{jk}\mathbf E_{ij}^n\right)\otimes \mathbf E_{kk}^{m}\right)
\end{align} Next, suppose $ \sum_{i=1}^n\sum_{j=1}^n a_{ij}b_{ik}b_{jk} \mathbf E_{ij}^n$ has eigenvectors independent of $k$, so that its eigendecomposition can be written as $$ \sum_{i=1}^n\sum_{j=1}^n a_{ij}b_{ik}b_{jk} \mathbf E_{ij}^n = \mathbf Q \mathbf \Omega_{k}\mathbf Q^{-1} \tag{2.2} $$ with $$	\mathbf \Omega_{k} = \operatorname{diag}(\omega_{k1},...,\omega_{kn}).$$ Then \begin{align}
\sum_{k=1}^{m}\left(\left(\sum_{i=1}^n\sum_{j=1}^n a_{ij}b_{ik} b_{jk}\mathbf E_{ij}^n\right)\otimes \mathbf E_{kk}^{m}\right)
&=\sum_{k=1}^{m}\left(\sum_{i=1}^n\omega_{ki}\mathbf Q\mathbf E_{ii}^n \mathbf Q^{-1}\otimes \mathbf E_{kk}^{m}\right)\\[1.5ex]
&=(\mathbf Q\otimes \mathbf I_{m})\left[\sum_{k=1}^{m}\sum_{i=1}^n(\mathbf E_{ii}^n\otimes \omega_{ki}\mathbf E_{kk}^{m})\right](\mathbf Q^{-1}\otimes \mathbf I_{m})\\[1.5ex]
&=(\mathbf Q\otimes \mathbf I_{m})\left[\sum_{i=1}^n(\mathbf E_{ii}^n\otimes \sum_{k=1}^{m}\omega_{ki}\mathbf E_{kk}^{m})\right](\mathbf Q^{-1}\otimes \mathbf I_{m})
\end{align} so, we can set
$$ \mathbf U = \mathbf Q,\quad \mathbf X_{i} =\sum_{k=1}^{m}\omega_{ki}\mathbf E_{kk}^{m},\quad\text{and}\quad \mathbf V = \mathbf Q^{-1}.$$","['matrices', 'kronecker-product', 'linear-algebra']"
1978310,Find $\sum_{k=1}^{\infty}\frac{1}{2^{k+1}-1}$,Calculate $$\sum_{k=1}^{\infty}\frac{1}{2^{k+1}-1}.$$ I used Wolfram|Alpha to compute it and got it to be approximately equal to $0.6$. How to compute it? Can someone give me a hint or a suggestion to do it?,"['summation', 'sequences-and-series']"
1978388,Counting permutations with k maximums,"I'm trying to solve a problem that counts the number of permutations of N elements with K maximums, where a maximum is defined as an element in the permutation that is greater than all of the elements to its left. For simplicity let's consider the elements ${1..n}$. As an example, for N=3 and K=2, the number is 3 1 3 2, 2 1 3 and 2 3 1 Here's a recurrence for this that I have found: $P[n][k] = \sum_{i=1}^{i<k} P[n-i][j-1] * {n-1 \choose n-i} * (i-1)! = \sum_{i=1}^{i<k} P[n-i][j-1] * \frac{(n-1)!}{(n-i)!}$ Here's how I got to this: I try to put every digit $1..n$ on the first position and then I need to calculate the number of permutations of n-i elements with j-1 maximums (because i already have a maximum on the first position). n-i because the elements $1..i-1$ can never be maximums. And once i got that number I figure out the number of ways i can arrange all those since I have n-1 total elements. Now I know there's a simpler recurrence for this, but I can't figure out how to get to it from what I have so far. The simpler recurrence by the way is $P[n][k] = (n-1) * P[n-1][k] + P[n-1][k-1]$ and is known as the Stirling number of the second kind if I'm not mistaken.","['permutations', 'combinatorics', 'recurrence-relations']"
1978403,How to show $(S \circ R)^{-1} = R^{-1} \circ S^{-1}$?,"Start Let $R$ be a relation from $A$ to $B$, and let $S$ be a relation
from $B$ to $C$. The composite of $R$ and $S$ is
$$S \circ R = \{(a, c):\text{ there exists $b \in B$ such that $(a, b) \in R$ and }(b, c) \in S\}.$$ pf. $(x,y) \in (S \circ R)^{-1}$ 
$$\text{ iff }{(a, c): ∃b \in B\text{ such .that }(a, b)\in R \text{ and }(b, c) \in S}$$
$$\text{iff }{(a,c): ∃b \in B\text{ such that }(b,a)\in R\text{ and }(a,c) \in S} $$ I am lost at this point. I am pretty sure I have to do an iff proof, and just use the definitions properly, but I seem to not clearly understand them. If I could get some assistance that would be great. And I believe my formatting is off, but I have no clue how line up the iff, so, if someone could show me that too. Thank you.","['relations', 'function-and-relation-composition', 'elementary-set-theory', 'inverse']"
1978407,Can an irreducible representation have a zero character?,"Is there an example of the following situation : $F$ is a field, $G$ is a finite group, $\rho$ is an irreducible $F$-representation of $G$ and the character of this representation takes the zero value at every element of $G$ ? If I'm not wrong, $F$ cannot be algebraically closed (Robinson, A Course in the Theory of Groups, 8.1.9, p. 220) and must have a nonzero characteristic $p$ such that the degree of the representation is divisible by $p$ and such that $G$ is not a $p$-group (Robinson, exerc. 8.1.5, p. 222). But that doesn't solve the problem. Thanks in advance for the answers. (Edit. Since I didn'get answers here, I asked the question on Mathoverflow : https://mathoverflow.net/questions/252855/can-an-irreducible-representation-have-a-zero-character )","['representation-theory', 'group-theory']"
1978452,Is it possible to find an infinite set of points in the plane where the distance between any pair is rational?,"The question is written like this: Is it possible to find an infinite set of points in the plane, not all on the same straight line, such that the distance between EVERY pair of points is rational? This would be so easy if these points could be on the same straight line, but I couldn't get any idea to solve the question above(not all points on the same straight line). I believe there must be a kind of concatenation between the points but  I couldn't figure it out. What I tried is totally mess. I tried to draw some triangles and to connect some points from one triangle to another, but in vain. Note: I want to see a real example of such an infinite set of points in the plane that can be an answer for the question. A graph for these points would be helpful.","['number-theory', 'rational-numbers', 'geometry']"
1978453,How to shake hand with everybody as fast as possible?,"A relative of mine asked once an optimization problem to our friend. None of us found the solution. Here is a formulation of the problem. Consider $2\times n$ board such that the squares of the form $(1,i)$ are empty and every square of the form $(2,i)$ contains one people per square, for all $1\leq i\leq n$. One says that two squares are adjacent if they have a common side. If there are two adjacent squares that both has a people, they can shake hands. And if a given square has a people and its adjacent square has no people, he or she can move to the free square. People will move simultaneously so that $B$ can move to $A$'s current position if $A$ is going to move to another square at the same turn. On every turn, each people can either stay on his or her place, shake hands with one of his or her neighbor if he or she is not shaking hands with another people, or move to the adjacent square. Is there a (closed form) formula how many turns it takes if all people wants to shake hands with every other people? Or any good upper bound? Or an algorithm that gives the optimal way to shake hands and move between squares?","['algorithms', 'discrete-mathematics']"
1978468,"Given a set of 6 digits, how many combinations can be generated?","I have 6 digits (e.g: 1,2,3,4,5,6), then I use these digits to form a sequence of six digit pairs. Each digit in the given set can be used twice, and only two different digits can be formed into a pair. How to determine the number of sequences that can be generated? Example: The digits provided: 1,2,3,4,5,6
This is a valid sequence:
#1 | #2 | #3 | #4 | #5 | #6
---------------------------
 1 |  3 |  5 |  1 |  3 |  5
 2 |  4 |  6 |  2 |  4 |  6
This sequence won't be counted since it's the same sequence as the one above
#1 | #2 | #3 | #4 | #5 | #6
---------------------------
 1 |  4 |  6 |  2 |  4 |  5
 2 |  3 |  5 |  1 |  3 |  6
But this one will:
#1 | #2 | #3 | #4 | #5 | #6
---------------------------
 1 |  1 |  3 |  3 |  5 |  5
 2 |  2 |  4 |  4 |  6 |  6
This is an invalid sequence (two '1' in the same pair)
#1 | #2 | #3 | #4 | #5 | #6
---------------------------
 1 |  3 |  5 |  3 |  2 |  5
 1 |  4 |  6 |  2 |  4 |  6
This one is invalid, too. Each digit must appear twice, not more or less.
#1 | #2 | #3 | #4 | #5 | #6
---------------------------
 1 |  3 |  5 |  2 |  3 |  5
 2 |  4 |  6 |  1 |  1 |  1 To sum up: We have a set of 6 distinct digits. Two different digits form a pair, without ordering. Each digit must be used twice (i.e: there will always be 2 pair containing the same digits). Six pairs will be placed into a sequence, with ordering.",['combinatorics']
1978490,If two consecutive numbers are removed from the series $1+2+3+\ldots+n$ the average becomes $99/4$. Find the two numbers.,"The initial average will be $\frac{n+1}{2}$. If the two numbers are $k$ and $k+1$ then the new average will be $\frac{n(n+1)/2-(2k+1)}{n-2}$. I couldn't figure further even though I got the relation between $n$ and $k$ in many different ways. If the question is not clear, here is an example to explain it.
If $n=10$, the initial average will be $5\cdot 5$ {$(1+2+\cdots + 10)/10$}
Now if two consecutive numbers like $2,3$ or $8,9$ are removed from this series, the new average changes, and this new average has been given to be $99/4$, however we also don't know the value of $n$, so the question seems to be pretty difficult.","['sequences-and-series', 'average', 'arithmetic']"
1978498,Show that $\left(\bigcup_{i \in I}A_{i}\right )^{c}=\bigcap_{i \in I}A_{i}^{c}$,"I'm learning for a test and I think there will be some task similar to this one. I'd like to know if I did it correct and if not can you please say how to do it correctly? Let $\Omega$ be a set, let $I$ be an index-set and let $A \subseteq
\Omega$ and $A_{i} \subseteq \Omega$ for all $i \in I$. Show that $$\left(\bigcup_{i \in I}A_{i}\right )^{c}=\bigcap_{i \in
I}{A_{i}}^{c}$$ We defined this in our readings: Complement: $A^{c} \equiv \bar{A}= \left\{x \in \Omega; (x \notin A)\right\}$ and $$\overline{\bigcup_{i \in I}A_{i}}=\bigcap_{i \in
I}\overline{A_{i}}$$ I did it like this: $$\left(\bigcup_{i \in I}A_{i}\right)^{c}=\overline{\bigcup_{i \in I}A_{i}}=\bigcap_{i \in
I}\overline{A_{i}}=\bigcup_{i \in I}{A_{i}}^{c}$$",['elementary-set-theory']
1978512,Volumes using triple integration,"I'm having a hard time with this problem, i have some ideas, but i don't know how to continue. Here is the exercise: ""Consider the solid $S$ bounded by the walls of the superior cone whose equation is  $z = \sqrt{3}\sqrt{x^2+y^2} $ and inside the sphere of equation $x^2+y^2+(z-2)^2=4$  whose density at a point $P$ is given by $\delta (x,y,z) =z$. Write (without calculating!) the mass of $S$ as triple iterated integral in both cylindrical and spherical coordinates. (Hint: in spheric coordinates, $dV = \rho^2 sin(\phi) d\rho d\theta d\phi$)."" First of all, i noticed that that $(z-2)^2$ would give me a headache to write in spherical coordinates, so i did the linear variable substitution:
$$u = x$$
$$ v = y$$
$$ w = z -2$$ So, the cone equation became $w+2 = \sqrt{3}\sqrt{u^2+v^2}$ and the sphere $u^2+v^2+w^2 = 4$.
Now, there are 2 problems: 1) I can't visualize the limits of integration. (this is more general, it happens in a LOT of problems that i have to change from rectangular to polar/cylindrical/spherical coordinates, so any tips here are welcome). Also, this is the biggest problem of this exercise, for me. 2) How can i make just one integral for the mass? I mean, in the 1st octant that will be okay, because all of the variables are positive. But, for example, in the 5º octant, wouldn't the volume (then the mass) be negative? When i did some problems calculating mass in spherical/cylindrical coordinates, i always ""broke"" the sphere into 8 parts (when the density was symmetric with respect to the origin) but in this case, i must write it all in just 1 integral. Any help would be great. Thanks!","['multivariable-calculus', 'multiple-integral', 'volume']"
1978562,What surface do we get by joining the opposite edges of a hexagon?,"We know that torus can be obtained from a square by joining the opposite sides. Can we do the same thing with a hexagon? If it's not possible to 'fold' hexagon in 3D Euclidean space, may be it's possible in highter dimensions? Of course, like in the example with a square, stretching is allowed. I intentionaly don't use the correct topology terms, since I don't know topology. I probably should've used terms such as manifold and homeomorphism. I hope the question is clear enough in layman terms. Related question: What are all topological spaces obtained by gluing the edges of a triangle? Edit A useful link from Fredrik Meyer which probably answers the question. And the image from the link which should help with orienation: we see that the surface is the torus. Says the link. If it's true, can we show how folding a hexagon results in a torus?","['manifolds', 'general-topology', 'surfaces']"
1978660,"Question about prove when $\{x_n\}$ and $\{y_n\}$ convergent $\lim x_n = \lim y_n$ where $x_{n+1}=\frac{x_n+y_n}2$, $y_{n+1}=\frac{2x_ny_n}{x_n+y_n}$","Problem $\mathbf{4}$ If $x_{n+1}=\frac{x_n+y_n}2$ , $y_{n+1}=\frac{2x_ny_n}{x_n+y_n}$ for $n=1,2,\ldots,n$ , prove that both $\{x_n\}$ and $\{y_n\}$ are convergent, and $\lim\limits_{n\to\infty}x_n=\lim\limits_{n\to\infty}y_n$ . Solution: Note that $$4x_ny_n\le(x_n+y_n)^2$$ Hence, $$\frac{y_{n+1}}{x_{n+1}}=\frac{4x_ny_n}{(x_n+y_n)^2}\le 1$$ That is, $\color{crimson}{x_n\le y_n}$ . Also, $$x_{n+1}-x_n=\frac{x_n+y_n}2-x_n=\frac{y_n-x_n}2\ge 0$$ implying that $\{x_n\}$ is monotonically increasing. Similarly, $$\frac{y_{n+1}}{y_n}=\frac{2x_n}{x_n+y_n}\le\frac{2x_n}{2x_n}=1$$ implying that $\{y_n\}$ is monotonically decreasing. Now we have $$x_1\le x_2\le\cdot\le x_n\le\cdot\le y_n\le\cdots\le y_2\le y_1$$ By the monotone convergence theorem, it follows immediately that both $\{x_n\}$ and $\{y_n\}$ are convergent. Let $$\lim_{n\to\infty}x_n=x,\lim_{n\to\infty}y_n=y$$ Taking limits on both sides of $x_{n+1}$ will yield that $$x=\frac{x+y}2$$ which implies that $\lim\limits_{n\to\infty}x_n=x=y=\lim\limits_{n\to\infty}y_n$ (Original image here .) I have a question about how I can know that $x_n\le y_n$ (in red above). I know that $y_{n+1} < x_{n+1}$ , but how do I know $x_n\le y_n$ ? Also, what is meant by ‘Taking limits on both sides of $x_{n+1}$ ’? (Questioned parts highlighted in original image.)","['means', 'real-analysis', 'limits', 'sequences-and-series', 'analysis']"
1978734,Expected number of rejected null hypotheses using FDR,"Problem: Let $X_1, X_2, \dots, X_{500}$ be independently identically distributed. For a constant $a$, suppose we know the probability $$P(X_i\leq k*a)\ \forall k = 1,\dots , 500.$$ We now sort the $X$'s so that $X_{(1)}\leq X_{(2)}\leq \dots\leq X_{(500)}$. Find the expectation $E(K)$, $K = \max(k)$ where $k$ satisfies $X_{(k)}\leq k*a.$ My attempt: I don't really have any rigorous proof, but I simply think of this as an ""average"" problem, and so the answer would be $$E(K) = \sum_{k=1}^{500}P(X_i\leq k*a).$$ Note: You may notice that this problem relates to false discovery rate. I have been banging my head against the wall for couple days and can't seem to get anywhere. Any help/suggestions/ideas are much appreciated!","['statistics', 'probability', 'expectation', 'hypothesis-testing']"
1978822,What rule is used in this example (derivative of a complex function)?,"I apologize for the second question on the same day, but I really do need to understand it. We've got a consumption function (it's from economics but the question is still mathematical) looking like: $$C=C(Y-T(Y))$$ where $C$ is the consumption, $Y$ is the income and $T(Y)$ is the tax as the function of income $Y$ (as in real world the taxes we pay are some proportion of the money we've earned); What we need here is to differentiate this function in respect to Y (e.g. find $\frac {dC}{dY}$); When browsing through internet, I've actually found a solution for this but I want to figure out how it's actually done (as it's a number of problems utilizing the same logic, so if I understand this one, it should be easy to do the rest). In that solution it's done the following way $$\frac{dC}{dY} = \frac{dC}{d(Y-T(Y))} * \frac{d(Y-T(Y))}{dY} + \frac{dC}{d(Y-T(Y))} * \frac{d(Y-T(Y))}{dT} * \frac{dT}{dY} $$ (and then it goes along so that the result looks more elegant) I wonder which rule is used to do this. It somehow resembles the multivariable version of the chain rule, but the function we've got here is quite different from classical examples (like $x=y^2z^3$ while $y$ and $z$ are both some functions). Is it really the chain rule? Or something else?","['derivatives', 'economics']"
1978852,"Professor: ""You're trying to take the union of an infinite number of sets, which is simply something you cannot do""","I found this video posted by a Professor N. J. Wildberger, which contains the quote in the title. Is this true? I looked up arbitrary unions on Wikipedia but didn't see much there about problems. Could someone explain why we cannot take an arbitrary union of sets? If it is not true, could you go into some detail about 1) why he might say that and 2) why it is wrong; or provide a source doing so? I'm familiar with the axiom of choice, ordinal numbers, well orderings, etc. and was watching this video for some background. Thank you so much!",['elementary-set-theory']
1978853,Prove that $\mathcal{C}$ is a monotone class,"Let $(\Omega_1,\mathcal{A}_1,\mu_1)$ and $(\Omega_2,\mathcal{A}_2,\mu_2)$ be two measure spaces, where $\mu_1$ and $\mu_2$ are $\sigma$-finite. Consider the product space $(\Omega=\Omega_1\times\Omega_2,\mathcal{A}=\mathcal{A}_1\oplus\mathcal{A}_2,\mu=\mu_1\times\mu_2)$. I want to prove that the set $$\mathcal{C}=\{A\in\mathcal{A}:\,\mu(A)=\int_{\Omega_1}\int_{\Omega_2}1_A\,d\mu_2\,d\mu_1\}$$ is a monotone class (that is, closed under increasing and decreasing sequences of sets), using Lebesgue convergence theorems (but not Fubini). My attempt: the fact that $\mathcal{C}$ is closed under unions is clear by the monotone convergence theorem and the fact that $\mu(\cup A_n)=\lim_n\mu(A_n)$. My problem arises when dealing with intersections. In this case, we may not have $\mu(\cap A_n)=\lim_n\mu(A_n)$, as we do not know whether $\mu(A_1)<\infty$. Here is where I need the $\sigma$-finiteness. Because of it, I can write $\Omega=\cup_{n=1}^{\infty}A_n$, where $Q(A_n)<\infty$ for all $n$. Let $\{B_n\}_n\subseteq\mathcal{C}$ with $B=\cap_n B_n$. I want to show that $B\in\mathcal{C}$. We have $\cap_n (B_n\cap A_m)=B\cap A_m$, with $\mu(B_n\cap A_n)\leq \mu(A_m)<\infty$, so now we do have $\mu(B\cap A_m)=\lim_n\mu(B_n \cap A_m)$. The idea now would be to have $B\cap A_m\in\mathcal{C}$ using a convergence theorem and then use the fact that $B=\cup_m(B\cap A_m)$ to obtain $B\in\mathcal{C}$. For that purpose, I would like to prove that $B_n\cap A_m\in\mathcal{C}$ from the fact that $B_n\in\mathcal{C}$ and that $A_m$ can be chosen as a rectangle in $\mathcal{A}$. Any ideas?","['stochastic-processes', 'monotone-class-theorem', 'probability-theory', 'lebesgue-integral', 'measure-theory']"
1978888,Integral of pointwise max function,"Consider given some functions $f_1,f_2,g\in C^\infty([a,b],\mathbb{R})$ which have both positive and negative values. Call $F:[a,b]\rightarrow \mathbb{R}$ the pointwise maximum of $f_1$ and $f_2$: $F(x):=max\{f_1(x),f_2(x)\}$. Suppose it is true $\int_a^bf_1gdx\ge 0$ and $\int_a^bf_2gdx\ge 0$. Then is it always true $\int_a^bFgdx\ge 0$? I suspect the answer is yes, but I can't work out a formal proof.","['real-analysis', 'integration', 'analysis']"
1979044,Prove that $AB = 0$ implies $\det\left(A + B\right)^2 = \det\left(A - B\right)^2$.,"Given matrices $A, B \in \mathcal{M}_{n \times n}\left(\mathbb{R}\right)$, prove that if $AB = 0$, $\det\left(A + B\right)^2 = \det\left(A - B\right)^2$. This is based on a quiz question from a few weeks ago that I never quite solved, and I forgot my instructors' solutions as well. Here are possible ideas I had: Show that $AB = BA$ (except it's not true). Show that $BA = -BA$—wait, isn't this equivalent to (1)? Show that $\forall i,j\, \left(A_{ij} = 0\right) \mathrm{or} \left(B_{ij}=0\right)$ so that $\det\left(A + B\right) = \left(-1\right)^k\det\left(A - B\right)$ for some $k$. This is definitely a sufficient condition, but I'm not sure if it's necessary. We have learned diagonalization, but we have not learned the Jordan canonical form yet, so it would be ideal if you avoided that kind of answer.","['matrices', 'linear-algebra', 'determinant']"
1979133,$\int_{a}^{\frac{a+b}{2}}\frac{g(x)}{(1-x)^2}dx\geq\int_{\frac{a+b}{2}}^{b}\frac{g(x)}{(1-x)^2}dx$?,"Suppose $g>0$, $g$ decreasing on $[a,b]$ where $0<a<b<1$. Is it true or false that
$$\displaystyle\int_{a}^{\frac{a+b}{2}}\frac{g(x)}{(1-x)^2}dx\geq\int_{\frac{a+b}{2}}^{b}\frac{g(x)}{(1-x)^2}dx ?$$","['integration', 'functions', 'interval-arithmetic']"
1979174,Required conditions for eigenvalues $\lambda_{\min}(A) >\lambda_{\min}(B)$ and $\lambda_{\max}(A) >\lambda_{\max}(B) $ etc?,"Given $\operatorname{tr}(X^TAX) > \operatorname{tr}(X^TBX)$ and $A$ and $B$ are p.s.d then under what conditions will we have $\lambda_{\max}(A)>\lambda_{\max}(B)$ to be guaranteed ? What are required conditions for  $\lambda_{\min}(A)>\lambda_{\min}(B)$ ? All matrix entries are real valued and $X$ is a rectangular matrix. Thirdly, what are the conditions for second smallest eigenvalue (algebraic connectivity) of $A$ to be greater than the same for $B$? Also fourthly, what are the conditions for the eigenvalues of $A$ to majorize the eigenvalues of $B$ from above, below and so forth? And by majorization I mean this mathematical property: https://en.wikipedia.org/wiki/Majorization . And finally and most important of all for me..What are the conditions w.r.t $A(X)$ and $B(X)$ for $\operatorname{tr}(X^TA(X)X) > \operatorname{tr}(X^TB(X)X)$ to be true if $A(X)$ and $B(X)$ are functions of X and matrix valued as well?","['matrices', 'eigenvalues-eigenvectors', 'positive-semidefinite', 'linear-algebra']"
1979203,Question about a lemma concerning vector fields in Do Carmo,"In the book Differential Geometry of Curves and Surfaces by Do Carmo, he gives a proof of the following lemma: Let $w$ be a vector field in an open set $U \subset \mathbb{R}^{2}$ and let $p \in U$ be such that $w(p) \neq 0$. Then there exist $W \subset U$ of $p$ and a differentiable function $f: W \rightarrow \mathbb{R}$ such that $f$ is constant along each trajectory of $w$ and $df_{q} \neq 0$ for all $q \in W$ A trajectory is a curve in $U$ where the tangent vector at each point of the curve is valued of the vector field at that point. I think this is called an integral curve. The proof proceeds, by essentially assuming $p=(0,0)$. A previous theorem lets us assume that there exists an open set $V \subset U$, an interval $I$ and a differentiable function $\alpha: V \times I \rightarrow U$ where $\alpha(p,0)=p$ and $\frac{\partial{\alpha}}{\partial{t}}(p,t)=w(\alpha(p,t))$. We then let $\alpha'$ be the restriction of $\alpha$ to when $x=0$. The text then says that $d\alpha'_{(p,0)}(\text{unit vector in y direction})=\text{unit vector in y direction}$ I don't see how this follows from the definition of $\alpha$.",['differential-geometry']
1979245,$\lim_{x\to \infty}\left(\frac{1}{x}\sum_{i=1}^x(\frac{i}{x})^9\right)$,"For precalculus I had an exam today and we had to solve the following question: $\lim_{x\to \infty}\left(\frac{1}{x}\sum_{i=1}^x(\frac{i}{x})^9\right)$ I can't use l'hôpitals rule to solve this. From calculating with a large value for x I know the value must be around 1, I just need to prove it. I thought it could be solved by moving the $x^9$ outside the summation, idk if thats alright? $\lim_{x\to \infty}\left(\frac{1}{x^{10}}\sum_{i=1}^x(i)^9\right)$","['algebra-precalculus', 'summation', 'limits']"
1979278,Is this a valid proof of Young's theorem?,"I tried to prove Young's theorem (symmetry of mixed partial derivatives) myself, but my proof seems considerably easier than the one I could find in my textbook. Can anybody point out where I went wrong? Let $f$ be a $C^2$ function, so that the first two derivatives of $f$ are continues  and that $f$ is two times differentiable in a point $(a,b)$. We wish to show that $\frac{\partial^2 f}{\partial x \partial y}(a,b)=\frac{\partial^2 f}{\partial y \partial x}(a,b)$. Define $$\Delta(h,k)=f(a+h,b+k)-f(a,b+k)-f(a+h,b)+f(a,b)$$ Notice that 
$$\lim_{h\rightarrow 0} \frac{\Delta(h,k)}{h}=\lim_{h \rightarrow 0}(\frac{f(a+h,b+k)-f(a,b+k)}{h}-\frac{f(a+h,b)-f(a,b)}{h})=$$
$$\frac{\partial f(a,b+k)}{\partial x}-\frac{\partial f(a,b)}{\partial x}$$
and 
$$\lim_{k \rightarrow 0} \frac{1}{k}\cdot \left (\lim_{h\rightarrow 0} \frac{\Delta(h,k)}{h} \right )=\lim_{k \rightarrow 0} \frac{\frac{\partial f(a,b+k)}{\partial x}-\frac{\partial f(a,b)}{\partial x}}{k}=\frac{\partial}{\partial y}(\frac{\partial f(a,b)}{\partial x})=\frac{\partial^2 f(a,b)}{\partial y \partial x}$$ Likewise, taking the limits in opposite order gives $$\lim_{h \rightarrow 0}(\lim_{k \rightarrow 0}\Delta(h,k))=\frac{\partial^2 f(a,b)}{\partial x \partial y}$$ I then conclude that: $$\lim_{(h,k) \rightarrow (0,0)} \frac{\Delta(h,k)}{hk}=\frac{\partial^2 f(a,b)}{\partial y \partial x}=\frac{\partial^2 f(a,b)}{\partial x \partial y}$$ as $f$ is twice differentiable at $(a,b)$, so it shouldn't matter how we approach $(a,b)$. Is this argument correct?","['multivariable-calculus', 'calculus', 'proof-verification']"
1979296,Stabilizers of the adjoint representation of $GL_n(\mathbb{C})$,"The Lie group of invertible matrices $GL_n(\mathbb{C})$ acts by conjugation on its Lie algebra $gl_n(\mathbb{C})$ of all matrices. Orbits of this action are in bijection with canonical forms of matrices: rational canonical forms or Jordan forms. But how one can compute stabilizers of this action? In concrete terms I want to find all invertible matrices commuting with a given matrix in its canonical form. Of course, this question has a general version for any Lie group and adjoint representation on its Lie algebra. Perhaps, it is possible to solve it in this generality, but the case of $GL_n(\mathbb{C})$ is already non-trivial for me. Update: Perhaps, it is easier to understand in terms of modules over $R=\mathbb{C}[t]$. Given an $n \times n$ matrix $A$ we can consider $M = \mathbb{C}^n$ as a $\mathbb{C}[t]$ module on length $n$. Matrices commuting with $A$ are $\text{End}_R(M)$, invertible matrices are invertible elements of this ring $\text{End}_R(M)^*$. Ring $R$ is a PID, and $M \cong \bigoplus_{i,j} R/(t-\lambda_i)^{n_{ij}}$. To compute the endomorphism ring we use $\text{Hom}_R(R/f,R/g) \cong R/\text{g.c.d.}(f,g)$. So
$$
\text{Hom}(\bigoplus_{i,j} R/(t-\lambda_i)^{n_{ij}}, \bigoplus_{k,l} R/(t-\lambda_k)^{n_{kl}}) \cong \bigoplus_{i,j,l} R/(t-\lambda_i)^{\text{g.c.d.}(n_{ij}, n_{il})}
$$
Then 
$$
\text{End}_R(M)^* \cong \bigoplus_{i,j,l} (R/(t-\lambda_i)^{\text{g.c.d.}(n_{ij}, n_{il})})^*
$$
Invertible elements of $R/(t-\lambda)^m$ are truncated polynomials with non-zero constant term.","['matrices', 'abstract-algebra', 'proof-verification', 'lie-groups']"
1979297,Proving the identity $\csc x−\sin x = (\cot x)(\cos x) $,"We recently started Trigonometry and I was trying to solve this. $$\csc x − \sin x=(\cot x)(\cos x) $$ So starting with LHS: \begin{align}
\frac{1}{\sin x} − \sin x &= \frac{1 − (\sin x)^2}{ \sin x } \\
 &= \frac{(\cos x)^2 }{ \sin x } 
\end{align} I am stuck now and wanted to know how should I proceed.  Is this much correct?",['trigonometry']
1979299,"Prove that if Ax = b has a solution for every b, then A is invertible","I am interested in the case that $A$ is a matrix over a commutative ring, not necessarily a field. Is it still true that if $Ax = b$ has a solution for every $b$, then $A$ is invertible? I know that in the general setting, $A$ having the trivial nullspace does not imply that it is invertible. However, I cannot seem to find a counterexample to the fact in the title of the question, so I am starting to believe it is true. Any ideas how to prove it?",['linear-algebra']
1979314,Two homogenous system are equivalent if they have the same answer,Prove that two homogeneous systems of linear equations are equivalent iff they have the same solution set. My definition of equivalent of linear equations is that each equation of system $A$ is a linear combination of the equations of system $B$ and converse. So i have a bit trouble proving this statement in general as $m\times n$ form. I worked with their coefficient matrices and if i show that these two are row equivalent the problem is solved. Is there any method to use for this? If not what's the general prove.,"['linear-algebra', 'systems-of-equations']"
1979344,"Distribution of the largest interval when partitioning [0,1] into n intervals with n-1 uniformly distributed points.","Let $\lbrace x_1,x_2,...,x_{n-1}\rbrace$ be n-1 points that are independently uniformly distributed on interval [0,1]. They create n segments. What is the expected length of the largest of such n segments? What is the distribution? A good if not complete solution here. Average length of the longest segment","['combinatorics', 'probability']"
1979352,Asymptotic growth of a convolution (counting n-bead 2-colour necklaces),"I'm seeking the asymptotic growth of the function $$f(n) = \frac{1}{n}\sum_{d | n} 2^d \varphi(n/d) = \frac{1}{n} \sum_{k=1}^n 2^{\gcd(n, k)} \tag{1}$$
which counts the number of necklaces made of $n$ black or white beads where turning over is not allowed (sequence A000031 in the OEIS).  Here, $\varphi$ is Euler's totient function. Formula (1) for $f$ is easily established by applying Burnside's lemma to the following action of $G = \mathbf{Z}/(n)$ on the set $\{0, 1\}^G$: $$(g\cdot f)(x) = f(xg).$$ The two representations of $f$ given in (1) yield the following bounds:
$$\frac{2^n}{n} \le f(n) \le \frac{2^{n+1}-2}{n} \tag{2}$$
the lower bound coming from picking $d = n$ and the upper bound by noting that $\gcd(n, k) \le k$ and summing the geometric series.
I'm pretty certain that $$f(n) \sim \frac{2^n}{n} \quad (n \to \infty)\tag{3}$$
but from (2) we can only get 
$$1 \le \varliminf_{n \to \infty} \frac{n f(n)}{2^n}\text{ and } \varlimsup_{n \to \infty} \frac{n f(n)}{2^n} \le 2.$$ My question(s) are: is (3) true? If so, how is it proved? If not, what is the correct statement?","['group-actions', 'combinatorics', 'asymptotics', 'totient-function']"
1979414,Can the derivative of a characteristic function be also a characteristic function?,"Suppose $\phi(t)$ is a characteristic function, and $\phi'(t)$ exists for all $t$. Then can $\phi'(t)$ be also a characteristic function for some random variable?",['probability-theory']
1979418,"Pre-image of Measurable sets are measurable under the mapping h(x,y)=x-y","I am trying to prove the following statement: For $h:\mathbb{R}^2\longrightarrow \mathbb{R}$, given by $h(x,y)=x-y$, if $L\in\mathcal{M}(\mathbb{R})$, where $\mathcal{M}(\mathbb{R})$ denotes the set of Lebesgue measurable sets in $\mathbb{R}$, then $h^{-1}(L)\in \mathcal{M}(\mathbb{R}^2)$. Here's the proof I came up with: Take $L\in\mathcal{M}(\mathbb{R})$, we have that:
\begin{align*}
h^{-1}(L)& =\{(x,y)\in\mathbb{R}^2: (x-y)\in L\}\\
& =\{(x,y)\in\mathbb{R}^2: x=z+y, \forall z\in L\}\\
& =\{(x,x-z)\in\mathbb{R}^2: z\in L\}\\
\end{align*} Now, let $F:L\times\mathbb{R}\longrightarrow \mathbb{R}^2$ be the linear transformation (shearing map in this case) associated with the matrix $A=\begin{bmatrix}1 &1\\0 & 1\end{bmatrix} $ in the standard basis for $\mathbb{R}^2$. Clearly, $F$ is injective. Notice, since $L\in\mathcal{M}(\mathbb{R})$, then $L\times\mathbb{R}\in\mathcal{M}(\mathbb{R^2})$, and since $F$ is injective then: $$\left(h^{-1}(L)=F(L\times \mathbb{R})\right)\in \mathcal{M}(\mathbb{R}^2)$$ Is this proof correct? Are there faster ways of going about this by wrapping $h^{-1}(L)$ in $\mathcal{G}_{\delta}$ or some other method?","['real-analysis', 'lebesgue-measure', 'measure-theory']"
1979470,Understanding the Taylor expansion of a function,"Suppose $$f(x) = \frac{1}{1+x^2}$$ We know this function is defined everywhere and is continuous everywhere and so on... Using the geometric series, we can write $$ \frac{1}{1+x^2} = \sum (-x^2)^n = \sum (-1)^n x^{2n} $$ But, this only converges iff $|-x^2|<1$ iff $|x|<1$. Why does this converge only in $(-1,1)$ when we know it is defined everywhere, though?","['taylor-expansion', 'calculus']"
1979471,"Let A1, A2,...,An be sets. Prove that if A1 is a subset of A2, A2 a subset of A3, ..., An subset of A1, then A1=A2=...=An","I need help with the proof of an elementary set theory question. I believe my proof is not rigorous enough. The statement is: Let $A_1$, $A_2$, ..., $A_n$ be sets. Prove that if $A_1\subset A_2$, $A_2\subset A_3$ ,..., $A_{n-1}\subset A_n$  and $A_n\subset A_1$ , then $A_1=A_2=...=A_n$ Attempt at Proof:
Since $A_1\subset A_2$, for every $x\in A_1$, $x \in A_2$. Further since $A_2\subset A_3$, for $x\in A_2$, $x \in A_3$. Continuing the same logic, we would finally have for $x\in A_n$, $x \in A_1$. Thus we showed that $A_2\subset A_1$ , which implies $A_1 = A_2$. Similarly, we would have $A_2 = A_3 = ...= A_n = A_1$ which we had set out to prove. Is my proof correct?",['elementary-set-theory']
1979476,Prove that the tensor product of non algebraic extensions is not a field,"Suppose that $E,F $ are two extensions of $K$ and  $E\otimes _K F$ is a field. Prove that $E$ or $F$ are algebraic over $K$. Is there any hint to prove this? 
Thanks.","['tensor-products', 'galois-theory', 'abstract-algebra', 'extension-field', 'field-theory']"
1979491,Group of symmetric invertible matrices,"All $2 \times 2$ symmetric invertible matrices form an infinite abelian group under matrix multiplication. Is the above statement true? I know it has identity, associative property and inverses exist. Product of invertible matrices is invertible and product of symmetric matrices is symmetric only if the matrices commute. Hence the answer should be no. They don't even form a Group. Is my argument correct?","['matrices', 'group-theory']"
1979526,"Double integral $\int\int x \sin(x+y)dA$ with $R=[0,\pi/6]\times[0,\pi/3]$","The closest of my tries was: $$\int^{\pi/6}_0 x \sin(x+y)dx=-x \cos(x+y)+ \sin(x+y) \bigg \lvert ^{\frac{\pi}{6}}_{0}$$ having used the formula $\int uv' =uv-\int u'v$. Integrating this with respect to $y$, we find $$\int^{\pi/3}_0 -\frac{\pi}{6} \cos(\frac{\pi}{6}+y)+ \sin(\frac{\pi}{6}+y)- \sin(y)dy = -\frac{\pi}{6} \sin(\frac{\pi}{6}+y)- \cos(\frac{\pi}{6}+y)+ \cos(y) \bigg \lvert^{\pi/3}_0$$ which got me something around $-0.0094011004$, when the answer should be $0.5(\sqrt{3}-1) \approx 0.3660$.","['multivariable-calculus', 'integration', 'trigonometry']"
1979577,"Differentiablity of the function $\min\{|x-2|,|x|,|x+2|\}.$ [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question How to check differentiablity of the function $\min\{|x-2|,|x|,|x+2|\}?$ I only know that inside functions are not differentiable at $2,0,-2$ resp. Please help. Thanks a lot.","['derivatives', 'real-analysis', 'absolute-value']"
1979589,Dual spaces for dummies?,"I'm at a complete loss for dual space right now, and linear functionals by association. My notes from lecture and my book are completely unhelpful, and I'm finding myself making up solutions to homework from patterns I'm gleaning from answers I find online or the examples in the book. The current problem I need to do reads: Define f (some special non-italicised notation for a linear functional, whatever that is -- I have no idea how to denote that in text and will just toss it into the \$'d limits) $\in(\Bbb R^2)^*$ by f$(x,y) = 2x + y$ and $T:\Bbb R^2 \rightarrow \Bbb R^2$ by $T(x,y) = (3x + 2y, x)$. (a) Compute $T^t(f)$. (b) Compute $[T^t]_{\beta^*}(f)$, where $\beta$ is the standard ordered basis for $\Bbb R^2$ and $\beta^* = \{f_1, f_2\}$ is the dual basis, by finding scalars $a, b, c$, and $d$ such that $T^t(f_1) = af_1 + cf_2$ and $T^t(f_2) = bf_1 + df_2$. (c) Compute $[T]_\beta$ and $([T]_\beta)^t$, and compare your results with (b). I have no clue where to even start. I thought maybe I could consider f as some sort of thing I could plug into the transformation (like $f = \{2, 1\}$ then $T(2,1) = (8,2)$) but that doesn't get me anywhere, let alone a matrix to transpose. I don't think I understand anything regarding this topic; I can't find any examples in the book that make any sense, my notes are equally cryptic (and most of my lecture time I'm frantically scribbling what the professor writes on the board with no concept of what's going on), and I'm not seeing anything online that's of help either. Usually there are a decent amount of pdf's to search... but not this time.","['change-of-basis', 'linear-algebra']"
1979599,Forward Euler Pendulum divergence,"I have implemented a simple forward euler simulation of the pendulum model: $$
\ddot{\phi} = -g \sin \phi
$$ The solution builds up energy (plot shows $\phi$ w.r.t. time), as it was expected. But after a while it switches from an oscillating behavior to a monotone divergence. Is that expected or a bug in my implementation? If it is expected, why does it happen (coincidentally around $\phi=\pi$)","['numerical-methods', 'integration', 'ordinary-differential-equations']"
1979600,Is it possible to find an infinite set of points in the 3D space where the distance between any pair is rational?,"This is the question: Is it possible to find an infinite set of points in the 3D space, not all on the same plane, such that the distance between every pair of points is rational?","['number-theory', 'rational-numbers', 'geometry']"
1979605,What's the purpose of the two different definitions used for limit?,"In my study on the concept of limit, I've come across two different definitions: Let's assume that $a \in \bar D$, i.e. $a$ belongs to the closure of the domain of function $f$. Then $\lim_{x\rightarrow a}f(x)=b$ $\forall_{\epsilon>0}\exists_{\delta>0}\forall_{x}\ \ x\in D
    \land|x-a|<\delta\implies |f(x)-b|<\epsilon$ $\forall_{\epsilon>0}\exists_{\delta>0}\forall_{x}\ \ x\in D
    \land(0<|x-a|<\delta)\implies |f(x)-b|<\epsilon$ I've seen definition 1. being used mainly in book written in continental Europe, while definition 2 in books written in USA or UK. I may be wrong though. The point is that these definitions are clearly very different. For example, if $f$ is equal to $1$ in every point in $\mathcal{R}$ except in $a$, where it's equal to $2$, then according to def.1. the limit doesn't exist at $a$, but according to def.2. it does exist.
Another difference I've notice is in proving the existence of composition of limits, in def1. it will be less demanding than in def2. Why would so many books use these different definitions? Edit: Because many of this forum may not be accquainted with def.1, you can check the book called ""Multidimensional Real Analysis"" by Duistermaat and Kolk, published by Cambridge University Press, where this is given as Definition 1.3.1 . Here's an image of the book: And clearly it's not a typo. (you can check the errata at the book's site.)","['definition', 'real-analysis', 'calculus', 'limits']"
1979699,Number of distinct UK bingo pages,"An UK bingo ticket contains 27 spaces, arranged in nine columns by three rows. Each row contains five numbers and four blank spaces. Each column contains up to three numbers, which are arranged as follows: The first column contains numbers from 1 to 9, The second column numbers from 10 to 19, The third column number from 20 to 29 so on up until the last column, which contains numbers from 80 to 90. Tickets are created as strips of 6, in such a way that every number from 1 to 90 appears exactly once. This is also called a bingo page. ( Wikipedia page ) Example of a valid bingo page:
$$
  \begin{array}{ | c |}
    \hline
  \begin{array}{ | c | c | c | c | c | c | c | c | c | }
    \hline
    3 & 12 &    & 33 &    &    &    & 70 & 83 \\ \hline
    6 & 16 & 20 &    &    &    & 61 & 75 &    \\ \hline
      &    & 29 &    & 43 & 56 & 65 & 78 &    \\ \hline
    \hline
  \end{array}
  \\
  \begin{array}{ | c | c | c | c | c | c | c | c | c | }
    \hline
      & 11 & 26 & 30 & 40 &    & 60 &    &    \\ \hline
    7 & 17 &    &    & 46 & 58 &    &    & 88 \\ \hline
      &    &    & 32 & 49 &    & 69 & 77 & 89 \\ \hline
    \hline
  \end{array}
  \\
  \begin{array}{ | c | c | c | c | c | c | c | c | c | }
    \hline
      & 10 & 23 & 34 &    & 54 &    &    & 90 \\ \hline
    1 &    & 25 &    &    & 57 & 66 & 73 &    \\ \hline
      & 19 &    & 38 & 48 & 59 & 68 &    &    \\ \hline
    \hline
  \end{array}
  \\
  \begin{array}{ | c | c | c | c | c | c | c | c | c | }
    \hline
      & 13 & 28 &    &    & 50 & 62 & 74 &    \\ \hline
    4 &    &    & 37 & 45 & 55 &    &    & 80 \\ \hline
    5 &    &    & 39 &    &    & 64 & 79 & 84 \\ \hline
    \hline
  \end{array}
  \\
  \begin{array}{ | c | c | c | c | c | c | c | c | c | }
    \hline
      & 15 & 21 &    & 44 & 52 &    &    & 81 \\ \hline
    2 &    & 22 & 35 &    &    & 67 &    & 82 \\ \hline
    9 &    &    & 36 &    & 53 &    & 71 & 86 \\ \hline
    \hline
  \end{array}
  \\
  \begin{array}{ | c | c | c | c | c | c | c | c | c | }
    \hline
      & 14 & 24 & 31 & 41 &    &    & 72 &    \\ \hline
    8 &    & 27 &    & 42 &    &    & 76 & 85 \\ \hline
      & 18 &    &    & 47 & 51 & 63 &    & 87 \\ \hline
    \hline
  \end{array}
  \end{array}
$$ How can one calculate how many different bingo pages can be created?",['combinatorics']
1979750,Is there a bijection between $\mathbb{N} \times \mathbb{R}$ and $\mathbb{R}$,"So I'm doing some practice on set theory, and I am having some trouble proving a lemma. Basically I want to ask if there is there a bijection between $\mathbb{N} \times \mathbb{R}$ and $\mathbb{R}$ If yes, could someone provide a simple construction of such a bijection? Any help or insights is deeply appreciated.",['elementary-set-theory']
1979770,"Combinatorial proof of $\sum_{1\le i\le n,\ 1\le j\le n}\min(i,j)=\sum_{i=1}^ni^2=\frac{n(n+1)(2n+1)}6$","$$\sum_{1\le i\le n,\ 1\le j\le n}\min(i,j)=\sum_{i=1}^ni^2=\frac{n(n+1)(2n+1)}6$$
Is there a link, if any, between these two identical sums?","['combinatorics', 'summation', 'combinatorial-proofs']"
1979783,Prove that the equation $x^{15} + 7x^{3} - 5 = 0$ has exactly one real solution,Prove that the equation $x^{15} + 7x^{3} - 5 = 0$ has exactly one real solution. A hint that has been given by the teacher is to analyze the function $f(x) = x^{15} + 7x^{3} - 5$.,['functions']
1979801,Forming a committee of 5 with at least 3 teachers,"I am working on the following problem In how many ways can a committee of 5 be formed from a group of 11 people consisting of 4 teachers and 7 students if the committee must include at least 3 teachers So I know that the answer is ${4 \choose 3}{7 \choose 2} + {4 \choose 4}{7 \choose 1} = 91$ and I can follow the reasoning behind this answer, but I still do not understand why my original approach to solving the problem is incorrect. My thought was to first choose 3 teachers and then add the extra teacher left to the group of students and pick two. In other words $${4 \choose 3}{8 \choose 2}$$ I thought that both cases of groups with 3 and 4 teachers would be covered by including the teacher in the group of students when choosing two more people to complete the committee, but I seem to overestimate the correct answer (ie the above is equal to 112), but I don't understand why. Question: What is wrong with my approach to this problem?",['combinatorics']
1979804,Combinatorics with repetition,"I'm having a hard time with combinatorics with repetition. Say we have the set $A = \{1,2,3,4,5,6\}$ and we are going to choose $3$ items from the set $A$ with $6$ items in. So my intuitive understanding of this is that you can divide the set $A$ into $3$ different bins and to do so we need to have $2$ ""walls"" to create $3$ bins: like $xx\mid xxx\mid x$, so we could add $2$ extra items in the set so we have $6 + 2$ items to choose from and we always want to use $2$ of those as ""walls"" So now the question is about in how many places we could place those two walls and we get $\binom{6 + 2}{2} = \binom82$. But the theorem says that : The number of unordered selections, with repetition, of $r$ objects from a set of $n$ objects is $\binom{n + r - 1}{r}$ In this case that would give us $n=6, r=3 \implies \binom{6 +3-1}{3} = \binom83$ which is not the same as $\binom82$. My intuitive reasoning is obviously wrong but I can’t understand exactly where my mistake is.","['combinatorics', 'discrete-mathematics']"
1979830,Factorizing the Sum of Two Fibonacci numbers,"The Fibonacci and Lucas numbers are defined for all integers $n$ by the recurrence relations
$$F_n=F_{n-1}+F_{n-2}\text{ where }F_1=1\text{ and }F_2=1,$$
$$L_n=L_{n-1}+L_{n-2}\text{ where }L_1=1\text{ and }L_2=3.$$
I would like to know for what values of $k\in\mathbb{N}$ can one write
$$F_{n+(2k+1)}\pm F_n=cP(k,n)$$
where $c\in\mathbb{N}$ and $P(k,n)$ is some product of Fibonacci or Lucas numbers. Note that it is easy to show that that:
\begin{align*}
F_{n+2k}+F_{n}=F_{n+k}L_k\text{ where $k$ is even,}\\
F_{n+2k}+F_{n}=L_{n+k}F_k\text{ where $k$ is odd.}\\
F_{n+2k}-F_{n}=F_{n+k}L_k\text{ where $k$ is even,}\\
F_{n+2k}-F_{n}=L_{n+k}F_k\text{ where $k$ is odd.}\\
\end{align*}
It is also easy to see that 
\begin{align*}
F_{n+1}+F_{n}=F_{n+2}\\
F_{n+1}-F_{n}=F_{n-1}\\
F_{n+3}+F_{n}=2F_{n+2}\\
F_{n+3}-F_{n}=2F_{n+1}\\
\end{align*}
Are these the only such expressions? Ideas tried: I've tried the Binet formula to see what insights this might provide, but I can't see anything. I've also tested small values of $n$ numerically but couldn't find any further examples than those four given.","['fibonacci-numbers', 'sequences-and-series']"
1979876,Ray-Casting algorithm in Ray-triangle intersection,"Currently I started studying about ray-casting when I came across this following problem based on ray-triangle intersection. The problem was: You are provided with a triangle with vertices , (x1,y1,z1) , (x2,y2,z2) and (x3,y3,z3) . A ray with origin (a1,b1,c1) and direction (a2,b2,c2) is also given. Your task is to find: 1)Whether or not the ray intersects the triangle 2) If the ray intersects the triangle, what's the point of intersection and also find the distance of theat point from the origin of the ray Examples: 1) For (x1,y1,z1) = (-2,2,6) , (x2,y2,z2) = (2,2,6) , (x3,y3,z3) = (0,-4,6) , (a1,b1,c1) = (1,0,0) , (a2,b2,c2) = (-0.2,0,1) the answer is: coordinates of intersection: (-0.2,0,6) and the distance of the origin of the ray from the point of intersection is 6.12 2) For (x1,y1,z1) = (-2,2,1) , (x2,y2,z2) = (2,2,1) , (x3,y3,z3) = (0,-4,1) , (a1,b1,c1) = (0,0,0) , (a2,b2,c2) = (0,0,1) the answer is: coordinates of intersection: (0,0,1) and the distance of the origin of the ray from the point of intersection is 1 3) For (x1,y1,z1) = (-10,-2.3,0) , (x2,y2,z2) = (4.4,20.3,9.5) , (x3,y3,z3) = (9.8,-10,0) , (a1,b1,c1) = (0,0,0) , (a2,b2,c2) = (0.68,-1.14,1.82) the answer is: coordinates of intersection: (0.67, -1.12, 1.79) and the distace of the origin of the ray from the point of intersection is 2.22 All I need is to understand the general equations to solve this question. I need to know how the equations are formed and how the problem is solved. This seems a pretty hard question for me.","['analytic-geometry', 'computer-algebra-systems', 'geometry']"
1979877,Shifting indexes in a double sum,"I wanted to ask if I shifted this sum correctly. I basically substitute $k$ with $k+N$. I often make mistakes when shifting the indexes. $$
\sum_{k=N}^{2N}(\sum_{j=0}^{k}a_{k-j}b_{j})x^k = \sum_{k=0}^{N}\sum_{j=0}^{k+N}(a_{k-j+N}b_{j})x^{k+N} 
$$ Any help would be great! Thanks!","['algebra-precalculus', 'real-analysis', 'calculus']"
1979886,Showing that the family of Borel sets is countably generated,"I am taking a course in Measure Theory this year, and as an exercise from the lectures we have been given the following: Show that B(R) is countably generated; that is, show the Borel sets are
generated by a countable class A. We have defined the Borel sets as: Let X be a metric space, the family of Borel sets in X is the sigma algebra B(X) generated by the family of open sets. A set A in B(X) is called a Borel set. So far, my thoughts are showing that the Borel sets can be generated by the intervals [q, infinity) where q is rational. I was wondering if anyone can help me formulate a proof of this, hopefully using my idea, as we don't get support classes or solutions and I would really like to understand this. Thank you.","['proof-writing', 'measure-theory']"
1979898,To find domian of $\log_{[x-1]}\sin(x)$,"I have to find the domain of $\log_{[x-1]}\sin(x)$. Now I have $\sin x>0$ $[x-1]>0$ $[x-1] \neq 1 $ From (2) I get $x \in [2,\infty]$. From (3) I get $x \in (-\infty,2) \cup (3,\infty)$ I do not know how to take intersection of these with $\sin x>0$. Help will be appreciated. P.S- [] DENOTES GREATEST INTEGER FUNCTION . i am stuck on this
Thanks","['algebra-precalculus', 'functions']"
1979968,"Maximizing the number of ""discrepancies"" in a vertex-labeled graph","I am working on a modeling project whose mathematical formalism uses 2D integer periodic lattices. The problem I'm actually interested in can be generalized to the following graph theory problem, which might perhaps be easier by virtue of there being less structure available. Given a fixed undirected graph $G=(V,E)$ and an arbitrary function $F : V \to \{ 1,2,\dots,|V| \}$ (not necessarily injective or surjective), we define $$B(F,v)=\# \{ u \in V : (u,v) \in E \text{ and } F(u) \geq F(v) \}$$
where $\#A$ is the number of elements in a finite set $A$. The occurrences being counted here are called bonds experienced at the site $v$. We then define: $$D(F)=\sum_{v \in V} \# \{ u \in V : (u,v) \in E \text{ and } F(u)>F(v) \text{ and } B(F,u)>B(F,v) \}.$$ Each of the occurrences being counted in this sum is called a discrepancy between $u$ and $v$ in favor of $u$. I want to understand the optimization problem of choosing $F$ to make $D(F)$ as large as possible. Some initial intuition follows: Discrepancies are possible. An example of a labeled graph with a discrepancy is  $V=\{ 1,2,3,4 \},E=\{ (1,2),(2,3),(2,4) \}$ (the ""T"" graph) labeled with $F(1)=2,F(2)=2,F(3)=2,F(4)=1$. Then there will be a discrepancy between $2$ and $4$ in favor of $2$. However, discrepancies are ""unusual"" because if $F(u)>F(v)$ then a bond is experienced to $u$ at $v$ but not vice versa. So vertices with higher values of $F$ will tend to have fewer bonds, especially if the degree of all vertices is constant (as in the cases of interest to me). My limited thoughts on the problem: The obvious bound on $D(F)$ is $\sum_{v \in V} \operatorname{deg}(v)-B(F,v)$, which is what you get if you just forget about the $B(F,u)>B(F,v)$ part. But my experiments show that at least in the class of graphs I am interested in (2D rectangular periodic lattices) $D(F)$ is much smaller. For instance this bound might be about $200$ while the actual value of $D$ is about $5$. Some alternative ways to formulate the problem: We can forget about $F$ and think purely in terms of an arbitrary total preorder on $V$. Unfortunately we cannot specialize to actual orders, because $F$ was allowed to be non-injective. We can pose everything in graph theory language. To do this, we take $G$ and then half-remove edges from it to make a directed graph. In this framework, a discrepancy occurs when there is a directed edge from $v$ to $u$ and the out-degree of $u$ (counting both directed edges and undirected edges) is higher than that of $v$. The problem with this formulation is that we cannot simply half-remove edges willy-nilly: we have to remove them in such a way that there are no cycles containing directed edges. Yet we still have more leeway than in the case of constructing an acyclic orientation of an undirected graph, because purely undirected cycles are OK (and in particular, the original graph is OK, though it has zero discrepancies).","['graph-theory', 'optimization', 'discrete-optimization', 'combinatorics', 'linear-algebra']"
1979977,How to put $\frac{1}{\cos\theta - j\sin \theta}$ in the form $a+jb$? ($j^2=-1$),I am new to complex numbers and am having trouble putting them in the form $a+jb$ (or $a+bi$) How would I go about putting this expression in the form $a+jb$? $$\frac{1}{\cos\theta - j\sin \theta}$$,['trigonometry']
1980000,Equivalence of sheaves and various categories of bundles,"The following equivalences are well-established:
$$\begin{align}
\text{Locally-free sheaves on }X &\leftrightarrow \text{Vector bundles over }X\\
\cap\qquad & \qquad\qquad\textbf{?}\\
\text{Sheaves on }X &\leftrightarrow \text{Étalé spaces over }X\\
\cup\qquad & \qquad\qquad\cup\\
\text{Locally-constant sheaves on }X &\leftrightarrow \text{Covering spaces over }X
\end{align}$$ Further, the ways that we construct a sheaf from a vector bundle and from an étalé space are 'identical': we just take the sheaf of sections of the projection map.
But it is also known that a non-trivial vector bundle is never étalé. This makes me confused/intrigued about two things: Why does the equivalence of categories (sheaves$\leftrightarrow$étalé) not descend to the equivalence (locally-free sheaves$\leftrightarrow$something) in the same way that it does for locally-constant sheaves and covering spaces? (i.e. why are the bottom two rows of this diagram 'nice' while the top two are 'not nice'?) Given some vector bundle, we can associate to it (uniquely?) a sheaf, to which we can then associate an étalé space (and vice versa). Does this give us an equivalence of categories (vector bundles$\leftrightarrow$some sort of étalé spaces)?","['vector-bundles', 'sheaf-theory', 'algebraic-geometry']"
1980028,"Proving uniqueness in $\mathbb{C}$ of function $f$ verifying $\hspace{0.2cm}f(u+v)=f(u)f(v),\forall u,v\in\mathbb{C},f(1)=e$","I am trying to prove that the complex exponential function 
$$exp\colon\mathbb{C} \to \mathbb{C},z \mapsto exp(z):=\sum_{n=0}^{\infty}\frac{z^n}{n!}$$
is the unique continuous function $\hspace{0.2cm} f\colon \mathbb{C} \to \mathbb{C}$ such that $f(u+v)=f(u)f(v), \forall u,v\in\mathbb{C}$ and $f(1)=\sum_{n=0}^{\infty}\frac{1}{n!}$. I have already proved that the exponential satisfies this condition. My difficulties are only in proving the uniqueness. I have tried several ways to prove it, but not a clue how to do it. Notice the definition of the exponential function I am using. Any ideas?","['complex-analysis', 'analysis']"
1980056,Covariant derivative versus exterior derivative,"$\def\alt{\textrm{Alt}} \def\d{\mathrm{d}}  \def\sgn{\mathrm{sgn}\,}$Let $\nabla$ be a symmetric linear connection in $M$, and $\omega$ be a $k$-form in $M$. I'm trying to find a relation between $\d\omega$ and $\nabla \omega$. I follow Spivak's notation in Calculus on Manifolds and write $$\alt(\nabla \omega)(X_1,\cdots,X_{k+1}) = \frac{1}{(k+1)!}\sum_{\sigma \in S_{k+1}} (\sgn \sigma)\nabla\omega(X_{\sigma(1)},\cdots,X_{\sigma(k+1)}).$$I also assume the formula $$\d\omega(X_1,\cdots,X_{k+1})=\sum_{i=1}^k(-1)^{i+1}X_i(\omega(X_1,\cdots,\widehat{X_i},\cdots,X_{k+1})) + \sum_{i<j}(-1)^{i+j}\omega([X_i,X_j],X_1,\cdots,\widehat{X_i},\cdots,\widehat{X_j},\cdots,X_{k+1}).$$I'd expect something like $\alt(\nabla\omega) = \d \omega$, apart from a multiplicative constant, maybe. I'm stuck. For $k=1$ I got $$\alt(\nabla \omega) = -\frac{1}{2}\d \omega.$$The $1/2$ I can accept, but the minus sign puts me off. I tried brute forcing my way through $k=2$ but I guess I just suck at doing computations like this. The best I can come up with is $$\begin{align} (k+1)!\alt(&\nabla\omega)(X_1,\cdots,X_{k+1}) = \sum_{\sigma \in S_{k+1}} (\sgn\sigma) \nabla\omega(X_{\sigma(1)},\cdots,X_{\sigma(k+1)}) \\ &= \sum_{\sigma \in S_{k+1}} (\sgn\sigma) \nabla_{X_{\sigma(k+1)}}\omega(X_{\sigma(1)},\cdots,X_{\sigma(k)}) \\ &= \sum_{\sigma \in S_{k+1}}(\sgn\sigma)\left(X_{\sigma(k+1)}(\omega(X_{\sigma(1)},\cdots,X_{\sigma(k)})) - \sum_{i=1}^n\omega(X_{\sigma(1)},\cdots, \nabla_{X_{\sigma(k+1)}}X_{\sigma(i)},\cdots,X_{\sigma(k)})\right)\\ &= \sum_{\sigma \in S_{k+1}}(\sgn\sigma)X_{\sigma(k+1)}(\omega(X_{\sigma(1)},\cdots,X_{\sigma(k)})) - \sum_{\sigma \in S_{k+1}}\sum_{i=1}^n(\sgn \sigma)\omega(X_{\sigma(1)},\cdots, \nabla_{X_{\sigma(k+1)}}X_{\sigma(i)},\cdots,X_{\sigma(k)}) \\ &\stackrel{\color{red}{(\ast)}}{=} \sum_{\sigma \in S_{k+1}}X_{\sigma(k+1)}(\omega(X_1,\cdots,X_k)) - \sum_{\sigma \in S_{k+1}}\sum_{i=1}^n(\sgn \sigma)\omega(X_{\sigma(1)},\cdots, \nabla_{X_{\sigma(k+1)}}X_{\sigma(i)},\cdots,X_{\sigma(k)})\end{align}$$ and I'm stuck. I'm not sure of the $\color{red}{(\ast)}$ step either. I know that we must use that $\nabla$ is symmetric to get rid of all these $\nabla$ but I don't know how to do this. What is the smart way to do this? This answer is the most similar thing to what I'm trying to do that I found, but I don't find it easy to see such relation, as it is said there. Also, I'd like to avoid coordinate computations if possible. I'll ignore the $\color{red}{(\ast)}$ step since I think it is wrong. I don't know how to write it neatly, but doing it for $k=1$ and $k=2$ suggests $$\sum_{\sigma \in S_{k+1}}(\sgn \sigma)X_{\sigma(k+1)}(\omega(X_{\sigma(1)},\cdots,X_{\sigma(k)})) = (-1)^kk!\sum_{i=1}^k(-1)^iX_i(\omega(X_1,\cdots,\widehat{X_i},\cdots,X_{k+1})).$$Also, it seems that $$\sum_{\sigma \in S_{k+1}}\sum_{i=1}^k(\sgn \sigma)\omega(X_{\sigma(1)},\cdots, \nabla_{X_{\sigma(k+1)}}X_{\sigma(i)},\cdots,X_{\sigma(k)})=(-1)^{k+1}k!\sum_{i<j}(-1)^{i+j}\omega([X_i,X_j],X_1,\cdots,\widehat{X_i},\cdots,\widehat{X_j},\cdots,X_{k+1})$$ So: $$(k+1)! {\rm Alt}(\nabla \omega) = (-1)^kk!{\rm d}\omega \implies (-1)^k(k+1)\alt(\nabla \omega) = {\rm d}\omega.$$","['differential-forms', 'connections', 'differential-geometry']"
1980090,Autocovariance function in $AR(1)$ process,"Let a autoregressive process $AR(1)$ given by
$$x_t=\sum_{j=0}^\infty \phi^jw_{t-j}$$
where $|\phi|<1$ and $w_{t-j}$ is i.i.d a white noise with mean 0 and variance $\sigma^2$. The autocovariance function is
$$\gamma(h)=cov(x_{t+h},x_t)=E\Big[\Big(\sum_{j=0}^\infty \phi^jw_{t+h-j}\Big)\Big(\sum_{k=0}^\infty \phi^k w_{t-k}\Big)\Big]$$
$$=E[(w_{t+h}+\dots+\phi^hw_t+\phi^{h+1}w_{t-1}+\dots)(w_t+\phi w_{t-1}+\phi^2 w_{t-2}+\dots)\qquad\textbf{(1)}$$
$$=\sigma^2\sum_{j=0}^\infty \phi^{h+j}\phi^j=\sigma^2\phi^h\sum_{j=0}^\infty \phi^{2j}=\frac{\sigma_2\phi^h}{1-\phi^2},\qquad h\geq 0$$ I know that last equality is the power series representation of that sum, but I'm having hard time to understand how they get
$$\sigma^2\sum_{j=0}^\infty \phi^{h+j}\phi^j$$ from $\textbf{(1)}$. I know that they are using $$E[w_{j}w_{k}]=E[w_t^2]=Var(w_t)=\sigma^2$$
in the expectations, but I don't understood how to get the exponents in $\phi$ Anyone can help me?","['stochastic-processes', 'self-learning', 'statistics', 'time-series']"
1980095,A question about the definition of generalized eigenspace,"Given an $n\times n$ matrix $A$ , its generalized eigenspace pertaining to an eigenvalue $\lambda_i$ is defined as $V_{\lambda_i}=\{x:(A-\lambda_i I)^n x=0\}$ The question is to prove $V_{\lambda_i}=\{x:(A-\lambda_i I)^n x=0\} = \{x:(A-\lambda_i I)^{m(\lambda_i)} x=0\}$ where $m(\lambda_i)$ is the algebraic multiplicity of $\lambda_i$ . It is obvious that $\{x:(A-\lambda_i I)^{m(\lambda_i)} x=0\} \subseteq \{x:(A-\lambda_i I)^n x=0\}$ , since $(A-\lambda_i I)^{m(\lambda_i)} x=0\Rightarrow (A-\lambda_i I)^n x=0$ . The problem is the other direction. The solution hints that using Hamilton-Cayley theorem, i.e. $\prod\limits_{{\lambda } \in \sigma (A)} {{{(A - {\lambda}I)}^{m({\lambda })}}}  = O$ where ${\sigma (A)}$ is the spectrum of $A$","['matrices', 'linear-algebra']"
1980171,I think there is an error in this solution,"I have been trying to reconcile this solution with my work for hours now. Could someone please help me confirm whether the solution provided below is incorrect? I get a solution of X > -3/2, the provided solution is X < 3/2",['algebra-precalculus']
1980175,How would you find the trigonometric roots of a cubic?,"Question: How would you find the roots of the cubic$$x^3+x^2-10x-8=0\tag{1}$$ I'm not too sure where to begin. I'm thinking of somehow, implementing $\cos 3\theta=4\cos^3\theta-3\cos\theta$. I've tried substituting $x$ with $t+t^{-1}$, but didn't get anywhere, and using Vieta's trigonometric solution formula, I got 
$$x_1=\frac {2\sqrt{31}}3\cdot\cos\left(\frac {\arccos \frac {2}{\sqrt{31}}}3\right)-\frac 13\\x_2=\frac {2\sqrt{31}}3\cdot\cos\left(\frac {2\pi+\arccos\frac {2}{\sqrt{31}}}3\right)-\frac 13\\x_3=\frac {2\sqrt{31}}{3}\cdot\cos\left(\frac {4\pi+\arccos\frac 2{\sqrt{31}}}3\right)-\frac 13$$
But that's not the form I want. I'm looking for a form of $2\left(\cos\frac {\text{something}}{31}+\cos\frac {\text{something}}{31}+\cos\frac {\text{something}}{31}\right)$. I've spent so much time, that I'm practically burnt out. -.-","['polynomials', 'roots', 'trigonometry']"
1980205,Difference between limit superior & supremum of a sequence,"Can anyone please tell what's the difference between the terms limit superior, limit inferior, supremum, infimum. They are confusing me.","['real-analysis', 'sequences-and-series']"
1980231,What is Relationship Between Distributional and Fourier Series Frameworks for Prime Counting Functions?,"I've defined three general methods for derivation of formulas for prime counting functions where each prime counting function is represented by an infinite series of Fourier series. In the distributional framework for prime counting functions the first-order derivatives are represented by Dirac delta distributions. In the case of $\psi'(x)$ , there is a Dirac delta distribution at each  prime-power value of $x$ with weight $Log(p)$ where $x$ is of the form $x=p^n$ . Note the real portion of the Fourier transform of a Dirac delta function is a Cosine term. (1) $\quad \operatorname{FourierTransform}(\delta(x-a),x,y)=e^{-2\,i\,\pi\,a\,y}=\cos(2\,\pi\,a\,y)-i\,\sin(2\,\pi\,a\,y)\,,\quad a\in\mathbb{R}$ In the Fourier series framework for prime counting functions, the first-order derivatives are represented by infinite series of Fourier series which converge to the Dirac delta distributions in the distributional framework. These Fourier series consist of Cosine terms, and note the Fourier transform of a Cosine function is a pair of Dirac delta distributions. (2) $\quad \operatorname{FourierTransform}(\cos(2\,\pi\,b\,x),x,y)=\dfrac{\delta(b-y)}{2}+\dfrac{\delta(b+y)}{2}$ The Fourier transforms in (1) and (2) above both assume the Fourier parameters $\{0,\,-2\,\pi\}$ . Question 1 : What is the relationship between the Cosine terms in the distributional framework and the Dirac delta distributions in the Fourier series framework? For example, do the Cosine terms in the distributional framework converge to the Dirac delta distributions in the Fourier series framework, analogous to the way the Cosine terms in the Fourier series framework converge to the Dirac delta distributions in the distributional framework? Question 2 : If the analogous relationship in question 2 is valid, is this convergence in any way sensitive to the Fourier parameters used for the two Fourier transforms? For example, will the convergence only apply when using the same set or a specific set of Fourier parameters for both Fourier transforms? Or will the convergence perhaps be faster if the same Fourier parameters are used for both Fourier transforms versus using different Fourier parameters for the two Fourier transforms? 1/1/2018 Update : I believe the discrepancies between the Fourier transforms of the distributional and Fourier series representations of prime-counting functions result from the idealization of the Fourier transform of $sin$ and $cos$ functions as Dirac delta ( $\delta$ ) functions. Please see the answer I posted below which I believe provides a fair amount of insight into the theory and value of Fourier series representations of non-periodic functions. Happy New Year! Direct link to answer I posted below 5/21/2022 Update : Formulas (7) and (8) of my question related to the nested Fourier series representation of $h(s)=\frac{i s}{s^2-1}$ define a general method for derivation of a nested Fourier series representation for $f(x)=\sum\limits_{n=1}^x a(n)$ which I believe converges for $x>0$ when the related Dirichlet series $F(s)=\sum\limits_{n=1}^\infty\frac{a(n)}{n^s}$ converges for $\Re(s)\ge 2$ . In some cases, such as the case when $a(n)=\delta_{n,1}$ (Kronecker delta function) which is addressed in my answer below, an evaluation limit can be chosen such that the nested Fourier series for $f(x)$ evaluates to zero at $x=0$ in which case it becomes an odd function of $x$ . This answer I posted to a question about an entire function interpolating $\mu(n)$ defines an alternate analytic representation for $f(x)=\sum\limits_{n=1}^x a(n)$ which I believe more generally converges to zero at $x=0$ (and hence an odd function of $x$ ) assuming the related Dirichlet series $F(s)=\sum\limits_{n=1}^\infty\frac{a(n)}{n^s}$ converges for $\Re(s)\ge 2$ .","['chebyshev-function', 'fourier-series', 'number-theory', 'prime-numbers', 'fourier-transform']"
1980257,Is this intuitive way of thinking about the implicit function theorem correct?,"""Intuitive version"": Let $f: \mathbb{R}^n \to \mathbb{R}^m$, $n\ge m$, be a smooth map. If , given $q \in \mathbb{R}^m$, for a fixed point $p \in f^{-1}(q)$ one can define a maximum-dimension vector space using the partial derivatives of $f$ then there exists a diffeomorphism between a neighborhood in $f^{-1}(q)$ of $p$  and $\mathbb{R}^{n-m}$ (i.e. one can ""smoothly deform"" a neighborhood of $f^{-1}(q)$ into $\mathbb{R}^{n-m}$). When I say ""non-degenerate"" I mean of maximum possible dimension, which is the maximum possible number of linearly independent vectors which can be defined by the partial derivatives. One has to specify the point $q$ before specifying the point $p$, because otherwise one would try to construct a diffeomorphism between a neighborhood in $\mathbb{R}^n$ of $p$ and $\mathbb{R}^{n-m}$, which obviously does not make any sense -- we have to specify that the neighborhood is in $f^{-1}(q)$. Questions: 1. Is this way of thinking about the implicit function theorem correct? 2. Why is $f^{-1}(q)$ of dimension $n-m$ when the dimension of the space spanned by the partial derivatives of $f$ at every point $p \in f^{-1}(q)$ is $m$? If $f^{-1}(q)$ has $m$-dimensional tangent spaces then why should it be an $n-m$ dimensional manifold, and not an $m-$dimensional one? 3. If for one of the points in $f^{-1}(q)$ the derivative of $f$ doesn't have full rank (i.e. we can only define a degenerate tangent space), then does that make that point a singular point of $f^{-1}(q)$? 4. If $p_0$ is the point from the third question, then is $f^{-1}(q) \setminus \{p_0 \}$ a smooth manifold? The example I have in mind is the singular point of a curve, e.g. $(0,0)$ of $y^2 - x^3=0$. 5. What prevents the following scenario: for every point $p \in f^{-1}(q)$, $Df(p)$ has rank $m-1$, so $f^{-1}(q)$ is a manifold of dimension $n-m-1$? Does the constant rank theorem allow this possibility, explaining why the constant rank theorem is a generalization?","['intuition', 'real-analysis', 'implicit-function-theorem', 'differential-geometry', 'implicit-differentiation']"
1980265,Origins of the conjecture on the existence of infinitely many palindromic primes,"A palindromic prime with respect to a base $b \geq 2$ is a prime number such that, when you reverse its sequence of digits in base $b$ , you get the same prime. For example in base $10$ , the prime $16661$ is palindromic. See the wiki here for more: https://en.wikipedia.org/wiki/Palindromic_prime It is a well-known conjecture that there are infinitely many of these in base $10$ (but likely in every base). I tried online to find info on the origins of this conjecture but with no success. I couldn't even find in what century it was first talked about. Perhaps it's a folklore problem? Nevertheless there should at least be some first papers which mention it. Would you know anything about the origins of the conjecture or at least the century in which it first appears in a paper or correspondence between mathematicians? Thanks a lot. Update: Several comments and answers point out heuristics for the conjecture and a recent paper. Would you also know of an old citable paper or book where the conjecture appears or at least something closely related is said about these primes?","['math-history', 'reference-request', 'number-theory', 'palindrome', 'prime-numbers']"
