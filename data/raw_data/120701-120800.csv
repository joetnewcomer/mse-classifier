question_id,title,body,tags
1806898,Visualization of the dual space of a vector space,"I am wondering what the motivation was for defining a dual space of a vector space, and how to visualize the dual space. I'm asking since it doesn't seem to me to be intuitive to deal with such a space. In particular, I'm looking for questions where it would be natural to consider the space of linear functionals in order to answer these questions.",['linear-algebra']
1806899,Is this relative homology equals to wedge sum of two tori?,"If $X$ is a connected sum of tow tori, and $A$ is its center circle as shown in picture below. I would like to compute $H_n(X,A)$. There is a statement in the book that $H_n(X,A)$ represents the homology of the quotient space $X/A$ obtained by collapsing $A$ to a point. If I do that, then I got wedge sum of two tori $T\vee T$ So $H_n(X,A)=H_n(T\vee T)$
$$H_0(X,A)=\mathbb{Z}$$
$$H_1(X,A)=\mathbb{Z}^4$$
$$H_2(X,A)=\mathbb{Z}^2$$
$$H_n(X,A)=0, n\geq 3$$ But why the solution for $n=2$ gives $H_2(X,A)=\mathbb{Z}$? Is the solution wrong or I am mistaken something here?","['algebraic-topology', 'general-topology', 'homology-cohomology']"
1806926,Which elements of $\mathbb{R}$ make sense as representatives for cosets of $\mathbb{Q}$ in the group $\mathbb{R/Q}$,"I am trying to better understand the group $\mathbb{R/Q}$. It's unclear to me when two irrational numbers will give the same coset of $\mathbb{Q}$, but I know that this must happen since, for example $\pi+1 = (\pi-1)+2$, meaning the cosets $\pi\mathbb{Q}$ and $(\pi−1)\mathbb{Q}$ share an element and thus are equivalent. Can we describe a set of irrational numbers that give each coset of $\mathbb{Q}$ exactly once in a way that, given an irrational number, we would be able to say whether or not it's in the set? Edit: I am also interested in understanding this group in other ways. What is it's order? Are there any groups it's isomorphic to?","['group-theory', 'irrational-numbers']"
1806940,Probability that two random matrices span the full matrix algebra,"Given two matrices $A$ and $B$ drawn at random in $\mathbb{R}^{n\times n}$, what is the probability that the matrix algebra generated by $A$ and $B$ is the full matrix algebra $\mathbb{R}^{n\times n}$? That is, if $\mathcal{A}$ is the set of matrices defined by $A,B\in \mathcal{A}$ and $XY\in \mathcal{A}$ for all $X,Y\in\mathcal{A}$, what is the probability that $\mathcal{A}$ contains $n^2$ linearly independent matrices? I have the intuition that this should occur with probability one, but I can't manage to prove it. If that helps, using Burnside's theorem I think that this is equivalent to the fact that there are no common invariant subspaces for all matrices in $\mathcal{A}$ except for $\{0\}$ and $\mathbb{R}^n$.","['matrices', 'abstract-algebra']"
1806945,Range of function $f(x) = \sqrt{x+27}+\sqrt{13-x}+\sqrt{x}$,"Range of function $f(x) = \sqrt{x+27}+\sqrt{13-x}+\sqrt{x}$ $\bf{My\; Try::}$ For $\min$ of $f(x)$ $$\left(\sqrt{13-x}+\sqrt{x}\right)^2=13-x+x+2\sqrt{x}\sqrt{13-x}= 13+2\sqrt{x}\sqrt{13-x}\geq 13$$ Now $$\sqrt{x+27} + \sqrt{13-x}+\sqrt{x} \geq \sqrt{27} + \sqrt{13}$$ and equality hold at $x=0$ Now How can i calculate $\max$ of $f(x)\;,$ Help required, Thanks",['functions']
1806964,Power Diophantine equation involving primes: $(p+q)^q-p^q-q^q+1=n^{p-q}$,"Suppose $p$ and $q$ are prime numbers, and $n>1$ is a positive integer. Find all solutions to the following Diophantine equation:$$(p+q)^q-p^q-q^q+1=n^{p-q}$$ What I have tried: Obviously $p>q$. If $q=2$, we get one solution: $(p, q, n)=(3, 2, 13)$. From now on $p>q>2$. Letting $p-q=2^k r$, where $r$ is an odd number and working on the evaluation of $2$ in the equation gives $k=1$. I think that $r=1$ is the only possibility. But I don't know how to prove it! I tried to show that $r$ cannot have a prime divisor, but I failed! Looking mod $p$, $q$ and $p+q$ gives $n^{p-q} \equiv 1 \pmod {pq^2(p+q)}$. Edit: Let's put a new restriction on the equation: Order of $n$ modulo $p$ is $p-q$. If you work on equation with the new constraint, please inform me about your results! Notice that $n^{p-q} \equiv 1 \pmod p$, so $\gcd(n, p)=1$ and it follows from the Fermat's little theorem that $n^{p-1} \equiv 1 \pmod p$. Order of $n$ modulo $p$ is $p-q$, hence $p-q|p-1$ and consequently $p-q|q-1$.","['number-theory', 'diophantine-equations', 'elementary-number-theory']"
1806975,Compute $\det{T}$ where $T(X)=AX+XA$,"Consider the linear transformation $T:V\to V$ given by $T(X) = AX + XA$, where 
  $$A = \begin{pmatrix}1&1&0\\0&2&0\\0&0&-1 \end{pmatrix}.$$
  Compute the determinant $\det T$. I know there was a similar problem with a different $A$, but that was a diagonal matrix, which made the situation easier. I computed $XA + AX$ using an arbitrary $X$, but I'm not sure where to go from there.","['matrices', 'linear-algebra', 'linear-transformations', 'determinant']"
1807001,"Error term between $f(x)$, its average value and value at midpoint","Let $f$ be a smooth function on interval $[a,b]$. Define the average $\bar{f}=\dfrac{1}{b-a}\int_a^bf(y)\,dy$ and $\bar{x}=\dfrac{a+b}{2}$, then for any $x\in [a,b]$, we can write $$f(x)-\bar{f}=c(x-\bar{x})+E,$$
where $c$ is something related to $f$ and $E$ is the error term. I want to know whether it's possible to get $E=O(b-a)^2$. By $O(b-a)^2$, I mean $\lim_{b-a\to 0}\frac{|E|}{(b-a)^2}\le C$ for some constant $C>0$. What I tried: By Taylor expansion, we have
$$f(x)=f(\bar{x})+f'(\bar{x})(x-\bar{x})+f''(\xi)(x-\bar{x})^2.$$
Then $f(x)-\bar{f}=f'(\bar{x})(x-\bar{x})+f''(\xi)(x-\bar{x})^2+f(\bar{x})-\bar{f}$. Hence it remains to study $f(\bar{x})-\bar{f}$. But by mean value theorem, we know 
$$f(\bar{x})-\bar{f}=f(\bar{x})-f(c)=f'(\eta)(c-\bar{x}),$$
which only gives me the first order approximation. Is there any way to get a better result?","['derivatives', 'continuity', 'analysis']"
1807003,Connection of $\mathcal{O}(n)$ on a toric manifold,"The holomorphic line bundle $\mathcal{O}_X(1)$ over a toric manifold $X$, admits a hermitian connection, $A^{(1)}$, whose $U(1)$ gauge transformation in a local patch of the base space is 
$$
A^{(1)}_Idx^I\rightarrow A^{(1)}_Idx^I-\textrm{ }d\lambda,
$$
where $x^I$ are sections of the bundle.
On page 61 of https://arxiv.org/abs/hep-th/0005247 , it is assumed without explanation that the hermitian connection of $\mathcal{O}_X(-n)$ is just 
$$
A^{(-n)}=-nA^{(1)}
$$
Why is this true? References would be appreciated.","['complex-geometry', 'algebraic-geometry', 'holomorphic-bundles']"
1807021,Definition of $n$-to-$1$ mapping.,"What is the definition of ""$n$-to-$1$ mapping""? Does an $n$-to-$1$ mapping mean to say that if $f$ is a function from $A$ to $B$, then for every $y\in R(f)$ there exists $n$ different elements in $A$ which maps to $y$ ?","['functions', 'definition']"
1807040,"What is an ""algèbre augmentée sur un corps?"" (EGA I)","In EGA I, Chapter 0, (1.1.10), Grothendieck is giving examples of terminal objects in different categories.  He says ""dans la catégorie des algèbres augmentées sur un corps $K$ (où les morphismes sont les homomorphismes d'algèbres compatibles avec les augmentations), $K$ est un objet final."" What is an augmented algebra?  I thought he just might mean the category of $K$-algebras, but this doesn't make sense because $K$ is an initial object in this category, not a final one.","['reference-request', 'algebraic-geometry']"
1807057,The integral is the area under the curve. Is there a similar notion for stochastic integrals?,"As discussed in the answers to this question , the integral is defined to be the (net signed) area under the curve. The definition in terms of Riemann sums is precisely designed to accomplish this. Now the stochastic integral in Ito calculus is more formally defined and the result of the integration is another stochastic process. Is there a similar geometric Interpretation of a stochastic integral? Are there special cases which are simpler to understand? For example what about Brownian Motion? Are there restrictions which allow pathwise integration? Edit: I found a related question here which asks how to compute $\int W_sdW_s$.","['integration', 'stochastic-integrals']"
1807067,Combinatorial formulas and interpretations,"I found that
$$ \sum_{j=0}^{s}(n-s+j)!\binom{s}{j}(s-j)! =s!  \sum_{j=0}^{s} \frac{(n-s+j)!}{j!} = \frac{(n+1)!}{n+1-s}$$
I proved this formula with induction, but I was wondering if there is a (combinatorial?) interpretation that can explain it. Moreover, I wanted to simplify in a similar way also the following: $$ \sum_{j=0}^{s}(n-s+j)!\binom{s}{j}3^{s-j}(s-j)! = s! 3^s \sum_{j=0}^{s} \frac{(n-s+j)!}{j!3^j} = ?$$
is it possible?","['combinatorics', 'factorial', 'summation', 'binomial-coefficients']"
1807078,My conjecture $\int_{0}^{1}{x^n-1 \over \ln(x)}dx=\ln(n+1)$ [duplicate],"This question already has answers here : What is $\int_0^1\frac{x^7-1}{\log(x)}\mathrm dx$? (7 answers) Closed 8 years ago . $$\int_{0}^{1}{x^n-1 \over \ln(x)}dx=\ln(n+1)$$ Let deal with case $n=1$ $$I=\int_{0}^{1}{x-1 \over \ln(x)}dx=\ln(2)$$ $u=\ln(x)$ $\rightarrow du=\frac{1}{x}dx$ $x \rightarrow 1 ,u=0$ $x \rightarrow 0, u=-\infty$ $$I=-\int_{0}^{\infty}\frac{e^{2u}-e^u}{u}du$$ Apply integration by parts $$I=\left.(e^{2u}-e^u)\ln(u)\right|_{0}^{\infty}-\int_{0}^{\infty}(2e^{2u}-e^u)\ln(u)du$$ Letting $$J=\int_{0}^{\infty}(2e^{2u}-e^u)\ln(u)du$$ Applying by parts again $$J=\left.(2e^{2u}-e^u)\ln(u)\right|_{0}^{\infty}-\int_{0}^{\infty}(2e^{2u}-e^u)\frac{1}{u}du$$ Anyway I skip the simplification and get to the result $$2I=\left.e^{2u}\ln(u)\right|_{0}^{\infty}+\int_{0}^{\infty}\frac{e^{2u}}{u}du$$ That doesn't looked correct! Integration by parts and substitution seem to failed here for me, so what is another method to evaluate this integral? Another attempt using $x^n-1=\sum_{k=0}^{n-1}(x-1)x^k$ $$\int_{0}^{1}{x^n-1 \over \ln(x)}dx=\ln(n+1)$$ $$\sum_{k=0}^{n-1}\int_{0}^{1}{(x-1)x^k \over \ln(x)}dx=\ln(n+1)$$ This is still involving Integration by parts, I am very sure is going to lengthy so I am stopping here for help. Please lend me a hand, thank you. Here is a link to Frullani's formula",['integration']
1807120,Why aren't CDFs left-continuous?,"Let $F$ be a cumulative density function on $\mathbb{R}$. From an argument in a textbook, it is shown that $F$ must be right-continuous: Let $x$ be a real number and let $y_1$, $y_2$, $\ldots$ be a sequence of real numbers such that $y_1 > y_2 > \ldots$ and $\lim_i y_i = x$. Let $A_i = (-\infty, y_i]$ and let $A = (- \infty, x]$. Note that $A = \cap_{i=1}^\infty A_i$ and also note that $A_1 \supset A_2 \supset \ldots$. Because the events are monotone, $\lim_i P(A_i) = P(\cap_i A_i)$. Thus, $$
F(x) = P(A) = P( \cap_i A_i) = \lim_i P(A_i) = \lim_i F(y_i) = F(x^+)
$$ But why doesn't this argument work in reverse to show that $F$ is left-continuous? That is, if we supposed that the $y_i$ were approaching $x$ from the left, why can't we analogously say: $$
F(x) = P(A) = P( \cup_i A_i) = \lim_i P(A_i) = \lim_i F(y_i) = F(x^-)?
$$","['probability-theory', 'probability-distributions']"
1807129,What is the period of $(2007)^{\sin x}$?,What is the period of $(2007)^{\sin x}$? Please explain how to proceed and what's the technique to generally solve these kind of problems.,"['periodic-functions', 'trigonometry']"
1807141,"How to solve trig equations and get all the solutions using graphs, $\cos(2x-\pi/3)=\cos(x)$","The question is to solve $$\cos\left(2x-\frac{\pi}{3}\right)=\cos(x)$$ I originally approached this using the addition formulae but the mark scheme showed a way by first replacing $x$ on the right with $2\pi-x$ and I understand this is due to the $\cos $ graph, however I don't understand where all the solutions came from, would really appreciate help understanding Answers: $\frac{\pi}{3}, \frac{7\pi}{3}$ and $\frac{13\pi}{3}$. Thanks",['trigonometry']
1807210,Derivatives of the Dirac delta function,"From what I understand the Dirac's Delta derivatives have the meaning
$$\int_{-\infty}^{\infty}\delta^{(k)}(x)\phi(x)dx=(-1)^k\int_{-\infty}^{\infty}\delta(x)\phi^{(k)}(x)dx$$
Assuming, of course that the function $\phi$ is differentiable up to order $k$. If that's true, you can then say
$$\phi^{(k)}(x_0)=(-1)^k\int_{-\infty}^{\infty}\delta^{(k)}(x-x_0)\phi(x)dx$$
Is this correct? And also, could it be useful in any circumstance?","['derivatives', 'real-analysis', 'distribution-theory', 'dirac-delta']"
1807219,Why aren't there uncountably many disjoint open intervals of $\mathbb{R}$?,"I know that this can't be true given that $\mathbb{R}$ is separable, but I'm having a hard time coming to grips with why this is, exactly. In particular, can't I just take some uncountable strictly increasing sequence of elements of $\mathbb{R}$, $\langle x_\alpha\mid \alpha<\omega_1\rangle$, and get uncountably many disjoint open intervals $(x_\alpha, x_{\alpha+1})$? I clearly have some kind of misunderstanding, but I'm having a hard time isolating it.","['order-theory', 'general-topology', 'elementary-set-theory']"
1807225,"For a densely defined symmetric operator $A$, is $A^2$ also densely defined?","Let $A : D(A) \to H$ be a possibly unbounded, densely defined symmetric operator on a Hilbert space $H$ ($A$ being symmetric means that $(\varphi, A\psi) = (A\varphi, \psi)$ for all $\varphi, \psi \in D(A)$). Consider the operator $A^2$ with domain $D(A^2) := \{ \psi \in D(A) | A\psi \in D(A) \}$. I would like to determine whether $D(A^2)$ is dense in $H$. The usual way to prove something like this is to assume that we have $\varphi \in H$ such that $$(\psi, \varphi) = 0$$ for all $\psi \in D(A^2)$. Then we want to show that this implies $\varphi = 0$. But I have not really been able to make any progress from this point. Hints or solutions are greatly appreciated.","['functional-analysis', 'unbounded-operators', 'hilbert-spaces']"
1807259,Finishing off this Sturm-Liouville BVP,"I'm looking at the Sturm-Liouville BVP $$\begin{cases} y'' + \lambda y = 0\\ y(0) + y'(0) = 0, y(1) = (0) \end{cases}.$$ I can do the problem but I can't finish it off at the very end (it's probably some easy algebra or trigonometric manipulation that I just can't see). For the case $\lambda > 0$ my working is as follows: Set $k^2 = \lambda$. After some working we find that our solution is $y = C_1 \cos kx + C_2 \sin kx$. Using $y(1) = 0$ we find that $C_1 \cos k + C_2 \sin k = 0$. Using $y(0) + y'(0) = 0$ we find that $0 = C_1 \cos k + C_2 \sin k$. Both of these give $0 = C_2(\sin k - k \cos k)$. So we have a non-trivial solution if $\sin k = k\cos k$, i.e. $\tan k = k$. My lecture notes are happy to take $k_n \approx (2n-1)\frac{\pi}{2}$, i.e. $\lambda_n \approx \frac{(2n-1)^2\pi^2}{4}$. I'm happy up to here, but I don't understand how we end up with eigenfunctions $y_n = \sin(k_n(1-x)$. Also, it seems that we only take $n$ to be non-negative, why is this the case?","['boundary-value-problem', 'sturm-liouville', 'ordinary-differential-equations', 'eigenfunctions']"
1807266,Is my proof correct that there are uncountably many sets of positive integers?,"Let $\mathbb{N}$ be the set of natural numbers. Prove that $2^{\mathbb{N}}$ is uncountable. Proof: Suppose that $2^{\mathbb{N}}$ is countable then $2^{\mathbb{N}}=\{A_1, A_2, A_3,\dots\}$. We have to construct the set $B$ which does not lie in $2^{\mathbb{N}}$. If $k\in A_k$ then $k\notin B$ and if $k\notin A_k$ then $k\in B$. Thus $B=\{i\in \mathbb{N}:i\notin A_i\}$. Note that $B$ is not empty since of one the $A_n$'s is empty. But $B$ is subset of $\mathbb{N}$ then $B=A_j$. But it's a contradiction since if $j\in A_j$ then $j\notin B$ and $j\notin A_j$ then $j\in B$ $\blacksquare$ Just now I am going to prove rigorously that if $S$ is countable then $2^S$ is uncountable. Proof: Since $S$ is countable then exists bijective function $A: \mathbb{N} \to S$ letting $A(n)=A_n$. The natural corresponding between $2^{\mathbb{N}}$ and $2^S$ is defined by $\varphi: 2^{\mathbb{N}}\to 2^S$ and $$\varphi(n_1,\dots, n_k)=(A_{n_1},\dots, A_{n_k}),$$ $$\varphi(n_1,\dots, n_k, \dots)=(A_{n_1},\dots, A_{n_k},\dots).$$
It's easy to check that this function is bijection. We showed that $2^\mathbb{N}$ is uncountable then $2^S$ is also uncountable. Sorry if this topic is repeated but I would like to know are my proofs right?",['elementary-set-theory']
1807269,square root commutes with multiplication for positive elements in a $C^*$ algebra?,"Let $A$ be a unital $C^*$ algebra.  If $z\in A$ is invertible, then so is $z^*$ and $z^*z$ and, furthermore, $z^*z$ is positive, so we can define using the functional calculus $|z|=\sqrt{z^*z}$.  My book then claims that $|z|$ is invertible with inverse $\sqrt{(z^*z)^{-1}}$.  Why is $|z|*|z|^{-1}=1$?  To me this looks like trying to say that $\sqrt{z^*z}*\sqrt{(z^*z)^{-1}}=\sqrt{(z^*z)(z^*z)^{-1}}=\sqrt1=1$, but I don't see why you can pull the product inside the square root like you can for reals (I don't know if this is actually how to prove the claim). I don't see how some of the basic properties about continuous functions pass through the functional calculus and still hold inside of $A$.",['functional-analysis']
1807309,Evaluate $\lim_{x\to 1} \frac{p}{1-x^p}-\frac{q}{1-x^q}$,"Evaluate $$\lim_{x\to 1} \frac{p}{1-x^p}-\frac{q}{1-x^q}$$
  where $p,q$ are Natural Numbers. I tried rationalization, but I wasn't able to get anywhere. I'm not being able to remove the $\infty-\infty$ indeterminate form. 
Any help would be appreciated. Many thanks! EDIT: The given answer is $\frac{p-q}{2}$","['limits-without-lhopital', 'calculus', 'limits']"
1807346,Calculating limits the easy way,"Couple of month ago in school we started learning limits. There were all sorts of ways some pretty hard and over-kill for something simple. Then I stumbled across l'Hopital (guess I spelled it right) and then all the simple limits got so easy, doing them in matter of seconds. So my question is: Are there more theorems or rules (like l'Hopital or similar) that you can apply to limits to calculate them more easily?","['calculus', 'limits']"
1807357,An interesting AM-HM-GM inequality: $\text{AM}+\text{HM}\geq C_n\cdot \text{GM}$,"It is not difficult to prove that if $x,y\in\mathbb{R}^+$ the inequality
$$ \frac{x+y}{2}+\frac{2}{\frac{1}{x}+\frac{1}{y}}\geq \color{purple}{2}\cdot\sqrt{xy} $$
holds, and the constant $\color{purple}{2}$ is optimal. In a recent question I proved, with a quite involved technique, that if $x,y,z\in\mathbb{R}^+$ then
$$ \frac{x+y+z}{3}+\frac{3}{\frac{1}{x}+\frac{1}{y}+\frac{1}{z}}\geq \color{purple}{\frac{5}{2\sqrt[3]{2}}}\cdot\sqrt[3]{xyz} $$
holds, and the constant $\color{purple}{\frac{5}{2\sqrt[3]{2}}}$ (that is a bit less than $2$) is optimal. Then I was wondering: Given $x_1,x_2,\ldots,x_n\in\mathbb{R}^+$, what is the optimal constant $C_n$ such that:
  $$\text{AM}(x_1,\ldots,x_n)+\text{HM}(x_1,\ldots,x_n)\geq \color{purple}{C_n}\cdot \text{GM}(x_1,\ldots,x_n)$$ I do not think my approach with $3$ variables has a simple generalization (also because in $\mathbb{R}_+^3$ the stationary points are non-trivial), but maybe something is well-known about the improvements of the AM-GM inequality, or there is a cunning approach by some sort of induction on $n$.","['inequality', 'calculus']"
1807358,Number of homomorphisms between two cyclic groups.,"Is it true that the number of homomorphisms between any two finite cyclic groups of order $m\,\&\,n$ is $\gcd(m,n)$? I have posted an answer which I believe is true, just wanted to know different approaches to this problem.","['finite-groups', 'group-homomorphism', 'group-theory', 'cyclic-groups']"
1807380,Irrationality of the concatenation of the rightmost nonzero digits in $n!$,"Surfing the internet I bumped into a very interesting problem, which I tried to solve, but got no results. The problem is following: let $h_n$ be the most right non-zero digit of $n!$, for example, $10!=3628800,$ so $h_{10}=8$. The task is to prove that decimal fraction $0,h_1h_2\ldots h_n\ldots$ is irrational.","['decimal-expansion', 'number-theory', 'rationality-testing', 'factorial', 'elementary-number-theory']"
1807381,"Homomorphism from $(\Bbb Q,+)$ to a finite group","Prove that if $f$ is a homomorphism from $(\Bbb Q,+)$ to a finite group $G$ then $f(q)=e_G$ for all $q$ in $\Bbb Q$. I attempted the following: Firsty I reason that $f(1)$ generates the entire image, because $f(p/q)=p/qf(1)$. But since $f[\Bbb Q]$ is an infinite subgroup of $G$ and $G$ itself is finite we are forced to let $f(1)=e_G$. Is this ok?","['finite-groups', 'abstract-algebra', 'group-theory']"
1807400,Trace term in the Itō formula,"I'm reading Stochastic Differential Equations in Infinite Dimensions and don't understand what the authors do in Chapter 2.3.1 . Let me introduce the necessary objects: Let $K$ and $H$ be real Hilbert spaces $Q\in\mathfrak L(K)$ be nonnegative and symmetric $K_Q:=Q^{1/2}K$ $(\Omega,\mathcal A,\operatorname P)$ be a probability space $\Phi:\Omega\times[0,\infty)\to\operatorname{HS}(K_Q,H)$ and $\varphi:\Omega\times[0,\infty)\to H$ $F:[0,\infty)\times H\to\mathbb R$ and $F_{xx}$ be the second Fréchet derivative of $F$ with respect to the second variable I don't understand the term $$\operatorname{tr}\left[F_{xx}(t,x)\left(\Phi_tQ^{\frac 12}\right)\left(\Phi_tQ^{\frac 12}\right)^\ast\right]\tag 1$$ which occurs in equation (2.53). By definition, $F_{xx}$ is an element of $\mathfrak L(H,\mathfrak L(H,\mathbb R))$. However, the authors obviously use the fact that $\mathfrak L(H,\mathbb R)\cong H$ and hence $F_{xx}$ can be identified with an element of $\mathfrak L(H)$. With this interpretation, we've got $$\underbrace{F_{xx}(t,x)}_{\in\mathfrak L(H)}\underbrace{\underbrace{\left(\Phi_tQ^{\frac 12}\right)}_{\in\mathfrak L(K_Q,H)}\underbrace{\left(\Phi_tQ^{\frac 12}\right)^\ast}_{\in\mathfrak L(H,K_Q)}}_{\in\mathfrak L(H)}\in\mathfrak L(H)\;.$$ Thus, at least it makes sense to talk about the trace of this expression. However, can we rewrite the expression $(1)$ without the identification? Above $\mathfrak L(A,B)$ and $\operatorname{HS}(A,B)$ denote the space of bounded, linear operators and Hilbert-Schmidt operators from $A$ to $B$, respectively. Moreover, $\mathfrak L(A):=\mathfrak L(A,A)$ and $L^\ast$ denotes the adjoint of a bounded, linear operator $L$.","['stochastic-processes', 'operator-theory', 'functional-analysis', 'stochastic-analysis', 'stochastic-calculus']"
1807405,"Is $[0,1] \cap \Bbb Q$ a compact subset of $\Bbb Q$?","Is $[0,1] \cap \Bbb Q$ a compact subset of $\Bbb Q$?  I get the feeling it isn't compact, but I can't figure out a way to prove this.  I understand that,  in $\Bbb Q$, any open set $U = \Bbb Q \cap O_i$ such that $O_i \subseteq R$.  But I can't figure out a way to construct a finite set that doesn't contain the intersection set.","['general-topology', 'real-analysis', 'compactness']"
1807410,Frullani 's theorem in a complex context.,"It is possible to prove that $$\int_{0}^{\infty}\frac{e^{-ix}-e^{-x}}{x}dx=-i\frac{\pi}{2}$$ and in this case the Frullani's theorem does not hold since, if we consider the function $f(x)=e^{-x}$ , we should have $$\int_{0}^{\infty}\frac{e^{-ax}-e^{-bx}}{x}dx$$ where $a,b>0$ . But if we apply this theorem, we get $$\int_{0}^{\infty}\frac{e^{-ix}-e^{-x}}{x}dx=\log\left(\frac{1}{i}\right)=-i\frac{\pi}{2}$$ which is the right result. Questions : is it only a coincidence? Is it possible to generalize the theorem to complex numbers? Is it a known result? And if it is, where can I find a proof of it? Thank you.","['complex-analysis', 'integration', 'reference-request']"
1807453,"Given block matrix $M$, show determinant relationship between $M$ and the block elements of $M.$","Given that $M = \begin{pmatrix} A & B \\ C &D \end{pmatrix}$ and $M^{-1} = \begin{pmatrix} P & Q \\ R & S \end{pmatrix},$ where $A, B,\dots$ are $k \times k$ matrices, show that $\det(M) \cdot \det(S) = \det(A).$ Gone through sheets of paper on this one... I feel like there is a trick and a quick answer.","['matrices', 'linear-algebra', 'determinant']"
1807512,Why can't I find anyone who has discovered the (irrational) constant 1.29128...? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question The constant is exactly $\sum_{n=1}^∞\frac{1}{n^n}$. Why does it seem that no one has written about it? Did I not search well enough? If so, what is the name for it? If not, it is not sufficiently ""interesting?"" I can't find it anywhere, which seems very strange. (I apologize about how little my experience in higher maths I have...)","['constants', 'sequences-and-series']"
1807549,"How can I show that $\mathcal{B} = \{(a,b)\subset \mathbb{R}\mid a,b \in \mathbb{Q}\}$ is a countable set?","I know that $\mathcal{B} = \{(a,b)\subset \mathbb{R}\mid a,b \in \mathbb{Q}\}$ is a basis on $\mathbb{R}$. I need to show that $\mathcal{B}$ is countable. How can this be done? Attempt: Take $(a,b) \subset \mathcal{B}$, let $\mathbb{Q}\cap(a,b) = \{q_1, \ldots, q_n\}$, then $(a,b) = \bigcup\limits_{i=1}^n B_\delta(q_i)$ for some $\delta > 0$ But then $\mathcal{B}$ is the set of all such $(a,b)$, how do I continue from here?","['general-topology', 'proof-writing', 'elementary-set-theory']"
1807575,"A club has 14 members. In how many ways can a president, vp, and treasurer be chosen if two specified club members refuse to serve together.","What I have so far. There are ${14 \choose 2}$ ways to choose a pair that will not serve together. If two members refuse to serve together, then there are 12 remaining club members to place into 3 distinguishable positions. This can be done in ${\frac {12!}{(12-3)!}}$ ways. Multiply ${14 \choose 2}$ . ${\frac {12!}{(12-3)!}}$ = 120,120.",['combinatorics']
1807603,when product of irrational numbers = rational number?,let $a$ and $b$ be irrational numbers. when do we have $ a \cdot b $ = rational number? for example $\sqrt{2} \cdot \sqrt{2}=2$. I was wondering if there some conditions for the product to be a rational number.,"['real-analysis', 'real-numbers', 'irrational-numbers']"
1807638,Strange behavior of $\sin(x^3)$ and $\tan(x^3)$,"I noticed this behavior a long time ago and never really figured this out but if you take the $\sin$ or $\cos$ or $\tan$ etc. of a cubic polynomial you get a very strange and erratic behavior. It seems to have no easily explainable pattern and I'm not sure why. The function is still smooth but the successive derivatives grow ever larger. My best guess is that because the derivatives keep growing larger and larger but are still periodic you get this behavior because the every more violent derivatives keep canceling each other out. Is this an example of a simple chaotic system? What features of these functions make it behave so erratically. Can I do this with other kinds of periodic functions? In short, why does this function behave so erratically?","['chaos-theory', 'trigonometry']"
1807645,Differential equation exercise.,"I am tasked with solving \begin{cases} 
y''(t) &=& \frac{(y(t)')^2}{y} - 2\frac{y'(t)}{y^4(t)} \\
y(0)   &=& -1 \\ 
y'(0)  &=& -2
\end{cases} I proceed by setting $v(s) = y' (y^{-1}(s))$ reducing the problem to \begin{cases} 
v'(s) &=& \frac{v(s)}{s} - \frac{2}{s^4} \\ 
v(-1) &=& -2 
\end{cases} This is a first order linear ODE, after multiplying by $\frac{1}{|s|}$ (I drop the absolute value and change the sign since $s$ will be in a negative interval). I obtain $$-\frac{v'(s)}{s} + \frac{v}{s^{2}} = \frac{2}{s^5}$$ This gives me $$\frac{v(s)}{s} = \frac{1}{(-2s^4)} + \frac{5}{2} \implies y'(t) =  \frac{-1 + 5y(t)^4}{2y(t)^3}$$ And I can't quite manage to integrate the reciprocal of this to get my $y(t)$. Are my calculations correct? How should I do this?  I am utilizing this method because it's the one I am expected to use at the exam (I have already been told that it can make things more difficult).","['real-analysis', 'ordinary-differential-equations']"
1807663,Derivating natural numbers.,"Some years ago I came up with a weird idea which I think was of my own, although it is not sensible to make such assertions. The idea was to define a function $f: \mathbb{N} \rightarrow \mathbb{N}$ which imitates the behaviour of the derivative of a product, i.e. (uv)' = u'v + uv'. Using the prime notation, I define the derivative of a natural number $n$ whose primary decomposition is $\; n = \prod_{i = 1}^{i = r} p_i^{\alpha_i}$ as $$n' := \sum _{j = 1}^{j = r} \alpha_j p_j^{\alpha_j - 1}\prod_{i \neq j}^{i = r} p_i^{\alpha_i} = n (\sum_{i=1}^{i = r} \frac{\alpha_i}{p_i}).$$ So I somehow applied the rule of the derivative of a multiple product, hence $(nm)' = n'm + mn'$ follows. Some particular cases are $1' = 0$ and $p' = 1$ if $p$ is prime. Although I have never found useful this ""natural derivative"", I got interested on it. Here are some nice facts about the natural derivative: Solutions of $n' = n$ are $n = p ^p,$ where $p$ is prime. It has an air of the exponential $e^x$. The ""average value"" of $n'$ is $n \times \sum_{p \in \mathbb{P} }\frac{1}{p(p-1)}$ One can reformulate the Goldbach conjecture as: All even numbers greater than $2$ are the image of a pseudoprime number (i.e. $pq$, where $p$ and $q$ are prime). It can be easily extended to $\mathbb{Q}$. So now I have introduced to you this marvellous function, I would like to ask you: Did I invented it or has it been used before? In the second case: In which context does it appear, and how much is known about it? Thanks a bunch for your answers.",['number-theory']
1807682,Why do some pdfs y axis values greater than 1,I've been trying to wrap my head around different types of pdf. Do pdfs show relative probabilities? Take this example of the exponential: When lamba=1.5 p(0)=1.4 Obviously the probability of something cannot be 1.4 Is this a relative probabilty? We can estimate that p(1)=0.3 is the event (x=0) 1.4/0.3 = 4.67 times more likely to occur than the event (x=1) ? I suppose I don't really know what a pdf really IS Is the reason you get values higher than 1.0 on the y axis just a consequence of scaling the pdf so the total area is 1?,"['statistics', 'probability', 'density-function']"
1807687,Definition of smallest equivalence relation,I came across the term 'smallest equivalence relation' in the course of a proof I was working on. I have never thought about ordering relations. I googled the term and checked stackexchange and couldn't find a clear definition. Is someone able to provide me a definition of what a smallest equivalence relation is and an example of one equivalence relation being smaller than another?,"['relations', 'equivalence-relations', 'elementary-set-theory']"
1807705,Is it natural how $L^p$ spaces measure local and global sizes the same?,"This is a continuation of my question Spaces of functions similar to $L^p$ but with different local and global sizes . I have been bothered by the fact that the $L^p$ norm on $\mathbb R^n$, which is ubiquitous in real analysis, measures the local and global sizes of a function using the same exponent. As a result, there are functions (eg. $f(x)=|x|^{-a}$ for $a>0$) that are in some $L^p$ when you restrict them to $\{|f|\leq \lambda\}$, and some $L^q$ when you restrict them to $\{|f|>\lambda\}$, but are not in any $L^r$ on all of $\mathbb R^n$ because necessarily $p>q$. I intuitively feel like the local and global $L^p$ness of a function have nothing to do with each other, and therefore it is natural to measure them with separate exponents. In my previous post, sandwich pointed out that Wiener amalgam spaces do something like this. Nonetheless, these are not very commonly encountered spaces (as seen by the Wikipedia article, which is a stub). This motivates my question: why have usual $L^p$ spaces been so successful? Am I missing something very natural about measuring the local and global parts of a function with the same $p$? Or perhaps these amalgam warrant more attention than they've seen.","['functional-analysis', 'real-analysis', 'lp-spaces']"
1807717,I'm stuck in a logarithm question: $4^{y+3x} = 64$ and $\log_x(x+12)- 3 \log_x4= -1$,"If $4^{y+3x} = 64$ and $\log_x(x+12)- 3 \log_x4= -1$ so $x + 2y= ?$ I've tried this far, and I'm stuck
$$\begin{align}4^{y+3x}&= 64 \\
4^{y+3x} &= 4^3 \\
y+3x &= 3 \end{align}$$ $$\begin{align}\log_x (x+12)- 3 \log_x 4 &= -1 \\
\log_x (x+12)- \log_x 4^3 &= -1 \\
\log_x(x+12)- \log_x 64 &= -1 \end{align}$$ then I substituted $4^{y+3x} = 64$
$\log_x (x+12) - \log_x 4^{y+3x} = -1$ I don't know what should I do next. any ideas?","['algebra-precalculus', 'logarithms', 'systems-of-equations']"
1807731,Similarity classes of matrices,"Let $M_n(K)$ be the set of all $n\times n$ matrices over a field $K$. If $\mathcal{R}$ is the equivalence relation defined by matrix similarity, what does the quotient $M_n(K)/\mathcal{R}$ looks like? Is there something that characterizes it in terms of cardinality? Is there a way to extend matrix operations to equivalence classes, making it an algebraic quotient structure of $M_n(K)$? Is there a good interpretation of $M_n(K)/\mathcal{R}$ in terms of similarity invariants (e.g., matrix determinants)? Today, on my linear algebra class, we were introduced the concept of determinants. Our teacher used geometric isometry invariants (such as area and volume) to introduce the motivation for matrix determinants. I then asked myself what would be the generalized interpretation of the determinant as some type of ''invariant''. Some Google search led me to matrix similarity, and my algebraic intuition says that this information would be codified in the quotient structure - once we ''kill' the big structure by the invariants, we would find exactly what is not varying. Well, though I never though about verifying it for any pathological case, I believe that the map $\det: M_n(K) \to K$ is surjective. That would mean that the quotient $M_n(K)/\mathcal{R}$ is at least the same cardinality as $K$ (because similar matrices have the same determinant, but I'm not sure about the converse. If it is true, matrix with the same determinant are similar, then cardinality equality would follow). I'm not sure how the operations can be extended. Maybe the quotient can be seen as a vector space over $K$ as well: scalar multiplication is surely well defined, but I'm not sure about the sum. If I restrict myself to some subset of $M_n(K)$, like the general linear group or the special linear group, could I get something more? And the interpretation or mathematical application is exactly what I long for.","['matrices', 'invariance', 'linear-algebra', 'determinant']"
1807765,"In a family with 3 children, what is the probability that they have 2 boys and 1 girl?","I'm doing some regents practice questions, and one of them asked In a family with 3 children, what is the probability that they have 2
  boys and 1 girl? And the answer choices are 3/8 1/4 1/8 1/2 My teacher said the answer was choice one but I'm having trouble understanding why. My approach was to draw out the probabilities, since we have 3 children, and we are looking for 2 boys and 1 girl, the probabilities can be Boy-Boy-Girl, Boy-Girl-Boy, and Girl-Boy-Boy. So a 2/3 chance, but I don't get how it's a 3/8 chance. Any help is appreciated.","['algebra-precalculus', 'probability']"
1807787,about a ninth-grade geometry problem,"My brother asked me this problem, and he is studying ninth-grade. I can't solve it using primitive tools of pure geometry. Hope someone can give me a hint to solve it. Thanks. Given a circle $(O, R)$ and $A$ is outside $(O)$ such that $OA > 2R$. Draw two tangents AB, AC of $(O)$. Let $I$ is midpoint of AB. Segment OI intersects with (O) at M. AM intersects with (O) at N, $N \neq M$. NI intersects with BC at Q. Prove that MQ perpendicular with OB Here is the picture","['recreational-mathematics', 'euclidean-geometry', 'geometry']"
1807798,proof that $\int_{a}^{x} = \int_{x}^{b}$,"I want to show that if $f$ is a continuous function on the interval $[a,b]$ then there must exist some $x \in [a,b]$ such that: $$\int_{a}^{x} = \int_{x}^{b}$$ Intuitively this seems very easy and I can see why it is true, its just the structure of the proof that I'm confused about. Is it sufficient to say that since $f$ is continuous, $\int_{a}^{x}$ exists $\forall x \in [a,b]$ and in particular we can find some $x_{0}$ such that: $$\int_{a}^{x_{0}} = \frac{1}{2} \int_{a}^{b}$$ then since: $$\int_{a}^{x_{0}}+\int_{x_{0}}^{b} = \int_{a}^{b} $$ we get that: $$\int_{x_{0}}^{b} = \int_{a}^{b} - \int_{a}^{x_{0}} $$ $$\int_{x_{0}}^{b} = \int_{a}^{b} - \frac{1}{2} \int_{a}^{b} $$
so: $$\int_{x_{0}}^{b} = \frac{1}{2} \int_{a}^{b} $$
and we get that: 
$$\int_{a}^{x_{0}} = \int_{x_{0}}^{b}$$ Is this rigorous enough? should I try a different method maybe using partitions and upper/lower sums? is there a sort of, mean value theorem equivalent for integrals with area instead of the derivative? Thanks guys.","['definite-integrals', 'calculus', 'proof-verification']"
1807823,Are my calculations of a new constant similar to Mill's constant based on $\lfloor A^{2^{n}}\rfloor$ and Bertrand's postulate correct?,"As Wikipedia explains in number theory, Mills' constant is defined as: ""The smallest positive real number $A$ such that the floor function of the double exponential function $\lfloor A^{3^{n}}\rfloor$ is a prime number, for all natural numbers $n$. This constant is named after William H. Mills who proved in 1947 the existence of $A$ based on results of Guido Hoheisel and Albert Ingham on the prime gaps. Its value is unknown, but if the Riemann hypothesis is true, it is approximately $1.3063778838630806904686144926...$ (sequence A051021 in OEIS)."" In other words, $\lfloor A^{3^{n}}\rfloor$ is said to be a prime-representing function , because $f(x)$ is a prime number for all positive integral values of $x$. The demonstration made by Mills in 1947 ( pdf here ) is very easy to follow (one must be careful about the already known typos of the paper to follow properly the explanation). It is also known that generically (as explained in this nice question at MSE) $\lfloor Q^{k^{n}}\rfloor$ always works if $k$ is at least $3$, so there are other constants $Q$ not defined yet that will work when $k$ is greater than $3$. It is also known that $k=2$ might not work because it depends on the Legendre's conjecture : is there always a prime between $N^2$ and $(N+1)^2$ (thought to be extremely difficult to demonstrate). The reason of this question is that based on the original demonstration and using the Bertrand's postulate as key for some manipulations, I tried to do my own version of the Mill's constant to learn, and it seems that was able to find a constant $L$ that for $\lfloor L^{2^{n}}\rfloor$  provides a sequence of integers associated to their next prime, so it is possible to calculate the associated sequence primes by a prime-representing function. The difference of this approach is that the sequence of integers obtained from $\lfloor L^{2^{n}} \rfloor$ are not primes directly, it is an integer sequence, but there is an associated sequence of primes obtained from them. Here is how it works (the questions are at the end of the explanation): According to Bertrand's postulate for a given integer (let also suppose for the demonstration that it is prime) $p_i$ it holds: $p_i < p_j < 2p_i$ for some existing prime $p_j$ Now adding $+p_i^2+1$ in the inequalities: $p_i+p_i^2+1 < p_j+p_i^2+1 < 2p_i+p_i^2+1$ Which is also true if in the left side we just leave $p_i^2$: $p_i^2 < p_j+p_i^2+1 < (p_i+1)^2$ Calling $e_j = p_j+p_i^2+1$ it is possible to build (from here I will use the same steps as in Mill's paper ) a sequence of integers $E_1, E_2,...E_n$ such as: $E_1=P_1=2$ $E_1^2 < E_2 < (E_1+1)^2$ $E_2^2 < E_3 < (E_2+1)^2$ ... $E_{n-1}^2 < E_n < (E_{n-1}+1)^2$ ... The same conditions for the sequence shown in Mill's demonstration would hold for this expression, and in this case the exponent is $2$ instead of $3$ but we do not depend on Legendre's conjecture because the associated prime $P_n$ to $E_n$ is a Bertrand's prime (so it exists always in the interval $[E_n,2E_n]$). In other words, it is possible to use the power of $2$ because the sequence is not directly a sequence of primes, but a sequence of integers attached to primes by Bertrand's postulate according to the manipulation: $P_n = E_n-E_{n-1}^2-1$ And in the same fashion as Mill's constant: $E_n=\lfloor L^{2^{n}} \rfloor$ Where L is obtained after some manual calculations (up to $n=11$) as $L_{11}=1.7197977844041078190854...$ The way of manually calculating the constant is as follows, in the same fashion that Mill's demonstration, the relationship between $L_n$ (the constant calculated after knowing the $n^{th}$ element of the sequence $E_n$) and $E_n$ is: $L_n=E_n^{\frac{1}{2^n}}$ These are the calculations for the first $5$ elements: For instance for $n=1..5$, $\lfloor L^{2^{n}} \rfloor$ is: $E_1=2$ $E_2=8$ and $P_1=E_2-(E_1)^2-1=8-2^2-1=3$ $E_3=76$ and $P_2=E_3-(E_2)^2-1=76-8^2-1=76-64-1=11$ $E_4=5856$ and $P_3=E_4-(E_3)^2-1=5856-76^2-1=5856-5776-1=79$ $E_5=34298594$ and $P_4=E_5-(E_4)^2-1=34298594-5856^2-1=5857$ $L$ slightly grows on every iteration but the growth is each time smaller, so it clearly tends to a fixed value when $n \to \infty$. This is the graph: So in essence the prime-representing function would be: $P_n = \lfloor L^{2^{n}} \rfloor -E_{n-1}^2-1 = \lfloor L^{2^{n}} \rfloor -(\lfloor L^{2^{n-1}} \rfloor)^2-1$ for $n \ge 2$, $P_n \in \Bbb P$. ...being the starting element of the sequence $E_1=2=P_1$ and $L$ the value of $L_n$ when $n \to \infty$. This is the Python code to calculate and test $L$, please be free to use it or modify it: def mb():
    from sympy import nextprime
    from gmpy2 import is_prime
    import matplotlib.pyplot as plt
    import matplotlib as mpl
    import decimal

    print(""L constant test. Equivalent to Mill's constant applying Bertrand's postulate"")
    print(""----------------------------------------------------------------------------"")
    print()

    # Decimal precision is required, change according to test_limit
    decimal.getcontext().prec = 2000
    # List of elements E_n generated by L^(2^(n))
    E_n = []
    # List of progressive calculations of L_n, the last one of the list is the most accurate
    # capable of calculate all the associated primes
    L_n=[]
    # List of primes obtained from L^(2^(n))-E_(n-1) associated to the Bertrand's postulate
    P_n=[]
    # depth of L: L will be able to calculate the primes L^(2^(n))-E_(n-1) for n=1 to test_limit-1
    test_limit=12
    for n in range(1,test_limit):
        if n==1:
            # E_1=2
            E_n.append(2)
        else:
            # E_n=(E_(n-1)^2)+1
            # Be aware that the Python list starts in index 0 and the last current element is index n-1:
            # that is why n-2 appears in the calculation below
            E_n.append((E_n[n-2]**(decimal.Decimal(2)))+P_n[n-2]+1)
        # Next prime greater than E_n: it will be in the interval [E_(n-1),2*E_(n-1)] (Bertrand's postulate)
        P_n.append(nextprime(E_n[n-1]))
        # Calculation of L_n
        L_n.append(E_n[n-1]**(decimal.Decimal(1)/(decimal.Decimal(2)**(decimal.Decimal(n)))))

    print(""List of Elements of L^(2^(n)) for n = 1 to "" + str(test_limit-1) + "":"")
    mystr = """"
    for i in E_n:
        mystr=mystr+str(i)+""\t""
    print(mystr)

    print()
    print(""List of Primes obtained from L constant: L^(2^(n))-E_(n-1)-1 for n = 1 to :"" + str(test_limit-1) + "":"")
    mystr = """"
    for i in P_n:
        mystr=mystr+str(i)+""\t""
    print(mystr)

    print()
    print(""List of calculations of L_n (the most accurate one is the last one) for n = 1 to :"" + str(test_limit-1) + "":"")
    mystr = """"
    ax = plt.gca()
    ax.set_axis_bgcolor((0, 0, 0))
    figure = plt.gcf()
    figure.set_size_inches(18, 16)
    n=1
    for i in L_n:
        mystr=mystr+str(i)+""\t""
        plt.plot(n,i,""w*"")
        n=n+1
    print(mystr)

    # Print the graph of the evolution of L_n
    # Clearly it tends to one specific value when n tend to infinity
    plt.show()

    #Testing the constant
    print()
    print(""L Accuracy Test: using the most accurate value of L will calculate the primes associated"")
    print(""to the constant and will compare them to the original primes used to create the constant."")
    print(""If they are the same ones, the constant is able to regenerate them and the theoretical background"")
    print(""applied would be correct."")
    #Using the most accurate value of L available
    L = L_n[len(L_n)-1]
    print()
    print(""L most accurated value is:"")
    print(L)
    print()
    for i in range(1,test_limit):
        print()
        tester = int(L**(decimal.Decimal(2)**(decimal.Decimal(i))))
        if i==1:
            tester_prime = 2
        else:
            tester_prime = decimal.Decimal(tester) - (E_n[i-2]*E_n[i-2]) - 1
        print(""Current calculated E:\t"" + str(tester) + "" for n = "" + str(i))
        print(""Original value of E:\t"" +str(E_n[i-1]) +  "" for n = "" + str(i))
        if tester == E_n[i-1]:
            print(""Current calculated prime:\t"" + str(tester_prime) +  "" for n = "" + str(i))
            if i==1:
                print(""Original value of prime:\t2 for n = "" + str(i))
            else:
                print(""Original value of prime:\t"" + str(P_n[i-2]) + "" for n = "" + str(i))
        else:
            # If we arrive to this point, then the constant and the theory would not be correct
            print(""ERROR L does not generate the correct original E"")
            return
    print()
    print(""TEST FINISHED CORRECTLY: L generates properly the primes associated to the Bertrand's postulate"")
mb() The list of first primes is (the Python code provides the output up to $n=11$) $2,3,11,79,5857,34298597,1176393584675453,1383901866065518680880707763873,$ $1915184374899624805461632693020424461862877625516091453480257,...$ Initially $L=1.71979778...$ seems not have been defined before, but I am not really sure. At least I was not able to find such constant at Mill's constant-related papers on Internet, but it seems interesting. The advantages of $L$ versus $A$ (the original Mill's constant) would be: The growth rate of the double exponential is lower due to the use of powers of $2$ instead of powers of $3$ (or more for other Mill's constant interpretations), so for the same quantity of $n$ values calculated, $L$ provides smaller primes than $A$ in less time. The use of $\lfloor L^{2^{n}} \rfloor$ would not require the validation of Legendre's conjecture because it depends on Bertrand's postulate, so the theory would hold as it is. I would like to ask the following questions: Are the manipulations for the calculation of the elements of $\lfloor L^{2^{n}} \rfloor$ applying Bertrand's postulate correct or there is a restriction or error? (initially the heuristics show that it would be correct) Is such constant interesting? Initially seems so because it is able to use a power of $2$ growth rate (versus power of $3$ in best of cases for standard Mill's constant). If the calculations are correct, are there other manipulations that could be applied to lower even more the growth rate? Thank you!","['proof-verification', 'number-theory', 'constants', 'prime-numbers', 'sequences-and-series']"
1807825,"Showing that $2\int_0^\infty {\cosh(x)-1\over x(e^{ax}-1)}\,\mathrm dx=\ln\left({\pi \over a\sin\left({\pi\over a }\right)}\right)$","$$2\int_{0}^{\infty}{\cosh(x)-1\over x(e^{ax}-1)}\,\mathrm{d}x=\ln\left({\pi \over a\sin\left({\pi\over a }\right)}\right)$$ $$\cosh(x)=1+{x^2\over 2!}+{x^4\over 4!}+{x^6\over 6!}+\cdots$$ $${\cosh(x)-1\over x}={x\over 2!}+{x^3\over 4!}+{x^5\over 6!}+\cdots=\sum_{n=1}^{\infty}{x^{2n-1}\over (2n)!}$$ $$I=\sum_{n=1}^{\infty}{2\over (2n)!}\int_{0}^{\infty}{x^{2n-1}\over e^{ax-1}}\,\mathrm{d}x$$ Recall the Zeta function $${a^k\over(k-1)!}\int_{0}^{\infty}{x^{k-1}\over e^{ax}-1}\,\mathrm{d}x=\zeta(k)$$ Now setting $k=2n$ $${a^{2k}\over(2k-1)!}\int_{0}^{\infty}{x^{2k-1}\over e^{ax}-1}\text{d}x=\zeta(2k)$$ Rearrange $$I=\sum_{n=1}^{\infty}{2\over (2n)!}\cdot{(2n-1)!\zeta(2n)\over a^{2n}}=\sum_{n=1}^{\infty}{\zeta(2n)\over a^{2n}\cdot{n}}$$ So finally we have $$\sum_{n=1}^{\infty}{\zeta(2n)\over a^{2n}\cdot{n}}=\ln\left({\pi \over a\sin\left({\pi\over a }\right)}\right)$$ This does not answer my question, it is only showing the transformation from integral into an infinite sum. Anyone can help here to show that the infinite sum produced this closed form?","['integration', 'sequences-and-series']"
1807845,Distribution on number of revisits in past $k$ steps of Markov chain,"Consider a finite-state Markov chain with transition matrix $P$. The chain starts in a state chosen uniformly over all the states and runs indefinitely from there. We're going to examine only the $k ≥ 0$ most recent steps of the chain. In particular, we will consider the number of states that appear $0, 1,\ldots, k$ times in those most recent $k$ steps. Let's define a vector $C = [c_0, c_1, \ldots,c_k]$ whose $i$-th element is the number of states that have been visited $i$ times within the past $k$ steps. For example, if $k = 4$, if the state space is $1, 2, \ldots, 10$, and if the chain is $$1, 2, 1, 4, 3, 2, 4, 3, 3, 2, 4, 5, 5, 3, 2, 1, 2, 3$$ then $c = [7, 2, 1, 0, 0]$. What is the expected value of $C$ and, ideally, how is $C$ distributed?","['markov-chains', 'statistics', 'markov-process', 'probability']"
1807866,Find the maximum and minimum values of $\sin^2\theta+\sin^2\phi$ when $\theta+\phi=\alpha$,"Find the maximum and minimum values of $\sin^2\theta+\sin^2\phi$ when $\theta+\phi=\alpha$(a constant). $\theta+\phi=\alpha\implies\phi=\alpha-\theta$ $\sin^2\theta+\sin^2\phi=\sin^2\theta+\sin^2(\alpha-\theta)$ Let $f(\theta)=\sin^2\theta+\sin^2(\alpha-\theta)$ $f'(\theta)=2\sin\theta\cos\theta-2\sin(\alpha-\theta)\cos(\alpha-\theta)$ Putting $f'(\theta)=0$ gives $\sin2\theta=\sin2(\alpha-\theta)$ $2\theta=2\alpha-2\theta\implies \alpha=2\theta$ If $\alpha=2\theta$,then by $\theta+\phi=\alpha$ gives $\phi=\theta$ I am stuck here,the answer given is maximum $1+\cos\alpha$ and minimum $1-\cos\alpha$.But i have found only one critical value(when $\phi=\theta$) and that too i cannot decide whether it will give maximum or minimum value. Please help.","['trigonometry', 'optimization', 'calculus', 'supremum-and-infimum']"
1807897,Which way should you run from the lions?,"This is a fun problem that I saw somewhere on the internet a long time ago: Suppose you are at the center of an equilateral triangle with side length $s$. At each of its vertices, there is a lion which is determined to eat you. The lions start at a constant speed of $v_l$, and they are always running directly towards your current location. You start in the center, and can run at a constant speed $v_h$ (assume instantaneous acceleration for all parties). You are NOT enclosed in the triangle, you are free to try to run wherever you want. Which patch should you take in order to survive the longest time possible? How long can you survive? At first, because everybody's speed is constant, I thought we can just work with functions of $x$ for the paths of the lions and the human, and try to maximize the arc length of the lions' paths. However, I think it's much easier to work with the functions in parametric form, because the tangent lines to the lions' path at $t=t_0$ should go trough your position at $t_0$. Also, I think it's reasonable to assume that at $t=0$ your direction is straight towards the midpoint of one of the sides, because any other direction would cause you to meet one of the lions faster.","['optimization', 'calculus', 'parametric', 'differential-games', 'parametrization']"
1807899,Finding the limit of a difficult exponential function,"I am stumped on this question, any help or tips would be appreciated! Find the limit, if it exists: $$\lim_{x\to\infty } \left(\sqrt{e^{2x}+e^{x}}-\sqrt{e^{2x}-e^{x}}\right)$$",['limits']
1807905,A Sine and Inverse Sine integral,"A demonstration of methods While reviewing an old text book an integral containing sines and sine inverse was encountered, namely,
$$\int_{0}^{\pi/2} \int_{0}^{\pi/2} \sin(x) \, \sin^{-1}(\sin(x) \, \sin(y)) \, dx \, dy = \frac{\pi^{2}}{4} - \frac{\pi}{2}.$$ One can express $\sin^{-1}(z)$ as a series and integrate, but are there other methods nearly as efficient?","['integration', 'trigonometric-series']"
1807912,How to compute $\det(A+J)$?,If $A$  be an $n\times n$ matrix and $J$ be a matrix of same order with all entries $1$ then Show that $\det( A + J)=\det A$ + sum of all cofactors of $A$. I have tried using Laplace Expansion but I am not getting it. Please give some hints at doing the same.,"['matrices', 'linear-algebra', 'determinant']"
1807953,Are they twin primes?,"Let $n$ be a positive integer and let $p_1, p_2,...,p_n$ be first n primes. And let $m$ be the smallest integer $m\ge\,p_n$ such that $m$ and $m+2$ are coprime to $p_1,p_2,..., p_n$. Are $m$ and $m+2$ always prime?","['number-theory', 'twin-primes']"
1807976,Angle of circular droplet,"I am trying to find the angle $\theta$ of the following droplet: I think using $\tan$ is the right way to go, and I thought of using it on the  angle formed by the line $r$ and $b$. However, that requires that I can relate it to $\theta$, which isn't clear to me is necessarily the case. I'd be happy to get some help or a push in the right direction.","['circles', 'angle', 'trigonometry', 'geometry']"
1807981,Is $f(x)$ constant under these conditions?,"Statement Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be an function that is concave up and increasing. If $\displaystyle \lim_{x\to \infty}\frac{f(x)}{x}=0$, then $f$ is constant. It'll be easy if we assume $f$ is second differentiable, the conditions just mean $f''(x)\geq 0$ and $f'(x)\geq 0$. (If $f'(x)=a>0$ at $x=b$ then for $x>b$ we have $f(x)>a(x-b)+f(b)$ by MVT and clearly in this case the limit cannot be $0$ and therefore $f'(x)$ has to be $0$ everywhere). Can this statement be proved without assuming $f$ differentiable? Here concave up is defined by for any $x,y\in \mathbb{R}$, $t\in [0,1]$, $f(tx+(1-t)y)\leq tf(x)+(1-t)f(y)$. Increasing is defined by $y>x\Rightarrow f(y)\geq f(x)$. Context of this problem: If the given statement is proved, then the solution in the thread How to prove Liouville's theorem for subharmonic functions given by Martin R can be extended to prove a stronger version of the result (see my comment on that answer).","['complex-analysis', 'real-analysis', 'harmonic-functions']"
1807985,optimize pasting text,"Someone asked me how can he paste a string 1000 times in Windows notepad.
While this can be done easily using editors like Vi, I'm trying to answer his question using notepad only.
So the problem goes like this,
we have 2 options: We can paste the contents of clipboard in 1 keystroke (C-v). We can double the text using 4 keystrokes (C-a C-c [down key] C-v). This doubles the text in clipboard too. How can we optimize on number of keystrokes required? I wrote a small program to calculate the number of keystrokes required which uses a simple heuristic:
If (value on screen for keystroke k - 4) > (screen value for keystroke k - 1) + clipboard value then doble the screen value using C-a C-c down C-p else keep on pasting using C-v #include <stdio.h>

#define TIMES_TO_PRINTED 1000

int main() {
  int clip[100], screen[100], keystroke;
  int i;

  for (i = 0; i < 100; i++) {
    clip[i] = 0;
    screen[i] = 0;
  }

  keystroke = 0;
  screen[1] = 1;
  clip[1] = 1;
  while (screen[keystroke] < TIMES_TO_PRINTED && keystroke < 100) {
    keystroke++;

    if (keystroke > 4) {
      if ((screen[keystroke - 4] * 2) >
          (screen[keystroke - 1] + clip[keystroke])) {
        screen[keystroke] = screen[keystroke - 4] * 2;
        clip[keystroke + 1] = screen[keystroke - 4];
      } else {
        screen[keystroke] = screen[keystroke - 1] + clip[keystroke];
        clip[keystroke + 1] = clip[keystroke];
      }
    } else {
      screen[keystroke] = screen[keystroke - 1] + clip[keystroke];
      clip[keystroke + 1] = clip[keystroke];
    }
    printf(""%d - %d - %d\n"", clip[keystroke], keystroke, screen[keystroke]);
  }

  printf(""%d %d"", screen[keystroke], keystroke);
  return 0;
} which outputs: Clipboard - keystroke - screen value
1 - 1 - 1
1 - 2 - 2
1 - 3 - 3
1 - 4 - 4
1 - 5 - 5
1 - 6 - 6
1 - 7 - 7
1 - 8 - 8
1 - 9 - 10
5 - 10 - 15
5 - 11 - 20
5 - 12 - 25
5 - 13 - 30
5 - 14 - 35
5 - 15 - 40
5 - 16 - 50
25 - 17 - 75
25 - 18 - 100
25 - 19 - 125
25 - 20 - 150
25 - 21 - 175
25 - 22 - 200
25 - 23 - 250
125 - 24 - 375
125 - 25 - 500
125 - 26 - 625
125 - 27 - 750
125 - 28 - 875
125 - 29 - 1000 Thus using this heuristic we can reach 1000 using 29 keystrokes. I'm not sure if this is the optimal solution, also is there any other way to solving it rather than enumerating all the possibilities?","['optimization', 'discrete-mathematics']"
1808041,Does cancellation impact vertical asymptotes?,"Question: Let $r(x) = \frac{(x^2 + x)}{(x + 1)(2x - 4)}$. Does the graph has $x = 1$ as one of its asymptotes? Answer: No. My reasoning: $\frac{(x^2 + x)}{(x + 1)(2x - 4)} = \frac{x(x + 1)}{(x + 1)(2x - 4)} = \frac{x}{(2x - 4)}$ and so, it cannot have $x=-1$ as one of its asymptotes. However , what if I don't calcel and then say that $-1$ is a vertical asymptote? Will I be wrong?","['algebra-precalculus', 'polynomials', 'functions', 'graphing-functions']"
1808067,"$f(f(...f(x)...))$ $a$ times, where $a\in\mathbb{R}$","Take $f(x)$ and do a ""double-call"": $f^2(x)=f(f(x))$ I use this notation here to explain my problem. This can be easy calculated for any function. Also $f^{100}(x)$ is not really a problem. This means this operation works well for natural numbers. Then it is already possible to define some easy rules: $f^1(x)=f(x)$ $f^{n+1}(x)=f(f^n(x))$ $f^a(f^b(x))=f^{a+b}(x)$ The next question is: Can this be extended to all integers? Yes. For this we only require the inverse function $f^{-1}(x)$, because we know $f^{-1}(f(x))=x$. Or more detailed: $f^{-1}(f^n(x))=f^{n-1}(x)$. This results in some more rules: $f^0(x)=x$ $f^{-1}(f(x))=x$ $f^{n-1}(x)=f^{-1}(f^n(x))$ Of course it is not that easy to calculate $f^{-1}(x)$ for many functions, but this is ok. One may ask if this can be extended to rational numbers? E.g. $f^{0.5}(x)$ would mean that $f^{0.5}(f^{0.5}(x))=f(x)$. It sounds reasonable, but this seems to be very hard to calculate for many function and the result does not seem to be unique. Take $f(x)=x$ then $f^{0.5}(x)$ could be $x$ or also $-x$. Maybe there are much more other solutions? Or is it also possible to extend this thing to the real numbers? Or even complex numbers? If it can be done: how? And if not: why? Thank you very much Kevin","['real-numbers', 'functions']"
1808103,Show that there exists a fixed point for this (set theoretic) class function,"I see that this question might be trivial but I can't seem to figure it out myself: Suppose that $F:ON\to ON$ is a class function: that is, for every ordinal $\alpha$ there is unique ordinal $F(\alpha)$ corresponding. Suppose that $F$ satisfies following conditions: $\alpha < \beta \implies F(\alpha)<F(\beta)$ i.e. $F$ is monotonic $F(\lambda)=\bigcup\{F(\gamma):\gamma<\lambda\}$ for limit ordinals $\lambda$ Show that, then there exists $\alpha \in ON$ such that $F(\alpha)=\alpha$. The hint suggested to prove $\alpha\le F(\alpha)$ for every ordinals, which can be done by induction on $\alpha$. Also suggested by hint, I constructed a ""candidate"" $\alpha$: By recursion, there is $f:\omega \to f[\omega]$ such that $f(0)=0$ and $f(n+1)=F(f(n))$, and then set $$\alpha=\bigcup f[\omega]$$ The problem is I don't see how this might help me. It is clearly suggested that above $\alpha$ works, but I have no information regarding $F(\alpha)$ at this point that proving $F(\alpha)\subset \alpha$ seems impossible.","['set-theory', 'functions', 'ordinals']"
1808110,Evaluate $\lim_{x\to 0}\left(\frac{(1+x)^{1/x}}{e}\right)^{1/x}$,"Evaluate $$\lim_{x\to 0}\left(\dfrac{(1+x)^{1/x}}{e}\right)^{1/x}$$ $$$$I recognized it as the $1^\infty$ form. Thus the limit is equal to 
$e^t$ where $$t=\lim_{x\to 0} \left(\dfrac{(1+x)^{1/x}}{e}-1\right)\dfrac1x$$ Albeit using L'Hopital, I cannot understand how to evaluate this limit. Could somebody please help me? Many thanks!","['limits-without-lhopital', 'calculus', 'limits']"
1808120,"Why is $\int_{0}^{r}\left(\int_{\partial B(x,s)}u\ dS\right)ds=u(x)\int_{0}^{r}n\alpha(n)s^{n-1}ds$","$\int_{B(x,r)}u\ dy=\int_{0}^{r}\left(\int_{\partial B(x,s)}u\ dS\right)ds=u(x)\int_{0}^{r}n\alpha(n)s^{n-1}ds=\alpha(n)r^nu(x)$ This appears in the proof of a theorem in the book of Evans PDE. I don't get the second equality, so in general $\int_{\partial B(x,s)}u\ dS\neq u(x)n\alpha(n)s^{n-1}$, they're only equal if we integrate both sides, am I wrong ? where does $s^{n-1}$ come from ? Is it valid only for radial symmetric $u$ ?","['multivariable-calculus', 'integration', 'partial-differential-equations']"
1808125,"If $P(n)$ divides $P(P(n)-2015)$, prove that $P(-2015)=0$","Q. Let $P(x)$ be a non-constant polynomial whose coefficients are
  positive integers. If $P(n)$ divides $P(P(n)-2015)$ for every natural
  number $n$, prove that $P(-2015)=0$. In one of the sources, the solution given is as follows:
Note that $P(n)-2015-(-2015)=P(n)$ divides $P(P(n)-2015)-P(-2015)$ for every positive integer $n$. But $P(n)$ divides $P(P(n)-2015)$ for every positive integer n. Therefore, $P(n)$ divides $P(-2015)$ for every positive integer $n$. Hence $P(-2015)=0$. I am not able to understand that how $P(n)-2015-(-2015)=P(n)$ divides $P(P(n)-2015)-P(-2015)$.
Please help me out.","['polynomials', 'functions']"
1808129,Generalised inclusion-exclusion principle,"In answers to combinatorial questions, I sometimes use the fact that if there are $a_k$ ways to choose $k$ out of $n$ conditions and fulfill them, then there are $$
\sum_{k=j}^n(-1)^{k-j}\binom kja_k
$$ ways to fulfill exactly $j$ of the conditions. This is true because a case in which exactly $m$ of the conditions are fulfilled is counted $\binom mk$ times in $a_k$ and thus contributes $$
\sum_{k=j}^n(-1)^{k-j}\binom kj\binom mk=\delta_{jm}\;.
$$ In particular, if the number of ways of fulfilling $k$ particular conditions is the same, $b_k$, for all choices of the $k$ conditions, then $a_k=\binom nkb_k$ and there are $$
\sum_{k=j}^n(-1)^{k-j}\binom kj\binom nkb_k
$$ ways to fulfill exactly $j$ of the conditions. I found that inclusion-exclusion seems to be almost exclusively applied to the case $j=0$, to find the number of ways to fulfill none (or, complementarily, at least one) of the conditions, and that many, even very experienced users are not familiar with this generalisation. That prompted me to look around for a reference for it, but I couldn't find one. So my questions are: Is this more general inclusion-exclusion principle well-known? If so, could you provide a reference for it that I could point to when asked about it?","['inclusion-exclusion', 'combinatorics', 'reference-request']"
1808158,"Find all pairs (x, y) of real numbers such that $16^{x^2+y} + 16^{x+y^2}=1$","Find all pairs (x, y) of real numbers such that $$16^{x^2+y} + 16^{x+y^2}=1$$",['algebra-precalculus']
1808171,Where am I removing solutions in this equation,"I have this equation : $\tan 2x = 3\cot x$ By rearranging I am getting the solutions: $37.8$, $142$, $218$ and $322$. However the mark scheme also has $90$, $270$. Hence I am wondering where I am getting rid of solutions. Here is my working: $$\frac{2\tan x}{1-\tan^2x} = 3\cot x$$
$$\frac{2\tan^2x}{1 - \tan^2x} = 3$$ and then by rearranging: $$\tan^2x = \frac{3}{5}$$ $$\tan x = \sqrt{\frac{3}{5}} , -\sqrt{\frac{3}{5}}$$ and then solve using the general solutions of trig equations formula $x = 180n + a$ The mark scheme converts the equation so that $... = 0$ which allows you to make the assumption that the denominator must equal infinity hence giving the other two solutions, however I would guess that it is my multiplication of $\tan x$ which caused the issue here. UPDATE: 
This is the solution that is in the mark scheme: $$\frac{2\tan x}{1-tan^2x} = \frac{3}{\tan x}$$
$$\frac{2\tan^2x - 3 + 3\tan^2x}{(1-\tan^2x)\tan x} = 0$$
$$5\tan^2x - 3 = 0$$
or denominator $= \infty$ $x = 37.8, 218, 142, 322, 90, 270$",['trigonometry']
1808183,Solve first order partial diferential equation,"Consider the Cauchy problem:
$$\left\{\begin{array}{lll}
x^2\partial_x u+y^2\partial_yu=u^2\\
u(x,2x)=1
\end{array}\right.$$
It is easy to show that the characteristic equations are given by:
$$\frac{dx}{x^2}=\frac{dy}{y^2}=\frac{dz}{px^2+qy^2}=\frac{dp}{2pu-2xp}=\frac{dq}{2qu-2yq}=dt$$
By the Lagrange-Charpit's method, we need a first integral $\Psi$ different to $\Phi=x^2p+y^2q-u^2$. How I can find  $\Psi$? Maybe we should get directly the characteristic curves of the previous system? Many thanks!","['ordinary-differential-equations', 'analysis', 'partial-differential-equations']"
1808241,How does one determine the singular points of a toric variety?,"Consider the toric surface corresponding to the fan $\Delta$ consisting of $$\sigma_1=\langle e_1,-e_1+2e_2\rangle$$ $$\sigma_2=\langle-e_1+2e_2,-e_1-2e_2\rangle$$ $$\sigma_3=\langle -e_1-2_2,e_1\rangle$$ and their faces. Since the fan is simplicial, the toric variety is a simplicial toric variety. So it has singularities. However I am unable to identify what the singular points are. For example if I just take the cone $\sigma_1$, the corresponding affine toric variety has a singularity at the origin. But how do I find the singular points of the toric variety corresponding to $\Delta$? Thank you.","['toric-varieties', 'algebraic-geometry', 'toric-geometry']"
1808246,Absorbing convex hull into Minkowski sum,"Let $B \subseteq \mathbb{R}^n$ be compact. Is there some bounded set $D \subseteq \mathbb{R}^n$ with $0 \in D$ such that $$ Conv(B) + D = B + D \quad ? $$ Here $+$ denotes the Minkowski sum and $Conv$ the convex hull. EDIT: I should point out that I can prove this for various special cases, including when $B$ has finitely many extremal points or contains the boundary of its convex hull. Assuming the statement is true, it just seems like something that should be known (and maybe have a very simple proof..).","['reference-request', 'geometry']"
1808248,Constructing a rational function from its asymptotes,"Question: Give an example of a rational function that has vertical asymptote $x=3$ now give an example of one that has vertical asymptote $x=3$ and horizontal asymptote $y=2$. Now give an example of a rational function with vertical asymptotes $x=1$ and $x=-1$, horizontal asymptote $y=0$ and x-intercept 4. My solution: $(a) \frac{1}{(x-3)}$. This is because when we find vertical asymptote(s) of a function, we find out the value where the denominator is $0$ because then the equation will be of a vertical line for its slope will be undefined. $(b) \frac{2x}{(x-3)}$. Same reasoning for vertical asymptote, but for horizontal asymptote, when the degree of the denominator and the numerator is the same, we divide the coefficient of the leading term in the numerator with that in the denominator, in this case $\frac{2}{1} = 2$ $(c) \frac{(x-4)}{(x-1)(x+1)}$. Same reasoning for vertical asymptote. Horizontal asymptote will be $y=0$ as the degree of the numerator is less than that of the denominator and x-intercept will be 4 as to get intercept, we have to make $y$, that is, $f(x)=0$ and hence, make the numerator 0. So, in this case; to get x-intercept 4, we use $(x-4)$ in the numerator so that $(x-4)=0 \implies x=4$. Are my solutions correct of have I missed anything, concept-wise or even with the calculations?","['algebra-precalculus', 'functions', 'rational-functions']"
1808258,Is this matrix always orthogonal?,"I was reading about orthogonal matricies and noticed that the $2 \times 2$ matrix 
$$\begin{pmatrix} \cos(\theta) & \sin(\theta) \\ -\sin(\theta) & \cos(\theta) \end{pmatrix} $$
is orthogonal for every value of $\theta$ and that every $2\times 2$ orthogonal matrix can be expressed in this form. I then wondered if this can be generalized to any smooth paramtrization of the unit circle. More precisely, let $x(t)$ and $y(t)$ be a smooth parametrization of the unit circle for all $t$ in some interval $I \subseteq \Bbb R$ such that $|\langle x(t),y(t) \rangle| = 1$ for all $t \in I$. Is the matrix
$$A= \begin{pmatrix}x(t)&y(t)\\x'(t) & y'(t) \end{pmatrix} $$
necessarily orthogonal?
At first I thought yes, but I'm having trouble proving it. Letting $v = \langle x(t), y(t) \rangle$, it suffices to show three things: 1) $|v| = 1$, 2) $v \cdot v' = 0$, and 3) $|v'| = 1$. (1) follows straight from how $x(t)$ and $y(t)$ were defined. (2) can be obtained by differentiating the equation $x(t)^2 + y(t)^2 = 1$:
\begin{align*}
&\frac{d}{dt} \Big[ x(t)^2 + y(t)^2 \Big]= 0 \\
&\implies 2x(t)x'(t) + 2y(t)y'(t)= 0 \\
&\implies v \cdot v' = 0.
\end{align*}
But I couldn't find a way to prove (3). Now I am unsure whether (3) is true at all. Is $A$ even orthogonal in the first place? Any hints would be much appreciated.","['linear-algebra', 'calculus']"
1808273,Continuity of a function $f : \mathbb R^n \to \mathbb R^k$,"So preparing for the end-term of the semester, strolling trough exercises in our huge book (Th. Rassias Calculus 2) and in the internet, I found one more theoritical one, that as always bothers me (because it's theoritical !!). Show that in the function $f : \mathbb R^n \to \mathbb R^k$ , $f=(f_1,\dots,f_k)$ is continuous at $\vec x_o$, if and only if the function $f_i$ is continuous at $\vec x_o$ for $i = [1,2,\dots,n]$. Now, it's pretty obvious that since the ""big"" function $f$ is formed by the ""smaller"" $f_i$'s , all the sub-functions must be continuous, but that's not a mathematical proof of course. Also, I could say that for $f$ to be continuous, its limit must be defined, which means that $\lim_{x \to x_o}f = f_o$, which would mean that : $\lim_{x \to x_o}f = (\lim_{x \to x_o}f_1^o,\dots,\lim_{x \to x_o}f_k^o) = f_o = (f_1^o,\dots,f_k^o)$. Now, by definition that means that each of the sub-functions must be continuous, which means that they are continuous if and only if $\forall e>0$ $\exists d>0$ : $\forall x \in \mathbb R^n$ with distance $d(x,x_o)<d \Rightarrow d(f(x),f(x_{io})<d$. Is that a kind of correct claim ? I am pretty sure it's not as mathematical it should be. Any tips-hints-corrections or help over this will be really appreciated !","['multivariable-calculus', 'real-analysis', 'calculus', 'limits']"
1808297,What should $\int \frac{1}{x} dx$ equal to?,"Before you say that $\int \frac{1}{x} dx$ is equal to $\ln|x| +C$ due to positve and negative, I would like to show you why it is not convincing to me. Problem 1 and its possible solution. Evaluate
$$
\begin{equation} \sum_{n=1}^\infty \frac{\sin(n)}{n}
\end{equation}
$$
From infinite geometric series
\begin{equation}
\sum_{n=1}^\infty x^{n-1}=\frac{1}{1-x} ;|x|<1
\end{equation}
Integrating this with respect to $x$ we would get (Constant vanishes due to $x=0$)
\begin{equation}
\sum_{n=1}^\infty \frac{x^n}{n}=-\ln|1-x|
\end{equation}
So $\sum_{n=1}^\infty \frac{\sin(n)}{n}$ is just an imaginary part of \begin{equation}
\sum_{n=1}^\infty \frac{z^n}{n}=-\ln|1-z|
\end{equation}
Where $z=e^i$ But the right hand side has no imaginary part at all, but the summation is clearly exists, this seems to suggest that the integral is equal to $\ln(x) +C$ There is another curious way to evaluate integral on negative reals if we only consider only principal values. Problem 2 and its possible solution. Evaluate
\begin{equation} \int_{-4}^{-2} \frac{1}{x} dx
\end{equation}
If we give that $\int \frac{1}{x} dx=\ln(x) +C $ then the integral is
\begin{equation} \int_{-4}^{-2} \frac{1}{x} dx=\ln(-2)-\ln(-4)
\end{equation}
Using principal values we will get
\begin{equation} \ln(-2)-\ln(-4)=\ln(2)+i\pi-\ln(4)-i\pi=-\ln(2)
\end{equation}
Which is exactly equal to when we use $\int \frac{1}{x} dx=\ln|x|+C$ These 2 problems are the reasons why the result $\ln |x| +C$ not convincing but there might be flaws in the proposed solutions. If there is a flaws please explain them too.","['logarithms', 'integration', 'complex-numbers', 'calculus']"
1808322,Solving a functional equation $2 f(2x)=f(x)(1+\cos(x))+f(x+\pi)(1-\cos(x))$,"I am trying to solve the following functional equation, which appears in some of my physics calculations : 
$f(x)=\frac{1}{2}\left(f(\frac{x}{2})(1+\cos(\frac{x}{2}))+f(\frac{x}{2}+\pi)(1-\cos(\frac{x}{2}))\right)$, for a function $f$ defined on $(0,2\pi)$. I am interested in all functions satisfying this equation, but let us start by finding all functions which are defined on $[0,2\pi]$, which are continuous and differentiable. Constant functions are solutions. Let us note $f(0)=a$ and $f(2\pi)=b$. 
My numerics, starting from some function and iterating until I get to a fixed point seem to point towards a unique solution depending only on $a$ and $b$. If $a=b$, this would be the constant function. If $a\neq b$, the solution seems to have the following properties (conjectures) : Monotonous $f(\pi)=(f(0)+f(2\pi))/2$ $f(\pi-x)+f(\pi+x)=2*f(\pi)$, so $f(\pi)$ is a symmetric point of the curve First derivative is $0$ in $0$ and $2\pi$ The last point can be proven easily by deriving the equation, which gives
$f'(x)=\frac{1}{4}\left(f'(\frac{x}{2})(1+\cos(\frac{x}{2}))+f'(\frac{x}{2}+\pi)(1-\cos(\frac{x}{2}))+\sin(\frac{x}{2})(f(\frac{x}{2}+\pi)-f(\frac{x}{2}+)\right)$
Evaluating at $0$ and $2\pi$ yields $0$. Evaluating the original equation at $0$ or $2\pi$ yields consistent results, at $\pi$ gives $f(\pi)=(f(\pi/2)+f(3\pi/2))/2$, which is consistent with the second conjecture at this particular point. So far I haven't managed to prove the other conjectures on the solution.
Note that all these properties are shared by $Cos(x/2)+C$, but this is not a solution, so there are other things needed to characterize a solution. Here is a plot of an approximate numerical solution. Any ideas how to solve this? Thanks!","['recurrence-relations', 'functional-equations', 'functions', 'functional-analysis', 'special-functions']"
1808331,On representations of a nonabelian group of order $pq$,"Let $p,q$ primes number s.t. $p>q$ and let $G$ a non abelian group of order $pq$. 1) Determine all degree of irreducible representation 2) Show that $|[G,G]|=p$ (where $[G,G]=\left<ghg^{-1}h^{-1}\mid g,h\in G\right>$) 3) Show that $q$ divide $p-1$ and that $G$ has $q+\frac{p-1}{q}$ conjugacy classes. My answers 1) There is of course the trivial representation of degree 1. I know that if there is $k$ irreducible representation, then $1+d_2^2+\cdot +d_k^2=pq$ where $d_i$ it the degree of the $i-$th representation, but I don't know how to apply it here. 2) Since $[G,G]$ is a subgroup of $G$ and that $G$ is not abelian, $|[G,G]|\neq 1$ and thus $|[G,G]|\in\{p,q,pq\}$. Now I know that a group of order $pq$ with $p$ and $q$ prime is resoluble, and thus, if $|[G,G]|=pq,$ the group $G$ wouldn't have any subgroup $H$ s.t. $G/H$ abelian what would be a contradiction with the fact that it's resoluble (isn't it ?). And thus, $|[G,G]|\in\{p,q\}$. Now, each group of prime order is abelian, then $[G,G]$ is abelian. But how can I choos between $p$ and $q$ ? 3) I have no idea.","['finite-groups', 'representation-theory', 'group-theory']"
1808332,What is the derivative of $x^n$?,"If $n$ is an integer I can evaluate the limit in the ""difference quotient"" to see that the derivative of $x^n$ is $nx^{n-1}$.  If $n=p/q$ is rational then I can write x as a $q$th root of $x^p$ and since $p$ is an integer I can evaluate the limit in the difference quotient. But what if $n$ is an irrational number?",['calculus']
1808345,What is the motivation behind the Bessel function of second kind,"I am studying Bessel function and found the good reference by G.N. Watson At some point in page 58 he introduces the following expression due to Hankel: \begin{eqnarray}
 \lim_{\nu \to n} \frac{J_{\nu}(x) - (-1)^{n} J_{-\nu}(x)}{\nu-n} 
\end{eqnarray} This made perfect sense since $J_{\nu}$ and $J_{-\nu}$ are not linearly
independent when $\nu = n \in \mathbb{Z}$.  Then the author goes into
great detail studying this equation. I was convinced that he would get to the equation: \begin{eqnarray*}
  Y_{\nu}(x) =  \frac{ \cos \nu \pi  \; J_{\nu}(x) - J_{-\nu}(x)}{\sin \nu \pi}.
\end{eqnarray*} but.....no.  He decided to introduce this equation as a new approach
from Hankel with no motivation and let the other development there hanging on the air. Any one can help me understand what could have motivated Hankel to introduce this equation? I know it is good and satisfies the requirements....but.... Thanks. Update: It is easy to see (shown in Watson's book) that the first equation here implies \begin{equation}
  \lim_{\nu \to n }  \left [ \frac{\partial J_{\nu}}{\partial \nu}
       - (-1)^n \frac{\partial J_{-\nu}}{\partial \nu} 
     \right ] \quad \quad (1).
\end{equation} On the other hand by applying L'Hôpital's rule we find from
the second equation above the following chain: (I avoid the argument $x$ on the functions to simplify notations) \begin{eqnarray*}
  Y_{\nu}(x) &=& \frac{ \cos \nu \pi  \; J_{\nu}(x) - J_{-\nu}(x)}{\sin \nu \pi} \\
 &=& \lim_{\nu  \to n}
     \frac{- \pi \sin \nu \pi J_{\nu} + \cos \nu \pi \frac{\partial}{\partial \nu} 
       J_{\nu} - \frac{\partial}{\partial\nu}{J_{-\nu}} }{\pi \cos \nu \pi} \\
&=& \lim_{\nu \to n } \frac{1}{\pi} \left [ \frac{\partial J_{\nu}}{\partial \nu}
       - (-1)^n \frac{\partial J_{-\nu}}{\partial \nu} 
     \right ] 
\end{eqnarray*} So, up to a factor of $1/\pi$, we are back to equation (1). I have no problem accepting equation (1) as a motivated linearly 
independent function of $J_{\nu}(x)$. My question specifically is how to go back from equation (1) to the representation $Y_{\nu}(x)$.  It is something like ""inverse"" 
L'Hôpital rule. Is there a way to do this? According to Watson, the formula above for $Y_{\nu}(x)$ was given by Weber after a small modification of a similar formula given by Hankel (which did not work
for half integer numbers). Watson says that Weber got this formula as a limit from an integral representation. ??","['bessel-functions', 'math-history', 'calculus', 'special-functions', 'ordinary-differential-equations']"
1808404,Prove $\lim_{n\to\infty} \frac{a_n}{n}$ exists for positive sequence where $a_{n+m} \leq a_n + a_m$,"Let $\{a_n\}_{n=1}^\infty$ be a positive sequence of real numbers such that $a_{n+m} \leq a_n + a_m.$  Prove that $\lim_{n\to\infty} \frac{a_n}{n}$ exists by showing that $\lim_{n\to\infty} \frac{a_n}{n} = \inf_{n \geq 1} \frac{a_n}{n}.$ A similar question has been posted previously, see Limit for sequence $a_{m+n}\leq a_m+a_n$ , and I have found that this is the general result of ""Fekete's Subadditive Lemma"" for sequences, but I am unhappy with the few proofs I've seen.  Perhaps the clearest one I have found is http://web.mat.bham.ac.uk/R.W.Kaye/seqser/fekete.html , but this seems a bit tricky for what is expected on this qualifier question. We are given a hint ""treat $\liminf$ and $\limsup$ separately"".  Does anyone know of a more direct way to show that this limit exists?","['sequences-and-series', 'analysis']"
1808412,Two subsequences with different limits $\implies$ not convergent,"I've already done a lot of searching around MSE for this. (In particular, If a sequence has two convergent subsequences with different limits, then it does not converge .) The Cauchy criterion has not been covered yet. I would like to show - without using the fact that a converging sequence has converging subsequences, in which both cases have the same limits - that if I have two subsequences with differing limits, the sequence does not converge. That is, $\exists \epsilon > 0$ such that for all $M \in \mathbb{N}$, there is an $n \geq M$ such that $|x_n - x| \geq \epsilon$. (I hope my negation above is correct.) I am given two subsequences of $x_n$, namely $x_{n_i} \overset{i \to \infty}{\to}a$ and $x_{m_i} \overset{i \to \infty}{\to}b$, $a \neq b$. Suppose, by way of contradiction, that $\lim\limits_{n \to \infty}x_n = x$. I have the subsequence convergence, so something like $|x_{n_i} - a| < \text{?}$ and $|x_{m_i} - b| < \text{??}$. I thought maybe to use a trick here:
$$|x_n - x| = |x_{n}-x_{n_i}+x_{n_i}-a-x_{m_i}+x_{m_i}-b+b+a-x|$$
but I'm not sure how to proceed from here (triangle inequality isn't helpful).","['real-analysis', 'sequences-and-series']"
1808430,Integral of Dirac delta function from zero to infinity,"I know that:
$$\int_{-\infty}^{+\infty} \mathrm{d}t \, f(t) \delta(t) = f(0)$$ However, I cannot figure out the result of the integral below: $$\int_{0}^{+\infty} \mathrm{d}t \, f(t) \delta(t) = ?$$ Is it $$\int_{0}^{+\infty} \mathrm{d}t \, f(t) \delta(t) = \frac{f(0)}{2}?$$ Please provide a source for the answer too.","['dirac-delta', 'distribution-theory', 'real-analysis', 'integration']"
1808470,"Is it okay to think of functions as of vectors with ""uncountable index""","In some applied areas that have a little scent of functional analysis (e.g., getting error bound in numerical methods), it is somewhat appealing for me to think of functions $\mathbb R \to \mathbb R$ as of infinite-dimensional vectors $f$ with real ""uncountable index"" $x \in \mathbb R$, as if $f(x) \approx f_x$, just like addressing the element of countable-index vector could look like $v_3$ or $v_{\frac 3 4}$ (which also seems okay as soon as $\mathbb Q$ it is countable? and vice versa $v \in \mathbb R^3$ can be seen as $v \in \mathbb [0; 3]\cap\mathbb N \to \mathbb R$; and our notation $v_3$ is a certain type of ""currying""). Are there any fundamental flaws in this type of thinking that might lead to counter-intuitive errors (something seems valid\invalid if you think of $x$ as just a uncountable index, but is actually invalid\valid; like, something bad happening about completeness or ?)","['functional-analysis', 'functions', 'vector-spaces']"
1808481,Proof that $C(K)$ is a Grothendieck space for $K$ an extremely disconnected compact space.,"I am looking for a proof, other than the original article by Grothendieck which is in French, that the space $C(K)$ is Grothendieck when $K$ is extremely disconnected.","['functional-analysis', 'banach-spaces', 'compactness']"
1808549,What's a good estimate of $\sum\limits_{k \mid n} {n \choose k}$?,"Question : What's a good estimate of $\sum\limits_{k \mid n} {n \choose k}$? We have found an OEIS page for that: https://oeis.org/A056045 $1, 3, 4, 11, 6, 42, 8, 107, 94, 308, 12, 1718, 14, 3538, 3474, 14827, 18, 68172, 20, 205316, 117632, 705686, 24, 3587174, 53156, 10400952, 4689778, 41321522, 30, 185903342, 32, 611635179, 193542210, 2333606816, 7049188, 10422970784, 38, \dots$ Remark : This sum appears naturally for getting an easy estimate of the cardinal of the subgroup lattice $\mathcal{L}(G)$ of a group $G$ of order $n$. For this post, we are interested in the sum above, not $|\mathcal{L}(G)|$.","['inequality', 'binomial-coefficients', 'number-theory', 'combinatorics', 'summation']"
1808606,Why does the pmf for a Poisson Distribution Maximize at $x = \lambda$?,"For a random variable $X$ s.t. $X$ has a Poisson distribution: $$
P(k \text{ events in interval}) = \frac{\lambda^k e^{-\lambda}}{k!}
$$ The following graph seems to indicate that the maximum probability value is $k = \lambda$ (and perhaps also $\lambda-1$). Question: Formally, why is this the case? I have been able to do some algebraic manipulation: $$
f_X(x) = \frac{e^{-\lambda}\lambda^x}{x!} = \frac{\lambda^x}{x! \sum_{n=0}^{\infty}\frac{\lambda^n}{n!}}
$$ so that $$
f_X(x) = \frac{\lambda^x}{\lambda^x + x! \left( \sum_{n=0}^{x-1} \frac{\lambda^n}{n!} + \sum_{m=x+1}^{\infty} \frac{\lambda^m}{m!} \right)}
$$ But it is still not clear to me why the maximum is at $x = \lambda$ and $x = \lambda-1$.","['probability-theory', 'statistics', 'poisson-distribution']"
1808611,Integral of conditional probability density function,"As far as I understand, when we fix the condition for the conditional density, we get probability distribution and the integral over all the space is $1$ $P(X|Y=y_0)$: $$\int_{\mathbb{R}}f_{X \mid Y}(x \mid y=y_0)dx=P(X|Y=y_0)<1 $$ However, suppose we want to take integral: $$\int_{\mathbb{R}}\bigg(\int_{\mathbb{R}}f_{X \mid Y}(x \mid y)dx\bigg)dy $$ I thought it is equal to $1$, but approximate numerical computation through summation for continuous conditional density
$$\sum_{i=1}^N \sum_{j=1}^N f_{X\mid Y}(a+\frac{b-a}{N}i \ \ \big| \ \ a+\frac{b-a}{N}j)\cdot(\frac{b-a}{N})^2 $$
 gives very big values, e.g. $3000$ or even $1e+25$.","['probability-theory', 'estimation', 'probability']"
1808629,Let $n \in \mathbb{N}$. Proving that $13$ divides $(4^{2n+1} + 3^{n+2})$ [duplicate],"This question already has answers here : $13\mid4^{2n+1}+3^{n+2}$ (5 answers) Closed 6 years ago . Let $n \in \mathbb{N}$. Prove that $13 \mid (4^{2n+1} + 3^{n+2} ). $ Attempt: I wanted to show that $(4^{2n+1} + 3^{n+2} ) \mod 13 = 0. $ For the first term, I have $4^{2n+1} \mod 13 = (4^{2n} \cdot 4) \mod 13  = \bigg( ( 4^{2n} \mod 13) \cdot ( 4 \mod 13 ) \bigg) \mod 13. $ But still I don't know how to simplify the first term in the large bracket. Any help/suggestions?","['number-theory', 'modular-arithmetic', 'elementary-number-theory']"
1808673,Explicit verification of signs in Morse complex,"I'm trying to check by hand that the signs in the Morse complex, defined via choices of orientations on the unstable manifolds, lead to $\partial^2=0$.  The books I've looked in seem to say either that this sign verification is easy (e.g. Audin-Damian), or prove $\partial^2=0$ by introducing more sophisticated notions like orientations of determinant line bundles of Fredholm operators (e.g. Schwarz). Is there a good reference which explicitly works out that the broken trajectories at opposite ends of a one-dimensional moduli space count with opposite sign? Here's what I've been attempting.  Suppose we have critical points $a$, $b$, $b'$ and $c$, of indices $k+1$, $k$, $k$ and $k-1$ respectively, and broken trajectories $a \rightarrow b \rightarrow c$ and $a \rightarrow b' \rightarrow c$ which are joined by a path of genuine trajectories $a \rightarrow c$.  We can view this as an embedding of $[0, 1]^2$ into our manifold $M$, with $a$ and $c$ at one pair of opposite corners and $b$ and $b'$ at the other, with the broken trajectories given by the edges. We can trivialise $TM$ over this square, and after fixing orientations of the unstable manifolds $U_a$, $U_b$, etc we get induced orientations on the stable manifolds $S_a$, $S_b$, etc.  The sign of the trajectory $a \rightarrow b$ comes from comparing the orientations of $U_a$ and $S_b$, whilst that of the trajectory $b \rightarrow c$ comes from comparing $U_b$ with $S_c$.  The orientations of $S_b$ and $U_b$ are related by the orientation of $M$.  What I can't see is how to piece this information together. The phenomenon I'm worried about is the following.  Suppose $V$ is a two-dimensional oriented real vector space with basis $e_1, e_2$.  Then the vectors $a=e_2$ and $b=2e_1+e_2$ both have the same orientation relative to $e_1$, and similarly $c=e_1$ and $d=e_1+2e_2$ both have the same orientation relative to $e_2$, but $a, c$ and $b, d$ have opposite orientations to each other. If I could relate the tangent space of $S_b$ to $S_c$ directly, say, rather than just in the quotient of $TB$ by $TU_a$, then I would be fine.  Intuitively this seems plausible: it looks like near to the trajectory $a \rightarrow b$, the tangent space $TS_c$ is simply spanned by $TS_b$ along with the inward pointing normal vector to the edge of the square.  But I don't know how to prove this. Added later: The following example explains my confusion.  Suppose we have trivialised $TM$ over the square as above, and let $e_1, e_2, f_1, f_2$ be a basis in this trivialisation.  Let the square lie in the $e_1, e_2$ plane, with $a$ at $(0, 0)$, $b$ at $(1, 0)$, $b'$ at $(0, 1)$ and $c$ at $(1, 1)$.  Now suppose $TU_a=\langle e_1, e_2, f_2\rangle$ $TS_b=\langle e_1, f_2 \rangle$ $TU_b=\langle e_2, f_2\rangle$ $TS_{b'}=\langle e_2, f_1+2f_2\rangle$ $TU_{b'}=\langle e_1, 2f_1+f_2\rangle$ $TS_c=\langle e_1, e_2, f_1\rangle$ $TU_c=\langle f_2\rangle$ with the given bases for the tangent spaces to the unstable manifolds positively oriented.  I calculate that all four edge trajectories count with positive signs. It seems like the problem is related to the discrepancy between $TS_{b'}$ and $TS_c$, as mentioned in the final paragraph of the original version of the question. This has now evolved into a separate question, here .","['differential-topology', 'reference-request', 'morse-theory', 'algebraic-topology', 'differential-geometry']"
1808706,"""Exactly one person"" quantifier [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question How do I translate the following English sentences without Uniqueness Quantifier: There is exactly one person who hates everyone All people hates exactly one person.",['discrete-mathematics']
1808727,Taking the integral of unit vectors?,"Question: Solution: This is confusing to me since it appears the solution takes an integral of unit vectors. How is this possible? Can't you only take integrals of scalars? The notation used is: $(x,y,z)$ is for rectangular coordinates, $(\rho,\varphi,z)$ for cylindrical coordinates and $(r,\theta,\varphi)$ for spherical coordinates. ${ { \hat { a }  } }_{ ρ }$ represents the unit vector for $\rho$ (same applies to $x, y, z$ and other coordinates). For each of these integrals, can't you simply take the vector out of the integral? What is the unit vector being substituted with?","['multivariable-calculus', 'vectors', 'integration', 'calculus']"
1808741,Infinite power tower paradox with e and pi.,"I was experimenting with Euler's Identity.  If $e^{((\pi*i)/2)}$ is $i$, couldn't you recursively plug in $i$ into the expression.  For example: 
$$e^{((\pi/2)*e^{((\pi/2)*e^{((\pi/2)*e^{((\pi/2)*e^{((\pi/2)*e^{((\pi/2)*e^{((\pi/2)i)))))))}}}}}}}.
$$ That should be equal to $i$.  So you could plug that whole thing in for the $i$ in the equation, giving a new equation equaling $i$, so that new equation can be plugged into itself.  If you do this an infinite number of times, you wind up with an infinitely large power tower which uses only positive numbers, and equals $i$.  The farther you go to this expression, the closer it should get to $i$.  Logically it can never equal $i$ when you apply the recursion a finite number of times, but it can go to $0$, which is the closest it can get.  However, when you plug this in, it quickly zooms off towards infinity, not $0$.  Could you help me resolve this?","['number-theory', 'trigonometry', 'power-towers', 'calculus']"
1808756,Minimal polynomial of $\alpha$ over $\mathbb{Q}$ and $\mathbb{Q}(\sqrt{2})$,"I have been working on these two minimal polynomial questions and am particularly concerned about (b) Find the minimal polynomial for $\sqrt[3]{4}+\sqrt[3]{2}$ (a) over $\mathbb{Q}$ By setting $\alpha=\sqrt[3]{4}+\sqrt[3]{2}$ , I found the minimal polynomial to be $\alpha^3-6\alpha-6=0$ ( edited ). This method involved just squaring out terms and was fairly lengthy - is this the standard procedure? (b) over $\mathbb{Q}(\sqrt{2})$ What does it mean to be the minimal polynomial over $\mathbb{Q}(\sqrt{2})$ ? Over $\mathbb{Q}$ , I see the minimal polynomial as the polynomial of lowest degree such that $\alpha$ is a root but I cannot see what is going on here. From (a) , we know that $[\mathbb{Q}(\alpha) : \mathbb{Q}]=3$ . So: $$[\mathbb{Q}(\alpha, \sqrt{2}) : \mathbb{Q}]=[\mathbb{Q}(\alpha, \sqrt{2}) : \mathbb{Q}(\alpha)][\mathbb{Q}(\alpha) : \mathbb{Q}]=3[\mathbb{Q}(\alpha, \sqrt{2}) : \mathbb{Q}(\alpha)]$$ So since the degree is a multiple of $3$ , the degree of the minimal polynomial of $\alpha$ over $\sqrt{2}$ is also a multiple of $3$ . Can we automatically conclude it is $3$ ? I do not believe so since we are considering a larger field ( $\mathbb{Q}(\sqrt{2}) \subset \mathbb{Q}$","['minimal-polynomials', 'abstract-algebra', 'galois-theory', 'extension-field']"
1808799,Positive definite if and only if determinants are positive,"Suppose $\mathbf{A} = [a_{ij}]$ is a $n \times n$ matrix. I have read that $\mathbf{A}$ is positive-definite if and only if 
$$\begin{vmatrix}
a_{11}
\end{vmatrix} > 0\text{, }\begin{vmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{vmatrix} > 0\text{, }\begin{vmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{vmatrix} > 0\text{, } \dots\text{,  etc}.$$
I'd like to prove this statement. Let $\mathbf{x} \in \mathbb{R}^n \setminus \{\mathbf{0}\}$. Perform induction on $n$. If $n = 1$, this is trivial: $a_{11}\|\mathbf{x}\|^2 > 0 \Longleftrightarrow a_{11} > 0$. Suppose this is true for $n = k$. This is where it gets particularly difficult: how do you consider every upper left submatrix (as above) of $\mathbf{A}$? Please avoid using the singular value decomposition or eigendecomposition.","['matrices', 'linear-algebra']"
1808808,Fractional anti-derivatives and derivatives of the logarithm,"Anti-derivatives and derivatives of the natural logarithm are well defined until we attempt to evaluate the fractional derivative and anti-derivatives. The background to this problem was that I was trying to evaluate fractional derivatives of $\frac1x$ , which is usually given as $D^n\frac1x=\frac{\Gamma(0)}{\Gamma(-n)}x^{-1-n}$ , but that is defined only for $n=0$ and nowhere else.  You can define it for $n\in\mathbb N$ , but this is not the best definition one could have.  ( $D^n$ is the $n$ th derivative with respect to $x$ ) I have the general $n$ th anti-derivative ( $I^n$ ) of the natural logarithm as $$I^n\ln(x)=\frac{x^n\left(\ln(x)-\int_0^1\frac{t^n-1}{t-1}dt\right)}{\Gamma(n+1)}$$ Proved by induction: $\frac d{dx}I^n\ln(x)=I^{n-1}\ln(x)$ , and holds true for $n=1$ . I have this graphed on Desmos. (I really like that it has an integral feature.  And if it is too slow, click the little circles on the right to turn off those functions.) While this is a great formula, it doesn't really work for derivatives ( $D^n=I^{-n}$ ), or at least, Desmos stops graphing at $n=-0.99$ , but before that, it appears as though $\lim_{n\to-1}I^n\ln(x)=\frac1x$ I attempted to evaluate it for $n=-1$ $$\frac1x=D^1\ln(x)=I^{-1}\ln(x)=\lim_{n\to-1}\frac{x^n\left(\ln(x)-\int_0^1\frac{t^n-1}{t-1}dt\right)}{\Gamma(n+1)}$$ If you try to directly substitute, you get an indefinite form, so I attempted to do a limit method instead.  I would like to apply L'Hostpital's rule, but I don't quite know how to deal with either the numerator nor the denominator. I have found that $$I^{1/2}\ln(x)=\frac{2\sqrt x\left(\ln(x)-2+\ln(4)\right)}{\sqrt\pi}$$ The $2-\ln(4)$ is wolframalpha's evaluation of $\int_0^1\frac{t^{1/2}-1}{t-1}dt$ From here, you can differentiate like normal to get $D^{(2n-1)/2}\ln(x)$ , $n\in\mathbb N$ . More importantly, if the limit from above is correct, then my formula works for $I^{-n}\ln(x)$ with $n\in\mathbb N$ and I can finally define what $D^n\frac1x$ is equal to! So the question: How can we evaluate the limit: $\lim_{n\to-1}I^n\ln(x)$ ?  What does it equal? Is the fractional derivative of the natural logarithm and $\frac1x$ already known?  I've made posts on this before, and it doesn't appear many people have tackled this problem. Here and Here .  Both have received little attention.  Attempting to use regular fractional derivative formulas are extremely messy, so I could not actually use them.  If someone more experienced can, that'd be great.","['fractional-calculus', 'calculus', 'limits']"
1808830,"Biholomorphic $f:\{z\in\mathbb{C}:|z|<1,~Im(z)>0\}\rightarrow\{z\in\mathbb{C}:|z|<1\}$","Is there a bijective holomorphic function $$f:\{z\in\mathbb{C}:|z|<1,~Im(z)>0\}\rightarrow\{z\in\mathbb{C}:|z|<1\}$$ such that $f^{-1}$ is holomorphic? You can give me a composition of functions. For example: If you know a biholomorphic function $$f_0:\{z\in\mathbb{C}:|z|<1,~Im(z)>0\}\rightarrow\{z\in\mathbb{C}:Im(z)>0\}=:H$$ that's enough as the function $$f_1:H\rightarrow\{z\in\mathbb{C}:|z|<1\}$$ is biholomorphic.","['complex-analysis', 'holomorphic-functions']"
1808845,Most students asked why is that ${a\over b}\div{c\over d}={ad\over bc}$,Most students asked why is that $${a\over b}\div{c\over d}={ad\over bc}$$ I just told them: inverse the second fraction and multiply. Why? They ask me. I have no idea. Any logical answers to them kids? These day teachers just told students at secondary to memorise and no why is allowed in lessons. I think that is wrong. Putting people of studying maths.,"['algebra-precalculus', 'fractions']"
1808887,Show that giving a right-action of a group $G$ on a set $A$ is the same as giving a left-action of $G^{op}$ on A,"This is a part of an exercise from ""Algebra: Chapter 0"" by Paolo Aluffi.
First, I provide the necessary definitions. An action of a group $G$ on a set $A$ is a set-function $\rho: G \times A \to A$ such that $\forall g,h \in G \ \ \forall a \in A$ we have $\rho(e_G, a) = a, \ \ \  \rho(gh,a) = \rho(g, \rho(h,a))$ or, alternatively, An action of a group $G$ on a set $A$ is a homomorphism $\sigma: G \to S_A$ where $S_A$ denotes the symmetric group of $A$ . We can call this a left-action . A right-action would be defined the same way expect for ""associativity"" axiom: $\forall a \in A \ \ \ \forall g,h \in G \ \ \ a(gh) = (ag)h$ Next: An opposite group $G^{op}$ of a group $G$ is a group $(G, \times )$ where $a \times b = ba$ . I know that $G^{op} \cong G$ with $f, f(g) = g^{-1}$ being an isomorphism. Now , I need to show that giving a right-action of $G$ on a set A is the same as giving a homomorphism $G^{op} \to S_A$ , that is, a left-action of $G^{op}$ on $A$ . Any ideas?","['group-actions', 'abstract-algebra', 'group-theory']"
1808900,Coloring Hats on Cats,"Imagine I have $n$ indistinguishable cats and $k$ designs to color the hats on
those cats. Consider the case where $k \leq n.$ I want to figure out the
number of ways I can color the hats on these indistinguishable cats such that
each design shows up at least once. I've been having some funky problems with trying to get this counting problem
right. I originally imagined that, given that the cats are indistinguishable,
We can first assign $k$ designs to $k$ of these cats 1 way and then
choose among the $k$ designs for each of the other $n-k$ indistinguishable
cats $k^{n-k}$. I assumed that we should divide this by the number of ways
we could permute those $n-k$ cats, since the order of assignment doesn't matter.
Thus, my original hypothesis for the counting proof was
$$\frac{k^{n-k}}{n-k}.$$
However, this hypothesis has failed on a number of inputs. Namely, when
$n=4$ and $k=2$, the number of ways should be 3, but this hypothesis says it
should be $2$. Does anyone have recommendations on how to go about this kind of
problem?",['combinatorics']
