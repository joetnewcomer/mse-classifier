question_id,title,body,tags
2755727,Obtaining the limiting probability,"It is required to obtain the limiting probability $$\lim_{n\rightarrow \infty}P\left (\frac{X_1-X_2+\cdots+X_{2n-1}-X_{2n}}{\sqrt{n}} \leq x \right )$$ The random variables are IID with variance $1.$ My approach The variance of the $X_1-X_2+\cdots+X_{2n-1}-X_{2n}$ will be $2n$. One $\sqrt{n}$ is already in the denominator. Hence, we would divide the both sides by $\sqrt{2}$. So, the Variable $\frac{X_1-X_2+\cdots+X_{2n-1}-X_{2n}}{\sqrt{2n}}$  will be a standard Normal. But, how to proceed from here. I don't have answers available. That's why I am getting difficulty in this problems. Any help?","['probability-theory', 'probability', 'limits']"
2755733,Understanding solution of tossing a coin 400 times..,"Why  in this If, tossing a coin 400 times, we count the heads, what is the probability that the number of heads is [160,190]? question heropup's asnwer is like that? I don't understand the blue text, I think it should be 190 instead of $\color{blue}{200}.$ And why to do this step $\color{blue}{\Pr[159.5 \le X \le 200.5] }?$ when you can pass directly to standarization. This is his/her answer: With $n = 400$ trials, the exact probability distribution for the number of heads $X$ observed is given by $X \sim {\rm Binomial}(n = 400, p = 1/2)$, assuming the coin is fair.  Since calculating $\Pr[160 \le X \le \color{blue}{200}]$ requires a computer, and $n$ is large, we can approximate the distribution of $X$ as ${\rm Normal}(\mu = np = 200, \sigma^2 = np(1-p) = 100)$.  Thus $$\begin{align*} \Pr[160 \le X \le 200] &\approx \color{blue}{\Pr[159.5 \le X \le 200.5] }\\ &= \Pr\left[\frac{159.5 - 200}{\sqrt{100}} \le \frac{X - \mu}{\sigma} \le \frac{200.5 - 200}{\sqrt{100}} \right] \\ &= \Pr[-4.05 \le Z \le 0.05] \\ &= \Phi(0.05) - \Phi(-4.05) \\ &\approx 0.519913. \end{align*}$$  Note that we employed continuity correction for this calculation.  The exact probability is $0.5199104479\ldots$. A similar calculation applies for $\Pr[160 \le X \le 190]$.  Using the normal approximation to the binomial, you would get an approximate value of $0.171031$.  Using the exact distribution, the probability is $0.17103699497659\ldots$.","['normal-distribution', 'probability-distributions', 'statistics', 'probability', 'proof-explanation']"
2755760,Applications of First Order Differential Equations,"Can I get help for this question please? Suppose that a tank containing a liquid is vented to the air at the top and has an outlet at the bottom through which the liquid can drain. It follows from Torricelli’s law in physics that if the outlet is opened at time $t = 0$ , then at each instant the depth of the liquid $h(t)$ and the area $A(h)$ of the liquid’s surface are related by $$A(h){dh\over dt} = −k\sqrt h$$ where $k$ is a positive constant that depends on such factors as the viscosity of the liquid and the cross-sectional area of the outlet. Assume that $h$ is in metres, $A(h)$ is in square metres, and $t$ is in seconds. Now a conic tank in the accompanying figure is filled to a depth of 1 metre at time $t = 0$ and suppose that the constant in Torricelli’s law is $k = 0.025$ . (a) Find the depth $h(t)$ at time t while the liquid is draining. (b) How many seconds will it take for the tank to drain completely? This is what I have tried so far: Since the base of the cone is just a circle, the area will then be: $$\mathrm{Area} = {\pi}r^2$$ I thus got the equation: $${\pi}r^2{dh \over dt}= -k {\sqrt h}$$ $${\pi}r^2{dh \over {\sqrt h}}= -k  dt$$ $$\int{1 \over {\sqrt h}} dh = \int {-k \over {\pi}r^2} dt$$ I then got my final equation as: $$h(t) = \left( {-2k \over {\pi}r^2}t + {c \over 2} \right)^2$$","['applications', 'ordinary-differential-equations', 'calculus']"
2755772,(Complex Numbers) I've almost solved this problem but can't understand which particular case is this problem talking about.,"If $x,y,z$ are the roots of the equation $x^3 + px^2 + qx + p = 0$, prove that $\tan^{-1}x + \tan^{-1}y + \tan^{-1}z = n\pi$ except in one particular case . Note: $n$ is any integer.","['complex-analysis', 'trigonometry', 'complex-numbers']"
2755779,How many ways to colour a $4 \times 4$ grid using four colours subject to three constraints,"In how many ways can a $4 \times 4$ square grid be coloured using four different colours so that no colour is repeated in any row, column, or along the two main diagonals. For clarity, one valid solution to this problem is shown below. I am after unique solutions, so: rotations of the grid through angles of $90^\circ, 180^\circ$, and $270^\circ$, and reflections about the horizontal, vertical, and the two main diagonals are not considered different. I am guessing this problem is perhaps well known, so I apologise in advance, but have not been able to make much progress towards its solution. It may even go by a well known name, making it easier to identify, and if this is indeed the case I would be interested in knowing what it is called.","['puzzle', 'combinatorics', 'coloring', 'latin-square']"
2755844,Find $cot\space x_n$ if $sin(x_{n+1}-x_n)+2^{-(n+1)}sin\space x_nsin \space x_{n+1}=0$ for $n\ge1$,Also Given Suppose $x_1=tan^{-1}2>x_2>x_3>......$ are positive real numbers I cannot understand as to how I should approach but I have a hunch which is a bit hand-wavy way to prove it but tried to show that $lim_{n \to \infty}x_n =\dfrac{\pi}{4}$ (For the proof see my edits in @hypernova's answer) But can anybody prove it and then use it to find $cot \space x_n$ please?,"['recursion', 'trigonometry', 'limits-without-lhopital']"
2755866,"Proof of Lemma 6.1, II in Hartshorne","In Hartshorne's proof for Lemma 6.1, II, he said ""it would be sufficient to show that there only finitely many prime divisors $Y$ of $U$ for which $v_Y(f) \neq 0$"". How can we conclude that $v_Y(f) \neq 0$ for $Y$ is a prime divisor in $X$ if and only if $v_{Y'}(f) \neq 0$, with $Y' = Y \cap U$ ?",['algebraic-geometry']
2755867,High iteration Newton's Method,"Suppose newton's iteration method is applied to $f(x)=1/x$ with $x_{0}=1$. Find $x_{50}$. So for me it seems like that newton's method is trying to approximate the root of the function. From Newton's Method $x_{i+1}=x_{i}-\frac{f(x_{i})}{f'(x_{i})}$ So \begin{align*}
x_{i+1}=x_{i}-\frac{\frac{1}{x_{i}}}{-\frac{1}{x^2_{i}}}=x_{i}+x_{i}
\end{align*}
From the above relation, we can compute the first values
\begin{align*}
x_{1}&=x_{0}+x_{0}=2\\
x_{2}&=x_{1}+x_{1}=4\\
x_{3}&=x_{2}+x_{2}=8\\
x_{4}&=x_{3}+x_{3}=16\\
x_{5}&=x_{4}+x_{4}=32
\end{align*}
So
\begin{align*}
x_{0}&=1\\
x_{1}&=x_{0} \cdot 2\\
x_{2}&=x_{0} \cdot 2^2\\
x_{3}&=x_{0} \cdot 2^3\\
x_{n}&=x_{0} \cdot 2^n
\end{align*} Therefore $x_{50}=2^{50}$
I just think my result looks strange, does it just mean what we can see intuitively that the function has no roots?","['numerical-methods', 'analysis']"
2755868,"kernel, range and adjoint of an operator in sequence space","I am working on an exercise in Brezis' functional analysis book (ex 2.23) and I'm having trouble with a certain step: Let $E=l^1$ and consider $T: E\to E$ with $Tu = (u_n/n)_{n\geq 1}$ . Determine $N(T), N(T)^\bot, T^*, R(T^*), \overline{R(T^*)}$ . I got that $N(T) = \{0\}$ , hence $N(T)^\bot = l^{\infty}$ , the adjoint looks identical to $T$ but operates on the dual space $E^* = l^\infty$ . The last two objects are interesting, though: $$R(T^*) = \{(v_n/n)_n, v\in l^\infty\} = \bigcap_{p>1}l^p$$ because $\|(v_n/n)_n\|_{l^p} < \infty$ for any $v\in l^\infty$ . Now I don't know what $$\overline{\bigcap_{p>1}l^p}^{l^\infty}$$ is. Is that just $l^\infty$ ?","['functional-analysis', 'operator-theory']"
2755881,Integral Representation of Dirac Delta on a Manifold,"In Cartesian coordinates on an $\Bbb{R}^d$ plane the Dirac delta can be represented as a Fourier transform:
$$\delta^d(\vec r-\vec r')=\int \frac{d^d \vec k}{(2\pi)^d}\;\;\exp\left({i\vec k\cdot (\vec r-\vec r')}\right)\tag{1}$$ Correct me if I am wrong, but on a general manifold it is typical to define the Dirac delta in two ways*: As:
$$\int d^d\xi \;  \sqrt{g(\vec \xi)} \delta(\xi'-\xi)=1$$ Or as:
$$\int d^d\xi \;   \delta(\xi'-\xi)=1$$ Do either of these permit an integral representation like the form of $(1)$? $^*$I believe this is what is indicated in this answer on PSE.","['dirac-delta', 'differential-geometry', 'fourier-transform']"
2755890,Equality at equilibrium of a ODE system arising in immunology,"Consider the following system of ODE's [1] :
\begin{align}
\dot x & = \lambda - d_1 x - \frac{k_1 x v}{x + v}\\
\dot y & = \frac{k_1 x v}{x + v} - d_2 y - k_2 y \\
\dot s &= k_2 y - d_3s \\
\dot v &= as - d_4 v
\end{align}
where $x(t)$, $y(t)$, $s(t)$ and $v(t)$ represent the concentrations of
uninfected CD4+ T cells, infected CD4+ T cells in latent stage, productively infected CD4+ T cells and free virus (HIV), respectively. Under some condition, this system has unique strictly positive equilibrium $Q_2 = (x^*, y^*, s^*, v^*)$ that is locally asymptotically stable. Assume this condition. Then at $Q_2$, the system becomes:
\begin{align}
0 & = \lambda - d_1 x^* - \frac{k_1 x^* v^*}{x^* + v^*}\\
0 & = \frac{k_1 x^* v^*}{x^* + v^*} - d_2 y^* - k_2 y^* \\
0 &= k_2 y^* - d_3 s^* \\
0 &= a s^* - d_4 v^*
\end{align} My question is: what justified the following equality? (cf. left side of p. 99 in [1] )
$$ k_1 = \frac{(d_2 + k_2) y^* (x^* + v^*)}{x^* v^*} \times \left[\frac{(d_2 + k_2) y^* (x + v^*)}{k_1 x v^*} \right] = \frac{x^* (x + v^*)}{x (x^* + v^*)}$$ Specifically, how are the following true?
$$ \left[\frac{(d_2 + k_2) y^* (x + v^*)}{k_1 x v^*} \right] = 1 $$
and 
$$ \frac{(d_2 + k_2) y^* (x^* + v^*)}{x^* v^*} \times \left[\frac{(d_2 + k_2) y^* (x + v^*)}{k_1 x v^*} \right] = \frac{x^* (x + v^*)}{x (x^* + v^*)}$$
I understand that the equation $ 0 = \frac{k_1 x^* v^*}{x^* + v^*} - d_2 y^* - k_2 y^* $ is the key, but I cannot figure out how the above two equalities are true. [1] Q. Sun, L. Min, Y. Kuang: ""Global stability of infection-free state and endemic infection state of a modified human immunodeficiency virus infection model"", IET Syst. Biol. 9 -3 (2015), 95–103 doi:10.1049/iet-syb.2014.0046","['mathematical-modeling', 'biology', 'ordinary-differential-equations', 'dynamical-systems']"
2755904,"How to approximate the sum of a convergent, positive series","$$S=\sum_{n=0}^{\infty}\frac1{e^n(n^2+1)}$$ This series converges because the general term goes to $0$ faster than $\dfrac1{n^2}$ I am asked to approximate the series with an error $R<10^{-3}$. How can I do this? I know that $$R=\left\lvert S-S_k\right\rvert=\left\lvert \sum_{n=0}^{\infty}\frac1{e^n(n^2+1)}-\sum_{n=0}^{k}\frac1{e^n(n^2+1)}\right\rvert=\sum_{n=k+1}^{\infty}\frac1{e^n(n^2+1)}$$ and that $$\int_{k+1}^\infty \frac1{e^x(x^2+1)}\mathrm dx\le\sum_{n=k+1}^{\infty}\frac1{e^n(n^2+1)}\leq\int_{k}^\infty \frac1{e^x(x^2+1)}\mathrm dx$$ My attempt: $$R=\sum_{n=k+1}^{\infty}\frac1{e^n(n^2+1)}\le\frac1{(k+1)^2+1}\sum_{n=k+1}^{\infty}\frac1{e^n}\underbrace\le_{(1)}\frac1{(k+1)^2+1}\int_k^\infty \frac1{e^x}\mathrm dx= \frac1{(k+1)^2+1}\frac1{e^k}<10^{-3}\quad (*)$$ $(1):$ as the series is decreasing, positive and continuous, I can switch to the integral. $(*)$ is true for $k=4$. Is this a right approach? P.S. The exercise comes from Calculus Problems , 16.18, page 231.","['sequences-and-series', 'calculus', 'approximation']"
2755933,Complex roots of $z^{10} - 6z^9 + 6^9$ inside the disk of radius six,Show that there are exactly $8$ roots of the polynomial $z^{10} - 6z^9 + 6^9$ inside the disk of radius $6$ centered at the origin in the complex plane. I am trying to use Rouche's theorem but am unable to find a suitable other function.,"['complex-analysis', 'rouches-theorem']"
2755935,Proving a property with logarithm,"For a polynomial $V(x)$ of degree $r$ defined in $\mathbb{R}^d$, we denote $$A_V(x)=\sum_{1\le |\alpha|\le r}|\partial^{\alpha}_xV(x)|^{\frac{1}{|\alpha|}}$$ I want to prove the following result: there exists a constant $c>0$ such that for all $x,x_0\in \mathbb{R}^d$ $$
|x-x_0|\le \Bigl(\frac{A_V(x)}{ \log(A_V(x))}\Bigr)^{-1}$$
implies
$$
\left(\frac{\dfrac{A_V(x)}{\log(A_V(x))}}{\dfrac{A_V(x_0)}{ \log(A_V(x_0))}}\right)^{\pm 1}\le c.
$$ Here we consider polynomials $V$ such that $A_V(x)\neq 1$ for all $x\in \mathbb{R}^d$. Please help me to do so. Thanks in advance","['multivariable-calculus', 'inequality']"
2755954,How many ways we can choose three points in a $n\cdot n$ grid?,"We can calculate the number of square or rectangle in a $n\cdot n$ grid. No of squares $=1^2+2^2+3^2+.....+(n-1)^2$ No of rectangles $=1^3+2^3+3^3+.....+(n-1)^3$ So what if we want to calculate the no of all possible quadrilateral?
We can choose $4$ points out of $n^2$ points.But in that case, there will be many instances where $3$ or more points will be co-linear.So these will not be a true quadrilateral. So I need to find those combinations of $3$ or more points being co-linear? References : How many squares are in the chessboard? Analysis of how-many-squares and rectangles are are there on a chess board?","['combinatorics', 'geometry']"
2755955,Weak convergence of linear combinations of Dirac measures to a signed measure,"For a signed measure $\mu$ on the Borel $\sigma$-algebra of $\mathbb{R}$ satisfying $\vert \mu \vert < \infty$ is it always possible to find a sequence of measures $\{\mu_n\}$, each a linear combination of Dirac measures, converging weakly to $\mu$? By weakly I mean $$\int f(x) \mu_n(\text{d} x) \rightarrow  \int f(x) \mu(\text{d} x)$$ as $n \rightarrow \infty$ for every bounded, continuous function $f$. If not, does it help to restrict the assumptions, e.g. to positive measures?","['functional-analysis', 'probability-theory', 'probability', 'measure-theory']"
2755963,"Finite dimensional sets in C[0,1]","If we look at the space $C[0,1]$ of continuous functions on $[0,1]$ we can define  $\mathcal{C}:=\{\pi_{t_1,\ldots, t_d}^{-1}(B) : {t_1,\ldots, t_d} \in [0,1]$ and $B \in \mathcal{B}(\mathbb{R}^d)\}$, where $\pi_{t_1,\ldots, t_d}:C[0,1]\to \mathbb{R}^d$. On the other hand we can tread $C[0,1]$ as a subspace of $\mathbb{R}^{[0,1]}$. On this space we can also define the finite-dimensional sets as $\mathcal{H}:=\{\tilde{\pi}_{t_1,\ldots, t_d}^{-1}(B) : {t_1,\ldots, t_d} \in [0,1]$ and $B \in \mathcal{B}(\mathbb{R}^d)\}$, where $\tilde{\pi}_{t_1,\ldots, t_d}:\mathbb{R}^{[0,1]}\to \mathbb{R}^d$. We can then define the ""trace"" collection as $\mathcal{H}_C=\{H\cap C[0,1]:H\in\mathcal{H}\}$. Are the collections $\mathcal{C}$ and $\mathcal{H}_{C}$ the same, i.e. do we have $\mathcal{C}=\mathcal{H}_{C}$? I know that they both generate the same $\sigma$-algebra, but I am not sure if this equality holds.","['functional-analysis', 'probability-theory', 'measure-theory', 'stochastic-processes']"
2755987,Eigenvalues for $4\times 4$ matrix,"Show that $0,2,4$ are the eigenvalues for the matrix $A$: 
$$A=\pmatrix{
2  &  -1 & -1 &  0  \\
-1  & 3 &  -1 &  -1  \\
-1  &  -1 &  3 & -1  \\
0  &  -1 & -1 &  2  \\
}$$
and conclude that $0,2,4$ are the only eigenvalues for $A$. I know that you can find the eigenvalues by finding the $\det(A-\lambda \cdot I)$, but it seems to me that the computation will be rather difficult to compute as it is a $4 \times 4$ matrix. My question: is there an easier method to calculate the eigenvalues of $A$? 
And if I have to conclude that these are the only eigenvalues, is there a theorem that argues how many eigenvalues a matrix can have?","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2756007,Connected locally homogenous space which is not globally homogenous?,"I'm using following definitions: Definition 1. A topological space $X$ is (globally) homogenous if for any two points $x,y\in X$ there exists a homeomorphism $f:X\to X$ such that $f(x)=y$. and the local version: Definition 2. A topological space $X$ is locally homogenous if for any two points $x,y\in X$ and any two open neighbourhoods $U_x, U_y$ of $x,y$ respectively there are open sub-neighbourhoods $x\in V_x\subseteq U_x$, $y\in V_y\subseteq U_y$ such that there is a homeomorphism $f:V_x\to V_y$ such that $f(x)=y$. Obviously every manifold is locally homogenous. Also I've done some research and actually connected manifolds are additionally globally homogenous. Now I'm quite sure that the global version implies the local one. I'm looking for a counterexample that the local one implies global. One obvious counterexample is the disjoint union $X=S^1\sqcup\mathbb{R}$. However this space is not connected. And the connectedness plays crucial role in showing that $X$ is not globally homogenous (otherwise we would get a homeomorphism $S^1\to\mathbb{R}$). I'm having trouble in finding a connected counterexample. Or perhaps there is none? Unlikely. Any help?","['homogeneous-spaces', 'general-topology']"
2756018,Motivation behind Primary Decomposition,"I am reading Atiyah's Commutative Algebra chapter on Primary Decomposition. I understand the proofs but I have no intuition as to How did one come up with definition of a primary ideal $q$ as $xy \in q \Rightarrow$ either $x \in q$ or $y^n \in q$ for some $n>0$ Why we know that this decomposition is a ""nice"" one. Perhaps there are other decompositions, that satisfies some uniqueness properties. What is so special/""canonical"" about this hecomposition? What properties of primary decomposition are ""important"" - what are the consequences? I am confused as to what properties yields more important results. I have skipped ahead and look at some corollaries; some results can also be proven  with out the theory primary decomposition. What I am look here is a more detailed motivation/intuition. Rather than ""prime decomposition in the ideals"". Feel free to close this post if it is too broad.","['big-picture', 'algebraic-geometry', 'commutative-algebra']"
2756031,How prove $\cos(\frac{2\pi}{17}) + \cos(\frac{18\pi}{17})+\cos(\frac{26\pi}{17})+\cos(\frac{30\pi}{17}) = \frac{\sqrt{17}-1}{4}$,"Prove that $\cos(\frac{2\pi}{17}) + \cos(\frac{18\pi}{17})+\cos(\frac{26\pi}{17})+\cos(\frac{30\pi}{17}) = \frac{\sqrt{17}-1}{4}$ Regards that value of $\cos(2\pi/17)$ , I can't find the easy way to solve that expression. Even if I had time, I wouldn't try that method to find the all roots others cosines expressions. IMHO",['trigonometry']
2756032,Use Baire's Category Theorem to show continuity point of a function,"Here is the problem. I want to contruct a collection of sets so that I can use Baire's Category Theorem but no thoughts came into mind. Let $f:\left(0,1\right)\times\left(0,1\right)\rightarrow\mathbb{R}$ be
      a real value function. Assume that for every fixed $x$ (respectively
      for every fixed $y$) the function $f\left(x,y\right)$ is continuous
      in $y$ (respectively in $x$). Prove that there exists a point $\left(x_{0},y_{0}\right)$
      where $f$ is continuous. (Hint: use Baire's category theorem) Any help is appreciated!","['functional-analysis', 'analysis']"
2756034,Differential equation using Laplace transform struck on inverse Laplace,"I was solving this differential equation 
$$ty''+2y'+ty=\cos{t}$$
with initial condition $y(0)=1$.
I took Laplace on both sides and after simplifying got this differential equation in terms of $Y(s)$
$$(s^2-2)Y'(s)+4sY(s)=\frac{s}{s^2+1}+3$$
Solving this I got $$Y(s)=\frac{s^2- 3\log{(s^2+1)}+2s^3-12s}{2(s^2-2)^2}$$
What is the Laplace inverse of $Y(s)$?","['inverse-laplace', 'ordinary-differential-equations']"
2756038,Convergence of $\sum_{n=1}^{\infty} \frac{\sin(n!)}{n}$,"Is there a way to assess the convergence of the following series?
$$\sum_{n=1}^{\infty} \frac{\sin(n!)}{n}$$
From numerical estimations it seems to be convergent but I don't know how to prove it.","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
2756040,Qualitative solutions of KDV,"I'm reading Solitons: An introduction [1] , and in the chapter two (§2.2 p. 21), the author make a qualitative analysis of KDV: $u_t -6uu_x + u_{xxx} = 0$, then he try find solutions on this form $u(x,t)=f(x-ct)$, and then make the following substitution $\xi =x-ct$ and then we have: $-cf'-6ff'+f'''=0$, integrating twice we have: $$\frac{1}{2}(f')^2 = f^3 + \frac{1}{2}c f^2 + Af +B \equiv F(f)$$ where $A$ and $B$ are constants. So I don't understand this step (cf. book [1] p. 23). He wrote the Taylor series for $F(f)$ around $f_1$, where $f(\xi_1) = f_1$ and $F(f_1) = 0$ $$(f')^2 = 2(f-f_1)F'(f_1) + O((f-f_1)^2)) \tag{1}$$ And then: $$f = f_1 + \frac{1}{2}(\xi - \xi_1)^2F'(f_1) + O((\xi - \xi_1)^4) \tag{2}$$ I did not understand what he did in p. 23 from $(\text{1})$ to $(\text{2})$, someone can explain to me? [1] P.G. Drazin, R.S. Johnson, Solitons: An Introduction , Cambridge University Press, 1989","['soliton-theory', 'ordinary-differential-equations', 'analysis', 'partial-differential-equations']"
2756048,Sufficient condition for graph isomorphism assuming same degree sequence,"We assume graph to be simple undirected.
In general, having the same degree sequence is not sufficient for two graphs to be isomorphic. A trivial example is a hexagon which is connected and two separated triangles, which is obviously not connected, yet their degree sequences are the same. Can we also exhibit counter examples with two non-isomorphic connected graphs having the same degree sequence? What about two such Euler graphs? Is it known for which extra conditions having the same degree sequence becomes sufficient for isomorphism?","['eulerian-path', 'combinatorics', 'graph-theory', 'graph-isomorphism']"
2756057,What is the parametric form of the unique line which crosses these other three lines?,"In the real affine space $\Bbb A^4$ let $A=(1,2,1,0), A'=(1,2,2,-1), B=(1,0,0,0), B'=(2,0,0,0), C=(2,1,1,0), C'=(-2,1,-1,0).$ Now let $a$ be the line that passes through $A$ and $A'$ , $b$ through $B$ and $B'$ , $c$ through $C$ and $C'$ . What is the parametric form of the unique line which crosses $a, b, c$ ? This is an exercise of which I was supposed to find the solution on my professor's webpage, as an example for others, but there was no solution actually. I've been trying with what I know from the book but I didn't quite get it, and I can't reach my professor right now... could you help me?","['affine-geometry', 'linear-algebra', 'geometry']"
2756068,How can I determine the type of bifurcation I have found?,"I was tasked with finding and classifying the bifurcation of the following system: $$\frac{dx}{dt} = rx(x+1)-1$$ I have found that the bifurcation occurs when $r=1$, with equilibrium points at $x=0$ and $x=\frac{1}{r}-1$, but I am not sure how to determine the type of bifurcation it is. I have created a plot of the equilibrium points against r, but it looks nothing like any other plots I have seen that give the type of bifurcation. Is there a way to find the type of bifurcation it is analytically, or how can I otherwise determine the type of the bifurcation?","['bifurcation', 'ordinary-differential-equations', 'dynamical-systems']"
2756082,Space of principal connections is affine modelled on $\Lambda^1(M;\mathfrak{g})$?,"I'm working within the jet-formulation espoused by Saunders in ""The Geometry of Jet Bundles"" and am struggling to prove the stated result. I would like to stay in this context and understand the result in the following terms. The context is this. Given a principal $G$-bundle $\pi:P\rightarrow M$ over a manifold $M$ for some Lie group $G$, the first jet prolongation of $\pi$ is the manifold $J^1P$, whose points are equivalence classes $j^1_ps$ of local sections $s$ defined in some neighbourhood $U$ of $p$. Two local sections $s$, $t$ at $p$ are equivalent if both conditions $$s(p)=t(p),\qquad \frac{\partial s^a}{\partial x^i}=\frac{\partial t^a}{\partial x^i}$$ hold for some fibred cordinates $(x^i,u^a)$ at $s(p)$. The first jet manifold fibres over $P$, with the projection $\pi_{1,0}:J^1P\rightarrow P$, $j^1_ps\mapsto s(p)$. The right $G$-action on $P$ extends to one on $J^1P$, given explicitly by $(j^1ps)\cdot g=j^1_p(s\cdot g)$, and defined thus the map $\pi_{1,0}$ is $G$-equivariant. There is a contact map $\lambda:J^1P\rightarrow Hom(\pi^*TM,TP)$ as well as a complementary map $\theta:J^1P\rightarrow Hom(TP,VP)$. These are bundle maps over $P$. They are embeddings and enjoy certain equivariance properties with respect to the $G$-actions on all spaces. Explicitly they are defined by $$\lambda(j^1_ps)=T_ps,\qquad \theta(j^1_ps)=1-T_ps\circ T_{s(p)}\pi.$$ In this context a connection on $\pi$ is a section of $\pi_{1,0}$, given by a map $\omega:P\rightarrow J^1P$. A connection $\omega$ is said to be principal if it is equivariant with respect to the right $G$-action. The standard result is that the ""difference"" of two connections descends to give a well-defined 1-form on $M$ with values in the Lie algebra $\mathfrak{g}$. This statement should be understood in terms of (one of) the embeddings $\lambda$, $\theta$. However I cannot prove it. My attempt below yields an $ad_g$-dependence on my choice of lifting vector fields of which I cannot rid myself. I proceed as follows. I assume given two principal connections $\omega_1,\omega_2:P\rightarrow J^1P$. At an arbitrary point $p_0\in M$ I choose a local section $s$ defined in some neighbourhood $U$ of $p_0$ and define a $\mathfrak{g}$-valued 1-form $A^s_U$ on $U$ by setting $$A^s_U(X)(p)=T_{s(p)}\hat s\circ(\theta\circ\omega_2(s(p))-\theta\circ\omega_1(s(p)))\circ T_ps(X_p)$$ where $X\in\mathcal{X}(U)$ is a vector field on $U$ and $p\in U$. Here $\hat s:P|_U\rightarrow G$ is the local trivialisation of $P$ determined by $s$. That is, for $e\in P|_U$, it is defined by $e=s(\pi(p))\cdot \hat s(e)$. The tangent map $T_ps$ lifts $X$ to $TP$. Then each $\theta(\omega_i(s(p))\in Hom(T_{s(p)}P,V_{s(p)}P)$ sends it to the vertical subspace. Finally $\hat s(s(p))=1_G$, so the tangent map $T_{s(p)}\hat s$ takes values in $T_1G\cong\mathfrak{g}$. I need to show that the choice of local section $s$ is of no consequence, so that $A_U^s=A_U$. This will then allow for the various $A_U$'s to be patched together as $U$ ranges over an open cover of $M$. My difficulty is that if I choose a different section $t:U\rightarrow P|_U$ (defined without loss of generality on the same neighbourhood of $p_0$) then over $U$ it is related to $s$ by $t(p)=s(p)\cdot f(p)$, for some map $f:U\rightarrow G$, and I get $$A^t_U=ad_{f(p)^{-1}}\circ A^s_U$$ where $ad_{f(p)^{-1}}:\mathfrak{g}\rightarrow \mathfrak{g}$ is the adjoint action of $G$ on its Lie algebra. I can't rid myself of this conjugation. Where am I going wrong?","['jet-bundles', 'principal-bundles', 'differential-geometry', 'manifolds', 'connections']"
2756086,Compare two functions in neighborhood of zero,"Basically, I have two functions:
$$
f(x, y) = \sqrt{(x-y^2)^2 + x^4}
$$
and 
$$
g(x, y) = | (x-y^2)^3 |
$$
I need to compare them in a punctured neighborhood of zero. I am a bit stuck here. Edit: compare in sense that there exists a punctured neighborhood of zero where for all $x$ and $y$ from that neighborhood one function is greater than the other. I.e. I want to prove that
$$
\exists O(0,0):\quad \forall x, y \in O(0,0) \quad |f(x, y)| > |g(x,y)|
$$",['multivariable-calculus']
2756119,"$\lim_{n\to\infty}\frac{n -\big\lfloor\frac{n}{2}\big\rfloor+\big\lfloor\frac{n}{3}\big\rfloor-\dots}{n}$, a Brilliant problem","I encounter a question when visiting Brilliant : Find $\space\space\space\space\lim_{n\to\infty}s_n$ $=\lim_{n\to\infty}\frac{n - \big \lfloor \frac{n}{2} \big \rfloor+ \big \lfloor \frac{n}{3} \big \rfloor - \big \lfloor \frac{n}{4} \big \rfloor + \dots}{n}$ $=\lim_{n\to\infty}\frac{\sum_{k=1}^n(-1)^{k+1}\lfloor\frac{n}{k}\rfloor}{n}$ The answer in the website above doesn't really satisfies me, as the answer does not tell how the sequence converge and I doesn't understand how we can take subsequence $n_k=k!$ to solve the problem. I had some idea but doesn't seems to work: 1) It is easy to show $$s_n=\frac{\sum_{k=1}^{\big\lceil\frac{n}{2}\big\rceil}(\big\lfloor\frac{n}{2k-1}\big\rfloor-\big\lfloor\frac{n}{2k}\big\rfloor)}{n}$$ 2) On the other hand, $$s_n\approx\sum_{k=1}^n(-1)^{k+1}\frac{1}{k}\to\ln2$$ So I am wondering how $s_n\approx$ the alternate hamonic series
$$\forall(n,k\in\mathbb N:n\ge k),\space\space\frac{n}{k}\in\Bigg[\bigg\lfloor\frac{n}{k}\bigg\rfloor,\bigg\lfloor\frac{n}{k}\bigg\rfloor+\bigg(\frac{k-1}{k}\bigg)\Bigg]$$ I tried to look at the graph , the sequence $s_n$ is very likely to converge to $\ln 2$, and the alternating harmonic series seems to be bounded by the graph of $s_n$ at most of the time. 3) Also I observed that $s_8=\frac{8-4+2-2+1-1+1-1}{8}=\frac{1}{2}$, the terms cancelled nicely, but I am afraid that the anlalogue is not generaly true for all $s_{2^k}$. 4) I have tried to use Stolz–Cesàro Theorem , but doesn't seems useful neither. 5) I know that $\forall x,y\in\mathbb R:x+y\in\mathbb Z, \lfloor x\rfloor+\lceil y\rceil=x+y$, which maybe is useful since we may thus write $s_n$ in a more beautiful manner? 6) If there is no $(-1)^{k+1}$, I think we can treat $s_n$ as a Riemann sum, but well, ... , seems useless. 7) I have tried to think about how many terms of summand of $ns_n$ is integer. 8) I have tried to think $\big\lfloor\frac{n}{k}\big\rfloor$ as the number of positive integer multiple of $k$ that $\lt n$, and I then considered sets of number that is counted and uncounted respectively, but well, the question doesn't seems that easy. Does this help? (1) (2) (3) Any help will be appreciate. Thank you! Remarks: I was wondering is there a deep subject studying this (if so references please). Can this (or variants) be represented as a simpler function?","['real-analysis', 'ceiling-and-floor-functions', 'sequences-and-series', 'limits']"
2756209,"Without using the Rule of Sarrus, prove that:","Without using the Rule of Sarrus, prove that:
$$\left|
\begin{matrix}
(b+c)&(a-b)&a \\
(c+a)&(b-c)&b \\
(a+b)&(c-a)&c \\
\end{matrix}\right|=3abc-a^3-b^3-c^3$$ My Approach:
$$LHS=
 \left|
\begin{matrix}
(b+c)&(a-b)&a \\
(c+a)&(b-c)&b \\
(a+b)&(c-a)&c \\
\end{matrix}\right|$$
$$C_1\to C_1+C_2$$
$$=
\left|
\begin{matrix}
(c+a)&(a-b)&a \\
(a+b)&(b-c)&b \\
(b+c)&(c-a)&c \\
\end{matrix}\right|$$
$$C_1\to C_1-C_3$$
$$=
\left|
\begin{matrix}
c&(a-b)&a \\
a&(b-c)&b \\
b&(c-a)&c \\
\end{matrix}\right|$$ How do I complete the rest?","['matrices', 'linear-algebra', 'determinant']"
2756301,"Computing $h^0(\mathbb{F}_n, \mathcal{O}_{\mathbb{F}_n}(aC_0+bF))$ in a Hirzebruch surface","Let $\mathbb{F}_n=\mathbb{P}(\mathcal{O}_{\mathbb{P}^1}\oplus\mathcal{O}_{\mathbb{P}^1}(-n))$, $n\neq 1$ be a Hirzebruch surface. There exists a section $C_0$ of the natural projection to $\mathbb{P}^1$ such that $C_0^2=-n$. On the other hand, we call $F$ to a fiber of this projection. We have that:
$$
\mathrm{Pic}(\mathbb{F}_n)=\mathbb{Z}\langle C_0\rangle\oplus\mathbb{Z}\langle F\rangle.
$$
In other words, any divisor is linearly equivalent to $aC_0+bF$ for some integers $a,b$. My question is, can we compute $h^0(\mathbb{F}_n, \mathcal{O}_{\mathbb{F}_n}(aC_0+bF))$?","['divisors-algebraic-geometry', 'sheaf-cohomology', 'algebraic-geometry']"
2756314,Cauchy Remainder = Lagrange Remainder,I was given a problem like this: Suppose $f$ has derivatives of all orders and all continuous. Suppose the sequence $\{a_n\}$ is bounded and $|f^{(n)}(x)| \le a_n$ for all $x$. Use the Cauchy Remainder Formula to show that $f$ equals its Taylor Expansion. So far I have been covered Lagrange Remainder and Cauchy Remainder Theorem. My intuition is showing the remainder part $R_n(x)$ of Cauchy equal $R_n(x)$ from Lagrange. But I don't know how to apply the info that $|f^{(n)}(x)| \le a_n$ for all $x$. Can someone please help with this? Thanks,"['derivatives', 'calculus']"
2756332,Uniqueness of trace as linearization of the rank,"It is not difficult to show that if $A \in M_n(k)$ for some field $k$ , and $A^2=A$ then $$\operatorname{tr}(A) = \dim(\operatorname{Im}(A))$$ In this comment , Terry Tao wrote: This property, together with linearity, determines the trace uniquely, and so one can view the trace as the linearised version of the dimension-counting operator. What does it mean precisely? If $f : M_n(k) \to k$ is $k$ -linear (say with $k = \Bbb C$ ), and $f(A) = \dim(\operatorname{Im}(A))$ for any $A^2= A \in M_n(k)$ , then we have that $f$ is the trace function?","['projection-matrices', 'matrices', 'trace', 'matrix-rank', 'linear-algebra']"
2756346,Lifts of flows to the universal cover,"We supposed $M$ is a compact ( i.e, closed without boundary) manifold. Let $\phi_{t}: M \mapsto M$ be a one-parameter family of flow and $p: \widetilde{M} \mapsto M$ be the universal cover of $M$. Let $X$ be the vector field associated to the flow $\phi_{t}$ on $M$. Since the universal covering map is a local diffeomorphism, we can take the pullback of the associated vector field to the universal cover, which will give us a new flow $\widetilde{\phi_{t}}$ on $\widetilde{M}$. We call it a lift of the flow. I wanted to see why can we extend this lifted flow for all time $t$ in the universal cover, as it's no longer compact. Also is it true that the flow will commute with the monodromy action of $\pi_{1}(M)$ on $\widetilde{M}$. That is to say for $\tilde{x}$ $\in$ $\widetilde{M}$, is it true that $\gamma . \widetilde{\phi_{t}}(\tilde{x})= \widetilde{\phi_{t}}(\gamma . \tilde{x}) $, where $\gamma$ is an element $\pi_{1}(M)$. 
Thanks in advance.","['vector-fields', 'dynamical-systems', 'differential-topology', 'geometric-topology', 'differential-geometry']"
2756351,Limits and adherent value of sequences in $\mathbb{R}^2$,"I have this topology on $\mathbb{R}^2$: $$ \sigma=\{\mathbb{R}^2,\emptyset, (B_r)_{r>0}\}$$ where $$B_r=\{(x,y)\in\mathbb{R}^2; (x-3)^2+y^2<r^2\}.$$ The question is to study the nature of the sequence $u_n=(2,\frac1n)$ and to find the adherent value of $v_n=(\frac1n,1)$. Concerning $u_n$, I found that it is convergent to every $(x,y)$ such that $$(x-3)^2+y^2\geq1 $$. We say that $l$ is an adherent value for $v_n$ iff $\forall V\in \mathcal{V}_l, \rm card\{n\in\mathbb{N}, v_n\in V\}=+\infty$. I found that $(x,y)$ is an adherent value of $(v_n)$ iff  $(x-3)^2+y^2\geq 10$. Is it correct or are there other limits or adherent values? Thank you.","['general-topology', 'real-analysis']"
2756353,Understanding groups that have a similar presentation as $S_n$,"It is well known that the symmetric group $S_n$ can re presented using the generatos $\sigma_1, \dots, \sigma_{n-1}$ subject to the relations:
$$\sigma_i^2=1,\\
\left[\sigma_i ,\sigma_j\right] = e  \quad (|i-j| \neq 1), \\ \langle\sigma_i,\sigma_{i+1} \rangle = e. $$
Where $\langle a,b\rangle$ is the triple relation between $a$ and $b$: 
$$aba = bab.$$ I am interested in groups that have a very similar presntation, but slightly different: One (and only one) of the commutators is replaced with a triple relation involving the same generators (e.g. $[\sigma_1,\sigma_3]=e$ is replaced with $\langle \sigma_1, \sigma_3 \rangle = e$). Such groups arise as the fundumental group of some geometric objects that I am interested in. Can something be said about these groups? In particular, I am interested in surjecting such groups on $S_n$ and finding the kernel of this surjection (note that the straightforward surejction does not work). What are the surjections from such groups onto $S_n$ and how can I find their Kernel?","['abstract-algebra', 'group-presentation', 'group-theory', 'symmetric-groups']"
2756359,Understanding solution to $y' = y$ and exponential distribution,"My Understanding: I would derive the exponential random variable as follows: I consider an experiment which consists of a continuum of trials on an interval $[0,t)$. The result of the experiment takes the form of an ordered $n$-tuple $\forall n \in \mathbb{N}$ containing distinct points on the interval. Every outcome is equally likely and I measure the size of the set containing tuples of $n$ different points by $I_n$ as: $$ I_n = \int_0^{t}  \int_0^{x_{n}} \int_0^{x_{n-1}} \cdots \int_0^{ x_2 } dx_1 dx_{2} dx_{3} \dots dx_{n-1} dx_{n} = \frac{ t^n } { n! }$$ Since the size of the set where no success occurs, $I_0 = 1$, the ratio of these sets gives the probability that no event occurs and its complement yields the c.d.f: $$
\begin{align}
P(\text{no successful trial in } [0,t)) &= \big(\sum_{n = 0}^{\infty} I_n(t)\big)^{-1} = e^{-t} \\
P(X \leq t) &= 1 - e^{-t} \\
\end{align}
$$ Where I Lose Intuition: It's easy to arrive at a power series solution to the ODE: $$y' = y \text{ with } y'(0) = 1$$
$$ \boxed{ y = \sum_{n = 0}^{\infty} \frac{ t^n }{ n ! } = e^{t}} $$ My problem is that I do not understand the role of each term in the expansion. Substitution by power series is an attractive idea, but I have no deep intuition as to why we'd do this and hence, I'm having trouble putting it all together to understand the solution. Question How do I interpret the power series solution of the ODE? Hopefully this will allow me to reconcile my understanding of both these processes. I am not willing to accept this as pure coincidence .","['intuition', 'exponential-function', 'ordinary-differential-equations', 'probability']"
2756366,Proving that an element of a given group has an infinite order,"I am given the following group:
$$G = \langle x_1,x_2,x_3 | x_1^2 = x_2^2 = x_3^2 = e, \langle x_1, x_2 \rangle = \langle x_2, x_3 \rangle = e  \rangle,$$
where $\langle a,b \rangle = e$ is the triple relation, meaning that
$$aba = bab.$$ I want to prove that the element $x_1 x_3 x_1 x_2$ has infinite order. This seems to me rather trivial, since (intuitively speaking) the triple relations in $G$ cannot reduce the number of generators in $(x_1 x_3 x_1 x_2)^n$. However, when trying to prove this formally, I could not finish the proof. I tried using induction, playing with the evenness of the number of the generators $x_1, x_2, x_3$ that appear in any power of this element, but without success.","['group-theory', 'group-presentation']"
2756379,Use induction to show that a polynomial with $k$ terms has $k$ solutions.,"First Part of the problem: Let $n_1 < . . . < n_k$ be non-negative integers, let $a_1, . . . , a_k$ be positive real numbers and let $f(x) = a_1x^
{n_1} + · · · + a_kx^{n_k}$ . Suppose $0 < s < t$ and $f(s) = f(t) = 0$. Show that the polynomial $g(x) = a_1 + a_2x^{n_2−n_1} + · · · + a_kx^{n_k−n_1} = \frac{1}{x^{n_1}}f(x)$ satisfies $g'(x) = 0$ for some $x$ between $s$ and $t$. (Hint: Use Rolle’s Theorem). My proof: We know that polynomials are continuous and differentiable, $\therefore$ we use Rolle's Theorem, since $f(s) = f(t)$, $\exists c \in (s,t)$ with $f'(c) = 0$. Now we take the derivative of $g(x)$ using the product rule: $g'(x) = (\frac{1}{x^n})'f(x) + (\frac{1}{x^n})f'(x)=$ $-\frac{n}{x^{-(1+n_1)}} + (\frac{1}{x^n})f'(x)$ From Rolle's theorem we know there is an x $\in (s,t)$ such that $f'(x) = 0$ and if $n_1=0$ then $g'(x) = 0$ for that same $x$. Second Part of the problem:  Use induction on $k$ and Question 7 to show that the polynomial $f(x) = a_1x^{n_1} + · · · + a_kx^{n_k}$ has at most $k$ positive solutions to $f(x) = 0$. My attempt: Quite honestly I'm not sure how to start. I know I can use the base case $k = 1$ (and there is only one solution to that: $x = 0$), so I can assume for $k$ terms there are $k$ solutions. But I don't know where to go from there. Do I just keep taking derivatives and prove $k-1$? How can I use induction to prove $k+1$?","['derivatives', 'induction', 'real-analysis', 'rolles-theorem']"
2756426,"Law of large numbers with one dependency: $\frac{1}{n}\sum_{i=1}^n g(X,Y_i)$","Let $\{Y_i\}_{i=1}^{\infty}$ be a sequence of independent and identically distributed (i.i.d.) random variables.  Let $X$ be another random variable (possibly dependent on $\{Y_i\}_{i=1}^{\infty}$).  Let $g:\mathbb{R}^2\rightarrow\mathbb{R}$ be a measurable function. Assume that $E[g(x,Y_1)]$ is well defined and finite for all $x \in \mathbb{R}$. I would like to know when there 
exists a deterministic function $f$ such that $$ \lim_{n\rightarrow\infty} \frac{1}{n}\sum_{i=1}^n g(X, Y_i) = f(X) \quad \mbox{with prob 1} $$
I am particularly interested in the candidate function $f(x) = E[g(x, Y_1)]$. I have been able to prove the result for the case when $X$ is discrete (see below). I am interested in results and/or counter-examples for more general cases. Context: This question is a refinement of the question here: Strong law of large numbers for function of random vector: can we apply it for a component only? In that link, I was able to prove the result always holds for $f(x)=E[g(x,Y_1)]$ when  $X$ takes values in a finite or countably infinite set.","['stochastic-processes', 'probability-theory']"
2756458,"Does all total sets have a ""stable"" property?","In Kreyszig's Introductory Functional Analysis with Applications , a total set in a normed space $X$ is defined as a subset $M \subset X$ whose span is dense in $X$. That is,
$\overline{\text{span } M} = X$. Let $X$ be a separable normed space, and $M = \{f_1, f_2, \dotsc\}$ be a countable total set in $X$. Prove or disprove the following: For each $x \in X$, there exist coefficients $c_1, c_2, \dotsc$, such that $\|x - \sum_{k=1}^n c_k f_k\| \to 0$ as $n \to \infty$. Note that $c_k$ does not change for each $n$, which is what I try to imply by ""stable"". Note: The question may be badly posed in the sense that there may still be assumptions I need to make, otherwise there will be a trivial answer. It's a personal question rather than a homework question.","['functional-analysis', 'normed-spaces']"
2756497,Find $\lim_{n\to \infty}\{(1+\frac{1}{2n})\cdot (1+\frac{3}{2n}) \cdot (1+\frac{5}{2n})\cdots(1+\frac{2n-1}{2n})\}^{{1}/{2n}}$,"Using the AM-GM inequality we get $$\dfrac{1}{n}\sum_{i=1}^{n}\bigg(1+\dfrac{(2i-1)}{n}\bigg)>\prod_{i=1}^{n}\bigg(1+\dfrac{(2i-1)}{n}\bigg)^{{1}/{n}}\\
\implies \dfrac{n+\frac{1}{2n}(1+3+5+...(2n-1))}{n}>\prod_{i=1}^{n}\bigg(1+\dfrac{(2i-1)}{n}\bigg)^{{1}/{n}}\\
\implies \frac{n+\frac{1}{2n}\left[\frac{n}{2}\left[2\cdot 1+(n-1)2\right]\right]}{n}>\prod_{i=1}^{n}\bigg(1+\frac{(2i-1)}{n}\bigg)^{{1}/{n}}\\
\implies \dfrac{3}{2}>\prod_{i=1}^{n}\bigg(1+\dfrac{(2i-1)}{n}\bigg)^{{1}/{n}}\therefore \sqrt{\dfrac{3}{2}}>\prod_{i=1}^{n}\bigg(1+\dfrac{(2i-1)}{n}\bigg)^{{1}/{2n}}$$(By taking square root of both sides) Then what can we say about the $$\lim_{n\to \infty}\left\{\left(1+\dfrac{1}{2n}\right)\cdot \left(1+\dfrac{3}{2n}\right) \cdot \left(1+\dfrac{5}{2n}\right)\cdots\left(1+\dfrac{2n-1}{2n}\right)\right\}^{{1}/{2n}}$$ @Jair Taylor has mentioned the usage of Reimann sums, I am going to try it here. $$\lim_{n\to\infty}\sum_{i=1}^{n}\log\left(1+\dfrac{2i-1}{n}\right)\cdot \dfrac{1}{n}$$ $$\therefore a=1, \Delta x_i=\dfrac{1}{n} \therefore \dfrac{1}{n}=\dfrac{b-1}{n} \therefore b=2$$ Hence the integral becomes $\int_{1}^2(\log(?)dx$ What do I put here instead of  '?' or is the approach wrong? But again just see another thing $\int_{1}^2(x)dx=\dfrac{3}{2}$ and we already have $\sqrt{\dfrac{3}{2}}$ from our inequality!!!","['sequences-and-series', 'limits-without-lhopital']"
2756498,Green's functions and Fredholm equations,"If I have an ODE $Lf(x)=g(x)$, then I can write the solution as $f(x)=\int G(x,y)g(y)dy$ where $G(x,y)$ is the Green's function of $L$ (it is the kernel of the inverse of $L$). Here $g(x)$ is given and $f(x)$ is to be found. I also read about the Fredholm equation, which is of the same form, $f(x)=\int K(x,y)g(y)dy$, but where now $f$ is given and $g$ is to be found. My question is about the relation between Green's functions and Fredholm equations. I have read that using the Green's function we are ""reducing"" the ODE to a Fredholm equation. I don't understand this statement. They look more like complementary problems, with different origins and different applications. I would appreciate a broad overview of this. Which problem is more fundamental? What is going on here?","['real-analysis', 'ordinary-differential-equations']"
2756504,"**UNSOLVED** Find an integer $\geqslant2$ that is build up out of only $1$'s and $0$'s in base $1,\;\ldots,\;10$.","This riddle bothers me for a few weeks now and I'm starting to worry that I need some $p$ -adic Number theory to solve this. I solve most of the riddles in a day, but this one is just annoying to me. I was thinking of taking the number $10!$ . Any help is appreciated! Edit I'm looking for a number that is written as only a series of $1$ 's and $0$ 's which represent the number $\sum_{n=0}^{\infty}a_n*p^n$ with $a_n\in\{0,\ldots,p-1\}$ where $a_n$ is either $1$ or $0$ . Edit 2 Probably this is a not so well known problem, but people are working on it with no solution yet found for this case. They let a program search for such numbers and there were no matches so far. There is a solution in base $1,\;\ldots,\;5$ , namely $$82000 = 10100000001010000\ (2) = 11011111001\ (3) = 110001100\ (4) = 10111000\ (5).$$ This number fails in the original problem for $82000=1431344\ (6)$ . Originally checked to $2^{65520}$ (or about $3*10^{19723}$ ) on Nov 07 2008. Conjectured to be complete. a(4), if it exists, it is greater than $10^{15}$ . Apr 06 2012. In 2016, a Mathematician stated that it is plausible that there are no more such terms, but it is not proven. See for yourself: https://oeis.org/A146025 .","['number-theory', 'abstract-algebra', 'contest-math']"
2756510,Iterating $f(x) = P \bmod x$,"Take a prime number $P \ge 3$ and some integer $x \in [1, P-1]$. Let's consider a sequence of values $x = x_0, x_1, \ldots, x_k = 1$, where $x_{n+1} = P \bmod x_n$ and the first occurence of $1$ happens after $k$ iterations. Are there any good lower bounds for such $k$? Since $x_{n+1} < x_n$, it is obviously $O(x)$. Exactly $x$ iterations can be reached using prime of form $P = k x! - 1$. I tried some random values of $P$ up to $2^{25}$ and the maximum value of $k$ did not exceed $2 \log_2 P$. Can you prove it is always $O(\log P)$ or are there counterexamples?","['number-theory', 'modular-arithmetic']"
2756523,Constructive proof of a classical result,"In a document about constructive maths, I saw the following exercise : prove that $f^{-1}: \mathcal{P}(Y) \to \mathcal{P}(X)$ is injective if and only if $f$ is surjective. There is an easy constructive proof of the ""if $f$ is surjective part"". However the ""if $f^{-1}$ is injective"" part eludes me: Let $y\in Y$. Then $\{y\}\neq \emptyset$ so $f^{-1}(\{y\}) \neq f^{-1}(\emptyset)=\emptyset$. But how can I conclude from $A\neq \emptyset$ that there is $x\in A$ constructively ? If I'm not mistaken, you can't (in the topos $\mathbf{Set}^2$, $(1, 0)$ has no global element though it's not the initial object - I don't know much about topos theory yet so I can't formalize this but heuristically this seems to indicate that there is no proof of $A\neq \emptyset \vdash \exists x, x\in A$) So this route seems doomed. Obviously I can't use the contrapositive because I would only get ""If $f^{-1}$ is injective then $f$ is not not surjective"" (and I'm not even sure there's an easy constructive proof of the contrapositive) I'm afraid I'm missing something obvious, but I'm not used to constructive reasoning at all so it would be of great help ! And also, can my argument about $\mathbf{Set}^2$ be formalized ?","['topos-theory', 'logic', 'elementary-set-theory', 'constructive-mathematics']"
2756593,application for Integration of non-negative measurable functions,"For each $n = 1, 2, 3, . . .,$  let $f_n : \Bbb R → \Bbb R$  be the function
$f_n = \chi[n,n+1)− \chi[n+1,n+2)$; i.e., let $f_n(x)$ equal $+1$ when $x ∈ [n, n + 1)$, equal $−1$ when $x ∈ [n + 1, n + 2)$, and $0$ everywhere else. Show that $\int_\Bbb R \sum_{n=1}^\infty f_n \ne \sum_{n=1}^\infty\int_\Bbb R  f_n$ My work: Notice that $f_n+f_{n+1}= χ[n,n+1)− χ[n+2,n+3)$. This implies that $\sum_{n=1}^{\infty} f_n= \chi[1,2)$  because every other point the characteristic function become zero for all rest of the partial sum. Therefore,
$\int_\Bbb R \sum_{n=1}^\infty f_n =\int_\Bbb R \chi[1,2)=1 $  but $\sum_{n=1}^\infty\int_\Bbb R  f_n=0$. Thus we are done. 
Did I miss anything? I was wondering if you could help me to solve this problem. I appreciate your kind help. Thank you!","['real-analysis', 'lebesgue-integral', 'measure-theory', 'lebesgue-measure']"
2756600,Non-linear ODE $y y'' = A/x^2$ approximate solution,"I'm currently stuck on approximating a solution to a non-linear ODE. I've searched high and low on this site for general methods for approximating non-linear ODE's of the type $$y(x)y''(x) = \frac{A}{x^2}, x > 0, A \in \mathbb{R}$$ with no luck. If you have a link in mind to a previously asked question regarding this, I'd be glad to see it. It could be that there is something entirely trivial that I am missing in approximating a solution to this equation! My experience with non-linear ODEs is quite limited. So this question is about finding an approximate solution to the above-mentioned non-linear ODE with BCs $y(0)=\alpha, y'(\infty)=0$, $\alpha \in [0,\pi/2]$. Any ideas? I'd love to hear your thoughts. The above equation follows by letting $u(x)=xy(x)$ in the equation $$u'''=A/u^2$$ with BCs $u(0)=\varepsilon, u'(0)=\alpha, u''(\infty)=0$ with $\varepsilon \ll 1$ (a small number) and using that $y'(x)$ is small so that $u'=y+xy'=y$ and $u''=y',u'''=y''$.",['ordinary-differential-equations']
2756608,Show that $\int_0^{\pi/2}\frac{\sin x\cdot \cos x}{x+1}dx=\frac12(\frac12+\frac1{\pi +2}-A)$,"Show that $\int_{0}^{\pi/2}\frac{\sin x\cdot \cos x}{x+1}dx=\frac{1}{2}(\frac{1}{2}+\frac{1}{\pi +2}-A)$
  where $A=\int_0^\pi\frac{\cos x}{(x+2)^2}dx$. I tried using partial integration on the integral $\int_0^{\pi/2}\frac{\sin x\cdot \cos x}{x+1}dx$ but I am in no luck. Also tried using $\frac12A=\int_0^{\pi/2}\frac{\cos x}{(x+2)^2}dx$. But still something is going wrong . Please suggest. The best I could think of is : $\int {\dfrac{sin2x}{2x+2}dx}=-\dfrac{1}{2x+2}cos2x+\dfrac{1}{4}\int {\dfrac{cos2x}{(x+1)^2}dx}$ Notice carefully that $\int_0^\pi\frac{\cos x}{(x+2)^2}dx$ has the same structure as $\int {\dfrac{cos2x}{(x+1)^2}dx}$ P.S.  See I purposefully avoided the long details of my efforts as that would hamper the understandibility of the problem and may mislead the answerer as well.","['integration', 'definite-integrals']"
2756637,Does every sum-of-squares equation have a plane geometric interpretation?,"The 2.1.2 sum-of-squares [SOS] equation
$$a^2=b^2+c^2$$
can be thought of in the [regular XY] plane as a right-angled triangle. Question: Is there an analogous interpretation for every SOS equation? For example, does the 2.1.3 SOS equation
$$a^2=b^2+c^2+d^2$$
have an interpretation in plane geometry?","['diophantine-equations', 'sums-of-squares', 'euclidean-geometry', 'geometry', 'elementary-number-theory']"
2756644,"Why is this discrete subgroup of $PSL(2,\mathbb{C})$ not Kleinian?","Definition: For a subgroup $G$ of the group $PSL(2,\mathbb{C})$ acting on  $\mathbb{P}^1$, its domain of discontinuity is the set of all points, $z$, with the following properties: $1.$ The stabilizer $G_z$ of $z$ is finite. $2.$ $\exists U$, a neighbourhood of $z$ such that, $\space \space\space\space(a)\space $$\forall g\in G_z$, $gU=U$. $\space \space\space\space(b)\space $$\forall g\in G - G_z$, $gU \cap U=\emptyset$. Definition: A Kleinian group is defined to be a subgroup $G$ of $PSL(2,\mathbb{C})$ such that its domain of discontinuity is not empty. Question: Consider the subgroup $\{M \in PSL(2,\mathbb{C}) \mid  m_{ij}\in \mathbb{Z}[i], det {M}=1\}$. Why is this not Kleinian? Is this clear from the above definition or would I have to prove some special property of Kleinian groups that this so called 'Picard Modular Group' does not satisfy?","['riemann-surfaces', 'riemannian-geometry', 'modular-group', 'complex-analysis', 'geometric-topology']"
2756668,Is this $0v=0$ proof correct?,"I'm taking my first linear algebra course at university and recently I've been introduced to vector spaces. Now, the teacher has asked us to prove that $0v=0$ for any $v$ using only the definition of a vector space. I've came up with a proof, but I'm unsure whether it's correct or not, as I'm not familiar with proofs in Math. $$
\begin{align}(1+1)v &= 2v \\
(1+1)v-2v &= 2v-2v\\
[(1+1)-2]v &= 2(v-v) \\
0v &= 0
\end{align}
$$ If this is correct I can go on proving other properties of vector spaces. Any feedback appreciated! EDIT: Ok, taking into account José's answer, I came up with this one
$$
(1+0)v=v \\
1v+0v=v \\
-v+v+0v=-v+v \\
0v=0
$$","['linear-algebra', 'proof-verification', 'vector-spaces']"
2756673,Interactive web game for the word problem in groups?,"I remember when I was in grad school, there was this website with a cool interactive game, where you were given certain ""rules"" (like 'abac=1'), and then you had to reduce a given word to the identity using those rules.  I remember it kept track of how many moves you made, and told you if you did it in the optimal number of moves or not.  I don't remember if it explicitly said you were doing group theory or not.  I remember the NY Group Theory Seminar page had a link to it, but that site seems to be dead now. Anyone know what I'm talking about / have a link?","['combinatorial-group-theory', 'reference-request', 'group-theory']"
2756686,Solving $\cos(\pi(x-1))=0$,"I have a second derivative that I need to use to find inflection points to create a graph. The second derivative is $$f^{\prime\prime}(x)=-4\pi^2\cos(\pi(x-1))$$ So I set the equation to $0$ and solve for $x$ $$-4\pi^2\cos(\pi(x-1))=0$$ I divide by the constant $-4\pi^2$ and get $$\cos(\pi(x-1))=0$$ But I am basically stuck at this point. I know I need to take the inverse cosine of both sides. The result I am getting is $x=3/2$, but the answer in the book is $x=1/2$, $3/2$. Can someone help me figure out how to solve the last steps of this problem?","['derivatives', 'trigonometry']"
2756693,How to solve the ODE using Newton's method?,"Let consider on $[0,20]$, the ODE : $y'(t)=y^2(t)-y^3(t)$ and $y(0)=1$. I start to use a backward Euler method using : $y_{n+1}=y_n+h(y_{n+1}^2-y_{n+1}^3)$ with $h>0$ given. I build $F:y_{n+1}\mapsto y_{n+1}-y_n-h(y_{n+1}^2-y_{n+1}^3)$ such that $F(y_{n+1})=0$. If $F'(y_{n+1})\neq 0$, I can use the Newton's method to determine $y_{n+1}$. Do I have to consider $y_{n+1}-\frac{F(y_{n+1})}{F'(y_{n+1})}=y_{n+1}-\frac{y_{n+1}-y_n-h(y_{n+1}^2-y_{n+1}^3)}{1-2hy_{n+1}+3hy_{n+1}^2}=\frac{y_n-hy_{n+1}^2+2hy_{n+1}^3}{1-2hy_{n+1}+3hy_{n+1}^2}$ ? Then how to continue ? Thanks in advance !","['numerical-methods', 'ordinary-differential-equations', 'newton-raphson']"
2756718,Minimal polynomial of $T(X)=A^{-1}XA$,"Let $A$ be an invertible, diagonalizable matrix and let $V$ be the space of $n \times n$ matrices. Define $T\colon V\to V$ be $T(X)=A^{-1}XA$. Find the eigenvalues, minimal, and characteristic polynomial of $T$. I think I have what the eigenvalues could be. If we let $A=QDQ^{-1}$, where $D$ is diagonal with entries $\lambda_1,\dots,\lambda_n$ , then $T(X)=\lambda X$ is equivalent to $D^{-1}ZD=\lambda Z$, where $Z=Q^{-1}XQ$. Hence, comparing entries on the left and right, I get that $\lambda z_{ii}=z_{ii}$ for each $i$, and if $i\not=j$, $\lambda z_{ij}=\frac{\lambda_j}{\lambda_i}z_{ij}$. So the possible eigenvalues for $T$ are $\lambda=1$ or $\lambda=\frac{\lambda_j}{\lambda_i}$, $i\not=j$. From here though, I don't see how to find the minimal or characteristic polynomial of $T$.","['eigenvalues-eigenvectors', 'minimal-polynomials', 'linear-algebra', 'vector-spaces']"
2756744,"For $2 \times 2$ matrices $AB=-BA$ with $BA$ not $0$, prove that $\mathrm{tr}(A)=\mathrm{tr}(B)=\mathrm{tr}(AB)=0$","It is easy to derive from $AB=-BA$ that $\mathrm{tr}(AB)=0$ since $\mathrm{tr}(AB)=\mathrm{tr}(-BA)=-\mathrm{tr}(BA)=-\mathrm{tr}(AB)$. However, I cannot get that $\mathrm{tr}(A)=\mathrm{tr}(B)=0$ without the fact that $A$ and $B$ are invertible. My Professor suggested I use the Cayley-Hamilton Theorem. However, that just gives me a few extra conditions on the elements of $A$ and $B$, and I still can't get that their traces equal $0$. Any ideas are greatly appreciated!","['matrices', 'trace', 'cayley-hamilton', 'linear-algebra']"
2756746,Why does the Dedekind zeta function of a number field have a pole at $s=1$?,The analytic class number formula tells us that the Dedekind zeta function $\zeta_K$ of a number field $K$ has a pole at $s=1$ with residue $$\frac{2^{r_1}(2\pi)^{r_2}\text{Reg}_Kh_K}{w_K\sqrt{|\Delta_K|}}.$$ Is there a quick way to see that $\zeta_K$ should have a pole at $s=1$ without explicitly computing the residue? Is there some theorem concerning $L$-functions that yields this fact immediately?,"['number-theory', 'zeta-functions', 'l-functions']"
2756861,Legendre symbol of $(-3/p)$.,"Question. Show $$\Big(\frac{-3}{p}\Big)=\begin{cases}+1 & p\equiv 1\bmod 3,\\
-1 & p\equiv 2\bmod 3. \end{cases}$$ Attempt. So, using the established results of $$\Big(\frac{3}{p} \Big) = \begin{cases}+1 & p\equiv\pm1\bmod 12,\\
-1 & p\equiv\pm5\bmod 12,\end{cases}$$ $$\Big(\frac{-1}{p}\Big)=\begin{cases}+1 & p\equiv1\bmod 4,\\
-1 & p\equiv3\bmod 4,\end{cases}$$ and observing that $$\Big(\frac{-1}{p} \Big) = \begin{cases}+1 & p\equiv1,5\bmod 12,\\
-1 & p\equiv7,11\bmod 12,\end{cases}$$ we get $$\Big(\frac{-3}{p}\Big)=\begin{cases}+1 & p\equiv 1,7\bmod 12,\\
-1 & p\equiv 5,11\bmod 12. \end{cases}$$ Providing I've carried this out right;  how can I reduce this further so it is in the form of modulo $3$?","['number-theory', 'legendre-symbol']"
2756867,find all analytic function such that real part is radial,"Let $W=\{\operatorname{Re} z>0\}$. I want to find all analytic functions $f: W \rightarrow \mathbb{C}$ for which there exists a $C^\infty$ function $\phi: (0,\infty)\rightarrow \mathbb{R}$ such that $\operatorname{Re}(f(z))=\phi(|z|^2)$. I feel like it should be something like $\log z^2$, but I don't know a general approach to find all of such functions.","['complex-analysis', 'analysis']"
2756913,How can tan(30°) be irrational and at the same time be a ratio within a triangle?,"Dumb question... if tan(30°) is irrational (what I believe it is, it should be $\sqrt 3$), how can it be that, at the same time, is is describing the ratio of two sides within a triangle? The triangle I have in mind looks like this (sorry for the cheap sketch): So, $a$ is the angle (in this case 30 degrees), and AFAIK it should be possible to describe that angle by the ratio of the opposite and adjacent sides, $A$ and $B$, so: $a = \frac{A}{B}$... and there certainly exists a real-world triangle for $a=30°$, where the other two angles are $90°$ and $60°$... and the two lengths, $A$ and $B$, are just some real values, so their ratio should be a rational number... or not? :-) It seems I'm missing something here...",['trigonometry']
2756948,"If $G,H$ are groups and $H$ is not trivial then $G \times H \ncong G$","My instinct tells me this should be true, and I hope there aren't any weird counterexamples. If $G,H$ are groups and $H$ is not trivial, then I think $G \times H \ncong G$ (i.e., $G \times H$ is not isomorphic to $G$). Surely this is right? Is there an elementary group theory based argument that I'm not seeing?",['group-theory']
2756993,Analytic continuation of the prime zeta series,"The prime zeta series is denoted by $ \sum_p \frac{1}{p^s} $ , where $ p $ is a prime number. It is absolutely convergent in the half plane right of the abscissa at $ \sigma_a = 1 $ . I have seen several resources asserting it is possible to extend it analytically to $ \sigma_c = 0 $ but not beyond. However, I haven’t been able to find how exactly the convergence plane can be extended. Does anyone know how to extend the half plane of convergence for prime zeta beyond $ \sigma_a = 1 $ ? References: https://en.m.wikipedia.org/wiki/Prime_zeta_function http://mathworld.wolfram.com/PrimeZetaFunction.html References Behind pay wall: https://link.springer.com/article/10.1007%2FBF01933420 https://page-one.live.cf.public.springer.com/pdf/preview/10.1007/BF03014596","['complex-analysis', 'dirichlet-series', 'sequences-and-series', 'convergence-divergence']"
2757022,Covering a sphere with spherical caps,"Consider the $n-1$ dimensional unit sphere embedded in $\mathbb{R}^n$. For example, when $n=3$, the sphere is characterized by $x_1^2+x_2^2+x_3^2=1$. Define a special point as a point whose coordinates are all zero except one coordinate that is either $1$ or $-1$. For example, if $n=3$, there are six special points: $(1,0,0)$, $(-1,0,0)$, $(0,1,0)$, $(0,-1,0)$, $(0,0,1)$, $(0,0,-1)$. In general, there are $2n$ special points in $\mathbb{R}^n$. Consider a spherical cap centered at each special point. So there are $2n$ caps as well. Suppose all caps have the same height $h$. I am interested in finding the smallest $h$ such that allows these caps to entirely cover the sphere. Thank you! Golabi",['geometry']
2757084,"How to see wether numbers are distributed ""evenly"" ([1,2,18,35,36]) or ""cluttered to one side"" ([1,2,3,30,31], [7,9,17,16,36])?","I have a set of 5 integer numbers {1,23, 17, 33, 35}. Elements can take values only from [1..36], and happen only once within the set. What math can I use to understand, wether the numbers are distributed ""evenly"" (means very symmetric with respect to 18 - like ([1,2,18,35,36]) or ""cluttered to one side"" ([1,2,3,30,31], [7,9,17,16,36]) within single given set of 5 numbers? ""cluttered to the left"" - means there are more small numbers - below 18 (say 3, 4, 5 numbers are below 18). I need to analyze many such sets (assigning ""evenly""/""cluttered"" value to each) and then understand what happens more often. Besides, such indicator must show Numbers tend to be cluttered on the left or on the right ([1,2,3,30,31], [7,9,17,16,36]). Numbers tend to be close to 18 [16,15,18,19,20] I think of variance and standard deviation, but I am not sure - maybe there are better applicable or more advanced indicators/analysis methods. P.S. Seems standard deviation is not helping, or I cannot understand how to use it: std([1,2,18,35,36]) = 15.21315220458929  (""evenly"" distributed) std([1,2,3,30,31]) = 13.97998569384104   (""cluttered/skewed"" to the left) std([7,9,16,17,36]) = 10.25670512396647  (""cluttered/skewed"" to the left) std([1,30,31,32,33]) = 12.24091499847948 (""cluttered/skewed"" to the right) Besides, non-parametric skew can be used - it is within [-1..1] and is zero if values are symmetric with respect to the ""middle"".",['statistics']
2757095,"Proof by contradiction that $f,g$ injective imply $f \circ g$ injective.","Statement: $g:A \to B$ and $f:B \to C$ are injective functions. Then $f \circ g:A \to C$ is injective. Attempt of proof: Suppose by contradiction that $f \circ g$ is not injective. Then there exist $x$ and $y$, $ x \neq y$, such that $f \circ g(x) = f\circ g(y)$. Then, by definition of composition, $f(g(x)) = f(g(y))$. Since $g$ is injective, $f(x) = f(y)$. Since $f$ is injective, $x = y$. But that is a contradiction, since $x \neq y$. I'm not sure if every step I'm doing here is alright, specialy that if $f \circ g$ isn't injective then there exist $x$ and $y$, $x \neq y$. Why there must be two points in $A$?","['functions', 'proof-verification']"
2757106,Isomorphisms between $O_n$ and the direct product of $SO_n$ with another group,"Let $O_n(\mathbb{R})$ be the group of real orthogonal $n\times n$ matrices and $SO_n(\mathbb{R})$ be the group of real orthogonal matrices with determinant $1$. (i) Show that $O_n(\mathbb{R}) = SO_n(\mathbb{R}) × \{\pm I_n\}$ if and only if $n$ is odd. (ii) Show that if $n$ is even, then $O_n(\mathbb{R})$ is not the direct product of $SO_n(\mathbb{R})$ with any
normal subgroup. Here is the progress I have made so far: (i) If $n$ is odd then consider the map $\phi: SO_n(\mathbb{R}) × \{\pm I_n\} \to O_n : (A,B) \to AB$. This is a homomorphism as all the elements of $\{\pm I_n\}$ commute with the elements of $SO_n(\mathbb{R})$. Furthermore it is injective as is $AB = CD$ then since $A,C$ have determinant $1$ we get that $B,D$ have the same determinant so $B,D$ are the same matrix so $A,C$ are the same as well. It is also surjective as if $E \in O_n(\mathbb{r})$ then either $E$ or $-E \in SO_n(\mathbb{R})$ and so either $(E,I_n)$ or $(-E,-I_n)$ maps to $E$. Hence we have an isomorphism and they are the same. If $n$ is even then note that $O_n(\mathbb{R})$ has center of order $2$ while $ SO_n(\mathbb{R}) × \{\pm I_n\}$ has center of order $4$ so they are not isomorphic. (ii) I am having trouble with this bit. I can't even manage to show $O_n(\mathbb{R})$ is not isomorphic to $SO_n(\mathbb{R})$ for even $n$. Any help is much appreciated.","['group-theory', 'linear-algebra']"
2757117,Complex argument for Fourier transform,"While computing the convolution of Gaussian functions of type $\varphi_a(x)=e^{-\pi x^2/a}$, $x\in\mathbb{R}^d$, $a\in\mathbb{R}, a\neq 0$, I wrote without thinking: $\begin{align}
\varphi_{a}*\varphi_{b}\left(x\right)&=\int e^{-\pi y^{2}/a}e^{-\pi\left(x-y\right)^{2}/b}\mathrm{d}y=e^{-\pi x^{2}/b}\int e^{2\pi xy/b}e^{-\pi\left(\frac{1}{a}+\frac{1}{b}\right)y^{2}}\mathrm{d}y\\&=e^{-\pi x^{2}/b}\int e^{-2\pi i\left(ix/b\right)y}e^{-\pi cy^{2}}\mathrm{d}y=\varphi_{b}\left(x\right)\mathcal{F}\left[\varphi_{\frac{1}{c}}\right]\left(\frac{ix}{b}\right)\\&=\varphi_{b}\left(x\right)\left(\frac{ab}{a+b}\right)^{d/2}\varphi_{c}\left(\frac{ix}{b}\right),
\end{align}$ where $c=\frac{1}{a}+\frac{1}{b}=\frac{a+b}{ab}$ and $\mathcal{F}$ stands for the Fourier transform. In particular, we have for Gaussian functions that
$$\mathcal{F}[\varphi_c](\omega)=c^{d/2}\varphi_{1/c}(\omega).$$ Even if this yields the expected correct result, I am not able to justify this fact. Is it legitimate to argue as before and thus evaluate the Fourier transform at a complex point even if it takes $\mathbb{R}^d$ arguments by definition? Perhaps Paley-Wiener theory or Laplace transform are involved, but I cannot unravel their role.","['complex-analysis', 'real-analysis', 'fourier-analysis', 'fourier-transform']"
2757118,British Maths Olympiad (BMO) 2005 Round 1 Question 1 how to make progress?,"The question is as follows: Each of Paul and Jenny has a whole number of pounds.
He says to her: “If you give me £3, I will have $n$ times as much as you”.
She says to him: “If you give me £$n$, I will have 3 times as much
as you”. Given that $n$ is a positive integer what are the possible values for $n$? We begin by representing Paul and Jenny's amounts by $x,y$ respectively. Then we have: $x+3 = n(y-3)\\y+n = 3(x-n)$ After some manipulation we can arrive at: $n = (x+3)/(y-3) = (3x-y)/(4)$ If we let $y-3 = 4$ and $x+3 = 3x-y$ we get a solution but beyond that I'm not quite sure how to progress. General ideas on how to approach problems like this would be appreciated as well as some help with the solution.","['algebra-precalculus', 'contest-math', 'elementary-number-theory']"
2757124,"If the sides $a$, $b$ and $c$ of $\triangle ABC$ are in Arithmetic Progression","If the sides $a$, $b$ and $c$ of $\triangle ABC$ are in Arithmetic Progression, then prove that:
$$\cos (\dfrac {B-C}{2})=2\sin (\dfrac {A}{2})$$ My Attempt: Since, $a,b,c$ are in AP
$$2b=a+c$$
$$\sin A+\sin C=2\sin B$$
$$2\sin (\dfrac {A+C}{2}).\cos (\dfrac {A-C}{2})=2\sin B$$
$$\sin (\dfrac {A+C}{2}).\cos (\dfrac {A-C}{2})=\sin B$$
$$\sin (\dfrac {A+C}{2}).\cos (\dfrac {A-C}{2})=2.\sin (\dfrac {A+C}{2}).\cos (\dfrac {A+C}{2})$$
$$2\cos (\dfrac {A+C}{2})=\cos (\dfrac {A-C}{2})$$","['arithmetic-progressions', 'trigonometry', 'triangles']"
2757135,how to find such matrices to compute $\sin A$,"Could anyone tell me how to solve this one? $A=\begin{bmatrix}5&-6&-6\\ -1&4&2\\3&-6&-4 \end{bmatrix}$, and given that $\sin A=B\times C\times E$, then find $B,C,E$. I never solved such type of problem in past thanks for helping. I see charpoly of $A$ is $\lambda^3-5\lambda^2+8\lambda-4=0$. I also checked $P^{-1}AP=\text{diagonal}(2,2,1), P=\begin{bmatrix}2&2&1\\1&0&\frac{-1}{3}\\ 0&1&1\end{bmatrix}$.","['matrices', 'matrix-equations']"
2757140,Differentiate $y=x$ with respect to $x^2$?,"Let $y=x$. Differentiate $y$ with respect to $x^2$. My intuition tells me to go down a substitution route here. So, let $u=x^2~~ \Leftrightarrow~~x=\sqrt{u}$, then what we are looking to find is simply $$\frac{d}{du}\sqrt{u}$$
which is
$$\frac{1}{2\sqrt{u}}$$
as $\sqrt{u}=x$,
$$\frac{d}{dx^2}~x=\frac{1}{2x}$$ is this approach/solution correct, or is there something fundamentally wrong here? Any help is appreciated.","['derivatives', 'calculus']"
2757147,Open subsets of the connected sum $M_1\# M_2$ [duplicate],"This question already has an answer here : Nice neighborhoods of each ""piece"" in a manifold connected sum (1 answer) Closed 6 years ago . I'm trying to solve a problem in John Lee's ITM (Problem 4-19), but seems that i need helps now. Here's the problem : Let $M_1 \# M_2$ be a connected sum of $n$-manifolds $M_1$ and $M_2$. Show that there are open subsets $U_1,U_2  \subseteq M_1 \# M_2$ and points $p_i  \in M_i$ such that $U_i \approx M_i \smallsetminus \{p_i\}$, $U_1 \cap U_2 \approx \mathbb{R}^n \smallsetminus \{0\}$, and $U_1\cup U_2 =  M_1 \# M_2$. The definition of the connected sum of $M_1\# M_2$ is the adjunction space of $M_1' \cup_f M_2'$ under a homeomorphism $f : \partial M_1' \to \partial M_2'$, where $M_i' = M_i \smallsetminus B_i$ and $B_i \subseteq M_i$ is a regular coordinate ball of $M_i$. Regular coordinate ball $B_i \subseteq M_i$ is a coordinate ball with homeomorphism $\varphi : B' \to \varphi(B')=B_{r'}(0)$, where $B' \supseteq B$, such that $\varphi(B) = B_r(0)$ and $\varphi(\bar{B}) =  \bar{B}_r(0)$ for $r'>r>0$. If we denote the embeddings as $e_i : M_i' \to M_1 \# M_2$, and the larger open subsets contain the coordinate balls $B_i$ as $B_i'$, 
i'm guessing that the desired open subsets are $U_1 = e_1(M_1')\cup e_2(B_2' \smallsetminus B_2)$ and $U_2 = e_1(B_1'\smallsetminus B_1) \cup e_2(M_2')$. But i having trouble to show that $U_i \approx M_i\smallsetminus \{p_i\}$. Actually  i managed to show that (with $p_i$ is chosen as the ""center"" of coordinate ball $B_i$), but my solution is quite long. So i'm not sure it is correct (i'll add my solution here if needed). If these choices of $U_1,U_2$ is correct i really appreciate if somebody tell me some hint to show that $U_i \approx M_i \smallsetminus \{p_i\}$. If it is not then i would like to know what the correct choices look like. Thank you. $\textbf{Edit : }$I think i already got what i want here . It has a nice answer.","['manifolds', 'general-topology', 'manifolds-with-boundary']"
2757195,Show that the spectral radius satisfies $\rho(AB)\le \rho(A)\rho(B)$ for commuting matrices,"For any square matrix $C$ with real entries, denote by $\rho(C)$ its spectral radius, i.e. the maximum magnitude of its eigenvalues. For symmetric matrices $A$ and $B$ with $AB=BA$ show that $$\rho(AB)\le \rho(A)\rho(B)$$ I think simultaneous diagonalization of $A$ and $B$ is to be used here, but couldn't find my way out. Also will the proposition hold if the condition of symmetry is dropped?","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2757213,Example of two homeomorphic spaces without a continuous deformation between them?,"on wikipedia it states: there need not be a continuous deformation for two spaces to be homeomorphic — only a continuous mapping with a continuous inverse function. I'm trying to intuitively understand this difference between a continuous mapping and continuous deformation, so I'm trying to find an example of two spaces that are homeomorphic but without a continuous deformation. Here is a definition of homotopy: Formally, a homotopy between two continuous functions f and g from a topological space X to a topological space Y is defined to be a continuous function H : X × [0,1] → Y from the product of the space X with the unit interval [0,1] to Y such that, if x ∈ X then H(x,0) = f(x) and H(x,1) = g(x) Does the following example satisfy? In a sense there is no continuous deformation between the two ( though I honestly cannot immedately see from the definition why not. i.e. I cannot see why simply letting the different parts of the knot ""move through"" each other is not allowed by the formal definition of homotopy. )","['general-topology', 'homotopy-theory']"
2757297,The intuition behind the definition of a monotone operator,"I started reading about monotone operators in Zeidlers'  book on Nonlinear functional analysis: The operator $A:X \to X^*$ is monotone on the reflexive Banach space $X$ if:
  $\langle Ax - Ay, x-y\rangle \geq 0$ for all $u,v \in X$. This definition (if I get it right) is supposed to be a generalization of a monotone function $f: \mathbb{R} \to \mathbb{R}$. He proceeds with an example: Set $X=\mathbb{R}$ and $F(u)=Au$. Then $X^*= \mathbb{R} $ and:
  $\langle Ax - Ay, x-y\rangle =\big(F(u) -F(v) \big)(u-v)$. But I'm thinking if $F$ is strictly decreasing and we have $v<u, u,v\in \mathbb{R}$ then $u-v >0$ while $F(u) -F(v)<0$, so $\big(F(u) -F(v) \big)(u-v)<0$. What am I missing here? If this were a generalization of a monotone function of $\mathbb{R}$ it should be positive for all $u,v \in \mathbb{R}$ I think? If someone could help me and explain where the definition comes from I'd also be very grateful.","['intuition', 'real-analysis', 'monotone-functions', 'operator-theory', 'functional-analysis']"
2757324,We can always find a projective line $L\subset\mathbb{CP^2}$ intersecting degree $d$ projective curve at $d$ distinct points,"Let $X$ be a smooth projective plane curve$\{ [Z_0, Z_1, Z_2]\in \mathbb{CP^2}\ \vert \ p(Z_0, Z_1, Z_2) = 0 \}$ defined by non-singular homogeneous polynomial $p$ of degree $d$. Then there exists a projective line $L\subset\mathbb{CP^2}$ intersecting $X$ at $d$ distinct points. Is there a way to prove the above statement using only elementary knowledge of Riemann surfaces? I do not know much of algebraic geometry.","['riemann-surfaces', 'complex-analysis', 'complex-geometry', 'algebraic-geometry']"
2757353,Non-negative concave functions are non-decreasing,"Let $f : [0, \infty) \longrightarrow [0, \infty)$ be concave with $f (0) = 0$ and $f (x) > 0$ for $x > 0$. Show that $f$ is non-decreasing. It is clear that a concave function $f$ can be decreasing. Intuitively, $f$ must then become negative beyond some point, though. Once it has turned ""right"" (i.e., downwards) concavity prevents it from avoiding the abscissa. I'm looking for a reasonably elegant proof. I've tried to formalize my intuition as follows, but I'm afraid my argument isn't very rigorous (if it is right in the first place). Maybe there are also different and better approaches than mine. Here's my attempt: Concacity implies that $f (\lambda x + (1 - \lambda) z) \ge \lambda f (x) + f ((1 - \lambda) z)$ for $0 \le x < z$ and $\lambda \in (0, 1)$ or, alternatively,
$$\frac{f (z) - f (y)}{z - y} \le \frac{f (z) - f (x)}{z - x} \le \frac{f (y) - f (x)}{y - x}$$
for $0 \le x < y < z$. (Let $1 - \lambda := \frac{y - x}{z - x}$, so that $\lambda x + (1 - \lambda) z = y$ to restate concavity without $\lambda$.) It follows that
$$f (z) \le \frac{f (y) - f (x)}{y - x} (z - y) + f (y).$$ Graphically speaking, $f (z)$ must be on or below the secant through $f (x)$ and $f (y)$ for all $z > y$. Suppose that $f (y) < f (x)$ for some $x < y$. The secant is then strictly decreasing—and so is $f$ for $z > y$. As a result, $f$ must turn negative where the secant intersects the abscissa or before. This contradicts the assumption. Hence, $f (y) \ge f (x)$ for all $0 \le x < y$. Note: I feel that the argument requires $f$ to be continuous, although I don't see how to apply the intermediate value theorem rigorously. Continuity follows from concavity, though.",['real-analysis']
2757361,What does 'central exposed to risk' mean?,"I am currently going through some actuarial lecture notes which, without definition, use the term 'central exposed to risk', which it denotes by $E_{x,t}^c$. Having googled this term, the most easily understandable explanation of this term seems to imply that if we have a population of initial size $p$ at time $x$ and at time $x+t$ the size of this population is $q$ then
$$
E_{x,t}^c = p - \frac{p-q}{t}
$$
i.e. the average number of lives which are alive within the population between times $x$ and $x+t$. Is this correct?","['statistics', 'actuarial-science']"
2757365,Find $f'(0)$ if $f(x)+f(2x)=x\space\space\forall x$,"Find $f'(0)$ if $f(x)+f(2x)=x\space\space\forall x$ If we assume $f'(0)\in\mathbb R$, then obviously, $f'(0)=\frac{1}{3}$. But what if we don't assume the derivative exists? I get this question when I am taking an exam, and I then asked the professor about whether there exist $f$ such that $f'(0)$ doesn't exist but satisfies the condition, but he says he think both exist or not is likely... What I have tried: If $f(x)+f(2x)=x\space\space\forall x\in\mathbb R$, $f(x)=\frac{x}{3}$ is the only nice function I have ever thought up$. If we restrict $dom(f)\in\mathbb R$, I think that any polynomial besides $\frac{x}{3}$ doesn't satisfy the condition, and somethings like $\vert x\vert$ doesn't help neither. On the other hand, I have tried to think about the equivalence statements with $f(x)+f(2x)=x\space\space\forall x$ in order to prove $f$ must be specific kind of problem s.t.$f'(0)$ exists. $f(2x)+f(4x)=2x$, so $f(4x)-f(x)=x$. In general, $$f(2^{2^n}x)-f(x)=x\prod_{k=1}^{n-1}(2^{2^k}+1)\forall n\in\mathbb N, x\in\mathbb R$$ But I don't think this helps. Can we find $f(2x)-f(x)$ by given condition? I havn't get an idea. But while I am thinking of it, I observe that $g(x)=f(2x)-f(x)$ is on its own satisfying $g(2x)+g(x)=x$. Will thinking about $f\circ f\circ f\circ f\circ f\circ\dots$ be useful? $f\biggl(f(x)+f(2x)\biggr)+f\biggl(2\bigl(f(x)+f(2x)\bigr)\biggr)=x\space\space\forall x$ Any help will be appreciate. Thank you!","['real-analysis', 'calculus', 'functional-equations']"
2757382,Strong law of large numbers for the conditional expectation of functions of random vectors,"Consider a collection of 0-1 random variables $Y_{n,N}$, for all $n$ and $N$.
The random variable $Y_{n,N}$ is a deterministic function of the collection of random variables in $\mathcal{F}_n = \{(U_k)_{k=1,\ldots,n}, (V_k)_{k=1,\ldots,n} \}$ where the $U_k$'s and the $V_k$'s are all independent among them and uniformly distributed over $[0,1]$.
Therefore, $Y_{n,N}=\mathbb{E}[Y_{n,N} | \mathcal{F}_n ]$. Let me denote by $\mathcal{F}_n\setminus V_n$ the set $\mathcal{F}_n$ where $V_n$ has been removed. My question is the following. Assuming that 
$$
\lim_{N\to\infty} \frac{1}{N} \sum_{n=1}^N Y_{n,N} 
$$
exists almost surely and that 
$$
\lim_{N\to\infty} \frac{1}{N} \sum_{n=1}^N \mathbb{E}[Y_{n,N} | \mathcal{F}_n\setminus V_n] = Z
$$
almost surely (where $Z$ is some other random variable), is it true that necessarily
$$
\lim_{N\to\infty} \frac{1}{N} \sum_{n=1}^N  Y_{n,N}  = Z
$$
almost surely? Why? Though different (because I am assuming the existence of the first limit), this question is related to the following questions: Strong law of large numbers for function of random vector: can we apply it for a component only? and Law of large numbers with one dependency","['stochastic-processes', 'law-of-large-numbers', 'probability-theory', 'probability', 'conditional-expectation']"
2757393,$\mathrm{E}[e^{u X}|\mathcal{A}] = \mathrm{E}[e^{u X}|\mathcal{B}]$ implies equality of conditional distributions,"Let $X$ be a random variable on the probability space $(\Omega, \mathcal{F}, P)$ and $\mathcal{A}
\subset \mathcal{B} \subset \mathcal{F}$ be a $\sigma$-subalgebras. I want to prove that if
$$
\mathrm{E}[e^{u X}|\mathcal{A}] = \mathrm{E}[e^{u X}|\mathcal{B}]
$$
holds for any $u \in \mathbb{C}$ then $P(X\in \Gamma|\mathcal{A}) = P(X\in \Gamma|\mathcal{B})$ for any borel $\Gamma$.
Any hints?","['probability-theory', 'conditional-expectation', 'probability', 'moment-generating-functions']"
2757407,Does this claim holds even if the set isn't closed?,"Let $M$ be a smooth manifold, and $C^\infty_0(M)$ the space of smooth compactly supported functions on $M$ . Let $\mathcal{D}'(M)$ be the space of distributions, meaning continuous linear functionals $\mathfrak{J} : C^\infty_0(M)\to \mathbb{R}$ . Now, we have the following definitions: Definition 1. Let $U\subset M$ , we say that $\mathfrak{J}$ vanishes in $U$ if for every $\phi \in C^\infty_0(M)$ with $\operatorname{supp}\phi\subset U$ we have $\mathfrak{J}[\phi]=0$ . Definition 2. Define $$\mathfrak{U}=\{U\subset M : \text{$U$ is open and $\mathfrak{J}$ vanishes on $U$}\},$$ we define the support of $\mathfrak{J}$ to be $$\operatorname{supp}\mathfrak{J}=M\setminus \bigcup \mathfrak{U}.$$ i.e., as the complement of the union of all open sets on which $\mathfrak{J}$ vanishes. Now consider the following situation: $\mathfrak{J}$ depends on the values of the test functions just on $A\subset M$ . This means that if $\phi_1,\phi_2\in C^\infty_0(M)$ with $\phi_1|_A = \phi_2|_A$ then $\mathfrak{J}[\phi_1]=\mathfrak{J}[\phi_2]$ . We can thus prove the following claim: Proposition: If $A$ is closed, $\operatorname{supp} \mathfrak{J}\subset A.$ Proof: To show that $\operatorname{supp}\mathfrak{J}\subset A$ it is enough to prove that $M\setminus A\subset \bigcup \mathfrak{U}$ . Since $A$ is closed, $M\setminus A$ is open. On the other hand $\mathfrak{J}$ vanishes on $M\setminus A$ . Indeed, let $\phi\in C^\infty_0(M)$ have $\operatorname{supp}\phi\subset A$ , meaning that $\phi|_A =0$ . Thus $\mathfrak{J}[\phi]=\mathfrak{J}[0]$ and the later is zero by linearity, showing that $\mathfrak{J}$ vanishes in $A$ . This shows that $M\setminus A\in \mathfrak{U}$ and hence $M\setminus A\subset \bigcup \mathfrak{U}$ which in turn shows that $\operatorname{supp}\mathfrak{J}\subset A$ . Now, I've used that $A$ is closed and hence $M\setminus A$ is open in order to conclude $M\setminus A\in \mathfrak{U}$ (since all sets on this collection are open), which seems one important part of the proof. But what if $A$ is not closed? It seems reasonable to me that whatever $A$ is if $\mathfrak{J}[\phi]$ depends on $\phi$ just inside $A$ we should have $\operatorname{supp}\mathfrak{J}\subset A$ . Is that indeed the case? How the proof would change?","['functional-analysis', 'distribution-theory', 'differential-geometry', 'analysis']"
2757414,Space curve: $\frac{\tau}{\kappa} =$ constant. Then all normal vectors $N(x)$ lie in a plane $A$.,"Consider a space curve $c : I \rightarrow \mathbb{R^3}$ (regular) with $\frac{\tau}{\kappa}$ constant. (Torsion divided by curvature) Show: a) There is a plane $A$ with :
          All normal vector $N(x)$ are in this plane $A$ . So I know that I have to find a vector $cos(\phi) \cdot T(x) + sin(\phi) \cdot B(x)$ with a constant $\phi$ to solve this exercise. But how can I find such a vector? And how can I use this vector to show a) ? Second task: Now the space curve is parameterized by arc length. Show: b) There is an isometry $J : \mathbb{R^3} \rightarrow \mathbb{R^3}$ with: $ J \circ c $ $(y)$ $=$ $(f(y),g(y), y \cdot cos(\phi))$ for two functions $f,g$ :$ I \rightarrow \mathbb{R} $. Remark : the $\phi$ is the $\phi$ from a) . So since $c$ is parameterized by arc length. I can use the following equations:
$T' = \kappa \cdot  N $ and $ N' = - \kappa \cdot T + \tau \cdot B $ and $B' = - \tau \cdot N $, right? I think then we need them. Remark2 : Definition of $T,N,B$ $T(x)$ := $\frac{1}{||c'(x)||} \cdot c'(x) $. $N(x)$ := $\frac{1}{||T'(x)||} \cdot T'(x) $. $B(x)$ := $T(x) \times N(x)$ $\kappa(x)$ := $\frac{|| c' \times c''||(x)}{||c'(x)||^3} $. (curvature) $\tau(x)$ := $\frac{det(c',c'',c''')(x)}{|| c' \times c''||^2(x)} $ (torsion)","['curves', 'isometry', 'differential-geometry', 'curvature']"
2757425,True or False: No Parametrized Surface is Closed in $\mathbb{R}^3$,"Is no parametrized surface (topologically) closed in $\mathbb{R}^3$? I use the definition of Fitzpatrick's Advanced Calculus (2009) . So far, all the examples I've seen have not been closed -- for example, $\{(x,y,z)\in\mathbb{R}^3:x^2+y^2+z^2=1,z>0\}$ or $\{(x,y,z)\in\mathbb{R}^3:x^2+y^2=z^2,z>0,(x-1)^2+y^2<1\}.$","['multivariable-calculus', 'general-topology', 'surfaces']"
2757431,"Calculate $ \lim_{n\to \infty} \frac{n}{\ln(n)} \int_{1}^{n^2} \frac{\ln(x)}{x^2+nx+n^2}\,dx $","For every $ n\in \mathbb{N} $ and positive $ x $ , $ x \neq 0 $ we consider the function $ f_{n}(x) = \frac{\ln(x)} {x^2+nx+n^2} $ Calculate
$ \lim_{n\to \infty} \frac{n}{\ln(n)} \int_{1}^{n^2} f_{n}(x) \,\mathrm dx$ The correct answer should be $ \frac{2\pi \sqrt{3}}{9} $ How to approach this using high-school techniques? The result suggests that  we have to work with an arctangent function probably.","['integration', 'calculus', 'limits']"
2757488,Laplacian of the inverse of a diffeomorphism,"Let $\Phi: \mathbb{R}^n \to \mathbb{R}^n$ be a diffeomorhism. Let $\Delta$ be the componentwise Laplacian. 
Is it possible to wirite
$$
\Delta (\Phi^{-1})\circ \Phi
$$
in such a form that involves only $\Phi$ and not $\Phi^{-1}$? To clarify my question: For the Jacobian matrix (which I will denote by $D$) it is clearly possible. Indeed
$$
D(\Phi^{-1})\circ \Phi = (D\Phi)^{-1}
$$","['multivariable-calculus', 'real-analysis', 'inverse-function-theorem', 'laplacian']"
2757509,Compute $\cos\frac{\pi}{7}-\cos\frac{2\pi}{7}+\cos\frac{3\pi}{7}$ [duplicate],"This question already has answers here : How to prove $\cos\left(\pi\over7\right)-\cos\left({2\pi}\over7\right)+\cos\left({3\pi}\over7\right)=\cos\left({\pi}\over3 \right)$ (4 answers) Closed 6 years ago . Compute $\cos\frac{\pi}{7}-\cos\frac{2\pi}{7}+\cos\frac{3\pi}{7}$ This question came after an exercise involving finding the $7$th roots of $-1$. The roots were $\operatorname{cis}\frac{\pi}{7},\operatorname{cis}\frac{3\pi}{7},\dots$ This made me wonder if I could somehow use those roots, along with the geometry of complex numbers, to compute the expression. Any insight would be helpful. Thanks!","['algebra-precalculus', 'trigonometry', 'complex-numbers']"
2757515,Product of a primitive matrix and its transpose.,"Is it true that if $A$ is a nonnegative primitive matrix, then $AA^T$ is also primitive? Obviously $A^T$ is primitive but in general product of primitive matrices is not primitive. 
Any hint?","['matrices', 'nonnegative-matrices']"
2757521,Prove that this sequence converges to a point,"I encounter a problem about genetics equilibrium under the condition of mutation that needs me to ask for whether the following sequence converges to a point and if yes, what that point is. $$\eqalign{
  & {a_1} = 0.5  \cr 
  & u = 0.7  \cr 
  & v = 0.55 \cr} $$ $$\eqalign{
  & {a_1} \in (0,1)  \cr 
  & {a_2} = {a_1}(1 - u) + v(1 - {a_1}){\rm{ }}  \cr 
  & u,v \in (0,1)  \cr 
  & {a_3} = {a_2}(1 - u) + v(1 - {a_2})  \cr 
  & {a_n} \to ?{\rm{ }} \cr} $$ when $$n \to \infty $$ The more important question is how is the answer derived? As a biomedical sciences student I only have background in Calculus I, II and linear algebra. No analysis background.","['ordinary-differential-equations', 'calculus', 'analysis']"
2757548,Why is the dual exponent what it is?,"The dual $p'$ of an exponent $p \in [0, \infty]$ is: $$p'= \begin{cases} \frac{p}{p-1}, & 1 < p < \infty \\
\infty, & p=1 \\
1, & p= \infty. \end{cases}$$ Why is this the dual of an exponent?",['functional-analysis']
2757560,Is Lebesgue integral w.r.t. counting measure the same thing as sum (on an arbitrary set)?,"TL:DR; For arbitrary sets (not necessarily countable) we have two notions: Lebesgue integral w.r.t. the counting measure and sum of family indexed by this set. Are these two notions equivalent? For an arbitrary set $X$ we can define counting measure simply by putting $\mu(A)=|A|$ . (I.e., if $A$ is finite then $\mu(A)$ is simply number of elements of $A$ ; otherwise it is $+\infty$ .) In this way we get a $\sigma$ -additive measure on $\mathcal P(X)$ and it is possible to work with Lebesgue integral with respect to this measure. If the integral $$\int f \;\mathrm{d} \mu$$ of a function $f\colon X\to\mathbb R$ exists, it is natural to interpret this integral as a sum of the values $f(x)$ over all $x\in X$ . There is also a (more-or-less standard) notion of a sum of values on a given set which includes uncountable sets. Let me briefly recall the definition. (Below I will add a few links to other posts on this site where this definition can be found.) Definition. Let $f\colon X\to\mathbb R$ be a function and $S\in\mathbb R$ . We say that $$\sum_{x\in X} f(x) = S$$ if and only if for every $\varepsilon>0$ there exists a finite set $F_0$ such that for all finite sets $F\supseteq F_0$ we have $\left| \sum\limits_{x\in F} f(x) - S \right| < \varepsilon$ . $$(\forall \varepsilon>0) (\exists F_0\text{ finite }) \left(F\text{ is finite and }F\supseteq F_0 \Rightarrow \left| \sum\limits_{x\in F} f(x) - S \right| < \varepsilon \right)$$ Some further remarks: We can modify the above definition in a natural way to be able to say when $\sum f(x)=+\infty$ and $\sum f(x)=-\infty$ . If we work with non-negative values, i.e., $f(x)\ge0$ , then we get a much simpler equivalent definition $$\sum_{x\in X} f(x) = \sup \{\sum_{x\in F} f(x); F\text{ is finite}\}.$$ This type of sum is also defined in the Wikipedia article about series: Summations over arbitrary index sets ( current revision ). This definition can be interpreted nicely using convergence of nets. We take the directed set consisting of finite subsets of $X$ ordered by inclusion. For every such finite set we have the value $s_F=\sum_{x\in F} f(x)$ . The sum as defined above is equal to $S$ iff $S$ is the limit of this net. With this definition, the distinction between conditional and absolute convergence no longer makes sense. (Which is natural, since we do not take any kind of ordering on $X$ into account.) In particular, in the case $X=\mathbb N$ this correspond to definition of sum of absolutely convergent series. (At least if we work with real values. In more general contexts, it can happen that unconditional convergence and absolute convergence might be different. This definition of sum corresponds to unconditional convergence.) A variant of Cauchy's criterion for such sums can be shown. The same definition can be used in more general settings. (You still probably need the structure to be at least a topological abelian group if you expect the sum to behave reasonably.) Question. Is the sum as defined above equivalent to the notion of Lebesgue integral with respect to the counting measure? In particular I would like to know: Are there some problems that arise if we work with uncountable sets, rather than just with countable ones? Are there any specific problems if I also allow negative values? I would be grateful for both references to some texts which deal with relationship between these two notions. And, of course, for a proof (or sketch of a proof) if this is sufficiently simple to fit into a post on this site. I have checked whether something about this is mentioned in the Wikipedia article Counting measure . This kind of sum is mentioned there, but in a slightly different context. The current revision of the Wikipedia article says that: The counting measure is a special case of a more general construct.  With the notation as above, any function $f \colon X \to [0, \infty)$ defines a measure $\mu$ on $(X, \Sigma)$ via $$\mu(A):=\sum_{a \in A} f(a)\, \forall A\subseteq X,$$ where the possibly uncountable sum of real numbers is defined to be the sup of the sums over all finite subsets, i.e., $$\sum_{y \in Y \subseteq \mathbb R} y := \sup_{F \subseteq Y, |F| < \infty} \left\{ \sum_{y \in F} y  \right\}.$$ Taking $f(x)=1$ for all $x$ ' in $X$ produces the counting measure. Some related links: There are several posts where the definition of sum over arbitrary (possibly uncountable) sum is given, including some properties and references. For example, in the following questions: The sum of an uncountable number of positive numbers... , Does uncountable summation, with a finite sum, ever occur in mathematics? , Use of $\sum $ for uncountable indexing set , Looking for a reference (textbook) for an elementary analysis problem on uncountable sums , Reference for series on arbitrary infinite sets . (And for each of them you can find a lot of further posts about this topic if you look at related and linked questions in the sidebar.) Proof that counting measure and summation coincide for non-negative functions on $\mathbb N$ can be found here: Integration with respect to counting measure. Although this answer claims that counting measure is not $\sigma$ -additive if $X$ is not countable, I do not really see why this should be the case. (I am not sure, but it is possible that the poster meant to say $\sigma$ -finite...?) In fact, there is another question on this site where the answer gives a proof for countable additivity: Counting measure proof .","['real-analysis', 'lebesgue-integral', 'measure-theory', 'summation', 'sequences-and-series']"
2757588,"Fixing proof that if $\overline{A}\cap B=A\cap\overline{B}=\varnothing$, there exists $U,V$ such that $A\subset U$ and $B\subset V$","I tried to prove the following theorem: Let $X$ be a metric space and $A,B\subset X$. Suppose that $\overline{A}\cap B=A\cap\overline{B}=\varnothing$. Then there exists two disjoint open sets $U$ and $V$ such that $A\subset U$ and $B\subset V$. My proof was the following: If $a\in A$, there exists $r_a>0$ such that $B(a,r_a)\cap B=\varnothing$. Otherwise, we would have $a\in\overline{B}$. (We could construct a sequence in $B$ that converges to $A$.) Similarly, if $b\in B$, there exists $r_b>0$ such that $B(b,r_b)\cap A=\varnothing$. Let
      $$\;\;U=\bigcup_{a\in A}B(a,\epsilon r_a) \;\text{ et }\; V=\bigcup_{b\in B} B(b,\epsilon r_b),$$
      where $\epsilon>0$ will be chosen later. Clearly, $A\subset U$ and $B\subset V$. If $U\cap V$ were not empty, there would exist $x\in B(a,\epsilon r_a)\cap B(b,\epsilon r_b)$, where $a\in A$ and $b\in B$. So, by the triangular inequality:
      $$d(a,b)\leq d(a,x)+d(b,x)<\epsilon(r_a+r_b).$$
      Since $a\notin \overline{B}$, $d(a,B)>0$. However, if we take $\epsilon=d(a,B)/(r_a+r_b)$ we have 
      $$f_B(a)\leq d(a,b)<\epsilon (r_a+r_b)=f_B(a),$$
      which is absurd! Now, I realised that I cannot take such $\epsilon$ since $a$ and $b$ depend on it. It seems intuitive to me that every $0<\epsilon<1$ would work but I can't figure out how to prove it.","['general-topology', 'metric-spaces']"
2757624,Number theory to estimate lower bound of spectrum in quantum mechanics?,"I recently worked on the following idea: Eigenvalue of an Euler product type operator? Summary of the idea We represent numbers by infinite dimensional matrices such as $3$ will have all $0$s except the $1$st row will have a $1$ in the third column, the $2$nd row will have a $1$ in the $6$th row and the $r$th row and $3r$th column also have $1$s: $$ \hat 3  = | 1 \rangle \langle 3 | + | 2 \rangle \langle 6 | + | 3 \rangle \langle 9 | + \dots = \begin{bmatrix}
    0       &0 & 1        &0 & 0& \dots & 0 \\
    0       &0 & 0 &0       &0 & 1 & \dots & \\
    \vdots \\
    0      & 0 & 0 & \dots 
\end{bmatrix}
$$ Similarly we can define $\hat 2$: $$ \hat 2 = | 1 \rangle \langle 2 | + | 2 \rangle \langle 4 | + | 3 \rangle \langle 6 | + \dots =\begin{bmatrix}
    0       &1 & 0        &0 & 0& \dots & 0 \\
    0       &0 & 0 &1       &0 & 0 & \dots & \\
    0       &0 & 0 &0       &0 & 1 & \dots & \\
    \vdots 
\end{bmatrix}
$$ One notices that these numbers obey multiplication $$ \hat 6 = \hat 3 . \hat 2 = \hat 2 . \hat 3$$ where the $r$th row of the $6r$th column has a $1$ One can also use this to define an Euler like product formula: $$ \hat \zeta (s) = \hat 1  + \hat 2^s + \hat 3^s + \dots = (1- \hat 2^s)^{-1}(1- \hat 3^s)^{-1}(1- \hat 5^s)^{-1} \dots$$ Note: $\zeta(1) |\lambda \rangle  = |\text{factors of } \lambda \rangle$ My Observation Let us define the following ladder operators $a$ and $a^\dagger$ from quantum mechanics: $$ A^\dagger|n \rangle = | n+1 \rangle$$ Now we make the following observation: $$ \frac{A^\dagger}{\hat I- A^\dagger} \geq \hat \zeta(1) - \hat I$$ Where I is the identity or $\hat 1$. To see what $\frac{A^\dagger}{1- A^\dagger} $ looks like apply it $\hat 1$ as: $ \frac{A^\dagger}{1- A^\dagger} \hat 1$. In the sense: $$ \langle m  |\frac{A^\dagger}{\hat I- A^\dagger} |n \rangle \geq  \langle m  |(\hat \zeta(1) - \hat I) |n \rangle $$ In fact for $n>1$: $$ \langle m  |\frac{A^{\dagger 2}}{\hat I- A^\dagger} |n \rangle \geq  \langle m  |(\hat \zeta(1) - \hat I) |n -1 \rangle $$ Subtracting the above equations: $$ \langle m|A^\dagger| n \rangle \geq  \langle m  |(\hat \zeta(1) - \hat I) ( |n \rangle - |n -1 \rangle) \geq  \langle m  |(\hat \zeta(1) - \hat I)  |n \rangle  $$ Question Given a Hamiltonian can be expressed as annihilation and creation operators can the above expression be used to get some lower bounds on the difficult to compute spectrums? $$H(a^\dagger, a) \geq H'(\hat \zeta,\hat \zeta^\dagger) $$","['number-theory', 'quantum-mechanics', 'linear-algebra']"
2757637,For $A\subset\Bbb R^n$ and $B\subset \Bbb R^m$ show that $\dim_H(A\times B)=\dim_H(A)+\dim_H(B)$,"Im stuck with this exercise For $A\subset\Bbb R^n$ and $B\subset \Bbb R^m$ show that $\dim_H(A\times B)=\dim_H(A)+\dim_H(B)$ where $\dim_H$ is the Hausdorff dimension. I know that when $A$ and $B$ are open the above holds. However Im unable to generalize the result. I tried to relate the following inequalities and identities $$\operatorname{diam}(A)\lor\operatorname{diam}(B)\le\operatorname{diam}(A\times B)\le\operatorname{diam}(A)+\operatorname{diam}(B)\tag1$$ $$\mathcal H_*^r(A)<\infty\implies\mathcal H_*^s(A)=0,\quad\forall s>r\\
\mathcal H_*^r(A)>0\implies\mathcal H_*^s(A)=\infty,\quad\forall s\in[0,r)\tag2$$ $$x,y\in[0,1]\implies xy<x+y\,\text{ and }\, x^{r+s}<x^r,\quad\forall r,s>0\tag3$$ $$\inf A+\inf B=\inf(A+B)\text{ and }\sup A+\sup B=\sup(A+B)\tag4$$ where $\mathcal H_*^s$ is the $s$ -dimensional Hausdorff outer measure and $\rm diam$ is the diameter of a set. By example I find that $$\dim_H(A)+\dim_H(B)=\inf\{r>0:\exists \alpha\in[0,r]\text{ such that }\mathcal H_*^{r-\alpha}(A)+\mathcal H_*^\alpha(B)=0\}\tag5$$ so a line of action is try to relate $(2)$ and $(5)$ in something like $$\mathcal H_*^{r-\alpha}(A)+\mathcal H_*^\alpha(B)=0\implies\mathcal H_*^r(A\times B)=0\tag6$$ using the definition of $\mathcal H_*^s$ , $(1)$ and maybe $(3)$ . However I found nothing. Some help will be appreciated. EDIT: to clarify some things: from the definitions of Hausdorff dimension the statement to be proved can be stated as $$
\begin{align}\dim_H(A)+\dim_H(B)&=\inf\left\{s+t>0:\sup_\epsilon\inf\left\{\sum_{k=0}^\infty a_k^r+b_k^s: a_k,b_k<\epsilon\right\}=0\right\}\\
&=\inf\left\{s+t>0:\sup_\epsilon\inf\left\{\sum_{k=0}^\infty c_k^{r+s}: c_k<\epsilon\right\}=0\right\}\\
&=\sup\left\{s+t\ge0:\sup_\epsilon\inf\left\{\sum_{k=0}^\infty a_k^r+b_k^s: a_k,b_k<\epsilon\right\}=\infty\right\}\\
&=\sup\left\{s+t\ge0:\sup_\epsilon\inf\left\{\sum_{k=0}^\infty c_k^{r+s}: c_k<\epsilon\right\}=\infty\right\}\\
&=\dim_H(A\times B)\end{align}\tag{*}
$$ where $a_k,b_k,c_k$ are the diameters of sequences of covers $(A_k),(B_k),(C_k)$ of $A\subset\Bbb R^n$ , $B\subset\Bbb R^m$ and $A\times B\subset\Bbb R^{n+m}$ respectively. This together with $(2)$ seems the way to go, however I can't found appropriate bounds because I can't relate covers $A$ and $B$ with covers of $A\times B$ such that it make possible to find these bounds. At my disposal, in the context where this exercise appear, there is not too many theorems to solve this exercise, by example I dont know the Frostman's lemma that @DavidUlrich state in the comment. I need to solve it from elementary theorems as the stated above. P.S.: I know some more identities related to the Hausdorff dimension, by example that is an increasing function or that $\dim_H(f(A))\le\dim_H(A)$ for $f$ Lipschitz or that $\dim_H(\bigcup_k A_k)=\sup_k\dim_H(A_k)$ .","['real-analysis', 'hausdorff-measure', 'dimension-theory-analysis', 'measure-theory', 'analysis']"
2757640,"Non linear Differential equation, sketching nullclines","Question Consider the system of ordinary differential equations \begin{equation}
  \begin{aligned}
\dot{x} &= 1 + y - \exp(-x) \\
\dot{y} &= x^3 -y
  \end{aligned}
\end{equation} Find and classify the fixed point(s) of this system Sketch the nullclines of the system and sketch a plausible phase portrait I have done the following : established that the only fixed point is (0,0) evaluated the Jacobian of the system at this fixed point So at (0,0) I have $$
\begin{bmatrix}
  1   & 1 \\
  0  & -1
\end{bmatrix}
$$ And the eigenvalues of this are $$
\lambda_1 = 1 , \lambda_2 = -1
$$ The eigenvectors for this are $$
v_1 = (1, 0)^T,
v_2 = (1, -2)^T
$$ From this point I'm unsure how to consider plotting. The plot that I have done is as follows : The solution is here : I don't understand how the green lines are formed there, and why they aren't
converging to the x axis. For my plot I have found the eigenvectors from the Jacobian at the fixed point,
and used those for the flow. My question is - how to know the direction of the green flow lines for a system
like this. I reviewed the following posts and was unable to answer my question : Plotting phase portrait of saddle node using
Nullclines , this question seemed as though it might have been similar, but there are no
sketches and the info is a bit sparse (for me). Help interpreting behaviour of a simple system of differential equations using
 nullclines and direction fields ,
 this question seems similar in nature, but the system is larger and the use of
 XPPAUT complicates things (for me).","['stability-in-odes', 'ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system']"
2757642,why symmetric matrix is always diagonalizable even when it has repeated eigenvalues?,"I am studying linear algebra and I have a very basic question. Why symmetric matrix is always diagonalizable even if it has repeated eigenvalues? I've seen that sufficient orthonormal eigenvectors can be generated by applying Gram-Schmidt process to the eigenspace of repeated eigenvalue. I know what this means. I have solved bunch of exercise problems and I've been always able to generate full set of orthonormal basis of eigenspace of repeated eigenvalue. But what I want to know is this: The dimension of eigenspace of repeated eigenvalue with multiplicity of ""k"" is always ""k""? Is it impossible that eigenspace of repeated eigenvalue of symmetric matrix is a 1-dimensional line? Thank you.","['eigenvalues-eigenvectors', 'diagonalization', 'linear-algebra']"
2757659,Hartshorne Exercise II. 6.3,"I've been turning this exercise over for a while, and I appear to be stuck in particular on part (c). The question is: Let $V$ be a projective variety in $\mathbb{P}^n$ of dimension $\geq 1$ and nonsingular in codimension $1$. Let $X$ be the affine cone over $V$ in $\mathbb{A}^{n+1}$, and $\bar{X}$ it's projective closure. Let $S(V)$ be the  homogeneous coordinate ring of $V$. Show $S(V)$ is a UFD if and only if $V$ is projectively normal and $\text{Cl } V \simeq \mathbb{Z}$, generated by the class of $V.H$ I can do one direction. Namely, if $S(V)$ is a UFD, then the following exact sequence $$\mathbb{Z} \to \text{Cl } V \to \text{Cl }X \to 0$$ can be modified to $$0 \to \mathbb{Z} \to \text{Cl }V \to 0 $$ since $\text{Cl }X$ is $0$, as $S(V)$ is also the coordinate ring of the cone $X$, and it is proven in the previous part that the map from $\mathbb{Z}$ is injective. This proves the divisor condition. However it is also true that a UFD is integrally closed, so projective normality follows. What I can't do is the other direction.  This should be just a statement about commutative algebra. I would like to use the equivalent condition that all prime ideals are principal, for example, but I don't know how to relate this to the divisor class group. Another possible approach that might be promising is the also equivalent condition that the ring satisfy the ascending chain condition on principal ideals (which is trivial, since the ring we're dealing with is already Noetherian, as a variety is a finite type scheme over a field and so is Noetherian) and every irreducible is prime. So it suffices to check that projective normality and having class group the integers is enough to prove every irreducible is prime. Is this the right way to go? Perhaps there is a better way?",['algebraic-geometry']
