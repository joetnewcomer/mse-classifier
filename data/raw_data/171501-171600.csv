question_id,title,body,tags
3041861,Measure of boundary of discontinuity point of integrable function,"Let $B\subset \mathbb{R}^n$ be a closed rectangle, $f:B\to\mathbb{R}$ Reimann integrable and $A\subset B$ a set of points where $f$ is not continous. I`m trying to show that the measure of $\partial A$ is $0$ ,where $\partial A$ is the boundary of $A$ . I know that the measure of $A$ is $0$ ,and that for general $0$ measure $A$ the statement isn`t correct, However, when $A$ is defined as the set of discontinuity points of an integrable function, I can't find a counter example. My attempt of a proof was: Take $A_\epsilon=\{x\in B : osc(f,x)\geq\epsilon\}$ ,and we have $A=\bigcup_{k=1}^\infty A_{\frac{1}{k}}$ We have $\partial A\subset \bigcup_{k=1}^\infty \partial A_{\frac{1}{k}}$ so it is sufficient to show $\partial A_{\frac{1}{k}}$ is of measure zero. $A_{\frac{1}{k}}$ is compact, and of measure $0$ (Following a proof I saw of the Lebesgue theorem), so we can find a finite cover of rectangles as small as we want, this is also a cover of the interior of $A_{\frac{1}{k}}$ , so the interior is of measure 0. Now we have $int(A_{\frac{1}{k}})\cup\partial A_{\frac{1} {k}}=A_{\frac{1}{k}}$ ( $A_{\frac{1}{k}}$ is closed). (*)Since the measure of $int(A_{\frac{1}{k}})=0$ and also the measure of $A_{\frac{1}{k}}=0$ , we must have $\partial A_{\frac{1}{k}}=0$ I`m unsure of my proof since because honestly, I find it hard to believe the statement is true. I`d be glad if someone can provide a counter example, or verify me and tell me this statement is actually correct.","['integration', 'multivariable-calculus', 'measure-theory', 'real-analysis']"
3041863,"Precise meaning of ""family of hypersurfaces""","I am a little confused by the term family of hypersurfaces . For simplicity, let $Y$ be a projective curve (and also given the embedding, so we can talk about the line bundle $\mathcal O_Y(1)$ ). When I say $X \to Y$ is a (flat) family of hypersurfaces in $\mathbb P^n$ of degree $d$ , I may mean the following: (i) $\pi\colon X \to Y$ is a (flat) morphism with every fiber isomorphic to some hypersurface in $\mathbb P^n$ of degree $d$ . (I believe the flatness is followed from the fact that each fibre is isomorphic to some hypersurface in $\mathbb P^n$ of degree $d$ , at least under some very weak assumption) (ii) The conditions in (i), moreover we assume $X\subset  Y\times \mathbb P^n$ and $\pi$ is the restriction of the first projection. (iii) The conditions in (ii), moreover we assume $X\subset  Y\times \mathbb P^n$ and $\pi$ is given by zero locus of some secion in $\mathcal O_Y(1)\boxtimes\mathcal O_{\mathbb P^n}(d)$ . (iv) The conditions in (iii), moreover we assume the embedding of $Y$ is $Y\subset \mathbb |\mathcal O_{\mathbb P^n}(d)|$ . I want to know that, are those contions all equivalent?","['definition', 'algebraic-geometry']"
3041910,"Is there a ""universal family of hypersurfaces""?","Consider the category of (flat) families of degree $d$ hypersurfaces in $\mathbb P^n$ , i.e. objects are $X \to Y$ whose fibers are degree $d$ hypersurfaces in $\mathbb P^n$ , and morphisms are the pullback diagrams. Then, is there a terminal (universal) element in this category? i.e. any family can be uniquely pullbacked by this family. I have considered the natural family on $|\mathcal O_{\mathbb P^n}(d)|$ , but the uniqueness does not hold. I tend to believe the universal family does not exists, but I don't know how to prove it.","['universal-property', 'algebraic-geometry', 'category-theory', 'reference-request']"
3041953,Equivalence relations and their classes,"Check for which $k$ given relations on set $\mathbb{N}$ are reflexive, symmetric or  transitive. For these relations, that are equivalence relations, describe their equivalence classes. $xR_ky \Longleftrightarrow k\:|\:(x+y)$ $xS_ky \Longleftrightarrow k\:|\:(x-y)$ $xT_ky \Longleftrightarrow x - y = k$ For the first example $xR_ky \Longleftrightarrow k\:|\:(x+y)$ , I tried to do it like this: check if the relation $R_k$ is reflexive, so $xR_kx \Longleftrightarrow k\:|\:(x+x)\equiv k\:|\:2x$ - from that we get that $2x$ is always divisible if $k=1$ or $k=2$ check if the relation $R_k$ is symmetric, so $\Big[xR_ky \Longleftrightarrow k\:|\:(x+y)\Big] \Longrightarrow \Big[yR_kx \Longleftrightarrow k\:|\:(y+x)\Big]$ , which is true, because addition is commutative. From that we can conclude, that $\exists n\in\mathbb{N} : x+y=k\cdot n$ . check if the relation $R_k$ is transitive, so $(xR_ky \wedge yR_kz) \Rightarrow xR_kz$ .
Thus $\exists n\in\mathbb{N} : x+y=k\cdot n$ and $\exists m\in\mathbb{N} : y+z=k\cdot m$ . And this is the first moment that I got stuck and I am not sure what to do next. Though I suppose that it is going to be an equivalence relation, but what would its equivalence classes look like? When it comes to examples (2) and (3) I can easily say that they are not equivalence relations, as in both cases $x-y \neq y-x$ , which tells us that the relation is not symmetric.","['elementary-set-theory', 'equivalence-relations', 'relations']"
3041980,Horrible limit envolving floor function,"Let $x\in [0,1]$ , $\ell\in\mathbb{Z}$ and $\tau>0$ . I want to calculate $$\lim_{L\to\infty}\sum_{k=0}^{\lfloor \tau L^2\rfloor}\frac{1}{2^{\lfloor \tau L^2\rfloor}}\binom{\lfloor \tau L^2\rfloor}{k}\cos\left(2\pi \ell\frac{\lfloor xL\rfloor-\lfloor \tau L^2\rfloor+2k}{L}\right).$$ I think the result is $\exp(-2\pi^2\ell^2\tau)\cos(2\pi\ell x)$ but I have no idea in how to prove it. I tried estimating the binomial with Stirling's aproximation but without success.","['limits', 'real-analysis']"
3042003,"$f^n+g^n=h^2\implies f,g,h$ all constant","Let $k$ be a field with char $(k)=0$ and suppose for $f,g,h\in k[x]$ having gcd $(f,g,h)=1$ and $n\in\mathbb{N}_{\geqslant4}$ it holds that $f^n+g^n=h^2$ . I want to show that $f,g,h$ are all constant. Of course, we go the Mason Stothers-way. Quick note: by gcd $1$ and the equation, we have that $f,g,h$ are pairwise co-prime. For the sake of contradiction, suppose that $f,g,h$ are not all constant. By the other conditions listed above, we can apply Mason-Stothers like so: $$\max(\deg f^n,\deg g^n,\deg h^2)\leqslant\deg(\text{rad}(fgh))-1.$$ Now let's look at two cases: 1. $f^n$ has maximal degree (and is non-constant). 2. $h^2$ has maximal degree. A contradiction should appear in both cases. Let's begin with 1. :
Since $f^n$ has max degree and the polynomials are pairwise co-prime, we can say $$\begin{align*}4\leqslant\deg(f^n)=n\deg(f)&<\deg({\text{rad}(fgh)})\\
&= \deg(\text{rad}(f)\text{rad}(g)\text{rad}(h))\\
&=\deg(\text{rad}(f))+\deg(\text{rad}(g))+\deg(\text{rad}(h))\\
&\leqslant3\deg(f).
\end{align*}$$ Now since $n>3$ , this is an immediate contradiction. Thanks for all the fast help!","['field-theory', 'number-theory', 'radicals', 'polynomials']"
3042057,"Let $f:(0;\infty)\times (0;\infty) \rightarrow \Bbb R$ be a Lebesgue measurable function, is $T_f(y)=\int_0^{+\infty} f(x,y) \, dy$ measurable?","Let $f:(0;\infty)\times (0;\infty) \rightarrow \Bbb R$ be a Lebesgue measurable function. Lets define $$T_f(y)=\int_0^{+\infty} f(x,y) \, dy$$ $$G_f(y)=\int_0^{+\infty} |f(x,y)| \, dy$$ By Tonelli's Theorem I know $G_f$ is measurable and I also know $G_f \in L^p(0;+\infty)$ (take it as an hypothesis). I would like to say $\Vert T_f \Vert _p \le \Vert G_f \Vert _p$ so $T_f \in L^p(0;+\infty)$ but I need $T_f$ to be measurable in order to be abel to calculate $\Vert T_f \Vert _p$ . Is this true? How can I prove it?","['lebesgue-measure', 'lebesgue-integral', 'analysis', 'real-analysis']"
3042059,What are the foundations of probability and how are they dependent upon a $\sigma$-field?,"I am reading Christopher D. Manning's Foundations of Statistical Natural Language Processing which gives an introduction on Probability Theory where it talks about $\sigma$ -fields. It says, The foundations of probability theory depend on the set of events $\mathscr{F}$ forming a $\sigma$ -field"". I understand the definition of a $\sigma$ -field, but what are these foundations of probability theory, and how are these foundations dependent upon a $\sigma$ -field?","['measure-theory', 'probability-theory']"
3042083,Solving an algebraic exercise using only one variable $x$,This exercise is supposed to be solved using only one  though I don't see how would that be possible. Could someone tell how to solve using only one unknown variable $x$ ? Thank you. Exercise. The sum of two numbers is $108$ and the double of the greater exceeds the triple of the minor in $156$ . Find the numbers. I solved using two variables $x$ and $y$ and specifying that $x>y$ .,"['algebra-precalculus', 'systems-of-equations']"
3042107,How do I find the volume of a parallelepiped given 4 vertices?,"""Find the volume of the parallelepiped by four vertices: $(0,1,0), (2,2,2), (0,3,0),$ and $(3,1,2)$ . I know the formula to find this volume is: $|\vec{a} \circ(\vec{b}\times \vec{c})|$ , and I know how to carry out the computation to get the actual value. What I need to know is the process of how I set up the values of the vectors $\vec{a},\vec{b},$ and $\vec{c}$ using the given points?","['multivariable-calculus', 'calculus', 'volume']"
3042130,Derivative of indefinite integral vs. definite,"So I understand what to do if I have $\frac{d}{dx} \int_a^b f(x)dx$ for some function $f$ . But if I have $\frac{d}{dx} \int f(x)dx$ - an indefinite integral -the derivative and the integral cancel each other out, and I just have the function: is that right? And if I have the $\int \frac{d}{dx} f(x)dx$ - it is just the function as well, correct? Thanks","['integration', 'indefinite-integrals', 'definite-integrals']"
3042138,"Let $V$ be a subspace of $\mathbb{R}^4$, spanned by $v$ and $u$. Find a linear transformation whose kernel is $V$.","And the vectors given are $v = (1,0,3,-2)$ and $u = (0,1,4,1)$ . It asks me to find the linear transformation from $\mathbb{R}^4$ to $\mathbb{R}^2$ , where the kernel of that transformation is $V$ . So what I know is that: the transformation I'm trying to find, applied to every vector in the span of $(1,0,3,-2)$ and $(0,1,4,1)$ , will give the zero vector. Please let me know if that interpretation is incorrect. I've really no idea how to get started on this question. I have the equation $Av = 0$ where $A$ is the matrix of the transformation in question, and v is any vector of the subspace V, but...I don't think that gets me anywhere. Any help is greatly appreciated.","['linear-algebra', 'linear-transformations']"
3042140,A generating set of cardinality $n$ in the free group $F_n$ is a free basis.,"Let $F_n$ be the free group on $n$ letters. Let $S=\{s_1,s_2,\ldots,s_n\}$ be a set of $n$ elements of $F_n$ .
 Is there any way to prove that $S$ is in fact a free basis for $F_n$ without using the Nielsen transformations?","['abstract-algebra', 'free-groups']"
3042183,Methods to solve $\int_{0}^{\infty} \frac{e^{-x^2}}{x^2 + 1}\:dx$,"I have a feel this will be a duplicate question. I have had a look around and couldn't find it, so please advise if so. Here I wish to address the definite integral: \begin{equation}
 I = \int_{0}^{\infty} \frac{e^{-x^2}}{x^2 + 1}\:dx
\end{equation} I have solved it using Feynman's Trick, however I feel it's limited and am hoping to find other methods to solve. Without using Residues, what are some other approaches to this integral? My method: \begin{equation}
 I(t) = \int_{0}^{\infty} \frac{e^{-tx^2}}{x^2 + 1}\:dx
\end{equation} Here $I = I(1)$ and $I(0) = \frac{\pi}{2}$ . Take the derivative under the curve with respect to ' $t$ ' to achieve: \begin{align}
 I'(t) &= \int_{0}^{\infty} \frac{-x^2e^{-tx^2}}{x^2 + 1}\:dx = -\int_{0}^{\infty} \frac{x^2e^{-tx^2}}{x^2 + 1}\:dx \\
&= -\left[\int_{0}^{\infty} \frac{\left(x^2 + 1 - 1\right)e^{-tx^2}}{x^2 + 1}\:dx \right] \\
&= -\int_{0}^{\infty} e^{-tx^2}\:dx  + \int_{0}^{\infty} \frac{e^{-tx^2}}{x^2 + 1}\:dx \\
&= -\frac{\sqrt{\pi}}{2}\frac{1}{\sqrt{t}} + I(t)
\end{align} And so we arrive at the differential equation: \begin{equation}
 I'(t) - I(t) = -\frac{\sqrt{\pi}}{2}\frac{1}{\sqrt{t}}
\end{equation} Which yields the solution: \begin{equation}
 I(t) = \frac{\pi}{2}e^t\operatorname{erfc}\left(t\right)
\end{equation} Thus, \begin{equation}
 I = I(1) \int_{0}^{\infty} \frac{e^{-x^2}}{x^2 + 1}\:dx =  \frac{\pi}{2}e\operatorname{erfc}(1)
\end{equation} Addendum: Using the exact method I've employed, you can extend the above integral into a more genealised form: \begin{equation}
 I = \int_{0}^{\infty} \frac{e^{-kx^2}}{x^2 + 1}\:dx =  \frac{\pi}{2}e^k\operatorname{erfc}(\sqrt{k})
\end{equation} Addendum 2:
Whilst we are genealising: \begin{equation}
 I = \int_{0}^{\infty} \frac{e^{-kx^2}}{ax^2 + b}\:dx =  \frac{\pi}{2b}e^\Phi\operatorname{erfc}(\sqrt{\Phi})
\end{equation} Where $\Phi = \frac{kb}{a}$ and $a,b,k \in \mathbb{R}^{+}$","['integration', 'definite-integrals', 'real-analysis']"
3042256,"If $f(x) \geq g(x)\ \forall x \in [0,1]$, prove that $\int_0^1 f \geq \int_0^1 g$","Suppose that $f, g: [0,1] \rightarrow \mathbb{R}$ are Riemann integrable and $f(x) \geq g(x)\ \forall x \in [0,1]$ . Show that $\int_0^1 f \geq \int_0^1 g$ . So this should be relatively simple. We have $f$ is Riemann integrable
if $L(f) = U(f)$ . Where $L(f) = sup\{L(f,P)\ |\ \text{P a partition}\}$ , $U(f) = inf\{U(f,P)\ |\ \text{P a partition}\}$ . Now we have that $f(x) \geq g(x)\ \forall x \in [0,1]$ . Because this holds, we have $L(f) \geq L(g)$ . Since $\int_0^1$ is the common value $L(f) = U(f)$ . We have $U(f) \geq U(g)$ as well. And so $\int_0^1 f \geq \int_0^1 g$ . Not too versed in proving stuff of this sort so would appreciate help/clarification.","['integration', 'proof-verification', 'real-analysis', 'functions', 'riemann-integration']"
3042291,$\int_{0}^{\infty} \frac{1}{1 + x^r}\:dx = \frac{1}{r}\Gamma\left( \frac{r - 1}{r}\right)\Gamma\left( \frac{1}{r}\right)$ [duplicate],"This question already has answers here : Prove $\int_0^{\infty}\! \frac{\mathbb{d}x}{1+x^n}=\frac{\pi}{n \sin\frac{\pi}{n}}$ using real analysis techniques only (5 answers) Closed 5 years ago . As part of a recent question I posted, I decided to try and generalise for a power of $2$ to any $r \in \mathbb{R}$ . As part of the method I took, I had to solve the following integral: \begin{equation}
 I = \int_{0}^{\infty} \frac{1}{1 + x^r}\:dx
\end{equation} I believe what I've done is correct, but I'm concerned that I may missed something (in particular whether it holds for all $r \neq 0$ ). So, here I have two questions (1) Is what I've done correct? and (2) What other methods can be employed that doesn't rely on complex analysis? Here is the method I took: First make the substitution $u = x^{\frac{1}{r}}$ to arrive at \begin{equation}
 I = \frac{1}{n} \int_{0}^{\infty} \frac{1}{1 + u} \cdot u^{1 -\frac{1}{r}}\:du
\end{equation} We now substitute $t = \frac{1}{1 + u}$ to arrive at: \begin{align}
 I &= \frac{1}{r} \int_{1}^{0} t \cdot \left(\frac{1 - t}{t}\right)^{\frac{1}{r} -1}\frac{1}{t^2}\:dt = \frac{1}{r}\int_{0}^{1}t^{-\frac{1}{r}}\left(1 - t\right)^{ \frac{1}{r} - 1}\:dt \\
&= \frac{1}{r}B\left(1 - \frac{1}{n}, 1 + \frac{1}{r} - 1\right) = \frac{1}{r} B\left(\frac{r - 1}{r},\frac{1}{r}\right) \\
&= \frac{1}{r} B\left(\frac{r - 1}{r},\frac{1}{r}\right)
\end{align} Wheer $B(a,b)$ is the Beta function. Using the relationship between the Beta and Gamma function we arrive at: \begin{equation}
I = \frac{1}{r} \frac{\Gamma\left( \frac{r - 1}{r}\right)\Gamma\left( \frac{1}{r}\right)}{\Gamma\left(\frac{r - 1}{r} + \frac{1}{r}\right)} = \frac{1}{r}\Gamma\left( \frac{r - 1}{r}\right)\Gamma\left( \frac{1}{r}\right)
\end{equation} And so, we arrive at: \begin{equation}
 I = \int_{0}^{\infty} \frac{1}{1 + x^r}\:dx = \frac{1}{r}\Gamma\left( \frac{r - 1}{r}\right)\Gamma\left( \frac{1}{r}\right)
\end{equation} for $r > 1$ As per KemonoChen 's comment and others, we can employ Euler's Reflection Formula to position this result for $\frac{1}{r} \not \in \mathbb{Z}$ Here, as $r \in \mathbb{R}, r > 1 \rightarrow \frac{1}{r} \not \in \mathbb{Z}$ and so our formula holds. \begin{equation}
 I = \int_{0}^{\infty} \frac{1}{1 + x^r}\:dx = \frac{1}{r}\Gamma\left( \frac{r - 1}{r}\right)\Gamma\left( \frac{1}{r}\right) = \frac{\pi}{r\sin\left(\frac{\pi}{r} \right)}
\end{equation} Thank you also to Winther , Jjagmath , and MrTaurho 's for their comments and corrections/clarifications.","['integration', 'big-list', 'definite-integrals', 'real-analysis']"
3042298,A.S. convergence of sum of square-integrable independent random variables with summable variation,"I'm working on the following exercise from Achim Klenke's ""Probability Theory: A Comprehensive Course"" (exercise 6.1.4): Let $X_1, X_2, \ldots$ be independent, square integrable, centered random variables with $\sum_{i=1}^\infty \mathbf{Var}[X_i] < \infty$ . Show that there exists a square integrable $X$ with $X = \lim_{n \to \infty} \sum_{i=1}^n X_i$ almost surely. Chebyshev's inequality gives us $$
\mathbf P\left[|S_m - S_n| > \epsilon\right] \leq \epsilon^{-2} \mathbf{Var}\left[ \sum_{i=m+1}^n X_i\right] = \epsilon^{-2} \sum_{i=m+1}^n \mathbf{Var}\left[X_i\right] \xrightarrow{m,n \to \infty} 0.
$$ whence $(S_n)_{n \in \mathbb N}$ is a Cauchy sequence in probability. Thus $S_n \xrightarrow{\mathbf P} X$ . Using a similar strategy, we can in fact show that $S_n \to X$ in $L^2$ . Now, to prove almost sure convergence, I'd like to use the following result (Corollary 6.13 in Klenke): Let $(E,d)$ be a separable metric space. Let $f, f_1, f_2, \ldots$ be measurable maps $\Omega \to E$ . Then the following statements are equivalent. (i) $\quad f_n \to f$ in measure as $n \to \infty$ . (ii) $\quad$ For any subsequence of $(f_n)_{n \in \mathbb N}$ , there exists a sub-subsequence that converges to $f$ almost everywhere. and somehow use the fact that we're working with a sum of centered random variables to show that in fact every subsequence converges a.s. But I'm not sure how to do this since our $X_i$ are not nonnegative. I tried reconstructing the proof of this theorem, but I've only been able to show once again that there are a.e. convergent subsequences. My other thought was to apply the Borel-Cantelli lemma to the events $B_n(\epsilon) := \left\{ |X - S_n| > \epsilon\right\}$ and prove that $\limsup_{n \to \infty} B_n(\epsilon) =: B(\epsilon)$ has probability $0$ , but in the latter case I don't know how to approximate the probability of $B_n(\epsilon)$ . Chebyshev doesn't seem available to us since strictly speaking we don't know what $X$ looks like, only that $S_n$ converges in $L^2$ to it. Even if we could say $X - S_n = \sum_{i=n+1}^\infty X_i$ , the above approximation using Chebyshev with $|X - S_n|$ instead of $|S_m - S_n|$ would work out to $$
\mathbf P\left[|X - S_n| > \epsilon\right] \leq \epsilon^{-2} \sum_{i=n+1}^\infty \mathbf{Var}[X_i]
$$ which would sum to $\epsilon^{-2} \sum_{n=1}^\infty n\mathbf{Var}[X_n]$ , but I don't see why this series converges. Any thoughts on how to prove $S_n \to X$ almost surely?","['real-analysis', 'borel-cantelli-lemmas', 'convergence-divergence', 'probability-theory', 'probability']"
3042328,Finding a formula for the number of $0$-$1$ sequences of length $n$ having a $0$ in any consecutive $m$ digits,"Let $m, n$ be positive integers. Let $S(n,m)$ be the number of sequences of length $n$ and consisting of $0$ and $1$ in which there exists a $0$ in any consecutive $m$ digits. Find a formula for $S(n,m)$ . Let $P(n,m)$ denote the number of strings in which there exists at least one block of $m$ consecutive $1$ 's. We have $S(n,m)=2^{n}-P(n,m)$ . Let $P_i(n,m)$ denote the number of strings in which there exists at least one block of $m$ consecutive $1$ 's from left starting from the $(i+1)$ 'th position and ending in the $(i+m)$ 'th position, with no blocks of $m$ $1$ 's starting before the $i+1$ 'th position. We shall have: $$P(n,m)=\sum_{i=0}^{n-m-1}{P_i(n,m)}$$ We shall first count $P_i(n,m)$ . From the $1$ 'st position to the $i-1$ 'th position there are $S(i-1,m)$ ways to choose $1$ 's and $0$ 's. The $i$ 'th position must be $0$ , and from the $i+1$ 'th to the $i+m$ 'th position, there must be $m$ number $1$ 's. From the $i+m+1$ 'th position to the $n$ 'th position, we can choose either $1$ or $0$ , there are $2^{n-m-i}$ ways to choose. Therefore: $$P_i(n,m)=2^{n-m-i}S(i-1,m)=2^{n-m-1}-2^{n-i-m}P(i-1,m)$$ Thus $$P(n,m)=(n-m+1)2^{n-m-1}-\sum_{i=0}^{n-m}{2^{n-m-i}P(i-1,m)}$$ with $P(-1,m)=1$ . Here I am stuck. How can I progress ? Or is there a better way to find a general formula for $S(n,m)$ ? Do such formula exist? (Please let me know if this question should be closed. Sorry for my English grammar)","['number-theory', 'calculus', 'combinatorics', 'elementary-number-theory']"
3042335,Seeking Methods to solve $\int_{0}^{\infty} \frac{e^{-x^n}}{x^n + 1}\:dx $,"As an extension of a question I posed earlier , I thought it would be best to try and final the result for a more general form: \begin{equation}
I_n = \int_{0}^{\infty} \frac{e^{-x^n}}{x^n + 1}\:dx 
\end{equation} with $n \in \mathbb{R}, n > 1$ As with the previous question, I'm interested in finding alternative ways of solving this that does not rely on complex analysis. My Method: I employ the exact same method as with my earlier question. Here first let \begin{equation}
J_n(t) = \int_{0}^{\infty} \frac{e^{-tx^n}}{x^n + 1}\:dx 
\end{equation} We see that $I_n = J_n(1)$ and that $J_n(0) = \frac{1}{n}\Gamma\left(1 - \frac{1}{n}\right)\Gamma\left(\frac{1}{n}\right)$ (This is shown here ) Now, take the derivative with respect to ' $t$ ' to achieve \begin{align}
 J_n'(t) &= \int_{0}^{\infty} \frac{-x^ne^{-tx^n}}{x^n + 1}\:dx = -\int_{0}^{\infty} \frac{\left(x^n + 1 - 1\right)e^{-tx^n}}{x^n + 1}\:dx \\
&= -\left[\int_{0}^{\infty}e^{-tx^n}\:dx - \int_{0}^{\infty}\frac{e^{-tx^n}}{x^n + 1}\:dx   \right] \\
&= -\left[ \frac{t^{-\frac{1}{n}}}{n}\Gamma\left(\frac{1}{n}\right) -J_n(t)\right]
\end{align} Which yields the differential equation: \begin{equation}
 J_n'(t) - J_n(t) = -\frac{t^{-\frac{1}{n}}}{n}\Gamma\left(\frac{1}{n}\right)
\end{equation} Which yields the solution: \begin{equation}
 J_n(t) = \frac{1}{n}\Gamma\left(1 - \frac{1}{n}, t\right)\Gamma\left(\frac{1}{n}\right)e^t
\end{equation} And finally: \begin{equation}
 I_n = J_n(1) = \int_{0}^{\infty} \frac{e^{-x^n}}{x^n + 1}\:dx = \frac{e}{n}\Gamma\left(1 - \frac{1}{n}, 1\right)\Gamma\left(\frac{1}{n}\right)
\end{equation} Which for me, is a nice result. Fascinated to see other methods! Edit - Thanks to spaceisdarkgreen for the pickup on my mistyping of the Incomplete Gamma Function.","['integration', 'gamma-function', 'definite-integrals', 'real-analysis']"
3042343,"$\int_{[0,1]} \int_{[0,1]} \frac {x^2-y^2}{(x^2+y^2)^2}\,dx\,dy$ and $\int_{[0,1]} \int_{[0,1]} \frac {x^2-y^2}{(x^2+y^2)^2}\,dy\,dx$","I am facing problem in calculating $$I_1=\int_{[0,1]} \int_{[0,1]} \frac {x^2-y^2}{(x^2+y^2)^2} \,dx\,dy$$ and $$I_2=\int_{[0,1]} \int_{[0,1]} \frac {x^2-y^2}{(x^2+y^2)^2} \,dy\,dx$$ after substituting $x=r\cos \theta$ and $y=r\sin \theta$ we have $I_1=\int_{[-\pi,\pi]} \int_{[0,1]}\frac{\cos(2\theta)}{r^2}r\,dr\, d\theta$ so I am getting $\infty \times 0=0$ So I am not getting $\pi/4$ and $-\pi/4$ resp. Maybe there is a silly point I am missing. Please help.","['integration', 'multivariable-calculus', 'calculus', 'definite-integrals']"
3042344,Evaluating $\sum^{\infty}_{k=1}\frac{k^2}{(2k-1)(2k)(2k+1)(2k+2)}$,"Finding sum of series $$\displaystyle \sum^{\infty}_{k=1}\frac{k^2}{(2k-1)(2k)(2k+1)(2k+2)}$$ Try: Let $$S = \displaystyle \sum^{\infty}_{k=1}\frac{k^2}{(2k-1)(2k)(2k+1)(2k+2)}$$ So, $$S =\sum^{\infty}_{k=1}\frac{k^2\cdot (2k-2)!}{(2k+2)!}=\frac1{3!}\sum^{\infty}_{k=0}\frac{(k+1)^2\cdot(2k)!\cdot 3!}{(2k+3+1)!}$$ with the help of identity $$B(m,n) = \int^{1}_{0}x^m(1-x)^ndx = \frac{\Gamma (m+1)\Gamma(n+1)}{\Gamma(m+n+2)}$$ $$B(m,n) = \frac{\Gamma (m+1)\Gamma(n+1)}{\Gamma(m+n+2)}=\frac{m!\cdot n!}{(m+n+1)!}$$ So $$S=\sum^{\infty}_{k=0}(k+1)^2\int^{1}_{0}(x)^{2k}(1-x)^3dx$$ $$S=\int^{1}_{0}x^{-2}(1-x)^3\sum^{\infty}_{k=1}(kx^k)^2dx$$ Can someone explain me how to calculate $\displaystyle \sum^{\infty}_{k=1}k^2x^{2k}$ in some short way . although I am trying to solve it but it is too lengthy. Please explain to me ,thanks.",['sequences-and-series']
3042392,Counting the directed paths in a particular directed graph,"I want to find out how many directed simple paths from $s$ to $t$ are in the following directed graph $G=(V,E)$ . $$\begin{align}
V=&\{s, v_1, v_2,\ldots, v_n, t\}, \quad n=2k, k \in \mathbb{N} \\
E=&\{ (s, v_1), (s, v_2), \\
&\;(v_1,v_3), (v_1,v_4), (v_2,v_3),(v_2,v_4), \\
&\;(v_3,v_5), (v_3,v_6), (v_4,v_5), (v_4,v_6), \\
&\;\ldots, \\
&\;(v_{n-5},v_{n-3}), (v_{n-5},v_{n-2}), (v_{n-4},v_{n-3}), (v_{n-4},v_{n-2}), \\
&\;(v_{n-3},v_{n-1}), (v_{n-3},v_{n}), (v_{n-2},v_{n-1}), (v_{n-2},v_{n}), \\
&\;(v_{n-1},t), (v_{n},t) \}
\end{align}$$ In my opinion, there are $n$ directed paths. Is that right?","['graph-theory', 'combinatorics', 'discrete-mathematics', 'directed-graphs']"
3042405,Equation of secant line in mean value theorem proof,"I'm going through a proof for the mean value theorem. We have a function $f(x)$ continuous on $[a, b]$ and differentiable on $(a, b)$ . Then we define a function $g(x)$ to be the secant line passing through $(a, f(a))$ and $(b, f(b))$ . The slope of said secant is: $$m=\frac{f(b)-f(a)}{b-a}$$ That is clear. Now the proof I'm following defines $g(x)$ like so: $$g(x) = \left[  \frac{f(b)-f(a)}{b-a} \right](x-a)+f(a)$$ What confuses me: why is the coefficient defined to be $(x-a)$ and not simply $(x)$ .","['proof-explanation', 'calculus', 'derivatives']"
3042416,Given a trapezoid with base $AD$ larger than side $CD$. The bisector of $\angle D$ meets $AB$ at $K$. Prove $AK > KB$,"We have a trapezoid $ABCD$ with base $AD$ larger than side $CD$ . The bisector of $\angle D$ intersects side $AB$ at point $K$ . Prove that $AK>KB$ . All that I have tried was to make such drawing in GeoGebra, which obviously showed me that $AK>KB$ , even if I extend $AD$ very, very long. I think the solution should go somehow through similar triangles, but I honestly have no idea how. I would really appreciate any help you provide. To mention more, I seriously don't need the entire solution. Even a little hint would be very helpful for me, since I don't really know where to start. EDIT: key mistake was made in the previous problem: it's not $AD$ that's larger than $BC$ , but $AD$ is larger than side $CD$ .","['quadrilateral', 'euclidean-geometry', 'geometry']"
3042463,"Showing any linear operator $T : X \to Y$ is bounded, where $X$ is a finite dimensional normed vector space, and $Y$ any normed vector space.","Let $X$ be a ﬁnite dimensional normed vector space and $Y$ an arbitrary normed vector space. Show that any linear operator $T : X \to Y$ is bounded. I got the hint to first show that $\| x\|_0 := \| x \| + \| Tx\|$ , $x \in X$ , deﬁnes a norm on $X$ , but I do not know how this should help me. Further I should calculate $\|T\|$ for where $X = K^n$ , equipped with the Euclidean norm $\|\cdot\|_2$ , $Y := \ell_1(\mathbb{N})$ and $Tx := (x_1,\ldots,x_n,0,0,\ldots) \in \ell_1(\mathbb{N})$ , for all $x = (x_1,\ldots,x_n) \in K^n$ . Can please someone help?","['normed-spaces', 'vector-spaces', 'functional-analysis']"
3042464,Find the geometric locus $z \in \mathbb C$ so that $\frac{z+2}{z(z+1)}\in \mathbb R$,"Find the geometric locus of the set of $z \in \mathbb C$ so that $$\frac{z+2}{z(z+1)}\in \mathbb R$$ Source: IME (Military Engineering Institute, Brazil, entrance exam, 1974) My attempt: With the notation $z=a+bi$ , the solution provided in a book is either the line $b=0$ or the circle $(a+2)^2+b^2=2$ but I could not find it, or not able to recognize this locus set from the algebraic development I did (or the solution or the statement has some mistake). Hints and solutions are welcomed.","['contest-math', 'algebra-precalculus', 'complex-numbers']"
3042500,"Bounded function in $[0,1]$ without max and min.","Do we have a function $f$ defined in $[0, 1]$ , which is bounded but has no maximum and minimum ? I do know that $\arctan x$ can give me a hint, which is bounded without max and min but that's in $\mathbb R$ .
Thanks! EDITED : Firstly thank all the answers that I received. I would like to ask a little additional question : could we find some edited or mixed trigonometric functions to satisfy this problem? If yes, a example please! The reason is, that I got stuck with the idea of $\arctan x$ and other possible trigonometric functions. Thanks!","['calculus', 'trigonometry', 'real-analysis']"
3042526,Why does the answer to $(x^2+1)y''+xy'-y=0$ converge for $|x|<1$?,"For $$(x^2+1)y''+xy'-y=0,$$ the answer is $$y=C_1x +C_0\left(1+\frac{x^2}{2} - \frac{x^4}{2^2\cdot2!}+ ......\right)$$ According to my textbook, this power series only converges for $|x|<1$ because $x= ±i$ is the questions singular point. Can anyone explain why this is the case?","['power-series', 'ordinary-differential-equations']"
3042529,How to prove $\dfrac{d}{dx}e^x=e^x$? [duplicate],"This question already has answers here : Proving that $\lim\limits_{x \to 0}\frac{e^x-1}{x} = 1$ (8 answers) Closed 5 years ago . I have a problem : How to prove $$\dfrac{d}{dx}e^x=e^x\;?$$ My answer is \begin{eqnarray}
\dfrac{d}{dx}e^x&=&\lim\limits_{h\to 0}\dfrac{e^{x+h}-e^x}{h}\\
&=&\lim\limits_{h\to 0}\dfrac{e^{x}e^h-e^x}{h}\\
&=&e^{x}\lim\limits_{h\to 0}\dfrac{e^h-1}{h}\\
\end{eqnarray} But I don't know to find $\lim\limits_{h\to 0}\dfrac{e^h-1}{h}$ . So how to find it? Note:  I cannot use L'Hospital rule.","['limits-without-lhopital', 'calculus', 'derivatives', 'exponential-function']"
3042546,"A triangle has sides $a$, $b$, $c$ and medians $m_a$, $m_b$, $m_c$. Show $(ab+bc+ca)(\frac{1}{a}+\frac{1}{b}+\frac{1}{c})\geq 2\sqrt{3}(m_a+m_b+m_c)$","Let $\triangle ABC$ have sides $BC=a$ , $CA=b$ , and $AB=c$ . Let $m_a$ , $m_b$ , $m_c$ be the medians to $BC$ , $CA$ , and $AB$ , respectively. Prove that $$(ab+bc+ca)\left(\frac{1}{a}+\frac{1}{b}+\frac{1}{c}\right)\geq 2\sqrt{3}(m_a+m_b+m_c)$$ My trying: $$\Leftrightarrow (ab+bc+ca)\left(\frac{1}{a}+\frac{1}{b}+\frac{1}{c}\right)\geq 3\sqrt{3}\sqrt{a^2+b^2+c^2}$$ Because: $$m_a+m_b+m_c\leq \sqrt{3(m_{a}^{2}+m_{b}^{2}+m_{c}^{2})}=\frac{3}{2}\sqrt{a^2+b^2+c^2}$$ $$\Leftrightarrow \left(p^2+4Rr+r^2\right)\left(\frac{4R+r}{pr}
\right)\geq 3\sqrt{3}\sqrt{2p^2-8Rr-2r^2}$$ I square it, but that doesn't help.","['geometry', 'triangles', 'geometric-inequalities', 'sum-of-squares-method', 'inequality']"
3042547,How did Euler show that number is idoneal?,"Euler famously showed that there are at least 65 idoneal (convenient) numbers. This was Euler's definition of idoneal number: Number $n$ is idoneal if following holds: Let $m>1$ be an odd number relatively prime to n which can be written in the form $x^2+ny^2$ with $x,y$ relatively prime. If the equation $m = x^2 + ny^2$ has only one solution with $x,y\ge0$ , then $m$ is a prime number. How did Euler prove that for example $15$ or $168$ , or any other, is in fact idoneal? I am not interested in proof with Gauss's genus theory, or anything sophisticated. I am interested in techniques that were available to Euler.","['number-theory', 'elementary-number-theory', 'math-history', 'prime-numbers', 'quadratic-forms']"
3042558,Does a function with these properties exist?,"For $x_1, x_2, x_3 \in \mathbb{Z}^+$ , does there exist a function $f(\cdot)$ defined on $\mathbb{Z}^+$ , not necessarily continuous or differentiable, such that: $$f(x_1) > f(x_2) \\ f(x_2) > f(x_3) \\ f(x_3) > f(x_1) $$ My immediate thought is that no such function exists, since a function can only be a many-to-one or one-to-one relation, and the above would require a one-to-many relation. If so, is there a more formal way of showing that the above is impossible? Or is it trivial to observe? If I am wrong and such a function does exist, what is an example of one?",['functions']
3042573,"Prove: $f$ is a constant function, as $\forall a \in \mathbb{R} \ \exists \lim_{x \to a}f(x) \ \ and \ \ \forall n : \frac{1}{n}$ is a period of $f$","Given $f: \mathbb{R} \to \mathbb{R}$ ,  for every $a \in \mathbb{R}$ there exists $\lim_{x \to a}f(x)$ .
Also. for every $n \in \mathbb{N}: \frac{1}{n}$ is a period of $f$ such that: $$\forall x \in \mathbb{R}, \forall n \in \mathbb{N}: \  f(x) = f(x+ \frac{1}{n})$$ Prove\disprove: $f$ is a constant function. my attempt:
I understand that you need to divide the proof for two cases, the first for $x \in \mathbb{R/Q}$ and the other for $x \in \mathbb{Q}$ . I can't understand how to formal this ideas.","['limits', 'calculus', 'functions', 'real-analysis']"
3042581,Dual spaces of uniformly convex Banach spaces,"I am interested in the dual spaces of uniformly convex Banach spaces. Given a uniformly convex Banach space $X$ , can anything be said about uniform convexity of its dual space $X^*$ ? Or given a uniformly convex dual space $X^*$ , can anything be said about the uniform convexity of $X$ ? I would like links to any papers discussing these points, or any counterexamples. Thank you.","['banach-spaces', 'analysis', 'reference-request', 'functional-analysis', 'convex-analysis']"
3042618,Stability VS orbital stability in ODEs,"Let $\dot x=f(x)$ and ODE. We say that $x$ is a stable solution if $\forall \varepsilon >0$ , $\exists \delta >0$ s.t for all solution $y(t)$ s.t. $\|x(t_0)-y(t_0)\|<\delta \implies \|x(t)-y(t)\|<\varepsilon .$ Let $\Phi_t$ denote the flow of the ODE, i.e. $x(t)=\Phi^{t,t_0}(x_0)$ for $x(t)$ is a solution. We say that $x$ is orbitaly stable if $\forall \varepsilon >0$ , $\exists \delta >0$ s.t. for all $\xi$ s.t. $\|x(t_0)-\Phi^{t_0,0}(\xi)\|<\delta \implies \text{dist}(\mathcal O^+(x_0,t_0),\Phi^{t,t_0}(\xi))<\varepsilon $ where $\mathcal O^+(x_0,t_0)=\{\Phi^{t,t_0}(x_0)\mid t\geq t_0\}$ . I see what is an stable solution, but I can't see what is an orbitaly stable solution. I don't really understand the example. Could someone explain ?","['ordinary-differential-equations', 'real-analysis']"
3042668,Continuous bijection between compact and Hausdorff spaces is a homeomorphism,"Exercise : Let $f:(X,\tau)\to (Y,\tau_1)$ be a continuous bijection. If $(X,\tau)$ is compact and $(Y,\tau_1)$ is Hausdorff, prove that $f$ is a homeomorphism. I tried to prove this on the following way: First I proved the following Lemma: Lemma : If $(X,\tau)$ and $(Y,\tau_1) $ are compact Hausdorff spaces and $f:(X,\tau)\to(Y,\tau_1)$ is a continuous mapping then $f$ is a closed mapping. Proof : If $A\subset X$ is compact than it is closed in $(X,\tau)$ . Then if $\{a_n:n\in\mathbb{N}\}$ is an arbitrary sequence in A then by the compactness there is a subsequence that converges in A such that $\lim_{n\to\infty}a_{in}=a$ where $a \in A$ . By continuity of $f$ , $\lim_{n\to\infty} f(a_{in})=f(a)$ so that $f(a)\in f(A)$ . So $f(A)$ is compact since the space $(Y,\tau_1)$ is compact then $f(A)$ is closed. So $f$ is a closed mapping. In the Exercise the function is continuous so if $B\in\tau_1$ then $f^{-1}(B)\in\tau$ , now it is left to show that $f$ send open sets to open sets. This is where my problem begins: Compactness is going to be preserved by continuity of $f$ , then $(Y,\tau_1)$ must be compact as every image of a subset of $(X,\tau)$ that would imply that $f$ is a closed mapping by the Lemma. If $C$ is a closed set in $X,\tau$ then $f(X\setminus C)=X\setminus f(C)$ which must be open. However I am not certain about this last step. Question: How should I solve the question? Is my proof right? Thanks in advance!","['general-topology', 'compactness']"
3042671,"How can ""relative frequency histogram"" become a ""probability density curve""?","Suppose I've rolled two dice and took the sum, for $25$ times; then plotted the results on below  histogram. If I added up all the heights, I get the total $25$ as expected : $$1+1+1+3+2+7+2+1+4+2+1=25$$ No issues so far. Next, if I want a relative frequency histogram, I just need to scale the heights of the bars by $1/25$ . Here if I add up all the heights , I will get $1$ . No issues here too. In this video of khan academy and everywhere they say that the ""area"" under a relative frequency histogram equals 1 . I don't know how this is true and it is throwing me off completely. I only see that the heights add up to 1 . Maybe I'm missing something... Any help ? EDIT : In above histogram the bin width is $1$ , so it may not be a good example. Kindly also consider general histograms like below :","['statistics', 'probability-distributions']"
3042734,What are the differences between these two methods of Semi-logarithmic plotting?,"I have my measurement data in (x, y). I am trying to plot a semi-logarithmic plot of y versus log (x). It looks like that there are two ways to plot such a graph. Transform the values of x to log (x) for all values of x. Then, I could plot (log x, y) similar to that of plotting (x, y). Arrange y in log scale (example, for a base of 10, arrange x axis by a decade), and plot the data set exactly like I would plot (x, y) in this logarithmic scale. In other words, I do not need to compute logarithmic values of x. Instead, I could simply rearrange x axis in logarithmic scale and then plot (x, y). Here are my two questions: Are both the options discussed above the same thing in terms of data representation? If both methods of plotting are the same, I would assume the slope of data set would be the same using both the methods. I would like to show that my variable in y is linear with log of the variable in x axis. In order to do so, I plotted the (x, y) data set using the second method of plotting discussed above and did a linear fit. I got a R2 value close to 0.95. Please provide comments or your suggestions about the validity of this method.","['statistics', 'graphing-functions', 'logarithms']"
3042787,"Prove that in a Hilbert space, $\lim_{k \rightarrow \infty} \langle x_{n_k} , y\rangle =\langle x,y \rangle$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Let $\mathscr{H}$ be a Hilbert space and $(x_n)_n$ a bounded sequence in $\mathscr H$ . How can I show that there exists a subsequence $(x_{n_k})_k$ and an $x \in \mathscr{H}$ so that $$ \lim_{k \rightarrow \infty} \langle x_{n_k},y\rangle= \langle x,y\rangle $$ for all $ y \in \mathscr{H} $ ? I know some of such proofs, but I don't know how to show that for a Hilbert space ! As a hint I got : using the diagonal consequences argument.
Appreciate any help of you !","['hilbert-spaces', 'functional-analysis']"
3042791,Continuity of the functional on the space $L_1$.,"The norm of the space $\mathbb E = L_1(0,1)$ is $$\Vert f \Vert = \int_{0}^1 \vert f(t) \vert dt.$$ We have $$Tf(t) = \int_{0}^t f(t)dt. $$ Show that T is continuous. I started from: $\Vert Tf(t) \Vert = \int_{0}^1 \vert Tf(t) \vert dt =   \int_{0}^1 \vert \int_{0}^t f(t) dt\vert dt \le  \int_{0}^1 \int_{0}^t \vert f(t) \vert  dt dt = \int_{0}^t \int_{0}^1 \vert f(t) \vert  dt dt = \int_{0}^t \Vert f \Vert dt = \Vert f \Vert \int_{0}^t dt = \Vert f \Vert t. $ I get $ \Vert f \Vert t $ lastly. But I need to get $\Vert Tf(t) \Vert \le M \Vert f \Vert$ , where $M>0$ (const). Can someone tell me where I made mistake?","['continuity', 'operator-theory', 'functional-analysis']"
3042861,Is the kernel of a matrix its nullspace and nothing more? And what is the term for the dimension of a kernel?,"Are null space and kernel perfect synonyms? If so, why are there two different terms for them? Also, does the term ""nullity"" have a ""kernel"" equivalent? Nullity refers to the dimension of the null space (and thus dimension of the kernel), and seems very important, so is there another term I should be aware of that refers to nullity?","['matrices', 'linear-algebra', 'terminology']"
3042947,Characterization of Wasserstein convergence,"Let $(X,d)$ be a complete metric space and define $$\mathcal{P}_2(X) := \{ \mu \text{ Borel probability measure} \mid \int_X d^2(x,x_0) d\mu(x) < \infty \text{ for some } x_0 \in X  \}$$ endowed with the Wasserstein distance $$ W^2_2(\mu, \nu) = \inf \{ \int_{X \times X} d^2(x,y) d\pi(x,y) \mid \pi \in \Gamma(\mu, \nu) \} $$ where $\Gamma(\mu, \nu)$ is the set of probability measure on $X \times X$ which marginals are $\mu$ and $\nu$ . I this setting, I am trying to understand the proof of Theorem If $\mu \in \mathcal{P}_2(X)$ and $ \{ \mu_n \} \subset \mathcal{P}_2(X)$ then $$ \mu_n \overset{W_2}{\longrightarrow} \mu \Leftrightarrow \biggl [ \mu_n  \rightharpoonup \mu \text{ and } \int_X d^2(x,x_0) d\mu_n \longrightarrow \int_X d^2(x,x_0)d\mu \text{ for some }x_0 \in X \biggr ]$$ There are 3 steps in the proof I can't completely understand, assume $(X,d)$ is compact for the first two points: Let $$Z:= \{ f \in \text{Lip}_1(X,d) \mid f(x_0)=0 \} $$ then $$\sup_{f \in \text{Lip}_1(X,d)} \biggl | \int_X f d(\mu_n -\mu) \biggr |= \sup_{f \in Z} \biggl | \int_X f d(\mu_n -\mu) \biggr | $$ Let $A \subset X$ be an open subset, then $$ \liminf_{n} \int_A d^2(x,x_0)d\mu_n \ge  \int_A d^2(x,x_0)d\mu $$ Given a sequence of compact subsets $ \{ K_k \}_{k \ge 1}$ s.t. $$ \lim_{k \to +\infty} \sup_n \int_{X \setminus K_k} d^2(x_0, \cdot)d\mu_n =0$$ define $$\mu_{n,k} := \mu_n |_{K_k} + (1-\mu_n(K_k))\delta_{x_0} $$ then, up to subsequences, $ \{ \mu_{n_k} \}_{n}$ is weak convergent. Any hint will be very appreciated!","['measure-theory', 'optimal-transport', 'weak-convergence']"
3042951,Optimizing a winning strategy for a quick tabletop game,"A friend of mine recently shared the following puzzle with me: Puzzle: A circular turntable is divided into four congruent quadrants by two perpendicular lines.  (Think of a circle in the $xy$ -plane centered at the origin and divided into quadrants by the coordinate axes.)  In each quadrant is a quarter covered by an upside down opaque cup.  The cups are identical in appearance as are their relative positions in each quadrant.  A ""move"" consists of removing any two cups and then changing the orientations (heads up\tails up) of none, one, or both of the corresponding coins.  After the move you turn your back and a judge inspects the coins under the remaining two cups.  You ""win"" if all four coins are oriented the same, either all heads up, or all tails up.  If you fail to win on a move then the judge replaces the cups back in their original (indistinguishable) positions over the (currently oriented) coins and gives the turntable a spin.  When it stops rotating you turn back around and are allowed another move.  Prove that you can win in at most five moves. After toying around with this puzzle for a bit, I was able to come up with three different solutions, all of which involved 5 moves. (My friend originally had a solution involving 6 moves but improved to 5, hence the requirement to find a solution involving at most 5 moves.) Question: Is it possible to guarantee a solution involving only 4 moves? Or a clever (i.e., not brute force) proof that demonstrates 5 moves is the smallest number of moves that guarantee a win?","['puzzle', 'extremal-combinatorics', 'combinatorics', 'combinatorial-game-theory', 'recreational-mathematics']"
3042972,"Show that if $f$ continuous, $f(x_0) = c > 0$, then $\exists$ interval with $x_0$ such that $f(x) > c/2$ for every $x \in I$.","I need to show that if $f: [0,1] \rightarrow \mathbb{R}$ is continuous, $f(x_0) = c > 0$ for some $x_0 \in [0, 1]$ , then there is an interval $I$ containing $x_0$ such that $f(x) > c/2$ for every $x \in I$ . I know that $f$ is Riemann integrable. This is part of a larger question on proving $f(x) = 0$ , where $f$ is a continuous function where $f(x) \geq 0$ and the integral of $f$ is $0$ . I am struggling to see the larger point here if it's clear to anyone else. Is a sufficient answer to the question the singleton point ${x_0}$ and as a result, the condition is satisfied. Unless we take the definition of an interval to need more than one point, which in that case, then I'm unsure.","['integration', 'continuity', 'functions', 'proof-verification']"
3043013,Probability that random walk will reach state $k$ for the first time on step $n$,"We have a random walk which starts in state $0$ . At each step, a coin is tossed with probability of heads: $P(H)=p$ . If we get a heads, we go to the next higher integer state and on tails, we go to the next lower integer state (so state $n$ would go to $n+1$ on heads and $n-1$ on tails). Now, I want to know the probability that we will reach state $k$ for the first time after exactly $n$ tosses of the coin. Turning out to be surprisingly hard for me. Here is my attempt: I define $a_n^{k}$ as the probability described above and $c_n^k$ as the probability that the random walk will be in state $k$ at toss $n$ (regardless of if it was there in a previous toss). It's clear that we need $\left(\frac{n-k}{2}\right)$ tails and $\left(\frac{n+k}{2}\right)$ heads. So if $n-k$ is not even, the sequences for those $n$ 's become $0$ . To get $a_n^k$ , we need to identify all sequences where the cumulative number of heads stays less than $k$ + the cumulative number of tails for all tosses leading upto $n$ . This is not so easy to solve. On the other hand, I got an expression for $c_n^k$ and hoped I could use it to get $a_n^k$ . I reasoned that the probability the walk reaches $k$ for the first time on toss $n$ is the probability it is in state $k$ on toss $n$ subtracted by the probabilities that it was in state $k$ in any previous toss. So, $$a_n^k = c_n^k - \sum_{i=1}^{n-1} c_i^k$$ But this can't be right since this expression will become negative for many values of $n$ .","['random-walk', 'recurrence-relations', 'combinatorics', 'sequences-and-series', 'probability']"
3043100,Operator norm $ ( \ell_2 \to \ell_1)$,"Let $X$ be a ﬁnite dimensional normed vector space and $Y$ an arbitrary normed vector space. $ T:X→Y$ . I want to  calculate $\|T\|$ for where $X = K^n$ , equipped with the Euclidean norm $\|\cdot\|_2$ , $Y := \ell_1(\mathbb{N})$ and $Tx := (x_1,\ldots,x_n,0,0,\ldots) \in \ell_1(\mathbb{N})$ , for all $x = (x_1,\ldots,x_n) \in K^n$ . I do not know how to continue $$ ||T∥_2 = \sup \limits_{x \neq 0} \frac{∥Tx∥_1}{∥x∥_2} =  \sup \limits_{x \neq 0} \frac{∥(x_1,…,x_n,0,0,…)∥_1}{∥(x_1,…,x_n)∥_2} = \sup \limits_{x \neq 0} \frac{|x_1|+…+|x_n|}{(|x_1|^2+…+|x_n|^2)^{\frac{1}{2}}}= ? $$","['operator-theory', 'normed-spaces', 'functional-analysis', 'lp-spaces']"
3043108,Show that $\mathscr{O}_{\mathbb{Q}(\sqrt{-7})}$ is a UFD,"It is known that the ring of integer is a Dedekind domain which means that it is a UFD iff it is a PID. Since $-7\equiv1$ mod $4$ , we have that $\mathscr{O}_{\mathbb{Q}(\sqrt{-7})}=\mathbb{Z}\left[\frac{1+\sqrt{-7}}{2}\right]$ . Now I read something in the sense of: if $\alpha:=\frac{1+\sqrt{-7}}{2}$ has an irreducible minimal polynomial mod $2$ and mod $3$ , then we have a PID; I don't know anything about that. I think I have stated that wrong since the minimal polynomial is $f_{\alpha}=x^2-x+2$ which is reducible mod $2$ . Dr. Math:
We pick an arbitrary complex number $x + iy\in\mathbb{Z}[\alpha]$ , and we must find a 
suitable lattice point: $$z = r + s\alpha = (r+s/2) + i(s\sqrt{7})/2.$$ It is natural to try to have the real and imaginary parts of $(x + yi - z)$ as small as possible. Let's start with the imaginary part $ y - s\sqrt{7}/2$ .  We take $s$ as 
the closest integer to $2y/\sqrt{7}$ .  This will give us the following: \begin{align*}
  | 2y/\sqrt{7} - s | &\leqslant 1/2\\
  | y - s\sqrt{7}/2 | &\leqslant \sqrt{7}/4.
\end{align*} Now, we turn to the real part $x - r - s/2$ .  If we select $r$ as the 
integer closest to $(x - s/2)$ , we will have: \begin{align*}
  | x - r - s/2 | \leqslant 1/2.
\end{align*} Putting both relations together, we get: $$
  N(x + yi - z) = (x - r - s/2)^2 + (y - s\sqrt{7}/2)^2
                \leqslant 1/4 + 7/16
                < 1$$ as desired. Hence, Euclidean domain, so PID, so UFD. Is this proof correct and can it be applied in all cases of showing that $\mathbb{Z}\left[\frac{1+\sqrt{d}}{2}\right]$ , $\square\neq d\in\mathbb{N}$ is a Euclidean domain?","['algebraic-number-theory', 'number-theory', 'unique-factorization-domains', 'euclidean-domain', 'ring-theory']"
3043113,"Can a tree graph have only one vertex? And if so, that means that a tree graph has at minimum one leaf?","Also, if the tree is two vertices connected by an edge, does the root count as a leaf too? Since it's also a vertex of degree one? I've had trouble clarifying this online and from my textbook. Thank you.","['trees', 'discrete-mathematics']"
3043134,"Greater of the angles $\alpha=2\tan^{-1}(2\sqrt{2}-1)$, $\beta=3\sin^{-1}\frac{1}{3}+\sin^{-1}\frac{3}{5}$","Find the greater of the two angles $\alpha=2\tan^{-1}(2\sqrt{2}-1)$ and $\beta=3\sin^{-1}\dfrac{1}{3}+\sin^{-1}\dfrac{3}{5}$ My Attempt $$
\alpha=2\tan^{-1}(2\sqrt{2}-1)=2\tan^{-1}(1.82)>2\tan^{-1}\sqrt{3}=2.\frac{\pi}{3}\\
\implies \boxed{\alpha>\frac{2\pi}{3}=0.67\pi}\\
\beta=3\sin^{-1}\dfrac{1}{3}+\sin^{-1}\dfrac{3}{5}=3\sin^{-1}(0.33)+\sin^{-1}(0.6)\\
=\sin^{-1}(0.8)+\sin^{-1}(0.6)<\sin^{-1}(1)+\sin^{-1}(0.7=1/\sqrt{2})\\
\boxed{\beta<\frac{\pi}{2}+\frac{\pi}{4}=\frac{3\pi}{4}=0.75\pi}
$$ I am stuck with the above result which does not seem to give enough information to decide which one is greater, wht's the easiest way to  solve this ?","['maxima-minima', 'trigonometry', 'inverse-function']"
3043139,Prove $f(x) = x^2\sin\left(\frac{1}{x}\right)$ is Lipschitz (no use of derivative),"Prove that $f:\mathbb{R}\to \mathbb{R}$ such that $$ f(x) = \left\{ 
  \begin{array}{c l}
    x^2\, \sin\left(\frac{1}{x}\right) & ,\quad x\neq 0\\
    0 & ,\quad x=0
  \end{array} \right.$$ is Lipschitz (without use of derivatives). Attempt . I am aware ( Lipschitz-continuous $f(x)=x^2\cdot \sin\left(\frac{1}{x}\right)$ )
 that: $$|f(x)-f(y)|\leq 3|x-y| ~~~\forall~x,~y\in \mathbb{R},$$ but I am looking for a proof, without use of derivatives. 
I tried: for $x,y\neq 0$ : \begin{eqnarray}
x^2\sin\frac{1}{x} - y^2 \sin\frac{1}{y}
 &=& (x^2-y^2)\sin\frac{1}{x} + y^2\left ( \sin\frac{1}{x} - \sin\frac{1}{y} \right ),\nonumber
\end{eqnarray} so: $$\left | x^2\sin\frac{1}{x} - y^2 \sin\frac{1}{y} \right |
\leq |x^2 - y^2| + y^2 \left | \sin\frac{1}{x} - \sin\frac{1}{y} \right |.$$ Since: $$\left | \sin\frac{1}{x} - \sin\frac{1}{y} \right | \leq  \left| \frac{1}{x} - \frac{1}{y}\right |= \frac{|x - y|}{xy},$$ we get: $$\left | x^2 \sin\frac{1}{x} - y^2 \sin\frac{1}{y} \right |
\leq \left(x+y+\frac{y}{x}\right)|x-y|.$$ Unfortunatelly , the quantity $x+y+\frac{y}{x}$ grows to $+\infty$ , either for large $x$ , or for $x\approx 0.$ Thanks in advance for the help.","['lipschitz-functions', 'analysis', 'real-analysis']"
3043176,"Why my process is wrong:-How many ways are there to choose $5$ questions from three sets of $4$, with at least one from each set?","Question A question paper on mathematics contains $12$ questions divided into three parts A, B and C, each containing $4$ questions. In how many ways can an examinee answer $5$ questions, selecting at least one from each part. Attempt Firstly, I selected three questions (one from each part) and it can be done $4 \cdot 4 \cdot 4$ ways. And hence the remaining two positions for two questions can be given in $^9{\mathrm C}_2$ since there is no restrictions now. So, total ways is $36 \times 64=2304$ . But, in the answer given in the solution manual is $624$ . And the process described is: A    B    C

1    1    3

1    2    2 which is then arranged for part to be $3 \times (4 \times 4 \times 4 + 4 \times 6 \times 6) = 624$ . Why my process is incorrect? I understand the second solution but, unable to find any fault in my attempt. Please explain.","['combinations', 'combinatorics']"
3043181,Growth rate of a divisor function,"Hi I read an very interesting article about divisor function: https://en.wikipedia.org/wiki/Divisor_function#CITEREFHardyWright2008 I was wondering about a formula which appear under the Growth rate section  : for all $ \varepsilon>0$ , $d(n)=o(n^\epsilon)$ and $d(n)=O(n^\epsilon)$ where $d(n)$ stands for number divisors $n$ has. I would like to know why this is true?","['number-theory', 'divisor-counting-function', 'asymptotics']"
3043200,Intuition for the invariance of the determinant under change of basis,"$$A' = PAP^{-1}$$ $$\det(A')=\det(P)\det(A)\det(P^{-1})=\det(A)$$ Now, that makes sense algebraically, but consider the below diagram: This a geometric representation of the two 'normal' basis vectors $\bf i$ and $\bf j$ (I will denote this set by $B$ ) in $\Bbb R^2$ , and my choice of two new basis vectors $\bf i'$ and $\bf j'$ (I will denote this set by $B'$ ). The determinant preserves the area of of the unit square, which is determined by our choice of basis vectors. The unit square area in the basis $B$ is different to the unit square area in basis $B'$ . The determinant gives the area of the image of the unit square. The image of the black B unit square will likely be different to the image of the red $B'$ unit square, so why is $\det(A)=\det(A')$ ?","['determinant', 'matrices', 'change-of-basis', 'linear-algebra', 'linear-transformations']"
3043231,Operator norm on Lebesgue integrable functions,"Let $L_1([0,1],m)$ be the Banach space of $\mathbb{K}$ -valued integrable functions with respect to Lebesgue measure $m$ , where $\mathbb{K}$ is either $\mathbb{R}$ or $\mathbb{C}$ . The norm on this space is defined like this: $||f||_1=\int_{[0,1]}|f| \ dm$ . I have to show that: $a)$ For $n \geq 2$ the operator $\varphi_n(f)=\int_{[0,1]}\ f g_n \ dm$ , where $g_n(x)=n \sin(n^2x)$ for $x \in [0,1]$ is bounded with $||\varphi_n||=n$ . $b)$ Show that there exists $f \in L_1([0,1],m)$ such that $\lim_{n \to \infty} |\varphi_n( f)|=\infty$ . MY ATTEMPT: $g_n$ is Lebesgue integrable on $[0,1]$ since it's Riemann integrable. Hence $fg_n \in L_1([0,1],m)$ and $|\int_{[0,1]}fg_n\ dm| \leq \int_{[0,1]}|fg_n| \ dm$ . We also have that $||fg_n||_1 \leq ||f||_1||g_n||_\infty$ .
Thus $||\varphi_n(f)||=|\int_{[0,1]}fg_n\ dm| \leq \int_{[0,1]}|fg_n|\ dm=||fg_n||_1 \leq ||f||_1 ||g_n||_\infty$ , i.e. $\varphi_n$ is bounded. Now $||g_n||_\infty=n$ since it's continuous on a bounded interval and the $essential$ $supremum$ is the same as the $max$ . Now I would like to attain the equality with some function, and once that I find it I can use in part $b)$ . Any ideas on the function?","['normed-spaces', 'lebesgue-measure', 'lebesgue-integral', 'functional-analysis']"
3043256,On $\int_0^\infty \frac{\exp(-x^2)}{1+x^2}dx=\frac{\pi e}2\text{erfc}(1)$,"I was attempting to answer this question , but then I came across a question of my own involving my attempt. Task: Prove $$\int_0^\infty\frac{\exp(-x^2)}{1+x^2}\mathrm dx=\frac{\pi e}2\text{erfc}(1)$$ Attempt: $$I=\int_0^{\infty}\frac{\exp(-x^2)}{1+x^2}\mathrm dx$$ We then use the Taylor series for the exponential function to find that $$I=\sum_{n\geq0}\frac{(-1)^n}{n!}\int_0^\infty\frac{x^{2n}}{1+x^2}\mathrm dx$$ Setting $x=\tan u$ , $$I=\sum_{n\geq0}\frac{(-1)^n}{n!}\int_0^{\pi/2}\tan(u)^{2n}\mathrm{d}u$$ $$I=\sum_{n\geq0}\frac{(-1)^n}{n!}\int_0^{\pi/2}\sin(u)^{2n}\cos(u)^{-2n}\mathrm{d}u$$ And using $$\int_0^{\pi/2}\sin(t)^a\cos(t)^b\mathrm{d}t=\frac{\Gamma(\frac{a+1}{2})\Gamma(\frac{b+1}{2})}{2\Gamma(\frac{a+b}{2}+1)}$$ We have $$I=\frac12\sum_{n\geq0}\frac{(-1)^n}{n!}\Gamma\bigg(\frac{1+2n}{2}\bigg)\Gamma\bigg(\frac{1-2n}{2}\bigg)$$ $$I=\frac12\sum_{n\geq0}\frac{(-1)^n}{n!}\Gamma\bigg(\frac12+n\bigg)\Gamma\bigg(\frac12-n\bigg)$$ Recall the Gamma reflection formula: $$\Gamma(s)\Gamma(1-s)=\pi\csc\pi s\ ,\qquad s\not\in\Bbb Z$$ Since $n\in\Bbb N_0$ , we have $\frac12+n\not\in\Bbb Z$ , which means we may plug in $s=\frac12+n$ : $$I=\frac12\sum_{n\geq0}\frac{(-1)^n}{n!}\pi\csc\bigg(\frac\pi2+\pi n\bigg)$$ $$I=\frac\pi2\sum_{n\geq0}\frac{(-1)^n}{n!}\csc\bigg(\frac\pi2(2n+1)\bigg)$$ Then we recall that $$\sin\bigg(\frac\pi2(2n+1)\bigg)=(-1)^n,\qquad n\in\Bbb Z$$ So we have $$I=\frac\pi2\sum_{n\geq0}\frac{(-1)^n}{n!}\frac1{(-1)^n}$$ $$I=\frac\pi2\sum_{n\geq0}\frac1{n!}$$ $$I=\frac{\pi e}2$$ But $$\frac{\pi e}2\neq \frac{\pi e}2\text{erfc}(1)$$ What did I do wrong? Thanks. Edit: I see that $$\int_{\Bbb R^+}\frac{x^{2n}}{1+x^2}\mathrm dx$$ diverges, and as was pointed out in the comments, I can't interchange the $\sum$ and $\int$ , but why? The Taylor series converges for all $x\in\Bbb R_0^+$ , so what's wrong with the swappage?","['integration', 'improper-integrals', 'real-analysis', 'sequences-and-series', 'error-function']"
3043297,$L_f(z) = \frac 1 {2 \pi i}\int_{ \mathbb{T} } \frac{ \zeta+z}{ \zeta ( \zeta -z)} f( \zeta ) d\zeta$,"I'm trying to prove that for any harmonic function $u$ , we have : let $ \Omega \subset \mathbb{R}^2$ and $ \overline B(0,R) \subset \Omega $ $$ u \colon \Omega \to \mathbb R  $$ $$\forall z \in B(0,R) : u(z) = \Re \ \ \frac 1 {2 \pi i}\int_{
 |\zeta| = R } \frac{ \zeta+z}{ \zeta ( \zeta -z)} u( \zeta ) d\zeta $$ I've tried a few things: I've shown that $$ \int_{|\zeta| = R } \frac{ \zeta+z}{ \zeta ( \zeta -z)} u( \zeta ) d\zeta 
=  
2 \int_{|\zeta| = R } \frac{ u( \zeta ) }{ ( \zeta -z)}  d\zeta 
-
\int_{|\zeta| = R } \frac{ u( \zeta ) }{ \zeta}  d\zeta $$ Which ( I'm not sure about this point... but in complex analysis I think it would have made sense ) is proportional to $$ 2 \operatorname{Res}_z(u) - \operatorname{Res}_0(u) $$ But I don't think I'm going anywhere ... So I started again and I studied the function : $$L_f(z) = \frac 1 {2 \pi i}\int_{ \mathbb{T} } \frac{ \zeta+z}{ \zeta ( \zeta -z)} f( \zeta ) d\zeta$$ I would still need to prove that this is holomorphic, but the derivative is given by : $$L'_f(z) = \frac 1 { \pi i}\int_{ \mathbb{T} } \frac{  f( \zeta ) }{ ( \zeta -z)^2    }d\zeta$$ I was very surprised because the RHS is exactly the expression of $ 2 f'(z) $ according to Cauchy Integral Formula. So I was believing that $L_f \equiv f$ . But in order to prove the equality, I would still need to prove that : $$- \int_{ \mathbb{T} } \frac{ f( \zeta ) }{ \zeta}  d\zeta 
=
\frac 1 { \pi i}\int_{ \mathbb{T} } \frac{  f( \zeta ) }{  \zeta -z    }d\zeta $$ So could you please tell me if my reasoning is true/ going somewhere, if I'm allowed to talk about residues for harmonic functions... 
Or if you have another solution for the main problem, I would also be very grateful to read it :)","['complex-analysis', 'cauchy-integral-formula', 'harmonic-functions']"
3043323,Why is Wiener measure a Gaussian measure?,"This is silly and trivial so let me be really clear with my definitions and where exactly I got stuck. The space $(C[0,T],\|\cdot\|_\infty,\sigma(\|\cdot\|_\infty),\gamma)$ is called classical Wiener space where $\gamma$ is Wiener measure. I define Wiener measure as follows: Wiener measure is the Kolmogorov extension of the finite dimensional distributions of the Wiener process. And Gaussian measure on Banach space: Let $\mathcal B$ be a Banach space with dual $\mathcal B^\ast$ . A Borel measure $\gamma$ is called Gaussian iff for all $\ell\in \mathcal B^\ast$ the pushforward measure $\ell^\ast \gamma$ is a Gaussian measure on $\Bbb R$ where $\ell^\ast \gamma (A)=\gamma(\ell^{-1}(A))$ is the pushforward measure. So I know the dual space of $C[0,T]$ is $\operatorname{RCA}([0,T])$ (by Riesz-Markov-Kakutani theorem ) the space of all complex signed Radon measures with finite total variation. So we have to check that $\mu^\ast(\gamma)$ is a Gaussian measure on $\Bbb R$ for all $\mu\in \operatorname{RCA}([0,T])$ . If $\mu$ is a finite linear combination of $\delta$ s we are fine as $\delta_{t}^{-1}(A)$ are all the paths that pass through the set $A$ at time $t\in [0,T]$ . Then $\gamma$ of this is Gaussian by definition. Linearity is not too bad after this. Then we have to extend to all measures and I am not sure how to do this. I know $\delta$ s should be dense in $\operatorname{RCA}([0,T])$ and Wiener measure is the Kolmogorov extension of the finite dimensions but I'm not sure exactly what the argument should be.","['probability-theory', 'functional-analysis', 'probability', 'measure-theory']"
3043380,Is there a substitution which transforms every Fermat curve into an elliptic curve?,"A Fermat Curve of degree $n$ is the set of solutions to $x^n+y^n=z^n$ , $x,y,z\in \mathbb R$ . In this question , the OP provides a substitution which relates a Fermat Curve of degree $n=3,4$ to two different elliptic curves. To transform the Fermat Curve of degree $3$ , the substitutions $$ a=\frac{12z}{x+y},\quad b=\frac{36(x-y)}{x+y} $$ produce $b^2=a^3-432$ , an elliptic curve. Similarly for the Fermat Curve of degree $4$ , the substitutions $$ a=\frac{2(y^2+z^2)}{x^2},\quad b=\frac{4y(y^2+z^2)}{x^3} $$ give $b^2=a^3-4a$ . However, the substitutions used are not at all obvious, which leads me to wonder, Is there a similar substitution which can relate a Fermat curve of arbitrary degree to an elliptic curve? How can we even begin to prove this? I suspect the proof or disproof of this statement will be way above my level; I truly have no clue where to begin. Can someone help out? If this is somehow an open problem, then any links to literature is also appreciated! Edit: This question has been asked and answered on MO . (Yay!)","['algebraic-curves', 'elliptic-curves', 'number-theory', 'algebraic-geometry', 'substitution']"
3043395,Absolute of all eigenvalues are always bounded by maximal singular value,"There are many discussions about the singular values and eigenvalues, such as What is the difference between Singular Value and Eigenvalue? . I want to ask the particular one in title. Usually, for a general square matrix, singular valure are not equal to eigenvalues. But singular values are alwyas nonnegative. My claim is $A\in \mathbb{R}^{n\times n}$ . $|\lambda_i(A)|\leq \sigma_\max(A)$ , for all $i$ . The reason is the definition of the maximal singular value of $A$ , which is $$\sigma_\max(A) = \|A\|_2 = \max_{\|x\|=1} \|Ax\|.$$ It reflects the maximal gain of $A$ . And this $x$ does not have to be the eigenvector of $A$ . However, the eigenvalue of $A$ is $$Av = \lambda v, \ \ \ \ \|v\|=1.$$ To get $\lambda_\max$ , we have to have the extra constraint $\|Av\| 
= \|\lambda_\max v\|$ . So $|\lambda(A)|\leq \sigma_\max (A)$ . Here $A$ is any real matrix not necessarily symmetric.  I am not sure if I am correct.","['matrices', 'singular-values', 'eigenvalues-eigenvectors']"
3043451,Weierstrass $\wp$-function defines a map from the torus to an elliptic curve. Why is it injective?,"For $L$ a lattice in $\mathbb C$ , the Weierstrass $\wp$ -function is the meromorphic function $$\wp(z) = \frac{1}{z^2} + \sum\limits_{0 \neq \lambda \in L}\frac{1}{(z-\lambda)^2} - \frac{1}{\lambda^2}$$ It can be shown to satisfy the differential equation $\wp'(z) = 4\wp(z)^3 - g_2\wp(z) - g_3$ , where $$g_2 = 60 \sum\limits_{0 \neq \lambda \in L} \frac{1}{\lambda^4}$$ $$g_3 = 120 \sum\limits_{0 \neq \lambda \in L} \frac{1}{\lambda^6}$$ If $E$ is the elliptic curve in $\mathbb P^2$ defined by the homogeneous polynomial $y^2z = 4x^3 - g_2xz^2-g_3z^3$ , then $$F(z) = \begin{cases} (\wp(z);\wp'(z);1) & \textrm{if }z\not\in L \\ (0;1;0) & \textrm{if } z \in L \end{cases}$$ can be shown to define a holomorphic function $\mathbb C \rightarrow E$ .  Since $\mathscr P$ and $\mathscr P'$ are well defined on $\mathbb C/L$ , so is $F$ , and $F$ induces a holomorphic function $$\bar{F}: \mathbb C /L \rightarrow E$$ which is automatically surjective, because $F$ is an open map (being holomorphic and nonconstant), and $\mathbb C/L$ and $E$ are compact.  I want to say that $\bar{F}$ is a biholomorphism, which is equivalent to saying $\bar{F}$ is injective. How do we know that $\bar{F}$ is injective?","['complex-analysis', 'riemann-surfaces', 'complex-manifolds', 'elliptic-curves']"
3043457,Adjoint of bounded linear map is isometric isomorphism implies original map is isometric isomorphism?,"Suppose $X$ and $Y$ are normed spaces. Let $T$ be a bounded linear map from $X$ to $Y$ . Let $T^*$ be the adjoint map from $Y^{*}$ to $X^{*}$ defined by $T^{*}(y^*) = y^* T$ . A straightforward calculation shows: Theorem 1. If $T$ is an isometric isomorphism from $X$ onto $Y$ , then $T^*$ is an isometric isomorphism from $Y^*$ onto $X^*$ . I'm trying to prove the converse. But the best I can get is the following. (It comes by applying the above theorem with $T^*$ in place of $T$ and using that $T^{**}$ extends $T$ [if $X$ is identified with a subspace of $X^{**}$ in the natural way]). Theorem 2. If $T^*$ is an isometric isomorphism from $Y^*$ onto $X^*$ , then $T$ is an isometric isomorphism from $X$ into $Y$ and $T(X)$ is dense in $Y$ . I cannot seem to strengthen the conclusion to $T$ is surjective. If $X$ is complete, or, more generally, if $T(X)$ is closed in $Y$ , then $T$ is surjective. But what happens if $X$ is not complete or $T(X)$ is not closed? In the discussion of the following question, the OP claims to be able to prove that $T^{∗}$ being an isomorphism implies $T$ is surjective, but I don't see how: $T$ is surjective if and only if the adjoint $T^*$ is an isomorphism (onto its image) There are also some Hilbert space examples in the following links, but they don't address what I am asking about: $T$ surjective iff $T^*$ injective in infinite-dimensional Hilbert space? Example: operator injective, then the adjoint is NOT surjective","['operator-theory', 'functional-analysis', 'analysis', 'dual-spaces']"
3043490,Reconciling two interpretations of $E(X^2)$,"It's been over 25 years since my last course in probability, so this may be obvious or elementary.  However, I've unexpectedly had to think deeply about this stuff for a research project, and I cannot reconcile my issue. For the sake of simplicity/common ground, let's assume Riemann integrals always suffice, there are no convergence issues, and my random variables take values over all of $\mathbb{R}$ .  All integrals below are $\int_\mathbb{R}  \ dx $ whether or not the domain is explicitly written. Given a r.v. $X$ with density function $f(x)$ , we define $E(X) = \int x f(x) \ dx$ .  Linearity is then proven, $E(aX+b)=aE(X)+b$ , and this is completely sensible.  In deriving the alternative formula for the variance, we bump into $E(X^2)$ .  In that derivation, we define $$
E(X^2) = \int x^2 f(x) \ dx
$$ and proceed.  This is fine, but here's my issue. $X^2$ is itself a random variable, so we could ask for its expected value (in reference to ""itself,"" not $X$ ).  To be clearer, we could set $Y=X^2$ and ask for $E(Y)$ .  This requires us to know the density function for $Y$ , and this to me is not clear at all and non-trivial to get your hands on.  So, setting $Y=X^2$ with density $h(y)$ , why is it true that $$
\int y h(y) \ dy = \int x^2 f(x) \ dx
$$ so that $E(Y) = E(X^2)$ ?  Why do these two very different interpretations agree?  I do not think this is as simple as a $u$ -substitution. For example, I can see that this works out fine if $X$ is standard normal.  There, $E(X)=0$ and $Y=X^2$ is $\chi^2$ distributed with $1$ degree of freedom.  Since $\sigma_X^2=1$ it is clear that $E(X^2)=1$ .  Chasing the calculations I also see that $E(Y)=E(\chi_1^2)=1$ , so they are in fact in agreement.  I can even follow the derivations of the $\chi_1^2$ distribution in terms of the $\Gamma$ -function and see the connection to the standard normal, but I see no reason for this to play out as nicely no matter the density of $X$ . It also seems to get worse when considering $E(X^\alpha)$ in general. Are these two viewpoints in potential disagreement, or is there a piece of theory that says that there is no ambiguity?","['expected-value', 'probability-theory']"
3043499,"Does $HK = G$ imply that $KH = G$ when $H,K$ aren't subgroups?","Let $G$ be a finite group and let $H$ and $K$ be two ""subsets"" of $G$ such that $HK = G$ . Does that imply that $KH = G$ ? Since $H$ and $K$ are not subgroups we cannot use the formula $o(HK) = \frac{o(H)o(K)}{o(H\cap K)}$ . I believe this is false and I am looking for counterexamples. Thank you.","['group-theory', 'finite-groups']"
3043550,What functions $g$ satisfy $\int_{-L}^{L} \frac{f(x)}{1 + g(x)}\:dx = \int_{0}^{L} f(x)\:dx$ for every even function $f$?,"As has been covered in a number of questions on this site, there is a well know property of single variable real continuous even functions $f(x)$ : \begin{equation}
 \int_{-L}^{L} \frac{f(x)}{1 + e^x}\:dx = \int_{0}^{L} f(x)\:dx
\end{equation} for $L \in \mathbb{R}^+$ being either finite or infinite. When you evaluate the proof, there is a fundamental property of $g(x) = e^x$ that allows for this to occur and that is: \begin{equation}
g(-x) = \frac{1}{g(x)}
\end{equation} We see this holds not only for $e$ but for any $a \in \mathbb{R}^+$ My question: outside of $a^x$ are there any real valued functions the satisfy this condition?","['integration', 'definite-integrals', 'real-analysis']"
3043558,Does a family of linearly independent injective maps have a vector with linearly independent images?,"Let $V,W$ be finite dimensional vector spaces over an infinite field $k$ . Fix a positive integer $n \leq \dim(W), \dim(V)$ . Given $n$ injective linear maps $f_i:V\rightarrow W$ , such that the $f_i$ are linearly independent as elements of $\operatorname{Hom}(V,W)$ , is there necessarily a vector $v\in V$ with $f_i(v)$ linearly independent in $W$ ? This holds for $n=1,2$ and if the $f_i$ commute and are diagonalisable, since then one may simultaneously diagonalise. In terms of the conditions, we clearly require linear independence of the maps for the conclusion, and injectivity is required to exclude the case of $\dim(V) > \dim(W)$ , where the images will never be linearly independent.","['linear-algebra', 'linear-transformations']"
3043594,Antiderivative of an odd function,"Is the antiderivative of an odd function even? The answer given by the book is yes. However, I found a counterexample defined in $\mathbb{R}\setminus \{0\}$ : $$f(x)=\begin{cases}\ln |x|+1& x<0\\\ln |x|&x>0\end{cases}$$ Its derivative is $\frac 1x$ , which is an odd function. Question: is my counterexample right?","['integration', 'calculus', 'real-analysis']"
3043631,Showing that an elliptic function has no poles,"Let $\Lambda = \{m \omega_1+n\omega_2; m,n \in \mathbb{Z}\}$ with $\omega_i \in \mathbb{C}$ with $\omega_2/\omega_1 \notin \mathbb{R}$ be a lattice. Define the Weierstrass $\mathscr{P}$ function on the torus $\mathbb{C}/\Lambda$ as $$\mathscr{P}(z) = \frac{1}{z^2}+\sum\limits_{\omega \in \Lambda} (\frac{1}{(z-\omega)^2}-\frac{1}{\omega^2})$$ I am asked to show that if we choose $a \in \mathbb{C}/\Lambda$ with $a \notin \frac{1}{2}\Lambda$ then the elliptic function $$h(z) = (\mathscr{P}(z-a) − \mathscr{P}(z+a))(\mathscr{P}(z)-\mathscr{P}(a))^2-\mathscr{P}'(z)\mathscr{P}'(a)$$ has no poles and is hence constant. To do this I tried to show that $h$ is analytic. I attempted to do this by taking Laurent expansions around $0,a,-a$ and showing that $h$ was analytic in a disc around all those points (as those are the only possible points where $h$ can have a pole) but was not able to get this result. I don't have any other ideas for what to do. How should I go about doing this problem? Any hints are appreciated!","['riemann-surfaces', 'riemannian-geometry', 'complex-geometry', 'complex-analysis', 'elliptic-functions']"
3043727,Finding limits of integration using spherical coordinates,"I would like to integrate some function $f:\mathbb{R}^3\to\mathbb{R}$ over $C_1\cap C_2$ where $$C_1=\{(x,y,z)\in\mathbb{R}^3:x^2+4y^2+9z^2\leq1\}$$ $$C_2=\{(x,y,z)\in\mathbb{R}^3:x^2+4y^2+9z^2\leq 6z\}$$ My method was to find parametric equations for $C_1,C_2$ using spherical coordinates. However, I am having trouble finding the limits of integration. I attempted to use some geometric intuition but I can't seem to get the right result. Any help would be appreciated.
I used the following parametrization for $C_1$ $$x=r\sin(a)\cos(b),\quad y= r \sin(a) \sin(b)/2,\quad z= r \cos(a)/3$$ For $C_2$ I used $$x=r\sin(a)\cos(b),\quad y= r \sin(a) \sin(b)/2,\quad z= r \cos(a)/3+1/3$$ In both cases, $0\leq r\leq 1, 0\leq a \leq \pi, 0\leq b\leq 2\pi$ .","['multivariable-calculus', 'spherical-coordinates', 'definite-integrals', 'parametrization']"
3043740,Does the sum of two functions satisfying the intermediate value property also have this property?,"If functions $f$ and $g$ both satisfy the intermediate value property, does their sum also satisfy this property? If not, what if I suppose in addition that $f$ is continuous? Thanks in advance! Edit: I found the second part of my question here: Is the sum of a Darboux function and a continuous function Darboux?","['continuity', 'calculus', 'analysis']"
3043790,Find locus of points by finding eigenvalues,"Let $\boldsymbol{x}=\left(\begin{matrix}x\\ y\end{matrix}\right)$ be a vector in two-dimensional real space. By finding the eigenvalues and eigenvectors of $\boldsymbol{M}$ , sketch the locus of points $\boldsymbol{x}$ that satisfy $$ \boldsymbol{x^TMx}=4$$ given that $$\boldsymbol{M}=\left(\begin{matrix}&5 &\sqrt{3}\\ &\sqrt{3} &3\end{matrix}\right). $$ I found two eigenvalues to be $\lambda_1 = 6$ and $\lambda_2=2$ , and the corresponding eigenvectors are $$ \boldsymbol{v}_1=\left(\begin{matrix}\sqrt{3}\\ 1\end{matrix}\right)\quad\text{ and }\quad \boldsymbol{v}_2=\left(\begin{matrix}1\\ -\sqrt{3}\end{matrix}\right)$$ (if I'm not mistaken :) ), but... what now? Frankly, I can't figure out how to make this helpful to find $\boldsymbol{x^TMx}=4$ . Any hints?","['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors']"
3043803,Inverse Limit of Dense Subsets is Dense,"Suppose that $(X_i,\leq)$ is an inverse system in Top, and $U_i$ is a dense subset of each $X_i$ and for all $i \leq j$ $$\pi^j_i[U_j]\subseteq U_i$$ where $\pi^j_i: X_j \to X_i$ . This means that $(U_i,\leq)$ is an inverse system in Top also (by considering the relative topology).  My question is, is $\varprojlim_{i} U_i$ then dense in $\varprojlim_{i} X_i$ ? Intuition This is true for product, for example see this post , which (albeit) are a special case of inverse limits, so I expect this to be true in general.","['general-topology', 'limits-colimits', 'algebraic-topology']"
3043820,$\operatorname{Aut} (G)$ is isomorphic to $\operatorname{Aut} (H)$ then is it necessary that $G$ is isomorphic to $H$?,"If $\operatorname{Aut} (G)$ is isomorphic to $\operatorname{Aut} (H)$ then is it necessary that $G$ is isomorphic to $H$ ? My answer is no. $\operatorname{Aut} (\mathbb{Z)}$ is isomorphic to $Z_2$ and $\operatorname{Aut} (Z_3)$ is also isomorphic to $U(3)$ , which is isomorphic to $Z_2$ . But $\mathbb Z$ is not isomorphic to $Z_3.$ Correct? Thanks","['group-theory', 'abstract-algebra']"
3043857,Solve $(1-x^2)y''-xy'-y=e^{\arcsin x}$. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Problem Solve $$(1-x^2)y''-xy'-y=e^{\arcsin x}.$$ Can it be solved to get the general solution?",['ordinary-differential-equations']
3043862,"Technical operator theory question on Albeverio's ""Solvable Models in quantum mechanics""","I'm currently studying S. Albeverio's book ""Solvable models in quantum mechanics"" where some technical things are used that I don't fully understand. It is a general technical operator theory question, but I will introduce the setting. General setting: Looking at the Hamiltonian $-\Delta + V$ with the underlying Hilbert space $\mathrm{L}^2(\mathbb{R}^3)$ , where $V$ is a real potential, the aim in the first chapter of the book is to approximate a $\delta$ -Potential in 3D by scaling $V$ . We denote $v:=|V|^{1/2}$ , where the potential $V$ is an element of the Rollnik-class, i. e. real functions for which $$\int_{\mathbb{R}^6}\frac{|V(x)||V(y)|}{|x-y|^2}dxdy < \infty$$ The operator $vG_0 v: \mathrm{L}^2(\mathbb{R}^3)\rightarrow \mathrm{L}^2(\mathbb{R}^3)$ defined by the kernel $$(vG_0 v)(x,y)=\frac{v(x)v(y)}{4\pi|x-y|}$$ plays an important role in this setting (again $v:=|V|^{1/2}$ ). Note that the kernel is pointwise positive and the Rollnik-condition ensures that $vG_0v$ is Hilbert-Schmidt. Furthermore $G_0$ denotes the Operator given by convolution with the fundamental solution of the Laplace operator, i.e. $$G_0(x,y)=\frac{1}{4\pi|x-y|},$$ and $G_0(-\Delta \varphi)=\varphi$ for all $\varphi \in C_c^\infty$ . The (technical) problem: On p.21 and p.22 he uses the fact, that $(f,vG_0vf)$ , $f\in\mathrm{L}^2(\mathbb{R}^3)$ , can be written as $$(f,vG_0vf)=\Vert G_0^{1/2}vf\Vert^2,$$ so in a sense he uses that $vG_0v$ is a positive Operator and can be written as $vG_0v=vG_0^{1/2}G_0^{1/2}v=(G_0^{1/2}v)^*(G_0^{1/2}v)$ . Also he uses that $\Vert G_0^{1/2}vf\Vert^2=0$ implies $vf=0$ . I know that if an bounded linear operator $A$ is positive, it can be decomposed as $A=B^*B$ with $B$ also bounded. But I'm having great trouble in understanding how $G_0^{1/2}v$ is defined, as the operators $v$ (multiplication) and $G_0$ , which $vG_0v$ is composed of, are unbounded. Why does $G_0^{1/2}v$ even exist and why is it bounded? How can I conclude $(f,vG_0vf)=\Vert G_0^{1/2}vf\Vert^2=0$ implies $vf=0$ and in what sense? Why is $vG_0v$ (obviously?) positive? I thought about this a long time but couldn't come up with a satisfying solution. I would appreciate your help and comments!","['fourier-analysis', 'harmonic-analysis', 'operator-theory', 'functional-analysis', 'quantum-mechanics']"
3043880,What is the fastest route to drop off weight when time is proportional to weight x distance?,"You have a lorry at the starting point which is carrying all the parcels for the day. $$\rm Time\ taken = Total\ Lorry\ Weight \times Distance\ travelled $$ After visiting each zone you have to return to the starting point. Lorry's weight is $30$ . You must take all undelivered parcels with you on every trip. Your fuel has no weight, your fuel is everlasting and you lose no time when dropping off packages. What is the optimal route to delivering all the packages and returning to the starting point? My initial thoughts for solving this was to work backwards and work out which delivery would add the smallest amount of total time across the whole journey. Then repeat this until I got to the first stop. I have been told that isn't the correct way to work this out so not sure what would make the route I got for that more optimal.","['graph-theory', 'optimal-transport', 'combinatorics']"
3043884,Calculate the ratio of the sides of a given triangle given the ratio of areas.,"Given a triangle $\triangle ABC$ , points $M$ , $N$ , $P$ are drawn on the sides of the triangle in a way that $\frac{|AM|}{|MB|} = \frac{|BN|}{|NC|}= \frac{|PC|}{|PA|}=k$ , where $k>0$ . Calculate $k$ , given that the area of the triangle $\triangle MNP$ and the area of the triangle $ \triangle ABC$ are in the following ratio: $Area_{\triangle MNP} = \frac{7}{25} \times Area_{\triangle ABC}$ . I have tried Heron formula, bot the calculations seem to be too complicated, I have also tried to simplify the problem and assume that k is equal to 1 and then calculate the ratio of the triangles area but it also doesn't help.
I was also looking for similar triangles. I would appreciate some hint.","['euclidean-geometry', 'area', 'geometry', 'triangles', 'algebra-precalculus']"
3043909,I need help with a problem involving the nth derivative of arcsin x,"I need help with a problem. For context, the section of the textbook the problem is in is about power series. Note that the textbook uses the convention that $f^{(n)}$ represents the $n$ th derivative of $f$ , and $f^{(0)}(x) = f(x).$ I'll now state the problem exactly as stated in the textbook: Consider the function $f$ defined by $f(x) = \arcsin x$ , for $\lvert x \rvert \leq 1$ .
The derivatives of $f(x)$ satisfy the equation $
(1 - x^2)f^{(n + 2)}(x) - (2n + 1)xf^{(n + 1)}(x) - n^2 f^{(n)}(x) = 0$ , for $n \geq 1.
$ The coefficient of $x^n$ in the Maclaurin series for $f(x)$ is denoted by $a_n$ . You may assume that the series only contains odd powers of $x$ . $\textbf{a.1)}$ Show that, for $n \geq 1, (n+1)(n+2)a_{n+2} = n^2 a_n.$ $\textbf{a.2})$ Given that $a_1 = 1$ , find an expression for $a_n$ in terms of $n$ , valid for odd $n \geq 3.$ $\textbf{b})$ Find the radius of convergence of this Maclaurin series. $\textbf{c})$ Find an approximate value for $\pi$ by putting $x = \frac{1}{2}$ and summing the first three non-zero terms of this series. Give your answer to $\textbf{four}$ significant figures. I'm stuck on $\textbf{a.1}$ . The way the question is formulated makes me think you're not supposed to use the actual derivatives of $\arcsin$ to solve it, but I can't figure out how to do it. I know that $a_n = \frac{f^{(n)}(0)}{n!}$ ,so I was thinking that if I can find a formula for the nth derivative of $f(x)$ , I should be good to go. 
I know the derivative of $f(x)$ : $f^\prime(x) = \frac{d}{dx}\arcsin x = \frac{1}{\sqrt{1- x^2}}$ . From here, I can easily also find the second, third, etc. derivatives. However, when I try to come up with a formla for the $\textit{nth}$ derivative, I have a problem. I came up with the following formula: $\frac{d^n}{dx^n}\arcsin x = (-1)^n \prod\limits_{k = 0}^n \left(\frac{1}{2} - k\right)$ . Unfortunately, I have no idea how to proveed from here, as I don't know how to evaluate the product $\prod\limits_{k = 0}^n \left(\frac{1}{2} - k\right)$ . Anyway, I don't think this is the right approrach, as my textbook hasn't dealt with products yet, only sums. Can anyone help with $\textbf{a.1}$ ?","['calculus', 'derivatives', 'taylor-expansion', 'sequences-and-series']"
3043962,Mysterious polynomial sequence,"Can someone identify this polynomial sequence? Is it known in mathematics? I'm interested in various properties of this sequence. I'd like to find $P(n)$ , $n\in \mathbb{Z}^+$ \begin{align}
P(0)&= 1\\
P(1)&= a\\
P(2)&= a^2+b\\
P(3)&= a^3+2ab\\
P(4)&= a^4+3a^2b+b^2\\
P(5)&= a^5+4a^3b+3ab^2\\
P(6)&= a^6+5a^4b+6a^2b^2+b^3\\
P(7)&= a^7+6 a^5 b+10 a^3 b^2+4 a b^3\\
P(8)&= a^8 + 7 a^6 b + 15 a^4 b^2 + 10 a^2 b^3 + b^4\\
P(9)&= a^9 + 8 a^7 b + 21 a^5 b^2 + 20 a^3 b^3 + 5 a b^4\\
P(10)&= a^{10} + 9 a^8 b + 28 a^6 b^2 + 35 a^4 b^3 + 15 a^2 b^4 + b^5
\end{align} More steps upon request. I'll be grateful for any hints!","['polynomials', 'sequences-and-series']"
3043963,Proving that every Cauchy sequence in measure converges in measure,"Let $(X,\mathcal{A},\mu)$ be a measure space and $(f_n)$ a sequence of real-valued functions on $X$ which is Cauchy in measure; that is, for any $\epsilon>0$ there exists $N\in\mathbb{N}$ such that for all $m,n\geq N$ we have $\mu(\{x\in X \ | \ |f_n(x)-f_m(x)|\geq \epsilon\})<\epsilon$ . Prove that there is a function $f$ on $X$ to which $(f_n)$ converges to $f$ in measure. (Convergence in measure: For any $\epsilon>0,\displaystyle\lim_{n \to \infty} \mu(\{x\in X \ | \ |f_n(x)-f(x)|\geq \epsilon\})=0.$ ) (This is not a homework problem.) I'm finding this very difficult, probably because the definitions are so complicated. I'm not sure how to approach it. Usually when one tries to prove 'Cauchy implies convergent'-type statements it's just a routine application of the completeness of $\mathbb{R}$ . One approach may be to establish to existence of a Cauchy subsequence for each $x$ but that's more of a guess rather than an idea inspired by understanding. It's just difficult to get a strong enough intuitive understanding for what's going on to solve it. Fixing an $x$ and constructing $f(x)$ individually is not easy as it seems to depend on all the other $f(x)$ as well (measure being a more global property). Any small hints would be appreciated. Edit: Not sure if this is common thing on StackExchange but I wanted to make a copy of this question myself in order to get a hint for the problem instead of seeing the answer (and spoiling the problem).","['measure-theory', 'cauchy-sequences', 'real-analysis']"
3043964,"Prove any number $c \in [a, b]$ is a subsequential limit if $\lim\inf x_n = a$, $\lim \sup x_n = b$, $a\ne b$, $\lim(x_n -x_{n+1})=0$","I'm trying to solve the following problem: Let $\{x_n\}$ denote a bounded sequence. Prove that any number $c \in [a, b]$ is a subsequential limit of $\{x_n\}$ if: $$
\begin{cases}
\lim_{n\to\infty} (x_n - x_{n+1})=0\\
\lim\inf x_n = a\\
\lim \sup x_n = b\\
a\ne b
\end{cases}
$$ Here are some of my thoughts. We know that $x_n$ is bounded. Then by Bolzano-Weierstrass we may choose some subsequence such that it has a finite limit: $$
\exists c \in [a, b] : \lim x_{n_k} = c \iff \forall \epsilon_1 > 0 \exists N_1\in\Bbb N: \forall n_k > N_1 \implies |x_{n_k} - c| < \epsilon_1
$$ We are also given that limsup and liminf exist and therefore: $$
\exists N_2 \in \Bbb N : \forall n_k > N_2 \implies x_{n_k} \ge a \\
\exists N_3 \in \Bbb N : \forall n_k > N_3 \implies x_{n_k} \le b
$$ If we now choose $N$ to be $\max\{N_2, N_3\}$ we obtain: $$
\exists N = \max\{N_2, N_3\}: \forall n_k > N \implies a \le x_{n_k} \le b \tag1
$$ Also we are given the fact that $\lim (x_n - x_{n+1}) = 0$ : $$
\forall \epsilon_2 > 0, \exists N_4 \in \Bbb N: \forall n_k > N_4\implies |x_n - x_{n+1}| < \epsilon_2
$$ But if $\lim (x_n - x_{n+1}) = 0$ , then it is also true for the subsequences: $$
\forall \epsilon_3 > 0, \exists N_5 \in \Bbb N: \forall n_k > N_5\implies |x_{n_k} - x_{n_k+1}| < \epsilon_3 \tag 2
$$ Now I'm struggling to combine that facts in order to show that any $c \in [a, b]$ is a subsequential limit of $\{x_n\}$ , how do I proceed? Feels like i have to consider $(1)$ and $(2)$ in tandem in order to finish the proof.","['limits', 'calculus', 'limsup-and-liminf']"
3043996,Proving that $\exists f \in X^*$ : $f(x) = \|x\|^2$ and $\|f\| = \|x\|$,"Exercise : Let $X$ be a normed space. Prove that for all $x \in X$ there exists $f \in X^*$ , such that $f(x) = \|x\|^2$ and $ \|f\| = \|x \|$ . Thoughts : I apologise for not providing a proper attempt but this is one of the first such exercises I'm handling, so I seem at loss. Initially, I thought about the Riesz Representation Theorem, which could yield the result straightforward, but the space we are working over must be a Hilbert Space, which we do not know in the given exercise. The second possible solution could (and probably should) be based on the Hahn-Banach Theorem (or one of its results/applications) but I cannot see a way out. Any hints or elaborations will be greatly appreciated.","['normed-spaces', 'functional-analysis', 'hahn-banach-theorem', 'riesz-representation-theorem']"
3044007,An Integral Designed to be on the Very Cusp of Convergence,"Say we have an integer $n$ ∊ℕ₀ & a sequence of $n+1$ real numbers $\alpha_k\in[0,\infty)∀k$ , where $k=0\dots n$ , and using $\log^{[k]}$ to denote $k$ functionings of the logarithm ( $\log^{[0]}x\equiv x$ , $\log^{[1]}x\equiv \log x$ , $\log^{[2]}x\equiv \log\log x$ , etc), I would conjecture that, $∀n, $ the integral $$\int_{e\uparrow\uparrow n}^\infty{dx\over\prod_{k=0}^n(\log^{[k]}x)^{\alpha_k}}$$ diverges , when $\alpha_k=1∀k\leq n$ , or when with ascending $k$ the first $\alpha_k≠1$ is $<1$ ; and converges when with ascending $k$ the first $\alpha_k≠1$ is $>1$ . In the integral given the lower limit is chosen simply to keep the function in the denominator well clear of taking any argument that would result in a negative value being fed into the logarithm - the convergence|divergence of the integral is determined purely by the behaviour of the integrand as its argument $\to\infty$ . I am wondering whether this surmise is correct. My reasoning for supposing it is is that if the variable $y$ be substituted for $\log^{[n]}x$ , then in the denominator we shall have successive orders of functioning of the exponential of $y$ from right to left ... but each raised to the power of its index $\alpha_k$ in order from left to right; and in the numerator we shall have the same product of the same factors, by reason of the chain rule, but each with unit exponent. So that considering the factors from left to right, the first one that does not completely cancel will be the first one at which $\alpha_k$ differs from unity; and also the one with the highest order of application of the exponential function: and if that $a_k$ is $<1$ the remnant will be in the numerator, and if $>1$ , in the denominator. And the integral will diverge in the former case & converge in the latter, as subsequent remnants will be completely overruled, regardless of the size of their exponent, as an exponential of a variable always overrules a mere power of a variable, regardless of the relative sizes of the scaling of the exponential and the degree of the power ... and the comparison will be at least that . Finally, in the case of all the $\alpha_k$ till the last being $=1$ , there will be complete cancellation of the exponentials; and we shall be left with $$\int_{e\uparrow\uparrow n}^\infty{dy\over y^{\alpha_n}} ,$$ the convergence|divergence of which depends on $\alpha_n$ in the well-familiar way. I would also surmise that this theorem - if it indeed is one (and the question here is essentially whether it is one, and not merely a surmise, or incorrectly infererred) - translates into sum over integers. I'll refrain from fully explicating the logic of that surmise; but basically it's that if the correspondence between Σ & ∫ of $1/x$ holds by reason of the asymptotically-flat -ness of the logarithm, then it could reasonably be expected to hold when functions that are progressively yet asymptotically-flatter are factored-in.","['integration', 'limits', 'tetration']"
3044044,How to prove upper bound for partial sum of binomial coefficients,"I just come across this inequality for the upper bound of partial sum of binomial coefficients, that is \begin{array}
$\sum_{k=0}^{m}\binom{n}{k}\leq (\frac{en}{m})^{m}, 
\end{array} I have trying to prove it but not having much success. I have used the stirling forluma $n!=\sqrt{2\pi}n^{n+1/2}e^{^{-n+r(n)}}$ , where $r(n)\in (\frac{1}{12n+1}, \frac{1}{12n})$ , thus I get $\frac{1}{k!}\leq (\frac{e}{k})^{k}$ , and $\binom{n}{m}\leq (\frac{en}{m})^{m}$ for all intergers $m \in [1, n]$ , which is just one item case, but that still has a big gap with $\sum_{k=0}^{m}\binom{n}{k}\leq (\frac{en}{m})^{m}$ .
I just don't how to further expand the case the partial sum case? Any hints or insights would be helpful! Thanks!","['inequality', 'binomial-coefficients', 'combinatorics']"
3044050,"Eigenvalues of Operator on $L^{2}[0, 1]$","Let $X=L^{2}[0, 1]$ and $$Ax(t)=\int_{0}^{1}{ts(1-ts)x(s)}ds.$$ I have shown that this operator is compact, hence all non-zero elements of the spectrum $\sigma(A)$ are eigenvalues of $A$ . But then I am stuck to find the eigenvalues in order to find the spectrum, i.e. all $\lambda \ne 0$ such that $$\int_{0}^{1}{ts(1-ts)x(s)}ds=\lambda x(t).$$ Can someone help?","['operator-theory', 'functional-analysis', 'eigenvalues-eigenvectors']"
3044090,Defining Addition of Natural Numbers as the Algebra of 'Push-Along' Functions,"Let $N$ be a set containing an element $1$ and $\sigma: N \to N$ an injective function satisfying the following two properties: $\tag 1 1 \notin \sigma(N)$ $\tag 2 (\forall M \subset N) \;\text{If } [\; 1 \in M \land (\sigma(M) \subset M) \;] \text{ Then } M = N$ We call $(N, 1, \sigma)$ a Peano system. The set of all injective functions on $N$ form a semigroup under composition. Let $\mathcal C$ denote the set of all injective functions on $N$ that commute with $\sigma$ . It is easy to see that $\mathcal C$ is a commutative semigroup containing the identity transformation. Theorem 1: If $\mu,\nu \in \mathcal C$ and $\mu(1) = \nu(1)$ then $\mu = \nu$ . Proof Let $M$ be the set of all elements in $N$ where the two functions agree. Applying induction with $\text{(2)}$ , it is immediately evident that $M = N$ . $\quad \blacksquare$ Theorem 2: For any $m \in N$ , there exist a $\mu \in \mathcal C$ such that $\mu(1) = m$ . Proof A̶g̶a̶i̶n̶,̶ ̶s̶i̶m̶p̶l̶y̶ ̶a̶p̶p̶l̶y̶ ̶i̶n̶d̶u̶c̶t̶i̶o̶n̶.̶ See this . $\quad \blacksquare$ So $\mathcal C$ is in bijective correspondence with $N$ . Theorem 3: Let $(N, \sigma)$ and $(N', \sigma')$ be two Peano systems and $\mathcal C$ and $\mathcal C'$ the corresponding semigroups. Then there exist one and only one bijective  correspondence $\beta: N \to N'$ satisfying $\tag 3 \beta(1) = 1'$ $\tag 4 \beta \circ \sigma = \sigma' \circ \beta$ Proof The function $\beta$ is defined using recursion. Induction is used to show that $\beta$ is injective. Induction is used to show that $\beta$ is surjective. $\quad \blacksquare$ Using the above an argument can be supplied to prove the following. Theorem 4: Let $(N, \sigma)$ and $(N', \sigma')$ be two Peano systems and $\mathcal C$ and $\mathcal C'$ the corresponding semigroups. Then the mapping $\sigma \mapsto \sigma'$ can be extended to an algebraic isomorphism between $\mathcal C$ and $\mathcal C'$ . We reserve the symbol $\mathbb N$ to denote $\mathcal C$ and use the symbol $+$ to denote the binary operation of composition. Using the axioms of $ZF$ , the existence of Peano systems is no problem. I couldn't find this technique here or on wikipedia, prompting Is the theory described above coherent? If it is it would certainly appeal to students who like to see some 'motion/action' when studying mathematical constructions, say somebody born to be a functional analyst. Since there has been no feedback except for two upvotes, the theory is sound. So here we knock off the remaining properties of $(\mathbb N,0,1,+)$ that we need to have under our belts. When this is competed, these properties will completely define the natural numbers under addition up to isomorphism. Note that when convenient, we can always regard $0 \in \mathbb N$ as the identity mapping on $N$ and view the remaining elements in $\mathbb N$ as (proper) injections. Define the function $\alpha$ on $\mathbb N$ by $k \mapsto k +1$ . Theorem 5: $(\mathbb N, 0, \alpha)$ is a Peano system. Proof (sketch) Translating back to $\mathcal C$ , the identity fucntion can't have the form $\sigma \circ \tau$ , since $\sigma$ can't have a left and right inverse. So $\text{(1)}$ is satisfied. To show $\text{(2)}$ , you use induction on $N$ and theorem 1 and theorem 2 to show that there is 'complete coverage'. $\quad \blacksquare$ The prior theorem is stating that $(\mathbb N, 0, \alpha)$ can be viewed as the 'dual Peano system' to $(N, 1, \sigma)$ . Theorem 6: For $z,x,y \in \mathbb N$ , if $z + x = z + y$ , then $x = y$ . Proof Injective functions always have a left inverse. $\quad \blacksquare$ Definition: For $m, n \in \mathbb N$ , we write $m \le n$ if there exist a $k \in \mathbb N$ such that $m + k = n$ . Theorem 7: The relation $(\mathbb N, \le)$ is a total ordering. Proof (sketch) At all steps, when necessary invoke theorem 6 and use algebraic manipulations. Transitivity is straightforward. To show antisymmetry, use the fact that $m + n = 0$ implies that both $m$ and $n$ are equal to $0$ . To show the connex property, use induction and split out the logic. $\quad \blacksquare$ Theorem 8: Every nonempty subset of $\mathbb N$ has a least element.","['elementary-set-theory', 'abstract-algebra', 'proof-verification', 'natural-numbers']"
3044092,Reason for the integer case and the rational case to be solved differently,"Assume $f$ is continuous, $f(0)=1$ , and $f(m+n+1)=f(m)+f(n)$ for all real $m, n$ . 
  Show that $f(x) = 1 + x$ for all real numbers $x$ . This is referenced from Terence Tao’s solving mathematical problems and in the exercise he provided a hint; first prove this for integer $x$ , then for rational $x$ , then finally for real $x.$ The questions are as follows:
Why would there be a separate case to be considered for this question? Wouldn’t a direct method of solving suffice? Is there another way of approaching the question? Any help would be much appreciated.","['continuity', 'functions', 'functional-equations', 'real-analysis']"
3044095,What are the positive integer solutions to $x^2-x+1 = y^3$?,"The only solutions that I know of till now are $(x,y) = (1,1) \space , (19,7)$ . We can note that: $$x^2-x+1 = y^3 \implies (2x-1)^2 = 4y^3-3$$ Thus, if odd prime $p \mid y$ , then $(2x-1)^2 \equiv -3 \pmod{p}$ and thus, $-3$ is a quadratic residue. This implies that $p \equiv 1 \pmod{6}$ . How can we further proceed into this problem? Note: As mentioned in one of the links in the comments, if we instead replace $x$ by $x+1$ , we get $x^2+x+1 = y^3$ . Thus, the solutions for this is $(x,y) = (0,1) \space, (18,7)$ .","['number-theory', 'eisenstein-integers', 'elliptic-curves', 'diophantine-equations']"
3044116,General commutators of derivations of the exterior algebra,"Let $M$ be a smooth manifold and let $\Omega(M)$ be the exterior algebra of smooth differential forms over $M$ . The $\mathbb R$ -linear map $D:\Omega(M)\rightarrow\Omega(M)$ is a derivation of the exterior algebra of degree $r\in\mathbb Z$ if it maps $k$ -forms to $k+r$ forms and it satisfies the Leibniz-rule $$D(\omega\wedge\eta)=D\omega\wedge\eta+\omega\wedge D\eta.$$ This map is instead an antiderivation if it satisfies the anti-Leibniz rule $$D(\omega\wedge\eta)=D\omega\wedge\eta+(-1)^k\omega\wedge D\eta,$$ where $k$ is the degree of $\omega$ . It is a known fact that if $D,D^\prime$ are antiderivations of odd degrees, then the anticommutator $$\{D,D^\prime\}=DD^\prime+D^\prime D$$ is a derivation of degree $\deg D+\deg D^\prime$ . For example this is often used to prove Cartan's magic formula. I am looking for a more general version of this statement. Ideally, given any derivation or anti-derivation $D$ and derivation or antiderivation $D^\prime$ , some kind of ""graded commutator"" $$ [D,D^\prime ]_\pm=DD^\prime \pm D^\prime D $$ should be definable, which should result in either a derivation or antiderivation, whose degree is the sum of the degrees of $D$ and $D^\prime$ . I have not been able to find one such relation. In fact I have tried to prove that for antiderivations of arbitrary degree $DD^\prime -(-1)^{\deg D \deg D^\prime}D^\prime D$ is a derivation, but I couldn't cancel enough terms in the general case. I have also looked up ""graded commutator"" and ""graded super Lie algebra"" on Wikipedia, however it seems to me that case is different, as a graded derivation if defined with the anti-Leibniz rule $D(\omega\eta)=(D\omega)\eta+(-1)^{k\deg D}\omega (D\eta)$ , which is slightly different from our anti-Leibniz rule here. Question: How can I unify the treatment of derivations and antiderivations of the exterior algebra to (hopefully) turn them into a graded super Lie algebra. In particular, given any two derivations or antiderivations (including the case of one being one, the other being the other), how do I define a (graded) commutator that will produce a derivation or antiderivation?","['abstract-algebra', 'exterior-algebra', 'lie-superalgebras', 'differential-geometry']"
3044135,Finding a particular solution to a linear PDE,"I want to solve the PDE $$\frac{\partial u}{\partial t}+x_1(x_2-x_3) \frac{\partial u}{\partial x_1}+x_2(x_3-x_1) \frac{\partial u}{\partial x_2}+x_3(x_1-x_2) \frac{\partial u}{\partial x_3}=\sum_{i=1}^3 \alpha_i \frac{\partial f}{\partial x_i}, \tag{1} $$ where $\alpha_1,\alpha_2,\alpha_3$ are constants and $f$ is the function $$f(\mathbf{x},t)= \frac{\alpha _1 \left(\wp '\left(t;g_2,g_3\right)+x_2x_3 \left(x_2-x_3\right)\right)}{2 \left(\wp \left(t;g_2,g_3\right)-\frac{1}{12} \left(x_1+x_2+x_3\right){}^2+x_2 x_3\right)}+\frac{\alpha_2 \left(\wp '\left(t;g_2,g_3\right)+x_1 x_3 \left(x_3-x_1\right)\right)}{2 \left(\wp \left(t;g_2,g_3\right)-\frac{1}{12} \left(x_1+x_2+x_3\right){}^2+x_1 x_3\right)}+\frac{\alpha _3 \left(\wp '\left(t;g_2,g_3\right)+x_1x_2 \left(x_1-x_2\right) \right)}{2 \left(\wp \left(t;g_2,g_3\right)-\frac{1}{12} \left(x_1+x_2+x_3\right){}^2+x_1 x_2\right)}+\left(\alpha _1+\alpha _2+\alpha _3\right) \left(\zeta \left(t;g_2,g_3\right)+\frac{1}{12} t \left(x_1+x_2+x_3\right){}^2\right). $$ Here $\wp$ and $\zeta$ are the Weierstraß p- and zeta- functions respectively, with the elliptic invariants $$    \begin{align}
        g_2 &= \frac{(x_1+x_2+x_3)^4}{12}-2 x_1 x_2 x_3 (x_1+x_2+x_3), \\
        g_3 &= -(x_1 x_2 x_3)^2+\frac{x_1 x_2 x_3 (x_1+x_2+x_3)^3}{6}-\frac{(x_1+x_2+x_3)^6}{216}.
    \end{align} $$ From this point onward the invariants $g_2,g_3$ will not be shown explicitly. My attempt: First, I managed to solve the associated homogeneous PDE $$ \frac{\partial u_h}{\partial t}+x_1(x_2-x_3) \frac{\partial u_h}{\partial x_1}+x_2(x_3-x_1) \frac{\partial u_h}{\partial x_2}+x_3(x_1-x_2) \frac{\partial u_h}{\partial x_3}=0, $$ via the method of characteristics. The solution is given by $$ u_h =\Phi \left( X_1(\mathbf{x},t), X_2(\mathbf{x},t) ,X_3(\mathbf{x},t) \right) $$ where $\Phi$ is an arbitrary function, and $$X_1=\frac{12 x_1 x_2 x_3}{\left(x_1+x_2+x_3\right)^2-12 \left(\frac{\left(\wp'(t)-x_{2}x_{3} \left(x_{3}-x_{2}\right) \right)^2}{4 \left(\wp (t)+ x_{2}x_{3} -\frac{1}{12} \left(x_1+x_2+x_3\right)^2\right)^2}-\wp (t)+ x_{2} x_{3}-\frac{1}{12} \left(x_1+x_2+x_3\right)^2\right)}, \\
X_2 = \frac{12 x_1 x_2 x_3}{\left(x_1+x_2+x_3\right)^2-12 \left(\frac{\left(\wp'(t)-x_{1} x_{3} \left(x_{1}-x_{3}\right) \right)^2}{4 \left(\wp (t)+x_{1} x_{3}-\frac{1}{12} \left(x_1+x_2+x_3\right)^2\right)^2}-\wp (t)+x_{1} x_{3}-\frac{1}{12} \left(x_1+x_2+x_3\right)^2\right)}, \\
X_3 = \frac{12 x_1 x_2 x_3}{\left(x_1+x_2+x_3\right)^2-12 \left(\frac{\left(\wp'(t)-x_{1} x_{2} \left(x_{2}-x_{1}\right) \right)^2}{4 \left(\wp (t)+ x_{1} x_{2}-\frac{1}{12} \left(x_1+x_2+x_3\right)^2\right)^2}-\wp (t)+ x_{1} x_{2}-\frac{1}{12} \left(x_1+x_2+x_3\right)^2\right)}. $$ The final ingredient is a particular solution. Denoting the RHS in Equation (1) above by $R(\mathbf{x},t)$ , Duhamel's principle (or the method of characteristics again) suggests that a particular solution is given by $$u_p=\int_0^t R \left( \mathbf{X}(\mathbf{x},t-u),u \right) \mathrm{d} u .$$ I tried computing this with Mathematica and it didn't go well. This is probably because Mathematica seems to be unaware of the elliptic identity $\wp'^2=4 \wp^3-g_2 \wp -g_3$ . I would appreciate help with the evaluation of the integral above, or any other method of obtaining a particular solution of Equation (1). Thank you!","['integration', 'ordinary-differential-equations', 'elliptic-functions', 'closed-form', 'partial-differential-equations']"
3044138,The formal definition of “angle”,"My main question is about the definition of ""angle"". Many linear algebra textbooks define the angle between two vectors in terms of their inner product. I superficially understand that this corresponds to the law of cosine in Euclidean geometry. But since axiomatic geometry and analytic geometry seem to have completely different approaches in dealing with geometric concepts, I would like to know what exactly motivates this definition, and what are the results of this definition. First of all, in order for this definition to work, we need to define the sine and cosine function. Usually, these functions are first introduced and therefore defined and only defined within a purely geometric context. In particular, most elementary definitions of these two functions require a previous understanding of the idea of an angle, such as the definition using a unit circle. Therefore, my question is, how can we have a valid cosine function that works outside of axiomatic geometry before we can even define ""angle"" in $\mathbb{R}^n$ via the cosine function? I know cosine function can be defined via a series, but again, how do we justify this definition if we were to use it to define ""angle""? Secondly, using these definitions, along with some other definitions for elementary geometric concepts such as planes, can one prove everything that is provable via Euclid's axioms of geometry using only algebra in $\mathbb{R}^n$ without any of those axioms? If not, what other axioms must we include in order to do so? Thanks!","['euclidean-geometry', 'analytic-geometry', 'linear-algebra', 'geometry']"
3044194,What's the average life expectancy if only dying from accidents?,"So, I curious and trying to determine what sort life expectancy a human being would have if they were immortal (as in, no more senescence (aging)). Accidental deaths only. I've googled around and found numbers from a few hundred years to nearly 9000 years. Europe has some great statistics on accidental deaths - https://ec.europa.eu/eurostat/statistics-explained/index.php/Accidents_and_injuries_statistics#Deaths_from_accidents.2C_injuries_and_assault The headline is that 3.1% of deaths were accidents in 2015. However, it occurs to me that the best representative sample to use is people aged 15-25 who generally don't really die from illnesses and are in better health so less susceptible to things like falls (and more to traffic accidents). There's a graph on that page that shows accidental deaths are about 35% of all deaths for those age groups. Unfortunately I lack the mathematical chops to be able to take those numbers and merge them together meaningfully. So.... given the information above (and in the link if I've missed anything useful out), what's the approximate average European life expectancy if we ""solved"" senescence tomorrow?","['average', 'statistics']"
3044218,"If $ \sum a_n$ converges, does $\sum\limits_{n=2}^{\infty} \frac{\sqrt{a_n}}{\ln n}\cdot \left( n^{a_n}-1 \right)$ converge as well?","If $ \sum a_n$ converges and $a_n>0$ for every $n$ , does $\sum\limits_{n=2}^{\infty} \frac{\sqrt{a_n}}{\ln n}\cdot \left( n^{a_n}-1 \right)$ converge as well? What I did: Define $u_n=\frac{\sqrt{a_n}}{\ln n}\cdot \left( n^{a_n}-1 \right)$ . Trying to use the ratio test, I consider: $$ \frac{u_{n+1}}{u_n}=\frac{\ln(n)}{\ln(n+1)} \cdot \sqrt{\frac{a_{n+1}}{a_n}} \cdot\frac{n^{a_{n+1}}-1}{n^{a_{n}}-1}=\frac{\ln(n)}{\ln(n+1)} \cdot \sqrt{\frac{a_{n+1}}{a_n}} \cdot\frac{\frac{n^{a_{n+1}}}{n^{a_{n}}}-\frac{1}{n^{a_{n}}}}{1-\frac{1}{n^{a_{n}}}}$$ and now: $$ \frac{\ln(n)}{\ln(n+1)} \rightarrow 1 $$ $$ \sqrt{\frac{a_{n+1}}{a_n}} \rightarrow g \in (0;1) $$ but what can I do with the last ratio $$\frac{\frac{n^{a_{n+1}}}{n^{a_{n}}}-\frac{1}{n^{a_{n}}}}{1-\frac{1}{n^{a_{n}}}}\ ?$$ Or has somebody any other idea for this task?","['sequences-and-series', 'real-analysis']"
3044284,Definite Integral in functional relation format,"If $$2f(x) + f(-x) = \frac{1}{x}\sin\Biggl(x-\frac{1}{x}\Biggl)$$ then find the value of $$\int_{\frac{1}{e}}^ef(x)dx$$ (its not given that the function is odd or even)
I don't know where to start on this one I'm not much experienced in solving functional relations like these and I'm not even able to come up with an approach to solve it can someone help","['functional-equations', 'calculus', 'definite-integrals']"
3044286,Showing algebraic dependence of meromorphic functions on a compact Riemann surface,"I have been given the following question to do: Let $f,g$ be meromorphic functions on a compact Riemann Surface $R$ . Show that there is some polynomial such that $P(f,g) = 0$ (i.e. show that any two meromorphic functions 
on $R$ are algebraically dependent). I have seen this result over the torus which follows from looking  at the Weierstrass $\wp$ function, however I have no idea how to generalise that to every compact Riemann Surface. There is a hint which says I should let $d = m+n$ where $m,n$ are the valencies of $f,g$ respectively and consider $P(f,g) = \sum\limits_{j = 0}^d\sum\limits_{k = 0}^d a_{jk}f(z)^jg(z)^k$ and show that this has at most $d^2$ poles and that I can choose the $a_{jk}$ so that $P(f,g)$ has at least $d^2+2d$ roots and so is constant by the valency theorem. Showing that there are at most $d^2$ poles is easy but I don't know how to select the $a_{jk}$ to get $d^2+2d$ roots. I don't see whether I should try and find them explicitly (seems hard) or use some indirect argument (but I can't see where to start). Any help is much appreciated.","['riemann-surfaces', 'complex-analysis', 'meromorphic-functions', 'polynomials', 'compact-manifolds']"
3044308,Is the sigma algebra generated by $X$ random variable and its square equal to the sigma algebra generated by $X$ alone?,"I would like to understand which relationships hold among the sigma algebras $\sigma(X, X^2)$ , $\sigma(X)$ and $\sigma(X^2)$ , where X is a random variable. I would expect that $\sigma(X, X^2)=\sigma(X)$ . If this is true, then is $X^2$ $\sigma(X)$ -measurable? Moreover, I would like to know if there exist more sigma algebras w.r.t. a process is adapted. I am thinking that if $X_i$ are i.i.d. than $S_n=\sum_{i=1}^n X_i$ is $\mathcal{F}_n:=\sigma(S_n)$ -measurable but also $\mathcal{G}_n:=\sigma(X_1, X_2, \dots, X_n)$ -measurable. What changes if I take $\{\mathcal{F}_n\}$ instead of $\{\mathcal{G}_n\}$ as a filtration for the process $\{S_n\}$ ? Because for both the process is a martingale!","['stochastic-processes', 'measure-theory', 'martingales', 'probability']"
3044309,How to show an equation gives a closed curve,"I am analyzing the following dynamical system: $\ddot{x} + x +\epsilon x^3 = 0$ , which I have rewritten to 2D system $\dot{x} = y, \dot{y} = -x-\epsilon x^3$ . I have found that this is a conservative system with conserved quantity $E(x, y) = \frac{1}{2}y^2 + \frac{1}{2}x^2 +  \frac{1}{4}\epsilon x^4$ . I am now trying to show that if $\epsilon>0$ , all orbits are closed. I have already found that in this case the only fixed point is the origin. I suspect that if I can show that the level curves of the conserved quantity are closed, this will imply the orbits also being closed. My first question is whether this reasoning is valid. My second question (and my main question) is how to show that an equation defines a closed curve. The level curves of the conserved quantity that result in an orbit are given by $\frac{1}{2}y^2 + \frac{1}{2}x^2 +  \frac{1}{4}\epsilon x^4 = K$ , for $K>0$ . This looks similar to a circle, but not quite. How can I show that it is indeed still a closed curve?","['curves', 'ordinary-differential-equations', 'dynamical-systems']"
3044375,"If two random variables are independent, why isn't their min and max?","Suppose $X_1, X_2$ are independent $U(0, 1)$ random variables, and $$Y = \min(X_1, X_2) $$ $$Z = \max(X_1, X_2) $$ By this question, they $Y$ and $Z$ should be independent: Are functions of independent variables also independent? But by this answer the covariance is not zero: What is cov(X,Y), where X=min(U,V) and Y=max(U,V) for independent uniform(0,1) variables U and V? How do I reconcile these two things? The $\min$ and $\max$ are a function of independent random variables, yet they have covariance.","['statistics', 'probability-theory', 'probability']"
3044380,Dividing by functions of $y$ in differential equations without knowledge of them being non-zero?,"A lot of books solve separable differential equations like this: $$ \frac{dy}{dx} = g(x)h(y),\  for \ x=x_0, y=y_0 \Leftrightarrow  \frac{dy}{dx}\frac{1}{h(y)} = g(x) \Rightarrow \int \frac{dy}{dx}\frac{1}{h(y)} dx = \int g(x)dx $$ ...and so on. My problem arises in the second step, where authors divide with $ h(y) $ . How do we know that $h(y)$ is not zero for all $x$ ? Why do they make that (unsafe) step? Some further thoughts Let's consider the differential equation $ f'(x) = f(x) \ \forall x \in \Bbb R $ with initial condition $ f(x_0)=0 $ where $x_0$ is some real number. I will solve this differential equation by using the integrating factor: $$ I(x) = e^{-x}$$ The differential equation becomes: $$ f'(x)=f(x) \Leftrightarrow f'(x)+(-f(x))=0\Leftrightarrow f'(x)e^{-x}+(e^{-x})'f(x)=0 \Leftrightarrow$$ $$ (e^{-x}f(x))'=0\Rightarrow e^{-x}f(x)=C\Leftrightarrow f(x)=Ce^{x} $$ Evaluating the initial condition, we get: $C=0$ and therefore $f(x) =0 \ \forall x \in \Bbb R$ If we had approached the same problem by the strategy shown in the beginning, the second step would have been completely incorrect.","['calculus', 'functions', 'ordinary-differential-equations', 'real-analysis']"
3044422,Mapping of an ellipse to an ellipse with different eccentricity that maps focal points to focal points,"The title describes what I'm looking for: Is there a (canonical) way of mapping an ellipse (interior and boundary) to an ellipse with different eccentricity that maps focal points to focal points? (Obviously, an affine linear map won't do it and I think it can't be a conformal mapping either.) Put in other words: Is there among all (let's say differentiable) maps that map a given ellipse (interior and boundary) to a given other ellipse and preserving the focal point one canonical (e.g. based on geometric reasons or extremal principles, e.g. mean square of deplacement of the points is minimal or the like). P.S. Note that you won't be able to map first to a circle then to the other ellipse,  because mapping to the circle won't be injective (both focal points map to the center.","['conic-sections', 'geometry']"
3044450,Show that the Normal distribution is a member of the exponential family,"I want to show that the Normal distribution is a member of the exponential family. I have been working under the assumption that a distribution is a member of the exponential family if its pdf/pmf can be transformed into the form: $f(x|\theta) = h(x)c(\theta)\exp\{\sum\limits_{i=1}^{k} w_{i}(\theta)t_{i}(x)\}$ This is my approach: $f(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\{-\frac{(x-\mu)^2}{2 \sigma^2}\}$ Taking the logs: $\log f(x|\mu, \sigma^2) = -\frac{1}{2}\log(2\pi\sigma^2) - \frac{(x-\mu)^2}{2 \sigma^2}$ Taking the exponential: $f(x|\mu, \sigma^2) = \exp\{-\frac{1}{2}\log(2\pi\sigma^2)-\frac{(x-\mu)^2}{2\sigma^2}\}$ = $\exp\{-\frac{1}{2}\log(2\pi\sigma^2)-\frac{(x^2 -2\mu + \mu^2)}{2\sigma^2}\}$ = $\exp\{-\frac{1}{2}\log(2\pi\sigma^2)-\frac{x^2}{2\sigma^2} + \frac{2x\mu}{2\sigma^2} - \frac{\mu^2}{2\sigma^2}\}$ = $\exp\{-\frac{1}{2}\log(2\pi\sigma^2)\} \exp\{-\frac{x^2}{2\sigma^2} + \frac{x\mu}{\sigma^2} - \frac{\mu^2}{2\sigma^2}\}$ = $\frac{1}{\sqrt{2\pi\sigma^2}} \exp\{-\frac{x^2}{2\sigma^2} + \frac{x\mu}{\sigma^2} - \frac{\mu^2}{2\sigma^2}\}$ = $\frac{1}{\sqrt{2\pi\sigma^2}} \exp\{-\frac{\mu^2}{2\sigma^2}\} \exp\{-\frac{x^2}{2\sigma^2} + \frac{x\mu}{\sigma^2}\}$ Now I have: $c(\theta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\{-\frac{\mu^2}{2\sigma^2}\}$ , $w_{1}(\theta) = -\frac{1}{2\sigma^2}$ , $t_{1}(x) = x^2$ , $w_{2}(\theta) = \frac{\mu}{\sigma^2}$ , $t_{1}(x) = x$ But I am missing $h(x)$ . Am I missing something?","['statistics', 'normal-distribution']"
