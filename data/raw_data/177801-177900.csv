question_id,title,body,tags
3212108,Solve $4+\frac{1}{x}-\frac{1}{x^2}$ using quadratic formula,"I am to solve for x using the quadratic formula: $$4+\frac{1}{x}-\frac{1}{x^2}=0$$ The solution provided in the answers section is: $\dfrac{-1\pm\sqrt{17}}{8}$ whereas I arrived at something entirely different: $$\dfrac{\frac{1}{x}\pm\sqrt{\frac{1}{x^2}+\frac{16}{x}}}{\frac{2}{x}}$$ Here's my working: Start with $$4+\frac{1}{x}-\frac{1}{x^2}=0$$ Rearranging into standard form: $$-\frac{1}{x^2}+\frac{1}{x}+4=0$$ Multiply by $-1$ to get a positive leading coefficient $a$ : $$\frac{1}{x^2}-\frac{1}{x}-4=0$$ I'm not sure how to determine my inputs $a,b$ and $c$ with these fractions but I guess $a=\dfrac{1}{x^2}$ , $b=\dfrac{1}{x}$ and $c=-4$ . Plugging into quadratic function: $$x = \frac{-\frac{1}{x}\pm\sqrt{\frac{1}{x^2}+\frac{16}{x}}}{\frac{2}{x}}$$ I find this challenging due to the coefficients $a$ and $b$ being fractions. How can I apply the quadratic formula to $4+\dfrac{1}{x}-\dfrac{1}{x^2}=0$ to arrive at $\dfrac{-1\pm\sqrt{17}}{8}$ ?",['algebra-precalculus']
3212129,A possible way to prove non-cyclicity of eventual counterexamples of the Collatz conjecture?,"I've been recreatively working on the Collatz conjecture for a few months now, and I think I may have found something that could potentially prove at least half of the conjecture, which is the non-existence of non-trivial cycles. $\textbf{If you want to tl;dr}$ , just check the framed equations. The first one is my conjecture, and the second one is a corollary which shows that if the conjecture is correct with all the conditions and everything, it would contradict the existence of non-trivial cyclic patterns. $\textbf{This is supposed to lead to a proof by contradiction}$ and so far, it seems to work. Otherwise, you could what I've done to get to this conjecture idea (because I'm narrating it chronologically so you can sort of get my process). I've not seen any peer-reviewed proof of their inexistence, so I guess it's still an open problem by itself. The fact is, I really think this conjecture is manageable, I just think I don't have the level required to tackle this kind of thing. Anyway, first things first, I didn't use the usual $$a_0\in\mathbb N,~a_{n+1}=\left\{\begin{array}{cc}(3a_n+1)/2&a_n~\rm odd\\a_n/2&\rm otherwise\end{array}\right.$$ but a more dynamical subsequence, which I randomly called $(e_n)$ , defined with $$e_0=\frac{a_0}{2^{\nu_2(a_0)}},~e_{n+1}=\frac{3e_n+1}{2^{\nu_2(3e_n+1)}}$$ where $\nu_2$ is the 2-adic valuation. This basically chops off all the even numbers and basically keeps the core dynamics of the sequences. First off, I had to prove by induction that $$\begin{array}{ccccc}
e_{n+1}&=&3^n\left(3e_0+1+\sum\limits_{k=1}^n\frac1{3^k}\prod\limits_{\ell=0}^{k-1}2^{\nu_2(3e_\ell+1)}\right)\prod\limits_{k=0}^n\frac1{2^{\nu_2(3e_k+1)}}&n\ge1&(1)\\
&=&3^n\left(3e_0+\left(\sum\limits_{k=0}^n\frac1{3^k}\prod\limits_{\ell=k}^n\frac1{2^{\nu_2(3e_\ell+1)}}\right)\prod\limits_{k=0}^n{2^{\nu_2(3e_k+1)}}\right)\prod\limits_{k=0}^n\frac1{2^{\nu_2(3e_k+1)}}&n\ge0&(2)
\end{array}$$ However, $\nu_2(3e_k+1)$ has a very chaotic behaviour for $k\in\mathbb N$ , so I had to bound it in some way or another. First obvious bound is that $\nu_2(3e_k+1)\ge1$ , since from how the sequence is defined, $3e_k+1$ is even. Hence, I deduced that $$e_{n+1}\prod_{k=0}^n2^{\nu_2(3e_k+1)}\le3^{n+1}e_0+\frac{3^n}{2^{n+1}}\left(\sum_{k=0}^n\left(\frac23\right)^k\right)\prod_{k=0}^n2^{\nu_2(3e_k+1)}$$ Since $\sum\limits_{k=0}^n\left(\frac23\right)^k<3$ for all $n\in\mathbb N$ , I found out that $$e_{n+1}\prod_{k=0}^n2^{\nu_2(3e_k+1)}<3^{n+1}e_0+\frac{3^{n+1}}{2^{n+1}}\prod_{k=0}^n2^{\nu_2(3e_k+1)}\\
\iff\frac1{e_0}\left(e_{n+1}-\left(\frac32\right)^{n+1}\right)\prod_{k=0}^n2^{\nu_2(3e_k+1)}<3^{n+1}$$ Now, I need to use a bit of a trick here. I'll assume $e_0$ to be minimal. In fact, for all $(e_n)$ which doesn't get to the trivial sequence, it can be shown that there are infinitely many $k\in\mathbb N$ such that for all $n\ge k$ , $e_k\le e_n$ , so this trick can describe literally any counterexample of the Collatz conjecture. Therefore, we get $$\prod_{k=0}^n2^{\nu_2(3e_k+1)}<\frac{3^{n+1}}{1-\frac1{e_0}\left(\frac32\right)^{n+1}}$$ if and only if $n+1 < \log_{3/2}e_0$ . Since we know that for all $e_0\le87\times2^{60}$ , $(e_n)$ is not a counterexample, we have $$\prod_{k=0}^n2^{\nu_2(3e_k+1)}<\frac{3^{n+1}}{1-\frac1{87\times2^{60}}\left(\frac32\right)^{n+1}}$$ for all $n+1 < \log_{3/2}(87\times2^{60})\approx113.58\ldots$ Hence, we got that $$\sum_{k=0}^n\nu_2(3e_k+1)<(n+1)\log_23-\log_2\left(1-\frac1{87\times2^{60}}\left(\frac32\right)^{113}\right)$$ for $n\le112$ . So, to sum it up, we just bounded $\sum\limits_{k=0}^n\nu_2(3e_k+1)$ is bounded from above by $(n+1)\log_23+c$ for some constant $c$ . Yet, we can also derive that for all $n\le107$ , $$\sum_{k=0}^n\nu_2(3e_k+1)<(n+1)\log_23$$ (NB : The $107$ is here because $\left\lfloor(n+1)\log_23\right\rfloor=\left\lfloor(n+1)\log_23-\log_2\left(1-\frac1{87\times2^{60}}\left(\frac32\right)^{n+1}\right)\right\rfloor$ for all natural $n\le107$ ). Anyway, basically, here is my conjecture : If $(e_n)$ doesn't converge to 1 and that for all $n\in\mathbb N$ we have $e_0\le e_n$ , then for all $n\in\mathbb N$ , $$\begin{array}{|c|}\hline\sum\limits_{k=0}^n\nu_2(3e_k+1)<(n+1)\log_23\\\hline\end{array}$$ I even have some numerical evidence supporting it. With a little algorithm which basically computes, for any $e_0$ , the sum $\sum\limits_{k=0}^n\nu_2(3e_k+1)$ and checks whether or not it's below $(n+1)\log_23$ for as long as for all $k\le n$ , we have $e_0\le e_k$ . Checked all odd $e_0$ from $3$ to $29\;322\;479$ and it worked, so I'm pretty confident with that ! Now, how is this even related to the non-existence of cyclic sequences ? Well, if we assume this conjecture and using formula $(2)$ , we'd have for minimal $e_0$ and $n\ge1$ $$\begin{array}{|c|}\hline e_{n+1}\ge 3^{n+1}\left(e_0+1/3+2/9\right)\frac1{3^{n+1}}=e_0+5/9>e_0\\\hline\end{array}$$ But this means that we could only reach $e_0$ once, which is a contradiction to cyclicity if it works for all minimal $e_0$ . So basically, if my upper bound turns out to be correct for all minimal $e_0$ and $n\ge0$ (or $n\ge1$ to be cautious but anyway), this would essentially imply that there is no non-trivial cycle ! I'm putting this here so people could eventually work out a way to prove it. Obviously tried by myself, but I figured out I might not be good enough for this !","['collatz-conjecture', 'sequences-and-series']"
3212165,"Sample $k$ of $n$ numbers (with replacement), what is the probability for a certain from the $n$ numbers to be the median of the $k$ numbers",Suppose we have ordered numbers $a_1 < a_2 < \dots < a_n$ . Now we sample $k$ of them with equal probability and with replacement and compute the median of these and call it $m$ . (Let us assume that $k$ is odd for the sake of simplicity.) What is the probability the $m = a_i$ for each $i$ ? I want to know this to construct the probability mass function for the median of $k$ random draws from a sample of $n$ numbers.,"['statistics', 'median', 'probability']"
3212181,Smooth images of manifolds are immersed?,"in various papers in symplectic geometry, I have encountered the following argument. Statement: Suppose $f: M \rightarrow N$ is a smooth map of constant rank . Then its image $f(M)$ can be equipped with a structure of an immersed submanifold of $N$ , that is a topology and a smooth structure making the canonical inclusion $f(M) \hookrightarrow N$ into a smooth injective immersion. Some people even say that with respect to this $f: M \rightarrow f(M)$ becomes a surjective submersion. This I do not believe, as this would mean that every smooth map of constant rank can be written as a composition of a smooth surjective submersion followed by a smooth injective immersion, which is not (people say) true for global manifolds. !!Edit:!! Maybe it is true that every subimmersion (and thus also a map of constant rank) is actually always a composition of a submersion followed by an immersion. I have found this both in Bourbaki (Book X, section about subimmersions) or in super-detailed exposition in the book: https://books.google.cz/books?id=G8nDGS5RfjQC See Proposition 5.2.7 there. Maybe this somehow does not apply here as they consider a bigger class of smooth manifolds (namely, they do not force them to be Hausdorff and second countable). In particular, they use the manifold of germs of submanifolds, as does Bourbaki. Edit 2: Bourbaki et al say that subimmersion is a composition of submersion followed by immersion. However, they do not say that submersion is surjective and immersion injective. Hence it is useless here. I have tried two approaches, both in a sense using the constant rank theorem. Attempt 1 : This uses the rank theorem directly. Let $n = \dim(M)$ , $k = \dim(N)$ and $r = rank(f)$ . Then, for every $m \in M$ , there exist coordinate charts $(U,\varphi)$ and $(V,\psi)$ for $M$ and $N$ , respectively, such that (i) $m \in U$ , $\varphi(U) = W \times W'$ , where $W \subseteq \mathbb{R}^{r}$ , $W' \subseteq \mathbb{R}^{n-r}$ are open sets and $\varphi(m) = (0,0)$ . (ii) $f(U) \subseteq V$ , $\psi(V) = W \times W''$ , where $W \subseteq \mathbb{R}^{r}$ , $W'' \subseteq \mathbb{R}^{k-r}$ are open and $\psi(f(m)) = (0,0)$ . (ii) The local form $\hat{f} = \psi \circ f \circ \varphi^{-1}$ in these coordinates has the form \begin{equation}\hat{f}(x,y) = (x,0), \text{ for all } (x,y) \in W \times W'.\end{equation} The idea is to directly use these coordinates to define the coordinate atlas on $f(M)$ . Namely, consider the sets $U_{0} := f(U)$ together with a map $\psi_{0}: U_{0} \rightarrow W$ which is obtained from $\psi$ by restricting it to $f(U)$ . This makes sense as $\psi(f(U)) = W \times \{0\}$ . Suppose there is another such defined chart, say $\psi'_{0}: U'_{0} \rightarrow Z$ , which is obtained from some local charts $(U',\varphi')$ and $(V',\psi')$ having the properties (i) - (iii) where we replace the open sets $W,W',W''$ with $Z,Z',Z''$ , such that $U_{0} \cap U'_{0} \neq \emptyset$ . In order to say something about the transition maps between these two charts, one would have to prove that $\psi'_{0}(U_{0} \cap U'_{0})$ and $\psi_{0}(U_{0} \cap U'_{0})$ are open subsets of $\mathbb{R}^{r}$ , This is where I fail and I even doubt that this is true in general. If one would be able to prove this, it is then easy to see that the transition maps are smooth and the composition $\psi \circ I \circ \psi_{0}^{-1}$ is just inclusion $x \mapsto (x,0)$ , hence $I$ is a smooth injective immersion. Remark 1: This construction in fact should fail, as the composition $\psi_{0} \circ f \circ \varphi^{-1}$ is the projection $(x,y) \mapsto x$ , hence $f: M \rightarrow f(M)$ a smooth surjective submersion. This would contradict the above observation that not every smooth map of constant rank can be decomposed as a smooth surjective submersion followed by a smooth injective immersion. Remark 2: If $f$ is assumed open or closed, the statement is true. In fact, then $f(M)$ is an embedded submanifold of $N$ . This is because (say if $f$ is open) the sets $f(U) \cap V$ together with the restriction of $\psi$ work as submanifold charts for $f(M)$ . Attempt 2: Here I will be brief, as I in fact suspect it to be just the first paragraph in disguise. Every map $f$ of constant rank is so called subimmmersion , each point $m \in M$ has a neighborhood $U$ , such that there exists a smooth manifold $M$ and a pair of maps $h: U \rightarrow P$ and $g: P \rightarrow N$ , such that $f|_{U} = g \circ h$ and (i) $h$ is a smooth surjective submersion, (ii) $g$ is a smooth injective immersion. The idea is now to equip $f(U) = g(P)$ with a smooth structure and topology by declaring it diffeomorphic to $P$ . To finish, one would have to prove that on overlaps $f(U) \cap f(U')$ , the induced smooth structures would be compatible. To my understanding, one would have to prove that the sets $g^{-1}(f(U'))$ and $g'^{-1}(f(U))$ would have to be shown diffeomorphic. Yet it is not clear that they are even open in $P$ and $P'$ , respectively. Any suggestions? I would be happy to read it in the literature, yet I have not found anything reasonable (usually, only level manifolds for constant rank maps are examined, or they do not like immersed submanifolds at all). Concluding remarks: Maybe something completely different has to be employed, say some Frobenius theorem (gives immersed submanifolds) or some transversality business?","['submanifold', 'smooth-functions', 'smooth-manifolds', 'differential-geometry']"
3212210,How to know when you can focus only on a specific part of an expression with an even power?,"I stumbled upon this problem as I was going through the Algebra 2 course of https://brilliant.org . It goes as follows: Starting from $x = \sqrt{\sqrt{3 \sqrt{\sqrt{3 \sqrt{\sqrt{3 \dots}}}}}}$ , I manage on my own to reach the following equation: $x^4 = 3x$ . Personally I took it from there as follows: $\frac{x^4}{x} = 3 \leftrightarrow x^3 = 3 \leftrightarrow x=\sqrt[3]{3}$ . The course accepts my final answer of $x = \sqrt[3]{3}$ as correct,
however they reason as follows: $x^4 = 3x \leftrightarrow x^4 - 3x = 0 \leftrightarrow x(x^3 - 3) = 0$ ,
since in the original equation $x > 0$ , we only worry about the root $x^3 - 3$ : $x^3 - 3 = 0 \leftrightarrow x^3 = 3 \leftrightarrow x = \sqrt[3]{3}$ . They do reach to the same answer but that is not important when learning. What bothers me is that I do not seem to understand on why the $x > 0$ part is important here, and how they use that to form their conclusion. It also makes me wonder about how it impacts my approach to the solution. And in general it makes me wonder if there are guidelines that can help me decide on what properties of an original equation I need to take into account when transforming equations while solving a problem. I suppose it is similar as how when $x$ is in the denominator of the original equation than $x \neq 0$ , no matter how you can transform the equation (e.g. cancel out the denominator). I do however not see at the moment why it would apply here.",['algebra-precalculus']
3212218,"lim$_{m , n \to \infty} S(n,m)$ exists but iterated limits do not.","$S(n,m)$ is a double sequence. Can anyone give me an example where lim $_{m , n \to \infty}  S(n,m)$ exists but lim $_{n  \to \infty}$ ( lim $_{m  \to \infty}  S(n,m)$ )  ,  lim $_{m  \to \infty}$ ( lim $_{n  \to \infty}  S(n,m)$ ) do not? My Attempt: I thought of an example. $S(1 ,m) =m $ , $S(n , 1) =n $ , $S(n,m) = 1 $ otherwise. But this does not seem to be a good example . As we ignore the first row and first column , two iterated limits exist. I want an example where  lim $_{m  \to \infty}  S(n,m)$ ,  lim $_{n  \to \infty}  S(n,m)$ will not exist for infinitely many $n$ and $m$ respectively.","['double-sequence', 'examples-counterexamples', 'real-analysis', 'sequences-and-series', 'limits']"
3212219,Is there another name for Goursat's Lemma on subgroups of a direct product of groups?,"I'm having trouble finding a textbook that discusses Goursat's Lemma on subgroups of a direct product of groups. I've looked in several standard Algebra textbooks and I've only seen it in Serge Lang's ""Algebra"" as an exercise. Is it more commonly known by another name, or perhaps subsumed by a more commonly-taught theorem? Bonus points, but not required: if not, why isn't it included in these texts? The direct product is one of the standard first constructions and it seems like one of the first questions one would ask is ""what is known about the subgroups of $G \times H$ ?""","['direct-product', 'group-theory', 'abstract-algebra', 'reference-request']"
3212244,Realizing every rotation of a tangent space on a sphere as a parallel transport,"I am taking a course on elementary differential geometry, in which we use Do Carmo ""Differential Geometry of Curves and Surfaces"" as our textbook.
I have handed in a written assignment solving - well, trying to - exersice 4.4.22 in this book, and my solution was not approved. My problem is that I do not understand why my solution is inadequate, and my tutor is unavailable for the time being. I start by sketching my solution, and then I will paraphrase his critique of it. The problem reads: Let $S^2={(x,y,z) \in R^3;x^2+y^2+z^2=1}$ and let $p \in S^2$ . For each piece-wise parameterized curve $\alpha:[0,l]→S^2$ with $α(0)=α(l)=p$ , let $P_{\alpha}:T_p(S^2)→T_p(S^2)$ be the map which assigns to each $v \in T_p(S^2)$ its parallel transport along $α$ back to p. By prop.1, $P_α$ is an isometry. Prove that for every such rotation $R$ of $T_p(S^2)$ there exists an $α$ such that $R=P_α$ . $\textbf{My solution:}$ Let $v \in T_p (S^2)$ , and let $p \in S^2$ be at the north pole of the sphere. Let $\theta \in [0,\pi]$ , and let $R_{\theta}: T_p (S^2) \rightarrow T_p (S^2)$ be a map that rotates $v$ by $\theta$ . Let $w=R_{\theta}(v)$ . We wish to show that such a rotation can be realized a parallel transport along some closed, piecewise regular parametrized curve $\alpha$ . We construct $\alpha$ as follows: 1) Move along the meridian defined by $\dot{\alpha}(0) = v$ from p at the north pole to the equator. 2) Move along the equator through the angle $\theta$ 3) Move along a meridian back to $p$ We have shown in exercise 4.15 a) that the parallel transport of an arbitrary $v \in T_p (S^2) $ along $\alpha$ makes an angle $\theta$ with respect to $v$ , that is, $P_{\alpha} (v) = w $ Since $\alpha$ is composed of three geodesics, it is a piecewise regular parametrized curve, and since for any $\theta \in [0, \pi]$ , we have that $P_{\alpha} (v)= R_{\theta} (v) $ for all $v \in T_p ( S^2) $ , we thus conclude that $P_{\alpha} = R_{\theta} $ , as desired. $\textbf{My Tutors response}$ : ""The problem is that your transformation depends on the given $v \in T_p (S^2)$ . You need to consider that when your transformation rotates this $v$ through an angle $\theta$ , it also rotates all other elements of $T_p (S^2) $ through the same angle."" I do not understand the problem with my transformation. I have chosen an arbitrary element $v \in T_p (S^2)$ , and I have shown that a rotation in $T_p (S^2 ) $ of such an element through an arbitrary angle $\theta$ can be realized as the parallel transport of a curve $\alpha$ as constructed above. Why is this not enough? What am I missing? Thanks in advance","['geodesic', 'surfaces', 'differential-geometry', 'tangent-spaces', 'rotations']"
3212258,How to calculate $\frac{\partial^2}{\partial \theta^2} \bigg(\frac{1}{n} \sum_{i=1}^{n} \exp(-Y_i (\theta^T X_i))\bigg)$?,"I have a learning sample $D_n = f(X_i, Y_i)_{i=1}^n$ where the $X_i$ ’s are $\mathbb{R}^d$ -valued and the $Y_i$ ’s are $\{-1, 1\}$ -valued. $$
f(\theta) = \frac{1}{n} \sum_{i=1}^{n} \exp(-Y_i (\theta^T X_i))
$$ Where $\theta \in [-B, +B]$ . I want to calculate the Hessian matrix: $\nabla^2 f(\theta)$ .","['hessian-matrix', 'matrix-calculus', 'derivatives']"
3212303,"$X_f$ of locally ringed space $(X, O_X)$.","Let $(X, O_X)$ be a locally ringed space. $f \in \Gamma(X,O_X)$ be a global section. $$X_f:= \{ x \in X \, ; \, f_x \text{ is invertible in } O_{X,x} \} $$ It is claimed that $X_f$ is an open subset The image of $f$ in $\Gamma(X_f, O_X)$ is invertible. How does one see this?","['ringed-spaces', 'algebraic-geometry', 'schemes', 'sheaf-theory']"
3212314,Correlation between two sequences of irrational numbers,"Let us consider the sequence $x(n+1) = \{b+x(n)\}$ with $x(0) = 0$ . Here the brackets represent the fractional part function. Thus $x(n)= \{nb\}$ is related to Beatty sequences. If $b$ is irrational, it is known that the numbers $x(n)$ are uniformly distributed, with a lag- $k$ auto-correlation equal to $6(b+k)^2 + 6(1-2\lfloor b+k+1\rfloor)(b+k)+6\lfloor b+k+1\rfloor^2
-6\lfloor b+k+1\rfloor+1$ . This result follows from section 5.4 in this article as well as results published here . Also, if $b_1$ and $b_2$ are two irrational numbers that are linearly independent over the set of rational numbers, then the correlation between the two sequences (one generated with $b_1$ and the other one with $b_2$ ) is zero. But what if $b_1 = -1 + \sqrt{5}/2$ and $b_2 = 2/\sqrt{5}$ ? In that case, I know with absolute certainty (yet with no proof so far) that the correlation between the two sequences is 1/20 = 0.05. How do you prove this result?","['statistics', 'number-theory', 'irrational-numbers', 'sequences-and-series', 'probability']"
3212342,Conjugacy of Singer cyclic groups in $\mathrm{P\Gamma L}$,"Motivation This is kind of a follow-up to this question on conjugacy of Singer cyclic groups in GL . The ""original"" definition of a Singer cycle is not in the GL, but the following slightly different geometric setting: Definition A cyclic group $\Sigma$ of collineations of a finite projective geometry $\mathrm{PG}(k-1,q)$ is called a Singer cyclic group if $\Sigma$ acts regularly on the set of points. 
(Any generator of a Singer cyclic group is called a Singer cycle ). The group of collineations of $\mathrm{PG}(k-1,q)$ is given by $\mathrm{P\Gamma L}(k,q)$ (with its natural action on the set of subspaces of $\mathrm{GF}(q)^k$ ). Necessarily, the order of $\Sigma$ equals the number of points in $\mathrm{PG}(k-1,q)$ , which is $\frac{q^{k}-1}{q-1}$ . Question Are any two Singer cyclic subgroups of $\mathrm{P\Gamma L}(k,q)$ conjugate? First thoughts My feeling is that the answer should still be ""yes"", but I don't know for sure. A first step could be to think about the question in PGL: Take a Singer cycle, find a Singer cycle preimage in GL and apply the GL-result. However, so far I don't see a rigorous argument why there should always be a suitable preimage in GL.","['projective-geometry', 'finite-fields', 'finite-groups', 'finite-geometry', 'group-theory']"
3212354,"$f \in C[-1,1]$, Prove ${\lim_{h \to 0^+}}{\int_{-1}^1 \frac{h}{h^2+x^2}f(x)\,dx} = \pi f(0)$","I took a look at the special situation that $f=1$ , $${\lim_{h \to 0^+}}{\int_{-1}^1 \frac{h}{h^2+x^2}f(x)\,dx} ={\lim_{h \to 0^+}}{\int_{-1}^1 \frac{h}{h^2+x^2}\,dx} = \left.{\lim_{h \to 0^+}} \arctan{\frac{x}{h}} \right|_{-1}^1 = \pi $$ but I don't know how to find the next step.","['calculus', 'definite-integrals']"
3212429,When is supremum of the expectation equal to the expectation of the supremum using control processes,"This is a question I have from stochastic control. I know that in general $\underset{y\in \mathcal Y} \sup \mathbb E\left[f(X,y)\right]\leqslant \mathbb E\left[\underset{y\in \mathcal Y} \sup f(X,y)\right]$ . I normally would assume that the reverse inequality does not always hold, just like in typical inequalities, such as with Jensen's inequality , but in the proof below, I see that a similar equality is proven, which makes it seem as if $\underset{y\in \mathcal Y} \sup \mathbb E\left[f(X,y)\right] = \mathbb E\left[\underset{y\in \mathcal Y} \sup f(X,y)\right]$ holds, but I don't fully understand/agree with part of the proof. In the following, $\mathbb E\left[f(X_T)|\mathcal F_t;\pi\right]$ , is saying that the stochastic process $X_t$ is controlled by the process $\pi$ , not that $\pi$ is assumed known. The argument is from Markov Decision Processes and Dynamic Programming at the top of page 9. In the solution for Bellman's principle (discrete time), I have seen that the following is done, for admissible control processes $\pi$ : \begin{equation}
\underset{\pi} \sup \mathbb E\left[V^{\pi}(t+1,X_{t+1})|X_t=x; \pi\right] = E\left[\underset{\pi} \sup V^{\pi}(t+1,X_{t+1})|X_t=x; \pi\right]
\end{equation} which is proven by showing both inequalities hold. So: \begin{equation}
\underset{\pi} \sup \mathbb E\left[V^{\pi}(t+1,X_{t+1})|X_t=x; \pi\right]\leqslant E\left[\underset{\pi} \sup V^{\pi}(t+1,X_{t+1})|X_t=x; \pi\right]
\end{equation} which I can see follows from $\underset{y\in \mathcal Y} \sup \mathbb E\left[f(X,y)\right]\leqslant \mathbb E\left[\underset{y\in \mathcal Y} \sup f(X,y)\right]$ Then the reverse inequality is proven, where $\pi^* =   \underset{\pi} {\text{argmax}}\left[\underset{\pi}\sup V^{\pi}(t+1,X_{t+1})\right]$ \begin{equation}
 E\left[\underset{\pi} \sup V^{\pi}(t+1,X_{t+1})|X_t=x; \pi\right] =  E\left[V^{\pi^*}(t+1,X_{t+1})|X_t=x; \pi^* \right]\leqslant \underset{\pi} \sup \mathbb E\left[V^{\pi}(t+1,X_{t+1})|X_t=x; \pi\right]
\end{equation} which follows by the definition of supremum. I am wondering how come we can't just do this same procedure for $\underset{y\in \mathcal Y} \sup \mathbb E\left[f(X,y)\right]\leqslant \mathbb E\left[\underset{y\in \mathcal Y} \sup f(X,y)\right]$ , and show that equality holds there as well. The issue seems to be with the step of the reverse inequality and I also can think of some simple examples that seem to defy the equality. For example: There is just one time step, from $T-1$ to $T$ , and the probability space has just $3$ points: $\{ \omega_1,\omega_2,\omega_3\}$ , all with equal likelihood. There is a function $f(y,X_T(\omega))$ , for $y \in \{0,1\}$ , such that whenever $y = 0$ , $f(0,X_T(\omega)) = 30$ , and that when $y = 1$ , $f(1,X_T(\omega_1)) = -3000$ , $f(1,X_T(\omega_2)) = -3000$ , and $f(1,X_T(\omega_3)) = 300$ . So if we wanted to calculate $\underset{y\in \{0,1\}} \sup \mathbb E\left[f(y,X_T(\omega))\right]$ , we maximize the expectation over choices of $y$ , so the choice would be $y = 0$ , for a value of $30$ , regardless of the state of the world. And for $ \mathbb E\left[ \underset{y\in \{0,1\}} \sup f(y,X_T(\omega))\right]$ , we choose $y = 1$ when the state of the world is $\omega_3$ and $y = 0$ otherwise, and so since each possibility has equal weighting, the value of the expression is $\frac{300 + 30 + 30}{3} = 120$ , which doesn't match the $30$ from $\underset{y\in \{0,1\}} \sup \mathbb E\big[f(y,X_T(\omega))\big]$ . So if we consider the control process to be the choice of $y$ , then this poses a contradiction with the proof of the Bellman principle . One issue I see with my example is that the supremum on the outside of the expectation seems to not be able to 'look into the future' while the supremum on the inside of the expectation can. I'm not sure how to reconcile this. So I think my main questions are: Can my simple (possibly incorrect) example be reconciled with how the proof for the Bellman principle was done? What exactly $\underset{\pi} \sup V^{\pi}(X_{t+1})$ means and how it can be evaluated. Is it a random variable, where the maximum of $V^{\pi}(X_{t+1})$ is chosen across policies, depending on the state of the world? Any clarification would be appreciated. I've been trying to get this answered for a while now. I'll award the bounty and correct answer even for just a link that'll help explain things! Thanks a lot!","['stochastic-analysis', 'real-analysis', 'stochastic-processes', 'probability-theory', 'probability']"
3212443,What's the remainder when $x^{7} + x^{27} + x^{47} +x^{67} + x^{87}$ is divided by $x ^ 3 - x$,"What's the remainder when $x^{7} + x^{27} + x^{47} +x^{67} + x^{87}$ is divided by $x ^ 3 - x$ in terms of $x$ ?I tried factoring $x$ from both polynomials but I don't know what to do next since there'd be a $1$ in the second polynomial.
Any help would be appreciated!","['algebra-precalculus', 'functions', 'polynomials']"
3212469,Integrating $\int_0^{\infty} \frac{\log x}{(x + a)^2 + b^2} \operatorname d\!x$,"I'm trying to show that $\int_0^{\infty} \frac{\log x}{(x + a)^2 + b^2} \operatorname d\!x = \frac{1}{b}\arctan \sqrt{a^2 + b^2}.$ However, I am a bit confused applying the key hole ""method."" I consider $f(z) = \frac{\log^2(z)}{(z+a)^2+b^2}$ and can see that there are poles at $z_{1,2} = -a + bi, -a-bi.$ Then, I'm not sure what to do from there. I know that the integral on the outer circle vanishes as $R\to \infty$ and the integral of the inner circle also goes to zero as $\epsilon \to 0.$ I'm not sure what my integrands are for the contours $C_1$ going from $R$ to $\epsilon$ and $C_2$ going from $\epsilon$ to $R.$ I also don't know how to compute their residues.","['complex-analysis', 'contour-integration']"
3212501,"compute the limit $\lim_{n\rightarrow\infty} \int_{0}^{\frac \pi 2} \frac{\sin^2(nx)}{1+x} \,dx$",I've tried using Taylor expansion but that didn't really work out. I'm really stuck and don't know where to begin. I even tried putting it on wolfram alpha but he couldn't solve it either.,"['integration', 'limits']"
3212551,find all possible functions : $f(a)f(b)-6ab=\frac{3}{2}f(a+b)$,"I'm trying to find all possible functions that satisfy this functional equation: $f(a)f(b)-6ab=\frac{3}{2}f(a+b),$ $f\in \mathbb{R}.$ My attempt : $a=b=0$ then $f(0)=0$ or $1.$ But I don't have any ideas to complete . Please give me hints.","['functional-equations', 'calculus', 'algebra-precalculus', 'real-analysis']"
3212561,Manhattan distance problem with infinite zig zags,"If you turn left/right any finite number of times going from point to point, it will be the same as if you traveled $x$ then turned once and traveled $y$ to get there. I hear that even an infinite number of turns will not suddenly shorten the distance to $\sqrt{x^2+y^2}$ . We have a similar situation in the staircase problem in which $\pi \ne 4$ because something happens [forgive me] at infinity. We no longer have size to the xy vectors. We have a set of points that coincide with the curve that is a circle. Here, we have a set of points that are $not$ $close$ $to$ , but $on$ the line that is the hypotenuse of a right triangle. In the staircase problem, we still have an infinite number of vectors not pointing in the direction of the curve except at four points. How does the Manhattan distance differ from $\pi \ne4$ in the staircase problem above where we know the answer $\pi$ as a given? I never learn why the staircase problem ends up as $3.14...$ . Could we need new theorems to explain both of these? Or will they both forever be nothing more than paradoxes? Perhaps the staircase problem has no answer. Can someone verify that that it always results in $\pi=4$ ? If so, I can accept that the Manhattan distance never changes.","['arc-length', 'infinity', 'intuition', 'limits', 'plane-geometry']"
3212596,Prove that a definite integral is an infinite sum [duplicate],"This question already has answers here : How to find $ \int_0^\infty \dfrac x{1+e^x}\ dx$ (5 answers) Closed 5 years ago . I've been trying to solve this given equality involving an improper integral and an infinite sum without any substantial progress: $$\int_0^\infty \frac{x}{1+e^x}dx=\sum_{n=1}^\infty \frac{(-1)^{n+1}}{n^2}$$ I tried various integration techniques such as change of variable ( $1+e^x=t$ ; $e^x=t$ ), integration by parts and used Taylor expansion at every integral I arrived. However, I did not find any way of expressing the integrand as an infinite sums of functions that I could integrate term by term. Any suggestions?","['integration', 'sequences-and-series', 'real-analysis']"
3212682,"Function to define sequence $1,2,1,3,1,2,1,4,1,2,1,3,1,2,1,5,1,\dots$","Given the following sequence: $1,2,1,3,1,2,1,4,1,2,1,3,1,2,1,5,1,\dots$ Let $x_1$ be the first element in the sequence, $x_2$ be the second, $x_3$ be the third and so on. Let the function $f(i)$ be defined such that $f(i) = x_i$ . If $i$ is odd, then $f(i) = 1$ . If $i = 2 + 4j$ where $j$ is an integer, then $f(i) = 2$ . If $i = 4 + 8j$ where $j$ is an integer, then $f(i) = 3$ . If $i = 8 + 16j$ where $j$ is an integer, then $f(i) = 4$ . If $i = 16 + 32j$ where $j$ is an integer, then $f(i) = 5$ . Etc. How do I define $f(i)$ in a compact, easy to understand equation?","['functions', 'sequences-and-series']"
3212708,How would you explain this method of solving quadratic equations?,I stumbled across this interesting geometrical method of solving quadratic equations. Can someone explain why are intersection points roots of equation? Why does circle have anything to do with quadratic equations? What is this method called?,"['quadratics', 'geometry']"
3212717,Mean squared error for vectors,"I know that when we compare estimators $\hat{b_1}$ and $\hat{b_2}$ to an unknown parameter $\beta$ , in classical statistics an estimator $\hat{b_1}$ is said to be ""better"" than $\hat{b_2}$ if: $$ MSE(\hat{b_1}) \leq MSE(\hat{b_2}) $$ where MSE is the mean squared error: $$ MSE(\hat{b_1}) = E((\hat{b_1}-\beta)^2 )$$ Now if I had a vector $ \boldsymbol{b} =(b_1,b_2,\ldots b_n)$ of parameters to estimate, how could I compare estimators in terms of the MSE? Because there is no unique ordering relation in vectors. I know some people compare component by component of both estimators, yet I seem to find no bibliography for that. Could you guys help me figure out a bibliography for that?","['mean-square-error', 'statistics', 'parameter-estimation', 'estimation']"
3212724,Integral of a generalized function,"Let $\mathcal{S}$ be a Schwartz space and $\delta_{a}$ the following distribution: $$\delta_{a}: \phi \rightarrow \phi(a) \ \ \ \ \text{ for each } \phi\in\mathcal{S}$$ Now, we routinely see something like: $$\int_{-\infty}^{\infty} f(x)\delta_a dx = f(a)$$ where $f$ is not necessarily $\in \mathcal{S}$ (e.g. $f \in L^2$ ). I'm having a hard time interpreting this integral using the language of distributions. For example, if $f$ was in $\mathcal{S}$ , I could say the integral is just giving me the value of the functional $\delta_a$ at a point in $\mathcal{S}$ . But what about when $f\notin \mathcal{S}$ ?","['dirac-delta', 'functional-analysis', 'distribution-theory']"
3212764,Properties of function $f(A) = \text{tr}(A^{2})$ on set of real matrices,"There is function $f$ on set $M$ , such that $$ f(A) = \text{tr}(A^{2}), \space M = \{A \in \text{Mat}_{2\times2}(\mathbb{R}) \space | \space A^{T} = A, \space \text{det} A \neq 0\} $$ Is it bounded below or above? If so, does it reach its min/max? My ideas are the following: Let $ A =  \left(
\begin{array}{ccc}
 a & c \\
 c & d \\
\end{array}
\right) $ . Since $\text{det} A \neq 0 $ , we have $ad-c^{2} \neq 0 $ . Our function is $f(A) = \text{tr}(A^{2}) = a^2 + 2c^{2} + d^{2} $ . It is bounded from below by zero, but can't reach its min because $a, c, d$ can't all equal zero at the same time since $ad-c^{2} \neq 0 $ . It is not bounded from above because we can let $a = d = 0$ and any $ c \in \mathbb{R}\setminus\{0\}$ . Is this a correct solution and does it fully answer to given questions?","['matrices', 'calculus', 'linear-algebra']"
3212784,Multi-variable Chain Rule in Quasi-linear PDEs,"In trying to understand the general solution of a first-order quasilinear PDE of the form $$a(x,y,z) \frac{\partial z}{\partial x}+b(x,y,z) \frac{\partial z}{\partial y}=c(x,y,z) $$ What approach is taken to differentiate $F(u,v)$ ? When I try to apply the chain rule I am finding $$\frac{dF}{dx} = \frac{\partial F}{\partial u}\frac{\partial u}{\partial x}+\frac{\partial F}{\partial v}\frac{\partial v}{\partial x}+\frac{\partial F}{\partial z}\frac{\partial z}{\partial x}$$ which is clearly not in agreement with the book derivation. Also, shouldn't the term $\frac{\partial z}{\partial x} = 0$ as z is not dependent on either x nor y?","['partial-differential-equations', 'derivatives', 'differential-geometry']"
3212788,Trigonometry substitution issue with sign,"When solving an integral such as $\displaystyle\int\frac{dx}{\sqrt{x^2+4}}$ , you eventually end up with $$ \ln\lvert\sec\theta+\tan\theta\rvert+C.$$ The next step is to rewrite this in terms of $x$ . My book does the following: $x=2\tan\theta$ , so by drawing a triangle, it can be seen that $x$ is the opposite side, $2$ the adjacent one, and finally, $\sqrt{2^2+x^2}$ the hypotenuse. Therefore, $$\sec\theta=\frac{\sqrt{2^2+x^2}}2.$$ The problem that I see however is that $\sqrt{2^2+x^2}$ is the magnitude of the hypotenuse, so this has lost a possible negative sign since it is not necessarily true that $$ \sec\theta = \frac{|r|}{2}.$$ How can this be omitted?","['integration', 'calculus', 'trigonometry', 'trigonometric-integrals']"
3212804,Does $\int\limits_\mathbb{R} f_n\ dm\to \int\limits_\mathbb{R} f\ dm$?,"Let $f_n$ be a sequence of measurable functions on $\mathbb{R}$ converging a.e. to $f$ . If $0\leq f_n\leq f$ a.e. Does it follow that $\displaystyle\int_\mathbb{R} f_n\ dm\to\displaystyle\int_\mathbb{R} f\ dm$ ? I think this is false but I can't think of any counterexample. Also, if I put another condition that $f_n$ is a sequence of integrable functions, does this imply that $f$ would also be integrable? Hence, the conclusion will hold by the Dominated Convergence Theorem? Thanks for any response.","['measure-theory', 'lebesgue-integral', 'real-analysis']"
3212845,isomorphic subgraph with tree $T$,"Let $T$ be a tree with $k$ edges. Proof that each graph $G$ , where $$ \forall_v \text{   } deg(v) \ge k $$ contains isomorphic subgraph with tree $T$ My idea If $ \forall_v \text{   } deg(v) \ge k $ then in $G$ is a path $P$ with $k$ vertices so we can choose random path from tree and it will be equivalent to our $P$ . If each vertex contains $k$ neighbors so next elements from graph can be connected to $P$ .","['graph-theory', 'discrete-mathematics']"
3212864,"If all diagonals are drawn in a regular polygon from a vertex, the angles formed in that vertex are equal","How can I prove this statement? I tried with a pentagon, but I have not achieved anything Restriction : I can not use the circumference to prove it, and by this I mean to inscribe the polygon in the circumference. The idea is prove it with properties of congruence of triangles or properties of parallelogram or properties of quadrilateral, trapezium or trapezoid. If all the possible diagonals are drawn in a regular polygon from a
  vertex, the angles formed in that vertex are equal each.","['proof-explanation', 'geometry']"
3212925,Integrals of the Bessel function $J_0(x)$ over the intervals between its zeros,"Let $J_0(x)$ be the Bessel function of the first kind . It has an infinite number of zeros on the positive real semi-axis. Let's denote them as $j_{0,n}$ : $$j_{0,1}=2.40482...,\quad j_{0,2}=5.52007...,\quad j_{0,3}=8.65372...,\quad\small...\tag1$$ We are interested in absolute values of the integrals of $J_0(x)$ over the intervals between its consecutive zeros: $$\sigma_n=(-1)^n\int_{j_{0,n}}^{j_{0,n+1}}\!\!J_0(x)\,dx.\tag2$$ Their values are: $$\sigma_1=0.80145...,\quad\sigma_2=0.59932...,\quad\sigma_3=0.49904...,\quad\small...\tag3$$ We are interested in the asymptotic behavior of this sequence. Empirically, it seems that $$\sigma_n\,\stackrel{\color{#a0a0a0}?}\sim\,\frac{2\sqrt2}{\pi\sqrt n}\left(1-\frac1{8n}+O\!\left(\frac1{n^2}\right)\right)\!.\tag4$$ Can we prove this? Can we find next coefficients in this expansion?","['integration', 'conjectures', 'asymptotics', 'real-analysis', 'bessel-functions']"
3212942,Smallest square to cover a right triangle,"I believe smallest possible triangle to cover a square is well studied... But how about square covering a triangle? I read the post here , but I think that I don't want such a general question (and complicated solution), just right triangle. I first encounter the problem, which is, find the square with minimum length that covers a triangle with sides $5,12,13$ completely. The answer is a square with length $\frac{144}{\sqrt{193}}$ and I believe so, using the method here . But the problem is, why is it the smallest possible square covering the triangle? My attempts: If a square covers the triangle and at the same time they have some vertex(s) coinciding: If it is $\angle BAC$ touching vertex of square, see here , the square covering it should have side length of at least 12. If it is $\angle BCA$ touching the vertex of square, see here , the square covering it should also have side length of at least 12. This leaves us to the problem of $\angle ABC$ , see here or below: It is the smallest because whatever it is rotated clockwise/anti-clockwise through the origin, the side length of square will be larger. For every square covering a triangle, there is a smaller square covering the same triangle while touching at least a vertex using its vertex(s). But how does it implies that the side length of square is at least $\frac{144}{\sqrt{193}}$ ? I mean, a square which touches the triangle with its vertex may have a shorter side length, but why is there no square with side length even shorter (doesn't touch the triangle using its vertex)? Why do we need only to consider these three cases? Any help will be appreciate! Thank you.","['euclidean-geometry', 'algebra-precalculus', 'trigonometry']"
3212953,Proving that a function satisfies the wave equation (chain rule and iterated partial derivatives),"I'm really anxious about getting this exercise right: Given  the function $\phi = f(x-t) + g(x+t)$ , with $f,g$ one variable functions in $C^2$ , prove that $\phi$ satisfies the wave equation $\frac{\partial^2\phi}{\partial t^2} = \frac{\partial^2\phi}{\partial x^2}$ I started by defining $a, b: \mathbb{R^2} \to \mathbb{R}$ , $a(x,t)=x-t, b(x,t)=x+t$ . Then we have $$\frac{\partial^2\phi}{\partial t^2} =
 \frac{d}{dt} \bigg(\frac{\partial \phi}{\partial t}\bigg) = \frac{d}{dt} \bigg(\frac{df}{da} \cdot \frac{\partial a}{\partial t} + \frac{dg}{db} \cdot \frac{\partial b}{\partial t}\bigg)$$ $$ = \frac{d}{dt} \bigg(\frac{df}{da}\bigg) \cdot \frac{\partial a}{\partial t} + \frac{df}{dt} \cdot \frac{\partial^2 a}{\partial t^2} + \frac{d}{dt} \bigg( \frac{dg}{db} \bigg) \cdot \frac{\partial b}{\partial t} + \frac{dg}{db} \cdot \frac{\partial^2 b}{\partial t^2}$$ But we know that $\frac{\partial a}{\partial t} = -1, \frac{\partial b}{\partial t}=1$ and $\frac{\partial^2 a}{\partial t^2} = \frac{\partial^2 b}{\partial t^2} = 0$ . Therefore $$= \frac{d}{dt} \bigg( \frac{dg}{db} \bigg) - \frac{d}{dt} \bigg(\frac{df}{da}\bigg) = \frac{d^2g}{db^2} + \frac{d^2f}{da^2}$$ The only difference when we differentiate w.r.t. $x$ is that $\frac{\partial a}{\partial x} = 1$ . And this doesn't make a difference because the negative sign of $\frac{\partial a}{\partial t}$ was cancelled out. Hence $\frac{\partial^2\phi}{\partial t^2} = \frac{\partial^2\phi}{\partial x^2}$ .","['partial-derivative', 'multivariable-calculus', 'proof-verification', 'chain-rule']"
3212961,"With the condition $\lim_{x\to\infty}(f(x+a)−f(x))=0$, how to prove that $f(x)$ is uniformly continuous?","Assume $f\in C[0,+\infty)$ , and for all $a>0$ , we have $$\lim_{x\to\infty}(f(x+a)−f(x))=0.$$ Prove that $f(x)$ is uniformly continuous. One hint is that we can use Baire category theorem, but I still don't know how to use it. Maybe there is another way to answer this question, I'm not sure. Looking forward to your answer.","['calculus', 'general-topology', 'baire-category', 'real-analysis']"
3212984,Definition of Negative Half Derivative,"I know the definition of positive fractional derivative, which is given by $$D^{\alpha} f(x)=\frac{1}{\Gamma(1-\alpha)} \frac{d}{d x} \int_{0}^{x} \frac{f(t)}{(x-t)^{\alpha}} dt,\quad\quad \alpha\in(0,1)$$ But I encounter the negative half derivative $D^{-1/2}$ , which I am quite confused with. My professor assigned a problem which let us solve the integral equation $$ D^{-1 / 2} h(t)=\int_{0}^{t} \frac{1}{\sqrt{\pi(t-s)}} h(s) \mathrm{d} s=g(t),$$ where $g$ is a nice function with $g(0)=0.$ And he gave a hint that lets us square $D^{-1/2}.$ I feel like the first equality is the definition of $D^{-1/2}$ but I am not sure. If it indeed is the definition of $D^{-1/2}$ , can someone tell me the definition of $D^{-\beta}$ for any $\beta \gt0\,?$ Furthermore, can someone give me a further hint about the problem he assigned? My attempt: Squaring $D^{-1/2}$ , we get $$h^{-1}(t)=\int_0^t h(s)\mathrm{d}s=D^{-1/2}g(t)=\int_{0}^{t} \frac{1}{\sqrt{\pi(t-s)}} g(s) \mathrm{d} s\,.$$ Differentiating both sides w.r.t $t$ and applying the Leibniz Integral Rule( but ${1\over \sqrt{t-t}}g(t) =\infty$ , and it is invalided to use the Leibniz Integral Rule), we get $$h(t)=\frac{-1}{2\sqrt{\pi}}\int_0^t\frac{1}{(t-s)^{\frac{3}{2}}}g(s)\mathrm{d}s$$ I think it may not be simplified further, but I am not sure. Could the integral in the right-hand-side be calculated further? Any help will be appreciated.","['derivatives', 'integral-equations', 'fractional-calculus']"
3212988,Use GAP to define finite Clifford group,"I am studying the computer algebra system GAP to do some  calculations about Clifford group, which is defined (cf. Lawson and Michelsohn, Spin Geometry , Princeton 1989) as followings Definition . Let's denote this group by $F_n$ , in accordance with the number of generators $e_1, e_2, \ldots, e_n$ . The group can be given $F_n = \, <\;e_1, e_2, \ldots, e_n \,:\; e_j^2 = -1, e_j e_k = - e_k e_j \quad \text{for } k \ne j; 1\leq j, k \leq n \;>.$ Thus we can calculate the order of this group, it is equal to $2^{n+1}$ . Given a number $n$ , for example, $n=3 \text{ or }4$ , I want to identify these groups with the groups listed in the Small Group Library with precise ID number. I want to know how to define this group $F_n$ in GAP, I tried like followings but failed Define $F_3$ My questions are How to modify the codes in my definition to get a correct one? I think the main problem is $e_j^2=-1$ . Once we can define this group, how do we identify the group with the group in Small Group Library? Since GAP can handle group whose order is less than about 2000. For the Clifford group case, this means I can work up to $n=8$ when the group order is $2^9=512$ . The Small Group Library does not have group of order $1024$ , and group of order $2^{11}$ and beyond are also not handles. How to deal with those $n \geq 9$ situations? Many thanks.","['gap', 'group-theory']"
3212990,How do you compare data values that come from different data sets using standard deviation?,"My textbook had this question: ""Two swimmers, Angie and Beth, from different teams, wanted to find out who had the fastest time for the 50 meter freestyle when compared to her team. Which swimmer had the fastest time when compared to her team?"" \begin{array}{|c|c|c|c|}
\hline
Swimmer& Time (seconds) & Team Mean Time & Team Standard Deviation \\ \hline
 Angie& 26.2& 27.2& 0.8\\ \hline
 Beth&  27.3& 30.1& 1.4\\ \hline
\end{array} Now my textbook's answer was this: ""From the above scores, it is clear that Angie's time of -1.25 is smaller than Beth's time of -2. And for swimming, lower time is better. Therefore, Angie has a better swimming time as compared to her team."" What I did: I calculate the number of standard deviations for both of them using the formula: z = $\frac{x - μ}{ σ}$ . I then got 
Angie's z-score to be -1.25 and Beth's z-score to be -2. Now since Beth has a more negative z-score, doesn't that mean that she's deviating from her team's mean time, therefore having a faster time when compared to her team? I don't get why the textbook's answer is saying that Angela had the fastest time when compared to her team or did I do something wrong?",['statistics']
3213006,Solve $2x^2-5x+2=$ $\frac{5-\sqrt{9+8x}}{4}$,Solve $2x^2-5x+2$ = $\frac{5-\sqrt{9+8x}}{4}$ I simply do square both sides solve it and I get two value of x one is 2 and other is $\frac{3-√5}{2}$ but this approach it take more time so is there any approach for solving this equation.,"['functions', 'quadratics', 'inverse-function']"
3213047,How can I compute this integral in closed form : $\int_0^{\frac{π}{4}}\ln^2(\tan x)dx$,How can I compute this integral in closed form : $$\displaystyle\int_{0}^{\displaystyle \tfrac{π}{4}}\ln^{2}\left(\tan x\right)dx$$ How can use Fourier series here ? $$-2\displaystyle \sum_{n=0}^{\infty}\frac{\cos((4n+2)x)}{2n+1}$$ $$=\ln\left(\tan x\right)$$ And what's about if $\displaystyle\int_0^{\displaystyle \tfrac{π}{4}}\ln^2(\cot x)dx$ Please give me ideas or hints,"['integration', 'definite-integrals', 'trigonometric-integrals', 'polylogarithm', 'closed-form']"
3213073,Finding an Explicit Formula for a Geometric Series,"CONTEXT: Uni question made up by lecturer If you have to take a $100$ mg drug every $8$ hours, and just before you take the drug, $20$ % of it remains in your body, how would you write an explicit formula for this? I've worked out that the recursive formula would be for integers $n\ge0$ where $Q_0=100$ , I have $Q_{n+1}=0.2Q_n+100$ where $Q_n$ is the quantity of the drug in the body just after the $n$ th dose is taken. My lecturer gave us a hint saying the solution involves geometric series, but I'm not sure how this can be the case since the recursive definition I've developed involves an added constant of $100$ . The explicit definition is then to be used to find the quantity of the drug remaining in the body in the long run (which would just be what the series converges to).","['calculus', 'taylor-expansion', 'sequences-and-series', 'convergence-divergence', 'geometric-series']"
3213085,A coin is tossed $5$ times. How many possible outcomes contain AT MOST $3$ heads,"There are $32$ possible outcomes in total when a coin is tossed $5$ times. I have found that there are 10 possible outcomes that contain exactly $ 3$ heads by using $5C3=5!/3!2!$ , 
but how do I find out how many possibilities contain at most $3$ heads?
Thanks!","['combinations', 'combinatorics', 'discrete-mathematics']"
3213115,Existence of Solutions to Elliptic Equations on Compact Manifolds -- Global Obstructions?,"Let $(M,g)$ be a smooth, closed Riemannian manifold and let $X$ and $c$ respectively be a smooth vector field and a smooth function on $M$ . Do there exist general criteria for determining for which smooth functions $f$ there exists a smooth solution to the elliptic PDE $$\Delta u +X(u) +cu =f?$$ I'm familiar with the local theory which guarantees for us the existence of local solutions in any coordinate patch but there seem to be global obstructions once we move to the manifold setting. For example, in the instance that $X$ and $c$ are both identically $0$ , there is a clear global obstruction to the problem, namely whether or not the integral of $f$ over $M$ vanishes (one can show, using the heat flow [or sheaf cohomology in the instance that $M$ is a Kähler] that, in this instance, this is in fact the only obstruction). I believe that in the case that either $c \geq 0$ or $c \leq 0$ the local theory combined with sheaf cohomology tells us that the answer is essentially the same as for the Laplacian, but if we don't have control over $c$ , then what happens? Any and all insights are welcomed!","['elliptic-equations', 'linear-pde', 'differential-geometry']"
3213117,Nilpotency class of Frattini subgroup and group order,"Suppose $\psi(n)$ denotes the minimal natural number $k$ , such that there exists a finite group $G$ , such that $k = \max \{m \in \mathbb{N}| \exists \text{ prime } p, p^m | |G| \}$ , and $\Phi(G)$ has nilpotency class exactly $n$ . Here $\Phi$ stands for Frattini subgroup. Is there some sort of a closed formula for $\psi(n)$ ? What I currently know: $$\psi(n) \geq n + 3$$ for $n \geq 2$ Proof: Using the method from the answer to “ If $|G|=p^3q^2$ then $\Phi(G)$ is cyclic for primes $p\neq q$. “, the statement becomes reduced to: If $G$ is a finite group, such, that $\Phi(G)$ is a $p$ -group of exact nilpotency class $n \geq 2$ , then $p^{n+3}| |G|$ . Here $p$ stands for an arbitrary prime number. Now, suppose, $p^{n+3}$ does not divide $|G|$ . Then, $|\Phi(G)| | p^{n + 1}$ . That means, that $|\Phi(G)| = p^{n + 1}$ , because all groups of order $p^m$ , with $m \leq n$ , have exact nilpotency class strictly less than $n$ . Thus $\Phi(G)$ is a maximal class group. Thus it contains a non-abelian characteristic subgroup of order $p^3$ (which is the second element of its upper central series). And there we receive the contradiction with Lemma 1 from “The nilpotence class of the Frattini subgroup” by W.M. Hill and D.B. Parker , which states: A non-abelian group of order $p^3$ can non occur as a normal subgroup contained in the Frattini subgroup of any finite group. EDIT: Actually that article proves an even more stronger result $$\psi(n) \geq 2n + 1$$ for $n \geq 2$ (which I did not know before, because I found the full text of the article only today) However, the question, whether this bound is sharp, or can be bettered, still remains.","['finite-groups', 'nilpotent-groups', 'abstract-algebra', 'group-theory', 'frattini-subgroup']"
3213124,Proof that if convex polyhedron doesn't contain triangles and quadrangles then $3m \le 5n - 10$,"Proof that if convex polyhedron doesn't contain triangles and quadrangles then $3m \le 5n - 10$ where $m$ is number of edges and $n$ number of vertices I don't know how to start this task but I suspect that there is something similar with planar graphs. If planar graph doesn't contain cycle with length $<r$ then we have $$ rf \le 2m \\
rf = 2r - nr  + mr \\
(r-2)m \le r(n-2) $$ so in our case for $r=5$ it will be $$ 3m \le 5n - 10 $$ But I don't know if each convex polyhedron is planar graph. Is that true? I did find anything about that in my lecture.","['graph-theory', 'discrete-mathematics', 'planar-graphs']"
3213128,Iwasawa's Theorem for Simple Groups,"I'm trying to work through a proof of Iwasawa's Theorem: Let $G$ act faithfully and primitively on a set $\Omega$ . Let $\alpha\in\Omega$ and denote by $H$ the point stabiliser $G_{\alpha}$ . Suppose further that $H$ has an abelian normal subgroup $A$ such that $G=<A^g\mid g\in G >$ ; G=[G,G], the derived subgroup of $G$ . Then $G$ is simple. The proof I'm working through is as follows: Suppose that $K$ is a non-trivial normal subgroup of $G$ , $K\lhd G$ . Then (as $G$ acts primitively) $K$ acts transitively on $\Omega$ , and thus $G=KH$ . But $AK$ is normalised by both $K$ and $H$ , so $AK\lhd G$ . But $G$ is the smallest normal subgroup of $G$ , i.e. $AK=G$ . Thus \begin{equation}
G/K \cong (AK)/K\cong A/(A\cap K),
\end{equation} so that $G/K$ is Abelian. Then $G=[G,G]\leq K$ , i.e. $G=K$ , whence $G$ is simple. $\qquad\square$ My first question is how to verify the (apparently trivial) claim that both $H$ and $K$ normalise $AK$ . For the first, I understand that \begin{equation}
(AK)^h = h^{-1}AKh=h^{-1}Ahh^{-1}Kh=A^h K^{h^{-1}}=AK,\ \forall h\in H
\end{equation} since $A\lhd H$ and $K\lhd G$ . For the second: \begin{equation}
(AK)^k = k^{-1}AKk=k^{-1}Ak^{-1}Kh=A^k K,\ \forall k\in K.
\end{equation} Do we know that $A\lhd K$ ? Why does $A^k=A$ ? My second question : does the fact that $G$ is the smallest normal subgroup of $G$ follow from the fact it is generated from the ` $A^g$ 's? If so, how exactly? If not, where does this condition come in? My final question is: how we deduce that $G/K$ is Abelian from this string of isomorphisms? I know basic facts about normal subgroups and derived subgroups, but this goes over my head. This course, in general, kills me. Any help with any of these three questions is greatly appreciated.",['group-theory']
3213142,Root of unity filter,"Can some one help me understand the technique called ""Root of unity filter"" . I just know how to use it. It's as follow: For series $f(x)=a_0+a_1x+a_2x^2+\cdots+a_nx^n$ we need to find the sum of coefficient of terms in which the power is a multiple of any number say $k$ for finding the same we have $\omega $ as $\mathrm{k^{th}}$ of unity and write $$ \dfrac{f(1)+f(\omega)+f(\omega ^2)+ \cdots+ f(\omega^{k-1})}{k}=(a_0 + a_k + a_{2k}+\cdots)$$ please help me understand why and how this works , I tried googling but didn't get any satisfactory answer","['summation', 'combinatorics', 'roots-of-unity', 'binomial-theorem', 'complex-numbers']"
3213146,"is $(x+y,x-y)$ a prime ideal in $\mathbb{Z}[X,Y] $","Is $ I=(x+y,x-y)$ a prime ideal in $\mathbb{Z}[x,y]$ Since $x+y, x-y \in \mathbb{Z}$ , therefore $(2x,2y) \in (x+y, x-y)$ , and this implies that in $R=\frac{\mathbb{Z}[x,y]}{(x-y,x+y)}$ , we have $2x=0$ . But neither $2$ nor $x$ is zero in $R$ . This shows that R is not an integral domain, which implies that $I$ is not a prime ideal.","['ring-theory', 'abstract-algebra']"
3213187,The sine cardinal function and $F_1 = F_2 = F_3 = F_4 = F_5 = F_6 = 0$,"Define the function, $$F_n=\frac12-\int_0^\infty \frac{\sin^n x}{x^n}\,dx+\sum_{x=1}^\infty \frac{\sin^n x}{x^n}\tag1$$ where $\rm{sinc}^n(x)=\frac{\sin^n x}{x^n}$ is the sine cardinal function . We have $$F_1 = F_2 = F_3 = F_4 = F_5 = F_6 = 0$$ Then suddenly, $$\begin{align*}
F_7 &= \frac{1}{46080}\Bigl(117649\pi-201684\pi^2+144060\pi^3\\
&\qquad\qquad\qquad-54880\pi^4+11760\pi^5-1344\pi^6+64\pi^7\Bigr)\tag2
\end{align*}$$ Fortunately, this and the next can be simplified as, $$F_7 = \frac{\pi\big(\tfrac72-\pi\big)^6}{6!}$$ $$F_8 = \frac{\pi\big(\tfrac82-\pi\big)^7}{7!}$$ Courtesy of an insight from robjohn's answer , it turns out the rest have beautifully consistent forms, $$F_9 = \frac{\pi\big(\tfrac92-\pi\big)^8}{8!}-\frac{9\pi\big(\tfrac72-\pi\big)^8}{8!}$$ $$F_{10} = \frac{\pi\big(\tfrac{10}2-\pi\big)^9}{9!}-\frac{10\pi\big(\tfrac82-\pi\big)^9}{9!}$$ and, $$F_{11} = \frac{\pi\big(\tfrac{11}2-\pi\big)^{10}}{10!} -\frac{11\pi\big(\tfrac92-\pi\big)^{10}}{10!} + \frac{11\pi\big(\tfrac72-\pi\big)^{10}}{2\times9!}$$ $$F_{12} = \frac{\pi\big(\tfrac{12}2-\pi\big)^{11}}{11!} -\frac{12\pi\big(\tfrac{10}2-\pi\big)^{11}}{11!} + \frac{12\pi\big(\tfrac82-\pi\big)^{11}}{2\times10!}$$ and so on. Q: Are there other functions $G_n$ similar to $(1)$ such that $G_n = 0$ for the first few $n$ , then for higher $n$ is suddenly a polynomial in $\pi$ (or some well-known constant)? P.S. The reason I ask is the polynomials of $G_n$ might have their own consistent forms similar to the one given by robjohn. I faintly remember a family, but can't explicitly recall it for now.","['pi', 'definite-integrals', 'closed-form', 'sequences-and-series']"
3213206,What is a logarithm in the light of group theory?,Logarithms connect the operation of addition and the operation of multiplication. How does group theory sheds light to this property of logarithms?,"['group-theory', 'logarithms']"
3213216,A question on discrete Fourier transform of some function,"Let $\sigma(n) = \sum_{d|n} d$ and $\tau(n) = $ number of divisors of $n$ . For each $k, 0 \le k \le n-1$ we can look at the discrete Fourier transform of the numbers $\sigma(\gcd(n,k))$ given by: $$\hat{\sigma}(k) = \sum_{l=0}^{n-1} \sigma(\gcd(n,l))\exp\left(\frac{-2 \pi i k l }{n}\right)$$ I implemented this in SAGEMATH, and it occured me that : Conjecture : $$\hat{\sigma}(k) = n \tau(\gcd(n,k))$$ Is there any way to prove this? Here is some sage-code which implements this: def tau(n):
    return len(divisors(n))

def FTS(n,k):
    # Fourier Tranformierte von sigma(gcd(n,k))
    return sum([ sigma(gcd(n,l))*exp(-2*pi*I/n*k*l ) for l in range(n)])

N = 8
for k in range(N):
    print k,abs(FTS(N,k).N()),N*tau(gcd(N,k)) which gives: 0 32.0000000000000 32
1 8.00000000000000 8
2 16.0000000000000 16
3 8.00000000000000 8
4 24.0000000000000 24
5 8.00000000000000 8
6 16.0000000000000 16
7 8.00000000000000 8 Edit :
From the conjecture above it follows, that, by setting $k=0$ : $$\tau(n) = \frac{1}{n}\sum_{l=0}^{n-1} \sigma(\gcd(n,l))$$ for which I have a proof.
By taking the inverse Fourier transform, and setting again $k=0$ we get: $$\sigma(n) = \sum_{l=0}^{n-1} \tau(\gcd(n,l))$$","['number-theory', 'conjectures', 'fourier-transform']"
3213253,Largest circle in basin of attraction of the origin.,"We're given the following dynamical system: $$ \begin{aligned} \dot x &= -x + y + x (x^2 + y^2)\\ \dot y &= -y -2x + y (x^2 + y^2) \end{aligned} $$ What's the largest constant $r_0$ such that the circle $x^2 + y^2 < r_0^2$ lies in the origin's basin of attraction? So far, with relatively easy algebra, I've got: $$ \begin{aligned} \dot r &= \frac{r}{2}(-2-\sin(2 \phi)+2r^2) \\ \dot \phi &= -(1+\cos^2(\phi)) \end{aligned} $$ Which immediately shows $r_0 \geq \sqrt{\frac12}$ . How to show that there is no better bound?","['basins-of-attraction', 'polar-coordinates', 'ordinary-differential-equations', 'dynamical-systems']"
3213258,"The number of positive integers of 5 digits such that each digit is 1, 2 or 3, and all three of the digits appear atleast once.","The number of positive integers of 5 digits such that each digit is 1, 2 or 3, and all three of the digits appear at least once. Working:
Three 1, one 2, one 3, number of numbers = 5!/3! = 20 Two 1, one 2, two 3, number of numbers = 5!/(2!*2!) = 30 Two 1, two 2, one 3, number of such numbers = 5!/(2!*2!) = 30 One 1, one 2, three 3, number of such numbers = 5!/3! = 20 One 1, two 2, two 3, number of such numbers = 5!/(2!*2!) = 30 One 1, three 2, one 3, number of such numbers = 5!/(3!) = 20 Therefore, total number of such numbers = 20 + 30 + 30 + 20 + 30 + 20 = 150 Question: Is there any other possible easier, and simpler approach?","['permutations', 'combinations', 'combinatorics']"
3213324,Simplifying $\sec^2 \frac{2\pi}{7} + \sec^2 \frac{4\pi}{7} + \sec^2 \frac{8\pi}{7}$,"Simplify the following expression: $$y =\sec^2 \frac{2\pi}{7} + \sec^2 \frac{4\pi}{7} + \sec^2 \frac{8\pi}{7}$$ Note. The source asks the value of $y/3$ , which, according to the instructions, has to be an integer from $0$ to $9$ . My Attempt: Man! I tried everything I could. Converted it into sin's and cosine's, tan's and sec's. Applied every identity I could find in my book. But to no avail. All I was aiming is to somehow make the squares disappear and converting everything in sin's and cosine's since we only know to simplify such expressions
like ( $\cos x + \cos 2x + \cos 3x+\cdots$ and $\cos x\cos 2x\cos 4x\cdots$ etc) in tems of sin's and cosine's. (Sorry for all the sin's and cosine's being repeated too many times :) ) Any help would be appreciated.","['trigonometric-series', 'trigonometry']"
3213326,Convert $\sin\left(2\cos^{-1}\left(\cot\left(2 \tan^{-1}x\right)\right)\right)$ into an algebraic function,"Convert trigonometric function into algebraic function. $$\sin\left(2\cos^{-1}\left(\cot\left(2 \tan^{-1}x\right)\right)\right)$$ My approach is as follows: $sin2\theta$ is to be calculated $$\tan^{-1}x=\gamma \tag{1}$$ Hence $$\begin{align}
\cos\theta&=\cot2\gamma \tag{2}\\[4pt]
\cos\theta&=\frac{\cot^2\gamma-1}{2\cot\gamma} \tag{3}\\[4pt]
\cos\theta&=\frac{1-x^2}{2x} \tag{4}
\end{align}$$ The value of $\sin\theta$ is coming in negative.",['trigonometry']
3213357,Connected Graph With Minimum Degree,"$G$ is a connected graph with $100$ vertices, where vertices have minimum degree $10$ . Show G has a path with $21$ vertices. I know that for a graph with minimum degree $n$ , there has to be a path of length of $n-1$ . But with a connected graph of $n$ vertices, all I can think of is that it has to have at least $n-1$ edges (since tree is the minimal connected graph). Is there a connection between minimum edges and degree and path length?","['graph-theory', 'path-connected', 'discrete-mathematics']"
3213397,Counting the Number of Real Roots of A Polynomial,"I am interested in solving problems which involve finding the number of real roots of any polynomial. Suppose I take a function $$f(x)=x^6+x^5+x^4+x^3+x^2+x+1$$ This does not have any real roots but I am trying to figure out if there is some analytical way that does not involve graphing to come to this conclusion. Using Descartes' Rule of Signs ,
there are zero sign changes in $f$ so by virtue of which there are no positive roots to the polynomial.
Considering $$f(-x) = x^6-x^5+x^4-x^3+x^2-x+1$$ I concluded that there are either 6 negative, 4 negative, 2 negative or zero negative roots.
So I have 4 cases to consider : 0 positive roots, 6 negative roots, 0 complex roots 0 positive roots, 4 negative roots, 2 complex roots 0 positive roots, 2 negative roots, 4 complex roots 0 positive roots, 0 negative roots, 6 complex roots (The correct case) I tried differentiating $f$ but the derivative is equally bad $$f'(x) = 6x^5+5x^4+4x^3+3x^2+2x+1$$ I am unable to conclude anything from this. I tried going about the problem the other way. If a polynomial with an even degree is always positive or negative depending on the leading coefficient, it will not have any real roots but then again, finding the extrema of the function is proving to be extremely difficult. I have tried using Bolzano's Intermediate Value Theorem . It guarantees the existence of at least one root but then again, there is a possibility that there might be more than one which can only be eliminated by monotonicity which again brings me back to the bad derivative. I believe there need to be some general rules by virtue of which, we are able to calculate the number of roots for any polynomial. Is graphing the best technique for polynomials like these and if it is, are there any ways by which a quick but accurate plot can be drawn? While reading about the relevant theory, I came across Sturm's Method and the Newton-Raphson Method but haven't touched these yet. Is it absolutely required to know these concepts to effectively draw conclusions? Have I missed something?","['real-analysis', 'calculus', 'polynomials', 'algebra-precalculus', 'derivatives']"
3213449,A coin flipping game,"I've been thinking about the following game for a while and am curious if anyone has any ideas of how to analyze it. Problem description Say I have two biased coins: coin 1 that shows heads with probability $p$ and coin 2 that shows heads with probability $q$ . You and I both know the statistics of the coins. The game proceeds in multiple rounds as follows: In the starting round $n=0$ : I (privately) pick a coin and flip it, we both observe the outcome you decide to make a guess of which coin I just flipped, or continue watching if you guess correctly, I pay you $\$100$ ; if you guess incorrectly, you receive no reward and the game is over At each subsequent round $n\ge 1$ : I decide to stay with my current coin or reach into my pocket and swap out the current coin for the other coin you can see whether I swapped out the coin or not (assume I must switch if I reach into my pocket) I flip the coin and we both observe the outcome you decide to guess which coin was just flipped, or keep watching if you guess correctly, I pay you $\$100\cdot\delta^n$ , where $\delta\in(0,1)$ ; if you guess incorrectly the game ends with you getting nothing Question I want to find the ""best"" switching strategy to minimize the (expected) amount of money I have to pay you. Notes The probabilities $p$ and $q$ can take on any value, but let's assume that they cannot be equal. Since you are trying to maximize your reward, the discount factor $\delta$ incentives you to guess correctly as quickly as possible. Since there are only two coins and you observe when I switch, you are trying to discern between two possible coin sequences, one where the initial coin was coin 1 and the other where the initial coin was coin 2. My first thoughts are that I would want to keep the empirical averages (of the two sequences) as close as possible to each other. Intuitively this will be easy if $p$ and $q$ are close, but hard if they are far apart.","['game-theory', 'puzzle', 'probability']"
3213450,Inner product of $k$-forms,"I'm working on the following problem from Lee's Introduction to Riemannian Manifolds : Let $(M,g)$ be a Riemannian $n$ -manifold. show that for each $k=1,\ldots, n$ , there is a unique fiber metric $\langle \cdot, \cdot \rangle_g$ on the bundle $\Lambda^k T^*M$ that satisfies $$
\left\langle \omega^1 \wedge \cdots \wedge \omega^k, \eta^1 \wedge \cdots \wedge \eta^k \right\rangle = \det \left( \left\langle \omega^i, \eta^j \right\rangle_g \right)
$$ whenever $\omega^1, \ldots, \omega^k$ , $\eta^1, \ldots, \eta^k$ are covectors at a point $p \in M$ . The problem comes with the following hint: define the inner product locally by declaring the set of $k$ -covectors \begin{equation}
\mathcal B := \left\{\varepsilon^{i_1} \wedge \cdots \wedge \varepsilon^{i_k}\big|_p : i_1 < \cdots < i_k \right\} 
\end{equation} to be an orthonormal basis for $\Lambda^k\left(T^*_pM\right)$ whenever $\left( \varepsilon^i\right)$ is a local orthonormal coframe for $T^*M$ , and proving the resulting inner product satisfies the above equation and is coframe-independent. But it's hard for me to see how $\mathcal B$ being an orthonormal basis implies the determinant formula. I know one can prove the determinant formula from the definition of wedge products $\omega \wedge \eta = \frac{(k+l)!}{k!l!} \mathrm{Alt}(\omega \otimes \eta)$ (with $\omega$ a $k$ -form and $\eta$ an $l$ -form) and by using the canonical inner product on $T^k T^*M$ given by $$
\left\langle \omega^1 \otimes \cdots \otimes \omega^k, \eta^1 \otimes \cdots \otimes \eta^k \right\rangle = \prod_{i=1}^k \langle \omega^i, \eta^i \rangle
$$ but this results in $\frac 1{k!} \det\left(\left\langle \omega^i, \eta^j\right\rangle_g\right)$ . Besides, for my own understanding of the algebra of alternating tensors, I'd like to use a strategy that involves the algebra of wedge products directly. Any suggestions? EDIT: There appears to be some connection with this inner product and the Gram determinant of (co)vectors, so maybe I can prove that analogous multilinearity properties have to hold for both determinants and this inner product?","['differential-geometry', 'tensors', 'tensor-products', 'differential-forms', 'exterior-algebra']"
3213455,Polynomials: $f_n \to p$ and $p$ has distinct real roots implies $f_n$ eventually has real roots.,"Recall that a polynomial is hyperbolic if all its roots are real. Let $p$ and $\{f_n\}_{n \geq 1}$ be polynomials of degree $d$ .  Suppose $\lim_{n \to \infty} f_n(x) = p(x)$ where the limit is taken pointwise with respect to $x$ .  Prove that if $p$ is hyperbolic and has distinct roots, then there exists an $N$ such that for all $n \geq N$ , the polynomials $f_n$ are hyperbolic. It is false if $p(x)$ does not have distinct roots, for example $f_n(x)=x^2+\frac{1}{n}.$","['limits', 'polynomials', 'analysis']"
3213463,Properties of base change of varieties,"Let $X$ be an algebraic variety over a field $K$ . Let's assume that $X$ is integral. Now let $L$ be any field extension of $K$ and let's construct the variety: $$X_L:= X\times_{\operatorname{Spec K}} \operatorname{Spec L}$$ Which one of the following statements is true? $X_L$ projective $\Rightarrow$ $X$ projective $X_L$ affine $\Rightarrow$ $X$ affine $X_L$ quasi-projective $\Rightarrow$ $X$ quasi-projective I wasn't able to find an explicit answer to those question, but it seems that 1. and 2. are true, whereas 3. is false.","['algebraic-geometry', 'schemes', 'commutative-algebra', 'projective-varieties']"
3213497,Searching for an equation (and proof of this equation) for internal path length and external path length,"I already know and proved the following statement: Let $B$ be a binary tree and we want an extended binary tree. Then the following equation holds: $$ E = I + 2 \cdot |V| $$ where $E$ is length of the external path and $I$ is the length of the internal path. I want a statement more general: So let us say that we have a tree $B_k$ ( $ k \geq 2$ ), where every node has up to $k$ childern.
We extend that tree, so every node has exact $k$ children. Now I am searching for a equation between the length of the external and internal length. Could it be : $$ E = I + k|V|$$ or what do you think? If my assumption is true can you tell me how I can prove this? My idea would be induction. For $k=2$ I have already shown this statement. but which argument could I choose for "" $k  \rightarrow k+1$ "" ? Or do you think I should use a contructive proof like I did with the statement about binary trees?","['graph-theory', 'trees', 'induction', 'discrete-mathematics']"
3213508,"Showing that $\lim\limits_{n\to\infty}\sqrt{n}\int_{-\infty }^{\infty}\frac{1}{( 1+x^{2})^n}\,dx=\sqrt{\pi}$","Define: $$ I(n) = \sqrt{n}\int_{-\infty }^{\infty} \frac{1}{( 1+x^{2})^n} \, dx$$ I have to show that: $$ \lim\limits_{n\to\infty}\sqrt{n} I(n) = \sqrt{\pi} $$ Classical solution: Based on this answer we have a closed form for the integral: $$ I(n) = \sqrt{\pi}\,\frac{\Gamma\left(n-\frac{1}{2}\right)}{\Gamma(n)}$$ Also from here we get: $$\sqrt{n}\,\frac{\Gamma\left(n-\frac{1}{2}\right)}{\Gamma(n)} \to 1 \quad \text{as} \quad n \to \infty $$ Any other methods?","['integration', 'limits', 'improper-integrals', 'real-analysis']"
3213553,"In Huffman coding, how do I choose the frequency to get the maximum average bit length?","First I want to give you a little summary about the Huffmann code to avoid misunderstandings. Summary begins So we have an alphabet $A$ with $|A| > 1 $ . For example $A$ = { $a,b,c,d,e$ }. Now we number the letters: $a=1,b=2,c=3,d=4,e=5$ . Every letter has a frequency. So we have a map $f : A \rightarrow [0,1]$ with $\sum_{a \in A} f(a) = 1$ . For example $(1|0,2), (2|0,3), (3,|0,1), (4|0,35), (5|0,05)$ . Let me explain my notation: $(1|0,2)$ means that the letter $a$ has frequency $0,2$ . So the Huffman code tells us that we take the two letters with the lowest frequency and combine them. So we get $(1|0,2), (2|0,3), (3,|0,15), (4|0,35)$ . We get : If we repeat this process we will get: So we can compute ABL (Average Bit Length ): $$ ABL(\gamma) = \sum_{a \in A} f(a) \cdot |\gamma(a)| $$ where $\gamma$ is the length of the codeword. For example $\gamma(e) = 4$ like you can see in my second picture. Any questions? Summary ends So I want to get the maximum average bit length. How do I have to choose $f$ for an alphabet $A$ with $|A| > 1$ ? I tried many things like taking the uniform distribution $f(a) = \frac{1}{n}$ for all $a \in A$ with $|A| = n$ . But this and other of my ideas didn't work. Can you help me out? Update: : thanks to the comments I've got I find out that I made a mistake about the uniform distribution. So it could be that the uniform distriution could be the solution.","['trees', 'coding-theory', 'discrete-mathematics', 'information-theory']"
3213574,$C_1$ and $C_2$ are non-crossing if they are translates of a convex disc,"Let $C$ be a convex disc in the plane, and $C_1$ and $C_2$ be two translates of $C$ . Prove that $C_1$ and $C_2$ are non-crossing, that is, it isn't possible that both $C_1 - C_2$ and $C_2 - C_1$ are non-connected. I need this result for a different problem, but my understanding of topology and connectedness is pretty weak. What would an effective solution look like? EDIT: A convex disc is a compact, convex region of the plane with a non-empty interior.","['general-topology', 'convex-analysis']"
3213649,The real positive root of $9x^5+7x^2-9=0$,"$$9x^5+7x^2-9=0$$ How do we evaluate the roots of the given polynomial? We're asked to find its real positive zero. What I tried doing: Let $$f(x)=9x^5+7x^2-9$$ Using Descartes' rule of signs, I deduced that the given polynomial can have one positive real zero at the most.
Also- $$f(-x)=-9x^5+7x^2-9$$ From this expression we can deduce that the given polynomial has two negative real zeros at the most. 
From here it is obvious that the polynomial has at least two complex roots. This is not much progress, and I'm not sure what else I could do. According to Wolfram Alpha this polynomial has one real positive zero and four complex zeros. Any hints or explanation would be appreciated, thanks! 
Also, I'm not sure whether this is an irreducible polynomial or not, so I will not be including that tag. EDIT: I found the maxima and minima points of $f(x)$ : $$f'(x)=45x^4+14x=0$$ $$x(45x^3+14)=0$$ $$x=0$$ or $$x=- \sqrt[3]{\frac {14}{45}}$$ This gives me the extremities of the curve, and I now have a rough approximation of the curve.","['irreducible-polynomials', 'algebra-precalculus', 'polynomials']"
3213682,"Let $A$ be a $101$-element subset of the set $S=\{1,2,\ldots,1000000\}$","Let $A$ be a $101$ -element subset of the set $S=\{1,2,\ldots,10^6\}$ . For each $s\in S$ let $$A_s = A+s =  \{a+s \mid a\in A\}$$ Prove that there exist $B\subset S$ such that $|B|=100$ and the sets in a family $$\{A_b \mid b\in B\}$$ are pairwise disjoint. Is a following proof corect? Let $B$ be maximal such set that sets in $\{A_b \mid b\in B\}$ are pairwise disjoint and let $|B|=k$ . Then for each $b'\in B'= S\setminus B$ exists such $b \in B$ that $A_b\cap A_{b'}\ne \emptyset$ , i.e. exists $a_1,a_2\in A$ so that $$b' = a_1-a_2+b\;\;\;\;\;\;\;\;\;(*)$$ Now make a bipartite graph with $B'$ on the left side and on right side: $$C:= \{(a_1,a_2,b)\mid a_1,a_2\in A, a_1\ne a_2, b\in B\}$$ Clearly $  |C| = 101\cdot 100\cdot k$ and $|B'| = 10^6-k$ . Connect $b'\in B'$ with $(a_1,a_2,b)\in C$ iff $b' = a_1-a_2+b$ . Then clearly each triple has degree at most $1$ and each $b'$ has because of $(*)$ degree at least $1$ . By double counting we have: $$ 10^6-k=|B'|\leq |C| = 101\cdot 100\cdot k$$ so $$k\geq {10^6\over 101\cdot 100+1}>99$$ So $k\geq 100$ and we are done. Apart from a proof verification, I'm interested in probabilistic solution of this problem.","['graph-theory', 'proof-verification', 'combinatorics', 'discrete-mathematics', 'discrete-optimization']"
3213709,$k\sum v_i v_i^T-\big(\sum v_i\big)\big(\sum v_i^T\big)\succeq 0$,"My professor claimed that $$k\sum_{i=1}^k v_i v_i^T-\Big(\sum_{i=1}^k v_i\Big)\Big(\sum_{i=1}^k v_i^T\Big)\succeq 0,$$ holds for any family of vectors $\{v_1,\dots,v_k\}$ , and can be shown using the Cauchy Schwarz inequality on the quadratic form. I'm unsure whether it's necessary assume: $k$ is a positive integer, and $v_i$ are vectors of ones and zeros such that $\sum_{i=1}^k v_i=\vec{1}$ . I don't think need to assume this due to the claim that it holds for any family of vectors. In trying to prove that the above is positive semidefinite, I get the quadratic form $$\begin{align}
k\sum_{i=1}^k x^T v_i v_i^T x-x^T \Big(\sum_{i=1}^k v_i\Big)\Big(\sum_{i=1}^k v_i^T \Big)x
&=
k\sum_{i=1}^k x^T v_i v_i^T x-|\langle \sum_{i=1}^k v_i, x\rangle|^2\\
&\geq 
k\sum_{i=1}^k x^T v_i v_i^T x-\|x\|^2 \bigg\|\sum_{i=1}^k v_i\bigg\|^2\\
&\equiv
k\sum_{i=1}^k x^T v_i v_i^T x-x^Tx n\\
&=
x^T\big(k\sum_{i=1}^k v_i v_i^T-n\mathbb{I}\big)x\\
\end{align}$$ where $n\geq k$ . I do not think this matrix in the parentheses is positive semidefinite, since its diagonals are negative. Can someone help me prove the claim of my professor?","['convex-optimization', 'linear-algebra', 'positive-semidefinite', 'quadratic-forms']"
3213712,"Solving a set of multivariate polynomials; four equations, four unknowns, maximum degree 2","I am currently trying to solve a set of four polynomial expressions for a geometric purpose. in short they take on this form: $A_0=\frac{w_0^2+w_1 w_0+w_2 w_0+w_3 w_0+27 w_0+6 w_1+6 w_2+6 w_3+117}{\left(w_0+w_1+w_2+w_3+27\right){}^2}$ $A_1=\frac{w_1^2+w_0 w_1+2 w_3 w_1+24 w_1+4 w_0+11 w_3+87}{\left(w_0+w_1+w_2+w_3+27\right){}^2}$ $A_2=\frac{w_2^2+w_0 w_2+2 w_1 w_2+24 w_2+4 w_0+11 w_1+87}{\left(w_0+w_1+w_2+w_3+27\right){}^2}$ $A_3=\frac{w_3^2+w_0 w_3+2 w_2 w_3+24 w_3+4 w_0+11 w_2+87}{\left(w_0+w_1+w_2+w_3+27\right){}^2}$ The values of $A_0,A_1,A_2,A_3$ are assumed known, and the purpose is to invert these equations to achieve $w_0,w_1,w_2,w_3$ . I initially solved for $w_0$ in terms of all knowns, but for the remaining $w$ values, the formulae that result are dependent on $w_0$ for their result. This may lead to issues down the road especially if there are no real positive candidate roots for $w_0$ . The framework I laid out mostly involves solving each of the four equations for each of the four unknowns, leading to a total of 16 equations, and substituting different ones into one another until only one unknown is left. Furthermore, the current equations so far have consistently relied on the solutions of cubic and quartic equations, which can be unwieldy and could provide incorrect or spurious results. My question is if I am taking this the wrong way and/or skipping key assumptions, and if there is a simpler approach that could be used to solve all the $w_1,w_2,w_3$ solely in terms of $A_0,A_1,A_2,A_3$ . Does this set of equations remind you of any mathematical construct in particular that has had 
a proven solution? How would you go about to attack this problem?","['proof-verification', 'functions', 'linear-algebra', 'polynomials', 'problem-solving']"
3213733,Can't solve complicated second order differential equation (Poisson-Boltzmann equation),"Is there anyone able to solve this second order differential equation? It is the Poisson-Boltzmann equation (found in the field of electrostatics) solved on cylindrical coordinates just on the radial direction. $$
(\varphi'+r\cdot\varphi'')=A\cdot e^{−(B\cdotφ+C)}
$$ where $\varphi$ is the variable I want to solve the problem for, the derivatives of $\varphi$ are with respect to $r$ , while $A,B,C$ are constants. I have been around it for a long time and can't solve it. What I had tried was to do a variable change by setting a new variable called z for instance which is equal to the exponential term, but I arrive nowhere. All the help is much appreciated :)","['calculus', 'ordinary-differential-equations']"
3213734,Determining if $f\in L^{p}(\mathbb R)$ from a bound on the measure of the level sets $\{|f|>\lambda\}$ for all $\lambda>0.$,"$\textbf{The Problem:}$ Let $f$ be a measurable function on $\mathbb R$ with respect to the Lebesgue measure $m$ . $\textbf{a)}$ Suppose that $$m(\{\vert f\vert>\lambda\})\leq(1+\lambda)^{-1}$$ holds for every $\lambda>0.$ For which $p\in\{1,\infty\}$ do we have $f\in L^{p}(\mathbb R)$ ? $\textbf{b)}$ Now suppose that $$m(\{\vert f\vert>\lambda\})\leq(1+\lambda)^{-10}.$$ For which $p\in[1,\infty]$ do we have $f\in L^{p}(\mathbb R)?$ $\textbf{My Thoughts:}$ $\textbf{a)}$ It doesn't necessarily follow that $f\in L^1(\mathbb R)$ or $f\in L^{\infty}(\mathbb R)$ since a counterexample in both cases is given by $f(x)=(2x)^{-1}\cdot\mathbf1_{x>0}.$ $\textbf{b)}$ The following formula for the $L^p-$ norm will be used $$\|f\|_{p}^{p}=\int_{0}^{\infty}p\lambda^{p-1}m(\{|f|>\lambda\})d\lambda.$$ With that in mind we have the following by hypothesis $$\|f\|_{p}^{p}\leq\int_{0}^{\infty}\frac{p\lambda^{p-1}}{(1+\lambda)^{10}}d\lambda,$$ so if $p-1\leq8$ we are golden, and the integral converges, and so $f\in L^{p}(\mathbb R)$ for all $1\leq p\leq9.$ If $p>9$ , then the bound on the $L^p-$ norm seems useless. So I think the claim may not follow, so I tried building up counterexamples from the hypothesis on the level sets. I came up with the function $f(x)=(x^{-1/10}-1)\cdot\mathbf1_{x>1}(x)$ by reverse engineering the bound on the level sets. This function will not be in $L^{p}(\mathbb R)$ for all $p>9.$ Is my work above accurate? Any comments are welcomed, be it about style, lack of details, and of course, correctness of the ideas. Thank you for your time.","['lp-spaces', 'lebesgue-integral', 'functional-analysis', 'real-analysis']"
3213741,Find number of all spanning tree of $G_n$,"Find $d_n$ which is number of all spanning tree of $G_n$ where $V =
 \left\{ 0,1,...,n  \right\} $ and $$E = \left\{ \left\{ 0,i \right\} :
 i \in [n]  \right\} \cup   \left\{ \left\{ i,i+1 \right\} : i \in
 [n-1]  \right\}  $$ This is our graph And I found that $$ d_n = 2d_{n-1} \text{ for } n \ge 4$$ $d_1 = 1$ $d_2 = 1$ $d_3 = 3$ But the pattern looks too simple, can somebody verify that?","['graph-theory', 'trees', 'discrete-mathematics']"
3213793,"If every nonidentity element in a group is of order $2$, the group is abelian [duplicate]","This question already has answers here : Prove that if $g^2=e$ for all $g$ in $G$ then $G$ is Abelian. (15 answers) Closed 5 years ago . Let $G$ be a group. Show that if every non-identity element in $G$ has order $2$ then $G$ is abelian. Proof: Let $a,b $ be non-identity elements in $G$ . Since $|a|=|b|=2$ , that means $ab=babaab$ $=$ $ba$ . Is the proof correct? How can I improve it?","['abelian-groups', 'group-theory', 'abstract-algebra', 'proof-verification']"
3213826,"Find the limit or prove that it does not exist $\lim_{(x, \space y) \to (0, \space 0)} f(x, y) $ where $f(x, y) = \frac{x^5-y^5}{x^4-2x^2y^2+y^4}$","Find the limit or prove that it does not exist $$\lim_{(x, \space y) \to (0, \space 0)}  f(x, y) $$ where $$f(x, y) = \frac{x^5-y^5}{x^4-2x^2y^2+y^4}$$ Iterated limit $\displaystyle{\lim_{y \to 0}} \space \displaystyle{\lim_{x \to 0}} \space f(x, y) = \displaystyle{\lim_{x \to 0}} \space \displaystyle{\lim_{y \to 0}} \space f(x, y) = 0$ , but it doesn't mean that $\displaystyle{\lim_{(x, \space y) \to (0, \space 0)}}  f(x, y) = 0 $ . I've also tried substitution $x = r \cdot \cos \phi, \space y = r \cdot \sin \phi$ , which gave me $ \frac{r(\cos^5\phi - \sin^5\phi)}{(\cos^2\phi-\sin^2\phi)^2} $ . If $x \to 0$ and $y \to 0$ , then $r \to 0$ . So, $\displaystyle{\lim_{r \to 0}} \space \frac{r(\cos^5\phi - \sin^5\phi)}{(\cos^2\phi-\sin^2\phi)^2} = 0$ . But I have a feeling that original limit doesn't exist and WolframAlpha says that too and I'm stuck here. If my assumption is correct, how to prove it properly?","['multivariable-calculus', 'limits', 'calculus']"
3213868,Clarification of algebra in moment generating functions,"Suppose $X$ has a range $\{1,2,\dots n \}$ and $p_X(j)=1/n$ for $1\leq j \leq n$ (uniform distribution). Then \begin{align*} 
g(t)&=\sum_{j=1}^{n}\frac{1}{n}e^{tj}\\
&=\frac{1}{n}(e^t+e^{2t}+\cdots+e^{nt})\\
&=\frac{e^t(e^{nt}-1)}{n(e^t-1)} \end{align*} I don't understand how the algebra goes from step 2 to step 3 here. I understand factoring out an $e^t$ , but how does the denominator come about. Is this polynomial division?","['algebra-precalculus', 'probability']"
3213912,"$\sum_{n=1}^\infty \chi(n)\phi(n)n^{-s} = \frac{L(\chi,s-1)}{L(\chi,s)}$","Let $\chi$ be a Dirichlet character mod 4. Show $\sum_{n=1}^\infty \chi(n)\phi(n)n^{-s} = \frac{L(\chi,s-1)}{L(\chi,s)}$ and $\sum_{n=1}^\infty \chi(n)d(n)n^{-s}=L(\chi,s)^2$ . ( $\phi$ is the Euler totient function and $d(n)$ is the number of divisors of $n$ .) First, is this true just for characters mod 4 and not true in general?
I'm not sure what specific properties about characters mod 4 I should use besides that $\chi(n)=0$ for $n$ even. I took the log of both sides and tried to use the following: $$L(\chi,s)=\prod_{p \text{ prime}}\frac{1}{1-\frac{\chi(p)}{p^s}}$$ $$\log L(\chi,s)=\sum_{p \text{ prime}}\sum_{n=1}^\infty \frac{\chi(p)^n}{np^{ns}}$$ $\chi$ and $\phi$ are multiplicative, so we can express $$\sum_{n=1}^\infty \chi(n)\phi(n)n^{-s}$$ as the Euler product $$\prod_p(1+\frac{\chi(p)\phi(p)}{p^s}+\frac{\chi(p^2)\phi(p^2)}{p^{2s}}+\cdots).$$ Manipulating things are not quite working. Any help would be appreciated.","['dirichlet-series', 'number-theory']"
3213915,"Find extreme values for $f(x,y,z)=xyz$ given the constraints $g_{1}(x,y,z)=x+y+z-5$ and $g_{2}(x,y,z)=xy+yz+zx-8$ using Lagrange multipliers method.","I want to calculate extreme values for $f(x,y,z)=xyz$ given the constraints $g_{1}(x,y,z)=x+y+z-5$ and $g_{2}(x,y,z)=xy+yz+zx-8$ using Lagrange multipliers method. Im skeptical about my solution for this problem which goes as follow. First I got: $$\nabla f= (yz)i+(xz)j+(xy)k,$$ $$\nabla g_{1}=i+j+k,$$ and $$\nabla g_{2}=(y+z)i+(x+z)j+(y+x)k.$$ So from having the equality $\nabla f = \lambda_{1} \nabla g_{1} + \lambda_{2} \nabla g_{2}$ I got the following equation system: \begin{align*}
yz&= \lambda_{1} + \lambda_{2}(y+z) \\
xz&=\lambda_{1}  + \lambda_{2}(x+z)\\
xy&=\lambda_{1} + \lambda_{2}(y+z)\\
\end{align*} But Im really stucked finding the extrem values from the last equations system. So far,  I realize that if I sum up the three equations from the system and the way $g_{2}(x,y,z)$ is defined I obtained: $$xy+yz+zx=8=3(\lambda_{1})+3(\lambda_{2})(x+y+z).$$ But from the way $g_{1}(x,y,z)$ is defined I got that last equation is $$8=3(\lambda_{1})+3(\lambda_{2})(5).$$ So I found  by trial and error that $\lambda_{1}=1$ and $\lambda_{2}=\frac{1}{3}$ , im not sure if there is more possible values for $\lambda_{1}$ and $\lambda_{2}$ satysfing the last equation. Anyways, from here I have been trying to find the values for $x,y$ and $z$ substituting the values I obtained for $\lambda_{1}$ and $\lambda_{2}$ in the equation from the original system of equations. For example, from the first equation I got $$yz=1+\frac{1}{3}(y+z)$$ But finding $x,y$ and $z$ that way is hard. I´ve been thinking also that from some equation before I have that: $$8=3\lambda_{1}+3\lambda_{2}(x+y+z)=3+(x+y+z)$$ . The problem of finding $x,y$ and $z$ this way is that I got a lot of points satisfying this last equation. Just to list a few: $$(5,0,0),(0,5,0),(0,0,5),(1,1,3),...$$ . Basically, all the points $(x,y,z) \in \mathbb{R}^{3}$ which satisfy $g_{1}(x,y,z)=0$ but I can tell there is something wrong from the fact a lot of these points dont satisfy the equations system obtained from $\nabla f = \lambda_{1} \nabla g_{1} + \lambda_{2} \nabla g_{2}$ .","['lagrange-multiplier', 'real-analysis', 'maxima-minima', 'multivariable-calculus', 'calculus']"
3213974,Limit definition of a derivative proof?,"Suppose that $f$ is a function with the properties: $f$ is differentiable everywhere, $f(x+y)=f(x)f(y)$ , $f(0)≠0$ , $f'(0)=1$ . I need to learn how to use limit definition of the derivative to show $f'(x) = f(x)$ for all values of $x$ . I have: $$\lim \limits_{h \to 0} \frac{f(x+h)-f(x)}{h}$$ $$\lim \limits_{y \to 0} \frac{f(x+y)-f(x)}{h}$$ $$\lim \limits_{y \to 0} \frac{f(x)f(y)-f(x)}{h}$$ $$\lim \limits_{y \to 0} \frac{f(x)(1)-f(x)}{h}$$ $$\lim \limits_{y \to 0} \frac{f(x)-f(x)}{h}$$ ...indeterminate form $\frac{0}{0}$","['limits', 'calculus', 'functions']"
3213977,Dimension of a maximal vector subspace of $M_3(\mathbb{C})$ which is commutative under the multiplication,"Let $V$ be a $\mathbb{C}$ -vector subspace of $M_3(\mathbb{C})$ satisfying following properties: (A) for every $A,B \in V$ , $AB = BA$ ; (B) if $W$ is a $\mathbb{C}$ -vector subspace of $M_3(\mathbb{C})$ which contains $V$ as a proper subset, then there exists $A,B \in W$ such that $AB \neq BA$ . Then what is $\dim V$ ? The vector space V generated by $$\begin{bmatrix}1 & 0 & 0\\0 & 0 & 0 \\ 0 & 0& 0\end{bmatrix},
\begin{bmatrix}0 & 0 & 0\\0 & 1 & 0 \\ 0 & 0& 0\end{bmatrix} \text{ and }
\begin{bmatrix}0 & 0 & 0\\0 & 0 & 0 \\ 0 & 0& 1\end{bmatrix},$$ satisfies these properties.
So I think $\dim V = 3$ in general. I think that,  if $\dim \lt 3$ , then $V$ is too small to be maximal, and if $\dim \gt 3$ , then $V$ is too big to be a commutative algebra.
So it seems to be reasonable for me. Thank you very much.","['matrices', 'linear-algebra', 'vector-spaces']"
3213979,$\Delta \mathbf n = -2 \mathbf n$ on the Euclidean sphere,"Let us consider the Euclidean two-sphere, defined by the embedding in the three dimensional Euclidean space as $$
\mathbf n \cdot \mathbf n = 1\,,
$$ where $\cdot$ denotes the standard scalar product. The metric on the sphere, in some coordinates $x^i$ , is expressed as $$
\gamma_{ij}=\mathbf e_i \cdot \mathbf e_j\,,
$$ where $\mathbf e_i=\partial_i\mathbf n$ . For instance, in the standard spherical coordinates $$
\mathbf n=(\sin\theta\cos\phi,\sin\theta\sin\phi,\cos\theta)
$$ and $$
\gamma_{\theta\theta}=1\,,\qquad \gamma_{\phi\phi}=\sin^2\theta\,,\qquad
\gamma_{\theta\phi}=0\,.
$$ We define the Laplace-Beltrami operator on the sphere by $
\Delta = \gamma^{ij} D_iD_i
$ ,
where $\gamma^{ij}$ is its inverse and $D_i$ is the associated Levi-Civita connection. I would like to prove that $$\Delta \mathbf n = -2 \mathbf n$$ and that in higher dimensions the same holds with $2$ replaced by the dimension of the sphere. I came to believe that this is true by an explicit check in spherical coordinates in dimensions $3$ , $4$ and $6$ . Considering that parallel transport of a given tangent vector $\mathbf v$ defined at the point $x+dx$ to the point $x$ is defined by keeping constant its embedding components and then projecting it on the sphere at the point $x$ , we have $$
\mathbf v_{\parallel}(x+dx,x)=\mathbf v(x+dx)-\mathbf v(x+dx)\cdot \mathbf n (x)\, \mathbf n(x)
$$ hence $$
D_i\mathbf v\, dx^i = \mathbf v_{\parallel}(x+dx,x)- \mathbf v (x)= (\partial_i\mathbf v+\partial_i\mathbf n \cdot \mathbf v\, \mathbf n)dx^i
$$ where we have used $\mathbf n \cdot \partial_i\mathbf v+\partial_i\mathbf n \cdot \mathbf v=0$ and $$
D_i\mathbf v = \partial_i\mathbf v+\partial_i\mathbf n \cdot \mathbf v\, \mathbf n\,.
$$ Applying this to the basis vectors $\mathbf e_j =\partial_j\mathbf n$ affords $$
D_i\mathbf e_j = \partial_i \mathbf e_j+\gamma_{ij}\mathbf n\,.
$$ But unfortunataly I am not able to go further.","['laplacian', 'surfaces', 'riemannian-geometry', 'differential-geometry']"
3214039,"How to show $\frac{1}{L}\|\nabla f(y)-\nabla f(x)\|_2^2 \leq \langle \nabla f(y) - \nabla f(x) ,y-x \rangle$ for strongly convex function?","Suppose $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is a strongly convex function, i.e., $$
\langle \nabla f(y) - \nabla f(x) ,y-x \rangle \geq \theta \|y-x\|_2^2 \,\,\,\,\, \forall x,y \in \mathbb{R}^n
$$ or $$
z^T(\nabla^2 f(x) -\theta I)z\geq 0 \,\,\,\,\forall  z \neq 0
$$ Also, suppose $\nabla f(x)$ is a Lipschitz with Lipschitz constant $L>0$ , i.e., $ \|\nabla f(y)- \nabla f(x)\|_2  \leq L \|y-x\|_2$ . Show $\frac{1}{L}\|\nabla f(y)-\nabla f(x)\|_2^2 \leq \langle \nabla f(y) - \nabla f(x) ,y-x \rangle$","['functions', 'convex-analysis', 'lipschitz-functions']"
3214042,"$x^3-8x^2+30x-20=0$ has roots $a$, $b$, $c$. Find the equation with roots $a+2$, $b+2$, $c+2$","I have the equation $$x^3-8x^2+30x-20=0$$ let's call the roots $a,b,c$ It's easy to find the equation with roots $(a+2)(b+2)(c+2)$ these are the steps in my books so the equation is $$ aX^3+ bX^2 + cX + D = 0 $$ $$ x^3 -14x^2 +74x -120 = 0$$ my problem is, I can't understand why divide by $-2$ not $2$ how do I do this division with long division? like what am I dividing by if the symbols were here?!","['algebra-precalculus', 'systems-of-equations', 'polynomials']"
3214086,Proving $\lim_{x\to\infty}\frac{x+3}{x^2-3}=0$ using delta-epsilon,"I'm trying to prove $$\lim_{x\to\infty}\frac{x+3}{x^2-3}=0$$ using delta-epsilon. In the definition of limit $$|f(x)-L|\lt\epsilon$$ $$|\frac{x+3}{x^2-3}-0|\lt\epsilon$$ $$|\frac{x+3}{x^2-3}|\lt\epsilon$$ and since the left side quite complicated I've come up with simple term that always bigger than $$\frac{x+3}{x^2-3}$$ and that is $$\frac{2x}{x^2/1.5}$$ or $$\frac{3}{x}$$ as long I take $x>3$ . so here I take $3$ as $M$ so it will turn $x>M$ into $x>3$ then for second inequality: $$\frac{3}{x}\lt\epsilon$$ $$x\gt\frac{3}{\epsilon}$$ and here I take $\frac{3}{\epsilon}$ as $M$ and wrapping things up $$\frac{x+3}{x^2-3}\lt\frac{3}{x}\lt\epsilon$$ . But in reality how both $M$ affect me to choose $x$ ? I mean in first inequality is demand me with $x>3$ . In other words if I choose $M$ that smaller than $3$ , it will crumble the first inequality (when I try simplifying things). 
So if I look at second $M (=\frac{3}{\epsilon})$ should I have the limitation $$\epsilon\lt1$$ so that I only can get $x$ more than $3$ ?",['limits']
3214107,Understanding Baby Rudin Theorem 3.37,"I am looking for some help with making sense of the proof in Baby Rudin (i.e. ""Principles of mathematical analysis"") for Theorem 3.37, shown below. 3.37 Theorem For any sequence $\{c_n\}$ of positive numbers, $$\liminf_{n \to \infty} \frac{c_{n+1}}{c_n} \leq \liminf_{n \to \infty} \sqrt[n]{c_n} \\ \limsup_{n \to \infty} \sqrt[n]{c_n} \leq \limsup_{n \to \infty} \frac{c_{n+1}}{c_n}.$$ Proof We shall prove the second inequality; the proof of the first is quite similar. Put $$\alpha = \limsup_{n \to \infty} \frac{c_{n+1}}{c_n}.$$ If $\alpha = + \infty$ , there is nothing to prove. If $\alpha$ is finite, choose $\beta > \alpha$ . There is an integer $N$ such that $$\frac{c_{n+1}}{c_n} \leq \beta$$ for $n \geq N$ . In particular, for any $p > 0$ , $$c_{N+k+1} \leq \beta c_{N+k}.$$ Multiplying these inequalities, we obtain $$c_{N+p} \leq \beta^p c_N,$$ or $$c_n \leq c_N \beta^{-N} \cdot \beta^n \quad (n \geq N).$$ Hence $$\sqrt[n]{c_n} \leq \sqrt[n]{c_N \beta^{-N}} \cdot \beta,$$ so that $$\limsup_{n \to \infty} \sqrt[n]{c_n} \leq \beta, \quad \quad (18)$$ by Theorem 3.20(b). Since (18) is true for every $\beta > \alpha$ , we have $$\limsup_{n \to \infty} \sqrt[n]{c_n} \leq \alpha.$$ I am struggling to understand the steps that are taken in the following part of the proof: Multiplying these inequalities, we obtain $$c_{N+p} \leq \beta^p c_N,$$ or $$c_n \leq c_N \beta^{-N} \cdot \beta^n \quad (n \geq N).$$ Specifically, I'm wondering: Where does the inequality $c_{N+p} \leq \beta^p c_N$ come from? Is the author saying that this is the result of multiplying the two preceding inequalities together, like $(\frac{c_{n+1}}{c_n} \cdot c_{N+k+1}) \leq (\beta \cdot \beta c_{N+k})$ ? What is the connection between $c_{N+p} \leq \beta^p c_N$ and the next inequality $c_n \leq c_N \beta^{-N} \beta^n$ ? Any help would be very much appreciated! Side note: I found this question to be really helpful with proving the first inequality.","['limits', 'real-analysis']"
3214126,Making a Dirac Delta expression rigorous (measure theory),"I have been trying to make an expression in terms of a ``Dirac Delta function'' rigorous, but I failed miserably. Please help, if you can. References are welcome. The motivation for this comes from calculating densities $\rho$ in a measure space $M$ , if one is given `samples' $\alpha(\omega)$ indexed by $\omega$ in another measure space $\Omega$ . This has applications in statistical physics, so perhaps people have done this already. For starters, if $\left(\Omega, \mathcal A \right)$ is a measurable space and $\omega \in \Omega$ , then the Dirac measure is $$ \delta_\omega \colon \, 
\mathcal A \mapsto \mathbb R \, , \quad A \to \delta_\omega\left(A \right) :=  
\begin{cases}
1 \, , \, \omega \in A \\
0 \, , \,\omega \notin A
\end{cases} \, .
$$ I believe that the problem at hand requires this concept, and not the Dirac distribution. Now let $(M,\mathcal A, \mu)$ , $(\Omega,\mathcal B, \nu)$ be $\sigma$ -finite measure spaces and let $\alpha \colon \Omega \to \mathcal M$ be a measurable function. Then I want that for any $\theta \in M$ and any sequence of measurable sets $\left(N_n (\theta)\right)_{n \in \mathbb N}$ containing $\theta$ , having finite measure and with $$\lim_{n \to \infty} \mu \left(N_n (\theta)\right) = 0$$ that $$\theta \mapsto \rho \left( \theta \right) = 
\lim_{n \to \infty} \frac{\nu \left( \alpha^{-1}
\left(N_n (\theta)\right)\right)}{\mu \left(N_n (\theta)\right)}$$ is well-defined and measurable. This is where the Dirac measure comes in: I'm quite sure that for another (arbitrary?) sequence $\left( A_n \right)_{n \in \mathbb N}$ of 
measurable sets in $\Omega$ with finite measure and with $$\lim_{n \to \infty} \nu \left( A_n^C \cap \Omega \right) = 0$$ (i.e. ` $A_n$ converges to $\Omega$ in measure') that $$\rho \left( \theta\right) \overset{!}{=} \lim_{n \to \infty} 
\frac{\int_{A_n} d \nu \negthinspace \left( \omega\right) \, \, \delta\left( \theta - \alpha_{\omega}\right)}{\nu \left( A_n\right)}$$ The problem is 1) to turn this into a mathematically sensible expression in terms of the Dirac measure, and 2) to show equality with the above one. Note that neither $M$ nor $\Omega$ can be assumed to be finite measure spaces. EDIT: Alright, let me clarify things a bit. 1) The above description is a formalization of the following problem: Say I decided to place an uncountable number of points in $M= \mathbb{R}^n$ , but I decided to label them in a rather inconvenient manner. So for each label $\omega \in \Omega$ I have a point $\alpha_\omega \in M$ . Now the problem is that, say, suddenly I don't care about the labels any more, but rather I want to know how these points are distributed in space with respect to some volume measure (e.g. the Lebesgue measure $\mu$ ). That is, I want a density. 
So I want to integrate over my label space (each label has equal weight, so I have a measure $\nu$ on it) and only count those points in which I'm evaluating the density at. The above expressions are what I came up with. 2) The last equation is purely symbolic. The point is to ""count"" only those points in $\Omega$ for which $\theta = \alpha_\omega$ . So it's not really useful to consider it as the Dirac delta of a function. The sequence of sets is only there, because $\nu(\Omega)$ may be infinite. 3) Yes, the first ""definition"" is probably problematic. For instance, what if all $\alpha_\omega$ are equal? We want to get a possibly singular measure out... Almost everywhere existence is perfect. Radon-Nikodym only works for $\Omega = M$ , which is too restrictive.","['integration', 'measure-theory', 'statistical-mechanics', 'probability-theory', 'density-function']"
3214140,Are all complex iterations in the form of $z_{n+1} = z_n + c$ fractals?,"I've learned that many fractals can be described like the below: $$z_n \in F \text{ if } \limsup_{n\to\infty} |z_{n+1}| \leq i$$ Where $i$ is some number, usually I see 2, and $z$ is a member of the set. This is paired with some iteration on $z_n$ to make $z_{n+1}$ . Classic example is the Mandelbrot set: $z_{n+1} = z_n^2 + c$ , where $c$ is a complex number. But some of the things I've read about fractals is they are self-similar, self-symmetric, have a finite area yet infinite perimeter. I have a few examples (color scheme = more red as approaches the set, magenta very close to the set, blue touching the set, black inside the set): A ""tri-wing"" vein set := $z_{n+1} = (-z_n^{-1})^2 + z_n$ A Julia set (k = -0.835 - 0.232i) := $z_{n+1} = z_n^2 + k$ These all are symmetric in some way, have finite area and infinite perimeter. But then I consider fractals like the Burning Ship ( $z_{n+1} = |z_n^2| + c$ ) which have no symmetry. Usually I can define a line somewhere on the graph intersecting the origin and it will be a mirror image on both sides, but not for everything I can define. Consider a basic example like $z_{n+1} = z_n^0 + c$ : This has symmetry but it's just a circle. This has a finite area as well as a finite perimeter. Or am I wrong? Only $\{0\} \in F$ for this example, so it's not a fractal. I'll disclaim I've only learned about fractals in the context of $z_{n+1} = z_n + c$ , with $z_n$ to the power of an integer, or adding extra terms. Is it safe to assume this is not a particularly good means of defining fractals?","['complex-dynamics', 'geometry', 'complex-numbers', 'fractals']"
3214177,"Almost everywhere (ae) Homogeneous function of degree $0$ equals to a constant for ae $x \in (0,\infty)$ provided $ f $ is measurable?","Is an almost everywhere (ae) Homogeneous function $f$ of degree $0$ equals to a constant for almost every $x \in (0,\infty)$ given that $ f $ is measurable? Let $f : \mathbb R \to \mathbb R$ . If $f(ax)=f(x)$ ae for any $a>0$ Then $f(x)=c$ for almost every $x \in (0,\infty)$ , where $c$ is a constant. Is the above true? I know it is true if $ f $ is locally integrable see here I encountered this problem while studying bounded linear operators $ T:L^2 \to L^2$","['integration', 'measure-theory', 'real-analysis']"
3214199,need help solving geometric series questions,"A geometric sequence has its first term as $10000$ and a fourth term as $−7290$ .
  If the pattern continues forever, find the sum of the terms in the sequence. I know that the $n^{th}$ term is found by $$t_n=a_1r^{n-1}$$ where $a_1$ is the first term and $r$ is the common ratio. Thus the fourth term is given by $$10000 * r^3 = -7290$$ so $$r^3 = -0.729$$ $$r= -0.9$$ So the sum of the series is: $$S_ \infty = \frac{10000}{1-(-0.9)}$$ Is this correct? The sum of the first 2018 terms of a geometric sequence is 200. The sum of
the first 4036 terms is 380. Find the sum of the first 6054 terms","['algebra-precalculus', 'proof-verification', 'geometric-series', 'sequences-and-series']"
3214244,Intuition behind joint pdf with transformations with partitioned support,"I'm trying to understand this part of Statistical Inference(Casella, Berger) regarding expressing the joint pdf of non-bijective transformations. More specifically, what is the intuition behind (4.3.6)? I understand the case of one-two-one transformations, but here, I cannot understand why the joint pdf becomes a summation. Thanks in advance!","['statistics', 'jacobian', 'intuition', 'probability', 'random-variables']"
3214326,"Eliminating $\theta$ from the system $x\sin\theta-y\cos\theta=-\sin4\theta$, $x\cos\theta+y\sin\theta=\frac52-\frac32\cos4\theta$","Eliminate $\theta$ from the system of equations. $$\begin{align}
x\sin\theta-y\cos\theta&=\phantom{\frac52\frac32}-\sin4\theta \\
x\cos\theta+y\sin\theta&=\frac52-\frac32\cos4\theta
\end{align}$$ I am stuck at this question after squaring and adding.",['trigonometry']
3214332,"Find the basis of $H = \{(x_1, x_2, x_3) \in \Bbb Z^3 | 6 \text{ divides } 2x_1+3x_2+4x_3\}$","Find the basis of $H = \{(x_1, x_2, x_3) \in \Bbb Z^3 |  6 \text{ divides } 2x_1+3x_2+4x_3\}$ . My attempt was to say that $$\{(3,0,0), (0,2,0), (0,0,3)\}$$ is the basis, since this automatically allows every element in $H$ to be divided by $6$ . I do not know the answer, but apparently I'm wrong. I would like some help on this problem, and a general method to solve these types of problems would be appreciated. edit: Apparently, the answer (or one of the answers) is $\{(3,0,0), (-6,2,0), (-2,0,1)\}$ . Still can't figure out why, or how to obtain the answer, though.","['group-theory', 'abstract-algebra']"
3214403,What does the abelianization of a Sylow $2$-subgroup of the symmetric group look like?,"I started thinking about this for no particular reason. Let $P_n$ be a Sylow $2$ -subgroup of the symmetric group $S_{2^n}$ . What does its abelianization $P_n/[P_n,P_n]$ look like? The groups $P_n$ have the well known recursive structure as a wreath product tower: $P_n\simeq P_{n-1}\wr C_2$ . $P_n$ is also isomorphic to the group of graph automorphisms of a full rooted binary tree of depth $n$ (so $2^n$ leaves).
See for example here . My thinking: The group $P_n$ is generated by the following permutations $g_i,i=1,2,\ldots,n$ , $$
\begin{aligned}
g_1&=(12),\\
g_2&=(13)(24),\\
g_3&=(15)(26)(37)(48),\\
\vdots&\\
g_n&=\prod_{j=1}^{2^{n-1}}(j;j+2^{n-1}),
\end{aligned}
$$ where $g_j$ is a product of $2^{j-1}$ disjoint $2$ -cycles pointwise intechanging the ranges $[1,2^{j-1}]$ and $[2^{j-1}+1,2^j]$ . All the generators $g_j$ have order two, so it seems to me that $P_n/[P_n,P_n]$ should be an elementary abelian $2$ -group. I have a hunch that the rank of this elementary abelian $2$ -group should be equal to $n$ . Basically this is because I don't see how there could be relations stopping one of the generators from having an impact on the abelianization, and in the cases $n=1$ and $n_2$ with $P_1\simeq C_2$ and $P_2\simeq D_4$ (=the dihedral group of symmetries of a square) it is easy to verify by hand that $$P_n/[P_n,P_n]\simeq C_2^n.$$ Is my hunch correct? Why or why not?","['group-theory', 'sylow-theory']"
3214438,"Proving $ \sum_{cyc}^{} \frac {a(a^3+b^3)}{a^2+ab+b^2} \ge \frac{2}{3} (a^2+b^2+c^2)$ for $a, b, c > 0$","For $a,b,c>0$ , I have to prove that $$ \sum_{cyc}^{} \frac {a(a^3+b^3)}{a^2+ab+b^2} \ge \frac{2}{3} (a^2+b^2+c^2).$$ We have: $$\begin{align} \sum_{cyc}^{} \frac {a(a^3+b^3)}{a^2+ab+b^2} 
&= \sum_{cyc}^{} \frac {a^4}{a^2+ab+b^2} + \sum_{cyc}^{}\frac {a b^3}{a^2+ab+b^2} \\[4pt]
&\ge \frac{(a^2+b^2+c^2 )^2}{a^2+b^2+c^2+ab+bc+ca} + \sum_{cyc}^{}\frac {a b^3}{a^2+ab+b^2} \\[4pt]
&\ge \frac{1}{2} (a^2+b^2+c^2) + \sum_{cyc}^{}\frac {a b^3}{a^2+ab+b^2}
\end{align}$$ but I can't move on.","['substitution', 'algebra-precalculus', 'buffalo-way', 'inequality']"
3214489,Why does every $ m \times n$ matrix of rank $r$ reduce to $(m \times r)$ times $(r \times n)$?,"How can I prove the following statement? Every $m \times n$ matrix of rank $r$ reduces to $(m \times r)$ times $(r \times n)$ : $A = $ (pivot columns of $A$ ) (first $r$ rows of $R$ ) = (COL)(ROW). [ Source: Gilbert Strang, Introduction to Linear Algebra , question $56$ , section $3.2$ . ] I think that the $R$ is reduced row echelon form. I believe there is a brief elegant proof.","['matrices', 'matrix-rank', 'linear-algebra', 'matrix-decomposition']"
3214499,"$\{x_{k}\}$ has an accumulation point and $z_k=||x_{k}-x_{k+1}||^2$ is summable, then does $\{x_{n}\}$ converge?","Let $\{x_k\}\subset \mathbb{R}^n$ and suppose $\{x_{k}\}$ has an accumulation point and $z_k=||x_{k}-x_{k+1}||^2$ is summable. I was trying to prove that $\{x_{n}\}$ converges, but the best I can prove is  that $$\sum^{\infty}_{n=1} \frac{\left(\sum^{n}_{j=1}||x_{j+1}-x_{j}||\right)^2}{n^2}<\infty$$ using Hardy's Inequality. I appreciate any help.","['multivariable-calculus', 'sequences-and-series']"
3214509,Elementary proof for infinitude of primes in an arithmetic progression of a special form,"In this recent question the asker was looking for a proof of the existence of infinitely many prime numbers $p$ such that both $p-2$ and $p+2$ are composite. A highly upvoted answer by Ege Erdil made the point that all the primes of the form $p=15n+8$ qualify. They then called upon Dirichlet's theorem of primes in an arithmetic progression to reach an affirmative answer. I would like to see an ""elementary"" proof of the infinitude of primes in an arithmetic progression that fits in here. So I generalize Ege's recipe to the following question: Is there an example of a pair of integers $(a,n)$ such that $\gcd(a-2,n)>1$ , $\gcd(a+2,n)>1$ , and that there is an elementary proof for the infinitude of primes $p\equiv a\pmod n$ ? Your definition of ""elementary"" may vary. I'm leaving that somewhat open on purpose, but at least anything more elementary than $L$ -functions will qualify. This may prove to be taxing. There is no shortage of elementary proofs for the infinitude of primes in an arithmetic progression on our site: $p\equiv\pm1\pmod4$ (many threads), $p\equiv1\pmod5$ , $p\equiv3\pmod8$ , $p\equiv1\pmod n$ , $p\equiv-1\pmod 6$ , $p\equiv7\pmod{12}$ . However, those methods don't really work for the purposes of my question. That's because there is a deeper result due to Murty and Thain , locally referred to here , stating that a ""Euclid style"" proof for the infinitude of prime $p\equiv a\pmod n$ exists if and only if $a^2\equiv1\pmod n$ . This rules out Euclid style proofs as an option. For if $a^2\equiv1\pmod n$ , then $n\mid a^2-1$ . But, together with this, the conditions $\gcd(a-2,n)>1$ and $\gcd(a+2,n)>1$ imply that $$1<\gcd((a-2)(a+2),n)=\gcd(a^2-4,n)=\gcd(3,n).$$ That gcd can thus only be $3$ , but it is obvious that $3$ cannot be a factor of both $a-2$ and $a+2$ . So something else is needed! This may be a tall order, but I'm asking this in case this rings a bell. A ""Euclid style"" proof means roughly the following: Assume that we have an exhaustive (finite) list of primes $p_1,\ldots,p_k$ in a given residue class. Then a cleverly chosen polynomial $P$ evaluated at $p_1p_2\cdots p_k$ can be shown to have a prime factor in this residue class, but not equal to any of $p_i$ . Ergo, there must be infinitely many such primes. In other words, mimicking Euclid's classical proof for the infinitude of primes.","['number-theory', 'arithmetic-progressions', 'elementary-number-theory', 'prime-numbers']"
3214573,Is the group of elements $t \in \mathbb{R}$ such that $A +t = A$ up to measure $0$ closed?,"My question: If $A \subset \mathbb{R}$ is a measurable subset and $t_n \rightarrow t$ is a convergent sequence such that $A +t_n = A$ mod $\mu$ , does $A+t=A$ mod $\mu$ hold? Here $A = B$ mod $\mu$ means that $\mu(A \Delta B)=0$ where $A \Delta B$ is the symmetric difference between $A$ and $B$ and $\mu$ is the Lebesgue measure on $\mathbb{R}$ .","['group-actions', 'measure-theory', 'lebesgue-measure', 'real-analysis']"
3214624,"Given points A, B, and C, how to determine whether both angles ABC and ACB are acute?","I'm trying to figure out a (computationally efficient) way to determine whether, given the x and y coordinates of points A, B, and C, both the angle going from A to B to C and the angle from A to C to B are less than 90 degrees. Basically, I want to determine whether Point A falls within the green area or the red area in this image . I could think of some convoluted ways to do this, but I feel like there should be a simpler one. Thanks!",['geometry']
3214657,Can the sum of a differentiable and a non differentiable function be differentiable?,"Can the sum of a differentiable and a non differentiable function be differentiable? In one of the solutions to a question, my book's author used the fact that the sum of a differentiable and non differentiable function cannot be differentiable. I am wondering if this is always true or are there some counter examples?",['calculus']
3214732,Problem with estimating a sequence with intuition,"I've frequently used “intuition” to solve limits at infinity. For example, if someone asked me what is: $$ \lim_{x \to \infty} f(x) = \frac {x^5 + x^3 + x}{x^2} $$ Or a sequence that can be represented by such a function, I would quickly argue that the only thing that matters as go through higher and higher numbers for $x$ would be the terms with the higher powers, and so the limit becomes: $$ \lim_{x \to \infty} f(x) = \frac {x^5 }{x^2} $$ The numerator is growing at a much faster rate than the denominator, and so the function will diverge to $ + \infty $ . But, when I was going through a problem from Paul's online notes , my intuition didn't go well with solving it by the book. The problem is: $$ \left\{ {\frac{{\ln \left( {n + 2} \right)}}{{\ln \left( {1 + 4n} \right)}}} \right\}_{n = 1}^\infty $$ Does this converge to a value? Now, if we use L'Hopital's rule, this is a easy enough problem. It converges to $1$ . $$ \mathop {\lim }\limits_{n \to \infty } \frac{\ln ( n + 2 )}{\ln  (1 + 4n )} = \mathop \lim \limits_{n \to \infty } \frac{^{1}/_{(n + 2)}}{^{4}/_{(1 + 4n)}} = \mathop \lim \limits_{n \to \infty } \frac{1 + 4n}{4( {n + 2} )} = 1 $$ But, as I said, sometimes I like to do these by intuition, and what I did was this: as $ n $ approaches infinity, the integers “ $2$ ” and “ $1$ ” won't matter. So the limit becomes: $$ \lim_{n \to \infty} \frac { \ln(n) }{\ln(4n)} $$ The denominator is increasing at a much faster rate than the  numerator, and so the limit will converge to $0$ . I understand that something is wrong with my intuition, and intuition is probably not a good way to solve mathematical problems — indeed, the tutorial this problem is from itself mentions that intuition can sometimes lead you astray — but I'd love to gather some insight as to where I'm going wrong.","['limits', 'logarithms', 'intuition', 'sequences-and-series']"
3214818,Prove that every position is equally as likely in this random walk scenario,"There are two points $A$ and $B$ . You are standing in the middle between them. In each step, go half the way to the point $A$ , or half the way to the point $B$ , each with probability of $0.5$ . Mark the point where you stop. Prove that the ratio of amount of marks on each pair of subintervals of interval $[A, B]$ of the same length converges to $1$ .","['random-walk', 'probability']"
