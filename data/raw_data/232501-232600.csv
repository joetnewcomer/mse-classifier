question_id,title,body,tags
4841666,"Prove that the following sequences monotonically decrease and increase correspondingly. Since they are bounded, find the limit.","Let $\xi_n$ be a Poisson random variable with $\lambda = n \in \mathbb{N}$ . That is $P(\xi_n = k) = \frac{n^ke^{-n}}{k!}$ , for $k \in \mathbb{N}_0$ . Let $f_+(n) = P(\xi_n \geq n),\ f_-(n) = P(\xi_n > n)$ . Show that $f_+(n)$ monotonically decreases and $f_-(n)$ monotonically increases. I have no idea how to prove this, I've tried mathematical induction and it didn't make any sense. All what I've succeeded in is that $f_+(n) = f_-(n) + \frac{n^n e^{-n}}{n!}$ . Last term here approaches $0$ when $n$ approaches infinity, what means that the limit of these two will be the same.","['convergence-divergence', 'probability-distributions', 'probability-theory', 'poisson-distribution']"
4841668,Solve functional equation: $f(x) + f(y) = f\left(\frac{x+y}{1-xy}\right) $,"The equation being : $$ f(x) + f(y) = f\left(\frac{x+y}{1-xy}\right) $$ The most obvious answer is, $$f(x) = C\arctan x$$ but there is another possible answer e.i $$ f(x) =  \log\left(\frac{1-x}{1+x}\right) $$ which i am unable to get by the process i have provided. First substituting: $$    x = y = 0 \Rightarrow f(0) = 0$$ then partially differentiating the original equation wrt to $x$ and then $y$ individually (provided $x \ne y$ ), we'll get : $$ f'(x) = f'\left(\frac{x+y}{1-xy}\right)\frac{1+y^2}{(1-xy)^2}   - (i)$$ $$ f'(y) = f'\left(\frac{x+y}{1-xy}\right)\frac{1+x^2}{(1-xy)^2}   - (ii)$$ diving equations $i$ by $ii$ we get $$  \frac{f'(x)}{f'(y)} = \frac{1+y^2}{1+x^2}$$ $$ \Rightarrow f'(x)\left(1+x^2\right) = f'(y)\left(1+y^2\right)$$ which is only possible if the above equation is equal to some constant $c$ , since $x \neq y$ $$    f'(x)\left(1+x^2\right) = C $$ $$ \Rightarrow f'(x) = \frac{C}{1+x^2} $$ $$ \Rightarrow f(x) = \int \frac{C}{1+x^2} = C \arctan x + K $$ since $f(0) = 0, k = 0$ $$ \therefore  f(x) = C \arctan x $$ Also, when do i know that there is no more functions that satisfy the original equation ?","['integration', 'calculus', 'functions', 'logarithms']"
4841713,Why can't we extend any field by simply adding a new symbol to it?,"After trying to recall some fundamental field theory, I got very confused at the notion of field extensions. For example, when we make $\mathbb{C}$ out of $\mathbb{R}$ , we can simply think of it as adding $i$ , which is a symbol for which $i^2 = -1$ . So: $\mathbb{C} = \mathbb{R}(i)$ . However, $\mathbb{C}$ is algebraically closed, since each polynomial with complex coefficients has a root in $\mathbb{C}$ . I suppose what confuses me is why it follows from here that there exists no further field extension of $\mathbb{C}$ . This is always stated as obvious, but I can not make a formal argument. Why can't we always simply add a new symbol $z$ to a given field $k$ , which squares to any element of $k(z) = \{ a + b z \; | \; a,b \in k \}$ . For simplicity, let's say that $z^2 = 1$ . I know that  we already have $1^2 = 1$ and $(-1)^2 = 1$ , but why is this an issue? Why can't we have two distinct elements square to the same element? By defining addition as $$ (a+bz) + (c+dz) = (a+c) + (b+d)z $$ and multiplication as $$ (a+bz)\cdot(c+dz) = ac + adz + bcz + bdz^2 = (ac + bd) + (ad + bc)z,$$ all the properties for being a field are easily verified. So, what's the issue here? Edit: Thank you everyone for very insightful comments. As a follow-up, since we can do extensions of any fields (although they are not themselves necessarily fields), how can we conclude that an algebraically closed field (say $\mathbb{C}$ ) has no finite field extension, i.e. none of such extension by some symbol is a field?","['field-theory', 'abstract-algebra', 'extension-field']"
4841715,Relation between inertia group in character theory and commutative algebra,"When studying character theory (specifically, of normal subgroups), one comes across the concept of the inertia group . If $N \unlhd G$ , where $G$ is a finite group, then, $G$ acts on $\operatorname{Irr}(N)$ , the group of irreducible characters of $N$ , by $\theta^g(x) = \theta(gxg^{-1})$ (which is well-defined by the normality of $N$ , and is easily seen to be a character). The inertia group of $\theta$ is then $$I_G(\theta) = \{g \in G \mid \theta^g = \theta\}$$ which is the stabilizer of $\theta$ in this action. On the seemingly distinct field of algebraic field theory, though, there also exists an inertia group . If $K$ is an algebraic number field, $\mathfrak{p}$ is one of its primes (i.e., prime ideals of its ring of algebraic integers) and $L$ is a Galois extension with group $G = \operatorname{Gal}(L:K)$ , then $G$ acts on the primes of $L$ which lie over $\mathfrak{p}$ . If $\mathfrak{P}$ is one such prime, we write $G(\mathfrak{P}) = \{\sigma \in G \mid \sigma(\mathfrak{P}) = \mathfrak{P}\}$ , which is the decomposition group , and the stabilizer of $\mathfrak{P}$ . The inertia group is the subgroup $$I(\mathfrak{P}) = \{\sigma \in G(\mathfrak{P}) \mid \sigma(r) \equiv r \pmod{\mathfrak{P}}, \forall r \in R'\}$$ where $R'$ is the ring of algebraic integers of $L$ . So, both of these groups are point stabilizers in some form, though I can't see any other similarity between the two. I reckon there must be some deep connection, since the terminology is the same, even when it comes to related topics such as ramification indices. In short: Is there some major connection between the two ideas of inertia groups? Thanks in advance!","['algebraic-number-theory', 'ramification', 'characters', 'finite-groups', 'group-theory']"
4841720,What topological principle is at work here?,"My question is inspired by a problem I discovered in Putnam and Beyond , #470, pg. 142: Does there exist a continuous function $f : [0, 1] \rightarrow \mathbb R$ that assumes every element of its range an even (finite) number of times? The answer is yes, and they provide a caricature of a solution on pg. 542: This solution consists of infinitely many linear segments in the neighborhood of $0$ .  One can prove that any solution must have infinitely many local extrema. As I was thinking about this problem, I noticed that it depends delicately on the domain. If the domain were the circle $S^1$ , then much simpler solutions exist, e.g. $e^{i\theta} \mapsto \cos(2\theta)$ . Similarly, if the domain were a union of two disjoint intervals $[0,1]\cup [2,3]$ , then $x\mapsto \left(x-\frac{3}{2}\right)^2$ works. If the domain were a single half-open interval $[0,1)$ , then $\cos(4\pi x)$ works. Question:  Is there a topological characterization of which domains admit ""easy"" solutions to this problem? Commentary: I leave open the question of what constitutes an ""easy"" solution, although I've noticed that ""piecewise monotonic with finitely many monotone segments"" suffices to distinguish all the examples above. My hazy intuition for this problem is that there is a sort of parity to 1D domains, with a closed interval having odd parity, a point having odd parity, a circle having even parity, unions have parity equal to the sum of the components, etc. I conjecture a generalization of this problem to higher dimensional spaces $X$ might be to consider maps $X\rightarrow X$ which hit each point in the domain an even number of times.","['general-topology', 'algebraic-topology']"
4841734,A question about inverse closed subsets of $G \setminus \{e\}$,"Let $G$ be a finite group and $S$ is an inverse closed subset of $G\setminus\{e\}$ . When does for all $a,b\in S$ , $|Sa\cap S|=|Sb\cap S|$ ? An obvious example here is when $S$ is a subgroup with deleted $e$ . Then given $|S|=k$ , we get $|Sa\cap S|=k-1$ for all $a\in S$ . Also, we can take $S=\{a,a^{-1}\}$ for every $a\in G\setminus\{e\}$ . Then we have $Sa\setminus\{e\}=\{a^2\}$ , $Sa^{-1}\setminus\{e\}=\{(a^2)^{-1}\}$ , and our condition follows from $a^2\in S\iff(a^2)^{-1}\in S$ . Motivation. Given an inverse closed subset $S\subseteq G\setminus\{e\}$ , we define the Cayley graph $\Gamma=\text{Cay}(G,S)$ such that $V(\Gamma)=G$ and vertices $a,b$ are adjacent iff $ab^{-1}\in S$ . One may note that the set of neighbors $N_a$ of a vertex $a\in G$ is simply $Sa$ . In particular, $\Gamma$ is $k$ -regular, where $k=|S|$ . The last statement also follows from the fact that Cayley graphs are vertex transitive (i. e. given vertices $a,b$ , there is a graph automorphism $\varphi$ such that $\varphi(a)=b$ ). Now, we are interested in when a Cayley graph $\Gamma$ is locally $m$ -regular, i. e. its induced subgraph $<N_a>$ is $m$ -regular for every vertex $a\in G$ . The vertex transitivity allows to restrict ourselves to the case $a=e$ . Then we have $N_a=N_e=Se=S$ . Thus for all $a,b\in S$ , we want the sets $Sa\cap S$ and $Sb\cap S$ have the same cardinality $m$ . So, my question is about properties of $S$ that are necessary or sufficient for this last condition.","['group-theory', 'finite-groups']"
4841759,How to fold a4 paper for 30 degree angle?,"I am trying to make a $30$ degree angle by folding an $A4$ paper, but I got nothing. It seems to be easy but I don't have a clue. I can make $A=45,B=67.5,C=67.5$ triangle or make $45 , \frac {45}{2} , \frac{45}{4}, \cdots$ by folding a right angle. Is there any way to make a triangle with $30,60 ,90 $ degree by folding an $A4$ paper? Remark: I search Google but there is almost nothing clear. Remark2: I saw how to make an equilateral triangle, and then fold any angles to half is the answer. But my question is concerned with not using equilateral triangle.","['euclidean-geometry', 'geometry']"
4841788,"Without using differential forms, can we unify or organize all the various multivariable integrals: multiple, line, flux, etc.?","In single dimension calculus, there is one and only one obvious way of integrating: over an interval.  Of course, there are multiple ways of defining the integral (Riemann sum, Darboux integral, Lebesgue integral), but, where defined, these definitions agree. Once we move from $\mathbb R$ to $\mathbb R^n$ , there seems to be a plethora of types of integrals, with most sources making no attempt to unify or organize them: In this section we will continue looking at line integrals and define the second kind of line integral we’ll be looking at... In this section we will define the third type of line integrals we’ll be looking at: line integrals of vector fields. The Area under a Curve and its Many Generalizations ... The method of doing this used is generalized to define a wide variety of integrals ... Is there a way to unify or at least organize all these types of integrals? My attempt to do so is below.  Is it correct? What revisions does it need? Update: My goal is to do this in a simple way, without invoking the complexity and abstraction of differential forms. For $f: \mathbb R \to \mathbb R^m$ , the one obvious way to integrate is over an interval of $\mathbb R$ .  When instead $f$ takes an input in $\mathbb R^n$ , we gain one, and in some cases two, new ways to integrate: Region (""Multiple"") Integrals For multiple dimensions, with $f: \mathbb R^n \to \mathbb R^m$ , we can take the integral over any ""nice"" $k \leq n$ dimensional region of $\mathbb R^n$ .  We can do this for regions of any dimension $k \leq n$ by partitioning it into very small subregions.  However, for $k > 1$ , these subregions are not defined by two endpoints such that we can define $\Delta x_i$ .  Rather, we need to take the size or region measure (hypervolume, volume, area, etc.) appropriate for $k$ (irrespective of $n$ ) of each subregion. This partition is, unlike intervals, unordered and unoriented, and the size of each subregion is always positive: $$\sum_{r_{i, j,...} \in R}f(\xi_{i,j,...})\cdot \|r_{i,j,...}\|.$$ The integral is the limit of such as $\max_{i,j,...} \|r_{i,j,...}\| \to 0$ (provided it exists).  (And the fact that the partition unordered and unoriented, with each subregion having positive size, is the reason why the Change of Variables Theorem uses the absolute value of the determinant and not the determinant itself.) By Fubini's theorem, this limit is (under reasonable conditions) equal to an iterated integral, and so these are often called multiple integrals .  However, it is important to emphasize that there is nothing ""multiple"" in the definition of multiple integrals: They are limits of single Riemann sums over partitions of regions with dimension $> 1$ . This definition therefore includes volume integrals , area integrals , line integrals over scalar fields ; all are simply region integrals, defined the same way, with the region having different dimensions. Vector Field Integrals: Line (dim = 1) and Flux (codim = 1) Furthermore, if $n = m$ , then $f$ defines a vector field , and we gain the ability to take the dot product $f(x) \cdot x$ .  This gives us two new ways to take integrals: line integrals and flux integrals . If the region is a curve (i.e. $k = 1$ ), we can approximate each subregion $r_i$ by a vector $s_i \in \mathbb R^n$ and take the Riemann sum of $$\sum f(\xi_i) \cdot s_i.$$ And if the region is a surface (i.e. $k = n - 1$ ), we can construct a flux integral by approximating each $r_i$ by an $k$ dimensional hyperplane $p_i$ and letting $n_i$ be the unique vector that is normal to $p_i$ , has magnitude equal to the ""size"" (volume, area, etc.) of $p_i$ , and has a sign determined by convention, giving the Riemann sum of $$\sum f(\xi_i) \cdot n_i.$$","['integration', 'real-analysis', 'multivariable-calculus', 'calculus', 'definition']"
4841830,Method of Moment and MLE of Bernoulli,"Given n observations $X_1, X_2, \dots, X_n$ from a random sample with Bernoulli probability function $$
\operatorname{Pr}(X=k) = p^k(1-p)^{1-k}, \text{ for }k=0,1.
$$ Denote the method of moment estimator of $p$ as $\tilde{p}$ and the MLE estimator of $p$ as $\hat{p}$ Here is what I got so far. For MOM, $$
\operatorname{E}(X) = \mu_{\text{sample}} 
\;\longrightarrow\; \tilde{p} = \dfrac{\sum_{i=1}^n X_i}{n}
$$ For MLE, $$
l(\hat{p};x) 
= \sum_{i=1}^n X_i(\log\hat{p}) 
+ \sum_{i=1}^n (1 - X_i)(\log(1-p)), 
$$ take the derivative and set the equation to zero gave me $\hat{p} = \dfrac{\sum_{i=1}^n X_i}{n}$ It seems like the two method gave me the same expression for the estimators; however, I got a hint for this problem as $\tilde{p}$ and $\hat{p}$ are not the same. So I am confused, but unsure about what I did wrong. Any help would be greatly appreciated!","['statistics', 'probability', 'maximum-likelihood']"
4841865,"Evaluate $\int_{0}^{1} \frac{\operatorname{arsinh}x}{x\sqrt{1-x^4}} \,\mathrm{d}x$","The problem is to show $$\int_{0}^{1} \frac{\operatorname{arsinh}x}{x\sqrt{1-x^4}} \,\mathrm{d}x = \frac{\pi^2}{8}$$ which is a typical integral emerges in many specific value, yet I have no more insight how to prove this one directly (any possible method other than the example below is acceptable). Here is an example when the integral pops out in Legendre-chi function $\chi_2$ , like (1) in this post , we have $$
\int_{0}^{\pi/2} {\arctan(\sin x) \,\mathrm{d}x} = 2\chi_2(\sqrt2-1) = \frac{\pi^2}{8} - \frac{\ln^{2}(\sqrt{2}+1)}{2}
$$ which is a very typical integral, like in this post , from where we already know that $$
\int_{0}^{\pi/2} {\arctan(\sin x) \,\mathrm{d}x} = \int_{0}^{1} {\frac{\arctan x}{\sqrt{1-x^2}} \,\mathrm{d}x} = \int_{0}^{1} {\frac{\operatorname{arsinh}x}{x\sqrt{1+x^{2}}} \,\mathrm{d}x}
$$ or $$
\int_{0}^{\pi/4} {\arcsin(\tan x) \,\mathrm{d}x} = \frac{\pi^2}{8} - \int_{0}^{\pi/2} {\arctan(\sin x) \,\mathrm{d}x} = \frac{\ln^{2}(\sqrt{2}+1)}{2}
$$ Now, for $a<1$ consider the following parameterization $$
I(a) = \int_{0}^{\pi/4} {\arcsin(a\tan x) \,\mathrm{d}x}
$$ let $u=\tan x$ and $y^2=(1-a^2u^2)/(1+u^2)$ in its derivative $$
\begin{aligned}
I'(a) &= \int_{0}^{\pi/4} {\frac{\tan x}{\sqrt{1-a^2\tan^2x}} \,\mathrm{d}x} = \int_{0}^{1} {\frac{u}{(1+u^2)\sqrt{1-a^2u^2}} \,\mathrm{d}u} \\
&= \frac1{\sqrt{1+a^2}}\int_{\sqrt{(1-a^2)/2}}^{1} {\frac{\mathrm{d}y}{\sqrt{a^2+y^2}}} = \frac1{\sqrt{1+a^2}}\operatorname{arsinh}\left(\frac{y}{a}\right)\biggr|_{y=\sqrt{(1-a^2)/2}}^{1} \\
&= \frac{1}{\sqrt{1+a^{2}}} \operatorname{arsinh}\frac1{a} - \frac{1}{\sqrt{1+a^{2}}} \operatorname{arsinh}\sqrt{\frac{1-a^2}{2a^2}}
\end{aligned}
$$ Integrtation by parts gives $$
\begin{aligned}
\int_{0}^{\pi/4} {\arcsin(\tan x) \,\mathrm{d}x} 
&= \int_{0}^{1} \frac{1}{\sqrt{1+a^{2}}} \operatorname{arsinh}\frac1{a} \,\mathrm{d}x - \int_{0}^{1} \frac{1}{\sqrt{1+a^{2}}} \operatorname{arsinh}\sqrt{\frac{1-a^2}{2a^2}} \,\mathrm{d}x \\
&= \operatorname{arsinh}^2(1) + \int_{0}^{1} {\frac{\operatorname{arsinh}a}{a\sqrt{1+a^{2}}}\,\mathrm{d}a} - \int_{0}^{1} \frac{\operatorname{arsinh}a}{a\sqrt{1-a^4}} \,\mathrm{d}a
\end{aligned}
$$ Combining all the special case of $\chi_2(\sqrt2-1)$ from above, we will find the required integral. And there are other equivalent form of required integral, for example $$
\int_{0}^{1} \frac{\operatorname{arsinh}x}{x\sqrt{1-x^4}} \,\mathrm{d}x = \frac1{2} \int_{0}^{1} \frac{1}{\sqrt{1+x^2}} \operatorname{arcosh}\frac1{x^2} \,\mathrm{d}x = \frac1{4} \int_{0}^{1} \frac{1}{\sqrt{x(1+x)}} \operatorname{arcosh}\frac1{x} \,\mathrm{d}x
$$ Yet, I still can not find any convenience for further calculation. Thanks in advance for any help.","['integration', 'improper-integrals', 'calculus', 'definite-integrals']"
4841894,Does $A \subseteq B \implies f^{-1}(A) \subseteq f^{-1}(B)$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 6 months ago . The community reviewed whether to reopen this question 6 months ago and left it closed: Original close reason(s) were not resolved Improve this question If it is the case that $ A \subseteq B,$ then is it always true that the inverse images obey $f^{-1}(A) \subseteq f^{-1}(B),$ where $f$ is a function that is not necessarily injective, and has a codomain containing $B$ ? The reason I am asking is due to caution that the inverse image of a set $S$ can be larger than the set $S$ . EDIT My attempt based on the comment by @jjagmath. Let $f:X \to Y$ and $A \subseteq B \subseteq Y$ . The definition of inverse image $f^{-1}$ applied to $A$ is $ f^{-1}(A) = \{x \in X: f(x) \in A\}$ . Similarly for $B, f^{-1}(B) = \{x \in X: f(x) \in B\}$ . Since $A \subseteq B$ , then $f(x) \in A \implies f(x) \in B$ . Thus, $ f^{-1}(A) \subseteq f^{-1}(B) \; \square$ .","['elementary-set-theory', 'functions']"
4841905,"""almost everywhere defined"" ordinary differential equations","Let $u(t)$ be a measurable function on time interval $[0,T]$ , consider the ""almost everywhere defined"" ordinary differential equation: $$\left\{\begin{matrix} \frac{dx(t)}{dt}=b(t,x(t),u(t)) \;\;\;a.e.t\in[0,T]\\
x(0)=x_0\end{matrix}\right.$$ where $b(t,x,u)$ is a measurable function satisfying: (1) $|b(t,0,u)|<L$ holds for some $L$ and every $(t,u)\in[0,T]\times R$ . (2) $b(t,x,u)$ is Lipschitz continuous in variables $x$ and $u$ . Some literature that I am working with claim that the solution to the above ODE exists and is unique. I have a few questions though: (1) Does the solution to the above ODE equivalent to ""weak solution""? That is $$\int_0^T x(t)\phi'(t)dt=-\int_0^T b(t,x(t),u(t))\phi(t)dt$$ holds for every $\phi\in C_0^\infty([0,T])$ ? (2) Does the above ODE equivalent to the integral equation $$x(t)=x(0)+\int_0^tb(s,x(s),u(s))d s$$ (3) Classical theory of ODEs require a continuous $f(t,x)$ to ensure the existence and uniqueness to $dx(t)=f(t,x)dt$ with initial value. Here we don't have any continuity assumed on the time variable, why are the existence and uniqueness still valid? Please help by leading me to related literatures, thanks.","['ordinary-differential-equations', 'real-analysis', 'measurable-functions', 'functional-analysis', 'partial-differential-equations']"
4841917,Why must $n-\lfloor\frac n2\rfloor+\lfloor\frac n3\rfloor-\dotsb$ grow like $n\ln2$? [duplicate],"This question already has answers here : $\lim_{n\to\infty}\frac{n -\big\lfloor\frac{n}{2}\big\rfloor+\big\lfloor\frac{n}{3}\big\rfloor-\dots}{n}$, a Brilliant problem (3 answers) Closed 6 months ago . Let $$a(n):=n-\left\lfloor\frac n2\right\rfloor+\left\lfloor\frac n3\right\rfloor-\left\lfloor\frac n4\right\rfloor+\left\lfloor\frac n5\right\rfloor-\dotsb.$$ Note that this is a finite sum. Naïvely, $$a(n)\approx n-\frac n2+\frac n3-\frac n4+\frac n5-\dotsb=n\ln2,$$ and so in particular \begin{align}\lim_{n\to\infty}\frac{a(n)}n=\ln2.\tag{$*$}\end{align} However, I'm having trouble proving this formally. In particular, the difficulty lies in showing that the error goes to zero. The OEIS page for this sequence, https://oeis.org/A059851 , claims that $(*)$ is correct, but I have been unable to prove it. Can someone find a rigorous proof?","['ceiling-and-floor-functions', 'divisor-sum', 'combinatorics', 'discrete-mathematics', 'sequences-and-series']"
4841980,Show that $\int_{0}^{\infty} P(Y \geq y)\ dy = \int_{0}^{\infty} P(Y \gt y)\ dy.$,"Let $Y$ be a non-negative random variable in a probability measure space $(\Omega, \mathcal F, \mathbb P).$ Then it is well known that $$\mathrm E[Y] = \int_{0}^{\infty} \mathbb P (Y \geq y)\ dy.$$ Can we conclude that $$\int_{0}^{\infty} \mathbb P (Y \geq y)\ dy = \int_{0}^{\infty} \mathbb P (Y \gt y)\ dy.$$ For that we need to show that $$\mu \left (\left \{y \in \mathbb R_{\geq 0}\ \big |\ \mathbb P (Y = y) \gt 0 \right \} \right ) = 0$$ where $\mu$ is the Lebesgue measure on $\mathbb R.$ If $Y$ takes only countably many values i.e. if $Y$ is a discrete random variable or if $Y$ is a continuous random variable having a density function then this is clearly true. But is it true for arbitrary random variables? Any help would be greatly appreciated. Thanks for your time.","['measure-theory', 'probability-theory']"
4842002,Constructing the orthocenter of a cyclic pentagon,"I’d like to construct the following thing in Georgebra: A non-regular cyclic pentagon $ABCDE$ such that the perpendiculars from $A$ to $CD$ , $B$ to $DE$ ... all concur in one point $H$ . I managed to prove that if $H$ exists then the pentagon formed by the midpoints of $AC,BE...$ is cyclic. Here’s how: Let $O$ be the circumcenter of $ABCDE$ and $M$ the midpoint of $OH$ then since rays $AH$ and $AO$ are isogonal in $\triangle ACD$ so $MB_1=ME_1$ where $B_1$ and $E_1$ are the midpoints of $AC$ and $AD$ . Applying this to every point of the pentagon we get $M$ is the circumcenter of the pentagon formed by the midpoints. It turns out the converse is also true.","['geometric-construction', 'geometry', 'math-software']"
4842056,"Find all functions $f: \mathbb{R} \rightarrow \mathbb{R} $ such that $f(xf(y) + f(x+y)) = y(f(x)+1)+f(x) \forall x, y \in \mathbb{R}$","$
\text{Find all functions } f: \mathbb{R} \rightarrow \mathbb{R} \text{ such that } f(xf(y) + f(x+y)) = y(f(x)+1)+f(x) \text{ for every } x, y \in \mathbb{R}
$ Let $x_1, x_2 \in \mathbb{R}$ such that $f(x_1) = f(x_2)$ . By plugging $x = 0$ , we get $f(f(y)) = y(f(0) + 1) + f(0)$ . Since $f(f(x_1)) = f(f(x_2))$ , we have $x_1(f(0) + 1) + f(0) = x_2(f(0) + 1) + f(0)$ . Therefore, $f$ is injective for $f(0) \neq -1$ . For the case where $f(0)$ is different from $-1$ : If we let $y=0$ , and $f(0)=c$ , then we get $ f(cx+f(x))=f(x). $ Since f is injective in this case, $cx+f(x)=x$ , or $f(x)=(1-c)x$ . Plugging back into the original function, we get $ (1-c)^2(xy+x+y)=y(x-xc+1)+x-xc. $ We can then compare the coefficients of $y$ , giving $ (1-c)^2(1+x)=x-xc+1. $ After some simplification we get $(c^2-c)x=2c-c^2$ . This is only true for all $x$ when $c=0$ . Thus, $f(x)=x$ is the only solution. $
\text{For } f(0) \neq -1, \text{ } f \text{ is bijective. By plugging } y=0 \text{ and } f(0)=c, \text{ we find that } f(x)=x. \text{ I got stuck at the case where } f(0)=-1? 
$ I think $f(x)=-1$ but I don’t know how to prove it. Are there any other solutions?","['contest-math', 'functions']"
4842088,"Except for $1$, are there any perfect powers in the sequence $1,21,321,4321,\cdots$?","Except for $1$ , are there any perfect powers in the sequence $1,21,321,4321,\cdots$ ? This is sequence OEIS A000422 , the concatenation of positive integers from $n$ down to $1$ . If there is any perfect square in the sequence, $n$ must be equal to $1\pmod{3}$ , as when $n\equiv1\pmod{3}$ , then the number is equal to $1\pmod{3}$ ,which can be a perfect square and also possibly a perfect cube. If $n\cdots4321$ is divisible by $9$ , then $n$ is equal to $8\pmod{9}$ in order to be a perfect square. In case $n\cdots4321$ is divisible by $27$ , then $n$ is equal to $26\pmod{27}$ in order to be a perfect cube. I don’t know about higher odd prime perfect powers. The only sure thing is that any number(except $1$ ) in OEIS A000422 is never a perfect 5th power, due to the fact that $21$ isn’t a fifth power residue $\pmod{100}$ . Also, the only possibilities that $n\cdots4321$ is a perfect power is that either $n\equiv1\pmod{3}$ , $n\equiv8\pmod{9}$ , or $n\equiv26\pmod{27}$ . I tried to used Pari GP to check if there is any perfect power in the sequence other than $1$ , but so far, I didn’t find one.","['number-theory', 'square-numbers', 'perfect-powers', 'sequences-and-series']"
4842118,The sum of unit vectors from a point in a polyhedron to its $n$ vertices has magnitude $< n-2$,"There is a polyhedron with $n$ vertices and a point $O$ inside it. Let $e_i$ be a unit vector directed from the point $O$ to the $i$ -th
vertex of the polyhedron. Prove that $$ |e_1+\ldots+e_n|<n-2. $$ This is an exercise from a book in Russian How to solve non-standard problems (page 11).
Also, I have encountered this problem on several forums, but nowhere have I seen a solution to this problem. It seems to me that a similar but somewhat simpler problem for the plane can be formulated as follows: Let us give an $n$ -gon and the point $O$ inside of it. Let $e_i$ be a
unit vector directed from $O$ to the $i$ -th vertex of the $n$ -gon.
Prove that $$ |e_1+\ldots+e_n|<n-2. $$ My hope was that these problems could be solved with the help of the statements
from the ""About four points lying on a sphere"" post. In the plane, for example, the reasoning could be as follows.
Let $e_i=\vec{OA_i}$ , $i=1,\ldots,n$ .
There are three points $A,B,C\in\{A_1,\ldots,A_n\}$ such that $O$ lies inside triangle $\triangle ABC$ .
We can assume without loss of generality that $A=A_1$ , $B=B_1$ , $C=C_1$ .
Then we get $$
|e_1+\ldots+e_n|\leq|e_1+e_2+e_3|+(|e_4|+\ldots+|e_n|)<1+(n-3)=n-2.
$$ Understandably similar reasoning in 3D. All is well and wonderful. But why do the above three vertices exist? And in the case of 3D, four
vertices? How to justify this? That's actually my question. However, perhaps there is another solution to these problems using
other considerations? I will gladly accept such answers too. Adding. (2024/01/11) It seems to me that whether a polyhedron ( $n$ -gon) is convex or not does not matter.
I have posted a picture that clarifies this point.
We know that the vertices of the $n$ -gon are located on the rays with origin at $O$ .
There are many such $n$ -gons, but the sum of the vectors depends only on the direction of the rays.",['geometry']
4842123,Show that $\dfrac{1}{2 \pi i} \int_{c- i \infty}^{c+ i \infty} F(s) G(w-s) ds = \int_0^{\infty} f(x) g(x) x^{w-1} dx.$,"Eq. 2.15.10 in Titchmarsh's book The Theory of the Riemann Zeta-Function comes with no proof: Let f(x) and $F(s)$ be ralated by $$F(s) = \int_0^{\infty} f(x) x^{s-1} dx, \ \ \ f(x) = \dfrac{1}{2 \pi i} \int_{\sigma- i \infty}^{\sigma+ i \infty} F(s) x^{-s} ds.$$ Then we have, subject to appropriate conditions, $$\dfrac{1}{2 \pi i} \int_{c- i \infty}^{c+ i \infty} F(s) G(w-s) ds = \int_0^{\infty} f(x) g(x) x^{w-1} dx.$$ First thing to do is to compute the integral of $f(x)g(x)x^{w-1}$ and see what happens but I failed to reach a result, especially that I very much am hoping for a rigorous proof. Then searching on web and some books that I had lead me nowhere. The two questions I have: How the equality is derived and what are the conditions are considered ""appropriate""? (perhaps the author is thinking of determining the $c$ based on $\sigma_f$ and $\sigma_g$ (?)) Note for Bounty: The post below from ""Daigaku no Baku"" answers the question partially - to get a complete answer I need to justify the interchange of integrals used in ""Daigaku no Baku"" 's answer and validity of the real interval for $\max (\alpha_1, 1- \beta_2 ) < c < \max ( \beta_1, 1- \alpha_2)$ mentioned in  en.wikipedia.org/wiki/Mellin_transform in the section concerning Parseval's theorem (which is my second question in OP).","['complex-analysis', 'proof-explanation', 'mellin-transform', 'improper-integrals']"
4842153,Substitution of Variable in Limits,So Here's the Question $$F(x)=\frac{x^2\sin(\frac1x)+2x}{(1+x)^\frac1x -e} \quad \text{then}\quad \lim_{x \to 0}{F(x)=}$$ I tried this Question by Substituting $x = \frac1y$ Where $\lim_{y \to \infty}$ But still in the outcome there are $2$ forms coming out which are $\frac00$ and $\frac \infty\infty$ . I also came across an alternate solution which is by substituting $x$ as $\frac1x$ and no limits specified for $x$ . Can anyone explain the logic behind the alternate solution and even how to proceed further with my approach. Thanks :),"['limits', 'calculus']"
4842170,A question about the differential equation $x^2 y' -2xy = 3y^4$,I try to solve the differential equation $$x^2 y' -2xy = 3y^4$$ by letting $$y = ux$$ And I get $$(ux)' = 2u + 3u^4 x^2$$ $$\Rightarrow u'x = u + 3u^4 x^2$$ But I don't know how to continue. Can someone give me some hint on how to continue or on another method to solve this? Thanks!,['ordinary-differential-equations']
4842203,On solving the differential equation $y''=y'+y+x$,"Now at some point, you might have come across the differential equation $$y'=y+x$$ which has the easily verifiable solution $$y(x)=ce^x-x-1$$ however, I wanted to try solving $$y''=y'+y+x$$ which while it would be more difficult, I thought that I might be able to solve it. Here is my attempt at doing so: This differential equation looked like the general solution might be a sum of the complementary solution and the particular solution. To find the complementary solution, we need to solve $$y''-y'-y=0$$ This is how this is done: Assume that a solution will be proportional to $e^{\lambda x}$ for some $\lambda$ . Substituting that in gets us $$\lambda^2e^{\lambda x}-\lambda e^{\lambda x}-e^{\lambda x}=0$$ and dividing all terms by $e^{\lambda x}$ gets us $$\lambda^2-\lambda-1=0$$ Wait a second. This looks familiar. The solution to this quadratic is $$\lambda=\phi,\overline\phi$$ where $\overline\phi$ represents the conjugate of the golden ratio $\phi$ . So the complementary function is $$y(x)=c_1e^{\phi x}+c_2e^{\overline\phi x}$$ Now to find the particular solution to the diff. eq., which is where I'm currently stuck, as I do not know how to do so. So my question is: How do I solve the diff. eq. $y''=y'+y+x$ ?","['calculus', 'ordinary-differential-equations']"
4842234,Differentiation: what rule am I missing here?,"I am trying to differentiate the below with respect to c: $\left(\frac{a-b}{c-b}\right)^d$ , however I get an answer different to what Mathematica (and other sources) is telling me, which is $-\frac{(a-b)\left(\frac{a-b}{c-b}\right)^{d-1}d}{(c-b)^2}$ The way I'm approaching it is: $\left(\frac{a-b}{c-b}\right)^d = \frac{(a-b)^d}{(c-b)^d} = (a-b)^d*(c-b)^{-d}$ ,
then I treat $(a-b)^d$ as a constant and differentiate the $(c-b)^{-d}$ term which gives me: $(a-b)^d * -d(c-b)^{-d-1} = \frac{-d(a-b)^d}{(c-b)^{d-1}}$ . I can't work out what rule or a method I'm missing here unfortunately so I would be very grateful for any advice or pointers on that.
Thanks for reading!",['derivatives']
4842235,How to prove an ideal is not principal?,"I've just asked this question in another post, but I'm reposting it here since it wasn't the only question there and it was suggested to me to  ask just one question per post. I would like to know if there is a generic method to prove or disprove that an ideal is principal. Before closing the question, someone commented that there isn't a single way to do it, so if that's the case I would like some review on two exercises I was given, where the ideals are in $\Bbb Z[X]$ . $(5, X^2 +3)$ : I believe this ideal to not be principal, as if it were principal then the generator would need to be of degree $0$ to generate $5$ , but $5$ is irreducible so it can only be generated by $1$ or $5$ . The number $5$ cannot be a generator since $X^2 +3$ is in the ideal, but it also cannot be $1$ as the ideal is different from $\Bbb Z[X]$ , for example $X$ is not in it, $(X^2 +1, X+2)$ : I believe this ideal to also not be principal. Those polynomials are irreducible and not associate, so if the ideal were to be generated by one element it would need to be a unit. But the remainder of division of $X^2+1$ by $X+2$ is $5$ , so every constant in this ideal is multiple of $5$ and so the ideal is different from $\Bbb Z[X]$ . Are there some mistakes?","['ring-theory', 'abstract-algebra', 'ideals']"
4842247,What is the number of possibilities to interpret a litteral number string? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 months ago . Improve this question Me and some friends came up with the following problem : Given a string of numbers of length $n$ , what is the number of possibilities to interpret this string as digits (while not accounting for pelling problems) ? For example with ""one three two one"", $n = 4$ , we could interpret it as "" $1321$ "", or even "" $311$ "" (one three and two ones). To simplify a bit, some additionnal rules are that two consecutive numbers cannot be the same, and that a digit can only demultiply the one after him, for example ""two three one"" cannot be interpreted as "" $3131$ "" (two times three one).
This might be related to the look and say sequence, we just don't now in what way. Our conjecture is that this number is fibonacci( $n$ ) (with fibonacci( $0$ ) = fibonacci( $1$ ) = $1$ ). This works up to n = 30, but we did not manage to complete a proof $\forall$$n\in$$\mathbb{N}$ . Our idea was to represent the string of length n by a sequence $A_1 A_2 A_3...A_n$ , with $A_i\in \{\{1, 1*\}, \{2, 2*\},\dots\}$ $\forall$$i\gt 1$ , and $A_1\in \{\{1\}, \{2\},\dots\}$ . A number ""star"" would represent a number that got itself demultiplied by the previous one. This representation is probably really bad (and maybe doesn't even work), but at least we can represent all the sequences of length $n$ as the set $A_1\times A_2\times \dots \times A_n$ minus the possibilities where two number ""stars"" are consecutive. After a bit of research on the internet, we didn't manage to find anything related to this, and so here we are. What are your thoughts on this, is our conjecture reasonable ? if yes, how could we prove it ? (We tried by induction, without success...) I also want to apologize if this post is not well-written, doesn't follow the math.stackexchange rules, or if the problem is not well formulated. As second year computer science students, this is totally new to us. Thank you !","['combinatorics', 'discrete-mathematics']"
4842250,Probability of forming an N-symbol cluster in a MxM matrix,"I have the following task: A 7x7 matrix with empty slots in each cell we must put a symbol randomly selected from 1000 symbols 25% of them are the symbol B(250 Bs).
The matrix is filled with symbols from the 1000 symbol pool. What is the probability of forming a 5 or more symbols clusters with Bs? (A cluster is a segment where a symbol has adjacent same symbol. Diagonals are not included.) Here is an example of a cluster: Cluster example Using computer simulations I get a probability around 30% and a probability of exactly 5 symbol cluster around 14%. However, I cant manually reach these numbers with mathematics. My approach was to use the hypergeometric distribution but the numbers I got were not relevant. Because with hypergeometric distribution I get all the possible ways of getting exactly 5 Bs randomly. Then I divide them by all the possible ways to arrange the matrix and I get 0.005196 which is no way near 14%. Also I might have problem in the code itself because it is complex so the numbers I get from the computer simulations may not be relevant. For now I assume they are. I will be glad to hear suggestions how to compute the probability. With computer simulations I also got that all possible ways of forming a 5 symbol cluster in 7x7 grid are 1086 I don't really know if this number matters.","['statistics', 'combinatorics']"
4842310,Isaacs Character Theory exercise 6.4,"Studying character theory of finite groups using Martin Isaacs' book ""Character Theory of Finite Groups"", I came across the following problem (exercise 6.4): Let $G$ be a finite group and let $\mathscr{H}(G) = \{H \subset G \mid \exists \psi \in \operatorname{Irr}(H) \text{ such that } \psi{\uparrow}^G \in \operatorname{Irr}(G)\}$ . Consider $\mathbb{H}(G) = \bigcap \mathscr{H}(G)$ . Prove that, if $G$ is an $M$ -group (that is, if every irreducible character of $G$ is induced by a linear character of a subgroup), then $\mathbb{H}(G)$ is abelian. This is one of those problems where I don't really know how to proceed. I believe the easiest way to go about this is probably to show that every irreducible character of $\mathbb{H}(G)$ is linear. So let $\psi$ be one such character, and write $$\psi{\uparrow}^G = \sum_{i=1}^t a_i\chi_i$$ as a combination of irreducible constituents, where $a_i \in \mathbb{N}^*$ . Computing both sides at $1$ yields $[G: \mathbb{H}(G)]\psi(1) \leq [G: \mathbb{H}(G)] \sum_{i=1}^t a_i$ , where we used the fact that $G$ is an $M$ -group to express each $\chi_i$ as a character induced from a linear character of a subgroup, and used the fact that said subgroup contains $\mathbb{H}(G)$ . This means $\psi(1)$ is bounded by the number of irreducible constituents that appear in the decomposition of $\psi{\uparrow}^G$ . This is pretty much where I get stuck. I think Clifford's Theorem probably plays a role somewhere, but I couldn't extract any meaningful information using it... My main difficulty is that I know next to nothing about the characters of $\mathbb{H}(G)$ besides the property I proved above... Is this a good approach? If so, how do I continue? And, if not, how to tackle this problem? Thanks in advance! PS: Does the characteristic subgroup $\mathbb{H}(G)$ defined above have a name? I couldn't find anything about it elsewhere...","['group-theory', 'abstract-algebra', 'finite-groups', 'characters']"
4842331,Topology Without Tears Exercise $1.1.9$ - Union of infinite rational open intervals,"I'm working through the book Topology without Tears by Sidney A. Morris, and I've come to a roadblock while trying to solve exercise $1.1.9$ , specifically with part iii. My background is primarily in mechanical and software engineering, and am not yet very comfortable with pure mathematics and set theory, but I'm hoping to get there. The problem is stated as:
"" Let $\mathbb{R}$ be the set of all real numbers. Precisely three of the following ten collections of subsets of $\mathbb{R}$ are topologies? Identify these and justify your answer "". It then continues to list 8 different sets that could potentially be topologies in $\mathbb{R}$ . I'm specifically concerned with the third one being listed: $\tau_3$ consists of $\mathbb{R}, \emptyset$ , and every interval $(-r,r)$ , for $r$ any positive rational number. My intuition tells me that this should be a topology. After all, any pair of positive rational numbers $r_1$ and $r_2$ must be such that either $r_1>r_2$ , $r_1<r_2$ , or $r_1=r_2$ . In any case, the intervals $(-r,r)$ formed by $r_1$ and $r_2$ would result in one interval being a subset of the other. Meaning that the union of the two intervals would simply be the larger of the two intervals and thus remain an open set, and the intersection of the two intervals would simply be the smaller of the two intervals and thus remain an open set. However, context clues (following this same logic results in way more than 3 valid topologies), and a broader but very similar question: Ex.1.1 9 from topology without tears suggests that my logic is flawed. Looking through the answer to the question linked above, the explanation for why this was not a topology is as follows: "" $\tau_3$ is not a topology since for some irrational $a$ we have $$\bigcup_{n=1}^\infty(-r_n,r_n) = (-a,a)\notin \tau_3$$ where $\lim_{n\to\infty}r_n = a$ ."" I don't follow this explanation. Why would an infinite union of these rational open intervals become an irrational open interval? Why would it not just be some infinitely large rational open interval? I feel like this alludes to some key piece of information that I'm missing that would likely be very helpful for me throughout the rest of this book.","['elementary-set-theory', 'real-numbers', 'general-topology', 'real-analysis']"
4842353,Lagrangian function (where does it come from?),"Where does the Lagrangian function come from? I know about gradient vectors and $\lambda$ : $$ \nabla f = \lambda \nabla g $$ But from there what steps were done to get the Lagrangian function? (This one): $$ L\left(x,\ y,\ \lambda\right)=f\left(x,y\right)-\lambda\left(g\left(x,y\right)-c\right) $$ Why can the constant $c$ be included here? I am a high school student trying to explain it, and it most probably goes way over my head, so I am mostly just looking for a reference proof I can point towards in my bibliography. I can't find one; thank you for any help in that regards.","['calculus', 'lagrange-multiplier']"
4842372,Residue of $\dfrac{\mathbb{e}^z \sin(\frac{1}{z})}{z}$ at $z=0$,"I know that $\dfrac{\mathbb{e}^z \sin(\frac{1}{z})}{z}$ has an essential singularity at $z=0$ . Now I don't know how to find the residue of a function with an essential singularity. I tried: \begin{align}
\dfrac{\mathbb{e}^z \sin(\frac{1}{z})}{z}&=(1+z+\frac{z^2}{2!}+\frac{z^3}{3!}+\frac{z^4}{4!}+...)\cdot (\frac{1}{z^2}-\frac{1}{3!}\cdot \frac{1}{z^4}+\frac{1}{5!}\cdot \frac{1}{z^6}-\frac{1}{7!}\cdot \frac{1}{z^8}+...)=\\\\ 
&=...+\frac{1}{z}\cdot(\sum_{k=0}^{\infty} \frac{(-1)^k}{((2k+1)!)^2})+...
\end{align} So my residue would be $\sum_{k=0}^{\infty} \frac{(-1)^k}{((2k+1)!)^2}$ . This series obviously converges, but I can't calculate it's value. So I think that there is an easier way to find the residue?","['complex-analysis', 'residue-calculus']"
4842433,"Question about the proof for ""every field is a euclidean domain.""","$\color{black}{Background:}$ $\textbf{Definition:}$ An integral domain $R$ is a Euclidean domain if there is a function $\delta$ from the nonzero elements of $R$ to the nonnegative integers with these properties; $(i)$ If $a$ and $b$ are nonzero elements of $R,$ then $\delta(a)\leq \delta(ab).$ $(ii)$ If $a,b\in R$ and $b\neq 0_R,$ then there exist $q,r\in R$ such that $a=bq+r$ and either $r=0_R$ or $\delta(r) < \delta(b).$ $\textbf{Exercise 14:}$ Let $R$ be a field.  Prove that $R$ is a Euclidean domain with the function $\delta$ given by $\delta(a)=0$ for each nonzero $a\in R.$ $\textbf{Exercise 29:}$ Let $R$ be a Euclidean domain.  If the function $\delta$ is a constant function, prove that $R$ is a field. $\color{black}{Questions:}$ I have a question about exercise 14 above.  I have seen in some proof of exercise 14 that the $\delta(a)=1$ instead of $\delta(a)=0$ . I am just wondering if it matters.  If $\delta(a)=1$ , the proof doesn't require that I need to worry about $r\neq 0.$ Also, in light of exercise 29, it seems that it really doesn't matter. Thank you in advance","['euclidean-domain', 'ring-theory', 'abstract-algebra']"
4842439,Mass spring damped system,"I have the following problem: A $8\,\mathrm{kg}$ mass stretches a spring $1.96\,\mathrm{m}$ . At $t=0$ , an external force $f(t)=2\cos(2t)$ is applied to the system. The damping constant is $3\,\mathrm{N}\cdot\mathrm{s}/\mathrm{m}$ . Find its position $u(t)$ . I know how to solve the ODE, but my question is: may I assume $u(0)=u'(0)=0$ ? If so, why? Or does the general solution depends on the constants?","['initial-value-problems', 'ordinary-differential-equations']"
4842440,Is defining measures as functionals ever insufficiently general in practice?,"Edit: Crossposted to Math Overflow The way I learned measures was as set functions on a $\sigma$ -algebra with certain properties. However, I'm aware of another viewpoint, used for example in Bourbaki's Integration , that defines measures on locally compact Hausdorff spaces to be functionals on the space of compactly-supported continuous functions. This is obviously less general, but I suppose the argument is that most measures that arise in practice are Radon measures on locally compact Hausdorff spaces. The counterpoint that I keep seeing cited is that this is insufficient for probability theory. In Real and Complex Analysis (p. 35), Rudin mentions the Wiener measure as an example of a measure that doesn't fall into this framework. However, I have also seen some sources say that the needs of probability theory can be met by extending the theory to general Hausdorff spaces. I've heard this as Bourbaki's rationale for adding a 9th chapter to Integration about measures on Hausdorff spaces (see for instance p. 14 of this review ), and in the introduction to Radon Measures on Arbitrary Topological Spaces and Cylindrical Measures , Schwartz says The fundamental defect of the usual theory of Radon measures is that the spaces of functions which occur in the theory of probability are not locally compact. The present theory of Radon measures on arbitrary (for the most part) Hausdorff topological spaces combines all the advantages of both methods [i.e., in terms of sets and in terms of functionals]... My question is: does extending the functional approach to Hausdorff spaces actually make it general enough for the needs of probability theory? If not, are there examples of probabilistic concepts that are less easy (or impossible) to describe from this viewpoint? (I would also be happy to hear about fields other than probability for which this viewpoint does not suffice, if they exist.)","['measure-theory', 'probability-theory']"
4842461,Why do Bell Polynomial coefficients show up here?,"The multinomial theorem allows us to expand expressions of the form ${\left( {{x_1} + {x_2} + {x_3} + {x_4} + ...} \right)^n}$ . I am interested in the coefficients when expanding ${\left( {\sum\limits_{k = 1}^\infty  {{x_k}} } \right)^n}$ in a particular way which I'll demonstrate below in the case where $n = 5$ . In the following sums, the indices run through positive integers such that ${k_i} \ne {k_j}$ for all $i,j$ . $${\left( {{x_1} + {x_2} + {x_3} + ...} \right)^5} = {a_1}\sum\limits_{k \geqslant 1} {x_k^5}  + {a_2}\sum\limits_{\scriptstyle {k_1},{k_2} \geqslant 1 \atop 
  \scriptstyle {k_1} \ne {k_2}}  {x_{{k_1}}^4{x_{{k_2}}}}  + {a_3}\sum\limits_{\scriptstyle {k_1},{k_2} \geqslant 1 \atop 
  \scriptstyle {k_1} \ne {k_2}}  {x_{{k_1}}^3x_{{k_2}}^2}  + {a_4}\sum\limits_{\scriptstyle {k_i} \ne {k_j} \atop 
  \scriptstyle {k_1},{k_2},{k_3} \geqslant 1}  {x_{{k_1}}^3{x_{{k_2}}}{x_{{k_3}}}}  + {a_5}\sum\limits_{\scriptstyle {k_i} \ne {k_j} \atop 
  \scriptstyle {k_1},{k_2},{k_3} \geqslant 1}  {x_{{k_1}}^2x_{{k_2}}^2{x_{{k_3}}}}  + {a_6}\sum\limits_{\scriptstyle {k_i} \ne {k_j} \atop 
  \scriptstyle {k_1},...,{k_4} \geqslant 1}  {x_{{k_1}}^2{x_{{k_2}}}{x_{{k_3}}}{x_{{k_4}}}}  + {a_7}\sum\limits_{\scriptstyle {k_i} \ne {k_j} \atop 
  \scriptstyle {k_1},...,{k_4} \geqslant 1}  {{x_{{k_1}}}{x_{{k_2}}}{x_{{k_3}}}{x_{{k_4}}}{x_{{k_5}}}} $$ When calculating the coefficients ${a_1},...,{a_7}$ , the multinomial coefficients are involved. However, since the summation goes through certain combinations of indices multiple times, a correction factor will need to be added to account for the duplicates produced by some of these sums. For example, in the 6th sum, the term $x_{{k_1}}^2{x_{{k_2}}}{x_{{k_3}}}{x_{{k_4}}}$ is the same under any permutation of ${k_1},{k_2},{k_3}$ , so ${a_6} = \frac{1}{{3!}}\left( \begin{array}{c}5\\2,1,1,1\end{array} \right) = 10$ . The rest of the coefficients can be found similarly: $${a_1} = \left( \begin{array}{l}5\\5\end{array} \right) = 1,\quad {a_2} = \left( \begin{array}{c}5\\4,1\end{array} \right) = 5\quad {a_3} = \left( \begin{array}{c}5\\2,3\end{array} \right) = 10\quad {a_4} = \frac{1}{{2!}}\left( \begin{array}{c}5\\3,1,1\end{array} \right) = 10\quad {a_5} = \frac{1}{{2!}}\left( \begin{array}{c}5\\2,2\end{array} \right) = 15\quad {a_6} = \frac{1}{{3!}}\left( \begin{array}{c}5\\2,1,1,1\end{array} \right) = 10\quad {a_7} = \frac{1}{{5!}}\left( \begin{array}{c}5\\1,1,1,1\end{array} \right) = 1$$ The coefficients 1, 5, 10, 10, 15, 10, 1 are exactly the coefficients of the 5th complete Bell polynomial: $${B_n}\left( {{x_1},...,{x_5}} \right) = x_1^5 + 10x_1^3{x_2} + 10{x_3}x_1^2 + 15x_2^2{x_1} + 5{x_1}{x_4} + 10{x_2}{x_3} + {x_5}$$ For each $n$ I've tried, the ${a_1},{a_2},{a_3},...$ end up being coefficients of the nth complete Bell polynomial. I am interested in proving (if it is in fact true): For all $n$ , not just $n=5$ , that these coefficients ${a_1},{a_2},,{a_3}...$ are the same as the coefficients of the nth complete Bell polynomial. I would appreciate if someone can provide me with a proof or additional information about this apparent relationship. I prefer a proof with generating functions but I will be happy with any proof. My unsuccessful attempt: I have tried proving this relation using generating functions but with no success. In particular, I wrote a partition based formula: $${\left( {{x_1}t + {x_2}{t^2} + {x_3}{t^3} + ...} \right)^n} = \sum\limits_{\scriptstyle{u_i} \ne {u_j}\atop\scriptstyle{P_{\ell,n} }:{k_1} + ... + {k_n} = n} {\left( {{a_{\ell,n} }\prod\limits_{r = 1}^n {x_{{u_r}}^{{k_r}}{t^{{k_r}{u_r}}}} } \right)} $$ where ${P_{\ell,n} }$ is the ${\ell ^{{\rm{th}}}}$ partition of $n$ , so that the coefficients ${a_{\ell,n} }$ are based on the chosen order of the partitions. The generating function for the incomplete Bell polynomials is $$\sum\limits_{n = 0}^\infty  {\frac{1}{{n!}}{{\left( {\sum\limits_{j = 1}^\infty  {{x_j}\frac{{{t^j}}}{{j!}}} } \right)}^n}}  = \sum\limits_{n = 0}^\infty  {\frac{{{t^n}}}{{n!}}\sum\limits_{k = 0}^n {{B_{n,k}}\left( {{x_1},...,{x_{n - k + 1}}} \right)} } $$ So I substituted the partition formula into the generating function: $$\sum\limits_{n = 0}^\infty  {\frac{{{t^n}}}{{n!}}\sum\limits_{k = 0}^n {{B_{n,k}}\left( {{x_1},...,{x_{n - k + 1}}} \right)} }  = \sum\limits_{n = 0}^\infty  {\frac{1}{{n!}}\sum\limits_{\scriptstyle{u_i} \ne {u_j}\atop\scriptstyle{P_{\ell,n} }:{k_1} + ... + {k_n} = n} {\left( {{a_{\ell,n} }\prod\limits_{r = 1}^n {\frac{{x_{{u_r}}^{{k_r}}}}{{{{\left( {{u_r}!} \right)}^{{k_r}}}}}{t^{{k_r}{u_r}}}} } \right)} } $$ I tried equating coefficients of ${t^m}$ by having ${k_1}{u_1} + ... + {k_n}{u_n} = m$ on the right-hand side to obtain $$\sum\limits_{k = 0}^m {{B_{m,k}}\left( {{x_1},...,{x_{m - k + 1}}} \right)}  = m!\sum\limits_{n = 0}^\infty  {\frac{1}{{n!}}\sum\limits_{\scriptstyle{u_i} \ne {u_j}\atop{\scriptstyle{P_{\ell,n} }:{k_1} + ... + {k_n} = n\atop\scriptstyle{k_1}{u_1} + ... + {k_n}{u_n} = m}} {\left( {{a_{\ell,n} }\prod\limits_{r = 1}^n {\frac{{x_{{u_r}}^{{k_r}}}}{{{{\left( {{u_r}!} \right)}^{{k_r}}}}}} } \right)} } $$ Where the left-hand side is an equivalent expression for the mth complete Bell polynomial. However, I must have made a mistake - although the right-hand side gives indices which do indeed match the Bell polynomial, this equation does not give the correct values for the coefficients.","['integer-partitions', 'exponentiation', 'multinomial-theorem', 'combinatorics', 'generating-functions']"
4842489,Let $f$ be the sum of distances to vertices. The Fermat point $p_0$ satisfies $ \left.\frac{df}{dt}(p_0+t\vec{u})\right|_{t=0}=0$ for all $\vec{u}$.,"Problem: We consider a triangle $a=\left(x_1, y_1\right), b=\left(x_2, y_2\right)$ and $c=\left(x_3, y_3\right)$ with interior angles strictly less than $2 \pi / 3$ . Let $p=(x, y) \in \mathbb{R}^2$ with $d_1(x, y)=|\overrightarrow{p a}|, d_2(x, y)=$ $|\overrightarrow{p b}|$ and $d_3(x, y)=|\overrightarrow{p c}|$ . Prove that if $f(x, y)=d_1+d_2+d_3$ has a minimum value at $p_0=\left(x_0, y_0\right)$ then for all $\vec{u} \in \mathbb{R}^2$ ,we have $$
\left.
\frac{d f}{d t}\left(p_0+t \vec{u}\right)
\right|_{t=0}=0
$$ and $\frac{\partial f}{\partial x}\left(p_0\right)=\frac{\partial f}{\partial y}\left(p_0\right)=0$ . We say that $p_0$ is a Fermat's point of $abc$ . My solution: Let $g(t) = f(p_0 + tu)$ , $t\in \mathbb{R}$ . Since $f$ has attains its minimum value at $p_0(x_0,y_0)$ then $g$ attains its minimum value at $t=0$ . By the first order necessary condition, we have $$g'(0) = 0 \Leftrightarrow \left.\dfrac{df}{dt}(p_0 + tu)\right|_{t=0} = 0.$$ This is equivalent to \begin{align*}
&\langle u, \nabla f(p_0)\rangle = 0,\ \forall u \in \mathbb{R}^2\\
\Leftrightarrow\ & \nabla f(p_0) = (0,0)^T.
\end{align*} Hence, $\dfrac{df}{dx}(p_0) = \dfrac{df}{dy}(p_0) = 0$ . I hope everyone could take a look at my solution and correct it if you are interested.","['multivariable-calculus', 'derivatives', 'geometry', 'analysis']"
4842491,Quotient Rule: Why does Tao assume $g$ is non-zero on $X$?,"Tao ( Analysis I , 2022, p. 220): Why can't Tao just assume $g(x_0)\neq 0$ ? Wouldn't the conclusion still hold if we changed the assumption "" $g$ is non-zero on $X$ "" to "" $g(x_0)\neq 0$ ""? In contrast, Bartle & Sherbert (2011, Introduction to Real Analysis , p. 163f): Why do Bartle & Sherbert use the assumption $g(c)\neq0$ (as I'd expect) but Tao doesn't?","['intuition', 'calculus', 'derivatives', 'analysis']"
4842493,Question about non-i.i.d. Bernoulli random variables,"Let $X_1, X_2, \dots, X_n \sim \operatorname{Bernoulli}(p)$ and $\bar{X}=\frac{1}{n}\sum\limits_{i=1}^nX_i$ . Find an upper bound for $\mathbb{P}(|\bar{X}-p| > \epsilon) \forall \epsilon > 0$ . Note that random variables are not independent. $\textbf{What I've tried:}$ We know that $\mathbb{E}[\bar{X}] = \mathbb{E}[X_1] = p$ . So we can use Chebyshev's inequality because $\mathbb{P}(|\bar{X}-p| > \epsilon) = \mathbb{P}(|\bar{X}-\mathbb{E}[\bar{X}]| > \epsilon) \le \mathbb{P}(|\bar{X}-\mathbb{E}[\bar{X}]| \ge \epsilon) \stackrel{Chebyshev}{\le} \frac{\operatorname{Var}(\bar{X})}{\epsilon^2} \le \frac{\operatorname{Var}(\sum\limits_{i=1}^n X_i)}{n^2\epsilon^2} \le \frac{n \sum\limits_{i=1}^n \operatorname{Var}(X_i)}{n^2\epsilon^2} \le \frac{n^2  \operatorname{Var}(X_1)}{n^2\epsilon^2} = \frac{p(1-p)}{\epsilon^2}$ So, $\frac{p(1-p)}{\epsilon^2}$ is an upper bound. Is my proof true? Thanks!","['statistics', 'probability']"
4842500,How to squeeze the logistic function obliquely?,"The original function: $f(x) = \frac{1}{1+e^{-10(x-0.5)}} $ . Its graph (blue line) is shown here : How can I squeeze this function obliquely along the $y=x$ line? The squeezed function $g$ needs to satisfy that $g(0.5) = 0.5$ , $g(0)$ is close to $0$ , and $g(1)$ is close to $1$ . The graph of this function $g$ that I have in mind is the black line in the image above.","['functions', 'continuous-variables', 'transformation', 'graphing-functions']"
4842504,How to prove that $\tan 15^{\circ} = \tan 27^{\circ} \tan 33^{\circ} \tan 39^{\circ}$?,"Found this equality $$\tan 15^{\circ} = \tan 27^{\circ} \tan 33^{\circ} \tan 39^{\circ}$$ with a random search. It checks with WA, and these kind of equalities can be proved automatically. I am looking for an elementary proof. Some similarly looking equalities are using the identity; $$\tan 3 x= \tan x\cdot \tan(\frac{\pi}{3}-x)\cdot  \tan (\frac{\pi}{3}+x)$$ We notice that $27^{\circ} + 33^{\circ} = 60^{\circ}$ , but does not seem enough. Thank you for interest! $\bf{Added:}$ Let me add the mechanical proof that works like here .  One considers the basic angle $x = 3^{\circ}= \frac{2 \pi}{120}$ , and we need to show that $\tan 5 x= \tan 9 x \tan 11 x \tan 13 x$ , which is equivalent to: $z= e^{\frac{2 \pi i}{120}}$ is a root of the equation $$\frac{z^{10}-1}{z^{10}+1} + \frac{z^{18} -1}{z^{18} + 1}\cdot \frac{z^{22} -1}{z^{22} + 1}\cdot  \frac{z^{26} -1}{z^{26} + 1}=0$$ LHS factors and we see the cyclotomic polynomial $\Phi_{120}(z)$ in the numerator. $\bf{Added:}$ Since in general we have to show some equality for functions of multiples of an angle $x$ , where $x$ is a rational multiple of $2\pi$ , we could prove an equivalent equality for the angle $m x$ , where $m$ is relatively prime to the denominator of $\frac{x}{2 \pi}$ , so we might reduce to other known identities. Perhaps this one works with say $m = 7$ ? Since $7\times 15^{\circ} = 90^{\circ} + 15^{\circ}$ .",['trigonometry']
4842508,"For a square of some size, what inscribed shape has the greatest area:perimeter ratio?","I'm not a mathematician. I'm trying to solve a difficult archived programming problem from Project Euler. I don't necessarily want to link it, because sharing exact solutions to problems is against PE site rules and I don't want anyone here to go too far solving the problem in a response, nor do I want someone to be able to find the answer by googling the problem. The question I'm asking here is just one intermediate step and is sufficiently general that it's within the spirit of the PE site to ask for help with it. If you really want to know the exact problem you probably can find it without too much work, or you can give me your email and I'll send it to you. Basically my question is exactly as stated in the title. I have to imagine there's a well-known mathematical result that gives the solution, but I can't seem to find it. Ideally I'd also like to develop some intuition for how to find this answer myself. Relevant info given in the problem description: Let's say you have a square with side lengths of 500 meters. The area:perimeter ratio of this square is $125$ (this is what we're trying to maximize). The inscribed circle ( $r = 250$ ) has the same ratio. If you cut from each corner of the square an isosceles triangle of sides $75, 75, 75 \sqrt{2}$ , the resulting inscribed shape has an area:perimeter ratio of ~ $130.87$ . Info from my work on the problem so far: -A regular inscribed octagon has a ratio of $125$ . Maybe this result can be generalized to any regular inscribed shape symmetric about both axes with >4 sides? -An inscribed squircle of definition $x^4 + y^4 = r^4$ , ( $r = 250$ ), has a ratio of about $132.1$ . None of my attempts at a solution based on this shape have worked, though, so I'm thinking this is not the optimal shape.",['geometry']
4842522,Solve Cauchy Integral using residues,"I have an exam tomorrow, and we were given like an ""example test"" without answers, and one question is to solve this Cauchy integral: $$
\oint_c = \frac {2z+1} {(z+1)^2(z-3)} 
$$ with circle $ c: |z-2|=5 $ the function has a double pole in $ -1 $ and a single pole in $ 3 $ , both of the poles being inside of the circle. Residue $ f(3) $ : $$
\lim_{z \rightarrow 3}(\frac{2z+1}{(z+1)^2}) = \frac 7 {16}
$$ Residue $ f(-1) $ : $$
\frac {1} {1!} \lim_{z \rightarrow -1}(\frac{2z+1}{z-3})' = \lim_{z \rightarrow -1}(\frac{2(z-3)-(2z+1)}{(z-3)^2}) = \frac {2(-4) - (-1)} {(-4)^2} = \frac {-7}{16}
$$ that makes the final result be: $$
2 \pi i (\frac {7}{16} + \frac {-7}{16}) = 0
$$ Am I correct? For some reason, I find it odd, that the answer would be a 0 on an exam, but I can't check it anywhere, and I don't seem to see a mistake in my math.","['integration', 'complex-analysis', 'cauchy-integral-formula']"
4842527,Dirichlet's test with unimodal coefficients,"Briefly: If we modify the hypotheses of Dirichlet's test to require a unimodal sequence of coefficients, not necessarily a monotonic sequence, then do we still get the same quantitative bound on $\sum a_nb_n$ ? And is this a known result? Let $(a_n)$ be a sequence of non-negative real numbers that converges to $0$ , and let $(b_n)$ be a sequence of complex numbers with bounded partial sums: there exists a constant $B$ such that for all $p\leq q$ , we have $\left|\sum_{n=p}^q b_n\right| \leq B$ . If our sequences are supported on the natural numbers, $n\in\mathbb N$ , and $(a_n)$ is monotonic, then Dirichlet's test states that $\sum a_nb_n$ converges. The proof proceeds by summation by parts, and in fact, we get $\left|\sum a_nb_n\right|\leq a_0B$ . But what if we generalize to sequences supported on the integers, $n\in\mathbb Z$ ? Then we should ask for $(a_n)$ to be a unimodal sequence, not a monotonic sequence. That is, $(a_n)$ increases up to some maximum value $a^*$ and then decreases thereafter. So let's do that: $(a_n)$ is now unimodal. But to simplify matters, we'll keep indexing on the natural numbers, $n\in\mathbb N$ , anyway, so that convergence is easy to talk about. Now $(a_n)$ is eventually decreasing, and Dirichlet's test still says $\sum a_nb_n$ converges. What's more interesting is the bound that we can get. The easy approach would be to split the series $\sum a_nb_n$ ""horizontally"" into two series of $a_nb_n$ terms, one with $a_n$ increasing and the other with $a_n$ decreasing. Then we would get $\left|\sum a_nb_n\right|\leq 2a^*B$ . But I believe we can decompose the sum ""vertically"" into a stack of rectangles, and that should give us the better bound $\left|\sum a_nb_n\right|\leq a^*B$ , i.e. without the extra factor of $2$ . Now, I haven't written out a formal proof of the above claim, which might make it hard to evaluate... but I would like to know: Did I make a mistake? Is this a known result? Is there some reference that I can cite for the inequality $\left|\sum a_nb_n\right|\leq a^*B$ ? For comparison, here are some related questions on the site: For Dirichlet's test, is it true that if $\ a_1=1\ $ then $\ \left\vert \sum_{n=1}^{\infty} a_n b_n \right\vert \leq M\ ?$ shows the traditional proof for the bound, in the special case of a monotonic sequence of coefficients. Is this generalization of Dirichlet's test true? is an attempted generalization that goes too far; if we drop all ordering hypotheses, then the resulting series might diverge. No amount of Googling has turned up a reference to the claim, so far. (Incidentally, these course notes contain a claim of the usual Dirichlet's test with an extra factor of $2$ , but that appears to be unrelated to my question; it's merely due to a typo where $k$ and $k+1$ are reversed, causing a series to telescope incorrectly.)","['monotone-functions', 'reference-request', 'real-analysis', 'upper-lower-bounds', 'sequences-and-series']"
4842548,Show that two random vectors have the same distribution,"Let $W_1,W_2,...$ be independent identically distributed random variables on $[0,\infty).$ Define $T_0=0,T_n=\sum_{k=1}^{n}W_k (n\ge 1).$ Show that, for any $n,m\in\mathbb{Z}^{+}$ and $0\le t_1<\cdots<t_{m},$ both $\left(\sum_{k=n+1}^{\infty} \mathbf{1}_{\left\{T_{k}-T_{n} \leq t_{1}\right\}}, \cdots, \sum_{k=n+1}^{\infty} \mathbf{1}_{\left\{T_{k}-T_{n} \leq t_{m}\right\}}\right)$ and $\left(\sum_{\ell=1}^{\infty} \mathbf{1}_{\left\{T_{\ell} \leq t_{1}\right\}}, \cdots, \sum_{\ell=1}^{\infty} \mathbf{1}_{\left\{T_{\ell} \leq t_{m}\right\}}\right)$ have the same distribution $$\text{i.e. }\left(\sum_{k=n+1}^{\infty} \mathbf{1}_{\left\{T_{k}-T_{n} \leq t_{1}\right\}}, \cdots, \sum_{k=n+1}^{\infty} \mathbf{1}_{\left\{T_{k}-T_{n} \leq t_{m}\right\}}\right)\stackrel{\textbf {d}}=\left(\sum_{\ell=1}^{\infty} \mathbf{1}_{\left\{T_{\ell} \leq t_{1}\right\}}, \cdots, \sum_{\ell=1}^{\infty} \mathbf{1}_{\left\{T_{\ell} \leq t_{m}\right\}}\right).$$ Applying characteristic function,I can deduce that for any  positive integer $k\ge n+1$ , $T_k-T_n\stackrel{\textbf {d}}=T_{k-n}$ . Then for any $c\ge 0 $ , $\mathbf{1}_{\left\{T_{k}-T_{n} \leq c \right\}}\stackrel{\textbf {d}}=\mathbf{1}_{\left\{T_{k-n}\leq c\right\}}$ .The random variables $X_1,X_2,Y_1,Y_2,$ satisfying $X_1\stackrel{\textbf {d}}=Y_{1}$ and $X_2\stackrel{\textbf {d}}=Y_{2}$ , can not get $X_1+X_2\stackrel{\textbf {d}}=Y_{1}+Y_2$ ,unless $X_1\perp\!\!\!\!\perp Y_1$ , $X_2\perp\!\!\!\!\perp Y_2$ ( $X \perp\!\!\!\!\perp Y$ means $X$ is independent of $Y$ ). As @jwhite's comment,define $$X:=\left(\sum_{k=n+1}^{\infty} \mathbf{1}_{\left\{T_{k}-T_{n} \leq t_{1}\right\}}, \cdots, \sum_{k=n+1}^{\infty} \mathbf{1}_{\left\{T_{k}-T_{n} \leq t_{m}\right\}}\right),Y:=\left(\sum_{\ell=1}^{\infty} \mathbf{1}_{\left\{T_{\ell} \leq t_{1}\right\}}, \cdots, \sum_{\ell=1}^{\infty} \mathbf{1}_{\left\{T_{\ell} \leq t_{m}\right\}}\right),$$ directly show that for any $m$ -tuple of non-negative integers $p=(p_1,...,p_m),$ $$\mathbb{P}(X=p)=\mathbb{P}(Y=p).$$ Considering the $\mathbb{P}(X=p)$ ,since $X=\left(\sum_{k=n+1}^{\infty} \mathbf{1}_{\left\{T_{k}-T_{n} \leq t_{1}\right\}}, \cdots, \sum_{k=n+1}^{\infty} \mathbf{1}_{\left\{T_{k}-T_{n} \leq t_{m}\right\}}\right)$ and $W_{n}$ are non-negative, $X=p\Rightarrow  \bigcap_{i=1}^{m}S_{i}$ ,where $$S_i=\left\{\sum_{k=1}^{p_i}W_{k+n}\le t_{i}\right\}\bigcap \left\{\sum_{k=1}^{p_i+1}W_{k+n}> t_{i}\right\}.$$ So $\mathbb{P}(X=p)=\mathbb{P}(\bigcap_{i=1}^{m}S_{i}).$ Similarly, $\mathbb{P}(Y=p)=\mathbb{P}(\bigcap_{i=1}^{m}R_{i})$ ,where $$R_i=\left\{\sum_{k=1}^{p_i}W_{k}\le t_{i}\right\}\bigcap \left\{\sum_{k=1}^{p_i+1}W_{k}> t_{i}\right\}.$$ Note those are not independent,how to prove that $\mathbb{P}(X=p)=\mathbb{P}(Y=p)?$ (this is my key issue)","['characteristic-functions', 'probability-distributions', 'probability-theory']"
4842584,Deriving $\operatorname{arccot}(x)=\int\limits_{x}^ {\infty} \frac{1}{1+t^2} ~dt$,"For $x\in \mathbb R$ we have $$\text{arccot}(x)=\int\limits_{x}^ {\infty} \frac{1}{1+t^2} ~dt.$$ How can we prove this? for if we have definite integral then for example after assuming some hypothesis we have $$\text{arcsin}(x)=\text{arcsin}(0)+\int\limits_{0}^ {x} \frac{1}{\sqrt{1-t^2}} ~dt.$$ By fundamental theorem of calculus. How we can use it for improper integral?
If $f:(0,\infty)\rightarrow \mathbb R$ such that for all $x$ , $\int\limits_{x}^ {\infty} f(t) ~dt$ converges. And if we define $~F$ on $(0,\infty)$ as $$ x\mapsto \int\limits_{x}^ {\infty} f(t) ~dt$$ then I proved $F'(x)=f(x)$ for all $x$ .So if we have any other $G$ on $(0,\infty)$ such that $G'(x)=f(x)$ . Then by mean value theorem we have $$
G(x)=C+\int\limits_{x}^ {\infty} f(t) ~dt$$ for some constant $C$ .
Is there any analogous for that constant $C$ like definite integral. Is $C=G(\infty)=\lim\limits_{x\to \infty} G(x)$ ? Because here $$\text{arccot}(x)=\lim\limits_{k\to \infty}\text{arccot}(k)+\int\limits_{x}^ {\infty} \frac{1}{1+t^2} ~dt.$$ So My question is about that constant $C$ . Is there any way to get $C$ ?","['integration', 'improper-integrals', 'calculus', 'definite-integrals']"
4842638,Are bijective morphisms of varieties homeomorphisms?,"Every topologist knows that a continuous bijection does not need to be a homeomorphism. I was just realizing that I don't know examples of this happening for varieties (irreducible, reduced and separated schemes of finite type over a field, preferably algebraically closed) with the Zariski topology. I know bijections which are not isomorphisms of varieties, like the normalisation of a cuspidal curve $\tilde C \to C$ , but this is still a homeomorphism on the underlying sets (because Zariski topology for curves is just complements of finite sets). So my question is: Is every bijective morphism of varieties $X \to Y$ a homeomorphism?","['general-topology', 'algebraic-geometry']"
4842652,What is the probability that the triangle formed by three uniformly random points on the sphere contains its circumcentre?,"In answering Conjecture: If $A,B,C$ are random points on a sphere, then $E\left(\frac{\text{Area}_{\triangle ABC}}{\text{Area}_{\bigcirc ABC}}\right)=\frac14$. it turned out that if you choose three points uniformly randomly on the sphere, the density distribution of their angles on the circle they form is proportional to the area of the triangle they form. So these points tend to be spaced further apart on the circle than points randomly uniformly chosen on the circle. (That makes sense, since on the sphere the probability distribution of the distance between two points vanishes at distance $0$ .) Since that seemed like a nice result, I wanted to do something else with it. A question often asked about random points on a circle is about the probability that the triangle they form contains the centre of the circle. If the points are uniformly distributed on the circle, that probability is $\frac14$ (see e.g. Probability that n points on a circle are in one semicircle ). If the points are spaced further apart, this probability should be higher. So what is it for points uniformly randomly chosen on a sphere? I’m posting this as a self-answered question , but I merely worked out the integrals and would be happy to see a more elegant solution that explains the simplicity of the answer with a symmetry argument.","['spheres', 'geometric-probability', 'geometry', 'triangles', 'probability']"
4842729,How the comparison of the cardinalities of sets affects the cardinalities of their powersets [duplicate],"This question already has an answer here : Do sets, whose power sets have the same cardinality, have the same cardinality? (1 answer) Closed 6 months ago . In my question, I denote by $|\cdot|$ the cardinality of any set. Moreover, if $f: X \to Y$ , we denote by $\mathcal{P}f$ its direct image, i.e. $\mathcal{P}f(A)=\{f(a) : a \in A\}$ . Let $X,Y$ be two sets. Is it true that $|X| \le |Y| \iff |\mathcal{P}(X)| \le |\mathcal{P}(Y)|$ ? My attempt. Assume first that $|X| \le |Y|$ . Then there exists an injective function $f: X \to Y$ . Hence we may easily check that the direct image $\mathcal{P}f: \mathcal{P}(X) \to \mathcal{P}(Y)$ is also injective, so that $|\mathcal{P}(X)| \le |\mathcal{P}(Y)|$ . [Actually, we should have $|X| < |Y| \implies |\mathcal{P}(X)| < |\mathcal{P}(Y)|$ because if the direct image $\mathcal{P}g$ of some function $g: X \to Y$ is surjective, then also $g$ is a surjection] Conversely, assume that $|\mathcal{P}(X)| \le |\mathcal{P}(Y)|$ and suppose by contradiction that $|X|>|Y|$ . Then by the above step we should have $|\mathcal{P}(X)| > |\mathcal{P}(Y)|$ , absurd. Is my argument correct? Furthermore, is it also true that $|\mathcal{P}(X)| < |\mathcal{P}(Y)| \implies |X| < |Y|$ ?","['elementary-set-theory', 'set-theory', 'cardinals', 'discrete-mathematics']"
4842743,Ring Structure on the geometric line in synthetic geometry,"Synthetic Differential Geometry by Kock opens with the following: The geometric line can, as soon as one chooses two distinct points on it,
be made into a commutative ring, with the two points as respectively 0
and 1. This is a decisive structure on it, already known and considered
by Euclid, who assumes that his reader is able to move line segments
around in the plane (which gives addition), and who teaches his reader
how he, with ruler and compass, can construct the fourth proportional
of three line segments; taking one of these to be [0, 1], this defines the
product of the two others, and thus the multiplication on the line...Of course, this basic structure does not depend on having the (arithmetically constructed) real numbers $\mathbb{R}$ as a mathematical model for [it]). I am confused about what exactly this ring structure is? Am I to understand that the elements of this Ring are not points on the line but line segements? Does one add two line segements by just adding their length? I looked up how to construct the fourth proportional of three line segments which is multiplication in this ring, leading me to suspect that perhaps he's referring to the ring of geometrically constructible numbers?","['synthetic-differential-geometry', 'geometry', 'geometric-construction']"
4842761,Autocommutator subgroup is a characteristic subgroup,"Autocommutator subgroup $K(G)$ of a group $G$ is defined as $$K(G):=\langle [g,‎\alpha‎]:g\in G,\; ‎\alpha ‎\in ‎Aut(G)‎\rangle ‎.,$$ where $[g,\alpha ]:=g^{-1}\alpha (g)$ . Is $K(G)$ a characteristic subgroup in $G$ ? Recall that a subgroup $H$ of a group $G$ is said to be charactristic in $G$ if $\alpha (H)\subseteq H$ for all $\alpha \in Aut(G)$ . My try: Assume that $[g,\alpha ]\in K(G)$ is an arbitrary generator and $\beta \in Aut (G)$ . We need to show that $\beta ([g,\alpha])\in K(G)$ . We have $\beta ([g,\alpha])=\beta (g^{-1})\beta (\alpha (g))$ . But I couldn't show that $\beta ([g,\alpha])\in K(G)$ .","['group-theory', 'characteristic-subgroups']"
4842762,Why are complex measures not allowed to attain $\infty$ while signed measures are?,"I saw another question similar to this one but I'm not satisfied by answers. Here I changed the question to clearify the point I am interested in. I study Measure Theory for Real & Complex Analysis and I wonder why in the following setting \begin{equation} \mu: \mathcal{M} \rightarrow \mathbb{C} \end{equation} \begin{equation} \mu(E) = \mu_r(E) + i\mu_i(E) \end{equation} where $\mu$ is a complex measure and $\mu_r$ & $\mu_i$ are signed measure some writers (like Folland if I remember correctly) does not let $\mu$ to attain at most one of $\infty$ or $-\infty$ . In the general definition for signed measures we can let signed measures to attain at most one of the plus or negative $\infty$ . For example, is the following setting not meaningful or not sensible? \begin{equation} \mu: \mathcal{M} \rightarrow \mathbb{C}\cup \{\infty\} \end{equation} \begin{equation} \mu(E) = \mu_r(E) + i\mu_i(E) \end{equation} where $\mu_r$ & $\mu_i$ are signed measures that does not attain $-\infty$ , i.e. they are signed measures which attain values in $\mathbb{R} \cup \infty$ , and we defined $\mu(E) = \infty$ whenever $\mu_r = \infty$ or $\mu_i = \infty$ . Here I rely on following artihmetical definitions in a formal way: \begin{equation} a + i\infty = \infty = \infty + ai \end{equation} where $-\infty$ is not considered. EDIT: Typo in the last equation.","['measure-theory', 'signed-measures', 'real-analysis', 'complex-analysis', 'complex-integration']"
4842857,Is smoothness of multiplication redundant in the definition of Lie Group?,"It is well known that in the definition of Lie groups, we actually only need that the multiplication be smooth, since this implies that inversion is smooth . I'm now trying to solve the following exercise: Show that in the definition of Lie group, it is enough to assume that the inverse map $G \to G, g \mapsto g^{-1}$ is smooth. I don't know how to attack this problem, and also couldn't find any other references to it in the literature. Could I get some hints or references on how to solve it?","['smooth-manifolds', 'lie-groups', 'differential-geometry']"
4842886,"How to find a continuous and positive $f$, such that the ODE $x''(t)=f(x(t))$ doesn't have unique solution given some initial value?","As the title . I want to find a continuous $f$ , such that the ODE $x''(t)=f(x(t))$ has more than one solutions when the initial value $x(0), x'(0)$ are given. I hope $f(x)\neq0$ for every $x\in\mathbb{R}$ . So assume $f$ is always positive. Is it possible to find one?",['ordinary-differential-equations']
4842894,Finishing a corollary in Tao's proof of Banach Tarski,"This is the proof I'm referring to: https://www.math.ucla.edu/~tao/preprints/Expository/banach-tarski.pdf I understand how we prove Corollary 1.2 and Lemma 1.3: Corollary 1.2 (Hausdorff paradox, first version). There exists a countable subset $C$ of the sphere $S^2$ , and a decomposition $$
\left(S^2 \backslash C\right)=\Omega_1 \uplus \Omega_2 \uplus \Omega_3 \uplus \Omega_4
$$ such that $$
\left(S^2 \backslash C\right)=\Omega_1 \uplus A \Omega_2=\Omega_3 \uplus B \Omega_4
$$ for some rotation matrices $A, B \in S O(3)$ . Lemma 1.3. Let $C$ be a countable subset of the sphere $S^2$ . Then there exists a decomposition $$
S^2=\Sigma_1 \uplus \Sigma_2
$$ such that $$
S^2 \backslash C=\Sigma_1 \uplus R \Sigma_2
$$ for some rotation matrix $R \in S O(3)$ . However, Tao then writes that using these two results we can conclude 1.4. Corollary 1.4. There exists a partition $$
S^2=\Gamma_1 \uplus \ldots \uplus \Gamma_8
$$ and rotation matrices $R_1, \ldots, R_8 \in S O(3)$ such that $$
S^2=\biguplus_{i=1}^4 R_i \Gamma_i=\biguplus_{i=5}^8 R_i \Gamma_i
$$ I was having difficulty showing how this follows from the above. How do we construct $\Gamma_i$ and $R_i$ ?","['group-theory', 'measure-theory']"
4842911,"Representations of semisimple lie algebras, the highest weight root strings spaces have dimension one","This is exercise 22.1 in Humphreys Introduction to Lie Algebras and Representation Theory book. Let $\mathfrak{g}$ be a semisimple complex Lie Algebra and let $\lambda$ be a dominant weight also let $\alpha$ be a root of $\mathfrak{g}$ . Prove without using Freudenthal's formula, $V(\lambda)$ , the irreducible representation of highest weight $\lambda$ ,  the dimension of each component in of the $\alpha$ string though $\lambda$ is $1$ i.e. the dimension of the $\lambda- k \alpha$ weight space of $V(\lambda)$ is 1 for $0\leq k \leq <\lambda,\check{\alpha}>$ . My thoughts on this are that the fact that the Weyl group preserves the dimension of weight spaces we immediately have that the start (as its highest weight) and end of the string are dim 1. I am not sure how to proceed, I want to try use $\mathfrak{sl}_2$ theory but haven't thought of how. Up to this point we have covered the structure theory of lie algebras, abstract root systems and most recently the construction of the $V\lambda$ using Verma modules. Edit: Simple root $\alpha$","['abstract-algebra', 'representation-theory', 'lie-algebras']"
4842959,How to prove this topological lemma?,"I am stuck trying to prove some result from topology. It is fairly complicated to state it here, but I realized that I could prove it, if I manage to show the following Lemma. Suppose $X$ is a normal space (if necessary, we can even assume perfectly normal). Suppose $A$ is a closed subspace of $X$ and $B$ is a closed subspace of $A$ , so $$ B \subseteq A \subseteq X. $$ Furthermore, suppose there is some other space $C \subseteq X$ , for which we know that $C \cap A = \emptyset$ and $\overline{C} \cap A \subseteq B$ . Can we find an open neighborhood $W$ of $C$ for which $\overline{W} \cap A \subseteq B$ ? If we can, how can we prove its existence? If not, could you please provide a counterexample?","['general-topology', 'separation-axioms']"
4843005,"Check that $T: \ell^2 \to \ell^2$ is not surjective where $T(x_1, x_2, x_3, \ldots) = (x_1 + x_2, x_2 + x_3, x_3 + x_4, \ldots)$","Let $T: \ell^2 \to \ell^2$ the operator defined as $$T(x_1, x_2, x_3, x_4, \ldots) = (x_1 + x_2, x_2 + x_3, x_3 + x_4, x_4+ x_5, \ldots)$$ where $$\ell^2 = \left\{ (x_n)_{n=1}^\infty : \sum_{n=1}^\infty |x_n|^2 < \infty \right\}$$ I need to prove that $T$ is not surjective. I managed to prove that $T$ is injective and that $T = I + S$ , where $I$ is the identity operator and $S$ is the right shift operator. I know that $S$ is not surjective, but no idea how to deal with $T$ . Any help will be appreciated.","['lp-spaces', 'functional-analysis']"
4843038,"$\sum_{n=1}^\infty (AB e_n) = \sum_{n=1}^\infty \sum_{m=1}^\infty (Be_n, e_m) (Ae_m, e_n)$","Let $H$ be a Hilbert space, $A$ and $B$ bounded linear operators on $H$ , and $\{e_n\}$ an orthonormal basis for $H$ . I am following a proof that claims it is obvious that $$\sum_{n=1}^\infty (AB e_n, e_n) = \sum_{n=1}^\infty \sum_{m=1}^\infty (Be_n, e_m) (Ae_m, e_n).$$ However I do not see why this is necessarily true. How can one prove this?","['hilbert-spaces', 'operator-theory', 'functional-analysis']"
4843051,Approximating a sum of products of binomial coefficients,"I have a function I want to approximate the growth of in a closed form: $$\frac{\sum_{i=0}^{n-1}\sum_{j=0}^{n} |n-i-j|\binom{n+i-j}{i}\binom{n-i+j-1}{j}}{\binom{2n}{n}}$$ Without the absolute value term, this sort of resembles Vandermonde's identity, but I'm a bit stuck trying to resolve the function with this term. I've also tried expressing it as a polynomial $$\left(\frac{1}{2\pi i}\right)^2\sum_{k=0}^{n-1}\sum_{j=0}^{n}|n-k-j|\oint_{|z|\ =\ 1}{\left(1 + z\right)^{n+k-j} \over z^{k+1}}dz\oint_{|z|\ =\ 1}{\left(1 + z\right)^{n+j-k-1} \over z^{n-k}}dz $$ which seems to lead no where. Can anyone suggest a way forward?","['numerical-methods', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
4843054,Find $\sum^{n}_{k=1} \binom{n}{k} \frac{(-1)^k}{k^a}$,"I have been trying to find a general representation of the following finite sum. $$
S(n,a) = -\sum^{n}_{k=1} \binom{n}{k} \frac{(-1)^k}{k^a}
$$ The sum seems to be related to Generalized Harmonic Numbers, as I've been able to work out the first few integer values of $a$ as the following. $$
S(n,1) = H_{n} \ , \ S(n,2) = \frac{(H_{n})^{2} + H_{n}^{(2)}}{2} \ , \ S(n,3) = \frac{(H_{n})^3}{6} + \frac{H_{n}^{(2)}H_{n}}{2} + \frac{H_{n}^{(3)}}{3}
$$ Where $ H_{n}^{(m)}$ is the n-th m-th order generalized harmonic number. It also sees to be closely related to the Stirling numbers of the second kind because if you take $a$ to be negative you get something very close to the explicit definition of the stirling numbers. I have looked online to see if there is any identities related to these special numbers and function that involve a sum like this but I haven't been able to find anything. If you have any insight please let me know.","['summation', 'harmonic-numbers', 'binomial-coefficients', 'combinatorics', 'stirling-numbers']"
4843067,Calculate the shape of a Tetrahedron given the dihedral angles,"Let's suppose we have 5 of the 6 dihedral angles of a tetrahedron, and the vertices A,B,C,D.
Where $A = (0,0,0)$ and $B = (1,0,0)$ . We also limit $C$ to be of the form: $(x_1,x_2,0)$ . and lastly $D = (x_3,x_4,x_5)$ . Given the 5 known values of 5 of the dihedral angles, we should be able to calculate the 6th dihedral angle and the values $x_i$ . But the math gets fairly complicated and I have not been able  to find a correct solution. So the question is: what are these values? Any help would be greatly appreciated.","['trigonometry', 'geometry', '3d']"
4843069,Uniqueness of initial value problem to ODE,"I was presented with an i.v.p.: $$y'=\frac{1+y^2}{1+x^2},y(0)=1$$ Using separation of variables I obtained $$\arctan(y)=\arctan(x)+c$$ Substituting x=0 and y=1 gives $c=\frac{\pi}{4}$ but if I solve for y and get $$y=\tan(\arctan(x)+c)$$ then plugging in those values reduces to $$1=\tan(c)$$ which has a solution $c=\frac{\pi}{4}+k\pi$ . Which approach is correct? Does this i.v.p. have only one solution or infinitely many of them","['initial-value-problems', 'ordinary-differential-equations']"
4843107,Solve PDE with condition,"\begin{equation}
    \begin{cases}
        \sqrt{x}u_x - \sqrt{y}u_y = u^2 \\
        u_{x=y} = \phi (y)
    \end{cases}
\end{equation} I want to change u(x,y) to v $(\tau, s )$ : \begin{equation}
    \begin{cases}
        \frac{dx}{ds} = \sqrt{x} , x(s=0) = \tau \implies x = (\frac{s+c}{2})^2\tau^2 \\
        \frac{dy}{ds} = -\sqrt{y} , y(s=0) = \tau \implies y = (\frac{-s+c}{2})^2\tau^2 \\
        \tau = \frac{\sqrt{x} + \sqrt{y}}{c} \\
        s^2 = 2\frac{x+y}{\tau^2} - c^2 = 2\frac{c^2(x+y)}{(\sqrt{x} + \sqrt{y})^2} - c^2 = (c\frac{\sqrt{x}-\sqrt{y}}{\sqrt{x} + \sqrt{y}})^2 \\
        s = c\frac{|\sqrt{x}-\sqrt{y}|}{\sqrt{x} + \sqrt{y}}
    \end{cases}
\end{equation} So we change the condition to : \begin{equation}
    \begin{cases}
        \frac{dv}{ds} = v^2 \\
        v_{s=0} = \phi(\tau^2) \\
    \end{cases}
\end{equation} We find the solution : \begin{equation}
    \begin{cases}
        v = \frac{-1}{s+c} \\
        v_{s=0} = \phi(\tau^2) \\
        \implies v = \frac{-\phi(\tau^2)}{s\phi(\tau^2)+c} \\
        Check : \frac{dv}{ds} = \frac{\phi^2(\tau^2)}{(s\phi(\tau^2)+c)^2} = v^2 \\
    \end{cases}
\end{equation} Change $v(\tau, s)$ to $u(x,y)$ : \begin{equation}
    u(x,y) = \frac{-\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2})}{c\frac{|\sqrt{x}-\sqrt{y}|}{\sqrt{x} + \sqrt{y}}\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2}) + c} = \frac{-\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2})}{\frac{|\sqrt{x}-\sqrt{y}|}{\sqrt{x} + \sqrt{y}}\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2}) + 1} \\ 
\end{equation} Choose simple case c = 1 \begin{equation}
    \begin{cases}
        \frac{du}{dx} = -\frac{(\sqrt{x} + \sqrt{y})sgn(\sqrt{x}-\sqrt{y}) - |\sqrt{x} - \sqrt{y}|}{2\sqrt{x}(\sqrt{x} + \sqrt{y})^2(\frac{|\sqrt{x}-\sqrt{y}|}{\sqrt{x} + \sqrt{y}}\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2}) + 1)^2}\phi^2(\frac{(\sqrt{x}+\sqrt{y})^2}{c^2}) \\ 
        \frac{du}{dy} = -\frac{-(\sqrt{x} + \sqrt{y})sgn(\sqrt{x}-\sqrt{y}) - |\sqrt{x} - \sqrt{y}|}{2\sqrt{y}(\sqrt{x} + \sqrt{y})^2(\frac{|\sqrt{x}-\sqrt{y}|}{\sqrt{x} + \sqrt{y}}\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2}) + 1)^2}\phi^2(\frac{(\sqrt{x}+\sqrt{y})^2}{c^2}) \\
        \sqrt{x}\frac{du}{dx} - \sqrt{y}\frac{du}{dy} = -\frac{sgn(\sqrt{x}-\sqrt{y}) }{(\sqrt{x} + \sqrt{y})(\frac{|\sqrt{x}-\sqrt{y}|}{\sqrt{x} + \sqrt{y}}\phi( \frac{(\sqrt{x} + \sqrt{y})^2}{c^2}) + 1)^2}\phi^2(\frac{(\sqrt{x}+\sqrt{y})^2}{c^2}) \neq u^2
    \end{cases}
\end{equation} Where did i do wrong ?","['change-of-variable', 'ordinary-differential-equations', 'partial-differential-equations']"
4843143,Right Differentiability of $\|f(x)\|$,"Problem: Let $E$ be a normed space, and $f:[a,b]\to E$ be continuous.
Define $g:[a,b]\to\mathbb R$ by $g(x)=\|f(x)\|$ .
Prove that if $f'_{+}(t_0)$ exists for some $t\in[a,b)$ , then so does $g'_{+}(t_0)$ ,
and moreover $$|g'_{+}(t_0)|\leq \|f'_{+}(t_0)\|.$$ What I have done: I couldn't prove the existence of $g'_{+}(x)$ , however assume it exists I can prove
the inequality as follows:
For $h>0$ , we have \begin{align*}
g(t_0+h)-g(t_0)
&=\|f(t_0+h)-f(t_0)+f(t_0)\|-\|f(t_0)\|\\
&\leq \|f(t_0+h)-f(t_0)\|
\end{align*} Since $\|\cdot\|$ is continuous, and by assumption $g'_{+}(t_0)$ exists, we obtain that $g'_+(t_0)\leq \|f'_+(t_0)\|.$ Similar argument yields $-g'_+(t_0)\leq \|f'_+(t_0)\|$ . Thus we obtain the desired inequality. Need Help: How to prove $g'_+(t_0)$ exist?","['differential', 'calculus', 'derivatives']"
4843188,Computing $\int _0^{\infty }e^{-b x} x^{b-1} \log \left((1-a x)^2\right)dx$ or $ \mathbb E \log\left[(c- \chi^2_{2b} )^2\right]$,"For $a>0,b\geq 1$ , what is the value of the following integral? $$\int _0^{\infty }e^{-b x} x^{b-1} \log \left((1-a x)^2\right)dx$$ Motivation: I need the following expectation for random variable $X$ where $2b X$ is distributed as Chi-squared with $2b$ degrees of freedom: $$E \log\left[(1-a X)^2\right]$$ The corresponding integral is $$\int_0^\infty \frac{\left(\frac{1}{b}\right)^{-b} e^{-b x} x^{b-1} \log \left((1-a x)^2\right)}{\Gamma (b)}$$ Mathematica gets the following formulas for $b=1,\ldots,6$ in terms of $\operatorname{Ei}(x)=-\int_{-z}^\infty e^{-t}/t \mathbb{d}t$ $$\begin{array}{ll}
b=1&-2 e^{-1/a} \text{Ei}\left(\frac{1}{a}\right)\\
b=2&2-\frac{2 (a+2) e^{-2/a} \text{Ei}\left(\frac{2}{a}\right)}{a}\\
b=3&\frac{3 a (a+1)-(2 (a+3) a+9) e^{-3/a} \text{Ei}\left(\frac{3}{a}\right)}{a^2}\\
b=4&\frac{a ((11 a+16) a+16)-2 (3 ((a+4) a+8) a+32) e^{-4/a} \text{Ei}\left(\frac{4}{a}\right)}{3 a^3}\\
b=5&\frac{5 a ((2 (5 a+9) a+25) a+25)-(4 (3 (2 (a+5) a+25) a+125) a+625) e^{-5/a} \text{Ei}\left(\frac{5}{a}\right)}{12 a^4}\\
b=6&\frac{a ((((137 a+288) a+486) a+648) a+648)-12 (5 ((((a+6) a+18) a+36) a+54) a+324) e^{-6/a} \text{Ei}\left(\frac{6}{a}\right)}{30 a^5}
\end{array}
$$ What is the general formula in terms of $b$ ? The code to compute the formulas (takes about 2 minutes to run) $Assumptions = {x > 0, a > 0};
dist[n_] = 
  TransformedDistribution[x/n, 
   x \[Distributed] ChiSquareDistribution[n]];
pdf[n_, x_] = PDF[dist[n], x];
mean[b_?IntegerQ] := 
  Integrate[Log[(1 - a x)^2] pdf[2 b, x], {x, 0, 1/a, \[Infinity]}];
formulas = FullSimplify@mean[#] & /@ Range[6] Background Solving for $a$ in $E[\log(1-a X^2)^2]=0$ gives the largest convergent learning rate for linear least squares mini-batch SGD with batch size $2b$ , step size $a$ and observations $X$ distributed as standard normal in $d=1$ dimensions. Batch-size 1 case is also known as LMS filter . Stability of LMS filter w.r.t. to step size is not fully understood, so characterizing it completely for a simple case of Gaussian observations is interesting. Related discussion here addresses the issue of convergence with batch size 1 and $d\ge 1$ Curiosuly, 1d iteration appears to become more unstable with increased batch size. In other words, applying random iteration several times and averaging updates at each step, can turn a stable random iteration into an unstable one. Notebook","['integration', 'special-functions', 'probability', 'sequences-and-series']"
4843193,"Are bilinear functionals continuous, in general?","If we define the implicit topology on an arbitrary real vector space $V$ to be the minimal topology in which all linear functionals $V\rightarrow\mathbb{R}$ are continuous, then are bilinear functions $V\times W\rightarrow\mathbb{R}$ necessarily continuous? I figured out that this way of giving a real vector space a topology is a nice way to introduce the standard topology of finitely-dimensional real vector spaces, without using the notion of a base. Because it is the standard topology, all well-known properties are true. But is it true in infinitely-dimensional case? Are bilinear functions continuous? Among things that I figured out are: A function to $V$ is continuous iff it is continuous when composed with all linear functionals $V\rightarrow\mathbb{R}$ , (I hoped this might help identify continuous functions from $V$ ) For arbitrary base $B$ identify the space $V$ with the space of functions $B\rightarrow\mathbb{R}$ with only finitely many non-zero points. Now restrict the product topology of all functions $B\rightarrow\mathbb{R}$ to $V$ . Call this topology the topology generated by base $B$ . Now, my topology is the smallest topology which contains topologies generated by each base, If one of $V$ or $W$ is finitely-dimensional, then bilinear functions $V\times W\rightarrow\mathbb{R}$ are indeed continuous. Every subspace of $V$ is a closed set, and the topology restricted to it is the same as the one that is implicitly defined on it. The topological product of implicit topologies is the implicit topology of the cartesian product of vector spaces (i.e. my implicit topology and vector space cartesian product commute). What I tried to do next is to consider an example where both $V$ and $W$ are the space of polynomials (with real coefficients) and where the bilinear functional is $(p,q)\mapsto\sum_{i=0}^\infty p_i q_i$ ( $p_i$ is the $i$ -th coefficient of the polynomial $p$ , and the same holds for $q_i$ ). I really couldn't derive anything meaningful about this example...","['continuity', 'general-topology', 'vector-spaces']"
4843276,Prove that the max area of a triangle inside a rectangle is half of the area of the rectangle,"Let $ABCD$ be a rectangle, and points $PQR$ be inside the triangle(or on it's edges). I want to prove that the maximum area of the triangle $PQR$ is the area of the rectangle $ABCD$ divided by $2$ . When I tried to prove it, I said that the diagonal(wlog) $AC$ is the largest edge in $ABCD$ so it have to be in there, and from there it is quite easy to prove, but I don't understand how can I prove that because it is the largest so it must be a part of the triangle with max area? EDIT: I was asked to try to prove again with the hints and here is what I got: If the largest edge(wlog) $PQ$ , not on the perimeter, then we can extend the corners $P,Q$ to get a triangle with a greater altitude, and a greater width $PQ$ , which will give us a greater area, but if $PQ$ on the perimeter then we can't extend it, thus it has to be on the perimeter. Assume $P$ is not a corner, then, by the same principle as $1.$ we can extend the other two to the edges, and then extend them even more to the corners. I don't know how to infer that.
I know it's not ideal prove of what I've been asked, but I guess that's the best I can do.","['area', 'geometry']"
4843314,Which conditions make an associative algebra a matrix algebra?,"If I have an algebra over a field, which additional conditions would make this algebra uniquely the matrix algebra? I'm already assuming that I have an $n^2$ dimensional vector-space equipped with an associative bilinear product. I think this still allows for more spaces than just matrices? Which additional conditions would fix this to be the matrix algebra over the field?","['matrices', 'abstract-algebra']"
4843321,Example of a finite projective plane which is not a translation plane,I've been studying finite projective geometry for several weeks and I came across the fact that the most studied planes are the translation planes. Is there any known example (of minimum order if possible) of a finite projective plane which is not a translation plane ?,"['projective-geometry', 'examples-counterexamples', 'finite-geometry', 'combinatorics', 'discrete-mathematics']"
4843329,"If whenever $\psi_n\rightharpoonup 0$, $A\psi_n \rightharpoonup 0$ then $A$ is bounded.","This if from a previous Princeton exam on functional analysis Let $A$ be a linear operator $A: X\rightarrow Y$ between normed vector spaces. If $\psi_n\rightharpoonup 0$ implies $A\psi_n \rightharpoonup 0$ , then $A$ is bounded. I tried to argue by contradiction. Boundedness is equivalent to continuity at zero, so suppose by way of contradiction that there exists a sequence $\psi_n \rightarrow0$ and $\varepsilon>0$ such that $\lVert A\psi_n\rVert\geq \varepsilon$ . Because $\psi_n\rightharpoonup 0$ this would imply $A\psi_n \rightharpoonup 0$ , in spite of the fact $\lVert A\psi_n\rVert\geq \varepsilon$ . I thought this might yield a contradiction, but then I remembered that $e_n\rightharpoonup0$ in $l_2$ in spite of the fact $\lVert e_n \rVert_\infty =1$ , so this is definitely not the way to go for a contradiction.","['functional-analysis', 'weak-convergence']"
4843342,Reference Request: proof that R-N Derivative is a ratio of PDFs for any two RVs on $\mathbb{R}$,"Consider any two continuous random variables defined on the Borel-measurable sets on $\mathbb{R}$ . I would like to reference a proof of the following statement: ""The distributions of the two random variables are related by a Radon-Nikodym derivative, and the derivative is the quotient of their PDFs"". I am looking for a text-book where the result is proven for the general case of any two continuous RVs.","['measure-theory', 'radon-nikodym', 'probability-theory', 'random-variables']"
4843357,"Necessary and sufficient conditions of linear separability of labelled cube $\{0,1\}^n$","This is a question I came up with when studying machine learning. A simple example revealing the limit of linear classifiers would be the four vertices of a square, one diagonal labelled $+1$ and the other $-1$ . What about the converse statement? For simplicity, consider a $\pm 1$ -labelling of $\{0,1\}^n$ not containing any rectangle with (both vertices of) one diagonal $+1$ and the other $-1$ . Would it necessarily be linearly separable? I have tried induction on dimension but failed as the separating plane is not unique. Neither did I find any counterexample in low dimensions. After all this is just a fun question and might not have a simple answer. Thank you in advance if you have any ideas or suggestions.","['machine-learning', 'combinatorics']"
4843422,Show that $\int_0^1\int_{1-y}^1\sqrt{(x-1)(y-1)(x+y-1)}\mathrm dx\mathrm dy=\frac{2\pi}{105}$.,"How can we show that $\int_0^1\int_{1-y}^1\sqrt{(x-1)(y-1)(x+y-1)}\mathrm dx\mathrm dy=\frac{2\pi}{105}$ ? Desmos says it's true. The inner indefinite integral is not nice . And strangely, when I plug in the inner limits of integration $x=1$ and $x=1-y$ , I get $0-0=0$ , contradicting the Desmos result. ( Edit: I should cancel factors in numerator and denominator, then I don't get $0-0$ . But then I get $\int_0^1 -\frac14 y^2\sqrt{y-1}\sinh^{-1}(i) \mathrm dy$ and I don't know how to continue.) Based on another question of mine about a double integral, I tried substituting $a=x-1$ and $b=y-1$ , which leads to $\int_{-1}^0\int_{-1-b}^0\sqrt{ab(a+b+1)}\mathrm da\mathrm db$ , which has another inner indefinite integral that is not nice. Context: I asked myself, ""A unit stick is broken at two uniformly random points; given that the three pieces form a triangle (which has probability $\frac14$ ), what is the expected area of the triangle? The answer is $\frac12$ times the integral in this question (which is supported by experimental trials, and also this article ). This question was inspired by another question about breaking a stick at random points.","['integration', 'definite-integrals', 'expected-value', 'calculus', 'closed-form']"
4843454,Hexagons whose vertices lie on a conic and whose sides touch a conic,"Yesterday I generalized the idea of ​​a bicentric quadrilateral Here is the following definition: A biconcentric hexagon is a hexagon whose vertices pass through a conic section and at the same time touch a conic section on its sides. We know from Brianchon's theorem that the red lines converge at a point, but what I found is that in a biconcentric hexagon, the blue lines converge at the same point, and the opposite is also true. This means that if the main diagonals of a hexagon tangent to a conic section converge at one point, the hexagon will be biconcentric. Is this feature already discovered? How do we prove that anyway?","['conic-sections', 'geometry']"
4843460,Is there a notion of tensor-completion of a category?,"Given an additive category $\mathcal{C}$ , sometimes one wants to consider its Karoubi envelope or idempotent completion $\mathrm{Kar}(\mathcal{C})$ , whose objects are summands of objects in $\mathcal{C}$ . So for instance, if $X\in Ob(\mathcal{C})$ , and $X\cong A\oplus B$ , then $A$ and $B$ are objects in $\mathrm{Kar}(\mathcal{C})$ . Formally, the object $A$ in $\mathrm{Kar}(\mathcal{C})$ is defined as a pair $(X,e_A)$ , where $e_A\in \mathrm{End}(X)$ satisfies $e_A^2=e_A$ . In my situation, I have a $k$ -linear, abelian, symmetric monoidal category $(\mathcal{D},\oplus,\otimes)$ , and I want to consider a new category $\mathrm{Ten}(\mathcal{D})$ whose objects include tensor factors of objects of $\mathcal{D}$ , that is, if $X\cong A\otimes B$ in $\mathcal{D}$ , I would like to view $A$ and $B$ as objects in $\mathrm{Ten}(\mathcal{D})$ . Does such a $\mathrm{Ten}(-)$ exist? How is it formally defined?","['abelian-categories', 'abstract-algebra', 'monoidal-categories', 'category-theory']"
4843512,Fisher information with known moments,"I have a sequence $X^n$ of length $n$ , where each $X_i$ takes a value from a finite set with probability vector $\mathbf{p} = [p_1, \ldots, p_K]^T$ , i.e., $X_i \in [K]$ , where $p_{X_i}(k) = p_k, k = 1, \ldots, K$ . In this case, I can show that the Fisher Information matrix $I_{X^n}(\mathbf{p}) := \mathbb{E}[\nabla \log p(X^n)\nabla \log p(X^n)^T]= \text{diag}(\frac{n}{p_1}, \ldots, \frac{n}{p_K})$ . My question is: can we compute $I_{X^n}(\mathbf{p})$ when additional information is available regarding moments of the distribution $\mathbf{p}$ (e.g., mean and variance), i.e., $\mathbb{E}_\mathbf{p}[\phi_j(X)]= c_j, j = 1, \ldots, m$ . My objective is to quantify the enhancement in the Cramer-Rao bound when incorporating side information about the moments. In fact, a lower bound on the Fisher Information would also work. While I am aware that there is existing work on constrained Cramer-Rao bounds, it tends to be intricate and occasionally unclear to me. I am hopeful though that one can establish more specific insights for this discrete setting with moment constraints (even if for some special cases such as mean and variance). Any assistance or guidance on this would be greatly appreciated.","['statistical-inference', 'statistics', 'probability-distributions', 'fisher-information', 'probability']"
4843545,Exterior derivative of 1,"In one of the examples of Loring W. Tu's Book ""Introduction to Manifolds"" (section 19.7 of 2nd ed.) the author takes the exterior derivative on both sides of the equation $x^2 + y^2 = 1$ wich results in $2x\,dx + 2y\,dy = 0$ . The left-hand side makes sense to me and, intuitively, the right-hand side too. But I don't understand what the exterior derivative of 1 is supposed to mean, since the exterior derivative acts on differential forms, then 1 what kind of differential form is supposed to be? What is its degree? And why should its exterior derivation be 0?
Thank you.","['exterior-derivative', 'manifolds', 'differential-forms', 'differential-geometry']"
4843565,Why are the coefficients of a smooth differential form smooth?,"Suppose $U$ is an open subset of $\mathbb R^n$ , and $\omega:U\to\operatorname{Alt}^k(\mathbb R^n)$ is a smooth differential $k$ -form. By $\operatorname{Alt}^k(\mathbb R^n)$ I mean the set of alternating $k$ -linear forms, and by ""smooth"" I mean that $\omega$ is infinitely differentiable (since $\operatorname{Alt}^k(\mathbb R^n)$ is a real normed space, we can make sense of $D\omega$ and $D^2\omega$ , etc., where $D$ denotes the Fréchet derivative). We can write $\omega$ in coordinates as $\sum_{I}\alpha_Idx^I$ , where each $\alpha_I:U\to\mathbb R$ is a function. However, how do we conclude that each $\alpha_I$ is smooth from the fact that $\omega$ is smooth?",['differential-geometry']
4843607,How do I solve $\lim_{p\to\infty}\left(\frac{S_{p}}{S_{p-1}}-\frac{S_{p-1}}{S_{p-2}}\right)$,"The Problem For reference, $$S_p=\sum_{n=1}^{\infty}\frac{n^{p}}{2^n}$$ This sequence is found on the OEIS as A000629 ( https://oeis.org/A000629 ). It can be easily shown that, $$S_p-1=\binom{p}{1}S_{p-1}+\binom{p}{2}S_{p-2}+\ldots+\binom{p}{p-1}S_{1}+\binom{p}{p}S_0$$ This formula is already well known and I have already proven it. Then I had proven the following: $$\frac{S_p}{S_{p-1}}\sim Cp$$ where $C$ is some non-zero constant. Now I went on to trying to find a $C$ that would best approximate this series because then an explicit formula would result. So this is, $$C=\frac{y_2-y_1}{x_2-x_1}=\frac{\frac{S_p}{S_{p-1}}-\frac{S_{p-1}}{S_{p-2}}}{p-(p-1)}=\frac{S_p}{S_{p-1}}-\frac{S_{p-1}}{S_{p-2}}$$ because $Cp$ approaches $\frac{S_p}{S_{p-1}}$ as $p\to\infty$ I add the limit and here is where my problem arises. That is, $$\lim_{p\to\infty}\left(\frac{S_{p}}{S_{p-1}}-\frac{S_{p-1}}{S_{p-2}}\right)$$ I can safely say that this limit will most likely go to $\frac{1}{\ln 2}$ as this is what occurs on Desmos and Wolfram when plotted; it is also the $C$ that can be seen on the OEIS that others have found. However when proving what the limit tends to, it wouldn't be sound to say ""It looks like it does, so it does"". My Attempts Using the Series Definition $$\lim_{p\to\infty}\left(\frac{S_{p}}{S_{p-1}}-\frac{S_{p-1}}{S_{p-2}}\right)=\lim_{p\to\infty}\left(\frac{\sum_{n=1}^{\infty}\frac{n^{p}}{2^n}}{\sum_{n=1}^{\infty}\frac{n^{p-1}}{2^n}}-\frac{\sum_{n=1}^{\infty}\frac{n^{p-1}}{2^n}}{\sum_{n=1}^{\infty}\frac{n^{p-2}}{2^n}}\right)$$ This of course does not look fun to solve and I figured it might have been a waste of my time but I attempted to make something of it anyways. I combined fractions, $$\lim_{p\to\infty}\left(\frac{\sum_{n=1}^{\infty}\frac{n^{p}}{2^n}}{\sum_{n=1}^{\infty}\frac{n^{p-1}}{2^n}}-\frac{\sum_{n=1}^{\infty}\frac{n^{p-1}}{2^n}}{\sum_{n=1}^{\infty}\frac{n^{p-2}}{2^n}}\right)=\lim_{p\to\infty}\left(\frac{\left(\sum_{n=1}^{\infty}\frac{n^{p}}{2^n}\right)\left(\sum_{n=1}^{\infty}\frac{n^{p-2}}{2^n}\right)-\left(\sum_{n=1}^{\infty}\frac{n^{p-1}}{2^n}\right)^2}{\left(\sum_{n=1}^{\infty}\frac{n^{p-1}}{2^n}\right)\left(\sum_{n=1}^{\infty}\frac{n^{p-2}}{2^n}\right)}\right)$$ This still unfortunately got me nowhere as I didn't know how to interpret nor simplify the multiplied sums. Using the Recursive Definition I have tried using the recursive definition of the limit but still that doesn't get me anything of interest. $$C=\lim_{p\to\infty}\left(\frac{\binom{p}{1}S_{p-1}+\ldots+\binom{p}{p}S_{0}+1}{\binom{p}{2}S_{p-2}+\ldots+\binom{p}{p}S_{0}+1}-\frac{\binom{p}{2}S_{p-2}+\ldots+\binom{p}{p}S_{0}+1}{\binom{p}{3}S_{p-3}+\ldots+\binom{p}{p}S_{0}+1}\right)$$ Then I know that the 1s become negligible as $p\to\infty$ then I also used that $\binom{n}{k}\sim\frac{n^k}{k!}$ $$C=\lim_{p\to\infty}\left(\frac{\frac{p^1}{1!}S_{p-1}+\ldots+\frac{p^p}{p!}S_{0}}{\frac{p^2}{2!}S_{p-2}+\ldots+\frac{p^p}{p!}S_{0}}-\frac{\frac{p^2}{2!}S_{p-2}+\ldots+\frac{p^p}{p!}S_{0}}{\frac{p^3}{3!}S_{p-3}+\ldots+\frac{p^p}{p!}S_{0}}\right)=\lim_{p\to\infty}\left(\frac{\frac{p^1}{1!}S_{p-1}}{\frac{p^2}{2!}S_{p-2}+\ldots+\frac{p^p}{p!}S_{0}}-\frac{\frac{p^2}{2!}S_{p-2}}{\frac{p^3}{3!}S_{p-3}+\ldots+\frac{p^p}{p!}S_{0}}\right)$$ This is where I am know and still I have no clue how to proceed with this limit. If anyone has any ideas at all on where to begin that would be excellent. Thanks!","['limits', 'summation', 'sequences-and-series']"
4843647,First fundamental theorem of Calculus continuity not necessary?,"I know if $f:[a,b] \to \mathbb{R}$ is continuous, then $g(x) = \int _{a}^{x}f(t) \, dt$ is differentiable on $[a,b]$ . Furthermore, $g'(x) = f(x)$ . This is known as the first fundamental theorem of Calculus. However, I wonder if the converse is true, that is when $g(x)$ is differentiable if $f(x)$ is necessarily continuous. I don't think this is the case, but neither can I conceive a counterexample. I was thinking about this because I think even if the distribution of a random variable is differentiable, the density is not necessarily continuous. I think as the comment said, we just need to change a continuous $f$ at one point so that it is no longer continuous, but $g$ is not changed. I'll link the following somewhat unrelated question for future reference.","['integration', 'measure-theory', 'real-analysis', 'calculus', 'probability-theory']"
4843651,Adding commutation rules to a free group?,"I'm interested in knowing how, given a set $S$ , one can modify the free group $F(S)$ by adding one or more commutation rules and get a new group. For instance, adding the commutation rule: $$\forall x,y\in S, xy=yx$$ yields the free abelian group generated by the base $S$ . So I'm wondering: does any commutation rule work? For instance, if I say $$\forall x,y\in S, xy=yx^2$$ or even that, for a fixed $z\in S$ , $$\forall y\in S,y\neq z, zy=y^{-1}z^2$$ the first is only possible in the trivial group ( $x=x1=1x^2=x^2$ ) but the second does seem to make sense, at least at a first glance, Which, in general, of these rules produces a new group as compared to those which induce a contradiction or work only in the trivial group? Is there for example a big family of such rules that will tune the original free group into a different, non trivial one? [ Note: I'm not sure what a proper definition of ""commutation rule"" could be. I just threw some random examples that fit my intuition for such a term.] In particular, take the following example. As a continuation of my previous post ( Extending automorphism to inner automorphism? ), I think I've come to a solution to the following problem. Let $H$ and $K$ be subgroups of a finite group $G$ and assume $H$ is isomorphic to $K$ . Prove that there exists a group $\bar G$ containing $G$ as a subgroup, such that $H$ and $K$ are conjugate in $\bar G$ . (Problem 56 on this problem set .) Let $\phi:H\to K$ be an isomorphism. My idea is to modify the free group in $G\cup\{x\}$ , $F(G\cup\{x\})$ , by letting all the elements of $G$ multiply themselves as usual and by adding the following commutation property. $$\forall h\in H,xh=\phi(h)x$$ Call this new group $\bar G$ . This way, $G\leq\bar G$ and for all $h\in H$ , $^xh=\phi(h)$ and in particular $^xH=K$ . [ Note: I know I've added another rule (the one about the elements of $G$ multiplying themselves as usual) but that one could radically be seen as a ""commutation property"": $\forall g,h\in G, gh=g\cdot_Gh$ .] As I'm not sure if this definition is enough by itself, here's a more schematic definition of $\bar G$ . $$\bar G=\{g_1x^{k_1}g_2x^{k_2}\ldots g_nx^{k_n}:g_i\in G,g_{i>1}\notin H,k_i\in\mathbb{Z},k_{i<n}\neq 0\}$$ Two elements are equal if they have the same canonical form, i.e. the same expression as an interleaved product of elements of $G$ and powers of $x$ , subject of the same conditions that I included in the previous definition. The product of two elements is carried out by concatenating their canonical forms and then commuting all the $g_i$ 's that belong to $H$ , possibly cancelling out some powers of $x$ in the process: since there are only finitely many powers of $x$ in their expressions, this process necessarily ends, yielding the canonical form of their product. Does my ""commutation rule"" actually generate a new group here? If so, is it a special case or is there a bigger family of ""commutation rules"" that will produce a group?","['group-theory', 'free-groups']"
4843660,asypmtotic series of $\tan x$ in terms of $x \cos x$,"I have come across an asymptotic series of $\tan x$ in the form of $\tan x \sim x\cos x + \frac{5}{6}(x\cos x)^3 + \frac{161}{120}(x\cos x)^5$ as $x\to 0$ , but I have no idea how this is derived.
I am only familiar with the result $\tan x \sim x + x^3/3 + 2/15 x^5$ by Taylor expansion.
How is the expansion around $x\cos x$ obtained?","['calculus', 'asymptotics', 'taylor-expansion', 'analysis']"
4843675,Equivalent definitions of Chern classes,"I have met two different definitions for Chern classes and I am wondering how are they all the same? The first one is from Jöran Schlömer's book and they define the Chern class of a complex vector bundle $E \to M$ via the coefficients of $\det\left(tI - \frac{1}{2\pi i}A\right)= \sum_{k=0}^n f_k(A)t^{n-k}$ by setting $$c_k(E) = [f_k(\Omega)]$$ where $\Omega$ is the curvature matrix. The problem I have with this one is that I think it should be $$c_k(E) = \left[f_k\left(\frac{1}{2\pi i} \Omega\right)\right]$$ instead, but I'm no longer entirely sure. The second definition is by Tu  and he states that the Chern classes are obtained from $$\det\left(I + \frac{i}{2\pi}\Omega\right)=1+c_1(E)+\dots+c_n(E).$$ This I think is the total Chern class instead? If anyone could elaborate on why these two should give me back the same thing and whether or not it should be $$c_k(E) = [f_k(\Omega)]$$ or $$c_k(E) = \left[f_k\left(\frac{1}{2\pi i} \Omega\right)\right]$$ I would be glad. I think this might be due to some sign conventions.","['characteristic-classes', 'differential-geometry']"
4843747,The sum of the squares of the diagonals in a polygon,"The first question that got me here:
A regular dodecagon $P_1 P_2 P_3 \dotsb P_{12}$ is inscribed in a circle with radius $1.$ Compute $
{P_1 P_2}^2 + {P_1 P_3}^2 + {P_1 P_4}^2 + \dots + {P_{10} P_{11}}^2 + {P_{10} P_{12}}^2 + {P_{11}P_{12}}^2.$ (The sum includes all terms of the form ${P_i P_j}^2$ where $1 \le i < j \le 12.$ We write $P_iP_j$ to mean the length of segment $\overline{P_iP_j}$ .) I solved the question and got 144. Since this is  I wondered if the total would be true for any n-gon?. More specifically, for any n-gon would this total be equal to $n^2$ . This is Dodecagon:","['euclidean-geometry', 'geometry', 'polygons']"
4843748,"If $a$ and $b$ are elements of a group $G$ that satisfy the same first order formulas, is there always an automorphism of $G$ that maps $a$ to $b$?","Let $a$ and $b$ be elements of group $G$ and assume that for any natural number $n$ and for any first-order formula $\varphi (x_1,x_2,...,x_n)$ with $n$ free variables in the language of groups, $G \models (\varphi (a,a, ... , a) \leftrightarrow \varphi (b,b,...,b))$ Is there always a group automorphism of $G$ that maps $a$ to $b$ ? If $b$ is the image of $a$ under an automorphism of $G$ , then $a$ and $b$ satisfy the same first order formulas. The question is whether the converse of that statement holds or not?","['group-theory', 'first-order-logic', 'model-theory']"
4843794,"Is this equality obvious? (""Calculus on Manifolds"" by Michael Spivak)","I am reading ""Calculus on Manifolds"" by Michael Spivak. I can prove the following equality, but is it obvious? $$\text{Alt}(T)(v_1,\dots,v_j,\dots,v_i,\dots,v_k)\\=\frac{1}{k!}\sum_{\sigma\in S_k}\text{sgn }\sigma\cdot T(v_{\sigma(1)},\dots,v_{\sigma(j)},\dots,v_{\sigma(i)},\dots,v_{\sigma(k)}).$$ My proof of the above equality is here (I used Theorem 4-3(1).): $$
\text{Alt}(T)(v_1,\dots,v_j,\dots,v_i,\dots,v_k)\\=
-\text{Alt}(T)(v_1,\dots,v_k)
\\=
\frac{1}{k!}\sum_{\sigma\in S_k}-\text{sgn }\sigma'\cdot T(v_{\sigma'(1)},\dots,v_{\sigma'(k)})
\\=\frac{1}{k!}\sum_{\sigma\in S_k}\text{sgn }\sigma\cdot T(v_{\sigma'(1)},\dots,v_{\sigma'(i)},\dots,v_{\sigma'(j)},\dots, v_{\sigma'(k)})
\\=\frac{1}{k!}\sum_{\sigma\in S_k}\text{sgn }\sigma\cdot T(v_{\sigma(1)},\dots,v_{\sigma(j)},\dots,v_{\sigma(i)},\dots,v_{\sigma(k)}).
$$ My proof of Theorem 4-3(1) is as follows: Let $\sigma_1:=(i,j)$ . $$\text{Alt}(T)(v_1,\dots,v_j,\dots,v_i,\dots,v_k)\\=\text{Alt}(T)(v_{\sigma_1(1)},\dots,v_{\sigma_1(i)},\dots,v_{\sigma_1(j)},\dots,v_{\sigma_1(k)})
\\=\frac{1}{k!}\sum_{\sigma\in S_k}\text{sgn }\sigma\cdot T(v_{\sigma_1(\sigma(1))},\dots,v_{\sigma_1(\sigma(i))},\dots,v_{\sigma_1(\sigma(j))},\dots,v_{\sigma_1(\sigma(k))})\\=
\frac{1}{k!}\sum_{\sigma\in S_k}\text{sgn }\sigma_1\cdot\text{sgn }(\sigma_1\circ\sigma)\cdot T(v_{\sigma_1(\sigma(1))},\dots,v_{\sigma_1(\sigma(i))},\dots,v_{\sigma_1(\sigma(j))},\dots,v_{\sigma_1(\sigma(k))})
\\=-\frac{1}{k!}\sum_{\sigma\in S_k}\text{sgn }(\sigma_1\circ\sigma)\cdot T(v_{\sigma_1(\sigma(1))},\dots,v_{\sigma_1(\sigma(i))},\dots,v_{\sigma_1(\sigma(j))},\dots,v_{\sigma_1(\sigma(k))})
\\=-\text{Alt}(T)(v_1,\dots,v_k).
$$","['permutations', 'multivariable-calculus', 'tensors']"
4843801,"Whether the series converges uniformly on [0,1]","Consider the following series $$\sum_{n=2}^{\infty}\frac{xe^{-nx}}{\ln n}.$$ It is easy to see that it converges uniformly on $[a,1]$ for any $a>0$ . I want to show that it does converge uniformly on $[0,1]$ . I have tried some values like $x=1/n$ and then letting $n\to\infty$ but this did not work: $$\sum_{n=k}^{2k}\frac{1/k\cdot e^{-n/k}}{\ln n}\geq(k+1)\frac{e^{-2}}{k\ln 2k}\sim\frac{1}{e^2\ln 2k}$$","['uniform-convergence', 'analysis', 'sequences-and-series']"
4843812,What are the local maxima and minima of $\frac{\sin(nx)}{\sin(x)}$,"We know the global maxima of the function $\sin(nx)/\sin(x)$ is $n$ (thanks to this question ), but what are the local maxima and minima points of the function in ( $-\pi,\pi$ )? (it represents the intensity of a narrow slit diffraction grating ) I tried equating the derivative to $0$ and the equation to be solved is $n\tan(x) = \tan(nx)$ . I am unable to progress from here. I also tried expanding $\sin(nx)$ using multiple-angle formula but made no progress. For convenience: 1st derivative $$ = \frac{n\cos(nx)\sin(x) - \sin(nx)\cos(x)}{\sin(x)^2}$$ and 2nd derivative = $$\frac{2\sin(nx)\cos(x)^2 + \sin(nx)\sin(x)^2 - n^2\sin(nx)\sin(x)^2 - 2n\cos(nx)\cos(x)\sin(x)}{\sin(x)^3}$$ Here is a plot of the function in Desmos: https://www.desmos.com/calculator/kt0hntsbcb","['maxima-minima', 'functions', 'derivatives']"
4843849,Calculate determinant of $n$-th order,"I have problem with creating a upper triangular matrix (in order to calculate the determinant) of the following matrix: $$\begin{pmatrix}
 1&  2&  3&  ...& n-2 &n-1  &1 \\ 
 1&  2&  3&  ...&  n-2&  1&n \\ 
 1&  2&  3&  ...&  1&n-1  &n \\ 
 .&  .&  .&  .&  .&  .& \\
 .&  .& .& .& .& .& \\
 1&  2&  1&  ...& n-2 &n-1  &n \\ 
 1&  1&  3&  ...&  n-2& n-1 &n \\ 
 1&  2&  3&  ...&  n-2& n-1 &n 
\end{pmatrix}$$ One of my attempts included subtracting the first row from the others which got me to this point: $$\begin{pmatrix}
 1&  2&  3&  ...& n-2 &n-1  &1 \\ 
 0&  0&  0&  ...&  0&  2-n&n-1 \\ 
 0&  0&  0&  ...&  3-n&0 &n-1 \\ 
 .&  .&  .&  .&  .&  .& \\
 .&  .& .& .& .& .& \\
 0&  0&  -2&  ...& 0 &0 &n-1 \\ 
 0&  -1&  0&  ...&  0&0 &n-1 \\ 
 0&  0&  0&  ...&  0&0 &n-1 
\end{pmatrix}$$ Any ideas on how to move on from here ?","['matrices', 'determinant']"
4843865,"Uniqueness of the writing $f(x)=cx+p(x)$, $p$ periodic","Show that, if a function $f:\mathbb{R}\rightarrow \mathbb{R}$ , defined as $f(x)=cx+p(x)$ , is the sum of a ""linear part"" $x\mapsto cx$ and a ""periodic part"" $x\mapsto p(x)$ , where $p$ is $\tau$ -periodic, then that writing is unique, i.e. if $f(x)=mx+q(x)$ , with $q$ $\mu$ -periodic, then $p=q$ and $c=m$ . Here $\tau,\mu>0$ . What I have done It's easy to see that, for every $x\in\mathbb{R}$ , $$p(x)-q(x)=(m-c)x \tag{1}$$ Furthermore, for every $n,r\in\mathbb{Z}$ we have $$f(x+n\tau)=c(x+n\tau)+p(x) \tag{2}$$ $$f(x+r\mu)=m(x+r\mu)+q(x) \tag{3}$$ and therefore, (2)-(3) and (1) give, for every $x\in\mathbb{R}$ and $n,r\in\mathbb{Z}$ $$f(x+n\tau)-f(x+r\mu)=cn\tau-mr\mu\tag{4}$$ Hypothesis: $\frac{\tau}{\mu} \in \mathbb{Q}$ In this case there are $n',r'\in\mathbb{Z}-\{0\}$ such that $$n'\tau=r'\mu \tag{5}$$ therefore, from (4) and (5) we have $$0=cn'\tau-mr'\mu=n'\tau(c-m)$$ so $c=m$ and, for (1), $p(x)=q(x)$ for every $x\in\mathbb{R}$ . How can we proceed when $\frac{\tau}{\mu} \in \mathbb{R}-\mathbb{Q}$ ? This hypothesis must give a contradiction. Any comments or answers will be appreciated. Thanks in advance.","['periodic-functions', 'functions', 'real-analysis']"
4843901,Limit of a double integral over the disk,"I am stuck with the following request. Compute $$
\lim_{r\to+\infty}e^{-r}\int_{B(0,r)}e^{|x|+|y|}dxdy
$$ For the integral, one gets $$
\int_{B(0,r)}e^{|x|+|y|}dxdy=4\int_{0}^r\int_0^{\sqrt{r^2-x^2}}e^{x+y}dxdy=4\int_{0}^r e^x(e^{\sqrt{r^2-x^2}}-1)dx
$$ but with this integral I don't know how to proceed (I tried with $x=r\cos(\theta)$ with no success).","['integration', 'multivariable-calculus']"
4843903,"I think the author's proof of Theorem 4-4(1) is not correct. (""Calculus on Manifolds"" by Michael Spivak)","I am reading ""Calculus on Manifolds"" by Michael Spivak. I think the author's proof of Theorem 4-4(1) is not correct. Am I right? The author wrote as follows in the proof of Theorem 4-4(1): $$\sum_{\sigma\in G\cdot\sigma_0} \text{sgn }\sigma\cdot S(v_{\sigma(1)},\dots,v_{\sigma(k)})\cdot T(v_{\sigma(k+1)},\dots,v_{\sigma(k+l)}) =\left[\text{sgn }\sigma_0\cdot\sum_{\sigma'\in G}\text{sgn }\sigma'\cdot S(w_{\sigma'(1)},\dots,w_{\sigma'(k)})\right]\cdot T(w_{k+1},\dots,w_{k+l})
=0.$$ But I think the following is correct: $$\sum_{\sigma\in \sigma_0\cdot G} \text{sgn }\sigma\cdot S(v_{\sigma(1)},\dots,v_{\sigma(k)})\cdot T(v_{\sigma(k+1)},\dots,v_{\sigma(k+l)})
\\=\sum_{\sigma'\in G} \text{sgn }(\sigma_0\cdot\sigma')\cdot S(v_{\sigma_0\cdot\sigma'(1)},\dots,v_{\sigma_0\cdot\sigma'(k)})\cdot T(v_{\sigma_0\cdot\sigma'(k+1)},\dots,v_{\sigma_0\cdot\sigma'(k+l)})
\\=\text{sgn }\sigma_0\cdot\sum_{\sigma'\in G} \text{sgn }\sigma'\cdot S(w_{\sigma'(1)},\dots,w_{\sigma'(k)})\cdot T(w_{\sigma'(k+1)},\dots,w_{\sigma'(k+l)})
\\=\text{sgn }\sigma_0\cdot\sum_{\sigma'\in G} \text{sgn }\sigma'\cdot S(w_{\sigma'(1)},\dots,w_{\sigma'(k)})\cdot T(w_{k+1},\dots,w_{k+l})
\\=\left[\text{sgn }\sigma_0\cdot\sum_{\sigma'\in G}\text{sgn }\sigma'\cdot S(w_{\sigma'(1)},\dots,w_{\sigma'(k)})\right]\cdot T(w_{k+1},\dots,w_{k+l})
=0.$$ The author defined $w_1,\dots,w_{k+l}$ as follows: $v_{\sigma_0(1)},\dots,v_{\sigma_0(k+l)}=w_1,\dots,w_{k+l}$ . So, $v_{\sigma_0\cdot\sigma'(1)},\dots,v_{\sigma_0\cdot\sigma'(k+l)}=w_{\sigma'(1)},\dots,w_{\sigma'(k+l)}$ .","['multivariable-calculus', 'tensors', 'multilinear-algebra']"
4843914,What is the expected area of a triangle in which each side is a random real number between $0$ and $1$?,"Let $a,b,c$ be three independent uniformly random real numbers between $0$ and $1$ . Given that there exists a triangle with side lengths $a,b,c$ , what is the expected area of the triangle? Using Heron's formula , the area is $\frac14\sqrt{(a+b+c)(-a+b+c)(a-b+c)(a+b-c)}$ . The expected area should be a triple integral with respect to $a,b,c$ , but I don't know how to set up the limits of integration. Previously I was able to set up an integral to answer a related question: ""What is the expected area of a random triangle with perimeter $1$ ?"" I tried to utilize my approach there, by asking ""What is the expected area of a random triangle with perimeter $p$ ?"" and then integrating from $p=0$ to $p=3$ . But each value of $p$ has a different ""weight"", so I think I should somehow take into account the distribution of $p$ , but I'm not sure how. Then, for the anti-derivative, I'm not sure what substitution would disengage $a,b,c$ to allow for indefinite integration. A simulation with $10^7$ trials yields an average area of $0.11600$ .","['integration', 'area', 'geometry', 'expected-value', 'triangles']"
4843928,Convergence of a sum as limit tends to infinity that seems to be harmonic series,"I have come across a mathematical problem that is to evaluate the expression: $$
lim_{n\rightarrow\infty}  \left\{\frac{1}{\sqrt{2n-1^2}}+\frac{1}{\sqrt{4n-2^2}}+\frac{1}{\sqrt{6n-3^2}}+...+\frac{1}{\sqrt{2n^2-n^2}}\right\}\tag1$$ and I have considered the following approach: $$
y=lim_{n\rightarrow\infty} \left\{{\frac{1}{n}}\left[\frac{1}{\sqrt{\frac{2}{n}-\frac{1}{n^2}}}+\frac{1}{\sqrt{\frac{4}{n}-\frac{4}{n^2}}}+\frac{1}{\sqrt{\frac{6}{n}-\frac{9}{n^2}}}+...1\right]\right\}
\tag2$$ $$
y=lim_{n\rightarrow\infty}\left\{\Sigma_{i=1}^{n}\frac{1}{\sqrt{\frac{2}{i}-\frac{1}{i^2}}}\frac{1}{i}\right\}
\tag3$$ $$
y=\int_0^1f(x)dx
\tag4$$ where it is considered that $$f(x)=\frac{1}{\sqrt{2x-x^2}}\tag5$$ and $a=0$ while $b=1$ . Integrating thus, we get: $$y=\int_0^1\frac{dx}{\sqrt{2x-x^2}}=[sin^{-1}(x-1)]_0^1=\pi/2\tag6$$ Therefore, the integration says that the sum up to infinite series is $\pi/2$ .
But I have a query here, which is, if I write the terms of the series, this is what I get here: $$
y=lim_{n\rightarrow\infty}\left\{{1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\frac{1}{5}}+...\frac{1}{n}\right\}
\tag7$$ Isn't this a harmonic series? But if so, harmonic series tend to diverge, then how can the infinite sum above be integrated and summed up to a definite value as it has converged? I am being confused.","['integration', 'riemann-sum', 'sequences-and-series', 'limits', 'convergence-divergence']"
4843967,Can the degree of a function take any value between $2$ and $2^n?$,"For each $n\in\mathbb{N},$ define $[n]:=\{1,2,\ldots,n\}.$ For a function $f:[n]\to[n],$ we say $D\subseteq[n]$ is invariant under $f$ if $f(D)\subseteq D.$ Let $\deg(f)$ be the number of invariant subsets under $f.$ Can $\deg(f)$ be any natural number between $2$ and $2^n?$ If not, what values are not possible? Note that the degree must be atleast $2$ since $\emptyset$ and $[n]$ are invariant under any function. For each $k\in[n],$ there exists a function $f:[n]\to[n]$ with $\deg(f)=2^k.$ Just map each element in $[k-1]$ to itself. Now, if $x\in[n-1]\setminus[k-1],$ let $f(x)=x+1.$ Let $f(n)=k.$ Some counting shows that this function will have $2^{k-1}+2^{k-1}=2^k$ subsets that are invariant under it. We can also achieve $2^{n-1}+1.$ Just map each element to $1.$ Then, any subset of $[n]$ containing $1$ will be invariant under it. There are $2^{n-1}$ of these. We also have to add $1$ to account for $\emptyset.$ Any other subset will not be invariant under this function. Hence, this function will have degree $2^{n-1}+1.$ We can also achieve $2^{k-1} + 2^{n-1}$ for each $k\in[n].$ Map everything in $[k]$ to itself. Map the rest to $k.$ This function will have $2^k+[(2^{n-k}-1)(2^{k-1})]=2^{k-1} + 2^{n-1}$ subsets that are invariant under it. Heuristically speaking, it seems to me that we should be able to cover all values between $2$ and $2^n.$ There are a total of $n^n$ functions on $[n]$ and only $2^n-1$ values for the degree to be. It seems to me that there's a lot of freedom in how we can modify a function to change it's degree. I was also thinking of writing the numbers as sums of distinct powers of $2.$ As in, if we write $13=2^3+2^2+2^0,$ maybe that will give us some insight on how to construct a function with degree $13.$ I haven't thought much about this idea yet, though. An interesting property is that if $f:[n]\to[n]$ is a bijection, then $\deg(f)$ is a power of $2.$ In fact, $\deg(f)=2^m,$ where $m$ is the number of cycles that $f$ is a product of. This is because any invariant subset must be formed by picking some of these $m$ cycles. There are obviously ${m\choose0}+{m\choose1}+\ldots+{m\choose m}=2^m$ ways to do this. We may ask another question at this stage. Let the degree of $f$ be a power of $2.$ Must $f$ be a bijection? Since for $[n]$ being a bijection is equivalent to being one-one which is in turn equivalent to being onto, we may be able to answer this easily. How do we proceed?","['contest-math', 'functions', 'combinatorics']"
4844055,Fundamental stress and pressure tensors of the Stokes system in $\mathbb{R}^3$.,"Let $\mathbf{\mathcal{G}}$ denote the Oseen-Burgers tensor and $\mathbf{\Pi}$ denote the fundamental pressure vector in $\mathbb{R}^3$ , i.e. on components we have $$\mathbf{\mathcal{G}}_{jk}(\mathbf{x}-\mathbf{x_0})=\frac{1}{8\pi}\left\{\frac{\delta_{jk}}{|\mathbf{x}-\mathbf{x_0}|}+\frac{\hat{x}_j\hat{x}_k}{|\mathbf{x}-\mathbf{x_0}|^3}\right\}, \mathbf{\Pi}_k(\mathbf{x}-\mathbf{x_0})=\frac{1}{8\pi}\frac{2\hat{x}_k}{|\mathbf{x}-\mathbf{x_0}|^3},$$ where $$\hat{\mathbf{x}}:=\mathbf{x}-\mathbf{x_0}=(\hat{x}_1,..., \hat{x}_n).$$ Then, the pair $(\mathbf{\mathcal{G}}, \mathbf{\Pi})$ satisfies the following equations: $$\Delta_{\mathbf{x}}\mathbf{\mathcal{G}}(\mathbf{x}-\mathbf{x_0})-\nabla_{\mathbf{x}}\mathbf{\Pi}(\mathbf{x}-\mathbf{x_0})=-\delta_{\mathbf{x_0}}(\mathbf{x})\mathbb{I}, \operatorname{div}_{\mathbf{x}}\mathbf{\mathcal{G}}(\mathbf{x}-\mathbf{x_0})=0, \forall \mathbf{x}\in \mathbb{R}^3.$$ This part I understand and it is all clear, but the following result I do not understand. Let $\mathbf{S}(S_{ijk})$ denote the fundamental stress tensor of the Stokes system in $\mathbb{R}^3$ and let $\mathbf{\Lambda}(\Lambda_{ik})$ denote the fundamental pressure tensor of the Stokes system in $\mathbb{R}^3$ , i.e. $$S_{ijk}(\mathbf{x}-\mathbf{x_0}):=-\mathbf{\Pi}_j(\mathbf{x}-\mathbf{x_0})\delta_{ik}+\frac{\partial \mathbf{\mathcal{G}}_{ij}}{\partial x_k}(\mathbf{x}-\mathbf{x_0})+\frac{\partial \mathbf{\mathcal{G}}_{kj}}{\partial x_i}(\mathbf{x}-\mathbf{x_0})=-\frac{3}{4\pi}\frac{\hat{x}_i\hat{x}_j\hat{x}_k}{|\mathbf{x}-\mathbf{x_0}|^5},$$ $$\Lambda_{ik}(\mathbf{x}-\mathbf{x_0}):=-2\frac{\partial \Pi_i(\mathbf{x}-\mathbf{x_0})}{\partial x_k}=-2\frac{\Pi_k(\mathbf{x}-\mathbf{x_0})}{\partial x_i}=\frac{1}{2\pi}\left(-\frac{\delta_{ik}}{|\mathbf{x}-\mathbf{x_0}|^3}+3\frac{\hat{x}_i\hat{x}_k}{|\mathbf{x}-\mathbf{x_0}|^5}\right).$$ Then, I am told that the pair $(S_{ijk}, \Lambda_{ik})$ satisfies the Stokes system in $\mathbb{R}^3$ whenever $\mathbf{x}\ne\mathbf{x_0}$ , i.e. $$\begin{cases} 
-\Delta_{\mathbf{x_0}}S_{ijk}(\mathbf{x}-\mathbf{x_0})+\frac{\partial \Lambda_{ik}(\mathbf{x}-\mathbf{x_0})}{\partial x_{0;j}}=0, \\
\frac{\partial S_{ijk}(\mathbf{x}-\mathbf{x_0})}{\partial x_{0;j}}=0,
\end{cases}$$ where $\mathbf{x_0}=(x_{0;1}, ..., x_{0:n})$ . Of course, the Einstein summation convention is used. This claim is supposedly obvious using the relations I know about $\mathbf{\mathcal{G}}$ and $\mathbf{\Pi}$ , but I cannot see how to obtain it. I tried writing on components the fact that $(\mathbf{\mathcal{G}}, \mathbf{\Pi})$ is a fundamental solution of the Stokes system and using the symmetry of these tensors, but I got nowhere.","['fluid-dynamics', 'multivariable-calculus', 'mathematical-physics', 'partial-differential-equations']"
4844099,"If $u \notin L^p[0,1]$, can we find some $w \in L^{\frac{p}{p-1}}[0,1]$ such that $\int_0^1 u \cdot w = \infty$?","The question is as in the title. For some fixed $p \in (1,\infty)$ , let $p' \in (1,\infty)$ be such that $\frac{1}{p}+\frac{1}{p'}=1$ . Then, for any $u \in L^1[0,1] - L^p[0,1]$ , I wonder if it is possible to find some $w \in L^{p'}[0,1]$ such that \begin{equation}
\int_0^1 u \cdot w = \infty
\end{equation} where we assume $u$ and $w$ to be $\mathbb{R}^N$ -valued for some $N \in \mathbb{N}$ . I think this must be true, but cannot really prove myself.. Could anyone please help me?","['measure-theory', 'lebesgue-measure', 'holder-inequality', 'lp-spaces']"
4844134,"About $\operatorname{Alt}(\varphi_{i_1}\otimes\cdots\otimes\varphi_{i_k})$ (""Calculus on Manifolds"" by Michael Spivak)","I am reading ""Calculus on Manifolds"" by Michael Spivak. The author wrote as follows: Since each $\operatorname{Alt}(\varphi_{i_1}\otimes\cdots\otimes\varphi_{i_k})$ is a constant times one of the $\varphi_{i_1}\wedge\cdots\wedge\varphi_{i_k}$ , these elements span $\Lambda^k(V)$ . I wonder why this holds. My proof of the above fact: Let $\omega_1,\dots,\omega_n\in\Lambda^1(V)$ . By the definition of the wedge product, $$\omega_1\wedge\omega_2=\frac{(1+1)!}{1!1!}\operatorname{Alt}(\omega_1\otimes\omega_2).$$ So, $\operatorname{Alt}(\omega_1\otimes\omega_2)=\frac{1}{2}(\omega_1\wedge\omega_2)$ . By Theorem 4-4(3) in the book, $$\omega_1\wedge\omega_2\wedge\omega_3=\frac{(1+1+1)!}{1!1!1!}\operatorname{Alt}(\omega_1\otimes\omega_2\otimes\omega_3).$$ So, $\operatorname{Alt}(\omega_1\otimes\omega_2\otimes\omega_3)=\frac{1}{6}(\omega_1\wedge\omega_2\wedge\omega_3)$ . By Theorem 4-4(2), $\operatorname{Alt}(\operatorname{Alt}(\omega\otimes\eta)\otimes\theta)=\operatorname{Alt}(\omega\otimes\eta\otimes\theta)$ . $$\operatorname{Alt}(\omega_1\otimes\omega_2\otimes\omega_3\otimes\omega_4)
=\operatorname{Alt}(\omega_1\otimes\omega_2\otimes(\omega_3\otimes\omega_4))
=\operatorname{Alt}(\operatorname{Alt}(\omega_1\otimes\omega_2)\otimes(\omega_3\otimes\omega_4)) \\ 
=\operatorname{Alt}\left(\frac{1}{2}(\omega_1\wedge\omega_2)\otimes\omega_3\otimes\omega_4\right)
=\frac{1}{12}(\omega_1\wedge\omega_2\wedge\omega_3\wedge\omega_4).$$ Let $n\geq 3$ . Similarly, $\operatorname{Alt}(\omega_1\otimes\cdots\otimes\omega_n)=\frac{1}{6}\left(\frac{1}{2}\right)^{n-3}(\omega_1\wedge\cdots\wedge\omega_n)$ . So, $\operatorname{Alt}(\varphi_{i_1}\otimes\cdots\otimes\varphi_{i_k})=\frac{1}{6}\left(\frac{1}{2}\right)^{k-3}(\varphi_{i_1}\wedge\cdots\wedge\varphi_{i_k})$ if $k\geq 3$ . Is my proof ok? If my proof is not ok, please tell me a proof. If my proof is ok, please tell me a better standard proof.","['multivariable-calculus', 'tensors', 'multilinear-algebra']"
4844145,Trigonometric formula for solving quadratic equations,"I provide an alternative trigonometric formula for solving quadratic equations where $a$ , $b$ , and $c$ are non-zero real numbers. Theorem 1 . If $ax^2+bx+c=0$ , then $$x_{1,2}=\left(1\pm\frac{2}{\tan{\frac{\theta}{2}\mp1}}\right)\sqrt{\frac{c}{a}},$$ where $\theta=\sec^{-1}{\left(\frac{b}{2\sqrt{ac}}\right)}$ . Proof . Multiplying by $a$ the quadratic equation we have $$(ax)^2+b(ax)+ac=0.$$ Let's make the substitution $\sec{\theta}=\frac{b}{2\sqrt{ac}}$ , then $$\begin{aligned}0&=(ax)^2+b(ax)+ac\\&=(ax)^2+2\sqrt{ac}(ax)\sec{\theta}+ac\\&=\sec{\theta}\left((ax)^2\cos{\theta}+2\sqrt{ac}(ax)+ac\cos{\theta}\right)\\&=\cos{\theta}((ax)^2+ac)+2\sqrt{ac}(ax)\\&= \cos{\theta}(ax+\sqrt{ac})^2-2\sqrt{ac}(ax)\cos{\theta}+2\sqrt{ac}(ax)\\&= \left(\cos^2{\frac{\theta}{2}}-\sin^2{\frac{\theta}{2}}\right)(ax+\sqrt{ac})^2-2\sqrt{ac}(ax)\left(1-2\sin^2{\frac{\theta}{2}}\right)+2\sqrt{ac}(ax)\\&=  \cos^2{\frac{\theta}{2}}(ax+\sqrt{ac})^2 -\sin^2{\frac{\theta}{2}}(ax+\sqrt{ac})^2 +4\sqrt{ac}(ax)\sin^2{\frac{\theta}{2}}\\&=   \cos^2{\frac{\theta}{2}}(ax+\sqrt{ac})^2 -\sin^2{\frac{\theta}{2}}\left((ax+\sqrt{ac})^2 -4\sqrt{ac}(ax)\right)\\&= \cos^2{\frac{\theta}{2}}(ax+\sqrt{ac})^2 -\sin^2{\frac{\theta}{2}}(ax-\sqrt{ac})^2\\&= \left(\cos{\frac{\theta}{2}}(ax+\sqrt{ac}) +\sin{\frac{\theta}{2}}(ax-\sqrt{ac})\right) \left(\cos{\frac{\theta}{2}}(ax+\sqrt{ac}) -\sin{\frac{\theta}{2}}(ax-\sqrt{ac})\right)\\&=\left(ax\left(\cos{\frac{\theta}{2}}+\sin{\frac{\theta}{2}}\right)+\sqrt{ac}\left(\cos{\frac{\theta}{2}}-\sin{\frac{\theta}{2}}\right)  \right) \left(ax\left(\cos{\frac{\theta}{2}}-\sin{\frac{\theta}{2}}\right)+\sqrt{ac}\left(\cos{\frac{\theta}{2}}+\sin{\frac{\theta}{2}}\right)  \right).\end{aligned}$$ Now, by setting the factors equal to zero and solving for x, we obtain, $$\begin{aligned}x_1&=\left(\frac{\sin{\frac{\theta}{2}}+\cos{\frac{\theta}{2}}}{\sin{\frac{\theta}{2}}-\cos{\frac{\theta}{2}}}\right)\sqrt{\frac{c}{a}}\\&= \left(1+\frac{2}{\tan{\frac{\theta}{2}-1}}\right)\sqrt{\frac{c}{a}}. \end{aligned}$$ Similarly we obtain, $$\begin{aligned}x_2= \left(1-\frac{2}{\tan{\frac{\theta}{2}+1}}\right)\sqrt{\frac{c}{a}} \end{aligned}.$$ The formula appears to be new, at least on the internet. Wikipedia presents a trigonometric method, but it is different from mine. Stuart Simons offers a method that does seem to be related to my approach, but he only provides direct formulas for complex roots. I was able to independently derive Simons' formulas using $\cos{\theta}$ in the substitution instead of $\sec{\theta}$ , and this was before coming across the Wikipedia article that references Simons' paper: Simons, Stuart, 'Alternative approach to complex roots of real quadratic equations,' Mathematical Gazette 93, March 2009, 91–92. EDITED . And this formula can be obtained by substituting $\sin{\theta}$ instead of $\sec{\theta}$ and following similar steps as my previous proof: $x_{1,2}=\pm i e^{\pm i \theta}\sqrt{\frac{c}{a}}$ . Note . My interest in these formulas is purely mathematical. My primary intention is to incorporate all of these formulas mentioned here into a unified framework based on half-angle formulas . ALL the formulas mentioned here by me and others can be derived using half-angle identities, as exemplified by my initial proof, so in that sense, I feel satisfied. The key observation I made is that $a^2+2abf(x)+b^2$ can be nicely factorized thanks to the properties of half-angle formulas, as long as $f(x)$ is a trigonometric function except $\tan{\theta}$ or $\cot{\theta}$ . I wouldn't be surprised if analogous formulas could be obtained for cubic and quartic equations, but at the moment, I have no idea how to do it. EDITED . Here I provide a proof for the formulas cited by njuffa in the comments. Theorem 2 .Let $a$ , $b$ and $c$ be non-zero real numbers. If $ax^2+bx+c=0$ , then $$\begin{aligned}x_{1}&=-\tan{\frac{\theta}{2}}\sqrt{\frac{c}{a}},\\
x_{2}&=-\cot{\frac{\theta}{2}}\sqrt{\frac{c}{a}},
\end{aligned}$$ where $\theta=\csc^{-1}{\left(\frac{b}{2\sqrt{ac}}\right)}$ . Proof . Multiplying by $a$ the quadratic equation $ax^2+bx+c=0$ , we have $$(ax)^2+b(ax)+ac=0.$$ Let's make the substitution $\csc{\theta}=\frac{b}{2\sqrt{ac}}$ , then $$\begin{aligned}0&=(ax)^2+b(ax)+ac\\&=(ax)^2+2\sqrt{ac}(ax)\csc{\theta}+ac\\&=\csc{\theta}\left((ax)^2\sin{\theta}+2\sqrt{ac}(ax)+ac\sin{\theta}\right)\\&=2(ax)^2\sin{\frac{\theta}{2}}\cos{\frac{\theta}{2}}+2\sqrt{ac}(ax)\left(\sin^2{\frac{\theta}{2}}+\cos^2{\frac{\theta}{2}}\right)+2ac\sin{\frac{\theta}{2}}\cos{\frac{\theta}{2}}\\&=(ax)^2\sin{\frac{\theta}{2}}\cos{\frac{\theta}{2}}+\sqrt{ac}(ax)\sin^2{\frac{\theta}{2}}+ 
\sqrt{ac}(ax)\cos^2{\frac{\theta}{2}}+ac\sin{\frac{\theta}{2}}\cos{\frac{\theta}{2}}\\&= ax\sin{\frac{\theta}{2}}\left(ax\cos{\frac{\theta}{2}}+\sqrt{ac}\sin{\frac{\theta}{2}}\right)+\sqrt{ac}\cos{\frac{\theta}{2}}\left(ax\cos{\frac{\theta}{2}}+\sqrt{ac}\sin{\frac{\theta}{2}}\right)\\&= \left(ax\cos{\frac{\theta}{2}}+\sqrt{ac}\sin{\frac{\theta}{2}}\right)\left(ax\sin{\frac{\theta}{2}}+\sqrt{ac}\cos{\frac{\theta}{2}}\right)\end{aligned}$$ Finally, by setting the factors equal to zero, we obtain the desired formulas. The following Euler-like identities have been suggested by formulas in this question. If complex $\theta_1=\cos^{-1}{(p)}$ and $\theta_2=\sec^{-1}{(p)}$ , where $p\geq-1$ and $p\neq0$ , then the following relation holds: $$e^{i\theta_1}=\frac{1-\tan{\frac{\theta_2}{2}}   }{1+\tan{\frac{\theta_2}{2}}} 
 \tag{1}$$ And when $p<-1$ , we have $$e^{-i\theta_1}=\frac{1-\tan{\frac{\theta_2}{2}}}{1+\tan{\frac{\theta_2}{2}}}\tag{2}$$ I came up with $(1,2)$ by equating the formulas for the positive roots of the original question's formula with Simons' formulas. Then, they need a slight modification because we cancel out $\sqrt{\frac{c}{a}}$ . Another simpler formula can be obtained by equating the formulas for the roots of the german text and the formulas I got from substituting by $\sin{\theta}$ instead of $\sec{\theta}$ : If complex $\theta_1=\sin^{-1}{(p)}$ and $\theta_2=\csc^{-1}{(p)}$ , where $p\leq1$ and $p\neq0$ , then the following relation holds: $$ie^{i\theta_1}=-\tan{\frac{\theta_2}{2}}\tag{3}$$ And for $p>1$ we have $$ie^{-i\theta_1}=\tan{\frac{\theta_2}{2}}\tag{4}$$ There are several variants we can get this way. I find identities $(1-4)$ interesting because they allow for the simplification of certain trigonometric integrals, as well as solving integrals that not even Mathematica can handle . For example, $$\int_{2}^{3}  \frac{{1 - \tan\frac{{\sec^{-1}x}}{2}}}{{1 + \tan\frac{{\sec^{-1}x}}{2}}}\sqrt{\tan\frac{\csc^{-1}x}{2}}\,dx$$ Question: Is this formula known? Did Simons consider it in his article (I don't have access to it)?","['algebra-precalculus', 'trigonometry', 'reference-request']"
4844161,"Proving that $f(\{x_1,\cdots,x_n\}) = x_1^2 + \cdots + x_n^2\;(x_i\in\mathbb{N}_{\ge 1})$ is injective.","I'm trying to prove that $f(\{x_1,\cdots,x_n\}) = x_1^2 + \cdots + x_n^2$ is injective (here each $x_i \in \mathbb N_{\geq 1}$ ). I'm quite confident that $f(x,y) = x^2 + y^2$ is injective (I don't know how to show that explicitly) however I'm sure there is a way to extend it from there. Perhaps there is an argument using geometry or algebra that I am not aware of? What I'm really trying to do is to show that there exists some injection from the set of all finite subsets of ℕ to ℕ. I thought of taking the sum of the squares of each subset should do the trick, but I just need to show that it's injective (my gut says it is)","['elementary-set-theory', 'combinatorics', 'real-analysis']"
4844177,Application of Meyer-Itô formula to a convex (but not differentiable) function,"I am trying to apply a simple example of Meyer-Itô's formula, which generalizes the classical Itô's formula over convex functions (instead of twice continuously differentiable functions). Applied to continuous semimartingales, Meyer-Itô's formula can be summarized as (see Theorem 70 in Protter's book): Let $X$ be a continuous semimartingale and let $f$ be a convex function. The following equality holds: \begin{equation}
f(X_t) = f(X_0) + \int_{0^+}^t f_-^{\prime}(X_s) \mathrm{d}X_s + \frac12 \int_{\mathbb{R}} L_t^x f''(x) \mathrm{d}x,
\label{meyer-ito}
\end{equation} where the signed measure $f''$ is the second derivative of $f$ in the sense of distributions, and where $L_t^x$ is the family of local times of $X$ . Suppose for simplicity I want to apply this equation with a standard Brownian motion $W = \{W_t : t \geq 0\}$ and with the convex function \begin{equation}
f(x) = {
	\left\{
		\begin{array}{ll}
			-x & \mbox{if } x < 0, \\
			0 & \mbox{if } x \geq 0.
		\end{array}
	\right.
}
\end{equation} How to deal with the last term of Meyer-Itô's formula? How do I proceed once I have computed the second derivative of $f$ in the sense of distributions? I don't really know where to begin to get an explicit integral for the last term. Any help would be appreciated. Thank you!","['stochastic-processes', 'probability-theory', 'stochastic-calculus', 'distribution-theory']"
