question_id,title,body,tags
4539968,"Find $a,b,c,d$ such that $\forall x \in \mathbb{R} : (x^2+cx+d)^{10}=(2x-1)^{20}-(ax+b)^{20}$","One of my colleagues show me the below question and asked me to solve it. find $a,b,c,d$ as real numbers such that $$\forall x \in \mathbb{R} : (x^2+cx+d)^{10}=(2x-1)^{20}-(ax+b)^{20}$$ so I started to put some $x$ into equation,for example $x=0 \to d^{10}=1-b^{20}$ or $x=1 \to (1+c+d)^{10}=1-(a+b)^{20} $ and so on... but honestly, I got no clue to find $a,b,c,d$ in an ordinary way. another trial was to show for the l.h.s and R.h.s coefficient, but it's complicated to solve $$\underbrace{(x^2)^{10}}_{1x^{20}}+\cdots=\underbrace{(2x)^{20}-(ax)^{20}}_{1x^{20}}+\cdots$$ can someone help me? Thanks in advance","['calculus', 'algebra-precalculus']"
4539994,Convergence of the inverse partial sums,"Given a sequence $(x_n)$ we define by induction the sequence of sequences $$\begin{cases}S_0(n)&=x_n\newline S_{m+1}(n)&=\sum_{k=0}^{n}\frac{1}{S_{m}(k)}\end{cases}$$ Let’s take the example of $(x_n)=\log(n+2)$ , we would have $$S_1(n)=\sum_{k=0}^{n}\frac{1}{\log(k+2)}$$ which diverges $$S_2(n)=\frac{1}{\frac{1}{\log(2)}}+\frac{1}{\frac{1}{\log(2)}+\frac{1}{\log(3)}}+…+\frac{1}{\frac{1}{\log(2)}+\frac{1}{\log(3)}+…+\frac{1}{\log(n+2)}}$$ which might diverge aswell according to python. My question is, given any sequence $(x_n)$ , is there a rank m for which (S_m(n)) converges ? And can we bound it by any constant ? I’ve never seen such an exercice and struggle to make use of the equivalents here…","['discrete-mathematics', 'sequences-and-series']"
4540011,The absolute continuity of push-forward measure,"Let $X$ be a real-valued random variable on $\mathbb R$ , and $f:\mathbb R \to \mathbb R$ differentiable such that $f'(x)>0$ for all $x \in \mathbb R$ . Let $Y := f(X)$ . Let $\mu_X, \mu_Y$ be the distributions of $X, Y$ respectively. Then $\mu_Y = f_{\sharp} \mu_X$ . Let $F_X, F_Y$ be the c.d.f. of $X, Y$ respectively. At page $14$ of this lecture note , the author said that Theorem: If $\mu_X$ is absolutely continuous w.r.t. Lesbesgue measure $\lambda$ , then so is $\mu_Y$ My attempt: Clearly, we have $F_Y (t) = F_X \circ f^{-1} (t)$ . Let $A$ be a Borel set such that $\lambda(A) = 0$ . Because $\mu_X \ll \lambda$ , we get $\mu_X (A) =0$ . We have $\mu_Y (A) = \mu_X(f^{-1} (A))$ . It suffices to prove $f^{-1} (A)$ is a $\lambda$ -null set. Could you shed some light on how to finish the proof? Update: I have found a related result here . However, it requires $f$ to be continuously differentiable, i.e., if $f\in C^1$ and $\{f' = 0\}$ is $\lambda$ -null then $f^{-1} (A)$ is also $\lambda$ -null.","['measure-theory', 'lebesgue-measure', 'probability-distributions', 'real-analysis', 'derivatives']"
4540027,Prove that the Euclidean distance is no more than the spherical distance,"In an $n$ -dimensional Euclidean space, for any unit vector $$\boldsymbol{y}=(y_1,y_2,\cdots, y_n)\in\mathbb{S}^{n-1}=\{\boldsymbol{x}\in\mathbb{R}^n:\|\boldsymbol{x}\|_2=1\},$$ we can express $\boldsymbol{y}$ in the spherical coordinate system via $\phi(\boldsymbol{y})=(\phi_1(\boldsymbol{y}), \phi_2(\boldsymbol{y}), \ldots, \phi_{n-1}(\boldsymbol{y}))^{\mathrm{T}}\in\mathbb{R}^{n-1}$ s.t. $$
\begin{aligned}
y_1 &=\cos \phi_1 \\
y_2 &=\sin \phi_1 \cos \phi_2 \\
y_3 &=\sin \phi_1 \sin \phi_2 \cos \phi_3 \\
& ~\,\,\vdots \\
y_{n-1} &=\sin \phi_1 \cdots \sin \phi_{n-2} \cos \phi_{n-1} \\
y_{n} &=\sin \phi_1 \cdots \sin \phi_{n-2} \sin \phi_{n-1},
\end{aligned}
$$ where $0 \leq \phi_{n-1}<2 \pi$ , and $0 \leq \phi_{i}\le \pi$ , $\forall\,i=1,2,\ldots,n-2$ . My question is, if the Euclidean distance is no more than the spherical distance, i.e., if one has $$
\boxed{\|\boldsymbol{x}-\boldsymbol{y}\|_2 \leq\|\phi(\boldsymbol{x})-\phi(\boldsymbol{y})\|_2}.
$$ I have been thinking this for several days, but am still unable to prove it. What is easy to show is that $\|\boldsymbol{x}-\boldsymbol{y}\|_2 \leq\|\phi(\boldsymbol{x})-\phi(\boldsymbol{y})\|_1$ , which simply follows from the fact that the Euclidean distance is the shortest between any two points, and $|\phi_i(\boldsymbol{x})-\phi_i(\boldsymbol{y})|$ is exactly the length of the arc used to align $\boldsymbol{x}$ and $\boldsymbol{y}$ along the $i$ -th spherical coordinate, which is longer than the corresponding chord. If this is not true, then does it hold for any two close vectors $\boldsymbol{x}$ and $\boldsymbol{y}$ in the sense that $\underset{i=1,2,\ldots,n}{\max}|\phi_i(\boldsymbol{x})-\phi_i(\boldsymbol{y})|\le\delta$ for some small $\delta$ ?","['geometry', 'spherical-geometry', 'spherical-coordinates', 'linear-algebra', 'inequality']"
4540066,"Entropy vs differential entropy (a good extension, even if it can be negative)?","The entropy of a discrete random variable, taking value $i$ with probability $p_i$ , is defined as $$ H(X)=-\sum_{i}p_i\log (p_i),$$ and can be seen as the amount of information gained once we are told the value of $X$ . It is a measure of disorder of the system. Clearly $H(X)$ will always be non-negative. When the definition is extended to continuous random variable $Y$ , it is called differential entropy, and defined as (when $Y$ has density $f$ ) $$H(Y)=-\int f(y) \log(f(y)) dy.$$ In this case $H(Y)$ can be negative, so how is it a good extension of the definition, since it can  no longer be seen as a measure of ""information gained once we are told the value of $Y$ "".","['entropy', 'information-theory', 'soft-question', 'probability-theory', 'probability']"
4540109,mean independence of a sequence of random variables,"A sequence of random variables $X_1, X_2...$ is said to be mean independent if $E[X_k∣X_1,...,X_{k-1}]=E(X_k)$ for all $k$ . So if we change the order, this property remains true? for example $E[X_1∣X_2,...,X_{k}]=E(X_1)$ . Thanks","['probability-distributions', 'probability-theory', 'probability', 'random-variables']"
4540128,3D rotation matrix question: Extension of Wahba's Problem,"This is an extended version of Wahba's Problem . Let $R_x(\alpha)$ denote a 3D rotation matrix around the axis $x$ by an amount $\alpha$ . For given unit vectors $\{u_k\}_{k=1}^3$ and $\{v_k\}_{k=1}^3$ I want to solve the system $$
RR_{u_k}(\alpha_k)v_k=v_k, \quad 1\leq k\leq 3.$$ The variables that I can solve over are the three angles $\alpha_k$ as well as a full rotation matrix $R$ . Clearly, $R=I$ and $\alpha_k=0, \forall k$ is one trivial solution.
Wahba's problem arises by setting $\alpha_k=0, \, \forall k$ and then asking for the best fit of $R$ but where the RHS vector $v_k$ is replaced with $u_k$ . (Wahba's also have weights). However, numerically, I find that there are other non-trivial solutions.
Using a very lengthy brute-force (analytical) method, I can establish that for the a particular choice $u_1=u_2=[1\; 0\; 0]^{\mathrm{T}}$ and $u_3=[0\; 1\;0]^{\mathrm{T}}$ there are either 2 or 4 solutions (including the trivial one) depending on the vectors $v_k$ . (Other number of solutions occur for a set of $v_k$ 's of measure zero). However, I cannot crack the general case of arbitrary $u_k$ 's. I feel that there could be an elegant geometric argument.","['orthogonal-matrices', 'linear-algebra', 'geometry', 'rotations']"
4540167,How many ways to arrange 18 students and 2 teachers with 8 students between the 2 teachers,"A school photo must be taken of a class consisting of 18 students and
two teachers. The photographer wants that all people should stand next
to each other in a row, but there should be exactly 8 students between
the two teachers. In how many ways can this arrangement take place? I am not sure how to do this, I assume it's a permutation since order matters but even if I know that there should be T S S S S S S S S T I can put the rest students however I want like 3 on the left and 7 on the right before and after the teacher. I don't know how to write this up.","['permutations', 'combinatorics', 'discrete-mathematics']"
4540170,Concept of weak solutions for ODEs,"Consider the inital problem for ODE \begin{eqnarray}
\frac{du}{dt}&=&f(t,u),\\
u(0)&=&u_0.
\end{eqnarray} If $f$ is locally bounded function i.e., $f(t,u) \leq M_K$ for all $(t,u)$ in some compactset $K,$ then $u(t)=u_0+\int\limits_{0}^t f(s,u(s))ds$ is a locally Lipschitz continuous functions. Thus, for a.e. $t>0,$ $t\mapsto \int\limits_{0}^t f(s,u(s))ds$ is differentiable and hence by Fundamental theorem of calculus, $u$ satisfies the following weak formulation. \begin{eqnarray}
\int\limits_0^{\infty} u\phi_t dt=\int\limits_0^{\infty}f(t,u(t))\phi(t)dt +u_0\phi(0) \quad \quad \text{for all } \phi \in C_c^1([0,\infty).
\end{eqnarray} In conclusion, the above IVP admits a Lipschitz continuous solution (globally for all $t\geq 0$ ) even for $f$ which can be discontinuous in $t$ as well as $u$ variable!
Is it correct or I am making some mistakes? Is this solution unique? In other words can this IVP have multiple weak solutions? Though the concept of weak solution is quite popular for PDEs, I have not seen this being discussed for initial value problems of ODEs in any of the textbooks. Whys is this so?","['partial-differential-equations', 'analysis', 'ordinary-differential-equations', 'real-analysis']"
4540174,"Find a closed expression for double sum $\sum_{n,m\ge 1} \frac{1}{n m (n^2+m^2)}$","Playing around with double sums related to harmonic numbers and having easily found a closed expression for this sum $$s_1 = \sum_{n,m\ge 1} \frac{1}{n m (n+m)}=2 \zeta(3)\tag{1}$$ I attempted to find a closed expression for this slightly more complicated sum $$s_2 = \sum_{n,m\ge 1} \frac{1}{n m (n^2+m^2)}\tag{2}$$ which turned out to be more than I could achieve, and I hope for your help. Here's what I did so far. First approach $$\begin{align}s_2 & = \sum_{n,m\ge 1} \frac{1}{n m (n^2+m^2)} \\
& = \sum_{n,m\ge 1} \frac{1}{n m }\int_{0}^{\infty } e^{-t(n^2+m^2)}\;dt\\
& =\int_{0}^{\infty }\left(\sum_{n,m\ge 1} \frac{1}{n m } e^{-t(n^2+m^2)}\right)\;dt\\
& =\int_{0}^{\infty } f_2(t)^2\;dt
\end {align}\tag{3}$$ where $$f_2(t) = \sum_{n\ge 1}\frac{e^{-t n^2}}{n}\tag{4}$$ Now $$e^{-n^2 t}=\frac{1}{\sqrt{\pi}}\int_{-\infty }^{\infty } e^{-x^2+i 2 n x\sqrt{t} } \, dx\tag{5}$$ This linearizes $n$ in the exponent, and, exchanging integral and sum, we can do the (conditionally convergent) $n$ -sum under the $x$ -integral which gives $$\frac{1}{\sqrt{\pi}}\sum _{n=1}^{\infty } \frac{e^{-x^2+i 2 n x \sqrt{t}}}{\sqrt{\pi } n}=-\frac{e^{-x^2}}{\sqrt{\pi}} \log \left(1-e^{2 i x \sqrt{t} }\right)\tag{6}$$ and $$f_2(t) = -\frac{1}{\sqrt{\pi}}\int_{-\infty }^{\infty } e^{-x^2} \log \left(1-e^{2 i x \sqrt{t} }\right)\, dx\tag{7}$$ Now in the $x$ -integral the imaginary part cancels out due to symmetry and the real part can be simplified so that $$f_2(t) = -\frac{1}{\sqrt{\pi}}\int_{-\infty }^{\infty } e^{-x^2} \frac{1}{2} \log \left(4 \sin ^2\left(x\sqrt{t} \right)\right)\; dx\tag{8}$$ Alas, here I am stuck, and summing up it seems that I have made a simple looking formula more complicated. Second approach Doing just the $n$ -sum in $s_2$ gives $$s_2 = \sum_{m\ge 1} \frac{1}{2 m^3} (H(i m) + H(-i m))=\Re\left(\sum_{m\ge 1} \frac{1}{m^3} H(i m)\right)\tag{2.1}$$ where $H(z)$ is the harmonic number of argument $z$ . Inserting the representation $$H(z) = \int_{0}^{1} \frac{1-x^z}{1-x}\;dx$$ and performing the $m$ -sum gives an interesting integral representation of our double sum $$s_2 = \left(\int_0^1 \frac{-\zeta (3)+\Re(\text{Li}_3\left(x^i\right))}{x-1} \, dx\right)\tag{2.2}$$","['harmonic-numbers', 'summation', 'closed-form', 'sequences-and-series']"
4540178,Meaning of curl defined as in differential geometry,"there is a short remark in the textbook about GR I have been reading, which has been bugging me for the last few hours: My questions are: (1): In multivariable calculus, curl can be intuitively described as the rotation vector of a vector field. Why is it here a (0,2)-tensor field? (2): How can one derive the second equation from the first equation? Best regards. EDIT: The second equation only holds for vector fields $Y,Z \perp X$ .","['multivariable-calculus', 'curl', 'differential-geometry']"
4540192,An alternating sum,"I ran into an alternating sum in my research and would like to know if the following identity is true: $$
\sum_{i = 0}^{\left\lfloor \left(n + 1\right)/2\right\rfloor} \frac{\left(n + 1 - 2i\right)^{n + 1}}{2^{n}\left(n + 1\right)!}\binom{n + 1}{i}\left(-1\right)^{i} = 1\quad
\forall\ \mbox{positive integers}\ n\geq 3
$$ Any help would be appreciated!. Edit. We might try to use an Iverson bracket $[[2q\le n]]$ in attempting to
evaluate $$S_n = \sum_{q=0}^{\lfloor n/2\rfloor}
(n-2q)^n {n\choose q} (-1)^q.$$ We obtain $$[v^n] \frac{1}{1-v}
\sum_{q\ge 0} v^{2q} (n-2q)^n {n\choose q} (-1)^q$$ Using a coefficient extractor, $$n! [z^n] \exp(nz) \;\underset{v}{\mathrm{res}}\;
\frac{1}{v^{n+1}} \frac{1}{1-v}
\sum_{q\ge 0} v^{2q} \exp(-2qz) {n\choose q} (-1)^q
\\ = n! [z^n] \exp(nz) \;\underset{v}{\mathrm{res}}\;
\frac{1}{v^{n+1}} \frac{1}{1-v} (1-v^2\exp(-2z))^n.$$ Now residues sum to zero and the residue at one yields $$- n! [z^n] \exp(nz) (1-\exp(-2z))^n.$$ We have that since $(1-\exp(-2z))^n = (2z-2z^2\pm\cdots)^n = 2^n z^n +\cdots$ this evaluates to $-2^n n!.$ We find for the residue at infinity $$- n! [z^n] \exp(nz) \;\underset{v}{\mathrm{res}}\;
\frac{1}{v^2}
v^{n+1} \frac{1}{1-1/v} (1-\exp(-2z)/v^2)^n
\\ = n! [z^n] \exp(nz) \;\underset{v}{\mathrm{res}}\;
\frac{1}{v^n} \frac{1}{1-v} (v^2-\exp(-2z))^n
\\ = n! [z^n] \exp(nz) \;\underset{v}{\mathrm{res}}\;
\frac{1}{v^n} \frac{1}{1-v}
\sum_{q=0}^n {n\choose q} (-1)^{n-q} \exp(-2(n-q)z) v^{2q}
\\ = \;\underset{v}{\mathrm{res}}\;
\frac{1}{v^n} \frac{1}{1-v}
\sum_{q=0}^n {n\choose q} (-1)^{n-q} (2q-n)^n v^{2q}
\\ = \sum_{q=0}^n {n\choose q} (-1)^q (n-2q)^n [[2q\le n-1]].$$ Now when $n$ is odd this gives the upper limit $\lfloor n/2\rfloor$ and
when $n$ is even $\lfloor n/2\rfloor -1$ however in the latter case we
may raise to $\lfloor n/2\rfloor$ because the added term is zero in the
sum per $(n-2q)^n = 0$ . We have obtained $$\sum_{q=0}^{\lfloor n/2\rfloor} {n\choose q} (-1)^q (n-2q)^n
= S_n.$$ Collecting everything we have shown that $S_n - 2^n n! + S_n = 0$ or $S_n = 2^{n-1} n!.$ The question now becomes, is there a simpler proof?","['elementary-number-theory', 'calculus', 'binomial-coefficients', 'summation']"
4540196,Canonical sheaf of product of two smooth projective curves,"Let $C_1, C_2$ be two smooth projective curves of genus atleast $2$ . Let $X:=C_1 \times C_2$ . If $P_1, P_2$ are denoted as the first and second projections, then it is known that the canonical bundle relation is given as follows : $K_X := P_1^*(K_{C_1}) \otimes P_2^*(K_{C_2})$ . From this relation is it elementary to see that $h^1(K_X)=h^1(\mathcal O_X) >0$ ? Any hint or explaination is appreciated.",['algebraic-geometry']
4540204,Inequality with norm in Space $L^2(\Omega)$,"Let $\Omega \subset \mathbb{R}^N$ a bounded domain. Let $v \in L^2(\Omega)$ . It is possible to make an estimate of the type $$\|v^2\|_{L^2(\Omega)} \leq \|v\|^k_{L^2(\Omega)}$$ ,
for some $k \in \mathbb{R}$ . Using Holder's inequality, I'm able to get something like $$\|v^2\|_{L^2(\Omega)} \leq \|v^3\|_{L^2(\Omega)}\|v\|_{L^2(\Omega)}.$$ But what I really want is to get that square out of the norm. Thanks.","['measure-theory', 'functional-analysis', 'estimation']"
4540234,Prove Summation Converge to $\frac{1}{e}$ or find an Upper Bound,"An experiment can be modeled with these two equations. Let $n$ be an even perfect square. The total number of experiments is given by $$T = {n \choose \sqrt{n}} {n-\sqrt{n} \choose \sqrt{n}},$$ and the total number of Bads events is given by $$B = \sum_{d=0}^{\sqrt{n}/2} {n/2 \choose d} {n/2 - d \choose \sqrt{n} - 2d} 2^{\sqrt{n} - 2d} {n-2(\sqrt{n}-d) \choose \sqrt{n}}.$$ I wrote a script and found out that as $n$ gets bigger, $\frac{B}{T} \sim \frac{1}{e}$ . But I don't need a tight bound on these quantities because I'm trying to prove that the probability of a good event is at least $1/2$ . Showing that $$1 - \frac{B}{T} \geq \frac{1}{2}.$$ What I have tried: As I'm not sure how (or even if it possible) to simplify theses equations I computed the following: $$\frac{B}{T} = u_0 + u_1 + u_2 + ... + u_{\sqrt{n}/2}$$ finding out that for a $n \geq 36$ the first $3$ terms are ""the ones that matter"" from $u_3$ onwards they start to get really small. So maybe a way to prove the probability of a bad event would be something like: $$\frac{B}{T} = u_0 + u_1 + u_2 + ... + u_{\sqrt{n}/2} \leq u_0 + u_1 + u_2 + (\sqrt{n}/2 - 3) u_3 \leq \frac{1}{2}.$$ But I'm not sure if this is the right way to go, maybe there is some clever argument involving $\frac{1}{e}$ . Any help or tips on how to prove this would be much appreciated.","['discrete-mathematics', 'binomial-coefficients', 'combinatorics', 'probability']"
4540329,Venn Diagram for Power Set,"I was wondering what the Venn Diagram for a power set might look like. For example for $\{A, B, C\}$ . I'm going to go out on a limb and say it's probably not this: However, that image is useful for picturing all the subsets, minus the empty set. Yet from my understanding it shows relationships between 3 sets, rather than within a power set. Can anyone shed some light on this please?",['elementary-set-theory']
4540368,$|\sin\phi_{n}(x)| = \sqrt{\frac{1}{2} - \dfrac{1}{2[1+ (\omega_{n}a(x))^{2}]^{1/2}}}$,"Let a sequence of real numbers $\omega_{n}$ with $\omega_{n} \to \infty$ with $a(\cdot) \in L^{\infty}(0,1)$ , $a(x) > 0$ for all $x \in (0,1]$ and $a(0) = 0$ . Let $\phi_{n}(x) = -\frac{1}{2}\text{arg}[1+i\omega_{n}a(x)]$ , then $$
|\sin\phi_{n}(x)| = \sqrt{\frac{1}{2} - \dfrac{1}{2[1+ (\omega_{n}a(x))^{2}]^{1/2}}}
$$ I am thinking that the author is using $\sin(\text{arg}(z)) = \dfrac{\text{Im}(z)}{|z|}$ , but I do not understand.","['limits', 'complex-numbers', 'sequences-and-series']"
4540457,Does left and right smoothness of a group product guarantee smoothness?,"Let $G$ be a smooth manifold and a group, whose product $G\times G\to G$ is ""separately smooth,"" meaning $L_g:h\mapsto gh$ and $R_g:h\mapsto hg$ are smooth for each $g\in G$ . Must $G$ (with the given product) be a Lie group? In other words, must this separately smooth product be jointly smooth? (Note that a jointly smooth product implies a smooth inverse, as shown here .) The same question is asked here , but the answer there is insufficient and perhaps even inaccurate. It shows that $G$ can be equipped with a product turning it into a Lie group, but not necessarily that the given product makes $G$ into a Lie group (as is pointed out in the comments). The purpose of my question is to clarify this important detail.","['topological-groups', 'smooth-manifolds', 'differential-topology', 'lie-groups', 'differential-geometry']"
4540533,The closed form for the integral: $\int_{0}^{1}\frac{x^{2a}\arctan{x}}{x^2+1}dx$,"I am trying to find a closed form for this integral: $$I=\int_{0}^{1}\frac{x^{2a}\arctan{x}}{x^2+1}dx$$ Where $a$ is any constant $\in R$ . I use this approach, but still don't get the result: First, use the series of $\arctan{x}=\sum_{k=0}^{\infty}\frac{(-1)^k}{2k+1}x^{2k+1}$ then $$I=\int_{0}^{1}\frac{x^{2a}\arctan{x}}{x^2+1}dx=\sum_{k=0}^{\infty}\frac{(-1)^k}{2k+1}\int_{0}^{1}\frac{x^{2a+2k+1}}{1+x^2}dx=\frac{1}{4}\sum_{k=0}^{\infty}\frac{(-1)^k}{2k+1}\left(H_{\frac{a+k}{2}}-H_{\frac{1}{2} (a+k-1)}\right)$$ But I got stuck at this step. Can you give me some hints, or another approaches? Thank you so much.","['integration', 'calculus']"
4540563,Determining an integral positive-definite symmetric matrix is diagonalizable over $\Bbb Z$,"According to this question: Diagonalization of a symmetric bilinear form over the integers , not all definite symmetric matrices with integer entries are diagonalizable over $\Bbb Z$ . (Here we are considering diagonalization of a bilinear form, so this means that given a definite symmetric matrix $H \in M_{n\times n}(\Bbb Z)$ , we cannot always find an invertible matrix $P\in GL(n,\Bbb Z)$ such that $P^t HP$ is diagonal.) I am curious about: if we are given an explicit definite symmetric integral matrix, then is there a way (or algorithm) to determine whether it is diagonalizable over $\Bbb Z$ ? Actually I want to determine whether the negative-definite matrix $$A=
\begin{bmatrix} -1 & 1 & 1 & 1 & & 1 &  \\ 1 & -2 & \\ 1 & & -5 & \\1 & & & -4 & 1 \\ & & & 1 & -2 &  \\1 & & & & & -71 & 1 \\ & & & & & 1 & -2 \end{bmatrix}$$ is diagonalizable over $\Bbb Z$ . Using elementary row&column operations (corresponding to $E^tAE$ for some integral elementary matrix $E$ ), I've got $$ A'=\begin{bmatrix} -1   \\  & -1 & \\  & & -3 & 2 & & 2 \\ & & 2 & -2 & 1 & 2 \\ & & & 1 & -2 &  \\ & & 2 & 2 & & -69 & 1 \\ & & & & & 1 & -2 \end{bmatrix} ,$$ and it seems $A'$ is not diagonalziable because the $5\times 5$ submatrix of $A'$ doesn't seem diagonalizable. Is there a way to prove that $A$ is not diagonalziable?","['matrices', 'bilinear-form', 'linear-algebra', 'diagonalization', 'positive-definite']"
4540564,Proving $4 \nmid n^2+2$ for all $n \in \mathbb{Z}$,"I was asked to prove $4 \nmid n^2+2$ for all $n \in \mathbb{Z}$ and I was wondering whether my procedure was correct. $I$ . Assume $n$ is even, or rather $n = 2Q, Q\in\mathbb{Z}$ . Assume as well $4|n^2+2$ . Then $$n^2+2=4q \tag{$q\in\mathbb{Z}$}$$ $$\implies (2Q)^2+2 =4q$$ $$\implies 2=4(q-Q^2)$$ Our result implies $4|2$ , which is absurd. Then if $n$ is even it can not happen $4|n^2+2$ . $II.$ Assume $n=2Q+1, Q\in\mathbb{Z}$ . Assume $4|n^2+2$ . Then $$n^2+2=4$$ $$\implies (2Q+1)^2+2=4q$$ $$\implies 4Q^2+4Q+3=4q$$ $$\implies 3=4(q-Q^2-Q)$$ which implies $4|3$ , which is absurd. Then if $n$ is odd, $4\nmid n^2+2$ . $III.$ We've shown $4|n^2+2$ is absurd if $n$ is odd and if $n$ is even. Thus we have shown $4\nmid n^2+2$ for all $n\in\mathbb{Z}$ . I have the two general questions one has when finishing a proof. $A)$ Is the proof correct? $B)$ Was there a simpler proof? In this case, perhaps a proof that could be directly applied to all integers instead of having to show the property for even and odd numbers separately?","['elementary-number-theory', 'solution-verification', 'divisibility', 'discrete-mathematics']"
4540580,Existence of subgraphs with certain property,"Let's take a look at the following two lemmas: Lemma 1 (Large bipartite subgraph) Every $G$ has a bipartite subgraph with at least $e(G)/2$ edges. Lemma 2 (Large average degree implies subgraph with large minimum degree) Let $t\in \mathbb{R}$ . Every graph with average degree $2t$ has a
subgraph with minimum degree greater than $t$ . I do not have any issues with the proofs of these lemmas since I was able understand them. But I am little confused about the statements. Regarding Lemma 1. Am I right that we need to assume that bipartite subgraph should have at least one edge. Otherwise one can take a subgraph with empty edge set because it is vacuously true. Regarding Lemma 2. Here we need to assume that $t>0$ (for $t=0$ the statement is false).  Also we need to assume that subgraph has nonempty vertex set (otherwise, it is vacuously true). Is my reasoning correct or am I missing something?","['graph-theory', 'combinatorics', 'discrete-mathematics']"
4540589,Convergence of $\sum_{n=1}^{\infty}\frac{2n+1}{(n^{2}+n)^{2}}$,"The series $\sum_{n=1}^{\infty}\frac{2n+1}{(n^{2}+n)^{2}}$ (a) converges to $1$ (b) converges to a number $>1$ (c) diverges to $\infty$ (d) has an oscillating sequence of partial sum How to deal with convergence of the series $\sum_{n=1}^{\infty}\frac{2n+1}{(n^{2}+n)^{2}}$ .
Here, I can use limit comparision test taking series $b_{n}=\sum\frac{1}{n^{3}}$ and got $\lim\frac{a_{n}}{b_{n}}=2$ , so by limit comparison test, since the series $\frac{1}{n^{3}}$ is convergent, $a_n=\sum_{n=1}^{\infty}\frac{2n+1}{(n^{2}+n)^{2}}$ is also convergent.
But will it converge to $1$ or to a number greater than $1$ ? How should I proceed?","['calculus', 'convergence-divergence', 'sequences-and-series']"
4540700,Linear recurrence sequence with integer values,"Let $(a_n)_{n \ge 0}$ be a linear recurrence sequence taking only integer values. Then $a_n$ satisfies a recurrence with integer coefficients. Notes: This  follows up  an older question on this site.
The title of the question mentioned ""the defining relation"". The counterexample was a constant sequence, that can satisfy many other linear recurrences ( multiply the polynomial $(T-1)$ by other poly).  However, if a sequence with integral values satisfies an integral linear recurrence, then the defining relation is also integral ( follows from Gauss lemma, and a bit of linear algebra). What progress I've made:  I am able to show the following: If we have $\alpha_1$ , $\ldots$ , $\alpha_{\ell}$ , $x_1$ , $\ldots$ , $x_{\ell}$ distinct complex numbers, such that $$\sum \alpha_i x_i^n$$ are integral for all $n\ge 0$ , then the $x_i$ 's are all algebraic integers. From here we can show that the above sums (depending on $n$ , and forming a recurrent sequence) satisfy an integral linear recurrence. I haven't considered yet the case of sequences of the form : $$\sum_{i=1}^{\ell} P_i(n) x_i^n$$ where $x_i$ are distinct, and $P_i(n)$ are polynomial in $n$ , not $0$ . Any feedback would be appreciated! $\bf{Added:}$ Some definitions for clarity: A linear recurrence sequence is a sequence satisfying a linear recurrence, that is, there exists a $d$ , and $c_k$ , $1\le k \le d$ (apriori $c_k \in \mathbb{C}$ ) such that  for all $n \ge d$ we have $$a_n = \sum_{k=1}^d c_k a_{n-k} \ \ \ (*)$$ that is, $a_n$ dependes linearly on the previous $d$ terms, for all $n\ge d$ . We say that the recurrence has integers coefficients if all $c_0$ , $\ldots$ , $c_d$ are integers. $\bf{Added:}$ A related question ( a particular case). $\bf{Added:}$ Ewan Delanoy showed that $a_n$ satisfies a recurrence with rational coefficients.  A big step forward. I have a proof along this lines: say $K\subset L$ field and and $c_1$ , $\ldots$ $c_d$ in $L$ .  Consider $V$ the $K$ span of $1, c_j$ in $L$ ( a finite dimensional $K$ vector subspace). There exists a $K$ -linear projection $\pi$ from $V$ to $K$ with $\pi(1) = 1$ ( basic linear algebra). Now consider an equality $$a = \sum c_j a_j$$ with $a_j$ , $a \in K$ . Apply the map $\pi$ to it and get $$a= \sum \pi(c_j) a_j$$ Moral: any linear dependence over $L$ of elements in $K$ produces a linear dependence over $K$ of said elements.","['elementary-number-theory', 'algebra-precalculus', 'algebraic-number-theory']"
4540706,Variety of Probabilistic Independence,"We may consider a probability space on $n$ outcomes as a point in $\mathbb{R}^{n+1}$ by assigning outcomes to standard basis vectors, and picking a point with its coordinate on each basis vector equal to the probability of the corresponding outcomes. The point then belongs to the probability simplex: $\Delta^n$ , which is a regular $n$ -simplex given as the convex hull of the standard basis vectors. This gives us a new geometric representation of independence. Let's consider all probability spaces where our outcomes are $(a,b,c...)$ for $1\leq a\leq A$ , $1\leq b\leq B$ and so on, with the coordinates of our outcomes mutually independent. This set is the intersection of $\Delta^{ABC...}$ with an affine variety $V$ in $\mathbb{C}^{ABC...+1}$ . $V$ is the zero set of equations $\
[p(a_0,b_1,c_1,...)+p(a_1,b_1,c_1,...)...]p(a_0,b_0,c_0,...)-[p(a_0,b_0,c_0,...)+p(a_1,b_0,c_0,...)...]p(a_0,b_1,c_1,...)$ , where $p(\cdot)$ denotes probability. What can we say about $V$ ? Its dimension is obvious. Is it connected? Does it have singular points? Does it have a nice parametrization?","['independence', 'geometry', 'affine-varieties', 'manifolds', 'probability-theory']"
4540729,"An upper bound of $\mathbb{P}(|S_n - \log n| \geq C \log n)$, where $S_n$ is a sum of $n$ independent Bernoulli$-\frac{1}{i}$ random variables","Let $(X_i)_{i=1}^n$ be independent Bernoulli random variables with parameter $\frac{1}{i}$ . Let $S_n = \sum_{i=1}^nX_i$ and $C > 0$ . I need a bound of $$\mathbb{P}(|S_n - \log n| \geq C \log n)$$ and apparently I can use Chebyshev's inequality to obtain it. My approach? Well, I first compute the expectation of $S_n$ . Then $$\mathbb{E}[S_n] = \sum_{i=1}^n\mathbb{E}[X_i] = \sum_{i=1}^n\frac{1}{i}.$$ Note that $\mathbb{P}(|S_n - \log n| \geq C \log n)$ can be rewritten as $$\mathbb{P}\left(\left|S_n - \sum_{i=1}^n\frac{1}{i} + \sum_{i=1}^n\frac{1}{i} -\log n\right| \geq C \log n\right).$$ But how should I now proceed? There is a hint that $-\log n + \sum_{i=1}^n\frac{1}{i} \to \gamma$ , as $n \to \infty$ . Here, $\gamma$ is the Euler-Mascheroni constant.","['inequality', 'probability', 'bernoulli-distribution']"
4540731,Sigma-algebra generated by collection of cylinders.,"Let $X$ = { $0,1$ } $^\mathbb{N}$ be the set of all infinite sequences of $0$ ’s and $1$ ’s. A typical element $x \in X$ is written as $x = x_1x_2x_3$ ···. A cylinder set is
a subset of $X$ of the form
{ $x \in X; x_1 = a_1,x_2 = a_2,...,x_m = a_m$ },
with $a_i \in$ { $0,1$ } and $m \in \mathbb{N}$ . The collection $S$ of
cylinder sets is a semi-ring and is closed under finite intersections. We have
also shown that the set function $\mu : S →[0,1]$ given by $\mu($ { $x \in X; x_1 = a_1,x_2 = a_2,...,x_m = a_m$ } $) = (1/2)^m$ ,
can be uniquely extended to a measure on $\sigma(S)$ , and we denote this measure
also by $\mu$ .
Let $B$ be the set $B := $ { $x \in X; x_2 = 0,x_4 = 0,x_6 = 0,...$ }. Show that $B \in σ(S)$ . My attempt is: $S$ can be written as $S =  \bigcup_{m \in \mathbb{N}}S_m$ , where $S_m =$ { $x \in X; x_1 = a_1,x_2 = a_2,...,x_m = a_m$ }. Then if $x_2 = 0$ in $S$ , $B \in S$ . Since $\sigma(S)$ is a $\sigma$ -algebra generated by $S$ , and $S$ is itself a $\sigma$ -algebra, it follows that $\sigma(S) = S$ and hence $B \in \sigma(S)$ . Is this correct?","['semiring', 'measure-theory']"
4540752,"$n$ voters ranks $m$ candidates, what is the probability of the Smith set having cardinality $k$?","Let’s say there are $n$ voters who vote on $m$ candidates. Each voter creates a list where they rank the candidates from most favorite to least favorite. There are $m!$ different possible lists each of them could have made, so there are $m!^n$ possible events. From each event you can define a unique Smith set . Let $f(n,m,k)$ be the amount of events with $n$ voters and $m$ candidates where the Smith set has cardinality $k$ , how can I define this function? Some additional info: $f(n,m,1) + f(n,m,2) + \ldots + f(n,m,m) = m!^n$ $f(n,m,a)=0$ when $a>m$ or $a<1$ The motivation behind the question is that $f(n,m,k)/m!^n$ is the probability of ending up with $k$ winners in a ranked choice vote where you consider the candidates in the Smith set as winners. I want to choose an $n$ and $m$ and plot $f(n,m,k)/m!^n$ where $k$ varies from 1 to $m$ to see the probability of getting different amount of winners depending on the $n$ and $m$ I choose.","['statistics', 'combinatorics', 'voting-theory']"
4540753,Set Of Density Points,"I'm curious about the following operator:  Take some set $A\subseteq\mathbf{R}^n$ , return the set $D(A)$ of all points $x\in \mathbf{R}^n$ such that there is no neighbourhood $U\in\mathcal{N}(x)$ with $A\cap U$ being a (Lebesgue) null set. Is $D(A)$ always measurable (Borel or Lebesgue)? Is $D$ idempotent, i.e. $D(D(A))=D(A)$ ? What can be said about $D(A)\setminus A$ , is it a null set? If the first two questions don't return both true, is there some other way to obtain a measurable set $A'$ from $A$ with $D(A)=D(A')$ ? EDIT I think $D(A)$ is always a Borel set.  Assume $n=1$ , for $i\in\mathbf{N}$ , let $A_i$ be the union of all intervals $I_{i,k}:=[k2^{-i},(k+1)2^{-i}]$ such that $A\cap I_{i,k}$ is not a null set.  Then $D(A)$ should be equal to $\cap_i A_i$ if I'm not mistaken. In $D(A)$ it seems like every point is also an accumulation point of the set, is there some name for this topological property? Regarding the second question, I believe it to be also positive.  One can iteratively cut away all intervals $I_{i,k}$ like above from $A$ when $A\cap I_{i,k}$ is a null set, then the resulting set $A'$ has the same measure as $A$ and $A'\subseteq D(A')$ .  I could imagine that $A'$ is just $A\cap D(A)$ and $D(A)=D(A')$ the topological closure of $A'$ .","['measure-theory', 'lebesgue-measure']"
4540760,Continuity of parametric integral $I(\alpha)=\int_0^\infty \frac{\ln{(1-\alpha^2+\alpha^2x^2)}}{x^2-1}dx$,"How to prove that parametric integral $I(\alpha)=\int_0^\infty \frac{\ln{(1-\alpha^2+\alpha^2x^2)}}{x^2-1}dx$ is differentiable on $(1,\infty)$ ?
I wrote $I(\alpha)$ as $\int_0^1 \frac{\ln{(1-\alpha^2+\alpha^2x^2)}}{x^2-1}dx+\int_1^\infty \frac{\ln{(1-\alpha^2+\alpha^2x^2)}}{x^2-1}dx=I_1(\alpha)+I_2(\alpha)$ , but I'm having difficulties to show that $I_2(\alpha)$ is differentiable on $(1,\infty)$ .
Since $f(x,\alpha)=
\begin{cases}
\frac{\ln{(1-\alpha^2+\alpha^2x^2)}}{x^2-1}, 0<x<1\\
\alpha^2, x=1
\end{cases}$ and $f'(x,\alpha)$ are continuous on $[0,1)\times (1,\infty)$ , $I_1(\alpha)$ is continuous on $(1,\infty)$ , am I right?
Any help is welcome.
Thanks in advance.","['integration', 'convergence-divergence', 'parametric', 'analysis']"
4540769,How is this integral form for the remainder for Taylor formula proved?,"Let $\varphi \colon \mathbb R^n \to \mathbb R, \varphi \in \mathcal C^2(\mathbb R^n)$ . How is this formula proved? $$
\varphi(x)-\varphi(x_0)=\langle D \varphi\left(x_0\right), x-x_0\rangle + \int_0^1\langle D^2 \varphi\left(x_0+(1-t)\left(x-x_0\right)\right)\left(x-x_0\right), x-x_0\rangle d t
$$ Note that in the first order case by the fundamental theorem we have: $$
\varphi(x)-\varphi(x_0)=\int_0^1 \frac{ d\varphi(x_0+t(x-x_0))}{dt} d t= \int_0^1\langle D \varphi\left(x_0+t\left(x-x_0\right)\right), x-x_0\rangle d t
$$ How is the other case proved?","['integration', 'definite-integrals', 'analysis', 'calculus', 'derivatives']"
4540809,If $f(f(x)) = x$ and $f(0) = 1$ then what is the value of $\int_0^1 (x - f(x)) ^{2n} dx$,"I've a question which is mentioned below. If $f(f(x)) = x$ where $x \in [0, 1]$ and $f(0) = 1$ , then find the value of $\displaystyle\int_0^1 (x - f(x))^{2n} dx$ where $n\in \mathbb{N}.$ I tried it and I think I've solved it. I wish if someone could check my work or provide any alternative or easy way to do the same. Here's my work. Given that $f(f(x)) = x$ so $f(x) = f^{-1}(x)$ $\forall\ x\in [0, 1] $ assuming inverse exists . And since $f(0)  = 1$ , we have $f^{-1}(0) = 1$ . Now, from the definition of inverse function, graph of $f(x)$ and $f^{-1}(x)$ are mirror images of each other about the line $y = x$ . Since $f(0) = 1, f^{-1}(1)  = 0$ . This means that $f(x)$ and $f^{-1}(x)$ are coincident lines with equation $x + y = 1$ or $f(x) = 1 -x$ . Graph of the same is attached below. Now Since, $f(x) = 1-x$ , our next task is to find $\displaystyle \int_0^1 (x - f(x))^{2n} dx$ i.e. $\displaystyle\int_0^1 (2x - 1)^{2n} dx$ which is not a hard nut to crack. We have, $$\int_0^1 (2x - 1)^{2n} dx = \frac{(2x-1)^{2n+1}}{2(2n+1)}\bigg|^1_0 = \frac{1}{2(2n+1)} + \frac{1}{2(2n+1)} = \boxed{\frac{1}{2n+1}}.$$ I'm unsure if assuming inverse of the function exists is right. Can anyone please check my work and let me know if I can improve it?","['integration', 'calculus', 'functions', 'definite-integrals']"
4540817,Maximizing $\mbox{tr} \left( {\bf X}^{-1} {\bf A} {\bf X} {\bf B} + 2 {\bf X} {\bf C} \right)$ subject to ${\bf X} {\bf X}^\top = \gamma {\bf I}$,"Can the following optimization statement be converted into a simpler form? $$\begin{array}{ll} \underset{{\bf X}}{\text{maximize}} & \mbox{tr} \left( {\bf X}^{-1} {\bf A} {\bf X} {\bf B} + 2 {\bf X} {\bf C} \right)\\ \text{subject to} & {\bf X} {\bf X}^\top = \gamma {\bf I}\end{array}$$ where $\bf A$ and $\bf B$ are symmetric matrices, and $\gamma > 0$ is a constant. I tried using Lagrange multipliers and ended up with the equation: ${\bf A} {\bf X} {\bf B} + {\bf C}^\top = {\bf X} {\bf \Lambda}$ , where ${\bf \Lambda}$ is the matrix with Lagrange multipliers along its diagonal. I'm not sure how to go about it after that. Please help.","['matrices', 'optimization', 'orthogonal-matrices']"
4540832,Problem with the definition of the Cissoid of Diocles,"As per the definition given on this site , Given an origin O and a point P on the curve, let B be the point where the extension of the line OP intersects the line $x=2a$ and C be the intersection of the circle of radius $a$ and center $(a,0)$ with the extension of OP. Then the Cissoid of Diocles is the curve which satisfies OP=CB. But is this condition applicable to the points on the curve that lie outside the circle? Clearly, if point 'P' was outside the circle, then the extension of OP wouldn't have intersected the circle. What is it that I'm missing here?","['plane-curves', 'algebra-precalculus', 'geometry']"
4540838,Expectation of the number of men seated between two specific women,"I’ve been having a hard time answering this question (I’m bad at combinatorics) - i’d like it if you could help! The Question $m \geq 1$ men and $n \geq 2$ women sit randomly on a bench with $m+n$ places, two of these women are Hilla and Nikki. What is the expectation of the number of men sitting between Hilla and Nikki? So far my thinking was to look at particular cases and then move on to a generalized expression I can calculate. I think that for $i$ men between Hilla and Nikki I can look at the permutations as $(n+m-i-2+1)! = (m+n-1-i)!$ “outside” permutations times “inner” permutation of the Hilla/Nikki area. $(m)(m-1)(m-2)\dots (m-i+1) = \frac{m!}{(m-i)!}$ for picking i men since order matters, and multiply it by $2$ since I can switch Hilla and Nikki around. So overall the expectation is $$E(M)=\sum_{i=1}^m i \cdot 2ֿֿ\cdot \frac{m!}{(m-i)!} \cdot (m+n-1-i) ! \cdot  \frac{1}{(n+m)!}$$ but I think I didn’t account for the possibilities of women sitting between them.
Overall this seems like a really complicated approach.","['expected-value', 'combinatorics', 'probability']"
4540880,Show there are $\frac{6^n}{2}$ possible even results from the sum of $n$ dice rolls.,"I was requested to show there are $\frac{6^n}{2}$ possible even results from the sum of $n$ dice rolls. I did it via induction and was wondering whether my demonstration is correct. $I$ . Let $a_n=\{a_1, a_2,..., a_n\}$ be the sequence of outcomes out of $n$ dice rolls. Let $s_n=\sum_{i=1}^n a_i$ be the sum of all $n$ elements of $a_n$ . $II.$ If $n=1$ we have $s_1=a_1$ , a single result whose possible outcomes are $\{1, 2, 3, 4, 5, 6\}$ . It is trivial to see $3=\frac{6^1}{2}$ possible outcomes are even and the property holds for the base case. Let us assume the number of possible even outcomes of $s_k$ , the sum of $k$ dice rolls, is $\frac{6^k}{2}$ . Then notice that $$s_{k+1}=s_k+a_{k+1}$$ By inductive hypothesis, there are $\frac{6^k}{2}$ even outcomes for $s_k$ . There are $\frac{6^1}{2}$ even outcomes for $a_{k+1}$ , a single dice roll. For the sum of both terms to be even, either both terms are odd or both terms are even. There are $$\frac{6^k}{2}\times \frac{6^1}{2}+\frac{6^k}{2}\times \frac{6^1}{2}=\frac{6^{k+1}}{2}$$ possible ways for this to occurr. Therefore there are $\frac{6^{k+1}}{2}$ possible even outcomes of $s_{k+1}$ and the proof by induction concludes. Two questions . $A)$ Is this proof correct? $B)$ What is an equivalent proof using probability theory instead of discrete mathematics/induction?","['solution-verification', 'combinatorics', 'discrete-mathematics', 'induction', 'probability']"
4540881,"Understanding $\mathcal V(I)$, $\mathcal I(X)$, and their relationship to each other.","The Details: Since definitions vary: A topological space $(X,\tau)$ is a set $\tau$ of subsets of $X$ , called closed subsets , such that $\varnothing, X\in\tau$ , The intersection $$\bigcap_{i\in I}X_i$$ of any closed subsets $(X_i)_{i\in I}$ is closed, where $I$ is arbitrary, and The union of finitely many closed sets is closed. Note that $\tau$ is omitted sometimes when the context is clear. Let $k$ be an algebraically closed field. We denote by $\mathcal{V}(I)$ the set of all zeros of an ideal $I$ of $S:=k[T_1,\dots, T_n]$ . Here, a zero is some $v\in V:=k^n$ such that $f(v)=0$ for all $f\in I$ . For any $X\subseteq V$ , let $\mathcal I(X)\subseteq S$ be the ideal of the $f\in S$ with $f(v)=0$ for all $v\in X$ . The topology on $V$ whose closed subsets are $\mathcal{V}(I)$ for ideals $I$ of $S$ is known as the Zariski topology . We have that the radical $\sqrt{I}$ of an ideal $I$ of $S$ is the ideal of all $f\in S$ with $f^m\in I$ for some $m\in\Bbb N$ . Theorem (Hilbert's Nullstellensatz): If $I$ is an ideal of $S$ , then $$\mathcal I(\mathcal V(I))=\sqrt{I}.$$ A proof can be found in any textbook on algebraic geometry. You could also see here . Lemma: Let $X\subseteq V$ . Then $$\mathcal V(\mathcal I(X))=\overline{X},$$ where $\overline{X}$ is the closure of $X$ w.r.t. the Zariski topology. For a proof, see here . The Question: What is the intuition behind $\mathcal V(I)$ and $\mathcal I(X)$ , and how does one think about the relationship described by Hilbert's Nullstellensatz and the Lemma above? Context: I think I understand some proofs of Hilbert's Nullstellensatz and the Lemma ; however, it doesn't feel like my intuition is strong enough yet for my purposes. I am studying for a postgraduate research degree in linear algebraic groups. Unless I am mistaken, $\mathcal V(I)$ is known as the vanishing set of the ideal $I$ . The name makes sense. I am aware of this: Lemma 2: Let $S$ and $V$ be as above. Then: $\mathcal V(\mathcal I(\mathcal V(P)))=\mathcal V(P)$ for any $P\subseteq S$ and $\mathcal I(\mathcal V(\mathcal I(X)))=\mathcal I(X)$ for any $X\subseteq V$ . However, I'm not sure I understand these relationships. See here (pdf) for a proof of the first bullet point. The proof of the second is said to be similar. What kind of answer am I looking for? Something that illuminates $\mathcal V(I)$ and $\mathcal I(X)$ . A couple of paragraphs each might be enough. Perhaps some exercises could be suggested. I'm not looking for a Royal Road; I'm just trying my best to grasp these key concepts. Do I think I could answer this myself? No, not any time soon. Please help :) Upon further reading, I have seen & understood proofs of the following: For $J\subseteq J'\subseteq S$ , we have $$\mathcal V(J')\subseteq \mathcal V(J).$$ For $X\subseteq X'\subseteq V$ , we have $$\mathcal I(X')\subseteq \mathcal I(X).$$ I get that this is basic stuff. However, I need to be thorough as I am building a lot of theory on these concepts.","['zariski-topology', 'algebraic-groups', 'algebraic-geometry', 'intuition']"
4540884,Solve $x^2y''+xy'+y=0$ using the power series,"Basically I have to find the power series of the ODE above. What I have so far: $y=\sum_{n=0}^{\infty }a_nx^n$ $y'=\sum_{n=1}^{\infty }na_nx^{n-1}$ $y''=\sum_{n=2}^{\infty }(n-1)(n)a_nx^{n-2}$ therefore, $xy'=\sum_{n=1}^{\infty }na_nx^{n}$ $x^2y''=\sum_{n=2}^{\infty }(n-1)(n)a_nx^{n}$ since the counters of the summations are different, I evaluate $y$ at $n=0,1$ and $xy'$ at $n=1$ I then get, ( $\sum_{n=2}^{\infty }(n-1)(n)a_nx^{n} )+(a_1x^1 + \sum_{n=2}^{\infty }na_nx^{n})+(a_0x^0+a_1x^1+\sum_{n=2}^{\infty }a_nx^n)=0$ This then leaves me with $a_0=0$ $2a_1=0; a_1 = 0$ $(n-1)(n)a_n+na_n+a_n=0$ I then get a recursion formula of: $a_n(n^2+1)=0$ which would make my $a_n$ equal to zero. Where am I wrong? Symbolab says that the answer to this ODE is $y=c_1\cos(\ln(x))+c_2\sin(\ln(x))$","['power-series', 'ordinary-differential-equations']"
4540910,Solve $y''-y'-2y=0$ using the power series,"First of all, by solving this the normal way, the answer should be: $y=Ae^{2x}+Be^{-x}$ What I have so far: $y=\sum_{n=0}^{\infty }a_nx^n$ $y'=\sum_{n=1}^{\infty }na_nx^{n-1}$ $y''=\sum_{n=2}^{\infty }(n-1)(n)a_nx^{n-2}$ Shifting the counter to make it all $\sum_{n=0}^{\infty }$ $y'=\sum_{n=0}^{\infty }(n+1)a_{n+1}x^{n}$ $y''=\sum_{n=0}^{\infty }(n+1)(n+2)a_{n+2}x^{n}$ Therefore, I get: $(n+1)(n+2)a_{n+2}-(n+1)a_{n+1}-2a_n=0$ From this, my recursion formula is: $a_{n+2}=\frac{2a_n+(n+1)a_{n+1}}{(n+1)(n+2)}$ I then evaluated this from $n=0$ to $n=6$ but my $a_2$ to $a_8$ values seem to be all over the place. $a_2 = \frac{2a_0+a_{1}}{2!}$ $a_3 = \frac{2a_0+3a_{1}}{3!}$ $a_4 = \frac{6a_0+5a_{1}}{4!}$ $a_5 = \frac{10a_0+11a_{1}}{5!}$ $a_6 = \frac{22a_0+21a_{1}}{6!}$ $a_7 = \frac{42a_0+43a_{1}}{7!}$ $a_8 = \frac{86a_0+85a_{1}}{8!}$ I researched a bit and found out that the coefficients of $a_1$ follow the Jacobsthal sequence while the coefficients of $a_0$ are twice the Jacobsthal sequence but I think this is wrong as it is nowhere near the power series expansion of $y=Ae^{2x}+Be^{-x}$","['power-series', 'ordinary-differential-equations']"
4540914,Using binomial distribution to solve packet-switching and circuit-switching problem,"I have the following problem that I'm really stuck on. I attempted to do part b) and c) but I think I am very wrong since I didn't get the results required in part d). And part (a) I am also stuck on. Can someone please help me solve this problem? In networks, we use physical cables/wires to send electromagnetic
signals from point to point (the signals carry information). You can
only fit so many signals on a wire at once, however. There are two
approaches towards sharing a cable/wire to transmit data in a network:
packet switching and circuit switching. In packet switching, users
randomly send information over the wire whenever they feel like it
whereas in circuit switching, users reserve a fixed percent of the
resources on the wire (regardless of whether they're actively using
it). Assume each user uses 10% of the wire's resources when they send
information on the wire and users are randomly active $p$ % of the
time. If more than 100% of the wire's resources are used at a point in
time, we declare a ""collision"" and no work gets done. Define the
utilization of the wire as the average percent of its resources being
used over time (utilization during a collision is 0). a) in the circuit switching scenario, how many users can share this wire as a function of $p$ ? What is the utilization? Max users = 10
Utilization is p% b) In the packet-switching scenario, how many users can share this wire, assuming we want to keep the probability of collision less than 0.1%? What is the utilization? (Write an equation that would let you solve for the number of users/utilization given p Max users = $$\sum_{k=0}^{10} {n\choose k} p^k(1-p)^{n-k} \geq 0.999$$ Utilization = $$\sum_{k=0}^{10} {n\choose k} \frac{k}{10} p^k(1-p)^{n-k} $$ c) Solve for the previous two answers with p=50 percent and p=5 percent $p=0.5$ : $$\sum_{k=0}^{10} {11 \choose k} (0.5)^k(1-0.5)^{11-k} >0.999$$ $$\sum_{k=0}^{10} {12 \choose k} (0.5)^k(1-0.5)^{12-k} <0.999$$ so max users is 11. utilization: $$\sum_{k=0}^{10} {11\choose k} \frac{k}{10} p^k(1-p)^{11-k} \approx 0.55$$ $p=0.05$ $$\sum_{k=0}^{10} {73 \choose k} (0.5)^k(1-0.5)^{73-k} >0.999$$ $$\sum_{k=0}^{10} {74 \choose k} (0.5)^k(1-0.5)^{74-k} <0.999$$ so max users is 73 Utilization is: $$\sum_{k=0}^{10} {73\choose k} \frac{k}{10} p^k(1-p)^{73-k} \approx 0.36$$ d) You should have found that for p=5 percent, the packet-switched network can support more users than for p=50 percent. Why, then, does the expected utilization decrease when p=5 percent? I found that p=5 supports more users than p=50 but the expected utilization doesnt decrease.","['probability-distributions', 'binomial-distribution', 'probability']"
4540918,What is the pattern in the powers of $\sqrt{2}-\sqrt{1}$? [duplicate],"This question already has answers here : proof that the pattern exists for all n [closed] (3 answers) Closed 1 year ago . What is the pattern in this? $$\begin{align}
\left(\sqrt{2}-\sqrt{1}\right)^1 &= \sqrt{2}-\sqrt{1}\\
\left(\sqrt{2}-\sqrt{1}\right)^2 &= \sqrt{9}-\sqrt{8}\\
\left(\sqrt{2}-\sqrt{1}\right)^3 &= \sqrt{50}-\sqrt{49}\\
\left(\sqrt{2}-\sqrt{1}\right)^4 &= \sqrt{289}-\sqrt{288}\\
\end{align}$$ I thought of applying the binomial theorem","['elementary-number-theory', 'algebra-precalculus', 'abstract-algebra', 'pattern-recognition']"
4540927,Are convex polytopes closed in arbitrary metric spaces?,"Let $(X,d)$ be a metric space. For all points $x,y \in X$ we define the metric segment between them as the following set: $$\left [ x,y \right ] =  \left \{ z \in X : d(x,z)+d(z,y)=d(x,y)\right \}$$ We then say that a set $S\subseteq X$ is convex if for all $x,y \in S$ it holds true that $\left [ x,y \right ] \subseteq S$ . It can be easily shown that arbitrary intersection of convex sets in metric spaces is a convex set. Therefore, for each subset $S \subseteq X$ of a metric space $(X,d)$ we define its convex hull as the set $\mathrm{conv}(S)=\bigcap_{}^{} \left \{ U  \supseteq S : U \; \mathrm{convex}  \right \}$ . We say that a set is a convex polytope if it is a convex hull of a finite set. My question is are convex polytopes in metric spaces closed sets?","['general-topology', 'convexity-spaces', 'metric-spaces', 'real-analysis']"
4540932,"If $f:\mathbb{R} \to \mathbb{C}$ is a Borel function such that $|f|=1,$ then there exist $\alpha<\beta$ such that $\int_{\alpha}^{\beta}f(x)dx \neq 0$",Let $f:\mathbb{R} \to \mathbb{C}$ be a Borel function such that $|f|=1.$ Is it true that there exist $\alpha<\beta$ such that $\int_{\alpha}^{\beta}f(x)dx \neq 0$ ? Of course if $f$ was continuous then this follows immediately since there exists $\delta>0$ such that $$\left|\left|\int_{0}^\delta f(x)dx\right|-\delta\right| \leq \int_0^{\delta}|f(x)-1|dx \leq \frac{1}{2}\delta$$ so that $\int_0^{\delta}f(x)dx \neq 0.$,"['integration', 'measure-theory', 'lebesgue-integral', 'analysis', 'real-analysis']"
4540940,Second Derivative Test in Banach Spaces,"According to $[$ Exercise $12.8$ , $1]$ we have the following version of the Second Derivative Test: Theorem. Let $E=(E,\|\cdot \|)$ be a Banach space, let $D$ be a subset of $E$ and suppose $f: D \rightarrow \mathbb{R}$ is $2$ times continuously differentiable at a point $x \in D$ . If $f'(x)=0$ and the bilinear form $f''(x)$ is positive definite that is $[f''(x)(v)](v)>0$ for every nonzero $v \in E$ , then $f$ attains a local mininum at $x \in D$ that is there exists an $\varepsilon>0$ such that if $\|y-x\|<\varepsilon$ , then $f(x)<f(y)$ . Here, the differentiability is in the Fréchet sense, where $f'(x):E \rightarrow \mathbb{R}$ denote the first Fréchet derivative of $f$ at $x$ and $f''(x)$ stands the second Fréchet derivative of $f$ at $x$ so that for each $v \in E$ , $f''(x)v \in \mathcal{L}(E, \mathbb{R})$ . Moreover, $f$ be $2$ times continuously differentiable in $x$ means that the function $y \mapsto f'(y)$ is continuously differentiable in $x$ that is $y \mapsto f'(y)$ is differentiable at each point in a neighborhood of $x$ and $y \mapsto f'(y)$ is continuous at $x$ . Question. The theorem above is true? If is true, how to prove? I've seen similar results $($ for instance in $[2])$ . However, in general such results require that the domain $D$ of the functional be either open/convex $($ for instance, a open ball in $E)$ . In this case, we have that the domain of the functional is just a subset of $E$ . So I seriously doubt whether such a result remains valid in this subset context. In $[1]$ the author gives a hint to prove the theorem: use the Mean Value Theorem $([$ Theorem $12.7$ , $1])$ twice to show that $f(y)-f(x)>0$ for all $y \in E$ in a sufficiently small ball around $x$ . But even then I still haven't been able to prove such a result. Is it possible to prove it this way? $[1]$ Baggett, L. Functional analysis. A primer . Marcel Dekker, Inc., New York, $1992$ . $[2]$ Deimling, K., Nonlinear functional analysis . Springer-Verlag, Berlin, $1985$ .","['nonlinear-analysis', 'frechet-derivative', 'maxima-minima', 'calculus', 'functional-analysis']"
4540956,Find the second derivative of $y=\left(1-2\sqrt{x}\right)^3$,"Find the second derivative of $$y=\left(1-2\sqrt{x}\right)^3$$ Let's find the first derivative: $$y'=3(1-2\sqrt{x})^2(1-2\sqrt{x})'=3\left(0-2\dfrac{1}{2\sqrt{x}}\right)(1-2\sqrt{x})^2=-3\dfrac{1}{\sqrt{x}}(1-2\sqrt{x})^2$$ The second derivative of a function is the derivative of the derivative of that function, so $$y''=\left(-3\dfrac{1}{\sqrt{x}}(1-2\sqrt{x})^2\right)'=-3\left(\dfrac{1}{\sqrt{x}}\cdot(1-2\sqrt{x})^2\right)'=-3T$$ I am really having troubles with finding that derivative as I get confused (too many things going on). $$T=\left(\dfrac{1}{\sqrt{x}}\right)'(1-2\sqrt{x})^2+\dfrac{1}{\sqrt{x}}\left((1-2\sqrt{x})^2\right)'=\\-\dfrac12x^{-\frac32}(1-4\sqrt{x}+4x)-\dfrac{2}{x}(1-2\sqrt{x})$$ Is there an easier approach?","['calculus', 'derivatives']"
4541004,Show $\binom{n}{0} + \binom{n}{1} + ... + \binom{n}{n} = 2^n \space \space \forall n \in \mathbb{N} $,"I was requested to show $$\binom{n}{0} + \binom{n}{1} + ... + \binom{n}{n} = 2^n \space \space \forall n \in \mathbb{N} $$ Since I self study, I have no professor to tell me whether my solution is correct or not. I wonder if anyone could provide some validation. Here is what I did. $I$ . Let $n=1$ and the equality trivially holds. $II.$ Let our inductive hypothesis $\text{HI}$ be that $$\sum_{i=0}^k \binom{k}{i} = 2^k$$ Then we want to show $$\sum_{i=0}^{k+1} \binom{k+1}{i} = 2^{k+1}$$ $III.$ $$
\begin{aligned}
\sum_{i=0}^{k+1} \binom{k+1}{i} 
&= \sum_{i=0}^k \binom{k+1}{i} + \binom{k+1}{k+1}
\\
&= \sum_{i=0}^k \Big(\binom{k}{i-1}+\binom{k}{i}\Big) + 1 
&&\text{By Pascal's triangle formula}
\\
&=\sum_{i=1}^k\binom{k}{i} + \sum_{i=0}^k \binom{k}{i} + 1
\\
&=\sum_{i=0}^k\binom{k}{i} - \binom{k}{0} + 2^k+1 
&&\text{HI}
\\
&=2^k-1+2^k+1
\\
&=2^{k+1}\ ,
\end{aligned}
$$ and thus we have proved what we set out to show. The two questions I always have when finishing a proof: Is the proof correct? Are there alternative, perhaps simpler proofs? Thanks in advance.","['solution-verification', 'binomial-coefficients', 'discrete-mathematics']"
4541006,"$f:[0,1]\times Y\to \mathbb{R}$ be a continuous function, prove $I_f(y)=\int_0^1f(x,y)dx$ is continuous. $Y$ is metric space","Let $f:[0,1]\times Y\to \mathbb{R}$ be a continuous function where $Y$ is a metric space. Now let $I_f(y):Y\to \mathbb{R}$ be the function $$I_f(y)=\int_0^1f(x,y)dx$$ then prove that $I_f(y)$ is continuous on $Y$ . I tried this problem first fixing $y$ .  If we fix $y$ then $g(x)=f(x,y)$ is continuous on $[0,1]$ , hence it is uniformly continuous. Therefore $\forall\ \epsilon >0\ \exists \ \delta>0$ such that $$|x-x'|<\delta\implies |f(x,y)-f(x',y)|<\epsilon$$ Suppose we want to prove continuity at $y$ . Now since $f$ is continuous for every $\epsilon>0$ there exists $\delta>0$ such that $|h|<\delta\implies |f(x,y+h)-f(x,y)|<\epsilon$ . I want to show $|f(x,y+h)-f(x,y)|<\epsilon$ for all $x\in [0,1]$ and for $|h|<\delta$ where $\delta>0$ so that $$\int_0^1|f(x,y+h)-f(x,y)|dx< \int_0^1\epsilon\, dx=\epsilon$$ But i cant not do anything further with this. How to do this","['integration', 'calculus', 'analysis', 'real-analysis']"
4541009,Inverse of a bounded operator on a Hilbert space,"Consider a bounded (linear) operator $T \in \mathcal{L}(H)$ on a Hilbert space $H$ . We say $T$ is invertible if it has a bounded inverse, i.e. if $\exists T^{-1} \in \mathcal{L}(H)$ such that $T^{-1} T = T T^{-1} = I \in \mathcal{L}(H)$ . It is easy to see that, if $T$ is invertible, then $T$ is bounded from below, meaning that $$ \exists c >0 : \, \| Tx \| \geq c \| x \| \, , \quad \forall x \in H $$ (one can take $c = 1/\| T^{-1} \|$ ). My question is: does the converse hold? i.e. does boundedness from below imply invertibility? If no, can you give an example of a $T$ which is bounded from below but not invertible?","['hilbert-spaces', 'inverse', 'functional-analysis']"
4541100,Variations of permutations and Combinations,"In my script about combinatorics, until now we have considered and studied the following cases: Permutation without repetition and distinguished objects. Permutation with repetition and distinguished objects. Combination without repetition and distinguished objects. Combination without repetition and distinguished objects. Then, in the end we consider the case of: Permutations without repetition and indistinguishable objects. (Here I'd like to point out, that we considered the case when we want to find the nr. of permutations when we use n elements from a set with n elements, in other words all the elements of the set. The formula, which was given but I was also able to derive on my own, is: $\frac{n!}{n_1!n_2!...n_k!}$ , where n_1 is the nr. of indistinguishable elements of type one in the set,etc etc. Would the formula be the same if we would want to find the nr. of permutations using r ( $r \le n$ ) elements?) Would it make sense to consider case 2-4 for when we have indistinguishable elements? How would the formulas change?","['statistics', 'combinatorics', 'stochastic-calculus', 'probability']"
4541103,Inverse of the function $f(x)=\sqrt{x-3}-\sqrt{4-x}+1$,"I am trying to find the inverse of the function $$f(x)=\sqrt{x-3}-\sqrt{4-x}+1$$ First of all its domain is $[3,4]$ As far as my knowledge is concerned, since $f'(x)>0$ , it is monotone increasing in $[3,4]$ , so it is injective. Also the Range is $[0,2]$ . So $$f:[3,4]\to [0,2],\:\:y=f(x)=\sqrt{x-3}-\sqrt{4-x}+1$$ is Invertible.
Now to find $f^{-1}(x)$ , we need to express $x$ in terms of $y$ . We have: $$\begin{aligned}
&(y-1)^2=1-2 \sqrt{(x-3)(4-x)} \\
&\Rightarrow \quad 2\sqrt{(x-3)(4-x)}=1-(y-1)^2 \\
&\Rightarrow \quad 4(x-3)(4-x)=4 y^2+y^4-4 y^3 \\
&\Rightarrow \quad 4x^2-28 x+y^4-4 y^3+4 y^2+48=0
\end{aligned}$$ Which is a quadratic in $x$ . But how to decide which root of $x$ ?","['functions', 'monotone-functions', 'inverse-function']"
4541136,Is every ordered abelian group the additive group of an ordered ring?,"Let $\Lambda$ be an ordered abelian group, (there is a total order on $\Lambda$ which is compatible with addition). Is there a multiplication map on $\Lambda$ that turns it into an ordered ring? I have tried using classification results, such as $\Lambda$ is a subgroup of $\mathbb{R}^\Omega$ for some set $\Omega$ (Hahn embedding theorem) and the classification of subgroups of $\mathbb{Q}$ . Partial results are also welcome.","['ordered-groups', 'ordered-rings', 'ring-theory', 'abstract-algebra', 'abelian-groups']"
4541238,Prove that a function is strictly increasing,"I have the following function $$
f(x)= x\left (1 + \displaystyle{\sum_{j=1}^M \  \frac{p}{1 + D_{j}\cdot x}}  \right)- t.
$$ $x$ is the only variable (the others are positive constants). We have been asked to show two things: Function is positive for some large values of $x$ . Function is strictly increasing for positive $x$ . For (1) I can just use a few values to prove it. For the second part, I'm thinking of taking the derivative of this function and proving that it will always be positive. I got the derivative and said that for very large values of $x$ (as $x\to\infty$ ), the derivative will converge to $1$ . Thus it will be strictly increasing. Just hoping someone can help me confirm this.","['calculus', 'functions', 'sequences-and-series']"
4541247,About set operations,"let $A,B$ be countable unions $A:=\cup_{i=1}^\infty A_i$ and $B:=\cup_{i=1}^\infty B_i$ . I am reading a book and it says $(A-B)\subseteq \cup_{i=1}^\infty (A_i-B_i)$ . It is probably trivial but I don't understand why? My attempt: Let $x\in A-B$ . If I can show that $x\in A_i-B_i$ for some $i$ , then I am done. Since $x\in A-B$ , we have $x\in A$ which means $x\in A_i$ for some $i$ . Now $x\in A-B$ again implies $x\in A_i-B_j$ for some $j$ . Is that the idea here? Thanks.",['elementary-set-theory']
4541308,Solution verification and how to prove a function isn't integrable,"Let $H(x)=x^2\sin(\pi/x^2)$ if $x\in]0,1]$ and $H(0)=0$ . Show that $H'(x)=h(x)$ exists for all $x\in[0,1]$ . Also, show that $H'$ is not Riemann integrable in $[0,1]$ . This way, $h$ has an anti derivative in $[0,1]$ but it isn't Riemann integrable in said interval. Finally, show that $\lim_{a\to 0+}\int_{a}^{1}h(x)dx$ exists. I think I was able to prove $H'(x)=h(x)$ exists for all $x\in[0,1]$ . My attempt: If $x=0$ $$\lim_{a\to 0}\frac{H(0+a)-H(0)}{a}$$ $$=\lim_{a\to 0} a\sin(\pi/a^2)=0,$$ using $|\sin(\beta)|\leq 1$ and squeeze theorem. So, $H'(0)=0$ Now, for $x\in(0,1]$ we have that $$\frac{d}{dx}\left(x^2\sin(\pi/x^2)\right)$$ $$=2x\sin(\pi/x^2)-\frac{2\pi\cos(\pi/x^2)}{x}.$$ Is my attempt correct? Does it prove what I was asked? Also, how can I do the rest of the exercise? I honestly don't know how to prove that $H'$ isn't integrable in $[0,1]$ or that $\lim_{a\to 0+}\int_{a}^{1}h(x)dx$ exists. Isn't the function $$h(x)=H'(x)=2x\sin(\pi/x^2)-\frac{2\pi\cos(\pi/x^2)}{x}$$ if $x\in(0,1],$ $$H'(0)=0$$ bounded?
Thanks in advance.","['real-analysis', 'calculus', 'solution-verification', 'derivatives', 'riemann-integration']"
4541322,Why use elementary matrices?,"I just started learning linear algebra and I'm having a hard time figuring out why creating an elementary matrix to perform row operations on another matrix is necessary if we could just perform the row operations on the matrix itself. Also, which of the two methods would be more efficient?","['matrices', 'gaussian-elimination', 'linear-algebra']"
4541323,Turan number of cycles of length at most $2k$ for $k\geq 2$.,"Theorem (short even cycles) For any integer $k\geq 2$ , we have $\text{ex}(n,\{C_4,\dots,C_{2k}\})=O_k(n^{1+1/k}).$ Remark: $\text{ex}(n,\{C_4,\dots,C_{2k}\})$ is the maximal number of edges in the $n$ -vertex graph which does not contains an even cycle of length at most $2k$ . To prove this theorem we need the following two technical lemmas. Lemma 1 (Large bipartite subgraph) Every $G$ has a bipartite subgraph with at least $e(G)/2$ edges. Lemma 2 (Large average degree implies subgraph with large minimum degree) Let $t>0$ . Every graph with average degree $2t$ has a
subgraph with minimum degree greater than $t$ . Proof of Theorem. Suppose that the graph $G$ is the maximizer, i.e. $\text{ex}(n,\{C_4,\dots,C_{2k}\})=e(G)$ . Applying Lemma 1, we find a bipartite subgraph $G'$ of $G$ such that $e(G')\geq \dfrac{e(G)}{2}$ . Now applying Lemma 2, we find a subgraph $G''$ of $G'$ such that $$\delta(G'')>\underbrace{\dfrac{d_{\text{ave}}(G')}{2}}_{=t}=\dfrac{e(G')}{\nu(G')}\geq\dfrac{e(G)}{2\nu(G)}.$$ Notice that $G''$ is a bipartite subgraph of $G'$ (and hence of $G$ ) with $\delta(G'')>t\geq\dfrac{e(G)}{2\nu(G)}$ . Let $u$ be an arbitrary vertex of $G''$ . For each $i=0,1,\dots,k$ , let $A_i$ denote the set of vertices at distance exactly $i$ from $u$ . For each $i=1,\dots,k-1$ , every vertex of $A_i$ has: no neighbors in $A_i$ (or else $G''$ would not be bipartite), exactly one neighbor in $A_{i-1}$ (else we can backtrace through two neighbours which must converge at some point to form an even cycle of length at most $2k$ ), and thus $>t-1$ neighbors in $A_{i+1}$ (by the minimum degree assumption on $G''$ ). Therefore, each layer $A_i$ expands to the next by a factor at least $t-1$ . Hence $$\nu(G)\geq |A_k|\geq (t-1)^k\geq \left(\frac{e(G)}{2\nu(G)}-1\right)^k.$$ And thus $$e(G)\leq 2\nu(G)^{1+1/k}+2\nu(G).$$ I understand the idea of the proof but there are some technical moments which I did not understand: Questions. "" every vertex of $A_i$ has no neighbors in $A_i$ "". Am I right that if this happens then we have an odd cycle in bipartite graph $G''$ which is absurd? Here $A_1$ is basically the neighbors of $u$ , i.e. $N(u)$ which exists because $|A_1|=\deg(u)\geq \delta(G'')>t>0$ and hence $|A_1|\geq 1$ . But how do we know that $A_2,\dots, A_k$ exist? I think that it follows if $t>1$ but I cannot prove that $t>1$ . Thanks a lot!","['graph-theory', 'extremal-combinatorics', 'combinatorics', 'discrete-mathematics', 'extremal-graph-theory']"
4541362,Finding the limit in terms of a without using L'Hospitals,"Find $\;\lim_\limits{x \to 0} \dfrac{1-\cos(ax)}{1-\sqrt{1+x^2}}\;$ in terms of a without using L'Hospitals Rule. I first graphed the function and noticed that that limit tends to go towards $-a^2$ I also tried this approach: $\lim_\limits{x \to 0} \dfrac{1-\cos(ax)}{1-\sqrt{1+x^2}}=\lim_\limits{x \to 0} \dfrac{2 \cdot \frac{1-\cos(ax)}{2}}{1-\sqrt{1+x^2}} = \lim_\limits{x \to 0} \dfrac{2\sin^2(\frac{ax}{2})}{1 - \sqrt{1+x^2}} $ However, I'm not sure how to proceed. I tried rationalizing the denominator but it only makes it more tangled.","['limits', 'calculus', 'limits-without-lhopital']"
4541368,What happens when an even natural number $n$ meets the integral $\int_{0}^{\frac{\pi}{2}} x\tan 2 x \ln^n (\tan x) d x$?,"In my post , I found that $$ \int_{0}^{\frac{\pi}{2}} x\tan 2 x \ln (\tan x) d x = -\frac{\pi^{3}}{32},$$ then I  want to generalise it to $$ I_n=\int_{0}^{\frac{\pi}{2}} x\tan 2 x \ln^n (\tan x) d x $$ Again, letting $x\mapsto \frac{\pi}{2}-x$ transform the integral $$
\begin{aligned}
I_n=&\frac{(-1)^{n+1} \pi}{2} \int_0^{\frac{\pi}{2}} \tan (2 x) \ln ^n(\tan x) d x +(-1)^n \int_0^{\frac{\pi}{2}} x \tan (2 x) \ln ^n(\tan x) d x
\end{aligned}
$$ Fortunately, when $n$ is odd , $$
I_n=-\frac{\pi}{4} \underbrace{\int_0^{\frac{\pi}{2}} \tan (2 x) \ln ^n(\tan x) d x}_{J_n}
$$ Helpfully, the substitution $y=\tan^2 x$ simplifies the integral $$
\begin{aligned}
J_n &=\int_0^{\frac\pi2} \frac{2 \tan x}{1-\tan ^2 x} \ln ^n(\tan x) dx =\frac{1}{2^n} \int_0^{\infty} \frac{\ln ^n y}{1-y^2} d y \\
&=\frac{1}{2^{n-1}} \int_0^1 \frac{\ln ^n y}{1-y^2} d y=\frac{n !}{2^{n-1}}\left(1-\frac{1}{2^{n+1}}\right)\zeta(n+1)
\end{aligned} $$ (For the last integral please refer to the footnote) Now we can conclude that $$\boxed{\int_{0}^{\frac{\pi}{2}} x\tan 2 x \ln^n (\tan x) d x =-\frac{n ! \pi }{2^{n+1}}\left(1-\frac{1}{2^{n+1}}\right) \zeta(n+1)} $$ where $n$ is a odd natural numbers. For examples, $$
\begin{aligned}
I_1 &=-\frac{\pi \cdot 1 !}{2^2}\left(1-\frac{1}{2^2}\right) \zeta(2)=-\frac{\pi^3}{32} \\
I_3 &=-\frac{\pi \cdot 3 !}{2^4}\left(1-\frac{1}{2^4}\right) \zeta(4)=-\frac{\pi^5}{256} \\
I_5 &=-\frac{\pi \cdot 5 !}{2^6}\left(1-\frac{1}{2^6}\right) \zeta(6)=-\frac{\pi^7}{512} \\
I_{23} &=-\frac{\pi \cdot 23 !}{2^{24}}\left(1-\frac{1}{2^{24}}\right) \zeta(24) =-\frac{968383680827 \pi^{25}}{536870912}
\end{aligned}
$$ Succeeding in finding a formula for $I_{2n+1}$ with odd powers, I am eager to find one for the $I_{2n}$ with even powers as the technique just used fails when $n$ is even. Could you help? Footnote: $$\int_0^1 \frac{\ln ^n y}{1-y^2} d y =\sum_{k=0}^{\infty} \int_0^1 y^{2 k} \ln ^n y d y =\left.\frac{\partial^n}{\partial a^n} \int_0^1 y^a d y\right|_{a=2 k} \\= \sum_{k=0}^{\infty}\frac{(-1)^n n !}{(2 k+1)^{n+1}} =n!\left(1-\frac{1}{2^{n+1}}\right) \zeta(n+1) $$","['integration', 'definite-integrals', 'calculus', 'trigonometric-integrals', 'zeta-functions']"
4541392,Is a trig substitution the only way to solve $ \int_a^b \frac{1}{\left(1 + cx^2\right)^{3/2}} \mathrm{d}x $?,"I have an integral that looks like the following: $$
\int_a^b \frac{1}{\left(1 + cx^2\right)^{3/2}} \mathrm{d}x
$$ I have seen a method of solving it being to substitute $x = \frac{\mathrm{tan}(u)}{\sqrt{c}}$ ; however, this seems somewhat sloppy to me. Is there perhaps a better way of tackling this integral?","['integration', 'trigonometric-integrals', 'definite-integrals']"
4541408,Why I am get complex values in this integral?,"I would like to get the following integral: $$\int -\frac{\log(a^2+x^2)}{(a^2+x^2)}dx \quad \text{or} \quad \int_{t}^{+\infty}-\frac{\log(a^2+x^2)}{(a^2+x^2)}dx$$ where $t>0$ . I used WolframAlpha to compute, and I got following expression: I am just wondering why some complex value i appears? Any ideas of how to get the closed form of integral from t to +inf ? I guess I need to give more specification to wolframalpha to compute ?","['indefinite-integrals', 'definite-integrals', 'analysis']"
4541425,Limiting Property of an Integral?,"I have observed an interesting property which could be wrong as it seems completely absurd but the motivation behind it makes me inclined to believe it is true (or at least in some cases). I do not have any pure mathematical background so I do not know what I am talking about but I wish this kind community would get the gist of what I am trying to ask and would kindly direct me to an answer. So here goes nothing. The Observation : Suppose $g(t)$ is a periodic, symmetric function if it satisfies the following conditions: There exists a nonzero constant T such that $g(t+T) = g(t)$ . (periodic) For all $ \delta \in[0,\frac{T}{2}]$ , $g(\frac{T}{2}-\delta) = g(\frac{T}{2}+\delta)$ . (symmetric) Then for any integrable function $f(t)$ , $\lim_{w \to \infty} \int f(t)g(wt)dt = \alpha \int f(t)dt$ where $\alpha$ is some constant. My question is: Is this in any way true? The Motivation: One function that satisfies the function $g(t)$ is $sin^2(t)$ to which I find with several computational evidences that $\lim_{w \to \infty} \int f(t)\sin^2(wt)dt = \frac{1}{2} \int f(t)dt$ . Thank you for entertaining this question. If you find this question to be nonsensical, then I apologize in advance.","['limits', 'calculus', 'real-analysis']"
4541437,Sum of i.i.d. random variables for which Chebyshev inequalities are tight,"Chebyshev's inequalities : Let $X$ be a random variable with finite expected value $\mu$ and finite non-zero variance $\sigma^{2}$ . Then for any real number $\delta > 0$ , $$ \Pr[|X - \mu| \geq \delta\sigma] \leq \frac {1}{\delta^{2}}$$ There is a tight example in wiki, $X_c$ is a random variable with $\sigma = 1/c$ : $$
\left\{
\begin{aligned}
&\Pr[X_c = -1] = \frac{1}{2c^{2}}  \\
&\Pr[X_c = 0] = 1 - \frac{1}{c^{2}}  \\
&\Pr[X_c = 1] = \frac{1}{2c^{2}}  \\
\end{aligned}
\right.
$$ If $\delta = c$ , then we have $$ \Pr[|X - \mu| \geq \delta\sigma] = \Pr[|X| \geq 1] = \frac {1}{\delta^{2}}$$ But for $\delta > c$ , it is not tight.
Is there another example that is tight for infinite large $\delta$ ? In addition, suppose $X_{1}, X_{2}, \ldots, X_{n}$ are i.i.d. random variables with finite expected value $\mu$ and finite non-zero variance $\sigma^{2}$ .
According to Chebyshev's inequalities: $$\Pr\left[\left|\sum_{i}^{n}X_{i} - n\mu\right| \geq \delta n\sigma\right] \leq \frac{1}{n\delta^{2}}$$ Is there also an (asymptotic) tight example for $\{ X_{i} \}_{i}$ ?","['concentration-of-measure', 'probability-distributions', 'probability', 'random-variables']"
4541453,"Pairs $(x,y)$ such that $3x-1$ is divisible by $y$ and $3y-1$ is divisible by $x$.","How many possible pairs of integers $x$ and $y$ exist such that $3x-1$ is divisible by $y$ and $3y-1$ is divisible by $x$ ? Since $y| 3x-1$ , there exists some $k$ such that $3x-1 = ky$ . On the other hand since $x| 3y-1$ we have $3y-1 = lx$ for some $l$ . i.e. $9y-3 = l(3x) = l(ky+1)$ which implies $(9-kl)y = l+3$ . I am not able to procced further. Any hints will be helpful. Thanks.","['number-theory', 'divisibility', 'elementary-number-theory']"
4541488,How to prove Cauchy Distribution doesn't belong to Exponential Family?,"My idea is that the Expectation of Cauchy Distribution is infinite, and I find the following lemma If $X$ were from the exponential family, it would have finite expectation. The question is solved if I can prove the lemma, but I have no idea how to prove it. I could get that if $f_X(x;θ)$ can be written as $$f_X(x;\eta)=h(x)\exp\left(\sum_{i=1}^s\eta_iT_i(x)−A(\eta)\right)$$ then $E[T_i(X)]=\frac{\partial A(\eta)}{\partial η_i}$ is finite But I don't know how to apply this to show that the expectation $E[X]$ is finite.","['expected-value', 'statistics', 'probability', 'exponential-family']"
4541545,A weighted sum of independent Poisson random variables $X_1 + 2X_2 + 3X_3+\dots+nX_n$,"I have that for $1 \leq i \leq n$ , the mutually independent random variables $$X_i \sim \text{Poisson}(\mu_i)$$ Then what is the distribution of $$Y \sim \sum_{i=1}^{n}i X_i$$ It looks a bit like an expectation, so I am interested to know if anything is known about it? Otherwise, the best we can do to obtain $P(Y = k)$ is to sum over integer partitions of $k$ with part numbers which are Poissonly distributed with means $\mu_1, \mu_2,\dots$ etc?","['poisson-distribution', 'probability-distributions', 'probability']"
4541582,Is there anything wrong with this proof of chain rule?,"Is there anything wrong with this proof of the Chain Rule? $$\begin{align}
(f(g(x)))'&=\lim_{h\to 0} \frac{f(g(x+h))-f(g(x))}{h}\tag{eq 1}\\
g'(x) &= \lim _{h\to 0} 
\frac{g(x+h))-g(x)}{h}\\
g'(x)\cdot\lim_{h\to 0} h &= \lim _{h\to 0}\,(g(x+h)-g(x))\\
g'(x)\cdot\lim _{h\to 0} h + \lim _{h\to 0} g(x) &= \lim _{h\to 0} g(x+h)\\
\lim _{h\to 0} g(x+h)) &= g'(x)\cdot \lim _{h\to 0} h + \lim _{h\to 0} g(x)\tag{eq 2}
\end{align}$$ Substitute Equation 2 into Equation 1: $$
\begin{align}
(f(g(x)))'&=\lim _{h\to 0}
\frac{f(g(x+h))-f(g(x))}{h}\\
&=\lim _{h\to 0} \frac{f(g'(x)\cdot h + g(x))-f(g(x))}{h}
\end{align}$$ Let $g'(x)\cdot h = h'$ . $$\begin{align}
\lim _{h'\to 0}
\frac{f(g'(x)\cdot h+ g(x))-f(g(x)))}{h}
&=\lim _{h'\to 0} \frac{
f(h' + g(x))-f(g(x))}{h}\\
&=g'(x)\cdot\lim _{h'\to 0}
\frac{f(h'+g(x))-f(g(x))}{g'(x)h}\\
&= g'(x)\cdot\lim_{h'\to 0}
\frac{f(h'+g(x))-f(g(x))}{h'}\\
&= g'(x) f'(g(x))
\end{align}$$ Hence $\;(f(g(x)))' = g'(x) f'(g(x))$","['limits', 'derivatives']"
4541589,Any reference on Jensen inequality for measurable convex functions on a Banach space?,"The only proof of Jensen inequality (and most general version) that I know is a direct consequence of the Fenchel-Moreau Theorem : If $X$ is a locally convex Hausdorff topological space, let $\mu$ be a Borel probability measure and $x\in X$ be such that for all continuous linear functional $x^\ast\in X^\ast$ , $\int_X \langle y,x^\ast\rangle~d\mu=\langle x,x^\ast\rangle$ then we say that $\mu$ averages to $x$ , in symbol $\mu\sim x$ . The Fenchel-Moreau theorem states that the bidual of a proper l.s.c. convex function $f$ is the function itself. Recall that $f^\ast(x^\ast)=\sup_{x\in X} \langle x,x^\ast \rangle-f(x)$ and $f^{\ast\ast}(x)=\sup_{x^\ast\in X^\ast} \langle x,x^\ast\rangle-f^\ast(x^\ast)$ , the theorem states that on $X$ , $f=f^{\ast\ast}$ . Suppose that $\mu\sim x$ and $f$ is a proper l.s.c. convex function, then by Fenchel's inequality, for any $y\in X$ and any $x^\ast\in X^\ast$ , $\langle y,x^\ast\rangle\leq f(y)+f^\ast(x^\ast)$ , taking the integral over $\mu$ we get \begin{align*}
\langle x,x^\ast\rangle &= \int_X \langle y,x^\ast\rangle d\mu(y)\\
&\leq \int_X \left[f(y)+f^\ast(x^\ast)\right] d\mu(y)\\
&= \int_X f ~d\mu + f^\ast(x^\ast)
\end{align*} and therefore $\langle x,x^\ast\rangle-f^\ast(x^\ast)\leq \int_X f ~d\mu$ for all $x^\ast\in X^\ast$ , taking the supremum of the LHS over $x^\ast\in X^\ast$ we get $f(x)=f^{\ast\ast}(x)\leq \int_X f~d\mu$ . I am wondering if the result can be extended to any measurable convex function and if there is any literature on the subject. Is there something like For any Borel probability measure $\mu$ such that $\mu\sim x\in X$ and any bounded measurable convex functional $f:X\to\mathbb R$ , $f(x)\leq \int_X f~d\mu$ . Or is there any reason to believe that this would be false ? Also if true can we generalize to other measurable spaces $X$ where all points can be separated by measurable linear functional ? I think that I have a hint of proof whenever $X$ is a convex open subset of a Banach space and $f$ is a bounded convex functional, I think this is along the lines of what @MaoWao suggests. I would like to generalize this proof to the case where $X$ is a closed and bounded subset of a Banach space. We prove that in this case $f$ is actually lower semi continuous, by way of contradiction. Suppose that there is $x\in X$ and $x_n\to x$ such that $\liminf_n f(x_n) =f(x)-\delta$ with $\delta>0$ (which we want to contradict). Since $X$ is an open set, there is $\varepsilon>0$ such that $B_{2\varepsilon}(x)\subseteq X$ , for any $n$ , define $y_n=x-\varepsilon\frac{x_n-x}{\| x_n-x \|}\in B_{2\varepsilon}$ , in particular $y_n\in X$ . Observe that for any $n$ , $x= (1-\alpha_n)x_n + \alpha_n y_n$ with $\alpha_n=\frac{\| x_n-x\|}{\|x_n-x\|+\varepsilon}$ and therefore by convexity, \begin{align*}
\Leftrightarrow&&(\| x_n-x\|+\varepsilon)f(x) &\leq \varepsilon f(x_n)+\| x_n-x\| f(y_n)\\
\Leftrightarrow&&\|x_n-x\| f(x) +\varepsilon (f(x)-f(x_n))\leq \|x_n-x\| f(y_n)\\
\Rightarrow && 0<\varepsilon \delta
\leq \liminf_n\|x_n-x\| f(y_n)\\
\end{align*} But if $f$ is bounded then the RHS is $0$ which is a contradiction. Now since $f$ is l.s.c. then Jensen inequality applies and we are done. I am much more interested in the case where $X$ is a closed, convex and bounded subset of Banach space, in this case it feels like a similar argument could be made by working in the largest relatively open subset of $X$ containing $x$ , i.e. the largest set containing $x$ in it's relative interior, but there are many point I do not master here, any reference on that would be welcome, the only one I know is Rockafelar for finite dimension.","['measure-theory', 'jensen-inequality', 'general-topology', 'convex-analysis', 'probability-theory']"
4541601,Iterating over numbers with many divisors,"I would like to iterate over the numbers with more than $D$ divisors in a large range $[x, x+N]$ . Current values I'm working with are $D=626$ and $x\approx N\approx10^{11}.$ At the moment I'm using a factorization sieve (PARI/GP's forfactored ) to iterate through the whole interval and skipping anything with at most $D$ divisors. (The factorization sieve computes the number of divisors 'for free'.) If $D$ were really big, I might be able to restrict to multiples of 2, or 6, or 12. But even odd numbers this size can have over a thousand divisors, so that's not workable. But it seems like much better should be possible, since numbers with this many divisors are quite rare. There should be some solution which loops over exponents for the first few (100? 10000?) primes, but I'm not sure what stopping criteria to use. Thoughts?","['number-theory', 'divisor-counting-function', 'sieve-theory', 'algorithms']"
4541629,Why do we only focus on finite-dimensional vector space?,"I have self-learned (theoretical) linear algebra and read various books about this topic, such as Linear Algebra Done Right (Sheldon Axler) and Linear Algebra (Stephen Friedberg et al.) . Also, I think I am familiar with set theory and the axiom of choice since I have completed chapters 3 and 8 of Terence Tao’s book Analysis I . My question: Why these books focus on finite-dimensional vector space
but not arbitrary ones? What is the main difference between
finite-dimensional vector spaces and infinite-dimensional ones? I will be grateful for any help you can provide.","['linear-algebra', 'vector-spaces']"
4541634,Do there exist continuous bijections from Euclidean space which change the topology?,"Do there exist continuous bijections from Euclidean space $X: \mathbf R^n\to M$ whose inverse is not continuous (where $M$ is a $n$ dimensional manifold)? I'm aware of continuous bijections from subsets of Euclidean space which change the topology, for example $\exp(2\pi i x):[0,1)\to \mathbf S^1$ . However without a boundary I can't imagine how space can be wrapped up to change the topology. Hence the question. The way one usually sets up a discontinuous inverse is to map faraway points nearby. To avoid messing up the manifold structure, these faraway points must be from infinity or the boundary. Euclidean space has no boundary so only the points at infinity can be used. But, there must be also be a point which bridges the points at infinity. This point can't exist because Euclidean space has no boundary. Hence the question.",['general-topology']
4541661,Functor of points of $X_{red}$,"Let $X$ be a scheme and let $X_{red}$ be its reduction (defined through its universal property). What is the functor of points of $X_{red}$ ? Put differently, which maps $\phi : \text{Spec }R \to X$ factor through the reduction of $X$ ? I am reading notes by Marc-Nieper Wißkirchen which claim that $\phi^{-1}X_{red} = \text{Spec }R/\sqrt{0}$ , but this can not be true, as witnessed by the morphism $\text{Spec }\mathbb C[x]/(x^2)\to \text{Spec }\mathbb C[x]$ . The reduction of the line $\mathbb A_\mathbb C$ is all of it, so the pullback must be all of the thickened point.","['algebraic-geometry', 'category-theory', 'commutative-algebra']"
4541689,Derivative of capital Pi product,"I wanted to find the derivative of this function at $x=6$ $$y= \prod_{i=1}^{10} (x-i) = (x-1)(x-2) \cdots (x-10) $$ without expanding all of the brackets, so I used the product rule to find a pattern. However, the resulting sum tells me that the derivative is zero at every whole number which is obviously not true. I've been over my solution and I can't see how I've gone wrong. Please could someone highlight where I went wrong? Thank you in advance. \begin{align*}
      \frac{\textit{d}y}{dx} &= (x-2)(x-3) \cdots (x-10) + (x-1) \frac{d}{dx} \biggl((x-2) \cdots (x-10) \biggr) \\
       &= \prod_{i=2}^{10} (x-i) +  (x-1) \frac{d}{dx} \biggl(\prod_{i=2}^{10} (x-i) \biggr) \\
       &= \prod_{i=2}^{10} (x-i) +  (x-1)\prod_{i=3}^{10} (x-i) + (x-1)(x-2)\frac{d}{dx} \biggl(\prod_{i=3}^{10} (x-i) \biggr) \\
                &= \prod_{i=2}^{10} (x-i) +  (x-1)\prod_{i=3}^{10} (x-i) + (x-1)(x-2)\biggl(\prod_{i=4}^{10} (x-i) \biggr) + \cdots \\
                      &= \frac{y}{x-1} + \frac{y}{x-2} + \frac{y}{x-3}+\cdots + \frac{y}{x-10} \\
                             &= \sum_{i=1}^{10} \biggl(\frac{y}{x-i}\biggr)
    \end{align*}","['calculus', 'derivatives', 'products']"
4541697,"On proving the following inequality: $ \int_X \text{min}(f,g) \ d\mu \geq \frac{1}{2} (\int_X \sqrt{fg} \ d\mu)^2. $","Let $f,g: X \rightarrow \mathbb{R}$ denote bounded, non-negative, measurable functions and $\mu$ a measure on $X$ . Then I would like to prove that $$
\int_X \text{min}(f,g) \ d\mu \geq \frac{1}{2} (\int_X \sqrt{fg} \ d\mu)^2.
$$ What I have tried so far is writing $\text{min}(f,g)= \frac{f+g}{2} - \frac{\vert f - g \vert}{2}$ . Then it is a well-known result that $\frac{f+g}{2} \geq \sqrt{fg}$ which seems promising (but might not be what we need though). I am not sure what to do about the second term. Also there needs to appear a square outside the integral which could be achieved with for instance Jensen's Inequality due to the convexity of $x \mapsto x^2$ . Then it would suffice to prove that $$
\int_X f+g - \vert f - g \vert \ d\mu \geq \int_X fg \ d\mu.
$$ Here I am stuck. Can anyone help me? Am I on the right track? As a final remark, it would be okay if we need to assume that $f,g$ integrate to 1 and $\mu(X)=1$ (as they will be densities for probability measures in the context I will be using this inequality). However I am not sure whether or not this is a necessary assumption and hence I have stated the problem without it initially.","['measure-theory', 'lebesgue-integral', 'inequality']"
4541769,Determinant of $A+A^T$ is an odd integer if $\text{det}(A-A^T)=1$.,"Let $A\in\text{Mat}(2n\times 2n;\mathbb{Z})$ be an integer matrix such that $\text{det}(A-A^T)=1$ . I want to show that $\text{det}(A+A^T)$ is an odd integer. Murasugi claims in his book ""Knot Theory and its Applications"" that this is trivial and it probably follows immeditiately from some determinant property of skew-symmetric resp. symmetric matrices. But I just can not proof it. Any help would be greatly appreciated!","['matrices', 'matrix-rank', 'linear-algebra']"
4541774,Almost Surely Convergence of a Series of Random Variables,"I am trying to solve this problem: Let $(X_n)_{n\in \mathbb{N}}$ be a sequence of independent random
variables on the probability space $(\Omega , \mathcal{F},
 \mathbb{P})$ and they all have uniform distribution on $[0,1]$ . Calculate $E(X_1 \cdots X_n)$ for each $n\in \mathbb{N}$ . Deduce that the series $\sum_{n=1}^\infty X_1\cdots X_n$ converges almost surely to a finite sum. These are my thoughts on the first question: Let $f(x)$ be the density function of uniform distribution on $[0,1]$ . Then $$E(X_1\cdots X_n) = \int_0^1 \cdots \int_0^1 x_1\cdots x_nf(x_1)\cdots f(x_n)dx_1\cdots dx_n.$$ Now, since $f(x)=1$ for $x\in [0,1]$ and by Fubini-Tonelli theorem we have $$E(X_1\cdots X_n) = \int_0^1 x_1dx_1 \cdots \int_0^1x_ndx_n = \left(\frac{1}{2}\right)^n.$$ For the second part, I defined $$Y_n := X_1\cdots X_n.$$ Since the expectation of $Y_n$ 's converges to $0$ , we say that their outcomes go to zero on average. Hence it makes sense if we can find some constant $c\in \mathbb{R}$ such that $$\mathbb{P}\left(\sum_{n=1}^\infty Y_n = c\right) = 1.$$ But I do not know how can I prove the existence of the constant $c$ mathematically. I appreciate any help.","['probability-distributions', 'convergence-divergence', 'probability-theory', 'probability', 'random-variables']"
4541833,About existence of martingale,"Question: Define $X_t=\exp (B_t-\frac{t}{2})$ to be a martingale, where $(B_t)$ is a brownian motion. Does $\lim_{t \rightarrow \infty} X_t$ exist? I am thinking around this result (long-term behavior of trajectories): Define $\{B_t\}_{t \in [0,\infty)}$ to be a brownian motion, then $\lim_{t \rightarrow \infty} \sup \frac{B_t}{\sqrt{t}}=\infty$ and $\lim_{t \rightarrow \infty} \inf \frac{B_t}{\sqrt{t}}=-\infty$ . To fit in this result, can I rewrite $B_t-\frac{t}{2}$ as $\sqrt{t}(\frac{B_t}{\sqrt{t}}-\frac{\sqrt{t}}{2})$ , then apply the result (long-term behavior of trajectories) to get infinity to get that $\lim_{t \rightarrow \infty} X_t$ exists and equal to 0? Another approach I am thinking about is writing $X_t=\exp (B_t-\frac{t}{2})$ as $X_t=\exp (t[\frac{B_t}{t}-\frac{1}{2}])$ , then apply strong law of large numbers for brownian motion. to get $X_t=\exp (-\infty)=0$ , so the limit exists and is equal to 0? I think the second approach works, but I am not sure if the first approach works. Appreciate any help.","['stochastic-processes', 'martingales', 'probability']"
4541860,What is the probability that you stop at only the first stop light?,"There are two traffic lights. Let $E$ be the event that you stop at the first light and $F$ be the event that you stop at the second light. Given: $P(E) = .6$ $P(F) = .4$ $P(E \text{ and } F) = .25$ What is the probability that you stop at only the first traffic light? My attempt: We want $P(E \text{ and } F^c)$ $P(E \text{ and } F^c) = P(E|F^c)P(F^c)$ The question then becomes what is $P(E|F^c)$ ? Conditioning on $F$ leads to: $P(E) = P(E|F) + P(E|F^c)$ Where $P(E|F) = \frac{P(E \text{ and } F)}{P(F)}$ $P(E) - P(E|F) = P(E|F^c)$ $P(E) - \frac{P(E \text{ and } F)}{P(F)} = P(E|F^c)$ But $P(E) = .6$ and $\frac{P(E \text{ and } F)}{P(F)}=.625$ so I get a negative number.... What did I do wrong, and what's the easiest way to solve this problem? Thanks",['probability']
4541962,Prove that the product $\prod_{i=1}^n\left(a_i+b_j\right)$ is also constant for all $j$.,"Let, $a_i, i=1,2, \ldots, n$ be distinct real numbers $b_1 b_2, \ldots, b_n$ be real numbers, such that the product $\prod_{j=1}^n\left(a_i+b_j\right)$ is the same for each $i$ . Prove that the product $\prod_{i=1}^n\left(a_i+b_j\right)$ is also constant for all $j$ . I am getting some issues here. It's given $a_i$ are distinct, so WLOG we can assume $a_1 <a_2<a_3< \dots <a_n$ and that would make $(a_1+b_1) < (a_2+b_1)$ and so on for all $a_i$ , but as we are given in the problem, $\prod_{j=1}^n\left(a_i+b_j\right)$ is same for all $i$ , we get $(a_1+b_1)(a_1+b_2) \dots (a_1+b_n)= (a_2+b_1)(a_2+b_2) \dots (a_2+b_n)$ . It's contradicting. Where am I going wrong?","['linear-algebra', 'products']"
4541978,How do I find the points of tangency given a 2 variable function and a normal vector?,"I'm given the two variable function $f(x,y) = 4x^2+7y^2+5xy+14$ and asked to find the (2) points on the surface where the vector $6\hat i + 69\hat j + 3\hat k$ is normal to the tangent plane.  So far I have written a generalized linearization using the points $f(a,b)$ : $$f(a,b) = 4a^2+7b^2+5ab+14$$ $$\Rightarrow z_0 = 4a^2+7b^2+5ab+14$$ $$ $$ $$\frac{\partial f}{\partial x} = 8x+5y$$ $$\Rightarrow x_0 = 8a+5b$$ $$ $$ $$\frac{\partial f}{\partial y} = 14y+5x$$ $$\Rightarrow y_0 = 14b+5a$$ For the following linearization: $$ $$ $$L(x,y) = 4a^2+7b^2+5ab+14+(8a+5b)(x-a)+(14b+5a)(y-b)$$ $$ $$ Rearranging this to be in the standard form of a plane: $$ $$ $$x(8a+5b)+y(14b+5a)-z = 4a^2+7b^2+5ab-14$$ $$ $$ If the goal is to find the two points where the given vector is normal to this plane I am at a loss. Any help from this point would be greatly appreciated. Edit: Fixed an error in the partial derivatives.","['linearization', 'vectors', 'tangent-spaces', 'multivariable-calculus', 'calculus']"
4541980,Is the minimal distance from a point inside a paralellopiped to its 8 vertices always less than the maximal length of all edges?,"Is the minimal distance from a point inside a paralellopiped to its 8 vertices is always less than the maximal length of all edges? Note that for an extreme case, the center of a unit cube has distance $\frac{\sqrt 3}{2}<1$ to its vertices. I can prove that if a point $X$ from the interior of a paralellopiped  lies in a ""corner"" tetrahedron say, in the following picture, $ABCF$ , then $BX$ is less than $\max(BA,BC,BF)$ , which is the same as the maximum of lengths all edges. But not every point lies in one of these eight ""corner"" tetrahedra, for example the center.","['euclidean-geometry', 'geometry']"
4541992,Continuity of a function defined by the Lebesgue measure of a closed ball,"My task is to prove that the function given by $f(x) = m(B_{x}(\alpha))$ is a continuous function, where $m(\cdot)$ denotes the Lebesgue measure and $B_{x}(\alpha)$ is a closed ball of radius $x$ centered at $\alpha$ , i.e. $$ B_x(\alpha) = \{ y \in \mathbb{R}^d \colon \Vert y-\alpha \Vert \leq x\} $$ The definition of continuity that I am using is as follows: A function $f(x)$ is continuous if for every $\varepsilon > 0$ there exists a $\delta > 0$ such that $\vert x - a \vert < \delta$ implies $\vert f(x) - f(a) \vert < \varepsilon$ . My main point of struggle is determining a $\delta$ that makes this all work nicely. Here is what I have currently: Proof: Let $ \vert x - a \vert < \delta - a$ . Without loss of generality, assume that $a < x$ , and so $x - a < \delta - a$ . Additionally, we may assume that $\alpha$ above equals $0$ without loss of generality. Consider any element $u \in \mathbb{R}^d$ such that $0 < \Vert u \Vert - a \leq x-a < \delta - a$ . This implies that $a < \Vert u \Vert \leq x < \delta$ . We may write the set of all possible $u$ as $$ B_x(0) \backslash B_a(0) = \{ u \in \mathbb{R}^d \colon a < \Vert u \Vert \leq x \} $$ By the properties of Lebesgue measure, we have $$m(B_x(0) \backslash B_a(0)) = m(B_x(0)) - m(B_a(0)) =  \vert m(B_x(0)) - m(B_a(0)) \vert$$ which is precisely $\vert f(x) - f(a) \vert$ . But $B_x(0) \backslash B_a(0) \subseteq B_\delta(0)$ and so $$m(B_x(0) \backslash B_a(0)) < m(B_\delta(0)) = \frac{\pi^{\frac{d}{2}}}{\Gamma(\frac{d}{2} + 1)} \delta^d$$ Hence we have $\vert f(x) - f(a) \vert < \frac{\pi^{\frac{d}{2}}}{\Gamma(\frac{d}{2} + 1)} \delta^d$ . If I have done everything else correctly, then this is where I am stuck. I am no expert when it comes to spheres of dimension $d$ but I think I have written the ""volume"" (or better yet, measure) of $B_\delta(0)$ correctly in the above semi-proof. Any direction in choosing $\delta$ to get the desired ""less than $\varepsilon$ "" condition would be greatly appreciated.","['continuity', 'measure-theory', 'solution-verification', 'lebesgue-measure']"
4542003,"Compute $\lim_\limits{n \to \infty} \int_{0}^{1} f_n\,\mathrm d\lambda$","Let $f_n =\big[\cos\left(1+\sin(x^5)\right)\big]^n$ and consider $L_1\big([0,1], \mathcal{L}, \lambda\big)$ ( $\mathcal{L}$ denotes the Lebesgue $-\,\sigma$ algebra and $\lambda$ the lebesgue measure) compute $$\lim_{n \to \infty} \int_0^1 f_n\,\mathrm d\lambda$$ First I note that $|f_n| \leq 1$ , $\forall n \in \mathbb{N}$ and since $1 \in L_1([0,1], \mathcal{L}, \lambda)$ then $f_n \in L_1([0,1], \mathcal{L}, \lambda)$ then I try to show that $$\lim_{n \to \infty} f_n=0$$ and next used the Lebesgue dominated convergence theorem, but I don’t sure to show that $\lim_\limits{n \to \infty} f_n=0\,.$ Any hint or help I will be very grateful.","['measure-theory', 'lp-spaces', 'lebesgue-integral']"
4542166,Find error in this algebraic transformation on a function,"Let $f(x)=x^2+x\sqrt{x^2-1}$ . Apparently the statement $\forall x$ for which $x,-x\in\mathbb D$ , $f(x)=f(-x)$ . is false, but I can “prove” it by \begin{align}x^2+x\sqrt{x^2-1}=x\left(x+\sqrt{x^2-1}\right)=\frac x{x-\sqrt{x^2-1}}=\frac1{1-\sqrt{1-\frac1{x^2}}}.\end{align} I still don’t see any problems in it.",['functions']
4542178,A graph such that each vertex has 36 neighbors,"Let $G$ be a simple graph such that each vertex in $G$ has exactly $36$ neighbors. Further for vertices $X,Y$ , if $X$ is neighbor to $Y$ , there are exactly $18$ other vertices that are neighbors to both $X$ and $Y$ ; and if $X$ is not neighbor to $Y$ , there are exactly $4$ other vertices that are neighbors to both $X$ and $Y$ . Determine the number of vertices in $G$ . So far, I obtain the relation $n=18m$ , where $m$ is the number of edges in $G$ , by counting triangles in $G$ and could not move forward. Any hint is appreciated.","['graph-theory', 'combinatorics', 'contest-math']"
4542201,"If $f$ is bounded and has bounded derivative on $\mathbb{R}$, then $xf(x)\in L^1\Rightarrow xf(x)\in L^\infty$?","Let $f$ be a differentiable and bounded function on $\mathbb{R}$ which has bounded derivative on $\mathbb{R}$ . Assume that $\int_{\mathbb{R}}|xf(x)|dx<\infty.$ Does this imply that $xf(x)$ is bounded on $\mathbb{R}$ ? I know that in general $g\in L^1$ does not imply $g\in L^\infty$ (e.g., the examples here ), but in this case our function is a product of $2$ ""good"" functions - the function $x$ and a bounded function $f$ which has bounded derivative on $\mathbb{R}$ , so probably in this case the implication $xf(x)\in L^1\Rightarrow xf(x)\in L^\infty$ is correct.","['lebesgue-integral', 'real-analysis']"
4542226,Convergence of a series that is only known numerically,"I hope this is not a duplicate, this is quite a general question but I couldn't find the answer in any of the other posts. Suppose one has a power series $\sum_{n=0}^{\infty}c_n x^n$ whose coefficients $c_n$ can only be numerically computed (for example, in my particular case, the $c_n$ are obtained by inverting a matrix). The general question here would be how to implement the usual convergence tests (ratio test and root test specifically) to extract whether the series has a finite or inifite convergence radius in $x$ . To be more specific on the problem of having access to only the numeric values of $c_n$ , I'll show my attempts with the root and ratio tests. I compute the first 150 $c_n$ coefficients and plot $\vert c_n\vert^{1/n}$ as a function of $n$ : To me this picture does not make it clear whether or not the sequence approaches a finite value. With a little effort, I compute more coefficients, this time I go to 450. This is the result Which looks as inconclusive as the previous picture, as I cannot tell if the sequence approaches a value at around $2\pi$ or grows to infinity. Interestingly, the ratio test looks like this: Where the sequence that I plotted is $\vert c_{n+1}/c_n\vert$ . It seems that the ""sub-sequence"" given by odd n+1 and even n goes to zero while that given by even n+1 and odd n goes to infinity. I am unsure if the presence of the seemingly divergent contribution is a reason strong enough to claim the power series has zero radius of convergence. In sum, I am having trouble extracting strong conclusions about the convergence radius of the power series from this kind of analysis, and I was wondering if there are methods that can be applied other than plotting a few terms and trying to guess the general behavior. Edit : from the comments I see there is in principle no way of determining the convergence radius by looking at a finite number of terms. This is already an answer, but I'll leave here the specific problem that motivated the question. Let us consider the matrix $S$ , whose coefficients $s_{n,k}$ are \begin{equation}
\begin{aligned}
s_{n,k}&=0,\quad k<n,\\
s_{n,k}&=\frac{-i(-1)^{k+n} \Gamma (k)}{2^k(i\pi)^{n}\Gamma^2(n) \Gamma(k-n+2)}\frac{f(n,k)}{g(n)},\quad k\geq n,
\end{aligned}
\end{equation} where \begin{equation}
\begin{aligned}
f(n,k)&=(2i\pi)^n \Gamma(k+1,-2 i \pi ,2 i \pi )\\&
+(2i\pi)^{k+1}[(-1)^{k+n} (\Gamma (n,-2 i \pi )-\Gamma (n))+\Gamma (n,2 i \pi )-\Gamma(n)],\\
g(n)&=2\pi(\tilde{\Gamma}(n,-2 i \pi )+\tilde{\Gamma}(n,2 i \pi)-2)+i n \tilde{\Gamma}(n+1,2 i \pi,-2i\pi).
\end{aligned}
\end{equation} In these expressions $i$ is the imaginary unit, $\Gamma(z,a,b)$ is the incomplete gamma function and $\tilde{\Gamma}(z,a,b)=\Gamma(z,a,b)/\Gamma(z)$ . The $S$ matrix is upper triangular, and all its diagonal elements are $1$ . Let us now consider the inverse matrix $T\equiv S^{-1}$ . Then the $c_n$ coefficients of the power series above are the elements of the first row of $T$ . In these conditions I have not been able to find a form for the entries $t_{n,k}$ of the $T$ matrix, and thus I've been forced to resort to generate $S$ numerically and invert it.","['power-series', 'numerical-calculus', 'numerical-methods', 'sequences-and-series']"
4542228,How to differentiate $y=\sqrt{\frac{1-x}{1+x}}$?,"It is an example question from ""Calculus Made Easy"" by Silvanus Thompson (page 68-69). He gets to the answer $$\frac{dy}{dx}=-\frac{1}{(1+x)\sqrt{1-x^2}}$$ The differentiation bit of the question is straightforward, but I'm having trouble simplifying it to get the exact answer. My working is the following: $$y=\sqrt{\frac{1-x}{1+x}}$$ This can be written as $$y=\frac{(1-x)^\frac{1}{2}}{(1+x)^\frac{1}{2}}$$ Using the quotient rule we get $$\frac{dy}{dx}=\frac{(1+x)^\frac12\frac{d(1-x)^\frac12}{dx}-(1-x)^\frac12\frac{d(1+x)^\frac12}{dx}}{1+x}$$ Hence $$\frac{dy}{dx}=-\frac{(1+x)^\frac12}{2(1+x)\sqrt{1-x}}-\frac{(1-x)^\frac12}{2(1+x)\sqrt{1+x}}$$ $$=-\frac{1}{2\sqrt{1+x}\sqrt{1-x}}-\frac{\sqrt{1-x}}{2\sqrt{(1+x)^3}}$$ What is the next step?","['calculus', 'algebra-precalculus']"
4542242,Why is $\int_0^x f(x) dx$ wrong?,"As soon as I entered higher education we stopped using functions of $x$ as the upper limit if we were integrating using a function application on $x$ like $f(x)$ . So we were told that an integral of the form: $$\int_0^x f(x) dx$$ Was considered incorrect, why is this technically incorrect? Is it due to the 'bound' occurrences of $x$ ? Or some other reason? Why do we tend to introduce a 'dummy' variable like the following: $$\int_0^x f(s) ds$$ In this case, is the bound variable's 'scope' limited to the integral, can I take a function's value at a 'free' occurrence $x$ and also use it in the integral, and have it keep it as the same variable?","['integration', 'calculus', 'functions', 'notation']"
4542322,"If $a_1,a_2,\dots,a_n$ are integers and $a_1a_2\cdots a_n=0$ then $a_i=0$ for some $1\le i \le n$","If $a_1,a_2,\dots,a_n$ are integers and $a_1a_2\cdots a_n=0$ then $a_i=0$ for some $1\le i \le n$ I'm trying to prove this by using induction We can prove this for $n=1$ means $a_1=0$ this implies true for $n=1$ Suppose this is true for $n=k$ ie $a_1,a_2,\dots,a_n$ are integers and $a_1a_2\cdots a_n=0 $ then $a_k=0$ for some $1\le k \le n$ we have to prove for $n=k+1, 1\le k+1 \le n$ I am not getting the idea from here any idea please thank you","['induction', 'discrete-mathematics']"
4542328,Trignometry problem: If $\sin^2\theta + 3\cos\theta = 2$ then find $\cos^3\theta + \frac{1}{\cos^3\theta}$,If $\sin^2\theta + 3\cos\theta = 2$ then find $\cos^3\theta + \frac{1}{\cos^3\theta}$ What I did: $\sin^2\theta + 3\cos\theta = 2$ $3\cos\theta - 1 = 1 - \sin^2\theta$ $3\cos\theta - 1 = \cos^2\theta$ $\cos^3\theta + \frac{1}{\cos^3\theta} = (\cos^2\theta + 1)^3 + \frac{1}{(\cos^2\theta + 1)^3}$ Then I'll have to take LCM and it would be bigger. Can anyone suggest an easy way to solve this problem?,['trigonometry']
4542339,Intuitively explain why $U: X \rightarrow X$ is invertible if it is close enough to the identity operator.,"Let $X$ be a Banach space. The Neumann theorem states that an operator $U: X \rightarrow X$ is invertible if it is close enough to the identity operator. This is the theorem. If $U: X \rightarrow X$ is bounded and $\|I-U\|<1$ , then $U$ is invertible, and $$
U^{-1}=\sum_{k=0}^{\infty}(I-U)^k
$$ Furthermore, $$
\left\|U^{-1}\right\| \leq \frac{1}{1-\|I-U\|}
$$ I'm not interested in the proof of this theorem, but why $I$ is so important for it? I mean, can I (for example) substitute the operator $I$ with another one and then obtain a similar theorem?","['banach-spaces', 'operator-theory', 'functional-analysis']"
4542340,Does the sample mean of the fatter tailed distribution $\to +\infty$ faster if both diverge?,"Let $X = (X_1, X_2, \dots)$ and $Y = (Y_1, Y_2, \dots)$ be independent sequences of non-negative random variables such that $$
X_i \overset{\text{i.i.d.}}{\sim} F \quad \text{and} \quad Y_i \overset{\text{i.i.d.}}{\sim} G,
$$ where $F$ and $G$ are cumulative distribution functions. Also, let $E[X_1] = E[Y_1] = \infty$ and $$
F(x) < G(x) \quad \text{for large $x$,}
$$ i.e. the $X_i$ have a fatter tailed distribution that the $Y_i$ . Does it then hold that $$
\liminf_{n \to \infty} \mathbf 1 \big\{ \overline X_n \geq \overline Y_n \big\} = 1 \quad \text{almost surely,}
$$ where $\overline X_n = \frac 1n \sum_{i=1}^n X_i$ and $\overline Y_n = \frac 1n \sum_{i=1}^n Y_i\;$ ? If not, does at least $$
P(\overline X_n \geq \overline Y_n) \to 1
$$ hold? Thoughts It's well known that both sample means diverge almost surely to $+\infty$ . But, intuitively, a fatter tail ought to make $\overline X_n$ diverge faster as $n \to \infty$ . But I'm not sure how to tackle the problem. I've tried using convolutions, but haven't gotten anywhere with that. I did a quick search for inequalities with Riemann-Stietjes integrals, since $E X_1 = \int_{\mathbb R} x \, dF(x)$ , but didn't find anything.","['statistics', 'probability-theory']"
4542380,"How to ""explain in words"" these $3$ conditions at infinity and zero?","Let $f:\mathbb{R}^N\setminus \{0\}\to\mathbb R$ and $g:\mathbb{R}^N\to\mathbb R$ be functions satisfying $$a)\qquad f(x)\le \frac{1}{|x|} \text{ when } |x|\sim 0,$$ $$b)\qquad \nabla f(x)\to 0\quad\text{ as } |x|\to +\infty,$$ $$c)\quad|g(x)|+\sum_{i=1}^N \left\vert\frac{\partial}{\partial x_i} g(x)\right\vert\to 1\quad\text{ as } |x|\to +\infty. $$ I am trying to ""traduce in words"" these three conditions. As on the one hand $a)$ can be said as $f$ grows less that $1/|x|$ for $x$ small and $b)$ as the gradient of $f$ is infinitesimal at infinity respectively, I have no idea how to explain condition $c)$ . Could someone please help me with that? Also, there is a better way to explain conditions $a)$ and $b)$ , too? Thank you in advance.","['real-analysis', 'notation', 'calculus', 'functions', 'limits']"
4542383,Flaw in proof of Jacobi's formula on Wikipedia?,"The Wikipedia article on Jacobi's formula (which gives the differential of the determinant function) contains two proofs, the second of which begins with a lemma claiming $\det'(I)=\operatorname{tr}$ , where $\det'$ is the differential of $\operatorname{det}$ . The article does not explicitly say "" total differential ,"" but I think it is reasonable to assume that this is intended, especially because it is true and one version of Jacobi's formula (the cleanest, in my opinion) gives the total differential of $\det$ as $\det'(T)(H)=\operatorname{tr}[\operatorname{adj}(T)H]$ . But the proof proceeds to show only that the directional derivative (or "" Gateaux derivative "") of $\det$ is as claimed. Even though the resulting operator $\operatorname{tr}$ is linear and continuous, this is not sufficient to guarantee that the total derivative exists, as I learned here . Am I correct that Wikipedia's proof is flawed, and if so, how can it be fixed? What I'd ideally like is a general theorem saying that if the directional derivative (in all directions) is given by a continuous linear operator $A$ , and some condition $X$ is satisfied, then the total derivative exists and is equal to $A$ (and then a proof that $X$ is satisfied in the given situation). Edit: I should add that I'm hoping to avoid something that explicitly references coordinates/components. For me, the appeal of the cited proof in the above article is its coordinate-free style. As PhoemueX points out in the comments, we could argue that, because $\det$ in coordinates is a polynomial, it has continuous partial derivatives, and hence its total derivative exists. But I'd prefer something more abstract that doesn't rely on coordinates, i.e., in the style of the original article if possible.","['gateaux-derivative', 'real-analysis', 'frechet-derivative', 'solution-verification', 'derivatives']"
4542508,Bounds or a nice form of the the partial sums of $\sum_{n=1}^\infty \frac{1}{n^2}$,"I do know that $\sum_{n=1}^\infty \frac{1}{n^2} = \frac{\pi^2}{6}$ , but so far I have not been able to found is either nice closed form for the sums $\sum_{n=K}^M \frac{1}{n^2}$ where $M > K \geq 1$ , or alternatively some tight bounds, maybe in terms of $M$ . By nice I mean that sure, you could expand each of the terms by hand, but then you would end up with a quite messy formula having a sum of products in the numerator and a quite clean product in the denominator. So I am asking for a reference/sketch for a nice exact sum (if such even exists) or alternatively some tight upper and lower bound, preferably with some explanation. Thanks! Edit: I feel dumb for not including this initially, but I was reading an algebra proof which used the bound $\sum_{i=N+1}^{M}\frac{1}{i^2} < \frac{1}{N^2}$ I got interested in knowing more about the bounds of the series in general. This means, unfortunately, that my specific question is a bit loose. Therefore my title could be: "" What cool bounds/asymptotic connections do you know for the partial sums of the series $\sum_{n=1}^\infty \frac{1}{n^2}$ ?","['summation', 'sequences-and-series', 'discrete-mathematics', 'real-analysis']"
4542532,Find the term that will have the larger coefficient,"Which of the expressions $$(1+x^2-x^3)^{100} \textrm{or}\:\: (1-x^2+x^3)^{100}$$ has the larger coefficient of $x^{20}$ after expending abd and collecting terms. I can easily do this question via multinomial expansion, but here's the twist. In the question it was written to use the hint given to solve the problem. The hint was replace $-x$ with $x$ to gain extra information. I replaced, searched for symmetric properties, odd-even properties but couldn't found any. I'm looking for a solution incorporating the above given hint. Also, if there is any method as easy as ABC to do this, I will also appreciate that.","['number-theory', 'functions', 'polynomials', 'elementary-number-theory']"
4542602,"For all $x \in \Bbb R$ there exists a $y \in (-\infty, 1)$ such that $3yx^2+y-x=0$","I have this question: Prove that for all real numbers $x$ there exist a number $y$ in the interval $(-\infty, 1)$ such that $$3x^2y+y-x=0$$ I proved that the range of the function $$y = \frac{x}{3x^2 + 1}$$ is $(-1/2\sqrt{3},1/2\sqrt{3})$ .
But the teacher said I should use another way.
I didn't find any.","['quantifiers', 'functions', 'logic']"
4542637,"If population keep trying till they have girl child, what will be the probability of population having more girls than boys and vice versa?","I was solving this problem: In a world where everyone wants a girl child, each family continues having babies till they have a girl. What do you think will the boy to girl ratio be eventually? (Assuming probability of having a boy or a girl is the same) The solution given was: Suppose there are N couples. First time, N/2 girls and N/2 boys are born (ignoring aberrations). N/2 couples retire, and rest half try another child. Next time, N/4 couples give birth to N/4 girls and rest N/4 boys. Thus, even in second iteration, ratio is 1:1. It can now be seen that this ratio always remain same, no matter how many times people try to give birth to a favored gender. My doubt is that will following be the case: P(population will have more girls)= P(population will have equal number of boys and girls)= 1/2 Consider there are 16 couples 8 give birth to girls and hence stop. 8 give birth to boys, so they give another chance. 4 give birth to girls and hence stop. 4 give birth to boys, so they give another chance. 2 give birth to girls and hence stop. 2 give birth to boys, so they give another chance. 1 give birth to a girl and hence stop. 1 give birth to a boy, so they give another chance. Note till now there Number of boys = Number of girls Now probability that a single remaining couple give birth to a girl is 1/2 . In that case they will stop and there will be one more girl than boys in the population. Probability that a single remaining couple give birth to a boy is 1/2 , in which case they will give another chance in which they will again have a 1/2 probability of giving birth to boys, thus again balancing girl-boy ratio . So am I correct with two facts: Fact 1 : P(population will have more girls than girls)= P(population will have equal number of boys and girls)= 1/2 Fact 2 : P(population will have more boys than girls) = 0","['geometric-distribution', 'probability']"
4542673,Find $\lim \limits_{x\to 0} \frac{\sin({\pi \sqrt{\cos x})}}{x} $,"So I am having trouble finding this limit: $$\lim \limits_{x\to 0} \frac{\sin({\pi \sqrt{\cos x})}}{x}$$ The problem is I can't use the derivative of the composition of two functions nor can I use other techniques like l'Hôpital's theorem.
I tried numerous techniques to calculate this limit but in vain so if you have any simple idea that is in the scope of my knowledge ( I am a pre-calculus student ), please do let me know without actually answering the question.","['limits', 'algebra-precalculus', 'limits-without-lhopital', 'trigonometry']"
4542691,"If $G$ has both chain conditions, then there is no proper $H \leq G$ with $G \cong H$, and no normal nontrivial $K \leq G$ with $G/K \cong G$.","I have been self-studying group theory, and thinking for a while without much progress on this exercise (Rotman “An Introduction to the Theory of Groups”, Exercise 6.31): If $G$ has both chain conditions (that is, every ascending or descending chain of normal subgroups of $G$ is eventually constant), then there is no proper $H \leq G$ with $G \cong H$ , and no normal nontrivial $K \leq G$ with $G/K \cong G$ . My thoughts so far: if $H$ were characteristic in $G$ then we could easily construct an infinite descending chain of normal subgroups of $G$ by repeatedly applying the isomorphism $G \to H$ , contradicting DCC. Trying to aim for a contradiction with the Krull-Schmidt theorem (since this exercise is after a section leading up to the Krull-Schmidt theorem), I get that $H$ and $G$ contain the same number of indecomposable factors, so if we could write $G$ as a direct product including those factors of $H$ (and others), then that would be a contradiction, but that would only be possible in the case that $H$ is itself a direct factor of G. I have also tried constructing various endomorphisms on the indecomposable factors of $G$ and $H$ (by composing with projections), to use the lemma that those endomorphisms must either be surjective or nilpotent.","['group-theory', 'abstract-algebra']"
