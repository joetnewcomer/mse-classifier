question_id,title,body,tags
4576580,Function defined differently on rationals and irrationals with derivative given by classic rules,"Today, one of my students was given the function $$f(x) = \begin{cases} x^3+1 \quad \text{ if }x\in \Bbb Q\\ x^3+x \quad \text{ if }x\in \Bbb R \backslash \Bbb Q \end{cases} $$ He had some questions on the derivative and he wrote $$ f'(x)=\begin{cases} 3x^2 \quad \text{ if }x\in \Bbb Q\\ 3x^2+1 \quad \text{ if }x\in \Bbb R \backslash \Bbb Q \end{cases}$$ Which is not true of course, but I was wondering if there exists some function where it could be the case i.e. a function defined differently on rationals and irrationals with derivative given by the classic rules of derivation like my student wanted to do. The main problem I see is that the function will not be continuous at almost every point so not differentiable almost everywhere, right ? Or can someone maybe find such a function ?","['derivatives', 'examples-counterexamples', 'real-analysis']"
4576593,Solving $\cos (\alpha-\beta)+\cos (\beta-\gamma)+\cos (\gamma-\alpha)$,"If $\cos \left( {\alpha  - \beta } \right) + \cos \left( {\beta  - \gamma } \right) + \cos \left( {\gamma  - \alpha } \right) =  - \frac{3}{2}$ , where $(Î±,Î²,Î³ âˆˆ R).$ (A) $\sum {\cos \alpha }  = 0$ (B) $\sum {\sin \alpha }  = 0$ (C) $\sin \alpha \sin \beta \sin \gamma = 0$ (D) $\sum {\cos \alpha }  +\sum {\sin \alpha }  = 0$ This is a multiple choice question with one or more options My approach is bases on Complex number $\cos \left( {\alpha  - \beta } \right) + \cos \left( {\beta  - \gamma } \right) + \cos \left( {\gamma  - \alpha } \right) =  - \frac{3}{2}$ $T = {e^{i\left( {\alpha  - \beta } \right)}} + {e^{i\left( {\beta  - \gamma } \right)}} + {e^{i\left( {\gamma  - \alpha } \right)}}$ ${\mathop{\rm Re}\nolimits} \left( T \right) =  - \frac{3}{2}$ ${e^{i\frac{\alpha }{\beta }}} + {e^{i\frac{\beta }{\gamma }}} + {e^{i\frac{\gamma }{\alpha }}} \Rightarrow {e^{i\frac{\alpha }{\beta }}}\left( {1 + \frac{{{e^{i\frac{\beta }{\gamma }}}}}{{{e^{i\frac{\alpha }{\beta }}}}}} \right) + {e^{i\frac{\gamma }{\alpha }}} \Rightarrow {e^{i\frac{\alpha }{\beta }}}\left( {1 + {e^{i\left( {\frac{\beta }{\gamma } = \frac{\alpha }{\beta }} \right)}}} \right) + {e^{i\frac{\gamma }{\alpha }}}$ How do I proceed from here","['trigonometry', 'complex-numbers']"
4576682,Proving boundedness of operator,"Let $E$ be a finite dimensional normed space with norm $||\cdot||$ . Show that $T : (E,||\cdot||) \rightarrow (\mathbb{K}^n,||\cdot||_\infty),$ given by $x = \sum_{I=1}^nx_ie_i \mapsto (x_1,\dots,x_n)$ is continuous. My attempt: Let $B = \{e_1,\dots,e_n\}$ be a base for $E$ . Then for any $x = \sum_{i=1}^nx_ie_i$ where $(x_i)_{I=1}^n \in \mathbb{K}^n$ we have the following: \begin{align*}||Tx||_\infty & = \max_{1 \leq i \leq n}|x_i|\\
& \leq \sum_{I=1}^n|x_i|\frac{||e_i||}{||e_i||}\\
& = \frac{1}{\sum_{I=1}^n||e_i||}\sum_{i=1}^n||x_ie_i||.\end{align*} But now I am stuck because I can't go from $\sum_{i=1}^n||x_ie_i||$ to $||\sum_{I=1}^nx_ie_i|| = ||x||$ . Ps .: I am not able to use the famous auxiliary lemma: that gives the following estimate $$c|\sum_{I=1}x_i| \leq ||x_1e_1 + \dots x_ne_n|| = ||x||.$$ Thank you.","['solution-verification', 'functional-analysis']"
4576690,Find for what values of a and b in R the limit exists (No De L'Hopital),"I was given this exercise in my math course at university.
The question is to find, without using De l'Hopitals and other methods which may use derivates and similars, for what values of $a$ and $b$ in $\mathbb{R}$ the following statement is true. $$
f(x)= \left\{\begin{matrix}  \dfrac{\sin x}{x} & x > 0  \\  ax + b & x \leq 0  \\ \end{matrix}\right. $$ $$ \lim_{x \to 0^+} \dfrac{f(x) - ax - b}{x} = 0$$ I started by writing it as: $$ \lim_{x \to 0^+} \dfrac{\dfrac{\sin x}{x} -ax - b}{x} = 0$$ because, due to the fact that we are approaching from $x > 0, \, f(x) = \dfrac{\sin x}{x}$ Then I multiplied $\frac{\sin x}{x}$ by $\frac{\sin x}{\sin x}$ in order to get: $$ \lim_{x \to 0^+} \dfrac{\dfrac{\sin^2x}{x\sin x} -ax - b}{x} = 0$$ From there: $$ \lim_{x \to 0^+} \dfrac{\dfrac{\sin^2x}{x\sin x} -ax - b}{x} = 0$$ $$ \Longleftrightarrow \lim_{x \to 0^+} \dfrac{\dfrac{1-\cos^2x}{x\sin x}-ax-b}{x} = 0 $$ $$ \Longleftrightarrow \lim_{x \to 0^+} \dfrac{\dfrac{1-\cos x}{x} \times \dfrac{1+\cos x}{\sin x}-ax-b}{x} = 0$$ $$ \Longleftrightarrow \lim_{x \to 0^+} \dfrac{\dfrac{1-\cos x}{x} \times \dfrac{1+\cos x}{x} \times \dfrac{x}{\sin x} - ax - b}{x} = 0$$ $$ \Longleftrightarrow \lim_{x \to 0^+} \dfrac{\dfrac{1 - \cos x}{x} \times \dfrac{1}{x} \times \left(1 + \cos x\right) \times \dfrac{x}{\sin x} -ax - b}{x} = 0$$ $$ \Longleftrightarrow \lim_{x \to 0^+} \dfrac{\dfrac{1-\cos x}{x^2} \times \left(1+ \cos x\right) \times \dfrac{x}{\sin x} -ax - b}{x} = 0$$ Which, substituting the value of $x$ to the value it is approaching, results in: $$ \dfrac{\dfrac{1}{2}\times\left(1+1\right)\times 1 - a \times 0 - b}{x} = 0 $$ $$ \Longleftrightarrow \dfrac{1 - b}{x} = 0 $$ Which should resolve to the I.F. $$\dfrac{0}{0}$$ when $b=1$ So to me it seems like, with the calculations I've done, I can't reach an answer about which value of $a$ and $b$ make the statement true","['limits-without-lhopital', 'analysis', 'real-analysis', 'functions', 'limits']"
4576715,Determine for which $p>0$ the double integral converges,"I would appreciate help solving the double integral below. I have started the task but cannot complete it fully. Let $D=\{(x,y)\in \mathbb(R)^2:x^2+y^2 < 1\}$ . Determine for which $p>0$ the double integral converges: $$\iint_D\frac{1}{(x^4+2x^3y+3x^2y^2+2xy^3+y^4)^p}.$$ My attempt: My idea is to use polar coordinates, so I started by rewriting the expression in the denominator as follows $$(x^4+2x^3y+3x^2y^2+2xy^3+y^4)^p=(x^2+xy+y^2)^{2p}.$$ Then I can put in polar coordinates and get that $(r^2+r^2\cos\theta\sin\theta)^{2p}=(r^2)^{2p}(1+\cos\theta\sin\theta)^{2p}$ which makes it possible to split the integral into the following $$\int_0^{2\pi}\frac{1}{(1+\cos\theta\sin\theta)^{2p}}d\theta\int_0^1\frac{r}{(r^2)^{2p}}dr.$$ For the second integral
I carried out the variable substitution $u=r^2$ , $du=2r dr$ and got the value $\frac{1}{2-4p}$ . But it is now when I have to find the value of the first integral that I get stuck. I started by looking at trigonometric rewrites for $1+\cos\theta\sin\theta$ but found nothing that would facilitate the calculations. I also looked at whether you could employ $u=\cos\theta\sin\theta$ , but then $du=\cos^2\theta-\sin^2\theta$ and that would not simplify the calculations either since I then need to divide with the expression. I would be happy to receive tips on how I could solve this task. Maybe there is something obvious that I have missed but I have been looking at this for a while now so any suggestions are warmly welcomed!","['multivariable-calculus', 'trigonometric-integrals']"
4576755,Find diagonal matrix $D$ so that $AD + B$ has specific eigenvalues.,"Problem I have the following matrix equation ( $A,B,D$ are all square, $D$ is diagonal): $$
C=AD + B
$$ Assuming $A$ and $B$ are completely known and the eigenvalues ( $\lambda_i$ ) of $C$ are completely known, how can I go about solving for the diagonal matrix $D$ (numerically or otherwise)? Out of curiosity, to make this more general, if we don't restrict $D$ to be diagonal, can we solve for its eigenvalues given the same information? Edit : To be clear, $C$ is not known, only its eigenvalues are known. I'm essentially trying to solve for $C$ by first solving for $D$ . What I've Tried Since I know the eigenvalues of $C$ , I can set up a system of equations in terms of the unknown diagonal elements $\gamma_i$ using the following relationship and the known eigenvalues $\lambda_i$ : $$
\det \left( A 
\begin{bmatrix} 
    \gamma_1 & \dots  & 0\\
    \vdots & \ddots & \vdots\\
    0 & \dots  & \gamma_n 
    \end{bmatrix}
+ B + \lambda_i I \right) = 0
$$ However, this system is non-linear and is cumbersome to set up and solve when the the matrices get large. Does anyone know of a better way?","['matrices', 'numerical-linear-algebra', 'linear-algebra', 'eigenvalues-eigenvectors']"
4576791,"Show $a^2+b^2+c^2 \equiv 0 \pmod {3}$ if $a, b, c$ are *not* multiples of $3$. [duplicate]","This question already has answers here : Prove or disprove: For every integer a, if a is not congruent to 0 (mod 3), the a^2 is congruent to 1 (mod 3) (4 answers) Closed 1 year ago . I was given the following problem: ðŸ’¡ Show $a^2+b^2+c^2 \equiv 0 \pmod {3}$ if $a, b, c$ are not multiples of $3$ . I would like a. verification of my proof (I self-study); b. alternative proofs, to enrich my appreciation of the problem. Here's what I did. $\text{Lemma (demonstration skipped)} :$ $\forall j \in \mathbb{N}| a \equiv b \pmod{m} \implies a^j \equiv b^j \pmod{m}$ . $\text{Consideration :}$ I will use the $\binom{a}{b}$ notation to refer to the number of multisets of size $b$ that can be drawn out of $a$ elements; i.e. , the number of collections with repetition but without order . This is due to the lack of a proper ""\multiset"" format in LaTex as provided in the site. $\text{Solution}$ . It is trivial to say $$\begin{align}
a &\equiv r_1 \pmod{3}\\
b &\equiv r_2\pmod{3}\\
c &\equiv r_3\pmod{3}
\end{align}$$ where $r_1, r_2, r_3$ are the remainders of $a, b, c$ respectively in the division by $3$ . $I$ . Let $S$ be the set of possible values of $r_i$ . Because $3\nmid a, \space 3\nmid b, \space 3\nmid c$ we have $S=\{1, 2\}$ . $II.$ There are $2^3=8$ ways to draw $3$ elements of $S$ . Since addition is conmutative, their ordering is irrelevant and therefore we have $\binom{2}{3}=\frac{4!}{3!}=4$ possible sums of the form $r_1+r_2+r_3$ , with the same principle applying to $r_1^2+r_2^2+r_3^2$ . $III$ . Let $S'$ be the set of the possible results of the sum $r_1^2+r_2^2+r_3^2$ . These possible sums, as is easy to manually compute, are $S'= \{3, 6, 9, 12\}$ , where $\forall s \in S' | s=3m, m\in\mathbb{Z}.$ It then follows $$a^2+b^2+c^2 \equiv r_1^2+r_2^2+r_3^2 \equiv0 \pmod{3}$$ Thanks in advance.","['solution-verification', 'modular-arithmetic', 'discrete-mathematics']"
4576818,"Efficient way to find all polygons of the same shape within a set, regardless of position, scale, or rotation","I've got a big set of 2D polygons described as a set of points. I would like to take this set of polygons and find any that are the same shape, regardless of rotation, translation, or scale. Each polygon has 3 or more points, and in some cases may have hundreds or thousands of points. There are hundreds of polygons in total. The polygons are optimised to remove runs of points that would form a straight line, e.g. the sequence $[(0,0), (10,10), (20,20)]$ would not be allowed. The polygons also aren't allowed to intersect themselves. The context for this problem is geometry instancing in 3D rendering, specifically rendering polygon regions and fills from PCB designs. Polygon regions are frequently repeated or copy-pasted around boards, so I would like to reduce the number of draw calls I need per frame by finding any polygon that I can instance using a transformation matrix. My first idea was to normalise the scale of all polygons, then compare the line lengths and angles between line vectors. Two polygons with the same shape should be composed from the same line lengths and same angles in the same order. I started implementing this, using the following procedure: Group all polygons by point count. Discard any groups that contain only one polygon, since they are guaranteed to be unique in shape. Center each polygon to $(0,0)$ and normalise its scale to a 1x1 bounding box. Calculate a ""hash"" for each polygon based on the sum of the magnitudes of all points, i.e. $\sum \sqrt{{x_i}^2 + {y_i}^2}$ , which for any two polygons sharing the same shape should be equal. Further group the polygons by this hash, and discard any groups that have only one member. This acts as a fast filtering step to quickly reject any polygons which are obviously not the same. Turn each polygon's point list into a circular list of angle-distance pairs, $(\theta,d)$ , where $\theta$ is the angle formed between the vectors meeting at the point, and $d$ is the length of the next clockwise vector. Within each group, compare all of the lists to each other to find matches. This works in theory, but I ran into two issues. First, ""clockwise"" isn't well-defined for arbitrary polygons that may have both concave and convex regions. This can be solved by traversing the lists in both directions during the final step, which has the added bonus of finding mirror-image copies of the same polygon, which could be described as a negative scale in one or both of the axes, and which I can handle just fine in the instancing code. Second, and more importantly, I couldn't come up with an efficient way to compare the two circular lists of points without knowing the correct starting offsets. It occurs to me that there might be some efficient mathematical way of identifying whether two circular sequences of number pairs contain the same ordered sequence of values regardless of whether the sequences are offset from each other. For example, $[1,2,3,5,2]$ and $[3,5,2,1,2]$ should be considered equal because they are the same set of numbers in the same order, just with a different starting offset (the latter is the former shifted two places left). However, $[1,2,3,5,2]$ and $[2,5,3,2,1]$ should not be considered equal, because the ordering is incorrect despite the same numbers appearing in the set. In practice the elements in each list would actually be a pair of numbers rather than a single number, but that's an implementation detail. Is there a clever way to solve this? If not, is there an alternative approach to the polygon shape comparison problem?","['combinatorics', 'geometry', 'polygons', 'algorithms']"
4576857,Computing integration $\int_0^{\infty}\frac{\log(x)\sin(x)}{x} dx$,"I can't seem to find this problem on Math.SE, so sorry if it actually exists. I want to evaluate $I(a) = \int_0^{\infty} \frac{\log(ax)\sin x}{x} dx$ . Then I separated it into two integrals $I(a) = \int_0^{\infty} \frac{\log(a)\sin x}{x} + \int_0^{\infty} \frac{\log(x)\sin x}{x}$ . The first term is constant and evaluates to $\frac{\pi}{2}\log a$ , and the second term is where I am struggling with, it's equal to $I(1)$ . I also tried using differentiation under the integral sign, but it gives the same result, as I get $I'(a) = \frac{\pi}{2a}$ , and I can't find the constant. I also tried putting the variable inside $\sin(x)$ , so $J(a) = \int_0^{\infty} \frac{\log x \sin(ax)}{x}$ , but $I'(a) = \int_0^{\infty} \log x \cos(ax) dx$ is divergent. WolframAlpha gives something about $\pi\gamma$ , so I am wondering where that comes from. Thanks!","['integration', 'derivatives']"
4576865,"Linearization of scalar curvature: $DR|_g(h)=-\Delta_g(\mathrm{tr}_g h)+\mathrm{div}_g(\mathrm{div}_g h)-\langle\mathrm{Ric}_g,h\rangle_g$","I'm working on an exercise from Geometric Relativity by Dan A. Lee, but things didn't go well: Following Lee's hint, I was trying to use Exercise 1.12 and view $\color{red}{g}$ as the background metric ( $\bar g$ in Exercise 1.12). In Exercise 1.12, I've shown that the Ricci curvature of a Riemannian metric $g$ and the Ricci curvature of another Riemannian metric $\bar g$ are related by $$R_{ij}=\bar{R}_{ij}+(\overline{\nabla}_k W_{ij}^k-\overline{\nabla}_j W_{ki}^k)+(W_{k\ell}^k W_{ij}^\ell-W_{j\ell}^k W_{ik}^\ell).\tag{1}$$ Please see Relating the Ricci curvatures of two Riemannian metrics to know about (1). Now I have $$R_{g_t}=g_t^{ij}\bar{R}_{ij}+g_t^{ij}(\overline{\nabla}_k W_{ij}^k-\overline{\nabla}_j W_{ki}^k)+g_t^{ij}(W_{k\ell}^k W_{ij}^\ell-W_{j\ell}^k W_{ik}^\ell)\tag{2}$$ and $$\left.\frac{d}{dt}\right|_{t=0}R_{g_t}=\lim_{t\to 0}\frac{R_{g_t}-R_g}{t}=\lim_{t\to 0}\frac{R_{g_t}-g^{ij}\bar{R}_{ij}}{t}.\tag{3}$$ To evaluate the difference quotient in (3), I write $$R_{g_t}\simeq (g^{ij}+t\dot{g}^{ij})\bar{R}_{ij}+(g^{ij}+t\dot{g}^{ij})(\overline{\nabla}_k W_{ij}^k-\overline{\nabla}_j W_{ki}^k)+(g^{ij}+t\dot{g}^{ij})(W_{k\ell}^k W_{ij}^\ell-W_{j\ell}^k W_{ik}^\ell)\tag{4}$$ for small $t$ . I'm not sure if the linear approximation of $g_t$ at $t=0$ is legitimate, but I can't imagine what it would be like if there were any other ways. Now I have $$\frac{R_{g_t}-g^{ij}\bar{R}_{ij}}{t}\simeq \dot{g}^{ij}\bar{R}_{ij}+(\frac{g^{ij}}{t}+\dot{g}^{ij})(\overline{\nabla}_k W_{ij}^k-\overline{\nabla}_j W_{ki}^k)+(\frac{g^{ij}}{t}+\dot{g}^{ij})(W_{k\ell}^k W_{ij}^\ell-W_{j\ell}^k W_{ik}^\ell)\tag{5}$$ for small $t\neq 0$ . If the above is true, all we have to do now is take its limit at $t=0$ . But after seeing that $$\lim_{t\to 0}\dot{g}^{ij}\bar{R}_{ij}=\dot{g}^{ij}\bar{R}_{ij}=\langle\mathrm{Ric}_g,\dot{g}\rangle_g,\tag{6}$$ I began to hesitate about the computation because I didn't have a minus sign to compensate (6) for the formula to be derived in Exercise 1.18. Why's that? What should I do next? Thank you. Remark 1. I'm not worried about $\dot{g}^{ij}(\overline{\nabla}_k W_{ij}^k-\overline{\nabla}_j W_{ki}^k)$ and $\dot{g}^{ij}(W_{k\ell}^k W_{ij}^\ell-W_{j\ell}^k W_{ik}^\ell)$ in (5) because these terms contribute nothing as indicated by the hint. Remark 2. The hint says that in any orthonormal frame $\{v_i\}_i$ , the double divergence of $\dot{g}$ is given by $$\mathrm{div}_g\mathrm{div}_g\dot{g}=\sum_{i,j}\dot{g}_{ij;ij},$$ which I have just checked. And in the same frame, I also checked that $$\begin{align}
\Delta_g\mathrm{tr}_g\dot{g}&=\mathrm{div}_g\mathrm{grad}(g^{ij}\dot{g}_{ij})\\
&=\sum_k\left(v_k(v_k(\sum_i \dot{g}_{ii}))+\sum_j v_j(\sum_i\dot{g}_{ii})\overline{\Gamma}_{kj}^k\right)
\end{align}$$ and that $$\langle\mathrm{Ric}_g,\dot{g}\rangle_g=\sum_{i,j}\bar{R}_{ij}\dot{g}_{ij}.$$ These local expressions follow from a straightforward computation. The crux of my attempt really lies in the expansion of those $W$ terms. Originally, I planned on taking it by employing $$W_{ij}^k=\frac{1}{2}g_t^{k\ell}\left(\overline{\nabla}_i(g_t)_{\ell j}+\overline{\nabla}_j(g_t)_{i\ell}-\overline{\nabla}_\ell(g_t)_{ij}\right),$$ another result obtained previously (see The components $W_{ij}^k$ of the difference $W=\nabla-\overline{\nabla}$ between two Levi-Civita connections ), but the algebra really hurts.","['curvature', 'linearization', 'riemannian-geometry', 'differential-geometry']"
4576898,A Nonrectifiable Curve,"Do Carmo's book Page 11. Let $\alpha : [0,1] \to \mathbb{R}^2$ be given as $$\alpha(t)=
\begin{cases} 
(t,t \sin (\pi/t)) & \text{ for } t \ne 0\\
(0,0) & \text{ for } t=0 \,.
\end{cases}$$ Show that the arc length of the portion of the curve corresponding to $\dfrac{1}{n+1} \leq t \leq \dfrac{1}{n}$ is at least $2/(n+1/2)$ . Use this to show that the length of the curve in the interval $1/N \leq t \leq 1$ is greater than $2 \sum_{n=1}^N 1/ (n+1)$ . For the first part, I considered the distance between $\bigl(\alpha(1/n), \alpha(2/(2n+1))\bigr)$ and $\bigl(\alpha(2/(2n+1)), \alpha(1/(n+1))\bigr)$ and proved that it should be at least $2/(n+1/2)$ . In the second part, we can see that \begin{align}
[1/N, 1] & = [1/N, 1/(N-1)]\cup [1/(N-1), 1/(N-2)]\cup \dots \cup [1/2,1] \,.
\end{align} Therefore, \begin{align}
|\alpha(1)-\alpha(1/2)|+ \dots + |\alpha(1/(N-1))-\alpha(1/N)| & \geq \sum_{n=1}^{N-1}\dfrac{2}{n+1/2} \,,
\end{align} and that won't give the desired solution. I don't know where I have made a mistake.","['differential-geometry', 'sequences-and-series']"
4576923,"Grafakos $5.2.1$: Show that directional Hilbert Transform maps $L^1(\mathbb{R}^n)$ to $L^{1,\infty}(\mathbb{R}^n)$","Show that directional Hilbert Transform maps $L^1(\mathbb{R}^n)$ to $L^{1,\infty}(\mathbb{R}^n)$ The definition of directional Hilbert Transform is: $$H_\theta(f)(x)= \frac{1}{\pi}p.v \int_{-\infty}^{\infty}f(x-t \theta) \frac{dt}{t} $$ To prove the statatement, first I need to show that Hilbert Transform is given by convolution with the distribution $\omega_\theta$ in $S'(\mathbb{R}^n)$ defined by $$\langle \omega_\theta, \phi\rangle =  \frac{1}{\pi}p.v \int_{-\infty}^{\infty} \frac{\phi(t\theta)}{t}dt $$ I would like to know how to prove the first step and then how to use it to prove the original statement.Any hint?","['harmonic-analysis', 'fourier-analysis', 'functional-analysis', 'real-analysis']"
4576945,Showing that a constant composition implies a constant input,"Regarding the problem: Consider the differentiable (continuously) functions $f$ and $g$ where $f: \mathbb{R}^k \rightarrow \mathbb{R}$ and $g: (a,d) \rightarrow \mathbb{R}^k$ solving the system of equations $$\frac{dg}{dx}=- \nabla f(g(x)) \space \text{ for } x \in (a,d)$$ Within the frameworks of the above setup, prove that for $[b,c] \subset (a,d)$ we have the following implication: $$f(g(b)) = f(g(c)) \implies \nabla f(g(b))=0 \space \text{ and } g(x) = g(b) \space \space \forall x \in [b,c]$$ I know that we can apply the chain rule here which gives us the following result $$ (f \circ g)'(x) =  \nabla f(g(x)) \space \cdot \space g'(x)$$ And from the system of equations presented in the question, this should allow us to make a direct substitution to find that: $$ (f \circ g)'(x) = - \Big{(} \frac{dg}{dx} \Big{)}^2 $$ This tells us that $f \circ g$ is decreasing. It is also clear the converse of the desired result holds (trivially). It feels intuitively clear, that if $f(g(b)) = f(g(c))$ for any open subset $[b,c]$ over the domain, then $g$ must be constant on this interval (with zero gradient), but I'm not clear on how to formalise this. Iâ€™m unsure if / how the progress I have made helps us with this particular problem, and would be grateful for any guidance.","['ordinary-differential-equations', 'real-analysis', 'multivariable-calculus', 'calculus', 'derivatives']"
4576950,An identity between iid R.V. and its ladder R.V. in Fluctuation theory/Random walk.,"Let $\{X_n\}_{n=0}^\infty \space$ be a sequence of i.i.d non-lattice R.V. with $X_0=0,\space \space  0\lt E[X_1]\lt\infty $ Let a partial sum $S_n = X_1 + X_2 + ... +X_n. \space(S_0 = 0)$ and first positive partial sum, $S_T$ where $T = min\{n;S_n\gt0\}$ and continuing $T_k = min\{n;n\gt T_{k-1}, S_n\gt S_{T_{k-1}}\} $ , Let $Z_k = S_{T_k} - S_{T_{k-1}}$ (aka Ladder R.V.) and a partial sum of $Z_k, \space\space   S_{Z_n} = Z_1 + Z_2 + ... +Z_n. \space(Z_0 = 0)$ For any real number $ x, h(\gt0)$ , $$ p\{x\le first \space\space S_n \lt x+h\} = p\{x\le first \space\space S_{Z_n} \lt x+h\}, \space as \space\space x\to\infty $$ I can't get this identity at all. Eventually, I've just found out and understood  that (by using an extended version of Renewal Theorem) $$\lim_{x\to\infty}\sum_{k=1}^\infty p\{x\le first \space\space S_{Z_k} \lt x+h\} = \frac{h}{E[Z_1]} =  \frac{h}{E[X_1]E[T]} = \lim_{x\to\infty}\sum_{n=1}^\infty p\{x\le first \space\space S_n \lt x+h\}$$ Nonetheless, that does NOT necessarily mean that the two probabilities have to be identical $for \space n$ ?","['statistics', 'random-walk', 'renewal-processes', 'stochastic-processes', 'probability']"
4577054,"Is the infinite product of {0, 1} countable?","In my math class, we had an exercise asking us to prove that the following set is not countable: $$\prod \limits^\infty_{i=1} \{0,1\} = \{0,1\} \times \{0,1\} \times \{0,1\} \times \cdots$$ By Cantor's diagonalization argument, we can show that $$f : \mathbb{N} \to \prod \limits^\infty_{i=1} \{0,1\}$$ defined by $f(n) = (b_{1_n}, b_{2_n}, \cdots)$ is not surjective if we fix some $q = (q_1, q_2, q_3, \cdots)  \in \prod \limits^\infty_{i=1} \{0, 1\}$ where $$q_i = \begin{cases}
1 & \operatorname{if} ~ b_{i_i} = 0 \\
0 & \operatorname{if} ~ b_{i_i} = 1
\end{cases}$$ (i.e., the $n$ th element of $q$ is opposite of the $n$ th element of $f(n)$ ), which would mean that $x \neq f(n)$ for all $n \in \mathbb{N}$ and $\prod \limits^\infty_{i=1} \{0, 1\}$ is uncountable. However, it seems possible to define a bijection for $f: \mathbb{N} \to \prod \limits^\infty_{i=1} \{0, 1\}$ if we define $f(n)$ to be an ordered tuple representing the coefficients of the binary expansion of $n -1$ . For example: $$
0 = 0 \cdot 2^0 + 0 \cdot 2^1 + 0 \cdot 2^2 + \cdots \\
1 = 1 \cdot 2^0 + 0 \cdot 2^1 + 0 \cdot 2^2 + \cdots  \\
2 = 0 \cdot 2^0 + 1 \cdot 2^1 + 0 \cdot 2^2 + \cdots \\
3 = 1 \cdot 2^0 + 1 \cdot 2^1 + 0 \cdot 2^2 + \cdots \\
n = a_{n_0} \cdot 2^0 + a_{n_1} \cdot 2^1 + a_{n_2} \cdot 2^2 + \cdots 
$$ So then we could define $f: \mathbb{N} \to \prod \limits^\infty_{i=1} \{0,1\}$ by $$f(k) = (a_{k-1_0}, a_{k-1_1}, a_{k-1_2}, \cdots)$$ which would give us $$
f(1) = (0, 0, 0, \cdots) \quad \text{(the coefficients of the binary expansion of 0)}\\
f(2) = (1, 0, 0, \cdots) \quad \text{(the coefficients of the binary expansion of 1)} \\
f(3) = (0, 1, 0, \cdots) \quad \text{(the coefficients of the binary expansion of 2)} \\
f(4) = (1, 1, 0, \cdots) \quad \text{(the coefficients of the binary expansion of 3)} \\
f(n + 1) = (a_{n_0}, a_{n_1}, a_{n_2}, \cdots) \quad \text{(the coefficients of the binary expansion of n)}
$$ (Note: I use $k$ and $k - 1$ to preserve the domain of $\mathbb{N}$ , but it is trivial to prove that $|\mathbb{N} \cup \{0\}| = |\mathbb{N}|$ . Also, assume that more significant coefficients omitted by the ellipses are an infinite number of zeros.) In my mind, $f$ would be injective because every $n \in \mathbb{N}$ would have a unique binary expansion comprised of only 0s and 1s as coefficients (hence, the basis for the base-2 number system working), and $f$ would be surjective because we can turn a tuple of coefficients back into a base-10 integer. (i.e., $(b_0, b_1, b_2, \cdots)$ would translate directly to ${\dots b_2 b_1 b_0}_2 = n = k - 1$ .) Thus we've established a bijection that allows us to conclude that $|\mathbb{N}| = |\prod \limits^\infty_{i=1} \{0, 1\}|$ , making it countable. Would this disprove Cantor's diagonalization argument for this instance, or are there flaws to the binary expansion argument? Reference: Doud, Darrin, and Pace P. Nielsen. â€œExercise 30.6.â€ A Transition to Advanced Mathematics, Provo, Utah, 2022, p. 231.","['elementary-set-theory', 'cardinals', 'infinity']"
4577063,Norm of the inner product on an inner product space when considered as a continuous linear functional,"Let $V$ be a vector space equipped with the inner product $\langle \cdot, \cdot \rangle$ . Fix some arbitrary $y \in V$ and define the map $f_y: V \to \mathbb{C}$ by $$f_y(x) = \langle x,y \rangle, \qquad x \in V.$$ A few questions on MSE have shown that $f_y$ is a continuous linear functional with respect to the standard norm on V $\lVert x \rVert_V = \sqrt{\langle x,x \rangle}$ , for example by showing that $f_y \in B(V,\mathbb{C})$ (the space of bounded linear functionals from $V$ to $\mathbb{C}$ ). I want to show that the norm of $f_y$ in $B(V, \mathbb{C})$ equals $\lVert y \rVert_V$ . By the Cauchy-Shwarz inequality we have that $\lvert f_y(x) \rvert = \lvert \langle x,y \rangle \rvert \leq \lVert x \rVert_V \lVert y \rVert_V$ , so $f_y$ is bounded and hence continuous. I suspect that I can use this result to find an upper and lower bound for $\lVert f_y \rVert$ that are equal (in other words showing that $\lVert f_y \rVert \leq K$ and $\lVert f_y \rVert \geq K$ , for $K = \lVert y \rVert_V$ ), but am not sure on how to proceed along this line of reasoning. Any help or guidance would be much appreciated. Thanks!","['operator-theory', 'functional-analysis', 'analysis']"
4577083,Seeking for other methods to evaluate $\int_0^{\infty} \frac{\ln \left(x^n+1\right)}{x^n+1} dx$ for $n\geq 2$.,"Inspired by my post , I go further to investigate the general integral and find a formula for $$
I_n=\int_0^{\infty} \frac{\ln \left(x^n+1\right)}{x^n+1} dx =-\frac{\pi}{n} \csc \left(\frac{\pi}{n}\right)\left[\gamma+\psi\left(1-\frac{1}{n}\right)\right] \tag*{}
$$ Letâ€™s start with its partner integral $$
I(a)=\int_0^{\infty}\left(x^n+1\right)^a d x
$$ and transform $I(a)$ , by putting $y=\frac{1}{x^n+1}$ , into a beta function $$
\begin{aligned}
I(a) &=\frac{1}{n} \int_0^1 y^{-a-\frac{1}{n}-1}(1-y)^{-\frac{1}{n}-1} d y \\
&=\frac{1}{n} B\left(-a-\frac{1}{n}, \frac{1}{n}\right)
\end{aligned}
$$ Differentiating $I(a)$ w.r.t. $a$ yields $$
I^{\prime}(a)=\frac{1}{n} B\left(-a-\frac{1}{n}, \frac{1}{n}\right)\left(\psi(-a)-\psi\left(-a-\frac{1}{n}\right)\right)
$$ Then putting $a=-1$ gives our integral $$
\begin{aligned}
I_n&=I^{\prime}(-1) \\&=\frac{1}{n} B\left(1-\frac{1}{n}, \frac{1}{n}\right)\left[\psi(1)-\psi\left(1-\frac{1}{n}\right)\right] \\
&=-\frac{\pi}{n} \csc \left(\frac{\pi}{n}\right)\left[\gamma+\psi\left(1-\frac{1}{n}\right)\right]\end{aligned}
$$ For examples, $$
\begin{aligned}& I_2=-\frac{\pi}{2} \csc \frac{\pi}{2}\left[\gamma+\psi\left(1-\frac{1}{2}\right)\right]=\pi \ln 2,\\ & I_3=-\frac{\pi}{3} \csc \left(\frac{\pi}{3}\right)\left[\gamma+\psi\left(\frac{2}{3}\right)\right]=\frac{\pi \ln 3}{\sqrt{3}}-\frac{\pi^2}{9} ,\\ &I_4=-\frac{\pi}{4} \csc \left(\frac{\pi}{4}\right)\left[\gamma+\psi\left(\frac{3}{4}\right)\right]=\frac{3 \pi}{2 \sqrt{2}}\ln 2-\frac{\pi^2}{4 \sqrt{2}},\\
& I_5=-\frac{\pi}{5} \csc \left(\frac{\pi}{5}\right)\left[\gamma+\psi\left(\frac{4}{5}\right)\right]=-\frac{2 \sqrt{2} \pi}{5 \sqrt{5-\sqrt{5}}}\left[\gamma+\psi\left(\frac{4}{5}\right)\right], \\
& I_6=-\frac{\pi}{6} \csc \left(\frac{\pi}{6}\right)\left[\gamma+\psi\left(\frac{5}{6}\right)\right]=\frac{2 \pi}{3} \ln 2+\frac{\pi}{2} \ln 3-\frac{\pi^2}{2 \sqrt{3}},
\end{aligned}
$$ Furthermore, putting $a=-m$ , gives $$\boxed{I(m,n)=\int_0^{\infty} \frac{\ln \left(x^n+1\right)}{(x^n+1)^m} dx = \frac{1}{n} B\left(m-\frac{1}{n}, \frac{1}{n}\right)\left[\psi(m)-\psi\left(m-\frac{1}{n}\right)\right] }$$ For example, $$
\begin{aligned}
\int_0^{\infty} \frac{\ln \left(x^6+1\right)}{(x^6+1)^5} dx  & =\frac{1}{6} B\left(\frac{29}{6}, \frac{1}{6}\right)\left[\psi(5)-\psi\left(\frac{29}{6}\right)\right] \\
& =\frac{1}{6} \cdot \frac{21505 \pi}{15552} \cdot\left(-\frac{71207}{258060}-\frac{\sqrt{3} \pi}{2}+\frac{3 \ln 3}{2}+2 \ln 2\right) \\
& =\frac{21505 \pi}{93312}\left(-\frac{71207}{258060}-\frac{\sqrt{3} \pi}{2}+\frac{3 \ln 3}{2}+2 \ln 2\right)
\end{aligned}
$$ Are there any other methods? Your comments and alternative methods are highly appreciated.","['integration', 'definite-integrals', 'digamma-function', 'calculus', 'beta-function']"
4577100,Global sections of algebraically trivial line bundle,"Let $\pi \colon X \rightarrow C$ be a smooth minimal elliptic surface over an algebraically closed ground field $k$ . Furthermore assume that $\pi$ has a section and that the fundamental line bundle $R^1\pi_*(\mathcal{O}_X) \in \text{Pic}(C)$ has positive degree. A standard result, found e.g. as Theorem 6.5 in SchÃ¼tt's survey on elliptic surfaces , states that the Neron-Severi group $\text{NS}(X) = \text{Pic}(X)/ \text{Pic}^0(X)$ and the numerical group $\text{Num}(X) =  \text{Pic}(X) /  \text{Pic}^{\tau}(X)$ agree. Since the numerical group is the torsion-free part of the Neron-Severi group, this can be reformulated as the Neron-Severi group of such an elliptic surface being torsion-free. I would like to understand why that statement is true. If we denote by $\mathcal{L}$ a representative of a non-trivial torsion class in $\text{NS}(X)$ , then I can manage to prove that the class of $\mathcal{L}$ is trivial based on the assumption that $h^0(\mathcal{L}) = 0$ , i.e. that $\mathcal{L}$ has no non-trivial global sections. This assumption seems very plausible to me: $\bullet$ If $X$ were a curve, so that algebraically trivial line bundles are exactly the line bundles with degree $0$ , then it is standard to show that such line bundles have no non-trivial global sections, unless they agree with the structure sheaf. $\bullet$ If $X$ were an abelian variety, I also have an argument. It is quite simple to show that a tensor power $\mathcal{L}^{\otimes n}$ of a line bundle is the pullback of $\mathcal{L}$ along the $n$ -th power map of the abelian variety (by induction). Therefore, if $\mathcal{L}$ has a section, so does its dual $\mathcal{L}^{\otimes -1}$ . Hence $\mathcal{L} = \mathcal{O}_X$ . $\bullet$ Furthermore, algebraically trivial line bundles are in particular numerically trivial and hence have degree $0$ when restricted to any curve in $X$ . Therefore we have no non-trivial global sections on curves in $X$ . Maybe one can pull this back? Most likely there is a simple generalization of the argument for curves  to general algebraically trivial line bundles, but I cannot seem to figure it out or to find it online. Therefore: Question: Why do non-trivial algebraically trivial line bundles (on a smooth minimal elliptic surface if necessary) have no non-trivial global sections?","['algebraic-geometry', 'intersection-theory', 'picard-scheme']"
4577115,Rieszâ€“Markovâ€“Kakutani representation for symmetric matrix-valued measures,"Let $\Theta$ be a closed $d$ -manifold (and hence a metric space with Borel- $\sigma$ -algebra $\Sigma \subset 2^\Theta$ ), $S^d$ the set of real symmetric $d \times d$ matrices, which we can identify with the vector space $\mathbb{R}^{\frac{d(d + 1)}{2}}$ , $M(\Theta; S^d)$ be space of Borel vector measures on $(\Theta, \Sigma)$ with values in $S^d$ , $C(\Theta; S^d)$ the space of continuous functions from $\Theta$ to $S^d$ . (If I should instead consider compactly supported continuous functions, then this is fine, too.) I am trying to find a duality statement along the lines of $[M(\Theta; S^d)]^* \cong C(\Theta; S^d)$ , where $^*$ denotes the dual vector space. For $d = 1$ I think I know how a isometric isomorphism $\Psi \colon [M(\Theta; S^d)]^* \to C(\Theta; S^d)$ and its inverse looks like, but I am not sure how to generalise this result and its proof to higher dimensions. It should be relatively straightforward since $S^d$ is finite-dimensional, so we should be able to ""copy the one-dimensional procedure componentwise"", see e.g. the comment on this highly related question or this pretty similar question , but I don't know how to do that. For $d = 1$ I have the following: consider $$
\Psi: [M(\Theta, \mathbb R)]^* \to C(\Theta; \mathbb R), \qquad
Z \mapsto \big(\theta \mapsto Z(\delta_{\theta})\big),
$$ where $\delta_{\theta}(A) := \begin{cases} 1, & \text{if } \theta \in A, \\ 0, &\text{else.}\end{cases}$ is the Dirac measure at $\theta \in \Theta$ for $A \in \Sigma$ . This map is clearly linear.
It is also injective: for $Z_1, Z_2 \in [M(\Theta; \mathbb R)]^*$ we have $\Psi(Z_1) = \Psi(Z_2)$ if and only if $Z_1(\delta_{\theta}) = Z_2(\delta_{\theta})$ for all $\theta \in \Theta$ .
By the linearity of $Z_1$ and $Z_2$ this implies that $Z_1$ agrees with $Z_2$ on any spike train $\sum_{k = 1}^{n} a_k \delta_{\tilde{\theta}_k}$ for $n \in \mathbb N$ , $(a_k)_{k = 1}^{n} \subset \mathbb R$ and $(\tilde{\theta}_k)_{k = 1}^{n} \subset \Theta$ . By the continuity of $Z_1$ and $Z_2$ and the density of those spike trains in $M(\Theta; \mathbb R)$ , we can conclude that $Z_1 = Z_2$ . As inverse I suggest $$
\Psi^{-1}(f)
:= \left(\mu \mapsto \int_{\Theta} f(\theta) \; \text{d}\mu(\theta) \right).
$$ Indeed, for $f \in C(\Theta; \mathbb R)$ we have $$
\Psi(\Psi^{-1}(f))
= \Psi\left(\mu \mapsto \int_{\Theta} f(\theta) \; \text{d}\mu(\theta) \right)
= \left( \tilde{\theta} \mapsto \int_{\Theta} f(\theta) \; \text{d}\delta_{\tilde{\theta}}(\theta\right)
= f.
$$ Furthermore, for $Z \in [M(\Theta; \mathbb R)]^*$ we have $$
\Psi^{-1}(\Psi(Z))
= \Psi^{-1}\left(\theta \mapsto Z(\delta_{\theta})\right)
= \left( \mu \mapsto \int_{\Theta} Z(\delta_{\theta}) \;\text{d}\mu(\theta) \right).
$$ Due to the linearity of $Z$ we have for any spike train as above $$
\big[\Psi^{-1}(\Psi(Z))\big]\left(\sum_{k = 1}^{n} a_k \delta_{\tilde{\theta}_k} \right)
= \sum_{k = 1}^{n} a_k Z(\delta_{\tilde{\theta}_k})
= Z \left(\sum_{k = 1}^{n} a_k \delta_{\tilde{\theta}_k} \right).
$$ Hence by continuity of $Z$ and the density of the spike trains we conclude $\Psi^{-1}(\Psi(Z)) = Z$ . What are the higher-dimensional analogues of $\Psi$ and $\Psi^{-1}$ ? I tried $\Psi(Z) := \big(\theta \mapsto Z(P \delta_{\theta})\big)$ , where $P = \text{id}_{d \times d} \in S^d$ and $\Psi^{-1}(f) := \left( \mu \mapsto \text{tr}\left( \int_{\Theta} f(\theta) \; \text{d}\mu(\theta)\right)\right)$ , but they don't seem to fulfil either $\Psi \circ \Psi^{-1} = \text{id}_{C(\Theta; S^d)}$ nor $\Psi^{-1} \circ \Psi = \text{id}_{[M(\Theta; S^d)]^*}$ . Update When equipping $M(\Theta; S^d)$ with the (strong) generalised total variation norm, then by Singer's representation theorem we have the opposite duality $[C(\Theta; S^d)]^* \cong M(\Theta; S^d)$ via the dual pairing $$
\langle \phi, u \rangle_{C(\Theta; S^d) \times M(\Theta; S^d)}
= \int_{\Theta} \langle \phi(\theta), u'(\theta) \rangle_{S^d} \; \text{d}| u |(\theta),
$$ where $| u | \in M(\Theta; [0, \infty))$ is the total variation measure of $u$ and $u'$ is the Radon-Nikodym derivative of $u$ with respect to $| u |$ . Hence we only have to figure out how $| u |$ and $u'$ look in this specific case.
If $u = \sum_{k = 1}^{N} P_k \delta_{\{ \theta_k \}}$ , we have $$
| u |
= \sum_{k = 1}^{N} \| P_k \|_{S^d} \delta_{\{ \theta_k \}}
\qquad \text{and} \qquad
u'(\theta_k)
= \frac{1}{\| P_k \|_{S^d}} P_k,
$$ so that $$
\langle \phi, u \rangle_{C(\Theta; S^d) \times M(\Theta; S^d)}
= \sum_{k = 1}^{N} \text{tr}\big(P_k \phi(\theta_k)\big).
$$","['measure-theory', 'riesz-representation-theorem', 'vector-measure', 'symmetric-matrices', 'duality-theorems']"
4577187,Well-posedness of BVP for Poisson's equation,"Consider the BVP, \begin{eqnarray}
-\Delta u&=&f \quad \text{ in } \Omega \subset \mathbb{R}^d,\\
u&=&g \quad \text{ on } \partial\Omega.
\end{eqnarray} The well-known weak formulation of the above BVP is to look for $u\in H^1(\mathbb{R}^d)$ satisfying \begin{eqnarray}
-\int\limits_{\mathbb{R}^d} \sum\limits_{i=1}^du_{x_i}v_{x_i} dx = \int\limits_{\mathbb{R}^d} fv dx \quad \text{ for all } v \in H^1_0(\mathbb{R}^d). \quad \quad \quad \quad \quad \quad \quad \quad (1)
\end{eqnarray} Why don't we consider the a more weaker (but more intuitive) definition: $u\in L^2(\mathbb{R}^d)$ and satisfies the following weak formulation \begin{eqnarray}
-\int\limits_{\mathbb{R}^d}u \Delta\phi dx =\int\limits_{\mathbb{R}^d} f\phi dx\quad \text{ for all } \phi\in C^2_c(\mathbb{R}^d). \quad \quad \quad \quad \quad  \quad \quad \quad \quad \quad \quad(2)
\end{eqnarray} Is it because of non uniqueness? If so, can wee explicitly construct two different $L^2$ functions satisfying the weak formulation(2)? P.S.: Clearly weak solution in $H^1(\mathbb{R}^d)$ satisfying (1) will automatically satisfy (2). So existence is not an issue at all.","['boundary-value-problem', 'functional-analysis', 'analysis', 'partial-differential-equations']"
4577209,Find the $\angle DBC$ in the quadrilateral below,"For reference: Given the quadrilateral ABCD such that $\angle CAD=25^\circ , \angle ACD=45^\circ$ and $\angle BAC=\angle BCA=20^\circ$ , what is the value of angle $\angle DBC$ ? I made the drawing and distributed the angles. I drew the perpendicular $CJ$ $\perp$ $JI$ I would need to demonstrate that $CJ$ is parallel to $BD$ and/or that $\angle CDB = \angle ICJ$ but I couldn't find the way","['euclidean-geometry', 'geometry', 'plane-geometry']"
4577234,The limit of $(\sin(n!)+1)^{1/n}$ as n approaches infinity,"Calculate the limit $$
\lim_{n\rightarrow\infty}(\sin(n!)+1)^{1/n}
$$ or prove that the limit does not exist. This appeared as a problem in my mathematical analysis test, and the answer was that the limit exists and it was $1$ . But later the teacher found a mistake in his proof and eventually removed the problem from the test. But I'm just curious. Does this problem have a certain answer? The biggest question for me, is that I can't show that there does not exist any $n_0$ so that $\sin(n_0!)$ is close to $-1$ enough so that the original term might not converge. Any help would be appreciated!","['factorial', 'analysis', 'sequences-and-series', 'limits', 'trigonometry']"
4577257,Proof that every finite tree with at least 2 vertices have at least two vertices of degree 1,"The proof in my lecture notes starts as such: Let $T$ be a finite tree on at least $2$ vertices. Consider a path $P = xe_1x_1e_2...y$ of maximum length in $T$ . This path begins with a vertex $x$ , and ends with a different vertex $y$ . Assume that one of $x,y$ (w.l.o.g. $x$ ) has degree at least $2$ . Thus $x$ has a neighbour $w$ different from $x_1$ . If $w$ is not contained in $P$ , then the path going from $w$ to $x$ and then via $P$ to $y$ is strictly longer than $P$ , contradicting the maximality of $P$ . Hence, $w$ is in $P$ . So far so good. Now comes the part I don't understand: but then $T$ contains a cycle: Starting at $x$ , follow along $P$ until $w$ , then go back to $x$ via the edge $\{x,w\}$ . I don't understand how this can be a cycle. When you ""start at $x$ , follow along $P$ until $w$ "", don't you walk along the edge $\{x,w\}$ ? .","['graph-theory', 'trees', 'discrete-mathematics']"
4577268,Proof that this set is open,"Consider the sequence $(a_n)_n$ that is defined by $a_n:=1/n$ and the set $A:=\{a_n \ | \ n \in \mathbb{N}\} \cup \{0\}$ . I want to show that $\mathbb{R} \setminus A$ is open in $\mathbb{R}$ . I had two ideas to show this. $(1)$ It should be the case that $\mathbb{R} \setminus A = (-\infty,0) \cup (1, \infty) \cup \bigcup_{n \geq 2} \left(\frac{1}{n+1}, \frac{1}{n}\right)$ , so it is in particular a union of open sets and thus open. $(2)$ Let $x \in \mathbb{R} \setminus A$ . Then define $\delta:= \min\{|x-a| \ : \ a \in A\}$ and $B:=\left(x-\frac{\delta}{2},x+\frac{\delta}{2}\right)$ . Then $B$ is open with $x \in B$ and $B \subseteq \mathbb{R} \setminus A$ . Im not quite sure if the minimum exists here, which is why I am not sure if this would work. If one would not have $0$ in A for example and choose $x=0$ , then one would have $\delta=0$ which would also be a problem. Are these attempts correct?",['general-topology']
4577298,Limit of $2^{n^2/2}\sum_{j=1}^{n/2} \sum_{k=1}^{n/2}\left(\cos^2(\frac{j \pi}{n+1}) + \cos^2(\frac{k \pi}{n+1})\right)$ as a double integral,"I am currently looking into Dimer coverings and my next step is to find how the following limit is calculated: $$\begin{align*}
L &= \lim_{n \to \infty}\frac{1}{n^2}\ln\left(2^{n^2/2}\prod_{j=1}^{n/2} \prod_{k=1}^{n/2}\left(\cos^2(\frac{j \pi}{n+1}) + \cos^2(\frac{k \pi}{n+1})\right)\right) \\[1ex]
&= \frac{1}{16 \pi^2}\int_{-\pi}^{\pi}\int_{-\pi}^{\pi}\ln[4+2\cos(x) + 2\cos(y)] \, dx \, dy
\end{align*}$$ I think I got quite far, but there is some error in my calculation, which gives the required result, but the lower bound of my integrals is 0 instead of $- \pi$ . I will write my calculations down here. If anyone finds what I did wrong, please tell me. It would help a lot. $$\begin{align*}
L &= \lim_{n \to \infty} \frac{1}{n^2} \ln\left(2^{n^2/2}\prod_{j=1}^{n/2} \prod_{k=1}^{n/2} \left(\cos^2\left(\frac{j \pi}{n+1}\right) + \cos^2\left(\frac{k \pi}{n+1}\right)\right)\right) \\[1ex]
&= \lim_{n \to \infty} \frac{1}{n^2} \sum_{j=1}^{n/2} \sum_{k=1}^{n/2}\ln\left(4\left(\cos^2\left(\frac{j \pi}{n+1}\right) + \cos^2\left(\frac{k \pi}{n+1}\right)\right)\right) \\[1ex]
&= \frac{1}{4 \pi^2} \int_0^{\pi/2} \int_0^{\pi/2} \ln(4(\cos^2(x)+\cos^2(y)) \, dx \, dy \\[1ex]
&= \frac{1}{4 \pi^2} \int_0^{\pi/2} \int_0^{\pi/2} \ln\left(4\left(\frac12\cos(2x)+\frac12+\frac12\cos(2y)+\frac12\right)\right) \, dx \, dy\\
&= \frac{1}{4 \pi^2} \int_0^{\pi/2} \int_0^{\pi/2} \ln(2\cos(2x)+2\cos(2y)+4) \, dx \, dy\\
&= \frac{1}{16 \pi^2} \int_0^{\pi} \int_0^{\pi} \ln(2\cos(x)+2\cos(y)+4) \, dx \, dy
\end{align*}$$","['integration', 'limits', 'summation', 'riemann-sum']"
4577322,"Proof of the ""rule of $4$"": If the last two digits of $x$ are divisible by $4$, then $x$ is divisible by $4$.","I'm a aware there's another question about this problem. However, it is a general question of the form ""how can one prove this property, with an incomplete proof attached, and not a proof verification such as mine. My proof is not present in the aforementioned question nor in its answers. I was requested to prove the following property: $\text{If the number formed by the last two digits of $x$ is divisible }$$\text{by $4$, then $x$ is divisible by $4$.}$ I'm self-studying discrete mathematics and was wondering if someone could validate my proof. Here's what I did. $\text{Lemma :}$ $\forall n>1|2^n \equiv 0 \pmod{4}$ . $\text{pf.}$ The base case is trivial. Assume $2^k=4q, k>1$ . Then $2^{k+1}=2^k\cdot 2=4q \cdot2 = 8q$ is divisible by four. Then $4|2^n$ for all $n > 1$ . $\text{Solution :}$ Notice that $$
x=x_{n-1}10^{n-1}+...+x_110+x_0 = \sum_{i=0}^{n-1} x_i10^i \tag{1}
$$ where $(x_{n-1}x_{n-2}...x_1x_0)_{10}$ is the representation of $x$ in base $10.$ It is clear that $10 \equiv 2 \pmod{4}$ and therefore $x_i10^i \equiv x_i2^i \pmod{4}$ . Particularly, for the last two digits of $x,$ we have $x_110+x_0 \equiv2x_1+x_0$ . Generally, for $x$ we get $$
x\equiv \sum_{i=0}^{n-1} x_i10^i \equiv \sum_{i=0}^n x_i2^i \pmod{4} \tag{2}
$$ Assume $4|(x_12+x_0)$ or rather $x_12+x_0 \equiv 0 \pmod{4}$ . Then $$\begin{align}
x\equiv \sum_{i=0}^nx_i2^i \equiv x_12+x_0+\sum_{i=2}^nx_i2^i \equiv0\end{align}$$ The result follows from our assumption and from the fact that our lemma guarantees $\sum_{i=2}^{n}x_i2^i \equiv 0\pmod{4}$ . This suffices to show $4|(x_i10+x_0) \implies 4|x$ . Is this proof correct? Thanks in advance.","['elementary-number-theory', 'solution-verification', 'modular-arithmetic', 'discrete-mathematics']"
4577334,"Lipschitzness of $f$ on a high-probabilty subset from $\|f(X)-f(Y)\|\le \|X-Y\|$ with high probability for iid $(X,Y)$","Consider two iid random vectors $(X,Y)$ both in $R^k$ and $f:R^k\to R^m$ . Assume that for some $\epsilon>0$ (that may be assumed small if necessary, say, $\epsilon<0.01$ ), it holds that $$P \Big( \|f(X)-f(Y)\| \le \|X-Y\|\Big) \ge 1-\epsilon.$$ The norm is the Euclidean norm. The goal is to derive from this high-probabilty bound on the product measure some Lipschitzness condition on $f$ . I am looking of a reverse, in some sense, of the simple observation that if the restriction of $f$ to some $A$ is 1-Lipschitz and $P(X\in A)\ge1-\epsilon/2$ then the above probability bound is true. Question: Assuming $P ( \|f(X)-f(Y)\| \le \|X-Y\|) \ge 1-\epsilon$ ,
Is it always true that there exists a measurable $A\subset R^k$ such that the restriction of $f$ to $A$ is 1-Lipschitz (or C-Lipschitz for some C) and such that $P(X\in A)\ge 1-g(\epsilon)$ for some $g$ with $g(\epsilon)\to_{\epsilon\to0}0$ ? An answer would still be of interest to me if $k=m=1$ and $X,Y\sim$ Uniform $[0,1]$ .","['independence', 'lipschitz-functions', 'functional-analysis', 'product-measure', 'probability']"
4577337,Filling space with polycube snakes,"One of Martin Gardner's ""Mathematical Games"" columns ( Scientific American June 1981, pp24â€“29 ; reprinted in The Last Recreations (1997), pp274â€“283 , and The Colossal Book of Mathematics (2001) pp203â€“211 ) describes the following problem due to Scott Kim: First we must define a snake. It is a single connected chain of identical unit cubes joined at their faces in such a way that each cube (except for a cube at the end of a chain) is attached face to face to exactly two other cubes. The snake may twist in any possible direction, provided no internal cube abuts the face of any cube other than its two immediate neighbors. The snake may, however, twist so that any number of its cubes touch along edges or at corners. A polycube snake may [...] have just one end and be infinite in length, or it may be infinite and endless in both directions. We now ask a deceptively simple question. What is the smallest number of snakes needed to fill all space? We can put it another way: Imagine space to be completely packed with an infinite number of unit cubes. What is the smallest number of snakes into which it can be dissected by cutting along the planes that define the cubes? [...] Kim has found a way of twisting four infinitely long one-ended snakes into a structure of interlocked helical shapes that fill all space. The method is too complicated to explain in a limited space; you will have to take my word that it can be done. [...] Kim has conjectured that in a space of $n$ dimensions the minimum number of snakes that completely fill it is $2(n-1)$ , but the guess is still a shaky one. In the reprints (I can't find this ever printed in the Scientific American back issues, but maybe I missed it), Gardner adds: Dr. Koh Chor Jin, a physicist at the National University of Singapore, sent a clever proof that given a finite volume of space it is possible to cover it with two of Kim's cube-connected snakes. However, as Kim pointed out, Jin's construction does not approach all of space as a limit [...] So my question for StackExchange is: What was Scott Kim's (or, what is a ) four-snake space-filling construction? And what was Koh Chor-Jin's (or, what is a ) way to cover any finite volume with two snakes? This guy is also looking for information on the problem. Footnote: At first I wondered if $2(n-1)$ was a typo for $2^{n-1}$ , since obviously it takes one snake (not zero snakes) to fill 1-space. But it's consistently typeset as $2(n-1)$ in all three primary sources, so I think the ""for $n\ge 2$ "" is implied.","['recreational-mathematics', 'conjectures', 'tessellations', 'discrete-mathematics']"
4577364,Is $\liminf\Bbb P(X_nY_n\leq b)\geq\Bbb P(Xy\leq b) $ if $X_n\to X$ in distribution and $Y_n\to y\in\Bbb R$ in probability?,"Take $b\in\mathbb{R}$ . Suppose $X_n\to X$ in distribution  and suppose $Y_n\to y$ in probability, where $y$ is a constant. How to prove $$\Bbb P(Xy\leq b) \leq \liminf_{n\to\infty}\Bbb P(X_nY_n\leq b)?$$ I've asked the question before but I didn't get any answer. So I tried myself and hopefully someone can follow my reasoning. Note : I know that this follows from Slutsky's theorem, but I'm trying to use this question to prove Slutsky's theorem. My try: Since $X_n\to X$ in distribution, then given $\epsilon>0$ there an $n\in\mathbb{N}$ such that for all $n\geq N$ $$\Bbb P(Xy\leq b)\leq\epsilon+\Bbb P(X_ny\leq b),\;\;\text{ and so }\;\;\; \Bbb P(Xy\leq b)\leq\epsilon+\inf_{n\geq N}\Bbb P(X_ny\leq b).$$ Now we need to use the fact that $Y_n\to y$ in probability.
We can write the set $$\{X_ny\leq b\}=\{X_ny\leq b,Y_n\in (y-\epsilon,y+\epsilon) \}\cup \{X_ny\leq b,Y_n\not\in (y-\epsilon,y+\epsilon) \},$$ and so $$
\{X_ny\leq b\}\subseteq\underbrace{\{X_nY_n\leq b+\epsilon}_{:=A_n} \}\cup \underbrace{\{Y_n\not\in (y-\epsilon,y+\epsilon) \}}_{:=B_n},$$ Now we know that $\liminf_{n\to\infty}\Bbb P(B_n)=0$ . Now let's look at $A_n$ . For the sake of simplicity let's assume $X_n>0$ .
Then we have that $$A_n=\{X_nY_n\leq b\}\cup \underbrace{\{b<X_nY_n\leq b+\epsilon\}}_{C_n}.$$ In total, we have that $$\Bbb P(X_ny\leq b)\leq \Bbb P(B_n) + \Bbb P(X_nY_n\leq b)+\mathbb{P}(C_n) $$ and $$ \Bbb P(Xy\leq b)\leq \Bbb P(X_nY_n\leq b)+\epsilon + \Bbb P(B_n) +\mathbb{P}(C_n) $$ and so finally we have made the set $\{X_nY_n\leq b\}$ appear. Taking $\liminf$ we have that $$ \Bbb P(Xy\leq b)\leq \liminf_{n\to\infty}\Bbb P(X_nY_n\leq b)+\epsilon  +\liminf_{n\to\infty}\mathbb{P}(C_n) $$ But then is it possible to bound the probability of $C_n=\{b<X_nY_n\leq b+\epsilon\}$ as a function of $\epsilon$ ?","['limsup-and-liminf', 'measure-theory', 'probability-theory', 'probability']"
4577368,What number of group elements of a specific order are (non-)realisable?,"My question is as follows: Let $G$ be an arbitrary finite group. Let $n_r(G) \in \mathbb{N}_0$ be the amount of elements that have exactly order $r \geq 2$ in $G$ . For which numbers $k \in \mathbb{N}_0$ can we find/construct groups $G$ such that $n_r(G) = k$ ? For $k$ such that it is possible, can one find representatives only constructed using cyclic groups and semidirect (including direct) products? What about if we drop the finiteness requirement? Disclaimer: My knowledge about Group Theory extends to about that of a first course in Abstract Algebra (Quotient Groups, Group Actions, Sylow theorems, Solvability) and I have just begun trying to understand semidirect products and getting used to GAP. Feel free to call me out on any nonsense I may write. As already discussed in this question debating the existence of a group with exactly $68$ resp. $92$ elements of order $3$ , this question is highly non-trivial for the case $r \neq 2$ (but affirmative if $r = 2$ by using dihedral groups).
I have been able to deduce from this post that for $p$ prime we have $$n_{p} \equiv p-1 \mod p(p-1)$$ by a result of Frobenius ( $n_p = p-1 \mod p$ ) and using that every element (except $e)$ has order $p$ in element-generated subgroups of size $p$ (such that $n_p \equiv 0 \mod p-1$ ). This weeds out the cases we must check dramatically, e.g. we have $$n_3 \equiv 2 \mod 6 \text{ and } n_5 \equiv 4 \mod 20 \text{ and } n_7 \equiv 6 \mod 42 \ldots$$ For a given prime $p$ and $m \in \mathbb{N}$ , we can obviously cover the cases $n_p = p^m-1$ with the group $(\mathbb{Z}_p)^m$ - the $m$ -times iterated direct product of $\mathbb{Z}_p$ . But for everything in between, it always has involved guess work and pattern spotting from my side. The reason why I ask if this can be done with cyclic groups and semidirect products only is simple: I used GAP to search for the smallest groups $G$ that have the property $n_r(G) = k$ and in all - except one - examined cases it has always spat out a combination of cyclic groups by (semi)direct products. Now, since the linked question answers the stated premise negatively, we obviously cannot realise all these possibilities given by above condition. Specifically, I constructed a table with small primes and their conjectured non-realisible numbers: $$\begin{array}{c|c}
p & k \in \mathbb{N}: ?\exists G: n_p(G) = k \text{ gives no output from GAP}  \\
\hline
3 & \require{enclose}\enclose{horizontalstrike}{68}, \require{enclose}\enclose{horizontalstrike}{92}^*, 110,140,164,176,212,230,236,260,\ldots
\\
5 & 84, 104, 144, 204, 224, 244, 264, 304, 324,\ldots
\\
7 & 90, 132, 216, 300, 468, \ldots
\\
11 & 340, 450, 560,\ldots
\end{array}$$ *As pointed out by spin in the comments. A crossed out number in this table means that one provably cannot find such a group, I still include it to keep it consistent with the (non-existing) output of my small GAP program. Also keep in mind that this table could include some false negatives since I cannot search with GAP for obscenely large groups that might fulfill the given property (or straight up do not exist as in the case $n_3 = 68$ ). This brings us back to the following thoughts: Are these truly not realisable? What general families of realisable groups do exist? Are there notable exceptions of realizations?... as possible jumping off points to attacking the question. Bonus question : Can you point me to literature references that have worked on this and related questions? Thank you for listening to my ramblings about Group Theory :^).","['group-theory', 'finite-groups', 'reference-request']"
4577427,"Investigate for convergence in $z\in\mathbb{C}, \ |z|<1$: $\Sigma_{k=1}^{+\infty}\left(\cos (z^k)-1\right).$","Investigate for convergence in $z\in\mathbb{C}, \ |z|<1$ : $$\Sigma_{k=1}^{+\infty}\left(\cos (z^k)-1\right).$$ We were recommended to use $\cos(z^k)$ expansion first: $$
\Sigma_{k=1}^{+\infty}\left(\cos (z^k)-1\right)=\Sigma_{k=1}^{+\infty}\Sigma_{n=1}^{+\infty}\left(\frac{(-1)^n z^{2kn}}{(2n)!}\right) =
$$ here we need to reference some theorem that says that $n$ and $k$ changing places $\Sigma_{k=1}^{+\infty}\Sigma_{n=1}^{+\infty}\left(\frac{(-1)^n z^{2kn}}{(2n)!}\right)=\Sigma_{n=1}^{+\infty}\Sigma_{k=1}^{+\infty}\left(\frac{(-1)^n z^{2kn}}{(2n)!}\right)$ is allowed. I don't remember such theorem, and failed to google it. I don't see why $n$ and $k$ changing places would not always be okay. I would be very grateful if someone could give a statement of the theorem. $$=\Sigma_{n=1}^{+\infty}\frac{(-1)^n}{(2n)!}\Sigma_{k=1}^{+\infty} 
 \ z^{2kn}=\Sigma_{n=1}^{+\infty}\left(\frac{(-1)^n}{(2n)!}\cdot\frac{z^{2n}}{1-z^{2n}}\right)$$ Ratio test: $$\frac{(-1)^{n+1} z^{2n+2}}{(2n+2)! \ (1-z^{2n+2})} \cdot \frac{(2n)! \ (1-z^{2n})}{(-1)^n \ z^{2n}} 
= 
\frac{(-1) z^{2}(1-z^{2n})}{(2n+1)(2n+2) \ (1-z^{2n+2})} \longrightarrow_{n \longrightarrow \infty} 0 < 1, $$ so the original series is convergent in the given region. I feel like my solution is wrong. If it is, could someone please point out mistakes, and give hints or explain how to solve correctly? Thank you.","['complex-analysis', 'convergence-divergence', 'solution-verification']"
4577428,When is a principal bundle with group $G \times H$ the product of a principal bundle with group $G$ and one with group $H$?,"In general, it is not true that a fiber bundle with a product fiber is the product of two fiber bundles with the factors as fibers (think of vector bundles). However, I read somewhere that every 2-torus bundle is the product of two circle bundles, and I do not know why. So, my question is both why is this true in the torus case and what can we say in the more general case of the product of two Lie groups?","['principal-bundles', 'fiber-bundles', 'gauge-theory', 'lie-groups', 'differential-geometry']"
4577443,Does the solution of $y' = (x^2 + y^2) e^{-(x^2+y^2)}$ have a limit for $x \to \infty$?,"An old exam problem I am trying to solve is as follows: Given the cauchy problem $y' = (x^2 + y^2) e^{-(x^2+y^2)}, y(x_0) = y_0$ , do the following: Show that there is a unique solution for all $x \in \mathbb{R}$ Does the limit $ \lim_{x \to \infty} y(x) $ exist? Hint: evaluate the limit $ \lim_{x \to \infty} (x^2 + y^2) e^{-(x^2+y^2)} e^x$ For part (1), I managed to bound the functions $f(x,y) = (x^2 + y^2) e^{-(x^2+y^2)}$ and $f_y(x,y)$ under some fixed value. Then, I concluded, that we have a solution at some interval $[x_0 - \varepsilon, x_0 + \varepsilon]$ and we can extend this to $\mathbb{R}$ by moving to the right and left and applying the same result. I had quite a hard time bounding the function $f_y(x,y)$ , so first of all, I would be glad if someone shows me a quick and elegant way to do so. Secondly, and more importantly, I don't know how to approach part (2). I suppose I should somehow bound the integral $\int  (x^2 + y^2) e^{-(x^2+y^2)}$ by $(x^2 + y^2) e^{-(x^2+y^2)} e^x$ and then show that this limit tends to $0$ . But I have been so far unsuccesful with showing either of these two claims to be true. So any help here would be much appreciated!","['cauchy-problem', 'analysis', 'ordinary-differential-equations', 'real-analysis']"
4577477,Fusion in the normaliser of a Sylow subgroup.,"Let $z \in P \cap Z(N_{G}(P))$ for some $P \in Syl_{p}(G)$ and $z \notin P'$ . If $tz^{n}t^{-1} \in P$ , for some $t \in G$ , then $tz^{n}t^{-1} = z^{n}$ . I have tried a lot solving the above question, but was unable to do it. What I know is that, $N_{G}(P)$ controls the fusion of $C_{G}(P)$ , so if I can show that $tz^{n}t^{-1} \in C_{G}(P)$ , then the rest follows from the Burnside's lemma. But I am not able to show that $tz^{n}t^{-1} \in C_{G}(P)$ .","['group-theory', 'normal-subgroups', 'finite-groups', 'sylow-theory']"
4577513,Collatz-like problem involving prime factors,"Unfortunately I am not well-versed in LaTeX so I will try my best to keep this looking presentable. As an overview, I was investigating a variation of the Collatz conjecture: Define $f(1) = 1$ Then, if $n$ is even, $f(n) = \frac{n}{2}$ Otherwise, let $n$ 's smallest prime factor be $p$ , then $f(n) = pn+1$ So then I was trying it out for small values: $f(1) = 1$ $f(2) = 1, f(1) = 1$ $f(3) = 10, f(10) = 5, f(5) = 26, f(26) = 13, ...$ and so on. Eventually you get to $f(213) = 640$ which reduces down to $10$ . Evidently, loops/cycles can form. $f(4) = 2, f(2) = 1$ $f(5) = 26$ which is already a part of $3$ 's cycle $f(6) = 3$ which is, again, part of $3$ 's cycle By now, I was thinking that this function had a nice trend of either reducing to $1$ or joining onto another number's cycle. However, $f(7)$ causes an issue. I have iterated for $f(7)$ around $100$ times using python, however it is still unclear whether $f(7)$ converges or not (for want of a better word). I also noticed that convergence of $f(9)$ depended on convergence of $f(7)$ . There is no reason why it should converge, I saw on another page that the average smallest prime factor of integers up to $n$ is asymptotic to $\frac{n}{2\log(n)}$ . So then $f(n)$ has average order $\frac{n^2}{2\log(n)}$ . My question then is ultimately: for any integer n, will $f(n)$ eventually reach 1 or form a cycle? And, if not, is there an interesting reason/proof why not? This question could be (and probably is) fairly difficult, so as a weaker question, does $f(7)$ converge? If this question is similar to, or a corollary of, another question asked here, I apologise. Please let me know and I will remove the question. Edit: I wrote some basic python code to do the computations for me, however, it is not very efficient. With that being said, I also have an interest in the computational complexity of this problem e.g. finding smallest prime factor, checking for repeated values (i.e. checking if a cycle has been formed) etc.","['collatz-conjecture', 'number-theory', 'recreational-mathematics', 'problem-solving']"
4577530,How to solve the differential equation $y'=\frac{y-xy^2}{x+x^2y}$,"$$y'=\frac{y-xy^2}{x+x^2y}$$ This is the equation I want to solve. My idea is to substitute $xy$ with $u,$ $u=xy$ and $\frac{du}{dx}=xy'+y.$ So, the equation becomes $y'=\frac{y(1-u)}{x(1+u)}.$ What I am struggling with is how to deal with the $x$ and $y$ that are left in the equation.","['integration', 'substitution', 'ordinary-differential-equations']"
4577579,Why does this approximation constant work?,"I found an algorithm that used an approximation of $\log_2(x + 1)$ from $0$ to $1$ which simply followed the line $y = x + k$ where $k$ was some constant they discovered to be something like $0.043$ . I wondered where this constant came from, so I made a Desmos page to experiment, and I graphically figured out what it represents. My idea was that I'd have to take the integral of the absolute value of the difference between the two curves and then minimize the average height with respect to the value $k$ . This did not yield the desired $0.043$ that the algorithm used. However, the $0.043$ seems to be the number where the integral is spread out the most ""evenly"" (another way I could see it is that the maximum error is minimized. Notice how the maxima all match up). Is there some sort of statistical or actual algebraic way of expressing this? I'm just a high school student, I'd love to know how to express my findings better. Cheers.","['integration', 'optimization', 'approximation', 'algorithms']"
4577607,How to find the second solution of this $\log_{x}(2x^{x-2} - 1) + 4 = 2x$ logarithmic equation?,"I have to find the points where: $$
\log_{x}(2x^{x-2} - 1) + 4 = 2x
$$ It's easy to see that $x=2$ is a solution but, when graphing the function I see that there is another solution around 1. How can I find the value exact value of $x$ ? If I try using $x$ as the base to eliminate the logarithm I get the equation: $$
2x^{x-2} - 1 = x^{2x- 4},
$$ and the solutions to this equation are: $x = 1$ and $x=2$ but $x=1$ cannot be a solution to the first equation.",['algebra-precalculus']
4577650,How to understand $\ln a + \ln b = \ln(ab)$ looking at the areas defining the three quantities?,"Im reading the book: What Is Mathematics An Elementary Approach to IDEAS AND METHODS.
There is a description before the proof of $\ln a + \ln b = \ln(ab)$ Intuitively, this formula could be obtained by looking at the areas defining the three quantities lna, lnb and ln(ab). But we prefer to derive it by a reasoning typical of the calculus.... (following the proof in calculus.) I can't understand this, why the areas defining this is intuitive? thanks. Edit: I know integrals and the proof in calculus way, what I want to know is just how to understand this quote from the book.","['calculus', 'intuition', 'logarithms']"
4577651,Let $\alpha$ belong to $S_n$. Prove that $|\alpha|$ divides $n!$,"From Gallian's ""Contemporary Abstract Algebra"", Part 2 Chapter 5 It looks like using Lagrange's theorem would work, since $|S_n| = n!$ and $\langle\alpha\rangle$ is a subgroup of $S_n$ . However, that hasn't been covered in the book at this point, so I'm assuming a different solution is expected $\alpha$ can be broken up into disjoint cycles $\alpha_1\dots\alpha_m$ such that $|\alpha_1| + \dots +|\alpha_m| = n$ , and then $|\alpha| = \operatorname{lcm}(|\alpha_1|, \dots, |\alpha_n|)$ . Don't know how to continue though","['group-theory', 'abstract-algebra', 'permutation-cycles']"
4577659,Cardinality of a set including fractions,"Attempt: Let $G = \{x > 0 : \lim_{nâ†’âˆž} \mathsf{frac}((n!)x) = 0\} .$ For each $f âˆˆ 2^Ï‰$ , define $x_{f} = \sum_{n\ge1} \frac{f(n)}{n!}$ Put $W=\{x_{f} :fâˆˆ2^Ï‰\}$ . I don't know how to proceed from here.","['limits', 'cardinals']"
4577719,general solution beta $(\sin(\alpha+\beta)+\cos(\alpha +2\beta)\sin(\beta))^2 = 4\cos(\alpha)\sin(\beta)\sin(\alpha+ \beta);\tan(\alpha)=3\tan(\beta)$,We need to find general solution of $\beta$ for $$(\sin(\alpha+\beta)+\cos(\alpha +2\beta)\sin(\beta))^2 = 4\cos(\alpha)\sin(\beta)\sin(\alpha+ \beta)$$ $$\tan(\alpha)=3\tan(\beta)$$ I took the first equation and managed to simplify it up till here: $$(\cos(\beta)\sin(\alpha+2\beta))^2=4\cos(\alpha)\sin(\beta)\sin(\alpha+ \beta)$$ What should I do next? I tried to use $2\sin(\beta)\sin(\alpha+\beta)=\cos(\alpha)-\cos(\alpha+2\beta)$ in the RHS in the hopes of cancelling or doing something about the $\alpha+2\beta$ terms. But I cannot seem to boil it down further. Thanks!,"['trigonometry', 'systems-of-equations']"
4577737,Cardinality of the set of terms of the Language of Abelian Groups,"I believe the cardinality of the set of terms of the Language of Abelian Groups ( $\lbrace \mathbb{Z}, 0, +, - \rbrace$ ) is $\aleph_o$ because $\mathbb{Z}$ is countable and we're performing addition and subtraction on it, so I believe the cardinality should also be countable. Is my reasoning correct. How would you rigorously prove it?","['elementary-set-theory', 'abelian-groups', 'cardinals']"
4577757,Struggles solving : $\frac23xyy'=\sqrt{x^6-y^4}+y^2.$,"Solve $\frac23xyy'=\sqrt{x^6-y^4}+y^2.$ (Hint: the equation can be boiled down to a homogeneous one.) My attempt: I didn't know how to obtain a homogeneous equation, so I'm going to show what I did instead. We can write $\frac23xyy'=|x|^3\sqrt{1-\frac{y^4}{x^6}}+y^2.$ Let $z=\frac{y^2}{x^3}=y^2x^{-3}.$ Then $$\begin{aligned}z'&=2yy'x^{-3}-3y^2x^{-4}\\ \implies x^4z'&=2xyy'-3y^2\\ \implies\frac23xyy'&=\frac{x^4z'+3y^2}3=\frac{x^4z'}3+y^2\\ \implies\frac{x^4z'}3+y^2&=|x|^3\sqrt{1-z^2}+y^2\\ \implies\frac{|x|z'}3&=\sqrt{1-z^2}\\ \implies\frac{dz}{\sqrt{1-z^2}}&=3\frac{dx}{|x|}\\ \implies\arcsin z&=3\ln|x|+C, C\in\Bbb R\\ \implies z&=\sin\ln(C|x|^3), C>0\\ \implies y^2&=x^3\sin\ln(C|x|^3)C>0.\end{aligned}$$ I'm not sure my answer is correct. I didn't get far plugging the result into the equation.
How do we solve the given ODE? Or should I better ask, how do we obtain a homogeneous ODE from it?",['ordinary-differential-equations']
4577779,Derivative of $f(x)\cdot x$ where $f$ is a $\mathbb{R}^3$ function: how to compute it?,"Let $f:\mathbb{R}^3\to\mathbb{R}^3$ . Could someone please help me to compute the the derivative of $$f(x)\cdot x?$$ If $f$ would be a real valued function I think it should be the derivative of a product, but how to proceed in this case? Thank you in advance.","['multivariable-calculus', 'calculus', 'derivatives', 'real-analysis']"
4577788,Can't $f(x)$ be negative here?,"The following question was asked in JEE Advanced 2020. Question: Let $b$ be a nonzero real number. Suppose $f:\mathbb R\to\mathbb R$ is a differentiable function such that $f(0)=1$ . If the derivative $f'$ of $f$ satisfies the equation $$f'(x)=\frac{f(x)}{b^2+x^2},$$ for all $x\in\mathbb R$ , then which of the following statements is/are True? A) If $b\gt0$ , then $f$ is an increasing function. B) If $b\lt0$ , then $f$ is a decreasing function. C) $f(x)f(-x)=1$ for all $x\in\mathbb R$ D) $f(x)-f(-x)=0$ for all $x\in\mathbb R$ My Attempt: Positive or negative nature of $f'(x)$ is dependent on the positive or negative nature of $f(x),$ not $b$ . Solving the given differential equation, we get, $$\ln|f(x)|=\frac1{|b|}\tan^{-1}\left(\frac x{|b|}\right)$$ (Constant of integration is zero here) Therefore, $$|f(x)|=e^{\frac1{|b|}\tan^{-1}\left(\frac x{|b|}\right)}\\ \implies f(x)=\pm e^{\frac1{|b|}\tan^{-1}\left(\frac x{|b|}\right)} $$ I can see that option C) is correct. Option D) is incorrect. How to decide for option A) and B)? Official Answer is A,C.","['contest-math', 'calculus', 'functions', 'ordinary-differential-equations']"
4577800,Positive definite matrices - symmetric - diagonal elements,"a) Show that the tridiagonal matrix $$A=\begin{pmatrix} 2 & 1 & \dots & 0 \\ 1 & 2 & 1 & \vdots \\ \vdots & \vdots & \ddots & \vdots \\ 0 & \ldots & 1 & 2\end{pmatrix}$$ is positive definite. b) Show that a symmetric, positive definite matrix $A\in \mathbb{R}^{n\times n}, \ A=\{a_{k\ell}\}_{k,\ell=1}^n$ has only positive diagonal elements and that it holds that $\displaystyle{\max_{k=1,\ldots, n}| a_{kk}| = \max_{k,\ell=1,\ldots,n}| a_{k\ell}|}$ . I have done the following : a) Let $x\in \mathbb{R}^n\setminus \{\vec{0}\}$ . \begin{align*}x^TAx&=\sum_{i,k=1}^na_{ik}x_ix_k \\ & [\text{ if } i=k : a_{ii}=2 \ , \  \text{ if } k=i-1 : a_{i(i-1)}=1 \ , \  \text{ if } k=i+1 : a_{i(i+1)}=1 ]\\ & =\sum_{i=1}^nx_i x_{i-1}+2\sum_{i=1}^n x_i^2+\sum_{i=1}^n x_ix_{i+1} \\ & =x_1 x_{0}+\sum_{i=2}^nx_i x_{i-1}+2\sum_{i=1}^n x_i^2+\sum_{i=1}^{n-1} x_ix_{i+1} +x_nx_{n+1} \\ & =\sum_{i=2}^nx_i x_{i-1}+2\sum_{i=1}^n x_i^2+\sum_{i=1}^{n-1} x_ix_{i+1} \\ & = \sum_{i=1}^{n-1}x_i x_{i+1}+2\sum_{i=1}^n x_i^2+\sum_{i=1}^{n-1} x_ix_{i+1} \\ & = 2\sum_{i=1}^{n-1}x_i x_{i+1}+2\sum_{i=1}^n x_i^2 \\ & = \left (\sum_{i=1}^{n-1} x_i^2+x_n^2\right )+2\sum_{i=1}^{n-1}x_i x_{i+1}+\left (x_1^2+\sum_{i=2}^n x_i^2\right ) \\ & = \sum_{i=1}^{n-1} x_i^2+2\sum_{i=1}^{n-1}x_i x_{i+1}+\sum_{i=2}^n x_i^2+x_1^2+x_n^2 \\ & = \sum_{i=1}^{n-1} x_i^2+2\sum_{i=1}^{n-1}x_i x_{i+1}+\sum_{i=1}^{n-1} x_{i+1}^2+x_1^2+x_n^2 \\ & = \sum_{i=1}^{n-1} \left (x_i^2+2x_i x_{i+1}+ x_{i+1}^2\right )+x_1^2+x_n^2 \\ & = \sum_{i=1}^{n-1} \left (x_i+ x_{i+1}\right )^2+x_1^2+x_n^2 \\ & >0 \end{align*} Since it is a sum of positive (squares) terms. It is equal to zero only if all terms are equal to zero and we get this only when $\vec{x}=\vec{0}$ . Is that correct and complete? $$$$ At b) I need some help. We have that $A$ is symmetric, that means that $\displaystyle{a_{k\ell}=a_{\ell k}}$ , or not? We also have that $A$ is positive definite, that means that $\displaystyle{x^TAx>0 \Rightarrow \sum_{k, \ell=1}^na_{k \ell }x_kx_\ell >0}$ for $x\neq 0$ , right? How do we get that $a_{kk}>0$ ? Do we maybe take the half sum and then the other sum is the same due to symmetric property? $$$$ EDIT : For b) I have done the following : We consider that the contrary is true, i.e $\displaystyle{\max_{k,\ell=1,\ldots,n}|a_{k\ell}|=|a_{ij}|}$ with $i<j$ . Since $A$ is positive definite we have that $x^T A x >0 \quad \forall x\neq 0$ . We consider the vector $x_{ij}$ where the $i$ -th entry is equal to $1$ and the $j$ -th entry is equal to $-1$ and all other entries are equal to $0$ , where $i,j\in \{1, \ldots , n\}$ arbitrary. For this vector we get : \begin{align*}&x_{ij}^T A x_{ij} >0 \\ & \Rightarrow  \begin{pmatrix}0  & \ldots & 0 & 1 & 0 & \ldots & -1 & \ldots & 0\end{pmatrix}\begin{pmatrix}a_{11} & \ldots & a_{1n} \\ \ldots & \ldots & \ldots \\ a_{i1} & \ldots & a_{in} \\ \ldots & \ldots & \ldots \\ a_{j1} & \ldots & a_{jn} \\ \ldots & \ldots & \ldots  \\ a_{n1} & \ldots & a_{nn}\end{pmatrix}\begin{pmatrix}0  \\ \vdots \\ 0 \\ 1 \\ 0 \\ \vdots \\ -1 \\ \vdots \\ 0\end{pmatrix}>0 \\ & \Rightarrow  \begin{pmatrix}a_{i1}-a_{j1}  & \ldots  & a_{ii}-a_{ji} & \ldots & a_{ij}-a_{jj} & \ldots & a_{in}-a_{jn}\end{pmatrix}\begin{pmatrix}0  \\ \vdots \\ 0 \\ 1 \\ 0 \\ \vdots \\ -1 \\ \vdots \\ 0\end{pmatrix}>0 \\ & \Rightarrow (a_{ii}-a_{ji})-(a_{ij}-a_{jj})>0\\ & \Rightarrow a_{ii}-a_{ji}-a_{ij}+a_{jj}>0\end{align*} Since $A$ is symmetric we get that $a_{ji}=a_{ij}$ , and so we get \begin{equation*}a_{ii}-a_{ji}-a_{ij}+a_{jj}>0 \Rightarrow a_{ii}-a_{ij}-a_{ij}+a_{jj}>0\Rightarrow a_{ii}+a_{jj}>a_{ij}+a_{ij}\end{equation*} and in that way we get a contradiction. Is that correct and complete?","['matrices', 'tridiagonal-matrices', 'positive-definite', 'symmetric-matrices']"
4577821,Existence of $x\in \mathfrak m \setminus \mathfrak m^2$ such that $xR$ is a prime ideal,"Let $(R,\mathfrak m)$ be a Noetherian local domain of dimension at least $2$ .
Then, must there exist $x\in \mathfrak m \setminus \mathfrak m^2$ such that $xR$ is a prime ideal of $R$ ? What if we also assume $R$ is normal? My thoughts: If $R$ is a UFD, then every height $1$ prime ideal is principal, and in that case the problem boils down to finding a height $1$ prime ideal not contained in $\mathfrak m^2$ , can this be always done? Outside the UFD case, I have no idea. Please help.","['cohen-macaulay', 'maximal-and-prime-ideals', 'algebraic-geometry', 'local-rings', 'commutative-algebra']"
4577857,compute $\int_0^1\sqrt{x(1-x)}dx$.,"My sister has a following integral to compute $$\int_0^1\sqrt{x(1-x)}dx.$$ I know how to compute it : doing the substitution $x=\sin^2(u)$ yields $$\int_0^{\pi/2}2\sin(x)^2\cos^2(x)dx=\frac{1}{2}\int_0^{\pi/2}\sin^2(2x)dx=\frac{1}{4}\int_0^{\pi/2}(1-\cos(4x))dx=\frac{\pi}{8}.$$ So I know how to do it. However, my sister is in high school, and they never saw substitution, so it's impossible that it's what her teacher expect. Is there an other way to compute it with more elementary tools ? I don't see it...","['integration', 'real-analysis']"
4577870,Solving the differential equation: $y' = y^2 +1$,"Solving the differential equation: $y' = y^2 +1$ , given $y(0)=1$ What I did initially was to Integrate $y^2 + 1$ , then I realised that $y$ is not the variable of the integration. I then grouped $y^2 - y' = 1$ and then this equation don't make sense to me so, I went to look at the answer key. It states: Since $y^2 + 1> 1$ , then $\int \frac{1}{y^2+1} y' =1$ This way we can get a function that is in the form of $y = x$ . But I do not still understand the reason of the answer key's method. Why did they say $y^2 + 1> 1$ ? and I know it has probably has got to do something with $y(0)=1$ , but I don't know the reason. I just started to study differential equations",['ordinary-differential-equations']
4577877,Weighted Mean of 3 Measurements: Too Little Information?,"Suppose I only have the following summary data: ""City A"" has an adult population of ""n1"" : a representative sample of 25% of this adult population was asked for their income, and the average income was ""x dollars"" ""City B"" has an adult population of ""n2"" : a representative sample of 25% of this adult population was asked for their income, and the average income was ""y dollars"" ""City C"" has an adult population of ""n3"" : a representative sample of 25% of this adult population was asked for their income, and the average income was ""z dollars"" Based on only this information, I want to estimate the average income of all 3 cities and the standard deviation. Originally, I had thought that the ""Weighted Mean"" ( https://en.wikipedia.org/wiki/Weighted_arithmetic_mean ) was a good approach for this problem. That is, cities with larger populations should have more of an influence on the final estimate, and cities with smaller populations should have less of an influence on the final estimate. I could then calculate the standard deviation as well. I started looking at references to perform this calculation (e.g.Â http://seismo.berkeley.edu/~kirchner/Toolkits/Toolkit_12.pdf), and it appears that individual measurements might be required for this calculation. For example, I would need to have the income of every person interviewed in City A, City B and City C. However, I am only provided with the average income from each of these cities - and furthermore, I am not even provided with the standard deviation of these averages. In such a problem, does it still make sense to calculated the Weighted Mean on essentially three measurements? Or in such cases, is it better to refrain from calculating any statistics, seeing that any estimate generated in such a context is likely to be inherently flawed? Note: these 3 measurements I am provided with are themselves ""means"" of other measurements - I am not sure if this further complicates things. E.g. since I am not provided with the variance of each individual mean - will it still be published to calculate the variance of the weighted mean?","['statistics', 'probability']"
4577922,Proving that an integral defines a rational function without evaluating,"For $x\in\mathbb{R}$ define $$I(x):=\int_0^\infty t e^{-t} \sin(xt) dt.$$ It can be shown that $$I(x)=\frac{2 x}{\left(x^2+1\right)^2}.$$ Question: Can we use a suitable characterization of rational functions to verify that $I(x)$ defines a rational function directly on the integral expression, without evaluating ?","['integration', 'rational-functions', 'analysis']"
4577931,$\binom{54}{5}+\binom{49}{5}+\binom{44}{5}+\cdots+\binom{9}{5}$,How to calculate the sum $$\binom{54}{5}+\binom{49}{5}+\binom{44}{5}+\cdots+\binom{9}{5}$$ I wrote this as $$\sum_{r=2}^{11}\binom {5r-1}{5}$$ $$=\frac{1}{120}\sum_{r=2}^{11}(5r-1)(5r-2)(5r-3)(5r-4)(5r-5)$$ I'm stuck after this. Any help is greatly appreciated.,"['summation', 'binomial-coefficients', 'combinatorics', 'binomial-theorem']"
4577939,What is the connection between Euler's Formula and solving differences of powers?,"Euler's formula is the following: $e^{ix} = \cos(x) + i\sin(x)$ By difference of powers, I mean $a^n - b^n = 0$ specifically $x^n - 1 = 0$ When I calculate $x^3 - 1 = 0$ I get $x = (1, \frac{-1}{2} +\frac{i \sqrt3}{2},\frac{-1}{2} -\frac{i \sqrt3}{2})$ This is similar to cos and sin relating to these numbers. Specifically $-\frac{1}{2} + \frac{i\sqrt{3}}{2} = \cos(\frac23\pi) + i\sin(\frac23\pi) = e^{\frac23\pi i}$ . My question is ""Why do these relate?""",['trigonometry']
4577945,How to solve a Fredholm equation with known $\lambda$?,"I have the Fredholm equation, $$\phi(x)=\sin x+\lambda\int_0^\pi\cos(x/2-3y)\phi(y)dy$$ and would like to solve it. First, I found  using the precondition for contraction of the Fredholm operator: $$|\lambda|<\frac{1}{M(b-a)}$$ That $|\lambda|<\sqrt{2}$ to give a contraction. So I insert 1 as a value for $\lambda$ and get: $$\phi(x)=\sin x+\int_0^\pi\cos(x/2-3y)\phi(y)dy$$ I solved this numerically using Mathematica. The solution is $\phi(x)=\sin(x)$ . However, how can I solve this ""by hand""? Thanks","['integral-equations', 'ordinary-differential-equations']"
4577984,"Precise examples for the solvable quintic with the ""most complexity"" of roots","We know that, the ""greater"" the degree of a polynomial equation, the greater the ""complexity"" of the roots in general. For example, the overall complexity of the roots of a general cubic equation is obviously more complicated than the general quadratic. However, the overall complexity of the roots of a general quartic equation is ""greater"" than any of them.  We understand this when we look at the general formulas. Motivation: I'm wondering how much ""more"" the complexity of the roots of a solvable quintic equation can be than the general quartic. But, here I would like to observe this ""event"" on the precise example quintic, rather than just Galois theory. For example, I would like to ""compare"" a solvable ""most complex"" quintic with the complexity of the roots of the equation $$2x^4+5x^3+7x^2+13x+3=0:$$ Then, I found the this solvable quintic: $$x^5-5x+12=0$$ and the exact real root equals to: However, I do not have information that this is included in the examples I am looking for. In short, I am looking for a precise example for the solvable quintic whose roots are the most complicated. Of course, the example this type of quintic itself can be interesting.  But, I would like to see its roots expressed in radicals. At least I know that the equation I'm looking for will be irreducible.","['irreducible-polynomials', 'radicals', 'galois-theory', 'quintics', 'algebra-precalculus']"
4578022,How to solve this problem with Linear Algebra?,"I have a $N \times N$ grid, each cell initialized with an integer between $0$ and $5$ .
If I select one cell, each adjacent cell (8 cells if not on an edge or a corner) - but not the selected one- are incremented by $1$ then taken $\text{mod } 5$ .
I need to find the cells to select to get from the initial state to a given targeted state.
I assume there is always a solution but I don't know if it is unique. I have done this heuristically, but my implementation is very inefficient, and I wondered if linear algebra could be used here. In case it can be done with LA, any leads to help me formulate the problems would be very appreciated. Thank you in advance for your help","['recreational-mathematics', 'linear-algebra', 'algorithms']"
4578063,The derivative of a multivariable function evaluated at $0$,"I am currently looking at the problem: For a function $f \in C^1( \mathbb{B}, \mathbb{R}^k)$ where there exists some positive value $\beta$ such that $\lvert Df(0)h \rvert \ge \beta \lvert h \rvert $ for all $h \in \mathbb{R}^n$ Now define $P: \mathbb{B} \rightarrow \mathbb{R}^k$ where $P(x) = f(x) - Df(0)x$ and calculate $DP(0)$ . I believe that the best way to find $DP(0)$ is to consider $P(x+h) - P(x)$ and look for terms that are linear in $h$ and evaluate this at $x=0$ . This gives us $$P(x+h) - P(x) = \big{(} f(x+h) - Df(0)(x+h) - f(x) + Df(0)x \big{)} = \big{(} f(x+h) - Df(0)h - f(x) \big{)}$$ Here we, see that $Df(0)h$ is linear in $h$ , however, it is unclear to me whether or not $f(x+h)$ will also have linear terms and so this approach seems to be indeterminate to me. I would be grateful for any guidance.","['analysis', 'real-analysis', 'multivariable-calculus', 'calculus', 'derivatives']"
4578070,why does the axiom of pair has 2 different claims on Wiki?,"I was reading ZFC Set theory on wiki, and here is the definition of the axiom of pair on its page: If $x$ and $y$ are sets, then there exists a set which contains $x$ and $y$ as elements. $$\forall x\forall y\exists z\Bigl( (x\in z)\wedge (y\in z)\Bigr).$$ Which only states that both $x$ and $y$ are elements of some $z$ .
But when I look into the ordered pair axiom page of the wiki
(or from Herbert Enderton's Elements of Set Theory ), it states the following: For any $A$ and $B$ sets, there is a set $C$ such that $$(x\in C)\iff (x=A\vee x=B).$$ In my understanding, this means there exists set $C$ that any set $A$ or $B$ would be an element of $C$ . So it seems like the axiom of order pair has stated 2 different things: the first statement must have 2 of any element belonging to some set $z$ . but the second statement says that is ok that only 1 of any element belongs to some set $C$ . Therefore I'm having a hard time understanding which is more correct. Thank you!","['elementary-set-theory', 'axioms', 'set-theory']"
4578083,Why do all norms in $\Bbb R^n$ generate the same open sets?,"The theorem states that all norms in $\Bbb R^n$ generate the same open sets. How can this be the case when we are talking about norms in terms of a distance/length? Moreover, what do we mean here with ""generate the same open sets""? How would we go about to prove this theorem? I have just started taking Advanced Math courses and thus I am a beginner. I would appreciate any help on the idea behind this theorem.","['general-topology', 'normed-spaces', 'vector-spaces']"
4578085,Is there a way of finding the turning point of a quadratic without calculus and without completing the square?,"Using completing the square it's trivial to find the minimum of a polynomial such as $x^2+9x+5$ . We rearrange to get $(x+\frac{9}{2})^2-\frac{61}{4}$ and can see that when $x=-\frac{9}{2}$ we have a minimum. Using calculus it's also trivial. The derivative of $x^2+9x+5$ with respect to $x$ is $2x+9$ , and the value of $x$ that makes $2x+9=0$ is $-\frac{9}{2}$ . My question is: are there any unusual methods to find the minimum of a quadratic (or, if possible, higher order polynomial)? Something not rooted in algebraic manipulation like completing the square or calculus?","['calculus', 'graphing-functions', 'algebra-precalculus']"
4578147,Adding edges and vertices to graph until it becomes regular,"I am reading of Lovasz's theorem, which asserts that every graph of maximal vertex degree $s+t-1$ can be partitioned in two graphs of maximal vertex degree $s$ and $t$ respectively. The following fact is used in the proof: we can add edges and vertices to our graph until it becomes regular. I fail to understand, why this is true.","['graph-theory', 'discrete-mathematics']"
4578184,Evaluate $\lim\limits_{n \to \infty} \sqrt[n^2]{1! + 2! + 3! + \dots + n!}$,"$$\lim_{n \to \infty} \sqrt[n^2]{1! + 2! + 3! + \dots + n!}$$ I can try at least to evaluate it from bottom: $$1 < \sqrt[n^2]{n^2} < \sqrt[n^2]{n!} < \sqrt[n^2]{1! + 2! + \dots + n!}$$ Well, it doesn't say anything. Ok, gonna try evaluation from top. $$\sqrt[n^2]{1! + 2! + \cdots + n!} < \sqrt[n^2]{n \times n!}$$ More than that, $$\lim_{n \to \infty} \frac{n!}{1! + 2! + \cdots + n!} = 1$$ but this equation is always less than one.  So instead of writing $\sqrt[n^2]{n \times n!}$ I can try fixed coefficient $1 < a < n$ . It will work for $a = 2$ , $a =1.5$ , but I'm not sure though about $a = (1 + \frac{1}{n})$ $$\sqrt[n^2]{1! + 2! + \cdots + n!} < \sqrt[n^2]{a \times n!}$$ Just to get broad idea how it looks I'll assume that $a = 1$ and I'll try to use Stirling's approximation $$\sqrt[n^2]{1 \times n!} \approx \left((2\pi n)^{\frac{1}{2}}\left(\frac{n}{e}\right)^n \right)^\frac{1}{n^2} = (2 \pi n)^{\frac{1}{2n^2}} \times \left(\frac{n}{e}\right)^{\frac{1}{n}}$$ $\lim_{n \to \infty} \sqrt[n]{n} = 1$ , $2n^2 > 2 \pi n$ , $\frac{n}{e} < n$ (for big numbers) mean, that $$\lim_{n \to \infty}(2 \pi n)^{\frac{1}{2n^2}} \times (\frac{n}{e})^{\frac{1}{n}} \approx 1$$ So I can guess, that $$\lim_{n \to \infty} \sqrt[n^2]{1! + 2! + 3! + \cdots + n!}  = 1$$ But I sincerely doubt that what I wrote here is barely a solution. Would you mind helping me? Hope I didn't make huge mistakes","['limits', 'calculus', 'real-analysis']"
4578185,Weak formulation for heat and wave equation,"Consider the IVP for heat equation given by $$
\begin{cases}
u_t-\Delta u =0 & (x,t) \in \mathbb{R}^d \times (0,\infty)\\
u(x,0)=u_0(x) &  x\in \mathbb{R}^d
\end{cases}
$$ We can define the weak formulations in two ways: The standard weak formulation of the above IVP (see for example the book Partial Differential Equations by L.C. Evans) is to look for $u\in L^2(\mathbb{R}^+;H^1(\mathbb{R}^d))$ with $u'\in L^2(\mathbb{R}^+,H^{-1}(\mathbb{R}^d))$ satisfying \begin{eqnarray}
\langle u',v\rangle - \int\limits_{\mathbb{R}^d}  \sum\limits_{i=1}^du_{x_i}v_{x_i} dx = 0. \qquad \text{ for all }v\in H^1_0(\mathbb{R}^d) \label{1}\tag{1} 
\end{eqnarray} On the other hand, one can also define the weak formulation as $u\in C([0,\infty);L^2(\mathbb{R}^d))$ satisfying \begin{eqnarray}
\int\limits_{\mathbb{R}^d\times \mathbb{R}^+}(u \phi_t +u \Delta \phi)dx dt=0 \qquad \text{for all } \phi \in C_c^{\infty}(\mathbb{R}^d \times \mathbb{R}^+) \label{2}\tag{2}
\end{eqnarray} Why the formulation \eqref{1} is preferred over \eqref{2}? The same question is applicable for the IVPs for wave equations as well. P.S.: I understand that \eqref{2} cannot be used for initial boundary value problems (IBVP) as the trace of $L^2$ functions do not exist in general. However, for IVPs we do not ned the existence of such traces traces so \eqref{2} seems more natural. Moreover, since the PDE is Parabolic, anyways solutions can be shown to be sufficiently smooth.","['functional-analysis', 'analysis', 'partial-differential-equations']"
4578211,Find the measure of angle $AMN=\theta$,"Let $ABC$ be an isosceles triangle where $AB = AC$ , so that $\angle BAC = 20Â°$ . Also let $M$ be the projection of the point $C$ on the side $AB$ and $N$ a point on the side $AC$ , so that $2CN = BC$ . The measure of angle $AMN$ is equal to?(A: $60^o$ ) Follow my progress..
I made the drawing and marked all the angles I could find but still need to find the path...maybe an additional segment","['euclidean-geometry', 'geometry', 'plane-geometry']"
4578243,How to lower bound $\tau$ based on the expression of $H$?,"Let $A=\{a_{ij}\}_{1\le i,j\le n}$ be an $n$ by $n$ normalized symmetric Gaussian random matrix with $E[a_{ij}]=0$ and $E[a_{ij}^2]=1/n$ . Ordering its eigenvalues by $\lambda_1\le \lambda_2\le \cdots \lambda_n$ with corresponding eigenvectors $v_1,\dots, v_n \in \mathbb{R}^n$ . Let $u_0$ be a vector on $\mathbb{R}^n$ uniformly distributed on the unit sphere. (We also know that $v_i$ is uniformly distributed on the unit sphere for $i=1,\dots, n$ .) Define $H_j(t)=u_t\cdot v_j$ for $j=1,\dots, n$ and time $t\ge 0$ , solving the following ODE with initial value $H_j(0)$ : $$
\frac{1}{2}H_j'(t)=\sum_{i=1}^n[(\lambda_i-\lambda_j)H_i^2(t)]H_j(t)
$$","['statistics', 'analysis', 'probability', 'inequality']"
4578252,Approximate $\int_0^\pi e^{e^x} dx$,"On the MIT $2021$ Integration Bee Qualifying Exam , it asked to approximate $$
\int_{0}^{\pi}{\rm e}^{{\rm e}^{x}}\,{\rm d}x
$$ I got $\displaystyle{\rm e} + {\rm e}^{\rm e} + {\rm e}^{{\rm e}^{2}} + {\rm e}^{{\rm e}^{3}}$ , which is about $4\ \%$ off the exact answer. How do I make a better approximation ( given the fact that you have graphing paper and no calculators allowed ) $?$ .","['integration', 'calculus', 'definite-integrals', 'approximation']"
4578282,"Which 3D solids have volume proportional to their surface area, if any?","This question isn't related to my math classes since I'm not taking any geometry, but I came up with the question of whether there's any solid where $V/S$ is a constant. If there was one, you could shrink or expand the solid and the ratio wouldn't change. The only progress I've made is deciding that the solid can't be a prism. If $A$ is the base area, $P$ is the perimeter of the base, and $h$ is the height of the prism, then $$V=Ah$$ $$S=2A+Ph$$ Dividing $V$ by $S$ and solving for $h$ gives you $$\frac{V}{S}=\frac{Ah}{2A+Ph}=c$$ $$Ah=c(2A+Ph)$$ $$Ah-Pch=2Ac$$ $$h=\frac{2Ac}{A-Pc}$$ So if I have a prism and change its height without changing the base, the ratio $c$ would have to change. I'm guessing such a solid doesn't exist, because I feel like the two formulas always involve different powers (like $s^2$ and $s^3$ for a square), but I'm wondering if there's a way to prove it's impossible or find a solid that does satisfy the condition. Edit: A comment pointed out that it might make more sense to assume the shape only changes proportionally, so every dimension increases by the same factor. Answers that use this assumption would also be appreciated.",['geometry']
4578294,Prove that the generalized eigenvalue equation in normal mode analysis has positive eigenvalues,"Context I am studying normal modes oscillations and normal modes [1,2].
I am trying to complete a proof to show that the eigenvalues (i.e., the eigenvalues are positive). There is a proof in [2], but I find it verbose and likely incorrect. There is a short proof of this in [4]. Nonetheless, I proceed with my questions on the topic. Questions Question 1 In my understanding an eigenvalue equation [3], I write the following definition. Definition
If $T$ is a linear transformation from a finite-dimensional vector space $\mathbb{C}^n$ over the field of complex number into itself and $\mathbf{v}$ is a  nonzero  vector in $\mathbb{C}^n$ , then $\mathbf{v}$ is an eigenvector of $T$ if $T\,\mathbf{v}$ is a scalar multiple of $\mathbf{v}$ . This can be written as the eigenvalue equation $$T(\mathbf{v}) = \lambda \mathbf{v},$$ where $\lambda$ is a scalar in $\mathbb{C}$ , known as the eigenvalue associated with $\mathbf{v}$ . However, in the analysis of normal mode of dynamical systems, I have $n$ -dimensional real symmetric matrices $\mathbf{V}$ and $\mathbf{T} $ , and the "" eigenvalue equation "" $$\mathbf{V}\,\mathbf{a} = \lambda\,\mathbf{T}\,\mathbf{a}.\tag{1}$$ In my mind, if $T$ is invertible, I can make an actual eigenvalue equation $$\left( \mathbf{V}\,\mathbf{T}^{-1}\right)\left(\mathbf{T}\,\mathbf{a}\right) = \lambda\,\left(\mathbf{T}\,\mathbf{a}\right).\tag{2}$$ Now, I would have that $\mathbf{T}\mathbf{a}$ is an eigenvector of $\mathbf{V}\mathbf{T}^{-1}$ since $\mathbf{V} \mathbf{a}$ is a scalar multiple of $\mathbf{T}\mathbf{a}$ . So my first series of questions: (1) Can we agree that, as written, Equation 1 is not actually an eigenvalue equation? (2) Does this type of equation have a name? If so what is it? Question 2 In [2] there is a claim that $\lambda$ is always finite and positive. It comes down to faith. I really like [2], but when it comes to such matters as the proofs of mathematical claim, I just have no faith in [2]. For example, there is even an example later in the book when one of the eigenvalues is zero valued. (3) How can we prove or disprove the following propositions? [Please note that both propositions cannot be proven to be true.  An amended proposition is given and proven in OP's answer below.] Proposition. Given two $n$ dimensional real symmetric matrices $\mathbf{T}$ and $\mathbf{V}$ show that any solution of the equation $$\mathbf{V}\,\mathbf{a} = \lambda\,\mathbf{T}\,\mathbf{a}$$ has a scalar lambda that is positive. Proposition. Given two $n$ dimensional real symmetric matrices $\mathbf{T}$ and $\mathbf{V}$ show that any solution of the equation $$\mathbf{V}\,\mathbf{a} = \lambda\,\mathbf{T}\,\mathbf{a}$$ has a scalar lambda that is non-negative. These two propositions are closely related. Thus, I assume that some other predicates need to be set in order to distinguish the proofs. Bibliography [1] https://en.wikipedia.org/wiki/Normal_mode [2] Goldstein, ""Classical Mechanics,"" 3rd edition, page 242-3. [3] https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Formal_definition [4] Prove that the eigenvalues for $A\vec{w} = \lambda B\vec{w}$ are real and positive","['matrices', 'physics', 'linear-algebra', 'positive-definite']"
4578350,Proof about block matrices,"During a test today I had this question: Given $$ M = \begin{pmatrix} A & C \\ 0 & B\\ \end{pmatrix}$$ where $A$ and $B$ are $n \times n$ diagonalizables matrices without eigenvalues in common, prove that $M$ is diagonalizable. No information about $C$ was given. First, I tried $$\det(M - \lambda I) = \det(A - \lambda I)\cdot \det(B - \lambda I)$$ So $$p_M(\lambda)=p_A(\lambda)\cdot p_B(\lambda)$$ So the set of eigenvalues of $M$ is the union of the eigenvalues of $A$ and $B$ (given they don't have any in common). My next step was to do $$ M^k = \begin{pmatrix}
     A^k & C'\\ 
     0 & B^k\\ \end{pmatrix}$$ so if $m_M(x)$ is the minimal polynomial of $M$ , we have that $m_A(x)|m_M(x)$ and $m_B(x)|m_M(x)$ . But how can I conclude that hence the minimal polynomial of M will have just linear factors? I know that $m_M(x) = m_A(x)\cdot m_B(x)\cdot Q(x)$ , but how can I show that $Q(x) = 1$ ? If $m_M(x) = m_A(x)\cdot m_B(x)$ itÂ´s clear that M is diagonalizable, but I canÂ´t see how to prove this. In fact, in the end my approach was the same of the first answer, but after I still looking to a solution using the minimal polynomial.","['eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'diagonalization', 'block-matrices']"
4578407,Durret 2.5.8: Question about probability of limsup,"I am interested in this problem by Durret in Probability: Theory and Examples . Let $X_1,X_2, \dots$ be i.i.d. and not $\equiv0$ . If $E|X_1| < \infty$ , show that $$\limsup_{n \to \infty} |X_n|^{1/n} = 1$$ almost surely. One idea I had, was trying to show that $P(\limsup |X_n|^{1/n} > 1) = 0$ and $P(\limsup |X_n|^{1/n} < 1) = 0$ . This would give us the result. Now for the first one, we have $$P(\limsup |X_n|^{1/n} > 1) \leq P(|X_n|^{1/n} > 1 \text{ i.o.}).$$ I want to show that the last probability above is $0$ maybe by Borel-Cantelli and using $E|X_1| < \infty$ if and only if $\sum_nP(|X_1| \geq n) < \infty$ . I am not sure if this is the right directon. Please help.","['probability-theory', 'probability']"
4578466,An inequality regarding $L^2$ and $H^{-1}$ norms,"Necas theorem gives that $$\|p\|_{L^2(\Omega)}\lesssim\|p\|_{H^{-1}(\Omega)}+\sum_{i=1}^n\|\frac{\partial p}{\partial x_i}\|_{H^{-1}(\Omega)}\tag{$*$},$$ where $\Omega$ is connected Lipschitz domain and $p\in L^2(\Omega)$ . Now I need to prove that $(*)$ is equavalent to $$\|p\|_{L^2(\Omega)}\lesssim\sum_{i=1}^n\|\frac{\partial p}{\partial x_i}\|_{H^{-1}(\Omega)},\tag{$**$}$$ where $p\in L_0^2(\Omega)=\{p\in L^2(\Omega):\int_{\Omega}p=0\}$ . I already know how to prove $(*)\Rightarrow(**)$ . In order to prove $(**)\Rightarrow(*)$ , I think $p\in L^2$ can be decomposed to $p=\bar{p} + p_0$ , where $\bar{p}=\frac{1}{|\Omega|}\int_{\Omega}p$ and $p_0=p-\bar{p} \in L_0^2(\Omega)$ . Hence, we only need to prove that $\|\bar{p}\|_{L^2}\lesssim\|p\|_{H^{-1}}\quad(***)$ . $\|p\|_{H^{-1}}=sup\frac{\int_\Omega pv}{\|v\|_{H^1}}$ where the supremum is taken for all $v\in H_0^1$ . If I could take v as a constant, I could have easily proved $(***)$ . I wonder if the inequality $(***)$ holds, and how to prove it if it holds. Maybe I can take $v \in H_0^1$ which is a constant in a large area?","['sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
4578501,Applying Chebyshev's inequality to a sequence of independent random variables,"Let $X_1, \dots, X_n$ be independent random variables. Assume that $E[X_k] = 0$ and $Ïƒ^2_k = E[X^2_k] < \infty$ for each $k$ . I want to show that for all $\epsilon > 0$ , $$
P( \max_k |S_k| \ge \epsilon) â‰¤ \frac{1}{\epsilon^2} \sum^n_{k=1} Ïƒ^2_k
$$ where $S_k = X_1 + X_2 + \dots + X_k$ . My thoughts: by Chebyshev, $P(|X_k| \ge \epsilon) \le \frac{1}{\epsilon^2} \sigma_k^2$ for all $k$ . Taking sum over $K$ , we have $$
\sum_{k=1}^n P(|X_k| \ge \epsilon) \le \frac{1}{\epsilon^2} \sum^n_{k=1} Ïƒ^2_k
$$ But I am having difficulty connecting $P(|X_k| \ge \epsilon)$ to $P( \max_k |S_k| \ge \epsilon)$ . Since the $X_k$ 's are independent, I know $
\sum_{k=1}^n P(|X_k| \ge \epsilon) = P(\max_k |X_k| \ge \epsilon)$ . But I don't know how to proceed next. Can someone give me a hint? Thanks in advance!","['independence', 'probability-theory', 'probability']"
4578519,Show that the identity $\int_0^\infty\sin(t)t^{z-1}\ dt = \Gamma(z)\sin\left(\pi{z\over 2}\right)$ holds on $-1<\operatorname{Re}(z)<1$,"The question is from Stein complex analysis 6.10(b) (b) Show that the following identity $$\int_0^\infty\sin(t)t^{z-1}\ dt = \Gamma(z)\sin\left(\pi{z\over 2}\right)\quad 0<\operatorname{Re}z<1$$ is valid in the larger strip $-1<\operatorname{Re}z<1$ . Since $\Gamma$ has a meromorphic continuation on $\Bbb C$ with simple poles at $-\Bbb N\cup\{0\}$ and $\sin$ has a simple zero at $0$ , I can conclude that the integral $$\int_0^\infty\sin(t)t^{z-1}\ dt$$ has an analytic continuation on $-1<\operatorname{Re}z<1$ since the RHS of the above identity does. But this does not show the identity holds on $-1<\operatorname{Re}z<1$ right? I think I need to show the LHS is also holomorphic on $-1<\operatorname{Re}z<1$ then by identity theorem I can say the identity holds. Why the integral holomorphic on $-1<\operatorname{Re}z<1$ ?","['complex-analysis', 'analytic-continuation', 'gamma-function']"
4578590,"Is $\{a, b\}$ a subset of $\big\{\{a, b\}\big\}\;?$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I read a lot of questions like that here, but it takes me more confused. In many books appear examples like: $\textbf{A} = \{a, b, c\}$ , so: $\{a\}\subset\textbf{A}$ , $\{b, c\}\subset\textbf{A}$ etc. But here I read things like: If $\;\textbf{B}=\big\{a,b,\{c, d\}\big\}$ , so $\textbf{B}$ has $3$ members: $a\in\textbf{B}$ , $b\in\textbf{B}$ , $\{c, d\}\in\textbf{B}$ , but : $c\not\in\textbf{B}$ and $d\not\in\textbf{B}$ . It made me more confused because: If there is $x\in\textbf{P}$ and $\textbf{P}\subset\textbf{Q}$ , so $x\in\textbf{Q}$ . E.g.: $1\in\mathbb Z$ and $\mathbb Z\subset\mathbb R$ , so $1\in\mathbb R$ . In this case: $\{c, d\}\subset\textbf{B}$ ? $\big\{\{c, d\}\big\}\subset\textbf{B}$ ? $\{c\}\subset\textbf{B}$ ? $\big\{\{c\}\big\}\subset\textbf{B}$ ?",['elementary-set-theory']
4578609,Levi-Civita connection acting on tensor density,"Consider an arbitrary $d$ -dimensional (pseudo)-Riemannian manifold $(\mathcal{M},g)$ with Levi-Civita connection and some covariant $2$ -tensor density $T$ . More precisely, we consider $T$ to be given by $$T=T^{\prime}\otimes\mathrm{vol}_{g}\in\Gamma^{\infty}(T^{\ast}\mathcal{M}^{\otimes 2}\otimes{\bigwedge}^{d}T^{\ast}\mathcal{M})\cong\Gamma^{\infty}(T^{\ast}\mathcal{M}^{\otimes 2})\otimes_{C^{\infty}(\mathcal{M})}\Omega^{d}(\mathcal{M}),$$ where $\mathrm{vol}_{g}$ denotes the volume $d$ -form of $(\mathcal{M},g)$ and $\Omega^{d}(\mathcal{M})$ denotes the space of top-degree differential forms on $\mathcal{M}$ . In other words, we consider a density $T$ with tensor part $T^{\prime}$ and density part $\mathrm{vol}_{g}$ . Now, I try to figure out what $\nabla_{X}T$ is for some vector field $X$ . Using the Leibniz rule, this can be written as $$\nabla_{X}T=(\nabla_{X}T^{\prime})\otimes\mathrm{vol}_{g}+T^{\prime}\otimes\nabla_{X}\mathrm{vol}_{g}.$$ Maybe its trivial, but is there are general way to compute $\nabla_{X}\mathrm{vol}_{g}$ ? By definition of the Levi-Civita connection, $\nabla g=0$ . Does this imply $\nabla_{X}\mathrm{vol}_{g}=0$ ? I am not sure, since there is the square root of $g$ appearing ing $\mathrm{vol}_{g}$ ...","['riemannian-geometry', 'connections', 'tensors', 'differential-forms', 'differential-geometry']"
4578621,Vector fields on a sphere: equivalence of two definitions,"I'm trying to solve this differential geometry exercise: Show that a vector field on a $n$ -sphere and a smooth map $\Phi: S^n\to \mathbb{R}^{n+1}$ such that $\Phi(x)$ is always orthogonal to $x$ are essentially the same object. (The $n$ -sphere is taken with its standard differentiable structure, given by the two stereographic charts $U,V$ ) I think that this exercise requires us to find a bijection between the set of vector fields on $S^n$ : $$\text{Der}(C^\infty (S^n))$$ and the set of smooth maps that satisfy that orthogonality property. I read a solution that went like this. On a stereographic chart, a vector field $X$ looks like this: $$X|_U=\sum_i \varphi_i \frac{\partial}{\partial x_i}$$ Evaluating this vector field on the function $\sum_{i=1}^n x_i^2$ , we get: $$\sum_{i=1}^n \varphi_i(x_1,...,x_n)x_i=0$$ So we just need to take $\Phi=(\varphi_1,...,\varphi_n)$ . But this doesn't seem rigorous. The functions $\varphi_i$ are defined only on $U$ and not on the whole sphere, so we need to account in someway for the missing point.","['vector-fields', 'differential-geometry']"
4578624,How to calculate the intersection point of cedar shakes which share the same angle?,"I'm sure the post title will be somewhat confusing so I'll try to explain my situation. I'm currently trying to draw, in CAD software, a cross-section through roof structure and I have encountered a geometrical issue which I can't quite work out how to solve. The roof finish is Oak or Cedar Shakes i.e. hand-cut timber rectangles with differing thickness at either end. The size I am using for a single Shake is shown below: Single Shake example My problem is relatively simple; as can been seen in the below image, each shake is 125mm apart from its neighbour, and both shakes are then rotated CCW by an unknown angle Î¸ about the pivot points indicated by the red dots. Eventually, when Î¸ is large enough, the opposing point on the bottom surface of Shake A (indicated by a green dot), will intersect with the sloping top surface of Shake B. The reason I am here is to seek advice how to calculate that intersection point? Shake arrangement problem n.b. I can manually determine an approximate intersection by process of trial and error applying rotational increments of increasing accuracy, but I would hope that there is some geometrical or computational method to calculate the exact intersection point.","['angle', 'geometry']"
4578701,Well-posedness for ODEs with discontinuous right hand side,"Consider the IVP for ODE with discontinuous right hand side, \begin{eqnarray}
\dot{y}&=&f(y,y) \quad \quad \text{for }t>0,\\
y(0)&=&y_0,
\end{eqnarray} where, $f\in L^{\infty}(\mathbb{R} \times\mathbb{R})$ and $\sup\limits_{x\in \mathbb{R}} {|f(x,\cdot)|_{Lip}< \infty}.$ A natural candidate for the weak solutions is \begin{eqnarray}
y(t)=y_0+\int\limits_0^t f(y(s),y(s))ds.
\end{eqnarray} Clearly $y\in C([0,\infty))$ and satisfies the ODE in the sense of distribution.
Is this weak formulation for IVP well-posed? Do we need additional regularity assumptions like $F(\cdot,c)$ piecewise constant to prove the uniqueness? P.S. : A typical example of  of ODE with discontinuous right hand side can be $f(y,y)=\lfloor y \rfloor y$ where $\lfloor \cdot \rfloor$ denotes the greatest integer function. I understand that the problem in general may admit multiple weak solutions. For example the initial value problem $\dot{y}=\frac{1}{2y}$ and $y(0)=0$ admits multiple soltions; namely $y=0$ and $y(t)=\sqrt{t}$ .","['measure-theory', 'analysis', 'ordinary-differential-equations']"
4578703,If Any continuous map from $A$ into $\mathbf{R}$ may be extended to a continuous map of all of $X$ into $\mathbf{R}$. Then $A$ is closed subset.,"TRUE/ FALSE:  Let $X$ be a normal space and $A$ be a subspace of $X$ . If Any continuous map from $A$ into $\mathbf{R}$ may be extended to a continuous map of all of $X$ into $\mathbf{R}$ . Then $A$ is closed subset. My Attempt: I think if $X = \mathbf{R}^n$ or any norm linear space, then it is true. But what about arbitrary metric space or topological space.","['general-topology', 'metric-spaces', 'separation-axioms']"
4578722,Most interesting exercises about the implicit and inverse function theorems,"I am a TA in a multivariable calculus course this semester. Right now I am writing the exercise session which deals with the implicit function theorem, inverse function theorem and open function theorem (i.e. submersions are open maps). I am looking for your favorite/most interesting exercises which I can use/adapt. I especially like exercises which combine application of the theorems with some geometrical problem, but I am open to anything interesting. For instance, I give one exercise which deals with computing the tangent to the intersection curve of two surfaces (which requires computing the derivatives implicitly). Thanks in advance!","['inverse-function-theorem', 'multivariable-calculus', 'implicit-function-theorem', 'education', 'soft-question']"
4578800,"Show that $g(x,y)\geq 0$ for all $x,y\in\mathbb R$","Let $f\in C^2(\mathbb R)$ $$
g(x,y)=\frac{1}{2}(y-hf'(x))^2+f(x+hy)-(\frac{1}{2}y^2+f(x))
$$ for any $h>0$ .
Show that $g(x,y)\geq 0$ for all $(x,y)\in\mathbb R^2$ . I dont know how to show this. I already know that $$
\frac{1}{2}(y-hf'(x))^2+f(x+hy)\geq 0.
$$ Now I don't know how to go on and where to use the convexity of $f$ . Thanks for help!","['functions', 'convex-analysis', 'real-analysis']"
4578817,Is every function $f\colon\mathbb N\to\mathbb R$ Lebesgue-integrable?,"I believe this is the case as the natural numbers have Lebesgue-measure $0$ , so the Lebesgue-integral of any function $f\colon\mathbb N\to\mathbb R$ is also $0$ . Is that correct?","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'analysis', 'real-analysis']"
4578819,A Cantor-type set,"Let $A=\{\sum_{k=1}^\infty \frac{a_k}{k!} : a_k\in\{0,\,1\}\}$ . We can prove that $A$ is a closed set with $\operatorname{int}(A)=\emptyset$ and Lebesgue measure of $0$ . Is there a $m \in \mathbb{N}$ s.t. $A+A+\cdots+A$ ( $m$ times) is of positive Lebesgue measure? (Note that for the Cantor set $C$ we have $C+C=[0,\,2]$ .)",['measure-theory']
4578836,How do I solve this comparing of coefficient in this differential equation?,"Solve $y'' + 2y' = \cos \pi x$ Homogeneous equation: $y= C_1 + C_2e^{-2x}$ Particular solution: $y_p = A \cos \pi x + B\sin \pi x$ $y'=-A \pi \sin \pi x + B\pi \cos \pi x$ $y'' = -A \pi^2 \cos \pi x - B\pi^2 \sin \pi x$ Substituting back into the equation, $-A \pi^2 \cos \pi x - B\pi^2\sin \pi x + 2(-A\pi\sin \pi x + B\pi \cos \pi x) = \cos \pi x$ $\cos \pi x (-A\pi^2 + 2B\pi) - \sin\pi x(B\pi^2 + 2A\pi)= \cos \pi x$ So, $ (-A\pi^2 + 2B\pi) =1 $ $(-B\pi^2 - 2A\pi) =0$ How do I solve this to get $y_p = \frac{1}{(4+\pi^2)\pi} (-\pi \cos \pi x + 2 \sin \pi x)$",['ordinary-differential-equations']
4578845,"Is there another ""wave"" function that isn't a combination of sines?","I have been wondering about the reasons trigonometric functions are the ones which we typically use to describe waves. I thought of the following conditions that we would want a wave function $f: \mathbb{R} \to \mathbb{R}$ to satisfy: $f(x)$ is continuously differentiable over $\mathbb{R}$ . $f(x)$ is periodic, i.e, for some $p\in \mathbb{R}$ , $f(x+p)=f(x)$ is true for all $x\in \mathbb{R}$ . $f(x)$ is symmetrical around its extremums, meaning that if $s\in \mathbb{R}$ maximizes or minimizes $f(x)$ , then $f(x-s)=f(s-x)$ for all $x\in \mathbb{R}$ . $f(x)$ oscillates above and below the $x$ -axis in a symmetrical way. Meaning, given $f(x)$ has a period $p$ , then $f(x+\frac{p}{2})=-f(x)$ . I am not sure if these are good conditions, but my question is as follows: Do we know about functions other than $\sin (x)$ and any finite combinations of powers or linear combinations of functions $\sin (tx)$ that can satisfy these conditions?
For example, $3\cos(2x) = 3\sqrt{1-\sin^2(2x)}$ isn't a good function. I appreciate any answers or help.","['continuity', 'trigonometry', 'functions']"
4578852,"Fourier spectrum of $\sqrt{1 + f(x)} - \sqrt{1- f(x)}$ where $f(x): \{0, 1\}^n \to [-1,1]$","Consider a function $f(x): \{0, 1\}^{n} \to [-1,1]$ defined over the Boolean cube $\{0,1\}^n$ mapping to the interval $[-1,1]$ . The Fourier transform, or more precisely Walsh-Hadamard transform, of $f(x)$ is given by $$\hat{f}(s) = \frac{1}{2^n} \sum_{x\in \{0,1\}^n} (-1)^{s\cdot x} f(x)$$ Now, consider the function $g(x)=\sqrt{1 + f(x)} - \sqrt{1- f(x)}$ where $g(x):\{0,1\}^n \to [-\sqrt{2}, \sqrt{2}]$ and its Fourier transform $$\hat{g}(s) = \frac{1}{2^n} \sum_{x\in \{0,1\}^n} (-1)^{s\cdot x} \left(\sqrt{1 + f(x)} - \sqrt{1- f(x)}\right)$$ I want to understand if (or under which conditions) the spectra $\hat{f}(s) $ and $\hat{g}(s) $ are related. In particular, I would like to know if the ordering in terms of absolute value is the same for $f$ and $g$ so that $$|\hat{f}(s)| \geq |\hat{f}(s')| \Rightarrow |\hat{g}(s)| \geq |\hat{g}(s')|$$ for all pairs $s,s' \in \{0,1\}^n$ or if at least the largest Fourier coefficient is the same: $$\mathrm{argmax}_s |\hat{f}(s)| = \mathrm{argmax}_s |\hat{g}(s)| $$ As a first observation, I looked at the series expansion of $g(x)$ in terms of $f(x)$ , and find $$g(x) = f(x) + \frac{1}{3}[f(x)]^3+ \frac{7}{128}[f(x)]^5 + \dots$$ and so hand-wavyly $$ \hat{g}(s) = \hat{f}(s) + \frac{1}{2^n} \sum_{x\in \{0,1\}^n} (-1)^{s\cdot x} \left( \frac{1}{3}[f(x)]^3 + \frac{7}{128}[f(x)]^5 + \dots \right)$$ One also has, in particular, $$f(x) > 0 \Rightarrow g(x)> 0$$ . Not sure how to proceed though to answer the above questions 1) and/or 2) based on this. Any ideas would be greatly appreciated.","['boolean-algebra', 'fourier-analysis', 'discrete-mathematics']"
4578900,Variance of sum of cards with and without replacement,"Suppose we draw 2 cards from a standard 52 card deck with value 1 - 13 (from Ace to King). How would expected values and variance compare with or without replacement? $E[X] = 14$ for both cases (linearity of expectation) $Var(X) = E[X^2] - E[X]^2$ however this seems like it would take incredibly long to compute for all the possible sums, is this the right approach? Would the variances even be different?","['statistics', 'variance', 'expected-value', 'card-games', 'probability']"
4578923,Does Tychonov's theorem imply Zorn's lemma,"I know that Tychonov's theorem, Zorn's lemma, the axiom of choice, the well-ordering theorem and many other results are logically equivalent in the sense that either of them implies all other in ZF. But this is not my question. I would like to know if there is a direct proof of Zorn's lemma using Tychonov's theorem for a suitably constructed produt of compact spaces. I am sure that everybody understands the question although it is hard to formalize what I want. But again, this is not the point of this question.","['general-topology', 'set-theory']"
4578958,Find the volume between the regions $x^2 + y^2+ z^2 = 4$ and $x = 4-y^2$,"I want the volume of the sphere $x^2 + y^2 + z^2 = 4$ from $x = 0$ to $x = 4-y^2$ .  The integral that gives this volume is $$\int\limits_{-2}^2 \int\limits_0^{4-y^2} \int\limits_{-\sqrt{4-x^2-y^2}}^{\sqrt{4-x^2-y^2}}\ 1\ dz\ dx\ dy$$ I don't find $T$ such that $T\bar{u} = \bar{x}$ for every $\bar{x}\in D$ where $D$ is our desired volume. I mean, I'm trying to find $D^*$ such that for every $\bar{u} \in D^*$ , $T$ is a change of variables for our problem. So, I'm trying the find the volume between the regions $x^2 + y^2 + z^2 = 4$ and $x = 4-y^2$ but I can neither find that scalar. Any ideas please.","['multivariable-calculus', 'calculus']"
4578983,"If $\vec{v}$ is orthogonal to every vector in $\mathbb{R}^3$, then $\vec{v} = \textbf{0}$","I'm preparing for a test and was wondering if someone could validate my solution to the following problem: If $\vec{v} \in \mathbb{R}^3$ is orthogonal to every vector in $\mathbb{R}^3$ , then $\vec{v} = \textbf{0}$ . I took the following approach. $I.$ If $\vec{v}$ is orthogonal to all vectors in $\mathbb{R}^3$ , then it must be orthogonal to itself. Then $\vec{v} \cdot \vec{v}=v_1^2+v_2^2+v_3^2=0$ , where $v_i$ is the $i$ th component of $\vec{v}$ . $II$ . $||\vec{v}||=\sqrt{v_1^2+v_2^2+v_3^2} = \sqrt{\vec{v}\cdot\vec{v}}= \sqrt{0}=0.$ Then $\vec{v}$ has length $0$ . $III$ . The vector of length $0$ is by definition the zero vector. Then $\vec{v}= \textbf{0}$ . As you can see, a pretty simple problem, but with the test ahead I want to leave no place for doubt. Thanks in advance. Oh, and by the way, alternative proofs are appreciated!","['multivariable-calculus', 'solution-verification', 'linear-algebra']"
4578997,Calculate $\int_0^1 \ln^{2020}(x)dx$,"In this video on mark 17:00 gives the tiebreaker question of the integration bee finals. It asks to calculate $$\int_0^1 \ln^{2020} (x)dx$$ We could create a reduction formula for $\ln^n(x)$ which would be $$I(n,x)=x\ln^n(x)-n\int\ln^{n-1}(x)dx$$ Repeatedly doing this we get this sum: $$I(n,x)=(-1)^nn!+x\sum_{k=0}^{n-1}(-1)^k\ln^{n-k}(x)\binom{n}{k}k!$$ Substituting $n=2020$ and then substituting $x=0$ , I realized that we could just simply substitute $x=1$ to get the answer, which is $2020!$ . This answer is correct, but is my procedure right?","['integration', 'calculus', 'solution-verification', 'contest-math']"
4579097,Bounding $\sin(\pi z)$ away from $0$ on a circular contour in $\Bbb C$,"I'm currently trying to show the following: Let $C_R$ be the circle of radius $R=N+\frac{1}{2}$ for some $N\in\Bbb N$ . Then $$\lim_{R\to\infty}\ \int_{C_R}\frac{1}{z^2\sin(\pi z)}\,dz=0.$$ This will follow immediately if we can prove $\sin(\pi z)$ is bounded away from zero on the contour. To that end, I've shown $|\sin(\pi z)|\geq1$ for $z\in C_R$ is equivalent to proving the inequality $$\cosh(x)\geq 2+\cos\left(\sqrt{(2\pi R)^2-x^2}\right)$$ for $-2\pi R\leq x\leq2\pi R$ . This brings me to my question: How can we prove this inequality? I've thought of trying to use Taylor series here, but $x$ isn't necessarily close to $0$ . I also thought of this as a Calculus I style problem (after all, we're just minimizing a single-variable function), but the derivative was too convoluted to reasonably work with unless I missed something. The plots that I know how to make on Mathematica agree with my computations, so I have reason to believe this is the correct idea, I'm just not sure how to formally prove it.","['hyperbolic-functions', 'complex-analysis', 'calculus', 'optimization', 'inequality']"
4579105,Simplest proof to calculate the center of M_n(F),"If we consider the set of all n by n matrices M, and denote by Z(M) the set of matrices which commute with every matrix I personally dislike the only proof I know. So I was wondering if anybody here knows of a more elegant proof, or rather just a different proof, given that ""elegant"" is subjective. Forgot to mention, the proof I know is where we consider $A\in Z(M)$ and calculate the effect on the standard basis of the set of n by n matrices, to notice that A must be scalar in light of the implied conditions on the rows and columns of A. Thank you for all the proofs, I am most definitely satisfied in elegance and variety.","['matrices', 'linear-algebra']"
4579114,Which region is this triple integral reffering to?,"$$Integral =  \iiint_V x+z\;dV $$ $$Region$$ $$x^2 + y^2 + z^2 \leq 1$$ $$z \leq \sqrt{x^2 + y^2}$$ Geogebra shows that the region looks like in the picture below but given the inquality sign in the equations, is the actual region inside the cone and sphere (icecream cone) or is the actual region inside the sphere but minus the icecream cone? Initially I thought it is the first one but given the inquality, it made me think it is the second one. Which one is it? Thank you very much!","['integration', 'analysis', 'multivariable-calculus', 'calculus', 'spherical-coordinates']"
4579120,"Given an isosceles right-triangle $\triangle ABC$ with 3 squares on the hypotenuse, find the area of the largest square.","As the title suggests, the question is to solve for the area of the largest square in the following figure given the area of 2 smaller ones. This is a pretty fun problem and I want to see if there are any more ways to solve it, such as with trigonometry. As always, I'll post my own approach as an answer below!","['contest-math', 'euclidean-geometry', 'geometry', 'triangles', 'trigonometry']"
4579170,Want to simplify $z\cos\phi_1\cos\phi_2+z^*\sin\phi_1\sin\phi_2$,"I have an integral that has the term $z\cos\phi_1\cos\phi_2+z^*\sin\phi_1\sin\phi_2$ located in an exponent term. I am attempting to solve this integral (there are other terms but I am focused on this one). Clearly, if $z$ was real, this would be pretty simple to simplify; however, $z$ is decidedly complex. I have tried using trigonometric identity $A\cos x+B\sin x=\sqrt{A^2+B^2}\cos[x-\arctan(B/A)]$ , but this didn't work. Does anyone have any idea of how to combine these terms somehow? I would appreciate any help with this.","['integration', 'trigonometry', 'complex-numbers']"
4579186,Urn Ball Probability problem from Sheldon Ross' Probability Textbook,"Seven balls are randomly withdrawn from an urn that contains 12 red, 16 blue, and 18 green balls. Find the probability that (b) at least 2 red balls are withdrawn; I understand that one way to do the problem is to subtract the probability that 0 or 1 red ball is drawn from 1 to get the probability of at least 2 red balls being drawn. However, I don't understand why this method fails: Pick 2 red balls, then pick any 5 balls of the remaining 44. For the position of the red balls to not matter, take $\binom{7}{2}$ . The total number of possibilities is $\binom{46}{7}$ . So P(at least 2 red) = $\frac{\binom{12}{2} \binom{44}{5} \binom{7}{2}}{\binom{46}{7}}$ This is greater than 1, so it's definitely wrong.","['combinations', 'probability']"
4579266,"Does there exist smooth functions $f_i,g_i \in C^{\infty} (\mathbb R)$ such that $\sin (xy) = \sum\limits_{i = 1}^{n}f_i (x) g_i (y)$ for all $x,y\ $?","Does there exist smooth functions $f_i,g_i \in C^{\infty} (\mathbb R)$ such that $\sin (xy) = \sum\limits_{i = 1}^{n} f_i (x) g_i (y)$ for all $x,y \in \mathbb R\ $ ? I don't think it's true but couldn't able to conclude it properly. Any help in this regard would be warmly appreciated. Thanks for your time.","['multivariable-calculus', 'smooth-functions', 'real-analysis']"
4579291,Are there any other decent methods to evaluate $\int_0^1 \frac{\ln \left(1-x^4\right)}{1+x^2} d x?$,"We first split the integrand into 3 parts as \begin{aligned}
\int_0^1 \frac{\ln \left(1-x^4\right)}{1+x^2} d x &= \underbrace{\int_0^1 \frac{\ln \left(1+x^2\right)}{1+x^2} d x}_J+\underbrace{\int_0^1 \frac{\ln (1+x)}{1+x^2} d x}_K+ \underbrace{\int_0^1 \frac{\ln (1-x)}{1+x^2} d x}_L
\end{aligned} Denotes the Catalanâ€™s constant by $G$ . By my post , $$J=\frac{\pi}{2}\ln2-G$$ Dealing with the last $2$ integrals, we use a powerful substitution $x=\frac{1-t}{1+t} ,$ then $dx=-\frac{2dt}{(1+t)^2}.$ $$
\begin{aligned}
K&=\int_1^0 \frac{\ln 2-\ln (1+t)}{\frac{2+2 t^2}{(1+t)^2}} \frac{-2 d t}{(1+t)^2} \\
&=\ln 2 \int_0^1 \frac{d t}{1+t^2}-\int_0^1 \frac{\ln (1+t)}{1+t^2}
\end{aligned}
$$ Hence $$K=\frac{\pi}{8} \ln 2 $$ $$
\begin{aligned}
L=& \int_0^1 \frac{\ln 2+\ln t-\ln (1+t)}{1+t^2} d t \\
&=\frac{\pi}{4} \ln 2+\int_0^1 \frac{\ln t}{1+t^2}-\int_0^1 \frac{\ln (1+t)}{1+t^2} d t . \\
&=\frac{\pi}{4} \ln 2-G-\frac{\pi}{8} \ln 2 \\
&=\frac{\pi}{8} \ln 2-G
\end{aligned}
$$ Combining them to get $$
\begin{aligned}
I &=\left(\frac{\pi}{2} \ln 2-G\right) +\frac{\pi}{8} \ln 2  +\left(\frac{\pi}{8} \ln 2-G\right)\\
&=\frac{3 \pi}{4} \ln 2-2 G
\end{aligned}
$$ I do want to know if it can be solved by any other elegant methods. Your comments and methods are highly appreciated.","['integration', 'calculus', 'catalans-constant', 'improper-integrals']"
