question_id,title,body,tags
2291061,Neumann problem has unique twice continuously differentiable solution,"I want to show that for each $f\in\mathcal{C}([0,\pi])$, there exists a unique twice continuously differentiable solution to the Neumann problem on an interval
$$
-\frac{d^2u}{dx^2}+u=f$$
on $(0,\pi)$ with boundary conditions $\frac{du}{dx}(0)=\frac{du}{dx}(\pi)=0$. I also want to show that the map that sends $f\mapsto u$ extends to a compact self-adjoint operator on $L^2(0,\pi)$. It was suggested that I use the fact that $c_k\cos(kx)$ is an orthonormal basis of $L^2(0,\pi)$ for normalizing coefficients $c_k$ but I'm unsure how to proceed. Ideally, I would use Hilbert space theory to solve this. Thanks!","['functional-analysis', 'lp-spaces', 'hilbert-spaces']"
2291070,Prime divisors of $k^2+(k+1)^2$,"It seems that the prime divisors of : $k^2+(k+1)^2$ are of the form $4k'+1$, and that they can be only one prime divisor of $k^2+(k+1)^2$ of the form : $4k''+1$. Yet I don't know how to prove this result, and how to study in general the prime divisors of : $k^2+(k+1)^2$.","['divisibility', 'algebraic-number-theory', 'number-theory', 'prime-numbers', 'elementary-number-theory']"
2291126,"Show that if $G$ is abelian, and $|G| \equiv2 \mod 4$, then the number of elements of order $2$ in $G$ is $1$.","I've tried proving it by contradiction, assuming the number of elements is different than one, which, by Sylows $3$, implies that $|G|=2^xm$ with $m$ odd. With that I managed to show that $m\equiv1\mod4$, but kinda got stuck there...","['abelian-groups', 'abstract-algebra', 'group-theory', 'sylow-theory']"
2291131,Please confirm that $\int_{0}^{\infty}{\ln(x)\sin(x)\cos\left(x\over \sqrt{\phi}\right)\over x} dx={1\over 2}\pi \left(\ln\phi-\gamma\right)$,"Observe this integral Where $\phi={\sqrt{5}+1\over 2}$ and $\gamma=0.5772156...$ is Euler's Constant $$\int_{0}^{\infty}{\ln(x)\sin(x)\cos\left(x\over \sqrt{\phi}\right)\over x}\mathrm dx={1\over 2}\pi \left(\ln\phi-\gamma\right)\tag1$$ $$I(a)=\int_{0}^{\infty}{\ln(x)\sin(x)\cos\left(ax\right)\over x}\mathrm dx\tag2$$ $$I^{'}(a)=-\int_{0}^{\infty}{\ln(x)\sin(x)\sin\left(ax\right)}\mathrm dx\tag3$$ Using $2\sin A\sin B=\cos(A-B)-\cos(A+B)$ $\sin x\sin(ax)=\cos[(1-a)x]-\cos[(1+a)x]$ $$I^{'}(a)=-\int_{0}^{\infty}\cos[(1-a)x]\ln x \mathrm dx+\int_{0}^{\infty}\cos[(1+a)x]\ln x\mathrm dx\tag4$$ $$\int \cos[(1-a)x]\ln x \mathrm dx={\sin[(1-a)x]\ln x\over 1-a}-\int {\sin[(1-a)x]\over x}\tag 5$$ $$\int \cos[(1-a)x]\ln x \mathrm dx={\sin[(1+a)x]\ln x\over 1+a}-\int {\sin[(1+a)x]\over x}\tag 6$$ $(5)$ and $(6)$ showed divgergent integrals This approach  I have tried is not working, what other method can we use to verify $(1)?$",['calculus']
2291175,"Why the function 1/x does not increase between (-infinity,0) and (0,+infinity)?","I know that the domain is: $(-\infty,0) \cup (0,\infty)$, or all Reals except Zero. But if a take the ""nearest negative number to zero"" and then, the ""nearest positive number to zero"", my function will hugely increase (Y will approach +infinity). And my domain have all numbers that approach Zero in both sides, except Zero. So, why the function 1/x does not increase between $(-\infty,0)$ and $(0,\infty)$? (Sorry for any english error)","['algebra-precalculus', 'calculus', 'functions']"
2291217,"You have 6 red balls, 6 blue, and 6 white. You randomly select a sample of 5 balls.","You have 6 red balls, 6 blue, and 6 white. You randomly select a sample of 5 balls. What are the odds that the sample contains 4 balls of one color and 1 of another color? I thought of it like this: $$P(\text{4 balls of one color and 1 of another color})=\frac{3 \cdot \binom{6}{4}\cdot 2 \binom{6}{1}}{\binom{18}{5}}\approx 0.0630252$$ since you have three choices for the color of the first four balls and only two (since you've can't use the same color that you did for the first four balls) for the last ball. All of this divided by the total number of ways to pick a group of five from the 18 balls you have in total. Is my approach correct?","['combinatorics', 'probability', 'discrete-mathematics']"
2291235,"$C^{n}[0, 1]$ is not a C*-algebra","Let $C^{n}[0, 1]$ be the space of all $n$-times continously differentiable functions endowed with the norm $$\|f\| = \sum_{k=0}^n \frac{\sup_{t \in [0, 1]}{|f^{(k)}(t)|}}{k!}$$ Let $*$ be the involution map that acts as follows: $f^{*}(t) = \overline{f(t)}$. I would like to show that $A = (C^{n}[0, 1], \| \cdot \|, *)$ is not a C*-algebra. So the first step is to show that there exists a sequence of self-adjoint (w.r.t. to the involution) functions of a unit norm, e.g. $\|f_n\| = 1$ so that $ f^2_n \rightarrow 0$ (if so, then the C*-identity would fail). Apparently, this example is a sort of a classical one for the first course on functional analysis, but i bumped into troubles while trying to invent something suitable. What are the possible approaches/examples? Update Looks as if $f_{n} = \frac{x^{n}}{n}$ works, since $f^{2}_{n} = \frac{x^{2n}}{n^{2}}$ converges to $0$ as $n \rightarrow + \infty$, but $||f_{n}|| = \frac{1}{n} + 1 \rightarrow 1$","['functional-analysis', 'normed-spaces', 'c-star-algebras', 'banach-algebras']"
2291238,How do I use strong induction (the second principle of finite induction) to prove $a^n - 1 = (a-1)(a^{n-1} + a^{n-2}+ \cdots + a + 1)$???,"$$a^n - 1 = (a-1)(a^{n-1} + a^{n-2}+ \cdots +  a + 1)$$ I have no idea how to use ""strong induction"" to prove this, I have used an example like $a^{3}-1$ but I'm assuming that is not what the answer is looking for by any means! Help! It's apparent that when you use $a^{n-1} -1$ that it works, but how can this be mathematically proved using this method. I feel like it has something to do with proving it for $n < 1$ because the equation is for all $n$ is greater than or equal to 1.","['number-theory', 'induction', 'elementary-number-theory']"
2291250,How do we prove $\lim\inf s_n\le\lim\inf\sigma_n$ in the following case?,"Let $s_n$ be a sequence of nonnegative numbers, and for each $n$ define $\sigma_n=\frac1n (s_1+s_2+...+s_n)$, how do we prove  $\lim\inf s_n\le\lim\inf\sigma_n$? I was told two ways could be achieved: $1.$ Suppose we have proved $\lim\sup \sigma_n\le\lim\sup s_n$, then we can do a little trick to prove the $\lim\inf$ version. My thought is that if we can somehow make $\lim\sup -\sigma_n\le\lim\sup -s_n$, then we can do $\lim\inf\sigma_n=-\lim\sup -\sigma_n\ge-\lim\sup -s_n=\lim\inf s_n$, but the problem is how do we make $\lim\sup -\sigma_n\le\lim\sup -s_n$. $2.$ Directly prove it. My thought is that we can pick a $N$, $n>N\implies$ $\sigma_n=\frac1n (s_1+s_2+...+s_n)\ge \min\{s_1, ...,s_n\}\ge\inf s_n$. Since $\inf s_n$ is lower bound for $\sigma_n$, $\inf s_n\le\inf\sigma_n$, then taking the limit, we get $\lim\inf s_n\le\lim\inf\sigma_n$. But I felt taking limit part is not quite logical to me. Could someone help?","['limits', 'limsup-and-liminf', 'proof-verification', 'calculus', 'proof-explanation']"
2291256,Question about the complex inner product axioms,"My textbook claims that from the axioms for the complex inner product: $$\left<y,x\right>=\overline{\left<x,y\right>}\tag{1}$$ 
$$c\left<x,y\right> = \left<cx,y\right>\tag{2}$$ we can derive: \begin{align}
\left<x,cy\right> &= \overline{\left<cy,x\right>}\\
&= \overline{c}\overline{\left<y,x\right>}\\
&= \overline{c}\left<x,y\right>
\end{align} I understand the first and last steps of the derivation, but in the middle step, I don't understand what justifies bringing the $c$ out of the inner product and taking its complex conjugate.","['linear-algebra', 'complex-numbers']"
2291287,Pigeonhole Principle homework problem,"How to solve this problem using Pigeonhole Principle? A worker plans to work 60 hours in the next 37 days, and he works at least 1 h/day, show that he will be working 13 hours in total in some continuous days. So there are 23 hours more to spend on 37 days, but how is that related to 13 on continuous days?","['pigeonhole-principle', 'discrete-mathematics']"
2291302,Why isn't the directional derivative generally scaled down to the unit vector?,"I'm starting to learn how to intuitively interpret the directional derivative, and I can't understand why you wouldn't scale down your direction vector $\vec{v}$ to be a unit vector. Currently, my intuition is the idea of slicing the 3D graph of the function along its direction vector and then computing the slope of the curve created by the intersection of the plane. But I can't really understand how the directional derivative would be a directional derivative if it were not scaled down to be a change in unit length in the direction of $\vec{v}$. Is there an intuitive understanding I can grasp onto? I'm just starting out so maybe I haven't gotten there yet. Note, I think there may be a nice analogy to linearization, like if you take ""twice as big of a step"" in the direction of $\vec{v}$ , then the change to the function due to the change in this step is twice as big. Is this an okay way to think about it?","['multivariable-calculus', 'vector-analysis', 'calculus', 'derivatives']"
2291308,$x^{13}+x+90$ is divisible by $x^2-x+a$ $(a\in\mathbb N)$. Find $a$ [duplicate],"This question already has answers here : Find all integers $a$ for which $x^2-x+a$ divide $x^{13}+x+90$. (3 answers) Closed 4 years ago . Question: $x^{13}+x+90$ is divisible by $x^2-x+a$ $(a\in\mathbb N)$. Find $a$. What I did was:
\begin{align}
&x^{13}+x+90 = Q(x)(x^2-x+a)\\
&\text{Let $t$ be one of roots of }x^2-x+a=0\\
&t^2=t-a\\
&t^4=(t-a)^2=t^2-2at+a^2=(1-2a)t+a(a-1)\\
\\
&t^8=((1-2a)t+a(a-1))^2=(1-2a)^2t^2+2(1-2a)a(a-1)t+a^2(1-a)^2\\
&=(1-2a)^2(t-a)+2(1-2a)a(a-1)t+a^2(1-a)^2\\
&=(1-2a)((1-2a)+2a(a-1))t+a^2(1-a)^2-a(1-2a)^2\\
\\
&t^{12}={(1-2a)t+a(a-1)}{(1-2a)((1-2a)+2a(a-1))t+a^2(1-a)^2-a(1-2a)^2}\\
&=(1-2a)^2((1-2a)+2a(a-1))t^2\\
&+ (a(a-1)(1-2a)((1-2a)+2a(a-1))+(1-2a)(a^2(1-a)^2-a(1-2a)^2))t\\
&+ a^2(a-1)(a(1-a)^2-(1-2a)^2)\\
&=(1-2a)^2((1-2a)+2a(a-1))(t-a)\\
&+(a(a-1)(1-2a)((1-2a)+2a(a-1))+(1-2a)(a^2(1-a)^2-a(1-2a)²^2))t\\
&+ a^2(a-1)(a(1-a)^2-(1-2a)^2)\\
&=\left((1-2a)^2((1-2a)+2a(a-1))+a(a-1)(1-2a)((1-2a)+2a(a-1))+(1-2a)(a^2(1-a)^2-a(1-2a)^2)\right)t\\
&+ a^2(a-1)(a(1-a)^2-(1-2a)^2)-a(1-2a)^2((1-2a)+2a(a-1))\\\\
&t^{13}=((1-2a)^2((1-2a)+2a(a-1))+a(a-1)(1-2a)((1-2a)+2a(a-1))+(1-2a)(a^2(1-a)^2-a(1-2a)^2))t^2\\
&+ (a^2(a-1)(a(1-a)^2-(1-2a)^2)-a(1-2a)^2((1-2a)+2a(a-1)))t\\
&=((1-2a)^2((1-2a)+2a(a-1))+a(a-1)(1-2a)((1-2a)+2a(a-1))+(1-2a)(a^2(1-a)^2-a(1-2a)^2))(t-a)\\
&+ (a^2(a-1)(a(1-a)^2-(1-2a)^2)-a(1-2a)^2((1-2a)+2a(a-1)))t\\
&=((1-2a)^2((1-2a)+2a(a-1))+a(a-1)(1-2a)((1-2a)+2a(a-1))+(1-2a)(a^2(1-a)^2-a(1-2a)^2)+a^2(a-1)(a(1-a)^2-(1-2a)^2)-a(1-2a)^2((1-2a)+2a(a-1)))t\\
&-a((1-2a)^2((1-2a)+2a(a-1))+a(a-1)(1-2a)((1-2a)+2a(a-1))+(1-2a)(a^2(1-a)^2-a(1-2a)^2))\\
&=-t-90\\\\
&(1-2a)^2((1-2a)+2a(a-1))+a(a-1)(1-2a)((1-2a)+2a(a-1))\\
&+(1-2a)(a^2(1-a)^2-a(1-2a)^2)\\
&+a^2(a-1)(a(1-a)^2-(1-2a)^2)-a(1-2a)^2((1-2a)+2a(a-1))\\&=-1\\
\\&\text{with an aid of Wlofram Alpha,}\\
&(a-2)(a^5-19a^4+32a^3-20a^2+5a-1)=0\\
&\therefore a=2
\end{align} Is there a smarter way to get to the answer? Thanks. (*) Any solution is welcomed, but this question was given to high school students who didn't learn calculus.","['algebra-precalculus', 'polynomials', 'divisibility']"
2291321,Series with factorial.,"Evaluate $$\frac{1}{2!}+\frac{2}{3!}+\frac{3}{4!}+...+\frac{99}{100!}$$ My attempt, I changed it into $$\sum_{n=1}^{99} \frac{n}{(n+1)!}$$ I really don't know how to attempt for it as it consists of factorial. Can anyone give me some hints? Thanks in advance.","['algebra-precalculus', 'sequences-and-series']"
2291369,Dimension of $F^X$,"Let $F$ be a field, let $X$ be a set, and let $F^X$ denote the set of functions from $X$ to $F$. Then $F^X$ forms a vector space in a natural manner. What can we say about the dimension of $F^X$? This question is easy when $X$ is finite, so from here on, suppose $X$ is infinite. The set $\{\chi_{\{x\}} \, \vert \, x\in X\}$ (where $\chi_S$ denotes the indicator function of $S$) is a linearly independent subset of $F^X$ with cardinality $\vert X \vert$, and so we can say that $\dim F^X \geq \vert X \vert$. In general, we don't necessarily have equality, since $\dim \mathbb{R}^\omega = \mathfrak{c}$. In fact, I think that $\dim F^\omega \geq \mathfrak{c}$ for any field $F$, which should be enough to show that $\dim F^X > \vert X \vert$ if I'm not mistaken. Is there anything more that we can say about $\dim F^X$? It would be ideal if there were a formula for $\dim F^X$ in terms of $\vert F \vert$ and $\vert X \vert$, but I doubt that's true.","['linear-algebra', 'elementary-set-theory']"
2291371,Linear Least Square Equation is Strictly Convex,I know linear least square equation $\|y−X\beta\|^2$ can have multiple solutions since it is NOT a strictly convex equation. How to prove it is only a convex function and NOT a strictly convex function?,"['linear-regression', 'optimization', 'calculus', 'statistics', 'convex-optimization']"
2291382,$C^2$ isometric embedding of the flat torus into $\mathbb{R}^3$,"What is the reason that there is no $C^2$ isometric embedding of the flat torus inside $\mathbb{R}^3$? Is there an explicit proof of this fact anywhere? The flat metric must violate some condition for the $C^2$ isometric embedding. And as we know, the condition for a local $C^2$ isometric embedding is given by the Gauss and Codazzi-Mainardi relations. I do not understand how these relations are getting violated by the flat metric on torus. Kindly cite some reference. Thanks in advance. Edit 1: I am following this paper: Han and Lin, On the isometric embedding of torus in $\mathbb{R}^3$, Methods and Applications of Analysis 15 , pp. 197-204, 2008. Here, the sufficient conditions for the existence of a global smooth isometric embedding of the torus of genus 1 $\mathbb{T}$ with a Riemannian metric $a$ $(\mathbb{T}, a)$ are given. But these conditions, as can be clearly seen, are given for the existence of the standard embedded torus in $\mathbb{R}^3$, which is too strict. The original question is about the existence of an isometrically embedded torus in $\mathbb{R}^3$, not necessarily the tadard torus. So there must be a different set of sufficient conditions than the ones given in the above reference. Can one of these sufficient conditions be the requirement that the the subset of $\mathbb{T}$ where the Gaussian curvature $K$ of $a$ is positive is non-empty ?",['differential-geometry']
2291388,Evaluate $(1+p)\cdots(1+\frac pn)$,"Prove for $\forall p\in\mathbb{R}$, 
  $$\lim_{n\rightarrow\infty}\dfrac{\prod_{k=1}^n(1+\frac{p}{k})}{n^p}=L_p$$
  where $L_p\in\mathbb{R}$. Moreover, I tried to calculate $L_p$, and it can be calculated if $p$ is an integer. What if $p$ is not an integer?","['real-analysis', 'limits']"
2291393,Converse of Spectral Theorem for Compact Self-Adjoint Operators,"If I have a bounded linear operator $A$ on a Hilbert space $H$ whose eigenvectors form an orthonormal basis for $H$ and whose corresponding eigenvalues go to $0$ then is $A$ compact and self-adjoint? I ask because I want to prove that $A$ defined on an orthonormal basis $\{e_k\}$ as $Ae_k=e_k/(k^2+1)$ is compact and self-adjoint. I know that it is, but I'm just wondering if appealing to the spectral theorem is valid. Thanks!","['functional-analysis', 'eigenvalues-eigenvectors', 'spectral-theory', 'hilbert-spaces']"
2291461,Relation of N-Dimentional Cubes and Pascal's triangle.,"If we take a point and we pass a plane through it, we see it hits it at one point once so we get $[1]$. If we take a line segment and pass a plane through where the normal line is the line from opposite corners we hit one point twice so we get $[1,1]$. Do the same with a square we see it hits a point, then 2 points simultaniously , then 1 point, so we get $[1,2,1]$. Do the same with a cube, we get $[1,3,3,1]$. Put them in a nice order, we get $$1$$$$1 \phantom{\_}1$$$$1\phantom{\_}2\phantom{\_}1$$$$1\phantom{\_}3\phantom{\_}3\phantom{\_}1$$ My question is does this pattern continue for subsequent dimensions?",['geometry']
2291479,"Is $f(x,y)=\frac{\sin\sqrt[3]{x^3+y^3}}{\sqrt[5]{x^5+y^5}}$ uniformly continuous or not","Find out if function $$f(x,y)=\frac{\sin\sqrt[3]{x^3+y^3}}{\sqrt[5]{x^5+y^5}}$$ is uniformly continous or not in area $D=\{0<x^2+y^2<1\}$. I found out that we have no $\lim\limits_{x,y\to 0}f(x,y)$, because $$\lim\limits_{x,y\to 0}\frac{\sin\sqrt[3]{x^3+y^3}}{\sqrt[5]{x^5+y^5}}=\lim\limits_{\rho\to0}\frac{\sqrt[3]{\rho^3\sin^3\alpha+\rho^3\cos^3\alpha}}{\sqrt[5]{\rho^5\sin^5\alpha+\rho^5\cos^5\alpha}}=\lim\limits_{\rho\to0}\frac{\sqrt[3]{\sin^3\alpha+\cos^3\alpha}}{\sqrt[5]{\sin^5\alpha+\cos^5\alpha}}$$ hence we can't use The Uniform Continuity Theorem as we can't determ $f(0,0)$. Function doesn't have bounded partial derivatives, so I think it's not uniformly continous, but I don't know how to show that","['limits', 'calculus', 'multivariable-calculus', 'continuity', 'uniform-continuity']"
2291484,Proof verification : $f$ is increasing at $x_0$.,"Q. Let $f:(a,b) \rightarrow \mathbb R$ and $x_0 \in (a,b)$. $f$ is differentiable function. Determine the relationship of following statements. (a) $f^\prime (x_0) \gt 0$ (b) $f$ is increasing at $x_0$ (c) $f$ is increasing function on some intervals that contain $x_0$. My proof (a) $\rightarrow$ (b) $\lim_{h \to 0} {{f(x_0 +h) - f(x_0)} \over h} = \lim_{h \to 0+} {{f(x_0 +h) - f(x_0)} \over h} = \lim_{h \to 0-} {{f(x_0 +h) - f(x_0)} \over h} = \alpha \gt 0$ Therefore, there exists $\delta \gt 0$ s.t. $0 \lt h \lt \delta \Rightarrow f(x_0+h) - f(x_0) \gt 0$ and $-\delta \lt h \lt 0 \Rightarrow f(x_0+h) - f(x_0) \lt 0$. So $f$ is increasing at $x_0$. (b) $\rightarrow$ (c) Using the above notation of $\delta$, it follows that $f$ is increasing function on $N(x_0, \delta)$. (c) $\rightarrow$ (a) If $f$ is increasing function on $N(x_0, \delta)$, then $\lim_{h \to 0+} {{f(x_0 +h) - f(x_0)} \over h} = \lim_{h \to 0-} {{f(x_0 +h) - f(x_0)} \over h} \gt 0$. Therefore $f^\prime (x_0) \gt 0$. (a), (b), (c) is equivalent. I heard that (b) $\rightarrow$ (c) can be false in some functions, but as $f$ is differentiable, I think it doesn't matter. Did I proved it right? Do I have to add some more?","['calculus', 'analysis']"
2291491,"how many $3\times 3$ matrices with entries from $\{0,1,2\}$.","How many $3 × 3$ matrices $M$ with entries from $\left\{0, 1, 2\right\}$ are there for which taken from the sum of the main diagonal of $M^TM$ is $5$. Attempt: Let $M = \begin{pmatrix}
a & b & c\\ 
d & e & f\\ 
g & h & i
\end{pmatrix}$. where  $a,b,c,d,e,f,g,h,i\in \{0,1,2\}$ $$M^{T}M= \begin{pmatrix}
a & d & g\\ 
b & e & h\\ 
c & f & i
\end{pmatrix}\begin{pmatrix}
a & b & c\\ 
d & e & f\\ 
g & h & i
\end{pmatrix} $$. sum of diagonal entries $$a^2+b^2+c^2+d^2+e^2+f^2+g^2+h^2+i^2 = 5$$ How can I form different cases?",['combinatorics']
2291525,Random Variable Distribution problem,"You just rented a large house and the realtor gave you five keys, one for the front door and the other four for each of the four side and back doors of the house. Unfortunately, all keys look identical, so to open the front door, you are forced to try them at random.
  Find the distribution and the expectation of the number of trials you will need to open the front door. (Assume that you can mark a key after you’ve tried opening the front door with it and it doesn’t work.) I started doing this problem by setting r.v. $X$ to be the number of trials (keys) you have to try in order to unlock the front door. Then in order to find the distribution, I know I have to find the probability of success for each number of trials. This is what I came up with X Pr[X]
1 1/5
2 1/4
3 1/3
4 1/2
5 1/1 I thought this was true because for example The probability of 1 key opening the front door is 1/5 The probability of 2 keys opening the door is 1/4 because one of the keys have already been marked as incorrect (sampling without replacement) However, the answer turned out to be this: I'm confused as to why $Pr[K=2] = \frac{4}{5} \times \frac{1}{4}$ ... where did those values come from? I think if I understand the method used for the first one I can figure out the rest","['probability-theory', 'probability', 'random-variables', 'probability-distributions']"
2291584,"Given that $a\cos\theta+b\sin\theta+A\cos 2\theta+B\sin 2\theta \leq 1$ for all $\theta$, prove that $a^2+b^2\leq 2$ and $A^2+B^2 \leq 1$","I tried to solve this question but things got too much complicated and hence my efforts were completely futile. Let $a,A,b,B \in \mathbb{R}$ and $$F(\theta)= 1- a\cos \theta - b\sin \theta- A\cos 2\theta- B\sin 2\theta$$ 
  It is given that $$F(\theta) \ge 0 \;\forall\;  \theta $$
   and we have to prove that $\color{red}{a^2+b^2 \le 2}$    and $\color{green}{A^2+B^2 \le 1}$. MY ATTEMPT We need to prove that $$a\cos \theta + b\sin \theta+A\cos 2\theta+ B\sin 2\theta \le 1$$
$$\begin{align}
& = a\cos \theta + b\sin \theta+A( \cos^2 \theta- \sin^2 \theta)+ B \sin \theta \cdot \cos \theta + B \sin \theta \cdot \cos \theta \le 1 \\
& =\ cos \theta (a+A \cos \theta+ B \sin \theta)+\sin \theta(b-A \sin \theta+B \ \cos \theta) \le1 \\
\end{align}$$ We know that $-\sqrt{x^2 + y^2} \le x \cos \theta + y \sin \theta \le \sqrt{x^2 + y^2}$
$$\Rightarrow (a+A \cos \theta+ B \sin \theta)^2 + (b-A \sin \theta+B \ \cos \theta)^2 \le 1$$
After solving this equation we get 
$$a^2 + b^2 + 2(A^2 +B^2)+ \cos \theta (2aA+2bB) + \sin \theta (2aB-2bA) \le1$$ Now If I again apply the same property, certainly the things are going to become more complicated and hence I think my approach is not at all right. Kindly Help me with this question.","['inequality', 'trigonometry']"
2291594,"A summation question ,Is $S \to \ln 2$?","I am in doubt with this question . let $S=\dfrac{\dfrac12}{1} +\dfrac{(\dfrac12)^2}{2}+\dfrac{(\dfrac12)^3}{3}+\dfrac{(\dfrac12)^4}{4}+\dfrac{(\dfrac12)^5}{5}+...$ Is it converge to $\ln 2$ ? I tried this 
$$x=\dfrac12  \to 1+x+x^2+x^3+x^4+...\sim\dfrac{1}{1-x}\to 2$$ by integration wrt x we have $$\int (1+x+x^2+x^3+x^4+...)dx=\int (\dfrac{1}{1-x})dx \to\\
x+\dfrac{x^2}{2}+\dfrac{x^3}{3}+\dfrac{x^4}{4}+...=-\ln(1-x)$$then put $x=0.5$ $$\dfrac{\dfrac12}{1} +\dfrac{(\dfrac12)^2}{2}+\dfrac{(\dfrac12)^3}{3}+\dfrac{(\dfrac12)^4}{4}+\dfrac{(\dfrac12)^5}{5}+..\sim -\ln(0.5)=\ln 2$$ now my question is : Is my work true  ? I am thankful for you hint,guide,idea   or solutions.  (I forgot some technics of calculus)","['power-series', 'sequences-and-series', 'calculus']"
2291627,How to prove or falsify this inequality?,"In STEP 2014 Paper II Question 2, an inequality is assumed for candidates to attempting the question about the approximation of $\pi $ $$\int_{0}^{\pi } (f(x))^2 dx \le \int_{0}^{\pi } (f'(x))^2 dx $$
Where,
$$f(0)=f(\pi )=0$$ It then asked for the construction of functions in the use of approximate $\pi $. The question itself is not difficult at all, but I'm pretty interested in the reason why the inequality works. However, it seems like a fresh high school student is not eligible for it XD and I even got something non-sense. So could anyone help me? Thanks a lot for any hint, guide, or most precisely, proof.","['derivatives', 'inequality', 'integral-inequality', 'calculus', 'integration']"
2291649,Integral of $\frac{x^\alpha}{(1 + x^2)^2}$ from 0 to $\infty$ using contour integration for $-1 < \alpha <3$,"I have tried to evaluate $\int_0^\infty \dfrac{x^\alpha}{(x^2 + 1)^2}\ dx$ for $-1 < \alpha < 3$ using the keyhole contour, but am not sure about my working, especially in defining the analytic branch of log $z$ when we consider the function $f(z) = \dfrac{z^\alpha}{(z^2 + 1)^2}$ where $z^\alpha  = e^{\alpha\text{log}\ z}$. Below is my working: Using the keyhole contour $K_{\varepsilon,M}$ and considering the complex function $\dfrac{z^\alpha}{(z^2 + 1)^2}$, we inspect the integral of the function over each part of the contour (namely $C_{\varepsilon},C_M, I_1$ and $I_2$) in the limit $\varepsilon \to 0,\ M \to \infty$. For $C_{\varepsilon}$, we have $|z^2 +1|$ to be the modulus of points lying on a circle of radius $\varepsilon^2$ centered at $z = 1$ and hence $|z^2 + 1| > 1/2$ for $\varepsilon$ small enough (e.g. $\varepsilon^2 < 1/2$). We thus have: 
\begin{align*}
\int_{C_{\varepsilon}} \dfrac{z^\alpha}{(z^2 + 1)^2}\ dz \ll \pi\varepsilon\ \text{max}_{C_\varepsilon}\bigg|\dfrac{z^\alpha}{(z^2 + 1)^2}\bigg| \ll 4\pi\varepsilon^{\alpha + 1} \Rightarrow \lim_{\varepsilon \to 0}\int_{C_{\varepsilon}} \dfrac{z^\alpha}{(z^2 + 1)^2}\ dz = 0 
\end{align*} For any $z \in C_M$, we have $|z^2 + 1| \geq |z^2| = M^2$ for $M$ big enough. We thus have:
\begin{align*}
\int_{C_M} \dfrac{z^\alpha}{(z^2 + 1)^2}\ dz \ll 2\pi M \ \text{max}_{C_M}\bigg|\dfrac{z^\alpha}{(z^2 + 1)^2}\bigg| \ll 2\pi M^{\alpha - 3} \Rightarrow \lim_{M \to \infty}\int_{C_M} \dfrac{z^\alpha}{(z^2 + 1)^2}\ dz = 0
\end{align*}
For $I_1$, we have $\begin{aligned} = \lim_{\varepsilon \to 0,\  M \to \infty}\int_{I_1}\dfrac{z^\alpha}{(z^2 + 1)^2}\ dz = \int_0^\infty \dfrac{x^\alpha}{(x^2 + 1)^2}\ dx \end{aligned}$. As for $I_2$, we have $z^\alpha = e^{\alpha(\text{ln}x + 2\pi i)} = x^{\alpha}e^{2\pi i\alpha}$, and hence we have $\begin{aligned} = \lim_{\varepsilon \to 0,\  M \to \infty}\int_{I_2}\dfrac{z^\alpha}{(z^2 + 1)^2}\ dz = -e^{2\pi i\alpha}\int_0^\infty \dfrac{x^\alpha}{(x^2 + 1)^2}\ dx \end{aligned}$. Then, by the Residue Theorem, we have:
\begin{align*}
\int_0^\infty \dfrac{x^\alpha}{(x^2 + 1)^2}\ dx &= \frac{2\pi i}{1 - e^{2\pi i\alpha}}\sum_k \text{Res}\bigg(\dfrac{z^\alpha}{(z^2 + 1)^2}; z_k\bigg) \\
&= \frac{2\pi i}{1 - e^{2\pi i\alpha}}\bigg(\lim_{z \to i} \frac{d}{dz}\bigg(\dfrac{z^\alpha}{(z + i)^2}\bigg) + \lim_{z \to -i} \frac{d}{dz}\bigg(\dfrac{z^\alpha}{(z - i)^2}\bigg)\bigg) \\
&= \frac{2\pi i}{1 - e^{2\pi i\alpha}}\bigg(\frac{\alpha -1}{4}i^{\alpha -3} + \frac{\alpha -1}{4}(-i)^{\alpha -3}\bigg) \\
&= \frac{\pi i(\alpha -1)\text{cos}(\pi(\alpha - 3)/2)}{1 - e^{2\pi i\alpha}} 
\end{align*} where the keyhole contour is given as !Keyhole Contour 1 Any help in correcting my working or giving hints or posting your solution is appreciated.","['complex-analysis', 'contour-integration']"
2291686,Solving ode similar to Adler's equation,"I would like to find the exact solution of the following form: $$f'(t) = ae^{-bt} - K \sin(f(t)) $$ It is similar to Adler's equation, but it can not be solved by the method for finding exact solution of Adler's equation. Is there any way to solve this?",['ordinary-differential-equations']
2291691,Infimum of the norms of all non-zero points in a lattice $\Gamma.$,"Let $K$ be a non empty-interior compact convex symmetric set such that $0\in \mathring{A}$. We have a norm the Minkowski functionnal denoted $\Vert \cdot\Vert_K$ so that we can speak about length. Given a path $\gamma:[a,b]\to \Bbb{R}^n$ we have $$lg_K(\gamma)=\int_{a}^b\Vert \gamma'\Vert_Kdt.$$ Now given a lattice $\Gamma$ the torus can be defined to be $(\Bbb{R^n},\Vert \cdot\Vert_K)/\Gamma:=\Bbb{T}^n_{K}.$ Now the systole is $$sys(\Bbb{T}^n_{K})=\inf\{lg_{K}(\gamma) / [\gamma]\ne 0 \in \pi_1(\Bbb{T}^n_{K})\}$$ Now I am reading a document that say: ""The systole of $\Bbb{T}^n_{K}$ is the infimum of the norms of all non-zero points in $\Gamma$."" I am not sure how can I prove that. I know that $p:\Bbb{R}^n\to \Bbb{T}^n_{K}$ is a covering map so that I can lift any path $\gamma:[0,1]\to \Bbb{T}^n_{K}$ to $\tilde\gamma: [0,1]\to \Bbb{R}^n$ such that $\gamma=p\circ\tilde\gamma.$ But what next ? EDIT: I know that the geodesics of $\Bbb{R}^n$ are line segments. I juste need to make the ""link"" between a non contractible path in $\Bbb{T}^n_{K}$ and line segments.","['algebraic-topology', 'differential-geometry']"
2291694,"For distinct odd primes $p, q$ we have $\sqrt{p^2 - 4q}$ is always irrational?","I am trying to show that $\sqrt{p^2 - 4q}$ is always irrational with $p, q$ as described above. Any hints?","['number-theory', 'radicals', 'rationality-testing', 'field-theory']"
2291711,Help me to simplify $\sum\limits_{i=0}^{\lfloor\frac{r}{2}\rfloor}\binom{r}{i}\binom{r-i}{r-2i}$,I tried to simplify $\sum\limits_{i=0}^{\lfloor\frac{r}{2}\rfloor}\binom{r}{i}\binom{r-i}{r-2i}$ by using generating function in the similar way of markus-scheuer's answer to the question How to prove $\sum\limits_{i=0}^{\lfloor\frac{r}{2}\rfloor}\binom{r}{i}\binom{r-i}{r-2i}2^{r-2i}=\binom{2r}{r}$ . What I got is $[u^r](1+u+u^2)^r$. I don't know how to compute the coefficient of $u^r$ in $(1+u+u^2)^r$. I tried using by trinomial expasion but I didn't suceed.,"['number-theory', 'combinatorics', 'elementary-number-theory']"
2291752,Extending an isometric embedding,"Let $C$ be a closed, simply connected Riemannian submanifold of $\mathbb{R}^{2}$. Let $F$ be an isometric embedding of the boundary $\partial C$ of $C$ in $\mathbb{R}^{3}$. In particular, assume that $F$ is smoothly isotopic to the trivial embedding $F_{0} \,\colon C \ni (x,y) \mapsto (x,y,0) \in \mathbb{R}^{3}$. Does there exist an isometric embedding $\tilde{F}$ of $C$ in $\mathbb{R^{3}}$ such that its restriction to $\partial C$ is precisely $F$? If yes, is it unique?","['riemannian-geometry', 'differential-topology', 'geometry', 'differential-geometry', 'surfaces']"
2291756,"Nested radical expressions involving the terms $1,2,3,..,n$","Find all natural numbers $n$ for which there is a permutation $\sigma$ of set {$1,2,...,n$} such that:$$\sqrt{\sigma(1) + \sqrt{\sigma(2) + \sqrt{... + \sqrt{\sigma(n)}}}}$$
is a rational number. I am pretty sure that the only solutions are $n = 1, 3$. I have also noticed that if: $$a_i = \sqrt{\sigma(i) + \sqrt{... + \sqrt{\sigma(n)}}} \text{ and if } a_1 ∈ Q \Rightarrow a_2,..,a_n ∈ Q\Rightarrow a_1,..,a_n ∈ Z.$$","['algebra-precalculus', 'nested-radicals', 'elementary-number-theory']"
2291773,Integral curve is periodic if and only if it is compact,"Suppose we have a complete, non-singular vector field on $M$. Take an integral curve $\phi: \mathbb{R} \rightarrow M$. If it is periodic than $\phi(\mathbb{R})$ is compact as an image of a compact set. Suppose $\phi(\mathbb{R})$ is compact, I have a problem with showing it is periodic, which I suppose is trivial and that even there is no arbitrary, continuous bijection from $\mathbb{R}$ to a compact space. EDIT: Since it may not be so trivial as I expected let me be very precise. M is compact smooth manifold, we have a smooth (necessarily complete since $M$ is compact) non-singular vector field on it. Is it true that an integral curve compact in induced topology must be periodic?","['dynamical-systems', 'general-topology', 'differential-geometry', 'differential-topology']"
2291858,How to show that a differential equation has an integrating factor of a specific form,"I am given the differential equation $$4x^2y + 3xy^2 + 2y^3 + (2x^3 + 3x^2y + 4xy^2)\frac{dy}{dx} = 0$$ and I am asked to show that this equation has an integrating factor $\mu \equiv \mu(xy)$. Setting $M(x,y) = 4x^2y + 3xy^2 + 2y^3$ and $N(x,y) = 2x^3 + 3x^2y + 4xy^2$ I know that I want to show that, for some $\mu$ that is a function of $xy$, I have that $$\mu(xy) M(x,y) + \mu(xy) N(x,y)\frac{dy}{dx} = 0$$ is an exact equation, i.e. $$\frac{\partial (\mu M)}{\partial y} = \frac{\partial (\mu N)}{\partial x}$$ setting $v = xy$ and taking the derivatives, knowing that $$\frac{\partial \mu}{\partial x} = y\frac{\partial \mu}{\partial v}, \frac{\partial \mu}{\partial y} = x\frac{\partial \mu}{\partial v}$$ I will eventually get to $$\frac{\mu '}{\mu} = \frac1{xy}$$ where I wrote $\mu '$ instead of $\frac{\partial \mu}{\partial v}$. My question is, is that enough to show that the equation indeed has an integrating factor $\mu(xy)$? Or do I need to explicitly solve for $\mu$ and show that it depends only on $(xy)$?","['integrating-factor', 'ordinary-differential-equations']"
2291875,Existence of an almost surely convergent subsequence.,"Edit: I slightly relaxed the question to positive probability instead of almost surely. Let $(\Omega,\mathcal{F},P)$ be a probability space. I have a sequence of random variables $(X_n)_{n\in\mathbb{N}}$ and a random variable $X$ such that
\begin{align}
\sup_{n}|X_n(\omega)| \le |X(\omega)| \qquad\forall\omega\in\Omega.
\end{align}
I also have the property 
\begin{align}
P(X_{n+1} = X_n) > \frac{n}{n+1}.
\end{align}
I am interested whether the probability of the existence of a converging subsequence of $(X_n)_{n\in\mathbb{N}}$ is nonzero. That is, whether there exists a subsequence $(n_k)_{k\in\mathbb{N}}$ such that 
\begin{align}
P\left(\lim_{k\rightarrow\infty}X_{n_k}\text{ exists}\right)>0.
\end{align} Thoughts: It is enough to show that there is a subsequence that converges in probability since that implies the existence of a further subsequence that converges almost surely. By the first property and the fact that $\mathbb{R}$ is separable and complete we can use Prohorov's theorem to ensure the existence of a subsequence that converges in distribution. I could potentially assume $E(|X|)<\infty$, so that the family $(X_n)_{n\in\mathbb{N}}$ is uniformly integrable, but I am not sure whether that helps. Thank you in advance!","['weak-convergence', 'probability-theory', 'convergence-divergence']"
2291895,"Prove $\forall x,c \in \mathbb R, |f(x) - f(c)| \le K|x - c|^2$ implies $f(x)$ is constant.","The problem asks Given $f: \mathbb R \mapsto \mathbb R, K \in \mathbb R > 0$ Prove $\forall x,c \in \mathbb R, |f(x) - f(c)| \le K|x - c|^2$ implies $f(x)$ is constant. There's a hint in the prompt that says Hint: Using the definition find the derivative Which I know is $$\lim_{x \to c} \frac{f(x)-f(c)}{x-c} = L$$ but I'm not quite sure what to do with it. Then, I was thinking maybe a proof by contradiction that $f(x)$ is not constant, which would then mean $f'(x) > 0$ , and by the mean value theorem $|f(x) - f(c)| > 0$ , but then I wasn't really sure what to do with that. Basically, I don't really know how to approach the problem, and would appreciate it if someone could point me in the right direction.","['derivatives', 'real-analysis', 'limits']"
2291901,"After the Cylon Holocaust, how many people would know each other?","If you are not familiar, in the show Battlestar Galactica (2003) the Twelve Colonies (planets) are wiped out by the Cylons which they created. Only about 50,000 humans survive in the form of a rag-tag fleet of civilian ships and one Battlestar (give or take). There are at least a few instances in the show of people who were not on the same ship who are re-united or at least know each other from before the attack. Considering that -99.9998% of the human population was wiped out I feel like this would be unlikely, but I want to know exactly how unlikely. Of course, I am not concerned with celebrities and notorious individuals who were ""known"" by a large number of people. I am interested in personal connections. Obviously the definition of ""knowing"" someone is fuzzy, but at least one study, cited in the New York Times, estimates that the Average American knows 600 people. In reality I suspect the real number could be significantly different, but this seems like a realistic ballpark number of people for which the average American could put a name to a face. I'd even be willing to round it up to a generous 1000 personal connections. American society seems like a good proxy for the societies of the Twelve Colonies. According to Wikipedia, there were about 28 billion people alive before the Cylons attacked. So we have all the numbers we need: 28 billion to start with, culled down to 50,000. Average connections of, say, 1000, generously speaking. To abstract and simplify the situation a little, why don't we just imagine that people everywhere died at random, ignoring the fact that survivors were clumped on ships. I am not interested in the intra-ship relationships anyway. I can calculate that the average person would know about 0.0018 people after the attack. What I don't know how to figure out is how many surviving connections between people there would be in a population of 50,000. And that is the real question. It's not just 0.0018 × 50,000, right? I can't figure out the logic, but I don't expect this to be too hard for some math major to solve. Once you figure out the formula then we could see how likely it would be for you to know people in various social circles, like say 50 for close friends, family, and work colleagues. Bonus question: would the fact that survivors were grouped into ships have a significant effect on the likelihood of inter ship connections?","['graph-theory', 'probability']"
2291910,Is the measurable transformation of stationary ergodic sequence also ergodic?,"Let $X_n$ be a stationary and ergodic sequence. Let function $f:R \to R $ be measurable. Is it true that the sequnce $f(X_n)$ would be also ergodic? If no, than for which $f$ it holds? (Continuous? Bounded?) I understand that this question should be basic, but was not able to find anywhere the idea of how to proof it.","['stationary-processes', 'probability-theory', 'ergodic-theory']"
2291911,How to show that a large given integer isn't a square number by using mod?,"Given a big integer like $4531893869$ the question is how to show that this number isn't a square number using mod $11$. Basically what we have is something like that:
$$ 4531893869 \equiv x \pmod{11}$$
and we know that $ x \in \{1\ldots10\}$. According to Wolfram Alpha the result is $8$. But I don't understand how this will show us that this large number isn't a square number. I have to explicitly use mod $11$ to solve this task but I don't understand the mathematical context.","['discrete-mathematics', 'modular-arithmetic', 'elementary-number-theory']"
2291947,Prove that $1+p$ is an element of order $p^{n-1}$ in $(\Bbb Z/p^n\Bbb Z)^\times$ using the binomial theorem,"Prove that $1+p$ is an element of order $p^{n-1}$ in $(\Bbb Z/p^n\Bbb Z)^\times$, where $p$ is an odd prime and $n$ a positive integer. Yeah, I'm aware of this: Showing $1+p$ is an element of order $p^{n-1}$ in $(\mathbb{Z}/p^n\mathbb{Z})^\times$ , but I want to construct a proof using the BINOMIAL THEOREM. Since $x^{p^n}=1$ implies $|x|=p^m$ for some $m\le n$, if we can show that $(1+p)^{p^{n-1}}\equiv1 \space(mod \space p^n)$ and $(1+p)^{p^{n-2}}\not\equiv1 \space(mod \space p^n)$, then the result follows. Let's take an example: let $p=7$, $n=5$ $(1+7)^{7^{5-1}}=(1+7)^{7^4}=1+C^{7^4}_17+C^{7^4}_27^2+C^{7^4}_37^3+C^{7^4}_47^4+C^{7^4}_57^5+..$ $\equiv1+C^{7^4}_17+C^{7^4}_27^2+C^{7^4}_37^3+C^{7^4}_47^4$ $\equiv1+(7^4i_1)7+(7^3i_2)7^2+(7^2i_3)7^3+(7i_4)7^4$ $\equiv0 \space (mod \space 7^5)$ for some integers $i_1,i_2,i_3,i_4$ so it seems I have to show that $p^{n-k}|C^{p^{n-1}}_k$, but I have no idea how to. Can somebody help? Thanks!","['abstract-algebra', 'binomial-theorem', 'divisibility']"
2291952,Degrees of Fundamental Invariants of Coxeter Groups $A_n$,"I think I misunderstood something simple but not sure what. According to https://en.wikipedia.org/wiki/Coxeter_element , The invariants of the Coxeter group acting on polynomials form a polynomial algebra whose generators are the fundamental invariants; their degrees are given in the table above The Coxeter group $A_{n}$ is a group generated by reflections in the hyperplans defined by $e_{i} - e_{i+1} \in \mathbb{C}^{n+1}$ where $\{e_i\}$ is a orthonormal basis of $\mathbb{C}^{n+1}$. So $A_{n}$ permutes coordinates of $\mathbb{C}^{n+1}$ and it is in fact isomorphic to the symmetric group $S_{n+1}$. Therefore the $A_{n}$-invariant polynomials of $\mathbb{C}[X_1,\dotsc,X_{n+1}]$ are elementary symmetric polynomials
\begin{equation}
Y_1 = X_1X_2\dotsm X_{n+1}, \qquad \dotsc \qquad Y_{n+1} = X_1 + X_2 + \dotsb + X_{n+1}.
\end{equation}
So the degrees of fundamental invariants should be $n+1,n,\dotsc,2,1$ but the same wikipedia page (and every other sources I checked) will say these degrees are $n+1,n,\dotsc,2$. Apparently we will always restrict the action of $A_n$ to the subspace $X_1 + \dotsb + X_{n+1} = 0$ in $\mathbb{C}^{n+1}$ and ignore $Y_{n+1}$ but why is that? Is $Y_{n+1}$ not a valid invariant polynomial of $A_n$?","['representation-theory', 'group-theory', 'coxeter-groups']"
2291997,Proving that $ \ln\left(\frac{49}{50}\right)<\sum^{98}_{k=1}\int^{k+1}_{k}\frac{k+1}{x(x+1)}dx<\ln(99)$,If $\displaystyle I = \sum^{98}_{k=1}\int^{k+1}_{k}\frac{k+1}{x(x+1)}dx.$ Then prove that $\displaystyle \ln\left(\frac{49}{50}\right)<I <\ln(99)$ Attempt: $$I = \sum^{98}_{k=1}\int^{k+1}_{k}(k+1)\bigg[\frac{1}{x(x+1)}\bigg]dx = \sum^{98}_{k=1}\int^{k+1}_{k}(k+1)\bigg[\frac{1}{x}-\frac{1}{x+1}\bigg]dx$$ So $$I = \sum^{98}_{k=1}(k+1)\bigg[\ln(x)-\ln(x+1)\bigg]\bigg|_{k}^{k+1}$$ $$I= \sum^{98}_{k=1}(k+1)\bigg[\bigg(\ln(k+1)-\ln(k+2)\bigg)-\bigg(\ln(k)-\ln(k+1)\bigg)\bigg]$$ $$ \sum^{98}_{k=1}(k+1)\ln(k+1)-k\ln(k)-\sum^{98}_{k=1}(k+1)\ln(k+2)-k\ln(k+1)+\sum^{98}_{k=1}\ln(k+1)-\ln(k)$$ So we have $$I = \ln(2)+\ln \bigg(\frac{99}{100}\bigg)^{100}$$ could some help me how to prove $$\ln\left(\frac{49}{50}\right)<I <\ln(99)$$,"['integration', 'calculus']"
2292001,"If $I^n = 0$ and $M$ is a simple module, then $IM = 0.$","Let $A$ be a ring with unity, and let $I$ be a two sided ideal of $A$ such that $I^n = 0.$ Let $M$ be a simple $A$-module. I want to show that $IM = 0.$ We know that $IM$ is a submodule of $M$. Suppose $IM = M.$ Then $I^2M = I(IM) = IM = M,$ and so $I^nM = M.$ But $I^nM = \{0\}M = 0,$ but $M$ cannot be $0$ as it is simple, hence nontrivial, a contradiction. So $IM = 0.$ This seems too simple to me (this question was on a past paper, and my solution seems much shorter than the number of marks), so I was wondering if it was correct. Particularly, I was wondering if $I^2M = I(IM)$ actually works? I can't see how it shouldn't work from the definitions, but I can never be too sure about products of ideals and modules.","['abstract-algebra', 'ring-theory', 'modules', 'noncommutative-algebra']"
2292002,Adding a constant to an exponential random variable,"Let $θ$ be an unknown constant. Let $W_1,…,W_n$ be independent exponential random variables each with parameter 1. Let $X_i=θ+W_i$. How do I calculate the distribution of $X_1$, given θ? Notice that I don't look for a conditional probability here since $θ$ is a constant. My guess is that $P_X(x;θ)= e^{-(x_1-θ)}$ but I'm not sure. Thank you.","['exponential-distribution', 'probability', 'random-variables']"
2292031,independent set of vertices in a directed graph,"I'm trying to prove the following statement: Every directed graph $G$ has an independent set $S$ such that any other vertex in G can be reached with a directed path of length $\leq 2$ from a vertex in $S$. So far, I've tried removing a vertex $v$ from the graph and considering cases where the vertex is in $S$, $A = \{ \text{the set of vertices 1 away from } S \}$ or $B = \{ \text{the set of vertices 2 away from } S \}$. But I'm not getting anywhere.","['combinatorics', 'graph-theory', 'directed-graphs']"
2292043,"Does the following series converge uniformly on $(0,+\infty)$?","I would like you to take a look at the following function-series. This is a problem I came across in Real Analysis some months ago and I had left it aside, but then I forgot about it. Note that Weierstrass's criterion is not applicable here and I can't seem to find a factorization that would make either Abel's or Dirichlet's criterion applicable. $\displaystyle{s(x)=\sum_{n=1}^{\infty}2^n\sin\left(\frac{1}{3^nx}\right)}$, for $x \in (0,+\infty)$. By the $\sin(x)<x$ inequality, it is obvious that it converges pointwise. It is also quite obvious that it converges uniformly in any $[\alpha,+\infty)$, where $\alpha>0$. What about $(0,+\infty)$ though?","['real-analysis', 'uniform-convergence', 'calculus', 'functions', 'sequence-of-function']"
2292095,Applications of some Differential Equation,"I was practicing some questions to get a grip of differential equation when I stumbled upon this problem. It looks like this: Tank A initially holds 100 gallons of brine that contains 100 pounds of salt, and tank B holds 100 gallons of water. Two gallons of water enter A each minute, and the mixture, assumed uniform, flows from A into tank B at the same rate. If the resulting mixture, also kept uniform, runs out of B at the rate of 2 gallons per minute, how much salt is in tank B at the end of 1 hour? It seems confusing......to build a differential equation that would describe that statement above. How do you answer it?",['ordinary-differential-equations']
2292102,Compute the gradient of a vector,"I have to compute the following expression: $$ \frac{ \mathrm{d} (\mathbf{x}- \mathbf{\mu})^T \Sigma^{-1} (\mathbf{x}- \mathbf{\mu})}{\mathrm{d} \mathbf{\mu}} $$ where $\mathbf{x}$ and $\mathbf{\mu}$ are a column vectors, $\Sigma^{-1}$ is a matrix. I tried to do it component-wise and decomposing the matrix product in sum of products: $$ \frac{ \mathrm{d} \sum_k (\sum_j (\mathbf{x}_j - \mathbf{\mu}_j) {\Sigma^{-1}}_{j})_k (\mathbf{x}_k - \mathbf{\mu}_k)}{\mathrm{d} \mathbf{\mu}} $$ and then selecting only one component of $\mathbf{\mu}$: $$ \frac{ \mathrm{d} \sum_k (\sum_j (\mathbf{x}_j - \mathbf{\mu}_j) {\Sigma^{-1}}_{j})_k (\mathbf{x}_k - \mathbf{\mu}_k)}{\mathrm{d} \mathbf{\mu}_k}  =  \mathbf{\mu}_k (\sum_j (\mathbf{x}_j - \mathbf{\mu}_j) {\Sigma^{-1}}_{j})_k - (\mathbf{x}_k - \mathbf{\mu}_k) {\mathbf{\mu}_k \Sigma^{-1}}_k $$ What is the best way to compute this derivation? Could you show the passages?",['multivariable-calculus']
2292115,Using Residue Theorem to evaluate Trigonometric Integral $I=\frac{1}{\pi}\int_0^\pi\frac{F(x)}{a^2+b^2(\cos x-\cos y)^2}dx$,"I'm trying to evaluate the following integral: $$I=\frac{1}{\pi}\int_0^\pi\frac{F(x)}{a^2+b^2(\cos x-\cos y)^2}dx ,\quad 0<y<\pi,$$ in the regime $a\ll b$, using Residue Theorem. I'm assuming that $F(x)$ is a well-behaved function with no singularities in the interval $[0,2\pi]$. I've tried the standard approach by defining $z=e^{ix}$ and $z_0=e^{iy}$ to convert the integral into the form $$I=-\frac{2i}{\pi}\oint_{|z|=1}\frac{z_0^2\,z\,F[-i \log (z)]}{4 a^2 \,z_0^2\,z^2+b^2 (z_0-z)^2 (z_0\,z-1)^2}dz.$$ In the regime considered, the first term in the denominator of the above expression can be neglected. The poles will be then $z=z_0$ and $z=\frac{1}{z_0}$, both of them with multiplicity 2. And here's my problem: both singularities relies on the unity circle $|z|=1.$ How can I proceed from here or what should be the best approach to deal with this integral? I was expecting to be able to find an answer that depends on $F(y)$ or $F^\prime(y)$. But that's only a guess. This is my very first question in stackexchange. I hope I had followed all the instructions properly.","['complex-analysis', 'trigonometric-integrals', 'residue-calculus']"
2292127,Prove that function $f$ is injective if its Jacobian matrix is positive definite.,"Assume that $\Omega\in\mathbb{R}^m$ is an open convex set and the vector-valued function $f:\Omega\rightarrow\mathbb{R}^m$ is differentiable. If Jacobian matrix $J_f(x)$ is positive definite for all $x\in\Omega$, prove that $f$ is an injective function on $\Omega$. I have no way of dealing with it. Is there a theorem to do with it?","['derivatives', 'real-analysis', 'linear-algebra', 'calculus']"
2292135,"Prob. 24, Chap. 5 in Baby Rudin: For $\alpha>1$, let $f(x) = (x+\alpha/x)/2$, $g(x) = (\alpha+x)/(1+x)$ have $\sqrt{\alpha}$ as their only fixed point","Here is Prob. 24, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: The process described in part (c) of Exercise 22 can of course also be applied to functions that map $(0, \infty)$ to $(0, \infty)$. Fix some $\alpha > 1$, and put $$ f(x) = \frac{1}{2} \left( x + \frac{\alpha}{x} \right), \qquad g(x) = \frac{\alpha+x}{1+x}. $$
  Both $f$ and $g$ have $\sqrt{\alpha}$ as their only fixed point in $(0, \infty)$. Try to explain, on the basis of properties of $f$ and $g$, why the convergence in Exercise 16, Chap. 3, is so much more rapid than it is in Exercise 17. (Compare $f^\prime$ and $g^\prime$, draw the zig-zags suggested in Exercise 22.) Do the same when $0 < \alpha < 1$. Here are the links to my posts here at Math SE on Prob. 22, Chap. 5, Prob. 16, Chap. 3, and Prob. 17, Chap. 3, in Baby Rudin, 3rd edition: Prob. 22, Chap. 5 in Baby Rudin: Fixed Points of Real Functions Prob. 16, Chap. 3 in Baby Rudin: $x_{n+1} = (x_n + \alpha/x_n)/2$, with $x_1 > \sqrt{\alpha}$, $\alpha > 0$ Prob. 17, Chap. 3 in Baby Rudin: For $\alpha > 1$, how to obtain these inequalities from this recurrence relation? My Attempt: We note that, for $0 < x < \infty$, $$ g(x) = 1 + \frac{\alpha-1}{x+1},$$
  and so 
   $$f^\prime(x) = \frac{1}{2} \left( 1 - \frac{\alpha}{x^2} \right), \qquad g^\prime(x) = - \frac{\alpha-1}{(x+1)^2},$$ 
  and for $\alpha > 1$, we see that, if $x > \sqrt{\alpha}$, then $x^2 > \alpha$, and so 
  $$0 <  f^\prime(x) < \frac{1}{2}.  $$
  which implies (by part (c) of Prob. 22, Chap. 5, in Baby Rudin, 3rd edition) that the sequence $x_0 > \sqrt{\alpha}$, $x_{n+1} = f \left( x_n \right)$, for $n = 0, 1, 2, 3, \ldots$, does converge to the only fixed point of $f$, which is $\sqrt{\alpha}$. In fact, for all $n$, we have 
  $$ \left| x_{n+1} - x_n \right| \leq \frac{1}{2} \left| x_n - x_{n-1} \right| \leq \cdots \leq \frac{1}{2^n} \left| x_1 - x_0 \right|,  $$ 
  and so for any $m < n$, we have 
  $$
\begin{align}
\left| x_n - x_m \right| &\leq \left| x_n - x_{n-1} \right| + \cdots+ \left| x_{m+1} - x_m \right| \\
&\leq \left( \frac{1}{2^{n-1}} + \cdots + \frac{1}{2^m} \right) \left| x_1 - x_0 \right| \\
&= \frac{1}{2^m} \left( 1 + \cdots + \frac{1}{2^{n-m-1}} \right) \left| x_1 - x_0 \right| \\
&= \frac{1}{2^m} \frac{ 1 - \frac{1}{2^{n-m}} }{ 1 - \frac{1}{2} } \left| x_1 - x_0 \right| \\
&= \left( \frac{1}{2^{m-1}} - \frac{1}{2^{n-1}} \right) \left| x_1 - x_0 \right|,
\end{align}
$$
  and upon letting $n \to \infty$, while keeping $m$ fixed, we obtain
  $$ \left| x_m - x \right| \leq \frac{1}{2^{m-1}} \left| x_1 - x_0 \right| = \frac{1}{2^{m-1}} \left| \frac{1}{2} \left(x_0 + \frac{\alpha}{x_0} \right) - x_0 \right| = \frac{1}{2^m } \left( x_0 - \frac{\alpha}{x_0} \right), $$
  for $m = 1, 2, 3, \ldots$, which gives the rate of convergence of this recursive algorithm. And, the similar situation occurs for $0 < \alpha < 1$ as well, provided we choose $x_0 > \sqrt{\alpha}$. Is my analysis correct? Or, have I erred anywhere or missed something of substance? And, what about $g$? How to analyze it? How to show what Rudin has demanded to be shown?","['real-analysis', 'recursive-algorithms', 'numerical-methods', 'convergence-divergence', 'analysis']"
2292168,Absolute convergence of Dyson series for evolution operator,"In this paper https://arxiv.org/abs/math-ph/9901018 the author (on page 4) mentions : ""...a standard argument for the absolute convergence of the Dyson series for [the evolution operator] $\hat{U}_{\tau}$ ..."" . Could you please give me a hint of what is a standard way of proving the absolute convergence of the Dyson series for the evolution operator? References are welcome. (I am not a mathematician, but a physicist)","['quantum-mechanics', 'ordinary-differential-equations', 'mathematical-physics']"
2292174,Upper-semicontinuous function can be bounded above by a continuous function.,"Problem: Show that for every real-valued upper-semicontinuous function  $f$: $\mathbb {R} \to \mathbb {R}$ there is a continuous function $g$ : $\mathbb {R} \to \mathbb {R}$ such that $f \leq g$. I constructed a few functions, one of them being $g(x)=\max_{x \in [0, x]}f(x)$  for $x \geq 0$ and analogously for negative values but this is not continuous. Please help I am struggling with this for hours.","['real-analysis', 'functions', 'continuity', 'general-topology', 'metric-spaces']"
2292194,Confused about developing a Taylor series of a function that implicates an integral,"There is a function that makes me confused: $$f(x)=\int_{\frac{\pi}{2}}^x \frac{\cos(t)}{t-\frac{\pi}{2}}~dt$$ The question wants me to find its Taylor series centered in $a=π/2$ and I don't know how, I tried separating $\cos(t)$ of $\dfrac{1}{t-π/2}$ and make the MacLaurin series of $\cos(t)$, and then multiply the series by $\dfrac{1}{t-\frac{π}{2}}
~dt$ . 
But at the end, it doesn't end as a Taylor series centered in $π/2$, it's more like a MacLaurin series : I found 
$$\sum_{n=0}^{\infty} \frac{t^n}{n!\cdot (t-\frac{π}{2})}$$ Please could you help me with this series ? Thank you a lot !","['integration', 'sequences-and-series']"
2292211,Nonlinear parallel transport: preserving level sets of an arbitrary function,"Motivation: Consider a smooth vector bundle $\pi:E\to M$, a fiber metric $g$, and an affine connection $\nabla$ compatible with $g$. Then the parallel transport $\Pi_{\gamma,t_0,t}$ determined by $\nabla$ is an isometry of the fibers of $E$. Consider the smooth function $f:E\to \mathbb{R}$ on a vector bundle defined by $f(v):= g(v,v)$. In particular, parallel transport yields a smooth family of smooth maps $\Pi_{\gamma,t_0,t}:E_{\gamma(t_0)}\to E_{\gamma(t)}$ satisfying $$f \circ \Pi_{\gamma,t_0,t} = f,$$
for any path $\gamma:[t_0,t]\to M$. Question: Suppose that $\pi:E\to M$ is a smooth fiber bundle and $f:E\to \mathbb{R}$ is an arbitrary smooth function. When is it possible to find, for any path $\gamma:[t_0,t]\to M$ a smooth family of smooth maps $\Pi_{\gamma,t_0,t}:E_{\gamma(t_0)}\to E_{\gamma(t)}$ satisfying $$f \circ \Pi_{\gamma,t_0,t} = f?$$ I.e., are there natural conditions that may be imposed on $f$ to ensure that this is possible? Idea: If the level sets of $f$ are all manifolds and there exists a smooth Ehresmann connection on $E$ such that the corresponding horizontal bundle $\mathcal{H}$ is contained in the distribution $\ker df$, then I think the parallel transport determined by this connection would solve the problem. But it isn't clear to me when such an Ehresmann connection exists. Evidently such a (linear) Ehresmann connection exists for the case of $E$ a vector bundle and $f(v) = g(v,v)$ described above.","['reference-request', 'differential-geometry', 'differential-topology']"
2292265,"Given two real numbers $a$ and $b$ such that $a<b$, what about the convergence of these two sequences?","Let $a$ and $b$ be two given real numbers such that $a < b$, and let $\left\{x_n\right\}$ and $\left\{ y_n \right\}$ be the sequences defined as follows: 
Let us choose $x_1$ and $y_1$ such that $$a < x_1 < b, \qquad a < y_1 < b$$ arbitrarily, and then let 
$$x_2 = \frac{a+x_1}{2}, \qquad y_2 = \frac{y_1 + b}{2},$$ 
$$x_3 = \frac{a + x_1 + x_2 }{3}, \qquad y_3 = \frac{ y_1 + y_2 + b}{3},$$
and so on 
$$ x_n = \frac{a+ x_1 + \cdots + x_{n-1} }{n}, \qquad y_n = \frac{ y_1 + \cdots + y_{n-1} + b}{n} $$
for $n= 3, 4, 5, \ldots$. Then what can we say about the convergence of these sequences? To generalize this problem a little further, let $\left\{ r_n \right\}$ be a given sequence of positive real numbers, and let us now define $$x_2 = \frac{r_1 x_1 + a r_2}{r_1 + r_2}, \qquad y_2 = \frac{r_1 y_1 + r_2 b}{r_1 + r_2},$$ 
$$x_3 = \frac{r_1 x_1 + r_2 x_2 + r_3 a }{r_1 + r_2 + r_3}, \qquad y_3 = \frac{ r_1 y_1 + r_2 y_2 + r_3 b}{r_1 + r_2 + r_3 },$$
and so on 
$$ x_n = \frac{r_1 x_1 + \cdots + r_{n-1} x_{n-1} + r_n a }{r_1 + \cdots + r_n }, \qquad y_n = \frac{ r_1 y_1 + \cdots + y_{n-1} + r_n b}{r_1 + \cdots + r_n} $$
for $n= 3, 4, 5, \ldots$. Then what can we say about the convergence of these sequences? What if we proceed as follows? Let $r_0 > 0$ be given, and let 
$$x_2 = \frac{r_0a+ r_1 x_1}{r_0 + r_1 }, \qquad y_2 = \frac{ r_1 y_1 + r_0 b}{r_1 + r_0},$$ 
$$x_3 = \frac{r_0 a + r_1 x_1 + r_2 x_2 }{r_0 + r_1 + r_2}, \qquad y_3 = \frac{ r_2 y_2 + r_1 y_1 + r_0 b}{r_2 + r_1 + r_0},$$
and so on 
$$ x_n = \frac{r_0 a + r_1 x_1 + \cdots + r_{n-1} x_{n-1} }{r_0 + \cdots + r_{n-1} }, \qquad y_n = \frac{r_{n-1} y_{n-1} + \cdots +  r_1 y_1 + r_0 b}{r_{n-1} + \cdots + r_0} $$
for $n= 3, 4, 5, \ldots$. What can we say about the convergence of these sequences now? I can handle the situation only if we have only unit weights and only average of two terms is involved at a time, but I simply have no idea of what happens in this case!! So, I would be really grateful for a detailed answer!","['real-analysis', 'sequences-and-series', 'calculus', 'convergence-divergence', 'analysis']"
2292266,Doubt about the definition of limit in two variables,"In this discussion Finding $\lim_{(x,y)\to(0,0)}\frac{x^2y}{x^3+y}$ I found that we can consider paths that don't belogs to the domain of $f(x,y)$ to prove that a limit doesn't exist, but my teacher would not agree. I propose to you the definition of limit that I know. Let $f:\mbox{dom}(f)\subset\mathbb{R}^2\to\mathbb{R}$ and $(x_0, y_0)$ an accumulation point of $\mbox{dom}(f)$. We say that $$\lim_{(x,y)\to (x_0, y_0)}f(x,y)=L$$ if and only if $\forall\varepsilon>0, \ \exists\delta>0$ such that if $(x,y)\in \left(B_{\delta}(x_0,y_0)-\{(x_0,y_0)\}\right)\cap\mbox{dom}(f)$ than $|f(x,y)-L|<\varepsilon$ To show that a limit doesn't exist, I have to find two path $P_1(x,y), P_2(x,y)$ such that $$P_1(x,y), \ P_2(x,y)\in\mbox{dom}(f)\ \ \ \mbox{locally}$$ and $$\lim_{(x,y)\to (x_0, y_0)}P_1(x,y)=(x_0, y_0)\wedge \lim_{(x,y)\to (x_0, y_0)}P_2(x,y)=(x_0, y_0)$$ but $$\lim_{(x,y)\to (x_0, y_0)}f(P_1(x,y))=\ell_1\wedge \lim_{(x,y)\to (x_0,y_0)}f(P_2(x,y))=\ell_2$$ with $\ell_1\ne \ell_2$. In the discussion that i linked, I discovered that I can choose all possible path... but this is strange to me, and I'm now confused. Please help me to understand. Thank you.","['real-analysis', 'calculus', 'limits']"
2292324,Confusion about proving $\frac{1}{1\cdot2} + \frac{1}{2\cdot3} + \frac{1}{3\cdot4} + \dotsb + \frac{1}{n(n+1)} = \frac{n}{n+1}$ by induction,"I know what the answer to this question is, but I am not sure how the answer was reached and I would really like to understand it! I am omitting the base case because it is not relevant for my question. Inductive hypothesis: $$\frac{1}{1\cdot2} + \frac{1}{2\cdot3} + \frac{1}{3\cdot4} + \dotsb + \frac{1}{n(n+1)} = \frac{n}{n+1}$$ is true when $n = k$ and $k > 1$ Therefore: $$\frac{1}{1\cdot2} + \frac{1}{2\cdot3} + \frac{1}{3\cdot4} + \dotsb + \frac{1}{k(k+1)} = \frac{k}{k+1}$$ Inductive step: Prove that $$\frac{1}{1\cdot2} + \frac{1}{2\cdot3} + \frac{1}{3\cdot4} + \dotsb + \frac{1}{k(k+1)} = \frac{k+1}{k+1+1} = \frac{k+1}{k+2}$$ $$\frac{1}{1\cdot2} + \frac{1}{2\cdot3} + \frac{1}{3\cdot4} + \dotsb + \frac{1}{k(k+1)} = \left[\frac{1}{1\cdot2} + \frac{1}{2\cdot3} + \frac{1}{3\cdot4} + \dotsb + \frac{1}{k(k+1)}\right] + \frac{1}{(k+1)(k+2)}$$ $$\frac{1}{1\cdot2} + \frac{1}{2\cdot3} + \frac{1}{3\cdot4} + \dotsb + \frac{1}{k(k+1)} = \frac{k}{k+1} + \frac{1}{(k+1)(k+2)}$$ $$\frac{1}{1\cdot2} + \frac{1}{2\cdot3} + \frac{1}{3\cdot4} + \dotsb + \frac{1}{k(k+1)} = \frac{k+1}{k+2}$$ What I am confused about is where the $\frac{1}{(k+1)(k+2)}$ comes from in the first line of the inductive step. Can someone please explain this in a little more detail? The source of the answer explains it as ""break last term from sum"", but I am unclear on what that means.","['induction', 'discrete-mathematics']"
2292341,Generators of integers modulo n under multiplication,"I was shown an alternate way of finding the generators of $Z^*_5 = Z_5 - \{0\}$ (i.e. the integers greater than $0$ modulo $n$) using Lagrange's Theorem opposed to calculation by hand. The steps are thus: We have already shown that 2 is a generator. $ord(Z^*_5) = 4$ Using Lagrange's Theorem: $ord(3) \; | \; 4$ i.e. the order of the subgroup generated by $3$ divides the order of the full group. $2^3 \equiv 3  \; mod \;5 \;$ so $ \; ord(2^3) = ord(3)$ $3^{ord(3)} \equiv 2^{3ord(3)}  \equiv 1 \; mod \; 5 $ Thus far I understand entirely what is going on. However, the next two steps, whilst I understand the maths i.e. I can work out the numbers I am not entirely sure why they are performed: $3 \; ord(3) \equiv ord(2) \; mod \; 4$ As $3$ and $4$ are co-prime, we can deduce that $ ord(3) \equiv 4 \equiv ord(Z^*_5)$ i.e. $3$ is a generator of $Z^*_5$. So I have 3 questions: In the case of step 6 my question is: why is this significant? Why $mod \; 4 \;$ and not $ \; mod \;5  \;$ ? How does this help us find the generator? In the case of step 7 my question is: why does the fact they are co-prime imply this and why did we have to do all the prior working? Is this a property of such groups? If a number is co-prime with the order of the group is it always a generator? Thank you for any answers, I've been looking through some textbooks but I couldn't find a comprehensive discussion of this particular case.","['abelian-groups', 'group-theory', 'discrete-mathematics']"
2292356,Why do we need isogonal trajectories?,"Could you, please, give me some examples, where we really need isogonal, but not orthogonal, trajectories? I know, how to find them using differential equations. I would like to see some examples in physics or engineering or somewhere else. Thanks in advance!","['ordinary-differential-equations', 'differential-geometry']"
2292391,Sheafification construction - Liu's book,"I am doing an exercise in Qing Liu's Algebraic Geometry and Arithmetic Curves on an alternative definition of the sheafification of a presheaf but can't see why the gluing property holds. More precisely, let $\mathcal{F}$ be a presheaf (of rings say) on a topological space $X$. Fix a cover $\mathcal{U}=\{U_i\}$ of $X$ then we get a complex $$0 \rightarrow \mathcal{F}(X) \overset{d_0}\rightarrow \prod\limits_i\mathcal{F}(U_i) \overset{d_1}\rightarrow \prod\limits_{i,j} \mathcal{F}(U_{ij}),$$ where $U_{ij} =U_i \cap U_j$, $d_0(s)=(s|_{U_i})_i$ and $d_1((s_i)_i)=(s_i|_{U_{ij}}-s_j|_{U_{ij}})_{i,j}$. Define $\mathcal{F}_{\mathcal{U}}(X)=\ker d_1$ and extend to all open sets $V \subset X$ with the cover $\{U_i \cap V\}$ to define the presheaf $\mathcal{F}_{\mathcal{U}}$. By refining our cover of $X$, we get a directed set and define $\mathcal{F}'(V)= \lim\limits_{\rightarrow} \mathcal{F}_{\mathcal{U}}(V)$ to be the direct limit across the covers. I want to show that this is actually a sheaf but can't seem to verify the gluing axiom. I know that this should actually be the sheafification and all the stalks are therefore equal but I don't see how to use this. Any help would be much appreciated.","['sheaf-theory', 'algebraic-geometry']"
2292398,Proof of the Weighted Generalized Inverse matrix,"I am having trouble understanding how to get the weighted generalized inverse of a matrix. Let me start from the beginning. Suppose $$a=Xb$$ Where $a$ is a vector with m elements, $b$ is a vector of n elements and $X$ is a matrix with mxn elements. Solving for $b$ we get $$b={X}^{+}a$$ Where ${X}^{+}$ is the Moore-Penrose inverse defined as $$X^+ = X^T (XX^T)^{-1}$$ Where $X^T$ is the transpose of $X$. The weighted generalized inverse is given by $$X_W= {W}^{-1}X^T (X{W}^{-1}X^T)^{-1}$$ Making the weighted solution of $b$
$$b_W= {W}^{-1}X^T (X{W}^{-1}X^T)^{-1}a$$ The proofs that I have found start from places that I don't understand how they end up there to begin with. I probably don't know something relatively basic, but can someone tell me where I can find the proof without taking any assumption for granted? Any help would be appreciated.",['matrices']
2292413,"Find all ideals $\mathbb{Z}[x,y]$ such that the corresponding factor ring is a UFD","I'm working on the following problem in Ring Theory: $\mathbb{Z}[x,y]/\left<y+1\right>$ is a UFD. This made me wonder if it is possible to classify all ideals $I$ in $\mathbb{Z}[x,y]$ such that $\mathbb{Z}[x,y]/I$ is a UFD?","['polynomials', 'abstract-algebra', 'unique-factorization-domains', 'ring-theory', 'ideals']"
2292415,When is a ruled parametrisation an actual parametrisation?,"I'm trying to get an answer to the following question: When is a ruled parametrization $ r(s, t) = c(s) + tv(s) : \mathbb{R}^2 \to \mathbb{R}^3 $ a parametrization? What constraints must $ c, v $ satisfy? This altogether is a vague question, so to clarify, I will assume the following definition"" A smooth map $ r : \mathbb{R}^2 \to \mathbb{R}^3 $ is called a parametrization if its derivative has rank $ 2 $ at every point This leads to the obvious observation that, in case of ruled surfaces, the following vectors: $$ \dot{c}(s) + t\dot{v}(s) \text{ and } v(s)$$ need be linearly independent. However, I am wondering if there is any deeper sense to it? How do I interpret this? I would love some feedback on it. Is this definition of parametrization the standard one?","['parametrization', 'differential-geometry', 'surfaces']"
2292416,How to calculate this surface area? (portion of a cylinder inside a sphere ),"The surface area of ​​the portion of the cylinder $x^2+y^2=8y$ located inside of the sphere $x^2+y^2+z^2=64$ I'm stuck, so any tip will be helpful Thanks in advance!",['multivariable-calculus']
2292419,"Prove that if $5^n$ begins with $1$, then $2^{n+1}$ also begins with $1$","Prove that if $5^n$ begins with $1$, then $2^{n+1}$ also begins with $1$ where $n$ is a positive integer. In order for a positive integer $k$ to begin with a $1$, we need $10^m \leq k < 2 \cdot 10^m$ for some positive integer $m$. Thus since $5^n$ begins with a $1$, we have $10^m \leq 5^n < 2 \cdot 10^m$ for some positive integer $m$. Then multiplying by $\dfrac{2^{n+1}}{5^n}$ we get $10^m \cdot \dfrac{2^{n+1}}{5^n} \leq 2^{n+1} < 10^m \cdot \dfrac{2^{n+2}}{5^n}$, but I didn't see how this helped.",['number-theory']
2292422,How can $G$ a simple group always be isomorphic to $Z_p$ for some prime $p$?,"Is it not a necessary condition that the order of two groups must be equal for them to be isomorphic? Does ""$G$ is a simple group of odd order"" somehow imply ""$|G|$ is prime""?"" If not, I don't see how it can't be the case that $|G|$ is some odd composite number, so it couldn't be isomorphic to any $Z_p$ where $p$ is prime.","['abstract-algebra', 'group-theory']"
2292456,"Prove that surfaces $x + 2y – lnz + 4 = 0$ and $x^2 - xy – 8x + z + 5 = 0$ are tangent at $(2,-3,1)$.","$x + 2y – ln(z) + 4 = 0$ $x^2 - xy – 8x + z + 5 = 0$ $\nabla [1,2,\frac{-1}{z}] $ $\nabla [2x-y-8,-x,1] $ $\nabla(P0) [1,2,-1] $ $\nabla(P0) [-1,-2,1] $ I've stuck at this point and i don't know what to do next.","['multivariable-calculus', 'real-analysis']"
2292472,Evaluate $\int_{|z|=1} \frac{1}{z^2 -\frac{3}{2}z + 1} dz$,Evaluate : $$\int_{|z|=1} \frac{1}{z^2 -\frac{3}{2}z + 1} dz$$ Using residue method : $$z=\frac{3}{4} \pm i \frac{\sqrt{7}}{4}$$ The problem is the two roots on the boundary $|z|=1$,"['complex-analysis', 'integration', 'residue-calculus']"
2292521,Why is the expected frequency during a chi square dependence test calculated the way that it is?,"I understand the chi square test for testing whether or not a certain model is appropriate. I understand the process based upon which we pick the expected values. But, when it comes to the dependence test (the one where we use a contingency table), I don't understand why the expected frequency is  calculated from the observed frequencies in the contingency table using (row total x column total)/grand total. Someone please explain.","['statistics', 'probability-distributions']"
2292541,How to take $\int_0^{+\infty} \frac{x^2+1}{x^4+1}dx$?,"The integral: $$\int_0^{+\infty} \frac{x^2+1}{x^4+1}dx$$ If num were greater than denum I would just devide it normally with long division, but it is not, how should I handle it then?","['improper-integrals', 'calculus']"
2292611,"""Visualizing'' rotation in even dimensions","So as we know, a linear transformation $\Bbb R^n\to\Bbb R^n$ must have an even number of non-real complex eigenvalues. One consequence of this is that, in $4$ dimensions, we cannot talk about rotation about a line — the only non-trivial rotation fixes a plane . Since we can't visualize $4$ dimensions, I was trying to think of a way to interpret these rotations. One useful way is to imagine a $3$ dimensional space where each point has a fourth coordinate, which we can interpret as something like ""temperature"" of the point. After playing around for a bit, I realized that rotations around a plane in this world look like stretching along one axis. For example, if we fix the $x$-$y$ plane, then the point $(0,0,1)$ might become $(0,0,2)$. Then the temperature of that point would have to decrease correspondingly. It is probably a coincidence, but in that case, a rotation looks like compressing/decompressing a gas. As far as I can tell, nothing about $6$ dimensions makes this any more interesting. You just have to add in some other properties (density, color) and you have the same basic picture. What other methods do you know for interpreting rotation about a plane? Without getting too much into physics, what is the connection between rotation in $4$ dimensions and our apparently $3$ dimensional world?","['rotations', 'visualization', 'linear-algebra']"
2292631,Intuition for Azuma inequality proof,"How could one come up with the proof of Azuma inequality? I'm looking for intuition for the proof.
I'm reading the proof provided here: http://willperkins.org/6221/slides/azuma.pdf I have a martingle $X_1,X_2...X_m$with small differences, why should I think of bounding $E[e^{a{X_m}}]$? Why not some other function? After making this steps I think I understand why we do the rest of the stuff, but this intial step... Thanks","['intuition', 'probability-theory']"
2292655,"Why if any path going toward a point yields the same limit, then limit at that point exist?","We have the definition of limit of multivariable function. Basically, for any given $t$, we can find a $q$, so that for any point, whose distance to the point where we want to calculate the limit is less than $q$, $t>|f-L|$. Then we say $L$ is its limit. There is another way to state that the limit exists. That is, for any curve going toward the point, they have to yield the same limit. So my question is: why is ""any curve going toward the point has the same limit"" equivalent to ""limit exists""?","['multivariable-calculus', 'calculus']"
2292687,Maximization in Two Variables,"I have not yet had the privilege of studying multivariable calculus, but I have made an educated guess about how to find the minimum or maximum of a function with two variables, for example, $x$ and $y$. Since, in three dimensions, a minimum or maximum would be represented by a tangent plane with no slope in any direction, could I treat $y$ as a constant and differentiate $z$ with respect to $x$, then treat $x$ as a constant and differentiate with respect to $y$, and find the places where both of these two are equal to zero? Sorry if this is just a stupid assumption... it may be one of those things that just seems correct but is actually wrong.","['multivariable-calculus', 'optimization', 'maxima-minima']"
2292691,"Integrating $\int_{0}^{\infty} \frac{p^6 dp }{1 + a p^4 + b p^6 } \int_{0}^{\pi}\frac{\sin^5 \theta \,d\theta}{1 + a |p-k|^4 + b |p-k|^6 }$","This is my first question here, so I hope I'm not giving too little/too much information. I need some help calculating (or even approximating) an integral which I've been wrestling with for a while. As part of my internship, I need to calculate or even just approximate a power spectrum which boils down to the following integral: $$I(k)=\int \frac{d^3p}{(2\pi)^3} (1 - \kappa^2)(1-\lambda^2) \frac{\left\lvert p \right\rvert^2}{(1 + a \left\lvert p \right\rvert^4 + b \left\lvert p \right\rvert^6)}  \frac{\left\lvert p-k \right\rvert^2}{(1 + a \left\lvert p-k \right\rvert^4 + b \left\lvert p-k \right\rvert^6)}$$ where $\kappa = \hat{k}.\hat{p}$ and $\lambda = \hat{k}.\widehat{k-p}$ (Obviously $\left\lvert p-k \right\rvert^2 = p^2 + k^2 - 2 p k \cos \theta$). I need the answer in terms of $a,b$, two positive constants that I'd like to vary to describe different physical situations. First attempt at a solution: I moved into spherical coordinates and (I'm reasonably sure that) the integral reduces to: $$\int \frac{d^3p}{(2\pi)^3} \frac{\left\lvert p \right\rvert^2}{1 + a \left\lvert p \right\rvert^4 + b \left\lvert p \right\rvert^6 }  \frac{\left\lvert p \right\rvert^2\sin^4 \theta}{1 + a \left\lvert p-k \right\rvert^4 + b \left\lvert p-k \right\rvert^6 }$$ So naturally I considered performing the integration $$\frac{1}{2\pi^2}\int_{0}^{\infty} \frac{\left\lvert p \right\rvert^6 dp }{1 + a \left\lvert p \right\rvert^4 + b \left\lvert p \right\rvert^6 }  \int_{0}^{\pi}\frac{\sin^5 \theta \,d\theta}{1 + a \left\lvert p-k \right\rvert^4 + b \left\lvert p-k \right\rvert^6 }$$ Sadly, this doesn't really help me too much since these integrals are still pretty hard. Mathematica does manage to calculate them, but the results are really far too cumbersome to handle. Further attempts: As an approximation, I considered sloppily breaking the integral into $p<k$ and $p>k$, which leads me to something of the form (modulo constants): $$\frac{1}{1 + a \left\lvert k \right\rvert^4 + b \left\lvert k \right\rvert^6 }\int_{0}^{k} \frac{\left\lvert p \right\rvert^6 dp}{1 + a \left\lvert p \right\rvert^4 + b \left\lvert p \right\rvert^6} +\int_{k}^{\infty} \frac{\left\lvert p \right\rvert^6 dp}{(1 + a \left\lvert p \right\rvert^4 + b \left\lvert p \right\rvert^6)^2 }  $$ But I found even these ""simpler"" integrals too hard to calculate, and I'm not particularly fond of the approximation either. Questions: 1) Does anyone know of a way I could solve or even approximate any of the above integrals as a function of the parameters $a$ and $b$? 2) It reminds me slightly on a convolution product, though not exactly since it's the norm of the vector $\left\lvert p-k \right\rvert$, which means I can't use the convolution theorem (can I?). 3) I know how to use the Feynman Parametrization for terms that are quadratic in the denominator. But I don't think it would work with $\left\lvert p\right\rvert^6$ (would it?). 3) The parameters $a$ and $b$ are quite small ($a$ is of the order of $10^{-2}$ and $b$ around the same), I was wondering if I could use that to my advantage, maybe perform some sort of Taylor expansion in either of them, reducing the integral to something more manageable, but I'm not sure that's allowed, and at any rate such integrals seem to diverge. Thanks!","['approximate-integration', 'physics', 'convolution', 'calculus', 'definite-integrals']"
2292743,Domain of derivative,"I know that given a certain function $f(x)$, there are some values of $x$ that could be in its domain but not in its derivative's. However, I believe that all the values that belong to the domain of $f'(x)$, have to belong to the domain of the original function as well. Is this true? If yes, how can it be proved?","['derivatives', 'calculus', 'functions']"
2292760,Uniform convergence in convex set,"I have some questions about the following theorem and it's proof. Theorem . Let $X\subset \mathbb R^n$, open, convex, bounded and $f_n:X \to \mathbb R^m$ differentiable.And let's consider the following: There exist $H$: $\mathbb R^n\rightarrow L(\mathbb R^n,\mathbb R^m)$ such that $Df_n \rightarrow H$ uniformly in X, where $Df_n$ is the differential of $f_n$. There exist $x_0 \in $ X such that the sequence $\{f_n(x_0)\}$ converges. Then there exist $f:X  \rightarrow \mathbb R^m$ such that $f_n \rightarrow f$ uniformly over X, $f$ is differentiable and $Df(x)=H(x)$, for all $x\in X$. Proof . Notice that
\begin{align*}
&  \frac{f(x)-f(x_{0})-H(x_{0})\cdot(x-x_{0})}{\Vert x-x_{0\Vert}}%
\color{fuchsia}=\frac{f(x)-f(x_{0})-[f_{n}(x)-f_{n}(x_{0})]}{\Vert x-x_{0\Vert}}\\
&  +\frac{f_{n}(x)-f_{n}(x_{0})-\nabla f_{n}(x_{0})\cdot(x-x_{0})}{\Vert
x-x_{0\Vert}}+\frac{(\nabla f_{n}(x_{0})-H(x_{0}))\cdot(x-x_{0})}{\Vert
x-x_{0\Vert}}\\
&  =:I+II+III.
\end{align*}
Since $X$ is convex, by applying the mean value theorem to the function
$$
g_{n,m}(t)=f_{m}(tx+(1-t)x_{0})-f_{n}(tx+(1-t)x_{0}),\quad t\in\lbrack0,1]
$$ $\color{fuchsia}{**...(2)**}$ there is $t_{0}$ such that
\begin{align*}
&  f_{m}(x)-f_{m}(x_{0})-[f_{n}(x)-f_{n}(x_{0})]=g_{n,m}(1)-g_{n,m}(0)\\
&  =g_{n,m}^{\prime}(t_{0})=(\nabla f_{m}(z_{0})-\nabla f_{n}(z_{0}%
))\cdot(x-x_{0}),
\end{align*}
where $z_{0}=t_{0}x+(1-t_{0})x_{0}$. By uniform convergence of the gradients, ...(1) $$
\Vert\nabla f_{m}(z)-\nabla f_{n}(z)\Vert\leq\Vert\nabla f_{m}(z)-H(z)\Vert
+\Vert\nabla f_{n}(z)-H(z)\Vert\leq2\varepsilon
$$
for all $n,m\geq n_{\varepsilon}$ and all $\color{fuchsia}z\in X$. Hence, by Cauchy's
inequality
\begin{align*}
\left\vert \frac{f_{m}(x)-f_{m}(x_{0})-[f_{n}(x)-f_{n}(x_{0})]}{\Vert
x-x_{0\Vert}}\right\vert  & =\left\vert \frac{(\nabla f_{m}(z_{0})-\nabla
f_{n}(z_{0}))\cdot(x-x_{0})}{\Vert x-x_{0\Vert}}\right\vert \\
& \leq\Vert\nabla f_{m}(z_{0})-\nabla f_{n}(z_{0})\Vert\leq2\varepsilon.
\end{align*}
Since $X$ is bounded, this inequality implies that
\begin{align*}
\vert f_{m}(x)-f_{n}(x)\vert\le|f_{m}(x_{0})-f_{n}(x_{0})|+\Vert
x-x_{0}\Vert  2\varepsilon\le |f_{m}(x_{0})-f_{n}(x_{0})|+2M\varepsilon.
\end{align*}
and so $\{f_n\}$ is a uniform Cauchy sequence ... (2) and so it converges uniformly to a function $f$. Letting $m\rightarrow\infty$ $\color{fuchsia}{**...(3)**}$ we get $$
\left\vert \frac{f(x)-f(x_{0})-[f_{n}(x)-f_{n}(x_{0})]}{\Vert x-x_{0\Vert}%
}\right\vert \leq2\varepsilon
$$
for all $n \geq n_{\varepsilon}$. This takes care of $I$. Taking
$n =n_{\varepsilon}\color{fuchsia}{**...(4)**} $ and using the fact that $f_{n_{\varepsilon}}$ is
differentiable at $x_{0}$ we get that
$$
\left\vert \frac{f_{n_{\varepsilon}}(x)-f_{n_{\varepsilon}}(x_{0})-\nabla
f_{n_{\varepsilon}}(x_{0})\cdot(x-x_{0})}{\Vert x-x_{0\Vert}}\right\vert
\leq\varepsilon
$$
for all $x\in X$ with $0<\Vert x-x_{0}\Vert\leq\delta_{\varepsilon}$. This
takes care of $II$. Lastly, by Cauchy's inequality
$$
\left\vert \frac{(\nabla f_{n}(x_{0})-H(x_{0}))\cdot(x-x_{0})}{\Vert
x-x_{0\Vert}}\right\vert \leq\Vert\nabla f_{n}(x_{0})-H(x_{0})\Vert
\leq\varepsilon
$$
for all $n\geq n_{\varepsilon}$. In conclusion we have that for all $x\in A$
with $0<\Vert x-x_{0}\Vert\leq\delta_{\varepsilon}$,
$$
\left\vert \frac{f(x)-f(x_{0})-H(x_{0})\cdot(x-x_{0})}{\Vert x-x_{0\Vert}%
}\right\vert \leq4\varepsilon
$$
which implies that $f$ is differentiable at $x_{0}$ with $\nabla
f(x_{0})=H(x_{0})$.$\color{fuchsia}{**...(5)**}$ By repeating the proof with $x_0$ replaced by any other point, we get that $f$ is differentiable in $X$. ...(4) My questions are in $\color{fuchsia}{pink}$ color. At the beginning why it is that $H(x_0) (x-x_0)= f_{n}(x)-f_{n}(x_{0})$? How is $g_{m,n}$ defined? I think its domain is $[0,1]$ but I don't know which codomain has. What is the form of $z$, is it a simple vector in X, or has the form of $z_0$? Why do we take $m\to\infty$? Could have been $n$? And why taking $m\to\infty$ implies the next inequality? Why do we take the $n=n_{\epsilon}$? I really don't see this step. Why do we need to repeat the proof?? Isn't $x_0$ arbitrary? How can the proof be formal? At the beginning, it is stated that it must be always $\varepsilon>0$, and maybe the $x,x_0\in X$ must be given too? Or they must be given in the middle of the proof? Or where should they be? Or how?","['cauchy-sequences', 'uniform-convergence', 'proof-explanation', 'convergence-divergence', 'analysis']"
2292775,"Recovering $a+b+\cdots$ from $\exp(a)+\exp(b)+\cdots$ for $a,b,\ldots\in\mathbb N$","In a problem I working on, I have the following value $$ y = f(a_1,a_2,\ldots,a_n) = \varphi^{a_1} + \varphi^{a_2} + \cdots + \varphi^{a_n} \enspace, $$
for $(a_1,a_2,\ldots,a_n)\in\{0,1,\ldots\}$ and $\varphi = 1 - 1/m$ for $m$ an integer greater than 1. I am supposed, given $y$, to recover $$ x = a_1 + a_2 + \cdots + a_n \enspace.$$
I am able to show that the $x$ is unique when the $a_i$'s are whole numbers. For this question, I consider $x$ to be the ""inverse function"" of $y$, which makes sense somewhat since they are both symmetric in the $a_i$'s. I have two solutions but none of them is satisfatory (one is an approximation and the other is computationally demanding and suffers from numerical issues). Solution 1 (using truncated Taylor expansion): $$ x = (y-n)/\ln(\varphi) + O(m^{-1}) \enspace. $$
The problem with this solution is that the error is high when $m$ is small, and I do not control $m$. Solution 2 (Tracing the hypersurface): Basically trying out all the integers lying (close) to the hypersurface defined by the implicit equation of $y$ until we find an exact match (lying exactly on the hypersurface). The problem with this solution is that it can take too much time if the $a_i$'s are huge, even if we exploit the symmetry that the order of $a_i$ doesn't matter. Additionally, $y$ lies between 0 and 2, and tiny changes in $y$ may lead to big changes in $x$ (the function may be numerically unstable). Furthermore, due to the use of floating point representation, we output the point closest to the hypersufrace instead. One more problem In addition to the above challenges, what I am actually given is a perturbed value of $y$: $y+\text{noise}$. So I am also interested in understanding the behaviour of a given algorithm for extracting $x$ under perturbation. So my questions are: Is this function known under a name?  Has it (or its ""inverse"") been
  studied before?  Is there a direct formula (not necessarily ""closed-form"") to compute the ""inverse"" than
  the two ways I proposed?  What is the behaviour of the inverse under
  perturbation?","['number-theory', 'logarithms', 'exponential-function', 'inverse-function']"
2292853,Lebesgue measurability: Rudin's vs Tao's,"I'm a bit confused about Lebesgue measurability of a set.  I saw two different forms of definition: (1) In Terence Tao's Introduction to Measure Theory , the definition is quite simple.  First, the Lebesgue outer measure of any $E \subset \mathbb R^p$ is $m^*(E)\triangleq \underset{\{B_n\}}\inf \: \sum_{n=1}^\infty m(B_n)$, where $\cup B_n$ is a cover of $E$, consisting of boxes $B_n$, and $m$ is Jordan measure.  Then, a subset $B$ of $\mathbb R^p$ is said to be Lebesgue measurable if for every $\epsilon > 0$, there exists an open set $U \subset \mathbb R^p$ such that $m^*(U\setminus B)<\epsilon.$ (2) In Walter Rudin's Principles of Mathematical Analysis , the definition is a bit more lengthy: First, the Lebesgue outer measure is defined almost identically to that in Tao's, with the only difference being that $B_n$'s are open elementary sets. Then, $A \subset \mathbb R^p$ is said to be $m$-finitely measurable, if there exists a sequence $\{A_n\}$ of elementary sets such that $A_n \to A,$ where $d(A, A_n) \triangleq m^*((A\setminus A_n)\cup (A_n\setminus A))$. Finially, $B \subset \mathbb R^p$ is said to be Lebesgue measurable, if $B$ is a countable union of $m$-finitely measurable sets. My questions are: (1) Are these two definitions of Lebesgue measurability equivalent? (2) If so, how may we prove it?  (If the proof is long, I'd also greatly appreciate a reference.) (3) It appears to me that Tao's definition is easier to use and comprehend, and also seems to be more prevalent nowadays.  Is this true?  Is Rudin's definition still useful in some cases? I'd appreciate any help, comments, partial answers, ... etc.  Thanks a lot! P.S.  This question is related to Equivalent definitions of Lebesgue Measurability (Rudin and Royden) , which is still open.  I compare Rudin's and Tao's, provide more details, and have more questions.","['real-analysis', 'measure-theory']"
2292929,In Euclidean space interior of finite set is empty?,"I seem to be somewhat lost in certain concepts. I'm asked to prove that if $W\subset\mathbb{R}^n$ is a linear subspace (a vector subspace) of $\mathbb{R}^n$, with $W\ne \mathbb{R}^n$, then the interior of $W$ is the empty set. But how is this possible? Take, for example, $\mathbb{R}^2\subset \mathbb{R}^3$. Consider an open ball $B(0;r)$ of radius $r$ around $0$ in $\mathbb{R}^2$. Don't we then have that the interior of $B(0;r)$ is $B(0;r)$ itself? Also, for every $r>0$ and every $x\in\mathbb{R}^2$, $B(x;r)\in\mathbb{R}^2$, so that the interior of $\mathbb{R}^2$ is $\mathbb{R}^2$. Also, in Wikipedia it is said that the interior of any finite subset of a Eucledian space is empty. Again, I don't see how this is possible. What is it that I'm missing here?","['general-topology', 'real-analysis', 'linear-algebra', 'vector-spaces']"
2292939,Why do we use combinations instead of permutation when finding amount of ways of flipping coins,"As on the bottom of this question there is a link in which it asked for the number of combinations with exactly three heads and three heads or more, why does the answer utilize combinations? When looking at the math problem, I was confused due to the fact that I thought that the order in flipping the coins mattered but the answer didn't make sense to me very much because HHHTTTTT is different from HTHHTTTT But according to numerous sources, the combination of flipping exactly three heads in a row is 8C3 which I didn't understand, because I thought that I was going to be using permutations because order matters? I believed that finding the number of flipping exactly three heads was going to be 8P3? (Link to the Math Problem I am Referring to)",['combinatorics']
2292977,"If number of zeros is increased in below diagonal entries of $A$, the largest eigenvalue of $B$ decreases. Is it right?","Let $A_i$ be a $n\times n$ lower triangular matrix with diagonal entries $1$ and below diagonal entries are $0$ or $1$. Let O be the zero matrix of order $n$. Consider
$$B_i=\left[ {\begin{array}{cc}
\text{O} & A_i \\
A_i^T & \text{O}
\end{array} } \right] \text{    };i=1,2$$
 Suppose $A_1$ has $a_1$ number of zero entries below diagonal and $A_2$ has $a_2$ number of zero entries below diagonal. I observed that when $a_2>a_1$, the largest eigenvalue of $B_2$ is less than that of $B_1$. Is it right? if so, any hint to prove it?",['matrices']
2293026,Is $x+1$ translated horizontally by -1 or is it translated vertically by 1?,"I know that if you have $y=f(x+a)$, that shifts everything to the left by $a$ (which I tend to think of in terms that you are ""tricking"" the function by giving it an input that isn't actually $x$ but $x+a$. If you have $y=f(x)+a$, that  shifts everything up by $a$ (because $y$ is equal to all of the outputs, $f(x)$, increased by the $+ a$). What I find a bit confusing is whether or not $x+1$ would be more like the first, in the sense that you are modifying the input to an identity function, or the second where you are modifying the results of an identity function.","['algebra-precalculus', 'transformation']"
2293067,What is the norm of this vector?,"I have the vector: $$(-\sin(\varphi)\cos(\theta) \varphi' - \cos(\varphi)\sin(\theta)\theta', \cos(\varphi)\sin(\theta)\varphi' + \sin(\varphi)\cos(\theta)\theta',\cos(\theta)\theta') $$ where $\varphi=\varphi(t) ; \theta = \theta(t);$ and $\varphi'= \frac{d \varphi}{dt};\theta'= \frac{d \theta}{dt};$ I have the answer to be : $\sqrt{1+\varphi'^2\cos^2\theta}$. I have tried and tried simplifying this to get the answer, but I cannot seem to. Does anyone see something that I cannot, how to get this?","['real-analysis', 'trigonometry', 'differential-geometry', 'analytic-geometry']"
2293069,"Is $e^{-\|x\|^2}$ Lipschitz? If so, what is the Lipschitz norm?","Suppose $x\in \mathbb{R}^n$ and $\|.\|_2$ is the Euclidean norm. Is $e^{-\|x\|^2}$ Lipschitz? If so, what is the Lipschitz norm? Here is my attempt: $\big(e^{-\|x\|^2}-e^{-\|y\|^2}\big) \leq |\|x\|^2-\|y\|^2| = |\|x\|-\|y\||(\|x\|+\|y\|)\leq \|x-y\|(\|x\|+\|y\|)$ Do we need to define it on a compact set to make it Lipschitz?","['calculus', 'analysis']"
2293125,"Does the series $\sum \sin^{(n)}(1)$ converge, where $\sin^{(n)}$ denotes the $n$-fold composition of $\sin$?","I'm trying to solve the following task Sequence $\{a_n\}$ is given by the rule: $a_1 = 1,\: a_{n+1} = \sin (a_n)$. Does the series $\sum a_n$ converge? Can you give me any hints how to solve it, cause i got totally stuck at the very beginning, please?","['convergence-divergence', 'trigonometry', 'sequences-and-series', 'calculus']"
2293127,Dimension of $W=\{p(x) : p(x)=p(1-x)\}$.,"Find the dimension of the subspace $W$ of $P_n(x)$ , space of all polynomials ; where $\displaystyle W=\{p(x) : p(x)=p(1-x)\}$. I just found that the polynomials satisfies the condition $p(x)=p(1-x)$ are of the types $p(x)=x^n(1-x)^n$ for every positive integer $x$. But I don't know whether there are more than this type of polynomials or not and how I can find the dimension.","['algebra-precalculus', 'polynomials', 'linear-algebra', 'vector-spaces']"
2293132,Equivalent metrics on a space of continuous functions,"Problem:   If $d$ and $d'$ are equivalent metrics on a Polish space $Y$, then show that the metrics $\delta$ and $\delta '$ are equivalent where $$\delta(f, g)= \sup_{x \in X} d\left(f (x), g (x)\right)$$ and $\delta' $ defined analogously  on $C (X, Y)$, space of all continuous functions from $X$ to $Y$ where $X$ is a compact metrizable space. I am trying to show that $\delta (f_n, f) \to 0$ implies $\delta ' (f_n, f) \to 0$. But the former is implying only that $f_n$ tends to $f$ pointwise (not uniformly) with $\delta '$. Please help.","['general-topology', 'metric-spaces', 'uniform-convergence', 'functions']"
2293191,Dividing a curve into chords of equal length,"I've been researching into this for quite a while, but I seem to be getting only answers involving some programming language of which I do not have any background knowledge. Let me explain the problem: I think ""equal length subdivision"" of the image should be a straightforward visualization of the problem. I have a curve (a parametric Bezier curve, to be more specific) that I want to divide such that any two consecutive points of division have equal Euclidean distances between each other. The first and the last chords have to be on the beginning and the end of the curve respectively, and all points of division should be on the curve. Ideally I would like to have a mathematical solution (i.e. no programming) in which I can specify the number of chords to obtain the resulting division points. I am perfectly fine with calculus and ready to learn more if needed. Thanks. [1]: https://i.sstatic.net/6RkAp.jpg",['geometry']
2293199,Derivative of vector product with respect to a vector [duplicate],"This question already has answers here : Vector derivative w.r.t its transpose $\frac{d(Ax)}{d(x^T)}$ (5 answers) Closed 7 years ago . I would like to derive the following expression with respect to a vector $x \in \mathcal{R}^N$: $a a^T b$ 
where $a, b$ are vectors in $\mathcal{R}^M$. My idea is to apply the following $ \frac{\partial a}{\partial x} a^T b + a (\frac{\partial a}{\partial x})^T b + a a^T \frac{\partial b}{\partial x}$
but it is dimensionally wrong. In fact:
$\frac{\partial a}{\partial x} a^T b$ has dimension: $(MxN) (NxM) (Mx1)$ or $a (\frac{\partial a}{\partial x})^T b$ has dimension: $(Mx1) (NxM) (Mx1)$ that are really dimensionally wrong. I'm assuming that the derivative of a vector $a \in \mathcal{R}^M$ with respect to another vector $a \in \mathcal{R}^N$ is a matrix $a \in \mathcal{R}^{MxN}$. Moreover, even in the very simple example: I want to derive $a^T b$ with respect to $x$:
$(\frac{\partial a}{\partial x})^T b + a^T \frac{\partial b}{\partial x}$, the first addendum seems to be a column vector in $\mathcal{R}^N$ while the second one is a row vector in $\mathcal{R}^N$. What am I doing wrong? I know I can fix this by transposing the function $a^T b$ when partially deriving with respect to $a$ (then the first addendum becomes $b^T \frac{\partial a}{\partial x}$) but I would like to know if there is a generic rule for that. Thank you to anyone that can provide to me some hints. Marco My question is a bit different with respect to this post since that is the differentiation about the transpose of a vector while here I'm questioning about the dimensionality of vector product derivative. Moreover, I add a dummy question that may help me in solving this:
In case of a derivative of scalar-vector product, i.e.
$\frac{\partial (c a)}{\partial x}$, with $c \in \mathcal{R}, a \in \mathcal{R}^M$ and $x \in \mathcal{R}^N$, I would say that the derivative is
$\frac{\partial (c a)}{\partial x} = \frac{\partial c}{\partial x} a + c \frac{\partial a}{\partial x}$. Now, the first addendum has a wrong dimensionality: (1xM) (Nx1). Now, I know that the things works if I do:
$\frac{\partial (c a)}{\partial x} = a \frac{\partial c}{\partial x} + c \frac{\partial a}{\partial x}$, i.e., if I set the scalar on the right side before deriving with respect to $c$. In this case I know that, for dimensionality, $a c$ is more correct than doing $c a$ but I would like to know if there are some rules that I'm aware of.
Then my question is: why do I have to do this?","['derivatives', 'partial-derivative', 'vector-analysis']"
2293216,Developable surfaces with prescribed boundary,"Let $F$ be a smooth embedding of the unit circle $\mathbb{S}^{1}$ in $\mathbb{R}^{3}$ such that the image $F(\mathbb{S}^{1})$ is the unknot. Does there exist a developable surface having $F(\mathbb{S}^{1})$ as boundary? More formally, can one always find an isometric embedding $G \,\colon V \to \mathbb{R}^{3}$ of some closed subset $V \subset \mathbb{R}^{2}$ such that $G(\partial V) = F(\mathbb{S}^{1})$?","['surfaces', 'riemannian-geometry', 'differential-geometry', 'differential-topology']"
2293330,Large powers of sine appear Gaussian -- why?,"As part of approximating an integral, I have noticed that $\sin^k(x), x\in[0, \pi]$ look almost identical to $\exp\left(-\frac{k}{2}(x-\frac{\pi}{2})^2\right)$ once $k$ is large enough (in practice, the two equations are visually identical for $k\geq 15$). As an example, see this picture of the two functions for $k=10$: Can anybody explain why these functions practically are identical? Edit: I should mention that the expression $\exp\left(-\frac{k}{2}(x-\frac{\pi}{2})^2\right)$ is derived by a 2nd order Taylor expansion of $\log\sin^k(x)$ around the mode $x_0 = \frac{\pi}{2}$, i.e. a Laplace approximation.","['calculus', 'normal-distribution']"
2293332,Simple Stochastic Epidemic Model,"While going through a book for the topic Simple Stochastic Model , it starts off with defining the notations as: Let $X(t)$ and $Y(t)$ denote the number of susceptible and infectives respectively at time $t$. Let $\beta$ be the infection rate such that the number of infections during an infinitesimal interval of time $(t, \delta t)$ is:
$$\beta X(t) Y(t) \delta t$$ subject to $X(0) + Y(0) = n+1$ Also, it is assumed that $X(0) = n$ and $Y(0) = 1$. Now, in the book it is stated that: $$p_r(t) = \mathbb{P}[X(t) = r | X(0) = n; Y(0) = 1]$$ and goes on to state that: Putting $\tau = \beta t \implies p_r(\tau) = \beta p_r(t)$ I am not able to understand how is $p_r(\tau) = \beta p_r(t)$? The book is also silent about any explanation for it. Could someone please explain why the equality should hold? Thanks in advance! Here is a snapshot of the portion of the book that I am talking about:","['stochastic-processes', 'probability-theory']"
2293337,Proving $dx.dy = r.drd\theta$ using x and y [duplicate],"This question already has answers here : how to get $dx\; dy=r\;dr\;d\theta$ [duplicate] (2 answers) Closed 7 years ago . I can prove that $dx dy = r.dr d\theta$ by drawing a circle and calculating the area of a small square in the polar coordinates, but when I try proving it using the equations below, I fail to prove it. What is my mistake? $x = r\cos(\theta) => dx = \cos(\theta).dr - \sin(\theta)r.d\theta$ $y = r\sin(\theta)=>dy=\sin(\theta).dr + \cos(\theta)r.d\theta$ $=> dxdy = r(\cos^2(\theta)-\sin^2(\theta))drd\theta=r\cos(2\theta).drd\theta$ I actually saw a similar question in this site: how to get $dx\; dy=r\;dr\;d\theta$ My problem was that I didn't understand why $drd\theta = -d\theta dr$","['derivatives', 'polar-coordinates', 'jacobian']"
2293374,How can we evaluate $S_{N}=\sum_{k=1}^{N}\sin^2\left({x\over 2k}\right)?$,I am interested in evaluating the sum of: $$S_{N}=\sum_{k=1}^{N}\sin^2\left({x\over 2k}\right)\tag1$$ Expanded $(1)$ $$\sin^2\left({x\over 2}\right)+\sin^2\left({x\over 4}\right)+\sin^2\left({x\over 6}\right)+\cdots+\sin^2\left({x\over 2N}\right)\tag2$$ Using $$\sin^2(x)+\cos^2(x)=1$$ $$S_{N}=N-\sum_{k=1}^{N}\cos^2\left({x\over 2k}\right)\tag3$$ Using $$\cos^2(x)-\sin^2(x)=\cos(2x)$$ $(1)$+$(3)\implies$ $$2S_{N}=N-\sum_{k=1}^{N}\cos\left({x\over k}\right)\tag4$$ Recall $${1\over 2}+\sum_{k=1}^{N}\cos(kx)={\sin(N+1/2)x\over 2\sin(x/2)}\tag5$$ $$\cos(A)-\cos(B)=2\sin\left({A+B\over 2}\right)\sin\left({A-B\over 2}\right)\tag6$$ $$\cos(kx)-\cos\left({x\over k}\right)=2\sin\left[x\left({k^2+1\over 2k}\right)\right]\sin\left[x\left({k^2-1\over 2k}\right)\right]\tag7$$ $(4)$+$(5)\implies$ $$2S_{N}+{\sin(N+1/2)x\over 2\sin(x/2)}=N+{1\over 2}+2\sum_{k=1}^{N}\sin\left[x\left({k^2+1\over 2k}\right)\right]\sin\left[x\left({k^2-1\over 2k}\right)\right]\tag8$$ How can we evaluate $(1)?$,"['sequences-and-series', 'trigonometric-series']"
2293419,Positive integer solutions to $\frac{x}{y+z}+\frac{y}{x+z}+\frac{z}{x+y}=4$ [duplicate],"This question already has answers here : What are $x$, $y$ and $z$ if $\frac{x}{y + z} + \frac{y}{x + z} + \frac{z}{x + y} = 4$ and $x$, $y$ and $z$ are whole numbers? (1 answer) Find integer in the form: $\frac{a}{b+c} + \frac{b}{c+a} + \frac{c}{a+b}$ (3 answers) Closed 7 years ago . I was wondering if anyone knew of any positive integer solutions to this Diophantine equation, or had a proof there are none. Integer solutions exist with negative values, such as (11,9,-5) and (4,11,-1), but checking positive integers up through 10,000 yielded nothing and I don't see a way to show there are none.","['number-theory', 'diophantine-equations']"
2293450,Prove that $\|UVU^{-1}V^{-1}-I\|\leq 2\|U-I\|\|V-I\|$,"$U,V$ are unitary $n\times n$ matrices, and the norm is the operator norm (so we can use $\|UV\|\leq\|U\|\|V\|$). I've noticed that
\begin{align}
\|UVU^{-1}V^{-1}-I\|&= \|(UV-VU)U^{-1}V^{-1}\|\\
&\leq \|UV-VU\|\|U^{-1}V^{-1}\|
\end{align} I can bound the first term by $\|UV\|+\|VU\|$, but I don't think this is useful. Hints (rather than complete answers) would be appreciated. The question comes from here (exercise 1)","['matrices', 'normed-spaces', 'matrix-equations', 'operator-theory']"
