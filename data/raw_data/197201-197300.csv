question_id,title,body,tags
3803738,How does meeting probability on a finite $d$-dimensional grid depend on the dimension?,"At the beginning, we put two people $A$ and $B$ randomly on a finite $d$ -dimensional closed grid.
Along each dimension there are $2^n$ positions $p_k$ , so $p_{2^n}=p_0$ . Think of a circle if $d=1$ or a torus if $d=2$ and so on... Let's say $m$ is the minimal number of steps to reach each other. $m$ is unknown to $A$ and $B$ .
Now both start to move step by step along a randomly chosen direction on the grid. If we wait for less than $m/2$ steps, it is not be possible for them to meet at all.
If we wait for infinitely many steps, I would expect them to meet somewhen for sure. How does the meeting probability as a function of steps on a finite $d$ -dimensional grid depend on the dimension?","['random-walk', 'probability']"
3803792,"If $(1+x)^{4n} +(1+x+x^2)^{2n} +(1+x+x^2+x^3+x^4)^n = a_0 + {a_1}x + {a_2} x^2 + .... +{a_{4n}}x^{4n}$ , then prove that $a_r=a_{4n-r}$","I tried solving this question by attempting to prove $(a_r)=(a_{4n-r})$ Now, $a_r$ is the coefficient of $x^r$ which we can obtain by adding up the coefficients of $x^r$ from the 3 separate expressions $(1+x)^{4n}$ , $(1+x+x^2)^{2n}$ and $(1+x+x^2+x^3+x^4)^n$ In the first expression $(1+x)^{4n}$ , it is fairly easy to see that the coefficient of $x^r$ can be obtained from binomial expansion and the coefficient $a_r=a_{4n-r}$ by some basic properties. I tried finding the coefficient of $x^r$ in $(1+x+x^2)^{2n}$ I first tried doing so by simplifying the expression as $$(1+x+x^2)^{2n} = [1+(x(1+x)]^{2n}$$ Then taking $x(1+x)$ as some $y$ , I applied binomial expansion and obtained the following expansion, $$[1+(x(1+x)]^{2n} = \binom {2n}{r}\binom{r}{0}+\binom{2n}{r-1}\binom{r-1}{1}+\binom{2n}{r-2}\binom{r-2}{2}+...+\binom{2n}{r/2}\binom{r/2}{r/2}$$ (when $r$ is even, otherwise instead of going upto $r/2$ we will go upto $(r-1)/2$ ) However, after calculating this, I was unable to find any relation between $a_r$ and $a_{4n-r}$ for this expression.
I was also not able to find any such simplification for the third expression $(1+x+x^2+x^3+x^4)^n$ I tried using the identity $$1+x+x^2+...+x^n = {(1-x^{n+1})}/{(1-x)}$$ But was unable to obtain any further simplification even on using negative binomial expansions. Any help on how to approach this question is appreciated Thanks in advance! Regards","['algebra-precalculus', 'polynomials']"
3803817,Is there a quick method to calculate the eigenvalues of this complex $4 \times 4$ matrix?,"I want to calculate the eigenvalues of a matrix given below. $A=\begin{pmatrix}1&1&1&1 \\ 1 & i & -1 & -i \\ 1 & -1 & 1 & -1 \\ 1 & -i &-1 &i\end{pmatrix}$ We can calculate $\det{(A-tE)}$ by the Laplace expansion(or cofactor expansion), but that method takes too much time for me. Since this question is from an exam question, I need a quick(and preferably easy) method to calculate the eigenvalues. Of course, I have seen these questions: Eigenvalues for $4\times 4$ matrix Quick method for finding eigenvalues and eigenvectors in a symmetric $5 \times 5$ matrix? However, the answers there do not seem to be applicable to our matrix. Is there any method that can be applied to our matrix?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3803892,How to calculate an integral containing a nonlinear function of a derivative?,"Consider the integral $I_1=\displaystyle \int_0^\tau \frac{dx(t)}{dt} dt$ that contains the derivative $\dot{x}$ . Because $\dot{x}$ is not inside a non linear expression, one calculates that it equals $\displaystyle \int_{x(0)}^{x(\tau)} dx(t) = x(\tau)-x(0)$ . But how can one calculate an integral such as $I_2=\displaystyle \int_0^\tau \sqrt{\frac{dx(t)}{dt}} dt$ ? I tried partial integration, but this does not work.","['integration', 'derivatives']"
3803905,G.Rhin's lower bounds for $ | S \log 2 - N \log 3 |$ (used by J. Simons in the Collatz-problem) adaptable to $ | S \log 2 - N \log 5 |$?,"By G. Rhin, cited by John Simons, 2007, we have the upper bound for $$ |S \log2 - N \log 3 | \gt \exp(-13.3(0.46057+\log(N))) \qquad \text{roughly:} {1\over 457 N^{13.3}}
$$ This has been used by John Simons to disprove the 1-cycle in the Collatz ( $3x+1$ )-Problem. I'm fiddling with the equivalent question in the $5x+1$ - problem. The 1-cycle here has already been handled by R. Steiner in 1981, and he disproved the existence of any 1-cycle for odd-step-length $N>3$ (the 1-cycles with $N=2$ and $N=3$ are well known), but it is very complicated for me to read the part with the A.Baker-based bounds, and I would like to apply instead a G. Rhin-like estimate for the lower bounds of $$ |S \log2 - N \log 5 | \gt ??? $$ I'm until now unable to apply and/or modify the underlying results of A. Baker myself accordingly. So my questions: Can I use (at least for large $N$ ) the given bound analoguously? Or what would be an adapted bound? If I could use that bound, it would be possible to disprove the 1-cycle for the $5x+1$ -problem much elementary with the need of direct checks only for $N=4 \ldots 104 $ (1-cycles with $N=2$ and $N=3$ exist and are well known) If details of my approach (and thus for my needs) are wished, see also my ""1-cycle for the $3x+1$ "" - text at my homepage Simons, John L. , On the (non-)existence of (m)-cycles for generalized Syracuse sequences , Acta Arith. 131, No. 3, 217-254 (2008). ZBL1137.11016 . In Simons' article cited: Rhin, Georges , Approximants de Padé et mesures effectives d’irrationalité. (Padé approximants and effective measures of irrationality), Théorie des nombres, Sémin. Paris 1985/86, Prog. Math. 71, 155-164 (1987). ZBL0632.10034 . Steiner's  disproof of the 1-cycle in the $5x+1$ -problem: Steiner, Ray , On the ”QX+1 problem,” Q odd, Fibonacci Q. 19, 285-288 (1981). ZBL0474.10005 .","['collatz-conjecture', 'number-theory', 'diophantine-approximation']"
3803906,"$p$-group and an normal subgroup, need help to start my solution","Let $G$ be a $p$ -group with | $G$ |= $p^{n}$ with $p$ is prime and $n>1$ .
Let's say that $N$ is a strict normal subgroup of $G$ . Prove there is an $x\in Z(G/N)$ of order $p$ , i.e. $x^{p}=e$ and $x \neq e$ . Prove that there exist a normal subgroup $M$ with $N\subset M\subset G$ and $[M:N]$ = $p$ . Prove $G$ is soluble. I wanted  to solve the above but I'm really sorry i can't come further then definitions.
I know it seems like there isn't a lot of effort in this post but I have really no clue how to start this. I looked here for some extra information but didn't find a post that could help me.
I would appreciate it if you could help me out so i can understand similair questions or understand how stuff with a p-group works. I'm really sorry i will edit the question with my solutions that i found with your help :) I got this hint :Since N⊊G is a strict normal subgroup, G/N is a nontrivial p-group, i.e. |G/N|= $p^{k}$ for some k∈N. This implies Z(G/N)≠{e} ,because if $G/N$ is a p-group then |Z(G/N)|>1 ,and Z(G/N) is again a p-group. Now it should be easy to find an element x∈Z(G/N) of order p. My solution: from the above we know that $Z(G/N)$ has a order $p^{k}$ for a $k\in \mathbb{N}$ . Let's take a $x\in Z(G/N)$ with order $s\in \mathbb{Z}$ . This means that $x^{s}=e$ . Now it follows that $p^{k}|s$ . It's trivial that if $s=p$ , $p^{k}|p$ so there exist an element with order $p$ .","['normal-subgroups', 'abstract-algebra', 'p-groups', 'group-theory', 'solvable-groups']"
3803933,"If $Df(a)$ is non singular and $f \in \mathscr{C}^{1}$, so $f$ is locally one-to-one.","Let $A$ be a open in $\mathbb{R}^{n}$ . Let $f: A \to \mathbb{R}^{n}$ be of class $\mathscr{C}^{1}$ .
If $Df(a)$ is non-singular, there exists an $\alpha>0$ s.t the inequality $$||f(x)-f(y)||\geq \alpha ||x-y||$$ holds for all $x,y$ in some open cube $C(a;\epsilon)$ in some open cube $C(a;\epsilon)$ centered at $a$ . It follows that $f$ is one-to-one on this open cube. My attempt: Let $E=Df(a)$ , so since $\det(Df(a))\not=0$ , we have that there exists one matrix $E^{-1}$ , such that $E^{-1}E=EE^{-1}=I_{n}$ . Let $T: \mathbb{R}^{n}\to \mathbb{R}^{n}$ by $T(x)=Ex$ , so we have that $\forall x,y \in \mathbb{R}^{n}$ , $$ ||x-y||=||E^{-1}(Ex-Ey)||\leq n ||E^{-1}||\cdot ||Ex-Ey||,n \in \mathbb{N} \iff \frac{1}{n||E^{-1}||}||x-y||\leq ||Ex-Ey||$$ Now, since $||E^{-1}||> 0$ and $n\in \mathbb{N}$ , so $2\beta:=\frac{1}{n||E^{-1}||}>0$ . Therefore, $\forall x, y \in \mathbb{R}^{n}$ , we have that $$||Ex-Ey||\geq 2\beta ||x-y||$$ Let $H: A\subset \mathbb{R}^{n} \to \mathbb{R}^{n}$ by $$H(x)=f(x)-Ex$$ since $f \in \mathscr{C}^{1}$ and $Ex \in \mathscr{C}^{1}$ ,  so $H\in \mathscr{C}^{1}$ . Therefore $\forall x \in A: DH(x)=Df(x)-D(Ex)=Df(x)-E$ . In particular, $DH(a)=Df(a)-E=E-E=0 \implies DH(a)=0$ . Claim: Since $H \in \mathscr{C}^{1}$ , so we can choose a $\epsilon>0$ such that $\left|DH(x)\right|<\beta$ for $x \in C=C(a;\epsilon)$ Question: How can I prove claim ? Thanks @ nicomezi and @peek-a-boo for the explain about my claim . EDIT: Since that $H \in \mathscr{C}^{1}$ , we can choose $\epsilon>0$ s.t $|DH(x)|<\beta$ for $x \in C(a;\epsilon)$ . Now, the TVM applied to the $i^{th}$ component function $H$ , tells us gives $x,y \in C(a;\epsilon)$ , there's a $c \in C(a;\epsilon)$ s.t: $$|H_{i}(x)-H_{i}(y)|\leq ||DH(c)||\cdot ||x-y||= \beta ||x-y||$$ Then, $\forall x, y \in C(a;\epsilon)$ , we have $$\begin{align*}
\beta||x-y||&\geq & ||H(x)-H(y)||\\
&=&||f(x)-Ex-f(y)+Ey||\\
&\geq& ||Ex-Ey||-||f(x)-f(y)||\\
&\geq& 2\beta ||x-y||-||f(x)-f(y)||\end{align*}$$ Now, let $\beta:=\alpha>0$ , so we have $$||f(x)-f(y)||\geq 2\alpha ||x-y||-\alpha||x-y||=||x-y||$$ So, there exists an $\alpha>0$ s.t $$||f(x)-f(y)||\geq \alpha||x-y||$$ holds for all $x,y \in C(a;\epsilon)$ . Is my prove correct, now? Any suggests? Thanks so much.","['multivariable-calculus', 'solution-verification', 'real-analysis']"
3803949,"Given two real numbers $x,y$ so that $x^{2}+y^{2}+xy+4=4y+3x$. Prove that $3\left(x^{3}-y^{3}\right)+20x^{2}+2xy+5y^{2}+39x\leq 100$.","Given two real numbers $x, y$ so that $x^{2}+ y^{2}+ xy+ 4= 4y+ 3x$ . Prove that $$3\left ( x^{3}- y^{3} \right )+ 20x^{2}+ 2xy+ 5y^{2}+ 39x\leq 100$$ I used derivative and Wolfram|Alpha but only the minimum value found $$\min\{3\left ( x^{3}- y^{3} \right )+ 20x^{2}+ 2xy+ 5y^{2}+ 39x\}\Leftrightarrow \left\{\begin{matrix} x\cong 0.0320241\\ y\cong 2.16078\\ \left ( z\cong 2.19235 \right ) \end{matrix}\right.$$ where $z$ is a root of $472z^{3}- 449z^{2}- 689z- 1305= 0$ , $y$ is a root of $27468y- 11800z^{2}+ 17833z- 41733= 0$ , $x$ is a root of $27468x+ 3304z^{2}- 11167z+ 7722= 0$ . Why is unsuccessful ? Here are two examples of my claim .","['roots', 'derivatives', 'optimization', 'inequality', 'quadratics']"
3803968,Maximum of $(\sin{x} + \sqrt3)(\cos{x}+1)$,"The problem is Maximum of $(\sin{x} + \sqrt3) (\cos{x}+1)$ I tried to set $\sin(t) = y$ , $\cos(t) = x$ , so that the problem turned out to be finding Max $(y + \sqrt3)(x + 1)$ s.t. $x^2 + y^2 =1 $ I tried Lagrange multipliers, but it turns out to be a quartic equation. I also tried a linear transformation such that the problem becomes Max $x^2 - y^2$ s.t. $(x + \frac{\sqrt3+1}{2})^2 + (y - \frac{\sqrt3-1}{2})^2 = 1$ Since $x^2-y^2$ is saddle surface, the answer is still not trivial. Can you help me find the way out?","['calculus', 'trigonometry']"
3803982,Prove that $\sqrt{p}$ is irrational where $p$ is prime [duplicate],"This question already has answers here : The contradiction method used to prove that the square root of a prime is irrational (3 answers) Closed 3 years ago . Here is the standard proof for proving that $\sqrt{p}$ , where $p$ is prime, is irrational. Assume $\sqrt{p}$ is rational, so that $\sqrt{p} = \frac{a}{b}$ , where $a, b \in \mathbb{Z}$ , $b \neq 0$ and hcf $(a, b) = 1$ Then $a^2 = pb^2$ , so $a^2$ is divisible by $p$ . Therefore $a$ is divisible by $p$ . Then $a = pk$ , where $k \in \mathbb{Z}$ . From earlier, $b^2 = pk^2$ , so $b^2$ is divisible by $p$ . Therefore $b$ is divisible by $p$ . The fact that $a$ and $b$ are both divisible by $p$ contradicts that hcf $(a, b) = 1$ . Therefore, $\sqrt{p}$ is irrational. In the proof, where do we use that $p$ is prime? I think it is in deducing that $a$ is divisible by $p$ from $a^2$ divisible by $p$ . But then again, we have that $a^2 = pk$ , and therefore $a \times a = p \times k$ and so if I divide both sides by $p$ , it must divide one of $a$ or $a$ . So maybe that's not where we use it.","['irrational-numbers', 'elementary-number-theory', 'algebra-precalculus', 'proof-explanation']"
3804008,Problem with lyapunov function,"I have a question about a differential equation I tried to analyse: $$
\begin{align}
\dfrac {dx}{dt} &= v \\
\dfrac {dv}{dt} &= -x+x^3-v^3 \\
\end{align}
$$ I plotted this differential equation, and it looks like it has a spiral going inward (see picture). Now I tried to prove this: Therfore I searched a liapunov function and I think I found a local one: $$  L = \dfrac 14(2v^2+2x^2-x^4)$$ for $v^2+x^2 > \dfrac { x^4}2 $ and $\dfrac {dL}{dt} = -v^4$ Now if I understand it right, the fixed point in the middle should attract all trajectories in the region where the lyapunov function is defined: $v^2+x^2 > x^4/2$ But if I compare the ‚region of attraction‘ of the lyapunov function with the computer simulation, it does not fit together. Can someone help me and tell me what I am doing wrong? I appreciate every help! Region for Lyapunov function Simulation with pplane","['lyapunov-functions', 'nonlinear-dynamics', 'ordinary-differential-equations']"
3804047,What is the 'limit' of a delta prime boundary condition?,"I recently came across the concept of delta prime boundary conditions, which can be imposed on a function $\psi$ at the origin in one dimensional space (say, in the context of partial differential equations): $\frac{\partial\psi}{\partial x}$ is 'continuous' at $0$ , in the sense that $\frac{\partial\psi}{\partial x}|_{0^+}=\frac{\partial\psi}{\partial x}|_{0^-}=:\psi'(0)$ , meaning that both one sided limits exist and are equal (although $\psi$ doesn't have to be differentiable in the usual sense). The double sided limits of $\psi (0)$ exist and we have - $$\psi(0^+)-\psi(0^-)=-\sigma \psi'(0)$$ Where $\sigma\in\mathbb R$ . Note that $\psi$ doesn't have to be continuous at $0$ (again - it need not be differentiable - only have one sided derivatives). This gives me a certain boundary condition for any choice of $\sigma$ . My question is - what happens to the condition as $\sigma\rightarrow \infty$ ? Is there a sense in which this limit can exist? What would be the corresponding boundary condition? It seems to me that for me to take this limit (and if I want a bounded solution), I must have in the corresponding boundary condition that $\psi'(0)=0$ . But what I'm interested in is - does $\psi$ now have to be continuous at $0$ ? We now have something of the form: $$\psi(0^+)-\psi(0^-)=""-\infty\cdot 0""$$ Naively, we can't estimate the RHS and claim if $\psi$ is continuous at $0$ . But maybe anyone here knows of a 'proper' way to take this limit (via distribution theory or something) so that we can know if maybe $\psi$ also needs to be continuous at $0$ for some reason? This question is (partially on purpose) a bit vague and not too well defined - I have not stated which space of functions $\psi$ belongs to, etc. For me, this is part of the question - what properties would you require $\psi$ to fulfill for this question to 'make sense'? In what context can we say anything meaningful about this limit? There's probably more than one 'correct' answer to this - I'm interesting in anything you can suggest. I'd be happy to hear all kinds of answers - intuitions, formal proofs (this is probably best), and even references to papers/books which do something similar to what I described. Keep in mind that I'm pretty new to this world of content, so ideally the more detailed the explanation - the better. Thanks in advance!","['measure-theory', 'distribution-theory', 'partial-differential-equations', 'boundary-value-problem', 'mathematical-physics']"
3804055,Hölder's theorem on gamma function,"I was reading the proof of Hölder's theorem on wikipedia that the gamma function $\Gamma(z)$ does not satisfy any differential equation of form $P(z;\Gamma(z),\Gamma'(z),...,\Gamma^{(n)}(z))=0$ , where $P(X;Y_0,Y_1,...,Y_n)$ is a polynomial (I don't know why there is a semicolon, but see the last paragraph). One of the key idea is to define a total order on monomials, and therefore we may talk about the highest term or ""degree"" of a polynomial. However the article is kind of vague about that. According to this there are several orders on monomials. Which of them is being used here? More specifically, in the wiki article the following example is given: $\deg \left(-3 X^{10} Y_{0}^{2} Y_{1}^{4} + i X^{2}Y_{2} \right) < \deg \left( 2 X Y_{0}^{3} - Y_{1}^{4} \right)$ Below is part of the proof. Furthermore, if $X^{h} Y_{0}^{h_{0}} Y_{1}^{h_{1}} \cdots Y_{n}^{h_{n}}$ is the highest-degree monomial term in $P$ , then the highest-degree monomial term in $Q\stackrel{\text{df}}{=} ~ P(X + 1;X Y_{0},X Y_{1} + Y_{0},X Y_{2} + 2 Y_{1},\ldots,X Y_{n} + n Y_{n - 1})$ is $X^{h + h_{0} + h_{1} + \cdots + h_{n}}Y_{0}^{h_{0}}Y_{1}^{h_{1}} \cdots Y_{n}^{h_{n}}$ . Consequently, the polynomial $Q - X^{h_{0} + h_{1} + \cdots + h_{n}} P$ has a smaller overall degree than $P$ . I think for the argument to work we should use the lexicographic order $Y_n<Y_n-1<...<Y_0<X$ , i.e., given two monomials first compare the order of $Y_n$ , and then $Y_{n-1}$ , and finally $X$ . In this case the example given above should be false. Am I correct? Wikipedia gives two references, A Survey of Transcendentally Transcendental Functions by Lee A. Rubel and Irresistible Integrals by George Boros and Victor Moll. Both of their proofs are quite similar to wikipedia article, but the order they use seems to be more mysterious: they view $X$ as coefficient rather than indeterminate and the order does not involve $X$ , so the ""highest degree term"" is something like $q(X)Y_{0}^{h_{0}}Y_{1}^{h_{1}} \cdots Y_{n}^{h_{n}}$ . Then they use some ""Euclidean algorithm"", which I do not understand how to work for multivariate polynomials. Should the monomial order involve $X$ or not ?","['ordinary-differential-equations', 'gamma-function', 'calculus', 'abstract-algebra', 'polynomials']"
3804056,"What does the sigma mean in the notation $\sigma(X,X^*)$ (weak topology)?","As a math student, I am also interested in the historical development of ideas in math. I am studying Conway's Functional analysis book, and he uses the notation: $\sigma(X,X^*)$ for the weak topology on $X$ and $\sigma(X^*,X)$ for the weak-* topology on $X^*$ . I think this sigma notation is pretty standard so it makes me wonder. So my question: is the sigma notation $\sigma(X,X^*)$ or $\sigma(X^*,X)$ an artifact of an older math concept applied to functional analysis? Or is it simply just that, notation?","['general-topology', 'math-history', 'functional-analysis']"
3804057,Asymptotic Expression for $ f(z) = z+ z^\frac{1}{2}+ z^\frac{1}{3}+ z^\frac{1}{4} +\dots + z^\frac{1}{N}$ with complex $z$?,"Question (corrected) I managed to prove: $$ f(z) \sim \left\{
\begin{array}{ll}
      - \ln |z| \int_0^{\frac{-N}{\ln|z|}} e^{-\frac{1}{|y|}} dy & |z|<< 1 \\
      ? & |z| \approx 1 \\
      ?? & |z|\gg 1 \\
\end{array} 
\right. $$ Where, $$ f(z) = z+ z^\frac{1}{2}+ z^\frac{1}{3}+ z^\frac{1}{4} +\dots + z^\frac{1}{N}$$ However, I do not have a asymptotic expression as $|z| \to 1$ or $|z| \to 
 \infty$ . Can someone provide that expressions as well? Background Consider the complex function: $$ f(z) = z+ z^\frac{1}{2}+ z^\frac{1}{3}+ z^\frac{1}{4} +\dots + z^\frac{1}{N}$$ Let, us write $z= r e^{i \theta}$ where $1>>  r > 0$ . $$ f(r e^{i \theta}) = r e^{i \theta}+ r^{1/2}e^{i \theta/2} + \dots +r^{1/n}e^{i \theta/n}$$ Consider, the function with $f(x) = e^\frac{-1}{|x|}$ and the integral: $$ \int_{0}^{N \epsilon} f(y) dy = \lim_{\epsilon \to 0,N \to \infty} \Big(f(\epsilon) +f(2 \epsilon) + \dots + f(N\epsilon) \Big)\epsilon$$ with $N \epsilon = b$ . We modify our considerations and use $^\ast$ : $$ \lim_{\epsilon \to 0,N \to \infty} \sum_{r=1}^{N} a_r f(r\epsilon) \epsilon = \lim_{s\to 1} \frac{1}{\zeta(s)} \times \sum_{r=1}^\infty \frac{a_r}{r^s} \int_0^{N\epsilon} f(y) dy$$ Choosing $a_r= e^{i\theta /r}$ and replacing $\epsilon = \frac{-1}{\ln \delta}$ $$ \lim_{\delta \to 0,N \to \infty} \Big(f(\frac{-1}{\ln \delta})e^{i \theta} + f(\frac{-2}{\ln \delta})e^{i\theta /2} + \dots + f(\frac{-N}{\ln\delta})e^{i \theta/N} \Big) \frac{-1}{{\ln\delta}} = \underbrace{\lim_{s\to 1} \frac{1}{\zeta(s)} \times\sum_{r=1}^\infty \frac{e^{i\theta /r}}{r^s}}_{=1}  \int_{0}^{- \frac{N}{\ln \delta}} e^\frac{-1}{|y|} dy$$ Substituting with $f$ , using asymptotics and solving the limit using this : $$ \delta e^{i \theta} + \delta^{1/2} e^{i \theta/2} + \dots  + \delta^{1/N} e^{i \theta/N} \sim  - \ln \delta  \int_0^{\frac{-N}{\ln \delta}} e^{-\frac{1}{|y|}} dy $$ $^\ast$ We split into real and imaginary parts to apply the formula.","['complex-analysis', 'solution-verification', 'asymptotics', 'dirichlet-series']"
3804078,Show there are 1977 non-similar triangles such that$\frac{\sin X+\sin Y+\sin Z}{\cos X+\cos Y+\cos Z}=\frac{12}7$and$\sin X\sin Y\sin Z=\frac{12}{25}$,"Show that there are 1977 non-similar triangles whose angles $X$ , $Y$ , $Z$ satisfy the conditions $$\begin{align}
\frac{\sin{X}+ \sin{Y}+ \sin{Z}}{\cos{X}+\cos{Y}+ \cos{Z}}=\frac{12}{7} \tag1\\[4pt]
\sin{X}\sin{Y} \sin{Z}=\frac{12}{25} \tag2
\end{align}$$ My attempt: $$\begin{align} \sin{X}+ \sin{Y}+ \sin{Z}=4\cos{\frac{X}{2}} \cos{\frac{Y}{2}} \cos{\frac{Z}{2}} \\ \cos{X}+ \cos{Y}+ \cos{Z}=1+4\sin{\frac{X}{2}} \sin{\frac{Y}{2}} \sin{\frac{Z}{2}}\\ X+Y+Z=\pi \\ \sin{X} \sin{Y}\sin{Z}=8\cos{\frac{X}{2}} \cos{\frac{Y}{2}} \cos{\frac{Z}{2}}\sin{\frac{X}{2}} \sin{\frac{Y}{2}} \sin{\frac{Z}{2}}=\frac{12}{25}\end{align}$$ Here after what to do to find the required results. How to show such points which satisfy these conditions? Please, help. Thanks in advance.","['contest-math', 'trigonometry']"
3804080,"If $T(n)$ implies $T(2n)$, $T(n)$ implies $T(n-5)$ for $n \geq 6$ and $T(1)$ is true, then is $T(n)$ true for all $n$ in natural numbers?","My reasoning:
From the assumption it follows that for all numbers in form $n = 2^m - 5k$ , where $n,m,k$ are some natural numbers $T(n)$ is true. When $n=5$ then $(5 = 2^m - 5k) \equiv 5(k+1) = 2^m$ , therefore $n$ can't equal $5$ . Hence we can't conclude (from given assumptions) that $T(n)$ is true for all $n$ in natural numbers. Neither we can say that $T(n)$ is not true for all natural numbers. So my answer would be that we can't tell. Am I wrong?","['elementary-number-theory', 'induction', 'logic', 'discrete-mathematics']"
3804117,Diophantine equation $x^2 + xy − 3y^2 = 17$,"Determine all integer solutions to the equation $x^2 + xy − 3y^2 = 17$ . The previous part of the question was finding the fundamental unit in $\mathbb{Q}(\sqrt{13})$ , which is $\varepsilon = \frac{3+\sqrt{13}}{2}$ , so my guess is that I should factorise the equation in $\mathbb{Q}(\sqrt{13})$ and then use the uniqueness of factorisation of ideals into prime ideals. (But it is also possible that the parts of the question are unrelated, because they certainly seem unrelated.)","['algebraic-number-theory', 'number-theory', 'diophantine-equations', 'abstract-algebra', 'ideals']"
3804118,"Is a bijective, norm-preserving and rotation-invariant mapping linear?","Let $h: \mathbb{S}^N\to \mathbb{S}^N$ , where $\mathbb{S}^N$ is the $N$ -dimensional unit hypersphere in $\mathbb{R}^{N+1}$ , be a mapping with the following properties: $h$ is bijective and smooth. $\left\|h(x)\right\| = 1$ (given that we operate on a hypersphere). $h(x)^\top h(y) = h(Rx)^\top h(Ry)$ for any rotation $R$ , i.e. the inner product is invariant to rotations in the domain of $h$ . Is $h$ a linear or affine transformation (i.e. is $h(x) = Qx$ where $Q$ is a rotation)? It took me a few days, but I think I finally found a proof using that if $h(x)^\top h(y) = x^\top y$ for all $x, y$ then $h$ is linear. This can be shown by realising that if $h(x)^\top h(y) \ne x^\top y$ for any $x, y$ then $h$ cannot be bijective, see the proof below.","['linear-algebra', 'functional-analysis']"
3804136,"$H^i(X, \mathcal{O}_X(H))$ for $X$ a cubic fourfold","$\newcommand{\oh}{\mathcal{O}}$ Let $X \subset \mathbb{P}^5$ be a cubic fourfold. I'd like to known the cohomology $H^i(X, \mathcal{O}_X(H))$ where $i : H \subset X$ is the ample divisor class. One idea I had was to use the short exact sequence $ 0 \to \mathcal{O}_X(-H) \to \mathcal{O}_X \to i_* \mathcal{O}_H \to 0$ associated to the divisor $H \subset X$ . Now tensor this by $\mathcal{O}_X(H)$ to get the sequence $0 \to \mathcal{O}_X \to \mathcal{O}_X(H) \to i_* \mathcal{O}_H \to 0$ . I think we have $i_*\oh_H \otimes \oh_X(H) \cong i_*\oh_H$ at the end because $i_* \oh_H \otimes \oh_X(H) \cong i_*(\oh_H \otimes i^*\oh_X(H))$ by projection formula, and the pullback of a structure sheaf is a structure sheaf, but there can be no functions on $H$ with poles on all of $H$ , so $i^* \oh_X(H) \cong \oh_H$ and thus $i_*\oh_H \otimes \oh_X(H) \cong i_*\oh_H$ (I'm not 100% sure this is right though). Now take the long exact sequence in cohomology of the short exact sequence above. We have $H^i(\oh_X)=\mathbb{C},0$ for $i=0, \neq 0$ respectively, but I'm unsure how to find $H^i(i_* \oh_H)$ .","['sheaf-cohomology', 'complex-geometry', 'vector-bundles', 'algebraic-geometry', 'sheaf-theory']"
3804147,"Does an integral expression exist for $\xi(s)\,\xi(-s)$?","The product $\xi^*(s)=\xi(s)\,\xi(-s)$ , with $\xi(s)$ the Riemann_xi_function , but ignoring the first factor $\frac12$ , possesses some 'beauty' in the sense that it yields: $$\xi^*(s)=s^2\,(s^2-1)\,\Gamma\left(\frac{s}{2}\right)\,\Gamma\left(-\frac{s}{2}\right)\,\zeta(s)\,\zeta(-s) = \prod_{n=1}^\infty \left(1- \left(\frac{s}{\rho_n}\right)^2 \right) \left(1- \left(\frac{s}{1-\rho_n}\right)^2 \right)$$ where $\rho_n$ is the $n$ -th non-trivial zero of $\zeta(s)$ . Obviously $\xi^*(s)=\xi^*(-s)$ and it remains an entire function. Its 'elegance' does come at the cost of introducing a second series of $\rho$ s at $-\rho_n$ . I wonder whether there could also exist a 'nice' integral expression for $\xi^*(s)$ ? (with 'nice', I mean not simply a double integral of the known integrals for $\xi(s)$ ). ADDED 1: my first (failed) attempt: Replace $\Gamma\left(\frac{s}{2}\right)\,\Gamma\left(-\frac{s}{2}\right)$ as follows: $$\xi^*(s)=s\,(1-s^2)\,\frac{2\pi}{\sin\left(\frac{\pi s}{2}\right)}\,\zeta(s)\,\zeta(-s)$$ Start the construction from the following integral representation: $$\frac{\pi}{\sin\left(\frac{\pi s}{2}\right)}=\int^\infty_0 \frac{t^{\frac{s}{2}}}{t+t^2}\, dt \qquad 0 < \Re(s) < 2$$ Change variable: $t=n^2\,x, dt = n^2\, dx$ : $$\frac{\pi}{\sin\left(\frac{\pi s}{2}\right)}=\int^\infty_0 n^s x^\frac{s}{2}\,\frac{1}{x+(n\,x)^2}\, dx \qquad 1 < \Re(s) < 2$$ Summation on both sides: $$\frac{\pi}{\sin\left(\frac{\pi s}{2}\right)}\sum^\infty_{n=1} \frac{1}{n^s}=\int^\infty_0 x^\frac{s}{2}\,\sum^\infty_{n=1}\frac{1}{x+(n\,x)^2}\, dx \qquad 1 < \Re(s) < 2$$ Which simplifies into: $$\frac{2\pi}{\sin\left(\frac{\pi s}{2}\right)}\zeta(s)= \int^\infty_0 x^{\frac{s}{2}-1}\,\left(\frac{\pi}{\sqrt{x}}\,\coth\left(\frac{\pi}{\sqrt{x}}\right)-1\right)\, dx \qquad 1 < \Re(s) < 2$$ Change variable once more: $x=k^{-2}\,u, dx = k^{-2}\, du$ and sum both sides: $$\frac{2\pi}{\sin\left(\frac{\pi s}{2}\right)}\zeta(s)\sum^\infty_{k=1} \frac{1}{k^{-s}}= \int^\infty_0 u^{\frac{s}{2}-1}\,\sum^\infty_{k=1}\left(\frac{\pi\,k}{\sqrt{u}}\,\coth\left(\frac{\pi\,k}{\sqrt{u}}\right)-1\right)\, du \qquad \infty$$ This obviously fails and unless I missed a tweak, I do need a new approach... ADDED 2: my second attempt (successful, however with a double integral): $$\xi^*(s)=s^2(s^2-1)\int^\infty_1\int^\infty_1 \left(x^\frac{s}{2}+x^{\frac{1-s}{2}}\right)\left(y^{-\frac{s}{2}}+y^{\frac{1+s}{2}}\right)\frac{\psi(x)\,\psi(y)}{x\,y}\, dx dy + \xi(s)+\xi(-s) -1$$ With $\displaystyle \psi(z)=\sum_{n=1}^\infty e^{-\pi\,n^2z}$ . I did not manage to simplify this any further.","['gamma-function', 'number-theory', 'riemann-zeta', 'hadamard-product']"
3804170,"In how many ways can we rearrrange the digits: $0,1,2,\ldots,9$ if the first digit should be $>1$ and the last one $<8$?","In how many ways can we rearrrange the digits: $0,1,2,\ldots,9$ if the first digit should be $>1$ and the last one $<8$ ? Given answer by the book : $10! - 2 \cdot 9! - 2 \cdot 9! + 4 \cdot 8!$ . I probably miss something here. My approach: There are $3$ ways to violate the constraints If the first digit is $\leq 1$ . Then we have $2$ choices for the first digit, $8$ for the last one ( since it's less than $8$ , the last digit $\in [0,7]$ , and since we picked already two digits out of $10$ available there are $8 \cdot 7 \cdot 6 \ldots \cdot 1 = 8!$ for the other digits of the arrangement . In total : $N(c1)=2 \cdot 8 \cdot 8!$ If the 10th digit is $\geq 8$ . This can happen in two ways ( $8$ or $9$ ), we also have $8$ choices for the first one and $8!$ for everything else . In total : $N(c2)=2 \cdot 8 \cdot 8!$ Both 1 and 2 cases :  If the first digit is $\leq 1$ and the 10th digit is $\geq 8$ . This can happen in $N(c1 \wedge c2)=2 \cdot 2 \cdot 8!$ ways. Without constraint : $10!$ Hence , from inclusion - exclusion principle There are \begin{align*} N(c1 \lor c2)) & = N - (N(c1) + N(c2) - N(c1 \wedge c2))\\ & = 10! - 2 \cdot 8 \cdot 8! -2 \cdot 8 \cdot 8! + 2 \cdot 2 \cdot 8!\\ & = 10! - 2 \cdot 8 \cdot 8! -2 \cdot 8 \cdot 8! + 4 \cdot 8!\end{align*} With the help of some fellow people here , I realised my mistake: If I want to define the fact $N(c1)$ as the case where only the first and only this digit violates the constraint then sure I can do it as long as  I then write $ N(c1 \wedge c2) = 0$ . Otherwise, we can define $N(c1)$ as the case where the first digit violates the constraint without wondering about the last and after we make sure we don't count twice, since $N(c1 \wedge c2) = 0$ this time","['combinatorics', 'discrete-mathematics']"
3804188,Prove that sequence defined by composition of a function is bounded given that function's derivative is bounded [duplicate],"This question already has an answer here : Proving a sequence is bounded under given conditions (1 answer) Closed 2 years ago . Let $f : \mathbb{R} → \mathbb{R}$ be a differentiable function such that its derivative $f'(x)$ .
is a continuous function. Moreover, assume that for all $x \in \mathbb{R}$ , $$0 \leq |f'(x)| \leq \frac{1}{2}.$$ Define a sequence of real numbers $\{a_n\}_{n\in\mathbb{N}}$ such that $$a_1=1$$ $$a_{n+1} = f(a_n) $$ for all $n \in \mathbb{N}$ . Prove that there exists a positive real number $M$ such that for all $n \in \mathbb{N}$ , $$|a_n| \leq M.$$ Source My attempt: We use the Mean Value Theorem. $$\frac{f(a_n)-f(a_{n-1})}{a_n-a_{n-1}} = f'(x)$$ for some $x \in [a_n,a_{n-1}]$ . We know that $-\frac{1}{2} \leq f'(x)\leq \frac{1}{2}$ , hence $$-\frac{1}{2} \leq\frac{f(a_n)-f(a_{n-1})}{a_n-a_{n-1}} \leq \frac{1}{2}.$$ Simplifying, we find that $|a_{n+1} - a_n| \leq \frac{|a_n-a_{n-1}|}{2}$ . Here we define a sequence $x_n = |a_{n+1} -a_n|.$ It is easy to see that $x_n \leq \frac{x_{n-1}}{2}$ which implies that $$ \lim_{n \rightarrow \infty} x_n = 0$$ Which means that $|a_{n-1} - a_n|$ is arbitrarily small and grows slower than $\{X_n : X_n = \Sigma \frac{1}{2^n} \}$ . Therefore the sequence must converge to a finite limit, say $L$ . We choose $M = L+1$ . QED. Is this proof correct? I have a few qualms about it, but a few of my peers looked over it and weren't able to find any problem with it. Please help me with a nudge in the right direction if there turns out to be an error.","['solution-verification', 'sequences-and-series', 'real-analysis']"
3804200,Is topology just a generalization of real analysis?,"While trying to learn undergraduate topology, I came across this lecture by Dr. Zimmerman who claims ""Topology is a generalization of real analysis, a lot of topology anyway."" They are obviously related and topology does seem more general, but this statement still surprised me. Can all of real (and complex) analysis be recast in the framework of topology? Edit: Could I say that real analysis is just studying the topology of $\mathbb{R}$ ?","['general-topology', 'real-analysis']"
3804214,Challenging integral: $\int_0^{\pi/2}x^2\frac{\ln(\sin x)}{\sin x}dx$,"How to tackle $$I=\int_0^{\pi/2}x^2\frac{\ln(\sin x)}{\sin x}dx\ ?$$ This integral popped up in my solution ( see the integral $\mathcal{I_3}\ $ at the end of the solution.) My attempt :  By Weierstrass substitution we have $$I=2\int_0^1\frac{\arctan^2(x)}{x}\ln\left(\frac{2x}{1+x^2}\right)dx$$ $$=2\int_0^1\frac{\ln(2)+\ln x}{x}\arctan^2(x)dx-2\int_0^1\frac{\ln(1+x^2)}{x}\arctan^2(x)dx$$ The first integral simplifies to known harmonic series using the identity $$\arctan^2(x)=\frac12\sum_{n=1}^\infty\frac{(-1)^n\left(H_n-2H_{2n}\right)}{n}x^{2n}$$ But using this series expansion in the second integral yields very complicated harmonic series. Also integrating by parts, yields the integrand $\frac{\text{Li}_2(-x^2)\arctan(x)}{1+x^2}$ which complicates the problem. Any thought how to approach any of these two integrals? Thank you.","['integration', 'definite-integrals', 'harmonic-numbers', 'closed-form', 'sequences-and-series']"
3804215,Let $f(x)=x+\frac{x^2}{2} + \frac{x^3}{3}+\frac{x^4}{4}+\frac{x^5}{5}$ and let $g(x)=f^{-1} (x)$. Find $g’’’(0)$,"My method is extremely inefficient, but here it is $$g(f(x))=x$$ $$g’(f(x)).f’(x)=1$$ Differentiating wrt x multiple times $$g’’’(f(x))(f’(x))^3 + (g’’(f(x)))(2f’(x))(f’’(x)) + g’’(f(x)).f’(x).f’’(x) + g’(f(x)).f’’’(x)=0$$ I may have made some computation errors in this, but I can’t seem to find any (I apologize if there are) $f(x)=0$ at $x=0$ $$g’’’(0) +2 g’’(0) + g’’(0) +2g’(0)=0$$ How do I proceed from here? Is there a better way to approach this ?",['derivatives']
3804243,"Show that there exists $c\in[a,b]$ such that $f(c)=0$.","Question: Let $f:[a,b]\to\mathbb{R}$ be a continuous function with the property that for every $x\in[a,b]$ , there exists $y\in[a,b]$ such that $|f(y)|\le\frac{1}{2}|f(x)|$ . Show that there exists $c\in[a,b]$ such that $f(c)=0$ . Solution: Select any $x\in [a,b].$ Let $x=x_1$ . Now by our hypothesis there exists $x_2\in [a,b]$ such that $|f(x_2)|\le \frac{1}{2}|f(x_1)|.$ Again by our hypothesis there exists $x_3\in[a,b]$ such that $|f(x_3)|\le \frac{1}{2}|f(x_2)|\le \frac{1}{4}|f(x_1)|.$ Continuing like this we will end up having a sequence $(x_n)_{n\ge 1}$ such that $$|f(x_n)|\le \frac{1}{2^{n-1}}|f(x_1)|, \forall n\in\mathbb{N}.$$ Notice that this implies that $$-\frac{1}{2^{n-1}}|f(x_1)|\le f(x_n)\le \frac{1}{2^{n-1}}|f(x_1)|, \forall n\in\mathbb{N}.$$ Thus by Sandwich theorem we can conclude that the sequence $f(x_n)$ is convergent and it converges to $0$ . Next notice that the sequence $(x_n)_{n\ge 1}$ is bounded. Thus, by Bolzano-Weierstrass theorem we can conclude that $(x_n)_{n\ge 1}$ has a convergent subsequence $(x_{n_k})_{k\ge 1}$ . Let us assume that $(x_{n_k})_{k\ge 1}$ converges to $c$ . Note that $a\le c\le b$ . Now since $f$ is continuous on $[a,b]$ , implies that $f$ is continuous at $c$ . Thus by the sequential definition of limit we can conclude that $f(x_{n_k})$ converges to $f(c)$ . Now note that we have already shown that the sequence $f(x_n)$ converges to $0$ , which implies that the subsequence $f(x_{n_k})$ also converges to $0$ . This implies that $f(c)=0.$ This completes the proof. It is also easy to see that if the inequality $|f(y)|\le \frac{1}{2}|f(x)|$ was replaced by the inequality $|f(y)|\le \lambda |f(x)|$ where $|\lambda|<1$ is arbitrary then also the statement in the question holds true. Is this solution correct and rigorous enough and is there any other way to solve this problem?","['solution-verification', 'sequences-and-series', 'real-analysis']"
3804264,Limit of sequence of Lebesgue integrals over symmetric domains,"I'm trying to show the following: If $f \in L^1(\mathbb{R})$ with $f$ nonnegative, then $$\lim_{n \to \infty}\frac{1}{n}\int_{-n}^{n}tf(t)dt=0$$ I""ve shown that for every $n≥0$ we have $$
\frac{1}{n}\int_{-n}^{n}tf(t)dt≤\int_{-n}^{n}f(t)dt$$ but I'm not sure if that's useful or not. My aim is to employ one of the standard convergence theorems, but I'm not sure how to set it up so far.","['measure-theory', 'lebesgue-integral', 'real-analysis']"
3804327,Proof that strong convexity implies Polyak-Lojasiewicz inequality is satisfied,"I am attempting to find a proof of strong-convexity implying the Polyak- Lojasiewicz (PL) inequality is satisfied. As I understand it, strong convexity means that: $$ \textbf{H} f \succcurlyeq \mu I$$ for some $\mu>0$ , where $\textbf{H}f$ is the Hessian matrix of $f$ and $I$ is the identity matrix. The PL inequality states that: $$\frac{1}{2}\|\nabla f(x)\|^2 \geq \mu(f(x) - f(x^*))$$ where $x^*$ is the value of $x$ for which $f(x)$ is minimized. This paper http://www.optimization-online.org/DB_FILE/2016/08/5590.pdf states that if a function is strongly convex with constant $\mu$ , then it must satisfy the PL inequality with the same constant (page 4, section 2.3, first sentence). I found a source that says this can be done by minimizing the following inequality: $$f(y)\ge f(x)+\nabla f(x)^T(y-x)+\frac{\mu}{2}\lVert y-x \rVert^2 \tag{1}$$ in terms of $y$ to get $$f(x^*) \ge f(x)-\frac{1}{2\mu}\|\nabla f(x)\|^2.$$ I don't understand this. How exactly is the inequality being minimized in terms of $y$ here? How do I go from strong convexity to equation (1) to this result?","['convex-optimization', 'multivariable-calculus', 'linear-algebra', 'optimization', 'convex-analysis']"
3804330,"Does there exist $f(x)$ such that $\int_0^x f(t) \, dt +x=1$?","Does there exist $f(x)$ (continuous) such that $\int_0^x f(t) \, dt +x=1$ ? My guess is that it doesn't since setting $x=0$ we obtain $0=1$ . Is this assertion correct?","['calculus', 'ordinary-differential-equations']"
3804359,Torsion in geometry v.s. Torsion in topology?,"Torsion in geometry: There are meanings of torsion of a curve https://en.wikipedia.org/wiki/Torsion_of_a_curve Torsion tensor in Riemannian geometry https://en.wikipedia.org/wiki/Torsion_tensor Torsion in topology: such as analytic torsion https://en.wikipedia.org/wiki/Analytic_torsion#:~:text=Reidemeister%20torsion%20was%20the%20first,used%20to%20classify%20lens%20spaces . Reidemeister Torsion https://mathworld.wolfram.com/ReidemeisterTorsion.html torsion in cohomology https://mathoverflow.net/questions/22583/why-torsion-is-important-in-cohomology My question is that: Whether torsion in geometry is related to the torsion in topology? Why they are all called Torsions?","['differential-geometry', 'torsion-groups', 'differential-topology', 'algebraic-topology', 'terminology']"
3804375,Folland ordinals exercise,"Consider following exercise from Folland's book: I succesfully proved (a),(b),(c) and now I want to prove (d). Attempt (outline) :
Put $E:= \bigcap_1^\infty E_n$ . We may assume $E \subseteq \Omega$ by removing the element $\omega_1$ (this modified is uncountable iff the original set is uncountable). We use (c) to prove that $E$ is uncountable. So, let $x \in \Omega$ . Define a sequence $(x_n)_n$ by a diagonal argument: Define $x_0:= x+1$ $x_1 \in E_1, x_2 \in E_2, x_3 \in E_1, x_4 \in E_2, x_5 \in E_3, x_6 \in E_1, x_7 \in E_2, x_8 \in E_3, x_9 \in E_4, x_{10}\in E_1$ and continue. We can also arrange that $x_1 < x_2 < x_3 <\dots$ , again using (c). Following the hint, $(x_n)_n$ converges. I proved that $\lim_n x_n = \sup_n x_n = \bigcup_n x_n$ . By construction thus $y:=\lim_n x_n \geq x_0 =x+1> x $ . Also, all subsequences of $(x_n)$ converge to $x$ so  since $(x_n)_n$ contains subsequences consisting entirely from terms of each $E_n$ , we see that $y=\lim_n x_n \in \overline{E_n} = E_n$ for all $n$ and hence $y \in E$ , as desired. Does the above proof outline look correct?","['general-topology', 'ordinals', 'measure-theory', 'real-analysis']"
3804385,"Proof of inner product property: $\langle Ax,x \rangle = \langle x,A^Tx \rangle$ [duplicate]","This question already has an answer here : Property of the conjugate transpose matrix with inner product (1 answer) Closed 3 years ago . $A$ is a matrix in $\mathbb{R}^{m \times n}$ and $x \in \mathbb{R}^n$ . I want to prove that: $$\langle Ax, x \rangle = \langle x , A^T x \rangle $$ Having $A^T$ as the transpose of the matrix $A$ . Just realized I use this property so much, yet I don't known how to prove it.","['matrices', 'inner-products', 'linear-algebra']"
3804467,Seeking intuition for why tesselations of space by hypercubes in dimensions 8+ need not have a face-to-face pair (Keller's conjecture counterexample),"According to Keller's conjecture : In any tiling of Euclidean space by identical hypercubes there are two cubes that meet face to face. Perhaps surprisingly, this is false for every dimension greater than 7. See here for 2D and 3D visuals. Is there any intuitive way to ""visualise"" how dimension 8 and above manage to tile the space in such a way?","['visualization', 'tessellations', 'geometry']"
3804512,Right triangle with rational sides and area = 1 equivalent to n = 3 case of Fermat's Last Theorem,"I watched a talk by Andrew Wiles in which he spoke about the proof of Fermat's Last Theorem, and he said something that has puzzled me. He mentioned that the $n=3$ case of FLT (i.e. proving $a^3+b^3=c^3$ , where $abc\neq0$ , has no integer solutions) was equivalent to proving that there cannot be a right triangle with rational sides and area $=1$ . I have been trying to deduce why that is the case. I haven't gotten very far with it, but my thoughts so far are as follows: Assume there is a right triangle with rational sides and area $=1$ . The side-lengths would have a least common denominator, call it $d$ . Then we can say the legs are $\frac{m}{d}$ , $\frac{n}{d}$ , and the hypotenuse is $\frac{p}{d}$ , for some positive integers $m, n$ , and $p$ . Pythagoras gives: $\frac{m^2}{d^2}+\frac{n^2}{d^2}=\frac{p^2}{d^2}$ , or just $m^2+n^2=p^2$ . The area condition gives $\frac12\cdot\frac{m}{d}\cdot\frac{n}{d}=1$ , or equivalently $mn=2d^2$ . This is my first dead end. Try as I might to do manipulations, I can't see how the above implies the existence of integers $a, b, c$ satisfying $a^3+b^3=c^3$ . So I thought I'd try in reverse, and start by assuming some non-zero integers satisfy $a^3+b^3=c^3$ . One thought is to factor the sum of cubes on the LHS, to get $(a+b)(a^2-ab+b^2)=c^3$ . Then, thinking of Pythagoras, I can rearrange to isolate $a^2+b^2$ , which looks like $a^2+b^2=\frac{c^3}{a+b}+ab$ . However here is where I am stuck again. I don't really have a good idea of what to do with the right side. I'd appreciate help connecting the dots. Just FYI, I don't know much at all about elliptic curves, other than their name and that they were something Wiles used in his full proof of FLT. So I'm hoping there are  elementary approaches to finish my work. But I'd also like to know if there is no elementary way to finish what I started, and elliptic curves (or something else) are necessary.","['algebra-precalculus', 'geometry', 'elliptic-curves']"
3804534,"Let $(X, S, \mu)$ a measure space, $f$ non negative function.","Let $(X, S, \mu)$ a finite measure space, $f$ non negative function. Show: $$\int f \,d\mu < +\infty$$ if only if $$\sum_{n=0}^\infty 2^{n}\mu(\{x\in X \mid f(x) \geq 2^n \}) < + \infty.$$ $(\implies)$ if $\int f \, d\mu = M < +\infty$ , $2^n$ is non-negative, by Chebyshev inequality: $$\mu(\{x\in X \mid f(x) \geq 2^n \} \leq \frac{M}{2^n}, \text{ where } \frac{M}{2^n} \rightarrow 0 \text{ when } n \rightarrow \infty.$$ $$\sum_{n=0}^\infty 2^n\mu(\{x\in X \mid f(x) \geq 2^n \}) \leq \sum_{n=0}^\infty 2^n \frac{M}{2^n} \rightarrow 0. $$ Then $$\sum_{n=0}^\infty 2^n\mu(\{x\in X \mid f(x) \geq 2^n \}) < +\infty.$$ ( $\Leftarrow$ ) If $$\sum_{n=0}^\infty 2^n \mu(\{x\in X \mid f(x) \geq 2^n \}) < + \infty.$$ Is the first implication correct?
I am trying the other implication also.","['measure-theory', 'lebesgue-measure', 'measurable-functions']"
3804563,"How to create a high dimensional Gaussian that isn't ""spiky""?","When sampling from a high dimensional Gaussian distribution the results are similar to sampling from a uniform distribution on a unit sphere. Upon searching, I was able to find this mentioned in [1]. This phenomenon already begins in 3D, see [2]. My objective is to construct a distribution in high dimensions such that the property of Gaussians in 1D and 2D holds, mainly that when given a random sample from such a desired distribution, the best guess as to the mean of that distribution is that given sample. And so that there would be a ""falloff"" from the mean. How can this be achieved? Can it be done? [1] https://www.inference.vc/high-dimensional-gaussian-distributions-are-soap-bubble/ [2] https://en.wikipedia.org/wiki/Proof_of_Stein%27s_example Edit: After some more keyword googling, I was able to find this very recent Twitter thread which further expands on the problem: https://twitter.com/johncarlosbaez/status/1298274201682325509","['statistics', 'gaussian', 'normal-distribution']"
3804576,"Euler products, Merten's theorems, and an unexpected result","I'm going to start by saying I'm mostly out of my depth here. I'm an amateur recreational mathematician. But I've been looking at the Twin Prime Conjecture lately, because it is so fascinating. Easy enough for an 8-year-old to understand, but perplexing mathematicians for millennia. As a science teacher, the whole preponderance-of-empirical-evidence thing seems reasonable enough... but of course, not for mathematicians. I have no delusion I'll prove anything, but maybe I can contribute something useful? And I have a weird result that may or may not be useful, that I also can't explain. In relation to the TPC, I've been working on a sieve that generates This OEIS sequence , the numbers for which $6k \pm 1$ are both prime. I'm trying to build off of the work of Dinculescu's various papers (one linked, others at the OEIS link). The TPC is the conjecture that this sequence is infinite. What does this have to do with Euler products? I've been working with the following one: $$\prod_{5 \leq p \leq n} \left(1-\frac2p\right)$$ Simple enough. It starts at 5 because it's looking at twin primes, and has a 2 in there because the sieve removes $\frac2p$ of $\mathbb{N}$ each pass. There are a lot of constants that can be calculated using Euler products . The one I'm working with above isn't one of them, or even similar. So I poked around a bit. Merten's 3rd theorem says this: $$\lim_{n \to \infty}\ln n\prod_{p \leq n} \left(1 - \frac1p \right) = e^{-\gamma}$$ This theorem seemed interesting to me because the Euler product portion is basically the leftovers in $\mathbb{N}$ after $n$ iterations of the Sieve of Eratosthenes. As $n \to \infty$ , the ""leftover"" set is just $\mathbb{P}$ . So, the Sieve of Eratosthenes diverges to zero (as we'd expect since the density of $\mathbb{P}$ is zero), but when multiplied by $\ln n$ , it converges. That's a pretty cool trick! For the heck of it, I tried multiplying the iterations of my product by $\ln n$ to see what came out, which was still divergence to zero, uninteresting. So I looked at Hardy and Littlewood and noticed their constant ends up with an $(\ln n)^2$ term when calculating $\pi_2(n)$ . Since my product diverges to zero about twice as fast as Mertens's, I decided to try that out, and I got something that converged! And I tried to figure out if the value it converged to ( $\approx 2.49726$ ) was a known constant. After some trial and error I found: $$\lim_{n \to\infty} (\ln n)^2 \prod_{5 \leq p \leq n} \left(1-\frac2p\right) \approx 4\lambda$$ Where $\lambda$ is... the Golomb-Dickman constant, something I'd never heard of. (This is by empricial computer calculation only, but it converges to four digits with about 6 million primes.) Apparently $\lambda d$ is the (asymptotic) average number of digits in the largest prime factor a $d$ -digit integer. But! We come around full circle. The complement of that OEIS sequence is the subset of $\mathbb{N}$ for which $6k \pm 1$ are not twin primes. These are all the natural numbers of the form $6ab \pm a \pm b, 1 \leq a \leq b$ . That fact was discovered by.... Solomon Golomb . I have exactly zero clue as to whether what I've written here is particularly useful. I certainly can't explain it. But at least to me, it's pretty interesting. Has anyone here worked with this sequence, and the TPC, and might have an explanation for what seems like it's maybe just a cosmic coincidence? Is this a new result, or am I chasing rabbits others have already chased? (Edited (1): Typo in the numerical result. After ~6M primes, my value has converged to $2.49726$ , not $2.49276$ . Only correct to four digits, not five.) (Edited (2): Gerry Myerson, in the comments, helpfully pointed out a reference that contained the product I was looking at, though starting at 3 rather than 5.  It turns out that my original product converges to $12C_2e^{-2\gamma} \approx 2.497287$ , which just happens to be very very close to $4\lambda \approx 2.497320$ . The $0.0013%$ difference between the makes them easy enough to confuse when you don't know what you're approaching asymptotically.)","['number-theory', 'twin-primes', 'euler-product', 'prime-numbers']"
3804596,"Let $n \geq 3$. Take an $2n \times 2n$ chessboard, and remove $2$ white pieces and $2$ black pieces, can you always cover it with dominoes?","I am reading ""Kombinatorika"" by Laszlo Lovasz, Katalin Vesztergombi and Jozsef Pelikan(in Japanese, translated and arranged by Jin Akiyama and Peter Frankl). There is the following problem in this book: Let $n \geq 3$ . Take an $2n \times 2n$ chessboard, and remove $2$ white pieces and $2$ black pieces, can you always cover it with  dominoes? The authors(or the translators) wrote ""No. But we don't write an example."". I wonder why the authors(or the translators) didn't write an example since there is an obvious example(see the image below): By the way, I checked the case $n = 3,4,5$ by my Java program (which uses bipartite matching algorithm) and I was not able to find a non-trivial example. Is there non-trivial examples?","['chessboard', 'recreational-mathematics', 'combinatorics', 'discrete-mathematics']"
3804597,Complete geometry theory other than Tarski?,"It is well known that Tarski's axioms for Euclidean geometry is recursively axiomatizable and complete, and hence also decidable. Basically it is because the theory of real closed field has these properties. Does there exist other Euclidean or non-Euclidean geometry theory, that extends Hilbert's axioms and share these properties? Regarding Euclidean geometry, since models of Hilbert's axioms plus parallel axiom are exactly cartesian products over ordered Pythagorean fields, the question is, as I understand, the same as asking for complete extension of theory of ordered Pythagorean field. I know very little about model theory but it seems there are not many theories known to be decidable. I know even less about models of non-Euclidean geometry. I just learned that (to my surprise) the theory of hyperbolic geometry is decidable . I could not find the reference. Does this refer to the finite theory consisting of Hilbert's axioms plus a suitable hyperbolic axiom, or some infinite theory that resembles Tarski's axioms?","['euclidean-geometry', 'model-theory', 'logic', 'geometry', 'hyperbolic-geometry']"
3804611,"ELMO 2019/G3: Prove that if $GH$ and $EF$ meet at $T$, then $DT\perp EF$.","Let $\triangle ABC$ be an acute triangle with incenter $I$ and circumcenter $O$ . The incircle touches sides $BC,CA,$ and $AB$ at $D,E,$ and $F$ respectively, and $A'$ is the reflection of $A$ over $O$ . The circumcircles of $ABC$ and $A'EF$ meet at $G$ , and the circumcircles of $AMG$ and $A'EF$ meet at a point $H\neq G$ , where $M$ is the midpoint of $EF$ . Prove that if $GH$ and $EF$ meet at $T$ , then $DT\perp EF$ . My Progress : After seeing this problem, the first thing that struck my mind was sharky devil lemma (not a very known lemma ) Here's the lemma:
In triangle $ABC$ , let $DEF$ be the contact triangle, and let $(M)$ be the midpoint of the arc $(BC)$ not containing $(A)$ in $(ABC)$ . Suppose ray $MD$ meets $(ABC)$ again at $R$ . If $I$ is the incenter of $(ABC)$ and ray $RI$ intersects $(ABC)$ again at $A'$ , then $A'$ is the antipode of $A$ . If $P=RA'\cap EF$ , then $DP\perp EF$ . Anyways, here's the problem's diagram: Here $J$ is defined as $(ABC)\cap (AEF) .$ Now, if I am able to show that $JITA'$ are collinear, then I am done. Moreover, I got that $T$ is the radical centre of $(AEF),(GHA')$ and $(AHG)$ . Here, I defined $K$ as $AT\cap (AEF)$ . Now,I thought of using Phantom points . So I defined $T'= \overline{JIA'}\cap EF$ . We want to show that $T'=T $ . To show that $T'=T $ , we can also show $G,T',H$ . Now, note that $AM\perp EF$ . Let $AJ\cap EF=L$ . So, by radical axis lemma on $(AEF),(ABC) ,(GH'EF)$ , we get $AJ,EF,GA'$ concur at $L$ . Also we have $T'KMI$ and $AJT'M$ cyclic . Again by radical axis lemma on $(AEF),(AJT'M),(T'MKI)$ , we get $AJ,TM(EF),KI$ concur at $L$ . Note that $\angle AGA'=90=\angle AMF$ . Since $LFE$ and $LGA$ are collinear  , we get $(AHMGLK)$ concyclic. Also note that $T'$ is the orthocentre of $\Delta ALI$ . This is what I got till now. Now after showing that J,I,T are collinear , by applying sharky devil lemma , we will be done . I know that this problem has a 1 para solution( by @Anand ), but can someone provide a non-projective solution ? Thanks in Advance!","['contest-math', 'euclidean-geometry', 'geometry']"
3804622,How to calculate the radius of a circle composed of many smaller circles?,"I'm making a CAD model of a ball bearing. It contains 14 balls (perfect spheres) arranged in a perfect circle pattern. So if each ball has a radius of $r$ , how do I calculate the radius of the entire circular pattern? (I'm looking for the radius of the circle containing the CENTER POINTS of all the balls.) This example photo is a ball bearing with 9 balls, but mine has 14 balls - but this should help illustrate the situation:","['trigonometry', 'circles']"
3804631,Find the derivative of $f(x)$,"$f(x)=\begin{cases}1 & x\in\Bbb{Q}\\ \sin x& x\notin\Bbb{Q}\end{cases}$ I know when $x=2\pi(n-1)+\frac{\pi}{2},\space f(x)=1$ when $x=2\pi(n-1)-\frac{\pi}{2},\space f(x)=-1$ when $x=n\pi,\space f(x)=0$ Is $f(x)$ then differentiable? I assume it is only differentiable when $x\in\Bbb{Q} $ and $x=2\pi(n-1)+\frac{\pi}{2}$ . Am I correct?","['continuity', 'derivatives']"
3804657,"Integral of Brownian motion over $[0,1]$","Define $$ Y \equiv \int_0^1 \frac{B_s}{\sqrt{s}} ds$$ I want to calculate the mean and variance of $Y$ but I'm not sure if I've done it correctly: The mean is fairly easy: $$E(Y) = \int \int_0^1 \frac{B_s(\omega)}{\sqrt{s}} ds dP(\omega) = \int_0^1 \int\frac{B_s(\omega)}{\sqrt{s}} dP(\omega) ds = 0$$ where we may exchange integrals by Fubini's theorem noting that $E|B_s| = \sqrt{2s/\pi}$ The variance is where I'm confused.  We know that by Ito's lemma $$\sqrt{t}B_t = \frac{1}{2}\int_0^t \frac{B_s}{\sqrt{s}} + \int_0^t\sqrt{s}dB_s$$ There is no quadratic variation term since $\sqrt{t}$ is FV.  Set $t = 1$ to obtain $$Y/2 = B_1 - \int_0^1 \sqrt{s} dB_s = \int_0^1 (1- \sqrt{s}) dB_s $$ so by Ito isometry we get $$E(Y^2) = 4 \int_0^t (1-\sqrt{s})^2 ds $$ I'm flat out too lazy to do the integral and finish this, but I just want to double check that I'm doing this question correctly.  Thanks if you can confirm!","['stochastic-integrals', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
3804696,Characterize hypersurface given by $\det(\sum_{k=1}^n x_k A_k) = 0$,"Consider $n$ matrices $A_k \in \mathbb R^{n \times n}$ . The equation $\det\left(\sum_{k=1}^n x_k A_k\right) = 0$ describes (in general) a hypersurface when $x \in \mathbb R^n$ . Below is an example of such surface in $\mathbb R^3$ given by $\det \begin{pmatrix}
x_1 + x_2 & x_3 & 0\\
x_2 & x_2 + x_3 & x_1\\
x_2 & 0 & x_1 + x_3
\end{pmatrix} = 0$ Since $\det\left(\sum_{k=1}^n x_k A_k\right) = 0$ implies $\det\left(\sum_{k=1}^n (\lambda x_k) A_k\right) = 0$ it is enough to consider this equation on the unit sphere $x^2 = 1$ . I'm wondering if these surfaces are known and studied. Particularly I'm interested in testing whether for given $A_k$ and starting and terminating points $A, B$ there exists a continuous path $x(t)$ such that $x(0) = A, \quad x(1) = B$ which never crosses the surface: $\det (A_k x_k(t)) \neq 0, \forall t \in [0, 1]$ . My observations: If all matrices are diagonal: $A_k = \operatorname{diag}_i(d_i^{(k)})$ the determinant is easy to compute: $$
0 = \det (\sum_{k=1}^n x_k A_k) = \prod_{i=1}^n \sum_{k=1}^n x_k d_i^{(k)}
$$ The solution is a union of $n$ planes given by $\sum_{k=1}^n x_k d_i^{(k)} = 0$ . For nonsingular $S, T \in \mathbb R^{n \times n}$ the substitution $A_k \to S A_k T$ does not change the equation. Combined with the first observation this gives a solution in case when all $A_k$ can be simultaneously diagonalized by a pair of matrices. If matrices are symmetric and positive definite then $\mathbb R_+^n$ does not intersect with the surface. This problem arose when I tried to solve numerically a quadratic system of equations given by $x^\top B_k x = c_k$ and observed that the numerical method ""hits a wall"" at a point where jacobian of the system vanishes. In my particular case matrices $B_k$ are indefinite.","['determinant', 'matrices', 'jacobian', 'manifolds', 'quadratic-forms']"
3804775,Find the inequality with the best possible $k= constant$ (with the condition $x^{2}+ y^{2}\leq k$).,"Find the inequality with the best possible $constant$ Given two non-negative numbers $x, y$ so that $x^{2}+ y^{2}\leq \frac{2}{7}$ . Prove that $$\frac{1}{1+ x^{2}}+ \frac{1}{1+ y^{2}}+ \frac{1}{1+ xy}\leq \frac{3}{1+ \left ( \frac{x+ y}{2} \right )^{2}}$$ where $constant= \frac{2}{7}$ is the best possible. Given two non-negative numbers $x, y$ so that $x^{2}+ y^{2}\leq \frac{2}{5}$ . Prove that $$\frac{1}{\sqrt{1+ x^{2}}}+ \frac{1}{\sqrt{1+ y^{2}}}+ \frac{1}{\sqrt{1+ xy}}\leq \frac{3}{\sqrt{1+ \left ( \frac{x+ y}{2} \right )^{2}}}$$ where $constant= \frac{2}{5}$ is the best possible. They are my two examples. I'm looking forward to seeing more many inequalities alike. Thanks for all your nice comments.","['uvw', 'fractions', 'optimization', 'inequality', 'derivatives']"
3804800,Convergence in Probability question.,"Let $\lambda_n = 1/n$ for $n=1,2,\ldots$ . Let $X_n \sim Poi(\lambda_n)$ . Show that a) $X_n \rightarrow_P 0$ . b) Let $Y_n = nX_n$ . Show that $Y_n \rightarrow_P 0$ (where $\rightarrow_{P}$ denotes convergence in Probability) Part a) is relatively straight forward as
We know that $\mathbb{E}(X_n) = \lambda_n = 1/n$ and $\mathrm{var}(X_n) = \lambda_n = 1/n$ . We know that since $X_n >0$ that $|X_n| = X_n$ . Then from Chebyshevs inequality we know that $$P(X_n >\epsilon) \leq \frac{\mathrm{var}(X_n)^2}{\epsilon^2} = \frac{1}{n^2\epsilon^2} \rightarrow 0\;\mathrm{as\;}n \rightarrow \infty. $$ For part b) i cannot use this same approach as $\mathbb{var}(Y_n) = n^2*1/n = n$ . Also i know convergence in quadratic mean implies convergence in probability but $\mathbb{E}(Y_n - 0)^2 = n^2(\mathbb{E}(X_n^2)) = n^2(1/n + 1/n^2) \not \rightarrow \infty$ as $n \rightarrow \infty$ . Any ideas?","['probability-limit-theorems', 'probability-distributions', 'convergence-divergence', 'probability-theory', 'probability']"
3804813,Holder's Inequality for integrals (non-negative functions),"Let's recall Young's Inequality . Statement: Let $u, v \geqslant 0$ , and $p, q \in (0, \infty)$ such that $$ uv \leqslant \frac{u^p}{p} + \frac{v^q}{q}$$ $\blacksquare~$ Problem: Let $p,q$ (Holder Conjgates) be positive real numbers satisfying $$\frac{1}{p} + \frac{1}{q} =1 $$ Then prove the following If $f,g$ are Riemann integrable non-negative functions, then $$
\int_a^b fg~ \mathrm{d}x  \leqslant \left\{\int_a^b f^p ~\mathrm{d}x\right\}^{\frac{1}{p}} \left\{\int_a^b g^q ~\mathrm{d}x\right
\}^{\frac{1}{q}}
$$ $\blacksquare~$ Solution: The problem is trivial (equality holds) when the value of both integrals is $0$ . Then let's consider the first case (reduced) as $\bullet~$ Case $1$ : If $$ \int_a^b f^p ~\mathrm{d}x = \int_a^b g^q ~\mathrm{d}x = 1 \implies \left\{\int_a^b f^p ~\mathrm{d}x \right\}^\frac{1}{p} \left\{ \int_a^b g^q ~\mathrm{d}x \right\}^\frac{1}{q} = 1  $$ then from Young's Inequality we have that \begin{equation*}
    \begin{split}
        fg &\leqslant \frac{f^p}{p} + \frac{g^q}{q}\\
      \implies   \int_a^b fg ~\mathrm{d}x &\leqslant \frac{1}{p} \int_a^b f^p~\mathrm{d}x + \frac{1}{q} \int_a^b g^q~\mathrm{d}x\\
      \implies \int_a^b fg~\mathrm{d}x &\leqslant \frac{1}{p} + \frac{1}{q} = 1 = \left\{\int_a^b f^p ~\mathrm{d}x \right\}^\frac{1}{p} \left\{ \int_a^b g^q ~\mathrm{d}x \right\}^\frac{1}{q}
    \end{split}
\end{equation*} Hence, we have established the inequality for $\textbf{Case 1.}$ $\bullet~$ Case $2$ : The general case, i.e., $$ \int_a^b f^p \mathrm{d}x ~\text{ and }~ \int_a^b g^q \mathrm{d}x \neq 1$$ Then let's assume that \begin{align}
\label{equation 4}
    \int_a^b f^p \mathrm{d}x = \alpha^p ~\text{ and }~ \int_a^b g^q \mathrm{d}x = \beta^q \quad \text{for }~\alpha, \beta \in \mathbb{R}^{+} 
\end{align} Thus we can easily see that \begin{align}
\label{general case}
    \int_a^b \left(\frac{f}{\alpha}\right)^p \mathrm{d}x = 1 ~\text{ and }~ \int_a^b \left(\frac{g}{\beta}\right)^q \mathrm{d}x = 1 \quad \text{for }~\alpha, \beta \in \mathbb{R}^{+}
\end{align} Then from Young's Inequality we have that \begin{equation*}
    \begin{split}
        \left( \frac{fg}{\alpha \beta} \right) &\leqslant \left( \frac{f^p}{p \cdot \alpha^p} \right) + \left( \frac{g^q}{q \cdot \beta^q} \right)\\
      \implies   \int_a^b \left( \frac{fg}{\alpha \beta} \right) ~\mathrm{d}x &\leqslant \frac{1}{p} \int_a^b \left(\frac{f}{\alpha}\right)^p~\mathrm{d}x + \frac{1}{q} \int_a^b \left( \frac{g}{\beta} \right)^q~\mathrm{d}x  \\
      \implies \frac{1}{\alpha \beta} \int_a^b fg~\mathrm{d}x &\leqslant \frac{1}{p} + \frac{1}{q} = 1 \quad  \\
      \implies \int_a^b fg ~\mathrm{d}x &\leqslant \alpha \beta = (\alpha^p)^{\frac{1}{p}} \cdot (\beta^q)^\frac{1}{q} \\
      \implies \int_a^b fg ~\mathrm{d}x &\leqslant \left\{\int_a^b f^p ~\mathrm{d}x \right\}^\frac{1}{p} \left\{ \int_a^b g^q ~\mathrm{d}x \right\}^\frac{1}{q} \quad [\text{From our construction}]
    \end{split}
\end{equation*} Thus, we have proved $\textbf{Case 2.}$ The equality holds when $\beta^q \cdot f^p = \alpha^p \cdot g^q$ . Hence we are done! Please check for glitches and if it's fine, it'll be great if I get another way of solution. Thanks in Advance Guys! :)","['definite-integrals', 'real-analysis', 'solution-verification', 'integral-inequality', 'holder-inequality']"
3804820,Find all $x\in \mathbb{R}$ for which $\frac{8^x+27^x}{12^x+18^x}=\frac{7}{6}$,"Find all $x\in \mathbb{R}$ for which $$\frac{8^x+27^x}{12^x+18^x}=\frac{7}{6}$$ Letting $a=2^x$ and $b=3^x$ we get $$\frac{a^3+b^3}{a^2b+ab^2} = \frac{7}{6}$$ from the numerator we have that $$a^3+b^3=(a+b)(a^2-ab+b^2)=7$$ since $7$ is a prime we can say that $$a+b=1, a^2-ab+b^2=7.$$ It follows that $$a=1-b$$ from where $$(1-b)^2-(1-b)b+b²=7$$ this quadratic has solutions $b=1, b=0.$ what I now did was consider cases. Firstly $a=b=0$ which has no solutions for $x$ . case $a=b=1$ has the solution $x=0$ . However the actual solutions for this were $x=1, x=-1$ which I don't see how they came up with. What is wrong with my approach?","['contest-math', 'algebra-precalculus', 'solution-verification']"
3804843,If $f'(x)=1+f(x)$ and $f(0)=0$ prove that $f(x) = e^x -1$,"Let $f: \mathbb{R} \to \mathbb{R} $ be differentiable.
Also let $f'(x)=1+f(x)$ and $f(0)=0$ How to prove that $f(x) = e^x -1$ ? My Approach $f'(x) = (e^x -1)' = e^x \iff \boxed{f(x) = e^x +c} \quad (1) $ $f(0)=0 \iff f(0) = e^0 +c=0 \iff 1+c = 0\iff \boxed{c=-1} \quad (2)$ Hence $f(x) = e^x -1$ I don't like this proof because it begins with the hypothesis $f(x) = e^x -1$ . Suppose we didn't know the result from the start, how could we derive to it by using only that: $f: \mathbb{R} \to \mathbb{R} $ , differentiable. And $f'(x)=1+f(x)$ and $f(0)=0$","['calculus', 'derivatives']"
3804853,Show that sum of directional derivatives at a point on a manifold doesn't depend on the representation of the curve,"If $ v_1 , v_2 \in T_{p} \mathcal M$ . $p$ is a point on a manifold $\mathcal M$ ,( $p \in \mathcal M$ ),
The tangent space $T_p \mathcal M$ is given a vector space structure by defining \begin{align}
	v_1 + v_2 := [\phi^{-1} \circ ( \phi \circ \sigma_1 + \phi \circ \sigma_1 )]
\end{align} Here $\phi$ is the map that maps from the open set of $p \in \mathcal M$ to $\mathbb{R}^m$ . $\phi(0) = \mathbf{0}$ And $\sigma_1,\sigma_2 : (-\epsilon,\epsilon) \to \mathcal M$ are curves. Such that $v_1 = [\sigma_1]$ and $v_2 = [\sigma_2]$ I want to prove that this definition is independent of charts. To do that I use the definition of directional derivatives \begin{align}
	v(f) := \left. \frac{d(f\circ \sigma)}{dt}\right|_{t=0} = (f\circ \sigma)'(0) \quad v = [\sigma]
\end{align} This is independet of the representation of $[\sigma]$ , (shown here ) I use chain rule as per the multi variable calculus Linearity of derivarives is also used This is my attempt of the proof \begin{align}
	(v_1 + v_2)(f) &= (f \circ \phi^{-1} \circ ( \phi \circ \sigma_1 + \phi \circ \sigma_2 ))'(0) 
\tag{1}\label{1} \\
&=  (f \circ \phi^{-1} )'\left(( \phi \circ \sigma_1 + \phi \circ \sigma_2 )(0)\right)\circ( \phi \circ \sigma_1 + \phi \circ \sigma_1 )'(0) \tag{2}\label{2} \\
&=(f \circ \phi^{-1} )'\left(0\right)\circ( \phi \circ \sigma_1 + \phi \circ \sigma_2 )'(0) \tag{3}\label{3} \\
&=(f \circ \phi^{-1} )'\left(0\right)\circ( \phi \circ \sigma_1  )'(0) +
(f \circ \phi^{-1} )'\left(0\right)\circ(\phi \circ \sigma_2 )'(0)  \tag{4}\label{4}
\\
&=(f \circ\sigma_1  )'(0) +(f \circ\sigma_2 )'(0)   \tag{5}\label{5}
\\
&= v_1(f) + v_2(f)\tag{6}\label{6}
\end{align} Writing the directional derivative along $ v_1 + v_2$ Using the chain rule $(f \circ g )'(x) = f'(g(x))\circ g'(x)$ Using , $ \sigma_1(0) = \sigma_2(0) = p$ , and $\phi(p) = \mathbf 0$ Using the linearity of derivatives $ f'(0)\circ (g_1 + g_2)(0) = f'(0) \circ g_1(0) + f'(0) \circ g_2(0)$ Reverse usage of chain rule and replacing $\phi^{-1} \circ \phi$ as $1$ Recognizing the forms of directional derivatives In the last step, each term in the expression is independent of the representation of the curve and also independent of the charts around $p \in \mathcal M$ for any arbitrary $f \in C^{\infty} (\mathcal M)$ , so $v_1 + v_2$ doesn't depend on the representation of the curves $\sigma_1$ or $\sigma_2$ , any representation from $[\sigma_1]$ or $[\sigma_2]$ will work. $$\tag*{$\blacksquare$}$$ Is this the right way of doing it? I am unsure about the linearity of the derivative \eqref{4}. P.S: This proof is essential to show that the tangent space is a vector space. I am following this book, Isham, Chris J. , Modern differential
geometry for physicists., World Scientific Lecture Notes in Physics.
61. Singapore: World Scientific. xiii, 289 p. (1999). ZBL0931.53002 .","['manifolds', 'differential-geometry']"
3804862,"How to calculate $\lim \limits_{(x,y)→(0,0)} \frac{(x^2+y^2)^2}{xy}$?","The wolframalpha gives the answer $0$ : Wolframalpha culculation I tried like this: Let $x=r\cos(\theta)$ and $y=r\sin(\theta)$ ,then: $\lim \limits_{(x,y)→(0,0)} \frac{(x^2+y^2)^2}{xy}$ = $\lim \limits_{r→0} \frac{r^4}{r^2 \sin(\theta)\cos(\theta)}$ = $\lim \limits_{r→0} \frac{2r^2}{\sin(2\theta)} =0$ but it seems wrong when $\theta =0$ and the limit $\lim \limits_{r→0} \frac{2r^2}{\sin(2\theta)}$ may not exist. So how to calculate the limit?","['limits', 'multivariable-calculus']"
3804937,Inscribed parallelogram in a quadrilateral,"It's well known that consecutively connecting midpoints of an arbitrary quadrilateral forms a parallelogram. Is it possible to inscribe other parallelograms inside a quadrilateral? I didn't find an example for that, so tried to come up with some argument which shows that parallelogram is unique but didn't get result. After that I used complex numbers to describe the problem. Let $ABCD$ be a quadrilateral with points $M$ , $P$ , $N$ and $Q$ on its sides. Also $MPNQ$ is a parallelogram. So we have $$\cases{\frac{A - M}{A^* - M^*}= \frac{M - B}{M^* - B^*} \\
   \frac{A - P}{A^* - P^*} = \frac{P - D}{P^* - D^*} \\
   \frac{D - N}{D^* - N^*} = \frac{N - C}{N^* - C^*} \\
   \frac{C - Q}{C^* - Q^*} = \frac{Q - B}{Q^* - B^*} \\
   M - Q + N - P = 0 }$$ I don't know how to proceed further.","['analytic-geometry', 'geometry', 'complex-numbers']"
3804978,What geometrical construction can be done with help of conics which aren't possible with compasses and rulers?,"Assuming we can construct parabola, hyperbola geometrically just as circle with compass and ellipses with string. What new things can be constructed can constructed. For, example we can double the cube by using parabola and circle i.e. we can construct cube-root of 2. For, example Can we do things like trisecting an angle or dividing them into n parts, or construction of heptagon? EDIT: Thanks for reply. I want to give context for my questions.
As a personal project, I want to make a puzzle game or app. The UI allows user to create that any conics that are circle, parabola, ellipse, hyperbola, ruled line. There are various parameters that can used to create these conics. In each puzzle some constructions are needed to solve the problems. I need various geometric results and such. That is the gist of it. Therefore, while gather information around this topics, I also need do devise some of my own results and objectives, to make various problems. I can use some guidance.","['conic-sections', 'geometry', 'geometric-construction']"
3805002,Spinors and Klein-Gordon Equation,"I'm currently working through Chapter 13 of Wald's General Relativity and spinors are being a little illusive to me. The question is pretty much: Using the Klein-Gordon equation in the form: $$\partial_{A'_{1}A}\phi^{A_{1}...A_{n}} = \frac{m}{\sqrt{2}}\alpha_{A'_{1}}^{A_{2}...A_{n}}$$ and $$\partial^{A'_{1}A_{1}}\alpha_{A'_{1}}^{A_{2}...A_{n}} = -\frac{m}{\sqrt{2}}\phi^{A_{1}...A_{n}}$$ for n = 2s, $\alpha_{A'_{1}}^{A_{2}...A_{n}}$ represents an auxiliary variable. Take s = $\frac{1}{2}$ and the pair of spinors $(\phi^{A},\alpha_{A'}) $ taking the components to be $\psi^{0},\psi^{1},\psi^{2},\psi^{3}$ respectively, show this boils down to the Dirac Equation shown in most textbooks. It gives in the question that you should choose the spinor basis $(o^{A},i^{A})$ so that $o_{A}i^{A}=1$ , which to me says that use the basis elements associated with $(o^{A},i^{A})$ are $(\frac{1}{\sqrt{2}}I,\frac{1}{\sqrt{2}}\sigma_{x},\frac{-1}{\sqrt{2}}\sigma_{y},\frac{1}{\sqrt{2}}\sigma_{z})$ . ( $I$ is the 2x2 identity, and $\sigma^{i}$ are Pauli matrices.) My approach is to set up the spacetime so that the basis vectors have the y component reversed, hence we have all the Pauli matrices of the same sign when we use that 'conversion' tensor $\sigma^{a}_{AA'}$ and then simply substituting into the equations (and putting into bases), I have something along the lines of $$I_{\Lambda\Lambda'}\partial_{t}\phi^{\Lambda} - \sigma^{a}_{\Lambda\Lambda'}\partial_{a}\phi^{\Lambda} - m\alpha_{\Lambda'} = 0$$ and $$I_{\Lambda}^{\Lambda'}\partial_{t}\alpha_{\Lambda'} - \sigma^{a\Lambda'}_{\Lambda}\partial_{a}\alpha_{\Lambda'} + m\phi_{\Lambda} = 0$$ You can use $\epsilon_{\Sigma\Omega}$ to raise and lower the indices as you want to get the component form and a = {x,y,z}. My logic was to effectively 'squish' the two equations together so that we can have a complex vector for $\psi^{\mu}$ as the components of $(\phi^{A},\alpha_{A'})$ and also some 4x4 matrix that represents the operators acting on components of $\psi$ . (In this, I've taken $\phi^{\Lambda} = (\psi^{0},\psi^{1})$ and $\sigma_{\Lambda'} = (\array{\psi^{2}\\\psi^{3}})$ ) But when I write out in components, it doesn't seem to match up to any form of the Dirac equation. This problem would be rectified if the t derivatives were acting on $\alpha$ instead of $\phi$ . I'm yet to study relativistic quantum mechanics so I don't actually know the Dirac equation at all. Clearly these equations are reminiscent of the Dirac equation but they also not exactly the same. I feel that my conversion to a basis is wrong and hence why the time derivatives are acting on the wrong elements of $\psi^{\mu}$ . I do apologise for my tensor notation though, I'm very inexperienced with LaTex and using Stack Exchange.","['quantum-mechanics', 'special-relativity', 'spin-geometry', 'differential-geometry']"
3805033,Is there a better/easier way to solve this matrix?,"\begin{equation*}
\begin{bmatrix}
4 & -1 & -1 & 0 &|&30 \\
-1 & 4 & 0 & -1&|&60 \\
-1 & 0 & 4 & -1&|&40 \\
0 & -1 & -1 & 4&|&70
\end{bmatrix}
\end{equation*} What's the best way to solve the matrix above? There's a clear pattern of the diagonal 4's and 0's and the -1's so I feel like there has to be a better way of doing things rather than using scaling and row reduction. If I do those methods I end up with messy fractions. My Step 1: New Row 2 = (1/4)Row 1 + Row 2 Even at step 1 I can tell the whole thing will be messy with fractions. Is there a better way to solve this matrix? Or am I doing it wrong? Thanks.","['matrices', 'calculus', 'matrix-equations', 'matrix-calculus']"
3805038,Confidence Intervals over time on expectation of having covid$-19$?,"I have the random indicator variable $X$ , which takes $1$ if the person has covid and $0$ otherwise. I have calculated $Prob(X_i = 1)$ (so the ${E[X_i]}$ or expectation of $X_i$ ) over the course of $137$ days. The next question is to ""Derive confidence intervals for your estimates of $Prob(X_0 = 1)$ using the CLT and Chebyshev Inequality and plot these confidence intervals vs time, e.g. as error bars about the estimates of $Prob(X_0 = 1)$ ."" So I got my array of $E[X_i]$ values, and for each day I calculated the mean and standard deviation based on that day and all previous days and then did my calculations. This leads to $95\%$ confidence intervals that seem to grow and grow ( graph ) Is this correct? what does this imply? Is it not incorrect to create confidence intervals on this time based sampling anyways???","['statistics', 'confidence-interval']"
3805128,"Prove that $\sum _{x=0}^{p-1}e^{\frac {2\pi ix^{2}}{p}}={\sqrt {p}} $ , $ p \equiv 1{\pmod {4}}$","Prove that : $$\sum _{x=0}^{p-1}e^{\frac {2\pi ix^{2}}{p}}={\begin{cases}{\sqrt {p}}&{\text{if}}\ p\equiv 1{\pmod {4}}\\i{\sqrt {p}}&{\text{if}}\ p\equiv 3{\pmod {4}}\end{cases}}$$ where $p$ is a prime number Can someone give me hints for this, I am completely stuck in this problem.
This was given in my roots of unity handout. I first, took $S=\sum _{x=0}^{p-1}e^{\frac {2\pi ix^{2}}{p}}$ . The handout asked me to prove $|S|=\sqrt p$ , but I am not able to proceed. Thanks in advance!","['contest-math', 'number-theory', 'gauss-sums', 'roots-of-unity']"
3805174,Prove that the product of compact sets in a product space is contained in a basic open set.,"The full question: Given topological spaces $X$ and $Y$ , and compact sets $A \subseteq X$ and $B \subseteq Y$ , and open set $W \subset X \times Y$ such that $A \times B \subseteq W$ , then there exists $U \subseteq X$ and $V \subseteq Y$ open such that $A \times B \subseteq U \times V \subseteq W$ This was my attempt at a proof, but it seems way too complicated and am unable to do the final step: Let $\{U_i \times V_i : i \leq n\}$ be a finite cover of basic open sets of $A \times B$ such that each $U_i \times V_i \subseteq W$ . Then define $U := \{x \in \cup U_i : \forall y \in B, (x,y) \in W\}$ and $V:=\{y \in \cup V_i : \forall x \in A, (x, y) \in W \}$ .
I can show that both of these sets are open: Given any $x \in U$ , for every $y \in B$ , $(x,y) \in W$ and so $\exists C_y \times D_y$ open and containing $(x,y)$ and contained in $W$ . Since $\{D_y: y \in B\}$ is an open cover of $B$ pass to a finite subcover $\{D_1, .... D_m \}$ . Then $x \in \cap_{1 \leq j \leq n} C_j$ and given any $x' \in \cap C_j$ , then for every $y \in B$ , $y \in D_k$ for some $k$ and thus $(x',y) \in C_k \times D_k$ and since $C_k \times D_k \subseteq W$ , $x' \in U$ . This proves $U$ is open and a symmetric argument can be used to show that $V$ is open. Am I on the right track here? Or am I making things needlessly complicated? I am also confused about how to show $U \times V \subseteq W$ . Thank you for the help.","['general-topology', 'product-space', 'compactness']"
3805182,Multiplying linear differential operators,"This seems like such a simple thing but I can't get it to work. I have two linear differential operators $$ L_1 = x\text D - 2 \text{ and } L_2 = (x+2)\text D + (x+1) $$ and a function $$ f(x) = a x^2 + b\exp(-x) $$ Now, I have that $$ L_1 f(x) = g(x) = -b(x+2)\exp(-x) $$ and that $$ L_2 g(x) = 0 $$ so reasonably, I should have $$ L_2 L_1 f(x) = Lf(x) = 0 $$ but when I multiply them, I get $$ L = (x^2+2x)\text D^2 + (x^2 - x - 4)\text D - (2x+2) $$ and I find that $$ Lf(x) = -(x+2)(2ax - b\exp(-x)) = -(x+2)f'(x) \neq 0 $$ I was under the impression that linear differential operators could be multiplied as polynomials to perform composition, but it doesn't seem to work here. However, for a different set of operators $$ M_1 = \text D+1 \text{ and } M_2 = (x^2+2x)\text D - (2x+2) $$ I get the product as $$ M = (x^2 + 2x)\text D^2 + (x^2 - 2)\text D - (2x+2) $$ and $$ Mf(x) = 0 $$ so it seems to work fine for this set. What's going on here? Why does it work for one but not the other?",['ordinary-differential-equations']
3805217,Weil Restriction and Distinguished Opens,"I have a pair of related questions about Weil Restriction. Let $E/F$ be a field extension, and let $A$ be an $E-$ algebra. Assume that all relevant restrictions of scalars exist. We have a norm map $n: A \rightarrow RA$ . I wish to show that for $f \in A$ we have $R(A_f) \simeq (RA)_{n(f)}$ (this is problem 11.4.7(6) in Springer's Linear Algebraic Groups). Using functoriality, given the map $A \rightarrow A_f$ , we have an induced map $RA \rightarrow R(A_f)$ and since $n(f) \in RA$ is mapped to something invertible in $R(A_f)$ , by universal properties, the morphism $RA \rightarrow R(A_f)$ factors through $(RA)_{n(f)} \rightarrow R(A_f)$ . However, I'm not sure how to proceed here. A more unified perspective here is probably given by the functors of points and thinking relatively. So here, working over a base scheme $S' \rightarrow S$ , for an $S$ -scheme $Y$ and an $S'$ -scheme $X$ we have $Hom_{S'}(Y \times_S S', X)=Hom_S(Y, \Pi_{S'/S}X).$ Moreover in this setting, we know that taking restrictions of scalars commutes with base change. So my question is; what is the schematic formulation of the above question? Let $X=Spec(A)$ be an $S'$ scheme. Then my impression is that the base change diagram for localization would be $Spec(A_f) \times_{S'} Spec(A)$ . Then by an easy argument, we have $\Pi_{S'/S}(Spec(A_f) \times_{S'} Spec(A)) \simeq \Pi(Spec(A_f)) \times_{\Pi(S')} \Pi(Spec(A))$ . Though it seems close, I cannot quite figure out if this is the correct analogue of the statement above. Moreover here it seems I don't even introduce the norm map at all, or use any universal properties about localization, so I am unsure of myself. Please let me know if you have any comments or if I should clarify anything.","['affine-schemes', 'ring-theory', 'algebraic-geometry', 'schemes']"
3805246,"Find all pairs of integers $(x, y)$ such that $x^3+y^3=(x+y)^2.$","Find all pairs of integers $(x, y)$ such that $$x^3+y^3=(x+y)^2.$$ Since $x^3+y^3 = (x+y)(x^2-xy+y^2)$ we get that $$x^2-xy+y^2=x+y$$ this can be expressed as $$x^2-(y-1)x+y^2-y=0.$$ Since we want integers we should probably look at when the discriminant is positive? $$\Delta = (y-1)^2-4(y^2-y)=-3y^2+6y+1$$ so for $\Delta \geqslant 0$ $$-\frac{2\sqrt3}{3}+1 \leqslant y \leqslant \frac{2\sqrt3}{3}+1$$ only possible solutions are $y=0,1,2.$ However I don't see how this is helpful at all here. What should I do?","['contest-math', 'algebra-precalculus']"
3805275,Is it true that $ \sum_{n=1}^{\infty}\sqrt{a_{n}} $ converge $ \Rightarrow\sum_{n=1}^{\infty}a_{n} $ converge? [duplicate],"This question already has answers here : $\sum_\limits{n=0}^{\infty} a_n$ converges $\implies \sum_\limits{n=0}^{\infty} a_n^2$ converges [duplicate] (7 answers) Closed 3 years ago . Let $ a_n $ be a non-negative sequence, such that $ \sum_{n=1}^{\infty}\sqrt{a_{n}} $ converge. Is it true that $ \sum_{n=1}^{\infty}a_{n} $ converge? I think that it is. But I want to make sure because it appeared in my final exam. Here is my reasoning: Since $ \sum_{n=1}^{\infty}\sqrt{a_{n}} $ converges, $ \sqrt{a_{n}}\underset{n\to\infty}{\to}0 $ and thus $ a_{n}\underset{n\to\infty}{\to}0 $ . So there exists some $ n_0 $ such that, for all $ n>n_0$ , it follows that $ 0\leq a_{n}<\frac{1}{2} $ . Thus, for each $ n>n_0 $ we have $$ a_{n}<\sqrt{a_{n}} .$$ So, from the comparison test, we get the convergence of $$ \sum_{n=1}^{\infty}a_{n}. $$ Do you agree?","['calculus', 'sequences-and-series']"
3805283,What is the sum of all the roots of the equation $f^{[5]}(x)=0$?,"Suppose $f(x)=x^2-3x+2$ , what is the sum of all the roots of the equation $f^{[5]}(x)=0$ ? I think one of the common ways of solving questions about $f^{[n]}(x)$ is fixed-point iteration. I used this method, solving $x^2-3x+2=x$ and got $x=2 \pm \sqrt{2}$ . Therefore $$f(x)-(2+\sqrt{2})=[x-(2+\sqrt{2})][x+(\sqrt{2}-1)]......(1)$$ And $$f(x)-(2-\sqrt{2})=[x-(2-\sqrt{2})][x-(\sqrt{2}+1)]......(2)$$ I tried $\frac{(1)}{(2)}$ and immediately got trapped since I can’t see the next step. The reason why I wanted to know what $f^{[5]}(x)$ is is that if I can know the coefficient of $x^{31}$ , I can easily obtain the answer by Vieta’s theorem. Maybe I’m missing something obvious here. Any suggestions or hints would be appreciated. Edit: I just realized different regions may have different expressions on this one.
Basically, $f^{[2]}(x)=f(f(x))$ , $f^{[3]}(x)=f(f(f(x)))$ , and so on. I am really sorry for not considering this trouble in the first place.","['functions', 'roots', 'fixed-point-theorems']"
3805300,The roots of the general quartic in $a+b+c$ form.,"The roots of $x^3+qx+r$ are $$1 := \alpha+\beta$$ $$2 := \,\,\omega\alpha+\omega^2\beta$$ $$3 := \;\;\omega^2\alpha+\omega\beta$$ where $1$ , $2$ and $3$ are defined to be labels for the picture below and $$\omega=-\frac{1}{2} + \frac{\sqrt{3}}{2}i$$ $$\alpha^3=\frac{1}{2}(-r+\sqrt{D})$$ $$D=r^2+\frac{4}{27}q^3$$ $$\beta = -\frac{1}{3} \frac{q}{\alpha}. $$ The proof for the solutions of the cubic in these terms can be found in ""A First Course in Abstract Algebra with Applications (Third Edition)"" by Joseph J. Rotman, Theorem 5.4  on page 438. We can draw a triangle with labels $1$ , $2$ , and $3$ as vertices and proceed to map all the symmetries of the triangle. Now we can ask, since each vertex is labeled with the roots of the cubic: what are the maps in terms of $\omega,\alpha,\beta$ that would allows us to transform a root to another thereby interchanging the labels to get another symmetry of the triangle? Say for example, from 123 to 312? $$
e: \begin{cases} 
          \alpha \mapsto \alpha \\
          \beta \mapsto \beta
  \end{cases}
$$ $$
r: \begin{cases} 
          \alpha \mapsto \omega^2\alpha \\
          \beta \mapsto \omega\beta
  \end{cases}
$$ $$
r^2: \begin{cases} 
          \alpha \mapsto \omega\alpha \\
          \beta \mapsto \omega^2\beta
  \end{cases}
$$ $$
s_0: \begin{cases} 
          \alpha \mapsto \omega\beta \\
          \beta \mapsto \omega^2\alpha
  \end{cases}
$$ $$
s_1: \begin{cases} 
          \alpha \mapsto \omega^2\beta \\
          \beta \mapsto \omega\alpha
  \end{cases}
$$ $$
s_2: \begin{cases} 
          \alpha\omega \mapsto \omega\beta \\
          \alpha\beta \mapsto \omega\beta
  \end{cases}
$$ We can check that we can go from 123 to 312 by applying the $r$ mapping to the labels, which corresponds to rotation by -120 degrees: $$
\begin{cases} 
  r(\;Label\;1\;) &= r(\alpha+\beta)               &= r(\alpha)+r(\beta)                   &= \omega^2\alpha+\omega\beta &=\;Label\;3\\
  r(\;Label\;2\;) &= r(\omega\alpha+\omega^2\beta) &= \omega r(\alpha)+\omega^2 r(\beta)   &=    \omega (\omega^2\alpha)+\omega^2 (\omega\beta)     &= \alpha+\beta = \;Label\;1\\ 
  r(\;Label\;3\;) &= r(\omega^2\alpha+\omega\beta) &= \omega^2 r(\alpha) + \omega r(\beta) &=
  \omega^2 (\omega^2\alpha) + \omega (\omega\beta) &= \omega\alpha + \omega^2\beta = \; Label\; 2
\end{cases}
$$ So, $$r(123)\mapsto 312$$ Now my question is, where may I find the solution of the general quartic $$x^4+px^2+qx+r$$ in terms of one of the roots being in the form $\alpha+\beta+\gamma$ analogous to the way the solution to the cubic above has one of the roots in the form $\alpha+\beta$ ? Thank you.","['permutations', 'number-theory', 'galois-theory', 'abstract-algebra', 'group-theory']"
3805301,Show that $\pi(n) \geq \log_2\log_2 2n$,"Doing some excercise on elementary number theory I have proved that for every $n \in \Bbb{N}, p_{n+1} \leq p_1p_2...p_n + 1$ , based on this result I'm also was able to prove that for every $n \in \Bbb{N}, p_{n} < 2^{2^n}$ . Based on this I have now to show that for every $n \in \Bbb{N}, \pi(n) \geq \log_2\log_2 2n$ , where $\pi$ is prime counting function. Here is what I tried. In order to $r=\log_2\log_2 2n$ be in $\Bbb{N}$ , let $n=2^m, m \in N$ . Then I got $r=log_2\log_2 2(2^m)=\log_2\log_2 2^{m+1}=\log_2 m+1$ . Once again, let $m=2^r - 1$ , then $r=\log_2 {(2^r-1) + 1}=\log_2 2^r=r$ . Substituting $m$ to $n$ I have $n=2^{2^r-1}$ , and initial inequality becomes $\pi(2^{2^r-1}) \leq r=\log_2\log_2{2^{2^r}}$ . From this point, If I consider prime $p_r$ , it's true that $\pi(2^{2^r}) \geq r$ , because it's already proven that $p_{r} < 2^{2^r}$ , so there is an $r_{th}$ prime between $1$ and $2^{2^r}$ . But I can't figure out why it's also true for $\pi(2^{2^r-1})$ . And that's where I stuck and need some help. Thank you in advance!","['number-theory', 'elementary-number-theory', 'prime-numbers']"
3805305,Issues with von Mises axioms of probability,"Is it possible to come up with a system of axioms that defines probabilities
as limits, instead of the traditional Kolmogorov axioms? I know historically
there was an attempt at this, mainly brought forward by von Mises,
but it somehow didn't reach widespread acceptance (there seem to be
some subtle issues with the concept of martingales that are formalizable
in his system of axioms). Has perhaps an improved variant of his axioms been published somewhere that is really equivalent to Kolmogorov axioms? What is the state of the art for this regarding this approach to probabilities? It feels as though this simulation approach to probabilities, that is encountered everywhere in computer science is much closer in spirit to von Mises approach to probabilities.","['probability-theory', 'axioms']"
3805307,"Problems with any non-Kolmogorovian (frequentistic, subjective etc.) approaches to probability","When introducing Kolmogorov's axiomatic approach probability, it is often claimed that this is are a way out of the problems associated with the following two interpretations of probability: • frequentistic approach: probability is the limit of relative frequence of an event occuring in long strings of repetitions of an experiment • subjective approach: the (hypothetical) monetary value that I would put on a bet that an event occurs My question is: What exactly are the problems, associated to these
interpretations? [Careful! Long sentence ahead!] Are they merely philosophical (e.g. in case of the
frequentistic approach: we can't associated a probability to all events,
that we would like to associated probabilities to, since not all events
can formulated within repeatable experiments; e.g. to estimate probability
of a politician being elected, we can hold 1000 times an election,
to approximate, as the fraction of those 1000 times in which he was
elected, the probability of election) or are there ``hard'' mathematical
obstacles that arise when one formalizes these other approaches and
states them axiomatically (e.g. in case of the frequentistic approach:
we assume that $(\Omega,\mathcal{F})$ is a measurable space, let $A\in\mathcal{F}$ be arbitrary and consider for each $k\in\mathbb{N}$ a finite sequence $(B_{i}^{k})_{i\leq k}\in\mathcal{F}^{k}$ such
that $\lim_{k\rightarrow\infty}\frac{N_{k}((B_{i}^{k})_{i\leq k})}{k}$ exist (which then necessarily lies within $[0,1]$ ), where $N_{k}$$((B_{i}^{k})_{i\leq k}):=|\{i:B_{i}^{k}=A\}|$ ;
the value of this limit is then called probability of $A$ . For the
subjective approach I don't know how to formalize this)?","['foundations', 'probability-theory', 'axioms']"
3805320,"If $f(x)=\cosh(x)+\sinh(x^2)$, what is$f^{(34)}(0)$?","If $f(x)=\cosh(x)+\sinh(x^2)$ , what is $f^{(34)}(0)$ ? I know that: $h(t)=\sinh(t)=\left(\frac{e^t-e^{-t}}{2}\right)$ and $g(t)=\cosh(t)=\left(\frac{e^t+e^{-t}}{2}\right)$ So, I noticed this: $g'(t)=\sinh(t)$ $g''(t)=\cosh(t)$ Then: $g^{(34)}(t)=\cosh(t) \rightarrow g^{(34)} (0)= \cosh(0)=1 $ With the same idea, I know that $h^{(34)}(0)=\sinh(0)=0$ The problem that I am having is that I don't know what is the $34^{th}$ derivative of $\sinh(x^2)$ , can I   use $\sinh(x)$ somehow? Also, I know that: $\sinh(x)= \sum_{k=0}^{n} \frac{(x^2)^{1+2k}}{(1+2k)!}$","['calculus', 'derivatives', 'taylor-expansion', 'hyperbolic-functions']"
3805357,Glivenko-Cantelli theorem proof,"Glivenko-Cantelli theorem states that: $$\sup_{x\in \Bbb R}|F_n(x)-F(x)|\to 0 \quad\text{almost surely}\,,$$ where $F_n(x)$ is an empirical CDF. There is a LINK with a proof for discrete random variable, but I don't really get this line: $$\sup_{x\in \Bbb R}|F_n(x)-F(x)|\leq \max_{j \in \{1,\ldots,m\}}|F_n (x_j)-F(x_j)| + \frac{1}{m}$$ Could anyone please explain me why that is true?","['proof-explanation', 'statistics', 'probability-theory']"
3805452,Probability of forming triangle from breaking a stick,"Given a stick and break it randomly at two places, what is the probability that you can form a triangle from the pieces? Here is my attempt and the answer does not match, so I am confused what went wrong with this argument. I first denote the two randomly chosen positions by $X$ and $Y$ , and let $A=\max(X,Y)$ , $B=\min(X,Y)$ . We are interested in the probability of the event $\{A>\frac{1}{2}, B>A-\frac{1}{2}\}$ . Thus, we want the joint distribution of $A$ and $B$ . To compute that, I computed $$F_{A,B}(w,z)=\mathbb{P}(A\leq w, B\leq z)=\mathbb{P}(A\leq w)-\mathbb{P}(A\leq w, B>z)=\mathbb{P}(X\leq w,Y\leq w)-\mathbb{P}(X\leq w, Y\leq w, X>z, Y>z)$$ Therefore, we have if $z\leq w$ $$F_{A,B}(w,z)=w^2-(w-z)^2$$ otherwise $$F_{A,B}(w,z)=w^2$$ Then the joint density of $A$ and $B$ is $$f_{A,B}(w,z)=\frac{\partial^2 F}{\partial w\partial z}(w,z)=2$$ if $z\leq w$ and $0$ otherwise. Finally $$\mathbb{P}(A>\frac{1}{2},B>A-\frac{1}{2})=\int_{\frac{1}{2}}^1\int_{w-\frac{1}{2}}^w2dzdw=\frac{1}{2}$$ The answer is $\frac{1}{4}$ instead, but I can't figure out what went wrong with this argument.",['probability']
3805490,Finding the smallest power of $A$ such that $A^n = I$,"Let $A=\begin{bmatrix}0 & 1\\-1 & 1\end{bmatrix}$ then the smallest positive positive integer $n$ such that $A^n = I$ is : (a) $1$ (b) $2$ (c) $4$ (d) $6$ proof: option (d) 6. The characteristic polynomial of $A$ is $\lambda^2 - \lambda + 1$ . So the eigenvalues of $A$ are $\omega, \omega^2$ where $\omega$ is a cube root of $-1$ and not equal to $-1$ . So the eigenvalues of $A^n$ are $\omega^n$ and $\omega^{2n}$ and the eigenvalue of $I$ is $1$ , so $\omega^{2n} = 1$ and $\omega^n = 1$ which implies $n$ as a multiple of $3$ and $2$ which means $6$ is the only option we have. Is my reasoning correct?? And can we solve this without using eigenvalues?","['matrices', 'solution-verification', 'linear-algebra']"
3805511,"Dummit and Foote: ""extensions"" of a function","Dummit and Foote define an extension of a function as follows. If $A \subseteq B$ and $g: A \to C$ and there is a function $f: B \to C$ such that $f \mid _A =  g$ , we shall say that $f$ is an extension of $g$ to $B$ (such a map $f$ need not exist nor be unique.) I do not understand, in particular, the notion that $f$ may not exist. I tried to consider an edge case where $ g(a) = \frac{1}{a}$ , which is clearly $g$ is undefined at $0$ , so we may have $0 \in B \setminus A$ . However, I can define $f$ in a piecewise manner, say, $f(b) = \frac{1}{b}$ if $b \in A$ and $f(b) = 5$ if $b \in B \setminus A$ . This function is well-defined and, when restricted to $A$ , is the same function as $g$ . Is there a way where this $f$ cannot exist?",['functions']
3805575,Roll a die until sum is over 63. Calculate the probability of second last roll,"The question is related to this question. I'll repeat the question for the sake of completeness: Roll a die repeatedly. Say that you stop when the sum goes above 63. What is the probability that the second to last value was X. Make a market on this probability. Ie what is your 90 percent confidence interval. I was trying to solve it with another approach. Let $T$ denotes the event of termination of the game. Then $$
1=P(T) = P(T|62)P(62) + ....+P(T|57)P(57)
$$ by the law of total probability.
So the question is to find out what is $P(T|X)$ . To obtain $P(T|X)$ , we need first to know what is $P(X)$ But I can't find a way to calculate the P(X). Is there any smart way to do it? Alternatively, is there any rigorous mathematical proof to solve this problem?","['conditional-probability', 'game-theory', 'dice', 'probability-theory']"
3805595,Are there any geometrically meaningful/useful mixed-grade objects in geometric algebra other than rotors?,"While reading about geometric algebra, I have seen variables that are meant to represent blades, and variables that are meant to represent rotors, i.e. multivectors with a scalar and bivector component. But I have not seen any applications where a variable represents a mixed-grade object that is not the sum of a scalar and bivector. Are there examples of such objects being geometrically meaningful or useful?","['geometric-algebras', 'abstract-algebra', 'clifford-algebras']"
3805696,"If $f(x)$ is differentiable over $(a,b)$, does that mean $f'(x)$ is continuous over $(a,b)$? [duplicate]","This question already has answers here : Discontinuous derivative. [duplicate] (2 answers) Closed 3 years ago . I was drawing smooth curves through points $(a,f(a))$ and $(b,f(b))$ in an attempt to come up with a proof of the Mean Value Theorem. As I was doing that, I noticed that I couldn't draw the curve in such a way that its derivative would be discontinous. I tried to think up a function that is differentiable but whose derivative is discontinous but I got nothing. I did a quick Google search for the same question that I've asked here but all I could find was that differentiability of a function implies continuity of the same which I already know. If $f(x)$ is differentiable over $(a,b)$ , does that mean $f'(x)$ is continuous over $(a,b)$ ? If not, what are some examples of functions where this is not the case?","['continuity', 'calculus', 'derivatives']"
3805716,Are probability distributions with similar characteristic functions close to each other?,"Suppose that $\mu$ and $\nu$ are real probability distributions with characteristic functions $\varphi_\mu$ and $\varphi_\nu$ that are close to each other in some metric (maybe $L^\infty$ ?). Can we say that $\mu$ and $\nu$ are close to each other in terms of some statistical distance? This is trivial when close means equal, but I'm unsure what can be said when we only have approximate equality. If it helps, we can restrict $\mu$ and $\nu$ to have compact support.","['measure-theory', 'probability-distributions', 'probability-theory']"
3805720,Elliptic integrals and $\zeta(5)$.,"Who can evaluate this one? $$
\int_0^1 \frac{K'(k)^4}{K(k)^2} \;k\;dk = \frac{31}{8} \zeta(5) .
$$ Note: I used the elliptic modulus $k$ (and not the parameter $m = k^2$ commonly
seen in Mathematica).  That is: \begin{align}
K(k) &:= \int_0^1\frac{dt}{\sqrt{(1-t^2)(1-k^2 t^2)}}
\\
K'(k) &:= K(\sqrt{1-k^2}\;)
\end{align}","['modular-forms', 'elliptic-integrals', 'definite-integrals', 'real-analysis']"
3805736,"Let $f$ be a real function and $a<b<c<d$. If $f$ is convex on $[a,c]$ and $[b,d]$, then can we say $f$ is convex on $[a,d]$?","Let $f$ be a real function and $a<b<c<d$ . If $f$ is convex on $[a,c]$ and $[b,d]$ , then can we say $f$ is convex on $[a,d]$ ? Below is my attempt. If we can show that when $x_1\in[a,b]$ , $x_2\in [c,d]$ , then $$f(\theta x_1+(1-\theta)x_2)\le \theta f(x_1)+(1-\theta)f(x_2),\forall \theta\in(0,1)$$ the proof is done. Let $x_3:=\theta x_1+(1-\theta)x_2$ . It is equivalent to prove $$\frac{f(x_3)-f(x_1)}{x_3-x_1}\le \frac{f(x_2)-f(x_3)}{x_2-x_3}.$$ When $x_3\in (b,c)$ , we have $$\frac{f(x_3)-f(x_1)}{x_3-x_1}\le \frac{f(c)-f(x_3)}{c-x_3}\le \frac{f(x_2)-f(x_3)}{x_2-x_3}.$$ However, when $x_3\in [a,b]\cup [c,d]$ , the approach fails.","['convex-analysis', 'real-analysis']"
3805780,How to approach $\sum_{n=0}^\infty(-1)^n\frac{H_{2n+1}}{(2n+1)^3}$ elegantly?,"How to show that $$\sum_{n=0}^\infty(-1)^n\frac{H_{2n+1}}{(2n+1)^3}=\frac{\psi^{(3)}\left(\frac14\right)}{384}-\frac{\pi^4}{48}-\frac{35\pi}{128}\zeta(3)$$ without using the generating function : \begin{align}
\sum^\infty_{n=1}\frac{H_n}{n^3}z^n
=&2{\rm Li}_4(z)+{\rm Li}_4\left(\tfrac{z}{z-1}\right)-{\rm Li}_4(1-z)-{\rm Li}_3(z)\ln(1-z)-\frac{1}{2}{\rm Li}_2^2\left(\tfrac{z}{z-1}\right)\\
&+\frac{1}{2}{\rm Li}_2(z)\ln^2(1-z)+\frac{1}{2}{\rm Li}_2^2(z)+\frac{1}{6}\ln^4(1-z)-\frac{1}{6}\ln{z}\ln^3(1-z)\\
&+\frac{\pi^2}{12}\ln^2(1-z)+\zeta(3)\ln(1-z)+\frac{\pi^4}{90}
\end{align} The common proof is to use the series property $$\sum_{n=0}^\infty (-1)^n f(2n+1)=\Im \left\{\sum_{n=1}^\infty i^n f(n)\right\}$$ then we apply the generating function above by setting $z=i$ but as you can see too much tedious calculations involved which is the reason I am asking for a different approach. By the way, you can find here a similar question that could be helpful. All methods are appreciated. Thank you. Edit: This question was solved here but I am looking for an elegant method as question title says.","['integration', 'alternative-proof', 'harmonic-numbers', 'polylogarithm', 'sequences-and-series']"
3805820,Prove that$(1-\omega+\omega^2)(1-\omega^2+\omega^4)(1-\omega^4+\omega^8)…$ to 2n factors$=2^{2n}$,Prove that $(1-\omega+\omega^2)(1-\omega^2+\omega^4)(1-\omega^4+\omega^8)…$ to 2n factors $=2^{2n}$ where $\omega$ is the cube root of unity My attempt: $(1+\omega^n+\omega^{2n})=0$ $\Rightarrow (1-\omega^n+\omega^{2n})=-2\omega^n$ $\Rightarrow \prod_{n=1 \to 2n}(-2\omega^n) =2^{2n}\omega^{1+2+3…2n}$ $\Rightarrow \prod_{n=1 \to 2n}(-2\omega^n) =2^{2n}\omega^{n(2n+1)}$ If $n$ is $3k$ type: $2^{2n}\omega^{n(2n+1)}=2^{2n}\omega^{3m}=2^{2n}$ If $n$ is $3k+1$ type: $2^{2n}\omega^{n(2n+1)}=2^{2n}\omega^{3m}=2^{2n}$ If $n$ is $3k+2$ type: $2^{2n}\omega^{n(2n+1)}=2^{2n}\omega^{(3k+2)(6k+5)} =2^{2n}\omega^{(3m+1)}=2^{2n}\omega^{3m}\omega=2^{2n}\omega$ What have I done wrong here?,"['roots', 'products', 'algebra-precalculus', 'sequences-and-series', 'complex-numbers']"
3805855,"$K/k$ Galois with group $\mathfrak{g}$, $V_{K} \cong V_{k} \otimes K$. Then, $V_{k}$ consists of elements of $V_{K}$ invariant under $\mathfrak{g}$.","I have been asked to read "" Andre A. Weil. Algebras with involutions and the classical groups. J. Indian Math. Soc.(N.S.) 24 (1960), 589–623 "" as part of a project, and I am encountering some difficulties with some notations and notions used by Weil, in particular, the notion of a universal domain . In the first page of the paper itself, Weil says In Part I, all spaces, varieties, groups are.......allowed to have points in the universal domain, and not only in their field of definition. If $V$ is a variety defined over a field $k$ , we shall denote by $V_{k}$ the set of points of $V$ with coordinates in $k$ . I think I understand what this means here, but I am confused when he carries the same thing for vector spaces for instance, later he says If $V$ is a vector space of dimension $n$ (over the universal domain) defined over $k$ , $V_{k}$ and $V_{K}$ are vector spaces of dimension $n$ over $k$ and over $K$ , respectively. We have $V_{k} \subset V_{K}$ , and we may identify $V_{K}$ with the tensor product $V_{k} \otimes K$ taken over $k$ ; $\mathfrak{g}$ operates in an obvious manner on $V_{K}$ , and $V_{k}$ consists of the elements of $V_{K}$ which are invariant under $\mathfrak{g}$ . I am attaching here snaps of the paper for reference : Pertaining to the first quotation , Pertaining to the second quotation Note: We are under the following setting: We have fixed a universal domain of characteristic $0$ , a groundfield $k$ inside it and a normal extension $K$ of $k$ of finite degree $d$ , with Galois group $\mathfrak{g}$ In the case of varieties, if we only think of affine varieties classically then we know they are embedded in some ambient space, so if $V$ is a variety over universal domain $U$ , say, then we can think  of $V$ sitting inside $U^{m}$ for some $m$ , and then it makes sense to say that $V_{k}$ is the set of points of $V$ with coordinates in $k$ . But in the case of vector spaces, I don't understand what connection do $V, V_{k}$ and $V_{K}$ share? They are not just same as sets I guess. One way I could think of this was that I thought of $V$ as $U^{n}$ (as a vector space of dimension $n$ over $U$ , the universal domain) and then $V_{k}, V_{K}$ are just the set of points in $U^{n}$ with coordinates in $k, K$ respectively. I am not able to see how exactly to identify $V_{K}$ with $V_{k} \otimes K$ if I go by my definition as in 1. My attempt was to construct the following linear function from $V_{k} \otimes K$ to $V_{K}$ sending $(x_{1}, x_{2}, ...., x_{n}) \otimes c \mapsto (cx_{1}, ..., cx_{n})$ and extending this linearly. Now both domain and codomain have the same dimension over $K$ , so proving that this is one-one suffices to show the isomorphism. I got stuck here. I think the action of $\mathfrak{g}$ on $V_{K}$ is given by $\sigma(v \otimes c) = v \otimes \sigma(c)$ . Using this definition of action, I couldn't prove that $V_{k}$ is the set of elements of $V_{K}$ invariant under $\mathfrak{g}$ . One direction is trivial, in the other direction I had problems because of the fact that $m \otimes n = 0$ doesn't imply that $m= 0$ or $n= 0$ . I seek serious help with the above mentioned doubts, if anyone has any idea about any of it, please help. I would be grateful for the help. I am quite unsure what tags to use, feel free to add the relevant ones. Update: (i) I got $3.$ figured out, since we are in a vector space, $m \otimes n = 0$ would indeed imply that $m = 0$ or $n = 0$ . (ii) In $2.$ instead of proving one-one, I proved it is onto like this : So, if we have a vector $(x_{1}, x_{2}, ..., x_{n})$ in the codomain, then consider the element $\sum_{i} e_{i} \otimes x_{i}$ in the domain of the function, its image is exactly $(x_{1}, x_{2}, ..., x_{n})$ .","['classical-groups', 'algebraic-geometry', 'algebraic-groups']"
3805872,"Wronskian, Linear Dependence and Construction of ODE","I already understand that for second-order, linear, homogeneous ODE, if the Wronskian of two solutions to the ODE ( or functions that satisfy the ODE) is zero at a certain point, the functions are linearly dependent. At another place (source at bottom), it has been written that if Wronskian is zero at a certain point, it is not necessary that they are linearly dependent. As an example, for f(x) = x, g(x) = sin(x), we find W(f, g) = x cos(x) − sin(x) which is
nonzero, for example, at x = π. Hence, x and sin(x) are Linearly
Independent. Note that W(f, g) may be zero at some point such as x =
0. The only way I can resolve these two statements is by saying that you can't construct a second-order, linear, homogeneous ODE which has these two functions, x and sin(x) as it's solutions (i.e. they satisfy the ODE). I don't know how to formally prove this, except that I observe that sin(x) already is the solution of second-order, linear, homogeneous ODE, of which x is not a solution, so adding it might make a third-order or say non-linear ODE. So, how do we actually resolve these statements and can we prove that we can't construct an ODE as required above? EDIT: This question was solved earlier, but I have a doubt in the solution by mathcounterexamples.net now: why can we not change the coefficient of y'' to the function in the denominator (xcos(x) - sin(x)) ? Source: https://home.iitk.ac.in/~sghorai/TEACHING/MTH203/ode7.pdf or https://drive.google.com/file/d/1OGRE00YNB0kjVHam9ZSpsDQfba0PxkYG/view?usp=sharing Relevant statement: Theorem 3: Two solutions $y_1,y_2$ of $$
y'' + p(x)y' + q(x)y = 0, \quad x \in \mathcal I
$$ are linearly dependent iff $W(y_1,y_2) = 0$ at a certain point $x_0 \in \mathcal I$ .","['linear-algebra', 'wronskian', 'ordinary-differential-equations']"
3805875,Performing antidifferentiation using $d/dr$,"So I'm confused about how to approach this question: $$\frac{d}{dr}\left(\frac12v^2\right)=-\frac{GM}{r^2}.$$ The solution seems fairly straightforward: $$\frac{1}{2} v^{2}=-\int \frac{G M}{r^{2}} d r=\frac{G M}{r}+C$$ but I don't really understand what specifically is happening. For context, I'm a university student studying maths, but I feel like I never really ""got"" the notation for derivatives. It seems weird that the $d/dr$ is being treated as a variable and being shifted to the RHS, and then where does the $d$ (from $dr/d$ ) go? And why is the LHS constant? We are integrating with respect to $r$ and thus we can consider the LHS as some constant. Say we were integrating the constant k with respect to x, then the result is $kx + c$ , so why isn't the result not $(1/2)\times (v^2)\times r$ on the LHS? Help is very much appreciated. For reference, here is the full question .","['integration', 'calculus', 'ordinary-differential-equations']"
3805882,When does every compact $A$ in a topology satisfy $A⊆B⊆C$ for some open $B$ and compact $C$?,"Let $X$ be a topological space such that for any compact subset $A$ of $X$ , there exists open set $B$ and compact set $C$ such that $A\subseteq B\subseteq C$ . Does this property have a name? If so, what is it? Does this property hold for all topologies? If so I would like a proof and if not a counter example. Thanks.","['general-topology', 'compactness']"
3805897,Shorter way to calculate number of ways to distribute varying number of balls into 3 distinct boxes such that sum of balls $\leq$ 99,"I need to find the number of ways to distribute a varying number of balls into 3 distinct boxes such that the sum of all balls is $\le 99$ . Since the balls are identical and the boxes are distinct, I have chosen to use $H^n_r$ i.e. ${r+n-1}\choose{r}$ to solve the problem, where $n$ = number of boxes and $r$ = number of balls. I have split the problem into different cases, such as when sum of balls is 99, sum of balls is 98.. all the way to if the sum of balls is 0. I have obtained the following sequence ${101}\choose{99}$ + ${100}\choose{98}$ + ${99}\choose{97}$ +...+ ${3}\choose{1}$ + ${2}\choose{0}$ Using the symmetry rule I have simplified this into ${101}\choose{2}$ + ${100}\choose{2}$ + ${99}\choose{2}$ +...+ ${3}\choose{2}$ + ${2}\choose{2}$ = $\sum_{r=2}^{101}{{r}\choose{2}}$ However I feel that my method is too long, is there some kind of way to simplify the answer even more so that I can get an integer solution?","['binomial-coefficients', 'combinatorics']"
3805915,"Compute the area of a surface, encountering a strange integral","compute the area of this yellow surface, which is actually a paraboloid: $x^2+y^2=2az$ (yellow one), cutted by $(x^2+y^2)^2=2a^2xy$ (blue one) To compute the part be surrounded in the blue surface, use polor coordinates: \begin{cases}
x=r\cos\theta\\
y=r\sin\theta\\
z=\frac{r^2}{2a}\\
\end{cases} use Gauss efficient to compute the area: \begin{cases}
E=(\frac{\partial x}{\partial r})^2+(\frac{\partial y}{\partial r})^2+(\frac{\partial z}{\partial r})^2&=1+\frac{r}{a}\\ 
F=\frac{\partial x}{\partial r}\frac{\partial x}{\partial\theta}+\frac{\partial y}{\partial r}\frac{\partial y}{\partial\theta}+\frac{\partial z}{\partial r}\frac{\partial z}{\partial\theta}&=0\\
G=(\frac{\partial x}{\partial\theta})^2+(\frac{\partial y}{\partial\theta})^2+(\frac{\partial z}{\partial\theta})^2&=r^2\\ 
\end{cases} $$S=\iint\limits_D\sqrt{EG-F^2}drd\theta$$ where D becomes $\left\{ (r,\theta )|\theta \in \left[ 0,\frac{\pi}{2} \right] \cup \left[ \pi ,\frac{3}{2}\pi \right] ,\mathrm{r}\in \left[ 0,\mathrm{a}\sqrt{\sin 2\theta} \right] \right\}$ since the blue surface can be written as $r^2=a^2\sin2\theta$ . I can't find anything wrong until now, but this intergal is extreme complex, and Mathematica give me a non elementary solution. However, the standard solution given by the text book to this question is $\frac{20-3 \pi}{9} a^{2}$ How can i get that answer? Or what's wrong with my method?","['multivariable-calculus', 'surface-integrals']"
3805923,"Solving $\cos(2x)\cos\left(x - \frac{\pi}{6}\right) = \sin(2x)\sin\left(\frac{\pi}{6} - x\right)$ for $x\in(0,\pi/2)$","Solve this equation for $x\in (0 , \frac{\pi}{2})$ $$\cos(2x)\cos\left(x - \frac{\pi}{6}\right) = \sin(2x)\sin\left(\frac{\pi}{6} - x\right)$$ I gave a try using the $\cos(a-b)$ and $\sin(a-b)$ formulas, but it seems the problem complicated a little bit more. Is any other elegant solution for this?","['calculus', 'trigonometry']"
3805926,Show that $f$ is infinitely differentiable,"Let $f:(0,\infty)\to\Bbb{R}$ satisfy $f(xy)=f(x)+f(y)$ for all $x,y\in(0,\infty)$ . If $f$ is differentiable at $x=1$ , show that $f$ is differentiable on $(0,\infty)$ and $f'(x)=\frac{f'(1)}{x}$ . Show that $f$ is in fact infinitely differentiable. What I did: Let $y=1,$ then $f(x)=f(x)+f(1)$ $\therefore f(1)=0$ is differentiable. Let $\mid h\mid<x$ and $y=1+\frac{h}{x}$ $f(xy)=f(x(1+\frac{h}{x}))=f(x+h)=f(x)+f(1+\frac{h}{x})=f(x)+f(1)+f'(1)\frac{}{}\frac{h}{x}+\text{o}(\frac{h}{x})$ How do I show that $\frac{\frac{h}{x}}{x-h}\to0$ as $x\to h$ ? And how do I show that $f$ is infinitely differentibale? I have to prove another function is infinitely differentiable as well so I hope I can apply the same technique there. Thank you guys.","['solution-verification', 'derivatives']"
3805943,A problem on Rolle's theorem,"So there's this question that asks us to prove that, between any two roots of $\tan x=1$ there exists at least one root of $\tan x =-1$ . Suppose we assume that $a,b$ are two roots of $\tan x-1=0$ , then $f(a)=f(b)=0$ , where $f(x)= \tan x-1$ . According to the theorem, $f'(c)=0$ where $c \in (a,b)$ ,i.e., $\sec^2 c =0$ ....and this is not defined. Either there's something wrong with my understanding or with the problem. Please help.","['rolles-theorem', 'derivatives']"
3805956,Books and references for Conformal Differential Geometry,"In this semester I want to study Conformal Differential Geometry, but I don't know which book is suitable as a textbook or reference. Can people recommend textbooks and/or other references to me? If there are some online videos for the lecture, it would be more friendly to me.","['conformal-geometry', 'book-recommendation', 'differential-geometry']"
3805964,"Gödel: If a statement of existence is neither provable nor refutable, doesn‘t that imply that the statement is false? [duplicate]","This question already has answers here : True vs. Provable (8 answers) Closed 3 years ago . I‘m not very versed in Gödel‘s incompleteness theorem but in a naive way: If a statement of existence is not provable, there you cannot find an example which fulfills the statement (otherwise the statement would be provable with this example). But when there is no element which fulfills the statement, doesn’t that imply that the statement ist false? I thought about that one in the context of the measure problem - because the statement $$\exists \text{ measure function } \mu: 2^{\mathbb R} \to [0,\infty] \, \forall I = [a,b] \subseteq \bar{\mathbb R}: \mu(I) = b - a$$ is neither provable nor refutable. But if I cannot prove there is a measure function, I cannot find a $\mu$ for which the statement is true. Because finding such a $\mu$ would prove the statement. But when there is no such $\mu$ , the statement of existence is false, isn‘t it? Where is my mistake in thinking?","['incompleteness', 'logic', 'real-analysis']"
3805997,Why is the Volume integration & Surface Area integration of a sphere different?,"For both volume & surface area, the sphere is split into many discs and the area or circumference of the discs are summed up in an integral. But the summation process uses $dy$ for volume & $r\,d\theta$ (arc-length) for surface area. Why this discrepancy? Supposing we have a sphere in the $x$ - $y$ - $z$ plane where you split the sphere into discs along the $y$ axis.. If you visualise the problem from $z$ axis looking down over the $x$ - $y$ plane.. The sphere will look like a circle and the disc will be a line segment inside the circle (chord). The length of the line segment will be the diameter of the disc. And the point where the line segment and circle meet - (x,y) can be solved by plugging in the value of y and the x we solve for will then be the radius of the disc. Now to calculate surface area, we need to sum up the circumference of each disc $ s(x) = 2\pi x$ & and for volume, we need to sum up the area of each disc $ v(x) = \pi x^2 $ Say, the point $(x,y)$ makes an angle $\theta$ with the origin. Then for surface area, we assume for length $r\,d\theta$ , the disc radius is not changing (across arc length) & we integrate it as: $$\int s(x)\, rd\theta $$ But for volume, instead of using the arc length, we use the diameter $dy$ to integrate it as: $$\int v(x) \,dy$$ Why this discrepancy? In both cases, the number of discs is the same so why should the summation be different? I tried interchanging the summation process and when i converted everything into polar co-ordinates ( $x = r\,cos\theta, y = r\,sin\theta $ ) i get an extra $cos\theta$ since $ dy = rd\theta.cos\theta$ The same happens to me when i calculate Moment of Inertia for a solid sphere & hollow sphere. Similarly when i calculate gravity for a point outside a solid sphere & hollow sphere. Can someone please tell me, why we need to change the summation process?? What decides the summation process, why the difference?","['integration', 'spheres', 'multivariable-calculus', 'multiple-integral', 'physics']"
3805999,"Is the definition of ""limit of function"" incomplete?","On Wikipedia the definition of limit of a function $f$ such that it assigns an output $f(x)$ to every input $x$ is given as follows: We say that the function has a limit $L$ at an input $p$ , if $f(x)$ gets
closer and closer to $L$ as $x$ moves closer and closer to $p$ . But I have a problem with it; If $L$ (for concrete example say $5$ ) is chosen as limit of the function then can't $L-0.1$ ( $4.9$ ) or $L-1$ ( $4$ ) or $L+1$ ( $6$ ) also be chosen as limit? Let me explain what I mean. If value of ""input"" is made to approach $p$ then, as given, the output will also approach $L$ , and also $L-0.1$ , $L-1$ ..... so what makes us choose only $L$ as the ""limit""? There is no special, explicit property, seems to be, mentioned which allows us to choose $L$ as the only ""limit"" and disregard other values(or does it?) like the fix difference between output, for given input, and the limit .","['limits', 'calculus', 'derivatives']"
3806047,Semicircle Question,I need help with the question in the image. I just need someone to help by pointing me in the right direction. I don't want a full solution. I want to try to work out this question myself but I just need someone to direct me. I was thinking if the question had something to do with joining the three points in the circle where the circle is touching the semicircles? Or perhaps this question has something to do with similar triangles? I know that an angle subtended by an arc inside a semicircle is 90 degrees.,"['radical-equations', 'euclidean-geometry', 'circles', 'geometry']"
3806065,How large is the area that the bug can access?,"The bug is placed at point $(0,0)$ . From $(x, y)$ the bug can move to $(x+1, y)$ , $(x-1, y)$ , $(x, y+1)$ , and $(x, y-1)$ . Some points are dangerous. To know which points are safe, we check if $$n(|x|)+n(|y|)\le23 \tag{1}$$ where $n(a)$ is the sum of digits of $a$ . Question : How large is the area (number of points) that the bug can access? Well, if it was just to check the sum of the absolute values of coordinates, then $|x|+|y| \le 23$ would draw a square whose diagonal is $d=46$ and area $d^2/2$ . However, with the rule $(1)$ I don't think one gets an ordinary geometrical object. By the help of some code, the answer seems to be $592597$ A Python program gives (for $14$ instead of $23$ ): Any help is appreciated.","['elementary-number-theory', 'combinatorics', 'geometry']"
3806072,The set of irreducible representations (over C) determines a finite group,"I'm trying to understand group representations, and I think that this statement is correct (where determines means up to isomorphism ), although I couldn't find a proof online (without references to more complicated things like ""Tannaka duality""), so I tried to prove it, but I may have overlooked something: Let $G = \{x_1, ..., x_n\}$ , $H = \{y_1, ..., y_n\}$ be two finite groups of same cardinality. Suppose G and H have the same set of irreducible representations in the following sense :
there is a finite set of morphisms $g_i$ (resp. $h_i$ ), each one from G (resp. H) to
some subgroup of $GL(n_i, \Bbb{C})$ and we can write $g_i(x_j) = P_i h_i(y_j) P_i^{-1}$ for some invertible matrix $P_i$ and all $j$ . Then if we consider the regular representation of G, we can write it as a sum of the $g_i$ representations
(with $n_i$ giving the multiplicity) and do the same for H. That shows that the two regular representations are isomorphic as we can build a new base using a block-diagonal transition matrix using $(P_i)$ .
As they are also faithful (isomorphic to the groups themselves), then G and H are isomorphic. Is that correct ? EDIT: I updated the ""same set of irreducible representations"" following the comments (that also requires the additional assumption of same cardinality)","['finite-groups', 'group-theory', 'linear-algebra', 'representation-theory']"
3806082,How to evaluate $\lim _{x\to 2^+}\left(\frac{\sqrt{x^2-4}+\sqrt{x}-\sqrt{2}}{\sqrt{x-2}}\right)$ (without L'Hopital)?,I am trying to evaluate the following limit: $$ \lim _{x\to 2^+}\left(\frac{\sqrt{x^2-4}+\sqrt{x}-\sqrt{2}}{\sqrt{x-2}}\right)$$ Approach #1 $ \frac{\sqrt{x^2-4}+\sqrt{x}-\sqrt{2}}{\sqrt{x-2}} = \\ \sqrt{x+2} + \frac{\sqrt{x}-\sqrt{2}}{\sqrt{x-2}} \cdot \frac{\sqrt{x}+\sqrt{2}}{\sqrt{x}+\sqrt{2}} =\\ \sqrt{x+2} + \frac{x-2}{\sqrt{x^2-2x}+\sqrt{2x-4}} =\\ \sqrt{x+2} + \frac{x-2}{\sqrt{x^2-2x}+\sqrt{2x-4}}\cdot\frac{\sqrt{x^2-2x}-\sqrt{2x-4}}{\sqrt{x^2-2x}-\sqrt{2x-4}} =  \\ \sqrt{x+2} + \frac{\sqrt{x^2-2x}-\sqrt{2x-4}}{x-2} $ But I still end up at the indefinite form $\frac12 + \frac{0}{\infty}$ My Approach #2 $\frac{\sqrt{x^2-4}+\sqrt{x}-\sqrt{2}}{\sqrt{x-2}} = \frac{\sqrt{x^2-4}+\sqrt{x}-\sqrt{2}}{\sqrt{x-2}} \cdot\frac{\sqrt{x^2-4}-(\sqrt{x}-\sqrt{2})}{\sqrt{x^2-4}-(\sqrt{x}-\sqrt{2})}= \frac1{\sqrt{x-2}}\frac{x^2-x+2\sqrt{2x}-2}{\sqrt{x^2-4}-(\sqrt{x}-\sqrt{2})}$ Which also seems to be a dead end. Any ideas on how to evaluate this?,"['limits', 'calculus', 'limits-without-lhopital']"
3806107,"Evaluating $\int_0^\infty \left| \frac{\sin t}{t} \right|^n \, \mathrm{d}t$ for $n = 3, 5, 7, \dots$","I would like to determine the general term of the following sequence defined by an infinite integral: $$
I_n = \int_0^\infty \left| \frac{\sin t}{t} \right|^n \, \mathrm{d}t \, ,
$$ wherein $n =3, 5, 7, \dots$ is an odd integer. It can be checked that the integral is convergent for all values of $n$ in the prescribed range. The case of even $n$ is solved in A sine integral $\int_0^{\infty} \left(\frac{\sin x }{x }\right)^n\,\mathrm{d}x$ . Also, $I_1 = \infty$ .
I have tried to use the method of multiple integrations by parts but in vein.
I was wondering whether there exists a suitable approach to address this problem more effectively.","['integration', 'improper-integrals', 'definite-integrals', 'real-analysis', 'calculus']"
3806166,"Proof of the identity $\int_0^{+\infty}\frac{\sin(x)}{x^\alpha}dx=\frac{\Gamma(\alpha/2)\Gamma(1-\alpha/2)}{2\Gamma(\alpha)}$ for $\alpha\in (0,2)$.",Let $0<\alpha<2.$ Looking for a proof for the following: $$\int_0^{+\infty}\frac{\sin(x)}{x^\alpha}dx=\frac{\Gamma(\alpha/2)\Gamma(1-\alpha/2)}{2\Gamma(\alpha)}.$$ Any ideas?,"['integration', 'measure-theory', 'improper-integrals', 'lebesgue-integral', 'real-analysis']"
