question_id,title,body,tags
3450856,Limit comparison Test for series 1/(n^2 * log n ) converge or diverge,"Doubt  - used inequality, $1/(n^2 \log n) < 1/ n^2$ will be true only for $n > 10$ as $\log n < 1$ for $1 < n < 10 $ . but here summation is running from $n=  2$ to infinite
 please check whether the solution which is arrived is correct with the correct procedure.",['limits']
3450860,Minimum number of commands to delete in order to stop a robot visiting all points of the lattice,"Suppose you place commands ""turn left"" or ""turn right"" in some points with integer coordinates (on a plane). Then, a robot is placed at $(0; 0)$ . The robot begins to move in the positive direction of X-axis and when it gets into a point where a command is written it turns by $90^\circ$ and begins to move in the new direction. When it gets to a point with no command it just continues to move straight. Suppose the commands are written so that the robot visits all points with integer coordinates. Now, you can delete commands from points. The question is: can you always delete just one command so that the property ""the robot visits all points"" breaks? If not, what is the minimum number of points which should be tampered with to achieve this? Any hints are welcome.","['recreational-mathematics', 'discrete-mathematics']"
3450861,Integral $\int_{-\infty}^{\infty} \frac{\cos t \sin( \sqrt{1+t^2})}{\sqrt{1+t^2}}dt$,"I want to evaluate the integral $$\int_{-\infty}^{\infty} \frac{\cos t \sin( \sqrt{1+t^2})}{\sqrt{1+t^2}}dt$$ I tried using Feynman’s trick and introduced the parameter $e^{-\sqrt{1+t^2} x}$ , then differentiating with respect to $x$ under the integral. However I don’t see a good way to evaluate the resulting integral. Is there a contour method? This was an AMM problem ( $12145$ ) .",['integration']
3450883,"Finding $x$, $y$ that minimize $(\frac{\sqrt3\sin y}{\sqrt2\sin(x+y)}+1)(\frac{\sqrt2\sin x}{3\sin y}+1)^2(\frac{\sin(x+y)}{7\sqrt3\sin x}+1)^4$","This problem was given in 2018 in the entrance exams for MSU(8th problem). The only information online about the exam is its answers and is supposedly aimed at highschool students. I am in a maths-oriented high school so we are studying what we would in the 1st year in uni. The problem is: Find all pairs of $x,y \in (0;\frac{\pi}{2}) $ where $f(x,y) = f_{min}$ . (When is this equal to its minimum value) $$ f(x,y) =   \left(\frac{\sqrt3\sin y} {\sqrt2\sin(x+y)} + 1\right)\left(\frac{\sqrt2\sin x}{3\sin y} + 1\right)^2\left( \frac{\sin(x+y)}{7\sqrt3\sin x} + 1\right)^4  $$ I only want a starting point, how do I approach this? Derivatives? Or do the (...)^4 ?","['trigonometry', 'functions']"
3450913,What is the flaw in this proof of the Collatz Conjecture?,"Would love any feedback for this proposed proof of the Collatz Conjecture. (For more details explaining each step, I made a video here: https://www.youtube.com/watch?v=P0F4zbNdbTU ) Ignoring all even number (which trivially lead to an odd number), we create a ""Collatz Odd-Only Variant"" as such, where $n$ is a positive odd integer: $f(n) = \left\{\begin{matrix}
\frac{n-1}{4} & \mbox{if }n\equiv 5 \mbox{ }(\mbox{mod }8)\\
\frac{3n+1}{4} & \mbox{if }n\equiv 1 \mbox{ }(\mbox{mod }8)\\ 
\frac{3n+1}{2} & \mbox{if }n\equiv 3 \mbox{ }(\mbox{mod }4)\\
\end{matrix}\right.$ This creates a tree that looks like this: Collatz Odd Only Variant Tree We then must show that all odd numbers in this variant are connected to 1. (That is, they do not create loops or diverge elsewhere.) To do this, we create a new simpler function, again where $n$ is a positive odd integer: $f(n) = \left\{\begin{matrix}\
\frac{n-1}{4} & \mbox{if }n\equiv 5 \mbox{ }(\mbox{mod }8)\\
\frac{n+3}{4} & \mbox{if }n\equiv 1 \mbox{ }(\mbox{mod }8)\\ 
\frac{n-1}{2} & \mbox{if }n\equiv 3 \mbox{ }(\mbox{mod }4)\\
\end{matrix}\right.$ In this function, we know that all $n$ 's will lead to 1, because $n$ must shrink with every iteration, and it cannot shrink below 1. This creates a tree like this: Simpler Odd-Only Tree Since we know this tree must contain all positive odd integers, all we have to do is disconnect and reattach some nodes so that this tree matches the ""Collatz Odd-Only Variant"" tree. As long as we do not reconnect a node to one of its own children (a node somewhere above it, representing the iteration of the function that preceded it), we are sure that we will never get loops and that the odd numbers will remain connected to 1. When we disconnected the $\frac{n+3}{4}$ nodes and reattach them to $\frac{3n+1}{4}$ nodes, we know we will never connect it to a child, because we must connect it to a node that is less than $n$ (and all of $n$ 's children will, by definition, be greater than $n$ ). Disconnecting $\frac{n-1}{2}$ nodes to $\frac{3n+1}{2}$ is also certain to never connect to one of its own children, because iterating backwards through our function can never give us $\frac{3n+1}{2}$ . (We can see this by checking the inverses of the equations in our function, even when we combine them to create composite functions.) Since we know all nodes can be disconnected and reconnected while remaining attached to the tree connecting them all to 1, doesn't this prove the conjecture? Or what am I missing? Would love to know your thoughts! Thank you!","['collatz-conjecture', 'number-theory', 'proof-verification']"
3450917,"$\operatorname{Hom}(X \times Z, Y) \cong \operatorname{Hom}(X, \operatorname{Map}(Z, Y))$ is not true in $\textbf{Top}$","I read that in the category of topological space it is not true that $\operatorname{Hom}(X \times Z, Y) \cong \operatorname{Hom}(X, \operatorname{Map}(Z, Y))$ for every $X, Y, Z$ topological spaces. In particular, I read that this does not work when $Z = \mathbb{R} \setminus \{ 1/n \mid n \in \mathbb{N}\}$ . This was not explained, i.e. there was no example. I am looking for an example or – maybe better – just a hint . EDIT $\operatorname{Map}(Z, Y)$ is endowed with the compact-open topology $\cong$ means just bijection; i.e. there is already an obvious bijection as sets, but the claim is that (I think) sometimes the ""output function"" is not continuous","['general-topology', 'category-theory']"
3450937,Maurer-Cartan form defines a connection on a Lie Group considered as a principal $G$ bundle.,"Given a Lie group $G$ we can consider it as a principal $G$ bundle over any element in $G$ . 
Consider the Maurer-Cartan form $\omega$ defined by the left translation: for $X\in T_gG$ we have $\omega(X)=d_g(L_{g^{-1}})(X)$ . To show that $\omega$ defines a connection on the principal bundle we must show the following: $R^*_g\omega=Ad_{g^{-1}}(\omega)$ , for all $g\in G$ . $\iota_{\alpha(X)}\omega=X$ for all $X$ in $\mathfrak{g}$ . I have not been able to get very far in my attempt to show 1/
On the left hand side we have $R^*_g\omega=R^*_gd_g(L_{g^{-1}})$ . 
On the right hand side we have $Ad_{g^{-1}}(\omega)=d(R_g L_{g^{-1}})(d_gL_{g^{-1}})=d(R_g(d_gL_{g^{-1}}))dL_{g^{-1}}(d_gL_{g^{-1}})$ . But I do not see how to reconcile these two sides.","['principal-bundles', 'vector-bundles', 'lie-groups', 'differential-geometry']"
3450956,Can you solve $\sin x=\sin(x/2)$ without a sum/double/half angle identity?,"If you have the equation $$\sin x=\sin\frac{x}{2}$$ and you need to solve for $x$ , such that $0\leq x\leq 2\pi$ , can you do this without the use of any identities? Someone asked me this question and I said that the way to solve could be by using the sum identity. First I subtracted $\sin(\frac{x}{2})$ from both sides and then applied the identity: $$-\sin(s)+\sin(t)=-2\cos(\frac{s+t}{2})\sin(\frac{s-t}{2})$$ So I simplified my equation to get $$2\cos(\frac{3x}{4})\sin(\frac{x}{4})=0$$ Then solving $\cos(\frac{3x}{4})=0$ and $\sin(\frac{x}{4})=0$ , I get that $x=\frac{2\pi}{3}, 2\pi, 0$ However the person is not familiar with any identities and I wanted to know if there is a way to solve this without them.","['algebra-precalculus', 'trigonometry']"
3450960,Multiplying two integrals becomes a double integral?,"I ran by this in a textbook: \begin{align*}
I^2 & =\left(\int_{-\infty}^\infty e^{-x^2/2}dx\right)\left(\int_{-\infty}^\infty e^{-y^2/2}dy\right)\\
& =\int_{-\infty}^\infty\int_{-\infty}^\infty e^{-x^2/2}e^{-y^2/2}dxdy=\int_{-\infty}^\infty\int_{-\infty}^\infty e^{-(x^2+y^2)/2}dxdy
\end{align*} How come this is valid? I assume it's not generally true because it doesn't seem like you can just go around mashing integrals together. Does it only work because they are independent variables? And therefore constants with respect to each other?","['integration', 'multivariable-calculus', 'multiple-integral', 'improper-integrals']"
3450982,What is the median of an empty set?,"The median of a singleton set is the element itself.
The median of a set with two elements is the average of the elements. What should a machine return when required the median of an empty set?",['statistics']
3451001,Is there a general stability result for the linear delay differential equation $x'(t)=Ax(t)+Bx(t-\tau)$?,"Is there a general stability result for the linear delay differential equation $x'(t)=Ax(t)+Bx(t-\tau)$ where $A$ and $B$ are $m\times m$ matrices?  If not, is there a current summary of the known stability conditions?  As of 2007, I believe the answer was no.  Matsunaga had a nice summary in a 2007 paper titled, ""Exact stability criteria for delay differential and difference equations"" that specified: I was unable to locate a more recent survey and didn't know if better criteria had been found since that publication.","['stability-in-odes', 'stability-theory', 'delay-differential-equations', 'ordinary-differential-equations']"
3451005,Verify the trig identity $\frac{\cot^2\theta-1}{\csc^2\theta}=\csc \theta -1$,"I've stumbled across a brain-teaser. After using some identities, I get the left hand side  equal to $\cos^2\theta - \sin^2\theta$ . I'm not aware of any other identities that could get me to the right hand side. I'm actually leaning towards there being a typo in the problem.",['trigonometry']
3451020,How $dxdy$ becomes $rdrd\theta$ during integration by substitution with polar coordinates,Does anyone know how $dxdy$ becomes $rdrd\theta$ in the example below? I always end up with cosines and sines in the expression no matter how I go about it and I'm not sure  how they are disappearing from the expression.,"['integration', 'substitution', 'multiple-integral', 'derivatives']"
3451053,prove two dependent sequences converge,"Please help with my homework question: Define $\{a_n\}_{n=1}^{\infty}$ , $\{b_n\}_{n=1}^{\infty}$ as: $b_{n+1}=\frac{a_n+b_n}{2}, a_{n+1}=\frac{a_n^2+b_n^2}{a_n+b_n}, 0<b_1<a_1$ Prove the limits converge and that $\lim \limits_{n \to \infty}a_n = \lim \limits_{n \to \infty}b_n $ . I have tried to show that they are monotone and bounded, tried looking for a property of arithmetic mean (related to $b_n$ ) but I'm completely stuck as none of these yielded anything useful. Any help would be appreciated","['convergence-divergence', 'sequences-and-series']"
3451088,pullback of Maurer-Cartan form for matrix lie groups,"I want to show that for a manifold $M$ and smooth map $\varphi:M\rightarrow GL(n,\mathbb{R})$ we have $\varphi^*\omega=\varphi^{-1} d\varphi$ . The Maurer-Cartan form is defined as for $X\in T_gG$ we have $\omega(X)=d_g(L_{g^{-1}})(X)$ . My idea to approach this probblem was to first show that for a matrix Lie group the Maurer-Cartan form is given by $g^{-1}dg$ and then statment $\varphi^*\omega=\varphi^{-1} d\varphi$ should somehow follow. First question: To show that $\omega(X)=d_g(L_{g^{-1}})(X)$ agrees with $g^{-1}dg$ . For a matrix group $L_h$ is a linear operaion so $d_g(L_h)=L_h$ and then $\omega(X)=L_{g^{-1}}(X)=g^{-1}(X)$ . But how do we get $dg$ into this? Second question: Does $\varphi^*\omega=\varphi^{-1} d\varphi$ follow from $g^{-1}dg$ and if so how?","['differential-forms', 'lie-groups', 'differential-geometry']"
3451103,Polynomial that indicates whether or not $x = 1 \pmod n$.,"For each $n$ , is there a polynomial that takes representatives $x \in \Bbb{Z}$ for $\bar{x} \in \Bbb{Z}_n$ , and returns whether or not $x = 1 \pmod n$ ? For example, $n = 2$ .  Then $x \mapsto x \pmod{2}$ works.  Is there a general formula for the $n$ th modulus? I'm stuck on case $n= 3$ . Let's loosen the criteria, we can use any of the classical elementary functions.  So exponentiation will work: $$
n = 3: \\
f(x) = 2^x - 1 \pmod 3\\
0 \to 2^0 - 1 = 1 - 1 = 0 \pmod 3\\
1 \to 2^1 - 1 = 1 \pmod 3\\
2 \to 2^2 - 1 = 3 = 0 \pmod 3
$$","['modular-arithmetic', 'elementary-number-theory', 'integers', 'functions', 'polynomials']"
3451118,Reference request for probability theory,"What are some good textbooks to learn probability theory at a graduate level (measure theory and all that)? I have read most of Durrett and it's a pretty good book, but I was hoping for another approach to the subject. If possible, it would be great if the reference focused on random walks and markov chains.","['probability-theory', 'reference-request']"
3451140,Karatzas and Shreve solution to uniform integrability of backward martingale,"Let $\{\mathscr{F}_n\}_{n=1}^\infty$ be a decreasing sequence of sub- $\sigma$ -fields of $\mathscr{F}$ , i.e. $\mathscr{F}_{n+1} \subset \mathscr{F}_n \subset \mathscr{F}$ , and let $\{X_n, \mathscr{F}_n\}$ be a backward submartingale, i.e. $E|X_n| < \infty$ , $X_n$ is $\mathscr{F}_n$ measurable and $E(X_n | \mathscr{F}_{n+1}) \ge X_{n+1}$ a.s. $P$ , for every $n \ge 1$ . Then $l:= \lim_{n \to \infty} E(X_n) > -\infty$ implies that the sequence $\{X_n\}$ is uniformly integrable. The solution to this problem is given below. However, I cannot understand how we get the final two inequalities. That is, how do we select $\lambda>0$ in such a way that $$\sup_{n>m} \int_{X_n < -\lambda} |X_m|dP < \epsilon/2?$$ And why do these choices of $m$ and $\lambda$ give $$\sup_{n>m} \int_{X_n^- > \lambda}X_n^- dP < \epsilon?$$ I would greatly appreciate any explanations.","['martingales', 'uniform-integrability', 'probability-theory', 'real-analysis']"
3451159,Set Theory Equivalence clarification,"When I read about equivalence $(A=B)$ between two sets I get two definitions: \begin{equation}  
A \subset B    \\
B \subset A    \\
\end{equation} \begin{equation}  
A \subseteq  B    \\
B \subseteq  A    \\
\end{equation} I feel definition (1) is not correct because in order to become proper subset there must be an element which is not a part of other set. I am ok with definition (2). But in some books I see definition (1). Could you confirm which is correct?","['elementary-set-theory', 'notation', 'real-analysis']"
3451206,Numerical Differentation for Complex Functions,"Given a function $f(x)$ , there is a way to approximate $f'(x)$ : the finite-step formula. ${\displaystyle {\frac {f(x+\epsilon)-f(x)}{\epsilon}}}$ , for $\epsilon$ arbitrarily small. The smaller $\epsilon$ is, the more accurate the answer would be. However, I don't believe that carries over to $f(z)$ . So, how would it be achieved?","['numerical-methods', 'derivatives']"
3451250,"If the principal curvatures of a surface are constant then either part of plane, sphere, or circular cylinder","We get stuck on this homework problem: Prove that if the principal curvatures of a surface $M \subset \mathbb{R}^{3}$ are constant, then $M$ is either part of a plane, a sphere, or a circular cylinder. In the case $k_{1} \neq k_{2}$ assume that there is a principal frame field on all of $M .$ Show that the $k_{1}$ -curves are parallel lines and that the $k_{2}$ -curves
are circles. We have finished proving the first part of the hint that $k_1$ -curves are parallel lines, but we could not find a good way to prove $k_2$ -curves are circles.",['differential-geometry']
3451301,An attempt to prove the generalization of $\sum_{n=1}^\infty \frac{(-1)^nH_n}{n^{2a}}$,"The following classical generalization $$\sum_{n=1}^\infty\frac{(-1)^{n}H_n}{n^{2a}}=-\left(a+\frac 12\right)\eta(2a+1)+\frac12\zeta(2a+1)+\sum_{j=1}^{a-1}\eta(2j)\zeta(2a+1-2j)$$ where $\eta(a)=\sum_{n=1}^\infty\frac{(-1)^{n-1}}{n^a}=(1-2^{1-a})\zeta(a)$ is the Dirichlet eta function. was proved by G. Bastien here page 7 Eq. 17 and also by Cornel here . I am trying to prove it in a different way but came across an integral that can be calculated by Beta function but I want it in $\zeta$ if possible to get the right result. Here is my approach which follows from the same idea of my solution here : By using $$\frac1{n^{2a}}=-\frac1{(2a-1)!}\int_0^1x^{n-1}\ln^{2a-1}(x)\ dx$$ we can write $$\sum_{n=1}^\infty\frac{(-1)^nH_n}{n^{2a}}=-\frac1{(2a-1)!}\int_0^1\frac{\ln^{2a-1}(x)}{x}\left(\sum_{n=1}^\infty(-x)^nH_n\right)\ dx$$ $$=\frac1{(2a-1)!}\int_0^1\frac{\ln^{2a-1}(x)\ln(1+x)}{x(1+x)}dx=\frac1{(2a-1)!}I_a\tag1$$ $$I_a=\int_0^1\frac{\ln^{2a-1}(x)\ln(1+x)}{x(1+x)}dx=\int_0^\infty\frac{\ln^{2a-1}(x)\ln(1+x)}{x(1+x)}dx-\underbrace{\int_1^\infty\frac{\ln^{2a-1}(x)\ln(1+x)}{x(1+x)}dx}_{x\mapsto 1/x}$$ $$=\int_0^\infty\frac{\ln^{2a-1}(x)\ln(1+x)}{x(1+x)}dx+\color{blue}{\int_0^1\frac{\ln^{2a-1}(x)\ln(1+x)}{1+x}dx}-\int_0^1\frac{\ln^{2a}(x)}{1+x}dx$$ By adding $$I_a=\int_0^1\frac{\ln^{2a-1}(x)\ln(1+x)}{x(1+x)}dx=\int_0^1\frac{\ln^{2a-1}(x)\ln(1+x)}{x}dx-\color{blue}{\int_0^1\frac{\ln^{2a-1}(x)\ln(1+x)}{1+x}dx}$$ to both sides, the blue integral nicely cancels out and we get $$2I_a=\int_0^\infty\frac{\ln^{2a-1}(x)\ln(1+x)}{x(1+x)}dx+\underbrace{\int_0^1\frac{\ln^{2a-1}(x)\ln(1+x)}{x}dx}_{IBP}-\int_0^1\frac{\ln^{2a}(x)}{1+x}dx$$ $$=\int_0^\infty\frac{\ln^{2a-1}(x)\ln(1+x)}{x(1+x)}dx-\frac{1+2a}{2a}\int_0^1\frac{\ln^{2a}(x)}{1+x}dx$$ where $$\int_0^1\frac{\ln^{2a}(x)}{1+x}dx=\sum_{n=1}^\infty(-1)^{n-1}\int_0^1 x^{n-1}\ln^{2a}(x)dx=(2a)!\sum_{n=1}^\infty\frac{(-1)^{n-1}}{n^{2a+1}}=(2a)!\eta(2a+1)$$ so $$I_a=\frac12\int_0^\infty\frac{\ln^{2a-1}(x)\ln(1+x)}{x(1+x)}dx-\left(a+\frac12\right)(2a-1)!\eta(2a+1)\tag2$$ Plug $(2)$ in $(1)$ $$\sum_{n=1}^\infty\frac{(-1)^nH_n}{n^{2a}}=-\left(a+\frac12\right)\eta(2a+1)+\frac1{2(2a-1)!}\int_0^\infty\frac{\ln^{2a-1}(x)\ln(1+x)}{x(1+x)}dx\tag{3}$$ So any idea how to evaluate the integral in $(3)$ in a way that completes my proof?","['integration', 'euler-sums', 'alternative-proof', 'harmonic-numbers', 'sequences-and-series']"
3451349,Spec$(\overline{\mathbb{Q}}\otimes\overline{\mathbb{Q}})$ and Gal$(\overline{\mathbb{Q}}/\mathbb{Q})$ natural bijection,"This is problem 9.2.E in Vakil's notes: Show that the points of Spec $(\overline{\mathbb{Q}}\otimes\overline{\mathbb{Q}})$ and are in natural bijection with Gal $(\overline{\mathbb{Q}}/\mathbb{Q})$ , and the Zariski topology on the former agrees with the profinite topology on the latter. I have worked out the case for finite Galois extensions.
If $K/\mathbb{Q}$ is a Galois extension of degree $n<\infty$ with minimal polynomial $f$ , then $\overline{\mathbb{Q}}\otimes_\mathbb{Q} K\cong \overline{\mathbb{Q}}[x]/(f)\cong \overline{\mathbb{Q}}^n$ , so $$\textrm{Spec}(\overline{\mathbb{Q}}\otimes_\mathbb{Q} K) \cong \bigsqcup \textrm{Spec}(\overline{\mathbb{Q}}).$$ This is the disjoint union of $n$ points, which is in bijection with $\textrm{Gal}(K/\mathbb{Q})$ since $|\textrm{Gal}(K/\mathbb{Q})|=\deg f = n$ . I want to be able to conclude by just taking the limit, but I'm not sure whether or not this will preserve the bijection, especially because we are taking limits in distinct categories (affine schemes and groups) both different from the category of sets. I'm also not sure how I would show that the topologies agree afterwards.","['algebraic-geometry', 'schemes']"
3451379,"Prove that for all $n ≥ 1$, $\sum_{i=1}^{n}\frac{1}{i^2}≤2 - \frac{1}{n}$","Prove that for all $n ≥ 1 $ $$\sum_{i=1}^{n}\frac{1}{i^2}≤2 - \frac{1}{n}$$ My attempt: By induction. Base case : $n = 1$ $$1 ≤ 2 - 1 = 1$$ Induction step: Suppose it is true for $n$ , i.e $$\sum_{i=1}^{n}\frac{1}{i^2}≤2 - \frac{1}{n}$$ Adding $\frac{1}{n^2+n}$ to both sides gives $$\sum_{i=1}^{n}\frac{1}{i^2} + \frac{1}{n^2+n} ≤2 - \frac{1}{n} + \frac{1}{n^2+n} \implies $$ $$\sum_{i=1}^{n}\frac{1}{i^2} + \frac{1}{n^2+n} ≤2 - \frac{1}{n+1}$$ Since $\frac{1}{n^2 + 2n + 1} < \frac{1}{n^2 + n}$ , we have $$\begin{align}
2 - \frac{1}{n+1} & ≥ \sum_{i=1}^{n}\frac{1}{i^2} + \frac{1}{n^2+n}\\
& ≥ \sum_{i=1}^{n}\frac{1}{i^2} + \frac{1}{n^2+2n+1} \\
& = \sum_{i=1}^{n}\frac{1}{i^2} + \frac{1}{(n+1)^2} \\
& = \sum_{i=1}^{n+1}\frac{1}{i^2}
\end{align}$$ $\Box$ Is it correct?","['proof-verification', 'discrete-mathematics']"
3451390,"Why is the sum of x1...xn uniformly distributed over [0,1] equal to 1/n!?","Given x1, x2,...,xn is iid over a uniform [0,1], what is the probability that x1+x2+...+xn < 1? Every explanation I've found seems to use simplex/n-dimension geometry which I can't understand. What's a probability/intuitive way of understand the result of $\frac{1}{n!}$ ? My questionable logic is that 1/2 + 1/4 + 1/8 ... = 1 towards infinity. So for the sum of finite n < 1, the probability has to use that logic. So $\sum \frac{1}{2^i}$ , then since order doesn't matter, multiply it by n to get $n\sum \frac{1}{2^i}$ . Now out of the (probably) many issues, the most first is that I can't get the probability of any point in a continuous distribution. That first 1/2 is based off of the mean.","['statistics', 'probability-distributions', 'probability']"
3451416,Least Upper Bound Existence Problem for Real and Rational Numbers,"Doubt is regarding the proof of the statement  such that there is no Least Upper Bound for the positive rational numbers with definitions such as $p^2<2$ . Before explaining the conceptual doubt let me mention some the definitions used in this question as follows Definition  1 Definition  2 Proof of no fixed maximum element for the set $p^2$ <2 Analyse   maximum element q which is less than $p^2$ . For any p we can always find q that is greater . So it never ends. As a result no maximum element q possible here. Reasoning for no least upper bound This is the claim they(Rudin Book) make for the reasoning of ""No least upper bound"" and is given as follows Rudin Claim The set A is bounded above. In fact, the upper bounds of A are exactly
the members of B. Since B contains no smallest member, A has no least
upper bound in Q. Doubts As per definition 1 and 2 , members of A also can be Upper Bound . Then how can the  Rudin reasoning mentioned above say "" In fact, the upper bounds of A are exactly  the members of B."".? Members of B can be upper bound so as  members of A.  Does the
statement ""In fact, the upper bounds of A are exactly  the members of
B."" shows correct meaning? Because not just members of B are upper
bound candidates, members of A also are eligible provided if we can
find a maximum fixed   q . Cant we say it has no least upper bound because we cant find maximum of the $p^2<2$ in A? We can have q which is maximum as mentioned above for any p.  Since we cant find fixed q, as it generates new maximum for any p,  we say there is no point in finding out the number which is greater than all members of A    . Would this point is sufficient enough to say there is no least upper bound than saying Rudin reasoning mentioned above ? If we could find a q which is above of all members of A,it would have been the least upper bound. My problem is you don't have to go B and find no smallest element there. We can say from the absence fixed maximum in A by seeing the property of q What is $p^2<2$ defined on Real numbers than rational? What could be the least upper bound? Would the same issue with q fixing happens here also? But book and lecture says there is least upper bound of R . Then what is that value in this particular case? NB :: I was following a internet video lecture which is based on Rudin's mathematical analysis 3rd edition book. This doubt is based on page 3-5 of that book. It is evident that there is no $\sqrt{2}$ in Q so we can say no least upper-bound. But book explains it based on absence maximum in A and minimum in B. I am bit confused about the proper reasoning","['elementary-set-theory', 'order-theory', 'calculus', 'real-analysis']"
3451431,How to prove that $2\sqrt{3}$ is greater than $\pi$,"Without calculator, how to prove that $2 \sqrt{3} > \pi$ ? The level is baccalauréat grade.
I confirm it's not a school exercise at all, as I left school like 35 years ago.",['calculus']
3451481,Differential Geometry: Isometries preserves angles.,"The statement is that: If $f:S_1\to S_2$ is an isometry between two regular surfaces and $v_1, v_2$ are two vectors in $T_pS_1$ , then the angle between $v_1, v_2$ = the angle between $(df)_p(v_1)$ and $(df)_p(v_2)$ . How can we show that by using the first fundamental form?",['differential-geometry']
3451486,Derivative of quadratic with Hadamard product,"I proposed a similar question involving logarithms , but the problem is about scalar. I am trying to solve the more generalized form: $$ \min_{\mathbf{x} \in \mathbb{R}^N_+} \left( \sum_i \left( h_i^T(\mathbf{x}\circ\mathbf{x}) - \mathbf{c_1}\log h_i^T(\mathbf{x}\circ\mathbf{x})\right) + r_1\parallel \mathbf{x}  - \mathbf{c_2}\parallel_2^2 \right)$$ where $\mathbf{c_1} \in \mathbb{R}^+$ , $\mathbf{c_2} \in \mathbb{R}^N_+$ , $h_i \in \mathbb{R}^{N}_+$ is each column of a known matrix, $r_1 \in \mathbb{R}^+$ . All are constants. So, for scalar quadratic eqation, we can find the square root via dividing $(1+r_1)$ , but when $1$ is a matrix $H$ , how to deal with this form? But I don't know whether directly extension of scalar quadratic equation is right. Can anyone help me? Thanks in advance! Edit: formulate the problem to make it clearly.","['hadamard-product', 'matrices', 'matrix-calculus', 'optimization', 'matrix-equations']"
3451508,Where do numerical solvers for differential equations fail?,"I am about to begin working on a numerical differential equation solver that uses heuristic optimisation methods (like evolution strategies or differential evolution ) to approximate a solution. However, since such methods can not guarantee global convergence their only ""raison d'être"" are differential equations that are numerically hard to solve. What could be some examples of such problems? Edit: The starting point for my work is this Ph.D. Thesis: Solving Differential Equations with Evolutionary Algorithms by José María Chaquet Ulldemolins . The optimiser generates approximated solutions, that are represented as a sum of shifted and distorted Gaussian Kernels. Especially interesting is the comparison performed in chapter 5.3.3 Comparison of DESCMA-ES with Numerical Methods , which compares the optimiser technique against a Runge-Kutta and an “simple explicit” method. This comparison is done on the Poisson Equation and the Wave Equation. It seems that the optimisation scheme is somewhat competitive, at least it reaches roughly the same residual error. Obviously, the classical methods are way faster.","['numerical-methods', 'ordinary-differential-equations', 'partial-differential-equations']"
3451518,Evaluate $\lim \limits_{x \to -1^+} (x+1)^{\frac{1}{x+1}}$,I am trying to evaluate limit $$\lim \limits_{x \to -1^+} (x+1)^{\frac{1}{x+1}}$$ The answer is $0$ but I am not able to get it. My working: $$\lim \limits_{x \to -1^+} e^{\frac{ln(x+1)}{x+1}}$$ $$e^{\lim \limits_{x \to -1^+} (\frac{ln(x+1)}{x+1})}$$ $$\Rightarrow \lim \limits_{x \to -1^+} \frac{ln(x+1)}{x+1}$$ Using L'hopital rule: $$\lim \limits_{x \to -1^+} \frac{1}{x+1}$$ Substituting $-1^+$ into the equation $$\frac{1}{-1+1} = \infty$$ $$\therefore e^\infty = \infty$$ Am I doing something wrong?,"['limits', 'calculus', 'exponentiation']"
3451594,"If I be the incenter of the triangle ABC and $x,y,z$ the circum radii of the triangles IBC,ICA & IAB, show that $4R^3-R(x^2+y^2+z^2)-xyz = 0$","If I be the incenter of the triangle ABC and $x,y,z$ the circum radii of the triangles IBC,ICA & IAB, show that $4R^3-R(x^2+y^2+z^2)-xyz = 0$ $$\angle BID=90^{\circ}-\dfrac{B}{2}$$ $$\angle CID=90^{\circ}-\dfrac{C}{2}$$ $$\angle BIC=180^{\circ}-\dfrac{B+C}{2}$$ $$\angle BIC=180^{\circ}-\dfrac{180-A}{2}$$ $$\angle BIC=90^{\circ}+\dfrac{A}{2}$$ $$x=\dfrac{a}{2\sin\left(90^{\circ}+\dfrac{A}{2}\right)}$$ $$x=\dfrac{a}{2\cos\dfrac{A}{2}}$$ In the similar way: $$y=\dfrac{b}{2\cos\dfrac{B}{2}}$$ $$z=\dfrac{c}{2\cos\dfrac{C}{2}}$$ $$R=\dfrac{abc}{4\triangle}$$ $$4R^3-R(x^2+y^2+z^2)-xyz$$ $$\dfrac{4(abc)^3}{64\triangle^3}-\dfrac{abc}{4\triangle}\left(\dfrac{a^2}{4\cos^2\dfrac{A}{2}}+\dfrac{b^2}{4\cos^2\dfrac{B}{2}}+\dfrac{c^2}{4\cos^2\dfrac{C}{2}}\right)-\dfrac{abc}{8\cos\dfrac{A}{2}\cos\dfrac{B}{2}\cos\dfrac{C}{2}}$$ $$\dfrac{1}{16}\left(\dfrac{(abc)^3}{\triangle^3}-\dfrac{abc}{\triangle}\left(\dfrac{a^2bc}{s(s-a)}+\dfrac{ab^2c}{s(s-b)}+\dfrac{abc^2}{s(s-c)}\right)-\dfrac{2(abc)^2}{s\triangle}\right)$$ $$\dfrac{(abc)^2}{16\triangle}\left(\dfrac{abc}{\triangle^2}-\dfrac{1}{s}\left(\dfrac{a}{s-a}+\dfrac{b}{s-b}+\dfrac{c}{s-c}\right)-\dfrac{2}{s}\right)$$ $$\dfrac{(abc)^2}{16\triangle}\left(\dfrac{abc}{\triangle^2}-\dfrac{1}{s}\left(\dfrac{a}{s-a}+\dfrac{b}{s-b}+\dfrac{c}{s-c}+2\right)\right)$$ Putting value of $s=\dfrac{a+b+c}{2}$ $$\dfrac{(abc)^2}{16\triangle}\left(\dfrac{abc}{\triangle^2}-\dfrac{2}{s}\left(\dfrac{a}{b+c-a}+\dfrac{b}{a+c-b}+\dfrac{c}{a+b-c}+1\right)\right)$$ $$\dfrac{(abc)^2}{16\triangle}\left(\dfrac{abc}{\triangle^2}-\dfrac{2}{s}\left(\dfrac{a(a+c-b)(a+b-c)+b(b+c-a)(b+a-c)+c(c+a-b)(c+b-a)+(a+c-b)(b+c-a)(a+b-c)}{(a+b-c)(b+c-a)(a+c-b)}\right)\right)$$ $$\dfrac{(abc)^2}{16\triangle}\left(\dfrac{abc}{\triangle^2}-\dfrac{2}{s}\left(\dfrac{a(a^2-b^2-c^2+2bc)+b(b^2-a^2-c^2+2ac)+c(c^2-a^2-b^2+2ab)+(b+c-a)(a^2-b^2-c^2+2bc)}{(a+b-c)(b+c-a)(a+c-b)}\right)\right)$$ $$\dfrac{(abc)^2}{16\triangle}\left(\dfrac{abc}{\triangle^2}-\dfrac{2}{s}\left(\dfrac{(b+c)(a^2-b^2-c^2+2bc)+b(b^2-a^2-c^2+2ac)+c(c^2-a^2-b^2+2ab)}{(a+b-c)(b+c-a)(a+c-b)}\right)\right)$$ $$\dfrac{(abc)^2}{16\triangle}\left(\dfrac{abc}{\triangle^2}-\dfrac{2}{s}\left(\dfrac{ba^2-b^3-bc^2+2b^2c+ca^2-cb^2-c^3+2bc^2+b(b^2-a^2-c^2+2ac)+c(c^2-a^2-b^2+2ab)}{(a+b-c)(b+c-a)(a+c-b)}\right)\right)$$ $$\dfrac{(abc)^2}{16\triangle}\left(\dfrac{abc}{\triangle^2}-\dfrac{2}{s}\left(\dfrac{ba^2-b^3+bc^2+b^2c+ca^2-c^3+b^3-ba^2-bc^2+2abc+c^3-a^2c-b^2c+2abc}{(a+b-c)(b+c-a)(a+c-b)}\right)\right)$$ $$\dfrac{(abc)^2}{16\triangle}\left(\dfrac{abc}{\triangle^2}-\dfrac{2}{s}\left(\dfrac{4abc}{(a+b-c)(b+c-a)(a+c-b)}\right)\right)$$ $$\dfrac{(abc)^2}{16\triangle}\left(\dfrac{abc}{\triangle^2}-\dfrac{abc}{s(s-a)(s-b)(s-c)}\right)$$ $$\dfrac{(abc)^2}{16\triangle}\left(\dfrac{abc}{\triangle^2}-\dfrac{abc}{\triangle^2}\right)$$ So this was a very long proof, in short time it is not feasible to do all of this, is there any shorter or smarter way to solve this question.","['trigonometry', 'triangles']"
3451609,"$\mathbb E[f(X,Y) \mid \mathfrak B] (\omega) = \mathbb E[f(X(\omega),Y) \mid \mathfrak B] (\omega)$, if $X$ is $\mathfrak B$-mb.","Let us discuss the following assertion, where $(\ast)$ stands for some extra assumption to be specified. --- (A) --- Let $(\Omega,\mathfrak A,\mathbb P)$ be a probability space. Let $\mathfrak B\subset \mathfrak A$ be a sub- $\sigma$ -field, $n\in\mathbb N$ . Let $(X,Y) \in L^0((\Omega,\mathfrak B),\mathbb R^n) \times L^0((\Omega,\mathfrak A),\mathbb R^n)$ be, respectively, $\mathfrak B$ - and $\mathfrak A$ -measurable random vectors of our space, and $f$ a bounded Borel function on $\mathbb R^{2n}$ . Then, if $(\ast)$ holds, we have, for almost any $\omega\in\Omega$ , \begin{equation}
\mathbb E[f(X,Y) \mid \mathfrak B] (\omega) = \mathbb E[f(X(\omega),Y) \mid \mathfrak B] (\omega).
\end{equation} Let us continue the discussion. --- (1) --- If the probability space if finite (= $(\ast)$ ), the assertion is easy to prove since every random vector takes only a finite number of values, so it suffices to prove the assertions for functions having the form $f = \mathbf 1_{A\times B}$ where $A,B\in \mathfrak B(\mathbb R^n)$ are Borel sets. --- (2) --- This can be extended to the case where $\Omega$ is countable (= $(\ast)$ ), by approximation of $f$ with simple functions being linear combinations of the above-mentioned type. But the argument is based upon the assumption that the union of $|\Omega|$ many null sets is still a null set. --- (3) --- It is rather easy to see that the assertion holds for general probability spaces if $Y$ and $\mathfrak B$ are independent (= $(\ast)$ ). --- (4) --- Whence the question : Is (A) true in general, i.e. $(\ast)$ being void? I suppose no, but I did not find a convenient counterexample. Do you know one? It would be great to obtain a useful answer to this.","['conditional-expectation', 'independence', 'probability-theory']"
3451784,Vector derivative in a unitary normal vector dependent function,"I am struggling in finding the paratial vector derivative of a function of a unitary normal vector of the form $E=\bf{\hat{n}}\cdot\bf{\hat{v}} = \hat{n}^T\hat{v}$ . Lets say we have $\bf{r_1}$ , $\bf{r_2}$ and $\bf{r_3}$ column-vectors and $\bf{\hat{n}} = \frac{(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})}{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|}$ . $\bf{\hat{v}}$ is other unitary column  vector that is not relevant until the end. The partial derivative with respecto $\bf{r_1}$ would be given by $\partial_\bf{r_1} = \partial_\bf{r_1}\bf{\hat{n}}\bf{\hat{v}}$ ,
where $\partial_\bf{r_1}\bf{\hat{n}} = \frac{1}{{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|}}\partial_\bf{r_1}(\bf{r_3 - r_1})\times(\bf{r_2 - r_1}) + (\bf{r_3 - r_1})\times(\bf{r_2 - r_1})\frac{\partial_\bf{r_1}}{{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|}}$ . By consideringthe skew-symmetric matrix $\bf{a} \times \bf{b}  = [\bf{a}]_xb = [\bf{b}]_xa$ ,and then, if $\bf{a = a(r_1)}$ and $\bf{b = b(r_1)}$ , $\bf\partial_{r1} = [a]_x(\partial_{r_1}b) - [b]_x(\partial_{r_1}a)$ . Consequently, $\partial_\bf{r_1}(\bf{r_3 - r_1})\times(\bf{r_2 - r_1}) = [r_3 - r_2]_x$ . My problem comes later, when finding the derivative in the modulus, $\frac{\partial_\bf{r_1}}{{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|}} = - \frac{(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})}{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|^3}[\bf{r_3-r_2}]_x = \frac{(\bf{r_3 - r_2})\times[(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})]^T}{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|^3}$ where in the last equality I have transposed the numerator to convert the skew to a cross product. The transpose in the second cross produdct is kept for clarity. My main problem is in the evaluation $(\bf{r_3 - r_1})\times(\bf{r_2 - r_1})\frac{\partial_\bf{r_1}}{{|(\bf{r_3 - r_1})\times (\bf{r_2 - r_1})|}}$ . I think that the transposition leads to a row vector, such that the previous product results in a matrix. Without transposition the dimensions does not agree. I find this confusing. Finally the multiplication by $\bf\hat{v}$ leads to a vector as it is expected from the vector derivative of a scalar. Can someone clarify me the problem or show me where are my mistakes? Thank you so much.","['vectors', 'multivariable-calculus', 'matrix-calculus', 'vector-analysis', 'differential-geometry']"
3451789,Is there a notion of similarity for shapes on the sphere?,"There are similar shapes in the plane, such as similar triangles. So is there a similar shape on the sphere? For example, is a great circle similar to a small circle on a sphere? I think great circles and small circles are similar, so there are similar shapes on the sphere. So as shown in the figure, are there similar two sides on the sphere? Here is my opinion: Can two rectangles of different sizes be similar on a plane? Obviously, rectangles can be similar. Under what conditions are they similar? The proportion of their corresponding sides is equal; Their corresponding angles are equal. These two conditions must be met at the same time. Squares are also similar because they meet both conditions 1 and 2. The conditions for the similarity of rectangle and triangle are not exactly the same. Although, we can think of each rectangle as consisting of two triangles. In the plane, two regular polygons with the same number of sides are similar because they meet the requirements of conditions 1 and 2 at the same time. We can think that any two circles are regular polygons with the same number of sides, so any two circles are similar. Because they meet both conditions 1 and 2. (their corresponding angles are all π). If we take two points on each circle, we get some shapes with two sides. Can these shapes be similar? Obviously they can be similar. As long as they meet the requirements of condition 1 (their corresponding angles are π). Can two shapes formed by closed curves be similar? I think they can be similar, and the conditions of judgment are similar, but the judgment is more complicated. Are two circles similar on a sphere? Obviously they are similar. Because they meet both conditions 1 and 2. On a sphere, are the two shapes shown in the figure similar (they are composed of arcs)? Obviously they can be similar as long as they meet the requirements of conditions 1 and 2 at the same time. All is exploration. I'm not sure what I said is right. I hope you can talk about your knowledge.",['geometry']
3451797,Bernoulli numbers tend to infinity,"Recall that the Laurent series of $\displaystyle\frac{1}{e^z-1}$ near $z=0$ is given as $$\frac{1}{e^z-1}=\frac1z-\frac12+\sum_{k=1}^\infty \frac{(-1)^{k+1}}{(2k)!}B_k z^{2k-1},$$ where the $B_k$ are the Bernoulli numbers. (This definition of the Bernoulli numbers is slightly different with that from the Wikipedia, but this definition is just the nonzero terms with all positive sign.) I want to prove that $\displaystyle\lim_{k \to \infty} B_k = \infty$ , but I have no idea. How do I have to prove these kinds of statements?","['complex-analysis', 'bernoulli-numbers', 'analysis']"
3451815,uniform convergent,"Given f is a differentiable function. Define $$f_n(x)=n\left(f\left(x+\frac{1}{n}\right)-f(x)\right)$$ , prove that $f_n$ is uniformly converge to $f'$ I have tried to make these equations: $n(f(x+\frac{1}{n})-f(x))=\frac{f(x+\frac{1}{n})-f(x)}{1/n}$ and taking limits as n $\to\infty$ but i got stuck to prove its uniform convergent","['functions', 'analysis', 'real-analysis']"
3451820,Gradient: field of tangent vectors vs. normal to surface at a point,"One definition of the gradient say that its a field of tangent vectors to a surface.  The gradient takes a scalar field f(x,y) (aka. a function), and produces a vector field $\vec{v}(x,y)$ , where the vector at each point of the field points in the the direction of greatest increase. $$\vec{v}(x,y) = \overbrace{\nabla \underbrace{f(x,y)}_\text{scalar field}}^{\text{vector field}} = \overbrace{\begin{bmatrix}\frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{bmatrix}}^{\text{vector field}}$$ Another definition of gradient say that its a normal to a surface of the form $F(x,y,z)=c$ . How to know when to apply which definition of gradient?  How is a field of tangent vectors related to the normal of a surface?  They seem like contradictory definitions. Let $\vec{r} = x \hat{\text{i}} + y \hat{\text{j}} + z \hat{\text{k}}$ be the position vector to any point P(x,y,z) on the surface $\phi(x,y,z)=c$ .  Then: $d\vec{r} = dx~\hat{\text{i}} + dy~\hat{\text{j}} + dz~\hat{\text{k}}$ lies in the tangent plane to the surface at P. $\phi(x,y,z)=c$ Taking differential of both sides: $$d\phi = \frac{\partial \phi}{\partial x} dx + \frac{\partial \phi}{\partial y} dy + \frac{\partial \phi}{\partial z} dz = 0$$ Therefore: $$\bigg(\frac{\partial \phi}{\partial x} \hat{\text{i}} + \frac{\phi}{\partial x}\hat{\text{j}} +  \frac{\phi}{\partial x}\hat{\text{k}}\bigg)  \cdot \bigg(dx\hat{\text{i}} +dy\hat{\text{j}} + dz\hat{\text{k} }\bigg) =0$$ $$\nabla \phi \cdot d\vec{r} = 0$$ Therefore $\nabla \phi$ is perpendicular to $d\vec{r}$ or normal to the surface at point P.","['multivariable-calculus', 'calculus']"
3451854,Defining a function at some given point to make it continuous,"So I have a function $$ f(x,y) = \frac{xy(x^2-y^2)}{x^2+y^2} $$ I have to define the function at $(0,0)$ so that it is continuous at origin. Is there any general approach we follow in this type of problems?  I am not sure but I think Sandwich Theorem has to be applied and some suitable function has to be taken .",['multivariable-calculus']
3451866,Does there exist a sequence of continuous functions converging to $f$?,"Let $g:\mathbb{N}\to\mathbb{Q}$ be a bijection. Let $f:\mathbb{R}\to\mathbb{R}$ be defined in the following way: $$f(x)=\begin{cases}
1/g^{-1}(x) & x\in\mathbb{Q}\\
0 &x\not\in\mathbb{Q}
\end{cases}$$ I have shown that $f$ is continuous at each $x\not\in\mathbb{Q}$ and not continuous at each $x\in\mathbb{Q}$ . However, I am wondering if there is a sequence $\{f_n\}$ of continuous functions which converges pointwise to $f$ . I would imagine that if there was, it would be similar to the construction in this question , concerning Thomae's function, but I cannot make that work because Thomae's function is constructed differently. A hint would be appreciated.","['sequences-and-series', 'real-analysis']"
3451876,length of curves for two conformally equivalent metrics,"I am learned about conformal metrics on surfaces and I have a question on how the length of a curve changes. Let $(M,g)$ be a Riemannian manifold of dimension 2 with $g$ being the Riemannian metric. Let $\tilde g=\rho g$ be a Riemannian metric conformally equivalent to $g$ . Here $\rho\in C^\infty(M)$ . Let $\mu_{g}$ and $\mu_{\tilde g}$ the measures with respect to $g$ and $\tilde g$ . Let $\gamma:[a,b]\rightarrow M$ be a geodesic with respect to the metric $\tilde g$ . Then its length is $L_{\tilde g}(\gamma)=\int_a^b \tilde g(\gamma', \gamma'(t))^{1/2} dt$ . I am reading in an article that the length of $\gamma$ with respect to $g$ is its $\tilde g$ -length multiplied by $\int_\gamma \rho^{-1/2} d\mu_{g}$ , i.e. $L_{g}(\gamma)=L_{\tilde g}(\gamma) \int_\gamma \rho^{-1/2} d\mu_{g}$ . I tried to compute it but I am stuck. I have $L_{g}(\gamma)=\int_a^b g(\gamma'(t), \gamma'(t))^{1/2} dt = \int_a^b (\rho(\gamma(t))^{-1/2} (\tilde g(\gamma'(t), \gamma'(t))^{1/2} dt$ . How can I ""separate"" the two integrand functions and keep the equality sign? The only thing that comes to my mind is to use the Cauchy-Schwartz inequality, but it brings me nowhere. Any help?","['riemann-surfaces', 'conformal-geometry', 'riemannian-geometry', 'differential-geometry']"
3451882,Prove that $\sum_{k=0}^{n} 2^{k} {n \choose k}{n-k \choose \lfloor{\frac{n-k}{2}}\rfloor} = {2n+1 \choose n}$,"The problem is the following: prove that $\sum_{k=0}^{n} 2^{k} {n \choose k}{n-k \choose \lfloor{\frac{n-k}{2}}\rfloor} = {2n+1 \choose n}$ I was trying to prove some concrete combinatorial examples to make the LHS equivalent to $2n+1 \choose n$ . We choose $k$ in the first $n$ items, but I am not sure how to map $2^{k} {n-k \choose \lfloor{\frac{n-k}{2}}\rfloor}$ to choosing $n-k$ elements from $n+1$ items [EDIT] This might help. Combinatorial proof $\sum_i^{\lfloor{n/2}\rfloor} (-1)^i {n-i\choose i} 2^{n-2i} = n+1$","['combinatorics', 'combinatorial-proofs']"
3451918,Does this version of law of large numbers holds true?,"Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space, $(\mathcal{X},\mathcal{F}_{\mathcal{X}})$ be a measurable space and $X,X_1,...,X_n,...$ be a sequence of $\mathbb{P}$ -i.i.d. random variables from $(\Omega,\mathcal{F})$ into $(\mathcal{X},\mathcal{F}_{\mathcal{X}})$ . If $A\in\mathcal{F}_{\mathcal{X}}$ then we know from the law of large numbers that $$\exists F\in\mathcal{F}, \left(\mathbb{P}(F)=0\right)\land\left(\forall\omega\in F^c, \mathbb{P}(X\in A)=\lim_{n\to+\infty}\frac{1}{n}\sum_{k=1}^n\chi_A(X_k(\omega))\right).$$ Then, by Egoroff theorem, we also know that $$\forall\varepsilon>0, \exists F\in\mathcal{F}, \left(\mathbb{P}(F)<\varepsilon\right)\land\left(\lim_{n\to+\infty}\left\|\mathbb{P}(X\in A)-\frac{1}{n}\sum_{k=1}^n\chi_A(X_k)\right\|_{L^\infty(F^c)}=0\right).$$ So $$\forall A\in\mathcal{F}_{\mathcal{X}}, \forall\varepsilon>0, \exists F\in\mathcal{F},\\ \left(\mathbb{P}(F)<\varepsilon\right)\land\left(\lim_{n\to+\infty}\left\|\mathbb{P}(X\in A)-\frac{1}{n}\sum_{k=1}^n\chi_A(X_k)\right\|_{L^\infty(F^c)}=0\right).$$ I'm wondering if this result holds uniformly in $A$ , i.e. is it true that $$\forall\varepsilon>0, \exists F\in\mathcal{F},\\ \left(\mathbb{P}(F)<\varepsilon\right)\land\left(\forall A\in\mathcal{F}_{\mathcal{X}},\lim_{n\to+\infty}\left\|\mathbb{P}(X\in A)-\frac{1}{n}\sum_{k=1}^n\chi_A(X_k)\right\|_{L^\infty(F^c)}=0\right)?$$","['law-of-large-numbers', 'probability-distributions', 'probability']"
3451968,"If the operator $T$ is defined by $Tf(x)=\int_0^xf(t)\,dt$, show that $Tf \in C[0,1]$","Consider the operator $T$ on $L^2[0,1]$ defined by $Tf(x)=\displaystyle \int_0^xf(t)\,dt.$ Show that $Tf \in C[0,1].$ I have one question before this: What are the implications between $L^p$ spaces, i.e if $f \in L^p$ does this imply that $f \in L^{p+1}$ ? My attempt: Assume $Tf$ is not in $C[0,1]$ , so there exists $y \in [0,1]$ and $\epsilon>0$ such that for all $\delta >0$ , we can find $x_0$ , with $|x-x_0|<\delta$ but $$\bigg\rVert\int_0^x f-\int_0^{x_0} f\,\bigg\rVert>\epsilon$$ WLOG assume $x>x_0$ , so $$\bigg\lVert \int_{x_0}^x f\,\bigg\rVert \geq \epsilon.$$ So for $\delta_n=\frac{1}{n},$ we can find $x_n \in [0,1]$ such that $|x-x_0|<\frac{1}{n}$ and $$\bigg\lVert\int_{x_n}^xf\,\bigg\rVert \geq \epsilon$$ I don't know if that will lead me to a contradiction. I would appreciate any help or hints with that.","['measure-theory', 'operator-theory', 'functional-analysis', 'real-analysis']"
3451974,Dominated convergence theorem for two integrals involving sine,"I am stuck on some other problems in introductory measure theory on the convergence theorems (monotone convergence theorem and dominated convergence theorem). The exercise asks to compute the limit as $n\to\infty$ of the following integrals. $$(1)\quad\int_{(0,\infty)}\frac{\sin x}{x^2}\frac{x^{1/n}}{1+x^{1/n}}\,dx$$ $$(2)\quad \hspace{13pt}\int_{(0,\infty)} \frac{\sin (nx^n)}{nx^{n+\frac12}}\,dx$$ ( $1$ ) The monotone convergence theorem clearly doesn't apply since $\sin x$ changes sign on $(0,\infty)$ . My hope goes to the dominated convergence theorem. For all $x\in(0,\infty)$ , $\frac{x^{1/n}}{1+x^{1/n}}=\frac{1}{1+\frac{1}{x^{1/n}}}\xrightarrow{n\to\infty} \frac12$ , so by positivity it is bounded by $\frac12$ for all $x\in(0,\infty)$ and all $n$ . If I cut the integral into $\int_{0}^1$ and $\int_1^\infty$ , then the second part is easy. Indeed, for all $n$ and for all $x\in [1,\infty)$ , by the above remark $f_n(x)\xrightarrow{n\to\infty} \frac{\sin x}{2x^2}$ ; furthermore $|f_n(x)|\leqslant \frac{1}{2x^2}$ which is integrable, so this part follows by the dominated convergence theorem. However, I am stuck at what to do with $\int_0^1 f_n(x)\,dx$ , as I seem to keep the $x$ in the denominator which avoids integrability.
I tried to use the inequality $\left\vert \frac{x^{1/n}}{1+x^{1/n}} \right\vert\leqslant \frac{1}{1+x}$ , but for the $\frac{\sin x}{x^2}$ term, the only inequality we can use is $\sin x\leqslant x$ and so we will always keep $x$ in the denominator. ( $2$ ) Here I have the same problem, $|\sin (nx^n)|\leqslant nx^{n}$ for all $x\in (0,\infty)$ and all $n$ , but then $\left\vert \frac{\sin (nx^n)}{nx^{n+\frac12}} \right\vert\leqslant \frac{nx^n}{nx^{n+\frac12}}=\frac{1}{x^{1/2}}$ , from where we have nowhere to go. I hope there is some slick trick I don't know about. Any help is welcome.","['integration', 'measure-theory', 'definite-integrals', 'real-analysis']"
3451994,Ito Derivative of White Noise,"We know that white noise $w_{t}$ is given by the time derivative of Brownian motion $\beta_{t}$ , ie that: $$ w_{t} = \frac{d \beta_{t}}{dt} $$ Now I want to define a new process, called blue noise $b_{t}$ , and define it as: $$ b_{t} = \frac{d w_{t}}{dt} = \frac{d^{2} \beta_{t}}{dt^{2}} $$ Does this make sense? How can I deduce what properties it has? Ie -- its mean and variance? I take the name 'blue noise' as inspired from signal processing theory. We know that the derivative of some signals $f(t)$ can be found via: $$ \frac{d f(t)}{dt} = \mathcal{L}^{-1}(s F(s)) $$ where $\mathcal{L}(\cdot)$ is the Laplace transform, and $F(s)$ is the Laplace transform of $f(t)$ . Can I use this to define the properties of this stochastic process? This problem is arising from trying to determine the differential equation corresponding to the following system driven by white noise input: $$ H(s) = \frac{s + \gamma}{s^{2} + 2 \alpha s + \gamma^{2}} $$ If $W(s)$ is the Laplace transform of the white noise input, then we have: $$ Y(s) = W(s)H(s) $$ $$ s^{2}Y(s) + 2 \alpha s Y(s) + \gamma^{2}Y(s) = sW(s) + \gamma W(s) $$ $$ y^{''}_{t} + 2 \alpha y^{'}_{t} + \gamma^{2} y_{t} = w^{'}_{t} + \gamma w_{t} $$ where $w^{'}_{t}$ is this blue noise process I am talking about. I am trying to make sense of this thing. Can I analyze this somehow in the Ito sense? I want to be able to put it into an SDE and do some analysis -- ie solve it or find the moments of the resulting equation etc. I'm just a lowly engineer so please keep that in mind haha Thanks for your help and comments!","['ordinary-differential-equations', 'noise', 'laplace-transform', 'stochastic-processes', 'stochastic-differential-equations']"
3452031,Functional equation $f(f(x)y+x)=f(x)f(y)+f(x)$,"Find all functions $ f:\mathbb{Z} \to \mathbb{Z}$ such that for all $x, y \in \mathbb{Z}$ : i) if $x \ne y$ then $f(x) \ne f(y)$ ; ii) $f(f(x)y+x)=f(x)f(y)+f(x)$ . My work . 
It is easy to see that a function $f(x)=kx$ , where $k=const$ ( $k \in \mathbb{Z}\backslash \{0\}$ ), is a solution of the problem. But is this the only solution? It is easy to see that $f(0)=0$ .","['contest-math', 'functional-equations', 'functions', 'integers']"
3452065,"extracting finite value from a non convergent integral, where am I wrong?","This following integral is not convergent $$
\int_0^\infty dx \, x^{ia} e^{i\omega x}
$$ but I know for example that $$
\int_0^\infty dx \, x^c e^{-b x} = b^{-1-c}\,  \Gamma(1+c)
$$ where $\Gamma$ is the Euler Gamma function. Therefore calling $c=ia$ and $i\omega = -b$ one gets $$
\int_0^\infty dx \, x^{ia} e^{i\omega x} = (-i \omega)^{1-ia} \Gamma(1+ia) = e^{-i \pi/2} \omega^{1-ia} \, \Gamma(1+ia)
$$ which looks finite to me, so can you tell me where I've made a mistake or an abuse?","['integration', 'improper-integrals', 'analysis', 'gamma-function', 'calculus']"
3452077,A new method to solve a SAS triangle: $\tan B =\frac{AC \sin C}{BC- AC \cos C}$. Should I publish?,"I've discovered a new method to solve SAS triangles without using the law of cosines: In $\triangle ABC$ , if sides $AC$ and $BC$ , and angle $C$ , are known, then: $$\tan B =\frac{AC \sin C}{BC- AC \cos C}$$ Putting in mind that I'm still a preparatory school student, which means that this is a great achievement for me, should I write a paper about this? or would it be a waste of time?",['trigonometry']
3452152,"Is $(0,0)$ a saddle point for the given function?","I need to find the critical points for the function $f(x) = 3(x^2 + y^2) - 2(x^3 - y^3) + 6xy$ and also test whether they are maxima/minima/saddle point. Now the only critical point is (0,0) however at (0,0) $rt - s^2  =0$ then second derivative test fails, If I take the line $y = -x$ then $f(x, -x) = -4x^3$ then clearly for along the neighborhood of $(0,0)$ $f$ has both positive and negative values . Hence , $(0,0)$ is a  saddle point . Is my solution and answer correct ? Can someone please verify ? Thank you.","['maxima-minima', 'multivariable-calculus']"
3452153,Derivative of Contour Integral Representation of Step Function,"The following is Problem 11.9 in ""Mathematical Physics: A Modern Introduction to Its Foundations, Second Edition"" by Sadri Hassani. Given the following representation of the step function: $$\theta(x) = \lim_{\epsilon\to 0}\frac{1}{2\pi i} \int_{-\infty}^\infty \frac{e^{i t x}}{t-i\epsilon} \, dt, $$ show that $\theta'(x) = \delta(x)$ . This was a homework problem for an undergraduate physics class, but I (the grad student TA) am having trouble solving it rigorously. The derivative is clearly $$\theta'(x) = \frac{1}{2\pi} \int_{-\infty}^\infty \frac{t e^{itx}}{t-i\epsilon} \, dt,$$ which is essentially the usual Fourier representation of the Dirac delta function $$\delta(x) = \frac{1}{2\pi} \int_{-\infty}^\infty e^{itx} \, dt.$$ However, the textbook has not introduced the Fourier transform yet, so this problem should be doable without using this representation. It is easy to show that $\theta'(x) = 0$ for $x>0$ and $x<0$ by coverting the integral over $t$ into a contour integral with a semicircular portion in the UHP for $x>0$ and LHP in the $x<0$ . One can also see that $\theta'(0)$ is infinite. Lastly, one can show that $\theta'(x)$ integrates to 1 over any region containing the origin. This is the answer that I will accept from the students. However, I would like to have a more rigorous derivation involving a test function. Here is my attempt. Let $$\theta_\epsilon(x) = \frac{1}{2\pi i} \int_{-\infty}^\infty \frac{e^{i t x}}{t-i\epsilon} \, dt $$ We wish to show $\displaystyle{\lim_{\epsilon\to 0} \theta_{\epsilon}'(x) = \delta(x)}$ . The derivative is: $$\theta_\epsilon'(x) = \frac{1}{2\pi i} \int_{-\infty}^\infty \frac{it e^{itx}}{t-i\epsilon} \, dt $$ Let $g(x) \in \mathcal{S}(\mathbb{R})$ be a smooth test function (Schwartz function). We wish to show that $$\lim_{\epsilon \to 0} A_\epsilon = g(0),$$ where $$ A_\epsilon \equiv \int_{-\infty}^\infty \theta_\epsilon'(x) g(x) \, dx.$$ We integrate and convert one of the integrals into a contour integral: \begin{align*}
A_\epsilon &= \frac{1}{2\pi} \int_{-\infty}^\infty \int_{-\infty}^\infty \frac{g(x) t e^{itx}}{t-i\epsilon} \, dt \, dx \\
& = \frac{1}{2\pi} \int_{-\infty}^\infty \underbrace{\int_{R} \frac{g(x)z e^{ixz}}{z-i\epsilon} \ dz}_{I_R} \, dx
\end{align*} where $R$ is the contour of real numbers in $\mathbb{C}$ . By Jordan's lemma, the contour integral $I_R$ is: $$I_R = \left\{\begin{matrix} g(x) \cdot 2\pi i \cdot  i\epsilon e^{-\epsilon x} & x>0 \\ 0 & x<0 \end{matrix}\right.,$$ where we have closed the contour in the UHP or LHP for $x>0$ and $x<0$ , respectively.
Thus: $$A_\epsilon = -\epsilon \int_0^\infty g(x) e^{-\epsilon x} \ dx $$ Integrating by parts: \begin{align*}
A_\epsilon 
&= -\epsilon \left[ -\frac{1}{\epsilon} e^{-\epsilon x} g(x)\Big|_0^\infty 
+ \frac{1}{\epsilon} \int_{0}^\infty e^{-\epsilon x} g'(x) dx \right] \\
& = -g(0) - \int_0^\infty e^{-\epsilon x} g'(x) \ dx \\
\end{align*} where we used the fact that $g\to 0$ as $x \to\infty$ . Examining the second term, we see that since $e^{-\epsilon x} g'(x)$ is dominated by $g'(x)$ on $[0,\infty)$ , we can use the Dominated Convergence Theorem to bring the $\epsilon \to 0$ limit inside the integral and find $$\lim_{\epsilon \to 0} A_\epsilon
= -g(0) - \int_0^\infty g'(x) \, dx
= -g(0) - g(x)\Big\vert_0^\infty = g(0) - g(0) = 0.$$ This is clearly not the desired result. I don't see anything wrong with the manipulations after the contour integral, so that step must be wrong. I guess splitting up the integral for $x>0$ and $x<0$ somehow misses the $x=0$ piece, which at the end of the day is the only piece that matters? Any ideas would be appreciated.","['complex-analysis', 'complex-integration', 'step-function', 'dirac-delta']"
3452200,Fun functional equation in a group: $af(ab)b=bf(ba)a$,"Suppose we have a finite group $G$ and a function $f:G\to G$ that satisfies the following property for all $a,b\in G$ : $$af(ab)b=bf(ba)a$$ Under what conditions (or, for what groups $G$ ) can such a function $f$ exist? When such a function exists, what can we say about how many such functions there are? Are there any other interesting properties of $f$ implied by the above functional equation? After playing around with this for a while, I noticed that If $G$ is abelian, then all functions $f:G\to G$ trivially satisfy this property For all $a\in G$ , $a^2$ commutes with $f(e)$ For all $a\in G$ , $a$ commutes with $f(a)$ For all $z\in Z(G)$ (the center of $G$ ) and all $a\in G$ , $a$ commutes with $f(az)$","['functional-equations', 'group-theory', 'functions', 'finite-groups']"
3452201,How many essentially distinct hands of size $n$ can be dealt from a standard deck?,"In many card games, suits are interchangeable, so that one would be indifferent between the (4-card) hands: $5\diamondsuit \ 6\spadesuit \ 7\heartsuit \ J\heartsuit$ and $5\spadesuit \ 6\diamondsuit \ 7\clubsuit \ J\clubsuit$ . I'm interested in the number of $n$ -card hands which are essentially different , that is, They are distinct under every possible relabeling of the suits. Put another way, two hands are essentially the same if there is some relabeling of the suits that makes them identical. Specifically, if it matters, I am most interested in the case $n=6$ . Lots of people have done similar calculations for $n=5$ , due to the poker connection, but their analyses are always poker-centric. In particular, every general explication of $n$ -card hands I've found assumes that one is going to make their best 5-card hand. I haven't been able to find any reference to this specific problem, and I'd really like to avoid having to do it from scratch. Searching the broader internet and M.SE both turn up lots of specific homework-type problems looking for the number of one specific type of hand. Obviously one could enumerate all distinct 6-card hands ignoring suits and then count the ways to add suit-shapes to them; I'd really like to avoid that. Surely there's either a quick way to answer the question, or an answer out there somewhere already?","['combinatorics', 'card-games']"
3452207,Using Chebychev's inequality to prove...,"The number $C_n$ of disjoint cycles in a uniformly chosen random permutation of $\{1, 2, 3, \ldots , n\}$ has the representation $C_n = \sum_{k=1}^n X_k$ where $X_1, X_2, \ldots, X_n$ are independent random variables satisfying $P(X_k = 1) = 1 − P(X_k = 0) = 1/k $ . Use Chebyshev’s inequality to prove that, for any $\varepsilon > 0$ , $P(|(C_n\log(n)) -1| \ge \varepsilon ) \to 0$ as $n → ∞$ I am unsure where even to begin this, however I am aware that the Euler–Mascheroni constant may be of some use here. Any help appreciated!","['statistics', 'probability']"
3452209,Primes $p$ and $q$ such that $3p^{q-1}+1$ divides $11^p+17^p$,"From the Balkan Mathematics Olympiad 2018: Find all prime numbers $p$ and $q$ such that $3p^{q-1}+1$ divides $11^p+17^p$ . I started using Fermat's Little Theorem, which states that $a^{p-1}-1$ is divisible by $p$ if $p$ is a prime. But I could not find any way to solve the problem. Any suggestions, please.","['contest-math', 'number-theory', 'prime-numbers']"
3452216,"Show function is continuous at $(0,0)$","Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ be a function with $$\space f(x, y) = \begin{equation}
\begin{cases}
\dfrac{y^2\log(1+x^2y^2)}{\sqrt{x^4+y^4}}, & (x, y) \neq (0, 0)\\
0, & (x, y) = 0 
\end{cases}\end{equation}$$ Show that f is continious.
I've already shown that f is continous for $(x, y) \neq (0,0)$ but I am having trouble with finding a proof for $(0, 0)$ using epsilon-delta or limits of sequences. Can anyone help? 
This is how far I got with simplifying for epsilon delta $$
\Bigg{|}\frac{y^2\log(1+x^2y^2)}{\sqrt{x^4+y^4}}\Bigg{|} \\ 
\Leftrightarrow \Bigg{|}\frac{y^2\log(1+x^2y^2)}{\sqrt{y^4*(\frac{x^4}{y^4}+1)}}\Bigg{|} \\
\Leftrightarrow \Bigg{|}\frac{y^2\log(1+x^2y^2)}{y^2\sqrt{(\frac{x^4}{y^4}+1)}}\Bigg{|} \\
\Leftrightarrow \Bigg{|}\frac{\log(1+x^2y^2)}{\sqrt{(\frac{x^4}{y^4}+1)}}\Bigg{|} \\
\leq \big{|}\log(1+x^2y^2)\big{|}
$$ But now I am clueless on how to connect $$
\big{|}log(1+x^2y^2)\big{|} < \epsilon \\
$$ and $$
\sqrt{x^2+y^2} < \delta
$$","['limits', 'multivariable-calculus', 'real-analysis']"
3452290,is differential geometry really required to understand anything in algebraic geometry?,"I heard a few rumours that knowledge of differential geometry is crucial to really understand anything in algebraic geometry. I want to ask You, people I don't know IRL to give Your opinion. If You know also about Lie's groups and algebras, You can write also. I know that commutative algebra and sth my faculty calls ""algebraic methods of geometry and topology"" are necessary, but what about DG? Is it ""must have"" or is it only making learning algebraic geometry easier? And why it is like that? I don't know anything about both of these geometries at that, moment, I'm asking in case to plan 2020/2021 season of my studies.","['advice', 'algebraic-geometry', 'education', 'differential-geometry']"
3452338,Index of a subgroup of the Modular Group,"The subgroup of $SL(2,\mathbb{Z})$ generated by $\begin{pmatrix}1&0\\1&1\end{pmatrix}$ and $\begin{pmatrix}1&5\\0&1\end{pmatrix}$ has come up in a research question in string theory, and I am interested in determining whether or not its index is infinite. I found the article ""Manipulating Subgroups of the Modular Group"" by Daniel Schultz in the Mathematica Journal, and it seems that the package referenced therein (ModularSubgroups) could answer this question for me. However, I have not been able to find where to download this package. Does anyone know if this particular subgroup has infinite index? More Details: I actually have the subgroup of $Sp(4,\mathbb{Z})$ generated by $\begin{pmatrix}1&0&0&0\\1&1&0&0\\0&-5&1&0\\0&0&-1&1\end{pmatrix}$ and $\begin{pmatrix}1&0&0&1\\0&1&0&1\\0&0&1&-5\\0&0&0&1\end{pmatrix}$ And I want to know if the action of this subgroup on the lattice $\mathbb{Z}^4$ has a finite or an infinite number of orbits. Since the third row of matrices in this subgroup is always $(0,0,1,0)$ mod 5, the answer to the modular group question will help me to guide my efforts toward solving the actual problem that I have.","['group-theory', 'modular-group', 'string-theory']"
3452421,Evaluate $\lim_{x\to 0}\bigg[1^{1/\sin^2x}+2^{1/\sin^2x}+....+n^{1/\sin^2x}\bigg]^{\sin^2x}$,"Solve that $$\lim_{x\to0}\bigg[1^{1/\sin^2x}+2^{1/\sin^2x}+....+n^{1/\sin^2x}\bigg]^{\sin^2x}$$ $$
\lim_{x\to 0}\bigg[1^{1/\sin^2x}+2^{1/\sin^2x}+....+n^{1/\sin^2x}\bigg]^{\sin^2x}=\lim_{x\to 0}\bigg[1+2^{1/\sin^2x}+....+n^{1/\sin^2x}\bigg]^{\sin^2x}\\
[1^t+1^t+....+1^t]^{1/t}=n^{1/t}\leq[1^t+2^t+....+n^t]^{1/t}\leq [n.n^t]^{1/t}=n.n^{1/t}
$$ Can I use squeeze theorem here or is there a better way ? Note: My reference gives the solution $n$","['limits', 'calculus', 'limits-without-lhopital']"
3452438,Is $f(f^{-1}(B))$ a Borel set?,"Let $f:\Bbb R \to \Bbb R$ be a Borel measurable function, i.e. $f^{-1}(B)$ is a Borel set for any Borel set $B$ . For any Borel set $B\subset \Bbb R$ , is it true that $f(f^{-1}(B))$ is a Borel set? I know that even a continuous image of a Borel set need not be Borel, i.e. $f(A)$ need not be Borel for a Borel set $A$ . However, does anything change if $A$ has a very special form of $A=f^{-1}(B)$ ? I have a feeling that $f(f^{-1}(B))$ need not be a Borel set but I can't think of a good counterexample yet.","['measure-theory', 'probability', 'real-analysis']"
3452448,Exercise of integration and sum [duplicate],"This question already has answers here : A curious integral (4 answers) Closed 4 years ago . Show that $$\int_0^\infty \frac{\sin x}{e^x-1}\,dx=\sum_{n=1}^\infty \frac{1}{n^2+1}.$$ Thoughts: I think I have to use the dominated convergence theorem, but I don't see how.. I tried expanding $\frac{1}{1-e^x}=1+e^x+e^{2x}+\ldots$ but then realised this works only $|e^x|<1$ .","['integration', 'calculus', 'sequences-and-series']"
3452455,Solving Log Equation with different bases,"I solved this log equation with different bases and I got $x={0, 3}$ . Here's my solution: $$\log_3 x = \log_9 3x$$ $$\log_3 x=\frac {\log_3 3x}{\log_3 9}$$ $$\log_3 x=\frac {1}{2}\log_3 3x$$ $$x=\sqrt {3x}$$ $$x^2=3x$$ $$x^2-3x=0$$ $$x(x-3)=0$$ $$x=0,3$$ I checked my answer on an online calculator (symbolab), but it only says that $x=3$ . I don't understand why.","['algebra-precalculus', 'logarithms']"
3452519,"What makes the hairy ball theorem ""hard""?","I am currently taking a course on differential geometry, and have been using Spivak as a reference. On page 69 of ""A Comprehensive Introduction to Differential Geometry, Vol. I"" Spivak says: ""It is a well-known (hard) theorem of topology that this is impossible (you can't comb the hair on a sphere)."" Obviously Spivak is referencing the hairy ball theorem here. The professor for the course also introduced the hairy ball theorem referring to it as a ""hard"" theorem. So, what is meant by ""hard"" in this context? It doesn't seem to mean hard to understand or prove, since I can think of other results even in the same text that are difficult to understand, yet Spivak doesn't refer to them as ""hard"". Thanks for your time.",['differential-geometry']
3452557,Self-duality of Koszul resolution,"Let $(A,m)$ be a commutative local noetherian ring, and suppose that $(x_1,...,x_\ell)$ is a regular sequence inside the maximal ideal $m$ . To this regular sequence, you can associate a canonical finite resolution of $A/(x_1,...,x_\ell)$ by free $A$ -modules, called the Koszul resolution: $$ K(\mathbf{x}):0 \to \bigwedge^\ell A^\ell \overset{d_\ell} \to \bigwedge^{\ell-1} A^\ell \to \cdots \to \bigwedge^1 A^\ell \overset{d_1}\to A \to 0$$ where the differential is $$d_k (e_1 \wedge \dots \wedge e_k) = \sum_{i=1}^k (-1)^{i+1} d_1(e_i) e_1 \wedge \cdots \wedge \widehat{e_i} \wedge \cdots \wedge e_k.$$ The Koszul resolution also admits a sort-of ""self-duality"" (see here ). That is, if one takes the complex above and applies $\operatorname{Hom}_A(-,A)$ , the resulting complex is quasi-isomorphic (up to a shift) to the original. In other words, in $\mathrm{D}(A)$ (the derived category of $A$ -modules), the Koszul complex $K(\mathbf{x)}$ satisfies $K(\mathbf{x})^\vee \cong K(\mathbf{x})[-\ell]$ . My question is the following: Does this ""self-duality"" characterize the Koszul resolution? Namely, suppose $I \subset A$ is an ideal such that $A/I$ has finite projective dimension over $A$ . If $Q \to A/I$ is the free resolution of this quotient, such that $Q^\vee \cong Q[-\ell]$ , can we conclude that $I$ is generated by a regular sequence of length $\ell$ and $Q$ is its associated Koszul resolution? I suspect that the answer to the above is no, so my follow up question in that case is if there are any other ""categorical"" characterizations of when a free resolution is the Koszul resolution of some regular sequence.","['homological-algebra', 'algebraic-geometry', 'commutative-algebra', 'reference-request']"
3452559,Matrix exponentiation of a Kronecker product of Pauli matrices,"I need to numerically compute the matrix-exponential of a Kronecker product of Pauli matrices (including the identity). For example, $$
\exp( X \otimes Y \otimes I \otimes Z \;\otimes \;... )
$$ or generally $$
\exp \bigotimes\limits_j \sigma_j, \; \; \text{where} \;\; \sigma_j \in \{I,X,Y,Z\}.
$$ I can construct the Pauli product easily enough. However, I want to avoid implementing a numerical routine for exponentiating general (or just square) complex matrices, since the Hermitian & unitary matrix resulting from Pauli products is very particular. For example, it's clear that the resulting matrix will only contain the elements $\{ \pm 1, \pm i, 0 \}$ . Surely this begs an analytic form, or at least a significantly simplified numerical routine, for computing the matrix exponential!","['matrices', 'unitary-matrices', 'matrix-exponential', 'kronecker-product']"
3452613,Learning roadmap for Complex Geometry,"I am still an undergraduate and I have taken courses like complex analysis and differential geometry. Also, I learnt myself manifold theory. (the book by Loring Tu). Currently, I am quite interested in complex geometry, where I guess it is the intersection between complex analysis and geometry. But what is the roadmap for me to reach that research field? What particular mathematics is required? Do I need to learn PDE?","['complex-geometry', 'several-complex-variables', 'complex-analysis', 'manifolds', 'soft-question']"
3452660,geodesics in a Liouville's surface,"A surface $S$ is a Liouville's surface if its coefficients of the first fundamental form satistifes $E=G=U+V$ and $F=0,$ where $U=U(u)$ and $V=V(v).$ I need to prove that the geodesics in a Liouville's surfaces  can be obtained by primitivation of the form $$\int \frac{\textrm{d}u}{\sqrt{U(u)-c}}=\pm\int\frac{\textrm{d}v}{\sqrt{V(v)+c}}. $$ My first try is to use the parallelism equations for a geodesic curve. If $\gamma$ is a geodesic, $\gamma(t)=\mathbb{x}(u(t),v(t)),$ where $\mathbb{x}$ is a local parametrization of $S,$ so $$
\begin{cases}
\displaystyle u''+\Gamma_{11}^{1}(u')^{2}+2\Gamma_{12}^{1}u'v'+\Gamma_{22}^{1}(v')^{2}=0, \\
\displaystyle v''+\Gamma_{11}^{2}(u')^{2}+2\Gamma_{12}^{2}u'v'+\Gamma_{22}^{2}(v')^{2}=0.
\end{cases}
 $$ Computing the Christofell symbols, I find $$\Gamma_{11}^{1}=\frac{U_{u}}{2(U+V)},\;\Gamma_{11}^{2}=-\frac{V_{v}}{2(U+V)},\; \Gamma_{12}^{1}=\frac{V_v}{2(U+V)\;}, \\
\Gamma_{12}^{2}=-\frac{U_{u}}{2(U+V)},\;\Gamma_{22}^{1}=-\frac{U_{u}}{2(U+V)},\;\Gamma_{22}^{2}=\frac{V_v}{2(U+V)}.
$$ So, applying this to the ODE, $$
\begin{cases}
\displaystyle u''+\frac{U_{u}(u)}{2(U(u)+V(v))}(u')^{2}+\frac{V_{v}(v)}{U(u)+V(v)}u'v'-\frac{U_{u}(u)}{2(U(u)+V(v))}(v')^{2}=0, \\
\displaystyle v''-\frac{V_{v}(v)}{2(U(u)+V(v))}(u')^{2}+\frac{U_{u}(u)}{U(u)+V(v)}u'v'+\frac{V_{v}(v)}{2(U(u)+V(v))}(v')^{2}=0.
\end{cases}
$$ But... well, I don't know if I can do something from here. I tried to manipulate the equations, and I got $$
\begin{cases}
2\left[u'\left(U(u)+V(v)\right)\right]'-\left((u')^{2}+(v')^{2}\right)U_{u}(u)=0 \\
2\left[v'\left(U(u)+V(v)\right)\right]'-\left((u')^{2}+(v')^{2}\right)V_{v}(v)=0
\end{cases}
 $$ Since, both equations equals to zero, one is equal to the other, so, manipulating, $$2\left[\left(U(u)+V(v)\right)\left(u'-v'\right)\right]'+\left((u')^{2}+(v')^{2}\right)\left(V_{v}(v)-U_{u}(u)\right)=0, $$ and now I don't know what to do.","['geodesic', 'surfaces', 'ordinary-differential-equations', 'differential-geometry']"
3452689,hyperbolic distance between two reals,"The hyperbolic distance between two points $z_1, z_2 \in D_1(0)$ is defined as \begin{equation}
d(z_1, z_2)= \inf_\gamma \int_0^1 \frac{|\gamma'(t)|} {1- |\gamma(t)|^2} dt \, ,
\end{equation} where the infimum is taken over all smooth curves $\gamma:[0,1] \to D_1(0)$ joining $z_1$ and $z_2$ . Now I want to prove that \begin{equation}
d(0,s) = \frac{1}{2} \log \left( \frac{1+s}{1-s}\right)
\end{equation} for $s \in [0,1)$ . I think I know where I have to go, since \begin{equation}
\int_0^s \frac{1}{1-x^2} dx = \frac{1}{2} \log \left( \frac{1+s}{1-s}\right) \,,
\end{equation} but I don't know how to get there. Any ideas?","['complex-analysis', 'complex-integration', 'hyperbolic-geometry']"
3452696,"Are the unconditionally convergent series, with terms in a Banach algebra, closed under the Cauchy product?","We have a Banach algebra $\mathbb L$ , and two sequences $(A_0,A_1,A_2,\cdots),\;(B_0,B_1,B_2,\cdots)\in\mathbb L^{\mathbb N}$ , for which the sums $\sum_{n\in\mathbb N}A_n,\;\sum_{n\in\mathbb N}B_n$ are unconditionally convergent . Is $$\sum_{n\in\mathbb N}\left(\sum_{l+m=n}A_lB_m\right)$$ also unconditionally convergent? If it helps, you may assume commutativity ( $A_lB_m=B_mA_l$ ), or that they're power series with scalar coefficients ( $A_n=a_nX^n,\;B_n=b_nX^n,\;X\in\mathbb L$ ). The case with absolute convergence is easy . (There, we just need to replace $|a_nb_k|=|a_n||b_k|$ with $|a_nb_k|\leq|a_n||b_k|$ .) Possibly related: Is the sequence space $\ell^p$ closed under the Cauchy product?","['cauchy-product', 'banach-algebras', 'reference-request', 'functional-analysis', 'convergence-divergence']"
3452707,"If $a_n \to z$, then does $\frac{{n \choose 1} a_1 + {n \choose 2} a_2 \dots {n \choose n} a_n}{2^n} \to z$?","It is well known that $\sum_{k=0}^n{n\choose k} =2^n$ . My question: If $z$ is the limit point of an infinite sequence of real numbers $\{ a_n \}$ , then does $$\frac{{n \choose 1} a_1 + {n \choose 2} a_2+ \cdots+ {n \choose n} a_n}{2^n}$$ converge to $z$ as $n\ \to \infty$ ?","['summation-method', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
3452744,Optimisation without calculus,"I am a tutor and a year 8 student of mine has been given a maths test question that I can not fathom how to solve and explain for the year level he is at. It asks about a piece of luggage that has the requirements of the 3 dimensions totalling no more than 270cm when added together. It then asks what is the maximum possible volume a suitcase could have without exceeding this requirement. Optimisation generally involves calculus which would be far beyond this grade. The teachers answer is Vmax length=width=depth which means 90x90x90=729,000cm^3 she offers no information though as to how these 3 numbers were selected to give the optimum volume. What am I missing?? Or am I overthinking it?","['optimization', 'geometry', 'volume']"
3452767,Proving $\frac1n\sum_{i=1}^na_i(B_i-\frac12)\stackrel{\text{a.s.}}\longrightarrow 0$ where $B_i$s are i.i.d Bernoulli$(\frac12)$,"Problem: Let $\{B_n\}$ be a sequence of independent, identically distributed Bernoulli random variables of parameter $p=\frac12$ . Let $\{a_n\}$ be a non-negative, non-random sequence such that $\sup\limits_{n\geq0}\frac1n\sum\limits_{i=1}^na_i^2<\infty$ and $\frac1n\sum\limits_{i=1}^{n}a_i$ converges. Prove that $\frac1n\sum\limits_{i=1}^na_i(B_i-\frac12)$ converges to zero almost surely. This is an exercise in the section of ""strong law of large numbers"". But I only managed to prove the case when $\{a_n\}$ is bounded. Any help is appreciated!!","['law-of-large-numbers', 'probability-theory', 'probability']"
3452776,The differentiability of absolute value of a function implies that the function is differentiable?,"I.e., if |f| is differentiable at a point $a$ , is $f$ differentiable at $a$ too? Obviously this will not work for a piecewise function $f$ whose absolute value is continuous, but I can't figure out the sufficient conditions for $f$ to be differentiable. It seems like continuity of $f$ at $a$ should force it to be differentiable at $a$ , but I don't see any algebraic manipulations in $\lim_{h\to0}\frac{f(a+h)-f(a)}{h}$ to make use of the continuity limit, that $\lim_{h\to 0}f(a+h)=f(a)$ .","['derivatives', 'analysis']"
3452793,Upper Bound on Radius of Cauchy Product of Power Series,"Let $\sum_{n = 0}^{\infty} a_nz^n$ and $\sum_{n = 0}^{\infty} b_nz^n$ be two power series, each with radius of convergence 1. How large can the radius of convergence of their Cauchy product, $\sum_{n = 0}^{\infty} c_nz^n$ be? (to be clear, $c_n = \sum_{k = 0}^{n} a_kb_{n - k}$ ). I know that the radius of convergence of the Cauchy product is lower bounded by the minimum of the radius of convergence of either power series, but this question appears to ask for an upper bound, which leaves me rather puzzled.","['power-series', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
3452851,supremum of expectation = expectation of supremum?,"In supremum of expectation $\le$ expectation of supremum? , can we have the reverse inequality up to a constant? Like $$ \underset{y\in \mathcal Y} \sup \mathbb E\big[f(X,y)\big] \ge C\mathbb E\big[\underset{y\in \mathcal Y} \sup f(X,y)\big] $$ for some $C>0?$ Or, having some conditions on $f$ , can we have $$ \underset{y\in \mathcal Y} \sup \mathbb E\big[f(X,y)\big] = \mathbb E\big[\underset{y\in \mathcal Y} \sup f(X,y)\big]? $$ Let us say $f(X,y)=X^Ty.$ Do we have an equality for this? Supremum of expectation equals expectation of supremum? : this post seems to be addressing this, but not sure if it's correct!","['expected-value', 'supremum-and-infimum', 'probability']"
3452904,Prove that if a function's average is nondecreasing then the function is nondecreasing.,"Let $\omega:\mathbb{R}_+\to\mathbb{R}_+$ such that $\omega(0) = 0$ and $\xi(s) = \frac{1}{s}\int\limits_{0}^s \omega(t)dt$ is nondecreasing, i.e., $\forall s,s': s\leq s'$ , $\xi(s)\leq \xi(s')$ . Show that $\omega(t)$ is nondecreasing a.e., i.e. find a function $w(t)$ such that $w$ is nondecreasing and $\omega = w$ a.e. I was able to show the reverse implication easily, that if $\omega$ is nondecreasing then $\xi$ is nondecreasing. This way seems more difficult.","['functions', 'real-analysis']"
3452958,Pullback of F is injective if and only if the image of F is dense - proof check,"This has been asked here and here before, but not to my satisfaction - the first answer is not incredibly detailed to my mind, neither is the second - so hopefully this will be allowed as a question on its own. Specifically, I have tried to write out in detail the proof, and am looking for feedback on improvement/where I've totally misunderstood something. The question is exercise 2.5.1 of Smith's Invitation to Algebraic Geometry. I'll use $ F: V \to W $ to denote a morphism of affine algebraic varieties, and $ F^* : \mathbb{C}[W] \to \mathbb{C}[V] $ the pullback of $F$ on the coordinate rings. The question: show that $F^*$ is injective if and only if $F(V)$ is dense in $W$ . To do so we need to use the characterisation of density as having nonempty intersection with all open sets of $W$ . In both directions I prove it by contradiction, which is (kind of) gross. Assume $F^*$ is not injective, so that there exists $ g \in \mathbb{C}[W]$ such that $g \neq 0$ on $W$ and $ (gF)(x) = 0 $ for all $ x \in \mathbb{C}[V]$ . We seek a contradiction of the second statement. Consider the open set $ U = \{y \in W : g(y) \neq 0\}$ . If $ y \in U \cap F(V)$ , then there exists $ x \in V $ so that $y = F(x)$ , and $ g(y) \neq 0$ . Thus $(gF)(x) \neq 0$ , a contradiction. Now assume $F(V)$ is not dense, i.e. there exists some nonempty open set $U \subseteq W$ so that $U\cap F(V) = \emptyset$ . So there exists some family of polynomials $p_i$ with $i \in I$ so that $U = \{y \in W : p_i(y) \neq 0, i \in I\}$ . Hence $U \cap F(V) = \emptyset$ implies that $(p_i F)(x) = 0$ for all $ i \in I$ and $x \in V$ , which by the injectivity of $F$ means $p_i = 0$ , i.e. $U = \emptyset$ , a contradiction. End of proof. I'm fairly confident in my writeup, but I'm also not sure if there might be better (maybe easier) ways of arguing the proof. I would like to avoid (if possible) more machinery than necessary, since at this point in time in the text Smith has not (formally) introduced many concepts. Any critique is welcome!","['algebraic-geometry', 'proof-verification', 'commutative-algebra']"
3453063,Find the $n^{th}$ derivative of $y=\tan^{-1} \left(\frac {1+x}{1-x}\right)$,Find the $n^{th}$ derivative of $y=\tan^{-1} \left(\dfrac {1+x}{1-x}\right)$ My Attempt: $$y=\tan^{-1} \left(\dfrac {1+x}{1-x}\right)$$ Put $x=\cos (\theta)$ $$y=\tan^{-1} \left(\dfrac {1+\cos (\theta)}{1-\cos (\theta)}\right)$$ $$y=\tan^{-1} \left(\dfrac {2\cos^2 \left(\dfrac {\theta}{2}\right)}{2\sin^2\left(\dfrac {\theta}{2}\right)}\right)$$ $$y=\tan^{-1} \left(\cot^2 \left(\dfrac {\theta}{2}\right)\right)$$,"['calculus', 'derivatives', 'real-analysis']"
3453093,"How do I solve $y+x\frac{\mathrm{d} y}{\mathrm{d} x}=3\; $ given $(x=1,y=1) $","$y+x\frac{\mathrm{d} y}{\mathrm{d} x}=3\; (x=1,y=1)$ Any help would be much appreciated. $$x\frac{\mathrm{d} y}{\mathrm{d} x}=3-y$$ $$\int \frac{1}{3-y} dy=\int\frac{1}{x}dx$$ $$3-y=x\cdot (\pm e^{C})$$ $$y=-x+3\cdot (\pm e^{C})$$ Let $(\pm e^{C}) = k$ $y=k(-x+3)$ , so $ k=\frac{1}{2} from (1,1)$ $y=-\frac{1}{2}(x-3)$ Is this right?? I have no idea.","['derivatives', 'ordinary-differential-equations']"
3453105,Definition of $C^k$ boundaries,"I am reading the book ""Partial Differential Equations"" of Lawrance c. Evans by myself and started with Appendix part. At the very beginning of Appendix C, there exists a definition
""We say $\partial U$ is $C^k$ if for each point $x^0\in\partial U$ , there exists $r>0$ and a $C^k$ function $\gamma:\mathbb{R}^{n-1}\longrightarrow\mathbb{R}$ such that we have $ U\cap B(x^0,r)=\{x\in B(x^0,r)\lvert x_n>\gamma(x_1,...,x_{n-1})\}$ I do not understand the intuition behind this definition.To my understanding it does not correspond to derivatives. Can anyone help me with that? Why we call such boundary sets as $C^k$ ?","['partial-differential-equations', 'derivatives', 'ordinary-differential-equations', 'real-analysis']"
3453110,"$f$ is measurable. Prove that $f$ has to be constant on $(0,∞)$. [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let $f : (0,\infty) \to \mathbb{R}$ be measurable and $0 < \lambda < 1$ . Suppose that $$f(x + y) = \lambda f(x) + (1 − \lambda)f(y)$$ holds for any $x, y > 0$ . Prove that $f$ has to be constant on $(0,\infty)$ .","['analysis', 'real-analysis']"
3453128,How to prove this Bessel integral identity,"I would like to prove the following identity which I found implemented in some code given to me: $$\int_0^\infty k e^{-k^2} J_0(kx)Y_0(kx)~\mathrm{d}k = -\frac{1}{2\pi} e^{-\frac{1}{2}x^2} K_0\left(\frac{1}{2}x^2\right)$$ where $J_0$ and $Y_0$ are the Bessel functions of the first and second kind, respectively, and $K_0$ is the modified Bessel function of the second kind. I am a physicist and not particularly familiar with Bessel functions or with their usual identities. Any particular ideas on how to prove this identity? I did a quick simulation in Python and the two sides of the equation indeed seem equal.","['integration', 'bessel-functions', 'real-analysis']"
3453154,Infinite prime cylinders?,"Define a prime $n$ -circle as a circular sequence of $n$ distinct natural numbers such that adjacent elements sum to a prime
(including $n^{\textrm{th}}$ $+$ $1^{\textrm{st}}$ ).
For example: $$6, 1, 2, 5, 8, 9, 4, 7 \;.$$ This is a variation on the prime circles counted in
the integer sequence A051252 ,
in that I am not insisting that the numbers be drawn from $\{1,2,\ldots,n\}$ . (The above circle misses $3$ .) Define a prime $n$ -cylinder as an aligned stacking of prime
circles, such that adjacent numbers vertically also sum to a prime.
(However, no wrap-around requirement from top to bottom.)
Further, every circular rung of the cyclinder should be a
distinct permutation—no repeats allowed (also disallowing
reversals). For example, here is a prime octagonal-cylinder consisting of 
four prime circles: And here displayed as a matrix: $$
\left(
\begin{array}{cccccccc}
 7 & 4 & 3 & 8 & 9 & 2 & 5 & 6
   \\
 16 & 15 & 8 & 9 & 4 & 3 & 2 &
   1 \\
 7 & 4 & 9 & 8 & 3 & 2 & 15 &
   16 \\
 6 & 1 & 2 & 5 & 8 & 9 & 4 & 7
   \\
\end{array}
\right)
$$ Q . Do there exist infinitely tall prime $n$ -cylinders,
  for each $n$ ? The answer should
be the same whether the cyclinder is
infinite just in one direction, or bi-infinite, extending infinitely
in both directions.
Although it seems relatively easy to add rungs in a greedy fashion,
I don't see how to prove that this approach can extend infinitely. For example, here is how one might get ""stuck.""
Suppose you are adding the last number $x$ to the
top prime circle. The three numbers adjacent to $x$ (left, right, below)
could be $1$ , $3$ , and $5$ . But the only number $x$ such
that each of $\{x+1, x+3, x+5 \}$ is prime is $x=2$ , which may already
have been used in that top circle.","['number-theory', 'prime-numbers', 'sequences-and-series']"
3453173,Convergence in measure metrizable?,"I am trying to show for a $\sigma$ -finite measure space $f_n\rightarrow f$ $\mu$ -stochastically iff $\lim_{n\rightarrow\infty} d(f_n,f)=0$ where $$d(f,g):=\sum^\infty_{k=1}\frac{2^{-k}}{\mu(\Omega_k)}\int I_{\Omega_k}\frac{|f_n-f|}{1+|f_n-f|}d\mu$$ but I'm having trouble proving either direction.
For the forward direction I say from the definition (w.l.o.g. replacing $f_n$ by $f_n-f$ ): $$\forall A\in\mathcal{A},\epsilon>0:\mu(A\cap\{f_n>\epsilon\})\rightarrow 0 (n\rightarrow\infty),$$ it follows that $$\forall A\in\mathcal{A},\epsilon>0:\mu(A\cap\{\frac{f_n}{1+f_n}>\frac{\epsilon}{1+\epsilon}\})\rightarrow 0 (n\rightarrow\infty).$$ Then by choosing a constant some $C=1/(\epsilon+1)>0$ argue as follows $$d(f_n,0):= \sum^\infty_{k=1}\frac{2^{-k}}{\mu(\Omega_k)}\int I_{\Omega_k}\frac{|f_n|}{1+|f_n|}d\mu$$ $$\leq\sum c_k \int I_{\Omega\cap\{|f_n|/(1+|f_n|)>C\}}\leq \sum c_k \mu(A\cap\{\frac{|f_n|}{1+|f_n|}>C\})\rightarrow 0(n\rightarrow\infty).$$ Is this the right approach? Is there such a constant?
For the other direction I argue for a non-empty measurable set $A$ with finite measure and $\epsilon,\delta>0$ we can choose $A$ to be one of the partitioning sets and it follows that for some $N\in\mathbb{N}$ and all $n\geq N$ : $$\delta>d(f_n,0)\geq \int_{A}\frac{|f_n|}{2\mu(A)(1+|f_n|)}\geq \mu(A\cap\{\frac{|f_n|}{(1+|f_n|)}>\epsilon\})/2\mu(A).$$ And now the direction follows with $\delta\rightarrow 0$ . Is this legal?
Any help would be great! Sorry for the sloppy tex.","['integration', 'measure-theory', 'metric-spaces', 'functional-analysis', 'convergence-divergence']"
3453176,Is an arbitrary union of a chain of countable sets countable?,"Let $C$ be a chain of countable sets, i.e. $\forall S, T \in C: S \subseteq T \lor T \subseteq S$ , and that every $S \in C$ is countable. Then, is $\bigcup C := \{ t \mid \exists S \in C: t \in S\}$ countable? The answer is no, and a counter-example is $C = \omega_1$ , the first uncountable cardinal/ordinal. Is there a more elementary example that doesn't involve, say, cardinals and ordinals?",['set-theory']
3453183,PDF of product of uniform variables,"I have encountered a problem in computing the PDF of a variable (call it $y_n$ ) that is the product of n uniformly distributed random variables $x$ : $y_n=\prod_i^n x_i.$ In https://math.stackexchange.com/a/2812234 there is the solution for the case $x \in (0,1),$ but in my case the random variables are distributed in the interval (0.2,1.8), or more generally in the interval (a,b). I have not been able to translate the formula for the product in the interval (0,1) to my case; if I follow the procedure used to retrieve the formula (see link above) I have a problem since the integrand has no pole, hence no residue. Can anyone point out any suggestions on how to proceed, or if there is any reference to books or articles where this kind of case is treated?",['statistics']
3453186,Solve differential equation $(y-x)\sqrt{1+x^2}y' = \sqrt[3]{1+y^2}$,Help me to solve differential equation: $$(y-x)\sqrt{1+x^2}y' = \sqrt[3]{1+y^2}$$ I can't think of a way to substitute a variable,['ordinary-differential-equations']
3453227,When to differentiate under the integral sign?,"I'm finishing up a semester of multivariable calculus and will be taking a course on analysis this Spring. In any of the calculus courses I've taken, we never covered anything beyond the standard techniques of integration ( $u$ -sub, parts, etc.) One of the techniques I saw used recently which I had not heard of was differentiation under the integral sign , which makes use of the fact that: $$\frac{d}{dx} \int_a^bf(x,t)dt = \int_a^b \frac{\partial}{\partial x}f(x,t)dt $$ in solving integrals. My question is, is there ever an indication that this should be used? Is there any explainable intuition or rule of thumb for the use of differentiation under the integral sign?","['integration', 'multivariable-calculus', 'calculus', 'derivatives', 'soft-question']"
3453230,The existence of an algebra homomorphism between $\mathcal{M}_n({\mathbb{K}})$ and $\mathcal{M}_s(\mathbb{K})$ implies $n | s$,"Let $n,s \geq 1$ be integers and $\mathbb{K}$ a field. We assume there exist $\Phi : \mathcal{M}_n(\mathbb{K}) \rightarrow \mathcal{M}_s(\mathbb{K})$ an unital algebra homomorphism ( $\Phi(I_n)=I_s$ ). See here for the definition . We must show that necessarily $n | s$ and there exists $P \in \textrm{GL}_s(\mathbb{K})$ such that for all $A \in \mathcal{M}_n(\mathbb{K})$ : $$
P \cdot \Phi(A) \cdot P^{-1} = \begin{pmatrix}
A & & (0)\\
 & \ddots &\\
(0) & & A \\
\end{pmatrix}
$$ I have tried, but I cannot find a way to exploit $\Phi$ to show this. I am looking for a solution of this that uses only ""basic"" theorems of linear algebra (Bachelor's level). Any help is welcome.","['matrices', 'abstract-algebra', 'linear-algebra', 'ring-homomorphism', 'similar-matrices']"
3453234,Name of partial derivatives where the order of differentiation can be reversed.,"Is there a name given to partial differential equations of the form: $$\frac{\partial{F}}{\partial{x}\partial{y}}=\frac{\partial{F}}{\partial{y}\partial{x}}$$ Not asking for any kind of proof, just specifically wondering if there is a name given to PDE's that satisfy this condition.","['multivariable-calculus', 'terminology', 'partial-differential-equations']"
3453235,"Zerodimensional, compact space is homeomorphic to spectrum of some ring -- elementary attempt","Let $X$ be a zerodimensional (X has a base with clopen sets) and compact (quasicompact and Haussdorf) topological space and I would like to prove that $X\cong Spec(A)$ of some ring $A.$ I don't want to refer to Stone duality because I didn't had this theorem on a lecture. I try to show that $X\cong Spec(C(X,\mathbb{Z_2})),$ where latter means continuous functions with values in $\mathbb{Z_2}.$ It is related with question: Spectrum of a ring homeomorphic to a compact, totally disconnected space Obviously, I can show injectivity of map given in the answer of this question, continuity, also that image of open base set is open base set. I have a trouble to deal with the surjectivity (attempt which works for real-valued functions doesn't work here because of addition in $\mathbb{Z}_2.$ I'm stuck in this point and I would like ask you for a help. It is possible that ring on a right isn't correct. I underline that I don't want to use the Stone duality, I prefer an elementar way.","['zariski-topology', 'ring-theory', 'abstract-algebra', 'general-topology', 'commutative-algebra']"
3453268,Sequence convergence almost surely,"If we consider $(Xn)_{n \in \mathbb{N}}$ , a sequence of independent random variables satisfying: $$ P(X_n = 1) = p_n \;\;\;\;\;\;\;\;\;\; P(X_n = 0) = 1 − p_n$$ then : $$X_n \;{\overset{a.s}{\longrightarrow}} \;0 ⇔\sum_{n \in \mathbb{N}} p_n < ∞$$ I couldn't show the first implication from convergence a.s to the sum of $p_n$ is finite!","['probability-limit-theorems', 'stochastic-analysis', 'probability-distributions', 'stochastic-processes', 'probability']"
3453270,Why is this translation not a linear transformation? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question From Linear map , the sixth example : The translation $x \rightarrow x+1$ is not a linear transformation. Why? What about $x \rightarrow x +dx$ ? Is this translation a linear transformation? Does it matter if the transformation is not linear?","['linear-algebra', 'terminology']"
3453309,Frusturatingly simple looking problem from Undergrad Real Analysis Course,"Let $\mathit f$ : $\Bbb R \rightarrow \Bbb R $ be a function defined by $$ \mathit f(x):= 2x+\sin x$$ Show that the equation : $\mathit f(x)=n$ , (Where n $\in \Bbb N $ ) admits a unique real solution, denoted $\mathit x_n$ . What is $x_0$ ? This is a problem from my real analysis homework, the first in a four part problem. I pursued the usual ""let's just set the equation equal to n and see if something falls out"" approach, which failed. This is being done within the context of a chapter on the intermediate value theorem and the mean value theorem.","['functions', 'real-analysis']"
3453400,Subnormal subgroups of the generalized Fitting subgroup,"Let $G$ be some finite group. A component of a finite groups is a quasisimple subnormal subgroup. The layer $E(G)$ is the subgroup generated by all components of $G$ , let $F(G)$ denote the Fitting subgroup , then $$
 F^*(G) = F(G) E(G)
$$ is called the generalized Fitting subgroup of $G$ . Let $L$ be some subnormal subgroup of $G$ in $F^*(G)$ , then $L = (L\cap F(G))(L \cap E(G))$ and $L \cap E(G)$ is a product of components of $G$ . This statement is taken from Kurzweil/Stellmacher, Theorie der endlichen Gruppen (page 130). But why is it true? I tried to prove it, or  to understand a very concise sentence given after the statement. But it confuses me. Every component of $L$ is a component of $G$ , hence $E(L) \le E(G)$ and also $F(L) \le F(G)$ as $L$ is subnormal.  Now $E/Z(E)$ is a direct product of simple non-abelian groups, and as $F^*(G)$ is a central product we have $$
 F^*(G) / Z(F^*(G)) \cong F(G) / Z(F(G)) \times E(G) / Z(E).
$$ So I tried to deduce the statement by looking at the quotient $LZ(F^*(G)) / Z(F^*(G))$ and write it as a product using that it embedds into a direct product where the first part is nilpoent, and the second a product of simple non-abelian groups, from which the subnormal subgroups have an easy form as direct product of a subset of the factors. But I do not get it. So could someone please supply a proof of the claim?","['group-theory', 'abstract-algebra', 'finite-groups']"
3453408,In this simple integral why is the constant natural logged?,I'm reading through some lecture notes and see this in the context of solving ODEs: $$\int\frac{dy}{y}=\int\frac{dx}{x} \rightarrow \ln{|y|}=\ln{|x|}+\ln{|C|}$$ why is the constant of integration natural logged here?,"['integration', 'ordinary-differential-equations']"
3453424,Applications of Lagrange Multiplier in Economics or Computer Science,"I'm a high school student studying grade 12 maths and I need to write a 12 page paper on the Lagrange Multiplier. I have understood the mathematics behind it but I need a real equation or application of this concept which one can observe in the real world. Like an equation which has been derived from a company's revenue or something like that. This is required since I need to incorporate the use of mathematics to a real-life situation. If you cannot think of any equations right off the top of your head, could you please suggest methods to derive the equation using data? Any type of help is very much appreciated. Thank you","['lagrange-multiplier', 'economics', 'multivariable-calculus', 'optimization', 'computer-science']"
3453456,"Prove $\iota ': \pi _1(A,a) \to \pi _1(B,a)$ is a bijection, if $C\setminus \{b\}\subseteq B$ is simply connected.","I am having trouble solving this problem: Let $A\subseteq B$ and consider the inclusion $\iota : (A, a) \to (B, a)$ with pointed spaces $(A, a)$ , $(B, a)$ . Let $A=B \setminus \{b\}$ where $b\in B\setminus \{a\}$ , and suppose $B$ is hausdorff and path connected. Let $C \subseteq B$ be open and simply connected with $b \in C$ . Define $\iota ': \pi _1(A,a) \to \pi _1(B,a)$ as $\iota'([\lambda])=[\iota\circ \lambda]$ . Clearly $\iota'$ has the morphism property. Supposing $C\setminus \{b\}$ is simply connected, then prove $\iota'$ is a bijection. I am trying to solve this with techniques from Hatcher chapters 0-1. I have tried to deduce this by considering the implications of a simply connected $C\setminus \{b\}$ , namely path connected with trivial $\pi_1$ . I have also considered how one might be able to use the hausdorff property of $B$ , but it is not clear to me. In fact it is not even clear that $A$ is path connected with this assumption because one can have a pair of points outside $C\setminus \{b\}$ that are only joined by a path through $b$ . More, if we restrict our attention to the Euclidean plane, then it seems obvious that $C\setminus \{b\}$ cannot be simply connected. So if I am correct, $B$ cannot be a subspace of $R^n$ for $n<3$ . I am not certain how to attack this problem. There seem to be many approaches and I don't see which one is likely to help. Any hint is much appreciated.","['general-topology', 'abstract-algebra', 'fundamental-groups', 'algebraic-topology']"
3453460,"Requested Hint For: If $\ker \varphi_1 \not \cong \ker \varphi_2$, then $H \rtimes_{\varphi_1} K \not \cong H \rtimes_{\varphi_2} K$","I am trying to determine if the following is true: Let $H, K$ be groups, and let $\varphi_1, \varphi_2$ be homomorphisms from $K \to \text{Aut}(H)$ . If $\ker \varphi_1 \not \cong \ker \varphi_2$ , then $H \rtimes_{\varphi_1} K \not \cong  H \rtimes_{\varphi_2}  K$ . EDIT: In the comments, I see that the above statement is not true. However, I am looking for conditions under which the statement is true. For example, is it true if we assume one of $H, K$ (or both) is abelian, or one of $H, K$ is cyclic? By book (Dummit & Foote) assert this in a specific example (where $H = Z_7$ and $K$ is an abelian group of order $8$ ), but I am trying to see if this is true in general. Could you please give me a hint on how I should prove this, if it is indeed true? What I tried: The only way I know to prove that $2$ groups are not isomorphic is by looking at various properties such as the number of elements of order $x$ , the centers, etc. But this doesn't seem applicable here since the groups are completely general. I tried looking at the groups $\{(h, k)|h \in H, k \in \ker \varphi_1 \}$ and $\{(h, k)|h \in H, k \in \ker \varphi_2 \}$ . I see they are normal, but I could not prove they are characteristic, and even if I could, I don't see how that would help. Thank you very much!","['semidirect-product', 'group-theory', 'abstract-algebra']"
3453477,Resonance and Repeated Factors,"When studying Calculus, I asked this question about why non-repeated and repeated factors are handled differently in partial fraction decomposition.  As I work Laplace Transform problems, I'm noticing repeated factors occur when there is resonance between an ODE's forcing term and associated homogeneous solution.  This makes at least half of a bit of sense, since the algebraic step of a Laplace Transform problem involves dividing everything by the characteristic polynomial, which is related to the associated homogeneous solution.  Perhaps the denominator of a forcing term's Laplace Transform is related to the homogeneous linear ODE which that forcing term would solve, creating this overlap in the resonant case. Is it accurate to say that repeated factors in the s-domain occur if, only if, or if and only if the ODE has resonance?  If so, is my linked question about the additional term in a repeated-factor partial fraction decomposition nothing more than the s-domain framing of the common question of why an additional factor of $t$ appears in the particular solution to a resonant linear ODE?","['partial-fractions', 'laplace-transform', 'ordinary-differential-equations']"
