question_id,title,body,tags
4425073,"$\frac{dy}{dx}=\frac{y}{x}$ for a homogeneous equation in $x,y$?","A problem book I use mentions the following: ...since the curve $32x^3y^2=(x+y)^5$ is homogeneous, $\frac{dy}{dx}=\frac{y}{x}$ I have indeed verified the derivative. Is this a known theorem? I generalized it to $$cx^ay^b=(x+y)^{a+b},$$ but does there exist a more generalized form?","['curves', 'derivatives']"
4425077,Discrepancy between binomial and nCr methods of calculating probability?,"A bag has 1000 balls, some colored red, and the rest green. The fraction of red-colored balls in the bag is 0.2. A blindfolded observer picks out 5 balls from this bag. What is the probability that 3 reds are picked? Method 1: The probability p of picking a red ball is 0.2. So, this can be treated as a binomial problem: $ Pr$ (3 red balls picked) $= {5 \choose 3} (0.2)^3 (0.8)^2 = 0.0512$ Method 2: We choose 3 red balls from a group of 200 and 2 green ones from the group of 800. So, the probability becomes: $ \frac{{200 \choose 3} {800 \choose 2}} {{1000 \choose 5}} \approx 0.0508785...$ Why is there a slight discrepancy in the probabilities? Is it an artifact of my calculator's limitations or am I missing something deeper?","['combinatorics', 'binomial-distribution', 'probability']"
4425078,Asymptotic on $\max\{d(n): 1\leq n\leq N\}$.,"Question: Let $d(n)=\sum_{d|n}1$ , be the divisor function. Estimate the asymptotic behaver $$D(N):=\max\{d(n):1\leq n\leq N\}$$ when $N$ is large. I know $$\sum_{n=1}^N d(n)=\sum_{n=1}^N [\frac{N}{n}]=N\sum_{n=1}^N\frac{1}{n} + O(N)\approx N{\rm log} N.$$ So $D(N)\geq \frac{1}{N}\sum_{n=1}^N d(n)\approx {\rm log}N$ . But I can't determine if this is optimal, i.e. $D(N)=O({\rm log}N)$ .","['analytic-number-theory', 'analysis']"
4425084,Cauchy-Schwarz inequality for nonsymmetric matrices,"I am interested in Cauchy-Schwarz inequalities for inner products of the form $\langle Ax,y\rangle$ where $A$ is some matrix. It is rather easy to see that the Cauchy-Schwarz inequality continues to hold when relaxing the assumption of positive definiteness on $A$ to positive semi-definitess. I asked myself whether one can loosen the assumption on symmetry of $A$ . Suppose $A \in \mathbb{R}^{n \times n}$ is a matrix such that $\langle Ax,x\rangle \ge \lambda |x|^2$ . Is it true that for all $x,y \in \mathbb{R}^n$ we have the following inequality $$\langle Ax,y \rangle^2 \le \langle Ax, x \rangle \langle Ay,y \rangle? $$ If not, is there any other similar inequality, which replaces the Cauchy-Schwarz inequality?","['cauchy-schwarz-inequality', 'linear-algebra', 'real-analysis']"
4425118,A sequence of functions in $L^1$ that does not converge weakly,"I have attempted the following part from exercise 5.49 in https://www.mat.uniroma2.it/~cannarsa/cam_0607.pdf : For every $n \in \mathbb{N}$ , let $f_n \colon \mathbb{R} \to \mathbb{R}$ be defined by: $$ f_n(x) = \begin{cases} \frac{1}{2^n}  &\text{if $x \in [2^n, 2^{n+1}]$} \\ 0 &\text{otherwise} \end{cases} $$ Show that: [...] $\{f_n\}$ does not converge weakly in $L^1(\mathbb{R})$ Is the following proof correct? Can it be made more concise? Aiming for a contradiction, suppose that $(f_n)$ does converge weakly in $L^1(\mathbb{R})$ , i.e. there is $f \in L^1(\mathbb{R})$ such that $f_n \rightharpoonup f$ .
By definition, this means that for all $\phi \in (L^1(\mathbb{R}))^*$ , we have $\phi(f_n) \to \phi(f)$ . The dual of $L^1(\mathbb{R})$ contains $L^\infty(\mathbb{R})$ through the following embedding: $$ \begin{aligned} L^\infty(\mathbb{R}) &\to (L^1(\mathbb{R}))^* \\ g &\mapsto \phi_{g} \end{aligned} $$ where $\phi_g$ is defined by: $$ \phi_g(h) = \int_\mathbb{R} gh\,\mathrm{d}\mu $$ First we prove that $f$ is non-negative almost everywhere.
Indeed, aiming for a contradiction, suppose it is not.
This means there exists a measurable $A \subseteq \mathbb{R}$ with $f(x) < 0 $ for all $x \in A$ .
Let $\chi_A$ denote the characteristic function of $A$ , which is in $L^\infty(\mathbb{R})$ , and consider $\phi_{\chi_A} \in (L^1(\mathbb{R}))^*$ .
On the one hand we have: $$\phi_{\chi_A}(f) = \int_\mathbb{R} \chi_A f\,\mathrm{d}\mu = \int_A f\,\mathrm{d}\mu < 0$$ On the other hand, for all $n$ , since $f_n \ge 0$ , we have: $$\phi_{\chi_A}(f_n) = \int_\mathbb{R} {\chi_A}f_n\,\mathrm{d}\mu = \int_A f_n\,\mathrm{d}\mu \ge 0$$ So $\phi_{\chi_A}(f_n)$ cannot converge to $\phi_{\chi_A}(f)$ as $n \to \infty$ , contradicting the assumption that $f_n$ converges weakly to $f$ . Now, we prove that $f$ must in fact be zero almost everywhere.
For any $m \in \mathbb{N}$ , consider $g_m := {\chi_{[2^m,2^{m+1}]}} \in L^\infty(\mathbb{R})$ and the corresponding functional $\phi_{g_m} \in (L^1(\mathbb{R}))^*$ .
Then: $$\phi_{g_m}(f_n) = \int_{[2^m, 2^{m+1}]} f_n \,\mathrm{d}\mu = \begin{cases} \frac{1}{2^n} (2^{n+1}-2^n) = 1 &\text{if $n = m$} \\ 0 &\text{otherwise} \end{cases} $$ Hence $\phi_{g_m}(f_n) \to 0$ as $n \to \infty$ .
But by hypothesis $\phi_{g_m}(f_n) \to \phi_{g_m}(f)$ , so $\phi_{g_m}(f) = 0$ .
That is: $$\int_{[2^m,2^{m+1}]} f\,\mathrm{d}\mu = 0$$ Since we have shown that $f$ is non-negative almost everywhere, this implies it is in fact zero almost everywhere on $[2^m, 2^{m + 1}]$ .
But this is true for all $m \in \mathbb{N}$ , and since a countable union of null sets is still null, this means $f$ is zero almost everywhere on $\bigcup_{m \in \mathbb{N}} [2^m, 2^{m+1}] = [1, \infty)$ . Apply the same kind of argument with $\phi_{\chi_{(-\infty, 1]}}$ to show that $f$ is zero almost everywhere on $(-\infty, 1]$ , concluding that $f$ is zero almost everywhere on all of $\mathbb{R}$ . So far we have proven that, assuming $f_n \rightharpoonup f$ , we have that $f$ is zero almost everywhere.
But this leads to a contradiction.
Indeed, consider the constantly-1 function ${\chi_{\mathbb{R}}}$ and the corresponding functional $\phi_{\chi_\mathbb{R}} \in (L^1(\mathbb{R}))^*$ , which is just integration (on all of $\mathbb{R}$ ).
Then: $$\phi_{\chi_\mathbb{R}}(f_n) = \int_\mathbb{R} f_n\,\mathrm{d}\mu = \frac{1}{2^n}(2^{n+1}-2^n) = 1 \to 1 \text{ as $n \to \infty$}$$ However, since $f$ is zero almost everywhere: $$\phi_{\chi_\mathbb{R}}(f) = \int_\mathbb{R} f\,\mathrm{d}\mu = 0$$ Thus $\phi_{\chi_\mathbb{R}}(f_n) \nrightarrow \phi_{\chi_\mathbb{R}}(f)$ , contradicting our initial assumption of the weak convergence of $(f_n)$ to $f$ .","['lp-spaces', 'solution-verification', 'functional-analysis', 'weak-convergence']"
4425121,How to solve this 2nd ODE with simple inequality constraints numerically?,"I have a 2nd ODE (derived from an elastic rod deflated naturally under its self-weight): $$
y'' + K (x - 1) \cos(y) = 0 
$$ K is a constant coefficient. The variables range between $x \in [0, 1], y \in [-\pi / 2, 0]$ The BCs: $$
y(0) = 0 \\
y'(1) = 0
$$ and a constraint $y' \le 0$ and $y'' \ge 0$ (from physical meaning). How can I solve it numerically? I have a weak background in the theory/classfication of ODEs, but I still tried to analysis and solve it as followed: 1. My own analysis on this problem This is an second order nonlinear ODE. Its BCs are located in two sides, so it is a BVP. A common practice is to split it into a 1st ODE system: $$
\begin{cases}
y_2 = y_1' \\
y_2' + K (x - 1) \cos (y_1) = 0
\end{cases}
$$ Then I discrete all derivates by explicit euler method, and employe shooting method (where I guess the init BCs y'(0) and adjust this guess for a satisfying solution). When K is big, I can solve it properly. But when K is small, my shooting method always failed. I have no idea how to analysis it anymore. 2. My questions Is my judgement about this problem correct? Is it an 2nd order ODE, Bounday value problems with not enough BCs? Why I can solve it properly in non-stiff case, and fail in stiff case? Some literatures point out that, auto07p package can help me on this problem. But I cannot understand its manual even a bit. Which book should I read to handle this problem? How can I know whether this problem have a solution? How to solve this problem numerically? Sorry for my weak background in ODE. Thanks a lot for your time! 3. solve_bvp and shooting failed when K > 200 and ignore the constraint When K > 200, we need to set up a special init guess for solve_bvp , otherwise we will fail. Please check @Lutz 's answer below.
$","['boundary-value-problem', 'numerical-methods', 'ordinary-differential-equations']"
4425132,"$30\%$ students have glasses. $20\%$ of students with glasses play. $60\%$ of students without glasses play. Probability, student without glass plays.","Problem: In a school, $30\%$ of students have glasses. $20\%$ of students with glasses play sports. $60\%$ of students without glasses play sports. If we randomly choose a student, find probability that a student without glasses (chosen randomly) plays a sport. My Approach $P(O)=0.3$ is the probability that a student has glasses. $P(S|O) = 0.2$ is the probability that a student with glasses plays sport (P of S given O). $P(S|O^c)=0.6$ , where $O^c$ is {Sample Space - $O$ }. The probability that a student without glasses plays sport $(P(S\text{ given }O^c)$ . I have to find $P(O^c|S)$ (the opposite of $P(S|O^c)$ , meaning ""a student plays sport without glasses""). I know that $80\%$ of students play a sport, and that $70\%$ of students don't have glasses. So in order to solve the problem I have to do $$P(O^c|S) = \frac{0.8 \times 0.7}{0.8} = 0.7 $$ but given correct solution is $0.87$ $(0.7\neq0.87)$ . EDIT: I think the problem is in this assumption: I've assumed that $0.8$ and $0.7$ were two independent probabilities, therefore I've multiplied $0.8$ with $0.7$ . If this is not correct, then I don't know how to find probability of intersection.","['conditional-probability', 'word-problem', 'intersection-theory', 'bayes-theorem', 'probability']"
4425139,Solving simple ODE,"I'm trying to solve this ODE: $$y'(x)=\cos^2(y) \\ y(1)=π$$ $$⇒\int \dfrac{1}{\cos^2(y)} \, dy =\int \, dx $$ $⇒\tan(y) = x +C.$ And $y(1)=π,$ so $\tan(π)=1+C⇔C=-1.$ Now if I solve for $y(x),$ $y(x)=\arctan(\tan(y))=\arctan(x-1)$ , which is wrong because $y(1)=π.$ What do I miss when I solve for $y(x)$ in the last step? The correct answer is $y(x)=\arctan(x-1)+π.$","['calculus', 'ordinary-differential-equations']"
4425206,Combinatorial proof of $\sum_{k=0}^{n} k \binom{n+1}{k+1} n^{n-k} = n^{n+1}$,"Show : $$\sum_{k=0}^{n} k \binom{n+1}{k+1} n^{n-k} = n^{n+1}$$ for natural number $n$ . I randomly discovered this identity, and managed to prove it using simple algebra. I tried a combinatorial proof of this, but it seems too difficult for me. The RHS is basically distributing $n+1$ people to $n$ different groups where an empty group is possible, but I could not show that the LHS is the same. Picking $k+1$ people out of $n+1$ equals $\binom{n+1}{k+1}$ , and distributing others( $n-k$ people) is equal to $n^{n-k}$ ; and now I am stuck with that $k$ . Also I have no idea what to do with $k+1$ people I just picked; if I distribute them to $n$ groups then it will be overlapped with other terms of the sum. A proof using algebra is also welcome, just in case.","['summation', 'binomial-coefficients', 'combinatorics', 'combinatorial-proofs']"
4425326,Relationship between definitions of Regular Conditional Distribution.,"There seem to be two different definitions of a regular conditional distribution, but they seem to define a slightly different kernel function. How are they connected? Definition 1 and Definition 2 . Set Up Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, $(\mathsf{E}, \mathcal{E})$ a measurable space, $X:\Omega\to\mathsf{E}$ a random variable with distribution $P_X = \mathbb{P}\circ X^{-1}$ , and $\mathcal{G}\subseteq \mathcal{F}$ be a sub-sigma algebra. Definitions Definition 1 : A kernel $k:\Omega\times \mathcal{E}\to [0, 1]$ that is a version of $\mathbb{E}[\mathbb{1}_{X\in A} \mid \mathcal{G}](\omega) = k(\omega, A)$ . Definition 2 : A kernel $k:\mathsf{E}\times\mathcal{F}\to[0, 1]$ satisfying $\mathbb{P}(A\cap X^{-1}(B)) = \int_B k(x, A) d P_X(x)$ . It almost seems that in the second definition one uses the inverse of $X$ as the random variable of interst, but this doesn't necessarily exist. MAJOR EDIT There is something really odd at play. I think there are actually many different definitions but people seem to not distinguish between them. Definition 1 is definitely a definition for a regular conditional distribution of a random variable given a sub-sigma algebra . That is $\mathbb{P}(X\in A \mid \mathcal{G})$ Definition 2 seems to be something like a regular conditional probability of a set given a random variable . That is $\mathbb{P}(A \mid X = x)$ .","['measure-theory', 'statistics', 'conditional-probability', 'probability-distributions', 'probability-theory']"
4425390,Importance of finitude in the Poincaré-Bendixson theorem,"I have a question regarding the Poincaré-Bendixson theorem: I understand everything, but I'm looking for an example that shows the importance of the finitude of the singular points within the theorem. What happens when we have an infinite number (countable or uncountable) of singular points? Until now I have only been able to find examples that teach me or show me how this theorem works but I would like to see an example about the importance of the finitude of said singular points.","['ordinary-differential-equations', 'dynamical-systems']"
4425432,A pattern of periodic continued fraction,"I am interested in the continued fractions which $1$ s are consecutive appears.
For example, it is the following values. $$
\sqrt{7} = [2;\overline{1,1,1,4}] \\
\sqrt{13} = [3;\overline{1,1,1,1,6}]
$$ In this article, let us denote n consecutive $1$ s as $1_n$ .
Applying this, the above numbers would be as follows. $$
\sqrt{7} = [2;\overline{1_3,4}] \\
\sqrt{13} = [3;\overline{1_4,6}]
$$ While investigating these numbers, the following pattern was found experimentally. $$ \sqrt{F(n)^2m^2-(F(n)^2-L(n))m+\frac{(F(n)-1)(F(n)-3)}{4}-\frac{F(n-3)-1}{2}}
=\left[F(n)m-\frac{F(n)-1}{2};\ \overline{1_{n-1},\ 2\left(F(n)m-\frac{F(n)-1}{2}\right)}\right] $$ ( $m,n \in \mathbb{N},\ n\equiv\pm1\ (mod3),\ n > 3,\ $$F(n)$ is Fibonacci number , $L(n)$ is Lucas number ) I confirm that it works correctly when $n$ and $m$ are single digits.
If you find a proof or counterexample, please let me know. (2022/04/13 edit) A general expression was derived. I think the expression I found is that special case. The condition is that the inside of the square root is always an integer. Here are some concrete examples. $$\begin{array}{|c|c|} 
\hline
n & pattern \\ \hline
4 & \sqrt{9m^2-2m} = [3m-1;\overline{1_3,2(3m-1)}] \\ \hline
5 & \sqrt{25m^2-14m+2} = [5m-2;\overline{1_4,2(5m-2)}] \\ \hline
7 & \sqrt{169m^2-140m+29} = [13m-6;\overline{1_6,2(13m-6)}] \\ \hline
8 & \sqrt{441m^2-394m+88} = [21m-10;\overline{1_7,2(21m-10)}] \\ \hline
\end{array}$$","['fibonacci-numbers', 'number-theory', 'sequences-and-series', 'continued-fractions', 'lucas-numbers']"
4425480,Integral of the Laurent series with Bessel function coefficients,"I'm trying to integrate a function of the following form, where $A$ and $B$ are both positive: $$\int_0^{\infty}\exp\left(\frac{A}{2}\left(\frac{B}{1+x^2}-\frac{1+x^2}{B}\right)\right)dx.$$ My first inclination was to put it into Laurent series form, which I eventually discovered was $$\int_0^{\infty} \sum_{k=-\infty}^{\infty} J_k(A)\left(\frac{B} {1+x^2}\right)^kdx.$$ Where $J_n(x)$ is the $n$ th order Bessel function of the first kind. Integrating the series directly was useless. Then, I thought I could try out a complex representation. I know this function is even, and with some basic analysis of the Laurent series shows that if I integrate around a semi circle of radius $R$ pointed up from the $x$ -axis, the integral around the contour should be: $$\oint_C\exp\left(\frac{A}{2}\left(\frac{B}{1+x^2}-\frac{1+x^2}{B}\right)\right)dx = J_1(A)B\pi.$$ I figured I could take the limit as $R$ increased without bound and get my answer, but swiftly realized that the circular contour does not seem to die off to zero as $R$ increases without bound. My question is either: what is the value of the following integral $$\lim_{R \to \infty}Ri\int_0^\pi\exp\left(\frac{A}{2}\left(\frac{B}{1+R^2e^{2\theta i}}-\frac{1+R^2e^{2\theta i}}{B}\right)\right)e^{\theta i}d\theta.$$ Or How else can I evaluate the initial integral? A numerical solution is fine, so long as it covers $A$ and $B$ being both large and close to zero.","['definite-integrals', 'complex-analysis', 'contour-integration', 'numerical-methods', 'bessel-functions']"
4425523,Expected value of minimum of hitting time of brownian motion.,"Let $(X_t)_{t\geq0}$ be a Brownian Motion starting from $0$ . Define for $a\in\mathbb{R}$ , $$T_a = \inf\{t\geq0:X_t=a\}.$$ For $a,b>0$ , I know that $\mathbb{P}(T_{-a}<T_b)=\frac{b}{a+b}$ , and I am expected to show that $\mathbb{E}(T_{-a}\wedge T_b)=ab$ . One idea I have is to define the process $Q_t = X_t^2-t$ . If the stopped process $Q^T$ was uniformly integrable, I could use the optional stopping theorem to easily get the result. However, I'm unsure as to whether $Q^T$ is UI, and if so how to show it.","['stochastic-processes', 'martingales', 'stopping-times', 'brownian-motion', 'probability-theory']"
4425526,Is the image of a measurable set by a measurable function measurable?,"Let $(\mathsf{X}, \mathcal{X})$ and $(\mathsf{Y}, \mathcal{Y})$ be measurable spaces and $f:\mathsf{X}\to\mathsf{Y}$ be a measurable function. Is this true? $$
\mathsf{A}\in\mathcal{X} \implies f(\mathsf{A})\in\mathcal{Y}
$$ If not, what extra conditions do I need to add? I know that given $\mathsf{B}\in\mathcal{Y}$ one has $f^{-1}(\mathsf{B})\in\mathcal{X}$ but not the reverse. Remarks Not sure if it would help but I could potentially assume that $\mathcal{Y}$ is countably-generated and contains all singleton sets. Also, I am mainly interested in the application of this to probability theory, so I am happy to assume that $(\mathsf{X}, \mathcal{X}, \mu)$ is a probability space so that $f$ is a random variable.","['measure-theory', 'real-analysis', 'abstract-algebra', 'probability-theory', 'probability']"
4425534,Calculating the Expected value of a function over a random vector,"let $$(X_1, X_2, \dots, X_n)$$ be the order statistics of $n$ i.i.d. uniform random variables. that satisfies the following condition. $$0 < x_1 < x_2 < \dots < x_n < 1$$ now consider a continuous function $f : [0,1] \rightarrow \mathbb{R}$ we define the random variable $R$ to be: $$
R = \sum_{i = 0}^{n - 1}f(X_{i+1}) \times (X_{i+1} - X_i) \hspace{0.5cm}X_0  = 0
$$ I need to prove the expected value of R equals: $$
E[R] = \int_{0}^{1}f(t)(1-(1-t)^n)dt
$$ What I've tried so far is that I tried to find the pdf of each $X_i$ using the following formula: $$
f_{X_k}(x_k) = \frac{n!}{(k-1)!(n-k)!}F_X^{k-1}(x)[1-F_X(x)]^{n-k}f_X(x)
$$ where $f_{X}$ is the pdf of our Uniform random variable(not to be mistaken with the f function described above) and F is the CDF of the aforementioned random variable. Using the said pdf and CDF and the linearity of expected value seems to get me nowhere, and I'm stuck.","['expected-value', 'statistics', 'probability']"
4425541,Probabilistic bound on difference of Lipschitz random function,"I am currently facing the following problem : Let $(X_1,Z_1),\ldots,(X_n,Z_n)$ be $n$ i.i.d. sample points from some distribution $p$ supported on $\mathcal X\times\{-1,1\}$ where $\mathcal X\subseteq \mathbb R^d$ is some compact subset. From this sample, I can define a function $$g :\lambda\mapsto g(\lambda\ |\ (X_i,Z_i)_{1\le i\le n})=:\theta_\lambda\in\mathbb R^p$$ Defined for any $\lambda >0$ . I do not know the explicit relation between $g$ and the $(X_i,Z_i)$ (it is the unique argmin of a function of the sample), but I know that $g$ is Lipschitz-continuous with respect to $\lambda$ (it is even differentiable if that helps). I would like to bound $\|\theta_\lambda-\theta_{\lambda/2}\| $ in probability. That is, for any positive $\varepsilon>0$ , I would like to get $$\mathbb P\left(\|\theta_\lambda-\theta_{\lambda/2}\| >\varepsilon\right)\le G(\varepsilon,n)\tag1 $$ Where $G$ is some deterministic function that goes to zero when $n$ goes to infinity. Here, the choice of norm $\|\cdot\| $ doesn't really matter since all norms in $\mathbb R^p$ are equivalent. My question : How to prove (or disprove ?) inequality $(1)$ ? Are some additional assumptions on $g$ needed for it to hold ? This a a bit different from usual concentration inequalities so I'm not sure how to proceed here. Thanks for any help. Update : I'm not 100% there yet, but I've made some progress. The way of getting an upper bound involving $n$ that occurred to me is by using a martingale difference sequence : First off, let's define the function $F:\lambda\mapsto\|g(\lambda)-g(\lambda/2)\|$ and the Martingale sequence $Y_k(\lambda) := \mathbb E[F(\lambda)\ |\ X_1,\ldots,X_k] $ . In my setting, $Y_0=0$ and so it follows that $Y_n =\sum_{k=1}^{n} Y_k - Y_{k-1} $ . By Azuma-Hoeffding inequality , it follows that $$\mathbb P\left(Y_n(\lambda)\ge\varepsilon\right) =\mathbb P\left(\|\theta_\lambda-\theta_{\lambda/2}\|\ge\varepsilon\right) \le\exp\left(\frac{-2\varepsilon^2}{2\sum_{k=1}^{n}c_k^2}\right)  $$ Where the $c_k$ are such that $|Y_k(\lambda) - Y_{k-1}(\lambda)|\le c_k$ . Now this isn't quite the desired result as the $c_k$ are not necessarily small enough (I want them of order $1/n$ ), but they can be upper bounded as follows : $$\begin{align}|Y_k-Y_{k-1}|&=\left|\mathbb E[F(\lambda)\ |\ X_1,\ldots,X_k]-\mathbb E[F(\lambda)\ |\ X_1,\ldots,X_{k-1}]\right|\\
&\le\mathbb E[F(\lambda)\ |\ X_1,\ldots,X_k]+\mathbb E[F(\lambda)\ |\ X_1,\ldots,X_{k-1}]\tag1\\
&=\mathbb E[\|g(\lambda)-g(\lambda/2)\|\ |\ X_1,\ldots,X_k]+\mathbb E[\|g(\lambda)-g(\lambda/2)\| |\ X_1,\ldots,X_{k-1}]\\ 
&\le\mathbb E[L_g\cdot \lambda/2\ |\ X_1,\ldots,X_k]+\mathbb E[L_g\cdot \lambda/2\ |\ X_1,\ldots,X_{k-1}]\tag2\\ 
&= \lambda/2\cdot\big(\mathbb E[L_g\ |\ X_1,\ldots,X_k]+\mathbb E[L_g\ |\ X_1,\ldots,X_{k-1}]\big)\end{align} $$ Where I used triangle inequality in $(1)$ and Lipschitz continuity of $g$ in $(2)$ . Now, for my desired result to hold, it is sufficient that there exists some constant $G$ such that the Lipschitz constant of $g$ is bounded by $G/n$ , but I think this is rather harsh... Can this approach be refined in any way to get the result under less restrictive assumptions ? I initially though that I could circumvent the problem by introducing a partition $$\lambda/2=:\lambda_0<\lambda_1<\ldots<\lambda_K:=\lambda $$ Such that each $|\lambda_i-\lambda_{i-1}|$ is smaller than $1/n$ , which would then allow me to conclude by splitting the probability accordingly, but sadly the subdivision depends on $n$ so it is not helpful...","['statistics', 'probability-theory', 'upper-lower-bounds', 'inequality']"
4425546,"Question on an improper integral of the form $\int_0^af(x,a)dx$","The question I'm about to ask isn't covered in our lectures, so pardon my ignorance. Suppose $f:(0,a]\times [c,d]$ . How is uniform convergence of the improper integral $\int_0^af(x,a)dx$ defined? Just to give an insight of what I've (hope so) learnt so far, I'm going to write down some definitions and results (and proofs) I think are relevant for functions $f:[a,+\infty)\times [c,d]$ that I found in the script by prof. Šime Ungar from 2004. It might be available here . $\underline{\boldsymbol{\text{ definition 1: }}}$ An improper integral $\int_a^\infty f(x)dx$ is defined as $\lim_{b\to\infty}\int_a^b f(x)dx$ under the condition that $f$ is integrable on $[a,b],\forall b>a$ and that the limit exists. In this case we also say the improper integral $\int_a^\infty f(x)dx$ converges. That is equivalent to the following condition: $f$ is integrable on $[a,b],\forall b>a$ and $$(\forall\varepsilon>0)(\exists a_0>a), \left|\int_b^c f(x)dx\right|<\varepsilon,\forall c>b>a_0.$$ Now, suppose we have a function $f:[a,+\infty)\times S\to\Bbb R,$ where $S\subseteq\Bbb R$ is an arbitrary set. Here we can observe convergence of the integral $\displaystyle\int_a^b f(x,y)dx,\forall y\in S.$ In this situation, the following definition makes sense: $\underline{\boldsymbol{\text{definition 2:}}}$ We say the improper integral $\int_a^\infty f(x,y)dx$ converges uniformly on $S$ if the integral converges $\forall y\in S$ and if $$\lim_{b\to\infty}\left(\sup_{y\in S}\left|\int_b^\infty f(x,y)dx\right|\right)=0.$$ $\underline{\boldsymbol{\text{result 1:}}}$ Suppose $\int_a^b f(x,y)dx$ exists $\forall b>a$ and $\color{red}{\forall y\in S}.$ Then, the improper integral $\int_a^\infty f(x,y)dx$ converges uniformly on $S$ if and only if $$(\forall\varepsilon>0)(\exists a_0>a), \color{red}{\sup_{y\in S}}\left|\int_b^c f(x,y)dx\right|<\varepsilon,\forall c>b\ge a_0.$$ $\boldsymbol{\text{ proof: }}$ $\boxed{\Rightarrow}$ Necessity of the conditions in the theorem is obvious. $\boxed{\Leftarrow}$ Let's prove thr sufficiency. From the conditions of the theorem, it first follows that, $\forall y\in S,$ the integral $\int_a^\infty f(x,y)dx$ converges. Furthermore, for $y\in S,$ since $\left|\int_b^c\right|<\varepsilon$ whenever $c>b>a_0,$ letting $c\to\infty,$ we have $\left|\int_b^\infty f(x,y)dx\right|\color{red}\le\varepsilon.$ But, as this holds $\color{red}{\forall y\in S},$ it is also true that $\color{red}{\sup_{y\in S}}\left|\int_b^\infty f(x,y)dx\right|\le\varepsilon$ and the claim follows. Something I believe is important (I'll write down my motivation in the end): $\underline{\boldsymbol{\text{Weierstrass M-test:}}}$ Suppose $\int_a^b f(x,y)dx$ exists $\forall b>a.$ If there is a function $M:[a,+\infty)\to\Bbb R$ s. t. $|f(x,y)|\le M(x),\forall x\in[a,+\infty)$ and $\forall y\in S$ and if $\int_a^\infty M(x)dx$ converges, then $\int_a^\infty f(x,y)dx$ converges (absolutely and) uniformly on $S$ . I've gone through the proof of the discrete version . If needed, I'll analyze this more carefully. Last result with the proof, I promise. $\underline{\boldsymbol{\text{result 2: }}}$ Suppose $f:[a,+\infty)\times [c,d]$ is continuous and that the following statements are true: $\exists y\in [c,d]$ s. t. the improper integral $\int_a^\infty f(x,y)dx$ converges $\partial_2f(x,y)$ exists and is continuous on $[a,+\infty)\times [c,d]$ $\int_a^\infty\partial_2f(x,y)dx$ converges uniformly on $[c,d].$ Then, the improper integral $\int_a^\infty f(x,y)dx$ exists and converges uniformly on $[c,d],$ and the function $F(y):=\int_a^\infty f(x,y)dx$ is differentiable on $[c,d]$ and $\color{purple}{F'(y)=\int_a^\infty\partial_2 f(x,y)dx}\forall y\in [c,d].$ $\boldsymbol{\text{ proof: }}$ Suppose that $\int_a^\infty f(x,y_0)dx$ converges. Apart from that, we also know that $\forall b>a$ and $\forall y\in [c,d]$ the integral $\int_a^b f(x,y)dx.$ Because of $2,\forall\varepsilon>0,\exists a_0$ s. t. $a_0<b<g\implies \left|\int_b^g\partial_2f(x,t)dx\right|<\varepsilon,\forall t\in [c,d]$ and at the same time $\left|\int_b^g f(x,y_0)dx\right|<\varepsilon.$ $$\begin{aligned}\left|\int_b^g(f(x,y)-f(x,y_0))dx\right|&=\left|\int_b^g\int_{y_0}^y\partial_2f(x,t)dtdx\right|\\&=\left|\int_{y_0}^y\int_b^g\partial_2f(x,t)dxdt\right|\\&\le\int_{y_0}^y\left|\int_b^g\partial_2f(x,t)dx\right|dt\\&\le (d-c)\varepsilon,\end{aligned}$$ whenever $a_0<b<g.$ $$\begin{aligned}\implies\left|\int_b^gf(x,y)dx\right|&\le\left|\int_b^g(f(x,y)-f(x,y_0))dx\right|+\left|\int_b^g f(x,y_0)dx\right|\\&\le(d-c)\varepsilon+\varepsilon\\&=(1+d-c)\varepsilon,\forall y\in [c,d]\end{aligned}$$ it follows that $\int_a^\infty f(x,y)dx$ exists and converges uniformly on $[c,d].$ Now, let's define a function $\color{purple}{G(y):=\int_a^\infty\partial_2f(x,y)dx}.$ Then $G$ is continuous (a result proven in the script priorly). It remains to notice that $$\int_c^ydt\int_a^\infty\partial_2f(x,t)dx=\int_a^\infty dx\int_c^y\partial_2f(x,t)dt.$$ In all the results, we used the fact the set was unbounded. In old materials, I've come across the following task: Prove that the  function $F(a)=\int_0^{1/a}\frac{e^{ax}-1}xdx$ is constant. I might be wrong, but I tried to use the substitution $x=\frac1t$ in order to get to an unbounded interval and then use the results above. This got a bit messy, so I was wondering if it makes sense to even consider the $\boldsymbol{\text{ result 2}}$ and write something as $F'(a)=\int_0^{1/a}\partial_2 f(x,a)dx.$ I'm primarily interested in justification for that. I apologize for the length of my post and thank you for reading in advance!","['multivariable-calculus', 'improper-integrals', 'real-analysis']"
4425579,Can every symmetric Jacobian matrix be a Hessian matrix?,"Say we have a function $\mathbf{f}(\mathbf{x})$ where $\mathbf{x}\in\mathbb{R}^n$ and $\mathbf{f}:\mathbb{R}^n\rightarrow\mathbb{R}^n$ with a Jacobian matrix $\mathbf{J} = \partial \mathbf{f}/\partial \mathbf{x}\in\mathbb{R}^{n\times n}$ .
If the Jacobian matrix $\mathbf{J}$ is real and symmetric everywhere, does a function $y:\mathbb{R}^n\rightarrow\mathbb{R}$ that satisfies $\mathbf{J}=\frac{\partial^2 y}{\partial \mathbf{x}\partial \mathbf{x}}$ exist?","['differential', 'jacobian', 'hessian-matrix', 'derivatives', 'differential-forms']"
4425586,Projection of a Sphere onto a Cylinder,"Source: Tristan Needham VDG I've been trying to determine the metric for the projection of a sphere onto a cylinder, where $0 \leq x \leq 2\pi R$ and $-R \leq y \leq R$ The book says to reason geometrically that $d\hat{s}^2 = \frac{\,dy^2}{R^2-y^2} + (R^2-y^2)\,dx^2$ where $d\hat{s}$ is the infinitesimal displacement on the sphere. I keep getting stuck at trying to determine what the individual changes $d\hat{s_1}$ due to $dx$ and $d\hat{s_2}$ due to $dy$ are, but I keep ending up with an $R^2$ in the denominator of the $dx$ term and an $R^2$ in the numerator of the $dy$ term. Any help would be greatly appreciated, I've beens staring at this one for quite a while! EDIT : I have discovered in the Errata for the book that my original solution was actually correct. Thank you everyone for taking the time to address my question!","['calculus', 'geometry', 'differential-geometry']"
4425596,prove that the sum of the elements in two subsets is the same,"Eight consecutive positive integers are partitioned into two subsets such that the sum of the squares in each subset is the same. Prove that the sum of the elements in each subset is also the same, assuming that the smallest element is at least 56. The method below is mostly a brute force approach. Is there a better method than brute force? For a set $S, $ let $f(S)$ denote the sum of the squares of its elements and let $g(S)$ denote the sum of the elements of S. Let the integers be $x + 1,\cdots, x + 8$ . The sum of the squares in each subset must be $\frac{1}2 \sum_{i=1}^8 (x+i)^2 = \frac{1}2 (8x^2 + 72x + 204) = 4x^2 + 36x+102.$ First we need to show that the partitions must in fact have the same number of elements. If one partition has more than $4$ elements, then its sum of squares will be at least $(x+1)^2 + (x+2)^2 + (x+3)^2 + (x+4)^2 + (x+5)^2 = 5x^2 + 30x + 55,$ which is too large (for $x\ge 11,$ since we would have $x^2 - 6x - 47 > 0$ ). So no subset can have more than 4 elements, implying than both subsets must have exactly 4 elements. One of the subsets, say $S_1$ , must contain $x+8$ , and so the sum of squares of the remaining three elements in this case must be $3x^2 + 20 x +38.$ Let $S_2$ be the other subset. Suppose this subset also contains $x+7$ . Then the remaining two elements have a sum of squares equal to $2x^2 + 6x -11.$ The minimum possible sum of squares of two elements is $(x+1)^2 + (x+2)^2 = 2x^2 + 6x + 5$ , so we cannot have $x+7$ in $S_1$ . If $x+6$ is in $S_1$ , then $f(S_1\backslash \{x+8, x+6\}) = 2x^2 +8x+2$ . The only two possible remaining elements are $x+1$ and $x+2$ , as $(x+1)^2 + (x+3)^2 > 2x^2 + 8x+2$ , and is the second smallest sum of squares of elements. But then $f(S_1) < 2X^2 + 8x+2$ , a contradiction. So suppose $x+5\in S_1$ . $f(S_1\backslash \{x+8,x+5\} ) = 2x^2 + 10x + 13,$ and in this case one can verify that $S_1 = \{x+8, x+5, x+2, x+3\}$ , since all other choices of two remaining elements give either a smaller or larger sum of squares than $2x^2 + 10x+13$ . In this case, $g(S_1) = 4x+18$ , so $g(S_1) = g(S_2)$ . We know $x+7$ and $x+6$ are in $S_2 $ . So we need to find the remaining two elements of $S_2$ . If $x+5$ is in $S_2$ , then the square of the remaining element in $S_2$ must be $ 4x^2 + 36x + 102 - ((x+7)^2 + (x+6)^2 + (x+5)^2) = x^2 - 8,$ which is clearly impossible. So the only possibility is $S_1 = \{x+8, x+5, x+2, x+3\}$ .","['elementary-set-theory', 'elementary-number-theory', 'contest-math']"
4425636,What are the Littlewood-Richardson coefficients for hook-shaped diagrams?,"I am trying to compute Littlewood-Richardson coefficients involving hook-shaped diagrams. In particular, if $\lambda$ is a hook-shaped Young diagram, $\rho$ is any diagram, and we consider, $$
s_\lambda s_\rho = \sum_\nu c_{\lambda \rho}^\nu s_\nu
$$ is there a simple expression of the sum over $\nu$ on the right? The reason I suspect a simplification is because of the Pieri formulas, which state that for a single row $\lambda =(r) $ and $\rho $ arbitrary, $$
s_\rho s_{(r)} = s_\rho h_r=\sum_\kappa s_\kappa~,
$$ where the sum is over all $\kappa$ so that $\kappa/\rho$ is a horizontal strip of weight $r$ and $h_k$ is the complete homogeneous symmetric polynomial. Similarly for $\lambda = (1^s)$ a column, $$
s_\rho s_{(1^s)} =s_\rho e_s= \sum_\kappa s_\kappa~,
$$ where the sum is now over $\kappa$ so that $\kappa/\rho$ is a vertical strip of weight $s$ and $e_s$ is the elementary symmetric polynomial. For a hook-shaped partition $(r,1^s)$ , we have $$
s_{(r,1^s)}=\sum_{k=0}^s (-1)^k h_{r+k} e_{s-k}~,
$$ so it is possible to calculate the product of a hook-shape with another diagram by applying the Pieri formulas to the above expansion of $s_{(r,1^s)}$ . However, this will generally give a complicated expression with many terms cancelling out, so I suspect that a simpler expression exists. Any help is much appreciated.","['representation-theory', 'combinatorics', 'symmetric-polynomials']"
4425659,"If task start time is chosen from a uniform random distribution, is inter-arrival time exponentially distributed","Question: If the start time is generated from uniform random distribution, is the inter arrival time exponentially distributed? Context:
We have a number of tasks to be run every day (and we have the flexibility to set the actual start time). To avoid overloading the system by running everything at a fixed time, and we want to spread the load through out the day. And everyday we want to run the task at the same time. To spread the load, we hash the taskid to generate the start time. (Almost equivalent to generating from uniform random distribution) The question is, since the arrival time is uniformly distributed and independent of each other, does the inter arrival time follow exponential distribution? I want to use queueing theory to model the system performance, since exponentially distributed inter arrival time is the most studied distribution, I want to see if I can use M/M/1 queuing model. I understand there are some differences. The start time generated is discrete, but the exponential distribution is continuous. I assume, I can increase the granularity of the time to approximate continuous time. Another option is to generate the start time as a Poisson process (At every t, decide whether to run or not with probability p. ) But at some point, we want to ensure the task runs every day, so we might have to force it to run at midnight eventually. So this might get a bit weird.","['statistics', 'probability-distributions', 'markov-chains', 'queueing-theory', 'probability']"
4425714,Can a norm on polynomials be supermultiplicative?,"A norm on a real algebra is supermultiplicative when $\lVert f\cdot g\rVert\geq\lVert f\rVert\cdot\lVert g\rVert$ for all $f$ and $g$ in the algebra. Is there a supermultiplicative norm on $\mathbb R[x]$ ? This is a one-sided form of my previous question . An answer to either question could provide an answer to the other. I suspect that the norm given by $\Big\lVert\sum_ka_kx^k\Big\rVert=\max_k(k!|a_k|)$ , or equivalently by $\Big\lVert\sum_ka_kx^k/k!\Big\rVert=\max_k|a_k|$ , is a multiple of a supermultiplicative norm (so $\lVert f\cdot g\rVert\geq C\lVert f\rVert\lVert g\rVert$ for some constant $C>0$ ). Is this true?","['normed-spaces', 'polynomials', 'functional-analysis', 'upper-lower-bounds']"
4425729,Partial Derivative Chain Rule When Variables Are Not Independent,"Let's say, $x$ is a function of $t$ ( $x = x(t)$ ) and $y$ is a function of $t$ ( $y = y(t)$ ). And, $f$ is a function of $x$ and $y$ ( $f = f(x, y)$ ). Then by the chain rule $$\frac{df}{dt} = \frac{\partial f}{\partial x}\frac{dx}{dt} + \frac{\partial f}{\partial y}\frac{dy}{dt}$$ . However, when you take the partial derivative of a function wrt a variable you keep all other variables constant. So, I am not sure how $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ above can be calculated, since you cannot keep $y$ constant if $x$ is allowed to be varied and vice-versa. Can someone please explain what I am understanding wrong?","['partial-derivative', 'multivariable-calculus', 'chain-rule']"
4425737,The Closure of Convex Hull of an orthonormal basis in a separable Hilbert space has empty interior,"Given $H$ a separable Hilbert space and $\left\{e_n\right\}$ an orthonormal basis, then $int(\overline{Conv(\left\{e_n\right\})}) = \emptyset.$ I don't know where to start, one thing I know is $\overline{span(\left\{e_n\right\})} = H$ , will Baire's Category Theorem be used in the proof? Could anyone give some hints? I come up with a solution today but I do not know if it is correct: Sol. ( I have edited the proof again! ) First, set $A_n := conv({e_1,e_2,...,e_n})$ , it is not difficult to show that $A_n$ is closed. I claim that for each $n \in \mathbb{N}$ , $int(A_n) = \emptyset$ . Otherwise, suppose $\exists N$ such that $int(A_N) \neq \emptyset$ , then it contains some open ball, say $$
B(x_0;\delta) \subset A_N,
$$ $\exists a_1, a_2,...,a_n \in \mathbb{R}^+$ and $\sum_{i=1}^n a_i = 1$ such that $x_0 = \sum_{i=1}^n a_i e_i$ , by choosing $\epsilon < \delta / \sqrt{2}$ , I can show $$
y := a_1 e_1 + a_2e_2 + ...+a_{N-1}e_{N-1} + (a_N - \epsilon)e_N + \epsilon e_{N+1} \notin A_N,
$$ but $$
\|y - x_0\| < \delta,
$$ this gives a contradiction. Finally, we have (It is not easy to show, but true) $$
\overline{conv(\left\{e_n\right\})} = \overline{\bigcup_{i=1}^{\infty}A_n},
$$ Note that a countable union of meager sets is still meager, thus, $int(\bigcup_{i=1}^{\infty}A_n) = \emptyset$ , also, $int(cl(\bigcup_{i=1}^{\infty}A_n)) = int(\bigcup_{i=1}^{\infty}A_n) = \emptyset$ (some topological facts of convex sets are used...) $\square$","['hilbert-spaces', 'functional-analysis']"
4425739,Why is Green's theorem a special case of Stokes theorem?,"I have already seen related questions and don't understand. Please help me. $\oint_C \mathbf{A} \cdot d\mathbf{r} = \iint_S (\nabla x \mathbf{A})\cdot \mathbf{n}$ dS Let $A \leq P,Q,0>$ Then $\nabla x \mathbf{A}= <0,0,(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} )>$ . So $\iint_S (\nabla x \mathbf{A})\cdot \mathbf{n}$ dS $=\iint_S <0,0,(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} )>\cdot \mathbf{n}$ dS $=\iint_S <0,0,(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} )>\cdot \mathbf{k}$ dS $=\iint_S (\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} )$ dS Now, here people replace dS with dA and then say its equal to green's theorem but why would be able to do that?
Isn't
dS = $|r_u x r_v|$ dA Please see screenshot or this second screenshot. Why do we take unit normal vector. We don't do that when going from dS to dA as seen in this screenshot. in this third screenshot, Wikipedia does it but doesn't explain the step I struggle with.","['greens-theorem', 'multivariable-calculus', 'stokes-theorem']"
4425753,"Verifying $\{0,1, \ldots, n-1\}$ is a group","I'm trying to verify that the set $S = \{0,1, \ldots, n-1\}$ ( $n > 1$ ) is a group under the operation $$
(a,b) \mapsto \begin{cases}
a + b & \text{ if $a + b \leq n-1$} \\ 
a + b - n & \text{ if $a+b > n - 1$}
\end{cases} 
$$ I know that this is in effect $\mathbb{Z}/n\mathbb{Z}$ , but at this point in the lecture notes I'm working through, equivalence classes have not been introduced, so I need to prove that this set under the given operation is a group. Here is my attempt. I'm going to denote addition of two elements of $S$ by $+_n$ to distinguish it from addition in $\mathbb{Z}$ , denoted $+$ . Closure. Given $a,b \in S$ , we either have $a + b \leq n-1$ or $a + b > n-1$ . If $a + b \leq n-1$ , then $a +_{n} b = a + b \in S$ . If $a + b > n-1$ , then $a +_{n} b = a + b - n$ . We need to check that $0 \leq a + b - n \leq n-1$ . If $a + b > n - 1$ , then $a + b - n > (n-1) - n = -1$ . As a sum of integers is another integer, we have $a + b - n \geq 0$ . Furthermore, $a,b \in S$ , so $a,b \leq n - 1$ . Therefore, in $\mathbb{Z}$ , $a + b \leq (n-1) + (n-1) = 2n - 2$ . Therefore $a + b - n \leq (2n - 2) - n = n - 2 \leq n - 1$ . Therefore, $a +_{n} b \in S$ . Identity. I claim that $0$ is the identity element in $S$ . Fix $a \in S$ . Then $0 \leq a \leq n - 1$ , so in $\mathbb{Z}$ , $0 \leq a + 0 \leq n - 1$ , so $a +_{n} 0 = a + 0 = a$ . Similarly, as $a + 0 = 0 + a$ in $\mathbb{Z}$ , we have $0 +_{n} a = a$ . Inverses. Fix $a \in S$ . First, $0 + 0 = 0$ , so $0$ is its own inverse, so we assume $a > 0$ . Then $n - a < n$ , i.e., $n - a \leq n - 1$ . Further, as $a < n$ , $n - a \geq 0$ , so $n - a \in S$ . Consider $a + _{n} (n-a)$ . In $\mathbb{Z}$ , we have $a + (n-a) = 0 \leq n - 1$ , so we have $a +_{n} (n-a) = 0$ . Similarly, $(n-a) +_{n} a = 0$ . The last thing I need to check is associativity. I don't know of a good way to check this other than by cases. If I take $a,b,c \in S$ , I need to show that $$
(a +_{n} b) +_{n} c = a +_{n} (b +_{n} c). 
$$ This produces four cases (considered in $\mathbb{Z}$ ) for the left-hand side: \begin{align*}
a + b \leq n - 1, \; (a + b) + c \leq n - 1 \\ 
a + b \leq n - 1, \; (a + b) + c > n - 1 \\ 
a + b > n - 1, \; (a + b - n) + c \leq n - 1 \\ 
a + b > n - 1, \; (a + b - n) + c > n  
\end{align*} There are similarly four cases for the right-hand side, just swapping $a$ and $b$ with $b$ and $c$ and $c$ with $a$ . Therefore, there are $4 \cdot 4 = 16$ cases in total. I can check all of these manually, but my question is whether this is the way to go. I want to say this result in some way ""follows by associativity in $\mathbb{Z}$ (since when I prove the result with equivalence classes, it does) but the operation is completely different here, so that isn't as clear to me. I'd appreciate if someone could check over my work on closure, identity, and inverses and let me know if I'm on the right track in proving associativity.","['group-theory', 'solution-verification']"
4425774,Finding basis for the differential equation,"So the question is to find a basis for the solution of following differential equation that and when $y(0)=3,y'(0)=2$ what is the solution $y'' + 2y' + y = 0$ So my attempt was to put it as auxiliary polynomial,which $p(t) = t^2 +2t +1$ $=(t+1)^2$ Hence zeros are -1 and -1, so that $\{e^{-t},te^{-t}\}$ So the solution is that $y(t)= b_1e^{-t} + b_2te^{-t}$ $y(0)= b_1e^{-0} + b_2(0)e^{-0}$ $=b_1 = 3$ $y'(t)= -b_1e^{-t} + b_2e^{-t} - b_2te^{-t}$ $=-b_1e^{-t} + b_2(1-t)e^{-t}$ $y'(0)= -b_1e^{-0} + b_2(1-0)e^{-0}$ $=b_2-b_1=2$ , which $b_2 = 5$ Hence, the solution $y(t)= 3e^{-t} + 5te^{-t}$ Am I doing it right? New to linear Algebra","['solution-verification', 'ordinary-differential-equations']"
4425777,"Prove that $f(x,y)+f(y,z)\ge f(x,z)$ where $f(x,y)=\sqrt{x\ln x+y\ln y-(x+y)\ln(\frac{x+y}2)}$","Denote $f(x,y)=\sqrt{x\ln x+y\ln y-(x+y)\ln(\frac{x+y}2)}$ . Show that $f(x,y)+f(y,z)\ge f(x,z)$ for $x,y,z> 0$ . This is a question from a friend, which is a deep learning homework. It looks like some square roots of KL divergence, but it seems no help. Some other friends have tried to square it but dealing with the crossing terms like $xy\ln x \ln y$ makes it tough. Some friends and the asker himself try to take the derivative of $y$ and calculate the minimum but to no avail... Some students suggested that it can be written as an integral. I don't have any idea, so I ask here.","['inequality', 'entropy', 'real-analysis']"
4425783,"How to find the closed form of $\int_{0}^{\frac{\pi}{2}} \frac{d x}{\left(p \cos ^{2} x+q \sin ^{2} x+r\right)^{n}}$, where $n\in N$?","In my post , I found the integral $$\int_{0}^{\frac{\pi}{2}} \frac{d x}{\left(a^{2} \cos ^{2} x+b^{2} \sin ^{2} x\right)^{2}}=\frac{\pi\left(a^{2}+b^{2}\right)}{4 a^{3} b^{3}}$$ Then  I want to find the generalized integral using similar technique by using the integral $$
I(p, q, r)=\int_{0}^{\frac{\pi}{2}} \frac{d x}{p \cos ^{2} x+q \sin ^{2} x+r} \quad \text{ where }p+r> 0 \textrm{ and }  q+r>0.
$$ Letting $t=\tan x$ yields $$
\begin{aligned}
I(p, q, r)&=\int_{0}^{\infty} \frac{d t}{p+qt^{2}+r\left(1+t^{2}\right)}\\
&=\int_{0}^{\infty} \frac{d t}{(q+r) t^{2}+p+r}\\&=\frac{1}{\sqrt{(p+r)(q+r)}} \left.\tan ^{-1}\left(\frac{t\sqrt{q+r} }{\sqrt{p+r}}\right)\right]_{0}^{\infty}\\
&=\frac{\pi}{2 \sqrt{(p+r)(q+r)}}
\end{aligned}
$$ Differentiating it w.r.t. $r$ by $n$ times yields $$
\begin{aligned}\int_{0}^{\frac{\pi}{2}} \frac{(-1)^{n}n! d x}{\left(p \cos ^{2} x+q \sin ^{2} x+r\right)^{n+1}}=\frac{\pi}{2 }\cdot \frac{\partial^{n}}{\partial r^{n}}\left(\frac{1}{\sqrt{(p+r)(q+r)}}\right) \\
\boxed{\int_{0}^{\frac{\pi}{2}} \frac{d x}{\left(p \cos ^{2} x+q\sin ^{2} x+r\right)^{n+1}}=\frac{(-1)^{n} \pi}{2n !} \frac{\partial^{n}}{\partial r^{n}}\left(\frac{1}{\sqrt{(p+r)(q+r)}}\right)}
\end{aligned}
$$ Using the formula, we can evaluate $$
\int_{0}^{\frac{\pi}{2}} \frac{d x}{\left(p \cos ^{2} x+q \sin ^{2} x+r\right)^{2}}=\frac{-\pi}{2} \frac{\partial}{\partial r}\left(\frac{1}{\sqrt{(p+r)(q+r)}}\right)= \frac{\pi(p+q+2r)}{4\left[(p+r)( q+r)\right]^{\frac{3}{2}}}
$$ For higher derivative, applying Leibniz’s Rule gives $$
\begin{aligned}\frac{d^{n}}{d r^{n}}\left[(p+r)^{-\frac{1}{2}}(q+r)^{-\frac{1}{2}}\right]&= (-1)^n \sum_{k=0}^{n}\left(\begin{array}{l}
n \\
k
\end{array}\right)\left(\frac{1}{2} \right)_k(p+r)^{-\frac{1}{2}-k}\left(\frac{1}{2} \right)_{n-k}(q+r)^{-\frac{1}{2}-n+k} \\&=\frac{(-1)^n}{(q+r)^{n} \sqrt{(p+r)(q+r)}} \sum_{k=0}^{n} \binom{n}{k} \left(\frac{1}{2}\right)_{k}\left(\frac{1}{2}\right)_{n-k}\left(\frac{q+r}{p+r}\right)^{k}\end{aligned}
$$ where $\displaystyle (\alpha)_n=\frac{\Gamma(\alpha+n)}{\Gamma(n)} $ . Now we can conclude that for any natural number $n\geq 2$ , $$
\int_{0}^{\frac{\pi}{2}} \frac{d x}{\left(p\cos ^{2} x+q\sin ^{2} x+r\right)^{n+1}}\\=\boxed{\frac{\pi}{2n!(q+r)^{n} \sqrt{(p+r)(q+r)}} \sum_{k=0}^{n} \binom{n}{k}\left(\frac{1}{2}\right)_{k}\left(\frac{1}{2}\right)_{n-k}\left(\frac{q+r}{p+r}\right)^{k}}
$$ Question: Is there any other solution?","['integration', 'calculus', 'trigonometry', 'gamma-function']"
4425798,Prove that $X \cap (Y \cap Z) = (X \cap Y) \cap Z$,"I have a question about sets, I need to prove that: $$X \cap (Y \cap Z)  = (X \cap Y) \,\cap \,Z$$ I tried to do something like this: $$1. \; X \cap (Y \cap Z) = \{a: a \in X\} \cap \{a: a \in Y \land a \in Z \}$$ $$2. \;= \{a: a \in X \land a \in Y \land a \in Z \}$$ $$3\;=\{a: \in X \land a \in Y \} \cap \{a: a \in Z\}$$ $$4. \; = (X \cap Y) \cap Z$$ I will be glad to hear what should I do after that, or how can I understand if my attempt was success?",['elementary-set-theory']
4425901,What's the difference between a total basin of attraction and an immediate basin of attraction?,"I understand that for an attracting fixed point $\hat{p}$ of a holomorphic self-map defined on some Riemann surface $S$ we define the total basin of attraction as $\mathcal{A}=\text{Bas}(\hat{p})=\{p\in S: (f^{\circ n}(p))_{n\geq 1}\to\hat{p}\}$ . Now, in his book, Dynamics of One Complex Variable (Pg. 79 - 3rd-edition), Milnor defines what's called an immediate basin of attraction $\mathcal{A}_0$ to be the connected component of $\mathcal{A}$ which contains $\hat{p}$ . I don't understand how this is any different from the total basin of attraction of $\hat{p}$ . Can someone provide an example that can clears the difference between these two ideas? I am attaching a plot of the filled Julia set of $f(z)=z^5+(0.8+0.8i)z^4+z$ which has the following fixed points $\hat{p_1} = 0$ with multiplier $|\lambda_1| = |f'(0)| =1$ (parabolic), $\hat{p_2} = -0.8-0.8i$ with multiplier $|\lambda_2| = |f'(-0.8-0.8i)| \approx |-13.7| > 1$ (repelling), $\hat{p_3} = \infty$ with multiplier $|\lambda_3| = |\lim_{z\to\infty} f(z)|^{-1} = 0$ (super-attracting) Since the only attracting fixed point here is $\infty$ , $\text{Bas}(\infty)=\mathcal{A}=\{p\in\hat{\mathbb{C}}: (f^{\circ n}(z))_{n\geq 1}\to\infty\}$ . But what is $\mathcal{A}_0$ in this case?","['complex-analysis', 'basins-of-attraction', 'complex-dynamics', 'polynomials']"
4425999,General Rule for Differentiation of Tetrations,"I'll start at the beginning. Initially, this sort of began as just what is $\frac{d}{dx}$ [ $x^x$ ] the answer being $x^x$ +ln(x) $x^x$ . This wasn't difficult to achieve, just some chain rule and product rule. I then took $\frac{d}{dx}$ [ $x^{x^x}$ ], this took some more chain and product rule but one should end up with an answer of ( $x^{x^x}$$x^x$$)\frac{1}{x}$ + $x^{x^x}$$x^x$ ln $(x)$ + $x^{x^x}$$x^x$ ln $^2(x)$ . Evidently, I now could just take the $\frac{d}{dx}$ [ $x^{x^{x^x}}$ ], but I saw the pattern with the other derivatives and thought that there might be a general formula or rule for the derivatives of tetration. After a search online, and on the exchange, it led to no nothing. So I decided to differentiate the 4th tetration of x, which is $x^{x^{x^x}}$ . For future reference the 4 tetration of x, I'll call $x^{x|4}$ . Additionally, I'll call $x^{x^{x^x}}$ * $x^{x^x}$ -> $x^{x|4,3}$ just to make my life easier typing this. The $\frac{d}{dx}$ [ $x^{x|4}$ ], is equal to ( $x^{x|4,3}$ + $x^{x|4,2}$ ln $(x)$ ) $\frac{1}{x}$ + $x^{x|4,2}$ *ln $^2(x)$ + $x^{x|4,2}$ *ln $^3(x)$ From here I noticed a pattern, to where I could accurately predict the $\frac{d}{dx}$ [ $x^{x|n}$ ] (The derivative of the nth tetration of x) The general pattern is: $\frac{d}{dx}$ [ $x^{x|2}$ ] = $x^{x|2}$ + $x^{x|2}$ ln $(x)$ $\frac{d}{dx}$ [ $x^{x|3}$ ] = $\frac{x^{x|3,2}}{x}$ + $x^{x|3,2}$ ln $(x)$ + $x^{x|3,2}$ ln $^2(x)$ $\frac{d}{dx}$ [ $x^{x|4}$ ] = $\frac{x^{x|4,3}}{x}$ + $\frac{x^{x|4,2}ln(x)}{x}$ + $x^{x|4,2}ln^2(x)$ + $x^{x|4,2}$ ln $^3(x)$ Some rules of the pattern are as follows: All terms except for the last two will be over x. The maxima, which is the highest tetration, will always be the number of tetrations in the original tetration that is being differentiated. The minima, the lowest tetration, will be the maxima-1 then count down to 2, where it will remain until the addition ends. The exponent on the logarithm starts at zero and counts up until it reaches the maxima-1. Note: Maxima and minima aren’t accepted terms so use your own as you wish. For example: The first segment of the $\frac{d}{dx}$ [ $x^{x|4}$ ]. $\frac{x^{x|4,3}}{x}$ notice how it is over x, the maxima is 4, the minima is 4-1, and there is no logarithm because ln $^0(x)$ =1. Then one would add, $\frac{x^{x|4,2}ln(x)}{x}$ this segment is also over x and has a maxima of 4 and a minima of one less than the previous. Additionally, this segment has an ln $(x)$ , this is because ln $^1(x)$ = ln $(1)$ , no surprises. This pattern continues bound by these rules, so now the $\frac{d}{dx}$ [ $x^{x|10}$ ]. However, I wasn’t satisfied with this answer; because I wanted a general formula. I had my friend, who’s a lot better at math than me, assist me in finding a general formula. He eventually devised $\frac{d}{dx}[x^{x|n}]$ : $$\sum_{k=0}^{n-2}\frac{x^{x|n,n-k}ln^{k-1}(x)}{x}+x^{x|2}ln^{n-2}+x^{x|2}ln^{n-1}, (n\ge2)$$ We've gone through the mathematical rigor, and so far this formula has seemed to uphold. The main reason I’m writing this is that I have a few questions about this formula and the general idea of there being a general formula. Can this formula be disproved? Is there a nicer-looking formula? Is there a formula with proper notation, not my made-up one? Can a formula be made so that n is a fraction or even a complex number?","['calculus', 'tetration', 'sequences-and-series']"
4426041,"$a_1=\sqrt{2}$, $a_{n+1}=\sqrt{2}(a_n+\frac{1}{a_n})$. Find $\lim_{n\to\infty}\frac{a_n}{2^{n/2}}$.","$a_1=\sqrt{2}$ , $a_{n+1}=\sqrt{2}\left(a_n+\dfrac{1}{a_n}\right)$ . Find $$\lim_{n\to\infty}\frac{a_n}{2^{n/2}}$$ Let $b_n=\dfrac{a_n}{2^{n/2}}$ , then $b_{n+1}=b_n+\dfrac{1}{2^nb_n}$ , then $b_n\nearrow$ . So $\lim_{n\to\infty} b_n$ exists or $\infty$ . How to find it?","['limits', 'calculus']"
4426113,How to prove $ (3\Bbb N + 1) \cap (5\Bbb N + 3) = 15\Bbb N + 13$ [duplicate],"This question already has an answer here : Prove that $c + \mathrm{lcm}(m,s)\mathbb Z=(m\mathbb Z+k)\cap(s\mathbb Z+t)$ for every $c$ in $(m\mathbb Z+k)\cap(s\mathbb Z+t)$ (1 answer) Closed 2 years ago . I'm trying to prove that: $$ (3\Bbb N + 1) \cap (5\Bbb N + 3) = 15\Bbb N + 13$$ What have I done so far is, I'm getting an x from left hand side of equation: $$
x \in (3\Bbb N + 1) \cap (5\Bbb N + 3)
$$ Then I'm simplifying it, like this: $$
x \in (3\Bbb N + 1) \land x \in (5\Bbb N + 3)
$$ $3\Bbb N + 1$ is basicly $1 \pmod{3}$ , and $(5\Bbb N + 3)$ is basicly $3 \pmod{5}$ , thus: $$
x \equiv 1 \pmod{3} \; and \; x\equiv 3 \pmod{5}
$$ Use CRT (i.e. https://en.wikipedia.org/wiki/Chinese_remainder_theorem ) to conclude result for left hand side: $$
a_1 = 1, \; m_1 = 5, \; y_1 \equiv 2 \pmod{3}
$$ After that,  Use CRT to conclude result for right hand side: $$
a_2 = 3, \; m_2 = 3, \; y_2 = 2 \pmod{5}
$$ Then plug in: $$
\begin{split}
x &\equiv a_1m_1y_1 + a_2m_2y_2 \pmod{M} \\
  & \equiv 1\cdot 5\cdot 2 + 3\cdot 3 \cdot 2 \pmod{3\cdot 5} \\
  & \equiv 28 \pmod{15}
    \equiv 13 \pmod{15} 
\end{split}
$$ then find for the right side; $$
x \in (15\Bbb N + 13) \implies x = 13 \pmod{15}
$$ Did I prove it right? Is there an easier way to do this?","['elementary-set-theory', 'modular-arithmetic']"
4426169,Volume integral of $(x+y+z)^{2n}$ over unit sphere,"$\iiint_{V}(x+y+z)^{2n}\,dV$ where $V$ is the unit sphere I tried converting to spherical polars but ended up needing to solve $$\int_0^\pi\int_0^{2\pi}(\sin\theta\cos\phi+\sin\theta\sin\phi + \cos\theta)^{2n}\sin\theta\,d\phi\,d\theta$$ which I cannot see a way of doing, anyone have any ideas?",['multivariable-calculus']
4426193,I have a little problem trying to understand differentials,"I have just learned differentials and I have a few things unclear. For example I found a contradiction trying to differentiate the function $f(f(x))$ , where $f:\mathbb{R} \to \mathbb{R}$ .
Using the chain rule we get $$(f(f(x)))'=f'(f(x))f'(x)$$ However, if we use the differentials we get that $$\frac{d(f(f(x)))}{dx}=\frac{df}{df}\frac{df}{dx}=\frac{df}{dx}$$ This is true if $f(x)$ is a constant or $f(f(x))=f(x)$ . Why doesn't this work for the general case?","['analysis', 'real-analysis', 'calculus', 'derivatives', 'chain-rule']"
4426197,Increasing sequence of permutation of an arithmetic progression,"Let $\sigma:\mathbb{N}\rightarrow \mathbb{N}$ be a permutation of the set $\mathbb{N}$ of all positive integers. Show that there must be an arithmetic progression $a,a+d,a+2d,a+3d$ where $d>0$ such that $\sigma(a)<\sigma(a+d)<\sigma(a+2d)<\sigma(a+3d)$ ? I could prove that $\sigma(a)<\sigma(a+d)<\sigma(a+2d)$ but I couldn't prove further.","['number-theory', 'combinatorics']"
4426228,Raising a matrix to the infinite power,"How do I raise a matrix to the infinite power? I know that the main method for doing this is by diagonalizing the matrix, but what if I can't? For example, let's say I have the matrix \begin{bmatrix}0&0&0&0&0\\2/3&0&0&0&0\\1/3&0&1&0&0\\0&3/7&0&1&0\\0&4/7&0&0&1\end{bmatrix} You can see that when I try diaganolizing the matrix in Mathematica, the eigenvector matrix is singular, so I'm unable to take its inverse. However, I know that when I raise this matrix to the power of infinity, I know I get the following matrix \begin{bmatrix}0&0&0&0&0\\0&0&0&0&0\\1/3&0&1&0&0\\2/7&3/7&0&1&0\\8/21&4/7&0&0&1\end{bmatrix} Is there any general algorithm or formula or steps I can take to get there?","['matrices', 'limits', 'matrix-calculus']"
4426296,Pointwise stabiliser of a union of orbits is a normal subgroup,"Let a group $G$ act on a set $S$ . Define the set orbit and pointwise stabiliser for all $A\subseteq S$ , \begin{align*}
G\cdot A&:=\{g\cdot a:g\in G,a\in A\}\\
\textrm{Stabp}_G(A)&:=\{g\in G:(\forall a\in A)(g\cdot a = a)\}
\end{align*} Let $X\subseteq S$ . Then we have $$\textrm{Stabp}_G(G\cdot X)\trianglelefteq G$$ I would like a source that refers to this, please. I am interested in this and any related results. I'm quite sure it's true - I wrote a proof, but haven't verified it with someone else. If someone could, that would be great. If the action is faithful, I also wonder if there is some sort of converse statement that can be made? (Normal subgroup $\implies$ ???) Proof: By definition, $\textrm{Stabp}_G(G\cdot X)=\{n\in G:(\forall h\in G,x\in X)(n\cdot(h\cdot x)=h\cdot x)\}$ . Let $g\in G$ , $n\in\textrm{Stabp}_G(G\cdot X)$ . Let $m=gng^{-1}$ . Let $h\in G$ , $x\in X$ . Then \begin{align}
n\cdot (h\cdot x) = h\cdot x
&\iff&
(gng^{-1}ghg^{-1})\cdot x &= (ghg^{-1})\cdot x\\
&\iff&
(mghg^{-1})\cdot x &= (ghg^{-1})\cdot x
\end{align} Note $gGg^{-1} = G$ , so letting $k=ghg^{-1}$ , we've shown for all $k\in G$ , $x\in X$ , $$(mk)\cdot x= k\cdot x$$ Hence $m=gng^{-1}\in\textrm{Stabp}_G(G\cdot X)$ .","['reference-request', 'solution-verification', 'normal-subgroups', 'group-theory', 'group-actions']"
4426329,Making a matrix with an unknown diagonalizable,"I'm stuck on a question about diagonalizability of matrices and need some help moving forward.I think I made some progress but I'm also not sure if its any good.  Let $a \in \mathbb{R}$ , $A$ is a matrix. For which values of $a$ is the matrix $A$ diagonalizable. $$A:= \begin{pmatrix}
1 & 2 & 3 & 4\\
0 & 2 & a & 5\\
0 & 0 & 2 & 6\\
0 & 0 & 0 & 7 \\
\end{pmatrix} \in \mathbb{R^{4\times4}}$$ Firstly, if I'm not mistaken, any matrix whose charachteristic polynomial is a product of distinct linear factors can be diagonalized. So I simply tried to find the characteristic polynomial first. $$ p_A(x)= \det(xE_n-A)=\det \begin{pmatrix}
x-1 & -2 & -3 & -4\\
0 & x-2 & -a & -5\\
0 & 0 & x-2 & -6\\
0 & 0 & 0 & x-7 \\
\end{pmatrix}= (x-2) \times \det \begin{pmatrix}
-2 & -a & -5 \\
x-2 & x-2 & -6 \\
0 & 0 & x-7 \\
\end{pmatrix}.$$ $$ (x-2) \times \det \begin{pmatrix}
-2 & -a & -5 \\
x-2 & x-2 & -6 \\
0 & 0 & x-7 \\
\end{pmatrix}=-a^2 x^3 + 2 a^2 x^2 - 9 a x^2 + 32 a x - 28 a - 2 x^3 + 23 x^2 - 66 x + 56.$$ Unless there is an error, this should be the characteristic polynomial. From here I just checked some simple cases like what if $a=0,a=1,a=2$ and found that if $a=2$ then $p_A(x)=-6x^3+13x^2-2x=(-x)(x-2)(6x-1).$ From here however I don't know how to generalize this for any other $a$ or if there is a way to check whether this is the only possible value for $a$ .","['matrices', 'diagonalization', 'linear-algebra']"
4426332,Proof verification: conditional distribution of integral of brownian motion,"I am looking to compute the conditional distribution of $$S_T=\int_0^T W_t dt$$ given $W_T=x$ . Thanks to this question and using the fact that $d(tW_t)=W_tdt + tdW_t$ by Itô's formula, we get \begin{align*}
    W_t dt &= d(tW_t) - t dW_t\\
    S_T=\int_0^T W_t dt & = T W_T - \int_0^T t dW_t
\end{align*} and hence $\mathbb{E}[S_T|W_T = x]= Tx$ because the second term is the stochastic integral of a deterministic process, which is normally distributed with mean 0. Furthermore, \begin{alignat*}{2}
    \text{Var}(S_T|W_T=x) & = \text{Var}\left(Tx - \int_0^T t dW_t\right)\\
    &= \text{Var}\left(\int_0^T t dW_t\right)\\
    & = \mathbb{E}\left[\left(\int_0^T t dW_t\right)^2\right] &&\text{ null mean}\\
    &= \int_0^T t^2 dt &&\text{ isometry}\\
    & = \frac{T^3}{3}
\end{alignat*} Is this correct?","['stochastic-integrals', 'normal-distribution', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4426370,Series expansion in binomial coefficients,"After noticing that the first three terms of OEIS A016269 coincide with those of OEIS A030053 and the fourth differs only by $1$ , and working with successive approximations, I obtained the following identity, valid for $0 \le n \le 26$ , where the binomial coefficients $\binom{m}{k}$ evaluate to $0$ for $k \lt 0$ : $$2^{n+1}(2^{n+2}+1)-3^{n+2} ={2n+7 \choose n}-{2n+6 \choose n-3}+{2n+7 \choose n-5}+{2n+7 \choose n-12}-{2n+6 \choose n-15}+{2n+7 \choose n-17}+{2n+7 \choose n-24}$$ The series is somewhat ""irregular"", but it could be expanded further. Forcing $m=2n+7$ always in $\binom{m}{k}$ we get a more ""regular"" sequence but with more terms: $$2^{n+1}(2^{n+2}+1)-3^{n+2} ={2n+7 \choose n}-{2n+7 \choose n-3}+{2n+7 \choose n-4}+{2n+7 \choose n-6}-{2n+7 \choose n-7}+{2n+7 \choose n-8}+{2n+7 \choose n-9}$$ valid for $0 \le n \le 9$ . This one implies that for $n \neq k_i$ and $2n+7$ prime, all the $\binom{2n+7}{n-k_i}$ terms are divisible by $2n+7$ and therefore $2n+7$ divides the LHS. What is special about $2^{n+1}(2^{n+2}+1)-3^{n+2}$ that it can be expanded in this way? And what is special about $2n+7$ ? Also is there some series expansion (maybe based on the gamma function?) of general application that I don't know?","['elementary-number-theory', 'gamma-function', 'binomial-coefficients', 'combinatorics', 'sequences-and-series']"
4426383,"Is this simple operator bounded from $L^p([1,2])$ to $L^p([0,1])$?","Let $f\in L^{p}([1,2])$ with $p>1$ , and define $$Tf(x):= \int_{1<y<2}\frac{f(y)}{|x-y|}dy,\quad 0<x<1.$$ I am trying to prove/disporove that $$\|Tf\|_{L^p([0,1])}\leq C \|f\|_{L^p([1,2])}\quad (1)$$ I did not go far enough by Minkowski's integral inequality which implies $$\left( \int_{0}^{1}\left(\int_{1<y<2}\frac{f(y)}{|x-y|}dy\right)^{p} dx \right)^{1/p}\leq \int_{1<y<2}f(y)\left(\int_{0}^{1}\frac{dx}{(y-x)^{p}}\right)^{1/p}dy$$ One can estimate $$\left(\int_{0}^{1}\frac{dx}{(y-x)^{p}}\right)^{1/p}=\frac{1}{p-1}\left(\frac{1}{(y-1)^{p-1}}-\frac{1}{y^{p-1}}\right)^{1/p}\leq \frac{C}{(y-1)^{1-\frac{1}{p}}}$$ for $y$ close enough to $1$ . But this is not very helpful since $\int_{0}^{1}\frac{f(y)}{(y-1)^{1-\frac{1}{p}}} dx$ is not bounded by $\|f\|_{L^{p}([1,2])}$ considering the counterexample $g_{p}(x):=\frac{\chi_{[1,3/2]}(x)}{|\log{(x-1)}|(x-1)^{\frac{1}{p}}}$ .","['harmonic-analysis', 'analysis', 'real-analysis', 'functional-analysis', 'inequality']"
4426457,Does a subgroup have countable index in the group generated with a countable subgroup?,"Suppose $H$ is a subgroup of a group $G$ and both have the same cardinality of $\mathbb{R}$ .
If $K$ is a countable subgroup of $G$ , is it possible that $H$ has not countable index in the subgroup generated by $K$ and $H$ ? Clearly $G$ must be not abelian and $HK \neq KH$ .","['group-theory', 'infinite-groups', 'set-theory']"
4426467,Prove that the orthocentre of a certain triangle is on the side of another triangle.,"This geometry question has been frustrating me for a week: Let ABC be an acute angled triangle with circumcenter O. A circle passing through A and O intersects AB, AC at P, Q respectively. Show that the orthocentre of triangle OPQ lies on the side BC. My observations and ideas: We can try to prove O is the circumcenter of PQH. But we may need a
different description of H. If H is the orthocentre, we can see that triangle PQH is similar to
triangle ABC (in some order). I suspect that there is a unique triangle PQX with X on BC and triangle PQX similar to triangle ABC if I put some additional condition. Then using that condition I can try to show O is the orthocentre. I have not been successful so far.","['euclidean-geometry', 'geometry']"
4426483,On a Stopping Time Argument in Harmonic Analysis,"Quite often there is a common trick in harmonic analysis like the following: Assume that $0<S(x)<\infty$ for all $x\in\mathbb{R}^{n}$ . Consider the set \begin{align*}
\Omega_{k}&=\{x\in\mathbb{R}^{n}: S(x)>2^{k}\},\\
\mathcal{D}_{k}&=\left\{\text{dyadic cube}~\mathcal{Q}\subseteq\mathbb{R}^{n}:|\mathcal{Q}\cap\Omega_{k}|>\dfrac{1}{2}|\mathcal{Q}|~\text{and}~|\mathcal{Q}\cap\Omega_{k+1}|\leq\dfrac{1}{2}|\mathcal{Q}|\right\},\quad k\in\mathbb{Z}.
\end{align*} The author claimed without further reasoning that for each dyadic cube $\mathcal{Q}\in\mathcal{D}_{k}$ , there is a maximal dyadic cube $\mathcal{R}\in\mathcal{D}_{k}$ such that $\mathcal{Q}\subseteq\mathcal{R}$ . I fail to see why such a statement is valid. He also mentioned that this is a stopping time argument, perhaps the labeling does not matter that much, what bothers me is the maximality argument. There are several facts regarding the properties of $\mathcal{D}_{k}$ . The union $\displaystyle\bigcup_{k\in\mathbb{Z}}\mathcal{D}_{k}$ of all $\mathcal{D}_{k}$ contains all the dyadic cubes in $\mathbb{R}^{n}$ , this is not too hard to check. Besides that, if $\mathcal{Q}\in\mathcal{D}_{k}$ for some $k$ , then $\mathcal{Q}\in\mathcal{D}_{j}$ for all $j\ne k$ , this is easy to check. So the crucial question is that, why the following cannot hold? \begin{align*}
\mathcal{Q}_{1}\subseteq\mathcal{Q}_{2}\subseteq\cdots\mathcal{Q}_{i}\subseteq\cdots,
\end{align*} each $\mathcal{Q}_{i}\in\mathcal{D}_{k}$ and the chain does not terminate. We know that for any dyadic cubes $\mathcal{Q},\mathcal{R}$ , either they are disjoint or they contain each other: $\mathcal{Q}\subseteq\mathcal{R}$ or $\mathcal{R}\subseteq\mathcal{Q}$ . So if the chain does not terminate, then there is no such maximal dyadic cube.","['measure-theory', 'harmonic-analysis', 'real-analysis', 'stopping-times', 'probability']"
4426514,"Solve a triangle given one angle of the median, and a side and an angle of its external triangle","This is a modified version of this question , and as such I'm using similar wording and visuals. Given: $α_2$ , one of the two angles which the vertex $A$ is split by the median $m$ ; $\overline{AD}$ , the length of the segment of a triangle external to $ABC$ ; $\overline{BM}$ , the length of half of the side split by the median; $θ$ , the angle opposite to the shared side $\overline{AB}$ ; Find the value of the angles $γ$ and $b_2$ . Similarly to the original question, the proportion $\frac{\sin{α_1}}{\sin{α_2}}=\frac{\sin{β_1}}{\sin{γ}}$ holds true. And it's still true that $β_1 = 180 - α_1 - α_2 - γ$ . However, unlike the original scenario, we don't know the value of $α_1$ . With the external angle theorem, we know that $α_1 = θ + β_2 - α_2$ , but now we have the $β_2$ variable to resolve. I figured we could use the law of sines to establish $\frac{\sin{β_2}}{\overline{AD}}=\frac{\sin{θ}}{\overline{AB}}$ , but after a few variable swaps the problem ultimately seems to loop back to $α_1$ . I feel that this problem is solvable, as the given parameters uniquely identify the two triangles. But I can't find out how to take it further than I have.","['trigonometry', 'angle', 'triangles', 'median']"
4426556,Does the convolution of the $1/3$-Cantor distribution with itself has a non trivial absolute-continuous part?,"This question is motivated by this postings 1 and more recently by 2 Let $\mu_{1/3}$ be the Devil's staircase distribution. This distribution is supported on the $1/3$ -Cantor set $C$ and so, it is singular with respect to the Lebesgue distribution $\lambda$ . There are several ways to construct this distribution (see for the Wikipedia article ). A probabilistic way to do it is by considering and sequence $(X_n:n\in\mathbb{N})$ of Bernoulli 0-1 random variables with parameter $1/2$ and define $Y=\sum_{n\in\mathbb{N}}\frac{2X_k}{3^k}$ . The law of $Y$ is $\mu_{1/3}$ . It is known that $C+C=\{x+y:(x,y)\in C\times C\}=[0,2]$ , and
in the older posting I referenced above, it is shown that $\mu_{1/3}*\mu_{1/3}\not\ll\lambda$ . My question is: in the Lebesgue decomposition $$\mu_{1/3}*\mu_{1/3}=f\cdot\lambda + \nu_a$$ where $\mu_a$ is singular with respect to $\lambda$ , is $\int f\,d\lambda>0$ ?
More generally, is there $n\in\mathbb{N}$ for which $\mu^{*n}_{1/3}$ has a nontrivial absolutely continuous part? Any hints, solutions or references will be appreciated.","['measure-theory', 'probability-theory']"
4426614,"Two ""racing"" sequences, what is the probability and expected stopping time that one will ""catch up"" to the other?","Let $a_n$ and $b_n$ be two sequences with $a_0 = 0$ and $b_0 = c$ with $c > 0$ . For simplicity, we can assume that $c$ is some small, positive integer, such as 3. Then, at every step, we flip a fair coin, and let $X_n$ denote the result of the coin flip. If $X_n = H$ , then $$ 
a_{n+1} = a_n + 2\\
b_{n+1} = b_n + 1
$$ Otherwise if $X_n = T$ , then $$ 
a_{n+1} = 0\\
b_{n+1} = b_n
$$ So the $a_n$ sequence gets ""zeroed out"" whenever we flip a $T$ . My interest is in the probability that $a_n$ catches up to $b_n$ , i.e. $a_n \geq b_n$ , at step $n$ . In other words, what is $P(a_n \geq b_n)$ ? In addition, if we define the stopping time $\tau$ to be the first $n$ such that $a_n \geq b_n$ , I am also interested in $P(\tau \leq k)$ , i.e. what is the probability that $a_n$ will have caught up to $b_n$ by step $k$ ? After running some simulations, I'm pretty sure this $\tau$ has infinite expectation, i.e. $E(\tau) = \infty$ . I think showing that $P(\tau = \infty) > 0$ may be easier than finding the probabilities above (I am still interested in those probabilities though). However, I haven't been able to find a way to prove even this yet.","['stochastic-processes', 'stopping-times', 'probability']"
4426743,Brownian Motion and Hitting Time expectation.,"The problem is as follows: Let $(W_t)$ be a Brownian Motion, $\alpha>0$ , and $\tau = \inf\{t>0 : W_t \geq \alpha\}$ be the First Exit Time. Compute $\mathbb{E}(\tau)$ . I am aware that the result is not finite, however I am having trouble showing that the integral does not converge. This problem has answers here . However they use results I haven't seen or understand, such as Wald's Identities. What I have done so far: $$
\mathbb{P}(\tau \leq t) 
= \mathbb{P}(W_t^* \geq \alpha)
= 2 \cdot \mathbb{P}(W_t \geq \alpha)
= 2(1- \Phi(\alpha / \sqrt{t}))
$$ Where $W_t^* = \underset{0\leq s \leq t}{\sup} W_s$ . $$
f(t) 
= \frac{\partial}{\partial t} \mathbb{P}(\tau \leq t)
= 2\phi\left(\frac{\alpha}{\sqrt{t}}\right)\left(\frac{\alpha}{2 \sqrt{t^3}}\right)
= \frac{\alpha}{\sqrt{2\pi t^3}} e^{-\frac{1}{2t}\alpha^2}
$$ I am stuck here: $$
\mathbb{E}(\tau)
= \int_0^{\infty} t \cdot \frac{\alpha}{\sqrt{2\pi t^3}} e^{-\frac{1}{2t}\alpha^2} dt
\overset{?}{=} \infty
$$ Any help would be appreciated.","['integration', 'statistics', 'normal-distribution', 'brownian-motion', 'probability']"
4426744,Infinite variation of Brownian motion and Continuity,"Let $C>0$ be a constant. Brownian motion is Hölder continuous for $\alpha=1/2$ : $$| B(t+h) - B(t) | \leq C \sqrt{h \log(1/h)} \leq C h^\alpha,$$ for every sufficiently small $h$ .
But Brownian motion is of unbounded variation: $$\sup \sum_{j}^k |B(t_j) - B(t_{j-1})| < \infty.$$ Consider a stochastic differential equation $$dX_t = X_t dt + \alpha B_t,$$ where $\alpha>0$ and $B_t$ is Brownian motion.
The Euler-Maruyama simulation gives $$X_{i+1} = X_{i} + X_{i}\Delta t + \alpha \cdot (B_{t_{i+1}} - B_{t_i}).$$ Is the diffusion term of the simulated equation $X_{i+1}$ then bounded w.r.t the Hölder continuity although Brownian motion is of infinite variation? Can we define an upper bound for the increment $B_{t_{i+1}} - B_{t_i}$ ?","['stochastic-processes', 'linear-algebra', 'stochastic-differential-equations', 'ordinary-differential-equations']"
4426795,Why can the exit distribution of a Brownian motion be found pretending it moves in a random straight line?,"Let $U$ be a domain in the plane and denote by $\mu_{x_0,U}$ the distribution of a Brownian motion starting at $x_0\in U$ by the time it hits the boundary of $U$ . Why is it that for $U=\mathbb{H}$ the upper half plane and any $x_0\in\mathbb{H}$ , the exit distribution $\mu_{x_0,\mathbb{H}}$ is a Cauchy distribution on the horizontal axis (centered at the first component of $x_0$ , and with scale parameter equal to the second component)? Note that the Cauchy distribution on the horizontal axis is the distribution of the intersections between the horizontal axis and random straight lines through the starting point with uniformly random angle. Obviously this uniform angle property also holds for the exiti distribution from $U=\mathbb{D}$ the unit disk when $x_0=0$ (in this case the exit distribution is just the Hausdorff measure on the circle). The question title was written in an attention gathering fashion though: I know this uniform-angle propery can't be true for all domains, but rather I'd like to ask what makes it true for the two special cases above and what makes it false for other domains. EDIT: In a lame sense, the answer is ""it's true for the half plane because the Cayley transform preserves BM up to time reparametrization and takes the horizontal axis to the circle and has derivative proportional to $1/(z+i)^2$ , the absolute value of which is the Cauchy distribution for real $z$ "". This isn't very revealing to what's going on though, at least to me. I still don't have a clear picture of whether the uniform angle property maybe is true for all, e.g., convex domains if only formulated well enough (e.g. first pick a random line, then simulate a one dimensional Brownian motion hitting the boundary intersected with that line)","['complex-analysis', 'stochastic-analysis', 'brownian-motion']"
4426839,Series expansion of $\text{Li}_3(1-x)$ at $x \sim 0$,"My question is simple, but maybe hard to answer. I would like to have a series expansion for $\text{Li}_3 (1-x)$ at $x \sim 0$ in the following form: $$\text{Li}_3 (1-x) = \sum_{n=0} c_n x^n + \log x \sum_{m=1} c_m x^m. \tag{1}$$ The first few terms are: $$\text{Li}_3(1-x) = \zeta (3)-\frac{\pi ^2 x}{6}+\left(\frac{3}{4}-\frac{\pi ^2}{12}\right) x^2+\left(\frac{7}{12}-\frac{\pi
   ^2}{18}\right) x^3+\left(\frac{131}{288}-\frac{\pi ^2}{24}\right) x^4+\left(\frac{53}{144}-\frac{\pi
   ^2}{30}\right) x^5+ \left(-\frac{x^2}{2}-\frac{x^3}{2}-\frac{11 x^4}{24}-\frac{5 x^5}{12}\right) \log x+O\left(x^6\right), \tag{2}$$ however Mathematica fails to go beyond $\mathcal{O}(x^{15})$ . The coefficients for the $\log$ term are easy to guess: $$c_m = \frac{H_{m-1}}{m}. \tag{3}$$ I also managed to obtain the coefficient with the $\pi^2$ : $$- \frac{\pi^2}{6n}, \tag{4}$$ but I am struggling with the remaining coefficient, especially with so few coefficients. Any idea what this coefficient could be?","['logarithms', 'polylogarithm', 'taylor-expansion', 'sequences-and-series', 'pattern-recognition']"
4426841,Pullback $\Bbb C [W] \xrightarrow{F^\sharp} \Bbb C[V]$ is surjective iff $F: V \to W$ defines an isomorphism between $V$ and some subvariety of $W$.,"Show that the pullback $\Bbb C [W] \xrightarrow{F^\sharp} \Bbb C[V]$ is surjective if and only if $F: V \to W$ defines an isomorphism between $V$ and some algebraic subvariety of $W$ . This problem originates from the book ""An invitation to algebraic geometry"". My work is the following. We assume that $F^\sharp : \Bbb C [W] \to \Bbb C [V]$ is surjective. Since the coordinate rings are $\Bbb C$ -algebras and $F^\sharp$ is an algebra homomorphism by the first isomorphism theorem we have that $\Bbb C[W]/ \ker F^\sharp \cong \Bbb C[V]$ . Now also $\ker F^\sharp = \{ P \in \Bbb C[W] : F^\sharp(P)=0 \}$ , but $F^\sharp (P)=0 \iff P(F(x)) = 0$ for every $x \in V$ . This implies that $P \in I(F(V))$ that is $P$ belongs to the polynomials that vanish at $F(V)$ . So I have that $\ker F^\sharp = I(F(V))$ and $$\Bbb C[W]/ \ker F^\sharp  = \Bbb C[W]/ I(F(V)) \cong \Bbb C[V]$$ But I still don't have the isomorphism $V \cong W'$ for $W' \subset W$ . I think that $W' = F(V)$ , but I cannot prove this?",['algebraic-geometry']
4426846,Finding all differentiable functions that satisfy an equation,"I have tried to solve a problem but it seems I have made a mistake and I don't know where. The problem: Find all differentiable functions $f : \mathbb{R} \to \mathbb{R}$ that satisfy the following condition: $f \circ f = f$ . My approach: $f \circ f = f \implies (f' \circ f)f'=f' \implies f'(x)=0$ or $(f' \circ f)(x)=1$ . If $f$ is invertible then we can muliply both sides by the inverse and get that $f(x)=x$ . If $f'(f(x))=1$ how can we continue. We also have to find $f(x)=x$ as a solution. I have tried using the differential notation but things got even messier: $\frac{df(f(x))}{dx}=\frac{df}{dx}$ . Using the chain rule, i.e.: $\frac{df(g(x))}{dx}=\frac{df}{dg}\frac{dg}{dx}$ we obtain $\frac{df}{df}\frac{df}{dx}=\frac{df}{dx}$ , which is just $\frac{df}{dx}=\frac{df}{dx}$ . In the meantime, I have found a way to prove that if $f(x)$ is not constant, then $f(x)=x$ , but I am not sure whether it is rigurous or not. Firstly suppose $f(x) \neq x$ . Then $f(x) < x$ or $f(x) > x$ . Assume $f(x) > x$ . Therefore, there exists $a : \mathbb{R} \to \mathbb{R}, a(x) > 0$ , such that $f(x)=x+a(x)$ . From the definition of the limit, $$f'(x)=\lim_{h \to 0} \frac{f(f(x)+h)-f(x)}{h}=\lim_{h \to 0} \frac{f(x+a(x)+h)-x-a(x)}{h}=\lim_{h \to 0} \frac{x+a(x)+h+a(x+a(x)+h)-x-a(x)}{h}=\lim_{h \to 0} \frac{h+a(x+h+a(x))}{h}$$ The last limit can exist if $\lim_{h \to 0}a(x+h+a(x))=0$ , but $a(x)$ is defined to be stictly positive. Analogously, for $f(x)<x$ .","['ordinary-differential-equations', 'real-analysis', 'continuity', 'calculus', 'derivatives']"
4426860,Folland theorem 6.19: $p=\infty$,"Consider the following theorem from Folland's book ""Real analysis: Modern techniques and their applications"": In the book, it can be found on p194 (second edition). I understand the proof of the theorem for part (a) completely. However if $p = \infty$ , I don't understand why (b) is true. In Folland's book, the following is written in the proof: This is a simple application of the monoticity of the integral. Can someone explain me why this is true? We have to prove that $$\left\|\int_Y f(-,y) d\nu(y)\right\|_\infty\le \int_Y \|f(-,y)\|_\infty d\nu(y)$$ so for example it suffices to show that $$\left|\int_Y f(x,y)d\nu(y)\right|\le \int_Y \|f(-,y)\|_\infty d\nu(y)$$ for $\mu$ -almost every $x \in X$ . Intuitively, we would want to do the following: If $x$ is in some set $A$ with complement contained in a set of measure $0$ , then $$\left|\int_Y f(x,y)d\nu(y)\right|\le \int_Y |f(x,y)|d\nu(y) \le \int_Y \|f(-,y)\|_\infty d\nu(y)$$ but to justify the last inequality we need to show that $$\int_Y |f(x,y)|d\nu(y) \le \int_Y \|f(-,y)\|_\infty d\nu(y)$$ for all $x \in A$ . However, why is this the case? Am I missing something obvious here?","['measure-theory', 'lebesgue-integral', 'convolution', 'lp-spaces', 'inequality']"
4426875,"Spivak Calculus, Ch 10, problem 32b: Can we use Leibniz's formula to calculate $f^{(k)}(x)$ if $f(x)=\frac{1}{x^2-1}$?","Spivak's Calculus, Chapter 10 on Differentiation, problem 32: What is $f^{(k)}(x)$ if a) $f(x) = \frac{1}{(x-1)^n}$ *b) $f(x)=\frac{1}{x^2-1}$ My question regards part $b)$ . $a)$ can be solved quite easily if you use the conclusion of a previous problem (30), which shows If $f(x)=x^{-n}$ for $n \in \mathbb{N}$ then $$f^{(k)}(x)=(-1)^k\frac{(n+k-1)!}{(n-1)!}x^{-n-k}\tag{1}$$ $$g(x)=f(x+1)=x^{-n}$$ $$g^{(k)}(x)=f^{(k)}(x+1)=(-1)^k\frac{(n+k-1)!}{(n-1)!}x^{-n-k}$$ $$g^{(k)}(x-1)=f^{(k)}(x)=\frac{d^{k} \frac{1}{(x-1)^n}}{dx^k}=(-1)^k\frac{(n+k-1)!}{(n-1)!}(x-1)^{-n-k}\tag{2}$$ Similarly, we can show that $$\frac{d^k \frac{1}{(x+1)^n}}{dx^k}=(-1)^k\frac{(n+k-1)!}{(n-1)!}(x+1)^{-n-k}\tag{3}$$ Now for part b). We can rewrite $f(x)=\frac{1}{x^2-1}$ in two different ways: $$f(x)=\frac{1}{x+1}\cdot \frac{1}{x-1}\tag{4}$$ $$f(x)=\frac{1}{2} \left ( \frac{1}{x-1}-\frac{1}{x+1} \right )\tag{5}$$ Spivak's solution manual differentiates $(5)$ . $$f^{(k)}(x)=\frac{1}{2} \left ( \frac{d^k \frac{1}{(x-1)^n}}{dx^k} - \frac{d^k \frac{1}{(x+1)^n}}{dx^k} \right )$$ So we can just sub in $(2)$ and $(3)$ : $$f^{(k)}(x)=\frac{1}{2}(-1)^k\frac{(n+k-1)!}{(n-1)!}((x-1)^{-n-k}-(x+1)^{-n-k})\tag{6}$$ My question is: can we apply Leibniz's formula to differentiate $(4)$ ? Leibniz's formula tells us that $$(f \cdot g)^{(n)}(x)=\sum_{k=0}^n \binom{n}{k}f^{(k)}(x) \cdot g^{(n-k)}(x)$$ If we apply this to $f(x)=\frac{1}{x+1}\cdot \frac{1}{x-1}$ then we get $$f^{(n)}(x)=\sum_{k=0}^n \left [(-1)^k \frac{k!}{0!}(x-1)^{-1-k} \right ] \left [(-1)^{n-k} \frac{(n+k)!}{0!}(x+1)^{-1-k}  \right ]$$ $$= \sum_{k=0}^n \binom{n}{k}(-1)^nk! (n+k)! (x^2-1)^{-1-k}\tag{7}$$ Differentiating the sum in $(5)$ was the easier route. But is the calculation of differentiating $(4)$ correct? Is one route better than the other?","['calculus', 'solution-verification', 'derivatives', 'factorial']"
4426886,Group algebra for quaternion group,"I'm trying to understand Hopf Galois Theory, and I decided to try studying some example of a non commutative ring extension. The papers I've studied tell me that, for a strongly $G$ -graded algebra $A$ over a fixed commutative ring $K$ , the corresponding Hopf algebra $H$ (that will verify that $A$ is a right $H$ -comodule algebra) is the group algebra $K[G]$ . So I tried to put the theory into an example where $G$ is the quaternion group $\{\pm1,\pm i,\pm j,\pm k\}$ . My question: It's about how are these $A$ and $H$ . $A$ is said to be a strongly $G$ -graded algebra over $K$ ( $K$ being a field). That means $A=\bigoplus_{g\in G}A_g$ , where each $A_g$ is a $K$ -subspace of $A$ ; and that $A_gA_h\subseteq A_{gh}$ , for all $g,h\in G$ . I guess $A$ can be expressed like this: $$A = A_1\oplus A_i\oplus A_j \oplus A_k = K\oplus Ki\oplus Kj \oplus Kk \ \cong \ Ke_1\oplus Ke_i\oplus Ke_j\oplus Ke_k,$$ where $e_1=(1,0,0,0), e_i=(0,1,0,0), e_j=(0,0,1,0), e_k=(0,0,0,1)$ (I prefer these vector space notation). I think I need just $4$ subalgebras instead of $8$ because $-g=-1_K\cdot g$ . On the other hand, I believe $H=K[G]$ can be represented the exact same way. Is this correct?","['modules', 'abstract-algebra', 'hopf-algebras', 'group-theory', 'quaternions']"
4426928,At most finite elements of the sequence $\{x_{n}\}_{n\in \mathbb{Z}^+}$ is nonzero under certain conditions.,"Let $\{x_{n}: n\in \mathbb{Z}_{+}\}$ be a sequence of complex numbers such that $$\sum\limits_{p\in \mathbb{Z}_{+}}\vert x_{p} \vert^2=1$$ and $$\sum\limits_{p\in \mathbb{Z}_{+} }x_{p} \overline{x_{p+r}}=0~~ \text{for all}~~ r\geq 1.$$ Does it imply at most finitely many $x_{p}$ can be non zero. Can you please provide  ideas how to solve this?
Any help is appreciated. Thank you in advance!","['operator-theory', 'sequences-and-series', 'matrix-equations', 'soft-question', 'complex-numbers']"
4427069,"Probability that in a line of 'n' distinguishable people, at least 'b' of the front 'a' spots are one of 'k' red-shirt-wearers","I've been approaching this problem for some time. Imagine that we have n distinguishable (uniquely named) people lined up in a line where k of these people are wearing red shirts. I want to find the probability that at least b of the front a spots of the line are occupied by someone wearing a red shirt. I approached the problem by first solving the problem of the number of ways for the first b spots to be occupied by a red-shirt-wearer. My result for this was: $$\frac{k!}{(k-b)!}(n-b)!$$ where the first term represents the number of ways to arrange the k people into the first b spots and the second term is the ways to arrange the rest of the people in the remaining spots in line. I then added a correction term for the number of ways we can pick b spots from the a top spots and get: $${a \choose b}\frac{k!}{(k-b)!}(n-b)!$$ Now, since I want the probability that at least b spots are filled, I sum this term from b to the minimum of a and k: $$\sum_{j=b}^{min(a,k)} {a \choose j}\frac{k!}{(k-j)!}(n-j)!$$ Now, to get the probability I divide by all possible arrangements of the line: $$\frac{1}{n!}\sum_{j=b}^{min(a,k)} {a \choose j}\frac{k!}{(k-j)!}(n-j)!$$ Since I am programming this and can't handle factorials of the size required, I then do some simplification: $$\sum_{j=b}^{min(a,k)} {a \choose j}\frac{k!}{(k-j)!}\frac{(n-j)!}{n!}$$ $$\sum_{j=b}^{min(a,k)} [{a \choose j}\prod_{i=0}^{j}\frac{k-i}{n-i}]$$ Sadly, when I run my program, I get answers above 1 for some input values which I don't think should be possible... It is possible that I implemented my code wrong, but my suspicion is that my math is incorrect. Is there a flaw in my thinking about this problem/derivation of an answer?","['permutations', 'combinations', 'probability-theory', 'probability']"
4427098,"If the boundary is adjoint to the differential, what is the ""coboundary"" adjoint to the codifferential in the continuum?","For a smooth manifold, Stoke's theorem says that the differential/exterior derivative $\mathrm{d}$ is adjoint to the boundary operator $\partial$ , i.e. $$\int_{\partial U} \omega = \int_{U} \mathrm{d}\omega.$$ where $\omega$ is a differential form. Given an inner product of forms (e.g. induced by a metric) $\langle\cdot,\cdot\rangle$ , the codifferential $\delta$ can be defined as the adjoint of $\mathrm{d}$ with respect to the inner product, $$\langle \mathrm{d}\alpha,\beta \rangle = \langle \alpha, \delta \beta\rangle.$$ My question is, is there a coboundary $\partial^\dagger$ such that we could write a dual version of Stoke's theorem, $$\int_{\partial^\dagger U}\omega = \int_U \delta \omega.$$ I am wondering if there is a meaningful version of this in the continuum. Naively it seems like the answer is ``no'', but it feels like $\partial^\dagger$ ought to have some sort of meaning. One could certainly define such a thing for chains, say on a simplicial complex.","['differential-forms', 'adjoint-operators', 'smooth-manifolds', 'differential-geometry']"
4427119,Choosing Coins To Flip,"Suppose I have two coins, and I don't know what the probability of getting a heads is with either coin. I flip one 60 times, and get heads 36 times. I flip another 20 times, and get heads 18 times. Which coin should I flip next for the highest probability of getting a heads? And how do I answer this more generally (changing # of flips, heads, and increasing the # of coins and ranking them)? I tried solving this problem using the Maximum Likelihood Estimation function for the Binomial Distribution, but it doesn't behave in a way that I found useful. Edit:
The most extreme example would be to consider I have hundreds of coins, some have only been flipped once, and some have been flipped hundreds of times. How do I rank the coins in order to decide which ones would give me the greatest chance of picking heads? The practical situation I'm looking at actually resembles this situation, after some abstraction!","['statistics', 'binomial-distribution']"
4427249,Absolute integrability and limits,"Suppose that $h \in L_1(\mathbb{R})$ and define $$
g(t) = \int_{t}^{\infty}h(\tau)\,\mathrm{d}\tau. 
$$ Is it true that $\lim_{t\to\infty}g(t) = 0$ ? Seems 'obviously' true to me, but what's the formal way to argue it?","['functional-analysis', 'real-analysis']"
4427251,Do Carmo Chapter 8 Exercise 2,"The statement is : Show that if $M^k$ is a closed, totally geodesic submanifold of $H^n$ (hyperbolic $n$ space), then $M^k$ is isometric to $H^k$ . My friend and I thought to use the ""Cartan Theorem"". This requires setting notation. Let $M$ and $\tilde{M}$ be two $n$ -dimensional Riemannian manifolds, say $p \in M$ and $\tilde{p} \in \tilde{M}$ are arbitrary points. Choose a linear isometry $i:T_pM \rightarrow T_{\tilde{p}}\tilde{M}$ Let $V \subset M$ be a normal neighborhood of $p$ small enough so that $\exp_{\tilde{p}}$ is defined on $i \circ \exp_p^{-1}(V)$ . Define a map $f:V \rightarrow \tilde{M}$ by $f(q) = \exp_{\tilde{p}} \circ i \circ \exp_p^{-1}(q)$ for $q \in V$ . Now, for all $q \in V$ , there exists a unique unit-speed geodesic $\gamma:[0,t] \rightarrow M$ such that $\gamma(0) = p$ and $\gamma(t) = q$ . Denote by $P_t$ the parallel transport along $\gamma$ from $\gamma(0)$ to $\gamma(t)$ . Define another map $\phi_t:T_qM \rightarrow T_{f(q)}\tilde{M}$ by: $\phi_t(v) = \tilde{P}_t \circ i \circ P_t^{-1}(v)$ for all $v \in T_qM$ where $\tilde{P}_t$ is the parallel transport along the normalized geodesic $\tilde{\gamma}:[0,t] \rightarrow \tilde{M}$ given by $\tilde{\gamma}(0) = \tilde{p}$ , $\tilde{\gamma}'(0) = i(\gamma'(0))$ . Finally, denote by $R$ and $\tilde{R}$ the curvature tensors for $M$ and $\tilde{M}$ respectively. We can now state Cartan's Theorem: With notation as above, if for all $q \in V$ and all $x,y,u,v \in T_qM$ we have: $\langle R(x,y)u,v \rangle = \langle \tilde{R}(\phi_t(x), \phi_t(y))\phi_t(u), \phi_t(v) \rangle$ then $f:V \rightarrow f(V) \subset \tilde{M}$ is a local isometry, and $df_p = i$ . Our work: I've heard that in negative curvature, things like conjugate points aren't a problem, and the injectivity radius is large or infinity, so I figured $\exp$ would be a global diffeomorphism for $H^n$ (which I didn't confirm). The hope was that the $f$ in Cartan's Theorem could give us an isometry onto $H^n$ . We looked at choosing the linear isometry $i$ to be the inclusion of the tangent space of the submanifold $T_pM^k$ restricted to its image in $T_pH^n$ . In that case, since the second fundamental form vanishes because $M^k$ is totally geodesic, the curvature tensors are equal via the Gauss formula. $\phi_t$ ends up being the identity (again, vanishing second fundamental form -> same covariant derivatives -> same parallel transport) with this choice of $i$ , and so the hypotheses of the theorem are met, and we get that $f$ is an isometry. Because of the way $f$ is defined, and since we chose the tangent space inclusion as $i$ , I believe this only shows that $M^k$ is isometric to its image in $H^n$ , which is not enough. I thought that maybe we could choose $i$ to be a different isometry whose image is all of $T_pH^n$ , but it seems obvious that this would fail to preserve length, or even be a diffeomorphism if $n \neq k$ . Maybe another approach would be to show that the image of $f$ is clopen in a connected manifold? We aren't sure where to go next. Everything we've done so far has only relied on ""totally geodesic"", so we need to bring in properties of hyperbolic space somehow. Thoughts? Edit: I've also realized that they aren't asking for an isometry onto $H^n$ , they're asking for an isometry onto $H^k$ . I think our proof works when $n = k$ because the tangent space inclusion is a linear isomorphism (if $\exp$ really is a global diffeo). I wonder what you get when exponentiating subspaces of a tangent space in $H^n$ ? Things that are isometric to lower-dimensional hyperbolic spaces perhaps?","['hyperbolic-geometry', 'riemannian-geometry', 'differential-geometry']"
4427305,How to prove that Hilbert Cube has the Fixed Point Property without using Brouwer Fixed point theorem?,"So these two statements might be equivalent, but still there is supposed an easier way to prove the former without knowledge in algebraic topology It's an exercise on my textbook after the chapter concerning compact metric spaces.
The following Lemma might help:
Let $(X,d)$ be a compact metric space, $f:X \mapsto X$ be a continous mapping, if for any $\epsilon>0$ there exists $x_{\epsilon}$ such that $d(x_{\epsilon},f(x_{\epsilon}))<\epsilon$ , then $f$ has a fixed point. The proof is quite easy. So I already know that hilbert cube is a compact metric space according to the Tychonoff theorem and the fact that the countable product of any metrizable spaces is metrizable. Note: The textbook skipped the proof of Tychonoff theorem. Hilbert Cube: $\prod_{n=1}^\infty[0,\frac{1}{n}]$","['fixed-point-theorems', 'general-topology', 'algebraic-topology']"
4427309,Condensed Mathematics and Analysis,"I've just come across a Twitter thread by Laurent Fargues explaining a work by Dustin Clausen & Peter Scholze under the name Condensed Mathematics and how Condensed Mathematics can algebraize topology and analysis. I find it somewhat surprising (or hard to believe) that you can do such a thing. So my question is can Condensed Mathematics completely algebraize topology and analysis, or can it only algebraize some analytic object close to algebraic geometry?. Can we reprove theorem and inequalities in analysis such as Mean value theorem , Hölder's inequality , and so on using Condensed Mathematics? Update : I crossposted to MO .","['general-topology', 'category-theory', 'algebraic-topology', 'analysis']"
4427333,Brownian motion has unbounded variation.,"I tried to solve this problem in the following way. Suppose $\{B_t | t\in [0,1]\}$ is our Brownian motion. Define, $$f_n(w) = \sum_{k=1}^{2^n} \bigg|B_{\frac{k}{2^n}}(w)-B_{\frac{k-1}{2^n}}(w)\bigg|.$$ Firstly, I showed that $f_n \leq f_{n+1}$ using triangle inequality. We have to show that $f_n \xrightarrow{a.s} \infty$ . For that I want to prove first the following, $\mathbb{P}(f_n\geq\alpha)\rightarrow 1\ \forall \alpha > 0....(*)$ .
Then from the definition of almost sure convergence we can conclude our desired result. But I couldn't able to show the $(*)$ . How to show that?","['measure-theory', 'weak-convergence', 'almost-everywhere', 'brownian-motion', 'probability-theory']"
4427406,Cohomology of sheaf of differentials on a hyperelliptic curve.,"This is 21.4.C in Vakil's Foundations of Algebraic Geometry. Let $C$ be a hyperelliptic curve over a field $k$ (algebraically closed, char not 2), with $r$ branch points ( $r \geq 4$ and is even). In Proposition 19.5.2, a cover with two affine open sets is given : $\operatorname{Spec}(k[x, y] / (y^2 - f(x)))$ and $\operatorname{Spec}(k[u, v]/(v^2 - g(u)))$ , where $g$ is $f$ with coefficients reversed, neither $g$ nor $f$ have any repeated roots, and $0$ is not a root of either $f$ nor $g$ . The transition function on the overlap is : $(k[x, y] /(y^2 - f(x)))_x \rightarrow (k[u, v] /(v^2 - g(u)))_u$ by $x \rightarrow 1/u, y \rightarrow v / u^{r/2}$ . The task is to find $h^1(C, \Omega_{C/k})$ , and that $x^{-1}dx$ is a generator. From Serre duality we know this is 1 dimensional. I'm stuck on the generator part. My approach is below. Let $A = k[x, y] / (y^2 - f(x)),B = k[u, v] / (v^2 - g(u)) $ . Then, $\Omega_{A/k} = A^2 / (-f'(x), 2y)$ , and $\Omega_{B/k} = B^2 / (-g'(u), 2v)$ . I then computed $du = d(1/x) = \frac{-1}{x^2} dx $ and $dv =d(y/x^{r/2}) = (dy x^{r/2} -  \frac{r}{2}x^{r/2 - 1}y  dx) /  x^r = x^{-r/2} dy - \frac{r}{2}x^{-r/2 - 1}y dx$ For simplicity, I'll write $f dx + g dy$ as $(f, g)$ , so we have $(-1/x^2, 0) $ and $(-\frac{r}{2} x^{-r/2-1} y, x^{-r/2})$ . Now I try to compute $h^1(C, \Omega_{C/k})$ using Cech cohomology using the two affine covers. I want to compute homology at the ""two open sets"" part. Everything on $\Omega_{A_x/k}$ is sent to 0. I just need to compute the range of $\Omega_{A/k} \oplus \Omega_{B/k}\rightarrow \Omega_{A_x/k}$ , and show that $(\frac{1}{x}, 0)$ is not in it. I'm not sure how to do this part. I think I see a way: the only way for there to be a difference of 1 in the coefficients is to use $dv$ , but that introduces an extra $y$ that we can't get rid of. How do I make this precise?","['algebraic-geometry', 'homology-cohomology', 'differential-forms']"
4427461,Is $\{a_n X_n +c\}$ this sequence bounded in probability?,Suppose $\{X_n\}$ converge weakly to standard normal distribution. Now we define a sequence $\{a_nX_n +c\}$ like that where $c>0$ is any constant and $a_n\rightarrow\infty$ . Can we conclude that this sequence is bounded in probability? Or how to proof that this sequence unbounded in probability? Here bounded in probability of a sequence of r.v. means For all $n$ and $\epsilon>0$ there exists $M>0$ such that $$\mathbb{P}(|X_n| > M) < \epsilon$$,"['weak-convergence', 'measure-theory', 'probability-theory', 'probability']"
4427503,$A_1 \cup A_2$ is not a sigma Algebra,"Let $X\neq \emptyset$ und $A_1, A_2$ be two sigma Algebras over $X$ with $A_1\not \subseteq A_2$ and $A_2\not \subseteq A_1$ . Show, that $A_1 \cup A_2$ cannot be a Sigma Algebra over $X$ .
My Idea: Because $A_1\not \subseteq A_2$ and $A_2\not \subseteq A_1$ , I find $y\in A_1$ with $y\not \in A_2$ and $z\in A_2$ with $z\not \in A_1$ . Also $y^c\not\in A_2$ and $z^c\not\in A_1$ . Now, I think I have to show that $y\cup z \not \in A_1 \cup A_2$ .
Does anyone have a hint?","['measure-theory', 'analysis']"
4427516,Statistical method of giving an upper estimate for $\|f\|_2^2$ from a data,"Suppose, that we are given an i.i.d. sample of $(X_i,Y_i), i=1,...,n$ input-output pairs, where $Y_i=f(X_i)+\varepsilon_i$ , where $f: [0,1] \to [0,1]$ is the data-generating function, and $\varepsilon_i$ are the independent noise terms. The task would be to give an upper approximation for the $L_2$ -norm of the data generating function, namely: $$\|f\|_2^2 = \int |f(x)|^2 \,dx$$ I have done some research, but I couldn't find any results which provide a suitable (let's denote it with $\kappa$ ) upper bound for this norm, when one is given a noisy input-output data pair. For the noise-free case (namely, when $\varepsilon_k = 0$ for every $k = 1,...,n$ ), a suitable approximation is the following term: $$\kappa := \frac{1}{n} \sum_{i=1}^n Y_i^2 \Rightarrow \mathbb{E}(\kappa) = \mathbb{E}\Big(\frac{1}{n}\sum_{i=1}^n Y_i^2\Big) = \mathbb{E} \Big(\frac{1}{n}\sum_{i=1}^n (f(X_i) \Big)^2 = \|f\|_2^2.$$ However, once the noise terms are not all $0$ , this term looks like this: $$\frac{1}{n} \sum_{i=1}^n Y_i^2 = \frac{1}{n} \sum_{i=1}^n (f(X_i)+\varepsilon_i)^2 = \frac{1}{n} \sum_{i=1}^n f(X_i)^2 + \varepsilon_i^2 + 2 f(X_i) \varepsilon_i$$ If we use the following notion (the difference between the noisy and the noise-free case) $$D := \frac{1}{n} \sum_{i=1}^n \varepsilon_i^2 + 2 f(X_i) \varepsilon_i,$$ we can easily obtain that $\mathbb{E}(D) > 0$ , because $\mathbb{E}(\varepsilon_i^2) > 0$ , therefore this approximation doesn't hold on the noisy case. I am gladly looking for any statistical methods or ideas, which provide me an upper estimate of this function norm on the noisy case. Any help is greatly appreciated!","['data-analysis', 'statistics']"
4427560,Two smooth bounded domains in $\mathbb{R}^2$ with the same boundary are equal,"Given two bounded smooth open subsets of the plane, can I say that if their boundary are equal then they are equal ? This question received a positive answer under a connectedness assumption on both sets, but I would like to know if this assumption can be removed. Edit: by smooth set I mean a set whose interior and boundary are locally the epigraph and the graph of a smooth (say $C^{\infty}$ ) function. Edit: I guess the result reduces to proving that for every connected component $E$ of the first set, there is a unique connected component $F$ of the second set such that $\partial E=\partial F$ . One could then conclude using the answer to the linked question.","['differential-topology', 'geometry', 'real-analysis']"
4427604,Evaluate $\int_0^\infty{\frac{(\ln(x))^2}{\sqrt{x}(1-x)^2}}dx$,$$I=\int_0^\infty{\frac{(\ln(x))^2}{\sqrt{x}(1-x)^2}}dx$$ I first substituted the $x=t^2$ to get $$I= 8\int_0^\infty{\frac{(\ln(t))^2}{(1-t^2)^2}}dt$$ After this I don't have much of an idea how to proceed forward. Any help would be appreciated! The answer given is $2\pi^2$,"['integration', 'definite-integrals']"
4427605,Inclusion–exclusion principle; what is $(-1)^{n+1}$,"could somebody kindly confirm that my understanding of inclusion-exclusion matches it's formula. for a 3 sets example; we add 3 unions, subtract the total of all 3 pairwise intersections and add the triple-wise intersections as such; $A_1\cup A_2\cup A_3= A_1+A_2+A_3-A_1\cap A_2 - A_1\cap A_3-A_2\cap A_3+A_1\cap A_2\cap A_3$ in summary, it is adding all sets, subtract the over-count and adding back the ""over-subtract"" for multiple sets; $ \bigcup_{i=1}^n A_i = \sum_{i=1}^n A_i - \sum_{i<j} A_i \cap A_j + \sum_{i<j<k} A_i \cap A_j \cap A_k - \dots + (-1)^{n+1} A_i \cap \dots A_n$ $\sum_{i=1}^n A_i $ ; Include the cardinalities of the sets $\sum_{i<j} A_i \cap A_j $ ; Exclude the cardinalities of the pairwise intersections. $\sum_{i<j<k} A_i \cap A_j \cap A_k $ ; Include the cardinalities of the triple-wise intersections. $\dots$ $(-1)^{n+1} A_i \cap \dots A_n $ ; Include cardinality of the $n$ -tuple-wise intersection. If the above are correct, what does $(-1)^{n+1}$ represents? kindly advise. Thank you","['statistics', 'inclusion-exclusion', 'combinatorics', 'elementary-set-theory', 'probability']"
4427617,"Weak convergence of Reversible and ergodic Markov chain on uncountable state space,","I think this may be a trivial question. But I could not find a proper reference due to my lack of knowledge. Suppose I have a Markov chain $X=(X_n)_{n\geq 1}$ on the uncountable (but compact!) state space $\mathcal{S}$ . It is known that $X$ admits a stationary distribution $\pi$ . Let $\mathbb{P}_{\pi}$ denote the canonical law of $X$ on the path space $\mathcal{S}^{\mathbb{N}}$ and assume that the probability preserving dynamical system $(\mathcal{S}^{\mathbb{N}},\mathbb{P}_{\pi},\theta)$ is also ergodic where $\theta$ is the left-shift on $\mathcal{S}^{\mathbb{N}}$ . By an application of pointwise Ergodic theorem and bounded convergence theorem, it is known that for $f\in C_b(\mathcal{S})$ , we have \begin{equation}
\tag{1}
\lim\limits_{n\to\infty}\frac{1}{n}\sum_{l=1}^{n}\mathbb{E}_x[f(X_l)]=\int_{\mathcal{S}}f(y)\,\rm{d}\pi(y),\quad\text{ for }\ \pi \ \text{ a.e }\ x\in\mathcal{S}.
\end{equation} Suppose we also know that $\pi$ reversible. By this I mean if $P(x,dy)$ is the probability transition kernel and $f,g\in C_B(\mathcal{S})$ then, $$ \int_{\mathcal{S}} f(x)\int_{\mathcal{S}}g(y)P(x,dy)\,d\pi(x) = \int_{\mathcal{S}} g(x)\int_{\mathcal{S}}f(y)P(x,dy)\,d\pi(x).$$ My question is that if reversibility of $\pi$ is enough to get the strong convergence out of (1), i.e., does it hold, $$ \lim\limits_{n\to\infty}\mathbb{E}_x[f(X_n)] =\int_{\mathcal{S}}f\,\rm{d}\pi$$ for $\pi$ a.e $x$ . I found the following reference (Theorem 2) https://www.ams.org/journals/bull/1962-68-02/S0002-9904-1962-10737-X/S0002-9904-1962-10737-X.pdf which seems to suggest that at least $\lim\limits_{n\to\infty}\mathbb{E}_x[f(X_{2n})]$ exists for $\pi$ a.e $x$ . I am not much familiar with the ergodic theory terminology. Update: $\lim\limits_{n\to\infty}\mathbb{E}_x[f(X_{2n})]$ exists because,
if $T\,:\,L^2(\pi)\to L^2(\pi)$ is the Markov operator, $$ Tf(x) := \int_{\mathcal{S}} f(y)\,P(x,dy), $$ then $T = T^*$ as $\pi$ is reversible. Also $T^nf(x) = \mathbb{E}_x[f(X_n)]$ for $f\in C_b(\mathcal{S})$ . Thus by the Theorem 2, $\mathbb{E}_x[f(X_{2n})] = (T^n) (T^*)^nf(x)$ converges for $\pi$ a.e $x$ . As mentioned in the comment, I am ok with assuming that the chain is aperiodic. Any help is very much appreciated!","['ergodic-theory', 'markov-chains', 'functional-analysis', 'spectral-theory', 'probability-theory']"
4427695,Zeros of Faulhaber's formula,"I've noticed that $f_p(n) := \sum_{k=1}^nk^p$ has something interesting going with its zeros: $$
\begin{align}
p = 0: &&0
\\
p = 1: &&-1, 0
\\
p = 2: && -1, -\frac12,0
\\
p = 3: &&-1,-1,0,0
\end{align}
$$ where in the latter multiplicity of zeros of $f_p$ is counted. For now I can say that $0$ is always a zero, all zeros are real and all are spread in $[-1, 0]$ . Unfortunately, the latter condition is not satisfied already for $p = 4$ , however the two zeros that fall out of this interval are still real and their sum is $-1$ . Is there anything interesting known about zeros of $f_p$ in general?","['number-theory', 'combinatorics', 'polynomials']"
4427761,A rearrangement of a conditionally convergent series is seldom convergent,"Let $\sum_{n=0}^\infty a_n$ be a conditionally convergent series of real numbers. Riemann's rearrangement theorem says that, for every $s\in\Bbb R$ , there is some bijection $b\colon\Bbb Z_+\longrightarrow\Bbb Z_+$ such that $\sum_{n=0}^\infty a_{b(n)}=s$ . However, it is intuitive that if you take a random bijection $b\colon\Bbb Z_+\longrightarrow\Bbb Z_+$ , then the series $\sum_{n=0}^\infty a_{b(n)}$ diverges. I would like to be able to state this in a formal way. To be more precise, let $\mathcal B$ be the set of all bijections from $\Bbb Z_+$ into itself. Is there a non-trivial measure $\mu$ in the $\sigma$ -algebra $\mathcal P(\mathcal B)$ such that, for every conditionally convergent series $\sum_{n=0}^\infty a_n$ , $$\mu\left(\left\{b\in\mathcal B\,\middle|\,\sum_{n=0}^\infty a_{b(n)}\text{ converges}\right\}\right)=0.$$ I've put “non-trivial” in the question in order to avoid taking $\mu$ equal to the null function.","['measure-theory', 'analysis', 'sequences-and-series']"
4427795,Why does interest rates need to be below thee increase of a stock for no arbitrage [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I'm doing a mathematical finance module in discrete time. One of the lemmas given is $0<d<1+r<u$ there is no arbitrage opportunity. Where $d$ is a multiplier for when the stock price goes down from time $0$ to time $1$ and $u$ is the multiplier of the stock price going up. So my question is why is that and if possible can you give an example where an arbitrage could exist and what it would look like say when $1+r = u$","['finance', 'discrete-mathematics']"
4427821,"Is $SL(2, \mathbb{R})$ dense in saturated elementarily extensions of the reals?","Suppose $\mathcal{A} = \langle A, <, +, \cdot \rangle$ is a $\aleph_1$ -saturated elementarily extension of the real field. Is $SL(2, \mathbb{R})$ dense in $SL(2,A)$ ? In compact groups one can find subgroups with infinitesimal entries, but is it still possible in $SL(2,A)$ ?","['first-order-logic', 'model-theory', 'geometric-topology', 'group-theory', 'lie-groups']"
4427829,$L^p_{\text{loc}}$ convergence implies almost everywhere convergence,"Suppose $\{f_n\}\subset L^p(B)$ for the unit ball $B\subset\mathbb{R}^n$ converges in $L^p_{\text{loc}}$ , i.e. $\int_V\lvert f_n(x)-f(x)\rvert^p dx\rightarrow 0$ for all $V\subset\subset B$ . Can we say that $f_{n_i}(x)\rightarrow f(x)$ almost everywhere in $B$ for some subsequence $\{f_{n_i}\}$ ? My idea for a proof was to consider a sequence of balls $B_{r_k}$ with $r_k\rightarrow 1$ and denote by $\{f^k_n\}$ the subsequence that converges almost everywhere in $B_{r_k}$ . Then I thought I could pick a diagonal subsequence that converges almost everywhere in $B$ . However I'm not sure how to show that the diagonal sequence converges almost everywhere. Maybe this is not true, in that case can we find a counter-example?","['analysis', 'real-analysis', 'functional-analysis', 'almost-everywhere', 'convergence-divergence']"
4427857,Isomorphism Classes of Real Closed Subfields of $\Bbb C$,The real line $\Bbb R$ is a maximal real closed subfield of the complex plane $\Bbb C$ . How many such maximal real closed subfields exist(up to isomorphism)? Is there a way to see that there must be at least infinitely many such maximal real closed subfields?,"['model-theory', 'logic', 'complex-analysis', 'field-theory', 'abstract-algebra']"
4427858,"I'm having trouble with the backwards direction, and I need some insight on how to do this while using a contradiction.","Consider the following set identity: $(N\cap M)\cup (D\cap M) = M\cap (N\cup D)$ . Rewrite this identity as a biconditional statement involving set membership. Prove this biconditional statement. First, consider the forward direction.
Assume: $x\in (N\cap M)\cup (D\cap M)$ Since $(x\in N \cap M) \lor (x\in D\cap M)$ , you can deduce $x\in M$ either way. Then $x\in N\cap M \implies x\in N$ . Similarly, $x\in D\cap M \implies x\in D$ . Thus either $x\in N\cup D$ . Therefore, $x\in M\cap (N\cup D)$ . Finally, consider the backward direction. Assume $x\in M\cap(N\cup D)$ . Aiming for a contradiction, suppose $x\not\in (N\cap M) \cup(D\cap M)$ . From $x\not\in(N\cap M) \cup (D\cap M)$ , we see $x\not\in (N\cap M)$ or $x\not\in (D\cap M)$ . By using Demorgan's Law, $x\in N\cup M$ and $x\in D\cup M$ . If $x\not\in N\cup D$ , then $x\in M$ . From the first assumption, $x\in M$ and $x\in N$ or $x\in D$ . If $x\in N$ we see $x\in M \cap N$ . The same thing can also be said if $x\in D$ , $x\in M \cap D$ . Since $x\in M$ and $x\not\in (N\cap M)\cup(D\cap M)$ , we see $x\in (N\cup D)$ . Thus we've found a contradiction: both $x\in N\cup D$ and $x\not\in N\cup D$ So the contradiction is false: $x\in M\cap(N\cup D)$ . Ergo, $x\in (N\cap M) \cup (D\cap M)$ .",['elementary-set-theory']
4427887,"Spivak, Ch 10 Differentiation, Problem *33: Very tricky proof that $f'(0),...,f^{(n)}(0)$ exist if $f(x)=x^{2n}\sin(1/x)$.","Let $f(x)=x^{2n}\sin(1/x)$ if $x \neq 0$ , and let $f(0)=0$ . Prove that $f'(0),...,f^{(n)}(0)$ exist, and that $f^{(n)}$ is not continuous at $0$ . The strategy to prove this can be split into the following steps: prove a conjecture about what the terms of $f^{(k)}$ look like, using induction on $k$ using another induction, prove that for $k<n$ , each of the terms of $f^{(k)}$ are a product of x raised to at least the second power, times a sinusoid, which we know is bounded. use yet another induction to show that $f^{(k)}(0)=0$ , for $k \leq n$ , by calculating the limit that defines each derivative. These calculations are quite easy given step 2. in the case of $f^{(n)}$ not all terms will have an $x$ raised to at least the second power, so when we calculate the limit it will not be 0. Therefore, $f^{(n)}$ is not continuous at $0$ . My question is about step 1. If we write out, say, the first, second, and third derivatives of $f$ we start to notice a pattern, and so we conjecture that $f^{(k)}(x)$ is composed only of the following types of terms: $$a \cdot \sin(1/x)x^{2n-k}\tag{1}$$ $$\pm \sin(1/x)x^{2n-2k}, \text{ if k even}\tag{2}$$ $$\pm \cos(1/x)x^{2n-2k}, \text{ if k odd}\tag{3}$$ $$\sum_{i=k+1}^{2k-1} [a_ix^{2n-i}\sin(1/x)+b_ix^{2n-i}\cos(1/x)]\tag{4}$$ We can try to prove that this is true by using induction on $k$ . $$f'(x)=\sin(1/x) \cdot 2nx^{2n-1}+\cos(1/x)(-x^{2n-2})$$ The first term is a term like in $(1)$ , and the second term is like in $(4)$ . Now assume that the conjecture is true for some $k$ . Then we should only see the following terms in $f^{(k+1)}(x)$ $$\sin(1/x)x^{2n-(k+1)}\tag{5}$$ $$\pm \sin(1/x)x^{2n-2(k+1)}, \text{ if k is even}\tag{6}$$ $$\pm \cos(1/x) x^{2n-2(k+1)}, \text{ if k is odd}\tag{7}$$ $$\sum_{i=(k+1)+1}^{2(k+1)-1} [a_ix^{2n-i}\sin(1/x)+b_ix^{2n-i}\cos(1/x)]\tag{8}$$ To check this, we can differentiate $(1)$ , $(2)$ , $(3)$ , and $(4)$ and check if the results only contain terms as in $(5)$ , $(6)$ , $(7)$ , and $(8)$ . Differentiation of $(1)$ , $(2)$ , and $(3)$ produces the correct results. My question regards the differentiation of $(4)$ . $$\frac{d}{dx}(\sum_{i=k+1}^{2k-1} [a_ix^{2n-i}\sin(1/x)+b_ix^{2n-i}\cos(1/x)])$$ $$=\sum_{i=k+1}^{2k-1} [a_i(2n-1)x^{2n-i-1}\sin(1/x)-a_i x^{2n-i-2}\cos(1/x)+b_i(2n-1)x^{2n-i-1}\cos(1/x)+b_ix^{2n-i-2}\sin(1/x)]$$ $$=\sum_{i=k+1}^{2k-1} [a_i(2n-1)x^{2n-(i+1)}\sin(1/x)+b_i(2n-1)x^{2n-(i+1)}\cos(1/x)-a_i x^{2n-(i+1)-1}\cos(1/x)+b_ix^{2n-(i+1)-1}\sin(1/x)]$$ $$=\sum_{i=(k+1)+1}^{2(k+1)-1} [a_i(2n-1)x^{2n-i}\sin(1/x)+b_i(2n-1)x^{2n-i}\cos(1/x)-a_i x^{2n-i-1}\cos(1/x)+b_ix^{2n-i-1}\sin(1/x)]\tag{9}$$ The first two terms in the sum are like the terms in $(8)$ , but the last two terms in the sum are problematic. They don't seem to fit. This problem is clearly quite tricky and I've spent a couple hours on it. I believe the reasoning is correct (and it is in the solution manual as well), but the solution manual does not go through every step of every differentiation (and specifically the differentiation of $(4)$ as I showed above). I am wondering if there is a mistake or if there is some algebraic manipulation of $(9)$ that I am missing.","['calculus', 'solution-verification', 'derivatives']"
4427913,Does there exist an unbiased estimator for the absolute value of the mean?,"Given random variable $Z$ , with samples $\{z_1,\dots,z_n\}$ is there an unbiased estimator for $|\mathbb{E}[Z]|$ ? If $Z$ is positive (or negative) with probability 1 this is straightforward, as then the absolute value can be essentially ignored. This is a followup to my previous question here . Based on the answers provided and after having thought about it for a while, I am intuitively convinced that no such estimator can exist (for general random variables). However, I am lost on how to prove it.","['statistics', 'probability', 'random-variables']"
4427925,Radial Lipschitz Mapping,"Let $f:\mathbb{R}^2\rightarrow \mathbb{R}^2$ defined by $$f(x,y)=(x\cos(y),x\sin(y)).$$ Is $f$ a Lipschitz function? Is it bi-Lipschitz? I tried this: \begin{align*}|f(x_1 ,y_1) - f(x_2 , y_2)|^2&= x_1^2 +x_2^2 - 2x_1 x_2 (\cos(y_1)\cos(y_2)+\sin(y_1)\sin(y_2))\\&=x_1^2 +x_2^2 - 2x_1 x_2 \cos(y_1 + y_2),\end{align*} but I don't know how to continue to make it appear $|(x_1 ,y_1)-(x_2 , y_2)|$ . Any help is welcome.","['functions', 'lipschitz-functions', 'analysis', 'real-analysis']"
4427985,Elegant solutions to $ \int_{0}^{\frac{\pi}{2}} x\tan 2 x \ln (\tan x) d x $ .,"Letting $x\mapsto \frac{\pi}{2} -x$ converts the integral $\displaystyle I=\frac{\pi}{4} \int_{0}^{\frac{\pi}{2}} \tan 2 x \ln (\tan x) d x \tag*{} $ Using the identity $ \displaystyle \tan x=\frac{\sin 2 x}{1+\cos 2 x} $ , we get $$\displaystyle I=\frac{\pi}{4} \int_{0}^{\frac{\pi}{2}}\left[\frac{\sin 2 x}{\cos 2 x} \ln (\sin 2 x)-\frac{\sin 2 x}{\cos 2 x} \ln (1+\cos 2 x)\right] d x \tag*{} $$ Letting $2x\mapsto x$ yields $\displaystyle I=\frac{\pi}{8}\left[\underbrace{\int_{0}^{\pi} \frac{\sin x}{\cos x} \ln (\sin x)}_{J} d x-\underbrace{\int_{0}^{\pi} \frac{\sin x}{\cos x} \ln (1+\cos x) d x}_{K}\right]\tag*{} $ As $ \displaystyle J \stackrel{x \rightarrow \pi-x}{=}-J \Rightarrow J=0 $ , therefore $\displaystyle I=-\frac{\pi}{8} K. $ By my post , $\displaystyle \begin{aligned}K \stackrel{y=\cos x}{=}& \int_{-1}^{1} \frac{\ln (1+y)}{y} d y=\frac{\pi^{2}}{4}\end{aligned}\tag*{} $ Now we can conclude that $\displaystyle \boxed{I=-\frac{\pi^{3}}{32}}\tag*{} $ Request for elegant solutions. Your suggestion and alternative methods are warmly welcome!","['integration', 'calculus', 'improper-integrals', 'trigonometry']"
4427996,Erdős theorem on 2-colorable sets,"Let $\mathcal{F}$ be a family of subsets of some finite set.  Can we
color the elements of the underlying set in red and blue so that no
member of $\mathcal{F}$ will be monochromatic? Such families are
called $2$ - colorable . Recall that a family is $k$ - uniform if each
member has exactly $k$ elements. Theorem. If $k$ is sufficiently large, then there exists a $k$ -uniform family $\mathcal{F}$ such that $|\mathcal{F}|\leq k^22^k$ and $\mathcal{F}$ is not $2$ -colorable. Proof: Set $r=[k^2/2]$ . Let $\mathbf{A}_1,\mathbf{A}_2,\dots$ be independent random members of $\binom{[r]}{k}$ , that is, $\mathbf{A}_i$ ranges over the set of all $A\subseteq \{1,\dots,r\}$ with $|A|=k$ , and $\text{Pr}[\mathbf{A}_i=A]=\binom{r}{k}^{-1}$ .
Consider the family $\mathcal{F}=\{\mathbf{A}_1,\dots,\mathbf{A}_b\}$ ,
where $b$ is a parameter to be specified later. Let $\chi$ be a
coloring of $\{1,\dots,r\}$ in red and blue, with $a$ red points and $r-a$ blue points. Using Jensen's inequality, for any such coloring
and any $i$ , we have $$\text{Pr}[\mathbf{A}_i \ \text{is
 monochromatic}]=\text{Pr}[\mathbf{A}_i \ \text{is
 red}]+\text{Pr}[\mathbf{A}_i \ \text{is
 blue}]=$$ $$=\dfrac{\binom{a}{k}+\binom{r-a}{k}}{\binom{r}{k}}\geq
 \frac{2\binom{r/2}{k}}{\binom{r}{k}}:=p,$$ where, by the asymptotic
formula $(1.9)$ for the binomial coefficients, $p$ is about $e^{-1}2^{1-k}$ . Since the members $\mathbf{A}_i$ of $\mathcal{F}$ are
independent, the probability that a given coloring $\chi$ is legal for $\mathcal{F}$ equals $$\prod_{i=1}^b(1-\text{Pr}[\mathbf{A}_i\
 \text{is monochromatic}])\leq (1-p)^b.$$ Hence, the probability that
at least one of all $2^r$ possible colorings will be legat for $\mathcal{F}$ does not exceed $2^r(1-p)^b<e^{r\ln 2-pb}$ , which is
less than $1$ for $b=(r\ln 2)/p=(1+o(1))k^22^{k-2}e\ln 2$ . But this
means that there must be at least one realization of the random family $\mathcal{F}$ , which has only $b$ sets and which cannot be colored
legally. This is an excerpt from Jukna's ""Extremal combinatorics"" and I guess that there are some technical flaws in the reasoning, however, I understood the underlying idea of the proof. So let me ask my questions. Am I right that the author considers the finite probability space $(\Omega,\text{Pr})$ , where $\Omega=\{A\subset [r]: |A|=k\}$ , i.e. $|\Omega|=\binom{r}{k}$ with uniform distribution, i.e. $\text{Pr}[\omega]=\frac{1}{|\Omega|}$ for each $\omega\in \Omega$ ? I was able to show that for each coloring $\chi$ we have $$\text{Pr}[\{A\in \Omega: A \ \text{is monochromatic under coloring}\  \chi\}]=\binom{r}{k}^{-1}\left[\binom{a}{k}+\binom{r-a}{k} \right].$$ But I am really confused what does the author mean by $\text{Pr}[\mathbf{A}_i \ \text{is monochromatic}]$ . Can anyone explain what does this event mean?","['discrete-mathematics', 'combinatorics', 'extremal-combinatorics', 'probability']"
4428022,Is the following an Initial Value Problem or not?,"I'm trying to solve an Initial Value Problem, but I'm not sure now if the problem I have in hand is even an Initial Value Problem. Notes from Paul Dawkins' Course states IVP has the definition below - those are the initial values we have: $$
\frac{dy}{dt} = f(t,y), \quad y(t_0) = y_0
$$ The problem I'm trying to solve on the other hand, is as follows: $$
\frac{dy}{dt} = f(y), \quad y(t_0) = y_0
$$ In my problem $\frac{dy}{dt}$ is only a function of $y$ , not $t,y$ . Does my problem qualify as an Initial Value Problem and can it be solved by Euler's method? The confusion arises, because I have an initial value and an initial direction. But the rate of change does not depend on time $t$ . Thanks!","['initial-value-problems', 'definition', 'ordinary-differential-equations']"
4428058,If the homeomorphism has the non-negative or non-positive determinant?,"Let $ \Omega_1 $ and $ \Omega_2 $ be domains(open and connected) in $ \mathbb{R}^2 $ . $ \psi:\Omega_1\to\mathbb{R} $ and $ \phi:\Omega_1\to\mathbb{R} $ are $ C^1 $ functions with two variables. Moreover, we assume that map $ (x,y)\to (\phi(x,y),\psi(x,y)) $ is homeomorphism from $ \Omega_1 $ to $ \Omega_2 $ , i.e. the map $ (x,y)\to (\phi(x,y),\psi(x,y)) $ is continuous from $ \Omega_1 $ to $ \Omega_2 $ and has continuous inverse map. I want to ask that if I can obtain that the Jacobi determunant of the map, denoted as $ \frac{\partial(\phi,\psi)}{\partial(x,y)} $ is either non-positive or non-negative in $ \Omega_1 $ , i.e. either $ \frac{\partial(\phi,\psi)}{\partial(x,y)}\geq 0 $ for all $ (x,y)\in\Omega_1 $ ,  or $ \frac{\partial(\phi,\psi)}{\partial(x,y)}\leq 0 $ for all $ (x,y)\in\Omega_1 $ , . I have tried by considering the image sets of a curve in $ \Omega_1 $ and by using the connectness of $ \Omega_1 $ but failed. Can you give me some references or hints?","['calculus', 'general-topology', 'analysis']"
4428142,Computing $\int_{0}^{1} \frac{\sin ^{-1} (x) \ln (1+x)}{x^{2}} d x$,"Applying integration by parts splits the integral into 3 integrals, $\displaystyle \begin{aligned}I&=\int_{0}^{1} \frac{\sin ^{-1} x \ln (1+x)}{x^{2}} d x\\&=-\int_{0}^{1} \sin ^{-1} x \ln (1+x) d\left(\frac{1}{x}\right) \\&=-\left[\frac{\sin ^{-1} x \ln (1+x)}{x}\right]_{0}^{1}+\underbrace{\int_{0}^{1} \frac{\ln (1+x)}{x \sqrt{1-x^{2}}}}_{K} +\underbrace{\int_{0}^{1}\frac{\sin ^{-1} x}{x}}_{L} d x-\underbrace{\int_{0}^{1} \frac{\sin ^{-1} x}{1+x}}_{M} d x  \end{aligned} \tag*{} $ Letting $x= \cos \theta$ for $K$ and $\sin^{-1}x \mapsto x$ for $L$ and $M$ , yields $\displaystyle  I=-\frac{\pi}{2} \ln 2 +\underbrace{\int_{0}^{\frac{\pi}{2}} \frac{\ln (1+\cos \theta)}{\cos \theta} d \theta}_{K}+\underbrace{\int_{0}^{\frac{\pi}{2}} \frac{x\cos x }{\sin x} d x}_{L}-\underbrace{\int_{0}^{\frac{\pi}{2}} \frac{x\cos x }{1+\sin x} d x }_{M}\tag*{} $ For the integral $ K,$ putting $ a=1$ in my post yields $\displaystyle \boxed{K=\frac{\pi^{2}}{8}}\tag*{} $ For the integral $ L,$ integration by parts yields $\displaystyle \begin{aligned}L &=\int_{0}^{\frac{\pi}{2}} x d \ln (\sin x) \\&=[x \ln (\sin x)]_{0}^{\frac{\pi}{2}}-\int_{0}^{\frac{\pi}{2}} \ln (\sin x) d x \\&=\boxed{\frac{\pi}{2} \ln 2}\end{aligned}\tag*{} $ For the integral $ M,$ integration by parts yields $\displaystyle \begin{aligned}M &=\int_{0}^{\frac{\pi}{2}} x d \ln (1+\sin x)\\&=[x \ln (1+\sin x)]_{0}^{\frac{\pi}{2}}-\int_{0}^{\frac{\pi}{2}} \ln (1+\sin x) d x \\&=\frac{\pi}{2} \ln 2-\underbrace{\int_0^{\frac{\pi}{2} }\ln (1+\sin x) d x}_{N}\end{aligned}\tag*{} $ For the integral $ N,$ using my post in the second last step yields $\displaystyle \begin{aligned}N \stackrel{x\mapsto\frac{\pi}{2}-x}{=}  &\int_{0}^{\frac{\pi}{2}} \ln (1+\cos x) d x \\=&\int_{0}^{\frac{\pi}{2}} \ln \left(2 \cos ^{2} \frac{x}{2}\right) d x \\=&\frac{\pi}{2} \ln 2+2 \int_{0}^{\frac{\pi}{2}} \ln \left(\cos \frac{x}{2}\right) d x \\=&\frac{\pi}{2} \ln 2+4 \int_{0}^{\frac{\pi}{4}} \ln (\cos x) d x \\=&\frac{\pi}{2} \ln 2+4\left(\frac{1}{4}(2 G-\pi \ln 2)\right) \\=&\boxed{-\frac{\pi}{2} \ln 2+2 G}\end{aligned}\tag*{} $ where $ G$ is the Catalan’s Constant. Putting them together yields $\displaystyle \boxed{I=-\pi \ln 2+\frac{\pi^{2}}{8}+2G} \tag*{} $ Question: Is there any shorter solution?","['integration', 'calculus', 'improper-integrals', 'trigonometry']"
4428145,Extension of dominated convergence in Lieb & Loss,"In their 'Analysis' (2nd ed, p. 20) Lieb and Loss give the following extension of the dominated convergence theorem I can't see why the assertion $f(x) \le G(x)$ at the end holds. Would appreciate any help.","['measure-theory', 'real-analysis']"
4428175,"Directional derivative of f(x,y)=xy at (0,0) in direction (1,1)","Intuitively, I think of the directional derivative as the slope of the graph in a particular direction. But this intuition seems flawed. Consider the directional derivative of $f(x, y) = xy$ at the point $(0, 0)$ in the direction $(1, 1)$ . The slope of the graph in this direction should be positive, but the directional derivative is $0$ . If the directional derivative doesn't represent the slope of the graph in some direction, what does it represent? How should I think of it intuitively?","['partial-derivative', 'multivariable-calculus', 'derivatives']"
4428187,Expectation of modified Ornstein-Uhlenbeck process with random long run mean,"I would like to compute the expectation of a modified Ornstein-Uhlenbeck process of the form $$ dx_t = \theta(\mu_t-x_t)dt + \sigma x_t dW_t \ ,$$ where $\kappa, \sigma>0$ and $W_t$ is a Brownian motion. $\mu_t$ is itself a stochastic process and is a solution to the SDE: $$ d\mu_t = \beta\mu_tdB_t \ ,$$ where $\beta>0$ and $B_t$ is a Brownian motion independent from $W_t.$ I read in this blog post cited in this question how to procede when $\mu$ is not a stochastic process. I started with the same approach and I obtained the following ODE: $$ d\big(x_t e^{-\sigma W_t +\frac{1}{2}\sigma^2t}\big)=e^{-\sigma W_t +\frac{1}{2}\sigma^2t}\theta(\mu_t-x_t)dt\ ,$$ which can be rewritten as: $$ \frac{dy_t}{dt}=\theta\mu_te^{-\sigma W_t +\frac{1}{2}\sigma^2t}-\theta y_t\ ,$$ where $y_t$ is defined as $$ y_t = x_t e^{-\sigma W_t +\frac{1}{2}\sigma^2t}\ .$$ I tried to use methods for linear homogeneous ODEs and I came up with the following solution: $$y_t=y_0e^{-\theta t}+e^{-\theta t}\int_0^te^{\theta s}\mu_se^{-\sigma W_s +\frac{1}{2}\sigma^2s}ds\ .$$ The I substitute the dynamics for $\mu_t$ , which I know and I get: $$y_t=y_0e^{-\theta t}+e^{-\theta t}\int_0^te^{\theta s}e^{-\sigma W_s +\beta B_s +\frac{1}{2}(\sigma^2-\beta^2)s}ds\ .$$ Now, integration by parts lead to: $$y_t=\frac{2\theta e^{-\sigma W_t +\beta B_t +\frac{1}{2}(\sigma^2-\beta^2)t}}{\sigma^2-\beta^2+2\theta}+e^{-\theta t}\bigg(y_0 -\frac{2\theta e^{-\sigma W_t +\beta B_t}}{\sigma^2-\beta^2+2\theta}\bigg)\ .$$ In turn this implies: $$ x_t = \frac{2\theta e^{\beta B_t-\frac{1}{2}\beta^2t}}{\sigma^2-\beta^2+2\theta}+e^{-(\theta+\frac{1}{2}\sigma^2) t +\sigma W_t}y_0 -e^{-(\theta+\frac{1}{2}\sigma^2) t}\frac{2\theta e^{\beta B_t}}{\sigma^2-\beta^2+2\theta}\ .$$ I'm wondering if what I wrote makes sense and how to explain the condition $\sigma^2-\beta^2+2\theta\neq0$ that emerges. Thanks in advance.","['ordinary-differential-equations', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
4428224,Number of isomorphism classes of subsets of the power set $\mathcal{P}([n])$,"I believe that this should have been studied, so am I asking for some reference or OEIS sequence for the number of isomorphism classes of subsets of the power set $\mathcal{P}([n])$ , $[n]=\{1,\ldots,n\}$ . Two families, subsets of the power set, are considered equivalent and then belonging to the same class, if they can be made equal through a permutation of elements: for example $\{\{a\},\{a,b\}\} \equiv \{\{b\},\{a,b\}\} \equiv \{\{c\},\{c,d\}\}$ . For example for $\mathcal{P}([1]) = \{\emptyset, \{a\}\}$ , we have the following $3$ classes: $\{\emptyset\}$ , $\{\{a\}\}$ , $\{\emptyset, \{a\}\}$ . For $\mathcal{P}([2]) = \{\emptyset, \{a\}, \{b\}, \{a,b\}\}$ , we have the following $11$ classes: $\{\emptyset\}$ , $\{\{a\}\}$ , $\{\emptyset, \{a\}\}$ , $\{\{a,b\}\}$ , $\{\emptyset, \{a,b\}\}$ , $\{\{a\},\{b\}\}$ , $\{\emptyset, \{a\},\{b\}\}$ , $\{\{a\},\{a,b\}\}$ , $\{\emptyset, \{a\},\{a,b\}\}$ , $\{\{a\},\{b\},\{a,b\}\}$ , $\{\emptyset, \{a\},\{b\},\{a,b\}\}$ .",['combinatorics']
4428284,Is the Fourier Basis a complete basis of $L^2$ if we consider functions that differ on a set of Lebesgue-measure 0 to be distinct?,"I am currently working on replicating some results from the following paper: https://arxiv.org/abs/1803.00798 https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.2846 In it, the authors define the square-integrable functions somewhat unconventional in that they consider two functions that differ on a set of Lebesgue-measure zero to be distinct. So effectively, we are not looking at the equivalence classes but the functions themselves. If I understand the paper correctly, the authors later make use of the fact that the Fourier basis is a complete orthogonal basis of the space of square-integrable functions when approximating a functional integral over $L^2$ .
My question is, whether this way of defining $L^2$ creates a problem with the desired properties of the Fourier basis. Sadly, my theoretical knowledge in this area is somewhat lacking and I would be glad to have any advice. [I originally asked this question on CrossValidated, but I was told that this is probably the more appropriate place.]","['function-spaces', 'measure-theory', 'functional-analysis']"
4428309,"If $-1 \le x \le 1$, what is the maximum value of $x+\sqrt{1-x^2}$? (Cannot use calculus method)","If $-1 \leq x \leq 1$ , what is the maximum value of $x+\sqrt{1-x^2}$ ? (Cannot use calculus method) As stated in the problem, I can't use calculus. Therefore, I'm using things I've learnt so far instead: One of the things I have tried most successfully is using trig substitutions... For example, if I substitute $x = \sin \phi$ , this yields $\sin \phi + \cos \phi$ But what should I do next? Or there are any methods else to solve the problem?","['trigonometry', 'linear-algebra']"
4428399,Show that a local martingale is a true martingale if and only if it is a process of class DL,"Let $M$ be a local martingale. Show that $M$ is a (true) martingale if and only if it is a process of class DL. Quick definitions: $\mathscr{S}_a$ is the class of all stopping times $T$ such that $P(T \le a) = 1$ . DL is the class of all right-continuous processes $\{X_t\}_t$ such that $\{X_T\}_{T \in \mathscr{S}_a}$ is uniformly integrable for every $0 < a < \infty$ . Since $M$ is a local martingale, there exists a sequence of stopping times $\{S_n\}$ such that $S_n \uparrow \infty$ and $M_t^{(n)} \triangleq M_{t \land S_n}$ is a martingale. We can define a variant $S_n' = S_n \land n$ such that $S_n'$ is in $\mathscr{S}_n$ and that $S_n'$ is still nondecreasing and $S_n' \uparrow \infty$ . Then $M_t^{(n)'} \triangleq M_{t \land S_n'}$ is a martingale. If $M$ is of class DL, then for every $n \ge 0$ , the family $\{ M_{T} \}_{T \in \mathscr{S}_n}$ is uniformly integrable. I'm stuck on where to go from here.","['local-martingales', 'martingales', 'stopping-times', 'probability-theory']"
4428485,Density of simple functions in a generating $\pi$ system for $L^p$,"Let $(E, \mathcal{E}, \mu)$ be a measure space and define the simple functions to be of the form $$\sum_{k = 1}^{n} a_{k} \mathbf{1}_{A_{k}}$$ where $a_{k} \in \mathbb{R}$ and $A_{k} \in \mathcal{E}$ with $\mu(A_k) < \infty$ . We know that these functions are dense in the Lebesgue space $L^{p}(E, \mathcal{E}, \mu)$ when $p \in [1, \infty)$ . See for example Theorem 3.13 in Rudin, Real and Complex Analysis. My question: does this still hold true when we restrict only to have $A_{k} \in I$ , where $I$ is a $\pi$ system which generates $\mathcal{E}$ , that is $\mathcal{E} = \sigma(I)$ ? Let's call such functions extra simple functions. It would suffice then to show that the extra simple functions can approximate $\mathbf{1}_A$ where $A \in \mathcal{E}$ is arbitrary with $\mu(A) < \infty$ .",['measure-theory']
