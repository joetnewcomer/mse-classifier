question_id,title,body,tags
3927338,Genus from Riemann surfaces to algebraic curves on $\mathbb{F}_p$,"The notion of genus is quite intuitive on compact Riemann surfaces, as it is a topological notion that can be visualized in this case. On the other hand, when defining an algebraic curve in characteristic $p > 0$ (elliptic curve over $F_p$ for instance), we lose that intuition. I've read that Model Theory can ""transfer"" some statements over $\mathbb{C}$ to statements over the algebraic closure of $F_p$ (with possible exceptions on finitely many $p$ and restrictions on the set of ""transferable"" statements) [Edit: See https://webusers.imj-prg.fr/~adrien.deloro/teaching-archive/Moskva-ACF.pdf Theorem 3.9 ""Cross-Characteristic Transfer and Ax’s Theorem"" for reference] Now here are my questions : Could we obtain the notion of ""genus"" on algebraic curves over $F_p$ from the fact that this genus exists and can be expressed algebraically on compact Riemann surfaces ? Can we use Model Theory to deduce results about curves on $F_p$ based on what we know on compact Riemann surfaces ? How far could we go (Riemann-Roch, structure of an Elliptic Curve group, etc.) ?","['riemann-surfaces', 'model-theory', 'logic', 'field-theory', 'algebraic-geometry']"
3927417,What an implication means when stating the solutions of an equation?,"I am having trouble understanding what the following statement means: $$x^2=4 \implies x=2 \hspace{1em} \text{or} \hspace{1em} x=-2.$$ How can we relate this statement to a material conditional? I am trying to see it as a material conditional. So on the left hand side we have the equation $A(x)$ which takes different truth values for different $x$ . The same goes for the right hand side which we can denote it as $B(x)$ . Now does the original statement means that $$\forall x \in \mathbb{R}: A(x) \implies B(x).$$ If I substitute the values of $x$ in these two expressions then the conditionals are all true. So how can such a statement make sense when we are trying to solve an equation or when stating the solutions of the equation? Does it mean that in the case that the left hand is true then the value ""we substituted"" was $2$ or $-2$ in order to make the right hand side also true?","['propositional-calculus', 'predicate-logic', 'first-order-logic', 'logic', 'discrete-mathematics']"
3927448,Does the categorical notion of a coequalizer coincide with the usual quotient objects considered in algebra?,"I heard that the category-theoretic formulation of ""quotient object"" is that of a coequalizer . Instead of taking the quotient of, say, a group $G$ by some normal subgroup, one takes the quotient (really, the coequalizer) by some diagram of the form $H\rightrightarrows G$ consisting of two arrows from some group $H$ to $G$ . I wonder: Can each quotient $G/N$ of $G$ by some normal subgroup $N$ be written as the coequalizer of some diagram $H\rightrightarrows G$ ? What about the converse: Can each coequalizer be written as the quotient by some normal subgroup? Of course, I'm not only interested in groups, but I also want to know whether the same things are true for rings/ideals, modules/submodules, and so on.","['ring-theory', 'group-theory', 'abstract-algebra', 'category-theory']"
3927499,Evaluate $T_k=\sum_{n\geq 1}\text{sech}^{2k}(n \pi)$,"The following formula is well known due to theory of elliptic functions: $\sum _{n=1}^{\infty } \text{sech}^2(n \pi )=-\frac{1}{2}+\frac{1}{2 \pi }+\frac{\Gamma \left(\frac{1}{4}\right)^4}{16 \pi ^3}$ . Question $1$ : How to evaluate the following $$T_k=\sum _{n=1}^{\infty }\text{sech}^{2k}(n \pi)$$ For instance Mathematica gives $T_2=\sum _{n=1}^{\infty } \text{sech}^4(\pi  n)=-\frac{1}{2}+\frac{1}{3 \pi }+\frac{\Gamma \left(\frac{1}{4}\right)^8}{192 \pi ^6}+\frac{\Gamma \left(\frac{1}{4}\right)^4}{24 \pi ^3}$ but I don't know how it's generated (it cannot evaluate $T_3$ , etc). Note that series of class $S_k=\sum _{n=1}^{\infty }\text{csch}^{2k}(n \pi)$ is evaluated in this post , by manipulating the normalized Eisenstein series $G_{2k}(i)=\sum_{m,n\in\mathbb R, mn\not=0}\ \frac{1}{(m+ni)^{2k}}$ . For $T_k$ the corresponding series should be $\sum_{m,n\in\mathbb R, mn\not=0}\ \frac{1}{(m+\frac12+ni)^{2k}}$ , but so far I can't see how to compute it for arbitrary $k$ . Question $2$ : By Jacobi theta identity one have $\sum _{n=1}^{\infty } \text{sech}(\pi  n)=\frac{\Gamma \left(\frac{1}{4}\right)^2}{4 \pi ^{3/2}}-\frac{1}{2}$ (Mathematica also 'remembers' this result), while no closed-form of $\sum _{n=1}^{\infty } \text{csch}(\pi  n)$ seems to exist. So, what causes the difference between two cases? More generally, is it possible to evaluate the odd-weight class $$\tilde T_k=\sum _{n=1}^{\infty }\text{sech}^{2k-1}(n \pi)$$ Any help will be appreciated.","['special-functions', 'gamma-function', 'sequences-and-series', 'modular-forms', 'elliptic-integrals']"
3927524,Are two spaces obtained by a quotient of the other homeomorphic?,"I was thinking about an exercise on quotient topology that asks the following: If $Y$ and $X$ are two topological spaces such that $Y$ is a quotient of $X$ and $X$ is a quotient of $Y$ , need $X$ and $Y$ to be homeomorphic? I think the answer is, in general, no, because quotient maps between spaces do not give all the requesites of an homeomorphism between spaces. But I don't see any counterexample to this. Could you give me any hint? Thanks in advance.","['general-topology', 'quotient-spaces']"
3927570,Demonstration of the impossibility to draw a parallel through a point using only a straightedge.,"From the responses to this question , it appears to be well know that it is impossible to trace a parallel   to a straight line: $\ell$ through  a point: $P$ ,  using exclusively a straightedge. Can you provide a demonstration of such fact?","['geometry', 'geometric-construction']"
3927646,Small angle approximation on cosine,"The problem is Using the small angle approximation of cosine, show that $3-2\cos(x)+4\cos^2(x)\approx 5-kx^2$ where k is a positive constant I did solve it by using $\cos^2(x)=1-\sin^2(x)$ on the $\cos^2(x)$ , by plugging $\sin^2(x)\overset{x\to 0}{\approx}x^2$ and $\cos(x)\overset{x\to 0}{\approx}1-\frac{x^2}{2}$ to get $$3-2(1-\frac{x^2}{2})+4(1-x^2)=5-3x^2$$ hence $k=3$ . But why does using $\cos^2(x)\overset{x\to 0}{\approx}(1-\frac{x^2}{2})^2$ doesn't work out? I originally tried plugging that into the $\cos^2(x)$ but got another complete thing. why?","['trigonometry', 'solution-verification', 'approximation']"
3927668,Show the inequality : $a^{(2(1-a))}+b^{(2(1-b))}+c^{(2(1-c))}+c\leq 1$,"Claim : Let $0.5\geq a \geq b \geq 0.25\geq c\geq 0$ such that $a+b+c=1$ then we have : $$a^{(2(1-a))}+b^{(2(1-b))}+c^{(2(1-c))}+c\leq 1$$ To prove it I have tried Bernoulli's inequality . For $0\leq x\leq 0.25$ we have : $$x^{2(1-x)}\leq 2x^2$$ As in my previous posts we have the inequality $x\in[0,0.5]$ : $$x^{2(1-x)}\leq 2^{2x+1}x^2(1-x)$$ applying this for each variables $a,b$ we want to show : $$2^{2a+1}a^2(1-a)+2^{2b+1}b^2(1-b)+2c^2+c\leq 1$$ Now by Bernoulli's inequality we have: $$2^{2x+1}\leq 2(1+2x)$$ Remains to show : $$2(1+2a)a^2(1-a)+2(1+2b)b^2(1-b)+2c^2+c\leq 1\quad(1)$$ The function : $$f(x)=2(1+2x)x^2(1-x)$$ is concave for $x\in [\frac{1}{8}+\frac{\sqrt{\frac{19}{3}}}{8},0.5]$ So we can use Jensen's inequality remains to show : $$2\left(2(1+a+b)\left(\frac{a+b}{2}\right)^2\left(1-\left(\frac{a+b}{2}\right)\right)\right)+2c^2+c\leq 1$$ So it reduces to a one variable inequality and using derivatives it's not hard to show that : $$g(c)=2f\left(\frac{1-c}{2}\right)+2c^2+c\leq 1$$ For $c\in[0,1-2\left(\frac{1}{8}+\frac{\sqrt{\frac{19}{3}}}{8}\right)]$ It shows the equality case $a=b=0.5$ and $c=0$ but inequality $(1)$ is false for the other equality case $a=0.5$ and $b=c=0.25$ . We have also the inequality for $x\in[0.25,0.5]$ (we can prove it using logarithm and then derivative) $$x^{(2(1-x))}\leq x^22^{-5(x-0.25)(x-0.5)+1}$$ Using Bernoulli's inequality : $$x^22^{-5(x-0.25)(x-0.5)+1}\leq 2(x^2+x^2(-5(x-0.25)(x-0.5)))$$ So Remains to show : $$2(a^2+a^2(-5(a-0.25)(a-0.5)))+2(b^2+b^2(-5(b-0.25)(b-0.5)))+2c^2+c\leq 1\quad (2)$$ Question : Have you a proof ? How to show $(2)$ ? Thanks in advance !","['exponentiation', 'jensen-inequality', 'multivariable-calculus', 'inequality', 'derivatives']"
3927748,Doubts about directional derivative in one variable,"Sometimes I read that directional derivatives for function in one variables are right and left derivatives. But this doesn't make sense to me. The only unit vector in $\mathbb{R}$ are $\pm1 $ , so we are saying that: $$D_1f(x)=f'_+(x)=\lim_{t\to 0^+} \frac{f(x+t)-f(x)}{t}$$ $$D_{-1}f(x)=f'_-(x)=\lim_{t\to 0^-} \frac{f(x+t)-f(x)}{t}$$ But by definition: $$D_{\mathbf{v}} f(\mathbf{x})=\lim_{t\to 0} \frac{f(\mathbf{x}+t\mathbf{v})-f(\mathbf{x})}{t}$$ By this logic: $$D_{\mathbf{-v}} f(\mathbf{x})=\lim_{t\to 0} \frac{f(\mathbf{x}-t\mathbf{v})-f(\mathbf{x})}{t}$$ I can substitute $-t=u$ ( $-t$ doesn't assume infinite times the value $0$ so I can apply composite function limit theorem), so: $$D_{\mathbf{-v}} f(\mathbf{x})=\lim_{u\to 0} \frac{f(\mathbf{x}+u\mathbf{v})-f(\mathbf{x})}{-u}=-\lim_{u\to 0} \frac{f(\mathbf{x}+u\mathbf{v})-f(\mathbf{x})}{u}=-D_{\mathbf{v}} f(\mathbf{x})$$ This should mean that: $$f'_+(x)=-f'_-(x)$$ That in general is false!
Wouldn't be more correct to say that by convention in one variable we use derivative along the unit vector $1$ and that right and left derivatives are simply the right and left directional derivatives along the unit vector $1$ .
Thanks in advance.","['multivariable-calculus', 'derivatives']"
3927769,Suguru puzzles with no given clues,"A Suguru puzzle consists of a bunch of polynominoes fit together in a rectangular shape. The goal of the puzzle is to fill all the cells in all the polynominoes with the numbers $1$ to the amount of cells the polynomino has. A pentomino requires you to fill in $1$ , $2$ , $3$ , $4$ , and $5$ , for example. A number is not allowed to touch the same number, not diagonally either! Recently I was wondering if it is possible to construct a Suguru puzzle without any given clues (no given numbers) with exactly one solution. I was able to find some examples so I started wondering for which sizes this would work. So my question is: For which $a$ , $b$ is it possible to construct an $a\times b$ Suguru puzzle with no given clues and exactly one solution? I was able to prove some cases, but not all cases. I was able to prove that if $a$ and $b$ are both odd, than there exists a possible arrangement with no given clues and exactly one solution unless both $a$ and $b$ are equal to $3$ , in which case it is impossible to construct one. If one side is even and the other one is odd, I was able to proof that there exists an arrangement if the even side is at least $6$ and the odd side at least $3$ . If the odd side is equal to $1$ than no arrangement with the other side being even exists. If the even side is equal to either $2$ or $4$ , I am not sure whether it will ever be possible to construct a puzzle with no given clues and exactly one solution. So far I have not been able to find any working examples, so I believe there aren't any at all. With the help of the $8 \times 8$ configuration which was provided by Richard Tobin, I was also able to prove that an arrangement exists if $a$ and $b$ are both even and at least 8, however, I still haven't figured out what happens if $a$ and $b$ are both even, but not both at least 8. With the help of the $6 \times 6$ configuration provided by Kris van Bael I was able to prove all $even \times even$ are possible if both evens are at least 6, slightly improving the argument from before. With the help of Kris van Bael who also provided a working 5x4 I was able to prove all $4 \times odd$ work if the odd is at least 5. It is not hard to prove that $3 \times 4$ is impossible. This brings all the unsolved cases down to two categories. $2\times n$ for all $n$ and $4\times n$ for all even $n$ Does anyone know how I could prove all cases, or just some of the cases I have not yet already solved?","['puzzle', 'combinatorics']"
3927793,Which of the following statements are true about finite cyclic groups?,"$1.$ If $G$ is a finite cyclic group, then for every subgroup $H,K$ of $G,$ we have either $H\subset K$ or $K\subset H.$ $2.$ If for every subgroup $H,K$ of a finite group $G,$ we have either $H\subset K$ or $K\subset H,$ then $G$ is cyclic. I have figured a counterexample for the first statement:
Let $G=\mathbb{Z}_{6}.$ Then $H=2\mathbb{Z}_{6}$ and $K=3\mathbb{Z}_{6}$ are subgroups of $\mathbb{Z}_{6}$ but neither $H\subset K$ nor $K\subset H.$ So the first statement is wrong. However, I am not sure about the 2nd statement.","['group-theory', 'abstract-algebra', 'finite-groups', 'cyclic-groups']"
3927834,Classification of countably infinite Abelian groups?,"There’s a pretty simple classification of finitely generated Abelian groups, and there’s a relatively understandable classification of countably infinite Abelian $p$ -groups for any prime $p$ .  But my question is, what is the general classification upto isomorphism of countably infinite Abelian groups? How complicated is it to state?  Is there a sentence we can write, like “Two countably infinite Abelian groups are isomorphic if and only if ...”?","['group-theory', 'abstract-algebra', 'abelian-groups', 'group-isomorphism']"
3927901,Calculating the limit $ \lim_{x \to 0} \frac{e^{\sin^2(x)} - e^x}{\sin(2x)}$ without L'Hospital's rule,"I calculated this limit before using L'Hospital and got a result of $-1/2$ , but I was wondering if we could somehow calculate this without resorting to L'Hospital and derivatives in general. Any tips and ideas will be appreciated.","['limits-without-lhopital', 'limits', 'calculus', 'derivatives']"
3927906,A hard geometry problem involving harmonic divisions,"Let acute triangle $ABC$ . Let $A_1$ and $A_2$ the intersections of the circle of diameter $(BC)$ and the altitude from $A$ to $BC$ ( $A_1$ is closer to $A$ than $A_2$ ). Similarily define points $B_1$ , $B_2$ , $C_1$ , $C_2$ . Let $A'$ the intersection of $B_1C_2$ and $B_2C_1$ . Similarily define points $B'$ and $C'$ . Prove that $AA'$ , $BB'$ and $CC'$ concur. My idea: Maybe it helps to see that $(A,H;A_1,A_2)$ (where $H$ is the orthocenter of triangle $ABC$ ) is a harmonic division. Also $H$ is the radical center of the 3 drawn circles, so $B_1C_1B_2C_2$ is cylcic (by power of a point). It is obvious by the figure that $A'$ must lie on $BC$ . But I don't know how to prove it. Can you please help me? Thanks in advance! If it helps, the problem comes from a Romanian book about harmonic divisions, but the solution is ommited.","['triangles', 'triangle-centres', 'geometry']"
3927911,Bound for Gaussian integral,"Let $g_1, ..., g_n \stackrel{\text{iid}}{\sim} N(0,1)$ and $T$ be an arbitrary index set for a bounded family of distributions, say bounded by $M$ . Let $Y_i(t), i \in \{1, ..., n\}, t \in T$ be iid from the family and also independent of the Gaussians. I want to show the following: $$E \left(\sup_{t \in T} \left \lvert \sum_{i=1}^n g_i Y_i(t) \right \rvert \right) \leq 2E\left(\sup_{t \in T} \sum_{i=1}^n g_i Y_i(t)  \right) + C\sqrt{n}$$ where $C$ is a fixed constant. I really don't know where to start, or which symmetrization principle I'm supposed to use (if any). I know that for $\epsilon_i$ iid Bernoulli's and independent of everything else that the LHS of the inequality is equal to $$E\left(\sup_{t \in T} \left \lvert \sum_{i=1}^n \epsilon_i |g_i| Y_i(t) \right \rvert \right)$$ but I don't know any tricks on how to leverage this. Since asking, I produced another rather feeble attempt (perhaps it's useful to someone).  Denote $g \cdot Y(t)$ the corresponding dot product of vectors and decompose the absolute value into pieces: $$\begin{aligned} LHS & = E\left(\sup_{t \in T} \bigg( g \cdot Y(t) + 2(-g \cdot Y(t) \mathbb{1}(g \cdot Y < 0) \bigg)  \right)\\
& \leq E\left(\sup_{t \in T} \sum_{i=1}^n g_i Y_i(t)  \right) + 2 E\left(\sup_{t \in T} - g \cdot Y(t) \mathbf{1}(g \cdot Y(t) < 0)  \right)\end{aligned}$$","['statistics', 'gaussian-integral', 'normal-distribution', 'inequality']"
3927974,Pythagoras Theorem Proof,"Is my logic correct below to prove the Pythagoras Theorem ? Thanks. Area Rectangle R \begin{align*}
R &= WL\\
  &=(2a+b)(2b+a)\\
  &=4ab+2a^2+2b^2+ab\\
  &=5ab+2a^2+2b^2
\end{align*} Total Area Yellow Triangles T \begin{align*}
T &= 10(\frac{ab}{2})\\
&=5ab
\end{align*} Calculate Area $c^2$ \begin{align*}
c^2 &= R-T-a^2-b^2 \\
   &=5ab+2a^2+2b^2-5ab-a^2-b^2\\
   &=a^2+b^2
\end{align*} ∎ Proof by rearrangement","['euclidean-geometry', 'solution-verification', 'geometry']"
3927979,Balancing chemical equations - Matrix determinant is zero for specific reaction,"I am currently working on a program which takes a chemical equation as input and returns the balanced chemical equation instead. It finds the appropiate chemical coefficients with matrix operations, using the formula $x = A^{-1}b$ . I am using JAMA for calculations involving matrices. According to their docs, it calculates $A$ 's inverse using Eigenvalue Decomposition of both symmetric and nonsymmetric square matrices Let's take an example to better visualize this. Suppose the following chemical equation with denoted coefficients for every compound: $aCa(OH)_2 + bH_3PO_4 \rightarrow cCa_3(PO_4)_2 + dH_2O$ The computer creates an equation for every element: $P: b - 2c = 0$ $H: 2a + 3b - 2d = 0$ $Ca: a - 3c = 0$ The last equation needs to be $a = det(A)$ for three reasons. We need a square matrix (and that's why oxygen's equation is omitted) to compute it's inverse and determinant. Secondly, we need to give a numerical value to one of the variables to be able to get a numerical result, and thirdly (as far as I understood because I learned nothing about matrices in school yet) $adj(A)$ is multiplied by $1/det(A)$ to get $A^{-1}$ which causes the computed coefficients to sometimes be non-integer numbers. I am not sure of how or why setting $a$ to $det(A)$ works, neither this is the most efficient way of doing it. $A = \left[\begin{array}{cccc}0&1&-2&0\\2&3&0&-2\\1&0&-3&0\\1&0&0&0\end{array}\right]$ $b = \left[\begin{array}{c}0\\0\\0\\det(A)\end{array}\right]$ After dividing every number to the GCD the computed vector $x$ will be: $x = A^{-1}b = \left[\begin{array}{c}6\\4\\2\\12\end{array}\right]$ = $\left[\begin{array}{c}3\\2\\1\\6\end{array}\right]$ And of course, the balanced chemical equation is $3Ca(OH)_2 + 2H_3PO_4 \rightarrow Ca_3(PO_4)_2 + 6H_2O$ This program works perfectly for even more complex equations: $299H_2SO_4 + 10K_4Fe(CN)_6 + 122KMnO_4 \rightarrow 60CO_2 + 5Fe_2(SO_4)_3 + 188H_2O + 60HNO_3 + 162KHSO_4 + 122MnSO_4$ $9Fe_{36}Si_5 + 836H_3PO_4 + 192K_2Cr_2O_7 \rightarrow 324FePO_4 + 45SiO_2 + 128K_3PO_4 + 384CrPO_4 + 1254H_2O$ However, in the process of testing, I found a chemical equation that produces the following error: java.lang.RuntimeException: Matrix is singular. Let's write everything down: $aB_{10}H_{12}CNH_3 + bNiCl_2 + cNaOH \rightarrow dNa_4(B_{10}H_{10}CNH_2)_2Ni + eNaCl + fH_2O$ $B: 10a - 20d = 0$ $C: a - 2d = 0$ $Na: c - 4d - e = 0$ $H: 15a + c - 24d - 2f = 0$ $Cl: 2b - e = 0$ This time, the code omits the equation for three elements ( $N, Ni, O$ ) because it would result in an overdetermined system and a non-square matrix. $A = \left[\begin{array}{cccccc}10&0&0&-20&0&0\\1&0&0&-2&0&0\\0&0&1&-4&-1&0\\15&0&1&-24&0&-2\\0&2&0&0&-1&0\\1&0&0&0&0&0\end{array}\right]$ This is completely unexpected, both for me and the script. Using a matrix calculator, I found out that $A$ has no inverse: its determinant is zero. I analyzed the matrix and found that $10a - 20d = 0$ and $a - 2d = 0$ are really
the same equation. I'm not sure if this has something to do with the matrix being singular. What should I do to make this work? Should I use a different method for finding solutions and representing my system of equations, or do I have to do something specific when $det(A) = 0$ ? I've read a little bit about Gaussian Elimination and LU Decomposition, but I don't seem to understand them very well. In case I have to use another method for solving the system, which one is the most suitable for this script? Also, I would be happy to get some details about it if possible. Keep in mind I'm a beginner in this matrix field. Any help is appreciated!","['systems-of-equations', 'determinant', 'matrices', 'linear-algebra', 'chemistry']"
3928016,Convex analysis question: Cone and convex hull,"I'm reading a publication, where a small bit of convex analysis is used in one of the lemmas. I have Rockafellar's book, but I haven't found anything too helpful yet as I am pretty new to the area. Lemma. Let $I \subset \{1, \dots, n\}$ for some $n \in \mathbb{N}$ with cardinality $\alpha = \#I$ . Define a cone as $$C = \left\{x \in \mathbb{R}^n  : \sum_{i \notin I} |x_i| \leq \sum_{i \in I} |x_i|\right\}.$$ Then for a set $$ G = \left\{x \in \mathbb{R}^n : \sum_{i=1}^n \mathbb{1}_{\{x_i \neq 0\}} \leq \alpha,~~||x|| = 1\right\},$$ we have that $$ C \cap \{x \in \mathbb{R}^n : ||x|| = 1\} \subset 3 \cdot \text{Conv}(G),$$ where $||\cdot||$ is the $L^2$ norm. Proof of Lemma Idea: A Theorem in Rockafellar states that a convex hull is equal to the set of convex combinations of $H$ , where for a vector $\lambda \geq 0$ we can define as $$K(G) := \left\{x \in \mathbb{R}^n : \lambda_1 x_1 + \dots + \lambda_n x_n ~\text{a combination in}~G~\text{s.t.}~~\sum_i^n \lambda_i = 1\right\},$$ and $\text{Conv}(G) = K(G)$ . But I'm not sure how to proceed?","['convex-geometry', 'convex-analysis', 'functional-analysis', 'convex-hulls']"
3928047,Is there a function whose second derivative is the inverse square of itself?,"Namely, is there a function $f(x)$ that satisfies $$\frac{d^2f(x)}{dx^2} = \frac{1}{(f(x))^2}$$ I've been messing with an overly simplified physical system in which a body has a gravitational attraction towards another body much more massive (allowing me to assume that the second body doesn't move), and I got the following differential equation: $$\frac{d^2x(t)}{dt^2} = \frac{Gm}{(x(t))^2}$$ That got me wondering if such differential equation has a known solution.",['ordinary-differential-equations']
3928081,Proving invariance of a functional under transformation,"Suppose we have a functional of the form $$
J(y)= \int_{x_{0}}^{x_{1}} f(x,y,y')dx
$$ and a smooth transformation $X=\theta (x,y: \epsilon)$ , $Y=\psi (x,y: \epsilon)$ where $\epsilon$ is a parameter. I have the following definition of invariance under a transformation: Definition. The functional $J$ is invariant under the above transformation if, for all $\epsilon$ sufficiently small, in any subinterval $[a,b] \subset [x_{0},x_{1}]$ we have that $$
\int_{a}^{b} f(x,y,y')dx= \displaystyle \int_{a_{\epsilon}}^{b_{\epsilon}} f(X,Y,Y')dX
$$ for all smooth functions $y$ defined on $[a,b]$ . Here, $a_{\epsilon}=\theta (a,y(a): \epsilon)$ , $b_{\epsilon}=\theta (b,y(b): \epsilon)$ Once said that, I'm interested in proving the invariance of the functional $$
J(y)=\int_{x_{0}}^{x_{1}} xy'^2dx
$$ under the transformation $$
\begin{cases}
X=x+2\epsilon x \ln(x)\\
Y=(1+\epsilon)y
\end{cases}
$$ My attempt: This would be easier if we could solve explicitly for $x$ in the first equation of the transformation, but, since we can't, suppose $x=\Theta (X; \epsilon)$ such inverse function exist(locally and for $\epsilon$ sufficiently small because of the inverse function theorem). On the other hand, we easily get $y=1/(1+\epsilon) Y$ . Therefore, $$
y'(x)=\displaystyle \frac{dy}{dx}=\displaystyle \frac{Y'(X)}{(1+ \epsilon) \Theta'(X)}
$$ which implies that $$
xy'dx= \frac{\Theta(X)}{\Theta '(X)} \frac{Y'(X)^2}{(1+\epsilon)^2}dX
$$ So, as you can see, if I could prove $$
\frac{\Theta(X)}{(1+\epsilon)^2 \Theta '(X)}=X
$$ I'd be done, but I could not prove it, and actually, it doesn't seem true. Any help? Thanks in advance. Update: Of course we can also try to express $XY'(X)^2 dX$ in terms of $x$ and $y'(x)$ . In this case, we get $$
XY'(X)^2 dX= \displaystyle \frac{x+2 \epsilon x ln(x)}{1+ 2 \epsilon ln(x)+2 \epsilon} (1+ \epsilon)^2 y'(x)^2 dx 
$$ In any case, it is not clear to me how to reconcile this expressions with the definition :( .","['multivariable-calculus', 'functional-analysis', 'calculus-of-variations']"
3928086,A closed and convex subset of an Hilbert space on which the norm does not attain its maximum,"I want to prove that the norm $\lvert \lvert \cdot \rvert \rvert _2$ has no maximum on $X=\{x\in \ell^2(k):\sum_{n \in \mathbb{N}} 2^{1/n}\lvert x_n \rvert^2 \le 1\}$ , where $k=\mathbb{R}$ or $\mathbb{C}$ . I have managed to see that this is close and bounded, I can't figure out if this is also convex and I don't know if this would be helpful. I've tried to do some contradiction argument, I have observed is that since $2^{1/n}>1$ , if there is $x \in X$ which attains the maximum for $\lvert \lvert \cdot \rvert \rvert _2$ , then this maximum, say $M$ , must be strictly smaller than $1$ , otherwise $x$ would be outside of $X$ . Now my idea would be that in this case we could add a small $\epsilon$ to some term $x_n$ of $x$ in such a way that $\sum_{n \in \mathbb{N}} 2^{1/n}\lvert x_n \rvert^2$ stays below one, but I can't carry out the computations, even because if $\lvert \lvert x \rvert \rvert _2<1,$ then it does not follow that $\sum_{n \in \mathbb{N}} 2^{1/n}\lvert x_n \rvert^2<1$ , as we can see taking the element $(2^{-1/4}, 0, 0, \ldots)$ .","['hilbert-spaces', 'banach-spaces', 'functional-analysis']"
3928123,"How is this read correctly in maths , and meaning (about: sets)?","$J \subseteq \left\{1,2,..,n\right\}$ is an index set ....   Fix $J
\subseteq \left\{1,2,..,n\right\}$ and let $S=\left\{x_j : j \in
J\right\}$ The problem I'm having here is ""Fix"" , what does that mean ? I think ""fix"" refers to some constant value, or in this case SOME set we are chosing? My other question is how is the set $S$ read correctly? I would read it like that: The set $S$ contains elements $x_j$ where each index $j$ is from index set $J$ "" I hope someone can help :)","['elementary-set-theory', 'notation', 'terminology']"
3928151,"Given two functions $f$ and $g$, prove that $f=g$ iff dom$f=$ dom $g$ and for every $x\in$ dom $f$, $f(x)=g(x)$","Given two functions $f$ and $g$ , prove that $f=g$ iff dom $f=$ dom $g$ and for every $x\in$ dom $f$ , $f(x)=g(x)$ Proof: Assume $f=g$ . Then we have $$(a_0,b_0)\in f\Rightarrow (a_0,b_0)\in g\wedge(a_1,b_1)\in g\Rightarrow (a_1,b_1)\in f$$ so $$a_0\in\mbox{dom }f\Rightarrow a_0\in\mbox{dom }g\wedge a_1\in\mbox{dom }g\Rightarrow a_1\in\mbox{dom }f$$ and $$\mbox{dom }f\subseteq\mbox{dom }g\wedge\mbox{dom }g\subseteq\mbox{dom }f$$ therefore, $$\mbox{dom }f=\mbox{dom }g$$ Now let $x\in\mbox{dom }f$ . Then $\exists y=f(x)\in\mbox{rng }f$ such that $(x,y)\in f$ . But if $f=g$ , then $(x,y)\in g$ . $$\therefore\forall x\in\mbox{dom }g, f(x)=g(x)$$ Now to show the converse, let $\mbox{dom }f=\mbox{dom }g$ and $\forall x\in\mbox{dom }f$ , $f(x)=g(x)$ . Then we have $\forall a_0\in\mbox{dom }f$ , $\exists b_0\in\mbox{rng }f$ such that $(a_0,b_0)\in f$ . But since $\forall x\in\mbox{dom }f$ , $f(x)=g(x)$ , we have $$(a_0,b_0)\in f\Rightarrow(a_0,b_0)\in g$$ Likewise, $\forall a_1\in\mbox{dom }g$ , $\exists b_1\in\mbox{rng }g$ such that $(a_1,b_1)\in g$ . But again, since $\forall x\in\mbox{dom }f$ , $f(x)=g(x)$ , we have $$(a_1,b_1)\in g\Rightarrow(a_1,b_1)\in f$$ Therefore, we have $f\subseteq g$ and $g\subseteq f$ , and thus $f=g$ . $\blacksquare$ I'm just looking for proof validation here. Is my argument sound? Thanks in advance. I'm using the usual set theoretic definition of a function: Let $A$ and $B$ be sets. A function from $A$ to $B$ is a nonempty relation $f\subseteq A\times B$ that satisfies the following two conditions: Existence - $\forall a\in A$ , $\exists b\in B$ such that $(a,b)\in f$ . Uniqueness - If $(a,b)\in f$ and $(a,c)\in f$ , then $b=c$ .","['elementary-set-theory', 'proof-writing', 'functions', 'solution-verification']"
3928192,Does conjugation preserve trace? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question If I were to conjugate by a matrix, what kind of matrix would it have to be in order to preserve trace? Does the trace of a matrix have anything to do with its spectrum? Any insights are appreciated, I'm just a bit confused.","['matrices', 'trace', 'linear-algebra']"
3928231,Need help solving Laplace Transform Question,"For context: I am just starting to learn LaPlace Transform and am stumped with this question. Find the PS of $x''+x'-12x=sin(3t) + e^{-4t}+e^{3t}$ Where $x(0)=0, x'(0)=0$ using LaPlace Transform My Approach: Let X(s)= $\mathscr{L}[x(t)]$ Apply the LaPlace Transform to both sides of the equation $\mathscr{L}[x''] + \mathscr{L}[x'] - 12\mathscr{L}[x]$ = $\mathscr{L}[sin(3t)] +\mathscr{L}[e^{-4t}] +\mathscr{L}[e^{3t}]$ to get $s^2X(s)-sx(0)-x'(0)+sX(s)-x(0)-12X(s) = \cfrac{3}{s^2+9}+\cfrac{1}{s+4}+\cfrac{1}{s-3}$ Plugging in the IC's $x(0) = 0$ and $x'(0)=0$ and then factoring $X(s)$ we get $X(s)[s^2+s-12] = \cfrac{3}{s^2+9}+\cfrac{1}{s+4}+\cfrac{1}{s-3}$ You should then make the RHS have a common denominator so you get $X(s)[s^2+s-12] = \cfrac{3(s+4)(s-3)}{(s^2+9)(s+4)(s-3)}+\cfrac{(s^2+9)(s-3)}{(s+4)(s^2+9)(s-3)}+\cfrac{(s+4)(s^2+9)}{(s+4)(s-3)(s^2+9)} = \cfrac{2s^3+4s^2+21s-27}{(s^2+9)(s-3)(s+4)}$ Dividing both sides by $[s^2+s-12]$ AKA $(s+4)(s-3)$ to get $X(s)$ by itself We get $X(s) = \cfrac{2s^3+4s^2+21s-27}{(s^2+9)(s-3)^2(s+4)^2}$ Now I Believe I'm supposed to use partial fraction decomposition here to make taking the Inverse LaPlace Transform easier so... $\cfrac{2s^3+4s^2+21s-27}{(s^2+9)(s-3)^2(s+4)^2} = \cfrac{As+B}{s^2+9} + \cfrac{C}{s-3} + \cfrac{D}{(s-3)^2} + \cfrac{E}{(s+4)} + \cfrac{F}{(s+4)^2}$ Then multiplying both sides by the LHS denominator and simplifying we get $2s^3+4s^2+21s-27 = (As+B)(s-3)^2(s-4)^2+C(s^2+9)(s-3)(s-4)^2+D(s^2+9)(s+4)^2+E(s+4)(s-3)^2(s^2+9)+F(s^2+9)(s-3)^2$ Here is where I get stuck. How do I solve for A,B,C,D,E,F? This seems ridiculously complex and seems like so much algebra that it makes me think that partial fraction decomposition isn't the right way. So what I've come to ask is partial fraction decomposition to right way to tackle this question? Is there an more efficient way besides partial fraction decomposition? If there's not and partial fraction decomposition is the correct way, How can I solve for A,B,C,D,E,F? Can someone help me determine the algebra, I'm not used to partial fractions being this long unless I've made an error somewhere... Thank You For Any Help","['laplace-transform', 'ordinary-differential-equations']"
3928270,Cyclic normal subgroup of perfect group is in the center,"I've been trying for a while to solve an exercise/prove a proposition, which at first seemed elementary, but now I even doubt if it's a true proposition. The proposition is: Let $G$ be a perfect group and let $K$ be a cyclic, normal subgroup of $G$ . Show that $K$ is contained in the center of $G$ (i.e. $Z(G)$ ). Obviously it's enough to show that the generator of $K$ is in the center, i.e. it commutes with every element of the group, but I can't figure out why that's true. I thought about using Grun's lemma and show that if the generator of $K$ wasn't in the center, then its $Z(G)$ -coset would be in the center of $G/Z(G)$ , but it turned out to be the same approach as the first one. Then I thought about showing that the orbit (under the action of conjugation) of the generator is a singleton and I found that $\sqrt{|G|}$ is a lower bound for the size of the stabilizer $|Stab(x)|$ , where $x$ is the generator, but that's just for a finite group $G$ , and I couldn't actually find a higher bound (it'd be fine if I could show that x is actually stable under conjugation from any $g\in G$ ). Thank you in advance for any help.","['group-theory', 'normal-subgroups', 'cyclic-groups']"
3928278,Prove that an irrotational vector field is conservative by showing the $\nabla$ of the scalar potential of $v$ written as an integral is equal to $v$,"I am trying to prove that if $\vec v$ is a continuously differentiable vector field define on $\mathbb R^n$ and satisfies $$\frac {\partial v_i} {\partial x_j} = \frac {\partial v_j} {\partial x_i}$$ then $\nabla f(x) = \vec v(x)$ where $f(x) = \int_0^1 x \cdot \vec v(tx) \, \mathrm d t$ . This is my attempt: It suffices to prove $v_i(x) = \frac {\partial f(x)} {\partial x_i}$ . I shall use implicit summation over $j$ here when I apply the chain rule. I first use the fundamental theorem of calculus: $$\begin{align} v_i(x) & = \int _0^1  \frac {\partial v_i(tx)} {\partial t}\, \mathrm d t + v_i(0) \\ & = \int_0^1 \frac {\partial v_i(tx)} {\partial x_j} \frac {\partial x_j} {\partial t}\, \mathrm d t + v_i(0) \\ & = \int_0^1 \frac {\partial v_j(tx)} {\partial x_i}  \frac {\partial x_j} {\partial t}\, \mathrm d t + v_i(0) \end{align}$$ I shall also use implicit summation over $j$ here when I expand out the dot product: $$\begin{align} \frac {\partial f(x)} {\partial x_i} & = \int_0^1 \frac {\partial } {\partial x_i} \left( x_j v_j(tx)\right) \, \mathrm d t \\ & = \int_0^1 v_i(tx) + x_j \frac {\partial v_j(tx)} {\partial x_i}\, \mathrm d t\end{align}$$ I am rather stuck here though and not sure how to proceed. Any hints would be appreciated. Have I made any mistakes so far?","['vector-fields', 'multivariable-calculus', 'scalar-fields']"
3928330,"Calculation of the integral $\int_0^\infty d x\, x^{2n+1} \tanh(\pi x)\, \log (1 - e^{-2\pi x})$ for non-negative integer $n$","In some physics problem I ran into the integral with a non-negative integer $n$ of the form: \begin{align}
I(n) \equiv \int_0^\infty d x\, x^{2n+1} \tanh(\pi x)\, \log (1 - e^{-2\pi x}) \ ,
\end{align} which I cannot perform even with Mathematica for general $n$ , but for small $n$ the values are given analytically (e.g. in Eq. (3.10) and (3.23) of https://arxiv.org/pdf/1708.00305.pdf ): \begin{align}
\begin{aligned}
I(0) &= - \frac{1}{8}\,\log 2 + \frac{9}{16\pi^2}\,\zeta(3) \ ,\\
I(1) &= - \frac{1}{64}\,\log 2 - \frac{3}{32\pi^2}\,\zeta(3) + \frac{225}{128\pi^4}\,\zeta(5) \ ,
\end{aligned}
\end{align} It is likely that the integral generally takes the form \begin{align}
I(n) = c_0\,\log 2 + \sum_{i=1}^{n+1}\,c_i\,\zeta(2i+1)\ .
\end{align} Can we fix the values of the coefficients $c_i~(i=0,1,\cdots,n+1)$ for integer $n\ge 0$ ? Thank you for your help in advance.","['integration', 'riemann-zeta', 'definite-integrals']"
3928359,Show that $\angle PAB = \angle CAQ$,Given a triangle $\triangle ABC$ with a point $P$ in the perpendicular bisector of $BC$ . Take point $Q$ also in the bisector of $BC$ s.t. $\angle ABP + \angle ACQ =180^o$ and such that $P$ and $Q$ are in the interior of $\angle BAC$ . Prove that $\angle PAB = \angle CAQ $ I tried to find some circles in this configuration. Tried to find some interesting locus and I failed so I went for trigonometry. This problem is equivalent to show that $\frac{BP}{AP} = \frac{BQ}{AQ}$ ... wait this looks like an apolonian circle. Ok it looks like $CQ$ is tangent to the apollonian circle that goes throught $P$ and $Q$ EDIT: it is not tangent. Can we work it with coaxial systems? Tha apollonian circle through fixed points $AB$ form a coaxial system orthogonal to the system of circles that pass throught $A$ and $B$,"['euclidean-geometry', 'trigonometry', 'geometry']"
3928383,"Do we need compactness hypothesis in Lemma 6.2 in Lee's ""Introduction to Smooth Manifolds""?","The following lemma is from Lee's "" Introduction to Smooth Manifolds ."" Lemma 6.2. Suppose $A\subseteq\mathbb R^n$ is a compact subset whose intersection with $\{c\}\times\mathbb R^{n-1}$ has $(n-1)$ -dimensional measure zero for every $c\in\mathbb R$ . Then $A$ has $n$ -dimensional measure zero. Do we need to hypothesize that $A$ is compact? I think the author included the compactness hypothesis so that the elementary proof (no measure theory) in the book will work. But if I prove the lemma using Tonelli's theorem, I think $A$ does not need to be compact: Let $\chi_A$ be the characteristic function of $A$ . We have to prove that $\int\chi_Adm^n=0$ . By Tonelli's theorem, $\int\chi_Adm^n=\int\left(\int(\chi_A)_cdm^{n-1}\right)dm(c)$ , where $(\chi_A)_c(x_2,\ldots,x_n)=(c,x_2,\ldots,x_n)$ . Since we are given that $\{c\}\times\mathbb R^{n-1}$ has $(n-1)$ -dimensional Lebesgue measure zero, it follows that $\int(\chi_A)_cdm^{n-1}=0$ for all $c\in\mathbb R$ . Thus $\int\chi_Adm^n=\int 0dm(c)=0$ . Am I right, or did I miss something?","['measure-theory', 'lebesgue-measure', 'fubini-tonelli-theorems', 'smooth-manifolds']"
3928462,An interesting application of the Hahn-Banach Theorem.,"Let $X$ be a normed space and $x_1,x_2\in X$ nonzero elements. Show that there are functionals $F_1,F_2\in X'$ such that $F_1(x_1)F_2(x_2)=\lVert x_1\rVert \lVert x_2\rVert$ and $\lVert F_1\rVert \lVert x_1\rVert =\lVert F_2\rVert \lVert x_2\rVert$ . My attempt: My idea was to define a functional $f_1:\langle \{x_1\}\rangle\to \mathbb{R}$ , by $f_1(\alpha x_1)=\alpha \lVert x_2\rVert.$ Then, by using Hahn-Banach, I extend $f_1$ to a functional $F_1:X\to \mathbb{R}$ such that $F_1(x_1)=\lVert x_2\rVert$ and $\lVert F_1\rVert=\lVert f_1\rVert=\frac{\lVert x_2\rVert}{\lVert x_1\rVert}$ . After that, I tried to define a functional $F_2:X\to \mathbb{R}$ such that $F_2(x_2)=\lVert x_1\rVert$ and $\lVert F_2\rVert=1$ , but I coundn't do that.",['functional-analysis']
3928574,Is $B(H \otimes K)$ generated by $B(H) \otimes B(K)$?,"Let $H$ and $K$ be Hilbert spaces and $H \otimes K$ their Hilbert space tensor product. Given $S \in B(H)$ and $T \in B(K)$ , there is a unique operator $S \otimes T \in B(H \otimes K)$ such that $$(S \otimes T) (h \otimes k) = Sh \otimes Tk, \quad h\in H, k \in K.$$ Is the following equality true? $$B(H \otimes K) = \overline{\text{span}}\{S \otimes T: S \in B(H), T \in B(K)\}$$ If so, how can one show this?","['hilbert-spaces', 'tensor-products', 'functional-analysis', 'operator-algebras']"
3928578,Lebesgue measure of $\partial A$ given $m(A^o) = m(\overline{A})$,"Suppose that $A \subseteq \mathbb{R}$ , and consider the Lebesgue outer measure $m: \mathcal{P}(\mathbb{R}) \to [0,\infty]$ on $\mathbb{R}$ . Denote by $A^o$ , $\overline{A}$ and $\partial A$ the interior, closure and boundary of $A$ , respectively. Suppose that $m(A^o) = m(\overline{A})$ . Is it then true that $m(\partial A) = 0$ ? This is certainly true if $m(A) < \infty$ . Since open sets and closed sets are Lebesgue measurable, we have that $A^o$ and $\overline{A}$ are Lebesgue measurable, and hence so is $\partial A = \overline{A} \setminus A^o$ . Therefore, we can write $m(\overline{A}) = m(A^o) + m(\partial A)$ . By the monotonicity of the Lebesgue outer measure, we also have that $m(A^o) < \infty$ . Hence, we can cancel to conclude. But what about the case $m(A) = \infty$ ? If we take for example $A = \bigcup_{n \geq 1} (n - 1, n)$ , then $m(A^0) = m(\overline{A}) = \infty$ , but $m(\partial A) = m(\mathbb{N}) = 0$ . Are there any counter examples?","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3928602,"Infinitely many $ n \in \mathbb{N} $ such that $ n^2+1 $ has two divisors $ a,b $ such that $a-b=n $","Prove that there is infinitely many $ n \in \mathbb{N} $ such that $ n^2+1 $ has two divisors $ a,b $ such that $a-b=n $ . It is obvious that if $ p\mid n^2+1 $ then $\gcd(p,n)=1$ . I tried to use the Chinese remainder theorem, but I got nothing. Please help me.","['elementary-set-theory', 'number-theory', 'modular-arithmetic', 'contest-math']"
3928606,Lagrange Multiplier In Proving AM-GM inequality [duplicate],"This question already has an answer here : Proving the AM-GM Inequality with Lagrange Multipliers (1 answer) Closed 3 years ago . I am dealing with a question in Stewart Calculus Book which states the following: Using Lagrange multiplier, find the maximum of the function $$f(x_1,x_2,...,x_n)=(x_1x_2...x_n)^{1/n}$$ subject to the constraint $$\sum_{i=1}^{n} x_i=c$$ ,where $x_i$ 's are all positive, and use this to prove the AM-GM inequality. Actually in AM-GM inequality, it holds for all nonnegative numbers, but I think the author wants to get rid of the case where $x_i$ 's are all zero, where Lagrange multiplier can no longer be applied. And the result I obtained is: $f$ will have its maximum/minimum (the arithmetic mean), or even saddle point at $(\frac{c}{n},\frac{c}{n},...,\frac{c}{n})$ , since Lagrange method gives no definiteness on the behaviour of the critical points found (whether max, min, or saddle). And my question is: In this case, how do we show that the point I found gives the maximum value of the function? My teacher gave me an argument that said ""the minimum value would be some place very near to $0$ , so the arithmetic mean will not be the minimum value"". But I think the argument is not persuasive enough to explain why the arithmetic mean is the maximum value. So, I hope that there will someone who are willing to give their opinions on this. Thanks.","['multivariable-calculus', 'a.m.-g.m.-inequality']"
3928656,Locally extend a morphism defined on stalks,"I have the following: a locally free sheaf $F$ on a scheme $X$ , an $\mathcal{O}_X$ -module $G$ and, for a fixed $x\in X$ , a morphisms of $\mathcal{O}_{X,x}$ -modules $F_{x}\to G_{x}$ . Is it true that we can extend this morphism locally, that is, to find $V$ open neighborhood of $x$ and a morphisms of $\mathcal{O}_V$ -modules $F|_V\to G|_V$ inducing the original one on stalks? My attempt really stops when trying to find this suitable $V$ , because I know that, since we're working in abelian categories, for any $s^x\in F_x$ I can find an open neighborhood $U$ of $x$ and a section $s\in F(U)$ such that $s_x=s^x$ , and mapping this with the morphism on stalks to, say, $t^x$ , lead to consider $W\ni x$ and a section $t\in G(W)$ such that $t_x=t^x$ . But it doesn't seem to be that this takes me somewhere in order to define a morphism of sheaves, not even on a small neighborhood, beacuse any of these small neighborhood that I can find depends on the element $s^x$ . (I also think that it would be easy to find such a morphism on any open $U$ if I had maps on stalks for every $x$ in $U$ , but here just one morphism in given). Any help is appreciated! Thanks in advance","['algebraic-geometry', 'sheaf-theory']"
3928718,Evaluate $\lim_ {n \to\infty} n (n^ {\frac {1} {n}}-2^ {\frac {1} {n}}) ^a$ as $a$ varies in the reals,"How would you evaluate $\lim_ {n \to\infty} n (n^ {\frac {1} {n}}-2^ {\frac {1} {n}}) ^a$ with $n$ an integer and $a$ a real parameter? I tried to apply the most common criteria and to compare this with some other easier functions but without success.
Thank you","['real-numbers', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
3928797,Proving spheres are orthogonal,"Given two spheres in $\mathbb{R}^3$ : $x^2+y^2+z^2=2ax; \ \  \ x^2+y^2+z^2 = 2by$ and $a,b>0$ , and $\gamma$ the intersection of the spheres, show that for any $p_0 \in \gamma$ , the spheres are orthogonal at $p_0$ . I am not sure I fully understand the problem but here is what I tried: First of all assuming $p_0$ is some point s.t each sphere can be represented as a function $z_1, z_2 : \mathbb{R}^2 \to \mathbb{R}$ in a neighborhood of $p_0$ (if it isn't we can use another variable), what I think we want to show is that the tangent spaces to $z_1, z_2$ are orthogonal at this point, i.e $\langle\nabla z_1(p_0), \nabla z_2(p_0)\rangle=0.$ But when I calculate this I get that it does not equal 0: $z_1 = \sqrt{2ax-x^2-y^2}, z_2 = \sqrt{2by-x^2-y^2}$ $\nabla z_1 = (\frac{2a-2x}{2 \sqrt{2ax-x^2-y^2}}, \frac{-2y}{2 \sqrt{2ax-x^2-y^2}})$ $\nabla z_2 = (\frac{-2x}{2 \sqrt{2by-x^2-y^2}}, \frac{2b-2y}{2 \sqrt{2by-x^2-y^2}})$ $\langle\nabla z_1(p_0), \nabla z_2(p_0)\rangle \neq 0.$ Can someone explain where is my mistake?","['spheres', 'tangent-spaces', 'solid-geometry', 'multivariable-calculus', 'vector-analysis']"
3928820,Why is the first eigenvalue on the principal component always superior to the second eigenvalue?,"I'm learning PCA and I wanted to know why the first eigenvalue on the principal component is always superior at second eigenvalue. Thanks for helping me Ps:  I tried to unterstand before asking this question but to be honest, I had difficult to unterstand what I was reading on book.
I would like just an easy answer who can allow me to unterstand it better","['matrices', 'statistics', 'principal-component-analysis', 'eigenvalues-eigenvectors']"
3928842,Is this stopping time finite a.s ? bounded?,"Let $B$ and $W$ be two independent Brownian motions on $(\mathcal{F}_{t})$ . Is $\tau$ defined as: $$\tau = \inf \{ t \ge 0: B_{t} \ge W_{t} + e^{-t} \}$$ almost surely finite ? Bounded ? I would say that it can not be finite due to Brownian motion and therefore can't be bounded. But I don't know how to prove it.
What do you think guys ?","['measure-theory', 'bounded-variation', 'stopping-times', 'probability-theory', 'stochastic-calculus']"
3928861,A problem about positive definite matrices,"Given $A\in\mathbb{R}^{n\times n}$ , show that all eigenvalues of A has negative real part if and only if for each positive definite matrix $C\in\mathbb{R}^{n\times n}$ , there exists an unique positive definite matrix $B\in\mathbb{R}^{n\times n}$ which satisfies $BA+A^TB=-C$ I thought that since $B$ and $C$ are both positive definite, then they can be simultaneously diagonalized, then I cannot go any further. Can anyone help me? Many thanks","['hurwitz-matrices', 'matrices', 'linear-algebra', 'linear-control', 'positive-definite']"
3928955,Prove $x_n=\frac{n}{2^n}$ converges to $0$,"Prove $x_n=\frac{n}{2^n}$ converges to $0$ . Since $2^n>\frac{n^2}{2}$ for all $n \in \mathbb{N}$ we have $$-\frac{2}{n}<\frac{n}{2^n}<\frac{2}{n}.$$ Since $\frac{1}{n} \rightarrow0$ as $n \rightarrow \infty$ , by Squeeze theoreom $\frac{n}{2^n}$ converges to $0$ . Is there anything wrong this proof?","['limits', 'solution-verification', 'sequences-and-series', 'real-analysis']"
3929030,"Let $\mu$ and $ \nu$ be measures on $(\mathscr \Omega , \mathscr F )$","I have two questions Question 1 and Question 2 see below. This is an extension of(Problem 32.9 from the Billiglsey Probability and measure.) $$$$ Let $\mu$ and $ \nu$ be measures on $(\mathscr \Omega , \mathscr F )$ and suppose that $\mathscr F^o$ is a σ-field contained in $\mathscr F$ . Then the restrictions $\mu^ο$ and $ \nu^ο$ of $\mu$ and $\nu$ to $\mathscr F^o$ are measures on $(\mathscr \Omega , \mathscr F^o )$ . Assume that $ \nu^ο$ is σ-finite (which implies that $ \nu$ is also σ-finite).
Let $ \nu_{ac}$ , $ \nu_{s}$ , $ \nu_{ac}^o$ , and $ \nu_{s}^o$ be, respectively, the absolutely continuous and singular parts of $ \nu$ and $ \nu^ο$ with respect to $\mu$ and $\mu^ο$ . Part A : Show $ \nu_{ac}^o (E) \ge \nu_{ac} (E)  $ and $ \nu_{s}^o (E)   \le \nu_{s} (E) $ for $E   \in  \mathscr F^o$ . I think I managed to solve Part A , since for every $E   \in  \mathscr F^o$ Case 1 : IF $\nu_{s} (E)=0$ , then $\nu_{s} (E) \ge \nu_{s}^o (E)    $ Case 2 : IF $\nu_{s} (E)>0$ , then $\mu^o (E) =\mu^o (E)=0 $ , then $ \nu_{ac}^o (E) = \nu_{ac} (E) =0  $ and $\nu_{s} (E) = \nu (E) - \nu_{ac} (E) =  \nu (E) =  \nu^o (E) = \nu^o (E) = \nu^o (E) - \nu_{ac}^o (E) = \nu_{s}^o (E) $ . So , $ \nu_{s} (E)  \ge \nu_{s}^o (E)$ Question 1 Does this mean $ \nu_{ac}^o (E) \ge \nu_{ac} (E)  $ and $ \nu_{s}^o (E)   \le \nu_{s} (E) $ for $E   \in  \mathscr F^o$ ??? My Question 2 is in Part B below: Part B : I need to give an example to show that strict inequality may hold. As a hint I should let $\mu$ and $ \nu$ be probability measures  measures on a two-point space and then take $\mathscr F^o = {(\emptyset , \Omega  )} $ (Problem 32.9 from the Billiglsey Probability and measure.)  a short solution for this part  which is not complete is the below For the case where $\mathscr F^o = { (\emptyset , \Omega)  } $ , in which $ \nu^o$ is absolutely continouse with respect to $ \mu^o$ ( provided that $ \mu (\Omega) \ge 0 )$ and hence $\nu_{s}^o$ vanishes. I do not understand this solution, and why it vanishes.","['self-learning', 'measure-theory', 'probability-theory', 'examples-counterexamples']"
3929115,How to prove $ ((S\times_A T)(1))_{(f\otimes g)}\cong S(1)_{(f)}\otimes_{S_{(f)}}(S\times_A T)_{(f\otimes g)}\otimes_{T_{(g)}} T(1)_{(g)}$?,"I'm currently trying to solve Exercise 5.11 in chapter 2 of Hartshorne: Let $S,T$ be $\mathbb{Z}_{\geq 0}$ -graded rings with $S_0=T_0=A$ , and define their Cartesian product $S\times_A T$ to be $$
S\times_A T=\bigoplus_{d\geq 0}S_d\otimes_A T_d.
$$ I already showed that $\operatorname{Proj}(S\times_A T)\cong \operatorname{Proj}(S)\times_{\operatorname{Spec}(A)}\operatorname{Proj}(T)$ . Now I am trying to show that $$
\mathcal{O}_{\operatorname{Proj}(S\times_A T)}\cong p_1^{*}\mathcal{O}_{\operatorname{Proj}(S)}\otimes p_2^{*}\mathcal{O}_{\operatorname{Proj}(T)}
$$ where $p_1$ and $p_2$ are the structure maps making $\operatorname{Proj}(S\times_A T)$ the product of $\operatorname{Proj}(S)$ and $\operatorname{Proj}(T)$ . I'm stuck at the following very specific part of the proof: Is it true that $$
((S\times_A T)(1))_{(f\otimes g)}\cong S(1)_{(f)}\otimes_{S_{(f)}}(S\times_A T)_{(f\otimes g)}\otimes_{T_{(g)}} T(1)_{(g)}
$$ For all $f\in S_d$ and $g\in T_d$ where $d\in\mathbb{Z}_{\geq 1}$ is arbitrary? I constructed a map going from right to left by mapping $s/f^n\otimes[(s'\otimes t')/(f\otimes g)^n]\otimes t/g^n$ to $((ss')\otimes (tt'))/(f\otimes g)^{2n}$ and showed that it is surjective. However, I'm struggling to prove that the map is injective. Edit: As suggested by KReiser, I tried to directly construct an inverse. However, I'm only able to do this under the extra assumption that $S$ and $T$ are generated by $S_1$ resp. $T_1$ as $A$ -algebras, so that we can assume that $f$ and $g$ are of degree $1$ . Here's why. To justify the well-definedness of the map $$
((S\times_A T)(1))_{(f\otimes g)}\to S(1)_{(f)}\otimes_{S_{(f)}}(S\times_A T)_{(f\otimes g)}\otimes_{T_{(g)}}T(1)_{(g)}\\
(s\otimes t)/(f\otimes g)^n\mapsto (s/f^n)\otimes 1\otimes (t/g^n)
$$ we would like to start by defining it on the homogeneous pieces of $(S\times_A T)(1)$ and then pass to the localisation. However, for this to work, we would have to relax the target to $S(1)_{f}\otimes_{S_{(f)}}(S\times_A T)_{f\otimes g}\otimes_{T_{(g)}}T(1)_{g}$ , and only afterwards show that the image is contained in $S(1)_{(f)}\otimes_{S_{(f)}}(S\times_A T)_{(f\otimes g)}\otimes_{T_{(g)}}T(1)_{(g)}$ , because otherwise, the map $$
s\otimes t\in (S\times_A T)(1)\mapsto (s/1)\otimes 1\otimes (t/1)
$$ wouldn't be well-defined. So at this point we have a map of abelian groups $(S\times_A T)(1)\to S(1)_{f}\otimes_{S_{(f)}}(S\times_A T)_{f\otimes g}\otimes_{T_{(g)}}T(1)_{g}$ . Now to pass to the localisation on the source, as we have only a morphism of abelian groups at this point, we would have to justify by hand why it is well defined. That is, we want to show that if $(s\otimes t)/(f\otimes g)^n$ is equal to $0$ inside $(S\times_A T)(1)_{f\otimes g}$ , then $(s/1)\otimes 1\otimes (t/1)$ is $0$ inside $S(1)_{f}\otimes_{S_{(f)}}(S\times_A T)_{f\otimes g}\otimes_{T_{(g)}}T(1)_{g}$ . If we now suppose that $f$ and $g$ are of degree $1$ , then this can be done: if $e$ is the degree of $s$ and $t$ , then $$
(s/1)\otimes 1\otimes (t/1)=((s/f^{e+1})sf^{e+1})\otimes 1\otimes ((t/g^{e+1})tg^{e+1})=\\
=(sf^{e+1}/1)\otimes ((s\otimes t)/(f\otimes g)^{e+1})\otimes (tg^{e+1}/1)=0
$$ because of the middle term. However, I don't see how to do this without assuming that $f$ and $g$ are of degree $1$ .","['quasicoherent-sheaves', 'algebraic-geometry', 'projective-schemes', 'sheaf-theory']"
3929122,How can we find the derivative of a circle if a circle is not a function?,"If we consider the equation of a circle, $x^2+y^2=r^2$ , then I understand that $dy/dx$ can be computed in the following way via implicit differentiation: \begin{align}
2x + 2y\frac{dy}{dx} &= 0 \\
\frac{dy}{dx} &= -\frac{2x}{2y} = -\frac{x}{y} \, .
\end{align} Although I feel comfortable deriving this result, I don't really understand how I should interpret it. On an intuitive level, the formula $dy/dx = -x/y$ seems to suggest that the gradient of the tangent to any given point $(x,y)$ is $-x/y$ . However, since the curve $x^2+y^2=r^2$ fails the vertical line test , it doesn't look like it is even a function. Usually, $dy/dx$ can be thought of as a shorthand for $$
\lim_{h \to 0}\frac{y(x+h)-y(x)}{h} \, .
$$ However, in this case each $x$ -value maps to two $y$ -values, and so the limit definition doesn't seem to apply here. So what does $dy/dx$ actually represent in this context?","['calculus', 'derivatives']"
3929133,Show that $\exists c \in \mathbb{R} : f(c) = 0$,"Let $f : \mathbb{R} \longrightarrow \mathbb{R}$ be a differentiable function such as $\forall x \in \mathbb{R}, f'(x) > 2$ . What could I do to show that $\exists c \in \mathbb{R} : f(c) = 0$ ? At first I thought I could use the mean value theorem but it did not help me. In fact, I don't really understand why $f(x)$ couldn't be strictly positive $\forall x \in \mathbb{R}$ . Would you know how to proceed ? Thanks for your help.","['functions', 'derivatives', 'analysis', 'real-analysis']"
3929152,Averages clarifications!,If i know an average such as ( catch an average of one rat every 39 minutes ) what is the average number of rats caught in 10 hours ? is it as simple as dividing 600/39 (600 = 10 hours in minutes) and multiplying this value by number of rats caught in 39 minutes ? in this case 15.38 rats caught in 10 hours.,['statistics']
3929173,Show $ρ( A_i \bigcup B_j ) = ρ( A_i - B_j ) + ρ(B_j - A_i ) + ρ( A_i \bigcap B_j ) $ finite,"If $\mu$ and $\nu$ are $\sigma$ -finite measures on $(\mathscr \Omega , \mathscr F )$ and $\nu$ is absolutely continuous with respect to $\mu$ : ( $\nu << \mu $ ) and let $ ρ  = \mu + \nu $ I want to prove that $ρ$ is $\sigma$ -finite and that $\nu << ρ $ : I tried the below: $\text {  There exists   } \{A_{n}\}   \text { , such that  }\mu(A_n)<\infty  \text { and } \bigcup_{n=1}^\infty A_n = \Omega $ . $\text { Also}, \text {  There exists   } \{B_{n}\}   \text { , such that  }\nu(B_n)<\infty  \text { and } \bigcup_{n=1}^\infty B_n = \Omega $ . $\text { Then , for } \bigcup_{i=1}^\infty \bigcup_{j=1}^\infty (A_i \bigcup B_j)   = \Omega $ , $ρ( A_i \bigcup B_j ) = ρ( A_i - B_j ) +  ρ(B_j -  A_i )   + ρ( A_i \bigcap B_j ) $ $\text {at this point , it is enough to show that each of the above terms are finite ,              
             because that would mean } ρ( A_i \bigcup B_j )<\infty   \text {  so ,   ρ is }  \sigma-finite. $ How can we show that $\ ρ( A_i - B_j ) \text { and }  ρ(B_j -  A_i ) \text { are finite} $ ?","['self-learning', 'measure-theory', 'probability-theory']"
3929200,How valid is this concept or does this already have a name?,"I was going through my school papers and found an interesting question, so I experimented a bit more and found out a pattern, so I made a formula for such matrices. $$A = \begin{bmatrix}x&-(x-1)\\x+1&-x\end{bmatrix}$$ where $x > 0$ is an integer $$A^n =
\begin{cases}
I,  & \text{if $n$ is even} \\
A, & \text{if $n$ is odd}
\end{cases}$$ where $I$ is identity matrix of order 2. I just wanted to know if this has been found before or whether it has a name too or if there are some cases that does not obey this. Hope someone can format my question properly, I'm new to this community. Hope this is the correct way of putting things together too. Thanks!!",['matrices']
3929240,Relationship between PDFs and derivatives,"There is a well-known formula in probability theory - and by the way I m looking for the name of this formula or theorem? (Because it would be nice to be able to refer to it by a name) - : $f_Y(y) = f_X(x)|dx/dy|$ The main question is: how to intuitively interpret the role of the derivative of $x$ with respect to $y$ ? Which by the way is harder to interpret given the kind of ""inverse"" notion $x$ with respect to $y$ instead of y with respect to x. Aslo y is a function of $x$ e.g. $y = g(x)$ I imagine it is some kind of variation in the output domain with respect to the input domain... but I can't formulate it well... Reference to formula in Wikipedia entry for pdf function under the section ""scalar to scalar"" .","['probability-theory', 'probability']"
3929478,Why does the limit definition of e fail?,"I'm trying to calculate the following limit: $$\lim_{x\to +\infty} \frac{(1+\frac{1}{x})^{x^2}}{e^x}$$ I tried to use the fact that $e^x = \lim_{x\to +\infty}(1+\frac{1}{x})^{x}$ , but this gives $\frac {e^x}{e^x} = 1,$ which is not the answer. WolframAlpha says the answer should be $\frac {1}{\sqrt{e}}$ . I was able to get the right answer using Taylor series of $\ln(1+ \frac{1}{x})$ , but I can't find a good reason why my first approach fails. Can someone please help? Thanks!","['limits', 'calculus', 'limsup-and-liminf']"
3929501,Is the set of allowed sequences for the golden mean shift uncountable?,"Consider infinite sequences of two symbols, $L$ and $R$ . The set of infinite sequences of these two symbols that do not contain consecutive $R$ 's, i.e. $LRRL...$ , is called the golden mean shift space. Is the set of all such sequences uncountable? COMMENT: Standard diagonalization arguments don't work, since you can produce forbidden sequences through the diagonalization process. However, there are certain properties that lead me to believe that this set is uncountable—for example, that there are an infinite number of sequences that contain $n$ $R$ 's for every integer $n\geq 1$ .",['elementary-set-theory']
3929567,Generalizing two infinite products for $\operatorname{sinc}(x)$ and their 'dual' infinite product,"$\newcommand{\sinc}{\operatorname{sinc}}$ Throughout, let $m,k$ be positive integers, $x>0$ a real number, and denote $\sinc(z)=\sin(z)/z$ with $\sinc(0)=1$ . A famous result of Euler gives $\sinc(x)$ as an infinite product: $$
\prod_{k=1}^{\infty}\cos\left( 2^{-k}x\right)=\sinc(x)
$$ Less well-known (although mentioned in Mathematica 's documentation for Product) but in a similar vein is $$
 \prod_{k=1}^{\infty}1-\frac{4}{3}\sin^2(3^{-k}x) = \sinc(x)
$$ In fact, each of these are special cases of a more general formula that I found (unfortunately after looking at this question ): $$
\prod_{k=1}^{\infty} \frac{1}{m} \csc(m^{-k} x)\sin(m^{1-k} x) = \sinc(x)
$$ The product term reduces to nice sums depending on the parity of $m$ . Further, for even $m$ the product telescopes by double-angle and the result is immediate. Note that the right-hand side does not depend on $m$ . My question : what would we get if we switched the roles of $m,k$ in the product? That is, what is the nature of $$
\prod_{m=1}^{\infty} \frac{1}{m} \csc(m^{-k} x)\sin(m^{1-k} x) =S_k(x)
$$ The cases $k=1$ and $k\ge 2$ need to be treated separately. For $k=1$ , the product term is $\frac{1}{m}\sin (x) \csc \left(\frac{x}{m}\right)$ , which approaches $\sinc(x)$ as $m\to \infty$ . Thus $S_1(x)=\delta_0(x)$ , as for $x\ne 0$ the product diverges to zero. For $k\ge 2$ , note that $$
\lim_{m\to\infty} m^{2 k-2} \log\left(\frac{\sin \left(x m^{1-k}\right) \csc \left(x m^{-k}\right)}{m}\right) = \frac{-x^2}{6},
$$ implying the product converges by comparison with the corresponding series $\sum_{m\ge 1} m^{2-2k}$ . Below are pictures of estimates for the $100^{th}$ partial products, for $k=2,\ldots, 6$ and $-6\pi\le x\le 6\pi$ . Experience has taught me that a closed-form is unlikely but I would nevertheless like to know how $S_k(x)$ depends on $k$ , in particular if they are a family of sinc functions as well.","['special-functions', 'closed-form', 'infinite-product', 'trigonometry', 'convergence-divergence']"
3929573,What is known about Lebesgue's universal covering problem in three dimensions?,"Lebesgue's universal covering problem is a relatively well-known open problem in geometry, asking for the convex set of minimal area which contains all planar sets of diameter 1. While the problem is not yet fully resolved, we have pretty good bounds on its size (between $0.832$ and $0.84409$ ) and, as I understand it, a decent sense of what an optimal solution ought to look like (some minor trimming-off from Pál's 1920 solution). What progress has been made on the analogous problem in three dimensions? As a very crude starting point, we can take a prism of height $1$ whose base is any solution to the covering problem in two dimensions, but this is almost certainly suboptimal; for instance, given any starting solution with an axis of symmetry (such as the one above), we can find two congruent regions of positive volume whose points are all of distance $1$ from each other (by cutting along planes normal to a sort of space diagonal), and then remove one of those regions. Are there more sophisticated modifications we can make, or entirely different regions to use? (After all, the best 2D solutions do not look much like the 1D solution crossed with a unit interval.)","['solid-geometry', 'convex-geometry', 'geometry', 'reference-request']"
3929582,"Traverse all permutations of pairs, triples, etc. in a minimal number of batches?","I have a collection of test cases $t_1, \ldots, t_n$ for my software. I suspect my tests themselves have a bug, in which some of them share global state and fail if run in the right order. I would like to find this by running all tests in some order. If the run fails, I'll employ a minimization technique to find a smallest example. Otherwise, my plan is to run the test suite in a different order until I find a failure or give up. I would like to choose the order in which I run my tests intelligently. For example, if on the second run I run the test cases in the reverse order of the first, the following holds: for every pair of test cases $t_i$ and $t_j$ , I have performed one run in which $t_i$ came before $t_j$ and one run in which $t_j$ came before $t_i$ . I would like to achieve something similar for triples of test cases, in $3! = 6$ runs. However, my own exhaustive search suggests that for $n \geq 5$ this is impossible. What is the smallest number $k$ such that there exists $k$ permutations of $\{1, \ldots, n\}$ containing all triples in all orders between them? Is there a simple scheme for generating such permutations? Is there a scheme which tries all $m$ -tuples, for each $m$ ? For $n = 4$ the set of permutations $(0, 1, 2, 3), (0, 3, 2, 1), (1, 3, 0, 2), (2, 1, 0, 3), (2, 3, 0, 1), (3, 1, 2, 0)$ tries all triples. One notes that $(0, 1, 2, 3)$ is here but $(3, 2, 1, 0)$ isn't. Is it ever possible to be optimal with respect to both pairs and triples? (i.e. try all orderings of pairs with the first two permutations and all triple-orderings with the first however-many-it-takes permutations)? Is it possible to be optimal with respect to all tuple sizes simultaneously?",['combinatorics']
3929716,Prove $\sum_{k=0}^n\binom{n}{k}\cos\left(\frac{k\pi}{2} - \frac{n\pi}{4}\right) = \sqrt{2^n}$,"While working on a separate problem, I stumbled upon a beautiful fact that I simply couldn't prove.. I suspect it requires some tools/tactics I've not encountered yet. The statement is the following: $$\sum_{k=0}^n\binom{n}{k}\cos\left(\frac{k\pi}{2} - \frac{n\pi}{4}\right) = \sqrt{2^n}$$ From my humble experience, I've got two suspicions for a successful approach: Complex number manipulation and working with the real part; Some kind of Fourier transform... this really just reminds me of it. I couldn't come up with anything nice using complex numbers and I'm not that familiar with Fourier transforms yet, so I don't have the right intuition there. Furthermore, this sum is just begging to employ Newton's binomial at some point, but I couldn't transform it to a form where it is applicable. Any advice is welcome! Thanks in advance! P. S. I don't opt for induction proofs, I'm rather interested in evaluating the sum on the LHS and arriving at the RHS.","['fourier-series', 'trigonometry', 'summation']"
3929751,Integrating Factor of $(x\ln(y) + xy)\mathrm{d}x + (y\ln(x) + xy)\mathrm{d}y$,"Last day I try to prove that the equation $$
(x\ln(y) + xy)\mathrm{d}x + (y\ln(x) + xy)\mathrm{d}y =0
$$ is not exact, really easy task, but I want to go further, I tried to find a integrating factor such that the equation become exact, but is so strange, I try every method that I know, but I get nothing, later I catch some hope with the factor $R = \ln\big(\ln(x+y)\big)$ , but then nothing again. There's some method to find this integrating factor? Do you know any? Thank you so much!","['integrating-factor', 'derivatives', 'ordinary-differential-equations', 'logarithms']"
3929809,Why Jacobson radical is 'radical'?,"Nilradical of ring R, $\text{nil}(R)$ is intersection of prime ideals, is radical of $(0)$ .
So I feel the name  'radical' is natural. But Jacobson radical, that is an intersection of maximal ideals, seem not to be radical of a certain ideal.
Why Jacobson radical is nevertheless called 'radical'? Thank you.","['ring-theory', 'abstract-algebra', 'terminology']"
3929818,How do we know that $\{x: x\text{ is a set and }x\text{ has at least one element}\}$ has any elements at all?,"I've been going through introductory set theory notes and came across the following. First, it is possible for a set to be an element of itself. An example of a set which is an element of itself is $$A =\{x: x\text{ is a set and }x\text{ has at least one element}\}$$ This set contains itself, because it is a set with at least one element. How do we know the set $A$ has at least one element? Could someone help understand this",['elementary-set-theory']
3929846,Hall's Marriage Theorem,"I am aware that Hall's Marriage theorem for complete matching goes like ""A bipartite graph $G$ with bipartition $(V_1, V_2)$ has a complete matching from $V_1$ to $V_2$ if and only if $$ |N(A)| \geq |A|, \forall A \subseteq V_1$$ I want to know in which cases does an equality hold, i.e. $$ |N(A)| = |A|, \forall A \subseteq V_1 $$ Any help is greatly appreciated.","['graph-theory', 'matching-theory', 'solution-verification', 'discrete-mathematics', 'bipartite-graphs']"
3929848,Show that $\lim_{n \to \infty} \prod_{k=1}^{k=n} (1+ \frac{1}{n}f(\frac{k}{n})) = e^{\int_{0}^{1}f(x) dx}$ [duplicate],"This question already has an answer here : Help finding the limit of $\lim_{n \to \infty}\prod_{k=1}^{n}\left(1+\frac{1}{n}f\left(\frac{k}{n}\right)\right)$. (1 answer) Closed 3 years ago . Let $f:[0,1] \to \mathbb{R}$ be a continuous function. Show that $\lim_{n \to \infty} \prod_{k=1}^{k=n} (1+ \frac{1}{n}f(\frac{k}{n})) = e^{\int_{0}^{1}f(x) dx}$ . Attempt:  let $\lim \prod_{k=1}^{k=n} (1+ \frac{1}{n}f(\frac{k}{n})) = l$ then $\lim_{n \to \infty} \log(\prod_{k=1}^{k=n} (1+ \frac{1}{n}f(\frac{k}{n})))= \log l \implies \lim \sum_{k=1}^{k=n} \log(1+ \frac{1}{n}f(\frac{k}{n})) = \log l$ . and working backwards we should have $\log l = \int_{0}^{1} f(x)dx = \lim_{n \to \infty} \sum_{k=1}^{k=n} \frac{1}{n} f(\frac{k}{n})$ . So here I'm trying to find the link between $\lim \sum_{k=1}^{k=n} \log(1+ \frac{1}{n}f(\frac{k}{n}))$ and $\lim_{n \to \infty} \sum_{k=1}^{k=n} \frac{1}{n} f(\frac{k}{n})$ . but $\log(1+x) \approx x$ for $x <1$ , so in order to use this approximation I have to prove that $\frac{1}{n}f(\frac{k}{n})< 1$ (or maybe I can use the fact that f is continuous and prove that f attains supremum at some point in $[0,1]$ ) after some terms and I'm stuck .  Is there any other approach which avoids this approximation?","['limits', 'definite-integrals']"
3929893,"How to show if $G$ is a finite group and $H$, $K$ normal subgroups of $G$, then $G = HK$ if and only if $G / (H \cap K) \cong G / H \times G / K$?","Let $G$ be a finite group and $H$ , $K$ normal subgroups of $G$ . Prove that $G=HK$ if and only if $G/(H\cap K)$ is isomorphic to $G/H\times G/K$ . For first part i think I have to take the function $\phi\colon G\rightarrow G/H\times G/K$ defined by $\phi(g)=(gH,gK)$ then the kernel would be $H\cap K$ but I am confusion how to show this function is onto and how to use the fact $G=HK$ ? For converse part since H and K be normal then $HK$ be a subgroup of $G$ so clearly $HK\subset G$ but now how to show that $G\subset HK$ ? It will be enough if I get a proper hint for both part.Thank you.","['group-theory', 'abstract-algebra', 'group-isomorphism']"
3929895,Prove inequality with powers of $n$,"Prove that $(a+b)^n \le a^n+b((a+1)^n-a^n)$ ,
n in N, $a \ge 0, 0<b<1$ , $a,b$ in $R$ ., With the little I remember from high school and college, I have tried the binomial expansion of $(a+1)^n$ and $(a+b)^n$ . Also tried to move $a^n$ at the LHS and use the formula: $x^n-y^n=(x-y)\left(x^{n-1}+x^{n-2}y+x^{n-3}y^2+\dots+x^2y^{n-3}+xy^{n-2}+y^{n-1}\right)$ but I am not getting anywhere. FYI this is not homework or anything. Just trying to keep my brain alive, to minimize the risk of developing dementia :)
Thank you very much in advance!!","['algebra-precalculus', 'abstract-algebra']"
3930001,"Exists a non-empty subset $K$ of $2005$ senators in senat such that for every senator, the number of his enemies in $K$ is an even number.","There are $2005$ senators in a senate. Each senator has enemies within the senate. Prove that there exists a non-empty subset $K$ of senators such that for every senator in the senate, the number of his enemies in $K$ is an even number. Let $n=2005$ and let $s_i$ be an indicator vector for $i$ -th senator. So we are working in $\mathbb{F}_2^n$ and we are looking for such a vector $u$ that $s_i\cdot u =0$ for all $i$ . Now we can make a matrix $M = [s_1,...,s_n]$ which is symmetric and thus it has $\det M =0$ since $n$ is odd. So it has nontrivial kernel so there exists $u\ne 0$ such that $Mu=0$ . Clearly $u$ is a vector we are looking for and thus we are done. Now, I wonder if it can be done without matrix theory, only in terms of linear independence and ''similar things''from linear algebra. My idea was to put $S := \langle s_1,s_2,...,s_n \rangle$ and prove that $S^{\bot} $ is nontrivial, which would be true if $s_1,s_2,...,s_n$ are lineary dependent. All I can think is that by handshake lemma and since $n$ is odd, at least one senator has even number of enemies. Suppose it is $s_1$ , so if $s_1\cdot s_j =0$ for all $j>1$ we are done. But what if that is not the case.","['graph-theory', 'linear-algebra', 'combinatorics', 'contest-math']"
3930017,"Proof Check: For a completed filtration, $\mathcal{F}_{t}^{B}$ is right continuous where $B$ is a standard Brownian motion","Let $B$ be a standard Brownian motion on $(\Omega, \mathcal{F}, \mathbb P)$ and further let $(\mathcal{F}_{t}^{B})_{t \geq 0}$ be the natural filtration associated with $B$ such that $\mathcal{F}_{t}^{B}$ for $t \geq 0$ contains all null sets. Show that the filtration is right-continuous. My approach: Trivially, we have $\mathcal{F}_{t}^{B}\subseteq \mathcal{F}_{t+}^{B}$ . Now for the "" $\mathcal{F}_{t+}^{B}\subseteq \mathcal{F}_{t}^{B}$ "", we assume that this does not hold: we choose $A \in \mathcal{F}_{t+}^{B}\setminus \mathcal{F}_{t}^{B}$ and let $N$ be the null set such that $B$ is continuous on $\overline{\Omega}:=\Omega\setminus N$ Then we can construct a sequence $(\varepsilon_{n})_{n \in \mathbb N}\subseteq(0,\infty)$ with $\varepsilon_{n}\downarrow 0$ as $n \to \infty$ such that $A$ is $B_{t+\varepsilon_{n}}-$ measurable for any $n \in \mathbb N$ . Furthermore $B$ is continuous on $A\setminus N_{A}$ where $N_{A}$ is some null set and thus since $A\setminus N_{A}$ is $B_{t+\varepsilon_{n}}-$ measurable for any $n \in \mathbb N$ , we have on $A\setminus N_{A}$ that $B_{t+\varepsilon_{n}}\xrightarrow{n \to \infty} B_{t}$ and thus $A \setminus N_{A}$ must be $B_{t}$ measurable. Hence $A = (A \setminus N_{A} )\cup N_{A}$ is $B_{t}$ -measurable which implies $A \in \mathcal{F}_{t}^{B}$ which contradicts the initial assumption. Is my proof correct? Any improvements?","['measure-theory', 'stochastic-analysis', 'stochastic-processes', 'brownian-motion', 'probability-theory']"
3930079,(Topologically) different tangent bundles on the same manifold,"Is there an example of a topological manifold in which different smooth structures give rise to tangent bundles which are not isomorphic as topological vector bundles? My (our) own attempts or remarks, mostly obtained from discussing this question with other people: By the Wu formula , the two tangent bundles would have in any case the same Stiefel--Whitney classes. This is in fact what originally motivated my question. Exotic 7-spheres are no good to produce such examples because they all have trivial tangent bundles . ""Different smooth structures correspond to different (stable) linear structures on the tangent microbundle.""","['smooth-manifolds', 'examples-counterexamples', 'vector-bundles', 'differential-topology', 'differential-geometry']"
3930097,Prove $x_n=n^2-n$ sequence converges to $\infty$,"Prove $x_n=n^2-n$ sequence converges to $\infty$ Here is my proof, I proved this using two attempt. Is there anything wrong please tell me? $1^{st}$ attempt, Let $ M \in \mathbb{R}$ and $M>0$ , $$x_n>M\Longleftrightarrow n^2-n>M\Longleftrightarrow n>\sqrt{M+\frac{1}{4}}+\frac{1}{2} $$ By the Archimedian property $N \in\mathbb{N}$ such that $$N>\sqrt{M+\frac{1}{4}}+\frac{1}{2}$$ Then $x_n>M$ whenever $n>N$ therefore $x_n=n^2-n$ sequence converges to $\infty$ $2^{nd}$ attempt, Let $ M \in \mathbb{R}$ and $M>0$ , For lower bonded, $x_n=n^2-n\geqslant n^2-\frac{n^2}{2}=\frac{n^2}{2}$ when $n\geqslant2 ,$ take $N > max\{2, \sqrt{2M}\}$ . Then $n>N$ implies $n>2$ and $n>\sqrt{2M}  $ thus for $n>N$ we have $x_n>M$ $x_n>M$ whenever $n>N$ therefore $x_n=n^2-n$ sequence converges to $\infty$","['limits', 'solution-verification', 'sequences-and-series', 'real-analysis']"
3930155,Example of a basis which is not a Riesz basis?,"I'm looking for an example of a countable ""basis"" $B=(\phi_i)_{i\in I}$ in a real Hilbert space $\mathcal{X}$ which is not a Riesz basis . So, we only require that the closure of the span of $B$ is $\mathcal{X}$ and that, for every $N\in\mathbb{N}$ and every cardinality- $N$ subset $\{i_1,\ldots,i_N\}$ of $I$ , \begin{equation}
(\forall (\alpha_{i_1},\ldots,\alpha_{i_N})\in\mathbb{R}^N)\quad 
\alpha_{i_1}\phi_{i_1} + \alpha_{i_2}\phi_{i_2}+\cdots+\alpha_{i_N}\phi_{i_N}=0\Rightarrow (\alpha_{i_j})_{1\leq j\leq N}\equiv 0.
\end{equation} (note that, by ""span"" I mean finite linear combinations.) A few observations / questions This must occur in infinite-dimensional $\mathcal{X}$ . I think this would mean that the basis decomposition operator $L\colon\mathcal{X}\to\ell_2\colon x\mapsto(\langle x\,|\,\phi_i\rangle)_{i\in I}$ is unbounded?","['hilbert-spaces', 'unbounded-operators', 'functional-analysis', 'analysis']"
3930177,One-sided alternative hypothesis,"I am a little bit confused about definitions of null and alternative hypotheses. My understanding is that the null and alternative hypotheses are defined based one a partition of the parameter space $\Theta$ . Suppose that $\Theta$ can be partitioned into two disjoint subsets $\Theta_0$ and $\Theta_1$ . Then, the null is such that $\theta \in \Theta_0$ and the alternative is such that $\theta \in \Theta_1$ . However, it is not uncommon to find examples where people test: $$ H_0: \theta = \theta_0 \quad\text{versus}\quad H_1: \theta > \theta_0 $$ or $$ H_0: \theta = \theta_0 \quad\text{versus}\quad H_1: \theta < \theta_0 .$$ For example, here : https://statweb.stanford.edu/~owen/courses/200/lec07.pdf (bottom of page 3) http://www.stat.yale.edu/Courses/1997-98/101/sigtest.htm These hypotheses don't form a partition of the parameter space (assuming $\theta\in\mathbb{R}$ ). Is it still correct to do that? Do people assume (implicitly) that the equality in the null is actually an inequality?","['statistical-inference', 'statistics', 'hypothesis-testing']"
3930184,"Evans Partial Differential Equations, chapter 7 exercise 1 (uniqueness of a regular solution to the heat equation with Neumann boundary conditions)","I would like to know how to solve the first exercise of chapter 7 of Evans' Partial Differential Equations , second edition. The problem goes like this: Let $U\subset\mathbb{R}^n$ be an open and bounded set, with smooth boundary, and let $T>0$ . Prove that there is at most one smooth solution of this initial/boundary value problem for the heat equation with Neumann boundary conditions $$\begin{cases}u_t-\Delta u=f&\text{in }U_T\\
\frac{\partial u}{\partial\nu}=0&\text{in }\partial U\times[0,T]\\
u=g&\text{in }U\times\{t=0\}
\end{cases}$$ This is what I've got so far: Suppose $u$ and $v$ are two regular solutions of the given problem. Then $u-v$ is a regular solution to the problem $$\begin{cases}u_t-\Delta u=0&\text{in }U_T\\
\frac{\partial u}{\partial\nu}=0&\text{in }\partial U\times[0,T]\\
u=0&\text{in }U\times\{t=0\}
\end{cases}$$ If $u$ is a solution to the last problem, on the one hand, we have that $$\begin{aligned}
\int_{U\times(0,T)}\Delta udx&=\int_0^T\left(\int_U\Delta u(x,\tau)dx\right)d\tau&\text{ (Fubini's theorem)}\\
&=\int_0^T\left(\int_{\partial U}\frac{\partial u}{\partial\nu}(x,\tau)dS\right)d\tau&\text{ (Green's formula)}\\
&=\int_{\partial U\times(0,T)}\frac{\partial u}{\partial\nu}dS&\text{ (Fubini's theorem)}\\
&=0
\end{aligned}$$ Because, by hypothesis, we know that $\partial u/\partial\nu$ is identically equal to zero in $\partial U\times[0,T]$ . While on the other hand $$\int_{U\times(0,T)}u_tdx=\int_{U\times(0,T)}\Delta udx=0\Rightarrow u_t=0\text{ in }U\times(0,T)\Rightarrow u\text{ is constant in }t\in (0,T)$$ Since $u=0$ in $U\times\{t=0\}$ and we are supposing this is a regular solution, by continuity we can conclude that $u$ is identically zero in $U\times [0,T]$ . But recall that the difference of two regular solutions of the first problem is a solution to the second, so if there is a regular solution to the original problem, it must be unique. What do you think? Are there flaws in my proof?  Is there another method to prove this result? Thanks in advance for your help.","['integration', 'parabolic-pde', 'multivariable-calculus', 'solution-verification', 'partial-differential-equations']"
3930277,"Prove ${M}\choose{N_1}$ ${M}\choose{N_2}$ $\leq$ ${2M}\choose{N_1+N_2}$ for $M, N_1, N_2 \in \mathbb{N},\,M\geq N_1,N_2$.","I need to prove the inequality ${M}\choose{N_1}$ ${M}\choose{N_2}$ $\leq$ ${2M}\choose{N_1+N_2}$ for $M, N_1, N_2 \in \mathbb{N},\,M\geq N_1,N_2$ . I wanted to prove it by induction. But since we are given 3 variables running, I was confused if the natural induction, as I know it, can be used in this case. Can you tell me if proving by induction is possible and necessary in this example ? We get $N_1, N_2, N_1+N_2$ fractions from ${M}\choose{N_1}$ , ${M}\choose{N_2}$ , and ${2M}\choose{M_1+M_2}$ , respectively. Without loss of generality, we assume $N_1\geq N_2.\,$ An appropriate approximation of all fractions from left to right leads to: $$ \frac{M}{N_1}\frac{M-1}{N_1 -1}\frac{M-2}{N_1-2}\cdots \frac{M-(N_1-1)}{1} \cdot\frac{M}{N_2}\frac{M-1}{N_2 -1}\frac{M-2}{N_2-2}\cdots \frac{M-(N_2-1)}{1}\leq \frac{2M}{N_1+N_2}\cdots $$ For the second fraction from the left I get: $\frac{M-1}{N_1 -1}=\frac{2M-2}{2N_1-2}\geq\frac{2M-1}{2N_1-1}\leq\frac{2M-1}{N_1+N_2-1}.$ The last term in the last inequality is not the convenient approximation for $\frac{M-1}{N_1 -1}$ unless we got $\leq$ instead of $\geq$ sign in the last inequality. Can somebody provide some hint how to go further ?
Thanks.","['inequality', 'binomial-coefficients', 'combinatorics', 'combinatorial-proofs']"
3930360,Exercise $\textrm{3.2.O}$ from Vakil's notes on AG: Is this statement/picture correct? How to interpret a map of spaces as a map between Spectrums?,"I'm not sure why we should picture this as a map from a parabola to a line. Shouldn't this be the other way around? We have that $x \mapsto x^2$ , so shouldn't the picture be from a line to a parabola? Also since $\mathbb C \to \mathbb C$ sends $x \mapsto y=x^2$ , is this the same as a map $\operatorname{Spec} \mathbb C[t] \to \operatorname{Spec} \mathbb C[t]$ where $(t-x) \mapsto (t-x^2)$ ?","['intuition', 'algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
3930367,can we say that $\lim\limits_{x\to0} \sqrt{-x^2}=0$ even if the function domain is a singleton $\{0\}$,"In Real analysis, the strict definition of a limit, we take a sequence of point that converging to the point a ""limit point"" . which is not verified here in this case we have only one element in the function domain : $$\lim_{x\to 0} \sqrt{-x^2}$$ but by replacing $0$ in the function we get $0$ so my question is : in a formal way can we say that $$\lim_{x\to 0} \sqrt{-x^2}=0$$","['limits', 'real-analysis']"
3930393,Maximum area of circles inscribed in sectors of the unit circle,"We divide the unit circle into equal sectors and inscribe circles within each sector.
Assuming we make smaller and smaller sectors, what does the total area of the inscribed circles converges to? I think I have solved it, but since I'm not a mathematician, I wonder if this is correct. I began by asking: What is the total area of all the circles in a single sector? We will start with the unit circle as the outer perimeter, with $\theta$ being half of the subtended angle, and $r_0$ being the radius of the first ring of circles, $r_1$ being the radius of the second ring of circles, and so on. Since the unit circle has a radius of 1, we can immediately deduce the following: $$r_{0} = ({\sin{\theta} \over 1+\sin{\theta}})$$ and we also know that: $$r_{n+1} = r_{n} ({1-\sin{\theta} \over 1+\sin{\theta}})$$ that means that: $$r_{1} = ({\sin{\theta} \over 1+\sin{\theta}}) ({1-\sin{\theta} \over 1+\sin{\theta}}) = {(\sin{\theta})({1-\sin{\theta})} \over (1+\sin{\theta})^2}$$ and: $$r_{2} = {(\sin{\theta})({1-\sin{\theta})^2} \over (1+\sin{\theta})^3}$$ so generally we can say that: $$r_{n} = {(\sin{\theta})({1-\sin{\theta})^n} \over (1+\sin{\theta})^{n+1}}$$ the area of a single circle in a sector is: $$A_{n} = \pi r_{n}^2 = \pi ({(\sin{\theta})({1-\sin{\theta})^n} \over (1+\sin{\theta})^{n+1}})^2$$ so the area of a single sector of circles will be: $$A_{s} = \pi \sum_{n=0}^{\infty} ({(\sin{\theta})({1-\sin{\theta})^n} \over (1+\sin{\theta})^{n+1}})^2 $$ and since we're dealing with two or more sectors, we can say that: $$ 0 < \theta \leq {\pi \over 2} $$ so this means the sum of areas of all the circles inscribed in a sector converges to: $$A_{s} = \pi \sum_{n=0}^{\infty} ({(\sin{\theta})({1-\sin{\theta})^n} \over (1+\sin{\theta})^{n+1}})^2 = {\pi \sin{\theta} \over 4}$$ I then continued with: What is the total area of all the circles in all the sectors? Assuming we have $k$ sectors, then $\theta = {\pi \over k}$ and our sum would be: $$A_{k} = {k A_{s}} = {\pi k \sin{\pi \over k} \over 4}$$ and if we take the limit of $k$ , we will get: $$\lim_{k \to \infty} {\pi k \sin{\pi \over k} \over 4} = {\pi^2 \over 4} \tag*{$\blacksquare$}$$ Is this valid?","['geometry', 'real-analysis', 'calculus', 'power-series', 'limits']"
3930407,To find maximum value of a function,"I am new to mathjax so couldn't format so I attached images regarding the problem this is the question . Find the maximum value of $$\left| \sqrt{x^4-3 x^2-6 x+13}-\sqrt{x^4+5 x^2+4}\right|$$ And this is my approach .In my approach I found that the equation is of the form $|PA-PB|$ which will be maximum when $P,A,B$ are collinear and $P$ must divide $A,B$ externally
But in this particular problem the points can't be collinear.I tried using derivative but that is big mess .so how to proceed further or any other methods?","['calculus', 'functions']"
3930460,Example of a curve which is C1 continuous but not G1 continuous.,"I'm reading about parametric and geometric continuity using these UC Berkeley Lecture notes. In the section ""Graph of the curve"", this is written: While the conditions for parametric continuity seem stronger than
geometric continuity, they are not. There are C1 curves that are not
G1. Can anyone give an example for such a curve? Found a similar question here but the example given is of a curve which is G1 continuous but not C1 continuous. These Cornell lectures seemingly give an example of such a curve but it is not very clear. (Page 6 of the pdf)","['continuity', 'geometry', 'curves']"
3930489,minimal description of polyhedron,"I have to find minimal description of a polyhedron $P$ described as follows: \begin{align*}x_1 -  x_2 &\leq 0\\-x_1 + x_2 &\leq 1\\2x_2 & \leq 5 \\ 4x_1 - x_2 &\leq 8 \\ x_1 + x_2 &\geq 1\\x_1 &\geq 0, x_2 \geq 0.\end{align*} So far I have managed to get: \begin{align*}x_1 -  x_2 &\leq 0\\-x_1 + x_2 &\leq 1\\2x_2 & \leq 5 .\end{align*} Can anyone tell me if that is correct?","['polyhedra', 'discrete-mathematics', 'linear-programming']"
3930532,Use of definition of limit to prove $\lim_{x→2} \dfrac{x}{x^2-2}=1$,"I know that by definition I have to prove that $$\lim_{x→2}\dfrac{x}{x^2−2}=1⟺∀ϵ>0,∃δ>0,\, 0<|x−2|<δ⟹\left|\frac{x}{x^2−2}−1\right|<ϵ.$$ I have: $\left|\dfrac{-x^2+x+2}{x^2-2}\right|= \left|\dfrac{-(x^2-x-2)}{x^2-2}\right|= \left|\dfrac{-(x^2-x+1-3)}{x^2-2}\right|=$ $\left|\dfrac{-(x-1)^2+3}{x^2-2}\right|$ . So, I don't know how to continue.","['limits', 'calculus', 'epsilon-delta', 'real-analysis']"
3930574,Singular solution for Lagrange differential equation,"I am familiar with the following theorem: Let $F(x,y,y')$ be defined, continuous, and with continuous first-order derivative with respect to $y$ and $y~$ . Then the singular solution of the ODE $F(x,y,y')=0$ satisfies the following: $F(x,y,y')=0$ and $F'_{y'}(x,y,y')=0$ . How can I use this theorem to find the form of the singular solution for Lagrange Differential Equation $y(x)=xf(y′)+g(y′)$ ? Thanks in advance.",['ordinary-differential-equations']
3930668,Converse of Dirichlet test: a result about existence,"I am wondering whether the following converse to the Dirichlet's test of convergence is true. Let $\lambda_n$ be a sequence of complex numbers such that $\sum_{k=1}^n \lambda_k$ is unbounded as $n \to \infty$ . Then there exists a sequence $x_n \to 0$ such that $\sum \lambda_n x_n$ diverges. There is a delicate balance between $\lambda$ and $x$ , which makes it really hard to prove the above result or find a counterexample. Is the statement above true? In fact, if a counterexample exists, then there might be some possibilities of strengthening the Dirichlet test to a stronger result.","['convergence-divergence', 'sequences-and-series', 'analysis', 'real-analysis']"
3930715,"If $f_{n}$ has a dense image, then $\bigcap (f_{1}\circ\cdots\circ f_{n})(X_{n})$ is dense","Let $\{(X_{n}, d_{n})\}_{n\in\mathbb{N}}$ be a sequence of complete metric spaces and $\{f_{n}: X_{n}\to X_{n − 1}\}_{n\in \mathbb{N}}$ a sequence of continuous functions. If $f_{n}$ has a dense image for each $n\in \mathbb{N}$ , show that $\bigcap_{n\in \mathbb{N}}(f_{1}\circ f_{2}\circ \cdots \circ f_{n}) (X_{n})$ is dense at $X_{0}$ . My attemps:
if $f_{n}$ has a dense image then $\overline{f_{n}(X_{n})}=X_{n-1}$ I want to show that $$\overline{\bigcap_{n\in \mathbb{N}}(f_{1} \circ f_{2} \circ \cdots \circ f_{n})(X_{n})} = X_{0}. $$ Let $F_{n}=(f_{1}\circ f_{2} \circ \dots ◦ f_{n})(X_{n})$ . By induction, I see that $F_{n+1} ⊆ F_{n} ⊆ X_{0}$ . Indeed, if $n=1$ , then $$F_{2}=(f_{1}\circ f_{2})(X_{2})=f_{1}(f_{2}(X_{2}))\subseteq f_{1}(\overline{f_{2}(X_{2}}))=f_{1}(X_{1})=F_{1}$$ if we assume that $F_{k+1}\subseteq F_{k}\subset X_{0}$ . Then, $$F_{k+2} = (f_{1} \circ f_{2} \circ \cdots \circ f_{k+1} \circ f_{k+2})(X_{k+2}) = (f_{1} \circ f_{2} \circ \cdots \circ f_{k+1})(f_{k+2}(X_{k+2}))\subseteq (f_{1} \circ f_{2} \circ \cdots \circ f_{k+1})(\overline{f_{k+2}(X_{k+2})}) = (f_{1} \circ f_{2} \circ \cdots \circ f_{k+1})(X_{k+1})= F_{k+1}\subseteq F_{k}\subseteq X_{0}$$ from the above we have $$\cdots\subseteq F_{n+1}\subseteq F_{n} \subseteq \cdots \subset F_{2}\subseteq F_{1}$$ then $$\cdots\subseteq \overline{F_{n+1}}\subseteq \overline{F_{n}} \subseteq \cdots \subset \overline{F_{2}}\subseteq \overline{F_{1}}=X_{0}$$ then $$\overline{\bigcap_{n\in \mathbb{N}}F_{n} }\subseteq \bigcap_{n\in \mathbb{N}}\overline{F_{n}}\subseteq X_{0} .$$ is my reasoning correct?.I've been thinking that Biare's theorem could be used, but I don't know if it's possible since $F_{n}'s$ don't necessarily open. If it is, how could the other containment prove?. I appreciate any help","['general-topology', 'the-baire-space']"
3930741,"$X$, $Y$ be metric spaces and $f: X \to Y$. If $X$ is Baire and $Y$ is separable then $f$ is continuous in a dense $G_{\delta}$ of $X$","Let $X$ , $Y$ be metric spaces and $f: X \to Y$ . Suppose $X$ is Baire and $Y$ is separable. If $f ^{− 1} (O)$ is $F_{\sigma}$ for every open $O \subset Y$ , show that $f$ is continuous in a dense $G_{\delta}$ of $X$ In the Dissertationes Matematicae (Rozprawy Matematyczne) book they prove that if $X$ is a Baire space, $Y$ is a space that has a countable base and $f: X \to Y$ , then there exists a dense subset $D$ such that $f |_{D}$ is continuous (Theorem 3.6). Also, in How to show that the set of points of continuity is a $G_{\delta}$ they show that the set of continuity points of a function is $G_{\delta}$ . Therefore $D$ would be $G_\delta$ . But I don't know how to use the hypothesis that $f ^{− 1} (O)$ is $F_{\sigma}$ for every open $O \subset Y$ . Any idea please?","['general-topology', 'the-baire-space']"
3930851,Number theory proof mod p,"For a given prime $p$ and integer $k$ , $k∈<1, p-1>$ $S(k)$ is defined as a sum of remainders of $k, k^2, k^3, ..., k^{p-1}$ when divided by $p$ . Prove that $S(k)=\frac{1}{2}p(p-1)$ is true for an odd number of different k. I managed to prove that $S(p-1)=\frac{1}{2}p(p-1)$ but can't see how to do it further.","['number-theory', 'divisibility']"
3930852,When exactly is the character space of a Banach algebra empty?,"It is well-known that the character space, (i.e. the set of multiplicative characters) of a commutative, unital Banach algebra is non-empty. But is there a complete characterization of when exactly the character space of Banach algebra is empty? Or even an incomplete characterization (ie certain classes of Banach algebras which we know have empty character space)? Edit: I’d appreciate any responses, especially those with links for further reading!","['operator-algebras', 'banach-algebras', 'representation-theory', 'gelfand-representation', 'functional-analysis']"
3930914,show by induction that $4^{2n+1}+3^{n+2}$ is divisible by $13$,"i'm stuck in this problem, anyone can give me a hint, please? ""Show by induction that $4^{2n+1} + 3^{n+2}$ is divisible by $13$ In my attempt i got the expression $$4^{2}\cdot 4^{2n+1} + 3\cdot 3^{n+2}$$ but i can't go more, any suggestion is thankful.","['induction', 'natural-numbers', 'discrete-mathematics']"
3931082,$\bigcap_{n\in\mathbb{N}}{F_{n}}$ is dense in $X_{0}$,"Let $\{(X_{n},d_{n})\}_{n\in\mathbb{N}}$ be a sequence of complete metric spaces. If $\{f_{n}\colon X_{n}\to X_{n-1}\}_{n\in\mathbb{N}}$ is a sequence of functions continuous such that \begin{equation}
\overline{f_{n}(X_{n})}=X_{n-1},\,\forall\,n\in\mathbb{N}
\end{equation} show that $\overline{\bigcap_{n\in\mathbb{N}}{(f_{1}\circ\cdots\circ f_{n})(X_{n})}}=X_{0}.$ My attempt: Let $F_{n}=(f_{1}\circ\cdots\circ f_{n})(X_{n})$ be. Let's see what \begin{equation*}
    \overline{\textstyle\bigcap_{n\in\mathbb{N}}{F_{n}}}=X_{0}.\tag{*}
\end{equation*} Note that \begin{align*}
    F_{n+1}&=(f_{1}\circ\cdots\circ f_{n}\circ f_{n+1})(X_{n+1})\\
    &=(f_{1}\circ\cdots\circ f_{n})( f_{n+1}(X_{n+1}))\\
    &\subseteq{(f_{1}\circ\cdots\circ f_{n})( \overline{f_{n+1}(X_{n+1})})}\\
    &=(f_{1}\circ\cdots\circ f_{n})( X_{n})\\
    &=F_{n}.
\end{align*} \begin{align*}
    F_{n}&=(f_{1}\circ\cdots\circ f_{n})( X_{n})\\
    &=(f_{1}\circ\cdots\circ f_{n})( \overline{f_{n+1}(X_{n+1})})\\
    &\subseteq{\overline{(f_{1}\circ\cdots\circ f_{n})(f_{n+1}(X_{n+1}))}}\\
    &=\overline{(f_{1}\circ\cdots\circ f_{n+1})( X_{n+1})}\\
    &=\overline{F_{n+1}}.
\end{align*} Therefore, we have \begin{equation}
    F_{n+1}\subseteq{F_{n}}\subseteq{X_{0}}\subseteq{\overline{F_{n}}=\overline{F_{n+1}}},\,\forall\, n\in\mathbb{N}.\tag{1}
\end{equation} By $ (1), $ it follows that \begin{equation*}
    \overline{\textstyle\bigcap_{n\in\mathbb{N}}{F_{n}}}\subseteq{X_{0}}.
\end{equation*} Let $x\in X_{0}.$ By $(1),$ exists a sequence $(F_{n}(x_{n}))_{n\in\mathbb{N}}\subseteq{X_{0}}$ such that $d_{0}(x,F_{n}(x_{n}))<1/n.$ Now a question arises: is the sequence $ (F_{n} (x_{n})) _{n \in \mathbb {N}}$ a Cauchy sequence at $X_{0} $ ? If that's true, then it must converge because $ X_{0} $ is a complete metric space. Let's say $ F_{n} (X_{n}) \to y \in X_{0}. $ Like $ d_{0} (x, F_{n} (x_{n})) <1 / n, $ then $ x = y$ and it follows that $ x \in \overline {\bigcap_ {n \in \mathbb {N}} {F_ {n}}}. $ I have doubts with the previous argument. I don't know if that's enough to justify that $ X_{0} \subseteq {\overline{\textstyle\bigcap_{n\in\mathbb{N}}{F_{n}}}}. $ Another question: If for every $ n \in \mathbb {N}, $ exists a open set $ U_{n} \subseteq {X_{n}} $ such that $ \overline {f_{n} ({U_{n}})} = X_{n-1}, $ then is the previous result still valid? Hope you can help me with some suggestions please.","['general-topology', 'the-baire-space', 'analysis', 'real-analysis']"
3931094,"True or false: If the function $f + g$ is continuous and $g$ is continuous, then $f$ is also continuous","This question comes from Advanced Calculus by Fitzpatrick, Chapter 3, Section 3.1 Exercise 1c. I wrote a similar question today and so I think I'm getting the hang of this. My question is: is my proof/justification correct? If not, why? If the functions $ f + g : \mathbb{R} \rightarrow \mathbb{R}$ and $g : \mathbb{R} \rightarrow \mathbb{R}$ are continuous, then function $f : \mathbb{R} \rightarrow \mathbb{R}$ is also continuous My attempt Since $g$ is continuous, then so is $-g$ . Note that $(f + g) + (-g) = f$ . Since $f$ is the sum of continuous functions, $f$ is continuous. Therefore this statement is true.","['solution-verification', 'real-analysis']"
3931100,Closed form of $\sum_{k=1}^{n}k!$,"I was trying to get a closed form of the sum $$\sum_{k=1}^{n}k!$$ Mathematica gives the answer $$(-1)^n\Gamma(n+2)(!(-n-2))-!(-1)-1$$ Here the ! sign before a number is the subfactorial . Here is my try: \begin{align}
\sum_{k=1}^{n}k!&=\sum_{k=1}^{n}\Gamma(k+1)\\
&=\sum_{k=1}^{n}\int_{0}^{\infty}x^{k}e^{-x}dx\\
&=\int_{0}^{\infty}e^{-x}\sum_{k=1}^{n}x^kdx\\
&=\int_{0}^{\infty}\frac{e^{-x}(x-x^{n+1})}{1-x}dx
\end{align} But I don't know what to do further. So, how can that closed form given by mathematica be derived? Also, that closed form expression looks complex for some numbers to me. Is it even right?","['summation', 'factorial', 'closed-form', 'sequences-and-series']"
3931101,Show that $H(U)$ the space of holomorphic functions on $U$ is not normable.,"I'm trying to show the following result: Let $U \subset \mathbb{C}$ be an open set. Show that the vector space $H(U)=\{f:U \to \mathbb{C} \; \text{analytic}\}$ is not normable. I know that $H(U)$ is a Frechet space. If we take $A_{k}:=\{z \in U\ : |z| \leq k \; , \; \text{dist}(\partial U,z) \geq \frac{1}{k} \}$ and $p_{k}(f)=\sup_{z \in A_{k}}|f(z)|$ for each $k \in \mathbb{N}$ , we have $\cup_{k}A_{k}=U$ and metric $$d(f,g)=\sum_{k=1}^{\infty}\frac{1}{2^{k}} \frac{p_{k}(f-g)}{1+p_{k}(f-g)}.$$ I'm not sure how to work with this. Any help would be greatly appreciated.","['normed-spaces', 'functional-analysis', 'analysis', 'real-analysis']"
3931135,How are (pre)sheaves even well-defined on sites?,"I am trying to learn a bit about étale cohomology from Milne’s book and I have run into some early problems. I learned ordinary abelian sheaf cohomology from Hartshorne. There, sheaves are defined on the category of open subsets of a scheme $X$ in which the morphisms are just the open inclusions. A presheaf on $X$ is then just a contravariant functor on this category. To motivate the étale site and étale cohomology, many sources first introduce sites via the Zariski open immersions, but I am already confused by this. The Zariski site on $X$ is defined to have objects $f \colon U \rightarrow X$ which are open immersions and morphisms given by maps $V \rightarrow U$ which commute with the structure morphisms of $U$ and $V$ . Then a presheaf $\mathscr{I}$ is a contravariant functor on this category. With this definition, many sources, including Milne, call the group $\mathscr{I}(U)$ the sections over $U$ . But already I don’t know what this means. In the case of Hartshorne this is fine. But in the case of the Zariski site, there could be many distinct open immersions from $U$ to $X$ . So what then does $\mathscr{I}(U)$ actually refer to? Is there any reason to believe that $\mathscr{I}(f \colon U \rightarrow X)$ is equal or even isomorphic to $\mathscr{I}(g \colon U \rightarrow X)$ if $f$ and $g$ are distinct open immersions? I suspect in the sheaf case one can use the sheaf axioms to argue that they are isomorphic, but what about in the presheaf case? The problem seems to be even worse when it comes to the étale site. In the case of the Zariski site, at least open immersions factor into an isomorphism and an open inclusion. But two étale morphisms $f \colon U \rightarrow X$ and $g \colon U \rightarrow X$ need not be related in any way as far as I can see. So what does $\mathscr{I}(U)$ mean, and is there any reason to think it is independent of the structure morphism? I realise this may be a completely silly question, but I can’t see an obvious answer after a while thinking about it.","['etale-cohomology', 'sheaf-cohomology', 'category-theory', 'algebraic-geometry', 'sheaf-theory']"
3931140,Decorating And Cutting A Cake Fairly,"Imagine an N x N square cake you are tasked with decorating with N types of toppings (colors). You then with to share the cake equally amongst N friends. All pieces must be rectangular and have same the area. It's also only fair that no matter what kind of piece your friends have, they each have the exact same type of toppings. How should you decorate the cake so that no matter how you cut it, it's always a fair division? For example below, the 4 x 4 case of 4 different colors for 4 friends: Take any equal area rectangular cut and each will contain all four different colors: It's not hard to see that any cut that's equal is a fair division by the definition above. Call any N x N coloring of this kind a fair division coloring . Below are fair division colorings for 2 x 2, 3 x 3, and 5 x 5, 7 x7, and 9 x 9: For N=p, p a prime, the coloring is trivial. However, I found it is impossible for N=6 and N=8. I used brute force methods and found that any coloring of 6 different colors for the 6 x 6 grid, there will exist a cut that doesn't create a ""fair"" division. The same is true for N=8 . My question: is there a general rule to determine for what N a fair division coloring of the square exists? EDIT: I am absolutely confident this kind of problem has been studied before, but I'm not sure where to find a paper on this. I appreciate any and all thoughts on where I can look.","['graph-theory', 'combinatorics', 'coloring', 'permutations']"
3931146,The Sum of Geometric-Factorial Series,"I was pondering over my Sequences & Series homework, when the following series struck my mind: $0!+1!x+2!x^{2}+3!x^{3}+.......+n!x^{n}$ And believe me, I am serious that I couldn't think of any approach, but it looked beautiful.I tried to look up all possible summation methods, but it seemed like they didn't offer a break to me.  So guys, any ideas or approach?","['summation', 'sequences-and-series']"
3931220,Finding the number of solutions for $e^{\tan x}=\sin x+ \cos x$ graphically,"The number of solutions of the equation $$e^{\tan x}=\sin x+ \cos x$$ In the interval $[-\pi,\pi]$ I tried to approach this question graphically but hit a dead end when trying to graph $e^{tanx}$ For graphing sinx+cosx I graphed $\sqrt 2(sin(x+\frac{\pi}{4}))$ I still feel this question can be solved graphically,Is there a method to graph $e^{tanx}$ to some accuracy ?","['trigonometry', 'graphing-functions']"
3931289,"Kolmogorov ""two series""-theorem","I have a question regarding the following theorem of the ""two-series"" : Theorem : Let $(X_n)_{n \geq 1}$ be a sequence of independant and uniformely limited random variable. Then $\sum\limits_{n=1}^{+\infty} X_n$ converges $\iff$ $\sum\limits_{n=1}^{+\infty} E[X_n], \hspace{0.1cm}\sum\limits_{n=1}^{+\infty} \text{Var}[X_n]$ converge. To imply that if $\sum\limits_{n=1}^{+\infty} X_n$ converge a.e then converge $\sum\limits_{n=1}^{+\infty} E[X_n]$ and $\sum\limits_{n=1}^{+\infty} \text{Var}[X_n]$ , we costruct $(Y_n)_{n=1}^{+\infty}$ independant with same law and we afirm (this is the part I don't understand in which sense I'm supposed to think about it) that $\sum\limits_{n=1}^{+\infty}Y_n $ converges a.e, because the convergence is a property dependant only on the law of the sequence. Details on this fact or clarification would be appreciated.","['almost-everywhere', 'convergence-divergence', 'probability-theory', 'probability', 'random-variables']"
3931324,A proof on the existence of an injective function,"In La matematica della verità (p.65-66, theorem 3.5.3 ), Ettore Casari proves the following theorem: $f : A \to B$ is injective iff  for all $g : C \to A, h : C \to A$ and all $C$ : $$f \circ g \equiv f \circ h \to g \equiv h$$ His proof for the right-to-left hand is unclear to me. He writes: Consider the set C and the functions $g$ and $h$ defined as follows: C: = { z } (where $z$ is some individual), $g,h : C \to A$ , $g(z) := x$ and $h(z) := y$ . It follows that $f(g(z)) = f(h(z))$ , i.e that $(f \circ g)\,z = (f \circ h)\,z$ , and since $z$ is the only argument of $f \circ g$ and $f \circ h$ , $f \circ g \equiv f \circ h$ and therefore, by hypothesis, $g \equiv h$ , so that $g(z) = h(z)$ and $x = y$ . I don't understand how he argues on the basis of a very specific set $C$ and a very specific stipulation (that $g(z) := x$ and $h(z) := y$ ) to the claim that for any $C$ , and any functions $g,h : C \to A$ , it follows that $f$ is injective. How does this proof generalise to other cases where $C$ is not the set as defined above, and $g$ and $h$ are not as defined)?",['elementary-set-theory']
3931391,A proof of a criterion of vanishing of a function.,"Let $f\in C^{\infty}[-1,1]$ , and there exists a constant $M>0$ s.t $$|f^{(j)}(x)|\le M \forall j \in \mathbb{Z}\forall x\in [-1,1]$$ Prove that if $f(1/k)=0$ for each $k\in \mathbb{N}$ then $f=0$ . I am thinking of using a Taylor expansion, I need somehow to show that all the coefficients vanish. So if I take $f(1/k)=0$ I take $k\to \infty$ then $f(0)=0$ but how to do the rest of the coefficients get to vanish? I don't see it.",['calculus']
3931392,Why can I not make this function equal zero and solve for x when finding y-intercept of a rational fraction w/ numerator (2x^2+9),"When graphing a rational fraction, $\frac{2x^2+9}{x}$ , and I have to find the $y$ -intercept, why can't I do this? Prof. Leonard from youtube says that if discriminant is negative, then there is no real answer, but why do I get a value for $x$ when I do this? $$2x^2+9 = 0 \to 2x^2= -9 \to x^2 = \frac{9}{2} \to x = \pm\frac{3}{\sqrt{2}}$$ $$x = \frac{-3}{\sqrt2}, x=\frac{3}{\sqrt2}$$ What am I doing wrong?",['algebra-precalculus']
3931433,"For real Lie algebras, is any invariant bilinear form a scalar multiple of the Killing form?","I know that for a complex Lie algebra, Schur's lemma can be used to show that any invariant bilinear form on a simple Lie algebra is a scalar multiple of the killing form, but Schur's lemma does not hold for $\mathbb{R}$ , so I guess this isn't true for real simple Lie algebras. Does someone know a example? I can't seem to find one.","['abstract-algebra', 'linear-algebra', 'lie-algebras', 'lie-groups']"
3931441,"A problem about convergence in distribution to $N(0,1)$.","Let $(Y_{n})_{n\geq 1}$ be a sequence of  independents random variables such that $$\mathbb{P}[Y_{n}=1]=\mathbb{P}[Y_{n}=-1]=\frac{1}{2n}, \quad \mathbb{P}[Y_{n}=0]=1-\frac{1}{n}, \quad n \geq 1$$ Let $S_{n}=\sum_{i=1}^{n}Y_{i}$ and prove that $$\frac{S_{n}}{n}\overset{a.s}{\to}0 \quad \text{and} \quad \frac{S_{n}}{\sqrt{\log(n)}}\overset{D}{\to}N(0,1)$$ My approach: For the first part, it's to say prove that $\frac{S_{n}}{n} \overset{a.s}{\to} 0$ I think that since that for $n\geq 1$ , we know that \begin{eqnarray*}
Y_{n}=1 \implies \mathbb{P}[Y_{n}=1]=\frac{1}{2n}\\
Y_{n}=-1 \implies \mathbb{P}[Y_{n}=-1]=\frac{1}{2n}\\
Y_{n}=0 \implies \mathbb{P}[Y_{n}=0]=1-\frac{1}{n}\\
\end{eqnarray*} other values is not possible. So, the support for $Y_{n}$ is $S_{Y_{n}}={-1,0,1}$ . So, by definition we have $$\mathbb{E}Y_{n}=\sum_{y_{n}\in S_{Y_{n}}}y_{n}\mathbb{P}[Y_{n}=y]=\sum_{y_{n}\in \{-1,0,1\}}y_{n}\mathbb{P}[Y_{n}=y]=0$$ similarly, we have $$\mathbb{V}Y_{n}=\mathbb{E}Y_{n}^{2}-(\mathbb{E}Y_{n})^{2}=\frac{1}{n}$$ and since that $\displaystyle \sum_{n=1}^{\infty}\frac{\mathbb{V}Y_{n}}{n^{2}}<\infty$ , so by SLLN-Kolmogorov with have $$\frac{S_{n}-\mathbb{E}S_{n}}{n}=\frac{S_{n}}{n} \overset{a.s}{\to} 0$$ For the second part, to prove that $\frac{S_{n}}{\sqrt{\log(n)}}\overset{D}{\to} N(0,1)$ but don't have the hypothesis identical distribution, so I can't use the central limit theorem. How can I solve this part? Note: I know that if $Y_{1},Y_{2},\ldots$ are independent random variables and let $\displaystyle S_{n}=\sum_{i=1}^{n}Y_{n}$ . For each $i$ let $\mu_{i}=\mathbb{E}Y_{i}$ and $\sigma^{2}_{i}=\mathbb{V}Y_{i}$ and let $m_{n}=\sum_{i=1}^{\infty}\mu_{i}$ and $s_{n}^{2}=\sum_{i=1}^{n}\sigma_{i}^{2}$ mean and variance respectively. Suppose that a) $s_{n}^{2}\to \infty \quad \text{as} \quad n \to \infty$ and b) there exists a constant $M$ sucha that $\mathbb{P}[|X_{i}|\leq M]=1, \forall i$ . So we have $$\frac{S_{n}-m_{n}}{s_{n}}\overset{D}{\to}N(0,1)$$","['statistics', 'probability-distributions', 'probability-theory', 'weak-convergence']"
3931465,Bound the absolute value of the partial sums of $\sum \frac{\sin(nx)}{n}$,"I would like to prove that for every $x \in \mathbb{R}, n$ natural, we have $$\!\left|\sin(x)+\frac{\sin(2x)}{2}+\ldots+\frac{\sin{(nx)}}{n}\right| \le \int_{0}^{\pi}\frac{\sin x}{x}\,dx$$ We can of course restrict our attention for $x \in [0, 2\pi]$ .
So far I tried a few things: letting $S_n(x)$ denote the quantity inside the absolute value, we have $$S_n'(x) = \sum_{k=1}^{n}\cos(kx) = -\frac1{2}+\frac{\sin(n+\frac12)x}{2\sin\frac{x}{2}}$$ and hence $S_n(x)$ has critical points at $x = \frac{2k\pi}{n}$ or $x = \frac{(2k+1)\pi}{n+1}$ where $k$ is an integer. Moreover: $$S_n(x) = -\frac{x}{2}+\int_{0}^{x}\frac{\sin(n+\frac12)t}{2\sin\frac{t}{2}}\,dt$$ I also know that the function $F(x) = \int_{0}^{x}\frac{\sin(t)}{t}\,dt$ has an absolute maximum for $x = \pi$ . I struggle to bound the integral in the expression for $S_n$ . Can you help me? Or maybe there's a different approach to solve this?","['integration', 'sequences-and-series', 'real-analysis']"
