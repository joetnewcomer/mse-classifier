question_id,title,body,tags
3401120,Show that a reflection matrix is given by $\begin{bmatrix}\cos2\theta&\sin2\theta \\ \sin2\theta&-\cos2\theta\end{bmatrix}$,"Reflection matrix: $$ \text{Reflection}(\theta) = 
\begin{bmatrix}
\cos2\theta & \sin2\theta \\
\sin2\theta & -\cos2\theta
\end{bmatrix}$$ Attempt: Inspiration: Speaking non-rigorously, it seems like the angle between the reflected vector and the original vector will be $2\theta$ . Armed with this, let's consider how $e_1 = \begin{bmatrix}1\\0\end{bmatrix}$ and $e_2 = \begin{bmatrix}0\\1\end{bmatrix}$ change when we reflect them across an arbitrary line. Let $$
\text{Reflection}(\theta) = 
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}$$ Then, $$\begin{align}
\text{Reflection}(\theta) \cdot e_1 &= 
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix} \cdot \begin{bmatrix}1\\0\end{bmatrix} \\
&= \begin{bmatrix}a\\c\end{bmatrix}
\end{align}$$ Using my assumption that reflected vectors have angle of $2\theta$ between itself and the original vector, $(\text{Reflection}(\theta)\cdot e_1) \cdot e_1 =  \begin{bmatrix}a\\c\end{bmatrix}\cdot \begin{bmatrix}1\\0\end{bmatrix} = a = ||\text{Reflection}(\theta)\cdot e_1||\cdot||e_1|| \cos(2\theta)$ (dot product). Simplifying the other side of the equation, we get: $$a = 1\cdot 1 \cos(2\theta) = \cos(2\theta)$$ Doing it similarly for $e_2$ yields $d=\cos(2\theta)$ which means our reflection matrix currently looks like this: $$ \text{Reflection}(\theta) = 
\begin{bmatrix}
\cos2\theta & b \\
c & \cos2\theta
\end{bmatrix}$$ which is not correct. Questions: I suspect my implicit assumption that a reflection can be represented as a rotation of $2\theta$ , where $\theta$ is the angle between the original vector and reflection line, is where I went wrong. Why is this wrong? $$ \text{Rotation}(2\theta) = 
\begin{bmatrix}
\cos2\theta & -\sin2\theta \\
\sin2\theta & \cos2\theta
\end{bmatrix} \text{ for reference. }$$ What's the correct way to do this? I would normally ask one question per SE question but I think my two questions are tightly coupled.","['matrices', 'proof-writing', 'proof-verification', 'linear-algebra']"
3401176,Number of solutions to the Diophantine equation $xy-6(x+y)= 0$ such that $x\leq y$,"The question is to find number of integer solutions to the equation $$xy-6(x+y)= 0$$ Given that, $$x \leq y$$ So I proceeded as follows, From the 1st equation I get $$x=\frac{6y}{y-6}$$ Putting it into the 2nd I got $$\frac{6y}{y-6} \leq y$$ $$\implies \frac{y(y-12)}{y-6} \geq0$$ So, $y \in [0,6)\cup[12,\infty)$ How do I proceed from here? As there seem to be infinity possible values of y which may satisfy the inequality...","['elementary-number-theory', 'algebra-precalculus', 'diophantine-equations']"
3401184,"Number of solutions so that atleast two of x,y,z are integers","I have a set of equations \begin{cases}x+2y+4z=9\\
4yz+2xz+xy=13\\
xyz=3
\end{cases} I need to find number of solutions $(x,y,z)$ such that at least two of $x,y,z$ are integers. I thought about trying to use the multinomial theorem to get number of non negative integral solutions to Equation $1$ , but that would be for all $x,y,z$ to be integers, how do I take into account that only two of them need to be an integer? Also, how do I use the other two equations. AM-GM probably wont help too, as that is for +ve integers. I'm just stuck, don't even know how to begin solving this.","['elementary-number-theory', 'algebra-precalculus']"
3401191,Union of a Set and an Interval,"How do you find the union of a set and an interval? Consider the following example: {1, 3, 4} ∪ (4, ∞) If I understand the concept correctly, the resulting set would contain the numbers 1 , 3 , 4 , as well as all the numbers from 4 to positive infinity. In case my understanding of the concept is correct, then what notation do I use to express the answer? Thank you.",['elementary-set-theory']
3401284,"In triangulated category, why do we use triangles, but not exact sequences?","Studying the theory of derived category, I came up with a little question. To define trianglated categories, first we choose a collection of sextuples, called the distinguished triangles. I know that, for the homotopy category $K(\mathscr{A})$ of an abelian category $\mathscr{A}$ , its distinguished triangles are almost the same to the short exact sequences.
(see this post .) And I think that in order to do the homological argument it's easy to use exact sequences rather than distinguished triangles. So why do we use distinguished triangles in the definition of triangulated categories? Can't we define the triangulated categories using the word ""choose a collection of maps $X \to Y \to Z$ , and call it an exact sequence""? (E.g., for the homotopy category of an abelian category, we choose the (classical) exact sequences as the ""exact sequences"" of the triangulated category.) Thank you very much!","['homological-algebra', 'derived-categories', 'category-theory', 'algebraic-geometry', 'soft-question']"
3401294,Is the set where there is a unique distance minimizer to a submanifold open?,"Let $M$ be a smooth Riemannian manifold, and let $S \subseteq M$ be a compact submanifold. Define $U=\{  p \in M \, | \, \text{there exist a unique closest point to } p \text{ in } S\}$ . Is $U$ open in $M$ ? Does the answer change if we assume that $M$ is complete or geodesically convex?","['submanifold', 'riemannian-geometry', 'metric-spaces', 'differential-topology', 'differential-geometry']"
3401375,"Evaluate $\lim\limits_{n \to \infty}\sum\limits_{k=0}^n \dfrac{\sqrt{n}}{n+k^2}(n=1,2,\cdots)$","I tried to change it into a Riemann sum but failed, since \begin{align*}
\lim_{n \to \infty}\sum_{k=0}^n \frac{\sqrt{n}}{n+k^2}=\lim_{n \to \infty}\frac{1}{n}\sum_{k=0}^n \frac{\sqrt{n}}{1+(k/\sqrt{n})^2}
,\end{align*} which is not a standard form. Maybe, it need apply the squeeze theorem, but how to evaluate the bound. By the way, WA gives its result \begin{align*}
\lim_{n \to \infty}\sum_{k=0}^n \frac{\sqrt{n}}{n+k^2}=\frac{\pi}{2}.
\end{align*}","['integration', 'limits', 'calculus']"
3401431,Rational with minimal denominator between two rationals [duplicate],"This question already has answers here : Minimal $ab$ for Rational Number $a/b$ in an Interval (3 answers) Closed 4 years ago . My question from an easy problem. $p,q$ are positive integers such that $$
\frac{5}{9}<\frac{p}{q}<\frac{4}{7}
$$ find $p,q$ such that $q$ is the smallest number that satisfies this inequality. Draw the line of $ y<\frac{9}{5}x$ and $y>\frac{7}{4}x$ , we can ""observe"" that $\frac{9}{16}$ is such number. However, if the question becomes $a,b,c,d$ are positive integers such that $$\frac{a}{c}<\frac{b}{d}
$$ find $p$ , $q$ such that $q$ is the smallest number that satisfies the inequality $$\frac{a}{c}<\frac{p}{q}<\frac{b}{d}$$ No idea about this.","['number-theory', 'elementary-number-theory', 'rational-numbers']"
3401483,Number of integral solutions of polynomial,"Consider the equation $f(x) = x^4-ax^3-bx^2-cx-d=0,$ $a, b, c, d \in \mathbb Z^+,$ $ a≥b≥c≥d$ then number of integral solutions can be. I am unable to use Integral Root theorem and just reached to the conclusion that if $\alpha$ is positive root of $f(x)$ then $\alpha >a$ , nothing else. How should it be done?","['functions', 'roots', 'polynomials']"
3401490,Prime numbers as the product of an arithmetic sequence,"Are there infinitely many prime numbers that can be represented as the product of three rational numbers bigger than 0 that create an arithmetic sequence? For example, the product of the arithmetic sequence {1, 1.5, 2} is the prime 3.","['number-theory', 'elliptic-curves', 'prime-numbers', 'sequences-and-series']"
3401509,finding grammar for languages,"Situation: So I have three languages. a) $L_1 =$ { $a^{2n-1}b^{2m} | n,m \geq 1$ } b) $L_2 =$ { $a^nb^ma^nb^m | n,m \geq 1$ } c) $L_3=$ { $a^nb^ma^{n+m} | n \geq 1, m \geq 0$ } So one of this languages is regular, one is context-sensitive, one is context-free. My work: for a) I found a context-free grammar: $S \rightarrow aTbb $ $ T \rightarrow aaTbb| \varepsilon | aaT | Tbb $ edit : found a regular grammar for a). So my problem is only b). For b) I tried so many grammars, but they did not represent the language $L_2$ . My assumption is that this grammar is context-sensitive. c) $ S \rightarrow aSa|B $ $ B \rightarrow bBa| \varepsilon $","['formal-languages', 'context-free-grammar', 'discrete-mathematics', 'computer-science']"
3401564,Integration over parametrizations in the border of a manifold with corners (John M. Lee Introduction to Smooth Manifolds exercise 16.22),"In exercise $16.22$ of Lee's Introduction to Smooth Manifolds , the author wants us to translate the result of proposition $16.8$ , the integration over parametrizations, to the border of a manifold with corners. I think the precise statement is the following one: Let $\partial M$ be the border a compact, oriented smooth $(n+1)-$ manifold with corners $M$ , and let $\omega$ be a compactly supported $n-$ form on $\partial M$ . Suppose $D_1,\dots,D_k$ are open domains of integration in $\mathbb{R}^n$ (this means that each $D_i$ is bounded and its (topological) boundary $\partial D_i$ has measure zero as a subset of $\mathbb{R}^n$ ), and for $i=1,\dots,k$ we are given smooth maps $F_i:\bar D_i\rightarrow\partial M$ satisfying: $(i)\;F_i$ restricts to an orientation-preserving diffeomorphism from $D_i$ onto an open subset $W_i\subseteq \partial M$ ; $(ii)\;W_i\cap W_j=\varnothing$ when $i\not=j$ ; $(iii)\;\text{supp }\omega\subseteq\bar W_1\cup\dots\cup\bar W_k$ . Then $$\int_{\partial M}\omega=\sum_{i=1}^k\int_{D_i} F_i^*\omega$$ However, I don't understand the following points: First $-$ why is $M$ required to be compact, while the statement of proposition $16.8$ doesn't require the manifolds in question to be compact? Second $-$ is my restatement of $16.8$ complete? I feel like something is missing, but I am not sure. After seeing an application of this result in the demonstration of Stokes' Theorem for chains, I can't help thinking that what I have written is good enough Third $-$ is there any point in the proof of $16.8$ where any step has to be completely rewritten? After reading the proof several times, I feel like there is nothing wrong with it when we change the sets and maps to adapt this demonstration to the case we are studying. However, there must be some technical detail I am not seeing at all. Thanks for your interest. PS1 : First point is somewhat obvious, since one can construct spaces where no finite decomposition $D_1,\dots,D_k$ and maps $F_i:\bar D_i\rightarrow \partial M$ exists (for instance, think of $M$ as a ""snake"" made of solid parallelepipeds that extends indefinitely across $\mathbb{R}^3$ ) PS2 : Needless to say, the sets $W_i$ cover the whole space $\partial M$ with the exception of some subset of measure zero in $\partial M$","['integration', 'differential-topology', 'smooth-manifolds', 'differential-geometry']"
3401611,"Determine all $f:\Bbb R\to \Bbb R$ such that $f\big(a-3f(b)\big)=f\left(a+f(b)+b^3\right)+f\left(4f(b)+b^3\right)+1$ for every $a,b\in\Bbb R$.","Im struggling with this functional equation: Determine all $f: \Bbb R \to \Bbb R$ such that $$f\big(a-3f(b)\big)=f\left(a+f(b)+b^3\right)+f\left(4f(b)+b^3\right)+1$$ for all $a,b\in\Bbb R$ . Clearly the constant function $f(x)=-1$ for all $x\in\mathbb{R}$ is a solution.  If $k=f(0)$ , then $k\neq 0$ .  Otherwise, if $a,b=0$ , we have $$f\big(-3f(0)\big)=f\big(f(0)\big)+f\big(4f(0)\big)+1\,,$$ which would give $0=0+0+1$ if $k=0$ .  By taking $b=0$ , we have $$f(a-3k)=f(a+k)+f(4k)+1,$$ or $$f(a+4k)=f(a)+r,$$ where $r=-f(4k)-1$ .  This proves that $$f(a+4kn)=f(a)+nr$$ for all integers $n$ .  What to do next? Thanks in advance for your time.","['contest-math', 'functional-equations', 'functions', 'hamel-basis']"
3401658,Proving $\lim_{x\to 4} \left(\frac{\sqrt {2x-1}}{\sqrt {x-3}}\right) = \sqrt 7$ using $ \varepsilon - \delta$,Prove that $$\lim_{x\to 4} \left(\frac{\sqrt {2x-1}}{\sqrt {x-3}}\right) = \sqrt 7$$ using $\varepsilon - \delta$ . We find $\delta$ such that $0<|x-4| <\delta$ $$\left|\frac{\sqrt {2x-1}}{\sqrt {x-3}}-\sqrt 7\right|= \left|\frac{\sqrt {2x-1}-\sqrt{7x-21}}{\sqrt {x-3}}\right|$$ I know that we should get to $|x-4|$ but i dont know how,"['limits', 'calculus', 'epsilon-delta']"
3401667,Derivative of sign function $\operatorname{sgn}(x)$ (in distribution sense).,"In the book of Schilling and Partzsch : Brownian motion (in the part of the Tanaka formula), they say that the derivative of $f(x)=\text{sgn}(x)$ is given by $f'(x)=\delta _0(x)$ (in distribution sense). But I find $f'(x)=2\delta _0(x)$ and I don't see where is my mistake : so let $\varphi$ a test function. $$\left<f',\varphi \right>=-\int_{\mathbb R}f\varphi '=\int_{-\infty }^0\varphi '-\int_0^\infty \varphi '=\varphi (0)+\varphi (0)=2\varphi (0)=\left<2\delta _0,\varphi \right>.$$ Did they do a mistake ?",['real-analysis']
3401767,Show that $(n^{-1})\max_{m=1}^{n}X_{m}\rightarrow 0$ a.s. if and only if $E(X_{1}^{+})<\infty$ and $P(X_{1}>-\infty)>0$.,"I am working on the question stated as below: Let $(X_{k})$ be i.i.d random variable taking values in $\overline{\mathbb{R}}$ and let $M_{n}:=\max_{k=1}^{n}X_{k}$ . Show that $M_{n}/n\longrightarrow 0$ a.s. if and only if $EX_{1}^{+}<\infty$ and $P(X_{1}>-\infty)>0$ . Similar questions are here: For i.i.d. $\{X_n\}$ proving $\frac1n\max\limits_{1\leqslant k\leqslant n}|X_k|\xrightarrow{\mathrm{a.s.}}0\Leftrightarrow E(|X_1|)<+∞$ and here: Show $n^{-1} \max_{k\leq n} X_k \rightarrow 0 \quad a.s.$ if and only if $\mathbb{E}(X_1)_+<\infty$ and $\mathbb{P}(X_1>-\infty)>0$ , while the first link does not include the condition $P(X_{1}>-\infty)>0$ , and the second post does not prove it completely. I followed the second link and have got some progress: $(\Leftarrow)$ . Define $M_{n}^{+}:=\max_{m=1}^{n}X_{m}^{+}$ , then I claim we have the following: $$E(X_{1}^{+})<\infty\iff X_{n}^{+}/n\longrightarrow 0\ \text{a.s.}\iff M_{n}^{+}/n\longrightarrow 0\ \text{a.s.}$$ Indeed, the first $\iff$ is due to the fact that $X_{n}/n\longrightarrow 0$ a.s. if and only if $E|X_{1}|<\infty$ and $E|X_{1}^{+}|=E(X_{1}^{+}).$ The second is due to an analytic fact that as $n\longrightarrow\infty$ , we have $$a_{n}/n\longrightarrow 0\iff \dfrac{1}{n}\max_{m=1}^{n}|a_{m}|\longrightarrow 0.$$ Now, by hypothesis $P(X_{1}>-\infty)>0$ , then if $P(X_{1}\geq 0)=k$ , for any $0<k\leq 1$ , then $P(M_{n}^{+}=M_{n})=1$ , and thus $M_{n}/n\longrightarrow 0$ a.s. However, I don't know how to show the case where $P(-\infty<X_{1}<0)=1$ . The link said that I can show it directly, but I could not see how. Also, how can I show $(\Rightarrow)$ ? Again it seems that $M_{n}/n\longrightarrow 0$ a.s. happens if and only if $EX_{1}^{+}<\infty$ , but I don't see how $M_{n}/n\longrightarrow 0$ implies $P(X_{1}>-\infty)>0$ . Some detailed answers would be really appreciated :) Edit 1: I noticed something. Most of the time we consider $X_{k}$ takes regular real value, but this question assumes that it can also take $\infty$ and $-\infty$ . I think that's why we need $P(X_{1}>-\infty)>0$ , but this restriction is just saying that $X_{1}\neq-\infty$ a.s., how could I use this condition?","['convergence-divergence', 'probability-theory', 'almost-everywhere']"
3401781,Difference between Chromatic Number and Clique Number,"Let $G=(V,E)$ be any graph with $|V|=n$ vertices. It is known that $\chi \ge \omega$ , where $\chi$ (resp., $\omega$ ) is the chromatic (resp., clique ) number of $G$ . There are several examples (e.g., the Mycielskian ) that show that $\chi - \omega$ can be made arbitrarily large. However, in all such examples this difference seems to grow very slowly (i.e., logarithmically) with the number of vertices $n$ . Is this is always the case? Are there graphs for which the difference between the chromatic number and the clique number can be made, say, of the order $n^a$ , for some $a>0$ ?","['graph-theory', 'discrete-mathematics']"
3401851,"If $3\sec^4\theta+8=10\sec^2\theta$, find the values of $\tan\theta$","If $3\sec^4\theta+8=10\sec^2\theta$ , find the values of $\tan\theta$ . $$3\sec^4\theta-10\sec^2\theta+8=0$$ $$3\sec^4\theta-6\sec^2\theta-4\sec^2\theta+8=0$$ $$(3\sec^2\theta-4)(\sec^2\theta-2)=0$$ $$\sec^2\theta=2 \text { or } \sec^2\theta=\frac{4}{3}$$ $$1+\tan^2\theta=2 \text { or } 1+\tan^2\theta=\frac{4}{3}$$ $$\tan^2\theta=1 \text { or } \tan^2\theta=\frac{1}{3}$$ $$\begin{equation}
\tan\theta=\pm1  \text { or } \tan\theta=\pm\frac{1}{\sqrt{3}}
\end{equation}$$ So $\tan\theta$ can have four values $1,-1,\dfrac{1}{\sqrt{3}},-\dfrac{1}{\sqrt{3}}$ . But answer is only $1 \text{ or } \dfrac{1}{\sqrt{3}}$ . What are the things I am missing here?",['trigonometry']
3401896,Rank of circulant matrix with $k$ ones per row,"Consider the $n\times n$ matrix over the field $\mathbb F_2$ formed by creating the circulant matrix of the vector consisting of $k$ ones followed by $n-k$ zeroes. E.g., for $n=4$ and $k=2$ , the resulting matrix is $$\begin{bmatrix}1 & 1 & 0 & 0\\ 0 & 1 & 1 & 0\\0 & 0 & 1 & 1\\1 & 0&0 &1\end{bmatrix}.$$ What is the rank of this matrix? I suspect the answer is $n-d+1$ , where $d=\gcd(n,k)$ . I can prove this is an upper bound on the rank, since the following vectors are linearly independent and lie in the kernel of the matrix: $$\begin{bmatrix}1 & (i\ \textrm{zeroes}) & 1 & (d-2-i\ \textrm{zeroes}) & 1 & (i\ \textrm{zeroes}) & \cdots \end{bmatrix}$$ with $d-1$ different choices for $i$ . Is there an easy way to see that these completely describe the kernel?","['matrix-rank', 'finite-fields', 'matrices', 'linear-algebra', 'circulant-matrices']"
3401927,$\int \frac{dx}{1-\sin x+\cos x}$,"Solve : $$ \int \frac{dx}{1-\sin x+\cos x} $$ I tried: $$\int \frac{\text{dx}}{(1+\cos x)-\sin x} \times \frac{(1+\cos x)+\sin x}{(1+\cos x)+ \sin x} dx=\int \frac{1+\cos x+\sin x}{(1+\cos x)^2-(\sin x)^2}$$ $$=\int \frac{1+\cos x+\sin x}{1-\sin x^2+2 \cos x+\cos x^2}=\int \frac{1+\cos x+\sin x}{2 \cos x(\cos x+1)}$$ $$=\int (\frac{1}{2 \cos x}+ \frac{\sin x}{2 \cos x(\cos x+1)})dx=\frac{1}{2} \ln(\sec x+\tan x)+ \int \frac{\sin x}{2\cos x(\cos x+1)}dx$$ $ \cos x=u$ , $du=-\sin xdx$ $$\int \frac{-du}{2u(u+1)}= \frac{-1}{2} \int (\frac{1}{u}-\frac{1}{u+1})= \frac{-1}{2}(\ln(\cos x)-\ln(\cos x+1))$$ final answer : $$\frac{1}{2} \ln(\sec x+\tan x)-\frac{1}{2} \ln(\frac{\cos x}{1+\cos x})+ c$$ First: Is my answer right? Second: Is there another approach or easier approach to solve this integral?","['integration', 'calculus', 'closed-form']"
3401948,Looking for the software F(z) from lascaux graphics,"I know this is not a typical math question. I also thought about asking on superUser or stackoverflow but I came to the conclusion that I have a higher chance of getting an answer here on the math stackexchange. I recently have read about the tool named f(z) from lascaux graphics which helps visualizing complex valued functions. This seems to be a very nice tool in complex analysis. 
However it looks a little bit outdated and I cant find anything like a website or something. 
Do you know this tool and/or have any idea where I can find additional information about it?","['complex-analysis', 'math-software']"
3401962,"Derivative of $f(x,y)=x-y$, where $x,y\in\mathbb R^2$","Suppose that $f:\mathbb R^2\times\mathbb R^2\to\mathbb R^2$ is given by $f(x,y)=x-y$ . How can I calculate the first and the second order derivative of $f$ ? If $g:\mathbb R^n\to\mathbb R$ , the gradient vector and the Hessian matrix of $g$ are given by $$
\nabla g(x)_i=\frac{\partial g(x)}{\partial x_i}
\quad\text{and}\quad
\nabla^2g(x)_{ij}=\frac{\partial^2g(x)}{\partial x_i\partial x_j}
$$ for $i,j=1,\ldots,n$ . But the function $f$ is defined on the Cartesian product $\mathbb R^2\times\mathbb R^2$ . Is the Cartesian product $\mathbb R^2\times\mathbb R^2$ the same as $\mathbb R^4$ in some sense? Any help is much appreciated!","['multivariable-calculus', 'derivatives', 'real-analysis']"
3401978,convergence in a special sequense,"If $(a_{n})$ is a decreasing sequence which is positive for all $n$ and the series $a_{1}+a_{2}+a_{3}+...$ is divergent:prove this sequence is convergent to one $(a_{1}+a_{3}+...+a_{2n-1})/(a_{2}+a_{4}+...+a_{2n})$ i have tried solving this a couple of times but had no luck
at first i tried to list when a decreasing  sequence is divergent but i couldn't match the result with the rest of the assumptions and it has been a while since i have had this at the back of my mind so if you have any input 
your help would be appreciated.","['analysis', 'sequences-and-series']"
3402000,Showing that $\overline{\mathbb{Z}}/n\overline{\mathbb{Z}}$ is infinite,"Let $\overline{\mathbb{Z}}$ be the ring of algebraic integers. Since $n\overline{\mathbb{Z}}$ is an ideal of $\overline{\mathbb{Z}}$ , we can form the quotient ring $\overline{\mathbb{Z}}/n\overline{\mathbb{Z}}$ . I want to prove that it is infinite. The exercise has a hint to consider $x_i=n^{1/{2^i}}$ , for all $i\geq 1$ . However it is not clear to me why would someone think about these numbers nor why they are distinct in the quotient ring.","['number-theory', 'ring-theory', 'abstract-algebra', 'algebraic-number-theory']"
3402087,Reeb foliation of the plane and the Palais-Smale condition,"Definition. Let $X$ be a manifold. A smooth map $f:X\to \mathbb R$ is said to satisfy the Palais-Smale condition over $y\in \mathbb R$ if whenever $y_n\to y$ , any sequence $x_n\in f^{-1}(y_n)$ such that $\nabla f(x_n)\to 0$ has a convergent subsequence. If $x_{n_k}\to x_0$ then $\nabla f(x_{n_k})\to \nabla f(x_0)=0$ meaning the gradient field actually has a critical point. I have read an interpretation of the failure of the Palais-Smale condition as ""integral curves of the gradient field disappearing at infinity"". Question 1. What does this mean? The only reasonable meaning I can think of is that integral curves are defined for all time, but do not intersect all fibers of $f$ that it should. Is this the intended meaning? I want to understand the geometry of the Palais-Smale condition with an example. Consider the Reeb foliation of the plane, defined by the surjective submersion $f:\mathbb R^2\to \mathbb R$ given by $(x,y)\to (x^2-1)e^y$ . The gradient is $\nabla f(x,y)=(2xe^y,(x^2-1)e^y)$ and its integral curves are shown below. The submersion $f$ does not satisfy the Palais-Smale condition above zero, since we have the sequence $a_n=(0,-n)$ on which the gradient and the function tend to zero, but there's no convergent subsequence. I would like to understand the picture here. The integral curves certainly slow down as $y\to -\infty $ , but it seems there's only one problematic curve: the $y$ -axis intersects the negative fibers orthogonally, so there's an integral curve of the gradient field heading downwards along it. There's no reason for this curve to be defined only for a finite time, so it seems to be defined for all time (including backwards). Evidently it never meets a non-negative fiber, consistent with the suggested interpretation above. I am confused by this. Any connection on $f$ is a parametric vector field on $\mathbb R^2$ (with real parameter) whose flow lifts the parametric radial field $(x,y)\mapsto x+ty$ whenever it exists. Taking the orthogonal connection seems to give the gradient field, so its integral curves should lift straight paths. Consequently, a gradient field curve defined for all time should lift an infinite straight path in $\mathbb R$ , and therefore intersect all fibers of $f$ . But this contradicts reality: the vertical gradient field curve along the $y$ -axis is defined for all time but does not intersect any non-negative fibers! Question 2. What am I missing here? Looking at the submersion again, there are many more Palais-Smale sequences. Consider for instance $a_n=(-n,-n)$ , on which the gradient and the function tend to zero. Let us restrict our submersion to e.g $ \left\{ x<-1 \right\}$ . Then despite not meeting the Palais-Smale condition, this seems to be a fiber bundle - we can ""comb the fibers downwards"". There don't seem to be any problematic integral curves for the gradient field. Question 3. What is the geometric interpretation of the failure of the Palais-Smale condition for $f|_ {\left\{ x<-1 \right\}}$ ? Question 4. Why does the sequence $(0,-n)$ ""see"" a problem that $(-n,-n)$ does not? Question 5. Why did it matter at all the the derivative tends to zero on the above sequences? It seems the same pathology with the vertical integral curve could occur even if it sped up as $y\to -\infty$ ...","['gradient-flows', 'vector-fields', 'foliations', 'fiber-bundles', 'differential-geometry']"
3402183,Calculating logarithmic integrals without using the derivatives of Beta function.,"How to prove the following generalizations  without using the derivatives of beta function: $$i)\int_0^1\frac{x^{n}\ln^m(x)\ln(1-x)}{1-x}\ dx=(-1)^{m-1}m!\sum_{k=1}^\infty\frac{H_k}{(k+n+1)^{m+1}}\\=\frac12\frac{\partial^m}{\partial n^m}\left(H_n^2+H_n^{(2)}\right),\quad n>-2,\quad m\in\mathbb{N}$$ $$ii)\int_0^1\frac{x^n\ln^m(x)\ln^2(1-x)}{1-x}\ dx=(-1)^mm!\sum_{k=1}^\infty\frac{H_k^2-H_k^{(2)}}{(k+n+1)^{m+1}}\\=-\frac1{3}\frac{\partial^m}{\partial n^m}\left(H_n^3+3H_nH_n^{(2)}+2H_n^{(3)}\right),\quad n>-2,\quad m\in\mathbb{N}$$ The common problem with the derivative of Beta function $\text{B}(a,b)$ is the case when $a$ or $b$ approaches zero because we know that Beta derivative involves $\psi(a),\psi_1(a),\psi_2(a) ..$ and $\psi(b),\psi_1(b),\psi_2(b)...$ and the limit of these polygamma is undefined when $a$ or $b$ approaches zero and we will need the help of Wolfram or Mathematica to calculate such derivatives. Using the above identities will help us avoid this issue as we just need to take the derivative of the harmonic number $\frac{\partial}{\partial n}H_n^{(a)}=a(\zeta(a+1)-H_n^{(a+1)})$ or you can simply convert the harmonic number to polygamma function $\psi_a(n+1)=(-1)^{a+1}a!(\zeta(a+1)-H_n^{(a+1)})$ as the derivative of polygamma is more straightforward. I will provide the proofs soon but variant approaches are always appreciated.","['integration', 'real-analysis', 'harmonic-numbers', 'calculus', 'sequences-and-series']"
3402214,Damaged lights proof,"There are $n$ lights in a row. Some of the $n$ lights are on. I can damage any light that is on. If I damage a light, each light directly adjacent to the damaged light will be turned on if it is off, off if it is on. Damaged lights remain unchanged. Prove that I can damage all $n$ lights if and only if the number of lights that are initially on is odd. Example: off, on, off I damage the 2nd light on, (damaged), on My thoughts are to prove by contradiction, where assume the number of lights turned on is even. But I can't see what follows.","['induction', 'proof-writing', 'discrete-mathematics']"
3402225,Redefining stochastic processes on new probability space,"I am reading a paper and having trouble with understanding the following.
The setup is two stochastic processes, $(X_t)_{t\ge 0}$ and $(W_t)_{t\ge 0}$ , where $(W_t)_{t\ge 0}$ is just a Brownian motion. It is assumed we can redefine both of these on a new probability space so that the joint distributions coincide. Let $(Y_n)_{n\in \mathbb{N}_0}$ be a discrete process and suppose that there exists non-negative random variables $T_k$ such that $\{W(\sum_{j\le M}T_j),M\ge 1\}$ and $\{\sum_{j\le M}Y_j,M\ge 1\}$ have the same distribution (Skorokhod representation). Then $(Y_n)$ is redefined on a new probability space by $Y_M=W(\sum_{j\le M}T_j)-W(\sum_{j<M}T_j)$ . For an $M\ge 1$ and define the process $S(t)=\sum_{j=1}^{M[t]}Y_j$ . The paper then says that following, which I do not understand: We can redefine $(X_t)_{t\ge 0}$ , $(S_t)_{t\ge 0}$ , and $(W_t)_{t\ge 0}$ on another probability space so that the joint distribution of $(S_t)_{t\ge 0}$ and $(W_t)_{t\ge 0}$ and the joint distribution of $(X_t)_{t\ge 0}$ and the old version of $(S_t)_{t\ge 0}$ remain unchanged. Simply choose the newer probability space in such a way that $(X_t)_{t\ge 0}$ and $(W_t)_{t\ge 0}$ are conditionally independent given $(S_t)_{t\ge 0}$ . Could anyone care to elaborate on this? The paper is ""Almost sure invariance principles for partial sums of weakly dependent random variables"" by Walter Philipp and William Stout.","['conditional-probability', 'independence', 'stochastic-processes', 'brownian-motion', 'probability-theory']"
3402231,convergence almost surely that uses lim sup of probabilities,"What's wrong with my intuition here? I read that given the following sequence of random numbers $X_i \in (0,1]$ , with $I$ as indicator function: $$
X_1 = I(0,1/2] \quad X_2 = I(1/2,1] \quad X_3 = I(0,1/4] \quad X_4 = I(1/4,1/2] \quad X_5 = I(1/2,3/4] \quad X_6 = I(3/4,1] \quad X_7 = I(0,1/8] \quad X_8 = I(1/8,1/4] \quad ...
$$ The sequence $X_i$ converges in probability as $i \rightarrow \infty$ since given any $\epsilon > 0$ , we have: $$
P[X_i \geq \epsilon] \rightarrow 0
$$ However, $X_i$ does not converge almost surely (or almost everywhere), since given any $i$ , there is a $n \geq i$ , s.t. $P[X_n] > \epsilon$ for some $\epsilon > 0$ (which means that $X_n$ is above $0$ infinitely)... However, I could not relate this with an alternative definition for almost sure convergence which is as follows given any $\epsilon > 0$ : $$
\text{lim sup}_{i \rightarrow \infty} \{ P[X_i \geq \epsilon] \} \rightarrow 0
$$ I guess my confusion in this definition is when it starts to use probabilities $P$ . Since the 'size' of the intervals where $P[X_i] > \epsilon$ also approaches zero, $0$ , i.e. it gets narrower and narrower, shouldn't it also follow that the Lebesgue measure of $P[X_i] > \epsilon \rightarrow 0$ such that its supremum $\rightarrow 0$ ?","['statistics', 'almost-everywhere', 'uniform-convergence', 'convergence-divergence', 'pointwise-convergence']"
3402237,The normal form of a skew symmetric matrix,"In Werner Greub's book Linear Algebra , 4th ed. on p. 230, he gives this proof of the normal form for a skew transformation on a finite-dimensional real inner product space . (Note Greub's convention for the matrix of a transformation is the transpose of that normally used with left-hand notation.) I believe this proof is incorrect because it is not true in general that the $a_n$ defined form an orthonormal basis of the space. For example in $\mathbb{R}^4$ , if we define the transformation $\psi$ by $$e_1\mapsto e_2\qquad e_2\mapsto -e_1\qquad e_3\mapsto e_4\qquad e_4\mapsto -e_3$$ where $e_i$ is the $i$ -th standard basis vector, then $\psi$ is skew and $\varphi=\psi^2=-\iota$ is diagonalized by the standard basis. If we follow the proof for this example, we get $a_1=e_1$ , $a_2=\psi e_1=e_2$ , $a_3=e_2$ , and $a_4=\psi e_2=-e_1$ , so the $a_n$ do not form a basis of $\mathbb{R}^4$ . Does anyone see a way to salvage this proof while still retaining its spirit (in particular, avoiding use of complex numbers)?","['proof-verification', 'skew-mapping', 'matrices', 'linear-algebra', 'linear-transformations']"
3402241,$X_n/n^\alpha\to a$ almost surely if $EX_n\sim an^\alpha$ and $\mathrm{var}(X_n)\leq Bn^\beta$,"I am going through the exercises in the Borel-Cantelli section of Durrett, and stuck on this question. (Exercise 2.3.18) Let $0\leq X_1\leq X_2\cdots$ be random variables with $EX_n\sim an^\alpha$ and $\mathrm{var}(X_n)\leq Bn^\beta$ , where $a,\alpha>0$ and $\beta<2\alpha$ .
Show that $X_n/n^\alpha\to a$ almost surely. My attempt: Since $(EX_n-an^\alpha)/n^\alpha\to0$ as $n\to\infty$ , it suffices to show that $(X_n-EX_n)/n^\alpha\to0$ almost surely.
For any $\epsilon>0$ , by Chebyshev's inequality we have $$
P\left(\left|\frac{X_n-EX_n}{n^\alpha}\right|>\epsilon\right)\leq\epsilon^{-2}\cdot n^{-2\alpha}\cdot\mathrm{var}(X_n)
\leq B\epsilon^{-2}\cdot n^{\beta-2\alpha}\to 0
$$ as $n\to\infty$ , so $(X_n-EX_n)/n^\alpha\to0$ in probability.
The condition $\beta<2\alpha$ is too weak, however, in a sense that the final estimate is not summable in $n$ and one cannot apply the first Borel-Cantelli lemma to ensure the almost sure convergence. I think the non-decreasing condition $0\leq X_1\leq X_2\cdots$ will help us get to the destination, but I am not sure how to use this assumption.","['variance', 'almost-everywhere', 'convergence-divergence', 'probability-theory', 'random-variables']"
3402341,Basic Number Theory Inequality assuming Bertrand's postulate false for some n.,"So I am having a hard time doing one problem from George E. Andrews Number Theory book . It is problem $4$ chapter $8$ - $3$ if anyone is curious. For it we must assume that Bertrand's Postulate is false for some $n$ (i.e. for some Integer $n$ there is NO prime $p$ such that $n<p<2n$ ). Also, we should use in our proof one inequality previously postulated. It is the following: $${2n\choose n}\leq\ Q_n \leq\ (2n)^{\pi(2n)}$$ where $Q_n$ is the product of all power of primes not exceeding $2n$ (i.e. the product of all $p^r$ where $r$ is such that $p^r \leq\ 2n <p^{r+1} $ ) and $\pi(2n)$ is the prime counting function for $2n$ .
Knowing all this, I need to prove that $${2n\choose n}\leq\ (2n)^{\sqrt {2n}}R_{\frac{2}{3}n} $$ where $R_{\frac{2}{3}n} $ is the product of all primes less than $\frac{2}{3} n $ .
So my first thought was to use the needed assumption to see that $\pi(2n) = \pi(n)$ and from the first inequality I would just need $$(2n)^{\pi(n)} \leq\ (2n)^{\sqrt {2n}}R_{\frac{2}{3}n} $$ to be true. But this is just false even with our assumption. Take for example $n = 7$ then you would get $14^6 = 7529536 \gt  14^{\sqrt {14}}(2 \cdot\ 3)= 116565.406$ So I am really stuck now because I find that $\frac{2}{3}n$ too specific and I don't know where it might come from for that to be true. EDIT: so I was thinking on $\phi (7)$ when I did that calculation, the actual result for $n = 7$ should be $14^4 = 38416$ and that is indeed less than $116565.406$ . So to the way might be to prove the last inequality but I still don't know how to put there the $R_ {\frac{2}{3}n}$ . And Knowing that primes are more dense than squares that ratio should also be enough to make that relation flip, considering  the desire result and the exponents on both sides","['prime-factorization', 'number-theory', 'binomial-coefficients', 'inequality', 'prime-numbers']"
3402368,What is the closed form of this sum?,"What is the closed form of this sum? $$
S = \sum_{k\ge1, r>s\ge 1}\frac{1}{k^2(r^2 + s^2)^2}
$$ Note : Though originally posted for Pythagorean triplets, their was an flaw in the question which changed the meaning of the question and was answered accordingly. I will post a separate question with the original question on Pythagorean triangles. Update : I have posted the related question of Pythagorean triangles in the link belwo What is the sum of the reciprocal of the square of hypotenuse of Pythagorean triangles?","['summation', 'number-theory', 'geometry', 'real-analysis', 'sequences-and-series']"
3402382,"What is an example of a number field with signature $(4,0)$?","I know why, say, quadratic fields have signature $(2,0)$ or complex number fields have signature $(0,1)$ . But what does a number field with signature $(4,0)$ look like?","['number-theory', 'abstract-algebra', 'algebraic-number-theory']"
3402414,$f$ is continuous at point $a$ iff $\lim_{h\to0}f(a+h)-f(a)=0$,"$$\lim_{h\to0}f(a+h)-f(a)=0\leftrightarrow \lim_{x\to a}f(x)=f(a)$$ Proof. $\Rightarrow:$ Assume $$\forall \varepsilon>0,\exists\delta>0,s.t.0<|\color{blue}{h}|<\delta\rightarrow |f(a+\color{blue}{h})-f(a)|<\varepsilon$$ Show $$\forall \varepsilon>0,\exists\delta>0,s.t.0<|x-a|<\delta\rightarrow |f(x)-f(a)|<\varepsilon$$ Let $h:=x-a$ that by assumption we have $$\forall \varepsilon>0,\exists\delta>0,s.t.0<|\color{blue}{x-a}|<\delta\rightarrow |f(a+\color{blue}{x-a})-f(a)|<\varepsilon$$ $$\forall \varepsilon>0,\exists\delta>0,s.t.0<|\color{blue}{x-a}|<\delta\rightarrow |f(x)-f(a)|<\varepsilon$$ $\Leftarrow:$ Assume $$\forall \varepsilon>0,\exists\delta>0,s.t.0<|\color{blue}{x-a}|<\delta\rightarrow |f(x)-f(a)|<\varepsilon$$ Show $$\forall \varepsilon>0,\exists\delta>0,s.t.0<|h|<\delta\rightarrow |f(a+h)-f(a)|<\varepsilon$$ Let $x-a:=h$ by assumption we have $$\forall \varepsilon>0,\exists\delta>0,s.t.0<|\color{blue}{x-a}|<\delta\rightarrow |f(a+\color{blue}{x-a})-f(a)|<\varepsilon$$ $$\forall \varepsilon>0,\exists\delta>0,s.t.0<|\color{blue}{h}|<\delta\rightarrow |f(a+\color{blue}{h})-f(a)|<\varepsilon\tag*{$\square$}$$ I saw some notes state that $\lim_{h\to0}f(a+h)-f(a)=0$ implies continuous, but i don't know this alternative definition before, so i'm tring to prove this. the proof seems like just plug in some variables to the limit-definition Is this correct, thanks for your help.","['proof-verification', 'real-analysis', 'continuity', 'definition', 'limits']"
3402434,Kernel and image of linear operator $\alpha(f) = f(t+1) - f(t)$,"Let $V = K[t]_n = \{f \in K[t], \deg f \le n\}$ . Find kernel and image of the operator $\alpha(f) = f(t+1) - f(t)$ . My attempt: I can take a standard basis $1, t, \cdots, t^n$ and find matrix of the linear operator: $\begin{bmatrix}
 0&  \binom{1}{0}&  \binom{2}{0}&  .& \binom{n}{0} \\ 
 0&  0&  \binom{2}{1}&  .& \binom{n}{1}\\ 
 .&  .&  0&  .&. \\ 
 .&  .&  .&  .&\binom{n}{n - 1} \\ 
 0&  0&  0&  0&0 
\end{bmatrix}$ So, if $\text{char}K = 0$ , then $\text{rank } A = \dim\text{Im } \alpha = n$ (it is $K[t]_{n - 1}$ ), and $\ker \alpha = K[t]_0 = K$ But I don't know how to solve it if $\text{char} K\ne 0$ ( $K$ is a field)","['matrices', 'elementary-number-theory', 'abstract-algebra', 'linear-algebra']"
3402446,"Show that $3\int_{[0,1)} |f|d\lambda+4\int_{[1,2]} |f|d\lambda\leq 5\bigg( \int_{[0,2]} |f|^2d\lambda \bigg)^{1/2}$","In some homework I have to show that for $f \in \mathcal{L}^2([0,2],\lambda)$ then: $$3\int_{[0,1)} |f|d\lambda+4\int_{[1,2]} |f|d\lambda\leq 5\bigg( \int_{[0,2]} |f|^2d\lambda \bigg)^{1/2}$$ But I have run out of good ideas. I tried applying Pythagoras and Minkowski but with no luck Any hint would be appreciated","['measure-theory', 'lebesgue-measure', 'lebesgue-integral']"
3402459,Pointwise approximation of a nested function by simple functions,"Let $(\Omega,\mathscr F)$ be a measurable space and $\mathscr G\subseteq\mathscr F$ a $\sigma$ -subalgebra of $\mathscr F$ . Let $(Y,\mathscr Y)$ be another measurable space. Suppose that $f:\Omega\times Y\to\mathbb R$ is a real-valued function measurable with respect to the product $\sigma$ -algebra $\mathscr F\otimes\mathscr Y$ (where $\mathbb R$ is endowed with the Borel $\sigma$ -algebra). Finally, suppose that $g:\Omega\to Y$ is a $\mathscr G/\mathscr Y$ - measurable function (note that $\mathscr G$ is the $\sigma$ -subalgebra here). Now consider the following nested function mapping from $\Omega$ to $\mathbb R$ : \begin{align*}
\omega\mapsto f(\omega,g(\omega))
\end{align*} I am wondering whether this nested function can be approximated by simple functions of sorts of the following form: \begin{align*}
f(\omega,g(\omega))\approx\sum_{i=1}^n I_{G_i}(\omega)f(\omega,y_i)\quad\text{for each $\omega\in\Omega$,}\tag{$*$}
\end{align*} where $n$ is a positive integer; $y_1,\ldots,y_n$ are elements of the set $Y$ ; $G_1,\ldots, G_n$ are disjoint sets in $\mathscr G$ whose union is $\Omega$ ; and $I_{G_i}$ is the indicator function of $G_i$ for each $i\in\{1,\ldots,n\}$ . The “ $\approx$ ” in ( $*$ ) is in the usual sense: there exists a sequence of functions of the form on the right-hand side converging pointwise (for each $\omega\in\Omega$ ) to the left-hand side. Note that no topological structure on $(Y,\mathscr Y)$ is assumed. Any suggestion about or reference to a proof or a counterexample would be greatly appreciated. UPDATE: The original conjecture is false (see below), so I am going to refine it as follows. Let $\mathbb P$ be a probability measure on $(\Omega,\mathscr F)$ and suppose that $f$ is bounded. Can the integral of $\omega\mapsto f(\omega,g(\omega))$ be approximated by integrals of functions of the form on the right-hand side of ( $*$ )? More precisely, let \begin{align*}
\mathcal H\equiv\{h:\Omega\to\mathbb R\mid&\bullet h(\omega)=\sum_{i=1}^nI_{G_i}(\omega)f(\omega,y_i)\text{ for each $\omega\in\Omega$},\\
&\bullet n\in\mathbb N,\\
&\bullet y_1,\ldots,y_n\in Y,\\
&\bullet \text{$G_1,\ldots, G_n$ are disjoint sets in $\mathscr G$ whose union is $\Omega$}\}.
\end{align*} Does there exist a sequence $(h_m)_{m\in\mathbb N}$ in $\mathcal H$ such that \begin{align*}
\lim_{m\to\infty}\left\{\int_{\Omega}h_m(\omega)\,\mathrm d\mathbb P(\omega)\right\}=\int_{\Omega}f(\omega,g(\omega))\,\mathrm d\mathbb P(\omega)?
\end{align*} My hunch (and hope, indeed) is that some kind of countability argument due to the ( $\sigma$ -)finiteness of $\mathbb P$ may make the approximation for at least integrals work. To see why pointwise approximation does not work, let $(\Omega,\mathscr F)$ and $(Y,\mathscr Y)$ both be $\mathbb R$ endowed with the discrete $\sigma$ -algebra and $\mathscr G=\mathscr F$ . Let $g$ be the identity function and $f$ the indicator function of the diagonal $$D\equiv\{(x,x)\,|\,x\in\mathbb R\}.$$ Note that the diagonal is $\mathscr F\otimes\mathscr F$ -measurable, since it is of the form $$D=\bigcap_{m\in\mathbb N}\bigcup_{q\in\mathbb Q}\left[q-\frac{1}{m},q+\frac{1}{m}\right]\times\left[q-\frac{1}{m},q+\frac{1}{m}\right].$$ Then, $$f(\omega,g(\omega))=1\quad\text{for each $\omega\in\mathbb R$}.$$ For any sequence $(h_m)_{m\in\mathbb N}$ of functions in $\mathcal H$ , let $\widetilde Y$ denote the union of all the $y_i$ ’s appearing in the definition of the $h_m$ ’s. Then, $\widetilde Y$ is a countable union of finite sets, so it is countable. Taking $\widetilde\omega\in\mathbb R\setminus\widetilde Y$ , one can see that $h_m(\widetilde\omega)=0$ for each $m\in\mathbb N$ , so pointwise convergence fails at $\widetilde\omega$ .","['conditional-expectation', 'measure-theory', 'reference-request', 'real-analysis']"
3402479,The sum of infinite fours: $\sqrt{4^0+\sqrt{4^1+ \sqrt{4^2+ \dots}}}=?$,"$\sqrt{4^0+\sqrt{4^1+\sqrt{4^2+\sqrt{4^3+\cdots}}}}=?$ I found this problem in a book. I tried to solve this but couldn't. Using calculator, I found the value close to $2$ . But how can this problem be solved with proper procedure?","['nested-radicals', 'calculus']"
3402513,"How do ""Eigenbases"" and ""orthonormal bases"" relate?","So I have a definition for each of the above: Eigenbasis : when the matrix in question is in diagonal form. Only possible when there are n eigenvectors for a matrix in n-dimensional space. Orthonormal basis : when basis vectors are a)orthogonal, b)unit-length. What I'm trying to understand is - how do they relate? Like, is eigenbasis always orthonormal? Or does orthonormal basis always have an eigenbasis? Or are the two the same even? Thanks!","['matrices', 'change-of-basis', 'linear-algebra', 'eigenvalues-eigenvectors']"
3402517,Construct a coherent subsheaf of a quasi-coherent sheaf,"I am trying to prove the following statement Given a Noetherian scheme $X$ and a surjective morphism of quasi-coherent sheaves on $X$ : $$\mathcal{F}\xrightarrow{f} \mathcal{G} \to 0$$ where $\mathcal{G}$ is coherent, we can find a subsheaf $\mathcal{F'}$ of $\mathcal{F}$ , which is coherent, such that the induced morphism $$\mathcal{F'}\rightarrow \mathcal{G} \to 0$$ is also surjective. My thought: The algebraic version of this is obvious, that is, given a Noetherian ring $A$ and an epimorphism of $A$ -modules $$M\to N\to 0$$ where $N$ is finitely generated, we can find a finitely generated submodule $M'$ such that the induced map $M'\to N$ is surjective. So it is natural to restrict the morphism of sheaves on an affine open subset (we can find a finite affine open cover $\{U_i\}$ of $X$ ), $$\mathcal{F}|_{U_i}\cong \widetilde{M_i}, \ \ \ \ \mathcal{G}|_{U_i}\cong \widetilde{N_i}$$ for some $A_i$ -modules $M_i$ and $N_i$ , where $U_i=\mathrm{Spec}(A_i)$ . Then for the map $M_i\to N_i$ , we can find the finitely generated submodule $M_i'$ . Thus, the coherent sheaf $\widetilde{M_i'}$ is the desired subsheaf, but unfortunately, only on $U_i$ . I am stuck at the point how to glue these subsheaves on $U_i$ together to get a coherent sheaf on $X$ . Any answer and hints are welcome! I am not sure if the statement is true, but it is a promising ""lemma"" to show the derived category of the complexes of coherent sheaves is equivalent to the derived category of the complex of quasi-coherent sheaves with coherent cohomology, that is, $$D^b(\mathrm{Coh}(X))\simeq D^b_{\mathrm{Coh}(X)}(\mathrm{QCoh}(X)).$$ Update: This is true (see Lemma 3.6 on Huybrechts' Fourier-Mukai Transforms in Algebraic Geometry ) and his proof is clear.","['homological-algebra', 'derived-categories', 'coherent-sheaves', 'algebraic-geometry', 'abstract-algebra']"
3402521,Calculating infinitely near points of a curve singularity,"Let $f(x,y)$ be a polynomial in $\mathbb{Q}[x,y]$ with a singular point at the origin $(0,0)$ . Currently I am writing a Maple program to compute its iterated blow-up embedded resolution as a hierarchy of charts which are linked by coordinate transformations of the type $[x = x, y = t x]$ and $[x = s y, y = y]$ . It works quite straightforward, the greatest difficulty was the bookkeeping and to find a proper order for recursion. See the bottom of the page http://www.aviduratas.de/compalg.html for the current version and an example use. The resolution graphs that I compute from this tree seem to come out right now, but I have problems with computing the (numbers and multiplicites of) infinitely near points of $(0,0)$ , that is the points of the transform of $f = 0$ that lie above $(0,0)$ in the collection of charts generated. For debugging it would be useful to have a computer algebra system that can calculate the (multiplicities of) infinitely near points, or at least to have a list of known correct examples. At first I thought that for example the Singular system should have such functionality but I could not find it in the manual. Does anyone know of a software for calculating the infinitely near points that is publicly available on the internet? Maybe someone who wrote for his own use such a routine? (I came to write this program to solve Hartshorne V, ex. 3.8. While researching the above question I noted the following post: Configuration of infinitely near points My own calculations support the view of the OP there that there is a mistake in the exercise, does anyone know more here?)","['computer-algebra-systems', 'algebraic-geometry', 'singularity-theory']"
3402580,What is the max length string that can be formed using k distinct characters so that all of its substrings are unique.,"Given k distinct characters , what is the max length string that can be formed using these characters one or more time so that all the sub-string whose size is greater than one are unique. Eg - For k = 3  {a,b,c} A string of max  10 length can be made so that all of its substring whose length is greater than one are unique.(45 sub strings) String = aabbccacba .
 Its sub-string of size greater than 2 are {aa , aab , aabb , aabbc , aabbcc , aabbcca , aabbccac , aabbccacb , aabbccacba , ab , abb , abbc , abbcc , abbcca , abbccac , abbccacb , abbccacba , bb , bbc , bbcc , bbcca , bbccac , bbccacb , bbccacba , bc , bcc , bcca , bccac , bccacb , bccacba , cc , cca , ccac , ccacb , ccacba , ca , cac , cacb , cacba , ac , acb , acba , cb , cba , ba} all of which are unique.","['permutations', 'combinatorics-on-words', 'binomial-coefficients', 'combinatorics', 'bit-strings']"
3402639,Find all real matrices such that $X^{6} + 2X^{4} + 10X = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$,"Find all matrices in $M_2(\mathbb R)$ such that $$X^{6} + 2X^{4} + 10X = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$$ I tried to take the determinant and trace of both side, but it seems like it only works when the determinant of the RHS equals $0$ . Can you guys help me please? Thank you.","['matrices', 'matrix-equations', 'linear-algebra']"
3402643,"Prove that if $x$ is the greatest lower bound of $U$, then $x$ is the least upper bound of $B$","Suppose $R$ is a partial order on $A$ , $B \subseteq A$ . Let $U$ be the set for all upper bounds for $B$ . Prove that if $x$ is the greatest lower bound (or g.l.b) of $U$ , then $x$ is the least upper bound (or l.u.b) of $B$ . My attempt: Suppose $L_u$ is the set containing all lower bounds of $U$ . Suppose $x$ is g.l.b. of $U$ . Take arbitrary $b \in B$ Take $u \in U$ . We know that $bRu$ . Since $u$ was arbitrary, we conclude that $b$ is lower bound for $U$ , which means $b \in L_u$ Since $x$ is g.l.b. of $U$ , we have $x \in L_u$ and $bRx$ . Since $b$ was arbitrary element of $B$ , it follows that $x$ is upper bound of $B$ , thus $x \in U$ . Since $x \in L_u$ , it also follows that for all $u \in U$ , we have $xRu$ , hence $x$ is the smallest element of $U$ , which implies that $x$ is l.u.b. of $B$ . $\Box$ Is it correct?","['elementary-set-theory', 'proof-verification']"
3402665,What's wrong with this proof of almost sure limits?,"Let $(X_n)$ be a sequence of I.I.D. random variables uniformly distributed on $(0,1)$ . Let $Y_n=\prod_{k=1}^nX_k$ . My task is to find the almost sure limit of $Y_n$ . Here is my work: It is somewhat obvious that the almost sure limit will be $0$ , so I prove that this is the almost sure limit. Next, note the following: $$0\leq \prod_{k=1}^nX_k \leq [\max_{k=1,...,n}X_n]^n, $$ and because $s^n$ converges to zero for all $|s|<1$ , it follows that the the product must converge to zero. I'm pretty sure that this proof is wrong because it seems for too simple. What's the issue if there is one?",['probability-theory']
3402706,"Prove that if $R$ is a relation on $A$, then $R$ has a reflexive closure.","Suppose $R$ is a relation on $A$ . Then $R$ has a reflexive closure. My attempt: Let $$i_A = \{(a,a) \mid a \in A\}$$ which means $i_A \in A \times A$ Consider set $R \cup i_A $ . To prove that $R \cup i_A $ is a reflexive closure, we need to show that $R \subseteq R \cup i_A $ $R \cup i_A$ is reflexive. For every relation $T \subseteq A \times A$ , if $R \subseteq T$ and $T$ is reflexive, then $S \subseteq T$ . 1. By definition of $\cup$ , we know that for all $x$ , $x \in R \implies R \cup i_A$ , hence $R \subseteq R \cup i_A $ 2. Take $a \in A$ . Since $(a,a) \in i_A$ it follows that $(a,a) \in R \cup i_A$ 3. Suppose $T \subseteq A \times A$ , $R \subseteq T$ and $T$ is reflexive. Since $T$ is reflexive, we have $i_A \subseteq T$ ; we also know that $R \subseteq T$ , which implies that $R \cup i_A \subseteq T$ We've shown that three properties hold, hence $R \cup i_A$ is a reflexive closure of $R$ . Is it correct?","['elementary-set-theory', 'proof-verification']"
3402723,Is x^1/3 the same as cube root(x)?,"I needed to evaluate the surjectivity of cuberoot(x) and used Wolfram-Alpha for that.  To my surprise, cuberoot(x) is different than x^1/3 , as can be seen A and B . Further digging shows two contradictory school of thoughts: equal not equal From my memory, x power 1/n is the nth root of x. However, article B mentions De Moivre's theorem and I don't think I ever encountered it in the undergrad engineering curriculum. So are they the same?  And in which class do you study this theorem?","['complex-analysis', 'exponentiation', 'radicals', 'wolfram-alpha']"
3402766,Prove that $\mathbb{P}(\frac{\sum_{i=1}^nX_i^2}{n}\geq \epsilon u^2/2) \geq 1-2e^{-c\epsilon n}$ given $\mathbb{P}(X_i^2 \geq u^2) \geq \epsilon$,Assume that $\mathbb{P}(X_i^2 \geq u^2) \geq \epsilon$ and $X_i$ are i.i.d. Prove that $\mathbb{P}(\frac{\sum_{i=1}^nX_i^2}{n}\geq \epsilon u^2/2) \geq 1-2e^{-c\epsilon n}$ for all $c > 0$ . Can anyone give me some hints? It sounds related to central limit theorem but I am not sure how.,"['inequality', 'probability-theory']"
3402779,A problem on recursive functions,"Given the function, $$f(n)= 
\begin{cases}
\ n-3 \quad  \text{ if }\quad n\ge1000\\
 \\
f (f (n+5)) \quad\text{ if } \quad n<1000
\end{cases}
$$ What is the value of $f (83)-f (84) $ ? One of my friends gave me this problem.
I tried to find a pattern for the numbers but couldn't. I programmed the recursive function on C++ and got $1$ as the answer.",['functions']
3402780,Convergence of moments implies convergence of absolute moments?,"Let $\mu$ be a probability measure on $\mathbb R$ and define $$
\alpha_n = \int_{\mathbb R} x^n\,\text d\mu(x)
$$ to be the $n$ th moment. Suppose that $$
\sum_{n=0}^\infty \frac{\alpha_nt^n}{n!} < \infty
$$ for $|t| < \delta$ with $\delta > 0$ being small. Let $$
\beta_n = \int|x|^n\,\text d\mu(x)
$$ be the $n$ th absolute moment. Can I conclude that there is some $\varepsilon > 0$ such that $$
|t| < \varepsilon \stackrel ?\implies \sum_{n=0}^\infty \frac{\beta_n t^n}{n!} < \infty
$$ ? I can show that for $|t|$ sufficiently small $\frac{\beta_n t^n}{n!}\to 0$ , but I'm not sure how to show convergence or how to come up with a counterexample.","['moment-generating-functions', 'probability-theory', 'real-analysis']"
3402783,What are the angles of this triangle?,"In triangle ABC, angle A is half of angle C and side BC is half of side AC.
What are the angles? Sorry about the drawing, lol I realize that it should be 90°, 60°, 30° but since it’s not given that it’s a right triangle I just can’t seem to arrive to the answer. 
I’m supposed to solve this with only the basic triangle and parallel line theorems, without any trigonometry.","['euclidean-geometry', 'triangles', 'geometry']"
3402842,Is a path which decreases a function in the quickest way a gradient flow?,"Let $U \subseteq \mathbb{R}^n$ be open, and let $F:U \to \mathbb{R}$ be a smooth function. Fix a point $p \in U$ , and suppose that $\nabla F(p) \neq 0$ . Let $\alpha(t)$ be a $C^{\infty}$ path starting at $p$ . Suppose that $\alpha$ ""beats"" all $C^{\infty}$ paths starting at time $t=0$ for a short time in the following sense: For every $C^{\infty}$ path $\beta(t)$ starting at $p$ that satisfies $\|\dot \beta(t)\|=\|\dot \alpha(t)\|$ , we have $F(\alpha(t)) \le F(\beta(t))$ for sufficiently small $t>0$ . (The ""sufficiently small"" here might depend on the path $\beta$ ). Question: Must $\alpha$ be a reparametrization of the  negative gradient flow of $F$ , i.e. $$ \alpha(0)=p, \, \, \dot \alpha(t)=c(t)\cdot \big(-\nabla F(\alpha(t))\big) \,\, \text{where } c(t)>0  \,\,?$$ It is not hard to see that we must have $\dot \alpha(0)=-\nabla F(p)$ (up to a positive rescaling). If we could show that $\alpha$ locally ""beats"" all $C^{\infty}$ paths starting at $\alpha(t)$ for sufficiently small $t>0$ , then the same logic would imply the required claim. I don't know how to ""propagate"" this optimality criterion from time $t=0$ to a time $t>0$ . Here is my naive attempt: Assume by contradiction that $\alpha$ does not beat all paths at some interval $[0,\epsilon)$ . Then there exist a decreasing sequence $t_n \to 0$ which demonstrates the non-optimality of $\alpha|_{[t_n,)}$ as a path starting at $\alpha(t_n)$ . This means that there exist smooth paths $\beta_n:[t_n,.) \to U$ , $\beta_n(t_n)=\alpha_n(t_n)$ , and $s_n>t_n$ where $s_n-t_n$ is arbitrarily small, such that $F(\alpha(s_n)) > F(\beta_n(s_n))$ . Now, I guess I should somehow take the limit of the $\beta_n$ or ""glue"" them together to obtain a path which starts at $p=\alpha(0)$ , and that beats $\alpha$ . I am not sure how to do that.","['gradient-flows', 'multivariable-calculus', 'optimization', 'gradient-descent', 'differential-geometry']"
3402942,When do a collection of rational balls with changing radius cover $\mathbb{R}$?,"Motivation: I was looking for a slick open cover of a set in $\mathbb{R}$ by open balls and ended up constructing the following. Let $\{q_i\}_{i\in I}$ be an enumeration of the rationals and let $r_i = \varepsilon/2^i.$ If $\bigcup_{i\in I}B_{r_i}(q_i)=\mathbb{R},$ then the Lebesgue measure says: $$
\mu(\mathbb{R})\leq \sum_{i\in I} \mu(B_{r_i}(q_i))=\varepsilon.
$$ Well, $\mathbb{R}$ doesn't have arbitrarily small measure, so the collection of balls must not be a cover for $\mathbb{R}.$ Question: Is there a way to see that this does not cover $\mathbb{R}?$ Moreover, is there a criteria for $r_i$ so that one can see immediately whether $\{B_{r_i}(q_i)\}$ will cover? Current Thoughts: I asked my friend this question this morning and he suggested that a necessary and sufficient condition might be that $\liminf r_i>0,$ but I only see why this should be sufficient. And in particular, I am still confused by my questions posed above. Thank you for any and all help.","['real-numbers', 'measure-theory', 'lebesgue-measure', 'real-analysis']"
3402955,APMO 2007 problem 2 solution,"Let $ABC$ be an acute angled triangle with $\angle{BAC}$ = 60◦ and $AB$ > $AC$ . Let $I$ be the incenter, and $H$ the orthocenter of the triangle $ABC$ . Prove that $2\angle{AHI}$ = $3\angle{ABC}$ . I let $D,E,F$ be altitude on $AB,BC,AC$ respectively and $HI$ meet $AB$ at $X$ . I observed that $\angle{AHD}$ = $\angle{ABC}$ . So I assume that the equality hold if $\angle{DHX}$ =1/2 $\angle{AHD}$ and to prove that is true I draw $HZ$ meet $AB$ at $Z$ so that $\triangle{AHZ}$ is isosceles triangle. But the problem is I don't know how to prove that $\angle{DHX}=\angle{XHZ}$ . Please help","['triangles', 'circles', 'geometry']"
3402969,Solving First Order PDE with Characteristics Method,"Let the first order PDE: $$u_t+au_x=b-cu$$ I tried Characterestics method: $\frac{dt}{ds}=1$ and $\frac{dx}{ds}=a$ and $\frac{du}{ds}=b-cu$ Then we have if we suppose that $t(0)=0$ , then we'll have $t=s$ and $x(t)=a.t+x(0)$ But I could not solve the last ode.","['ordinary-differential-equations', 'partial-differential-equations']"
3402995,Finding integral of modified Weibull density,"For two constants, $a,b$ I am interested in the following integral: $$\int_0^{b-a} \frac{k}{\lambda} \bigg(\frac{y}{\lambda}\bigg)^{k-1}
 \exp \bigg[-\big(\frac{y}{\lambda}\big)^k - \big(\frac{(b-y)}{\theta}\big)^l \bigg] dy$$ The first part of the integral up to the second term in the exponent is equivalent to a Weibull density with easy to derive definite integral. I am not sure if it is possible to handle the second term in the exponent in closed form. Do you have any ideas how to approach this?","['integration', 'probability-distributions', 'calculus', 'probability-theory', 'probability']"
3403026,Is f continuous? lower semicontinuous? upper semicontinuous?,"I'm learning analysis on my own, and I'm having difficulties. Let $$\mu_{x_0}(E)=\begin{cases} 1 & x_0\in E \\ 0 & x_0\not\in E \end{cases}$$ and $V= B_r(x_0)=\{x|d(x,x_0)<r\}$ . Is $f(x)=\mu(V+x)$ continuous? lower semicontinuous? upper semicontinuous? My attempt: So $V+x=B_r(x_0+x)=\{x|d(x,x_0+x)<r\},$ $d(x,x_0+x) \leq d(x,x)+d(x_0,x) < r$ so $x_0 \not\in V+x,$ so $\mu(V+x)=0=f(x)$ How do we proceed?","['measure-theory', 'real-analysis']"
3403041,Find the least Positive Integer Satisfying $1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n} \ge 4$ [duplicate],This question already has answers here : Harmonic series sum approximation (4 answers) Harmonic number inequality (3 answers) Closed 4 years ago . Find the least Positive Integer Satisfying $1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n} \ge 4$ My try: I actually applied a Brute Force taking: $$\int \frac{dx}{x} \ge 4$$ Then we get: $$\ln x \ge 4$$ Hence $$x \ge e^4$$ So approximately $n=55$ Is there any better approach?,"['integration', 'algebra-precalculus', 'inequality', 'sequences-and-series']"
3403050,Convergence of the expected value of the maximum,"Let $\mathbf{X}^m = (X_1^m, ..., X_n^m)$ denote a vector of $n$ i.i.d. and bounded random variables. Suppose that the sequence $ (X_1^1, ..., X_n^1),(X_1^2, ..., X_n^2), (X_1^3, ..., X_n^3) ...$ converges in distribution to the random vector $(X, ..., X) $ as $m \rightarrow \infty$ (i.e. every random variable $X_i^m$ converges in distribution to the random variable $X$ ). Finally, consider a sequence of bounded functions $ f^1, f^2, f^3, ...$ which converge uniformly to the function $f$ . Under these assumptions, it seems clear that $$\mathbb{E}[\max(f^m(X_1^m), ..., f^m(X_n^m))] \rightarrow \mathbb{E}[\max(f(X), ..., f(X))]$$ as $m \rightarrow \infty$ . But how can one show this rigorously (e.g. by appealing to established results)? My thoughts so far: Since $X_i^m \rightarrow_d X$ and $f^m \rightarrow f$ uniformly, it seems plausible that $f^m(X_i^m) \rightarrow_d f(X)$ . By the continuous mapping theorem, this would (?) then ensure that $\text{max}\{f^m(X_1^m), ..., f^m(X_n^m)\} \rightarrow_d
   \text{max}(f(X), ..., f(X))$ . Finally, use something like the bounded convergence theorem to ensure
the convergence of the expected value? Many thanks in advance for any ideas or pointers!","['expected-value', 'convergence-divergence', 'probability-theory', 'real-analysis']"
3403053,2-D random walk bounding probability of number of return times to origin within a given time interval,"Let $(S_n)_{n\geq 0}$ be a simple random walk on $\mathbb Z^2$ . It is well known that the probability of the walk returning to $0$ at time $2n$ is given by \begin{equation}\tag{1}\label{eq1}
p_0(2n)  = \left(\frac{1}{2}\right)^{4n} \:{2n\choose n}^{\!\!2}.
\end{equation} Now consider $\{a,b\}\subset \mathbb N$ , with $a<b$ , and define a new process $(R_n)_{n\geq 1}$ as $R_n:=|\{S_m=0:an\leq m\leq bn\}|.$ That is: $R_n$ is the number of equalizations of the random walk between times $an$ and $bn$ . As part of a homework assignment I was interested in obtaining a uniform bound of $P(R_n>x)$ . I believe this can be done via combining Markov's inequality with equation \eqref{eq1}, and applying some asymptotics. We know \begin{align}\mathbb E[R_n]&=\sum_{k=na}^{nb}p_0(k)\sim\sum_{k=na}^{nb}\frac{1}{\pi n}\leq\log\frac{2b+1/n}{2a-1/n}\\&\leq\log\frac{2b+1}{2a-1},\end{align} which should suffice. However, this got me interested in the following What is known about the probability distribution of $R_n$ ? I have trawled the internet for a good few hours, and have not been able to find anything, so would appreciate any references.","['random-walk', 'probability-theory', 'markov-chains', 'reference-request']"
3403076,What are the applications of the Mean Value Theorem?,"I'm going through my first year of teaching AP Calculus.  One of the things I like to do is to impress upon my students why the topics I introduce are interesting and relevant to the big picture of understanding the nature of change. That being said, while I know that the Mean Value Theorem is one of the central facts in the study of calculus, I'm not really clear on why.  I feel that it's a bit like IVT for the derivatives of a continuous and differentiable function.  But I feel that the only thing I did with it when I studied Calc is to identify the point where the tangent was parallel to the secant of the endpoints.  If the class had been a little smaller or I had been a little bolder, I might have raised my hand and asked the professor ""So what?""  But I didn't, so here we are. To be clear, I am not at all arguing that MVT is not critical, so I don't plan on the answers being opinion-based.  But can you discuss some uses of MVT that justify the lofty place it has in the curriculum?","['motivation', 'calculus', 'soft-question', 'education']"
3403195,Showing a ring is not a division ring,"I'm working on a question that assumes a group with $|G| >1$ where $G$ is a finite group. I am to show that the ring $\mathbb{Z}_2(G)$ is not a division ring. I think, more than anything, I am still confused on the notation $\mathbb{Z}_2[G]$ . I know I am essentially working with polynomials where the coefficients come from $\mathbb{Z}_2$ . I also understand that I'm trying to show that there is a non-zero element that is not invertible, but I am confused on how to use this information in practice. Thanks in advance!","['ring-theory', 'group-theory', 'abstract-algebra']"
3403233,How to prove (if possible) $n^2 \in o(n!)$?,"I'd like to prove $n^2 \in o(n!)$ for $n \in \mathbb{N}$ . More specifically, this means $$
\exists c \in \mathbb{R}, n_0 \in \mathbb{N} \forall n \geq n_0: n^2 < c \cdot n!
$$ My ideas I've initially considered a proof by induction, using $c=1$ and $n_0 = 4$ . In the induction step (i.e. considering $n > n_0$ , I find that $$
n^2 + 2n + 1 < n! + 2n + 1
$$ However, I still have to conclusively show that $n! + 2n + 1 < (n+1) \cdot n!$ . This fact seems rather obvious, however I am unable to express it. Is there a better way to go about this task?","['asymptotics', 'discrete-mathematics']"
3403238,Show that the collection of subsets $A\subset\mathbb{Z}$ such that $A$ or $A^{c}$ is finite form an algebra but is not a $\sigma$-algebra,"I am reviewing probability theory for my final exams in 7-9 weeks, and I am reading Durrett in detail. Then, in measure theory part, Example 1.1.6, he provides an example showing an algebra is not necessarily a $\sigma$ -algebra, as follows Let $\Omega=\mathbb{Z}$ , and $\mathcal{A}$ = the collection of $A\subset\mathbb{Z}$ so that $A$ or $A^{c}$ is finite. A related post is here . Firstly, I wanted to show that $\mathcal{A}$ is an algebra, but I got stuck in the end. Let $A\in\mathcal{A}$ , then if $A$ is finite, $(A^{c})^{c}$ is finite which implies that $A^{c}\in\mathcal{A}$ , if $A^{c}$ is finite, then $A^{c}\in\mathcal{A}$ immediately. For $A,B\in \mathcal{A}$ , if $A, B$ finite, then $A\cup B$ is finite so $A\cup B\in\mathcal{A}$ , if $A^{c}$ and $B^{c}$ are finite, then $(A\cup B)^{c}=A^{c}\cap B^{c}$ is finite, so $(A\cup B)\in\mathcal{A}$ . But I don't know how to show the case if $A$ is finite but $B^{c}$ is finite (or vice versa). How could I express $A\cup B$ as something with $A$ and $B^{c}$ ? To show it is not a $\sigma$ -algebra, consider the collection $\{A_{k}\}_{k=1}^{\infty}$ , where $A_{k}:=\{2k\}$ , which is a countable collection of subsets in $\mathcal{A}$ , since each $A_{k}$ is finite. Then, $B:=\bigcup_{k=1}^{\infty}A_{k}$ is countably infinite, but $B^{c}$ includes all the odd numbers which is also infinite. Thus, $B\notin\mathcal{A}$ , and thus $\mathcal{A}$ is not closed under countable union. Is my proof right? Thank you!","['measure-theory', 'proof-verification']"
3403258,Alternative definition of support of a measure,"Is there any ""intrinsic definition"" of support of a measure which does not rely on any topological concept? We've been discussing this in our class, and what we do feel about the support of a measure is that it must be an element of the sigma-algebra which is maximal in the sense that it is the biggest set whose measure is positive. Is it possible to define an ""intrinsic definition"" of support of a measure function? Why do mathematicians rely it on a topological notion?",['measure-theory']
3403263,"if $(f(z))^{2} + (g(z))^{2} = 1$, $f, g$ analytic functions, then $f = cos(h)$ and $g = sin(h)$","If $f, g: \mathbb{C} \to \mathbb{C}$ are analytic functions that satisfy $(f(z))^{2} + (g(z))^{2} = 1$ for all $z \in \mathbb{C}$ , show that exist an analytic function $h: \mathbb{C} \to \mathbb{C}$ such that $f = cos(h)$ and $g = sin(h)$ . Someone can help me? Thank you in advance!",['complex-analysis']
3403269,Convex functions lack saddle points?,"I am reading ""Deep Learning"" by Ian Goodfellow. At page 86, the author explains how to use the Hessian to evaluate whether a point of a multivariate function is a maximum or a minimum At a critical point, where $ \nabla_x f(x)=0 $ , we can examine the
  eigenvalues of the Hessian to determine whether the critical point is
  a local maximum, local minimum or saddle point. When the Hessian is
  positive definite (all its eigenvalues are positive), the point is a
  local minimum. [...] Likewise when the Hessian is negative (all its
  eigenvalues are negative), the point is a local maximum. In multiple
  dimensions, it is actually possible to find positive evidence of
  saddle points in some cases. When at least one eigenvalue is positive
  and at least one eigenvalue is negative, we know that $x$ is a local
  maximum on one cross section of $f$ but a local minimum on another
  cross-section. [...] The test is inconclusive whenever all the nonzero
  eigenvalues have the same sign but at least one eigenvalue is zero.
  This is because the univariate second derivative test is inconclusive
  in the cross section corresponding to the zero eigenvalue So far so good. At page 89 it talks about convex optimization, and says that: Convex functions - functions for which the Hessian is positive
  semi-definite everywhere [..] are well-behaved because they lack
  saddle points But if the Hessian is positive-semidefinite, it means that some eigenvalues may be zero, while the others are positive. I thought that ""whenever all the nonzero
eigenvalues have the same sign but at least one eigenvalue is zero"" the test was inconclusive. So why does it says that they surely lack saddle points?","['convex-optimization', 'optimization', 'multivariable-calculus', 'hessian-matrix']"
3403301,"Prob. 9 (c), Sec. 2.3, in Herstein's TOPICS IN ALGEBRA, 2nd ed: Every group of order $5$ is abelian","Here is Prob. 9, Sec. 3.3, in the book Topics in Algebra by I.N. Herstein, 2nd edition: (a) If the group $G$ has three elements, show it must be abelian. (b) Do part (a) if $G$ has four elements. (c) Do part (a) if $G$ has five elements. I think I am clear on how to tackle Part (a). And, this is my Mathematics Stack Exchange post on Part (b). So here I will only be attempting Part (c). My Attempt: Suppose our group $G$ has five distinct elements, say, $e, a, b, c, d$ , with $e$ being the identity element. Now suppose, if possible, that $ab \neq ba$ . As the elements $e, a, b, c, d$ of $G$ are all distinct, so by virtue of the cancellation laws (i.e. Lemma 2.3.2 in Herstein), we cannot have $ab = a$ , $ab = b$ , $ba = a$ , or $ba = b$ . Therefore we must $ab, ba \in \{ e, c, d \}$ . Since $ab \neq ba$ by our supposition, therefore without loss of generality we have the following two cases: Case 1. Suppose that $ab = c$ [Or, we can also have $ab = d$ .] and $ba = e$ . Thus our group $G$ has the five distinct elements $e, a, b, ab, d$ . Since $ba = e$ , we also have $$ a = ae = a(ba). \tag{1} $$ However, since $G$ is a group, we must have $$ a(ba) = (ab)a. \tag{2} $$ From (1) and (2) above, we obtain $$ a = (ab)a, $$ from which we obtain $$ e = ab, $$ again by Lemma 2.3.2 in Herstein. This contradicts the fact that $e$ and $ab$ are two distinct elements of $G$ . So our supposition that $ab \neq ba$ is wrong. Therefore we must have $$ ab = ba. $$ Case 2. Suppose that $ab = c$ and $ba = d$ . [Or, vice versa.] Then our group $G$ has the five distinct elements $e, a, b, ab, ba$ . By Lemma 2.3.2 in Herstein, $a^2$ cannot equal $a$ (for otherwise $a$ would equal $e$ , which is contrary to our choice), nor can $a^2$ equal $ab$ or $ba$ (for otherwise $a$ would equal $b$ , again contrary to our choice). So we must have $a^2 = b$ or $a^2 = e$ . However, if $a^2 = b$ , then we obtain $$ ab = aa^2 = a^3 = a^2a = ba, $$ which is contrary to our supposition that $ab \neq ba$ . So we must have $a^2 = e$ , which implies that $a^{-1} = a$ . Similarly, we also have $b^2 = e$ , which implies that $b^{-1} = b$ . Therefore we have $$ (ab)^{-1} = b^{-1}a^{-1} = ba. \tag{3} $$ Now consider $(ab)a$ .  By the closure property of our group $G$ , $(ab)a$ must be one of the elements $e, a, b, ab, ba$ . However we show in the following paragraphs that each of these possibilities leads to a contradiction. We note that if $(ab)a = e$ , then $(ab)^{-1} =a$ , and thus from (3) above (and the uniqueness of the inverse of each element in a group) we obtain $ba = a$ , a contradiction. So $$ (ab)a \neq e. $$ If $(ab)a = a$ , then we obtain $ab = e$ , a contradiction. So $$ (ab)a \neq a. $$ If $(ab)a = b$ , then we obtain $$ ab = abe = aba^2 = [(ab)a]a = ba, $$ again a contradiction to our supposition that $ab \neq ba$ . So $$ (ab)a \neq b. $$ If $(ab)a = ab$ , then we obtain $a = e$ , a contradiction. So $$ (ab)a \neq ab.$$ Finally, if $$ (ab)a = ba, $$ then we obtain $$ a(ba) = ba, $$ which implies $$ a = e, $$ a contradiction. So $$ (ab)a \neq ba.$$ Thus we have shown that, in either of Cases 1 and 2 above, our supposition that $ab \neq ba$ has led to contradictions. So we must have $$ ab = ba. $$ An analogous argument yields $bc = cb$ , $bd = db$ , $ac = ca$ , $cd = dc$ . Thus any two of the elements $a, b, c, d$ of $G$ commute. And, the identity element $e$ of course commutes with itself as well as with each of $a$ , $b$ , $c$ , and $d$ . Hence our group $G$ is indeed abelian. Is this proof correct? If so, is it rigorous enough for Herstein? Or, are there problems and issues?","['proof-verification', 'finite-groups', 'group-theory', 'abstract-algebra', 'abelian-groups']"
3403312,Do the coefficients of these polynomials alternate in sign?,"Define polynomials $p_i(x)$ by the recurrence \begin{align}
p_0(x)&=0 \\
p_1(x)&=1 \\
p_{2i}(x)&=p_{2i-1}(x)-p_{2i-2}(x) \\
p_{2i+1}(x)&=xp_{2i}(x)-p_{2i-1}(x) \\
\end{align} The first few are given by \begin{align}
p_0(x)&=0\\
p_1(x)&=1\\
p_2(x)&=1\\
p_3(x)&=x-1\\
p_4(x)&=x-2\\
p_5(x)&=x^2-3x+1\\
p_6(x)&=x^2-4x+3\\
p_7(x)&=x^3-5x^2+6x-1\\
p_8(x)&=x^3-6x^2+10x-4
\end{align} It is reasonable to ask whether the coefficients of these polynomials alternate in sign. Any thoughts?","['combinatorics', 'polynomials']"
3403318,Minimum number of $k$-partitions of a set of size $n$ to enumerate all $n \choose k$ combinations,"Given a set $\mathcal{S}$ of size $n$ , let a $k$ -partition of $\mathcal{S}$ be a grouping into $k$ disjoint classes $$(S_1 ,S_2,...,S_k)$$ Where the $S_i$ do not necessarily contain the same number of elements. Let $C(S_1, S_2, ... S_k)$ be the set of combinations which can be made by taking exactly one element of each class, (i.e. $C(S_1, S_2, ... S_k)$ is the Cartesian Product $S_1 \times S_2 \times ...\times S_k$ ). For a given $(n,k)$ , what is the minimum number of k-partitions $N$ such that $\bigcup\limits_{i=1}^{N} C(S_1^{(i)}, S_2^{(i)}, ...,S_3^{(i)}) = $ All Possible Combinations Example: for $n=4$ and $k=2$ , $\mathcal{S} = \{1,2,3,4\}$ we can choose: $$ S^{(1)} = \left\{\left\{ 1, 2\right\}, \left\{ 3, 4\right\}\right\}$$ $$ S^{(2)} = \left\{\left\{ 1, 3\right\}, \left\{ 2, 4\right\}\right\}$$ Then $$C_{1} = \left\{\left\{ 1, 3\right\} ,\left\{ 1, 4\right\},\left\{ 2, 3\right\},\left\{ 2, 4\right\}\right\}$$ $$C_{2} = \left\{\left\{ 1, 2\right\} ,\left\{ 1, 4\right\},\left\{ 2, 3\right\},\left\{ 3, 4\right\}\right\}$$ and $C_1 \cup C_2 =$ All possible combinations and $N=2$ .","['set-partition', 'combinatorics', 'discrete-mathematics']"
3403323,When to connect points on a graph?,"Say I have a graph that has a set of points on it. The points represent some data. When should I connect the points with a straight line, when should I connect the points with a curved approximation, and when should I not connect the points at all? Does, in the end, choosing whether to connect the points and how to connect the points come down to some personal belief based on whatever feels reasonable in a certain situation? Or perhaps there are some ""official"" rules to follow that must not be violated, and violating such rules would disqualify the graph from being correct?","['data-analysis', 'statistics', 'graphing-functions']"
3403386,Deriving the Pollaczek - Khintchine formula,"I used this result in a paper a while back and now I want to be clear about where it comes from (I know it's correct). I wonder if someone could explain? I have a moment generating function for an M/G/1 queue (ie Markovian, General, 1 server) of: $\frac{1-\rho}{1-\rho h^*(s)}$ Where $\rho$ is the traffic intensity ( $\rho=\lambda\mu$ where $\lambda$ is the poisson parameter - ie rate of arrivals and $\mu$ is the average service time) While $h^*(s)=\frac{1-g^*(s)}{s\mu}$ - and $g^*(s)$ is a Laplace transform (into the domain of s) of a distribution function $g(x)$ for service times. Now - so far, so standard - differentiating this and taking the negative of its value at $s=0$ should give us the expectation of waiting time in the queue. The standard result for this differentiation (and negation) is $\frac{\rho\mu(1+c^2)}{2(1-\rho)}$ , where $c$ is ""the co-efficient of variation"", namely the ratio of the standard deviation to the mean for service times. Now this differentiation is laborious and I don't think I am getting it right - and I suspect it involves knowledge of what property is revealed by $\frac{dg^*(s)}{ds}$ - which I am not clear about. I am happy to read a source if that's easier than an explanation but I would appreciate some pointers about how we get from $\frac{dg^*(s)}{ds}$ to $c$ .","['queueing-theory', 'stochastic-processes', 'statistics', 'probability']"
3403441,"Are min$(X_1,\ldots,X_n)$ and min$(X_1Y_1,\ldots,X_nY_n)$ independent for $n$ to infinity?","This is a question that I posted on stats.stackexchange.com but since I received no satisfying answer but still the question was upvoted by many, I want to use the oppurtunity to further extend the question and hopefully address a larger audience; The original question can be found here: https://stats.stackexchange.com/questions/432396/are-minx-1-ldots-x-n-and-minx-1y-1-ldots-x-ny-n-independent-for-n-to Assume that we have given two continuous iid random variables $X$ and $Y$ with support $[1,c)$ , where $c$ is some constant greater than one. (The exact value is probably unimportant anyway)
Now assume I have a given iid sample $X_1, \ldots,X_n$ and $Y_1, \ldots,Y_n$ (so absolutely no dependence here). Imagine that I know that: $$(1): \mathbb P \left(\frac{\min(X_1,\ldots,X_n)-a_n}{b_n}\leq x_1\right) \sim F(x_1), \text{ for }n \to \infty,$$ where $F(x_1)$ is some non-degenerate cdf; Given some weak condition, it is usually quite easy to derive sequences $a_n$ , $b_n$ and the limit distribution $F$ , since it is very much connected to Extreme Value theory; Moreover, I know $$(2):\mathbb P \left(\frac{\min(X_1Y_1,\ldots,X_nY_n)-\bar a_n}{\bar b_n}\leq x_2\right) \sim G(x_2), \text{ for }n \to \infty,$$ Is it true that then it also follows that $$(3):\mathbb P \left(\frac{\min(X_1,\ldots,X_n)-a_n}{b_n}\leq x_1,\frac{\min(X_1Y_1,\ldots,X_nY_n)-\bar a_n}{\bar b_n}\leq x_2\right) \sim F(x_1) G(x_2),$$ for $n$ to infinity? At first I thought this cannot work since $X$ and $XY$ are obviously absolutely dependent; But then I thought the following: The probability that the minimum of $X_1,\ldots,X_n$ and the minimum $X_1Y_1,\ldots,X_nY_n$ is obtained in the same realization converges to zero for n to infinity Since the sample itself is iid, the minima should be kinda independent; So I don't know if this is true; Unfortunately, I cannot think of a counterexample and I also have no idea how to prove it; The only thing I thought of to prove 1. is: The probability, that the minimum is obtained in the same realization is given by \begin{align*}
&\sum_{i=1}^n\mathbb P\big(X_i=\min(X_1,\ldots,X_n), X_iY_i=\min(X_1Y_1,\ldots,X_nY_n)\big) \\ \leq &\sum_{i=1}^n\mathbb P\big(X_i=\min(X_1,\ldots,X_n), Y_i \leq \min(X_1Y_1,\ldots,X_nY_n)\big) \\
=  &n \cdot \mathbb P\big(X_i=\min(X_1,\ldots,X_n)\big) \mathbb P\big(  Y_i \leq \min(X_1Y_1,\ldots,X_nY_n) \vert X_i=\min(X_1,\ldots,X_n) \big) \\
= &n \cdot 1/n \mathbb P\big(  Y_i \leq \min(X_1Y_1,\ldots,X_nY_n) \vert X_i=\min(X_1,\ldots,X_n) \big)
\end{align*} where the latter probability converges to zero, since $\min(X_1Y_1,\ldots,X_nY_n)$ gets arbitrarily close to 1 for $n \to \infty$ (and the condition does not seem to change that). Therefore, the probability, that the minimum is realized in the same observation is something like $n \cdot 1/n \cdot o(1)=o(1)$ , so converges to zero... Now this is obviously not a rigorous proof; So is there anyone smarter or more knowledgeable than me with an idea of a proof or a counterexample why my idea is wrong? Since antkam was commenting something, I want to give a brief look into Extreme Value Theory: Most people probably know the central limit theorem: $$\frac{S_n-n\mu}{\sqrt n \sigma} \xrightarrow[]{D}N(0,1) $$ So something similar, we can get for maxima which is based on Extreme Value Theory; It is known that $$\frac{\vee X-a_n}{b_n}$$ can only converge to one of the three extreme value distributions (or to some constant), where $\vee X= \max(X_1,\ldots,X_n)$ . So for minima, we can use this and also find limit distributions and those sequences, such that $$\frac{\land X-a_n}{b_n}$$ converges to some distribution; Now what you, antkam, probably wanted to say: If $\frac{\land XY−\bar a_n}{\bar b_n}\leq x_2$ , then it holds that $\land XY \leq x_2 \bar b_n+\bar a_n$ and therefore in particular it holds that: $\land X \leq x_2 \bar b_n+\bar a_n$ (Since $Y \geq 1$ ) Now $\frac{\land X− a_n}{ b_n}\leq x_1$ is equivalent to $\land X \leq x_1 b_n + a_n$ . So if $x_2 \bar b_n+\bar a_n \leq x_1 b_n+ a_n$ , then $\frac{\land XY−\bar a_n}{\bar b_n}\leq x_2$ already implies $\frac{\land X− a_n}{ b_n}\leq x_1$ ; That was by the way also a way, someone wanted to prove that this in incorrect;  Therefore, if we solve it for $x_1$ we get: $$ x_1  \geq\frac{ x_2 \bar b_n+\bar a_n- a_n}{b_n}$$ So if $x_1$ is greater than the term, then it certainly is incorrect; The problem is, that we do not know these sequences and therefore, the right term could (and probably will) go to infinity and then it does not work, obviously. So if you want to prove it like that, there are basically just 2 ways I can think of: You take some distribution for $X$ and $Y$ , calculate the corresponding sequences and show that the right term indeed does not go to infinity; You take some other sequences $a_n$ , $b_n$ , $\bar a_n$ , $\bar b_n$ (I never said that it only works for the sequences such that we can get some nice limit distribution) but then the limit of $\frac{\land X-a_n}{b_n}$ would somewhat converge in probability only to a fixed number, or it goes to infinity or -infinity or something like that; So the cdf of $\frac{\land X-a_n}{b_n}$ or also $\frac{\land XY-\bar a_n}{\bar b_n}$ would only have values 0 and 1... Hope this helps some people to understand the problem a bit better... :-) @ Sangchul Lee: Thank you very much, for your answer; This is actually very interesting, because given the uniform distribution,then $X-1$ and $\ln(X)$ is regularly varying at zero with exponent $\alpha=1$ ; This is equivalent to $1/(X-1)$ or $1/\ln(X)$ being regularly varying at infinity with exponent $\alpha=-1$ ; Using well-known results, we can show that $\frac{\lor (1/\ln(X))}{b_n}$ then converges to a Frechet distribution with exponent $\alpha$ and by the close connection we know that $$\mathbb P\left(\frac{\land \ln(X)}{b_n}\leq x\right)$$ or also $$\mathbb P\left(\frac{\land X-1}{b_n}\leq x\right)$$ converges to $(1-\phi_{\alpha}(1/x))$ and since $XY-1$ is regularly varying with exponent $2\alpha$ , you get this convergence with $e^{-t^2}$ for the tail; Which brings me back to your idea; I suspect that it might be possible to prove it more generally for all regularly varying random variables, but I need to think about it tomorrow... But it is definitely a very interesting and smart answer, thank you so much for it... :-) Okay, I give things a try, although I did not manage to use the regular variation and I have no idea how I could use it; Also, I am not sure if this is a proper proof; The only thing I am going to use is that $\mathbb P(Y \in [1,a])\to 0$ for $a \downarrow 1$ and $\mathbb P(X \in [1,a])\to 0$ for $a \downarrow 1$ . (Does this follow from the continuity or can we construct some weird distribution that is continous but still does not satisfy this?) Anyway: We generally assume $X$ and $Y$ to be independent but not necessarily identically distributed; We know that $$\lim\limits_{n \to \infty} \mathbb P \left(\frac{\land X-1}{b_n}> x_1 \right) = \exp(-x_1^{\alpha_1})$$ and $$\lim\limits_{n \to \infty} \mathbb P \left(\frac{\land XY-1}{\bar b_n}> x_2 \right) = \exp(-x_2^{\alpha_2})$$ So these are our assumptions; Then it follows that: $$\lim\limits_{n \to \infty} \mathbb P \left(\frac{\land X-1}{b_n}> x_1 \right) =\lim\limits_{n \to \infty} \mathbb P \left(\frac{ X-1}{b_n}> x_1 \right)^n= \exp(-x_1^{\alpha_1})$$ and therefore, in particular: $$\lim\limits_{n \to \infty} \mathbb P \left(\frac{ X-1}{b_n}> x_1 \right)=\left(1-\frac{x_1^{\alpha_1}}{n}\right)$$ respectively $$\lim\limits_{n \to \infty} \mathbb P \left(\frac{ X-1}{b_n}\leq x_1 \right)=\frac{x_1^{\alpha_1}}{n}$$ and also it holds that $$\lim\limits_{n \to \infty} \mathbb P \left(\frac{ XY-1}{\bar b_n}> x_2 \right)=\left(1-\frac{x_2^{\alpha_2}}{n}\right)$$ respectively $$\lim\limits_{n \to \infty} \mathbb P \left(\frac{ XY-1}{\bar b_n}\leq x_2 \right)=\frac{x_2^{\alpha_2}}{n}$$ Now we have: $$\lim\limits_{n \to \infty}\mathbb P \left( \frac{\land X-1}{ b_n}> x_1 \right) \mathbb P \left(\frac{\land XY-1}{\bar b_n}> x_2 \right)=\exp(-x_1^{\alpha_1})\exp(-x_2^{\alpha_2})=\exp(-x_1^{\alpha_1}-x_2^{\alpha_2})$$ and $$\lim\limits_{n \to \infty}\mathbb P \left(\frac{ X-1}{ b_n}> x_1,\frac{ XY-1}{\bar b_n}> x_2 \right)\\
=\lim\limits_{n \to \infty}\left(1- \mathbb P \left(\frac{ X-1}{ b_n}\leq x_1\right)-\mathbb P \left(\frac{ XY-1}{\bar b_n}\leq x_2\right)+\mathbb P \left(\frac{ X-1}{ b_n}\leq x_1,\frac{ XY-1}{\bar b_n}\leq x_2\right)\right)\\
=\lim\limits_{n \to \infty}\left(1-\frac{x_1^{\alpha_1}}{n}-\frac{x_2^{\alpha_2}}{n} +\mathbb P \left(\frac{ X-1}{ b_n}\leq x_1,\frac{ XY-1}{\bar b_n}\leq x_2\right)\right)$$ Moreover, we have $$\mathbb P \left(\frac{ X-1}{ b_n}\leq x_1,\frac{ XY-1}{\bar b_n}\leq x_2\right) = \mathbb P \left(\frac{ X-1}{ b_n}\leq x_1\right) \mathbb P \left(\frac{ XY-1}{\bar b_n}\leq x_2 \bigg \vert \frac{ X-1}{ b_n}\leq x_1\right) \\ 
\leq \frac{x_1^{\alpha_1}}{n} \mathbb P \left(\frac{Y-1}{\bar b_n}\leq x_2 \bigg \vert \frac{ X-1}{ b_n}\leq x_1\right)=\frac{x_1^{\alpha_1}}{n} \mathbb P \left(\frac{Y-1}{\bar b_n}\leq x_2\right)=\frac{x_1^{\alpha_1}}{n} o(1), $$ since $\bar b_{n} \to 0$ for $n \to \infty$ . Therefore we can see that $$\lim\limits_{n \to \infty}\mathbb P \left(\frac{\land X-1}{ b_n}>x_1,\frac{ XY-1}{\bar b_n}> x_2 \right)\\
=\lim\limits_{n \to \infty}\left(1-\frac{x_1^{\alpha_1}}{n}(1-o(1))-\frac{x_2^{\alpha_2}}{n} \right)^n = \exp\left(-x_1^{\alpha_1}-x_2^{\alpha_2}\right)$$ Now I am curious: Is this a valid proof?","['statistics', 'probability', 'extreme-value-analysis']"
3403451,"If $\omega$ is a volume form, then $X\mapsto i_X\omega$ generates all $(n-1)$-forms","I've read the following claim in a textbook (I don't think the context is relevant): Let $M$ be a compact smooth manifold. If $\omega\in\Omega^n(M)$ is a volume form, then every $(n-1)$ -form $\eta\in\Omega^{n-1}(M)$ is given by $\eta=i_X\omega$ for some vector field $X\in\Gamma(TM)$ . In order to convince myself, I tested the statement for $n=3$ , only locally (i.e., in coordinate neighbourhoods), but couldn't do it generally. The author, rather vaguely, says that $X\mapsto i_X\omega$ is an ""isomorphism"", but I don't know what this is supposed to mean here. Any ideas?","['differential-topology', 'exterior-algebra', 'smooth-manifolds', 'differential-geometry']"
3403526,Diophantine equations with cubic roots.,"Find all positive, integral $x, y$ such that $$x^3-y^3=xy+61$$ Little work: $$(x-y)(x^2+xy+y^2)-xy=61$$ Also $x^3-xy-y^3$ seems to to be unable to be ""completed by the square,"" or factored, so I'm stuck. Thanks!","['number-theory', 'elementary-number-theory']"
3403528,Can $\tanh(\arctan(x))$ be simplified?,"Can $\tanh(\arctan(x))$ be simplified or re-expressed in terms of algebraic functions (or maybe a log here and there), kind of like the forward-inverse identities ? (perhaps using the myriad of trig and hyperbolic formulas) The only thing I found is the Gudermannian function , which in some sense yields the opposite of what I need $$gd(x) = 2 \arctan\left(\tanh\left(\frac{x}{2}\right)\right)$$ Also, when I let Mathematica spit out the MacLaurin series, it seems that $\tan(\arctan(x))$ and $\tanh(\arctan(x))$ have similar expansions (differing only by the usual alternating sign). Is there a place where these kind of relationships between the trigonometric and hyperbolic functions are studied? Anything known about the properties of the coefficients of such series?","['trigonometry', 'hyperbolic-functions']"
3403537,Diophantine equation with quartic polynomial,"What are all integral solutions to $$y^2=x^4+x^3+x^2+x+1$$ The RHS could become $$(x^2-x+1)(x^2+x+1)+x(x^2+1)$$ or $$\frac{x^5-1}{x-1}$$ I have no idea how to manipulate the equation into something useful or what the first step should be.
Also, A quartic diophantine equation looks useful, but none of the answers completely solve the question? Thanks!",['number-theory']
3403579,Prove that $\angle ABC <$ $ 84^\circ$.,"Angles A, B and C meet the circle at P, Q and R respectively. I made the picture below, but what would the solution be?","['contest-math', 'euclidean-geometry', 'geometry']"
3403594,Is there a monotone bijection between the rationals of two intervals?,"Given two nontrivial intervals $I$ and $J$ (both open or both closed), do there always exist a monotone bijection between $I\cap \mathbb{Q}$ and $J\cap \mathbb{Q}$ ? If the endpoints of $I$ and $J$ are rational numbers, then such a bijection is easy to find (just take the linear function that sends the endpoints of $I$ to those of $J$ ). But in general, it's not clear what to do.","['elementary-set-theory', 'rational-numbers']"
3403611,Proving that a function is injective and strictly increasing,"I have recently started doing proofs, and I find it quite hard to construct one. I am not really clear on how proofs should generally look like. What information should it include? Are we always need to suppose something is true and then arrive at the conclusion that our assumption along with the reasonings we provided imply something else (what's asked from us to be proved?) More specifically, questions such as these two are perfect examples of questions that I am not sure what I am supposed to show in a format of a proof. Let I ⊆ ℝ be an interval. We say that a function f: I → ℝ is strictly increasing if whenever $a,b ∈ I$ satisfy $a < b$ , then $f(a) < f(b)$ . Show that a strictly increasing function is injective. That's what I have done: Given that a function is increasing for some a,b ∈ R, assume a≠b. Since the function is strictly increasing, then $a < b$ , which would also imply that $f(a)<f(b)$ . Therefore, this shows that $f(a)≠f(b)$ and that the function is injective as needed to be shown. Suppose f: I → ℝ is strictly increasing if whenever $a,b ∈ I$ satisfy $a < b$ , then $f(a) < f(b)$ . Show that $f^{-1}$ is also strictly increasing. For this question, should I apply the same reasoning as that above, and add that taking the inverse of that function wouldn't change the fact that $a < b$ ? Would that be sufficient?","['proof-explanation', 'calculus', 'functions', 'proof-writing']"
3403623,"If $f(x,y)/g(x,y)$ is increasing in $x$ for all $y$, will $\int f(x,y)d\mu(y)/\int g(x,y)d\mu(y)$ be also increasing?","You can assume that the functions and the measure $\mu$ are strictly positive and the integrals are finite. I am asking because I found this paper that establishes a result that is rather similar. However, I am not sure if the monotonicity I am looking for is obvious, or if it is a special case of their result. Thank you","['integration', 'measure-theory', 'monotone-functions']"
3403625,$n$ people at a party and friendship,"There are $n\geq 2$ people at a party. Each person has at least one friend inside the party. Show that it is possible to choose a group of no more than $\frac{n}{2}$ people at the party, such that any other person outside the group has a friend inside it. This is just the definition of Dominating sets! I think I could use the  marriage theorem or Hall's theorem... the solution I know (without using it) involves dividing into spanning trees and coloring with one of two colors.","['graph-theory', 'arithmetic', 'combinatorics']"
3403633,need feedback on some simple discrete math proofs,"I have a couple of proofs that I am not sure if I have solved correctly and was wondering if someone could give me some feedback. You guys are the best! Here are the proofs: Using rules of inference, determine whether the argument being made is correct or incorrect. Provide a sequence of premises and conclusions justifying your answer. Be sure to state all of your premises and domains. 1.) All calicos like milk. My cat, Mr Boots, is not a calico. Therefore, Mr Boots does not like milk. C(x) = x is a calico M(x) = x likes milk 1. ∀x(C(x) ∧ M(x))

2. ∃x(¬C(x))

3. C(x) ∧ M(x) universal instantiation

4. ¬C(x) ∧ ¬M(x) inverse law 3

5. ¬M(x) ∧ ¬C(x) commutative law

6. ¬M(x) simplification 5

7. ∃x(¬M(x)) Existential Generalization 2.) All tennis players are fast. John is a tennis player. Therefore, John is fast. P(x) = ”x is a tennis player” F(x) = ”x is fast” 1. ∀x(P(x) ∧ F(x))

2. ∃x(P(x))

3. P(x) ∧ F(x) Universal Instantiation 1

4. F(x) ∧ P(x) commutative law

5. F(x) simplification 4

6. ∃x(F(x)) Existential Generalization","['proof-explanation', 'proof-writing', 'discrete-mathematics']"
3403636,Conditional Markov Inequality using Conditional Measures,"Setup Let $(\Omega, \mathcal{A}, \mu)$ be a measure space and $\{A_{n}: \mu(A_{n})\lt\infty\}_{n\geqslant 1} \subset \mathcal{A} ~~$ s.t. $A_{n} \uparrow \Omega$ . With $d(\cdot,\cdot)$ being a metric (i.e. distance function), let $$
\widetilde{d}_{N}(f,f_{n}) = \int_{A_{N}} (1\wedge d(f(\omega),f_{n}(\omega)) \mu(d\omega)
$$ Assume $\widetilde{d}_{N}(f,f_{n})\overset{n\rightarrow\infty}{\longrightarrow} 0$ . Let $B\in\mathcal{A}$ with $\mu(B)\lt\infty$ . Fix $\delta\gt 0$ and choose $N\in\mathbb{N}$ large enough that $\mu(B\setminus A_{N})\lt \delta$ . Then, for $\epsilon \in (0,1)$ $$
\begin{align}
\mu(B~\cap~\{d(f,f_{n})\ > \epsilon\}) \leq&~ \delta + \mu(A_{N}\cap\{d(f,f_{n})\ > \epsilon)\\
\leq&~ \delta + \epsilon^{-1}\widetilde{d}_{N}(f,f_{n})
\end{align}
$$ Question How does one show $$
\mu(A_{N}\cap\{d(f,f_{n}) > \epsilon \}) \leq \epsilon^{-1}\widetilde{d}_{N}(f,f_{n})
$$ Also any intuition behind what is going on would be welcome! References: Theorem 6.7 from A. Klenke, Probability Theory , Universitext, DOI 10.1007/978-1-4471-5361, © Springer-Verlag London 2014","['self-learning', 'measure-theory', 'lebesgue-integral', 'probability-theory']"
3403641,A question about the norm of an element in a field extension.,"Background: Since $x^3\not\equiv2\pmod{7}, \forall x\in\mathbb{Z}$ , we can let $K=\mathbb{F_7}[\sqrt[3]{2}]$ so that $K$ is an extension of $\mathbb{F_7}$ , which can be think of as a finite-dimensional vector space over $\mathbb{F_7}$ with a basis $(1,\sqrt[3]{2},\sqrt[3]{4})$ . Then the norm of an element $\alpha=x+y\sqrt[3]{2}+z\sqrt[3]{4}$ is defined to be $N(\alpha)=x^3+2y^3+4y^3-6xyz$ , which is the determinant of the linear transformation $f_\alpha:K \rightarrow K$ defined as left multiplication by $\alpha$ in $K$ . My question is that if I have an element $\beta\in K$ having a unit norm, i.e. $N(\beta)=1$ , can I then conclude that $N(\beta\alpha)=N(\beta)N(\alpha)=N(\alpha)$ ? I.e., is $N$ a homomorphism? My attempt Observe that $f_{\beta\alpha}=f_\beta\circ f_\alpha$ , then $$N(\beta\alpha)=\det(f_{\beta\alpha})=\det(f_\beta)\det(f_\alpha)=N(\beta)N(\alpha).$$ My motivation: Then $x^3+2y^3+4z^3-6xyz=2$ has infinitely many integral solutions since we can just find an $\alpha$ s.t. $N(\alpha)=2$ and some non-identity $\beta$ s.t. $N(\beta)=1 \Rightarrow \forall n\in\mathbb{N}, N(\beta^n\alpha)=2$ , i.e. we have infinitely many solutions to the give cubic form.","['field-theory', 'linear-algebra']"
3403708,divisibility of the order of Aut(G) of a p-group,"If you were given a p-group G of order $p^{n}$ , and you were interested in finding the possible divisors of |Aut(G)|, how would one go about doing this? For instance, could you prove that $p^{n}-1$ divides |Aut(G)|? And finally what if G was not necessarily a p-group, how would one generalize the divisibility of the order of its automorphism group?","['group-theory', 'abstract-algebra']"
3403715,Counting problem on permutation,"How many distinct permutations on { $1,2,3,4,5,6$ } have cycle structure $(abc)(def)$ . The answer is $40$ but how come? Firstly, I select 3 elements out of 6 by ${6\choose 3}$ , then considering each inter-group permutation by $2! * 2!$ . Really appreciate if anyone can enlighten me. There is a similar question by defining structure as $(ab)(cdef)$ , my answer is ${6\choose 2}*3!=90$ . I don't know if this is correct.","['combinatorics', 'discrete-mathematics']"
3403754,show this statement is a tautology using conditional statements,Have been doing some homework questions and got stuck on this question $$(p ∨ q) ∧ (p → r) ∧ (q → r) → r $$ I got it simplified to $$((¬p ∧ ¬q) ∨ ¬r ∧ (p ∨ q)) ∨ r$$ but now i'm not sure what to do now. Any help would be appreciated.,"['boolean-algebra', 'propositional-calculus', 'logic', 'discrete-mathematics']"
3403819,How to integrate$\int_0^1 \frac{\ln x}{x-1}dx$ without power series expansion,"I happen to watch the video here ,
which gives a solution to the definite integral below using the power series approach. Then answer is $\frac{\pi^2}{6}$ , given by: $$\int_0^1 \frac{\ln x}{x-1}dx=\int_{-1}^0 \frac{\ln(1+u)}{u}du=\sum_{n=0}^{\infty}\frac{1}{(n+1)^2}=\frac{\pi^2}{6},$$ where the power seires expansion of the function $\ln(1+u)$ is used. I tried for some time, but could not find another approach. Does anyone know any alternative methods to evaluate above definite integral without using the infinite series expansion? Any comments, or ideas, are really appreciated.","['integration', 'calculus', 'definite-integrals']"
3403914,"Evaluate $f(t -) := v \uparrow t \int _0^v x(s)F(ds)$ where $F$ is non-decreasing, non-negative and right-continuous?","Consider the following: we have a non-decreasing, non-negative and right-continuous function $F$ from which we get a unique measure $\mu$ from $\mu((a,b]):=F(b)-F(a)$ . Further assume that we have a function $x$ that is integrable w.r.t. $\mu$ on finite intervals. I would like to get the left-sided limits of the function $$f(t) :=\int _0^t x(s)F(ds)$$ that is evaluate $$f(t -) := v \uparrow t \int _0^v x(s)F(ds)$$ To do so we follow the usual route by considering $x$ to first be an indicator function, then a simple function, then a non-negative function and lastly writing it as a difference of its positive and negative parts. Thus let $x(s)=1_B(s)$ and assume $B=(t_0 -v, t_0+v)$ for some $v>0$ - this is to get at the heart of the problem but other cases of course exists. Take any sequence $h_n \uparrow 0$ and consider that $$\lim_{h_n \uparrow 0 }\int 1_{[0,t_0+h_n]} 1_B F(ds)= \lim_{h_n \uparrow 0 } \int 1_{(t_0-v, t_0+h_n ]}(s) F(ds)=\lim_{h_n \uparrow 0 } F(t_0 + h_n)-F(t_0-v)$$ Which equals $F(t_0 -)-F(t_0-v)$ For a simple function $x(s)=\sum_{j=1 } ^k \alpha_k 1_{A_k } (s)$ , with the same reasoning we get that the limit of the integral equals $$\sum_{j=1 } ^{k-1 } \alpha_j \mu(A_j) + \alpha_k(F(t_0 - ) - F(t_0 - v))$$ assuming $x$ equls $\alpha_k$ for some interval $(t_0-v,t_0+v)$ about $t_0$ . My question then is: what happens when we assume that $x$ is a non-negative function and we take a sequence of simple function converging pointwise to it? $$\lim_{h_n \uparrow 0 } \lim_k \int 1_{[0,t_0+h_n ]} \sum_{j=1 } ^k \alpha_j 1_{A_j } (s)F(ds) $$ May we change the two limits? If this cannot generally be written in closed form consider the jump that $x$ makes at $t_0$ instead - that is consider $x(t_0 +)-x(t_0 -)$ . Grateful for any  help!","['measure-theory', 'lebesgue-integral', 'real-analysis']"
3403945,Finding radius of convergence of two power series,"I have two series (1) $\sum \frac{x^n}{n^{\log n}}$ and (2) $\sum \frac{x^n}{n (\log n)^2}$ . I need to find the radius of convergence for both the cases. Attemp: (1) i know i have to find this limit i.e $$\lim_{n\to \infty}\left |\frac{a_{n+1}}{a_n} \right|=\lim_{n\to \infty}\frac{n^{\log n}}{(n+1)^{\log (n+1)}}=\lim_{n\to \infty}e^{\log^2 n-\log^2 (n+1)}\\=\lim_{n\to \infty} \left(1+ \mathcal{O}\left(\frac{1}{n}\right)\right) \to 1$$ So $R=1$ for this case. (2) $$L=\lim_{n\to \infty}\left |\frac{a_{n+1}}{a_n} \right|=\lim_{n\to \infty}\left(\frac{n}{n+1}\right) e^{\log \log^2 n-\log \log^2 (n+1)}$$ How do i further simplify this limit ? Is my attempt correct or did i mess something up ? any suggestions are welcome !! 
These are exercise problems from Serge Lang's Undergraduate analysis, Chapter IX, exercise 6.","['power-series', 'limits', 'sequences-and-series', 'real-analysis']"
3403969,Is there a general matrix to reflect about the line $y=mx+c$?,"I have been looking into matrix transformations and found the following matrix to reflect about the line $y=(\tan\theta)x$ . $$R = \begin{bmatrix}
      \cos(2\theta)&  \sin(2\theta)\\
      \sin(2\theta)& -\cos(2\theta)\\
      \end{bmatrix}$$ However, is there a general matrix to reflect about the line $y=mx+c$ ?","['matrices', 'reflection']"
3403992,Is it possible to cover an $11 \times 12$ rectangle with $19$ rectangles of $1 \times 6$ or $1 \times 7$?,"Is it possible to cover an $11 \times 12$ rectangle with $19$ rectangles of $1 \times 6$ or $1 \times 7$ ? Attempt: There should be $132$ unit squares to be covered. Since there are $19$ rectangles to be used, let $x$ be the number of $1 \times 6$ rectangles and $19-x$ be the number of $1 \times 7$ rectangles. The solution of $$ (19-x)7 + (x)6 = 132 $$ is $$ 133-7x + 6x = 132 \implies x = 1$$ So there should be only $1$ rectangle of $1 \times 6$ , and $18$ rectangles of $1 \times 7$ . Now color the $132$ unit squares black and white like a chessboard, the top left is black..then its right is white...then its right is black again.. and so on. Odd rows should be $$ [black]-[white]-[black]- ... -[black]-[white]-[black] $$ Even rows is 
should be $$ [white]-[black]-[white]- ... -[black]-[white]-[black] $$ For a $1 \times 6$ rectangle, it will definitely cover $3$ blacks and $3$ whites. For a $1 \times 7$ rectangle, it will either cover $4$ blacks and $3$ whites, or cover $3$ blacks and $4$ whites. Let the number of $4$ blacks- $3$ whites coverings be $y$ , and $18-y$ for the other one. Notice that in total we must have $66$ blacks and $66$ whites. So in total there will be $$ |black \: squares| = 3 + 4y + 3(18-y) = 57 + y \implies y = 9$$ $$ |white \: squares| = 3 + 3y + 4(18-y) = 75 - y \implies y = 9$$ So there should be $9$ rectangles that cover $4$ blacks- $3$ whites, and $9$ rectangles that cover $3$ blacks- $4$ whites.","['coloring', 'combinatorics', 'discrete-mathematics', 'recreational-mathematics', 'tiling']"
3403993,Does every $4$-dimensional lattice have a minimal system that's also a lattice basis?,"An full $n$ -dimensional lattice $\Lambda$ is a discrete subgroup of $\mathbb{R}^n$ (equipped with some norm $\lVert \cdot \rVert$ ) containing $n$ linearly independent points. If $\Lambda = \{ A z, z\in \mathbb{Z}^n\}$ for $A \in GL(n,\mathbb{R})$ , we call the $n$ columns of $A$ a basis of $\Lambda$ (every full lattice has a basis). We call $n$ points $l_1,\dots,l_n \in \Lambda$ a minimal system if for $k\in\{1,\dots,n\},$ $$\lVert l_k \rVert = \min \{ \lVert l \rVert: l \in \Lambda \setminus \left< l_1,\dots,l_{k-1}\right>_\mathbb{R}\}.$$ Let's just consider the standard $2$ -norm $\lVert x \rVert = \left< x,x\right>^\frac{1}{2}$ . In two dimensions, every minimal system is also a basis. In five dimensions, this is no longer true, as is stated in the answer to this post . The lattice generated by $$ A = \begin{pmatrix}
1 & 0 & 0 & 0 & \frac{1}{2} \\
0 & 1 & 0 & 0 & \frac{1}{2} \\
0 & 0 & 1 & 0 & \frac{1}{2} \\
0 & 0 & 0 & 1 & \frac{1}{2} \\
0 & 0 & 0 & 0 & \frac{1}{2} \\
\end{pmatrix}
$$ has the five standard unit vectors as a minimal system, but they do not form a basis of the lattice. In four dimensions, the lattice generated by the basis $$
A = \begin{pmatrix}
1 & 0 & 0 & \frac{1}{2} \\
0 & 1 & 0 & \frac{1}{2} \\
0 & 0 & 1 & \frac{1}{2} \\
0 & 0 & 0 & \frac{1}{2} \\
\end{pmatrix}
$$ has both $A$ and the standard basis as minimal systems. This is not a stable property: If the last vector is perturbed a little, the minimal system is unique (up to reflections) and a basis again. This has led to my feeling that this is an extremal case, and the situation is as follows: Every four-dimensional lattice has a minimal system that's also a lattice basis. Furthermore, only a zero set of matrices generate lattices that have minimal systems that are not bases. Is that true?","['integer-lattices', 'linear-algebra', 'diophantine-approximation']"
3403994,"Let $v,w \in R^n$ with $||u||=||w||=1$. Prove that there exists orthogonal matrix $A$ such that $Av=w$.","Let $v,w \in \mathbb R^n$ with $\|u\|=\|w\|=1$ . Prove that there exists
  orthogonal matrix $A$ such that $Av=w$ . Also prove that $A$ can be
  chosen such that $\det(A)=1$ Here, $u\neq 0$ and $w\neq 0$ so I can extend these to get $B_1 = \{u,x2,x3\dots x_n\}$ and $B_2=\{w,y2\dots y_n\}$ both orthonormal basis of $R^n$ and define $T:R^n \to R^n $ as $T(u)=w$ and $T(x_i)=y_i \;\; \forall i\leq i \leq n$ here will $A=[T]_{B_1}^{B2}$ satisfy the given conditions? I can't think of another way to solve this. please help","['matrices', 'orthogonal-matrices', 'matrix-equations', 'linear-algebra']"
3404074,Integrating $\arcsin \left(\frac{1}{2r} \left(\left(\lambda^2+(r+t)^2\right)^{\frac{1}{2}}-\left(\lambda^2+(r-t)^2\right)^{\frac{1}{2}}\right)\right)$,"In a mathematical physical problem, one has to deal with a non-trivial finite integral given by $$  
f(r,R,\lambda) = \int_0^R \arcsin \left( \frac{1}{2r} 
\left( \left( \lambda^2+(r+t)^2\right)^{\frac{1}{2}} 
-\left( \lambda^2+(r-t)^2\right)^{\frac{1}{2}}
\right) \right) \, \mathrm{d}t \, ,
$$ where $0<r<R$ and $\lambda>0$ are parameters. In particular, for $\lambda=0$ , it can easily be shown that $$
f(r,R,0) = \int_0^r \arcsin \left( \frac{t}{r} \right) \, \mathrm{d}t
+ \frac{\pi}{2} \int_r^R \mathrm{d} t
= -r+\frac{\pi R}{2} \, .
$$ Is there a way to deal with the above integral for arbitrary values of the parameter $\lambda$ ?
Your suggestions and inputs are most welcome. Thank you","['integration', 'complex-analysis', 'definite-integrals', 'real-analysis']"
3404089,Principle of Induction and Well-Ordering Principle,"My Analysis lecturer defined $\mathbb{N}$ as the smallest set such that $0 \in \mathbb{N}$ and if $n \in \mathbb{N}$ then $n+1 \in \mathbb{N}$ . He then proceeded to ""prove"" the Principle of Mathematical Induction as follows: Let $P(n)$ be a family of propositions indexed on $\mathbb{N}$ . Suppose that $P(0)$ is true and that for any $n \geq 0$ , if $P(n)$ is true then $P(n+1)$ is true. Let $S$ be the set of natural numbers such that $P(n)$ is true. Then $S$ is a subset of $N$ . Since $P(0)$ is true, $0 \in S$ , and if $n \in S$ then $P(n)$ is true, so $P(n+1)$ is true, or equivalently $n+1 \in S$ . Thus $S$ has the properties $0 \in S$ and if $n \in S$ then $n+1 \in S$ ; as $N$ is the smallest set with these properties, $N$ is a subset of $S$ , but since we already know $S$ is a subset of $N$ it follows that $S=N$ , i.e. $P(n)$ is true for all natural numbers. My question is whether this proof is actually valid. I can't see any obvious flaw in the logic, but at the same time, I always thought that either the Principle of Mathematical Induction or (equivalently) the Well-Ordering Principle had to be taken as an axiom and couldn't be proved itself.","['elementary-set-theory', 'induction']"
3404090,Show that $\sum _{k=0}^n \frac{k C_n^k\times!k}{n!}=n-1$,"I'm trying to solve this problem: $n$ people have $n$ different hats, put all the hats into a box, find the expected value of people that take the wrong hat. We have $n!$ cases, where the probability of $k$ people take the wrong hat (and other $n-k$ people take the correct hat) is $$\frac{!k\times C_n^k}{n!}$$ Where $!k=D_k$ is the derangement number. And the expected value is $$
\sum _{k=0}^n k\times\frac{C_n^k\times!k}{n!}
$$ Mathematica told me that the value of this expression is $n-1$ for $n=\{1,2,\ldots,100\}$ , how to prove it?","['expected-value', 'combinatorics', 'sequences-and-series']"
3404113,Which mappings preserve convex bodies?,"Let $$f:\mathbb{R}^n\to\mathbb{R}^n,$$ $n\geq 2$ , be a mapping which maps every convex body (compact convex set with nonempty interior) to a convex body. If we assume $f$ to be a homeomorphism, it needs to be affine. Is there something we can say without this assumption?","['analysis', 'real-analysis']"
