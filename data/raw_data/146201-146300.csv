question_id,title,body,tags
2393049,Definition of transitivity.,"Definition : A relation $\rm R$ in a set $\rm A$ called transitive, if $(a_1, a_2) \in \mathrm R$ and $(a_2, a_3) \in \mathrm R  \implies (a_1, a_3) \in \mathrm R \quad  \forall a_1, a_2, a_3 \in \mathrm A$ Problem : ( source ) Let $\mathrm A$ be finite set of human beings. Let $\mathrm R$ be a relation on the set $\mathrm A$ defined as $$\mathrm R = \{ (x,y) : \text{$x$ is wife of $y$}\}$$ Determine whether it is transitive or not. I would say it is not transitive because if $x$ is wife of $y$ then $y$ can't be wife of $z$ and certainly $x$ can't be wife of $z$ assuming no same sex marriage or extramarital affairs by the people of set $\mathrm A$. Here if we define $p : (x,y) \in \mathrm R \  \land \ (y,z) \in \mathrm R $   and $q : (x, z) \in \mathrm R$, Then clearly both $p,q$ is false here and so $p \implies q$ should be false. By the definition of transitivity ,$\text{if $(p \implies q)$ then transitive}$, the relation $\mathrm R$ is not transitive because $p \implies q$ is false. Here is the part I don't understand, in the source of this problem the answers suggest that the relation is transitive and it is so because $p \implies q$ is false, provided I understand them properly. I don't understand why if both $p,q$ is false then the relation is transitive and how does this follows from the definition of transitivity ?","['definition', 'relations', 'discrete-mathematics']"
2393061,Evaluate the limit inf of $\cos(n^4+n+1)$,"I want to calculate $\liminf\cos(n^4+n+1).$ In fact, I don't know how to proceed. I know that $\liminf\cos(n)$ is $-1$, but I have no idea if this will help me. So, I am asking if someone has an idea for the solution","['real-analysis', 'limsup-and-liminf', 'analysis']"
2393092,Find the value of an angle $x$,"$\odot O$ is the circumcircle of an isoceles triangle $\Delta ABC$. $AB=AC$, $\measuredangle BAC=20^0$; $BD$ is a bisects of $\angle ABC$ and intersects $AC$ at $D$. Find the value of $\measuredangle BDO \qquad$ or prove that $\measuredangle BDO=100^{\circ}$.","['circles', 'trigonometry', 'triangles', 'geometry']"
2393105,Taylor series of $\tan x - \tan (\sin x)$ has all coefficients positive. Why?,"It's well known that $x > \sin x$ for $x> 0$. The Taylor series of $ x - \sin x$ is also well known, and the coefficients are alternating. However, it appears that the Taylor coefficients of the function $\tan x - \tan (\sin x)$ are all positive ( and this implies  that $x > \sin x$ on $(0, \pi/2)$, as it should). It's not clear for me why this is true. In fact, one can go further as follows. It is known that we have inequalities of the form
$$\sum_{k=0}^{2 l} (-1)^k\frac{x^{2k+1}}{(2k+1)!}< \sin x < \sum_{k=0}^{2 m + 1}(-1)^k \frac{x^{2k+1}}{(2k+1)!}$$ for $x > 0$. Let's consider for instance the inequality 
$$x< \sin x + \frac{x^3}{6}$$ for $x>0$. Now, it appears again that the function $\tan ( \sin x + x^3/6) - \tan x$ has a ""positive"" Taylor expansion. Similarly for  $\tan ( x + \frac{x^5}{120}) - \tan( \sin x + x^3/6)$, and so on, for any inequality with positive coefficients obtained from the above by switching sides of terms. One can substitute the function $\sec$ for the function $\tan$. I am aware of the Taylor expansions of the functions $\tan$ and $\sec$ ( see the wikipedia article on trigonometric functions), they are all positive, and have a combinatorial interpretation. One can do some testing with WolframAlpha or any computer algebra system.",['real-analysis']
2393107,$\sum_{k=1}^{\infty}\frac{1}{k(k+1)^{\frac{1}{n}}}>n$,"I think the following question is true: For each positive integer $n\geq 2$, prove $$\sum_{k=1}^{\infty}\frac{1}{k(k+1)^{\frac{1}{n}}}>n$$ I try using by induction on $n$, but I think this is not easy with induction. Do you have any idea or comment for proving it? So thanks for any comment and help.","['combinatorics', 'real-analysis', 'calculus']"
2393130,"Given distances from a point to three vertices of a rectangle, find the distance to the fourth vertex","Problem: $ABCD$ is a rectangle. A point $P$ is $11$ from $A$ , $13$ from $B$ and $7$ from $C$ . What is the length of $DP=x?$ (Note: $P$ can be inside the rectangle or outside of it.) I drew this scenario as best as I could, but I only have triangles with two sides and no angles. How do I begin? Any weird and relatively unknown theorems I should use?","['rectangles', 'euclidean-geometry', 'geometry']"
2393139,How to find $n^{th}$ derivative of $e^{2x}\cos^2x \sin x$?,Given : $y=e^{2x} \times \cos^2x \times \sin x$ and I have to find it's $n^{th}$ derivative I have managed to break $y$ down to: $y= \frac{1}{4} \times (e^{2x}.\sin 3x + e^{2x}.\sin x)$ But I don't know how to apply Leibnitz Rule in cases like $(e^{ax} \times \sin bx)$. Please guide,"['derivatives', 'calculus']"
2393148,"If $(f_n)_n\rightarrow f$ almost everywhere and in measure, does $(f_n)_n\rightarrow f$ almost uniformly?","If $(f_n)_n\rightarrow f$ almost everywhere and in measure, does $(f_n)_n \rightarrow f$ almost uniformly? I've been wondering about this question and came with no result. I know that convergence almost everywhere does not imply convergence almost everywhere, but I am not aware of any counterexample if the extra condition of convergence in measure is added. The other implication is true. If $(f_n)_n \rightarrow f$ almost uniformly, then it is correct that it converges for $f$ both almost everywhere and in measure, which makes me think that the converse (the question) is not true. Thank you!","['almost-everywhere', 'examples-counterexamples', 'uniform-convergence', 'measure-theory', 'convergence-divergence']"
2393201,(Dis-)proving the series $\sum\limits_n\left( 1+ \frac{1}{n} \right)^n$ converges,I am trying to prove that the series: $$\sum^\infty_{n=1}\left( 1+ \frac{1}{n} \right)^n$$ converges. Now I know that $$\lim_{n\rightarrow\infty} \left( 1+ \frac{1}{n} \right)^n=e$$ But how can I use that knowledge to prove the convergance ? Intuitively I would say that the series diverges since it doesn't approach zero but how can I formally prove this?,"['self-learning', 'sequences-and-series', 'convergence-divergence']"
2393238,Expressing invertible maps $\bigwedge^{d-1} V \to \bigwedge^{d-1} V$ as $\bigwedge^{d-1}A$ for some $A$,"Let $V$ be a real $d$-dimensional vector space, let $\bigwedge^{d-1} V$ be its exterior power. Consider the following claim: Proposition: If $d$ is even, then every invertible linear map $\bigwedge^{d-1} V \to \bigwedge^{d-1} V$ equals $\bigwedge^{d-1}A$ for some $A \in \text{GL}(V)$. If $d$ is odd, then every orientation-preserving*  invertible map $\bigwedge^{d-1} V \to \bigwedge^{d-1} V$ equals $\bigwedge^kA$ for some $A \in \text{GL}(V)$. I found a proof for the above proposition, but it is based on endowing $V$ with an inner product, which I don't like very much.  Since there is no mention of products in the claim, it's natural to expect a metric-free proof. Is there such a proof? Edit: Here is an argument for showing that when $d$ is odd, it is impossible to express orientation-reversing maps $\bigwedge^{d-1} V \to \bigwedge^{d-1} V$ as ""$(d-1)$-wedge"" of a map $V \to V$. Let $A:V \to V$. Since $$\det (\bigwedge^k A)=(\det A)^{\binom{d-1}{k-1}},$$ we get for $k=d-1$ that
$$ \det (\bigwedge^{d-1} A)=(\det A)^{\binom{d-1}{d-2}}=(\det A)^{d-1},$$ so if $d$ is odd, we see that $\det (\bigwedge^{d-1} A)$ is always positive, whether or not $A$ was orientation-preserving to begin with. *Note there is no need for a choice of orientation on $\bigwedge^{d-1} V$ to define which maps $\bigwedge^{d-1} V \to \bigwedge^{d-1} V$ are orientation preserving. (If you like you can put the same orientation on ""both sides"", it does not matter which).","['differential-geometry', 'orientation', 'linear-algebra', 'exterior-algebra']"
2393243,Probability measures are representable through measurable functions?,"Let $f$ be a measurable function $f:D\to \mathbb C$, where $D$ is a measurable set in $\mathbb R^k$ with finite non-zero measure. We know that the linear positive operator $\Lambda:C_c(\mathbb C) \to \mathbb R$ given by
$$
\Lambda(G) = \frac{1}{|D|}\int_D G(f(x)) dx 
$$
is (uniquely) representable by a radon probability measure $\mu$ such that
$$
\Lambda(G) = \int_{\mathbb C} G d\mu 
$$
I wonder if the opposite is also true. That is Given a probability measure $\mu$ on $\mathbb C$, there exists a measurable function $f:[0,1]\to \mathbb C$ s.t.
  $$ \int_{\mathbb C} G d\mu  = \int_0^1 G(f(x)) dx$$
  for every $G\in C_c(\mathbb C)$? It's easy to build $f$ if $\mu$ is atomic, but I don't know if it is true for singular or continuous measures.","['probability-theory', 'measurable-functions', 'measure-theory']"
2393267,G be finite and solvable and M is maximal subgroup of G with $core_G(M)=1$,"I am reading ""Algebra, a graduate course""   of Martin Isaacs. While looking at exercises of chapter 8 (""Solvable and Nilpotent Groups""), I couldn't solve the part (d) of the following question. 8.2 Let $G$ be finite and solvable and suppose $M<G$ is a maximal subgroup and $core_G(M)=1$ . Let $N$ be minimal normal subgroup of $G$ . Show the following: a. $ NM=G$ and $N\cap M=1$ . b. $ C_M(N)=1$ . c. $ N=C_G(N)$ . d. $ N$ is the unique minimal normal subgroup of $G$ . Firstly, $NM$ is a subgroup of $G$ , because $N$ is normal, and $N\cap M < N$ because $core_G(M)=1$ (So $M$ cannot contain any normal subgroup of $G$ ). $\Rightarrow NM > M \Rightarrow NM=G=MN$ . $N$ is solvable because $G$ is solvable $\Rightarrow N'<N \Rightarrow N'=1 $ because $N$ is minimal normal subgroup. So N is abelian. If $t\in N\cap M$ , then for any $n\in N$ $n^{-1}tn=t \Rightarrow t\in core_G(M) \Rightarrow t=1$ (Because $core_G(M)=\bigcap_{g\in G} M^g=\bigcap_{mn\in G} M^{mn}=\bigcap_{n\in N} M^n$ ). So $N\cap M=1$ . If $s\in C_M(N)$ ,
then for any $n\in N$ $n^{-1}sn=s \Rightarrow s\in core_G(M) \Rightarrow s=1$ . So $C_M(N)=1$ and this implies $C_G(N)=N$ because $N$ is abelian and $G=MN$ . But I couldn't prove the uniqueness of minimal normal subgroup.","['finite-groups', 'abstract-algebra', 'solvable-groups', 'group-theory']"
2393302,Eigenvalues and Eigenvectors of a block tridiagonal block Matrix,"Let $T$ be the $2 N \times 2 N$ matrix defined by
$$
T = 
\begin{pmatrix}
A && B\\
-B^* && -A^*
\end{pmatrix}
$$
where $*$ is entry wise complex conjugation, $A$ is a Hermitian $N \times N$ tridiagonal Toeplitz matrix
$$
A
=
\begin{pmatrix}
	a & \alpha& 0 & \dots & 0\\
	\alpha^* & a & \alpha & \vdots & \vdots\\
	0 & \alpha^* & \ddots  & \ddots & \vdots\\
	\vdots & \ddots & \ddots & a & \alpha\\
	0 & \dots & \dots & \alpha^* & a
	\end{pmatrix}
$$
with $a$ real and $\alpha$ in general complex and $B$ is the symmetric tridiagonal Toeplitz matrix
$$
B
=
\begin{pmatrix}
	b & \beta& 0 & \dots & 0\\
	\beta & b & \beta & \vdots & \vdots\\
	0 & \beta & \ddots  & \ddots & \vdots\\
	\vdots & \ddots & \ddots & b & \beta\\
	0 & \dots & \dots & \beta & b
	\end{pmatrix}
$$
where $b$ is real and $\beta$ is complex. I want to find the eigenvalues of eigenvectors of $T$. Here's what I have so far. If $\alpha$ is real, then the solution is quite simple since $A$ is symmetric. Thus, $T$ can be written as
$$
T = i \sigma_x \otimes Im(B)+i\sigma_y \otimes Re(B)+\sigma_z \otimes A
$$
where $\sigma_i$ are the usual Pauli matrices. It is well known that symmetric tridiagonal Toeplitz matrices all the same eigenvectors and their eigenvalues are particularly simple. We can simultaneously diagonalize $Re(B), Im(B),$ and $A$ and rewrite $T$ as a sum of $2 \times 2$ block matrices
$$
T = \sum_{n =1}^N 
\begin{pmatrix}
a+ 2 \alpha \cos(\frac{\pi n}{(N+1)}) && b+\beta \cos(\frac{\pi n}{(N+1)})\\
-(b+\beta^* \cos(\frac{\pi n}{(N+1)})) && -(a+ 2 \alpha \cos(\frac{\pi n}{(N+1)}))
\end{pmatrix}
$$
And finding the eigenvalues and eigenvectors becomes diagonalizing a $2 \times 2$ matrix. Now if $\alpha$ is complex, we encounter a problem. $T$ can now be written as
$$
T = Id \otimes Im(A) +i \sigma_x \otimes Im(B)+i\sigma_y \otimes Re(B)+\sigma_z \otimes Re(A)
$$
by hermicity of $A$, $Im(A)$ is an antisymmetric tridiagonal Toeplitz matrix which doesn't commute with $Re(A), Re(B), Im(B)$ (but it almost commutes in that the only non zero elements of the commutator are at the top left and bottom right of the matrix). So I tried something else. In a different basis (well really just swapping the tensor product) we get 
$$
T = Im(A) \otimes Id + Im(B) \otimes i \sigma_x + Re(B) \otimes i\sigma_y+ Re(A) \otimes \sigma_z
$$
Which we can write as a block tridiagonal Toeplitz matrix
$$
T 
=
\begin{pmatrix}
	C & D& 0 & \dots & 0\\
	E & C & D & \vdots & \vdots\\
	0 & E & \ddots  & \ddots & \vdots\\
	\vdots & \ddots & \ddots & C & D\\
	0 & \dots & \dots & E & C
	\end{pmatrix}
$$
with $C, D, E$ the $2 \times 2$ matrices.
$$
C = 
\begin{pmatrix}
a && b\\
-b && -a
\end{pmatrix}
\: \: \:
D =
\begin{pmatrix}
\alpha^* && \beta\\
-\beta^* && -\alpha
\end{pmatrix}
\: \: \:
E =
\begin{pmatrix}
\alpha && \beta\\
-\beta^* && -\alpha^*
\end{pmatrix}
$$
And so the eigenvalue problem becomes a three term difference equation 
$$
E\begin{pmatrix}x_{j-1} \\ y_{j-1}\end{pmatrix} + C\begin{pmatrix}x_{j} \\ y_{j}\end{pmatrix} + D\begin{pmatrix}x_{j+1} \\ y_{j+1}\end{pmatrix}
=
\lambda \begin{pmatrix} x_{j} \\ y_{j} \end{pmatrix}
$$
with boundary conditions $x_0 = x_{N+1} = y_0 = y_{N+1} = 0$. Now, like in the regular tridiagonal Topeplitz matrix case, I make an ansatz of $x_j = x r^j$ and $y_j = y r^j$ the difference equation becomes
\begin{equation}
(rE+(C-\lambda)+r^{-1}D )
\begin{pmatrix}
x \\y
\end{pmatrix}
=
0
\end{equation}
For a non trivial solution to our equation we need the determinant to vanish, which is a polynomial of degree four in $r$ and we assume for the moment that there are four distinct roots $r_k$, $k = {1,2,3,4}$. Thus the general solution is of the form
$$
\begin{pmatrix}
x_j \\ y_j 
\end{pmatrix}
=
\sum_{k = 1}^4 c_k \lambda^j_k 
\begin{pmatrix}
x_k \\ y_k
\end{pmatrix}
$$
where $(x_k,y_k)$ is in the kernel of the difference equation above. We need to find the $c_k$ which satisfy the boundary conditions which equivalent to finding a non trivial solution to
$$
\begin{pmatrix}
x_1 & x_2 & x_3 & x_4 \\
y_1 & y_2 & y_3 & y_4 \\
\lambda_1^{N+1} x_1 & \lambda_2^{N+1} x_2 & \lambda_3^{N+1} x_3 & \lambda_4^{N+1} x_4 \\
\lambda_1^{N+1} y_1 & \lambda_2^{N+1} y_2 & \lambda_3^{N+1} y_3 & \lambda_4^{N+1} y_4
\end{pmatrix}
\begin{pmatrix}
c_1\\
c_2\\
c_3\\
c_4
\end{pmatrix}
=
\begin{pmatrix}
0\\
0\\
0\\
0
\end{pmatrix}
$$
For a nontrivial solution, we need the determinant to vanish, which imposes a conditions on the roots $\lambda_k$. So I tried something else. The difference equation above implies that once we know $(x_1, y_1)$, we know can recursively find the value of $(x_j, y_j)$ for any $j$. So defined the $2 \times 2$ matrix $T_j$ by
$$
\begin{pmatrix}
u_j \\ v_j
\end{pmatrix}
=
T_j
\begin{pmatrix}
u_1 \\ v_1
\end{pmatrix}
$$
Then this sequence of matrices satisfies a second order matrix difference equation
$$
E T_{j-1} + (C-\lambda)T_j+D T_{j+1} = 0
$$
with boundary conditions $T_0 = 0$ and $T_1 = 1$ (the two $\times$ two identity matrix). Again if $\alpha$ is real, then $E = D$ and we can multiply both sides of the difference equation by $E^{-1}$ to obtain
$$
T_{j-1}+E^{-1} (C-\lambda) T_{j} + T_{j+1} = 0
$$ This difference equation is solved by the matrix
$$
T_j = \dfrac{\sin(\beta j)}{\sin(\beta)} 
$$
where the matrix $\beta$ is defined by $\cos(\beta) = - \frac{1}{2}E^{-1}(C-\lambda) $. The boundary condition for $x_{N+1} = y{N+1} = 0$ means that $(x_1, y_1) $ must be in the kernel of $T_{N+1}$ which can only occur if $\det(T_{N+1}) = 0$. This conditions then translates to a condition on the eigenvalues $\lambda$ and we do in fact recover what we obtained before. But when $\alpha$ is complex, $E \neq D$ and we can't do as we did before since we have to worry about matrices which don't commute. The only time we can in fact do that is if $a = b = 0$ so that our difference equation becomes
$$
E T_{j-1}-\lambda T_j + D T_{j+1}
$$
Since $E$ and $D$ commute, we can work in the basis which diagonalizes both simultaneously, and our matrix equation just becomes two uncoupled scalar second order difference equations which are easy to solve. All this to say that I'm quite stuck. My other idea was to first find the eigenvalues of $T$ directly by computing the determinant (the determinant must also satisfies some recursion relation, right?) Anyway, any help would be greatly appreciated!","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2393311,How to get PDF from moments?,"I want to make joint PDF in the case when I have the mean, the variance, the skewness and kurtosis of my solution. These four moments are spatial and time variables. mean(x,t), skewness(x,t)... Thanks","['statistics', 'probability-distributions']"
2393326,Top cohomology and irreducible components,"Consider a connected (possibily reducible) proper algebraic variety $X$ over $\mathbb{C}$ of pure dimension $n$. Is it true that $H^{2n}(X,\mathbb{Q})\cong \mathbb{Q}^r$ with $r=$#irreducible components of $X$, where the left-hand side is the singular cohomology with respect to the analytic topology?","['algebraic-topology', 'homology-cohomology', 'algebraic-geometry']"
2393404,Can we obtain Ricci curvature tensor from sectional curvature?,"Let $(M,g)$ be a $3$-dimensional Riemannian manifold and $\{e_1,e_2,e_3\}$ an orthonormal frame. About sectional curvature of this manifold we know that
$$K(e_1,e_2)=a,\quad K(e_1,e_3)=b,\quad K(e_3,e_2)=c,$$
My question is : Can we determine Ricci Curvature from above assumption? Thanks.","['riemannian-geometry', 'differential-geometry', 'curvature']"
2393415,Trace of the multiplication of two matrix,"1) Let $F$ be a field and $f$ be a linear functional on $M_{n}(F)$. Then there is a nonzero matrix $T$ in $M_{n}(F)$ such that $f(M)=\mathrm{tr}(TM)$ for all $M\subset M_{n}(F)$. Every one help me. Why? 2) Let $F$ be a  field. Consider the a mapping on the algebra of all linear transformations on $F^n$ as follows:
$$(T,A)=trace(TA).$$ Clearly, this is an inner product. Then this is non-singular. Why? What's the ralation between two  parts of questions?","['matrix-equations', 'matrices', 'functional-analysis', 'linear-transformations', 'linear-algebra']"
2393454,Computing determinant without expansion,$$\begin{align}\mathrm D &= \left|\begin{matrix} (b+c)^2 & a^2 & a^2 \\ b^2 & (a+c)^2 & b^2 \\ c^2 & c^2 & (a+b)^2 \end{matrix}\right|\\ &= (a+b+c)\left|\begin{matrix} b+c - a & a^2 & a^2 \\ b - a -c & (a+c)^2 & b^2 \\ 0 & c^2 & (a+b)^2 \end{matrix}\right| \\ &= (a+b+c)^2\left|\begin{matrix} b+c - a & 0 & a^2 \\ b - a -c & a+c - b & b^2 \\ 0 & c - a-b & (a+b)^2 \end{matrix}\right|\\ &= (a+b+c)^2\left|\begin{matrix} b+c - a & 0 & a^2 \\ 0 & a+c - b & b^2 \\ c - a-b & c - a-b & (a+b)^2 \end{matrix}\right|\end{align}$$ Can $\rm D$ be further simplified without expanding ?  I feel it should be because this was competition question.,"['matrices', 'contest-math', 'linear-algebra', 'determinant']"
2393466,curve windings proof,"There is a step in the following proof for the equality of
$\int_\gamma \frac{dz}{z-a}=2\pi in$, for some $n\in\mathbb{Z}$ which I don't understand. Given $\gamma:[\alpha,\beta] \rightarrow \mathbb{C},$
 we can write:
$\int_\gamma \frac{dz}{z-a}=\int_\alpha^\beta\frac{\gamma'(s)ds}{\gamma(s)-a}$ Then with  $\alpha\leq t \leq \beta$
we get, 
$h(t):=\int_\alpha^t\frac{\gamma'(s)ds}{\gamma(s)-a}$,
from which  $h'(t)=\frac{\gamma'(t)dt}{\gamma(t)-a}$ follows. Now my notes tell me that as a consequence it follows: $\frac{d}{dt}e^{-h(t)}(\gamma(t)-a)=0$. I don't understand how they get there. Anyone could help me?",['complex-analysis']
2393507,Determine IFS of the von Koch Curve,"I am (still) reading Falconer's Fractal Geometry book and I have reached the section on Iterated Function Systems (IFSs). He defines the IFS for the middle third Cantor set: as $S_{1}(x)=\frac{1}{3}x$, $S_{2}(x)=\frac{1}{3}x+\frac{1}{3}$ since, at each iteration of the construction, you shorten the line to a third of its length and then do the same to get a second interval of a third the length and then move it along to the right by $\frac{2}{3}$. I am trying to determine the IFS for the von Koch curve: So far I have worked out that $S_{1}(x,y)=(\frac{1}{3}x,y)$ and $S_{2}(x,y)=(\frac{1}{3}x+\frac{2}{3},y)$ since we are now working in $\mathbb{R}^{2}$. I have also partially worked out $S_{3}$ and $S_{4}$ to be $S_{3}(x,y)=(\frac{1}{5}x+\frac{1}{3},Y_{3})$ and $S_{4}(x,y)=(\frac{1}{5}x+\frac{2}{3},Y_{4})$. Does anyone know how to determine the $Y_{3}$ and $Y_{4}$?","['fractals', 'functions']"
2393513,What is the connection between group action transitivity and relation transitivity?,"Group actions can be transitive . Relations can be transitive . Why do these concepts have the same name?  Is there some  relation that is naturally associated with a group action such that the relation is transitive if and only if the group action is transitive? If not, who decided to use the word “transitive” for this property of group actions, and why?  (I know the term appears in the Burnside finite groups book of 1911: “A permutation-group is called transitive when, by means of its permutations, a given symbol $a_1$ can be changed into every other symbol $a_2, a_3,\ldots, a_n$ operated on by the group.”) For transitive graphs, the notion of “transtitivity” seems to be straightforwardly related to transitive group actions:  A graph is vertex-transitive if its symmetry group acts transitively on the set of vertices.  (And similarly edge-transitivity.)  Did transitive graphs come first, and transitive group actions were a later generalization of that idea, or did transitive group actions come first, and then transitive graphs were an obvious special case?","['terminology', 'math-history', 'group-theory']"
2393564,Prove that $(x^2 + y^2 - z^2)^2 = 4x^2y^2$.,"We have $$x\cos \theta+y\cos \phi = -z\cos \psi \tag 1$$ $$x\sin \theta+y\sin \phi = -z\sin \psi \tag 2$$ $$x\sec \theta+y\sec \phi = -z\sec \psi \tag 3$$ and we have to prove that
$$(x^2 + y^2 - z^2)^2 = 4x^2y^2$$ squaring (1) & (2), and adding them we have $$x^2 + y^2 - z^2 = -2xy \cos(\theta - \phi)$$ multiplying (1) & (3), we have $$x^2 + y^2 - z^2 = -xy (\cos\theta \sec\phi + \cos\phi \sec\theta)$$ from here I can't move forward help me",['trigonometry']
2393589,are there known cases where $\binom{n}{k}$ is a perfect prime power?,"I was wondering about cases where $\binom{n}{k}=p^j$ with $p$ a prime (nontrivially, so that $ n-k>1$ and $n \neq p^j$.) I had the terrible idea of checking binomial expansions
$$(x+y)^n \equiv x^n+y^n.$$ Upon some further googling, I learned that $\binom{50}{3}=140^2$, which means that this can occur for composite numbers. Are there any ways that one can use primeness to argue that this will not occur? Or, are there any known counterexamples in which $\binom{n}{k}$ is a prime power? Edit: Thanks to the comments, this question answers the question for cases of $j \geq 2$ and $4 \leq k \leq n-4$. The main answer mentions that there are examples where $k=j=2$, are any among these solutions with $p$ a prime? this question gives a negative answer for $p=2$ with $n-k \geq 2$. I suppose there is only one question left then: can the theorem be strengthened to contain the extreme cases with the assumption that $p>2$ is a prime.","['number-theory', 'combinatorics', 'binomial-coefficients', 'prime-numbers']"
2393677,"Let $f$ be continuous on $[-1,1]$, Show that $g_n \to f$ uniformly on $[-1,1]$","Let $f$ be continuous on $[-1,1]$, Show that $g_n \to f$ uniformly on $[-1,1]$
where $$
g_n(x) = \int_{-1}^{1}f(y)p_n(y - x)dy
$$ and $$
p_n(x) = \frac{(1-x^2)^n}{\int_{-1}^{1}(1-x^2)^ndx}
$$ I was trying to show that $$
\lim_{n\to\infty}\sup_{x \in [-1,1]}|g_n - f| = 0
$$ I permute the sums and integral and use the binomial theorem. Also, I use the fact that $f$ is bounded (continuous on a compact set). After all of that, I could only get an upper bound for the sup. But I can not show that the bound goes to $0$ as $n$ goes to $\infty$. There is a simpler approach? Thanks.","['continuity', 'real-analysis', 'measure-theory', 'uniform-convergence']"
2393683,Showing Legendre's Properties,"I've solved parts (a) and (b), but i don't know how to use this to solve (c) (a) let $\ell>1$ an odd integer. Show  $\displaystyle (\ell-1)!=2^{\frac{\ell-1}{2}}\left(\frac{\ell-1}{2}\right)!\left(1\cdot3\cdot...\cdot\ell-2\right)$ (b)let $\ell>1$ an odd integer. show that mod $\ell$, $$ \left\{-a: 0 < a \leq \frac{\ell-1}{2} \ | \ \mbox{$a$ is odd} \right\}\ = \ \left\{ b: \frac{\ell-1}{2} <  b \leq \ell-1 \ | \ \mbox{$b$ is even}\right\}$$ and have cardinality $\lfloor\frac{l-1}{4}\rfloor +1$ . (c)Conclude from previous parts that if $\ell>0$ is an odd integer, then: 
$$(\ell-1)! \equiv  2^{\frac{\ell-1}{2}} (\ell-1)! (-1)^{\left \lfloor \frac{\ell-3}{4} \right\rfloor +1} \pmod{\ell}$$","['algebraic-number-theory', 'number-theory', 'elementary-number-theory', 'modular-arithmetic', 'legendre-symbol']"
2393709,Definition/Axioms for the Set of Real Numbers: Why Aren't P-Adic Numbers Real Numbers?,"Why aren't p-adic numbers real numbers? I found the definition of real numbers can be found here on wikipedia which I put directly below for convenience. I think it would break the fact that every nonempty subset of $\mathbb{R}$ would have a least upper bound, but I am not quite sure. I am relatively new to this concept but any help would be greatly appreciated. Let ℝ denote the set of all real numbers. Then: The set ℝ is a field, meaning that addition and multiplication are defined and have the usual properties. The field ℝ is ordered, meaning that there is a total order ≥ such that, for all real numbers x, y and z: if x ≥ y then x + z ≥ y +
  z; if x ≥ 0 and y ≥ 0 then xy ≥ 0. The order is Dedekind-complete; that is: every non-empty subset S of ℝ with an upper bound in ℝ has a least upper bound (also called
  supremum) in ℝ.",['number-theory']
2393719,Is $^{\mathbb{R}^n}\mathbb{R}^n$ isomorphic to $(^{\mathbb{R}^n}\mathbb{R})^n$?,"The title pretty much says it all -- I'm wondering if the set of functions from $\mathbb{R}^n$ to itself, $^{\mathbb{R}^n}\mathbb{R}^n$, is isomorphic to $(^{\mathbb{R}^n}\mathbb{R})^n$, the cartesian product of $^{\mathbb{R}^n}\mathbb{R}$ with itself $n$ times. The motivating example here is in vector calculus, where (for example) we may view the function $\mathcal{V}:\mathbb{R}^2\rightarrow\mathbb{R}^2$ defined by $$\mathcal{V}(x,y)=\begin{bmatrix} x^2y^2 \\ e^xe^y \end{bmatrix}$$ as two functions $f:\mathbb{R}^2\rightarrow\mathbb{R}$ and $g:\mathbb{R}^2\rightarrow\mathbb{R}$  with $f(x,y)=x^2y^2$ and $g(x,y)=e^xe^y$. It seems to me that we could very naturally define operations like $\nabla\circ$,$\nabla\times$ and so on with relative ease in $(^{\mathbb{R}^2}\mathbb{R})^2$ as coordinate wise operations. Is this a well established relationship, and if so are there any texts that exploit this relationship to treat vector calculus like 'iterated multivariate calculus'?","['vector-spaces', 'calculus', 'functional-analysis', 'multivariable-calculus', 'vector-analysis']"
2393723,Integrating against compactly supported continuous functions,"Fix $1<p<\infty$. Let $f \in L^p(E)$, where $E$ is a measurable subset of $\mathbb{R}^d$. Assume that $$
\int_E f(x)g(x)dx =0
$$
for all compactly supported continuous functions $g: \mathbb{R}^d \to \mathbb{R}$. Is it true that $f(x)=0$ for almost every $x \in E$? Here is what I thought: (1) Let $q$ be such that $1/p + 1/q=1$. I claim that $\displaystyle \int_E f(x)g(x)dx =0$ for all $g \in L^q$. This is because the space of compactly supported continuous functions is dense in $L^q(E)$, so by Holder's inequality we obtain the claimed result. (2) Now consider the natural isomorphism between ${L^q}^\ast$ and $L^p$. The linear functional $\kappa_f (g):=\displaystyle \int_E f(x)\cdot g(x) dx$ is hence zero in ${L^q}^\ast$, so $f(x)$ is equivalent to the zero function in $L^p$, so $f(x)=0$ a.e. I was wondering if the preceding argument is valid? Is there a more straight-forward way of showing the result (e.g. without using the duality)? Any help is appreciated!","['real-analysis', 'dual-spaces', 'functional-analysis', 'lebesgue-integral', 'measure-theory']"
2393755,"Separable equation, initial condition","I am confused about this example that I am trying to understand. We solve the initial value problem $$x' = \frac{te^{x^2}}{x}, \space x(0) = 1,  $$ and we do it ""the usual way"" by writing the equation as $x'xe^{-x^2} = t $ and integrating both sides. We end up with $x^2 = \mathrm{ln}\frac{1}{e^{-1}-t^2} \iff x = \pm \sqrt{\mathrm{ln}\frac{1}{e^{-1}-t^2}}.$ Then the text says that we know, because of the initial condition, which sign to choose in front of the root expression, so it should be $+\sqrt{\mathrm{ln}\frac{1}{e^{-1}-t^2}}.$ That is what confuses me. I have learned that if there is a constant solution $x(t) \equiv k$, then any nonconstant solution cannot cross the line $x=k$. But from what I can gather, there is no constant solution to this equation. Because $x\equiv k \iff \frac{e^{x^2}}{x} = 0,$ which cannot happen for any $x$. Other than that I have no idea what the initial condition could possibly tell me about which sign to choose.","['ordinary-differential-equations', 'initial-value-problems']"
2393795,Every linear operator on $\mathbb{R}^5$ has an invariant 3-dimensional subspace,"I am trying to determine whether the following statement is true or false: Every linear transformation on $\mathbb{R}^5$ has an invariant 3-dimensional subspace. Since $\dim(\mathbb{R}^5)=5$ then given any linear operator $T$ on $\mathbb{R}^5$ I know that $\deg(\text{char}_T(x))=5$, and hence,$\text{char}_T(x)$ has at least one real root, meaning that $T$ has at least one real eigenvalue, $\lambda$. Thus,
$$\text{char}_T(x)=(x-\lambda)f(x),$$
where $f$ can be factored as the product of irreducibles into two quadratics, a quadratic and two linear factors, or 4 linear factors. I don't know where to go from there. Perhaps the statement is false? Thank you for your help!","['invariant-subspace', 'linear-algebra', 'linear-transformations', 'vector-spaces']"
2393800,"Meaning of the slash ""/"" in $\mathbb{Z}/p\mathbb{Z}$","What the meaning of the slash ""/"" in expression like this: $\mathbb{Z}/p\mathbb{Z}$ ? I know that it is called ""a quotient ring"", but quotient reminds division. It's an operator somewhat related to division? I'm an engineer trying to fully understand the abstract algebra behind error correction codes. May you leave a suggestion of a book for non-mathematicians that explain this topic in an easy manner?",['abstract-algebra']
2393810,Elliptic Integrals & Gamma Functions of Rational values.,"I recently saw this question  ... https://math.stackexchange.com/questions/2393668/double-factorial-sum-k-0-infty-left-frac-2k-1-2k-right#2393668 ... & I am unable to show it. The result
  $$1-\left( \frac{1}{2} \right)^{3}+\left( \frac{1\cdot 3}{2\cdot 4} \right)^{3}-\left( \frac{1\cdot 3\cdot 5}{2\cdot 4\cdot 6} \right)^{3}+\left( \frac{1\cdot 3\cdot 5\cdot 7}{2\cdot 4\cdot 6\cdot 8} \right)^{3}-...=\frac{\Gamma ^{2}\left( \frac{9}{8} \right)}{\Gamma ^{2}\left( \frac{7}{8} \right)\cdot \Gamma ^{2}\left( \frac{10}{8} \right)}$$ Now I know several results relating to those ratios of double factorials 
\begin{eqnarray*}
\int_0^{\frac{\pi}{2}} \sin^{2n} \theta d \theta= \frac{ \pi}{2} \frac{(2n-1)!!}{(2n)!!} \tag{1} \\
\sum_{n=0}^{\infty} \frac{(2n-1)!!}{(2n)!!} y^n = \frac{1}{\sqrt{1-y}} \tag{2}
\end{eqnarray*}
From these two results it is reasonably easy to derive the series for the elliptic integral of the first kind
\begin{eqnarray*}
K(k)=\int_0^{\frac{\pi}{2}} \frac{d \theta}{\sqrt{1-k^2 \sin^{2}(\theta)}} =\frac{\pi}{2} \left(1+ \left(\frac{1}{2}\right)^2+ \left(\frac{1.3}{2.4}\right)^2+ \cdots \right)
\end{eqnarray*}
& it is well known that this can be evaluated for special values of $k$ in terms of Gamma functions whose arguements are rational values. See this question Can $\Gamma(1/5)$ be written in this form? and the reference cited there.
So my first thought is to use $(1)$ three times, sum the geometric plum & we have 
\begin{eqnarray*}
\int_{0}^{\frac{\pi}{2}} \int_{0}^{\frac{\pi}{2}} \int_{0}^{\frac{\pi}{2}} \frac{d \alpha d \beta d \gamma}{1+ \sin^2 \alpha \sin^2 \beta \sin^2 \gamma}
\end{eqnarray*}
This triple integral looks difficult so ... Second thoughts ... use $(1)$ twice & then use $(2)$ to get the double integral
\begin{eqnarray*}
\int_{0}^{\frac{\pi}{2}} \int_{0}^{\frac{\pi}{2}}  \frac{d \alpha d \beta }{\sqrt{1+ \sin^2 \alpha \sin^2 \beta }}
\end{eqnarray*}
now substitute $ \sqrt{s} =\sin \alpha$ and $ \sqrt{t} =\sin \beta$ (might have lost a factor of $4$)
\begin{eqnarray*}
\int_{0}^{1} \int_{0}^{1}  \frac{d s d t }{\sqrt{s(1-s)t(1-t)(1+st) }}
\end{eqnarray*}
& I am not sure what to do with this. In both of these attempts I feel I have taken a wrong turn. Can someone either give me a Big hint or a reference to the original derivation of this result or a reasonably complete solution ? Bonus question ... why was it stated with $\frac{10}{8}$ instead of $\frac{5}{4}$ ? The result
  $$1-\left( \frac{1}{2} \right)^{3}+\left( \frac{1\cdot 3}{2\cdot 4} \right)^{3}-\left( \frac{1\cdot 3\cdot 5}{2\cdot 4\cdot 6} \right)^{3}+\left( \frac{1\cdot 3\cdot 5\cdot 7}{2\cdot 4\cdot 6\cdot 8} \right)^{3}-...=\frac{\Gamma ^{2}\left( \frac{9}{8} \right)}{\Gamma ^{2}\left( \frac{7}{8} \right)\cdot \Gamma ^{2}\left( \color{red}{\frac{5}{4}} \right)}$$ Hint from Jack D'Auirzio : look at equation (6) here http://mathworld.wolfram.com/CompleteEllipticIntegraloftheFirstKind.html and use $2kk'=i$ where $k'$ is the complementary modulus.","['real-analysis', 'multivariable-calculus', 'integration', 'elliptic-integrals', 'sequences-and-series']"
2393814,Prime factors of numbers of the form $n^{n^n}+n^n+1$,"Here : Prime candidates of the form $n^{(n^n)}+n^n+1$? I asked for the prime factors of $f(12)$ and $f(60)$, where $$f(n)=n^{(n^n)}+n^n+1$$ I would like to accelarate the search of the prime factors of $f(n)$. Are there special properties considerably restricting the possible prime factors of $f(n)$ ? The only numbers I factored completely so far, are $f(1),f(2),f(3)$ and $f(4)$. For those, who are interested, here are the factoriations : https://factordb.com/index.php?query=n%5En%5En%2Bn%5En%2B1","['number-theory', 'perfect-powers', 'prime-factorization']"
2393828,Show that for $a_i>0$ $\frac{a_1+\cdots+a_n}{n}$ converges to $0$ if and only if $\frac{a_1^2+\cdots+a_n^2}{n}$ converges to $0$.,"Let $\{a_n\}$ be a bounded and positive sequence. Show that $$\lim_{n\to \infty}\frac{a_1+\cdots+a_n}{n}=0$$
if and only if
$$\lim_{n\to \infty}\frac{a_1^2+\cdots+a_n^2}{n}=0.$$ My attempt: The ""$\Rightarrow$"" is obvious. Note that 
$$\frac{a_1^2+\cdots+a_n^2}{n}\leq |M|\cdot\frac{a_1+\cdots+a_n}{n} $$
where $|M|$ is the bound of the sequence. So the convergence of the right side implies the convergence of the left side. As for the converse direction, I really have no idea... @kimchi lover points out using the Cauchy-Schwarz inequality and I had the following attempt... $$\frac{a_1+\cdots+a_n}{n}=\frac{\frac{1}{\sqrt{n}}(a_1+\cdots+a_n)}{\frac{1}{\sqrt{n}}n}\leq \frac{(a_1^2+\cdots+a_n^2)(\frac{1}{n}+\cdots+\frac{1}{n})}{\sqrt{n}}$$","['sequences-and-series', 'calculus', 'limits']"
2393850,What is the smallest n-gon such that there can be an interior point further from all boundary points than the points are from each of their neighbors?,"This is a ""sequel"" question to What is the maximum n-gon such that there can't be an interior point further from all boundary points than the boundary points are from each other? In the previous question I asked: Given a polygon with n vertices, how small can n be before it is impossible to have an interior point p such that the distance from any vertex b to p is greater than the distance from b to its nearest adjacent vertex, b'? For example, for any triangle, it seems that an interior point will be closer to at least one vertex than that vertex is to its nearest neighbor. From my own inspection, it seems like the answer might be 6 - a regular hexagon is the first shape for which a point at the center is the same distance from each vertex as the vertices are from their nearest neighbors. However, I can't figure out how to formulate a rule, or apply the intuition from the regular hexagon to irregular shapes. For my use case, it would be useful to know ""below n vertices, there is no way to have such an interior point."" My updated question is: how small can $n$ be before it is impossible to have a point $p$ such that the distance from any vertex $b$ to $p$ is greater than the distance from $b$ to either of its adjacent vertices , $b'$ ?",['geometry']
2393852,Proof of isomorphism between $\text{PGL}_2(\mathbb{F}_5)$ and $S_5$,"This question has been asked here before but I don't think any of the previous answers are clear to someone like me who only has an elementary background in abstract algebra. So can I take the time to ask once again: Why do we have $PGL_2(\mathbb{F}_5) \cong S_5$? So far I have tried to find an action of $GL_2(\mathbb{F}_5)$ on a set with 5 elements but have had no luck. However if you let $GL_2(\mathbb{F}_5)$ act on the projective line $P^1(\mathbb{F}_5)$ then we get a homomorphism to $S_6$ whose kernel is the set of scalar matrices which is exactly $Z(GL_2(\mathbb{F}_5))$. So we get an isomorphism from $PGL_2(\mathbb{F}_5)$ to a subgroup of $S_6$ . I then tried to consider the action of $S_6$ on $S_6:PGL_2(\mathbb{F}_5)$ and tried to use that to show the isomorphism but it didn't help. Does anyone know how it might be possible to proceed from here or am I going down completely the wrong track? Any hints are much appreciated! EDIT: Here is the full question as requested: Show that the groups $SL_2(\mathbb{F}_4)$ and $PSL_2(\mathbb{F}_5)$ both have order 60. Use this and some results from previous questions to show that they are both isomorphic to the alternating group
$A_5$. Show that $SL_2(\mathbb{F}_5)$ and $PGL_2(\mathbb{F}_5)$ both have order 120, that $SL_2(\mathbb{F}_5)$ is not isomorphic
to $S_5$, but $PGL_2(\mathbb{F}_5)$ is. The previous questions which the question refers to (and I was able to do) were: Let $G$ be a group of order 60 which has more than one Sylow 5-subgroup. Show that $G$ is simple. Let $G$ be a simple group of order 60. Deduce that $G \cong A_5$, as follows. Show that $G$ has six Sylow
5-subgroups. By considering the conjugation action of the set of Sylow 5-subgroups, show that $G$
is isomorphic to a subgroup $G \leq A_6$ of index 6. By considering the action of $A_6$ on $A_6:G$, show
that that there is an automorphism of $A_6$ taking $G$ to $A_5$.","['finite-groups', 'abstract-algebra', 'group-isomorphism', 'exceptional-isomorphisms', 'group-theory']"
2393864,Iterated integrals,"Is there any example of a function $f:[a,b]\times[c,d]\to \mathbb R$ so that $\int_a^b\int_c^d f(x,y)\,dy\,dx$ and $\int_c^d\int_a^b f(x,y)\,dx\,dy$ exist and are equal but $\int\int f(x,y)\,dy\,dx$ does not exist in $[a,b]\times[c,d]$? I've been trying to find an example but I have nothing so far.","['multivariable-calculus', 'multiple-integral', 'integration']"
2393875,Why doesn't $\frac{1}{\sqrt{n}}\geq h_n $ proves that the limit is indeed $1$?,"I am reading Courant's Differential and Integral Calculus, here: I believe what he done is the following: $$\sqrt{n}=(b_n)^n=(1+ h_n)^n\geq 1+nh_n \\ \frac{ \sqrt{n}}{n}=\frac{(b_n)^n}{n}=\frac{(1+ h_n)^n}{n}\geq \frac{1}{n}+h_n\\  \frac{ \sqrt{n}-1}{n}=\frac{(b_n)^n-1}{n}=\frac{(1+ h_n)^n-1}{n}\geq h_n$$ Now: $$\frac{1}{\sqrt{n}}=\frac{\sqrt{n}}{n}\geq \frac{ \sqrt{n}-1}{n}$$ And we have: $$\frac{1}{\sqrt{n}}\geq\frac{ \sqrt{n}-1}{n}=\frac{(b_n)^n-1}{n}=\frac{(1+ h_n)^n-1}{n}\geq h_n$$ And he concludes it here: $\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad $ But he adds another step after that, why is the previous conclusion not enough to prove the limit is $1$? After all, $\frac{1}{\sqrt{n}}\to 0$ as $n\to\infty$, as $h_n \leq \frac{1}{\sqrt{n}}$ it should be $h_n\leq 0$, this should be enough, no? I've been thinking about what could go wrong, but until now I had no success. Perhaps $h_n$ could be negative? But for not well known reasons, it doesn't seems likely.","['real-analysis', 'calculus', 'limits']"
2393958,How do you approach solving problems like these using rules of inference?,"So using rules of inference can you show that the premises: $$\neg p\vee q\rightarrow r,\;s\rightarrow\neg q,\;p\rightarrow t,\;\neg t,\;\neg p\wedge r\rightarrow s$$ lead to the conclusion $\neg q$ ? How would one approach trying to solve a problem like this? Mainly how do you know which premise to start with and the combinations to use to get to the conclusion? Or is it just one of those things that require time and practice to get the hang of.","['logic', 'discrete-mathematics']"
2393969,How can I determine the Steenrod Square $Sq^2$ for complex projective space?,"I am trying to learn about Steenrod Squares for algebraic varieties so that I can compute examples of complex topological K-theory using the Atiyah-Hirzebruch Spectral Sequence (AHSS). One of the key points about this sequence is that the third differential $d_3$ is the steenrod squaring operation $Sq^3$ on integral cohomology. This is defined as the composition 
$$
\beta \circ Sq^2 \circ r
$$
where $r$ is the reduction mod $2$ cohomology and $\beta$ is the connecting morphism associated to $$0 \to \mathbb{Z} \to \mathbb{Z} \to \mathbb{Z}/2 \to 0$$
Since these operations play well with functoriality, I will only need to determine the Steenrod operations in $\mathbb{CP}^n$ in specific cases: If I have a smooth projective 3-hypersurface $X$ then the only non-trivial differential factors through $H^2(\mathbb{CP}^4)$. How can I determine this cohomology operation?","['algebraic-geometry', 'spectral-sequences', 'algebraic-topology', 'k-theory', 'cohomology-operations']"
2394026,Why is it difficult to develop Algebraic Geometry over the language of Set Theory?,"Quoting Wikipedia: The subsequent development of category theory was powered first by the computational needs of homological algebra, and later by the axiomatic needs of algebraic geometry, the field most resistant to being grounded in either axiomatic set theory or the Russell-Whitehead view of united foundations . It is clear now that Algebraic Geometry has benefitted a lot from the language provided by Category Theory, but, historically, what were the complications when people tried to formulate A.G. over the language of set theory? Why did we require Category Theory?","['math-history', 'algebraic-geometry']"
2394033,How to prove that $\frac{\text{d}}{\text{d}x} x^n = nx^{n-1}$ without using the Binomial Theorem?,How does one show that $\frac{\text{d}}{\text{d}x} x^n =nx^{n-1}$ without resorting to the Binomial Theorem? Edited : I'm interested in this approach as I've been tinkering with the proof for the Binomial Theorem using Taylor Series but then later realized that I've assumed $\frac{\text{d}}{\text{d}x} x^n = nx^{n-1}$ which I have proven earlier using the Binomial Theorem itself. So the proof ended up being circular in nature. I'm trying to circumvent that by proving it without the Binomial Theorem. Link to question : Binomial Theorem Proof from Taylor Series,"['derivatives', 'binomial-theorem']"
2394045,Mean value property of harmonic functions on manifolds,"A well-known feature of harmonic functions on (domains of) $\mathbb{R}^n$ is the mean-value property: that is, if $\Delta u = 0$, then
$$ u(x_0) = \frac{1}{\text{Vol}(\partial B_r(x_0))}\int_{\partial B_r(x_0)}{u\,dS} = \frac{1}{\text{Vol}(B_r(x_0))}\int_{B_r(x_0)}{u\,dV}. $$
Is the same true on manifolds in general? That is, given a manifold $(M,g)$ and a smooth function $u:M\rightarrow\mathbb{R}$ satisfying
$$\Delta_gu = 0\quad\text{where}\quad\Delta_g = \frac{1}{\sqrt{\det g}}\frac{\partial}{\partial x^i}g^{ij}\sqrt{\det g}\frac{\partial}{\partial x^j},$$
is it true that
$$ u(x_0) = \frac{1}{\text{Vol}(\partial B_r(x_0))}\int_{\partial B_r(x_0)}{u\,dS}\quad\text{or}\quad u(x_0) = \frac{1}{\text{Vol}(B_r(x_0))}\int_{B_r(x_0)}{u\,dV}?$$
Here, $B_r(x_0)$ is the geodesic ball of radius $r$ around $x_0$. If the equalities do not hold, how would the average value (either over a sphere or over a ball) change with $r$? For example, on a surface the Gaussian curvature appears in the higher-order terms in the Taylor expansion of the length/area of a circle/ball of radius $r$--does a similar phenomenon occur for the mean value?","['harmonic-functions', 'differential-geometry']"
2394083,Is $\frac{200!}{(10!)^{20} \cdot 19!}$ an integer or not?,"A friend of mine asked me to prove that $$\frac{200!}{(10!)^{20}}$$ is an integer I used a basic example in which I assumed that there are $200$ objects places in $20$ boxes (which means that effectively there are $10$ objects in one box). One more condition that I adopted was that the boxes are distinguishable but the items within each box are not. Now the number of permutations possible for such an arrangement are : 
$$ \frac{200!}{\underbrace{10! \cdot 10! \cdot 10!\cdots 10!}_{\text{$20$ times}}}$$ 
$$\Rightarrow \frac{200!}{(10!) ^{20}}$$ Since these are just ways of arranging, we can be pretty sure that this number is an integer. Then he made the problem more complex by adding a $19!$ in the denominator, thus making the problem:
Is $$\frac{200!}{(10!)^{20} \cdot 19!}$$ an integer or not? The $19!$ in the denominator seemed to be pretty odd and hence I couldn't find any intuitive way to determine the thing. Can anybody please help me with the problem?","['permutations', 'combinatorics']"
2394106,$\lim_{t \to 0} \frac1{t} \int_{\mathbb R} |f(x+t) -f(x)|dx=0 \implies f =0$,"Suppose $f$ is Lebesgue integrable on $\mathbb R$ and satisfies: $$\lim_{t\to 0} \frac1{t} \int_{\mathbb R} |f(x+t)-f(x)|dx = 0$$ Prove that $f = 0$ almost everywhere. It seems to me that there is no way to solve this problem using elementary methods. If $f$ is differentiable we can use Fatou's lemma as follows: let $g_n(x) = n\left( f\left(x + \frac1n \right) - f(x) \right)$ . $$\int_{\mathbb R} |f'(x)| dx = \int_{\mathbb R} \liminf |g_n(x)| dx  \le \liminf \int_{\mathbb R} |g_n(x)|dx = 0$$ so $f' = 0$ a.e. and $f = c$ . Since $f$ is integrable, $c = 0$ . We can always use Fatou's lemma to obtain: $\liminf g_n(x) = 0$ but this doesn't look very useful to me. How to solve this problem?","['real-analysis', 'lebesgue-integral', 'measure-theory']"
2394107,Understanding the dependency of trials.,"Problem : A certain population is made up of 80% Mexican Americans.Select a jury of 12.Find the mean and standard deviation. Problem Link : https://youtu.be/4Ew60JEPGUk?list=PL5102DFDC6790F3D0&t=865 The instructor assumes success as selecting a Mexican American.So the problem boils down to calculating the expected no of successes i.e if X is a random variable representing the number of successes then we have to find 
$E(X)$. According to my understanding, we can use binomial distribution only if the trials are independent but in this problem selecting a person from the population changes one of the amounts and hence the probability of success so how is it feasible to apply binomial distribution here? Let me know if I'm not clear.","['statistics', 'binomial-distribution', 'probability-distributions']"
2394138,How do I prove that this space is a Banach space?,"We define the following space 
$$
L^{\Phi}(\Omega)=\left\{u:\Omega\rightarrow \mathbb{R}~\text{measurable};~\int_{\Omega}\Phi\left(\frac{u}{\lambda}\right) dx<+\infty, ~\text{for any}~\lambda>0\right\}
$$ where $\Phi:\mathbb{R}\to\mathbb{R}^+$ satisfies the following conditions: $(i)$ $\Phi$ is a continuous and a convex function; $(ii)$ $\Phi(t)=0$ if, and only if $t=0$; $(iii)$ $\displaystyle\frac{\Phi(t)}{t}\overset{t\rightarrow0}{\longrightarrow}0$ and $\displaystyle\frac{\Phi(t)}{t}\overset{t\rightarrow+\infty}{\longrightarrow}+\infty$; $(iv)$ $\Phi$ is even (that is $\Phi(t)=\Phi(-t)$) Endowed with the norm:
$$
||u||_{\Phi}=\inf
\left\{\lambda>0; \int_{\Omega}\Phi\left(\frac{u}{\lambda}\right)dx\leq 1\right\}
$$ I start the proof by : Let $(f_n)\subset L^{\Phi}(\Omega)$ be a Cauchy sequence that is $||f_n-f_m||_{\Phi}\to0$ when $m,n\to\infty$ How do we find $f\in L^{\Phi}(\Omega)$ such that $||f_n-f||_{\Phi}\to0$ ? Thank you","['functional-analysis', 'banach-spaces', 'orlicz-spaces']"
2394154,Definition of measurability of a stochastic process,"Definition: A stochastic process $(X_t)_{t \in [0,\infty)}$ on a filtered probability space $(\Omega,\mathcal{F},(\mathcal{F}_t)_{t \in [0,\infty)},\mathbb{P})$ with values in a measurable space $(E,\mathcal{E})$ is called measurable if the map
  $$
f_{\infty} \colon \Omega \times [0,\infty) \rightarrow E \colon (\omega,s) \mapsto X_s(\omega)
$$
  is $\mathcal{F} \otimes \mathcal{B}_{[0,\infty)}$-measurable. Sometimes it is also required that the map 
$$
f_t \colon \Omega \times [0,t) \rightarrow E \colon (\omega,s) \mapsto X_s(\omega)
$$
is $\mathcal{F} \otimes \mathcal{B}_{[0,t)}$-measurable for every $t \in [0,\infty)$. Now my question is, are those two definitions equivalent? If not, is there one which implies the other?","['stochastic-processes', 'probability-theory', 'measure-theory']"
2394193,"Is there a sensible way of composing ""non-instantaneous"" functions?","If we try to implement a function $\mathbb{R}^n \rightarrow \mathbb{R}$ using real-world devices gadgets, there'll inevitably be a delay between changing the input values and seeing the desired change in the output value. We could try to model this using a first-order differential equation. Think of $\mathbb{R}^n$ as a smooth manifold $A$ and a $\mathbb{R}$ as a smooth manifold $B$. we want to define that a ""non-instantaneous map"" $f : A \rightarrow B$ is a way of assigning to each $a \in A$ a corresponding autonomous first-order differential equation $f(a)$ that moves us non-instantaneously toward the desired output value in $B$. We can formalize this as follows: Definition. Given smooth manifolds $A$ and $B$, a non-instantaneous function $A \rightarrow B$ is a smooth function $f:A \times B \rightarrow TB$, where $TB$ is the tangent bundle of $B$, such that for all $a \in A$, the function $f(a,-) : B \rightarrow TB$ is a section of the bundle projection $TB \rightarrow B$. By currying $f$ into the form $A \rightarrow (B \rightarrow TB),$ we see that this really just a clever way of assigning a vector field $f(a)$ to each $a \in A$, and since vector fields are basically first-order autonomous differential equations in disguise, this is a reasonable definition. Now, I guess there's a reasonable way to compose such things, yielding a category. But honestly, I cannot see it. (In fact, I can't even see how to implement identity morphisms.) Question. Is there a sensible way of composing non-instantaneous functions as defined above? If not, what is the correct definition of non-instantaneous functions, such that composing them becomes possible and we get a category?","['vector-fields', 'differential-topology', 'calculus', 'category-theory', 'ordinary-differential-equations']"
2394318,"Computation of nearby cycles, monodromy action and action of $sl_{2}$ on $gr(\Psi)$ for Picard-Lefshetz family.","Let $f: \mathbb{A}^{2} \rightarrow \mathbb{A}^{1}$ be a map that sends $(x,y)$ to $xy$. Let $U \hookrightarrow \mathbb{A}^{2}$ be the preimage $f^{-1}(\mathbb{A}^{2}\setminus \{ 0\})$ and $X:=f^{-1}(0)$. Consider the shifted constant sheaf $\mathbb{C}_{U}[2]$ on $U$. Let $\Psi(\mathbb{C}_{U}[2])$ denote the nearby cycles functor applied to $\mathbb{C}_{U}[2]$. It gives us a perverse sheaf on $X$. My question is: how to compute this perverse sheaf on $X$ and the action of monodromy on it? Also if the action is unipotent then how to calculate the action of ""Lefshetz"" $sl_{2}$ on $\operatorname{gr}(\Psi(\mathbb{C}_{U}[2]))$? Thanks!","['homological-algebra', 'representation-theory', 'algebraic-geometry']"
2394343,How to calculate $\iint\ln(x^2+y^2)$ over a part of a circle?,"Given the range $\frac{1}{\sqrt2}\le x\le 1$ and $\sqrt{1-x^2}\le y\le x$
  calculate $\iint\ln(x^2+y^2)$. This is how the domain looks like: We need to calculate the integral on the area in red. It seems quite hard next to impossible to pull off the calculation with the given ranges so I thought to calculate the integral on the triangle $ABC$ (let this domain be $B$) and from that to subtract the integral in on circle with the angle between $0$ and $\pi/4$ (let this domain be $C$). So $\iint_C$ I actually was to able to pull off. But $\iint_B$ doesn't seem feasible at least to me.
$$
\iint_C\ln(x^2+y^2)=\int_0^{\pi/4}\int_0^1 \ln(r^2)r \,dr\,d\theta=\frac{1}{2}\int\bigg[ r^2\ln r^2-r^2\bigg]_0^1=\frac{1}{2}\int-1\,d\theta=-\frac{\pi}{8}
$$ Now:
$$
\iint_B \ln(x^2+y^2)\,dx \,dy=\int_0^1\int_0^x \ln(x^2+y^2)
$$
I don't see how this can be integrated, substitution method doesn't help here. Can I convert to polar coordinates again? But then:
$$
\int_0^1\int_0^{r\cos\theta} r\ln r^2
$$
which is not feasible for me. How to tackle this?","['multivariable-calculus', 'integration']"
2394433,"Prove that a quadrilateral, and the quadrilateral formed by the orthocenters of four related triangles, have the same area.","In a forum the following theorem was proposed, but I couldn't find a synthetic solution. Given a quadrilateral $ABCD$, let $P$, $Q$, $R$ and $S$ be resp. the
  orthocenters of triangles $ABC$, $BCD$, $CDA$ and $DAB$. Prove that
  quadrilaterals $ABCD$ and $PQRS$ have the same area. I found online a proof for the case $ABCD$ is cyclic, but have no clue for the general case. EDIT. Note that if $ABCD$ is cyclic a much stronger result holds: $ABCD$ and $PQRS$ are equal. This is problem 2.35 in Mathematical Olympiad Treasures by Andreescu and Enescu. They prove the theorem using vectors and derive it from a lemma which may be useful: if $H$ is the orthocenter of a triangle $ABC$ and $O$ its circumcenter, then one has: $\vec{OA}+\vec{OB}+\vec{OC}=\vec{OH}$. EDIT 2. Finding an expression for the coordinates of the orthocenter, as a function of the coordinates of the vertices, is not difficult. For instance, we get for $P$:
$$
x_P=\frac{\left(y_A-y_B\right) \left(y_C-y_A\right) \left(y_B-y_C\right)-x_A x_B
   \left(y_A-y_B\right)-x_A x_C \left(y_C-y_A\right)-x_B x_C \left(y_B-y_C\right)}{x_C
   \left(y_A-y_B\right)+x_A \left(y_B-y_C\right)+x_B \left(y_C-y_A\right)},\\
\ \\
y_P=\frac{\left(x_A-x_B\right) \left(x_C-x_A\right) \left(x_B-x_C\right)-y_A y_B
   \left(x_A-x_B\right)-y_A y_C \left(x_C-x_A\right)-y_B y_C \left(x_B-x_C\right)}{y_A
   \left(x_B-x_C\right)+y_B \left(x_C-x_A\right)+y_C \left(x_A-x_B\right)}.
$$
The signed area $S_{ABC}$ of triangle $ABC$ can be conveniently expressed as:
$$
2S_{ABC}=A\times B+B\times C+C\times A=
x_A y_B-x_B y_A +x_B y_C-x_C y_B +x_C y_A -x_A y_C,
$$
where I used the shortcut $A\times B=\det(A,B)$. Of course, the sign of $S_{ABC}$ depends on the order of the vertices.
Also notice that denominators in the above formulas for $x_P$ and $y_P$ are just $-2S_{ABC}$. We can then define a signed area for quadrilateral $ABCD$:
$$
2S_{ABCD}=S_{ABC}+S_{BCD}+S_{CDA}+S_{DAB}.
$$
I used Mathematica to find general expressions for the signed areas of $ABCD$ and $PQRS$ as a function of the coordinates of $A$, $B$, $C$, $D$, and checked they are indeed equal. This counts as a proof, but the resulting expressions are too long and involved. Moreover, that gives no insight for the reason of that equality. In doing the above, I also realized that a stronger equality holds, of which I hadn't been aware before. It turns out that The area of triangle $PQR$ is the same as the area of triangle $DAB$
  (that is of the triangle having $S$ as orthocenter), and so on. Of course this implies that $ABCD$ and $PQRS$ have the same area. Hopefully, this stronger result could be easier to prove without using algebra and coordinates.","['euclidean-geometry', 'area', 'geometry']"
2394450,Equivalence of definitions of simple connectedness of a region $\Omega$ in $\overline{\mathbb{C}}$,"I've started reading up on complex analysis recently, and many books give their own definition of simple connectedness, all of which I assume to be equivalent, so I'm trying to prove their equivalence. Here are the definitions I've encountered (I'm excluding all the definitions which I've proven to be equivalent to one of the following three): A region (i.e. open and connected subset of $\overline{\mathbb{C}}$) $\Omega$ is simply connected in $\overline{\mathbb{C}}$ if $\partial\Omega$ is connected in $\overline{\mathbb{C}}$. A region $\Omega$ is simply connected in $\overline{\mathbb{C}}$ if $\Omega^\complement$ is connected in $\overline{\mathbb{C}}$. (A very unusual definition in my opinion, but interesting nonetheless) A region $\Omega$ in $\overline{\mathbb{C}}$ is simply connected if for any closed, simple (simple meaning no self-intersections) polygonal line $\Lambda \subset \Omega$, $\Lambda^{\circ}\subset \Omega$ holds, where ${}^{\circ}$ denotes the interior of a set. I've managed to figure out $1. \implies 3$, $1. \implies 2.$ and partially $3. \implies 2$, where I've managed to figure out that if I can construct a polygon $\Lambda \subset \Omega$ around a compact connected component $C$ of $\Omega^{\complement}$ (which exists because either $\Omega$ or one component of $\Omega^{\complement}$ will contain $\infty$, and any closed set in $\overline{\mathbb{C}}$ is compact if we look at $\overline{\mathbb{C}}$ as a sphere; I'm trying $\neg 2. \implies \neg 3$, so there's more than one connected component), I'll have a point in $C \subset \Lambda^{\circ}$ outside of $\Omega$, which is exactly what was needed. So my problems here are: The proof $2. \implies 1$, which I don't know how to approach because the only thing that occurs to me is to try $\neg 1. \implies \neg 2,$ which gets me nowhere because if I assume that the boundary is disconnected, i.e. $\partial \Omega = A \cup B$ where $A$, $B$ are disjoint, non-empty and open, I have no idea how to construct a set whose boundary is $A$ or $B$, so I can't turn these ""lines"" into ""areas"" (I don't mean this in a literal sense; I'm just saying I don't know how to use the disconnectedness of the boundary to prove the disconnectedness of the complement). The construction of a polygon $\Lambda$. Now, I've been reading something in Ahlfors, where in the proof of Theorem 14, pages 139-140, the author constructs such a polygon (for a different reason altogether, though) under the assumption that if $\Omega^{\complement} = A \cup B$ where $A$, $B$ are disjoint, non-empty and clopen in $\Omega^{\complement}$, then $d(A, B)>0$, which I'm not entirely clear on. There are examples of disjoint, closed sets with distance $0$, such as $\mathbb{N}$ and $\{n+\frac{1}{n} | n \in \mathbb{N}\}$, so just saying that they're closed and disjoint isn't enough. I'm looking for a clarification of $d(A, B) > 0$, which seems intuitive, but I'm mentally blocked on how to prove it.","['complex-analysis', 'general-topology']"
2394452,Why is a gradient field a special case of a vector field?,"My calculus manual suggests a gradient field is just a special case of a vector field. That implies that there are vector fields that there are not gradient fields. The gradient field is composted of a vector and each $\mathbf{i}$, $\mathbf{j}$, $\mathbf{k}$ component (using 3 dimensions) is multiplied by a scalar that is a partial derivative. Is this because the scalar may be of the form that there is no antiderivative of? Perhaps of a function that can not be differentiated? Maybe a vector field that has sharp turns but I can't come up with any that I can't find a derivative for. Does anybody have any so I can get some intuition of the problem?",['multivariable-calculus']
2394469,What are the functions for which ${f f''\over f'^2} < 2$?,"What are the functions $f$ on $[0,1]$ (with continuous first and second derivatives) that satisfy the following conditions: Monotonically increasing in $[0,1]$, with $f(0)\geq 0$ and $f(1)=1$; for all $x\in[0,1]$: $$
{f(x) f''(x) \over f'(x)^2} < 2
$$ ? Some simple examples are: $f(x) = x^k$ for some $k> 0$. Then: $f(x)f''(x)=k(k-1)x^{2k-2}$ and $f'(x)^2 = k^2 x^{2k-2}$ so the quotient is $(k-1)/k < 1 < 2$. $f(x) = e^{k (x-1)}$ for some $k\geq 0$. Then: $f(x)f''(x) = k^2 f(x) = f'(x)$ so the quotient is $1 < 2$. Is there a general form of functions that satisfy these conditions?","['functional-analysis', 'real-analysis', 'ordinary-differential-equations']"
2394500,How to calculate line integral over an ellipse with vector field undefined inside the ellipse?,"Given vector fields:
  $$
P=\frac{-y}{(x-1)^2+y^2}\\
Q=\frac{x-1}{(x-1)^2+y^2}
$$
  calculate $\oint_CP\,dx+Q\,dy$ over ellipse $\frac{x^2}{25}+\frac{y^2}{36}=1$. Define the domain where the function $U=\arctan\big(\frac{y}{x-1}\big)$ is the potential function to the given field. I tried doing parametrization for the curve that can de derived from ellipse:
$$
c(t)=\langle 5\cos t,6\sin t\rangle
$$
and plugging this directly into the integral. But very quickly the integral becomes quite complicated. I think there must be some trick here which I don't see. If there's a potential function then the field is conservative. But the problem is that the field is not defined at point $(1,0)$ which is inside the ellipse. For the same reason I don't think we can use Green's theorem as well.","['multivariable-calculus', 'vector-fields', 'integration']"
2394513,Geometric algebra: Rotation of a rotor,"In short my question is: Why is the rotation of a rotor in geometric algebra implemented by a single-sided rotation? To elaborate: In geometric algebra, rotating an object is done by multiplying it double-sidedly by a rotor and its inverse: $$V_{\text{rotated}}=RVR^\dagger\qquad(1)$$ A rotor is defined as a normalized element consisting of scalar part and a bivector part: $$R(\theta) = \cos(\theta/2) + \sin(\theta/2)B = e^{\theta/2 B}$$
$$RR^\dagger=1,$$ where $B$ is a normalized bivector. However it is claimed, in e.g. Doran & Lasenby's Geometric algebra for physicists, that the rotation of a rotor itself is obtained by multiplying it single-sidedly by a rotor: $$R_{1,\text{rotated by }R2} = R_2R_1$$ As a clarifying example the rotation of rotated vector is then shown: $$V_{\text{rotated by }R1\text{ and }R2}=R_2R_1VR_1^\dagger R_2^\dagger=R_{\text{tot}}VR_{\text{tot}}^\dagger$$
$$R_{\text{tot}}=R_2R_1,$$ claiming that this shows the rotor $R_1$ is rotated by left multiplication by $R_2$. I do not understand this claim. I would expect that to know how $R_1$ behaves under rotation by $R_2$, one should check that equation $(1)$, with $R=R_1$, still holds in a frame rotated by $R_2$. One should therefore rotate all elements in equation $(1)$: $$V_{\text{rotated by }R1\text{ and }R2}=R_{1,\text{rotated by }R2}V_{\text{rotated by }R2}R_{1,\text{rotated by }R2}^\dagger$$ This leads to a double-sided multiplication rule for rotors: $$R_{1,\text{rotated by }R2}=R_2R_1R_2^\dagger$$ Which is clearly different from the single sided form. Why is this incorrect?","['clifford-algebras', 'geometric-algebras', 'geometry']"
2394559,Do functions really always take only one argument?,"I am used to $f(x_{1},x_{2})$ or $f(x_{1})$ (and to say this function takes one, two etc arguments). But the more I think about it from a set-theoretic or linear-algebra point of view there is really just a single n-dimensional vector or tuple $\mathbf{x}$, which is our $\mathbf{x} = (x_{1},x_{2},...)$ input. So functions always have one input argument (be it univariate, bivariate or multivariate). Is this correct or gibberish?","['notation', 'functions']"
2394581,Understanding partial derivatives of multi-variable functions,"I'm having trouble convincing myself of the validity of the following set of steps in differentiation of a multi-variable function under a constraint: Suppose $f(x,y) = 0$ The above constraint implicitly makes $x$ dependent on $y$.
I can do the following operation 
\begin{align}
\frac{df(x,y)}{dx} = 0
\end{align}
Since the derivative is not partial, $y$ is allowed to change with changing $x$ and the constraint $f(x,y) = 0$ can still be satisfied. I know that taking partial derivative wouldn't be legal since that would make $y$ constant and the constraint will no longer be satisfied. How do I prove this formally? That \begin{align}
\frac{{\partial f(x,y)}}{{\partial x}}
\end{align} is invalid given that $f(x,y) = 0$.","['multivariable-calculus', 'partial-derivative', 'calculus', 'derivatives']"
2394582,Show that $f(x)$ is an Odd Function,"Show that $$f(x) = \ln \left(x+\sqrt{x^2+1}\right)$$ is an odd function. My attempt: $$f(-x)=\ln\left(-x+\sqrt{(-x)^2+1}\right)=\ln\left(-x+\sqrt{x^2+1}\right).$$ How should I proceed? I know that if $f(-x)=-f(x)$, the function is odd.","['logarithms', 'roots', 'functions', 'algebra-precalculus', 'fractions']"
2394585,In how many different from a set of numbers can a fixed sum be achieved?,"I have a set of number, and I want to know in how many ways from that set with each number being used zero, once or more times can a certain sum if at all, be achieved. The order doesn't matter. For example, I have a sum of '10' and set of [1,2] and I want to know in how many ways can 1 and 2 be added up to reach 10. Yes, '1' and '2' have to be used more than once. Example 10 = 1 + 1 + 2 + 2 + 2 + 2 10 = 1 + 1 + 1 + 1 + 2 + 2 + 2 10 = 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 ... There are more possibilities for the example sum. I don't need the way in which the sum is reached i.e (10 = 1 + 1 + 2 + 2 + 2 + 2 or 10 = 1 + 1 + 2 + 2 + 2 + 2 ...) just how many combinations of '1' and '2' give me 10. I am working with larger sets and numbers, so simple approach will be very useful. Thank-you No the order does not matter for the sum. Whether it is 10 = 2*one + 4*two or 4*two + 2*one or any other permutation of them. The attempt I made involves the use of a computer. I was thinking about something like this. for (i = 0; i < sum/i; i++):
    for (j = 0; j<sum/j; j++):
        if (num1*i + num2*j == sum):
            numberOfWays +=1 But this is not useful for larger sets as several nested for loops will be cumbersome. I am looking for an elegant solution. UPDATE A number can be used 0 times.","['computer-science', 'sumset', 'puzzle', 'combinatorics', 'discrete-mathematics']"
2394601,Asymmetric random walk,"Let $(X_i)_{i=1}^n$ be a sequence of i.i.d. random variables with $\Pr(X_i=1)=1-\Pr(X_i=-1)=p$, for some $0\leq p\leq 1$. Let $S_0=0$ and, for $1\le i\le n$, $S_i = X_1+X_2+\cdots+X_i$. Let $\Pi_n$ denote the number of strictly positive terms among $S_1,\ldots,S_n$. In the symmetric case, namely, when $p=1/2$, the law of $\Pi_n$  is known to be (e.g. Feller, Ch. XII.8)
$$
\Pr(\Pi_{n}=k) = {2k\choose k}{2n-2k\choose n-k}\frac1{2^{2n}}.
$$
Is the law of $\Pi_n$ (even an asymptotic expression in $n$ and $k$) known for the asymmetric case? Edit: I think that to solve the above general case, it will be suffice to say something about 
$$
\Pr(\Pi_{n}=n)
$$
which is perhaps simpler, but my attempts failed.","['random-walk', 'probability-theory']"
2394604,Proof of a theorem of Bonnet about integrals,"In this paper from 1849, Bonnet claims that it's ""very easy"" to prove the following statement Let $f,\phi:[a,b]\to \mathbb R$ be two continuous functions. Suppose $\forall x\in [a,b], A\leq \int_a^x \phi(t) dt\leq B$. If $f\geq 0$ and $f$ decreases then $\forall x\in [a,b], Af(a)\leq \int_a^x \phi(t)f(t) dt\leq Bf(a)$ Can you prove Bonnet's statement ? If $\phi \geq 0$, it's not difficult to prove $\int_a^x \phi(t)f(t) dt\leq Bf(a)$, but I can't find anything else...","['real-analysis', 'integration', 'definite-integrals']"
2394613,Minimizing the function.,"Minimizing the following function $f(x_1,x_2,\cdots,x_n)=\prod\limits_i^n x_i^{x_i}$ such that $x_1+x_2+\cdots+x_n=P, 2\le x_i$ and $x_i$ are integers. My attempt: In my opinion we obtain the result when all $x_i's$ are almost equal i.e. $|x_i-x_j|\le 0$ for all $i$ and $j$. I am trying to solve by Lagrange's multiplier and I obtained a system of equation which looks messy.","['multivariable-calculus', 'real-analysis', 'integer-programming']"
2394625,Show that $\left( \frac {11} {10}\right) ^{n}$ is divergent.,"Show that $\left( \dfrac {11} {10}\right) ^{n}$ is divergent. My proof. Let $B\in\mathbb{R}$. By the Archimedean property there is a $N$ in $\mathbb{N}$ such that $N>B$. Let $\varepsilon >0$ By the Bernoulli inequality, we have $\left( 1+\varepsilon \right) ^{n}\geq 1+n\varepsilon$ for all $n\in\mathbb{N}$. Now, take $\varepsilon=( \dfrac {11} {10}-1)$. Then, we obtain, $\left( \dfrac {11} {10}\right) ^{n}\geq \dfrac {n} {10}+1$. So, for all $n\geq N$ we have $\dfrac {n} {10}+1>\dfrac {n} {10}>n>N>B.$ Thus, since $\left( \dfrac {11} {10}\right) ^{n}\geq \dfrac {n} {10}+1$,  $\left( \dfrac {11} {10}\right) ^{n}>B$ for all $n\geq N$. We are done. Can you check my proof?","['proof-verification', 'divergence-operator', 'analysis', 'limits']"
2394646,"properties of, 3x3 matrix, determinant 1, real eigenvalues","Let A be a 3x3 matrix with determinant 1. Suppose there exists x such that 
$$\lim_{n\to\infty} (A^n x) = v$$ and $x\neq v$, $v \neq 0$. Then I can prove that $A$ must have real eigenvalues. I want to show that there exists $y$ such that $$\lim_{n\to\infty}(A^{-n}y) = v,$$ $y \neq v$. From the condition on $x$ above, we know $Av = v$, so one eigenvalue of $A$ is 1. As I mentioned above, the others are real, and so the 3 eigenvalues of $A$ are $1, \lambda, 1/\lambda$. If $\lambda > 1$, then $1/\lambda < 1,$ so let $w$ be the eigenvector corresponding to $\lambda$. $$ \lim_{n\to\infty} (A^{-n}(v + w)) = lim_{n\to\infty} v + \lambda^{-n}w = v.$$ In other words, this choice of $y$ works. However, I cannot see how to find a $y$ when all the eigenvalues of $A$ are 1. Assuming there is always a $y$ the question I am trying to solve says that $y,x,v$ are linearly independent. How can I prove this?","['eigenvalues-eigenvectors', 'linear-algebra']"
2394650,$\mathbb{Z}$ is the symmetry group of what?,"It is often said that the notion of group has a kinship with that of symmetry: many groups appear as the symmetry group of some object -- take,for example, dihedral groups. What is the object related to $\mathbb{Z}$?",['group-theory']
2394668,Evaluating $\sum_{m=j_2-j}^{j_1} \frac{(j_1+m)!(j_2+j-m)!}{(j_1-m)!(j_2-j+m)!}$ using combinatorial identities,"I'm trying to prove the identity
$$
\sum_{m=j_2-j}^{j_1} \frac{(j_1+m)!(j_2+j-m)!}{(j_1-m)!(j_2-j+m)!}=
\frac{(j+j_1-j_2)!(j-j_1+j_2)!(j+j_1+j_2+1)!}{(j_1+j_2-j)!(2j+1)!} \tag{1}
$$
where $2j_1,2j_2\in \mathbb{Z}^+$ and $\vert 2j_1-2j_2\vert\le 2j\le 2(j_1+j_2)$.  The summation over $m$ is in steps of $1$ from $j-j_2$ to $j_1$. This summation occurs as part of a normalization condition on $su(2)$ Clebsch-Gordan coefficients. Apparently to prove this one must use the binomial identities 
\begin{align}
\sum_{x} {a\choose x}{b\choose c-x}&={a+b\choose c} \tag{2}\\
{u \choose v}&= (-1)^v {v-u-1\choose v}\, .  \tag{3}
\end{align} I found in Eq.(6.1) of this volume that (2) is known as the integral Vandermonde Convolution.  (3) can be found as Eq.(2.1) of this paper. My difficulty is in seeing the relevance of (2) since the left hand side of
(1) contains the summation index $m$ in the numerator and the denominator while the dummy index in (2) appears only in the denominator. I have to deal with several variants of this summation formula so any help in bridging the gap between the left and right hand side of (1) would be appreciated.","['combinatorics', 'summation', 'combinatorial-proofs']"
2394683,Doubts regarding $L_p$ space,"I had to dive into the basic definitions and properties of $L_p$ spaces as part of a course project I am doing. (More specifically while I was trying to understand Barbalet's Lemma). It was my first time in this topic and I have got really confused now. It would be great if you could answer some of my questions with a numerical example if possible. What exactly is an essentially bounded function ? 
The definition I know of is:- function $f:[0,1]→ℝ $ is called essentially bounded if there is a number $M $ such that $|f(x)|≤M $ for almost all $ x∈(0,1)$. (That is, the inequality holds on some set $E$ such that $(0,1)$∖E has zero measure.) Why is $f(x)=x^{−1/(p+1)}$ which defines an element of $L_p((0,1))$ not essentially bounded.
I would also like to ask what is the use of equivalence class of function in such cases? Also, what would be the set consisting of ""almost everywhere"" in any appropriate example (maybe above one)? Does essentially bounded function imply it belongs to $L_{\infty}$ space? How can a function have $\|f\|_p = 0$ but $f \ne 0$. Please explain as if I don't know anything. I am not very sure how much I have understood. I am asking the question only after reading many answers and online lecture notes.","['functional-analysis', 'normed-spaces']"
2394715,Matrix involving the central moments of a random variable,"Consider the matrix :
  $$M=\begin{pmatrix} 1& 0& \mu_2 \\
0& \mu_2& \mu_3 \\
\mu_2& \mu_3& \mu_4 \end{pmatrix}$$ where $\mu_k$ denotes the $k^{th}$ central moment of a RV X i.e. $\mu_k := E(X-E(X))^k$. Prove that: (a) $M$ has non-negative determinant. (b) $M$ is non-negative definite. (c) Generalize this result to higher dimensions. My thoughts: (a) I tried to break the moments to lower order moments and to obtain the result but it didn't workout. (b) & (c) Could not come up with anything.","['expectation', 'probability-theory', 'linear-algebra']"
2394732,Solving differential equation with finite boundary condition,"This is a follow-up to my previous question , as I try to understand how to solve the differential equation under less conventional boundaries. Consider a finite diffusion in the boundary of $0 < x < l$ where $l$ is a rigid wall (the diffusing species do to pass through). $$\frac{\partial c}{\partial t} = D\frac{\partial^2c}{\partial x^2}$$
with boundary conditions of: $c(0,t) = A$ $c(l,t) = 0$ $c(x,0) = 0    \quad (l > x > 0)$ How can we solve it with the condition that causes the concentration to increase by $t$ at $x=l$. Example: $c(0,t)=A$ is the concentration at the membrane. $l$ is a rigid wall, and the concentration increases by time. I believe the concentration profile should be something like","['ordinary-differential-equations', 'partial-differential-equations']"
2394785,Can the boy escape the teacher for a regular $n$-gon?,"This is related to Prove that the boy cannot escape the teacher Suppose there is a boy in the center of a regular $n$-gon. The teacher is on the edge of the $n$-gon (but cannot leave the edge) and wants to capture the boy. At the beginning he is on a vertex. The teacher is $v(n)$ times faster than the boy. Which is the maximum $v(n)$ such that the boy can escape? (By escaping means he reaches the edge of the $n$-gon and the teacher is not there) From the linked question we know $3 \le v(4) < 6$, and for $n= \infty$ (a circle) then I know a strategy such that $v(\infty) = \pi + 1$ suffice; I don't know if this is optimal. I also put convergence in the tags because my wild hypothesis is that the maximum $v(n)$ will converge to some value and it would be interesting to know which one! Any cool way to solve this?","['analytic-geometry', 'curves', 'convergence-divergence', 'geometry']"
2394789,"For $n>1$ and a ring $R$ s.t. $k[x_1,\dots, x_n]\subset R\subset k(x_1,\dots, x_n)$ is $R$ a localization of $k[x_1,\dots, x_n]$?","Let $k$ be a field. Consider $n>1$ and a ring $R$ sitting between $k[x_1,\dots, x_n]$ and $k(x_1,\dots, x_n)=\mathrm{Frac}(k[x_1,\dots, x_n])$ . Q1: Is it true that $R$ is some localization of $k[x_1,\dots, x_n]$ as I cannot come up with a counter-example? For $n=1$ , it is clear as $k[x]$ is a Bezout domain which says everything sitting between $k[x]$ and $k(x)$ is some localization of $k[x]$ . Q2. The reason I am asking this question is that for ring of regular functions $O_{A^n}(U)$ on any open set is finite intersection of $O(D_i)$ where $D_i$ are distinguished open sets/basis covering $U$ and $O(D_i)$ . So $O_{A^n}(U)$ is realized as inverse limit of $O(D_i)$ . It looks like for most of $U\subset A^n$ , I see them as some sort of localization of $O(A^n)$ . Is there a counter example to this? Q3. $O(U)=O(\cup D_i)=\cap O(D_i)$ . $\cup D_i$ can be realized as direct limit. $O(\cup D_i)$ are regular functions from $D_i$ to $k$ which is basically $\mathrm{Hom}(-,k)$ . So $\mathrm{Hom}$ converts the direct limit to inverse limit. However this conversion is in abelian category. How should I realize this notion? Or is this notion correct?","['abstract-algebra', 'sheaf-theory', 'algebraic-geometry', 'commutative-algebra']"
2394815,"infinite abelian group where all elements have order 1, 2, or 4","Let $A$ be a (not necessarily finitely generated) abelian group where all elements have order 1, 2, or 4. Does it follow that $A$ can be written as a direct sum $(\bigoplus _\alpha \mathbb Z/4) \oplus (\bigoplus_\beta \mathbb Z/2)$?","['abelian-groups', 'p-groups', 'group-theory']"
2394824,"There is an island and 20 houses at the beach around the island, each house with 20 wrestlers.","There is an island and 20 houses at the beach around the island, each house with 20 wrestlers. Each wrestler fights with all wrestlers from other houses. There is no two wrestlers with the same power and the stronger wrestler always win. We say that house $A$ is stronger than house $B$ if there is $k$ fights in which fighters from house $A$ wins. What is a maximum $k$ if we know that each house is stronger than the neighboring house in the direction of the clock movement? I was trying to solve this was but in the end I gave up and I read an official solution. I was very unsatisfied how it was solved, because they never say how they find this $k_{\max}$ , just proved it is ok. Perhaps someone will have a different aproach here. Here is an offical solution: http://natjecanja.math.hr/wp-content/uploads/2015/02/2012_izborno-rjesenja.pdf Appeared: Serbia and Montenegro preparation test for IMO $2006$ ; III International Festival of Young Mathematicians Sozopol $2012$ ; Croatia TST for IMO $2015$ ; Swiss TST for IMO $2018$ .","['alternative-proof', 'extremal-combinatorics', 'combinatorics', 'contest-math', 'discrete-mathematics']"
2394827,"Prove that $\mathbb{Q}(\sqrt{a}+\sqrt[3]{b}) = \mathbb{Q}(\sqrt{a}, \sqrt[3]{b})$ without Galois theory","Let $a,b \in \mathbb{Z}$ be integers such that $\sqrt{a} \notin \mathbb{Z}$ and $
\sqrt[3]{b} \notin \mathbb{Z}$ (the number $a$ is allowed to be negative). I need to prove that $$ \mathbb{Q}(\sqrt{a}+\sqrt[3]{b}) = \mathbb{Q}(\sqrt{a}, \sqrt[3]{b})$$ but I cannot use any Galois theory to do so, as we have not gotten to it yet in my course. I was given a hint: ""let $G$ and $H$ denote the left-hand side and the right-hand side, respectively; analyze possibilities for the dimension $[H:G]$ and use thee fact that $G(\sqrt{a}) = G(\sqrt[3]{b}) = H$"". However, instead, I approached it similarly to Paramanand Singh's answer to this question . But, I am not sure such an approach is correct in this situation. Anyway, this is what I did, according to Paramanand's method: It is obvious that $\mathbb{Q}(\sqrt{a} + \sqrt[3]{b}) \subseteq \mathbb{Q}(\sqrt{a}, \sqrt[3]{b})$, since $\sqrt{a} + \sqrt[3]{b}$ is a linear combination of $\sqrt{a}$ and $\sqrt[3]{b}$ Now, for the other side, let $c = \sqrt{a}+\sqrt[3]{b}$. WTS: $e=\sqrt{a}$, $f = \sqrt[3]{b}$ are rational functions of $c$. $$ (c-e)^{3} = b \, \to \, c^{3}-3c^{2}e + 3ac - ae = b \, \to \, 3c^{2}e + ae = -b + c^{3}+3ac \, \to \, e = \frac{c^{3} + 3ac - b}{3c^{2} + a}\, \to \, \sqrt{a} = \frac{c^{3} + 3ac -b }{3c^{2}+a} $$ So that $\sqrt{a}$ is a rational function of $c$. Also, since $c = e + f = \sqrt{a} + \sqrt[3]{b}$, we have that $\sqrt[3]{b} = c - \sqrt{a}$, so that $\sqrt[3]{b}$ is also a rational function of $c$. Therefore $\mathbb{Q}(\sqrt{a}, \sqrt[3]{b}) \subseteq \mathbb{Q}(\sqrt{a} + \sqrt[3]{b})$ However, something about this is not sitting right with me. Is this, in fact, the correct way to prove it? Or should I do it the way my professor suggested? I would also like some help doing it that way. I guess for that way, we could still start out by stating that ""It is obvious that $\mathbb{Q}(\sqrt{a} + \sqrt[3]{b}) \subseteq \mathbb{Q}(\sqrt{a}, \sqrt[3]{b})$, since $\sqrt{a} + \sqrt[3]{b}$ is a linear combination of $\sqrt{a}$ and $\sqrt[3]{b}$"" (if this is, in fact, true). Then, $\mathbb{Q}(\sqrt{a}, \sqrt[3])$ has degree $6$, right? So, I would need to show that $\mathbb{Q}(\sqrt{a} + \sqrt[3]{b})$ also has degree $6$, but I still don't see what this has to do with the hint... If I let $G = \mathbb{Q}(\sqrt{a} + \sqrt[3]{b})$ and $H = \mathbb{Q}(\sqrt{a}, \sqrt[3]{b})$, then of course $G(\sqrt{a}) = \mathbb{Q}(\sqrt{a} + \sqrt[3]{b})(\sqrt{a}) = \mathbb{Q}(\sqrt{a} + \sqrt[3]{b})(\sqrt[3]{b})$, but I still don't see how this helps! Could someone please explain this to me? I would be most appreciative! Thank you!","['abstract-algebra', 'extension-field', 'polynomials', 'rational-numbers']"
2394840,"Showing the function $f(x)=0$ on $[a,b]$","Suppose for real numbers $a<b$ one has a function with continuous derivative 
$$f:[a,b]\to \mathbb{R}$$
such that $f(a)=0$ and there exists a real number $C$ with 
$$|f'(x)|\leq C|f(x)|\:\:\:\text{for all}\:\:x\in [a,b].$$ Show that $f(x)=0$ for all $x\in[a,b].$ Given $\epsilon >0$ there exists a $\delta >0$ such that for any $x\in B_{\delta}(a)$ we have $|f(x)|<\epsilon $, that is $f(x)=0$ in that neighborhood.  Note that $f$ is continuous on a compact set. If $f$ is nonzero, there exists $x\in[a,b]$ such that $f(x)\neq 0$. Let $w=\inf\{x:f(x)\neq 0\}.$
 Hence for any  $x<w$, we have $f(x)=0. $ Now considering the following, we get $f(w)=0$ which is a contradiction. $$|\frac{f(w)-f(a)}{w-a}|=|f'(\xi)|\leq C|f(\xi)|, \:\:\text{where}\:\:a<\xi<w.$$ Is my my argument valid? I also didn't use the continuity of the derivative. Thank you!","['derivatives', 'proof-verification', 'calculus', 'continuity', 'analysis']"
2394845,"If $V$ is Volterra Operator, proof that $\|V^n\| = \frac{1}{(n-1)!}$","Define the Volterra Operator $V:C([0,1])\to C([0,1])$ given by
$$ Vf(x) = \int_0^x f(t) \, dt, \quad f \in C([0,1]). $$
I'm asked to proof that $\|V^n\| = \frac{1}{(n-1)!}$. My attempt:
First, I'm gonna proof that 
$V^n f (x) = \frac{1}{(n-1)!} \, \int_0^x (x-t)^{n-1} f(t) \, dt  \quad (I)$. Which holds for $n=1$, assume that $(I)$ holds for some $n > 1$. Then, $$ V^{n+1} f(x) = \int_0^x V^n f(t) dt = \int_0^x \left [ \frac{1}{(n-1)!}\int_0^t (t-s)^{n-1} f(s) ds \right ] dt =\\ \frac{1}{(n-1)!} \int_0^x f(s)\left [\int_s^x (t-s)^{n-1} \right ] ds  = \frac{1}{n!} \int_0^x (x-s)^n f(s) ds.$$ $$|V^n f (x)| \leq \frac{1}{(n-1)!} \, \int_0^x |x-t|^{n-1} |f(t)| \, dt \leq \frac{\|f\|_0}{(n-1)!} \, \int_0^x (x-t)^{n-1}  \, dt = \frac{\|f\|_0}{n!} x^n $$ Hence, $\|V^n f \| \leq \frac{1}{n!} \, \|f\|_0 \Rightarrow \|V^n\| \leq \frac{1}{n!}$ and $V^n 1 (x) = \frac{x^n}{n!}$, i.e $\| V^n 1 \|_0 = \frac{1}{n!}$. With this I conclude that $\|V^n \| = \frac{1}{n!}$ and not $\frac{1}{(n-1)!}$. What Am I doing wrong in this exercise?",['functional-analysis']
2394851,Symmetric random walk,"The following question is somewhat related to: Asymmetric random walk Let $(X_i)_{i=1}^n$ be a sequence of i.i.d. random variables with $\Pr(X_i=1)=\Pr(X_i=-1)=1/2$. Let $S_0=0$ and, for $1\le i\le n$, $S_i = X_1+X_2+\cdots+X_i$. Let $\Pi_n$ denote the number of strictly positive terms among $S_1,\ldots,S_n$. In the symmetric case, namely, when $p=1/2$, the law of $\Pi_n$  is known to be (e.g. Feller, Ch. XII.8)
$$
\Pr(\Pi_{n}=k) = {2k\choose k}{2n-2k\choose n-k}\frac1{2^{2n}}.
$$
I am trying to find asymptotic (in $k,m,n$) formula for the number of positive terms given the value of the last term, or more generally,
$$
\Pr(\Pi_{n}=k,S_n=m)
$$
Of course not all values of $k,m$ are possible. Also, clearly, the additional event that $S_n=m$ breaks the nice symmetry of the process $(X_i)_{i=1}^n$, so in some sense $(X_i)_{i=1}^n$ can be thought as being drawn uniformly over some subspace of the hyper-cube. Note that this probability can be probably upper bounded by removing the $S_n=m$ constraint but assuming that $(X_i)_{i=1}^n$ is now an asymmetric process with success probability that is proportional to $m/n$.","['random-walk', 'probability-theory', 'probability']"
2394853,Calculation of the $s$-energy of the Middle Third Cantor Set,"As the title suggests, I am trying to calculate the $s$-energy of the middle third Cantor set. I am reading Falconer's Fractal Geometry book, available here: http://www.dm.uba.ar/materias/optativas/geometria_fractal/2006/1/Fractales/1.pdf and this is an exercise (exercise 4.9 on page 45 of the pdf - 68 of the book). First, we define the $s$-potential at a point $x\in\mathbb{R}^{n}$ as $$\phi_{s}(x)=\int\frac{\text{d}\mu(y)}{|x-y|^{s}}$$ and the define the $s$-energy as $$I_{s}(\mu)=\int\phi_{s}(x)\text{d}\mu(x)=\iint\frac{\text{d}\mu(x)\text{d}\mu(y)}{|x-y|^{s}}.$$ Now, let $F$ be the middle third Cantor set and let $\mu$ be the mass distribution on $F$ so that each $2^{k}$ $k$th level interval of length $3^{-k}$ has mass $2^{-k}$. Estimate the $s$-energy of $\mu$ for $s<\log{2}/\log{3}$, and deduce that $\dim_{\text{H}}F\geq\log{2}/\log{3}$. I get the feeling that this isn't an expecially hard exercise, but I'm stuck on where to start. Could someone help me please?","['potential-theory', 'dimension-theory-analysis', 'measure-theory', 'fractals']"
2394858,Singular $p$-subgroups of a finite group,"Let $G$ be a finite group and $p$ be a prime. Suppose that there is a $p$-subgroup $S$ of $G$ such that there is only one Sylow $p$-subgroup containing it. Let $N$ be a normal subgroup of $G$. Is it true that there is only one Sylow $p$-subgroup of $G/N$ containing $SN/N$? I think the answer is yes, but I cannot prove it; if so, can you give me any hint to prove it? I know that the Sylow $p$-subgroup of $G/N$ are of the type $QN/N$ where $Q$ is a Sylow $p$-subgroup of $G$, but how can one use this?","['finite-groups', 'abstract-algebra', 'group-theory', 'sylow-theory']"
2394946,Uniform Convergence of $(\sin x)^n$,"Is the sequence of functions $(\sin(x))^n$ uniformly convergent in $[0,\pi]$? Can you give me a hint or solution, I have already already prove that it is UC in $[0,1]$ but I don't know how to proceed in $[1,\pi]$.","['sequence-of-function', 'real-analysis', 'uniform-convergence']"
2394978,Are there axioms for dividing by zero that don't destroy math?,"I concluded that - in a certain sense - all fractions are made up. That is, when we say $$\frac{a}{b}=x$$
We are merely declaring the number $x$ to satisfy the equation: $$b\cdot x=a$$
Of course there is much more to fractions than this. But it naturally came next - why not define division by zero to be exist? So write that
$$\omega=\ ""\frac{1}{0}"" \quad\text{meaning simply that}\quad \omega\cdot 0 =1$$
We declare $\omega$ to have this property $\textit{and only this property}$. If we were to give $\omega$ the same properties every other number has then math is quickly ""destroyed"" - meaning every number is equal to every other number. The next goal is to give $\omega$ as many properties of other integers as possible without destroying algebra. For an example failed attempt, let's give $\omega$ $\textbf{distributivity}$ and an $\textbf{additive inverse}$. We have
$$1=\omega\cdot 0=\omega\cdot(1-1)=\omega-\omega=0$$
and it happened. It seems safe to give $\omega$ the following commutative properties
$$\omega \cdot a=a\cdot\omega$$
$$\omega + a = a + \omega = \omega$$ Is there a foolproof way to check if giving $\omega$ some particular property is ""safe""? (i.e. leaves us with a consistent number theory)","['number-theory', 'abstract-algebra', 'axioms']"
2395000,Dimension of the kernel of a square matrix.,If I have an $n$ by $n$ matrix. Is the dimension of the kernel always equal to the number of zero rows when a matrix is in rref form? I believe it is but if so why is this the case and what would be a proof? Thanks.,"['matrices', 'gaussian-elimination', 'linear-algebra', 'linear-transformations']"
2395048,Give a sequence such that root test works while ratio test fails,"Question : Give a sequence $(a_n)_{n=1}^\infty$ with $a_n>0$ such that root test works while ratio test does not work, that is, 
  $$\lim_{n\rightarrow\infty}(a_n)^{\frac{1}{n}} \text{ exists}$$
  while 
  $$\lim_{n\rightarrow\infty}\frac{a_{n+1}}{a_n} \text{ does not exist}.$$ My attempt: Define a sequence $(a_n)_{n=1}^\infty$ such that 
    $$ a_n =  \begin{cases} 
      2^{n-1} & \text{if }n \text{ is odd,} \\
	  2^{n+1} & \text{if }n \text{ is even}.
   \end{cases}
	$$ Odd subsequence $(a_{n_j})_{j=1}^\infty$ implies that $\lim_{j\rightarrow\infty}(a_{n_j})^{\frac{1}{n_j}} = \lim_{j\rightarrow\infty} 2^{1-\frac{1}{n_j}} = 2 $ while even subsequence $(a_{n_k})_{k=1}^\infty$ implies that $\lim_{k\rightarrow\infty}(a_{n_k})^{\frac{1}{n_k}} = \lim_{k\rightarrow\infty} 2^{1+\frac{1}{n_k}} = 2.$
    It follows that $\lim_{n\rightarrow\infty}(a_n)^{\frac{1}{n}} = 2,$ that is, root test works. 
    However, for odd subsequence $(a_{n_j})_{j=1}^\infty,$
    $$\lim_{j\rightarrow\infty}\frac{a_{n_j+1}}{a_{n_j}} = \lim_{n\rightarrow\infty} \frac{2^{n_j+1}}{2^{n_j-1}} = \lim_{n\rightarrow\infty} 4 = 4.$$
    For even subsequence $(a_{n_k})_{k=1}^\infty,$
    $$\lim_{k\rightarrow\infty}\frac{a_{n_k+1}}{a_{n_k}} = \lim_{k\rightarrow\infty} \frac{2^{n_k-1}}{2^{n_k+1}} = \lim_{k\rightarrow\infty} \frac{1}{4} = \frac{1}{4}.$$ Therefore, $\lim_{n\rightarrow\infty}\frac{a_{n+1}}{a_n}$ does not exist, that is, ratio test does not work. Does my example work?","['real-analysis', 'examples-counterexamples', 'limits', 'proof-verification', 'sequences-and-series']"
2395052,Why are Baire functions defined in terms of ordinals?,"The Baire-$0$ functions, $\mathcal{B}_{0}$, are defined as the continuous real functions, and the Baire-$n$ functions, which I denote as $\mathcal{B}_{n}$, are defined as functions which can be expressed as pointwise limits of a sequence of functions $\{f_k\}_{k \geq 1}$ where each $f_k \in \bigcup_{0\leq i < n} \mathcal{B}_{i}$. According to Wikipedia, some authors require the additional condition that if $f \in \mathcal{B}_{n}$, then $f \notin \mathcal{B}_{i}$ for any $i <n$. Wiki also states that Baire functions are defined in terms of ordinal numbers. In other words, we consider Baire functions of class $\alpha$ for any countable ordinal $\alpha$ instead of Baire functions of class $n$ where $n$ is a natural number (zero inclusive). What exactly goes wrong if we work with natural numbers? Why do we need to use ordinals? I don't know much about ordinals, so sorry if this is a naive question.","['real-analysis', 'elementary-set-theory']"
2395053,Why are naturals < modulus equal if congruent? [uniqueness of remainder in a consecutive residue system],"Let $a$ , $b$ , and $n$ be integers with $0 \le a,b \lt n$ . If $\,a \equiv b \pmod n,\,$ i.e. $\,a-b = pk\,$ for an integer $k,\,$ then $a=b$ . Why does this congruence imply the equality?","['abstract-algebra', 'modular-arithmetic', 'elementary-number-theory']"
2395060,Find $x\in \Bbb Z$ such that $x=\sqrt[3]{2+\sqrt{5}}+\sqrt[3]{2-\sqrt{5}}$,"Find $x\in \Bbb Z$ such that $x=\sqrt[3]{2+\sqrt{5}}+\sqrt[3]{2-\sqrt{5}}$ Tried (without sucess) two different approaches: (a) finding $x^3$ by raising the right expression to power 3, but was not able to find something useful in the result that simplifies to an integer; (b) tried to find $a$ and $b$ such that $(a+\sqrt{b})^3=2+\sqrt{5}$ without success. The answer stated for the problem in the original source (a local Math Olympiad Constest) is $x=1$.","['radicals', 'cubics', 'algebra-precalculus', 'contest-math', 'nested-radicals']"
2395070,Show that $M=\alpha I_n\iff$ no matrix in $S$ has a zero anywhere on its diagonal.,"Let $M$ be a square complex matrix, and let $S = \{XMX^{-1} | \det(X)\neq 0 \}$ i.e.  $S$ is the set of all matrices similar to $M$
  . Show that $M=\alpha I$ for some $\alpha\neq 0$ if and only if no matrix in $S$ has a zero anywhere on its diagonal. The only if part is trivial. No idea on the other half...","['matrices', 'linear-algebra']"
2395094,Using Stokes' theorem to define the exterior derivative operator,"In the excellent paper ""Differential forms and integration"" by Terence Tao, the author has mentioned that ""... one can view Stokes' theorem as a definition of the derivative operation $\omega\rightarrow d\omega$,  thus differentiation is the adjoint of the boundary operation"". My question is how exactly (rigorously) can one define the exterior derivative via Stokes' theorem? Where can I read about that in detail?",['differential-geometry']
2395234,$\sin^3 a\sin(b-c)+\sin^3b\sin(c-a)+\sin^3c\sin(a-b)+\sin(a+b+c)\sin(b-c)\sin(c-a)\sin(a-b)=0$,"Verify that$$\sin^3 a\sin(b-c)+\sin^3b\sin(c-a)+\sin^3c\sin(a-b)$$
  $$+\sin(a+b+c)\sin(b-c)\sin(c-a)\sin(a-b)=0.$$ I tried to break $\sin^3a$ into $\sin^2\cdot\sin a$ and use it to make sum with $\sin(b-c)$, but eventually messed up everything. Please help.","['algebra-precalculus', 'summation', 'trigonometry']"
2395247,"Contour method to show that $\int_0^\infty\frac{x-\sin x}{x^3} \, dx=\frac\pi4$","Show that $$\int_{0}^{\infty} \frac{x - \sin(x)}{x^3} \, dx = \frac{\pi}{4}$$ My attempt is as follows: Let $$f(z) = \frac{z - i e^{iz}}{z^3}$$ and consider the contour on $[\epsilon, R]$ followed by a semicircular arc in the counter clockwise direction, then on $[-R, -\epsilon]$, then the semicircular clockwise contour avoiding the origin. We have, then, that $$0 = \int_{\Gamma} f(z) dz = \int_{[\epsilon, R]} f(t) dt + \int_{C_R}f(Re^{it})Rie^{it}dt + \int_{[-R, -\epsilon]}f(t)dt + \int_{C_{\epsilon}}f(\epsilon e^{-it})\epsilon i e^{-it} dt$$ Then the first and third integrals ( $I_1$ and $I_3$) combine so that $$I_1 + I_3 = 2\int_{\epsilon}^R \frac{t - \sin{t}}{t^3}\,dt$$ Further, $$|I_{C_R}| \leq \int_0^\pi \left|\frac{Re^{it} - ie^{-R\sin{t}}e^{iRcos{t}}}{R^2 e^{2it}} \right|dt \rightarrow 0 \text{ as } R\rightarrow \infty$$ (I've omitted the details, it isn't too bad to bound) However, I'm having trouble computing the limit $$\lim_{\epsilon \rightarrow 0}\int_{C_{\epsilon}} f(\epsilon e^{-it})\epsilon i e^{-it} dt$$ No matter which way I look at it, it seems like this limit does not exist. Perhaps I'm seeing something wrong or have I chosen a bad $f(z)$?","['complex-analysis', 'contour-integration']"
2395270,"$f(x)=\frac{x^2 -4}{x-2}$, Can $x=2$ or not? [duplicate]","This question already has an answer here : Graphing $y = \frac{x^2-4}{x-2}$ (1 answer) Closed 6 years ago . Given: $$f(x)=\frac{x^2-4}{x-2}$$ First thoughts would be than $x\ne 2$ But since $f(x)$ can be simplified to: $$f(x)=x+2$$ It now seems than $x$ can be a solution to $f(x)$. If I graph this function I get the line $f(x)=x+2$, but it that with the point $(2,4)$ not included? My book gives an answer where $x\ne 2$","['algebra-precalculus', 'functions']"
2395298,Focal point free geodesics are locally length minimizing (Jost Exercise 4.2),"I'm working on the following Exercise from Jost's Riemannian Geometry and Geometric Analysis (Exercise 4.2). Let $M$ be a submanifold of the Riemannian manifold $N$, $c:[a,b]\to N$ geodesic with $c(a)\in M$, $\dot{c}(a)\in (T_{c(a)}M)^\perp$. For $\tau\in(a,b]$, $c(\tau)$ is called a focal point of $M$ along $c$ if there exists a nontrivial Jacobi field $X$ along $c$ with $X(a)\in T_{c(a)}M$, $X(\tau)=0$.
  Show: If $M$ has no focal point along $c$, then for each $\tau\in(a,b)$, $c$ is the unique shortest connection to $c(\tau)$ when compared with all sufficiently close curves with initial point in $M$. I initially tried solving this by generalizing the techniques used for showing the analogous result for conjugate points. For part (1) I began by showing that the focal points of $M$ are exactly the critical values of $\exp^\perp$, where $\exp^\perp$ is the exponential map restricted to the normal bundle, $TM^\perp$. Using this and the fact that we are assuming there are no focal points of $M$ along $c$ we have that $\exp^\perp_{c(a)}$ is of maximal rank along any radial curve $t\mapsto tv$ for $t\in[0,1]$ with $v\in T_{c(a)}M^\perp$. By the inverse function theorem we see that for every $t$ that $\exp_{c(a)}^\perp$ is a diffeomorphism in a suitable neighborhood of $tv\in T_{c(a)}M^\perp$. By compactness, we can cover $\{tv~:~0\leq t\leq 1\}$ by finitely many neighborhoods $\Omega_i$ for $i=1,\dots,k$. Now define $U_i:=\exp_{c(a)}^\perp\Omega_i$. Here's where I'm getting stuck: I would like to find sufficiently small $\varepsilon > 0$ such that if $d(g(a),c(a))<\varepsilon$, $g(\tau)=c(\tau)$, and $d(g(t),c(t))<\varepsilon$ for all $t\in[a,\tau]$, then $g([t_{i-1},t_{i}])\subseteq U_i$. How do I find such an $\varepsilon$? I'm not sure how to deal with the issue that $g$ can have different initial conditions in $M$. Any ideas or hints would be extremely helpful. Maybe there's a more efficient way to go about proving this result altogether!","['riemannian-geometry', 'differential-geometry', 'geodesic']"
2395394,Solving P(TRUE) of finding counterpart pairs in 2 sets with constraints involving the Universal Genetic Code,"Solving P(TRUE) of finding counterpart pairs in 2 sets with constraints involving the Universal Genetic Code Posting this question here as opposed to in biology, etc. as I only want the answer from a purely mathematical standpoint without any contextual bias. I apologize for the lack or incorrect usage of formal notation.  Did my best. I do not know if this can be done via some form of clever mathematics or I'd just need to brute force through with a computer(s) in some way to obtain a sample and then extrapolate that to the population. Regardless, here is my question. ( My use of ""|"" ANYWHERE means OR ) $$D = D\_EitherUnlessInitialSpecificed = (D\_Initial | D_CounterpartToInitial)$$ $$B = B\_EitherUnlessInitialSpecificed = (B\_Initial | B\_CounterpartToInitial)$$ Yes, I wrote that redundantly, but hopefully clearly.I hope I wrote that correctly. Definition of counterparts: a , c , g , and t do not represent anything quantifiable.  They only represent a quality. As stated more generally above, and more specifically down below: $$D\; can\; only = a\;  or \; c\; ; \; a < -- > c$$
$$if\;  D = a,\; then \;D_c = c$$$$if\;  D = c,\; then \;D_c = a$$ $$B\; can\; only = t\;  or \; g\; ; \; t < -- > g$$
$$if\;  B = t,\; then \;B_c = g$$$$if\;  B = g,\; then \;D_c = t$$ In the end, everything comes down to the placement of $a$, $c$, $g$, and $t$ within constraints I'll define shortly. $$W = (D\;|B)$$ and we'll be using 2 instances of
$W$, $W_1$ & $W_2$ which are not exclusive of each other in any context, nor are they exclusive/equal when the same instance is repeated in the same or a different set/array/etc. They are more like placeholders for a position which you're trying to find the counterpart to. If I use $W_2$ or $W_1$ and then use it again multiple times, each time it takes on a value which is assigned actually only by you. Now, $$W_1 = (D | B)$$$$W_2 = (D | B)$$ $$Wa\; =\; an\; instance\; of\; the 
\;set \;or\; what\; I\; am\; going \;to \;begin \;calling\; an\;""array""\; \{W_1,W_2\}$$ We'll make 8 individual instances of these paired arrays and then store them into an array, $ArrayTop$ aka $ArrT$ $$ArrT\;=\;\{Wa_1,Wa_2,Wa_3,Wa_4,Wa_5,Wa_6,Wa_7,Wa_8\}$$ You can also reference the items in the array from their order via 
$ArrT_1,ArrT_2...etc.$ if needed. Now, we'll make 16 more individual instances of these paired arrays and then store them into another array, $ArrayBottom$ aka $ArrB$. However, we'll start $Wa_x$ at 9. $$ArrB\;=\;\{Wa_9,Wa_{10},Wa_{11},Wa_{12},Wa_{13},Wa_{14},Wa_{15},Wa_{16},Wa_{17},Wa_{18},Wa_{19},Wa_{20},Wa_{21},Wa_{22},Wa_{23},Wa_{24}\}$$ or $$ArrB_1, ArrB_2...ArrB_{16}$$ If, given $ArrT$ & $ArrB$ and these constraints , $$Each$$
$$ a, c, g,\ and\ t$$ $$has \;a \;required \;""point\; usage"" \;of \;exactly \;16\; for\; W_1 \;collectively\; and\; W_2\; collectively.$$ Every time one of these 4 letters is used in: $ArrT$, it acquires 4 points. $ArrB1$ or $ArrB2$, then it acquires 3 points. $ArrB3$ through to $ArrB14$, then it acquires 2 points. $ArrB15$ or $ArrB16$, then it acquires 1 point. To clarify further, a letter acquires points for it's $W_1$ and separately for its $W_2$. Find the number of all possible scenarios for the placements of 
$$a,t,g,\; and\; c$$
where, for each item in $ArrT$, two items in $ArrB$ contain the respective counterparts of $W_1$ and $W_2$ from that item in $ArrT = TRUE$. Also, the total # of possible scenarios if you want, although that's somewhat easy to figure out. EDIT: I have included a picture to provide the context for what this is for and where it comes from to hopefully help resolve any confusion. One very important thing I need to stress here and which can cause confusion with this picture is the fact that there are actually a few symmetries with very low probabilities to occur especially in tandem between the two halves (pending you're looking for them).  I will say that this is our genetic code for everything on earth...so that's kind of cool but the question I'm asking has absolutely nothing to do with symmetry and only the presence of things in the top group that then have their counterparts in the bottom regardless of their location.  If you're just looking at how I define the problem above in the mathematical sense you would already understand that though. You can basically ignore the upper left part too....the picture explains the rest.","['probability-theory', 'probability']"
2395406,"Every open cover of $[0,1]$ has finite subcover","I am understanding proof of theorem stated in title from Spivak's calculus. It is as below. (0) Let $\mathcal{O}$ be an open cover of $[0,1]$. (1) Let $A=\{x\in [0,1]:[0,x] \mbox{ has finite subcover from } \mathcal{O}\}$. (2) Then $A$ is non-empty, bounded above by $1$; let $\alpha$ be its supremum. (3) Since $\mathcal{O}$ is open cover of $[0,1]$,  $\alpha$ is in some $U$ from $\mathcal{O}$. (4) There is an open interval $J$, $\alpha\in J\subseteq U$ s.t.all points of $J$  to the left of $\alpha$ are also in $U$. (5) Since $\alpha$ is supremum of $A$, there is an $x\in J$ such that $x\in A$. How? (6) Then $[0,x]$ is covered by finite subcover; this together with $U$ covers $[0,\alpha]$; so $\alpha\in A$. (7) One tries to prove that $\alpha=1$, and proof will complete. Q.1 It is in step 5, which I don't understand. Q.2 Are there  different proofs of this theorem? (I don't find other  in 5-6 standard books than this).","['real-analysis', 'metric-spaces', 'compactness', 'proof-explanation']"
2395417,Why are all circles similar? (Why is $\pi$ a constant?) [duplicate],"This question already has answers here : Proof that Pi is constant (the same for all circles), without using limits (7 answers) Closed 6 years ago . I just know that I'm going to look like a crackpot, but here goes. The number $\pi$ is defined as the ratio of the circumference of a circle to its diameter. So there is an assumption here that all circles are similar. Using undergraduate calculus (Moise), I can analytically convince myself that this is true. But how would a 5th century geometer prove this synthetically?","['math-history', 'geometry']"
2395441,Determinant of sum of squares of two matrices.,"Problem : $\rm P$ and $\rm Q$ such that $\rm P^3 = Q^3$, $\rm Q^2P = P^2Q$ and $\rm P \ne Q$. Find $\rm \det(P^2 + Q^2)$ if (i) If both matrices are $3\times 3$ (ii) If both matrices are $n\times n$,  for $n \in \Bbb N$ $$\rm (P^2 + Q^2)(P+ Q) = P^3 + P^2Q + Q^2P + Q^3 = 2(P^3 + P^2Q) = 2P^2(P + Q)\tag{1}$$ Taking determinant on both sides, $$\rm \det(P^2 + Q^2)\det(P+ Q) = 2^n\det(P^2)\det(P+Q) \\ \implies \det(P^2 + Q^2) = 2^n\det(P^2) = 2^n (\det(P))^2$$ Also, $$\rm Q^2P = P^2Q \\ \implies  (\det (Q))^2 \det P = (\det P)^2 \det(Q) \\\implies  \det P\det Q (\det Q - \det P) = 0 $$ Therefore $\rm \det P = 0$ or $\rm \det Q = 0$ or $\rm \det P = \det Q$. If $\rm\det P =0$ then I can say $\rm \det(P^2 + Q^2) = 0$ but I am not sure about other case, i.e when $\rm \det Q = 0$ or $\rm \det P = \det Q$. Can I still say $\rm \det(P^2 + Q^2) = 0$ ?","['matrices', 'contest-math', 'linear-algebra', 'determinant']"
2395449,$Ax=y$ has a solution for every $y \in Y$.,"Let $X$ be a linear space, $Y$ be a normed linear space and $A:X \to Y$ be a linear operator. Suppose $y_0 \in R(A)$ and $r>0$ are such that the equation $Ax=y$ has solution for every $y\in B_Y(y_0,r)$. Show that $Ax=y$ has a solution for every $y \in Y$. Need some hints to proceed with the problem.","['functional-analysis', 'normed-spaces', 'linear-algebra']"
2395460,"If $x^3+y^3+(x+y)^3+33 xy=2662$, $x,y\in \Bbb R$, find $S=x+y$.","If $x^3+y^3+(x+y)^3+33 xy=2662$ and $\{x,y\}\subset \Bbb R$, find $S=x+y$. This question from an olympiad contest. Answer stated: $S=x+y=11$ Tried to develop $(x+y)^3$ to find something useful for the situation, but without success.","['inequality', 'polynomials', 'substitution', 'algebra-precalculus', 'contest-math']"
