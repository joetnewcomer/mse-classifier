question_id,title,body,tags
4852140,Function is decreasing,"I am pushing this calculation a little further and editing it. Perhaps someone could help. The function $s(t)$ defined as $$
s(t) = -\alpha r_i -\alpha r_0 \left( \frac{\sum \limits_{h=j}^{i-1}{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}}}{\sum \limits_{h=j}^i{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}} \right)
$$ with $$
\Gamma_{hi}^{\left(j\right)}=\prod \limits_{l=j, l\neq h}^i \left(\gamma^h-\gamma^l \right), r_i = r_0 \gamma^i
$$ where $\gamma > 0, N_0 > 0, r_0 > 0 $ .
It can be easily plotted and seen that this function is always decreasing. However, I want to show this analytically. My attempt is as under, but I am stuck on how to proceed further. Any other method is also appreciated. $$
s'(t) = \frac{\alpha^2 r_0}{N_0} \left(\frac{\sum \limits_{h=j}^i{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}} \times \sum \limits_{h=j}^{i-1}{\gamma^h \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}} - \sum \limits_{h=j}^{i-1}{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}} \times \sum \limits_{h=j}^{i}{\gamma^h \frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}{\left( \sum \limits_{h=j}^i{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}} \right)^2} \right)
$$ If $s(t)$ is always decreasing, then $ s'(t) < 0, \forall t $ , and we must have $$
\sum \limits_{h=j}^i{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}} \times \sum \limits_{h=j}^{i-1}{\gamma^h \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}} - \sum \limits_{h=j}^{i-1}{\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}} \times \sum \limits_{h=j}^{i}{\gamma^h \frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}} < 0
$$ or $$
\frac{\sum \limits_{h=j}^{i-1}{\gamma^h \frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}}}{\sum \limits_{h=j}^{i-1}{\frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}}}
 < \frac{\sum \limits_{h=j}^i{\gamma^h\frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}} {\sum \limits_{h=j}^{i}{ \frac{e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}
$$ Obviously, $\Gamma_{hi}^{\left(j\right)}$ has one factor more than $\Gamma_{h,i-1}^{\left(j\right)}$ , and therefore, due to the involvement of the summation index $h$ , these can't be compared. However, taking their absolute values, one can compare them easily. Thus $$
\lvert{\frac{1}{\Gamma_{hi}^{\left(j\right)}}} < \lvert{\frac{1}{\Gamma_{h,i-1}^{\left(j\right)}}}\rvert \; \; \;
{\&} \; \; \;
\lvert{\frac{\gamma^h}{\Gamma_{hi}^{\left(j\right)}}}\rvert < \lvert{\frac{\gamma^h}{\Gamma_{h,i-1}^{\left(j\right)}}}\rvert, \;\; \text{as $\gamma > 1$ }
$$ Now $$
   \lvert{\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}\rvert
    \le
    \sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{hi}^{\left(j\right)}}\rvert}}
   \le
\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}
$$ $$
\implies
\lvert{\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}\rvert
    \le
    \sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}
$$ Further, by dropping the $i^{th}$ term from the summation, we have $$
\sum \limits_{h=j}^{i-1}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}
    <
    \sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}
$$ also, $$
\lvert{\sum \limits_{h=j}^{i-1}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}}}\rvert
    \le
    \sum \limits_{h=j}^{i-1}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}
$$ Since $\gamma > 1$ , therefore, $$\gamma^h > 0 \quad \text{for any} \quad h \in \mathbb{Z}^+$$ . Thus from above, we can write $$
 \lvert{\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}\rvert
    <
    \sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}}\rvert}.
$$ Now combining the above $$
    \frac{\lvert{\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}\rvert}
    {\lvert{\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}\rvert}
    >
    \frac{\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}}
    {\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}}
    \ge
    \frac{\sum \limits_{h=j}^{i-1}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}}
    {\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}}
$$ By the $\triangle$ ular inequality on the right numerator, we can wirte $$
    \frac{\lvert{\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}}}\rvert}
    {\lvert{\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{hi}^{\left(j\right)}}\rvert}}}
    >
    \frac{\sum \limits_{h=j}^{i}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}}}\rvert}
    {\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}}
    \ge
    \frac{\lvert{\sum \limits_{h=j}^{i-1}{ \frac{\gamma^h  e^{-\alpha\frac{\gamma^h}{N_0} t}}{\Gamma_{h,i-1}^{\left(j\right)}}}}\rvert}
    {\sum \limits_{h=j}^{i}{ \frac{ e^{-\alpha\frac{\gamma^h}{N_0} t}}{\lvert{\Gamma_{h,i-1}^{\left(j\right)}}\rvert}}}
$$ Now how to proceed further?","['complex-analysis', 'analysis', 'real-analysis']"
4852153,Directional derivative confusion about terminology,"I am confused about a definition of directional derivatives I found in the MIT Deep Learning book. The directional derivative in direction $u$ (a unit vector) is the slope of the
function $f$ in direction $u$ . In other words, the directional derivative is the derivative
of the function $f (x + \alpha u)$ with respect to $\alpha$ , evaluated at $\alpha = 0$ .
Using the chain rule, we can see that $ \frac \partial {\partial \alpha} f(x + \alpha u) $ evaluates to $u^T \nabla_{x} f(x)$ when $ \alpha = 0$ I understand the first sentence but I don't understand what they are trying to say after that. What I know: If $f: \mathbb{R}^n \to \mathbb{R}$ , the directional derivative of $f$ at $x_0$ in the direction of the vector $v$ is defined as the limit $$D_v f(x_0) = \lim_{h\to 0}\frac{f(x_0+hv)-f(x_0)}{h}.$$ Some people use $ \frac {\partial f} {\partial x}$ to talk about $f'(x)$ What I am confused about: derivative of the function $f (x + \alpha u)$ with respect to $\alpha$ For me this sentence means that we have another function $g(\alpha) = f (x + \alpha u)$ and then compute the derivative of $g$ with respect to $\alpha$ $$ g'(\alpha) = \lim_{h\to 0} \frac {g(\alpha + h) - g(\alpha)} h = \lim_{h\to 0} \frac {f (x + (\alpha + h) u) - f (x + \alpha u)} h $$ But I don't see how this is equivalent to the definition with $D_v f(x_0)$ . Where does $u^T$ come from and what does this $\nabla_{x} f(x)$ even mean? Is it just the partial derivative with respect to $x$ ? I only the chain rule written in terms of functions $(f(g(x)))' = f'(g(x)) g'(x)$ I don't understand what chain rule they are applying there. Please be patient with me, I am just trying to understand how to reason about directional derivatives.","['partial-derivative', 'multivariable-calculus', 'vector-analysis']"
4852160,Pullback of a very ample line bundle under an étale covering,"I would like to find an example of very ample line bundle on a smooth projective variety whose pull-back under an étale covering is non-very ample. More precisely: Is there an example of very ample line bundle $L$ on a smooth projective variety $Y$ such that the pullback $f^\ast L$ under a finite étale morphism $f\colon X\rightarrow Y$ is non-very ample? In the above situation, $f^\ast L$ is ample and globally generated. Moreover, given that the linear series $f^\ast|L|\subseteq |f^\ast L|$ separates tangent vectors, it is clear that $|f^\ast L|$ separates tangents as well. One has to check that $|f^\ast L|$ does not separate points. My attempts regarded surfaces, but I did not find anything. Any help is greatly appreciated.","['algebraic-geometry', 'surfaces', 'line-bundles']"
4852254,"$\lim_{{n \to \infty}} \int_{{\frac{1}{n^2}}}^{{n}} (n^2x - 1)e^{-n^2x^2} \,dx$","The limit of the integral is given $\lim_{{n \to \infty}} \int_{{\frac{1}{n^2}}}^{{n}} (n^2x - 1)e^{-n^2x^2} \,dx$ (a) Express the integral in the form $\int_0^\infty f_n(y) \, dy$ where $f_n(y)$ represents the general term of the sequence of functions. (b) Utilize the properties of the sequence of functions and apply either the Monotone Convergence Theorem or the Dominated Convergence Theorem to compute the given limit. Attempt: Introduce a new variable $t=nx$ : $\lim_{{n \to \infty}} \int_{{\frac{1}{n}}}^{{n^2}} (nt - 1)e^{-t^2} \,dt$ How do I continue? Can someone please share her/his solution :).","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'real-analysis']"
4852278,"Does this recursively defined set contain all rational numbers $r\in(0,1)$?","Let A be a subset of $\mathbb{R}$ with the following properties: $\frac{1}{2}\in{A}$ and If $x\in{A}$ , then both $\frac{x}{2}\in{A}$ and $\frac{1}{1+x}\in{A}$ . Prove that $A$ contains all rational numbers in the interval $(0, 1)$ , that is, if $r\in{(0,1)}$ with $r\in{\mathbb{Q}}$ , then $r\in{A}$ . This problem has me stumped. It's easy to prove that if $\frac{2p}{q}\in{A}$ or $\frac{q-p}{p}\in{A}$ , then $\frac{p}{q}\in{A}$ , with $p<q$ and $p, q\in{\mathbb{N}}$ , but I don't know where to take it from there. Any hints or solutions would be appreciated. Bonus points for finding a costructive proof and for investigating whether picking any other rational other than $\frac{1}{2}$ would have worked.","['algebra-precalculus', 'rational-numbers']"
4852283,Maximizing log-likelihood: Determinining whether critical points are maximums,"Consider the following proof of the fact that $\bar{X}$ , the sample mean, is the MLE of parameter $\lambda$ in a Poisson distribution. Let $x_1, \ldots, x_n$ be the observations of $X_1, \ldots, X_n$ with $X_i \sim
\mathcal{P}(\lambda)$ . Then we want to solve \begin{align*}
    \text{argmax}_{\lambda} \mathcal{L}(\lambda \mid x_1, \ldots, x_n)
\end{align*} Observe that \begin{align*}
    P(x_1, \ldots, x_n \mid \lambda) &= \prod_{i=1}^{n}  \frac{\lambda^{x_i}
    e^{-\lambda}}{x_i!}
\end{align*} To maximize this with respect to $\lambda$ , we observe that \begin{align*}
    \frac{d}{d\lambda} \ln \left[  \prod_{i=1}^{n}  \frac{\lambda^x_i
    e^{-\lambda}}{x_i!}\right] &= \frac{d}{d\lambda} \sum_{i=1}^{n} \left(\ln
\frac{\lambda^{x_i}e^{-\lambda}}{x_i !}\right) \\ 
&= \frac{1}{\lambda} \sum_{i=1}^{n} x_i - n
\end{align*} Then, if we let $S := \sum_{i=1}^{n} x_i$ , \begin{align*}
    \frac{S}{\lambda} - n &= 0 \iff \lambda = \frac{S}{n}
\end{align*} where obviously $S/n = \bar{X}$ . My question regards the claim that setting the derivative to zero finds the maximizing $\lambda$ . That the log-likelihood has a critical point at $\lambda = \bar{X}$ is proven; however, what guarantees that this critical point is not a minimum? I am aware that the second derivative test answers this question, because it is negative at $\lambda = \bar{X}$ . But the book this comes from is entirely on probability theory and I presume that it's using some fact about the Poisson distribution to implicitly conclude that the point is a maximum. In short, can one conclude that $\lambda = S/n$ is a maximizing point instead of a minimizing point from the nature of the distribution at hand?","['probability-distributions', 'parameter-estimation', 'maximum-likelihood', 'probability-theory', 'probability']"
4852303,Let $∼$ be a relation on $\mathbb{R}$ and $x ∼ y \iff x=y$ or $x+y=6$. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question Let $∼$ be a relation on $\mathbb{R}$ and $x ∼ y \iff x=y$ or $x+y=6$ . I proved that $∼$ is an equivalence relation.
Now I have to find a complete set of representatives. I know that $[a]_∼ := \{b \in \mathbb{R} \mid b ∼ a\}$ , but I don't know how to use this to find a CSR.","['elementary-set-theory', 'equivalence-relations']"
4852344,"If two vertices induce isomorphic subgraphs when they are removed, are they conjugate?","This feels like a very natural question, but I am struggling to find a reference. Consider a simple graph $G$ , and say two vertices are conjugate if there exists an automorphism of $G$ that switches them. It is not hard to show that if $v,w$ are conjugate than the induced subgraphs on $G\setminus v$ and $G\setminus w$ are isomorphic. But is the converse true? More generally, suppose I have a symmetric square matrix $M$ . For each index $i$ , I will denote $M_i$ to be the square matrix $M$ with row and column $i$ deleted. If for some indices $i,j$ , $M_i$ is a permutation of $M_j$ , does it follow that there exists a permutation of the indices of $M$ that fixes $M$ but swaps $i$ and $j$ ?","['graph-theory', 'graph-isomorphism', 'combinatorics']"
4852414,Projective spaces: why adding points to make linear intersections work make everything else work too?,"The (real) projective plane is often motivated by the issue of lines in $\mathbb R^2$ having exactly one intersection, except in the case of parallel lines. The solution is to mimic what we see when we stand on the plane and see parallel train tracks on the ground: add ""points at infinity""/""at the horizon"" (using the language of perspective in art ), one in each possible direction of a line. By construction, all lines have 1 intersection (the ""correct number""). In higher dimensions, the construction is still to add a point at infinity for every possible direction of a line, so that all parallel lines of that direction intersect once at that newly added point at infinity. I also understand that another major part of projective spaces is projective duality. Still, this is about linear things (affine linear subspaces). What I'm bothered by is the apparent miracle, that spaces constructed to make linear things work well (e.g. the initial motivation of making sure parallel lines intersect, or the ""richer"" observation of duality), somehow work well for many other things. For example, in the simplest non-trivial example of conic sections, with these extra points at infinity, all the conic sections now look like simple loops! Is there any intuition that tells us why making things nice for lines, also makes everything else nice too? (I'm aware that another point brought up is that projective spaces are compact , and hence projective spaces will have the nice properties that compactness bestows. But there are many ways of making affine space compact, so again it's not clear why this specific compactification is so magical.) Some further remarks/""analogies"" to better contextualize my question, and to paint a picture of what type of answer I might be looking for: I'm reminded of a similar phenomenon for algebraic closedness of $\mathbb C$ : adding in the points needed to solve quadratic equations over $\mathbb R$ , somehow we get enough to solve all polynomial equations! With the technology of Galois and group theory , we see that this miracle boils down to (1) Every polynomial of odd degree in $\mathbb{R}[X]$ has a root in $\mathbb{R}$ . (2) Every polynomial of degree 2 in $\mathbb{C}[X]$ has a root in $\mathbb{C}$ . (or as a comment pointed out, and explained in this answer , further reduced to just the fact that $\mathbb R$ is a ""ordered field in which every positive element has a square root and every odd degree polynomial has a root""). Although still an amazing fact, these answers investigate the miracle piece by piece and lay out clearly the foundational facts on which standard machinery (which one can develop completely independent of the miracle) can produce the result. I'm also reminded of examples of limits in multivariable calculus where approaching along straight lines things are fine, but along polynomial or other curves, the limit doesn't exist. So somehow lines are ""enough"" for projective spaces, but not ""enough"" for even basic limits in multiple variables. Of course the ""analogy"" is bad, but I write it to point out it's reasonable to expect that a situation which works well for lines may not work well at all for higher degree polynomials. ( cross posted to MO ) EDIT 2/7/24: I've been reading the book Ideals, Varieties, and Algorithms and in particular the proof of Bezout's theorem using resultants. The fundamental difference between ordinary polynomials and homogeneous polynomials is that the resultant for the latter, with $f$ of degree $n$ and $g$ of degree $m$ , has degree exactly $mn$ , whereas the resultant for ordinary polynomials may have ""perfect cancellation"" and end up with a lower degree than $mn$ . For example, taking the resultant of $f=y^{2}+\left(2\sqrt{3}x-2\sqrt{3}\right)y+3x^{2}+2x$ and $g=\left(1+\epsilon \right)y^{2}+\left(2\sqrt{3}x-\left(4-2\sqrt{3}\right)\right)y+\left(3x^{2}-\left(4\sqrt{3}+2\right)x\right)=0$ is degree $2$ for $\epsilon=0$ , but otherwise degree $4$ (the expected number). So somehow homogeneous polynomials are better algebraically behaved, perhaps captured by the idea of ""grading"" (e.g. for a homogeneous polynomial in $k[x,y,z]$ of degree $d$ , the coefficients (in $k[x,y]$ ) of all $z^k$ terms is guaranteed to be a homogeneous polynomial in $k[x,y]$ of degree $d-k$ ). Also, geometrically, it seems like ""generically"" (in the sense of $\epsilon$ -perturbations of the coefficients like I did above) we get the correct counts over $\mathbb C$ , and bringing $\epsilon \searrow 0$ , the only issue is that some intersection points run away to infinity. So yes it is an issue with the noncompactness of affine space. But the reason lines suffice is somehow that when intersection points run away in this setting, they run away ""polynomially"" and that is well-behaved enough that having an observer standing one unit in a new orthogonal direction looking ""down at the 2D-ground"" their line of sight following the point running away doesn't move so much, and in fact converges to a horizon tal line of sight. E.g. the point can not run off to infinity an any wild oscillatory/spiral path (in those situations the line of sight would not converge). This heuristic seems to be supported by Bezout's theorem itself, since Bezout's theorem tells us that algebraic curves can not have infinite oscillations or spirals. However, I still feel like these 2 reasons (algebraic and geometric) that I have provided are somewhat artificial. It does not give the air of some beautiful philosophy about the ""true nature""/""soul"" of projective space. A friend also pointed out that perhaps there is a ""good reason"" that things intersect at all in projective space: Reference request for the dimension of intersection of affine varieties tells us that if the intersection of 2 varieties over affine space intersect once, then they do so ""many times"" (with a lower bound on the dimension of the intersection space). Projective space extends curves in affine space to be ""cones"" intersecting at a point (that is by construction fundamentally what projective space does: project everything through one point ), and so now the intersection theorem applies and tells us we should have one line of intersection, corresponding to a point of intersection in projective space. This I think is a pretty substantial part, but of course it doesn't say anything about capturing all the intersections, just at least one.","['algebraic-geometry', 'intuition', 'math-history', 'soft-question', 'projective-space']"
4852480,"Three random points on $x^2+y^2=1$ are the vertices of a triangle. Is the probability that $(0,0)$ is inside the triangle's incircle exactly $0.13$?","Three uniformly random points on the circle $x^2+y^2=1$ are the vertices of a triangle. What is the probability that $(0,0)$ is inside the triangle's incircle ? (This a variation of the question ""What is the probability that $(0,0)$ is inside the triangle?"".) A simulation with $10^8$ trials gives $P\approx 0.129963$ . Is the probability exactly $0.13$ ? If so, this would be the weirdest geometrical probability I have ever seen. My attempt: Let $d=$ distance between the circles' centres, and let $r=$ radius of the triangle's incircle. We are looking for $P(d<r)$ . Euler's triangle formula tells us that $d=\sqrt{1-2r}$ . So we are looking for $P(\sqrt{1-2r}<r)=P(r>\sqrt2-1)$ . I do not know how to calculate this probability. I know that $r=\sqrt{\frac{(s-a)(s-b)(s-c)}{s}}$ where $a,b,c$ are the side lengths of the triangle and $s=\frac{a+b+c}{2}$ .","['conjectures', 'geometric-probability', 'circles', 'geometry', 'triangles']"
4852495,Still more elliptic curves for $a^4+b^4+c^4=d^4$,"There are 31 known primitive solutions to $a^4+b^4+c^4 = d^4$ with $d<10^{28}.$ ( Update : As of Feb. 21, there are now 93 . See this MSE table .) Old statistics are, \begin{array}{|c|c|}
\hline
\text{Range} &  \text{# of sol} \\
\hline
10^5-10^7 &  \;3 \\
\hline
10^7-10^9 &  12 \\
\hline
10^9-10^{28} & 16  \\
\hline
\text{Total} & 31  \\
\hline\end{array} A brute-force search was done for $d<10^9$ , and explains why it is more dense than the range $10^9-10^{28}$ (where solutions were found mostly using several elliptic curves). The objective of this post is to find more $d<10^{28}$ using other elliptic curves. I. Curve 1 Given $a^4+b^4+c^4 = d^4$ in the form, $$(15968 - 2334 v - 59v^2)^4 + (7068 + 3082 v + 10v^2)^4 + t^4 = (22628 + 54 v + 159v^2)^4$$ where, $$4(110301312 + 10244932v - 1285119v^2 + 5299v^3 - 6260v^4) = t^2$$ For any $v$ , the terms $(a,b,c,d)$ satisfy the simple relationship, $$m_1=\frac{(a+b)^2-c^2-d^2}{a^2+ab+b^2+(a+b)d}=-\frac{9}{20}$$ For infinitely many $v$ , then $t$ is also rational. ""Smallish"" solutions are, $$v = \frac{77 }{9}, \frac{171808 }{16161}, \frac{2465138 }{293763}, \ \frac{5207881}{ 1383765}, \frac{13617482}{ 1280007}, \frac{1251197642}{314528967}$$ $$\color{red}{-v} = \frac{1022}{ 243}, \frac{50191 }{8685}, \frac{128416}{ 29685}, \frac{1116448 }{2565009}, \frac{10267558}{ 1775127}, \frac{237282598}{377952087}\quad$$ For example, let $v = -\frac{1022}{ 243}$ and $v=\frac{77 }{9}$ , substituting $(v,t)$ then, after removing common factors, yields the $1$ st and $2$ nd smallest solutions, $$95800^4+ 217519^4+ 414560^4= 422481^4\\
673865^4 + 1390400^4 + 2767624^4 = 2813001^4$$ This curve has been well-explored and the 12 points yield 6 primitive $(a,b,c,d)$ found by Tomita with $d<10^{20}$ . It may serve to show how the growth of the $v_k$ seems reasonably slow and it is hoped that the next curves will be similar. II. Curve 2 $$(11980 - 1673 v - 54v^2)^4 + (-36 + 2321 v - 3v^2)^4 + t^4 = (24677 + 203 v + 71v^2)^4$$ where, $$591800025 + 20030510v + 1671327v^2 + 92762v^3 - 4112v^4 = t^2$$ The terms satisfy, $$m_2=\frac{(a+b)^2-c^2-d^2}{a^2+ab+b^2+(a+b)d}=-\frac{29}{12}$$ Only two small solutions are known. Can you find more? $$v=-\frac{2020}{127}, \frac{76164}{2063}$$ After removing common factors, either $v$ will yield the $3$ rd smallest, $$1705575 ^4 + 5507880^4 + 8332208^4 = 8707481^4$$ III. Curve 3 $$(- 1058960 + 11203324v + 178500100v^2)^4 + (518320 + 16483396v - 294176372v^2)^4 + t^4 = (1304433 - 27003006v + 345712797v^2)^4$$ where, $$-1251219988511 + 78204922436804v - 1649103906705762v^2 + 19988050672538996v^3 - 76026722992074935v^4=t^2$$ The terms satisfy, $$m_3=\frac{(a+b)^2-c^2-d^2}{a^2+ab+b^2+(a+b)d}=-\frac{93}{80}$$ Only four solutions are known, $$v=\frac{12040}{ 110133}, \frac{10691}{353335}, \frac{737109}{  5187253}, \frac{7880680}{ 207097317}$$ The 4 $v_k$ will give 2 primitive $(a,b,c,d)$ with the first $v$ yielding the $4$ th smallest, $$5870000^4 + 8282543^4 + 11289040^4 = 12197457^4$$ V. Curve 4 $$(- 20150032 + 34614497v + 67829197v^2)^4 + (5444488 + 58502527v - 148896448v^2)^4 + t^4 = (30141789 - 124521519v + 197120781v^2)^4$$ where, $$-812211871484873 + 7722674874928166v - 27864960882719827v^2 + 48860237516933014v^3 - 31577722089576368v^4 = t^2$$ The terms satisfy, $$m_4=\frac{(a+b)^2-c^2-d^2}{a^2+ab+b^2+(a+b)d}=-\frac{136}{133}$$ Only four solutions are known, $$v = \frac{17611}{74168}, \frac{438773}{1417384}, \frac{3258337}{6134331}, \frac{298041047}{414260301}$$ The 4 $v_k$ will also give 2 primitive $(a,b,c,d)$ , with the second $v$ yielding the $5$ th smallest, $$4479031^4 + 12552200^4 + 14173720^4 = 16003017^4$$ V. Question We skip Curve 1. Can more rational points to Curves 2,3,4 be found so it yields primitive solutions to $a^4+b^4+c^4 = d^4$ with the restriction $|d|<10^{28}$ so that it will fit in this table ? P.S. The list has no additions since 2015.","['number-theory', 'elliptic-curves', 'diophantine-equations']"
4852522,Proof and example of the formulas for $\sin\left(\sum_{i=1}^\infty\theta_i\right)$ and $\cos\left(\sum_{i=1}^\infty\theta_i\right)$,"Was reading the page on ""List of trigonometric identities"" on Wikipedia and came across an identity for sines and cosines of sums of infinitely many angles whose sum converges absolutely towards a single value. The identities are as follows: I can't for the life of me find a proof of these identities online nor are they cited from anywhere on the wikipedia page and where would this be useful could someone give a problem for this. Any help would be appreciated","['trigonometry', 'sequences-and-series']"
4852544,Is this condition sufficient to conclude the graph is a straight line?,"Out of curiosity, I was wondering whether if a continuous function $f:\mathbb{R}\rightarrow\mathbb{R}$ has the property that in  any 2 intervals $[a,b]$ and $[c,d]$ on which it is defined, $(a-b = c -d) \implies (f(a)-f(b) = f(c)-f(d))$ , then the graph of this function is a straight line. So far, I have tried reasoning the contrapositive of this-  that not being a straight line means there exist intervals of equal length where the function changes a different amount over them. While this seemed more intuitive, I didn't see a rigorous way to prove this. So is this claim true, and if so, what is the proof?","['analytic-geometry', 'functions', 'real-analysis']"
4852614,Finding the limit of $a_{n} = \frac{n!}{\left(\frac{2}{7} + 1\right)\left(\frac{2}{7} + 2\right)\ldots\left(\frac{2}{7} + n\right)}$,"My task was to prove that the limit in the title exists and to calculate it. First I showed that the sequence is monotonically decreasing $\left(\frac{a_{n+1}}{a_{n}} = \frac{n+1}{\frac{2}{7} + n + 1} < 1\right)$ and it's obvious that $a_{n} > 0$ for all $n$ . Therefore, by the Monotone Convergence theorem $(a_n)_{n \geq 1}$ must converge. My first idea was to use the relation I obtained earlier between $a_{n+1}$ and $a_{n}$ : $a_{n+1} = \frac{n+1}{\frac{2}{7} + n + 1} \cdot a_{n}$ and passing the limit on both sides, but I arrive at the trivial equality that $\lim_{n \to \infty} a_{n+1} = \lim_{n \to \infty} a_{n}$ . My second idea was to rationalize the denominator to see if a more familiar expression will arise, here is what I arrived at: $$a_{n} = \frac{7^n \cdot n!}{(2+7 \cdot 1)(2 + 7 \cdot 2)...(2 + 7 \cdot n)}$$ which didn't help. If it is of any help, the first exercise was to study the convergence of: $$\sum_{n=1}^{\infty} \frac{n!}{(2x+1)...(2x+n)} \text{ for } x \in (0, +\infty)$$ I determined that it converges when $x \in \left(\frac{1}{2}, \infty\right)$ by Raabe-Duhammel and diverges otherwise.","['limits', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
4852641,Practical engineering combinatorial-design optimization problem,"I am a very experienced electrical engineer. I am kindly asking for help!
There is a practical case in electrical engineering that leads to the following question. Combinations without repetition (n=10, r=2) or C(10,2)
Using Items: 1,2,3,4,5,6,7,8,9,0
List has 45 entries. https://www.mathsisfun.com/combinatorics/combinations-permutations-calculator.html Combination list C(10,2) Basic engineering task is to make 45 separate measurements for each C(10,2)pair.
Alternative is to make automated C(4,2) measurements in groups of four Items C(10,4) to cover again 45 pairs C(10,2). The questions are:
What is theoretically the minimal number of groups of four C(10,4) to cover all 45 combinations in C(10,2) ?
What combinations C(10,4) are concrete solution ? My concrete intuitive solution for C(10,4) is: Combination list C(10,4) So I came up with a solution of 10 combinations of C(10,4), which together cover 45 combinations of C(10,2). Since each of the C(10,4) combinations gives 6 C(10,2) combinations, I have a total of 10x6 or 60 C(10,2) combinations, that is, 60-45=15 double C(10,2) combinations. Theoretically, to get 45 C(10,2) combinations, 8 C(10,4) combinations should be enough, because 8x6=48 C(10,2) combinations, with only 3 duplicate C(10,2) combinations. My mathematical knowledge is not enough to prove or disprove that claim. My further effort was to write a VBA program to randomly calculate the most favorable combinations of C(10,4). After about 400,000 random calculations I got 4 solutions with 10 x C(10,4) given below: Random Simulation Solution for Combination list C(10,4) So, I got solutions that are not better than my intuitive solution, that is, they do not contain less than 10 combinations of C(10,4). I would appreciate any help in terms of: A concrete solution to the problem in the theoretically smallest possible number of C(10,4) combinations. Theoretical proof for the smallest number of C(10,4) combinations. Tip for a software tool that can help solve the problem. Mathematical classification of the problem and reference to the relevant literature or a solution to a similar problem. Thank you in advance!","['elementary-set-theory', 'combinatorial-designs', 'combinations', 'combinatorics']"
4852664,"counting number of arithmetic sequenses of diff 1 of length of at least 2 that could be made from numbers {1,2,...,n}","im asked to find the number of arithmetic sequences of d=1 (or consecutive sequences the way the question describes it), of length of at least 2 from the numbers {1,2,...,n}. the question adds that it needs to be done by proving via bijection that this number is equal to the cardinality of a certain set. the way i approached the question is: for sequences of length 2 i could start with one sequence 1,2 and slide across till it reaches n-1, so for every sequence of length x there will be n-x+1 sequences and then we sum for all values of x from 2 to n. but this clearly isnt a bijection, the only bijection i could think of is a function that sends each x from 1 to n to the subgroup of seuneces of length x (represented as x-long vectors). but after that i get stuck, can someone guide me? (i learn this course in a different language so excuse my mistakes)","['arithmetic-progressions', 'combinatorics']"
4852699,How does $d \sqrt{1+\left(\frac{h_t+h_r}{d}\right)^2}$ become $d \left(1+\frac{1}{2}\left(\frac{h_t+h_r}{d}\right)^2\right)$?,"I am trying to understand the following step related to square root in equation (2). $$\Delta = d \sqrt{1+\left(\frac{h_t+h_r}{d}\right)^2} \tag1$$ $$\Delta = d \left(1+\frac{1}{2}\left(\frac{h_t+h_r}{d}\right)^2\right) \tag2$$ My query is that, I am not getting how square root disappears in equation (1) and $\frac{1}{2}$ appears in equation (2). Any help in this regard will be highly appreciated.","['trigonometry', 'calculus', 'roots', 'algebra-precalculus']"
4852707,$\binom {100} 0 \binom{100}{98}+\binom{100}2 \binom{100}{96}+\cdots+\binom{100}{98} \binom{100}0$,"$$\binom {100} 0 \binom{100}{98}+\binom{100}2 \binom{100}{96}+\cdots+\binom{100}{98}  \binom{100}0$$ Clearly the sum of bases is 98.
I have solved this problem algebraically by finding half of
Coefficient of $x^{98}$ in $$(1+x)^{100}(1+x)^{100}+(1-x)^{100}(1+x)^{100}$$ The answer comes out to be $$\frac{\binom{200}{98}-\binom{100}{49}}{2}$$ I wanted a combinatorial intuition behind it I framed a situation as if there are 2 bags and in total we need to pick 98 balls each contains 100 balls and we can only choose even number of balls from each bag and I'm stuck after this","['alternative-proof', 'combinatorics']"
4852769,Does Cauchy's integral formula generalize to non-analytic functions?,"Cauchy's integral formula states that if the complex function $f(z)$ is analytic on a closed domain $D$ of the complex plane and $a$ is in the interior of $D$ , then $$f(a) = \frac{1}{2 \pi i}\oint_{\partial D} \frac{f(z)}{z-a} dz.$$ This formula can be interpreted in two ways: If we ""use the RHS to learn the LHS"", then we can think of the formula as telling us that if we only know the values of $f(z)$ on the boundary curve $\partial D$ , then that is enough information to fully determine the values of $f(z)$ on the interior of $D$ . If we ""use the LHS to learn the RHS"", then we can think of the formula as telling us that if we want to calculate the contour integral $\oint_{\partial D} \frac{f(z)}{z-a} dz$ , then we need only know the single value $f(a)$ on the interior. In this interpretation, it's perhaps more natural to think of the ""primary function"" being integrated as $g(z) := \frac{f(z)}{z-a}$ , which is analytic on $D$ except for at a simple pole at $z = a$ , and then to rewrite Cauchy's integral formula as $$\oint_{\partial D} g(z)\ dz = 2 \pi i \ \lim_{z \to a} \left[ g(z) (z-a) \right] = 2 \pi i\ \mathrm{Res}(g, a).$$ As pointed out at https://math.stackexchange.com/a/1654381/268333 , it's clear how to generalize the second interpretation to the case of a function $g(z)$ that has any finite number of isolated singularities in $D$ : via the residue theorem . Can we also generalize the first interpretation to the case where $f(z)$ may not be analytic on all of $D$ ? In other words, if we only know the value of $f(z)$ on the boundary curve $\partial D$ , then are there any looser requirements for $f$ (e.g. that it be analytic except at a finite number of isolated singularities in the interior of $D$ ) that are still enough to allow us to reconstruct it (or at least learn some information about it) on the interior of $D$ ? What happens if you try to apply Cauchy's integral formula to a function that is not analytic on all of $D$ but has singularities or branch cuts?","['complex-analysis', 'contour-integration', 'cauchy-integral-formula', 'residue-calculus']"
4852808,Hessenberg sum/natural sum of ordinals definition,"I was given the following definition of Hessenberg sum: Definition. Given $\alpha,\beta \in \text{Ord}$ their Hessenberg sum $\alpha \oplus \beta$ is defined as the least ordinal greater than all Hessenberg sums of the form $\alpha' \oplus \beta'$ , for $\alpha ' \leq \alpha$ , $\beta' \leq \beta$ and $(\alpha',\beta') \ne (\alpha,\beta)$ . Reading online I understood (I hope) that this is equivalent to making the sum between $\alpha$ and $\beta$ component-wise in Cantor Normal Form. Does anyone have a simple proof of this fact?","['elementary-set-theory', 'ordinals', 'well-orders', 'set-theory']"
4852809,Strict partial order and strict linear order,"Task : A binary relation on a set of 7 elements contains exactly 20 pairs. Could it be : a) a strict partial order relation? b) a relation of strict linear order? In strict linear order, any pair of distinct elements is comparable. My thinking : a) There can be a total of $7 \times (7 - 1) / 2 = 21$ pairs of elements. We are given $20$ pairs of elements. Since a strict partial order does not have to be linear, it is very likely that a strict partial order can be created. I can't give any other arguments. Maybe I need to demonstrate some example of strictly partial order on $7$ elements with $20$ pairs to prove the existence of a strictly partial order relation? b) It seems that there certainly cannot be a strict linear linear order relation, because for that , each pair must be comparable, that is, there must be at least $7 \times (7 - 1) / 2 = 21$ pairs of elements. We have only $20$ pairs, but we need $21$ pairs, one pair is missing, so there is no strict linear order relationship. Is the reasoning correct?","['order-theory', 'solution-verification', 'examples-counterexamples', 'discrete-mathematics']"
4852852,Uniform Distribution Game,"Here is a probability game description: You generate a uniformly random number in the interval
(0,1). You can generate additional random numbers as many times as you want for a fee of
$
0.02
per generation. This decision can be made with the information of all of the previous values that have been generated. Your payout is the maximum of all the numbers you generate. Under the optimal strategy, find your expected payout of this game. For this game, my approach was to determine the number of rolls n, at which the expected difference between the maximum of n rolls and n-1 rolls is below 0.02. This would be the point at which I wouldn't roll anymore, as the fee overshadows the expected gain. Setting this up as follows: $X = Max( X_1,...X_n)$ , $Y = Max(X_1,...,X_{n-1})$ $E(X) - E(Y) \leq 0.02 $ $\frac{n}{n+1} - \frac{n-1}{n} \leq 0.02$ Solving this, we get n = 6.58. Then, as the number of rolls is discrete, we take n=6. Then the expected payout should be: $Payout = \frac{6}{7} - 0.02*5 = 0.76$ However, the answer is 0.82. What is wrong with my logic?","['expected-value', 'game-theory', 'uniform-distribution', 'probability']"
4852877,Identities using only binomial coefficient,"There are many known combinatorial identities involving the binomial coefficient, although essentially all of the well known ones involve some function or constant other than the binomial coefficient itself. For example, Pascal's identity ${n\choose k} = {n-1\choose k-1} + {n-1 \choose k}$ makes use of subtraction, addition, and the constant $1$ . The identities ${n\choose 0} = 1$ and ${n\choose 1} = n$ use the constants $0$ and $1$ . I am interested in identities which use only variables and the binomial coefficient (in the same way that the associative and commutative laws for multiplication use only variables, multiplication and no other functions). Notice, for example, that we have the identity ${a\choose a}=1$ , which allows us to write the last of the above identities as $${n\choose {a\choose a}} = n$$ Furthermore, since $1=1$ we have the identity $${a\choose a} = {b\choose b}$$ These two identities are built out of only variable and binomial coefficients. Do there exist any other such identities, which don't follow logically from these two above? In the language of universal algebra, is the smallest variety $V$ containing $\langle\mathbb{N}, B\rangle$ (where $B$ is the 2-ary binomial coefficient function) exactly the class of algebraic structures whose signature is a single 2-ary function $B$ satisfying the identities $B(n,B(a,a)) = n$ and $B(a,a) = B(b,b)$ ? If not, how can one describe this variety $V$ ? (Note: I take the convention that if $a < b$ , then ${a\choose b}=0$ )","['universal-algebra', 'binomial-coefficients', 'combinatorics']"
4852894,"Prove that if two polynomial functions have equal values over a closed interval, they are equal. [duplicate]","This question already has answers here : Is There A Polynomial That Has Infinitely Many Roots? (12 answers) I don't understand why if two polynomials degrees do not exceed $n$ and they coincide at $n+1$ points then they are equal. [duplicate] (2 answers) Can Two Different Polynomials Agree on an open interval? [duplicate] (2 answers) Closed 5 months ago . I want to prove that two polynomial functions that are equal over a specific interval, $(a, b) \in \mathbb R$ (closed interval with more than one point if that condition is necessary) are equal over $\mathbb R$ . I had an attempt that intuitively makes sense but I want to make the argument more formal. These were my attempts: I have tried expanding the general equation of a polynomial as a product of its roots but got stuck. I then simplified the question. I examined the case of two linear functions equivalent at a point. I figured if it is equivalent at another point (not necessarily over an interval), it would be trivial to prove that the two functions are the same. I moved to extend it to quadratic functions. I figured all we need is three points to pinpoint the expression of the polynomial. Given that the functions are equal over an interval, we have more than enough. I figured that, intuitively, this should continue for any two polynomials of arbitrary degree. Therefore, if two polynomials of $n$ -degree are equal over $n + 1$ points on the plane, then the two polynomials are the same. However, something feels very wishy-washy with this proof. How can I make this proof more formal and are there any alternate proofs?","['proof-writing', 'real-analysis', 'functions', 'solution-verification', 'polynomials']"
4852987,How to evaluate $\int_{-\infty}^{+\infty}\frac{\cos x}{\left(1+x+x^2\right)^2+1}\mathrm{~d}x$,"Question $$\int_{-\infty}^{+\infty}\frac{\cos x}{\left(1+x+x^2\right)^2+1}\mathrm{~d}x$$ Wolfram alpha says it is $$\int_{-\infty}^{\infty} \frac{\cos(x)}{\left(1 + x + x^2\right)^2 + 1} \,dx = \frac{\pi (1 + 2\sin(1) + \cos(1))}{5e} \approx 0.745038$$ My try \begin{align} &\quad\int_{-\infty}^{+\infty}\frac{\cos x}{\left(1+x+x^2\right)^2+1}\mathrm{~d}x\\ &=\underbrace{\int_{-\infty}^{0}\frac{\cos x}{\left(1+x+x^2\right)^2+1}\mathrm{~d}x}_{x\to-x}+\int_{0}^{+\infty}\frac{\cos x}{\left(1+x+x^2\right)^2+1}\mathrm{~d}x\\ &=\int_{0}^{\infty}\left[\frac{\cos x}{\left(1+x+x^2\right)^2+1}+\frac{\cos x}{\left(1-x+x^2\right)^2+1}\right]\mathrm{~d}x\\ &=\int_{0}^{\infty}\frac{\cos x}{\left(1+x^2\right)}\left[\frac{1}{2+2x+x^2}+\frac{1}{2-2x+x^2}\right]\mathrm{~d}x\\ &=2\int_{0}^{\infty}\frac{\left(2+x^2\right)\cos x}{\left(1+x^2\right)\left(4+x^4\right)}\mathrm{~d}x\\ &=\frac{2}{5}\int_0^\infty\frac{\cos x}{1+x^2}\mathrm{~d}x+\frac{12}{5}\int_0^\infty\frac{\cos x}{4+x^4}\mathrm{~d}x\\ &\quad-\frac{2}{5}\int_0^\infty\frac{x^2\cos x}{4+x^4}\mathrm{~d}x\\ &=\frac{2}{5}I_1+\frac{12}{5}I_2-\frac{2}{5}I_3. \end{align} Alright, now we have three integrals, within which the $I_1$ is a specific case of the so-called $\text{Laplace}$ integral. We can solve it with the following trick. \begin{align} I_1&=\int_0^\infty\frac{\cos x}{1+x^2}\mathrm{~d}x\\ &=\int_0^\infty\cos x\int_0^\infty\mathrm{e}^{-\left(1+x^2\right)t}\mathrm{~d}t\mathrm{d}x\\ &=\int_0^\infty\mathrm{e}^{-t}\int_0^\infty\mathrm{e}^{-x^2t}\cos x\mathrm{~d}x\mathrm{d}t\\ &=\int_0^\infty\mathrm{e}^{-t}\int_0^\infty\mathrm{e}^{-x^2t}\sum_{n=0}^\infty\frac{(-)^nx^{2n}}{(2n)!}\mathrm{~d}x\mathrm{d}t\\ &=\sum_{n=0}^\infty\frac{(-)^n}{(2n)!}\int_0^\infty\mathrm{e}^{-t}\underbrace{\int_0^\infty\mathrm{e}^{-x^2t}x^{2n}\mathrm{~d}x}_{x^2t\to x}\mathrm{d}t\\ &=\frac{1}{2}\sum_{n=0}^\infty\frac{(-)^n}{(2n)!}\int_0^\infty\mathrm{e}^{-t}t^{-n-\frac{1}{2}}\int_0^\infty\mathrm{e}^{-x}x^{n-\frac{1}{2}}\mathrm{~d}x\mathrm{d}t\\ &=\frac{1}{2}\int_0^\infty\mathrm{e}^{-t}t^{-\frac{1}{2}}\sum_{n=0}^\infty\frac{(-)^n}{(2n)!}\Gamma\left(n+\frac{1}{2}\right)t^{-n}\mathrm{d}t\\ &=\frac{1}{2}\int_0^\infty\mathrm{e}^{-t}t^{-\frac{1}{2}}\sum_{n=0}^\infty\frac{(-)^n(2n-1)!!}{(2n)!2^n}\Gamma\left(\frac{1}{2}\right)t^{-n}\mathrm{d}t\\ &=\frac{\sqrt{\pi}}{2}\int_0^\infty\mathrm{e}^{-t}t^{-\frac{1}{2}}\sum_{n=0}^\infty\frac{1}{n!}\left(-\frac{1}{4t}\right)^n\mathrm{d}t\\ &=\frac{\sqrt{\pi}}{2}\underbrace{\int_0^\infty\mathrm{e}^{-t-\frac{1}{4t}}t^{-\frac{1}{2}}\mathrm{d}t}_{t\to t^2}\\ &=\sqrt{\pi}\int_0^\infty\mathrm{e}^{-t^2-\frac{1}{4t^2}}\mathrm{d}t=\frac{\sqrt{\pi}}{\mathrm{e}}\underbrace{\int_0^\infty\mathrm{e}^{-\left(t-\frac{1}{2t}\right)^2}\mathrm{d}t}_{t-\frac{1}{2t}\to t}\\ &=\frac{\sqrt{\pi}}{\mathrm{e}}\int_{-\infty}^\infty\mathrm{e}^{-t^2}\left(\frac{1}{2}+\frac{t}{2\sqrt{2+t^2}}\right)\mathrm{d}t\\ &=\frac{\sqrt{\pi}}{\mathrm{e}}\int_{0}^\infty\mathrm{e}^{-t^2}\mathrm{d}t\quad (\text{Gauss Integral})\\ &=\frac{\pi}{2\mathrm{e}}. \end{align} My question is how to evaluate $I_2$ and $I_3$ ? or any other method?","['integration', 'calculus', 'definite-integrals', 'closed-form']"
4853006,"There are sets $A,B$, and $C$ such that $A∈B,B∈C$, and $C∈A$. How to avoid Axiom of regularity, since Halmos avoids it in his Naive Set Theory?","Give an example or disprove: There are sets $A, B$ , and $C$ such that $A ∈ B,B ∈ C$ , and $C ∈ A$ .
How can I do this without using the Axiom of regularity, since Halmos avoids it in his Naive Set Theory?","['elementary-set-theory', 'set-theory']"
4853011,Constructing a Diffeomorphism between a Cone like and cylinder surface,"I am told very specifically that a cone has the following definition: $$C_1 = \{(x,y,z) \in \mathbb{R^3} | z^2 = x^2 + y^2 , z > 0\}$$ Please note the $z>0$ part. I want to map it to a cylinder of the form: $$C_2 = \{(x,y,z) \in \mathbb{R^3} | x^2 + y^2 = 1\}$$ If $f:C_1 \rightarrow C_2$ is smooth, inverse exist and is smooth, then we are in business! I know that in topology we would say that the point in the cone would make these two not topologically equivalent, but the definition here is excluding that point. So would the following map work: $$f(x,y,z) = (\frac{x}{z}, \frac{y}{z}, z)$$ Clearly, $(x/z)^2 + (y/z)^2 = 1$ as required. furthermore, the function is differentiable in $z$ as long as $z > 0$ . The inverse also exists as: $$f^{-1}(u,v,w) = (uw,vw,w)$$ Which is clearly smooth wrt all variables. Hence, am I correct in concluding that these two are diffeomorphic?","['diffeomorphism', 'surfaces', 'differential-geometry']"
4853021,How do I evaluate integrals of the form: $\int_{0}^{1}{\dfrac{\operatorname{arctanh}(x)x^{n}}{1+x^{n+1}} \ dx}$,Few cases have been evaluated: $n=1$ : https://math.stackexchange.com/q/4795632 $n=3:$ https://math.stackexchange.com/q/4801245 I want to know if there is a method to find all related integrals of the form $$\int_{0}^{1}{\dfrac{\operatorname{arctanh}(x)x^{n}}{1+x^{n+1}} \ dx}$$ How do I evaluate these kinds of integrals?,"['integration', 'calculus', 'definite-integrals']"
4853065,"Let $0<\alpha\leq\frac{\pi^2}{6}.\ $ Does $\exists\ A\subset\mathbb{N}$ and $f:A\to\{-1,1\}$ such that $\sum_{n\in A} \frac{f(n)}{n^2}=\alpha?$","If we let $0<\alpha\leq \frac{\pi^2}{6},\ $ then it is not always true that $\exists\ A\subset \mathbb{N}$ such that $\displaystyle\sum_{n\in A} \frac{1}{n^2} = \alpha.\ $ To see this, consider the fact that $\ \displaystyle\sum_{n=1}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6} \approx 1.645,\ $ and $\displaystyle\sum_{n=2}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6}-1 \approx 0.645,\ $ and so there is no $\ A\subset \mathbb{N}$ such that $\displaystyle\sum_{n\in A} \frac{1}{n^2} = 0.9,\ $ which is $<1$ but $>0.645.$ This prompted me to ask the following: Let $0<\alpha\leq \frac{\pi^2}{6}.\ $ Does $\exists\ A\subset \mathbb{N}$ and $f:A\to \{-1,1\}$ such that $\displaystyle\sum_{n\in A} \frac{f(n)}{n^2} = \alpha?$","['sequences-and-series', 'problem-solving', 'examples-counterexamples', 'real-analysis']"
4853073,Lattices/Topology and the Stone Duality,"For some context I have some partial understanding of lattices and an intermediate understanding of topology. I at some point in the past week started thinking about a funny way to view a topology on a space as constituted by a lattice of open sets. In the sense that a union of two sets corresponds to a join and a intersection to a meet. Then in some sense the axioms of a topology on a space can be translated to a set of axioms on our lattice For example since the space itself and the null set are both open then there exist maximal and minimal elements. Since arbitrary unions of opens are opens then we can take joins of arbitrary number of elements, but since only finite intersections of opens are (gaurenteed to be) open then we can only take meets for finite sets of elements. I see these ideas more or less reflected in the wikipedia page for the stone duality https://en.wikipedia.org/wiki/Stone_duality#The_adjunction_of_Top_and_Loc . However most resources on this connection are fairly advanced often bringing in a good chunk of category theory. My background in category theory is very limited. While I understand that a formal treatment of this duality inevitably will involve some construction of some pair of functors between topological spaces and lattices, I was wondering if there was a resource that might give a softer and more intuitive introduction to this area. Or is it simply time to bite the category theory bullet Furthermore I am interested if there is any work in applying topological notions to the appropriate types of lattices. For example it seems that we can extend notions of homotopy equivalences between spaces to homotopy equivalences between latices. Similarly we can extend compactness quite easily. Furthermore one could also construct algebraic invariants like the homology or fundamental groups of a lattice. This doesn't seem to hold for all the usual topological concepts since  it seems by switching to lattice one loses the notion of points and thus cannot straightforwardly talk about Hausdorfness, limits etc. Thus it seems one would recover a slightly altered version of the usual topological notions for suitable lattices. I can find bits and pieces of these ideas scattered across several papers and articles and blogs. Is there any centralized resource focused on this type of stuff?","['lattice-orders', 'general-topology', 'category-theory', 'reference-request']"
4853087,"I wonder why $E(G)$ must be the edge set of $G$. (""Graph Theory Fourth Edition"" by Robin J. Wilson.)","I am reading ""Graph Theory Fourth Edition"" by Robin J. Wilson. Exercise 2.15: If $G$ is a simple graph with edge-set $E(G)$ , the vector space of $G$ is the vector space over the field $\mathbb{Z}_2$ of integers modulo $2$ , whose elements are subsets of $E(G)$ . The sum $E+F$ of two subsets $E$ and $F$ is the set of eges in $E$ or $F$ but not both, and scalar multiplication is defined by $1.E=E$ and $0.E=\varnothing$ . Show that this defines a vector space over $\mathbb{Z}_2$ , and find a basis for it. I wonder why $E(G)$ must be the edge set of $G$ . I think we can construct the vector space even if we replace $E(G)$ with an arbitrary finite set $E$ .","['elementary-set-theory', 'graph-theory', 'vector-spaces']"
4853171,Concentration inequalities on the covariance between observations of the samples,"I have the following problem : I have $X_1, \dots, X_n$ , $n$ i.i.d. random variables in $\mathbb{R}^p$ . They are bounded, but I think that it is not important. I also have an embedding map $f : \mathbb{R}^p \rightarrow \mathbb{R}^q$ that has a bounded image. I think that this hypothesis is much more important. I am looking at concentration inequalities of the matrix $((f(X_i) - E(f(X_i))^T(f(X_j) - E(f(X_j)))_{i, j}$ around its expectation, if possible in terms of operator norm. In particular, it is easily seen that this expectation is the diagonal matrix with the variances of the $f(X_i)$ 's on the diagonal. I no not know where to start. Regular concentration inequalities that typically require independence do not apply here. Thanks in advance.","['statistics', 'concentration-of-measure', 'probability-theory']"
4853200,"Limit evaluation, de l'Hopital seems not working","I have to evaluate \begin{equation}
L=\lim_{x\to 0} \frac{1-e^{\frac{x^2}{2}}\cos x}{2\sin^2x -x \arctan2x}
\end{equation} I tried with de l'Hopital's rule but it seems not working, am I missing something? EDIT: I'll explain better what I tried & where I got stuck. Straightforward substitution leads to a $\frac{0}{0}$ form.
So I tried de l'Hopital: Let $f(x)=1-e^{\frac{x^2}{2}}\cos x$ (the numerator) and $g(x)=2\sin^2x -x \arctan2x$ (the denominator). We have: \begin{equation}
f'(x) = e^{\frac{x^2}{2}}(\sin x -x \cos x)
\end{equation} \begin{equation}
g'(x) = -\frac{2x}{1+4x^2}-\arctan(2x) +4\sin x \cos x
\end{equation} This still leads to a $\frac{0}{0}$ form. Differentiating again, we have \begin{equation}
f''(x) = xe^{\frac{x^2}{2}}(2\sin x -x \cos x)
\end{equation} \begin{equation}
g''(x)= 4\left(-\sin^2 x +\cos^2 x -\frac{1}{(4x^2+1)^2} \right)
\end{equation} Still, evaluating the limit again leads to a $\frac{0}{0}$ form. Am I losing something?","['indeterminate-forms', 'limits', 'calculus']"
4853203,Probability of no friends meeting at a coffee house,"I came across what seems to be in my opinion a very challenging problem: A number n of friends each visit Old Slaughter’s coffee house independently and uniformly at
random during their lunch break from noon to 1pm. Each leaves after δ hours (or at 1pm if that is
sooner), where δ < 1/(n−1). Show that the probability that none of them meet inside is $(1−(n−1)\delta)^n$ . I've tried to consider the events $A_{i,j}$ that correspond to "" $i$ and $j$ meet inside the coffee house"", and by calling the arrival times of the $k$ -th person $U_k \sim Uniform([0,1])$ , I can indeed compute the $\mathbb{P}(A_{i,j}) = \mathbb{P}(|U_i - U_j| < \delta)$ by using the continuous version of the total probability formula and chopping up my integral in several pieces : $\mathbb{P}(A_{i,j}) = \int_0^1 \mathbb{P}(U_j - \delta < U_i < U_j + \delta | U_j = x)dx = \int_0^1\int_{max(0,x-\delta)}^{min(1,x+\delta)}dydx$ etc... But I don't think I'm on the right track because I've got no plan to go from $\mathbb{P}(A_{i,j})$ to $\mathbb{P}(\cap_{i<j}A_{i,j}^C)$ . By peeking at the Grimmet & Stirzaker solution manual I get the impression that it might have something to do with the ""order statistic"" but it all seems pretty cryptic to me, humble probability novice.","['order-statistics', 'uniform-distribution', 'probability']"
4853257,$L^p$ stochastic processes and boundedness,"I have been looking at a proof on page 299 of the following paper: Promel, D. J., and Scheffels, D. Stochastic volterra equations with
Hölder diffusion coefficients. Stochastic Processes and their
Applications, 3 (2023), 291–315. While I think I understand most of the proof, there is an element which I do not quite grasp. I have tried to capture it in the following setting. Hopefully this is clear enough. Le $T>0$ and $p>2$ . Let $(\Omega,\mathcal{F},(\mathcal{F}_t)_{t\in [0,T]},\mathbb{P})$ be some filtered probability space. Let further $X = (X_t)_{t\in [0,T]}$ and $Y = (Y_t)_{t\in [0,T]}$ be some $(\mathcal{F}_t)$ -progressively measurable stochastic processes. Suppose further that $X$ is in $L^p(\Omega \times [0,T])$ and let $f:[0,T] \to \mathbb{R}$ be some continuous function. Suppose that, for some $C>0$ , we have that $$\mathbb{E}\big[\lvert Y_t\rvert ^p\big] \leq C \bigg(1 + \lvert f(t) \rvert + \int_0^t\mathbb{E}\big[\lvert X_s\rvert ^p\big]ds,  \bigg),$$ holds for all $t\in[0,T]$ . Then Is the map $t\to \mathbb{E}[\lvert Y_t\rvert ^p]$ bounded? I would argue that yes, it is. Indeed for $t\in[0,T]$ , we have \begin{equation}
\begin{aligned}
\mathbb{E}\big[\lvert Y_t\rvert ^p\big] & \leq C \bigg(1 + \lvert f(t) \rvert+ \int_0^t\mathbb{E}\big[\lvert X_s\rvert ^p\big]ds,  \bigg) \\
& \leq C \bigg(1 + \sup\lvert f(t) \rvert + \int_0^t\mathbb{E}\big[\lvert X_s\rvert ^p\big]ds,  \bigg) \\
& \leq C \bigg(1 + \sup\lvert f(t) \rvert + \int_0^T\mathbb{E}\big[\lvert X_s\rvert ^p\big]ds,  \bigg).
\end{aligned}
\end{equation} Since $X$ is in $L^p$ , then there is some $M>0$ , such that $$\mathbb{E}\big[\lvert Y_t\rvert ^p\big]<M, \quad \hbox{for all $t\in[0,T]$}.$$ Is $Y$ in $L^p(\Omega \times [0,T])$ ? I would argue that, by integrating on $[0,T]$ we obtain that $Y$ is in $L^p$ . Is my development correct? In the paper the authors have that $X = Y$ and wish to use Grönwall's lemma to obtain some bound on $X$ . However, they use a sequence of stopping times seemingly to ensure that $t\to \mathbb{E}[\lvert Y_t\rvert ^p]$ is bounded. If my development above is correct, then using the stopping times seems unnecessary. Some help would be much appreciated. Thanks in advance.","['stochastic-integrals', 'stochastic-processes', 'measure-theory', 'stochastic-calculus']"
4853268,Spectrum of linear operator $Tf(x) = f(x+1) + f(x-1)$ on $L^2(\mathbb{R})$,"We are working on the Hilbert space $H = L^2(\mathbb{R})$ and consider the bounded linear operator $T : H \to H$ defined by $(Tf)(x) = f(x+1) + f(x-1)$ . What is the spectrum of $T$ ? What I've tried: It's not so hard to show that $T$ is indeed a linear operator and bounded, with $\|Tf\|^2 \leq 4\|f\|^2$ wrt the $L^2$ norm on $\mathbb{R}$ . Using for instance the functions ${\bf1}_{[-n,n]}$ , I could even show that $\|T\| = 2$ . Also not so hard to show that $T$ is self-adjoint, being the sum of obvious self-adjoint $T_1f(x) = f(x+1)$ and $T_2f(x) = f(x-1)$ , but this can also be proved by straightforward calculation. The spectrum is therefore a subset of $[-\|T\|,\|T\|]$ . I do not know how to proceed.","['hilbert-spaces', 'operator-theory', 'lebesgue-integral', 'functional-analysis']"
4853311,"Combinatorics, 72 groups of 4.","I’m doing an exercise where I simulate a cohort of 288 students and sort them into different groups. For one of the questions, my task is to place the entire cohort into 72 groups of 4, simulating their homework groups. Using code I’ve done this easily enough, however, I’m having trouble with the follow-up question. It asks how many different ways of sorting the 288 students into these 72 groups exist. My immediate thought was to use the multinomial coefficient, I got a very large answer: $${n\choose n_1,n_2,…,n_{72}}= \frac{n!}{n_1!n_2!…n_{72}!}= \frac{288!}{(4!)^{72}}\approx 3.03\times 10^{485}$$ using wolframAlpha’s calculator.
I’m unsure of whether this is the answer my lecturers are looking for or whether I’ve made a mistake somewhere. The follow-up questions ask for probabilities of specific events with trivially small answers due to the large sample space of choices. I’d appreciate any help with this or points in the right direction. Maybe I’m just overthinking and it is supposed to be that large. Edit: Some useful comments have helped me realise that my answer is true if the order of the groups themselves matters. The question itself is slightly ambiguous on this point as it doesn't mention how we should treat the groups, however, given the fact that the groups are numbered and are simulating groups of students who take simulated tests, I am assuming that the order of said groups does indeed matter. Also, I should clarify what I meant when I spoke about the follow-up questions. One such question goes ""What is the probability that a student ends up in the exact same homework group (one of the 72 groups of 4) as the other people from their report group (report group: a separate group formed by splitting the cohort into 16 groups of 18(labs), then, and within these labs, students are further split into 6 groups of 3, which are labelled 'A to F' and exist within each lab)?"" For this follow-up question, as each report pod has 3 members, 2 not including the student in question, then, each homework group that satisfies the condition in the question must contain the entire report pod and then there are 285 candidates for the fourth member. I thought that the number of ways we could choose a homework group in this way was $288*285$ - 288 candidates for the student in question and then there are only 285 further choices to make given we fix the report pod into the homework group. Then, assuming each way of choosing the homework groups is equally likely, I deduce the probability of the event as $$\frac{82080}{3.03\times 10^{485}}$$ which is extremely small. The final follow-up asks what the expected number of students in one of the groups of 18 with at least one fellow student from their homework group(one of the 72 groups of 4, chosen independently of the other groups) that was also in their report pod(a group of 3 students within the same group of 18. As you can see, near-zero probabilities don't help me much when dealing with the second follow-up, unless I'm just calculating the probability itself in the completely wrong way. However, I'm not sure how else to think about it. Thank you!","['combinatorics', 'probability']"
4853316,Orthogonal trajectories to family of curves $\left\Vert x \right\Vert_p=1$ where $x\in\mathbb{R}^2$,"I have been trying to find the orthogonal trajectories of the family of $p$ -norm curves $\left\Vert x \right\Vert_p=1$ , where $x\in\mathbb{R}^2$ and $p>0$ . I eventually reached a step where I must find $p(x,y)$ such that $(1-x^{p(x,y)})^\frac{1}{p(x,y)}=y$ for all $x,y\in(0,1)$ . Does a closed-form expression for $p(x,y)$ exist? Graphing the equation with $p$ on the $z$ axis in a 3D graphing calculator at least suggests that $p$ is injective. More importantly, is this not a good approach for finding the orthogonal trajectories? I am following the steps described in Find the orthogonal trajectories of the family of curves .","['ordinary-differential-equations', 'plane-curves', 'curves', 'normed-spaces', 'closed-form']"
4853326,Uniqueness of complete binary trees,"While writing a code about Huffman coding (it doesn't matter what it is), I came with an idea that I could check whether the tree would be flat * without actually constructing it. For that quick check (which I won't specify here) to work, it would suffice that one little lemma is true and that is what I couldn't prove. The hypothesis: Every rooted tree which satisfies: it is completely binary, the number of leaves in it is a power of 2 and if it has a leaf at depth $d$ , then it has less than $2^{k-1}$ leaves at depth $d+k$ , for all $k\ge2$ , is necessarily flat. My thoughts: It is hard to test because of the condition 2: there are just a few completely binary trees with 2 and 4 leaves, but many with 8. However, I didn't find any counterexample to this hypothesis. I tried to prove it clasically case by case together with simple logic but nothing... I also tried formulating the conditions in terms of series where $i$ -th coefficient is the number of leaves at depth $i$ , but it didn't see much glory either. The definitions: a tree is a finite non-empty acyclic graph, a rooted tree is a tree with one distinguished node called the root , the depth of a node $x$ in a rooted tree is the number of edges in the shortest path from $x$ to the root, a child of a node $x$ in a rooted tree is a neighbour of $x$ which is not on the shortest path from $x$ to the root, a leaf of a rooted tree is its node with no children, a completely binary tree is a rooted tree where every node has exactly 0 or 2 children, a flat tree is a rooted tree whose all leaves have the same depth.","['trees', 'combinatorics']"
4853350,Strict order and adjacent elements,"Task: Prove that there is no strict order on 14 elements in which there are exactly 50 pairs of adjacent elements. Some clarifications: Elements x, y of order (X, <) are adjacent if x < y and there is no z such that x < z < y. My thinking: (in this text: pair of adjacent elements = adjacent pair) In fact, the maximum number of adjacent pairs that we can get on 14 elements is 49. Let's build a bipartite graph in which both parts consist of 7 elements. As a result, this will give us 7 * 7 = 49 adjacent pairs. Bipartite graphs where there is not the same number of elements in each part are not suitable for us: 6 * 8 = 48, 5 * 9 = 45, etc., all these cases will give less than 49 pairs of adjacent elements. But I have a problem: I can’t strictly prove that the maximum number of adjacent pairs that we can get will be exactly 49. I need to prove that besides a bipartite graph with 7 elements in each part, there is no other option for implementing strict partial order so that it reaches >= 49 adjacent pairs.","['order-theory', 'discrete-mathematics']"
4853353,Property of villainous matrices,"Say that an $N \times N$ binary matrix $M \in \mathrm{Mat}_{N \times N}(\{0,1\})$ is villainous if it does not contain the submatrix $I_2 = \begin{pmatrix}
1& 0\\
 0 & 1
\end{pmatrix}$ . Show that for any constant $\kappa < 1/80$ , every villainous $N\times N$ matrix contains a submatrix of size at least $\kappa N\times \kappa N$ such that all of the elements of the submatrix are equal. I've tried to think about the problem and had a few thoughts. For example, a villainous matrix may contain the submatrix $\begin{pmatrix}
0 & 1\\
1 & 0
\end{pmatrix}$ , and so there's some level of 'invariance' here: the transpose of a villainous matrix is not necessarily villainous, so maybe a symmetry argument will not be helpful here. Next I tried to define precisely what a submatrix is, because the book doesn't mention it at all. It cannot refer to a contiguous block, since the statement wouldn't be true then: if we take a $N \times N$ matrix with $1$ 's on the diagonals directed north-east to south-west and spaced 3 positions apart with all other elements be zeroes, it does not contain $I_2$ as contiguous block and it does not contain (contiguous) $3 \times 3$ submatrix with equal elements. So, by $N \times N$ ""submatrix"", I'm pretty sure we're referring to the intersection of any $N$ rows and $N$ columns. The problem though is that I'm not sure how I might attack the problem. The book is for a first course in linear algebra and only marks it as a medium difficulty problem, so I don't believe we'll absolutely need to resort to any deep theorems in linear algebra to make real progress on the problem. Any help will be great!","['matrices', 'linear-algebra']"
4853402,Is this a general form for the confidence interval of a uniform distribution?,"Let $X_1, \ldots, X_n$ a random sample with $X_i \sim \mathcal{U}[0,
    \theta]$ (where $\mathcal{U}$ = uniform dist). Let $Y = \max(X_1, \ldots, X_n)$ , the MLE of $\theta$ . It
can be proven that $U = \frac{Y}{\theta}$ has density \begin{align*}
    f_U(u) = \begin{cases}
        n u^{n-1} & 0 \leq u \leq 1 \\ 
        0 & \text{otherwise}
    \end{cases}
\end{align*} Evidently, $P(A \leq \frac{Y}{\theta} \leq B)$ is given by \begin{align*}
F_U(B) - F_U(A)
\end{align*} Then, if $\mathcal{P}(A, B) := P(A \leq \frac{Y}{\theta} \leq B)$ , we have \begin{align*}
\mathcal{P}(A, B) 
&= n \left[\int_0^{B}  u^{n-1} ~ du - \int_0^{A} u^{n-1}
\right] \\ 
&= n \left[ \frac{B^n}{n} - \frac{A^n}{n} \right]  \\ 
&= B^n - A^n
\end{align*} From this follows that $\mathcal{P}(\alpha^{\frac{1}{n}}, 1) = 1 - \alpha$ . I want to use this to form confidence intervals for $\theta$ , but I am unsure about whether my procedure to do this is correct. It takes simple manipulations to show that \begin{align*}
P(\alpha^{\frac{1}{n}} \leq \frac{Y}{\theta} \leq 1) = P( Y \leq \theta \leq
\frac{Y}{\alpha^{\frac{1}{n}}}) = 1 - \alpha
\end{align*} Thus, the expression seems to serve to produce the CI $[Y, Y
\alpha^{-\frac{1}{n}}]$ with confidence $1-\alpha$ . For example, for $\alpha = 1/2
$ , we have that $\theta$ will belong to $[Y, \frac{Y}{2^{\frac{1}{n}}}]$ .
Since $2^{\frac{1}{n}} \to 1$ as $n \to \infty$ , with $n$ sufficiently large this means $\theta \in [Y, Y + \epsilon]$ for a very marginal $\epsilon$ , with probability $95\%$ . Is this correct? If not, how can one use the derivations presented to form confidence intervals for the real value of $\theta$ ?","['statistics', 'confidence-interval', 'parameter-estimation', 'probability-theory', 'probability']"
4853444,"(Dis)Prove $\forall x \in \mathbb{R}, \sqrt{x^2} = x$","I'm in my discrete math class, and I'm being asked to prove or disprove: $\forall x \in \mathbb{R}, \sqrt{x^2} = x$ I think that would be false because I know that when you square a negative number, the result is positive. HOWEVER the reason that I'm uncertain is because I learned in algebra that every positive real number has $2$ real square roots, one of those being negative, which would make the above statement true.",['discrete-mathematics']
4853471,Natural Probability distribution on finite group involving irreducible representation and characters,"Let $\lambda$ be an irrep (irreducible representation) of some finite group $G$ (say over the complex number $\mathbb{C}$ ). Then consider the function $P_\lambda: G \to \mathbb{R}$ given by $$
P_\lambda(g) = \frac{1}{|G|} \chi_\lambda^*(g) \chi_\lambda(g) = \frac{1}{|G|} |\chi_\lambda(g)|^2,
$$ where $\chi_\lambda(g) = Tr(\lambda(g))$ is the character corresponding to $\lambda$ . Note that $P_\lambda$ can be interpreted as a probability distribution on the set $G$ because $P_\lambda \geq 0$ and $$
\sum_{g \in G} P_\lambda(g) = \frac{1}{|G|} \sum_{g \in G} \chi_\lambda^*(g) \chi_\lambda(g) = \langle \lambda, \lambda \rangle = 1,
$$ which follows from the inner product of characters and the irreducibility of $\lambda$ . Does this probability distribution have a name? Or has this probability distribution ever been studied? In what contexts has this distribution come up? Of course if $\lambda$ is $1$ -dim then this is just the uniform distribution on $G$ but more generally this seems to be an interesting and natural distribution.","['probability-distributions', 'representation-theory', 'finite-groups', 'group-theory', 'probability-theory']"
4853476,Does a set of full measure contain an affine copy of any countable set?,"In this paper here (beginning of Section 1.3 at page 4) it is stated that any measureable set $E\subset[0,1]$ with $\lambda(E)=1$ contains an affine copy of any countable set $A\subset \mathbb{R}$ . Is the same true in multiple dimensions? I.e. does any measureable set $E\subset[0,1]^n$ with $\lambda^n(E)=1$ contain a copy of any countable set $A\subset \mathbb{R}^n$ ?","['measure-theory', 'lebesgue-measure', 'analysis', 'real-analysis']"
4853488,Why does this nominally divergent limit of an infinite sum of bessel functions converge,"The setup isn't important, but in case you're curious, this is a physically relevant thing I'm trying to calculate that should have a finite value.
This equation gives the electric field of a small patch of voltage $V$ of length $l$ on the surface of a grounded conducting cylinder of radius $R$ in the center of the cylinder: $$
|\mathbf{E}|=\frac{Vl^2}{2\pi R^3}\lim_{\epsilon\rightarrow 0^+}\sum_{m=1}^\infty \frac{\alpha_{1m}\exp\left(-\alpha_{1m}\epsilon\right)}{J_2(\alpha_{1m})}\approx 1.77578 \frac{Vl^2}{2\pi R^3}
$$ $J_1$ is the first Bessel function of the first kind, and $\alpha_{1m}$ is it's $m$ 'th zero (not including the one at $x=0$ ). The $\approx$ comes from me evaluating this numerically.
I understand that I cannot move the limit into the sum because if I do, the sum no longer converges. What I don't understand is why it even has a well defined limit with the limit outside of the sum. For large $m$ , the zeros go toward $\alpha_{1m}\rightarrow (m+1/4)\pi$ . For large inputs $J_2(x)$ asymptotes to: $$
J_2(x)\rightarrow -\sqrt{\frac{2}{\pi}}\sin\left(x+\frac{\pi}{4}\right)\sqrt\frac{1}{x}
$$ So it seems to me that the sum for large $m$ turns into: $$
\sum_{m=N}^\infty m^{3/2} (-1)^m\exp(-m\pi\epsilon)
$$ And even if I add even and odd terms together, I should get $$
\sum_{m'=N/2}^\infty m'^{1/2} \exp(-2m'\pi\epsilon)
$$ So I think as $\epsilon$ goes to zero, my sum will go to infinity. Certainly the above sum diverges, so I must have done something too handwavy with limits here. Strangely though, as $\epsilon$ goes to zero, the sum goes to a finite value.","['convergence-divergence', 'bessel-functions', 'sequences-and-series']"
4853516,Can a discontinuous function have a continuous derivative?,"Main Question: Is it possible for a function f to have a derivative at a point x but be discontinuous at that point? Consider a function that is defined as follows: $$f(x)=\begin{cases} x^2 & x \leq c \\ax+b & x > c \end{cases}$$ We are asked to find constants a,b,c such that the derivative of f at c exists.  Consider the situation where $a=2c$ .  Following the limit definition of the derivative, we have that $f'(c)$ is equals to: $$\lim_{h\to0}\frac{f(c+h)-f(c)}{h}$$ provided that the limit is defined.
To see that, we may examine the limit as $x\to0-$ and as $x\to0+$ . $$\begin{equation}\lim_{h\to0+}\frac{f(c+h)-f(c)}{h} \\ =\lim_{h\to0+}\frac{a(c+h)+b-(ac+b)}{h}\end{equation}\\=\lim_{h\to0+}\frac{ah}{h}\\=a=2c$$ For when $x\to0-$ : $$\begin{equation}\lim_{h\to0-}\frac{f(c+h)-f(c)}{h} \\ =\lim_{h\to0-}\frac{(c+h)^2-c^2}{h}\end{equation}\\=\lim_{h\to0-}\frac{2ch+h^2}{h}\\=2c$$ Thus, by definition if $a=2c$ then for all values b $f'(c)$ exists and is equal to 2c.  Here is the issue, if $f'(c)$ exists for all values of b with the aforementioned condition then it could certainly be possible that $f(x)$ is discontinuous at c which contradicts an earlier theorem which states that if a function f has a derivative at a point x, then it is also continuous at x.
This is an example graph in desmos: https://www.desmos.com/calculator/acym19fdm2 . This creates a situation where the function f has a derivative at the point x but it is not continuous at that point.  Am I doing something wrong here? Edit: As X-Rui and many others in the comments and answers pointed out, my error was in computing the right-hand limit.  f(c) should always be equal to $c^2$ regardless of the limit since c is constant and f(c) was so defined.  Hence the right-hand side limit computation should be as follows: $$\begin{equation}\lim_{h\to0+}\frac{f(c+h)-f(c)}{h} \\ = \lim_{h\to0+}\frac{a(c+h)+b-c^2}{h}\end{equation}$$ From here we intend to find an values of a,b,c such that this limit will agree with the value of the left limit of 2c.  Assume that there exists such a value c, then since f is differentiable at c f is continuous at c.  Hence $ac+b=c^2$ by continuity (as pointed out by stoic-santiago).  Then the limit above simplifies to $$\lim_{h\to0+}\frac{a(c+h)+b-(ac+b)}{h}=a$$ Hence our requirement of differentiability simplifies to the requirement that $a=2c$ (along with the above requirement for continuity). Therefore, any a,b,c satisfying the two conditions $ac+b=c^2$ and $a=2c$ will suffice.","['continuity', 'calculus']"
4853594,Question about dimensionality of radians and their function when double integrating in polar coordinates,"When integrating differential forms, we know classically that $$ dx  \, dy = -dy \, dx$$ Which geometrically comes from the orientation of an area formed by the two forms, such that the counter-clockwise motion from $dx$ to $dy$ defines a positive area, and so forth. Radians are typically given as being a dimensionless and unitless quantity, but observe the following integral expressed in polar coordinates: $$ \int_\limits{\theta} \int_\limits{r} dr \, d\theta$$ (i) Does it hold still that $dr \, d\theta = - d\theta \, dr$ ? Furthermore, while in the cartesian coordinates $dx , dy$ we can see that walking in some direction has a vector-like quality to it, which extends to tangent bundles and makes integrals obvious. (ii) If radians are truly dimensionless then how do you extend the wedge product to $d \theta$ and $dr$ ? Even neglecting integration and differentiation, are you not allowed to put a vector field on a surface given in polar coordinates? If either are true, it seems there must be a dimensional and oriented quantity associated with a displacement in the $\theta$ -direction. Neglecting all formalism, I can intuitively walk counter-clockwise around a circle (positive radian displacement) and then walk clockwise instead (negative radian displacement). What then would give radians any less legitimacy as a dimensional unit than meters or joules? Or perhaps the 360 degrees of the circle, where there's a similarity between the conversion of radians to degrees as there is for the conversion of Celsius to Fahrenheit. Thanks in advance for the help.","['multivariable-calculus', 'calculus', 'differential-forms', 'differential-geometry']"
4853599,"$f$ and $1/p$ are positive and decreasing, $\int_1^\infty f<\infty$, $\int_1^\infty\frac{1}{p}=\infty$, show $\lim_{x\rightarrow \infty}p(x)f(x)=0$","Here I'm interesting in prove (or disprove and counterexamples) the following Claim : Claim :
For $p: [1,\infty)\longrightarrow\mathbb R$ and $f: [1,\infty)\longrightarrow\mathbb R$ where $p$ is positive and monotonic increasing, $f$ is non-negative and monotonic decreasing. If $\int_1^\infty f<\infty$ , $\int_1^\infty\frac{1}{p}=\infty$ , then $\lim\limits_{x\longrightarrow \infty}p(x)f(x)=0$ . This claim is based on the following proposition: Proposition :
For $f: [1,\infty)\longrightarrow\mathbb R$ to be nonnegative and decreasing with $\int_1^\infty f<\infty$ , show that $\lim\limits_{x\longrightarrow\infty}xf(x)=0$ . For the above proposition is the special case for the claim when $p(x)=1/x$ . The proof of the proposition can proceed in many ways. The following is my proof. Proof of the proposition: Assume not, there exists $\varepsilon_0>0$ , $\forall n\in\mathbb N_+$ , there exists $x_n\in \mathbb R$ such that $x_n>n$ and $f(x_n)\geq \frac{\varepsilon_0}{x_n}$ . Then we integrate $f(x)$ over $[x_n/2,x_n]$ , by the monotonic decreasing property of $f$ , we may know that $f(x)\geq f(x_n)\geq\frac{\varepsilon_0}{x_n}\geq \frac{\varepsilon_0}{2x}dx$ for all $x\in [x_n/2,x_n]$ . Hence: $$\int_{x_n/2}^{x_n} f\geq \int_{x_n/2}^{x_n} \frac{\varepsilon_0}{2x}dx=\frac{\varepsilon_0 \log 2}{2}$$ contradicting with the condition that $\int_1^\infty f<\infty$ My Question : I have tried to prove the claim in the similar way to the proof of the proposition . Then I have $$\int_{x_n/2}^{x_n} f\geq \int_{x_n/2}^{x_n} \frac{\varepsilon_0}{p(2x)}= \int_{x_n}^{2x_n} \frac{\varepsilon_0}{2p(t)}dt$$ However I find it difficult to use Cauchy criterion or the monotone convergence theorem to argue that $\int_{x_n}^{2x_n} \frac{\varepsilon_0}{2p(t)}dt\geq \eta$ for some $\eta$ . Or I can proceed the proof only if $p$ has an explicit form. Any thoughts and inspirations are welcome. Thank you!","['divergent-series', 'improper-integrals', 'analysis', 'real-analysis', 'calculus']"
4853632,Proof verification: Steinhaus Theorem,"Theorem (Steinhaus): If $E \subseteq \mathbb{R}$ is a measurable set with positive Lebesgue measure, then the set $$
E - E = \{e_1 - e_2 : e_1, e_2 \in E\}
$$ contains an open interval around the origin. Let $m$ denote the lebesgue measure. It suffices to prove the case where $m(E) < \infty$ , since $m$ is a semifinite measure. There exists an interval $(a,b)$ where $a,b\neq \pm \infty$ , such that $m((a,b)\cap E) > \frac{3}{4} m((a,b))$ (by a well-know theorem? Chapter 1 exercise 30 in folland). Let $I = [a,b)$ . It is clear that $I$ satisfies the same inequality since it differs $(a,b)$ by a measure of $0$ . We we will show that $(-\frac{1}{10}m(I),\frac{1}{10}m(I)) \subseteq E-E$ . Suppose for sake of contradiction that $\exists k \in (-\frac{1}{10}m(I),\frac{1}{10}m(I))$ such that it is not in $E-E$ . We assume $k>0$ , since $E-E$ is clearly symmetric. Consider the intervals $I_0 = [a,a+k), I_1 = [a+k,a+2k),\dots, I_n = [a+nk,a+(n+1)k)$ where $a+nk<b\leq a+(n+1)k.$ Basically $\{I_j\}$ is the minimal half interval cover of $[a,b)$ each of length $k$ . Then $$m(E\cap I)\leq m(\bigcup_{i=0}^n E \cap I_i)=\sum_{i=0}^nm(E\cap I_i),$$ since $I \subseteq \cup I_i$ and $\{I_i\}$ are pairwise disjoint. Assume that $n+1$ is even. The odd case is similar with some trivial details in the boundary case. Let $\omega_0 = m(E\cap I_0), \omega_1 = m(E\cap I_1)\omega_2 = m(E\cap I_2), \dots ,\omega_n = m(E\cap I_n)$ . Notice that $E\cap I_{2t+1} \subseteq I_{2t+1} \setminus (E \cap I_{2t} + k)$ , since if $x \in E\cap I_{2t+1}$ and $x \in (E \cap I_{2t} + k)$ , then $x=y+k$ for some $y \in E \cap I_{2t}$ , so that $x-y = k$ , a contradiction to $k\notin E-E$ . Thus $m(E\cap I_{2t+1}) \leq m(I_{2t+1} \setminus (E \cap I_{2t} + k)) = m(I_{2t+1}) - m(E \cap I_{2t} + k)$ . By translation invariance of lebesgue measure and the fact that each interval is of length $k$ , it is also equal to $k-\omega_{2t}$ . Thus, $\omega_{2t}+\omega_{2t+1} \leq k$ . So $$\sum_{i=0}^nm(E\cap I_i)\leq \frac{n+1}{2}k \leq \frac{m(I)/k+1}{2}\cdot k= \frac{m(I)}{2}+\frac{1}{2}k<\frac{m(I)}{2}+\frac{m(I)}{10}<\frac{3}{4}m(I),$$ So $m(E\cap I) < \frac{3}{4}m(I)$ . But $m(E\cap I) > \frac{3}{4} m(I)$ by our choice of $I$ , a contradiction.","['measure-theory', 'lebesgue-measure', 'analysis', 'real-analysis']"
4853652,Proving a Proposition about outer measures,"I want to prove a proposition, that tells us a way to generate outer measures. I will first start with the definition of Definition (outer measure)
Let $X$ be a set. An outer measure is a function $\mu^*$ defined on the collection of all subsets of $X$ such that (i) $\mu^*(\emptyset)=0$ (ii) if $A \subseteq B \Rightarrow \mu^*(A)\leq \mu^*(B)$ (iii) $\mu^*(\cup_{i=1}^{\infty} A_i)\leq \sum_{i=1}^{\infty} \mu^*(A_i)$ , where $A_i$ are arbitrary subsets of $X$ . Proposition Suppose $\mathcal{C}$ is a collection of subsets of $X$ such that
(1) $\emptyset \in \mathcal{C}$ (2) there exist $D_1,D_2,... \in \mathcal{C}$ such that $X=\cup_{i=1}^{\infty}D_i$ . Further, suppose $l:\mathcal{C}\rightarrow [0,\infty]$ with $l(\emptyset)=0$ . Define $\mu^*(E):=\inf\{\sum_{i=1}^{\infty} l(A_i): A_i \in \mathcal{C} \text{ for all i 
 and } E\subseteq \cup_{i=1}^{\infty} A_i\} $ Then $\mu^*$ is an outer measure. Proof:
(i) holds by definition of $l$ .
(ii) holds as a property of the infimum, i.e. if $A \subset B \Rightarrow infA \leq infB$ Now we came to (iii):
To get a better feeling of what is going on, I wanted to first show a weaker condition than (iii).
I want to show that, for $A,B \subseteq X$ we get: $\mu^*(A \cup B)\leq \mu^*(A) +\mu^*(B)$ . For this prove I will use the \epsilon-definiton of the infimum, i.e.
I is the infimum of some set M, iff I is a lower bound of M $\forall \epsilon>0 \exists m \in M: I+\epsilon>m$ So, lets start:
Since $A \subseteq X$ we can find a cover $A_i$ such that $A\subseteq \cup_{i=1}^{\infty} A_i$ .
By the same argument, $B\subseteq \cup_{i=1}^{\infty} B_i$ . Since $\mu^*(A)$ is the infimum of the set of sums of the form $\sum_{i=1}^{\infty} A_i $ , we can find a $\epsilon/2$ such that $\mu^*(A)+\frac{\epsilon}{2}> \sum_{i=1}^{\infty}A_i$ The same applies to $\mu^*(B)$ and we get: $\mu^*(B)+\frac{\epsilon}{2}> \sum_{i=1}^{\infty}B_i$ Now since $A\subseteq \cup_{i=1}^{\infty} A_i, B\subseteq \cup_{i=1}^{\infty} B_i$ we know that $A \cup B \subseteq \bigl(\cup_{i=1}^{\infty} A_i \bigr) \bigcup \bigl(\cup_{i=1}^{\infty} B_i \bigr)$ .
Thus we can find a covering $\cup_{i=1}^{\infty} (A_i \cup B_i)$ of $A \cup B$ . Now the idea would be to write $\mu^*(A \cup B)\leq \sum_{i=1}^{\infty}l(A_i \cup B_i)=\sum_{i=1}^{\infty} l(A_i)+\sum_{i=1}^{\infty} l(B_i)\leq \mu^*(A )+ \frac{\epsilon}{2}+\mu^*(B)+ \frac{\epsilon}{2}=\mu^*(A )+ \mu^*(B)+ \epsilon$ And since this wolds for all $\epsilon$ we get the result. But I do not know how to argue the step $\sum_{i=1}^{\infty}l(A_i \cup B_i)=\sum_{i=1}^{\infty} l(A_i)+\sum_{i=1}^{\infty} l(B_i)$ . Since the only thing about $l$ I know is that it maps the empty set to zero. It was never mentioned that it behaves linear or any other property.","['measure-theory', 'outer-measure']"
4853656,Why is $y$ not defined as a function of $x$ in $x=4y^2$?,"$$x = 4y^2$$ This equation confuses me, how is $y$ not defined as a function of $x$ ? I manipulated the function and it works just fine,
but my book says  ""Explain why the given equation does not define $y$ as a function of $x$ ."" $$\frac{x}{4} = y^2 \tag1$$ $$y = \sqrt \frac{x}{4} \tag2$$ $$y = \frac {\sqrt x}{2} \tag3$$",['functions']
4853690,What is the volume of the largest surface of revolution with constant Gaussian curvature that can be placed inside the unit cube?,"Consider a surface of revolution $S$ and an embedding $e:S \hookrightarrow X^3$ for $X^3=[0,1]^3$ with points $p,q$ elements of $\partial X^3$ where $\partial X^3=X^3-(0,1)^3$ for $\mathrm {sup}~ \mathrm{dist}(p,q)=\sqrt{3}$ . What is $\rho_{\mathrm{max}}=\mathrm{max} \lbrace \mathrm{vol}(S) \rbrace_{p,q}$ assuming $S$ must remain a surface of revolution and have constant positive Gaussian curvature? In other words, what is the volume of the largest surface of revolution with constant Gaussian curvature that can be embedded in $X^3$ with a pair of antipodal corners as cone points?","['riemannian-geometry', 'geometric-topology', 'optimization', 'differential-topology', 'differential-geometry']"
4853698,"Does the copula define a measure on $[0,1]^n$?","Sklar's Theorem states for a random variable $(X_1,...,X_n)$ on $\mathbb{R}^n$ , there exists a copula $C: [0,1]^n \rightarrow [0,1]$ such that $\forall (x_1,...,x_n) \in \mathbb{R}^n$ $$
C(F_1(x_1),...,F_n(x_n)) = P(X_1 \leq x_1,...,X_n \leq x_n).
$$ where $F_k(t) = P(X_k \leq t)$ . Also $C(u_1,...,u_{i-1}, 0, u_{i+1},...,u_n) = 0$ and $C(1,...,1,u,1,...,1) = u$ for each component. I want to define a Borel measure $\mu$ on $[0,1]^n$ using the copula. One would set $$
\mu([0,F_1(x_1)] \times \cdots \times [0,F_n(x_n)]) = C(F_1(x_1),...,F_n(x_n)), \ \forall x \in \mathbb{R}^n \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1)
$$ and require ${\pi_k}_\#(\mu)$ (the pushforward of $\mu$ to $[0,1]$ ) is uniform for $k=1,...,n$ . However, $(1)$ might only define $\mu$ for one or two sets, leaving the measure of most sets in $[0,1]^n$ undefined. $\textbf{Question}$ : Does there exist a Borel measure $\mu$ that satisfies $(1)$ and has ${\pi_k}_\#(\mu)$ uniform? (Note if $F_k$ are not continuous, then $F_k(X_k)$ is not necessarily uniformly distributed on $[0,1]$ , so $(F_1(X_1),...,F_n(X_n))$ doesn't work).","['measure-theory', 'probability-theory', 'analysis', 'real-analysis']"
4853727,On counterexamples to Euler's conjecture using Bremner's and Durman's elliptic curves,"Noam Elkies found the first counter-example to Euler's sum of powers conjecture that, $$a^4+b^4+c^4 = d^4$$ was not solvable by expressing the equation as an intersection of two quadric surfaces dependent on a parameter $m$ . There are only eleven known rational $m$ of small height, with 4 found by Andrew Bremner discussed in this post . Smaller $m$ tend to yield $(a,b,c,d)$ as polynomials with small coefficients. I. Bremner's elliptic curve Find rational $(v,z)$ such that, $$z^2 = 42856039590241 + 4301879366236v - 65877950554v^2 - 710638564v^3 - 109887359v^4 $$ then we have, $$10^4(690689 - 17642v - 407v^2)^4 + 10^4(260257 + 43910v + 473v^2)^4+ z^4 = 3^4(2676749 + 26902v + 3549v^2)^4$$ For any $v$ , the terms satisfy the nice relation, $$m=\frac{(a + b)^2 - c^2 - d^2}{a^2 + a b + b^2 + (a + b)d} =-\frac{5}{44}$$ with $m =-\frac{5}{44}$ being one of Bremner's. They also obviously obey, $$-c^4+d^4 = 0 \text{ mod }5^4$$ a general property of such Diophantine equations. There are only two known solutions $v$ with ""smallish"" height, $$v = -\frac{103605703}{47433977},\; \frac{2950708837949}{171081882189}$$ both yielding, after removing common factors, the same, $$2024155336530384440^4+ 585715960903147640^4 + 2556827383749699103^4= 2778996090487120353^4$$ where $d \approx 2.77\times10^{18}$ . II. Durman's elliptic curve Find rational $(v,z)$ such that, $$z^2 = -583937117447 + 1322131490860v - 1113337123194v^2 + 437040946060v^3 - 64494011447v^4 $$ then we have, $$12^4(-50071 + 28490v + 2829v^2)^4 + 12^4(15601 + 21202v - 14883v^2)^4+ z^4 = (829109 - 804482v + 268253v^2)^4$$ and for any $v$ , $$m=\frac{(a + b)^2 - c^2 - d^2}{a^2 + a b + b^2 + (a + b)d} =-\frac{41}{36}$$ There are only two known solutions $v$ , $$v = \frac{416887}{178391},\; \frac{149943493}{118146461}$$ both yielding the same, $$588903336^4 + 859396455^4 + 1166705840^4 = 1259768473^4$$ where $d \approx 1.25\times10^{9}$ . Question: Compared to the five related elliptic curves of this post , it seems Bremner's first pair of $v$ is unusually ""large"", while Durman's pair is not quite so. For both, are there $v$ of smaller height, or at least others such that it yields $d < 10^{28}$ ?","['number-theory', 'elliptic-curves', 'diophantine-equations']"
4853766,Homology groups of $X \times S^1$,"I am trying to solve the following question: Given a topological space X, show that $H_{i}(X \times S^1) = H_{i}(X) \times H_{i-1}(X)$ for $i>0$ and $H_{0}(X \times S^1)=H_0(X)$ . The (almost) same problem was posed here: Calculating Homology Groups of $S^1\times X$ , however there is no answer and it uses the additional assumption that the homology groups of X are free. I have the following questions: Can the assumption of freeness be left out? Can this be generalised to $S^n$ for larger n? Note that I have managed to solve the problem with the assumption of freeness, using Mayer-Vietoris: Set $A=X \times S^1 \setminus \{p\}$ and $B=X \times S^1 \setminus \{q\}$ for distinct points $p, q$ and $Y=X \times S^1$ . Then Mayer-Vietoris gives the following long exact sequence: $\cdots \rightarrow H_p(A\cap B) \rightarrow H_p(A)\oplus H_p(B)\rightarrow H_p(Y)\rightarrow H_{p-1}(A\cap B)\rightarrow \cdots$ Denote the homomorphisms $d,i_A\oplus i_B, j_A+j_B$ in this order. Here A and B are homotopy equivalent to X and the intersection is homotopy equivalent to two copies of X. We can use the long exact sequence above to create a short exact sequence: $0 \rightarrow ker(d) \rightarrow H_p(Y) \rightarrow Im(d) \rightarrow 0$ By exactness we have $Im(d)=ker(i_A\oplus i_B)$ and $ker(d)=Im(j_A+j_B)=(H_p(A)\oplus H_p(B))/ker(j_A+j_B)=(H_p(A)\oplus)H_p(B))/ker(i_A\oplus i_B)$ . So I only need to compute $i_A\oplus i_B$ . Given a cycle $c_p\in S_p(X)$ , take corresponding cycles $a_p, b_p$ in the two copies of X that form $A\cap B$ . Then $i_A\oplus i_B$ takes both $a_p, b_p$ to $(c_p, -c_p)$ in $H_p(A)\oplus H_p(B)$ . Thus $ker(i_A\oplus i_B)=\langle a_p-b_p \rangle$ and $Im(i_A\oplus i_B)=\langle (c_p,-c_p) \rangle$ . The above short exact sequence becomes $0 \rightarrow (H_p(A)\oplus H_p(B))/\langle (c_p,-c_p) \rangle \rightarrow H_p(Y) \rightarrow \langle a_{p-1}-b_{p-1} \rangle \rightarrow 0$ The first group in this sequence is isomorphic to $H_p(X)$ and the last one to $H_{p-1}(X)$ . So, if I know that $H_{p-1}(X)$ is free, the sequence automatically splits and we are done. By the way that $d$ is defined, I would expect that the sequence splits even without the additional assumption, but I have not been able to find $H_{p-1}(X)\rightarrow H_p(Y)$ that satisfies the splitting condition. Any help regarding the two questions/comments on my solution is much appreciated!","['general-topology', 'abstract-algebra', 'homology-cohomology', 'algebraic-topology']"
4853823,$\rho(T)=\rho(T^*) $ if $T$ is defined between Banach Spaces.,"Let $T:E\rightarrow E$ be a continuous linear map between Banach spaces. We define $T^*:E^*\rightarrow E^*$ by $T^*(e^*)(e)=e^*(T(e))$ . Under these conditions prove that the resolvent set $\rho(T)=\{\lambda \:| T-\lambda I \: \text{ is invertible}\}$ satisfies $\rho(T)=\rho(T^*)$ . That $\rho(T)\subseteq \rho(T^*)$ is straightforward from the definition. Indeed, if $T-\lambda I $ is invertible, then in particular it is surjective and this implies that: $$f_1((T-\lambda)(x))=f_2((T-\lambda)(x))\: \forall x\in E\Rightarrow f_1=f_2$$ But this implies $T^*-\lambda I^*$ is injective. For surjectivity it is enough to consider $g=f\circ (T-\lambda I)^{-1}\in E^*$ , because $(T^*-\lambda I^*)(g)=f$ . For the other inclusion I am having trouble. Suppose we have $\lambda \in \rho(T^*)$ , then $T^*-\lambda I^*$ is invertible. Take $x\in \ker(T-\lambda I)$ , then $(T-\lambda I)(x)=0$ and so $(T^*-\lambda I^*)(f)(x)=0$ for all $f\in E^*$ . But because of surjectivity, this means $f(x)=0$ for all $f\in E^*$ and so by Hahn Banach, $\lVert x \rVert=0$ which implies $x=0$ . Hence, $T-\lambda I$ is injective. So far I haven't been able to prove $T-\lambda I$ is surjective.  I wanted to suppose by way of contradiction $E\setminus (T-\lambda I)(E)\not=\emptyset$ and build a nonzero function which is zero in $(T-\lambda I)(E)$ , but I cannot use geometric Hahn-Banach, because $(T-\lambda I)(E)$ is not necessarily closed.",['functional-analysis']
4853858,Expectation of a monotone function of CDF: $\mathbb E \left [g(F(X)) \right ]$,"If $X$ is a random variable with distribution function $F,$ then $\mathbb{E}[(F[X])^{-1/2}]$ can be computed by integration by parts, if $X$ has a continuous density $f$ . What happens in the general case? Can we always compute it (or bound it)?","['expected-value', 'cumulative-distribution-functions', 'statistics', 'probability']"
4853865,"Given a $3\times 3$ $\operatorname{adj} A$, find $A$",Given $\operatorname{adj}A=\begin{bmatrix} -1 & -2 & 1\\ 3 & 0 & -3 \\ 1 & -4 & 1 \end{bmatrix}$ . Find $A$ . My Attempt We know that $|\operatorname{adj}A|=|A|^{n-1}\Rightarrow |A|=\pm\sqrt{|\operatorname{adj}A|}=\pm2\sqrt3$ . Also it is well known that $A(\operatorname{adj}A)=|A| I\Rightarrow A=|A|(\operatorname{adj} A)^{-1}=\pm\frac{1}{2\sqrt3}\begin{bmatrix} -12 & -2 & 6\\ 6 & -2 & 0 \\ -12 & 6 & 6 \end{bmatrix}$ My doubt is that should there be a unique $A$ or there are two possibilities as shown in working above.,"['matrices', 'cayley-hamilton', 'linear-algebra', 'inverse']"
4854023,"Probability of subsequence 123456 in $n$ rolls, combinatorics approach?","I was trying to solve this problem: We roll a 6-sided die n times. What is the probability that all faces have appeared in order, in some
six consecutive rolls (i.e., what is the probability that the subsequence 123456 appears among the n
rolls)? My immediate reflex was to use Markov chains, but I soon backed down because of how tedious the calculations seemed. I know that it's supposed to be the correct way to tackle it but I've tried something different that ended up being wrong and I'd really appreciate it if you could help pointing out my mistakes. So I decided to count the number of sequences containing the subsequence 123456. Indeed, any sequence of $n$ numbers is equiprobable of probability $\frac{1}{6^n}$ . When we consider such we can pick any of $n-5$ starting point for the subsequence 123456, the rest of the sequence doesn't matter and each term can be any number of $\left\{1,2,3,4,5,6\right\}$ so we've got $6^{n-6}$ ways to pick the remaining numbers. So following this reasoning, I would be tempted to say that the wanted probability is $\frac{(n-5)6^{n-6}}{6^n} = \frac{n-5}{6^6}$ which obviously doesn't make sense because I could take an $n$ that's strictly greater than $6^6$ and it would be a probability strictly greater than $1$ ... I really struggle with combinatorics like that and I never know why my reasoning is wrong.","['dice', 'combinatorics', 'probability']"
4854035,Taking the limit in Holder's inequality,"I have a standard normal random variable $X\sim\mathcal{N}(0,1)$ and an event $E$ with $\mathbb{P}(E)=p$ , and this event is about $X$ and some other variables. I am interested in upper-bounding the expression $\mathbb{E}[1_E\cdot X^2]$ . The random variables $1_E$ and $X$ are $not$ independent. One obvious solution is using the Cauchy-Schwarz inequality and it gives $\mathbb{E}[1_E\cdot X^2]\leq\sqrt{\mathbb{E}[1_E]\mathbb{E}[X^4]}=\sqrt{3p}$ . This is, however, not strong enough for what I need. Ideally, I would want to have an upper-bound that involves $p$ , not $\sqrt{p}$ . In order to get this, I was thinking about using Holder's inequality: for any $p,q>1$ with $\frac{1}{p}+\frac{1}{q}=1$ , $\mathbb{E}[|XY|]\leq(\mathbb{E}[X^p])^{1/p}(\mathbb{E}[X^q])^{1/q}$ with $p=1+\varepsilon$ any take the limit $\epsilon\rightarrow 0_+$ . The main steps are as follows: \begin{align*}
\mathbb{E}[1_E\cdot X^2]&\leq (\mathbb{E}[1_E])^{1/(1+\varepsilon)}(\mathbb{E}[X^{2(1+\varepsilon)/\varepsilon}])^{\varepsilon/(1+\varepsilon)}\\
&=p^{1/(1+\varepsilon)}\left(\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}(x^2)^{(1+\varepsilon)/\varepsilon}e^{-x^2}dx\right)^{\varepsilon/(1+\varepsilon)}\\
&=p^{1/(1+\varepsilon)}\left(\frac{1}{\sqrt{2\pi}}\int_{0}^{\infty}x^{1/2+1/\varepsilon}e^{-x}dx\right)^{\varepsilon/(1+\varepsilon)}\\
&=p^{1/(1+\varepsilon)}\left(\frac{1}{\sqrt{2\pi}}\Gamma\left(\frac{3}{2}+\frac{1}{\varepsilon}\right)\right)^{\varepsilon/(1+\varepsilon)}
\end{align*} Taking the limit $\varepsilon\rightarrow 0_+$ , one gets $\mathbb{E}[1_E\cdot X^2]\leq p$ . I am not 100% sure about this very last step, is it correct? And if not, are there any other approaches that come to mind that I should try? Thank you!","['measure-theory', 'normal-distribution', 'expected-value', 'upper-lower-bounds', 'holder-inequality']"
4854053,Proof of $\forall$ statement given interval notation.,"I'm working on my discrete math homework, and I'm blanking on how to write a proof for a logical statement involving the quantifier $\forall$ The statement I need to prove is as follows: $\forall x\in \Bbb R,$ if $x \in [1,2],$ then $(3x-1) \in [2,5]$ We worked on some of those in class today, but as always, I didn't seem to take very strong notes, and can't remember what to do now. Thanks in advance for all the help.",['discrete-mathematics']
4854181,Embedding discrete functions as continuous functions,"$\require{AMScd}$ Given a map $f:\mathbb{N}\to\mathbb{N}$ . Is there a countable set $A \subset [0,1]$ , bijection $\tau:\mathbb{N} \to A$ , and a continuous function $g \in C([0,1],\mathbb{R})$ , such that $\tau \circ f = g \circ \tau$ , i.e., $$
f(n) =  \tau^{-1}(g(\tau(n)), \quad \forall n \in \mathbb{N}.
$$ $$
\begin{CD}
\mathbb{N} @>{f}>> \mathbb{N} \\
@V{\tau}VV @A{\tau^{-1}}AA \\
A @>{g}>> A 
\end{CD}
$$ Try for example: $$
f(n) = 
\begin{cases}
1, ~ n=2k+1\\
n/2, ~n=2k
\end{cases}, k \in \mathbb{N},
$$ and another example: $f(n) = m$ if $n$ is a product of $m$ prime numbers (identical or different).","['number-theory', 'analysis', 'real-analysis', 'general-topology', 'set-theory']"
4854195,Why do randomly drawn numbers tend to repeat themselves?,"I track the behavior of random numbers and I have discovered that once a number appears, it tends to reappear again shortly thereafter. For example, I've been tracking the Red Powerball in the Powerball lottery, it's the single red ball from a pool of $1$ through $26$ numbers. The plot below is a gap chart or plot ... a consolidation of the distance between when a number is drawn and the next time it is drawn again. For example, at position $1$ on the horizontal axis, indicates that $37$ times balls repeat on the next drawing. Positions $2$ and $3$ on the horizontal axis indicate that balls have appeared $40$ times once again $2$ and $3$ draws later respectively. As we move toward the right, we see the lengths of gaps between repeats increase gradually. At the very far right, there was a gap of a ball repeating after 198 draws, that's a drought. A Suspect Chart is also included to indicate a gap chart that would indicate manufactured repeats at locations $44$ through $50$ . Is there a statistical explanation for this phenomena? The number of games (aka draws) represented in the Powerball plot is $969$ and the number of draws in the Megemilions plot is $605$ . A chart of a Suspect Drawing . If one were to see this when analyzing gap analysis , then one could assert that the drawing was not random? Also added for completeness, is the Megamilions Blue Ball MB, which has the range $1$ through $25$ . It also indicates that when any given MB is drawn, it's likely to be drawn again within the next few draws. It would be nice if I could attach text data to this question for reviewers to download. Here is an example of $35$ random numbers (pool size $1$ through $10$ ) drawn in an Excel spreadsheet taken out to gap of $5$ . Horizontal position $1$ in my plot is gap $0$ , horizontal position $2$ in my plot is gap $1$ , ... and so on.","['random', 'statistics', 'probability-distributions', 'clustering']"
4854227,"There exists some real $2 \times 2$ matrix $A$, such that $A^2+A+I=0$?","There exists some real $2 \times 2$ matrix $A$ , such that $A^2+A+I=0$ ? $$
A\ =\ \left[\begin{array}{ c c }
a & b\\
c & d
\end{array}\right] $$ $$ A^{2}  = \left[\begin{array}{ c c }
a & b\\
c & d
\end{array}\right] \left[\begin{array}{ c c }
a & b\\
c & d
\end{array}\right] =\ \left[\begin{array}{ c c }
a^{2} +bc\  & ab+bd\\
ac+cd & bc+d^{2}
\end{array}\right] $$ $$ A^{2} \ +\ A\ +I\ =\ 0\ =\ \left[\begin{array}{ c c }
a^{2} +bc\  & ab+bd\\
ac+cd & bc+d^{2}
\end{array}\right] +\left[\begin{array}{ c c }
a & b\\
c & d
\end{array}\right] +\left[\begin{array}{ c c }
1 & 0\\
0 & 1
\end{array}\right] \ \ =\ 0 $$ $$ \left[\begin{array}{ c c }
a^{2} +bc\ +a+1 & ab+bd+b\\
ac+cd+c & bc+d^{2} +d+1
\end{array}\right] = 0 $$ $$ ab + bd + b = 0 $$ $$ b(a+d+1) = 0 $$ $$ ac + cd + c = c(a+d+1) = 0 $$ $$ (a+d+1) = 0 \text{ or } c = 0 \text{ or } b = 0 $$ $$ -d = a+1 $$","['matrix-equations', 'linear-algebra']"
4854251,"If all of the angles of a polygon are equal, and half of its sides are equal, then must the polygon be regular? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question If we have that all the angles in a 2d polygon are equal, and if we have that half the sides of the polygon are equal(for example, if the polygon has 10 sides, we know that 5 of them are equal), is it enough to prove that the polygon is regular? If so, how?","['geometry', 'polygons']"
4854283,Amending a differential geometry formula,"I'm reading my professor's notes on Differential Geometry. In the chapter regarding connections he deals with torsion and there is an observation in which he affirms that if $\alpha$ is a 1-form and $\nabla$ is a zero torsion connection then the following hols: \begin{equation}
\alpha(X,Y) = (\nabla_X\alpha)(Y) - (\nabla_Y\alpha)(X)
\end{equation} I'm trying to make sense of this equation but I think there is a typo of some  sort. Am I missing something or should this formula be amended?","['connections', 'differential-forms', 'differential-geometry']"
4854292,How many sides of an odd-sided equiangular polygon must be proven equal to conclude that it is regular?,"If we know that all angles are equal in a polygon with an odd number of sides, how many sides need to be shown to be equal to claim that the polygon is regular? For example, if we know that a triangle is equiangular, we can directly claim that it is regular. However, we cannot do the same for a pentagon, since there are pentagons which are equiangular but not equilateral. So, does it depend on the number of sides?","['angle', 'geometry', 'polygons']"
4854311,Evaluating $\int_{0}^{\infty}{\frac{1}{\cosh^{2k+1}(x)} dx}$,I tried : $$\begin{align}\int_{0}^{\infty}{\frac{1}{\cosh^{2k+1}(x)} dx}&=2^{2k+1}\int_{0}^{\infty}{(e^{x}+e^{-x})^{-(2k+1)}dx}\\&=2^{2k+1}\int_{0}^{\infty}{\frac{1}{u}\left(u+\frac{1}{u}\right)^{-(2k+1)}du}\\&=2^{2k+1}\int_{0}^{\infty}{u^{-2k}(u^2+1)^{-(2k+1)}du}\\&=2^{2k}\int_{0}^{\infty}{t^{-(k+1/2)}(t+1)^{-(2k+1)}dt}\end{align}$$ I know this has answers here but I'm wondering if what I have so far can be continued to arrive at one of those answers.,"['integration', 'definite-integrals', 'hyperbolic-functions']"
4854332,derivative of a matrix to a power of 1/2,"I'm trying to solve the following matrix derivative : $$\frac{d}{dx}(I + x\Sigma)^{1/2},$$ where $I$ is identity matrix and $\Sigma$ is a constant (positive definite) matrix which is not a function of $x$ . I tried to search for relavent materials, such as wikipedia and matrix cookbook textbook, but I failed to find a formula regarding derivative of a matrix to a power of 1/2. When $U(x)$ is matrix of a function of $x$ , is there a formula of $\frac{d}{dx}U(x)$ ? If so, how can I induce the formula? I tried to induce it using $\frac{dUV}{dx} = \frac{dU}{dx}V + U\frac{dV}{dx}$ , assuming $U,V$ are a function of $x$ , but I'm not sure whether this is useful for the desired result. Any help regarding this question would be grateful. Thank you.","['derivatives', 'matrix-calculus', 'linear-algebra']"
4854345,Which Property Did the Sport Climbing Point System Violate?,"In the 2020 Summer Olympics Sport Climbing was added as a new Olympic Sport.
Competitors had to compete in three categories: lead climbing, bouldering and speed climbing. Within each category the competitors were given a rank from $1$ to $n$ . $1$ being the best in the category, $2$ being second-best and so on. The total points by which the contestants are then ranked is given by $$
\prod_{i=1}^{n} r_{i}
$$ where $r_{i}$ denotes the competitor's order in category $i$ . These are then sorted, and the lowest score wins gold, the second lowest silver, and so on. While this is nice and simple, the system seemed unsatisfactory at the end of the last category based on the results . What concerns me is that the results of one competitor can reorder other competitor. To take a simplified example, consider the following situation: Contestant Category A Category B Category C Total Rank Alice 3 2 1 1 (6) Bob 2 1 4 2 (8) Charlie 1 3 3 3 (9) Dan 4 4 2 4 (32) If Dan had won the 3rd category, Alice and Bob's positions would be swapped like so: Contestant Category A Category B Category C Total Rank Bob 2 1 4 1 (8) Charlie 1 3 3 2 (9) Alice 3 2 2 3 (12) Dan 4 4 1 4 (16) Here we see that Dan's result has penalized Alice considerably. This seems to be an unfair way to combine ranks, as the partial order between Alice and Bob depends on the partial order between Dan and Alice. The property the seems to be violated here appears to be very similar to the Independence of Irrelevant Alternatives . What I'm looking for is whether there is a theory for functions that combine independent orderings into a global one, and what properties these rank-combining functions may have. For instance, whether excluding or adding a late contestant can change the relative order of others, whether the winner of all categories necessarily wins the whole event, etc.","['voting-theory', 'combinatorics', 'discrete-mathematics']"
4854351,Continuous functions are weak*-dense in $L^\infty$,"I'm faced with the following situation: let $X$ be a Hausdorff compact space and let $\mu$ be a regular finite Borel measure on $X$ . In classes, we proved that $C(X)$ (meaning all continuous complex functions on $X$ , endowed with the sup norm) are weak*-dense in $L^\infty (X, \mu)$ using Luzin's theorem. However, in the earlier lecture we proved the following lemma: Lemma: Let $X$ be a Banach space. Then the image of the inclusion $$\iota: X \to X^{**},\quad \iota (x) = (f \mapsto f(x))$$ is weak*-dense in $X^{**}$ . My question is: could we somehow avoid using Luzin's theorem and prove that $C(X)$ is weak*-dense in $L^\infty (X, \mu)$ using just the above lemma?","['lp-spaces', 'functional-analysis', 'real-analysis']"
4854385,Why does the Kullback-Leibler divergence use the expected value based on the data/new distribution instead of the reference distribution?,"The Kullback–Leibler (KL) divergence is defined as $$
D_{\text{KL}}(P\parallel Q)=\sum _{x\in {\mathcal {X}}}P(x)\ \log \left({\frac {\ P(x)\ }{Q(x)}}\right) = \mathbb E_P \left[ \log \left({\frac {\ P(x)\ }{Q(x)}}\right) \right]
$$ where Q is the reference distribution ( https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence ) and P is the measurement distribution. In a way Q is little like a $H_0$ -hypothesis in a hypothesis test. Or $D_{\text{KL}}(P\parallel Q)$ is a little bit like the model likelihood in a Bayesian setting with Q being the model and P the data. So far my understanding. I do not understand, why we are using the expected value $\mathbb E_P$ , based on the measurements though if we consider the Q to be the reference or the true distribution and are trying to think how much P differs from the reference distribution. I would have expected $\mathbb E_Q$ . Can someone explain?","['measure-theory', 'probability-theory', 'statistics']"
4854445,Conjecture: Expected total area of a certain set of random triangles in a unit disk is $1/\pi$.,"Choose $3n$ independent uniformly random points in a disk with perimeter $x^2+y^2=1$ . Label the points $P_1,P_2,P_3,\dots,P_{3n}$ in order of increasing $x$ -coordinates. Form triangles $\triangle P_1P_2P_3,\space \triangle P_4P_5P_6,\space \dots,\space \triangle P_{3n-2}P_{3n-1}P_{3n}$ . Here is an example with $3n=21$ . What does the total area of the triangles approach as $n\to\infty$ ? I did a simulation of $100$ trials with $3n=99999$ , and the average total area was $0.318343178$ . This number multiplied by $\pi$ is approximately $1.00010$ , so I conjecture that the answer is $1/\pi$ . My attempt I considered a simpler related question: What is the expected area of a triangle with vertices $(1,a_1),(2,a_2),(3,a_3)$ where the $a$ values are independent uniformly random real numbers in $[0,1]$ ? The probability that $a_2$ is between $a_1$ and $a_3$ is $1/3$ . We have $\mathbb{E}(\max (a_1,a_2,a_3)-\min (a_1,a_2,a_3))=\frac12$ ( explanation ). The left and right sub-triangles share a vertical side, which in this case has expected length $\frac18$ . So the expected area of the triangle is $2\times\frac12\times\frac18\times 1=\frac18$ . The probability that $a_2$ is not between $a_1$ and $a_3$ is $2/3$ . Again we have $\mathbb{E}(\max (a_1,a_2,a_3)-\min (a_1,a_2,a_3))=\frac12$ . The left and right sub-triangles share a vertical side, which in this case has expected length $\frac38$ . So the expected area of the triangle is $2\times\frac12\times\frac38\times 1=\frac38$ . So the overall expected area of the triangle is $\frac13\times\frac18+\frac23\times\frac38=\frac{7}{24}$ . This value is supported by a simulation. In the original question, the triangles are not limited to a rectangular box of height $1$ , but rather are limited to a unit disk, which has an average height of $2\int_0^1\sqrt{1-x^2}\mathrm dx=\frac{\pi}{2}$ . The rectangle and the disk have the same width, but in the disk there are horizontal gaps between neighboring triangles, so we multiply by a factor of $\frac23$ . So the total area of the triangles in the disk should approach approximately $\frac{7}{24}\times\frac{\pi}{2}\times\frac23=\frac{7\pi}{72}\approx 0.30543$ . This is slightly less than $\frac{1}{\pi}\approx 0.31831$ . The difference may be due to the fact that the $x$ -coordinates of the points in the disk do not have equal spacing. EDIT: I made a mistake in my simulation. Using the shoelace theorem , the area of the triangle with vertices $(x_1,y_1),(x_2,y_2),(x_3,y_3)$ is $\frac12|x_1y_2-x_2y_1+x_2y_3-x_3y_2-x_1y_3+x_3y_1|$ , but I was accidentally using $\frac12|x_1y_2-x_2y_1+x_2y_3-x_3y_2-x_1y_3+x_\color{red}{1}y_1|$ . After correcting, my simulations suggest the answer is approximately $0.320$ , which agrees with @Rei Henigman's answer. So I no longer think my conjecture is true, but I would still like to know if the limiting area of the triangles has a closed form. I also did a (correct) simulation using a square lamina instead of a disk, and it seems that the proportion covered by the triangles is about $0.102$ , which seems to match the proportion in the disk. Context This question was inspired by the question ""A disc contains $n$ random points. Each point is connected to its nearest neighbor. What does the average cluster size approach as $n\to\infty$ ?"".","['conjectures', 'circles', 'geometry', 'expected-value', 'triangles']"
4854486,What kind of functions satisfy $y'(x) =x'(y)$,"What kind of functions satisfy the property $y'(x) =x'(y)$ ? I don't know if it makes sense. For example, for $x^2+y^2=1$ , $$\begin{align}y'(x)& = \frac{-x}{\sqrt{1-x^2}}\\
x'(y) & = \frac{-y}{\sqrt{1-y^2}}\end{align}$$ What I am trying to ask is that what kind of other functions are symmetric like this, as it only changes the $x$ and $y$ between $\frac{dx}{dy}$ and $\frac{dy}{dx}$ .","['calculus', 'derivatives']"
4854516,Are $\sigma$-algebras that aren't countably generated always sub-algebras of countably generated $\sigma$-algebras?,"It is well known that there are countably generated sigma-algebras containing sub-sigma-algebras that cannot be countably generated. (Some tail sigma-algebras serve as examples.) Question. Suppose $\mathcal{A}$ is a sigma-algebra of subsets of $X$ that cannot be countably generated. Does there exist a countably generated sigma-algebra $\mathcal{B}$ on $X$ such that  $\mathcal{A}$ is a sub-$\sigma$-algebra of $\mathcal{B}$? I'm inclined to think, but don't know how to prove, that the answer is no. If that's right, under what conditions can the desired $\mathcal{B}$ be found?","['measure-theory', 'set-theory']"
4854537,Dice probabilities and stars/bars,"I'm trying to apply stars and bars to answer a dice question: if $n$ players throw $k$ -sided dice, what is the probability that exactly $i$ players throw a unique number (where $0 \leq i \leq n$ )? For example, if $4$ players throw $4$ -sided dice, and three throw a $3$ , and one throws a $4$ , then $1$ player threw a unique number.  If two throw a $1$ and two throw a $3$ , then $0$ players threw a unique number. This is easy to enumerate and count for small $n$ and $k$ :  for $4$ players throwing $4$ sided dice, then we have $4^4$ (or $k^n$ total outcomes), with the following probabilities: $$ P_0 = \frac{40}{256}, \, P_1 = \frac{48}{256},\, P_2 = \frac{144}{256},\, P_3 = \frac{0}{256},\, P_4 = \frac{24}{256},$$ where $P_i$ means the probability of exactly $i$ players throwing a unique number. I was thinking this could be calculated with a stars-and-bars formulation, where we partition $n$ players into $k$ equally likely bins (each bin is a side of the dice).  So a partition $[0,0,0,4]$ means all $4$ players threw a $4$ , and $[1,1,1,1]$ means each player threw a different number (one player threw a $1$ , the next threw a $2$ , etc). There would be $n \choose k$ or ${4+4-1} \choose {4-1}$ = $7 \choose 3$ = $35$ different partitions of $4$ players into $4$ dice-side bins.  Of those partitions, $10$ represent the $P_0$ case: of the form $[0,0,0,4]$ or $[0,2,0,2]$ , $12$ represent $P_1$ , and look like $[0,0,1,3]$ , $12$ represent $P_2$ and look like variations of $[0,1,1,2]$ , there are zero for $P_3$ , and $1$ representing $P_4$ looking like $[1,1,1,1]$ . So I think the analogous question here is: of the $35$ partitions, how many contain exactly $i$ $1$ 's? I don't see a way of generalizing this into a general expression of $n$ and $k$ .","['dice', 'combinatorics']"
4854592,Solutions to equation $x^{\frac2 7}=16$,"My son had this question on his college algebra homework.
Solve $x^\frac{2}{7}=16$ . The question doesn't specify the domain, but they have learned about complex numbers. It's an online homework assignment, and it tells him if his answer is correct and will let him change to a different answer. The online system would not accept the answer 16384, but would only accept the answer {16384, -16384}. R says $(-16384)^\frac{2}{7}$ is not a number (NaN). Mathematica says $(-16384)^\frac{2}{7}=16 (-1)^\frac{2}{7}$ , which begs the question: What is $(-1)^\frac{2}{7}$ ? I understand that $((-16384)^2)^\frac{1}{7}=16$ , but $((-16384)^\frac{1}{7})^2$ is again not a number according to R and is $16 (-1)^\frac{2}{7}$ according to Mathematica. Finally, using Solve or Reduce in Mathematica, it says there is only one solution $x=16384$ .","['algebra-precalculus', 'roots', 'complex-numbers']"
4854611,Continuous bijection from non-compact set to compact set,"So, I was wondering if one could find a continuous bijection from a non-compact set to a compact set. I had an exam today where one of the questions asked me to find a continuous bijection (if there is one) from $\mathbb{R}$ to $\left[0,1\right]$ which obviously does not exist because $f$ has to be monotonic. That generalization sort of popped up in my head, and at some point I thought I had a proof (I've tried to prove that for every compact $X$ in the codomain, $f^{-1}(X)$ is compact), which proved to be wrong. So the question is, is that fact even true, at least  maybe by adding some more conditions?","['general-topology', 'analysis']"
4854615,Is there a continuous version of the Taylor Series?,"From what I understand, summations are typically discrete and integrals continuous. In fact, it can be argued that a summation is a discrete equivalent of an integral and vice versa. On a seemingly unrelated tangent, the general formula for the Taylor Series is $\sum\limits^{\infty}_{n=0}\frac{f^{(n)}(a)}{n!}(x-a)^n$ . As you can see, this uses a summation (as series usually do). Using fractional calculus, I figured it would be possible to have an integral $\int\limits^{\infty}_{0}\frac{f^{(n)}(a)}{n!}(x-a)^ndn$ . I know this would remove the usefulness of the Taylor series but I am curious as to whether something like this exists.","['integration', 'improper-integrals', 'taylor-expansion']"
4854642,"How to derive from scratch the identities $\sin^2x=\frac{t^2}{1+t^2}$, $\cos^2x=\frac1{1 + t^2}$, $\sin x\cos x=\frac{t}{1+t^2}$, where $t=\tan x$?","I am currently working on evaluating the integral: $$ \int \frac{\sin x \cos x}{1 + \sin^4 x} \, dx. $$ In order to simplify the integrand, I've considered the substitution $ \tan(x) = t $ . In my textbook, I found the following trigonometric identities: $$ \sin^2(x) = \frac{t^2}{1 + t^2}, $$ $$ \cos^2(x) = \frac{1}{1 + t^2}, $$ $$ \sin(x)\cos(x) = \frac{t}{1 + t^2}, $$ where $t=\tan(x)$ . While I can verify these identities by myself, my question pertains to how I would derive these identities without prior knowledge of their right-hand sides. Essentially, given only the idea to express them in terms of $ \tan(x) $ , how would I approach deriving these identities from scratch? I mean in this problem I would need $\sin(x)\cos(x)$ and $\sin^2(x)$ . How do I start just knowing that I have to find them in terms of $\tan(x)$ . Thank you for any insights or explanations you can provide!","['trigonometry', 'trigonometric-integrals']"
4854649,Artin's theorem for the linear representation of finite groups,"I'm studying the book of Serre, Linear Representation of Finite Groups and in the section 9.2 he states a theorem, ""the Artin's theorem"", which is: Let $G$ a finite group, $X$ a family of finite subgroups of $G$ and $$
\operatorname{Ind}: 
\bigoplus_{H \in X} R(H) \to R(G)
$$ for which each component is $\operatorname{Ind}_{H}^{G}: R(H) \to R(G)$ . The following assertions are equivalent: $G$ is the union of all conjugates of subgroups in $X$ ; the cokernel of the morphism $\operatorname{Ind}$ is finite. I understood the proof, but I don't see how it can be useful. Does someone see the utility of this theorem? Thank you.","['representation-theory', 'group-theory', 'abstract-algebra', 'finite-groups']"
4854761,Question regarding supports of random variables and their transformations,"Disclaimer : I preface by saying, I am currently reading ""Hogg, McKean, and Craig, introduction to mathematical statistics"", hence, kindly forgive me if I get some definition/concept wrong here, as all of these are stitched together by me from various random sources. The book does not talk about any of this. Some preliminary definitions : Let $X$ be a random variable $X: \Omega \to \mathbb{R}$ , and let $P_X$ be a probability $P_X:\mathcal{B_0} \to [0,1]$ , where $\mathcal{B_0}$ is the Borel Sigma algebra, defined on $X$ . Here ( $\Omega$ , $\Sigma(\Omega)$ , $P$ ) form a probability space, and ( $\mathbb{R}$ , $\mathcal{B_0}$ , $P_X$ ), where $P_X$ is the induced probability, also form a probability space. Let $Y=g(x)$ be a measurable function $g:\text{Range(X)}\to \mathbb{R}$ , and let $P_Y:\mathcal{B_0} \to [0,1]$ be ""induced"" by $P_X$ . That is, $P_Y(\text{some set in }\mathcal{B_0}):=P_X(g^{-1}(\text{some set in }\mathcal{B_0}))$ . The associated sigma algebra for $\text{range}(X)$ I shall take to be $\mathcal{B_0}$ . I hope till here, things are properly defined (?). Now I define my ""support"" $R_X$ of the random variable $X:\Omega \to \mathbb{R}$ to be the smallest closed subset of $\mathbb{R}$ such that $P_X(R_X)=1$ . Or equivalently, $R_X:=\{x\in\mathbb{R} :\forall \ \varepsilon>0, P_X(V_{\varepsilon}(x))>0 \}$ . In a similar vein, for $Y$ , $R_Y$ would be the smallest closed subset of $\mathbb{R}$ such that $P_Y(R_Y)=P_X(g^{-1}(R_Y))=1$ , or again, equivalently, $R_Y=\{y\in\mathbb{R} :\forall \ \delta>0, P_Y(V_{\delta}(y))=P_X(g^{-1}(V_{\delta}(y)))>0 \}$ . Note : When the probability density function $\text{pdf}_X(x)$ exists, another equivalent definition would be $R_X:=\text{closure}(\{x\in\mathbb{R}:\text{pdf}_X(x)>0 \})$ My question is should it be that $R_X \subseteq g^{-1}(R_Y)$ ? I was able to show this fact for the case of finite ranged Random variables, but not the general case, based on the definitions above. If I further assume that the probability density function exists (both for $X$ and $Y$ ), and moreover, $y=g(x)$ is one-one and continuous (hence, monotone) on the support $R_X$ of $X$ , then I can quite easily show that $R_X = g^{-1}(R_Y)$ . But in any other case, I wasn't able to show my claim. Kindly provide some insights on the range of applicability of the above assertion, and (if it is true at all in the general case) provide a proof (or a counter example?) for the general case. I would also be satisfied with a proof (or a disproof, or a counter example) in the case where the probability density function exists for both $X$ and $Y$ (referred to as ""absolutely continuous"" in my book)","['measure-theory', 'probability-distributions', 'probability-theory', 'real-analysis']"
4854767,Analytic continuation of function given by Moser-de Bruijn sequence,"I was wondering about the function $$F(x) = \prod_{n=0}^{\infty}{(1+x^{4^{n}})} = 1+x+x^4+x^5+x^{16}+x^{17}+...$$ where the exponents in the resulting power series are given by the Moser-de Bruijn sequence . Wikipedia says that it has the functional equations $$F(x)F(x^2)=\frac{1}{1-x}$$ and $$F(x)=(1+x)F(x^4).$$ My question is: Is there an analytic continuation of this function outside of its radius of convergence? Is it possible to represent this series using a definite integral? What other identities does this function have? The product represention suggests that the function should have singularities at every $4^n$ th root of $1$ , since all terms in the product after a certain point would equal $2$ . Also, the Hadamard gap theorem says that if the asymptomatic ratio between non-zero exponents is greater than 1, the function can't be analytically continued, while the Moser-de Bruijn sequence is proportional to the square numbers.","['complex-analysis', 'functions', 'analytic-continuation']"
4854801,Why did abstract group theory take its current form?,"These days, I'm interested in group theory. Why is the group axiom the way it is now?  As an example, among mathematics, algebra has fundamental properties called associative property, commutative property, and distributive property.  However, among these, only the associative property is included in the group axioms.  And among the above properties, when the commutative property is established, it is called a special term, Abelian group.  And there are certainly many good properties in the Abelian group.  So I want to know why the group axiom ended up being the way it is now. And I understand that the group theory comes from linear equations.  So, in my personal opinion, I think that the generalization of possible solutions in linear equations is reflected in the group axiom, but I am asking this question because I want to know for sure.  And as an additional question, what was the reason in the history of mathematics that led to thinking of groups based on linear equations?","['group-theory', 'abstract-algebra', 'math-history']"
4854808,Why the symmetric property doesn't fit to the question: How many even numbers with $9 $ different digits (without zero) the digit $1 $ is before $2 $?,"The question contains $\ 2$ sections : How many numbers with $\ 9 $ different digits (not include zero) the digit $1$ is before $2$ ? How many $even$ numbers with $9$ different digits (not include zero) the digit $1$ is before $2$ ? I've calculated the first section and made sure that my answer is correct $36 * 7!$ I thought that because we know that the right-most digit cannot be $1$ , it can only be $2-9$ , so we have the same number of right-most digits for $even$ numbers $(2,4,6,8)$ and for $odd$ numbers $(3,5,7,9)$ .
Hence, we can use the symmetric property so the answer is $18 * 7! $ but, it seems incorrect, where is my mistake ?","['combinatorics', 'discrete-mathematics']"
4854917,Can I decompose a set of positive Lebesgue measure into two subsets which have NO positive measure subsets themselves?,"Stated in a non-oneliner way: Let $X\subset\mathbb{R}$ be Lebesgue-measurable with positive measure. Is it possible to find sets $A, B$ with the following properties? $A\cap B = \emptyset$ $A\cup B = X$ all measurable subsets of $A$ are null sets all measurable subsets of $B$ are null sets These properties of course imply that if such a decomposition exists, $A$ and $B$ must be non-measurable. Continuing my descent into madness measure theory, my current research topic somehow hinges on the above question. My intuition tells me, and I sincerely hope, that such a decomposition is impossible, but I have learnt to be quite afraid of non-measurable sets.","['measure-theory', 'lebesgue-measure', 'outer-measure']"
4854940,Well-ordered subsets of directed sets [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 months ago . The community reviewed whether to reopen this question 4 months ago and left it closed: Original close reason(s) were not resolved Improve this question In general, can we always find a well-ordered cofinal subset of any given directed set? In this case, is there any specific reference?","['elementary-set-theory', 'logic', 'set-theory']"
4854944,Intersection of parabola and line,"Suppose that a parabola and line are given by $$y = 2x-k, y = x^2-(k+2)x+2k$$ and if one of the points at which they intersect is on $x$ -axis, how can we find the ordinate of the other intersection point? I approached the problem as follows: $$x^2-(k+2)x+2k = 2x-k\iff x^2-(k+4)x+3k = 0$$ Whose roots are given by $$x_{\pm} = \frac{(k+4)\pm \sqrt{(k+4)^2-12k}}{2}$$ If one of the intersection points lies on $x$ -axis, there are two possibilities: $2x_{+}-k = 0$ or $2x_{-}-k = 0$ . For the former condition, $$4 + \sqrt{(k+4)^2-12k} = 0\implies 4 = -\underbrace{\sqrt{(k+4)^2-12k}}_{\geq 0}$$ Which is false, so $2x_{-}-k = 0$ $$4 = \sqrt{(k+4)^2-12k}$$ And which gives us $k = 0$ or $k = 4$ . But we also have that $$x^2_{-}-(k+2)x_{-}+2k = 0$$","['functions', 'quadratics']"
4855014,How to solve $\int \frac{1}{x^2\sqrt{x^2-1}}dx $,"I have a problem with the following integral: $$\int\frac{1}{x^2\sqrt{x^2-1}}\,\mathrm dx$$ My work: Let $x=\sec (t)$ and $\mathrm dx=\sec(t)\tan(t)\mathrm dt$ $$\int \frac{\sec t\tan t}{\sec^2t\sqrt{\tan^2t}}\,\mathrm dt=\int \cos t\,\mathrm dt=\sin t +c$$ Actually I should write $\int sgn(t)\cos(t)\,dt$ but given that $t=\sec^{-1}x$ is always positive i can omit it (Am i right?) . Now I have $$\sin t+c = \sin (\sec^{-1}x)+c = \frac{\sqrt{x^2-1}}{|{x}|}+c$$ but the solution in my book is $$\frac{\sqrt{x^2-1}}{x}+C$$ Where am i wrong? A similar exercise is the following: $$\int\frac{1}{x^3\sqrt{x^2-1}}\,\mathrm dx$$ where using the same substitution i have to solve: $$\int \cos^2t\,\mathrm dt=\frac12\sec^{-1}x+\frac14\sin(2\sec^{-1}x)=\frac12\sec^{-1}x+\frac12\sin(\sec^{-1}x)\cos(\sec^{-1}x)$$ where the final form is: $$\frac12\sec^{-1}x+\frac12\frac{\sqrt{x^2-1}}{|{x}|x}+c$$ instead in my book the solution is $$\frac12\sec^{-1}x+\frac12\frac{\sqrt{x^2-1}}{x^2}+c$$ Thanks to all.","['integration', 'definite-integrals', 'calculus', 'trigonometric-integrals', 'trigonometry']"
4855068,Evaluating $\lim_{n\to \infty} n \int_{0}^{1} \frac{x^n}{x+1} dx$,"Evaluating $$L=\lim_{n\to \infty} n\int_{0}^{1} \frac{x^n}{x+1} dx.$$ Let $$I_n=\int_{0}^{1}\frac{x^n}{x+1} dx=\int_{0}^{1} x^{n-1}dx-I_{n-1}$$ When $n$ infinitely large $I_{n-1}\sim I_{n}$ , then $$I_n\sim \frac{1}{2n} \implies L= \frac{1}{2}.$$ The question is how else one can evaluate $L$ ?","['integration', 'limits', 'definite-integrals']"
4855076,Geometric Pattern in Triangle Construction with Squares,"I've been exploring some constructions in geometry recently and have been looking at the following algorithm: Start with an arbitrary triangle (red) Construct 3 squares from each of the sides of the triangle where the squares do not intersect Connect the centers of each square to form a new triangle (green) Repeat ad infinitum It appears that this process will converge to an equilateral triangle after infinite repetitions. I'm curious as to why that is. I started brute forcing the calculation algebraically with arbitrary coordinates for the vertices of the original triangle, but it gets messy quickly. I'm wondering if there's a more elegant explanation.","['geometry', 'geometric-construction']"
4855102,Why is the closure of X equal to X for topological spaces,"In the book ""Introduction to Topology"" by Bert Mendelson, ch3, theorem 4.7 states that for any Topological Space (X, T) $\bar{X} = X$ . Now I understand that any subset of $X$ is also a subset of $\bar{X}$ , but don't see why $X = \bar{X}$ . An answer with examples would be appreciated",['general-topology']
4855149,Stopping time and supremum of Brownian motion,"We consider on a filtered probability space $(\Omega,\mathcal{F},\mathcal{F}_{r \in \mathbb{R}_+},P)$ with the usual conditions. Let $B$ be a $(\mathcal{F}_r)_{r \geq 0}$ -Brownian motion. Let $\theta:=\inf\{r \geq 0,r\sup_{u \in [0,r]}|B_u|>1\}.$ Can we claim that $\theta$ is a stopping time? Attempt: it's sufficient consider for $v >0,\{\theta \geq v\} \in \mathcal{F}_v.$ We have $\{\theta \geq v\}=\bigcap_{r \in [0,v[}\{r \sup_{u \in [0,r]}|B_u| \leq 1\}$ where we obtained an uncountable intersection. What do you think?","['stochastic-processes', 'stopping-times', 'brownian-motion', 'probability-theory']"
4855172,"Using open subsets, why is 1/x not a continuous function on $\mathbb{R}$","Looking at this theorem of continuous function by Bert Mendelson in ""Introduction to Topology"" ch 3, theorem 5.3: A function $f: (X, T) \rightarrow (Y, T')$ is continuous if and only if for each open subset $O$ of $Y$ , $f^{-1}(O)$ is an open subset of $X$ . This follows from the definition of continuous functions which is continuous at each point, after he defined continuous at a point. However, if I just focus on the theorem: Now let's take as an example the function $f(x) = 1/x$ , and an open subset of $Y$ which I choose to be $(-1, 1)$ , the preimage of this subset is $(-\infty, -1) \cup (1, \infty)$ . Now, as far as I'm concerned this is an open subset of $\mathbb{R}$ . Then for it not to be continuous I should be able to find an open subset of $Y$ such that the preimage is not an open subset of $X$ . However, I cannot find such a subset. I would like to understand why $1/x$ is not a continuous function on $\mathbb{R}$ using open subsets (not an epsilon and delta answer)","['continuity', 'general-topology']"
4855191,liminf estimate of a integral of weak convergence and almost everywhere convergence,"I am considering a integral of product of three functions i.e. $$\int_{\Omega}f_{\epsilon}^{2}g_{\epsilon}dx$$ where $f_{\epsilon}\to f$ weakly in $L^{2}(\Omega)$ and $g_{\epsilon}\to g$ almost everywhere  and $0\leq g_{\epsilon}\leq C$ for some $C>0$ and all $\epsilon$ and $\Omega$ is a bounded domain. Moreover, I also know that this integral is uniformly bounded that is $$\int_{\Omega}f_{\epsilon}^{2}g_{\epsilon}dx\leq C$$ for some $C>0$ . What am I expecting is whether can I have some estimate for the limit integral i.e. if I can have $$\int_{\Omega}f^{2}g dx\leq \liminf_{\epsilon} \int_{\Omega}f_{\epsilon}^{2}g_{\epsilon}dx$$ Many thanks for any help! My attempt is as following: since $g_{\epsilon}$ is uniformly postive, so $$\int_{\Omega}f_{\epsilon}^{2}g_{\epsilon}dx$$ can be viewed as the $L^{1}$ norm of $f_{\epsilon}^{2}g_{\epsilon}$ , then by the weak lower semicontinuous of norm, I can get my estimate. Does this make sense?
My another try according to @daw's reply. \begin{align*}
\int_{\Omega}f_{\epsilon}^{2}g_{\epsilon}dx=\int_{\Omega}f_{\epsilon}^{2}gdx+\int_{\Omega}f_{\epsilon}^{2}(g_{\epsilon}-g)dx.
\end{align*} First $$\int_{\Omega}f^{2}gdx\leq \liminf \int_{\Omega}f_{\epsilon}^{2}gdx$$ by weakly lower semiconitnuous. Then by Egorov's theorem, for all $\eta>0$ there exists $|\Omega_{\eta}|<\eta$ such that $g_{\epsilon}\to g$ uniformly on $\Omega-\Omega_{\eta}$ . Then \begin{align*}
|\int_{\Omega}f_{\epsilon}^{2}(g_{\epsilon}-g)dx|&\leq|\int_{\Omega_{\eta}}f_{\epsilon}^{2}(g_{\epsilon}-g)dx|+|\int_{\Omega-\Omega_{\eta}}f_{\epsilon}^{2}(g_{\epsilon}-g)dx|.
\end{align*} Moreover, the integrablity of $f_{\epsilon}$ and $g_{\epsilon}$ implies the first term goes to zero as $\eta$ goes to zero. For the second term \begin{align*}
|\int_{\Omega-\Omega_{\eta}}f_{\epsilon}^{2}(g_{\epsilon}-g)dx|&\leq \lVert f_{\epsilon}\rVert_{L^{2}(\Omega)}^{2}\lVert g_{\epsilon}-g\rVert_{L^{\infty}(\Omega-\Omega_{\eta})}\\
&\leq C\lVert g_{\epsilon}-g\rVert_{L^{\infty}(\Omega-\Omega_{\eta})}\to0.
\end{align*} So we are done. Does this make more sense? Okay, maybe this estimate is not something I can expect? But what about if I assume that $g_{\epsilon}$ is strictly positive i.e. $0<C_{1}\leq g_{\epsilon}\leq C_{2}$ . From this, I think the bounded implies that $$\lVert \sqrt{g_{\varepsilon}}f_{\varepsilon}\rVert_{2}\leq C$$ Then by the weak lower semicontinous and a.e. convergence, I can get the inequality I want. For this, i think we first take arbitrary $\phi\in L^{2}(\Omega)$ . Then \begin{align*}
&|\int_{\Omega}(\sqrt{g_{\epsilon}}f_{\epsilon}-\sqrt{g}f)\phi dx|\\
\leq&|\int_{\Omega}\sqrt{g}(f_{\epsilon}-f)\phi dx|+|\int_{\Omega}(\sqrt{g_{\epsilon}}-\sqrt{g})f_{\epsilon}\phi dx|
\end{align*} The first term goes to zero by weak convergence and the second due to dominated convergence theorem. Does this make sense?","['functional-analysis', 'analysis', 'real-analysis']"
4855209,"Why $(\mathfrak g_{\mathbb C},K)$-modules?","Let $G$ be a reductive group over $\mathbb Q$ , $\mathfrak g=Lie(G)$ be its Lie algebra, $\mathfrak g\otimes_{\mathbb R} \mathbb C$ be its complexification. A definition of automorphic representation on $G$ is that it is an irreducible admissible $(\mathfrak g_{\mathbb C},K)\times G(\mathbb A^{\infty})$ module that is isomorphic to a subquotient of the space of automorphic forms on $G$ . (e.g. this definition is used in page 146 of Getz and Hahn's book ) Why do we consider $(\mathfrak g_{\mathbb C},K)\times G(\mathbb A^{\infty})$ -modules, rather than $(\mathfrak g,K)\times G(\mathbb A^{\infty})$ -modules?","['lie-algebras', 'number-theory', 'automorphic-forms', 'representation-theory', 'langlands-program']"
4855296,Can the sum of $2025$ consecutive factorials be a perfect power?,"Can the sum of $2025$ consecutive factorials be a perfect power? My thoughts: If the sum of $2025$ consecutive factorials is a perfect power, then the number of trailing zeroes in the sum must be a perfect (number of trailing zeroes)th power. So this means that if the sum of $2025$ consecutive factorials is a perfect power, then the sum must be a perfect (number of trailing zeores)th residue $\pmod{p}$ , where $p$ is a certain prime number. For example, the sum of $10!+11!+12!+\cdots+2034!$ contains $2$ trailing zeroes as all factorials from $10!$ to $14!$ contains $2$ trailing zeroes, and since the sum contains $2$ trailing zeroes, then it must be a perfect square in order to be a perfect power. By using Pari GP, I tried to check all factorials from $1!$ to $10^{6}!$ , but I did not succeed.","['number-theory', 'perfect-powers']"
4855393,What is statistically more accurate - the average of a dataset of calculated concentrations or the sum of the total mass divided by the total volume? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question Hopefully an easy question to answer. But if I have a series of five water samples, each sample is a different volume with a different number of plastic particles contained within. I can calculate the concentration of each sample by dividing the mass of particles by the volume of the sample (e.g. 500 mg of particles and 1 litre of water = 500 mg/L). If I want to understand the true average of the dataset, would it be more appropriate to take the average of the concentrations of all the samples OR take the total mass of all the samples combined and divide that by the total volume to get an average concentration? See below for example: Total Sample volume (L) Sample mass (mg) Sample concentration (mg/L) 4 253 63.3 6 439 73.2 5 205 41.0 9 226 25.1 4 30 7.5 Total 28 1153 Would the correct average in this case be 42.0 mg/L (the average of the calculated concentrations) OR the calculated average that divides the total sum of the sample mass (i.e. 1153 mg) by the total sample volume (28 L), i.e. 41.2 mg/L? I know in this case the discrepancy between the values is small, however if you expand the dataset this discrepancy will increase. I feel the answer to this question is a simple mathematical response, but I just can't seem to come to it myself. I appreciate the help!","['statistics', 'means', 'arithmetic', 'average', 'algebra-precalculus']"
4855452,"If the roots of $z^4+az^3+bz^2+z$ are distinct and concyclic in the complex plane, does $ab\in\mathbb R$ imply $1<ab<9$?","HMMT February 2022, Team Round, Problem 6 (proposed by Akash Das ) is: Let $\operatorname{\it P\!}{\left(x\right)}=x^4+ax^3+bx^2+x$ be a polynomial with four distinct roots that lie on a circle in the complex plane. Prove that $ab\neq9$ . Its solution may be found here . Recently, certain netizens claim ed that there exists a stronger result when $ab$ is real: Let $P(z)=z^4+az^3+bz^2+z$ be a polynomial with four distinct roots that are concyclic in the complex plane; then $ab\in\mathbb R\implies1<ab<9$ . Nevertheless, I could find neither any proof nor any counterexample of it. Does the above proposition hold?  (And what if $ab\in{\mathbb C\setminus\mathbb R}$ ?) Edit. It appears that when $ab\in{\mathbb C\setminus\mathbb R}$ , $ab$ can take any values in the complex plane other than the real axis. Here is a simulation:","['contest-math', 'complex-analysis', 'polynomials', 'plane-geometry', 'inequality']"
