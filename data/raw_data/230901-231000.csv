question_id,title,body,tags
4796242,How fast does the coprime probability converge to $6/\pi^2$?,"It is known that the probability that two positive integers are coprime is $6/\pi^2$ . This is an amazing result. I wanted to see experimentally how the probability converges to $6/\pi^2$ , but I found that the sequence converges terribly slow, and I would not believe that it converges unless I didn't know a proof that it actually does. Let $a(n)$ be the number of pairs $1 \leq a,b \leq n$ with $\mathrm{gcd}(a,b)=1$ . The ""probability"" in question is defined as a natural density here by $$\lim_{n\to \infty} a(n)/n^2.$$ It evaluates to $6/\pi^2$ . For computations it should be useful to have the formula $$a(n) = 1 + 2 \sum_{a=2}^{n} \varphi(a),$$ where $\varphi$ is the Euler totient function . I wrote a little program that computes the values of $a(n)/n^2$ , or actually $n \sqrt{6/a(n)}$ since that value should converge to $\pi$ , and I would like to see the digits of $\pi$ coming out. Here is a typical excerpt from the sequence (for $10000 \leq n \leq 10005$ ): 3.141534239016629
3.141342469859083
3.14148445699957
3.14135604503421
3.1414222455373713
3.14148184775969 The sequence ""wiggles"" and takes ""forever"" to get close to 3.14159... . Why is that? More precisely: What is the ""convergence speed"" of that sequence? This would be formalized with the order of convergence . If $d \geq 1$ , is there some $n_0$ such that for all $n \geq n_0$ the number $n \sqrt{6/a(n)}$ has the first $d$ digits of $\pi$ ? For example, is this true for $d=4$ , so that we have 3.1415... from some point on? Numerical experiments suggest that this indeed the case, but $n_0$ is very large. A bit more broader: can we adjust the sequence slightly, or at least find a different sequence related to the coprimality of integers, that converges to $\pi$ more fast?","['number-theory', 'analytic-number-theory', 'computational-mathematics', 'totient-function', 'limits']"
4796244,Is it possible to find this limit without using L'Hopital?,"Is it possible to find this limit without using L'Hopital's Rule? $$\lim_{x \to \infty}\frac{\sin(\frac{2}{x})+\frac{2}{x}}{\sin(\frac{1}{x})}$$ This is a problem that I came across in a Calc 1 HW. I consider myself to be really good at Calc 1, yet I am unsure of the solution. Hence, I find the problem to be relevant to the community. Here is my best try so far: Use the trig double angle formula to change it to: $$\lim_{x \to \infty}\frac{2\sin(\frac{1}{x})\cos(\frac{1}{x})+2(\frac{1}{x})}{\sin(\frac{1}{x})}$$ Let $u=\frac{1}{x}$ and separate into two fractions: $$\lim_{u \to 0^+}\left[\frac{2\sin(u)\cos(u)}{\sin(u)}+2\frac{u}{\sin(u)}\right]$$ Cancel the $\sin$ terms in the first fraction and use the known limit of $\lim_{u \to 0}\frac{u}{\sin(u)}=1$ on the second fraction: $$2\cdot1+2\cdot1=4$$ The problem for me is - doesn't the proof of $\lim_{u \to 0}\frac{u}{\sin(u)}=1$ use L'Hopital? So does my solution really count?","['limits', 'calculus', 'limits-without-lhopital', 'trigonometry']"
4796258,Finding infinitely many $\mathbb{Q}$-linearly independent polynomial preimages of the integers.,"It is well-known that the set $\{\sqrt{s}:s\in\mathbb{N}\text{ square-free}\}$ is $\mathbb{Q}$ -linearly independent. This implies that, if $f(x)=x^2\in\mathbb{Z}[x]$ , then the set of algebraic integers defined by $$f^{-1}[\mathbb{Z}]=\{z\in\mathbb{C}:f(z)\in\mathbb{\mathbb{Z}}\}\subseteq\overline{\mathbb{Z}}$$ contains infinitely many $\mathbb{Q}$ -linearly independent numbers. So I was wondering if: Q: Is it true that $\forall f\in\mathbb{Z}[x]$ monic with $\text{deg}(f)\geq2$ the set $f^{-1}[\mathbb{Z}]\subseteq\overline{\mathbb{Z}}$ contains infinitely many $\mathbb{Q}$ -linearly independent algebraic integers? If so, how can one prove it? Feel free to use as much Galois theory as needed (if needed) or any algebraic number theory apparatus. Also, I guess that the monic condition can be omitted but then we would have to talk about algebraic numbers $\overline{\mathbb{Q}}$ instead. Update: Sungjin Kim has proven this for $f\in\mathbb{Z}[x]$ with $\text{deg}(f)\geq2$ such that $\exists g\in\mathbb{Z}[x]$ with $\text{deg}(g)\geq1$ and $g(x)^2\ \vert\ f(x)-f(0)$ . Since $f$ satisfying the condition of Q implies that $\forall a,b\in\mathbb{Z}:f(x+a)+b$ also satisfies the condition, the previous observation can be generalized for all $f\in\mathbb{Z}[x]$ with $\text{deg}(f)\geq2$ such that $$\exists a\in\mathbb{Z}, g\in\mathbb{Z}[x]\text{ with deg}(g)\geq1:g(x)^2\ \vert\ f(x+a)-f(a)$$ This includes the case where $\exists a\in\mathbb{Z}:f'(a)=0$ as then $x^2\ \vert\ f(x+a)-f(a)$ . However, not every integral polynomial of degree $\geq2$ is included here. For example, the polynomial $f(x)=x^2+x$ is such that $\forall a\in\mathbb{Z}:f(x+a)-f(a)=x^2+(2a+1)x$ is always square-free. So how could we include such polynomials? Or is there a counterexample?","['algebraic-number-theory', 'number-theory', 'field-theory', 'galois-theory', 'polynomials']"
4796311,"Suppose $f,g$ are polynomials with degree $\leq d$. If there exist $C$ and $c$ such that $|f-g|\leq C|x|^{d+1}$ for any $|x|<c$. Do we have $f=g$?","I guess the answer is yes, and here's my sketch of proof, but I'm not sure my argument is correct or not. (Since I didn't use any information about $c$ .) proof: Let $f=\sum_{i=0}^{d} a_{i}x^{i}$ and $g=\sum_{i=0}^{d} b_{i}x^{i}$ . Suppose that $f\neq g$ , i.e., there is some $a_{i}\neq b_{i}$ . By assumption, we have $$C\geq \dfrac{|f(x)-g(x)|}{|x|^{d+1}}\geq\sum_{i=0}^{d}|a_{i}-b_{i}|\dfrac{1}{|x|^{d-i+1}}\geq \dfrac{|a_{i}-b_{i}|}{|x|^{d-i+1}}$$ However, as $x\to 0$ , RHS $\to\infty$ and LHS $=C$ , which contradicts. Therefore $a_{i}=b_{i}$ for all $i$ , and so $f=g$ .","['polynomials', 'analysis', 'real-analysis']"
4796316,Integrate $\int\limits_0^\infty\frac{x^{2n-2}}{(x^4-x^2+1)^n}dx$,"Complete question: If $I_n$ = $\int\limits_0^\infty\frac{x^{2n-2}}{(x^4-x^2+1)^n}dx$ and $I_5$ = $\frac{p\pi}{q}$ where p and q are coprime natural numbers, then the value of 8p-q is what? I have no clue how to go about this question. But I am thinking maybe $I_1$ , $I_2$ , $I_3$ ...  are a part of some sequence and evauating a simpler integral will lead us to $I_5$ without actually solving for it. I got this idea since solving $I_1$ and $I_2$ is quite easy. My level- High School.","['integration', 'calculus', 'definite-integrals', 'real-analysis']"
4796319,How to evaluate $ \sum\limits_{k=0} ^{\infty} \frac{(-1)^k}{4k+3}$?,I was trying to solve the integral $\int_0 ^{\frac{\pi}{4}} \sqrt{\tan{x}}dx$ and I noticed I can do the following: $$\int_0 ^{\frac{\pi}{4}} \sqrt{\tan{x}}dx=\int_0 ^{\frac{\pi}{4}} \sqrt{\tan{x}} \sec^2(x) -\int_0 ^{\frac{\pi}{4}} \sqrt{\tan{x}} \tan^2(x) dx $$ $$=\int_0 ^{\frac{\pi}{4}} \sqrt{\tan{x}} \sec^2(x) -\int_0 ^{\frac{\pi}{4}} \tan^{2+\frac{1}{2}}(x) \sec^2(x)dx + \int_0 ^{\frac{\pi}{4}} \tan^{4+\frac{1}{2}}(x) $$ continue with that and we will get $$\int_0 ^{\frac{\pi}{4}} \sqrt{\tan{x}}dx=\sum_{k=0}^n \left(\int_0 ^{\frac{\pi}{4}} \tan^{2k+\frac{1}{2}}(x) \sec^2(x)dx \right) + (-1)^{n+1}\int_0 ^{\frac{\pi}{4}} \tan^{2n+2+\frac{1}{2}}(x) dx$$ since for any converging positive sequence $a_n$ $(\sum a_k)^n >\sum (a_k ^n)$ $\int_0 ^{\frac{\pi}{4}} \tan^{n}(x) dx <\left( \int_0 ^{\frac{\pi}{4}} \tan(x) dx \right) ^n \to 0$ so we get $$ \int_0 ^{\frac{\pi}{4}} \sqrt{\tan{x}}dx=2 \ \lim_{n \to \infty } \sum_{k=0} ^ n \frac{(-1)^k}{4k+3}=\frac{\pi +\ln{(3-2 \sqrt{2})}}{2 \sqrt{2}} $$ I don't know if my approach is correct or not but even if it is correct I am interested in finding out if there are any other more obvious ways to evaluate $ \sum\limits_{k=0} ^{\infty} \frac{(-1)^k}{4k+3}$ as my approach is very difficult to notice if try to solve $ \sum\limits_{k=0} ^{\infty} \frac{(-1)^k}{4k+3}$ without any mention of the integral. another question can we generalise this result  for all $m \in \mathbb{R}$ st $m>1$ $$ \int_0 ^{\frac{\pi}{4}} \left(\tan{x} \right)^{\frac{1}{m}} dx=m \  \sum_{k=0} ^ {\infty } \frac{(-1)^k}{2mk+m+1} $$,"['summation', 'definite-integrals', 'calculus', 'sequences-and-series', 'limits']"
4796365,solve $xyy''+yy'-(x)^2(y)'^3=0$ by using reduction of order method,I tried to set $$y'= p$$ $$y''=p.p'$$ $$xypp'+ yp-x^{2}p^3=0$$ $$\frac{xyp'}{p^2} +\frac{y}{p^2}=x^2$$ $$u=\frac{y}{p}  \rightarrow u'= 1-\frac{yp'}{p^2}$$ $$ \rightarrow x(1-u) + \frac{u}{p} =x ^2$$ I'm stuck in here and don't know what to do next. Am I wrong somewhere ? Can I get a hint or a solution for this problem? Thank you all!,"['calculus', 'ordinary-differential-equations']"
4796402,Where did the $i$ go in the second order differential equation solution?,"Let $a\ddot{y}+b\dot{y}+cy=0$ such that $b^2-4ac<0$ , we then have a solution of the form $y=e^{\alpha t}(A\cos(\beta t)+B\sin(\beta t))$ Where $\alpha$ is the real part, and $\beta$ the absolute value of the imaginary part of the solutions of the following polynomial equation: $ax^2+bx+c=0$ However, when I try to prove this general solution, I find $y=Ae^{(\alpha+i\beta)t}+Be^{(\alpha-i\beta)t}$ $y=e^{\alpha t}(Ae^{i\beta t}+Be^{-i\beta t})$ $y=e^{\alpha t}(A(\cos(\beta t)+i\sin(\beta t))+B(\cos(-\beta t)+i\sin(-\beta t)))$ $y=e^{\alpha t}((A+B)\cos(\beta t)+i(A-B)\sin(\beta t))$ Could anyone please explain to me how we get rid of the $i$ to find the general solution?","['complex-numbers', 'ordinary-differential-equations']"
4796462,Left hand limit of right hand limit?,"I need help figuring this out Suppose we have an increasing function $f : \mathbb{R} \rightarrow \mathbb{R}$ and we define a function $g: \mathbb{R} \rightarrow \mathbb{R}$ in the following way: $$ g(x) := \lim_{t \to x^{+}}f(t).$$ We know that since $f$ is monotone, then the right hand limit exists and the function $g$ is well defined for all $x \in \mathbb{R}$ . My question is: what can we say about $\lim_{t \to x^{-}}g(t)$ and is it true that it is equal to $\lim_{t \to x^{-}}f(t)$ ? I proved that the function $g$ is right-continuous, obtaining $$\lim_{t \to x^{+}}g(t) = g(x) = \lim_{t \to x^{+}}f(t)$$ but I’m having a hard time proving what happens for the left hand limits.
Intuitively, I’d say that the left hand limits are also equal. One can easily prove that $g(x) = f(x)$ for all $x \in \mathbb{R}$ such that $f$ is continuous at $x$ .
Thus, the only points where $f$ and $g$ may differ are the discontinuity points of $f$ and, in particular, those for which $f(x) = lim_{t \to x^{-}}f(t)$ . Still, I don’t know if this is true and how to prove it rigorously. Any help would be appreciated.","['limits', 'continuity', 'real-analysis']"
4796463,"Show that the real K-G equation, $(\Box + m^2)\phi=0$ is the EOM for the action $S=\frac12\int d^4x(\partial^\mu{\phi}\partial_\mu{\phi}-m^2\phi^2)$","This question concerns a real scalar field. Show that the real Klein-Gordon equation, $(\Box + m^2)\phi=0$ is the equation of motion, $\delta S[\phi(x)]/\delta\phi(x)=0$ , for the action $$S=\frac12\int d^4x\left(\partial^\mu{\phi}\partial_\mu{\phi}-m^2\phi^2\right)$$ by performing a functional variation. Here $\mu = 0,1,2,3$ , ( $3+1$ space-time dimensions). The Lagrangian density is $$\mathcal{L}=\mathcal{L}\left(\phi,\ \partial^\mu{\phi},\ \partial_\mu{\phi}\right)=\partial^\mu{\phi}\partial_\mu{\phi}-m^2\phi^2\tag{A}$$ Now since the change in functional variation $$\frac{\delta S[\phi(x)]}{\delta\phi(x)}=\lim_{\delta \phi\to 0}\left(\frac{S(\phi + \delta\phi)-S(\phi)}{\delta\phi}\right)=0$$ It follows that $S(\phi + \delta\phi)-S(\phi)$ must be zero for any $\delta\phi$ . The variation in the action, $S$ is therefore $$\delta{S[\phi]}=\frac12\int d^4x\left[\phi+\delta\phi,\ \partial^\mu{\phi}+\delta\left(\partial^\mu{\phi}\right),\ \partial_\mu{\phi}+\delta(\partial_\mu{\phi})\right]$$ and after Taylor expanding to first order in $\delta\phi$ becomes $$\delta{S[\phi(x)]}$$ $$=\int \frac{d^4x}{2} \left[\mathcal{L}\left(\phi, \partial^\mu{\phi}, \partial_\mu{\phi}\right)+\frac{\partial\mathcal{L}}{\partial \phi}\delta\phi+\frac{\partial\mathcal{L}}{\partial\left(\partial^\mu\phi\right)}\delta\left(\partial^\mu\phi\right)+\frac{\partial\mathcal{L}}{\partial\left(\partial_\mu\phi\right)}\delta\left(\partial_\mu\phi\right)-\mathcal{L}\left(\phi, \partial^\mu{\phi}, \partial_\mu{\phi}\right)\right]$$ $$=\frac12\int d^4x\left[-2m^2\phi\delta\phi+\partial_\mu{\phi}\delta\left(\partial^\mu\phi\right)+\partial^\mu{\phi}\delta\left(\partial_\mu\phi\right)\right]\tag{B}$$ This is as far as I can get and matches the first line of the solution. I will typeset this solution in exactly the same way as the author did to illustrate my confusion: Take the action $$S=\frac12\int d^4x\left(\partial^\mu{\phi}\partial_\mu{\phi}-m^2\phi^2\right)$$ Now we vary it; $$\delta{S} = \frac12\int d^4x\left[\left(\delta\partial^\mu\phi\right)\partial_\mu{\phi}+\partial^\mu{\phi}\delta\left(\partial_\mu\phi\right)-2m^2\phi\delta\phi\right]\tag{1}$$ $$= \frac12\int d^4x\left[\color{#085}{\left(\partial^\mu\delta\phi\right)\partial_\mu{\phi}}+\partial^\mu{\phi}\left(\partial_\mu\delta\phi\right)-2m^2\phi\delta\phi\right]\tag{2}$$ $$= \int d^4x\left[\partial^\mu\phi\partial_\mu{\delta\phi}-m^2\phi\delta\phi\right]\tag{3}$$ $$=\int d^4x \left[-\partial_\mu (\partial^\mu\phi)\delta\phi-m^2\phi\delta\phi\right]+\int d^4x \partial_{\mu}\left(\delta\phi\partial^\mu\phi\right)\tag{4}$$ and we drop the last term, the total divergence due to boundary conditions, so, $$\delta{S}=-\int d^4x \left(\partial^2\phi+m^2\phi\right)\delta\phi\tag{5}$$ Was it correct to have 3 arguments for the Lagrangian density in $(\mathrm{A})$ , namely to distinguish between the contravariant and covariant derivatives? What is the justification for commuting the $\delta$ past the derivative in going from $(1)$ to $(2)$ ? How did the author get from $(2)$ to $(3)$ ? It's almost as if the first term in the integrand of $(2)$ (marked green) has been forgotten about. To go from eqn. $(3)$ to $(4)$ I think integration by parts has been used to factor the $\delta\phi$ out, so integrating the first term of $(3)$ by parts gives $$\int\partial^\mu{\phi}\left(\partial_\mu\delta\phi\right)d^4x=\color{blue}{\left[\partial^\mu{\phi}\int\partial_\mu\left(\delta\phi\right) d^4x \right]}-\int\bigg(\partial_\mu(\partial^\mu\phi)\delta\phi\bigg) d^4x$$ This explains the negative sign for the first term in $(4)$ , but why does the boundary term (marked blue) not match the final term of equation $(4)$ ? I don't really understand why the final term of $(4)$ can be dropped, but a more pressing question I have is how $(5)$ was deduced from $(4)$ . Put another way, it was my understanding that the covariant derivative is such that $$\partial_\mu\equiv\frac{\partial}{\partial x^\mu}\tag{C}$$ and the contravariant derivative is defined as $$\partial^\mu\equiv\frac{\partial}{\partial x_\mu}\tag{D}$$ but the way it's written in going from $(4)$ to $(5)$ suggests that $$\partial_\mu(\partial^\mu\phi)\stackrel{\color{red}{\mathrm{?}}}{=}\partial^2\phi$$ But how can this possibly be true? Okay, so the $\mu$ index is summed over in accordance with the Einstein summation convention since it is a repeated (dummy) index, but by virtue of $(\mathrm{C})$ and $(\mathrm{D})$ without the $\mu$ index $\partial^2\phi$ does not tell me which variable the field, $\phi$ is being differentiated with respect to. Update: I've been given a good answer that addresses most of my questions nicely. The only part that still puzzles me is how the author was able to immediately write down eqn. $(1)$ in the solution. Is it blatantly obvious that the functional variation, $\delta{S} = \frac12\int d^4x\left[\left(\delta\partial^\mu\phi\right)\partial_\mu{\phi}+\partial^\mu{\phi}\delta\left(\partial_\mu\phi\right)-2m^2\phi\delta\phi\right]$ ? I had to go through several lines of logical reasoning to justify that equation, including a Taylor expansion. Closing remarks With the bounty time reaching its conclusion and my subsequent comments below one of the answers I know there may not be time to address these questions before the bounty ends. So I'll award the bounty regardless of whether I get a reply to these comments. I would just like to say a massive thanks to all those that took time and effort to write such great answers. I know I haven't been very good at keeping on top of this question, I just wish I had more time.","['proof-explanation', 'tensors', 'functional-analysis', 'quantum-field-theory', 'mathematical-physics']"
4796481,How to evaluate $\underset{n\rightarrow \infty}{\lim}\left(\frac{\sqrt[n+1]{(n+1)!(2n+1)!!}}{n+1}-\frac{\sqrt[n]{n!(2n-1)!!}}{n}\right) $?,"for $n$$\in$$N^+$ ,solve that: $$\underset{n\rightarrow \infty}{\lim}\left( \frac{\sqrt[n+1]{\left( n+1 \right) !\left( 2n+1 \right) !!}}{n+1}-\frac{\sqrt[n]{n!\left( 2n-1 \right) !!}}{n} \right) $$ i notice double factorials so i try to express it in terms of simple factorials and powers of 2,then it's feasible to use Stirling formula $$f\left( n \right):=\frac{\sqrt[n+1]{\left( n+1 \right) !\left( 2n+1 \right) !!}}{n+1}$$ since $\left( 2n+1 \right) !!=\frac{\left( 2n+1 \right) !}{\left( 2n \right) !!}=\frac{\left( 2n+1 \right) !}{2^nn!}$ and $n!\sim \sqrt{2\pi n}\left( \frac{n}{e} \right) ^n$ ,we have $$f\left( n \right)=\frac{2}{e^2}\left[ 2\pi \left( n+1 \right) \left( 2n+1 \right) \left( n+\frac{1}{4n}+1 \right) ^{2n+1} \right] ^{\frac{1}{2n+2}}$$ hence we should find $$\underset{n\rightarrow \infty}{\frac{2}{e^2}\lim}\left( \left[ 2\pi \left( n+1 \right) \left( 2n+1 \right) \left( n+\frac{1}{4n}+1 \right) ^{2n+1} \right] ^{\frac{1}{2n+2}}-\left[ 2\pi n\left( 2n-1 \right) \left( n+\frac{1}{4n-4} \right) ^{2n-1} \right] ^{\frac{1}{2n}} \right)$$ i guess this can be seen as $$\underset{n\rightarrow \infty}{\frac{2}{e^2}\lim}\left( \left( n+\frac{1}{4n}+1 \right) -\left( n+\frac{1}{4n-4} \right) \right) $$ which equals $\frac{2}{e^2}$ .but how to prove it strictly？or is there any approach  better
?","['limits', 'calculus']"
4796514,How many squares can 4 equilateral triangles form?,We draw 4 equilateral triangles on the plane (not necessarily congruent) with no repeating vertices. Let $S$ denote the set of the 12 vertices of the triangles. Suppose $S$ has the property of not having a subset of 3 aligned points. What's the maximum number of squares that can exist with vertices from $S$ ? This photo shows a construction that yields 3 squares. Can anyone do better? Edit: 4 squares are achievable as proved by heropup. What about 5?,['geometry']
4796658,What is a simple way of understanding associated bundles?,"I am currently struggling to understand the definition of an associated bundle. The one I have been given is Let $(P,M,\pi)$ be a $G$ -principal bundle (with right G action $R$ ) over some manifold $M$ . Given another manifold $F$ equipped with a left $G$ action, $R$ . We define the equivalence relation over $P\times F $ as $(p,f)\sim (R(g,p),L(g^{-1},f))$ (where of course we mean by this that there is a $g\in G$ ... etc ) and then the set $P_F=P\times F /\sim$ . Then, $(P_F,M,\pi_F)$ , with $\pi_F$ sending $[p,f] \mapsto \pi(p)$ , is called an associated bundle. I would like someone to help me navigate this definition. I have understood conceptually principal bundles as bundles which can have groups as fibers in a non trivial way, so to speak . I can see how the definition of a principal bundle formalizes this idea. But what is the idea behind an associated bundle? What are we trying to do? I kind of get why this bundle is associated with the principal bundle; after all, we constructed it with the total space, the projection map and the group action of the principal bundle. However, it's this construction that is not clear. I can see why one would consider $P \times F$ ; but why would one mod this set out by that equivalence relation? Why that one specifically? Why are the actions kind of """"""inverses""""""?? I have seen the example of the GL- $\mathbb{R}^n$ associated bundle to the frame bundle, which is isomorphic as a bundle to the tangent bundle; in this example, I can see how somehow these ""actions working in reverse"" and taking the quotient by that relation conspire to give something isomorphic to the tangent bundle. But beyond that I can't see what this construction was designed to achieve.","['principal-bundles', 'lie-groups', 'differential-geometry']"
4796679,Why is this optimization problem difficult?,"A Normal (Gaussian) Probability Distribution Function can be defined as follows: $$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{ -\frac{(x-\mu)^2}{2\sigma^2} }$$ Now, we can create an linear weighted (with weights $\pi_i$ ) sum of these Normal Distributions. This is usually referred to as a Mixture Distribution ( https://en.wikipedia.org/wiki/Mixture_distribution ): \begin{align*}
p(x|\theta) &= \sum_{i=1}^{k} \pi_i \mathcal{N}(x|\mu_i, \sigma_i^2) 
\end{align*} $$\sum_{i=1}^{k} \pi_i = 1$$ Now, suppose we want to estimate all parameters (i.e. $\theta$ = $\pi_i$ , $\mu_i$ , $\sigma_i$ )  of this function (i.e. all $\pi_i$ , $\mu_i$ , $\sigma_i$ ) In Statistics, we usually estimate these parameters using Maximum Likelihood Theory ( https://en.wikipedia.org/wiki/Maximum_likelihood_estimation ): \begin{align*}
L(\theta|X) &= \prod_{j=1}^{n} p(x_j|\theta) = \prod_{j=1}^{n} \sum_{i=1}^{k} \pi_i \mathcal{N}(x_j|\mu_i, \sigma_i^2)
\end{align*} From here, we would set the following derivatives to 0 and solve for these parameters (for all $i$ ): $$\frac{\partial}{\partial \pi_i} \log L(\theta|X) = 0 $$ $$\frac{\partial}{\partial \mu_i} \log L(\theta|X) = 0 $$ $$\frac{\partial}{\partial \sigma_i} \log L(\theta|X) = 0 $$ However, apparently if we try to solve the above optimization problem using standard Maximum Likelihood Estimation (MLE), we run into a problem of ""Identifiability"". Supposedly, one of the main problem that arises when using MLE is that for a given set of $\mu_i$ , $\sigma_i$ - the values of $\pi_i$ can be interchanged with one another and still produce the same value of the objective function being optimized. But I don't really understand why this is a bad thing. This just means that there are many solutions that satisfy this optimization problem: Nothing good, nothing bad - just a fact. There are many (equally) valid solutions to this optimization problem. To fix this supposed problem, the EM algorithm ( https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm ) is used. My Question: I don't understand why this happens. Suppose if I just proceed anyways and use MLE - how is this a undesirable? And how would using EM fix this supposed problem? Could someone please help me understand why MLE is not well suited for the above optimization problem and why the EM algorithm can be beneficial here? If there are multiple values of $\pi_i$ that can satisfy this optimization problem - won't the EM algorithm run into the same problem? Thanks! Note: I have heard some people say that the EM algorithm is sometimes more computationally efficient/feasible  than MLE - but this is not the point I am interested in. I also heard that the function being optimized  (mixture distribution) is multi-modal and is a difficult function to optimize through MLE - but I am not sure how exactly the EM algorithm has an advantage over MLE in this regard. I heard that the main advantage of the EM algorithm is that it can remedy fundamental mathematical problems (i.e. identifiability) that MLE runs into - but I can't understand what these problems are to begin with and why EM can be useful compared to MLE in this regard. How does the EM algorithm handle identifiability problems better than MLE? References: https://stephens999.github.io/fiveMinuteStats/intro_to_em.html https://link.springer.com/article/10.1007/s42952-022-00180-6 https://stats.stackexchange.com/questions/262538/why-expectation-maximization-is-important-for-mixture-models https://people.csail.mit.edu/rameshvs/content/gmm-em.pdf https://ardianumam.wordpress.com/2017/11/07/how-em-expectation-maximization-method-works-for-clustering/ https://www.youtube.com/watch?v=f2HIW37Ohho","['optimization', 'probability']"
4796686,Probability of line segments intersecting on a plane - A generalization to Buffon's needle problem,"I came up with this problem: If I draw a length 1 line segment randomly, then draw another one,
what's the probability that they'll intersect? More precisely, Consider a rectangular area of size $w\times l$ with periodic boundary
conditions, where $w\ge 2,l\ge 2$ . Draw two length 1 line segments
randomly. Here, by randomly it means, first choose a random point with
uniform distribution over the rectangular area, then from the circle
that is centered at the first point with radius one, randomly choose a
point with uniform distribution. These two points are the end points
of the line segment. Question: What's the probability that the
two line segments intersect? The solution can be found by simply integrating the probability densities, which gives $p=\frac{1}{A}\times 4\times\frac{1}{2\pi}(p_1+p_2+p_3+p_4)$ ,
where $A=lw$ is the area, and $p_1=\int_0^1 rdr\int_0^{\frac{\pi}{2}}\theta-\sin^{-1}(r\sin\theta)d\theta$ $p_2=\int_{\frac{1}{2}}^1dx\int_0^\sqrt{1-x^2}\tan^{-1}\frac{x}{y}+\tan^{-1}\frac{1-x}{y}dy$ $\quad=\frac{1}{2}\int_0^1 rdr\int_{\sin^{-1}\frac{r}{2}}^{\frac{\pi}{2}}\theta+\tan^{-1}\frac{1-r\sin\theta}{r\cos\theta}d\theta$ $p_3=\int_0^1 rdr\int_0^{\sin^{-1}\frac{r}{2}}\theta+\cos^{-1}(r\cos\theta)d\theta$ $p_4=\int_0^{\frac{1}{2}}dx\int_\sqrt{1-x^2}^1 2\cos^{-1}y\ dy$ And the results are $p_1=\frac{1}{4}$ $p_2=0.583664$ $p_3=0.155138$ $p_4=0.011197 =\frac{1}{72}(27 - 3 \sqrt 3 \pi - \pi^2)$ Details can be found on my blog post . The summation of these numbers is 0.999999, which is probably not a coincidence. My questions are, Do they sum up to exactly 1, so that the probability is exactly $\frac{2}{\pi A}$ ? (Probably true, just need to find the exact integration value.) (✓ Proved) Is there an easier and more intuitive way to prove this? How can this be generalized? I searched similar problems but I didn't find anything quite the same. Considering that Buffon's needle problem has probability $\frac{2l}{\pi t}$ , are they somehow related? A more general description of this problem can be stated as the following: Assuming there is a random uniform distribution with
density $\frac{1}{A}$ of line segments with length 1 on the 2D plane.
More precisely, the distribution is, the middle points of the line
segments are uniformly distributed on the plane with density $\frac{1}{A}$ and the angles are also uniformly distributed from 0 to $2\pi$ . Then, if we draw another line segment of length one randomly, as
described in the beginning, the expected number of intersections between
this line segment and the line segments in the ""background"" is $\frac{2}{\pi A}$ . Edit: I think, indeed, the Buffon's needle problem can be seen as a special case of this problem . If we see each line as an infinite chain of length 1 line segments, and the distance between the lines is $t$ , then each line segment occupies an area $t$ , thus the expectation of the number of intersections between a length 1 needle with the lines is $\frac{2}{\pi A}=\frac{2}{\pi t}$ . Due to the linearity of expectation, a length $l$ needle will have expectation $\frac{2l}{\pi t}$ . When $l\leq t$ , there can be at most one intersection, thus this is the probability of them intersecting. Update 2023.10.30 After seeing @Claude Leibovici's answer, it seems that the integration of $p_3$ is particularly complicated, so I thought maybe doing it in the other direction is easier. If we integrate $r$ first instead,the interval of $\theta$ is $[0,\frac{\pi}{6}]$ and the interval of $r$ is $[2\sin\theta,1]$ . The integration is $p_3=\int_0^{\frac{\pi}{6}}d\theta\int_{2\sin\theta}^1 (\theta+\cos^{-1}(r\cos\theta))rdr$ $\quad=\int_0^{\frac{\pi}{6}}\frac{\theta}{2}(1-4\sin^2\theta)+\left[\frac{r^2\cos^2\theta}{2}\cos^{-1}(r\cos\theta)+\frac{1}{4}\sin^{-1}(r\cos\theta)-\frac{r\cos\theta}{4}\sqrt{1-r^2\cos^2\theta}\right]_{2\sin\theta}^1\frac{1}{\cos^2\theta}d\theta$ $\quad=\int_0^{\frac{\pi}{6}}\frac{\theta}{2}(1-4\sin^2\theta)+\left(\theta\frac{\cos^2\theta}{2}+\frac{1}{4}(\frac{\pi}{2}-\theta)-\frac{\sin\theta\cos\theta}{4}-\frac{\sin^2 2\theta}{2}(\frac{\pi}{2}-2\theta)-\frac{1}{4}(2\theta)+\frac{\sin 2\theta \cos 2\theta}{4}\right)\frac{1}{\cos^2\theta}d\theta$ which is exactly $\frac{1}{36}(9+3\sqrt 3\pi-2\pi^2)$ . Combined with the other exact results, I think it's safe to say that the probability equals $\frac{2}{\pi A}$ is proved? Some variations that I can think of: Change background line segments into disks of radius 1. (Should be $\frac{1}{A}\left(\pi+\int_1^{\sqrt 2} 2r\sin^{-1}\left(\frac{1}{r}\right)dr+\int_{\sqrt 2}^2 2r\cos^{-1}\left(\frac{r}{2}\right)dr\right)=\frac{\pi+2}{A}$ if my calculation is correct.) Disks of radius R. Empty circles of radius R<0.5. (Expected number of intersections, or probability of intersecting.) Empty circles of radius R>0.5. (Expected number of intersections, or probability of intersecting.) Are the expected numbers for cases 3 and 4 the same as considering the circle as the limit of an $n$ -gon as $n\rightarrow\infty$ ? From the linearity of expectation, I expect that... If these problems turn out to be interesting, I'll post them in a seperate question.","['integration', 'geometric-probability', 'geometry', 'probability']"
4796759,Proving that the zero solution of a linear periodic ODE system is unstable,"Prove that the zero solution is unstable for the system $x' = A(t)x$ with $A = \begin{pmatrix} \frac{1}{2} - \cos(t) & 12 \\ 147 & \frac{3}{2} + \sin(t)\end{pmatrix}$ . I've tried the following: The system is equivalent to $x' = Bx + D(t)x$ , with $B= \begin{pmatrix} \frac{1}{2} & 12 \\ 147 & \frac{3}{2} \end{pmatrix}$ and $D(t) = \text{diag}(-\cos(t), \sin(t))$ , then find the eigenvalues of $B$ and notice that one of them is positive. Therefore the linear system $x' = Bx$ has unstable zero solution. I'm unsure however on how to translate that into instability of the original system, since the system is not autonomous. The question is in the Floquet theory section of the book so presumably there is a way to find/involve the characteristic multipliers/exponents. Any help is appreciated.","['periodic-functions', 'stability-theory', 'ordinary-differential-equations']"
4796773,"Derived set of subset $A$ of metric space $(X,d)$ is a closed set","I have to prove that derived set of subset $A$ of metric space $(X,d)$ is a closed set.
And here is my attempt: it is sufficient to prove $\overline{A^{'}}=A^{'}$ , and it is equivalent to prove $(A^{'})^{'}\subset A^{'}$ .Suppose $A$ is infinite. (if $A$ is finite, $A^{'}$ is empty, it's closed. )So, $\forall x\in A^{'}, \exists  p_n\in A^{'}$ s.t. $p_n\rightarrow x$ . If $A^{'}$ is finite, it is closed. If $A^{'}$ is infinite, we have $\forall y\in (A^{'})^{'}, \exists  x_n\in A^{'}$ s.t. $x_n\rightarrow y$ .Now, I need to prove $\forall y\in (A^{'})^{'} \exists p_n\in A$ s.t. $p_n\rightarrow y$ , which means every point of $(A^{'})^{'}$ is a limit point of $A$ , i.e. $(A^{'})^{'}\subset A^{'}$ . Fix $y\in(A^{'})^{'}$ , then we have $x_n(\in A^{'})\rightarrow y$ , and for each $x_n$ , we have $p_n^m(\in A)\rightarrow x_n$ , for each n, there exists $p_n^k$ s.t. $d(p_n^k,x_n)<\frac{1}{n}$ , let $b_n=p_n^k$ , then we have $b_n\rightarrow y$ , and $y$ is arbitrary, thus $(A^{'})^{'}\subset A^{'}$ .
I have 3 questions. 1.Is my proof correct? 2.If my proof is correct, can we use this method to prove the same property in Hausdorff space, we don't have something like ""distance"" in Hausdorff space, so I don't know how to select an element from $p_n^m$ . 3.In my attempt, I find a subsequence $p_1^{k(1)},p_2^{k(2)}...$ .And we can do the same thing in $\mathbb{R}$ . If we have countable subset of $\mathbb{R}$ , $A_{1}, A_{2},...,A_{n}...$ , and for each $n$ , $A_{n}$ is countable too, in other words, it is sequence, denoted as $A_{n}=\{a_{n1},a_{n2},a_{n3}...a_{nm}...\}$ , suppose for each n, the sequence $a_{nm}$ is converge to $b_n$ , now we have sequence $b_n$ , and suppose $b_n$ is converge to $y$ , like we did above, we can select $c_1\in A_1,c_2\in A_2...c_n\in A_n...$ , then sequence $c_n$ is converge to $y$ . We know the countable union of countable set is also countable, in this sense, $\bigcup\limits_{i=1}^{\infty}A_i$ is also a sequence $T_n$ , so we can say there is a subsequence of $T_n$ converge to $y$ . Is it true for sequence $T_n$ ? I guess it is related to the arrangement of $T_n$ , for the arrangement of the set of rational numbers, it's false, however, I just know one arrangement of the countable union of countable set, it is the same as set of rational numbers. Do we have any other arrangement of these two sets? Especially, do we have a special arrangement makes proposition always true? Thanks!","['elementary-set-theory', 'general-topology', 'solution-verification']"
4796783,Constant area of triangle from tangent line and axes,"Let $f:(0,+\infty)\to (0,+\infty)$ be a differentiable function such that $f'(x)\not=0$ with the following property: the area of the triangle formed by the tangent line of $C_f$ at a point $M(x_0,f(x_0))$ , $x'x$ and $y'y$ is constant and independent of $x_0\in (0,+\infty)$ . Find all functions $f$ with the above properties. I know that $f(x)=\frac{1}{x}, x>0$ satisfies the above conditions. Attempt Let $\varepsilon_{x_0}:\ y-f(x_0)=f'(x_0)(x-x_0)$ be the equation of a tangent line of $C_f$ at a point $M(x_0,f(x_0))$ and $A\bigg(\frac{x_0f'(x_0)-f(x_0)}{f'(x_0)},0\bigg)$ , $B(0,-x_0f'(x_0)+f(x_0))$ the intersection points of $\varepsilon_{x_0}$ with the axes. Then $(OAB)=c=\text{constant}\Leftrightarrow  (x_0f'(x_0)-f(x_0))^2=-2cf'(x_0)$ . Questions Is there any reference of the above problem in the literature? Is there an easy way to solve the differential equation $(xf'(x)-f(x))^2=-2cf'(x)$ ?","['area', 'ordinary-differential-equations', 'tangent-line', 'calculus', 'derivatives']"
4796863,Naturalness of definition of line integral,"Let $I = [a,b]$ denote some interval, and $f : I \rightarrow \mathbb{C}$ a continuous function of bounded variation, in other words, $f$ is a parameterisation of $\gamma = f(I)$ , a rectifiable curve. Let $\mathcal{l}(\gamma) = L$ be its length, and write $\hat{f} : [0,L] \rightarrow \mathbb{C}$ for the normal parameterisation (that is, $\hat{f}$ is obtainable from $f$ via a continuous, non-decreasing change of parameter, and the length function of $\hat{f}$ is given by $\sigma(s) = s$ for $0 \leq s \leq L$ ). Suppose $\rho : \mathbb{C} \rightarrow \mathbb{C}$ is Borel measurable and define: $$
\int_{\gamma} \rho |dz| = 
\begin{cases}
\displaystyle\int_{[0,L]} \rho \circ \hat{f} dm = \int_{\gamma} \rho \hat{f}_{\ast}(dm)&\text{if }L>0\\
\\
0 &\text{otherwise}
\end{cases}
$$ where $dm$ is the one-dimensional Lebesgue measure. This is independent of the parameterisation $f$ , in the sense that if $g = f \circ \phi$ , where $\phi : [c,d] \rightarrow I$ is non-decreasing and continuous then $\hat{g} = \hat{f}$ . $\textbf{Question}$ : Is it necessarily the case that $\hat{f}_{\ast}(dm) = \mathcal{H}^{1} \restriction \gamma$ ? Here $\mathcal{H}^1$ is the one-dimensional Hausdorff measure in $\mathbb{C} \equiv \mathbb{R}^2$ and $\hat{f}_{\ast}(dm)$ is the pushforward of the one-dimensional Lebesgue measure on $[0,L]$ onto $\gamma$ . If $f$ is not injective this can't be true, but is there still a way to relate everything to $\mathcal{H}^1$ with multiplicity?","['bounded-variation', 'measure-theory', 'hausdorff-measure', 'real-analysis']"
4796876,Proving that $(G_\lambda)_\lambda$ is a resolvent family,"Let $\mathcal{E}$ be a bilinear form on dense subset $\mathcal{D}\subset H$ of a Hilbert space $H$ . Assume $\mathcal{E}$ is a closed, symmetric and positive definite, i.e., $\mathcal{E}(u,u)\geq0$ . For $\lambda>0$ , define the scalar product $\mathcal{E}_\lambda(\cdot,\cdot): \mathcal{D} \times\mathcal{D}\to \Bbb R$ , $$
\mathcal{E}_\lambda(u,v)= \lambda (u,v)_H+ \mathcal{E}(u,v). 
$$ Since $\mathcal{E}(\cdot,\cdot)$ is closed, it follows that $(\mathcal{D}, \mathcal{E}_\lambda(\cdot,\cdot))$ is a Hilbert space. Applying the  Riesz representation theorem on the space $(\mathcal{D}, \mathcal{E}_\lambda(\cdot,\cdot))$ yields the existence of a linear operator $G_\lambda : H\to \mathcal{D}$ such that for $u\in H$ and $v\in \mathcal{D}$ we have $$
 (u,v)_H= \mathcal{E}_\lambda(G_\lambda u,v)= \lambda (G_\lambda u,v)_H+ \mathcal{E}(G_\lambda u,v). 
 $$ In particular,  taking $v=G_\lambda u$ yields $$
 \lambda \|G_\lambda u\|^2_H\leq  \lambda \|G_\lambda u\|^2_H+ \mathcal{E}(G_\lambda u,G_\lambda u)= (u,G_\lambda u)_H\leq \|u\|_H\|G_\lambda u\|_H. 
 $$ This implies $$
  \|G_\lambda u\|_H\leq  \frac{1}{\lambda}\|u\|_H.
 $$ Question Show that $(G_\lambda)_\lambda$ is a resolvent family: that is $G_{\lambda_2}G_{\lambda_1}=G_{\lambda_1}G_{\lambda_2}$ , $$G_{\lambda_1}-G_{\lambda_2}= (\lambda_2-\lambda_1) G_{\lambda_1}G_{\lambda_2}$$ and $$\lim_{\lambda\to\infty}\|\lambda G_\lambda u-u\|_H=0.$$","['semigroup-of-operators', 'operator-theory', 'functional-analysis', 'analysis']"
4796910,Solving System of Equations Issue,"$$ \begin{aligned}
 x^2+4y^2+9z^2 &= 1 \\
 8yz+2ax &=0 \\
 8zx+8ay &= 0\\ 
 8xy+18az &= 0
\end{aligned}$$ How would I solve these equations? I tried making $a$ the subject of the final $3$ but struggling to solve it.","['systems-of-equations', 'real-analysis', 'multivariable-calculus', 'linear-algebra', 'algebra-precalculus']"
4796932,What's wrong with this limit solution?,"I was solving this limit: $$
\lim_{x\rightarrow +\infty} \left( \sqrt{x^2+x+1}-x \right) 
$$ And here is my solution: $$
\lim_{x\rightarrow +\infty} \left( \sqrt{x^2+x+1}-x \right) =\lim_{x\rightarrow +\infty} \left[ \sqrt{x^2\left( 1+\frac{1}{x}+\frac{1}{x^2} \right)}-x \right] =\lim_{x\rightarrow +\infty} \left( \sqrt{x^2}-x \right) =0
$$ The correct answer is 1/2. I know the correct solution to get the right answer. I just want to know where this solution went wrong. Thanks for the responses. After reading some answers, I have developed some ideas, and i don't know if they are right... When $
x\rightarrow \infty 
$ , I always choose to neglect terms with lower orders than x. For example, $$
\lim_{x\rightarrow \infty} \frac{\left( 1+2x \right) ^{10}\left( 1+3x \right) ^{20}}{\left( 1+6x \right) ^{15}}
$$ I would transform it into $$
\lim_{x\rightarrow \infty} \frac{\left( 2x \right) ^{10}\left( 3x \right) ^{20}}{\left( 6x \right) ^{15}}
$$ And the calculation result is correct. If you want to write down each step in detail, it should be like this： $$
\lim_{x\rightarrow \infty} \frac{\left( 1+2x \right) ^{10}\left( 1+3x \right) ^{20}}{\left( 1+6x \right) ^{15}}=\frac{\lim_{x\rightarrow \infty} \left( 1+2x \right) ^{10}\lim_{x\rightarrow \infty} \left( 1+3x \right) ^{20}}{\lim_{x\rightarrow \infty} \left( 1+6x \right) ^{15}}=\frac{\lim_{x\rightarrow \infty} \left( 2x \right) ^{10}\lim_{x\rightarrow \infty} \left( 3x \right) ^{20}}{\lim_{x\rightarrow \infty} \left( 6x \right) ^{15}}=\lim_{x\rightarrow \infty} \frac{\left( 2x \right) ^{10}\left( 3x \right) ^{20}}{\left( 6x \right) ^{15}}
$$ But the condition for me to be able to do this is that every limit here exists. Returning to the original question, if I write out the calculation process in detail, it would look like this $$
\lim_{x\rightarrow +\infty} \left( \sqrt{x^2+x+1}-x \right) =\lim_{x\rightarrow +\infty} \left[ \sqrt{x^2\left( 1+\frac{1}{x}+\frac{1}{x^2} \right)}-x \right] =\lim_{x\rightarrow \infty} \sqrt{x^2}\cdot \lim_{x\rightarrow \infty} \sqrt{\left( 1+\frac{1}{x}+\frac{1}{x^2} \right)}-\lim_{x\rightarrow \infty} x
$$ However, $
\lim_{x\rightarrow \infty} \sqrt{x^2}
$ and $
\lim_{x\rightarrow \infty} x
$ do not exist, so the second equal sign is not valid. I'm not sure if this is correct... I think I can expand on this question. The reason I calculate this way is because I'm accustomed to discarding lower-order terms, and this approach has consistently proven effective in previous calculations. Like $
\lim_{x\rightarrow \infty} \frac{4x^2-1}{2x^2-x-1}=\lim_{x\rightarrow \infty} \frac{4x^2}{2x^2}=\frac{1}{2}
$ and $
\lim_{x\rightarrow \infty} \frac{\left( 2-3x \right) ^3\left( 3+2x \right) ^5}{\left( 1-6x \right) ^8}=\lim_{x\rightarrow \infty} \frac{\left( -3x \right) ^3\left( 2x \right) ^5}{\left( -6x \right) ^8}=-\frac{1}{2^3\times 3^5}
$ , their calculation results are all correct. However, in this particular question, this method is not applicable. So, my question is whether this method is actually correct? If it's not correct, why did it work in the previous equations? If it is correct, then why did it fail in this particular question?","['limits', 'calculus', 'radicals', 'fake-proofs']"
4796955,What is the Expected Time of Crystallization?,"Consider the process of periodic 1D crystallization, where multiple sites initiate crystallizing waves at random with speed $v$ . The fraction of the crystallized substance, at position $x$ and time $t$ , is given by $$
f(x,t)=1-e^{-\int_{V_X(v)}p(Y) \,dY} 
$$ where $p(x,t)$ is the crystallization initiation rate at position $x$ and time $t$ , and $V_X(v)$ is the past light-cone of the spacetime point $X=(x,t)$ (see figure below, where $x_\pm = x\mp vt$ ). My goal is to find an expression for $p$ in terms of the expected time of crystallization at each space point, which can be given by $$
t_E(x)=\int_0^\infty t \frac{\partial f(x,t')}{\partial t'}|_{t'=t} \,dt
$$ Some ideas: From this paper (in the context of DNA replication, analogous to the process of 1D crystallization, see also this publication and chapter II of this thesis ), defining the fraction of uncrystallized substance $s(x,t)=1-f(x,t)$ , $p$ is shown to satisfy $$
p(x,t)=-\frac{v}{2}\square \log(s(x,t)),
$$ where $\square =\frac{1}{v^2}\partial_t^2-\partial_x^2$ is the d'Alembert operator . Could I use this to write it in terms of the expected time of crystallization at each space point? I was thinking that, given the shape of $f$ , perhaps a Laplace Transform could be used to rewrite the fraction $s$ in terms of the expected time at each space point, which would be our data. A simple example: When $p(x,t)=p$ is constant in spacetime, we have $f(x,t)=1-e^{-pvt^2} $ and thus $$
t_E(x)=2pv\int_0^\infty t^2 e^{-pv t^2}\,dt=\frac12\sqrt{\frac{\pi}{pv}}
$$ which can easily be inverted. Any ideas for the general case?","['integration', 'laplace-transform', 'analysis', 'expected-value', 'physics']"
4796958,Non-polynomial entire function with finitely many zeros tends to zero on circles,"Suppose $f$ is entire with finitely many zeros. Assume $f$ is not a polynomial. Let $m(r) = \inf_{|z| = r} |f(z)|$ . I want to show that $$\lim_{r \rightarrow \infty} m(r) =0.$$ I want to note that a previous problem required me to prove that the function $M_f(r) = \sup_{|z|=r} |f(z)|$ is a non-decreasing function of $r$ (should be true for any entire function). A hint I was given for this problem was to factor out the zeros to obtain a non-vanishing entire function $g$ . Then supposing that $m(r)$ does not go to zero, I should try to estimate $m(r)$ using $M_{1/g}(r)$ and show that $1/g$ is a polynomial. My current progress so far is that I can write $f(z) = g(z) \prod_{i=1}^k (z-z_i)^k$ where $z_1, \ldots, z_k$ are the finitely many zeros of $f$ , the $k_i$ are the respective orders of the zeros, and $g(z)$ is entire and non-vanishing. Then analyzing $M_{1/g}(r)$ , we have for large enough $r$ so that $f(z) \neq 0$ , $$ M_{1/g}(r) = \sup_{|z|=r} \frac{1}{|g(z)|} = \sup_{|z|=r} \frac{\prod_{i=1}^k |z-z_i|^k}{|f(z)|} \leq  \frac{\sup_{|z|=r}\prod_{i=1}^k |z-z_i|^k}{\inf_{|z|=r}|f(z)|} = \frac{\sup_{|z|=r}\prod_{i=1}^k |z-z_i|^k}{m(r)}. $$ So rearranging gives us $$ m(r) \leq \frac{\sup_{|z|=r}\prod_{i=1}^k |z-z_i|^k}{M_{1/g}(r)}. $$ From here, I'm not sure how to make use the assumption that $m(r)$ does not go to zero and how to show $1/g$ is a polynomial. Furthermore, I'm not sure how that leads to contradiction. I would appreciate any hints.","['complex-analysis', 'entire-functions']"
4797009,Convex function on the graph but impossible to prove??!,"How can we show that the function $f:\left (0,a\right )\to (0,1)$ given by $$f(x)=\dfrac{1}{1+e^{\frac{a}{x}-\frac{a}{a-x}}}$$ is convex on $\left (0,\frac{a}{2}\right )$ ? I made the graph and I computed its first derivative: $$f'(x)=\dfrac{a\left (\dfrac{1}{x^2}+\dfrac{1}{(a-x)^2}\right )e^{\frac{a}{x}-\frac{a}{a-x}}}{\left (1+e^{\frac{a}{x}-\frac{a}{a-x}} \right )^2}$$ . Now we have to show that $f'$ is increasing on $\left (0,\frac{a}{2}\right )$ . I computed the second derivative and even the third derivative (they are too ugly to post them here: I used https://www.derivative-calculator.net ). What I show is that $x=a/2$ is a solution of $f''(x)=0$ . The graph of $f'$ shows that $a/2$ is the unique maximum point. I am stucked in proving that the equation: $$\left (\dfrac{2}{(a-x)^3}-\dfrac{2}{x^3}\right )\cdot \left (e^{\frac{a}{a-x}-\frac{a}{x}}+e^{\frac{a}{x}-\frac{a}{a-x}}+2 \right )=a\left (\dfrac{1}{x^2}+\dfrac{1}{(a-x)^2} \right )^2\cdot \left (e^{\frac{a}{x}-\frac{a}{a-x}}-e^{\frac{a}{a-x}-\frac{a}{x}} \right )$$ has a unique solution $x\in (0,a)$ and that solution is $x=\dfrac{a}{2}$ . It is obviously true from the graphic but seems impossible to prove. I feel very frustrated because of that. Maybe there is some trick...or other way of doing it.","['roots', 'functions', 'exponential-function', 'real-analysis']"
4797031,In a trapezoid with perpendicular diagonals the square of sum of the bases is equal to sum of the squares of the diagonals,"Diagonals of a trapezoid are perpendicular to each other. Prove that the square of sum of the bases of the trapezoid is equal to sum of the squares of the diagonals. One way to solve this problem is to extend $AB$ from $B$ and then from $C$ drawing a line parallel to $BD$ . Assume the intersection is $F$ . Now it can be proven that the quadrilateral $BFCD$ is parallelogram hence $\angle ACF= 90^{\circ}$ . And after applying the Pythagorean theorem, the problem is solved. But I'm trying to solve this problem with a different approach, Here we want to prove $$(a+b)^2=(n+q)^2+(m+p)^2$$ $$a^2+b^2+2ab= n^2+q^2+2nq+m^2+p^2+2mp$$ Since $a^2+b^2 = m^2+n^2+p^2+q^2$ the problem is reduced to proving $ab= mp+nq$ . Here I'm not sure how to continue. Since $\triangle ABO$ and $\triangle CDO$ are similar triangles we have $\frac ab=\frac mp = \frac nq$ but don't know if this helps.","['euclidean-geometry', 'geometry']"
4797033,Dynkin's $\pi$ lemma,"Consider the following Lemma 1.4.1 (Dynkin's $\pi$ -system lemma). Let $\mathcal{A}$ be a $\pi$ -system. Then any $d$ -system containing $\mathcal{A}$ contains also the $\sigma$ -algebra generated by $\mathcal{A}$ . Proof. Denote by $\mathcal{D}$ the intersection of all $d$ -systems containing $\mathcal{A}$ . Then $\mathcal{D}$ is itself a $d$ -system. We shall show that $\mathcal{D}$ is also a $\pi$ -system and hence a $\sigma$ -algebra, thus proving the lemma. Consider $$
\mathcal{D}^{\prime}=\{B \in \mathcal{D}: B \cap A \in \mathcal{D} \text { for all } A \in \mathcal{A}\}
$$ Then $\mathcal{A} \subseteq \mathcal{D}^{\prime}$ because $\mathcal{A}$ is a $\pi$ -system. Let us check that $\mathcal{D}^{\prime}$ is a $d$ -system: clearly $E \in \mathcal{D}^{\prime} ;$ next, suppose $B_1, B_2 \in \mathcal{D}^{\prime}$ with $B_1 \subseteq B_2$ , then for $A \in \mathcal{A}$ we have $$
\left(B_2 \backslash B_1\right) \cap A=\left(B_2 \cap A\right) \backslash\left(B_1 \cap A\right) \in \mathcal{D}
$$ because $\mathcal{D}$ is a $d$ -system, so $B_2 \backslash B_1 \in \mathcal{D}^{\prime}$ ; finally, if $B_n \in \mathcal{D}^{\prime}, n \in \mathbb{N}$ , and $B_n \uparrow B$ , then for $A \in \mathcal{A}$ we have $$
B_n \cap A \uparrow B \cap A
$$ so $B \cap A \in \mathcal{D}$ and $B \in \mathcal{D}^{\prime}$ . Hence $\mathcal{D}=\mathcal{D}^{\prime}$ .
Now consider $$
\mathcal{D}^{\prime \prime}=\{B \in \mathcal{D}: B \cap A \in \mathcal{D} \text { for all } A \in \mathcal{D}\} .
$$ Then $\mathcal{A} \subseteq \mathcal{D}^{\prime \prime}$ because $\mathcal{D}=\mathcal{D}^{\prime}$ . We can check that $\mathcal{D}^{\prime \prime}$ is a $d$ -system, just as we did for $\mathcal{D}^{\prime}$ . Hence $\mathcal{D}^{\prime \prime}=\mathcal{D}$ which shows that $\mathcal{D}$ is a $\pi$ -system as promised. I am struggling to understand the bigger picture. Moreover, what is $\mathcal{D}'$ used for? Reading the proof it seems that the author claims that the work we have done earlier shows that $\mathcal{A} \subseteq \mathcal{D}''$ but I don't see how. Could someone re-elaborate the last paragraph?",['measure-theory']
4797035,Integer series related to the functional differential equation $f'(x) = f(f(x))$,"There was an interesting discussion on the functional differential equation $$f'(x) = f(f(x)) \tag{1a}$$ The essence of which was to find a solution of $(1a)$ by Taylor expansion of $f$ about a point $x=a$ , i.e. $$f(x,a) = \sum_{k\ge 0} \frac{(x-a)^k}{k!} f^{(k)}(a)\tag{2}$$ Matters simplify considerably if $a$ is assumed to be a fixed point of $f$ , i.e. $$f(a) = a\tag{1b}$$ This program was carried out to a large extent in the reference quoted. Here I'd like to draw attention to the emerging sequence of integers which in itself is of interest, and could contribute the the estmation of the radius of convergence of $(2)$ . We can conveniently split our goal in two tasks: task 1: Calculate the $n$ -th derivative of a function $f(x)$ which obeys $(1a)$ and $(1b)$ task 2: Find the general term of the integer sequence found in task 1. Here is what I did so far. To start task 1 let us calculate the first few derivatives at a general $x$ as well as at the fixed point. We will be using the abbreviated notation $f^{(k)}(x)$ for the k-th derivative, and $f_k(x)$ for the k-fold iterated function $f(x)$ . Then we have $$\begin{align*}\frac{d}{dx}f(x) &=f^{(1)}(x)\overset{\text{(1a)}}= f(f(x)) = f_2 (x) \end{align*}\tag{3a}$$ $$f^{(1)}(a) = f_2(a) = a\tag{3b}$$ $$\begin{align*}f^{(2)}(x) &= \frac{d}{dx}f^{(1)}(x) =\frac{d}{dx}f(f(x)) = f'(f(x))\cdot\frac{d}{dx}f(x)\\&
\overset{\text{(1a)}}=f(f(f(x))) \cdot f(f(x))= f_3(x) \cdot f_2(x)\end{align*} \tag{4a}$$ $$f^{(2)}(a) =f_3(a) \cdot f_2(a) = a\cdot a = a^2\tag{4b}$$ Before we continue we note that the derivative of the k-fold iterated function is, for $k \ge 2$ , given by $$\frac{d}{dx} f_k(x) =f_{k+1}(x)f_{k}(x)f_{k-1}(x)\cdots f_{2}(x) \tag{5a}$$ and at the fixpoint we have (notice that since $f_{1}(a)=a$ and $f_{k+1}(a)=f(f_{k}(a))$ we find that $f_{k}(a)=a$ for all $k$ ) so that $$\frac{d}{dx} f_k(x) |_{x \to a} =f_{k+1}(a)f_{k}(a)f_{k-1}(a)\cdots f_{2}(a)=a^k \tag{5b}$$ Now continuing $$f^{(3)}(x)= \frac{d}{dx}f^{(2)}(x)=\frac{d}{dx}\left(f_3(x) \cdot f_2(x)\right)=\frac{d}{dx}\left(f_3(x)\right) \cdot f_2(x)+f_3(x) \frac{d}{dx}\left(f_2(x)\right)\\
= \left(f_4(x)f_3(x)f_2(x)\right)f_2(x) + f_3(x)\left(f_3(x)f_2(x)\right) 
=f_2(x)^2 f_3(x) f_4(x) + f_2(x) f_3(x)^2$$ $$f^{(3)}(x)|_{x \to a} =a^2 a a + a a^2= a^4+a^3$$ Now one step more $$f^{(4)}(x)= \frac{d}{dx}f^{(3)}(x)=
\frac{d}{dx}\left(f_2(x)^2 f_3(x) f_4(x) + f_2(x) f_3(x)^2\right)\\
=2 f_2 (f_3 f_2 )  f_3  f_4 + f_2^2 (f_4 f_3 f_2 ) f_4  +f_2 ^2 f_3 (f_5 f_4 f_3 f_2 ) + (f_3 f_2 ) f_3^2+ 2 f_2 f_3 (f_4 f_3 f_2 )\\
=2 f_2^2 f_3^2 f_4+f_2^3 f_3 f_4^2 +f_2 ^3 f_3^2 f_4 f_5+f_2  f_3^3 + 2 f_2^2 f_3^2 f_4 \\
=4 f_2^2 f_3^2 f_4+f_2^3 f_3 f_4^2 +f_2 ^3 f_3^2 f_4 f_5+f_2  f_3^3$$ $$f^{(4)}(x)|_{x \to a} =4 a^2 a^2 a + a^3 a a^2 +a^3a^3 a a + a a^3\\
=a^4+4a^5 + a^6+a^7$$ A litte Mathematica code yields more $$
\begin{array}{l}
 \{\text{d1f(a)},a\} \\
 \left\{\text{d2f(a)},a^2\right\} \\
 \left\{\text{d3f(a)},a^4+a^3\right\} \\
 \left\{\text{d4f(a)},a^7+a^6+4 a^5+a^4\right\} \\
 \left\{\text{d5f(a)},a^{11}+a^{10}+4 a^9+8 a^8+11 a^7+11 a^6+a^5\right\} \\
 \left\{\text{d6f(a)},a^{16}+a^{15}+4 a^{14}+8 a^{13}+22 a^{12}+22 a^{11}+60 a^{10}+58 a^9+66 a^8+26 a^7+a^6\right\} \\
 \left\{\text{d7f(a)},a^{22}+a^{21}+4 a^{20}+8 a^{19}+22 a^{18}+38 a^{17}+76 a^{16}+122 a^{15}+220 a^{14}+319 a^{13}+387 a^{12}+553 a^{11}+424 a^{10}+302 a^9+57 a^8+a^7\right\} \\
\end{array}
$$ Writing generally $$f^{(n)}(x)|_{x \to a} =\sum_{k=0}^{\infty}c_{n,k}a^k$$ task 2 ist to find a formula for the coefficient fields $c_{n,k}$ . For small $n$ we have $$c_{2} = (1)$$ $$c_{3} = (1,1)$$ $$c_{4} = (1,4,1,1)$$ $$c_{5} = (1,11,11,8,4,1,1)$$ $$c_{6} = (1,26,66,58,60,22,22,8,4,1,1)$$ $$c_{7} = (1,57,302,424,553,387,319,220,122,76,38,22,8,4,1,1)$$ I tried to identify sequences in the OEIS library but with only modest success: If read horizontally, no match was found. If read vertically, (1,4,11,26,57) -> https://oeis.org/A000295 , Eulerian numbers (Euler's triangle: column k=2 of A008292, column k=1 of A173018).
(Formerly M3416 N1382) (1,11,66,302) -> https://oeis.org/A000460 Eulerian numbers (Euler's triangle: column k=3 of A008292, column k=2 of A173018).
(Formerly M4795 N2047) (8,58,424) -> no match Here I am stuck. Can you do better?","['functional-equations', 'ordinary-differential-equations', 'fixed-points', 'derivatives', 'integer-sequences']"
4797044,"How can we use undefined term ""infinity"" in limits?","Consider function $f$ such that $f:R \to R$ and $$\displaystyle \lim_{x \to \infty} f(x) $$ Now in standard textbooks of calculus such a limit can be found out if it exist. Like for example $$\displaystyle \lim_{x \to \infty} 1+\dfrac{1}{x^2}=1.$$ But my question is how can limit of $x$ tending to"" $\infty""$ can be found if infinity itself is not even defined? Like i know that we are finding limit when $x$ is ""very large"" but the fact that we are using infinity is not very appealing to me . Instead can we say that $$\displaystyle \lim_{x \to M} f(x) $$ where $M$ is such that $\forall m \in R$ $$M>m.$$ But again we do not know what $M$ is. So can someone clarify this?","['real-numbers', 'limits', 'calculus']"
4797061,Product Rule for differential forms [duplicate],"This question already has an answer here : Why is the exterior product an antiderivation intuitively? (1 answer) Closed 8 months ago . For a $p$ -form, $\alpha$ , and a $q$ -form, $\beta$ , why is the exterior derivative product rule given by $$d(\alpha\wedge\beta)=d\alpha\wedge\beta+(-1)^p\alpha\wedge d\beta,$$ with $(-1)^p$ ? Wikipedia says that it comes from the graded product (Leibniz) rule, but those Wikipedia articles give this relation as a definition that applies to differential forms. Both resources I am using to learn about differential forms give this relation without proof. How can I mathematically prove it and/or physically/geometrically see it?","['derivatives', 'differential-geometry']"
4797084,When is the restriction of scalars functor essentially surjective?,"For an algebra map $f: A \to B$ , under what conditions is the restrictions of scalars functor $\mathbf{Mod}_B \to \mathbf{Mod}_A$ essentially surjective?","['ring-theory', 'abstract-algebra', 'category-theory', 'modules']"
4797130,What is the expected number of wins in this game?,"Let's imagine a two player game where a player rolls a dice and then can keep the first score or decide to re-roll to get his/her final score. Is there a general formula for the average expected number of points against a player who does the 'common sense' thing and re-rolls if they get less than $\frac{n}{2}+1$ , where $n$ is the dice size.  Each player fixes their re-roll value before the game, it is then unchanged. e.g. 6 sided dice, 3 example games  (...means re-rolls and uses second roll).  player 1 re-rolls if the score is less than 4, r(1)=4.  player 2 re-rolls if the score is less than 5, r(2)=5. game 1)  player 1 rolls 2 ....5   player 2 rolls 3 ...1   result (5,1) 1 point to player 1 game 2)   player 1 rolls 4.    Player 2 rolls 4...5   result (4,5) 1 point to player 2 game 3)  player 1 rolls 6 .   Player 2 rolls 3 ...6   result (6,6) 0.5 points to both players Average score $1.5/3=0.5$ for each player The work done so far is in the answer section below, but it would be interesting if a general result for the expected average points could be found, in terms of the dice size $n$ , and re-roll if the score is less than $r_1$ for player 1 and $r_2$ for player 2.","['statistics', 'probability']"
4797142,Is left multiplication well defined on a quotient set of a group?,"Let $G$ be a group and $H$ a subgroup, now for every $g \in G$ we define $\sigma_{g}:G/H \rightarrow G/H: xH \mapsto gxH$ . Note that we know nothing about the subgroup $H$ . My question is whether or not the function $\sigma_{g}$ is even well-defined in all cases. After all if $xH=yH$ , why should $\sigma_{g}(xH) = gxH = \sigma_{g}(yH) = gyH$ ? It seems to me like the only way this works for every $g \in G$ , is if $H$ is a normal subgroup. Am I missing something? Context: I had this question after reading option 2 in https://www.math3ma.com/blog/4-ways-to-show-a-group-is-not-simple .","['group-theory', 'quotient-set']"
4797205,Commutation of operators,"Let $(\mathcal{H},\langle\cdot,\cdot\rangle)$ be a complex Hilbert space. Furthermore, let $E:\mathcal{D}(E)\to\mathcal{H}$ be a self-adjoint unbounded operator and $A:\mathcal{D}(A)\to\mathcal{H}$ (not necessarily self-adjoint) another unbounded densely-defined operator such that $$EA=AE$$ on the common domain $\mathcal{D}(AE)\cap\mathcal{D}(EA)$ . Lets now take a bounded continuous function $f\in C(\sigma(E),\mathbb{C})$ and define the bounded operator $f(E)\in\mathcal{B}(\mathcal{H})$ by means of the spectral calculus. Under which additional assumptions is it true that $f(E)A=Af(E)$ on $\mathcal{D}(A)\cap\mathcal{D}(Af(E))$ ? I was looking in the internet and there seems to be some theorems about the commutation of operators and the spectral theorem, but I was not able to find a version in which both operators are unbounded.","['operator-theory', 'reference-request', 'hilbert-spaces', 'functional-analysis', 'spectral-theory']"
4797233,How to solve functional equation $f(x^{2}) = \frac{f(x)}{1 + x}$?,"I'm trying to solve this task: Find all $f : (-1, 1) \to \mathbb{R}$ continuous at $x = 0$ and satisfying $f(x^{2}) = \frac{f(x)}{1 + x}$ . I tried substituting $x$ with $-1, 0, 1$ , but it doesn't really do anything. I thought that maybe I should use limits here since I know that function is continuous at $x = 0$ , so I also tried to decrease the power: $f(x) = \frac{f(x^{\frac{1}{2}})}{1 + x^{\frac{1}{2}}}$ $f(x^{\frac{1}{2}}) = \frac{f(x^{\frac{1}{4}})}{1 + x^{\frac{1}{4}}}$ $...$ I am not sure what to do next. Could somebody please give a hint? Thanks in advance.","['continuity', 'functions', 'functional-equations']"
4797250,What is the $\lim_{n\to\infty}\sinh^{\circ n}\sqrt{\dfrac{3}{n}}$?,"Let $\sinh^{\circ n}x$ be the $n$ -fold iteration of $\sinh x$ . I'm interested in the limit $$
\lim_{n\to\infty}\sinh^{\circ n}\sqrt{\dfrac{3}{n}}.
$$ This limit exists because the general term is decreasing: it suffices to show that $$
\sqrt{\dfrac{3}{n}}>\sinh\sqrt{\dfrac{3}{n+1}},\quad\forall n\in\mathbb{N}^*,
$$ which can be deduced from $$
\operatorname{arsinh}x=\ln(x+\sqrt{x^2+1})>\dfrac{\sqrt{3}x}{\sqrt{x^2+3}},\quad\forall x>0.
$$ The following graph is the plot for the first million terms. So I would like to know if the limit can be determined and, if the limit is $0$ , can we say something about the asymptotic behavior of $\sinh^{\circ n}\sqrt{\dfrac{3}{n}}$ ? Any help appreciated.","['hyperbolic-functions', 'limits', 'analysis', 'sequences-and-series']"
4797265,Calc 1 : maximum and minimum values : finding t in this equation(confused about what I did vs. what my teacher did),"Calculus 1: Section 4.1 Maximum and Minimum Values Question: Find the critical numbers of the function $$f(t) = 2\cos(t) + \sin^2 (t).$$ What I'm stuck with about the questions is after getting the equation $$-2\sin(t) + 2\sin(t)\cos(t) = 0,$$ I added $2\sin(t)$ to both sides and solved it for $t$ , in order to find the critical numbers. I got $cos(t) = 1$ which I know that $t = 0$ would be the only critical number for this equation. But my teacher did something different: $$-2\sin t(1 - \cos t) = 0$$ and got different results (2 critical numbers).
Can you please explain why my solution doesn't work, and his is correct?","['calculus', 'derivatives']"
4797280,Raise a Matrix to Arbitrary Power,"I have a $k\times k$ matrix $$
A_{k}=
  \begin{pmatrix}
1 & 1 & \cdots & 1 & 1 & 1 \\
1 & 1 & \cdots & 1 &1 & 0\\
 &\vdots & &\vdots \\
1 & 1 & \cdots & 0 & 0 & 0\\
1 & 0 & \cdots & 0 & 0 & 0
  \end{pmatrix}
$$ where $k\ge 2$ . I would like to raise this matrix to arbitrary power $n$ . Does anyone have any idea on how to proceed? The only method I know is to convert the matrix in diagonal form, i.e. find the eigenvectors. However, I don't think that this method works here, because $k$ is arbitrary. In addition, as long as $k\ge 3$ , the eigenvalues are extremely complicated. I would appreciate it if anyone can suggest some ideas! In addition, it would be also helpful if you can provide some idea in calculating $A_{k}^{n}$ , even for some small $n$ (preferably large than $5$ ). Thank you every much!","['matrices', 'numerical-linear-algebra', 'linear-algebra', 'computational-algebra']"
4797346,Definition of transitivity of relations,"We are currently learning about transitivity, and my teacher gave us the following definition: For some binary relation $R$ of type $A^2$ (so $R \subseteq A^2$ holds): $$R \ \text{is transitive} \triangleq \forall a,c \in A ( \ \exists b \in A ( \ (aRb \wedge bRc) \implies aRc \ ) \ )$$ But online, I instead find the following definition: $$R \ \text{is transitive} \triangleq \forall a,b,c \in A ( \ (aRb \wedge bRc) \implies aRc \ )$$ These are equivalent to each other? But I don't quite (intuitively) get how so. Does that then generalise to this? $$\forall a,b,c( \ P(a,b,c) \ ) \iff \forall a,c ( \ \exists b( \ P(a,b,c) \ ) \ )$$ If so, is there a proof for it?","['elementary-set-theory', 'predicate-logic', 'first-order-logic']"
4797449,Is Beta function essential to evaluating the integral $\int_0^{\infty} \frac{d x}{\left(1+x^4\right)^n}$?,"After investigating the indefinite integral in the post , I found that $$\int \frac { d x } { x ^ { 4 } + 1 }   =  \frac { 1 } { 4 \sqrt { 2 } } \left[ 2 \tan ^ { - 1 } \left( \frac { x ^ { 2 } - 1 } { \sqrt { 2 } x } \right) + \ln \left| \frac { x ^ { 2 } + \sqrt { 2 } x + 1 } { x ^ { 2 } - \sqrt { 2 } x + 1 } \right|\right] + C;$$ $$\int \frac{d x}{\left(1+x^{4}\right)^{2}}=\frac{x}{4\left(1+x^{4}\right)}+\frac{3}{16 \sqrt{2}}\left[2 \tan ^{-1}\left(\frac{x^{2}-1}{\sqrt{2} x}\right)+\ln \left|\frac{x^{2}+\sqrt{2} x+1}{x^{2}-\sqrt{2} x+1}\right|\right]+C
$$ Plugging limits yields $$I_1= \int_0^{\infty} \frac{d x}{1+x^4}=\frac \pi{2\sqrt2};\quad 
I_2=\int_0^{\infty} \frac{d x}{\left(1+x^4\right)^2}=\frac{3 \pi}{8 \sqrt{2}}
$$ I guess about the value of the general integral $$I_n=\int_0^{\infty} \frac{d x}{\left(1+x^4\right)^n} =\frac {p\pi}{q\sqrt 2} $$ The form of the integrand reminds me that Beta function may help. Then I put the substitution $y=x^4$ and get $$
I_n =\frac{1}{4} \int_0^{\infty} \frac{y^{-\frac{3}{4}} d y}{(1+y)^n} =\frac{1}{4} B\left(\frac{1}{4}, n-\frac{1}{4}\right)
$$ Expressing in terms of Gamma function gives $$
\begin{aligned}I_n&=\frac{1}{4} \frac{\Gamma\left(\frac{1}{4}\right) \Gamma\left(n-\frac{1}{4}\right)}{\Gamma(n)} \\&= \frac{1}{4(n-1) !} \Gamma\left(\frac{1}{4}\right) \Gamma\left(n-\frac{1}{4}\right)\\
&=\frac{1}{4(n-1)!} \Gamma\left(\frac{1}{4}\right)\left(n-1-\frac{1}{4}\right)\left(n-2-\frac{1}{4}\right) \cdots \frac{3}{4} \Gamma\left(\frac{3}{4}\right) \cdots (*) \\
&=\frac{1}{4(n-1) !} \left[\frac{1}{4^{n-1}}(4 n-5)(4 n-9) \cdots 3 \right] \pi \csc \left(\frac{\pi}{4}\right) \cdots (**)\\
&=\frac{\pi \sqrt{2}}{4^n(n-1) !} \prod_{k=1}^{n-1}(4 k-1) 
\end{aligned}
$$ (*) uses $\Gamma(z+1)=z \Gamma(z)$ for all $z\in \mathbb{C}$ and (**) uses $\Gamma(x) \Gamma(1-x)=\pi \csc (\pi x)$ for $z\not \in \mathbb{Z}.$ For examples, $$
\begin{aligned}
& I_3=\frac{\pi \sqrt{2}}{128} \cdot 3 \cdot 7=\frac{21 \pi \sqrt{2}}{128} \\
& I_4=\frac{\pi \sqrt{2}}{256 \cdot 6} \cdot 3 \cdot 7 \cdot 11=\frac{77 \pi \sqrt{2}}{512}\\& I_{10}=\frac{\pi \sqrt{2}}{4^{10} \cdot 9 !} \prod_{k=1}^{9}(4 k-1)=\frac{15646785 \pi \sqrt{2}}{134217728}
\end{aligned}
$$ checked by WA . My Question: Are there any alternative methods? Your comments and alternative solutions are highly appreciated.","['integration', 'definite-integrals', 'gamma-function', 'calculus', 'beta-function']"
4797476,How to prove that $(1+\frac{1}{n})^{n+\frac{1}{2}}>e$ for all positive integer $n$?,"It has already been proved that $$
\left(1+\dfrac{1}{n}\right)^n<\mathrm{e}<\left(1+\dfrac{1}{n}\right)^{n+1}
$$ The problem is to prove that the geometric average between $\left(1+\dfrac{1}{n}\right)^n$ and $\left(1+\dfrac{1}{n}\right)^{n+1}$ is still bigger than $\mathrm{e}$ : $$
\left(1+\dfrac{1}{n}\right)^{n+\frac{1}{2}}>\mathrm{e}
$$ One possible approach is to prove that $\left(1+\frac{1}{n}\right)^{n+\frac{1}{2}}$ strictly declines with $n$ . Then, consider the limit of $\left(1+\frac{1}{n}\right)^{n+\frac{1}{2}}(n\to\infty)$ equals to $e$ . But how to prove it? I need help.","['limits', 'calculus', 'inequality', 'sequences-and-series']"
4797493,The complement of an algebraic set in $\mathbb{P}^n$ is path-connected,"I'm studying the marvellous book ""Algebraic Curves and Riemann Surfaces"" of Rick Miranda and I've found this problem (III.5.Q pag.103). Actually it's not related to Riemann surfaces, but I think it's interesting anyway. A subset $Z\subseteq  \mathbb{P}^n$ is an algebraic set if there is a set of homogeneous polynomials $\{F_{\alpha} \}$ such that $Z=\{p\in \mathbb{P}^n | F_{\alpha}(p)  = 0$ for every $\alpha\}$ . Here $\mathbb{P}^n$ is the complex projective n-space, i.e. the set of 1-dimensional subspaces of $\mathbb{C}^{n+1}$ . Show that the complement of an algebraic set in $\mathbb{P}^n$ is path-connected. Remarks. Since an algebraic set is an intersection of hypersurfaces, if the proposition above holds for hypersurfaces, a fortiori holds for generic algebraic sets. So, it suffices to prove that the complement of the set of zeroes in $\mathbb{P}^n$ of a homogeneous polynomial $F$ is path-connected. Proof. (asked by ronno) Assume that the proposition holds for hypersurfaces. Let $Z_1$ and $Z_2$ be two hypersurfaces defined by the zeroes sets in $\mathbb{P^n}$ respectively of $F_1$ and $F_2$ homogeneous polynomials. Since $F_1F_2$ is a homogeneous polynomial, its zeroes in $\mathbb{P^n}$ define a hypersurface $Z_3 = Z_1 \cup Z_2$ , so the finite union of hypersurfaces is a hypersurface. Since $\mathbb{P^n} -Z_3 = (\mathbb{P^n}-Z_1) \cap (\mathbb{P^n}-Z_2)$ , the intersection of the complements of two hypersurfaces is path-connected. Therefore $(\mathbb{P^n}-Z_1) \cup (\mathbb{P^n}-Z_2)$ is path-connected. Hence the complement of an algebraic set, being union of sets two-by-two path-connected, is path-connected. The problem is equivalent to say that the open sets in the Zariski topology on $\mathbb{P}^n$ are path-connected . Thanks for any hint or answer.","['path-connected', 'zariski-topology', 'algebraic-geometry', 'general-topology', 'projective-space']"
4797517,Is $p^{-1}\mathcal F = p^!\mathcal F$ if $p: Y \to X$ is a covering map?,"Suppose $p: X \to Y$ is an unramified (possibly infinite) covering map of complex manifolds. The functor $p^!: D^b(Y) \to D^b(X)$ is supposed to be the right-adjoint of $R p_!:D^b(X) \to D^b(Y)$ , where $p_!$ is the push-forward with proper support, i.e. $$p_! \mathcal F(U) = \{s \in \mathcal F(p^{-1}(U)) \,|\, \operatorname{supp}(s) \to U \text{ is proper}\} \subset p_* \mathcal F(U).$$ Is it true that $p^! = p^{-1}$ , where $p^{-1}$ is the inverse image functor? I know that if $p$ is a covering, then $p_!$ is an exact functor, so that there are no higher derived functors. I guess¹ this implies that $p^!$ is right-adjoint to $p_!$ . But the functor $p^{-1}$ is right-adjoint to $p_*$ , and if $p$ is an infinite covering, then $p_! \neq p_*$ . Here is my motivation: In Sheaves on Manifolds , Chapter 8.6, Kashiwara and Schapira consider a situation where one has a holomorphic map $g:X \to \mathbb D \subset \mathbb C$ , and considers a cartesian diagram $$\require{AMScd}
\begin{CD}
X @<\tilde p<< \tilde X^* \\
@VgVV @VVV \\
\mathbb D @<p<< \mathbb H
\end{CD}
$$ where $\mathbb H \subset \mathbb C$ is the upper half-plane, and $p(z) = e^{2\pi i z}$ . So the covering map is $\tilde p: \tilde X^* \to X \setminus g^{-1}(0)$ . On the top of page 351, Kashiwara and Schapira claim exactly $\tilde p^{-1} = \tilde p^!$ . Is that true only in this situation? ¹ I'm still a bit insecure dealing with the derived category. Is that true? Because purely formally, $p^!$ is defined on the derived category, whereas $p_!$ is defined on the category of sheaves. Or does $p^!$ actually give a functor on the abelian category of sheaves if $p_!$ is exact?","['derived-categories', 'complex-geometry', 'algebraic-geometry', 'adjoint-functors']"
4797583,"Roll a fair 6−sided die until a 6 appears.Given that the first 6 occurs before the first 5, find the expected number of times the die was rolled","I am trying to solve this problem but my answer is coming out to be 1.5 where the answer is 3. I even ran a simulation to verify it and its correct. I am not really sure whats wrong in my method and how can I correct it. I considered for getting my first 6 on Nth trials, without any 5's till now, I have probability of $$P(A^N) = \left(\frac{4}{6}\right)^{N-1} \times \frac{1}{6}$$ Now to get the expected number of rolls, $$\sum_{N=1}^{\infty} N \times \left(\frac{4}{6}\right)^{N-1} \times \frac{1}{6}$$ So, If we bring 1/6 outside, and consider x = 4/6 then the above equation is the 1st derivative of x/(1-x) $$\frac{d}{dx} \left( \frac{x}{1 - x} \right) = \frac{1}{{(1 - x)}^2}$$ Putting back the values and multiplying by 1/6 we get 1.5 (9/6). Where am I going wrong?","['conditional-probability', 'probability']"
4797628,Function that is constant on connected components but not locally constant,"The Wikipedia page for locally constant function says that a locally constant function is constant on each connected component, but that the converse only holds if the space is locally connected.
What would be an example of a function that is constant on each connected component but not locally constant? I first thought of the topologist's sine curve, which is not locally connected, but since this is a connected space, such a function would be constant on the entire space and therefore automatically locally constant.",['general-topology']
4797663,"How to solve $\delta''(\vec{k},\tau)+\mathcal{H}(\tau)\delta'(\vec{k},\tau)-\dfrac{3}{2}\Omega_m(\tau)\mathcal{H}^2(\tau)\delta(\vec{k},\tau)=0$","While self-studying a set of notes about Cosmology, I have encountered the following claim: We have the linear growth equation: $$\delta''(\vec{k},\tau)+\mathcal{H}(\tau)\delta'(\vec{k},\tau)-\dfrac{3}{2}\Omega_m(\tau)\mathcal{H}^2(\tau)\delta(\vec{k},\tau)=0 \label{1}\tag{1}$$ We can easily confirm the above differential equation has a growing and a decaying mode solution: $$\delta(\vec{k},\tau)=D_+(\tau)\delta_{+,0}(\vec{k})+D_-(\tau)\delta_{-,0}(\vec{k}) \label{2}\tag{2}$$ where $D_-(\tau)=D_{-,0}H=D_{-,0}\mathcal{H}/a$ . The growing mode solution can then be obtained as: $$D_+(\tau)=D_{+,0}H(\tau)\int^{a(\tau)}_0\dfrac{da'}{\mathcal{H}^3(a')} \label{3}\tag{3}$$ First, I will explain the notation and the relationships between the different variables, that have nothing to do with mathematics and come from the physics background of the question: $t$ and $\tau$ are two different measurements of time, respectively named coordinate time and conformal time. $a=a(t)=a(\tau)$ is a scalar that depends on time. $H=\dfrac{1}{a}\dfrac{da}{dt}$ and $\mathcal{H}=\dfrac{1}{a}\dfrac{da}{d\tau}$ , where $d\tau=\dfrac{dt}{a}$ , and therefore $\mathcal{H}=aH$ . The following equality holds: $$\dfrac{\partial\mathcal{H}}{\partial\tau}=\mathcal{H}^2\bigg(1-\dfrac{3}{2}\Omega_m\bigg)$$ I am completely lost here. First, I don't know how to solve the differential equation itself, and then, it is not clear to me either how to prove that the expression given for $D_+(\tau)$ is a solution. I assume that the notes use a very confusing notation, since I understand that the $a'$ inside the integral is not $da/d\tau$ , but simply a way to rename $a$ to that it is not represented by the same letter than the upper limit of the integral itself. So we would actually have, if I am correct: $$D_+(\tau)=D_{+,0}H(\tau)\int^{a(\tau)}_0\dfrac{da}{\mathcal{H}^3(a)}$$ In order to prove that this is a solution of \eqref{1}, we would need to find its first and second derivatives with respect to $\tau$ . However, when trying to do this by applying Leibniz rule for the derivation of an integral, given in the case of  multiple parameters by: $$\dfrac{\partial F}{\partial\lambda_k}=\int^{b(\lambda)}_{a(\lambda)}\dfrac{\partial f}{\partial\lambda_k}dx+\dfrac{\partial b}{\partial\lambda_k}\cdot f(\lambda,b(\lambda))-\dfrac{\partial a}{\partial\lambda_k}\cdot f(\lambda,a(\lambda))\ \ \ \ \text{where}\ \ \ \ F(\lambda)=\int^{b(\lambda)}_{a(\lambda)}f(\lambda,x)dx$$ I encounter a problem: we don't have here an independent integration variable and parameter, but rather an integration variable $a$ that depends on the parameter $\tau$ . So I have no idea how to proceed. I assume that the reasoning behind \eqref{2} is related to the fact that there are no derivatives with respect to $\vec{k}$ in \eqref{1}, so the solution can be written as a constant that only depends on $\vec{k}$ multiplied by the solution that depends on $\tau$ , but I don't know why we can separate that solution into a growing and a decaying function of $\tau$ . Any help would be greatly appreciated. I've dwelled on this for weeks and can't find an explanation or a way to solve the differential equation. Thank you very much in advance!","['ordinary-differential-equations', 'leibniz-integral-rule']"
4797685,Conditional distribution of sum of squared iid uniforms,"Is it true, that if $U_{1}$ and $U_{2}$ are iid uniform distributed variables on $\left[-1,1\right]$ , then the sum of $U_{1}^{2}$ and $U_{2}^{2}$ is still uniform distributed conditioned on the set that this sum is not greater than $1$ ? Other words: $$X\dot{=}U_{1}^{2}+U_{2}^{2}|U_{1}^{2}+U_{2}^{2}\leq1\sim Uni\left[0,1\right]?$$ I've read this in a book and for first glance this hasn't been clear. I guess we have to compare some kind of “conditional charachteristic” functions to decide this question.","['uniform-distribution', 'conditional-probability', 'probability-distributions', 'probability-theory', 'probability']"
4797702,Is the maximum modulus of an entire function on a circle of radius $r$ a smooth function of $r$?,"Let $f$ be a non-constant holomorphic function on $\mathbb{C},$ for any $r>0,$ set $$M(r)=\max_{|z|=r}|f(z)|$$ Is $M(r)\in C^{\infty}(0,+\infty)$ ? I have showed that $M(r)$ is continuous and strictly increasing, but I have no idea to deal with the derivative of $M(r),$ can someone help me?","['complex-analysis', 'smooth-functions']"
4797708,"Finite roots of $f : [0,1] \longrightarrow \mathbb{R}$ derivable such that not exists $x \in [0,1]$, $f(x) = f'(x) = 0$.","My doubt is about the problem who has already asked here several times: Let $f : [0,1] \longrightarrow \mathbb{R}$ be a differentiable function. If there do not exist any $x \in [0,1]$ such that $f(x) = f′(x) = 0$ , prove that f  has only finite number of zeros in [0,1]. In this question: Prove that $f$ has finite number of roots , the second answer suggests that: First use the Taylor formula $f(x)=f(x_0)+f′(x_0)(x−x_0)+o(x−x_0)$ to show that zeros with nonvanishing derivative must be isolated. This was not clear to me. I would like to check my demonstration and point out possible errors. My proof: Suppose that the set $$Z = \{x \in [0,1] ; f(x) = 0\}$$ is infinite. Let us now suppose that $Z$ is not discrete. Then there exists $x_0 \in Z \cap Z'$ . Then, for every $\varepsilon > 0$ we have that $$[(x_0 - \varepsilon, x_0 + \varepsilon) - x_0] \cap Z \ne \emptyset.$$ Let $h \ne 0$ such that $x_0 + h \in [0,1]$ . Consider then formula of taylor \begin{align*}
f(x_0 + h) & \ = \ f(x_0) + f'(x_0)h + r(h)\\
& \ = \ f'(x_0) + r(h)\\
& \ = \ \bigg[f'(x_0) + \dfrac{r(h)}{h}\bigg]h
\end{align*} Since $x_0$ is an accumulation point, if we take $\varepsilon$ small enough, we will find $h'$ small enough such that $f(x_0 + h') \in Z$ . But this must be absurd because for $h'$ sufficiently small we will have to \begin{align*}
f(x_0 + h') & \ = \ \bigg[\underbrace{f'(x_0)}_{\ne 0} + \underbrace{\dfrac{r(h)}{h}}_{= 0}\bigg]\underbrace{h}_{\ne 0}\\
& \ \ne \ 0.
\end{align*}","['solution-verification', 'derivatives', 'real-analysis']"
4797725,Is $\left(1+\frac1n\right)^{n+1/2}$ decreasing?,"Using the Cauchy-Schwarz Inequality , we have $$
\begin{align}
1
&=\left(\int_n^{n+1}1\,\mathrm{d}x\right)^2\\
&\le\left(\int_n^{n+1}x\,\mathrm{d}x\right)\left(\int_n^{n+1}\frac1x\,\mathrm{d}x\right)\\
&=\left(n+\frac12\right)\log\left(1+\frac1n\right)
\end{align}
$$ which means that $$
\left(1+\frac1n\right)^{n+1/2}\ge e
$$ This hints that $\left(1+\frac1n\right)^{n+1/2}$ might be decreasing. In this answer , it is shown that $\left(1+\frac1n\right)^n$ is increasing and $\left(1+\frac1n\right)^{n+1}$ is decreasing. The proofs use Bernoulli's Inequality . However, applying Bernoulli to $\left(1+\frac1n\right)^{n+1/2}$ is inconclusive. Attempt to show decrease: $$
\begin{align}
\frac{\left(1+\frac1{n-1}\right)^{2n-1}}{\left(1+\frac1n\right)^{2n+1}}
&=\left(1+\frac1{n^2-1}\right)^{2n}\frac{n-1}{n+1}\\
&\ge\left(1+\frac{2n}{n^2-1}\right)\frac{n-1}{n+1}\\[6pt]
&=1-\frac{2}{(n+1)^2}
\end{align}
$$ Attempt to show increase: $$
\begin{align}
\frac{\left(1+\frac1n\right)^{2n+1}}{\left(1+\frac1{n-1}\right)^{2n-1}}
&=\left(1-\frac1{n^2}\right)^{2n}\frac{n+1}{n-1}\\
&\ge\left(1-\frac2n\right)\frac{n+1}{n-1}\\[6pt]
&=1-\frac{2}{n(n-1)}
\end{align}
$$ Neither works. Without resorting to derivatives, is there something stronger than Bernoulli, but similarly elementary, that might be used to show that $\left(1+\frac1n\right)^{n+1/2}$ decreases?","['inequality', 'cauchy-schwarz-inequality', 'exponential-function', 'eulers-number-e']"
4797788,Probability that mean is larger than median,"Let $X_1,\ldots,X_n$ be i.i.d random variables taking values in $\mathbb{R}$ .
Suppose that $n$ is odd and the $X_i$ follow a continuous distribution.
I am interested in the probability that the mean of these random variables is larger than their median, i.e. $$
\mathbb{P}\left[
\frac{1}{n}\sum_{i=1}^nX_i>X_{\left(\frac{n+1}{2}\right)}
\right]
$$ My guess is that in general, there is no nice formula for this probability.
But I wonder:
Are there (easy or known) special cases, where there is a (nice) formula?","['order-statistics', 'statistics', 'combinatorics', 'probability']"
4797798,does $1^{z} = 2 $ have any solutions?,"In this video , the author claims that the equation in question has complex solutions $z=-\frac{i\ln(2)}{2\pi n}, n\in \mathbb{Z}, n\neq 0$ Obtained by considering $1$ outside the principal argument (eg: using $2n\pi$ as it’s argument) and using a non principal complex logarithm. Im not fully convinced that this is an valid way of solving the equation, my understanding is only the principal branch is consistent with real basis exponentiation (which is what the equation uses). [In fact, wolfram alpha says there is no solution] Can someone elaborate on wether the video is correct or not? Preferably including the why or why not. Thanks,
-Almeida ps: Im in highschool, so please avoid using too much advanced math, my understanding of complex analysis is VERY limited.","['complex-analysis', 'complex-numbers']"
4797802,Searching for Functions Exhibiting Semigroup Property for Energy-Efficient Neuronal Modeling,"I am working on designing energy-efficient neurons for neuromorphic computing. One of the critical aspects of the dynamics I'm exploring is that they should adhere to the semigroup property. Specifically, given a function $f$ with state $x$ and time $t$ , it should satisfy: $ f(x, t_1 + t_2) = f(f(x, t_1), t_2) $ This property is vital as, during training, I am updating the dynamics in equally-sized small timesteps, while during inference, the evolution of the state $x$ over many timesteps should be condensable into a single, cheap function evaluation. The semigroup property ensures consistent behavior between these stages. It's worth noting that my primary concern is not biological plausibility. Instead, I aim to discover and design energy-efficient neural models. To this end: I am interested in classes of functions (potentially vector-valued) that inherently satisfy the semigroup property, which I can explore through grid search or genetic algorithms to find well-performing candidates. I am aware of matrix exponentiation, which offers a class of functions/differential equations that inherently possess the semigroup property. However, I am curious if there are additional classes of functions or mathematical models that exhibit this property. Are there methods or techniques to construct functions (or systems of functions for multi-dimensional states) that adhere to this property? Given the context of energy-efficient neural dynamics, are there any known mathematical constructs or approximations that might be suitable? Any insights, references, or pointers on this topic would be highly appreciated.","['semigroup-of-operators', 'neural-networks', 'ordinary-differential-equations']"
4797850,Find any two consecutive $100$-digit numbers whose sum of digits is a perfect square,Find any two consecutive $100$ -digit numbers whose sum of digits is a perfect square. this is all I did and I am stuck lets say $abc \dots m$ is a $100$ digit number whose sum of its digits is a perfect square $$a+b+c+.....+m = k^2$$ lets say $abc \dots m+1$ is a $100$ digit number whose sum of its digits is a perfect square $$a+b+c+.....+m+1 = q^2$$ $$k^2 = q^2 - 1$$ and I am just stuck,['algebra-precalculus']
4797868,Friedrich's second inequality for functions with zero mean value,"Friedrich's second inequality (or Maxwell Estimate or Gaffney’s inequality in the literature) is referred as follows: For all $\mathbf{u} \in H^1(\Omega)^2$ satisfying either $\mathbf{n} \cdot \mathbf{u}=0$ or $\mathbf{n} \times \mathbf{u}=\mathbf{0}$ on $\partial \Omega$ where $\Omega$ is a simply connected domain with Lipchitz boundary, then $$
\|\mathbf{u}\|_{L^2(\Omega)} \leq C_1\left(\|\nabla \cdot \mathbf{u}\|_{L^2(\Omega)}+\|\nabla \times \mathbf{u}\|_{L^2(\Omega)}\right).
$$ My question is that if the boundary condition is satisfied, we only know the mean value of $\mathbf{u}$ is $0$ or simply replace the LHS with $\|\mathbf{u}-\frac{1}{|\Omega|}\int_{\Omega}\mathbf{u}\|_{L^2(\Omega)}$ . Does the inequality above hold? Or at least, particularly, if $\mathbf{u}$ is divergence-free, do we have $$
\|\mathbf{u}-\frac{1}{|\Omega|}\int_{\Omega}\mathbf{u}\|_{L^2(\Omega)} \leq C_1\|\nabla \times \mathbf{u}\|_{L^2(\Omega)}.
$$ Thank you in advance!","['inequality', 'sobolev-spaces', 'functional-analysis']"
4797917,"Show that there is $c \in (-1,2)$ such that $f'(c)=0$.","$f:\mathbb{R} \rightarrow \mathbb{R}$ differentible function and $F:\mathbb{R} \rightarrow \mathbb{R}$ a primitive of $f$ . We know $5F(xy^{2}-y)+F(x^{2}y-x)-F(x^{3}-y)F(2xy-2)\geq 9$ for every $x,y \in \mathbb{R}$ . Show that there is $c \in (-1,2)$ such that $f'(c)=0$ .
I think that inequality somehow is related to a function that is positive and somehow we need to use Rolle's Thoerem with that function.","['functional-equations', 'derivatives']"
4797941,Expected Value Via ODE,"I recently learned about a technique to use ODEs to find the mathematical expectation. I decided to apply it to the following problem: Let $X_1$ , $X_2$ , $\ldots\simeq$ Uniform $(0,1)$ iid. Let $N$ be the first index $n$ where $X_n\neq \max\{X_1,X_2,\ldots,X_n\}$ . Find $E[N]$ . To solve, we generalize the problem by defining a function $m(z)$ . The function $m(z)$ is the expected number of uniform variables needed so that the monotonic condition no longer holds when initially starting at $z$ . Specifically, it is the average of the first index $n$ such that $X_n<X_{n-1}\geq\ldots X_1\geq z$ . Then the total law of expectation - where we condition on $X_1$ - yields $$m(x) = \int^1_x(1+m(z)) \mathrm{d}z$$ For now, let us assume that $m(x)$ is differentiable. Then differentiating both sides yields $$m'(x)=-(1+m(x))$$ or $$m'(x)+m(x)=-1$$ This is a linear ODE with general solution $m(x)=ce^{-x}-1$ . Plugging this equation back into the integral shows that $$\int^1_z(1+m(z))\;\mathrm{d}z=-ce^{-1}+ce^{-z}=ce^{-x}+1$$ or $c=e$ . All the remains to do is to evaluate at $x=0$ : $$m(0)=e^{1-0}-1=e-1$$ However, solving it through alternative means, I obtain $e$ . What went wrong?","['expected-value', 'conditional-expectation', 'ordinary-differential-equations']"
4797943,Finiteness of the Measure Space appearing in Spectral Theorem of Bounded Operators,"Consider the multiplication operator version of the spectral theorem for bounded operators in a Hilbert space. Quoted from Wikipedia: Let $A$ be a bounded self-adjoint operator on a Hilbert space $H$ .  Then there is a measure space $(X, \Sigma, \mu)$ and a real-valued essentially bounded measurable function $f$ on $X$ and a unitary operator $U:H \to L^2(X, \mu)$ such that $U^\dagger T_f U = A$ where $T_f$ is the multiplication operator: $T_f [\varphi](x) = f(x) \varphi(x)$ and $\|T_f\| = \|f\|_\infty$ . It is not much work to find an example where this measure space is not unique. For example, one can look at the sum of the left shift and right shift operators in $L^2(\mathbb{Z})$ and use Fourier series to see that the space $X$ can be any real interval. My question is, using the boundedness of $A$ , can we always find a measure space with finite measure satisfying this theorem? In particular, can we always find one such that $\mu(X)=1$ ?","['measure-theory', 'spectral-theory', 'functional-analysis']"
4797967,Show there exists such $x\in F^n$ for some $A\in M_{m\times n}(F)$ such $Ax$ has no zero entry,"$M_{m\times n}(F)$ is the set of all ${m\times n}$ matrices over field $F$ . Now, assume $A\in M_{m\times n}(F)$ and $A$ has no zero-rows (rows that all their elements are zero). Also $F$ is an infinite field. Show that there exists an $x\in F^n$ ( ${n\times1}$ vector) such that none of entries of $Ax$ is zero. I know about these: elementary operations row-reduced echelon forms linear dependence or independence equivalent matrices matrix block multiplication Assume $A=[a_{ij}]$ . We know the conditions of problem are satisfied when: $$\begin{bmatrix}
a_{11}&a_{12}&\dots&a_{1n}\\
a_{21}&a_{22}&\dots&a_{2n}\\
\vdots&\vdots&\ddots&\vdots\\
a_{m1}&a_{m2}&\dots&a_{mn}\\
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
\vdots\\
x_n
\end{bmatrix}=\begin{bmatrix}
u_1\\
u_2\\
\vdots\\
u_m
\end{bmatrix}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(u_i\neq0\text{ for } 1\leq i\leq m)$$ So we should find a vector $x$ that all equations like this are not zero: $$\sum_{j=1}^n{a_{ij}}x_j\neq 0\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, (0\leq i\leq m)$$ I tried assuming that there's no matrix like this then reach a contradiction, but I couldn't. I also tried to construct the vector $x$ and use linear dependency, but this also didn't give me any thing good. Any help is so much appreciated!","['matrices', 'linear-algebra']"
4798144,A tight positivity conjecture about sums over divisors of square-free integers.,"Let $p_n$ be the $n$ th prime number and all variables, unless otherwise specified, are natural numbers. Conjecture: For all square-free $n \geq 2$ , the following function evaluates to a positive integer: $$F(n) = -1 + \sum_{d \mid n} \mu(d) \sum_{a^2 = 1\mod d, \ \\ 0 \leq a \lt d}\left\lfloor\frac{\frac{12}{5} p_{\text{max}}(n)-a}{d} \right\rfloor$$ where $p_{\text{max}}(n)$ is the maximum prime divisor of $n$ .
And this conjecture is tight with respect to the constant $C = \frac{12}{5}$ .  Any deviation to a lower value will yield a false conjecture. Verification Code: from sympy import *

N = 5000
epsilon =  0.00000000    # Try setting to 0.000000001

for n in range(2, N):
   if all(j == 1 for j in factorint(n).values()):  # n is square-free
      m = ((12 / 5) - epsilon)* max(primefactors(n))
      S = -1
      for d in divisors(n):
         for a in range(0, d):
            if (a ** 2 - 1) % d == 0:
               S += (-1) ** primeomega(d) * floor((m - a)/d)         
      assert S > 0
      print(m,n, S) What is interesting is that $\{ \bar{a} \in \Bbb{Z}/d : a^2 = 1\mod d\} \leqslant (\Bbb{Z}/d)^{\times}$ is a subgroup of the units modulo $d$ and you can't change that either unless you adjust $C$ . Anyway, was wanting some information about sums of this type.  There's not much data on the web that I could find about strictly sums over divisors of square-free integers.  When you remove the square-free requirement, the summation goes chaotic, i.e. takes on negative or positive values.  Are there any good papers out there about this? Attempt. This doesn't attempt to answer the question but instead partially prove the result for when $n = q$ is a prime number.  Let $[z]_q =$ the usual (least, non-negative) residue of $z$ modulo $q$ , for any $z \in \Bbb{Q}$ . $$\begin{align}
F(q) &= -1 + \sum_{d \in \{1,q\}}\mu(d) \sum_{a^2 = 1 \pmod d} \lfloor\frac{\frac{12}{5} q - a}{d} \rfloor \\
&= -1 + \frac{12}{5}q -  \lfloor\frac{\frac{12}{5} q - 1}{q} \rfloor - \lfloor\frac{\frac{12}{5} q - (q-1)}{q} \rfloor \\
&= -1 + \frac{12}{5}q - \frac{\frac{12}{5}q - 1 - [\frac{12}{5}q - 1]_q}{q} - \frac{\frac{12}{5}q - (q-1) - [\frac{12}{5}q - (q-1)]_q}{q} \\
&= -1 + \frac{12}{5} q - \frac{12}{5} +\frac{1}{q} +\frac{[\frac{12}{5}q - 1]_q}{q} -\frac{12}{5} +1 -\frac{1}{q} + \frac{[\frac{12}{5}q + 1]_q}{q} \\
&= \frac{12}{5}q - \frac{24}{5} + \frac{[\frac{12}{5} q - 1]_q}{q} + \frac{[\frac{12}{5} q + 1]_q}{q}
\end{align}$$ Now plug in $q = 2$ .  We have: $$
\frac{24}{5} - \frac{24}{5} + \text{ two numbers that can't both be zero} \gt 0
$$ since the function can only (clearly) take on integer values, we have that $F(2) \geq 1$ .  By induction, for larger primes, the value is clearly also $\geq 1$ . $\blacksquare$ What remains to be proven is that it's true for $F(n)$ with $\Omega(n) \gt 1$ , $n$ square-free.","['modular-arithmetic', 'abstract-algebra', 'squarefree-numbers', 'group-theory', 'alternating-expression']"
4798201,"For what $\lambda \in \mathbb{R}$ is this a spanning set (Erzeugendensystem) of the vector space $\mathbb{R}^3$ ? {(1, 3, 4), (1, λ, 5), (1, 4, 3)}","The set forms a spanning set (German: Erzeugendensystem) if the matrix is linearly independent, which is true if the determinant is non-zero. We have $$1 \cdot (\lambda \cdot 3 - 4 \cdot 5) - 1 \cdot (3 \cdot 3 - 4 \cdot 4) + 1 \cdot (3 \cdot 5 - \lambda \cdot 4) = 3\lambda - 20 - 9 + 16 + 15 - 4\lambda = -\lambda + 2 \neq 0 \implies \lambda \neq 2$$ So we have $\lambda \in \mathbb{R}$ \ { $2$ } Is this correct ? The course I take is in German, but it is not my mother tongue. Is ""spanning set of vector space"" an accurate translation for ""Erzeugendensystem"" ?","['systems-of-equations', 'vectors', 'vector-spaces', 'matrices', 'linear-algebra']"
4798213,Proving a collection is a set in ZF,"Suppose $C$ is a set and consider $$
F:= \{ x : x \cup C = C \} 
$$ Is it true that $F$ is a set? I say yes . My attempt: Define $G:= \{x \in P(C) : x \cup C = C \}$ where $P(C)$ is the power set of $C$ . (It is a set by the axiom of the power set, since $C$ is a set). By the axiom of specification the latter is a set. Notice that: $$
F = G
$$ By extension $F$ and $G$ are equal. Now some questions: It is not explicitly stated, but is every $x$ in $F$ a set? I guess the answer is yes, because under ZF union makes sense only for sets (considering pairing and union axioms) The axiom of extension tells us that, provided $A$ and $B$ are sets, they are equal iff their elements are equal. Then is it correct to say that $F$ and $G$ are equal? Technically I hadn't proved $F$ is a set.","['elementary-set-theory', 'first-order-logic', 'axioms', 'set-theory']"
4798273,Proving a result related to square prism,"question In the regular quadrilateral prism $ABCDA'B'C'D'$ we denote by $M$ the intersection of the lines $A'C$ and $D'B$ . If $AB>A'A$ , prove that $\dfrac{\sqrt{2}}{2}<\sin(\angle MAD)<\dfrac{\sqrt{2}}{\sqrt{3}}$ drawing my idea I'm quite new to these geometries in space problems so I wonder if $M$ is the centre of a regular quadrilateral prism and $A, M, C'$ are colinear too. We can show that $ABC'D''$ is a rectangle. If we get a new point called $F$ , that is in the middle of AB as
@Joshua Finlayson said in comments. The thing is I don't know how to calculate each side to express them between $\frac{\sqrt{2}}{2}$ and $\frac{\sqrt{2}}{\sqrt{3}}$ I hope one of you can help me! Thank you!",['trigonometry']
4798282,"If $0<A\le x_n\le B~~ (\forall n)$, and $\lim_{n\to\infty}\sqrt[n]{x_1x_2\cdots x_n}=A$. Prove $\lim_{n\to\infty}\frac{x_1+x_2+\cdots+x_n}{n}=A.$","Problem Assume $ 0<A \le x_n\le B~~ (\forall n)$ , and $\displaystyle
\lim_{n \to \infty}\sqrt[n]{x_1x_2\cdots x_n}=A$ . Prove $$\displaystyle\lim_{n \to \infty}\frac{x_1+x_2+\cdots+x_n}{n}=A.$$ Is it true or not ? Probably it holds, but seems to be difficult to prove. If we consider using the squeezing theorem, then $$\sqrt[n]{x_1x_2\cdots x_n}\le \frac{x_1+x_2+\cdots+x_n}{n},$$ but how to estimate the upper bound?","['limits', 'calculus', 'real-analysis']"
4798298,Can a matrix be orthogonal without being orthonormal? [duplicate],"This question already has answers here : orthogonal vs orthonormal matrices - what are simplest possible definitions and examples of each ?? (1 answer) Orthogonal and Orthonormal Matrix (1 answer) Difference between orthogonal and orthonormal matrices (2 answers) Closed 8 months ago . I know a set of orthogonal vectors is a set where vectors have a null dot product pairwise, whatever their norms, and a set of orthonormal vectors is a set of orthogonal vectors where all vectors have a norm equal to 1. Now I'm trying to dig into SVD decomposition $\small X=U \Sigma V^T$ introduced by San José University and read about U and V: there exist two orthogonal matrices So I'm trying to understand what are orthogonal matrices. Wikipedia says: In linear algebra, an orthogonal matrix, or orthonormal matrix, is a real square matrix whose columns and rows are orthonormal vectors So Wikipedia implies orthogonal matrix is synonymous for orthonormal matrix , and a matrix cannot be orthogonal if a row or a column is a vector with a norm different of 1. This site says: For matrices, an orthogonal matrix has orthogonal rows and columns. This means that the dot product of any two rows or columns is zero An orthonormal matrix, on the other hand, not only has orthogonal rows and columns but also has orthonormal rows and columns. This means that each row and column is a unit vector On the other hand this answer here states: There is no thing as an ""orthonormal"" matrix I'm confused about the difference between orthogonal and orthonormal matrices. Can this be clarified: Is there a definition for orthogonal matrix and/or orthonormal matrix ? Can a matrix be orthogonal without being orthonormal? Does that depend on whether the matrix is square or not? Please focus on these three questions, not adding further confusion. Related, but not helpful: Difference between orthogonal and orthonormal matrices","['matrices', 'definition', 'linear-algebra']"
4798384,An exercise in analysis about normed vector spaces,"I have found the following exercise : Let $E$ be a normed vector space and $x,y\in E$ be non-zero vectors. Show that $\left\Vert x-y\right\Vert \geq \frac{1}{4}\left( \left\Vert
x\right\Vert +\left\Vert y\right\Vert \right) \left\Vert \frac{x}{\left\Vert
x\right\Vert }-\frac{y}{\left\Vert y\right\Vert }\right\Vert $ Show that the constant $\frac{1}{4}$ can not be replaced with a
greater one. I was able to prove the above inequality but concerning the constant $\frac{1%
}{4}$ I have shawn that it can be replaced with $\frac{1}{2}$ in case of
Hilbert spaces. So my question is : is there an example of a normed verctor space where the
constant $\frac{1}{4}$ can not be improved ? Thank you !","['normed-spaces', 'vector-spaces', 'analysis', 'inequality', 'problem-solving']"
4798436,If $f((\text{deg}(f) + 1$) consecutive $\mathbb{Z}\text{s}) \subset \mathbb{Z}$ then $f(\mathbb{Z}) \subseteq \mathbb{Z}$,"Question: Let $f:\mathbb{R}\to\mathbb{R}$ be a polynomial of degree $n$ . Suppose there exists an integer $m\in\mathbb{Z}$ such that $f(m)\in\mathbb{Z}$ , $f(m+1)\in\mathbb{Z}$ , ..., $f(m+n)\in\mathbb{Z}$ (i.e. there exists $n+1$ consecutive integers all satisfying $f(x)\in\mathbb{Z}$ ). Prove that $\forall x\in\mathbb{Z}, f(x)\in\mathbb{Z}$ . Context: This is a problem I thought of on my own when I was preparing for a contest next month. This problem at first glance ""felt"" like one of those classic and old problem with lots of solutions and thoughts around it. Surprisingly, I could find neither solutions nor questions regarding the same problem. After spending a while on the problem, I came up with a finite difference solution. The solution is as follows: My solution: Let $P(n)$ denote the statement we want to prove for a function of degree $n$ . We shall prove that $P(n)$ is true for all $n$ by induction. Base case: When $n=0$ , $f(x)=c$ where $c$ is a constant. Since $f(m)\in\mathbb{Z}$ , we have $c\in\mathbb{Z}$ , which means that $\forall x\in\mathbb{Z}, f(x)=c\in\mathbb{Z}$ . Hence, $P(0)$ is true. Induction hypothesis: $P(k)$ is true where $k\in\mathbb{Z}^+$ . Inductive step: We claim that $P(k+1)$ is true. We have the condition that there exists an integer $m\in\mathbb{Z}$ such that $f(m)\in\mathbb{Z}$ , $f(m+1)\in\mathbb{Z}$ , ..., $f(m+k+1)\in\mathbb{Z}$ . Consider a function $g(x)=f(x)-f(x-1)$ (i.e. first finite difference). Since $f(x)$ has degree $k+1$ , we have that $g(x)$ has degree $k$ . From the conditions given we also know that $$g(m+1)=f(m+1)-f(m)\in\mathbb{Z} \\ g(m+2)=f(m+2)-f(m+1)\in\mathbb{Z} \\ \dots \\ g(m+k+1)=f(m+k+1)-f(m+k)\in\mathbb{Z}$$ Hence, we have $k+1$ consecutive integers all satisfying $g(x)\in\mathbb{Z}$ . By the induction hypothesis, this means that $\forall x\in\mathbb{Z}, g(x)\in\mathbb{Z}$ . Then, we have $$f(m+k+2)=f(m+k+1)+g(m+k+2)\in\mathbb{Z} \\ f(m+k+3)=f(m+k+2)+g(m+k+3)\in\mathbb{Z} \\ \dots$$ and $$f(m-1)=f(m)-g(m)\in\mathbb{Z} \\ f(m-2)=f(m-1)-g(m-1)\in\mathbb{Z} \\ \dots$$ It can now be easily proven by induction that $\forall x\in\mathbb{Z}, f(x)\in\mathbb{Z}$ . This completes the induction, $Q.E.D.$ First I'd like to ask you to verify if my proof is correct (and the spaces for improvements). Second, I'd also like to ask for more insightful and/or elegant proofs (or any generalisation from this). Any help is appreciated!","['contest-math', 'linear-algebra', 'polynomials', 'finite-differences']"
4798523,An inequality involves sum of products of consecutive numbers,"Does the following inequality hold for all $N\geq2$ ? \begin{equation*}\frac{\displaystyle\sum_{i=1}^{N-1}i\cdot(N-i-1)!\sum_{k=1}^{N-i}\prod_{j=k}^{k+i}j}{N(N-1)N!}<1.\end{equation*} I have tried some numerical examples. It seems they all checked out. If so, a proof (or any hint) will be highly appreciated.","['inequality', 'factorial', 'discrete-mathematics']"
4798536,Proof by contradiction using properties of normal subgroup: A group is cyclic if $x^m=e$ has at most $m$ solutions.,"Statement : Let $G$ be a finite group. Show that if for each positive integer $m$ the number of solutions $x$ of the equation $x^n=e$ in $G$ is at most $n$ , then $G$ is cyclic. There are various proofs available as answers to this post . Inspired by this answer (which I did not find convincing), I am attempting to write a proof on my own ( point out errors or fallacies, if any ). My attempt : Define : $\mathcal S(n) := \{ g\in G \ | \ g^n=e\}$ Given : $G$ is finite and $ |\mathcal S(n)|\leq n$ for all $n\in \mathbb Z^+$ . Assumption : Suppose, if possible, $G$ be non-cyclic. $G$ is finite $\implies$ G has at least one element (say, $x$ ) of maximal order $M$ . $M=\max\{\DeclareMathOperator{\ord}{ord}\ord(g) \ | \ g\in G\}\tag*{}$ $G$ being non-cyclic, $M\neq |G|$ , for otherwise, $x$ will generate $G$ . $G$ is non-cyclic $\implies$ $G\setminus \langle x \rangle$ is non-empty, so it has at least one element (say, $y$ ) of minmal order $m$ . $m=\min\{\ord(g) \ | \ g\in G\setminus \langle x \rangle \}\tag*{}$ Claim 1 : $\mathcal S(M) =\langle x\rangle$ and $\mathcal S(m) =\langle y\rangle$ This can be proven by a counting argument.
All elements of $\langle x\rangle$ satisfy the equation $g^M=e$ so, $\langle x\rangle\subseteq \mathcal S(M)$ . Now, use that fact that $|\langle x\rangle|= \ord(x)=M$ and that $|\mathcal S(M)|\leq M$ . Claim 2 : $\langle x\rangle$ and $\langle y \rangle$ are normal in $G$ .
For all $g\in G$ and $h\in \langle x\rangle $ , $(ghg^{-1})^M=gh^Mg^{-1}=e\implies ghg^{-1}\in \mathcal S(M)=\langle x\rangle\tag*{}$ Repeat the same argument for $\langle y\rangle$ . Lemma : If $H$ and $K$ are normal subgroups of any group $G$ such that $H\cap K=\{e\}$ then the elements of $H$ and $K$ commute with each other. For all $h\in H$ and $k\in K$ , $hkh^{-1}{k^{-1}}=\underbrace{(hkh^{-1})}_{\in K}k^{-1}=h\underbrace{(khk^{-1})^{-1}}_{\in H}\tag*{}$ By normality and closure, $hkh^{-1}k^{-1}\in H\cap K$ so, $hkh^{-1}k^{-1}=e$ . Now  with these claims and lemmas at hand, we are equipped to proceed. There are three possible cases: Case 1 : $\DeclareMathOperator{\gcd}{gcd}\gcd(M,m)=1$ . This would mean that $S(M)\cap S(m)=\{e\}$ and hence, $\langle x\rangle \cap \langle y\rangle=\{e\}$ . Now, claim 2 followed by the lemma, we know that $xy=yx$ . $\therefore$ $\ord(xy)=\DeclareMathOperator{\lcm}{lcm}\lcm(M,m)=Mm\leq M$ ( $\because$ $M$ is the maximal order of $G$ ) $\iff m=1$ . Now, $\ord(y)=1\implies y=e \in \langle x\rangle$ . This is a contradiction because $y$ was chosen such that $y\in G\setminus \langle x\rangle$ . We can conclude that in this case, $G\setminus \langle x\rangle$ is empty, so $G=\langle x\rangle$ as required. Case 2 : $1<\gcd(M,m)<m$ Suppose $\text{gcd}(M,m)=d$ for some $1<d<m$ . $\quad$ Claim 2.1 : $y^d\not \in \langle x\rangle$ . $\ord(y^d)=m/d$ . However, as a consequence of Lagrange's theorem, $\langle x\rangle $ cannot not have any element of this order because $m/d$ does not divide $M$ . In fact, $\gcd(M, m/d)=1$ because we have removed the highest possible common factor from $m$ . By the requirement that $m$ is the minimal order in $G\setminus \langle x\rangle$ : $m=\min\left(m, \ord(y^d) \right)\implies m\leq \frac{m}d \implies d=1$ which is the case 1. Case 3 : $\gcd(M,m)=m$ i.e., $m$ divides $M$ . $\ord(y)=m, \ m\ | \ M \implies y^M=e \implies y\in\mathcal S(M)=\langle x\rangle\tag*{}$ This is again a contradiction because of choice of $y\in G\setminus \langle x\rangle$ . This completes the proof. $\blacksquare$ What I did not find convincing in the linked answer : In the linked answer, the case 2 was $d=\gcd(m,M)>1$ and the element $y^{m/d}$ was picked from $\langle y\rangle$ which is of order $d$ . Then it was argued that $m$ is the minimal order so, $d\leq m$ and hence, $d=m$ . But $m$ is the minimal order of elements of $G\setminus \langle 
x\rangle$ . How are we guaranteed that $y^{m/d}\not \in \langle x\rangle$ ? I think it is possible that $y^{m/d}\in \langle x\rangle$ so it can have an order less than $m$ . Am I missing out on something or is this really a flaw in the linked proof ? Update : Unfortunately, I found a mistake...The argument for claim 2.1 is false. $m/d$ may divide $M$ and it's not necessary that $\text{gcd}(m/d, M)=1$ . For instance, $\text{gcd}(4,6)=2$ but $4/2$ does divide $6$ .  So there needs to be some way to rewrite the proof for case 2.","['proof-explanation', 'group-theory', 'solution-verification', 'cyclic-groups']"
4798557,"Find the value of the integral: $\int_{0}^{1}\frac{x}{1+x^4}\arctan(x)\,\mathrm{d}x$","Prove that: $$\int_{0}^{1}\dfrac{x}{1+x^4}\,\arctan(x)\,\mathrm{d}x\,=\,\left(\dfrac{\pi^2}{16} -\dfrac{\ln^2{(\sqrt{2}+1)}}{4}\right)\,\approx\,0.211322\dots$$ Also prove that: $$\int_{0}^{1}\dfrac{x^3}{1+x^4}\,\text{artanh}(x)\,\mathrm{d}x\,=\,\left(\dfrac{\pi^2}{16} -\dfrac{\ln^2{(\sqrt{2}+1)}}{4}\right)\,\approx\, 0.211322\dots\,=\,\int_{0}^{1}\dfrac{x}{1+x^4}\arctan(x) \,\mathrm{d}x$$ Notice that $\left(\dfrac{\pi^2}{16} -\dfrac{\ln^2{(\sqrt{2}+1)}}{4}\right)\Longrightarrow$ $\frac{1}{2}\,\operatorname{Re}(\arctan^2(\sqrt{\mathrm{i}}))$ I used a very long and probably wrong method, following the technique used in one of my previous posts: The definite Integral $I=\int_{0}^{1}\frac{x\cdot \operatorname{artanh} x}{1+x^2} dx$ Basically one can recreate this integral by replacing $x=\sqrt{\mathrm{i}}$ in the $\arctan^2(x)$ expansion and equating the real part to the corresponding infinite G.P generated paralelly... (I’m sorry for being so vague, I’ve been off my medications for a week) I would greatly appreciate any alternate derivations :) My derivation: Consider: $$\arctan(\sqrt{i})=\sqrt{i}-\dfrac{\sqrt{i}^3}{3}+\dfrac{\sqrt{i}^5}{5}-\dfrac{\sqrt{i}^7}{7}+\cdots=\sqrt{i}\left(\left(\dfrac{1}{1}-\dfrac{1}{5}+\dfrac{1}{9}-\cdots\right)-\left(\dfrac{i}{3}-\dfrac{i}{7}+\dfrac{i}{11}-\cdots\right)\right)$$ WKT: $\arctan(\sqrt{i})=\dfrac{\pi}{4}+i\dfrac{\ln{(\sqrt{2}+1})}{2}$ and $\sqrt{i}=\dfrac{i+1}{\sqrt{2}}$ From these identities it is easy to extrapolate the identities: $$\dfrac{\pi}{4\sqrt{2}}+\dfrac{\ln(\sqrt{2}+1)}{2\sqrt{2}}=\dfrac{1}{1}-\dfrac{1}{5}+\dfrac{1}{9}-\dfrac{1}{13}+\cdots$$ And: $$\dfrac{\pi}{4\sqrt{2}}-\dfrac{\ln(\sqrt{2}+1)}{2\sqrt{2}}=\dfrac{1}{3}-\dfrac{1}{7}+\dfrac{1}{11}-\dfrac{1}{15}+\cdots$$ Multiplying theses two gives $$\dfrac{\pi^2}{16}-\dfrac{\ln^2{(\sqrt{2}+1)}}{4}=\dfrac{1}{1\cdot3}-\dfrac{1}{1\cdot7}+\cdots-\dfrac{1}{5\cdot3}+\dfrac{1}{5\cdot7}-\cdots$$ This sequence can be alternatively derived from: $$\int_{0}^{1}{\sum_{n=1}^{\infty}(-1)^{n+1}x^{4n+1}\arctan(x) dx}=\int_{0}^{1}\dfrac{x}{1+x^4}\arctan(x) dx$$ Therefore, $$\int_{0}^{1}\dfrac{x}{1+x^4}\,\arctan(x)\,\mathrm{d}x\,=\,\left(\dfrac{\pi^2}{16} -\dfrac{\ln^2{(\sqrt{2}+1)}}{4}\right)$$ I might have made a few mistakes somewhere... Also I would like to know if these kinds of integrals have a particular name.","['integration', 'calculus', 'definite-integrals']"
4798584,Is the set of continuous functions a Borel measurable subset of $L^2$?,"Let $C([0,1])$ be the set of continuous real-valued functions on $[0,1]$ and $L^2([0,1])$ the Hilbert space of (equivalence classes) of square-integrable realvalued functions on $[0,1]$ . Then $C([0,1])$ can be identified with a subset of $L^2([0,1])$ . I am wondering if $C([0,1])$ is in the Borel sigma-algebra of $L^2([0,1])$ . (It is well-known that $C([0,1])$ is not an element of the product sigma-algebra of the product space $\mathbb R^{[0,1]}$ , because the latter only sees a countable set of values of a function).","['stochastic-processes', 'measure-theory', 'functional-analysis', 'stochastic-calculus']"
4798596,Double integral over the hypocycloid,"A 3-cusped hypocycloid has the form and defined by he parametric equations: $$
x =  2 \cos \phi - \cos(2 \phi),\\
y =  2 \sin \phi + \sin(2 \phi),\\
\phi \in [0,2\pi].
$$ Question: Suppose I want to find a double integral over the area enclosed by the curve. Is there a change of variables that transforms the 3-cusped hypocycloid  bounded area  into a more convenient area, for example, into a square?","['integration', 'multivariable-calculus']"
4798607,Generating a group of transformations from vector fields,"Consider i) Find the vector fields $V_1, V_2, V_3$ which generate the following smooth one-parameter groups of transformations of $\mathbb{R}$ : $$
x \mapsto \psi_1^s x=x+s, \quad x \mapsto \psi_2^s x=e^s x, \quad x \mapsto \psi_3^s x=\frac{x}{1-s x} .
$$ ii) Deduce that these vector fields generate a group of transformations of the form $$
x \mapsto \frac{a x+b}{c x+d}, \quad a d-b c=1 .
$$ I don't understand what is meant by a group of transformations generated by the vector fields. I don't understand how we generate elements from these. Could someone elaborate on this?","['differential-geometry', 'lie-algebras', 'ordinary-differential-equations', 'lie-groups', 'dynamical-systems']"
4798641,Showing $\frac{[\sin y \ (x+\frac{1}{x}-2\cos y)]^2}{x^2+6} \leq 1$ for $x>1$,"I was trying to solve Ex 21 Chapter 5 from Stevenhagen, Number Theory and I came up into this trigonometric inequality which would be: $$\frac{[\sin y \ (x+\frac{1}{x}-2\cos y)]^2}{x^2+6} \leq 1
 \quad \text{ for } x>1$$ I tried to maximize the multivariate function but couldn't conclude. Does anyone have any ideas/hints on how to solve this?","['multivariable-calculus', 'examples-counterexamples', 'analysis', 'inequality']"
4798735,"Show that if $f\{1,\dots,n\}\to\{1,\dots,n\}$ is surjective then it is also bijective.","Show by induction that if $f:\{1,\dots,n\}\to\{1,\dots,n\}$ is surjective then it is also bijective. My approach: Base case $n=1$ : $f:\{1\}\to\{1\}$ is trivially bijective. Induction hypothesis for a $n\in\mathbb{N}$ : $f:\{1,\dots,n\}\to\{1,\dots,n\}$ is surjective then it is also bijective. Induction step $n\to n+1$ : Let's consider the surjective function $f:\{1,\dots,n+1\}\to\{1,\dots,n+1\}$ . If $f^{-1}(n+1)$ contains only one element, i.e. $f^{-1}(n+1)=:k$ , then $g:\{1,\dots,n\}\to\{1,\dots,n\}$ with $$\begin{align*}
g(x):=\begin{cases}f(x),&x\neq k,\\ f(n+1),&x=k\end{cases}
\end{align*}$$ is surjective and we can apply the induction hypothesis. Hence, $g$ is bijective. Consequently, $$\begin{align*}
f(x):=\begin{cases}g(x),&x\neq k,\\ n+1,&x=k,\\ f(n+1),&x=n+1\end{cases}
\end{align*}$$ is bijective. If $f^{-1}(n+1)$ contains more than one element, WLOG we denote those elements by $f^{-1}(n+1)=:\{1,\dots,k\}$ , then $g:\{1,\dots,n\}\to\{1,\dots,n\}$ with $$\begin{align*}
g(x):=\begin{cases}f(x),&x\notin \{1,\dots,k\},\\ f(n+1),&x\in\{1,\dots,k\}\end{cases}
\end{align*}$$ is surjective and we can apply the induction hypothesis. Hence, $g$ is bijective. Consequently, $$\begin{align*}
f(x):=\begin{cases}g(x),&x\notin \{1,\dots,k\},\\ n+1,&x\in \{1,\dots,k\},\\ f(n+1),&x=n+1\end{cases}
\end{align*}$$ is bijective. Is this correct? It seems a bit odd to conclude bijectivity as we have $f(n+1)$ for all $x\in\{1,\dots,k\}$ . EDIT: As $f:\{1,\dots,n+1\}\to\{1,\dots,n+1\}$ is surjective we know that $f^{-1}(n+1)\neq\emptyset$ and define $S:=f^{-1}(n+1)$ . We consider two cases: If $n+1\notin S$ , then we define $g:\{1,\dots,n\}\to\{1,\dots,n\}$ by $$\begin{align*}
g(x):=\begin{cases}f(x),&x\notin S,\\ f(n+1),&x\in S.\end{cases}
\end{align*}$$ We take an arbitrary $y\in\{1,\dots,n\}$ and by surjectivity of $f$ there exists a $x\in\{1,\dots,n+1\}$ such that $f(x)=y$ . 1.) Either $x=n+1$ so that any $z\in S$ yields $g(z)=f(n+1)=y$ , 2.) or $x<n+1$ and because of $y< n+1$ we know that $x\notin S$ which yields $g(x)=f(x)=y$ . Consequentially, $g$ is surjective and we can apply the induction hypothesis so that $g$ must be bijective. This means that $S$ contains exactly one element which we denote by $s$ . Now we can represent $f:\{1,\dots,n+1\}\to\{1,\dots,n+1\}$ equivalently by $$\begin{align*}
f(x):=\begin{cases}g(x),&x\in\{1,\dots,n\}\setminus\{s\}\\ n+1,&x=s\\f(n+1),&x=n+1.\end{cases}
\end{align*}$$ We know that $g$ is bijective so $f(n+1)\neq g(x)$ for all $x\in\{1,\dots,n\}\setminus\{s\}$ . Also $n+1\neq f(n+1)$ by assumption and $g(x)\neq n+1$ for all $x\in\{1,\dots,n\}$ because $g$ maps to $\{1,\dots,n\}$ . Bearing this in mind we check for injectivity of $f$ . If $f(x)=f(y)$ then this could only be possible if $x=y=s$ , $x=y=n+1$ or $x,y\in\{1,\dots,n\}$ . But as $g$ is bijective the third possibility also delivers $x=y$ . So $f$ is injective and hence bijective. Now we assume $n+1\in S$ and define $g:\{1,\dots,n\}\to\{1,\dots,n\}$ by $$\begin{align*}
g(x):=\begin{cases}f(x),&x\notin S,\\ 1,&x\in S.\end{cases}
\end{align*}$$ Let's take an arbitrary $y\in\{1,\dots,n\}$ , then by surjectivity of $f$ there exists a $x\in\{1,\dots,n+1\}$ such that $f(x)=y$ . As $y\leq n$ the corresponding $x$ satisfies $x\notin S$ so that $f(x)=g(x)=y$ . Hence, $g$ is bijective by induction hypothesis. This means, that $S\cap\{1,\dots,n\}=\emptyset$ because otherwise $g$ couldn't be injective. So $S$ contains only one element, namely $n+1$ .  Now we can represent $f:\{1,\dots,n+1\}\to\{1,\dots,n+1\}$ equivalently by $$\begin{align*}
f(x):=\begin{cases}g(x),&x\in\{1,\dots,n\}\setminus\{n+1\}\\ n+1,&x=n+1.\end{cases}
\end{align*}$$ In this case we directly see that $f$ must be injective. Hence, $f$ is bijective.","['real-analysis', 'functions', 'solution-verification', 'elementary-set-theory', 'induction']"
4798760,Forcing fractional parts to be less than $\varepsilon$ simultaneously,"Edit : For $x \in \mathbb{R}$ , let $\langle x \rangle $ denote the unique number in $[-\frac{1}{2},\frac{1}{2})$ s.t. $x - \langle x \rangle \in \mathbb{Z}$ . Suppose that we have $x_1, ..., x_n \in \mathbb{R}$ . Then, given $\varepsilon >0$ , does there necessarily exist $k \in \mathbb{N}$ such that: $$|\langle kx_i \rangle| < \varepsilon$$ for every $i$ ? The original problem, which I read incorrectly from where I was working on it (the same question with $\langle x \rangle = x - \lfloor x \rfloor$ the standard fractional part), was disproved by the counterexample given by Haran.","['combinatorics', 'analysis', 'real-analysis']"
4798776,Do conditional expectation and orthogonal projection coincide?,"Consider some random variables $Y_1,\ldots, Y_n\in L^2(\Omega,\mathcal{F},P)$ and the inner product $$\langle X,Y\rangle_{L^2}=E[XY].$$ Moreover, let $\mathcal{L}^Y=\overline{\operatorname{span}}(Y_1,\ldots,Y_n)\subset L^2$ be the closure of the all linear combinations of $Y_1,\ldots,Y_n$ . By the Hilbert projection theorem, for any $X\in L^2$ there exists a unique $P_Y^\bot X\in\mathcal{L}^Y$ , that minimizes the distance to $\mathcal{L}^Y$ , i.e. for any $Z\in\mathcal{L}^Y$ it holds $$E[(X-P_Y^\bot X)^2]=\lVert X-P_Y^\bot X\rVert_{L^2}^2\le \lVert X-Z\rVert_{L^2}^2=E[(X-Z)^2]$$ Something similiar is true for the conditional expectation $E[X|Y_1,\ldots, Y_n]$ . It is $\sigma(Y_1,\ldots, Y_n)$ -measurable and for any other $\sigma(Y_1,\ldots, Y_n)$ -measurable random variable $Z$ , it holds $$E[(X-E[X|Y_1,\ldots,Y_n])^2]\le E[(X-Z)^2].$$ with equality if and only if $Z=E[X|Y_1,\ldots,Y_n]$ . Now I am wondering, if these two random variables coincide? I am tempted to say , that $P_Y^\bot X$ is a linear combination of $Y_1,\ldots, Y_n$ , such that there is a Borel function $f$ with $P_Y^\bot X=f(Y_1,\ldots, Y_n)$ yielding that $P_Y^\bot$ is $\sigma(Y_1,\ldots, Y_n)$ -measurable. This would imply $$E[(X-E[X|Y_1,\ldots,Y_n])^2]\le E[(X-P_Y^\bot X)^2].$$ However, in reality $P_Y^\bot X$ only needs to sit in the closure of the $\operatorname{span}(Y_1,\ldots, Y_n)$ . So I am not convinced yet. The other way around, I am tempted to say , that any Borel function $f$ can be somehow represented or approximated by linear combinations of its arguments, which should then imply $E[X|Y_1,\ldots,Y_n]\in\mathcal{L}^Y$ and $$ E[(X-P_Y^\bot X)^2] \le E[(X-E[X|Y_1,\ldots,Y_n])^2],$$ such that $ E[X|Y_1,\ldots,Y_n]=P^\bot_Y X$ with the inequality above. But this argumentation is even less convincing to me. Is my claim true? How should I prove it? Or do there exist any counter examples?","['conditional-expectation', 'stochastic-processes', 'linear-algebra', 'functional-analysis', 'probability-theory']"
4798811,Convergence of exponential of Gaussian random walk,"Let $\xi_1, \xi_2, \ldots$ be iid. standard normal random variables. (Recall their moment generating function: $\mathbb{E}\left(\mathrm{e}^{\lambda \xi_i}\right)=\mathrm{e}^{\lambda^2 / 2}$ .) Let $a, b \in \mathbb{R}$ , $$
S_n=\sum_{k=1}^n \xi_k, \quad \text { and } \quad X_n=\mathrm{e}^{a S_n-b n} .
$$ Show that $$
X_n \rightarrow 0 \text { a.s. } \Leftrightarrow b>0
$$ but for any $r \geq 1$ $$
X_n \rightarrow 0 \text { in } \mathcal{L}^r \Leftrightarrow r<\frac{2 b}{a^2}
$$ here's my solution but I'm stuck on how I can justify removing the $n$ from the final inequality. Might be having a bit of brain fart but it's late. Now let $r \geq 1$ , then using Markov's inequality: $$\mathbb{P}\left(\left|X_n-0\right| \geq \varepsilon\right)=\mathbb{P}\left(\left|X_n\right|^r \geq \varepsilon^r\right) \leq \frac{\mathbb{E}\left|X_n\right|^r}{\varepsilon^r}=\frac{\left\|X_n-X\right\|_r^r}{\varepsilon^r} \rightarrow 0$$ Using the moment generating function provided, $$\frac{\mathbb{E}\left|X_n\right|^r}{\varepsilon^r}=\frac{e^{r(\frac{a^2r}{2}-bn)}}{\varepsilon^r} \to 0.$$ By the definition of convergence, there exists an $N$ s.t $\forall n \geq N,$ $$\frac{e^{r\left(\frac{a^2 r}{2}-b n\right)}}{\varepsilon^r} \leq \frac{e^{r\left(\frac{a^2 r}{2}-b N\right)}}{\varepsilon^r}$$ so we choose $N$ s.t. $$\frac{e^{r\left(\frac{a^2 r}{2}-b N\right)}}{\varepsilon^r} =1 = e^0$$ then $$\frac{a^2 r}{2}-b n < 0$$","['moment-generating-functions', 'measure-theory', 'probability-limit-theorems', 'probability-theory']"
4798849,"If $a_{n}>0$ and $\sum a_{n}$ converges, does $\sum a_{n}^{1- \frac{1}{\sqrt{ n }}}$ converges?","Given that $a_n >0$ , $\sum_{n=1}^\infty a_n$ converges, how can I judge if $\sum_{n=1}^\infty a_{n}^{1- \frac{1}{\sqrt{ n }}}$ converges? I already know that for any $s\ge 1$ , given the conditions $\sum a_n^s$ also converges. and for any $0<s<1$ , let $a_n= n^{-s^{-1}}$ then $\sum a_n$ converges but $\sum a_n^s$ diverges. But those arguments don't apply to $1- \frac{1}{\sqrt n}$ .","['sequences-and-series', 'real-analysis']"
4798859,Cut a ribbon with a minumum amount of cuts (Middle School Math),My daughter has this question in her Math Club: Ann needs to cut a ribbon 1 in x 48 in into 48 1x1 squares. What is the minimal number of cuts Ann needs to make if she can stack together several pieces and cut them at once but cannot bend any piece? She is getting 6 as the answer but she has done this with brute force. Is there a mathematical way around such questions?,['geometry']
4798861,An integral with square root and fractional powers,"How to evaluate $$\int \sqrt{1+(1+(1+x)^{\frac{1}{4}})^\frac{1}{3}}\,\mathrm dx ?
$$ I am trying this question by substituting $1+x=u$ . Then $\mathrm dx=\mathrm du$ and the integral will become $$
\int \sqrt{1+(1+u^{\frac{1}{4}})^{\frac{1}{3}}}\,\mathrm du.
$$ Then, I am thinking of substituting $u=(\tan\theta)^8$ . Now $\mathrm du = 8\,(\tan\theta)^7 \,(\sec\theta)^2 \,\mathrm d\theta$ . So, finally the integral will be $$
8 \int \sqrt{1+(\sec\theta)^{\frac{2}{3}}} \,(\tan\theta)^7\,(\sec\theta)^{2} \,\mathrm d\theta.
$$ Now I am facing problem to integrate further. So, please help me out to integrate.","['integration', 'calculus']"
4798904,Giving some numbers to people to satify given condition,"When I read a math  book by myself, I have encounter with a question. I tried to solve it but my answer seems wrong according to my sense. Moreover, there is not any answer key because of the question number is even. The question roughly says that we want to disperse two primes for each person in the world.Moreover, those primes must exactly have the length of a hundred.Each person must get distinct numbers to be secured. We also assume that the primes numbers selected randomly. Then, what is the probability of being non-secured ?
(Assume that eight billion people live in world). What I did: I firstly calculated the number of primes with length of a hundred using prime number theorem s.t $$\pi(10^{100})-\pi(10^{99})=\frac{10^{100}}{\ln(10^{100})}-\frac{10^{99}}{\ln(10^{99})} \approx3.9 \times10^{97}$$ So, the total answer is all - secured situation: $$1-\frac{\binom{39 \times 10^{96}}{2}\binom{(39 \times 10^{96})-2}{2}...\binom{(39 \times 10^{96})-{16,000,000,000}}{2}}{(39 \times 10^{96})^{8,000,000,000}}$$ However, the result would be big . According to my friends, the probability of being non-secured must be very small. Can you help me ?","['discrete-mathematics', 'elementary-number-theory', 'combinatorics', 'probability']"
4798914,A model inspired by Plants vs. Zombies: What is the expected value of damage dealt by a Kernel-Pult when a zombie walks one step?,"Here is an interesting model inspired by the game Plants vs. Zombies. A Kernel-Pult can lob either a kernel (with probability $1 - p$ , dealing $1$ damage) or a butter (with probability $p$ , dealing $2$ damage with stunning effect) to a zombie. (The damage is counted relative to a peashooter.) Suppose that an invincible zombie is walking in front of him (so the zombie will not die prematurely). A zombie will walk one ""step"" during the interval between firing two projectiles if not stunned. If the zombie is hit by a butter, then he will be unable to walk for an interval of $t$ steps (assuming $t$ is a positive integer for simplicity). If he is hit by another butter when he is already stunned, the number of unable-to-walk-steps will be reset to $t$ . The question is: What is the expected value of damage taken by the zombie within one step? I think the possible values of damage taken within one step ranges from $1$ (one kernel) to $\infty$ (consecutive butters as much as possible). However, I can't derive the distribution behind it. (PS: In the real-world PVZ game, $p = 0.25$ , and the time interval between two projectiles is slightly randomized.)",['probability']
4798967,Prove $\frac{a}{\sqrt{4a+3bc}}+\frac{b}{\sqrt{4b+3ca}}+\frac{c}{\sqrt{4c+3ab}}\le \frac{\sqrt{a+b+c+2}}{2}.$,"Problem. Let $a,b,c\ge 0: ab+bc+ca=1.$ Prove that $$\frac{a}{\sqrt{4a+3bc}}+\frac{b}{\sqrt{4b+3ca}}+\frac{c}{\sqrt{4c+3ab}}\le \frac{\sqrt{a+b+c+2}}{2}.$$ Equality holds at $abc=0.$ I've tried to use Cauchy-Schwarz inequality $$\sum_{cyc}\sqrt{\frac{a}{4a+3bc}}\cdot \sqrt{a}\le \sqrt{(a+b+c)\cdot \left(\frac{a}{4a+3bc}+\frac{b}{4b+3ca}+\frac{c}{4c+3ab}\right)},$$ hence it's enough to prove $$\frac{a}{4a+3bc}+\frac{b}{4b+3ca}+\frac{c}{4c+3ab}\le \frac{a+b+c+2}{4(a+b+c)}.$$ The last inequality is wrong when $a=0$ . I'd like to ask a better approach. All idea and comment are welcome.","['uvw', 'multivariable-calculus', 'cauchy-schwarz-inequality', 'symmetric-polynomials', 'inequality']"
4798987,Does $\sqrt[n]{a_n}$ converges implies $\frac{a_{n+1}}{a_n}$ converges?,"I know that the following statement is true (where $a$ always denote a real number): Suppose $\lim_{n\to\infty}\frac{a_{n+1}}{a_n}=a$ , then $\lim_{n\to\infty}\sqrt[n]{a_n}$ converges to $a$ . The proof of this statement is simple, it suffices to notice that $$\liminf_{n\to\infty}\frac{a_{n+1}}{a_n}\le\liminf_{n\to\infty}\sqrt[n]{a_n}\le\limsup_{n\to\infty}\sqrt[n]{a_n}\le\limsup_{n\to\infty}\frac{a_{n+1}}{a_n}.$$ What about the converse statement? Does $\lim_{n\to\infty}\sqrt[n]{a_n}=a$ implies $\lim_{n\to\infty}\frac{a_{n+1}}{a_n}=a$ ? I think it is not true, but I can't find a counter-example. Can someone help me? Thanks in advance.","['limits', 'calculus', 'real-analysis']"
4799000,FInd the minimum value if the function $f(x)=\sqrt{4x^2-4x+2}+|x|$,"For the function $f(x)=\sqrt{4x^2-4x+2}+|x|$ the minimum value of $f(x)=$ My approach is as follow $f(x)=2\sqrt{(x-\frac{1}{2})^2+\frac{1}{4}}+|x|$ For the term in square root we get minimum at $x=\frac{1}{2}$ but the absolute term value occurs at $x=0$ . But not able to find the value for which value of $x$ , $f(x)$ is minimum.","['calculus', 'functions']"
4799011,Algebraic vs geometric approach different results confusion,"I consider the following system: $$
x+y+z=A\\
x^2+y^2+z^2 = B
$$ where $A,B ∈ ℝ; B ≥0$ . Now, the geometric approach here is that these describe a sphere and a plane. So they can: Have no intersection: the system would then have no solutions. Since we have a very specific plane and a sphere here, this might be impossible, but mentioning it just in case Be tangent: the system would then have exactly one solution. An example of that happening is $A=15, B=75$ Intersect via a circle: the system would then have infinitely many solutions and the set of solutions would form that circle. So far so good. Now, let's try algebraic approach and consider this ""trick"": let's multiply 1st equation by $2w$ where $w ∈ ℝ, w ≠ 0$ and then add it to the first equation. This yields $x^2+2wx+y^2+2wy+z^2+2wz=2Aw+B$ , or - after moving things around: $$(x+w)^2+(y+w)^2+(z+w)^2=3w^2+2Aw+B$$ And here's the thing: the $w$ we brought here was completely arbitrary. So we now try to make it so that the right hand side is 0, i.e. $3w^2+2Aw+B=0$ . And here's what I don't understand: This equation could have no roots. Since the leading coefficient is positive, this would mean that the sum of squares on the left hand side is always positive. So as far as I see it, it corresponds to ""infinitely many solutions"" of the geometric approach. This equation could have exactly one root. Then it's clear that it corresponds to a ""tangent case"" from the geometric approach. For instance, if $A=15, B=75$ we have $𝛥 = 0$ meaning it's exactly such case. This equation could have two roots. This is the case I do not understand . What does it correspond to? Furthermore, the algebraic approach doesn't seem to even work for the last case. For example, if we have $A=5, B=3$ then: $$
x+y+z=5\\
x^2+y^2+z^2 = 3
$$ And so $3w^2+10w+3$ , so $𝛥 = 64$ and $w_1=-\frac{1}{3}, w_2=-3$ . If we take $w=-3$ then it corresponds to equation $x^2-6x+y^2-6y+z^2-6z=3-(2∙3∙5)=-27 ⇒ (x-3)^2+(y-3)^2+(z-3)^2=0$ and there are several problems here: First, this implies $x=y=z=3$ because over reals the only way a sum of squares can add up to 0 is if all summands are 0 themselves. But $(3,3,3)$ doesn't satisfy the system Second, this isn't even unique because we could've used $w=-\frac{1}{3}$ Third and most important - this was done with only simple linear operations so the resulting equation is equivalent to the system. What do I miss with this last case when there are two solutions in $w$ ? And why does it fail algebraically whereas the case with only one solution in $w$ works correctly?","['algebra-precalculus', 'systems-of-equations', 'geometry']"
4799032,Prove that area of $ABC$ is $\frac{1}{4}|BC|^2\cdot \cot \frac{\theta}{2}$ where $\theta=\angle BAD$,"Let $ABC$ be a triangle such that $\angle B=2\angle C$ . We extend the side $BC$ by a segment $CD$ equal to $\frac{1}{3}BC$ . Prove that area of $ABC$ is $$\frac{1}{4}|BC|^2\cdot \cot \frac{\theta}{2}$$ where $\theta=\angle BAD$ My attempt, Let $a,b,c,d$ be the respective lengths of $BC, CA, AB, AD$ and suppose that the bisector of angle $ABC$ intersects $AC$ at $E$ . So we can see that $BE=EC=\frac{ab}{(a+c)}$ . We also know that $AC:AB=BC:BE$ . Hence, $b^2=c(a+c)$ . By applting the Cosine Law to the triangles $ABC$ and $ACD$ , we can get $ad^2+(\frac{a}{3})c^2=(b^2+\frac{a^2}{3})(\frac{4a}{3})$ . By using the equation of $b^2$ , we get $9d=4a^2+12b^2-3c^2=(3c+2a)^2 $ and $d=\frac{1}{3}(3c+2a)$ . Applying the Cosine Law again to triangle $ABD$ and using the previous result to get $$16a^2=9(c^2+d^2-2cd\cos \theta)=(18c^2+12ac+4a^2)-6c(3c+2a)\cos \theta$$ so that $$6(3c^2+2ac)\cos \theta=6(3c^2+2ac-2a^2)$$ Hence, $$\cos \theta=1-\frac{2a^2}{3c^2+2ac}$$ We can get $1+\cos \theta=\frac{6c^2+4ac-2a^2}{3c^3+2ac}$ and $\cot^2 \frac{\theta}{2}=\frac{1+\cos \theta}{1-\cos \theta}=\frac{(c+a)(3c-a)}{a^2}$ By using Heron's area formula, $16[ABC]^2=a^4 \cot^2 \frac{\theta}{2}$ $[ABC]=\frac{1}{4}a^2 \cot \frac{\theta}{2}$ Is there any other way/method to solve this question? Thanks in advance.","['triangles', 'geometry']"
4799051,Zero distance between a closed set and a bounded closed set which are disjoint in a metric space,"I've known that the distance between a compact set and a closed set is zero if and only if they are not disjoint, and I can also find counterexamples in which two disjoint closed set has zero distance. But how about a closed set and a bounded closed set? Does there exist a closed set and a bounded closed set with zero distance between them, and they are disjoint? The counterexample should not be found in Euclidean spaces though.","['general-topology', 'metric-spaces']"
4799097,Bounding $L^1$ norm of the difference between a function $f:\mathbb R^n\to\mathbb R$ of bounded variation and a piecewise constant approximation,"As a follow up to this question, which deals with univariate functions, I assume that we are given a function $f:\mathbb R^n\to\mathbb R$ which is of bounded variation on bounded sets, meaning, according to Wikipedia , that for $\Omega\subset\mathbb R^n$ open and bounded $$V(f,\Omega)=\sup\left\{\int_\Omega f(x)\mathrm{div}\phi(x)\ \mathrm dx:\phi\in C_c^1(\Omega,\mathbb R^n),\|\phi\|_{L^\infty(\Omega)}\leq1\right\}$$ is finite.
I want to take some bounded set $A\subset\mathbb R^n$ ; I'm willing to assume that $A$ is an hyperrectangle, i.e. $A$ is of the form $A=\prod_{i=1}^n[a_i,b_i)$ . Then we partition $A$ into $d$ hyperrectangles $A_1^d,\ldots,A_d^d$ , and we denote this partition by $\mathcal P^d$ . We define its mesh by the largest Lebesgue measure (EDIT: it seems that we need to define the mesh as the largest diameter , see @dsh's comment and second update to their answer) among the elements from this partition. Then the piecewise constant function $f^d$ is obtained by averaging over the elements of the partition, i.e. with $|A_i^d|$ the Lebesgue measure of $A_i^d$ $$f^d|_{A_i^d}=\frac1{|A_i^d|}\int_{A_i^d}f(x)\ \mathrm dx.$$ Similarly to the univariate case, I was wondering whether we can prove that $$\|f^d-f\|_{L^1(A)}\leq V(f,A)\cdot \mathrm{mesh}(\mathcal P^d).$$ Since my multivariate calculus is not that strong, I'm having some difficulty proving this. I'm wondering whether this can be done similarly as in the univariate case, as done by Alex Ravsky in the linked question. Intuitively, the answer should definitely be yes, but I was wondering how to bound $\Delta_i:=\sup_{x\in A_i^d}f(x)-\inf_{x\in A_i^d}f(x)$ by the integrand involving the divergence operator. More specifically, ignoring the fact that we should deal with equivalence classes for the moment, some easy bounding gives \begin{align*}
\|f^d-f\|_{L^1(A)}&=\int_A|f^d(x)-f(x)|\ \mathrm dx=\sum_{i=1}^d\int_{A_i}|f^d(x)-f(x)|\ \mathrm dx\leq\sum_{i=1}^d\int_{A_i}\Delta_i\ \mathrm dx\\
&\leq\mathrm{mesh}(\mathcal P^d)\sum_{i=1}^d\Delta_i,
\end{align*} so that it suffices to prove that for any partition $\mathcal P^d$ of (the hyperrectangle) $A$ into $d$ subsets (hyperrectangles) $A_i$ it holds that $$\sum_{i=1}^d\Delta_i\leq V(f,A).$$ Any help is much appreciated.","['measure-theory', 'geometric-measure-theory', 'bounded-variation', 'real-analysis', 'multivariable-calculus']"
4799108,Determining the Big O Order for a Non-Standard Recurrence Relation,"Question: Find the best possible order for $ T(n) $ , for sufficiently large values of $ n $ . $ T(n) = 2T\left(\frac{n}{2}+\sqrt{n}\right)+T\left(\frac{n}{2}\right)+1 $ My try: Here is my attempt at finding the solution using induction to prove $T(n) \le cn^{\log_{2 - \varepsilon}(3)} + b$ : $
\begin{align*}
\varepsilon & > 0 \\\\
T(n) & \leq 3T \left( \frac{n}{2} + \sqrt{n} \right) + 1 \\\\
& \leq 3T \left( \frac{n}{2 - \varepsilon} \right) + 1 \quad \text{(For n )} \\\\
T(n) & \leq 3c \left( \frac{n}{2 - \varepsilon} \right)^{\log_2(3)} + 3b + 1 \\\\
& = \frac{3cn^{\log_{2 - \varepsilon}(3)}}{(2 - \varepsilon)^{\log_{2 - \varepsilon}(3)}} + 3b + 1 \\\\
T(n) & \leq cn^{\log_2(3)} + 3b + 1 \leq cn^{\log_2(3)} + b \quad \text{if } b < -\frac{1}{2} \\\\
\varepsilon & > 0 \Rightarrow T(n) \in O \left( n^{\log_{2 - \varepsilon}(3)} \right)
\end{align*}
$ What I can't find: Provide a proof that setting $\varepsilon = 0$ is permissible. $ T(n) = 2T\left(\frac{n}{2}+\sqrt{n}\right)+T\left(\frac{n}{2}\right)+1 \in O \left( n^{\log_{2}(3)} \right)$","['computational-complexity', 'asymptotics', 'recurrence-relations', 'analysis']"
4799125,Is the Weierstrass function strictly non-monotonic across every interval?,"I have a strong suspicion that the Weierstrass function is non-monotonic across every interval, though this suspicion is rooted in the visualization of its generation. I don't know how to prove it, nor am I certain about whether the non-monotonicity is strict or not, though I suspect it is strict. One way to prove strict non-monotonicity across every interval would be to prove the following, which seems to me to be a simple target: $$\forall x,y \in \Bbb R\biggr( W(x) \ne W(y) \iff \exists z \in [x,y] \biggr [W(z) < \min(W(x), W(y)) \lor W(z) > \max(W(x),W(y)) \biggr] \biggr)$$ Perhaps another way to prove it would be via the proof for the nowhere-differentiability of the Weierstrass function, because maybe nowhere-differentiability implies strict non-monotonicity across every interval? The Weierstrass function","['derivatives', 'intuition', 'fractals']"
4799162,"Strategies for solving the infinite integral $\int_0^\infty \frac{k}{k^2+\alpha^2} \, J_0(k) J_0(kr) \, \mathrm{d}k $ where $r \ge 0$ and $\alpha > 0$","While conducting an analysis of an inverse Fourier transform stemming from a fluid mechanics problem related to flows in porous media, I encountered the following infinite integral: $$
f(r) = \int_0^\infty \frac{k}{k^2+\alpha^2} \, J_0(k) J_0(kr) \, \mathrm{d}k \, , 
$$ where $r \ge 0$ and $\alpha > 0$ . It's evident that the integrand behaves as $\mathcal{O}\left( k^{-2} \right)$ as $k \to \infty$ , indicating the convergence of the integral. My attempted approach involved utilizing the classical expression of the zeroth-order Bessel function: $$
J_0(u) = \frac{1}{2\pi} \int_0^{2\pi} \exp \left(-iu \sin t \right) \, \mathrm{d} t
$$ I applied this expression to one as well as to both Bessel functions and attempted to evaluate them. Unfortunately, my efforts have not led to a successful result. If anyone here can offer guidance or provide hints that might assist in evaluating this integral, I would greatly appreciate it. E D I T Based on numerical tests, it appears that for $\alpha = 1$ , $f(r)$ exhibits a proportionality to the modified Bessel function of the second kind, $K_0(r)$ . The approximate proportionality coefficient appears to be around $1.2660658\dots$ . For other values of $\alpha$ , the trend is less discernible. It's possible that the behavior is a combination of multiple modified Bessel functions of the second kind, but this is a speculative hypothesis derived from intuition and not necessarily a reflection of reality.","['integration', 'improper-integrals', 'real-analysis', 'calculus', 'bessel-functions']"
4799217,Continuous Section of Length Map for Triangles,"Interpret $\mathbb{R}^6 = (\mathbb{R}^2)^3$ as the space of ordered triangles in the real plane (degenerate triangles are included). There is a map $L \colon \mathbb{R}^6 \to \mathbb{R}^3$ sending a triangle to the lengths of its sides. Concretely, $$L(x_1, y_1, x_2, y_2, x_3, y_3) = \left(\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}, \sqrt{(x_1-x_3)^2+(y_1-y_3)^2}, \sqrt{(x_2-x_3)^2+(y_2-y_3)^2}\right).$$ Let $B$ be the image, that is, $B = \{(a,b,c) \in \mathbb{R}_{\ge 0}^3 \mid a +b \ge c, a+c \ge b, b+c \ge a\}$ . Is there a continuous section $s \colon B \to \mathbb{R}^6$ of $L$ , i.e., $L \circ s = \operatorname{id}$ ? My thoughts so far: When restricting to non-degenerate triangles, there is such a section. Just fix the first corner at the origin, let the second corner be on the positive $x$ -axis and choose the third point of the triangle to be the unique one in the upper half plane to have the right distances to the other ones. One can write down formulas for the points without problems and the resulting map is continuous. A problem arises for degenerate triangles where $(x_1, y_1)$ and $(x_2, y_2)$ have distance zero. The paths $t \mapsto (t, 1, \sqrt{1+t^2})$ and $(t, 1, \sqrt{1-t+t^2})$ both approach $(0,1,1)$ as $t \to 0$ . Applying my conjectured $s$ and taking the limit, however, results in different degenerate triangles. I tried several other candidates for $s$ , but there is always a class of degenerate triangles where discontinuities arise. Therefore, I suppose that no such $s$ exists. Unfortunately, I have no idea how to disprove its existence. The only thing I figured out is that we may assume the first two coordinates of $s$ to be constantly zero. Otherwise we may subtract these coordinates from all components of $s$ without changing the distances between the points. I also tried rotating the triangles given by $s(a,b,c)$ so that the second point lies on a fixed ray starting at the origin. This failed as the angle between $(x_2, y_2)$ and the fixed ray is undefined if $(x_2, y_2)$ is zero. Another idea I had was using the cohomology of the topological spaces involved. But this does not seem helpful as $B$ is contractible. EDIT — New Idea: If a section $s$ exists, then $L$ is a quotient map. This means $U \subseteq B$ is open only if $L^{-1}(U)$ is open. This statement looks more susceptible to a refutation.","['invariant-theory', 'continuity', 'general-topology', 'algebraic-topology']"
4799244,Number of groups with a bounded short presentation,"How many groups there are (up to isomorphism) with a presentation with at most $n$ generators and with relators of length at most $3$ ? I don't expect there exist a sharp solution, since I know that the group isomorphism problem is undecidable (even though I wonder if it's easier when we give an upper bound on both the number of generators and the length of the relators). On the other hand there is an obvious combinatoric estimate $$N\leq 2^n\cdot 2^{n^3},$$ choosing among all possible generators and relations. This estimate is very rough though, as for example we may pick a lot of relations among generators that don't even appear in $S$ . So, what is the best estimate we can make? I would be happy either to have nice combinatoric estimates (for example it can go down by a factor of $n!$ simply by looking at permutations, can we do better?), but expecially if there were group theory arguments, even if they apply to specific classes of groups. For example I would be satisfied to have an estimate for the number of hyperbolic groups with such a presentation .","['group-presentation', 'hyperbolic-groups', 'combinatorial-group-theory', 'combinatorics', 'group-theory']"
4799255,Typo in book or I am wrong?,"Find the equation of the tangent to the parabola $y = x^2$ ,
if the $x$ -intercept of the tangent is $2$ . Now $$y = mx + b$$ $$0 = m(2) + 2$$ $$m = -1$$ so $$m = \frac{y-0}{x-2}$$ $$m(x-2) = y$$ $$-x-y = -2$$ but the book answer is $$8x-y =16$$","['calculus', 'slope', 'algebra-precalculus', 'tangent-line']"
4799275,Is the zero expectation of the residuals in Gauss-Markov theorem really an hypothesis?,"Wikipedia says The Gauss–Markov theorem states that the ordinary least squares (OLS) estimator has the lowest sampling variance within the class of linear unbiased estimators, if the errors in the linear regression model are uncorrelated, have equal variances and expectation value of zero. Alright. But the fact is that, when I compute the expectation of the residuals taking into account the OLS estimator, I naturally obtain $0$ . So my question is: do we need to explictly consider the zero-expectation of the residuals as an hypothesis ? Isn't it already induced by the computation of the estimator through OLS ?","['statistical-inference', 'statistics', 'regression', 'parameter-estimation', 'least-squares']"
4799304,An interesting finding on twin primes,"I was doing some research on prime gaps including twin primes and this led me to this finding, which is: $$ \lim\limits_{n\to \infty} \frac{\pi^2(n)}{n\pi_2(n)} = 0.7550363087870907 \cdots\cdots (1) $$ where the symbols have their usual meanings. Chart shown below. $\frac{\pi^2(n)}{n\pi_2(n)}$ "" /> The number $0.755..$ did not make any sense to me until accidentally I inverted it in my Jupyter notebook which resulted in: $$ \lim\limits_{n\to \infty} \frac{n\pi_2(n)}{\pi^2(n)} = 1.3203236316937392 \cdots\cdots (2) $$ Chart shown below. $\frac{n\pi_2(n)}{\pi^2(n)}$ "" /> Now $1.32032... = 2C_2$ where $C_2$ is the twin prime constant. Question : It appears the graphs are converging but that could be just an illusion. Is the second chart indeed converging to $2C_2$ ? Philosophically it would make sense as it gives real meaning to the twin prime constant. This if true will obviously prove the twin prime conjecture. This result is a much stronger result than the Hardy-Littlewood twin prime conjecture. Of course, how I arrived at this is perhaps more interesting but that would take up too much space here. Finally if we rewrite (2) as: $$ \lim\limits_{n\to \infty} \frac{\frac{\pi_2(n)}{\pi(n)}}{\frac{\pi(n)}{n}} = 1.3203236316937392 \cdots\cdots (3) $$ which is saying the number of twin primes in primes is more than the number of primes in $n$ .","['twin-primes', 'number-theory', 'elementary-number-theory', 'prime-gaps', 'prime-numbers']"
4799336,"Covariance between a vector $X$ and its norm $\lVert X \rVert$ for $X \sim \mathcal{N}(\mu, \Sigma)$","As stated in the title, I need an expression for the covariance between the elements of a vector $X \in \mathbb{R}^n \sim \mathcal{N}(\mu, \Sigma)$ , and its norm $\lVert X \rVert$ . There is a formula for the covariance between $X$ and $\lVert X \rVert^2$ , as I wrote in the answer to another question, which seems to be an easier case due to the lack of the square root, but I can't find anything for $\lVert X \rVert$ . I'm not sure what approach I could take to try to derive such an expression myself. Any help would be greatly appreciated.","['statistics', 'normal-distribution', 'probability']"
4799369,Find $x$ if $AD=CB$. Justify geometrically.,"I need help with this geometry question. Find $x$ if $AD=CB$ . Justify geometrically. I tried plotting it on GeoGebra and found that $x=30\unicode{176}$ but I don't know how to prove it, I also tried with a system of linear equations but it has no solution: $\begin{cases}70\unicode{176}+x_2+x_3=180\unicode{176}\\[5pt]
70\unicode{176}+x_1+40\unicode{176}+x_2=180\unicode{176}\\[5pt]
40\unicode{176}+x_1+\left(180\unicode{176}-x_3\right)=180\unicode{176}\end{cases}$ I also tried drawing the heights of the $2$ triangles to get right triangles but I end up with more unknowns. Thanks in advance.","['triangles', 'trigonometry', 'angle', 'geometry']"
