question_id,title,body,tags
2791881,Connecting smooth functions in a smooth way,"Let $a<b<c<d$ be real values, and let $f \in C^{\infty}([a,b])$ and $g \in C^{\infty}([c,d])$. Is there a way to ""connect"" these functions in a smooth way? That is, is there a function $h \in C^{\infty}([a,d])$ such that $h=f$ on $[a,b]$ and $h=g$ on $[c,d]$? If this is true, how is the proven? Is the proof non-constructive, or is there an explicit way to do it?","['derivatives', 'smooth-functions', 'calculus']"
2791890,Prove that $Z-(Y-X)=X\cup(Z-Y)$ if $X\subset Y\subset Z$,"Let $X\subset Y\subset Z$. Prove that $Z-(Y-X)=X\cup(Z-Y)$. Here, $A-B$ is the complement of $B$ in $A$. To establish equality, I need to show that $Z-(Y-X)\subset X\cup(Z-Y)$ and $X\cup(Z-Y)\subset Z-(Y-X)$. Here's how I start the proof: Let $\alpha\in Z-(Y-X)$. Then $\alpha\in Z$ and $\alpha\not\in Y-X$, which means $\color{red}{\alpha\not\in Y}$ and $\alpha\in X$. The fact that $\alpha\in X$ is sufficient to establish that $\alpha\in X\cup(Z-Y)$, and so $Z-(Y-X)\subset X\cup(Z-Y)$. But $X\subset Y$, which would mean $\alpha\in Y$. This seems to contradict the highlighted portion above. What am I missing here? I can see that equality holds by looking at the Venn diagram below. $X$ is green/innermost region, $Y$ is blue/middle, and $Z$ is red/outermost. Then $Y-X$ is the blue region alone, and so $Z-(Y-X)$ is made up of both the green and red regions. On the other hand, $Z-Y$ is the red region alone, so $X\cup(Z-Y)$ is that plus the green region. So it's (pictorially) true that $Z-(Y-X)=X\cup(Z-Y)$. Based on this picture, it seems to be the case that $\alpha$ cannot belong to $X$. Then is the ""contradiction"" I point to above actually of no consequence to the proof?","['elementary-set-theory', 'proof-verification']"
2791909,"A puzzling ""failure"" of the Chain Rule","Consider the standard transformation equations between Cartesian and polar
coordinates: \begin{align*}
x&=r \cos \theta\\
y&=r \sin \theta
\end{align*} and the inverse: $r=\sqrt{x^2+y^2}, \theta=\arctan\frac{y}{x}$. Now consider the following product of derivatives: ${\displaystyle f=\frac{\partial r(x,y)}{\partial y}\frac{\partial y(r,\theta)}{\partial r}}.$
By the chain rule ${\displaystyle f=\frac{\partial r(x,y)}{\partial y}\frac{\partial y(r,\theta)}{\partial r} = 
\frac{\partial r}{\partial r} = 1}.$ However, if we calculate each multiplicand in isolation, then transform the 
mixed-coordinate result into a single coordinate system, we get: \begin{align*}
\frac{\partial r(x,y)}{\partial y}& =\frac{y}{\sqrt{x^2+y^2}}=\sin\theta\\
\frac{\partial y(r,\theta)}{\partial r}& = \sin\theta
\end{align*} and therefore, ${\displaystyle f=\frac{\partial r(x,y)}{\partial y}\frac{\partial y(r,\theta)}{\partial r} = \sin^2\theta}$ But we've shown by the chain rule that
$f=1$! :giantfireball: I must be abusing the chain rule in some way (in the original context in which I stumbled on this, the correct result is $\sin^2\theta$), but I can't see what I did wrong.  What's going on?","['chain-rule', 'calculus']"
2791913,Digraph continuity vs topological continuity,"Continuing the topic of Topological ""closure"" of a binary relation : Let $\mu$, $\nu$ be binary relations on a set $U$. Topology $T \mu = \{ E \in \mathscr{P} U \mid
\mu [E] \subseteq E \}$ (here $\mu[E]$ is the image of a set $E$ by binary relation $\mu$). By definition, a function $f$ (on $U$) is a continuous function from $\mu$ to $\nu$ iff $f\circ\mu\subseteq\nu\circ f$. (It is the definition of discrete continuity if we consider $\mu$ and $\nu$ as directed graphs.) Conjecture If $f$ is a continuous function from $\mu$ to $\nu$ then $f$ is a continuous function from the topology $T\mu$ to the the topology $T\nu$.","['continuity', 'graph-theory', 'general-topology', 'elementary-set-theory']"
2791937,Characterization of the matrix inverse,Suppose that $A$ is an invertible matrix and let $X$ be such that $AX+XA=2I$. Does this imply that $X=A^{-1}$? I have tried simple algebra manipulations but I have not been able to conclude. For a simple example of 2x2 matrices I found it was true.,"['matrices', 'linear-algebra']"
2791948,Mathematical induction: $n^4 + n$,"Question: $n^4 + n$ is even for all Natural number. I got all the way to where Basis step is $n=1$ and Inductive step is $k^4 + k$ (evens) If my goal is to find $(k+1)^4 + (k+1)$, how would I get there?","['induction', 'discrete-mathematics']"
2791972,Determinant of a matrix over a field of characteristic 2,"I first saw a proof for the Leibniz formula for computing determinants when I was learning about tensors and the exterior product at college (a ""proof"" considering that the definition of determinant is an alternating multilinear map from the columns or rows of a square matrix to its field such, that the determinant of the identity matrix equals 1). We were told at some point that any alternating multilinear map $m$ satisfies $m(\vec{v}_1,...,\vec{v}_n)=0 \iff$ some $\vec{v}_i$ is a linear combination of all the other $\vec{v}_{j\neq i}$, only when we're working in a field of characteristic different from 2. We actually used this property in our proof of the Leibnitz formula, but maybe there exists another proof of the Leibnitz formula which doesn't need that property to hold. So... is the Leibnitz formula valid for matrices over a field of characteristic 2?, or does characteristc 2 only imply that $\mathrm{rang}(A)<n \iff \mathrm{det}(A)$ won't be true (for a $n \times n$ matrix $A$)?","['matrices', 'multilinear-algebra']"
2792001,Probability of landing on the nth stair.,"Initial Question : We begin climbing a staircase beginning at stair zero.  We  choose to take either 1,2, or 3 steps at a time, where each number of steps have an equal chance of being chosen.  What is the probability of hitting the fourth stair? How I proceeded : In lieu of recognizing what type of problem I was looking at, I formed a tree, and found that there are seven ways to hit the fourth stair.  Taking into consideration the weights of each of these outcomes I found the probability of hitting the fourth stair to be 37/81. Deeper Question : While I found that finding the number of ways to hit the $n$th step was not too difficult (I think it is the sum of the ways of hitting the three previous steps, $k(n-3)+k(n-2)+k(n-1)$), I was not able to use this successfully to find general rule for the probability of hitting the $n$th step. I am looking for some advice classifying this type of probability problem, or perhaps a link to a similar problem.  Could this be viewed as some sort of random walk?  Is there any hope for a closed form (explicit) solution?","['random-walk', 'combinatorics', 'probability']"
2792012,Cohomology class of a meromorphic differential form,"Exercise 4.2 in Voisin's ""Hodge Theory ... I"" refers to the class in $H^1(X,K_X)$ of the form $\bar \partial \mu_i$, where $\mu_i$ is a differential form of type $(1,0)$, which is $C^{\infty}$ away from $x_i$, and equal to $\frac{dz_i}{z_i}$ in a neighborhood of $x_i$. Here, $X$ is a compact complex curve, $K_X$ is the sheaf of holomorphic $1$-forms, $x_i$ is a point on $X$, and $z_i$ is a local coordinate centered at $x_i$. How does $\mu_i$ define such a cohomology class?","['complex-geometry', 'algebraic-geometry']"
2792028,"To what number does $ n^{-2} \times \sum_{m=1}^{n-1} n \bmod m$ converge, as $n$ gets large?","I evaluated the expression $$ n^{-2} \times \sum_{m=1}^{n-1} n \bmod m$$ for ""large"" $n$ values ($10^3$, $10^4$, $10^5$, $...$) and it seems to converge to the number approximately $0.17753188$. I tried to search for this number on the internet, found nothing and I also tried to analyze the expression, but my mathematical knowledge seems to be too small for this problem. Does anybody have an idea what this number could be (if it has a closed form)?","['modules', 'sequences-and-series', 'convergence-divergence', 'limits']"
2792103,Meaning of $x^T A x$,"I've seen the term $x^T A x$ , where $A$ is a square and usually symmetric matrix, come up in a bunch of different areas of linear algebra. Places I've seen it include defining the Raleigh quotient, defining positive/negative semi-definite matrices, and in the derivation of PCA. I've also seen it sometimes referred to as describing a quadratic form. Is there some general definition/ intuitive description of what $x^T A x$ means with respect to a vector and a matrix? My sort of vague understanding is that it describes how a vector is changed under a linear transformation defined by $A$ (for example if $A$ causes x to rotate 90 $^\circ$ then $x^T A x = 0$ ) but I can't seem to come up with a more precise or insightful description of $x^T A x$ , and I'm surprised how little I could find online considering how often I see this term come up.","['eigenvalues-eigenvectors', 'matrices', 'positive-semidefinite', 'quadratic-forms', 'linear-algebra']"
2792142,Find the correlation coefficient,"In studying the relation between the two variables $x$ and $y$ , if the equation of the regression line of $y$ on $x$ was 
$$y=0.421x+0.67$$ and the equation of the regression line of $x$ on $y$ was 
$$x=1.58y+3.9$$ \Find\
\ The linear correlation coefficient between $x$ and $y$ 
My solution is 
$$r= \pm\sqrt{0.421\times 1.58}= \pm0.8155$$
Does my solution correct or i would not take the negative value into account ?",['statistics']
2792159,Inverse Fourier Transform of the Cauchy distribution,"Given the characteristic function of the cauchy distribution in the form $$\hat{f}(q) = \exp(-\gamma|q|) $$ I am unsure how to derive the original probability distribution function $$ f(x) = \frac{\gamma}{\pi(\gamma^2+x^2)}$$ via the inverse Fourier transform, which I have tried using the following form. $$ f(x) = \frac{1}{2\pi}\int_{-\infty}^\infty \hat{f}(q)e^{iqx} \, dq $$ I suspect I am going wrong when transforming with respect to the absolute value $|q|$ in the characteristic function as I am unsure how to eliminate it. Any insight is very much appreciated.","['fourier-analysis', 'characteristic-functions', 'probability-distributions', 'statistics', 'fourier-transform']"
2792160,Proving two random variables are equal with probability $1$,"Suppose we have two random variables such that $1 \leq X \leq Y$ almost surely. Now, suppose that all moments of $X$ and $Y$ exist, and, $$ E[Y^m] \leq E[X^m] + 1 $$ for all $m \geq 1$ . Prove that $X$ and $Y$ are equal almost surely. By assumption, $Y \geq X$ almost surely, and so it suffices to prove that $X \geq Y$ almost surely. Specifically, I would like to show that $P\left(X > Y + \frac{1}{n}\right) = 0$ for all $n\geq1$ . My attempt was to manipulate the Markov inequality, $$ P\left(X > Y + \frac{1}{n}\right) \leq P\left(X > 1 + \frac{1}{n}\right) \leq \dfrac{E[X^m]}{\left(1+\frac{1}{n}\right)^m} \leq \dfrac{C}{\left(1+\frac{1}{n}\right)^m}$$ The first inequality comes from the fact that $Y \geq 1$ . The last comes from the fact that $E[X^m] < \infty$ for all $m$ . Taking limits as $m \rightarrow \infty$ shows the desired result. Am I justified in making the last inequality? Specificially, is what I said in bold above true? If the moments were uniformly bounded, then sure, but otherwise I can't be sure that the expression will go to zero.","['probability-theory', 'random-variables']"
2792165,"Let $a$ be a $p$-cycle in $S_p$, and let $b$ be a transposition in $S_p$. Show $S_p$ is generated by $a$ and $b$.","I want to solve the following: Let $p$ be a prime number and let $a$ be a $p$ -cycle in $S_p$ , and let $b$ be a transposition in $S_p$ . Show $S_p$ is generated by $a$ and $b$ . My attempt Write $a=(y_1 \space y_2 \space ...\space y_p)$ and $b=(z_1 \space z_2)$ . WLOG, $y_1=z_1=1$ . WLOG, we can also assume $a=(1 \space 2 \space ... \space p).$ Let $σ\in S_p$ . It should be obvious that $σ$ is a product of transpositions, so to show $σ \in S_p$ , it suffices to show every transposition can be written in terms of $(1 \space 2 \space ... \space p)$ and $(1 \space z_2)$ . But how do I show this? Duplicate? I think not. Although another question (already answered on math.SE) is similar to mine, I do not believe that mine is a duplicate. The other question is equivalent to mine only for the special case when $z_2=2$ , but not for general $z_2$ .","['galois-theory', 'abstract-algebra', 'permutations', 'group-theory', 'symmetric-groups']"
2792166,Homotopy equivalence of pairs,"Let $ I = [0,1] \subseteq \mathbb{R}$. I want to prove that the pair $(I^n,\partial I^n)$ is homotopy equivalent to $(\mathbb{R}^n,\mathbb{R}^n\setminus\{ 0,0,...,0 \})$, but I have a problem with the definition itself. Can someone please state the definition of homotopy equivalence of pairs? Apparently, I couldn't find it anywhere.","['general-topology', 'homotopy-theory']"
2792219,"Apostol's Calculus Vol 2, Chapter 10.20, Exercise 7","Let $\mu(x,y)$ be an integrating factor of the differential equation $P(x,y)\,dx+Q(x,y)\,dy=0$. I already showed that we have $$\frac{\partial P}{\partial y}-\frac{\partial Q}{\partial x}=Q\frac{\partial}{\partial x} 
\log|\mu| - P\frac{\partial}{\partial y} \log|\mu|.$$ Now I have to deduce that if $(\partial P/\partial y-\partial Q/\partial x)/Q$ is a function of $x$ alone, say $f(x)$, then the function $e^{\int f(x) \, dx}$ is an integrating factor of the equation. I couldn't find the way to see this. The previous exercise shows that the equation $y'+P(x)y=Q(x)$ has the integrating factor $e^{\int P(x)\,dx}$, but I don't know if I can use it here.","['multivariable-calculus', 'integrating-factor', 'ordinary-differential-equations']"
2792220,Trigonometric Inequality $\sin (2x) \gt \sqrt 2 \sin (x)$,"I wish to solve this inequality: $\sin (2x) \gt \sqrt 2 \sin (x)$ My approach: I tried to isolate the $x$ on the left side by using the sine sum formula: $2\sin(x)\cos(x) \gt \sqrt2\sin(x)$ then I divided by $\sin(x) \over 2$ both sides: $\cos(x) \gt {\sqrt2 \over2}$ $x \lt \cos^{-1}({\sqrt2 \over2})$ $x < {\pi \over4}$ From that I can conclude that $x < {7\pi \over4}$, but I know the answer is still incomplete as it should be $0 \lt x \lt {\pi \over 4}$, $\pi \lt x \lt {7 \over 4}\pi$ As I was able to see on Desmos graph plotter. Does my approach gives the tools to reach this answer or have I commited a mistake?","['inequality', 'trigonometry']"
2792324,How to show that this integral is correct? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question How can one show that $$\int_0^{\pi/2}\cos\left(\frac{x}{2}\right)\ln\left[\frac{1}{\alpha} \tan(x) \tan\left(\frac{x}{2}\right)\right] \sqrt{\sin(x) \tan \left(\frac{x}{2}\right)} \, \mathrm dx=-\frac{\ln(\alpha)}{\sqrt{2}}$$ assume $\alpha\ge1$. I can't see how to simplify $\tan(x)\tan(x/2)$ and $\sin(x)\tan(x/2)$.",['integration']
2792369,What is a bi-affine classifier?,"Recently, doing research on NLP papers, I came across with affine classifiers and biaffine classifiers. What is meant by these terms? What are affine classifiers and what are not?","['statistics', 'convex-optimization']"
2792385,derivative of the determinant of the sum of two matrices,"From Eq. 51 of the matrix cookbook we know that $\frac{\partial \log\det (AXB)}{\partial X} = (X^{-1})^\top$, where $\det(X)$ is the determinant of $X$. I was wondering what is the derivative of
$\frac{\partial \log\det (AXB + C)}{\partial X}$. Is it still  $ (X^{-1})^\top$? Thanks!","['matrices', 'trace', 'matrix-calculus', 'derivatives']"
2792390,Derivative of Euclidean norm (L2 norm),"Let:$x=[x_1, x_2]$ and $y = [y_1, y_2]$ What is the derivative of the square of the Euclidean norm of $y-x $? I'm not sure if I've worded the question correctly, but this is what I'm trying to solve: $$
\frac{d}{dx}(||y-x||^2)
$$ It has been a long time since I've taken a math class, but this is what I've done so far: $$
\frac{d}{dx}(||y-x||^2)=\frac{d}{dx}(||[y_1,y_2]-[x_1,x_2]||^2)
$$
Subtracting $x $ from $y$:
$$
\frac{d}{dx}(||y-x||^2)=\frac{d}{dx}(||[y_1-x_1,y_2-x_2]||^2)
$$
Taking the norm:
$$
\frac{d}{dx}(||y-x||^2)=\frac{d}{dx}((y_1-x_1)^2+(y_2-x_2)^2)
$$
Then at this point do I take the derivative independently for $x_1$ and $x_2$?
This is where I am guessing:
$$
\frac{d}{dx}(||y-x||^2)=[\frac{d}{dx_1}((y_1-x_1)^2+(y_2-x_2)^2),\frac{d}{dx_2}((y_1-x_1)^2+(y_2-x_2)^2)]
$$
Which would result in:
$$
\frac{d}{dx}(||y-x||^2)=[2x_1-2y_1,2x_2-2y_2]
$$ Is this correct? Thank you for your time.","['derivatives', 'normed-spaces']"
2792428,"let $f(x) = (x-1)\ln x$, and given $0 < a < b$. If $f(a) = f(b)$, prove that $\frac{1}{\ln a}+\frac{1}{\ln b} < \frac{1}{2}$","Let $f(x) = (x-1)\ln x$ , and given $0 < a < b$ . If $f(a) = f(b)$ , how to prove that $\frac{1}{\ln a}+\frac{1}{\ln b} < \frac{1}{2}$ ?","['algebra-precalculus', 'inequality']"
2792437,"Evaluating definite integral $\int_0^{2\pi} \frac{1}{13-5\sin\theta}\,\mathrm{d}\theta$","Question : $$\int_0^{2\pi} \frac{1}{13-5\sin\theta}\,\mathrm{d}\theta$$ is equals to (a) $-\frac\pi6$ (b) $-\frac{\pi}{12}$ (c) $\frac\pi{12}$ (d) $\frac\pi6$ My attempt : Denoting given integral by $I$ and letting $z=e^{iθ}$ then given integral becomes, \begin{align*}
I&=\int_C\frac{1}{13-5(\frac{z-\bar{z}}{2i})}\frac{\mathrm dz}{iz}\\
&=\frac{1}{i}\int_C\frac{2i}{26iz-5z^2+5|z|^2}\mathrm dz\\
&=2\int_C\frac{\mathrm dz}{-5z^2+26iz+5}\hspace{0.5in}\text{As }C: |z|=1\\
&=2\int_C \frac{\mathrm dz}{(z-5i)(z-i/5)}\\
&=2\left(\frac{5}{24i}\int_C\frac{1}{z-5i}-\frac{5}{24i}\int_C \frac{1}{z-i/5}\right)
\end{align*} Now as point $z=5i$ lies outside $C$ so it's integral evaluates to $0$ and by Cauchy integral formula, above becomes, $$I=0-2\frac{5}{24i}2\pi i = -\frac{5\pi}{6}$$ But none of the given answer matches with mine. So is am i incorrect? Please help me..stuck on this from hours...","['cauchy-integral-formula', 'complex-analysis', 'definite-integrals', 'line-integrals']"
2792438,Locus of centre of ellipse sliding along coordinate axes?,"An ellipse of major and minor axes of length √3 and 1 respectively, slides along the co-ordinate axes and always remains confined in the first quadrant. The locus of the centre of the ellipse will be the arc of a circle the length of which is: The answer given is 0.52. In the solution it is given that locus of centre is $x^2+y^2=1$ and difference in parametric angles at two extreme positions is π/6. (This is because the two extreme positions are $tan^{-1} (π/6)$ and $tan^{-1} (π/3)$). I am actually having difficulty understanding why locus of centre is $x^2+y^2=1$. I understand that at the two extreme positions(major axis parallel to x axis and y axis respectively) the distance of the centre from the origin is 1 ($1^2 = (√3/2)^2 + (1/2)^2$), but what about the other positions in between these two? I only have an intuition about the fact that the distance of centre from origin remains 1 as ellipse slides, and would welcome some clearer guidance.","['analytic-geometry', 'geometry']"
2792441,Evaluate $\lim\limits_{n\rightarrow \infty}\frac{n+n^2+n^3+\cdots +n^n}{1^n+2^n+3^n+\cdots +n^n}.$,"Problem Evaluate $$\lim\limits_{n\rightarrow \infty}\frac{n+n^2+n^3+\cdots +n^n}{1^n+2^n+3^n+\cdots +n^n}.$$ My solution Notice that $$\lim_{n \to \infty}\frac{n+n^2+n^3+\cdots +n^n}{n^n}=\lim_{n \to \infty}\frac{n(n^n-1)}{(n-1)n^n}=\lim_{n \to \infty}\frac{1-\dfrac{1}{n^n}}{1-\dfrac{1}{n}}=1,$$ and $$\lim_{n \to \infty}\frac{1+2^n+3^n+\cdots+n^n}{n^n}=\frac{e}{e-1}.$$ Hence, \begin{align*}\lim\limits_{n\rightarrow \infty}\frac{n+n^2+n^3+\cdots +n^n}{1^n+2^n+3^n+\cdots +n^n}&=\lim_{n \to \infty}\frac{\dfrac{n+n^2+n^3+\cdots +n^n}{n^n}}{\dfrac{1+2^n+3^n+\cdots +n^n}{n^n}}\\&=\frac{\lim\limits_{n \to \infty}\dfrac{n+n^2+n^3+\cdots +n^n}{n^n}}{\lim\limits_{n \to \infty}\dfrac{1+2^n+3^n+\cdots +n^n}{n^n}}\\&=1-\frac{1}{e}.\end{align*} The solution posted above need to quote an uncommon limit. Is there another more simple and more direct solution?","['real-analysis', 'limits', 'calculus', 'sequences-and-series', 'geometric-series']"
2792443,Why is the fiber of finite type homomorphism Noetherian?,Suppose $f:R\to S$ is a homomorphism of rings of finite type. Then why is that the fibers of $f^*:\mathrm{Spec}~S\to\mathrm{Spec}~R$ a Noetherian subspace of $\mathrm{Spec}~S$? Does this have any geometric meaning? The fiber of $f^*$ over a point $\mathfrak{p}$ is $\mathrm{Spec}~S_{\mathfrak{p}}/\mathfrak{p}S_{\mathfrak{p}}$.,"['algebraic-geometry', 'commutative-algebra']"
2792610,Fixed points sets sine of the topologist,"Consider the sine of the topologist set $X:=\{(x,\sin(1/x)):x\in (0,1]\}\cup \{(0,y):y\in [-1,1]\}$, in the euclidean space $\mathbb{R}^{2}$. It is not very hard to show, see this proof , that every continuous $f:X\longrightarrow X$ has some fixed point. My question is the following: Can we find a closed subset of $X$, say $C$, such that $C$ is not the fixed point set of any continuous $f:X\longrightarrow X$? Intuitively, it i seems that such set $C$ could be of the form $C_{1}\cup C_{2}$ with $C_{1}$ and $C_{2}$ closed sets of the arc-wise connected component of $X$, what do you think? Many thanks in advances for your comments.","['fixed-points', 'general-topology']"
2792618,triviality of etale fundamental group vs triviality of topological fundamental group,"Let $k$ be a number field with a fixed embedding to $ \mathbb{C}$. If $V$ is an algebraic variety over $k$ and $V_\mathbb{C} = V \times_k \mathbb{C}$ has nice enough properties one can show that the profinite completion of the topological fundamental group $\pi_1(V(\mathbb{C}))$ is the etale fundamental group $\pi^{et}_1(V_\mathbb{C})$. I'm omitting basepoints for simplicity. My question is: Is there an example of a smooth projective geometrically connected algebraic variety $V$ over $k$ such that $\pi_1(V(\mathbb{C})) \neq 0$ and $\pi^{et}_1(V_\mathbb{C}) = 0$? I suspect the answer is yes based on the example of the unit circle, which has $\mathbb{Z}$ as topological fundamental group and has trivial profinite completion.","['algebraic-topology', 'arithmetic-geometry', 'fundamental-groups', 'algebraic-geometry']"
2792664,How does taking a real part of a complex matrix change the eigenvalues?,"If $A\in \mathbb{C}^{n\times n}$ with eigenvalues $(\mu_1,\ldots,\mu_n)$, is there anything we can say about the eigenvalues of $T = \Re(A)$; let's call them $(\lambda_1,\ldots, \lambda_n)$? Especially, does it hold that $|\lambda_i|\leq|\mu_i|$? The last part holds for the largest eigenvalue via a norm argument, but I can't really come up with anything for the other eigenvalues.","['eigenvalues-eigenvectors', 'linear-algebra']"
2792722,Continuity of support functionals,"Let $X$ be a normed space(not necessarily Banach) and let $G$ be a $w^*-$compact subset of $X^*,$ the dual space of $X.$ Consider the function $f:X\to \Bbb R$ defined by $$f(x)= \max_{g^*\in G}\{\langle g^*,x\rangle\}.$$ Is $f$ continuous? If not, what about lower semicontinuous? Could you present a example of a discontinuous $f$? I can prove that if $X$ is Banach the function is Lipschitz(by using the Uniform Boundedness Theorem). However, in the general case I am still curious.","['optimization', 'convex-analysis', 'functional-analysis', 'continuity', 'convex-optimization']"
2792741,Classification of groups with integer valued characters,"I am interested if there is a good classification of finite groups whose characters are all integer valued. One can prove the follow result using Galois Theory: A group $G$ has the property that $\chi(g) \in \mathbb{Z} \ \forall g\in G,$ and  characters $\chi$ of $G$ if and only if $g^{n}$ is conjugate to $g$ for all $g \in G$ and $n \in \mathbb{Z}$ with $n$ and $\operatorname{o}(g)$ coprime. (where $\operatorname{o}(g)$ denotes the order of $g$ in $G$) An example of such a group is the symmetric group $S_{n}$ for any $n$. So my request would be equivalent to asking which finite groups have the property that $g^{n}$ is conjugate to $g$ for all $g \in G$ and $n \in \mathbb{Z}$ with $n$ and $\operatorname{o}(g)$ coprime.","['representation-theory', 'galois-theory', 'group-theory']"
2792750,Are the partial derivatives of a mollifier bounded?,"More precisely, I am wondering whether for any mollifier $ \varphi : \mathbb{R}^n \to \mathbb{R} $ there exists a constant $ M $ such that $ \frac{\partial \varphi}{\partial x^i} (x) \leq M $ for all $ x \in \mathbb{R}^n $. Recall, a mollifier $ \varphi $ is a function which satisfies the following conditions: $ \varphi \in C^{\infty}_{0}(\mathbb{R}^n) $, with $ \mathrm{supp} = \{ x \in  \mathbb{R}^n : \lvert x\rvert \leq 1 \} $; $ \varphi > 0 $; $ \int_{\mathbb{R}^n} \varphi = 1 $. – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – This question occurred to me whilst trying to prove the following result: Let $ \varphi : \mathbb{R}^n \to \mathbb{R} $ be a mollifier and define $ \varphi_{\epsilon} := \epsilon^{-n} \varphi(x/\epsilon) $, so that the defining properties of a mollifier listed above automatically imply that $ \varphi_{\epsilon} \in C^{\infty}_{0}(\mathbb{R}^n) $, $ \mathrm{supp} = B(0,\epsilon) $ and $ \int_{\mathbb{R}^n} \varphi_{\epsilon} = 1 $. Furthermore let $ u: \mathbb{R}^n \to \mathbb{C} $ be a locally integrable function and define $ u_{\epsilon} $ to be the convolution $ u_{\epsilon} := u \ast \varphi_{\epsilon} $. Then the function $ u_\epsilon : \mathbb{R}^n \to \mathbb{C}$ is $ C^{\infty} $. My attempt of proving this result begins as follows: Let $ e_i $ be the $i$'th unit vector of the standard basis of $ \mathbb{R}^n $, that is, $ e_i = (e_i^1, ..., e_i^n) $ with $ e_i^j = \delta_{ij} $, where the latter is the Kronecker delta. Then, for a fixed $ x \in \mathbb{R}^n $, we have \begin{align*}
\frac{\partial u_{\epsilon}}{\partial x^i} (x) &= \frac{\partial}{\partial x^i} (u \ast \varphi_{\epsilon}) (x) \\
&= \lim_{h \to 0} \frac{1}{h} \left\{ (u \ast \varphi_{\epsilon})(x + h e_i) - (u \ast \varphi_{\epsilon})(x) \right\} \\
&= \lim_{h \to 0} \int_{\mathbb{R}^n} u(y) \left\{ \frac{\varphi_{\epsilon}(x-y+he_i) - \varphi_{\epsilon}(x-y)}{h} \right\} \mathrm{d}y .
\end{align*} Now the most elegant way to reach the desired conclusion would probably be to use the mean value theorem , by which there must exist a constant $ 0 < c(h) < h $ such that \begin{align*}
\lim_{h \to 0} \int_{\mathbb{R}^n} u(y) \left\{ \frac{\varphi_{\epsilon}(x-y+he_i) - \varphi_{\epsilon}(x-y)}{h} \right\} \mathrm{d}y &= \lim_{h \to 0} \int_{\mathbb{R}^n} u(y) \: \frac{\partial \varphi_{\epsilon}}{\partial x^i} (x-y+c(h)) \: \mathrm{d}y \\
&= \lim_{h \to 0} \left(u \ast \frac{\partial \varphi_{\epsilon}}{\partial x^i}\right) (x+c(h)) .
\end{align*} It is not hard to show that the convolution of a locally integrable function with a continuous function of compact support is continuous and thus we obtain \begin{align*}
\frac{\partial u_{\epsilon}}{\partial x^i} (x) &= \lim_{h \to 0} \left(u \ast \frac{\partial \varphi_{\epsilon}}{\partial x^i}\right) (x+c(h)) \\
&= \left(u \ast \frac{\partial \varphi_{\epsilon}}{\partial x^i}\right) (x) .
\end{align*} I began wondering, however, if there is an alternative to the application of the mean-value theorem. If one could show, for instance, that the integrand $ \left\{ u(y) \left( \frac{\varphi_{\epsilon}(x-y+he_i) - \varphi_{\epsilon}(x-y)}{h} \right) \right\} $ satisfies the conditions of Lebesgue's dominated convergence theorem , then one could interchange the limit and integral in the first equation above to immediately obtain that $ \frac{\partial u_{\epsilon}}{\partial x^i} (x) = \left(u \ast \frac{\partial \varphi_{\epsilon}}{\partial x^i}\right) (x) $. But the difficulty lies in showing that the integrand is dominated by an $ L^1 $ function. However, if we knew that the partial derivatives of $ \varphi $ (and thus of $ \varphi_\epsilon $) were bounded, then the continuity of these partial derivatives would imply (via the mean value theorem I believe...) that for all $ h \leq \delta $, where $ \delta > 0 $ is some constant, there exists a constant $ c > 0 $ such that for all $ y $ \begin{align}
\left \lvert \frac{\varphi_{\epsilon}(x-y+he_i) - \varphi_{\epsilon}(x-y)}{h} - \frac{\partial \varphi_{\epsilon}}{\partial x^i}(x-y) \right \rvert \leq c ,
\end{align} (remember $ x $ was fixed) and so \begin{align}
\frac{\partial \varphi_{\epsilon}}{\partial x^i}(x-y) - c \leq \frac{\varphi_{\epsilon}(x-y+he_i) - \varphi_{\epsilon}(x-y)}{h} \leq \frac{\partial \varphi_{\epsilon}}{\partial x^i}(x-y) + c .
\end{align} Now just let $ M := \max \left\lvert \frac{\partial \varphi_{\epsilon}}{\partial x^i}(x-y) - c \pm \right\rvert $. Then \begin{align}
\left\lvert u(y) \left\{ \frac{\varphi_{\epsilon}(x-y+he_i) - \varphi_{\epsilon}(x-y)}{h} \right\} \right\rvert
\leq
M \: \Bigr\lvert \: u\bigr\vert_{supp\left( \varphi_{\epsilon}(x+\delta e_i - \: \cdot) \right) \cap supp\left( \varphi_{\epsilon}(x- \: \cdot) \right)}(y) \Bigr\rvert .
\end{align} Hence the integrand is bounded by an integrable function. This is how my question originated.","['real-analysis', 'partial-derivative', 'convolution', 'integration', 'analysis']"
2792754,"Does unit sphere $S$ homeomorphic to $[0,1]$","Let $(\mathbb R^2, \|\cdot\|)$ be two dimensional normed space and $S=\{x\in\mathbb R^2 : \|x\|=1\}$ the unit sphere of $(\mathbb R^2, \|\cdot\|)$. Since $S$ bounded and closed  in $\mathbb R^2$ then it is compact. Does $S$ homeomorphic to the segment $[0,1]$ with usual topolgy? And if so then how to construct such an homeomorphism?","['functional-analysis', 'normed-spaces', 'general-topology', 'compactness']"
2792758,examples of random variables that are not measurable,"Let $(\Omega,\mathcal{F},\mathbb{P})$ a probability space and $(\Omega',\mathcal{F}')$ a measurable space. Furthermore let
\begin{equation}
X:(\Omega,\mathcal{F},\mathbb{P}) \rightarrow (\Omega',\mathcal{F}')
\end{equation}
be any function. Can someone give examples where $X$ is not measurable?","['probability-theory', 'measure-theory', 'random-variables']"
2792816,Equivalent definition of proper action with sequences,"I have to prove the following : Let G be a metric group acting continuously on X a metric space. Then the following are equivalent :
(a) $G\curvearrowright X$ is proper. (b) For all compact $K\subseteq X$ the set $C= \{g;g\cdot K\cap K\neq\emptyset\}\subseteq G$ is compact. (c) For all converging sequences $(x_{n})_{n\in\mathbb{N}}\in X^{\mathbb{N}}$, for all diverging sequences $(g_{n})_{n\in\mathbb{N}}\in G^{\mathbb{N}}$, the sequence $(g_{n}\cdot x_{n})_{n\in\mathbb{N}}\in X^{\mathbb{N}}$ diverges. I have managed to prove (a) $\Leftrightarrow$ (b), but I am stuck when it comes to prove (b) $\Rightarrow$ (c) or (c) $\Rightarrow$ (b). I don't see how the properness and the convergence of sequences are linked (I know that in a metric space, a definition of compactness is that any sequence has a converging subsequence, but I don't see how it helps). I know that G and X are $\sigma$ locally compact because of the definition of a proper action. Since X is a metric space it is in particular hausdorff and hence any compact subspace of X is closed as well. I tried to do it by contradiction. I assumed that the sequence $g_{n}.x_{n}$ was converging and then constructed a compact set $K \subset X$ so that $g_{n}\in C \forall n$. But I don't know how to do conclude from here that C is not compact from there. Thank you very much beforehand for your help","['group-actions', 'differential-geometry', 'lie-groups']"
2792822,Classifying singularities of $\frac {z^{1/2}-1}{\sin{\pi z}}$,"I am trying to classify the singularities of $$\frac {z^{1/2}-1}{\sin{\pi z}}$$
where $-\pi<\arg z<\pi$. I am confused by this because of the branch cut of $\sqrt z$ but here is my (bad) attempt: The singularities are at $z = n$ for $n \in \mathbb Z$. We have a branch point at $z=0$ so this is a non-isolated singularity (not a simple pole, as I almost said). When $z$ is a negative integer the function is not even defined so I suppose these are not considered singularities? For $n=1$ it is removable since $1$ is a simple zero of both the numerator and denominator - the derivative of $z^{1/2}-1$ is $z^{-1/2}/2$ which is non-zero at $z=1$ so it must be a simple zero. Is that right? Then for $n>1$ we just have simple poles.","['complex-analysis', 'singularity', 'branch-points', 'branch-cuts']"
2792827,Maximize $\det (A)$ subject to $\|A\|_{\text{F}} \le 1$,"$A$ is an $n \times n$ matrix such that the sum of squares of its elements is less than or equal to 1. What is $\max \det (A)$ ? (a) $n = 2$ (b) $n = 3$ My partial solution For $n = 2$ , I think the answer is 0.5. Define the constrained maximisation problem as $$\max(a_{11}a_{22}-a_{21}a_{12})$$ such that $$a_{11}^2+a_{12}^2+a_{21}^2+a_{22}^2 \le 1$$ If $a_{21}=a_{12}=0$ , the maximum $0.5$ is achieved at $a_{11}=a_{22}=1/\sqrt{2}$ . If $a_{21} \ne 0, a_{12}\ne0$ , the maximum $0.5$ is achieved at $a_{11}=a_{22}=a_{21}=0.5$ and $a_{12}=-0.5$ But how to I prove that formally? And how to I tackle the more difficult case of $n=3$ ?","['matrices', 'optimization']"
2792828,How to derive the approximation $\tan(x)\simeq \frac{x}{1-x^2/3}$,"I was wondering how one could derive the $$\tan(x)\simeq \frac{x}{1-x^2/3}$$ valid for small $x$ values. This is similar to the ratio of the small $x$ expansions of $\sin(x)$  an $\cos(x)$, however that would yield $$\tan(x)\simeq \frac{x}{1-x^2/2}$$ so I have been left slightly confused. Many thanks in advance. EDIT: In my notes this seems to be some sort of recursive fraction approximation. A second version I have written is: $$
\tan(x)\simeq 
%%%
\frac{\lambda}
{1-\frac{\lambda^2}{3-\frac{\lambda^2}{5-\lambda^2/2}}}
$$ EDIT: Thanks for your great answers! You can also find this derived in the references to Equation 33 here: http://mathworld.wolfram.com/Tangent.html Wall, H. S. (1948). Analytic theory of continued fractions. pg. 349 C.D., O. (1963). Continued fractions. pg. 138","['trigonometry', 'approximation']"
2792896,How to evaluate $\int_{-\infty}^{\infty}\frac{x^2e^x}{\left(1 + e^x\right)^2}dx$,"How to evaluate the following integral: $$\int_{-\infty}^{\infty}\frac{x^2e^x}{\left(1 + e^x\right)^2}dx$$ So far I know that this function is even, so we can take $\int_{0}^{\infty}\frac{x^2e^x}{\left(1 + e^x\right)^2}dx$ or $\int_{-\infty}^{0}\frac{x^2e^x}{\left(1 + e^x\right)^2}dx$  and then multiply by 2. If we substitute $x = \ln z$ then $\int_{-\infty}^{0}\frac{x^2e^x}{\left(1 + e^x\right)^2}dx = \int_{0}^{1}\frac{\ln^2z}{\left(1 + x\right)^2}dz$ And I don't know what to do next.","['integration', 'definite-integrals', 'calculus']"
2792901,A group of order 20,Let $G$ be a group of order $20$ and its class equation is given by $$1+4+5+5+5$$ My question is whether Sylow-$2$ subgroup of $G$ is normal or not. By Sylow theorem we know that the Sylow-$2$ subgroup will be normal if and only if there is only one Sylow-$2$ subgroup inside $G$. Sylow theorem says that number Sylow-$2$ subgroup is $2k+1$ where $2k+1$ divides $5$. Only $1$ and $5$ are possible. How to decide what will be the case here. Thank you!,"['abstract-algebra', 'group-theory', 'sylow-theory']"
2792926,An holomorphic function with zero real or imaginary part is constant,"I'm trying to prove this proposition: Let $f:\Omega \to \mathbb{C}$, $\Omega \subset \mathbb{C}$ a domain, such that $Re(f)=0$ or $Im(f)=0$. Prove that if $f$ is holomorphic on $\Omega$, then $f$ is constant on that domain. So I see that it's true, because I know that holomorphic $\implies$ analytic $\implies$ $C^1(\Omega)$. So we can say that $\frac{\partial f}{\partial \bar{z}}=0$, and then use CR equations. But I haven't been proved the first implication yet, so I can't use the fact that we know that $f$ is $C^1$. How can we prove this then? Thanks for your time.","['derivatives', 'complex-analysis', 'calculus']"
2792930,Sequential almost sure continuity implies almost sure continuity,"Let $(B_t)_{t\in\mathbb{R}}$ be a real stochastic process. $\forall\omega\in\Omega$, let $f_\omega(t) = B_t(\omega)$. $\forall x\in\mathbb{R}$, $(B_t)_{t\in\mathbb{R}}$ is almost surely continuous at $x$ iff $f_\omega$ is continuous at $x$ for almost all $\forall\omega\in\Omega$. Define sequential almost sure continuity at $x$ this way: $\forall (x_n)_n\subseteq\mathbb{R}\setminus\{x\}$, such that $x_n \rightarrow x$, it's almost sure that $B_{x_n} \rightarrow B_x$. $\forall x\in\mathbb{R}$, is it true that sequential almost sure continuity at $x$ implies almost sure continuity at $x$? If not, then also assume that, $(B_t)_{t\in\mathbb{R}}$ is almost surely continuous on $\mathbb{R}\setminus\{x\}$. That is, $f_\omega$ is continuous on $\mathbb{R}\setminus\{x\}$ for almost all $\forall\omega\in\Omega$. Would that be enough to show almost sure continuity at $x$?","['stochastic-processes', 'real-analysis', 'probability-theory']"
2792953,Integrate $\int \frac{dx}{\sqrt{(x-a)(b-x)}}$,"Integrate $\int \frac{dx}{\sqrt{(x-a)(b-x)}}$ where $b>a$ My Attempt $$
\begin{align}
&\int \frac{1}{\sqrt{(x-a)(b-x)}}dx=\int\frac{dx}{\sqrt{-x^2+(a+b)x-ab}}\\
&=\int\frac{dx}{\sqrt{-(x^2-2.\frac{a+b}{2}x+ab+\frac{(a+b)^2}{4}-\frac{(a+b)^2}{4})}}\\
&=\int\frac{dx}{\sqrt{\frac{a^2+b^2+2ab}{4}-ab-\big[x-\frac{a+b}{2}\big]^2}}=\int\frac{dx}{\sqrt{\frac{(a-b)^2}{4}-\big(x-\frac{a+b}{2}\big)^2}}\\
&=\sin^{-1}\frac{x-\frac{a+b}{2}}{\frac{b-a}{2}}+C\color{red}{=\sin^{-1}\frac{2x-(a+b)}{b-a}+C}
\end{align}
$$
My reference shows the solution $I=2\sin^{-1}\sqrt{\frac{x-a}{b-a}}$, yet why am I getting a different solution or both the same ? Thanx @lab bhattacharjee for the hint. $$
\begin{align}
\sin^{-1}\frac{2x-(a+b)}{b-a}+C&=\frac{\pi}{2}-\cos^{-1}\frac{2x-(a+b)}{b-a}+C\\&=\frac{\pi}{2}-\pi+\cos^{-1}\frac{(a+b)-2x}{b-a}+C_1\\
&=\cos^{-1}\frac{(a+b)-2x}{b-a}+C_2
\end{align}
$$
Let,
$$
y=\cos^{-1}\frac{(a+b)-2x}{b-a}\implies\cos y=\frac{(a+b)-2x}{b-a}\\
2\sin^2\frac{y}{2}=1-\cos y=1-\frac{(a+b)-2x}{b-a}=\frac{b-a-a-b+2x}{b-a}=\frac{2x-2a}{b-a}\\
\sin^2\frac{y}{2}=\frac{x-a}{b-a}\implies \sin\frac{y}{2}=\sqrt{\frac{x-a}{b-a}}\\
\frac{y}{2}=\sin^{-1}\sqrt{\frac{x-a}{b-a}}\implies\boxed{y=2\sin^{-1}\sqrt{\frac{x-a}{b-a}}}
$$","['trigonometry', 'inverse-function', 'calculus', 'indefinite-integrals', 'integration']"
2792978,exponentiating the natural numbers,"I'm not very knowledgeable on number theory, but the other day, I was thinking about this problem: Given any integer number $N>0$ which is not a power of $10$, there
  exists a positive integer $M$ such that $N^M$ contains all the digits
  from $0$ to $9$. In other words, if I take a number, say $2$, the smallest power of $2$ that contains all the decimal digits is $2^{68} = 295147905179352825856$.  A particularly nice example is $32043^2 = 1026753849$, the result containing all decimal digits each exactly once. Is this conjecture true?  Does anyone know if this has already been proven true/false?","['number-theory', 'diophantine-approximation', 'elementary-number-theory']"
2792994,Certain generators for the normalization of the cuspidal cubic $x^3=y^2$,"Let $X=V(X^3-Y^2)$ the cuspidal cubic over an algebraically closed field $k$. This variety $X$ has a singularity at $(0,0)$ and this fact is reflected in the coordinate ring as $A=k[X,Y]/(X^3-Y^2)=k[x,y]$ being not normal. The normalization of this ring (i.e the integral closure in the fraction field) is given by $A[\frac{y}{x}]$ as is proved here . I want to write $A[\frac{y}{x}]$ as a quotient of the polinomial ring $A[Z]$ by an ideal $I$ in order to find an embedding of $\text{spec}(A[\frac{y}{x}])$ in $\mathbb{A}^3$ (given by $V(I)$) and a natural map $\text{spec}(A[\frac{y}{x}])\rightarrow \text{spec}(A)=X$ induced by $A\rightarrow A[Z]/I$. For this I need to compute the kernel of the map $A[Z]\rightarrow A[\frac{y}{x}]$ given by $z\mapsto \frac{y}{x}$ but I can't do this. I have noticed that this kernel contains the ideal $(Zx-y,Zy-x^2)$ but I think this is not the entire kernel because the variaty $V(X^3-Y^2,ZX-Y,ZY-X^2)$ is still singular at 0. Does someone has any idea in how to proceed here? Thanks in advance.","['algebraic-geometry', 'commutative-algebra']"
2792997,Conjugacy class with $1$ element in an Infinite Simple Group,"We would like to show that if $G$ is an infinite simple group, then
  the only conjugacy class of exactly one element is $\{1_G\}$. My thoughts : We want to proove that if $|\mathrm{orb}(x)|=1\iff \mathrm{orb}(x)=\{x\}$ then $x=1_G$. We know that $|\mathrm{orb}(x)|=1\iff x\in Z(G)$. But, $Z(G)\trianglelefteq G$ and $G$ is simple, so $Z(G)$ is $G$ or $\{1_G\}$. Also, from the Orbit-Stabilizer Theorem $|G|=|C_G(x)|=\infty$. But, I'm afraid these don't help as. Any ideas please? Thank you.","['abstract-algebra', 'infinite-groups', 'group-theory', 'simple-groups']"
2793021,A distribution as a linear combination of Dirac delta and its derivatives,"This problem is from Exercise 1.13.22, Tao's An Epsilon of Room . Let $\lambda$ be a distribution whose domain is the set of compactly supported smooth functions on $\mathbb{R}$. (i) Show that if $\lambda$ is a distribution and $n\ge 1$ is an integer, then $\lambda x^n=0$ iff $\lambda$ is a linear combination of $\delta$ and its first $n-1$ derivatives $\delta', \delta'', ...,\delta^{(n-1)}$. (ii) Show that a distribution $\lambda$ is supported on ${0}$ iff it is a linear combination of $\delta$ and finitely many of its derivatives. My attempt: For both (i) and (ii), the 'if' parts are easy, and I already did them easily. The problems are the 'only if' parts. For the 'only if' part of (i), I let $a_i = (-1)^i \dfrac{\lambda(x^i)}{i!}$, and tried to prove that $\lambda= \sum_{i=0} ^{n-1}a_i \delta^{(i)}$, but I don't know how to proceed further. For the 'only if' part of (ii), I don't have any idea to proceed. Does anyone have ideas? Any hints or advices will help a lot! Thanks!","['functional-analysis', 'real-analysis', 'distribution-theory']"
2793030,Higher order factorials.,"The function $y=x!$ can be drawn using the gamma functions, what could function would give higher order factorials like $y=x!!$ where $x!! = x\cdot(x-2)\cdot(x-4)\cdot\ldots\cdot5\cdot3\cdot1$ . I have this: $$\prod\limits_{n=0}^{\lfloor x/k\rfloor-1}(x-kn)$$ Where k is the amount of !'s but obviously this is only valid for integer k, but I'm looking for something which would make would make sense and be continuous for non-integers.","['factorial', 'gamma-function', 'functions']"
2793040,An example of a compact topological space which is not the continuous image of a compact Hausdorff space?,"The definition of the compact-open topology differs slightly depending on whether one is working in the context of compactly generated topological spaces or arbitrary topological spaces. If $X$ and $Y$ are topological spaces, then the compact-open topology on the set of continuous functions $C(X,Y)$ has, as a sub-basis, subsets of the form $V(K,U)$ where $K$ is a compact subset of $X$, $U$ is an open subset of $Y$, and $V(K,U):= \{f\in C(X,Y)\ \vert \ f(K)\subseteq U\}$. When working with compactly generated topological spaces, this definition is modified to only allow compact sets $K$ which are the image of a compact Hausdorff space (see https://en.wikipedia.org/wiki/Compact-open_topology ). This suggests that not every compact space is the continuous image of a compact Hausdorff space. What is an example of such a space?","['general-topology', 'compactness']"
2793098,"Reasoning for the function $f(x,y)=x^{2}e^{y}$ is differentiable on $\mathbb{R}^{2}$.","Reasoning for the function $f(x,y)=x^{2}e^{y}$ is differentiable on $\mathbb{R}^{2}$. Explanation : the correct reason should be $f_{x}=2xe^{y}$ and $f_{y}=x^{2}e^{y}$ are both continuous on $\mathbb{R}^{2}$ , then $f$ is differentiable at every point on $\mathbb{R}^{2}$. But my friend told me : $f_{x}$ and $f_{y}$ both exist at each point $(x,y)\in\mathbb{R}^{2}$ and for all point $(x,y)\in\mathbb{R}^{2}$, we have $$f_{xy}=2xe^{y}=f_{yx}$$ Hence , $f$ is differentiable  on $\mathbb{R}^{2}$. I confused . Is this reason valid for testing function differentiable  ? Or is there a counterexample for this ?","['derivatives', 'real-analysis', 'partial-derivative', 'calculus', 'multivariable-calculus']"
2793150,Why is the Jacobian determinant of a map the reciprocal of the Jacobian determinant of the inverse map?,"Consider a mapping $g: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ \begin{align}
g(u,v) &= (x, y)\\
&=(f(u,v), \;g(u, v))
\end{align} Consider the inverse mapping $g^{-1} = \mathbb{R}^2 \rightarrow \mathbb{R}^2$ \begin{align}
g^{-1}(x,y) &= (u,v) \\
&= (h(x,y), \;I(x,y))
\end{align} Question Why is it that the determinant of the Jacobian of $g$ denoted as $|\text{Jac}\; g|$ equal to $1$ over the determinant of the Jacobian of $g^{-1}$ denoted $|\text{Jac}\; g^{-1}|$? $$ |\text{Jac}\;g| = \frac{1}{|\text{Jac}\;g^{-1}|}$$
You could do the computation. But I'm not sure how I would connect the two sides of the equality. The left side is $x_uy_v - y_ux_v$. But I can't seem to get this equal to the other side. Thoughts Consider a function $y = f(x)$. Consider it's inverse function $x = f^{-1}(y)$. The two are related as follows $$\frac{df^{-1}}{dy}\Bigg|_y = \frac{1}{\frac{df}{dx}\Big|_x} $$ where the evaluation values $x,y$ are images of one another (people write this formula in many different ways). This last equation makes intuitive sense to me. Draw a graph of $f(x)$. To find the graph of the inverse function, all you have to do is rotate this graph $90^\circ$ counterclockwise. If you draw a tangent line to the graph of $f(x)$, such an instantaneous line has some rise/run. Once you rotate the graph $90^\circ$ counterclockwise to view the inverse graph, this tangent line now has a slope of run/rise. Therefore the formula follows. Is there such an interpretation for my question? I tried writing out both determinants. However, it's not clear to me how I would related $\partial x/\partial u$ to $\partial u/\partial x$. You would think that $$\frac{\partial x}{\partial u}\Bigg|_u = \frac{1}{\frac{\partial u }{\partial x}\Big|_x} $$ This would be true given $x = f(u)$. However we have $x = f(u,v)$ and $u = h(x,y)$ and I'm not sure of the relationship between the two. Thanks in advance","['matrices', 'jacobian', 'multivariable-calculus', 'determinant']"
2793163,Find new first integral of DE System,"I am trying to find two first integrals of the following system:
$$\begin{cases}x' = x+z\\
y'= y \\
z'=z + y^2 \end{cases}$$
A first integral can be found considering only the system in $\mathbb{R}^2$ for $y$ and $z$. Using the integrating factors $\mu_1 = \frac{1}{y^2}$ and $\mu_2 = \frac{1}{(z-y^2)^2}$ we can obtain the function $$H_1(y,z) = \sqrt{\frac{\mu_1}{\mu_2}}=\frac{z}{y} -y$$ which is a first integral of the $y,z$ system, hence a first integral of the system in $\mathbb{R}^3$. Now I'm trying to get another first integral, functionally independent to $H_1$, but can't really find a way to do it unless I use PDE's, which I want to avoid. Any help on how to get it will be appreciated! Thanks. EDIT: by first integral I refer to a function $H(x,y,z)$ such that $$H' = \left<\nabla H, (x',y',z') \right> = 0.$$","['ordinary-differential-equations', 'dynamical-systems']"
2793166,when is a longer series BAD for the better team?,"This question is inspired by: A longer series is better for a better team: Can you see this at a glance? Also obviously inspired by the NBA playoffs happening right now.  :) Suppose two teams are playing a series of $2k-1$ games, and the first team to win $k$ games wins the overall series.  (No game can end in a tie.)  Moreover, team A is ""better"" than team B. Let $A_i$ denote the event that team A wins game $i$. Let $A_{series}$ denote the event that team A wins the series, i.e., A wins $k$ or more games. If the game results are i.i.d., and $P(A_i) = p > 1/2$ for any game $i$, then a longer series (larger $k$) increases A's chance of winning the series, i.e. $P(A_{series})$ is an increasing function in $k$.  This is intuitively obvious, and a proof can be found in the link above (although that post asks an excellent question re: why such an ""obvious"" result requires an algebraically convoluted proof). I wanna know under what conditions, i.e. under what probability model, would a longer match be BAD for the better team A.  Perhaps some dependence that encodes ""reversion to the mean"" and/or (opposite of) ""momentum""? What I tried My $0$-th attempt: If we allow ""fatigue"" in the form of decreasing $P(A_i),$ then a longer series can be bad for A, even when all $P(A_i) > 1/2.$  A simple 3-game example: if $P(A_1) = 1, P(A_2) = P(A_3) = 0.51$, then A always wins in a 1-game ""series"" ($k=1$) but B has a chance ($0.49^2$) in a 3-game series ($k=2$).  @Henry in his answer gave an infinite length example. So I'm looking for something where the marginal probabilities $P(A_i)$ are constant (i.e. no fatigue). My 1st attempt: let $P(A_1) = p > 1/2$, and thereafter every game $i$ result = game 1 result.  This means: (1) $P(A_i) = p > 1/2$ (even though they are dependent), and yet (2) a longer series would give no advantage (nor disadvantage) to team A, because $P(A_{series}) = p$ regardless of $k$.  However, I want a scenario where a longer series actually decreases $P(A_{series})$. My 2nd attempt is something convoluted: the first 7 games ($k=4$) are played ""normally"" (i.i.d.) but then games 8 and 9 are always won by the loser of the best-of-first-7 series.  I have not worked this out fully, but while this may give an example where $P(A_{series} | k = 4) > P(A_{series} | k = 5)$, I think this also implies the marginal probabilities $P(A_8), P(A_9) < 1/2$.  So this isn't satisfying as a counterexample, since not only $P(A_i)$ are non-constant, team A actually becomes the worse team in a sense. My 3rd attempt is a ""surgical tweak"" to the 2nd attempt: (1) if the first 7 games include exactly 4 wins by A, then games 8 & 9 are won by team B (therefore making B the overall winner), but (2) if the first 7 games have any other result, then games 8 & 9 are played i.i.d. but with an enhanced $P(A_8)=P(A_9) = p' > p$.  By restricting the special dependence to an event of small enough probability, and balancing it out with $p' > p$ in the case (2), I think I can manage $P(A_8) = P(A_9) =p $.  I have not worked out whether $P(A_{series} | k = 4) > P(A_{series} | k = 5)$ for some choice of $p, p'$. 
 Also, this counterexample is too ""artificial"" for my taste. What I seek is a probability model where: For every game $i$, the marginal probability $P(A_i)$ is the same, i.e. $\forall i: P(A_i) = p > 1/2$. There exists $k_0$ s.t. $P(A_{series} | k = k_0) > P(A_{series} | k = k_0 + 1)$. Bonus if this is true for all $k_0$, or all sufficiently large $k_0$. Aesthetic requirement :) - Any dependence is as ""simple"" as possible, i.e. I prefer not to have something like my 3rd attempt (or even more convoluted).  I know this isn't a math requirement, and people can have different tastes...  comments on this point are welcome.",['probability']
2793169,Entire function mapping a parallelogram onto another one is a degree 1 polynomial,"I am reading Bak and Newman's Complex Analysis and I can't figure out how to do the following exercise: Prove that an entire function which maps a parallelogram onto another parallelogram, and maps each side of the original parallelogram onto a side of its image, must be a linear polynomial. There is no solution, but a hint: to use the previous exercise, in which I proved the following, using Schwarz reflection principle: If an entire funtion maps two horizontal lines onto two other horizontal lines, then its derivative is periodic. I tried to show that the derivative must be periodic in two directions, hence be bounded, thereby constant, so that we could conclude $f (z)=az+b $. But I couldn't figure out how to do this, because we are not explicitely given the hypotheses of the previous exercise. Thank you for your help.","['complex-geometry', 'complex-analysis', 'entire-functions', 'complex-numbers']"
2793183,Prove that $\sum\limits_{\mathrm{cyc}} x\frac{(y+z)^2}{(1+yz)^2} \ge \frac{3\sqrt 3}{4}$ for $xy + yz + zx = 1$,"This is an old inequality but I haven't seen a satisfactory solution yet and am hoping someone here can provide one. There are a couple of brute force solutions but they provide no insight into the inequality and I'd be surprised if there isn't a trick to it:
$$x\frac{(y+z)^2}{(1+yz)^2} + y\frac{(x+z)^2}{(1+xz)^2} + z\frac{(x+y)^2}{(1+xy)^2} \ge \frac{3\sqrt 3}{4}$$
for $xy+yz+zx = 1$, all positive. My attempt was to try to find a lower bound of the left side in terms of symmetric quantities like $u=x+y+z$ and $w=xyz$, however I haven't had much success despite multiple attempts. For example, the left hand side above can be bounded from below by
$$\sum x\frac{(y+z)^2}{(1+yz)^2} \ge\frac{\left(\sum x(y+z)^2\right)^3}{\left(\sum (y+z)(1-y^2z^2)\right)^2} = \frac{\left(u+3w\right)^3}{\left(u+2u^2w+w\right)^2}$$
but the above has a minimum just a tad less than $3\sqrt 3/4$. I have also tried a cotangent substitution and breaking the symmetry (i.e. assuming $x\ge y\ge z$) but I didn't get far. I don't know the origin of the inequality but it's supposed to be competition level so I suspect it has a nice and tricky solution and not just a brute force one. So, I am hoping someone here will find it.","['inequality', 'a.m.-g.m.-inequality', 'substitution', 'algebra-precalculus', 'uvw']"
2793201,Cardinality of a subset of $A$ without elements $x = 2y$ [duplicate],"This question already has answers here : Find set with maximal cardinality given constraints (4 answers) Closed 4 years ago . Given set $A = \{1, 2, 3, ..., 256\}$ , find the cardinality of the biggest subset $A'$ of $A$ such that elements $x, y$ of the form $x = 2y$ do not belong to $A'$ Attempt Split $A$ into odd and even integers. $128$ odd integers belong in $A'$ . Even integers which are divisible by $2$ but not by $4$ are out. Then take $32$ integers divisible by $4$ but not by $8$ . They belong in $A'$ . Those divisible by $8$ but not by $16$ are out. Same with $8$ integers divisible by $16$ but not by $32$ Same with $2$ integers divisible by $64$ but not by $128$ Same with $1$ integer divisible by $256$ Answer: $128 + 32 + 8 + 2 + 1 = 171$ . How do I prove formally this is indeed the max cardinality?",['elementary-set-theory']
2793202,Set of discontinuity of monotone function is countable,"The result I am trying to prove is the following: Let $F:\Bbb R\to\Bbb R$ be increasing. Then the set of points, at which it is discontinuous, is countable. I have been reading this form Folland's Real Analysis and he proves this by considering the sum $\sum_{|x|<N}[F(x^+)-F(x^-)]$ which has to be finite by some sort of telescoping sum. Now, I am not sure how he defines this sum. I am assuming he does this by using nets (let me know if there is another way to look at this), but he doesn't introduce the concept of nets until the next chapter. So I was looking for some other ways to prove this result and I found the following result (from Stein-Shakarchi ) which is quite similar but assumes $F$ to be bounded. A bounded increasing function $F$ on $[a,b]$ has at most countably many discontinuities. This is shown by using the density of rationals in $\Bbb R$ , i.e. $\exists r_x\in\Bbb Q$ s. t. $F(x^-)<r_x<F(x^+)$ corresponding to each point of discontinuity $x$ . Now I feel like there shouldn't be a problem in doing the exact same thing for unbounded increasing function. Is this right? Secondly how does this proof relate to the first one? I feel like there isn't much of a difference between the two proofs. To be precise I think that the previous sum is well defined/finite because of this countability of discontinuous points. What do you think about this way of thinking? Edit. Here is the Folland's proof in more detail: Since $F$ is increasing, the intervals $(F(x^-),F(x^+))$ are disjoint for all $\forall x\in\Bbb R$ . Moreover, $\forall|x| < N$ such intervals are contained in $(F(-N),F(N^+))$ and so $$\sum_{|x|<N}[F(x^+)-F(x^-)] \leqslant F(N) - F(-N) < \infty. $$ Hence the set $\{ x \in (-N,N) : F(x^+) \neq F(x^-)\}$ is countable.","['real-analysis', 'lebesgue-measure', 'measure-theory']"
2793213,"Evaluating $\lim_{(x,y) \to (\pi,0)} {\cos(x) + 1 + {y^2/2} \over (x - \pi)^2 + y^2}$","I'm struggling to evaluate this limit problem: $$\lim\limits_{(x,y) \to (\pi,0)} {\cos x + 1 + {y^2/2} \over (x - \pi)^2 + y^2}$$ I've tried this with paths $x=\pi$, $y=0$, $y=x-\pi$, ... and so far all of them have resulted in $1 \over 2$. However when I try to evaluate this in WolframAlpha, it says that the limit does not exist. I haven't found any paths that result in a value other than $1/2$. How can I prove that this limit does not exist?","['multivariable-calculus', 'limits']"
2793220,"If X is independent of $\mathcal{G}$ and $Y$ is $\mathcal{G}$ measurable, why are $X,Y$ independent?","Suppose there is a probability space $(\Omega, \mathcal{F}, P)$ and there exists a $\sigma$-algebra called $\mathcal{G}\subseteq \mathcal{F}$. Assume that $Y$ is a random variable which is $\mathcal{G}$-measurable and $X$ is independent of $\mathcal{G}$. My lecturer seems to think this means that $X,Y$ are independent? I do not understand that this is true, since it seems to me that $\mathcal{G} \subseteq \sigma(Y)$, but there is not necessarily equality. My questions: (1) Is the statement in the box above true? (2) Can you prove it?","['independence', 'probability-theory', 'measure-theory', 'random-variables']"
2793239,Does standard deviation change if multiplied by a constant? or if a constant is added to it?,"We have a function which returns a value d with a standard deviation of s . Afterwards, let us plug d into the following formula: y = 1.12-0.01*d Would y still have the same standard deviation s ? If not, how would it change?","['statistics', 'standard-deviation', 'calculus']"
2793255,Is this the correct radius of convergence of the following series,"Find the radius of convergence of the following series $$\sum_{n=0}^\infty ni^nz^n$$ Using $$\frac1R = \limsup_{n \to\infty} |a_n|^{1/n}, \quad \text{where }a_n=ni^n,$$
I found $R=1$.","['complex-analysis', 'sequences-and-series', 'convergence-divergence']"
2793281,The Sobolev space with respect to different norms,"Consider two different norms on the Sobolev space $W^{1,2}([0,1])$: the $L_2$ norm
$$
\|f\|_{L_2}^2:=\int_{[0,1]} f^2(x)\, dx
$$
and the usual standard norm: $$
\|f\|_{H^1}^2:=\|f\|_{L_2}^2+\|f'\|_{L_2}^2.
$$ It is obvious that this two norms are not equivalent and generate different topologies. Is it true that these two norms, nevertheless, generate the same Borel $\sigma$--algebra?","['real-analysis', 'functional-analysis', 'measure-theory', 'general-topology', 'sobolev-spaces']"
2793317,Why are $\cos \frac{\theta}{2}$ and $-\sin \frac{\theta}{2}$ the same in the polar coordinate system?,"I encountered this when trying to prove polar-axis symmetry for $\sin \frac{\theta}{2}$, and ended up with: $r(\pi-\theta)=\sin\left(\frac\pi2-\frac\theta2\right)=\cos\frac\theta2$ Implying that $\cos \frac{\theta}{2} = -\sin \frac{\theta}{2}$, per the symmetry test $(r,\theta) \rightarrow (-r, \pi-\theta)$. I noticed that while in the rectangular system, $\cos \frac{x}{2}$ and $-\sin \frac{x}{2}$ are different, but in the polar system, they are graphically identical. Why is this so, and how do I proceed to prove the symmetry algebraically?","['symmetry', 'trigonometry', 'polar-coordinates']"
2793366,Fundamental group obtained by glueing two mobius bands,"Let us glue two Mobius Bands $A$, $B$ together along their boundaries. We want to apply Van-Kampen's theorem to their union. The intersection $L$ is homotopic to $S^1$. So we get
$$\pi_1(X) = \mathbb{Z}*_\mathbb{Z}\mathbb{Z}$$
We look at what happens when we include the boundary into each space. Let $\pi_1(A)$ have generator $a$, $\pi_1(B)$ have generator $b$, $\pi_1(L)$ have generator $\sigma$. As the boundary goes around the generator twice, we get $$i^*(\sigma)=a^2$$
$$j^*(\sigma) = b^2$$ Where $i,j$ the natural inclusions. Therefore
$$\pi_1(X) = \langle a,b \mid a^2=b^2 \rangle $$. But this doesn't seem right, as (apparently) $X$ is just the Klein bottle $K$ and $$\pi_1(K) = \langle  a,b\mid abab^{-1} = 1 \rangle$$
But these groups don't look the same.","['algebraic-topology', 'general-topology', 'fundamental-groups']"
2793380,Seat n people in 4 benches,"How many ways are there to seat n people in 4 benches so that no bench
  is left empty with order? Hints from the teacher Each bench should have at least 1 person This question is similar to distributing different object among n children The answer is one line I am not sure how to deal with this question, but this was what I tried. Let $A_i$ be the set of all distributions in which $i-$th bench is empty. Then $|A_i| = (n-1)^4$ for each $i$. $|A_i\cap A_j|= (n-2)^4$ for each $i\ne j$ $|A_i\cap A_j\cap A_k|= (n-3)^4$ for each $i\ne j\ne k\ne i$ $|A_i\cap A_j\cap A_k \cap A_l|= (n-4)^4 = 1$ for all pairvise different $i,j,k,l,m$ Now we are interested in
  $$\Big| \bigcap_{i=1}^n \overline A_i\Big| = \Big|\; \overline{\bigcup_{i=1}^n A_i}\;\Big|
= n^4-\Big|\; \bigcup_{i=1}^n A_i\;\Big|
$$We have by PIE:
\begin{eqnarray*}
  \Big|\; \bigcup_{i=1}^n A_i\;\Big| &=& \sum_{i=1}^n  \Big|\; A_i\ \Big| - \sum_{i=1}^n  \Big|\;A_i\cap A_j\ \Big|+...\\
&=& 4\cdot (n-1)^4-2n\cdot (n-2)^4+2n\cdot (n-3)^4-4 ..... +0=\\
\end{eqnarray*} So the final answer is $n^4-$(the above value). Am I correct, or is there something that I am doing wrong? Edit: Also maybe I want to apply pigeonhole principle since n items are to be put into m containers, with n > m, then at least one container must contain more than one item..","['pigeonhole-principle', 'inclusion-exclusion', 'permutations', 'combinatorics', 'discrete-mathematics']"
2793393,"Cardinality of the set of Riemann integrable functions on [0,1]","What is the cardinality of the set $\mathcal{R}[0,1]$ of all Riemann integrable real functions on [0,1]? I expect it to be $2^\mathfrak{c}$. A function is Riemann integrable if and only if it is continuous almost everywhere and bounded. Since a set of measure 0 can be uncountable, I assume one can construct $2^\mathfrak{c}$ subsets of $[0,1]$ that have measure 0 yet are discontinuity sets of real functions. But then I also realize that there can be measure zero sets which are never discontinuity sets of a real function. So, I am stuck at this point and have no idea how to proceed.","['real-analysis', 'riemann-integration']"
2793396,$\int{\cos(x)\cosh(x)+\sin(x)\sinh(x)}dx$,"How would I evaluate the integral:
$$I=\int{(\cos(x)\cosh(x)+\sin(x)\sinh(x)})\,dx$$
My thought was to use:
$$\cos(ix)=\cosh(x)$$
and
$$\sin(ix)=i\sinh(x)$$
or expand all four trig functions into exponentials but this was very messy EDIT: If I split this into two integrals where $I=I_1+I_2$
$$I_1=\int{cos(x)cosh(x)}dx$$
$$I_2=\int{sin(x)sinh(x)}dx$$ $$I_1=sin(x)cosh(x)-\int{sin(x)sinh(x)}dx$$
This second part is equal to $I_2$ so does that mean that 
$$I=sin(x)cosh(x)$$",['integration']
2793434,Orbit of a matrix can generates a basis?,"Let A be a matrix $n\times n$, given a n-vector $v$, what conditions over $v$ and $A$ are necessaries for $[v, Av,..., A^{n-1}v]$ will be linearly independent? For example if $v$ is a eigenvector or $A^k=Id$ $(k<n)$,  they are not linearly independent. In other words, when the orbit of a matrix over a vector space $V$ (finite dimensional) can  generates a basis of $V$?","['matrices', 'group-actions', 'topological-vector-spaces', 'linear-algebra', 'vector-spaces']"
2793448,Computing the discrete-time difference equation when the system matrix is non-invertible,"If we have a continuous time equation, $$ \dot{x}(t) = A x(t) + B u(t)$$ where $A \in \mathbb{R}^{n \times n}, x \in \mathbb{R}^{n\times1}, B \in \mathbb{R}^{n \times m}, u \in \mathbb{R}^{m \times 1}$, the analytical solution is obtained as $$x(t) = e^{A t} x(0) + \int_0^t e^{A(t-\tau)}B u(\tau)\, d\tau$$ Converting this to discrete-time, with sample-time $T_s$, and assuming that all eigen-values of $A$ are real & distinct and $u$ remains constant within each sample interval, $k$ to $k+1$, we get the following difference equation, $$x[k+1] = e^{A T_s} x[k] + \left[\int_0^t e^{A\tau}B \, d\tau\right] u[k]$$ Which can be written as $$x[k+1] = A_d  x[k] + B_d u[k]$$ When the original matrix $A$ is invertible, the integral term corresponding to $B_d$ can be written as $$B_d = A^{-1}(A_d - I)$$ The question is, What about the case when A is non-invertible ?  In particular, if my $A=\begin{bmatrix}a_1 & 0 & 0 \\ 0 & a_2 & 0 \\ 0 & 0 & 0  \end{bmatrix}$ and $B=\begin{bmatrix}b_1 \\ b_2 \\ b_3 \end{bmatrix}$. How do I obtain the corresponding $B_d$?","['matrix-equations', 'ordinary-differential-equations', 'dynamical-systems']"
2793509,log likelihood function and MLE for binomial sample,"Let $X_1,X_2,...;X_n$ be a random sample with $X_i$~$Binomial(m,p)$ for $i=1,...,n$ and $m=1,2,3,...$ and let $p\in (0,1)$. We assume $m$ is known and we are given the following data $x_1,...,x_n\in\{0,...,m\}$ Write up the log-likelihood function and find the MLE $\hat{P}ML$ for p I'm not quite sure how to approach this. This is what I've tried: I believe the likelihood function of a Binomial trial is given by $P_{X_i}(x;m)=$ ${m}\choose{x} $$p^x(1-p)^{m-x}$ From here I'm kind of stuck. I'm uncertain how I find/calculate the log likelihood function. I've understood the MLE as being taking the derivative with respect to m, setting the equation equal to zero and isolating m (like with most maximization problems). So finding the log likelihood function seems to be my problem Edit: I might be misunderstanding it but could the log likelihood function simple be log of the likelihood function? so $log(P_{X_i}(x;m))$","['maximum-likelihood', 'probability', 'log-likelihood', 'proof-verification']"
2793541,Compact embedding between $H^{m+1}(\Omega)$ and $H^{m}(\Omega)$ for $\Omega$ bounded,"I know that we have Rellich-Kondrachov Theorem that says that there is a compact embedding between $H^{1}(\Omega)$ and $H^{0}(\Omega)$, or more generally as Adams states (pag 168 theorem 6.3) we have a compact embedding between $H^{j+k}(\Omega)$ and $H^{j}(\Omega)$ for $j\geq 0$ and $k>0$ integers. So, we are excluding negative exponents, so i would like to know if we can have the same result for every Sobolev Space,i.e, i would like to know if we can have the compact embedding between $H^{m+1}(\Omega)$ and $H^{m}(\Omega)$, $m \in \mathbb{R}$, just knowing Rellich-Kondrachov theorem. I'm particularly interested in knowing if there is a compact embedding for $H^{-1}(\Omega)$ and $H^{0}(\Omega)$. Thank you in advance, and any reference or help would be useful","['functional-analysis', 'compact-operators', 'sobolev-spaces']"
2793545,Example of a Monohedral Regular Heptagonal Polygon,"I had asked Example of a heptagonal polyhedron? when looking for an example of a polyhedron with only heptagonal sides. The answers revealed that no such convex polyehdron exists; but that if one introduces topological holes it may still be possible. My question: Is it possible to construct such genus $\ge 1$  polyhedron purely out of regular congruent heptagons? What would an example of such a construction? Some searching on google with the terms ""monohedral regular heptagonal polyhedron"" and ""monohedral heptagonal torus"" revealed nothing. Eventually I found a paper looking at a similar problem: http://archive.bridgesmathart.org/2008/bridges2008-433.pdf but it seems the author wasn't particularly interested in genus $ \ge 1 $ I'm not sure how one even begins proving that such an object CAN exist let alone finding it.",['geometry']
2793552,Find generating function of series $a_{n} = 2^{n} + 3^{n} $,"Find generating function of series $a_{n} = 2^{n} + 3^{n} $ I'm having a problem with this because at first i have to find recursive equation for $a_{n}$ . I find that $a_{0} = 2$ , $a_{1} = 5$ but how do I find the rest? Also I can't simply add generating functions, I have to solve recursive equation using generating functions. First step is to find recursive equation for $a_{n}$ then some other stuff that I can handle.","['generating-functions', 'sequences-and-series', 'discrete-mathematics']"
2793567,The simplest billiards problem,"The Problem: Let's say we have a rectangle of size $m \times n$ centered at the origin (or, if it makes the math easier, you can place it wherever on the plane). We take a billiard ball, represented by a point, in the center of the rectangle with a constant velocity vector $\vec{v}$. Assuming no other forces acting on the ball, write a function $f(m,n,t)$ that gives the position of the ball after $t$ seconds. My Work: I don't actually know all that much vector math (this is a junior high pre-calc class), so I tried to set this up as a geometry problem. With the constraints above, if we expand our velocity vector $\vec{v}=\langle a,b\rangle$, our problem is as shown: Let's call the angle the vector makes with the x-axis $\theta$. Making a triangle with our vector and the x-axis, we get that $\tan(\theta)=\frac{b}{a}$. So, $\theta=\arctan(\frac{b}{a})$. Now, if we want to find the vector pointing to where the ball is going to hit the boundary of the rectangle $\vec{h}$, we know it is going to be the same direction as $\vec{v}$, just a different magnitude. Mathematically put:
$$\frac{1}{||v||}\vec{v}=\frac{1}{||h||}\vec{h}$$ We can calculate the magnitude of $\vec{h}$ by noticing that $$\sin(\theta)=\sin(\arctan(\frac ba))=\frac{\frac{m}{2}}{||h||}$$
Which simplifies quite nicely to:
$$||h||=\frac{m \sqrt{a^2 + b^2}}{2 b}$$
Returning to our original equation, we can re-arrange to get that $\frac{||h||}{||v||}\vec{v}=\vec{h}$. Since the magnitude of $\vec{v}$ is $\sqrt{a^2+b^2}$, this, astoundingly, simplifies to $\vec{h}=\frac{m}{2b}\vec{v}$, leading me to believe there was a way easier way to do what I did. So, we found the first point that the ball will ""bounce"" at. But I have no idea how to model the ball ""reflecting"" and then to calculate where it will bounce again and again. This was extra credit on an exam, and I didn't have time for it during the exam but it still interests me. Any help?","['billiards', 'dynamical-systems', 'vectors', 'geometry']"
2793603,Motivation of Functional Analysis,"What is the motivation of functional analysis? What originally motivated its study? By functional analysis, I mean the study of Banach and Hilbert spaces.","['functional-analysis', 'analysis']"
2793615,Verifying a Stubborn Inequality,"I have an inequality which arose in an unrelated problem, and it's been proving to get the better of me.  I essentially know it has to be true, but cannot verify it.  For integers $n_{1}, n_{2}, \ell_{1}, \ell_{2}$, I want to show that $$8 n_{1} + 8n_{2} -\ell_{1}^{2} - \ell_{2}^{2} - 2\ell_{1} \ell_{2} > 0$$ Given only the constraints $4n_{1} - \ell_{1}^{2} \geq -1$, $4n_{2} - \ell_{2}^{2} >0$, $n_{1} \geq 0$, and $n_{2} > 0$.  I am also happy to assume $\ell_{1} + \ell_{2} >0$, and that $\ell_{1} \geq \ell_{2} >0$.  In my particular problem, this case will suffice.  Does anyone have any tips on proving this? I realize (given the form of the constraints) it's tempting to rewrite the lefthand side as $$(4n_{1} -\ell_{1}^{2})+(4n_{2} - \ell_{2}^{2}) + 4n_{1} + 4n_{2} - 2\ell_{1} \ell_{2},$$ but the term $-2 \ell_{1} \ell_{2}$ is troubling, as it is negative by my assumptions.","['algebra-precalculus', 'inequality', 'polynomials']"
2793619,"At what values of $\beta$ does $\sum a_n$ converge, where $a_n=\sqrt{1+\frac{(-1)^n}{n^\beta}}-1\,$?","I'm trying to figure out the values of $\beta$ for which $\sum a_n$ converges, where $a_n=\sqrt{1+\dfrac{(-1)^n}{n^\beta}}-1\,$. Here is what I have done: I tried to re-write $a_n$ as the following, so as to get the monotonic pattern first but I couldn't even forge ahead. $$a_{2n}=\sqrt{1+\dfrac{1}{(2n)^\beta}}-1\,$$
and
$$a_{2n-1}=\sqrt{1+\dfrac{1}{(2n-1)^\beta}}-1\,.$$ Please, can anyone show a way-out?","['real-analysis', 'sequences-and-series', 'analysis']"
2793629,"What is the justification for $\lim_{m \rightarrow \infty} \frac 1 m \sum_{i = 1}^m X_i= \int_{-\infty}^{\infty} x f(x) \, dx = \operatorname EX$","I am reading Explaining the Gibbs Sampler . What I have understood so far is that this sampler allows us to generate $X_1, \ldots, X_m$ with density $f(x)$ without actually knowing what $f(x)$ is. The author says that if we wanted to calculate the mean of $f(x)$ we could use this fact, $$\lim_{m \rightarrow \infty} \frac 1 m \sum_{i=1}^m X_i = \int_{-\infty}^\infty x f(x) \, dx = \operatorname EX.$$ I've never seen the first equality in any text when the topic of expectation of a random variable is introduced. How exactly is the first equality true?","['probability-theory', 'probability', 'expectation', 'statistics']"
2793635,Calculating the lattice of the tori of a non-singular projective cubic curve,"If $C$ is the curve in $\mathbb{C}\mathbb{P}^{2}$ defined by the zero set of the polynomial $P^{\lambda}(x,y,z) = y^{2}z - x(x-z)(x-\lambda z)$, for $\lambda$ not $0$ or $1$. Then we know that $C$ is homeomorphic to a torus. Tori are homeomorphic to $\mathbb{C} / \Lambda $ for $\Lambda$ a lattice in $\mathbb{C}$. My question is given a specific value of $\lambda$, how would one approach calculating the lattice $\Lambda$? Edit - 
$\textbf{Motivating example:}$ My motivation for asking this question came from question 5 on this past exam paper: https://www.dropbox.com/s/hfet5ki0nv3bm0d/2011.pdf?dl=0 In part a) we're asked to calculate a projective map $\sigma$ of $\mathbb{C}\mathbb{P}^{2}$ that sends the curve $C^{\lambda}$ (defined by the zero set of the polynomial $P^{\lambda}$), to the curve $C^{1 / \lambda}$ whilst fixing the point $[0,1,0]$. Then we specialise to the case $\lambda = -1$ and you are then asked to show that $\sigma$ has order 4. Then we're told that holomorphic maps from $\mathbb{C} / \Lambda $ to $\mathbb{C} / \Lambda $ are of the form $ z \mapsto az + b$ for some $a,b \in \mathbb{C}$ and $a \cdot \Lambda \subseteq \Lambda$, and also that for every $\lambda \neq 0,1$ we have that, for some lattice $\Lambda$, $C^{\lambda} \cong \mathbb{C} / \Lambda$. Then we're asked to calculate the lattice $\Lambda_{-1}$ such that $C^{-1} \cong \mathbb{C} / \Lambda_{-1}$. Others have shown me that I can use the Modular Lambda function to calculate $\Lambda$ for any $\lambda$, but I don't think this is what I'm meant to do, since we haven't been introduced to this function before. Would I be able to get some hints on how to tackle this? This is what I've done so far: $\textbf{Progress:}$ If we let $\sigma$ be defined by the matrix $M(\lambda)$ where $$
M(\lambda) = \begin{bmatrix}
    1       & 0 &  0 \\
    0       & 1 / \sqrt{\lambda} & 0  \\
    0       & 0 & \lambda 
\end{bmatrix}
$$ Then I think $\sigma$ maps $C^{\lambda}$ to $C^{1/\lambda}$ and fixes $[0,1,0]$. Then it is easy to show when $\lambda = -1$ that $M(\lambda)^{k}$ is not a scalar multiple of the identity, unless $k$ is a multiple of 4. Hence $\sigma$ has order 4. Then suppose $\psi : \mathbb{C} / {\Lambda}_{-1} \rightarrow C^{-1}$ is an isomorphism. Then $\sigma' = \psi^{-1} \circ \sigma \circ \psi $ is a holomorphic map from $\mathbb{C} / {\Lambda}_{-1}$ to $\mathbb{C} / {\Lambda}_{-1}$, and so there exists $a, b \in \mathbb{C}$ with $a \cdot \Lambda_{-1} \subseteq \Lambda_{-1}$ such that $\sigma'(z) = az + b$. Then $\sigma'$ has order 4, and since $\sigma'^{4}(z) = a^4z + b(a^3 + a^2 + a + 1)$, we see that the map $z \mapsto a^4z + b(a^3 + a^2 + a + 1)$ must be the identity on $ \mathbb{C} / {\Lambda}_{-1}$. Then $a^4z + b(a^3 + a^2 + a + 1) - z = (a^3 + a^2 + a + 1)((a-1)z + b) \in \Lambda_{-1}$ for every $z \in \mathbb{C}$. But then I do not know how to proceed from here. From wolfram alpha, I can see that the lattice must be $\Lambda_{-1} = \left< {1, \tau} \right>_{\mathbb{Z}}$ where $\tau = \frac{log(\operatorname{q}(-1))}{\pi \cdot i} = 1 + i $, where $\operatorname{q}$ is the ""Elliptic Nome"" function. But I have no idea where this comes from.","['elliptic-functions', 'algebraic-geometry', 'projective-geometry', 'elliptic-curves', 'elliptic-integrals']"
2793662,How can I calculate the odds of winning successive games of paper/scissors/rock?,"A friend and I have a game we play, and when we draw or call at the same time we use one round of paper/scissors/rock to determine the winner. I am currently 9-0 up on him and was wondering what the mathematical odds are of that? Is it as simple as $\frac{1}{3\times9}$? Or is there more to it? :edit: they were all individual times, we have only played 9 times, separated by weeks, each time there were no draws, I beat him first time every time, I'm not concious of any ""tactics"" that help win games of p/s/r, to my knowledge we are both playing completely randomly.","['statistics', 'probability']"
2793667,Let $N \subset M$ be a submanifold. A minimizing geodesic which joins a point $q \in M-N$ minimum distance from $N$ is orthogonal to $N$ at $q$.,"Let $M$ be a complete Riemannian manifold and let $N \subset M$ be a closed submanifold of $M$. Let $p_0 \in M$ where $p_{0} \notin N$ and let $d(p_{0},N)$ be the distance from $p_{0}$ to $N$. Show that there exists a point $q_{0} \in N$ such that $d(p_0,q_0)=d(p_{0},N)$. Moreover that a minimizing geodesic which joins $p_{0}$ to $q_{0}$ is orthogonal to $N$ at $q_{0}$. I've done the first part but am at a total loss for the second part (highlighted). This comes from chapter 9 of do Carmo on variations of energy so I assume you have to use a variation somehow.","['riemannian-geometry', 'differential-geometry']"
2793674,Matrix Exponentiation $A^{15}$,"A is a 2x2 matrix. Let it satisfy $A^2 = A - I$, where $I = \begin{bmatrix} 1 & 0\\ 0 & 1\end{bmatrix}$. I want to find $A^{15}$ : My Approach: I isolated $A$: $A^2-A=-I$ $A(A-I)=-I$ From that, I just tried to solve the system that would generate: $\begin{bmatrix}a & b \\ c & d\end{bmatrix}\begin{bmatrix}a-1 & b \\ c & d-1\end{bmatrix} = \begin{bmatrix}-1 & 0 \\ 0 & -1\end{bmatrix}$ Turns out this doesn't help me get closer to the answer as it leads to: $\begin{bmatrix}a(a-1)+bc & b(d-1)+ab\\c\left(a-1\right)+dc &d(d-1)+bc\end{bmatrix} = \begin{bmatrix}-1 & 0 \\ 0 & -1\end{bmatrix}$ Is that the correct approach, or else, which property am I missing in order to solve this problem?","['matrices', 'exponentiation']"
2793686,Range of linear operator $P$ and its adjoint $P^*$,"Let $\mathcal{H}$ be a Hilbert space, $P:\mathcal{H}\to\mathcal{H}$ a bounded linear operator, $P^*$ be the adjoint, and $R(P),N(P)$ be the range and nullspace. (a) Show that $N(P^*) = R(P)^\perp$ and $\overline{R(P^*)} = N(P)^\perp$ (b) Show that $R(P^*)$ is closed if $R(P)$ is closed (c) Assume that $P^2 = P$.  Show that the following are equivalent $\quad$ (i) $R(P)$ is closed and $||x-Px|| = \inf_{y\in R(P)}||x-y||$ for every $x\in \mathcal{H}$ $\quad$ (ii) $P=P^*$ $(a)$ is an exercise I've done before, but for the rest I haven't been able to make progress. For $(b)$, if I directly apply $(a)$, then I obtain the closure $\overline{R(P^*)} =  N(P)^\perp$, which is not useful.","['functional-analysis', 'real-analysis', 'hilbert-spaces']"
2793741,Showing that $m^3+4m+2=0$ has only one real root,"I actually have to find the number of real roots of $m^3+4m+2=0$ for a conic sections question in which $m$ is the slope of the normal. The answer is that there is only one real value of $m$, and therefore only one normal. How did they get this? How can I find out how many real roots such a cubic equation has without spending too much time on it?","['cubics', 'roots', 'conic-sections', 'geometry']"
2793763,Circle on a two-dimensional Riemann manifold,"I was told that if we consider a circle of a small radius $\varepsilon$ on a two-dimensional Riamannian manifold then it's length and area are
$$
L(\varepsilon) = 2 \pi \varepsilon + A \varepsilon^3 + o(\varepsilon^3),\qquad
S(\varepsilon) = \pi \varepsilon^2 + B \varepsilon^4 + o(\varepsilon^4),
$$
where $A$ and $B$ are some functions of Gaussian curvature $K$. I can directly verify it for a sphere to get $A = - \frac{\pi}{3} K$, $B = - \frac{\pi}{12} K$. I proved the identities above (without explicitly finding $A$ and $B$) and want my proof to be checked. Consider orthonormal basis $e_a$ at tangent space at point $p$ and consider Riemann normal coordinates $x^a$ around it. In this coordinates geodesic starting at $p$ along vector $v = v^a e_a$ has coordinates
$$
x^a(t) = v^a t
$$
and Christoffel symbols vanish at $p$ so $g_{ab,c}(p) = 0$, so the metric is
$$
g_{ab}(x) = \delta_{ab} + \frac12 g_{ab,cd}(p) x^c x^d + o(x^2). \qquad (*)
$$
Consider geodesic that starts at $p$, has initial velocity $v$, $|v| = 1$, which has length $\varepsilon$. Then its end has coordinates $x^i_v = v^i \varepsilon + o(\varepsilon)$. Consider unit vectors at $p$ $v(\varphi) = \cos \varphi e_1 + \sin \varphi e_2$.
Then the length of a circle centered at $p$ with radius $\varepsilon$ is
$$
L(\varepsilon) = \int_0^{2\pi} \left|\frac{dx^a}{d\varphi}\right| d \varphi
= \int_0^{2\pi} \sqrt{\varepsilon^2 \delta_{ij}v^i{}'v^j{}' + \varepsilon^4 \frac12 g_{ij,kl}(p)v^i{}'v^j{}'v^k{}'v^l{}' + o(\varepsilon^4)} d\varphi\\
= \int_0^{2\pi} \left( \varepsilon + \varepsilon^3 \frac14 g_{ij,kl}(p)v^i{}'v^j{}'v^k{}'v^l{}' + o(\varepsilon^3) \right) d\varphi\\
= 2 \pi \varepsilon + \varepsilon^3 \frac14 g_{ij,kl}(p) \int_0^{2\pi} v^i{}'v^j{}'v^k{}'v^l{}' d\varphi + o(\varepsilon^3),
$$
where $v' = \frac{dv}{d\varphi} = -\sin \varphi e_1 + \cos \varphi e_2$.
I didn't check whether
$$
g_{ij,kl}(p) \int_0^{2\pi} v^i{}'v^j{}'v^k{}'v^l{}' d\varphi
= \frac{\pi}{4} \left( 3 (g_{11,11} + g_{22,22}) + 4 g_{12,12} \right).
$$
is proportional to scalar curvature, it's too cumbersome and I don't see any way to check it in not computationally awful way. Consider ''polar coordinates'' $(r, \varphi)$ s.t. $r = \sqrt{(x^1)^2 + (x^2)^2}$, $\tan \varphi = \frac{x^2}{x^1}$. Note that for a point on a circle of radius $\varepsilon$ we have $r = \varepsilon + o(\varepsilon)$. The volume form is $\omega = \sqrt{g} dx^1 \wedge dx^2$. Using coordinate expansion for metric $(*)$ we have
$$
\omega = \sqrt{g} dx^1 \wedge dx^2 = (1 + r^2 C + o(r^2)) dx^1 \wedge dx^2 = (1 + r^2 C + o(r^2)) r dr \wedge d\varphi,
$$
where $C$ is a function of $v$, that is a function of $\varphi$.
So finally
$$
S(\varepsilon) = \int_0^{2\pi} d\varphi \int_0^\varepsilon r dr + \int_0^{2\pi} C(\varphi) d\varphi \int_0^\varepsilon r^3 dr + o(\varepsilon^4)
= \pi \varepsilon^2 + \varepsilon^4 \frac14 \int_0^{2\pi} C(\varphi) d\varphi + o(\varepsilon^4).
$$","['riemannian-geometry', 'differential-geometry', 'geodesic']"
2793775,description of dual space of space of Radon measure equipped with topology of weak convergence,"Let $\mathcal{M}(\mathbb R)$ be the space of Radon measures, equipped with topology $\tau$ generated by the following ""weak convergence"": $$
\mu_n \rightarrow \mu \quad \text{iff} \quad \int f \, d\mu_n \rightarrow \int f \, d\mu \quad 
$$ for all continuous function $f$ with quadratic growth: $|f(x)|\leq C(1+|x|^2)$ for some $C>0$ . Let $\mathcal{M}_2(\mathbb R)$ be the subspace of $\mathcal{M}(\mathbb R)$ that contains all Radon measures with finite second moment. I would like to know if there is a description of the topological dual of $(\mathcal{M}(\mathbb R),\tau)$ and $(\mathcal{M}_2(\mathbb R),\tau)$ . I know $\mathcal{M}(\mathbb R)$ is the dual of $C_0(\mathbb R)$ , so we have $$
(\mathcal{M}(\mathbb R),\sigma(\mathcal{M}(\mathbb R),C_0(\mathbb R)))^*=C_0(\mathbb R)
$$ where $\sigma(\mathcal{M}(\mathbb R)$ is the weak star topology. It is also obvious that convergence $\tau$ implies convergence in the weak star topology. So I was hopping the dual of  dual of $(\mathcal{M}(\mathbb R),\tau)$ or $(\mathcal{M}_2(\mathbb R),\tau)$ would just be the family of continuous functions wit quadratic growth. I also notice that $\tau$ convergence is the same as convergence in Wasserstein 2 distance, when restricted to probability measures with finite second moment. I will also be interested to see if there is any connection. I hope my question makes sense and looking forward to any hints and ideas!","['functional-analysis', 'probability', 'measure-theory']"
2793789,Derivative of exponential of a sparse parametrized matrix with respect to the parameters,"I have a Hermitian matrix of the type $$H = H(c_1, c_2, \dots, c_n)$$ where $c_i$ 's are some complex parameters. I need to find the derivatives $$\frac{\partial}{\partial c_j}\text{exp}(H)$$ for each $c_j$ . I don't think it will be of the form $\text{exp}(H)\frac{dH}{dc_j}$ because $H$ and $\frac{dH}{dc_j}$ won't necessarily commute. Does anyone have an analytical/numerical suggestion for this problem? This looks promising: Derivative of matrix exponential - but would be very complicated to implement numerically. Are there any other methods for this? Edit: Will the fact that $H$ is a sparse matrix help?","['derivatives', 'matrices', 'matrix-calculus', 'numerical-methods', 'matrix-exponential']"
2793792,Second order Taylor expansion and Laplacian relation in d dimensions,"My question concerns how the second-order term in a Taylor expansion of a scalar-valued function of a vector can be represented by a Laplacian instead of the product of vectors times dels. The origin of this question has led me to expect the numerical coefficient on the Laplacian term will have a denominator divisible by d, the dimension of the vector space. That is because the expansion below (with some assumptions) generates a differential equation solved by a Gaussian probability distribution with a variance scaled by d. Thomas Witten in his book ""Structured Fluids"" looks at an unspecified function f of the magnitude of an $\mathbb{R^3}$ vector. He Taylor expands this function to second order to derive a differential equation. Here's a simplified version of his statement, and it's the second order term that's making me curious. $p(|\vec{r}-\vec{r}_1|) = p(|\vec{r}|)-\vec{r}_1\cdot \nabla p(|\vec{r}|) + \frac{1}{6}r_1^2\nabla^2p(|\vec{r}|) $ + ... I typically write my Taylor expansions as 
$f(\vec{x}-\vec{a}) = f(\vec{x}) - \vec{a}\cdot\nabla f(\vec{x}) + \frac{1}{2}(\vec{a}\cdot\nabla) (\vec{a}\cdot\nabla) f(\vec{x}) + ...$ How can I transform from the general second-order Taylor expansion to the quoted expansion with a Laplacian and different numerical prefactor? What changes if we look at a vector argument in $\mathbb{R^d}$?","['multivariable-calculus', 'laplacian', 'taylor-expansion']"
2793794,Converging sequence of solution to a differential equation,"I was working on a problem about a sequence of functions, each of which is a solution to a sequence of differential equations, that converges to a function which is supposed to be the limit of the previous sequence I mentioned. The problem is as follows Let $R=\lbrack t_0-a,t_0+a\rbrack\times\lbrack x_0-b,x_0+b\rbrack$, Suppose $(f_n)_{n\in\Bbb N}$ is a sequence of functions defined in $R$ which converges uniformly to $f$. This function is Lipschitz with respect to $x$ and continuous with respect to $t$. Consider the following differential equations
  $$
\begin{cases}
x'(t)=f(t,x)\\
x(t_0)=x_0
\end{cases}
\quad
\begin{cases}
x'(t)=f_n(t,x)\\
x(t_0)=x_n
\end{cases}
$$ Let $\varphi,\varphi_n$ be solutions to both differential equations respectively defined on a common subinterval $\lbrack c,d\rbrack\subseteq \lbrack t_0-a,t_0+a\rbrack$. Show that if $x_n\xrightarrow[]{}x_0$ then $\varphi_n\to\varphi$ uniformly in $\lbrack c,d\rbrack$. What I've done is to try and follow the definitions of uniform convergence for a sequence of functions and I can't quite grasp how to get to the convergence of $\varphi_n$. I know that the Lipschitz condition on $f$ guarantees the existence of such solutions to the differential equations. It would seem that the problem has a bit of tricks behind it's back. I'm not quite sure on how should I work with this problem, any help is very much appreciated.","['sequence-of-function', 'lipschitz-functions', 'ordinary-differential-equations']"
2793816,Clarification needed: Smallest subfield of a field,"Q: Given a field does there exist a smallest subfield ? A: Yes, intersection of all subfields gives us the smallest subfield.
  Suppose a field has $p^n$ elements (where $p$ is prime), then the
  smallest subfield will be isomorphic to $\Bbb Z_p$. In terms of
  polynomials find an irreducible polynomial of degree $n$. Confusions: How are we being able to guarantee that there there will exist a smallest subfield having exactly $p$ elements? If we use Lagrange's theorem for the additive group formed by the field then we could say that the smallest subgroup of it would be of some order $p^m$ ($m\leq n$). Similarly if we use Lagrange's theorem for the multiplicative group we could only say that the smallest subgroup of it would be of some order $p^k$ ($k\leq n$). Thus the order of any subfield would probably be $\text{LCM}(p^k,p^m)$ (I'm just guessing this part). Anyhow, I have no idea how they claimed that the smallest subfield will/must be isomorphic to $\Bbb Z_p$ No idea what they mean by ""in terms of polynomials find an irreducible polynomial of degree $n$"" in this context. Could someone please clarify?","['abstract-algebra', 'field-theory']"
2793835,Evaluating integral $\int_0^1 \frac{x-x^2}{\sin \pi x} dx = \frac{7 \zeta(3)}{\pi^3}$,"To evaluate the integral \begin{equation}
\int_0^1 \frac{x-x^2}{\sin \pi x} dx =  \frac{7 \zeta(3)}{\pi^3}
\end{equation} I tried to use the series for $\sin \pi x$ and maybe find something related to $\zeta(3)$ .  But it didn't work. I'm guessing this integral needs more than the little amount of calculus that I know.","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'trigonometric-integrals']"
2793915,Prove that the function is uniformly continuous in $ \mathbb{R}$.,"$f(x)=(\sin x+\cos x)^{100}$   in $ \mathbb{R}$. (My attempt) I first thought that it is not uniformly continuous. Using   $\sin x+\cos x=\sqrt{2}\sin\left(x+\frac{\pi }{4}\right)$, then $f(x)=2^{50}\left(\sin\left(\frac{\pi}{4}\right)\right)^{100}$ Let $\epsilon > 0 $ be given. For every $\delta > 0$,  let $ x=2n\pi -\frac{\pi }{4}+\frac{\delta }{2} $ and $ y=2n\pi -\frac{\pi }{4}-\frac{\delta }{2}$  for any  $n\in \mathbb{N}$. That is $\left \lvert x-y \right\rvert< \frac{\delta }{2}< \delta $.
Then $\left \lvert f(x)-f(y) \right \rvert=2^{50}\left \lvert \sin\left(x+\frac{\pi }{4}\right)-\cos\left(x+\frac{\pi }{4}\right)  \right \rvert\left \lvert \left(\sin\left(x+\frac{\pi }{4}\right)\right)^{99}+ \dots +\left(\cos(x+\frac{\pi }{4}) \right)^{99} \right \rvert=2^{51}\left \lvert \sin\left(\frac{\delta }{4}\right) \right \rvert\left \lvert 100\left(\sin\left(\frac{\delta }{4}\right)\right)^{99} \right |=200\cdot  2^{50}\left \lvert \left(\sin\left(\frac{\delta }{4}\right)\right)^{100}\right \rvert> 1=\epsilon $. So, it is contradiction. Hence, $f$ is not uniformly continous in $ \mathbb{R}$. But, the answer to this problem is that $f(x)$ is uniformly continuous $ \mathbb{R}$. I don't know why this is uniformly continuous.","['uniform-continuity', 'trigonometry', 'analysis']"
2793973,What is the probability that white balls will be in the same box? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I have trouble computing a probability, I wish someone can help me with this. This is not a homework or anything like that. So there are $71$ balls, and two of them are white, the rest are black.
The balls are randomly distributed between $14$ boxes. $5$ balls in each box, one box will have $6$ balls. What is the probability that white balls will be in the same box?",['probability']
2793983,Is there a quick way to write say positive integers in an interval in mathematical notation?,"For example I find myself wanting to write $x$ is an element of the integers from $1$ to $50$, Is this the quickest way? $x\in \left[ 1,50\right] \cap \mathbb{N} $ Also is this standard on here? $\mathbb{N} = \{0, 1, 2,\dotsc \}$,
$\mathbb{ℤ}_+ = \{1, 2, \dotsc \}$.","['algebra-precalculus', 'integers', 'notation']"
2793985,Rank of a locally free coherent sheaf on a not-necessarily connected scheme,"If $X$ is a noetherian scheme and $\mathcal{F}$ is a coherent locally free sheaf on $X$, we define the rank of $\mathcal{F}$ over some trivialising open subset $U \subseteq X$ to be the smallest integer $n$ such that
$$
\mathcal{F}|_{U} \simeq \mathcal{O}_{X}|_{U}^{\oplus n}.
$$
We can define a rank function via Nakayama's lemma and show that if $X$ is connected then there is a globally well-defined rank. This is all standard. My confusion is in Hartshorne, II Proposition 7.11. There he simply gives us that $X$ is a noetherian scheme, $\mathscr{E}$ is coherent locally free, and that $\text{rank }\mathscr{E} \geq 2$. Is there some more general definition of rank that would make this well-defined for an arbitrary noetherian scheme $X$? Or does he mean the largest rank over each of the connected components? I have never seen rank used this way before, so I'm really not sure.","['schemes', 'coherent-sheaves', 'algebraic-geometry']"
2794010,How to evaluate the limit $\int_{0}^{\frac{\pi}{2}}Re^{-R\sin\theta}d\theta \quad (\text{as } R \rightarrow \infty)$,"While doing a mathematical exercise(stein Complex Analysis chapter2,exercise 3),
I managed to reduce the problem to the following one: $$\int_{0}^{\omega}Re^{-R\cos\theta}d\theta  \rightarrow 0  \quad  (\text{as } R \rightarrow \infty)$$ where $0\le \omega <\frac{\pi}{2}$ . I can prove this without much difficulty: $$\int_{0}^{\omega}Re^{-R\cos\theta}d\theta \le \int_{0}^{\omega}Re^{-R\cos\omega}d\theta =\omega Re^{-R\cos\omega}  \rightarrow 0  \quad  (\text{as } R \rightarrow \infty)$$ It is crucial that $\omega $ is strictly less than $\frac{\pi}{2}$ . This lead me to raise another interesting problem: what the limit will be if we replace $\omega$ by $\frac{\pi}{2}$ . After changing $\cos\theta$ to $\sin\theta$ (this doesn't matter), now my question is $$\int_{0}^{\frac{\pi}{2}}Re^{-R\sin\theta}d\theta  \rightarrow ?  \quad  (\text{as } R \rightarrow \infty)$$ I have no idea how to calculate, I even don't know if the limit exists.","['integration', 'definite-integrals', 'limits']"
2794044,Taking derivatives of Trig Functions,So I have the equation $\frac{d}{dx}[\cos(2x)-3\sin(x)-1]$ What I did was simplify $\cos(2x)$ to $1-2\sin^2(x)$ by applying the double angle formula. Here are my steps: $\frac{d}{dx}[1-2\sin^2(x)-3\sin(x)-1]$ Simplified = $\frac{d}{dx}[-2\sin^2(x)-3\sin(x)]$ Apply chain rule for $-2\sin^2(x)$ = $-4\sin(x)\cos(x)$ Derivative of $-3\sin(x)$ = $-3\cos(x)$ Final result = $-4\sin(x)\cos(x)-3\cos(x)$ The correct answer is $-2\sin(2x)-3\cos(x)$ . I have a feeling I shouldn't be using the double angle formula but I'm not sure why? Simplifying using algebra is allowed before differentiating/integrating as far as I know. Any help is appreciated!,"['derivatives', 'trigonometry', 'calculus']"
2794144,Probabilty of having exactly one sibling,"I'm stuck on the following probability problem: parents keep having children until they have one girl, at which point they stop; and babies are girls with probability 0.49. If we select a child uniformly at random (from the entire population of children) what's the probability he or she has exactly one sibling? If a couple has only $2$ children it means that they had first a boy and then a girl. So why the probability asked is not $0.51 \cdot 0.49$? What I am missing? It seems that the correct answer is $2 \cdot 0.51 \cdot (0.49)^2$, but I don't know how to get this result.
Thanks for your help.",['probability']
2794176,"Finding the sum of $\cos\frac{\pi}{7}$, $\cos\frac{3\pi}{7}$, $\cos\frac{5π}{7}$ by first finding a polynomial with those roots","Without using tables, find the value of $$\cos\frac{\pi}{7}+\cos\frac{3\pi}{7}+\cos\frac{5\pi}{7}$$ This is a very common high school trigonometric problem, and the usual way to solve this is by repeated application of trigonometric identities. But I thought of a bit different approach. Somehow, if we can find a polynomial whose roots are the three terms of the above expression, then we can apply Vieta's formula to find the value. So please help me with it. (Any hint will be appreciated.)","['algebra-precalculus', 'polynomials', 'trigonometry']"
