question_id,title,body,tags
1801255,weak convergence and unbounded functions with bounded moment,"I want to prove the following: Given a topological space (it is a Lusin space, but I think that does not matter) $\Omega$ , a function $f \in C(\Omega,\mathbb{R})$ and a sequence of Radon measures $P^{N}$ defined on it that converges weakly to a measure $P$ , then $$
     \mathbb{E}^{P^{N}}\left[ f \right] = \int_{\Omega} f dP^{N} \rightarrow \int_{\Omega} f dP = \mathbb{E}^{P}\left[f\right].
$$ The problem is that the function $f$ is not bounded, i.e. $f \notin C_{b}(\Omega)$ but instead satisfies the condition $$
      \sup_{N} \mathbb{E}^{P^{N}}\left[\left|f\right|^{1+\varepsilon}\right] \leq C
$$ for an $\varepsilon > 0$ . I read this claim in a paper, but unfortunately with neither a proof nor a reference. A similar question was posed in this thread: weak convergence of probability measures and unbounded functions with bounded expectation but there we had $\varepsilon = 0$ . As far as I can see, the counterexample posted there is not a counterexample here. The result appears to me rather elementary but I haven't found it anywhere yet...","['weak-convergence', 'probability-theory', 'probability', 'convergence-divergence']"
1801270,Are sets just predicates with syntactic sugar?,"Do mathematicians agree/accept that ""sets are just predicates with syntactic sugar""? If not, then Why not? I mean, I can translate between $ x \in S $ and $ S(x) $. Will that change the correctness of a proof? Is there some nuance in university-level math that makes the distinction necessary?","['logic', 'elementary-set-theory']"
1801279,Spivak's Calculus on Manifolds - Proof of Inverse Function Theorem,"I have a small confusion in a step in the proof of the Inverse Function Theorem from Spivak's Calculus on Manifolds . Theorem 2-11 (Inverse Function Theorem) Suppose that $f : \mathbb{R}^n \to \mathbb{R}^n$ is continuously differentiable in an open set containing $a$, and $\det f'(a) \neq 0$. Then there is an open set $V$ containing $a$ and an open set $W$ containing $f(a)$ such that $f : V \to W$ has a continuous inverse $f^{-1} : W \to V$ which is differentiable and for all $y \in W$ satisfies
  $$
(f^{-1})'(y) = (f'(f^{-1}(y)))^{-1}.
$$ To prove this, we first reduce it to the case when $Df(a)$ is the identity map. Then, we find a closed rectangle $U$ containing $a$ in its interior which satisfies the following properties: $f(x) \neq f(a)$ for $x \in U$. $\det f'(x) \neq 0$ for $x \in U$. $|D_j f^i(x)-D_j f^i(a)| < 1/2n^2$ for all $i$, $j$, and $x \in U$. $|x_1 - x_2| < 2|f(x_1) - f(x_2)|$ for $x_1, x_2 \in U$. There is also an open ball $W$ around $f(a)$ such that for all $y \in W$ and $x \in \operatorname{boundary} U$, we have $|y - f(a)| < |y - f(x)|$. Next, we will show that for any $y \in W$ there is a unique $x$ in $\operatorname{interior} U$ such that $f(x) = y$. To prove this, consider the function $g : U \to \mathbb{R}$ defined by
$$
g(x) = |y-f(x)|^2 = \sum_{i=1}^n (y^i - f^i(x))^2.
$$
This function is continuous and therefore has a minimum on $U$. Since $g(a) < g(x)$ for $x \in \operatorname{boundary} U$ by (5), the minimum occurs in the interior. So, there is a point $x$ in the interior of $U$ such that $D_j g(x) = 0$ for all $j$, that is
$$
\sum_{i=1}^n 2(y^i - f^i(x))\cdot D_j f^i (x) = 0 \quad \text{for all $j$}.
$$
By (2.) the matrix $(D_j f^i (x))$ must have non-zero determinant. Therefore we must have $y^i - f^i(x) = 0$ for all $i$, that is $f(x) = y$. This proves the existence of $x$. Uniqueness follows immediately from (4). Then, Spivak says If $V = (\operatorname{interior} U) \cap f^{-1}(W)$, we have shown that the function $f : V \to W$ has an inverse $f^{-1} : W \to V$. My confusion is that since we have already shown that for all $y \in W$ there is a unique $x$ in the interior of $U$, why is it necessary to take $V = (\operatorname{interior} U) \cap f^{-1}(W)$? Doesn't $V = f^{-1}(W)$ suffice?","['multivariable-calculus', 'proof-explanation']"
1801287,Show that $W^2 _t - t$ is a $\mathbb{P}$-martingale.,"Claim : $V_t = W^2 _t - t$ is a $\mathbb{P}$-martingale. I have shown via Ito's formula, that $dV_t = 2 W_t \, dW_t$. For reference, I will list this ""Proposition"": If $X$ is a stochastic process with volatility $\sigma _t$ (that is, $dX_t = \sigma _t \, dW_t + \mu _t \, dt$) which satisfies the technical condition $\mathbb{E}[( \int_0^T \sigma^2 _s \, ds)^{\frac{1}{2}}] < \infty$, then: $X$ is a martingale $\iff$ $X$ is driftless ($\mu _t \equiv 0$). Now, my book states the following: $V_t$ is a proper martingale; if we let $X$ be $(\int_{0}^{T} W^2 _t \, dt)^{\frac{1}{2}}$, then it is enough to show (by the above proposition) that $\mathbb{E}[X] < \infty$. In fact, $$(\mathbb{E}[X])^2 \leq \mathbb{E}[X^2] = \int_0^T \mathbb{E}(W^2 _t) \, dt = \frac{1}{2} T^2.$$ My attempt : Now, obviously $V$ is driftless since $dV_t$ has no drift term. But according to our ""Proposition"" shouldn't we be checking that $\mathbb{E}[( \int_0^T 4 W^2 _t \, dt)^{\frac{1}{2}}] < \infty$ ??? Furthermore, is $(\mathbb{E}[X])^2$ $\leq$ $\mathbb{E}[X^2]$ an intrinsic property of the expectation? I don't see where they get that from... I do see that $\mathbb{E}[X^2] = \mathbb{E}[((\int_0^T W^2 _t \, dt)^{\frac{1}{2}})^2] = \mathbb{E}[(\int_{0}^{T} W^2 _t \, dt)] = \int_0^T \mathbb{E}[W^2 _t] \, dt = \frac{1}{2} T^2$. This is since $Var(W_t) = \mathbb{E}[W^2 _t] - (\mathbb{E}[W_t])^2 = \mathbb{E}[W^2 _t] + 0 = t$ $\implies \mathbb{E}[W^2 _t] = t.$ So : If $\mathbb{E}[X]^2 \leq \mathbb{E}[X^2]$, then we have that $\mathbb{E}[X] \leq \pm \frac{1}{\sqrt{2}} T$. So the expectation is bounded. Then by our proposition, we have that $V_t$ is a $\mathbb{P}$-martingale. I am just confused why we want to check $\mathbb{E}[(\int_{0}^{T} W^2 _t \, dt)^\frac{1}{2}] < \infty$? Is it because our $\mathcal{F}$-previsible process is $(2W_t)$? So we just ignore the constant multiple, square $W_t$, and integrate over $W^2 _t$??? Thanks. Any and all help is appreciated. I am so confused about this technical condition.","['stochastic-processes', 'probability-theory', 'martingales', 'proof-explanation', 'stochastic-calculus']"
1801318,Dimensions of immersions vs embeddings,"Let's say that you have a manifold which you know can be immersed in $\mathbb{R}^n$. Is there a $k$ such that you can say, for sure, that the manifold is embedded in $\mathbb{R}^{n+k}$? I imagine that there is and that this is common knowmedge, but cursory googling did not throw up anything. Thank you in advance!!",['differential-geometry']
1801338,Prove that sum is convergent,"How to prove that the following sum is convergent? $$\sum_1^\infty\frac{\sin(n + \ln{n})}{n}$$
I tried to use formula $$\sin(n+ \ln{n}) = \sin{n}\cos \ln{n} + \sin \ln{n}\cos{n}$$ and $$\sum_1^N \sin{n} \leq \frac{1}{\sin{1/2}}$$
But I can't make same estimates for $\sin{n}\cos \ln{n} $ and $\sin \ln{n} \cos{n}$.","['convergence-divergence', 'summation', 'sequences-and-series', 'calculus']"
1801362,Generalized inverse of a function,"It is well-known that if a function is strictly increasing, then it has an inverse function. I also see the concept of ""generalized inverse"" in the litarature, which has the definition 
$$f^{-1}(x)=\inf\{y: f(y)>x\}.$$ What is the motivation of definition and can you give me examples which has not ordinary inverse but has generalized inverse?","['real-analysis', 'calculus', 'probability-theory']"
1801378,"What shapes, with boundary collapsed to a point, are homeomorphic to $S^n$?","Consider the following construction: Given a set $A\subseteq\Bbb R^n$, form the quotient space $A/\sim$ which identifies all the points on the boundary $\partial A$ (w.r.t $\Bbb R^n$). For which sets $A$ is the resulting topological space homeomorphic to $S^n$? Obviously this works for $D^n$ (the unit ball in $\Bbb R^n$), as well as $[0,1]^n$ and $\Delta^n$ (the simplex). But I think it will also work on much more complicated sets, like the Koch curve (with interior). In 2D I believe the Riemann mapping theorem will help to construct a homeomorphism, but I don't know whether that generalizes to $\Bbb R^n$. Some necessary conditions: $A^\circ$ must be path-connected, because there is a path on $S^n$ connecting any two points and avoiding the pole. More generally, $A^\circ$ must be homeomorphic to $\Bbb R^n$, so it must in fact be simply connected. For similar reasons, $A$ cannot be nowhere dense. $A^\circ$ must be bounded. If not, take some sequence $(x_n)\in A^\circ$ that diverges to infinity (and satisfies $d(x_m,x_n)\ge1$ for $m\ne n$), and take a subsequence that converges in $S^n$ (necessarily to the pole). Then $A\setminus\bigcup_n\bar B(x_n,\frac12\min(d(x_n,\partial A),1))$ is open in $A$, because all the closed sets in the union are separated from each other, and it contains $\partial A$, hence the image is an open set containing the pole and missing all the $x_n$'s, a contradiction.","['complex-analysis', 'general-topology']"
1801396,An elevator containing five people can stop at any of seven floors.,"An elevator containing five people can stop at any of seven floors. What is the probability that no two people get off at the same floor? Assume that the occupants act independently and that all floors are equally likely for each occupant. Solution: I don't really have too much issue with the solution except that I keep on over counting and I'm curious as to what is it extra that I am considering that I should not be considering? My solution: $$P(A) = \frac{(7)(6)(5)(4)(3) 5!}{7^5}$$ Now I am aware that the $5!$ should not be there, but my reason for including it is because we do not know which of the 5 people is the ""first"" person, ""second"" person, etc...","['probability-theory', 'probability']"
1801403,Decomposition of a positive semidefinite matrix,"Let $Y \in \mathbb{R}^{n \times n}$ be a symmetric, positive semidefinite matrix such that $Y_{kk} = 1$ for all $k$. This matrix is supposed to be factorized as $Y = V^T V$, where $V \in \mathbb{R}^{n \times n}$. Does this factorization/decomposition have a name? How is it possible to compute $V$?","['matrices', 'symmetric-matrices', 'positive-semidefinite', 'matrix-decomposition']"
1801418,Variation of parameters exercise -- harmonic motion,"I am self-studying differential equations using MIT's publicly available materials.     One of the assignments asks us to show that the general solution of the inhomogeneous DE $y'' + k^2y = R(x)$ is given by
    \begin{align}
	y & = \frac{1}{k}\left[\int_a^t\sin k(x - t)R(t)dt\right] + c_1\sin kx + c_2 \cos kx.
	\end{align} (The problem, like many of those assigned in the course, comes from Birkhoff and Rota.) It is clear that a basis of solutions to the corresponding homogeneous equation is given by $\cos kx$ and $\sin kx$.  So the general solution is given by
\begin{align}
y = y_p + c_1\sin kx + c_2\cos kx
\end{align}
where $y_p$ is a particular solution of the inhomogenous equation.  Thus we are to establish that $y_p = \frac{1}{k}\left[\int_a^t\sin k(x - t)R(t)dt\right].$ The Wronskian of our basis of solutions is $k$.  So by variation of parameters, we obtain
\begin{align}
	y_p & = \cos kx\int_a^x\frac{-R(t)\sin kt}{k}dt + \sin kx\int_a^x\frac{R(t)\cos kt}{k}dt\\
	& = \frac{1}{k}\left[\cos kx \int_a^x -R(t)\sin ktdt + \sin kx\int_a^xR(t)\cos ktdt\right]
\end{align}
which is where I get stuck.  I have two questions: 1)  What's up with the limits of integration in the purported solution?  As far as I know, it doesn't make a lot of sense to integrate with respect to $t$ while $t$ is also a limit of integration. 2)  Assuming the preceding is a typical Birkhoffian/Rotarian typo, how would one move from from \begin{align}
\frac{1}{k}\left[\cos kx \int_a^x -R(t)\sin ktdt + \sin kx\int_a^xR(t)\cos ktdt\right]
\end{align} to something that in some way resembles \begin{align}
\frac{1}{k}\left[\int_a^t\sin k(x - t)R(t)dt\right]?
\end{align} I had thought of applying integration by parts to the first expression in hopes that I could take advantage of the Pythagorean identity to obtain some helpful simplification, but no dice. Any help would be appreciated.",['ordinary-differential-equations']
1801432,Help in showing that the cusp $(y^2-x^3)\subset \mathbb{C}^2$ is not isomorphic to $\mathbb{C}$,"Let $X:=(y^2-x^3)\subset \mathbb{C}^2$ be the vanishing of the polynomial $f(x,y)=y^2-x^3.$ I have proved an exercise in Hartshorne: If $\varphi:\mathbb{C} \to X, \ t \mapsto (t^2,t^3)$ is the parametrization for the cusp defined above, then $\varphi$ is a bijective morphism of affine algebraic varieties but is not an isomorphism. The fact that $\varphi$ is a bijective morphism can be easily checked. To show that it is not an isomorphism, I first consider a homomorphism $\varphi^*:A(X)\to A(\mathbb{C})$ induced at the level of coordinate rings of $X$ and $\mathbb{C}$. Since isomorphism of affine algebraic varieties is equivalent to isomorphism of coordinate rings, it's enough to show $\varphi^*$ is not surjective. We know $A(X)=\mathbb{C}[x,y]/I(X)$ where $I(X)$ is the radical ideal of $X$, and $A(\mathbb{C})=\mathbb{C}[t].$ Given the definition of $\varphi^*$, which is $x\mapsto t^2$ and $y\mapsto t^3,$ one observes that $t\in \mathbb{C}[t]$ does not lie in the image of $\varphi^*.$ This shows $\varphi$ is not an isomorphism. I read a text somewhere that says that in fact there is no isomorphism between $X$ and $\mathbb{C}.$ I need some guidance to prove this. At this stage, I know very little of algebraic geometry. So I tried to use the idea as above. If $X$ and $C$ were isomorphic, then their coordinate rings would also be. The goal now would be to arrive at a contradiction. I know that the maximal ideals of $A(X)$ and $A(\mathbb{C})$ correspond to points of $X$ and $\mathbb{C},$ respectively. But I don't know if this might lead me anywhere. If it does, how can I proceed? Thanks!","['algebraic-geometry', 'commutative-algebra']"
1801440,Probability of being poisoned,"You are playing a game in which you have $100$ jellybeans, $10$ of them are poisonous (You eat one, you die). Now you have to pick $10$ at random to eat. Question : What is the probability of dying? How I tried to solve it: Each jellybean has a $\frac{1}{10}$ chance of being poisonous. Since you need to take $10$ of them, I multiply it by $10$ which gave $1$ (Guaranteed death). How other people tried to solve it : Each jellybean is picked out separately. The first jellybean has a $\frac{10}{100}$ chance of being poisonous, the second -- $\frac{10}{99}$, the third -- $\frac{10}{98}$ and so on.. which gives a sum of roughly $\sim 1.04$ (More than guaranteed death!) Both these results make no sense since there are obviously multiple possibilities were you survive since there are $90$ jellybeans to pick out of. Can someone explain this to me?",['probability']
1801454,Inverse Function Theorem Question.,"The inverse function theorem is a general statement about finding local open sets and an inverse on those sets. The standard proof by Spivak doesn't tell you about the sizes of the sets. A different proof using the contraction mapping theorem does using the norm of the differential, - is it possible to recover this estimate within Spivaks proof? Or is the result strictly stronger? For clarity, I mean set containment with reference to balls or cubes. For example if we have a C^1 transformation F with nonzero jacobian where the differential is approximately equal to the identity on some set S, we can say that for a circle C contained in S, that F(C) contains a circle slightly smaller than C and is contained in a circle slightly larger than F(C).","['multivariable-calculus', 'real-analysis']"
1801491,Can we use sequences to test continuity of a weak$^*$-continuous operator?,"Let $X,Y$ be Banach spaces. Now assume we have a map $T:X'\rightarrow Y'$ where $X'$ and $Y'$ are equipped with the weak$^*$ topology and not the norm topology. Can I infer from this that an operator satisfying $x_n'(x) \rightarrow x'(x) \Rightarrow (Tx_n')(x) \rightarrow (Tx')(x)$ for $x_n',x' \in X'$ is continuous in the weak $^*$ topology? If anything is unclear, please let me know.","['real-analysis', 'operator-theory', 'functional-analysis', 'general-topology', 'analysis']"
1801493,Prove that $\text{rank } T = \operatorname{rank} T^2 \iff \operatorname{Im}T \cap \ker T = \{ \vec 0\}$,"$\newcommand{\r}{ \operatorname{rank} } $ Let $T: V\to V$ be a linear transformation with $\dim V< \infty$ . Prove that: $$ \r T = \r T^2 \iff \operatorname{Im} T \cap \ker T = \{ \vec 0 \}.$$ $""\Rightarrow""$ Let $\r T = \r T^2$ . Then, by rank - nullity theorem we have that $$\dim \ker T  =\dim \ker T^2 \tag 1.$$ But it is always true that: $\ker T \subseteq \ker T^2 .\tag 2$ By $(1),(2)$ we have that $\ker  T = \ker T^2.$ So, instead of $\r T = \r T^2$ we can say that $\ker T = \ker T^2$ and we need to prove that $\operatorname{Im} T \cap \ker T = \{ \vec 0\}$ . Proof: Suppose that there is a $z \in \operatorname{Im}T \cap \ker T$ with $z \neq 0$ . Since $z \in \ker T \implies T(z) = 0$ . Also, since $z \in \operatorname{Im}T \implies \exists y\in V$ such that $T(y) = z \implies T^2(y) = T(z) = 0.$ But this implies that $y \in \ker T^2 $ and by our hypothesis we have that $y \in \ker T \implies T(y) = 0 = z, $ which is absurd, because we assumed that $z \neq 0$ . $""\Leftarrow""$ We need to prove that $\ker T = \ker T^2$ or $\ker T^2  \subseteq \ker T.$ Proof: Let $x \in \ker T^2$ , which implies $T^2(x) = T\left(T(x)\right) = 0$ . It is implied $T(x) \in \ker T,$ but also $T(x) \in \operatorname{Im}T.$ Thus, $T(x) \in \operatorname{Im}T \cap \ker T = \{0\}$ . Thus, $T(x) = 0 \implies x \in \ker T.$ I would like to know if my reasoning is correct and if all the points are clear. Also, I would like to know if there is any shorter proof.","['linear-algebra', 'proof-verification', 'linear-transformations']"
1801498,Is this an exponential family of distributions? from casella and berger 6.20,"I am trying to do 6.20 in Casella and Berger part d. The solutions manual says that the order statistics are minimal sufficient and not complete. I understand their logic, but why doesn't this work? The distribution is $$f(x)=e^{-(x-\theta)}e^{-e^{-(x-\theta)}}$$ The range is $-\infty<x<\infty$  $-\infty<\theta<\infty$ I think this breaks into an exponential family with $$f(x)=e^{-x}e^{\theta}e^{-e^{-x}e^{\theta}}$$ And this would imply $\sum{e^{-x}}$ is complete sufficient. A quick computation makes me think its minimal as well. Am I correct?
The sample is $X_1, X_2....X_n$ iid from this distribution.","['statistics', 'probability', 'statistical-inference', 'probability-distributions']"
1801512,Finite Number of Partitions of Unity in a Compact Hausdorff Space,"I'm working on this proof in Gamelin ""Introduction to Topology"" and I think I'm almost at the result, I'm just a little stuck with how to proceed. It is this. Let $X$ be a be compact Hausdorff space and let {$U_\alpha$}$_{\alpha \in A}$ be an open cover of $X$. Show that there exist a finite number of continuous valued functions $h_1, . . ., h_2$ on $X$ with the following properties: (a) $0\leq h_j \leq1$, $1\leq j \leq m$, (b) $\Sigma h_{j} = 1$ (c) For each $1\leq j \leq m$, there is an index $\alpha_{j}$ s.t. the closure of the set {$x : h_{j}(x) > 0$} is contained in $U_{\alpha_{j}}$ So I know by a theorem in the book that compact Hausdorff spaces are normal. I took a point $x\in X$ and noted that the {$x$} is closed since the space is Hausdorff. By definition of open cover, $\exists$ some $U_{\alpha}$ in the open cover s.t. $x\in U_{\alpha}$. The complement, $X-U_{\alpha}$ is closed. Further by normality, $\exists$ open sets $V,W$ s.t $V \cap W=\emptyset$ and s.t. {$x$}$\subseteq V$ and {$X-U_{\alpha}$}$\subseteq W$. $W$ is open hence $X-W$ is closed. Further $U_{\alpha}\subseteq X-W$ and $V\subseteq U_{\alpha} \subseteq X-W$. $\overline{V}$ is the smalest closed set containing V hence $\overline{V}\subseteq \overline{U_{\alpha}} \subseteq {X-W}$. Now I want to apply Urysohn's Lemma which would say here that $\exists$ a continuous function $g$ s.t. $g$({x})=1 and $g$({$X-W$})=0. So I think I've shown properties (a) and (c), but I'm not sure where to go to show that there is only a finite number of these functions. Couldn't I just do this same process at all points $x \in X$ and find perhaps infinitely many of these functions? Thanks for any help you can offer. Edit: So I considered and thought about what you suggested and I think I can continue from where I left off with some of your input and some of the ""Remark"" from the textbook. I think my construction would suggest that supp($g_{x}$)=$V_{x}$={$y\in X : g_{y}(x) > 0$}. For each $x\in X$ I can find another such function $g_{i}$ and another supporting set $V_{x_{i}}$. Since $X$ is compact, I can choose $x_{1}, x_{2}, ... , x_{n}$ s.t. the resulting collection {$V_{x_{i}}$} is a finite sub-cover of $X$. Hence, I have finite number of functions with these properties? Thanks again.",['general-topology']
1801522,How to calculate duration of event at different speeds,"Specifically I want to figure out the formula which will tell me: how long will it take to watch this video (normal length $L$) at speed $x$. I think this will be asymptotic, no matter how fast you play a video, it will take some time, even as that dwindles down towards $0$, it will never hit.  I also assume that playing a video at speed $x=2$ would mean $\text{duration} = \frac{L}{2}$, but I'm unclear how to translate that into a general equation.  For instance, when $x=1.4$. A brief explanation of the method of calculating this would be best.","['algebra-precalculus', 'asymptotics']"
1801525,Greatest Integer Function,"OK so it's been a while since I've done this math. I'm familiar with graphing greatest integer functions as this $[3x]$. I've bumped into this problem and I can't quite figure out how to graph it. 
$$f (x) = x [2x]$$
How do I go about the $x$ outside?","['algebra-precalculus', 'calculus']"
1801543,Surface of the intersection of $n$ balls,"Suppose there are $n$ balls (possibly, of different sizes) in $\mathbb R^3$ such that their intersection $\mathfrak C$ is non-empty and has a positive volume (i.e. is not a single point). Apparently, $\mathfrak C$ is a convex body with a piecewise smooth surface — a ""quilt"" of sphere fragments. Let $f(n)$ be the maximal number of fragments that can be achieved for a given $n$. Is there a simple formula or recurrence relation for $f(n)$?","['convex-geometry', 'combinatorial-geometry', 'spheres', 'geometry', 'surfaces']"
1801556,Bayes' Theorem and Law of total propability for CDF,"The calculation of conditional probability is the same for conditional PDF and CDF(according to a number of questionable sources: first , second ) (I will use rough notation, with just $x$ and $y$): $$F(x \ | \ y) = \frac{F(x,y)}{F(y)}, \ \  f(x \ | \ y) = \frac{f(x,y)}{f(y)}  $$ The Bayes' Theorem for probability density functions looks like $$f(x \ | \ y) = \frac{f(y \ | \ x)f(x)}{f(y)}$$ and can be derived using second definition above. Looks like the same can be postulated for cumulative distribution function: $$F(x \ | \ y) = \frac{F(y \ | \ x)F(x)}{F(y)}$$ Really: $$\frac{F(x,y)}{F(y)} = F(x \ | \ y) = \frac{F(y \ | \ x)F(x)}{F(y)},$$ $$\frac{F(x , y)}{F(x)} = F(y \ | \ x).$$ Now the question - ""law of total probability"" for PDFs: $$f(x \ | \ y) = \frac{f(y \ | \ x)f(x)}{\int_{\Omega}f(y \ | \ x)f(x)dx}$$ Can it be expressed in terms of CDFs somehow (I don't have $F(y)$)? EDIT: maybe some computational approach. I know that I can set in joint CDF some very big $x$: $\lim_{x \rightarrow \infty} F(x,y) = F(y), \ F(x,y)=F(y\ | \ x)F(x)$, but I can't obtain joint CDF from my data. I know also that I can find find ""derivative"": $f(x) \approx \frac{F(x+\Delta)-F(x-\Delta)}{\Delta}$, but it will be poor estimate. I have: $F(y \ | \ x)$ and $F(x)$ estimated - both functions of $x$ ($y$ is actual observation data).","['bayes-theorem', 'probability-theory', 'probability', 'statistics']"
1801562,How to show that $ \int_0^x\left[\int_0^uf(t)dt \right] du = \int_0^xf(u)(x-u)du$?,"I have been asked to show that 
$$
 \int_0^x\left[\int_0^uf(t)dt \right] du = \int_0^xf(u)(x-u)du.
$$ 
But it has not been specified whether or not $f$ is continuous or if it has an anti-derivative. I have shown this is true if $f$ does have an anti-derivative but can't find a way to show it's true otherwise. Is this statement true even if $f$ does not have an anti-derivative or does it become nonsense? I appreciate any help. My proof for when $f$ has an anti-derivative, $F$
\begin{align}
 \int_0^xf(u)(x-u)du &= \left[F(u)(x-u)\right]_0^x + \int_0^xF(u)du \\
& = -F(0)x + \int_0^xF(u)du \\
& = \int_0^x(F(u) - F(0))du \\
& = \int_0^x\left[\int_0^uf(t)dt\right]du 
\end{align}","['multivariable-calculus', 'calculus']"
1801564,Solving for homography - SVD vs linear least squares (Matlab),"so I had an assignment in Matlab for solving for a homography (and stitching images) and I solved it by converting the coordinates into homogeneous form (since scale doesn't matter in our assignment) by sticking a 1 at the end and then using the built in linear least squares operator (x = A\B that one). My though process was if a homography is a transformation such that Hx=x' (where x is a set of points in image1 and x' is a corresponding set of points in image2), if you solve the linear least squares equation H = x'\x, that should give you the H that most closely transforms all points in image1 to the domain of image2. However, all of the literature I read used the SVD method, where you try to minimize AX=0, where A is this massive matrix storing all of the coefficients and X is the elements of the homography in vector form (like the one you can see in the answer here: How to compute homography matrix H from corresponding points (2d-2d planar Homography) ) And you basically do [U S V] = svd(A) and then X is the last column of V (corresponding to the smallest singular value). From what I understand, A\B is supposed to be for nonhomogenous least squares and a homography needs homogeneous least squares. But after I converted my x,y points into homogenous coordinates, ran the built in linear equation solver and then used my homography to warp and image and stitch them together, I got a pretty good mosaic. Maybe it was because we were doing a pretty simple panorama and it just happened to work out. But is there some major difference between the two methods? I converted nonhomogenous coordinates into homogenous coordinates before using mldivide. That seems like it should be equivalent to the other method, but I'm not sure. Thanks ahead of time!","['matrices', 'linear-transformations']"
1801583,Distance in 3-space between two parallel vectors,"I know these two lines are parallel, but I don't know how to find the distance between them. Any suggestions? Line 1: (3, 4, 7) + {6, 2, 4}t Line 2: (-1, 5, -1) + {3, 1, 2}t I tried creating a triangle composed of an orthogonal line to both lines (the shortest distance between the lines), a hypotenuse that connects the two known points (3, 4, 7) and (-1, 5, -1), and a leg that connects (-1, 5, -1) to the intersection of the orthogonal line on line 2. On line 1, the point (3, 4, 7) was the intersection of the orthogonal line and the hypotenuse. I'm sorry if that doesn't make any sense, I did my best. I have no idea how to include pictures on this website, this is my first post. And this is not a homework assignment, it's a practice problem for a Multivariable Calculus class. I'm studying for a test we have next week. Any and all help is appreciated!","['multivariable-calculus', 'calculus']"
1801588,Most natural way to prove $\sum_{n=1}^{\infty}\frac{1}{n+2}$ diverges,"I don't know how my teacher wants me to prove that $$\sum_{n=1}^{\infty}\frac{1}{n+2}$$ diverges. All I know is that I have to use the $a_n>b_n$ criteria and prove that $b_n$ diverges. I tried this: $$\sum_{n=1}^{\infty} \frac{1}{n+2} = 
\frac{1}{1+2}+\frac{1}{2+2}+\frac{1}{3+2} + \cdots + \frac{1}{n+2} = \left(\frac{1}{1}+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n}\right)-\left(\frac{1}{1}+\frac{1}{2}\right) = \left(\sum_{n=1}^{\infty}\frac{1}{n}\right)-\frac{3}{2}$$ but I can't get a relationship between $a_n$ and $b_n$ of these series. If I go to the root of the comparsion criterion, I know that:
$$\sum_{n=1}^{\infty} \frac{1}{n+2} = 
\frac{1}{1+2}+\frac{1}{2+2}+\frac{1}{3+2} + \cdots + \frac{1}{n+2} = \left(\frac{1}{3}+\frac{1}{4}\right)+\left(\frac{1}{5}+\frac{1}{6}+\frac{1}{7}+\frac{1}{8}\right)+\cdots+\frac{1}{n}> \frac{2}{4}+\frac{4}{8}+\cdots+g_n$$ for some $g_n$ that I'm lazy to calculate. Therefore, by the properties of limits, since the righthand sum $p_n$ diverges, the lefthand sum $s_n$ diverges too, because $s_n>p_n$.  But I don't know if my teacher would accept that, I think she would only accept that I use the argument for the $a_n$ of the sum, not for the partial sum. I also thought about rewriting: $$\sum_{n=1}^{\infty} \frac{1}{n+2} = \sum_{n=3}^{\infty} \frac{1}{n}$$ but I don't see how it helps because the indexes are different. All I need is an argument that will work with the $a_n$ of the $\sum_{n=1}^{\infty}a_n$, in relation with $b_n$ from $\sum_{n=1}^{\infty} b_n$","['real-analysis', 'sequences-and-series', 'calculus', 'limits']"
1801597,Asymptotic expansion of $(1+\epsilon)^{s/\epsilon}$,"I have taken the logarithm of this expression and computed the Taylor expansion of the $\log(1+\epsilon)$ term but by doing this we're required to calculate powers of this series when using the definition of the exponential function, but this gives a combinatorical mess. eg. $(1+\epsilon)^{s/\epsilon}=\exp\left(s\right)\exp\left(s\delta\right),$
where $\delta=\sum_{i=1}^\infty \frac{(-1)^n \epsilon^n}{n+1}.$ I was wondering if anyone knows of a general formula for the coefficient of $\epsilon^n$ of $(1+\epsilon)^{s/\epsilon}$ as $\epsilon \rightarrow 0$. Any known papers on this topic too? Thanks","['power-series', 'epsilon-delta', 'asymptotics', 'sequences-and-series']"
1801612,"A stick is broken into two pieces, at a uniformly random chosen break point. Find the CDF.","I'm having trouble understanding how the CDF is found in the solution below: We can assume the units are chosen so that the stick has length $1$. Let $L$ be the length of the longer piece, and let the break point be $U \sim Unif(0,1)$. For any $l \in [1/2,1]$, observe that $L<l$ is equivalent to $U<l,1-U<l$, which can be written as $1-l<U<l$. We can thus obtain $L$'s CDF as $$F_L(l) = P(L<l)=P(1-l<U<l)=2l-1$$ Can someone please explain why $L<l$ is equivalent to $U<l,1-U<l$? Isn't the break point $U$ in between $[1/2,1]$?","['uniform-distribution', 'probability']"
1801627,Prove that $\det(A)=p_1p_2-ba={bf(a)-af(b)\over b-a}$,"Let $f(x)=(p_1-x)\cdots (p_n-x)$ $p_1,\ldots, p_n\in \mathbb R$ and let $a,b\in \mathbb R$ such that $a\neq b$ Prove that $\det A={bf(a)-af(b)\over b-a}$ where $A$ is the matrix: $$\begin{pmatrix}p_1 & a & a & \cdots & a \\ b & p_2 & a & \cdots & a \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ b & b & b & \cdots & p_n \end{pmatrix}$$ that is the entries $k_{ij}=a$ if $i<j$, $k_{ij}=p_i$ if $i=j$ and $k_{ij}=b$ if $i>j$ I tried to do it by induction over $n$. The base case for $n=2$ is easy $\det(A)=p_1p_2-ba={bf(a)-af(b)\over b-a}$ The induction step is where I don´t know what to do. I tried to solve the dterminant by brute force(applying my induction hypothesis for n and prove it for n+1) but I don´t know how to reduce it. It gets horrible. I would really appreciate if you can help me with this problem. Any comments, suggestions or hints would be highly appreciated","['polynomials', 'matrices', 'determinant', 'induction', 'linear-algebra']"
1801632,References on the moduli space of flat connections as a symplectic reduction,"In their Yang Mills equations over Riemann surfaces paper, Atiyah & Bott famously remark that the moduli space of flat connections on a principal bundle over a compact orientable surface may be obtained as a symplectic reduction on the space of connections (with the curvature as the moment map). In that paper, however, it is only a passing remark. One must be careful about this construction since the spaces involved are infinite-dimensional (e.g. the space of connections). I've only found references which ignore this technical detail and present the ideas in analogy to the finite-dimensional case. While they are helpful to give a simple understanding of what's going on, I was wondering if there is a reference that actually goes through the infinite-dimensional analysis to formalize this process. Any recommendation is appreciated.","['symplectic-geometry', 'reference-request', 'differential-geometry']"
1801649,Ration of sum and Product of Trigonometric expression.,"If $A,B,C \in \mathbb{R}$ and $\displaystyle\cos(A-B)+\cos(B-C)+\cos (C-A)=-\frac{3}{2}\;,$ Then $\displaystyle \frac{\sum \cos^3(\theta+A)}{\prod\cos(\theta+A)}\;, $ Where $\theta \in \mathbb{R}$ $\bf{My\; Try::}$ Given $\displaystyle\cos(A-B)+\cos(B-C)+\cos (C-A)=-\frac{3}{2}\;,$ after expanding, We get $$(\cos A+\cos B+\cos C)^2+(\sin A+\sin B+\sin C)^2=0$$ So we get $$\cos A+\cos B+\cos C=0$$ and $$\sin A+\sin B+\sin C=0$$ Now How can I solve after that, Help Required, Thanks",['trigonometry']
1801751,"The value of $(a+b)$, according to the question.","My friend gave me a question I tried my best, but I'm low on triangle concept. Points $ O, A, B, C... $ are shown in the figure where $ OA=2AB=4BC=...$ and so on. Let $A$ be the centroid of a triangle whose orthocentre and circumcentre are $(2,4)$ and $(\frac72,\frac52)$, respectively, if an insect starts moving from the point $O=(0,0)$ along the straight line in zig zag fashions and terminates ultimately at point $P(a,b)$, then find the value of $(a+b)$. I tried using the collinearity property of centroid, circumcentre and orhtocentre, and the distance property also, but reached nowhere. Please help.","['triangles', 'geometry']"
1801753,How to generate correlated random numbers with specific distributions?,"After read the answers of some similar questions on this site, e.g., Generate Correlated Normal Random Variables Generate correlated random numbers precisely I wonder whether such approaches can assure the specific distributions of random variables generated. In order to make it easier to present my question, let us consider a simple case of creating correlated two uniform continuous random variables on $[0,1]$ with correlation coefficient $\dfrac{1}{2}=\rho$. The methods by Cholesky decomposition (or spectral decomposition, similarly) first generates $X_1$ and $X_2$ which are independent pseudo random numbers uniformly distributed on $[0,1]$, and then creates $X_3=\rho X_1+\sqrt{1-\rho^2} X_2$. The $X_1$ and $X_3$ thus created are random variables  with correlation coefficient $\rho$. But the problem is, $X_3$ 's probability density fuction is triangle /trapezoid distribution which can be deducted by the convolution of the density functions of $X_1$ and $X_2$. The probability density functions of $\rho X_1$ and $\sqrt{1-\rho^2} X_2$ are: The convolution (sum) of them $X_3$ has density function: This means, the distribution of $X_3$ is not the desired uniform one on $[0,1]$. What should I do in order to create  random variables uniformly distributed on $[0,1]$ with correlation coefficient $\rho$ ? The similar issue persists when I want to create multiple correlated random variables with predefined correlation matrix. Considering the pseudo random variables usually are not really independent with a correlation coefficient between -1 and 1, it seems that: it is difficult to generate numerically independent $[0,1]$ uniform random variables since the uncorrelation transformation seems to always change the distribution profile. PS: Before asking this question, I had read the following questions and links but didnot find an answer : http://www.sitmo.com/article/generating-correlated-random-numbers/ http://numericalexpert.com/blog/correlated_random_variables/ https://en.wikipedia.org/wiki/Whitening_transformation","['correlation', 'probability-theory', 'probability-distributions', 'probability', 'random-variables']"
1801760,Intersection of subgroups is a subgroup: What if collection of subsets is empty?,"Theorem: The intersection of any arbitrary collection of subgroups of a group is again a subgroup. http://groupprops.subwiki.org/wiki/Intersection_of_subgroups_is_subgroup I don't understand the comment at this link: ""If the collection  is empty, the intersection is defined to be the whole group. In this case, the intersection is clearly a subgroup."" Should the theorem not be for ""any arbitrary non-empty collection of subgroups?"" (Edit: Relatively easy to formally prove using ordinary logic and set theory.)","['logic', 'group-theory', 'elementary-set-theory']"
1801800,questions about 2 sample t-tests,"So I'm just a bit confused about 2 sample t-tests and just want to write out what I think I know and see if that's correct, so if anyone could tell me whether or not what I'm writting is true that would be great. What I'm mostly asking about is if you have 2 samples with populations $X$ and $Y$ respectively and you want to measure at some confindence intervel some hypothesis about the difference of their means. The part that's confusing me is I know 2 methods of doing this and I'm not sure which should be applied where so the first one is: 1) Let me start by saying you're always given $\sum x, \sum y, \sum x^2, \sum y^2$. So what we do is we calculate $\overline x, \overline y$ and we calculate the estimate for $Var(X)$ and $Var(Y)$ using the formulas $$\frac{1}{n}(\sum x^2 -\frac{(\sum x)^2}{n})$$ and then once we have both variances we then define $Z=X-Y$ for example, then we calculate $Var(Z)$ by $$Var(Z)=\frac{Var(X)}{n_x}+\frac{Var(Y)}{n_y}$$
and then we get our t-value by 
$$t=\frac{\overline x - \overline y}{Var(Z)}$$
and then simple check the t-table. 2) Method 2 goes by calculating the pooled estimate of population variance through:$$S^2=\frac{\sum (x-\overline x)^2+\sum (y - \overline y)^2}{n_x + n_y -2}. $$ Then once we have that we calculate the t value using: $$t=\frac{\overline x - \overline y}{S^2(\frac{1}{n_x}+\frac{1}{n_y})}$$ Now my understanding is that method 2 requires the assumption that the variance of both $X$ and $Y$ is equal, is that the only difference between the two? Is the second method usually more accurate if the variance is actually equal","['statistics', 'statistical-inference']"
1801816,What is the fallacy of this trigonometrical proof that $1=-1?$,"I have this equation which I solved- $$\sin^4x-\cos^4x=1$$
  $$\implies -\cos^4x=1-\sin^4x$$
  $$\implies-\cos^4x=(1+\sin^2x)(1-\sin^2x)$$
  $$\implies-\cos^4x=(1+\sin^2x)\cos^2x$$
  $$\implies-\cos^2x=(1+\sin^2x)$$
  $$\implies-(\cos^2x+\sin^2x)=1$$
  $$\implies-1=1$$ I came across this thing while solving a problem and I am at total loss why this is coming.Where am I wrong? Thanks for any help!!","['fake-proofs', 'trigonometry']"
1801828,"$a_n = \frac{1}{n}b_n$, $\lim b_n = L>0, L\in\mathbb{R}$, prove $\sum a_n$ diverges","I have to prove that if $$a_n = \frac{1}{n}b_n$$for $n\ge 1$ and $$\lim_{n\to\infty} b_n = L>0, L\in\mathbb{R}$$ then $$\sum_{n=1}^{\infty} a_n$$ diverges. My idea was to show that it's not true that $a_n\to 0$ but I guess it's true because in $\frac{1}{n}b_n$, $b_n$ is limited because converges, and $\frac{1}{n}$ goes to $0$, and there is a theorem that says that when these two things happen in a product, it goes to $0$. So I cannot affirmate anything with this result. I guess it has something to do with comparsion but I cannot find any good comparsion between $a_n$ and $\frac{1}{n}b_n$","['real-analysis', 'sequences-and-series', 'calculus', 'limits']"
1801830,On the proof that every positive continuous random variable with the memoryless property is exponentially distributed,"The theorem to prove is: $X$ is a positive continuous random variable with the memoryless property, then $X \sim Expo(\lambda)$ for some $\lambda$ . The proof is explained in this video , but I will type it out here as well. I would like to get some clarification on certain parts of this proof. Proof Let $F$ be the CDF of $X$ , and let $G(x)=P(X>x)=1-F(x)$ . The memoryless property says $G(s+t)=G(s)G(t)$ , we want to show that only the exponential will satisfy this. Try $s=t$ , this gives us $G(2t)=G(t)^2,G(3t)=G(t)^3,...,G(kt)=G(t)^k$ . Similarly, from the above we see that $G(\frac{t}{2})=G(t)^\frac{t}{2},...,G(\frac{t}{k})=G(t)^{\frac{1}{k}}$ . Combining the two, we get $G(\frac{m}{n}t)=G(t)^\frac{m}{n}$ where $\frac{m}{n}$ is a rational number. Now, if we take the limit of rational numbers, we get real numbers. Thus, $G(xt)=G(t)^x$ for all real $x>0$ . If we let $t=1$ , we see that $G(x)=G(1)^x$ and this looks like the exponential. Thus, $G(1)^x=e^{xlnG(1)}$ , and since $0 <G(1) \leq  1$ , we can let $lnG(1)=-\lambda$ . Therefore $e^{xlnG(1)}=e^{-\lambda x}$ and only exponential can be memoryless. So there are several parts that I am confused about: Why do we use $G(x)=1-F(x)$ instead of just $F(x)$ ? What does the professor mean when he says that you can get real numbers by taking the limit of rational numbers. That is, how did he get from the rational numbers $\frac{m}{n}$ to the real numbers $x$ ? In the video, he just says that $G(x)=G(1)^x$ looks like an exponential and thus, $G(x)=G(1)^x=e^{xlnG(1)}$ . How did he know that this is an exponential?","['exponential-distribution', 'probability-theory', 'proof-explanation', 'probability-distributions']"
1801836,Relationship between subset medians and the median,"Suppose we have a set of data $A = \{a_1, a_2, \dots, a_n\}$ and $B = \{b_1, b_2, \dots, b_n \}$. So there are $2n$ elements in total. Further suppose the median of $A$ and $B$ is $a$ and $b$, respectively. I'm wondering if whether there is any relationship between the median $m$ of $A \cup B$ and $a$, $b$? Now suppose we divide the data sets into $A_L$, $A_R$, $B_L$, and $B_R$ so that $A_L$ contains all the elements of $A$ that are less than or equal to the median $a$, $A_R$ contains all the elements that are greater than or equal to $a$, and with a similar definition for $B$. Can we narrow down the search for the actual median by knowing $a$ and $b$? For example, can we decide if $m$ lies in any one or more of $A_L$, $A_R$, $B_L$, or $B_R$ (but not all)? I'm not sure if this question makes sense since I thought of it randomly but I am extremely curious to know if there is a real answer or to understand why there isn't.","['statistics', 'data-analysis', 'median']"
1801838,Determinant of a large block matrix,"$\newcommand{\lmt}{\left[\begin{matrix}}$
$\newcommand{\rmt}{\end{matrix}\right]}$ Hi, I was reading through a proof of the number of domino tilings of a $(2n)\times(2n)$ chessboard, and somewhere in the proof was the following unjustified claim: Let $A=\lmt 0&1&0&\cdots&0\\
-1&0&1&\ddots&\vdots\\
0&-1&0&\ddots&0\\
\vdots&\ddots&\ddots&\ddots&1\\
0&\cdots&0&-1&0
\rmt$ and $B=\lmt 0&1&0&\cdots&0\\
1&0&1&\ddots&\vdots\\
0&1&0&\ddots&0\\
\vdots&\ddots&\ddots&\ddots&1\\
0&\cdots&0&1&0
\rmt$. In case my notation isn't clear, they have $\pm1$ on the superdiagonal and subdiagonal and $0$ everywhere else. Let $C$ be the matrix with blocks $\lmt -A&I&0&\cdots&0\\
I&-A&I&\ddots&\vdots\\
0&I&-A&\ddots&0\\
\vdots&\ddots&\ddots&\ddots&I\\
0&\cdots&0&I&-A
\rmt$. Let $p_B$ be the characteristic polynomial of $B$. Then, the claim is that $$ \det C = \det p_B(A). $$ This was stated without proof, so I'm wondering if this follows from a well-known theorem, or if there's a slick proof of it. Also, I'm curious to what extent this claim generalizes. Thanks!","['matrices', 'determinant']"
1801852,Restriction to equivalence relation is equivalence relation,"Let $\mathcal{R}$ be relation on $A$ and $A_0 \subseteq A$ . The $\mathbf{restriction}$ of $\mathcal{R}$ to $A_0$ is defined to be the
relation $\mathcal{R} \cap (A_0 \times A_0) $ . $\mathbf{Homework \; Problem:}$ Please prove that that the restriction of equivalence relation is also an equivalence relation. Attempt: Let $\mathcal{R}$ be equivalence relation on set $A$ . Pick any point $x \in A_0$ . We have to show that $(x,x) \in \mathcal{R} \cap (A_0 \times A_0) $ . But, we are given that $x \in A$ as well, so $(x,x) \in \mathcal{R}$ . Since $x \in A_0$ , then $(x,x) \in A_0 \times A_0$ . therefore, $(x,x) \in \mathcal{R} \cap ( A_0 \times A_0) $ . Next, suppose $(x,y) \in \mathcal{R} \cap (A_0 \times A_0) $ and since $ \mathcal{R} \cap (A_0 \times A_0) \subseteq \mathcal{R} $ , then $(x,y) \in \mathcal{R}$ . so $(y,x) \in \mathcal{R}$ . Also, we know $(x,y) \in A_0 \times A_0$ . So by definition of product of a set with itself, then $(y,x)$ must be in $A_0 \times A_0$ as well. Therefore $(y,x) \in \mathcal{R} \cap ( A_0 \times A_0 )$ . So we have symmetry. Finally, say $(x,y)$ and $(y,z) $ are in $\mathcal{R} \cap (A_0 \times A_0) $ . First, we obtain $(x,z) \in \mathcal{R}$ by transitivity of $\mathcal{R}$ . since $x,y,z \in A_0$ , then $ (x,z)$ must be in $A_0 \times A_0$ and so $(x,z) \in \mathcal{R} \cap (A_0 \times A_0)$ and so we get transitivity. Is this a sufficient argument? Thanks.","['relations', 'equivalence-relations', 'elementary-set-theory', 'proof-verification']"
1801867,Finding the Centre of an Abritary Set of Points in Two Dimensions,I am currently working on a program that needs to transform one set of coordinates by shifting them to the center of the screen. The points are offset from the middle of the screen - either to the left or to the right. The coordinates of the points are first presented within a rectangular bound. In the next step I need to figure out how to move them to the center of the screen. The number of points vary from $2$ to $5$ points. How can I... Find the center of a finite set of points? Translate the positions to a new set of coordinates centered around a new center?,['geometry']
1801911,Determinant of determinant is determinant?,"Looking at this question , I am thinking to consider the map $R\to M_n(R)$ where $R$ is a ring, sending $r\in R$ to $rI_n\in M_n(R).$ Then this induces a map. $$f:M_n(R)\rightarrow M_n(M_n(R))$$ 
Then we consider another map $g:M_n(M_n(R))\rightarrow M_{n^2}(R)$ sending, e.g. $$\begin{pmatrix}
\begin{pmatrix}1&0\\0&1\end{pmatrix}&\begin{pmatrix}2&1\\3&0\end{pmatrix}\\ \begin{pmatrix} 0&0\\0&0 \end{pmatrix}&\begin{pmatrix} 2&3\\5&2\end{pmatrix} \end{pmatrix}$$ to $$\begin{pmatrix}1&0&2&1\\
0&1&3&0\\
0&0&2&3\\
0&0&5&2\end{pmatrix}.$$ Is it true that $$\det_{M_n(R)}(\det_{M_n(M_n(R))}A)=\det_{M_{n^2}(R)}g(A)$$ for some properly-defined determinant on $M_n(M_n(R))?$ If this is true, then $\det_{M_n(R)}\operatorname{ch}_{A}(B)=\det_{M_n(R)}\circ\det_{M_n(M_n(R))}(f(A)-B\cdot I_{M_n(M_n(R))})=\det_{M_{n^2}}\circ g(f(A)-B\cdot I_{M_{n^2}(R)}),$ which is what OP of the linked question wants to prove. Any hint or reference is greatly appreciated, thanks in advance. P.S. @user1551 pointed out that determinant is defined on commutative rings only and $M_n(R)$ is in general a non-commutative ring. So I am thinking maybe we could use the Dieudonné determinant . In any case, I changed the question accordingly.","['matrices', 'linear-algebra', 'determinant']"
1801935,Does there exist a function such that $\lim_{x \to a} f(x) = L$ for all $a \in \mathbb R$ but $f(x)$ is never $L$?,"Does there exist a function $f:\mathbb R \to \mathbb R$ such that $\lim_{x \to a} f(x) = L$ for
  all $a \in \mathbb R$ but $f(x) \neq L$ for all $x$? I found such a function in $\mathbb Q \to \mathbb Q$, where $f(\frac{p}{q})= \text{first p+q digits of }\pi$ satisfies the above condition. However, I have not been able to extend it to reals. So, is such a function possible, and if it is, is there any explicit example preferably related to the above function in rationals?","['real-analysis', 'calculus', 'limits']"
1801940,Decouple a system of two second order differential equations,"I have a system of second-order differential equations that I want to decouple. they are, $\ddot{x} = \frac{\omega_1^2}{2} x + \omega_2 \dot{y}$ and $\ddot{y} = \frac{\omega_1^2}{2} y - \omega_2 \dot{x}$ I am thinking that I should use some transformation, but it just isn't clear in my head yet! Thanks!","['ordinary-differential-equations', 'systems-of-equations']"
1801950,Fake proofs using matrices,"Having gone through the 16-page-list of questions tagged fake-proofs , and going though both the relevant MSE Question and Wikipedia page , I didn't find a single fake proof that involved matrices . So the question (or challange) here is: what are some fake proof using matrices? In particular, the fake proof should use a property, an operation, ..., specific to matrices (or at least not present in $\mathbb{R}$ or $\mathbb{C}$), e.g. Noncommutativity (Non-)existence of an inverse Matrix sizes Operations as $\det$, $\text{trace}$, ... Eigenvalues and diagonalization Matrix decompositions and normal forms ... Note: It does not matter if the result being ""proven"" is correct or not. The fallacy in the proof itself is what matters. Examples: Proof that 1 = 0 Proof: it is a well-known fact that $(x+y)^2 = x^2 + 2xy + y^2$. 
  Now let
  $$x = \begin{pmatrix}0 & 1\\ 0 & 0 \end{pmatrix},\;\;y = \begin{pmatrix}1 & 0\\ 0 & 0 \end{pmatrix}.$$
  On the one hand, we have that
  $$ (x+y)^2 = \begin{pmatrix}1 & 1\\ 0 & 0 \end{pmatrix}^2 = \begin{pmatrix}1 & 1\\ 0 & 0 \end{pmatrix},$$
  on the other hand we have
  $$x^2 + 2xy + y^2 = \begin{pmatrix}0 & 0\\ 0 & 0 \end{pmatrix} + 2\begin{pmatrix}0 & 0\\ 0 & 0 \end{pmatrix} + \begin{pmatrix}1 & 0\\ 0 & 0 \end{pmatrix} = \begin{pmatrix}1 & 0\\ 0 & 0 \end{pmatrix}.$$
  Since two matrices are equal if and only if all their entries are equal, we conclude that $1 = 0$. The mistake here is that $x$ and $y$  do not commute. Thus $(x+y)^2 = x^2 + xy + yx + y^2 \neq x^2 + 2xy + y^2$. Proof that 2 = 0 Proof: We know that $\det (AB) = \det (BA)$, since $$\det (AB) = (\det A) (\det B) = (\det B) (\det A) = \det (BA).$$
  Now consider the matrices
  $$ A = \begin{pmatrix}1 & 0 & 1\\ 0 & 1 & 0 \end{pmatrix}, \; \; B = \begin{pmatrix}1 & 0\\ 0 & 1\\ 1 & 0 \end{pmatrix}.$$
  We have that
  $$AB = \begin{pmatrix}2 & 0\\ 0 & 1 \end{pmatrix}, \;\; BA = \begin{pmatrix}1 & 0 & 1\\ 0 & 1 & 0 \\1 & 0 & 1\end{pmatrix}.$$
  Hence $\det (AB) = 2$ and $\det (BA) = 0$, therefore $2 = 0$. The mistake here is that $\det$ is defined for square matrices only, and thus $\det AB = \det BA$ only holds in general if $A$ and $B$ are square.","['big-list', 'matrices', 'fake-proofs', 'soft-question', 'linear-algebra']"
1801980,Ideal generated by an element,"Let $R\left [ x \right ]$ denote the set of all polynomials with real coefficients and let A denote the subset of all polynomials with
constant term 0. Then A is an ideal of $R\left [ x \right ]$ and $ A=\left \langle x \right \rangle.$ Attempt to verify the above: Let $A =\left \{ p\left ( x \right ) \in R\left [ x \right ] \mid p\left ( x \right ) \text{has constant term 0} \right \}$ Using the Ideal test: Suppose $X\left ( x \right ),Y\left ( x \right ) \in A.$ Then, by the ideal test, $X\left ( x \right )+\left ( -\left ( Y\left ( x \right ) \right ) \right ) \in A$ Now, suppose $R\left ( x \right ) \in R\left [ x \right ] and p\left ( x \right ) \in A.$ Then, $R\left ( x \right )\cdot p\left ( x \right ) \in A$ since the constant term $0$ in $p\left ( x \right )$ kills any constant term. Thus, A is an ideal of $R\left [ x \right ]$ . To show that $A=\left \langle x \right \rangle, $ $x = \left \{ x^{1},x^{2},\cdot \cdot \cdot ,x^{n} \right \}$ Then, $\left \langle x \right \rangle=\left \{ \alpha_{1}x^{1}+\cdot \cdot \cdot +\alpha_{n}x^{n} \right \}$ and I suppose we would have shown that this is equivalent to A. But what confuses me is the notation for $\left \langle x \right \rangle$ . Shouldn't $\left \langle x \right \rangle=\left \{ x^{1},x^{2},\cdot \cdot \cdot ,x^{n} \right \}$ according to cyclic notation?","['ring-theory', 'group-theory', 'notation']"
1801985,Prove $A=B=\pi$,"$\gamma=0.57721566...$ $\phi=\frac{1+\sqrt5}{2}$ Let, $$A=\sum_{n=1}^{\infty}\arctan\left(\frac{(e+e^{-1})(\phi^{\frac{2n-1}{2}}+\phi^{\frac{1-2n}{2}})}{e^2+e^{-2}-1+\phi^{2n-1}+\phi^{1-2n}}\right)$$
$$B=\sum_{n=1}^{\infty}\arctan\left(\frac{(\gamma+\gamma^{-1})(\phi^{\frac{2n-1}{2}}+\phi^{\frac{1-2n}{2}})}{\gamma^2+\gamma^{-2}-1+\phi^{2n-1}+\phi^{1-2n}}\right)$$ (1) $$A=B=\pi$$ We found (1) accidentally while trying to search for $\pi$ in term of $\arctan(x)$ most idea are from Dr Ron Knott his Fibonacci site. Can anyone prove (1)",['sequences-and-series']
1801992,"Real Analysis, Folland Proposition 1.7 elementary family","Definition - An elementary family is a collection $\varepsilon$ of subsets of $X$ such that i.) $\emptyset\in \varepsilon$ ii.) if $E,F\in \varepsilon$ then $E\cap F\in \varepsilon$ iii.) if $E\in \varepsilon$ then $E^c$ is a finite disjoint union of members of $\varepsilon$ 1.7 Proposition - If $\varepsilon$ is an elementary family, the collection $\mathcal{A}$ of finite disjoint unions of members of $\varepsilon$ is an algebra. Attempted proof: i.) If $A,B\in \varepsilon$, set $B^c = \bigcup_{1}^{n} C_j$ where $\{C_j\}_{1}^{n}\subseteq \varepsilon$. Then $$A\setminus B = A\cap B^c = A\cap \left(\bigcup_{1}^{n}C_j\right) = \bigcup_{1}^{n}C_j\cap A$$ which is a finite disjoint union of members of $\varepsilon$. Thus we have closure under complements. ii.) Now we can write $A\cup B = (A\setminus B)\cup B$ and we can see that $A\cup B\in \mathcal{A}$ (If more details needs to be provided please let me know) iii.) Suppose for a fixed $n$ we always have $\bigcup_{1}^{n} A_j\in \mathcal{A}$ whenever $\{A_j\}_{1}^{n}\subset \varepsilon$. Thus, $$\bigcup_{1}^{n+1}A_j = A_{n+1}\cup \bigcup_{1}^{n}A_j$$ I am not sure where to go from here. Any suggestions on this is greatly appreciated. Please let me know if something is not clear in what I have written so far or if this can be more neatly presented.","['real-analysis', 'measure-theory']"
1802001,Proving an algebraic identity,Prove: $$(a + b + c)(ab + bc + ca) - abc = (a + b)(b + c)(c + a)$$ Problem: I am not sure how to proceed after expanding the brackets on the RHS. I am not sure if I also expanded correctly. My solution is:,['algebra-precalculus']
1802009,Why is there no general solution for the general 2nd order linear ODE,"We can always solve a general first order linear ODE: $$y'(x)+a(x)y(x)=b(x).$$ I am looking for some intuition why the general 2nd order linear ODE $$y''(x)+a(x)y'(x)+b(x)y(x)=c(x) $$ does not have a gerneral formula. Is it mathematically impossible, or is there a chance, that someone will find a general solution? If it is mathematically impossible, is there any intuitive explanation to this phenomenon?",['ordinary-differential-equations']
1802020,Showing that $A\cup B$ is an element of $S$,"Let $S$ be a set such that if $A,B\in S$ then $A\cap B,A\triangle B\in S,$ where $\triangle$ denotes the symmetric difference operator. I would like to show that if $S$ contains $A$ and $B$, then it also contains $A\cup B, A\setminus B$. The difference was easy to find, but I am not succeeding with the union. I was able to show that each of the sets $$\emptyset,A\setminus B, B\setminus A,(A\cap B)\cup(A\setminus B),(A\cap B)\cup(B\setminus A),A\cup(A\triangle B),B\cup(A\triangle B),\\(A\cap B)\cup(B\setminus A)\cup(A\triangle B)$$ is an element of $S$. Combining these I could go on producing other elements, and probably I would eventually find $A\cup B$. But I have already spent too long on the problem, there must be a cleverer approach, right?",['elementary-set-theory']
1802030,Hardy- Littlewood Circle Method,"I'm currently trying to get to grips with the Hardy Littlewood circle method so I'm working through Vaughan's book. In the past I've been very bad for leaving a point behind if I don't follow it so I'm not trying to get rid of that habit by asking more questions.
For example on page 3 of his book ( see page here ) Vaughan tells us that for $F(z)= \sum_{m=1}^{\infty} z^{a_m}$ if $a_m=m^2,\  \rho=1- \frac{1}{n}$, with n large and $e(\alpha)= e^{2 \pi i \alpha} $ then the function F has 'peaks' when $z= \rho e(\alpha)$ is 'close' to the point $e(a/q)$ with $q$ 'not too large'. So (although I accept that this sounds stupid) I'm not sure about what he means by peaks, does he simply mean F has a peak or? and also why do the values of $z$ close the point $e(a/q)$ cause this? He then goes on to tell us that F has an asymptotic expansion in the neighbourhood of such points, roughly speaking valid when $|\alpha - a/q| \leq 1/(q \sqrt{n})$ and $ q \leq \sqrt{n}$. Here I'm not sure what he means by asymptotic expansion and why that neighbourhood is valid. Any help with any of this would be greatly appreciated.","['number-theory', 'analytic-number-theory', 'diophantine-approximation']"
1802032,Sum of all sine harmonics,"I was discussing this with my calculus teacher, but she didn't come up with anything. I would like to take an infinite sum of functions (sine specifically) but don't know how to do that.  I would like to sum every $\sin(nx)$ where $n$ is a positive multiple of $1/2$, where the amplitude of each sine is infinitesimal. Something like this, although I would guess this isn't correct form to do this sort of problem. $$
f(x) = \sum_{n=1}^\infty \sin\left(\frac{nx}{2}\right) dx
$$","['summation', 'integration', 'calculus']"
1802042,Inclusion of Schwartz space on $L^p$,"I'm looking for a proof of $\mathcal{S}(\mathbb{R}) \subset L^p(\mathbb{R})$ for $1 \leq p \leq \infty$. My informal probe follow like this: For any function $f \in L^p(\mathbb{R})$ exists a piecewise function $h_n$ such that $||h_n(x) - f(x)||_{L^p} \rightarrow 0 \ \text{with} \ n\rightarrow \infty \ (\forall x)$, and any $h_n$ could be approximated by compactly supported smooth functions ($C_{c}^{\infty}(\mathbb{R})$). And Since $C_{c}^{\infty}(\mathbb{R})$ is dense in $\mathcal{S}(\mathbb{R})$, then can be concluded that $\mathcal{S}(\mathbb{R}) \subset L^p(\mathbb{R})$. Any help to formalize that, or any different proof will be helpful.","['functional-analysis', 'lp-spaces', 'schwartz-space']"
1802044,"Real Analysis, Folland Theorem 1.9, extention of a measure to a complete measure","I have posted this theorem before but I am re-posting it again because I have a different question. Theorem 1.9 - Suppose that $(X,M,\mu)$ is a measure space. Let $\mathcal{N} = \{N\in M:\mu(N) = 0\}$ and $\overline{M} = \{E\cup F: E\in M, F\subset N, N\in\mathcal{N}\}$. Then $\overline{M}$ is a $\sigma$-algebra and there is a unique extension $\overline{\mu}$ of $\mu$ to a complete measure on $\overline{M}$. Claim 1 - $\overline{M}$ is a $\sigma$-algebra proof: i.) Since $M$ is a $\sigma$-algebra, $\emptyset\in M\subset \overline{M}$ so $\emptyset\in \overline{M}$. ii.) Suppose $B\in\overline{M}$ then there is an $E\in M$ such that $B = E\cup F$ where $F\subset N$ and $N\in\mathcal{N}$. Then 
\begin{align*}
X\setminus B &= X\cap B^c\\
&= X\cap (E\cup F)^c\\
&= X\cap ((E^c\cap N^c\cap F^c)\cup(F^c\setminus N^c))\\
&= X\cap ((E^c\cap N^c)\cup (F^c\cap N))\\
&= X\cap ((E\cup N)^c\cup (N\setminus F))
\end{align*}
Since $(E\cup N)^c\in M$ and $(N\setminus F)\subset N\in M$ with $\mu(N) = 0$, then $B^c\in\overline{M}$. iii.) Let $\{B_j\}_{1}^{\infty}\in\overline{M}$ then for each $j$ there is an $E\in M$ such that $B_j = E_j\cup F_j$ where $F_j\subset N_j$ and $\mu(N_j) = 0$. So, $$\bigcup_{1}^{\infty}B_j = \bigcup_{1}^{\infty}(E_j\cup F_j) = \bigcup_{1}^{\infty}E_j \cup \bigcup_{1}^{\infty}F_j$$ Note that $\bigcup_{1}^{\infty}F_j\subset \bigcup_{1}^{\infty}N_j$ and $\mu\left(\bigcup_{1}^{\infty}N_j\right) = 0$. So we have $\bigcup_{1}^{\infty}B_j\in\overline{M}$. Therefore $\overline{M}$ is a $\sigma$-algebra. Claim 2 - There is a unique extention $\overline{\mu}$ of $\mu$ to a complete measure on $\overline{M}$ Proof: We first need to show that $\overline{\mu}$ is well-defined. Suppose $E\cup F\in\overline{M}$, set $\overline{\mu}(E\cup F) = \overline{\mu}(E)$. This is well-defined since if $E_1\cup F_1 = E_2\cup F_2$ where $F_j\subset N_j\in\mathcal{N}$. Then we know that $E_1\subset E_1\cup F_1 = E_2\cup F_2 = E_2\cup N_2$ then $E_1\subset E_2\cup N_2$ and so by monotonicity $\mu(E_1)\leq \mu(E_2) + \mu(N_2) = \mu(E_2)$. Also, since $E_2\subset E_2\cup F_2 = E_1\cup F_1 = E_1\cup N_1$ then again by monotonicity $\mu(E_2)\leq \mu(E_1)+\mu(N_1) = \mu(E_1)$. Thus $\overline{\mu}$ is well-defined. I now need to show that $\overline{\mu}$ is a complete measure on $\overline{M}$ and that $\overline{\mu}$ is the only measure on $\overline{M}$ that extends $\mu$. Step 1 - Show $\overline{\mu}$ is a measure. Proof: i.) $\overline{\mu}(\emptyset) = \overline{\mu}(\emptyset \cup F) = 0$ since $F\subset N$ with $\mu(N) = 0$. ii.) Let $\{A_n\}_{1}^{\infty}\in\overline{M}$, disjoint, then there is an $\{E_n\}_{1}^{\infty}\in M$ and a sequence $\{F_n\}_{1}^{\infty}\subset N\in\mathcal{N}$ such that $A_n = E_n\cup F_n$ for all $n$. Note from part (iii.) of claim 1, $\cup_{1}^{\infty}F_n\subset \cup_{1}^{\infty}N_n$. Thus, 
\begin{align*}
\overline{\mu}\left(\bigcup_{1}^{\infty}A_n\right) = \overline{\mu}\left(\bigcup_{1}^{\infty}E_n\cup F_n\right) &=
\overline{\mu}\left(\bigcup_{1}^{\infty}E_n\cup \bigcup_{1}^{\infty}F_n\right)\\
&= \overline{\mu}\left(\bigcup_{1}^{\infty}E_n\right)\cup \overline{\mu}\left(\bigcup_{1}^{\infty}F_n\right)\\
&= \sum_{1}^{\infty}\overline{\mu}(E_n) + \sum_{1}^{\infty}\overline{\mu}(F_n)\\
&= \sum_{1}^{\infty}\overline{\mu}(A_n)
\end{align*} Thus $\overline{\mu}$ is a measure. Step 2 - Show $\overline{\mu}$ is a complete measure. Proof: Let $A\subset X$ and suppose there is an $B\in\overline{M}$ such that $A\subset B$ and $\overline{\mu}(B) = 0$. Set $B = E\cup F$ where $E\in M$ and $F\subset N\in M$ with $\mu(N) = 0$. Since $A\subset B = E\cup F = E\cup N$ since $F\subset N$ then by monotonicity $\mu(A)\leq \mu(E) + \mu(N) = \overline{\mu}(E) + 0 = \overline{\mu}(E)\leq \overline{\mu}(B) = 0$. Thus $A\in \overline{M}$? Step 3: Show $\overline{\mu}$ is a unique extention of $\mu$ Not exactly sure how to show this or if I already have shown this. These are the steps I have taken, please let me know if this is the correct sequence or any other additional comments that you have in regards to my proofs.","['real-analysis', 'measure-theory', 'proof-verification']"
1802112,The complete solution to a system of polynomials over $\mathbb{R}$,"If I am solving a positive-dimensional system of polynomials over $\mathbb{R}$, and specifically am searching only for real solutions, how do I know that my solution is complete and there are no other possibilities?  For example, consider the system: $$x_1^2 + x_2^2 = 1$$ I can solve it by letting $x_1 = t \in [-1,1]$, then $x_2 = \pm \sqrt{1-t^2}$ and I'm golden.  As far as I know there are no more real solutions that are unique (excluding the one that is the same up to the choice of parameter).  But how do I know that?  When the system gets more complicated, is there a nice way to figure it out? I'd be interested in a general proof over an arbitrary field as well.  However, I suspect my life is going to be made harder because I am working with the real solutions only, and as a result there are solutions over the field extension that I don't consider. Specifically, the equations are: $$f\sum_{i=1}^n x_{i4} - K1 = 0$$ $$f\sum_{i=1}^n x_{i3}x_{i2} - K_2 = 0$$ $$f\sum_{i=1}^n x_{i1}x_{i3} - K_3 = 0$$ $$f\sum_{i=1}^n p_{z1}x_{i2}x_{i3} + f\sum_{i=1}^np_{y1}x_{i1}x_{13} - K_4 = 0$$ $$f\sum_{i=1}^n p_{z1}x_{i4} - f\sum_{i=1}^np_{x1}x_{i1}x_{13} - K_5 = 0$$ $$f\sum_{i=1}^n -p_{y1}x_{i4} - f\sum_{i=1}^np_{z1}x_{i1}x_{13} - K_6 = 0$$ along with the ""circularity equations"": $$x_{i1}^2 + x_{12}^2 = 1$$ $$x_{i3}^2 + x_{14}^2 = 1$$ where $f \in \mathbb{R}^+$, $K_j \in \mathbb{R}$ (there is technically another set of circularity equations but they are on variables that do not appear in the above system).","['real-algebraic-geometry', 'polynomials', 'algebraic-geometry', 'systems-of-equations', 'quadratics']"
1802122,Minimum length $m$ of $n$ string with pairwise Hamming distance $m/2$,"I want to construct $n$ binary strings, each of the same length $m$ (to be determined), such that each pair of string has Hamming distance exactly $m/2$ (i.e. the strings disagree on $m/2$ positions). I want to minimize the length $m$ of the constructed sequences. I can do it with $m = 2^{n - 1}$ (I believe). My question: is $m$ necessarily exponential in $n$? Or is there some $c$ such that length $m \leq n^c$ is enough?","['combinatorial-designs', 'combinatorics', 'discrete-mathematics']"
1802163,Can Haar Measures Exist On Not Locally Compact Spaces?,"In a reading course on measure theory this semester I had the pleasure of preparing a lecture covering the existence-uniqueness of Haar measure on locally compact groups. Since the proofs (as presented in Cohn's book) depend on the local compactness already at the very start, it is obvious that the reasoning and constructions presented don't work for groups that aren't locally compact. I wonder (for the sake of curiosity, the presentation is already over and this is not homework) if there can exist Haar measures in those cases as well. Does anyone know of any examples of such groups, where existence or uniqueness doesn't hold? I have tried searching and thinking on my own, but I have trouble coming up with groups that aren't locally compact to begin with…","['measure-theory', 'locally-compact-groups', 'group-theory']"
1802175,placing balls inside ball [duplicate],"This question already has answers here : Can all circles of radius $1/n$ be packed in a unit disk, excluding the circle of radius $1/1$? (3 answers) Closed 8 years ago . Is it possible to put pairwise disjoint open 3d-balls with radii $\frac{1}{2},\frac{1}{3},\frac{1}{4},\dots$ inside a unit ball? not an original question,  I found it somewhere in the internet once, but without any answer.","['euclidean-geometry', 'geometry']"
1802220,Behavior of a Collatz-like mod-4 sequence: Do some numbers increase without limit?,"Define
\begin{eqnarray}
f(n) &=& (n-1)^2 \; \textrm{if} \; (n \bmod 4) = 1\\
f(n) &=& \lfloor n/4 \rfloor \; \textrm{otherwise}
\end{eqnarray}
and let $f^k(n) = f(f( \cdots (n) \cdots ) )$ be the result of applying $f(\;)$
$k$ times to $n$.
So $f^3(5)=0$ because the iterates produce the sequence $(5,16,4,1,0)$,
$f^8(13)=0$ because the sequence is $(13,144,36,9,64,16,4,1,0)$, and so on.
Many numbers map to $0$.
But some seem not to, i.e., to grow without bound. E.g., here is the start of
the sequence for $n=53$, the smallest ""problematic"" number:
$$
(53,2704,676,169,28224,7056,1764,441,193600,48400,12100,3025,9144576,2286144,571536,142884,35721,1275918400,318979600,79744900,19936225,\ldots)
$$
My question is: What is special about the numbers $53, 85, 77, 101, \ldots$ that seem to grow
  without bound?
  Can one prove that, for any particular number $n$, 
  $\lim_{k \to \infty}f^k(n) = \infty$ ?","['collatz-conjecture', 'sequences-and-series', 'arithmetic-dynamics']"
1802243,How to differentiate product of vectors (that gives scalar) by vector?,"I'm trying to understand derivation of the least squares method in matrices terms:
$$S(\beta) = y^Ty - 2 \beta X^Ty + \beta ^ T X^TX \beta$$
Where $\beta$ is $m \times 1$ vertical vector, $X$ is $n \times m$ matrix and $y$ is $n \times 1$ vector.
The question is: why $$\frac{d(2\beta X^Ty)}{d \beta} = 2X^Ty$$
I tried to derive it directly via definition of derivative:
 $$\frac{d(2\beta X^Ty)}{d \beta} = \lim_{\Delta \beta \to 0} \frac{2\Delta\beta X^T y}{\Delta \beta} = \lim_{\Delta \beta \to 0}   2\Delta\beta X^T y \cdot \Delta \beta^{-1}$$
May be the last equality must be as in the next line, but anyway I don't understand why $$2\Delta\beta \Delta \beta^{-1} X^T y  $$And, what is $\Delta \beta^{-1}$? Vectors don't have the inverse form. The same questions I have to this quasion:
$$(\beta ^ T X^TX \beta)' =2 X^T X \beta$$","['derivatives', 'scalar-fields', 'linear-algebra']"
1802251,which is the relationship between infinite set and the orbits of their points?,"I have been studying the proof of the following theorem: Theorem: Let's suppose that $X$ is some metric space and $X$ is a infinite set. If $f:X\to X$ is transitive and has dense periodic points then $f$ has sensitive dependence on initial conditions. It was proved by J. Banks, Here's a link Almost all the proof is clear, but I can't understand how it's possible to choose two arbitrary periodic points $q_{1}$ and $q_{2}$ with disjoint orbits $\mathcal{O}(q_{1},f)$ and $\mathcal{O}(q_{2},f)$. I know that this is related to the fact that $X$ is a infinite set, but i don't see it! Also, I have the following question: Question 2: Suppose that $q_{1},q_{2}\in Per(f)$, i.e, exists $n,m\in\mathbb{N}$ such that $f^{n}(q_{1})=q_{1}$ and $f^{m}(q_{2})=q_{2}$. If $X$ is infinite or finite, which is the relationship between $\mathcal{O}(q_{1},f)$ and $\mathcal{O}(q_{2},f)$?. $\mathcal{O}(q_{1},f)\cap \mathcal{O}(q_{2},f)\neq \emptyset$. If $n<m$ or $n>m$, $\mathcal{O}(q_{1},f)\subset \mathcal{O}(q_{2},f)$ etc,.. I've been thinking on and off about this questions for a couple of days,...","['general-topology', 'dynamical-systems']"
1802275,Why can we not just use the chain rule to derive $f(x) = x^x$?,"I know that in order to derive $f(x) = x^x,$ you have to take the log of both sides first and then derive it to get $f'(x) = x^x(ln(x)+1).$ I know that if you take the derivative directly using the chain rule, you get the wrong answer. Why is this? I assume it has something to do with the fact that the definition of the derivative has $h\rightarrow 0$ and we would potentially have $(x+h)^{x+h} \rightarrow 0^0$ somehow (which is indeterminate form), but I'm not immediately seeing this. EDIT: So as has been pointed out, this question has been answered elsewhere. Additionally, I did not provide enough information. Specifically: how am I applying the chain rule to get the wrong answer? (And I suppose 'What wrong answer am I getting?') As it turns out, I couldn't decide which function is the ""outside"" function and which function is the ""inside"" function. Initially, I did the power rule first to get
$$f'(x) = x\cdot x^{x-1} \cdot (x^x\cdot \ln(x))= x^{2x}ln(x),$$
which we've seen is wrong.","['derivatives', 'calculus']"
1802289,"Does $E[e^{it(aX + bY)}]=E[e^{itaX}]E[e^{itbY}]$ for every $a,b\in\mathbb{R}$ imply that $X$ and $Y$ are independent?","Let $X, Y$ be two random variables such that for every $\alpha, \beta \in \mathbb{R}$, 
$$E[e^{it(\alpha X + \beta Y)}]=E[e^{it\alpha X}]E[e^{it\beta Y}]$$ for all $t\in\mathbb{R}$.  Does it follow that $X$ and $Y$ are independent?",['probability-theory']
1802309,"How many maps $A \overset{f}{\rightarrow} A$ satisfy $f \circ f = f$ with the given set $A=\{a, b, c\}$. A few related questions inside.","I am trying to calculate how many maps $A \overset{f}{\rightarrow} A$ satisfy $f \circ f = f$ with the given set $A=\{a, b, c\}$. I would like to see the explicit mappings and learn how you constructed these mappings by hand, so I can apply these same techniques to future problems. I'm self-studying this summer and have a few follow up questions, this will help solidify my understanding of the problems being asked. Question 1 : What are the explicit mappings for this problem & how did you go about constructing them? Question 2: Is there a general procedure to calculate how many such mappings exist for any set A with an arbitrary number of elements? Many thanks for the help!","['combinatorics', 'functions']"
1802339,Condition probability distributions: Two people flipping fair coins,"Suppose that two people are playing a game where they each flip a fair coin 100 times. The winner of this game is the person who has flipped the most heads. What is the expected number of heads flipped by the winner? I understand that in general the probability of a given number of heads flipped will be given by the binomial distribution and we can approximate it using a normal distribution. On average, we expect them to both flip around the same number of heads, but conditional on the fact that there will be a winner, we should expect the number of heads of the winner to be slightly above 50. How does one get the distribution of the winning player from the initial distribution?","['conditional-expectation', 'probability', 'probability-distributions']"
1802365,Infimum of distance between point and (closed) set,"I'm having a little trouble with the following exercise: Let $V \subset\mathbb{R}^p$ be a non-empty, closed set and $a \in \mathbb{R}^p$. For $x, y \in \mathbb{R}^p$ we note $d(x,y) = \|x-y\|$. Furthermore, let $d(a,V) = \inf \{\, d(a,x) \mid x \in V \,\}$. Show that there exists $b \in V$ such that $d(a,V) = d(a,b)$. Hint: consider the set $V \cap \overline{B(a; R)}$ for a suitably chosen $R > 0$. Now I know that $a \in V \Leftrightarrow d(a,V) = 0$, so if $a \in V$ we can simply choose $b = a$. However, I am not quite sure how to prove this statement for $a \notin V$, and I don't really know how to use the hint that was given. I already looked at similar questions, but they all include theorems about compact spaces, and in this exercise $V$ isn't necessarily compact (or so I think). Any hints would be very much appreciated.","['supremum-and-infimum', 'analysis']"
1802387,prove $\sum_{k = 0}^{n} \binom{n}{k} \binom{m-n}{n-k} = \binom{m}{n}$,"prove $\sum_{k = 0}^{n} \binom{n}{k} \binom{m-n}{n-k} = \binom{m}{n}$ Attempt:I was thinking of trying to prove this through induction, but I am having trouble with a base case: base case:  let $n = 2$: $$LHS = \binom{2}{0} \binom{m-2}{2} + \binom{2}{1} \binom{m-2}{1} + \binom{2}{2} \binom{m-2}{0} \\ = \frac{(m-2)[(m-3) + 2!2!(m-3)!] + 2!}{2!} \ (after\ simplification)$$ $$RHS = \frac{m!}{2!(m-2)!}$$ But I am stuck as what to try next to at least equate these two expressions. Note: I took a look at Vandermonde's identity on wikipedia and the ensuing proof, but the proof still leaves out how to make the transition between the two initial expressions",['combinatorics']
1802414,Finding subgroups of $D_8$,"$$D_8=\{(),(1234),(13)(24),(1432),(13),(24),(14)(23),(12)(34) \}$$ Am I right to say that to find the subgroups, we have to make sure that the identity can be generated or is in the group and the inverse of each element can be generated or is in the group? Is there an easy way to do this though because it seems really time consuming.","['abstract-algebra', 'group-theory']"
1802415,Why was I wrong about the monster-gem riddler,"Every week I like to do the fivethirtyeight.com Riddler , an interesting and pleasantly challenging (at least for me) weekly math puzzle which comes out Fridays, with the answer and explanation to the previous problem being supplied when the new one is posted.  I just looked at the solution to last week's problem, and while I understand the explanation given, I am at a loss as to why my approach produced an incorrect answer. The problem is as follows: You're playing a game in which you kill monsters. Each time you kill a monster, it drops a gem, which you then collect.  The gems can be either common, uncommon or rare, with probabilities ${1\over2}$, ${1\over3}$, and ${1\over6}$, respectively.  If you play until you have 1 gem of each type, how many common gems will you have on average? The official answer is 3.65, and the explanation is available here (scroll down past the introduction and todays puzzle-- not quite halfway down the page).  My answer was 4.65 (interesting that I was off by exactly 1), and my reasoning was as follows: It's possible that you collect both other rarities before you find your first common gem.  This occurs with probability $ {1 \over 3} * {1 \over 4} +{1 \over 6 }*{2\over5}={3\over20}$.  In such cases you will stop after you find your first common gem, and the number you've collected (henceforth $N$) will be 1. In all other cases, play will proceed in two phases: Phase 1: you find some number $N_a$ (possibly 0) of common gems followed by a single non-common gem (call it type $a$) Phase 2: you find some number $N_b$ (possibly 0) of common gems, mixed with an irrelevant number of type $a$ gems, followed by a single gem of the other non-common type (call it type $b$). Now I make a couple of simplifying observations. First, since the case addressed above where the common gem was the last one we found corresponds to the case were $N_a=N_b=0$, we don't need to explicitly remove this case to avoid double-counting.  It represents the situation where $N$ exceeds $N_a+N_b$ rather than a case where the 2-phase model doesn't apply at all. Second, for given types $a$ and $b$, $N_a$ and $N_b$ are independent, because the individual gem drops are independent. Third, because there are (by definition) no type $b$ gems found in phase 1 and type $a$ gems found in Phase 2 are irrelevant, it doesn't matter which non-common type (rare or uncommon) is type $a$ and which is type $b$ because in either case $N_{rare}$ (the number of common gems found in the phase terminated by a rare gem) will be a geometrically distributed random variable with $p=$ (the probability of a rare gem being dropped given that an uncommon gem is not dropped) $= {1\over4}$, and likewise for $N_{uncommon}$. Thus we have
$$E[N]=E[N_{rare}]+E[N_{uncommon}]+{3\over20}=\frac{3\over5}{2\over5}+\frac{3\over4}{1\over4}+{3\over20}={3\over2}+3+{3\over20}={93\over20}=4.65$$ What's wrong with this method?","['puzzle', 'probability', 'probability-distributions']"
1802420,Is $\sin z/z$ analytic at the origin?,"For $z\in\Bbb C$ let
$$
f(z) = \frac{\sin z}{z}
$$
Along the real line this is well behaved, and approaches $1$ as $z\to 0$. But is $f(z)$ analytic at the origin ($z=0$)? I tried explicitly checking the Cauchy conditions but that gets ugly (unless I am missing something). The function I was originally interested in is
$$
g(z) = z\,\sin\left( \frac{1}{z} \right)
$$
which my gut tells me must not be analytic at the origin, but is the singularity essential or a pole, or the end of a branch cut or something even uglier?",['complex-analysis']
1802425,Proof of Tychonoff's Theorem for an undergrad,"In the midst of learning about compactness I come across Tychonoff's Theorem: Let $\{X_i : i \in \mathcal{A}\}$ be any collection of compact spaces. Then $\displaystyle\prod_{i \in \mathcal{A}}X_i$ is compact in the product topology. I've just come from the fact that a finite product of compact spaces is compact, and I also know from studying bases of topologies that uncountable products aren't necessarily as nice (for example, the box topology has some problems for uncountable products). The proof for Tychonoff's Theorem is: Omitted (this is much harder than anything we have done here). Internet searches lead to math overflow and topics that are very outside of my comfort zone. Is there a proof of Tychonoff's Theorem for an undergrad?","['alternative-proof', 'product-space', 'general-topology', 'compactness']"
1802442,Prove that it is sufficient to check $\lceil \log(k) \rceil$ pairs to tell if a set of integers is pairwise coprime,"I am reading chapter 31 of Introduction to Algorithms (CRLS) and I encountered some difficulties while solving 31.2-9. I managed to prove the first part of a problem, but I can't prove the generalized version. This is the problem statement: Prove that $n_1, n_2, n_3,$ and $n_4$ are pairwise relatively prime if and only if $gcd(n_1 n_2, n_3 n_4) = gcd(n_1 n_3, n_2 n_4) = 1$. More generally, show that $n_1, n_2, ..., n_k$ are pairwise relatively prime if and only
  if a set of $\lceil \log(k) \rceil$ pairs of numbers derived from the $n_i$ are relatively prime. Proof of the first part: $gcd(n_1 n_2, n_3 n_4) = 1$ means that $n_1 n_2$ and $n_3 n_4$ doesn't have any common factors, so $gcd(n_1, n_3) = gcd(n_1, n_4) = gcd(n_2, n_3) = gcd(n_2, n_4) = 1$. The same is for the second equation so, $gcd(n_1, n_2) = gcd(n_1, n_4) = gcd(n_3, n_2) = gcd(n_3, n_4) = 1%%$","['number-theory', 'algorithms']"
1802465,Questions about the distribution of $Y$ given the distributions of $X$ and of $Y$ conditionally on $X$,"$\newcommand{\Var}{\operatorname{Var}}\newcommand{\E}{\operatorname{E}}$Given: $X$ uniform on $(0,1)$ and $Y\mid X=x$ with distribution $N(x,1)$. Question 1: Determine $\E(Y^2)$ and $\Var (Y)$. Answer 1: 
$$ \Var(Y)= \E[\Var (Y \mid X)] + \Var[\E(Y\mid X)] = 1 + \Var(X) = 13/12$$ $$\Var(Y) = \E(Y^2) - [\E(Y)]^2 $$ Rewrite this into $$\E(Y^2) = \Var (Y) + (\E[\E(Y\mid X)])^2 = 4/3$$ Question 2: Show  $P[Y \le 1] = \int^1_0 \Phi(1-x)\,dx $ with $\Phi$ as standard normal cdf. Answer 2: $$P(Y \le 1 \mid X) = P(Y-x \le 1-x \mid X=x) = \Phi(1-x). $$ Then we take the expectation to get the probability we are looking for: $$P[Y \le 1] = E[P(Y \le 1 \mid X)] = \int^1_0 \Phi(1-x)f_X(x)\,x = \int^1_0 \Phi(1-x)\,dx$$ So that we get the desired result. I'm asking whether the answer to question 1 is right and for a little bit of help for the second one. EDIT: Just saw $f_{X,Y}(x,y) = f_{Y\mid X}(x,y)$. EDIT2: Thanks for the help guys really appreciate it!","['probability-theory', 'probability-distributions']"
1802477,If $\lim\limits_{A \rightarrow \infty} \sup\limits_{n} \frac{\int_{|x|>A}x^2 dF_n(x)}{\int_\mathbb Rx^2 dF_n(x)}=0$ then $\{F_n\}$ is tight,"Suppose $X_n$, $n \geq 1$, are random variables with distribution functions $F_n$ satisfying $EX_n^2 < \infty$ for all $n$ and $$\lim_{A \rightarrow \infty} \sup_{n} \frac{\int_{\{x: |x|>A\}}x^2 dF_n(x)}{\int_{\mathbb R}x^2 dF_n(x)}=0.$$ Show that $\{F_n\}$ is tight. By the limit in the condition, we know that, when $A$ is large enough, the fraction will be small, and we can obtain an estimate of $\frac{EX_n^2}{A^2}<2$, but I don't know how  to start from here to estimate $$\lim_{A \rightarrow \infty} \sup_{n}P(|X_n|>A).$$","['probability-theory', 'probability-distributions']"
1802488,References for gradient systems,"I am interested in the gradient system $$\dot{x}(t)=-\nabla f(x(t))$$ where $f:\mathbb{R}^n \to \mathbb{R}$ is a $C^{1,1}$ function (that is, a differentiable function whose gradient is Lipschitz continuous). I would be grateful if someone could propose some reference books related to this system.","['gradient-flows', 'ordinary-differential-equations', 'dynamical-systems']"
1802490,Definition of measurable space - sigma algebra,"A measurable space is a set $S$, together with a nonempty collection,
  $\mathcal{S}$, of subsets of $S$ satisfying the following two
  conditions: For any $A$, $B$ in the collection of $\mathcal{S}$, the set $A-B$ is also in $\mathcal{S}$ For any $A_1, A_2, ... \in \mathcal{S}$, their union is in $\mathcal{S}$. Source Is $\mathcal{S}$ a sigma algebra? The definition of sigma algebra states it's a collection of subsets of $X$ closed under countable union, complementation, and that it contains the empty set. Point 1. guarantees it contains the empty set, point 2. states its closed under countable union. But how 1. and 2. imply that it's closed under complement? How to prove that $\mathcal{S}$ contains $S$ as well, as it should (because it contains the empty set), given it really is a sigma-algebra?",['measure-theory']
1802497,Check if $f(x)=2 [x]+\cos x$ is many-one and into or not?,"If $f(x)=2 [x]+\cos x$ 
Then $f:R \to R$ is: $(A)$ One-One and onto $(B)$ One-One and into $(C)$ Many-One and into $(D)$ Many-One and onto $[ .]$ represent floor function (also known as greatest integer function
) Clearly $f(x)$ is into as $2[x]$ is an even integer and $f(x)$ will not be able to achieve every real number. Answer says option$(C)$ is correct but I cannot see $f(x)$ to be many-one as it does not look like that value of $f(x)$ is same for any two values of $x$ e.g. $f(x)= [x]+\cos x$, then $f(0)=f(\frac{\pi}{2})=1$ making the function many-one but can't see it happening for $f(x)= 2[x]+\cos x$ Could someone help me with this?","['calculus', 'functions']"
1802507,Subgroups of $\mathrm{Gal}(\overline{\mathbb{Q}}/\mathbb{Q})$,"If I'm understanding the main theorem of (infinite) Galois theory correctly, applied to $\mathrm{Gal}(\overline{\mathbb{Q}}/\mathbb{Q})$, it gives us: a) all its open subgroups are $\mathrm{Gal}(\overline{\mathbb{Q}}/K)$, with $K/\mathbb{Q}$ a finite extension. b) all its closed (and not-open) subgroups are $\mathrm{Gal}(\overline{\mathbb{Q}}/K)$, with $K/\mathbb{Q}$ an infinite extension. Is this correct? I've also heard that for example $\mathrm{Gal}(\overline{\mathbb{Q}}_p/\mathbb{Q}_p)$ is a closed subgroup of $\mathrm{Gal}(\overline{\mathbb{Q}}/\mathbb{Q})$. How does this fit with the facts above? Does this mean that
  $\mathrm{Gal}(\overline{\mathbb{Q}}_p/\mathbb{Q}_p) \cong \mathrm{Gal}(\overline{\mathbb{Q}}/K)$ for some $K$? Also, what are its non-open subgroups? I assume this includes the Galois groups $\mathrm{Gal}(K/\mathbb{Q})$.","['galois-theory', 'abstract-algebra', 'algebraic-number-theory', 'number-theory', 'group-theory']"
1802515,How to model a checking account with continuous-time compounding?,"Say you have a bank account in which your invested money yields 3% every year, continuously compounded. Also, you have estimated that you spend $1000 every month to pay your bills, that are withdrawn from this account. Create a differential model for that, find its equilibriums and determine its stability. My problem here is that the \$1000 withdrawal is not continuous on time, it's discrete. The best I could achieve is, if $S(t)$ is the current balance: $\dot S (t) = 0,0025S(t) - 1000$ . I'm using $0,0025$ as the interest rate because it yields 3% every year, so it should yield 0,25% every month. But I'm pretty confident that it's wrong. Any help would be highly appreciated! Thanks!","['stability-in-odes', 'finance', 'ordinary-differential-equations']"
1802517,"What do sets in $S := \{ (-\infty, b) : b \in \Bbb{R} \} \cup \{ (a, \infty) : a \in \Bbb{R} \}$ look like","If I'm given a collection
$S := \{ (-\infty, b) : b \in \Bbb{R} \} \cup \{ (a, \infty) : a \in \Bbb{R} \}$
Then would the sets of S only be of the form $(-\infty, b) \cup (a, \infty)$ or could they also be just $(-\infty,b) , (a, \infty)$.",['elementary-set-theory']
1802526,Recommended books on commutative algebra stressing links with algebraic geometry,"Can someone recommend some books on commutative algebra stressing links with algebraic geometry? My concern is this. It seems to me that most of commutative algebra was formulated at least initially by algebraists, and only later were the links with geometry made more explicit. As a result, definitions which are natural to algebraists, might correspond to some complicated definitions in geometry and vice versa. Ultimately, I would prefer a book on commutative algebra which is: 1) always reinterpreting algebraic definitions geometrically (so in some sense, written for geometers) 2) containing a lot of examples, which can be used as counterexamples to various claims, and thus exposing, rather than hiding, the subtleties of the various dictionaries between algebra and geometry 3) preferably not too big (so that it could be read entirely in a reasonable amount of time). I have read most of Atiyah-Macdonald, and own a copy of Eisenbud's ""Commutative Algebra with a view toward Algebraic Geometry"". I love both books, but would like to know whether some other excellent books exist, particularly with a strong geometric bias.","['book-recommendation', 'algebraic-geometry', 'commutative-algebra']"
1802541,Elementary properties of gradient systems,"Consider $x_0\in\mathbb{R}^n$ and a $C^{1,1}$ function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ (that is, a differentiable function whose gradient is Lipschitz function). Consider the system $$
\begin{cases}
\dot{x}(t)=-\nabla f(x(t)),\\
x(0)=x_0.
\end{cases}
$$ Let $\gamma_{x_0}(t)$ be a trajectory (orbit) of the above system which starts at the point $x_0$ is defined for all $t$ in the maximal interval $[0,T_{x_0})$ ( $T_{x_0}\in(0,+\infty]$ ). I am interested in the following elementary properties of the trajectory (orbit) $\gamma_{x_0}(t)$ : Consider the mapping $t\mapsto\rho(t):=f(\gamma_{x_0}(t))$ . Prove that if $\nabla f(x_0)\ne 0$ then $\rho(t)$ is strictly deacreasing. The $\omega-$ limit set of $\gamma_{x_0}$ is defined by $$
\Omega(\gamma_{x_0}):=\{\gamma_\infty:\exists (t_n)\rightarrow\infty, \lim\gamma_{x_0}(t_n)=\gamma_\infty\}.
$$ Prove that if $\gamma_{x_0}$ is bounded (that is, there exists a bounded set $B$ such that $\gamma_{x_0}(t)\in B$ for all $t$ ) then: $\Omega(\gamma_{x_0})$ is either singleton or infinite; $\nabla f(\gamma_\infty)=0$ for all $\gamma_\infty\in\Omega(\gamma_{x_0})$ ; $f$ is constant on $\Omega(\gamma_{x_0})$ ; $\text{dist}(\gamma_{x_0}(t),\Omega(\gamma_{x_0}))\leq\text{dist}(\gamma_{x_0}(t), S)$ , where $$
S:=\{x:\nabla f(x)=0\}.
$$ Moreover, $\text{dist}(\gamma_{x_0}(t), S)\rightarrow 0$ as $t\rightarrow\infty$ . If $\gamma_{x_0}$ has finite length, that is $$
\text{length}(\gamma):=\int_{0}^{T_{x_0}}\|\dot{\gamma}(t)\|
dt<+\infty$$ then $\Omega(\gamma_{x_0})$ is a singleton which is denoted by $\gamma_\infty$ and $$
\lim_{t\rightarrow\infty}\gamma_{x_0}(t)=\gamma_\infty.
$$ My attempt The derivative of $\rho$ is given by $$
\rho^{\prime}(t)=\langle\nabla f(\gamma_{x_0}(t)),\dot{\gamma}(t)\rangle=-\|\nabla f(\gamma_{x_0}(t)\|^2=-\|\dot{\gamma}(t)\|^2\leq 0.
$$ Hence $\rho(t)$ is decreasing. But I cannot know how to prove $\rho(t)$ is strictly decreasing. I have tried to prove these properties but it is uneasy to prove them. I would be grateful if someone could help me to make clear all the properties.
Thank you for all kind help.","['dynamical-systems', 'gradient-flows', 'ordinary-differential-equations', 'differential-geometry']"
1802574,A characterization of Borel measurability,"I need help proving the following  fact.   Let $(X,\textbf{X})$ be a measurable space.  Then $f:X \to \mathbb{R}$ is X -measurable iff $f^{-1}(E) \in \textbf{X}$, $\forall E \in \textbf{B}$. Defns and notations: $X$ is a set, X is  a $\sigma$-algebra of subsets of $X$, $f^{-1}(E):=\{x \in X: f(x) \in E \}$, B is the $\sigma$-algebra of subsets of $\mathbb{R}$ generated by open intervals $(a,b)$. $f$ is X -measurable means $f^{-1}( \alpha, \infty) \in \textbf{X}$, $\forall \alpha \in \mathbb{R}$. What I have: ""$\leftarrow$"" is easy since $( \alpha, \infty) \in \textbf{B}$.   I'm struggling with the formal way to argue ""$\to$"".  Informally, I know the preimage $f^{-1}(E)$ is well-behaved wrt the operations of unions, intersections, complements, etc:
$$f^{-1}(\cup_{i=1}^\infty E_i)=\cup_{i=1}^\infty f^{-1}(E_i), f^{-1}(E^c)=(f^{-1}(E))^c$$  Thus, since everything in B is generated by those operations starting from open intervals $(a,b)$, we should have that, starting from sets of the form $f^{-1}(a,b)$ and applying the same operations, we ""stay"" in X . Sorry I'm new to measure theory and still trying to pick up how proofs of this nature are done.","['real-analysis', 'measure-theory']"
1802588,Can any higher-dimensional Spheres be rotated everywhere equally?,"You can rotate a circle so that every point on it (just the perimeter, not the interior) moves ""equally"".  That is, every point moves with the same speed and even has the same ""acceleration"" (first-order derivative of velocity, which is the first-order derivative of a point's movement wrt time). However, there is no way to rotate a sphere with this same character: the farthest points from the axis of rotation (equator) move much faster and have greater acceleration than the points closest to the axis (poles). AFAIK, there is no closed (finite, no edges) 3-dimensional surface with this property.  You can do it with a cylinder, but that is not a closed shape. You could also do it with a torus by rotating it through the center (rather than around the center) the way a smoke ring does, but that's not a true ""rigid"" rotation (all points maintain the same distance to all other points throughout the rotation).  The points on the inner part of the ring are closer together than those on the outside. What I've been wondering is how this plays out in higher dimensions?  Can the volume-surface of a 4D sphere be rigidly rotated equally like a 2D circle?  Or does it fall to the same problems as 3D spheres?  What about even higher dimensional spheres? For that matter, are there any higher dimensional closed shapes that can be so rotated? For clarity, I am only looking for ""smooth"" shapes, no significant discontinuities, and not just a disconnected set of points.","['rotations', 'geometry']"
1802607,Check series for convergence,$$ \sum_{n = 1}^\infty \sin(n)\sin\left(\frac{(-1)^n}{n^{1/4}}\right) $$ I have no idea how to deal with it.,"['sequences-and-series', 'convergence-divergence']"
1802676,Killing fields on product metrics,"Let $(M_i,g_i)$ be Riemannian manifolds, $i=1,2$. (Save Euclidiean factors) Is it true that a Killing field $Z$ on $(M_1\times M_2,g_1\times g_2)$ will split as a sum of Killing fields $Z=X+Y$, where $X$ is Killing on $M_1$ and $Y$ on $M_2$? The converse is obviously true: if $X$ and $Y$ are Killing, so it is $Z$; and it is obviously false for a product of Euclidean spaces: $(\mathbb R^2,dx^2+dy^2)=(\mathbb R, dt^2)\times(\mathbb R, dt^2)$ and the isometry group of $\mathbb R^2$ is pretty bigger then the product of the groups of $\mathbb R$. The question arises from a question on foliations: does Riemannian foliations with (reducible) totally geodesic leaves (locally) splits as products of orthogonal Riemannian foliations?","['riemannian-geometry', 'differential-geometry']"
1802710,"Using the Weierstrass M-test, show that the series converges uniformly on the given domain","$\sum_{k \geq 0} \frac{z^k}{z^k+1}$ on the domain $\overline{D}[0, r]$, where $0 \leq r < 1$ I'm honestly not sure how to do this.  My text mentions the Weierstrass M-test but the example they gave after stating it uses a completely different method (looks like a repeat of a previous example) and looks nothing like the M-test.",['complex-analysis']
1802743,Checking injectivity of a certain function from a union of a family indexed by $K$ to $K$,"Let $A = \{ A_k | k \in K \}$ be a family of sets indexed by $K$. By Zermelo's theorem, $K$ can be well-ordered. Now, let $\leq$ be a well-order on $K$. Define $j: \bigcup\limits_{k \in K} A_k \to K$, $j(x)=\min_{\le}\{k\in K:x\in A_k\}$ Is this function injective? How can this be checked and proved or disproved?","['elementary-set-theory', 'functions', 'order-theory']"
1802752,Row replacement operation not changing the determinant,Can someone prove why a row replacement operation does not change the determinant of a matrix? **row replacement operation being adding one row to another or something of that sort,"['matrices', 'gaussian-elimination', 'linear-algebra', 'determinant']"
1802755,Can you recover a distribution from mollification?,"Let $f\in \mathcal S'(\mathbb R)$ be a Schwartz distribution. Given $\rho \in C^\infty_c(\mathbb R)$ define the convolution as the function  $$x\mapsto (f\ast\rho)(x):=\langle f, \rho (\cdot -x)\rangle.$$ I believe this function should be smooth. Anyway my question is: suppose further that $\rho\geq 0$ and $\int \rho = 1$ and for $\varepsilon>0$, define 
$$\rho_\varepsilon(x):=\frac{1}{\varepsilon}\rho(x\varepsilon^{-1}).$$
Then is it the case that $f\ast\rho_\varepsilon\to f$ in $\mathcal S'(\mathbb R)$ as $\varepsilon\to 0$?","['functional-analysis', 'real-analysis', 'distribution-theory', 'fourier-analysis']"
1802768,Doubt with vectorial spaces (Basis and dimension),"Good night, i'm working in a problem, i need an basis and the dimension of the space. $a_{1}=(1,0,0,-1),\:a_{2}=(2,1,1,0),\:a_{3}=(1,1,1,1),\:a_{4}=(1,2,3,4),\:a_{5}=(0,1,2,3)$ I make this: $\left[ \begin {array}{cccc} 1&0&0&-1\\ 2&1&1&0
\\ 1&1&1&1\\ 1&2&3&4
\\ 0&1&2&3\end {array} \right]$ and i apply gauss for reduce the matrix: $ \left[ \begin {array}{cccc} 1&0&0&-1\\ 0&1&0&1
\\ 0&0&1&1\\ 0&0&0&0
\\ 0&0&0&0\end {array} \right]$ now, i have 3 linearly independent vectors and my dimensions is 3. but, i can take any vector and they go linearly independent?  for example:
$a_{1}=(1,0,0,-1),\:a_{2}=(2,1,1,0),\:a_{3}=(1,1,1,1)$ or $a_{3}=(1,1,1,1),\:a_{4}=(1,2,3,4),\:a_{5}=(0,1,2,3)$ and going to be linearly independent? thanks!!","['matrices', 'linear-algebra', 'vector-spaces']"
1802795,What are the practical applications of this trigonometric identity?,"On various occasions people have asked here how to prove (but that is NOT the question here) that
$$
\text{if } \alpha+\beta+\gamma = \pi \text{ then } \frac{\sin(2\alpha) + \sin(2\beta) + \sin(2\gamma)} 2 = 2\sin\alpha\sin\beta\sin\gamma.
$$
I first saw this identity in 2006 when it got added to Wikipedia's List of trigonometric identities by a logged-in user called Sriramoman, who alleged that it occurred perenially on the Joint Entrance Examination of the Indian Institutes of Technology. It can be proved by methods that we all learned in secondary school, but I prefer a geometric proof that views the two equated quantities as different ways of expressing the area of a triangle inscribed in a circle of unit radius and in which the three angles are $\alpha,\beta,\gamma$. (That is why I wrote it with factors of $1/2$ on the left and $2$ on the right rather than $1$ on the left and $4$ on the right.) (The fact that there is yet another reason to write it that way will wait for another occasion . . . ) My question is whether this identity has known applications, in physics, engineering, graphics, surveying, cartography, mathematics, or anything else?",['trigonometry']
1802809,$I_m - AB$ invertible if and only if $I_n - BA$ invertible,"Let $A$ and $B$ be $m\times n$ and $n\times m$ matrices respectively. Prove that if $\lambda$ is a non-zero eigenvalue of $AB$ then it is also an eigenvalue of $BA$ Prove that $I_m-AB$ is invertible if and only if $I_n-BA$ is invertible. Part (1) is easy: $$ABx=\lambda x$$ By definition, $x\ne 0$ , and by assumption $\lambda \ne 0$ . So we have $Bx\ne 0$ Now, $$B(AB)x=(BA)Bx=\lambda Bx$$ and so $Bx$ and a non-zero vector with eigenvalue $\lambda$ . My problem is I have no idea how to use this to do (2). Any help is much appreciated. Thanks!","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra', 'inverse']"
1802810,Is every hyperbolic isometry the restriction of an orthochronous Lorentz transformation?,"I know that every isometry of the sphere $\Bbb S^2$ is the restriction of some $A \in {\rm O}(3,\Bbb  R)$: namely, if $A_0:\Bbb S^2\to \Bbb S^2$ is an isometry, then $A_0 = A\big|_{\Bbb S^2}$ where $$Ax = \begin{cases} \|x\| A_0\left(\frac{x}{\|x\|}\right), \text{ if }x \neq 0 \\0 , \text{ if }x=0\end{cases}$$Then I thought of isometries $\Lambda_0:\Bbb H^2 \to \Bbb H^2$, where I see $\Bbb H^2$ inside Lorentz-Minkowski space $\Bbb L^3$. I'd guess that every such $\Lambda_0$ is a restriction of some orthochronous Lorentz transformation $\Lambda: \Bbb L^3 \to \Bbb L^3$, and I add the orthochronous condition so that $\Lambda$ doesn't swap the connected components of the two-sheeted hyperboloid. Trying to define $\Lambda$ from $\Lambda_0$ mimicking the construction for $\Bbb S^2$ doesn't work immediately for lightlike vectors. Every lightlike vector can be approximated by timelike vectors, so I think that this approach is fixable, but I'd like to see that details addressed. Where can I find the proof of that (if the result is actually true)? Thanks.","['hyperbolic-geometry', 'semi-riemannian-geometry', 'differential-geometry']"
1802862,Expanding $\frac{\Gamma(n)}{\Gamma(n-k)}$ as a polynomial,"I want to expand $\frac{\Gamma(n)}{\Gamma(n-k)}$ as a polynomial, where $\Gamma$ is the gamma function. For $k\in\mathbb{N}$, it can be ""simplified"" as $$\frac{\Gamma(n)}{\Gamma(n-k)}=(n-1)(n-2)(n-3)\dots(n-k)$$ I was wondering if it were possible to expand that into $\sum_{i=0}^ka_in^{k-i}$ form. Then, there is the harder case of $k\in\mathbb R$. I imagined that it would be of the form $\Pi_{i=0}^\infty(n-r_i)$ where $r_n$ is the $n$th root. If this were the case, we see that $$\Gamma(n)\ne0\implies\frac1{\Gamma(n-k)}=0$$ This occurs at the poles $$\implies r_i-k=-i$$ $$r_i=i+k$$ And put into product form $$\frac{\Gamma(n)}{\Gamma(n-k)}=\Pi_{i=0}^\infty(n-r_i)=\Pi_{i=0}^\infty(n-k-i)$$ Sadly, I don't think it is possible to multiple the product out because it diverges. Q1 Is there a polynomial/expanded form for $k\in\mathbb N$ using summations if needed? Q2 What about $k\in\mathbb R$? EDIT From below, you can see I have worked the case for $k\in\mathbb N$, but I still need $k\in\mathbb R$. $$\frac{\Gamma(u)}{\Gamma(u-n)}=\sum_{k=0}^n(-1)^{n-k}s(n,k)(u-n)^k$$ My assumption for $n\in\mathbb R$ is that the formula becomes $$\frac{\Gamma(u)}{\Gamma(u-n)}=\sum_{k=0}^\infty(-1)^{n-k}s(n,k)(u-n)^k$$ For similar reasons for why Euler extended binomial expansion the way he did.  I note that my proposed formula holds true if $n\in\mathbb N$, but doesn't make much sense for $n\in\mathbb R$.","['algebra-precalculus', 'factorial', 'stirling-numbers']"
1802887,$\sum_{n=0}^{\infty}\frac{a^2}{(1+a^2)^n}$ converges for all $a\in \mathbb{R}$,"$$\sum_{n=0}^{\infty}\frac{a^2}{(1+a^2)^n}$$ Can I just see this series as a geometric series? Since $c = \frac{1}{1+a^2}<1$, we can see this as the geometric series: $$\sum_{n=0}^{\infty}bc^n = \sum_{n=0}^{\infty}a^2\left(\frac{1}{1+a^2}\right)^n$$ that converges because $c<1$. The sum of this series is: $$S_n = b(c^0+c^1+c^2+\cdots c^n)\rightarrow cS_n = b(c^1+c^2 + c^3 + \cdots + c^{n}+c^{n+1})\rightarrow $$$$cS_n - S_n = b(c^{n+1}-1)\rightarrow S_n(c-1) = b(c^{n+1}-1)\rightarrow S_n = b\frac{c^{n+1}-1}{c-1}$$ $$S = \lim S_n = b\frac{1}{1-c}$$ So $$\sum_{n=0}^{\infty}\frac{a^2}{(1+a^2)^n} = b\frac{1}{1-c} = a^2\frac{1}{1-\frac{1}{1+a^2}} =$$","['real-analysis', 'limits', 'proof-verification', 'calculus', 'sequences-and-series']"
1802897,Pythagoras tree bounding size,"The Pythagoras tree is a fractal generated by squares. For each square, two new smaller squares are constructed and connected by their corners to the original square. The angle of the triangle formed can be varied. The problem is finding the size of the bounding box (to a certain precision) for this fractal. The graph is essentially a rooted tree. Each node represents a square with a certain size and orientation. The tree is infinitely deep. For a certain precision, the problem is easily solved with a computer using branch and bound. I want to know if there are any estimates or hard limits on the size. Any bounds on the area of the fractal are also appreciated. The isosceles right triangle case gives a nice bounding box of $6 \times 4$ , which can be calculated easily with a geometric series. Other cases are more difficult for me (the maximal tree paths follow a zigzag pattern - for a while).","['trees', 'fractals', 'limits']"
1802917,Question regarding curl in dimensions higher than 3,"According to the wikipedia page about curl curl can be defined implicily
as $$(\nabla \times \textbf{F} ) \scriptsize{\bullet} \normalsize{\hat{n}} = \lim_{A \rightarrow 0} \frac{1}{|A|} \oint_C \textbf{F} \scriptsize{\bullet} \normalsize{\textbf{dr}}$$ My question is, specifically, is this definition limited to only two dimensions? If not, how would I apply it to higher dimensions? For example, if I had a vector field defined by $\textbf{F}(x,y)=<x^2-y,y^2+x>$ the line integral at (a,b) could be defined by $$x-a=r \cos(t) \wedge y-b=r \sin(t)$$ $$\frac{dx}{dt}=-r\sin(t) \wedge \frac{dy}{dt}=r\cos(t)$$ $$\oint_C \textbf{F} \scriptsize \bullet \normalsize \textbf{dr} \\= \int_0^{2\pi} \large [ \normalsize (r \ \cos(t)+a)^2-(r \ \sin(t)+b) \large] \normalsize \frac{dx}{dt} dt + \int_0^{2\pi} \large [ \normalsize (r \ \sin(t) + b)^2+(r \cos(t) + a ) \large ] \normalsize \frac{dy}{dt} dt\\= 2 \pi r^2$$
and $A = \pi r^2$ so $\frac{1}{A}=\frac{1}{\pi r^2}$ which makes $$\lim_{A \rightarrow 0} \frac{1}{A} \oint_C \textbf{F} \scriptsize{\bullet} \normalsize{\textbf{dr}} = \lim_{r \rightarrow 0} \frac{2 \pi r^2}{\pi r^2} =\lim_{r \rightarrow 0} 2 = 2 $$ Since $\hat{n}$ for the $xy$-plane is simply $<0,0,1>$ and since the curl would also be pointing in the same direction, then the curl, expressed as a vector in 3-space must be $<0,0,2>$. On WolframAlpha, entering curl [x^2-y,y^2+x,0] gives us the exact same answer. That was a little involved, to be honest, but I feel it was necessary for this question. A follow up question I have to ask is, if I were to apply this specifically to three dimensions, would a simple line integral in 3-space provide a sufficient answer, or would it need a surface integral?","['multivariable-calculus', 'geometry']"
