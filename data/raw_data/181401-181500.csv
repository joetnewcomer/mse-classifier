question_id,title,body,tags
3307162,"Ten lockers are in a row. Each locker is to be painted either blue, red or green. In how many ways can the collection of lockers be painted?","Ten lockers are in a row. The lockers are numbered in order with the positive integers 1 to 10. Each locker is to be painted either blue, red or green subject to the following rules: Two lockers numbered $m$ and $n$ are painted different colours whenever $m−n$ is odd. It is not required that all 3 colours be used. In how many ways can the collection of lockers be painted? Attempt: Notice that the total number of coloring schemes without the rules is $3^{10}$ . The total number of coloring schemes where every two side-by-side lockers have different color is $3 \cdot 2^{9}$ (this corresponds to $m-n=1$ ). The total number of coloring schemes where $m-n = 3$ is $(3 \cdot 2)^{7}$ , without considering the others $m-n$ . Also for $m-n = 5$ , the total number of schemes is $(3 \cdot 2)^{5}$ . For $m-n=7$ , $(3 \cdot 2)^{3}$ . For $m-n=9$ , $(3 \cdot 2)$ . How to continue?","['permutations', 'combinatorics', 'contest-math']"
3307169,Number of nodes in cubic graphs with no non-trivial automorphisms,"Is there some sort of characterization of all such numbers $n$ , that there exists a cubic graph with $2n$ vertices and no non-trivial automorphisms. Frucht theorem states, that any finite group is an automorphism group of some cubic groups. In particular case, the minimal cubic graph with trivial automorphism group is called Frucht graph and has $12$ vertices. However, I do not know, of what size can the larger examples be...","['automorphism-group', 'graph-theory', 'combinatorics', 'discrete-mathematics', 'group-theory']"
3307175,"Is there anyway to see that $h(x,y) = \frac{20}{3+x^2+2y^2}$ represents a graph looking like a mountain?","The function $h(x,y) = \frac{20}{3+x^2+2y^2}$ represents the following graph: Is there anyway to see from the equation, without plotting it, that its graph will look as shown above? Moreover, is there a simple way of coming up with another equation that represents two ""mountains"", of given heights above the $xy$ -plane, next to each other (or even more complicated ""mountain ranges"" with given properties)?","['multivariable-calculus', 'graphing-functions', 'differential-geometry']"
3307189,"Intuitively, $why$ do defective matrices have extra eigenvalues?","I've recently seen the example on how a shear $$\begin{bmatrix} 1 & 1\\ 0 & 1\end{bmatrix}$$ is an example of a defective matrix, since it has eigenvalues $1,1$ but only one independent eigenvector $\mathbf{v}_1 = (1,0)$ . So in this case, I can see that the algebraic multiplicity is greater than the geometric multiplicity, But what I am wondering is why the system created ""two"" eigenvalues when only one of them was ""actually an eigenvalue"" - (It's not even that there were two eigenvectors where each one corresponded $1$ , $1$ ). Is there any underlying reason why the system came up with two eigenvalues? (If I was to intuitively guess what would happen purely from a geometric perspective, I would have guessed that the characteristic polynomial would just be linear: $\lambda - 1$ . Although this isn't possible for a $2 \times 2$ , I was wondering whether there was any other meaning for the second eigenvalue?)","['eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'intuition', 'linear-transformations']"
3307204,Spectral theorem/ exchanging limit of series and operator,"I am currently learning quantum mechanics and there is one typical scenario i encounter in my physics books: Suppose $\mathcal{H}$ is a Hilbert space and $A: \operatorname{Dom}(A)\to \mathcal{H}$ is a (linear) operator, $Dom(A)\subseteq\mathcal{H}$ . In addition, let $\mathcal{B}:=\{v_n: n \in\mathbb{N}\}$ be a set of eigenvectors of $A$ : $A(v_n)=\lambda_nv_n$ . If there is a $v \in \operatorname{Dom}(A)$ and a sequence of coefficients $(c_n)_{n \in\mathbb{N}}$ with $v=\sum\limits_{n=1}^{\infty}c_nv_n$ , then the authors write $A(v)=A(\sum\limits_{n=1}^{\infty}c_nv_n)=\sum\limits_{n=1}^{\infty}c_nA(v_n)=\sum\limits_{n=1}^{\infty}(c_n\lambda_n)v_n$ . From what i know, A is not continuous in general, but this is a special case of the spectral theorem. I think that a physicist would write $A=\sum\limits_{n=1}^{\infty}|v_n\rangle\langle v_n|$ and call this the spectral theorem for the discrete case .
The problem is that when looking into a math book about the spectral theorem, i see a lot of integrals and things get very complicated. So it would be nice if someone could explain this special case of the spectral theorem (its requirements and statements), or suggest a good source.","['operator-theory', 'spectral-theory', 'linear-algebra']"
3307228,How sub-Gaussian is a Truncated Normal?,"We say the expectation random variable $X$ is $C$ -subgussian to mean $$\mathbb E[e^{\lambda X - \mathbb E [X]}] \le e^{C\lambda^2}$$ for all $\lambda \in \mathbb R$ . One slightly different definition is that $$P(X-\mathbb E [X] > \lambda) \le e^{-D \lambda^2} \text{ and } P(X-\mathbb E [X] < \lambda) \le e^{-D \lambda^2}$$ for some $D$ . The constants $C$ and $D$ can be bounded in terms of each other. It is well-known a $N(0,\sigma^2)$ variable $X$ satisfies the above with $C= \sigma^2/2$ and $D = 1/2 \sigma^2$ . Suppose we truncate the variable to some finite interval $[a,b]$ that contains $E [X] $ . For $Y$ the truncated variable it is straightforward to show say $$P(|Y-E [X] | < \lambda) \ge 1-2e^{-D \lambda^2}$$ for $D = 1/2 \sigma^2$ whenever $a \le -\lambda \le \lambda \le b $ . This is because the PDF of $Y$ is just the PDF of $X$ set to zero outside $[a,b]$ and then scaled upwards to make it a probability distribution. However this does not give a subgaussian constant for $Y$ since $Y$ may not have expectation $\mathbb E [X] $ . Are there any ways to get a good subgaussian constant for $Y$ in terms of $a,b$ ? Note Hoeffding's lemma gives the constant $(b-a)^2/8$ but I would like something in terms of $\sigma^2$ that ideally tends to the original constant as $a,-b \to \infty$ .","['normal-distribution', 'probability']"
3307242,Prove that there exists a row or a column of the chessboard which contains at least √n distinct numbers.,"On each cell of an $n \times n$ chessboard, we write a number from $1, 2, 3, .
 . . , n$ in such a way that each number appears exactly $n$ times. Prove
  that there exists a row or a column of the chessboard which contains
  at least $\sqrt{n}$ distinct numbers. This question is dying for a proof by contradiction, but I can't seem to show how. When I searched through the Internet, there is the following hint: Assume each row and each column has less than $\sqrt{n}$ distinct numbers in
  it. For each row or column, consider the number of distinct numbers in
  it. I feel like double counting is involved in the process, but what to count in two different ways? Also I can prove the statement with the probabilistic method but I would really like to learn the double counting approach. Here is the other proof: Choose a random row or column ( $2n$ choices). Let $\textbf X$ be the number of distinct entries in it. Use the indicator variable $I_i$ , for the existence of number $i$ in the block: $\textbf X = \sum I_i.$ Clearly, $E[I_i] = P[I_i = 1]$ .  Then $P[I_i = 1] \geq 2\sqrt n / (2n) = 1/\sqrt n$ . The lower bound is obtained if all the $i$ happens to be in some $\sqrt n$ edged square sub-matrix. Hence, by linearity $\textbf E[X] \geq \sqrt n$ . This proves the existence.","['probabilistic-method', 'combinatorics', 'discrete-mathematics']"
3307280,Fundamental Theorem of Calculus in complex analysis?,"This following fact came up in a course of complex analysis I was studying, and I was wondering how to prove it. Suppose that $f:D \rightarrow \mathbb{C}$ is continuous, and that $\oint f(z) dz=0$ . $D$ is a domain, not necessarily simply connected. Let $\Gamma$ be a curve connecting $z_0,z \in \mathbb{C}$ , define $F(z)=\int_{\Gamma} f(w) dw$ . Then, $F$ is an analytic function. Comments: It is easy to show that $F$ is well defined, and I was able to do that. I know that there is a real analysis version that is the fundamental theorem of calculus, but to prove analycity I need to show that the C-R eq's hold which is what I am struggling with.","['complex-analysis', 'calculus', 'cauchy-integral-formula']"
3307290,When is this estimate about L2-norms true?,"Let $f$ be a function in $L^2(U)$ where $U$ is some (not necessarily bounded) domain. For example, if $f$ is bounded, then for all $g \in L^2(U)$ , $$\|fg\|_{L^2(U)} \leq \|f\|_{\infty}\|g\|_{L^2(U)} = C\|g\|_{L^2(U)}. $$ Are there any other cases when this is true (i.e $f$ not necessarily bounded) or is this a necessary assumption?","['lp-spaces', 'functional-analysis']"
3307339,What does the plot/graph of an associative function on the unit square look like?,"I have a function $f(x,y):[0,1]\times[0,1]\rightarrow[0,1]$ . Let's say that I plot the function as a heat map (that is, I color each pixel in the square $[0,1]\times[0,1]$ according to the value of the function at that point) I'm interested in knowing what the plot would look like given that the function is associative: $f(x,f(y,z))=f(f(x,y),z)$ for all $x,y,z$ . Commutative functions $f(x,y)=f(y,x)$ have a plot which is mirror-symmetric along the diagonal of the square. Is there, similarly, some simple visual property that characterizes all associative functions (and only them)? (To make it simpler, we may assume that we are talking about a ""regular-enough"" function, e.g. piecewise continuous or something like that, so that it would make sense to speak about its plot)","['functions', 'associativity', 'graphing-functions']"
3307342,Excellent Rings Geometric Intuition,"A ring $R$ is called excellent if is quasi-excellent (so is a G-ring and a J-2 ring) and universally catenary. Following two questions: 1) Wiki says that excellent ring ""behave well with respect to the completion"" $R \to \hat{R}$ . What does this mean concretely? Is there a wide class of properties properties P such that one have "" $R$ has property P $\Rightarrow \hat{R}$ has also property P""? If yes, which properties P are meant? Which arguments can be done working with excellent rings? Or has ""behave well with respect to the completion"" another meaning? 2) Is there any geometric intuition (in sense of corresponding top space $Spec(R)$ behind excellent rings?","['ring-theory', 'algebraic-geometry', 'commutative-algebra']"
3307345,Geometric Proof for the Derivative of Sine,"I was following a geometric proof for the derivative of sine when I came across this unjustified assertion: In the diagram, as we choose Q ever closer to P, the chord PQ approximates the corresponding arc arbitrarily well. While I understand the intuitive reason for this, how could this idea be made more rigorous so that the proof is guaranteed to work. Something with epsilon delta was what I was thinking?","['calculus', 'geometry']"
3307418,Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$. Is the set of points at which $f$ is differentiable a Borel set?,"Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a function. Is the set of points at which $f$ is differentiable a Borel set? The answer is ""yes"" for $n=1$ , even for arbitrary $f$ (not assumed to be measurable). But what happens for $n>1$ ? 
The proof for $n=1$ (refer Characterization of sets of differentiability ) does not seem to generalize easily to higher dimensions. (The motivation for asking this question came from the Rademacher's theorem, which states that any Lipschitz map $f:\mathbb{R}^n \rightarrow \mathbb{R}$ is differentiable almost everywhere. I was wondering if the points where $f$ is not differentiable forms a Borel set. Therefore, if the solution to the original question seems obscure, please feel free to help me with partial results, especially for the Lipschitz case.)","['borel-sets', 'measure-theory']"
3307424,Continuous function between a lower semi-continuous function and an upper semi-continuous function.,"Let $X$ be a compact metric space, $u: X \to [0, 1]$ an upper semi-continuous function and $l: X \to [0, 1]$ a lower semi-continuous function such that $u(x) < l(x)$ for each $x \in X$ . Does there exist a continuous function $f: X \to [0, 1]$ such that $u(x) < f(x) < l(x)$ for each $x \in X$ ?","['continuity', 'semicontinuous-functions', 'metric-spaces', 'real-analysis']"
3307428,"Simplifying the determinant of the matrix whose $(i,j)$-th entry is $b_i c_j$ for $i=j$ and $-b_i c_j$ for $i\neq j$","$A$ is a $n \times n$ real matrix. $A_{ij} = \begin{cases} \phantom{-}b_{i}c_{j} & \text{if } i = j \\ 
-b_{i}c_{j} & \text{if } i \ne j  \end{cases}$ How to simplify $\det(A)$ ? Update: Can I simplify the determinant with elementary row and column operations as described at http://www.maths.nuigalway.ie/~rquinlan/MA203/section2-5.pdf ? Divide each row by $b_{i}$ (elementary row operation) Divide each column by $c_{j}$ (elementary column operation) $\det(A) = \left(\prod_{i=1}^{n} b_{i} \right) \left(\prod_{j=1}^{n} c_{j} \right) \det(S)$ where $S_{ij} = \begin{cases} + 1 & \text{if } i = j \\ 
-1 & \text{if } i \ne j  \end{cases}$ So, the problem reduces to finding $\det(S)$ .","['determinant', 'linear-algebra']"
3307507,Mechanics of Horizontal Stretching and Shrinking,"I know $y=2f(x)$ doubles the $y$ value in vertical shifting, and I know that $y=(1/2)f(x)$ halves the $y$ value. But, why does $y=f(2x)$ halve the $x$ value, and why does $y=f((1/2)x)$ double the $x$ value in horizontal shifting? I looked at the graphs, and I see the difference, but I want to know the math behind why it halves instead of doubling the $x$ value for $y=f(2x)$ . Maybe this is a stupid question, but could someone help me out? Thank you.",['algebra-precalculus']
3307520,"Unit square, Riemann zeta function: $\int_0^1\int_0^1\frac{(-\log(xy))^s}{1-xy}\,dx\,dy = \Gamma(s+2) \zeta(s+2), \operatorname{Re}(s)>1$","I want to prove following integral: For $\operatorname{Re}(s)>1,$ $$
\int_0^1 \int_0^1 \frac{(-\log(xy))^s}{1-xy}\,dx\,dy = \Gamma(s+2)\,\zeta(s+2). 
$$ I am having trouble with proving this equation. What can be a good parametrization for this integral?  My trial was treating $t = -\log(xy),$ but this does not work well...","['integration', 'multivariable-calculus', 'calculus', 'riemann-zeta', 'zeta-functions']"
3307548,Bounds on integer solutions to $w^2 - x^2 - y^2 - z^2 = m$,"Consider the space $I(m) \subset \mathbb{Z}^4$ which corresponds to integer $(w, x, y , z)$ that are solutions of $$w^2 - x^2 - y^2 - z^2 = m$$ where $w, x, y, z, m \in \mathbb{Z}^{+}$ Are there any known bounds for space $I(m)$ ? For example, if we ignore $y$ and $z$ , then the equation $w^2 - x^2 = m$ will have solutions within $\vert w \vert \le m$ and $\vert x \vert \le m$ (because $(w + x) \mid m$ and $(w - x)\mid m$ , from now the result easily follows) Unfortunately the technique won't generalize for more variables. Using a computer it seems that an upper bound on the absolute value $\vert \cdot \vert \le 2m$ looks believable for the variables $w, x, y$ and $z$ . Can we prove this? (It seems that from computer experiments, for $m > 1$ , even the bound $\vert \cdot \vert \le m$ seems to work; for $m = 1$ there is a non-trivial solution $(2, 1, 1, 1)$ ) Motivation : I'm trying to write a computer program to generate such numbers. Having such a bound will mean that I can compute the entire set using simple brute-force algorithm. In my problem there are three more additional constraints: $x + y \le w$ , $y + z \le w$ and $x + z \le w$ . I've omitted them here to form a simple problem statement.","['number-theory', 'integer-lattices', 'diophantine-equations']"
3307571,How to evaluate $ \lim_{x \to 0} [\frac{\sin x \tan x}{x^2}] $where [] is GIF,"$$ \lim_{x \to 0} \biggl[\frac{\sin x \tan x}{x^2}\biggl] $$ where [] is GIF. The challenge I'm facing is the fact that while $\frac{\sin x}{x}< 1, \frac{\tan x}{x} >  1$ . Now their product is becoming a bit undetermined as to will it be $>1$ or $< 1 $ . How do i solve it then ?? My first thought was to use the series expansion, which yields $\biggr[1+ \frac{x^2}{3}...\biggl]$ how can I be sure that the further negative terms that are coming in this infinite series will not overpower the $\frac{x^2}{3}$ term and make the whole value < 1","['limits', 'calculus']"
3307589,Finding $\sum_{k=1}^\infty\frac{(-1)^{k-1}}{k^n 2^k {2k \choose k}}$,"Inspired by these two questions asking about the case n = 3 and n=4 , I was wondering what is $$S =\sum_{k=1}^\infty\frac{(-1)^{k-1}}{k^n 2^k {2k \choose k}}$$ for positive integer $n \ge 3$ . For $n = 3$ , the sum is $\frac{1}{4}\zeta (3)-\frac{1}{6}\ln^3(2) = \frac{1}{2}\operatorname{Li}_3\left(\frac{1}{2}\right)+\frac{1}{2}\ln(2)\operatorname{Li}_2\left(\frac{1}{2}\right)-\frac{3}{16}\zeta(3)$ . For $n = 4$ , the sum is $4\operatorname{Li}_4\left(\frac12\right)-\frac72\zeta(4)+\frac{13}4\ln2\zeta(3)-\ln^22\zeta(2)+\frac5{24}\ln^42$ . Using similar work as from the two linked questions, the sum can be re-expressed as the integral $$\frac{2\cdot(-1)^{n-1}}{(n-3)!}\int_0^1\text{arcsinh}^2\left(\sqrt{\frac{x}{8}}\right)\frac{\ln^{n-3}(x)}{x}dx$$ Setting $u = \text{arcsinh}\left(\sqrt{\frac{x}{8}}\right)$ , we get $$S = \frac{4\cdot(-1)^{n-1}}{(n-3)!}\underbrace{\int_0^{\frac{\ln(2)}{2}} u^2\ln^{n-3}(8\sinh^2(u))\coth(u) du}_{\large {I}}$$ $$I = \int_0^{\frac{\ln(2)}{2}}u^2\coth(u)\sum_{k=0}^{n-3}\left({n-3\choose k}\ln^{n-3-k}(8)(2\ln(\sinh(u)))^{k}\right) du$$ $$I = \sum_{k=0}^{n-3}{n-3\choose k}\ln^{n-3-k}(8)2^{k}\underbrace{\int_0^{\frac{\ln(2)}{2}}u^2\coth(u)\ln^{k}(\sinh(u))du}_{\large J}$$ Making the substitution $v = \sinh(u)$ and simplifying, we get $$J = \int_0^{\frac{1}{2\sqrt{2}}}\frac{\text{arcsinh}^2(v)\ln^k(v)}{v}dv$$ Although this may or may not help, making the substitution $w = \ln(v)$ , we get $$J = \int_{-\infty}^{-\ln(2\sqrt{2})}w^k\text{arcsinh}^2(e^w)dw$$ From here, I don't know what to do to find $J$ . How can I, either through this process or a completely different one, find: $1.$ The value of $S$ for integer $n \ge 3$ ? $2.$ The value of $J$ for integer $k \ge 0$ ?","['integration', 'definite-integrals', 'binomial-coefficients', 'closed-form', 'sequences-and-series']"
3307598,Sums of binomial coefficients in which lower index remains fixed,"How can I prove that $ \sum_{k=1}^{m} {{k+n} \choose {n+1}} = {{ m + n + 1 } \choose {n+2}} $ ? I have managed to prove that $ \sum_{k=0}^{m} {{k+n} \choose {n}} = {{ m + n + 1 } \choose {n+1}} $ Although it seams intuitive that the second implies the first, I haven't been able to deal with the extra +1. Thanks in advance","['summation', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
3307599,Prove stochastic differential relation $(dN_t)^2 = dN_t$,"I have some difficulties in showing the following problem. Any hints will be appreciated! Show $$(dN_t)^2 = dN_t$$ for $N_t$ is the Poisson process I have thought of two approaches, both were met with difficulties Approach 1: Use previously proven relations: I have $(dM_t)^2 = dN_t$ , where $M_t$ is the compensated Poisson process - $M_t = N_t - \lambda t$ and $d_t\,dM_t = 0$ . Also $dM_t = dN_t -\lambda dt$ so \begin{align}
(dN_t)^2 &= (dM_t + \lambda dt)^2 \\
&= (dM_t)^2 +2\lambda dt\,dM_t + {\lambda}^2dt^2 \\
&= dN_t + {\lambda}^2dt^2
\end{align} So the remaining piece for this approach is for the $\lambda^2dt^2$ term to disappear. But will it necessarily disappear? Approach 2: By definition. Let $a < b$ and consider the partition $a = t_0 < t_1 < \cdots < t_{n-1} < t_n = b$ then I want to show $$\textrm{ms}-\lim_{||\Delta_n|| \to 0} \sum_{k=0}^{n-1}[\Delta N_k^2 - \Delta N_k] = 0$$ where $||\Delta_n|| = \sup_{0 \leq k \leq n-1}(t_{k+1} - t_k)$ I attempt to prove the above by showing $E[\Delta N_k^2 - \Delta N_k] \to 0$ and $\textrm{var}[\Delta N_k^2 - \Delta N_k] \to 0$ but was unsuccessful - so I am suspecting that approach 2 is off. Thanks!","['stochastic-calculus', 'stochastic-processes', 'poisson-process', 'probability-theory', 'probability']"
3307694,Equivalence of $\frac{S_n}{n}\xrightarrow{a.s.}0$,"I'm trying to prove the next equivalence: Let $\{X_n\}_{n\in\mathbb{N}}$ be a sequence of independent r.v. and $S_n=\sum_{i=1}^{n}X_i.$ Then $\frac{S_n}{n}\xrightarrow{a.s.}0$ if and only if the two following conditions holds: a) $\frac{S_n}{n}\xrightarrow{P}0,$ b) $\frac{S_{2^n}}{2^n}\xrightarrow{a.s.}0.$ If $\frac{S_n}{n}\xrightarrow{a.s.}0$ then we have immediatly a) because convergence a.s. implies in probability and every subsequence converges to $0,$ so b) is satisfied. For the other direction I am having problems. I was trying to use that every subsequence of $\frac{S_n}{n}$ has some subsequence which converges a.s; then mixed each term of such subsequence with b), but this cannot ensure the desired result. Any kind of help is thanked in advanced.",['probability-theory']
3307695,How do I proof a characterisation of uniform integrability?,"The remark of interest is given in my measure-theory script but without a proof and I don't know how to proof it. I am using the following definition. Let in the following $(\Omega,\mathfrak{A},\mu)$ be a measure space. $\newcommand\diff{\mathop{}\!\mathrm{d}}$ $\newcommand\integrable{\bar{\mathfrak{L}}^1(\mu)}$ $\newcommand\integrableP{\bar{\mathfrak{L}}^1_+(\mu)}$ Def .: (1) A familiy of generalised integrable functions (generalised means that the functions can have values in $\bar{\mathbb{R}}=\mathbb{R}\cup\{\pm\infty\}$ ) $\mathfrak{F}\subseteq\integrable$ is called $\mu$ -uniformly integrable iff for all $\epsilon>0$ there exists a generalised nonnegative integrable function $h\in\integrableP$ such that $$\sup_{f\in\mathfrak{F}}{\int_\mu{|f|\chi_{\{|f|\geq h\}}\diff\omega}}<\epsilon.$$ Remark 1 The above definition (1) is equivalent to definition (2): For all $\epsilon>0$ we have $h'\in\integrableP$ so that $$\sup_{f\in\mathfrak{F}}{\int_\mu{(|f|-h')^+\diff\omega}}<\epsilon.$$ Remark 1 is fine for now. I want to show that any familiy of generalised integrable functions satisfying definition (1) also satisfies definition (3). Of course we can use remark 1. Remark 2 If $\mu$ is finite then the above definition (1) is equivalent to definition (3) $$\lim_{n\to\infty}\sup_{f\in\mathfrak{F}}{\int_\mu{(|f|-n)^+\diff\omega}}=0.$$ The goal is to show that remark 2 holds. In order to do so I need to show that if some family satisfies definition (1) then it also satisfies definition (3). My idea is to plug in the definition of limit and see how we need to guess. So if $\epsilon>0$ then I need to find a $N\in\mathbb{N}$ such that for all $n\geq N$ I have that $$\sup_{f\in\mathfrak{F}}{\int_\mu{(|f|-n)^+\diff\omega}}<\epsilon.$$ Definition (2) yields an $h'\in\integrableP$ such that $$\sup_{f\in\mathfrak{F}}{\int_\mu{(|f|-h')^+\diff\omega}}<\frac{\epsilon}{2}$$ holds. By triangle inequality I have $(|f|-n)^+\leq(|f|-h')^++(h'-n)^+$ . The question thus boils down to asking if $\int_\mu{(h'-n)^+\diff\omega}<\frac{\epsilon}{2}$ holds for large $n\in\mathbb{N}$ . This is where I am stuck. I tried different approaches but nothing worked. This is what may help: (a) $$h'\in\integrable\implies\mu(\{h=\infty\})=0$$ (b) $$\mu(\{h=\infty\})=0\implies\lim_{n\to\infty}\mu(\{h'\geq n\})=0$$ (c) $$\mu(\Omega)<\infty$$","['uniform-integrability', 'measure-theory']"
3307741,"The Sub Gradient and the Proximal Operator of the of $ {L}_{2, 1} $ Norm (Mixed Norm)","What would be the the Sub Gradient of $$ f \left( X \right) = {\left\| A X \right\|}_{2, 1} $$ Where $ X \in \mathbb{R}^{m \times n} $ , $ {A} \in \mathbb{R}^{k \times m} $ and $ {\left\| Y \right\|}_{2, 1} = \sum_{j} \sqrt{ \sum_{i} {Y}_{i,j}^{2} } $ . What would be the Prox of: $$ \operatorname{Prox}_{\lambda {\left\| \cdot \right\|}_{2,1}} \left( Y \right) = \arg \min_{X} \frac{1}{2} {\left\| X - Y \right\|}_{F}^{2}  + \lambda {\left\| X \right\|}_{2, 1}, \; X, Y \in {\mathbb{R}}^{m \times n} $$ Can either be generalized for $ {\left\| \cdot \right\|}_{q, p} $ ?","['convex-optimization', 'proximal-operators', 'multivariable-calculus', 'linear-algebra', 'convex-analysis']"
3307813,"How to pronounce ""$\mapsto$"", as in ""$x\mapsto f(x)$""? [duplicate]","This question already has answers here : What does the function f: x ↦ y mean? (4 answers) Closed 4 years ago . Lots of symbols have their pronunciations. For instance, "" $\tbinom {n}{k}$ "" is pronounced "" $n$ choose $k$ "", and "" $\forall$ "" is pronounced ""for all"". The symbol "" $\mapsto$ "" is used to represent a mapping , as in $$x \mapsto f(x)$$ How to pronounce this symbol?","['pronunciation', 'functions', 'terminology']"
3307822,Final Questions - Part 1 (Poisson Process),"After owing a lot to you guys on this website, I finally took my exam on Stochastic Process today! There were some questions in this exam that I was not sure about. I want to share it with you guys and see what you guys think. This is from my recollection of the exam that I took few hours ago so might not be 100% rigorous. Please understand and let me know if I am missing some details necessary. Of course, for legal issue, this is not completely the same as the exam itself. Black birds arrive at the John's office at the according to Poisson Process with parameter $\alpha$ . Independently of the black birds, yellow birds arrive at the office according to Poisson Process with parameter $\beta$ . Starting from 9:00AM until 10:AM, given that there were 3 black birds and 5 yellow birds arrived at the office, what is the probability that all 3 black birds arrived before 5 yellow birds? My answer : Unfortunately, I could not solve this question. But I tried my best to guess and gave two solutions: [Solution 1] Let $X_i$ ~ Unif( $0,1$ ), $i = 1,2,3$ be the arrival time of the black birds and $Y_k$ ~ Unif( $0,1$ ), $k = 1,2,3,4,5$ be the arrival time of the yellow birds. Therefore, we calculate: $\int^{1}_{0} P(max{X_i}<t) \cdot P(min{Y_k}>t) \ dt$ = $\int^{1}_{0} t^3(1-t)^5 \ dt$ Then I could not solve this integral by substitution or integration by parts. [Solution 2] $P$ ( $3$ black birds arrived before all $5$ yellow birds) = $P$ (first arrival was black bird) + $P$ (second arrival was black bird) + $P$ (third arrival was black bird)  = $\ {3 \choose 1}\frac{\alpha}{3\alpha + 5 \beta} + {2 \choose 1}\frac{\alpha}{2\alpha + 5 \beta} + {1 \choose 1}\frac{\alpha}{\alpha + 5 \beta} $ Is any of the solutions correct? Haha :')","['statistics', 'markov-chains', 'stochastic-processes', 'poisson-process', 'probability']"
3307827,Preimage of Union is Union of Preimage,"Struggling on what steps to take to construct a successful proof for this. This will be the last. Can I get a verification? I think the approach and logic is wrong. Prove: If $f:A \rightarrow B$ is a function with domain $A$ and $T_i$ with $i \in I$ is a family of sets where $\forall i \in I$ , $T_i \subseteq B$ then $f^{-1}( 
\bigcup\limits_{i \in I} T_{i})= \bigcup\limits_{i \in I}f^{-1} (T_{i})$ Proving Subset both ways Prove $f^{-1}( 
\bigcup\limits_{i \in I} T_{i}) \subseteq \bigcup\limits_{i \in I}f^{-1} (T_{i})$ Assume $s \in f^{-1}( 
\bigcup\limits_{i \in I} T_{i})$ $\implies$ $f(s) \in \bigcup\limits_{i \in I} T_{i}$ $(\exists i \in I)$ s.t. $f(s) \in  T_{i}$ $(\exists i \in I)$ s.t. $s \in f^{-1} (T_{i})$ So $f^{-1}( 
\bigcup\limits_{i \in I} T_{i}) \subseteq \bigcup\limits_{i \in I}f^{-1} (T_{i})$ Next Prove $f^{-1}( 
\bigcup\limits_{i \in I} T_{i}) \supseteq \bigcup\limits_{i \in I}f^{-1} (T_{i})$ Assume $s \in \bigcup\limits_{i \in I}f^{-1} (T_{i})$ $\implies (\exists i \in I)$ s.t. $s \in f^{-1}(T_i)$ $\implies (\exists i \in I)$ s.t. $f(s) \in T_i$ $\implies f(s)\in  \bigcup\limits_{i \in I}T_{i}$ $\implies s \in f^{-1}(\bigcup\limits_{i \in I}T_{i})$","['elementary-set-theory', 'solution-verification']"
3307844,"Final Questions - Part 3 (Birth and Death Process) - (hardest and nastiest B&D question I have encountered, ever)","There is a selective group that people can join. Arrival of people who want to join the group is Poisson Process with parameter $\lambda$ . The person who arrive at the group can join the group only if he/she gets approval from all existing members. Each member will say 'yes' with probability 1/2 and 'no' with probability 1/2. Also, each member of the group leaves the group independently at a Exponential distribution of time $\mu$ . The number of members of this group defines a birth and death process. What is the limiting probability? My attempt: I came up with a very nasty answer. $\alpha_i = \frac{1}{i!}(\frac{1}{2})^{\sum_0^{i-1}i}(\frac{\lambda}{\mu})^i\alpha_0 \ $ for $i = 1,2,3,...$ $\alpha_0 = (1 + \sum_{1}^{\infty}\frac{1}{i!}(\frac{1}{2})^{\sum_0^{i-1}i}(\frac{\lambda}{\mu})^i\alpha_0 )^{-1}$ Does anyone has any idea if this is correct?","['birth-death-process', 'statistics', 'markov-chains', 'stochastic-processes', 'probability']"
3307863,Are linear functions always continuous?,"I'm trying to clarify a definition for middle school mathematics students (grade 8) for ""linear function."" I have always viewed the NCTM (National Council of Teachers of Mathematics) definition of a linear functions as accurate: 
""a relationship whose graph is a straight line."" A physicist and mathematics teacher is stating this definition is incorrect and says that linear functions can be discrete, as in an arithmetic sequence. Because my area of expertise is not higher-level maths, but I want to be accurate, I am asking for confirmation that a linear function is always continuous.","['continuity', 'functions', 'sequences-and-series']"
3307864,Approximating a Discrete Function with a Polynomial,"Consider a discrete function $f : \{x_1,\dots,x_n\} \to [a,b]$ that, for some $\omega>0$ , satisfies $$\max_i |f_i-f_{i-1}| \le \omega. \tag1$$ Let $q$ be the polynomial of order $m<n$ that, for some $p$ , approximates the function $f$ by minimizing $$D_p\equiv||f-q||_p = \left(\sum_{i=1}^n|f(i) - q(i)|^p\right)^{\frac{1}{p}}.$$ Is there a sharp or a ""relatively tight"" upper bound to $D_p$ that does not depend on the values of $f$ ? I thought that this would be easy to find online but all approximation results I could find (like Jackson's inequality ) are for continuous functions. One would think that tighter bounds could be established for discrete functions, but I haven't been able to find one. Any suggestion in this direction would be very welcome. Edit based on the comments by @Ian Let $X$ be a matrix with elements $X_{ij}=x_i^j$ , for $i=\{1,\dots,n\}$ and $j=\{0,\dots,m\}$ , and let $y=[f_1,\dots,f_n]^T$ . Then, for $p=2$ it follows from the least squares formula that $$D_2=||(I_n-X(X^T X)^{-1}X^T)y||_2.$$ It would also follow that, for all $p$ $$D_p\leq ||(I_n-X(X^T X)^{-1}X^T)y||_p.$$ So, one way to get a relatively tight bound (I haven't figure out how to formalize this) would be to use condition $(1)$ to bound $D_2$ .","['approximation-theory', 'polynomials', 'discrete-mathematics']"
3307894,"Proof verification: If $g\circ f$ is surjective, show that $g$ is surjective.","Let $X,Y,Z$ be sets and $f: X\to Y$ , $g: Y\to Z$ functions. If $g\circ f$ is surjective, prove that $g$ is surjective. Here's my sketch : Since $g\circ f  : X\to Z$ is surjective, there are for every $z\in Z$ at least one $x\in X$ with $f(z)=x$ . This means that $\#X \geq \# Z$ . Furthermore, we know that every $x\in X$ will mapped to $f(x)=y$ . $g$ can only map $g(y)=z$ with $y$ being the elements of the image of $f$ . Ultimately, the amount of $y=f(x)$ -values is dependent on the amount of values in $Z$ . There can never be more Elements $f(x)=y\in Y$ then there are elements in $Z$ . Hence $g$ is surjective. I hope you get what I just wrote since it is really hard to explain in English.","['elementary-set-theory', 'functions', 'proof-verification']"
3307911,Faithful Action using Sylow Subgroups,"For a finite group $G$ of size $|G| = p_1^{i_1} \times\ldots\times p_n^{i_n}$ , where $p_i$ s are distinct primes, can we use $G$ -action on Sylow $p_i$ -subgroups to define a faithful action of $G$ ? Under what conditions on $G$ this is possible? What if we do not limit ourselves to action by conjugation and also consider coset action? The following post seems relevant when the action is limited to a single-family of $p_i$ , but does not consider simultaneous action on multiple set of Sylow subgroups. Group action on Sylow subgroups","['group-theory', 'group-actions', 'finite-groups', 'sylow-theory']"
3307915,Variant on Bayes's Rule and weather forecasting,"A common application of Bayes's Rule is with weather forecasting, but I have a question mapping this to a real-world situation. As in a previous StackExchange question , the simple Bayes's problem is usually framed as below: Marie is getting married tomorrow, at an outdoor ceremony in the
  desert. In recent years, it has rained only 5 days each year.
  Unfortunately, the weatherman has predicted rain for tomorrow. When it
  actually rains, the weatherman correctly forecasts rain 90% of the
  time. When it doesn't rain, he incorrectly forecasts rain 10% of the
  time. What is the probability that it will rain on the day of Marie's
  wedding? To use common notation with the selected answer in that question, let $R$ denote the event that it actually rains (prior). And $P$ denote the event that the weather forecaster is correct. We can then apply Baye's to calculate $P(R|P)$ . However, in reality, weather forecasters themselves seem to issue probabilities that it will rain as opposed to a binary response. For example, the weather forecaster says there is a 70% chance it will rain tomorrow (while being correct only 90% of the time, as before). How does one rationally update their prior $P(R)$ in this case?","['bayesian', 'probability']"
3307929,"Noncommutative rings, matrices and homomorphisms of free modules","In Blyth's book ""Module Theory: An Approach to Linear Algebra"" matrix theory is developed generally over a noncommutative ring $R$ with $1$ . However, it seems that there is a mistake in way that an important property doesn't carry over to noncommutative rings, namely the isomorphic of a ring of $n\times n$ matrices with coefficients in $R$ and the endomorphism ring of a free module over $R$ Given an a homomorphism $\phi\colon M\to N$ of free $R$ -modules with respective bases $(a_i)_m$ and $(b_i)_n$ , the matrix $\mathrm{Mat}(\phi,(b_i)_n,(a_i)_m)$ of this homomorphism with respect to said bases is a matrix $(r_{ij})$ such that $r_{ij}$ is the unique element of $R$ so that $\phi(a_i) = s_{1i}b_1 + ... + r_{ij}b_j + ... + s_{ni}$ . One can prove that, for the respective bases, there is an isomorphism $\vartheta\colon\mathrm{Hom}_R(M,N)\to\mathrm{Mat}_{n\times n}(R), \phi \mapsto \mathrm{Mat}(\phi,(b_i)_n,(a_i)_m)$ . But am I right to assume that this is not a ring homomorphism? It seems modules over noncommutative rings lack the multiplicative property $$\mathrm{Mat}(\psi\circ\phi, (c_i)_p,(a_i)_n) = \mathrm{Mat}(\psi,(c_i)_p,(b_i)_m)\mathrm{Mat}(\phi,(b_i)_m,(a_i)_n)$$ for free modules $M,N,P$ with respective bases $(a_i)_n, (b_i)_m$ and $(c_i)_p$ and their homomorphisms $\phi\colon M\to N, \psi\colon N\to P$ . Am I right or there is a mistake there? I was doing all the matrix theory for commutative rings before now, but encountered a proof in the aforementioned book which uses (probably wrong) ring isomorphism. Let $\mathrm{Mat}(\phi,(b_i)_m,(a_i)_n) = (r_{ij})$ and $\mathrm{Mat}(\psi,(c_i)_p, (b_i)_m) = (s_{ij})$ . Then we have $\mathrm{Mat}(\psi,(c_i)_p, (b_i)_m)\mathrm{Mat}(\phi,(b_i)_m,(a_i)_n) = (t_{ik})$ where $t_{ik} = \sum_{j = 1}^m s_{ij}r_{jk}$ . Also, $$(\psi\circ\phi)(a_i) = \psi\left(\sum_{i = 1}^n r_{ij}b_i\right) \\
= \sum_{i = 1}^n r_{ij}\psi(b_i) \\
= \sum_{i = 1}^n r_{ij}\left(\sum_{k = 1}^p s_{ki}c_k\right) \\
= \sum_{k = 1}^p \left(\sum_{i = 1}^n r_{ij}s_{ki}\right)c_k \\
\neq \sum_{k = 1}^p \left(\sum_{i = 1}^n s_{ki}r_{ij}\right)c_k
$$ generally.","['free-modules', 'modules', 'matrices', 'linear-algebra', 'noncommutative-algebra']"
3307959,Show that $\frac{f(x)}{x}$ is a decreasing function implies that $f(x)$ is subadditive,"I am studying Carother's Real Analysis for my qualifying exams. In the book I am to prove that if $f : [0, \infty) \rightarrow [0, \infty)$ is an increasing function, $f(0) = 0$ , and $f(x) > 0$ for all $x > 0$ , then $\frac{f(x)}{x}$ being decreasing for $x > 0$ implies that $f$ is subadditive, or that $f(x + y) \leq f(x) + f(y)$ . So far I have tried: $\frac{f(x+y)}{x+y} \leq \frac{f(y)}{y}$ and $\frac{f(x+y)}{x+y} \leq \frac{f(x)}{x}$ implies that $2\frac{f(x+y)}{x+y} \leq \frac{f(x)}{x} +\frac{f(y)}{y}$ , so $\frac{f(x+y)}{x+y} \leq 2\frac{f(x+y)}{x+y} \leq \frac{f(x)}{x} +\frac{f(y)}{y} \leq f(x) + f(y)$ I think that I am close but I can't get rid of the $x + y$ in the denominator. Any help would be appreciated.",['real-analysis']
3307996,"Exercise asks to show that $\sum_{n=1}^{\infty}\frac{n+1}{n}\frac{1}{3^n}x^n$ converges for $x\in(-1,1)$. I show $x\in(-3,3)$. Is something wrong?","Consider the power series $\sum_{n=1}^{\infty} \frac{n+1}{n}\frac{1}{3^n}x^n$ . 1. Show that the series converges for all $x\in (-1,1)$ . If we let $a_n = \frac{n+1}{n}\frac{1}{3^n}x^n$ then using the ratio test we get \begin{equation*}
\begin{split}
\lim_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right| &= \lim_{n\to\infty} \left|\frac{\frac{n+2}{n+1}\frac{1}{3^{n+1}}x^{n+1}}{\frac{n+1}{n}\frac{1}{3^n}x^n}\right| \\
&= \lim_{n\to\infty} \left|\frac{n+2}{n+1}\frac{1}{3^{n+1}}x^{n+1}\times \frac{n}{(n+1)x^n}3^n\right| \\
&= \left|\frac{x}{3}\right|\lim_{n\to\infty} \left|\frac{n(n+2)}{(n+1)^2}\right| \\
&= \frac{|x|}{3}.
\end{split}
\end{equation*} So if $\frac{|x|}{3} < 1$ , then this series will converge. Since $\frac{|x|}{3} < 1 \Longleftrightarrow |x| < 3$ , the radius of convergence is $3$ , i.e. the series converges for all $x\in (-3,3)$ .
Have I gone wrong somewhere or is the question wrong???","['limits', 'convergence-divergence']"
3308010,"Alternative methods to obtain the derivative of an elementary function (besides the definition, and the sum/product/chain rules)","Given a differentiable function $f:\mathbb R \to \mathbb R$ which is an elementary function , what methods are there to calculate the its derivative? (1) We of cause have the possibility of using the definition itself: $$
f'(x) = \lim_{h\to 0}\frac{f(x+h)-f(x)}{h}
$$ (2) We define some basic functions for which we look up their derivative, and use sum, product, and chain rule to calculate $f'(x)$ . What other methods are there?","['functions', 'derivatives']"
3308020,roll a dice repeatedly until the sum goes above 63,"I saw this question online but I don't know if my solution is right or not. Here is the original question: Roll a die repeatedly. Say that you stop when the sum goes above 63. What is the probability that the second to last sum value (total) was X. Make a market on this  probability. Ie what is your 90 percent confidence interval. This is my solution: X=63, 6 possible last roll as 1,2,3,4,5,6; X=62, 5 possible last roll as 2,3,4,5,6; X=61, 4 possible last roll as 3,4,5,6; X=60, 3 possible last roll as 4,5,6; X=59, 2 possible last roll as 5,6; X=58, 1 possible last roll as 6. BUT I don't know if it's right to think backward instead of considering the possible second to last value first? i.e. how many possibility to get a 63 regardless of last roll. Thank you very much!","['dice', 'probability']"
3308027,Average queue length with impatient customers,"Suppose customers join a queue with a poisson arrival rate $m$ . If a customer is not served within a unit of time, she abandons the queue. Customers are served in a first-come-first-served (FCFS) manner. There is a single server, and the service time for each customer is distributed exponentially with mean $\frac{1}{\lambda m}$ , where $\lambda<1$ . I would like to show that the average length of the queue is at least $m-o(m)$ . Any input will be appreciated! P.S. The claim is intuitive, but I have not been able to find a formal proof. I have also run simulations, which confirm that this holds.","['stochastic-processes', 'poisson-process', 'queueing-theory', 'probability-theory', 'probability']"
3308067,"If $G$ is a simple graph with at least two vertices, prove that $G$ must contain two or more vertices of the same degree","If $G$ is a simple graph with at least two vertices, prove that $G$ must contain two or more vertices of the same degree. I proved this theorem, I need to check if my proof is correct. Base case : Let $V(G)=\{v_1,v_2\}$ , then the most edges the graph can have is $\frac{2(2-1)}{2}=1$ , so this means $E(G)=\{v_1 v_2\}$ , and $deg.v_1 = deg.v_2=1$ Inductive hypothesis : Let $V(G) = \{ v_1, ..., v_k \} | k > 2$ , then the most edges this graph can have is $\frac{k(k-1)}{2}$ , so this means $E(G)=\{e_1, ..., e_{\frac{k(k-1)}{2}}\}$ . Suppose $deg.v_i=deg.v_j|i≠j∧v_i,v_j∈V(G)$ Inductive step : Let $V(G) = \{ v_1, ..., v_k, v_{k+1} \}$ , then the most edges this graph can have is $\frac{(k+1)k}{2}$ , which means that $E(G)=\{e_1, ..., e_{\frac{(k+1)k}{2}}\}$ , and that there are $\frac{(k+1)k}{2}-\frac{k(k-1)}{2}=k$ new edges, each one coming from $v_1,...,v_k$ vertices, which mans that each vertex's degree is increased by one, and, since $deg.v_i=deg.v_j$ is true, $deg.v_i+1=deg.v_j+1$ is also true, which comples the proof. The only thing that kinda' makes me doubt about the correctness of the proof, is that I always set the number of edges to the highest possible, and I don't know if this breaks the proof's generality. Thanks in advance.","['permutations', 'graph-theory', 'proof-verification', 'combinatorics', 'discrete-mathematics']"
3308068,Limit Cycle at $r=1$?,"Consider the non-linear ODE $$u''+(u^2+u'^2-1)u'+u=0.$$ Transforming this to polar coordinates: $$r'=-(r^2-1)r\sin^2(\theta),$$ $$\theta'=-\sin(\theta)\cos(\theta)(r^2-1)-1.$$ If we consider an annulus (trapping region), $\frac{1}{2}<x^2+y^2<2,$ how do we deal with the $\sin^2(\theta)$ term? We can use the fact that $$\sin^2(\theta)=\frac{1-\cos(2\theta)}{2}.$$ If we take $\cos(2\theta)=-1\implies r'>0$ for $r<\frac{1}{2}$ and $r'<0$ for $r>2$ . But , if we take $\cos(2\theta)=1\implies r'=0$ for $r<\frac{1}{2}$ and $r'=0$ for $r>2$ . Ideally $r'$ should point towards the annular region for both cases. I can't see an error in my logic.","['calculus', 'ordinary-differential-equations', 'dynamical-systems']"
3308105,$f:\mathbb{R}^{+}\to\mathbb{R}^{+}$ is uniformly continuous. Does $\lim_{x\to\infty}\frac{f(x+\frac{1}{x})}{f(x)}=1$ hold? [duplicate],"This question already has an answer here : Difficulty in finding a counterexample (1 answer) Closed 4 years ago . If $f$ is uniformly continuous,it's easy to show $f(x+\frac{1}{x})-f(x)=o(1).$ So we have $$\frac{f(x+\frac{1}{x})}{f(x)}=1+o(\frac{1}{f(x)})$$ So I think $\lim_{x\to\infty}\frac{f(x+\frac{1}{x})}{f(x)}=1$ won't necessarily hold.But I cannot find a counterexample.Any help will be thanked.","['limits', 'calculus']"
3308137,Challenging sum: Calculate $\sum_{k=1}^\infty\frac{(-1)^{k-1}}{k^52^k{2k \choose k}}$,"We proved in our previous solutions here and here the following two sums: $$\sum_{k=1}^\infty\frac{(-1)^{k-1}}{k^32^k {2k\choose k}}=\frac1{4}\zeta(3)-\frac1{6}\ln^32$$ $$\sum_{k=1}^\infty\frac{(-1)^{k-1}}{k^42^k{2k \choose k}}=4\operatorname{Li}_4\left(\frac12\right)-\frac72\zeta(4)+\frac{13}4\ln2\zeta(3)-\ln^22\zeta(2)+\frac5{24}\ln^42$$ and below I managed to prove the following equality $$\sum_{k=1}^\infty\frac{(-1)^{k-1}}{k^52^k{2k \choose k}}=10\operatorname{Li}_5\left(\frac12\right)+6\ln2\operatorname{Li}_4\left(\frac12\right)-\frac{19}{2}\zeta(5)+\frac72\ln2\zeta(4)+\ln^22\zeta(3)\\-\frac{2}{3}\ln^32\zeta(2)+\frac{19}{120}\ln^52$$ but solution turned out very long as many results were used in the calculations. I would love to see different methods that save us such tedious work. By the way, $k^5$ is the highest power I could get and I think a sum will be really complicated for higher weight and even has no closed form if we come across $\sum_{n=1}^\infty\frac{(-1)^nH_n}{n^a}$ for odd $a>3$ as the last has no known closed form. My solution is too long to be posted here so I will just post it in the answer section. Different approaches are always appreciated. thank you all.","['integration', 'harmonic-numbers', 'polylogarithm', 'binomial-coefficients', 'sequences-and-series']"
3308188,Closed doesn't imply complete,"Let $X$ be a norm  linear space.  If $X$ is banach space then subspace $Y$ is closed iff $Y$ is complete. But if $X$ is not banach space then $Y$ is closed need not imply $Y$ is complete. Can u give me such an example? I took a non-banach space $C[0, 1]$ with integration norm. But I am unable to find such an example here.",['functional-analysis']
3308219,"If $f: \mathbb R^n \to \mathbb R^n$ satisfies $\|f(x - y)\| = \|f(x) - f(y)\|$, is $f$ additive?","Question Main question: Let $\| \cdot \|$ be a norm on a finite-dimensional real vector space $V$ . If $f : V \to V$ is a function satisfying $$
\| f(x-y)\| = \|f(x) - f(y)\|
$$ for all $x, y \in V$ , does it follow that $f$ is additive? I.e., does it follow that $f(x + y) = f(x) + f(y)$ for all $x, y$ ? Follow-up: Does the answer change depending on the choice of $\| \cdot \|$ ? In particular, what if $\| \cdot \|$ is an inner product norm? Background Last week, user C.F.G. asked a very similar question , to which the answer was ""no for trivial reasons"": they asked whether the function $f$ was necessarily linear, but clearly all additive functions satisfy the condition, and there are non-linear additive functions. User Charlie Cunningham pointed out in the comments that the question is actually interesting if you remove the trivial reasons. Because the question had been answered, I tried to get an answer to the non-trivial question myself. The original question included the requirement that the vector space be finite-dimensional; I removed this requirement because it erroneously struck me as an irrelevant restriction (I thought you could just take the subspace containing $x, y, f(x), f(y)$ to get a finite-dimensional $V$ ). (The functional relation in those questions has a different form, but this is equivalent the formulation above, as pointed out by user Omnomnomnom in comments.) However, we then got a negative answer to my question -- an example of a non-additive $f$ satisfying the condition -- where the infinite-dimensionality of $V$ was crucial. This has left us with the tantalizing option that the finite-dimensionality of $V$ was crucial to the resolution of the question. I did not want to ask yet another very similar question here, but I also did not want to edit my question, as it had gotten a correct answer that did not deserve to be made irrelevant. Maybe third time's the charm?","['functional-equations', 'normed-spaces', 'linear-algebra']"
3308256,Showing $\binom{2d-1}{n}-(n+1){d-1\choose n}=\sum_{i=1}^n (-1)^{n-i}{n+1\choose i+1}{id-d+n\choose n}$ for $n\geq1$ and $d\geq n+1$,"Recently, when I was trying to compute Hodge numbers of hypersurfaces in toric varieties, I discovered the following combinatorial identity: For every positive integers $n\geq 1$ and $d\geq n+1$ the following is an identity $$
   \binom{2d-1}{n}-(n+1){d-1\choose n}=\sum_{i=1}^n (-1)^{n-i}{n+1\choose i+1}{id-d+n\choose n}
 $$ I tried to prove it by induction on $n$ ; for $n=1$ the identity becomes trivial, but the inductive step seems to be very hard (at least for me!). For $d=n+1$ the identity can be proved by using arguments of V. Batyrev involving polar duality of reflexive polytopes. But for greater $d$ I have not any idea... Someone can help me?
Thank you a lot!",['combinatorics']
3308259,Calculus/real analysis problem from a Differential geometry exercice,"I found a calculus/real analysis problem from a differential geometry exercice of surfaces in $\mathbb{R}^3$ . Consider $U=\mathbb{R}\times(0,1/2)$ and let be $f:U\subset\mathbb{R}^2\to \mathbb{R}$ given by $$f(x,y)=(2x-\sqrt{4x^2+1})y\sqrt{1-4y^2}+\sqrt{x^2+y^2}.$$ I ploted the graph of $(x,y,f(x,y))$ in $\mathbb{R}^3$ and I saw that $f\geq 0$ , but I don't realized how prove that $f\geq 0$ on $U$ by hand. First, I worked with the inequality $(2x-\sqrt{4x^2+1})y\sqrt{1-4y^2}+\sqrt{x^2+y^2}\geq 0$ and the assumption that $y\in(0,1/2)$ but nothing. Secondly, I tried a change of variables to polar coordinates, but nothing too. Some help for this?
I will appreciate, thanks.","['multivariable-calculus', 'calculus', 'differential-geometry', 'real-analysis']"
3308260,Understanding the statement $S = \{x : x \in S\}$,"In Introduction to analysis by Rosenlicht there is the following set definition: $$S = \{x : x \in S\}$$ There is no specific context to this it's just that I find this a little recursive. I would read it "" $S$ is a set of $x$ for which every $x$ is an element of $S$ "". Is that correct?",['elementary-set-theory']
3308264,Quadratic Residues in $\{a\}\cup\{ah^2+bh+c:0\leq h<p\}$,"Let $p$ be an odd prime and $p\nmid b^2-4ac$ for integers $a>0,b,c$ . Can we show the set $$
\{a\}\cup\{ah^2+bh+c:0\leq h<p\}
$$ contains an equal number of quadratic residues and non-residues modulo $p$ ?","['number-theory', 'finite-fields', 'quadratic-residues', 'elementary-number-theory']"
3308273,Is or is not A set determined by its ancestors (elements)?,"this post says Axiom 1a. A set is determined by its elements Remark 1. It is
  important to notice that this axiom is a non trivial assertion about
  belonging. To understand this consider an analogous situation in which
  we consider human beings in the place of sets and elements, and x ∈ A
  means x is an ancestor of A. Then clearly A is not determined by its
  ancestors. I consider ancestors as elements here, which I am not sure I understand it the right way, if yes, ""A is not determined by its ancestors"" seems to be a typo, which should be ""A is determined by its ancestors"", is it? let A = {'a', 'b', 'c'} is 'a' an ancestor and an element of set A? I suppose it is. are 'a', 'b', 'c' ancestors and elements of set A? I suppose they all are. biologically, this makes definitely sense. My sister and I have exactly the same ancestors but we are different
  people mathematically, I cannot understand 'a', 'b', 'c' are ancestors of set A while set A is not determined the ancestors {'a', 'b', 'c'}.",['elementary-set-theory']
3308337,Gauss-Weingarten relations on an arbitrary Riemannian manifold,"I'm trying to derive the Gauss-Weingarten relations on a hypersurface immersed on an arbitrary Riemannian manifold (see page $468$ of this paper for the context): $\begin{cases}
\frac{\partial^2 F^{\alpha}}{\partial x_ix_j} - \Gamma^k_{ij} \frac{\partial F^{\alpha}}{\partial x_k} + \overline{\Gamma}^{\alpha}_{\rho\sigma} \frac{\partial F^{\rho}}{\partial x_i} \frac{\partial F^{\sigma}}{\partial x_j} = -h_{ij} \nu^{\alpha}\\
\frac{\partial \nu^{\alpha}}{\partial x_j} + \overline{\Gamma}^{\alpha}_{\rho\sigma} \frac{\partial F^{\rho}}{\partial x_j} \nu^{\sigma} = h_{jl} g^{lm} \frac{\partial F^{\alpha}}{\partial x_m}
\end{cases}$ $\textbf{My attempt:}$ Firstly, I will prove the first equation. By one hand, I used the local formula for covariant derivative of the vector field $\overline{\nabla}_j F$ along $i$ -parameter curves (see pages $122$ and $123$ of Barrett O'Neill - Semi-Riemannian Geometry with Applications to General Relativity for details): $\begin{align*}
\overline{\nabla}_i \overline{\nabla}_j F &= \sum_\limits{\alpha=1}^n \left\{ \frac{\partial^2 F^{\alpha}}{\partial x_i \partial x_j} + \sum_\limits{\rho,\sigma} \overline{\Gamma}^{\alpha}_{\rho\sigma} \frac{\partial F^{\rho}}{\partial x_i} \frac{\partial F^{\sigma}}{\partial x_j} \right\} \frac{\partial}{\partial x_{\alpha}}
\end{align*}$ By the other hand, $\begin{align*}
\overline{\nabla}_i \overline{\nabla}_j F &= \sum_\limits{\alpha=0}^n \overline{\Gamma}^{\alpha}_{ij} \frac{\partial}{\partial x_{\alpha}}\\
&= \sum_\limits{k=1}^n \Gamma^k_{ij} \frac{\partial}{\partial x_k} - h_{ij} \nu
\end{align*}$ Combining these two observations, $$\frac{\partial^2 F^{\alpha}}{\partial x_ix_j} - \Gamma^k_{ij} \frac{\partial F^{\alpha}}{\partial x_k} + \overline{\Gamma}^{\alpha}_{\rho\sigma} \frac{\partial F^{\rho}}{\partial x_i} \frac{\partial F^{\sigma}}{\partial x_j} = -h_{ij} \nu,$$ which is different from the original equation by the term $-h_{ij} \nu$ . I would like to know where I'm missing. I will prove the second equation. By one hand, I used the local formula for covariant derivative of the vector field $\nu$ along $j$ -parameter curves: $\begin{align*}
\overline{\nabla}_j \nu &= \sum_\limits{\alpha=0}^n \left\{ \frac{\nu^{\alpha}}{\partial x_j} + \sum_\limits{\rho,\sigma} \overline{\Gamma}^{\alpha}_{\rho\sigma} \frac{\partial F^{\rho}}{\partial x_j} \nu^{\sigma} \right\} \frac{\partial}{\partial x_{\alpha}}
\end{align*}$ By the other hand, $\begin{align*}
\overline{\nabla}_j \nu &= h_{jl} g^{lm} \frac{\partial F^{\alpha}}{\partial x_m}
\end{align*}$ I want to combine these two observations in order to obtain $$\frac{\partial \nu^{\alpha}}{\partial x_j} + \overline{\Gamma}^{\alpha}_{\rho\sigma} \frac{\partial F^{\rho}}{\partial x_j} \nu^{\sigma} = h_{jl} g^{lm} \frac{\partial F^{\alpha}}{\partial x_m},$$ but I can't because I have a derivation $\frac{\partial}{\partial x_{\alpha}}$ by one hand and a number $\frac{\partial F^{\alpha}}{\partial x_m}$ by the other hand. I would like to know what I'm missing here. Thanks in advance!","['mean-curvature-flows', 'riemannian-geometry', 'proof-explanation', 'solution-verification', 'differential-geometry']"
3308364,Effect of integral domain structure on the additive group of the ring,"I was wondering about the implications of the following statement:
The characteristic of an integral domain $R$ must be zero or prime $p$ . This implies that all elements in $R\backslash\{0\}$ should have order $p$ in the commutative group $R,+$ . So imposing the ring structure without zero divisors on a commutative group $R,+$ might result in a drastic change of the group structure, because suddenly all the elements are of the same order $p$ ? How can this be understood intuitively?","['ring-theory', 'group-theory', 'abstract-algebra', 'finite-groups']"
3308424,"If $\sin\theta-\cos\theta\leq\mu (\cos\theta + \sin\theta)$, then $\tan\theta \leq \frac{1+\mu}{1-\mu}$","In a text I'm reading, an implication is made: $$\sin\theta-\cos\theta\leq\mu (\cos\theta + \sin\theta) \quad\implies\quad \tan\theta \leq \frac{1+\mu}{1-\mu}$$ I tried using some trigonometric identities to reproduce this result, but it seems I'm not familiar enough with them. How was this implication made?",['trigonometry']
3308451,"I need proof .$\int^{\infty}_{0} e^{-at^2} \cos(2 x t) \, dt = \frac{1}{2} \sqrt{\frac{\pi}{a}} e^{\frac{-x^2}{a}}$. [duplicate]","This question already has answers here : Gaussian-like integral : $\int_0^{\infty} e^{-x^2} \cos( a x) \ \mathrm{d}x$ (7 answers) Closed 4 years ago . $$\int^{\infty}_{0} e^{-at^2}  \cos(2 x t) \, dt = \frac{1}{2} \sqrt{\frac{\pi}{a}} e^{\frac{-x^2}{a}}$$ for Re $(a)>0$ ,  for all real value greater than zero. I took this equation from the book, Abramowitz and Stegun , Equation number $7.4.6$","['integration', 'trigonometry', 'exponential-function']"
3308481,Trying to compute limit of singular integrals : $L= \lim_{s\to 1}(1-s)\int_{\Omega}\frac{(u(x)-u(y))}{|x-y|^{d+2s}} d y.$,"Let $\Omega\subset \Bbb R^d$ be a bounded $C^1$ domain. Let $u:\Bbb R^d\to \Bbb R$ be a function in $C^2_b(\Bbb R^d)$ . I would like to compute the following limit: for $x\in \partial \Omega$ $$L= \lim_{s\to 1}(1-s)\int_{\Omega}\frac{(u(x)-u(y))}{|x-y|^{d+2s}} d y. $$ Here is what I did so far: Let $r>0$ be arbitrarily small enough. Then as $u$ is bounded, we have $$\begin{align}&\lim_{s\to 1}(1-s)\int_{\Omega\cap \{|x-y|\geq r\}}\frac{|u(x)-u(y)|}{|x-y|^{d+2s}}dy\\&\leq C \lim_{s\to 1}(1-s)\int_{|x-y|\geq r}\frac{dy}{|x-y|^{d+2s}} \\&= Cc_d\lim_{s\to 1}(1-s) \int_r^\infty t^{-2s-1} dt= 0. \end{align}$$ so that $$L= \lim_{s\to 1}(1-s)\int_{\Omega \cap B_r(x)}\frac{(u(x)-u(y))}{|x-y|^{d+2s}} d y. $$ Using the fundamental theorem of calculus and  the relation $\nabla [|x|^\alpha]= \alpha x|x|^{\alpha-2}$ , $$\begin{align}&(1-s)\int_{\Omega \cap B_r(x)}\frac{(u(x)-u(y))}{|x-y|^{d+2s}} d y\\ & = -\int_0^1 dt (1-s)\int_{\Omega \cap B_r(x)}\frac{\nabla u(x+ t(y-x))\cdot (y-x)}{|x-y|^{d+2s}}dy\\&=  \int_0^1 dt\frac{ (1-s)}{d-2(1-s))}\int_{\Omega \cap B_r(x)}  \nabla u(x+ t(y-x))\cdot \nabla_y [|x-y|^{-d+2(1-s)}]dy \end{align}$$ Therefore my initial question could be resumed to computaion of $$\lim_{s\to 1} (1-s)\int_{\Omega \cap B_r(x)} \nabla u(x+ t(y-x))\cdot \nabla_y [|x-y|^{-d+2(1-s)}]dy.$$ Any idea on to move further? My feeling is the should be  a multiple factor of $\frac{\partial u}{\partial n}(x)= \nabla u(x).n(x)$ where $n(x)$ is the normal derivative on $\partial \Omega$ at the point $x$ . Don't be mind corrupted, I may have wrong expectation.","['singular-integrals', 'real-analysis', 'greens-theorem', 'contour-integration', 'partial-differential-equations']"
3308509,How to generate a list of two-element sets that make all possible four-elements sets?,"I have a list of 6 elements (A...F) that I need to organize into the smallest number of unique two-element combinations (AB, AC, etc) possible so that they form all possible four-element combinations (AB + CD = ABCD). The order of the elements is unimportant and I can use each two-element combination as many times as possible (i.e. AB + CD, but also AB + EF) I know that: -There are fifteen possible 4-way combinations and 
-it should be solvable with 9 two-element combinations ...but I can't figure out conceptually how to generate my list of two-element combinations. Any ideas on how I could get started on this?","['graph-theory', 'combinatorics']"
3308530,"$f,g : X \rightarrow Y$ are continuous functions and Y is an ordered set then $\{x \in X: f(x) \leq g(x)\}$ is closed","$f,g : X \rightarrow Y$ are continuous functions and Y is an ordered set then $\{x \in X: f(x) \leq g(x)\}$ is closed I saw a proof of this by showing that its complement is open. But the way i started thinking about the problem was trying to prove that it contained all its limit points hence be closed, and using the fact that $Y$ will be Haussdorff since it will have the order topology, but i got nowhere. I was wondering if its possible to do it the way i started thinking about the problem.Thanks in advance.",['general-topology']
3308570,The expectation of a function from a uniform and conditionally uniform distribution.,"This is from a section on long cycles from Billingsley's Convergence of Probability Measures. Here, $C_u^n$ is the length of the $u$ th cycle with $n$ total letters. I have trouble understanding the equation from the second paragraph. Namely, how do we get $$E[f(L_1^n , \frac{L_2^n}{1-L_1^n})] = \frac{1}{n^2} \sum_{i+j \le n} \frac{1}{1-i/n}f(i/n, \frac{j/n}{1-i/n})?$$ Also how do we make the change of variables to get $\int \int_{[0,1]^2} f(y_1,y_2)dy_1 dy_2$ ? If I make the straight change of variables $y_1 = x_1$ and $y_2 = x_2 / (1-x_1)$ then I can't get rid of the $1/(1-x_1)$ factor. I would greatly appreciate any help.","['measure-theory', 'analysis', 'real-analysis', 'probability-theory', 'probability']"
3308584,Studying representations of orthogonal group via symmetric group?,"Let $V$ be the vector space of $n\times n$ symmetric matrices with real entries. On $V$ we have the natural action of the orthogonal group $\textrm{O}(n)$ defined by $g.A:=g\cdot A\cdot g^{t}$ for $g\in\textrm{O}(n)$ and $A\in V$ . We identify $\mathbb{R}^n$ with the set of diagonal matrices. Thus we get an inclusion $\textrm{diag}:\mathbb{R}^n\hookrightarrow V$ . Now let $G=\mathfrak{S}_n$ be the symmetric group on $n$ elements. Considering $G$ as the subgroup of $\textrm{O}(n)$ consisting of permutation matrices, we get an action of $G$ both on $\mathbb{R}^n$ and $V$ that makes $\textrm{diag}$ to a homomorphism of $G$ -modules. If we pass to the dual map, we get a surjective homomorphism of $G$ -modules $$\textrm{diag}^\vee:V\to\mathbb{R}^n.$$ The decomposition of the $\textrm{O}(n)$ -module $V$ into irreducibles is $(\mathbb{R}\cdot\textrm{I}_n)\oplus V_0$ where $\textrm{I}_n$ is the identity matrix and $V_0$ is the set of traceless matrices. We note that the image under $\textrm{diag}^\vee$ of both is an irreducible $G$ -module. My question is, whether this property is preserved under taking the symmetric power: Let $W$ be an irreducible $\textrm{O}(n)$ -submodule of $\textrm{Sym}^p(V)$ . Is it true that $\textrm{diag}^\vee(W)$ is irreducible as $G$ -module?","['representation-theory', 'abstract-algebra', 'linear-algebra']"
3308592,Understanding a proof of sum of stopping times,"Lemma :Let $\sigma$ and $\tau$ be stopping times. If $\sigma,\tau\geqslant 0$ then $\sigma+\tau$ is also a stopping time. Proof : Let $t\in I$ . By a  previous result, $\tau\wedge t$ and $\sigma\wedge t$ are stopping times for any $t\in I$ (where $\tau\wedge t$ reads $\min\{\tau, t\}$ ).
In particular $\{\tau\wedge t< s\}\in\mathscr{F}_s\subset \mathscr{F}_t$ for any $s\leqslant t$ . On the other hand, we have $\tau\wedge t\leqslant s$ for $s>t$ . Hence $\tau'=(\tau\wedge t)+\mathbb{1}_{\{\tau>t\}}$ and $\sigma'=(\sigma\wedge t)+\mathbb{1}_{\{\sigma>t\}}$ and thus $\tau' +\sigma'$ are $\mathscr{F}_t$ measurable. We conclude $\{\tau+\sigma\leqslant t\}=\{\tau'+\sigma'\leqslant t\}\in\mathscr{F}_t$ . Observation : I have been thinking about this proof. But I cannot make sense of $\tau'=(\tau\wedge t)+\mathbb{1}_{\{\tau>t\}}$ and $\sigma'=(\sigma\wedge t)+\mathbb{1}_{\{\sigma>t\}}$ once both expressions in case $\tau>t$ are going to equal $t+1>t$ . So how come $\tau'$ , $\sigma'$ and its sum be $\mathscr{F}_t$ measurable? Question : Can someone explain me this proof? Why is $\tau'+\sigma'$ measurable? Thanks in advance!","['stochastic-processes', 'stopping-times', 'probability-theory']"
3308622,Solving a polynomial congruence with rational number unknowns for absolute factorisation,"I am implementing Gao's factorisation algorithm for bivariate rational polynomials $f\in\mathbb Q[x,y]$ . An overview and the reference to the paper describing the algorithm are in this answer . I see value in the algorithm because it performs absolute factorisation – if the polynomial splits over some algebraic field, the algorithm will calculate it; I do not need to guess. I am following the original paper closely and there is a step I am unable to implement explicitly (using SymPy). Theorem 2.8. Suppose that $g_1,\dots,g_r$ form a basis for $G$ over $\mathbb F$ [which is $\mathbb Q$ in this question's context]. For
  any $g\in G$ , there is a unique $r×r$ matrix $A=(a_{ij})$ over $\mathbb F$ such that $$gg_i\equiv\sum_{j=1}^ra_{ij}g_jf_x\mod f\tag1$$ $r$ is the number of absolutely irreducible factors of $f$ . I have successfully implemented procedures to compute the $g_i$ (which arise as the nullspace of a linear system), and $g$ is a randomly chosen linear combination of the $g_i$ . If $g$ is such that $A$ 's characteristic polynomial $c_A(x)$ has no repeated roots, then it is shown that $f$ splits over $\mathbb Q(\alpha)$ where $c_A(\alpha)=0$ . What is the procedure to compute the $a_{ij}$ in $(1)$ when given $f$ , the $g_i$ and the chosen $g$ ? I believe the main difficulty is ensuring that the $a_{ij}$ are in $\mathbb Q$ – the routines I've examined in SymPy for Bézout decompositions of multivariate polynomials don't seem to be able to enforce this. The $\bmod f$ is also tripping me up. There is a worked example given which may help with the explanation, with $f=9+23y^2+13yx^2+6y+7y^3+13y^2x^2+x^4+6yx^4+x^6$ . This polynomial has three absolutely irreducible factors ( $r=3$ ) with computed $g_i$ $$g_1=-12x-8xy-19xy^2-12x^3y-2x^5+x^3$$ $$g_2=12x+10xy+18xy^2+12x^3y+2x^5$$ $$g_3=-18x-12xy-22xy^2-14x^3y-2x^5$$ $$g=g_1+g_2=2xy-xy^2+x^3$$ The computed $A$ is $$\begin{bmatrix}
-62/247&63/988&189/988\\
63/247&-17/247&-51/247\\
-54/247&135/494&79/247\end{bmatrix}$$","['algebraic-geometry', 'factoring', 'splitting-field', 'polynomials', 'commutative-algebra']"
3308700,Invertibility (and eigen-decomposition) of complex symmetric matrix?,"Consider $Z = K - i \omega S - \omega^2 M$ , where: $\omega$ is a positive real number; $K$ is a real, symmetric, and positive semi-definite matrix; $M$ is a real, symmetric, and positive definite matrix. $S$ is a non-zero matrix. Which property (or properties) $S$ should satisfy so that $Z$ is invertible?
Or so that $(Z, M)$ has an eigendecomposition $Z = M \Phi \Lambda \Phi^T M$ (with $\Phi^T M \Phi = I$ )? Or which reference book discusses this class of problems? Remark 1: Matrices like $Z$ arise in the discretization of the Helmholtz equation with absorbing boundary conditions or of the frequency response analysis with damping. Remark 2: Many posts on the forum discuss the matrix $\left[ \begin{array}{cc} 1 & i \\ i & -1 \end{array} \right]$ as an example of non-diagonalizable complex symmetric matrix. Here I want to know which non-zero matrix $S$ would work. EDIT: Generalization of proportional damping gives us a family of $S$ matrices that would allow the eigendecomposition: $S = M \sum_{j = J_1}^{J_2} s_j \left( M^{-1} K \right)^j$","['linear-algebra', 'inverse', 'eigenvalues-eigenvectors']"
3308749,closed formula for: $(g\partial)^n$,"The objective is to obtain a closed formula for: $$
\boxed{A(n)=\big(g(z)\,\partial_z\big)^n,\qquad n=1,2,\dots}
$$ where $g(z)$ is smooth in $z$ and $\partial_z$ is a derivative with respect to $z$ . 
I think the first few terms are, \begin{equation}
\begin{aligned}
A(1) &= g\,\partial\\
A(2)&= g\,(\partial g)\,\partial+g^2\,\partial^2\\
A(3)&= \big[(\partial^2g)g^2+(\partial g)^2g\big]\partial+3(\partial g)g\,\partial^2+g^2\partial^3\\
A(4) &= \big[(\partial^3g)g^3+4(\partial^2g)(\partial g)g^2+(\partial g)^3g\big]\partial\\
&\quad +\big[4(\partial^2g)g^3+7(\partial g)^2g^2\big]\partial^2+6(\partial g)g^3\partial^3+g^4\partial^4\\
&\,\,\vdots
\end{aligned}
\end{equation} and presumably there is a simple pattern that I'm failing to see. The coefficients do not seem (to me) to be associated to a special function (such as a Bell polynomial) in a simple way. Any ideas? Perhaps there is a standard formula? Thanks!","['calculus', 'combinatorics', 'special-functions']"
3308751,Polynomial satisfying a relation for all positive integers,"Let $P \in \mathbb{R}[X]$ such that $$P(1)+P(2)+\dots+P(n)=n^5,$$ $\forall n\in \mathbb{N}$ . Compute $P\left(\frac{3}{2}\right)$ . I think that from that relation it is mandatory that $\deg P=5$ , but from the given relation we also have that $P(1)+P(2)+...+P(n-1)=(n-1)^5$ , so it follows that $P(n)=n^5-(n-1)^5,\forall n\in \mathbb{N}$ , so $\deg P=4$ . I know that this only holds for positive integers, but it still seems kind of contradictory to me. Anyway, I don't know if any of my ideas actually help to solve the actual task, so I am looking forward to seeing your ideas.","['algebra-precalculus', 'polynomials']"
3308782,"Sqrt of polynomial, How to find integer X that gives integer y","I am trying to solve how to find an integer X that gives an integer result of Y on an equation that has the square root of a polynomial.  The equations are of the type: $$y =\sqrt{ax^2+bx+c}$$ where x>0 and y>=0.
as a more concrete example: $$y= \sqrt{x^2 + 10036x - 10015}$$ While if I manually step x I get y=2369 at x=532, I'm trying to find a numerical way to solve these without having to step. How can I solve these equations to quickly find the (first) integer x and y pair?  Thank you.","['number-theory', 'calculus']"
3308799,Indicator method for isolated chairs,"I am working on the following problem for an applied probability qualifying exam. Fix positive integers $m\leq n$ with $n>4$ . Suppose $m$ people sit at a circular table with n seats, with all ${n \choose m}$ seatings equally likely. A seat is called isolated if it is occupied and both adjacent seats are vacant. Find the mean and variance of the number of isolated seats. Letting $X$ be the number of isolated seats, I have written $X=\sum_{i=1}^{n}\mathbb{1}_{A_{i}}$ , where $A_{i}=\{$$i^{th}$ seat is isolated $\}$ for i=2,...,n-1, and $A_{1}$ and $A_{n}$ defined appropriately given the circular arrangement. This yields $\mathbb{E}X=\sum_{i=1}^{n}\mathbb{P}(A_{i})$ . Now I know that $A_{i}$ occurs only when chairs $i\pm1$ are vacant and chair $i$ is occupied. I mistakenly initially computed that the probability is $p(1-p)^2$ where $p$ is the probability of a single chair being occupied (which turned out to be $m/n$ ). However, since these events are not independent, I am unsure how to compute the actual probability.","['conditional-probability', 'probability']"
3308814,Proof of the union of two indicator functions,"How do I prove that $$1_{A∪B} = 1_A + 1_B - 1 _{A∩B}$$ ? For the proof of intersection I found on mathexchange that: \begin{align}1_A(x)1_B(x)&=\begin{cases}
1& x\in A\\
0& x\in A^C
\end{cases}\begin{cases}
1& x\in B\\
0& x\in B^C
\end{cases}\\&=\begin{cases}
1& x\in A \cap x\in B\\
0\cdot 1& x\in A^C\cap B\\
1\cdot 0& x\in A \cap B^C\\
0\cdot 0& x\in A^C \cap B^C\\
\end{cases}\\&=\begin{cases}
1& x\in A \cap B\\
0& x\in \underbrace{(A^C\cap B)\cup(A \cap B^C )\cup(A^C \cap B^C)}_{=(A\cap B)^C}\\
\end{cases}\\&=1_{A\cap B}(x)\end{align} I tried to do the same thing as in writing $ 1_A + 1_B - 1 _{A∩B}$ out like that but ended up getting really confused and can't seem to be able to prove this. \begin{align}1_A(x)+1_B(x)-1_A(x)1_B(x)&=\begin{cases}...+\begin{cases}...-\begin{cases}
\end{cases}\end{cases}\end{cases}...\end{align}","['stochastic-calculus', 'probability', 'real-analysis']"
3308825,Measurable-set-valued random variables and their relation to conditional probabilities,"Let $(\Omega, \mathcal F, P)$ be a probability space. I am seeking references on measurable-set-valued random variables . That is, I am interested in functions $X$ from $\Omega$ into $\mathcal F$ . In order for such $X$ to be measurable, $\mathcal F$ must be equipped with a sigma-field. My first question, then, is: Is there a natural sigma-field for $\mathcal F$ ? Will $2^{\mathcal F}$ do? Here is some partial motivation for the first question, which leads to a second question. Suppose that $X$ takes countably many values $F_1, F_2,...$ in $\mathcal F$ . Suppose, moreover, that $F_n = \{\omega: X(\omega) = F_n\}$ , so that $\{F_n\}$ is a countable partition of $\Omega$ . Then, viewed as a function of $\omega$ , $P(\cdot \mid X(\omega))$ is just a version of the regular conditional probability $P(\cdot \mid \mathcal A)$ , where $\mathcal A$ is the sigma-field generated by the $F_n$ . In general, it won't be the case that $X$ takes only countably many values, however. And it won't be the case that the values $F$ of $X$ satisfy $F = \{X=F\}$ . But $X$ always generates a sub-sigma-field $\sigma(X)$ of $\mathcal F$ in the usual way. Thus, a second question: What is the general relationship between the conditional probability $P(A \mid \sigma(X))$ and $P(A \mid X(\omega))$ , viewed as a function of $\omega$ ? Note that the quantity $P(A \mid X(\omega))$ is an elementary conditional probability, i.e. it is equal to $P(A \cap X(\omega))/P(X(\omega))$ . (For the purposes of addressing the last question, let us stipulate that $P(A \mid B)=P(A)$ whenever $P(B)=0$ .)","['conditional-probability', 'measure-theory', 'probability-theory', 'reference-request']"
3308828,Method of Cauchy for particular solution of second order inhomogeneous differential equation,"Consider $$
- u''(t) + p(t) u'(t) + q(t) u(t) = f(t), \quad t \in (a,b).
$$ For $s \in (a,b)$ find coefficient functions $c_1$ and $c_2$ from the general solution of the homogenous equation $$
u_{\text{hom}}(t) = c_1 u_1(t) + c_2(t)
$$ such that $u_{\text{hom}}(s) = 0$ and $u'_{\text{hom}}(s) = - f(s)$ .
  Then $$
u_p(t)
:= \int_{t_0}^{t} c_1(s) u_1(t) + c_2(s) u_2(t)
$$ is a particular solution of the inhomogeneous equation. And now I am supposed to find a particular solution to $$
t^2(1 - t) u''(t) + 2t (2 - t) u'(t) + 2(1 + t) u(t) = \frac{1}{t - 1}, \quad t \in (0,1).
$$ I have found the solution for the homogeneous equation to be $$
u_{\text{hom}}(t) = c_1 t^{-2} + c_2 \cdot \frac{t^2 - 3t + 3}{3t}
$$ But the instructions confuse me in particular because in one equation $c_i$ seem to be constants and the other functions and I con't know how they are connected or if that is the case at all. Can somebody please give me a hint on how to continue? Also, in the first equation of the yellow block, the highest derivate doesn't have a coefficient function, but since $p$ and $q$ are not relevant to the instructions that follow I don't have to divide my equation by $t^2(t - 1)$ , right? So taking the hints from the answer below I got:
So my ""Wronski-Matrix"" is $$\begin{pmatrix} t^{-2} & \frac{t^2 -3t + 3}{3t} \\ -2 t^{-3} & \frac{1}{3} - t^{-2}\end{pmatrix},$$ solving I get $$C_1'(s) = \frac{t^3(t^2 - 3t + 3)}{(t - 3)^2(t - 1)}$$ and $$C_2'(t) = \frac{3t^2}{(1 - t)(t - 3)^2}.$$ Integrating those gives horrible terms , so what have I done wrong?",['ordinary-differential-equations']
3308907,Why is the eigenvalue equation $ Ax = \lambda x $ nonlinear?,"I read in a book that the eigenvalue equation is nonlinear but I can't see why that is form just looking at it... $$
Ax = \lambda x
$$ looks a lot similar to $$
Ax = b
$$ and if the matrix $A$ represents a linear transformation of $x$ then I don't see why $Ax = \lambda x$ would be nonlinear...","['linear-algebra', 'linear-transformations', 'eigenvalues-eigenvectors']"
3308938,Show that $\lim_{x\to \frac{\pi}{2}} \frac{1}{\big(x-\frac{\pi}{2}\big)}+{\tan(x)}=0$.,Prove that $$\lim_{x\to \frac{\pi}{2}} \frac{1}{\big(x-\frac{\pi}{2}\big)}+{\tan(x)}=0.$$ I'm not really sure how to proceed. I know that I should not try L'Hôpital's rule (tried that) but not sure how I would incorporate into the Squeeze Theorem or how I would use continuity. Thanks! Edit: Turns out I was really dumb and you do use L'Hôpital's rule twice. I made the mistake of differentiating the whole quotient rather than the function on top and the bottom of the vinculum separately.,"['limits', 'limits-without-lhopital']"
3308969,On analytic function with $f(e^z)=z$,"Show that there does not exist any function $f$ which is analytic in $\mathbb C$ minus a (denumerable) set of isolated singularities and such that $f(e^z)=z$ wherever $f(e^z)$ is defined. My attempt : Suppose $w_0\ne 0$ is an isolated singularity of $f$ , then for every sequence $\{z_n\}$ in the domain of $f$ , such that $e^{z_n}\to w_0$ ( $e^{z_n}$ can take any value in the nbhd of $w_0$ since we can choose an analytic branch of $\ln w_0$ for each $w_0\ne 0$ ) and $f(e^{z_n})=z_n$ , $\lim_{n\to\infty}z_n=w$ and we conclude that $f$ can only have an isolated singularity at $z=0$ . Now note that $f(e^1)=1$ and $f(e^{2\pi i})=2\pi i$ but $e^1=e^{2\pi i}$ which is a contradiction. Is that correct? Edit: I think the main idea of this question is that we have to take a branch cut along $[0,\infty]$ to make $f$ well-defined, otherwise it is nonsense to claim $f(e^z)=z$ as was pointed out in my attempt above. On the other hand, $f$ is forced to exist other than a set with isolated singularities which is unrealistic, since $[0,\infty]$ does not consist of isolated points.","['complex-analysis', 'proof-verification']"
3308988,"When does the exact value of the integral $\int_{-3}^{3}(x^3\cos(x)+x)\,\mathrm{d}x$ occur when it is solved by the Simpson method?","When solving $$\int_{-3}^{3}(x^3\cos(x)+x)\,\mathrm{d}x$$ by Simpson's method, taking $n$ subintervals, the exact value is obtained if: $n$ is even. $n$ is greater than $3$ . $n$ is even and greater than $6$ . never possible. $n$ is odd. none of the above. Definitions: $$n=\frac{b-a}{h}$$ $$A=\frac{h}{3}(EX+4O+2E),\quad\left\{\begin{aligned}&EX=\text{extremes},\\&O=\text{odd},\\&E=\text{even}.\end{aligned}\right.$$ $$\text{Error}=E=\frac{a-b}{180}h^4f^{(4)}(\xi),\quad\xi\in[a,b].$$ I know the following result: If $f$ is a cubic function (i.e. a polynomial of degree $3$ ) then the Simpson's formula calculates the integral exactly whatever $h$ ( $n$ must be even). $b-a=3-(-3)=6\implies n=\dfrac{6}{h}$ . We cannot use the above result since $f(x)=x^3\cos(x)+x$ is not a polynomial , right? So what would be the correct answer? I thought about using the odd function property: if $g$ is odd then $\int_{-a}^{a}g(x)\,\mathrm{d}x=0$ . Since $f(x)=-f(-x)$ then $f$ is odd, so the integral is equal to $0$ . But I cannot conclude anything about the parity of $n$ nor the minimum value ( $>3$ ?, $>6$ ?). Thanks!!","['simpsons-rule', 'discrete-mathematics']"
3309178,Two Solutions for an ODE: $x' = x^{\frac45}$,"Find two different solutions $x_1, x_2 : \mathbb{R} \to \mathbb{R}$ of $$ \dot{x} = x^{\frac45}, \quad x(1) = 1. $$ This is a problem in a 60-minute exam, so it should be quite simple but still I'm failing. I can get $x(t) = \left(\frac{t+4}{5} \right)^5$ via separation of variables (might have miscomputed but that's not too important) but how can we find another solution? The solutions on Wolfram-Alpha do not look too simple...",['ordinary-differential-equations']
3309207,Examining $\int_0^1 \left(\frac{x - 1}{\ln(x)} \right)^n\:dx$,"I'm currently working on the following family of integrals: \begin{equation}
 I_n = \int_0^1 \left(\frac{x - 1}{\ln(x)} \right)^n\:dx
\end{equation} Where $n \in \mathbb{N}$ . I employed Feynman's Trick coupled with the Dominated Convergence Theorem and Leibniz's Integral Rule. In doing so, I introduced the following function: \begin{equation}
 J_n(t) = \int_0^1 \left(\frac{x^t - 1}{\ln(x)} \right)^n\:dx
\end{equation} Where $0 \leq t \leq 1 \subset \mathbb{R}$ .  With some fairly easy steps, I end up with the following ODE: \begin{equation}
    J_n^n(t) =  (-1)^n \sum_{j = 1}^n {n \choose j} (-1)^j \frac{j^n}{jt + 1}   \nonumber
\end{equation} Where $J_n^k(t)$ is the $k$ -th derivative of $J_n(t)$ with the conditions $J_n^k(0) = 0$ for $ 0 \leq k \leq n$ . As such, to resolve $J_n(t)$ I need to integrate $J_n^n(t)$ $n$ times whilst applying the initial conditions. Although I can do it for any fixed $n$ , I'm yet to be able to generalise it for any $n$ . I was wondering if anyone has working with this type of ODE and if so, is there any preferable ways to approach it? Initially I thought that using Laplace Transforms would be ideal as in applying it to $J_n^n(t)$ all terms would be removed given the initial condition. This felt apart as the Laplace Transform of $\frac{1}{t + a}$ is a nasty Special Function to work with. So, to repeat, is there an approach people can recommend? For anyone who may be interested, here is my work on this integral: In this section, I would like to address the following family of integrals: \begin{equation}
    I_n = \int_0^1 \left( \frac{x - 1}{\ln(x)} \right)^n \:dx \nonumber 
\end{equation} 0
To begin with, consider the case when $n = 1$ : \begin{equation}
    I_1 = \int_0^1 \frac{x - 1}{\ln(x)}\:dx \nonumber 
\end{equation} Here we introduce the function: \begin{equation}
    J_1(t) = \int_0^1 \frac{x^t - 1}{\ln(x)}\:dx \nonumber 
\end{equation} We observe that $I_1 = J_1(1)$ and $J_1(0) = 0$ . Here we employ Leibniz's  Integral Rule and differentiate under the curve with respect to $t$ : \begin{equation}
    J_1'(t) = \int_0^1 \frac{\frac{d}{dt}\left[x^t - 1 \right]}{\ln(x)}\:dx = \int_0^1 \frac{\ln(x)x^t}{\ln(x)}\:dx = \int_0^1 x^t \:dx = \left[ \frac{x^{t  +1}}{t + 1}\right]_0^1 = \frac{1}{t + 1} \nonumber 
\end{equation} We now integrate with respect to $t$ : \begin{equation}
    J_1(t) = \int \frac{1}{t + 1} \:dt = \ln\left|t + 1 \right| + C \nonumber 
\end{equation} Where $C$ is the constant of integration. To resolve $C$ we employ $J_1(0) = 0$ : \begin{equation}
    J_1(0) = 0 = \ln\left|0 + 1\right| + C \rightarrow C = 0 \nonumber
\end{equation} Thus, \begin{equation}
    J_1(t) = \ln\left|t + 1\right| \nonumber
\end{equation} We now resolve $I_1$ using $I_1 = J_1(1)$ : \begin{equation}
    I_1 = J_1(1) = \ln\left|1 + 1\right| = \ln\left|2\right| \nonumber 
\end{equation} The question I have is: Can this approach be used for other or all values of $n$ ?. To address this, I will proceed by applying the same method to $n = 2$ : \begin{equation}
    I_2 = \int_0^1  \frac{\left(x - 1 \right)^2}{\ln^2(x)}\:dx \nonumber
\end{equation} We introduce the function: \begin{equation}
    J_2(t) = \int_0^1 \frac{\left( x^t - 1\right)^2}{\ln^2(x)}\:dx \nonumber
\end{equation} We observe that $I_2 = J_2(1)$ and $J_2(0) = 0$ . We proceed here by employ Leibniz's Integral Rule and differentiate under the curve with respect to $t$ : \begin{equation}
    J_2'(t) = \int_0^1 \frac{\frac{d}{dt}\left[\left(x^t - 1\right)^2 \right]}{\ln^2(x)}\:dx = \int_0^1 \frac{2\left(x^t - 1\right)\ln(x)x^t}{\ln^2(x)}\:dx = 2 \int_0^1 \frac{x^t\left(x^t - 1\right)}{\ln(x)}\:dx \nonumber 
\end{equation} We observe that $J_2'(0) = 0$ . We now differentiate again with respect to $t$ : \begin{equation}
    J_2''(t) = 2\int_0^1 \frac{\ln(x)x^t\cdot \left(x^t - 1\right) + x^t \cdot \ln(x)x^t}{\ln(x)}\:dx = 2\int_0^1 2x^{2t} - x^t \:dx = 2\left[\frac{2x^{2t + 1}}{2t + 1 } - \frac{x^{t + 1}}{t + 1} \right]_0^1 = 2\left[\frac{2}{2t + 1} - \frac{1}{t + 1}\right] \nonumber 
\end{equation} We now integrate with respect to $t$ : \begin{equation}
    J_2'(t) = 2\int \frac{2}{2t + 1} - \frac{1}{t + 1} \:dt =2\bigg[ \ln\left|2t + 1\right| - \ln\left|t + 1\right| \bigg]  + C \nonumber 
\end{equation} Where $C$ is the constant of integration. To resolve $C$ , we use $J_2'(0) = 0$ : \begin{equation}
    J_2'(0) = 0 = 2\bigg[\ln\left|2\cdot 0 + 1\right| - \ln\left|0 + 1\right|\bigg] + C = 0 + C \rightarrow C = 0 \nonumber
\end{equation} Thus, \begin{equation}
    J_2'(t) = 2\bigg[\ln\left|2t + 1\right| - \ln\left|t + 1\right|\bigg] \nonumber \nonumber 
\end{equation} We now integrate again with respect to $t$ : \begin{equation}
    J_2(t) = 2\int  \ln\left|2t + 1\right| - \ln\left|t + 1\right| \:dt = 2\bigg[\left(\frac{2t + 1}{2}\right)\bigg[ \ln\left|2t + 1\right| - 1 \bigg] - \bigg[ \left(t + 1\right)\ln\left|t + 1\right| - t \bigg] \bigg] + D \nonumber 
\end{equation} Where $D$ is the constant of integration. To resolve $D$ we use the condition $J_2(0) = 0$ : \begin{equation}
     J_2(0) = 0 = 2\bigg[\left(\frac{2\cdot 0 + 1}{2}\right)\bigg[ \ln\left|2\cdot 0 + 1\right| - 1 \bigg] - \bigg[ \left(0 + 1\right)\ln\left|0 + 1\right| - 0 \bigg]\bigg] + D   = -1+ D \rightarrow D = 1 \nonumber 
 \end{equation} Thus, \begin{equation}
     J_2(t) = 2\bigg[\left(\frac{2t + 1}{2}\right)\bigg[ \ln\left|2t + 1\right| - 1 \bigg] - \bigg[ \left(t + 1\right)\ln\left|t + 1\right| - t \bigg]\bigg] + 1 \nonumber = \left(2t + 1\right)\ln\left|2t + 1\right| -2\left(t  + 1\right)\ln\left|t + 1\right| \nonumber 
 \end{equation} Thus, we now may resolve $I_2$ using $I_2 = J_2(1)$ : \begin{equation}
     I_2 = J_2(1) = \left( 2\cdot 1 + 1\right)\ln\left|2\cdot 1 + 1\right| -2 \left(1 + 1\right)\ln\left|1 + 1\right| = 3\ln(3) -4\ln(2) \nonumber 
\end{equation} Here I will attempt to resolve the integral in it's general form. I will employ the same approach as for $n = 1, 2$ and introduce the function: \begin{equation}
    J_n(t) = \int_0^1 \frac{\left(x^t - 1 \right)^n}{\ln^n(x)}\:dx \nonumber 
\end{equation} We observe that $I_n = J_n(1)$ and $J_n(0) = 0$ . We begin by expanding the integrand's numerator using the Binomail Expansion: \begin{equation}
    J_n(t) = \int_0^1 \frac{\sum_{j = 0}^n { n \choose j} \left(x^t\right)^j \left(-1 \right)^{n - j}}{\ln^n(x)}\:dx = (-1)^n \sum_{j = 0}^n {n \choose j} (-1)^j \int_0^1 \frac{x^{jt}}{\ln^n(x)}\:dx \nonumber
\end{equation} Taking the same approach as before, we now employ Leibniz's Integral Rule and differentiate $n$ times under the curve with respect to $t$ : \begin{equation}
    J_n^n(t) = (-1)^n \sum_{j = 0}^n {n \choose j} (-1)^j \int_0^1 \frac{\frac{d^n}{dt^n}\left[x^{jt}\right]}{\ln^n(x)}\:dx \nonumber
\end{equation} Here we note: \begin{equation}
    \frac{d^n}{dt^n}\left[x^{jt}\right] = j^n \ln^n(x)x^{jt} \nonumber 
\end{equation} , 
Noting that for $j= 0$ ,  the derivative is $0$ . Thus, \begin{equation}
    J_n^n(t) = (-1)^n \sum_{j = 1}^n {n \choose j} (-1)^j \int_0^1 \frac{j^n \ln^n(x)x^{jt}}{\ln^n(x)}\:dx = (-1)^n \sum_{j = 1}^n {n \choose j} (-1)^j j^n \int_0^1 x^{jt}\:dx  =  (-1)^n \sum_{j = 1}^n {n \choose j} (-1)^j \frac{j^n}{jt + 1}   \nonumber
\end{equation} Where $J_n^k(0) = 0$ for $k = 0,\dots, n$ .","['integration', 'definite-integrals', 'special-functions', 'ordinary-differential-equations']"
3309219,Find numbers of the form $\frac{n(n+1)}{2}$ such that the digits of the number are all same [duplicate],"This question already has answers here : Series of natural numbers which has all same digits (4 answers) Closed 4 years ago . Find all $a \in \{1, 2, \ldots, 8, 9\}$ such that $\exists n \in \mathbb{N}$ and the digits of $\dfrac{n(n+1)}{2}$ are all $a$ . In other words, we have to find the values of $a$ such that $\dfrac{n(n+1)}{2} = a \cdot (1111\ldots1111)$ where $a$ is a Numerical Digit of Base $10$ , i.e. $a \in \{1, 2, \ldots, 8, 9\}$ . I made the following progress: Let's take $(1111\ldots1111) = R$ for convenience. So we get the following equation: $n^2 + n - 2aR = 0$ . Taking the roots of $n$ by using the Quadratic Formula, we get $n = \dfrac{-1\pm\sqrt{1+8aR}}{2}$ . As $n \in \mathbb{N}$ , $-1\pm\sqrt{1+8aR}$ must be even, so $\sqrt{1+8aR}$ must be odd and hence $1+8aR$ must also be odd. We also know that $1+8aR$ must be a perfect square. So, our problem reduces down to $1+8aR = f^2$ where $f \in \mathbb{N}$ . I couldn't proceed ahead from here. Series of natural numbers which has all same digits also talks about pretty much the same problem. However, no conclusive solution was reached in that thread. I want to do the above question with an analytical approach. Hence, I beleive that my question is not a duplicate. [Source: As far as I know, this problem is from IMOTC India]","['contest-math', 'number-theory']"
3309360,"Calculating the 1st quartile: different result with Excel, Wolfram Alpha and using formula in my math book - why?","I have the following set of numbers:
1, 1, 8, 12, 13, 13, 14, 16, 19, 22, 27, 28, 31 I'm supposed to calculate the value of the 1st quartile (25th percentile) in this data set. Using the formula in my math book gives me 10 . Using Excel's or Google Sheet's built-in QUARTILE function gives me 12 . Using Wolfram Alpha https://www.wolframalpha.com/input/?i=first+quartile+(1,+1,+8,+12,+13,+13,+14,+16,+19,+22,+27,+28,+31) gives me 11 . Somebody has noted in a Stack Overflow post ( https://stackoverflow.com/a/53551756 ) this behavior with Python programming language too. Can someone please explain why the results differ in these different methods? What makes this very specific data set special so that these methods give different values? Which of these values is the correct / most correct one and why? Many thanks in advance!","['statistics', 'wolfram-alpha']"
3309382,Distance to circle inside triangle,"I apologise for the lack of a precise term or title, math isn't my strong suit. I'm trying to calculate the length of L, given angle A and radius of circle D, so that lines b and c tangents with circle D. Can anyone help me?","['trigonometry', 'geometry']"
3309395,Is being diffeomorphic to a manifold equivalent to being a manifold?,"Obviously manifolds are diffeomorphic to manifolds (namely themselves by the identity map). Is the converse true with diffeomorphism in this sense ? To be explicit : Let $M$ and $N$ be (smooth) manifolds. If need be, then you may give them dimension (Note: In some textbooks, not all manifolds have dimension ). Let $X$ be a subset of $M$ . Is $X$ a (regular/an embedded) submanifold of $M$ if there exists a map $f:X \to f(X)=N$ that is a diffeomorphism in this sense ? Edit : I previously asked if $X$ was a manifold but based on ljr's comment and I guess based on this question and this question , I guess asking for $X$ to be a manifold is not a very good question. We have that: such $f$ is bijective such $f$ is smooth in this sense : For each $p \in X$ , there exists a neighborhood $U_p$ of $p$ in $M$ and a smooth map $g: U_p \to N$ such that the restrictions $g|_{U_p \cap X}: U_p \cap X \to N$ and $f|_{U_p \cap X}: U_p \cap X \to N$ agree on $U_p \cap X$ : $g|_{U_p \cap X} = f|_{U_p \cap X}$ . the inverse of such $f$ , $f^{-1}$ , is smooth in this sense : For each $q=F(p) \in N$ , with $p \in X$ , there exists a neighborhood $V_q$ of $q$ in $N$ and a smooth map $h: V_q \to M$ such that the restrictions $h|_{V_q \cap N = V_q}: V_q \to M$ and $f^{-1}|_{V_q}: V_q \to X$ agree on $V_q$ : $h|_{V_q} = f^{-1}|_{V_q}$ . So far I've thought of extending $h$ to $\tilde h: N \to M$ (in whatever extension possible given $h$ might not have compact support), of $\tilde h(V_q)=h(V_q)=f^{-1}(V_q)$ possibly being a subset of $X$ or something and of this . I don't know if the above counts as effort towards answering the question, but if the above doesn't, then may you just please provide a link proving or providing a counterexample and then I'll just work out the details myself (I would think of the justification or counterargument after I know what the answer is)? Context: Are immersed submanifolds something like local manifolds the same way manifolds are locally Euclidean?","['diffeomorphism', 'manifolds', 'general-topology', 'differential-topology', 'differential-geometry']"
3309401,Direct image of structure sheaf under blow-up along non-singular subvariety,"I'm trying to prove the following statement: Theorem A Let $X$ be a non-singular variety over a field $k$ and let $Y \subset X$ be a smooth subvariety. Consider the blow-up $f : \widetilde X = Bl_Y(X) \to X$ . Then for $i > 0$ : $$R^i f_* \mathcal O_{\widetilde X} = 0.$$ This is mentioned for example in Hironaka, Resolution of Singularities of an Algebraic Variety Over a Field of Characteristic Zero I, p. 153 without a reference or a proof. My attempt (following the proof of Proposition V.3.4 in Hartshorne's Algebraic Geometry): let $\mathcal F^i := R^i f_* \mathcal O_{\widetilde X}$ and let $y$ be the generic point of $Y$ . Then the support of $\mathcal F^i$ is contained in $Y$ and using the Formal Functions Theorem, we get: $$ \mathcal F^i_y = \lim_{\leftarrow} H^i(E_n, \mathcal O_{E_n}),$$ where $E_1 = E = f^{-1}(Y)$ and $E_n$ is given by the ideal sheaf $\mathcal J^n$ (where $\mathcal J$ is the ideal sheaf of $E$ in $\widetilde X$ ). Thus the above statement should be equivalent to: $$ H^i(E_n, \mathcal O_{E_n}) = 0 \qquad \text{for all } i, n \ge 1.$$ Also, we have an exact sequence: $$ 0 \to \mathcal J^n/\mathcal J^{n + 1} = \mathcal O_E(n) \to \mathcal O_{E_{n+1}} \to \mathcal O_{E_{n}} \to 0 \qquad (*)$$ Thus, it seems to me that the Theorem A is equivalent to the statement that $$H^i(E, \mathcal O_{E}(n)) = 0 \qquad \text{for all } i, n > 0. $$ On the other hand, $E = \mathbb P(\mathcal I/\mathcal I^2)$ is a projective bundle over $Y$ (where $\mathcal I$ is the ideal sheaf of $Y$ in $X$ ). Thus $$ R^i g_* \mathcal O_E (d) = 0 $$ for $i, d > 0$ (where $g = f|_E : E \to Y$ ) - see e.g. Stacks . Therefore by Leray spectral sequence we obtain: $$ H^i(E, \mathcal O_{E}(n)) = H^i(Y, g_* \mathcal O_{E}(n)) 
= H^i(Y, S^n(\mathcal I/\mathcal I^2)).  $$ The right hand side seems to be non-zero in general. Question: where's the mistake? How to fix it? Alternatively, what is a reference for the proof of Theorem A?","['algebraic-geometry', 'blowup']"
3309403,What is the probability that after this process the content in two bags remains unchanged?,"Each of Alice and Bob has an identical bag containing 6 balls numbered 1, 2, 3, 4, 5, and 6. Alice randomly selects one ball from her bag and places it in Bob’s bag, then Bob randomly selects one ball from his bag and places it in Alice’s bag. What is the probability that after this process the content in two bags remains unchanged? I would have thought that the probability would be $\frac{1}{6} + \frac{2}{7}$ as Alice picks 1 out of balls, now Bob has 7 balls and 2 contain the same number. Or should I have multiplied them in this scenario? The balls are indistinguishable.",['probability']
3309410,Confusion over Motivating Example for a $\sigma$ Algebra,"I was attending a Probability theory lecture and the professor introduced to the Algebra of sets. His definition was the standard definition as seen in many texts and it read like this, Let $\Omega$ be a sample space. Define an algebra $\mathcal{A}$ over $\Omega$ as the collection of subsets of $\Omega$ such that $\Omega \in \mathcal{A}$ If $A \in \mathcal{A} \implies A^c \in \mathcal{A}$ If $A_i \in \mathcal{A}$ for $i=1,2,\dots,n$ , then $\bigcup\limits_{i=1}^n A_i \in \mathcal{A}$ He then said the algebra even though it captures or gives a structure for the study of random experiments most times there will be events which is of interest and won't fit into an Algebra and then gives an examples as follows. Consider the infinite coin toss experiment until we see the first head. For which $\Omega = \{H,TH,TTH,TTTTH, \dots \}$ . Now suppose we are interested in the ""event"" that the first head occurs in an even toss, i.e, we look at the event $A = \{TH,TTTH,TTTTTH, \dots \}$ . Then he said such a kind of ""event cannot be captured under an algebra structure- because we just have finite unions and intersections"" I don't understand clearly why this example motivates us to see an $\sigma$ -algebra, allowing closure under countably infinite unions. Any help would be deeply appreciated.","['measure-theory', 'probability-theory']"
3309421,Writing a rule for a function: NOTATION,"Consider the function $f$ such that $a \in A$ , $b \in B$ : $f : {A} \to {B} \quad$ where $A, B \subset \mathbb{R}$ . When writing down $f(a)=b$ , how may we define what a rule is for $f$ ? I know we may write $f(a)=b=a^2$ but I feel like I get confused when describing to someone that $a^2$ is the image of $a$ and also how $a$ is being mapped to $b$ . Can someone please help me clarify this or does it sound like I have the right idea?","['notation', 'functions']"
3309426,"Are these two explicitly given $10 \times 10$ matrices similar over the integers? Or equivalently, are they shift equivalent over $\mathbb{Z}$?","Are the following two $10 \times 10$ matrices $$A = \left[ \begin {array}{cccccccccc} 0&1&0&0&1&0&0&0&0&0
\\ 0&0&1&0&0&1&0&0&0&0\\ 1&0&0&1&0
&0&1&0&0&0\\ 1&0&0&0&0&0&0&1&0&0
\\ 0&0&0&0&0&1&0&0&0&0\\ 0&0&0&0&0
&0&1&0&0&0\\ 0&0&0&0&1&0&0&1&0&0
\\ 0&0&0&0&1&0&0&0&1&0\\ 0&0&0&0&0
&0&0&1&1&1\\ 0&0&0&0&0&0&0&0&1&1\end {array}
 \right] $$ and $$ B = \left[ \begin {array}{cccccccccc} 1&1&0&0&0&0&0&0&0&0
\\ 1&1&1&0&0&0&0&0&0&0\\ 0&1&0&0&0
&1&5&1&5&5\\ 0&0&0&0&1&0&6&10&5&1
\\ 0&0&1&0&0&1&6&6&10&5\\ 0&0&0&1&0
&0&10&5&1&5\\ 0&0&0&0&0&0&0&1&0&0
\\ 0&0&0&0&0&0&0&0&1&0\\ 0&0&0&0&0
&0&1&0&0&1\\ 0&0&0&0&0&0&1&0&0&0\end {array}
 \right]
$$ similar over $\mathbb{Z}$ ? I believe the answer to be no, but if the answer is yes, then these matrices form a counterexample to something (to be described below) I've been trying to prove recently in my PhD project. Let me give some context to my question by first indicating what I've tried and found out so far, and then giving some motivation and background after that. What I've tried: Both matrices have characteristic polynomial $${t}^{10}-2\,{t}^{9}-{t}^{8}-{t}^{7}+2\,{t}^{6}+6\,{t}^{5}-{t}^{3}-4\,{
t}^{2}-2\,t+1,$$ and they both have the same Frobenius/Rational canonical form, this being just the companion matrix of said polynomial: $$ F = \left[ \begin {array}{cccccccccc} 0&0&0&0&0&0&0&0&0&-1
\\ 1&0&0&0&0&0&0&0&0&2\\ 0&1&0&0&0
&0&0&0&0&4\\ 0&0&1&0&0&0&0&0&0&1
\\ 0&0&0&1&0&0&0&0&0&0\\ 0&0&0&0&1
&0&0&0&0&-6\\ 0&0&0&0&0&1&0&0&0&-2
\\ 0&0&0&0&0&0&1&0&0&1\\ 0&0&0&0&0
&0&0&1&0&1\\ 0&0&0&0&0&0&0&0&1&2\end {array}
 \right] $$ So we know that $A$ and $B$ are similar over $\mathbb{Q}$ (as they are both similar to $F$ ). I have actually been able to show that $A$ is similar to $F$ also over $\mathbb{Z}$ . This was done using a ""lucky"" computer search: I ran through all size $10$ vectors $\vec v$ with entries in $\{-1,0,1\}$ and checked whether the matrix with columns $\vec v$ , $A \vec v$ , $\ldots$ , $A^9 \vec v$ had determinant $\pm 1$ . This was the case for a handful of $\vec v$ 's, so these matrices witness the integral similarity of $A$ and $F$ . The same search for $B$ did not yield any positive results though. I would like to try a search on a larger set of vectors, but this would require more computing power than I posess currently (being home for the summer holidays). If the matrices are not similar, this method will of course never give the answer though... From the above we nevertheless have the following equivalent question: Are the matrices $B$ and $F$ above similar over $\mathbb{Z}$ ? Another idea: Another idea I've had is to consider the matrices $A$ and $B$ (or $F$ and $B$ ) as matrices over the finite field $\mathbb{F}_p$ for various choices of the prime $p$ . As this can be decided using the Frobenius form or Smith normal form. If there is a $p$ for which the matrices are not similar over $\mathbb{F}_p$ , then they are not similar over $\mathbb{Z}$ either.  So that would settle it. However, if for some large $p$ , $A$ and $B$ are similar over $\mathbb{F}_p$ , I hope that a the transformation matrix witnessing this similarity also could work over $\mathbb{Z}$ , because the $p$ is so large. This is just speculation though. My problem is that I'm not sure which software I could use to perform these finite field calculations effeciently. Any feedback on this is greatly appreciated! Motivation: This part will be a bit brief/cryptic, in order to keep it short, so please just let me know if I should give more details on something. This question has come out of my recent attempts to prove that shift equivalence implies flow equivalence for (certain) non-negative integral matrices and their associated shifts of finite type. The dynamical systems called shifts of finite type , which are associated to a non-negative integer matrix, are described in the introductory textbook ""An Introduction to Symbolic Dynamics and Coding"" by Douglas Lind and Brian Marcus. Shift equivalence of matrices and flow equivalence of shifts of finite type are also explained in the book (starting on pages 233 and 453). For non-negative integer matrices which are irreducible (meaning that for each $i,j$ $A^n(i,j) > 0$ for some $n$ ), it is known that shift equivalence implies flow equivalence. This is because in the irreducible case there is a complete algebraic invariant in terms of the defining matrix which determines flow equivalence. And it is not hard to see that shift equivalence over $\mathbb{Z}$ (which is weaker than mere shift equivalence) preserves the invariant. For reducible matrices, however, the invariant is a lot more complicated. So in this case it is an open problem whether shift equivalence (over $\mathbb{Z}$ ) implies flow equivalence. It is not clear at all whether shift equivalence preserves the invariant. There is a recent paper in another field which hints at this potentially being true, so that is why I've spent some time trying to prove it. After failing to prove it I started searching for counterexamples instead. There are counterexamples if one allows the matrices to have ""cyclic components"", meaning that one irreducible component is essentially a permutaton matrix, but this makes the shifts of finite type a bit ""degenerate"" in some sense, so I want to avoid that. The way shift equivalence over $\mathbb{Z}$ connects with similarity over $\mathbb{Z}$ is that this is actually equivalent, for matrices with determinant $\pm 1$ (i.e. those which are invertible over $\mathbb{Z}$ ). This is the case for $A$ and $B$ . The matrices $A$ and $B$ are constructed to not be flow equivalent. They both have a similar block structure which is a by-product of the construction (edit: see the comments for the block structure), but I have chosen to not get into that. Edit: Just to be clear, what I ""hope for"" is that the matrices are similar . For that would settle my underlying research question. If they turn out to not be similar (or if we cannot tell), then what I would like is to either try to construct some other matrices that might be similar (while still not being flow equivalent) based on the feedback here, or to try find some ""underlying reason"" why they can never be similar. For that might help me prove that shift equivalence implies flow equivalence.","['algebraic-number-theory', 'matrices', 'computational-mathematics', 'linear-algebra', 'dynamical-systems']"
3309445,Condition number and $LU$ decomposition,"Consider $A: n \times n$ non-singular and the factors $L$ and $U$ of $A$ obtained with partial pivoting strategy, such as: $PA = LU$ .
  Proof that $$\kappa_{\infty}(A) \geq \dfrac{||A||_{\infty}}{\min_{j}|u_{jj}|}.$$ The condition number $\kappa_{\infty}(A)$ is defined by $\kappa_{\infty}(A)=||A||_{\infty}||A^{-1}||_{\infty}$ . All I could show was that $\kappa_{\infty}(A) \geq \dfrac{||A||_{\infty}}{n ||U||_{\infty}}$ . But I can't get the "" $n$ "" from the denominator. This question seems to me to have some trick that I can´t get it.
I discussed with some colleagues and we were thinking that this question is wrong. But still, we don´t know how to prove it.","['condition-number', 'matrices', 'lu-decomposition', 'numerical-linear-algebra', 'matrix-decomposition']"
3309464,About the sequence $n^{\frac{1}{n}} -1$,"I am having trouble with this multiple-select mcq question: The value of $n^{\frac{1}{n}} -1$ tends to $0$ as $n\to \infty$ , is greater than $\frac{\log n}{n}$ for all $n\geq 3$ , is greater than $\log n$ for all $n\geq 3$ , is greater than $\frac{1}{\sqrt{n}}$ for all $n\geq 3$ . I know that option 1 is correct, because $\lim_{n\to \infty}n^{\frac{1}{n}} =1$ . Also, $$n^{\frac{1}{n}} -1=-1+ e^{\frac{\log n}{n}} =\frac{\log n}{n} + \frac{(\log n)^2}{n^2}/2! +...\geq \frac{\log n}{n},$$ meaning option 2 is also correct. But I don't see how to prove or disprove the other two options.
Any help would be appreciated!","['limits', 'sequences-and-series', 'real-analysis']"
3309470,Topology Generated by Homomorphisms into a Topological Group,"Let $G$ be a group, and let $\Gamma$ be a topological group. Give $G$ the coarsest (initial?) topology with respect to which all homomorphisms from $G$ to $\Gamma$ are continuous. My question is, Does such a topology turn $G$ into a topological group? If not, what if there were an embedding (injective homomorphism) from $G$ to $\Gamma$ ? What if $\Gamma$ were a Lie group? EDIT Not sure how relevant this is, but here is something to consider. Note that $\Bbb{Q}$ and $\Bbb{R}$ are topological groups with their standard topologies. A simple group theory exercise shows that all homomorphisms from $\Bbb{Q}$ to $\Bbb{R}$ are of the form $q \mapsto mq$ for some $m \in \Bbb{R}$ . All of these homomorphisms are continuous, but none of them are open maps (right?). I don't think this resolves the question in any way, but it suddenly occurred to me, so I included it.","['group-homomorphism', 'group-theory', 'topological-groups', 'lie-groups']"
3309503,Is the ring of global functions on an integral scheme integral?,Is the ring of global functions on an integral scheme an integral domain (if the scheme is affine then it is try by definition so we are interested in non-affine schemes)? It is necessarily a reduced ring ( https://stacks.math.columbia.edu/tag/01OL ) so the question is whether it is irreducible.,['algebraic-geometry']
3309523,Abel-Dirichlet improper integral test (without continuity required),"I was searching a proof of the following Abel-Dirichlet test : Theorem : If $\Phi$ is bounded and monotonic in $[𝑎,+∞)$ and tends to zero at $+\infty$ , and $\int_{a}^{x} f dt$ is bounded for $x \geq a$ , then $\int_{a}^{+\infty} f \cdot \Phi dt$ is convergent. The most similar question asked i found is the following : Dirichlet's test for convergence of improper integrals But no proofs were given, I was obviously interested in a proof that requires minimal hypothesis, in particular with $f$ that has not to be necessarly continuous. Since i can't consult the pdf of the book quoted in the link above, i was wondering wether we could discuss about the proof of the Theorem, in the easiest way, since my knowledge involves the first year of analysis\calculus at university and some notions about Null set and its consequences as the Vitali-Lebesgue Theorem.","['integration', 'measure-theory', 'lebesgue-measure', 'improper-integrals', 'real-analysis']"
3309555,"If $A, B$ and $C$ are non-empty sets, simplify $(A\cap B)\cap (B\cap C)\cap (C\cap A)$.","If we solve it by taking examples, let $A=\{1,2\}, B=\{2,3\}, C=\{3,4\}$ . Then $$A\cap B= \{2\}, \quad B\cap C= \{3\}, \quad C\cap A= \emptyset.$$ So the intersection with $\emptyset$ will always be empty. So shouldn't the answer be $\emptyset$ ? The answer given is $A\cap B\cap C$ .",['elementary-set-theory']
3309587,Domain for Knuth's up-arrow,"The wikipage for Knuth's up-arrow notation says $a \uparrow ^n b$ is defined recursively for integer $a$ and non-negative integers $b$ and $n$ like so: \begin{equation}
a\uparrow^n b =
   \begin{cases}
    a^b, & \text{if }n=1; \\
    1, & \text{if }n\ge 1\text{ and }b=0; \\
    a\uparrow^{n-1}(a\uparrow^{n}(b-1)), & \text{otherwise }
   \end{cases}
\end{equation} However, some negative values of $a$ will push the $b$ in further iterations into the negatives, and consequently into infinitely small $b$ - with no defined value for the expression. Is there a different definition? Is it not supposed to be defined for all integer $a$ and non-negative integers $b$ and $n$ ? Example: $-1 \uparrow^3 2 = -1 \uparrow^2 (-1 \uparrow^3 1) = -1 \uparrow^2 (-1 \uparrow^2 (-1 \uparrow^3 0)) = -1 \uparrow^2 (-1 \uparrow^2 1) = -1 \uparrow^2 -1$","['number-theory', 'hyperoperation', 'computability']"
3309598,At what values of $x$ do $e^{3x}$ and $x^3$ have parallel tangent lines?,"I was given this problem: Let $f$ be the function defined by $f(x)=e^{3x}$ , and let $g$ be the function defined by $g(x)=x^3$ . At what value of $x$ do the graphs of $f$ and $g$ have parallel tangent lines? To solve this problem, I set the derivatives of $f$ and $g$ equal to each other: $3x^2=3e^{3x}$ . Now, I just need to solve for $x$ , but for some reason I am completely at a loss of how to do that. I divided the problem by $3$ , which got me $x^2=e^{3x}$ . But, now what? I also plugged it into my calculators solver , which got me the correct answer: $-.484$ . But, I don't know how to find that answer without solver .","['calculus', 'derivatives']"
3309601,Why does abstract algebra have just binary functions? [duplicate],"This question already has answers here : Is abstract algebra (mostly?) restricted to $2$-ary operators? (4 answers) Closed 4 years ago . Are there operations/functions that take any other than 2 arguments in abstract algebra? If there are, then why are they not used or shown while teaching the topic? If there are not, then why is the subject constrained to just binary functions?","['binary-operations', 'abstract-algebra']"
3309632,Eigenvalues of tensor product of matrices : question about general properties,"I would like to refresh my mind on eigenvalues/eigenvector of tensor product of operator (for Quantum Mechanics purposes). Consider two hermitic matrices $A$ and $B$ living in Hilbert spaces of finite dimensions $N_A$ and $N_B$ respectively. I would like to know the most general properties about the eigenvalues of the operator $U=A \otimes B$ . Let's assume I know the spectrum of $A$ and $B$ : $$A |a^i\rangle = a |a^i\rangle$$ $$B |b^j\rangle = b |b^j\rangle$$ Where the indices $i$ and $j$ are here to take in account possible degeneracy in the eigenvalues. Proposition 1 A possible eigenbasis of $U$ is the set of all $|a^i\rangle \otimes |b^j\rangle$ with eigenvalues $ab$ . Proof: $|a^i\rangle \otimes |b^j\rangle$ is an eigenvector of $U$ with eigenvalue $ab$ . And as I have $N_A$ orthogonal vectors $|a^i\rangle$ and $N_B$ orthogonal vectors $|b^j\rangle$ , I have found $N_A*N_B$ orthogonal vectors that are eigenstates of $U$ . However, there might have quantum state of $U$ that are not of the form of a tensor product. Proposition 2 $U=A \otimes B$ admits one degenerated eigenvalue if and only if there exist eigenvectors of $U$ that are not a tensor product. Proof: Necessary condition: As $\{ |a^i\rangle \otimes |b^j\rangle \}$ are basis of eigenstate of $U$ , and there is a degenerated eigenvalue of $U$ , then there exist $(a,b) \neq (a',b')$ such that $|a\rangle \otimes |b\rangle$ and $|a'\rangle \otimes |b'\rangle$ are eigenstates of $U$ with the same eigenvalue. Thus $|a\rangle \otimes |b\rangle + |a'\rangle \otimes |b'\rangle$ also. Then I found an eigenstate of $U$ that is not a product state. Sufficient condition: If there is an eigenvector of $U$ that is not a tensor product, then it must be a linear combination of different $|a^i\rangle \otimes |b^j \rangle$ as they diagonalise $U$ . And if a linear combination of eigenvector is an eigenvector, then the two initial eigenvector must have the same eigenvalue. Then, $U$ has degenerated eigenvalues. Do you agree ? Especially with my second proposition (I am quite confident about the first one).","['hilbert-spaces', 'linear-algebra', 'tensor-products']"
3309637,"""Baby"" Rudin theorem 8.5: is this proof correct?","Theorem 8.5 of ""Baby"" Rudin is the following: Suppose the series $\sum a_{n} x^{n} $ and $ \sum b_{n} x^{n} $ converge in the segment $S=(-R, R)$ . Let $E$ be the set of all $x \in S$ at which $\sum_{n=0}^{\infty} a_n x^n = \sum_{n=0}^{\infty} b_n x^n $ If $E$ has a limit point in $S$ , then $a_n = b_n $ for $n=0,1,2,\cdots$ . And here is my proof, which I want to get verified. Put $c_n = a_n - b_n $ , then $f(x) = \sum c_n x^n =0$ for all $x \in E $ Let $t$ be a limit point of $E$ . Then there is a sequence in $E$ which converges to $t$ , and there exist a subsequence of it which consists of numbers bigger than $t$ or smaller than $t$ . wlog assume there is a increasing sequence $ \{ t_n \} $ which converges to $t$ and $t_n < t$ for all $n$ Then we have $f(t_n )=0$ for all $n$ . Since $f$ is continuous on $(-R, R)$ , we have $f(t)=0$ . (Note that $t \in S$ , so $f(t)$ can be defined.) Also, since $f$ is differentiable on $(-R, R)$ , by mean value theorem, there exists a sequence $s_n $ such that $t_{n} < s_n < t_{n+1} $ and $f'(s_n )=0$ for all $n$ . It is obvious that $s_n$ converges to $t$ , and since $f'$ is continuous on $(-R, R)$ too, so we have $f'(t)=0$ . We can repeat this, (rigorous proof can done by induction) to get $f^{(n)}(t)=0$ for all $n$ . Thus by Theorem 8.4 (which I state at the end of this question), $f(x)=0$ for $x \in ( t-\epsilon , t+\epsilon ) \subset (-R, R) $ , where $\epsilon >0$ is arbitrary. Now we can repeat this stuff to get $f(x) =0$ for all $x \in (-R,R)$ . So $f^{(n)}(0)=0$ for all $n$ , which gives $c_n =0$ for all $n$ . Here is Theorem 8.4, which I used above. Suppose $f(x) = \sum_{n=0}^{\infty} c_{n} x^{n} $ , the series converging in $|x|<R$ . If $-R < a < R$ , then $f$ can be expanded in a power series about the point $x=a$ which converges in $|x-a| < R-|a|$ , and $f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!} (x-a)^{n} $ Thank you for reading my question.
Any commnets will help me a lot.","['proof-verification', 'real-analysis']"
3309687,Prove that a ball with radius $r<1$ is a subset of the range of approximate identity function from a unit ball,"I am struggling with finding a proof (or counterexample) for the following Theorem: Let $x\in\mathbb{R}^n$ and $f:\mathbb{R}^n\to \mathbb{R}^n$ a continuous function. Assume $\forall x\in B_1$ , $||x-f(x)||_{\ell^2}<\varepsilon$ for some given $0\leq\varepsilon<1$ . Then $B_{1-\varepsilon}\subset f(B_1)$ , where $B_r:=\{x\in\mathbb{R}^n~|~||x||_{\ell^2}\leq r\}$ is the closed ball centered at $0$ with radius $r$ . Intuitively, it is clear that the unit ball can only compress up to an $\varepsilon$ when the function $f$ is applied. However, it is not directly clear that there are no 'holes' in the image $f(B_1)$ . Continuity of $f$ , together with local similarity intuitively takes care of this, but I don't know how to make it exact. I think the assumptions should be enough for giving the proof, but on top of them, I can say that $f$ is piecewise differentiable, $f$ is Lipschitz. There is no known point $x$ for which $||x-f(x)||=0$ AND $f(x)$ is differentiable. Also, the function $f$ is not injective. I believe it is surjective, but I'm not completely sure.","['continuity', 'general-topology', 'real-analysis']"
3309696,Extension of Continuous Mapping Theorem for Bounded in Probability,"The continuous mapping theorem states that if $X_{n} \to X$ in probability and $g$ is continuous almost surely then $g(X_{n}) \to g(X)$ in probability. In little-O notation this is $$X_{n} - X = o_{P}(1) \Rightarrow g(X_{n}) - g(X) = o_{P}(1).$$ Are there any extensions for bounded in probability ? More specifically, If $X_{n} - X = O_{P}(a_{n})$ , what are conditions on $g$ which give $g(X_{n}) - g(X) = O_{P}(a_{n})$ ? If I have a sequence of functions $g_{n}$ and $X_{n} - X = O_{P}(a_{n})$ , what are minimal conditions on $g_{n}$ to give $g_{n}(X_{n}) - g_{n}(X) = O_{P}(a_{n})$ ? This is for self study so textbook references where I can read further are appreciated.","['probability-limit-theorems', 'probability-theory', 'probability']"
3309706,Nonnegativity of solution of $u_t=\Delta u+u$,"Consider the following evolution equation $$u_t=\Delta u+u$$ in a bounded and regular open subset $\Omega$ of $\mathbb{R}^N$ , with smooth initial conditions $u_0\geq 0$ and homogeneous Dirichlet boundary conditions. It is known that this equation has a smooth global solution $u$ . My goal is to prove that the solution remains nonnegative. So I consider $w=\min(0,u)$ and its energy $E(t):=\int_\Omega w^2 dx$ . We know that \begin{align}
E(0) &= \int_\Omega w(0,x)^2 dx \\
&=\int_\Omega \min(0,u(0,x))^2 dx \\
&=\int_\Omega \min(0,u_0(x))^2 dx \\
&=0
\end{align} By differentiating $E(t)$ and using integration parts we get \begin{align}
E'(t) &= 2\int_\Omega ww_t \\
&= 2\int_\Omega wu_t \\
&= 2\int_\Omega w\Delta u+2\int_\Omega wu \\
&= -2\int_\Omega \nabla w \cdot \nabla u+2\int_\Omega w^2 \\
&= -2\int_\Omega |\nabla w|^2+2E(t) \\
&\leq-\frac{2}{c^2}\int_\Omega w^2 dx+2E(t)\\
&\leq\left(2-\frac{2}{c^2}\right)E(t),\quad \text{for almost every} \ t
\end{align} where $c$ is the Poincaré constant. 
Thus $E(t)\leq e^{\left(2-\frac{2}{c^2}\right)t}E(0)=0$ for almost every $t$ which implies that for a.e $t\geq 0$ $w(t,x)=0$ for a.e $x\in \Omega$ . But since $w=\min(0,u)$ is continuous then $w(t,x)=0$ for all $t\geq 0$ and for all $x\in \Omega.$ Therefore $u(t,x)\geq 0$ for all $t\geq 0$ and for all $x\in \Omega.$ My concerns are : 1) How can I justify the derivation under integral sign $E'(t)=2\int_\Omega ww_t$ because unlike $u$ which is smooth, $w=\min(0,u)$ has only weak time derivative $w_t=u_t \mathbb{1}_{\{u\leq0\}}.$ 2) In the end I proved that for a.e $t\geq 0$ , $\int w(t,x)^2dx=0$ , thus for a.e $t\geq 0$ : $w(t,x)=0$ for a.e $x\in \Omega$ . I then concluded by continuity of $w$ that this holds for all $t\geq 0$ and for all $x\in \Omega.$ I am not use but the space negligable sets of $\Omega$ might depend on time $t$ . Does this make my argument still valid?","['measure-theory', 'lebesgue-integral', 'lp-spaces', 'partial-differential-equations', 'weak-derivatives']"
3309717,Why is the map $\mathrm{SL}_2(\mathbb{R})/\mathrm{SO}(2) \rightarrow \mathbb{H} : A \mapsto Ai$ injective?,"First some notation: $\mathrm{SL}_2(\mathbb{R})=\left\lbrace\begin{pmatrix} a&b \\ c &d \end{pmatrix}\;\Biggm|\;a,b,c,d \in \mathbb{R} ,ad-bc=1 \right\rbrace$ $\mathrm{SO}(2)=\lbrace K\in \mathrm{SL}_2(\mathbb{R}) : A^TA=AA^T=I \rbrace$ . $\mathbb{H}$ denotes the upper half plane. The map $$\mathrm{SL}_2(\mathbb{R})/\mathrm{SO}(2) \rightarrow \mathbb{H} : A \mapsto Ai$$ is bijective. My problem is to understand why the map is injective. My idea is to take $N\in \mathrm{SO}(2)$ and consider $Mi=Ni$ . Since $\mathrm{SO}(2)$ is the stabilizer of $i$ it follows that $M=N$ . Thanks for the help.","['number-theory', 'modular-forms']"
