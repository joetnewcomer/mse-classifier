question_id,title,body,tags
4737402,What is the mean value of $|\sin x +\sin (\pi x)|$?,"I was thinking about non-periodic trigonometric functions, and came up with this question: What is the mean value of $|\sin x +\sin (\pi x)|$ ? Here is the graph of $y=|\sin x +\sin (\pi x)|$ : The mean value should be $\lim\limits_{n\to\infty} \frac{1}{n}\int_0^n |\sin x +\sin (\pi x)|dx$ , but I don't know how to evaluate this. I tried complex numbers, to no avail. Desmos and Wolfram don't do a good job with numerical investigation of this limit, but we can consider the equivalent limit $\lim\limits_{n\to\infty} f(n)$ where $f(n)=\frac{1}{n}\sum\limits_{k=1}^n |\sin (\sqrt2 k)+\sin (\pi \sqrt2 k)|$ . (I put $\sqrt2$ in front of $k$ so that that the terms do not have integer multiples of $\pi$ .) $f(10^{5})\approx1.0000113115\left(\dfrac{8}{\pi^2}\right)$ $f(10^{6})\approx1.0000003459\left(\dfrac{8}{\pi^2}\right)$ $f(10^{7})\approx1.0000001068\left(\dfrac{8}{\pi^2}\right)$ $f(10^{8})\approx1.0000000137\left(\dfrac{8}{\pi^2}\right)$ This suggests that the answer is $\dfrac{8}{\pi^2}$ . Note: In the question, if we replace $\pi$ with any other irrational number, it seems that we always get the same mean value. EDIT: Here is my attempt to generalize this question. EDIT2: Related claims: The mean value of $|(\sin x)(\sin (\pi x))|$ is $\dfrac{4}{\pi^2}$ . The mean value of $\dfrac{|\sin x|}{|\sin (\pi x)|+1}$ is $\dfrac{4}{\pi^2}$ . The mean value of $|\sin (x+\sin (\pi x))|$ is $\dfrac{2}{\pi}$ . The mean value of $\sin^2 (x+\sin (\pi x))$ is $\dfrac{1}{2}$ . The mean value of $\dfrac{1}{|\sin x+\sin (\pi x)|+2}$ is $\dfrac{4G}{\pi^2}$ , where $G$ is Catalan's constant .","['definite-integrals', 'ergodic-theory', 'real-analysis', 'limits', 'average']"
4737418,Properties of a random permutation matrix,"Given a uniformly sampled permutation matrix $\Pi\in\{0,1\}^{n\times n}$ , what can we say about the matrix $E$ where $$
\Pi = I + E,
$$ where $I$ is the identity matrix. More precisely. what can we say about the following: What is the statistical distribution of $E$ ? What is the mean of $E$ ? What is the variance of $E$ ? Thanks. Motivation: This is useful in the shuffled linear regression problem where observations are in the form of $$
y=\Pi X\beta=X\beta+EX\beta,
$$ where $y$ and $X$ are observed, $\Pi$ and $\beta$ are unknown, and the goal is to estimate $\beta$ . I was wondering if we can view $EX\beta$ like some sort of noise, akin the usual linear regression with additive noise.","['permutations', 'statistics', 'permutation-matrices', 'random-matrices', 'probability']"
4737435,When does the infinite recursive fraction integral $\int_a^b \frac{g(x_1)dx_1}{\int_a^{x_1}\frac{g(x_2)dx_2}{\int_a^{x_2}\cdots}}$ converge?,"I write problems for integration bees, and I am considering problems of the form in the title, i.e. $$\int_a^b \frac{g(x_1)dx_1}{\int_a^{x_1}\frac{g(x_2)dx_2}{\int_a^{x_2}\frac{g(x_3)dx_3}{\int_a^{x_3}\cdots}}}$$ where $g$ is of the special form $$\begin{cases}g(x) = f(x)f'(x) \\ f(a) = 0 \\ f(x)\geq 0\end{cases}$$ Or more formally, consider the sequence $$F_{n+1}(x) = \int_a^x \frac{g(t)dt}{F_n(t)} \hspace{20 pt}F_0(t) =  1$$ If $g$ is of the special form above, in the limit we get $$F'(x) = \frac{f(x)f'(x)}{F(x)} \implies F(x) = |f(x)|$$ by L'Hopital's rule. I think a necessary condition for convergence might be $f'(x) \geq 0$ as taking $f(x) = \sin x$ it looks like the sequence diverges if $b > \frac{\pi}{2}$ when $a=0$ , but I am not sure if this is a sufficient condition. This gives a solution $$\int_0^{\frac{\pi}{2}}\frac{\sin x_1\cos x_1\:dx_1}{\int_0^{x_1}\frac{\sin x_2 \cos x_2 \:dx_2}{\int_0^{x_3}\frac{\sin x_3 \cos x_3 \:dx_3}{\int_0^{x_3}\cdots}}} = 1$$ $\textbf{My Question}$ : Is $f'(x)\geq 0$ is a sufficient (or even necessary) condition for convergence? If not, what other conditions would I need on the kernel $g$ to get limit convergence? Thanks in advance for your help!","['integration', 'limits', 'definite-integrals', 'sequences-and-series']"
4737448,Real number $x$ such that $\{ x^n\}$ is constant for all $n\in S$,"The golden ratio satisfies the property that $$\{\phi^{-1}\}=\{\phi\}=\{\phi^2\} = 0.618\cdots$$ where $\{x\}$ is the fractional part of $x$ , equal to $x-\lfloor x\rfloor$ . Inspired by that, I was wondering for what subsets $S$ of $\mathbb{Z}\setminus\{0\}$ (e.g. $S=\{-1,1,2\}$ as with the golden ratio), there exist $x\in\mathbb{R}$ such that for all $n\in S$ , $\{x^n\}$ is equal (and not equal to $0$ , because otherwise, there would just be trivial integer/integer root solutions). If $|S|=2$ , then I think there must exist a solution. Write $S=\{m,n\}$ . If both elements are positive, on $[0,2^{mn})$ , $\{x^m\}$ and $\{x^n\}$ have differing number of discontinuities (I think $2^n-1$ and $2^m-1$ ), but they're both increasing from $0$ to $1$ except at those discontinuities. So there must exist some intersection point. If both elements are negative, instead of a solution to $\{x^m\}=\{x^n\}$ , you can just consider the solution to $\{x^{-m}\}=\{x^{-n}\}$ and just take the inverse of that. Finally, if one element's positive and one's negative, the graph for the negative one would be monotonically decreasing to $0$ after $x=1$ , while the graph for the positive one would be increasing (except at those discontinuities) from $0$ to $1$ , so there would be some intersection in the graphs. Obviously, what's a lot more tricky is when $|S|\ge 3$ . I'm not even sure if there's any solution with $|S|=3$ other than when $S=\{-1k,1k,2k\}$ with $k\in\mathbb{Z}$ . I did get that if $S=\{1,2\}$ , the set of solutions for $x$ is given by $$\left\{-\sqrt{m+\frac{3-\sqrt{5+4m}}{2}}:m\in\mathbb{Z}_{\ge 0}\right\}\bigcup\left\{\sqrt{m+\frac{1+\sqrt{1+4m}}{2}}:m\in\mathbb{Z}_{\ge 0}\right\}\bigcup\{0\}$$ Also, if $S$ works, then $kS=\{ks:s\in S\}$ works, where $k$ is an integer. Is there a simple way to characterize the sets $S$ that work? More specifically, is there any way to find out what sets with only three elements work? Edit: This is a small comment but might motivate looking at it through the lens of algebra. If $\{x^a\}=\{x^b\}=\{x^c\}$ with $a>b>c\ge 1$ , then there should exist an integer $m$ such that $x^a-x^b-m$ is reducible over $\mathbb{Z}[x]$ . In fact, we would need integers $m,n$ such that $\deg(\gcd(x^a-x^b-m,x^b-x^c-n))\ge 1$ .","['fractional-part', 'real-analysis']"
4737490,Derivative of cross product w.r.t. a vector,"How to compute the derivative of $\vec{a}\times\vec{b}$ w.r.t. $\vec{c}$ , all of which are 3D vectors for simplicity? Here $\vec{a}(\vec{c})$ and $\vec{b}(\vec{c})$ are both dependent on $\vec{c}$ . I know that the result should be a $3\times3$ matrix. And we cannot simply apply the chain rule as $$\frac{\partial(\vec{a}\times\vec{b})}{\partial \vec{c}} = \frac{\partial\vec{a}}{\partial\vec{c}} \times \vec{b} + \vec{a} \times \frac{\partial\vec{b}}{\partial\vec{c}}$$ as $\frac{\partial\vec{a}}{\partial\vec{c}}$ would be a matrix then. Is there any analytical expression for the result? For example, $$\frac{\partial\vec{x}}{\partial\vec{x}} = \left[\array{1&0&0\\0&1&0\\0&0&1}\right]$$","['cross-product', 'multivariable-calculus', 'calculus', 'matrix-calculus', 'derivatives']"
4737511,Prime squares of the form $a^2+b^4$,"The Friedlander–Iwaniec theorem states there are infinitely many primes of the form $p=a^2+b^4$ . I am interested in whether there are infinitely many primes satisfying $p^2=a^2+b^4$ and $a\neq p$ . I tested Friedlander-Iwaniec primes up to 50000 for this property, and found 5, 41, 1201, 8521, and 41761. I did this in the naive hope of finding some pattern that might allow me to leverage the fact that $a^2+b^2=p \Rightarrow (a^2-b^2)^2+(2ab)^2=p^2$ . However, I could not extract anything useful about these numbers in comparison to other Friedlander-Iwaniec primes to make progress. Is this known? Would the sieve methods used in proving the Friedlander-Iwaniec theorem have any bearing on this question? (I have zero knowledge of these tools, so please excuse the naivete of my question) Or is this likely completely out of reach currently, as e.g. Landau's fourth problem is?","['number-theory', 'prime-numbers', 'sieve-theory']"
4737519,Sum with Binomial Coefficients and Sine; $S=\sum_{k=0}^n \binom{n}{k} \sin(kx)$,"Sum with Binomial Coefficients Let $n ∈ ℕ₀$ and $x ∈ ℝ$ . $$S=\sum_{k=0}^n \binom{n}{k}  \sin(kx)$$ Simplify the sum to a polynomial in n. I tried to use Euler's Formula and the Binomial Theorem, similar to Tricky Sum involving Binomial Coefficients and Sine . Alas I could not fully understand the option presented. I would be grateful for an explanation or even better another way of approaching the problem.","['trigonometry', 'binomial-coefficients', 'summation']"
4737525,Question about isosceles triangle,"Problem $\Delta ABC$ is an isosceles triangle with $AC=BC$ . $BC$ is extended to $D$ such that $CD=AB$ . If $\angle ADB=30^\circ$ , Find $\angle ABD$ . My Progress First of all, by using geogebra, I discovers that $\angle ABD=20^\circ$ for $\angle ACB$ being obtuse and $\angle ABD=60^\circ$ for $\angle ACB$ being acute. I also try by extending $AC$ to $E$ such that $CE=AB$ . Then $ABED$ would be an isosceles trapezium. I am not sure whether it helps.","['euclidean-geometry', 'geometry']"
4737569,What's wrong with my solution for $2\csc^{2}x+\cot x-3=0$?,"I'm trying to solve the equation $2\csc^2 x+\cot x-3=0$ for $0 \le x \le 2\pi$ . While I know that the identity $1+\cot^2 x=\csc^2 x$ can be used to solve this problem, I attempted to solve it using an unconventional approach. Here are the steps I took: $2\csc^{2}x+\cot x-3=0$ $\frac{2}{\sin^2x}+\frac{\cos x}{\sin x}-3=0$ $2+\cos x\sin x-3\sin^2x=0,x\neq 0$ $2\pm\sqrt{1-\sin^2x}\sin x-3\sin^2x=0$ Then, I substituted $u=\sin x$ to get $10u^4-13u^2+4=0$ , which has four solutions: $u=\frac{2\sqrt{5}}{5}, \frac{\sqrt{2}}{2}, -\frac{2\sqrt{5}}{5}, -\frac{\sqrt{2}}{2}$ . This leads to eight solutions for $x:1.107,2.035,\frac{\pi}{4},\frac{3\pi}{4},\frac{7\pi}{4},\frac{5\pi}{4},5.176,4.248$ I had substituted $\sin x$ as $\sqrt{1-\cos^2x}$ and got the same 8 solutions for $x$ . However, when I checked my answer using a calculator, only $1.107,\frac{3\pi}{4},\frac{7\pi}{4},4.248$ were correct. The other four solutions gave me either $2$ or $-1$ . Can someone help me understand where I went wrong?",['trigonometry']
4737587,Probability of getting 2 cards with the same color,"You have two decks of cards: a 52 card deck (26 black, 26 red) and a 26 card deck (13 black, 13 red). You randomly draw two cards and win if both are the same color. Which deck would you prefer? What if the 26 card deck was randomly drawn from the 52 card deck? Which deck would you prefer then? The first question is straightforward. By symmetry, $$P(\text{same color}) = 2P(\text{two red}) = 2P(\text{second red}|\text{first red})P(\text{first red}).$$ In the 52 card deck, the probability is thus $2 \cdot\frac{25}{51}\frac 12 = \frac{25}{51}$ while in the 26 card deck, it is $2 \cdot \frac{12}{25}\frac 12 = \frac{12}{25}$ , and since $\frac{25}{51}> \frac{12}{25}$ the first deck has higher winning odds. For the second question, here's my approach. Let $R$ be a r.v. modelling the number of red cards in the smaller deck. $R$ follows a hypergeometric distribution: $$P(R=r) = \frac{\binom{26}{r}\binom{26}{26-r}}{\binom{52}{26}},$$ thus $$\begin{align}
P(\text{same color}) 
&= P(\text{two red})+P(\text{two black}) 
\\
&= \sum_{r=0}^{26} P(\text{second red}|\text{first red, }R=r)P(\text{first red}|R=r)P(R=r) + P(\text{second black}|\text{first black, }R=r)P(\text{first black}|R=r)P(R=r)
\\ 
&=\sum_{r=0}^{26} (\frac{r-1}{25} \frac{r}{26} + \frac{25-r}{25} \frac{r}{26})\frac{\binom{26-r}{r}) \binom{26}{26-r}}{\binom{52}{26}}
\\
&= \frac{2}{25\cdot 26} E[R(R-1)] = \frac{2}{25\cdot 26}(V[R]+E[R]^2-E[R]) = \frac{25}{51}
\end{align}
$$ This is the same probability as for the full deck ! I'm very surprised with this result, I'd like to see an intuitive explanation or a shorter proof that doesn't involve as many computations.","['conditional-probability', 'card-games', 'probability']"
4737590,Three equations with a common positive root,"If the equations $x^2+ax+12=0$ , $x^2+bx+15=0$ and $x^2+(a+b)x+36=0$ have a common positive root, then $(b-2a)$ is equal to What I tried: Let $\alpha$ be common positive root of all equation. Then $$\alpha^2+a\alpha+12=0 \tag{1}$$ $$\alpha^2+b\alpha+15=0 \tag{2}$$ $$\alpha^2+(a+b)\alpha+36=0 \tag{3}$$ From (1) and (2), we get $$(a-b)\alpha=3 \tag{4}$$ And doing (3) minus (1) and (3) minus (2), we get $$b\alpha=-24 \text{ and} \\ a\alpha=-21$$ Now I don’t understand how to find the values of $b$ and $a$ . Help me, please. Thanks.",['algebra-precalculus']
4737613,"If $f : \mathbb{R} \to \mathbb{R}$ and $f(f(x))=x^2-x+1$, then find $f(2021)+f(1971)+f(50)$","I came across this question while solving a compendium of previous year questions of regional Mathematics Olympiads. If $f : \mathbb{R} \to \mathbb{R}$ and $f(f(x))=x^2-x+1$ , then find $f(2021)+f(1971)+f(50)$ . I have proceeded till finding out: $$f(0)=f(1)=1$$ $$f(1-x)=f(x)$$ I proceeded like this: $$f(f(f(x)))=(f(x))^2-f(x)+1$$ This implies that $f(f(f(1)))=(f(1))^2-f(1)+1$ . Now $f(f(1))=1$ therefore $f(1)=(f(1))^2-f(1)+1$ which is why $f(1)=1$ . Next I proved that $f(f(1-x)=f(x)$ using the fact that $$f(f(x)=(x-\dfrac{1}{2})^2+\dfrac{3}{4}$$ Then I proved that $f(x)$ is symmetric along $\dfrac{1}{2}$ . Next we put $x=\dfrac{1}{2}-x$ in the equations $f(\dfrac{1}{2}-x)=f(\dfrac{1}{2}+x)$ since it is valid for all values of x. That is how I got $f(0)=f(1)=1$ and $f(1-x)=f(x)$ . But how do I proceed further to find the value of $f(2021)+f(1971)+f(50)$ ?","['contest-math', 'functional-equations', 'functions']"
4737616,Would this be an example of a Sidon set?,"I am a high schooler doing a project on Sidon sets. There aren't much non-convoluted examples of Sidon sets out there on the internet as far as I know so I just want to make sure I understand what they are. For example, would {1, 2, 3} be a Sidon set since each pairwise sum is different or would it not be since 1 and 2 add to 3?","['elementary-number-theory', 'combinatorics', 'discrete-mathematics']"
4737632,Maximum of $xy + 2 yz$ subject to $x^2 + y^2 + z^2 = 36$,"I want to find the maximum of $ f(x, y, z) = xy + 2 y z $ subject to $ x^2 + y^2 + z^2 = 36 $ My Approach: The most direct way is to parameterize $(x, y, z)$ which is easy in this case because $(x,y,z)$ lie on a sphere centered at the origin with radius $6$ .  Therefore, using spherical parameterization, $ (x, y, z) = 6 ( \sin \theta \cos \phi, \sin \theta \sin \phi, \cos \theta ) $ Hence our function becomes a function of $\theta$ and $\phi$ $f(\theta, \phi) = 36 ( \sin^2 \theta \cos \phi \sin \phi + 2 \sin \theta \cos \theta \sin \phi ) $ Using $\sin^2 \theta = \dfrac{1}{2} (1 - \cos(2 \theta) ) $ and $ 2 \sin \theta \cos \theta = \sin(2 \theta) $ The function now becomes, $ f(\theta, \phi) = 9 \bigg( (1 - \cos(2 \theta)) \sin(2 \phi) + 4 \sin(2 \theta ) \sin \phi \bigg) $ Maximizing over $\theta$ first $ f_1(\phi) = 9 \sqrt{ (1 - \cos(2 \phi))^2 + 16 \sin^2(\phi) } $ Using the identities mentioned above this simplifies to $ f_1(\phi) = 9 \sqrt{ (1 - \cos(2 \phi) )^2 + 8 (1 - \cos(2 \phi) ) } $ Let $u = \cos(2 \phi) $ , then basically we want to maximize $ g(u) = u^2 - 10 u + 9 $ over $[-1, 1]$ .  It is easy to find that the maximum of $g$ is at $u = -1$ and the maximum is $g_{Max} = 20 $ Thus the maximum of our function is $ f_{Max} = 9  \sqrt{ 20 } = 18 \sqrt{5} \approx 40.25 $ Another method is one that requires finding the eigenvalues of the matrix $ Q = \begin{bmatrix} 0 && \dfrac{1}{2} && 0 \\ \dfrac{1}{2} && 0 && 1 \\ 0 && 1 && 0 \end{bmatrix} $ The characteristic polynomial is $ \lambda ( \lambda^2 - \dfrac{5}{4} ) = 0 $ Thus $\lambda_{Max} = \sqrt{\dfrac{5}{4}} = \dfrac{1}{2} \sqrt{5} $ Multiplying this by $36$ gives the answer as $ 18 \sqrt{5} $ . I look forward to alternative solutions, or comments on my solutions. Thank you all.","['calculus', 'solution-verification', 'quadrics', 'optimization', 'trigonometry']"
4737654,Proving composite function properties,"A continuous function $f: [-1, 1] \to R$ satisfies that $$
  f(2x^2 - 1) = 2xf(x)
$$ Furthermore, another function defined $g: G \to R$ with $G = \{t \in R: t \ne nπ, n \in Z\}$ as $$
  g(t) = \dfrac{f(\cos{t})}{\sin{t}}
$$ With these two functions provided, I would like to know f(-1) and f(1) Show that function $g$ is an odd function For every $t \in G$ , show that $g(t) = g(t/2)$ I am trying to figure out what is the main point of such a question, and as far as I know about functions, I tried the first and second ones as follows 1. $$
    \begin{align*}
        & 2x^2 -1 = -1, \quad x = 0 \\\
        & f(-1) = 2 \cdot 0 \cdot f(-1) = 0
    \end{align*}
$$ $$
    \begin{align*}
        & 2x^2 -1 = 1, \quad x = \pm1 \\\
        & f(1) = 2 \cdot f(1) = -2 \cdot f(1), \quad f(1) = 0
    \end{align*}
$$ 2. $$
    \begin{align*}
        & g(-t) = \dfrac{f(\cos(-t))}{\sin(-t)} = -\dfrac{f(\cos(t))}{\sin(t)} \\
        & 2x^2 - 1 = \cos{x} (?)
    \end{align*}
$$ Actually, I think I do not fully understand this question, and I think what I did is probably nonsense. What I want to know is how can I solve them, and what concepts are these questions asking, it looks like this is a composite function, yet I am not sure where to start or to delve into related concepts.","['functional-equations', 'calculus', 'functions']"
4737770,Tensor Product Of $L^p$ Spaces Is Dense In The Product $L^p$ Space,"I want to prove the following: Proposition: Let $(X,\mathcal{A},\mu)$ and $(Y, \mathcal{B}, \nu)$ be $\sigma$ -finite measure spaces and let $(X\times Y , \mathcal{A} \otimes  \mathcal{B} , \mathcal{\mu}\otimes \mathcal{\nu})$ be the product measure space. Further let $p \in [1, \infty) $ . Then $$\overline{L^p(X) \otimes L^p(Y)} =  L^p(X \times Y), $$ where the closure is with respect to the norm in $L^p(X \times Y)$ . Here $L^p(X) \otimes L^p(Y)$ is identified with a subspace of $L^p(X \times Y)$ through the natural embedding $f \otimes g  \mapsto \big( (x,y) \mapsto f(x) g(y) \big) $ . I have proven the statement in the special case when $\mu$ and $\nu$ are finite (not $\sigma$ -finite) and my question is: Is my proof of the finite measure case correct and if it is, how can i adapt it to the case where the measures are $\sigma$ -finite? My proof (of the finite case): It suffices to show that $\chi_C \in \overline{L^p(X) \otimes L^p(Y)} $ for every $C \in \mathcal{A} \otimes \mathcal{B}$ with $\mu \otimes \nu (C) < \infty$ . Here $\chi_C$ denotes the indicator function of the set $C$ . This is because the linear combinations of such indicators are dense in $L^p(X \times Y)$ . Now assume that $\mu$ and $\nu$ are finite (and hence also $\mu \otimes \nu$ ), then we can use the following theorem, which is proven here : Theorem: Let $(X,\mathcal B,\mu)$ be a finite measure space. Let $\mathcal A\subset \mathcal B$ be an algebra generating $\cal B$ .
Then for all $B\in\cal B$ and $\varepsilon>0$ , we can find $A\in\cal A$ such that $$\mu(A\Delta B)<\varepsilon.$$ Here $\Delta$ is the symmetric difference: $A \Delta B = (A \setminus B ) \cup (B\setminus A) = (A \cup B) \setminus (A \cap B)$ . To use the theorem define $$\mathcal{E}_0 =  \{ A \times B : A \in \mathcal{A}, B \in \mathcal{B}  \}$$ and $$\mathcal{E} = \big \{ \bigcup_{i=1}^n C_i : C_i \in \mathcal{E}_0 , n \in \mathbb{N}  \big \}.$$ Then $\mathcal{E}$ is an algebra that generates $\mathcal{A} \otimes \mathcal{B}$ .
Clearly the indicator functions of all elements of $\mathcal{E}_0 $ are in $L^p(X) \otimes L^p(Y)$ , because they have product form and finite measure.
The indicator function $\chi_E$ of every member $E$ of $ \mathcal{E}$ is also in $L^p(X) \otimes L^p(Y)$ , because $E$ can be written as a finite and disjoint union of sets in $\mathcal{E}_0$ and then $\chi_E$ is just the sum of the indicator functions on these rectangle sets. Now let $ C \in \mathcal{A} \otimes \mathcal{B}$ . Then by the theorem there exist for every $\varepsilon >0$ an $E \in \mathcal{E}$ so that $ \mu \otimes \nu (C \Delta E )< \varepsilon $ and therefore $$ \| \chi_C - \chi_E \|= \| \chi_{C \Delta E} \| = \big( \mu \otimes \nu (C \Delta E ) \big)^{1/p} < \varepsilon^{1/p},$$ which concludes the proof. My idea to extend the proof to the $\sigma$ -finite case is that the $\sigma$ -finite case can perhaps be reduced to the finite case by restricting the product measure space in some suitable way to make it finite. But i could not come up with a suitable restriction.","['tensor-products', 'measure-theory', 'solution-verification', 'functional-analysis']"
4737777,Longest 'increasing' path inside a square,"Say you have a function $f: [0,1] \to [0,1]$ such that: $f(0)=0$ $f(1)=1$ Let's just say that $f$ is everywhere differentiable (not really necessary) $f$ is increasing i.e order preserving Now what I'm trying to show is that the 'length' of such a function (i.e of it's graph) is always longer than $\sqrt{2}$ and always shorter than $2$ .
The lower bound is pretty straightforward (shortest path is the diagonal of the unit square).
Now I'm struggling with the upper bound. I'm convinced it should be trivial, but cannot for the life of me find a proof. Would greatly appreciate any help on the matter.
Cheers!","['inequality', 'geometry', 'real-analysis']"
4737804,"How to prove $\sum_{n=0}^{\infty}a_n \cos{(nx)}$ does not uniformly converge in $\left( 0,2\pi \right)$ with the conditions below?","Let $a_n$ be a sequence of real numbers such that: $a_n \ge 0$ $\forall n\in \mathbb{N}: a_n \ge a_{n+1}$ $\lim_{n \to \infty} a_n =0$ $\sum_{n=0}^{\infty } a_n =\infty $ Consider the sum $\sum_{n=0}^{\infty}a_n \cos{(nx)}$ . Prove: $\sum_{n=0}^{\infty}a_n \cos{(nx)}$ uniformly converges in $\left[ \frac{\pi}{2},\pi \right]$ $\sum_{n=0}^{\infty}a_n \cos{(nx)}$ does not uniformly converge in $\left( 0,2\pi \right)$ I only need help with 2 (I already solved 1). I'm trying to solve it for almost a day now. I'd appreciate if you could explain how to solve it. Thanks.","['analysis', 'real-analysis', 'uniform-convergence', 'sequences-and-series', 'convergence-divergence']"
4737806,"Does this family of sequences always reach $(1, 1)$ for any starting value?","The sequence is defined as follows: $$a(1) = 1$$ $$a(2) = k$$ $$a(n + 1) = \frac{a(n) + a(n - 1)}{2^{v}}$$ Basically, you add the last two numbers and divide it by $2$ until it's an odd number. For example, when $k = 11$ , you start with $$(1, 11)$$ $11 + 1 = 12$ , now you divide $12$ by $2$ until it's an odd number, $\frac{12}{2} = 6, \frac{6}{2} = 3$ , so the next term is $3$ . $$(1, 11, 3)$$ $3 + 11 = 14$ , $\frac{14}{2} = 7$ $$(1, 11, 3, 7)$$ $7 + 3 = 10$ , $\frac{10}{2} = 5$ $$(1, 11, 3, 7, 5)$$ $5 + 7 = 12$ $\frac{12}{2} = 6$ , $\frac{6}{2} = 3$ $$(1, 11, 3, 7, 5, 3)$$ $3 + 5 = 8$ , $\frac{8}{2} = 4$ , $\frac{4}{2} = 2$ , $\frac{2}{2} = 1$ $$(1, 11, 3, 7, 5, 3, 1)$$ $1 + 3 = 4$ , $\frac{4}{2} = 2$ , $\frac{2}{2} = 1$ $$(1, 11, 3, 7, 5, 3, 1, 1)$$ once you reach $(1, 1)$ the next terms will all be just 1's, if you try other values of $k$ the same happens, here's a table of the first few: 1: (1, 1, ...)
2: (1, 2, 3, 5, 1, 3, 1, 1, ...)
3: (1, 3, 1, 1, ...)
4: (1, 4, 5, 9, 7, 1, 1, ...)
5: (1, 5, 3, 1, 1, ...)
6: (1, 6, 7, 13, 5, 9, 7, 1, 1, ...)
7: (1, 7, 1, 1, ...)
8: (1, 8, 9, 17, 13, 15, 7, 11, 9, 5, 7, 3, 5, 1, 3, 1, 1, ...)
9: (1, 9, 5, 7, 3, 5, 1, 3, 1, 1, ...)
10: (1, 10, 11, 21, 1, 11, 3, 7, 5, 3, 1, 1, ...)
11: (1, 11, 3, 7, 5, 3, 1, 1, ...)
12: (1, 12, 13, 25, 19, 11, 15, 13, 7, 5, 3, 1, 1, ...)
13: (1, 13, 7, 5, 3, 1, 1, ...)
14: (1, 14, 15, 29, 11, 5, 1, 3, 1, 1, ...)
15: (1, 15, 1, 1, ...)
16: (1, 16, 17, 33, 25, 29, 27, 7, 17, 3, 5, 1, 3, 1, 1, ...)
17: (1, 17, 9, 13, 11, 3, 7, 5, 3, 1, 1, ...)
18: (1, 18, 19, 37, 7, 11, 9, 5, 7, 3, 5, 1, 3, 1, 1, ...)
19: (1, 19, 5, 3, 1, 1, ...)
20: (1, 20, 21, 41, 31, 9, 5, 7, 3, 5, 1, 3, 1, 1, ...)
21: (1, 21, 11, 1, 3, 1, 1, ...)
22: (1, 22, 23, 45, 17, 31, 3, 17, 5, 11, 1, 3, 1, 1, ...)
23: (1, 23, 3, 13, 1, 7, 1, 1, ...)
24: (1, 24, 25, 49, 37, 43, 5, 3, 1, 1, ...)
25: (1, 25, 13, 19, 1, 5, 3, 1, 1, ...)
26: (1, 26, 27, 53, 5, 29, 17, 23, 5, 7, 3, 5, 1, 3, 1, 1, ...)
27: (1, 27, 7, 17, 3, 5, 1, 3, 1, 1, ...)
28: (1, 28, 29, 57, 43, 25, 17, 21, 19, 5, 3, 1, 1, ...)
29: (1, 29, 15, 11, 13, 3, 1, 1, ...)
30: (1, 30, 31, 61, 23, 21, 11, 1, 3, 1, 1, ...)
31: (1, 31, 1, 1, ...)
32: (1, 32, 33, 65, 49, 57, 53, 55, 27, 41, 17, 29, 23, 13, 9, 11, 5, 1, 3, 1, 1, ...)
33: (1, 33, 17, 25, 21, 23, 11, 17, 7, 3, 5, 1, 3, 1, 1, ...)
34: (1, 34, 35, 69, 13, 41, 27, 17, 11, 7, 9, 1, 5, 3, 1, 1, ...)
35: (1, 35, 9, 11, 5, 1, 3, 1, 1, ...)
36: (1, 36, 37, 73, 55, 1, 7, 1, 1, ...)
37: (1, 37, 19, 7, 13, 5, 9, 7, 1, 1, ...)
38: (1, 38, 39, 77, 29, 53, 41, 47, 11, 29, 5, 17, 11, 7, 9, 1, 5, 3, 1, 1, ...)
39: (1, 39, 5, 11, 1, 3, 1, 1, ...)
40: (1, 40, 41, 81, 61, 71, 33, 13, 23, 9, 1, 5, 3, 1, 1, ...)
41: (1, 41, 21, 31, 13, 11, 3, 7, 5, 3, 1, 1, ...)
42: (1, 42, 43, 85, 1, 43, 11, 27, 19, 23, 21, 11, 1, 3, 1, 1, ...)
43: (1, 43, 11, 27, 19, 23, 21, 11, 1, 3, 1, 1, ...)
44: (1, 44, 45, 89, 67, 39, 53, 23, 19, 21, 5, 13, 9, 11, 5, 1, 3, 1, 1, ...)
45: (1, 45, 23, 17, 5, 11, 1, 3, 1, 1, ...)
46: (1, 46, 47, 93, 35, 1, 9, 5, 7, 3, 5, 1, 3, 1, 1, ...)
47: (1, 47, 3, 25, 7, 1, 1, ...)
48: (1, 48, 49, 97, 73, 85, 79, 41, 15, 7, 11, 9, 5, 7, 3, 5, 1, 3, 1, 1, ...)
49: (1, 49, 25, 37, 31, 17, 3, 5, 1, 3, 1, 1, ...)
50: (1, 50, 51, 101, 19, 15, 17, 1, 9, 5, 7, 3, 5, 1, 3, 1, 1, ...)
51: (1, 51, 13, 1, 7, 1, 1, ...)
52: (1, 52, 53, 105, 79, 23, 51, 37, 11, 3, 7, 5, 3, 1, 1, ...)
53: (1, 53, 27, 5, 1, 3, 1, 1, ...)
54: (1, 54, 55, 109, 41, 75, 29, 13, 21, 17, 19, 9, 7, 1, 1, ...)
55: (1, 55, 7, 31, 19, 25, 11, 9, 5, 7, 3, 5, 1, 3, 1, 1, ...)
56: (1, 56, 57, 113, 85, 99, 23, 61, 21, 41, 31, 9, 5, 7, 3, 5, 1, 3, 1, 1, ...)
57: (1, 57, 29, 43, 9, 13, 11, 3, 7, 5, 3, 1, 1, ...)
58: (1, 58, 59, 117, 11, 1, 3, 1, 1, ...)
59: (1, 59, 15, 37, 13, 25, 19, 11, 15, 13, 7, 5, 3, 1, 1, ...)
60: (1, 60, 61, 121, 91, 53, 9, 31, 5, 9, 7, 1, 1, ...)
61: (1, 61, 31, 23, 27, 25, 13, 19, 1, 5, 3, 1, 1, ...)
62: (1, 62, 63, 125, 47, 43, 45, 11, 7, 9, 1, 5, 3, 1, 1, ...)
63: (1, 63, 1, 1, ...)
64: (1, 64, 65, 129, 97, 113, 105, 109, 107, 27, 67, 47, 57, 13, 35, 3, 19, 11, 15, 13, 7, 5, 3, 1, 1, ...)
65: (1, 65, 33, 49, 41, 45, 43, 11, 27, 19, 23, 21, 11, 1, 3, 1, 1, ...)
66: (1, 66, 67, 133, 25, 79, 13, 23, 9, 1, 5, 3, 1, 1, ...)
67: (1, 67, 17, 21, 19, 5, 3, 1, 1, ...)
68: (1, 68, 69, 137, 103, 15, 59, 37, 3, 5, 1, 3, 1, 1, ...)
69: (1, 69, 35, 13, 3, 1, 1, ...)
70: (1, 70, 71, 141, 53, 97, 75, 43, 59, 51, 55, 53, 27, 5, 1, 3, 1, 1, ...)
71: (1, 71, 9, 5, 7, 3, 5, 1, 3, 1, 1, ...)
72: (1, 72, 73, 145, 109, 127, 59, 93, 19, 7, 13, 5, 9, 7, 1, 1, ...)
73: (1, 73, 37, 55, 23, 39, 31, 35, 33, 17, 25, 21, 23, 11, 17, 7, 3, 5, 1, 3, 1, 1, ...)
74: (1, 74, 75, 149, 7, 39, 23, 31, 27, 29, 7, 9, 1, 5, 3, 1, 1, ...)
75: (1, 75, 19, 47, 33, 5, 19, 3, 11, 7, 9, 1, 5, 3, 1, 1, ...)
76: (1, 76, 77, 153, 115, 67, 91, 79, 85, 41, 63, 13, 19, 1, 5, 3, 1, 1, ...)
77: (1, 77, 39, 29, 17, 23, 5, 7, 3, 5, 1, 3, 1, 1, ...)
78: (1, 78, 79, 157, 59, 27, 43, 35, 39, 37, 19, 7, 13, 5, 9, 7, 1, 1, ...)
79: (1, 79, 5, 21, 13, 17, 15, 1, 1, ...)
80: (1, 80, 81, 161, 121, 141, 131, 17, 37, 27, 1, 7, 1, 1, ...)
81: (1, 81, 41, 61, 51, 7, 29, 9, 19, 7, 13, 5, 9, 7, 1, 1, ...)
82: (1, 82, 83, 165, 31, 49, 5, 27, 1, 7, 1, 1, ...)
83: (1, 83, 21, 13, 17, 15, 1, 1, ...)
84: (1, 84, 85, 169, 127, 37, 41, 39, 5, 11, 1, 3, 1, 1, ...)
85: (1, 85, 43, 1, 11, 3, 7, 5, 3, 1, 1, ...)
86: (1, 86, 87, 173, 65, 119, 23, 71, 47, 59, 53, 7, 15, 11, 13, 3, 1, 1, ...)
87: (1, 87, 11, 49, 15, 1, 1, ...)
88: (1, 88, 89, 177, 133, 155, 9, 41, 25, 33, 29, 31, 15, 23, 19, 21, 5, 13, 9, 11, 5, 1, 3, 1, 1, ...)
89: (1, 89, 45, 67, 7, 37, 11, 3, 7, 5, 3, 1, 1, ...)
90: (1, 90, 91, 181, 17, 99, 29, 1, 15, 1, 1, ...)
91: (1, 91, 23, 57, 5, 31, 9, 5, 7, 3, 5, 1, 3, 1, 1, ...)
92: (1, 92, 93, 185, 139, 81, 55, 17, 9, 13, 11, 3, 7, 5, 3, 1, 1, ...)
93: (1, 93, 47, 35, 41, 19, 15, 17, 1, 9, 5, 7, 3, 5, 1, 3, 1, 1, ...)
94: (1, 94, 95, 189, 71, 65, 17, 41, 29, 35, 1, 9, 5, 7, 3, 5, 1, 3, 1, 1, ...)
95: (1, 95, 3, 49, 13, 31, 11, 21, 1, 11, 3, 7, 5, 3, 1, 1, ...)
96: (1, 96, 97, 193, 145, 169, 157, 163, 5, 21, 13, 17, 15, 1, 1, ...)
97: (1, 97, 49, 73, 61, 67, 1, 17, 9, 13, 11, 3, 7, 5, 3, 1, 1, ...)
98: (1, 98, 99, 197, 37, 117, 77, 97, 87, 23, 55, 39, 47, 43, 45, 11, 7, 9, 1, 5, 3, 1, 1, ...)
99: (1, 99, 25, 31, 7, 19, 13, 1, 7, 1, 1, ...) def a(k):
    sequence = [1, k]
    while sequence[-2:] != [1, 1]:
        next_term = sequence[-1] + sequence[-2]
        while next_term % 2 == 0:
            next_term //= 2
        sequence.append(next_term)
    return sequence[:-2] If you graph the number of terms $k$ needs before reaching $(1, 1)$ it looks pretty random: Does the sequence reach $(1, 1)$ for every positive integer value of $k$ ?","['elementary-number-theory', 'recurrence-relations', 'sequences-and-series']"
4737834,Derivation of an asymptotic error formula for the Trapezoidal method for IVPs,"I am trying to prove the following theorem, which is a paraphrased version of Exercise 6.18 in Kendall Atkinson's An Introduction to Numerical Analysis : Theorem. Let $[x_0,b]$ be a finite interval, let $h > 0$ , and let $N \equiv N(h) := \lfloor \frac{b-x_0}{h} \rfloor$ . Let $f \in C^3([x_0,b] \times \mathbb{R},\mathbb{R})$ be uniformly Lipschitz in its second argument with constant $K \geq 0$ . Assume $f_{yy}$ is bounded and that $hK \leq 1$ . Let $Y:[x_0,b] \to \mathbb{R}$ be the unique solution to the IVP \begin{equation}
       \hspace{3.7cm} Y'(x) = f(x,Y(x)), \quad Y(x_0) = Y_0.  \hspace{3.8cm} (1)
\end{equation} Let $x_n := x_0 + nh$ for $n=0,1,\ldots,N(h)$ , let $y_0 \in \mathbb{R}$ , and let $(y_h(x_n))_{n \geq 1} = (y_n)_{n \geq 1}$ be defined according to the implicit Trapezoidal method applied to (1): \begin{equation}
   \hspace{2cm} y_{n+1} = y_n + \frac{h}{2}\left[f(x_n,y_n) + f(x_{n+1},y_{n+1}) \right], \qquad n \geq 0.  \hspace{2.1cm} (2)
\end{equation} Let $e_n := Y(x_n) - y_n$ be the error at the $n$ -th iterate, let $\delta_0 \in \mathbb{R}$ , and assume that the initial error satisfies $e_0 = \delta_0 h^2 + O(h^3)$ . Then \begin{align*}
    \hspace{5cm} e_n = D(x_n)h^2 + O(h^3) \hspace{5cm} (3)
\end{align*} for all $n \geq 0$ , where $D:[x_0,b] \to \mathbb{R}$ is the solution of the IVP \begin{equation}
   \hspace{1.95cm} D'(x) = f_y(x,Y(x))D(x) - \frac{1}{12}Y^{(3)}(x), \quad D(x_0) = \delta_0. \hspace{2.1cm} (4) 
\end{equation} Some remarks: The Lipschitz condition on $f$ is not really relevant to this proof; it's just there to guarantee the convergence of the Trapezoidal method. Atkinson proves a similar asymptotic error formula for the forward Euler method (p.352-354), which I have included at the end of this post for reference. Assuming the proof of the above theorem is analogous, I think it should follow these steps: Step 1 . Derive a recurrence relation for the error $e_n$ in terms of $h$ , $f$ and $Y$ , using equation (2) and the truncation error of the trapezoidal method. Step 2 . Identify the so-called principal part of the error, call it $g_n$ , in the recurrence formula for $e_n$ . (The principal part of the error consists of the terms that make the dominant contribution to the error.) Set up a new recurrence formula of the form $g_{n+1} := F(g_n)$ for the accumulation of the principal part of the error. Step 3 . By making an appropriate change of variables from $g_n$ to $\delta_n := h^{-2} g_n$ (or something like that), obtain a formula that is the Trapezoidal method applied to the IVP in (4), with $\delta_{n+1}$ playing the role of $y_{n+1}$ : \begin{align*}
   \hspace{5mm} \delta_{n+1} = \delta_n + \tfrac{h}{2} \big[ f_y(x_n,Y(x_n)) \delta_n - \tfrac{1}{12}Y^{(3)}(x_n) + f_y(x_{n+1},Y(x_{n+1})) \delta_{n+1} - \tfrac{1}{12}Y^{(3)}(x_{n+1}) \big]  \hspace{5mm} (5)
\end{align*} Step 4 . Prove that $g_n = D(x_n)h^2 + O(h^3)$ and $e_n - g_n = O(h^m)$ for some $m \geq 3$ . The first equality follows easily from Step 3: Since the Trapezoidal method is convergent as $h \to 0$ , we have \begin{align*}
   \delta_n - D(x_n) = O(h).
\end{align*} Multiplying through by $h^2$ then gives $g_n - D(x_n)h^2 = O(h^3).$ I think I should be able to prove the second equality. We'll then have \begin{align*}
    e_n &= g_n + [e_n - g_n] \\[3pt]
        &= D(x_n)h^2 + O(h^3) + O(h^m) \\[3pt]
        &= D(x_n)h^2 + O(h^3) 
\end{align*} as desired. My main difficulty is in Step 3. Here is what I have come up with so far. Step 1 : Firstly, since $f \in C^3([x_0,b] \times \mathbb{R},\mathbb{R})$ , we have $Y \in C^4([x_0,b],\mathbb{R})$ (by this post ). Then from the local truncation error formula of the Trapezoidal rule (equation (6.5.1) in Atkinson), \begin{align*}
    Y(x_{n+1}) = Y(x_n) + \frac{h}{2}\big[f(x_n,Y(x_n)) + f(x_{n+1},Y(x_{n+1})) \big] - \frac{h^3}{12} Y^{(3)}(\xi_n)
\end{align*} for some $\xi_n \in [x_n,x_{n+1}]$ . By Taylor's Theorem applied to the last term, $$ Y^{(3)}(x_n) = Y^{(3)}(x_n) + Y^{(4)}(\xi_n)(x_n - \xi_n)$$ and so we can write \begin{equation}
    Y(x_{n+1}) = Y(x_n) + \frac{h}{2}\big[f(x_n,Y(x_n)) + f(x_{n+1},Y(x_{n+1})) \big] - \frac{h^3}{12} Y^{(3)}(x_n) + O(h^4). \qquad (6) 
\end{equation} To ease notation, let $Y_n := Y(x_n)$ . Then subtracting (6) from (2) gives \begin{align*}
   e_{n+1} &= e_n + \frac{h}{2}\left[f(x_n,Y_n) - f(x_n,y_n) + f(x_{n+1},Y_{n+1}) - f(x_{n+1},y_{n+1})  \right] \\[4pt]
           &\quad - \frac{h^3}{12} Y^{(3)}(x_n) + O(h^4).  \qquad (7)
\end{align*} Now applying Taylor's theorem to the functions $y \mapsto f(x_n,y)$ , we get \begin{align*}
    f(x_n,y_n) = f(x_n,Y_n) + f_y(x_n,Y_n)(y_n - Y_n) + f_{yy}(x_n,\zeta_n) \frac{(y_n - Y_n)^2}{2}
\end{align*} for some $\zeta_n$ between $y_n$ and $Y_n$ . Rearranging, we get \begin{align*}
    f(x_n,Y_n) - f(x_n,y_n) = e_n f_y(x_n,Y_n) - \tfrac{1}{2}f_{yy}(x_n,\zeta_n)e_n^2. 
\end{align*} Similarly, \begin{align*}
    f(x_{n+1},Y_{n+1}) - f(x_n,y_n) = e_n f_y(x_{n+1},Y_{n+1}) - \tfrac{1}{2}f_{yy}(x_{n+1},\zeta_{n+1})e_n^2 
\end{align*} for some $\zeta_{n+1}$ between $y_{n+1}$ and $Y_{n+1}$ . Substituting the above into (7) gives \begin{align*}
    e_{n+1} &= e_n + \frac{h}{2} \left[ e_n f_y(x_n,Y_n) - \tfrac{1}{2}f_{yy}(x_n,\zeta_n)e_n^2 +  e_n f_y(x_{n+1},Y_{n+1}) - \tfrac{1}{2}f_{yy}(x_{n+1},\zeta_{n+1})e_n^2  \right] \\[5pt]
    & \quad -\frac{h^3}{12} Y^{(3)}(x_n) + O(h^4) \\[5pt]
    & \hspace{-5mm} = \left[1 + \frac{h}{2}f_y(x_n,Y_n) + \frac{h}{2}f_y(x_{n+1},Y_{n+1}) \right] e_n - \frac{h^3}{12} Y^{(3)}(x_n) - \frac{h}{4}e_n^2 \left[f_{yy}(x_n,\zeta_n) + f_{yy}(x_{n+1},\zeta_{n+1}) \right] + O(h^4)
\end{align*} Step 2. We should have $e_n = O(h^2)$ for the Trapezoidal method (by (6.5.18) in Atkinson), so we have $\frac{h}{4}e_n^2 \left[f_{yy}(x_n,\zeta_n) + f_{yy}(x_{n+1},\zeta_{n+1}) \right] = O(h^5)$ . Dropping this term and the $O(h^4)$ terms, we define \begin{align*}
    \hspace{2cm} g_{n+1} =  \left[1 + \frac{h}{2}f_y(x_n,Y_n) + \frac{h}{2}f_y(x_{n+1},Y_{n+1}) \right] g_n - \frac{h^3}{12} Y^{(3)}(x_n), \qquad n \geq 0  \hspace{2cm} (8)
\end{align*} with $g_0 := \delta_0 h^2$ , to represent the principal part of the error. Step 3. Now define a new sequence $\delta_n := g_n/h^2$ . Then write (8) in terms of $\delta_n$ : \begin{align*}
    h^2 \delta_{n+1} =  \left[1 + \frac{h}{2}f_y(x_n,Y_n) + \frac{h}{2}f_y(x_{n+1},Y_{n+1}) \right] h^2 \delta_n - \frac{h^3}{12} Y^{(3)}(x_n) 
\end{align*} Cancelling out $h^2$ gives and rearranging leads to the equation \begin{align*}
    \delta_{n+1} = \delta_n + \frac{h}{2}\left[f_y(x_n,Y_n) \delta_n + f_y(x_{n+1},Y_{n+1}) \delta_n - \frac{1}{6} Y^{(3)}(x_n)   \right]
\end{align*} This is the point where I got stuck. How can arrive at equation (5)? Or did I go wrong in a previous step? Any help would be greatly appreciated! For reference, here is Atkinson's proof of a similar theorem for Euler's method:","['initial-value-problems', 'numerical-methods', 'recurrence-relations', 'ordinary-differential-equations']"
4737874,Maximize $xy + 2 yz + 6 x $ subject to $x^2 + y^2 + z^2 = 36 $,"Question: Maximize $f(x,y, z) = x y + 2 y z + 6 x $ subject to $ x^2 + y^2 + z^2 = 36 $ . This question is different from a a previous one due to the existence of the linear term $6x$ . Here is my approach: Following the analysis done in the previous problem, we find that $ (x, y, z) = 6 ( \sin \theta \cos \phi, \sin \theta \sin \phi, \cos \theta ) $ So, $ f(x,y,z) = f(\theta, \phi) = 36 ( \sin^2 \theta \cos \phi \sin \phi + 2 \sin \theta \cos \theta \sin \phi + \sin \theta \cos \phi) $ I am not sure how to maximize this expression because of the existence of linear terms as well as quadratic terms. So, instead, may be using Lagrange multiplier method will prove to be the right way. The Lagrange multiplier function is $ g(x,  y, z) = x y + 2 y z + 6 x + \lambda ( x^2 + y^2 + z^2 - 36) $ This can be written concisely as $ g(r) = r^T Q r + b^T r + \lambda ( r^T r - 36 ) $ where $ r = [x, y, z]^T $ , $ Q = \begin{bmatrix} 0 && \dfrac{1}{2} && 0 \\ \dfrac{1}{2} && 0 && 1 \\ 0 && 1 && 0 \end{bmatrix} $ and $b = [6, 0, 0]^T$ Taking the partial derivatives leads to $ \nabla_r \ g(r) = 2 Q r + b +  \lambda (2 r ) = \mathbf{0} $ $ r^T r = 36 $ From the first equation (the vector equation), I can write $ 2 (Q + \lambda I) r = - b $ So, $ r = - \dfrac{1}{2} (Q + \lambda I)^{-1} b $ Plugging this into the constraint equation, and simplifying, I get, $ b^T (Q + \lambda I)^{-2} b = 144 $ But how can this be solved for $\lambda$ ?? That's where I am stuck. Your help is greatly appreciated.","['maxima-minima', 'calculus', 'quadrics', 'optimization', 'algebra-precalculus']"
4737888,Homogeneity of Topological Manifolds: Are they all quotients of topological groups?,"I am trying to prove that every connected topological manifold $M$ (assume Hausdorff and second countable) is homogeneous in the sense defined by Bourbaki's General Topology Ch.3 Sec.2.5 pg 232. The requirement on $M$ for topological homogeneity is that it admits a group action $\theta:G\times M\to M$ which is continuous and transitive and also has for every $x\in M$ that the orbit map $\theta_x:G\to M$ defined by $\theta_x(g):=\theta(g,x)$ is an open map. The requirement that all of these orbit maps are open is here included so that the quotient of $G$ with its stabilizer subgroup $G/G_x$ ends up being homeomorphic to $M$ . Essentially what I am asking is: Is every connected topological manifold $M$ homeomorphic to the quotient of some two topological groups? Firstly, I know that $M$ being connected implies that $\text{Homeo}(M)$ acts transitively on it, see here . Moreover, I know from Arens (1946) that $\text{Homeo}(M)$ is a topological group in this case (edit: with the compact-open topology). In fact, I know from here that this is the coarsest topology for $\text{Homeo}(M)$ which makes it a topological group and which makes its natural action on M continuous in both variables simultaneously. Thus, all that remains is to show that the orbit maps $\theta_x:\text{Homeo}(M)\to M$ defined by $\theta_x(h)=h(x)$ are open maps. Let $U$ be a generic open set in $\text{Homeo}(M)$ . Our goal is to show that the image of this map, $\theta_x(U)=U(x)$ , is an open set in $M$ . I think I could prove this if I restrict my attention to topological manifolds which can be given a smooth structure. Then I can talk about $\text{Diff}(M)$ instead and get the open map condition by thinking about the Lie group exponential. I'd prefer to keep the result as general as possible, however. Any thoughts on how to proceed?","['topological-groups', 'manifolds', 'homogeneous-spaces', 'group-actions', 'general-topology']"
4737907,Approximating cube roots to high precision.,"I learned that to approximate $\sqrt{n}$ to one digit of precision (where $n$ is a real number), we find the closest square to $n$ ( $s^2$ ), take the square root of that, and add the following $$\frac{n-s^2}{2s}\tag{1}$$ where the two in the denominator of $(1)$ comes from the two in the denominator of the power ( $\sqrt{n}=n^{1/2}$ ). I thought that to approximate cube roots, we replace that two with a three and we choose the closest cube instead. However, I don’t get great approximations when doing that. For example, using this method we get that the cube root of 200 is about 6.1, which isn’t the desired $5.8$ . So how can we approximate cube roots for at least one decimal digit without technical assistance? Edit: By ""technical assistance"" I originally meant computers, calculators, or any machinery that computes expressions quickly. But this now includes things like no big coefficients, radicals, or anything too complicated. The purpose of this question was to be able to compute cube roots mentally.","['radicals', 'calculus', 'approximation']"
4737914,differential equation with distribution,"I'm trying to solve the differential equation: \begin{equation}
\frac{d^{2}y}{dx^{2}}=\delta(x)y
\end{equation} My 'guess' was a solution of the following form: \begin{equation}
y=A\exp\left[\int_{x_{0}}^{x}dx^{\prime}\int_{x_{0}}^{x^{\prime}}dx^{\prime\prime}\delta(x^{\prime\prime})\right]+B\exp\left[\int_{x_{0}}^{x}dx^{\prime}\delta(x^{\prime})\right]
\end{equation} Is there any way to solve such an equation? EDIT: more interestingly, \begin{equation}
\frac{d^{2}y}{dx^{2}}=\delta^{\prime}(x)y
\end{equation}","['ordinary-differential-equations', 'distribution-theory']"
4737929,What is the name of a generator of a $\mathbb{k}G$-module in representation theory?,"Let $G$ be a group and $V$ be a representation of $G$ . We say that $V$ is a cyclic representation if there exists $v \in V$ such that $ \{ g \cdot v \ | \ g \in G \} $ generates $V$ . Is there a common name for such an element? I know that as a $\mathbb{k}G$ -module (which is an equivalent construction), it is called a generator. But because I work in representation theory (and I want to keep this approach in my article), $V$ is a vector space, so in that setting, generator is the name of an element of the set $ \{ g \cdot v \ | \ g \in G \} $ , and of many other sets. Is there another name?","['representation-theory', 'reference-request', 'definition', 'group-theory', 'terminology']"
4737931,Prime solutions to $p^2(p^3-1) = q(q+1)$,"Solve: $$p^2(p^3-1)=q(q+1)$$ where $p$ and $q$ are prime numbers. Could someone help me with this question? I tried several things but I couldn't get it. I can show what I already did about that question, but I didn't go too far. I don't have the answers.","['number-theory', 'diophantine-equations', 'elementary-number-theory', 'prime-numbers']"
4737962,"Conjecture about the mean value of an almost periodic function (the ""Mountains of Guilin"")","Consider the function $f_{p_j,n}(x)=|\sin (p_1x)+\sin (p_2x)+\dots+\sin (p_nx)|$ where $p_j$ is any sequence such that no two $p$ are rational multiples of each other where the $p_j$ are linearly independent over $\mathbb{Q}$ (so the function is almost periodic). (After reading @mollyerin's answer, I changed the condition on $p_j$ .) For example, here is the graph of $f_{2^{1/j},\color{red}{5}}(x)$ : And here is the graph of $f_{2^{1/j},\color{red}{10}}(x)$ : I call this kind of function $f_{p_j,n}(x)$ , the "" Mountains of Guilin "". Now consider the mean value of $f_{p_j,n}(x)$ . I have evidence that suggests that: For any given value of $n$ , the mean value of $f_{p_j,n}(x)$ is independent of the choice of sequence $p_j$ (as long as the $p_j$ are linearly independent over $\mathbb{Q}$ ). The mean value of $f_{p_j,\color{red}{2}}(x)$ is $8/\pi^2$ . I have not been able to find a general expression for the mean value of $f_{p_j,n}(x)$ in terms of $n$ , but I have the following conjecture : $$\color{red}{\lim_{n\to\infty}\frac{\text{mean value of $f_{p_j,n}(x)$}}{\sqrt n}=\frac{1}{\sqrt \pi}}$$ Is my conjecture true? Numerical evidence for my conjecture: Using desmos, I approximated the mean value of $f_{2^{1/j},n}(x)$ as $M_n=\frac{1}{10^7}\sum\limits_{k=1}^{10^7}|\sin (2^1k)+\sin (2^{1/2}k)+\dots+\sin (2^{1/n}k)|$ Letting $L(n)=\dfrac{M_n}{\sqrt n}$ , we have: $L(1)\approx 1.128379\left(\frac{1}{\sqrt \pi}\right)$ $L(2)\approx 1.015898\left(\frac{1}{\sqrt \pi}\right)$ $L(10)\approx 1.006373\left(\frac{1}{\sqrt \pi}\right)$ $L(100)\approx 1.000334\left(\frac{1}{\sqrt \pi}\right)$ This suggests that my conjecture may be true, but I don't know how to prove it. Visual representation of the mean value: If my conjecture is true, then there is a nice way to visually represent the mean value of the function. Draw a circle of area $n$ with its centre on the $x$ -axis. A horizontal line through the top of the circle represents, approximately, the average value of the function.","['integration', 'conjectures', 'means', 'ergodic-theory', 'real-analysis']"
4737974,$\ell(f)>\int_{a}^{b}||f'(t)||dt$,"We know that if $f:[a,b] \rightarrow \mathbb{R}^{n}$ is a $C^1$ path then $$\ell(f)=\int_{a}^{b}\|f'(t)\|dt.$$ Moreover, in the proof of this result we use explicitly the continuity of $f'.$ I'm trying to find an example of a differentiable function (at every point of $[a,b]$ ) such that $f':[a,b] \rightarrow \mathbb{R}^n$ is not continuous and $$\ell(f) > \int_{a}^{b} \|f'(t)\|dt.$$ I found this answer , but if I'm not mistaken, the Cantor function is not differentiable at every point. Can anyone help me with this example?","['arc-length', 'derivatives', 'real-analysis']"
4737982,How to evaluate $\int_0^1 \frac{dx}{\sqrt{x + \sqrt{x^2 + \sqrt{x^3}}}}?$,$$\int_0^1 \frac{dx}{\sqrt{x + \sqrt{x^2 + \sqrt{x^3}}}}$$ How to evaluate the above integral? I am trying this question by substituting $x = \tan^{2 / 3} y$ . Then the denominator will become $$\sqrt{\tan^{2/3} y+\sqrt{\tan^{4/3} y+ \tan y}} .$$ But how to proceed further?,"['integration', 'improper-integrals', 'calculus', 'definite-integrals']"
4737994,When do polynomial rings admit extensions over which every polynomial factorizes?,"Given a polynomial ring $F[x_1...x_n]$ over a field $F$ , I wanted to ask about when we can factor some $f \in F[x_1...x_n].$ I was curious to see whether a fundamental theorem of algebra-esque notion exists for every polynomial ring (i.e., we can always devise a field extension wherein all polynomials of our given field can be factored). My thought process is inspired by the case of $\mathbb{Q}= F$ - here, we cannot factor polynomials such as $f(x) = x^2 - 2$ , so we devise the algebraic extension $\mathbb{Q}(x)/x^2-2$ , quotienting by the ideal generated by $f$ . Similarly, it seems that for an arbitrary case, we can simply consider $\mathbb{F}[x_1...x_n]/f$ , which is a field whenever $f$ is irreducible. Therefore, perhaps we get the restriction that we can always construct field extensions wherein all irreducible polynomials are factorable. This is Idea 1. However, Idea 1 is the same as Idea 2, wherein we take some wonderful object $a$ such that it satisfies the relation $f(a) = a^2 - 2=0$ , adjoin it to $\mathbb{Q}$ , and yield $\mathbb{Q}(a)$ - a field wherein $f$ as above now factors. We know that Idea 1 is isomorphic to Idea 2, but somehow in Idea 2, we bring out a strange element and (assuming $f$ is irreducible), derive a field by adjoining it to $\mathbb{Q}$ . Now, I imagine that when we have $f$ reducible, then we cannot do this if the polynomial ring is not a UFD - but in that case, what stops us from taking our field $F$ , and simply adjoining it to it objects $a_i$ defined by the relation that $f(a_i) = 0$ ? I guess it would not be a field any longer, but what exactly breaks down? Moreover, are there examples of fields which don't admit any extension over which every polynomial splits? What do such examples tell us about our ability to devise some object, call it a root, and attach it to our field?","['galois-theory', 'algebraic-geometry', 'abstract-algebra', 'polynomials']"
4738002,What is the precise definition of infinite limit at infinity of a function?,"I saw the following definition on youtube. Let $f$ be a function defined on some interval $(a, +\infty)$ $\lim\limits_{x \to +\infty} f(x)= +\infty$ (1) means $\forall N > 0$ $\exists M > 0$ such that $x > M$ implies $f(x) > N$ . Is this a correct and general definition for equation (1)? For example, the function $f$ defined below doesn't satisfy the above definition, but it is obviously divergent, as $x \to +\infty$ . $f(x) \equiv \log(x) \sin^2(x)$ , $x \in (0, +\infty)$","['analysis', 'definition', 'functions', 'infinity', 'limits']"
4738005,Difference between “for some $k$” and “for some arbitrary $k$”,"I am told that the “for some” and “for some arbitrary”  are different. For example, when proving the statement “if n is odd, then $n^2$ is odd”, one of the steps includes writing $$\text{$n = 2k+1,\:\:$ where $k$ is some integer}.$$ I am told that writing “ $k$ is some arbitrary integer” here is wrong? On the other hand, in mathematical induction, when performing the inductive step, before writing the inductive hypothesis for $k,$ we write “for some arbitrary $k\text”$ . I am then told that the “arbitrary” here is compulsory to write. To me, they sound the same. Am I wrong? Are they different in terms of notation? PS I’m relatively new to this stuff, maybe explain less using notation and appeal more to logical understanding.","['quantifiers', 'proof-writing', 'logic', 'analysis', 'logic-translation']"
4738040,Proving a specific reccurrence relation. Proof Improvements.,"Attempting to write proof for the given question. Are the proofs sufficient? Would appreciate any feedback and criticisms. The relation $\star$ is defined on $\mathbb{R}-\{0\}$ by $x \star y$ iff $x y \geq 0 $ . Proof of reflexivity : For any non-zero real number $x \in \mathbb{R} - \{0\}$ , we have $x^2 \geq 0$ . Thus, $\star$ is reflexive. Proof of symmetry : Since real number multiplication is commutative, if $x, y \in \mathbb{R} - \{0\}$ and $x \star y$ satisfies $xy \geq 0$ , then the relation $y \star x$ , which is equivalent to $yx \geq 0$ , is symmetric. Proof of anti-symmetry : Anti-symmetry states that when $(x, y) \in \mathbb{R} - \{0\}$ and $(y, x) \in \mathbb{R} - \{0\}$ , then $x = y$ . However, due to the commutativity of multiplication, this property does not hold. For example, when we take $x = 1$ and $y = 2$ , the relation $x \star y$ implies $x \times y = 1 \times 2 = 2 = 2 \times 1 \geq 0$ . Nevertheless, it is evident that $2 \neq 1$ . Thus, the relation is not anti-symmetric. Proof of transitivity : When considering $(x, y) \in \mathbb{R} - {0}$ such that $xy \geq 0$ , it is necessary for both $x$ and $y$ to share the same sign. Similarly, for $(y, z) \in \mathbb{R} - {0}$ , $y$ and $z$ must have the same sign to satisfy $yz \geq 0$ . Given that $x$ , $y$ , and $z$ share the same sign and are non-zero, it becomes evident that $x \star z$ , where $xz \geq 0$ , is achievable. Hence, the relation is transitive.","['equivalence-relations', 'relations', 'order-theory', 'solution-verification', 'discrete-mathematics']"
4738046,"Is there a sequence $(a_n)$ so that for every $r \in \mathbb{R}$, there is a subsequence of $(a_n)$ convergent to r?","My guess is that there is no such sequence and prove it by contradiction. My attempt is as follows: Let $A_r$ denote the set containing all terms of such subsequence for each $r \in \mathbb{R}$ . Since each subsequence that converges to $r$ contains at least one distinct term, then the union $\bigcup_{r\in \mathbb{R}}A_r$ has uncountably many elements. However, the sequences cannot have uncountably many terms. Is there anything wrong with my attempt? Or a better solution to go?",['real-analysis']
4738132,How to evaluate this integral without using numerical method?,$$\int_{-0.5}^{0.5} \frac{x^2\cos(4\pi x^3)}{1+e^x} dx $$ I tried it using Numerical method and got roughly the same with Wolfram. Are there any other methods to evaluate this integral?,"['integration', 'calculus']"
4738151,Suppose $X_{n}\sim bern(p_{n})$ and $\sum_{n}p_{n}<\infty$. Then $\frac{\sum_{k=1}^{n}X_{k}}{\sum_{n}p_{n}}$ does not converge to $1$ in probability,"This may seem like a weird question. Suppose $X_{n}\sim \operatorname{bern}(p_{n})$ , independent and $\sum_{k=1}^{\infty}p_{k}<\infty$ . Then does $\dfrac{\sum_{k=1}^{n}X_{k}}{\sum_{k=1}^{n}p_{k}}$ converge to $1$ in probability? Well, the first thing that one notices is that $\operatorname{Var}(\dfrac{\sum_{k=1}^{n}X_{k}}{\sum_{k=1}^{n}p_{k}})$ does not go to $0$ which is the variance of $1$ . Ideally, if $(\sum_{k=1}^{n}X_{k})^{2}$ was uniformly integrable, we can conclude that as $\dfrac{\sum_{k=1}^{n}X_{k}}{\sum_{k=1}^{n}p_{n}}$ does not converge in $L^{2}$ to $1$ and hence it cannot converge in proability to $1$ (by using uniform integrability). The other argument is that $S_{n}$ should converge then to $\sum_{n}p_{n}$ which if we assume that it is an integer, then $S_{n}$ must also converge almost surely to it as $S_{n}$ 's are monotone but this is getting me nowhere. To try and conclude uniform integrability, I look at $\bigg(\sum_{k=1}^{n}X_{k}\bigg)^{4}$ and try and show that $\sup_{n}\mathbb{E}\left(\sum_{k=1}^{n}X_{k}\right)^{4}<\infty$ . But after opening the brackets and using multinomial theorem, I am getting complicated expressions. Well, $\sum_{k=1}^{n}X_{k}$ is uniformly bounded in $L^{2}$ but I also think that $\sum_{k=1}^{n}X_{k}$ will be bounded in $L^{4}$ but I am having trouble to show it rigorously. Any help? Is there an easier way to do this?","['probability-distributions', 'probability-theory', 'probability', 'bernoulli-distribution']"
4738160,Pairwise distances between n points on a unit circle,"Consider $n $ points on the circumference of a unit circle.What configuration of them will maximise the total pairwise distance between them.To me it seems the points should be equally spaced .The total pairwise  distance is given by : \begin{equation*}
D = \displaystyle \sum_{j=1}^{n} \sum_{k=j+1}^{n} \sqrt{2 - 2\cos(\theta_j - \theta_k)}
\end{equation*} where the angles are between 0 and $2\pi.$ If my instinct is right then all agles are equal.But proving that using calculus seems almost impossible for large n.Can there be some argument using some simple geomtric or symmetry argument or any other approach by passing messy  calculus? I would be obliged for any help/ hints in this regard","['optimization', 'geometry']"
4738175,Wiener Lemma for matrix valued functions,"The ordinary Wiener lemma states that if $f(x):=\sum_{n \in \mathbb{Z}} a_n \exp(inx)$ ( $\sum |a_n|<\infty$ ) and if $f(x)\neq 0$ everywhere, then $g:=1/f$ can also be written as $$g(x)= \sum b_n \exp(inx)$$ with $\sum_n |b_n|<\infty$ The proof of this uses the fact, that an element $u$ of a commutative Banach Algebra is invertible iff $h(u)\neq 0$ for each complex homomorphism. Now, I have seen a similar result used in a paper - more specifically: substitute square- matrices in place of the coefficients $a_n$ , i.e. consider functions of the form $$f(x):= \sum A_n \exp(inx)$$ Assume now that each $f(x)$ is an invertible matrix. Does it then follow (and if so, why) that $f^{-1}(x)$ can be expressed as $$f^{-1}(x):= \sum B_n \exp(inx)$$ for some matrices satisfying $\sum |B_n|<\infty $ I don't know how to proceed here, because in this case the Banach Algebra is not commutative. Can I still adapt the proof of the original Wiener Lemma somehow?","['banach-algebras', 'wiener-algebra', 'functional-analysis']"
4738280,What does $P(X\ge Y)$ mean?,"This is from Joseph Blitzstein, Introduction to Probability, pg 197, Question 20 : Let $X\sim \text{Bin}(100, 0.9)$ . For each of the following parts, construct an example showing
that it is possible, or explain clearly why it is impossible. In this problem, $Y$ is a random variable on the same probability space as $X$ ; note that $X$ and $Y$ are not necessarily
independent. (a) Is it possible to have $Y\sim \text{Pois}(0.01)$ with $P(X \ge Y ) = 1$ ? I would like to know what does $P(X \ge Y)$ mean in this context. Does it mean that for all values that $X$ and $Y$ take $P(X \ge Y) = 1$ ?","['statistics', 'poisson-distribution', 'binomial-distribution', 'probability', 'random-variables']"
4738281,Fundamental Description Of Algebras Over The Monad $PP^{op}:\text{Set}\to\text{Set}$,"$
\newcommand{\eps}{\epsilon}
\newcommand{\op}{^\text{op}}
\newcommand{\id}{\text{id}}
\newcommand{\set}{\text{Set}}
\newcommand{\xra}{\xrightarrow}
$ Can someone help me to understand how algebras over the monad $T:=PP\op$ , with $P:\set\op\to\set$ being the powerset functor taking maps to inverse images, look like?  Note that this is not a duplicate of understanding the algebras of the powerset monad , which concerns the monad $P':\set\to\set$ which takes maps to direct images.  This question was also flagged as duplicate of what are the algebras of the double powerset monad , which is very close, although I am more interested in sets than in Boolean algebras. This is what I got so far: Let $P:\set\op\to\set$ be the powerset functor with left adjoint $P\op:\set\to\set\op$ where the unit $\eta(U):U\to PP\op U$ maps $x\in U$ to the set $\{X\subseteq U:x\in X\}$ and the counit $\eps$ is just the corresponding arrow in the opposite category.  So we can construct a monad $T:=PP\op$ from this adjoint pair as usual with $\mu(U):TTU\to TU$ defined via $$TTU=PP\op PP\op U\xra{P\eps(P\op U)} PP\op U=TU$$ and I wonder how the algebras over this monad look like. Recall that an algebra over $U$ is a map $a:TU\to U$ such that $U\xra{\eta(U)}TU\xra{a} U=\id(U)$ and $TTU\xra{\mu(U)}TU\xra{a}U=TTU\xra{T(a)}TU\xra{a}U$ . (1.) is relatively straight forward, the set of all $X$ containing $x$ must be mapped back to $x$ by $a$ . For (2.), note is that both, $\mu(U)$ and $T(a)$ , are of the shape $Pf$ for some $f:P\op U\to P\op TU$ , so given some $W\in TTU$ , they return all $X\in P\op U$ such that $f(X)\in W$ .  This relation can further simplified by noting that if $a$ already satisfies (1.), $f$ is in both cases injective.  For $\mu(U)$ we have $f_0:=e(P\op U)$ mapping $X$ to the principal filter containing all $Y\in TU$ with $X\in Y$ , which is clearly injective, and for $T(a)$ we have $f_1:=P\op(a)$ which maps $X$ to the set of $Y$ with $a(Y)\in X$ , which is also injective since $\eta(U)$ is injective and $a$ satisfies (1.).  Let $W_i$ be the image of $P\op U$ under $f_i$ and let $Y_{0\cap 1}$ be the inverse image of $W_0\cap W_1$ . So $a$ is only allowed to care for elements in $Y_{0\cap 1}$ , since the others can be independently added or removed under $f_0$ and $f_1$ by changing $W$ accordingly. We can see that $W_0$ is what I call a disjoint free antichain, for $Z_0\neq Z'_0\in W_0$ , let's say $Z_0$ is the principal filter of $X$ and $Z'_0$ the principal filter of $X'$ , neither $Z_0\subset Z'_0$ nor $Z'_0\subset Z_0$ nor $Z_0\cap Z'_0=\emptyset$ since only $Z_0$ contains $\{X\}$ , only $Z'_0$ contains $\{X'\}$ and both contain $\{X,X'\}$ .  On the other hand, if $X\subset X'$ , clearly $f_1(X)\subseteq f_1(X')$ and by (1.) also $f_1(X)\subset f_1(X')$ , a contradiction.  Similar with $X\cap X'=\emptyset$ , so it follows that $Y_{0\cap 1}$ must inherit the disjoint free antichain property from $W_0$ . But from (1.) we can also see that if $x,x'\in U$ with $x\neq x'$ , we must have $X,X'\in Y_{0\cap 1}$ such that $X$ contains $x$ but not $x'$ and $X'$ contains $x'$ but not $x$ , otherwise $\eta(U)x\cap Y_{0\cap 1}=\eta(U)x'\cap Y_{0\cap 1}$ and $a$ would not be able to differentiate the two sets. So all in all we have this strange system of disjoint free antichains which can differentiate between all elements.  I don't know if there is a special name for this property.","['elementary-set-theory', 'monads', 'topos-theory']"
4738341,Kernel and image of vector bundle morphism over smooth submersion. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 11 months ago . The community reviewed whether to reopen this question 11 months ago and left it closed: Original close reason(s) were not resolved Improve this question Suppose we have vector bundles $\pi_E:E\to M$ and $\pi_{E'}:E'\to M'$ , smooth submersion $G:M\to M'$ and vector bundle morphism $F:E\to E'$ over $G$ . Are then the sets $ker F$ and $im F$ smooth vector subbundles of $E$ and $E'$ . I know that the answer is yes if $G=id_M$ , however I am not sure what the answer is in this case. Can someone give me some literature with the proof please. EDIT: I forgot to say that $F$ has a constant rang.","['vector-bundles', 'smooth-manifolds', 'differential-geometry']"
4738361,"Central Limit Theorem and Law of Large Numbers for Non-Constant ""N""?","This is a question I have been having for a while. Usually, we define the Central Limit Theorem as: Let $X_1, X_2, \dots, X_n$ be a random sample of size $n$ from a population with mean $\mu$ and finite variance $\sigma^2$ . As $n$ approaches infinity, the distribution of the sample mean $\overline{X}$ converges to a normal distribution with mean $\mu$ and variance $\frac{\sigma^2}{n}$ , i.e., $$\sqrt{n} \left( \frac{\overline{X} - \mu}{\sigma} \right) \xrightarrow{d} N(0,1)$$ Similarly, we can also define the Law of Large Numbers as: Let $X_1, X_2, \dots, X_n$ be a sequence of independent and identically distributed random variables with finite mean $\mu$ . The Law of Large Numbers states that as the sample size $n$ increases, the sample mean $\overline{X}$ converges in probability to the population mean $\mu$ , i.e., $$\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{p} \mu$$ Given this definition, I thought of the following example: Suppose there are fish in a river Naturally, fish have the ability to enter and exit the river (e.g. birth , death, migration) We are interested in estimating the average mercury level in the average fish Suppose we take a random sample of fish from this river and measure the mercury level of each fish in the sample In most introductory mathematics textbooks, we would use both the Central Limit Theorem and Law of Large Numbers to argue that : As we measure the mercury level of more and more fish - our average sample measurement would better and better reflect the true average mercury measurement of the population. However, it seems that there is an implicit assumption that the underlying fish population of the river is constant. This brings me to my question: Are there any variations of the Central Limit Theorem and Law of Large Numbers that can be applied in situations where the population size is not-constant? Or is this actually irrelevant? (i.e. The results of both the Central Limit Theorem and Law of Large Numbers are still valid when the population size is non-constant provided that there is a large enough sample size?) Thanks!","['statistics', 'central-limit-theorem', 'law-of-large-numbers', 'probability']"
4738387,Total number of permutations of elements following a set of rules,"Consider the following elements: $s, a, b, \alpha_1, \alpha_2, \alpha_3, \beta_1, \beta_2$ and the corresponding rules: $s$ is always the first element. $a$ must always be in front of $\alpha_i, i = 1, 2, 3$ . $b$ must always be in front of $\beta_j, j = 1, 2$ . How many (permissible) permutations are there of this set of elements? How can this be generalized to a setting where there are more ""a's and b's"", i.e., more elements which must occur before a specific set of other elements? What I know so far: For $a$ , there are $|\{ \alpha_i : i = 1, 2, 3 \}| ! = 3! = 6$ permutations for its dependent elements. For $b$ , there are therefore $2! = 2$ permutations for its dependent elements. How can I put this together though to form permutations of all elements?","['permutations', 'combinations', 'combinatorics']"
4738393,"Suppose set X = {1, 2, 3}. I'll define the relation R = {(1,3), (3, 1)}. Is R symmetric or not.","I would assume R is symmetric because 1, 3 and 3, 1 are swapped, but in the definition of a symmetric relation, the property must hold for all a,b in the set X. But 2 is not covered in the relation.","['elementary-set-theory', 'relations', 'discrete-mathematics']"
4738407,Conjecture about determinant of matrix with column $j$ with all numbers not divisible by $j+1$,"Consider an $n \times n$ matrix $A_n$ with elements $a_{i,j}$ such that $a_{1,j},a_{2,j},a_{3,j},\ldots$ is the sequence of numbers not divisible by $j+1$ in increasing order starting from $1$ (e.g. $1,2,4,5,7,8,10,\ldots$ for $j = 2$ ). For fun, I have computed the determinant for $n \le 8$ and then conjectured that: $$\lvert A_n \rvert = (-1)^{n+1}$$ Is the conjecture true? How would you prove it?","['conjectures', 'determinant', 'elementary-number-theory', 'matrices', 'linear-algebra']"
4738445,"Is a relation considered reflexive only if (a, a) ∈ R for every a ∈ M","Consider the set $M = \{1, 2, 3\}$ and the relation $R = \{(1, 1), (2, 2)\}$ . Does this relation satisfy reflexivity with respect to the concept of an ""equivalence relation""? Someone told me that for an relation to satisfy reflexivity, it is not necessary for every element $a ∈ M$ to have $(a, a) ∈ R$ , as long as $a$ doesn't appear in any pair in the relation $R$ . (i.e. in the above example, since $3$ doesn't appear in any pairs like $(3, 1)$ , $(1, 3)$ , $(3, 2)$ , ...etc. then $(3, 3)$ isn't necessary to belong to $R$ ) Thus, R above satisfies reflexivity. However, I don't think that's correct because, as I remember, in order for an relation to satisfy reflexivity, then for every $a ∈ M$ , there must be $(a, a) ∈ R$","['elementary-set-theory', 'equivalence-relations']"
4738466,Bypassing division by $0$ with limits?,"math.stackexchange.com/questions/462199/… After reading the linked answer, I still don't completely understand what is going on. Using the example $\frac{2xh+3h^2}{h}=2x+3h$ is true for all $h$ except when $h=0$ . I understand this. So for all values arbitrarily close to $h=0$ this is true. Hence the functions behaves more like $2x$ as we get closer to $h=0$ . However, it never reaches there, so what allows us to substitute $h=0$ . We have basically said that this statement is true for all $h$ when not $0$ , then have gone ahead and let $h=0$ . This seems extremely unintuitive to me.","['calculus', 'algebra-precalculus']"
4738507,Olympic question about functions,"I have been trying to solve the following question: Let $f: \left \{ 1,2,... \right \}\rightarrow \mathbb{R}$ be a function such that $f(n) - f(n+1) = f(n)f(n+1)$ . If $f(2020) = \frac{1}{4040}$ , find $f(1)$ The answer is f(1) = $\frac{1}{2021}$ . Using a simple algebraic manipulation, I found that if $f(2020) = \frac{1}{4040}$ then $f(2019) = \frac{1}{4039}$ , what is interesting for the problem considering the answer... How can i progress? Or there a trick way in these question","['contest-math', 'functions', 'problem-solving']"
4738535,"Almost finished with integral but hit roadblock $\int_{0}^{\infty}\frac{\arctan^2(x^2)}{x^2}\,dx$","I was evaluating the following integral $$I:=\int_{0}^{\infty}\frac{\arctan^2(x^2)}{x^2}\,dx$$ Here is my process. I just need some guidance to finish my work. Solution process: Define $$I(t):=\int_{0}^{\infty}\frac{\arctan^2(tx^2)}{x^2}\,dx$$ $$I’(t)=\int_{0}^{\infty}\frac{2\arctan(tx^2)}{t^2x^4+1}\,dx$$ $$I’(t)=\frac{1}{t}\int_{0}^{\infty}\frac{\arctan(tx^2)}{x}\frac{d}{dx}\arctan(tx^2)\,dx$$ $I.B.P$ $$I’(t)=\frac{1}{2t}\int_{0}^{\infty}\frac{\arctan^2(tx^2)}{x^2}\,dx$$ $$I’(t)=\frac{1}{2t}I(t)$$ solving the separable ODE yields: $$I(t)=C\sqrt{t}$$ Here is the problem! Recalling the definition of $I(t)$ : $$I(t):=\int_{0}^{\infty}\frac{\arctan^2(tx^2)}{x^2}\,dx$$ I cannot find any initial conditions that can help me fish out the value of $C$ . Is it possible? How can we do it?","['integration', 'calculus', 'definite-integrals', 'real-analysis']"
4738652,What is the derivative of $\sin^{-1}(\frac{x+\sqrt{1-x^2}}{\sqrt{2}})$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 11 months ago . Improve this question If we substitute $x=\sin \theta$ then we get the derivative to be $\frac{1}{\sqrt{1-x^2}}$ shown in the picture below But instead of assuming $x=\sin \theta$ if we assume $x=\cos \theta$ we get the answer to be $-\frac{1}{\sqrt{1-x^2}}$ . Why the answers are different in this case? Where is the mistake here?","['calculus', 'inverse-function', 'derivatives']"
4738723,Can we classify all functions whose gradient is an eigenvector of the Hessian?,"Let's treat the case of two dimensions, then we don't have freedom with the other eigenvector. Is there any classification of all smooth functions $f$ on $\mathbb{R^2}$ such that $\nabla f$ is an eigenvector of $D^2f$ at every point? Such functions solve a second-order quasilinear PDE, but makes me wonder if there is a reference with a more ""explicit"" classification.","['scalar-fields', 'eigenvalues-eigenvectors', 'matrices', 'multivariable-calculus', 'hessian-matrix']"
4738765,Solving the infinite series [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 11 months ago . Improve this question Let $a_1,a_2...$ and $b_1,b_2...$ be sequences of positive real numbers such that $a_1=b_1=1$ and $b_n=b_{n-1}a_n-2$ for $n=2,3...$ Assume that the sequence $b_j$ is bounded, find $$S=\sum_{n=1}^\infty \frac{1}{a_1a_2...a_n}$$ I tried to guess a sequence but couldn't continue like this. The biggest difficulty I'm facing is with $b_n$ .","['summation', 'sequences-and-series']"
4738772,Find max N people with 2022 different colored balls choosing two balls each so that no 3 people exactly contain balls of 3 colours,"Here is the ""exact"" wording given in the problem. There are $N$ people. Each person gets two balls of different colours among $2022$ balls with different colours. The combinations of the ball colours they get are different from each other. Find the maximum possible value of $N$ such that no $3$ people exactly contain balls of $3$ colours. The English sounds a little strange to me as I am still questioning whether I understood the problem correctly. The given ""Answer key"" (no solution) states $1022121$ . The only number coincidence I can see is that $(2022)(2022)/4 = (1011)^2 = (1011)(1011) = 1022121$ which could mean nothing. Here are some thoughts regarding how or why I got stuck: Number of ways two get two balls of different colours among $2022$ balls with different colours is $(2022)(2021)/2 = 2043231$ . If we choose three colours say $a, b, c$ then the pairs containing those colours are $(a, b), (b,c)$ or $(c, a)$ where order is not important, i.e. there are ""three"" of them. Number of different three colour combinations which can be derived from $2022$ different colours is $C(2022, 3) = 1 375, 775, 540$ . Help part How do I link these to count the answer of $1022121$ ?
Am I missing something or did I misunderstand the question?
I have relatively average level secondary school level Math Olympiad abilities. Any help is appreciated. Thank you. Jon","['contest-math', 'combinations', 'combinatorics', 'discrete-mathematics', 'recreational-mathematics']"
4738815,On the structure of a probability space for the extension of a measure via a martingale,"This is a question about theorem 3.2.2 of James Norris's notes on advanced probability ( http://www.statslab.cam.ac.uk/~james/Lectures/ap.pdf ). Let $(\Omega,\mathcal{F}, (\mathcal{F}_n)_{n∈ \mathbb{N}}, \mathbb{P})$ be a filtered probability space with $\mathcal{F} = \mathcal{F}_{∞}$ and let $(X_n)_{n∈ \mathbb{N}}$ be a martingale on this probability space. Then the above theorem affirms that by Caratheodory $\mathbb{E}[X_T] = 1$ for all ""finite"" stopping times $T$ if and only if there exists a probability measure $\tilde{\mathbb{P}}$ on the filtered space $(\Omega,\mathcal{F}, (\mathcal{F}_n)_{n∈ \mathbb{N}})$ such that $$\tilde{\mathbb{P}}(A) = \mathbb{E}[X_n\mathbb{1}_{A}]\qquad \mbox{for all $A∈ \mathcal{F}_n$}$$ My first question is about the meaning of ""finite"" here. Is it correct that this should be strictly understood in the sense that $T:\Omega→ \mathbb{N}$ (i.e. this is not in the almost-sure sense)? Secondly, this suggests that the extension solely depends on the statistical properties of $(X_n)_{n∈ \mathbb{N}}$ : is it correct that in this case the extension does not depend on structure of $\Omega$ ? e.g. say two martingale $X = (X_n)_{n∈ \mathbb{N}}, X' = (X'_n)_{n∈ \mathbb{N}}$ are ""defined identically"" on two different probability spaces $(\Omega,\mathcal{F}, (\mathcal{F}_n)_{n∈ \mathbb{N}}, \mathbb{P})$ and $(\Omega',\mathcal{F}', (\mathcal{F}'_n)_{n∈ \mathbb{N}}, \mathbb{P}')$ , i.e. for all Borel measurable subset $A$ of $\mathbb{R}^{\mathbb{N}}$ , $$\mathbb{P}(X∈A)=\mathbb{P}'(X'∈ A)$$ and set $$\tilde{\mathbb{P}}_n(B) = \mathbb{E}[X_n\mathbb{1}_B]\qquad \text{for all $B ∈ \mathcal{F}_n$}$$ $$\tilde{\mathbb{P}}_n'(B) = \mathbb{E}[X_n'\mathbb{1}_{B'}]\qquad \text{for all $B' ∈ \mathcal{F}'_n$}$$ Now, say an extension $\tilde{\mathbb{P}}$ of $(\tilde{\mathbb{P}}_n)_{n∈ \mathbb{N}}$ is achieved by other means (say Kolmogorov extension) on $\Omega$ . Does that automatically imply that the extension $\tilde{\mathbb{P}}'$ of $(\tilde{\mathbb{P}}_n')_{n∈ \mathbb{N}}$ can be achieved on $\Omega'$ ? If not, are there conditions so that this can be done (without any condition of uniform integrability on $(X_n)_{n∈ \mathbb{N}})$ ?","['stochastic-processes', 'filtrations', 'martingales', 'probability-theory', 'probability']"
4738816,Projection is a covering map iff the topology is discrete,"I know that the following is true: Let $Y$ have the discrete topology. Show that if $p:X\times Y\to X$ is the projection, then $p$ is a covering map. But is the opposite direction also true? So does "" $p$ is a covering map"" imply that the topology on $Y$ is discrete? I would argue: Yes it is. We have $p^{-1}(U) = U \times Y$ and we require it to be homeomorphic to $U \times F$ for some non-empty discrete set $F$ . So if $Y$ wouldn't be discrete via it's topology, then the product topology of $U \times Y$ would be different to the one of $U \times F$ . Is this correct?","['geometric-topology', 'general-topology', 'algebraic-topology']"
4738830,What can we get by row/column addition?,"Let $\Bbb K$ be a field and let ${\bf M} \in {\Bbb K}^{n \times n}$ be a full rank matrix. Applying elementary row and column operations, one can transform $\bf M$ into the identity matrix. What can we achieve when only row/column addition is allowed? By row/column addition, I mean replacing a row/column by the sum of that row/column and a multiple of another row/cloumn. It is clear that the determinant remains the same. But can any two matrices $n\times n$ of full rank and the same determinant be transformed into each other by row and column addition? Example In the comments, I was asked to transform the matrix $$\begin{pmatrix}2&0\\ 0&\frac{1}{2}\end{pmatrix}$$ into the identity. Here is how this can be done, though I am not sure if this is the optimal way. $$\begin{pmatrix}1&{0}\\ 1&{1}\end{pmatrix}
\begin{pmatrix}1&{1}\\ 0&{1}\end{pmatrix}
\begin{pmatrix}2&0\\ 0&\frac{1}{2}\end{pmatrix}
\begin{pmatrix}1&0\\ -2&{1}\end{pmatrix}
\begin{pmatrix}1&-\frac{1}{2}\\ 0&{1}\end{pmatrix}.$$","['matrices', 'gaussian-elimination', 'linear-algebra']"
4738832,"Can $\sin(\frac{n\pi}{m})$ with $n,m \in \mathbb{Z}$ always be represented using only algebraic functions?","Can $\sin(\frac{n\pi}{m})$ with $n,m \in \mathbb{Z}$ always be represented using only algebraic functions? In other words can $sin$ of a rational multiple of $\pi$ always be represented using only only algebraic functions (e.g. addition, subtraction, division, multiplication, and roots)? This question came after failing to find a calculator that could give me the exact value of inputs that I knew had exact values (things like $sin(\frac{\pi}{8})$ and $sin(\frac{\pi}{180})$ ). As a side question, is there a calculator that can do this? I tried WolframAlpha but it wouldn't always give me an exact value.","['elementary-functions', 'trigonometry', 'rational-numbers']"
4738834,Solving quadratic ODE with a constraint,"I am trying to force a constraint on a ODE. I have the following equation: $$ \begin{aligned} \dot x &= -0.04 x + 10 y z \\ \dot y &= 0.04 x - 10 y z - 3000 y^2 \\ \dot z &= 3000 y^2 \end{aligned} $$ This equation was derived assuming a continuity equation where $x+y+z=1$ Instead of solving for $\dot z$ I would like to solve the first two terms and apply the continuity constraint. I think this should be done with a Lagrangian multiplier technique. However, I do not know how to derive the expression.
Can anyone  give me a hand with the derivation? I know that for a function $f\left(x,y\right)$ subjected to a constraint $g\left(x,y \right)$ , the lagrangian multiplier expression will be: $$
\left\{\begin{matrix}\nabla f\left(x,y\right)=-\lambda\nabla g\left(x,y\right)\\g\left(x,y\right)=0\end{matrix}\right.
$$ Will this still hold for constraints in ODE's?","['lagrange-multiplier', 'ordinary-differential-equations']"
4738839,A sequence derived from $\lceil \sin (2n) \rceil$ with some interesting features,"I was playing with Excel and created a sequence with some interesting features. In column A, list the sequence $\color{red}{a_n=\lceil \sin (2n) \rceil}$ (using the ceiling function ). In column B, list the sequence of gaps between the terms of the previous sequence. In column C, list the sequence of gaps between the terms of the previous sequence. And so on. Let $b_n$ be the sequence in the top diagonal (highlighted). Behold the graph of $|b_n/2^n|$ against $n$ : The terms on the approximate horizontal line are very close to $1/44$ . What's going on here? More specifically: Why does dividing the terms of $|b_n|$ by $2^n$ yield such stability (as opposed to making the terms go toward $0$ or infinity)? Why are there long strings of values that are very close to $1/44$ ? Why does the graph dip to $0$ at $n=698$ and then go back up? How does the graph behave for $n>1000$ ? I only have a fuzzy notion that the last three questions may be related to rational approximations for $\pi$ . For comparison purposes only: graphs derived from other starting sequences If $a_n=\lceil \sin (\color{red}{1}n) \rceil$ , here is the graph of $|b_n/2^n|$ against $n$ : If $a_n=\lceil \sin (\color{red}{3}n) \rceil$ , here is the graph of $|b_n/2^n|$ against $n$ : If $a_n=\lceil \sin (\color{red}{4}n) \rceil$ , here is the graph of $|b_n/2^n|$ against $n$ : If $a_n$ is a random binary sequence with $0$ and $1$ equally likely, here is one possible graph of $|b_n/2^n|$ against $n$ : If $a_n$ is the sequence of prime numbers, here is the graph of $|b_n/2^n|$ against $n$ : (Incidentally, the last two graphs resemble "" Mountains of Guilin "" functions, i.e. $f_{p_j,n}(x)=|\sin (p_1x)+\sin (p_2x)+\dots+\sin (p_nx)|$ where $p_j$ are linearly independent over $\mathbb{Q}$ .) (The answers to this question might resolve an earlier question of mine: Strange dips in sequence $u_n=\log{|(n-1)^{\text{st}}\text{ difference of first $n$ primes}|}$ .)","['number-theory', 'graphing-functions', 'pi', 'sequences-and-series', 'trigonometry']"
4738859,Standard Matrix,"During an exam, we were asked to determine the standard matrix of the linear image, $P: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ which projects a vector onto the $y=-x$ axis. What I did is, I took the unit vector in the direction of the $y=-x$ axis, which is $\begin{bmatrix} 1 \\ -1 \end{bmatrix}$ . Then I used the general formula for vector projection, using an arbitrary vector $\vec{x}$ , with entries $x_1$ and $x_2$ . In the answers model, they give that: $ \begin{bmatrix}
1 & 0 \\
0 & -1 
\end{bmatrix}$ A standard matrix is for projection. So is what I did unnecessarily difficult, or just wrong? In fact, I came up with: $ \begin{bmatrix}
\frac{1}{2} & -\frac{1}{2} \\
-\frac{1}{2} & \frac{1}{2} 
\end{bmatrix}$ After all, I now don't see how my answer matches theirs, if it were right, which I would think? What else is wrong with my approach?","['projection', 'matrices', 'linear-algebra', 'linear-transformations', 'projection-matrices']"
4738865,"A continuous, almost nowhere differentiable function","Consider the function $$f(x)=\sum _{k=0}^{\infty }\frac{\sin \left(k^2\pi x\right)}{k^2}.$$ It converges uniformly on $\mathbb{R}$ according to the Weierstrass M-test and is continuous on $\mathbb{R}$ due to the uniform limit theorem . According to my lecture notes, the termwise differentiated series $\sum_{k=1}^\infty \pi \cos (k^2\pi x)$ diverges. It doesn't state for which values of $x$ it diverges, however, using a similar argument as in this answer , I believe it diverges for all $x\in \mathbb{R}$ (the argument goes like this; whenever $\cos (k^2 \pi x)$ is small, then $k^2 x$ will be close to an odd multiple of $1/2$ , and therefor $(2k)^2 x$ will be close to an even multiple of $1/2$ and correspondingly larger). Now, my lecture notes go on to claim that $f$ in fact is differentiable at some rational points with derivative $-1/2$ . I was wondering if this is true and if yes, if it's easy to show (on an undergraduate level). Also, I'd be grateful if you could explain why the linked argument fails to show divergence on all of $\mathbb{R}$ , if my lecture notes really are correct.","['real-analysis', 'continuity', 'uniform-convergence', 'sequences-and-series', 'derivatives']"
4738874,Differentiable and Non-decreasing with $f' = 0$ a.e. But Not Constant,"Context / Motivation In this question , the OP asks for a proof that if $f : [0,1] \to [0,1]$ is increasing and differentiable with $f(0) = 0$ and $f(1) = 1$ , then the length of the graph $L(f)$ is less than $2$ . As is shown in the answers to that question, if we only know that $f$ is non-decreasing, $f(0) = 0$ and $f(1) = 1$ , we may conclude $L(f) \leq 2$ . However, additional assumptions are required for the strict inequality. If we additionally suppose, for example, that $f' > 0$ on some interval in $[0,1]$ (assuming nothing about the differentiability of $f$ outside of this interval), then we may easily obtain the strict inequality $L(f) < 2$ . By contrast, if we instead suppose that $f$ is differentiable almost everywhere on $[0,1]$ , we cannot guarantee strict inequality. The famous Cantor function has $f(0) = 0$ , $f(1) = 1$ , is non-decreasing, and is differentiable almost everywhere, yet $L(f) = 2$ . The above considerations made me curious how weak the hypotheses on $f$ could be. What seems to have gone wrong with the Cantor function is that the function was not differentiable everywhere, only almost everywhere. This made me wonder whether it would be enough to assume that $f$ is differentiable everywhere. What could go wrong in this case? The only way to still have $L(f) = 2$ would be if $f' = 0$ a.e. But is this possible? Can we have a non-decreasing differentiable function with $f' = 0$ a.e., $f(0) = 0$ , and $f(1) = 1$ ? My Question Does there exist a function $f : [0,1] \to [0,1]$ such that $f$ is non-decreasing $f$ is differentiable everywhere (this rules out the Cantor function) $f' = 0$ almost everywhere $f(0) = 0$ and $f(1) = 1$ Note that through re-scaling and translation this is the same as the question in the title. I have only asked it this way here to be consistent with with the context/motivation above.","['derivatives', 'monotone-functions', 'examples-counterexamples']"
4739009,Wedge product defined on not alternating tensors?,"I am currently reading Calculus on Manifolds by Spivak. In there, it defined wedge product as follows To determine the dimensions of $\Lambda^k(V)$ , we would like a theorem analogous to Theorem 4-1. Of course, if $\omega \in \Lambda^k(V)$ and $\eta \in \Lambda^l(V)$ , then $\omega \otimes \eta$ is usually not in $\Lambda^{k+l}(V)$ . We will therefore define a new product, the wedge product $\omega \wedge \eta \in \Lambda^{k+l}(V)$ by $$
\omega \wedge \eta=\frac{(k+l) !}{k ! l !} \operatorname{Alt}(\omega \otimes \eta)
$$ I am not very sure if here Spivak assumed $\omega \in \Lambda^k(V)$ and $\eta \in \Lambda^l(V)$ , i.e. $\omega$ and $\eta$ are the alternating tensors. In the rest of the text, should I always assume whenever Spivak used the $\wedge$ symbol, he means that both operands are alternating tensors? Or in other words, is there any reason to use this definition on tensors that are not alternating? Later on Spivak defined differential forms with alternating tensors, and I don't think I have experienced the case where $\omega$ and $\eta$ are not alternating. I'm aware of other definitions of wedge products as the product in exterior algebra. However, I don't know anything about exterior algebra, and it seems to be very time consuming for me to learn about it just for this simple question.","['multivariable-calculus', 'exterior-algebra', 'differential-forms', 'differential-geometry']"
4739037,Separability of $B(X)$.,"Assume $X$ is a separable space, or even a countable one. Is then the space of bounded complex-valued functions on $X$ with the supremum norm, $\|f\|_{\infty}=\sup_{x\in X} |f(x)|$ , separable as well? Edit: Assume that $X$ is a countable set. Is the answer still no? All the counterexamples seem to consider $X$ as uncountable.","['functional-analysis', 'real-analysis']"
4739042,How to evaluate $\displaystyle \int_{-\pi/2}^{\pi/2} f(x) dx$ where $f(x)=\cos(x)+\sin(f(x))$,"So I want to find the area of this circular looking thing. I had the following thought process in solving it. Consider the implicit derivative of the function. $\begin{align} y &= \cos(x)+\sin(y) \\ \mathrm{d}y &= -\mathrm{d}x \sin(x)+\mathrm{d}y \cos(y) \\ \mathrm{d}y (\cos (y) -1) &= \mathrm{d}x \sin (x) \\ \dfrac{\mathrm{d}y}{\mathrm{d}x} &= \dfrac{\sin (x)}{\cos(y)-1} \end{align}$ Then consider a parametric equation $r(t)=\begin{bmatrix} x(t) \\ y(t) \end{bmatrix}$ that lines up with the implicit differentiation above i.e. $r'(t)=\begin{bmatrix} x'(t) \\ y'(t) \end{bmatrix}=\begin{bmatrix} \cos(y(t))-1 \\ \sin(x(t)) \end{bmatrix}$ . This is what the vector field $\begin{bmatrix} x \\ y \end{bmatrix} \to \begin{bmatrix} \cos(y)-1 \\ \sin(x) \end{bmatrix}$ looks like superimposed on the graph. The derivative vectors go anticlockwise relative to the graph, which gave me an idea. Take Green's theorem: $\displaystyle \oint_C \vec{F} \cdot \vec{dr} = \iint_R \nabla \times \vec{F} dA$ and apply it to this problem in an interesting fashion. If $\vec{F}(x,y) = \begin{bmatrix}0\\x\end{bmatrix}$ , then $\nabla \times \vec{F} = 1$ which means I can use the line integral to calculate the area. I should maybe point out that this is 2D curl, which in 3D would technically be $\hat{k}$ . I got stuck at this assortment of half-ideas. Of course I can write the equation in terms of x like this: $f(y) = \cos^{-1} (y - \sin(y))$ , do  approximations to get the upper y bound at the crest of the thing like this: $a = \cos(0) + \sin(a) \implies a = 1 + \sin a$ then approximate an integral to get the answer like this: $\text{Area} = 2\displaystyle \int_0^a \cos^{-1} (y - \sin(y)) \mathrm{d}y$ , which gives me this answer to several decimal places: $4.9296448613689990829170456909320900725529417147308463240261037884$ . But that's boring. Is there any other way?","['functional-equations', 'greens-theorem', 'multivariable-calculus', 'calculus', 'partial-differential-equations']"
4739129,Linear Programming conceptual question,"Linear programming involves solving a linear problem subject to the constraint $x \geq  0$ , and minimizing a Cost function for the specific case. The book (Gilbert Strang, Linear Algebra, section on Linear Programming), states that in Linear Programming, if we have $m$ equations, $n$ unknowns ( $n > m$ as an under determined system of equations). Then the number of corners is $n!/m!(n-m)!$ . Taking a plane $x1 + x2 + x3 = 4$ , a corner-example would be $(0,4,0)$ . (useful for reading the extract.) When it is stated that for $m$ equations there is a large number of corners with $m$ non-zeroes, where does this come from? Maybe a single example for 2 3D planes would help me understand better. PS: in this case, according to the definition, corner would always be $(x,0,0)$ , $(0,y,0)$ , $(0,0,z)$ but here is an intuition from the book It is a vector $x \geq 0$ such that at most $m$ components are non-zero, and is a solution to $Ax=b$ . Applied to the example, with a single equation, at most 1 component is non-zero.","['linear-algebra', 'linear-programming']"
4739180,"Find all functions $f∶ \Bbb R \to \Bbb R$ such that, for all $n \in \Bbb N, \left[\frac{{f(x+n) - f(x-n)}}{{2n}} = f'(x)\right]$","There was a similar question in the Putnam ( $2010$ A $2$ ) but I can't wrap my head around this one. I've tried attacking it in some standard ways of eliminating arbitrary functions but it doesn't seem to be an easy manipulation. Also noticed that this is the expression for the average rate change of a differentiable function in the interval $[x-n,x+n]$ but again don't know how I can use that information. Linear and quadratic functions seem to fit the bill but how do you find the other ones or prove that these are the only kinds?","['functional-equations', 'functions', 'derivatives', 'delay-differential-equations']"
4739205,For what real values of $a$ does $\sum\limits_{k=1}^\infty k^{-(1+|\sin k|^a)}$ converge? [duplicate],"This question already has an answer here : Does $\sum_{n=1}^\infty n^{-1-|\sin n|^a}$ converge for some $a\in(0,1)$? (1 answer) Closed 11 months ago . I was thinking about the $p$ -series , and came up with this question: For what real values of $a$ does $S=\sum\limits_{k=1}^\infty \dfrac{1}{k^{1+|\sin k|^a}}$ converge? My attempt: I know that if $a\le 0 $ , then $S\le\sum\limits_{k=1}^\infty \dfrac{1}{k^{2}}=\pi^2/6$ , so $S$ converges. If $a>0$ , the standard tests do not work: The $n$ th term divergence test is inconclusive, because the terms approach $0$ . The comparison test with a $p$ -series test does not apply, because the exponent, $1+|\sin k|^a$ , is not a constant. ( $1+|\sin k|^a$ is always greater than $1$ since $k\in\mathbb{N}$ , but this does not imply that the series converges. For example, $\sum\limits_{k=2}^\infty \dfrac{1}{k^{1+\frac{1}{\ln k}}}=\sum\limits_{k=2}^\infty \dfrac{1}{ek}$ diverges) The integral test does not apply, because $\dfrac{1}{x^{1+|\sin x|^a}}$ is not a decreasing function. The ratio test does not apply, because the limit of the ratio does not exist. The root test is inconclusive, because the limit of the root is $1$ . Numerical investigation: If $a=1$ , numerical investigation suggests that, for large $n$ , $\sum\limits_{k=1}^n \dfrac{1}{k^{1+|\sin k|^a}}\approx c_1\ln (\ln n)+c_2$ where $c_1$ and $c_2$ are constants. If this approximation is valid, then $S$ would diverge when $a\ge 1$ . If $0<a<1$ , numerical investigation does not elucidate what happens.","['real-analysis', 'calculus', 'trigonometry', 'sequences-and-series', 'convergence-divergence']"
4739215,Prove $\frac{1}{\sqrt{a+8b}}+\frac{1}{\sqrt{b+8c}}+\frac{1}{\sqrt{c+8a}}\ge 1$ for $ab + bc + ca = 3$,"Let $a,b,c\ge 0: ab+bc+ca=3.$ Prove that $$\frac{1}{\sqrt{a+8b}}+\frac{1}{\sqrt{b+8c}}+\frac{1}{\sqrt{c+8a}}\ge 1.$$ This problem is from a book. This cyclic inequality becomes an equality at $a=b=c=1.$ I try to used some estimate which based on this point $(1,1,1)$ . By AM-GM $$\sum_{cyc}\frac{1}{\sqrt{a+8b}}\ge 6\sum_{cyc}\frac{1}{a+8b+9}, $$ but $$\frac{1}{a+8b+9}+\frac{1}{b+8c+9}+\frac{1}{c+8a+9}\ge \frac{1}{6}$$ is already wrong at $a=\dfrac{\sqrt{30}}{50},b=5\sqrt{30},c=0.$ The following Cauchy-Schwarz also doesn't help here since $$\sqrt{a+8b}+\sqrt{b+8c}+\sqrt{c+8a}\le 9,$$ is doesn't hold when $a=\dfrac{\sqrt{30}}{50},b=5\sqrt{30},c=0.$ Even $(a+8b)(b+8c)(c+8a)\le 27$ leads wrong inequality with same counter example. Now, I tried Holder $$\left(\sum_{cyc}\frac{1}{\sqrt{a+8b}}\right)^2.\sum_{cyc}\left[(a+8b)(ma+nb+pc)^3\right]\ge \left[(m+n+p)(a+b+c)\right]^3,$$ but it seems hard. I can not find any special $(m,n,p)$ . I think my Holder estimae is not good enough. Hope to see better ideas. Thank you.","['uvw', 'inequality', 'symmetric-polynomials', 'algebra-precalculus', 'holder-inequality']"
4739237,Does pi's decimal expansion certainly repeat once (of course not infinitely many times)??,"My 8-year-old asked me this and I could see arguments for either answer. Is pi guaranteed to have an expansion of the form 3.NNM where N is a finite sequence of digits?  For example, 3.14151415696745... (N=1415) or 3.1415926141592696033... (N=1415926).  Of course these are not the true value of N. Argument for: At any position P in the expansion, The chance of the next P digits being equal to the first P is nonzero. Since there are an infinite number of positions P, eventually we will succeed. Argument against: The chance of succeeding at P=1 is 1 in 10.  (We fail, as pi does not begin with 3.11). The chance of succeeding at P=2 is 1 in 100. (Fail again: pi does not begin with 3.1414).  Etc.  If I play a game where I attempt to succeed at P=1, and then at P=2, and repeat infinitely until I succeed, the fact that the chance at each step becomes 10 times less causes the total probability to converge on a number less than 100%. Which argument is correct, if either? If it's the latter, what is the overall probability that we converge upon?","['pi', 'probability', 'sequences-and-series']"
4739256,Positive-definite matrix with simple structure,"The $n\times n$ matrix $A$ has this specific structure: The diagonal entries $a_{ii}$ are strictly increasing and strictly positive real numbers; The non-diagonal entries are calculated as $a_{ij}=a_{ji}=a_{ii}$ where $i$ is the smaller index. In other words, $a_{ij}=a_{ji}=\min(a_{ii},a_{jj})$ . Is the matrix $A$ positive-definite? For the $1\times1$ case, the result is trivial. For the $2\times2$ case, we can use Sylvester's criterion and see that $a_{11}a_{22}-a_{11}^2>0$ is true, so $A$ is positive-definite. Is there an argument that can be used to extend this result to larger matrices? Some empirical results I generated one million $20\times20$ matrices using an exponential random variable for the diagonal. Every random matrix is positive-definite. I expect this result to be true in general but I just don't find a proof.","['matrices', 'linear-algebra', 'positive-definite']"
4739274,Number of unpaired $x$ and $y$ samples needed to determine $A\in\mathbb{R}^{2\times2}$,"This is a question from an exam on deep learning that I took not so long ago, and I'd like to know the answer. The Setting There is a Gaussian distribution from which we sample $x \in \mathbb{R}^{2}$ values, but what we observe is $y \in \mathbb{R}^{2}$ , where $y = A x$ . We have an oracle that takes as input a number $n \in \mathbb{N}$ and returns $n$ different $x$ values sampled from the Gaussian and $n$ $y$ values corresponding to the $x$ values, but they are unpaired (meaning that if we label the $x, y$ values as $$\begin{array}{c}
x_{1},\ldots,x_{n}\\
y_{1},\ldots,y_{n}
\end{array}$$ there is some permutation $\pi$ such that for all $i$ we have $y_{i}=Ax_{\pi\left(i\right)}$ . The Question What is the minimal number $n$ that we need to give to the oracle, in order to determine $A$ (prove your answer). If it isn't possible with any finite (or even infinite) number of samples - prove it. Are the cases where $A$ is invertible different from the case where it isn't? My Attempt My initial gut feeling was that it's impossible, but then I had the following Idea. Clearly, if we had two paired samples $\left(x_{1},y_{1}\right),\left(x_{2},y_{2}\right)$ we could determine $A$ easily. But since they are unpaired, what we could maybe do is choose $n=3$ and go over all different permutations, using the first two pairs to determine $A$ and check if it fits the 3rd pair. However, I am not sure if this is correct, as perhaps I could find two permutations yielding different $A$ 's. I wasn't able to prove this nor find a counter-example","['permutations', 'matrices', 'linear-algebra', 'decidability', 'sampling']"
4739291,Taylor Series Expansion of a Vector Cross product,"This is confusing me a little. What is the Taylor series expansion of: $$f(u,v) = u \times v$$ Around points (i.e., vectors) $u_0$ and $v_0$ up until the first derivative (i.e., no higher order terms than order 2)? Thank you! Here's what I have so far: so we pick a small values near $(u_0, v_0)$ i.e.,: $$u = \bar{u} + u_0 $$ $$v = \bar{v} + v_0 $$ we want to express $f(u,v)$ as follows: $$f(u,v) \approx f(u_0, v_0) + \frac{\partial f (u,v)}{\partial u}\Big|_{(u_0,v_0)}\,\bar{u} + \frac{\partial f (u,v)}{\partial v}\Big|_{(u_0,v_0)}\,\bar{v}$$ . Which means the Taylor series expansion is given as follows: $$
%
f(u,v) \approx u_0 \times v_0 + \frac{\partial}{ \partial u}(u \times v)\Big |_{(u_0,v_0)}\cdot \bar{u} \ \ + \frac{\partial}{ \partial v}(u \times v)\Big |_{(u_0,v_0)}\cdot \bar{v}
%
$$ $$
= u_0 \times v_0 + (\mathbb{1} \times v_0) \cdot \bar{u} \ \ + (u_0 \times \mathbb{1}) \cdot \bar{v}
$$ where $\mathbb{1}$ is simply a column vector of ones i.e., $\frac{\partial }{\partial u}\begin{bmatrix}u_1 \\u_2 \\u_3 \end{bmatrix} = \begin{bmatrix}1 \\1 \\1 \end{bmatrix}$ . Is this correct? Or should $\mathbb{1}$ be the idenity matrix $I_{3\times3}$ instead?","['multivariable-calculus', 'linear-algebra', 'taylor-expansion', 'functional-analysis', 'partial-differential-equations']"
4739300,"Compute ; $\Gamma=\lim_{n\to\infty} \int_{t_n}^{t_{n+1}} \frac{(f(x-t_n))^{g(t_{n+1}-x)}}{(f(t_{n+1}-x))^{g(x-t_n)}+(f(x-t_n))^{g(t_{n+1}-x)}} \, dx$","Let the functions be defined as $g:R\to R$ and $f:R \to (1,\infty)$ . Both f and g are continuous. Let $F_n$ denote the $n^{th}$ Fibonacci number, then for $n ∈ N$ let $\Gamma$ and $t_n$ be defined as follows; $$t_n=\sqrt [n]{(2n-1)!! F_n}$$ $$\Gamma=\lim_{n\to\infty} \int_{t_n}^{t_{n+1}}
 \frac{(f(x-t_n))^{g(t_{n+1}-x)}}{(f(t_{n+1}-x))^{g(x-t_n)}+(f(x-t_n))^{g(t_{n+1}-x)}}
 \, dx$$ My initial attempt involved the recurrence relation of Fibonacci numbers for $n>1$ ; $\boxed{F_n=F_{n-1}+F_{n-2}}$ and $\boxed{F_0=0, F_1=1}$ . The closed form being; $\boxed{F_n=\frac{1}{\sqrt 5}\left[(\phi)^n-(\psi)^n\right]}$ $\phi$ being the golden ratio and $\psi$ being the conjugate of golden ratio. Not being familiar with the double factorial notation, here's what I have understood/found; $\boxed{n!!=\sum_{k=0}^{\lceil \frac{n}{2} \rceil-1}(n-2k)=n(n-2)(n-4)\cdots}$ and $\boxed{n!=n!!(n-1)!!}$ $$\boxed{t_n=\sqrt[n] {\frac{(2n-1)!!(\phi^n-\psi^n)}{\sqrt5}}}$$ $$\boxed{t_{n+1}=\sqrt[n+1] {\frac{(2n+1)!!(\phi^{n+1}-\psi^{n+1})}{\sqrt5}}}$$ Substituting, $t_n=\sqrt [n]{(2n-1)!! F_n}$ and $t_{n+1}=\sqrt [n+1]{(2n+1)!! F_{n+1}}$ . Let $$\Gamma=\lim_{n\to\infty} \beta_n$$ where $\beta_n$ is; $$\boxed{\int_{\sqrt [n]{(2n-1)!! F_n}}^{\sqrt [n+1]{(2n+1)!! F_{n+1}}} \frac{(f(x-\sqrt [n]{(2n-1)!! F_n}))^{g\left(\sqrt [n+1]{(2n+1)!! F_{n+1}}-x\right)} \, dx}{(f(\sqrt [n+1]{(2n+1)!! F_{n+1}}-x))^{g\left(x-\sqrt [n]{(2n-1)!! F_n}\right)}+(f(x-\sqrt [n]{(2n-1)!! F_n}))^{g\left(\sqrt [n+1]{(2n+1)!! F_{n+1}}-x\right)}}}$$ On just a little simplification; $$\boxed{\int_{\sqrt [n]{(2n-1)!! F_n}}^{\sqrt [n+1]{(2n+1)!! F_{n+1}}} \frac{1}{\left[\frac{\left(f\left(\sqrt [n+1]{(2n+1)!! F_{n+1}}-x\right)\right)^{g\left(x-\sqrt [n]{(2n-1)!! F_n}\right)}}{\left(f\left(x-\sqrt [n]{(2n-1)!! F_n}\right)\right)^{g\left(\sqrt [n+1]{(2n+1)!! F_{n+1}}-x\right)}}\right]+1}\,dx}$$ I'm now left more confused than before, was this the right approach at all? If so, how does one continue from here? Note - This question did not come with the solution, so I am not certain if the limit converges (if that is necessary)","['integration', 'calculus', 'recurrence-relations']"
4739328,Can the limit at an essential singularity be infinite from all directions?,"Suppose a function $f$ is holomorphic in a punctured neighborhood of a point $z_0 \in \mathbb{C}$ . Suppose that for all fixed $\theta \in \mathbb{R}$ , $\lim_{r \to 0} |f(re^{i\theta})|=\infty$ . Does it necessarily follow that $f$ has a pole at $z_0$ ? Or is it possible for the singularity to be essential? (Recall that if $\lim_{z \to z_0} |f(z)|=\infty$ , then $f$ must have a pole. But I'm requiring only the limit from any fixed direction to be infinite.) If the latter, this should be one of those funny counterexamples in analysis. On the other hand, the compactness of $S^1$ vaguely suggests to me that it has to be a pole.","['complex-analysis', 'limits', 'singularity']"
4739335,Is a killed Feller process again Feller?,"I am interested in the question whether a Feller process retains the Feller property when we kill it upon entering a closed subset of the state space. Setup Let $S$ be a locally compact topological Hausdorff space and $X = (X_t)$ a strong Markov process on some probability space $(\Omega, \mathcal{A}, \mathbb{P})$ with values in the one point compactification $S \cup \{\vartheta\}$ of $S$ . For $f \in \mathcal{B}_b$ bounded and (Borel-)measurable and $t>0$ denote by $$
P_t f(x) = \mathbb{E}_x[f(X_t)]
$$ the semigroup of $X$ . Write $C(S)$ for the continuous functions $S \to \mathbb R$ and $C_\infty(S) \subset C(S)$ for the continuous functions vanishing at infinity and $C_b(S)$ for the bounded continous functions. I am working with the following definition of the Feller property: (F1) $\lim_{t \to 0} P_t f(x) = f(x)$ for all $x \in S$ and $f \in C_\infty(S)$ , (F2) $P_t f \in C_\infty(S)$ for all $f \in C_\infty(S)$ , (F3) $P_t f \in C_b(S)$ for all $f \in \mathcal{B}_b(S)$ . We say that a Markov process $X$ is Feller if its semigroup $(P_t)_t$ satisfies (F1) and (F2) . We say that $X$ is strongly Feller if $(P_t)_t$ satisfies (F1) and (F3) and we say that $X$ is doubly Feller if $(P_t)_t$ satisfies (F1) , (F2) and (F3) . We always assume that we are working with a modification of a Feller process that has cadlag paths. Now suppose $A \subset S$ is closed and write $$
\tau_A := \inf\{t>0\ \mid\ X_t \in A\}
$$ for the first hitting time of $A$ . We define the killed process, more specifically the process killed upon hitting $A$ , $X^A$ by $$
X_t^A = X_t, \quad t< \tau_A \qquad X_t^A = \vartheta, \quad t \geq \tau_A.
$$ Observe that the new point at infinity (or cemetery point) $\vartheta$ may not coincide with the original one. I can show that $X^A$ is again a strong Markov process with values in $D=S\setminus A$ and semigroup $$
P_t^A f(x) = \mathbb{E}_x\left[ f(X_t^A) \right] = \mathbb{E}_x[f(X_t);\ t < \tau_A].
$$ Note that $D$ is open and therefore locally compact and regular in the sense that $\mathbb{P}_x( X_0 = x) = 1$ for all $x \in D$ . Further note that we can identify $$
C_\infty(D) = \{f \in C_\infty(S)\ \mid \ f|_A = 0\}.
$$ On the other hand, every $f \in C_\infty(D)$ or $f \in \mathcal{B}_b(D)$ is naturally extended to $C_\infty(S)$ and $\mathcal{B}_b(S)$ , respectively, by setting $f=0$ on $A$ . Question Is it true that $X^A$ is again Feller if $X$ is Feller? Is it true that $X^A$ is again strongly Feller if $X$ is strongly Feller? Literature Strangely enough I could not find many results in this direction in the literature. Maybe I was searching for the wrong keywords? What I found however is a paper by Chung (1986) https://doi.org/10.1142/9789812833860_0051 where he proves that the killed process is again doubly Feller, when $X$ is doubly Feller. The proof relies on the observation that for every $f \in \mathcal{B}_b(D)$ , $$
P_t^A f(x) = \mathbb{E}_x[ \psi_s(X_s);\ s < \tau_B],
$$ where $$
\psi_s(x) = \mathbb{E}_x[f(X_{t-s});\ t-s < \tau_A].
$$ Then, $\psi_s \in \mathcal{B}_b(S)$ and by the strong Feller property $P_s \psi_s \in C_b(D)$ and we can deduce that for all $x \in D$ , $$
\left| P_t^A f(x) - P_s \psi_s(x) \right| \leq P_x(\tau_A \leq s) ||\psi_s||.
$$ Since the right hand side converges uniformly on compacta to $0$ as $s \to 0$ , we can conclude that $P_t^A f \in C_b(D)$ . Detailed Question I am trying to show that the simple Feller property (F2) is retained by the killed process, i.e. that $P_t^A: C_\infty(D) \to C_\infty(D)$ . One possible way would be to go the analytical way and use Dirichlet forms (there are results on part processes in the theory of Dirichlet forms, namely Theorem 3.3.9 in the book by Chen and Fukushima https://www.jstor.org/stable/j.ctt7s6w6 ) or Hille-Yosida. I am trying to find a more probabilistic argument. Inspired by Chung's argument I am trying to approximate $P_t^A$ by $C_\infty(D)$ -functions. I would be very indebted if anyone can give me a hint in the right direction. Either an idea how to proof the statement, a reference or a counter example would be extremely helpful. Thank you! Edit: Clarification about cadlag paths.","['stochastic-processes', 'markov-process', 'probability-theory', 'probability']"
4739346,Pathwise differentiability of stochastic integrals,"My question: Is there a necessary and/or sufficient condition we can place on suitable continuous $f : [0,\infty) \rightarrow \mathbb{R}$ which allows us to determine whether the process $$X_t := \int_0^t f(t-s) dW_s$$ has differentiable paths? I was motivated to ask this after looking at the SDE in this thread : \begin{align}
dX_t &= Y_t dt \\\\
dY_t &= -X_t dt + b dW_t
\end{align} to which I found the solution, when $X_0 = Y_0 = 0$ , \begin{align}
X_t &= b \int_0^t \sin(t-s) dW_s \\\\
Y_t &= b \int_0^t \cos(t-s) dW_s \\\\
\end{align} Now, from the SDE and the pathwise continuity of solutions, it is clear to see that $X_t$ will be pathwise differentiable (with derivative $Y_t$ ), whereas $Y_t$ will not be. However this was unintuitive to me just looking at the integrals themselves! I know from this thread that we cannot hope for differentiable paths when we replace the integrand with just $f(s)$ . Moreover, I know that convolutions with white noise of the form \begin{equation}
Y_t := \int_{-\infty}^\infty f(t-s) dW_s
\end{equation} are pathwise differentiable if the square of the Fourier transform of $f$ decays faster than $|k|^{-1}$ as $|k| \rightarrow \infty$ (the square of the Fourier transform of $f$ is the spectral density of $Y$ ). However, I don't think this result holds in our case since we aren't integrating over the whole line and, in any case $\sin$ and $\cos$ are not $L^1$ .","['stochastic-integrals', 'stochastic-analysis', 'probability', 'stochastic-processes', 'stochastic-calculus']"
4739347,About the integral $\int_{\frac{1}{2}}^{2}\ln\left(\frac{\ln\left(x+\frac{1}{x}\right)}{\ln\left(x^{2}-x+\frac{17}{4}\right)}\right)dx$,"$$I=\int_{\frac{1}{2}}^{2}\ln\left(\frac{\ln\left(x+\frac{1}{x}\right)}{\ln\left(x^{2}-x+\frac{17}{4}\right)}\right)dx$$ Using CAS the answer comes out to be: $$I=-\frac{3}{2}\ln(2)$$ For the Indefinite Integral, Wolfram says it has no solution in terms of standard mathematical functions. The limits of the Definite Integral do give out the hint of replacing $x \to \frac{1}{x}$ , but after trying it, I do not end up with a solvable Integral either. $$I=\int_{\frac{1}{2}}^{2}\ln\left(\frac{\ln\left(x+\frac{1}{x}\right)}{\ln\left(\frac{1}{x^{2}}-\frac{1}{x}+\frac{17}{4}\right)}\right)\frac{1}{x^{2}}dx$$ Adding these two integrals which usually helps does not resolve the problem here either.","['integration', 'indefinite-integrals', 'calculus', 'definite-integrals']"
4739377,Minimum & maximum options for simple graph connected component,"Let $G = \langle V, E \rangle$ be a simple graph with no simple circles. Suppose that $|E| = 11$ s.t for each $u \in V : deg(u) \in$ { $1, 2, 3$ }. Moreover, there are exactly $10$ vertices with degree of 1 in the graph $G$ . Find the minimal & maximal number of connected components in $G$ . I showed that $G$ is not connected itself (namely, it has at least two connected components) by bounding the sum of the degrees, which equals to $22$ (under these assumptions). Then, I argued there are at least three connected components by the way of contradiction & using the observation that although $G$ is not a tree itself, each component of it is a tree, and by denoting $x_1, y_1$ the number of vertices and edges respectively of the component $R_1$ . Then it followed that $|V| = 13$ but this is impossible since $22 = 2|E| \leq 10 + (3 + 3 + 3) = 19$ .
I found an example for 3 components and then found the maximum by using the same observation above, which implies there are at least 2 leafs at each component. I feel like I couldve abstracted it more or found the lower bound at once instead of going one after one. I've already solved some questions of the same type and sometimes the lower bounds are higher which requires a lot more effort. I'll be glad if someone who is more experienced could've shed some light\point out some observations I can make at these problems which I haven't. Thanks!","['graph-theory', 'extremal-graph-theory', 'discrete-mathematics']"
4739383,"If $(a_n)$ is a decreasing real sequence and $\sum a_n$ converges, then does $\sum (-1)^n n a_n\ $ converge?","""Motivation""/Introduction: If $(a_n)$ is a decreasing real sequence and $\displaystyle\sum a_n $ converges, then $n a_n \to 0,\ $ for example, by the Cauchy Condensation test . If $(a_n)$ is a real sequence and $\displaystyle\sum a_n $ converges but $(a_n)$ is not necessarily decreasing, then does $n a_n\to 0?$ No: for example, take: $
a_n:=
\begin{cases}
 2^{-n}&\text{if}\, n\neq 2^k\\
\frac{1}{k^2}&\text{if}\, n = 2^k\\
\end{cases}
$ Therefore, if $(a_n)$ is a real sequence and $\displaystyle\sum a_n $ converges but $(a_n)$ is not necessarily decreasing, then $\displaystyle\sum n a_n\ $ and $\displaystyle\sum (-1)^n n a_n\ $ do not necessarily converge, by the counter-example above. $$$$ This leads to the following question, for which I do not have an answer: If $(a_n)$ is a decreasing real sequence and $\displaystyle\sum a_n $ converges, then does $\displaystyle\sum (-1)^n n a_n\ $ converge? I was thinking we could apply Dirichlet's test or Cauchy's Condensation test somehow, but I don't quite see how.","['convergence-divergence', 'sequences-and-series', 'alternating-expression', 'real-analysis']"
4739395,Invariant measure for wrapped diffusion,"Consider the diffusion $${\rm d}X_t=b(X_t){\rm d}t+\sigma(X_t){\rm d}W_t\tag1$$ on $\mathbb R^d$ . Denote the solution starting at $x\in\mathbb R^d$ by $X^x$ . Let $$\kappa_t(x,B):=\operatorname P\left[X^x_t\in B\right]\;\;\;\text{for }(x,B)\in\mathbb R^d\times\mathcal B(\mathbb R^d)\text{ and }t\ge0$$ denote the transition semigroup. Now consider the wrap around $$\iota:\mathbb R\to[0,1)\;,\;\;\;x\mapsto x-\lfloor x\rfloor=x\operatorname{mod}1$$ and let $$(\iota(x))_i:=\iota(x_i)\;\;\;\text{for }x\in\mathbb R^d.$$ Assume $b(x+k)=b(x)$ and $\sigma(x+k)=\sigma(x)$ for $k\in\mathbb Z^d$ and $x\in\mathbb R^d$ . Now let $$\tilde X^x_t:=\iota(X^x_t)\;\;\;\text{for }(t,x)\in[0,\infty)\times[0,1)^d$$ and $$\tilde\kappa_t(x,B):=\operatorname P\left[\tilde X^x_t\in B\right]\;\;\;\text{for }(x,B)\in[0,1)^d\times\mathcal B([0,1)^d)\text{ and }t\ge0.$$ We easily see that $$\tilde\kappa_t(x,\;\cdot\;)=\iota(\kappa_t(x,\;\cdot\;))\tag2$$ for all $x\in[0,1)^d$ and $t\ge0$ . Assume $\mu$ is a probability measure on $\mathbb R^d$ and $\mu$ is invariant with respect to $(\kappa_t)_{t\ge0}$ . Question : Can we show that the pushforward $\iota(\mu)$ is invariant with respect to $(\tilde\kappa_t)_{t\ge0}$ ? This claim seems to be made in Proposition 1-i of this paper . However, I don't get why it holds. Let $t\ge0$ . Since $\mu$ is $\kappa_t$ invariant we have $\mu\kappa_t=\mu$ . Building the pushforward under $\iota$ on both sides yields $$\int\mu({\rm d}x)\kappa_t(x,\iota^{-1}(B))=\iota(\mu\kappa_t)(B)=\iota(\mu)(B)\tag3$$ for all $B\in\mathcal B([0,1)^d)$ . This is not precisely what we need. On the other hand, $$(\iota(\mu)\tilde\kappa_t)(B)=\int\mu({\rm d}x)\kappa_t(\iota(x),\iota^{-1}(B))\tag4$$ for all $B\in\mathcal B([0,1)^d)$ . It seems like $\iota$ is occurring one time too often ... What am I doing wrong here? EDIT : In the paper, they are assuming that $\kappa_t$ and $\mu$ both have densities with respect to the $d$ -dimensional Lebesgue measure. However, I don't see how this assumption helps. EDIT2 : Maybe my definition of $\tilde\kappa_t$ is wrong. Maybe it should be $$\tilde\kappa_t(x,B):=\kappa_t(\iota^{-1}(x),\iota^{-1}(B))=\sum_{k,\:l\:\in\:\mathbb Z^d}\kappa_t(k+x,l+B)\tag5$$ for $(x,B)\in[0,1)^d\times\mathcal B([0,1)^d)$ instead ... However, even with this definition I don't see why the claim holds. (I'm abusing notation a bit in $(5)$ , since $\iota^{-1}(x)$ is actually the set $\bigcup_{k\in\mathbb Z^d}\{k+x\}$ .)","['measure-theory', 'stochastic-analysis', 'stochastic-processes', 'markov-process', 'probability-theory']"
4739440,Why can't we compute this series using Riemann's sum?,"Consider a series, $$S=\sum_{n\ge0}\frac{(-1)^n}{2n+1}$$ Which can also be written as, $$S=\lim_{n\rightarrow\infty}\sum_{r=0}^{n} \frac{2n}{(4r+1)(4r+3)}\cdot \frac{1}{n}$$ Substituting $$\frac{r}{n}=x,r=nx,\frac{1}{n}=dx$$ Gives, $$S=\int_{0}^{1}\frac{2n}{(4nx+1)(4nx+3)}\cdot dx$$ Which gives, $$S=\frac{\log(12n+3)-\log(4n+3)}{4}$$ As $n$ goes to infinity $S$ goes to $\frac{\log{3}}{4}$ . But this page states the sum to be $\frac{\pi}{4}$ . I can't figure it out that what am I missing in this approach? Can it be solved using Riemann's sum?","['power-series', 'riemann-sum', 'riemann-integration', 'sequences-and-series']"
4739464,A group generated by two elements with uncountably many normal subgroups,"Proposition. There exists a group generated by two elements with uncountably many normal subgroups. The constructive proof below appears in Geometric Group Theory: An Introduction by Clara Löh (Proposition $2.2.30$ ). Proof. The basic idea is as follows: We construct a group $G$ generated by two elements that contains a central subgroup $C$ (i.e., each element of this subgroup is fixed under conjugation by all other group elements) isomorphic to the big additive group $\bigoplus_{\Bbb N} \Bbb Z$ . The group $C$ contains uncountably many
subgroups (e.g., given by taking subgroups generated by the subsystem of the
unit vectors corresponding to different subsets of $\Bbb N$ ), and all these subgroups
of $C$ are normal in $G$ because $C$ is central in $G$ . What is the explanation for why $\bigoplus_{\Bbb N} \Bbb Z$ has uncountable many subgroups? I don't understand the stuff about the ""subsystem of unit vectors"" corresponding to subsets of $\Bbb N$ . An example of such a group is $G := \langle s,t \, \vert \, R\rangle$ , where $$R := \{[[s,t^nst^{-n}],s]: n \in \Bbb Z\} \cup \{[[s,t^nst^{-n}],t]: n \in \Bbb Z\}$$ Let $C$ be the subgroup of $G$ generated by the set $\{[s,t^nst^{-n}]]: n \in \Bbb Z\}$ . All
elements of $C$ are invariant under conjugation with $s$ by the first part of the
relations, and they are invariant under conjugation with $t$ by the second part
of the relations; thus, $C$ is central in $G$ . Moreover, using the so-called calculus
of commutators, it can be shown that $C$ contains the additive group $\bigoplus_{\Bbb N} \Bbb Z$ . For the last part, we must construct an injective group homomorphism $\varphi: \bigoplus_{\Bbb N} \Bbb Z \to C$ . It is enough to define $\varphi$ on a generating set of $\bigoplus_{\Bbb N} \Bbb Z$ , for which the ideal pick is $S := \{e_n: n \in \Bbb N\}$ where $e_n = (0, \ldots, 0, 1, 0, \ldots)$ has a $1$ in the $n$ th coordinate and $0$ s elsewhere for each $n\ge 1$ . The relations in $G$ can be rewritten as $[st^nst^{-n}s^{-1}t^ns^{-1}t^{-n},s] = e_G$ and $[st^nst^{-n}s^{-1}t^ns^{-1}t^{-n},t] = e_G$ for all $n \in \Bbb Z$ where $e_G \in G$ is the identity element. How should I define $\varphi$ ? I don't see why the commutators $[[s,t^nst^{-n}],s]$ and $[[s,t^nst^{-n}],t]$ are useful to consider in the first place. Edit: As in Derek's answer, I am tempted to map generators to generators, i.e., define $\varphi(e_n) = [s, t^n s t^{-n}]$ for all $n \in \Bbb N$ . However, I haven't been able to check if $\varphi$ is injective. Thanks for any help!","['proof-explanation', 'group-theory', 'abstract-algebra', 'free-groups']"
4739481,Solving $f(z) = f( z-z^2)$ for $f : \mathbb{C} \to \mathbb{C}$ analytic,"This is a question that I came across in a math competition and I solved it the following way. But unfortunately, I received 0 out of 20. Are there any errors in my reasoning? $f: \mathbb{C}\to \mathbb{C}$ and $f$ is analytic everywhere in the complex plane. $\forall z \in C$ we have $f(z)= f\left(z-z^2\right)$ . Prove that $f(z)$ has to be a constant in the entire plane. My approach: Since $f$ is analytic everywhere, therefore its Taylor series expansion about $z=0$ has to be convergent $\forall z\in C$ . Therefore we have: $$
\begin{split}
f(z)     &= f(0) + f'(0) z + \frac{f''(0)}{2} z^2 + \ldots \\
f(z-z^2) &= f(0) + f'(0)(z-z^2) + \frac{f''(0)}{2} (z-z^2)^2 + \ldots
\end{split}
$$ So the two series are equal for all complex inputs, therefore the coefficients of the same powers of $z$ have to be equal. So I went on calculating the coefficient of $z^2$ in the second series and concluded that $\frac{f''(0)}{2} = - f'(0) + \frac{f''(0)}{2}$ . So $f'(0)=0$ . I decided to prove by induction that $\forall n \in N, f^n(0)=0$ and therefore, the proof will be complete. Suppose that $f'(0) = f''(0) = ... = f^n(0)=0$ . If we can prove that $f^{n+1}(0)=0$ , the induction will be complete. By the hypothesis of the induction we have: $$
f(z) = f(0) + \frac{f^{n+1}(0)}{(n+1)!}z^{n+1}
     + \frac{f^{n+2}(0)}{(n+2)!}z^{n+2} + \ldots 
$$ and therefore, $$
f(z-z^2) = f(0) + \frac{f^{n+1}(0)}{(n+1)!}(z-z^2)^{n+1}
         + \frac{f^{n+2}(0)}{(n+2)!}(z-z^2)^{n+2} + \ldots
$$ By using the Newton's binomial formula I calculated the coefficients of $ z^{n+1}$ and $z^{n+2}$ and concluded that $ \frac{f^{n+2}(0)}{(n+2)!} = \frac{f^{n+2}(0)}{(n+2)!}- (n+1)\frac{f^{n+1}(0)}{(n+1)!}$ and finally $f^{n+1}(0) = 0$ Therefore, $f(z) = f(0)  \  \forall z \in C$ .","['functional-equations', 'complex-analysis', 'solution-verification', 'taylor-expansion', 'induction']"
4739488,What do the secant and tangent functions have to do with their respective lines? [duplicate],"This question already has answers here : Why do we need so many trigonometric definitions? (7 answers) Closed 10 months ago . SO I know that a tangent line is a line that touches a curve at one point, and a secant line is a line that touches a curve at TWO points, but what do tangent and secant lines have to do with tangent (tan(x)) and secant (sec(x)) functions?","['calculus', 'trigonometry', 'terminology']"
4739494,Markov Process and Exponential Growth Rate: is this a martingale?,"Suppose we have a Markov process $X=(X_t)_{t\geq 0}$ , taking values on $\mathcal{N}$ , with this Q-matrix (Example 17.27 of Probability Theory, 3rd version, by A. Klenke): $$
q(x,y) = 
\begin{cases}
x &\text{if } y=x+1\\ 
-x &\text{if } y=x\\
0 &\text{otherwise}
\end{cases}
$$ Is $W_t=e^{-t}X_t$ a martingale? I know that $P_1[X_t>e^tx]=(1-e^{-t})^{\lfloor e^tx\rfloor}$ from a previous result in the same book above. To prove if it is a martingale, I am thinking about the trick about the expected value of a positive random variable but with no success. I would also like to know if the time growth was linear, how could I spot it? Let me know if more context is needed. Thank you.","['martingales', 'markov-process', 'probability-theory', 'markov-chains']"
4739507,What is ⨋ (U+2A0B: summation with integral) used for?,"I initially thought that symbol ⨋ is some sort of joke, but apparently it is used, and it made it's way to Unicode in 2002.
The only reference to its meaning or usage I found is its site on wiktionary , which references ""The principles of modern thermodynamics"" by Deffner and Campbell: The distribution of work values is then given by averaging over an ensemble of realizations of the same process, $ P(W) = \left< \delta(W - W[\left| m\right>; \left| n \right>] )\right>$ which can be rewritten as $ P(W) = \operatorname{⨋}_{m, n}\delta(W - W[\left| m\right>; \left| n \right>] )p(\left|m\right> \to \left|n\right>)$ .
In the latter equation the symbol ⨋ denotes that we have to sum over the discrete part of the eigenvalue spectrum and integrate over the continuous part. I admit that I have no Idea what author is talking about, but it seems that the ""summation over the discrete part and integration over the continuous part"" could be achieved by integrating with respect to adequate measure.
Since integral itself can be viewed as generalization of summation, why bother with this ⨋ symbol? I think it could be some sort of notation convention used by physicists to link visuals to the method of computation involved, but that's just my blind guess. Does this symbol have formal (precise) meaning? Where is it used?","['integration', 'summation', 'notation', 'mathematical-physics']"
4739538,what is the exact meaning of the identity relation? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 11 months ago . Improve this question I would like some clarification regarding the exact meaning of the identity relation. Specifically, if $a=b$ , does this mean $(1)$ $a$ is the very same object as $b$ or does it leave open the possibility that $(2)$ $a$ and $b$ are two distinct objects having all properties in common Note that $(1)$ above implies $a$ and $b$ have all properties in common, but the converse does not appear to be necessarily true. In other words, it seems to me there can exist two objects, not one and the same, that are exact duplicates of one another in every respect. This feels like deep philosophical waters. Ultimately, I just want clarification on the exact meaning of the identity relation. Thanks!","['logic', 'relations', 'discrete-mathematics', 'elementary-set-theory', 'philosophy']"
4739564,Expected time until you reach the top of staircase in a dice game,"I received this question during an online assessment and it continues to bug me. The setup is the following: you have a staircase with $100$ stairs in front of you, and at each time step $t$ you roll a fair six-sided die. You climb the same number of stairs as your roll except if you roll the same amount as the previous time step. If you roll the same number at time $t$ as you did in time $t - 1$ then you take a step back. For example, if you start on the $0$ th step and roll a $6$ , you go to stair $6$ . Then if you roll a $6$ again, you take one step back to be on stair $5$ . What is the expected number of rolls until you reach the top of the staircase? This was my reasoning: let $X_t$ be a random process denoting the position of the player after t rolls and let $d_t$ be the roll at time t. $E[d_t] = 3.5$ and we can calculate the expectation of $X_t$ using the law of iterated expectations and the law of total expectation. But first we consider the term $E[X_{t}|d_t \neq d_{t-1}] = X_{t-1} + E[d_t|d_t \neq d_{t-1}] = X_{t-1} + \frac{(1+2+...+6 - d_{t-1})}{5} = X_{t-1} + \frac{21-d_{t-1}}{5}.$ Then, \begin{align*}
E[X_t] &= E[E[X_t|d_{t-1}]] = E\left[\frac{1}{6}(X_{t-1} - 1) + \frac{5}{6}(X_{t-1} + \frac{21-d_{t-1}}{5})\right] \\
& = E\left[X_{t-1} -\frac{d_{t-1}}{6} + \frac{20}{6} \right] = E[X_{t-1}] + \frac{11}{4}
\end{align*} So it seems that at each time step, we are advancing on average $2.5$ stairs. Since $X_t$ is not a martingale , I cannot simply use the optional stopping theorem to calculate the stopping time. This is where I doubt my calculation. $E[X_t]$ is a recurrence relation with a solution of $E[X_t] = 3.5 + 2.75(t-1)$ , so what if we simply calculate the time until the expected position is at least $100$ ? Doing that we find $t = 36.0909...$ so would our answer then be $t = 37$ ? I suppose another way to do it is by using random variables $t_{i}$ to denote the time until you reach the end of the staircase starting at staircase $i$ . Then of course $E[t_{100}] = 0$ , $E[t_{99}] = \frac{5}{6}(1) + \frac{1}{6}E[t_{98}]$ but this soon gets complex as to calculate $E[t_{98}]$ we have to keep track of $d_{98}$ and $d_{97}$ and so on and so on.","['puzzle', 'markov-chains', 'probability']"
4739573,Solve the integral $\int\frac{1}{4x^2 + 9} dx$,"Im a newish calc 1 student and i have been trying to solve the integral: $$\int\dfrac{1}{4x^2 + 9} dx $$ I was able to solve the integral $\int\frac{1}{x^2 + 1} = \arctan(x) + C$ , by doing the method of $x = \tan(u)$ but with the factor of $4$ in the $x^2$ term and + $9$ I'm unsure how to go about solving this, when I put it into wolfram alpha it spits out $$\frac{1}{6} \arctan\left(\frac{2x}{3}\right) + C$$ But I'm unsure how its came to this answer and cant find any explanations online. I'm not as interested in the actual answer to this specific question as I am how the difference in the terms leads to the different answer Any help would be greatly appreciated.","['integration', 'calculus']"
4739619,Method of Dominant Balance with high order system,"This question comes from Bender and Orszag's Asymptotic Methods and Perturbation Theory.
I'm practicing applying the method of dominant balance to study behavior as $x\to \infty$ for systems which have an irregular singular point at infinity. My question is about how to find what Bender and Orszag call the leading behavior, after having found the controlling factor, especially in systems with high order derivatives.
For example, the system $x^3 d^5y/dx^5 = y$ has an irregular singular point at $\infty.$ We ansatz $y=e^{S(x)}$ , and make the standard assumption that $S'' \ll (S^\prime)^2$ as $x\to \infty$ . It is easy to compute that $y'' = (S''+(S')^2)e^s \sim (S')^2 e^s$ , which then implies that $y''' \sim ((S')^3+2S'S'')e^S$ . However, by assumption, $S'S''\ll (S')^3$ , so $y''' \sim (S')^3 e^S.$ Continuing in this way, we find that $d^5y/dx^5 \sim (S^\prime)^5 e^S$ as $x\to \infty$ . The asymptotic relation for $y$ then reads $x^3 (S')^5 e^S \sim e^S$ . This is easily solved for $S$ : $$S \sim \frac{5 \omega x^{2/5}}{2},$$ where $\omega$ is a $5$ th root of unity. Now that we have the controlling factor $S\sim\frac{5 \omega x^{2/5}}{2}$ , we seek the leading behavior by searching for a $C(x)$ such that $S(x) = \frac{5 \omega x^{2/5}}{2} + C(x)$ and $C(x)\ll \frac{5 \omega x^{2/5}}{2}$ as $x\to \infty$ . This is where I have a question. In computing the aymptotic relation satisfied by $S$ , we were able to simplify the derivatives of $e^S$ by repeatedly making use of the assumption that $S''\ll (S')^2$ . However, now that I want to find $C$ , it seems that I can't do the same. Indeed, if we do, then we would find ourselves with the relation $$(S')^5 \sim x^{-3}$$ $$((\frac{5 \omega x^{2/5}}{2} + C)')^5 \sim x^{-3}$$ which after some simplification leaves us with $$C' \sim 0,$$ which is obviously useless. It therefore seems like what we need to do is actually fully compute $\frac{d^5}{dx^5}\left ( \exp(\frac{5 \omega x^{2/5}}{2} + C(x)) \right )$ and then do dominant balance on the result. This seems like a huge pain, because the derivative will generate a heap of terms. It feels like this can't be how the method works. My question is: What is the ""right"" way of finding an asymptotic relation for $C(x)$ in situations like this? Thank you for your time.","['asymptotics', 'ordinary-differential-equations', 'perturbation-theory']"
4739696,making sense of $A\subseteq P(A)$,"I am trying to make sense of Problem 3.3.4 in Velleman's How to prove it. I don't understand how the following can be a valid expression: $$A\subseteq \mathcal{P}(A)$$ So $A$ is contained in its power set. What this means is that every element of $A$ is an element of $\mathcal{P}(A)$ But how can this be? $A$ is a set and an element of A is just a element (not sure how to express this here). However $\mathcal{P}(A)$ is a family of sets and an element of this is a set.
So how does this expression make any sense at all?
The solution to the problem is already here , but I am just trying to make sense of the notation not the actual solution.",['elementary-set-theory']
4739737,Evaluate $ \lim_{n \to \infty} \sin^n\left(\frac{2\pi n}{3n+1}\right)$,I'm trying to solve this limit rigorously $ \lim_{n \to \infty} \sin^n\left(\frac{2\pi n}{3n+1}\right)$ I can see that the answer is $0$ as $$\lim_{n \to \infty} \sin\left(\frac{2\pi n}{3n+1}\right) = \sin\left(\frac{2\pi}{3}\right) = \frac{\sqrt{3}}{2} \lt 1$$ But this solution doesn't seem rigorous and I'm unable to come up with a rigorous approach,"['limits', 'calculus', 'limits-without-lhopital', 'real-analysis']"
4739760,Derivative of the derivative of a neural network w.r.t. itself,"I'm trying to find the following derivative: \begin{equation}
\frac{\partial^2 f}{\partial f \partial x}
\end{equation} where $f$ is a neural network, and $x$ is an input.
To be more accurate, let's say $f$ is a 2-layer neural net with a 1-D output and 1-D input. Then, we can write $f$ as: $$
f(x) = tanh(xW_1 + b_1)W_2 + b 
$$ I tried deriving the formula for $\frac{\partial f}{\partial x}$ and got the following: $$
\frac{\partial f}{\partial x} = W_2^T\times (W_1^T\circ tanh'(xW_1 + b_1))
$$ where $\circ$ is the element-wise product. Now, I need to take its derivative w.r.t. $f$ itself. Any ideas on how to proceed from here? Edit: As @NinadMunshi pointed out, I intend to find the following derivative after a variable change: $$
z = tanh(xW_1 + b_1)W_2 + b\\
g = \frac{\partial f}{\partial x} = W_2^T\times (W_1^T\circ tanh'(xW_1 + b_1))\\
\frac{\partial g}{\partial z} = ?
$$","['partial-derivative', 'derivatives', 'neural-networks']"
4739808,Why can't I use the infinite geometric series formula for this question?,"$$\sum_{n=1}^{\infty} \frac{12}{(-5)^n}$$ If I rewrite this as $\sum_{n=1}^{\infty} 12(-\frac{1}{5})^n$ and then use the infinite geometric series formula, $\frac{a}{1-r}$ where $a = 12$ and $r = (-\frac{1}{5})$ ? I get 10 but since it starts at 0, I subtracted one from my total equation to get 9, when my textbook says the answer is -2. There must be something I'm doing wrong or misunderstanding.","['summation', 'calculus', 'sequences-and-series', 'limits', 'geometric-series']"
4739824,"Why is it ""obvious"" that the expected value of a continuous uniform distribution is (a+b)/2?","For a continuous uniform random variable X with support on an interval [a,b], where a<b, one can always calculate the expected value, by integrating, to arrive at the value of (a+b)/2. However, since, I have been, for better or worse, ""corrupted"" by formal mathematics, I seem to no longer trust my intuition; even worse, I seem to question even the most ""obvious"" mathematical conclusions, given that I have been humbled one too many times whenever I have thought things to be ""obvious"". However, I keep running into the fact that it is ""obviously obvious"" that the expected value of a continuous uniform distribution is (a+b)/2 time and time again. Everyone I talk to, literally everyone, no matter their backgrounds, their occupations, can see this to be true. Even middle school children can see this to be true (when uniform distribution is explained in an informal way). I, however, can't. I seem to be completely oblivious to its obviousness. So, could someone please explain why it is ""obvious"" that the expected value of a continuous random variable on an interval [a,b], where a<b, is (a+b)/2? I understand the 'why' of it all when one formally defines a uniform distribution and integrates it. But I would be grateful if someone could provide an intuitive explanation for the same, preferably through an example.","['statistics', 'uniform-distribution', 'expected-value', 'intuition', 'probability']"
4739848,How to prove that $\int_0^{2\pi}e^{-x^2}\cos xdx>0$,"How to prove that $\displaystyle I=\int_0^{2\pi}e^{-x^2}\cos(x)\mathrm{d}x>0$ Clearly, $\displaystyle I=\int_0^\pi \left(e^{-x^2}-e^{-(x+\pi)^2}\right)\cos(x)\mathrm{d}x$ , but this does not help. Or else, $$I=\int_0^{2\pi} e^{-x^2}\mathrm{d}\sin(x)
=2\int_0^{2\pi} xe^{-x^2}\sin(x)\mathrm{d}x
=2\int_0^\pi \left(xe^{-x^2}-(x+\pi)e^{-(x+\pi)^2}\right)\sin(x)\mathrm{d}x.$$ This time, $\sin(x)$ has a good sign, but $xe^{-x^2}$ has no monotonicity property. Any ideas?","['integration', 'calculus']"
4739854,How do I evaluate $\int_{0}^{\infty}{e^{-x}\frac{\sin^2{x}}{x}dx} = \frac{\ln{5}}{4}$ using iterated integrals?,"I'm studying for quals and I'm trying to solve the following. Prove using the results concerning iterated integrals (i.e., you should write the integral below as a a double integral) that $$\int_{0}^{\infty}{e^{-x}\frac{\sin^2{x}}{x}dx} = \frac{\ln{5}}{4}$$ (Note: In arguing for this problem, you may assume without proof that a continuous function $h : X \times Y \to \mathbb{R}$ is measurable with respect to the product measure if $X$ and $Y$ are Borel subsets of $\mathbb{R}$ .) Since the problem says to use iterated integrals, I tried to use the fact that $$e^{-x} = \int_{x}^{\infty}{-e^{-y}dy}$$ and re-write this as $$\int_{0}^{\infty}{\int_{x}^{\infty}{-e^{-y}dy}\frac{\sin^2{x}}{x}dx} = \int_{0}^{\infty}{\int_{x}^{\infty}{-e^{-y}}\frac{\sin^2{x}}{x}dydx} = \int_{0}^{\infty}{\int_{0}^{y}{-e^{-y}}\frac{\sin^2{x}}{x}dxdy}.$$ But this doesn't make things any easier. I have a feeling I'm missing something important since this is a practice problem for a real analysis/measure theory qual but I don't remember/can't find any measure theory theorems that really help with this.","['multivariable-calculus', 'calculus', 'measure-theory', 'real-analysis']"
4739912,Isolated points of the spectrum are always eigenvalues?,"Let $A=A^*:D(A)\subset H \rightarrow H$ be a linear operator, where $H$ is a seperable Hilbert space.
The discrete spectrum of $A$ is defined to be $\sigma_{disc}(A):=\left\lbrace \lambda \in \sigma(A)\Big|\exists\varepsilon>0 :B_\varepsilon(\lambda)\cap \sigma(A)=\lbrace \lambda\rbrace\text{ }\& \text{ }\mathrm{dim}PH<\infty\right\rbrace.$ Where $P:=\oint\limits_{|z-\lambda|=\varepsilon}R(z,A)dz$ is the corresponding Riesz-projector of $A$ . $R(z,A):=(z-A)^{-1}$ is the resolvent of $A$ . It is probably known among you analysts, that the spectral measure of an isolated point $\lambda \in \sigma(A)$ , can be presented as an Riesz-projector i.e. $E_{\lbrace \lambda \rbrace}=\oint\limits_{|z-\lambda|=\varepsilon}R(z,A)dz$ Where $E$ is the spectral measure of $A$ . At the same time we have $E_{\lbrace
 \lambda\rbrace}=E_{(\lambda-\varepsilon,\lambda+\varepsilon)}\neq 0$ for some $\varepsilon>0$ , due to the properties of the spectral measure. This implies $\lambda$ is an eigenvalue. I suppose that means if any point is isolated in the spectrum, then it is also an eigenvalue if $A=A^*$ . Hence we could also define the discrete spectrum by $\sigma_{disc}(A):=\left\lbrace \lambda \in \sigma_p(A)\Big|\exists\varepsilon>0 :B_\varepsilon(\lambda)\cap \sigma(A)=\lbrace \lambda\rbrace\text{ }\& \text{ }\mathrm{dim}PH<\infty\right\rbrace.$ Question: How is it in case that $A$ is not selfadjoint? Are isolated points in the spectrum also eigenvalues?","['operator-theory', 'self-adjoint-operators', 'spectral-theory', 'functional-analysis', 'adjoint-operators']"
4739934,Maximal number of nonzero diagonals in a permutation matrix,"Let $P = (p_{ij})_{0\leq i, j < n}$ be a $n \times n$ permutation matrix, i.e. each row and column has exactly one one and $(n-1)$ zeros.
Let $\mathbf{d}_k := (p_{0, k}, p_{1, k +1}, \dots, p_{n-1, k + n-1})$ be the $k$ -th diagonal, which is an $n$ -dimensional vector. Here the indices are regarded modulo $n$ . Q1. What is $a_n := \max_{P \in S_n} \#\{0\leq k < n: \mathbf{d}_k \neq \mathbf{0}\}$ , i.e. the maximum of the number of nonzero diagonals? Obviously $a_n \leq n$ . For odd $n$ , we have $a_n = n$ by taking $P_{i, 2i} = 1$ : for example, when $n= 7$ $$
\begin{bmatrix}
1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 \\
0 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & 0 \\
\end{bmatrix}
$$ For even $n$ , it seems that $a_n = n -1$ , but I don't know how to prove this. What I know is that $a_n \geq n - 1$ by considering $$
P = \begin{bmatrix} 1 & \\ & P' \end{bmatrix}
$$ where $P'$ is the $(n - 1) \times (n - 1)$ permutation matrix that is contructed as above. Q2. What is $b_n = \#\{P \in S_n: \text{number of nonzero diagonals in }P = a_n\}$ ? Here's a python code with list of $b_n$ for $n \leq 10$ : $n$ 1 2 3 4 5 6 7 8 9 10 $b_n$ 1 2 3 16 15 144 143 2048 2025 46400 import itertools

def I(n):
    A = []
    for i in range(n):
        A.append([1 if j == i else 0 for j in range(n)])
    return A

def count_nonzero_diag(m):
    cnt = 0
    n = len(m)
    for k in range(n):
        for j in range(n):
            if m[j][(k + j) % n] == 1:
                cnt += 1
                break
    return cnt

def max_diag(n):
    A = I(n)
    md = -1
    for m in itertools.permutations(A):
        nd = count_nonzero_diag(m)
        if nd > md:
            md = nd
    return md

def max_diag_mat_num(n):
    cnt = 0
    md = max_diag(n)
    A = I(n)
    for m in itertools.permutations(A):
        nd = count_nonzero_diag(m)
        if nd == md:
            cnt += 1
    return cnt

N = 10
for k in range(1, N + 1):
    print(k, max_diag(k), max_diag_mat_num(k)) ```","['permutations', 'combinatorics']"
