question_id,title,body,tags
4920095,Hamming distance in real analysis,"Given two binary strings $x, y\in (0,1)^*$ such that $|x|=|y|$ , then the set $$\delta{(x,y)}=\frac{|\{i\in[|x|]:x_i\neq y_i\}|}{|x|}$$ is called relative hamming distance. Given $x\in (0,1)^*$ and $S:$ = a collection of binary strings, define $$\delta_S(x)=\min_{y\in S, |x|=|y|}\delta(x,y).$$ Given $\epsilon>0,x\in (0,1)^*$ is said to be $\epsilon$ -far from $S$ provided $\delta_S(x)>\epsilon.$ The majority language is given by: $$\text{MAJ}:=\{x\in (0,1)^*:\sum_{i=1}^ {|x|}x_i>\frac{|x|}{2}\},\text{where $x_i$ is the $i$-th position value(either $0$ or $1$) of $x$}.$$ My question is  how can I prove if $\delta_{MAJ}(x)>\epsilon,$ then $$\frac{\sum_{i=1}^ {|x|} x_i}{|x|}<\frac{1}{2}-\epsilon?$$","['expected-value', 'probability-distributions', 'probability', 'real-analysis']"
4920096,Is it circular to include reachability from $0$ like this as a Peano axiom?,"I am wondering whether it makes logical and semantic sense to include, as an axiom to define the natural numbers, that ""every natural number is either $0$ or the result of potentially repeated successor operations from $0$ "", where one idea I have to define what it means to be repeatedly use the successor operations from $0$ is this: If $n= 0$ , we have repeatedly used the successor operation from $0$ $0$ times. If we have repeatedly used the successor operation from $0$ $n$ times to get $k$ , then if $k' = S(n)$ , we have repeatedly used the successor function $S(n)$ times to get $k'$ . Where $n,k,k'$ were any natural numbers and $S$ is the successor function. I am wondering because by using the term ""repeated"", we may be relying on a notion of number, as in order to define what it means to be repeated, we need to distinguish doing something once versus doing something twice; and as we are trying to define numbers, this therefore may introduce circularity into our definition. However, is this really an issue? Every axiom is referring to the concept of a number, by stipulating something of natural numbers- saying $0$ is a natural number, or merely talking about $n$ , is using a concept of a number in some way. So why can't we (potentially) refer to a number by using 'repeated'? Is my definition really different, inasmuch as potential circularity is concerned, than the induction axiom? Anyways, if this is an issue, it seems this axiom can be logically and semantically salvaged if perhaps  the word choice was to distinguish between doing something $0$ times, and doing something at all, whether it be done once or multiple times? If this statement doesn't work, what is another statement which is in the same ""semantic spirit"" (as in not merely logically equivalent, but a way to semantically express reachability from $0$ in a non-circular way)? I am asking because in this post I asked whether this statement was equivalent to the axiom of induction. Here, I am concerned solely with the semantic nature and potential circularity of using this as an axiom, as opposed to in my other post how I was asking whether this statement is equivalent to the axiom of induction.","['peano-axioms', 'induction', 'logic', 'discrete-mathematics']"
4920108,Calculate $I(\alpha)=\int_0^\pi \frac{x}{1+\sin(x)\cos(\alpha)}dx$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last month . Improve this question I have tried to derive it 1 and 2 times but I do not see any relation to the original $I(\alpha)$ , is there any breakthrough that I am missing?","['integration', 'derivatives']"
4920166,Is the action of $\operatorname{Out}(G)$ on $[\operatorname{Rep}(G)]$ faithful?,"Let $G$ be a finite group and let $\phi$ be an automorphism of $G$ . We define an action of $\operatorname{Aut}(G)$ on the set $\operatorname{Rep}(G)$ of complex-valued representations of $G$ by ${}^\phi\rho(g) := \rho(\phi(g))$ . Let $[\operatorname{Rep}(G)]$ denote the set of isomorphism classes of representations of $G$ , where $\rho \cong \rho'$ iff there exists an invertible linear map $T$ such that $T^{-1}\rho(g)T = \rho'(g)$ for all $g \in G$ . If $\phi$ is an inner automorphism of $G$ , then the induced action of $\phi$ on $[\operatorname{Rep}(G)]$ is trivial. Thus we get an action of the outer automorphism group $\operatorname{Out}(G)$ on $[\operatorname{Rep}(G)]$ . My question: is this action faithful, i.e. can there be outer automorphisms of $G$ that fix all isomorphism classes of representations of $G$ ?","['group-theory', 'group-actions', 'representation-theory']"
4920219,"Find out $\int_0^\infty\frac{x^2+ax+1}{1+x^4}\arctan\frac1x\,{\rm d}x$.","$$
\mbox{Find out}\quad\int_{0}^{\infty}\frac{x^{2} + ax +1}{1 + x^{4}}\,\arctan\left(\frac{1}{x}\right){\rm d}x
$$ My attempt: \begin{align}I &= \int_{0}^{\infty}
\frac{x^2 + ax + 1}{1 + x^4}\,\arctan\left(\frac{1}{x}\right){\rm d}x
\\[2mm] & = \int_{0}^{\infty}
\frac{x^2 + ax + x}{1 + x^4}\,\arctan\left(x\right)
{\rm d}x\qquad\qquad \left[x \mapsto \frac{1}{x}\right]
\\[2mm] \implies 2I & = \int_{0}^{\infty}
\frac{\left(x^{2} + ax + 1\right)
\left[\arctan\left(1/x\right) + \arctan\left(x\right)\right]}{1+x^4}{\rm d}x
\\[2mm] & =
\frac{\pi}{2}\int_{0}^{\infty}\frac{x^{2} + ax + 1}{1+x^4}{\rm d}x
\end{align} No idea how to proceed (I'm expecting cooler approaches without Feymann here) .","['integration', 'calculus', 'improper-integrals']"
4920239,Number of edges in planar bipartite graph.,"Suppose G=(V,E) is a planar bipartite graph such that $V_1$ and $V_2$ are the partite sets. Suppose for all $a \in V_1$ , $deg(a)\le p$ and for all $b \in V_2$ , $deg(b)\le q$ . If $|V_1|=x$ and $|V_2|=y$ , then can we deduce a bound for the number of edges in G in terms of p, q, x, and y?","['graph-theory', 'bipartite-graphs', 'discrete-mathematics', 'planar-graphs']"
4920245,Proving Perpendicularity and Equal Lengths in Rotated Isosceles Right Triangles,"Given two isosceles right triangles $( \triangle ABC )$ and $( \triangle BDE )$ with $( \angle BAC = \angle BDE = 90^\circ )$ , triangle $( \triangle BDE )$ is rotated counterclockwise by an angle $( \gamma ) (where ( 0^\circ < \gamma < 360^\circ ))$ about point $( B )$ . Point $( E )$ is then connected to point $( C )$ , and $( F )$ is the midpoint of segment $( EC )$ . It appears that segments $( AF )$ and $( DF )$ are always perpendicular and equal in length regardless of the rotation angle $( \gamma )$ . Here is a diagram to illustrate the problem: I am looking for a proof of why $( AF \perp DF )$ and $( AF = DF )$ hold true for any rotation angle $( \gamma )$ . What I've considered so far: Since both triangles are isosceles right triangles, their properties might be useful. The rotation of $( \triangle BDE )$ around point $( B )$ does not change the relative properties of points $( D )$ and $( E )$ . I need help with a formal proof to show that $( AF )$ and $( DF )$ are always perpendicular and equal in length regardless of $( \gamma )$ . How can we approach this problem? https://www.geogebra.org/classic/eghcku7y","['trigonometry', 'geometry']"
4920251,Probability that two elements commute in a noncommutative simple finite group,"Good afternoon ! Let $G$ a finite non-abelian group. Let $p_G$ the probability that two elements randomly chosen commute. It is well known that : $$p_G \leqslant \frac{5}{8}$$ The upper bound is optimal (if we consider for example $\mathbb{D_4}$ the dihedral group of the square or the quaternion group $\mathbb{H_8}$ ), and there is no interesting lower bound because of the symmetric group $S_n$ for example. We can even show with Burnside's lemma that : $$p_G = \frac{k_G}{|G|}$$ With $k_G$ the number of conjugacy classes in $G$ . I've seen that if moreover we suppose $G$ simple (i.e $G$ has only trivial normal subgroups), we have : $$p_G \leqslant \frac{1}{12}$$ I would love to prove it. What I tried so far : It is sufficient to show that if $k_G \geqslant \frac{|G|}{12}$ , then $k_G = \frac{|G|}{12} = 5$ . There are $k_G$ irreducible representations of degree $d_1, d_2, \dots, d_{k_G}$ , with $d_1 = 1$ (trivial representation), $d_1 \leqslant \dots \leqslant d_{k_G}$ and $d_1^2 + \dots + d_{k_G}^2 = |G|$ . $G$ is simple so the only irreducible representation of degree 1 is the trivial representation. We can show that : $$d_2 = 3$$ Indeed G is simple so it has no irreducible representations of degree 2. Furthermore if $d_2 \geqslant 4$ then $|G| \geqslant 1 + 16(k_G - 1)$ , so by the inequality $k_G \geqslant \frac{|G|}{12}$ we have $|G| \leqslant 15$ , which is impossible because the smallest simple non-abelian subgroup is $A_5$ , and $|A_5| = 60 > 15$ . From now I don't know how to conclude. We have : $$12k_G \geqslant |G|= 1 + 3^2 + \dots + d_{k_G}^2$$ And I don't know what to say. Any help ? :(","['group-theory', 'simple-groups', 'finite-groups', 'probability']"
4920258,Solve the differential equation $ y_n^{\prime \prime}(r) + \frac{1}{r} y_n^{\prime}(r) - \frac{n^2}{r^2} y_n(r) = 0$,"I would like to solve the differential equation $$
y_n^{\prime \prime}(r) + \frac{1}{r} y_n^{\prime}(r) - \frac{n^2}{r^2} y_n(r) = 0,
$$ I thought to start by trying a solution of the form $y_n(r) = r^m$ . Substituting $y_n(r) = r^m$ into the differential equation, we compute the first and second derivatives: $$
y_n^{\prime}(r) = m r^{m-1}
$$ and $$
y_n^{\prime \prime}(r) = m (m-1) r^{m-2}
$$ Now substituting $y_n(r) = r^m$ , $y_n^{\prime}(r) = m r^{m-1}$ , and $y_n^{\prime \prime}(r) = m(m-1) r^{m-2}$ into the differential equation, we get: $$
m(m-1) r^{m-2} + \frac{1}{r} (m r^{m-1}) - \frac{n^2}{r^2} r^m = 0
$$ Simplifying each term: $$
m(m-1) r^{m-2} + m r^{m-2} - n^2 r^{m-2} = 0
$$ Combining like terms: $$
[m(m-1) + m - n^2] r^{m-2} = 0
$$ Factoring out $r^{m-2}$ , we obtain the characteristic equation: $$
m(m-1) + m - n^2 = 0
$$ Simplifying the quadratic equation: $$
m^2 - m + m - n^2 = 0
$$ $$
m^2 - n^2 = 0
$$ This factors into: $$
(m-n)(m+n) = 0
$$ Thus, the solutions for $m$ are: $$
m = n \quad \text{or} \quad m = -n
$$ Therefore, the general solution to the differential equation is a linear combination of the two independent solutions $r^n$ and $r^{-n}$ : $$
y_n(r) = C_1 r^n + C_2 r^{-n}
$$ where $C_1$ and $C_2$ are arbitrary constants. Is my solution correct? Even if it turns out to be correct, I don't like that I arrived at this solution 'by trial and error'. Can someone solve this equation in a different way? Thanks in advance.",['ordinary-differential-equations']
4920278,Computing an integral using differential under the integral sign,"The following integral is in question. $$I(x) =\int_0^x \frac{\ln(1+tx)}{1+t^2}\,dt$$ My attempt is finding $I’(x)$ which is $$I’(x) = \int_0^x \frac{t}{(1+t^2)(1+tx)}\,dt + \frac{\ln(1+x^2)}{1+x^2} $$ Now we can use partial fraction decompostion for the first integral. $$\frac{t}{(1+t^2)(1+tx)} = \frac{At +B}{1+t^2}+\frac{C}{1+tx}$$ Solving this gives the following values: $$A = \frac{1}{1+x^2}$$ $$B = \frac{x}{1+x^2}$$ $$C = \frac{-x}{1+x^2}$$ Now we can solve the first integral and we obtain that $$I’(x) = \frac{1}{2}\frac{\ln(1+x^2)}{1+x^2} + \frac{x\arctan(x)}{1+x^2}$$ Here Intuitively I would get something that is easy to integrate and find the constant by limits or some other obvious way but I get a function that I cannot integrate, can someone help me and give me a hint of what I am doing wrong ? Thanks!!","['integration', 'real-analysis', 'multivariable-calculus', 'calculus', 'derivatives']"
4920314,"Let $ABCD$ be a convex quadrilateral. If the measure of the angles $A=90, C=96, D=78$ and $BC=2*AB$, then the measure of the angle $ABD$ is?","The problem Let $ABCD$ be a convex quadrilateral. If the measure of the angles $A=90°, C=96°, D=78°$ and $BC=2*AB$ , then the measure of the angle $ABD$ is...? The idea As you can see I calculated above $B$ as $96$ .Let point $X$ be the midpoint of $BC$ and then we got the isosceles triangle $BAX$ . I also looked for inscribed quadrilaterals but found none. I Hope one of you can help me with a non trigonometric proof! Thank you!","['quadrilateral', 'euclidean-geometry', 'angle', 'geometry']"
4920318,Nontrivial advantages of thinking of groups as groupoids with one object?,"There are already a number of questions here asking ""what do people mean when they say a group is a groupoid with one object?"". A natural question to ask is ""are there any interesting group theoretic facts that one can recover from the category theoretic study of a groupoid with one object?"" I am seeking as new examples besides the claim ""Yoneda Lemma $\Rightarrow$ Cayley's Theorem"" which is nice, but seems to be the only one people ever talk about. I am aware that one could use the groupoid definition to construct the free group on a finite set of generators, but that feels more like a tautology.","['group-theory', 'category-theory']"
4920361,"how to integrate $\int_0^1 \ln^4(1+x) \ln(1-x) \, dx$?","I'm trying to evaluate the integral $$\int_0^1 \ln^4(1+x) \ln(1-x) \, dx,$$ and I'd like some help with my approach and figuring out the remaining steps. or is it possible to evaluate $$\int_0^1 \ln^n(1+x) \ln(1-x) \, dx \;
?$$ My Attempt First, I transformed the variable to simplify the integral: $$
\Omega = \int_0^1 \ln^4(1+x) \ln(1-x) \, dx \stackrel{1+x \rightarrow x}{=} \int_1^2 \ln^4(x) \ln(2-x) \, dx
$$ This allowed me to split the integral into two parts: $$
\int_1^2 \ln^4(x) \left( \ln(2) + \ln\left(1-\frac{x}{2}\right) \right) \, dx = \ln(2) \underbrace{\int_1^2 \ln^4(x) \, dx}_{A} + \underbrace{\int_1^2 \ln^4(x) \ln\left(1-\frac{x}{2}\right) \, dx}_{B}
$$ So we have: $$
\Omega = A \ln(2) + B
$$ I was able to evaluate part $A$ : $$
A = \int_1^2 \ln^4(x) \, dx = \left. \left( 24x + x \ln^4(x) - 4x \ln^3(x) + 12x \ln^2(x) - 24x \ln(x) \right) \right|_1^2
$$ Evaluating this, we get: $$
A = 24 + 2 \ln^4(2) - 8 \ln^3(2) + 24 \ln^2(2) - 48 \ln(2)
$$ However, I'm stuck on how to handle part $B$ . I think it involves expanding $\ln\left(1 - \frac{x}{2}\right)$ using a Taylor series, but I'm unsure how to proceed from here. Any suggestions are welcome closed form: \begin{align*}
&-6\left(8\text{Li}_{4}\left(\frac{1}{2}\right)+8\text{Li}_{5}\left(\frac{1}{2}\right)-\zeta(3)-8\zeta(5)+20\right)+\log(4)\left(6\zeta(3)(\log(4)-4)+96+(\log(2)-4)\log(2)(12+(\log(2)-1)\log(2))\right)\\
&-\frac{4}{3}\pi^{2}\left(-3+\log^{3}(2)+\log(8)-\log(2)\log(8)\right)-\frac{8}{15}\pi^{4}(\log(2)-1)
\end{align*} Thanks David G. Stork for the closed form!","['integration', 'definite-integrals', 'closed-form', 'logarithms']"
4920421,Expressing a continuous local martingale as an integral against a Brownian motion,"I'm interested in the following problem. Suppose $X$ , $X_0=0$ is a continuous local martingale with quadratic variation $$ [X]_t = \int_0^t A_s\mathrm{d}s $$ for a non-negative previsible process $(A_t)_{t\ge 0}$ . Show that there exists a Brownian motion $B$ such that $$X_t = \int_0^t A_s^{1/2} \mathrm{d}B_s.$$ The solution I've seen involves defining $$B_t = \int_0^t A_s^{-1/2} 1_{A_s>0} \mathrm{d}X_s+\int_0^t 1_{A_s=0} \mathrm{d}W_s$$ where $W$ is a Brownian motion independent of $X$ . It is not hard to check that $B$ is a Brownian motion by the Levy characterisation, but I'm confused as to how we can conclude $A_t^{1/2} \mathrm{d}B_t = \mathrm{d}X_t$ ? It is immediate that $A_t^{1/2}\mathrm{d}B_t = 1_{A_t>0} \mathrm{d}X_t$ but why is this enough to conclude? Intuitively it may have something to do with continuous local martingales being constant on intervals where their quadratic variation is but I can't see how to justify this rigorously.","['stochastic-integrals', 'stochastic-calculus', 'stochastic-processes', 'probability-theory', 'probability']"
4920448,Is the set of all antichains of $\omega^{<\omega}$ avoiding all chains Borel?,"Let's identify $\mathcal P(\mathbb N^{<\mathbb N})$ with $2^{\mathbb N^{<\mathbb N}}$ , so it is a Polish space. The set $\mathcal A=\{A\subseteq \omega^{<\omega}: A \text{ is a maximal antichain}\}$ is easily seen to be Borel (where the order in question is the standard tree order of $\mathbb N^{<\mathbb N}$ ). I am trying to decide whether $\{A\in\mathcal A: \forall g \in \omega^\omega\exists n \in \mathbb N\,g|n\in A\}$ is Borel. This is the set of maximal antichains that avoid all chains. I can see it is coanalytic, as its complement is the projection of $F\subseteq \mathbb N^\mathbb N\times \mathcal A$ in the second coordinate, where $F=\{(g, A)\in \omega^\omega\times \mathcal A: \forall n \in \mathbb N\, g|n\notin A\}$ is easily seen to be Borel. Thus, it is Borel if and only if it is analytic.
Any help is appreciated.","['general-topology', 'descriptive-set-theory', 'set-theory']"
4920529,Finding Smith normal form of a $\mathbb C[\lambda]$-matrix,"Let $J_n(\lambda)$ denote the Jordan block of size $n$ with eigenvalue $\lambda$ , i.e. $$J_n(\lambda)=\begin{pmatrix}
\lambda & 1 & & \\
& \lambda & \ddots & \\
& & \ddots & 1\\
& & & \lambda
\end{pmatrix},$$ Then $J_n(\lambda)$ could be viewed as a matrix over the ring $\mathbb C[\lambda]$ . What is the Smith normal form of $J_n^n(\lambda)=(J_n^n(\lambda))^n$ ? For example, the matrix $J_3^3(\lambda)$ is $\begin{pmatrix}
\lambda^3 & 3\lambda^2 & 3\lambda\\
0 & \lambda^3 & 3\lambda^2\\
0 & 0 & \lambda^3
\end{pmatrix}$ , and the Smith form is $\text {diag}\{\lambda, \lambda^3, \lambda^5\}$ . Some brute force calculations show that the Smith form of general $J_n^n(\lambda)$ is very likely to be $$\text {diag}\{\lambda, \lambda^3, \dots, \lambda^{2n-1}\}  (\ast)$$ Is there any way to prove it? Moreover, if possible, can we do the more general case $J_n^m(\lambda)$ where $m\ne n$ ? Thanks for any help in advance. (Motivation: I ended up with this $\lambda$ -matrix while trying to determine the Jordan form of $I_n\otimes J_n(0)+ J_n(0)\otimes I_n$ . I have already shown in other approaches that the largest Jordan block of $I_n\otimes J_n(0)+ J_n(0)\otimes I_n$ has size $2n-1$ and it has exactly $n$ Jordan blocks (which matches with the conjecture $(\ast)$ ). But I can't prove that the explicit Jordan form is $\text{diag}\{J_1(0),J_3(0),\dots, J_{2n-1}(0)\}$ . It would also be appreciated if any of u solve this more original problem:) Edit: Jyrki's answer probably does the work, but as I don't have any knowledge of Lie algebra, I need a more elementary answer","['matrices', 'jordan-normal-form', 'smith-normal-form', 'linear-algebra']"
4920564,"When using the method of substituting variables to find the limit, why can we still use the original numbers that need to be approximated?","I'm trying to solve this problem: Assume $$f(x+\frac{1}{x}) = x^2 + \frac{1}{x^2}$$ then $$\lim_{x\to 3} f(x) = ?$$ I saw the answer is $$t = x+\frac{1}{x}$$ $$\lim_{x\to 3} f(x) = \lim_{t\to 3} t^2 - 2 = 7$$ I am a little confused, since we have set $t=x+\frac{1}{x}$ , why $x\to 3$ doesn't become $t\to \frac{10}{3}$ ? This question may be a bit low-level, but I am really not good at math. Thank you for your help!",['limits']
4920571,"There exists a nonempty subsequence of $n$ integers within the range $[1, n]$, whose sum is equal to the sum of their indices.","Given $n$ integers $a_1, a_2,\cdots,a_n,$ such that for each $1\leq i \leq n$ , it holds that $1 \leq a_i \leq n$ . The problem is to prove that there exists a nonempty subsequence (not necessarily consecutive) of these integers, whose sum is equal to the sum of their indices. Formally, it is to prove that there exists a nonempty subset $S \subseteq [n]$ that $\sum_{i\in S}a_i = \sum_{i \in S}i$ , where $1 \leq a_i \leq n$ . The idea for the proof might be the pigeonhole principle, but I am stuck on how to construct the appropriate pigeons and pigeonholes.",['combinatorics']
4920573,Understanding the difference between df and $| \triangledown f|$,"I am seeking some clarification regarding a couple of vector calculus topics. Let's suppose z = f(x,y) is a surface in $\mathbb{R}^3$ , and for the sake of having something to hold onto that z represents money we make based on x (number of workers) and y (products we sell). As I understand it, $df = \frac{\partial f}{\partial x}dx + \frac{\partial f}{\partial y}dy$ . This represents the total differential of f(x,y). What I thought this represented was if I'm at some point on the surface $(x_o, y_o, z_o)$ , then I could somehow figure out the amount z would change if we modified our $x_o$ and $y_o$ a tiny bit. To be more precise with my thinking, since $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ represent the particular change in z we would experience if we wandered off a tiny bit in the x or y direction respectively, if we scale each rate by dx and dy respectively (our tiny bit of wandering) and add the results, we get the total change in z we would experience ( df ). Now we have this thing called the gradient which consists of $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ , specifically $\triangledown f = <\frac{\partial f}{\partial x}$ , $\frac{\partial f}{\partial y}>$ . And to top it off, the idea is that this is a vector that lies on the x-y plane and points in the direction of steepest ascent. Suppose the gradient at a particular point was $<4, 3>$ , I have been taking this to mean that if we increase the number of workers by 4 and the number of units sold by 3, then we will increase our revenue the quickest possible AND we would expect to make \$5 more as a result, since $|\triangledown f| = 5$ . So my questions are as follows: Is my intuition correct for both concepts? In what (simple) scenarios would I want to use the result of df vs. the result of $|\triangledown f|$ when talking about what to expect to happen to the z-values of my surface? I'm having trouble putting into words the lack of ability I'm having differentiating between the two concepts and uses. Not to mention that if df measures change in z based on the change in the x and y direction, but $|\triangledown f|$ also measures the change but then says it's the maximum change...I'm just a bit confused.",['multivariable-calculus']
4920585,How can I compute the limit with an integral？,"I came across a problem that I couldn't solve in a mathematical analysis textbook： Let $\alpha, L \in \mathbb{R}$ and $L \neq 0$ : $$
\mbox{If}\quad\lim_{n \to \infty}
\dfrac{\displaystyle n^{\alpha}\int_{0}^{\pi/2}x^{n}\sin\left(x\right)\,
\mathrm{d}x}{\displaystyle\int_{0}^{\pi/2}x^{n}\cos\left(x\right)\,\mathrm{d}x} = L,\quad\mbox{then find}\quad\alpha, L
$$ My attempt：Let $a_n=\int_0^{\frac{\pi}{2}} x^n \sin x \mathrm{~d} x$ ， $b_n=\int_0^{\frac{\pi}{2}} x^n \cos x \mathrm{~d} x$ ,then we get \begin{aligned}
    a_n&=-\int_0^{\frac{\pi}{2}} x^n \mathrm{~d} (\cos x)=-x^n\cos x\big |_0^{\frac{\pi}{2}}+n\int_0^{\frac{\pi}{2}} x^{n-1} \cos x \mathrm{~d} x=nb_{n-1}\\
    b_n&=\int_0^{\frac{\pi}{2}} x^n  \mathrm{~d} (\sin x)=x^n\sin x\big |_0^{\frac{\pi}{2}}-n\int_0^{\frac{\pi}{2}} x^{n-1} \sin x \mathrm{~d} x=\left(\frac{\pi}{2}\right)^{n}-na_{n-1}\\
    &=\left(\frac{\pi}{2}\right)^{n}-n(n-1)b_{n-2}.
\end{aligned} But the subsequent iterations were so complex that I couldn't figure out the answer. Are there any other ideas to solve this problem?","['integration', 'limits', 'calculus', 'real-analysis']"
4920589,Pythagoras' Theorem used to prove a triangle is right angled,"I was asked to prove that the vertices of a triangle, namely $A = 3 +i$ , $B = 6$ and $C = 4 + 4i$ form a right angled triangle. I went down the path of showing that the squaring the two shorter sides and adding them, equals to the longest side, squared. This got me thinking that how can we say that no other triangle will satisfy this equation since Pythagoras' theorem simply tells us that the The sum of the areas of the two squares on the legs (a and b) equals the area of the square on the hypotenuse (c). Just because this equality stands for right angled triangles, how can we assume that it won't stand for others? Is there a more rigorous method to show this?","['euclidean-geometry', 'geometry']"
4920605,Functional equation arising in computing an integral,"Let $f(x)$ be a twice differentiable function (with a continuous second derivative) satisfying the identity: $$f \left(\frac{x}{2} \right)+f \left(\pi-\frac{x}{2} \right)=\frac{f(x)}{2}$$ Determine $f(x)$ . This functional equation arose when I was computing the value of an integral (I replaced $a$ by $a/2$ and $\pi - a/2$ , added the two and then substituted $t=x^2$ ); $$I(a)=\int_0^1 \dfrac{\log|x^2-2x\cos a+1|}{x} dx$$ I have tried many things with this functional equation, like trying to find $f(0),f(\pi)(f(2\pi)$ , etc but I was unsuccessful. Then I tried to prove injectivity or surjectivity and again failed because I couldn't get a conclusive result. Next, I differentiated the equation twice to obtain $f''(t)+f''(\pi -t)=2f''(2t)$ . Since $f''(x)$ is continuous, it is bounded but I couldn't use that fact either. I have a feeling that given the simplicity of the functional equation, the solution to it must be symmetric about some point but I have no way of finding this point or proving this assertion either. What can I do to solve this functional equation? EDIT:At the back of the book, a hint is given: try proving that $f''(x)$ is constant using continuity. Can someone help me understand how I can develop an alternate solution by proving this? I think it must involve somewhat similar reasoning as in Functional equation $f (x) = f \left (\frac x2 \right ) + f \left (\frac x2 + \frac 12\right)$ but I'm unsuccessful in applying it.","['functional-equations', 'functions', 'definite-integrals']"
4920616,Homogenous but not isotropic manifolds,"$(M,g)$ is homogeneous if $\forall a,b\in M$ there is an isometry $f:M\to M$ such that $f(a)=b$ $(M, g)$ is called isotropic if $\forall p\in M$ $x,y\in T_pM$ of unit length, there is an isometry $f:M\to M$ s.t. $f_\ast(x)=y$ I know that Every connected isotropic manifold is homogeneous (given 2 points, take the middle of the geodesics connecting them) Now, are there homogenous manifolds that are not isotropic? Please, explain in detail if so.","['riemannian-geometry', 'differential-geometry']"
4920698,Jensen's inequality: are there missing details from the hypothesis?,"I'm trying to prove Jensen's inequality for an exercise, that is: having $f:[0, 1] \to \mathbb{R}$ , continuous, I want to prove $e^{\int_0^1 f(x) dx} \leq \int_0^1 e^{f(x)} dx$ . I think that I got stuck in the proof because I tried to use the fact that $ax + b \leq e^x$ where the equality only holds at a tangent point $x^*$ , and due to this, applying the integral to both: $$\int_0^1 ax + b\ dx \leq \int_0^1 e^x dx$$ I then would replace $x \mapsto f(x)$ . But then I don't know how to proceed to prove that $\int_0^1 e^{f(x)}\ dx \geq e^{\int_0^1 f(x)\ dx}$ Also, those doubts arose: Is this true only for functions $f$ defined as $f:[0, 1] \to \mathbb{R}$ or even for a generic $f:[a, b]\to \mathbb{R}$ ? Does the continuity hypothesis of $f$ suffice? Wouldn't I need something more, like positivity of $f$ ? Or maybe even that $f$ should be an increasing function? Also I know that somehow, somewhere, I should take into account the convexity of $e^x$ , but it doesn't sound stable to me: $e^x$ is convex, ok. But only knowing $f$ is continuous, how can I say $e^{f(x)}$ is convex too?","['integration', 'proof-explanation', 'analysis', 'real-analysis', 'solution-verification']"
4920725,Understanding intuition behind integral involving mixed product,"The topological charge i.e. skyrmion number (also called wrapping number) is defined as the number of times the spin vectors in a 2D configuration (i.e. lying on a 2D plane, as shown in the image above, bottom 2) wrap around a unit sphere (as shown in the image above, top 2).  The integration is performed over this 2D configuration/plane (far away, the spins are zero). For a magnetic skyrmion, this is an integer number of times. The topological charge is defined as follows: $$
Q = \frac{1}{4\pi} \int \vec{n} \cdot \left( \frac{\partial \vec{n}}{\partial x} \times \frac{\partial \vec{n}}{\partial y} \right) dxdy,
$$ where $\vec{n}$ is the (normalized) spin vector. You can write the spin vector as follows: $$
\vec{n} = \begin{pmatrix}
\sin\theta \cos\phi \\
\sin\theta \sin\phi \\
\cos\theta
\end{pmatrix}
$$ If I understand correctly, this can be seen as a mapping from a two-dimensional space to the surface of a sphere $S^2$ , but not sure how to precisely define this. Any way, I am curious behind the intuition of this integral. Why exactly does this integral calculate the number of times the spins in a 2D configuration wrap a unit sphere? I recognize the fact that there is a mixed product which essentially measures the volume of the parallelepiped by the three vectors, and the fact that this could be related to solid angle, but it is not clear to me exactly how this follows (both mathematically and visually).","['integration', 'multivariable-calculus', 'vectors']"
4920889,"Suppose $1\leq p_1<p_2<+\infty$ and $\mu$ a finite measure, then $\mathscr{L}^{p_2}(X,\mathscr{A},\mu)\subseteq\mathscr{L}^{p_1}(X,\mathscr{A},\mu)$","I need to prove the following result: Suppose $1\leq p_1<p_2<+\infty$ and $\mu$ a finite measure, then $\mathscr{L}^{p_2}(X,\mathscr{A},\mu)\subseteq\mathscr{L}^{p_1}(X,\mathscr{A},\mu)$ . Here is my attempt: Let $f\in\mathscr{L}^{p_2}(X,\mathscr{A},\mu)$ . Then $f$ is an $\mathscr{A}$ -measurable function such that $|f|^{p_2}$ is integrable. So $\int(|f|^{p_2})^+d\mu = \int|f|^{p_2} < +\infty$ . For $x$ in $X$ that $|f(x)|\geq1$ , we have $|f(x)|^{p_1}\leq|f(x)|^{p_2}$ . For $x$ in $X$ such that $0\leq|f(x)|<1$ , we have $|f(x)|^{p_1}<1$ . Define a function $g$ by letting \begin{align*}
g(x) = 
\begin{cases}
f(x)\quad &\text{if $|f(x)|\geq1$},\\
\\
1\quad &\text{if $0\leq|f(x)|<1$}.
\end{cases}
\end{align*} Then $|f(x)|^{p_1}\leq|g(x)|^{p_2}$ for all $x$ in $X$ . Define a function $h$ by letting \begin{align*}
h(x) = 
\begin{cases}
f(x)\quad &\text{if $|f(x)|\geq1$},\\
\\
0\quad &\text{if $0\leq|f(x)|<1$}.
\end{cases}
\end{align*} Then $g(x)=h(x)+\chi_{A}$ , where $A=\{x\in X:0\leq|f(x)|<1\}$ , and $|g(x)|^{p_2} = |h(x)|^{p_2} + \chi_{A}$ for all $x$ in $X$ . We prove that $|h|^{p_2}$ is $\mathscr{A}$ -measurable. For each $t\geq1$ , the set $\{x\in X:|h(x)|<t\} = \{x\in X:|f(x)|<t\}\in\mathscr{A}$ , because $|f|$ is integrable and so $\mathscr{A}$ -measurable. For each $t<1$ , the set $\{x\in X:|h(x)|<t\} = \{x\in X:|f(x)|<s\} \in \mathscr{A}$ for some $x\in\mathbb{R}$ . Thus, $|h|$ is $\mathscr{A}$ -measurable (and real-valued), and so $|h|^{p_2}$ is $\mathscr{A}$ -measurable. Since $|h|^{p_2}$ and $|f|^{p_2}$ are both nonnegative real-valued $\mathscr{A}$ -measurable functions such that $|h(x)|^{p_2}\leq|f(x)|^{p_2}$ holds at each $x$ in $X$ , it follows that $\int|h|^{p_2}d\mu \leq \int|f|^{p_2}d\mu < +\infty$ . So, $|h|^{P_2}$ is integrable. Since $\mu$ is a finite measure on the measurable space $(X,\mathscr{A})$ , it follows that $\int\chi_{A}d\mu = \mu(A) < +\infty$ , and so $\chi_{A}$ is integrable. Therefore, $int|g|^{p_2}d\mu = \int(|h|^{p_2}+\chi_{A})d\mu = \int|h|^{p_2}d\mu + \int\chi_{A}d\mu < +\infty$ . Since $|f|^{p_1}$ and $|g|^{p_2}$ are nonnegative real-valued $\mathscr{A}$ -measurable functions on $X$ such that $|f(x)|^{p_1}\leq|g(x)|^{p_2}$ holds at each $x$ in $X$ , we have \begin{align*}
\int|f|^{p_1}d\mu \leq \int|g|^{p_2}d\mu < +\infty.
\end{align*} Hence, $|f|^{p_1}$ is integrable, $f\in\mathscr{L}^{p_1}(X,\mathscr{A},\mu)$ . Is my attempt correct? I am unconfident about the part proving $|h|^{p_2}$ is $\mathscr{A}$ -measurable (e.g., ""for each $t<1$ , the set $\{x\in X:|h(x)|<t\} = \{x\in X:|f(x)|<s\} \in \mathscr{A}$ for some $x\in\mathbb{R}$ ""). Also I'm not sure if there are mistakes elsewhere. I would really appreciate it if someone could help me check my proof! Thank you very much! Definition $\quad$ Let $(X,\mathscr{A},\mu)$ be a measure space, and let $p$ satisfy $1\leq p<+\infty$ . Then $\mathscr{L}^p(X,\mathscr{A},\mu)$ is the set of all $\mathscr{A}$ -measurable functions $f$ such that $|f|^p$ is integrable. Reference: Example 3.3.5 from Measure Theory by Donald Cohn.","['measure-theory', 'proof-writing', 'analysis', 'real-analysis', 'solution-verification']"
4920890,Canonical divisor of Kummer Surface,"I want to prove that a Kummer surface is  K3 so, first of all, I want to focus on the canonical divisor $K_X$ . Just to fix some notations: $A$ is a complex torus of the form $\mathbb{C}^2/\Gamma$ , where $\Gamma$ is a lattice of rank four. $\epsilon:\tilde{A}\rightarrow A$ is the blow up of $A$ in the 16 points fixed by the involution $\tau: x \mapsto -x$ . $E_i:=\epsilon^{-1}(P_i)$ is the exeptional divisor corresponding to the torsion point $P_i$ . $\iota: \tilde{A} \rightarrow \tilde{A}$ is the lift of $\tau$ and $f:\tilde{A} \rightarrow \tilde{A}/\iota=:X$ is the quotient map to the Kummer surface $X$ . Now I'm going to follow the argument in Huybrechts ""Lectures on K3 Surfaces"" (p.3), that uses some results concerning the calculation of canonical bundle for branched coverings (they can be found in Barth, Hulek, Peters, Van de Ven ""Compact Complex Surfaces"" , section I.16). The formulas for the blow up $\epsilon: \tilde{A}\rightarrow A$ and for the branched covering $f: \tilde{A} \rightarrow X$ give $$K_{\tilde{A}}=\mathcal{O}(\sum E_i),\,\,\,\,\,\, K_{\tilde{A}}=f^* K_X \otimes \mathcal{O}(\sum E_i);$$ so $$f^*K_X=\mathcal{O}_\tilde{A}.$$ Calling $\overline{E_i} \subset X$ the image of the exeptional divisor $E_i$ via $f$ , we have the relation $$f^*\mathcal{O}(\overline{E_i})=\mathcal{O}(2E_i),$$ and we also know that $$f_*\mathcal{O}_{\tilde{A}}=\mathcal{O}_X\oplus L^*,$$ where $L$ is the square root of $\mathcal{O}(\sum \overline{E_i}).$ How can I conclude now?","['complex-geometry', 'vector-bundles', 'algebraic-geometry', 'k3-surfaces']"
4920930,Why the tautological bundle of the Grassmannian has only zero as global sections?,"I'm in a course on Complex Geometry, and I have been studying the Grassmannian $G_r(\mathbb{C}^N)$ as a complex manifold and the construction of its tautological bundle. As in the case of the tautological bundle of $\mathbb{CP}^n$ , there are no global sections except the zero section, but I don't know how to prove this using complex geometry techniques. A global section is also defined by a family of holomorphic functions $\{s_\alpha : U_\alpha \rightarrow \mathbb{C}^r \}$ , where $U_\alpha$ are open subsets of $G_r(\mathbb{C}^N)$ , satisfying the cocycle conditions $s_\alpha (x)= g_{\alpha \beta} (x) s_\beta(x)$ for all $x\in U_\alpha \cap U_\beta$ . We can describe $U_\alpha=\{[A]: \operatorname{det}A_\alpha \not = 0 \}$ , where $A_\alpha$ is a $k\times k$ -minor matrix of $A$ . Can anyone prove that the tautological bundle of $G_r(\mathbb{C}^N)$ has no global sections except the zero section using these results? Thanks.","['complex-geometry', 'algebraic-geometry', 'grassmannian', 'differential-geometry']"
4920945,Estimating a complex integral for a probability problem: $HH$ is more likely to appear than $THT$ or $TTT$ in $n$ coin flips,"The probability problem asks us to toss a coin $n$ times (actually $100$ times) and Alice gets a point whenever there is a $HH$ and Bob gets a point whenever there is a $THT$ or a $TTT$ . Who is more likely to win? [As an example, consider the sequence $THHHTHTHTTT$ . This sequence gives $2$ points to ALice and $3$ points to Bob.] Using (two separate) Markov chains and matrix multiplications (done on a computer), we found out that Alice is more likely to win (Alice wins $622452888834764723839990461444$ times while Bob wins $597223930770097681231606561162$ times when we have $100$ tosses). We tried it for several other $n$ , and the graph shows that Alice is always more likely to win. We verified this answer using a recursive solution (also done using a computer), so we believe that the answers are correct. So we tried to do this analytically by looking at the generating function for the probabilities as such. We denote by $p_{n,k,TT}=\mathbb P(X_n=k|Y_{n-1}=T,Y_n = T)$ where $X_i$ is a random variable for the difference of scores between Alice and Bob after $i$ -th toss, and $Y_j$ represents the outcome of the $j$ -th toss. Similarly, we define $p_{n,k,TH},p_{n,k,HT},p_{n,k,HH}$ . The recursion that comes out is the following: $$
\begin{align}
p_{n,k,TT}&=\frac12(p_{n-1,k+1,TT}+p_{n-1,k,HT}) \\
p_{n,k,TH}&=\frac12(p_{n-1,k,HT}+p_{n-1,k,TT}) \\
p_{n,k,HT}&=\frac12(p_{n-1,k+1,TH}+p_{n-1,k,HH}) \\
p_{n,k,HH}&=\frac12(p_{n-1,k-1,TH}+p_{n-1,k-1,HH}) \\
\end{align}
$$ We define the generating function $\varphi_n(z) = \sum_k \mathbb P(X_n=k)z^k$ and define $\varphi_{n,TT}(z),\varphi_{n,TH}(z),\varphi_{n,HT}(z),\varphi_{n,HH}(z)$ as the respective conditional generating functions. Along with proper base cases, this gives $$
\varphi_n(z) = \frac1{2^n}\begin{bmatrix}1 & 1 & 1 & 1\end{bmatrix}\begin{bmatrix}\frac1z & 0 & 1 & 0 \\
1 & 0 & 1 & 0\\
0 & \frac1z & 0 & 1\\
0 & z & 0 & z\end{bmatrix}^{n-2}\begin{bmatrix}1 \\ 1 \\ 1 \\ z\end{bmatrix}
$$ Now realizing that $$
\mathbb P(\text{Bob winning})={\frac 1 {2\pi i}}\int_C\left(\sum_{k<0}z^{-k-1}\right)\varphi_n(z)dz
$$ $$
\mathbb P(\text{Alice winning})={\frac 1 {2\pi i}}\int_{C'}\left(\sum_{k>0}z^{-k-1}\right)\varphi_n(z)dz
$$ for proper contours $C,C'$ we define later. Doing some complex analysis magic, we get $$\mathbb P(\text{Alice winning})-\mathbb P(\text{Bob winning})={\frac 1 {2\pi i}}\int_C {\frac {\varphi_n(z^{-1})-\varphi_n(z)} {1-z}}~\mathrm dz$$ where $C$ is a contour satisfying $\max \{|z|: z\in C\}<1$ and containing the origin inside (the half radius circle should work). Now we want to show that this integral is greater than $0$ for all $n\geq3$ but really stuck. We tried diagonalizing the matrix to understand $\varphi_n(z)$ but it was horrible. Any help is appreciated.","['complex-analysis', 'contour-integration', 'linear-algebra', 'probability']"
4920963,"Show that $\mathbb{P}(X_n\leq b)$ tends to zero as $n\to \infty$ for $X_n\sim B(n,p)$","I found this question in ""Probability Essentials"" of Jacod and Protter, exercise 5.13: For $X_n\sim B(n,p)$ with fixed $p$ show, that for any fixed $b>0$ it holds that $\mathbb{P}(X_n\leq b)$ tends to $0$ for $n\to \infty$ . Based on the previous content of the book (which includes only basic definitions on random variables and probability spaces) I would have expected to be able to either use the Markov inequality (which doesn't really seem applicable in this case) or to simply calculate starting from \begin{equation}
\mathbb{P}(X_n\leq b)=\sum_{i=1}^{\lfloor b \rfloor} \binom{n}{i}p^i(1-p)^{n-i} 
\end{equation} I would have only considered the $\binom{n}{i}$ and $(1-p)^{n}$ part since $(\frac{p}{1-p})^{i}$ can be bounded by taking $i=0$ or $i=b$ . Now my question would be to you how I can show that $(1-p)^{n}$ converges stronger to $0$ than the sum of $\binom{n}{i}$ tends to infinity. Since it doesn't seem like a nice approach to me I would also be very open to input of how I can arrive at the result in another way. Thanks a lot!","['binomial-distribution', 'probability-distributions', 'probability-theory', 'probability']"
4920964,Prove that this sequence is eventually one,"Let $ \ \mathbb{N} = \{ 0,1,2,3,4,...\} \, $ , $ \ O = \{ n \in \mathbb{N} : n \text{ is odd} \} \ $ and $ \ T: O \to O \ $ be such that, for all $ \ n \in \mathbb{N} \, $ , \begin{align*}
T(8n+1) & = 6n+1 \\
T(8n+3) & = 12n+5 \\
T(8n+5) & = 2n+1 \\
T(8n+7) & = 12n+11
\end{align*} For each $ \ n \in O$ , let $ \ S(n) = \big( n, T(n) , T(T(n)), T(T(T(n))), ... \big)$ . Prove that $S(n)$ is eventually $1$ , for all $ \ n \in O \, $ . A sequence of real numbers $ \ (x_1,x_2,x_3,x_4, ...) \ $ is eventually $ \, c \, $ if, and only if, there exists $ \ N \in \mathbb{N} \ $ such that $ \ x_n = c \, $ , for all $ \ n > N$ . My attempt: I tried to separate classes where this function were closed. \begin{align*}
T(8n+1) & = 6n+1 & = & \left\{ 
    \begin{array}{ll}
8 \cdot (3k) +1 \ , & n=4k \ ; \\
8 \cdot (3k) +7 \ , & n=4k+1 \ ; \\
8 \cdot (3k+1) +5 \ , & n=4k+2 \ ; \\
8 \cdot (3k+2) +3 \ , & n=4k+3 \ .
    \end{array}  
                     \right. \\
T(8n+3) & = 12n+5 & = & \left\{ 
    \begin{array}{ll}
8 \cdot (3k) +5 \ , & n=2k \ ; \\
8 \cdot (3k+2) +1 \ , & n=2k+1 \ .
    \end{array}  
                     \right. \\
T(8n+5) & = 2n+1 & = & \left\{ 
    \begin{array}{ll}
8k+1 \ , & n=4k \ ; \\
8k+3 \ , & n=4k+1 \ ; \\
8k+5 \ , & n=4k+2 \ ; \\
8k+7 \ , & n=4k+3 \ .
    \end{array}  
                     \right. \\
T(8n+7) & = 12n+11 & = & \left\{ 
    \begin{array}{ll}
8 \cdot (3k+1) +3 \ , & n=2k \ ; \\
8 \cdot (3k+2) +7 \ , & n=2k+1 \ .
    \end{array}  
                     \right. \\
\end{align*} But I failed and I don't know how to proceed from here.","['elementary-number-theory', 'sequences-and-series']"
4921028,Diameter of $S_{n^2}$ with respect to two copies of $S_{n} \wr S_{n}$,"Let $G=S_{n^2}$ the symmetric group on a grid of $n^2$ letters. Now $G$ contains a maximal subgroup $H \cong S_n \wr S_n$ by the O'Nan-Scott theorem, and we can think of elements that look something like $(\sigma_1,\dots,\sigma_n)\tau$ where $\sigma_i$ acts on the $i$ th row of letters, permuting letters within the row, and $\tau$ permutes the rows themselves. Similarly, there exists another isomorphic copy of this subgroup, call it $H'$ that acts as above but on columns. By maximality, $G=\langle H, H' \rangle$ . So then my question: what is the diameter of the Cayley graph $\Gamma (G,H\cup H')$ ?","['permutations', 'group-theory', 'finite-groups']"
4921135,Doubt about a proof in complex analysis,"In proving that a continuous function $$f:S(0,1)\to \mathbb{R}$$ Has a continuous extension to $$h:\overline{B(0,1)}\to \mathbb{R}$$ Such that $h|_{B(0,1)}$ is harmonic, one needs to show that the following integral vanishes $$\int_{0}^{2\pi}\frac{1-|z|^2}{|e^{i\theta}-z|^2}|f(w)-f(e^{i\theta})|d\theta$$ As $z\to w$ where $|w| = 1$ . In this (pdf download) set of notes, the proof of this is given as follows: Now, we split the integral above in two. One over the range of values of $\theta$ for which $|e^{i\theta} - w| < \delta$ and the other over the complementary range. For any $\epsilon > 0$ the former can be smaller than $\epsilon$ by the choice of $\delta$ . This is because $f$ is continuous. And for fixed $\delta$ the latter tends to zero as $z$ approaches $w$ in view of property (c) of the Poisson Integral. Since $\epsilon$ is arbitrary, the limit in Part (b) follows. The issue with this proof is that it seems like we fix $\delta$ first, and then choose $z$ close enough to the unit circle, however, our choice of $\delta$ is such that the integral is less than $\epsilon$ , but this integral necessarily depends on $z$ , and so it is not possible to choose $\delta$ without first having chosen $z$ , and this is not a trivial issue, since the term $$ \frac{1-|z|^2}{|e^{i\theta}-z|^2} $$ Diverges as $z\to e^{i\theta}$ . Perhaps this argument can be fixed, but I cannot see a simple way to justify it. Am I correct in finding this reasoning suspicious? What I've tried: Some simplifying assumptions can be made. Firstly, we may assume w.l.o.g. that $w = 1$ i.e. that it lies on the positive real axis, we can make the same assumption about $z$ , calling it now instead $s \in (0,1)$ . If we are able to bound the integral by choosing $|s-1|<\delta$ where $\delta$ does not depend on our choice of $w$ to be on the real line, so that the same $\delta$ would work along any ""ray"", then through some uniform continuity arguments we get that this integral vanishes. So the new limit is taken over the real numbers, i.e. we need to show that $$\lim_{s\to 1}\int_{0}^{2\pi}\frac{1-s^2}{|e^{i\theta}-s|^2}|f(1)-f(e^{i\theta})|d\theta = 0$$","['definite-integrals', 'harmonic-functions', 'real-analysis', 'complex-analysis', 'limits']"
4921252,Bounds on growth of Gröbner bases,"Let $I$ be an ideal with $k$ generators i.e. $I = \langle f_1, \dots, f_k\rangle$ where $f_1, \dots, f_k \in k[x_1, \dots, x_n]$ and fix a monomial ordering. I am interested in what happens to the size $s$ of the reduced Gröbner basis $G$ of $I$ if we add a new generator $f_{k+1}$ . If $\overline{f_{k+1}}^G = 0$ the size $s$ does not change, if $f_{k+1}=1$ the size will drop to $s=1$ because $I=\langle1\rangle$ . Since the process of adding new elements defines an ascending chains of ideals it is also true that the size doesn't change at some point. Is there an upper limit on how much the size of the reduced Gröbner basis can increase in general? I am interested in examples like this. Where we have a differential operator like $$\partial = u\frac{\partial}{\partial x} + v\frac{\partial}{\partial y} + x\frac{\partial}{\partial u} + (y-1)\frac{\partial}{\partial v}$$ and a polynomial $$ q = x^2 + y^2-1$$ and study the ideals $$ I_k = \langle \partial^l q \,|\, l \le k\rangle.$$ This example is derived from a simple pendulum.
The sizes of the reduced Gröbner basis with fixed grevlex ordering are ( $k\to \# G$ ) $$0\to1, 1\to3, 2\to7, 3\to7, 4\to11,$$ at $k=4$ it becomes stationary.","['differential-algebraic-equations', 'commutative-algebra', 'groebner-basis', 'algebraic-geometry', 'differential-algebra']"
4921269,Simulate a Brownian motion by exponential time stepping,"Let $(B_t)_{t\ge0}$ be a Brownian motion. We can simulate a path of $(B_t)_{t\ge0}$ using the Euler-Maruyama discretization scheme. Now, in the paper Efficient Numerical Solution of Stochastic Differential Equations Using Exponential Timestepping a different time-discretization is proposed, where the timesteps are drawn from an exponential distribution (say with parameter $\alpha>0$ ), which is independent of the Brownian motion. While it's clear to me that when $T\sim\operatorname{Exp}(\alpha)$ is independent of $(B_t)_{t\ge0}$ , then $B_T$ has density $$\mathbb R\ni x\mapsto\frac\alpha2 e^{-\sqrt{2\alpha}x}\tag1$$ with respect to the Lebesgue measure, I don't get how I turn this into a practical algorithm. First of all, from their eq. (5), it seems like they agree with my claim about the density $(1)$ above. But then, in their eq. (18), they say that the density of $B_{t+T}-B_t$ would be $$\mathbb R\ni x\mapsto\frac\nu2e^{-\nu|x|}\tag2$$ instead; where $\nu:=\sqrt{2\alpha}$ . Inserting $t=0$ clearly yields a contradiction to the previous result (due to the $|\;\cdot\;|$ now occurring). Then, when it comes to simulation, we start our simulation at $B_0=0$ and then need to sample from $B_{T_1}$ , where $T_1:=T$ . If $(2)$ would be the density, then this should be a Laplace distribution . If $(1)$ is the distribution, I don't know ... In any case, we now can use that $$B^{(1)}_t:=B_{T_1+t}-B_{T_1}\;\;\;\text{for }t\ge0$$ is again a Brownian motion. Using that, we see that sampling of the next increment $B_{T_1+T_2}=B^{(1)}_{T_2}$ can be done the same way we sampled $B_{T_1}$ . Intermediate question : The only problem with that is that we don't get the actual time steps $T_i$ if we directly draw $B_{T_i}$ , right? How do we solve this issue? Can we simply draw $T_2$ from $\operatorname{Exp}(\alpha)$ and then generate the next increment from $\mathcal N(B_{T_1},T_2)$ ? But the what I don't understand is the following: The actual goal was simulation of a path of $(B_t)_{t\ge0}$ . Are we still generating such a path, when we follow the scheme above? Don't we alter the distribution of the paths by this exponential time stepping? Remark : I know that the paper is motivated by situations where $(B_t)_{t\ge0}$ could hit some ""boundary wall"". I'm actually exactly in this situation and so that approach seems promising for my application.","['exponential-distribution', 'stochastic-processes', 'brownian-motion', 'probability-theory']"
4921285,Find $X$ and $\theta$ in this quadrilateral,"$ABCD$ is a convex quadrilateral, with $\angle A = 120^\circ$ . $\angle B = \angle C =\theta$ . $AB = 10$ , $AD = BC = X $ and $ CD = 2 X $ . Find $ X $ and $ \theta $ . My attempt: From the law of cosines applied to $\triangle ACD $ and $\triangle ABC $ , we have: $AC^2 = (2 x)^2 + x^2 - 4 x^2 \cos(360^\circ - 120^\circ - 2 \theta ) = 10^2 + x^2 - 20 x \
cos \theta $ Similarly, from the law of cosines applied to $\triangle DAB $ and $\triangle BCD$ , we have: $BD^2 = (2 x)^2 + x^2 - 4 x^2 \cos \theta = 10^2 + x^2 - 20 x \cos 120^\circ $ Now I have two equations relating the two unknowns $X$ and $\theta$ .  However, I don't know how to solve them, except numerically using the multivariate Newton-Raphson method. Similar problem can be found here and here .","['trigonometry', 'geometry']"
4921412,Why does $\left(\vec{a}\cdot\nabla\right)\nabla\frac{1}{r}=\vec{a}\frac{4\pi}{3}\delta\left(\vec{r}\right)$?,"I got this identity from a physics textbook , but figure the Math Stack Exchange could help. Allegedly: $$
\left(\vec{a}\cdot\nabla\right)\nabla\frac{1}{r}=\vec{a}\frac{4\pi}{3}\delta\left(\vec{r}\right)
$$ Where $r=\left(x^2+y^2+z^2\right)^{1/2}$ ans $\vec{a}$ is a constant vector. And I must have misunderstanding since when I carry out the computation, $$
\nabla\frac{1}{r}=-\frac{\vec{r}}{r^3}=-\hat{x}\frac{x}{r^3}-\hat{y}\frac{y}{r^3}-\hat{z}\frac{z}{r^3}
$$ $$
\left(\vec{a}\cdot\nabla\right)\nabla\frac{1}{r}=\left(
a_x\partial_x+a_y\partial_y+a_z\partial_z
\right)\left(
-\hat{x}\frac{x}{r^3}-\hat{y}\frac{y}{r^3}-\hat{z}\frac{z}{r^3}
\right)
$$ Using: $$
\partial_x\frac{1}{r^3}=-3\frac{x}{r^5}
$$ I get, for the x-component of $\left(\vec{a}\cdot\nabla\right)\nabla\frac{1}{r}$ : \begin{equation}
\tag{1}\label{xComponent}
a_x\frac{-1}{r^3}+3a_x\frac{x^2}{r^5}+3a_y\frac{xy}{r^5}+3a_z\frac{xz}{r^5}
\end{equation} I see how the x-component goes to $\infty$ when $\vec{r}=0$ , but do not see how it is zero for arbitrary $\vec{r}\neq 0$ and the factors of $a_y$ and $a_z$ make it seem that the x-component of $\left(\vec{a}\cdot\nabla\right)\nabla\frac{1}{r}$ is not proportional to $a_x$ . Which brings me to my question: Why does $\left(\vec{a}\cdot\nabla\right)\nabla\frac{1}{r}=\vec{a}\frac{4\pi}{3}\delta\left(\vec{r}\right)$ ? By the way, the identity can be found in 2.8 of appendix F in Blundell's Magnetism in Condensed Matter . Thanks all in advance",['multivariable-calculus']
4921419,Extensions by scalars along Frobenius,"My question probably just needs the knowledge of some commutative algebra but it is ultimately motivated from the theory of formal groups in chromatic homotopy theory. Question. Let $R$ be an $\mathbb{F}_p$ -algebra and $\operatorname{Frob}_R : R \to R, \ r \mapsto r^p$ . Is $$R \otimes_{R, \mathrm{Frob}_R} R [[x ]] \cong R [[x]]$$ (as topological rings with the $(x)$ -adic topology)? If it's easier, feel free to deal with polynomial rings and just rings (without a topology) - I'm not sure how relevant that is. This would show $\operatorname{Spf}(R \otimes_R R [[x ]]) \cong \operatorname{Spf}(R [[x ]])$ which has to do with the relative Frobenius and is claimed in Example 13.2 of Piotr's Finite Height Chromatic Homotopy Theory . If $R$ has some additional properties, like being a perfect field over $\mathbb{F}_p$ or so, then I would be fine with it. But right now it looks like the tensor product is isomorphic to $R^p[[x^p]]$ which should not be isomorphic to $R[[x]]$ in general. So possibly some details about the topology are relevant...?","['algebraic-geometry', 'abstract-algebra', 'homotopy-theory', 'commutative-algebra']"
4921431,Families of $4$-subsets with small intersection,"Consider this MSE question which has following setup: We have a set $X$ with cardinality $= n$ and a family $\mathcal{F}$ of $4-$ subsets of $X$ such that for two distinct $A, B \in \mathcal{F}$ , $|A\cap B| \leq 1$ . How large can such a family be? The answer given there implies an upper bound of $1 + n + \binom{n}{2}$ using a much more general theorem of Frankl and Wilson. How can we directly prove a quadratic (perhaps improved) upper bound? For a lower bound, the best I can do is a simple $n^2/16$ bound: Let $X = [n]$ and for $l(x) = ax + b$ , define the set $S_{l}$ to be $\{l(0), l(1), l(2), l(3)\}$ . Then the family $\mathcal{F} = \{S_{ax+b}|a, b \in [n/4]\}$ satisfies the intersection condition because two lines can only meet at a single point, and $\mathcal{F}$ has size $n^2/16$ .","['combinatorial-designs', 'combinatorics', 'extremal-combinatorics']"
4921438,Find the exact value of $\DeclareMathOperator{\cosec}{cosec} \cosec(10^\circ) + \cosec(50^\circ) - \cosec(70^\circ)$ [duplicate],"This question already has answers here : Evaluating $\csc\left(\frac{\pi}{18}\right) + \csc\left(\frac{5\pi}{18}\right) - \csc\left(\frac{7\pi}{18}\right)$ (3 answers) Closed last month . Find the exact value of $\cosec(10^\circ) + \cosec(50^\circ) - \cosec(70^\circ)$ . The equation can be written as $$
\cosec(x) + \cosec(60^\circ-x) - \cosec(60^\circ+x),
$$ where $x = 10^\circ$ . I also know that $$
\sin(x) + \sin(60^\circ-x) + \sin(60^\circ+x) = \frac{\sin(3x)}{4}.
$$ This equation and identity have the same format, so I got the idea to use it. Can I use this information to find the answer? (This question is from a book called Advanced Problems in JEE, in case someone wants to know about it.)","['algebra-precalculus', 'trigonometry']"
4921490,Continuity on the torus,"Let $\mathbb{T}^d$ be the $d$ -dimensional torus, for all $r >0,\eta_r(x):=e^{-r|x|^2},x \in \mathbb{R}^d,$ we denote by $\mathscr{F}^{-1}\eta_r$ the inverse Fourier transform defined by $\mathscr{F}^{-1}\eta_r(x):=\sum_{k \in \mathbb{Z}^d}\eta_r(k)e^{2\pi\mathrm{i}\langle x,k\rangle},x \in \mathbb{T}^d.$ Let $v_0 \in C^{\infty}(\mathbb{T}^d),$ Is it true that $$\forall x \in \mathbb{T}^d,\lim_{r \to 0}\mathscr{F}^{-1}\eta_r*v_0(x)=v_0(x)?$$ I know how to prove this when we are dealing with $\mathbb{R}^d:$ $$\forall x \in \mathbb{R}^d,\int_{\mathbb{R}^d}v_0(x-y)\mathscr{F}^{-1}\eta_r(y)dy=\int_{\mathbb{R}^d}\mathscr{F}^{-1}\eta_1(y)v_0(x-ry)dy$$ where we used a simple linear change of variable, we conclude using the dominated convergence theorem, that the limit is $v_0(x).$ How to treat the problem when we are on the torus? Unfortunately, the above change of variable won't work.","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'analysis', 'real-analysis']"
4921494,Fractal Geometry: Show that the Hausdorff dimension of a set and its image under $f(x)=x^2$ are the same.,"Let $f:\mathbb{R} \to \mathbb{R}$ be the function $f(x) = x^2$ , and let $F$ be any subset of $\mathbb{R}$ . Show that $\text{dim}_Hf(F) = \text{dim}_\mathrm{H} F$ . Here $\text{dim}_\mathrm{H}$ refers to Hausdorff dimension. It was easy to show that $\text{dim}_\mathrm{H} f(F) \leq \text{dim}_\mathrm{H} F$ since $f$ is Lipschitz in bounded regions. For the reverse inequality I am stuck. I can do it in the case that $F \subset \mathbb{R}_{\geq 0}$ because then $f$ is invertible and $f^{-1}$ is Lipschitz in regions bounded away from $0$ . In general, I don't know how to deal with the fact that $f$ is not injective.","['measure-theory', 'hausdorff-measure', 'fractals']"
4921504,Hypergeometric-like distribution but where non-successes are replaced?,"I'm working on a card game and say the deck has C cards where the player draws 3 cards every turn, picks a card, and the rest are shuffled back into the deck. They can do this N times. There are X of special cards which the player will nearly always pick if drawn. I'm trying to determine the best value of X to ensure the player sees on average around 2 or 3 of these cards in N draws. This seems similar to what hypergeometric calculators do, but if I'm not mistaken, they assume that no cards are replaced, and binomial calculators assume all cards are replaced. I could simulate this over a couple thousand runs in a python script, but is there a good mathematical formula I could apply to think kind of problem?","['statistics', 'card-games', 'probability']"
4921614,What is the time derivative of speed?,"For a particle in projectile motion with a constant upward acceleration of $-9.8\,\mathrm{m/s},$ the time derivative of its speed looks like this: The time derivative of velocity is acceleration, but what is the time derivative of speed , specifically $\sqrt{(x'(t))^2+(y'(t))^2}$ ?","['calculus', 'terminology']"
4921626,Find $x$ in this quadrilateral,"A quadrialteral $ABCD$ has $AB = 10$ , $\angle A = 50^\circ, \angle B = 120^\circ$ , $ BC = x , CD = x + 2 , AD = x + 4 $ .  Find $x$ . My attempt: Applying the law of cosines to $\triangle DAC$ and $\triangle ABC$ , to obtain $ AC^2 = (x + 4)^2 + (x+2)^2 - 2 (x + 2) (x + 4) \cos(190^\circ - C) = 10^2 + x^2 - 20 x \cos 120^\circ $ And by applying the law of cosines to $\triangle DAB $ and $\triangle BCD $ , I obtained $ BD^2 = 10^2 + (x + 4)^2 - 20 (x + 4) \cos 50^\circ = x^2 + (x+2)^2 - 2 x (x + 2) \cos C $ It remains to solve the two equations for $x$ and $\cos C$ . Your help on this or through an alternative solution method is much appreciated. Similar problem are found here and here .","['trigonometry', 'algebra-precalculus', 'quadrilateral', 'geometry']"
4921634,Simplifications to a Large (Dottie) Integral,"While working on a problem, I encountered this integral. $$
\alpha \in \left(-\frac{3\pi }{2},\frac{\pi }{2}\right),\ \ \ \mathfrak{D}_{\alpha }=\pi+\frac{1}{\pi}\int _0^{\infty }\left(1-\cosh \left(z\right)\right)\left(\frac{\frac{9\pi ^2}{4}+\frac{3\pi }{2}\alpha -z\sinh \left(z\right)+z^2}{\left(\sinh \left(z\right)-z\right)^2+\left(\frac{3\pi }{2}+\alpha \right)^2}-\frac{\frac{\pi ^2}{4}+z^2-z\sinh \left(z\right)-\frac{\pi }{2}\alpha }{\left(\alpha -\frac{\pi }{2}\right)^2+\left(z-\sinh \left(z\right)\right)^2}\right)\mathrm{d}z
$$ Derriving inspiration from this paper ( NOTE: THIS LINK DOWNLOADS A PDF!! ) I also found that: $$
\alpha \in \left(-\frac{\pi }{2},\frac{\pi }{2}\right), \ \ \ \mathfrak{D}_{\alpha }=\frac{\pi }{2}-\frac{1}{2\pi }\int _0^{\infty }\ln \left(\frac{\left(t+\sinh \left(t\right)\right)^2+\left(\alpha +\frac{\pi }{2}\right)^2}{\left(t-\sinh \left(t\right)\right)^2+\left(\alpha -\frac{\pi }{2}\right)^2}\right)\mathrm{d}t
$$ Are there any simplifications possible to express the first integral more concisely? (If you are curious, $\mathfrak{D}_{\alpha }=\cos \left(\cos \left(\cdots \left(\cos \left(x\right)-\alpha \right)\cdots -\alpha \right)-\alpha \right)-\alpha $ . Which is the solution to $\cos \left(x\right)-x=\alpha $ ).","['complex-analysis', 'calculus']"
4921643,Wirtinger Matrix Derivative Chain rule,"I'm trying to compute the matrix Wirtinger derivative $$\frac{\partial (f\circ g)(Z)}{\partial Z}$$ where $g(Z) := B(A Z-Z A)$ and $f(g(Z)):= \mathrm{Tr}\left(\sqrt{g(Z)^* g(Z)}\right)$ .
Here $Z$ is a complex Hermitian (also positive) matrix and $f:\mathbb{C}^{n\times n} \mapsto \mathbb{C}^{n\times n}$ is the nuclear or Ky Fan norm. I'm stuck with applying the chain rule: I'm not sure how to contract the tensors arising from $\frac{\partial g(Z)}{\partial Z}$ (which is a fourth-order tensor according to MatrixCalculus.org ) with the matrix $\frac{\partial f(W)}{\partial W}\big{\vert}_{W=g(Z)}$ . I know from this answer that $$\frac{\partial f(W)}{\partial W} = W(W^TW)^{-1/2} \, .$$ Could I get some help on this? I can derive $$\frac{\partial \mathrm{Tr}(AZ)}{\partial Z} = A^T$$ using the chain rule, but I can't extend it to this $f\circ g$ .","['complex-analysis', 'matrix-calculus', 'derivatives', 'chain-rule']"
4921646,Understanding the proof of $L^{\infty}$ is complete.,"I got lost when reading the proof of $L^{\infty}$ is complete. The book proceed the proof as follows: We show that each absolutely convergent series in $L^{\infty}(X,\mathscr{A},\mu)$ is convergent. We do this by considering functions (instead of equivalence classes) in $\mathscr{L}^{\infty}(X,\mathscr{A},\mu)$ . Let $\{f_k\}$ be a sequence of functions that belong to $\mathscr{L}^{\infty}(X,\mathscr{A},\mu)$ and satisfy $\sum_k\|f_k\|<+\infty$ . For each positive integer $k$ , let $N_k=\{x\in X:|f_k(x)|>\|f_k\|_{\infty}\}$ . Then the series $\sum_kf_k(x)$ converges at each $x$ outside $\bigcup_kN_k$ , and the function $f$ defined by \begin{align*}
f(x) = 
\begin{cases}
\sum_kf_k(x)\quad&\text{if $x\notin\bigcup_kN_k$},\\
\\
0\quad&\text{if $x\in\bigcup_kN_k$}
\end{cases}
\end{align*} is bounded and $\mathscr{A}$ -measurable. Since $\bigcup_kN_k$ is locally $\mu$ -null, the inequality \begin{align*}
\left\|f-\sum_{k=1}^nf_k\right\|_{\infty} \leq \sum_{k=n+1}^{\infty}\|f_k\|_{\infty}\tag1
\end{align*} holds for each $n$ , and so \begin{align*}
\lim_{n\to\infty}\left\|f-\sum_{k=1}^nf_k\right\|_{\infty} \leq \lim_{n\to\infty}\sum_{k=n+1}^{\infty}\|f_k\|_{\infty} = 0.\tag2
\end{align*} Thus $L^{\infty}(X,\mathscr{A},\mu)$ is complete. I have a couple of questions about this proof. The definition of $L^p(X,\mathscr{A},\mu)$ says that the elements of $L^p(X,\mathscr{A},\mu)$ are equivalence classes of functions. Why is it legit to proceed the proof by considering functions in $\mathscr{L}^p(X,\mathscr{A},\mu)$ ? In the proof, it says ""the series $\sum_kf_k(x)$ converges at each $x$ outside $\bigcup_kN_k$ "". Here is how I understand this step, and I want to know if it is correct? If $x\notin\bigcup_{k=1}^{\infty}N_k$ , then $|f_k(x)|\leq\|f_k\|_{\infty}$ for all $k\in\mathbb{N}$ , and thus the convergence of $\sum_{k=1}^{\infty}\|f_k\|_{\infty}$ implies that $\sum_{k=1}^{\infty}f_k(x)$ converges by the comparison test. The proof claims that $f$ is $\mathscr{A}$ -measurable. I couldn't see why this is true. I want to show that for each $t\in\mathbb{R}$ the set $\{x\in X:f(x)<t\}\in\mathscr{A}$ . But I honestly don't know how to do this. Can someone please help me out? I got complete lost by inequality (1) and (2). Why does $\bigcup_kN_k$ being locally $\mu$ -null imply that the inequality (1) holds for each $n$ ? Why can we just take limit for both side without proving the limits exist? Please please help! I really appreciate it! Note: $\quad$ The book I am reading defines $\|f\|_{\infty}$ to be the infimum of those nonnegative numbers $M$ such that $\{x\in X:|f(x)>M|\}$ is locally $\mu$ -null. Reference: $\quad$ Theorem 3.4.1 from Measure Theory by Donald Cohn","['measure-theory', 'proof-explanation', 'analysis', 'real-analysis', 'lp-spaces']"
4921680,Maximum value for an function limited to points on the unit circle,"I am hoping to find the maximum value for the function $\left(2^x \cdot 3^y\right)$ , but limited to points on the unit circle $\left(x^2 + y^2 = 1\right)$ . I have found approximations, $x=0.533, y=0.846$ , function value $= 3.665$ ,
but I would like to be able to know how to express exact values, and this is beyond my math know-how. I would also be interested in expanding to 3 dimensions for the function $(2^x \cdot 3^y \cdot 5^z)$ , for points on a unit sphere $(x^2 + y^2 + z^2 = 1)$ .
I have similarly found approximations $x=0.335, y=0.531, z=0.778$ ; function value $= 7.911$ ,
but I would also like to be able to know how to express exact values. Any help would be greatly appreciated!
Best,
Erik","['maxima-minima', 'functions']"
4921690,"How to evaluate $\int_0^{\frac{\pi}{4}} \tan(x) \ln^2(\sin(4x)) \, dx$?","Question: How to evaluate $$\int_0^{\frac{\pi}{4}} \tan(x) \ln^2(\sin(4x)) \, dx?$$ My attempt We will denote the main integral as $\Omega$ . $$\Omega=\int_0^{\frac{\pi}{4}} \tan(x) \ln^2(\sin(4x)) \, dx=  \int_0^{\frac{\pi}{4}} \tan(x)  \left[\left(\ln 2 + \ln(\sin(2x)) + \ln(\cos(2x))\right)^2\right] dx$$ We start by using the logarithmic property for the argument $\sin(4x)$ $$
\ln^2(\sin(4x)) = \left(\ln 2 + \ln(\sin(2x)) + \ln(\cos(2x))\right)^2
$$ Expanding the square, we obtain: $$
\ln^2(\sin(4x)) = \ln^2(2) + \ln^2(\sin(2x)) + \ln^2(\cos(2x)) + 2 \ln 2 \ln(\sin(2x)) + 2 \ln 2 \ln(\cos(2x)) + 2 \ln(\sin(2x)) \ln(\cos(2x))
$$ Substituting this back into the integral, we get: $$
\Omega = \int_0^{\frac{\pi}{4}} \tan(x) \left[\ln^2(2) + \ln^2(\sin(2x)) + \ln^2(\cos(2x)) + 2 \ln 2 \ln(\sin(2x)) + 2 \ln 2 \ln(\cos(2x)) + 2 \ln(\sin(2x)) \ln(\cos(2x))\right] dx
$$ Can someone help me proceed further, or suggest any better ideas?
Thanks!!","['integration', 'definite-integrals', 'calculus', 'trigonometric-integrals', 'closed-form']"
4921696,Is there a faster way to find max and min of $P = 4x + 2y + 3$ given $|x| + 2|y| = 4$?,"Is there a faster way to find max and min of $P = 4x + 2y + 3$ given $|x| + 2|y| = 4$ ? From a geometrical standpoint, the set of points that satisfy $|x| + 2|y| = 4$ is a diamond on the $Oxy$ plane. We also have that $y = -2x + \dfrac{P-3}{2},$ which is a line with slope $-2.$ In order for this line to intersect the diamond, it must be within the lines $y = -2x + 8$ and $y = -2x -8,$ as shown in the image. Thus, $-8\leq \dfrac{P-3}{2} \leq 8,$ so we have $-13 \leq P \leq 19$ . While this argument allows me to obtain the final answer (for this multiple-choice question), it sounds a bit like ""Looking at the image, we see that..."" I hope to receive some help to make this argument more rigorous and also hear alternative, faster approaches. Thank you!","['analytic-geometry', 'calculus', 'inequality']"
4921712,How to solve $\int \frac{2020x^{2019}+2019x^{2018}+2018x^{2017}}{x^{4044}+2x^{4043}+3x^{4042}+2x^{4041}+x^{4040}+1}dx$,"One of my friends sent me a list of integrals (all without solutions )  one of those problems is: $$\int \frac{2020x^{2019}+2019x^{2018}+2018x^{2017}}{x^{4044}+2x^{4043}+3x^{4042}+2x^{4041}+x^{4040}+1}dx$$ the numerator is $\frac{d}{dx}x^{2018}(1+x+x^2)$ and the denominator is $\left(x^2\left(x^{2018}(1+x+x^2)\right)\right)^2 +1$ so unless I am mistaken this integral is in the form of $\int\frac{f'(x)}{1+(x^2f(x))^2}dx$ which I don't know how to solve, maybe (If this problem is unsolvable ) there is a typo,  but I couldn't verify that whether this problem has a typo or not since wolfram alpha for some reason don't understand my input. Since this question seems to be incorrect I wounder what is the result of $\int_{- \infty}^{\infty} \frac{2020x^{2019}+2019x^{2018}+2018x^{2017}}{x^{4044}+2x^{4043}+3x^{4042}+2x^{4041}+x^{4040}+1}dx$ Does this have a nice closed form
?","['integration', 'definite-integrals', 'calculus', 'closed-form', 'indefinite-integrals']"
4921752,Barycentre of a ball,"I saw the following definition for the barycentre of a set $\Omega \subseteq \mathbb{R}^n$ : $$
\mathrm{bc}^\Omega=\frac{1}{\mathrm{vol}(\Omega)}\int_\Omega x dx \in\mathbb{R}^n.
$$ So I wanted to compute the barycenter for a ball of radius $r$ with center $x_0, \quad \Omega= B_r(x_0)$ , and from my understanding it should be $\mathrm{bc}^\Omega=x_0.$ In the Original post my computation was wrong here is the corrected version: \begin{equation}
\begin{aligned}
\mathrm{bc}^\Omega_i&=\frac{1}{\omega_n r^n}\int_0^r\int_{\partial B_\rho(x_0)}\xi_i dS\xi d\rho\\
  &=\frac{1}{\omega_n r^n}\int_0^r\int_{\partial B_\rho(x_0)}|\xi-x_0|\left(\left\langle e_i, \frac{\xi-x_0}{|\xi-x_0|} \right\rangle + \left\langle e_i, \frac{x_0}{|\xi-x_0|} \right\rangle \right) dS\xi d\rho\\
  &=\frac{1}{\omega_n r^n}\int_0^r\rho\int_{B_\rho(x_0)}div(e_i) dx + \int_{\partial B_\rho(x_0)}\rho\left\langle e_i, \frac{x_0}{|\xi-x_0|} \right\rangle dS\xi d\rho\\
  &=\frac{x_{0, i} \cdot n}{r^n}\int_{0}^{r}\rho^{n-1}d\rho=x_{0, i}
\end{aligned}
\end{equation} Where I first transformed the integral domain to integrate over spheres, then reformulated the integrand to include the unit normal to those spheres. Applied the divergence theorem to the constant $e_i$ and integrated as usual in the second summand.
I also used the formulae: $$
\int_{B_r(x_0)}1 dx= r^n\omega_n,\quad 
\int_{\partial B_r(x_0)}1 dS\xi= r^{n-1}\omega_n\cdot n
$$ where $\omega_n=\frac{\pi^{n/2}}{\Gamma(\frac n2+1)}$ is the volume of the unit ball.","['integration', 'analysis', 'real-analysis', 'calculus', 'vector-analysis']"
4921770,"Is the ordered pair definition $(𝑥,𝑦):=\{𝑥,\{𝑥,𝑦\}\}$ a good definition?","In answer to this post There is no one fixed way to define an ordered pair in terms of sets. It is also common to define an ordered pair as $(𝑥,𝑦):=\{𝑥,\{𝑥,𝑦\}\}$ . One can prove that $\{x_1, \{x_1,y_1\}\} = \{x_2, \{x_2,y_2\}\} \iff x_1 = x_2 \text{ and }y_1=y_2 \label{1}\tag{$*$}.$ I've tried the proof, and the process is questionable:
take simplest case when $$
x=y,\, u=v,\, \langle x,y\rangle=\langle u,v\rangle \iff \{𝑥,\{𝑥\}\}=\{u,\{u\}\},$$ to which $x=u,\{𝑥\}=\{u\}$ is a solution. As we can immediately see,  also $x=\{u\},u=\{𝑥\}$ can be a solution as long as there exist a set such that $x=\{\{x\}\}$ , i.e. if $x$ is a set containing a set containing itself. Now searching the internet for an example of a set containing itself, I've found this Quora Q&A , where it states that in ZFC set theory there isn't a set containing itself (so I think it also means a set containing a set containing itself wouldn't exist). On the other hand,
other non-ZFC set theories allows a set containing itself to exist (so I assume a set containing a set containing itself would also exist under such theory), so under this theory
we cannot prove \eqref{1}. On the contrary to Kuratowski's definition $\langle x,y\rangle=\{\{x\},\{x,y\}\}$ , Wiener's definition $$
\langle x,y\rangle=\{\{\{x\},\emptyset\},\{\{y\}\}
$$ is a good one because it don't rely on such a particular axiom of set theory. It works in set theories other than ZFC, even works in naive set theory.","['elementary-set-theory', 'set-theory']"
4921791,Almost Sure Convergence of Rayleigh Distributed Random Variables,"I am working on a problem involving almost sure convergence and Rayleigh distributed random variables, and I need some help to confirm my understanding and approach. The problem is as follows: Let $X_1, X_2, X_3, \ldots$ be a sequence of random variables such that $X_n \sim \operatorname{Rayleigh}\left(\frac{1}{n}\right)$ , i.e., $$
f_{X_n}(x)= \begin{cases}n^2 x \exp \left\{-\frac{n^2 x^2}{2}\right\} & \text { if } x>0 \\ 0 & \text { otherwise. }\end{cases}
$$ I need to show that $X_n \xrightarrow{\text { a.s. }} 0$ .
Here is my approach: I understand that to prove $X_n \xrightarrow{\text { a.s. }} 0$ , I need to show that for any $\epsilon>0$ , $$
\sum_{n=1}^{\infty} P\left(\left|X_n\right|>\epsilon\right)<\infty
$$ Using the given PDF, I computed the probability: $$
P\left(X_n>\epsilon\right)=\int_\epsilon^{\infty} n^2 x \exp \left\{-\frac{n^2 x^2}{2}\right\} d x \text {. }
$$ I made a change of variable $u=n x$ , so $d u=n d x$ , leading to: $$
P\left(X_n>\epsilon\right)=\int_{n \epsilon}^{\infty} u \exp \left\{-\frac{u^2}{2}\right\} \frac{d u}{n}
$$ Recognizing the integral as the tail of a Rayleigh distribution, which can be expressed in terms of the complementary error function erfc, I approximate: $$
P\left(X_n>\epsilon\right) \approx \exp \left\{-\frac{n^2 \epsilon^2}{2}\right\}
$$ I then check the series: $$
\sum_{n=1}^{\infty} \exp \left\{-\frac{n^2 \epsilon^2}{2}\right\}
$$ Given the rapid decay of the exponential term, this series converges.
Therefore, by the Borel-Cantelli Lemma, $X_n \xrightarrow{\text { a.s. }} 0$ . Could someone please verify if this approach and conclusion are correct? Is there any step I should reconsider or any additional insight needed to solidify the proof? Thank you (Also, I am very new in probability theory)!","['convergence-distribution', 'statistics', 'probability-distributions', 'probability-theory', 'probability']"
4921867,How to Minimize $PC + \frac{1}{2}PA$ for a Point on a Circle Geometrically?,"Given the points $ A(1, 0) $ , $ B(5, 0) $ , and $ C(0, 5) $ , a circle is drawn with center at point $ B $ and radius 2. Let $ P $ be a moving point on this circle. I need to find the minimum value of $ PC + \frac{1}{2}PA $ , where $ P $ is the point on the circle. Any insights or alternative approaches would be greatly appreciated! Here's a solution. To find the minimum value of $ PC + \frac{1}{2}PA $ geometrically, we can use the following approach: Identify Points and Segments: Given points: $A(1, 0)$ , $B(5, 0)$ , and $C(0, 5)$ . Circle centered at $B$ with radius $2$ . Let $P$ be a point on the circle. Select point $ F $ on segment $ AB $ such that $ BF = 1 $ . Triangles and Similarity: Since $ PB = 2 $ and $ BF = 1 $ : $$
 \frac{BF}{PB} = \frac{1}{2}
 $$ Triangles $ \triangle PBF $ and $ \triangle ABP $ are similar because: $$
 \frac{PB}{AB} = \frac{2}{4} = \frac{1}{2} = \frac{BF}{PB}
 $$ Therefore, by AA similarity ( $\angle PBF = \angle ABP$ ), we have: $$
 \triangle PBF \sim \triangle ABP
 $$ Proportional Segments: Since the triangles are similar: $$
 \frac{PF}{PA} = \frac{BF}{PB} = \frac{1}{2} \implies PF = \frac{1}{2}PA
 $$ Inequality and Minimum Value: From the above proportional relationship: $$
 PC + \frac{1}{2}PA = PC + PF \geq CF
 $$ When $ C $ , $ P $ , and $ F $ are collinear, $ PC + \frac{1}{2}PA $ is minimized, and this minimum value is the length of segment $ CF $ . Calculate ( CF ): Since $ O $ is the origin, $ OC = 5 $ and $ OF = OB - 1 = 5 - 1 = 4 $ : $$
 CF = \sqrt{OC^2 + OF^2} = \sqrt{5^2 + 4^2} = \sqrt{41}
 $$ Thus, the minimum value of $ PC + \frac{1}{2}PA $ is: $$
\sqrt{41}
$$ The limitation of this method is that if the coordinates of key point A change, and we still need to find the minimum value of $PC + \frac{1}{2}PA$ , this method will no longer be applicable.","['optimization', 'geometry']"
4921913,Evaluation of the given line integral,"Question: Evaluate $\int_{C}$ B .d r along the curve $x^{2}$ + $y^{2}$ =1, $z$ = 1 in the positive direction from (0,1,2) to (1,0,2);given B = (xz²+y) i +(z-y) j +(xy-z) k The question itself is easy,but I don't know how to handle z=1 Here's my attempt:- $\int_{C}$ B .d r = $\int_{C}$ (xz²+y) i +(z-y) j +(xy-z) k .(dx i +dy j +dz k ) = $\int_{C}$ (xz²+y)dx+ $\int_{C}$ (z-y)dy+ $\int_{C}$ (xy-z)dz Should I put z=1 in the above integral?after this I will integrate all the three integrals and put up the values given in the question.","['integration', 'definite-integrals', 'multivariable-calculus', 'multiple-integral', 'line-integrals']"
4921953,A geometry problem regarding the relationship between the circumradius and the radii of three other triangles [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last month . The community reviewed whether to reopen this question last month and left it closed: Original close reason(s) were not resolved Improve this question I came across this problem in my geometry book and still cannot find the solution. I am strugling with it for over a week now and I kind of ran out of ideas how to approach it. So some hints would be really appreciated. The problem states: Three tangents parallel to the sides of the triangle are drawn to the circle inscribed in the triangle. These tangents cut off three triangles from the given triangle, the radii of the circumscribed circles of which are equal to $R_1$ , $R_2$ , $R_3$ . Express the radius of the circumscribed circle of the given triangle in terms of $R_1$ , $R_2$ , and $R_3$ .","['euclidean-geometry', 'circles', 'geometry']"
4921986,Construct an explicit biholomorphism between two domains,"This problem is from my homework: Construct an explicit biholomorphism between $D_1=\mathbb{C}-\{-x\pm\sqrt{-1}\pi\mid x\ge1\}$ and $D_2=\{x+\sqrt{-1}y\mid -\infty<x<\infty, -\pi<y<\pi\}$ . It is obvious that we can find a biholomorphism between $D_2$ and the upper plane, but the defination of $D_1$ is so strange and complex that I have no idea how to deal with it. Any help would be appreciated.","['complex-analysis', 'conformal-geometry', 'complex-numbers']"
4922004,"Largest collection of subsets of [1,n] such that any union of two subsets is unique","Let $S$ be the set of integers from 1 to $n$ . Then there are $2^n$ subsets of $S$ . My question is what is the size of the largest collection, $F$ , of these subsets such that the union of any distinct $X,Y\in F$ is unique. So given any set, $T$ , that is the union of two sets in $F$ , those two sets should be uniquely determined by $T$ . $F$ has $|F|\choose 2$ sets that are the union of two sets in $F$ and each are unique. This means $|F|\choose 2$$\leq 2^n$ , so $|F|\leq\lceil 2^{\frac{n+1}{2}}\rceil$ . On the other hand, if we take $F$ to be the collection of sets with one element as well as the empty set, we get that $|F|\geq n+1$ . I am not sure how to proceed to find better bounds. Edit: In P Erdös, P Frankl, Z Füredi's paper, 'Families of finite sets in which no set is covered by the union of two others', https://www.sciencedirect.com/science/article/pii/0097316582900048?ref=pdf_download&fr=RR-2&rr=889623b298c079c3 , they look at the size of the largest collection of subsets, $F$ , of an $n$ element set such that for any distinct $A,B,C \in F$ , $A\notin B\cup C$ . They find a lower bound of $1.134^n$ . If $A,B,C,D\in F$ are such that $A\neq B$ , $C\neq D$ , { $A,B$ } $\neq$ { $C,D$ } and $A\cup B=C\cup D$ then $A\in C\cup D$ and $B\in C\cup D$ . As either $A,C,D$ or $B,C,D$ are all distinct, this is a contradiction. So any $F$ that satisfies the condition in the paper also satisfies our condition. Therefore $|F|\geq 1.134^n$ .","['combinatorics', 'extremal-combinatorics', 'set-theory']"
4922055,"Regarding a Coin Toss Experiment by Neil DeGrasse Tyson, and its validity","In one of his interviews, Clip Link , Neil DeGrasse Tyson discusses a coin toss experiment. It goes something like this: Line up 1000 people, each given a coin, to be flipped simultaneously Ask each one to flip if heads the person can continue If the person gets tails they are out The game continues until 1* person remains He says the ""winner"" should not feel too surprised or lucky because there would be another winner if we re-run the experiment! This leads him to talk about our place in the Universe. I realised, however, that there need not be a winner at all , and that the winner should feel lucky and be surprised! (Because the last, say, three people can all flip tails) Then, I ran an experiment by writing a program with the following parameters: Bias of the coin : 0.0001 - 0.8999  (8999 values) Number of people : 10000 Number of times experiment run per Bias : 1000 I plotted the Probability of 1 Winner vs Bias The plot was interesting with zig-zag for low bias (for heads) and a smooth one after p = 0.2 . (Also, there is a 73% chance of a single winner for a fair coin). Is there an analytic expression for the function $$f(p) = (\textrm{probability of $1$ winner with a coin of bias $p$}) \textbf{?}$$ I tried doing something and got here: $$
f(p)=p\left(\sum_{i=0}^{e n d} X_i=N-1\right)
$$ where $X_i=\operatorname{Binomial}\left(N-\sum_{j=0}^{i-1} X_j, p\right)$ and $X_0=\operatorname{Binomial}(N, p)$","['analyticity', 'graphing-functions', 'binomial-distribution', 'combinatorics', 'probability']"
4922073,Is Schwarz's Lemma true for squares?,"In a recent complex analysis exam, we were asked which step(s) in the proof of the Riemann Mapping Theorem fail, when you replace every instance of the open unit ball $\mathbb{E}$ with the square $$\mathbb{S} = \{z\in\mathbb{C}:|Re(z)|, |Im(z)| < 1 \}$$ One error that arose is that the square root function is not a self map of $\mathbb{S}$ . But what I thought was a problem was that we used Schwarz's Lemma, in particular Given a holomorphic $f:\mathbb{E}\to \mathbb{E}$ that fixes $0$ , we have $|f'(0)|\leq 1$ The proof for this goes by noting that we can find a holomorphic $g$ such that $$f(z)= zg(z)$$ and then for $0<r<1$ since $|g|$ achieves it's maximum on $\overline{B(0,r)}$ at a point $z_r\in\partial B(0,1)$ , we have $$|g(0)|\leq \frac{|f(z_r)|}{|z_r|} \leq \frac{1}{r}$$ where this last inequality follows since the codomain of $f$ is $\mathbb{E}$ , and $z$ in $\partial B(0,r)$ implies $|z| = r$ . In the case of $\mathbb{S}$ however, it may happen that $|z_r| = r$ , but our only obvious bound for the numerator is $$|f_r(z)|< \sqrt{2}$$ So we get only that $$|f'(0)| < \sqrt{2}$$ But of course, just because this proof doesn't work, doesn't mean the result is not true, but it is suspicious. Hence Question: Prove or disprove Schwarz's Lemma for $\mathbb{S}$ If it is true, can the same be said for all simply connected open sets? If it is false, is the disc the only geometry for which this result holds?","['complex-analysis', 'inequality', 'analysis', 'real-analysis']"
4922091,Ring where all elements are zero divisors.,"I know that the collection of all nilpotent elements in a commutative ring form such a ring where all elements are zero divisors. For example, if we take $S=\lbrace 0,2,4,6 \rbrace$ as a subring of $\mathbb{Z_8}$ , then all elements of $S$ are zero divisors and nilpotent. If we look at $\mathbb{Z_{p^n}}$ , then we arrive at another example of this. But I'm struggling to find a well-known example where not every element is nilpotent. More specifically, I want to find such a ring that at least one of the element of the ring is a zero divisor but not nilpotent. How should I think of such an example?","['ring-theory', 'abstract-algebra']"
4922126,"Let $a,b,c,d\in\mathbb{C}$ such that $|a|+|b|\leq 1$ and $|c|+|d|\leq 1$. Show that $|3a+b+3c-d|+|a+3b-c+3d|\leq 7$.","Let $a,b,c,d\in\mathbb{C}$ such that $|a|+|b|\leq 1$ and $|c|+|d|\leq 1$ . Show that $|3a+b+3c-d|+|a+3b-c+3d|\leq 7$ , or find a counterexample -- I don't know for sure that the inequality stated in the title is true. In the case of $a,b,c,d\in\mathbb{R}$ , the constraint set is a polytope and the objective is convex. Thus, simply by checking its value on all vertices $\{(\pm 1,0), (0,\pm 1)\}^2$ , we see that the upper bound is $6$ . I'm not sure about the complex case though. It looks like the above approach won't work, because the feasible set has uncountably many extreme points. By naive Monte Carlo sampling it seems that the maximum is around $6.33$ .","['complex-analysis', 'inequality', 'complex-numbers']"
4922135,Change of coordinates and compatibility condition for an area constraint,"Let $x=x(a,b), y=y(a,b)$ and consider the change of coordinates generated by the infinitesimal transformation $\delta a = -b \delta \phi$ and $\delta b = a \delta \phi$ for some function $\delta\phi$ . This is an (rigid or isometric) rotation of the coordinates. Initially, we take $\delta \phi$ to be independent of $(a,b)$ . Let $(x,y)$ satisfy $$\frac{\partial(x,y)}{\partial(a,b)}=x_ay_b-x_by_a=J(a,b).$$ Are there any restrictions on the functions $(x,y)$ so that the function $J$ remains fixed under the coordinate transformation? I believe this implies that there is a compatibility condition restricting $x_{aa}+x_{bb}=y_{aa}+y_{bb}=0$ . What about the case where $\phi=\phi(b)$ , that is the angle of the rotation depends on one of the independent variables? Is there a corresponding compatibility condition?","['multivariable-calculus', 'calculus', 'differential-geometry']"
4922137,"For a reduced abelian $p$-group $G$, does $P$ being finite imply $G$ is finite?","Let $G$ be a reduced abelian $p$ -group, and let $P$ be the subgroup of elements of order $p$ . If $P$ is finite, is $G$ necessarily also finite? Recall that a group $G$ is said to be reduced if it contains no divisible subgroups. A group $G$ is said to be divisible if every element is divisible by $n$ for each natural number $n$ . This seems to be true in the case where the length of the group is at most $\omega$ (i.e. $G$ is a direct sum of cyclic groups), but I cannot find a proof of this fact without this restriction. Here is the proof in that case: in order for the group to be infinite, the group must contain either infinitely many copies of $\mathbb{Z}_{p^n}$ for some $n$ , in which case clearly $P$ is infinite, or it must contain copies of $\mathbb{Z}_{p^n}$ for arbitrarily high $n$ , each copy introducing at least one new element of order $p$ .","['group-theory', 'abelian-groups', 'p-groups']"
4922153,Vector field with almost non-periodic orbits,"Let $M$ be an n-dimensional smooth manifold (Open or compact). I want to know if it is possible to construct an smooth vector field with exactly one singularity, such that the set of periodic integral curves has null n-dimensional Lebesgue measure. Any reference/idea is appreciate.","['vector-fields', 'differential-topology', 'dynamical-systems', 'differential-geometry']"
4922167,Solving $y''(x)-2xy'(x)+y(x) = 0$ using power series,"I'm solving $y''(x) -2xy'(x) +y(x) = 0$ using the power series ansatz $y(x) = \sum_{n=0}^{\infty}a_n x^n$ . Plugging in I get: \begin{equation}
\bigg(\sum_{n=0}^{\infty}n(n-1)a_nx^{n-2}\bigg)-2x\bigg(\sum_{n=0}^{\infty}na_nx^{n-1}\bigg)+\sum_{n=0}^{\infty}a_n x^n = 0 
\end{equation} \begin{equation}
\sum_{n=0}^{\infty}x^n \big((n+2)(n+1)a_{n+2} -2na_n+a_n\big) = 0 
\end{equation} \begin{equation}
(n+2)(n+1)a_{n+2} -2na_n+a_n = 0 
\end{equation} \begin{equation}
a_{n+2} = \frac{2n-1}{(n+2)(n+1)}a_n
\end{equation} Now we set $a_0$ and $a_1$ as parameters, to observe a pattern: \begin{align}
n\qquad & \\\\
0\qquad & a_2 = \frac{-1}{2 \cdot 1} a_0 &= \frac{a_0}{2!}\prod_{l=0}^{0}(4l-1) \\\\
1\qquad & a_3 = \frac{1}{3\cdot 2} a_1 &= \frac{a_1}{3!}\prod_{l=0}^{0}(4l+1) \\\\
2\qquad & a_4 = \frac{3}{4\cdot 3} a_2 = \frac{3}{4 \cdot 3} \frac{-1}{2\cdot 1} a_0 &= \frac{a_0}{4!} \prod_{l=0}^{1}(4l-1) \\\\
3\qquad & a_5 = \frac{5}{5\cdot 4} a_3 = \frac{5}{5\cdot 4}\frac{1}{3\cdot 2} a_1 &= \frac{a_1}{5!}\prod_{l=0}^{1}(4l+1) \\\\
4 \qquad & a_6 = \frac{7}{6\cdot 5} a_4 = \frac{7}{6\cdot 5}\frac{3}{4 \cdot 3} \frac{-1}{2\cdot 1} a_0 &= \frac{a_0}{6!} \prod_{l=0}^{2}(4l-1)\\\\
5 \qquad & a_7 = \frac{9}{7\cdot 6} a_5 = \frac{9}{7\cdot 6}\frac{5}{5\cdot 4}\frac{1}{3\cdot 2} a_1 &= \frac{a_1}{7!}\prod_{l=0}^{2}(4l+1) \\\\
6 \qquad & a_8 = \frac{11}{8\cdot 7} a_6 = \frac{11}{8\cdot 7}\frac{7}{6\cdot 5}\frac{3}{4 \cdot 3} \frac{-1}{2\cdot 1} a_0 &= \frac{a_0}{8!} \prod_{l=0}^{3}(4l-1)\\\\
7 \qquad & a_9 = \frac{13}{9\cdot 8} a_7 = \frac{13}{9 \cdot 8}\frac{9}{7\cdot 6}\frac{5}{5\cdot 4}\frac{1}{3\cdot 2} a_1 &= \frac{a_1}{9!}\prod_{l=0}^{3}(4l+1) \\\\
\cdots
\end{align} So I now only need to plug these into $y(x)$ . For that I split the sum into odd and even parts: \begin{align}
y(x) &= \sum_{n=0}^{\infty}a_n x^n \\\\&= \bigg( \sum_{k=0}^{\infty}a_{2k} x^{2k} \bigg) + \bigg( \sum_{k=0}^{\infty}a_{2k+1} x^{2k+1} \bigg) \\\\&= \bigg(a_0 + \sum_{k=1}^{\infty}a_{2k} x^{2k} \bigg) + \bigg(a_1x + \sum_{k=1}^{\infty}a_{2k+1} x^{2k+1} \bigg) \\\\&= \bigg(a_0 + \sum_{k=1}^{\infty} x^{2k} \frac{a_0}{(k+2)!}\prod_{l=0}^{k}(4l-1)  \bigg) + \bigg(a_1x + \sum_{k=1}^{\infty} x^{2k+1} \frac{a_1}{(k+2)!} \prod_{l=0}^{k}(4l+1) \bigg) \\\\&= a_0 \bigg(1 + \sum_{k=1}^{\infty} \frac{x^{2k}}{(k+2)!}\prod_{l=0}^{k}(4l-1)  \bigg) + a_1 \bigg(x + \sum_{k=1}^{\infty} \frac{x^{2k+1}}{(k+2)!} \prod_{l=0}^{k}(4l+1) \bigg) 
\end{align} But now I tried to check the solution by plugging it back in the equation. But first I made following abbreviations: \begin{equation}
\alpha_k = \frac{1}{(2+k)!}\prod_{l=0}^{k}(4l-1) \qquad\qquad \beta_k = \frac{1}{(2+k)!}\prod_{l=0}^{k}(4l+1)
\end{equation} So we now have: \begin{align}
y(x) &= a_0 \bigg(1 + \sum_{k=1}^{\infty} x^{2k} \alpha_k  \bigg) + a_1 \bigg(x + \sum_{k=1}^{\infty}x^{2k+1} \beta_k \bigg) \\\\
y'(x) &= a_0 \bigg(\sum_{k=1}^{\infty} 2k x^{2k-1} \alpha_k  \bigg) + a_1 \bigg(1 + \sum_{k=1}^{\infty}(2k+1)x^{2k} \beta_k \bigg) \\\\
y''(x) &= a_0 \bigg(\sum_{k=1}^{\infty} 2k(2k-1) x^{2k-2} \alpha_k  \bigg) + a_1 \bigg(\sum_{k=1}^{\infty}(2k+1)2kx^{2k-1} \beta_k \bigg) 
\end{align} I started with following term: \begin{align}
2xy'(x)-y(x) &= a_0 \bigg(\sum_{k=1}^{\infty} 4kx^{2k} \alpha_k  \bigg) + a_1 \bigg(2x + \sum_{k=1}^{\infty}2(2k+1)x^{2k+1} \beta_k \bigg) \\\\ & \quad- a_0 \bigg(1+\sum_{k=1}^{\infty} x^{2k} \alpha_k  \bigg) - a_1 \bigg(x + \sum_{k=1}^{\infty}x^{2k+1} \beta_k \bigg) \\\\ &= a_0 \bigg(-1 + \sum_{k=1}^{\infty} x^{2k}(4k-1) \alpha_k  \bigg) + a_1 \bigg(x + \sum_{k=1}^{\infty}(4k+1)x^{2k+1}\beta_k\bigg) 
\end{align} But this obviously doesn't equal $y''(x)$ . Where did I go wrong?","['power-series', 'ordinary-differential-equations']"
4922170,The Fourier transform of $e^{-i/x}$,"$\def\R{\mathbb R}$ Question. Does anyone know what is the Fourier transform of $$ f(x)=e^{-i/x} $$ on the real line? I would like to compute it explicitly, or to establish some properties to have a good feeling of “how it looks like”. What I know I know that $\widehat f$ is real-valued by the symmetries of the Fourier transform, since $f(-x)=\overline{f(x)}$ . It is natural to consider $h(x)=f(x)-1$ . It is immediate that $h$ decays like $-i/x$ for large $x$ , so $\widehat h$ belongs to $L^p(\mathbb R)$ for any $p\in [2,\infty)$ by Hausdorff-Young inequality. Then $\widehat f$ is recovered just by adding a Dirac delta. Motivation. (not necessary to understand the problem) For some reason, I was looking at the “anti-transport” equation $$ \left\{\begin{aligned}&u_t-\partial_x^{-1}u=0,\\&u|_{t=0}=u_0.\end{aligned}\right. $$ Even if it does not entirely make sense, one can consider the unitary group $e^{t\partial_x^{-1}}$ , which is well-defined on $L^2$ , that acts on the Fourier side as a multiplication by $e^{-it/\xi}$ . The kernel of this PDE coincides with a rescaled version of the anti-Fourier transform of $e^{-i/\xi}$ , in particular it holds $$ u(t)=u_0+H_t*u_0, \quad\text{with}\quad H_t(x):=t\hat h(tx) $$ for any $u_0\in L^1(\R)\cap L^2(\R)$ (up to multiplicative constants). This is why I considered the above problem. I was wondering what is the time-regularity of solutions of such PDEs, even for smooth initial data: I find a bit funny that the operator $\partial_x^{-1}$ should in principle “smoothen things out” if well-defined, but it seems that the time-derivative of a solution of the PDE is not necessarily smooth for smooth data, unless one imposes some low-frequency condition on the initial datum (see below). A similar PDE with dispersion relation that is singular at low frequencies pops up in the linear water wave theory for the deep water case: the PDE would look something like $$ u_t-|\partial_x|^{-1/2}u=0. $$ Edit : as pointed out by Matthew Cassell, one can write the PDE as a system: $$ \left\{\begin{aligned}u_t&=v\\v_x&=u\end{aligned}\right. $$ In principle, $v$ is defined from $u$ only up to an additive constant, which could depend on time in a non-trivial way. This system gives the following information for solutions defined via the $L^2$ -group, $u(t)=e^{t\partial_x^{-1}}u_0$ : if the initial datum $u_0$ lies in $\partial_x L^2(\R)$ , then one can see that $u\in C(\mathbb R;\partial_x L^2(\R))$ , in particular the anti-derivative $v$ is well-defined and $v\in C(\mathbb R; L^2(\R))$ , so since $u_t=v$ , we obtain that $u-u_0\in C^1(\R; L^2(\R))$ . This procedure can be iterated: Lemma Let $u_0\in \partial_x^j L^2(\R)\cap H^k(\R)$ , and $u(t):=e^{t\partial_x^{-1}}u_0$ . One has $u-u_0\in C^j(\R;H^{j+k}(\R))$ . This is a way of seeing how the time regularity of the solution is related to the low frequencies of the initial datum $u_0$ vanishing. More precise information can be derived (like fractional regularity in time) by looking at the Fourier transform of the solution in space-time.","['fourier-analysis', 'fourier-transform', 'real-analysis', 'functional-analysis', 'partial-differential-equations']"
4922186,Derived set of a closed subspace,"Suppose $X$ is a compact Hausdorff topological space with a basis of clopen sets. Let $A$ be a closed subspace of $X$ and let $A^{(0)}=A$ , $A^{(1)}=A^\prime$ , etc. My question is: it is true that $A^{(n)}=A\cap X^{(n)}$ for all $n\in\mathbb{N}$ ? Since $A\subset X$ , then $A^{(n)}\subset X^{(n)}$ . Also, since $A$ is closed, one can prove by induction on $n$ that $A^{(n)}\subset A$ . These two things together prove one of the directions of the problem. But I can’t prove that $A\cap X^{(n)}\subset A^{(n)}$ .","['general-topology', 'set-theory']"
4922245,Manipulating Algebraic Equations,"I'm sorry if this question (or possibly multiple depending on how long this I intend for this to be) is a little too elementary, but I've been seriously struggling with this for the past week.  I never really questioned this until recently and it has me stumped and I can't move on with math without knowing the answer. Say I have the equation $3x+5=20$ So, I understand that subtracting 5 from both sides and dividing both sides by 3 would keep the equality true.  What I don't understand is how doing all this somehow doesn't change the value of x.  The equality is true but what if x is not equal to what it was in the original equation anymore?  The solution is of course x=5, I have no problem mechanically solving the equation but this intuitively is hard for me to grasp.  How can we really know that x=5 is true and have complete confidence in the answer without substitution?  It's even harder to trust the solution after a series of these operations, especially when they're more complex like taking the log of both sides, exponentiation, etc.",['algebra-precalculus']
4922280,Is there a way to determine if the n-th roots of a polynomial is a polynomial?,"I was this problem: $$\int\frac{dx}{\sqrt{x^4+2x^3+3x^2+2x+1}}$$ I solved this question because I just knew that $(1+x+x^2)^2=x^4+2x^3+3x^2+2x+1$ but this made me wonder is there is a way to know if the $n-$ th root of a certain polynomial is a polynomial?
Given a polynomial how to determine $n$ where the $n-th$ root of this polynomial is a polynomial and how to determine the $n-$ th root of this polynomial? There is Taylor theorem or the extended binomial theorem that can find $(P(x))^{1/n}$ but this is not what I am looking for because It only deals with two terms and the more terms that a polynomial have the more one needs to use the binomial theorem on each term which gets very ugly. It gives an infinite series which doesn't tell if each the $n-$ th root is a polynomial or not Even if one can make simplifications on the  infinite series to conclude it is a polynomial finding suitable $n$ would need one to check all the cases from $2-m$ where $m$ is the degree of polynomial, which is not very effective.","['algebra-precalculus', 'binomial-theorem', 'soft-question', 'polynomials']"
4922294,How to verify a scheme is a fibre product,"Above is one proposition that I found in Wedhorn& Görtz's Algebraic Geometry I in page 103. I do not understand two parts: (1) He uses the assumption II that the induced maps on stalk at any points are isomorphisms to conclude that the schemes $(X',\mathscr O_{X'})\cong (f(X'),\mathscr O_{X|f(X')})$ is isomorphism. I know if the induced maps on stalks are isomorphism at any points then the sheaves are isomorphic but how to show the underlying topological spaces are homeomorphic? I think it has something to do with the assumption that $f$ can be decomposed into homeomorphisms onto it image. But I do not have any ideas. I do not even know if $f(X')$ is open in $X$ . (2) The author claims that $(p^{-1}(f(X')),\mathscr O_{Z|p^{-1}(f(X'))})$ is the fibre product. I know why $p^{-1}(f(X'))$ should be fibre product as topological spaces but I do not know how to check why $\mathscr O_{Z|p^{-1}(f(X'))}$ is the right sheaf that we want to put on such that the whole diagram becomes not only the fibre product as topological spaces but also as schemes.","['algebraic-geometry', 'sheaf-cohomology', 'schemes', 'sheaf-theory']"
4922330,Calculating a Conditional expectation,"My question is the following. Given that we have $n$ i.i.d. random variables $X_1,...,X_n$ with distribution $f(x)=\frac{2}{\lambda^2}x\mathbf{1}_{[0,\lambda]}(x)$ , where $\lambda> 0$ is some parameter, how do I calculate the the conditional expectation $$E[X_i|X_{\max}],$$ with $X_{\max}=\max\{X_1,...,X_n\}$ . My idea was writing $X_i=X_i\mathbf{1}_{\{X_i=X_{\max}\}}+X_i\mathbf{1}_{\{X_i<X_{\max}\}}$ . But this only left me with $$E[X_i|X_{\max}]=X_{\max}\mathbb{P}(X_i=X_{\max}|X_{\max})+E[X_i\mathbf{1}_{\{X_i<X_{\max}\}}|X_{\max}],$$ which doesn't really gelp me I think. I've been given one hint: In the end we should have $$\sum_{i=1}^n E[X_i|X_{\max}]=\frac{2n+1}{3}X_{\max},$$ So my guess is $$E[X_i|X_{\max}]=\frac{2n+1}{3n}X_{\max}.$$ Any suggestions on how to proceed are welcome. Thank you.","['statistics', 'conditional-probability', 'conditional-expectation', 'probability-theory', 'random-variables']"
4922358,"The Roman army has 2018 units guarding their provinces. Prove that after 64 days, there were no more provinces with at least 64 units.","Problem: The Roman army has 2018 units guarding their provinces. The Emperor was worried that when there are at least 64 units in a province, they might get together and overthrow the Emperor. So on each day, he visited one such potentially troublesome province and sent all its units to other provinces, no two going to the same province. Prove that after 64 days, there were no more provinces with at least 64 units. I don't really understand this problem. If on the first day, there are 2018 units in province 1, the Emperor must send them to 2018 provinces. So the number of provinces must be more than 2018. I would appreciate any help, hint, approach or solution The problem comes from IIMC 2018 Keystage 3 (EMIC), according to https://chiuchang.org/imc/wp-content/uploads/sites/2/2018/07/BIMC-2018_Keystage-3_Team.x17381.pdf , problem 6 from the Team category","['contest-math', 'pigeonhole-principle', 'combinatorics', 'discrete-mathematics']"
4922363,"Is $f(x) = \sin x$ the unique function satisfying all five: $f(0)=0;\ f'(0)=1;\ f(\pi/2)=1;\ f'(\pi/2)=0;\ -1\leq f''(x)\leq 0$ for $x\in [0,\pi/2] ?$","I would like to prove or find a counter-example to the following proposition (which I came up with), please. Suppose $f:[0,\pi/2]\to [0,1]$ is twice differentiable in the
interval $[0,\pi/2]$ . Suppose further that, $f(0) = 0;$ $f'(0) = 1;$ $f(\pi/2) = 1;$ $f'(\pi/2) = 0.$ Then, if $-1\leq f''(x)\leq 0$ for $x\in [0,\pi/2],$ then $f(x) = \sin
 x.$ I think if we did not impose the condition that $-1\leq f''(x)\leq 0$ for $x\in [0,\pi/2],$ then I think we can find a counter-example like: $f(x)$ is convex - i.e. $f''(x)>0$ - on $[0,0.1]$ , and then concave all the way to the point $(\pi/2,1).$ I first tried supposing that $f(x)$ is a quadratic, and got a contradiction, so no quadratics satisfy the first four conditions. I then tried supposing that $f(x)$ is a cubic, and determined that the only cubic satisfying the first four conditions is, $$ f(x) = \frac{4\pi-16}{\pi^3}x^3 + \frac{12 - 4\pi}{\pi^2} x^2 + x, $$ but then $f''(\pi/2) = -1.158\ldots < -1.$ So we can rule out cubics also. I'm not sure quite where to go from here, other than more tedious calculations trying quartics, quintics etc. Does anyone have the answer to this question off hand, or have any ideas for a counter-example or an affirmative proof of the proposition by contradiction?","['calculus', 'derivatives', 'trigonometry', 'polynomials']"
4922382,Limits of arbitrary polynomials divided by constants greater than 1 raised to the power of n,"Problem statement: Prove that for any $a > 1$ and any polynomial $p(x)$ , we have: \begin{align*}
\lim\limits_{n \to \infty} \dfrac{p(n)}{a^n} =0	
\end{align*} Solution: We need to use squeeze theorem and consider this expression as a sequence. The goal to show that: \begin{align*}
c_n	\leq \dfrac{p(n)}{a^n} \leq d_n \\
\text{where} \lim\limits_{n \to \infty}c_n =0 \text{ and }\lim\limits_{n \to \infty}d_n =0
\end{align*} For some arbitrary sequences $c_n$ and $d_n$ . consider $p(n)$ having some finite degree $k \in \mathbb{N}$ : \begin{align*}
p(n) = b_kn^k + b_{k-1}n^{k-1}+ ... + b_1n+b_0 	
\end{align*} Lemma: For any $a \in (-1,1)$ and $k,n \in \mathbb{N}$ we have: \begin{align*}
\lim\limits_{n \to \infty} n^ka^n =0
\end{align*} proof: when $a=0$ , the result is trivial. consider $0<a<1$ . Write: \begin{align*}
	a = \dfrac{1}{\frac{1}{a}} = \frac{1}{1+b}	
\end{align*} Where $\dfrac{1}{a} > 1$ and $b = \dfrac{1}{a} - 1 > 0$ . Using the binomial theorem: \begin{align*}
a^n &= \dfrac{1}{(1+b)^n} = \dfrac{1}{1 + nb + C^n_2 b^2+...+C^n_{n-1}b^{n-1} +b^n} \\
n^ka^n &= \dfrac{n^k}{1 + nb + C^n_2 b^2+...+C^n_{n-1}b^{n-1} +b^n}	
\end{align*} WLOG assume $n>k$ hence we have: \begin{align*}
	n^ka^n &= \dfrac{n^k}{1 + nb + C^n_2 b^2+...+C^n_{k+1}b^{k+1}+...+C^n_{n-1}b^{n-1} +b^n}	
\end{align*} Observe that: \begin{align*}
	C^n_{k+1} &= \dfrac{n!}{(n-k -1)!(k+1)!} = \dfrac{n(n-1)(n-2)...(n-k)}{(k+1)!}\\ 
	\implies \dfrac{n^k}{C^n_{k+1}} &= \dfrac{(k+1)!}{n(1-\frac{1}{n})(1-\frac{2}{n})...(1-\frac{k}{n})}
\end{align*} Now because: \begin{align*}
 C^n_{k+1}b^{k+1} \leq 	1 + nb + C^n_2 b^2+...+C^n_{k+1}b^{k+1}+...+C^n_{n-1}b^{n-1} +b^n 
 \end{align*} We have: \begin{align*}
	n^ka^n = \dfrac{n^k}{1 + nb + C^n_2 b^2+...+C^n_{n-1}b^{n-1} +b^n}	\leq \dfrac{n^k}{C^n_{k+1}} = \dfrac{(k+1)!}{n(1-\frac{1}{n})(1-\frac{2}{n})...(1-\frac{k}{n})}\\
	\lim\limits_{n \to \infty }\dfrac{(k+1)!}{n(1-\frac{1}{n})(1-\frac{2}{n})...(1-\frac{k}{n})} = \dfrac{\lim\limits_{n\to \infty}\frac{(k+1)!}{n}}{\lim\limits_{n\to \infty}(1-\frac{1}{n})\lim\limits_{n\to \infty}(1-\frac{2}{n})...\lim\limits_{n\to \infty}(1-\frac{k}{n})}
\end{align*} Because $\lim\limits_{n\to \infty}\dfrac{k}{n} =0$ for any constant $k \in \mathbb{N}$ : \begin{align*}
	 \dfrac{\lim\limits_{n\to \infty}\frac{(k+1)!}{n}}{\lim\limits_{n\to \infty}(1-\frac{1}{n})\lim\limits_{n\to \infty}(1-\frac{2}{n})...\lim\limits_{n\to \infty}(1-\frac{k}{n})} = \dfrac{(k+1)! \times 0}{(1-0)(1-0)...(1-0)} =0
\end{align*} Hence, we can indeed bound $n^ka^n$ above and below. $n^ka^n$ has a trivial lower bound of $0$ , as here we consider $0<a<1$ : \begin{align*}
0\leq 	n^ka^n \leq \dfrac{n^k}{C^n_{k+1}} \\
\lim\limits_{n \to \infty}0 =0 ,\lim\limits_{n \to \infty}\dfrac{n^k}{C^n_{k+1}} =0\\
\implies \lim\limits_{n \to \infty} n^ka^n =0 \text{ by squeeze theorem for } a \in (0,1)
\end{align*} For the negative case where $a \in (-1,0)$ consider: \begin{align*}
-|n^ka^n| \leq n^ka^n \leq |n^ka^n| = n^k|a^n| =n^k|a|^n
\end{align*} Because we have already proved that for $a \in (0,1)$ , $\lim\limits_{n \to \infty}n^ka^n =0$ : \begin{align*}
\lim\limits_{n \to \infty} n^k|a|^n &=0 \\
\implies \lim\limits_{n \to \infty}-|n^ka^n| &=0	 \\
\text{by the squeeze theorem, } \lim\limits_{n \to \infty} n^ka^n &=0 \qquad \text{for } \quad a \in (-1,0)
\end{align*} Hence, for any $a \in (-1,1)$ and $k \in \mathbb{N}$ we have: \begin{align*}
\lim\limits_{n \to \infty} n^ka^n =0
\end{align*} Now back to the main proof. For a polynomial $p(n)$ of some degree $k \in \mathbb{N}$ and some $a >1$ : \begin{align*}
 \lim\limits_{n \to \infty}\dfrac{p(n)}{a^n} &=  \lim\limits_{n \to \infty}\dfrac{b_kn^k + b_{k-1}n^{k-1}+ ... + b_1n+b_0}{a^n}\\
 &=  \lim\limits_{n \to \infty} \left[ \dfrac{b_k}{a^n}n^k +\dfrac{b_{k-1}}{a^n}n^{k-1} +...+\dfrac{b_1}{a^n}n+\dfrac{b_0}{a^n} \right]
\end{align*} consider any index $j \in (0,k)$ \begin{align*}
	b_j\dfrac{1}{a^n}n^j = b_j\left(\dfrac{1}{a}\right)^nn^j \qquad \because a>1 \implies \frac{1}{a} <1 \text{ and } \frac{1}{a} \in (-1,1)\\
	\implies \text{from the lemma} \lim\limits_{n \to \infty}  b_j\left(\dfrac{1}{a}\right)^nn^j = \lim\limits_{n \to \infty}b_j \lim\limits_{n \to \infty} \left(\dfrac{1}{a}\right)^nn^j = \lim\limits_{n \to \infty}b_j \times 0 =0
\end{align*} Hence, \begin{align*}
	\lim\limits_{n \to \infty} \left[ \dfrac{b_k}{a^n}n^k +\dfrac{b_{k-1}}{a^n}n^{k-1} +...+\dfrac{b_1}{a^n}n+\dfrac{b_0}{a^n} \right] &=0+0+...+0+0\\
	\lim\limits_{n \to \infty}\dfrac{p(n)}{a^n} &=0
\end{align*} Queries: Is this a valid proof? Are there more elegent ways to prove this, specifically using the properties of limits, sequences and/or polynomials? PS: This is not a homework question","['calculus', 'solution-verification', 'polynomials', 'sequences-and-series', 'limits']"
4922422,Görtz and Wedhorn Hilbert Nullstellensatz: what is lemma 1.9 being applied to?,"I am a physicist who made the hubris-filled decision to try and learn algebraic geometry; I am on the proof of Hilbert Nullstellensatz (in Görtz and Wedhorn) and am already stuck in the mud. For the setup, $A$ is a finite-generated $K$ -algebra ( $K$ not necessarily algebraically closed). We are considering some prime ideal $P \subset A$ and would like to show that in $R := A/P$ , the intersection of all maximal ideals is zero. To start, we consider some element $x \in R$ and the map $\phi$ defined as the composition of the natural localization map $R \rightarrow R[t]/(xt - 1)$ and another map into some field extension $R[t]/(xt-1) \rightarrow (R[t]/(xt-1)) / m$ , with $m$ being any maximal ideal of the localization ring. The proof in Görtz and Wedhorn is acceptable to me, except for the last part of this one statement: If $L$ is a finite field extension of $K$ and $\phi: A\rightarrow L$ is a $K$ -algebra homomorphism, the image of $\phi$ is an integral domain that is finite over $K$ . Thus $\mathrm{Im}\ \phi$ is a field by lemma 1.9. For reference, lemma 1.9 states: Let $A$ and $B$ be integral domains and let $A \rightarrow B$ be an injective integral ring homomorphism. Then $A$ is a field if and only if $B$ is a field. What ""injective integral"" map is lemma 1.9 being applied to? I don't think it is $\phi$ , since I don't believe the localization map is integral and the quotient map probably isn't injective. If it is the inclusion map $\iota: \mathrm{Im}\ \phi \rightarrow (R[t]/(xt-1))/m$ , then how can we see that it is an integral ring homomorphism?","['ring-theory', 'algebraic-geometry', 'commutative-algebra']"
4922436,Alternating series test to prove the convergence of the series,"I want to apply the alternating series test to prove the convergence of the series $\sum_{n\geq 1} (-1)^n u_n$ where $u_1=1$ and $\forall n\in \mathbb{N},\quad u_{n+1}=\frac{\cos(u_n)}{n^\alpha}$ where $\alpha>0$ . We can show that $0\leq u_n\leq 1$ for all $n$ and that $u_n\to 0$ . Numerically, it seems that the sequence $u_n$ is decreasing after a certain point, but I don't see how to prove it.","['sequences-and-series', 'real-analysis']"
4922487,Multivariable chain rule for function with one negative component,"I have a (smooth) function $\psi:\mathbb{R}^2 \to \mathbb{R}\: : \: (y_1,y_2) \mapsto \psi(y_1,y_2)$ . Now I want to calculate the partialderivative of $\psi(-y_1,y_2)$ with respect to $y_1$ . I thought of writing $\psi(-y_1,y_2) = (\psi \circ f)(y_1,y_2)$ where $f(y_1,y_2) = (-y_1,y_2)$ and then applying the chain rule. To do this I wrote $u_1 = -y_1, u_2 = y_2$ . $$
\frac{\partial \psi(-y_1,y_2)}{\partial y_1} = \frac{\partial \psi}{\partial u_1}(u_1,u_2) \frac{\partial u_1}{\partial y_1} + \frac{\partial \psi}{\partial u_2}(u_1,u_2) \frac{\partial u_2}{\partial y_1}  
$$ and since $\partial_{y_1} u_2 = 0$ and $\partial_{y_1} u_1 = -1$ we get: $$
\frac{\partial \psi(-y_1,y_2)}{\partial y_1} = -\frac{\partial \psi}{\partial u_1}(u_1,u_2) 
$$ Now I was wondering if we can rewrite the right hand side to some derivative of $y_1$ instead of a derivative of $u_1$ . Any help would be appreciated.","['analysis', 'multivariable-calculus', 'calculus', 'partial-derivative', 'chain-rule']"
4922506,Deriving the telegrapher PDE from a run-and-tumble model,"Generalized question Under what conditions on $f(t,x,v)$ do we have $$\mathbb{E}_v(v^T \nabla_x^2 f(t,x,v) v^T)=\mathbb{E}_v(\Delta_x f(t,x,v)),$$ where $v\sim \mathcal{U}(S^{n-1})$ ? Here $\nabla_x^2$ is the Hessian with respect to $x$ , and $\Delta_x$ is the Laplacian. Context: A rather general run-and-tumble process can be defined as a process that travels at a finite constant velocity in between poisson arrival times, at which point it changes direction, picking a random direction, i.e., picking a vector uniformly distributed on the hypersphere. The infinitesimal generator of such process is $$\mathscr{G} f = c v^T \nabla_x f(x,v) +\lambda \int_{S^{n-1}} [f(x,v') -f(x,v)] \nu\{dv'\}.$$ Here, $\nu$ is the uniform measure on the hypersphere's surface. The transition density $f(t,x,v)$ then satisfies the Kolmogorov forward pde, $$f_t + c v^T \nabla_x f = \lambda \int_{S^{n-1}} [f(x,v') -f(x,v)] \nu\{dv'\}.$$ We want to prove the following: Conjecture: Let $$p(t,x) = \int_{S^{n-1}} f(t,x,v) \nu\{dv\}.$$ then $p$ solves the telegrapher PDE, $$p_t = \frac12 \Box p,$$ where $\Box:-\frac{1}{c^2} \partial_tt+\Delta$ is the d'Alembertian operator with sign convention $(-1, 1,\dotsc, 1)$ . Proof Attempt Integrating the Kolmogorov forward PDE for $f$ over $v\in S^{n-1}$ with respect to the uniform measure on $S^{n-1}$ gives $$p_t + \nabla_x \cdot \vec{J} = 0,$$ where $$\vec{J} = \int_{S^{n-1}} cf \vec{v} \nu\{dv\}.$$ we have used the fact that $\vec{v}^T \nabla_x f = \nabla_x \cdot (f \vec{v})$ . differentiating this with respect to $t$ gives $$p_{tt} + \nabla_x \cdot \vec{J}_t = 0.$$ but $$\vec{J}_t = \int_{S^{n-1}} c f_t \vec{v} \nu\{dv\}.$$ Now, multiply the original balanced kolmogorov forward pde for $f$ by $c\vec{v}$ and integrate over $S^{n-1}$ . We obtain $$\vec{J}_t + c^2\int (v^T \nabla_x f) v \nu\{dv\} = \lambda p \mathbb{E}(V)-\lambda \vec{J},$$ $$=-\lambda \vec{J}$$ since $\mathbb{E}(V)=0$ . On the LHS we can pull out the gradient into a divergence, so that we obtain $$\vec{J}_t + c^2 \int \nabla_x \cdot( f \vec{v}) \vec{v} \nu\{dv\}=-\lambda \vec{J}.$$ Applying the divergence $\nabla_x \cdot $ gives, $$\nabla_x \cdot \vec{J}_t + c^2 \int_{S^{n-1}} \text{Tr}(vv^T \nabla_x^2 f) \nu\{dv\}=-\lambda \nabla_x \cdot \vec{J}.$$ Here we used the fact that $\nabla \cdot (\nabla \cdot (f \vec{v}) \vec{v}) = \text{Tr}(vv^T \nabla^2 f)$ . Thus, substituting the expressions for $\nabla_x \cdot \vec{J}$ and $\nabla_x \cdot \vec{J}_t$ , we obtain \begin{align*}
    p_t &= -\nabla_x \cdot \vec{J} \\
        &= \frac{1}{\lambda } \left[\nabla_x \cdot \vec{J}_t + c^2 \int_{S^{n-1}} \text{Tr}(vv^T \nabla_x^2 f) \nu\{dv\}\right] \\
        &= \frac{1}{\lambda} \left[-p_{tt} + c^2 \int_{S^{n-1}} \text{Tr}(vv^T \nabla_x^2 f) \nu\{dv\}\right]
\end{align*} from which the result follows if we can show that $$\int_{S^{n-1}} \text{Tr}(vv^T \nabla_x^2 f) \nu\{dv\} = \Delta p,$$ and upon setting $\lambda = 2c^2$ . It is possible to show, by taking the Hessian of the original forward pde in $(t, x,v)$ , multiplying by $v^T$ and $v$ on the left and right and using the trace trick to compute $\mathbb{E}[v^T \nabla_x^2 p v] = \text{Tr}(\nabla_x^2 p \mathbb{E}(vv^T))=\Delta_x p$ , that $$\mathbb{E}(v^T \nabla_x^2 f v) = -\frac{1}{\lambda}\mathbb{E}(v^T \nabla_x^2 f_t v + cv^T D_x[\nabla_x^2 f v]v) + \Delta_x p.$$ Thus, it suffices to show that that first expectation on the RHS is zero, i.e. $$\mathbb{E}[v^T \nabla_x^2 f_t v ]=-c\mathbb{E}[v^T D_x[\nabla_x^2 f v] v].$$ Here $D_x[\cdot]$ is the Jacobian matrix wrt $x$ . I have tried to think of many ideas to tackle this. It doesn't seem to hold in general for arbitrary joint densities $f(X,V)$ , by checking examples numerically, so it should depend on the fact that $f$ solves the integro-PDE of the run-and-tumble model. I know since $v$ is uniform on the hypersphere, it is rotationally invariant hence if $\nabla_x^2 f= Q \Lambda Q^T$ , $Q^TQ = I$ , i.e. orthogonal columns, then we have $\mathbb{E}(v^T \nabla_x^2 f v) = \mathbb{E}(v^T \Lambda v)$ where $\Lambda$ is the diagonal matrix of eigenvalues of the Hessian of $f$ . But I don't think this helps anywhere... Is there something obvious I missed, or is it possible that this is not true? Update 5/28/2024: It is now obvious to me that if $v\sim \mathcal{U}(\{e_1,\dotsc, e_n\})$ then for each $t,x$ , we indeed have $$\mathbb{E}(v^T \nabla_x^2 f(t,x,v) v) = \mathbb{E}(\Delta_x f(t,x,v)).$$ Here the $e_i$ are the standard basis vectors, e.g. in $3$ dimensions $(1,0,0), (0,1,0), (0,0,1)$ . In fact this still holds if $v\sim \mathcal{U}(S)$ where $S$ is the finite set containing each $e_i$ up to a sign. This is not hard to see, so I will omit the computation. This case includes the classic one dimensional telegrapher process that switches velocities from $\pm c$ , studied by Kac and Goldstein. But the general case still eludes me and is now starting to appear likely to be false.","['stochastic-processes', 'probability-theory', 'partial-differential-equations']"
4922552,Multicolor Ramsey Number - prove lower bound,"Prove lower bound $R(3, n, m) > 2 R(n, m) - 2$ where $m, n > 2$ I guess if we wanna prove that $R(3, n, m) > N$ we need to show an example of construction that would have $N$ vertices and would not match the definition. But I can't come up with such an example","['graph-theory', 'ramsey-theory', 'discrete-mathematics']"
4922608,Translation of odd and even functions,"Let $\varphi: \mathbb{R} \to \mathbb{R}$ be a periodic function of period $L>0$ , that is, \begin{equation}\label{periodicitycondition}
\varphi(x+L)=\varphi(x),\; \forall\; x \in \mathbb{R}. \tag{1}
\end{equation} First, it follows, immediately from \eqref{periodicitycondition} that, \begin{equation}\label{periodicityconsequence}
\varphi\left(x-\frac{L}{2}\right)=\varphi\left(x-\frac{L}{2}+L\right)=\varphi\left(x+\frac{L }{2}\right).\tag{2}
\end{equation} Now, let us define the function $\psi: \mathbb{R} \to \mathbb{R}$ given by $$
\psi(x)=\varphi\left(x-\frac{L}{4}\right),\; \forall \, x \in \mathbb{R}.
$$ Note that from \eqref{periodicitycondition}, it follows that $$
\psi(x+L)=\varphi\left(x+L-\frac{L}{4}\right)=\varphi\left(x-\frac{L}{4}+L\right)= \varphi\left(x-\frac{L}{4}\right)=\psi(x),\; \forall \, x \in \mathbb{R},
$$ that is, $\psi$ is also periodic with period $L>0$ . Note that, $\varphi$ and $\psi$ are defined in whole real line $\mathbb{R}$ . Question $\mathbf{1}$ . If $\varphi$ is odd, then $\psi$ is even? Question $\mathbf{2}$ . If $\varphi$ is even, then $\psi$ is odd? Attempt to answer Question $\mathbf{1}$ : On the one hand, from since $\varphi$ is odd we see that \begin{equation}\label{oddcondition3}
\psi(-x)=\varphi\left(-x-\frac{L}{4}\right)=-\varphi\left(x+\frac{L}{4}\right),\; \forall \, x \in \mathbb{R}.\tag{3}
\end{equation} On the other hand, using \eqref{periodicityconsequence} and the fact that $-\tfrac{L}{4}=\tfrac{L}{4}-\tfrac{L}{2}$ , we obtain that \begin{equation}\label{oddcondition4}
\psi(x)=\varphi\left(x-\frac{L}{4}\right)=\varphi\left(x+\frac{L}{4}-\frac{L}{2}\right )=-\varphi\left(-x-\frac{L}{4}+\frac{L}{2}\right)=-\varphi\left(-x+\frac{L}{4}\right ),\; \forall \, x \in \mathbb{R}.\tag{4}
\end{equation} Therefore, I cannot conclude that $\psi$ is even. I have a similar problem when I try to answer Question $2$ . Are the answers to Questions $1$ and $2$ affirmative? If so, how can I proceed?","['even-and-odd-functions', 'periodic-functions', 'real-analysis', 'calculus', 'functions']"
4922655,$79 \neq n_1^4+\cdots + n_{18}^4\ $ [$79$ is not a sum of eighteen $4$'th powers of integers],"This is something from Rosen's Discrete Math Textbook that was left for the reader to verify. I tried to prove it but I'm stuck: I understand that the 4th powers of integers are: $0^{4} = 0, 1^{4} = 1, 2^{4} = 16, 3^{4} = 81, \dots$ and so on. I also understand repetition is allowed here. So since $3^{4} > 79$ , we really can only use $0^{4} = 0, 1^{4} = 1$ , and $2^{4} = 16$ to create our sum of 18 4th powers that add to 79. Hence we have the following equations/inequalities: $x + 16y = 79$ (where x is number of 1's to 4th power and y is number of 2's to 4th power) $0 \leq x + y \leq 18$ (since we can have 4th powers of 0's taking up some of the 18 spots) $0 \leq y \leq 4$ (since at least 5 4th powers of 2 would be $ \geq 80 > 79$ ) $0 \leq x \leq 18$ (18 spots for 4th powers of 1's) Not sure how to proceed from here. I mean I graphed on desmos to see that the line $x + 16y = 79$ doesn't lie in the region of intersection of the inequalities. So there really is no solution that fits these restrictions, let alone an integer one. However, is there a better way to prove this? I feel if the author left it for the reader to verify themselves it should be much simpler than this. Kindly please do help me here.","['inequality', 'discrete-mathematics']"
4922675,$1^\alpha+2^\alpha+3^\alpha+\cdots+n^\alpha$,"Let $\alpha>0$ and $m$ be a positive integers, use Euler's summation formula we can prove that there exists a constant $C$ such that $$    \sum_{k=1}^nn^\alpha=\frac{n^{\alpha+1}}{\alpha+1}+\frac{n^\alpha}{2}+{\color{red}C}
  +\sum_{k=1}^{m}\frac{B_{2k}}{(2k)!}\alpha(\alpha-1)\cdots(\alpha-2k+2)n^{\alpha-2k+1}$$ $$+\frac{\alpha(\alpha-1)\cdots(\alpha-2m)}{(2m+1)!}
  \int_1^nB_{2m+1}(x-[x])\cdot x^{\alpha-2m-1}dx. $$ $$=\frac{n^{\alpha+1}}{\alpha+1}+\frac{n^\alpha}{2}+{\color{red}{C_1}}
  +\sum_{k=1}^{m}\frac{B_{2k}}{(2k)!}\alpha(\alpha-1)\cdots(\alpha-2k+2)n^{\alpha-2k+1}+O(n^{\alpha-2m-1}).$$ Here the two constants $C$ and $C_1$ maybe different. My question is: can we prove that the constant $C_1$ equals to $\zeta(-\alpha)$ ? Here $\zeta(s)$ is the Riemann zeta function.","['complex-analysis', 'number-theory', 'algebraic-number-theory', 'analytic-number-theory']"
4922704,Estimating Population Growth with Limited Information,"I have been thinking about these problems for a while and think I might have found a way to partly answer them. These problems deal with estimating the birth and death rate of a system in different scenarios. Scenario 1: Suppose at time=0 my population is $k_1$ . At some final time =t, my population is $k_2$ ( $k_1, k_2$ are positive integers) At each time point from (0,T), I assume that there is a probability of $p_1$ that the current population can increase by $j$ units and a probability of $p_2$ that the current population decrease by $l$ units (assume $p_1, p_2, j, l$ are all constant) I only observe the system at time=0 and time=t  (i.e. I know the exact values of $k_1, k_2$ . Assuming that the population can never go below 0, what are the most likely values of $p_1, p_2, j, l$ ? Scenario 2: Suppose at time=0 my population is $k_1$ . At some final time =t, my population is $k_2$ ( $k_1, k_2$ are positive integers) At each time point from (0,T), I assume that there is a probability of $p_1$ that the current population can increase by $j$ units and a probability of $p_2$ that the current population decrease by $l$ units (assume $p_1, p_2, j, l$ are all constant) I observe the population of time =0 and time=T ... as well as at some times $t_i$ (e.g. suppose I have intervals 0,1,2,3,4,5,6,7,8,9,10. I observe the population at times=0, 5,6,9,10 ... at time=0 population is $k_1$ , at time=5 population is $a$ , time= 6 population is $b$ , time = 9 population is $c$ , time=10 population is $k_2$ ) Assuming that the population can never go below 0, what are the most likely values of $p_1, p_2, j, l$ ? My Answers: For Scenario 1, I used a latent variable approach (e.g. EM algorithm/Gaussian Mixture) wrote: $$ L(p_1, p_2, j, l) = \sum_{j=0}^{J} \sum_{l=0}^{L} \sum_{n_1=0}^{T} \sum_{n_2=0}^{T} \left[ p_1^{n_1} \cdot (1 - p_1)^{T - n_1} \cdot p_2^{n_2} \cdot (1 - p_2)^{T - n_2} \right] \cdot I(n_1 \cdot j - n_2 \cdot l = k_2 - k_1) $$ $p_1^{n_1}$ represents the probability of $n_1$ increases in the population. $p_2^{n_2}$ represents the probability of $n_2$ decreases in the population. $(1 - p_1 - p_2)^{T - n_1 - n_2}$ represents the probability of the remaining time intervals during which the population neither increases nor decreases. This happens with probability $1 - p_1 - p_2$ , and there are $T - n_1 - n_2$ such intervals. $I(n_1 \cdot j - n_2 \cdot l = k_2 - k_1)$ is an indicator function that takes values 1 if the condition is true else 0. This is to ensure that the total number of decreases and increases respect the initial and final population However, I am not sure if this Likelihood Function prevents the population from going below 0 and some intermediate time point. I am also not sure if a combinatorial/multinomial term is needed in the likelihood I think its more difficult to write the likelihood function for Scenario 2 , even though Scenario 2 has more information compared to Scenario 1. I think this question can be broken down into two parts: A likelihood function for the times where we have full information, and a likelihood function for the times where we have missing information: $$ L(p_1, p_2, j, l) = L_{obs}(p_1, p_2, j, l) \cdot L_{unobs}(p_1, p_2, j, l) $$ From this point on, I think we can use a similar approach as in Scenario 1: $$ L_{obs}(p_1, p_2, j, l) = \prod_{i=0}^{N_{obs}-1} \sum_{n_1=0}^{t_{o_{i+1}} - t_{o_i}} \sum_{n_2=0}^{t_{o_{i+1}} - t_{o_i}} \sum_{j=0}^{J} \sum_{l=0}^{L} \left[ p_1^{n_1} \cdot (1 - p_1)^{t_{o_{i+1}} - t_{o_i - n_1}} \cdot p_2^{n_2} \cdot (1 - p_2)^{t_{o_{i+1}} - t_{o_i - n_2}} \right] \cdot I(n_1 \cdot j - n_2 \cdot l = k_{t_{o_{i+1}}} - k_{t_{o_i}}) $$ $$ L_{unobs}(p_1, p_2, j, l) = \prod_{i=0}^{N_{unobs}-1} \sum_{n_1=0}^{t_{u_{i+1}} - t_{u_i}} \sum_{n_2=0}^{t_{u_{i+1}} - t_{u_i}} \sum_{j=0}^{J} \sum_{l=0}^{L} \left[ p_1^{n_1} \cdot (1 - p_1)^{t_{u_{i+1}} - t_{u_i - n_1}} \cdot p_2^{n_2} \cdot (1 - p_2)^{t_{u_{i+1}} - t_{u_i - n_2}} \right] \cdot I(n_1 \cdot j - n_2 \cdot l = k_{t_{u_{i+1}}} - k_{t_{u_i}}) $$ I have been thinking about how to correctly write the likelihood functions for both Scenario 1 and Scenario 2 for quite some time and find myself getting lost/confused. Can someone please help me write these correctly?","['combinatorics', 'probability']"
4922712,Understanding this ratio trick,"Two numbers are in the ratio $3:5.$ If $9$ is subtracted from each, they become in the ratio $12:23.$ Find the smaller number. Solution Let's denote the two numbers as $3x$ and $5x$ . According to the problem: $ \dfrac{3x - 9}{5x - 9} = \dfrac{12}{23}$ Cross-multiplying: $$23(3x - 9) = 12(5x - 9)\\69x
- 207 = 60x - 108\\x = 11$$ Therefore, the smaller number $3x$ is $33$ . There is a trick to solving this problem. The difference between $12$ and $23$ is $11.$ If you multiply $11$ to $3$ and $5$ you get $33$ and $55.$ So, the smaller number is $33.$ I understand the above solution. How does the trick work, though?","['algebra-precalculus', 'ratio']"
4922757,Product of quotient map $p: X \to Y$ and identity $Id_Z$ is a quotient map when $Z$ is compact and Hausdorff,"Let $X,Y,Z$ be topological spaces where $Z$ is compact and Hausdorff. Let $p: X \to Y$ be a quotient map. I want to show that $$p \times Id_z: X \times Z \to Y \times Z: (x,z) \mapsto (p(x),z)$$ is a quotient map. My attempt I need to show that for an open $(p \times Id_z)^{-1}[U]$ , we have that U is open. I try to show this by finding an open neighbourhood V for any $([x],y) \in U$ which is contained in $U$ . Take $(x,y) \in (p \times Id_z)^{-1}[{([X],y)}]$ . Then we know there exists opens $U_x$ and $U_z$ such that $(x,y) \in U_x \times U_z \subseteq (p \times Id_z)^{-1}[U]$ I'm trying to show that $p(U_x) \times U_z$ is open in $Y \times Z$ . How to use that $Z$ is Hausdorff and compact? Do I need to use that $Z$ is normal? Thank you!","['general-topology', 'compactness']"
4922780,Graph $G$ with $n$ vertices is connected AND has diameter at most 2 if $\Delta$(G) + $\delta(G) \geq n-1$,"I was going through Chartrand and Zhang's ""Introduction to Graph Theory"" and found exercise 2.12 as stated in the title. I have what I think is a proof for the statement but it only shows that the diameter of the graph is at most $4$ . Here's a brief sketch of my argument; my question is whether or not the statement in the title is even true and if so, whether my proof of the statement with the relaxed diameter condition is correct, then how I can modify it to prove the title statement.
Proof (of modified statement): Let $x$ be a vertex in graph $G$ with $\deg(x) = \Delta(G)$ and $y$ be a vertex with $\deg(y) = \delta(G)$ . Then assume $x$ and $y$ are non-adjacent. Also assume $x \neq y$ . Since $\deg(x) + \deg(y) \geq n-1$ and they are non-adjacent and distinct, we know (by the pigeonhole principle) that $x$ and $y$ share at least one neighbor. In fact, $x$ and $u$ share at least one neighbor for any vertex $u \in V(G)$ since the inequality holds for any vertex not equal to $y$ as well. Thus, every vertex $u$ in the graph is either adjacent to $x$ or there is a $u-x$ path of length $2$ . Therefore, $G$ is connected and has diameter at most $4$ since the longest distance between any two vertices in the graph is at most $4$ , with two disjoint paths of length $2$ through $x$ . If $x$ and $y$ are adjacent, the argument can still be applied for any vertex $u$ and $x$ and you can always show that when $u$ and $x$ are non-adjacent, they must share a common vertex. If $x = y$ then we have $\Delta(G) = \delta(G)$ so $\delta(G) + \delta(G) \geq n-1$ therefore $\delta(G) \geq \frac{n-1}{2}$ , and it is known that such a graph must be connected with diameter at most $2$ by the same argument pigeonhole principle argument provided above. This concludes all cases. Question : How do we guarantee that the diameter is at most $2$ ? This seems like a very strong requirement","['graph-theory', 'combinatorics', 'discrete-mathematics']"
4922798,Equivalent definitions of Cantor-Bendixson Rank,"Yesterday I asked this question: Derived set of a closed subspace The motivation of my question is that I am studying the notion of Cantor-Bendixson Rank, and I have found two different definitions. I would like to know if both definitions are equivalent. I will use the following notation: if $A$ is a subset of a topological space $X$ , then $A^{(0)}=A$ and $A^{(n+1)}$ is the set of accumulation points of $A^{(n)}$ . This can be extended to ordinals, but for what I am going to ask is enough to have the definition for natural numbers. Let´s start with the first definition. Suppose $X$ is a compact topological space. First we will define the Cantor-Bendixson Rank of a point $a\in X$ as the largest natural number $n$ such that $a\in X^{(n)}$ . We will write $\textrm{CBR}_X(a)=n$ . Yes, I know that this definition should take into account not only natural numbers but also ordinals, but for the moment we may assume that the Cantor-Bendixson Rank is always finite. If $A$ is a non-empty closed subset of $X$ , we will say that the Cantor-Bendixson Rank of $A$ is $n$ if $n$ is the largest natural number for which there is some $a\in A$ with $\textrm{CBR}_X(a)=n$ . So $\textrm{CBR}_X(A)=n$ if and only if $A\cap X^{(n)}\neq\emptyset$ and $A\cap X^{(n+1)}=\emptyset$ . The second definition consists on defining the Cantor-Bendixson Rank of a closed subset $A$ of $X$ as the Cantor-Bendixson Rank of the topological space $A$ with the inherited topology, that is, the largest natural number $n$ for which $A^{(n)}$ is non-empty (calculated as a topological space itself, with the inherited topology). This makes sense, because $A$ is again compact. I cannot prove that the two definitions of $\textrm{CBR}_X(A)$ are equivalent, but I am studying some model theory (where Cantor-Bendixson Rank is important) and the definition used depends on the author. Are both definitions equivalent, at least, under some assumptions?","['elementary-set-theory', 'general-topology', 'set-theory']"
4922805,Given the maximum likelihood function- estimate the value of the parameter,"Lets  say I have the pdf and maximum likelihood function: $
f_X(x) = \begin{cases} 
\frac{\alpha \beta^\alpha}{x^{\alpha+1}}, & x > \beta, \\
0, & x \leq \beta.
\end{cases}
$ $
\begin{aligned}
& l(\alpha, \beta) = \log L(\alpha, \beta) = n \log \alpha + n\alpha \log \beta - (\alpha + 1)\sum_{i=1}^{n} \log x_i
\end{aligned}
$ In the solution they answered: \begin{aligned}
& \text{To determine the maximum likelihood estimation of } \beta, \text{ we note that the only term containing } \beta \text{ is } n\alpha \log \beta. \text{ It is monotonic and increasing in } \beta, \text{ meaning we maximize } l, \text{ and thus } L, \text{ by choosing } \beta \text{ as large as possible. The constraint we must consider is that } x_i \geq \beta \text{ for each } i = 1, \ldots, n. \text{ Therefore, we set } \hat{\beta} = \min_{1 \leq i \leq n} x_i \text{ to maximize } l \text{ with respect to } \beta.
\end{aligned} I don't really understand the explanation at all especiall anything after the ""The constraint we must consider is that...."". I'd really appreciate it if someone could explain it in easier terms.","['statistical-inference', 'statistics', 'maximum-likelihood', 'probability-theory', 'probability']"
4922825,Boundedness of the sequence $\left\{\sum_{k=1}^n \frac{1}{k}\sin\frac{n}{k}\right\}$.,"I want to study the boundedness of the sequence $\left\{\sum_{k=1}^n \frac{1}{k}\sin\frac{n}{k}\right\}$ . In one of my old post $\lim_{n\to\infty}\sum_{k=1}^n \frac{1}{k}\sin\frac{n}{k}$ , we got that: the limit $$\lim_{n\to\infty}\sum_{k=1}^n\frac{1}{k}\sin\frac{n}{k}$$ doesn't exist. Specifically, the sequence $\left\{\sum_{k=1}^n \frac{1}{k}\sin\frac{n}{k}\right\}$ is unbounded or $\left\{\sum_{k=1}^n \frac{1}{k}\sin\frac{n}{k}\right\}$ doesn't admit a finite limit. Actually, in that post, we proved that(due to Conrad 's nice answer): $$\lim_{n\to\infty}\left(\sum_{k=1}^{n+1}\frac{1}{k}\sin\frac{n+1}{k}-\sum_{k=1}^n\frac{1}{k}\sin\frac{n}{k}\right)\neq0.$$ Further more, it is not clear whether the sequence $\left\{\sum_{k=1}^n\frac{1}{k}\sin\frac{n}{k}\right\}$ is bounded or not. If it is unbounded, can it be $$\lim_{n\to\infty}\sum_{k=1}^n\frac{1}{k}\sin\frac{n}{k}=\infty?$$ I don't have enough tools to deal the sum $\sum_{k=1}^n \frac{1}{k}\sin\frac{n}{k}$ ,
any help and hints will welcome! Thanks for your attention.",['sequences-and-series']
4922833,every neighborhood of every zero of Brownian motion takes positive and negative values almost surely,"How can I prove that almost surely every neighborhood of every zero of Brownian motion takes positive and negative values? (*) I can already prove that for every zero of Brownian motion, Brownian motion takes positive and negative values on every neighborhood of that  zero almost surely. (**) That is I can prove it for every single zero but that set of measure one for different zeros can be different. If set of all zeros of Brownian motion were countable then I would be done because intersection of countably many events with probability one is an event of probability one, but it is uncountable. My book says that in order to prove (*) one can use fact that set of zeros of Brownian motion is closed set with no isolated points (^), but I don't see how to use this fact. I can prove (**) using different method than the fact (^).","['stochastic-processes', 'brownian-motion', 'probability-theory', 'probability']"
4922926,Understanding summation identity,"I'm currently working on proposition 6.3.1 from {Introduction to Statistical Time Series and I have the following equality (consider $\theta_0 = 1$ ) \begin{equation*}
\sum_{j = 1}^n \left(\theta_0 u_j + \theta_1 u_{j-1} + \cdots + \theta_q u_{j - q} \right) = \sum_{k = 0}^q \theta_k \sum_{j = 1}^{n} u_j + \sum_{s = 1}^q \sum_{j = s}^q \theta_j u_{j - s} - \sum_{s = 0}^{q-1}\sum_{j = s + 1}^q \theta_j u_{n - s}.
\end{equation*} I didn't manage to find if this right. Any tip on how to proof that this works will be appreciated!","['statistics', 'summation']"
4922986,Does the presence of neural bias change the hypothesis space of a NN?,"This question has to do with neural networks, but it is a purely mathematical question, so I think it belongs here. Consider $f: \mathbb{R}^N \to \mathbb{R}^M$ such as to be implementable with a feedforward neural network with $H$ hidden layers and without biases, so: $$ f(\vec{x}) = W_{H+2,H+1} \circ \sigma \circ W_{H+1,H} \circ \sigma \ldots \circ \sigma \circ W_{2,1} \vec{x}\tag{1}$$ where $\sigma$ is a non linearity (let's say a ReLU ), and $W_{ij}$ are weights matricies (that connect layer $j$ with layer $i$ ). Does the hypothesis space change if we add neural bias on all the hidden neurons? In other words: if we modify (1) by adding vectors of parameters ( $\vec{b_i}$ ) before each non linearity the space of functions that we can represent with $f$ (by changing the values of the parameters) changes in some way? I found no literature on the matter, some bibliographical reference would be much appreciated.","['nonlinear-system', 'nonlinear-analysis', 'functions', 'neural-networks']"
4923010,"Probability that maximum of two iid Unif(0, 1) r.v.s is less than the minimum of two other iid Unif(0,1) r.v.s?","Specifically, let $X_1, X_2, X_3, X_4$ ~ $Unif(0, 1)$ . What is $$P(max(X_1, X_2) \lt min(X_3, X_4))$$ Similarly, what is $$P(min(X_1, X_2) \gt max(X_3, X_4))$$ ?","['statistics', 'uniform-distribution', 'probability']"
4923017,Is this an equivalence of connectedness?,"Let $X$ be a Hausdorff topological space and $A\subseteq X$ . Suppose that for every $B\subseteq X$ , $A\cap Bd(B)\neq \emptyset$ (i.e., $A$ has non-empty intersection with the boundary of $B$ ) whenever $A\cap B \neq \emptyset \neq A\cap (X\setminus B)$ . Must $A$ be connected? Clearly, the other implication always holds, but I have not been able to determine if it is actually an equivalence of connectedness.","['general-topology', 'connectedness']"
4923040,Is there a way to show that $5a(a+1)\over 3a+4$ $\notin \mathbb{N}$ when $a\in\mathbb N\setminus\{2\}$? [duplicate],"This question already has answers here : Compute $\gcd(13n+2,5n-1)$ by generalized Euclidean algorithm (1 answer) When is $\small \gcd(2n^7\!+1,{3n^3\!+2})>1,$ i.e. when is $\frac{2n^{7}+1}{3n^{3}+2}$ reducible? (5 answers) Closed last month . Is there a way to show that $5a(a+1)\over 3a+4$ $\notin \mathbb{N}$ for $a\in \mathbb{N}$ (except when $a=2$ )? The expression has a few different forms, but I don't see how to show this. Any hints are greatly appreciated.","['algebra-precalculus', 'rational-functions']"
4923041,Why is properness a good analogue of compactness in scheme theory?,"Let $X$ be a $Z$ -scheme, i.e. equip $X$ with a morphism $f:X\rightarrow Z$ . Then $X$ is proper over $Z$ if it is separated over $Z$ , of finite type over $Z$ , and if $f$ is universally closed. Why is this a good analogue for compactness in the topological sense? In general, schemes that ""should not be"" compact are compact in the topological sense of the word, like the affine plane over $\mathbb C$ , is compact but since this is the algebraic geometer's version of the $\mathbb C^n$ it really ought not to be. I remember being introduced to properness as a ""good analogue"" of compactness in scheme theory, and I can't for the life of me figure out why. I was also introduced to separatedness being a good analogue of Hausdorfness, however this does make sense to me. We essentially realize that cartesian products don't really exist in the category of schemes unless we fix a base scheme $Z$ , and then we just translate the equivalent definition of Hausdorfness (i.e. the diagonal is closed) into this setting. If we are doing something like that for proper schemes over a fixed base scheme, what is the equivalent definition? Edit: To clear up what I meant about separateness, I meant we cannot take a naive cartesian product in the set theoretic sense and endow it with a scheme structure so that it fulfills the role of the direct product in the category of schemes. What we can do is take a fibre product over $\operatorname{Spec}\mathbb Z$ or take a fibre product over $Z$ in the category of $Z$ -schemes, these then fill the role of the direct product in the respective categories. Being separated as analogue of being Hausdorff is now extremely clear, but I don't get how being proper over a base scheme or over $\operatorname{Spec}\mathbb Z$ gives us the same thing as compactness.","['general-topology', 'algebraic-geometry', 'schemes', 'compactness']"
4923044,Find the radius of the red circle,"Four circles are arranged as shown in the figure below.  They're numbered from $1$ to $4$ .
If the diameter of circle $C_2$ is equal to $9$ , and $ PT = 6 , QT = 3 \sqrt{5} $ .  It is also given that $UV = 18$ .  Find the radius of the red circle. My attempt: Let the center $C_2 = (0,0)$ , and let $UP = a$ and $QV = b$ , and let $r_i$ be the radius of the $i$ -th circle, then we have $r_1, r_3, r_4$ unknown , while $r_2 = 4.5 $ .  The coordinates of $C_i$ are $ C_1 = (-4.5 - a, r_1) $ $C_2 = ( 0, 0) $ $ C_3 = (4.5 + b , r_3 )$ $ C_4 = (x_4, y_4) $ Now we write $5$ equations stemming from the distance formula relating the squared distances between pairs of centers chosen out the $4$ circles, and the square of the sum of their radii. Taking the following pairs of circles: $(1, 2), (2, 3), (1, 4), (2, 4), (3, 4) $ gives us $ (4.5 + a)^2 + r_1^2 = (r_1 + 4.5)^2 \tag{1} $ $ (4.5 + b)^2 + r_3^2 = (r_3 + 4.5)^2 \tag{2} $ $ (4.5 + a + x_4)^2 + (y_4 - r_1)^2 = (r_1 + r_4)^2 \tag{3}$ $ x_4^2 + y_4^2 = (4.5 + r_4)^2 \tag{4}$ $ (4.5 + b - x_4)^2 + (y_4 - r_3)^2 = (r_3 + r_4)^2 \tag{5}$ In addition to these five equations, we know that point $T$ is given by $ T = P + PT ( \cos \phi, \sin \phi ) $ Where $ \cos \phi = \dfrac{PT}{PQ} = \dfrac{6}{9} = \dfrac{2}{3} $ , therefore, $\sin \phi = \dfrac{\sqrt{5}}{3} $ Therefore, $ T = (-4.5, 0) + 6 ( \dfrac{2}{3} , \dfrac{\sqrt{5}}{3} ) = (-0.5 , 2 \sqrt{5} ) $ It follows that $ C_4 = \left(\dfrac{ r_4 + 4.5 }{4.5}\right) T $ And this reads $ x_4 = - 0.5 (1 + \dfrac{2}{9} r_4 ) \tag{6}$ $ y_4 = 2 \sqrt{5} (1 + \dfrac{2}{9} r_4 ) \tag{7}$ With this equation $(4)$ becomes redundant. And finally we have $ a + b = 9 \tag{8}$ Now, equations $(1),(2), (3), (5)$ after expansion, become: $9 a+ a^2 = 9 r_1 \tag{9} $ $ 9 b + b^2 = 9 r_3 \tag{10} $ $ 4.5^2 + a^2 + x_4^2 + 9 a + 9 x_4 + 2 a x_4 + y_4^2 - 2 y_4 r_1 = 2 r_1 r_4 + r_4^2 \tag{11} $ $ 4.5^2 + b^2 + x_4^2 + 9 b - 9 x_4 - 2 b x_4 + y_4^2 - 2 y_4 r_3 = 2 r_3 r_4 + r_4^2 \tag{12} $ Substituting equations $(9)$ and $(10)$ into $(11)$ and $(12)$ gives $ 4.5^2 + 9 r_1 + x_4^2 + y_4^2 + 9 x_4 + 2 a x_4 - 2 y_4 r_1 = 2 r_1 r_4 + r_4^2  \tag{13}$ $ 4.5^2 + 9 r_3 + x_4^2 + y_4^2 - 9 x_4 - 2 b x_4 - 2 y_4 r_3 = 2 r_3 r_4 + r_4 ^ 2 \tag{14}$ So our system of equations is given by equations $(6), (7), (8)$ which are linear and equations $(9), (10), (13), (14)$ which are quadratic. What remains is to how to solve these equations. So the question here is:  Are there further simplifications that can render this system of equations solvable? I appreciate your input on this. Edit: I found a reduction that can prove helpful.  Subtracting equation $(10)$ from $(9)$ , we get $ 9 (a - b) + (a - b)(a+b) = 9 r_1 - 9 r_3 $ But $ a + b = 9 $ , therefore, $ 18 (a - b) = 9 (r_1 - r_3) $ i.e. $ 2 (a - b) = r_1 - r_3 \tag{15} $ which is linear in the variables.  So now we have $4$ linear equations which are equations $(6),(7),(8),(15)$ and three quadratic equations which are equations $(9), (13), (14)$ .","['analytic-geometry', 'algebra-precalculus', 'solution-verification', 'geometry']"
