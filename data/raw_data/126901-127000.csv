question_id,title,body,tags
1934586,Algebraic subalgebras,"Let $G$ be a complex linear algebraic group with Lie algebra $\mathfrak{g}$. A subalgebra $\mathfrak{h}$ in $\mathfrak{g}$ is called algebraic if the unique connected Lie subgroup $H$ of $G$ with Lie algebra $\mathfrak{h}$ is an algebraic subgroup of $G$ (i.e. Zariski-closed). Question: Does this definition depend on the group $G$? In other words, if $\mathfrak{h}$ is algebraic, and $\tilde{G}$ is another linear algebraic group with Lie algebra $\mathfrak{g}$, is $\mathfrak{h}$ algebraic with respect to $\tilde{G}$?","['lie-algebras', 'algebraic-geometry', 'lie-groups']"
1934591,Prerequisites for studying interacting particle systems?,"I am an undergraduate physics student, and I wanted to study about interacting particle systems. I have studied probability theory, linear algebra, and statistical mechanics. I know basic measure theory and stochastic processes, but have very little knowledge of algebra. What more do I have to know to study interacting particle systems? (Please give names to some reference books if possible)","['stochastic-processes', 'probability-theory', 'statistical-mechanics']"
1934598,How many natural solutions to $x_1 + x_2 + x_3 + x_4 + x_5 + x_6 = 24$ if $x_1 + x_2 + x_3 > x_4 + x_5 + x_6$?,Here's a problem in my combinatorics textbook: Find how many solutions exist to the following equation over the natural numbers $$x_1 + x_2 + x_3 + x_4 + x_5 + x_6 = 24$$ if $$x_1 + x_2 + x_3 > x_4 + x_5 + x_6$$ I think that the problem can be solved with a generating function. First because $x_1+x_2+x_3>x_4+x_5+x_6$ then we can deduce that $x_1+x_2+x_3=x_4+x_5+x_6+y$ where $y\ge1$ because by problem definition we're dealing with natural numbers. Thus we get the following: $2x_4+2x_5+2x_6+y=24$ which translates into the following function: $$(1+x^2+x^4+...)^3(x+x^2+x^3+...)=(1+x^2+x^4+...)^3x(1+x+x^2+x^3+...)=$$ $$=\frac{1}{(1-x^2)^3}*\frac{1}{1-x}$$ now we need to find the coefficient of $x^{23}$ . Is the correct solution so far and if yes is there any closed form of the generating function or any other tip to find the coefficient or I should just open the brackets by brute force?,"['diophantine-equations', 'combinatorics', 'generating-functions']"
1934616,Why this proof of 'absolute convergence $\implies$ convergence' works for complex sequences?,"I'm reading this wikipedia proof about the absolute convergence of a complex sequence implying convergence, but it considers a real sequence. Why does it works for complex ones?","['complex-analysis', 'real-analysis', 'calculus']"
1934634,Regression: Combining errors with idempotent symmetric matrix gives chi-squared distribution,"Proposition: Consider a linear regression model $Y_n=X_\pi \beta + \varepsilon_n$ . $X_\pi$ is $n\times p_n$ matrix, and the errors $$\varepsilon _n=\left ( \varepsilon _{n1},...,\varepsilon _{nn} \right )'$$ consists of $n$ independent and identically distributed variables, $\varepsilon _{ni} \sim N(0,1)$ , for $i=1,...,n$ . Consider $M_\pi =X_\pi\left ( X_\pi'X_\pi \right )^{-1}X_\pi'$ . Then, $$\varepsilon _n' M_\pi \varepsilon _n \sim \chi ^2\left ( p_n \right ),$$ where $M_\pi$ is idempotent and symmetric matrix. How can I prove $\varepsilon _n' M_\pi \varepsilon _n $ follows chi-squared distribution??","['regression', 'statistics', 'statistical-inference']"
1934650,Is there a unique finitely additive extension of a countably additive product probability measure to the product $\sigma$-algebra for finite products?,"Consider a finite collection of measure spaces $\{(A_i, \mathcal{A}_i)\}_{i=1}^N$ where each $\mathcal{A}_i$ is a $\sigma$-algebra. Let $\mathcal{A}$ be the algebra on $\prod_{i=1}^N A_i$ generated by the sets of the form $B_1\times\cdots\times B_N$ with $B_i \in \mathcal{A}_i$. Suppose that for each $i$ we have a countably additive probability measure $\tau_i$ on $\mathcal{A}_i$. Let $\tau$ be the measure on $\mathcal{A}$ defined by 
$$\tau(B_1\times\cdots\times B_N) = \tau_1(B_1)\times\cdots\times\tau_N(B_N)$$
when $B_i \in \mathcal{A}_i$ for each $i$. The measure $\tau$ is countably additive on $\mathcal{A}$, so the Carath√©odory extension theorem lets us extend $\tau$ to a countably additive measure (which we will call $\hat{\tau}$) on $\sigma(\mathcal{A})$. The extension $\hat{\tau}$ is the only countably additive extension of $\tau$ and it is a probability measure. My question: Is $\hat{\tau}$ the only finitely additive extension of $\tau$ to a probability measure on $\sigma(\mathcal{A})$? What if each $A_i$ is a separable metric space with the Borel $\sigma$-algebra? Note : By probability measure I mean a non-negative, finitely additive set function that assigns $1$ to the whole space. I.e, probability measure does not assume countable additivity. Second note : The answer to the question is yes when each $A_i$ is countable and $\mathcal{A}_i$ is the discrete $\sigma$-algebra. The reason is that we are able to write each set in $\sigma(\mathcal{A})$ as a countable union of sets in $\mathcal{A}$: Let $\mu$ be some finitely additive extension of $\tau$ to a probability measure. Since $\mu$ is a probability measure, it is monotone. Then, for any $B \in \sigma(\mathcal{A})$,
$$\mu(B) \geq \sum_{b \in B} \tau(\{b\}) = \hat{\tau}(B)$$
and the same argument shows that $\mu(B^c) \geq \hat{\tau}(B^c)$. Since $\hat\tau$ and $\mu$ are finitely additive and assign probability $1$ to $A$, this implies that $\mu(B) = \hat{\tau}(B)$. Third note : The second note implies that the answer is yes when each $\tau_i$ is a (finite or countable) sum of point masses since we can just ignore all of the other points. An almost proof that the answer is yes : In Finitely Additive Measures by Yosida and Hewitt, Theorem 1.23 says that any non-negative finitely additive measure $\phi$ has a unique decomposition $\phi = \phi_c + \phi_p$ where $\phi_c$ is countably additive and non-negative and $\phi_p$ is purely finitely additive and non-negative. The measure $\phi_p$ being purely finitely additive means (for non-negative measures) that if $\psi$ is any countably additive measure with $0\leq\psi\leq \phi_p$, then $\psi = 0$. In our situation, we might conjecture that if $\tau'$ is any probability measure extending $\tau$, then $\tau'_c = \hat{\tau}$. This would mean that $\tau'_p(A) = 0$, so $\tau'_p = 0$. The important thing here was that we knew that $\tau'_p \geq 0$.","['functional-analysis', 'measure-theory']"
1934746,Find the sum of the series $\sum \frac{1}{n(n+1)(n+2)}$,"I got this question in my maths paper Test the condition for convergence of $$\sum_{n=1}^\infty \frac{1}{n(n+1)(n+2)}$$
  and find the sum if it exists. I managed to show that the series converges but I was unable to find the sum. Any help/hint will go a long way. Thank you.",['sequences-and-series']
1934762,"Find missing angles $x,y$ in the following diagram","The diagram is this I was trying to express all unknown angles in terms of $x$ and $y$, and do some manipulation. But unfortunately this did not give me anything since all $x,y$'s cancelled out. Any hints for the appropriate approach? Thanks.",['geometry']
1934790,Do the Taylor series of $\sin x$ and $\cos x$ depend on the identity $\sin^2 x + \cos^2 x =1$?,"I had this crazy idea trying to prove the Pythagorean trigonometric identity;$$\sin^2x+\cos^2x=1$$by squaring the infinite Taylor series of $\sin x$ and $\cos x$. But it came out quite beautiful, involving also a combinatorics identitie. The proof: $$\sin x=\frac{x}{1}-\frac{x^3}{3!}+\frac{x^5}{5!}-\frac{x^7}{7!}+...=\sum_{n=0}^{\infty}(-1)^{n}\frac{x^{2n+1}}{(2n+1)!}\\\\\sin^2x=x^2-x^4\left (\frac{1}{1!3!}+\frac{1}{3!1!}\right )+x^6\left (\frac{1}{1!5!}+\frac{1}{3!3!}+\frac{1}{5!1!}\right )-...\\\\\cos x=1-\frac{x^2}{2!}+\frac{x^4}{4!}-\frac{x^6}{6!}+...=\sum_{n=0}^{\infty}(-1)^n\frac{x^{2n}}{(2n)!}\\\\\cos^2x\!=\!1\!-\!x^2\left(\!\frac{1}{0!2!}\!+\!\frac{1}{2!0!}\!\right)\!+\!x^4\left(\!\frac{1}{0!4!}\!+\!\frac{1}{2!2!}\!+\!\frac{1}{4!0!}\!\right)\!-\!x^6\left(\!\frac{1}{0!6!}\!+\!\frac{1}{2!4!}\!+\!\frac{1}{4!2!}\!+\!\frac{1}{6!0!}\!\right)\!+...$$We should have shown that the series for both $\sin x$ and $\cos x$ converge absolutely (since we changed the arrangement), but it's obvious since the absolute value of all terms of $\sin x+\cos x$ add up to $e^x$.$$\sin^2x+\cos^2x=\\=1-x^2\left(\frac{1}{0!2!}-\frac{1}{1!1!}+\frac{1}{2!0!}\right)+x^4\left(\frac{1}{0!4!}-\frac{1}{1!3!}+\frac{1}{2!2!}-\frac{1}{3!1!}+\frac{1}{4!0!}\right)-x^6\left(\frac{1}{0!6!}-\frac{1}{1!5!}+\frac{1}{2!4!}-\frac{1}{3!3!}+\frac{1}{4!2!}-\frac{1}{5!1!}+\frac{1}{6!0!}\right)+...=\\\\=1+\sum_{n=1}^{\infty}(-1)^nx^{2n}\sum_{k=0}^{2n}\frac{(-1)^k\binom{2n}{k}}{(2n)!}$$
Since we can show easily that $\sum_{i=0}^n(-1)^i\binom{n}{i}=0$ by expanding $(1-1)^n$ using Binom's formula. So:$$\sin^2x+\cos^2x=1-0+0-0+...=1$$ I think it's beautiful. I just wanted to ask, do Taylor's series of those functions depend on this identity? Because if they do, the proof will be circular.","['combinatorics', 'taylor-expansion', 'trigonometry', 'power-series']"
1934819,Intuition behind convolution,"I always wondered about the idea behind convolution. I get what the definition of the convolution does (and I saw all the animations), but what I don't understand is how it relates to so many topics in physics. It seems to me that it is not really an intuitive concept. I guess my question is - what were the ideas, thoughts of the guy that discovered convolution? About two years ago I read a blog post about convolution that explains it pretty intuitively, but I forgot the name of the website.","['intuition', 'real-analysis', 'convolution']"
1934888,"How to intuitively understand why for a sequence of a sets $A_n$, $\liminf_{n}A_n \subseteq\limsup_{n}A_n$?","I am trying to understand exactly why for a sequence of sets $A_n$, $\liminf_{n}A_n \subseteq\limsup_{n}A_n$. My understanding is that a member of
$$
\liminf_{n}A_n =\bigcup_{N=1}^\infty \bigcap_{n\ge N} A_n
$$
is a member of at least one of the sets
$$
\bigcap_{n\ge N} A_n,
$$
so that it is a member of all but finitely many $A_n$. A member of
$$
\bigcap_{N=1}^\infty \bigcup_{n\ge N} A_n
$$
is a member of every one of the sets
$$
\bigcup_{n\ge N} A_n,
$$
So, my understanding is that it is a member of infinitely many of them, but that there may be infinitely many sets it is not a member of, since if I were to ""put"" some element inside $A_{100000}$, then all the union sets that came before have membership. However, what confuses me is why the first case is a proper subset of the second case. It seems that in BOTH the limit infimum and limit supremum case, an element of each would be an element of infinitely many $A_n$. However, in the limit supremum case, it is possible that an element only comes up every, say 1 trillion times. Then, my element is in $A_1 \cup A_2 \cup \ldots$, and $A_2 \cup A_3 \cup \ldots$, BUT, it is not in the sequence of sets until the trillionth case. In contrast, because the limit infimum deals with the intersection, the restrictions seem tighter in that an element needs to be in each and every one of them expect for finitely many. In total, it seems that the limit infimum case appears more populated and less sparse than the limit supremum case. Can anyone tell me where my logic went wrong here? thanks!!","['limsup-and-liminf', 'probability', 'elementary-set-theory']"
1934917,"Is there a way to find a pythagorean triple so that when you place a given digit before it, it still is a pythagorean triple?","For example, in base 10: $$5^2 + 12^2 = 13^2$$ And when I put a one before each number, the equality still holds: $$15^2 + 112^2 = 113^2$$ So my question is, in a given base, is there a way to get pythagorean triples that still hold when a given digit precedes each? Bonus question: And if there is, are there infinitely many?","['number-theory', 'pythagorean-triples']"
1934931,Bijection between $SO(3)$ and $S^2\times S^1$,"$SO(3)$ and $S^2\times S^1$ are in a set bijection because of having the same cardinality. Tapp in his Matrix groups book informally ""defines"" $SO(3)$ as ""all positions of a globe on a fixed stand"" and then asks: Question Is there a natural bijection between $SO(3)$ and $S^2\times S^1$? When I was reading this I thought the answer should be Yes, because each position of a (unit) globe is determined by first placing the north pole at its correct place, say $p$, and then applying a rotation to the globe around $OP$. However, later in the book Tapp writes: $SO(3)$ is not homeomorphic to $S^2\times S^1$ which implies a negative answer to the Question . I understand that the author has intentionally been vague at the beginning of the book, but I still wonder whether the map that I have described above can count as at least a discontinuous (natural) bijection between $S^2\times S^1$ and $SO(3)$. As a separate, but related question, what are the most elementary (resp. quickest) ways of seeing that $S^2\times S^1$ and $SO(3)$ are not homeomorphic or diffeomorphic. Thanks a lot!","['matrices', 'general-topology', 'smooth-manifolds']"
1934976,Interval Notation for Increasing and Decreasing Intervals of a Function,"This was brought up by another student in one of my pre-calculus classes. The graph was a simple quadratic $x^2$. The teacher stated that the graph was decreasing from $(-\infty,0)$, and increasing from $(0, \infty)$. Why would zero not be included? i.e: decr. $(-\infty,0]$ and incr. $[0, \infty)$","['notation', 'functions', 'quadratics']"
1934978,Kepler's Second Law of Planetary Motion. Solving for Theta at a known time in orbit.,The second law has been described to me above. I have taken a class on calculus and differential equations and am familiar with how to find a derivative. Given the earth's orbit I would say that the perihelion is theta of 0. Given an interval of 2592000 seconds (seconds per month) how can I use Kepler's second law to solve for the value of (THETA).Thank you. P.S. I am also interested in the values of r and r's corresponding height at time t as stated in the law's description.,['ordinary-differential-equations']
1935028,"$f(z)$ analytic, then $|f(z)|$ or $\arg(z)$ constant, then $f(z)$ constant","Ir order to show that $f(z)$ is constant I need to show that its partial derivatives are all $0$, that is: $$\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y} = \frac{\partial v}{\partial x} = -\frac{\partial u}{\partial y} = 0$$ I tried this: If $|f(z)|$ is constant, then $\sqrt{u^2+v^2} = c \implies$ $$\frac{2u}{2\sqrt{u^2+v^2}}=0\implies u = 0$$
and
$$\frac{-2v}{2\sqrt{u^2+v^2}}=0 \implies v = 0$$ That's strange because I didn't even use Cauchy-Riemann and I tought it would be necessary. Also, how to do the part of the argument?","['derivatives', 'complex-analysis', 'calculus']"
1935055,"If $u$ and $v$ satisfy Cauchy Riemann, then $u_1 = u^2-v^2$ and $v_1 = 2uv$ also satisfy","If $u$ and $v$ satisfy Cauchy Riemann, then: $$\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y}$$ $$\frac{\partial v}{\partial x} = -\frac{\partial u}{\partial y}$$ I have to show that a) $$u_1 = u^2-v^2, v_1 = 2uv$$ also satisfies. So let's begin calculating the partial derivatives: $$\frac{\partial u_1}{\partial x} = \frac{\partial u_1}{\partial u}\frac{\partial u}{\partial x} = 2u\frac{\partial u}{\partial x}$$ $$\frac{\partial v_1}{\partial y} = \frac{\partial v_1}{\partial v}\frac{\partial v}{\partial y} = 2u\frac{\partial v}{\partial y}$$ therefore $$\frac{\partial u_1}{\partial x} = \frac{\partial v_1}{\partial y}$$ Now, the other condition: $$\frac{\partial v_1}{\partial x} = \frac{\partial v_1}{\partial v}\frac{\partial v}{\partial x} = 2u\frac{\partial v}{\partial x}$$ $$\frac{\partial u_1}{\partial y} = \frac{\partial u_1}{\partial u}\frac{\partial u}{\partial y}= 2u\frac{\partial u}{\partial y}$$ which implies in: $$\frac{\partial v_1}{\partial x} = -\frac{\partial u_1}{\partial y}$$ Did I do it right? Because it seems this would almost always work. Could somebody shine a light? UPDATE As pointed below in the comments, I should differentiate $u$ and $v$ with respect to $x$, then it should look like this for the first Cauchy equality: $$\frac{\partial u_1}{\partial x} = \frac{\partial u_1}{\partial u}\frac{\partial u}{\partial x}+\frac{\partial u_1}{\partial v}\frac{\partial v}{\partial x} = 2u\frac{\partial u}{\partial x}-2v\frac{\partial v}{\partial x}$$ $$\frac{\partial v_1}{\partial y} = 2\left(u\frac{\partial v}{\partial y}+v\frac{\partial u}{\partial y}\right) = 2\left(\frac{\partial u}{\partial x}-v\frac{\partial v}{\partial x}\right)$$ therefore $$\frac{\partial u_1}{\partial x} = \frac{\partial v_1}{\partial y}$$","['derivatives', 'complex-analysis', 'calculus', 'proof-verification']"
1935086,Prove that $\frac1n\sum\limits^n_{i=1}(X_i-\overline{X})^2=\overline{X^2}-\overline{X}^{\ 2}$,"Prove that $\frac{1}{n}\sum\limits^n_{i=1}(X_i-\overline{X})^2=\overline{X^2}-\overline{X}^{\ 2}$ where $\overline{X}=\frac{1}{n}\sum\limits^n_{i=1}X_i$ and $\overline{X^2}=\frac{1}{n}\sum\limits^n_{i=1}X_i^2$ From the left, I can see that $$\frac{1}{n}\sum^n_{i=1}(X_i-\overline{X})^2=\frac{(X_1-\overline{X})^2+\cdots+(X_n-\overline{X})^2}{n} = \frac{(X_1-\frac{X_1+\cdots+X_n}{n})^2+\cdots+(X_n-\frac{X_1+\cdots+X_n}{n})^2}{n} = \cdots$$ I don't see how we can get to the right. Any suggestions?","['means', 'statistics', 'summation', 'standard-deviation']"
1935092,Prove: $\sin 2 \theta \;\ge\; \frac{1}{2}\left(-1-3 \cos^2 \theta\right)$,"Please help me prove the following inequality: $$\sin 2 \theta \;\ge\; \frac{1}{2}\left(-1-3 \cos^2 \theta\right)$$ I have been working on it for an hour in vain. I have derived $2\sin \theta \cos \theta$ from the left side and $\frac{1}{2}\left(-4+3 \sin^2 \theta\right)$ from the right side, but I don't know where to go from there.","['inequality', 'trigonometry']"
1935105,Discrete Mathematics - Graph Theory Question,"Q: https://i.sstatic.net/iHThf.jpg Pretty confident most of my answers are correct- side from the last one in Q3 iv). I've done q2 where the last walk was determined to be For Walks of length i for G2: when i is odd then the counting sequence is 3^(((N+1)^/2) ^-1) , OW 0. For G3 I've figured out that the # of vertices is a combination of r = 3 and n = i for Gi. I've drawn all 20 vertices (and their edges) for i) and ii) is 8C3. My problem is for the last 2 questions. How do I determine adjacency between vertices? Is it the edges that determine this attribute for a vertex? For g8 - there are 5 new numbers added aside from 1,2 and 3. (4,5,6,7,8) and we want the number of possibilities where order doesn't matter and repetition is not allowed from a pool of 5 numbers for 3 positions. (5c3) 456 457 458 468 467 478 578 568 567 678 iv) makes it clear that a triangle is 3 vertexes with an edge between each vertex. g8 doesn't have 9 unique values and since an edge requires the 3 points to be unique between sets, there will be at most 2 edges between any set of 3 vertices - we want 3. g9, with its 9 disintct values, can however have triangles. This one I'm still trying to figure out I tried going about it as such - given (1,2,3) we have 6 values with 6 spots to choose from remaining. The first 3 can be chosen 6C3 ways and the last 3 3C3 ways. So by the rule of product we have (6C3)(3C3) ways of getting a triangle for the point (1,2,3). - Which isn't right because we consider (123)(456)(789) and (123)(789)(456) to be the same triangle. So 6C3 /2. But then (789)(123)(456), (789)(456)(123) and also  (456)(123)(789),  (456)(789)(123) are all the same triangle as the one above, just with a different starting vertex. So for every triangle theres 6 different ways of obtaining it. if we have 9C3 ways of choosing the first 3 numbers and theres 6 identical triangles for each set of 3 beginning values - then there is a total of 9C3 of choosing unique first values. Each value has 10 different triangles - and the overlap is shared with 6 other combinations of values. So how many triangles are there? My answer is ( 9C3 * 6C2 / 2 ) / 6","['combinatorics', 'graph-theory', 'discrete-mathematics']"
1935119,"Help on Surjection, Injection, and Bijection [duplicate]","This question already has answers here : If $g \circ f$ is surjective, show that $f$ does not have to be surjective? (2 answers) Closed 7 years ago . I am a undergraduate majoring in CS. In preparation for a discrete mathematics exam coming up next week, I am looking through problems I got wrong on the homework. A concept I don't understand are surjections, injections, and bijection. From lecture, for a function to be a bijection, it has to be both an injection and a surjection. So say I proved a function is not a surjection, why couldn't I say that it has to be injection since we know it can't be a bijection by definition? So my homework problem is in the link below. Assignment Problem 4.26. Let $A$, $B$, and $C$ be sets and let $f\colon B\to C$ and $g\colon A\to B$ be functions. Let $h\colon A\to C$ be the composition, $f\circ g$, that is, $h(x)::=f(g(x))$ for $x\in A$. Prove or disprove the following claims. (a) If $h$ is surjective, then $f$ must be surjective. (b) If $h$ is surjective, then $g$ must be surjective. (c) If $h$ is injective, then $f$ must be injective. (d) If $h$ is injective and $f$ is total, then $g$ must be surjective I got 
a) True
b) False
c) True
d) False When the answer is supposed to be 
a) True
b) false
c) false
d) true I think the reason why I got them wrong is because I assumed that if a function is not surjective, then it has to be injective and vice versa. Could someone help me understand this concept? That would be much appreciated!","['examples-counterexamples', 'function-and-relation-composition', 'functions', 'discrete-mathematics']"
1935148,Graph of a continuous function has measure zero in $\mathbb{R}^2$,"I've been working on the following exercise and can't quite get it. If anyone has any suggestions, please let me know. Let $f \colon \mathbb{R} \to \mathbb{R}$ be continuous. Show that the graph of the function, $G_f = \{(x,f(x)) \colon x \in \mathbb{R}\}$ is a set of measure zero in $\mathbb{R}^2$. My intuition:  Consider writing $\mathbb{R} = \bigcup_{n \in \mathbb{Z} } [n,n+1]$, and note that since $f$ is continuous on $\mathbb{R}$, it's uniformly continuous on each compact interval comprising the union mentioned above. Thus, given any $\epsilon > 0$ there exists a $\delta > 0$ such that $|f(x) - f(y)| < \epsilon$ whenever $|x-y| < \delta$ for all $x,y \in [n,n+1]$. I would like to use this idea, however, I'm not able to cook up a clean way to handle this when I union over all $n.$. :/","['real-analysis', 'lebesgue-measure', 'measure-theory', 'analysis']"
1935155,Cube root of a binomial,The cube of a certain binomial is $8y^3-36y^2+54y-27$. Find the binomial. I know that $(a + b)^3 = a^3 + 3a^2b + 3ab^2 + b^3$ and that$(a - b)^3 = a^3 - 3a^2b + 3ab^2 - b^3$ but don't know how to go further...,['algebra-precalculus']
1935181,Lebesgue Dominated Convergence Theorem,"Good night or day or whatever time of the day you're living in. 
I'm reading the book ""The Elements of Integration and Lebesgue Measure"" written by Robert G. Bartle. When he gives the proof to the Lebesgue Dominated Convergence Theorem, he sais:
""Be redefining the functions $f_n , f$ on a set of measure $0$ we may assume that the convergence takes place on all of $X$ "". Can someone help me to understand what does he means with this redefinition, please.","['lebesgue-integral', 'measure-theory']"
1935244,"Let $p(x)$ be a monic cubic polynomial with three distinct real roots. How many real roots does $\big(p'(x)\big)^2 - 2\,p(x)\,p''(x)$ have?","So I came across this problem Let $p(x)$ be a monic polynomial of degree $3$ with three distinct real roots. How many real roots does the polynomial $\big(p'(x)\big)^2 - 2\,p(x)\,p''(x)$ have? If you let the given expression equal $f(x)$ and take its derivative, you get that $f'(x)=-12\,p(x)$ This means that the roots of $p(x)$ are where the extrema of $f(x)$ are. You can also figure out that the leading coefficient of $p(x)$ is $-3$ so for positively large and negatively large values of $x$, $f(x)$ is negative. From here, how do I figure out what is going on in between and how many real roots there are? For those who want to know, this problem is from the Swedish Mathematical Olympiad in 1988.","['derivatives', 'real-analysis', 'polynomials', 'calculus', 'contest-math']"
1935249,Prove that $\sum\limits_{A\subseteq [n]}\sum\limits_{B\subseteq [n]} |A\cap B|=n4^{n-1}$,"I want to prove the following. $$\sum_{A\subseteq [n]}\sum_{B\subseteq [n]} |A\cap B|=n4^{n-1}$$ Here is what I have thought of so far: We can treat subsets of $[n]=\{1,2, \ldots, n\}$ as sequences consisting of $1$s and $0$s, where $1$ in the $k$th entry indicates that $k$ is in the subset and a $0$ indicates it is not. When comparing two subsets $A,B \subseteq [n]$ at a particular $k$th entry, there are $4$ possibilities: 1) both $A$ and $B$ have a $1$ at the $k$th entry, 2) $A$ has a $0$ and $B$ has a $1$ at the $k$th entry, 3) $A$ has a $1$ and $B$ has a $0$ at the $k$th entry, 4) both $A$ and $B$ have a $0$ at the $k$th entry. Since $0\leq k \leq n$ then there are $4^n$ possibilities in total. However, for each $k$ only one possibility contributes to the sum, namely possibility (1), so we must divide by 4 to count each of the four possibilities as $1$, which is how I believe we have $4^{n-1}$. Maybe this is a bogus explanation but I fail to see where the $n$ comes from.","['combinatorics', 'summation', 'discrete-mathematics']"
1935262,How to calculate the limit $\lim_{n\to \infty}\prod _{i = 1}^{n}\left(1+{1 \over 2^{i}\ -\ i}\right)$,"How to calculate the limit $$\lim_{n\to \infty}\prod _{i = 1}^{n}\left(1+{1 \over 2^{i}\ -\ i}\right)\ ?$$ I can prove this limit exists by comparing it to the limit
$$
\lim_{n \to \infty}\,\,\prod _{i = 1}^{n}\left(1 + {1 \over 2^{i}}\right)\,,
$$
and this sequence seems to be related to q-Pochhammer, but I have no idea about the sequence in the title.","['sequences-and-series', 'limits']"
1935314,"Linear Algebra: If $A^3 = I$, does $A$ have to be $I$?","So it's been a while since I've taken Linear Algebra, but my friend asked me a question, that I couldn't answer. If a matrix $A$ exists such that $A^3 = I$, does $A$ have to equal the identity matrix $I$? My first instinct was to say no, but...
(edited out my incorrect math) EDIT: thanks guys for the awesome examples EDIT2: Followup question: Is there a way to solve for all possibilities of A if given A^3 = I?","['symplectic-linear-algebra', 'numerical-linear-algebra', 'linear-algebra', 'multilinear-algebra']"
1935415,Characterisation of Compact Subsets in Banach Spaces,"There is a question here about whether every compact subset $K$ of Banach space $E$ is such that $\forall \epsilon >0$, $\exists \, V \subset E$ finite dimensional subspace with $d(x,V) < \epsilon$, $\forall x \in K$. Is the converse also true? That is, being $K$ a closed, bounded set of a Banach space $E$ such that $\forall \epsilon >0$, $\exists \, V \subset E$ finite dimensional subspace with $d(x,V) < \epsilon$, $\forall x \in K$, is $K$ compact?","['functional-analysis', 'general-topology', 'banach-spaces', 'compactness']"
1935558,Is there a relation between trace of congruent matrices?,"If $A$ and $\bar{A}$ are two congruent positive definite matrices such that $A=V\bar{A}V^T$ (det$(V) \neq 0$), then is there an expression for trace$(A)$ in terms of trace$(\bar{A})$? I know trace$(\bar{A})$ and I want to find trace$(A)$. Thanks.","['matrices', 'positive-definite', 'trace', 'quadratic-forms', 'linear-algebra']"
1935570,Circular logic in set definition - Tautology?,"How useful is a definition of this form: $C=\{\beta\,:\,Q(\beta,\gamma)=0,\,\gamma\notin C\}$? (Q is some function of $\beta$ and $\gamma$.) Would this not be an instance of circular logic or does it indeed pin down the $\beta$'s uniquely? In a discussion, I was made aware of the fact that it's perfectly legitimate to use the orthogonal complement in a definition of a set but to me it seems like a tautology. Any comments on this would be much appreciated! Thank you.","['logic', 'elementary-set-theory']"
1935613,Fundamental unit of the quadratic integer ring $\mathbb{Z}[\sqrt{n}]$,"I looked in other questions but I didn't find any answers regarding quadratic integer rings. Apologies if I missed it. Given the ring $\mathbb{Z}[\sqrt{n}]$ where $n$ is a square-free positive integer, I would like to find the fundamental unit (i.e. some $a + b\sqrt{n}$ such that $\langle a + b\sqrt{n}\rangle = \mathbb{Z}[\sqrt{n}]^\times$, the units of $\mathbb{Z}[\sqrt{n}]$). I do know if I take the smallest $y$ such that $ny^2$ is of the form $x^2 \pm 1$, I get a unit, $x + y\sqrt{n}$. After all, we have $ny^2 = x^2 \pm 1$, so $ny^2 - x^2 = \pm 1$, meaning that $N(x + y\sqrt{n}) = x^2 - ny^2 = \mp 1$ ($N$ is just a standard Euclidean function). Since this particular Euclidean function is also multiplicative we know that any power of $x + y\sqrt{n}$ is also a unit. However, this is where I get stuck. Is this $x + y\sqrt{n}$ indeed a fundamental unit? If so, how do I show it can generate all units? If not, how do I find the actual fundamental unit?","['number-theory', 'quadratic-integer-rings']"
1935642,Residue field degrees for number fields,"Let $S$ be a finite set of primes and $K/\mathbb{Q}$ a finite Galois extension unramified outside $S$. For each primes $p\notin S$, let $n_p$ be the number of primes of $K$ above $p$ and $f_p$ the degree of the residue field extension. An elementary result in algebraic number theory says, $n_pf_p=[K:\mathbb{Q}]$. With the setting, my question is: for any pair of integer $(n,f)$ such that $nf=[K:\mathbb{Q}]$, does there exist $p\notin S$ such that $n_p=n$ and $f_p=f$?","['number-theory', 'galois-theory', 'algebraic-number-theory', 'field-theory']"
1935658,Find a derivable function $f$ for which $f(x) - f(x-1) =\alpha ( f(x-1) - f(x-2) )$,"Find a derivable function $f$ for which $f(x) - f(x-1) =\alpha ( f(x-1) - f(x-2) )$. My initial conditions would be:
$$\begin{align*}
f(0) &= 0\\
f(1) &= \beta
\end{align*}$$
and $\alpha < 1$
and my domain $[0,+\infty[$ Basically, if looping on integers, every increment will be $\alpha$ times the previous increment, but I want a derivable function. for example, for $\beta = 0.5$ and $\alpha = 1$: $$\begin{align*}
f(2) &= 1.5\\
f(3) &= 1.75\\
f(4) &= 1.875
\end{align*}$$
I want to be able to evaluate $f(5.7)$ for instance.","['functional-equations', 'calculus', 'functions']"
1935684,Showing the standard topology on $\mathbb{R}$ has a countable basis,"I would like to show that the standard topology on $\mathbb{R}$ has a countable basis. As far as I understand, the standard topology on $\mathbb{R}$ is generated by open intervals. So I can construct one basis as $\{(i,i+1),(i+1/2,i+3/2)\}$ for $i\in\mathbb{Z}$ which is clearly countable. EDITED: Thanks to Mees de Vries for pointing it out that my example is not a base. So is there any better way to construct a base? Is it okay to construct one basis to show the existence of a basis? Or is there way to show in general without constructing a specific example? Could somebody please give some light on this? Thanks.","['general-topology', 'proof-writing', 'alternative-proof']"
1935692,Volume of a cylindrical wedge,I would like to compute the volume of the following cylindrical wedge (the portion in yellow) using an integral. The plane that cuts the wedge goes through the very bottom of the cylinder leading to an ellipse as the cross section of the wedge. The ellipse has major axis $R$ and minor axis $r$. The ellipse's major axis makes an angle $\theta_{0}$ with the vertical axis/end of the cylinder. We have the following relations $$R\cos\theta = r$$ $$h=R\sin\theta_{0}=r\tan\theta_{0}$$ where $h$ is the height of the cylindrical wedge. How do I think through how to set up an integral for this volume?,"['multivariable-calculus', 'integration', 'volume', 'calculus']"
1935700,Can $C^k(\overline{\Omega})$ functions extend to $C^k(\mathbb{R}^n)$?,"Consider the following definition. Let the open set $\Omega\subset{\mathbb R}^n$, and $k$ be a positive integer. $C^k(\Omega)$ will denote the space of functions possessing continuous derivatives up to order $k$ on $\Omega$, and $C^k(\overline{\Omega})$ will denote the space of all $u\in C^k(\Omega)$ such that $\partial^{\alpha}u$ extends continuously to the closure $\overline{\Omega}$ for $0\leq|\alpha|\leq k$. Here is my question : If $u\in C^k(\overline{\Omega})$, can $\partial^\alpha u$ extend continuously to $\mathbb{R}^n$ for $0\leq|\alpha|\leq k$? This question is motivated by the following ones: Relationship among the function spaces $C_c^\infty(\Omega)$, $C_c^\infty(\overline{\Omega})$ and $C_c^\infty(\Bbb{R}^d)$ What does the notation $C(\bar U)$ mean for $U\subset\Bbb{R}^d$ open? Reference request: $C^k(\overline\Omega)$ as restriction of $C^{k}(\mathbb{R}^d)$ functions on $\Omega$","['multivariable-calculus', 'real-analysis', 'notation', 'partial-differential-equations']"
1935724,Why is Axiom of Choice required for the proof of countable union of countable sets is countable?,"I know that this question has been asked a lot here, some of them are duplicate of each other. I‚Äôve read every single of them, but my problem has not been resolved. I can just memorize that Axiom of Choice (AC) is needed, but I want to be clear of this, logically. Note : ‚ÄúCountable‚Äù here means infinitely countable. Recall the proof, it goes like this: First, I have countably many of countable sets. Let me enumerate them to $\langle A_i\rangle_{i\in\mathbb{N}}$. For each $i$, $A_i$ is countable. Since $A_i$ is countable for each $i$, there exists a bijection $h_i:\mathbb{N}\rightarrow A_i$ for each $i$. Then this is where Axiom of Choice (AC) comes into play. There are countably many $A_i$, so I have to choose ‚Äòcountably many‚Äô times. Then I‚Äôve got $h_i:\mathbb{N}\rightarrow A_i$ for each $i$ I want. But why is AC needed? What I understand about AC is, if I have a collection of non-empty sets, and I want to construct a new set by picking an element from each set in the collection, then AC allows me to pick them to ‚Äòconstruct‚Äô my new set. This is explicitly stated in its logical formula. For example, if I have a collection $\mathcal{A}=\lbrace A_i\rbrace_{i\in\mathbb{N}}$ where $A_i$ is non-empty for each $i$ and I want to construct a new set $\lbrace a_i\rbrace_{i\in\mathbb{N}}$ such that $a_i\in A_i$ for each $i$, then I need AC to guarantee that such set can be made. However, if my collection $\mathcal{A}=\lbrace A_i\rbrace_{i=0}^n$ is finite, then I can construct the new set without AC, i.e., it can be proved that such $\lbrace a_i\rbrace$ can be constructed without the need of AC. But what I don‚Äôt understand is, why am I not even permitted to just pick an element and manipulate them? Return to the same example, if I have a collection $\mathcal{A}=\lbrace A_i\rbrace_{i\in\mathbb{N}}$ where $A_i$ is non-empty for each $i$, then I know that there exists $a_i\in A_i$ for each $i$. Am I permitted to manipulate $a_i$ for each $i$?, like constructing $\lbrace a_i\rbrace$ by the Axiom of Pair? Can I construct a set $A_i‚Äô:=A_i-\lbrace a_i\rbrace$ for each $i$? Each $a_i$ exists logically, and I want to put bracket around them like this $\lbrace a_i\rbrace$. This is allowed by the Axiom of Pair. Of course, I cannot make $\lbrace a_i\rbrace_{i\in\mathbb{N}}$ since this is not implied by any axiom (even by the Axiom of Union or Axiom of Infinity) except AC. Most answer I found here is, I cannot even 'pick' an element from each set infinitely many times without AC (not to mention constructing a new set from them). Some people even said that each $A_i$ in the proof being countable does not mean that its enumeration is given. Well, in that case, can't I just say that since it does not have enumeration, then it's not countable and hence a contradiction to the assumption? An enumeration must exists. Some might say that it exists but is not given. Then what does 'given' mean in logic? Even in the proof that utilizes AC, we claim that a choice function exists, but not clearly stated 'which' choice function. We only know that it exists and use it to finish the proof. Why is this situation is different from knowing that each bijection $h_i$ exists for each $i$. Why are we permitted to manipulate existing choice function, but not allowed to manipulate existing bijection? Is this related to the fact that it is formulated in First-order logic? Thank you!","['elementary-set-theory', 'axiom-of-choice']"
1935730,Linear separation theorem for closed convex sets of measures,"Let $\mathcal P([0, 1])$ be the space of all probability measures on $[0, 1]$ endowed with the total variation metric. Let $P\subseteq \mathcal P([0, 1])$ be its closed convex subset, and $p'$ a measure that lies outside of it. Does there exist a bounded Borel-measurable function $f:[0,1]\to \Bbb R$ such that 
$$
  p'f > \sup\limits_{p\in P} pf.
$$ 
It seems that Hahn‚ÄìBanach separation theorem can be applied here, but I am not sure in which way exactly. Update: The answer to the original question is negative, unless we use a refined version of convexity that I had in mind initially, but did not put in the beginning, hoping that the usual definition of convexity would suffice. Unfortunately, it also means that Hahn-Banach separation theorem cannot be readily applied to my case. First, why the original question has negative answer. Let $P$ be the closure (in total variation) of all discrete measures on $[0, 1]$ with finite support. No absolutely continuous measure belongs to $P$ since they are mutually singular with discrete measures. Yet, for any measure $p'$ and any given function $f$ we cna always construct a sequence of discrete measures $p_n$ such that $\sup_n p_nf \geq p'f$. Second, instead of usual convexity I am working with barycentric convexity. A measurable set of measures $P\subseteq \mathcal P([0, 1])$ is said to be barycentrcially convex if for any measure $\nu \in \mathcal P(\mathcal P([0, 1]))$ such that $\nu(P) = 1$ it holds that $\int_P \mu \;\nu(\mathrm d\mu)\in P$. So, instead of only taking convex combinations with finite weights (like in usual convexity) we are allowed to take any possible linear combinations. So my question should read as: if $P$ is a barycentrically convex, closed (in TV) subset of $\mathcal P([0,1])$... In this formulation the previous counterexample does not apply: the set of all discrete measures is not barycentrically closed, in fact its barycentric closure consists of all measures.","['functional-analysis', 'probability-theory', 'measure-theory', 'convex-analysis']"
1935772,Are there linear transformations from vector spaces over different fields?,"I'm in a matrix theory class, and today we started talking about linear transformations. My professor noted that the range and domain of a linear transformation must be vector spaces over the same field. This made sense to me at first because of the fields are different, then something like $T(cx)=cT(x)$ doesn't make any sense when c is only in one of the vector spaces. But then I thought that maybe some fields might be compatible, like for example, $\mathbb{Z}_5$ and $\mathbb{Z}_7$ Are there any linear transformations for two spaces over different fields? or Are there functions that have all the same properties as linear transformations except that the spaces involved use different fields? If the answer is no, can you prove it?","['abstract-algebra', 'linear-algebra', 'linear-transformations']"
1935775,Pathway to number theory?,"My questions come down to these two: What are the major branches of number theory? What is a recommended pathway to these branches of number theory from only elementary mathematics (those covered in the high school curriculum)? In particular, what do I need to study? What are some texts that suit for this purpose? Here's my background. I'm a high school student who started to study number theory several months ago and quickly got fascinated by this beautiful subject. So I decided to delve deep into it. But then I realized that it's such a huge subject with so many branches to study, and what's more, most of these more advanced topics exploit tools from higher mathematics, which I know little about. With so many things to learn (number theory itself and so many prerequisites) I don't know where to start. Therefore I ask this question, seeking a self-study pathway so that I can make a study plan. I'm more ""mathematically mature"" than ordinary high school students because first, I have been preparing for (high school level) mathematical competitions and second, I was exposed to higher mathematics already. I know some basic concepts from real analysis, linear algebra, combinatorics etc. (But that doesn't mean I've learned those.) So please recommend serious texts. By the way, I'm reading Hardy & Wright's An Introduction to the Theory of Numbers and Thomas Hungerford's Algebra (GTM 73) for a foundation in abstract algebra. I hope this question will not be closed. I think a lot of people (like freshmen) can benefit from such a pathway. But for me, I don't have anyone to mentor me so I really need a detailed pathway, from which I can learn what exactly I need to do. This is really important to me and I will really appreciate your help.","['number-theory', 'reference-request', 'soft-question']"
1935789,Surjectivity problem involving permutations,"I am preparing for a contest and I have just started learning permutations. I found this interesting problem: Let $S_4$ be the set of all permutations of length $4$. Consider the function $f:S_4 \to S_4$, $f(\sigma)=\sigma^3$. Is the function $f$ surjective?","['permutations', 'functions']"
1935794,Show that $\nabla m_t$ is a polynomial o degree $p-1$,"If $m_t=\sum_{k=0}^p c_kt^k$ $t=0,\pm 1,\pm 2,\dots$ show that $\nabla
 m_t$ is a polynomial o degree $p-1$ int $t$ and hence that
  $\nabla^{p+1} m_t=0$ EDIT: I made a mistake $\nabla$ is not a gradient operator, it is a difference operator, such that $\nabla x_t=x_t-x_{
t-1}$ $$\nabla m_t=\sum_{k=0}^p c_kt^k-\sum_{k=0}^p c_k(t-1)^k$$ $$=c_pt^p+\sum_{k=0}^{p-1}c_kt^k-c_p(t-1)^p-\sum_{k=0}^{p-1}c_k(t-1)^k$$ $$=c_pt^p-c_p(t-1)^p+\sum_{k=0}^{p-1}c_kt^k-\sum_{k=0}^{p-1}c_k(t-1)^k$$ $$=c_p(t^p-(t-1)^p)+\sum_{k=0}^{p-1}c_kt^k-\sum_{k=0}^{p-1}c_k(t-1)^k$$ What I know is that when I expand $(t-1)^p$, it will give a coefficient $t^p$ that will cancel with other $t^p$ then the order of polynomial will be $p-1$, but I do not know how to develop it formally. The answer just say $$\nabla m_t=pc_pt^{p-1}+\sum_{k=0}^{p-2}b_kt^k$$","['derivatives', 'self-learning', 'time-series', 'polynomials', 'proof-verification']"
1935801,"Are $\sin(n\pi x /L)$ a basis of $L^2[0,L]$?","I'm studying a book ""Applied linear algebra"" by Sadun. The book deals linear algebra and functional analysis somewhat informally. Here is a quote from the book. I have some questions about the italicized sentences (italicizing is mine, not of the author). What we have done is exhibit three orthogonal bases for the same space ($L^2[0,L]$). One orthogonal basis is the set of functions $\sin(n \pi x/L)$ . A second orthogonal basis is the set of functions $\sin(2n\pi x/L)$ together with the functions $\cos(2n\pi x/L)$. Closely related to the second basis is the third basis $\{\exp(2n\pi xi /L)\}$ with $n$ now ranging from $-\infty$ to $\infty$. These bases are in turn obtained as the eigenfunctions of the three different operators on $L^2[0,L]$. The first operator is $d^2/dx^2$ with Dirichlet boundary conditions, whose eigenvalues are $-n^2\pi^2/L^2$, and whose eigenfunctions are the functions $\sin(n\pi x/L)$ $\cdots$ i) Are $\sin(n\pi x/L)$ really a basis for $L^2[0,L]$? How can a function whose value is nonzero at the boundaries be represented as a linear combination of them? ii) I'm not comfortable with defining an operator with boundary conditions. Are such definitions of an operator with boundary conditions conventional or usual? I think it is more appropriate to define a subspace by the boundary condition and define an operator on the subspace. Then $\sin(n\pi x/L)$ can be a basis for the subspace in which functions vanish at the boundaries. I'm not much familiar with rigorous functional analysis nor linear algebra, but the sentences in the book are somewhat wiered for me.","['functional-analysis', 'self-learning', 'linear-algebra', 'hilbert-spaces']"
1935809,What does $P(dx|H)$ stand for in probability theory?,"In the Conditional expectation Wikipedia page, in the Classical definition: Conditional expectation with respect to an event section, there is a $P(dx | H)$ notation, whose meaning I don't understand: According to the explanation below it should be $P(dx\cap H)/P(H)$, but the $dx$ part is still confusing. What I want is a very clear, purely real analysis notation, i.e., one that specifies the domain over which to integrate, the corresponding $\sigma$-algebra, the measure and the integrand function, instead of the sorta elusive probabilistic formulation. It seems the integral domain is $\mathcal X$, but neither the $\sigma$-algebra nor the measure is readily apparent to me. Any help?","['expectation', 'probability-theory', 'conditional-expectation', 'notation']"
1935819,Decide when product of discrete topologies is discrete itself,"$\{X_{\alpha}\}_{\alpha\in \Lambda}$ be discrete topological spaces and $X=\prod_{i\in\Lambda} X_i.$ Then which of the following statements imply that the product topology on $X$ equals the discrete topology on $X\ ?$ $1.\Lambda \text{ is finite}.$ $2.\Lambda \text{ is countably infinite and }X_{i} \text{ are singletons for all but a finite number of } i's.$ $3.\Lambda \text{ is uncountably infinite and }X_{i} \text{ are singletons for all but a finite number of } i.$ $4.\Lambda \text{ is infinite and }X_i \text{ are infinite for all  }\  i.$ For case $1$ , product and discrete topology on $X$ are same. For case $2$ , let the ones that are not singletons be names $X_1,X_2,............X_n,$ and the rest are $\{x_{n_1}\},\{x_{n+2}\}...............$ and so on. Here too both the topologies are same follows from the definition of product topology. For case $3$ , since only finitely many of the $X_i$ 's have more than one element, product and box topology are same on this too. Now case $4$ is confusing me a little but I think this will  have both the topologies same since arbitrary  union of open sets is open. Am I correct ? Please correct me if there is any mistake and help find the right answers. Thanks.",['general-topology']
1935837,Is every commutative ring having the invariant basis number property equivalent to AC?,"The proof I know that every commutative ring has the invariant basis number property involves quotienting by a maximum ideal to get a field, and so reducing to the case where the commutative ring is a field, for which the result is already proven. I wondered if there might be a ""direct"" proof, and tried to formalize it like this. The fact that every commutative ring has a maximal ideal is proven with Zorn's lemma, and I suspect is in fact equivalent to Zorn's lemma. If we work in a set theory without Zorn's lemma/AC, is it still true that every commutative ring has the invariant basis number?","['modules', 'abstract-algebra', 'ring-theory', 'set-theory', 'axiom-of-choice']"
1935842,How do I prove that a function decreases/increases on an interval?,"I had to determine where the following function is increasing and where it's decreasing. I can figure those out, but how do I write it down with correct notation and how could I prove it? $$ f:\mathbb{R}\to\ \mathbb{R} \qquad x\mapsto(x-3)^4 $$ I know that I can calculate the extremum/extrema by taking the second derivative: $$ f''(x)=((x-3)^4)''=(4(x-3)^3)'=12(x-3)^2 $$ and taking finding its root(s): $$
12(x-3)^2=0 \\
(x-3)^2=0 \\
x-3=0 \\
x=3
$$ I know of course, that $\mathit{f}$ has an extremum, more specifically a minimum at this point ($x=3$) And I can see from it's graph and by substituting values that it decreases on $]-\infty,3[$ and increases on $]3,+\infty[$ But how do I write this down and prove it? I was thinking about using sequences to prove, maybe?","['derivatives', 'calculus']"
1935852,"Is every $C_0(K,X)$ space isomorphic to $C(L,X)$, for some compact $L$?","Let $K$ be a locally compact Hausdorff space and $X$ be a Banach space. Denote by $C_0(K,X)$ the Banach space of all continuous $X$-valued functions defined on $K$ that vanish at infinity, equipped with the supremum norm. ($f: K \to X$ vanishes at infinity if for every $\varepsilon > 0$ there exists a compact subset $K_\varepsilon$ of $K$ satisfying $\|f(k)\| < \varepsilon$ for all $k \in K \setminus K_\varepsilon$) For a compact Hausdorff space $L$, denote $C_0(L,X)$ simply by $C(L,X)$. Is it true that for every locally compact Hausdorff space $K$ and Banach space $X$ there exists a compact Hausdorff space $L$ such that the spaces $C_0(K,X)$ and $C(L,X)$ are isomorphic? I only know this is true for the special case when $K$ is an infinite set equipped with the discrete topology (it suffices to define $L$ as the Alexandroff compactification of $K$).","['functional-analysis', 'banach-spaces']"
1935933,How many distinct bags can be formed containing at least one bagel of each kind?,"A bakery has $8$ kinds of bagels. A bag holds a dozen bagels. How many distinct bags can be formed containing at least one bagel of each kind? My Answer: We start by choosing one bagel of each type. Then we have already selected 8 bagels, and need to add 4 more to complete the dozen. This corresponds to the number of non-negative integer solutions to the equation $$\sum_{i=1}^{8}x_{i} = 4$$ which is $C_{7}^{11}=330$. Would anyone mind checking if this is correct?","['combinations', 'combinatorics']"
1935943,Does $X\setminus\emptyset=X$?,"Let $X$ be a non-empty set, my question is the following: Is $X\setminus\emptyset=X$? Instinctively, if you take off the null set from a non-empty set, it won't change anything but the null set is included in every set so if you ""take it off"" you ultimately change the set.",['elementary-set-theory']
1935970,Why isn't every linear subspace of an infinite-dimensional normed space closed?,"Can somebody please point out the error in the following reasoning? I know that there exist non-closed subspaces of infinite-dimensional normed spaces. Let $V$ be a vector space of infinite dimension equipped with a norm and $U \subsetneq V$ a proper subspace. $U$ has a basis $\{\hat{u}_\alpha\}_{\alpha \in A}$ and we can extend this basis with additional vectors $\{\hat{w}_\beta\}_{\beta \in B}$ so that $\{\hat{u}_\alpha\} \cup \{\hat{w}_\beta\}$ is a basis of $V$ . Let's say that all these basis vectors are normed. Now I want to prove that $\partial U = U$ : Let $u$ be a vector in $U$ and $\beta \in B$ arbitrary. For any $\varepsilon > 0$ the point $u + \frac{\varepsilon}{2} \hat{w}_\beta$ lies outside of $U$ , so $u \in \partial U$ . Let $v \in V \setminus U$ , so there exist a $\beta \in B$ and an $a \in \mathbb{R}$ so that $a \,\hat{w}_\beta$ appears in the linear combination that describes $v$ . Now for every $\varepsilon < |a|$ the open ball of radius $\varepsilon$ around $v$ doesn't intersect with $U$ . As a consequence $v \not\in \partial U$ . Since the boundary of a set is always closed, $U$ has to be closed.","['functional-analysis', 'linear-algebra', 'vector-spaces']"
1936005,"Subnets and compactness in $\{ 0, \dots, 9 \}^{[0,1]}$","I'm going over my knowledge of nets in preparation for some more work in functional analysis, and I came across an area of knowledge which I thought I had understood, but now seems a little flaky. The set $\{ 0, \dots, 9 \}^{[0,1]}$ is compact with the product topology, since it is the product of finite discrete (and therefore compact) sets. Therefore every net contains a convergent subnet. This is the classical example where sequential compactness fails, because if we take the sequence $$ f_k(x) = \text{The $k$'th element of the binary expansion of x} $$ Then there is no convergent subsequence $f_{i_k}$, because we can always take the number $$ y = \sum_{k = 0}^\infty \frac{(-1)^k + 1}{10^{i_k}} $$ and $f_{i_k}(y)$ alternates between 0 and 2, and hence cannot converge. Now the theory of nets tells us that there must exist a subnet $\mathfrak{a} = f_{i_\alpha}$ which converges in this set, where $i$ is an order preserving map from the directed set which forms the domain of $\mathfrak{a}$ to $\mathbf{N}$. If we then take the elements $i_1 < i_2 < \dots$ which form the image of $i$ in $\mathbf{N}$ how does the construction of $y$ fail here? Can one construct a convergent subnet for this sequence explicitly?","['functional-analysis', 'general-topology', 'nets', 'compactness']"
1936020,Why is the inverse of an orthogonal matrix equal to its transpose? [duplicate],This question already has answers here : Why is inverse of orthogonal matrix is its transpose? (5 answers) Closed 2 years ago . I don't get why that's the case. Or is it a definition? The way the concept was presented to me was that an orthogonal matrix has orthonormal columns. And that's it.,"['matrices', 'linear-algebra']"
1936062,Dividing an ODE by $x$,"When solving ODEs in class, the lecturer frequently divides the equations by $x$, usually in order to get it to its normalized form. But $x$ might be zero sometimes, How do we handle this operation, and what are its implications? Example: $x\cdot y' = y$ Divided by x: $y' = \frac{y}{x}$ Thank you.",['ordinary-differential-equations']
1936070,Easier way to discover the area of a right triangle,"In the following right triangle: $y-x=5$ and the altitude to the hypotenuse is 12. Calculate its area. I've managed to discover its area using the following method, but it ends up with a 4th degree equation to solve. Is there an easier way to solve the problem? $ha=xy \Rightarrow 12 \sqrt{x^2+y^2} = xy$ Substitute $y=5+x$ and square both sides: $144 (x^2 + (5+x)^2)=x^2 (5+x)^2 \Rightarrow x^4+10x^3-263x^2-1440x-3600=0$ Which only positive solution is $x=15$ and therefore $y=20$ and the area is $\frac{15 \cdot 20 }{2}=150$ Thanks in advance.","['problem-solving', 'triangles', 'geometry']"
1936076,"Double Integral $\iint\limits_D\frac{dx\,dy}{(x^2+y^2)^2}$ where $D=\{(x,y): x^2+y^2\le1,\space x+y\ge1\}$","Let $D=\{(x,y)\in \Bbb R^2 : x^2+y^2\le1,\space x+y\ge1\}$.  The integral to be calculated over $D$ is the following:
\begin{equation}
\iint_D \frac{dx\,dy}{(x^2+y^2)^2}
\end{equation} I do not know how to approach the problem.  I have tried integrating the function in cartesian coordinates but it doesn't seem to work out. I have also tried the variable change $u=x^2+y^2$ and $v=x+y$ (with the associated jacobian transformation) and again I cannot obtain the result.","['multiple-integral', 'bounds-of-integration', 'calculus', 'multivariable-calculus', 'integration']"
1936098,"Is there are function $f$ on the positive integers, so that $f(f(n))=n+1$","Can one find $f: \mathbb{N}\to\mathbb{N}$ so that $f(f(n))=n+1\quad \forall n \in\mathbb{N} $ ? My intuition says it should not be possible, but I don't really see a way to prove that, right now.","['algebra-precalculus', 'number-theory', 'functions']"
1936121,Prove that $\sin^4\theta+\cos^4\theta=\frac{3+\cos4\theta}{4}$,"I have a question that goes exactly like this: By considering $(\sin^2\theta+\cos^2\theta)^2$ and $(\sin^2\theta+\cos^2\theta)^3$ prove that
$$
\text{a) }\sin^4\theta+\cos^4\theta=\frac{3+\cos4\theta}{4},\qquad\text{b) }\sin^6\theta+\cos^6\theta=\frac{5+3\cos4\theta}{8}
$$ I have not idea how to do this. Please help.",['trigonometry']
1936200,Can a multi-perfect number be a perfect square?,"It is fairly easy to show that a perfect number $\Gamma$ cannot be written in the form $\Gamma=n^2$ for integer values of $n$. However, does this property hold true for multi-perfect numbers---that is, integers $R$ such that $\sigma(R)=kR,$ for $k>2$? This question is out of pure curiosity, as I can not find an answer online. UPDATE: I now know that for even $k$ and/or an even $k$-perfect number $R$ the answer is no, as it follows from contradiction by assuming $R=n^2$ and then computing $$\sigma(R)=\prod_{i=1}^{l}\sigma(p_i^{2a_i}), $$ where $n=\prod_{i=1}^lp_i^{a_i}$ (here all $p_i$ are distinct primes and all $a_i\geq1$). Given this result, my question is now whether the same is true for odd $k$ and, assuming they exist, an odd $k$-perfect number $Q$. That is, if both $k$ and $Q$ are odd in $\sigma(Q)=kQ,$ is there a way to show that $Q$ cannot be a perfect square? I believe this could be done by considering the number of divisors of $Q=\prod_{i=1}^rp_i^{a_i}$---given by $$d(Q)=\prod_{i=1}^r(a_i+1)$$ and showing that at least one $a_i\equiv1\pmod2,$ but I have not found any results similar to this online.","['number-theory', 'reference-request', 'perfect-numbers', 'square-numbers']"
1936228,Where is $f(x+iy) = x^2y^2$ complex differentiable?,"Here's my proof attempt: $$f(x+iy) = x^2y^2$$ Applying Cauchy Riemann conditions: $$\frac{\partial u}{\partial x} = 2xy^2 = \frac{\partial v}{\partial y} = 0$$ only when $x=0$ or $y=0$. The second condition is: $$\frac{\partial v}{\partial x} = 0 = -\frac{\partial u}{\partial y} = 2yx^2$$ only when $x=0$ or $y=0$. Here, I do not have $x=0$ and $y=0$ at the same time, so what can I say about the set where this function can be complex differentiable? Since the partial derivatives are continuous, this funciton will be differentiable wherever the Cauchy Riemann equations are satisfied.","['multivariable-calculus', 'complex-analysis', 'calculus', 'derivatives']"
1936232,Examples of functions that do not belong to any Baire class,"Recently I read this post , which gives examples of Baire class 2 functions.  I have also been reading the Wikipedia article on Baire functions .  The article claims that ""Henri Lebesgue proved that ... there exist functions that are not in any Baire class.""  I would like to see an example of such a function. I am having a hard time finding anything that directly addresses my specific question.  What I know so far is the definition: such a function is not the pointwise limit of any sequence of Baire class $\alpha$ functions, for any countable ordinal number $\alpha$.  I have also found this article, which shows that any such function is not Lebesgue measurable. I would also like to see some details of Lebesgue's proof that such functions exist.  To be more specific, I would like to know whether it is constructive or is only an existence result.","['continuity', 'real-analysis', 'examples-counterexamples', 'analysis']"
1936266,Where are the other solutions to this ODE disappearing in this analysis?,"I am asked to solve the following ODE involving constants $\alpha, L, V_0 > 0$ and $E < 0$: $$-\psi'' - \alpha V_0\psi\cdot [\delta(x)+\delta(x-L)] = \alpha E\psi.$$ In particular, we want solutions $\psi:\mathbb{R}\to \mathbb{C}$ that are: Continuous $L^2$ We are given that the solutions look something like this (shifted): Now pretend we didn't know this. Here is my attempt at arriving at these functions: Away from $0$ and $L$, the ODE is just the wave equation, whose solutions are always of the form $a_1e^{-i\sqrt{\alpha E}x}+a_2e^{i\sqrt{\alpha E}x}.$ So we know $\psi$ is also of this form piece-wise:
  $$\psi(x)=\left\lbrace \begin{array}{ll}
    A_1e^{\sqrt{|\alpha E|}x}+A_2e^{-\sqrt{|\alpha E|}x} & x<0 \\
    B_1e^{\sqrt{|\alpha E|}x}+B_2e^{-\sqrt{|\alpha E|}x} & 0\leq x< L \\
    C_1e^{\sqrt{|\alpha E|}x}+C_2e^{-\sqrt{|\alpha E|}x} & x\geq  L \\
\end{array} \right.$$
  (I expanded a little here, since $E<0\Rightarrow \sqrt{\alpha E}=i\sqrt{|\alpha E|}$) For $\psi$ to be integrable, we also need the exponentials with infinite > support be integrable. Then $A_2=C_1=0.$ Then we can simplify it:
  $$\psi(x)=\left\lbrace \begin{array}{ll}
    A_1e^{\sqrt{|\alpha E|}x} & x<0 \\
    B_1e^{\sqrt{|\alpha E|}x}+B_2e^{-\sqrt{|\alpha E|}x} & 0\leq x< L \\
    C_2e^{-\sqrt{|\alpha E|}x}& x\geq L \\
\end{array} \right.$$
  All that's left to do is to find the space of coefficients $A_i,B_i,C_i$ so that it is continuous and satisfies the DE. For $\psi$ to be continuous, $\lim_{x\to 0^+}\psi(x)=\lim_{x\to 0^-}\psi(x)$ and likewise at $L$. So $A_1=B_1+B_2,$ and a similar equation at $L.$ To satisfty the ODE at $0$ and $L$, because of the deltas, we know that the derivative must have a discintinuity of size $-\alpha V_0 \psi$ there. In particular: $(\lim_{x\to 0^+}\psi'(x))-(\lim_{x\to 0^-}\psi'(x))=-\alpha V_0 \psi(0),$ so computing these terms, $(\sqrt{|\alpha E|}B_1 - \sqrt{|\alpha E|}B_2)- (\sqrt{|\alpha E|}A_1) = -\alpha V_0A_1$. We have a similar equation at $L$. Now we have 4 equations relating all the coefficients to each other. I solved this system and plotted the solutions. The problem is, varying values of $\alpha, L, V_0, E$, I can only get solutions that look like the 'even' graph. Where did the odd solutions disappear in this analysis and how do you get them back?","['quantum-mechanics', 'physics', 'eigenfunctions', 'ordinary-differential-equations', 'linear-algebra']"
1936273,"Definition of complex differential forms of bidegree $(p,q)$","Let $M$ be a complex manifold of dimension $n$. It means that $M$ is a real smooth smooth manifold of dimension $2n$. Suppose that the real tangent bundle of $M$ has a local basis:
$$\left\{\frac{\partial }{\partial x_1},\ldots,\frac{\partial}{\partial x_n},\frac{\partial }{\partial y_1},\ldots,\frac{\partial}{\partial y_n}\right\}$$
and the real cotangent bundle has local basis
$$\left\{dx_1,\ldots,dx_n,dy_1,\ldots dy_n\right\}$$ Then we put $$\frac{\partial}{\partial z_j}:=\frac{1}{2}\left(\frac{\partial }{\partial x_j}-i \frac{\partial }{\partial y_j}\right)$$ 
$$\frac{\partial}{\partial \bar z_j}:=\frac{1}{2}\left(\frac{\partial }{\partial x_j}+i \frac{\partial }{\partial y_j}\right)$$
$$dz_j:=dx_j+idy_j$$
$$d\bar z_j:=dx_j-idy_j$$ Now consider the complexified cotangent bundle $(T^\ast M)_{\mathbb C}$, it has a decomposition:
$$(T^\ast M)_\mathbb C:=T^\ast M^{1,0}\oplus T^\ast M^{0.1}$$ where $$T^\ast M^{(1,0)}=\left<dz_j:j=1,\ldots n\right>$$ $$T^\ast M^{(1,0)}=\left<d\bar z_j:j=1,\ldots n\right>\,.$$ At this point one defines the algebra of differential $(p,q)$-forms on $M$ as:
$$\bigwedge^{p,q}M:=\bigwedge^{p}T^\ast M^{1,0}\otimes \bigwedge^{q}T^\ast M^{0,1}$$ So locally any $(p,q)$-differential form can be written as $$\omega=\sum_{i_1<\ldots<i_p} \alpha_{i_1,\ldots,i_p}dz_{i_1}\wedge\ldots\wedge dz_{i_p}\otimes \sum_{j_1<\ldots<j_q}\beta_{i_1,\ldots,i_q}d\bar z_{j_1}\wedge\ldots\wedge d\bar z_{j_q}\,.$$ So why in every textbook a $(p,q)$-dffierential form is written simply as:
  $$\omega=\sum \alpha_{i_1,\ldots,i_p}dz_{i_1}\wedge\ldots\wedge dz_{i_p}\wedge d\bar z_{j_1}\wedge\ldots\wedge d\bar z_{j_q}\;\;?$$
  Where is the tensor product?","['manifolds', 'complex-analysis', 'differential-geometry', 'differential-forms']"
1936331,Why the chevalley $G/B$ and plucker $G/B$ are isomorphic $G$- projective varieties,"Let $G$ be an algebraic group acting transitively on closed projective varieties $X$ and $Y$.  Let $X \xrightarrow{f} Y$ be a bijective continuous map, that is $G-$equivariant.  Does it follow that $X \to Y$ is an isomorphism of algebraic varieties? It seems roughly intuitive because if the derivative of this map is zero somewhere then it should be zero everywhere by the equivariance. If there is a transversality proof that is easier for $G$-manifolds and diffeomorphisms, or if it easier to prove it for complex algebraic varieties and biholomorphisms, i'd be all for it. My motivation: 
There are two structures as a projective variety, that one can put on $GL(n)/B$ where $B$ is the Borel subgroup(which I define as a maximal connnected solvable subgroup of $GL(n)$). Structure one:  One can use Chevalley's theorem to define what $G/B$ means as a quasiprojective variety:  its an orbit in $P(W)$ for some $G$ module $W$, which is therefore open in its closure.  Then one can show that its closed $P(W)$ because $B$ is solvable, and orbits of minimal dimension are closed.  Therefore $GL/B$ is a closed orbit in $P(W)$. Structure two:  There is a bijection of $G-$sets between $GL(n)/B$ and $Flag(\mathbb{C}^n)$.  There is a closed embedding of this into a product of grassmanians, and grassmanians are closed subvarieties of projective space by plucker.  So $GL(n)/B$ can be regarded as a closed subspace of $P(\wedge^1 \mathbb{C^n} \otimes ...\wedge^{n-1} \mathbb{C^n})$. I want to know 'Why are these $G$-varieties isomorphic as projective varieties?' and the question above would allow me to answer this. What I have got from professors:  A professor at tea told me that I need to use the correspondence between maps from projective space and line bundles over the domain.
Brainstorming:  Maybe I need to use something about $G$ equivariant line bundles over $G/B$ since these will correspond to $G$-equivariant maps from $G/B$..  Borel-weil-bott theorem anyone?","['algebraic-geometry', 'reference-request', 'algebraic-groups', 'differential-geometry', 'lie-groups']"
1936350,Determine $ax^4 + by^4$ for system of equations,"I found the following recreational problem without further specification for $a,b$. Let $x,y$ be real numbers s.t. $a + b = 6$, 
$ax + by = 10$, 
$ax^2 + by^2 = 24$,
$ax^3 + by^3 = 62$. Determine $ax^4 + by^4$. I am new to problem solving exercises like this and therefore appreciate diverse approaches to this problem as well as comments on how to tackle those types of exercises.","['algebra-precalculus', 'self-learning', 'recreational-mathematics']"
1936365,Is there an error in this AP Calculus quiz on function transformations?,"I'm taking an online AP calculus course because my high school does not offer it. One of the questions on a practice quiz (for the online course, not a problem from an official AP practice test) is as follows: Suppose a friend of yours gives you a graph of $y=f(x)$, and asks you to graph the function $y=-f(2(x-3))+4$. How would you go about doing this? The choices are: A. Start with the graph of $y=f(x)$, flip it over, squash it horizontally by a factor of 2, shift it 3 units to the right and 4 units up. B. Start with the graph of $y=f(x)$, shift it 3 units to the right and 4 units up, then squash it horizontally by a factor of 2, and finally flip it over vertically. C. Start with the graph of $y=f(x)$, squash it horizontally by a factor of 2, flip it over, shift it 3 units to the right and 4 units up. D. Start with the graph of $y=f(x)$, shift it 3 units to the left and 4 units up, then squash it horizontally by a factor of 2, and finally flip it over vertically. E. Start with the graph of $y=f(x)$, shift 4 units up, squash it horizontally by a factor of 2, flip it vertically, and finally shift it 3 units to the right. The quiz says the correct answer is B. It gives the following ""Feedback"": ""Remember to shift first, then stretch or squash, and then flip."" I think answer B is wrong. I think the correct answer should be C. A few drawings support my claim. I know that the order in which we carry out the transformations matters. I think B is wrong because if we shift horizontally before we squash horizontally, we actually need to shift SIX units right, not three. If we squash first, however, as in C, we only need to shift three units. On the other hand, maybe the issue is exactly what ""squash horizontally"" is supposed to mean. My understanding is that when we transform $y=f(x)$ into $y=f(2x)$, the graph gets squashed only because the entire plane gets squashed: all points $(x,y)$ get moved to $(x/2,y)$, and this causes the shape of the graph to look squashed relative to the original. So we're squashing about the line $x=0$. I think the teacher is mistakenly using this phrase to mean ""squash about the line $x=3$."" Who is right? Am I right that C could be the correct answer on some reasonable interpretation of ""squash horizontally by a factor of 2""?","['algebra-precalculus', 'functions', 'graphing-functions']"
1936400,Stochastic Optimization Techniques,"I am looking to collect stochastic optimization techniques
that would say under what conditions
\begin{align}
E[ f(X_2)] \le   E[ f(X_1)].
\end{align}
for some random variables $X_1$ and $X_2$. 
Here is an example of one such result that  I am aware of If $X_1$ stochastically dominates $X_2$ and $f(x)$ is  non-decreasing then
          \begin{align}
    E[ f(X_2)] \le   E[ f(X_1)].
    \end{align}
      Where $X_1$ stochasticly dominates $X_2$ if $F_{X_1}(x) \le F_{X_2}(x)$ for all $x$. The above is an example of first order stochastic dominance. There are other similar notions called second and third order stochastic dominance . This is tool can be very powerful in some stochastic optimization scenarios. However, it has its limitations. That is why I would like to collect of few other results that say under what conditions expectation with respect to distribution $X_1$ is larger (or smaller) than expectation with respect to $X_2$. If anyone is aware of any other result with similar flavor or any good reference, please let me know. Thank you.","['stochastic-analysis', 'probability-theory', 'optimization']"
1936480,Convergence of $z_n = \sqrt[n]{ni}$,I need to study the convergence of $$z_n = \sqrt[n]{ni}$$ but I don't even know where to start since the nth root of a complex number can have up to n values... How do I even begin analyzing such thing?,"['complex-analysis', 'sequences-and-series', 'calculus']"
1936503,inequality two measures in a field,"Let $\mathcal{A}$ be a field of subsets of  $X$ such that $\mathcal{F}$ is the $\sigma$-field generated by $\mathcal{A}$, and let $\mu_1$, $\mu_2$ be two measures which are $\sigma$-finite. Show that:
if $\mu_1(A)\leq \mu_2(A)$ for every $A\in\mathcal{A}$, then this inequality holds for every element in $\mathcal{F}$. I think it is related to the Caratheodory Extension Theorem, but I have no idea how to deal with the inequality given. Thanks in advance!",['measure-theory']
1936516,"If $X$ is symmetric, show $k(X^2)$ = $k(X)^2$","Suppose we have a symmetric invertible matrix $X$. How can I find $k(X^2)$ in terms of $k(X)$? Note that $k$ represents the condition number operation, that is $k(X) = \|X\|\,\|X^{-1}\|$. So, after messing around in Matlab for a while, I think $$k(X^2) = k(X)^2$$ although I dont know how to prove it. I think I need to take advantage of the fact that symmetric matrices can be diagonalized by orthogonal matrices , but I wouldn't know where to go from there. Does anybody know how to show this?","['matrices', 'normed-spaces', 'matlab', 'numerical-methods', 'linear-algebra']"
1936543,Should I put commas after my matrices in a paper?,"When I need to put a matrix in my paper, I usually use $$  to center it on my page, and I usually describe something about the matrix immediately after, e.g., ""which has real entries..."" should I put a comma after the matrix?  Is that the standard thing to do when writing a paper? There's a bit of spacing between the matrix and the comma, so it looks a bit weird, but I also want to be grammatically correct, too. What do you think? Thanks,","['matrices', 'notation', 'linear-algebra', 'article-writing']"
1936554,Sketching weird Exponential graph,"The formula of the line is - $$ y = 1 + 2^{-x} $$ Sketch the graph and show clearly whether it passes through the point $ (1,1) $ When $X = 0$ , 
$y =2$, so the $y$ intercept is at $y = 2$ . When $X = 1$, 
$Y = 1.5$ , this shows that it does not pass through the point $(1,1)$ When $X = 3$ , 
$Y = 1.125$. However when $X = 1000, 10000,100000$ 
All $Y$ values is $1$, So how do I go about sketching this graph ? I'm a little confuse . Thanks for the help !",['functions']
1936565,"Is it true that $f(z)=u(z,0)+iv(z,0)$ for complex $z$ and why?","As in the title, i came across this equivalence $$f(z)=f(x+iy)=u(x,y)+iv(x,y)=u(z,0)+iv(z,0)$$ while reading my notes on complex analysis, and tried to see why is it true, but i couldn't figure it out. Maybe it could be true only in the case that f is holomorphic.",['complex-analysis']
1936582,Prove that a number is rational if and only if from some point on its decimal expansion becomes periodic. [duplicate],"This question already has answers here : How can I prove that all rational numbers are either terminating decimal or repeating decimal numerals? (4 answers) Is there an alternative proof for periodic expansion of decimal fraction? (3 answers) Closed 1 year ago . Q: ""Prove that a number is rational if and only if from some point on its decimal expansion becomes periodic"" Please help!! I am relatively new to algebra and I find these questions very abstract. Any input/hint/solution would be highly appreciated! God bless you math guys on stackexchange!","['algebra-precalculus', 'decimal-expansion', 'rational-numbers', 'fractions']"
1936597,Kolmogorov Continuity Theorem proof in Durrett's book,"In the proof of theorem 8.1.3 of Durrett's book Probability:Theory and Examples, there is a bit which I don't understand. I don't get how we arrived at the last inequality with just triangle inequality. I can see
$|X(q)-X(r)|\le \sum_{i=1}^n|X(s_i)-X_(s_{i-1})| \le n \delta(\omega)^\gamma $ but that's about it .","['stochastic-processes', 'probability-theory']"
1936623,Why is differentiation called differentiation?,What is the etymological link between the word 'differentiation' and the procedure it describes?,"['terminology', 'math-history', 'calculus']"
1936654,What is the value of $\sum_{k=1}^{n}k!$? [duplicate],"This question already has answers here : $\sum k! = 1! +2! +3! + \cdots + n!$ ,is there a generic formula for this? (4 answers) Closed 6 years ago . What is the sum of all the factorials starting from 1 to n? Is there any generalized formula for such summation?","['factorial', 'summation', 'sequences-and-series', 'elementary-number-theory']"
1936657,"The space $C^1[a,b]$ with respect to the norm $\int ^a _b |f| +\int ^a _b |f'|$ is not complete","I am trying to find a Cauchy sequence that does not converge in this space. My attempt is $f_n = \sqrt{x^2+\frac{1}{n}}$, but I do not know how to prove it. Also, what would be the completion of this space? I have the feeling that it should be the space of absolute continuous functions since we need something that is differentiable, but I still do not know how to prove this.","['functional-analysis', 'real-analysis']"
1936671,$f:\mathbb R^2 \rightarrow \mathbb R^2$ and $G_1$ and $G_2$ are subsets of $\mathbb R^2$,"$f:\mathbb R^2\rightarrow \mathbb R^2$ is any function and $G_1$ and $G_2$ are subsets of $\mathbb R^2.$ Then $f^{-1}(G_1\cup G_2)=f^{-1}(G_1)\cup f^{-1}(G_2).$ $f^{-1}(G_1^c)=(f^{-1}(G_1))^c$ $f(G_1\cap G_2)=f(G_1)\cap f(G_2).$ $G_1$ is open and $G_2$ is closed then $G_1+G_2=\{x+y : x\in G_1 ,y\in G_2\}$ is neither closed nor open. $f(G_1\cup G_2)=f(G_1)\cup f(G_2)$ $f^{-1}(G_1\cap G_2)=f^{-1}(G_1)\cap f^{-1}(G_2)$ By using the method of set inclusion and reverse inclusion $1,3,5,6$ are proved. Here is the proof for $3:$ $$x\in f(G_1\cap G_2)\\ \implies f^{-1}(x)\in G_1\cap G_2\\ \implies f^{-1}(x)\in G_1\ \text{and} \ f^{-1}(x)\in G_2\\ \implies x\in f(G_1)\ \text{and} \ x\in f(G_2)\\ \implies x\in f(G_1)\cap f(G_2)\\ \implies f(G_1\cap G_2)\subset f(G_1)\cap f(G_2) $$ Conversely $$y\in f(G_1)\cap f(G_2)\\ \implies y\in f(G_1)\ \text{and} \ y\in f(G_2)\\ \implies f^{-1}(y)\in G_1\ \text{and}\ f^{-1}(y)\in (G_2)\\ \implies f^{-1}(y)\in G_1\cap G_2\\ \implies y\in f(G_1\cap G_2)\\ \implies f(G_1)\cap f(G_2)\subset f(G_1\cap G_2)$$ Together they imply $$f(G_1\cap G_2)=f(G_1)\cap f(G_2).$$ Similar proofs hold for $1,5,6.$ Now the problem is that if I take $G_1=\mathbb Q$ and $G_2=\mathbb Q^c$ and $f(x)=0\ \forall x\in \mathbb R$ then we find a contradiction to $3.$ So what happened? What was the wrong step in the proof written above? And because of this now I cannot say which one of $1,3,5,6$ are correct or not. Also please help prove or disprove $2$ and $4$ . Thank you.","['general-topology', 'functions']"
1936691,How mean change standard deviation?,"A college statistics class conducted a survey of how students spend their money. They asked 25 students to estimate how much money they typically spend each week on fast food. They determined that the mean amount spent on fast food is $31.52$ with a standard deviation of $21.60$.
Later they realized that a value entered as $3$ should have been $30$. They recalculate the mean and standard deviation. The mean is now $32.60$.
Which of the following is true about the standard deviation? The standard deviation will increase, because we have increased the value of a data point. The standard deviation will stay the same, because the standard deviation is not affected by a change in a single measurement. The standard deviation will decrease, because this change moved a data point closer to the mean.",['statistics']
1936711,Conditions under which an arbitrary set $A$ satisfies $\mathscr{P}(\bigcup A) \subseteq A$,"I'm working within the axioms of $\mathbf{ZF}$, and we're only considering sets made out of the empty set, $\emptyset$. One could try out simple cases, and see that $  \{\emptyset\}$ and $ \{ \emptyset, \{\emptyset \}\}$ satisfy $\mathscr{P}(\bigcup A) \subseteq A$. But how could one prove that these are the only cases, and no other sets fulfill the condition? Addendum : Is there a general method to find special cases of sets which satisfy a certain subset relation, say $\mathscr{P}(A\cup B) \subseteq \mathscr{P}(A) \cup \mathscr{P}(B)$, and prove that no other sets fit the bill? Of course, $\mathscr{P}(A)  \cup \mathscr{P}(B) \subseteq \mathscr{P}(A \cup B)$ in general, but I'm considering only subset relations which don't hold generally.  One could guess and check, but I'm thinking about a systematic procedure to find these sets.",['elementary-set-theory']
1936769,How to find ideal of an algebraic set,"I know that if $k$ is an algebraically closed field then $\mathcal {I}(Z(I))=\sqrt I$, where $I$ is an ideal in $k[x_1,x_2,\cdots ,x_n]$. But what if $k$ is not a algebraically closed field? For example if I need to find ideal of $Z(xy-1)$ or $Z(x^2+y^2-1)$, where the underlying field is $\mathbb Q$. Is there any general method for this? $\underline{Edit}$: I just need to show that the two varieties $Z(xy-1)$ and $Z(x^2+y^2-1)$ are not isomorphic over $\mathbb Q$. For this it is enough to show that their coordinate rings are not isomorphic. But I cannot figure out their coordinate rings. Any idea how to proceed? Thank you in advance.","['algebraic-geometry', 'commutative-algebra']"
1936780,Presentations of Semidirect Product of Groups,"I have seen here that given two groups $G=\langle X|R \rangle := F(X)/N(R)$ and $H=\langle Y|S \rangle := F(Y)/N(S)$, then their semidirect product can be written as: 
$$
G\rtimes_\phi H \;=\; \langle X, Y \mid R,\,S,\,yxy^{-1}=\phi(y)(x)\text{ for all }x\in X\text{ and }y\in Y\rangle \tag{1}
$$ I'm trying to prove this result with an epimorphism $\theta:F(G\times H)\rightarrow G\rtimes_\phi H$, where $\theta ((g,h))=(g,h)$ for all the ""letters"" $(g,h) \in G\times H$, and then considering the kernel: $$\ker(\theta)=\{w\in F(G\times H): \theta (w)=(1,1)\},$$
where $(1,1)$ is the identity element of $G\rtimes_\phi H$, and then $F(G\times H)/\ker(\theta) \cong G \rtimes_{\phi}H$. In $F(G\times H)$ we have a generic element: 
$$w=\prod_{i}(g_{i},h_{i})^{r_{i}}=\prod_{i}(g_{i}^{r_{i}},h_{i}^{r_{i}}),$$
which is transformed by $\theta$: 
$$\theta (w)=\prod_{i}{}^{'}(g_{i}^{r_{i}},h_{i}^{r_{i}})=\left(\prod_{i}\phi\left(\prod_{j=1}^{i-1}g_j{}^{r_{j}} \right)(h_{i}^{r_{i}}) , \prod_{i}h_{i}^{r_{i}} \right),$$
where the primed product means that it is the product of $G\rtimes_{\phi}H$. Here I'm stuck. I don't know if the relations ""$R,\,S,\,yxy^{-1}=\phi(y)(x)\text{ for all }x\in X\text{ and }y\in Y$"" have something to do with $\ker(\theta)$. Also I don't know if this approach to proving (1) is correct, nor if there is a more ""natural"" way of viewing this. Unfortunately, I can't get the book ""Presentations of Groups"" by D.L. Johnson where it's said that this is proved. Any help will be greatly appreciated!","['abstract-algebra', 'semidirect-product', 'group-theory', 'group-presentation']"
1936782,Some numbers are sums of consecutive numbers. Which numbers can be written in more than one way?,"As the title states: 
Some numbers are sums of consecutive
numbers. Which numbers can be written in more than one way? For example numbers like $15$ can be written as $1+2+3+4+5$ or $7+8$ or $4+5+6$. What algebraic proof can be made to show which numbers can be written in more than one way? Please make it simple enough for a child to understand. Thank you.",['algebra-precalculus']
1936792,Notation for ordered lists with no repetition of elements,"If $\{a_1, \ldots, a_n\}$ is the notation for a set of $n$ elements (with no repetition) and $(a_1, \ldots, a_n)$ for an ordered list of $n$ elements (with potential repetition), what is the notation for an ordered list of $n$ elements with no repetition of elements?","['notation', 'elementary-set-theory']"
1936810,Inverse trigonometry - how does this work?,"I'm learning inverse trigonometry functions from reference books,  and certain questions bother me... 
They're of the type - 
Prove that 
$$\arctan(a)  + \arctan(b) + \arctan(c) = \pi$$ And they're usually done by adding the angles in terms of $\tan$, but if $\tan(f) = 0$, $f$ should be $\pi$ or $0$, so how does it serve as a concrete proof? EDIT The proof usually follows the following ""steps"" - 
$$let arctan(a) = \alpha, arctan(b) = \beta, arctan(c) = \gamma$$
$$Also, tan(\alpha + \beta + \gamma) = \frac{tan(\alpha) + tan(\beta) + tan(\gamma) - tan(\alpha)tan(\beta)tan(\gamma)}{1 - tan(\alpha)tan(\beta) - tan(\beta)tan(\gamma) - tan(\gamma)tan(\alpha)}$$ And then we subsitute the values into the second identity, and the answer comes out  to be zero, but that doesn't quite prove that the sum of the angles is $\pi$, it could also be $0$, which is my question",['trigonometry']
1936811,"Covering a $ 31\times 31 $ square grid with $1\times 1, 2\times 2$ and $3\times 3$ tiles","I want to solve the following problem : A $31\times 31$ square grid is completely tiled by $1\times 1$, $2\times 2$ and $3\times 3$ tiles. What is the minimum number of $1\times 1$ tiles required. My approach so far :
I have tried to cover the grid with as little $1\times 1$ tiles as possible. The following is one such arrangement So here I have used six $1\times 1$ tiles. Now the question is whether I can cover the grid using fewer than six $1\times 1$ tiles. If the number of $1\times 1$, $2\times 2$ and $3\times 3$ tiles is $x,y$ and $z$ respectively, then $9x+4y+z=961$. Is this going to help to show that the arrangements for $z=1,2,3,4,5$ are / are not possible?","['combinatorics', 'contest-math']"
1936824,Is $\sqrt{n}$ polynomially larger than $\log{n}$?,"For functions $f(x)$ and $g(x)$, $f(x)$ is polynomially larger than $g(x)$ if $f(x)$ is asymptotically larger than $g(x)$ by a factor of $n^{\epsilon}$ for some constant $\epsilon > 0$. $\lim_{n \rightarrow  \infty} \frac{\sqrt{n}}{\log{n}} = \infty$ tells me that $\sqrt{n}$ is asymptotically larger than $\log{n}$, but I am stuck determining whether or not it is polynomially larger. If $\sqrt{n}$ is polynomially larger than $\log{n}$, how can I find a factor $n^{\epsilon}$?","['number-theory', 'computational-complexity', 'asymptotics', 'polynomials']"
1936829,Could someone provide me with an elegant visual proof of the Taylor Series expansion?,"I'm looking for an elegant visual proof the Taylor Series Expansion. It could be a picture or an animation, whichever you think would work best. The proof doesn't necessarily have to be rigorous or actually include the expansion, but I'd really like it if you could make it intuitive.","['alternative-proof', 'taylor-expansion', 'sequences-and-series', 'calculus']"
1936831,Locus of points satisfying a given condition.,"Find the locus of the points P in the plane of an equilateral triangle ABC for which the triangle formed with PA, PB, and PC has constant area. I have used the Heron's formula but no avail ( I already suspected that this problem could not be easily solved by this formula but I had no other idea since the only thing I know about the triangle is its sides PA PB PC ). I have no idea how to proceed. So any help will be appreciated. Update : I have seen that futurologist says that the locus will be any circle centred at the circumcentre of ŒîABC. But can anyone say why this is so?","['locus', 'area', 'geometry']"
1936865,What is the length of a point on the real number line?,"Since an interval is made up of an infinite number of points, I am considering the relation of the length of an interval and the length of a point, this lead me to ask what is the length of a point on the real number line ? The nested interval theorem made me feel the length of a point should be $0$ because of $\lim_{{n\to\infty}}(b_{n}\!-\!a_{n})=0$ While if the length of a point  on the real number line is $0$, then I get a contradiction : Supposing we remove the point $0$ on the real number line, then there is a gap there, and the width of the gap is $0$ since the length of a point  on the real number line is $0$, however I think the width of the gap being $0$ is equivalent to there being no such gap on the real number line, so this leads to a contradiction. What's wrong here? Does a point on the real number line have a width? If so, what is the length of a point on the real number line? Infinitesimal ?",['real-analysis']
1936898,What is backward reasoning in logic proofs? [duplicate],"This question already has answers here : Is it okay to reverse engineer proofs in homework questions? (5 answers) Closed 9 months ago . I was reading about Backward Reasoning, but I was not able to figure out how it works and what it really is. While reading I came across the following example mentioned in Kenneth Rosen book. Question: Given two positive real numbers x and y, their arithmetic mean is (x + y)/2 and their geometric mean is ‚àöxy . When we compare the arithmetic and geometric means of pairs of distinct positive real numbers, we find that the arithmetic mean is always greater than the geometric mean. [For example, when x = 4 and y = 6, we have 5 = (4 + 6)/2 > ‚àö4 ¬∑ 6 =‚àö24.] Can we prove that this inequality is always true? Solution: To prove that (x + y)/2 > ‚àöxy when x and y are distinct positive real numbers , we can work backward. We construct a sequence
of equivalent inequalities. The equivalent inequalities are (x + y)/2 > ‚àöxy, (x + y) 2 /4 > xy, (x + y) 2 > 4xy, x 2 + 2xy + y 2 > 4xy, x 2 ‚àí 2xy + y 2 > 0, (x ‚àí y) 2 > 0. Because (x ‚àí y) 2 > 0 when x != y , it follows that the
final inequality is true. Because all these inequalities are
equivalent, it follows that (x + y)/2 > ‚àöxy when x != y . Once we have
carried out this backward reasoning, we can easily reverse the steps
to construct a proof using forward reasoning. We now give this proof. Suppose that x and y are distinct positive real numbers. Then (x ‚àí y) 2 > 0 because the square of a nonzero real number is positive. Because (x ‚àí y) 2 =  x 2 - 2xy + y 2 > 0 . Adding 4xy to both sides, we obtain x 2 + 2xy + y 2 > 4xy . Because x 2 + 2xy + y 2 = (x + y) 2 , this means that (x + y) 2 ‚â• 4xy . Dividing both sides of this equation by 4, we see that (x + y) 2 /4 > xy . Finally, taking square roots of both sides (which preserves the inequality because both sides are positive) yields (x + y)/2 > ‚àöxy . We conclude that if x and y are distinct positive real numbers, then their arithmetic mean (x + y)/2 is greater than their geometric mean ‚àöxy . I didn't understand this proof. This proof is like if you have to prove a = b, then you add 5 both sides and get a+5 = b+5. Now using forward reasoning you prove a = b. Correct me if I am wrong.
Also, please illustrate how backward reasoning works.You can use appropriate examples.
Thank You.","['proof-theory', 'proof-explanation', 'proof-verification', 'discrete-mathematics']"
1936901,Showing that positive definite symmetric matrices have a smooth square root,"Show that a map in the vector space of positive-definite symmetric $n \times n$ matrices $$f:S_n^+(\mathbb{R}) \rightarrow S_n^+(\mathbb{R})$$
  $$f(s) = s^2$$
  is a smooth diffeomorphism. My thought is that if I can show that the derivative $f'(a)(b)$ is invertible for all $s \in S_n^+(\mathbb{R})$, then I will have shown that there is a positive definite square root that varies smoothly with $s$. Since $s$ is a symmetric positive definite matrix, we know that it has $n$ eigenvectors with positive eigenvalues. Further, we know that 
$$f'(a)(b) = ab + ba$$ $$ ab + ba = 0 $$
$$ abb^T + bab^T = 0 $$ Suppose that $b$ has non-zero entries. We know that $a, bb^T$ are both positive definite matrices, and we know that diagonal entries of $bab^T$ are positive, since they are equal to $b_iab_i^T$, which is positive because $a$ is positive definite. How do I show that the product $abb^T$ has non-negative entries on the diagonal? I want to show that this results in a contradiction.","['differential-geometry', 'positive-definite', 'linear-algebra', 'lie-groups']"
1936947,Is the sum of two tight sequences tight?,"Suppose that $X_n$ and $Y_n$ are sequences of random elements defined on the same probability space $(\Omega,\mathcal F,P)$ and taking values in some separable Hilbert space $H$ (I am particularly interested in the case when $H=L^2([0,1],\mathbb R)$). We say that the sequence $X_n$ is tight if for each $\varepsilon>0$ there exists a compact subset $K_\varepsilon$ of $H$ such that
$$
P(X_n\in K_\varepsilon)\ge1-\varepsilon
$$
for each $n\ge1$. Suppose that the sequences $X_n$ and $Y_n$ are tight. Is the sequence $X_n+Y_n$ tight? If $X_i$ and $Y_i$ are independent for each $i\ge1$, the following proof seems to work. Let $H\times H$ denote the Cartesian product of the Hilbert space $H$ with itself. The inner-product of $H\times H$ is given by
$$
\langle (f,g),(h,k)\rangle_{H\times H}=\langle f,h\rangle_H+\langle g,k\rangle_H.
$$
Then the sequence $(X_n,Y_n)$ with values in $H\times H$ is also tight since the Cartesian product of two compact sets is a compact set and we have that
\begin{align*}
	P((X_n,Y_n)\in K_\varepsilon\times T_\varepsilon)
	&=P(X_n\in K_\varepsilon,Y_n\in T_\varepsilon)\\
	&=P(X_n\in K_\varepsilon)P(Y_n\in T_\varepsilon)\\
	&\ge (1-\varepsilon)^2.
\end{align*}
 Let us define the function $\mathcal S:H\times H\to H$ by setting $\mathcal S(f,g)=f+g$ for each $(f,g)\in H\times H$. This function is continuous since
$$
	\|f+g-(h+k)\|_H\le\|f-h\|_H+\|g-k\|_H
$$
and
$$
	\|(f,g)-(h,k)\|_{H\times H}^2=\|f-h\|_H^2+\|g-k\|_H^2.
$$
Hence, the sequence $X_n+Y_n$ is tight using the fact that if the sequence $X_n$ is tight, then so is $f(X_n)$, where $f$ is a continuous function. Is this proof correct? Does this proof depend on the particular form of the inner product of the space $H\times H$? Is it possible to relax the assumption of independence? Any help is much appreciated!","['hilbert-spaces', 'reference-request', 'probability-theory', 'proof-verification', 'random-variables']"
1936957,Prove that a sequence converges to $\sqrt{2}$ [duplicate],"This question already has answers here : Showing the sequence converges to the square root [duplicate] (2 answers) Closed 7 years ago . Given $x_1 = 2$ and $x_{n+1} = \dfrac{1}{2} x_n + \dfrac{1}{x_n}$, prove that $x_n \to \sqrt{2}$. I thought I could use monotone convergence but I have a hard time proving the monotonicity of the sequence.",['sequences-and-series']
1936983,continuity of a function of two variables on a circle,"Let 
$f(x,y)=1$ on $x^2+y^2=1$ and 0 otherwise. At what points $f$ is not continuous? The answer is, $f$ is discontinuous everywhere on the circle $x^2+y^2=1$. Could anyone explain to me intuitively why? Rigorously how?","['multivariable-calculus', 'real-analysis', 'calculus']"
1936985,On the definition of the holomorphic tangent space,"First of all I'll motivate my question: when we define the tangent space to a smooth manifold $M$ we can go through two different approaches that at the end give the same result. First approach We pick $U \subset X$ an open subset and $p \in U$. We define a derivation at $p$ as a $\mathbb{R}$ linear map $D: C^{\infty}_X(U) \rightarrow \mathbb{R}$ that verifies Liebntiz's condition at $p$, i.e. $D(f \, g) = D(f) \, g(p) + f(p) \, D(g)$. We then define the tangent space in $p$ at $U$ as $T_{p, U} =  \{ D: C^{\infty}_X(U) \rightarrow \mathbb{R} \; \text{derivation at $p$}\}$. Using bump functions one can prove that the inclusion $i : U \hookrightarrow X$ induces an isomorphism between the tangent spaces at $p \in U$ and so the concept of tangent space turns out to be local. Second approach We do the same except that we define a derivation as a $\mathbb{R}$ linear map $D: C^{\infty}_{p,X} \rightarrow \mathbb{R}$, where with $C^{\infty}_{p,X}$ it's the stalk at $p$ of the sheaf $C^{\infty}_X$. In this way the tangent space is defined as a local concept. My question deal with the holomorphic tangent space. It is defined via derivation at a point $p$: $\mathbb{C}$ linear map $D: \mathcal{O}_{p,X} \rightarrow \mathbb{C}$.
Is the reason for which it is defined this way that we don't have a holomorphic analogous of $C^{\infty}$ partitions of unity? I guess it is, but I would like a confirmation. If we had defined the holomorphic tangent space as we did in the first approach for the smooth tangent space, what would have been the problem? Would have we had some pathological situation? Thank you!","['complex-geometry', 'differential-geometry', 'definition']"
1936988,"""The digits used in artificial numbers are random while the real numbers aren't and their digits distribution is specific to their business""","Related to the question https://math.stackexchange.com/questions/1924178/tools-to-measure-the-nonrandomness-of-database , I'm somehow looking for some tools to measure the nonrandomness of databases. Igael gave me a hint I don't understand so far : ""the digits used in artificial numbers are random while the real numbers aren't and their digits distribution is specific to their business"". Question : Could anyone be able to explain to me in details what it means exactly?","['statistics', 'math-history', 'algorithmic-randomness']"
1936991,$G$ is a $p$-group $\implies $ $G$ has a normal subgroup of $p^k \ \forall k$,"I am a beginner to $p$ -groups, so please help me with the following qustion: If $|G|=p^n$ , where $p$ is prime, and $0\le k\le n$ , then $G$ contains a normal subgroup of order $p^k$ . My work: I have just been able to prove that $G$ has a normal subgroup of order $p$ . Since $G$ is a $p$ group, its center $Z(G)\ne \{e\}$ $\therefore$ $Z(G)$ is $p$ -subgroup of $G$ .By Cauchy's lemma, there is an element $a\in Z(G)$ such that $|a|=p$ .Then $N=\langle a\rangle$ is a subgroup of order $p$ .Moreover since every subgroup of $Z(G)$ is normal in $G$ , $\therefore$ $N\triangleleft G$ . Now I dont know how to proceed. Maybe induction will help? Or is there some better way?","['finite-groups', 'abstract-algebra', 'normal-subgroups', 'p-groups']"
